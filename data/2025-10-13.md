<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 5]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Neptune: Advanced ML Operator Fusion for Locality and Parallelism on GPUs](https://arxiv.org/abs/2510.08726)
*Yifan Zhao,Egan Johnson,Prasanth Chatarasi,Vikram Adve,Sasa Misailovic*

Main category: cs.PL

TL;DR: Neptune是一个张量编译器，通过打破循环依赖并构造代数校正表达式，实现了复杂归约计算（如注意力机制）的高级算子融合，在多种GPU架构上平均加速1.35倍。


<details>
  <summary>Details</summary>
Motivation: 现有张量编译器难以融合涉及循环依赖的复杂归约计算，如注意力机制，这限制了算子融合优化的效果。

Method: 提出Neptune编译器，通过有意打破现有依赖关系，并构造代数校正表达式来补偿，从而允许内核产生正确结果。

Result: 在10个注意力基准测试中，Neptune从简单注意力代码和高级调度模板开始，在NVIDIA和AMD的4种GPU架构上平均比最佳替代方案快1.35倍。

Conclusion: Neptune通过创新的依赖打破和代数校正方法，有效解决了复杂归约计算的融合问题，显著提升了深度学习工作负载的性能。

Abstract: Operator fusion has become a key optimization for deep learning, which
combines multiple deep learning operators to improve data reuse and reduce
global memory transfers. However, existing tensor compilers struggle to fuse
complex reduction computations involving loop-carried dependencies, such as
attention mechanisms.
  The paper introduces Neptune, a tensor compiler for advanced operator fusion
for sequences of reduction operators. Neptune presents a new approach for
advanced operator fusion, which intentionally breaks some existing dependencies
and compensates by constructing algebraic correction expressions that allow the
kernel to produce the correct result.
  On ten attention-based benchmarks, Neptune, starting from simple attention
code and a high-level scheduling template, outperforms existing compilers like
Triton, TVM, and FlexAttention, including Triton-based implementations of
FlashAttention. Across four different GPU architectures from NVIDIA and AMD,
Neptune-generated kernels have average speedup of $1.35\times$ over the next
best alternative, demonstrating its effectiveness for deep learning workloads.

</details>


### [2] [Typestate via Revocable Capabilities](https://arxiv.org/abs/2510.08889)
*Songlin Jia,Craig Liu,Siyuan He,Haotian Deng,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 提出了一种统一作用域管理和流敏感类型状态跟踪的新方法，通过扩展流不敏感的能力机制来实现流敏感的类型状态跟踪，在Scala 3中实现原型。


<details>
  <summary>Details</summary>
Motivation: 解决状态资源管理的挑战：作用域结构（如Java同步块）虽然易于推理但限制表达性和并行性；流敏感管理提供细粒度控制但需要复杂类型状态分析和显式状态跟踪。

Method: 扩展流不敏感能力机制为流敏感类型状态跟踪，将能力生命周期与词法作用域解耦，允许函数以流敏感方式提供、撤销和返回能力。在Scala 3编译器中实现，利用路径依赖类型和隐式解析。

Result: 原型支持广泛的状态模式，包括文件操作、高级锁定协议、DOM构建和会话类型，实现了简洁、静态安全且表达性强的类型状态编程。

Conclusion: 证明了通过最小化扩展现有基于能力的语言，可以实现表达性强且安全的类型状态管理，为更健壮和符合人体工程学的状态编程铺平道路。

Abstract: Managing stateful resources safely and expressively is a longstanding
challenge in programming languages, especially in the presence of aliasing.
While scope-based constructs such as Java's synchronized blocks offer ease of
reasoning, they restrict expressiveness and parallelism. Conversely,
imperative, flow-sensitive management enables fine-grained control but demands
sophisticated typestate analyses and often burdens programmers with explicit
state tracking.
  In this work, we present a novel approach that unifies the strengths of both
paradigms by extending flow-insensitive capability mechanisms into
flow-sensitive typestate tracking. Our system decouples capability lifetimes
from lexical scopes, allowing functions to provide, revoke, and return
capabilities in a flow-sensitive manner, based on the existing mechanisms
explored for the safety and ergonomics of scoped capability programming.
  We implement our approach as an extension to the Scala 3 compiler, leveraging
path-dependent types and implicit resolution to enable concise, statically
safe, and expressive typestate programming. Our prototype generically supports
a wide range of stateful patterns, including file operations, advanced locking
protocols, DOM construction, and session types. This work demonstrates that
expressive and safe typestate management can be achieved with minimal
extensions to existing capability-based languages, paving the way for more
robust and ergonomic stateful programming.

</details>


### [3] [Free to Move: Reachability Types with Flow-Sensitive Effects for Safe Deallocation and Ownership Transfer](https://arxiv.org/abs/2510.08939)
*Haotian Deng,Siyuan He,Songlin Jia,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 提出了一种基于可达性类型的流敏感效应系统，支持高阶不纯函数式语言中的显式内存管理，包括Rust风格的移动语义。


<details>
  <summary>Details</summary>
Motivation: 将可达性推理与显式资源控制相结合，为高阶函数式语言提供安全的手动内存管理方案，无需区域或线性类型。

Method: 通过多态的use和kill效应来细化现有可达性限定符，记录引用的读取、写入、转移和释放操作。效应规则使用限定符跟踪每个资源的操作。

Result: 系统能够表达所有权转移、上下文新鲜度和破坏性更新，验证释放后使用安全性，并机械化所有元理论结果。

Conclusion: 该工作将基于可达性的推理与显式资源控制集成，推动了高阶函数式语言中安全手动内存管理的技术发展。

Abstract: We present a flow-sensitive effect system for reachability types that
supports explicit memory management, including Rust-style move semantics, in
higher-order impure functional languages. Our system refines the existing
reachability qualifier with polymorphic \emph{use} and \emph{kill} effects that
record how references are read, written, transferred, and deallocated. The
effect discipline tracks operations performed on each resource using
qualifiers, enabling the type system to express ownership transfer, contextual
freshness, and destructive updates without regions or linearity. We formalize
the calculus, its typing and effect rules, and a compositional operational
semantics that validates use-after-free safety. All metatheoretic results,
including preservation, progress, and effect soundness, are mechanized. The
system models idioms such as reference deallocation, move semantics, reference
swapping, while exposing precise safety guarantee. Together, these
contributions integrate reachability-based reasoning with explicit resource
control, advancing the state of the art in safe manual memory management for
higher-order functional languages.

</details>


### [4] [Concept-Based Generic Programming in C++](https://arxiv.org/abs/2510.08969)
*Bjarne Stroustrup*

Main category: cs.PL

TL;DR: 介绍C++概念编程技术，通过概念表达泛型代码约束，提供消除窄化转换和范围检查的类型系统，展示概念在泛型编程中的实用性和基本原理。


<details>
  <summary>Details</summary>
Motivation: 展示C++概念编程的设施和原理，概念作为表达泛型代码约束的方式，旨在提供用户自定义类型系统扩展，强调概念在泛型编程中的实用价值。

Method: 使用概念编程技术，构建简单类型系统来消除窄化转换并提供范围检查，通过概念实现用户定义的类型系统扩展，利用统一符号、lambda、可变模板等C++特性。

Result: 开发出无冗余符号或运行时开销的类型系统，成功展示了概念在泛型编程中的实用性，验证了概念作为C++泛型编程核心组件的有效性。

Conclusion: 概念是C++泛型编程的重要组成部分，而非孤立子语言，提供了强大的类型约束机制，支持通用编程和泛型编程的统一，是C++类型系统的关键扩展。

Abstract: We present programming techniques to illustrate the facilities and principles
of C++ generic programming using concepts. Concepts are C++'s way to express
constraints on generic code. As an initial example, we provide a simple type
system that eliminates narrowing conversions and provides range checking
without unnecessary notational or run-time overheads. Concepts are used
throughout to provide user-defined extensions to the type system. The aim is to
show their utility and the fundamental ideas behind them, rather than to
provide a detailed or complete explanation of C++'s language support for
generic programming or the extensive support provided by the standard library.
Generic programming is an integral part of C++, rather than an isolated
sub-language. In particular, key facilities support general programming as well
as generic programming (e.g., uniform notation for types, lambdas, variadic
templates, and C++26 static reflection). Finally, we give design rationales and
origins for key parts of the concept design, including use patterns, the
relationship to Object-Oriented Programming, value arguments, notation, concept
type-matching, and definition checking.

</details>


### [5] [A Multilingual Python Programming Language](https://arxiv.org/abs/2510.09591)
*Saad Ahmed Bazaz,Mirza Omer Beg*

Main category: cs.PL

TL;DR: 提出了一个基于Python的通用语言转译器UniversalPython，允许用户使用自己的母语编写Python代码，旨在降低编程语言对英语知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有编程语言都要求英语知识，这对许多没有时间和资源学习英语的新手构成了障碍。研究表明人们用母语学习效果更好。

Method: 构建一个基于Python的语言转译器，能够将用各种人类语言编写的代码转换为标准Python代码，并以乌尔都语Python为例进行演示。

Result: 成功开发了开源转译器，能够创建"乌尔都语Python"版本，证明了该方法的可行性。

Conclusion: 该转译器有潜力扩展到更多人类语言，从而增加编程的普及性，代码已在GitHub上开源。

Abstract: All widely used and useful programming languages have a common problem. They
restrict entry on the basis of knowledge of the English language. The lack of
knowledge of English poses a major hurdle to many newcomers who do not have the
resources, in terms of time and money, to learn the English language. Studies
show that people learn better in their own language. Therefore, we propose a
language transpiler built on top of the Python programming language, called
UniversalPython, which allows one to write Python in their own human language.
We demonstrate the ability to create an "Urdu Python" with this transpiler. In
the future, we aim to scale the language to encapsulate more human languages to
increase the availability of programming. The source code for this transpiler
is open-source, and available at
https://github.com/universalpython/universalpython

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Maple: A Multi-agent System for Portable Deep Learning across Clusters](https://arxiv.org/abs/2510.08842)
*Molang Wu,Zhao Zhang*

Main category: cs.DC

TL;DR: Maple是一个多代理系统，能够通过用户自然语言输入生成正确的深度学习命令行，解决了在异构GPU集群上训练深度学习模型时命令行配置的复杂性。


<details>
  <summary>Details</summary>
Motivation: 在GPU集群上训练深度学习模型时，用户需要适应异构启动器、调度器、亲和性选项、DL框架参数和环境变量，手动组合命令行容易出错且令人沮丧，阻碍研究进展并浪费资源。

Method: Maple采用四代理系统架构，包括信息提取、模板检索、命令行验证和错误纠正功能，利用多个总参数达10B的语言模型。

Result: 在9个美国国家计算中心的GPU集群、5个代表性深度学习模型家族和4种常用并行训练范式上进行测试，Maple在567个测试案例中实现了92.0%的准确率，性能与GPT-5、Claude和Gemini等最先进模型相当。

Conclusion: Maple在异构高性能计算环境中实现了可移植和可扩展的分布式深度学习训练，具有重要的实用价值。

Abstract: Training deep learning (DL) models across Graphics Processing Unit (GPU)
clusters is technically challenging. One aspect is that users have to compose
command lines to adapt to the heterogeneous launchers, schedulers, affinity
options, DL framework arguments, and environment variables. Composing correct
command lines is error-prone and can easily frustrate users, impeding research
or wasting resources. In this work, we present Maple, a multi-agent system that
generates correct DL command lines with users' natural language input. Maple
consists of four agents with the functionalities of information extraction,
template retrieval, command line verification, and error correction. We
evaluate Maple on nine GPU clusters across national computing centers in the
U.S., five representative deep learning model families, and four commonly used
parallel DL training paradigms. Our experiments also cover schedulers of SLURM
and PBS and heterogeneous architectures, such as NVIDIA A100/H200 GPUs and
Intel Max series GPUs. Maple achieves 92.0% accuracy in generating command
lines across the 567 test cases. Leverage multiple language models with an
aggregated size of 10B parameters, Maple delivers comparable performance to the
state-of-the-art models of GPT-5, Claude, and Gemini. Together, these results
highlight Maple's practical value in enabling portable and scalable distributed
DL across heterogeneous HPC environments.

</details>


### [7] [Slicing Is All You Need: Towards A Universal One-Sided Algorithm for Distributed Matrix Multiplication](https://arxiv.org/abs/2510.08874)
*Benjamin Brock,Renato Golin*

Main category: cs.DC

TL;DR: 提出了一种通用的单边分布式矩阵乘法算法，支持所有分区和复制因子组合，通过切片计算重叠瓦片集合，性能与PyTorch DTensor相当。


<details>
  <summary>Details</summary>
Motivation: 现有分布式矩阵乘法算法仅支持部分分区方式，需要多个算法实现来覆盖所有可能的分区组合，否则需要重新分布操作数增加通信成本。

Method: 使用切片（索引算术）计算需要相乘的重叠瓦片集合，然后直接执行或重新排序并降低到优化IR以最大化重叠。基于高级C++ PGAS编程框架实现，使用节点内互连进行GPU到GPU直接通信。

Result: 在各种分区和复制因子下评估性能，发现与针对AI模型优化的PyTorch DTensor性能相当。

Conclusion: 提出的通用单边算法能够支持所有分区和复制因子组合，解决了现有算法只能支持部分分区方式的问题，性能表现优异。

Abstract: Many important applications across science, data analytics, and AI workloads
depend on distributed matrix multiplication. Prior work has developed a large
array of algorithms suitable for different problem sizes and partitionings
including 1D, 2D, 1.5D, and 2.5D algorithms. A limitation of current work is
that existing algorithms are limited to a subset of partitionings. Multiple
algorithm implementations are required to support the full space of possible
partitionings. If no algorithm implementation is available for a particular set
of partitionings, one or more operands must be redistributed, increasing
communication costs. This paper presents a universal one-sided algorithm for
distributed matrix multiplication that supports all combinations of
partitionings and replication factors. Our algorithm uses slicing (index
arithmetic) to compute the sets of overlapping tiles that must be multiplied
together. This list of local matrix multiplies can then either be executed
directly, or reordered and lowered to an optimized IR to maximize overlap. We
implement our algorithm using a high-level C++-based PGAS programming framework
that performs direct GPU-to-GPU communication using intra-node interconnects.
We evaluate performance for a wide variety of partitionings and replication
factors, finding that our work is competitive with PyTorch DTensor, a highly
optimized distributed tensor library targeting AI models.

</details>


### [8] [Co-designing a Programmable RISC-V Accelerator for MPC-based Energy and Thermal Management of Many-Core HPC Processors](https://arxiv.org/abs/2510.09163)
*Alessandro Ottaviano,Andrino Meli,Paul Scheffler,Giovanni Bambini,Robert Balas,Davide Rossi,Andrea Bartolini,Luca Benini*

Main category: cs.DC

TL;DR: 提出了一种轻量级MPC控制器的软硬件协同设计，用于多核HPC处理器的能量和热管理，通过算子分裂二次规划求解器和嵌入式RISC-V控制器实现高效控制。


<details>
  <summary>Details</summary>
Motivation: 传统MPC方法在PE上执行控制器会因操作系统开销产生抖动并限制控制带宽，而专用片上控制器又面临面积和功耗开销的担忧。

Method: 基于算子分裂二次规划求解器和嵌入式多核RISC-V控制器，通过剪枝弱热耦合减少模型内存，并采用提前调度优化稀疏三角系统的并行执行。

Result: 在500MHz频率下控制144个PE时实现亚毫秒级延迟，比单核基线延迟降低33倍，能效提高7.9倍，内存占用小于1MiB，功耗仅325mW，占用典型HPC处理器芯片面积不到1.5%。

Conclusion: 该硬件-软件协同设计方法成功解决了MPC控制器在面积、功耗和性能方面的挑战，为多核HPC处理器的热管理提供了高效解决方案。

Abstract: Managing energy and thermal profiles is critical for many-core HPC processors
with hundreds of application-class processing elements (PEs). Advanced model
predictive control (MPC) delivers state-of-the-art performance but requires
solving an online optimization problem over a thousand times per second (1 kHz
control bandwidth), with computational and memory demands scaling with PE
count. Traditional MPC approaches execute the controller on the PEs, but
operating system overheads create jitter and limit control bandwidth. Running
MPC on dedicated on-chip controllers enables fast, deterministic control but
raises concerns about area and power overhead. In this work, we tackle these
challenges by proposing a hardware-software codesign of a lightweight MPC
controller, based on an operator-splitting quadratic programming solver and an
embedded multi-core RISC-V controller. Key innovations include pruning weak
thermal couplings to reduce model memory and ahead-of-time scheduling for
efficient parallel execution of sparse triangular systems arising from the
optimization problem. The proposed controller achieves sub-millisecond latency
when controlling 144 PEs at 500 MHz, delivering 33x lower latency and 7.9x
higher energy efficiency than a single-core baseline. Operating within a
compact less than 1 MiB memory footprint, it consumes as little as 325 mW while
occupying less than 1.5% of a typical HPC processor's die area.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [Mozart: A Chiplet Ecosystem-Accelerator Codesign Framework for Composable Bespoke Application Specific Integrated Circuits](https://arxiv.org/abs/2510.08873)
*Haoran Jin,Jirong Yang,Yunpeng Liu,Barry Lyu,Kangqi Zhang,Nathaniel Bleier*

Main category: cs.AR

TL;DR: Mozart是一个芯片生态系统和加速器协同设计框架，通过算子级解耦、芯片异构性和内存优化，构建低成本定制ASIC芯片，显著提升能效和性能。


<details>
  <summary>Details</summary>
Motivation: 传统AI加速器假设存在内存需求、批处理效果和延迟-吞吐量权衡的系统级泛化问题，忽略了神经网络算子的异构计算模式。芯片级定制虽然能解决此问题，但面临高昂的非重复性工程成本。

Method: 采用算子级解耦方法探索芯片和内存异构性、张量融合和张量并行，通过布局布线验证确保物理可实现性。框架支持从数据中心到边缘计算等多种部署场景的约束感知系统级优化。

Result: 仅使用8个战略性选择的芯片，Mozart生成的复合BASIC芯片相比传统同构加速器，在能耗、能耗-成本乘积、能耗-延迟乘积和能耗-延迟-成本乘积方面分别降低43.5%、25.4%、67.7%和78.8%。在数据中心LLM服务中能耗降低15-19%，能耗-成本改善35-39%。

Conclusion: Mozart框架通过系统化的芯片生态系统设计，有效解决了AI加速中的异构计算挑战，在保持低成本的同时显著提升了能效和性能表现。

Abstract: Modern AI acceleration faces a fundamental challenge: conventional
assumptions about memory requirements, batching effectiveness, and
latency-throughput tradeoffs are systemwide generalizations that ignore the
heterogeneous computational patterns of individual neural network operators.
However, going towards network-level customization and operator-level
heterogeneity incur substantial Non-Recurring Engineering (NRE) costs. While
chiplet-based approaches have been proposed to amortize NRE costs, reuse
opportunities remain limited without carefully identifying which chiplets are
truly necessary. This paper introduces Mozart, a chiplet ecosystem and
accelerator codesign framework that systematically constructs low cost bespoke
application-specific integrated circuits (BASICs). BASICs leverage
operator-level disaggregation to explore chiplet and memory heterogeneity,
tensor fusion, and tensor parallelism, with place-and-route validation ensuring
physical implementability. The framework also enables constraint-aware
system-level optimization across deployment contexts ranging from datacenter
inference serving to edge computing in autonomous vehicles. The evaluation
confirms that with just 8 strategically selected chiplets, Mozart-generated
composite BASICs achieve 43.5%, 25.4%, 67.7%, and 78.8% reductions in energy,
energy-cost product, energy-delay product (EDP), and energy-delay-cost product
compared to traditional homogeneous accelerators. For datacenter LLM serving,
Mozart achieves 15-19% energy reduction and 35-39% energy-cost improvement. In
speculative decoding, Mozart delivers throughput improvements of 24.6-58.6%
while reducing energy consumption by 38.6-45.6%. For autonomous vehicle
perception, Mozart reduces energy-cost by 25.54% and energy by 10.53% under
real-time constraints.

</details>


### [10] [A High-Efficiency SoC for Next-Generation Mobile DNA Sequencing](https://arxiv.org/abs/2510.08940)
*Abel Beyene,Zhongpan Wu,Yunus Dawji,Karim Hammad,Ebrahim Ghafar-Zadeh,Sebastian Magierowski*

Main category: cs.AR

TL;DR: 本文提出了一种基于RISC-V的SoC设计，用于手持DNA测序机，通过专用加速器实现了13倍性能提升和近3000倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 当前手持DNA测序机缺乏足够的嵌入式计算能力，依赖外部设备处理大量测量数据，这限制了设备的移动性和实时测序能力。

Method: 设计了一个基于RISC-V核心的22nm CMOS SoC，集成了DNA检测专用加速器。

Result: 相比商用嵌入式多核处理器，系统性能提升13倍，能效提升近3000倍。

Conclusion: 该SoC设计为下一代手持DNA测序机提供了高性能、高能效的嵌入式处理解决方案。

Abstract: Hand-sized Deoxyribonucleic acid (DNA) sequencing machines are of growing
importance in several life sciences fields as their small footprints enable a
broader range of use cases than their larger, stationary counterparts. However,
as currently designed, they lack sufficient embedded computing to process the
large volume of measurements generated by their internal sensory system. As a
consequence, they rely on external devices for additional processing
capability. This dependence on external processing places a significant
communication burden on the sequencer's embedded electronics. Moreover, it also
prevents a truly mobile solution for sequencing in real-time. Anticipating
next-generation machines that include suitably advanced processing, we present
a System-on-Chip (SoC) fabricated in 22-nm complementary metal-oxide
semiconductor (CMOS). Our design, based on a general-purpose reduced
instruction set computing (RISC-V) core, also includes accelerators for DNA
detection that allow our system to demonstrate a 13X performance improvement
over commercial embedded multicore processors combined with a near 3000X boost
in energy efficiency.

</details>


### [11] [HERO: Hardware-Efficient RL-based Optimization Framework for NeRF Quantization](https://arxiv.org/abs/2510.09010)
*Yipu Zhang,Chaofang Ma,Jinming Ge,Lin Jiang,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: HERO是一个基于强化学习的硬件感知量化框架，用于优化NeRF的3D重建性能，在延迟、成本效率和模型大小方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF量化方法未考虑硬件架构，导致在精度、延迟和模型大小的设计空间中无法找到最优解；而现有加速器依赖人工探索设计空间，效率低下。

Method: 使用强化学习框架结合NeRF加速器模拟器，实时生成硬件反馈，实现完全自动化的硬件约束适应。

Result: 相比现有最优方法CAQ，HERO实现了1.31-1.33倍的延迟改善、1.29-1.33倍的成本效率提升，以及更紧凑的模型大小。

Conclusion: HERO能够有效探索硬件与算法需求之间的复杂设计空间，为NeRF实现发现更优的量化策略。

Abstract: Neural Radiance Field (NeRF) has emerged as a promising 3D reconstruction
method, delivering high-quality results for AR/VR applications. While
quantization methods and hardware accelerators have been proposed to enhance
NeRF's computational efficiency, existing approaches face crucial limitations.
Current quantization methods operate without considering hardware architecture,
resulting in sub-optimal solutions within the vast design space encompassing
accuracy, latency, and model size. Additionally, existing NeRF accelerators
heavily rely on human experts to explore this design space, making the
optimization process time-consuming, inefficient, and unlikely to discover
optimal solutions. To address these challenges, we introduce HERO, a
reinforcement learning framework performing hardware-aware quantization for
NeRF. Our framework integrates a NeRF accelerator simulator to generate
real-time hardware feedback, enabling fully automated adaptation to hardware
constraints. Experimental results demonstrate that HERO achieves 1.31-1.33
$\times$ better latency, 1.29-1.33 $\times$ improved cost efficiency, and a
more compact model size compared to CAQ, a previous state-of-the-art NeRF
quantization framework. These results validate our framework's capability to
effectively navigate the complex design space between hardware and algorithm
requirements, discovering superior quantization policies for NeRF
implementation. Code is available at https://github.com/ypzhng/HERO.

</details>


### [12] [Sequencing on Silicon: AI SoC Design for Mobile Genomics at the Edge](https://arxiv.org/abs/2510.09339)
*Sebastian Magierowski,Zhongpan Wu,Abel Beyene,Karim Hammad*

Main category: cs.AR

TL;DR: 开发了一个用于移动基因分析的CMOS系统芯片，结合多核RISC-V处理器和深度学习加速器，实现实时设备端基因组分析。


<details>
  <summary>Details</summary>
Motivation: 微型DNA测序硬件在移动场景中取得成功，但纳米孔测序原始数据速率比音频高100倍以上，需要更高效的计算和内存处理。

Method: 采用硬件/软件协同设计策略，将多核RISC-V处理器与紧密耦合的深度学习和生物信息学加速器集成在CMOS系统芯片中。

Result: 实现了跨异构计算架构的能效操作，针对实时设备端基因组分析。

Conclusion: 这项工作展示了深度学习、边缘计算和领域专用硬件集成如何推动下一代移动基因组学发展。

Abstract: Miniature DNA sequencing hardware has begun to succeed in mobile contexts,
driving demand for efficient machine learning at the edge. This domain
leverages deep learning techniques familiar from speech and time-series
analysis for both low-level signal processing and high-level genomic
interpretation. Unlike audio, however, nanopore sequencing presents raw data
rates over 100X higher, requiring more aggressive compute and memory handling.
In this paper, we present a CMOS system-on-chip (SoC) designed for mobile
genetic analysis. Our approach combines a multi-core RISC-V processor with
tightly coupled accelerators for deep learning and bioinformatics. A
hardware/software co-design strategy enables energy-efficient operation across
a heterogeneous compute fabric, targeting real-time, on-device genome analysis.
This work exemplifies the integration of deep learning, edge computing, and
domain-specific hardware to advance next-generation mobile genomics.

</details>
