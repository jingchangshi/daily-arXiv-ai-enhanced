{"id": "2511.01860", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01860", "abs": "https://arxiv.org/abs/2511.01860", "authors": ["Leszek Sliwko"], "title": "A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks", "comment": "This is the accepted author's version of the paper. The final\n  published version is available in Global Journal of Computer Science and\n  Technology, 2019", "summary": "This review analyzes deployed and actively used workload schedulers'\nsolutions and presents a taxonomy in which those systems are divided into\nseveral hierarchical groups based on their architecture and design. While other\ntaxonomies do exist, this review has focused on the key design factors that\naffect the throughput and scalability of a given solution, as well as the\nincremental improvements which bettered such an architecture. This review gives\nspecial attention to Google's Borg, which is one of the most advanced and\npublished systems of this kind.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5df2\u90e8\u7f72\u548c\u4f7f\u7528\u7684\u8d1f\u8f7d\u8c03\u5ea6\u5668\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u67b6\u6784\u548c\u8bbe\u8ba1\u7684\u5c42\u6b21\u5316\u5206\u7c7b\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u5f71\u54cd\u541e\u5410\u91cf\u548c\u53ef\u6269\u5c55\u6027\u7684\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\u4ee5\u53ca\u67b6\u6784\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u5206\u7c7b\u6cd5\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u91cd\u70b9\u5173\u6ce8\u5f71\u54cd\u8d1f\u8f7d\u8c03\u5ea6\u5668\u541e\u5410\u91cf\u548c\u53ef\u6269\u5c55\u6027\u7684\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\uff0c\u4ee5\u53ca\u67b6\u6784\u7684\u6e10\u8fdb\u5f0f\u6539\u8fdb\u3002", "method": "\u5206\u6790\u5df2\u90e8\u7f72\u548c\u4f7f\u7528\u7684\u8d1f\u8f7d\u8c03\u5ea6\u5668\u89e3\u51b3\u65b9\u6848\uff0c\u5efa\u7acb\u57fa\u4e8e\u67b6\u6784\u548c\u8bbe\u8ba1\u7684\u5c42\u6b21\u5316\u5206\u7c7b\u6cd5\uff0c\u7279\u522b\u5173\u6ce8Google Borg\u7b49\u5148\u8fdb\u7cfb\u7edf\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5f71\u54cd\u8c03\u5ea6\u5668\u6027\u80fd\u7684\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\uff0c\u5e76\u5c55\u793a\u4e86\u67b6\u6784\u6539\u8fdb\u7684\u6f14\u8fdb\u8def\u5f84\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6cd5\u4e3a\u7406\u89e3\u8d1f\u8f7d\u8c03\u5ea6\u5668\u7684\u8bbe\u8ba1\u9009\u62e9\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u6846\u67b6\uff0c\u7279\u522b\u5f3a\u8c03\u4e86Google Borg\u4f5c\u4e3a\u5148\u8fdb\u7cfb\u7edf\u7684\u8bbe\u8ba1\u7406\u5ff5\uff0c\u5bf9\u540e\u7eed\u7cfb\u7edf\u5f00\u53d1\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2511.01861", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01861", "abs": "https://arxiv.org/abs/2511.01861", "authors": ["Johan Messchendorp", "Mohammad Al-Turany", "Volker Friese", "Thorsten Kollegger", "Bastian Loeher", "Jochen Markert", "Andrew Mistry", "Thomas Neff", "Adrian Oeftiger", "Michael Papenbrock", "Stephane Pietri", "Shahab Sanjari", "Tobias Stockmanns"], "title": "Conceptual Design Report for FAIR Computing", "comment": "88 pages, Conceptual Design Report for FAIR Computing", "summary": "This Conceptual Design Report (CDR) presents the plans of the computing\ninfrastructure for research at FAIR, Darmstadt, Germany. It presents the\ncomputing requirements of the various research groups, the policies for the\ncomputing and storage infrastructure, the foreseen FAIR computing model\nincluding the open data, software and services policies and architecture for\nthe periods starting in 2028 with the \"first science (plus)\" phase to the\nmodularized start version of FAIR. The overall ambition is to create a\nfederated and centrally-orchestrated infrastructure serving the large diversity\nof the research lines present with sufficient scalability and flexibility to\ncope with future data challenges that will be present at FAIR.", "AI": {"tldr": "\u672c\u6982\u5ff5\u8bbe\u8ba1\u62a5\u544a\u4ecb\u7ecd\u4e86\u5fb7\u56fd\u8fbe\u59c6\u65bd\u5854\u7279FAIR\u7814\u7a76\u8bbe\u65bd\u7684\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u8ba1\u5212\uff0c\u6db5\u76d6\u4ece2028\u5e74\"\u9996\u6b21\u79d1\u5b66\uff08\u589e\u5f3a\uff09\"\u9636\u6bb5\u5230\u6a21\u5757\u5316\u542f\u52a8\u7248\u672c\u671f\u95f4\u7684\u5404\u9879\u89c4\u5212\u3002", "motivation": "\u4e3aFAIR\u5404\u7814\u7a76\u7ec4\u63d0\u4f9b\u6ee1\u8db3\u5176\u8ba1\u7b97\u9700\u6c42\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u5e94\u5bf9\u672a\u6765\u7684\u6570\u636e\u6311\u6218\uff0c\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u670d\u52a1\u591a\u6837\u5316\u7814\u7a76\u7ebf\u8def\u7684\u8054\u90a6\u5f0f\u3001\u96c6\u4e2d\u7f16\u6392\u7684\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u5236\u5b9a\u8ba1\u7b97\u548c\u5b58\u50a8\u57fa\u7840\u8bbe\u65bd\u653f\u7b56\uff0c\u8bbe\u8ba1FAIR\u8ba1\u7b97\u6a21\u578b\uff0c\u5305\u62ec\u5f00\u653e\u6570\u636e\u3001\u8f6f\u4ef6\u548c\u670d\u52a1\u653f\u7b56\u53ca\u67b6\u6784\uff0c\u91c7\u7528\u8054\u90a6\u5f0f\u96c6\u4e2d\u7f16\u6392\u7684\u57fa\u7840\u8bbe\u65bd\u67b6\u6784\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u8db3\u591f\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u7684\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u8ba1\u5212\uff0c\u80fd\u591f\u5e94\u5bf9FAIR\u672a\u6765\u5c06\u9762\u4e34\u7684\u6570\u636e\u6311\u6218\u3002", "conclusion": "\u8be5\u8ba1\u5212\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u8054\u90a6\u5f0f\u3001\u96c6\u4e2d\u7f16\u6392\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u670d\u52a1\u4e8eFAIR\u591a\u6837\u5316\u7684\u7814\u7a76\u7ebf\u8def\uff0c\u5177\u5907\u5e94\u5bf9\u672a\u6765\u6570\u636e\u6311\u6218\u6240\u9700\u7684\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2511.01862", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.01862", "abs": "https://arxiv.org/abs/2511.01862", "authors": ["Vanessa Sochat", "Daniel Milroy"], "title": "Possible Futures for Cloud Cost Models", "comment": "10 pages", "summary": "Cloud is now the leading software and computing hardware innovator, and is\nchanging the landscape of compute to one that is optimized for artificial\nintelligence and machine learning (AI/ML). Computing innovation was initially\ndriven to meet the needs of scientific computing. As industry and consumer\nusage of computing proliferated, there was a shift to satisfy a multipolar\ncustomer base. Demand for AI/ML now dominates modern computing and innovation\nhas centralized on cloud. As a result, cost and resource models designed to\nserve AI/ML use cases are not currently well suited for science. If resource\ncontention resulting from a unipole consumer makes access to contended\nresources harder for scientific users, a likely future is running scientific\nworkloads where they were not intended. In this article, we discuss the past,\ncurrent, and possible futures of cloud cost models for the continued support of\ndiscovery and science.", "AI": {"tldr": "\u4e91\u8ba1\u7b97\u5df2\u6210\u4e3aAI/ML\u521b\u65b0\u7684\u4e3b\u5bfc\u529b\u91cf\uff0c\u4f46\u5f53\u524d\u7684\u8d44\u6e90\u6210\u672c\u6a21\u578b\u4e0d\u9002\u5408\u79d1\u5b66\u8ba1\u7b97\u9700\u6c42\uff0c\u53ef\u80fd\u5bfc\u81f4\u79d1\u5b66\u5de5\u4f5c\u8d1f\u8f7d\u5728\u4e0d\u9002\u5408\u7684\u73af\u5883\u4e2d\u8fd0\u884c\u3002", "motivation": "\u5206\u6790\u4e91\u8ba1\u7b97\u4ece\u79d1\u5b66\u8ba1\u7b97\u9a71\u52a8\u8f6c\u5411AI/ML\u4e3b\u5bfc\u7684\u6f14\u53d8\uff0c\u63a2\u8ba8\u5f53\u524d\u8d44\u6e90\u6a21\u578b\u5bf9\u79d1\u5b66\u8ba1\u7b97\u7684\u4e0d\u9002\u5e94\u6027\u53ca\u5176\u672a\u6765\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5386\u53f2\u56de\u987e\u548c\u73b0\u72b6\u5206\u6790\uff0c\u8ba8\u8bba\u4e91\u8ba1\u7b97\u6210\u672c\u6a21\u578b\u7684\u6f14\u53d8\u8fc7\u7a0b\u53ca\u5176\u5bf9\u79d1\u5b66\u8ba1\u7b97\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0AI/ML\u9700\u6c42\u4e3b\u5bfc\u7684\u4e91\u8ba1\u7b97\u8d44\u6e90\u6a21\u578b\u4e0e\u79d1\u5b66\u8ba1\u7b97\u9700\u6c42\u4e0d\u5339\u914d\uff0c\u53ef\u80fd\u5bfc\u81f4\u79d1\u5b66\u5de5\u4f5c\u8d1f\u8f7d\u5728\u975e\u9884\u671f\u73af\u5883\u4e2d\u8fd0\u884c\u3002", "conclusion": "\u9700\u8981\u91cd\u65b0\u601d\u8003\u4e91\u8ba1\u7b97\u6210\u672c\u6a21\u578b\uff0c\u4ee5\u786e\u4fdd\u79d1\u5b66\u53d1\u73b0\u548c\u7814\u7a76\u80fd\u591f\u6301\u7eed\u83b7\u5f97\u9002\u5f53\u7684\u8ba1\u7b97\u8d44\u6e90\u652f\u6301\u3002"}}
{"id": "2511.01863", "categories": ["cs.DC", "cs.DM"], "pdf": "https://arxiv.org/pdf/2511.01863", "abs": "https://arxiv.org/abs/2511.01863", "authors": ["Robert Fabian Lindermann", "Paul-Niklas Ken Kandora", "Simon Caspar Zeller", "Adrian Asmund Fessler", "Steffen Rebennack"], "title": "SPHERE: Spherical partitioning for large-scale routing optimization", "comment": null, "summary": "We study shortest-path routing in large weighted, undirected graphs, where\nexpanding search frontiers raise time and memory costs for exact solvers. We\npropose \\emph{SPHERE}, a source-target-aware heuristic that identifies an\n$s$-$t$ overlap: vertices that are close to both $s$ and $t$ in hop count.\nSelecting an anchor $a$ in this overlap partitions the task into two\nsubproblems with unchanged problem-topology, $s\\to a$ and $a\\to t$; if either\nremains large, the procedure recurses on its induced subgraph. Because the cut\nlies inside the overlap, concatenating the resulting subpaths yields a valid\n$s\\to t$ route without boundary repair. SPHERE is independent of the downstream\nsolver (e.g., Dijkstra) and exposes parallelism across subproblems. On large\nnetworks, it achieves faster runtimes and smaller optimality gaps than\nLouvain-based routing and a METIS-based pipeline, even on graphs with more than\na million nodes and edges, while also outperforming Dijkstra in runtime.", "AI": {"tldr": "SPHERE\u662f\u4e00\u79cd\u6e90-\u76ee\u6807\u611f\u77e5\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522bs-t\u91cd\u53e0\u533a\u57df\u6765\u5206\u5272\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u5927\u89c4\u6a21\u52a0\u6743\u65e0\u5411\u56fe\u4e2d\u7684\u8def\u7531\u6548\u7387\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u52a0\u6743\u65e0\u5411\u56fe\u4e2d\uff0c\u6269\u5c55\u641c\u7d22\u8fb9\u754c\u4f1a\u589e\u52a0\u7cbe\u786e\u6c42\u89e3\u5668\u7684\u65f6\u95f4\u548c\u5185\u5b58\u6210\u672c\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002", "method": "SPHERE\u7b97\u6cd5\u8bc6\u522bs-t\u91cd\u53e0\u533a\u57df\uff08\u9760\u8fd1\u6e90\u70b9\u548c\u76ee\u6807\u70b9\u7684\u9876\u70b9\uff09\uff0c\u9009\u62e9\u951a\u70b9\u5c06\u95ee\u9898\u5206\u5272\u4e3a\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u5e76\u5728\u9700\u8981\u65f6\u9012\u5f52\u5904\u7406\uff0c\u65e0\u9700\u8fb9\u754c\u4fee\u590d\u5373\u53ef\u62fc\u63a5\u8def\u5f84\u3002", "result": "\u5728\u5927\u578b\u7f51\u7edc\u4e0a\uff0cSPHERE\u6bd4\u57fa\u4e8eLouvain\u7684\u8def\u7531\u548c\u57fa\u4e8eMETIS\u7684\u6d41\u6c34\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u5feb\u7684\u8fd0\u884c\u65f6\u95f4\u548c\u66f4\u5c0f\u7684\u6700\u4f18\u6027\u5dee\u8ddd\uff0c\u5373\u4f7f\u5bf9\u4e8e\u8d85\u8fc7\u767e\u4e07\u8282\u70b9\u548c\u8fb9\u7684\u56fe\u4e5f\u4f18\u4e8eDijkstra\u7b97\u6cd5\u3002", "conclusion": "SPHERE\u662f\u4e00\u79cd\u72ec\u7acb\u4e8e\u4e0b\u6e38\u6c42\u89e3\u5668\u7684\u6709\u6548\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21\u56fe\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u6700\u77ed\u8def\u5f84\u8ba1\u7b97\uff0c\u5e76\u652f\u6301\u5b50\u95ee\u9898\u95f4\u7684\u5e76\u884c\u5904\u7406\u3002"}}
{"id": "2511.02491", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.02491", "abs": "https://arxiv.org/abs/2511.02491", "authors": ["Roland Meyer", "Jakob Tepe"], "title": "Oriented Metrics for Bottom-Up Enumerative Synthesis", "comment": "37 pages, 16 figures", "summary": "In syntax-guided synthesis, one of the challenges is to reduce the enormous\nsize of the search space. We observe that most search spaces are not just flat\nsets of programs, but can be endowed with a structure that we call an oriented\nmetric. Oriented metrics measure the distance between programs, like ordinary\nmetrics do, but are designed for settings in which operations have an\norientation. Our focus is on the string and the bitvector domains, where\noperations like concatenation and bitwise conjunction transform an input into\nan output in a way that is not symmetric. We develop several new oriented\nmetrics for these domains. Oriented metrics are designed for search space\nreduction, and we present four techniques: (i) pruning the search space to a\nball around the ground truth, (ii) factorizing the search space by an\nequivalence that is induced by the oriented metric, (iii) abstracting the\noriented metric (and hence the equivalence) and refining it, and (iv) improving\nthe enumeration order by learning from abstract information. We acknowledge\nthat these techniques are inspired by developments in the literature. By\nunderstanding their roots in oriented metrics, we can substantially increase\ntheir applicability and efficiency. We have integrated these techniques into a\nnew synthesis algorithm and implemented the algorithm in a new solver. Notably,\nour solver is generic in the oriented metric over which it computes. We\nconducted experiments in the string and the bitvector domains, and consistently\nimprove the performance over the state-of-the-art by more than an order of\nmagnitude.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u5b9a\u5411\u5ea6\u91cf\u7684\u65b0\u7ed3\u6784\u6765\u51cf\u5c11\u8bed\u6cd5\u5f15\u5bfc\u7efc\u5408\u4e2d\u7684\u641c\u7d22\u7a7a\u95f4\u5927\u5c0f\uff0c\u5e76\u5728\u5b57\u7b26\u4e32\u548c\u4f4d\u5411\u91cf\u57df\u4e2d\u5f00\u53d1\u4e86\u591a\u79cd\u5b9a\u5411\u5ea6\u91cf\u3002\u901a\u8fc7\u56db\u79cd\u6280\u672f\uff08\u4fee\u526a\u3001\u5206\u89e3\u3001\u62bd\u8c61\u548c\u679a\u4e3e\u4f18\u5316\uff09\u663e\u8457\u63d0\u5347\u4e86\u7efc\u5408\u6027\u80fd\u3002", "motivation": "\u8bed\u6cd5\u5f15\u5bfc\u7efc\u5408\u9762\u4e34\u641c\u7d22\u7a7a\u95f4\u8fc7\u5927\u7684\u6311\u6218\uff0c\u5927\u591a\u6570\u641c\u7d22\u7a7a\u95f4\u4e0d\u662f\u7b80\u5355\u7684\u5e73\u9762\u7a0b\u5e8f\u96c6\u5408\uff0c\u800c\u662f\u5177\u6709\u7279\u5b9a\u7ed3\u6784\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\u8fd9\u4e9b\u7a7a\u95f4\u53ef\u4ee5\u8d4b\u4e88\u5b9a\u5411\u5ea6\u91cf\u7ed3\u6784\uff0c\u7528\u4e8e\u6d4b\u91cf\u7a0b\u5e8f\u95f4\u7684\u8ddd\u79bb\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5b57\u7b26\u4e32\u548c\u4f4d\u5411\u91cf\u7b49\u975e\u5bf9\u79f0\u64cd\u4f5c\u9886\u57df\u3002", "method": "\u5f00\u53d1\u4e86\u5b57\u7b26\u4e32\u548c\u4f4d\u5411\u91cf\u57df\u7684\u65b0\u5b9a\u5411\u5ea6\u91cf\uff1b\u63d0\u51fa\u4e86\u56db\u79cd\u641c\u7d22\u7a7a\u95f4\u7f29\u51cf\u6280\u672f\uff1a\u56f4\u7ed5\u771f\u5b9e\u503c\u7684\u7403\u4f53\u4fee\u526a\u3001\u5b9a\u5411\u5ea6\u91cf\u8bf1\u5bfc\u7684\u7b49\u4ef7\u5206\u89e3\u3001\u5ea6\u91cf\u62bd\u8c61\u4e0e\u7cbe\u5316\u3001\u57fa\u4e8e\u62bd\u8c61\u4fe1\u606f\u5b66\u4e60\u7684\u679a\u4e3e\u987a\u5e8f\u4f18\u5316\uff1b\u5b9e\u73b0\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u7efc\u5408\u7b97\u6cd5\u6c42\u89e3\u5668\u3002", "result": "\u5728\u5b57\u7b26\u4e32\u548c\u4f4d\u5411\u91cf\u57df\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u6280\u672f\u7684\u6027\u80fd\u63d0\u5347\u8d85\u8fc7\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u901a\u8fc7\u7406\u89e3\u5b9a\u5411\u5ea6\u91cf\u7684\u7406\u8bba\u57fa\u7840\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8bed\u6cd5\u5f15\u5bfc\u7efc\u5408\u6280\u672f\u7684\u9002\u7528\u6027\u548c\u6548\u7387\uff0c\u65b0\u63d0\u51fa\u7684\u5b9a\u5411\u5ea6\u91cf\u6846\u67b6\u4e3a\u641c\u7d22\u7a7a\u95f4\u7f29\u51cf\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02132", "categories": ["cs.AR", "cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.02132", "abs": "https://arxiv.org/abs/2511.02132", "authors": ["Mansi Choudhary", "Karthik Sangaiah", "Sonali Singh", "Muhammad Osama", "Lisa Wu Wills", "Ganesh Dasika"], "title": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects", "comment": "11 pages, 14 figures", "summary": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in\nlarge-scale attention workloads: non-uniform memory access (NUMA). As\nmulti-chiplet designs become the norm for scaling compute capabilities, memory\nlatency and bandwidth vary sharply across compute regions, undermining the\nperformance of traditional GPU kernel scheduling strategies that assume uniform\nmemory access. We identify how these NUMA effects distort locality in\nmulti-head attention (MHA) and present Swizzled Head-first Mapping, a\nspatially-aware scheduling strategy that aligns attention heads with GPU NUMA\ndomains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our\nmethod achieves up to 50% higher performance over state-of-the-art attention\nalgorithms using conventional scheduling techniques and sustains consistently\nhigh L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware\nscheduling is now fundamental to achieving full efficiency on next-generation\ndisaggregated GPUs, offering a path forward for scalable AI training and\ninference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u82af\u7247GPU\u67b6\u6784\u7684NUMA\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\uff0c\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u7684\u6ce8\u610f\u529b\u5934\u6620\u5c04\u6765\u4f18\u5316\u591a\u6ce8\u610f\u529b\u5934\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740AI GPU\u5411\u591a\u82af\u7247\u67b6\u6784\u53d1\u5c55\uff0c\u975e\u7edf\u4e00\u5185\u5b58\u8bbf\u95ee(NUMA)\u6210\u4e3a\u5927\u89c4\u6a21\u6ce8\u610f\u529b\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4f20\u7edfGPU\u8c03\u5ea6\u7b56\u7565\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5185\u5b58\u8bbf\u95ee\u5ef6\u8fdf\u548c\u5e26\u5bbd\u7684\u4e0d\u5747\u5300\u6027\u3002", "method": "\u63d0\u51fa\u4e86Swizzled Head-first Mapping\u7b56\u7565\uff0c\u5c06\u6ce8\u610f\u529b\u5934\u4e0eGPU NUMA\u57df\u5bf9\u9f50\uff0c\u5229\u7528\u82af\u7247\u5185\u7f13\u5b58\u91cd\u7528\uff0c\u4f18\u5316\u591a\u6ce8\u610f\u529b\u5934\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7a7a\u95f4\u8c03\u5ea6\u3002", "result": "\u5728AMD MI300X\u67b6\u6784\u4e0a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u6ce8\u610f\u529b\u7b97\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe50%\uff0cL2\u7f13\u5b58\u547d\u4e2d\u7387\u7ef4\u6301\u572880-97%\u7684\u9ad8\u6c34\u5e73\u3002", "conclusion": "NUMA\u611f\u77e5\u8c03\u5ea6\u5bf9\u4e8e\u4e0b\u4e00\u4ee3\u5206\u89e3\u5f0fGPU\u5b9e\u73b0\u5168\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u53ef\u6269\u5c55\u7684AI\u8bad\u7ec3\u548c\u63a8\u7406\u63d0\u4f9b\u4e86\u524d\u8fdb\u8def\u5f84\u3002"}}
{"id": "2511.01866", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.01866", "abs": "https://arxiv.org/abs/2511.01866", "authors": ["Benjamin Kubwimana", "Qijing Huang"], "title": "EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs", "comment": "Published in the Proceedings of the 2025 IEEE International Symposium\n  on Workload Characterization (IISWC 2025)", "summary": "Edge intelligence paradigm is increasingly demanded by the emerging\nautonomous systems, such as robotics. Beyond ensuring privacy-preserving\noperation and resilience in connectivity-limited environments, edge deployment\noffers significant energy and cost advantages over cloud-based solutions.\nHowever, deploying large language models (LLMs) for reasoning tasks on edge\nGPUs faces critical challenges from strict latency constraints and limited\ncomputational resources. To navigate these constraints, developers must balance\nmultiple design factors - choosing reasoning versus non-reasoning\narchitectures, selecting appropriate model sizes, allocating token budgets, and\napplying test-time scaling strategies - to meet target latency and optimize\naccuracy. Yet guidance on optimal combinations of these variables remains\nscarce. In this work, we present EdgeReasoning, a comprehensive study\ncharacterizing the deployment of reasoning LLMs on edge GPUs. We systematically\nquantify latency-accuracy tradeoffs across various LLM architectures and model\nsizes. We systematically evaluate prompt-based and model-tuning-based\ntechniques for reducing reasoning token length while maintaining performance\nquality. We further profile test-time scaling methods with varying degrees of\nparallelism to maximize accuracy under strict latency budgets. Through these\nanalyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency\nconfigurations, offering systematic guidance for optimal edge deployment of\nreasoning LLMs.", "AI": {"tldr": "EdgeReasoning \u662f\u4e00\u9879\u5173\u4e8e\u5728\u8fb9\u7f18 GPU \u4e0a\u90e8\u7f72\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7efc\u5408\u7814\u7a76\uff0c\u7cfb\u7edf\u91cf\u5316\u4e86\u4e0d\u540c LLM \u67b6\u6784\u548c\u6a21\u578b\u5927\u5c0f\u7684\u5ef6\u8fdf-\u51c6\u786e\u7387\u6743\u8861\uff0c\u4e3a\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u4f18\u5316\u6307\u5bfc\u3002", "motivation": "\u8fb9\u7f18\u667a\u80fd\u8303\u5f0f\u5bf9\u65b0\u5174\u81ea\u4e3b\u7cfb\u7edf\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4f46\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u5230\u8fb9\u7f18 GPU \u4e0a\u9762\u4e34\u4e25\u683c\u5ef6\u8fdf\u7ea6\u675f\u548c\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u7684\u6311\u6218\uff0c\u7f3a\u4e4f\u5173\u4e8e\u8bbe\u8ba1\u56e0\u7d20\u7ec4\u5408\u7684\u6307\u5bfc\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c LLM \u67b6\u6784\u548c\u6a21\u578b\u5927\u5c0f\u7684\u5ef6\u8fdf-\u51c6\u786e\u7387\u6743\u8861\uff1b\u8bc4\u4f30\u57fa\u4e8e\u63d0\u793a\u548c\u6a21\u578b\u8c03\u4f18\u7684\u6280\u672f\u4ee5\u51cf\u5c11\u63a8\u7406\u4ee4\u724c\u957f\u5ea6\uff1b\u5206\u6790\u4e0d\u540c\u5e76\u884c\u5ea6\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5206\u6790\u7ed8\u5236\u4e86\u53ef\u5b9e\u73b0\u51c6\u786e\u7387-\u5ef6\u8fdf\u914d\u7f6e\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u4e3a\u63a8\u7406\u578b LLM \u7684\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u7cfb\u7edf\u6307\u5bfc\u3002", "conclusion": "EdgeReasoning \u4e3a\u5728\u4e25\u683c\u5ef6\u8fdf\u9884\u7b97\u4e0b\u4f18\u5316\u8fb9\u7f18\u90e8\u7f72\u63a8\u7406\u578b LLM \u63d0\u4f9b\u4e86\u5168\u9762\u7684\u91cf\u5316\u5206\u6790\u548c\u914d\u7f6e\u6307\u5bfc\u3002"}}
{"id": "2511.01872", "categories": ["cs.DC", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.01872", "abs": "https://arxiv.org/abs/2511.01872", "authors": ["Etash Guha", "Tianxiao Jiang", "Andrew Deng", "Jian Zhang", "Muthu Annamalai"], "title": "Learned Cost Model for Placement on Reconfigurable Dataflow Hardware", "comment": "7 pages, 2 figures, 2 tables, DAC Conference style (2022)", "summary": "Mapping a dataflow-graph of an ML model onto a reconfigurable system is\ndifficult, as different mappings have different throughputs and consume\nresource constraints differently. To solve this, a model to evaluate the\nthroughput of mappings is necessary as measuring throughput completely is\nexpensive. Many use a hand-designed analytical model, relying on proxy features\nor intuition, introducing error. We provide a Learned Approach that predicts\nthroughput 31%-52% more accurately over a variety of graphs. In addition, our\napproach shows no accuracy degradation after removing performance annotations.\nWe show that using this approach results in 5.6% faster compiled graphs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u65b9\u6cd5\u6765\u9884\u6d4b\u6570\u636e\u6d41\u56fe\u5728\u53ef\u91cd\u6784\u7cfb\u7edf\u4e0a\u7684\u541e\u5410\u91cf\uff0c\u6bd4\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u51c6\u786e31%-52%\uff0c\u5e76\u80fd\u751f\u62105.6%\u66f4\u5feb\u7684\u7f16\u8bd1\u56fe\u3002", "motivation": "\u5c06ML\u6a21\u578b\u7684\u6570\u636e\u6d41\u56fe\u6620\u5c04\u5230\u53ef\u91cd\u6784\u7cfb\u7edf\u5f88\u56f0\u96be\uff0c\u56e0\u4e3a\u4e0d\u540c\u6620\u5c04\u5177\u6709\u4e0d\u540c\u7684\u541e\u5410\u91cf\u5e76\u6d88\u8017\u4e0d\u540c\u8d44\u6e90\u7ea6\u675f\u3002\u5b8c\u5168\u6d4b\u91cf\u541e\u5410\u91cf\u6210\u672c\u9ad8\u6602\uff0c\u800c\u4f20\u7edf\u624b\u5de5\u5206\u6790\u6a21\u578b\u4f9d\u8d56\u4ee3\u7406\u7279\u5f81\u6216\u76f4\u89c9\uff0c\u4f1a\u5f15\u5165\u8bef\u5dee\u3002", "method": "\u91c7\u7528\u5b66\u4e60\u65b9\u6cd5\u6765\u9884\u6d4b\u6620\u5c04\u7684\u541e\u5410\u91cf\uff0c\u8be5\u65b9\u6cd5\u5728\u53bb\u9664\u6027\u80fd\u6ce8\u91ca\u540e\u4ecd\u80fd\u4fdd\u6301\u51c6\u786e\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u56fe\u4e0a\u6bd4\u4f20\u7edf\u65b9\u6cd5\u51c6\u786e\u9884\u6d4b\u541e\u5410\u91cf31%-52%\uff0c\u4f7f\u7528\u8be5\u65b9\u6cd5\u80fd\u751f\u62105.6%\u66f4\u5feb\u7684\u7f16\u8bd1\u56fe\u3002", "conclusion": "\u5b66\u4e60\u65b9\u6cd5\u662f\u9884\u6d4b\u6570\u636e\u6d41\u56fe\u5728\u53ef\u91cd\u6784\u7cfb\u7edf\u4e0a\u541e\u5410\u91cf\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u624b\u5de5\u5206\u6790\u6a21\u578b\uff0c\u5e76\u80fd\u63d0\u5347\u7f16\u8bd1\u6027\u80fd\u3002"}}
{"id": "2511.02196", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02196", "abs": "https://arxiv.org/abs/2511.02196", "authors": ["Liwei Ni", "Jiaxi Zhang", "Shenggen Zheng", "Junfeng Liu", "Xingyu Meng", "Biwei Xie", "Xingquan Li", "Huawei Li"], "title": "BoolSkeleton: Boolean Network Skeletonization via Homogeneous Pattern Reduction", "comment": null, "summary": "Boolean equivalence allows Boolean networks with identical functionality to\nexhibit diverse graph structures. This gives more room for exploration in logic\noptimization, while also posing a challenge for tasks involving consistency\nbetween Boolean networks. To tackle this challenge, we introduce BoolSkeleton,\na novel Boolean network skeletonization method that improves the consistency\nand reliability of design-specific evaluations. BoolSkeleton comprises two key\nsteps: preprocessing and reduction. In preprocessing, the Boolean network is\ntransformed into a defined Boolean dependency graph, where nodes are assigned\nthe functionality-related status. Next, the homogeneous and heterogeneous\npatterns are defined for the node-level pattern reduction step. Heterogeneous\npatterns are preserved to maintain critical functionality-related dependencies,\nwhile homogeneous patterns can be reduced. Parameter K of the pattern further\nconstrains the fanin size of these patterns, enabling fine-tuned control over\nthe granularity of graph reduction. To validate BoolSkeleton's effectiveness,\nwe conducted four analysis/downstream tasks around the Boolean network:\ncompression analysis, classification, critical path analysis, and timing\nprediction, demonstrating its robustness across diverse scenarios. Furthermore,\nit improves above 55% in the average accuracy compared to the original Boolean\nnetwork for the timing prediction task. These experiments underscore the\npotential of BoolSkeleton to enhance design consistency in logic synthesis.", "AI": {"tldr": "BoolSkeleton\u662f\u4e00\u79cd\u5e03\u5c14\u7f51\u7edc\u9aa8\u67b6\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u5904\u7406\u548c\u5f52\u7ea6\u4e24\u4e2a\u6b65\u9aa4\uff0c\u5c06\u5e03\u5c14\u7f51\u7edc\u8f6c\u6362\u4e3a\u5e03\u5c14\u4f9d\u8d56\u56fe\u5e76\u8fdb\u884c\u6a21\u5f0f\u5f52\u7ea6\uff0c\u63d0\u9ad8\u8bbe\u8ba1\u4e00\u81f4\u6027\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5e03\u5c14\u7b49\u4ef7\u6027\u4f7f\u5f97\u529f\u80fd\u76f8\u540c\u7684\u5e03\u5c14\u7f51\u7edc\u5177\u6709\u4e0d\u540c\u7684\u56fe\u7ed3\u6784\uff0c\u8fd9\u4e3a\u903b\u8f91\u4f18\u5316\u63d0\u4f9b\u4e86\u63a2\u7d22\u7a7a\u95f4\uff0c\u4f46\u4e5f\u7ed9\u5e03\u5c14\u7f51\u7edc\u95f4\u7684\u4e00\u81f4\u6027\u4efb\u52a1\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u5305\u542b\u9884\u5904\u7406\u548c\u5f52\u7ea6\u4e24\u4e2a\u6b65\u9aa4\uff1a\u9884\u5904\u7406\u5c06\u5e03\u5c14\u7f51\u7edc\u8f6c\u6362\u4e3a\u5e03\u5c14\u4f9d\u8d56\u56fe\uff0c\u8282\u70b9\u88ab\u8d4b\u4e88\u529f\u80fd\u76f8\u5173\u72b6\u6001\uff1b\u5f52\u7ea6\u6b65\u9aa4\u5b9a\u4e49\u540c\u8d28\u548c\u5f02\u8d28\u6a21\u5f0f\uff0c\u4fdd\u7559\u5f02\u8d28\u6a21\u5f0f\u4ee5\u7ef4\u6301\u5173\u952e\u529f\u80fd\u4f9d\u8d56\uff0c\u5f52\u7ea6\u540c\u8d28\u6a21\u5f0f\uff0c\u53c2\u6570K\u63a7\u5236\u6a21\u5f0f\u6247\u5165\u5927\u5c0f\u4ee5\u8c03\u8282\u56fe\u5f52\u7ea6\u7c92\u5ea6\u3002", "result": "\u5728\u5e03\u5c14\u7f51\u7edc\u7684\u56db\u4e2a\u5206\u6790/\u4e0b\u6e38\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff1a\u538b\u7f29\u5206\u6790\u3001\u5206\u7c7b\u3001\u5173\u952e\u8def\u5f84\u5206\u6790\u548c\u65f6\u5e8f\u9884\u6d4b\uff0c\u5728\u65f6\u5e8f\u9884\u6d4b\u4efb\u52a1\u4e2d\u76f8\u6bd4\u539f\u59cb\u5e03\u5c14\u7f51\u7edc\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad855%\u4ee5\u4e0a\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\u4e86BoolSkeleton\u5728\u589e\u5f3a\u903b\u8f91\u7efc\u5408\u4e2d\u8bbe\u8ba1\u4e00\u81f4\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.01871", "categories": ["cs.DC", "68M15, 93B12"], "pdf": "https://arxiv.org/pdf/2511.01871", "abs": "https://arxiv.org/abs/2511.01871", "authors": ["S. Tsiramua", "H. Meladze", "T. Davitashvili", "J. M. Sanchez", "F. Criado-Aldeanueva"], "title": "Structural Analysis of Multi-Core Processor and Reliability Evaluation Model", "comment": null, "summary": "In the present paper, the models of structural analysis and evaluation of\nefficiency indicators (reliability, fault tolerance, viability, and\nflexibility) of a multi core processor with variable structure, equipped with\nmulti functional cores, are considered. Using logical probabilistic methods,\nthe following has been developed: models for evaluating the reliability and\nfault tolerance of processor cores as multi functional elements; logical\nprobabilistic models of the shortest paths, flexibility, and performance\nconditions for successful operation of multi core processors based on multi\nfunctional cores; and models for estimating the reliability, fault tolerance,\nand lifetime of multi core processors considering all possible states of\nperformance. The results of the structural analysis of two core and four core\nprocessors and the trends of increasing the efficiency indicators of multi core\nprocessors are presented.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u591a\u6838\u5904\u7406\u5668\u7ed3\u6784\u5206\u6790\u548c\u6548\u7387\u6307\u6807\u8bc4\u4f30\u6a21\u578b\uff0c\u5305\u62ec\u53ef\u9760\u6027\u3001\u5bb9\u9519\u6027\u3001\u751f\u5b58\u6027\u548c\u7075\u6d3b\u6027\u8bc4\u4f30\uff0c\u4f7f\u7528\u903b\u8f91\u6982\u7387\u65b9\u6cd5\u5206\u6790\u591a\u529f\u80fd\u6838\u5fc3\u7684\u6027\u80fd\u72b6\u6001\u3002", "motivation": "\u7814\u7a76\u591a\u6838\u5904\u7406\u5668\u7ed3\u6784\u5206\u6790\u548c\u6548\u7387\u6307\u6807\u8bc4\u4f30\uff0c\u65e8\u5728\u63d0\u9ad8\u591a\u6838\u5904\u7406\u5668\u7684\u53ef\u9760\u6027\u3001\u5bb9\u9519\u6027\u548c\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u903b\u8f91\u6982\u7387\u65b9\u6cd5\u5f00\u53d1\u4e86\u53ef\u9760\u6027\u3001\u5bb9\u9519\u6027\u8bc4\u4f30\u6a21\u578b\uff0c\u6700\u77ed\u8def\u5f84\u3001\u7075\u6d3b\u6027\u548c\u6027\u80fd\u6761\u4ef6\u6a21\u578b\uff0c\u4ee5\u53ca\u8003\u8651\u6240\u6709\u6027\u80fd\u72b6\u6001\u7684\u591a\u6838\u5904\u7406\u5668\u53ef\u9760\u6027\u3001\u5bb9\u9519\u6027\u548c\u5bff\u547d\u4f30\u8ba1\u6a21\u578b\u3002", "result": "\u63d0\u4f9b\u4e86\u53cc\u6838\u548c\u56db\u6838\u5904\u7406\u5668\u7684\u7ed3\u6784\u5206\u6790\u7ed3\u679c\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u6838\u5904\u7406\u5668\u6548\u7387\u6307\u6807\u7684\u589e\u957f\u8d8b\u52bf\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5206\u6790\u548c\u6548\u7387\u6307\u6807\u8bc4\u4f30\uff0c\u591a\u6838\u5904\u7406\u5668\u7684\u53ef\u9760\u6027\u3001\u5bb9\u9519\u6027\u548c\u6027\u80fd\u5f97\u5230\u6709\u6548\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u591a\u6838\u5904\u7406\u5668\u7684\u53d1\u5c55\u6f5c\u529b\u3002"}}
{"id": "2511.02285", "categories": ["cs.AR", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02285", "abs": "https://arxiv.org/abs/2511.02285", "authors": ["Zhuorui Zhao", "Bing Li", "Grace Li Zhang", "Ulf Schlichtmann"], "title": "VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning", "comment": "accepted by SOCC 2025", "summary": "Large Language Models (LLMs) have shown impressive potential in generating\nVerilog codes, but ensuring functional correctness remains a challenge.\nExisting approaches often rely on self-consistency or simulation feedback to\nselect the best candidate, but they miss opportunities to focus LLM reasoning\non the most informative parts of the design. We propose VFocus, a three-stage\nframework that enhances Verilog generation by sharpening the focus of LLM\nreasoning onto critical decision points in the code generation process. In the\n\\textbf{pre-ranking stage}, VFocus generates multiple code candidates through\nLLM prompting, retries for syntactically valid outputs, and introduces a\n\\textit{Density-guided Filtering} to retain candidates that fall within the\n\"reasoning sweet spot\" for functional correctness. In the \\textbf{ranking\nstage}, we simulate each code candidate using an automatically generated\ntestbench and apply self-consistency-based clustering to identify the most\nconsistent outputs. Finally, in the \\textbf{post-ranking refinement stage},\nVFocus performs inconsistency mining on top-ranked candidates and invokes\nreasoning-augmented LLM prompts for candidate refinement. Experiments on the\nVerilogEval-Human benchmark show that VFocus significantly improves the pass@1\ncorrectness across multiple reasoning LLMs, demonstrating its effectiveness in\nenhancing Verilog generation for complex hardware design tasks.", "AI": {"tldr": "VFocus\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7126LLM\u63a8\u7406\u4e8e\u4ee3\u7801\u751f\u6210\u7684\u5173\u952e\u51b3\u7b56\u70b9\u6765\u589e\u5f3aVerilog\u751f\u6210\u3002\u5b83\u5305\u62ec\u9884\u6392\u540d\u9636\u6bb5\u7684\u5bc6\u5ea6\u5f15\u5bfc\u8fc7\u6ee4\u3001\u6392\u540d\u9636\u6bb5\u7684\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u805a\u7c7b\uff0c\u4ee5\u53ca\u540e\u6392\u540d\u9636\u6bb5\u7684\u7ec6\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86Verilog\u4ee3\u7801\u7684\u529f\u80fd\u6b63\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u81ea\u4e00\u81f4\u6027\u6216\u6a21\u62df\u53cd\u9988\u6765\u9009\u62e9\u6700\u4f73\u5019\u9009\uff0c\u4f46\u672a\u80fd\u5c06LLM\u63a8\u7406\u805a\u7126\u5728\u8bbe\u8ba1\u4e2d\u4fe1\u606f\u6700\u4e30\u5bcc\u7684\u90e8\u5206\uff0c\u5bfc\u81f4\u529f\u80fd\u6b63\u786e\u6027\u96be\u4ee5\u4fdd\u8bc1\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u9884\u6392\u540d\u9636\u6bb5\uff1a\u751f\u6210\u591a\u4e2a\u4ee3\u7801\u5019\u9009\uff0c\u5e94\u7528\u5bc6\u5ea6\u5f15\u5bfc\u8fc7\u6ee4\u4fdd\u7559\u5728\"\u63a8\u7406\u751c\u70b9\"\u8303\u56f4\u5185\u7684\u5019\u9009\uff1b2) \u6392\u540d\u9636\u6bb5\uff1a\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u5e73\u53f0\u6a21\u62df\u4ee3\u7801\uff0c\u57fa\u4e8e\u81ea\u4e00\u81f4\u6027\u805a\u7c7b\u8bc6\u522b\u6700\u4e00\u81f4\u8f93\u51fa\uff1b3) \u540e\u6392\u540d\u7ec6\u5316\u9636\u6bb5\uff1a\u5bf9\u6392\u540d\u9760\u524d\u7684\u5019\u9009\u8fdb\u884c\u4e0d\u4e00\u81f4\u6027\u6316\u6398\uff0c\u4f7f\u7528\u63a8\u7406\u589e\u5f3a\u7684LLM\u63d0\u793a\u8fdb\u884c\u5019\u9009\u7ec6\u5316\u3002", "result": "\u5728VerilogEval-Human\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVFocus\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u4e2a\u63a8\u7406LLM\u7684pass@1\u6b63\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u786c\u4ef6\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u589e\u5f3aVerilog\u751f\u6210\u7684\u6709\u6548\u6027\u3002", "conclusion": "VFocus\u901a\u8fc7\u805a\u7126LLM\u63a8\u7406\u4e8e\u5173\u952e\u51b3\u7b56\u70b9\uff0c\u6709\u6548\u63d0\u5347\u4e86Verilog\u4ee3\u7801\u751f\u6210\u7684\u529f\u80fd\u6b63\u786e\u6027\uff0c\u4e3a\u590d\u6742\u786c\u4ef6\u8bbe\u8ba1\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02269", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02269", "abs": "https://arxiv.org/abs/2511.02269", "authors": ["Takuto Ando", "Yu Eto", "Ayumu Takeuchi", "Yasuhiko Nakashima"], "title": "Energy-Efficient Hardware Acceleration of Whisper ASR on a CGLA", "comment": "This paper is accepted at The Thirteenth International Symposium on\n  Computing and Networking (CANDAR2025)", "summary": "The rise of generative AI for tasks like Automatic Speech Recognition (ASR)\nhas created a critical energy consumption challenge. While ASICs offer high\nefficiency, they lack the programmability to adapt to evolving algorithms. To\naddress this trade-off, we implement and evaluate Whisper's core computational\nkernel on the IMAX, a general-purpose Coarse-Grained Linear Arrays (CGLAs)\naccelerator. To our knowledge, this is the first work to execute a Whisper\nkernel on a CGRA and compare its performance against CPUs and GPUs. Using\nhardware/software co-design, we evaluate our system via an FPGA prototype and\nproject performance for a 28 nm ASIC. Our results demonstrate superior energy\nefficiency. The projected ASIC is 1.90x more energy-efficient than the NVIDIA\nJetson AGX Orin and 9.83x more than an NVIDIA RTX 4090 for the Q8_0 model. This\nwork positions CGLA as a promising platform for sustainable ASR on\npower-constrained edge devices.", "AI": {"tldr": "\u5728IMAX CGLA\u52a0\u901f\u5668\u4e0a\u5b9e\u73b0\u548c\u8bc4\u4f30Whisper\u6838\u5fc3\u8ba1\u7b97\u5185\u6838\uff0c\u76f8\u6bd4CPU\u548cGPU\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u80fd\u6548\u8868\u73b0\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u4efb\u52a1\u4e2d\u7684\u5174\u8d77\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u80fd\u8017\u6311\u6218\uff0cASIC\u867d\u7136\u9ad8\u6548\u4f46\u7f3a\u4e4f\u53ef\u7f16\u7a0b\u6027\u6765\u9002\u5e94\u4e0d\u65ad\u6f14\u8fdb\u7684\u7b97\u6cd5\u3002", "method": "\u91c7\u7528\u786c\u4ef6/\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5728FPGA\u539f\u578b\u4e0a\u5b9e\u73b0Whisper\u6838\u5fc3\u8ba1\u7b97\u5185\u6838\uff0c\u5e76\u57fa\u4e8e28nm ASIC\u8fdb\u884c\u6027\u80fd\u9884\u6d4b\u3002", "result": "\u6295\u5f71\u7684ASIC\u5728Q8_0\u6a21\u578b\u4e0a\u6bd4NVIDIA Jetson AGX Orin\u80fd\u6548\u9ad81.90\u500d\uff0c\u6bd4NVIDIA RTX 4090\u9ad89.83\u500d\u3002", "conclusion": "CGLA\u662f\u529f\u7387\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u53ef\u6301\u7eedASR\u7684\u6709\u524d\u666f\u5e73\u53f0\u3002"}}
{"id": "2511.01881", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01881", "abs": "https://arxiv.org/abs/2511.01881", "authors": ["Zhengxin Fang", "Hui Ma", "Gang Chen", "Rajkumar Buyya"], "title": "HGraphScale: Hierarchical Graph Learning for Autoscaling Microservice Applications in Container-based Cloud Computing", "comment": null, "summary": "Microservice architecture has become a dominant paradigm in application\ndevelopment due to its advantages of being lightweight, flexible, and\nresilient. Deploying microservice applications in the container-based cloud\nenables fine-grained elastic resource allocation. Autoscaling is an effective\napproach to dynamically adjust the resource provisioned to containers. However,\nthe intricate microservice dependencies and the deployment scheme of the\ncontainer-based cloud bring extra challenges of resource scaling. This article\nproposes a novel autoscaling approach named HGraphScale. In particular,\nHGraphScale captures microservice dependencies and the deployment scheme by a\nnewly designed hierarchical graph neural network, and makes effective scaling\nactions for rapidly changing user requests workloads. Extensive experiments\nbased on real-world traces of user requests are conducted to evaluate the\neffectiveness of HGraphScale. The experiment results show that the HGraphScale\noutperforms existing state-of-the-art autoscaling approaches by reducing at\nmost 80.16\\% of the average response time under a certain VM rental budget of\napplication providers.", "AI": {"tldr": "HGraphScale\u662f\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5fae\u670d\u52a1\u81ea\u52a8\u6269\u7f29\u5bb9\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5fae\u670d\u52a1\u4f9d\u8d56\u5173\u7cfb\u548c\u5bb9\u5668\u4e91\u90e8\u7f72\u65b9\u6848\uff0c\u5728\u7528\u6237\u8bf7\u6c42\u8d1f\u8f7d\u5feb\u901f\u53d8\u5316\u65f6\u505a\u51fa\u6709\u6548\u7684\u6269\u7f29\u5bb9\u51b3\u7b56\u3002", "motivation": "\u5fae\u670d\u52a1\u67b6\u6784\u5728\u5bb9\u5668\u4e91\u4e2d\u7684\u90e8\u7f72\u5e26\u6765\u4e86\u7ec6\u7c92\u5ea6\u7684\u5f39\u6027\u8d44\u6e90\u5206\u914d\u80fd\u529b\uff0c\u4f46\u5fae\u670d\u52a1\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u548c\u5bb9\u5668\u4e91\u7684\u90e8\u7f72\u65b9\u6848\u7ed9\u8d44\u6e90\u6269\u7f29\u5bb9\u5e26\u6765\u4e86\u989d\u5916\u6311\u6218\u3002", "method": "\u63d0\u51faHGraphScale\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u8bbe\u8ba1\u7684\u5206\u5c42\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u83b7\u5fae\u670d\u52a1\u4f9d\u8d56\u5173\u7cfb\u548c\u90e8\u7f72\u65b9\u6848\uff0c\u4e3a\u5feb\u901f\u53d8\u5316\u7684\u7528\u6237\u8bf7\u6c42\u8d1f\u8f7d\u505a\u51fa\u6709\u6548\u7684\u6269\u7f29\u5bb9\u51b3\u7b56\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u8bf7\u6c42\u8f68\u8ff9\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHGraphScale\u5728\u7279\u5b9aVM\u79df\u8d41\u9884\u7b97\u4e0b\uff0c\u6700\u591a\u80fd\u51cf\u5c1180.16%\u7684\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u65b9\u6cd5\u3002", "conclusion": "HGraphScale\u901a\u8fc7\u5206\u5c42\u56fe\u795e\u7ecf\u7f51\u7edc\u6709\u6548\u89e3\u51b3\u4e86\u5fae\u670d\u52a1\u5728\u5bb9\u5668\u4e91\u4e2d\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2511.02408", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02408", "abs": "https://arxiv.org/abs/2511.02408", "authors": ["Takuto Ando", "Yusuke Inoue"], "title": "Facial Expression Recognition System Using DNN Accelerator with Multi-threading on FPGA", "comment": "This paper was published in the proceedings of the 2024 Twelfth\n  International Symposium on Computing and Networking Workshops (CANDARW)", "summary": "In this paper, we implement a stand-alone facial expression recognition\nsystem on an SoC FPGA with multi-threading using a Deep learning Processor Unit\n(DPU). The system consists of two steps: one for face detection step and one\nfor facial expression recognition. In the previous work, the Haar Cascade\ndetector was run on a CPU in the face detection step due to FPGA resource\nlimitations, but this detector is less accurate for profile and variable\nillumination condition images. Moreover, the previous work used a dedicated\ncircuit accelerator, so running a second DNN inference for face detection on\nthe FPGA would require the addition of a new accelerator. As an alternative to\nthis approach, we run the two inferences by DNN on a DPU, which is a\ngeneral-purpose CNN accelerator of the systolic array type. Our method for face\ndetection using DenseBox and facial expression recognition using CNN on the\nsame DPU enables the efficient use of FPGA resources while maintaining a small\ncircuit size. We also developed a multi-threading technique that improves the\noverall throughput while increasing the DPU utilization efficiency. With this\napproach, we achieved an overall system throughput of 25 FPS and a throughput\nper power consumption of 2.4 times.", "AI": {"tldr": "\u5728SoC FPGA\u4e0a\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8eDPU\u7684\u591a\u7ebf\u7a0b\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7cfb\u7edf\uff0c\u4f7f\u7528DenseBox\u8fdb\u884c\u4eba\u8138\u68c0\u6d4b\u548cCNN\u8fdb\u884c\u8868\u60c5\u8bc6\u522b\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u5de5\u4f5c\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u548c\u7cfb\u7edf\u6548\u7387\u3002", "motivation": "\u4e4b\u524d\u7684\u5de5\u4f5c\u4f7f\u7528Haar Cascade\u68c0\u6d4b\u5668\u5728CPU\u4e0a\u8fd0\u884c\u4eba\u8138\u68c0\u6d4b\uff0c\u51c6\u786e\u7387\u8f83\u4f4e\uff0c\u4e14\u9700\u8981\u4e13\u7528\u7535\u8def\u52a0\u901f\u5668\u3002\u672c\u6587\u65e8\u5728\u4f7f\u7528DPU\u7edf\u4e00\u5904\u7406\u4e24\u4e2aDNN\u63a8\u7406\uff0c\u63d0\u9ad8\u51c6\u786e\u7387\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002", "method": "\u5728SoC FPGA\u4e0a\u4f7f\u7528DPU\u8fd0\u884cDenseBox\u4eba\u8138\u68c0\u6d4b\u548cCNN\u8868\u60c5\u8bc6\u522b\u4e24\u4e2aDNN\u63a8\u7406\uff0c\u5f00\u53d1\u591a\u7ebf\u7a0b\u6280\u672f\u63d0\u9ad8\u541e\u5410\u91cf\u548cDPU\u5229\u7528\u7387\u3002", "result": "\u5b9e\u73b0\u4e8625 FPS\u7684\u6574\u4f53\u7cfb\u7edf\u541e\u5410\u91cf\uff0c\u529f\u8017\u6548\u7387\u63d0\u9ad8\u4e862.4\u500d\u3002", "conclusion": "\u4f7f\u7528DPU\u7edf\u4e00\u5904\u7406\u4eba\u8138\u68c0\u6d4b\u548c\u8868\u60c5\u8bc6\u522b\u4efb\u52a1\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u5c0f\u7535\u8def\u89c4\u6a21\u7684\u540c\u65f6\u9ad8\u6548\u5229\u7528FPGA\u8d44\u6e90\uff0c\u5e76\u901a\u8fc7\u591a\u7ebf\u7a0b\u6280\u672f\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2511.01888", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01888", "abs": "https://arxiv.org/abs/2511.01888", "authors": ["Cynthia Marcelino", "Thomas Pusztai", "Stefan Nastic"], "title": "Roadrunner: Accelerating Data Delivery to WebAssembly-Based Serverless Functions", "comment": "26th International Middleware Conference (MIDDLEWARE 25)", "summary": "Serverless computing provides infrastructure management and elastic\nauto-scaling, therefore reducing operational overhead. By design serverless\nfunctions are stateless, which means they typically leverage external remote\nservices to store and exchange data. Transferring data over a network typically\ninvolves serialization and deserialization. These operations usually require\nmultiple data copies and transitions between user and kernel space, resulting\nin overhead from context switching and memory allocation, contributing\nsignificantly to increased latency and resource consumption. To address these\nissues, we present Roadrunner, a sidecar shim that enables near-zero copy and\nserialization-free data transfer between WebAssembly-based serverless\nfunctions. Roadrunner reduces the multiple copies between user space and kernel\nspace by mapping the function memory and moving the data along a dedicated\nvirtual data hose, bypassing the costly processes of serialization and\ndeserialization. This approach reduces data movement overhead and context\nswitching, achieving near-native latency performance for WebAssembly-based\nserverless functions. Our experimental results demonstrate that Roadrunner\nsignificantly improves the inter-function communication latency from 44% up to\n89%, reducing the serialization overhead in 97% of data transfer, and\nincreasing throughput by 69 times compared to state-of-the-art\nWebAssembly-based serverless functions.", "AI": {"tldr": "Roadrunner\u662f\u4e00\u4e2asidecar shim\uff0c\u901a\u8fc7\u96f6\u62f7\u8d1d\u548c\u514d\u5e8f\u5217\u5316\u6570\u636e\u4f20\u8f93\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347WebAssembly\u65e0\u670d\u52a1\u5668\u51fd\u6570\u7684\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u4e2d\u51fd\u6570\u95f4\u6570\u636e\u4f20\u8f93\u9700\u8981\u5e8f\u5217\u5316/\u53cd\u5e8f\u5217\u5316\uff0c\u5bfc\u81f4\u591a\u526f\u672c\u3001\u4e0a\u4e0b\u6587\u5207\u6362\u548c\u5185\u5b58\u5206\u914d\u5f00\u9500\uff0c\u589e\u52a0\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u3002", "method": "\u4f7f\u7528sidecar shim\u67b6\u6784\uff0c\u901a\u8fc7\u6620\u5c04\u51fd\u6570\u5185\u5b58\u548c\u4e13\u7528\u865a\u62df\u6570\u636e\u7ba1\u9053\uff0c\u7ed5\u8fc7\u5e8f\u5217\u5316\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u8fd1\u96f6\u62f7\u8d1d\u6570\u636e\u4f20\u8f93\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u901a\u4fe1\u5ef6\u8fdf\u964d\u4f4e44%-89%\uff0c97%\u7684\u6570\u636e\u4f20\u8f93\u6d88\u9664\u4e86\u5e8f\u5217\u5316\u5f00\u9500\uff0c\u541e\u5410\u91cf\u63d0\u534769\u500d\u3002", "conclusion": "Roadrunner\u901a\u8fc7\u8fd1\u96f6\u62f7\u8d1d\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u670d\u52a1\u5668\u51fd\u6570\u95f4\u6570\u636e\u4f20\u8f93\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u539f\u751f\u6027\u80fd\u7684\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2511.02494", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02494", "abs": "https://arxiv.org/abs/2511.02494", "authors": ["Raul Murillo", "Julio Villalba-Moreno", "Alberto A. Del Barrio", "Guillermo Botella"], "title": "Digit-Recurrence Posit Division", "comment": "11 pages, 9 figures", "summary": "Posit arithmetic has emerged as a promising alternative to IEEE 754\nfloating-point representation, offering enhanced accuracy and dynamic range.\nHowever, division operations in posit systems remain challenging due to their\ninherent hardware complexity. In this work, we present posit division units\nbased on the digit-recurrence algorithm, marking the first implementation of\nradix-4 digit-recurrence techniques within this context. Our approach\nincorporates hardware-centric optimizations including redundant arithmetic,\non-the-fly quotient conversion, and operand scaling to streamline the division\nprocess while mitigating latency, area, and power overheads. Comprehensive\nsynthesis evaluations across multiple posit configurations demonstrate\nsignificant performance improvements, including more than 80% energy reduction\nwith small area overhead compared to existing methods, and a substantial\ndecrease in the number of iterations. These results underscore the potential of\nour adapted algorithm to enhance the efficiency of posit-based arithmetic\nunits.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u6570\u5b57\u9012\u5f52\u7b97\u6cd5\u7684posit\u9664\u6cd5\u5355\u5143\uff0c\u9996\u6b21\u5728posit\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u57fa\u6570-4\u6570\u5b57\u9012\u5f52\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u548c\u8fed\u4ee3\u6b21\u6570\u3002", "motivation": "Posit\u7b97\u672f\u4f5c\u4e3aIEEE 754\u6d6e\u70b9\u8868\u793a\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u52a8\u6001\u8303\u56f4\uff0c\u4f46\u5176\u9664\u6cd5\u64cd\u4f5c\u7531\u4e8e\u786c\u4ef6\u590d\u6742\u6027\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u6570\u5b57\u9012\u5f52\u7b97\u6cd5\uff0c\u7ed3\u5408\u5197\u4f59\u7b97\u672f\u3001\u5728\u7ebf\u5546\u8f6c\u6362\u548c\u64cd\u4f5c\u6570\u7f29\u653e\u7b49\u786c\u4ef6\u4f18\u5316\u6280\u672f\u6765\u7b80\u5316\u9664\u6cd5\u8fc7\u7a0b\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8d85\u8fc780%\u7684\u80fd\u8017\u964d\u4f4e\uff0c\u4ec5\u6709\u5c0f\u9762\u79ef\u5f00\u9500\uff0c\u5e76\u5927\u5e45\u51cf\u5c11\u4e86\u8fed\u4ee3\u6b21\u6570\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u6709\u6f5c\u529b\u663e\u8457\u63d0\u5347\u57fa\u4e8eposit\u7684\u7b97\u672f\u5355\u5143\u7684\u6548\u7387\u3002"}}
{"id": "2511.01893", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.01893", "abs": "https://arxiv.org/abs/2511.01893", "authors": ["Bin Ma", "Viktor Nikitin", "Xi Wang", "Tekin Bicer", "Dong Li"], "title": "mLR: Scalable Laminography Reconstruction based on Memoization", "comment": null, "summary": "ADMM-FFT is an iterative method with high reconstruction accuracy for\nlaminography but suffers from excessive computation time and large memory\nconsumption. We introduce mLR, which employs memoization to replace the\ntime-consuming Fast Fourier Transform (FFT) operations based on an unique\nobservation that similar FFT operations appear in iterations of ADMM-FFT. We\nintroduce a series of techniques to make the application of memoization to\nADMM-FFT performance-beneficial and scalable. We also introduce variable\noffloading to save CPU memory and scale ADMM-FFT across GPUs within and across\nnodes. Using mLR, we are able to scale ADMM-FFT on an input problem of\n2Kx2Kx2K, which is the largest input problem laminography reconstruction has\never worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8%\nperformance improvement on average (up to 65.4%), compared to the original\nADMM-FFT.", "AI": {"tldr": "mLR\u901a\u8fc7\u8bb0\u5fc6\u5316\u6280\u672f\u4f18\u5316ADMM-FFT\u7b97\u6cd5\uff0c\u7528\u7f13\u5b58\u66ff\u4ee3\u91cd\u590d\u7684FFT\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6027\u80fd\u5e76\u964d\u4f4e\u5185\u5b58\u6d88\u8017\uff0c\u652f\u6301\u66f4\u5927\u89c4\u6a21\u95ee\u9898\u6c42\u89e3\u3002", "motivation": "ADMM-FFT\u7b97\u6cd5\u5728\u5c42\u6790\u6210\u50cf\u91cd\u5efa\u4e2d\u7cbe\u5ea6\u9ad8\u4f46\u8ba1\u7b97\u65f6\u95f4\u957f\u3001\u5185\u5b58\u6d88\u8017\u5927\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u8fed\u4ee3\u4e2dFFT\u64cd\u4f5c\u76f8\u4f3c\u7684\u89c2\u5bdf\uff0c\u5f15\u5165\u8bb0\u5fc6\u5316\u6280\u672f\u7f13\u5b58\u8ba1\u7b97\u7ed3\u679c\uff1b\u91c7\u7528\u53d8\u91cf\u5378\u8f7d\u6280\u672f\u8282\u7701CPU\u5185\u5b58\uff1b\u652f\u6301\u8de8GPU\u548c\u8de8\u8282\u70b9\u6269\u5c55\u3002", "result": "\u6210\u529f\u57282Kx2Kx2K\u89c4\u6a21\u95ee\u9898\u4e0a\u6269\u5c55ADMM-FFT\uff0c\u8fd9\u662f\u8be5\u7b97\u6cd5\u5728\u6709\u9650\u5185\u5b58\u4e0b\u5904\u7406\u7684\u6700\u5927\u89c4\u6a21\u95ee\u9898\uff1b\u76f8\u6bd4\u539f\u59cbADMM-FFT\u5e73\u5747\u6027\u80fd\u63d0\u534752.8%\uff0c\u6700\u9ad8\u8fbe65.4%\u3002", "conclusion": "mLRA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86ADMM-FFT\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u66f4\u5927\u89c4\u6a21\u7684\u5c42\u6790\u6210\u50cf\u91cd\u5efa\u95ee\u9898\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.02530", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.02530", "abs": "https://arxiv.org/abs/2511.02530", "authors": ["Takuto Ando", "Yu Eto", "Yasuhiko Nakashima"], "title": "Implementation and Evaluation of Stable Diffusion on a General-Purpose CGLA Accelerator", "comment": "This paper is accepted at 2025 IEEE 18th International Symposium on\n  Embedded Multicore/Many-core Systems-on-Chip (MCSoC)", "summary": "This paper presents the first implementation and in-depth evaluation of the\nprimary computational kernels from the stable-diffusion.cpp image generation\nframework on IMAX3, a general-purpose Coarse-Grained Reconfigurable Array\n(CGRA) accelerator. We designed IMAX3 as a versatile computational platform,\nand this work assesses its capabilities by executing a demanding image\ngeneration workload. We evaluate its performance on a current\nField-Programmable Gate Array (FPGA) prototype to establish a baseline and\nproject its potential for a future Application-Specific Integrated Circuit\n(ASIC) implementation. Our results demonstrate that, despite its\ngeneral-purpose architecture, IMAX3 achieves promising performance and power\nefficiency, particularly in its projected ASIC form. This work provides\nconcrete guidelines for future IMAX architectural designs and establishes a\nfoundation for developing next-generation, AI-specialized Coarse-Grained Linear\nArray (CGLA) accelerators by refining this versatile platform. Ultimately, this\nachievement contributes to the realization of energy-efficient, on-device,\nmulti-modal AI platforms.", "AI": {"tldr": "\u5728IMAX3 CGRA\u52a0\u901f\u5668\u4e0a\u9996\u6b21\u5b9e\u73b0\u5e76\u8bc4\u4f30stable-diffusion.cpp\u56fe\u50cf\u751f\u6210\u6838\u5fc3\u8ba1\u7b97\u4efb\u52a1\uff0c\u8bc4\u4f30\u5176\u5728FPGA\u539f\u578b\u4e0a\u7684\u6027\u80fd\u5e76\u9884\u6d4bASIC\u5b9e\u73b0\u7684\u6f5c\u529b\u3002", "motivation": "\u8bc4\u4f30\u901a\u7528CGRA\u52a0\u901f\u5668IMAX3\u5728\u5904\u7406\u8981\u6c42\u8f83\u9ad8\u7684\u56fe\u50cf\u751f\u6210\u5de5\u4f5c\u8d1f\u8f7d\u65f6\u7684\u80fd\u529b\uff0c\u4e3a\u672a\u6765IMAX\u67b6\u6784\u8bbe\u8ba1\u548cAI\u4e13\u7528CGLA\u52a0\u901f\u5668\u5f00\u53d1\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u5728IMAX3 CGRA\u52a0\u901f\u5668\u4e0a\u5b9e\u73b0stable-diffusion.cpp\u7684\u4e3b\u8981\u8ba1\u7b97\u5185\u6838\uff0c\u5728FPGA\u539f\u578b\u4e0a\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\uff0c\u5e76\u9884\u6d4bASIC\u5b9e\u73b0\u7684\u6027\u80fd\u6f5c\u529b\u3002", "result": "\u5c3d\u7ba1\u91c7\u7528\u901a\u7528\u67b6\u6784\uff0cIMAX3\u5728\u6027\u80fd\uff08\u7279\u522b\u662f\u9884\u6d4b\u7684ASIC\u5f62\u5f0f\uff09\u548c\u80fd\u6548\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5b9e\u73b0\u80fd\u6548\u9ad8\u3001\u8bbe\u5907\u7aef\u3001\u591a\u6a21\u6001AI\u5e73\u53f0\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765IMAX\u67b6\u6784\u8bbe\u8ba1\u548cAI\u4e13\u7528CGLA\u52a0\u901f\u5668\u5f00\u53d1\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002"}}
{"id": "2511.02034", "categories": ["cs.DC", "cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02034", "abs": "https://arxiv.org/abs/2511.02034", "authors": ["Shashank Motepalli", "Naman Garg", "Gengrui Zhang", "Hans-Arno Jacobsen"], "title": "GPoS: Geospatially-aware Proof of Stake", "comment": "Published in ACM TWEB", "summary": "Geospatial decentralization is essential for blockchains, ensuring regulatory\nresilience, robustness, and fairness. We empirically analyze five major Proof\nof Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui,\nrevealing that a few geographic regions dominate consensus voting power,\nresulting in limited geospatial decentralization. To address this, we propose\nGeospatially aware Proof of Stake (GPoS), which integrates geospatial diversity\nwith stake-based voting power. Experimental evaluation demonstrates an average\n45% improvement in geospatial decentralization, as measured by the Gini\ncoefficient of Eigenvector centrality, while incurring minimal performance\noverhead in BFT protocols, including HotStuff and CometBFT. These results\ndemonstrate that GPoS can improve geospatial decentralization {while, in our\nexperiments, incurring minimal overhead} to consensus performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5730\u7406\u7a7a\u95f4\u611f\u77e5\u7684\u6743\u76ca\u8bc1\u660e\u673a\u5236\uff08GPoS\uff09\uff0c\u901a\u8fc7\u5c06\u5730\u7406\u7a7a\u95f4\u591a\u6837\u6027\u4e0e\u57fa\u4e8e\u6743\u76ca\u7684\u6295\u7968\u6743\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86PoS\u533a\u5757\u94fe\u7684\u5730\u7406\u7a7a\u95f4\u53bb\u4e2d\u5fc3\u5316\u7a0b\u5ea6\u3002", "motivation": "\u5f53\u524d\u4e3b\u8981PoS\u533a\u5757\u94fe\u5b58\u5728\u5730\u7406\u7a7a\u95f4\u96c6\u4e2d\u95ee\u9898\uff0c\u5c11\u6570\u5730\u533a\u4e3b\u5bfc\u5171\u8bc6\u6295\u7968\u6743\uff0c\u5f71\u54cd\u4e86\u533a\u5757\u94fe\u7684\u76d1\u7ba1\u5f39\u6027\u3001\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51faGPoS\u673a\u5236\uff0c\u5c06\u5730\u7406\u7a7a\u95f4\u591a\u6837\u6027\u4e0e\u6743\u76ca\u8bc1\u660e\u76f8\u7ed3\u5408\uff0c\u5728HotStuff\u548cCometBFT\u7b49BFT\u534f\u8bae\u4e2d\u5b9e\u73b0\u5730\u7406\u7a7a\u95f4\u611f\u77e5\u7684\u5171\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0cGPoS\u4f7f\u5730\u7406\u7a7a\u95f4\u53bb\u4e2d\u5fc3\u5316\u7a0b\u5ea6\u5e73\u5747\u63d0\u9ad8\u4e8645%\uff08\u57fa\u4e8e\u7279\u5f81\u5411\u91cf\u4e2d\u5fc3\u6027\u7684\u57fa\u5c3c\u7cfb\u6570\uff09\uff0c\u540c\u65f6\u5bf9\u5171\u8bc6\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "GPoS\u80fd\u591f\u6709\u6548\u6539\u5584PoS\u533a\u5757\u94fe\u7684\u5730\u7406\u7a7a\u95f4\u53bb\u4e2d\u5fc3\u5316\uff0c\u4e14\u5728\u5b9e\u8df5\u4e2d\u4ec5\u5e26\u6765\u6700\u5c0f\u7684\u6027\u80fd\u5f00\u9500\u3002"}}
{"id": "2511.02168", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02168", "abs": "https://arxiv.org/abs/2511.02168", "authors": ["Octavian Alexandru Trifan", "Karthik Sangaiah", "Muhammad Awad", "Muhammad Osama", "Sumanth Gudaparthi", "Alexandru Nicolau", "Alexander Veidenbaum", "Ganesh Dasika"], "title": "Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs", "comment": null, "summary": "As large language models (LLMs) continue to scale, their workloads\nincreasingly rely on distributed execution across multiple GPUs. However, the\nconventional bulk synchronous parallel~(BSP) model used in such settings\nintroduces significant performance inefficiencies. To characterize these\nbottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel\nData Locality, and Kernel Launch Overhead) as an analytical framework. We\npropose moving beyond the rigid BSP model to address key inefficiencies in\ndistributed GPU execution. By exploiting libraries like Iris for Triton, we\ngain access to in-kernel communication primitives that enable the design of\nnovel fine-grained programming patterns, offering greater flexibility and\nperformance than traditional BSP-based approaches. These patterns\nsystematically eliminate the three taxes by creating direct, tile-level\nproducer-consumer pipelines and replacing global barriers with fine-grained\ndataflow synchronization. Applying this methodology to critical kernels, from\nthe foundational All-Gather + general matrix multiplication operation to the\ncomplex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end\nlatency over BSP-based approaches, establishing a more programmable and\nefficient paradigm for distributed LLM workloads.", "AI": {"tldr": "\u63d0\u51fa\u8d85\u8d8a\u4f20\u7edfBSP\u6a21\u578b\u7684\u5206\u5e03\u5f0fGPU\u6267\u884c\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7f16\u7a0b\u6a21\u5f0f\u6d88\u9664\"\u4e09\u5927\u7a0e\"\uff0c\u5728LLM\u5206\u5e03\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u5b9e\u73b010-20%\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u4f20\u7edfBSP\u6a21\u578b\u5728\u591aGPU\u5206\u5e03\u5f0f\u6267\u884c\u4e2d\u5f15\u5165\u663e\u8457\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6267\u884c\u8303\u5f0f\u3002", "method": "\u5229\u7528Iris for Triton\u7b49\u5e93\u63d0\u4f9b\u7684\u6838\u5185\u901a\u4fe1\u539f\u8bed\uff0c\u8bbe\u8ba1\u7ec6\u7c92\u5ea6\u7f16\u7a0b\u6a21\u5f0f\uff0c\u5efa\u7acb\u76f4\u63a5\u7684\u74e6\u7247\u7ea7\u751f\u4ea7\u8005-\u6d88\u8d39\u8005\u6d41\u6c34\u7ebf\uff0c\u7528\u7ec6\u7c92\u5ea6\u6570\u636e\u6d41\u540c\u6b65\u66ff\u4ee3\u5168\u5c40\u5c4f\u969c\u3002", "result": "\u5728\u5173\u952e\u5185\u6838\uff08\u4eceAll-Gather + GEMM\u64cd\u4f5c\u5230\u590d\u6742Flash Decode\u7b97\u6cd5\uff09\u4e0a\uff0c\u76f8\u6bd4BSP\u65b9\u6cd5\u5b9e\u73b0\u4e8610-20%\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u52a0\u901f\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u66f4\u53ef\u7f16\u7a0b\u548c\u9ad8\u6548\u7684\u5206\u5e03\u5f0fLLM\u5de5\u4f5c\u8d1f\u8f7d\u6267\u884c\u8303\u5f0f\uff0c\u7cfb\u7edf\u6027\u5730\u6d88\u9664\u4e86BSP\u6a21\u578b\u7684\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2511.02248", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02248", "abs": "https://arxiv.org/abs/2511.02248", "authors": ["Xingqi Cui", "Chieh-Jan Mike Liang", "Jiarong Xing", "Haoran Qiu"], "title": "From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models", "comment": "16 pages, 13 figures", "summary": "Serving large generative models such as LLMs and multi- modal transformers\nrequires balancing user-facing SLOs (e.g., time-to-first-token,\ntime-between-tokens) with provider goals of efficiency and cost reduction.\nExisting solutions rely on static provisioning or model-level autoscaling, both\nof which treat the model as a monolith. This coarse-grained resource management\nleads to degraded performance or significant resource underutilization due to\npoor adaptability to dynamic inference traffic that is common online.\n  The root cause of this inefficiency lies in the internal structure of\ngenerative models: they are executed as graphs of interconnected operators.\nThrough detailed characterization and systematic analysis, we find that\noperators are heterogeneous in their compute and memory footprints and exhibit\ndiverse sensitivity to workload and resource factors such as batch size,\nsequence length, and traffic rate. This heterogeneity suggests that the\noperator, rather than the entire model, is the right granularity for scaling\ndecisions.\n  We propose an operator-level autoscaling framework, which allocates resources\nat finer (operator)-granularity, optimizing the scaling, batching, and\nplacement based on individual operator profiles. Evaluated on production-scale\ntraces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less\nenergy, or under fixed resources achieves 1.6x higher throughput with 5% less\nenergy. These results show that the operator, rather than the model, is\nfundamentally a more effective unit for scaling large generative workloads.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b97\u5b50\u7ea7\u522b\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8d44\u6e90\u7ba1\u7406\u4f18\u5316\u5927\u6a21\u578b\u63a8\u7406\u670d\u52a1\uff0c\u76f8\u6bd4\u4f20\u7edf\u6a21\u578b\u7ea7\u65b9\u6cd5\u53ef\u51cf\u5c1140%GPU\u548c35%\u80fd\u8017", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u670d\u52a1\u65b9\u6848\u91c7\u7528\u9759\u6001\u8d44\u6e90\u914d\u7f6e\u6216\u6a21\u578b\u7ea7\u81ea\u52a8\u6269\u7f29\u5bb9\uff0c\u5c06\u6a21\u578b\u89c6\u4e3a\u6574\u4f53\u8fdb\u884c\u7c97\u7c92\u5ea6\u7ba1\u7406\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u6216\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u63a8\u7406\u6d41\u91cf", "method": "\u901a\u8fc7\u5206\u6790\u751f\u6210\u5f0f\u6a21\u578b\u5185\u90e8\u7b97\u5b50\u56fe\u7ed3\u6784\uff0c\u53d1\u73b0\u7b97\u5b50\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u5360\u7528\u4e0a\u5177\u6709\u5f02\u8d28\u6027\uff0c\u63d0\u51fa\u7b97\u5b50\u7ea7\u81ea\u52a8\u6269\u7f29\u5bb9\u6846\u67b6\uff0c\u57fa\u4e8e\u4e2a\u4f53\u7b97\u5b50\u7279\u5f81\u8fdb\u884c\u6269\u7f29\u5bb9\u3001\u6279\u5904\u7406\u548c\u8d44\u6e90\u5206\u914d\u4f18\u5316", "result": "\u5728\u751f\u4ea7\u89c4\u6a21\u6d4b\u8bd5\u4e2d\uff0c\u5728\u4fdd\u6301SLO\u7684\u524d\u63d0\u4e0b\u51cf\u5c1140%GPU\u548c35%\u80fd\u8017\uff0c\u6216\u5728\u56fa\u5b9a\u8d44\u6e90\u4e0b\u5b9e\u73b01.6\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c5%\u80fd\u8017\u964d\u4f4e", "conclusion": "\u7b97\u5b50\u6bd4\u6a21\u578b\u66f4\u9002\u5408\u4f5c\u4e3a\u5927\u751f\u6210\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6269\u7f29\u5bb9\u57fa\u672c\u5355\u5143\uff0c\u7ec6\u7c92\u5ea6\u7b97\u5b50\u7ea7\u7ba1\u7406\u80fd\u663e\u8457\u63d0\u5347\u8d44\u6e90\u5229\u7528\u6548\u7387"}}
{"id": "2511.02257", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.02257", "abs": "https://arxiv.org/abs/2511.02257", "authors": ["Oguz Selvitopi", "Emin Ozturk", "Jie Chen", "Ponnuswamy Sadayappan", "Robert G. Edwards", "Ayd\u0131n Bulu\u00e7"], "title": "Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators", "comment": null, "summary": "Computation of correlation functions is a key operation in Lattice quantum\nchromodynamics (LQCD) simulations to extract nuclear physics observables. These\nfunctions involve many binary batch tensor contractions, each tensor possibly\noccupying hundreds of MBs of memory. Performing these contractions on GPU\naccelerators poses the challenge of scheduling them as to optimize tensor reuse\nand reduce data traffic. In this work we propose two fast novel scheduling\nalgorithms that reorder contractions to increase temporal locality via\ninput/intermediate tensor reuse. Our schedulers take advantage of\napplication-specific features, such as contractions being binary and locality\nwithin contraction trees, to optimize the objective of minimizing peak memory.\nWe integrate them into the LQCD analysis software suite Redstar and improve\ntime-to-solution. Our schedulers attain upto 2.1x improvement in peak memory,\nwhich is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data\ntraffic, resulting in upto 1.9x faster correlation function computation time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u8c03\u5ea6\u7b97\u6cd5\u6765\u4f18\u5316LQCD\u6a21\u62df\u4e2d\u7684\u5f20\u91cf\u6536\u7f29\u8ba1\u7b97\uff0c\u901a\u8fc7\u589e\u52a0\u65f6\u95f4\u5c40\u90e8\u6027\u548c\u5f20\u91cf\u91cd\u7528\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u548c\u6570\u636e\u4f20\u8f93\u91cf\uff0c\u63d0\u5347\u4e86\u8ba1\u7b97\u6027\u80fd\u3002", "motivation": "LQCD\u6a21\u62df\u4e2d\u7684\u5173\u8054\u51fd\u6570\u8ba1\u7b97\u6d89\u53ca\u5927\u91cf\u4e8c\u8fdb\u5236\u6279\u91cf\u5f20\u91cf\u6536\u7f29\uff0c\u6bcf\u4e2a\u5f20\u91cf\u53ef\u80fd\u5360\u7528\u6570\u767eMB\u5185\u5b58\u3002\u5728GPU\u52a0\u901f\u5668\u4e0a\u6267\u884c\u8fd9\u4e9b\u6536\u7f29\u65f6\uff0c\u9700\u8981\u4f18\u5316\u8c03\u5ea6\u4ee5\u63d0\u9ad8\u5f20\u91cf\u91cd\u7528\u5e76\u51cf\u5c11\u6570\u636e\u6d41\u91cf\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u5feb\u901f\u8c03\u5ea6\u7b97\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u6392\u5e8f\u6536\u7f29\u64cd\u4f5c\u6765\u589e\u52a0\u65f6\u95f4\u5c40\u90e8\u6027\uff0c\u5229\u7528\u8f93\u5165/\u4e2d\u95f4\u5f20\u91cf\u91cd\u7528\u3002\u7b97\u6cd5\u5229\u7528\u4e86\u5e94\u7528\u7279\u5b9a\u7279\u5f81\uff0c\u5982\u6536\u7f29\u7684\u4e8c\u5143\u6027\u548c\u6536\u7f29\u6811\u4e2d\u7684\u5c40\u90e8\u6027\uff0c\u4ee5\u6700\u5c0f\u5316\u5cf0\u503c\u5185\u5b58\u4e3a\u76ee\u6807\u3002", "result": "\u8c03\u5ea6\u5668\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.1\u500d\u7684\u5cf0\u503c\u5185\u5b58\u6539\u8fdb\uff0c\u51cf\u5c11\u4e86\u9ad8\u8fbe4.2\u500d\u7684\u9a71\u9010\u6b21\u6570\u548c\u9ad8\u8fbe1.8\u500d\u7684\u6570\u636e\u6d41\u91cf\uff0c\u4f7f\u5173\u8054\u51fd\u6570\u8ba1\u7b97\u65f6\u95f4\u52a0\u5feb\u4e86\u9ad8\u8fbe1.9\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8c03\u5ea6\u7b97\u6cd5\u6210\u529f\u96c6\u6210\u5230LQCD\u5206\u6790\u8f6f\u4ef6Redstar\u4e2d\uff0c\u663e\u8457\u6539\u5584\u4e86\u6c42\u89e3\u65f6\u95f4\uff0c\u8bc1\u660e\u4e86\u5728\u4f18\u5316\u5f20\u91cf\u6536\u7f29\u8ba1\u7b97\u8c03\u5ea6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.02293", "categories": ["cs.DC", "cs.CV", "C.2.4; I.2.10"], "pdf": "https://arxiv.org/pdf/2511.02293", "abs": "https://arxiv.org/abs/2511.02293", "authors": ["Taisuke Noguchi", "Takuya Azumi"], "title": "3D Point Cloud Object Detection on Edge Devices for Split Computing", "comment": "6 pages. This version includes minor lstlisting configuration\n  adjustments for successful compilation. No changes to content or layout.\n  Originally published at ACM/IEEE RAGE 2024", "summary": "The field of autonomous driving technology is rapidly advancing, with deep\nlearning being a key component. Particularly in the field of sensing, 3D point\ncloud data collected by LiDAR is utilized to run deep neural network models for\n3D object detection. However, these state-of-the-art models are complex,\nleading to longer processing times and increased power consumption on edge\ndevices. The objective of this study is to address these issues by leveraging\nSplit Computing, a distributed machine learning inference method. Split\nComputing aims to lessen the computational burden on edge devices, thereby\nreducing processing time and power consumption. Furthermore, it minimizes the\nrisk of data breaches by only transmitting intermediate data from the deep\nneural network model. Experimental results show that splitting after\nvoxelization reduces the inference time by 70.8% and the edge device execution\ntime by 90.0%. When splitting within the network, the inference time is reduced\nby up to 57.1%, and the edge device execution time is reduced by up to 69.5%.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u5206\u5272\u8ba1\u7b97\u6280\u672f\u6765\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u4e2d\u76843D\u7269\u4f53\u68c0\u6d4b\uff0c\u901a\u8fc7\u5728\u70b9\u4e91\u4f53\u7d20\u5316\u540e\u6216\u7f51\u7edc\u5185\u90e8\u8fdb\u884c\u5206\u5272\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8fb9\u7f18\u8bbe\u5907\u7684\u63a8\u7406\u65f6\u95f4\u548c\u80fd\u8017\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u4e2d\u76843D\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\u590d\u6742\uff0c\u5bfc\u81f4\u8fb9\u7f18\u8bbe\u5907\u5904\u7406\u65f6\u95f4\u957f\u3001\u529f\u8017\u9ad8\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5272\u8ba1\u7b97\uff08Split Computing\uff09\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u63a8\u7406\u65b9\u6cd5\uff0c\u5728\u70b9\u4e91\u4f53\u7d20\u5316\u540e\u6216\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u8fdb\u884c\u5206\u5272\uff0c\u51cf\u8f7b\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u8d1f\u62c5\u3002", "result": "\u4f53\u7d20\u5316\u540e\u5206\u5272\uff1a\u63a8\u7406\u65f6\u95f4\u51cf\u5c1170.8%\uff0c\u8fb9\u7f18\u8bbe\u5907\u6267\u884c\u65f6\u95f4\u51cf\u5c1190.0%\uff1b\u7f51\u7edc\u5185\u90e8\u5206\u5272\uff1a\u63a8\u7406\u65f6\u95f4\u6700\u591a\u51cf\u5c1157.1%\uff0c\u8fb9\u7f18\u8bbe\u5907\u6267\u884c\u65f6\u95f4\u6700\u591a\u51cf\u5c1169.5%\u3002", "conclusion": "\u5206\u5272\u8ba1\u7b97\u80fd\u6709\u6548\u964d\u4f4e\u81ea\u52a8\u9a7e\u9a763D\u7269\u4f53\u68c0\u6d4b\u7684\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u8d1f\u62c5\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u5e76\u964d\u4f4e\u80fd\u8017\uff0c\u540c\u65f6\u901a\u8fc7\u4ec5\u4f20\u8f93\u4e2d\u95f4\u6570\u636e\u589e\u5f3a\u6570\u636e\u5b89\u5168\u6027\u3002"}}
{"id": "2511.02647", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02647", "abs": "https://arxiv.org/abs/2511.02647", "authors": ["Xiumei Deng", "Zehui Xiong", "Binbin Chen", "Dong In Kim", "Merouane Debbah", "H. Vincent Poor"], "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks", "comment": null, "summary": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments.", "AI": {"tldr": "FedAttn\u662f\u4e00\u4e2a\u65b0\u7684\u5206\u5e03\u5f0fLLM\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8054\u90a6\u5b66\u4e60\u8303\u5f0f\u96c6\u6210\u5230\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u901a\u4fe1\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u8fb9\u7f18\u534f\u4f5c\u90e8\u7f72\u4e2d\u7684\u9690\u79c1\u6f0f\u6d1e\u3001\u901a\u4fe1\u5f00\u9500\u548c\u8ba1\u7b97\u74f6\u9888\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u8054\u90a6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8ba9\u53c2\u4e0e\u8005\u5728\u672c\u5730\u6267\u884c\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u5b9a\u671f\u4ea4\u6362\u548c\u805a\u5408KV\u77e9\u9635\uff0c\u534f\u4f5c\u751f\u6210LLM\u54cd\u5e94\u800c\u4e0d\u66b4\u9732\u79c1\u6709\u63d0\u793a\u3002", "result": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u9519\u8bef\u4f20\u64ad\u52a8\u6001\u548c\u54cd\u5e94\u8d28\u91cf\u4e0e\u901a\u4fe1/\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u7a00\u758f\u6ce8\u610f\u529b\u548c\u81ea\u9002\u5e94KV\u805a\u5408\u7684\u4f18\u5316\u673a\u4f1a\u3002", "conclusion": "FedAttn\u5177\u6709\u5728\u5b9e\u9645\u8fb9\u7f18\u90e8\u7f72\u4e2d\u63d0\u4f9b\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u7684\u6f5c\u529b\uff0c\u4e3a\u534f\u4f5cLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002"}}
{"id": "2511.02655", "categories": ["cs.DC", "cs.MS"], "pdf": "https://arxiv.org/pdf/2511.02655", "abs": "https://arxiv.org/abs/2511.02655", "authors": ["Johansell Villalobos", "Josef Ruzicka", "Silvio Rizzi"], "title": "Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks", "comment": null, "summary": "Scientific computing in the exascale era demands increased computational\npower to solve complex problems across various domains. With the rise of\nheterogeneous computing architectures the need for vendor-agnostic, performance\nportability frameworks has been highlighted. Libraries like Kokkos have become\nessential for enabling high-performance computing applications to execute\nefficiently across different hardware platforms with minimal code changes. In\nthis direction, this paper presents preliminary time-to-solution results for\ntwo representative scientific computing applications: an N-body simulation and\na structured grid simulation. Both applications used a distributed memory\napproach and hardware acceleration through four performance portability\nframeworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single\nnode of the Polaris supercomputer using four NVIDIA A100 GPUs revealed\nsignificant performance variability among frameworks. OCCA demonstrated faster\nexecution times for small-scale validation problems, likely due to JIT\ncompilation, however its lack of optimized reduction algorithms may limit\nscalability for larger simulations while using its out of the box API. OpenMP\nperformed poorly in the structured grid simulation most likely due to\ninefficiencies in inter-node data synchronization and communication. These\nfindings highlight the need for further optimization to maximize each\nframework's capabilities. Future work will focus on enhancing reduction\nalgorithms, data communication, memory management, as wells as performing\nscalability studies, and a comprehensive statistical analysis to evaluate and\ncompare framework performance.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u56db\u79cd\u6027\u80fd\u53ef\u79fb\u690d\u6027\u6846\u67b6\uff08Kokkos\u3001OpenMP\u3001RAJA\u3001OCCA\uff09\u5728\u79d1\u5b66\u8ba1\u7b97\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u6846\u67b6\u5728N\u4f53\u6a21\u62df\u548c\u7ed3\u6784\u5316\u7f51\u683c\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u5f02\u6784\u8ba1\u7b97\u67b6\u6784\u7684\u5174\u8d77\uff0c\u79d1\u5b66\u8ba1\u7b97\u9700\u8981\u80fd\u591f\u5728\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\u9ad8\u6548\u6267\u884c\u7684\u4f9b\u5e94\u5546\u65e0\u5173\u6027\u80fd\u53ef\u79fb\u690d\u6846\u67b6\u3002", "method": "\u5728Polaris\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u4f7f\u7528\u56db\u4e2aNVIDIA A100 GPU\uff0c\u5bf9N\u4f53\u6a21\u62df\u548c\u7ed3\u6784\u5316\u7f51\u683c\u6a21\u62df\u8fdb\u884c\u5206\u5e03\u5f0f\u5185\u5b58\u65b9\u6cd5\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u56db\u79cd\u6846\u67b6\u7684\u65f6\u95f4\u5230\u89e3\u51b3\u65b9\u6848\u6027\u80fd\u3002", "result": "OCCA\u5728\u5c0f\u89c4\u6a21\u9a8c\u8bc1\u95ee\u9898\u4e0a\u6267\u884c\u66f4\u5feb\uff08\u53ef\u80fd\u7531\u4e8eJIT\u7f16\u8bd1\uff09\uff0c\u4f46\u7f3a\u4e4f\u4f18\u5316\u7684\u5f52\u7ea6\u7b97\u6cd5\uff1bOpenMP\u5728\u7ed3\u6784\u5316\u7f51\u683c\u6a21\u62df\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u8282\u70b9\u95f4\u6570\u636e\u540c\u6b65\u548c\u901a\u4fe1\u6548\u7387\u4f4e\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u5404\u6846\u67b6\u7684\u5f52\u7ea6\u7b97\u6cd5\u3001\u6570\u636e\u901a\u4fe1\u548c\u5185\u5b58\u7ba1\u7406\uff0c\u5e76\u8fdb\u884c\u53ef\u6269\u5c55\u6027\u7814\u7a76\u548c\u7edf\u8ba1\u5206\u6790\u6765\u5168\u9762\u8bc4\u4f30\u6846\u67b6\u6027\u80fd\u3002"}}
{"id": "2511.02743", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.02743", "abs": "https://arxiv.org/abs/2511.02743", "authors": ["Fedor Ryabinin", "Alexey Gotsman", "Pierre Sutra"], "title": "Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)", "comment": "Extended version of a paper in OPODIS'25: International Conference on\n  Principles of Distributed Systems", "summary": "Classical state-machine replication protocols, such as Paxos, rely on a\ndistinguished leader process to order commands. Unfortunately, this approach\nmakes the leader a single point of failure and increases the latency for\nclients that are not co-located with it. As a response to these drawbacks,\nEgalitarian Paxos introduced an alternative, leaderless approach, that allows\nreplicas to order commands collaboratively. Not relying on a single leader\nallows the protocol to maintain non-zero throughput with up to $f$ crashes of\nany processes out of a total of $n = 2f+1$. The protocol furthermore allows any\nprocess to execute a command $c$ fast, in $2$ message delays, provided no more\nthan $e = \\lceil\\frac{f+1}{2}\\rceil$ other processes fail, and all concurrently\nsubmitted commands commute with $c$; the latter condition is often satisfied in\npractical systems.\n  Egalitarian Paxos has served as a foundation for many other replication\nprotocols. But unfortunately, the protocol is very complex, ambiguously\nspecified and suffers from nontrivial bugs. In this paper, we present EPaxos*\n-- a simpler and correct variant of Egalitarian Paxos. Our key technical\ncontribution is a simpler failure-recovery algorithm, which we have rigorously\nproved correct. Our protocol also generalizes Egalitarian Paxos to cover the\nwhole spectrum of failure thresholds $f$ and $e$ such that $n \\ge \\max\\{2e+f-1,\n2f+1\\}$ -- the number of processes that we show to be optimal.", "AI": {"tldr": "EPaxos*\u662f\u4e00\u4e2a\u66f4\u7b80\u5355\u4e14\u6b63\u786e\u7684Egalitarian Paxos\u53d8\u4f53\uff0c\u901a\u8fc7\u7b80\u5316\u7684\u6545\u969c\u6062\u590d\u7b97\u6cd5\u89e3\u51b3\u4e86\u539f\u534f\u8bae\u7684\u590d\u6742\u6027\u548c\u9519\u8bef\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u5230\u6700\u4f18\u7684\u8fdb\u7a0b\u6570\u91cf\u914d\u7f6e\u3002", "motivation": "\u7ecf\u5178\u7684Paxos\u7b49\u72b6\u6001\u673a\u590d\u5236\u534f\u8bae\u4f9d\u8d56\u5355\u4e00\u9886\u5bfc\u8005\uff0c\u5b58\u5728\u5355\u70b9\u6545\u969c\u548c\u5ef6\u8fdf\u95ee\u9898\u3002Egalitarian Paxos\u5f15\u5165\u4e86\u65e0\u9886\u5bfc\u8005\u65b9\u6cd5\u4f46\u534f\u8bae\u590d\u6742\u3001\u89c4\u8303\u6a21\u7cca\u4e14\u5b58\u5728\u4e25\u91cd\u9519\u8bef\u3002", "method": "\u63d0\u51fa\u4e86EPaxos*\u534f\u8bae\uff0c\u5173\u952e\u8d21\u732e\u662f\u7b80\u5316\u7684\u6545\u969c\u6062\u590d\u7b97\u6cd5\uff0c\u5e76\u6269\u5c55\u5230\u8986\u76d6\u6700\u4f18\u7684\u6545\u969c\u9608\u503c\u8303\u56f4\uff1an \u2265 max{2e+f-1, 2f+1}\u3002", "result": "EPaxos*\u4fdd\u6301\u4e86Egalitarian Paxos\u7684\u4f18\u52bf\uff1a\u5728\u6700\u591af\u4e2a\u8fdb\u7a0b\u6545\u969c\u65f6\u4fdd\u6301\u975e\u96f6\u541e\u5410\u91cf\uff0c\u5728\u4e0d\u8d85\u8fc7e\u4e2a\u8fdb\u7a0b\u6545\u969c\u4e14\u547d\u4ee4\u53ef\u4ea4\u6362\u65f6\u5b9e\u73b02\u6d88\u606f\u5ef6\u8fdf\u7684\u5feb\u901f\u6267\u884c\u3002", "conclusion": "EPaxos*\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u7b80\u5355\u3001\u6b63\u786e\u4e14\u6700\u4f18\u7684Egalitarian Paxos\u53d8\u4f53\uff0c\u89e3\u51b3\u4e86\u539f\u534f\u8bae\u7684\u590d\u6742\u6027\u548c\u6b63\u786e\u6027\u95ee\u9898\u3002"}}
