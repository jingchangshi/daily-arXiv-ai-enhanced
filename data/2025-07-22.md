<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 8]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 7]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [NPUEval: Optimizing NPU Kernels with LLMs and Open Source Compilers](https://arxiv.org/abs/2507.14403)
*Sarunas Kalade,Graham Schelle*

Main category: cs.PL

TL;DR: NPUEval是一个用于评估NPU内核编写的基准测试，包含102个常见机器学习算子，评估LLM生成代码的功能正确性和向量化效率。


<details>
  <summary>Details</summary>
Motivation: NPU编程较新且社区分散，LLM预训练数据中缺乏领域优化代码示例，因此需要基准测试来推动研究。

Method: 使用开源编译器工具在AMD NPU上评估LLM生成代码的功能和向量化效率，测试多种前沿LLM模型。

Result: 前沿模型如DeepSeek R1在部分内核上实现50%+向量化，但整体平均得分仅约10%，表明任务具有挑战性。

Conclusion: NPUEval为代码生成和NPU内核优化研究提供了重要基准，数据集和评估代码将开源。

Abstract: Neural processing units (NPUs) are gaining prominence in power-sensitive
devices like client devices, with AI PCs being defined by their inclusion of
these specialized processors. Running AI workloads efficiently on these devices
requires libraries of optimized kernels. Creating efficient kernels demands
expertise in domain-specific C++ with vector intrinsics and in-depth knowledge
of the target architecture. Unlike GPU programming, which has had years to
mature, NPU programming is new, with smaller and more fragmented developer
communities across hardware platforms. This fragmentation poses a challenge
when utilizing LLMs to assist in writing NPU kernels, as domain-specific
optimized code examples are underrepresented in LLM pre-training data.
  In this paper we introduce NPUEval -- a benchmark for writing and evaluating
NPU kernels, consisting of 102 common operators for machine learning workloads.
We evaluate LLM generated code on actual hardware based on both functional
correctness and vectorization efficiency using open source compiler tools
targeting the AMD NPU. We evaluate a range of state-of-the-art LLMs with a mix
of proprietary and open-weight models. Latest reasoning models like DeepSeek
R1, show promising results achieving out-of-the-box 50%+ vectorization on
select kernels. However, the average score across the entire dataset remains
roughly 10% even with compiler feedback and vectorized kernel examples --
showing that this is a challenging dataset even for frontier models. The
dataset and evaluation code will be released with a permissive open source
license, providing an essential benchmark for advancing research in code
generation and NPU kernel optimization.

</details>


### [2] [Timetide: A programming model for logically synchronous distributed systems](https://arxiv.org/abs/2507.14471)
*Logan Kenwright,Partha Roop,Nathan Allen,Călin Caşcaval,Avinash Malik*

Main category: cs.PL

TL;DR: 提出了一种名为Timetide的多时钟同步语言，解决了分布式系统中确定性编程的挑战，无需物理时钟同步。


<details>
  <summary>Details</summary>
Motivation: 传统同步语言依赖昂贵的物理时钟同步，难以扩展，分布式系统的确定性编程仍具挑战性。

Method: 开发了一种多时钟语义的同步程序模型，基于逻辑同步模型，避免了物理时钟同步需求。

Result: Timetide是首个既适合分布式又支持形式化验证的多时钟同步语言。

Conclusion: Timetide为分布式系统提供了一种无需物理时钟同步的确定性编程解决方案。

Abstract: Massive strides in deterministic models have been made using synchronous
languages. They are mainly focused on centralised applications, as the
traditional approach is to compile away the concurrency. Time triggered
languages such as Giotto and Lingua Franca are suitable for distribution albeit
that they rely on expensive physical clock synchronisation, which is both
expensive and may suffer from scalability. Hence, deterministic programming of
distributed systems remains challenging. We address the challenges of
deterministic distribution by developing a novel multiclock semantics of
synchronous programs. The developed semantics is amenable to seamless
distribution. Moreover, our programming model, Timetide, alleviates the need
for physical clock synchronisation by building on the recently proposed logical
synchrony model for distributed systems. We discuss the important aspects of
distributing computation, such as network communication delays, and explore the
formal verification of Timetide programs. To the best of our knowledge,
Timetide is the first multiclock synchronous language that is both amenable to
distribution and formal verification without the need for physical clock
synchronisation or clock gating.

</details>


### [3] [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
*Sayed Mahbub Hasan Amiri,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Mohammad Shawkat Ali Mamun,Sk. Humaun Kabir,Naznin Akter*

Main category: cs.PL

TL;DR: 该研究提出了一种创新的Python语音辅助调试插件，通过听觉和视觉多模态反馈显著降低认知负荷并加快错误识别。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统调试方法对视觉的依赖，提升编程可访问性，特别是对视觉障碍者和新手程序员。

Method: 采用全局异常钩子架构，结合pyttsx3文本转语音和Tkinter GUI可视化，提供并行听觉和视觉错误反馈。

Result: 实验显示认知负荷降低37%，错误识别速度提升78%，系统延迟低于1.2秒，CPU开销小于18%。

Conclusion: 该插件显著提升了调试效率和可访问性，未来将整合GPT修复建议和多语言翻译，推动听觉调试范式发展。

Abstract: This research introduces an innovative voice-assisted debugging plugin for
Python that transforms silent runtime errors into actionable audible
diagnostics. By implementing a global exception hook architecture with pyttsx3
text-to-speech conversion and Tkinter-based GUI visualization, the solution
delivers multimodal error feedback through parallel auditory and visual
channels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,
n=50) compared to traditional stack-trace debugging, while enabling 78% faster
error identification through vocalized exception classification and
contextualization. The system achieves sub-1.2 second voice latency with under
18% CPU overhead during exception handling, vocalizing error types and
consequences while displaying interactive tracebacks with documentation deep
links. Criteria validate compatibility across Python 3.7+ environments on
Windows, macOS, and Linux platforms. Needing only two lines of integration
code, the plugin significantly boosts availability for aesthetically impaired
designers and supports multitasking workflows through hands-free error medical
diagnosis. Educational applications show particular promise, with pilot studies
indicating 45% faster debugging skill acquisition among novice programmers.
Future development will incorporate GPT-based repair suggestions and real-time
multilingual translation to further advance auditory debugging paradigms. The
solution represents a fundamental shift toward human-centric error diagnostics,
bridging critical gaps in programming accessibility while establishing new
standards for cognitive efficiency in software development workflows.

</details>


### [4] [Invariant Generation for Floating-Point Programs via Constraint Solving](https://arxiv.org/abs/2507.15017)
*Xuran Cai,Liqian Chen,Hongfei Fu*

Main category: cs.PL

TL;DR: 本文提出了一种理论框架，结合约束求解方法和微分特性，用于生成浮点程序的紧致不变量，并设计了两种多项式不变量生成算法。实验表明，该方法在时间和精度上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 浮点运算中的舍入误差累积可能导致程序失败，因此需要生成考虑浮点误差的紧致不变量以确保程序正确性。

Method: 结合FPTaylor的一阶微分特性和约束求解方法，提出两种多项式不变量生成算法：一种需要初始不变量输入，另一种适用于多项式程序。

Result: 实验结果显示，该方法在多种基准测试中，生成的不变量在时间和精度上优于现有技术。

Conclusion: 提出的框架和算法有效解决了浮点程序不变量生成问题，尤其在处理条件分支时表现优异。

Abstract: In numeric-intensive computations, it is well known that the execution of
floating-point programs is imprecise as floating point arithmetics (e.g.,
addition, subtraction, multiplication, division, etc.) incurs rounding errors.
Albeit the rounding error is small for every single floating-point operation,
the aggregation of such error in multiple operations may be dramatic and cause
catastrophic program failures. Therefore, to ensure the correctness of
floating-point programs, the effect of floating point error needs to be
carefully taken into account. In this work, we consider the invariant
generation for floating point programs, whose aim is to generate tight
invariants under the perturbation of floating point errors. Our main
contribution is a theoretical framework on how to apply constraint solving
methods to address the invariant generation problem. In our framework, we
propose a novel combination between the first-order differential
characterization by FPTaylor (TOPLAS 2018) and constraint solving methods,
aiming to reduce the computational burden of constraint solving. Moreover, we
devise two polynomial invariant generation algorithms to instantiate the
framework. The first algorithm is applicable to a wide range of floating-point
operations but requires an initial (coarse) invariant as external input, while
the second does not require an initial invariant but is limited to polynomial
programs. Furthermore, we show how conditional branches, a difficult issue in
floating-point analysis, can be handled in our framework. Experimental results
show that our algorithms outperform SOTA approaches in both the time efficiency
and the precision of the generated invariants over a variety of benchmarks.

</details>


### [5] [A Few Fit Most: Improving Performance Portability of SGEMM on GPUs using Multi-Versioning](https://arxiv.org/abs/2507.15277)
*Robert Hochgraf,Sreepathi Pai*

Main category: cs.PL

TL;DR: 论文提出了一种多版本化框架（portability tuning），用于生成性能可移植的代码，避免了传统自动调优（autotuning）因环境变化而需要重新调优的问题。


<details>
  <summary>Details</summary>
Motivation: 手动优化线性代数核在不同GPU设备和应用上复杂且耗时，而自动调优容易过拟合且需重复调优。

Method: 采用多版本化技术，生成多个代码变体，并通过框架自动选择最优版本，实现性能可移植。

Result: 在CLBlast库的GEMM核上测试，性能接近理论最大值（10%以内），且能泛化到新设备。

Conclusion: 多版本化框架在性能可移植性上优于传统自动调优，无需为新设备重新调优。

Abstract: Hand-optimizing linear algebra kernels for different GPU devices and
applications is complex and labor-intensive. Instead, many developers use
automatic performance tuning (autotuning) to achieve high performance on a
variety of devices. However, autotuning "overfits", and must be redone if any
part of the environment changes, such as if the device or input characteristics
change.
  In most non-trivial cases, a single compute kernel cannot maintain
near-optimal performance across all environments. Changing the kernel to
specialize it to the current execution environment is possible, but on GPUs,
runtime tuning and compilation can be expensive.
  In this work, we use multi-versioning -- producing several variants of the
same code -- as a way to generate performance portable code. We describe a
framework called portability tuning that can automatically generate
multi-versioned code whose performance is portable, requiring no retuning.
  We evaluate our framework on a dataset of execution times for GEMM kernels
from the CLBlast linear algebra library. We find our portability tuning
techniques outperform CLBlast's default kernels -- often approaching within 10%
of the theoretical maximum performance -- despite CLBlast using autotuning
techniques. Further, we find that our generated programs generalize well to new
and unseen devices, matching the performance of autotuning without ever
portability tuning for those devices.

</details>


### [6] [Bayesian Separation Logic](https://arxiv.org/abs/2507.15530)
*Shing Hin Ho,Nicolas Wu,Azalea Raad*

Main category: cs.PL

TL;DR: 本文提出了一种新的概率分离逻辑（BaSL），用于处理贝叶斯概率编程语言（BPPLs）中的贝叶斯更新问题，填补了现有分离逻辑的空白。


<details>
  <summary>Details</summary>
Motivation: 现有分离逻辑无法处理贝叶斯更新，而贝叶斯更新是BPPL的核心特征。本文旨在填补这一空白。

Method: 基于Rokhlin-Simmons分解定理，提出BaSL，并利用Kripke资源幺半群和σ-有限测度空间建模。

Result: BaSL能够建模贝叶斯更新、未归一化分布、条件分布等概念，并通过实例验证了其有效性。

Conclusion: BaSL为BPPL提供了语义支持，并成功验证了其在统计模型中的实用性。

Abstract: Bayesian probabilistic programming languages (BPPLs) let users denote
statistical models as code while the interpreter infers the posterior
distribution. The semantics of BPPLs are usually mathematically complex and
unable to reason about desirable properties such as expected values and
independence of random variables. To reason about these properties in a
non-Bayesian setting, probabilistic separation logics such as PSL and Lilac
interpret separating conjunction as probabilistic independence of random
variables. However, no existing separation logic can handle Bayesian updating,
which is the key distinguishing feature of BPPLs.
  To close this gap, we introduce Bayesian separation logic (BaSL), a
probabilistic separation logic that gives semantics to BPPL. We prove an
internal version of Bayes' theorem using a result in measure theory known as
the Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model
probabilistic programming concepts such as Bayesian updating, unnormalised
distribution, conditional distribution, soft constraint, conjugate prior and
improper prior while maintaining modularity via the frame rule. The model of
BaSL is based on a novel instantiation of Kripke resource monoid via
$\sigma$-finite measure spaces over the Hilbert cube, and the semantics of
Hoare triple is compatible with an existing denotational semantics of BPPL
based on the category of $s$-finite kernels. Using BaSL, we then prove
properties of statistical models such as the expected value of Bayesian coin
flip, correlation of random variables in the collider Bayesian network, and the
posterior distributions of the burglar alarm model, a parameter estimation
algorithm, and the Gaussian mixture model.

</details>


### [7] [Formal Analysis of Networked PLC Controllers Interacting with Physical Environments](https://arxiv.org/abs/2507.15596)
*Jaeseo Lee,Kyungmin Bae*

Main category: cs.PL

TL;DR: 提出了一种统一的形式化框架，用于分析PLC驱动的系统，结合离散PLC语义、网络通信和连续物理行为，并采用偏序减少技术缓解状态爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 现有形式化验证技术通常孤立分析单个PLC程序，忽略了与物理环境和网络通信的交互，难以应对实际工业系统中的连续动态和通信延迟。

Method: 提出一个统一框架，整合离散PLC语义、网络通信和连续物理行为，并应用偏序减少技术以减少状态空间。

Result: 框架能够精确分析具有连续动态和网络通信的PLC驱动系统，同时通过偏序减少显著降低状态数量。

Conclusion: 该框架为复杂工业系统的形式化验证提供了有效方法，解决了现有技术的局限性。

Abstract: Programmable Logic Controllers (PLCs) are widely used in industrial
automation to control physical systems. As PLC applications become increasingly
complex, ensuring their correctness is crucial. Existing formal verification
techniques focus on individual PLC programs in isolation, often neglecting
interactions with physical environments and network communication between
controllers. This limitation poses significant challenges in analyzing
real-world industrial systems, where continuous dynamics and communication
delays play a critical role. In this paper, we present a unified formal
framework that integrates discrete PLC semantics, networked communication, and
continuous physical behaviors. To mitigate state explosion, we apply partial
order reduction, significantly reducing the number of explored states while
maintaining correctness. Our framework enables precise analysis of PLC-driven
systems with continuous dynamics and networked communication.

</details>


### [8] [Closure Conversion, Flat Environments, and the Complexity of Abstract Machines](https://arxiv.org/abs/2507.15843)
*Beniamino Accattoli,Dan Ghica,Giulio Guerrieri,Cláudio Belo Lourenço,Claudio Sacerdoti Coen*

Main category: cs.PL

TL;DR: 论文研究了闭包转换与抽象机器之间的关系，提出了新的证明技术、环境处理方法和时间复杂性分析，表明闭包转换不影响整体复杂性。


<details>
  <summary>Details</summary>
Motivation: 探讨闭包转换与抽象机器中闭包和环境概念的关系，以改进现有方法。

Method: 使用简单的λ演算和元组作为源语言，研究闭包转换前后的抽象机器，重点关注扁平闭包/环境。

Result: 提出了新的闭包转换正确性证明技术、改进的环境处理方法，并证明闭包转换不改变整体复杂性。

Conclusion: 闭包转换虽动态成本降低且代码增大，但整体复杂性保持不变，为编译器优化提供了新视角。

Abstract: Closure conversion is a program transformation at work in compilers for
functional languages to turn inner functions into global ones, by building
closures pairing the transformed functions with the environment of their free
variables. Abstract machines rely on similar and yet different concepts of
closures and environments.
  In this paper, we study the relationship between the two approaches. We adopt
a very simple {\lambda}-calculus with tuples as source language and study
abstract machines for both the source language and the target of closure
conversion. Moreover, we focus on the simple case of flat
closures/environments, that is, with no sharing of environments. We provide
three contributions.
  Firstly, a new simple proof technique for the correctness of closure
conversion, inspired by abstract machines.
  Secondly, we show how the closure invariants of the target language allow us
to design a new way of handling environments in abstract machines, not
suffering the shortcomings of other styles.
  Thirdly, we study the machines from the point of view of time complexity,
adapting analyses by Accattoli and co-authors. We show that closure conversion
decreases various dynamic costs while increasing the size of the initial code.
Despite these changes, the overall complexity of the machines before and after
closure conversion turns out to be the same.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [9] [Characterizing Communication Patterns in Distributed Large Language Model Inference](https://arxiv.org/abs/2507.14392)
*Lang Xu,Kaushik Kandadi Suresh,Quentin Anthony,Nawras Alnaasan,Dhabaleswar K. Panda*

Main category: cs.DC

TL;DR: 论文研究了分布式LLM推理中的通信动态，分析了不同并行化方法在GPU间的数据交换，提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: 分布式LLM推理中，GPU间通信成为性能瓶颈，影响实际服务质量。

Method: 结合详细的分析测量和预测模型，研究不同并行化配置下的通信行为。

Result: 张量并行带来高网络开销但响应时间短，流水线并行减少数据传输但增加延迟，组合方法需精细调优。

Conclusion: 研究为生产环境中的并行化方案选择提供了实用建议，并指出了优化方向。

Abstract: Large Language Models (LLMs) built on transformer architectures have
transformed natural language processing, achieving remarkable performance
across diverse applications. While distributed inference frameworks enable
practical deployment of these models, inter-GPU communication creates
significant performance constraints that limit service quality in real-world
systems. This paper investigates communication dynamics in distributed LLM
serving-analyzing how various parallelization approaches coordinate data
exchange between GPU workers during inference. We study dense transformer-based
models as representative examples of contemporary architectures widely used in
operational deployments. Our work combines detailed profiling measurements with
predictive analytical models to characterize communication behavior across
different parallelization configurations. Results show that tensor parallelism
incurs substantial network overhead but delivers superior response times for
brief sequences, pipeline parallelism minimizes data transfer requirements
while increasing total latency, and combined approaches demand careful tuning
to achieve balanced performance. These insights offer practical recommendations
for selecting appropriate parallelization schemes in production LLM services
and identify key opportunities for optimizing inference frameworks and
communication infrastructure.

</details>


### [10] [Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning](https://arxiv.org/abs/2507.14597)
*Eugene Armah,Linda Amoako Bannning*

Main category: cs.DC

TL;DR: 论文提出了一种三步解决方案，用于主动边缘流处理自动扩展问题，包括GRU神经网络预测上游负载、迁移学习框架处理离线与在线域差异，以及基于预测负载的动态水平扩展模块。


<details>
  <summary>Details</summary>
Motivation: 随着数字经济发展，高速数据处理需求增加，边缘流处理面临资源动态分配的挑战，现有方法存在滞后或浪费问题。

Method: 使用GRU神经网络预测负载，迁移学习框架整合预测模型，动态水平扩展模块调整并行度。

Result: GRU模型在真实数据集上SMAPE值低至1.3%，优于CNN、ARIMA和Prophet，训练时间短于强化学习模型。

Conclusion: 提出的方法有效解决了边缘流处理的资源动态分配问题，具有高预测精度和低计算成本。

Abstract: Processing data at high speeds is becoming increasingly critical as digital
economies generate enormous data. The current paradigms for timely data
processing are edge computing and data stream processing (DSP). Edge computing
places resources closer to where data is generated, while stream processing
analyzes the unbounded high-speed data in motion. However, edge stream
processing faces rapid workload fluctuations, complicating resource
provisioning. Inadequate resource allocation leads to bottlenecks, whereas
excess allocation results in wastage. Existing reactive methods, such as
threshold-based policies and queuing theory scale only after performance
degrades, potentially violating SLAs. Although reinforcement learning (RL)
offers a proactive approach through agents that learn optimal runtime
adaptation policies, it requires extensive simulation. Furthermore, predictive
machine learning models face online distribution and concept drift that
minimize their accuracy. We propose a three-step solution to the proactive edge
stream processing autoscaling problem. Firstly, a GRU neural network forecasts
the upstream load using real-world and synthetic DSP datasets. Secondly, a
transfer learning framework integrates the predictive model into an online
stream processing system using the DTW algorithm and joint distribution
adaptation to handle the disparities between offline and online domains.
Finally, a horizontal autoscaling module dynamically adjusts the degree of
operator parallelism, based on predicted load while considering edge resource
constraints. The lightweight GRU model for load predictions recorded up to
1.3\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and
Prophet on the SMAPE and RMSE evaluation metrics, with lower training time than
the computationally intensive RL models.

</details>


### [11] [Simulating Chirality: Solving Distance-$k$-Dispersion on an 1-Interval Connected Ring](https://arxiv.org/abs/2507.14723)
*Brati Mondal,Pritam Goswami,Buddhadeb Sau*

Main category: cs.DC

TL;DR: 论文研究了在1-区间连通环网络中同步移动代理的Distance-$k$-Dispersion问题，提出了一种无需共同方向感（chirality）的解决方案，并扩展了理论理解。


<details>
  <summary>Details</summary>
Motivation: 解决在无共同方向感的动态网络中移动代理的协调问题，特别是D-$k$-D问题，填补了现有研究的空白。

Method: 提出了一种利用局部信息、视觉和有限内存模拟共同方向感的方法，并设计了解决D-$k$-D问题的算法。

Result: 证明了D-$k$-D问题在任意环网络中可解，并提出了一个时间复杂度为$O(ln)$的算法。

Conclusion: 研究显著扩展了对动态网络中移动代理协调的理论理解，并明确了共同方向感在分布式计算中的作用。

Abstract: We study the Distance-$k$-Dispersion (D-$k$-D) problem for synchronous mobile
agents in a 1-interval-connected ring network having $n$ nodes and with $l$
agents where $3 \le l \le \lfloor \frac{n}{k}\rfloor$, without the assumption
of chirality (a common sense of direction for the agents). This generalizes the
classical dispersion problem by requiring that agents maintain a minimum
distance of $k$ hops from each other, with the special case $k=1$ corresponding
to the standard dispersion.
  The contribution in this work is threefold. Our first contribution is a novel
method that enables agents to simulate chirality using only local information,
vision and bounded memory. This technique demonstrates that chirality is not a
fundamental requirement for coordination in this model.
  Building on this, our second contribution partially resolves an open question
posed by Agarwalla et al. (ICDCN, 2018), who considered the same model (1-
interval connected ring, synchronous agents, no chirality). We prove that
D-$k$-D, and thus dispersion is solvable from any arbitrary configuration under
these assumptions (excluding vertex permutation dynamism)for any size of the
ring network which was earlier limited to only odd sized ring or to a ring of
size four.
  Finally, we present an algorithm for D-$k$-D in this setting that works in
$O(ln)$ rounds, completing the constructive side of our result.
  Altogether, our findings significantly extend the theoretical understanding
of mobile agent coordination in dynamic networks and clarify the role of
chirality in distributed computation.

</details>


### [12] [ACME: Adaptive Customization of Large Models via Distributed Systems](https://arxiv.org/abs/2507.14802)
*Ziming Dai,Chao Qiu,Fei Gao,Yunfeng Zhao,Xiaofei Wang*

Main category: cs.DC

TL;DR: ACME提出了一种分布式系统方法，用于自适应定制基于Transformer的大型模型，解决了集中式方法的低效问题，显著减少了数据传输量并提高了性能。


<details>
  <summary>Details</summary>
Motivation: 解决云端部署大型模型时的数据隐私和响应延迟问题，同时应对模型不匹配、资源限制和能源效率低等挑战。

Method: 采用双向单循环分布式系统，逐步实现细粒度协作模型定制，包括主干生成、Pareto前沿识别、头部生成和数据分布驱动的个性化架构聚合。

Result: 在模型大小限制下实现高效模型，数据传输量降至6%，平均准确率提高10%，权衡指标提升近30%。

Conclusion: ACME通过分布式定制方法有效解决了集中式系统的低效问题，显著提升了模型性能和资源利用率。

Abstract: Pre-trained Transformer-based large models have revolutionized personal
virtual assistants, but their deployment in cloud environments faces challenges
related to data privacy and response latency. Deploying large models closer to
the data and users has become a key research area to address these issues.
However, applying these models directly often entails significant difficulties,
such as model mismatching, resource constraints, and energy inefficiency.
Automated design of customized models is necessary, but it faces three key
challenges, namely, the high cost of centralized model customization,
imbalanced performance from user heterogeneity, and suboptimal performance from
data heterogeneity. In this paper, we propose ACME, an adaptive customization
approach of Transformer-based large models via distributed systems. To avoid
the low cost-efficiency of centralized methods, ACME employs a bidirectional
single-loop distributed system to progressively achieve fine-grained
collaborative model customization. In order to better match user heterogeneity,
it begins by customizing the backbone generation and identifying the Pareto
Front under model size constraints to ensure optimal resource utilization.
Subsequently, it performs header generation and refines the model using data
distribution-based personalized architecture aggregation to match data
heterogeneity. Evaluation on different datasets shows that ACME achieves
cost-efficient models under model size constraints. Compared to centralized
systems, data transmission volume is reduced to 6 percent. Additionally, the
average accuracy improves by 10 percent compared to the baseline, with the
trade-off metrics increasing by nearly 30 percent.

</details>


### [13] [Byzantine-Robust Decentralized Coordination of LLM Agents](https://arxiv.org/abs/2507.14928)
*Yongrae Jo,Chanik Park*

Main category: cs.DC

TL;DR: DecentLLMs提出了一种去中心化的多智能体LLM系统共识方法，通过并行生成答案和独立评分来解决传统领导者驱动系统的缺陷。


<details>
  <summary>Details</summary>
Motivation: 单智能体LLM系统存在幻觉和单点故障问题，而现有的多智能体系统容易受到针对领导者的攻击和低质量提案的影响。

Method: DecentLLMs采用去中心化架构，工作智能体并行生成答案，评估智能体独立评分并选择最佳答案。

Result: 实验表明，DecentLLMs能有效容忍拜占庭智能体，并显著提高所选答案的质量。

Conclusion: DecentLLMs为多智能体LLM系统提供了一种高效且鲁棒的共识解决方案。

Abstract: Collaboration among multiple large language model (LLM) agents is a promising
approach to overcome inherent limitations of single-agent systems, such as
hallucinations and single points of failure. As LLM agents are increasingly
deployed on open blockchain platforms, multi-agent systems capable of
tolerating malicious (Byzantine) agents have become essential.
  Recent Byzantine-robust multi-agent systems typically rely on leader-driven
coordination, which suffers from two major drawbacks. First, they are
inherently vulnerable to targeted attacks against the leader. If consecutive
leaders behave maliciously, the system repeatedly fails to achieve consensus,
forcing new consensus rounds, which is particularly costly given the high
latency of LLM invocations. Second, an underperforming proposal from the leader
can be accepted as the final answer even when higher-quality alternatives are
available, as existing methods finalize the leader's proposal once it receives
a quorum of votes.
  To address these issues, we propose DecentLLMs, a novel decentralized
consensus approach for multi-agent LLM systems, where worker agents generate
answers concurrently and evaluator agents independently score and rank these
answers to select the best available one. This decentralized architecture
enables faster consensus despite the presence of Byzantine agents and
consistently selects higher-quality answers through Byzantine-robust
aggregation techniques.
  Experimental results demonstrate that DecentLLMs effectively tolerates
Byzantine agents and significantly improves the quality of selected answers.

</details>


### [14] [AMPED: Accelerating MTTKRP for Billion-Scale Sparse Tensor Decomposition on Multiple GPUs](https://arxiv.org/abs/2507.15121)
*Sasindu Wijeratne,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.DC

TL;DR: AMPED是一种多GPU并行算法，用于加速大规模稀疏张量的MTTKRP计算，解决了内存和性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着稀疏张量规模增长至数十亿非零元素，现有硬件加速器在内存容量和计算吞吐量方面面临挑战。

Method: 采用分区策略和动态负载均衡方案，在多GPU上分配计算任务以减少空闲时间。

Result: 在4个GPU上，AMPED比现有最佳GPU基线实现了5.1倍的几何平均加速。

Conclusion: AMPED成功满足了大规模稀疏张量分解的计算需求，显著提升了性能。

Abstract: Matricized Tensor Times Khatri-Rao Product (MTTKRP) is the computational
bottleneck in sparse tensor decomposition. As real-world sparse tensors grow to
billions of nonzeros, they increasingly demand higher memory capacity and
compute throughput from hardware accelerators. In this work, we present AMPED,
a multi-GPU parallel algorithm designed to accelerate MTTKRP on billion-scale
sparse tensors. AMPED scales beyond the limits of a single GPU, meeting both
the memory and performance requirements of large-scale workloads. We introduce
a partitioning strategy combined with a dynamic load balancing scheme to
distribute computation and minimize GPU idle time. On real-world billion-scale
tensors, AMPED achieves a 5.1x geometric mean speedup in total execution time
over state-of-the-art GPU baselines using 4 GPUs on a single CPU node.

</details>


### [15] [Dynatune: Dynamic Tuning of Raft Election Parameters Using Network Measurement](https://arxiv.org/abs/2507.15154)
*Kohya Shiozaki,Junya Nakamura*

Main category: cs.DC

TL;DR: Dynatune动态调整Raft选举参数以减少服务中断时间。


<details>
  <summary>Details</summary>
Motivation: 传统Raft算法在网络波动时难以有效调整选举参数，导致服务中断时间增加。

Method: Dynatune基于网络指标（如往返时间和丢包率）动态调整选举参数。

Result: 实验显示Dynatune将领导者故障检测和服务中断时间分别减少80%和45%。

Conclusion: Dynatune显著提升了SMR服务在不同网络条件下的性能和可靠性。

Abstract: Raft is a leader-based consensus algorithm that implements State Machine
Replication (SMR), which replicates the service state across multiple servers
to enhance fault tolerance. In Raft, the servers play one of three roles:
leader, follower, or candidate. The leader receives client requests, determines
the processing order, and replicates them to the followers. When the leader
fails, the service must elect a new leader to continue processing requests,
during which the service experiences an out-of-service (OTS) time. The OTS time
is directly influenced by election parameters, such as heartbeat interval and
election timeout. However, traditional approaches, such as Raft, often struggle
to effectively tune these parameters, particularly under fluctuating network
conditions, leading to increased OTS time and reduced service responsiveness.
To address this, we propose Dynatune, a mechanism that dynamically adjusts
Raft's election parameters based on network metrics such as round-trip time and
packet loss rates measured via heartbeats. By adapting to changing network
environments, Dynatune significantly reduces the leader failure detection and
OTS time without altering Raft's core mechanisms or introducing additional
communication overheads. Experimental results demonstrate that Dynatune reduces
the leader failure detection and OTS times by 80% and 45%, respectively,
compared with Raft, while maintaining high availability even under dynamic
network conditions. These findings confirm that Dynatune effectively enhances
the performance and reliability of SMR services in various network scenarios.

</details>


### [16] [GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis](https://arxiv.org/abs/2507.15230)
*Guoxi Liu,Thomas Randall,Rong Ge,Federico Iuricich*

Main category: cs.DC

TL;DR: 提出了一种针对异构CPU-GPU系统的任务并行方法GALE，用于优化非结构化网格的连接信息计算，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 非结构化网格的复杂连接性导致计算和存储瓶颈，现有方法受限于CPU资源竞争。

Method: 将网格连接信息计算卸载到GPU线程，让CPU专注于可视化算法，提出CUDA-based数据架构GALE。

Result: 在20核CPU和NVIDIA V100 GPU上，GALE比现有技术快2.7倍，且内存高效。

Conclusion: GALE通过异构任务并行有效解决了非结构化网格的性能瓶颈。

Abstract: Unstructured meshes present challenges in scientific data analysis due to
irregular distribution and complex connectivity. Computing and storing
connectivity information is a major bottleneck for visualization algorithms,
affecting both time and memory performance. Recent task-parallel data
structures address this by precomputing connectivity information at runtime
while the analysis algorithm executes, effectively hiding computation costs and
improving performance. However, existing approaches are CPU-bound, forcing the
data structure and analysis algorithm to compete for the same computational
resources, limiting potential speedups. To overcome this limitation, we
introduce a novel task-parallel approach optimized for heterogeneous CPU-GPU
systems. Specifically, we offload the computation of mesh connectivity
information to GPU threads, enabling CPU threads to focus on executing the
visualization algorithm. Following this paradigm, we propose GALE (GPU-Aided
Localized data structurE), the first open-source CUDA-based data structure
designed for heterogeneous task parallelism. Experiments on two 20-core CPUs
and an NVIDIA V100 GPU show that GALE achieves up to 2.7x speedup over
state-of-the-art localized data structures while maintaining memory efficiency.

</details>


### [17] [An ML-Driven Participant Selection Technique for Federated Recommendation System in Edge-Cloud Computing](https://arxiv.org/abs/2507.15233)
*Jintao Liu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 论文提出了一种基于多目标强化学习的联邦推荐系统参与者选择方法，优化了效率与公平性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦推荐系统中设备能力异构、数据非独立同分布和通信瓶颈等问题。

Method: 结合历史客户端性能声誉、数据效用和系统效率，设计多臂老虎机框架动态选择参与者。

Result: 在MovieLens-100K数据集上，方法加速收敛32-50%，减少训练时间46%，并保持或提升推荐指标。

Conclusion: 自适应客户端采样显著提升了联邦推荐系统的效率和公平性。

Abstract: Recommendation systems (RS) personalize content by analyzing user
preferences, but typically require centralized collection of user data, raising
privacy and scalability concerns. Federated Recommendation Systems (FRS)
address these issues by enabling distributed, privacy-preserving model training
across edge devices, keeping raw data on-device. Although existing FRS
frameworks benefit from on-device feature extraction and privacy preservation,
they suffer from heterogeneous device capabilities, non-independent and
identically distributed (non-IID) data, and communication bottlenecks. To
overcome these limitations, we propose a multi-objective reinforcement learning
(RL) participant selection that jointly optimizes historical client performance
reputation (CPR), data utility, and system efficiency. First, we define a
composite client-utility function combining CPR, system capability, and data
quality. Next, we embed this utility into a multi-armed bandit (MAB) framework
and dynamically balance exploration-exploitation to select participants.
Finally, we practically implement our approach using the PySyft framework on an
edge-cloud testbed, and evaluate it on a multimodal movie-recommendation task
built from the MovieLens-100K dataset. Across four different skewed
data-partition scenarios, our MAB-based selection accelerates convergence by
32-50% in time-to-target AUC and reduces total wall-clock training time by up
to 46%, while matching or slightly improving final AUC, NDCG@50, and Recall@50
compared to existing FRS baselines. Our results demonstrate that adaptive,
reward-driven client sampling can substantially enhance both efficiency and
fairness in real-world federated deployments.

</details>


### [18] [Efficient Routing of Inference Requests across LLM Instances in Cloud-Edge Computing](https://arxiv.org/abs/2507.15553)
*Shibo Yu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 本文提出了一种基于NSGA-II的路由算法，用于在云边计算环境中分配LLM推理请求，优化响应质量、时间和成本，实验显示在响应时间和成本上分别提升了95.2%和34.9%。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）推理服务需求的增加，计算资源压力导致延迟和成本问题，需要一种高效的资源分配方法。

Method: 采用NSGA-II算法，将问题建模为多目标优化，平衡响应质量、时间和成本，适应请求和节点的多样性。

Result: 实验结果表明，与基线相比，响应时间和成本分别提升了95.2%和34.9%。

Conclusion: 该算法在动态负载下表现优异，适用于可扩展的LLM部署。

Abstract: The rising demand for Large Language Model (LLM) inference services has
intensified pressure on computational resources, resulting in latency and cost
challenges. This paper introduces a novel routing algorithm based on the
Non-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference
requests across heterogeneous LLM instances in a cloud-edge computing
environment. Formulated as a multi-objective optimization problem, the
algorithm balances response quality, response time, and inference cost,
adapting to request heterogeneity (e.g., varying complexity and prompt lengths)
and node diversity (e.g., edge vs. cloud resources). This adaptive routing
algorithm optimizes performance under dynamic workloads. We benchmark the
approach using a testbed with datasets including Stanford Question Answering
Dataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With
Adversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K).
Experimental results show our solution, compared to the baselines, achieves up
to 95.2% and 34.9% improvements in terms of response time and cost,
respectively. These findings validate the algorithm's effectiveness for
scalable LLM deployments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [19] [SpeedLLM: An FPGA Co-design of Large Language Model Inference Accelerator](https://arxiv.org/abs/2507.14139)
*Peipei Wang,Wu Guan,Liping Liang,Zhijun Wang,Hanqing Luo,Zhibin Zhang*

Main category: cs.AR

TL;DR: SpeedLLM是一种基于Xilinx Alevo U280平台的神经网络加速器，专为Tinyllama框架优化，提升边缘计算性能。


<details>
  <summary>Details</summary>
Motivation: 通过优化边缘计算性能，解决传统Tinyllama实现中的延迟和能耗问题。

Method: 采用数据流并行、内存重用策略和Llama2算子融合，优化数据管道架构和内存策略。

Result: SpeedLLM比传统Tinyllama实现快4.8倍，能耗降低1.18倍。

Conclusion: SpeedLLM显著提升了边缘设备的性能和能效。

Abstract: This paper introduces SpeedLLM, a neural network accelerator designed on the
Xilinx Alevo U280 platform and optimized for the Tinyllama framework to enhance
edge computing performance. Key innovations include data stream parallelism, a
memory reuse strategy, and Llama2 operator fusion, which collectively reduce
latency and energy consumption. SpeedLLM's data pipeline architecture optimizes
the read-compute-write cycle, while the memory strategy minimizes FPGA resource
demands. The operator fusion boosts computational density and throughput.
Results show SpeedLLM outperforms traditional Tinyllama implementations,
achieving up to 4.8* faster performance and 1.18* lower energy consumption,
offering improvements in edge devices.

</details>


### [20] [Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need](https://arxiv.org/abs/2507.14397)
*Michael Davies,Neal Crago,Karthikeyan Sankaralingam,Christos Kozyrakis*

Main category: cs.AR

TL;DR: 本文研究了基于Transformer的大型语言模型（LLM）推理的性能瓶颈，包括内存带宽、容量和分布式系统中的同步开销。通过硬件无关的性能模型，分析了当前及未来硬件技术的性能极限。


<details>
  <summary>Details</summary>
Motivation: 探索LLM推理中的性能瓶颈，为未来硬件设计和优化提供指导。

Method: 开发硬件无关性能模型，分析HBM3、HBM4、3D堆叠DRAM、SRAM等技术及分布式集群的性能。

Result: 发现LLM推理需数百GB内存，高带宽对吞吐量至关重要，同步延迟需控制在1微秒以内，DRAM设计在系统效率上占优。

Conclusion: 未来硬件进步和算法优化可进一步提升LLM推理性能，为部署策略提供指导。

Abstract: This paper presents a limit study of transformer-based large language model
(LLM) inference, focusing on the fundamental performance bottlenecks imposed by
memory bandwidth, memory capacity, and synchronization overhead in distributed
inference systems. We develop a hardware-agnostic performance model that
abstracts away implementation details, enabling the analysis of a wide range of
current and near-future hardware technologies. Our analysis spans from current
HBM3 memory technology used in AI accelerators like GPUs and TPUs to systems
based on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers
SRAM-based designs and scaling techniques from distributed clusters with
varying numbers of chips to wafer-scale integration. Our key findings for
auto-regressive decoding are: i) serving LLMs requires 100s of GB per server to
serve a model instance; ii) high memory bandwidth is critical for high per-user
throughput; iii) exposed synchronization latencies to achieve collective
communication must be around 1us else they make the memory bandwidth
ineffective; iv) DRAM-based designs have a fundamental advantage in terms of
system-level efficiency as measured in throughput per cost or watt; and v)
hardware designs can easily reach 2000+ user token/sec but getting to 10,000+
tokens/sec will need smaller models, smaller context, or other forms of
algorithmic advances. This study provides valuable insights into the
fundamental performance limits of LLM inference, highlighting the potential
benefits of future hardware advancements and guiding the optimization of LLM
deployment strategies.

</details>


### [21] [Enabling Efficient Hardware Acceleration of Hybrid Vision Transformer (ViT) Networks at the Edge](https://arxiv.org/abs/2507.14651)
*Joren Dumoulin,Pouya Houshmand,Vikram Jain,Marian Verhelst*

Main category: cs.AR

TL;DR: 提出了一种针对混合视觉Transformer的硬件加速方案，通过可配置PE阵列、时间循环重排序和层融合优化，显著提升了边缘设备的能效。


<details>
  <summary>Details</summary>
Motivation: 混合视觉Transformer在资源受限的边缘设备上部署时面临多样化的NN层类型和大规模中间数据张量的挑战，阻碍了硬件加速效率。

Method: 1. 可配置PE阵列支持所有混合ViT层类型；2. 时间循环重排序优化归一化和softmax层；3. 层融合减少片外内存传输。

Result: 在28nm CMOS工艺下实现，峰值能效为1.39 TOPS/W，计算速度为25.6 GMACs/s。

Conclusion: 提出的硬件调度优化方案有效解决了混合ViT在边缘设备上的部署难题，显著提升了能效和计算效率。

Abstract: Hybrid vision transformers combine the elements of conventional neural
networks (NN) and vision transformers (ViT) to enable lightweight and accurate
detection. However, several challenges remain for their efficient deployment on
resource-constrained edge devices. The hybrid models suffer from a widely
diverse set of NN layer types and large intermediate data tensors, hampering
efficient hardware acceleration. To enable their execution at the edge, this
paper proposes innovations across the hardware-scheduling stack: a.) At the
lowest level, a configurable PE array supports all hybrid ViT layer types; b.)
temporal loop re-ordering within one layer, enabling hardware support for
normalization and softmax layers, minimizing on-chip data transfers; c.)
further scheduling optimization employs layer fusion across inverted bottleneck
layers to drastically reduce off-chip memory transfers. The resulting
accelerator is implemented in 28nm CMOS, achieving a peak energy efficiency of
1.39 TOPS/W at 25.6 GMACs/s.

</details>


### [22] [GCC: A 3DGS Inference Architecture with Gaussian-Wise and Cross-Stage Conditional Processing](https://arxiv.org/abs/2507.15300)
*Minnan Pei,Gang Li,Junwen Si,Zeyu Zhu,Zitao Mo,Peisong Wang,Zhuoran Song,Xiaoyao Liang,Jian Cheng*

Main category: cs.AR

TL;DR: GCC是一种新型3DGS加速器，通过交叉阶段条件处理和高斯渲染优化，解决了现有加速器的数据流问题，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS加速器在预处理和渲染中存在大量未使用的高斯和重复加载问题，导致计算和数据移动开销大。

Method: GCC引入交叉阶段条件处理和高斯渲染优化，结合alpha边界识别方法，减少渲染成本。

Result: GCC在28nm技术下实现，性能与能效显著优于现有最佳加速器GSCore。

Conclusion: GCC通过优化数据流和渲染方法，为3DGS推理提供了高效且节能的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading neural rendering
technique for high-fidelity view synthesis, prompting the development of
dedicated 3DGS accelerators for mobile applications. Through in-depth analysis,
we identify two major limitations in the conventional decoupled
preprocessing-rendering dataflow adopted by existing accelerators: 1) a
significant portion of preprocessed Gaussians are not used in rendering, and 2)
the same Gaussian gets repeatedly loaded across different tile renderings,
resulting in substantial computational and data movement overhead. To address
these issues, we propose GCC, a novel accelerator designed for fast and
energy-efficient 3DGS inference. At the dataflow level, GCC introduces: 1)
cross-stage conditional processing, which interleaves preprocessing and
rendering to dynamically skip unnecessary Gaussian preprocessing; and 2)
Gaussian-wise rendering, ensuring that all rendering operations for a given
Gaussian are completed before moving to the next, thereby eliminating
duplicated Gaussian loading. We also propose an alpha-based boundary
identification method to derive compact and accurate Gaussian regions, thereby
reducing rendering costs. We implement our GCC accelerator in 28nm technology.
Extensive experiments demonstrate that GCC significantly outperforms the
state-of-the-art 3DGS inference accelerator, GSCore, in both performance and
energy efficiency.

</details>


### [23] [The New LLM Bottleneck: A Systems Perspective on Latent Attention and Mixture-of-Experts](https://arxiv.org/abs/2507.15465)
*Sungmin Yun,Seonyong Park,Hwayong Nam,Younjoo Lee,Gunjun Lee,Kwanhee Kyung,Sangpyo Kim,Nam Sung Kim,Jongmin Kim,Hyungyo Kim,Juhwan Cho,Seungmin Baek,Jung Ho Ahn*

Main category: cs.AR

TL;DR: 论文指出，传统Transformer模型的计算负载存在明显分化，但新型架构（如MLA和MoE）减少了对专用注意力硬件的需求，转而需要更平衡的系统设计。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于传统Transformer模型中Multi-Head Attention（MHA）与feedforward层的计算特性差异，以及如何通过新型架构（MLA和MoE）解决这一问题。

Method: 通过分析MLA的算术强度和MoE的批处理能力，证明它们更适合现代加速器，从而减少对专用硬件的依赖。

Result: MLA的算术强度显著高于MHA，MoE通过批处理可以平衡计算负载，表明专用注意力硬件的需求减少。

Conclusion: 下一代Transformer的设计重点应转向平衡系统，而非专注于单一内存密集型层的加速。

Abstract: Computational workloads composing traditional Transformer models are starkly
bifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic
intensity, while feedforward layers are compute-bound. This dichotomy has long
motivated research into specialized hardware to mitigate the MHA bottleneck.
  This paper argues that recent architectural shifts, namely Multi-head Latent
Attention (MLA) and Mixture-of-Experts (MoE), challenge the premise of
specialized attention hardware. We make two key observations. First, the
arithmetic intensity of MLA is over two orders of magnitude greater than that
of MHA, shifting it close to a compute-bound regime well-suited for modern
accelerators like GPUs. Second, by distributing MoE experts across a pool of
accelerators, their arithmetic intensity can be tuned through batching to match
that of the dense layers, creating a more balanced computational profile.
  These findings reveal a diminishing need for specialized attention hardware.
The central challenge for next-generation Transformers is no longer
accelerating a single memory-bound layer. Instead, the focus must shift to
designing balanced systems with sufficient compute, memory capacity, memory
bandwidth, and high-bandwidth interconnects to manage the diverse demands of
large-scale models.

</details>


### [24] [When Pipelined In-Memory Accelerators Meet Spiking Direct Feedback Alignment: A Co-Design for Neuromorphic Edge Computing](https://arxiv.org/abs/2507.15603)
*Haoxiong Ren,Yangu He,Kwunhang Wong,Rui Bao,Ning Lin,Zhongrui Wang,Dashan Shang*

Main category: cs.AR

TL;DR: 提出了一种软硬件协同设计方法PipeSDFA，通过硬件友好的SDFA算法和RRAM-based IMC架构加速SNN训练，减少计算复杂度和能耗。


<details>
  <summary>Details</summary>
Motivation: SNN在资源受限的边缘设备上部署具有优势，但传统反向传播算法训练SNN计算量大，需要更高效的解决方案。

Method: 软件层面采用SDFA算法消除顺序误差传播，硬件层面设计三级流水线数据流并行化训练。

Result: 实验显示PipeSDFA在五个数据集上准确率损失小于2%，训练时间和能耗分别降低1.1X~10.5X和1.37X~2.1X。

Conclusion: PipeSDFA是一种高效且节能的SNN训练加速方案，适用于边缘设备。

Abstract: Spiking Neural Networks (SNNs) are increasingly favored for deployment on
resource-constrained edge devices due to their energy-efficient and
event-driven processing capabilities. However, training SNNs remains
challenging because of the computational intensity of traditional
backpropagation algorithms adapted for spike-based systems. In this paper, we
propose a novel software-hardware co-design that introduces a hardware-friendly
training algorithm, Spiking Direct Feedback Alignment (SDFA) and implement it
on a Resistive Random Access Memory (RRAM)-based In-Memory Computing (IMC)
architecture, referred to as PipeSDFA, to accelerate SNN training.
Software-wise, the computational complexity of SNN training is reduced by the
SDFA through the elimination of sequential error propagation. Hardware-wise, a
three-level pipelined dataflow is designed based on IMC architecture to
parallelize the training process. Experimental results demonstrate that the
PipeSDFA training accelerator incurs less than 2% accuracy loss on five
datasets compared to baselines, while achieving 1.1X~10.5X and 1.37X~2.1X
reductions in training time and energy consumption, respectively compared to
PipeLayer.

</details>


### [25] [VeriRAG: A Retrieval-Augmented Framework for Automated RTL Testability Repair](https://arxiv.org/abs/2507.15664)
*Haomin Qi,Yuyang Du,Lihao Zhang,Soung Chang Liew,Kexin Chen,Yining Du*

Main category: cs.AR

TL;DR: VeriRAG是一个基于LLM的DFT-EDA框架，通过RAG方法自动修复RTL设计以确保DFT合规性，成功修复率提高了7.72倍。


<details>
  <summary>Details</summary>
Motivation: DFT在EDA工具中研究较少，VeriRAG旨在填补这一空白，利用LLM实现自动化DFT修复。

Method: VeriRAG结合了基于自动编码器的相似性检索模型和迭代代码修订流程，从VeriDFT数据集中检索参考设计进行修复。

Result: VeriRAG实现了完全自动化的DFT修复，修复成功率比零样本基线提高了7.72倍。

Conclusion: VeriRAG展示了LLM在DFT-EDA中的潜力，开源了数据和模型以促进进一步研究。

Abstract: Large language models (LLMs) have demonstrated immense potential in
computer-aided design (CAD), particularly for automated debugging and
verification within electronic design automation (EDA) tools. However, Design
for Testability (DFT) remains a relatively underexplored area. This paper
presents VeriRAG, the first LLM-assisted DFT-EDA framework. VeriRAG leverages a
Retrieval-Augmented Generation (RAG) approach to enable LLM to revise code to
ensure DFT compliance. VeriRAG integrates (1) an autoencoder-based similarity
measurement model for precise retrieval of reference RTL designs for the LLM,
and (2) an iterative code revision pipeline that allows the LLM to ensure DFT
compliance while maintaining synthesizability. To support VeriRAG, we introduce
VeriDFT, a Verilog-based DFT dataset curated for DFT-aware RTL repairs. VeriRAG
retrieves structurally similar RTL designs from VeriDFT, each paired with a
rigorously validated correction, as references for code repair. With VeriRAG
and VeriDFT, we achieve fully automated DFT correction -- resulting in a
7.72-fold improvement in successful repair rate compared to the zero-shot
baseline (Fig. 5 in Section V). Ablation studies further confirm the
contribution of each component of the VeriRAG framework. We open-source our
data, models, and scripts at https://github.com/yuyangdu01/LLM4DFT.

</details>
