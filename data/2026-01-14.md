<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Formalization and Implementation of Safe Destination Passing in Pure Functional Programming Settings](https://arxiv.org/abs/2601.08529)
*Thomas Bagrel*

Main category: cs.PL

TL;DR: 该论文提出了一个带有目的地的λ演算系统{λ_d}，通过模态类型系统结合线性类型和年龄系统来安全管理目的地传递，并在Haskell中实现原型，展示了在处理大型数据结构时的优势。


<details>
  <summary>Details</summary>
Motivation: 目的地传递风格编程允许调用者控制内存管理，在纯函数式编程中能够实现传统不可变数据结构无法表达的程序。然而现有系统表达能力有限，需要更灵活且安全的目的地传递机制。

Method: 开发核心λ演算{λ_d}，采用模态类型系统结合线性类型和年龄系统来管理作用域，确保目的地传递的安全性。然后在Haskell中实现该演算，通过类型系统适配和实现优化来平衡安全性与灵活性。

Result: 核心演算的类型安全性在Coq中形式化证明。Haskell原型实现表明，在遍历或映射大型数据结构（如列表或数据树）时，目的地传递风格编程能带来显著的性能优势。

Conclusion: 目的地传递风格编程在纯函数式语言中具有实用价值，通过精心设计的类型系统可以在保证安全性的同时提供灵活性。虽然Haskell实现需要在安全性和易用性之间权衡，但实际应用效果令人鼓舞。

Abstract: Destination-passing style programming introduces destinations, which represent the address of a write-once memory cell. These destinations can be passed as function parameters, allowing the caller to control memory management: the callee simply fills the cell instead of allocating space for a return value. While typically used in systems programming, destination passing also has applications in pure functional programming, where it enables programs that were previously unexpressible using usual immutable data structures.
  In this thesis, we develop a core λ-calculus with destinations, {λ_d}. Our new calculus is more expressive than similar existing systems, with destination passing designed to be as flexible as possible. This is achieved through a modal type system combining linear types with a system of ages to manage scopes, in order to make destination-passing safe. Type safety of our core calculus was proved formally with the Coq proof assistant.
  Then, we see how this core calculus can be adapted into an existing pure functional language, Haskell, whose type system is less powerful than our custom theoretical one. Retaining safety comes at the cost of removing some flexibility in the handling of destinations. We later refine the implementation to recover much of this flexibility, at the cost of increased user complexity.
  The prototype implementation in Haskell shows encouraging results for adopting destination-passing style programming when traversing or mapping over large data structures such as lists or data trees.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Where to Split? A Pareto-Front Analysis of DNN Partitioning for Edge Inference](https://arxiv.org/abs/2601.08025)
*Adiba Masud,Nicholas Foley,Pragathi Durga Rajarajan,Palden Lama*

Main category: cs.DC

TL;DR: ParetoPipe：一个基于帕累托前沿分析的多目标优化框架，用于在边缘设备上平衡DNN推理的延迟与吞吐量权衡。


<details>
  <summary>Details</summary>
Motivation: 现有DNN分区研究大多关注单目标优化（如最小化延迟或最大化吞吐量），但实际边缘部署中需要在延迟和吞吐量之间进行复杂权衡，且网络条件变化进一步加剧了这一挑战。

Method: 提出ParetoPipe框架，将DNN分区重新定义为多目标优化问题，利用帕累托前沿分析系统性地识别最优分区策略。框架包含双通信后端（PyTorch RPC和自定义轻量级实现）以减少开销。

Result: 在树莓派和GPU边缘服务器组成的异构测试平台上进行了流水线分区推理基准测试，识别了不同网络条件下的帕累托最优点来分析延迟-吞吐量权衡。

Conclusion: ParetoPipe提供了一个灵活的开源框架，支持分布式推理和基准测试，有助于在资源受限的边缘设备上实现DNN部署的延迟与吞吐量平衡。

Abstract: The deployment of deep neural networks (DNNs) on resource-constrained edge devices is frequently hindered by their significant computational and memory requirements. While partitioning and distributing a DNN across multiple devices is a well-established strategy to mitigate this challenge, prior research has largely focused on single-objective optimization, such as minimizing latency or maximizing throughput. This paper challenges that view by reframing DNN partitioning as a multi-objective optimization problem. We argue that in real-world scenarios, a complex trade-off between latency and throughput exists, which is further complicated by network variability. To address this, we introduce ParetoPipe, an open-source framework that leverages Pareto front analysis to systematically identify optimal partitioning strategies that balance these competing objectives.
  Our contributions are threefold: we benchmark pipeline partitioned inference on a heterogeneous testbed of Raspberry Pis and a GPU-equipped edge server; we identify Pareto-optimal points to analyze the latency-throughput trade-off under varying network conditions; and we release a flexible, open-source framework to facilitate distributed inference and benchmarking. This toolchain features dual communication backends, PyTorch RPC and a custom lightweight implementation, to minimize overhead and support broad experimentation.

</details>


### [3] [Hierarchical Precision and Recursion for Accelerating Symmetric Linear Solves on MXUs](https://arxiv.org/abs/2601.08082)
*Vicki Carrica,Rabab Alomairy,Evelyne Ringoot,Alan Edelman*

Main category: cs.DC

TL;DR: 提出基于MXU的混合精度对称线性求解器，通过递归分解和精度控制，在NVIDIA H200和AMD MI300X上实现显著加速


<details>
  <summary>Details</summary>
Motivation: 对称线性求解在科学工程应用中广泛使用，但传统方法在矩阵处理单元上效率有限，需要针对MXU优化的混合精度算法

Method: 采用嵌套递归分解Cholesky为TRSM和SYRK子问题，设计自定义递归数据结构，对大型非对角块使用FP16低精度，对角块保持高精度确保数值稳定性

Result: 在H200上，递归FP64 SYRK比cuBLAS快14倍，混合精度SYRK比全精度基线快27倍，TRSM快5倍，Cholesky整体比cuSOLVER FP64快5倍，精度比纯FP16高100倍

Conclusion: 该混合精度求解器在NVIDIA和AMD GPU上均表现出优异性能，为对称线性求解提供了高效可移植的解决方案

Abstract: Symmetric linear solves are fundamental to a wide range of scientific and engineering applications, from climate modeling and structural analysis to machine learning and optimization. These workloads often rely on Cholesky (POTRF) decomposition and its supporting operations, triangular solves (TRSM) and symmetric rank-k updates (SYRK), which together form the computational core for solving symmetric positive-definite systems. To accelerate these kernels, we present a portable, mixed-precision solver designed for Matrix Processing Units (MXUs), including NVIDIA Tensor Cores (H200) and AMD Matrix Cores (MI300X). Our algorithm builds on a nested recursive formulation in which Cholesky exposes parallelism through recursive decomposition of its TRSM and SYRK sub-problems. This structure yields a hierarchical recursion that maximizes GEMM throughput while enabling fine-grained control over numerical precision. We introduce a custom recursive data structure that assigns low-precision FP16 arithmetic to large off-diagonal blocks, while preserving high precision on diagonal blocks to ensure numerical stability. The solver is implemented in Julia, leveraging array programming, multiple dispatch, and dynamic type inference to enable seamless expression of mixed-precision computation. This design provides a high-level, hardware-agnostic interface while efficiently interfacing with low-level vendor libraries for backend portability. On H200, our recursive FP64 SYRK achieves a 14x speedup over cuBLAS, while mixed-precision delivers up to 27x speedup in SYRK and 5x in TRSM over full-precision baselines. This results in a 5x overall speedup for Cholesky versus cuSOLVER FP64, with 100x better accuracy than pure FP16 while retaining 88% of its peak speedup. Comparable performance and accuracy trends are observed on MI300X, demonstrating broad applicability across GPUs.

</details>


### [4] [Matrix-PIC: Harnessing Matrix Outer-product for High-Performance Particle-in-Cell Simulations](https://arxiv.org/abs/2601.08277)
*Yizhuo Rao,Xingjian Cui,Jiabin Xie,Shangzhi Pang,Guangnan Feng,Jinhui Wei,Zhiguang Chen,Yutong Lu*

Main category: cs.DC

TL;DR: MatrixPIC：利用CPU上的矩阵处理单元（MPU）重新设计PIC模拟中的电流沉积步骤，通过矩阵外积原语实现性能显著提升


<details>
  <summary>Details</summary>
Motivation: 传统PIC模拟中粒子-网格交互的细粒度原子更新成为CPU多核性能瓶颈，而现代CPU集成的专用矩阵处理单元（MPU）为克服这一限制提供了新机会

Method: 提出MatrixPIC，包含三个核心设计：1）将电流沉积算法重新表述为块矩阵形式，自然映射到MPU外积原语；2）混合执行流水线，结合MPU高密度累加和VPU数据准备；3）基于间隙打包内存数组的O(1)摊销增量排序器，保持数据局部性

Result: 在下一代HPC平台上，MatrixPIC实现显著性能提升：LWFA模拟总运行时间加速2.63倍；三阶沉积核心内核加速8.7倍（相比基线）和2.0倍（相比最佳VPU实现）；达到CPU理论峰值性能的83.08%，比数据中心GPU上高度优化的CUDA内核高近2.8倍

Conclusion: MatrixPIC展示了面向矩阵的协同设计在加速新兴CPU架构上PIC模拟方面的有效性，为利用专用硬件加速科学计算提供了新途径

Abstract: Particle-in-Cell (PIC) simulations spend most of their execution time on particle--grid interactions, where fine-grained atomic updates become a major bottleneck on traditional many-core CPUs. Recent CPU architectures integrate specialized Matrix Processing Units (MPUs) that efficiently support matrix outer-product operations, offering new opportunities to overcome this limitation. Leveraging this architectural shift, this work focuses on redesigning the current deposition step of PIC simulations under a matrix-centric execution model.
  We present MatrixPIC, the first holistic co-design of the deposition kernel, data layout, and incremental particle sorting tailored to the hybrid MPU--VPU SIMD model on modern CPUs. MatrixPIC introduces: (i)~a block-matrix formulation of the current deposition algorithm that maps naturally to MPU outer-product primitives; (ii)~a hybrid execution pipeline that combines MPU-based high-density accumulation with VPU-based data preparation and control flow; and (iii)~an $O(1)$-amortized incremental sorter based on a gapped packed-memory array to preserve data locality for efficient MPU execution.
  Evaluated on a next-generation HPC platform, MatrixPIC achieves significant performance gains. In Laser-Wakefield Acceleration (LWFA) simulations, it delivers up to $2.63\times$ speedup in total runtime. For third-order deposition, the core kernel is accelerated by $8.7\times$ over the baseline and $2.0\times$ over the best hand-optimized VPU implementation. Moreover, MatrixPIC reaches $83.08\%$ of theoretical CPU peak performance, nearly $2.8\times$ higher than a highly optimized CUDA kernel on a data center GPU. These results demonstrate the effectiveness of matrix-oriented co-design for accelerating PIC simulations on emerging CPU architectures.

</details>


### [5] [Shifting the Sweet Spot: High-Performance Matrix-Free Method for High-Order Elasticity](https://arxiv.org/abs/2601.08374)
*Dali Chang,Chong Zhang,Kaiqi Zhang,Mingguan Yang,Huiyuan Li,Weiqiang Kong*

Main category: cs.DC

TL;DR: 本文针对高阶有限元弹性分析中的矩阵自由方法进行优化，通过张量分解、Voigt对称性利用和宏核融合等技术，将性能最佳点从p≈2提升到p≥6，在主流CPU架构上实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有矩阵自由方法未能充分利用现代CPU架构和张量积单元的特殊结构，导致性能最佳点异常停留在低阶p≈2，严重限制了高阶方法的潜力。需要解决这一性能瓶颈以实现大规模高阶弹性模拟。

Method: 在MFEM框架内设计高度优化的矩阵自由算子，与几何多重网格预处理器深度集成。采用多级优化策略：1) 用基于张量分解的O(p^4)算法替代原始O(p^6)通用算法；2) 利用Voigt对称性减少弹性问题的冗余计算；3) 使用宏核融合增强数据局部性并突破内存带宽瓶颈。

Result: 实验表明方法成功将性能最佳点转移到高阶区域p≥6。优化后的核心算子比MFEM基线加速7-83倍，完整求解过程端到端性能提升3.6-16.8倍。在主流x86和ARM架构上均验证了有效性。

Conclusion: 本文为在主流CPU硬件上进行大规模高阶弹性模拟提供了经过验证的高效实践路径，通过深度优化矩阵自由方法成功解决了传统实现中的性能瓶颈问题。

Abstract: In high-order finite element analysis for elasticity, matrix-free (PA) methods are a key technology for overcoming the memory bottleneck of traditional Full Assembly (FA). However, existing implementations fail to fully exploit the special structure of modern CPU architectures and tensor-product elements, causing their performance "sweet spot" to anomalously remain at the low order of $p \approx 2$, which severely limits the potential of high-order methods. To address this challenge, we design and implement a highly optimized PA operator within the MFEM framework, deeply integrated with a Geometric Multigrid (GMG) preconditioner. Our multi-level optimization strategy includes replacing the original $O(p^6)$ generic algorithm with an efficient $O(p^4)$ one based on tensor factorization, exploiting Voigt symmetry to reduce redundant computations for the elasticity problem, and employing macro-kernel fusion to enhance data locality and break the memory bandwidth bottleneck. Extensive experiments on mainstream x86 and ARM architectures demonstrate that our method successfully shifts the performance "sweet spot" to the higher-order region of $p \ge 6$. Compared to the MFEM baseline, the optimized core operator (kernel) achieves speedups of 7x to 83x, which translates to a 3.6x to 16.8x end-to-end performance improvement in the complete solution process. This paper provides a validated and efficient practical path for conducting large-scale, high-order elasticity simulations on mainstream CPU hardware.

</details>


### [6] [MixServe: An Automatic Distributed Serving System for MoE Models with Hybrid Parallelism Based on Fused Communication Algorithm](https://arxiv.org/abs/2601.08800)
*Bowen Zhou,Jinrui Jia,Wenhao He,Yong Zhang,Fang Dong*

Main category: cs.DC

TL;DR: MixServe是一个用于高效部署MoE模型的自动分布式服务系统，采用基于融合AR-A2A通信算法的TP-EP混合并行策略，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: MoE模型由于参数量巨大，通常需要多GPU或多节点部署，通信成为主要瓶颈。现有TP方法节点间效率低，EP方法负载不均衡，需要更好的并行策略来解决这些问题。

Method: 提出MixServe系统：1）自动评估不同并行策略的通信开销，根据模型超参数和硬件配置选择最优策略；2）提出基于融合AR-A2A通信算法的TP-EP混合并行，重叠节点内AR通信和节点间A2A通信。

Result: 在DeepSeek-R1和Qwen3模型上的实验表明，MixServe相比现有方法：首token延迟加速1.08~3.80倍，token间延迟加速1.03~1.66倍，吞吐量提升5.2%~50.3%。

Conclusion: MixServe通过创新的TP-EP混合并行和融合通信算法，有效解决了MoE模型分布式部署中的通信瓶颈问题，显著提升了推理性能。

Abstract: The Mixture of Experts (MoE) models are emerging as the latest paradigm for Large Language Models (LLMs). However, due to memory constraints, MoE models with billions or even trillions of parameters can only be deployed in multi-GPU or even multi-node & multi-GPU based serving systems. Thus, communication has became a major bottleneck in distributed serving systems, especially inter-node communication. Contemporary distributed MoE models are primarily implemented using all-reduce (AR) based tensor parallelism (TP) and all-to-all (A2A) based expert parallelism (EP). However, TP generally exhibits low inter-node efficiency and is thus confined to high-speed intra-node bandwidth. In contrast, EP tends to suffer from load imbalance, especially when the parallel degree is high.
  In this work, we introduce MixServe, a novel automatic distributed serving system for efficient deployment of MoE models by a novel TP-EP hybrid parallelism based on fused AR-A2A communication algorithm. MixServe begins by evaluating the communication overhead associated with various parallel strategies, taking into account the model hyperparameters and the configurations of network and hardware resources, and then automatically selects the most efficient parallel strategy. Then, we propose the TP-EP hybrid parallelism based on fused AR-A2A communication algorithm that overlaps intra-node AR communication and inter-node A2A communication. Extensive experiments on DeepSeek-R1 and Qwen3 models demonstrate that MixServe achieves superior inference performance, with 1.08~3.80x acceleration in time to first token (TTFT), 1.03~1.66x acceleration in inter-token latency (ITL), and 5.2%~50.3% throughput improvement compared to existing approaches.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [A New Tool to Find Lightweight (And, Xor) Implementations of Quadratic Vectorial Boolean Functions up to Dimension 9](https://arxiv.org/abs/2601.08368)
*Marie Bolzer,Sébastien Duval,Marine Minier*

Main category: cs.AR

TL;DR: 提出新工具实现9位以内二次函数的AND-depth 1电路，优化AND门数量，比现有工具更高效


<details>
  <summary>Details</summary>
Motivation: 现有电路综合工具主要针对大型电子芯片设计，而密码学需要小型函数实现工具。现有工具局限于5-6位函数，计算时间长，无法处理更大规模

Method: 开发新工具，采用特定算法实现9位以内二次函数的AND-depth 1电路，最小化AND门数量，工具通过CodeOcean和GitHub提供

Result: 新工具比现有工具时间效率更高，能在6位或更小规模上探索更大实现，并能处理高达9位的函数

Conclusion: 新工具填补了密码学中小型函数电路综合的空白，突破了现有工具在函数位数和计算时间上的限制

Abstract: The problem of finding a minimal circuit to implement a given function is one of the oldest in electronics. It is known to be NP-hard. Still, many tools exist to find sub-optimal circuits to implement a function. In electronics, such tools are known as synthesisers. However, these synthesisers aim to implement very large functions (a whole electronic chip). In cryptography, the focus is on small functions, hence the necessity for new dedicated tools for small functions. Several tools exist to implement small functions. They differ by their algorithmic approach (some are based on Depth-First-Search as introduced by Ullrich in 2011, some are based on SAT-solvers like the tool desgined by Stoffelen in 2016, some non-generic tools use subfield decomposition) and by their optimisation criteria (some optimise for circuit size, others for circuit depth, and some for side-channel-protected implementations). However, these tools are limited to functions operating on less than 5 bits, sometimes 6 bits for quadratic functions, or to very simple functions. The limitation lies in a high computing time. We propose a new tool (The tool is provided alongside the IEEE article with CodeOcean and at https://github.com/seduval/implem-quad-sbox) to implement quadratic functions up to 9 bits within AND-depth 1, minimising the number of AND gates. This tool is more time-efficient than previous ones, allowing to explore larger implementations than others on 6 bits or less and allows to reach larger sizes, up to 9 bits.

</details>
