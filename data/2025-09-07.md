<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [When Lifetimes Liberate: A Type System for Arenas with Higher-Order Reachability Tracking](https://arxiv.org/abs/2509.04253)
*Siyuan He,Songlin Jia,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 这篇论文提出了一种统一的静态资源管理方案，结合了区域系统、所有权类型和可达性类型的优点，支持任意共享和静态生命周期保证。


<details>
  <summary>Details</summary>
Motivation: 解决高阶函数语言中静态资源管理的挑战，区域系统、Rust所有权类型和可达性类型都存在各自的限制，需要统一它们的优点。

Method: 在可达性类型基础上提出两个新扩展：A<:使用二维存储模型支持区域内资源的粗粒度跟踪，{A}<:实现词法生命周期控制和静态保证。

Result: 设计了两个形式化的算法，在Rocq中完成了类型安全性证明，避免了流效感推理的复杂性。

Conclusion: 该方法统一了不同资源管理方案的优点，为高阶函数语言提供了灵活且安全的静态资源管理能力。

Abstract: Static resource management in higher-order functional languages remains
elusive due to tensions between control, expressiveness, and flexibility.
Region-based systems [Grossman et al. 2002; Tofte et al. 2001] offer control
over lifetimes and expressive in-region sharing, but restrict resources to
lexical scopes. Rust, an instance of ownership types [Clarke et al. 2013],
offers non-lexical lifetimes and robust safety guarantees, yet its global
invariants make common sharing patterns hard to express. Reachability types
[Wei et al. 2024] enable reasoning about sharing and separation, but lack
practical tools for controlling resource lifetimes.
  In this work, we try to unify their strengths. Our solution enables grouping
resources as arenas for arbitrary sharing and static guarantees of lexically
scoped lifetimes. Crucially, arenas and lexical lifetimes are not the only
choice: users may also manage resources individually, with non-lexical
lifetimes. Regardless of mode, resources share the same type, preserving the
higher-order parametric nature of the language.
  Obtaining static safety guarantee in a higher-order language with flexible
sharing is nontrivial. To this end, we propose two new extensions atop
reachability types [Wei et al. 2024]. First, A<: features a novel
two-dimensional store model to enable coarse-grained reachability tracking for
arbitrarily shared resources within arenas. Building on this, {A}<: establishes
lexical lifetime control with static guarantees. As the first reachability
formalism presented for lifetime control, {A}<: avoids the complication of
flow-sensitive reasoning and retains expressive power and simplicity. Both
calculi are formalized and proven type safe in Rocq.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Combining Performance and Productivity: Accelerating the Network Sensing Graph Challenge with GPUs and Commodity Data Science Software](https://arxiv.org/abs/2509.03653)
*Siddharth Samsi,Dan Campbell,Emanuel Scoullos,Oded Green*

Main category: cs.DC

TL;DR: 本文展示了使用NVIDIA RAPIDS生态系统中的现成ETL工具来实现HPEC图挑战的新方法，相比CPU上的Pandas实现获得了147-2185倍的加速


<details>
  <summary>Details</summary>
Motivation: HPEC图挑战代表了传统基准测试无法覆盖的复杂HPC工作负载，最新挑战需要更多软件组件。本文旨在探索使用数据科学语言和现成工具来替代GraphBLAS实现

Method: 使用NVIDIA RAPIDS生态系统中的cuDF和cupy等现成ETL工具，重新解释GraphBLAS公式，实现网络感知图挑战的端到端工作负载

Result: 在NVIDIA A100 GPU上获得147-509倍加速，H100 GPU上243-1269倍加速，H200 GPU上332-2185倍加速，相比CPU上的Pandas实现

Conclusion: 使用现成的企业级软件工具可以实现显著的软件加速，无需编写特定的HPC代码，为图分析工作负载提供了高效的替代方案

Abstract: The HPEC Graph Challenge is a collection of benchmarks representing complex
workloads that test the hardware and software components of HPC systems, which
traditional benchmarks, such as LINPACK, do not. The first benchmark, Subgraph
Isomorphism, focused on several compute-bound and memory-bound kernels. The
most recent of the challenges, the Anonymized Network Sensing Graph Challenge,
represents a shift in direction, as it represents a longer end-to-end workload
that requires many more software components, including, but not limited to,
data I/O, data structures for representing graph data, and a wide range of
functions for data preparation and network analysis. A notable feature of this
new graph challenge is the use of GraphBLAS to represent the computational
aspects of the problem statement. In this paper, we show an alternative
interpretation of the GraphBLAS formulations using the language of data
science. With this formulation, we show that the new graph challenge can be
implemented using off-the-shelf ETL tools available in open-source, enterprise
software such as NVIDIA's RAPIDS ecosystem. Using off-the-shelf software,
RAPIDS cuDF and cupy, we enable significant software acceleration without
requiring any specific HPC code and show speedups, over the same code running
with Pandas on the CPU, of 147x-509x on an NVIDIA A100 GPU, 243x-1269X for an
NVIDIA H100 GPU, and 332X-2185X for an NVIDIA H200 GPU.

</details>


### [3] [Distributed Download from an External Data Source in Asynchronous Faulty Settings](https://arxiv.org/abs/2509.03755)
*John Augustine,Soumyottam Chatterjee,Valerie King,Manish Kumar,Shachar Meir,David Peleg*

Main category: cs.DC

TL;DR: 本文首次研究异步通信网络中的分布式数据检索问题，提出了在崩溃故障和拜占庭故障模型下的查询最优确定性解决方案，并建立了随机协议的下界和上界。


<details>
  <summary>Details</summary>
Motivation: 扩展分布式数据检索研究到异步通信网络，解决之前工作主要关注同步网络的问题，旨在最小化查询复杂度同时最大化容错能力。

Method: 在异步通信模型中设计确定性协议处理崩溃故障，使用随机协议处理拜占庭故障，分析查询复杂度的上下界。

Result: 对于崩溃故障，提出了可容忍任意固定比例β<1故障的查询最优确定性方案；对于拜占庭故障，证明了随机协议在β≥1/2时的Ω(n)下界，并在β<1/2时给出了接近最优的随机协议。

Conclusion: 这是首个解决异步网络中数据下载问题的工作，为异步分布式数据检索提供了理论基础和实用协议设计。

Abstract: The distributedData Retrieval (DR) model consists of $k$ peers connected by a
complete peer-to-peer communication network, and a trusted external data source
that stores an array $\textbf{X}$ of $n$ bits ($n \gg k$). Up to $\beta k$ of
the peers might fail in any execution (for $\beta \in [0, 1)$). Peers can
obtain the information either by inexpensive messages passed among themselves
or through expensive queries to the source array $\textbf{X}$. In the DR model,
we focus on designing protocols that minimize the number of queries performed
by any nonfaulty peer (a measure referred to as query complexity) while
maximizing the resilience parameter $\beta$.
  The Download problem requires each nonfaulty peer to correctly learn the
entire array $\textbf{X}$. Earlier work on this problem focused on synchronous
communication networks and established several deterministic and randomized
upper and lower bounds. Our work is the first to extend the study of
distributed data retrieval to asynchronous communication networks. We address
the Download problem under both the Byzantine and crash failure models. We
present query-optimal deterministic solutions in an asynchronous model that can
tolerate any fixed fraction $\beta<1$ of crash faults. In the Byzantine failure
model, it is known that deterministic protocols incur a query complexity of
$\Omega(n)$ per peer, even under synchrony. We extend this lower bound to
randomized protocols in the asynchronous model for $\beta \geq 1/2$, and
further show that for $\beta < 1/2$, a randomized protocol exists with
near-optimal query complexity. To the best of our knowledge, this is the first
work to address the Download problem in asynchronous communication networks.

</details>


### [4] [Gathering of asynchronous robots on circle with limited visibility using finite communication](https://arxiv.org/abs/2509.04004)
*Avisek Sharma,Satakshi Ghosh,Buddhadeb Sau*

Main category: cs.DC

TL;DR: 本文提出了一种在π-可见性模型下解决圆形环境中有限可见度机器人聚集问题的算法，支持非刚性移动和完全异步调度器


<details>
  <summary>Details</summary>
Motivation: 解决有限可见度机器人在圆形环境中的聚集问题，特别是在π-可见性模型下，此前的研究要么需要半同步调度器，要么需要特殊异步调度器，本文旨在在完全异步调度器下实现聚集

Method: 使用有限通信能力(ℱCOM)的机器人，在π-可见性模型下，通过非刚性移动方式，在完全异步调度器下实现聚集

Result: 成功设计了能够在π-可见性模型下解决聚集问题的算法，支持非刚性移动和完全异步调度

Conclusion: 本文算法证明了在π-可见性模型下，即使使用完全异步调度器和非刚性移动，也能实现机器人聚集，扩展了此前研究的局限性

Abstract: This work addresses the gathering problem for a set of autonomous, anonymous,
and homogeneous robots with limited visibility operating in a continuous
circle. The robots are initially placed at distinct positions, forming a
rotationally asymmetric configuration. The robots agree on the clockwise
direction. In the $\theta$-visibility model, a robot can only see those robots
on the circle that are at an angular distance $<\theta$ from it. Di Luna
\textit{et. al.} [DISC'20] have shown that, in $\pi/2$ visibility, gathering is
impossible. In addition, they provided an algorithm for robots with $\pi$
visibility, operating under a semi-synchronous scheduler. In the $\pi$
visibility model, only one point, the point at the angular distance $\pi$ is
removed from the visibility. Ghosh \textit{et. al.} [SSS'23] provided a
gathering algorithm for $\pi$ visibility model with robot having finite memory
($\mathcal{FSTA}$), operating under a special asynchronous scheduler.
  If the robots can see all points on the circle, then the gathering can be
done by electing a leader in the weakest robot model under a fully asynchronous
scheduler. However, previous works have shown that even the removal of one
point from the visibility makes gathering difficult. In both works, the robots
had rigid movement. In this work, we propose an algorithm that solves the
gathering problem under the $\pi$-visibility model for robots that have finite
communication ability ($\mathcal{FCOM}$). In this work the robot movement is
non-rigid and the robots work under a fully asynchronous scheduler.

</details>


### [5] [Counterfactual simulations for large scale systems with burnout variables](https://arxiv.org/abs/2509.04038)
*Benjamin Heymann*

Main category: cs.DC

TL;DR: 提出基于不确定性松弛的新算法，用于高效并行计算具有burnout变量的系统中的反事实估计


<details>
  <summary>Details</summary>
Motivation: 大规模系统中存在burnout变量（激活后不可逆失活），传统顺序模拟方法计算成本高，难以扩展，特别是在在线广告等场景中

Method: 引入不确定性松弛算法，将顺序处理转换为并行计算，提高反事实估计的可扩展性

Result: 新算法显著提高了具有burnout变量系统的计算效率

Conclusion: 不确定性松弛方法为解决burnout变量系统的反事实分析提供了有效的可扩展解决方案

Abstract: We consider large-scale systems influenced by burnout variables - state
variables that start active, shape dynamics, and irreversibly deactivate once
certain conditions are met. Simulating what-if scenarios in such systems is
computationally demanding, as alternative trajectories often require sequential
processing, which does not scale very well. This challenge arises in settings
like online advertising, because of campaigns budgets, complicating
counterfactual analysis despite rich data availability. We introduce a new type
of algorithms based on what we refer to as uncertainty relaxation, that enables
efficient parallel computation, significantly improving scalability for
counterfactual estimation in systems with burnout variables.

</details>


### [6] [LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for High-Performance Distributed Training Systems](https://arxiv.org/abs/2509.04084)
*Chenxuan Yao,Yuchong Hu,Feifan Liu,Zhengyu Liu,Dan Feng*

Main category: cs.DC

TL;DR: LowDiff是一个高效的分布式训练检查点框架，通过重用压缩梯度作为差异检查点来降低存储成本，支持每迭代检查点频率且运行时开销低于3.1%。


<details>
  <summary>Details</summary>
Motivation: 分布式大规模深度学习训练容易失败，传统频繁检查点方法产生大量检查点导致成本高昂，而现有的差异检查点方法仅限于推荐系统，无法应用于通用分布式训练系统。

Method: 提出LowDiff框架：1)重用压缩梯度作为差异检查点；2)批量梯度写入优化；3)动态调整检查点频率和批处理大小；4)分层梯度重用和快照方法；5)基于CPU的异步持久化策略。

Result: 在各种工作负载上的实验表明，LowDiff可以实现每迭代检查点频率，运行时开销低于3.1%。

Conclusion: LowDiff通过创新的梯度重用和优化技术，成功解决了分布式训练中频繁检查点的高成本问题，实现了高效的故障恢复机制。

Abstract: Distributed training of large deep-learning models often leads to failures,
so checkpointing is commonly employed for recovery. State-of-the-art studies
focus on frequent checkpointing for fast recovery from failures. However, it
generates numerous checkpoints, incurring substantial costs and thus degrading
training performance. Recently, differential checkpointing has been proposed to
reduce costs, but it is limited to recommendation systems, so its application
to general distributed training systems remains unexplored.
  This paper proposes LowDiff, an efficient frequent checkpointing framework
that \textit{reuses} compressed gradients, serving as differential checkpoints
to reduce cost. Furthermore, LowDiff incorporates a batched gradient write
optimization to persist these differentials to storage efficiently. It also
dynamically tunes both the checkpoint frequency and the batching size to
maximize performance. We further enhance LowDiff with a layer-wise gradient
reusing and snapshotting approach and a CPU-based asynchronous persistence
strategy, enabling frequent checkpointing without gradient compression.
Experiments on various workloads show that LowDiff can achieve checkpointing
frequency up to per iteration with less than 3.1\% runtime overhead.

</details>


### [7] [Trustworthy Second-hand Marketplace for Built Environment](https://arxiv.org/abs/2509.04085)
*Stanly Wilson,Kwabena Adu-Duodu,Yinhao Li,Ringo Sham,Yingli Wang,Ellis Solaiman,Charith Perera,Rajiv Ranjan,Omer Rana*

Main category: cs.DC

TL;DR: 基于区块链和IPFS的数字化市场平台，促进建筑材料可持续重复利用


<details>
  <summary>Details</summary>
Motivation: 解决建筑行业材料浪费和可持续性挑战，需要整合自动化、可追溯性和去中心化决策的创新解决方案

Method: 开发了一个基于区块链和易完整性IPFS的数字化市场平台框架，确保材料交换的透明度和可追溯性

Result: 平台框架展示了市场的运营过程，证明了其实际应用效果和可行性

Conclusion: 该市场平台能够促进可重复使用材料的高效、可信交换，是向更可持续建筑实践进行的重要一步

Abstract: The construction industry faces significant challenges regarding material
waste and sustainable practices, necessitating innovative solutions that
integrate automation, traceability, and decentralised decision-making to enable
efficient material reuse. This paper presents a blockchain-enabled digital
marketplace for sustainable construction material reuse, ensuring transparency
and traceability using InterPlanetary File System (IPFS). The proposed
framework enhances trust and accountability in material exchange, addressing
key challenges in industrial automation and circular supply chains. A framework
has been developed to demonstrate the operational processes of the marketplace,
illustrating its practical application and effectiveness. Our contributions
show how the marketplace can facilitate the efficient and trustworthy exchange
of reusable materials, representing a substantial step towards more sustainable
construction practices.

</details>


### [8] [On the impact of unlimited computational power in OBLOT: consequences for synchronous robots on graphs](https://arxiv.org/abs/2509.04383)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 本文证明在同步机器人在有限图上移动时，无限计算能力对OBLOT模型有显著影响，并提出了一种保证最少移动和轮数的通用解决算法


<details>
  <summary>Details</summary>
Motivation: OBLOT模型中机器人能力有限（匿名、无方向感、无记忆、静默），算法设计具有挑战性。现有研究主要关注移动次数和轮数成本，而忽略了计算能力的影响

Method: 利用同步机器人在有限图上的无限计算能力，开发了一种通用解决算法

Result: 该算法适用于广泛类别的问题，同时保证最少的移动次数和轮数

Conclusion: 无限计算能力在OBLOT模型中具有重要影响，可以显著提升算法性能

Abstract: The OBLOT model has been extensively studied in theoretical swarm robotics.
It assumes weak capabilities for the involved mobile robots, such as they are
anonymous, disoriented, no memory of past events (oblivious), and silent. Their
only means of (implicit) communication is transferred to their positioning,
i.e., stigmergic information. These limited capabilities make the design of
distributed algorithms a challenging task. Over the last two decades, numerous
research papers have addressed the question of which tasks can be accomplished
within this model. Nevertheless, as it usually happens in distributed
computing, also in OBLOT the computational power available to the robots is
neglected as the main cost measures for the designed algorithms refer to the
number of movements or the number of rounds required. In this paper, we prove
that for synchronous robots moving on finite graphs, the unlimited
computational power (other than finite time) has a significant impact. In fact,
by exploiting it, we provide a definitive resolution algorithm that applies to
a wide class of problems while guaranteeing the minimum number of moves and
rounds.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [Hardware-Aware Data and Instruction Mapping for AI Tasks: Balancing Parallelism, I/O and Memory Tradeoffs](https://arxiv.org/abs/2509.03846)
*Md Rownak Hossain Chowdhury,Mostafizur Rahman*

Main category: cs.AR

TL;DR: 通过预测神经网络行为进行前置计算和通信规划，生成统一的指令数据流，减少主机干预和片外内存使用，实现高效深度学习推理执行。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习推理中常见的I/O瓶颈、片外内存过多依赖以及主机经常干预的问题，提高硬件执行效率。

Method: 采用流式计算框架，利用粒度消息传递在可编程计算架构上实现数据本地移动，通过静态重用、数组内多播和分段约简等技术协调计算。

Result: 在VGG-19上实现了88-92%的高利用率，97%消息内部生成，89%时间用于片内传输，计算吞吐量超1 TFLOP/s，数据重用和本地聚合减少达100MB每层。

Conclusion: 流式计算框架通过紧密协调数据和指令流，能够有效提高深度学习推理的硬件执行效率，显著减少外部依赖。

Abstract: We introduce a mapping framework for deep learning inference that takes
advantage of predictable neural network behavior to plan both computation and
communication ahead of time. The framework generates a unified stream of
instructions and data, enabling the hardware to execute operations and route
information on its own, without frequent involvement from the host and with
minimal off-chip memory use. This naturally reduces reliance on I/O, off-chip
memory, and host control. By leveraging fine-grained message passing on a
programmable, message-based compute architecture, the framework keeps data
movement local and coordinates computation across the array using techniques
such as stationary-weight reuse, in-array multicasting, and staged reductions.
Applied to VGG-19, the framework sustains high utilization (88 to 92 percent),
with over 97 percent of messages generated internally and nearly 89 percent of
time consumed on-chip transfers. Computation throughput scales beyond 1 TFLOP/s
on larger arrays, while traffic reductions from reuse and local aggregation
reach up to 100 MB per layer. Overall, the results highlight the effectiveness
of streaming-based computation and show how our mapper enables this execution
style by tightly coordinating data and instruction flow across the hardware.

</details>


### [10] [Real Time FPGA Based CNNs for Detection, Classification, and Tracking in Autonomous Systems: State of the Art Designs and Optimizations](https://arxiv.org/abs/2509.04153)
*Safa Mohammed Sali,Mahmoud Meribout,Ashiyana Abdul Majeed*

Main category: cs.AR

TL;DR: 本文全面综述了在FPGA上部署卷积神经网络进行目标检测、分类和跟踪的最新进展，重点分析了FPGA相比GPU和ASIC的优势以及各种优化技术和平台架构。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶、机器人和监控等实时计算机视觉应用需求的增长，FPGA因其可重构性、低功耗和确定性延迟等优势，成为GPU和ASIC的有力替代方案。

Method: 通过批判性审查最先进的FPGA实现，涵盖算法创新、硬件加速技术、剪枝、量化和稀疏感知等优化策略，以及现代FPGA平台和软件开发工具的分析。

Result: 系统比较了不同FPGA架构的能力，总结了软硬件协同设计实践、数据流优化和流水线处理技术，为实时推理提供了有效解决方案。

Conclusion: 该综述为研究人员和工程师提供了开发下一代高效能、高性能视觉系统的关键见解，特别适用于边缘和嵌入式应用的FPGA部署。

Abstract: This paper presents a comprehensive review of recent advances in deploying
convolutional neural networks (CNNs) for object detection, classification, and
tracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand
for real-time computer vision applications in domains such as autonomous
vehicles, robotics, and surveillance, FPGAs have emerged as a powerful
alternative to GPUs and ASICs due to their reconfigurability, low power
consumption, and deterministic latency. We critically examine state-of-the-art
FPGA implementations of CNN-based vision tasks, covering algorithmic
innovations, hardware acceleration techniques, and the integration of
optimization strategies like pruning, quantization, and sparsity-aware methods
to maximize performance within hardware constraints. This survey also explores
the landscape of modern FPGA platforms, including classical LUT-DSP based
architectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration
Platforms (ACAPs), comparing their capabilities in handling deep learning
workloads. Furthermore, we review available software development tools such as
Vitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the
design and deployment of AI models on FPGAs. The paper uniquely discusses
hybrid architecture that combine GPUs and FPGAs for collaborative acceleration
of AI inference, addressing challenges related to energy efficiency and
throughput. Additionally, we highlight hardware-software co-design practices,
dataflow optimizations, and pipelined processing techniques essential for
real-time inference on resource-constrained devices. Through this survey,
researchers and engineers are equipped with insights to develop
next-generation, power-efficient, and high-performance vision systems optimized
for FPGA deployment in edge and embedded applications.

</details>


### [11] [Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs and Optimizations](https://arxiv.org/abs/2509.04162)
*Safa Mohammed Sali,Mahmoud Meribout,Ashiyana Abdul Majeed*

Main category: cs.AR

TL;DR: 这篇论文综述了基于FPGA的Transformer和视觉-语言模型推理加速的设计交易、优化策略和实现挑战，包括硬件选型、内存系统、数据流管理、量化策略等关键因素，并提出了未来可重构、可穿透的FPGA解决方案的发展方向。


<details>
  <summary>Details</summary>
Motivation: Transformer和视觉-语言模型虽然在计算机视觉和多模态AI中表现突出，但其高计算复杂度、大内存占用和不规则数据访问模式给延迟和功耗受限环境中的部署带来了重大挑战。FPGA因其可重构性、细粒度并行性和能源效率优势而成为理想的硬件平台。

Method: 论文进行了全面的设计交易分析，考察了设备类型选择、内存子系统限制、数据流组织、量化策略、稀疏性利用、工具链选择等关键因素，以及多模态VLMs特有的异构计算平衡和交叉注意力内存管理问题。

Result: 论文结合了硬件-算法协同设计中的创新趋势，包括注意力机制、压缩技术和模块化过洗方案的创新，以提高效率和适应性。同时讨论了运行时灵活性、验证开销等实际问题。

Conclusion: 论文提出了向可扩展、可穿透、可重构FPGA解决方案的未来发展方向，这些方案能够适应不断发展的模型架构，同时保持高利用率和可预测的性能。这一综述为高级多模态AI模型与高效FPGA部署之间的间隔提供了技术基础和前瞻性视角。

Abstract: Transformers and vision-language models (VLMs) have emerged as dominant
architectures in computer vision and multimodal AI, offering state-of-the-art
performance in tasks such as image classification, object detection, visual
question answering, and caption generation. However, their high computational
complexity, large memory footprints, and irregular data access patterns present
significant challenges for deployment in latency- and power-constrained
environments. Field-programmable gate arrays (FPGAs) provide an attractive
hardware platform for such workloads due to their reconfigurability,
fine-grained parallelism, and potential for energy-efficient acceleration. This
paper presents a comprehensive review of design trade-offs, optimization
strategies, and implementation challenges for FPGA-based inference of
transformers and VLMs. We examine critical factors such as device-class
selection, memory subsystem constraints, dataflow orchestration, quantization
strategies, sparsity exploitation, and toolchain choices, alongside
modality-specific issues unique to VLMs, including heterogeneous compute
balancing and cross-attention memory management. Additionally, we discuss
emerging trends in hardware-algorithm co-design, highlighting innovations in
attention mechanisms, compression, and modular overlays to improve efficiency
and adaptability. Practical issues such as runtime flexibility, verification
overhead, and the absence of standardized FPGA multimodal benchmarks are also
considered. Finally, we outline future directions toward scalable, portable,
and reconfigurable FPGA solutions that adapt to evolving model architectures
while sustaining high utilization and predictable performance. This synthesis
offers both a technical foundation and a forward-looking perspective to help
bridge the gap between advanced multimodal AI models and efficient FPGA
deployment.

</details>


### [12] [Real-time Object Detection and Associated Hardware Accelerators Targeting Autonomous Vehicles: A Review](https://arxiv.org/abs/2509.04173)
*Safa Sali,Anis Meribout,Ashiyana Majeed,Mahmoud Meribout,Juan Pablo,Varun Tiwari,Asma Baobaid*

Main category: cs.AR

TL;DR: 这篇论文统计了自主驾驭汽车中的实时目标检测算法及硬件加速器技术，分析了以CNN为代表的AI算法在实时性、准确性和计算效率方面的进展与挑战，并持次提出学术界与商业界之间的技术差距问题。


<details>
  <summary>Details</summary>
Motivation: 自主驾驭汽车对实时目标检测的性能要求极高，需要在检测准确性、处理速度和计算资源之间取得平衡。但商业竞争导致技术保密，学术界与商业界存在明显技术差距，需要结合学术研究和商业应用做综合评估。

Method: 采用综述性评估方法，系统分析了当前最先进的实时目标检测算法（以CNN为主）和硬件加速器（GPU、ASIC等）。重点关注算法在边缘设备上的部署性能，包括处理速度、带宽需求和实际应用效果。

Result: 现有技术已能达到每秒数百帧的处理速度，但对于自主驾驭汽车多摄像头的全面检测需求，仍需要算法和硬件方面的进一步改进。商业系统与学术研究之间存在明显技术差距，导致学术成果较难直接转化为商业产品。

Conclusion: 这篇论文为自主驾驭汽车的实时目标检测技术提供了全面的技术路线图，帮助研究人员理解商业系统与学术研究之间的差距，为未来全自动驾驭汽车的设计开发提供了实用参考。论文特别强调了结合硬件加速技术的重要性，指出了技术发展的方向。

Abstract: The efficiency of object detectors depends on factors like detection
accuracy, processing time, and computational resources. Processing time is
crucial for real-time applications, particularly for autonomous vehicles (AVs),
where instantaneous responses are vital for safety. This review paper provides
a concise yet comprehensive survey of real-time object detection (OD)
algorithms for autonomous cars delving into their hardware accelerators (HAs).
Non-neural network-based algorithms, which use statistical image processing,
have been entirely substituted by AI algorithms, such as different models of
convolutional neural networks (CNNs). Their intrinsically parallel features led
them to be deployable into edge-based HAs of various types, where GPUs and, to
a lesser extent, ASIC (application-specific integrated circuit) remain the most
widely used. Throughputs of hundreds of frames/s (fps) could be reached;
however, handling object detection for all the cameras available in a typical
AV requires further hardware and algorithmic improvements. The intensive
competition between AV providers has limited the disclosure of algorithms,
firmware, and even hardware platform details. This remains a hurdle for
researchers, as commercial systems provide valuable insights while academics
undergo lengthy training and testing on restricted datasets and road scenarios.
Consequently, many AV research papers may not be reflected in end products,
being developed under limited conditions. This paper surveys state-of-the-art
OD algorithms and aims to bridge the gap with technologies in commercial AVs.
To our knowledge, this aspect has not been addressed in earlier surveys. Hence,
the paper serves as a tangible reference for researchers designing future
generations of vehicles, expected to be fully autonomous for comfort and
safety.

</details>
