{"id": "2601.04476", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.04476", "abs": "https://arxiv.org/abs/2601.04476", "authors": ["Chuanzhen Wang", "Leo Zhang", "Eric Liu"], "title": "Memory-Guided Unified Hardware Accelerator for Mixed-Precision Scientific Computing", "comment": "22 pages", "summary": "Recent hardware acceleration advances have enabled powerful specialized accelerators for finite element computations, spiking neural network inference, and sparse tensor operations. However, existing approaches face fundamental limitations: (1) finite element methods lack comprehensive rounding error analysis for reduced-precision implementations and use fixed precision assignment strategies that cannot adapt to varying numerical conditioning; (2) spiking neural network accelerators cannot handle non-spike operations and suffer from bit-width escalation as network depth increases; and (3) FPGA tensor accelerators optimize only for dense computations while requiring manual configuration for each sparsity pattern. To address these challenges, we introduce \\textbf{Memory-Guided Unified Hardware Accelerator for Mixed-Precision Scientific Computing}, a novel framework that integrates three enhanced modules with memory-guided adaptation for efficient mixed-workload processing on unified platforms. Our approach employs memory-guided precision selection to overcome fixed precision limitations, integrates experience-driven bit-width management and dynamic parallelism adaptation for enhanced spiking neural network acceleration, and introduces curriculum learning for automatic sparsity pattern discovery. Extensive experiments on FEniCS, COMSOL, ANSYS benchmarks, MNIST, CIFAR-10, CIFAR-100, DVS-Gesture datasets, and COCO 2017 demonstrate 2.8\\% improvement in numerical accuracy, 47\\% throughput increase, 34\\% energy reduction, and 45-65\\% throughput improvement compared to specialized accelerators. Our work enables unified processing of finite element methods, spiking neural networks, and sparse computations on a single platform while eliminating data transfer overhead between separate units.", "AI": {"tldr": "\u63d0\u51faMemory-Guided Unified Hardware Accelerator\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5b58\u5f15\u5bfc\u7684\u7cbe\u5ea6\u9009\u62e9\u3001\u7ecf\u9a8c\u9a71\u52a8\u7684\u4f4d\u5bbd\u7ba1\u7406\u548c\u8bfe\u7a0b\u5b66\u4e60\u81ea\u52a8\u7a00\u758f\u6a21\u5f0f\u53d1\u73b0\uff0c\u7edf\u4e00\u52a0\u901f\u6709\u9650\u5143\u65b9\u6cd5\u3001\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u548c\u7a00\u758f\u5f20\u91cf\u8ba1\u7b97\uff0c\u5728\u5355\u4e00\u5e73\u53f0\u4e0a\u5b9e\u73b0\u6df7\u5408\u7cbe\u5ea6\u79d1\u5b66\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u4e13\u7528\u52a0\u901f\u5668\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\uff1a\u6709\u9650\u5143\u65b9\u6cd5\u7f3a\u4e4f\u5168\u9762\u7684\u820d\u5165\u8bef\u5dee\u5206\u6790\u4e14\u4f7f\u7528\u56fa\u5b9a\u7cbe\u5ea6\u7b56\u7565\uff1b\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u65e0\u6cd5\u5904\u7406\u975e\u8109\u51b2\u64cd\u4f5c\u4e14\u4f4d\u5bbd\u968f\u7f51\u7edc\u6df1\u5ea6\u589e\u52a0\u800c\u6269\u5927\uff1bFPGA\u5f20\u91cf\u52a0\u901f\u5668\u4ec5\u4f18\u5316\u5bc6\u96c6\u8ba1\u7b97\u4e14\u9700\u8981\u624b\u52a8\u914d\u7f6e\u7a00\u758f\u6a21\u5f0f\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86\u6df7\u5408\u5de5\u4f5c\u8d1f\u8f7d\u5728\u7edf\u4e00\u5e73\u53f0\u4e0a\u7684\u9ad8\u6548\u5904\u7406\u3002", "method": "\u63d0\u51faMemory-Guided Unified Hardware Accelerator\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u589e\u5f3a\u6a21\u5757\uff1a1) \u5185\u5b58\u5f15\u5bfc\u7684\u7cbe\u5ea6\u9009\u62e9\uff0c\u514b\u670d\u56fa\u5b9a\u7cbe\u5ea6\u9650\u5236\uff1b2) \u7ecf\u9a8c\u9a71\u52a8\u7684\u4f4d\u5bbd\u7ba1\u7406\u548c\u52a8\u6001\u5e76\u884c\u6027\u9002\u5e94\uff0c\u589e\u5f3a\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\uff1b3) \u8bfe\u7a0b\u5b66\u4e60\u81ea\u52a8\u7a00\u758f\u6a21\u5f0f\u53d1\u73b0\u3002\u8be5\u6846\u67b6\u5728\u7edf\u4e00\u5e73\u53f0\u4e0a\u96c6\u6210\u5904\u7406\u6709\u9650\u5143\u65b9\u6cd5\u3001\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u548c\u7a00\u758f\u8ba1\u7b97\u3002", "result": "\u5728FEniCS\u3001COMSOL\u3001ANSYS\u57fa\u51c6\u6d4b\u8bd5\u3001MNIST\u3001CIFAR-10\u3001CIFAR-100\u3001DVS-Gesture\u6570\u636e\u96c6\u548cCOCO 2017\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a\u6570\u503c\u7cbe\u5ea6\u63d0\u9ad82.8%\uff0c\u541e\u5410\u91cf\u589e\u52a047%\uff0c\u80fd\u8017\u964d\u4f4e34%\uff0c\u4e0e\u4e13\u7528\u52a0\u901f\u5668\u76f8\u6bd4\u541e\u5410\u91cf\u63d0\u534745-65%\u3002\u6d88\u9664\u4e86\u5355\u72ec\u5355\u5143\u95f4\u7684\u6570\u636e\u4f20\u8f93\u5f00\u9500\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5b9e\u73b0\u4e86\u6709\u9650\u5143\u65b9\u6cd5\u3001\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u548c\u7a00\u758f\u8ba1\u7b97\u5728\u5355\u4e00\u5e73\u53f0\u4e0a\u7684\u7edf\u4e00\u5904\u7406\uff0c\u901a\u8fc7\u5185\u5b58\u5f15\u5bfc\u7684\u9002\u5e94\u673a\u5236\u89e3\u51b3\u4e86\u73b0\u6709\u4e13\u7528\u52a0\u901f\u5668\u7684\u6839\u672c\u9650\u5236\uff0c\u4e3a\u6df7\u5408\u7cbe\u5ea6\u79d1\u5b66\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6548\u7edf\u4e00\u7684\u786c\u4ef6\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.04801", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.04801", "abs": "https://arxiv.org/abs/2601.04801", "authors": ["Lei Xu", "Shanshan Wang", "Chenglong Xiao"], "title": "MPM-LLM4DSE: Reaching the Pareto Frontier in HLS with Multimodal Learning and LLM-Driven Exploration", "comment": null, "summary": "High-Level Synthesis (HLS) design space exploration (DSE) seeks Pareto-optimal designs within expansive pragma configuration spaces. To accelerate HLS DSE, graph neural networks (GNNs) are commonly employed as surrogates for HLS tools to predict quality of results (QoR) metrics, while multi-objective optimization algorithms expedite the exploration. However, GNN-based prediction methods may not fully capture the rich semantic features inherent in behavioral descriptions, and conventional multi-objective optimization algorithms often do not explicitly account for the domain-specific knowledge regarding how pragma directives influence QoR. To address these limitations, this paper proposes the MPM-LLM4DSE framework, which incorporates a multimodal prediction model (MPM) that simultaneously fuses features from behavioral descriptions and control and data flow graphs. Furthermore, the framework employs a large language model (LLM) as an optimizer, accompanied by a tailored prompt engineering methodology. This methodology incorporates pragma impact analysis on QoR to guide the LLM in generating high-quality configurations (LLM4DSE). Experimental results demonstrate that our multimodal predictive model significantly outperforms state-of-the-art work ProgSG by up to 10.25$\\times$. Furthermore, in DSE tasks, the proposed LLM4DSE achieves an average performance gain of 39.90\\% over prior methods, validating the effectiveness of our prompting methodology. Code and models are available at https://github.com/wslcccc/MPM-LLM4DSE.", "AI": {"tldr": "MPM-LLM4DSE\u6846\u67b6\uff1a\u7ed3\u5408\u591a\u6a21\u6001\u9884\u6d4b\u6a21\u578b\u548cLLM\u4f18\u5316\u5668\uff0c\u63d0\u5347HLS\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u6548\u7387", "motivation": "\u73b0\u6709HLS DSE\u65b9\u6cd5\u4e2d\uff0cGNN\u9884\u6d4b\u6a21\u578b\u65e0\u6cd5\u5145\u5206\u6355\u6349\u884c\u4e3a\u63cf\u8ff0\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u4f20\u7edf\u591a\u76ee\u6807\u4f18\u5316\u7b97\u6cd5\u7f3a\u4e4f\u5bf9pragma\u6307\u4ee4\u5982\u4f55\u5f71\u54cdQoR\u7684\u9886\u57df\u77e5\u8bc6\u8003\u8651", "method": "\u63d0\u51faMPM-LLM4DSE\u6846\u67b6\uff1a1) \u591a\u6a21\u6001\u9884\u6d4b\u6a21\u578b(MPM)\u878d\u5408\u884c\u4e3a\u63cf\u8ff0\u4e0e\u63a7\u5236\u6570\u636e\u6d41\u56fe\u7279\u5f81\uff1b2) \u4f7f\u7528LLM\u4f5c\u4e3a\u4f18\u5316\u5668\uff0c\u914d\u5408\u4e13\u95e8\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5305\u542bpragma\u5bf9QoR\u5f71\u54cd\u5206\u6790", "result": "\u591a\u6a21\u6001\u9884\u6d4b\u6a21\u578b\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5ProgSG\u63d0\u5347\u9ad8\u8fbe10.25\u500d\uff1b\u5728DSE\u4efb\u52a1\u4e2d\uff0cLLM4DSE\u6bd4\u5148\u524d\u65b9\u6cd5\u5e73\u5747\u6027\u80fd\u63d0\u534739.90%", "conclusion": "MPM-LLM4DSE\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u548cLLM\u9a71\u52a8\u7684\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86HLS\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u7684\u6548\u7387\u548c\u6548\u679c"}}
{"id": "2601.05047", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05047", "abs": "https://arxiv.org/abs/2601.05047", "authors": ["Xiaoyu Ma", "David Patterson"], "title": "Challenges and Research Directions for Large Language Model Inference Hardware", "comment": "Accepted for publication by IEEE Computer, 2026", "summary": "Large Language Model (LLM) inference is hard. The autoregressive Decode phase of the underlying Transformer model makes LLM inference fundamentally different from training. Exacerbated by recent AI trends, the primary challenges are memory and interconnect rather than compute. To address these challenges, we highlight four architecture research opportunities: High Bandwidth Flash for 10X memory capacity with HBM-like bandwidth; Processing-Near-Memory and 3D memory-logic stacking for high memory bandwidth; and low-latency interconnect to speedup communication. While our focus is datacenter AI, we also review their applicability for mobile devices.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86LLM\u63a8\u7406\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u56db\u79cd\u67b6\u6784\u7814\u7a76\u673a\u4f1a\u6765\u89e3\u51b3\u5185\u5b58\u548c\u4e92\u8fde\u74f6\u9888\u95ee\u9898", "motivation": "LLM\u63a8\u7406\u9762\u4e34\u5185\u5b58\u548c\u4e92\u8fde\u74f6\u9888\uff0c\u800c\u975e\u8ba1\u7b97\u74f6\u9888\uff0c\u8fd9\u6e90\u4e8eTransformer\u7684\u81ea\u56de\u5f52\u89e3\u7801\u7279\u6027\u4ee5\u53caAI\u53d1\u5c55\u8d8b\u52bf", "method": "\u63d0\u51fa\u56db\u79cd\u67b6\u6784\u7814\u7a76\u673a\u4f1a\uff1a\u9ad8\u5e26\u5bbd\u95ea\u5b58\u3001\u8fd1\u5185\u5b58\u5904\u7406\u30013D\u5185\u5b58\u903b\u8f91\u5806\u53e0\u3001\u4f4e\u5ef6\u8fdf\u4e92\u8fde", "result": "\u8fd9\u4e9b\u67b6\u6784\u521b\u65b0\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5185\u5b58\u5bb9\u91cf\u548c\u5e26\u5bbd\uff0c\u52a0\u901f\u901a\u4fe1\uff0c\u89e3\u51b3LLM\u63a8\u7406\u7684\u6838\u5fc3\u74f6\u9888", "conclusion": "\u9700\u8981\u65b0\u7684\u786c\u4ef6\u67b6\u6784\u6765\u5e94\u5bf9LLM\u63a8\u7406\u7684\u5185\u5b58\u548c\u4e92\u8fde\u6311\u6218\uff0c\u8fd9\u4e9b\u65b9\u6848\u65e2\u9002\u7528\u4e8e\u6570\u636e\u4e2d\u5fc3\u4e5f\u90e8\u5206\u9002\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907"}}
{"id": "2601.04327", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04327", "abs": "https://arxiv.org/abs/2601.04327", "authors": ["Erel Kaplan", "Tomer Bitan", "Lian Ghrayeb", "Le Chen", "Tom Yotam", "Niranjan Hasabnis", "Gal Oren"], "title": "ParaCodex: A Profiling-Guided Autonomous Coding Agent for Reliable Parallel Code Generation and Translation", "comment": null, "summary": "Parallel programming is central to HPC and AI, but producing code that is correct and fast remains challenging, especially for OpenMP GPU offload, where data movement and tuning dominate. Autonomous coding agents can compile, test, and profile on target hardware, but outputs are brittle without domain scaffolding.\n  We present ParaCodex, an HPC-engineer workflow that turns a Codex-based agent into an autonomous OpenMP GPU offload system using staged hotspot analysis, explicit data planning, correctness gating, and profiling-guided refinement. We evaluate translation from serial CPU kernels to OpenMP GPU offload kernels on HeCBench, Rodinia, and NAS. After excluding five kernels, ParaCodex succeeded on all 31 valid kernels. The generated kernels improved GPU time over reference OpenMP implementations in 25/31 cases, achieving geometric-mean speedups of 3x on HeCBench and 5x on Rodinia, and outperforming a zero-shot Codex baseline on all suites. We also evaluate CUDA to OpenMP offload translation on ParEval, where ParaCodex maintains high compilation and validation rates in code-only and end-to-end settings.", "AI": {"tldr": "ParaCodex\u662f\u4e00\u4e2a\u57fa\u4e8eCodex\u7684\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u7cfb\u7edf\uff0c\u4e13\u95e8\u7528\u4e8e\u5c06\u4e32\u884cCPU\u5185\u6838\u8f6c\u6362\u4e3aOpenMP GPU\u5378\u8f7d\u5185\u6838\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u70ed\u70b9\u5206\u6790\u3001\u663e\u5f0f\u6570\u636e\u89c4\u5212\u3001\u6b63\u786e\u6027\u95e8\u63a7\u548c\u6027\u80fd\u5206\u6790\u5f15\u5bfc\u7684\u4f18\u5316\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5e76\u884c\u7f16\u7a0b\u5bf9\u4e8e\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u4eba\u5de5\u667a\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f16\u5199\u6b63\u786e\u4e14\u9ad8\u6548\u7684\u4ee3\u7801\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5728OpenMP GPU\u5378\u8f7d\u65b9\u9762\uff0c\u6570\u636e\u79fb\u52a8\u548c\u8c03\u4f18\u662f\u4e3b\u8981\u96be\u70b9\u3002\u73b0\u6709\u7684\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u867d\u7136\u80fd\u5728\u76ee\u6807\u786c\u4ef6\u4e0a\u7f16\u8bd1\u3001\u6d4b\u8bd5\u548c\u5206\u6790\uff0c\u4f46\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u7684\u652f\u6491\u7ed3\u6784\uff0c\u8f93\u51fa\u7ed3\u679c\u8106\u5f31\u3002", "method": "ParaCodex\u91c7\u7528HPC\u5de5\u7a0b\u5e08\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5c06\u57fa\u4e8eCodex\u7684\u4ee3\u7406\u8f6c\u53d8\u4e3a\u81ea\u4e3b\u7684OpenMP GPU\u5378\u8f7d\u7cfb\u7edf\u3002\u65b9\u6cd5\u5305\u62ec\uff1a\u5206\u9636\u6bb5\u70ed\u70b9\u5206\u6790\u8bc6\u522b\u5173\u952e\u4ee3\u7801\u533a\u57df\u3001\u663e\u5f0f\u6570\u636e\u89c4\u5212\u4f18\u5316\u6570\u636e\u79fb\u52a8\u3001\u6b63\u786e\u6027\u95e8\u63a7\u786e\u4fdd\u4ee3\u7801\u6b63\u786e\u6027\u3001\u6027\u80fd\u5206\u6790\u5f15\u5bfc\u7684\u7ec6\u5316\u4f18\u5316\u3002\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4ece\u4e32\u884cCPU\u5185\u6838\u5230OpenMP GPU\u5378\u8f7d\u5185\u6838\u7684\u8f6c\u6362\uff0c\u4ee5\u53caCUDA\u5230OpenMP\u5378\u8f7d\u7684\u8f6c\u6362\u3002", "result": "\u5728HeCBench\u3001Rodinia\u548cNAS\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\uff0cParaCodex\u6210\u529f\u5904\u7406\u4e8631\u4e2a\u6709\u6548\u5185\u6838\u4e2d\u768431\u4e2a\uff08\u6392\u9664\u4e865\u4e2a\u5185\u6838\uff09\u3002\u751f\u6210\u7684GPU\u5185\u6838\u572825/31\u7684\u60c5\u51b5\u4e0b\u6bd4\u53c2\u8003OpenMP\u5b9e\u73b0\u66f4\u5feb\uff0c\u5728HeCBench\u4e0a\u5b9e\u73b0\u4e863\u500d\u7684\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u6bd4\uff0c\u5728Rodinia\u4e0a\u5b9e\u73b0\u4e865\u500d\u7684\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u6bd4\uff0c\u5728\u6240\u6709\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u90fd\u4f18\u4e8e\u96f6\u6837\u672cCodex\u57fa\u7ebf\u3002\u5728ParEval\u7684CUDA\u5230OpenMP\u5378\u8f7d\u8f6c\u6362\u8bc4\u4f30\u4e2d\uff0cParaCodex\u5728\u7eaf\u4ee3\u7801\u548c\u7aef\u5230\u7aef\u8bbe\u7f6e\u4e2d\u90fd\u4fdd\u6301\u4e86\u9ad8\u7f16\u8bd1\u548c\u9a8c\u8bc1\u7387\u3002", "conclusion": "ParaCodex\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u7684\u5de5\u7a0b\u5de5\u4f5c\u6d41\u7a0b\u548c\u57fa\u4e8eCodex\u7684\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\uff0c\u6210\u529f\u89e3\u51b3\u4e86OpenMP GPU\u5378\u8f7d\u7f16\u7a0b\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u4ece\u4e32\u884cCPU\u4ee3\u7801\u5230\u9ad8\u6548GPU\u5378\u8f7d\u4ee3\u7801\u7684\u81ea\u52a8\u5316\u8f6c\u6362\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u548cAI\u9886\u57df\u7684\u5e76\u884c\u7f16\u7a0b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.04492", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.04492", "abs": "https://arxiv.org/abs/2601.04492", "authors": ["Yuanzhuo Zhang", "Zhoulai Fu", "Binoy Ravindran"], "title": "Scalable Floating-Point Satisfiability via Staged Optimization", "comment": null, "summary": "This work introduces StageSAT, a new approach to solving floating-point satisfiability that bridges SMT solving with numerical optimization. StageSAT reframes a floating-point formula as a series of optimization problems in three stages of increasing precision. It begins with a fast, projection-aided descent objective to guide the search toward a feasible region, proceeding to bit-level accuracy with ULP$^2$ optimization and a final $n$-ULP lattice refinement.\n  By construction, the final stage uses a representing function that is zero if and only if a candidate satisfies all constraints. Thus, when optimization drives the objective to zero, the resulting assignment is a valid solution, providing a built-in guarantee of soundness.\n  To improve search, StageSAT introduces a partial monotone descent property on linear constraints via orthogonal projection, preventing the optimizer from stalling on flat or misleading landscapes. Critically, this solver requires no heavy bit-level reasoning or specialized abstractions; it treats complex arithmetic as a black-box, using runtime evaluations to navigate the input space.\n  We implement StageSAT and evaluate it on extensive benchmarks, including SMT-COMP'25 suites and difficult cases from prior work. StageSAT proved more scalable and accurate than state-of-the-art optimization-based alternatives. It solved strictly more formulas than any competing solver under the same time budget, finding most satisfiable instances without producing spurious models. This amounts to 99.4% recall on satisfiable cases with 0% false SAT, exceeding the reliability of prior optimization-based solvers. StageSAT also delivered significant speedups (often 5--10$\\times$) over traditional bit-precise SMT and numeric solvers. These results demonstrate that staged optimization significantly improves performance and correctness of floating-point satisfiability solving.", "AI": {"tldr": "StageSAT\uff1a\u4e00\u79cd\u65b0\u7684\u6d6e\u70b9\u53ef\u6ee1\u8db3\u6027\u6c42\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u4f18\u5316\u5c06SMT\u6c42\u89e3\u4e0e\u6570\u503c\u4f18\u5316\u7ed3\u5408\uff0c\u65e0\u9700\u4f4d\u7ea7\u63a8\u7406\uff0c\u5728\u6027\u80fd\u548c\u6b63\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6d6e\u70b9\u53ef\u6ee1\u8db3\u6027\u6c42\u89e3\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u74f6\u9888\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u9700\u8981\u7ed3\u5408SMT\u6c42\u89e3\u7684\u7cbe\u786e\u6027\u548c\u6570\u503c\u4f18\u5316\u7684\u9ad8\u6548\u6027\uff0c\u540c\u65f6\u907f\u514d\u590d\u6742\u7684\u4f4d\u7ea7\u63a8\u7406\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\uff1a1\uff09\u5feb\u901f\u6295\u5f71\u8f85\u52a9\u4e0b\u964d\u5f15\u5bfc\u641c\u7d22\u5230\u53ef\u884c\u533a\u57df\uff1b2\uff09\u4f4d\u7ea7\u7cbe\u5ea6\u7684ULP\u00b2\u4f18\u5316\uff1b3\uff09n-ULP\u683c\u70b9\u7cbe\u70bc\u3002\u901a\u8fc7\u6b63\u4ea4\u6295\u5f71\u5f15\u5165\u90e8\u5206\u5355\u8c03\u4e0b\u964d\u7279\u6027\uff0c\u5c06\u590d\u6742\u7b97\u672f\u89c6\u4e3a\u9ed1\u76d2\u3002", "result": "\u5728SMT-COMP'25\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cStageSAT\u6bd4\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u66f4\u591a\u516c\u5f0f\uff0c\u5728\u53ef\u6ee1\u8db3\u6848\u4f8b\u4e2d\u8fbe\u523099.4%\u53ec\u56de\u7387\u4e140%\u8bef\u62a5\uff0c\u901f\u5ea6\u6bd4\u4f20\u7edf\u4f4d\u7cbe\u786eSMT\u548c\u6570\u503c\u6c42\u89e3\u5668\u5feb5-10\u500d\u3002", "conclusion": "\u5206\u9636\u6bb5\u4f18\u5316\u663e\u8457\u63d0\u9ad8\u4e86\u6d6e\u70b9\u53ef\u6ee1\u8db3\u6027\u6c42\u89e3\u7684\u6027\u80fd\u548c\u6b63\u786e\u6027\uff0c\u4e3a\u6d6e\u70b9\u7ea6\u675f\u6c42\u89e3\u63d0\u4f9b\u4e86\u53ef\u9760\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.04349", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.04349", "abs": "https://arxiv.org/abs/2601.04349", "authors": ["Xaver Stiensmeier", "Alexander Kanitz", "Jan Kr\u00fcger", "Santiago Insua", "Adri\u00e1n Ro\u0161inec", "Vikt\u00f3ria Spi\u0161\u00e1kov\u00e1", "Luk\u00e1\u0161 Hejtm\u00e1nek", "David Yuan", "Gavin Farrell", "Jonathan Tedds", "Juha T\u00f6rnroos", "Harald Wagener", "Alex Sczyrba", "Nils Hoffmann", "Matej Antol"], "title": "Hybrid Cloud Architectures for Research Computing: Applications and Use Cases", "comment": null, "summary": "Scientific research increasingly depends on robust and scalable IT infrastructures to support complex computational workflows. With the proliferation of services provided by research infrastructures, NRENs, and commercial cloud providers, researchers must navigate a fragmented ecosystem of computing environments, balancing performance, cost, scalability, and accessibility. Hybrid cloud architectures offer a compelling solution by integrating multiple computing environments to enhance flexibility, resource efficiency, and access to specialised hardware.\n  This paper provides a comprehensive overview of hybrid cloud deployment models, focusing on grid and cloud platforms (OpenPBS, SLURM, OpenStack, Kubernetes) and workflow management tools (Nextflow, Snakemake, CWL). We explore strategies for federated computing, multi-cloud orchestration, and workload scheduling, addressing key challenges such as interoperability, data security, reproducibility, and network performance. Drawing on implementations from life sciences, as coordinated by the ELIXIR Compute Platform and their integration into a wider EOSC context, we propose a roadmap for accelerating hybrid cloud adoption in research computing, emphasising governance frameworks and technical solutions that can drive sustainable and scalable infrastructure development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u6df7\u5408\u4e91\u5728\u79d1\u7814\u8ba1\u7b97\u4e2d\u7684\u90e8\u7f72\u6a21\u578b\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u7f51\u683c\u4e0e\u4e91\u5e73\u53f0\uff08OpenPBS\u3001SLURM\u3001OpenStack\u3001Kubernetes\uff09\u548c\u5de5\u4f5c\u6d41\u7ba1\u7406\u5de5\u5177\uff08Nextflow\u3001Snakemake\u3001CWL\uff09\uff0c\u63a2\u8ba8\u4e86\u8054\u90a6\u8ba1\u7b97\u3001\u591a\u4e91\u7f16\u6392\u548c\u8d1f\u8f7d\u8c03\u5ea6\u7b56\u7565\uff0c\u5e76\u57fa\u4e8e\u751f\u547d\u79d1\u5b66\u9886\u57df\u7684\u5b9e\u65bd\u6848\u4f8b\u63d0\u51fa\u4e86\u6df7\u5408\u4e91\u5728\u79d1\u7814\u4e2d\u52a0\u901f\u91c7\u7528\u7684\u8def\u7ebf\u56fe\u3002", "motivation": "\u968f\u7740\u79d1\u7814\u5bf9IT\u57fa\u7840\u8bbe\u65bd\u4f9d\u8d56\u7684\u52a0\u6df1\uff0c\u7814\u7a76\u4eba\u5458\u9762\u4e34\u5206\u6563\u7684\u8ba1\u7b97\u73af\u5883\u751f\u6001\u7cfb\u7edf\uff0c\u9700\u8981\u5728\u6027\u80fd\u3001\u6210\u672c\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u6df7\u5408\u4e91\u67b6\u6784\u901a\u8fc7\u6574\u5408\u591a\u79cd\u8ba1\u7b97\u73af\u5883\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u80fd\u8bbf\u95ee\u4e13\u7528\u786c\u4ef6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u91c7\u7528\u7efc\u8ff0\u5206\u6790\u65b9\u6cd5\uff0c\u5168\u9762\u6982\u8ff0\u6df7\u5408\u4e91\u90e8\u7f72\u6a21\u578b\uff0c\u91cd\u70b9\u7814\u7a76\u7f51\u683c\u548c\u4e91\u5e73\u53f0\uff08OpenPBS\u3001SLURM\u3001OpenStack\u3001Kubernetes\uff09\u4ee5\u53ca\u5de5\u4f5c\u6d41\u7ba1\u7406\u5de5\u5177\uff08Nextflow\u3001Snakemake\u3001CWL\uff09\u3002\u63a2\u8ba8\u8054\u90a6\u8ba1\u7b97\u3001\u591a\u4e91\u7f16\u6392\u548c\u8d1f\u8f7d\u8c03\u5ea6\u7b56\u7565\uff0c\u5e76\u57fa\u4e8eELIXIR\u8ba1\u7b97\u5e73\u53f0\u5728\u751f\u547d\u79d1\u5b66\u9886\u57df\u7684\u5b9e\u65bd\u6848\u4f8b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u8bba\u6587\u8bc6\u522b\u4e86\u6df7\u5408\u4e91\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u4e92\u64cd\u4f5c\u6027\u3001\u6570\u636e\u5b89\u5168\u3001\u53ef\u91cd\u590d\u6027\u548c\u7f51\u7edc\u6027\u80fd\u7b49\u95ee\u9898\u3002\u901a\u8fc7\u5206\u6790ELIXIR\u8ba1\u7b97\u5e73\u53f0\u5728EOSC\u66f4\u5e7f\u6cdb\u80cc\u666f\u4e0b\u7684\u96c6\u6210\u6848\u4f8b\uff0c\u63d0\u51fa\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u7684\u6280\u672f\u65b9\u6848\u548c\u7b56\u7565\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u52a0\u901f\u79d1\u7814\u8ba1\u7b97\u4e2d\u6df7\u5408\u4e91\u91c7\u7528\u7684\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u6cbb\u7406\u6846\u67b6\u548c\u6280\u672f\u89e3\u51b3\u65b9\u6848\u5bf9\u4e8e\u63a8\u52a8\u53ef\u6301\u7eed\u548c\u53ef\u6269\u5c55\u57fa\u7840\u8bbe\u65bd\u53d1\u5c55\u7684\u91cd\u8981\u6027\u3002\u6df7\u5408\u4e91\u67b6\u6784\u80fd\u591f\u6709\u6548\u6574\u5408\u7814\u7a76\u57fa\u7840\u8bbe\u65bd\u3001NREN\u548c\u5546\u4e1a\u4e91\u63d0\u4f9b\u5546\u7684\u670d\u52a1\uff0c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u63d0\u4f9b\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.04573", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.04573", "abs": "https://arxiv.org/abs/2601.04573", "authors": ["Kazutaka Matsuda", "Minh Nguyen", "Meng Wang"], "title": "Lenses for Partially-Specified States (Extended Version)", "comment": "Extended version of our paper to appear in ESOP 2026", "summary": "A bidirectional transformation is a pair of transformations satisfying certain well-behavedness properties: one maps source data into view data, and the other translates changes on the view back to the source. However, when multiple views share a source, an update on one view may affect the others, making it hard to maintain correspondence while preserving the user's update, especially when multiple views are changed at once. Ensuring these properties within a compositional framework is even more challenging. In this paper, we propose partial-state lenses, which allow source and view states to be partially specified to precisely represent the user's update intentions. These intentions are partially ordered, providing clear semantics for merging intentions of updates coming from multiple views and a refined notion of update preservation compatible with this merging. We formalize partial-state lenses, together with partial-specifiedness-aware well-behavedness that supports compositional reasoning and ensures update preservation. In addition, we demonstrate the utility of the proposed system through examples.", "AI": {"tldr": "\u63d0\u51fa\u90e8\u5206\u72b6\u6001\u900f\u955c\uff0c\u901a\u8fc7\u90e8\u5206\u6307\u5b9a\u6e90\u548c\u89c6\u56fe\u72b6\u6001\u6765\u7cbe\u786e\u8868\u793a\u7528\u6237\u66f4\u65b0\u610f\u56fe\uff0c\u652f\u6301\u591a\u89c6\u56fe\u66f4\u65b0\u5408\u5e76\uff0c\u5e76\u786e\u4fdd\u66f4\u65b0\u4fdd\u6301\u6027\u3002", "motivation": "\u5728\u53cc\u5411\u8f6c\u6362\u4e2d\uff0c\u5f53\u591a\u4e2a\u89c6\u56fe\u5171\u4eab\u4e00\u4e2a\u6e90\u65f6\uff0c\u4e00\u4e2a\u89c6\u56fe\u7684\u66f4\u65b0\u4f1a\u5f71\u54cd\u5176\u4ed6\u89c6\u56fe\uff0c\u96be\u4ee5\u5728\u4fdd\u6301\u7528\u6237\u66f4\u65b0\u7684\u540c\u65f6\u7ef4\u6301\u5bf9\u5e94\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5728\u591a\u4e2a\u89c6\u56fe\u540c\u65f6\u66f4\u6539\u65f6\u3002\u5728\u7ec4\u5408\u6846\u67b6\u4e2d\u786e\u4fdd\u8fd9\u4e9b\u5c5e\u6027\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u90e8\u5206\u72b6\u6001\u900f\u955c\uff0c\u5141\u8bb8\u90e8\u5206\u6307\u5b9a\u6e90\u548c\u89c6\u56fe\u72b6\u6001\u6765\u7cbe\u786e\u8868\u793a\u7528\u6237\u66f4\u65b0\u610f\u56fe\u3002\u8fd9\u4e9b\u610f\u56fe\u91c7\u7528\u504f\u5e8f\u5173\u7cfb\uff0c\u4e3a\u5408\u5e76\u6765\u81ea\u591a\u4e2a\u89c6\u56fe\u7684\u66f4\u65b0\u610f\u56fe\u63d0\u4f9b\u6e05\u6670\u8bed\u4e49\uff0c\u5e76\u5b9a\u4e49\u4e0e\u8fd9\u79cd\u5408\u5e76\u517c\u5bb9\u7684\u66f4\u65b0\u4fdd\u6301\u6027\u6982\u5ff5\u3002", "result": "\u5f62\u5f0f\u5316\u90e8\u5206\u72b6\u6001\u900f\u955c\uff0c\u4ee5\u53ca\u652f\u6301\u7ec4\u5408\u63a8\u7406\u5e76\u786e\u4fdd\u66f4\u65b0\u4fdd\u6301\u6027\u7684\u90e8\u5206\u6307\u5b9a\u611f\u77e5\u826f\u597d\u884c\u4e3a\u6027\u3002\u901a\u8fc7\u793a\u4f8b\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u90e8\u5206\u72b6\u6001\u900f\u955c\u4e3a\u89e3\u51b3\u591a\u89c6\u56fe\u53cc\u5411\u8f6c\u6362\u4e2d\u7684\u66f4\u65b0\u5408\u5e76\u548c\u4fdd\u6301\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u652f\u6301\u7ec4\u5408\u63a8\u7406\u5e76\u786e\u4fdd\u66f4\u65b0\u8bed\u4e49\u7684\u7cbe\u786e\u6027\u3002"}}
{"id": "2601.04523", "categories": ["cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.04523", "abs": "https://arxiv.org/abs/2601.04523", "authors": ["Ajay Singh", "Nikos Metaxakis", "Panagiota Fatourou"], "title": "Sharded Elimination and Combining for Highly-Efficient Concurrent Stacks", "comment": "extended version of paper in PPoPP 2026", "summary": "We present a new blocking linearizable stack implementation which utilizes sharding and fetch&increment to achieve significantly better performance than all existing concurrent stacks. The proposed implementation is based on a novel elimination mechanism and a new combining approach that are efficiently blended to gain high performance. Our implementation results in enhanced parallelism and low contention when accessing the shared stack. Experiments show that the proposed stack implementation outperforms all existing concurrent stacks by up to 2X in most workloads. It is particularly efficient in systems supporting a large number of threads and in high contention scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u963b\u585e\u7ebf\u6027\u5316\u5806\u6808\u5b9e\u73b0\uff0c\u901a\u8fc7\u5206\u7247\u548cfetch&increment\u6280\u672f\uff0c\u6027\u80fd\u6bd4\u73b0\u6709\u5e76\u53d1\u5806\u6808\u63d0\u5347\u9ad8\u8fbe2\u500d", "motivation": "\u73b0\u6709\u5e76\u53d1\u5806\u6808\u5728\u5927\u91cf\u7ebf\u7a0b\u548c\u9ad8\u7ade\u4e89\u573a\u666f\u4e0b\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u63d0\u4f9b\u66f4\u597d\u5e76\u884c\u6027\u548c\u66f4\u4f4e\u7ade\u4e89\u7684\u65b0\u5b9e\u73b0", "method": "\u91c7\u7528\u5206\u7247\u6280\u672f\u3001fetch&increment\u64cd\u4f5c\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u6d88\u9664\u673a\u5236\u548c\u7ec4\u5408\u65b9\u6cd5\uff0c\u6709\u6548\u6df7\u5408\u8fd9\u4e9b\u6280\u672f\u4ee5\u83b7\u5f97\u9ad8\u6027\u80fd", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u5b9e\u73b0\u6bd4\u6240\u6709\u73b0\u6709\u5e76\u53d1\u5806\u6808\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe2\u500d\uff0c\u5728\u652f\u6301\u5927\u91cf\u7ebf\u7a0b\u7684\u7cfb\u7edf\u548c\u9ad8\u7ade\u4e89\u573a\u666f\u4e2d\u7279\u522b\u9ad8\u6548", "conclusion": "\u63d0\u51fa\u7684\u5806\u6808\u5b9e\u73b0\u901a\u8fc7\u521b\u65b0\u7684\u6d88\u9664\u673a\u5236\u548c\u7ec4\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e76\u53d1\u5806\u6808\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5e76\u53d1\u73af\u5883\u4e0b"}}
{"id": "2601.05012", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.05012", "abs": "https://arxiv.org/abs/2601.05012", "authors": ["Luke A. D. Hutchison"], "title": "The Squirrel Parser: A Linear-Time PEG Packrat Parser Capable of Left Recursion and Optimal Error Recovery", "comment": null, "summary": "We present the squirrel parser, a PEG packrat parser that directly handles all forms of left recursion with optimal error recovery, while maintaining linear time complexity in the length of the input even in the presence of an arbitrary number of errors. Traditional approaches to handling left recursion in a recursive descent parser require grammar rewriting or complex algorithmic extensions. We derive a minimal algorithm from first principles: cycle detection via per-position state tracking and $O(1)$-per-LR-cycle communication from descendant to ancestor recursion frames, and fixed-point search via iterative expansion. For error recovery, we derived a set of four axioms and twelve constraints that must be imposed upon an optimal error recovery design to ensure completeness, correctness, optimality of performance, and intuitiveness of behavior. We utilized a constraint satisfaction mechanism to search the space of all possibilities, arriving at a provably optimal and robust error recovery strategy that maintains perfect performance linearity.", "AI": {"tldr": "Squirrel parser\uff1a\u4e00\u4e2aPEG packrat\u89e3\u6790\u5668\uff0c\u80fd\u76f4\u63a5\u5904\u7406\u6240\u6709\u5f62\u5f0f\u7684\u5de6\u9012\u5f52\u5e76\u5b9e\u73b0\u6700\u4f18\u9519\u8bef\u6062\u590d\uff0c\u540c\u65f6\u4fdd\u6301\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5904\u7406\u5de6\u9012\u5f52\u9700\u8981\u8bed\u6cd5\u91cd\u5199\u6216\u590d\u6742\u7b97\u6cd5\u6269\u5c55\uff0c\u73b0\u6709\u89e3\u6790\u5668\u5728\u5904\u7406\u5de6\u9012\u5f52\u548c\u9519\u8bef\u6062\u590d\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u57fa\u4e8e\u7b2c\u4e00\u539f\u7406\u63a8\u5bfc\u6700\u5c0f\u7b97\u6cd5\uff1a\u901a\u8fc7\u6bcf\u4f4d\u7f6e\u72b6\u6001\u8ddf\u8e2a\u8fdb\u884c\u5faa\u73af\u68c0\u6d4b\uff0c\u540e\u4ee3\u5230\u7956\u5148\u9012\u5f52\u5e27\u7684O(1)\u901a\u4fe1\uff0c\u4ee5\u53ca\u901a\u8fc7\u8fed\u4ee3\u6269\u5c55\u8fdb\u884c\u5b9a\u70b9\u641c\u7d22\u3002\u9519\u8bef\u6062\u590d\u65b9\u9762\uff0c\u63a8\u5bfc\u4e86\u56db\u4e2a\u516c\u7406\u548c\u5341\u4e8c\u4e2a\u7ea6\u675f\uff0c\u4f7f\u7528\u7ea6\u675f\u6ee1\u8db3\u673a\u5236\u641c\u7d22\u6240\u6709\u53ef\u80fd\u6027\u7a7a\u95f4\u3002", "result": "\u5b9e\u73b0\u4e86\u80fd\u76f4\u63a5\u5904\u7406\u6240\u6709\u5f62\u5f0f\u5de6\u9012\u5f52\u7684\u89e3\u6790\u5668\uff0c\u5177\u6709\u6700\u4f18\u9519\u8bef\u6062\u590d\u80fd\u529b\uff0c\u5373\u4f7f\u5728\u4efb\u610f\u6570\u91cf\u9519\u8bef\u5b58\u5728\u65f6\u4e5f\u80fd\u4fdd\u6301\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "conclusion": "Squirrel\u89e3\u6790\u5668\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u4e0a\u6700\u4f18\u4e14\u9c81\u68d2\u7684\u9519\u8bef\u6062\u590d\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u5b8c\u7f8e\u6027\u80fd\u7ebf\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5b8c\u6574\u7684\u5de6\u9012\u5f52\u5904\u7406\u548c\u76f4\u89c2\u7684\u884c\u4e3a\u3002"}}
{"id": "2601.04659", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.04659", "abs": "https://arxiv.org/abs/2601.04659", "authors": ["Gijun Park"], "title": "Quantifying Autoscaler Vulnerabilities: An Empirical Study of Resource Misallocation Induced by Cloud Infrastructure Faults", "comment": null, "summary": "Resource autoscaling mechanisms in cloud environments depend on accurate performance metrics to make optimal provisioning decisions. When infrastructure faults including hardware malfunctions, network disruptions, and software anomalies corrupt these metrics, autoscalers may systematically over- or under-provision resources, resulting in elevated operational expenses or degraded service reliability. This paper conducts controlled simulation experiments to measure how four prevalent fault categories affect both vertical and horizontal autoscaling behaviors across multiple instance configurations and service level objective (SLO) thresholds. Experimental findings demonstrate that storage-related faults generate the largest cost overhead, adding up to $258 monthly under horizontal scaling policies, whereas routing anomalies consistently bias autoscalers toward insufficient resource allocation. The sensitivity to fault-induced metric distortions differs markedly between scaling strategies: horizontal autoscaling exhibits greater susceptibility to transient anomalies, particularly near threshold boundaries. These empirically-grounded insights offer actionable recommendations for designing fault-tolerant autoscaling policies that distinguish genuine workload fluctuations from failure artifacts.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u5206\u6790\u56db\u79cd\u5e38\u89c1\u57fa\u7840\u8bbe\u65bd\u6545\u969c\u5bf9\u4e91\u8d44\u6e90\u81ea\u52a8\u6269\u7f29\u5bb9\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5b58\u50a8\u6545\u969c\u5bfc\u81f4\u6700\u9ad8\u6210\u672c\u5f00\u9500\uff0c\u8def\u7531\u6545\u969c\u5bfc\u81f4\u8d44\u6e90\u5206\u914d\u4e0d\u8db3\uff0c\u6c34\u5e73\u6269\u7f29\u5bf9\u77ac\u6001\u5f02\u5e38\u66f4\u654f\u611f\u3002", "motivation": "\u4e91\u73af\u5883\u4e2d\u7684\u8d44\u6e90\u81ea\u52a8\u6269\u7f29\u673a\u5236\u4f9d\u8d56\u51c6\u786e\u7684\u6027\u80fd\u6307\u6807\u6765\u505a\u51fa\u6700\u4f18\u8d44\u6e90\u914d\u7f6e\u51b3\u7b56\u3002\u5f53\u57fa\u7840\u8bbe\u65bd\u6545\u969c\uff08\u5305\u62ec\u786c\u4ef6\u6545\u969c\u3001\u7f51\u7edc\u4e2d\u65ad\u548c\u8f6f\u4ef6\u5f02\u5e38\uff09\u635f\u574f\u8fd9\u4e9b\u6307\u6807\u65f6\uff0c\u81ea\u52a8\u6269\u7f29\u5668\u53ef\u80fd\u4f1a\u7cfb\u7edf\u6027\u5730\u8fc7\u5ea6\u6216\u4e0d\u8db3\u914d\u7f6e\u8d44\u6e90\uff0c\u5bfc\u81f4\u8fd0\u8425\u6210\u672c\u589e\u52a0\u6216\u670d\u52a1\u53ef\u9760\u6027\u4e0b\u964d\u3002", "method": "\u901a\u8fc7\u53d7\u63a7\u6a21\u62df\u5b9e\u9a8c\uff0c\u6d4b\u91cf\u56db\u79cd\u5e38\u89c1\u6545\u969c\u7c7b\u522b\u5bf9\u5782\u76f4\u548c\u6c34\u5e73\u81ea\u52a8\u6269\u7f29\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u6db5\u76d6\u591a\u79cd\u5b9e\u4f8b\u914d\u7f6e\u548c\u670d\u52a1\u6c34\u5e73\u76ee\u6807\uff08SLO\uff09\u9608\u503c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u5b58\u50a8\u76f8\u5173\u6545\u969c\u4ea7\u751f\u6700\u5927\u7684\u6210\u672c\u5f00\u9500\uff0c\u5728\u6c34\u5e73\u6269\u7f29\u7b56\u7565\u4e0b\u6bcf\u6708\u589e\u52a0\u9ad8\u8fbe258\u7f8e\u5143\uff1b\u8def\u7531\u5f02\u5e38\u6301\u7eed\u5bfc\u81f4\u81ea\u52a8\u6269\u7f29\u5668\u504f\u5411\u8d44\u6e90\u5206\u914d\u4e0d\u8db3\uff1b\u4e0d\u540c\u6269\u7f29\u7b56\u7565\u5bf9\u6545\u969c\u5f15\u8d77\u7684\u6307\u6807\u626d\u66f2\u7684\u654f\u611f\u6027\u5dee\u5f02\u663e\u8457\uff0c\u6c34\u5e73\u6269\u7f29\u5bf9\u77ac\u6001\u5f02\u5e38\u66f4\u654f\u611f\uff0c\u7279\u522b\u662f\u5728\u9608\u503c\u8fb9\u754c\u9644\u8fd1\u3002", "conclusion": "\u8fd9\u4e9b\u57fa\u4e8e\u5b9e\u8bc1\u7684\u89c1\u89e3\u4e3a\u8bbe\u8ba1\u5bb9\u9519\u7684\u81ea\u52a8\u6269\u7f29\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u884c\u5efa\u8bae\uff0c\u80fd\u591f\u533a\u5206\u771f\u5b9e\u7684\u5de5\u4f5c\u8d1f\u8f7d\u6ce2\u52a8\u548c\u6545\u969c\u4f2a\u5f71\u3002"}}
{"id": "2601.04750", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.04750", "abs": "https://arxiv.org/abs/2601.04750", "authors": ["Krishna Chaitanya Sunkara"], "title": "Cognitive Infrastructure: A Unified DCIM Framework for AI Data Centers", "comment": "71 pages, 10 figures, 5 tables, 9 chapters including cases study. Published independently under Creative Commons BY 4.0. Includes comprehensive technical diagrams, quantitative models, JSON schema specifications, and production deployment validation.This is comprehensive manuscript synthesizing original research and systems engineering practices in AI Scale data center infrastructure management", "summary": "This work presents DCIM 3.0, a unified framework integrating semantic reasoning, predictive analytics, autonomous orchestration, and unified connectivity for next-generation AI data center management. The framework addresses critical challenges in infrastructure automation, sustainability, and digital-twin design through knowledge graph-based intelligence, thermal modeling, and the Unified Device Connectivity Protocol (UDCP).Keywords-Data Center Infrastructure Management, DCIM, AI Data Centers, Knowledge Graphs, Digital Twin, Thermal Management, Infrastructure Automation, Sustainability, GPU Computing, Data Center", "AI": {"tldr": "DCIM 3.0\u662f\u4e00\u4e2a\u96c6\u6210\u4e86\u8bed\u4e49\u63a8\u7406\u3001\u9884\u6d4b\u5206\u6790\u3001\u81ea\u4e3b\u7f16\u6392\u548c\u7edf\u4e00\u8fde\u63a5\u7684\u4e0b\u4e00\u4ee3AI\u6570\u636e\u4e2d\u5fc3\u7ba1\u7406\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u3001\u70ed\u5efa\u6a21\u548c\u7edf\u4e00\u8bbe\u5907\u8fde\u63a5\u534f\u8bae\u89e3\u51b3\u57fa\u7840\u8bbe\u65bd\u81ea\u52a8\u5316\u3001\u53ef\u6301\u7eed\u6027\u548c\u6570\u5b57\u5b6a\u751f\u8bbe\u8ba1\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u4e0b\u4e00\u4ee3AI\u6570\u636e\u4e2d\u5fc3\u5728\u57fa\u7840\u8bbe\u65bd\u81ea\u52a8\u5316\u3001\u53ef\u6301\u7eed\u6027\u548c\u6570\u5b57\u5b6a\u751f\u8bbe\u8ba1\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\uff0c\u4f20\u7edf\u6570\u636e\u4e2d\u5fc3\u7ba1\u7406\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9GPU\u8ba1\u7b97\u7b49\u65b0\u5174\u9700\u6c42\u3002", "method": "\u63d0\u51faDCIM 3.0\u7edf\u4e00\u6846\u67b6\uff0c\u96c6\u6210\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u8bed\u4e49\u63a8\u7406\u3001\u9884\u6d4b\u5206\u6790\u3001\u81ea\u4e3b\u7f16\u6392\u5f15\u64ce\u548c\u7edf\u4e00\u8bbe\u5907\u8fde\u63a5\u534f\u8bae(UDCP)\uff0c\u7ed3\u5408\u70ed\u5efa\u6a21\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u5904\u7406AI\u6570\u636e\u4e2d\u5fc3\u590d\u6742\u7ba1\u7406\u9700\u6c42\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u57fa\u7840\u8bbe\u65bd\u7684\u667a\u80fd\u81ea\u52a8\u5316\u7ba1\u7406\u3001\u70ed\u7ba1\u7406\u4f18\u5316\u548c\u53ef\u6301\u7eed\u6027\u63d0\u5347\u3002", "conclusion": "DCIM 3.0\u4e3a\u4e0b\u4e00\u4ee3AI\u6570\u636e\u4e2d\u5fc3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u4e86\u81ea\u52a8\u5316\u3001\u53ef\u6301\u7eed\u6027\u548c\u6570\u5b57\u5b6a\u751f\u8bbe\u8ba1\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u3001\u667a\u80fd\u7684\u6570\u636e\u4e2d\u5fc3\u8fd0\u8425\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.04813", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.04813", "abs": "https://arxiv.org/abs/2601.04813", "authors": ["Homayoun Maleki", "Nekane Sainz", "Jon Legarda"], "title": "Proof of Commitment: A Human-Centric Resource for Permissionless Consensus", "comment": null, "summary": "Permissionless consensus protocols require a scarce resource to regulate leader election and provide Sybil resistance. Existing paradigms such as Proof of Work and Proof of Stake instantiate this scarcity through parallelizable resources like computation or capital. Once acquired, these resources can be subdivided across many identities at negligible marginal cost, making linear Sybil cost fundamentally unattainable.\n  We introduce Proof of Commitment (PoCmt), a consensus primitive grounded in a non-parallelizable resource: real-time human engagement. Validators maintain a commitment state capturing cumulative human effort, protocol participation, and online availability. Engagement is enforced through a Human Challenge Oracle that issues identity-bound, time-sensitive challenges, limiting the number of challenges solvable within each human window.\n  Under this model, sustaining multiple active identities requires proportional human-time effort. We establish a cost-theoretic separation showing that protocols based on parallelizable resources admit zero marginal Sybil cost, whereas PoCmt enforces a strictly linear cost profile. Using a weighted-backbone analysis, we show that PoCmt achieves safety, liveness, and commitment-proportional fairness under partial synchrony.\n  Simulations complement the analysis by isolating human-time capacity as the sole adversarial bottleneck and validating the predicted commitment drift and fairness properties. These results position PoCmt as a new point in the consensus design space, grounding permissionless security in sustained human effort rather than computation or capital.", "AI": {"tldr": "PoCmt\u662f\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u5b9e\u65f6\u53c2\u4e0e\uff08\u975e\u5e76\u884c\u8d44\u6e90\uff09\u7684\u65b0\u578b\u5171\u8bc6\u539f\u8bed\uff0c\u901a\u8fc7\u4eba\u7c7b\u6311\u6218\u9884\u8a00\u673a\u5f3a\u5236\u6267\u884c\u8eab\u4efd\u7ed1\u5b9a\u3001\u65f6\u95f4\u654f\u611f\u6311\u6218\uff0c\u4f7f\u7ef4\u6301\u591a\u4e2a\u6d3b\u8dc3\u8eab\u4efd\u9700\u8981\u6210\u6bd4\u4f8b\u7684\u4eba\u7c7b\u65f6\u95f4\u6210\u672c\uff0c\u4ece\u800c\u5728\u90e8\u5206\u540c\u6b65\u7f51\u7edc\u4e2d\u5b9e\u73b0\u5b89\u5168\u3001\u6d3b\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5e76\u884c\u8d44\u6e90\uff08\u5982\u7b97\u529b\u6216\u8d44\u672c\uff09\u7684\u5171\u8bc6\u534f\u8bae\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff1a\u4e00\u65e6\u83b7\u5f97\u8fd9\u4e9b\u8d44\u6e90\uff0c\u53ef\u4ee5\u4ee5\u8fd1\u4e4e\u96f6\u8fb9\u9645\u6210\u672c\u5206\u914d\u7ed9\u591a\u4e2a\u8eab\u4efd\uff0c\u65e0\u6cd5\u5b9e\u73b0\u7ebf\u6027\u5973\u5deb\u653b\u51fb\u6210\u672c\u3002\u9700\u8981\u4e00\u79cd\u57fa\u4e8e\u975e\u5e76\u884c\u8d44\u6e90\u7684\u65b0\u5171\u8bc6\u8303\u5f0f\u3002", "method": "\u63d0\u51faProof of Commitment (PoCmt)\u5171\u8bc6\u539f\u8bed\uff0c\u57fa\u4e8e\u4eba\u7c7b\u5b9e\u65f6\u53c2\u4e0e\u8fd9\u4e00\u975e\u5e76\u884c\u8d44\u6e90\u3002\u9a8c\u8bc1\u8005\u7ef4\u62a4\u5305\u542b\u7d2f\u8ba1\u4eba\u7c7b\u52aa\u529b\u3001\u534f\u8bae\u53c2\u4e0e\u548c\u5728\u7ebf\u53ef\u7528\u6027\u7684\u627f\u8bfa\u72b6\u6001\u3002\u901a\u8fc7\u4eba\u7c7b\u6311\u6218\u9884\u8a00\u673a\u5f3a\u5236\u6267\u884c\u8eab\u4efd\u7ed1\u5b9a\u3001\u65f6\u95f4\u654f\u611f\u6311\u6218\uff0c\u9650\u5236\u6bcf\u4e2a\u65f6\u95f4\u7a97\u53e3\u5185\u53ef\u89e3\u51b3\u7684\u6311\u6218\u6570\u91cf\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793a\uff1a\u57fa\u4e8e\u5e76\u884c\u8d44\u6e90\u7684\u534f\u8bae\u5141\u8bb8\u96f6\u8fb9\u9645\u5973\u5deb\u6210\u672c\uff0c\u800cPoCmt\u5f3a\u5236\u6267\u884c\u4e25\u683c\u7ebf\u6027\u6210\u672c\u66f2\u7ebf\u3002\u52a0\u6743\u9aa8\u5e72\u5206\u6790\u8bc1\u660ePoCmt\u5728\u90e8\u5206\u540c\u6b65\u7f51\u7edc\u4e2d\u5b9e\u73b0\u5b89\u5168\u3001\u6d3b\u6027\u548c\u627f\u8bfa\u6bd4\u4f8b\u516c\u5e73\u6027\u3002\u4eff\u771f\u9a8c\u8bc1\u4e86\u4eba\u7c7b\u65f6\u95f4\u5bb9\u91cf\u4f5c\u4e3a\u552f\u4e00\u5bf9\u6297\u74f6\u9888\uff0c\u4ee5\u53ca\u627f\u8bfa\u6f02\u79fb\u548c\u516c\u5e73\u6027\u7279\u6027\u3002", "conclusion": "PoCmt\u4e3a\u5171\u8bc6\u8bbe\u8ba1\u7a7a\u95f4\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5c06\u65e0\u8bb8\u53ef\u5b89\u5168\u6027\u5efa\u7acb\u5728\u6301\u7eed\u7684\u4eba\u7c7b\u52aa\u529b\u800c\u975e\u8ba1\u7b97\u6216\u8d44\u672c\u4e0a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8303\u5f0f\u4e2d\u7684\u5973\u5deb\u653b\u51fb\u6210\u672c\u95ee\u9898\u3002"}}
{"id": "2601.04904", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.04904", "abs": "https://arxiv.org/abs/2601.04904", "authors": ["Vincent Maillou", "Matthias Bollhofer", "Olaf Schenk", "Alexandros Nikolaos Ziogas", "Mathieu Luisier"], "title": "Parallel Quadratic Selected Inversion in Quantum Transport Simulation", "comment": "12 pages, 9 figures", "summary": "Driven by Moore's Law, the dimensions of transistors have been pushed down to the nanometer scale. Advanced quantum transport (QT) solvers are required to accurately simulate such nano-devices. The non-equilibrium Green's function (NEGF) formalism lends itself optimally to these tasks, but it is computationally very intensive, involving the selected inversion (SI) of matrices and the selected solution of quadratic matrix (SQ) equations. Existing algorithms to tackle these numerical problems are ideally suited to GPU acceleration, e.g., the so-called recursive Green's function (RGF) technique, but they are typically sequential, require block-tridiagonal (BT) matrices as inputs, and their implementation has been so far restricted to shared memory parallelism, thus limiting the achievable device sizes. To address these shortcomings, we introduce distributed methods that build on RGF and enable parallel selected inversion and selected solution of the quadratic matrix equation. We further extend them to handle BT matrices with arrowhead, which allows for the investigation of multi-terminal transistor structures. We evaluate the performance of our approach on a real dataset from the QT simulation of a nano-ribbon transistor and compare it with the sparse direct package PARDISO. When scaling to 16 GPUs, our fused SI and SQ solver is 5.2x faster than the SI module of PARDISO applied to a device 16x shorter. These results highlight the potential of our method to accelerate NEGF-based nano-device simulations.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5e03\u5f0f\u5e76\u884c\u65b9\u6cd5\u52a0\u901f\u7eb3\u7c73\u5668\u4ef6\u91cf\u5b50\u8f93\u8fd0\u6a21\u62df\u4e2d\u7684\u9009\u62e9\u6c42\u9006\u548c\u4e8c\u6b21\u77e9\u9635\u65b9\u7a0b\u6c42\u89e3\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u572816\u4e2aGPU\u4e0a\u5b9e\u73b05.2\u500d\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u6676\u4f53\u7ba1\u5c3a\u5bf8\u7f29\u5c0f\u5230\u7eb3\u7c73\u5c3a\u5ea6\uff0c\u9700\u8981\u7cbe\u786e\u7684\u91cf\u5b50\u8f93\u8fd0\u6a21\u62df\u3002\u975e\u5e73\u8861\u683c\u6797\u51fd\u6570(NEGF)\u65b9\u6cd5\u662f\u7406\u60f3\u9009\u62e9\uff0c\u4f46\u8ba1\u7b97\u91cf\u5927\uff0c\u6d89\u53ca\u77e9\u9635\u9009\u62e9\u6c42\u9006\u548c\u4e8c\u6b21\u77e9\u9635\u65b9\u7a0b\u6c42\u89e3\u3002\u73b0\u6709\u7b97\u6cd5\u5982\u9012\u5f52\u683c\u6797\u51fd\u6570(RGF)\u9002\u5408GPU\u52a0\u901f\uff0c\u4f46\u901a\u5e38\u662f\u987a\u5e8f\u6267\u884c\u3001\u9700\u8981\u5757\u4e09\u5bf9\u89d2\u77e9\u9635\u8f93\u5165\uff0c\u4e14\u4ec5\u9650\u4e8e\u5171\u4eab\u5185\u5b58\u5e76\u884c\uff0c\u9650\u5236\u4e86\u53ef\u6a21\u62df\u7684\u5668\u4ef6\u5c3a\u5bf8\u3002", "method": "\u57fa\u4e8eRGF\u6280\u672f\u5f00\u53d1\u5206\u5e03\u5f0f\u5e76\u884c\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5e76\u884c\u9009\u62e9\u6c42\u9006\u548c\u4e8c\u6b21\u77e9\u9635\u65b9\u7a0b\u6c42\u89e3\u3002\u6269\u5c55\u65b9\u6cd5\u5904\u7406\u5e26\u7bad\u5934\u7ed3\u6784\u7684\u5757\u4e09\u5bf9\u89d2\u77e9\u9635\uff0c\u4ee5\u652f\u6301\u591a\u7aef\u6676\u4f53\u7ba1\u7ed3\u6784\u7814\u7a76\u3002", "result": "\u5728\u7eb3\u7c73\u5e26\u6676\u4f53\u7ba1\u91cf\u5b50\u8f93\u8fd0\u6a21\u62df\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\uff0c\u4e0e\u7a00\u758f\u76f4\u63a5\u6c42\u89e3\u5668PARDISO\u6bd4\u8f83\u3002\u572816\u4e2aGPU\u4e0a\uff0c\u878d\u5408\u7684\u9009\u62e9\u6c42\u9006\u548c\u4e8c\u6b21\u77e9\u9635\u65b9\u7a0b\u6c42\u89e3\u5668\u6bd4\u5e94\u7528\u4e8e16\u500d\u77ed\u5668\u4ef6\u7684PARDISO\u9009\u62e9\u6c42\u9006\u6a21\u5757\u5feb5.2\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u52a0\u901f\u4e86\u57fa\u4e8eNEGF\u7684\u7eb3\u7c73\u5668\u4ef6\u6a21\u62df\uff0c\u5c55\u793a\u4e86\u5206\u5e03\u5f0f\u5e76\u884c\u65b9\u6cd5\u5728\u91cf\u5b50\u8f93\u8fd0\u8ba1\u7b97\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5904\u7406\u66f4\u5927\u89c4\u6a21\u7684\u5668\u4ef6\u6a21\u62df\u95ee\u9898\u3002"}}
{"id": "2601.04930", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.04930", "abs": "https://arxiv.org/abs/2601.04930", "authors": ["Antonella Del Pozzo", "Achille Desreumaux", "Mathieu Gestin", "Alexandre Rapetti", "Sara Tucci-Piergiovanni"], "title": "Asynchronous Secure Federated Learning with Byzantine aggregators", "comment": null, "summary": "Privacy-preserving federated averaging is a central approach for protecting client privacy in federated learning. In this paper, we study this problem in an asynchronous communications setting with malicious aggregators. We propose a new solution to provide federated averaging in this model while protecting the client's data privacy through secure aggregation and differential privacy. Our solution maintains the same performance as the state of the art across all metrics. The main contributions of this paper are threefold. First, unlike existing single- or multi-server solutions, we consider malicious aggregation servers that may manipulate the model to leak clients' data or halt computation. To tolerate this threat, we replicate the aggregators, allowing a fraction of them to be corrupted. Second, we propose a new privacy preservation protocol for protocols in asynchronous communication models with Byzantine aggregators. In this protocol, clients mask their values and add Gaussian noise to their models. In contrast with previous works, we use the replicated servers to unmask the models, while ensuring the liveness of training even if aggregators misbehave. Third, the asynchronous communication model introduces new challenges not present in existing approaches. In such a setting, faster clients may contribute more frequently, potentially reducing their privacy and biasing the training. To address this, we introduce an inclusion mechanism that ensures uniform client participation and balanced privacy budgets. Interestingly, the solution presented in this paper does not rely on agreement between aggregators. Thus, we circumvent the known impossibility of consensus in asynchronous settings where processes might crash. Additionally, this feature increases availability, as a consensus-based algorithm only progresses in periods of low latency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u5f02\u6b65\u901a\u4fe1\u73af\u5883\u4e0b\u6076\u610f\u805a\u5408\u670d\u52a1\u5668\u7684\u8054\u90a6\u5e73\u5747\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\uff0c\u7ed3\u5408\u5b89\u5168\u805a\u5408\u548c\u5dee\u5206\u9690\u79c1\uff0c\u901a\u8fc7\u590d\u5236\u805a\u5408\u5668\u5bb9\u5fcd\u62dc\u5360\u5ead\u6545\u969c\uff0c\u65e0\u9700\u805a\u5408\u5668\u95f4\u5171\u8bc6", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u4e3b\u8981\u9488\u5bf9\u540c\u6b65\u901a\u4fe1\u548c\u53ef\u4fe1\u805a\u5408\u5668\uff0c\u4f46\u5728\u5f02\u6b65\u901a\u4fe1\u73af\u5883\u4e2d\uff0c\u6076\u610f\u805a\u5408\u5668\u53ef\u80fd\u64cd\u7eb5\u6a21\u578b\u6cc4\u9732\u5ba2\u6237\u7aef\u6570\u636e\u6216\u505c\u6b62\u8ba1\u7b97\uff0c\u4e14\u5feb\u901f\u5ba2\u6237\u7aef\u53ef\u80fd\u66f4\u9891\u7e41\u53c2\u4e0e\u5bfc\u81f4\u9690\u79c1\u6cc4\u9732\u548c\u8bad\u7ec3\u504f\u5dee", "method": "1) \u590d\u5236\u805a\u5408\u5668\u4ee5\u5bb9\u5fcd\u90e8\u5206\u88ab\u8150\u8680\uff1b2) \u5ba2\u6237\u7aef\u4f7f\u7528\u63a9\u7801\u548c\u9ad8\u65af\u566a\u58f0\u4fdd\u62a4\u6a21\u578b\u66f4\u65b0\uff1b3) \u901a\u8fc7\u590d\u5236\u670d\u52a1\u5668\u89e3\u63a9\u6a21\u578b\uff0c\u786e\u4fdd\u8bad\u7ec3\u6d3b\u6027\uff1b4) \u5f15\u5165\u5305\u542b\u673a\u5236\u4fdd\u8bc1\u5ba2\u6237\u7aef\u5747\u5300\u53c2\u4e0e\u548c\u9690\u79c1\u9884\u7b97\u5e73\u8861\uff1b5) \u65e0\u9700\u805a\u5408\u5668\u95f4\u5171\u8bc6\uff0c\u907f\u514d\u5f02\u6b65\u73af\u5883\u4e0b\u7684\u5171\u8bc6\u4e0d\u53ef\u80fd\u6027", "result": "\u65b9\u6848\u5728\u6240\u6709\u6307\u6807\u4e0a\u4fdd\u6301\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6848\u76f8\u540c\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u5bf9\u6076\u610f\u805a\u5408\u5668\u7684\u5bb9\u5fcd\u3001\u5f02\u6b65\u901a\u4fe1\u73af\u5883\u4e0b\u7684\u9690\u79c1\u4fdd\u62a4\uff0c\u5e76\u907f\u514d\u5171\u8bc6\u74f6\u9888\u63d0\u9ad8\u53ef\u7528\u6027", "conclusion": "\u8be5\u65b9\u6848\u9996\u6b21\u5728\u5f02\u6b65\u901a\u4fe1\u73af\u5883\u4e2d\u89e3\u51b3\u4e86\u6076\u610f\u805a\u5408\u5668\u5a01\u80c1\u4e0b\u7684\u8054\u90a6\u5e73\u5747\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u901a\u8fc7\u590d\u5236\u805a\u5408\u5668\u3001\u5b89\u5168\u805a\u5408\u548c\u5dee\u5206\u9690\u79c1\u7684\u7ec4\u5408\uff0c\u5728\u65e0\u9700\u5171\u8bc6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60"}}
{"id": "2601.05109", "categories": ["cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.05109", "abs": "https://arxiv.org/abs/2601.05109", "authors": ["Marco Laju", "Donghyun Son", "Saurabh Agarwal", "Nitin Kedia", "Myungjin Lee", "Jayanth Srinivasa", "Aditya Akella"], "title": "Nalar: An agent serving framework", "comment": null, "summary": "LLM-driven agentic applications increasingly automate complex, multi-step tasks, but serving them efficiently remains challenging due to heterogeneous components, dynamic and model-driven control flow, long-running state, and unpredictable latencies. Nalar is a ground-up agent-serving framework that cleanly separates workflow specification from execution while providing the runtime visibility and control needed for robust performance. Nalar preserves full Python expressiveness, using lightweight auto-generated stubs that turn agent and tool invocations into futures carrying dependency and context metadata. A managed state layer decouples logical state from physical placement, enabling safe reuse, migration, and consistent retry behavior. A two-level control architecture combines global policy computation with local event-driven enforcement to support adaptive routing, scheduling, and resource management across evolving workflows. Together, these mechanisms allow Nalar to deliver scalable, efficient, and policy-driven serving of heterogeneous agentic applications without burdening developers with orchestration logic. Across three agentic workloads, Nalar cuts tail latency by 34--74\\%, achieves up to $2.9\\times$ speedups, sustains 80 RPS where baselines fail, and scales to 130K futures with sub-500 ms control overhead.", "AI": {"tldr": "Nalar\u662f\u4e00\u4e2a\u4ece\u5934\u6784\u5efa\u7684\u667a\u80fd\u4f53\u670d\u52a1\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u5de5\u4f5c\u6d41\u89c4\u8303\u4e0e\u6267\u884c\u3001\u63d0\u4f9b\u8fd0\u884c\u65f6\u53ef\u89c1\u6027\u548c\u63a7\u5236\uff0c\u5b9e\u73b0\u5f02\u6784\u667a\u80fd\u4f53\u5e94\u7528\u7684\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u670d\u52a1\u3002", "motivation": "LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5e94\u7528\u5728\u81ea\u52a8\u5316\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u65f6\u9762\u4e34\u6311\u6218\uff1a\u5f02\u6784\u7ec4\u4ef6\u3001\u52a8\u6001\u6a21\u578b\u9a71\u52a8\u7684\u63a7\u5236\u6d41\u3001\u957f\u8fd0\u884c\u72b6\u6001\u548c\u4e0d\u53ef\u9884\u6d4b\u5ef6\u8fdf\uff0c\u9700\u8981\u9ad8\u6548\u7684\u670d\u52a1\u6846\u67b6\u3002", "method": "1) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u81ea\u52a8\u751f\u6210\u7684\u5b58\u6839\u5c06\u667a\u80fd\u4f53\u548c\u5de5\u5177\u8c03\u7528\u8f6c\u6362\u4e3a\u643a\u5e26\u4f9d\u8d56\u548c\u4e0a\u4e0b\u6587\u5143\u6570\u636e\u7684futures\uff1b2) \u7ba1\u7406\u72b6\u6001\u5c42\u89e3\u8026\u903b\u8f91\u72b6\u6001\u4e0e\u7269\u7406\u653e\u7f6e\uff1b3) \u4e24\u7ea7\u63a7\u5236\u67b6\u6784\u7ed3\u5408\u5168\u5c40\u7b56\u7565\u8ba1\u7b97\u4e0e\u672c\u5730\u4e8b\u4ef6\u9a71\u52a8\u6267\u884c\u3002", "result": "\u5728\u4e09\u4e2a\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\uff1a\u5c3e\u90e8\u5ef6\u8fdf\u964d\u4f4e34-74%\uff0c\u901f\u5ea6\u63d0\u5347\u6700\u9ad82.9\u500d\uff0c\u5728\u57fa\u7ebf\u5931\u8d25\u65f6\u7ef4\u630180 RPS\uff0c\u6269\u5c55\u523013\u4e07\u4e2afutures\u4e14\u63a7\u5236\u5f00\u9500\u4f4e\u4e8e500ms\u3002", "conclusion": "Nalar\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u5f00\u53d1\u8005\u7f16\u6392\u903b\u8f91\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u7b56\u7565\u9a71\u52a8\u7684\u5f02\u6784\u667a\u80fd\u4f53\u5e94\u7528\u670d\u52a1\u3002"}}
