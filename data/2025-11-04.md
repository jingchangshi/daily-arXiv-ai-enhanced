<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 5]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Equality Saturation Guided by Large Language Models](https://arxiv.org/abs/2511.00403)
*Wentao Peng,Ruyi Ji,Yingfei Xiong*

Main category: cs.PL

TL;DR: LGuess通过将e-graphs作为LLMs和重写系统之间的中间层，解决了LLMs无法保证正确性的问题。它只向LLM查询高层次的重写检查点，并使用e-graphs提供这些检查点之间的低层次重写链。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型无法保证正确性，虽然可以将其应用于形式重写系统来解决这个问题，但当前LLMs在生成可靠重写链方面仍然不足。

Method: 提出LLM引导的等式饱和方法LGuess，通过e-graphs作为中间层，学习LLM的概率模型来预测可能的检查点，从饱和e-graph中有效提取合适的检查点。

Result: 在多变量多项式因式分解问题上，LGuess相比直接等式饱和和直接查询LLM重写链的方法具有显著优势。

Conclusion: LGuess通过结合LLMs和e-graphs，有效解决了LLMs在形式重写系统中的可靠性问题，在多项式因式分解任务上表现出色。

Abstract: One critical issue with large language models (LLMs) is their inability to
guarantee correctness. Although this problem can be addressed by applying LLMs
to formal rewrite systems, current LLMs are still far from adequate to generate
sound rewrite chains. To bridge this gap, this paper proposes LLM-guided
equality saturation, dubbed LGuess, by incorporating e-graphs as an
intermediate layer between LLMs and rewrite systems. LGuess queries LLMs only
for high-level rewrite checkpoints and uses e-graphs to supply low-level
rewrite chains between these checkpoints. The key technical challenge in this
procedure lies in effectively extracting a suitable checkpoint from a saturated
e-graph, which LGuess addresses by learning a probabilistic model from the LLM.
The model predicts probable checkpoints while remaining simple enough for
effective extraction. We implement a prototype of LGuess and evaluate it on the
problem of factorizing multivariable polynomials. The results demonstrate a
significant advantage of LGuess compared to both straightforward equality
saturation and the approach that queries the LLM directly for the rewrite
chain.

</details>


### [2] [\texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs](https://arxiv.org/abs/2511.00488)
*Jun Gao,Yun Peng,Xiaoxue Ren*

Main category: cs.PL

TL;DR: 本文提出了ReMind多智能体框架，通过Mutator、Executor和Inspector的协作来解决大语言模型在演绎代码推理中的局限性，显著提升了推理能力和零样本泛化性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码相关任务上取得了显著进展，但实证研究表明它们在演绎代码推理（理解程序执行过程）方面仍存在困难，且根本原因尚未充分探索。

Method: 提出ReMind多智能体框架：Mutator生成代码变体以减少对代码源的偏见；Executor逐步追踪变量状态以暴露不一致性；Inspector识别有问题的推理步骤并提供控制流优化。三者协同工作系统性地识别和优化推理缺陷。

Result: 在两个基准测试和五个大语言模型上的广泛实验表明，ReMind在演绎代码推理方面相比基线方法具有显著优势，实现了出色的性能和鲁棒的零样本泛化。

Conclusion: ReMind框架通过多智能体协作有效解决了大语言模型在演绎代码推理中的三个关键挑战，为提升代码推理能力提供了系统性的解决方案。

Abstract: Large Language Models (LLMs) have achieved remarkable progress in
code-related tasks. Despite their advancement, empirical evidence reveals that
they still struggle with \emph{deductive code reasoning}, the ability to reason
about the program execution process. While prior studies have recognized this
limitation, the underlying causes remain largely underexplored. In this paper,
we begin by presenting a comprehensive empirical study that reveals three key
challenges undermining deductive code reasoning: (1) an intrinsic gap between
generation and reasoning abilities, (2) a consistent bias towards code sources,
and (3) weak zero-shot generalization on complex benchmarks. In light of these
challenges, we propose \texttt{ReMind}, a multi-agent framework composed of
\texttt{Mutator}, \texttt{Executor}, and \texttt{Inspector}. The
\texttt{Mutator} generates code variants to mitigate bias towards code sources,
the \texttt{Executor} traces variable states step-by-step to expose
inconsistency, and the \texttt{Inspector} identifies problematic reasoning
steps and provides control-flow refinement to bridge the intrinsic reasoning
gap. Through their coordinated collaboration, \texttt{ReMind} systematically
identifies and refines reasoning flaws, achieving outstanding performance and
enabling robust zero-shot generalization. Extensive experiments on two
benchmarks with five LLMs demonstrate the superior advantages of
\texttt{ReMind} compared to baseline approaches in deductive code reasoning.

</details>


### [3] [Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization](https://arxiv.org/abs/2511.00592)
*Massinissa Merouani,Islem Kara Bernou,Riyadh Baghdadi*

Main category: cs.PL

TL;DR: ComPilot是一个利用现成LLM作为交互式优化代理的框架，通过与编译器建立反馈循环来优化代码，在PolyBench基准测试中实现了2.66-3.54倍的几何平均加速。


<details>
  <summary>Details</summary>
Motivation: 现代硬件上复杂循环嵌套的自动代码优化仍然是一个困难挑战，需要探索新的优化方法。

Method: 使用现成的LLM作为交互式优化代理，与编译器建立反馈循环：LLM提出转换建议，编译器尝试转换并反馈合法性和性能数据，LLM据此迭代优化策略。

Result: 在PolyBench基准测试中，ComPilot实现了单次运行2.66倍和最佳5次运行3.54倍的几何平均加速，性能优于最先进的Pluto多面体优化器。

Conclusion: 通用LLM在编译器反馈的指导下可以有效引导代码优化过程，为代码优化中的智能代理AI开辟了有前景的研究方向。

Abstract: Automatic code optimization remains a difficult challenge, particularly for
complex loop nests on modern hardware. This paper investigates a novel approach
to code optimization where Large Language Models (LLMs) guide the process
through a closed-loop interaction with a compiler. We present ComPilot, an
experimental framework that leverages off-the-shelf LLMs, without any
task-specific fine-tuning, as interactive optimization agents. ComPilot
establishes a feedback loop where an LLM proposes transformations for a given
loop nest to a compiler. The compiler attempts the transformations, reporting
back legality status and measured speedup or slowdown. The LLM utilizes this
concrete feedback to iteratively refine its optimization strategy. Our
extensive evaluation across the PolyBench benchmark suite demonstrates the
effectiveness of this zero-shot approach. ComPilot achieves geometric mean
speedups of 2.66x (single run) and 3.54x (best-of-5 runs) over the original
code. Furthermore, ComPilot demonstrates competitive performance against the
state-of-the-art Pluto polyhedral optimizer, outperforming it in many cases.
This experimental study demonstrates that general-purpose LLMs can effectively
guide the code optimization process when grounded by compiler feedback, opening
promising research directions for agentic AI in code optimization.

</details>


### [4] [Typed Embedding of miniKanren for Functional Conversion](https://arxiv.org/abs/2511.00740)
*Igor Engel,Ekaterina Verbitskaia*

Main category: cs.PL

TL;DR: 本文提出了一种将miniKanren嵌入Haskell的类型化无标签最终方法，解决了之前转换方法中的类型不敏感、需要确定性标注和隐式生成器线程等问题。


<details>
  <summary>Details</summary>
Motivation: 之前的函数式转换方法存在性能开销问题，且实现不够优雅：对类型不敏感、需要确定性标注、存在隐式生成器线程。本文旨在解决这些问题。

Method: 采用类型化无标签最终嵌入方法将miniKanren嵌入到Haskell中，减少了样板代码。

Result: 该方法显著减少了样板代码，同时保持甚至提升了之前的性能加速效果。

Conclusion: 类型化无标签最终嵌入方法为关系式编程提供了更优雅的实现方案，在减少代码复杂度的同时保持了性能优势。

Abstract: Relational programming enables program synthesis through a verifier-to-solver
approach. An earlier paper introduced a functional conversion that mitigated
some of the inherent performance overhead. However, the conversion was
inelegant: it was oblivious to types, demanded determinism annotations, and
implicit generator threading. In this paper, we address these issues by
providing a typed tagless-final embedding of miniKanren into Haskell. This
improvement significantly reduces boilerplate while preserving, and sometimes
enhancing, earlier speedups.

</details>


### [5] [Cobble: Compiling Block Encodings for Quantum Computational Linear Algebra](https://arxiv.org/abs/2511.01736)
*Charles Yuan*

Main category: cs.PL

TL;DR: Cobble是一种用于量子计算线性代数编程的语言，通过高级符号自动编译为正确的量子电路，提供性能分析和优化，在基准测试中实现2.6x-25.4x的加速。


<details>
  <summary>Details</summary>
Motivation: 量子线性代数算法虽然承诺指数级加速，但开发人员需要将复杂的矩阵运算表达式转化为正确高效的量子电路，面临传统优化方法不适用等挑战。

Method: 开发Cobble语言，支持用高级符号表达和操作矩阵的量子表示（块编码），自动编译为量子电路，包含性能分析和优化技术如量子奇异值变换。

Result: 在模拟、回归、搜索等应用的基准测试中，Cobble实现了2.6x-25.4x的加速，优于现有电路优化器。

Conclusion: Cobble为量子计算线性代数提供了有效的编程解决方案，显著简化了开发过程并提升了性能。

Abstract: Quantum algorithms for computational linear algebra promise up to exponential
speedups for applications such as simulation and regression, making them prime
candidates for hardware realization. But these algorithms execute in a model
that cannot efficiently store matrices in memory like a classical algorithm
does, instead requiring developers to implement complex expressions for matrix
arithmetic in terms of correct and efficient quantum circuits. Among the
challenges for the developer is navigating a cost model in which conventional
optimizations for linear algebra, such as subexpression reuse, can be
inapplicable or unprofitable.
  In this work, we present Cobble, a language for programming with quantum
computational linear algebra. Cobble enables developers to express and
manipulate the quantum representations of matrices, known as block encodings,
using high-level notation that automatically compiles to correct quantum
circuits. Cobble features analyses that estimate leading factors in time and
space usage of programs, as well as optimizations that reduce overhead and
generate efficient circuits using leading techniques such as the quantum
singular value transformation. We evaluate Cobble on benchmark kernels for
simulation, regression, search, and other applications, showing 2.6x-25.4x
speedups not achieved by existing circuit optimizers on these benchmarks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [AeroResQ: Edge-Accelerated UAV Framework for Scalable, Resilient and Collaborative Escape Route Planning in Wildfire Scenarios](https://arxiv.org/abs/2511.00038)
*Suman Raj,Radhika Mittal,Rajiv Mayani,Pawel Zuk,Anirban Mandal,Michael Zink,Yogesh Simmhan,Ewa Deelman*

Main category: cs.DC

TL;DR: AeroResQ是一个边缘加速的无人机框架，用于野火场景下的可扩展、弹性和协作式逃生路线规划，采用多层编排架构，包含服务无人机和协调无人机，实现实时决策和任务协调。


<details>
  <summary>Details</summary>
Motivation: 在野火响应中，配备机载摄像头和DNN模型的无人机群对于实时时空决策至关重要，需要监控火灾动态、支持消防员协调和促进安全疏散。

Method: 采用多层编排架构，服务无人机负责调查火灾区域、检测被困人员，协调无人机使用加权A*搜索算法动态生成最优地面逃生路线，并包含智能负载平衡和弹性机制。

Result: 实验结果显示AeroResQ实现了≤500ms的端到端延迟，远低于2秒请求间隔，同时保持超过98%的任务重新分配和完成成功率。

Conclusion: AeroResQ证明了其在紧急响应和消防员安全操作中实时现场部署的可行性，为野火场景提供了有效的逃生路线规划解决方案。

Abstract: Drone fleets equipped with onboard cameras, computer vision, and Deep Neural
Network (DNN) models present a powerful paradigm for real-time spatio-temporal
decision-making. In wildfire response, such drones play a pivotal role in
monitoring fire dynamics, supporting firefighter coordination, and facilitating
safe evacuation. In this paper, we introduce AeroResQ, an edge-accelerated UAV
framework designed for scalable, resilient, and collaborative escape route
planning during wildfire scenarios. AeroResQ adopts a multi-layer orchestration
architecture comprising service drones (SDs) and coordinator drones (CDs), each
performing specialized roles. SDs survey fire-affected areas, detect stranded
individuals using onboard edge accelerators running fire detection and human
pose identification DNN models, and issue requests for assistance. CDs,
equipped with lightweight data stores such as Apache IoTDB, dynamically
generate optimal ground escape routes and monitor firefighter movements along
these routes. The framework proposes a collaborative path-planning approach
based on a weighted A* search algorithm, where CDs compute context-aware escape
paths. AeroResQ further incorporates intelligent load-balancing and resilience
mechanisms: CD failures trigger automated data redistribution across IoTDB
replicas, while SD failures initiate geo-fenced re-partitioning and
reassignment of spatial workloads to operational SDs. We evaluate AeroResQ
using realistic wildfire emulated setup modeled on recent Southern California
wildfires. Experimental results demonstrate that AeroResQ achieves a nominal
end-to-end latency of <=500ms, much below the 2s request interval, while
maintaining over 98% successful task reassignment and completion, underscoring
its feasibility for real-time, on-field deployment in emergency response and
firefighter safety operations.

</details>


### [7] [COOL Is Optimal in Error-Free Asynchronous Byzantine Agreement](https://arxiv.org/abs/2511.00263)
*Jinyuan Chen*

Main category: cs.DC

TL;DR: OciorACOOL是COOL协议的异步自适应版本，在异步设置下实现了无错误、信息论安全的拜占庭共识，具有最优弹性n≥3t+1、O(max{nℓ, nt log q})通信复杂度和O(1)轮次。


<details>
  <summary>Details</summary>
Motivation: 将COOL协议从同步设置扩展到异步设置，同时保持其低通信复杂度和信息论安全性，解决异步环境下的拜占庭共识问题。

Method: 基于COOL协议设计自适应变体，使用传统的(n,k)纠错编码（k=t/3），仅需调用一次异步二进制BA协议。

Result: 在异步设置下实现了与COOL相同的通信复杂度O(max{nℓ, nt log q})、O(1)轮次，同时保持无错误和信息论安全性。

Conclusion: OciorACOOL成功将COOL协议扩展到异步环境，在保持所有优点的同时适应了异步网络条件。

Abstract: COOL (Chen'21) is an error-free, information-theoretically secure Byzantine
agreement (BA) protocol proven to achieve BA consensus in the synchronous
setting for an $\ell$-bit message, with a total communication complexity of
$O(\max\{n\ell, nt \log q\})$ bits, four communication rounds in the worst
case, and a single invocation of a binary BA, under the optimal resilience
assumption $n \geq 3t + 1$ in a network of $n$ nodes, where up to $t$ nodes may
behave dishonestly. Here, $q$ denotes the alphabet size of the error correction
code used in the protocol.
  In this work, we present an adaptive variant of COOL, called OciorACOOL,
which achieves error-free, information-theoretically secure BA consensus in the
asynchronous setting with total $O(\max\{n\ell, n t \log q\})$ communication
bits, $O(1)$ rounds, and a single invocation of an asynchronous binary BA
protocol, still under the optimal resilience assumption $n \geq 3t + 1$.
Moreover, OciorACOOL retains the same low-complexity, traditional $(n, k)$
error-correction encoding and decoding as COOL, with $k=t/3$.

</details>


### [8] [Tetris: An SLA-aware Application Placement Strategy in the Edge-Cloud Continuum](https://arxiv.org/abs/2511.00294)
*Lucas Almeida,Maycon Peixoto*

Main category: cs.DC

TL;DR: 提出Tetris应用放置策略，在边缘-云连续体中通过启发式算法分配计算服务，优先考虑SLA紧急性和资源效率，减少76%的SLA违规。


<details>
  <summary>Details</summary>
Motivation: 边缘-云连续体整合边缘和云资源提供灵活基础设施，需要在用户需求与基础设施约束间进行战略性应用放置以实现高效资源利用。

Method: 使用启发式算法在边缘和云资源间分配计算服务，基于SLA紧急性和资源效率进行优先级排序以避免系统过载。

Result: 与基准方法相比，Tetris减少了约76%的SLA违规，提升了服务质量。

Conclusion: Tetris为边缘-云连续体环境中的延迟敏感应用提供了有效的放置方法，显著改善了用户服务质量。

Abstract: An Edge-Cloud Continuum integrates edge and cloud resources to provide a
flexible and scalable infrastructure. This paradigm can minimize latency by
processing data closer to the source at the edge while leveraging the vast
computational power of the cloud for more intensive tasks. In this context,
module application placement requires strategic allocation plans that align
user demands with infrastructure constraints, aiming for efficient resource
use. Therefore, we propose Tetris, an application placement strategy that
utilizes a heuristic algorithm to distribute computational services across edge
and cloud resources efficiently. Tetris prioritizes services based on SLA
urgencies and resource efficiency to avoid system overloading. Our results
demonstrate that Tetris reduces SLA violations by approximately 76% compared to
the baseline method, which serves as a reference point for benchmarking
performance in this scenario. Therefore, Tetris offers an effective placement
approach for managing latency-sensitive applications in Edge-Cloud Continuum
environments, enhancing Quality of Service (QoS) for users.

</details>


### [9] [EPARA: Parallelizing Categorized AI Inference in Edge Clouds](https://arxiv.org/abs/2511.00603)
*Yubo Wang,Yubo Cui,Tuo Shi,Danyang Li,Wenxin Li,Lide Suo,Tao Wang,Xin Xie*

Main category: cs.DC

TL;DR: EPARA是一个端到端的边缘AI并行推理框架，通过任务分类和资源分配提升边缘AI服务能力，在边缘服务器上实现比现有框架高2.1倍的生产负载吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和计算机视觉AI应用的广泛采用，AI推理系统的计算需求不断增长，需要在现有硬件基础上提升边缘云的任务处理能力。

Method: EPARA包含三个核心组件：1)任务分类并行分配器决定每个任务的并行模式；2)分布式请求处理器执行具体请求计算；3)状态感知调度器定期更新边缘云中的服务部署。通过基于任务对延迟/频率敏感度和GPU资源需求的分类，实现请求级和服务级的任务-资源分配。

Result: 在包含边缘服务器、嵌入式设备和微计算机的测试平台实验中，EPARA在生产负载中相比现有框架实现了高达2.1倍的高吞吐量，并能适应各种边缘AI推理任务。

Conclusion: EPARA框架有效提升了边缘AI推理能力，通过智能的任务分类和资源分配机制，在边缘计算环境中实现了显著的性能提升。

Abstract: With the increasing adoption of AI applications such as large language models
and computer vision AI, the computational demands on AI inference systems are
continuously rising, making the enhancement of task processing capacity using
existing hardware a primary objective in edge clouds. We propose EPARA, an
end-to-end AI parallel inference framework in edge, aimed at enhancing the edge
AI serving capability. Our key idea is to categorize tasks based on their
sensitivity to latency/frequency and requirement for GPU resources, thereby
achieving both request-level and service-level task-resource allocation. EPARA
consists of three core components: 1) a task-categorized parallelism allocator
that decides the parallel mode of each task, 2) a distributed request handler
that performs the calculation for the specific request, and 3) a state-aware
scheduler that periodically updates service placement in edge clouds. We
implement a EPARA prototype and conduct a case study on the EPARA operation for
LLMs and segmentation tasks. Evaluation through testbed experiments involving
edge servers, embedded devices, and microcomputers shows that EPARA achieves up
to 2.1$\times$ higher goodput in production workloads compared to prior
frameworks, while adapting to various edge AI inference tasks.

</details>


### [10] [AReaL-Hex: Accommodating Asynchronous RL Training over Heterogeneous GPUs](https://arxiv.org/abs/2511.00796)
*Ran Yan,Youhe Jiang,Tianyuan Wu,Jiaxuan Gao,Zhiyu Mei,Wei Fu,Haohui Mai,Wei Wang,Yi Wu,Binhang Yuan*

Main category: cs.DC

TL;DR: AReaL-Hex是一个面向异构GPU的异步强化学习训练系统，通过两阶段调度器在保持数据新鲜度约束的同时，最大化异构环境下的训练吞吐量和成本效益。


<details>
  <summary>Details</summary>
Motivation: 为了降低LLM强化学习训练的门槛，需要最大化训练吞吐量和成本效益。异构GPU部署是一个有前景但具有挑战性的方法，因为RL训练包含三个计算特性不同的阶段。

Method: 使用两阶段调度器：1）通过混合整数线性规划选择并行化策略和工作负载分配；2）通过图分区分配异构GPU和互连以最大化端到端吞吐量。系统将内存I/O密集型生成和计算密集型优化映射到成本效益更高的资源上。

Result: 在数学推理任务上，相比同构部署的最先进异步RL系统：1）相同预算下训练吞吐量提升1.50倍；2）相同吞吐量下训练成本降低1.46倍。

Conclusion: AReaL-Hex证明了在异构GPU环境中有效调度RL训练阶段的可行性，显著提升了训练效率和成本效益。

Abstract: Maximizing training throughput and cost-efficiency of RL for LLMs is
essential to democratize this advanced technique. One promising but challenging
approach is to deploy such a computational workflow over heterogeneous GPUs.
Unlike conventional large-scale LLM pretraining, RL training generally
decomposes into three coupled stages, i.e., rollout generation, reward
computation, and policy/value updates, which exhibit markedly different compute
intensities, memory footprints, and communication patterns. Recent research
shows that fully asynchronous RL training can disaggregate these stages across
disjoint hardware pools without sacrificing training stability, creating a
great opportunity for real-world heterogeneous deployment. To this end, we
present AReaL-Hex, a heterogeneity-aware asynchronous RL training system that
effectively schedules how to execute rollout generation and policy model
training over heterogeneous GPUs while enforcing data staleness bounds.
Concretely, we use a two-phase scheduler: (i) a constrained search with MILP to
select per-stage parallelization strategies and workload assignments given a
resource budget, and (ii) a graph-partitioning step that allocates
heterogeneous GPUs and interconnects to maximize end-to-end throughput. Built
atop a fully asynchronous RL architecture, AReaL-Hex maps HBM-I/O-bound
generation and compute-bound optimization to more cost-efficient resources and
balances their producer-consumer interactions to avoid both idleness and stale
rollout trajectories. On the mathematical reasoning task with various model
scales (1.5B, 7B, and 14B), compared to homogeneous deployments of
state-of-the-art asynchronous RL systems: (i) When maintaining the same total
budgets, AReaL-Hex delivers up to 1.50x higher training throughput; (ii) When
achieving the same training throughput, AReaL-Hex results in up to 1.46x
reduction in training cost.

</details>


### [11] [FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving on Heterogeneous GPUs](https://arxiv.org/abs/2511.00807)
*Xuan He,Zequan Fang,Jinzhao Lian,Danny H. K. Tsang,Baosen Zhang,Yize Chen*

Main category: cs.DC

TL;DR: FREESH是一个联合路由和调度的LLM服务系统，通过利用时空计算灵活性，在异构GPU集群中优化能源消耗和碳排放，同时满足延迟和公平性要求。


<details>
  <summary>Details</summary>
Motivation: 随着LLM和AI代理的计算和能源需求不断增长，需要全面优化LLM服务系统。异构GPU集群在地理上分布部署，而LLM负载在查询流量和服务模式上具有多样性，这为通过时空计算灵活性来优化能源和碳排放提供了机会。

Method: FREESH通过匹配不同GPU实例的功耗-吞吐量特性与可预测的LLM查询长度和工作负载，实现负载均衡服务。系统结合优化并行度和查询路由调度，动态GPU频率调节以节省功耗，以及Least-Laxity-First（LLF）服务策略进行查询调度。

Result: 在生产工作负载上进行1小时服务测试，FREESH实现了能源消耗降低28.6%，碳排放减少45.45%，同时提高了SLO达成率和公平性。

Conclusion: FREESH通过联合路由和调度优化，在异构分布式GPU集群中有效降低了LLM服务的能源消耗和碳排放，同时保证了服务质量和公平性，为可持续的AI计算提供了可行方案。

Abstract: The ever-increasing computation and energy demand for LLM and AI agents call
for holistic and efficient optimization of LLM serving systems. In practice,
heterogeneous GPU clusters can be deployed in a geographically distributed
manner, while LLM load also observes diversity in terms of both query traffic
and serving patterns. LLM queries running on advanced GPUs during a
high-emission hour at one location can lead to significantly higher carbon
footprints versus same queries running on mid-level GPUs at a low-emission time
and location. By observing LLM serving requirements and leveraging
spatiotemporal computation flexibility, we consider the joint routing and
scheduling problem, and propose FREESH to cooperatively run a group of data
centers while minimizing user-specified carbon or energy objectives. FREESH
identifies the optimal configurations of balanced load serving by matching
distinct GPU instance's power-throughput characteristics with predictable LLM
query length and workloads. To ensure both latency and fairness requirements,
FREESH identifies optimized parallelism and query routing schedules together
with dynamic GPU frequency scaling for power saving, and Least-Laxity-First
(LLF) serving strategy for query scheduling. During the 1-hour serving on
production workloads, FREESH reduces energy by 28.6% and emissions by 45.45%
together with improvements in SLO attainment and fairness.

</details>


### [12] [Towards Portability at Scale: A Cross-Architecture Performance Evaluation of a GPU-enabled Shallow Water Solver](https://arxiv.org/abs/2511.01001)
*Johansell Villalobos,Daniel Caviedes-Voullième,Silvio Rizzi,Esteban Meneses*

Main category: cs.DC

TL;DR: 本文对SERGHEI-SWE浅水方程求解器在四种异构HPC系统上的性能进行了全面研究，展示了良好的可扩展性和性能可移植性，同时指出内存带宽是主要性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 气候变化对数值建模提出重大挑战，水文建模对高分辨率实时模拟的需求增加，促使采用GPU加速平台和性能可移植编程框架。

Method: 在四种异构HPC系统（Frontier、JUWELS Booster、JEDI、Aurora）上评估SERGHEI-SWE求解器的强扩展和弱扩展性能，使用屋顶线分析识别性能瓶颈，应用调和与算术均值指标评估性能可移植性。

Result: 在1024个GPU上实现32倍加速，效率超过90%；屋顶线分析显示内存带宽是主要瓶颈；性能可移植性在调整问题大小时可达70%。

Conclusion: SERGHEI-SWE是一个稳健、可扩展且可移植的模拟工具，但在内核优化方面仍有改进空间，可通过Kokkos团队和架构特定参数进一步提升性能。

Abstract: Current climate change has posed a grand challenge in the field of numerical
modeling due to its complex, multiscale dynamics. In hydrological modeling, the
increasing demand for high-resolution, real-time simulations has led to the
adoption of GPU-accelerated platforms and performance portable programming
frameworks such as Kokkos. In this work, we present a comprehensive performance
study of the SERGHEI-SWE solver, a shallow water equations code, across four
state-of-the-art heterogeneous HPC systems: Frontier (AMD MI250X), JUWELS
Booster (NVIDIA A100), JEDI (NVIDIA H100), and Aurora (Intel Max 1550). We
assess strong scaling up to 1024 GPUs and weak scaling upwards of 2048 GPUs,
demonstrating consistent scalability with a speedup of 32 and an efficiency
upwards of 90\% for most almost all the test range. Roofline analysis reveals
that memory bandwidth is the dominant performance bottleneck, with key solver
kernels residing in the memory-bound region. To evaluate performance
portability, we apply both harmonic and arithmetic mean-based metrics while
varying problem size. Results indicate that while SERGHEI-SWE achieves
portability across devices with tuned problem sizes (<70\%), there is room for
kernel optimization within the solver with more granular control of the
architecture specifically by using Kokkos teams and architecture specific
tunable parameters. These findings position SERGHEI-SWE as a robust, scalable,
and portable simulation tool for large-scale geophysical applications under
evolving HPC architectures with potential to enhance its performance.

</details>


### [13] [Neuro-Inspired Task Offloading in Edge-IoT Networks Using Spiking Neural Networks](https://arxiv.org/abs/2511.01127)
*Fabio Diniz Rossi*

Main category: cs.DC

TL;DR: 提出基于脉冲神经网络的边缘计算任务卸载框架，相比传统方法显著降低延迟和能耗


<details>
  <summary>Details</summary>
Motivation: 传统任务卸载策略在高度动态和资源受限环境中表现不佳，需要更高效自适应的解决方案

Method: 将SNN决策模块集成到边缘节点，使用YAFS和Brian2混合仿真环境评估

Result: 相比传统方法，延迟降低26%，能耗减少32%，高负载下成功率提高25%

Conclusion: SNN框架在边缘计算任务卸载中表现出优越的性能和能效

Abstract: Traditional task offloading strategies in edge computing often rely on static
heuristics or data-intensive machine learning models, which are not always
suitable for highly dynamic and resource-constrained environments. In this
paper, we propose a novel task-offloading framework based on Spiking Neural
Networks inspired by the efficiency and adaptability of biological neural
systems. Our approach integrates an SNN-based decision module into edge nodes
to perform real-time, energy-efficient task orchestration. We evaluate the
model under various IoT workload scenarios using a hybrid simulation
environment composed of YAFS and Brian2. The results demonstrate that our
SNN-based framework significantly reduces task processing latency and energy
consumption while improving task success rates. Compared to traditional
heuristic and ML-based strategies, our model achieves up to 26% lower latency,
32% less energy consumption, and 25\% higher success rate under high-load
conditions.

</details>


### [14] [Scalable Maxflow Processing for Dynamic Graphs](https://arxiv.org/abs/2511.01235)
*Shruthi Kannappan,Ashwina Kumar,Rupesh Nasre*

Main category: cs.DC

TL;DR: 提出了一种新颖的GPU并行最大流算法，能够动态更新图的最大流，并设计了高性能的静态GPU算法和CUDA优化方案。


<details>
  <summary>Details</summary>
Motivation: 最大流问题是图论和组合优化的核心问题，在计算机网络、交通系统等领域有广泛应用。Push-Relabel算法因其局部顶点操作特性适合并行化，这激发了利用GPU并行架构进行最大流计算的研究。

Method: 开发了GPU并行最大流算法，支持动态图的批量边更新后增量重计算最大流；设计了高性能静态GPU算法计算初始最大流；实现了一系列CUDA特定的优化技术。

Result: 算法在GPU平台上展现出优异的性能、可扩展性和内存效率。

Conclusion: 提出的GPU并行最大流算法能够有效处理动态图的最大流计算问题，为相关应用提供了高效的解决方案。

Abstract: The Maximum Flow (Max-Flow) problem is a cornerstone in graph theory and
combinatorial optimization, aiming to determine the largest possible flow from
a designated source node to a sink node within a capacitated flow network. It
has extensive applications across diverse domains such as computer networking,
transportation systems, and image segmentation. The objective is to maximize
the total throughput while respecting edge capacity constraints and maintaining
flow conservation at all intermediate vertices.
  Among the various algorithms proposed for solving the Max-Flow problem, the
Push--Relabel algorithm is particularly notable for its efficiency and
suitability for parallelization, owing to its localized vertex-based
operations. This property has motivated extensive research into GPU-accelerated
Max-Flow computation, leveraging the high degree of parallelism inherent to
modern GPU architectures.
  In this paper, we present a novel GPU-parallel Max-Flow algorithm capable of
incrementally recomputing the maximum flow of a dynamic graph following a batch
of edge updates. In addition, we introduce a high-performance static GPU
algorithm designed for efficiently computing the initial Max-Flow on static
graphs. We further describe a series of CUDA-specific implementation
optimizations that enhance performance, scalability, and memory efficiency on
GPU platforms.

</details>


### [15] [Design of quasi phase matching crystal based on differential gray wolf algorithm](https://arxiv.org/abs/2511.01255)
*He Chen,ZiHua Zheng,JingHua Sun*

Main category: cs.DC

TL;DR: 本文提出了一种融合混合优化算法和GPU并行加速技术的方案，用于解决非周期极化晶体性能优化这一高维离散组合NP难题。


<details>
  <summary>Details</summary>
Motivation: 传统算法在非周期极化晶体优化中存在收敛慢、易陷入局部最优的问题，而启发式方法受限于CPU串行计算效率低下。

Method: 采用差分进化算法(DE)进行全局搜索，灰狼优化算法(GWO)加强局部搜索和收敛速度，两者协同平衡全局和局部优化需求，并利用GPU多核架构实现线程级并行计算。

Result: 该方案有效突破了高维离散空间的优化问题，提高了晶体域控制精度，相比传统CPU串行计算，准相位匹配设计效率提升了数百至数千倍。

Conclusion: 为复杂非线性光学器件设计提供了新范式，有助于推动量子光学和激光加工等领域相关器件的性能突破和工业应用。

Abstract: This paper focuses on the key problem in the development of nonlinear optical
technology, the performance optimization of aperiodically polarized crystals.
The performance of the crystal depends on the precise control of the micro
distribution of crystal domains, but its optimization belongs to the
high-dimensional discrete combination "NP hard" problem. The traditional
algorithm has the bottleneck of slow convergence and easy to fall into local
optimization, while the heuristic methods such as genetic algorithm are limited
by the CPU serial calculation and inefficient. In order to solve the above
challenges, this paper proposes the fusion scheme of hwsda hybrid optimization
algorithm and GPU parallel acceleration technology: the differential evolution
algorithm (DE) is used to realize the global search, and the gray wolf
optimization algorithm (GWO) is used to strengthen the local search and
convergence speed, and the two coordinate to balance the global and local
optimization requirements; At the same time, it relies on GPU multi-core
architecture to realize thread level parallel computing and improve
optimization efficiency. This scheme effectively breaks through the
optimization problem of high-dimensional discrete space, improves the accuracy
of crystal domain control, improves the efficiency of quasi phase matching
design by hundreds to thousands of times compared with traditional CPU serial
computing, provides a new paradigm for the design of complex nonlinear optical
devices, and helps promote the performance breakthrough and industrial
application of related devices in the fields of quantum optics and laser
processing.

</details>


### [16] [Transformer-Based Sparse CSI Estimation for Non-Stationary Channels](https://arxiv.org/abs/2511.01333)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Hassan Rizwan,Sagnik Bhattacharya,Muhammad Ali Jamshed,John M. Cioffi*

Main category: cs.DC

TL;DR: 提出了一种基于Flash-Attention Transformer的CSI估计框架，在非平稳信道条件下显著提升估计精度，同时大幅降低导频开销。


<details>
  <summary>Details</summary>
Motivation: 下一代无线系统在非平稳条件下运行，传统导频辅助估计器开销大，深度学习方法在动态导频模式和时变衰落下性能下降。

Method: 结合模型驱动导频获取与数据驱动CSI重建，使用补丁级自注意力机制和物理感知复合损失函数，确保相位对齐、相关性一致性和时频平滑性。

Result: 在3GPP NR配置下，比LMMSE和LSTM基线在相位不变归一化均方误差上提升约13dB，误码率显著降低，同时导频开销减少16倍。

Conclusion: 基于注意力的架构能够在保证链路质量的同时实现可靠的CSI恢复和增强的频谱效率，解决了非平稳5G及后5G网络中自适应、低开销信道估计的基本瓶颈。

Abstract: Accurate and efficient estimation of Channel State Information (CSI) is
critical for next-generation wireless systems operating under non-stationary
conditions, where user mobility, Doppler spread, and multipath dynamics rapidly
alter channel statistics. Conventional pilot aided estimators incur substantial
overhead, while deep learning approaches degrade under dynamic pilot patterns
and time varying fading. This paper presents a pilot-aided Flash-Attention
Transformer framework that unifies model-driven pilot acquisition with data
driven CSI reconstruction through patch-wise self-attention and a physics aware
composite loss function enforcing phase alignment, correlation consistency, and
time frequency smoothness. Under a standardized 3GPP NR configuration, the
proposed framework outperforms LMMSE and LSTM baselines by approximately 13 dB
in phase invariant normalized mean-square error (NMSE) with markedly lower
bit-error rate (BER), while reducing pilot overhead by 16 times. These results
demonstrate that attention based architectures enable reliable CSI recovery and
enhanced spectral efficiency without compromising link quality, addressing a
fundamental bottleneck in adaptive, low-overhead channel estimation for
non-stationary 5G and beyond-5G networks.

</details>


### [17] [Gradient Clock Synchronization with Practically Constant Local Skew](https://arxiv.org/abs/2511.01420)
*Christoph Lenzen*

Main category: cs.DC

TL;DR: 本文提出了一种改进的梯度时钟同步模型，通过考虑测量误差和频率误差的稳定性而非最坏情况，突破了现有的下界限制，显著提升了同步性能。


<details>
  <summary>Details</summary>
Motivation: 现有GCS方法存在两个主要问题：1) 本地偏移界限依赖于整个系统生命周期内的偏移估计上界；2) 假设本地振荡器频率存在最坏情况偏差，而实际中频率在短期内更稳定。现有部署方法虽然适应真实误差，但无法提供非平凡的本地偏移保证。

Method: 提出改进模型和现有技术的重新分析，仅要求测量和频率误差的稳定性，从而规避现有下界。通过考虑相关时间尺度上的误差变化而非最坏情况，实现性能提升。

Result: 在链路具有统一最坏情况估计误差Δ和相关时间尺度上误差变化δ≪Δ时，本地偏移界限为O(Δ+δ log D)，突破了现有的Ω(Δ log D)下界。同时限制了本地振荡器对δ的影响，使其仅与单个振荡器在相关时间尺度上的频率变化相关。

Conclusion: 该模型在保证自稳定的同时，显著提升了梯度时钟同步性能，并可将结果扩展到外部同步场景，仅需有限的稳定时间增加。

Abstract: Gradient Clock Synchronization (GCS) is the task of minimizing the local
skew, i.e., the clock offset between neighboring clocks, in a larger network.
While asymptotically optimal bounds are known, from a practical perspective
they have crucial shortcomings:
  - Local skew bounds are determined by upper bounds on offset estimation that
need to be guaranteed throughout the entire lifetime of the system.
  - Worst-case frequency deviations of local oscillators from their nominal
rate are assumed, yet frequencies tend to be much more stable in the (relevant)
short term.
  State-of-the-art deployed synchronization methods adapt to the true offset
measurement and frequency errors, but achieve no non-trivial guarantees on the
local skew.
  In this work, we provide a refined model and novel analysis of existing
techniques for solving GCS in this model. By requiring only stability of
measurement and frequency errors, we can circumvent existing lower bounds,
leading to dramatic improvements under very general conditions. For example, if
links exhibit a uniform worst-case estimation error of $\Delta$ and a change in
estimation errors of $\delta\ll \Delta$ on relevant time scales, we bound the
local skew by $O(\Delta+\delta \log D)$ for networks of diameter $D$,
effectively ``breaking'' the established $\Omega(\Delta\log D)$ lower bound,
which holds when $\delta=\Delta$. Similarly, we show how to limit the influence
of local oscillators on $\delta$ to scale with the change of frequency of an
individual oscillator on relevant time scales, rather than a worst-case bound
over all oscillators and the lifetime of the system.
  Moreover, we show how to ensure self-stabilization in this challenging
setting. Last, but not least, we extend all of our results to the scenario of
external synchronization, at the cost of a limited increase in stabilization
time.

</details>


### [18] [Adaptive Multidimensional Quadrature on Multi-GPU Systems](https://arxiv.org/abs/2511.01573)
*Melanie Tonarelli,Simone Riva,Pietro Benedusi,Fabrizio Ferrandi,Rolf Krause*

Main category: cs.DC

TL;DR: 提出了一种分布式自适应求积方法，将多维积分转化为多GPU架构上的分层域分解问题，通过循环轮询策略实现负载重平衡。


<details>
  <summary>Details</summary>
Motivation: 解决多维积分计算中随着自适应过程进展而出现的显著负载不平衡问题，提高高维情况下的计算效率和鲁棒性。

Method: 采用分层域分解方法，将积分域递归划分为子域，基于局部误差估计器指导细化，使用基于循环轮询策略的分散负载重分配方案，通过非阻塞的CUDA感知MPI通信与计算重叠。

Result: 相比最先进的GPU定制包，该方法在高维情况下具有更高效率，对积分函数正则性和目标精度的鲁棒性更好。

Conclusion: 提出的分布式自适应求积方法在多GPU架构上有效解决了负载不平衡问题，在高维积分计算中表现出优越的性能和鲁棒性。

Abstract: We introduce a distributed adaptive quadrature method that formulates
multidimensional integration as a hierarchical domain decomposition problem on
multi-GPU architectures. The integration domain is recursively partitioned into
subdomains whose refinement is guided by local error estimators. Each subdomain
evolves independently on a GPU, which exposes a significant load imbalance as
the adaptive process progresses. To address this challenge, we introduce a
decentralised load redistribution schemes based on a cyclic round-robin policy.
This strategy dynamically rebalance subdomains across devices through
non-blocking, CUDA-aware MPI communication that overlaps with computation. The
proposed strategy has two main advantages compared to a state-of-the-art
GPU-tailored package: higher efficiency in high dimensions; and improved
robustness w.r.t the integrand regularity and the target accuracy.

</details>


### [19] [LARK - Linearizability Algorithms for Replicated Keys in Aerospike](https://arxiv.org/abs/2511.01843)
*Andrew Goodng,Kevin Porter,Thomas Lopatic,Ashish Shinde,Sunil Sayyaparaju,Srinivasan Seshadri,V. Srinivasan*

Main category: cs.DC

TL;DR: LARK是一种同步复制协议，通过消除有序日志和引入分区可用性条件，在线性一致性下显著提高可用性并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统基于日志的共识协议（如Raft、Paxos）在故障恢复时需要暂停提交，且可用性受限。LARK旨在解决这些问题，提供更高的可用性和更低的延迟。

Method: 引入分区可用性条件（PAC），基于整个数据库集群而非固定副本集进行推理；消除有序日志，使分区在领导者变更后立即可用；仅需f+1数据副本即可容忍f次故障。

Result: 相比传统协议，在容忍1次故障时可用性提高约3倍，容忍2次故障时提高约10倍；在数据节点故障时仍能继续提交；支持零停机滚动重启。

Conclusion: LARK在线性一致性保证下，显著提升了系统可用性，降低了延迟和基础设施成本，为分布式数据库提供了更优的复制解决方案。

Abstract: We present LARK (Linearizability Algorithms for Replicated Keys), a
synchronous replication protocol that achieves linearizability while minimizing
latency and infrastructure cost, at significantly higher availability than
traditional quorum-log consensus. LARK introduces Partition Availability
Conditions (PAC) that reason over the entire database cluster rather than fixed
replica sets, improving partition availability under independent failures by
roughly 3x when tolerating one failure and 10x when tolerating two. Unlike
Raft, Paxos, and Viewstamped Replication, LARK eliminates ordered logs,
enabling immediate partition readiness after leader changes -- with at most a
per-key duplicate-resolution round trip when the new leader lacks the latest
copy. Under equal storage budgets -- where both systems maintain only f+1 data
copies to tolerate f failures -- LARK continues committing through data-node
failures while log-based protocols must pause commits for replica rebuilding.
These properties also enable zero-downtime rolling restarts even when
maintaining only two copies. We provide formal safety arguments and a TLA+
specification, and we demonstrate through analysis and experiments that LARK
achieves significant availability gains.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [20] [PDA-LSTM: Knowledge-driven page data arrangement based on LSTM for LCM supression in QLC 3D NAND flash memories](https://arxiv.org/abs/2511.00075)
*Qianhui Li,Weiya Wang,Qianqi Zhao,Tong Qu,Jing He,Xuhong Qiang,Jingwen Hou,Ke Chen,Bao Zhang,Qi Wang*

Main category: cs.AR

TL;DR: 提出PDA-LSTM模型，通过LSTM神经网络优化3D NAND闪存中的数据排列，抑制横向电荷迁移(LCM)，降低比特错误率(BER)，相比无数据排列策略平均BER降低80.4%。


<details>
  <summary>Details</summary>
Motivation: QLC 3D NAND闪存因存储密度提升导致读取裕度变窄，易受横向电荷迁移(LCM)影响。现有算法仅关注页内数据映射，而页间数据排列也能有效抑制LCM。

Method: 设计PDA-LSTM模型，使用LSTM神经网络计算数据排列概率矩阵，将输出矩阵转换为非重复序列生成概率矩阵以辅助训练，优化页间数据排列以最小化LCM的全局影响。

Result: PDA-LSTM相比无数据排列策略平均BER降低80.4%，相比WBVM和DVDS（码长64）分别降低18.4%和15.2%，且无需额外标志位。

Conclusion: PDA-LSTM通过智能数据排列有效抑制LCM，显著降低BER，为QLC 3D NAND闪存提供了高效的错误率控制方案。

Abstract: Quarter level cell (QLC) 3D NAND flash memory is emerging as the predominant
storage solution in the era of artificial intelligence. QLC 3D NAND flash
stores 4 bit per cell to expand the storage density, resulting in narrower read
margins. Constrained to read margins, QLC always suffers from lateral charge
migration (LCM), which caused by non-uniform charge density across adjacent
memory cells. To suppress charge density gap between cells, there are some
algorithm in form of intra-page data mapping such as WBVM, DVDS. However, we
observe inter-page data arrangements also approach the suppression. Thus, we
proposed an intelligent model PDA-LSTM to arrange intra-page data for LCM
suppression, which is a physics-knowledge-driven neural network model. PDA-LSTM
applies a long-short term memory (LSTM) neural network to compute a data
arrangement probability matrix from input page data pattern. The arrangement is
to minimize the global impacts derived from the LCM among wordlines. Since each
page data can be arranged only once, we design a transformation from output
matrix of LSTM network to non-repetitive sequence generation probability matrix
to assist training process. The arranged data pattern can decrease the bit
error rate (BER) during data retention. In addition, PDA-LSTM do not need extra
flag bits to record data transport of 3D NAND flash compared with WBVM, DVDS.
The experiment results show that the PDA-LSTM reduces the average BER by 80.4%
compared with strategy without data arrangement, and by 18.4%, 15.2% compared
respectively with WBVM and DVDS with code-length 64.

</details>


### [21] [H-FA: A Hybrid Floating-Point and Logarithmic Approach to Hardware Accelerated FlashAttention](https://arxiv.org/abs/2511.00295)
*Kosmas Alexandridis,Giorgos Dimitrakopoulos*

Main category: cs.AR

TL;DR: H-FA是一种改进的FlashAttention硬件实现，通过混合使用浮点数和定点数对数域表示，在保持性能的同时显著降低了硬件面积和功耗。


<details>
  <summary>Details</summary>
Motivation: Transformer的注意力机制在处理长序列时存在计算瓶颈，FlashAttention虽然通过分块计算缓解了这个问题，但其硬件实现仍有优化空间，特别是在降低面积和功耗方面。

Method: 使用浮点数计算查询和键矩阵的注意力分数，同时在对数域中使用定点数算术简化softmax归一化和与值矩阵乘法的融合计算，将向量级的浮点乘除操作替换为定点数的加减操作。

Result: 在28nm工艺下，H-FA相比仅使用浮点数据路径的FlashAttention并行硬件架构，平均减少了26.5%的面积和23.4%的功耗，且不影响性能。

Conclusion: H-FA通过混合浮点-定点对数域计算方法，成功实现了FlashAttention硬件实现的面积和功耗优化，为长序列Transformer的高效硬件加速提供了有效解决方案。

Abstract: Transformers have significantly advanced AI and machine learning through
their powerful attention mechanism. However, computing attention on long
sequences can become a computational bottleneck. FlashAttention mitigates this
by fusing the softmax and matrix operations into a tiled computation pattern
that decouples performance from sequence length. Though designed for GPUs, its
simplicity also makes it well suited for direct hardware acceleration. To
improve hardware implementation, we compute FlashAttention using a mixture of
floating-point and fixed-point logarithm domain representations. Floating-point
is used to compute attention scores from query and key matrices, while
logarithmic computation simplifies the fused computation of softmax
normalization and the multiplication with the value matrix. This
transformation, called H-FA, replaces vector-wide floating-point multiplication
and division operations by additions and subtractions implemented efficiently
with fixed-point arithmetic in the logarithm domain. Exponential function
evaluations are effectively omitted and fused with the rest operations, and the
final result is directly returned to floating-point arithmetic without any
additional hardware overhead. Hardware implementation results at 28nm
demonstrate that H-FA achieves a 26.5% reduction in area and a 23.4% reduction
in power, on average, compared to FlashAttention parallel hardware
architectures built solely with floating-point datapaths, without hindering
performance.

</details>


### [22] [Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits](https://arxiv.org/abs/2511.00321)
*Dowon Kim,MinJae Lee,Janghyeon Kim,HyuckSung Kwon,Hyeonggyu Jeong,Sang-Soo Park,Minyong Yoon,Si-Dong Roh,Yongsuk Kwon,Jinin So,Jungwook Choi*

Main category: cs.AR

TL;DR: 提出了一种基于CXL的PNM KV缓存管理系统，通过近内存处理技术解决大语言模型长上下文推理中的内存和计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型上下文窗口扩展到数百万token，KV缓存管理面临严重的内存和计算瓶颈，现有的CXL非驱逐框架在长上下文场景下仍存在昂贵的数据传输成本。

Method: 设计CXL启用的KV缓存管理系统，将token页面选择卸载到CXL内存中的PNM加速器，提出混合并行化策略和稳态token选择机制。

Result: 在405B参数和1M token上下文的LLM上实现了一致的性能提升，PNM-KV和PnG-KV方案分别实现了最高21.9倍吞吐量提升、60倍每token能耗降低和7.3倍总成本效率提升。

Conclusion: CXL启用的多PNM架构可以作为未来长上下文LLM推理的可扩展基础架构。

Abstract: The expansion of context windows in large language models (LLMs) to
multi-million tokens introduces severe memory and compute bottlenecks,
particularly in managing the growing Key-Value (KV) cache. While Compute
Express Link (CXL) enables non-eviction frameworks that offload the full
KV-cache to scalable external memory, these frameworks still suffer from costly
data transfers when recalling non-resident KV tokens to limited GPU memory as
context lengths increase. This work proposes scalable Processing-Near-Memory
(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that
coordinates memory and computation beyond GPU limits. Our design offloads token
page selection to a PNM accelerator within CXL memory, eliminating costly
recalls and enabling larger GPU batch sizes. We further introduce a hybrid
parallelization strategy and a steady-token selection mechanism to enhance
compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM
system, our solution delivers consistent performance gains for LLMs with up to
405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)
and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x
throughput improvement, up to 60x lower energy per token, and up to 7.3x better
total cost efficiency than the baseline, demonstrating that CXL-enabled
multi-PNM architectures can serve as a scalable backbone for future
long-context LLM inference.

</details>


### [23] [Simulation-Driven Evaluation of Chiplet-Based Architectures Using VisualSim](https://arxiv.org/abs/2511.01244)
*Wajid Ali,Ayaz Akram,Deepak Shankar*

Main category: cs.AR

TL;DR: 使用VisualSim仿真多芯片系统架构，重点研究基于小芯片的系统建模和性能分析，评估通信延迟、内存访问效率等关键指标。


<details>
  <summary>Details</summary>
Motivation: 传统单片芯片面临制造成本、能效和性能扩展的挑战，小芯片技术通过集成多个模块化硅单元提供更灵活、可扩展且成本更低的解决方案。

Method: 开发了基于小芯片系统的详细仿真模型，包含多核ARM处理器集群，通过ARM CMN600片上网络实现高效通信，使用VisualSim框架评估系统性能。

Result: 仿真揭示了影响小芯片系统性能的关键因素，包括芯片间通信延迟、内存访问效率和功耗性能权衡。

Conclusion: 该研究为优化未来基于小芯片的半导体设计提供了基础，展示了仿真驱动分析在小芯片系统开发中的重要性。

Abstract: This paper focuses on the simulation of multi-die System-on-Chip (SoC)
architectures using VisualSim, emphasiz- ing chiplet-based system modeling and
performance analysis. Chiplet technology presents a promising alternative to
traditional monolithic chips, which face increasing challenges in manufactur-
ing costs, power efficiency, and performance scaling. By integrat- ing multiple
small modular silicon units into a single package, chiplet-based architectures
offer greater flexibility and scalability at a lower overall cost. In this
study, we developed a detailed sim- ulation model of a chiplet-based system,
incorporating multicore ARM processor clusters interconnected through a ARM
CMN600 network-on-chip (NoC) for efficient communication [4], [7]. The
simulation framework in VisualSim enables the evaluation of critical system
metrics, including inter-chiplet communication latency, memory access
efficiency, workload distribution, and the power-performance tradeoff under
various workloads. Through simulation-driven insights, this research highlights
key factors influencing chiplet system performance and provides a foundation
for optimizing future chiplet-based semiconductor designs.

</details>
