{"id": "2507.17766", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17766", "abs": "https://arxiv.org/abs/2507.17766", "authors": ["Felix Quinque", "Alan Aboudib", "Szymon Fonau", "Rodrigo Lopez Portillo Alcocer", "Brian McCrindle", "Steffen Cruz"], "title": "Incentivised Orchestrated Training Architecture (IOTA): A Technical Primer for Release", "comment": null, "summary": "In August 2024, Bittensor's Subnet 9 (SN9) demonstrated that a distributed\nnetwork of incentivized, permissionless actors could each pretrain large\nlanguage models (LLMs) ranging from 700 million to 14 billion parameters, while\nsurpassing established baselines. While that work validated blockchain-based\ndecentralized pretraining as viable, it contained core issues: (i) every miner\nhad to fit an entire model locally, and (ii) \"winner-takes-all\" rewards\nencouraged model hoarding.\n  Here we introduce IOTA (Incentivized Orchestrated Training Architecture), an\narchitecture that addresses these limitations by transforming SN9's previously\nisolated competitors into a single cooperating unit that can scale arbitrarily\nwhile still rewarding each contributor fairly.\n  Key preliminary results: (1) Data- and Pipeline-parallel SWARM architecture -\nAn orchestrator distributes model layers across heterogeneous miners and\nstreams activations between them, enabling model sizes to scale with the number\nof participants rather than being constrained by the VRAM of a single machine;\n(2) Granular, continuous incentives - Validators measure each miner's\ncontribution and allocate token emissions proportionally; (3) Activation\ncompression - We used model-bottlenecks to cut communication bandwidths of\nactivations by up to 128x, vastly improving training speed; (4) Butterfly\nAll-Reduce - Miners average disjoint parameter slices in O(1) bandwidth,\noffering linear scalability, redundancy and built-in collusion detection; (5)\nCLASP (Contribution Loss Assessment via Sampling of Pathways) - A fair\nattribution scheme assigns credit to miners proportional to their marginal\nutility and detects exploits, even when contributions are interdependent across\nthe pipeline.", "AI": {"tldr": "Bittensor\u7684SN9\u5c55\u793a\u4e86\u5206\u5e03\u5f0f\u7f51\u7edc\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4f46\u5b58\u5728\u6a21\u578b\u672c\u5730\u5316\u548c\u5956\u52b1\u5206\u914d\u95ee\u9898\u3002IOTA\u67b6\u6784\u901a\u8fc7\u534f\u4f5c\u8bad\u7ec3\u3001\u516c\u5e73\u5956\u52b1\u548c\u4f18\u5316\u6280\u672f\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3SN9\u4e2d\u6a21\u578b\u672c\u5730\u5316\u548c\u5956\u52b1\u5206\u914d\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u516c\u5e73\u7684\u5206\u5e03\u5f0f\u9884\u8bad\u7ec3\u3002", "method": "\u91c7\u7528\u6570\u636e\u5e76\u884c\u548c\u6d41\u6c34\u7ebf\u5e76\u884c\u7684SWARM\u67b6\u6784\uff0c\u7ed3\u5408\u6fc0\u6d3b\u538b\u7f29\u3001Butterfly All-Reduce\u548cCLASP\u516c\u5e73\u5956\u52b1\u673a\u5236\u3002", "result": "\u5b9e\u73b0\u4e86\u6a21\u578b\u89c4\u6a21\u7684\u52a8\u6001\u6269\u5c55\u3001\u901a\u4fe1\u5e26\u5bbd\u4f18\u5316\u3001\u7ebf\u6027\u53ef\u6269\u5c55\u6027\u53ca\u516c\u5e73\u8d21\u732e\u8bc4\u4f30\u3002", "conclusion": "IOTA\u901a\u8fc7\u534f\u4f5c\u548c\u4f18\u5316\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u5206\u5e03\u5f0f\u9884\u8bad\u7ec3\u7684\u6548\u7387\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2507.17769", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17769", "abs": "https://arxiv.org/abs/2507.17769", "authors": ["Kan Zhu", "Haiyang Shi", "Le Xu", "Jiaxin Shan", "Arvind Krishnamurthy", "Baris Kasikci", "Liguang Xie"], "title": "PolyServe: Efficient Multi-SLO Serving at Scale", "comment": null, "summary": "Advances in Large Language Models (LLMs) have led to a surge of LLM-powered\napplications. These applications have diverse token-generation latency\nrequirements. As a result, simply classifying workloads as latency-sensitive\n(LS) or best-effort (BE) overlooks the nuances within the latency-sensitive\ncategory and results in suboptimal user experiences and scheduling\nopportunities. However, efficiently serving requests with multiple SLO\nrequirements poses significant challenges. First, all requests within a batch\ngenerate new tokens simultaneously, which can misalign them with their distinct\nSLO requirements. Moreover, while existing systems focus on auto-scaling for\nhandling various overall request rates, the diversity of SLOs necessitates\nfine-grained auto-scaling among these SLO tiers. Finally, unlike LS/BE\nscenarios, where BE requests can be aborted at any time to ensure the SLO\nattainment of LS requests, those with different latency-sensitive SLOs cannot\ntolerate prolonged delays, and tail latency must be controlled.\n  To tackle these challenges, we propose PolyServe, a novel multi-SLO\nscheduling policy at scale that maintains high SLO attainment while maximizing\nthroughput. PolyServe first groups requests into multiple bins based on their\nper-token latency requirement, then schedules each bin to a subset of the\nserver fleet. PolyServe routes requests to the highest-load but still\nSLO-attainable server to create a load gradient that facilitates auto-scaling.\nTo increase utilization, PolyServe permits looser-SLO requests to share\ntighter-SLO instances when their own servers are saturated. PolyServe uses\nprofiling data to guide scheduling decisions and manage tail latency through\nrequest-wait-time-aware scheduling, dynamic chunking, and continuous chunked\nprefill prediction. PolyServe achieves 1.23x goodput gain compared to existing\npolicies, achieving up to 92.5% of optimal goodput.", "AI": {"tldr": "PolyServe\u662f\u4e00\u79cd\u65b0\u578b\u7684\u591aSLO\u8c03\u5ea6\u7b56\u7565\uff0c\u65e8\u5728\u9ad8\u6548\u5904\u7406\u5177\u6709\u4e0d\u540c\u5ef6\u8fdf\u9700\u6c42\u7684LLM\u8bf7\u6c42\uff0c\u901a\u8fc7\u5206\u7ec4\u3001\u8d1f\u8f7d\u5747\u8861\u548c\u8d44\u6e90\u5171\u4eab\u4f18\u5316\u541e\u5410\u91cf\u548cSLO\u8fbe\u6210\u7387\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5c06\u5de5\u4f5c\u8d1f\u8f7d\u7b80\u5355\u5206\u4e3a\u5ef6\u8fdf\u654f\u611f\uff08LS\uff09\u548c\u5c3d\u529b\u800c\u4e3a\uff08BE\uff09\uff0c\u5ffd\u7565\u4e86\u5ef6\u8fdf\u654f\u611f\u7c7b\u522b\u5185\u7684\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u7528\u6237\u4f53\u9a8c\u548c\u8c03\u5ea6\u673a\u4f1a\u4e0d\u4f73\u3002", "method": "PolyServe\u5c06\u8bf7\u6c42\u6309\u6bcf\u4ee4\u724c\u5ef6\u8fdf\u9700\u6c42\u5206\u7ec4\uff0c\u8c03\u5ea6\u5230\u670d\u52a1\u5668\u5b50\u96c6\uff0c\u5e76\u901a\u8fc7\u8d1f\u8f7d\u68af\u5ea6\u3001\u8d44\u6e90\u5171\u4eab\u548c\u52a8\u6001\u586b\u5145\u9884\u6d4b\u4f18\u5316\u8c03\u5ea6\u3002", "result": "PolyServe\u76f8\u6bd4\u73b0\u6709\u7b56\u7565\u5b9e\u73b0\u4e861.23\u500d\u7684\u541e\u5410\u91cf\u589e\u76ca\uff0c\u8fbe\u5230\u6700\u4f18\u541e\u5410\u91cf\u768492.5%\u3002", "conclusion": "PolyServe\u901a\u8fc7\u7cbe\u7ec6\u5316\u7684\u591aSLO\u8c03\u5ea6\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u8bf7\u6c42\u5904\u7406\u7684\u6548\u7387\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2507.17770", "categories": ["cs.DC", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.17770", "abs": "https://arxiv.org/abs/2507.17770", "authors": ["Pei-Kun Yang"], "title": "Comparative Evaluation of PyTorch, JAX, SciPy, and Neal for Solving QUBO Problems at Scale", "comment": "14 pages, 5 figures", "summary": "Quadratic Unconstrained Binary Optimization (QUBO) is a versatile framework\nfor modeling combinatorial optimization problems. This study benchmarks five\nsoftware-based QUBO solvers: Neal, PyTorch (CPU), PyTorch (GPU), JAX, and\nSciPy, on randomly generated QUBO matrices ranging from 1000x1000 to\n45000x45000, under six convergence thresholds from 10^-1 to 10^-6. We evaluate\ntheir performance in terms of solution quality (energy) and computational time.\nAmong the solvers tested, Neal achieved the lowest energy values but was\nlimited to problems with up to 6000 variables due to high memory consumption.\nPyTorch produced slightly higher energy results than Neal but demonstrated\nsuperior scalability, solving instances with up to 45000 variables. Its support\nfor GPU acceleration and CPU multi-threading also resulted in significantly\nshorter runtimes. JAX yielded energy values slightly above those of PyTorch and\nwas limited to 25000 variables, with runtimes comparable to PyTorch on GPU.\nSciPy was the most constrained solver, handling only up to 6000 variables and\nconsistently producing the highest energy values with the longest computation\ntimes. These findings highlight trade-offs between solution quality,\nscalability, and runtime efficiency, and suggest that PyTorch is the most\nbalanced choice for large-scale QUBO problems when computational resources\npermit.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e94\u79cd\u57fa\u4e8e\u8f6f\u4ef6\u7684QUBO\u6c42\u89e3\u5668\u5728\u968f\u673a\u751f\u6210\u7684QUBO\u77e9\u9635\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0Neal\u5728\u89e3\u8d28\u91cf\u4e0a\u6700\u4f18\u4f46\u53ef\u6269\u5c55\u6027\u5dee\uff0cPyTorch\u5728\u53ef\u6269\u5c55\u6027\u548c\u8fd0\u884c\u65f6\u95f4\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u8bc4\u4f30\u4e0d\u540c\u8f6f\u4ef6\u6c42\u89e3\u5668\u5728\u89e3\u51b3\u5927\u89c4\u6a21QUBO\u95ee\u9898\u65f6\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u9009\u62e9\u4f9d\u636e\u3002", "method": "\u751f\u6210\u968f\u673aQUBO\u77e9\u9635\uff081000x1000\u523045000x45000\uff09\uff0c\u5728\u516d\u79cd\u6536\u655b\u9608\u503c\u4e0b\u6d4b\u8bd5\u4e94\u79cd\u6c42\u89e3\u5668\u7684\u89e3\u8d28\u91cf\u548c\u8ba1\u7b97\u65f6\u95f4\u3002", "result": "Neal\u89e3\u8d28\u91cf\u6700\u4f18\u4f46\u4ec5\u652f\u63016000\u53d8\u91cf\uff1bPyTorch\u89e3\u8d28\u91cf\u7a0d\u900a\u4f46\u53ef\u6269\u5c55\u81f345000\u53d8\u91cf\u4e14\u8fd0\u884c\u65f6\u95f4\u77ed\uff1bJAX\u4ecb\u4e8e\u4e24\u8005\u4e4b\u95f4\uff1bSciPy\u8868\u73b0\u6700\u5dee\u3002", "conclusion": "PyTorch\u5728\u5927\u89c4\u6a21QUBO\u95ee\u9898\u4e2d\u8868\u73b0\u6700\u5747\u8861\uff0c\u9002\u5408\u8d44\u6e90\u5145\u8db3\u65f6\u4f7f\u7528\u3002"}}
{"id": "2507.17771", "categories": ["cs.DC", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.17771", "abs": "https://arxiv.org/abs/2507.17771", "authors": ["Dmitri Lyalikov"], "title": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN Inference Acceleration", "comment": null, "summary": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5f02\u6784\u548c\u9886\u57df\u7279\u5b9a\u67b6\u6784\u5728\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u91cd\u70b9\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0aCNN\u90e8\u7f72\u7684\u7cfb\u7edf\u96c6\u6210\u548c\u7f16\u8bd1/\u6267\u884c\u6a21\u578b\u95ee\u9898\uff0c\u5c55\u793a\u4e86RISC-V Vector 1.0\u6269\u5c55\u5728\u51cf\u5c11\u9884\u5904\u7406\u74f6\u9888\u548cCPU\u56de\u9000\u8fc7\u7a0b\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u5668\u548c\u795e\u7ecf\u5904\u7406\u5355\u5143\u7684\u591a\u6837\u5316\uff0c\u4f20\u7edf\u7845\u7f29\u653e\u63a5\u8fd1\u6781\u9650\uff0c\u5d4c\u5165\u5f0fSoC\u9700\u8981\u89e3\u51b3\u6027\u80fd/\u529f\u8017\u6743\u8861\u95ee\u9898\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u6548\u7684\u7cfb\u7edf\u96c6\u6210\u548c\u7f16\u8bd1/\u6267\u884c\u6a21\u578b\u3002", "method": "\u5229\u7528RISC-V Vector 1.0\u6269\u5c55\uff0c\u8bbe\u8ba1\u7075\u6d3b\u7684\u7f16\u7a0b\u6a21\u578b\u548c\u7f13\u5b58\u5c42\u6b21\u7ed3\u6784\uff0c\u4f18\u5316\u9884\u5904\u7406\u548cCPU\u56de\u9000\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u56fe\u50cf\u9884\u5904\u7406\u901f\u5ea6\u63d0\u53479\u500d\uff0cYOLOv3\u56de\u9000\u5c42\u6267\u884c\u901f\u5ea6\u63d0\u53473\u500d\uff0c\u540c\u65f6\u529f\u8017\u4f4e\u4e8e\u4f20\u7edf\u5e76\u884c\u6267\u884c\u5e73\u53f0\u3002", "conclusion": "RISC-V Vector 1.0\u6269\u5c55\u4e3a\u52a0\u901f\u5668\u4e30\u5bcc\u7684\u5d4c\u5165\u5f0fSoC\u63d0\u4f9b\u4e86\u5e73\u8861\u8ba1\u7b97\u548c\u5185\u5b58\u5360\u7528\u7684\u7075\u6d3b\u7f16\u7a0b\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u6570\u636e\u6d41\u7684\u6548\u7387\u3002"}}
{"id": "2507.18509", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.18509", "abs": "https://arxiv.org/abs/2507.18509", "authors": ["Henning Urbat"], "title": "Higher-Order Behavioural Conformances via Fibrations", "comment": null, "summary": "Coinduction is a widely used technique for establishing behavioural\nequivalence of programs in higher-order languages. In recent years, the rise of\nlanguages with quantitative (e.g.~probabilistic) features has led to extensions\nof coinductive methods to more refined types of behavioural conformances, most\nnotably notions of behavioural distance. To guarantee soundness of coinductive\nreasoning, one needs to show that the behavioural conformance at hand forms a\nprogram congruence, i.e. it is suitably compatible with the operations of the\nlanguage. This is usually achieved by a complex proof technique known as\n\\emph{Howe's method}, which needs to be carefully adapted to both the specific\nlanguage and the targeted notion of behavioural conformance. We develop a\nuniform categorical approach to Howe's method that features two orthogonal\ndimensions of abstraction: (1) the underlying higher-order language is modelled\nby an \\emph{abstract higher-order specification} (AHOS), a novel and very\ngeneral categorical account of operational semantics, and (2) notions of\nbehavioural conformance (such as relations or metrics) are modelled via\nfibrations over the base category of an AHOS. Our main result is a fundamental\ncongruence theorem at this level of generality: Under natural conditions on the\ncategorical ingredients and the operational rules of a language modelled by an\nAHOS, the greatest behavioural (bi)conformance on its operational model forms a\ncongruence. We illustrate our theory by deriving congruence of bisimilarity and\nbehavioural pseudometrics for probabilistic higher-order languages.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u8303\u7574\u5316\u65b9\u6cd5\uff08Howe's method\uff09\uff0c\u7528\u4e8e\u8bc1\u660e\u9ad8\u9636\u8bed\u8a00\u4e2d\u884c\u4e3a\u4e00\u81f4\u6027\uff08\u5982\u5173\u7cfb\u6216\u5ea6\u91cf\uff09\u7684\u7a0b\u5e8f\u540c\u4f59\u6027\u3002", "motivation": "\u968f\u7740\u5177\u6709\u5b9a\u91cf\u7279\u5f81\uff08\u5982\u6982\u7387\u6027\uff09\u8bed\u8a00\u7684\u5174\u8d77\uff0c\u9700\u8981\u6269\u5c55\u5171\u5f52\u7eb3\u65b9\u6cd5\u4ee5\u652f\u6301\u66f4\u7cbe\u7ec6\u7684\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u5e76\u786e\u4fdd\u5176\u4f5c\u4e3a\u7a0b\u5e8f\u540c\u4f59\u6027\u7684\u6b63\u786e\u6027\u3002", "method": "\u901a\u8fc7\u62bd\u8c61\u9ad8\u9636\u89c4\u8303\uff08AHOS\uff09\u5efa\u6a21\u8bed\u8a00\uff0c\u5e76\u4f7f\u7528\u7ea4\u7ef4\u5316\u5efa\u6a21\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u8303\u7574\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u81ea\u7136\u6761\u4ef6\u4e0b\uff0cAHOS\u5efa\u6a21\u7684\u8bed\u8a00\u7684\u6700\u5927\u884c\u4e3a\uff08\u53cc\uff09\u4e00\u81f4\u6027\u5f62\u6210\u540c\u4f59\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u6982\u7387\u9ad8\u9636\u8bed\u8a00\uff0c\u6210\u529f\u8bc1\u660e\u4e86\u53cc\u76f8\u4f3c\u6027\u548c\u884c\u4e3a\u4f2a\u5ea6\u91cf\u7684\u540c\u4f59\u6027\u3002"}}
{"id": "2507.18040", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.18040", "abs": "https://arxiv.org/abs/2507.18040", "authors": ["Harsh Sharma", "Janardhan Rao Doppa", "Umit Y. Ogras", "Partha Pratim Pande"], "title": "Designing High-Performance and Thermally Feasible Multi-Chiplet Architectures enabled by Non-bendable Glass Interposer", "comment": "Paper accepted at ACM Transactions on Embedded Computing Systems. To\n  be presented in Taiwan, Sept. 2025", "summary": "Multi-chiplet architectures enabled by glass interposer offer superior\nelectrical performance, enable higher bus widths due to reduced crosstalk, and\nhave lower capacitance in the redistribution layer than current silicon\ninterposer-based systems. These advantages result in lower energy per bit,\nhigher communication frequencies, and extended interconnect range. However,\ndeformation of the package (warpage) in glass interposer-based systems becomes\na critical challenge as system size increases, leading to severe mechanical\nstress and reliability concerns. Beyond a certain size, conventional packaging\ntechniques fail to manage warpage effectively, necessitating new approaches to\nmitigate warpage induced bending with scalable performance for glass interposer\nbased multi-chiplet systems. To address these inter-twined challenges, we\npropose a thermal-, warpage-, and performance-aware design framework that\nemploys architecture and packaging co-optimization. The proposed framework\ndisintegrates the surface and embedded chiplets to balance conflicting design\nobjectives, ensuring optimal trade-offs between performance, power, and\nstructural reliability. Our experiments demonstrate that optimized\nmulti-chiplet architectures from our design framework achieve up to 64.7%\nperformance improvement and 40% power reduction compared to traditional 2.5D\nsystems to execute deep neural network workloads with lower fabrication costs.", "AI": {"tldr": "\u73bb\u7483\u4e2d\u4ecb\u5c42\u591a\u82af\u7247\u67b6\u6784\u5728\u7535\u6c14\u6027\u80fd\u548c\u80fd\u8017\u65b9\u9762\u4f18\u4e8e\u7845\u4e2d\u4ecb\u5c42\u7cfb\u7edf\uff0c\u4f46\u968f\u7740\u7cfb\u7edf\u5c3a\u5bf8\u589e\u5927\uff0c\u5c01\u88c5\u53d8\u5f62\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u70ed\u3001\u53d8\u5f62\u548c\u6027\u80fd\u611f\u77e5\u7684\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u67b6\u6784\u4e0e\u5c01\u88c5\u534f\u540c\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u548c\u529f\u8017\u964d\u4f4e\u3002", "motivation": "\u89e3\u51b3\u73bb\u7483\u4e2d\u4ecb\u5c42\u591a\u82af\u7247\u7cfb\u7edf\u56e0\u5c3a\u5bf8\u589e\u5927\u5bfc\u81f4\u7684\u5c01\u88c5\u53d8\u5f62\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u5316\u6027\u80fd\u548c\u529f\u8017\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u70ed\u3001\u53d8\u5f62\u548c\u6027\u80fd\u611f\u77e5\u7684\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u67b6\u6784\u4e0e\u5c01\u88c5\u534f\u540c\u4f18\u5316\uff0c\u5e73\u8861\u6027\u80fd\u3001\u529f\u8017\u548c\u7ed3\u6784\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f18\u5316\u540e\u7684\u591a\u82af\u7247\u67b6\u6784\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u534764.7%\uff0c\u529f\u8017\u964d\u4f4e40%\uff0c\u4e14\u5236\u9020\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u6846\u67b6\u4e3a\u73bb\u7483\u4e2d\u4ecb\u5c42\u591a\u82af\u7247\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u3001\u529f\u8017\u548c\u53ef\u9760\u6027\u7684\u4f18\u5316\u3002"}}
{"id": "2507.17772", "categories": ["cs.DC", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17772", "abs": "https://arxiv.org/abs/2507.17772", "authors": ["Ahmad Alhonainy", "Praveen Rao"], "title": "Caching Techniques for Reducing the Communication Cost of Federated Learning in IoT Environments", "comment": "Journal", "summary": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f13\u5b58\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff08FL\uff09\uff0c\u901a\u8fc7FIFO\u3001LRU\u548c\u4f18\u5148\u7ea7\u7b56\u7565\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u6a21\u578b\u66f4\u65b0\u4f20\u8f93\uff0c\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u5206\u5e03\u5f0f\u8bbe\u5907\u4e0a\u8bad\u7ec3\u5171\u4eab\u6a21\u578b\u65f6\uff0c\u901a\u4fe1\u6210\u672c\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u3002", "method": "\u5f15\u5165FIFO\u3001LRU\u548c\u57fa\u4e8e\u4f18\u5148\u7ea7\u7684\u7f13\u5b58\u7b56\u7565\uff0c\u9009\u62e9\u6027\u8f6c\u53d1\u91cd\u8981\u6a21\u578b\u66f4\u65b0\u3002", "result": "\u5728CIFAR-10\u548c\u533b\u7597\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u4fe1\u91cf\u51cf\u5c11\u4e14\u7cbe\u5ea6\u635f\u5931\u6781\u5c0f\u3002", "conclusion": "\u667a\u80fd\u7f13\u5b58\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u548c\u5185\u5b58\u6548\u7387\uff0c\u652f\u6301\u8fb9\u7f18\u7269\u8054\u7f51\u4e2d\u7684\u53ef\u9760\u8054\u90a6\u5b66\u4e60\uff0c\u9002\u7528\u4e8e\u667a\u6167\u57ce\u5e02\u548c\u533b\u7597\u7b49\u5ef6\u8fdf\u654f\u611f\u573a\u666f\u3002"}}
{"id": "2507.18454", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.18454", "abs": "https://arxiv.org/abs/2507.18454", "authors": ["Juntao Zhao", "Jiuru Li", "Chuan Wu"], "title": "Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving", "comment": null, "summary": "Utilizing CPUs to serve large language models (LLMs) is a resource-friendly\nalternative to GPU serving. Existing CPU-based solutions ignore workload\ndifferences between the prefill and the decode phases of LLM inference,\napplying a static per-NUMA (Non-Uniform Memory Access) node model partition and\nutilizing vendor libraries for operator-level execution, which is suboptimal.\nWe propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses\ndifferent execution plans for the prefill and decode phases and optimizes them\nseparately.\n  We evaluate Sandwich across diverse baselines and datasets on five CPU\nplatforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON.\nSandwich achieves an average 2.01x throughput improvement and 90% satisfactory\ntime-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up\nto 3.40x lower requirements in single sequence serving, and significant\nimprovement in Goodput in continuous-batching serving. The GEMM kernels\ngenerated by Sandwich outperform representative vendor kernels and other\ndynamic shape solutions, achieving performance comparable to static compilers\nwith three orders of magnitude less kernel tuning costs.", "AI": {"tldr": "Sandwich\u662f\u4e00\u79cd\u57fa\u4e8eCPU\u7684LLM\u670d\u52a1\u5f15\u64ce\uff0c\u9488\u5bf9\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u91c7\u7528\u4e0d\u540c\u7684\u6267\u884c\u8ba1\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CPU\u89e3\u51b3\u65b9\u6848\u5ffd\u7565\u4e86LLM\u63a8\u7406\u4e2d\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u7684\u5de5\u4f5c\u8d1f\u8f7d\u5dee\u5f02\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faSandwich\u5f15\u64ce\uff0c\u5206\u522b\u4f18\u5316\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u7684\u6267\u884c\u8ba1\u5212\uff0c\u5e76\u5728\u591a\u79cdCPU\u5e73\u53f0\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "Sandwich\u5728\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u6027\u80fd\u548c\u5185\u6838\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Sandwich\u901a\u8fc7\u786c\u4ef6\u4f18\u5316\u548c\u52a8\u6001\u6267\u884c\u8ba1\u5212\uff0c\u4e3aCPU\u4e0a\u7684LLM\u670d\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17773", "categories": ["cs.DC", "cs.LG", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.17773", "abs": "https://arxiv.org/abs/2507.17773", "authors": ["Zhongzhen Wen", "Yinghui Zhang", "Zhong Li", "Zhongxin Liu", "Linna Xie", "Tian Zhang"], "title": "MultiKernelBench: A Multi-Platform Benchmark for Kernel Generation", "comment": null, "summary": "The automatic generation of deep learning (DL) kernels using large language\nmodels (LLMs) has emerged as a promising approach to reduce the manual effort\nand hardware-specific expertise required for writing high-performance operator\nimplementations. However, existing benchmarks for evaluating LLMs in this\ndomain suffer from limited hardware support, coarse-grained kernel\ncategorization, and imbalanced task coverage. To address these limitations, we\nintroduce MultiKernelBench, the first comprehensive, multi-platform benchmark\nfor LLM-based DL kernel generation. MultiKernelBench spans 285 tasks across 14\nwell-defined kernel categories and supports three major hardware platforms:\nNvidia GPUs, Huawei NPUs, and Google TPUs. To enable future extensibility, we\ndesign a modular backend abstraction layer that decouples platform-specific\nlogic from the core benchmarking infrastructure, allowing easy integration of\nnew hardware platforms. We further propose a simple yet effective\ncategory-aware one-shot prompting method that improves generation quality by\nproviding in-category exemplars. Through systematic evaluations of seven\nstate-of-the-art LLMs, we reveal significant variation in task difficulty, poor\ngeneralization to platforms with less training exposure, and the effectiveness\nof targeted prompting strategies. MultiKernelBench is publicly available at\nhttps://github.com/wzzll123/MultiKernelBench.", "AI": {"tldr": "MultiKernelBench\u662f\u4e00\u4e2a\u5168\u9762\u7684\u591a\u5e73\u53f0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u6df1\u5ea6\u5b66\u4e60\u5185\u6838\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u51cf\u5c11\u624b\u52a8\u7f16\u5199\u9ad8\u6027\u80fd\u6df1\u5ea6\u5b66\u4e60\u5185\u6838\u7684\u5de5\u4f5c\u91cf\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u786c\u4ef6\u652f\u6301\u3001\u4efb\u52a1\u8986\u76d6\u548c\u5206\u7c7b\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e86MultiKernelBench\uff0c\u652f\u6301285\u4e2a\u4efb\u52a1\u548c14\u4e2a\u5185\u6838\u7c7b\u522b\uff0c\u8986\u76d6\u4e09\u79cd\u786c\u4ef6\u5e73\u53f0\uff0c\u5e76\u5f15\u5165\u4e86\u6a21\u5757\u5316\u540e\u7aef\u62bd\u8c61\u5c42\u548c\u7c7b\u522b\u611f\u77e5\u7684\u4e00\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u3002", "result": "\u8bc4\u4f30\u4e86\u4e03\u79cd\u5148\u8fdbLLM\uff0c\u63ed\u793a\u4e86\u4efb\u52a1\u96be\u5ea6\u5dee\u5f02\u3001\u5bf9\u65b0\u786c\u4ef6\u5e73\u53f0\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4ee5\u53ca\u9488\u5bf9\u6027\u63d0\u793a\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "MultiKernelBench\u4e3aLLM\u5728\u6df1\u5ea6\u5b66\u4e60\u5185\u6838\u751f\u6210\u9886\u57df\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.18581", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.18581", "abs": "https://arxiv.org/abs/2507.18581", "authors": ["Ravan Nazaraliyev", "Saber Ganjisaffar", "Nurlan Nazaraliyev", "Nael Abu-Ghazaleh"], "title": "PRACtical: Subarray-Level Counter Update and Bank-Level Recovery Isolation for Efficient PRAC Rowhammer Mitigation", "comment": null, "summary": "As DRAM density increases, Rowhammer becomes more severe due to heightened\ncharge leakage, reducing the number of activations needed to induce bit flips.\nThe DDR5 standard addresses this threat with in-DRAM per-row activation\ncounters (PRAC) and the Alert Back-Off (ABO) signal to trigger mitigation.\nHowever, PRAC adds performance overhead by incrementing counters during the\nprecharge phase, and recovery refreshes stalls the entire memory channel, even\nif only one bank is under attack.\n  We propose PRACtical, a performance-optimized approach to PRAC+ABO that\nmaintains the same security guarantees. First, we reduce counter update latency\nby introducing a centralized increment circuit, enabling overlap between\ncounter updates and subsequent row activations in other subarrays. Second, we\nenhance the $RFM_{ab}$ mitigation by enabling bank-level granularity: instead\nof stalling the entire channel, only affected banks are paused. This is\nachieved through a DRAM-resident register that identifies attacked banks.\n  PRACtical improves performance by 8% on average (up to 20%) over the\nstate-of-the-art, reduces energy by 19%, and limits performance degradation\nfrom aggressive performance attacks to less than 6%, all while preserving\nRowhammer protection.", "AI": {"tldr": "PRACtical\u4f18\u5316\u4e86DDR5\u4e2d\u7684PRAC+ABO\u673a\u5236\uff0c\u901a\u8fc7\u51cf\u5c11\u8ba1\u6570\u5668\u66f4\u65b0\u5ef6\u8fdf\u548c\u5b9e\u73b0\u94f6\u884c\u7ea7\u7c92\u5ea6\u7f13\u89e3\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u80fd\u8017\u3002", "motivation": "\u968f\u7740DRAM\u5bc6\u5ea6\u589e\u52a0\uff0cRowhammer\u95ee\u9898\u52a0\u5267\uff0c\u73b0\u6709DDR5\u6807\u51c6\u4e2d\u7684PRAC\u673a\u5236\u867d\u80fd\u7f13\u89e3\u4f46\u5e26\u6765\u6027\u80fd\u5f00\u9500\u3002", "method": "\u63d0\u51faPRACtical\u65b9\u6cd5\uff1a1\uff09\u5f15\u5165\u96c6\u4e2d\u5f0f\u589e\u91cf\u7535\u8def\u51cf\u5c11\u8ba1\u6570\u5668\u66f4\u65b0\u5ef6\u8fdf\uff1b2\uff09\u901a\u8fc7DRAM\u5bc4\u5b58\u5668\u5b9e\u73b0\u94f6\u884c\u7ea7\u7c92\u5ea6\u7f13\u89e3\u3002", "result": "\u6027\u80fd\u5e73\u5747\u63d0\u53478%\uff08\u6700\u9ad820%\uff09\uff0c\u80fd\u8017\u964d\u4f4e19%\uff0c\u653b\u51fb\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u9650\u5236\u57286%\u4ee5\u5185\u3002", "conclusion": "PRACtical\u5728\u4fdd\u6301Rowhammer\u9632\u62a4\u7684\u540c\u65f6\uff0c\u663e\u8457\u4f18\u5316\u4e86\u6027\u80fd\u548c\u80fd\u8017\u3002"}}
{"id": "2507.17793", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.17793", "abs": "https://arxiv.org/abs/2507.17793", "authors": ["Joel Brogan", "Matthew Yohe", "David Cornett"], "title": "CHAMP: A Configurable, Hot-Swappable Edge Architecture for Adaptive Biometric Tasks", "comment": null, "summary": "What if you could piece together your own custom biometrics and AI analysis\nsystem, a bit like LEGO blocks? We aim to bring that technology to field\noperators in the field who require flexible, high-performance edge AI system\nthat can be adapted on a moment's notice. This paper introduces CHAMP\n(Configurable Hot-swappable Architecture for Machine Perception), a modular\nedge computing platform that allows operators to dynamically swap in\nspecialized AI \"capability cartridges\" for tasks like face recognition, object\ntracking, and document analysis. CHAMP leverages low-power FPGA-based\naccelerators on a high-throughput bus, orchestrated by a custom operating\nsystem (VDiSK) to enable plug-and-play AI pipelines and cryptographically\nsecured biometric datasets. In this paper we describe the CHAMP design,\nincluding its modular scaling with multiple accelerators and the VDiSK\noperating system for runtime reconfiguration, along with its cryptographic\ncapabilities to keep data stored on modules safe and private. Experiments\ndemonstrate near-linear throughput scaling from 1 to 5 neural compute\naccelerators, highlighting both the performance gains and saturation limits of\nthe USB3-based bus. Finally, we discuss applications of CHAMP in field\nbiometrics, surveillance, and disaster response, and outline future\nimprovements in bus protocols, cartridge capabilities, and system software.", "AI": {"tldr": "CHAMP\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u8fb9\u7f18\u8ba1\u7b97\u5e73\u53f0\uff0c\u652f\u6301\u52a8\u6001\u66f4\u6362AI\u529f\u80fd\u6a21\u5757\uff0c\u9002\u7528\u4e8e\u7075\u6d3b\u7684\u9ad8\u6027\u80fd\u8fb9\u7f18AI\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u73b0\u573a\u64cd\u4f5c\u5458\u63d0\u4f9b\u7075\u6d3b\u3001\u9ad8\u6027\u80fd\u7684\u8fb9\u7f18AI\u7cfb\u7edf\uff0c\u652f\u6301\u5373\u65f6\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u9700\u6c42\u3002", "method": "\u91c7\u7528\u57fa\u4e8eFPGA\u7684\u4f4e\u529f\u8017\u52a0\u901f\u5668\u548c\u9ad8\u541e\u5410\u603b\u7ebf\uff0c\u7ed3\u5408\u5b9a\u5236\u64cd\u4f5c\u7cfb\u7edfVDiSK\uff0c\u5b9e\u73b0\u5373\u63d2\u5373\u7528\u7684AI\u6d41\u6c34\u7ebf\u548c\u52a0\u5bc6\u751f\u7269\u8bc6\u522b\u6570\u636e\u5b58\u50a8\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c1\u81f35\u4e2a\u795e\u7ecf\u8ba1\u7b97\u52a0\u901f\u5668\u7684\u541e\u5410\u91cf\u63a5\u8fd1\u7ebf\u6027\u6269\u5c55\uff0c\u540c\u65f6\u63ed\u793a\u4e86USB3\u603b\u7ebf\u7684\u6027\u80fd\u589e\u76ca\u548c\u9971\u548c\u9650\u5236\u3002", "conclusion": "CHAMP\u5728\u751f\u7269\u8bc6\u522b\u3001\u76d1\u63a7\u548c\u707e\u96be\u54cd\u5e94\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\uff0c\u672a\u6765\u53ef\u6539\u8fdb\u603b\u7ebf\u534f\u8bae\u3001\u6a21\u5757\u529f\u80fd\u548c\u7cfb\u7edf\u8f6f\u4ef6\u3002"}}
{"id": "2507.17843", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.17843", "abs": "https://arxiv.org/abs/2507.17843", "authors": ["Bruno Marques da Silva", "Larissa Ferreira Rodrigues Moreira", "Fl\u00e1vio de Oliveira Silva", "Rodrigo Moreira"], "title": "Optimizing Edge Gaming Slices through an Enhanced User Plane Function and Analytics in Beyond-5G Networks", "comment": null, "summary": "The latest generation of games and pervasive communication technologies poses\nchallenges in service management and Service-Level Agreement compliance for\nmobile users. State-of-the-art edge-gaming techniques enhance throughput,\nreduce latency, and leverage cloud computing. However, further development of\ncore functions such as the User Plane Function (UPF) is needed for\nnon-intrusive user latency measurement. This paper proposes a closed-loop\narchitecture integrating the Network Data Analytics Function (NWDAF) and UPF to\nestimate user latency and enhance the 5G control plane by making it\nlatency-aware. The results show that embedding an artificial intelligence model\nwithin NWDAF enables game classification and opens new avenues for mobile edge\ngaming research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u95ed\u73af\u67b6\u6784\uff0c\u6574\u5408NWDAF\u548cUPF\u4ee5\u4f30\u8ba1\u7528\u6237\u5ef6\u8fdf\u5e76\u589e\u5f3a5G\u63a7\u5236\u5e73\u9762\uff0c\u901a\u8fc7AI\u6a21\u578b\u5b9e\u73b0\u6e38\u620f\u5206\u7c7b\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u7528\u6237\u5728\u670d\u52a1\u7ba1\u7406\u548cSLA\u5408\u89c4\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u8fb9\u7f18\u6e38\u620f\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u95ed\u73af\u67b6\u6784\uff0c\u7ed3\u5408NWDAF\u548cUPF\uff0c\u5d4c\u5165AI\u6a21\u578b\u8fdb\u884c\u6e38\u620f\u5206\u7c7b\u548c\u5ef6\u8fdf\u4f30\u8ba1\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cAI\u6a21\u578b\u80fd\u6709\u6548\u5206\u7c7b\u6e38\u620f\uff0c\u4e3a\u79fb\u52a8\u8fb9\u7f18\u6e38\u620f\u7814\u7a76\u5f00\u8f9f\u65b0\u9014\u5f84\u3002", "conclusion": "\u8be5\u67b6\u6784\u6210\u529f\u589e\u5f3a\u4e865G\u63a7\u5236\u5e73\u9762\u7684\u5ef6\u8fdf\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.17904", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.17904", "abs": "https://arxiv.org/abs/2507.17904", "authors": ["Talha Mehboob", "Luanzheng Guo", "Nathan Tallent", "Michael Zink", "David Irwin"], "title": "PowerTrip: Exploiting Federated Heterogeneous Datacenter Power for Distributed ML Training", "comment": null, "summary": "The exponential growth of large-scale AI models has led to computational and\npower demands that can exceed the capacity of a single data center. This is due\nto the limited power supplied by regional grids that leads to limited regional\ncomputational power. Consequently, distributing training workloads across\ngeographically distributed sites has become essential. However, this approach\nintroduces a significant challenge in the form of communication overhead,\ncreating a fundamental trade-off between the performance gains from accessing\ngreater aggregate power and the performance losses from increased network\nlatency. Although prior work has focused on reducing communication volume or\nusing heuristics for distribution, these methods assume constant homogeneous\npower supplies and ignore the challenge of heterogeneous power availability\nbetween sites.\n  To address the challenge of training large models in power-constrained,\ngeo-distributed environments, we introduce PowerTrip, a system that dynamically\nselects a subset of sites during runtime to optimize the power-communication\ntrade-off. Specifically, PowerTrip selects sites based on a power-to-cost\nheuristic, prioritizing those with high power availability and low network\nlatency. PowerTrip employs a dynamic greedy approach and uses the marginal gain\nin training efficiency, i.e., accuracy improvement per unit of time, to\noptimize for the number of sites where the performance penalty from network\noverhead negates the benefit of adding more computational power. Our\nevaluation, which uses real-world Google power traces to model realistic power\ncapacity constraints, demonstrates that PowerTrip can reduce time-to-accuracy\nby up to 50% compared to existing baseline policies.", "AI": {"tldr": "PowerTrip\u7cfb\u7edf\u52a8\u6001\u9009\u62e9\u5730\u7406\u5206\u5e03\u5f0f\u7ad9\u70b9\u4ee5\u4f18\u5316\u5927\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u7535\u529b-\u901a\u4fe1\u6743\u8861\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u7535\u529b\u9700\u6c42\u53ef\u80fd\u8d85\u8fc7\u5355\u4e2a\u6570\u636e\u4e2d\u5fc3\u7684\u5bb9\u91cf\uff0c\u800c\u5730\u7406\u5206\u5e03\u5f0f\u8bad\u7ec3\u867d\u80fd\u89e3\u51b3\u7535\u529b\u9650\u5236\uff0c\u5374\u5e26\u6765\u901a\u4fe1\u5f00\u9500\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u7535\u529b\u4f9b\u5e94\u7684\u5f02\u6784\u6027\u3002", "method": "PowerTrip\u57fa\u4e8e\u7535\u529b-\u6210\u672c\u542f\u53d1\u5f0f\u52a8\u6001\u9009\u62e9\u7ad9\u70b9\uff0c\u4f18\u5148\u9ad8\u7535\u529b\u53ef\u7528\u6027\u548c\u4f4e\u7f51\u7edc\u5ef6\u8fdf\uff0c\u91c7\u7528\u52a8\u6001\u8d2a\u5a6a\u65b9\u6cd5\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u4f7f\u7528\u771f\u5b9eGoogle\u7535\u529b\u6570\u636e\u8bc4\u4f30\uff0cPowerTrip\u6bd4\u57fa\u7ebf\u7b56\u7565\u51cf\u5c1150%\u7684\u65f6\u95f4\u8fbe\u5230\u76ee\u6807\u7cbe\u5ea6\u3002", "conclusion": "PowerTrip\u6709\u6548\u89e3\u51b3\u4e86\u7535\u529b\u53d7\u9650\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u7535\u529b-\u901a\u4fe1\u6743\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2507.18005", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.18005", "abs": "https://arxiv.org/abs/2507.18005", "authors": ["Shengye Song", "Minxian Xu", "Zuowei Zhang", "Chengxi Gao", "Fansong Zeng", "Yu Ding", "Kejiang Ye", "Chengzhong Xu"], "title": "C-Koordinator: Interference-aware Management for Large-scale and Co-located Microservice Clusters", "comment": "15 pages", "summary": "Microservices transform traditional monolithic applications into lightweight,\nloosely coupled application components and have been widely adopted in many\nenterprises. Cloud platform infrastructure providers enhance the resource\nutilization efficiency of microservices systems by co-locating different\nmicroservices. However, this approach also introduces resource competition and\ninterference among microservices. Designing interference-aware strategies for\nlarge-scale, co-located microservice clusters is crucial for enhancing resource\nutilization and mitigating competition-induced interference. These challenges\nare further exacerbated by unreliable metrics, application diversity, and node\nheterogeneity.\n  In this paper, we first analyze the characteristics of large-scale and\nco-located microservices clusters at Alibaba and further discuss why cycle per\ninstruction (CPI) is adopted as a metric for interference measurement in\nlarge-scale production clusters, as well as how to achieve accurate prediction\nof CPI through multi-dimensional metrics. Based on CPI interference prediction\nand analysis, we also present the design of the C-Koordinator platform, an\nopen-source solution utilized in Alibaba cluster, which incorporates\nco-location and interference mitigation strategies. The interference prediction\nmodels consistently achieve over 90.3% accuracy, enabling precise prediction\nand rapid mitigation of interference in operational environments. As a result,\napplication latency is reduced and stabilized across all percentiles (P50, P90,\nP99) response time (RT), achieving improvements ranging from 16.7% to 36.1%\nunder various system loads compared with state-of-the-art system. These results\ndemonstrate the system's ability to maintain smooth application performance in\nco-located environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCPI\uff08\u6bcf\u6307\u4ee4\u5468\u671f\uff09\u7684\u5e72\u6270\u9884\u6d4b\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86C-Koordinator\u5e73\u53f0\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u89c4\u6a21\u5171\u7f6e\u5fae\u670d\u52a1\u96c6\u7fa4\u7684\u8d44\u6e90\u5229\u7528\u548c\u5e72\u6270\u7f13\u89e3\u3002", "motivation": "\u5171\u7f6e\u5fae\u670d\u52a1\u96c6\u7fa4\u867d\u7136\u63d0\u9ad8\u4e86\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u8d44\u6e90\u7ade\u4e89\u548c\u5e72\u6270\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u548c\u5f02\u6784\u73af\u5883\u4e0b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5927\u89c4\u6a21\u5171\u7f6e\u5fae\u670d\u52a1\u96c6\u7fa4\u7684\u7279\u6027\uff0c\u91c7\u7528CPI\u4f5c\u4e3a\u5e72\u6270\u5ea6\u91cf\u6307\u6807\uff0c\u5e76\u5229\u7528\u591a\u7ef4\u6307\u6807\u5b9e\u73b0CPI\u7684\u51c6\u786e\u9884\u6d4b\u3002\u57fa\u4e8e\u6b64\uff0c\u8bbe\u8ba1\u4e86C-Koordinator\u5e73\u53f0\u3002", "result": "\u5e72\u6270\u9884\u6d4b\u6a21\u578b\u51c6\u786e\u7387\u8d85\u8fc790.3%\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5e94\u7528\u5ef6\u8fdf\uff0c\u54cd\u5e94\u65f6\u95f4\uff08P50\u3001P90\u3001P99\uff09\u5728\u4e0d\u540c\u7cfb\u7edf\u8d1f\u8f7d\u4e0b\u63d0\u5347\u4e8616.7%\u81f336.1%\u3002", "conclusion": "C-Koordinator\u5e73\u53f0\u80fd\u6709\u6548\u7f13\u89e3\u5171\u7f6e\u73af\u5883\u4e2d\u7684\u5e72\u6270\uff0c\u63d0\u5347\u5e94\u7528\u6027\u80fd\u3002"}}
{"id": "2507.18006", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.18006", "abs": "https://arxiv.org/abs/2507.18006", "authors": ["Jingfeng Wu", "Yiyuan He", "Minxian Xu", "Xitong Gao", "Kejiang Ye", "Chengzhong Xu"], "title": "Unlock the Potential of Fine-grained LLM Serving via Dynamic Module Scaling", "comment": "15 pages", "summary": "The rise of large language models (LLMs) has created new opportunities across\nvarious fields but has also introduced significant challenges in resource\nmanagement. Current LLM serving systems face a fundamental tension: balancing\nserving demands with limited resources while adapting to unpredictable traffic\npatterns. Static deployments lead to suboptimal resource utilization and\nperformance degradation under dynamic workloads. Furthermore, the high cost of\nadjusting instances hinders dynamic scaling, limiting the true potential of\nefficient LLM serving.\n  To address this, we propose CoCoServe, an elastic system that facilitates\ndynamic and fine-grained scaling. Its key innovation lies in the module-level\noperations for the replication and migration of LLM modules, such as decoder\nlayers and projections. Through a comprehensive analysis of the trade-offs\nassociated with these operations, we develop an auto-scaling mechanism that\ndynamically regulates module-level resource allocation and performance\noptimization, enabling a more cost-effective deployment of LLMs. Our evaluation\ndemonstrates that the scaling operations employed by CoCoServe exhibit\nexcellent scalability and can reduce costs by 46% while maintaining\navailability. Compared to state-of-the-art LLM serving systems (e.g., Hugging\nFace Transformers and vLLM), our approach reduces latency by 14%-75% and\nachieves 1.16x-4x throughput on average across different model sizes and\nworkloads.", "AI": {"tldr": "CoCoServe\u662f\u4e00\u4e2a\u5f39\u6027\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u5757\u7ea7\u64cd\u4f5c\u5b9e\u73b0\u52a8\u6001\u548c\u7ec6\u7c92\u5ea6\u7684\u6269\u5c55\uff0c\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8d44\u6e90\u7ba1\u7406\u548c\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u670d\u52a1\u7cfb\u7edf\u5728\u8d44\u6e90\u7ba1\u7406\u548c\u52a8\u6001\u8d1f\u8f7d\u9002\u5e94\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u7387\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u6a21\u5757\u7ea7\u590d\u5236\u548c\u8fc1\u79fb\u64cd\u4f5c\uff0c\u5f00\u53d1\u81ea\u52a8\u6269\u5c55\u673a\u5236\uff0c\u52a8\u6001\u8c03\u8282\u8d44\u6e90\u5206\u914d\u548c\u6027\u80fd\u4f18\u5316\u3002", "result": "CoCoServe\u53ef\u964d\u4f4e\u6210\u672c46%\uff0c\u5ef6\u8fdf\u51cf\u5c1114%-75%\uff0c\u541e\u5410\u91cf\u63d0\u53471.16x-4x\u3002", "conclusion": "CoCoServe\u663e\u8457\u63d0\u5347\u4e86LLM\u670d\u52a1\u7684\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\uff0c\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002"}}
{"id": "2507.18007", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.18007", "abs": "https://arxiv.org/abs/2507.18007", "authors": ["Minxian Xu", "Junhan Liao", "Jingfeng Wu", "Yiyuan He", "Kejiang Ye", "Chengzhong Xu"], "title": "Cloud Native System for LLM Inference Serving", "comment": "10 pages", "summary": "Large Language Models (LLMs) are revolutionizing numerous industries, but\ntheir substantial computational demands create challenges for efficient\ndeployment, particularly in cloud environments. Traditional approaches to\ninference serving often struggle with resource inefficiencies, leading to high\noperational costs, latency issues, and limited scalability. This article\nexplores how Cloud Native technologies, such as containerization,\nmicroservices, and dynamic scheduling, can fundamentally improve LLM inference\nserving. By leveraging these technologies, we demonstrate how a Cloud Native\nsystem enables more efficient resource allocation, reduces latency, and\nenhances throughput in high-demand scenarios. Through real-world evaluations\nusing Kubernetes-based autoscaling, we show that Cloud Native architectures can\ndynamically adapt to workload fluctuations, mitigating performance bottlenecks\nwhile optimizing LLM inference serving performance. This discussion provides a\nbroader perspective on how Cloud Native frameworks could reshape the future of\nscalable LLM inference serving, offering key insights for researchers,\npractitioners, and industry leaders in cloud computing and artificial\nintelligence.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u4e91\u539f\u751f\u6280\u672f\uff08\u5982\u5bb9\u5668\u5316\u3001\u5fae\u670d\u52a1\u548c\u52a8\u6001\u8c03\u5ea6\uff09\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u670d\u52a1\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u8d44\u6e90\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u5728\u4e91\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u8d44\u6e90\u6548\u7387\u4f4e\u3001\u6210\u672c\u9ad8\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e91\u539f\u751f\u6280\u672f\uff08\u5982Kubernetes\u81ea\u52a8\u6269\u5c55\uff09\u52a8\u6001\u8c03\u6574\u8d44\u6e90\u5206\u914d\uff0c\u4f18\u5316LLM\u63a8\u7406\u670d\u52a1\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e91\u539f\u751f\u67b6\u6784\u80fd\u6709\u6548\u51cf\u5c11\u5ef6\u8fdf\u3001\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u5e76\u52a8\u6001\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\u3002", "conclusion": "\u4e91\u539f\u751f\u6846\u67b6\u4e3aLLM\u63a8\u7406\u670d\u52a1\u7684\u672a\u6765\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u4e91\u8ba1\u7b97\u548cAI\u9886\u57df\u7684\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u5177\u6709\u91cd\u8981\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2507.18047", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.18047", "abs": "https://arxiv.org/abs/2507.18047", "authors": ["Lucas Liebe", "Thanh-Tung Nguyen", "Dongman Lee"], "title": "FCPO: Federated Continual Policy Optimization for Real-Time High-Throughput Edge Video Analytics", "comment": "13 pages, 14 figures, 2 tables", "summary": "The growing complexity of Edge Video Analytics (EVA) facilitates new kind of\nintelligent applications, but creates challenges in real-time inference serving\nsystems. State-of-the-art (SOTA) scheduling systems optimize global workload\ndistributions for heterogeneous devices but often suffer from extended\nscheduling cycles, leading to sub-optimal processing in rapidly changing Edge\nenvironments. Local Reinforcement Learning (RL) enables quick adjustments\nbetween cycles but faces scalability, knowledge integration, and adaptability\nissues. Thus, we propose FCPO, which combines Continual RL (CRL) with Federated\nRL (FRL) to address these challenges. This integration dynamically adjusts\ninference batch sizes, input resolutions, and multi-threading during pre- and\npost-processing. CRL allows agents to learn from changing Markov Decision\nProcesses, capturing dynamic environmental variations, while FRL improves\ngeneralization and convergence speed by integrating experiences across\ninference models. FCPO combines these via an agent-specific aggregation scheme\nand a diversity-aware experience buffer. Experiments on a real-world EVA\ntestbed showed over 5 times improvement in effective throughput, 60% reduced\nlatency, and 20% faster convergence with up to 10 times less memory consumption\ncompared to SOTA RL-based approaches.", "AI": {"tldr": "FCPO\u7ed3\u5408\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\u548c\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\uff08FRL\uff09\uff0c\u4f18\u5316\u8fb9\u7f18\u89c6\u9891\u5206\u6790\uff08EVA\uff09\u7684\u5b9e\u65f6\u63a8\u7406\u670d\u52a1\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u3001\u964d\u4f4e\u5ef6\u8fdf\u5e76\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u8fb9\u7f18\u89c6\u9891\u5206\u6790\uff08EVA\uff09\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u73b0\u6709\u8c03\u5ea6\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c40\u90e8\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5b58\u5728\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "FCPO\u901a\u8fc7CRL\u548cFRL\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6279\u6b21\u5927\u5c0f\u3001\u8f93\u5165\u5206\u8fa8\u7387\u548c\u591a\u7ebf\u7a0b\u5904\u7406\uff0c\u7ed3\u5408\u7279\u5b9a\u4ee3\u7406\u805a\u5408\u65b9\u6848\u548c\u591a\u6837\u6027\u611f\u77e5\u7ecf\u9a8c\u7f13\u51b2\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cFCPO\u5728\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709RL\u65b9\u6cd5\uff0c\u5185\u5b58\u6d88\u8017\u51cf\u5c1110\u500d\u3002", "conclusion": "FCPO\u4e3a\u52a8\u6001\u8fb9\u7f18\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.18050", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.18050", "abs": "https://arxiv.org/abs/2507.18050", "authors": ["Xiaoning Jia", "Ruilin Kong", "Guangya Si", "Bilong Shen", "Zhe Ji"], "title": "A large-scale distributed parallel discrete event simulation engines based on Warped2 for Wargaming simulation", "comment": null, "summary": "Rising demand for complex simulations highlights conventional\nengines'scalability limits, spurring Parallel Discrete Event Simulation (PDES)\nadoption.Warped2, a PDES engine leveraging Time Warp synchronization with\nPending Event Set optimization, delivers strong performance, it struggles with\ninherent wargaming limitations: inefficient LP resource allocation during\nsynchronization and unaddressed complex entity interaction patterns. To address\nthese challenges, we present an optimized framework featuring four synergistic\nimprovements: (1) Asynchronous listener threads are introduced to address event\nmonitoring latency in large-scale scenarios, instead of synchronous polling\nmechanisms, (2) METIS-based load rebalancing strategy is incorporated to\naddress the issue of dynamic event allocation during real-world simulation, (3)\nEntity interaction solver with constraint satisfaction mechanisms is designed\nto mitigate state conflicts, and (4) Spatial hashing algorithm to overcome\nO(n^2) complexity bottlenecks in large-scale nearest-neighbor searches.\nExperimental validation through a GridWorld demo demonstrates significant\nenhancements in temporal fidelity and computational efficiency. Benchmark\nresults show our framework achieves 16x acceleration over baseline\nimplementations and maintains 8x speedup over 1-thread configuration across MPI\nand Pthreads implementations.The combined load balancing and LP migration\nstrategy reduces synchronization overhead by 58.18%, with load balancing\naccounting for 57% of the total improvement as the dominant optimization\nfactor. These improvements provide an enhanced solution for PDES implementation\nin large-scale simulation scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u5e76\u884c\u79bb\u6563\u4e8b\u4ef6\u4eff\u771f\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfPDES\u5f15\u64ce\u5728\u8d44\u6e90\u5206\u914d\u548c\u590d\u6742\u5b9e\u4f53\u4ea4\u4e92\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u56db\u9879\u6539\u8fdb\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5e76\u884c\u79bb\u6563\u4e8b\u4ef6\u4eff\u771f\uff08PDES\uff09\u5f15\u64ce\u5728\u8d44\u6e90\u5206\u914d\u548c\u590d\u6742\u5b9e\u4f53\u4ea4\u4e92\u4e2d\u5b58\u5728\u6548\u7387\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u4eff\u771f\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u56db\u9879\u6539\u8fdb\uff1a\u5f02\u6b65\u76d1\u542c\u7ebf\u7a0b\u3001METIS\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\u3001\u5b9e\u4f53\u4ea4\u4e92\u6c42\u89e3\u5668\u548c\u7a7a\u95f4\u54c8\u5e0c\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u6846\u67b6\u5728\u6027\u80fd\u4e0a\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u5feb16\u500d\uff0c\u540c\u6b65\u5f00\u9500\u51cf\u5c1158.18%\uff0c\u8d1f\u8f7d\u5747\u8861\u8d21\u732e\u4e8657%\u7684\u6539\u8fdb\u3002", "conclusion": "\u4f18\u5316\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u4eff\u771f\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684PDES\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.18459", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.18459", "abs": "https://arxiv.org/abs/2507.18459", "authors": ["Amir Najjar", "Riad Mokadem", "Jean-Marc Pierson"], "title": "Towards Designing an Energy Aware Data Replication Strategy for Cloud Systems Using Reinforcement Learning", "comment": null, "summary": "The rapid growth of global data volumes has created a demand for scalable\ndistributed systems that can maintain a high quality of service. Data\nreplication is a widely used technique that provides fault tolerance, improved\nperformance and higher availability. Traditional implementations often rely on\nthreshold-based activation mechanisms, which can vary depending on workload\nchanges and system architecture. System administrators typically bear the\nresponsibility of adjusting these thresholds. To address this challenge,\nreinforcement learning can be used to dynamically adapt to workload changes and\ndifferent architectures. In this paper, we propose a novel data replication\nstrategy for cloud systems that employs reinforcement learning to automatically\nlearn system characteristics and adapt to workload changes. The strategy's aim\nis to provide satisfactory Quality of Service while optimizing a trade-off\nbetween provider profit and environmental impact. We present the architecture\nbehind our solution and describe the reinforcement learning model by defining\nthe states, actions and rewards.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u636e\u590d\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u4e91\u7cfb\u7edf\u52a8\u6001\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\uff0c\u4f18\u5316\u670d\u52a1\u8d28\u91cf\u4e0e\u8d44\u6e90\u5229\u7528\u3002", "motivation": "\u5168\u7403\u6570\u636e\u91cf\u5feb\u901f\u589e\u957f\uff0c\u4f20\u7edf\u9608\u503c\u673a\u5236\u4f9d\u8d56\u4eba\u5de5\u8c03\u6574\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff0c\u5b9a\u4e49\u72b6\u6001\u3001\u52a8\u4f5c\u548c\u5956\u52b1\uff0c\u81ea\u52a8\u5b66\u4e60\u7cfb\u7edf\u7279\u6027\u5e76\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u3002", "result": "\u7b56\u7565\u5728\u670d\u52a1\u8d28\u91cf\u4e0e\u8d44\u6e90\u5229\u7528\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u517c\u987e\u63d0\u4f9b\u5546\u5229\u6da6\u4e0e\u73af\u5883\u5f71\u54cd\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u4e3a\u6570\u636e\u590d\u5236\u63d0\u4f9b\u4e86\u52a8\u6001\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u9608\u503c\u673a\u5236\u3002"}}
