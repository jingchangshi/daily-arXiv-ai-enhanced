<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [One Weird Trick to Untie Landin's Knot](https://arxiv.org/abs/2507.21317)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: 论文探讨了Landin's Knot在编码递归中的作用，指出高阶引用本身不导致非终止性，关键在于对函数环境的无限制量化。通过闭包转换语言展示了这一机制，并提出通过限制环境量化可以安全添加高阶引用。


<details>
  <summary>Details</summary>
Motivation: 研究Landin's Knot在编码递归中的作用，澄清高阶引用与非终止性的关系，探索如何在终止语言中安全添加高阶引用。

Method: 使用闭包转换语言，明确函数环境并通过非预测性量化隐藏环境类型，展示如何利用高阶引用编码递归。

Result: 发现高阶引用本身不导致非终止性，关键在于对函数环境的无限制量化。

Conclusion: 通过限制环境量化，可以安全地在终止语言中添加高阶引用，无需复杂类型系统或限制函数存储。

Abstract: In this work, we explore Landin's Knot, which is understood as a pattern for
encoding general recursion, including non-termination, that is possible after
adding higher-order references to an otherwise terminating language. We observe
that this isn't always true -- higher-order references, by themselves, don't
lead to non-termination. The key insight is that Landin's Knot relies not
primarily on references storing functions, but on unrestricted quantification
over a function's environment. We show this through a closure converted
language, in which the function's environment is made explicit and hides the
type of the environment through impredicative quantification. Once references
are added, this impredicative quantification can be exploited to encode
recursion. We conjecture that by restricting the quantification over the
environment, higher-order references can be safely added to terminating
languages, without resorting to more complex type systems such as linearity,
and without restricting references from storing functions.

</details>


### [2] [Fixed-Point-Oriented Programming: A Concise and Elegant Paradigm](https://arxiv.org/abs/2507.21439)
*Yong Qi Foo,Brian Sze-Kai Cheong,Michael D. Adams*

Main category: cs.PL

TL;DR: FPOP是一种新兴编程范式，旨在简化自引用计算问题的实现，如图算法、静态分析和分布式计算。它通过高级抽象和结构化推理规则，使开发者能够编写简洁的声明式规范，同时编译器确保高效执行。


<details>
  <summary>Details</summary>
Motivation: 传统编程范式缺乏对固定点计算的直接支持，导致实现复杂且易出错。FPOP通过提供高层次的抽象，简化了这些问题。

Method: FPOP利用结构化推理规则和用户导向的优化，支持声明式规范编写，并由编译器处理高效执行。

Result: FPOP显著简化了算法实现，提高了可维护性，并支持快速原型设计。例如，图距离问题仅需两行代码即可实现。

Conclusion: FPOP填补了理论固定点公式与实际实现之间的鸿沟，有望推动该范式的进一步研究和应用。

Abstract: Fixed-Point-Oriented Programming (FPOP) is an emerging paradigm designed to
streamline the implementation of problems involving self-referential
computations. These include graph algorithms, static analysis, parsing, and
distributed computing-domains that traditionally require complex and
tricky-to-implement work-queue algorithms. Existing programming paradigms lack
direct support for these inherently fixed-point computations, leading to
inefficient and error-prone implementations.
  This white paper explores the potential of the FPOP paradigm, which offers a
high-level abstraction that enables concise and expressive problem
formulations. By leveraging structured inference rules and user-directed
optimizations, FPOP allows developers to write declarative specifications while
the compiler ensures efficient execution. It not only reduces implementation
complexity for programmers but also enhances adaptability, making it easier for
programmers to explore alternative solutions and optimizations without
modifying the core logic of their program.
  We demonstrate how FPOP simplifies algorithm implementation, improves
maintainability, and enables rapid prototyping by allowing problems to be
clearly and concisely expressed. For example, the graph distance problem can be
expressed in only two executable lines of code with FPOP, while it takes an
order of magnitude more code in other paradigms. By bridging the gap between
theoretical fixed-point formulations and practical implementations, we aim to
foster further research and adoption of this paradigm.

</details>


### [3] [Composable Effect Handling for Programming LLM-integrated Scripts](https://arxiv.org/abs/2507.22048)
*Di Wang*

Main category: cs.PL

TL;DR: 论文提出使用可组合效应处理来分离工作流逻辑与效应操作（如LLM调用、I/O和并发），以提升模块化和性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM集成脚本中模块化与性能的挑战，避免脚本与特定LLM实现耦合并利用并行化机会。

Method: 通过将效应操作视为抽象接口，并通过效应处理器实现，分离逻辑与效应操作。

Result: 在Tree-of-Thoughts案例中实现了10倍的加速，同时保持模块化。

Conclusion: 提倡可组合效应处理作为LLM脚本编程风格，以兼顾模块化和性能优化。

Abstract: Implementing LLM-integrated scripts introduces challenges in modularity and
performance, as scripts are often coupled to specific LLM implementations and
fail to exploit parallelization opportunities. This paper proposes using
composable effect handling to separate workflow logic from effectful
operations, such as LLM calls, I/O, and concurrency, enabling modularity
without sacrificing the opportunity for performance optimization. By treating
these operations as abstract interfaces and discharging them via effect
handlers, this paper shows that scripts can achieve significant speedups (e.g.,
10$\times$ in a Tree-of-Thoughts case study) without compromising modularity.
This paper aims to promote composable effect handling as a programming style
for LLM scripting.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Improving SpGEMM Performance Through Matrix Reordering and Cluster-wise Computation](https://arxiv.org/abs/2507.21253)
*Abdullah Al Raqibul Islam,Helen Xu,Dong Dai,Aydın Buluç*

Main category: cs.DC

TL;DR: 本文提出了一种基于层次聚类的稀疏矩阵乘法（SpGEMM）优化方法，通过行重排序和聚类计算提高性能，平均加速1.39倍，且预处理成本低。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵乘法（SpGEMM）因不规则内存访问模式成为性能瓶颈，现有研究多关注稀疏矩阵向量乘法（SpMV），本文旨在解决SpGEMM的性能问题。

Method: 采用层次聚类方法，结合行重排序和聚类计算，提出新的行聚类矩阵格式和访问模式，以提高输入矩阵B的复用性。

Result: 实验表明，该方法平均加速1.39倍，预处理成本低（约90%的输入上低于单次SpGEMM成本的20倍）。

Conclusion: 层次聚类方法在SpGEMM中表现优于其他重排序方案，且预处理时间相近，为稀疏矩阵计算提供了高效优化手段。

Abstract: Sparse matrix-sparse matrix multiplication (SpGEMM) is a key kernel in many
scientific applications and graph workloads. Unfortunately, SpGEMM is
bottlenecked by data movement due to its irregular memory access patterns.
Significant work has been devoted to developing row reordering schemes towards
improving locality in sparse operations, but prior studies mostly focus on the
case of sparse-matrix vector multiplication (SpMV).
  In this paper, we address these issues with hierarchical clustering for
SpGEMM that leverages both row reordering and cluster-wise computation to
improve reuse in the second input (B) matrix with a novel row-clustered matrix
format and access pattern in the first input (A) matrix. We find that
hierarchical clustering can speed up SpGEMM by 1.39x on average with low
preprocessing cost (less than 20x the cost of a single SpGEMM on about 90% of
inputs). Furthermore, we decouple the reordering algorithm from the clustered
matrix format so they can be applied as independent optimizations.
  Additionally, this paper sheds light on the role of both row reordering and
clustering independently and together for SpGEMM with a comprehensive empirical
study of the effect of 10 different reordering algorithms and 3 clustering
schemes on SpGEMM performance on a suite of 110 matrices. We find that
reordering based on graph partitioning provides better SpGEMM performance than
existing alternatives at the cost of high preprocessing time. The evaluation
demonstrates that the proposed hierarchical clustering method achieves greater
average speedup compared to other reordering schemes with similar preprocessing
times.

</details>


### [5] [Using Containers to Speed Up Development, to Run Integration Tests and to Teach About Distributed Systems](https://arxiv.org/abs/2507.21464)
*Marco Mambelli,Bruno Moreira Coimbra,Namratha Urs,Ilya Baburashvili*

Main category: cs.DC

TL;DR: GlideinWMS通过工作空间容器简化开发和测试，支持快速配置和离线开发，提升团队协作和学生培训效率。


<details>
  <summary>Details</summary>
Motivation: 为实验（如CMS和DUNE）提供资源管理，同时简化开发、测试和培训流程。

Method: 构建基于容器的工作空间，包括核心组件（计算节点、调度器等）和可选组件，支持多种容器运行时。

Result: 工作空间易于在笔记本电脑上运行，集成IDE（如VS Code），提升开发和调试效率，并成功用于学生培训。

Conclusion: GlideinWMS工作空间显著简化了开发、测试和培训，适用于多种场景，包括离线开发和教学。

Abstract: GlideinWMS is a workload manager provisioning resources for many experiments,
including CMS and DUNE. The software is distributed both as native packages and
specialized production containers. Following an approach used in other
communities like web development, we built our workspaces, system-like
containers to ease development and testing. Developers can change the source
tree or check out a different branch and quickly reconfigure the services to
see the effect of their changes. In this paper, we will talk about what
differentiates workspaces from other containers. We will describe our base
system, composed of three containers: a one-node cluster including a compute
element and a batch system, a GlideinWMS Factory controlling pilot jobs, and a
scheduler and Frontend to submit jobs and provision resources. Additional
containers can be used for optional components. This system can easily run on a
laptop, and we will share our evaluation of different container runtimes, with
an eye for ease of use and performance. Finally, we will talk about our
experience as developers and with students. The GlideinWMS workspaces are
easily integrated with IDEs like VS Code, simplifying debugging and allowing
development and testing of the system even when offline. They simplified the
training and onboarding of new team members and summer interns. And they were
useful in workshops where students could have first-hand experience with the
mechanisms and components that, in production, run millions of jobs.

</details>


### [6] [GlideinBenchmark: collecting resource information to optimize provisioning](https://arxiv.org/abs/2507.21472)
*Marco Mambelli,Shrijan Swaminathan*

Main category: cs.DC

TL;DR: GlideinBenchmark是一个新的Web应用，利用GlideinWMS的基础设施进行资源基准测试，帮助自动化选择最优资源。


<details>
  <summary>Details</summary>
Motivation: 选择合适的资源可以加速任务完成、提高硬件利用率并降低成本，但基准测试过程繁琐耗时。

Method: 利用GlideinWMS的pilot基础设施开发GlideinBenchmark，控制基准测试执行，并将数据用于资源优化选择。

Result: GlideinBenchmark能够高效执行基准测试，并将结果用于调度器（如HEPCloud的Decision Engine）优化资源分配。

Conclusion: GlideinBenchmark简化了资源基准测试过程，为自动化资源选择提供了有效工具。

Abstract: Choosing the right resource can speed up job completion, better utilize the
available hardware, and visibly reduce costs, especially when renting computers
in the cloud. This was demonstrated in earlier studies on HEPCloud. However,
the benchmarking of the resources proved to be a laborious and time-consuming
process. This paper presents GlideinBenchmark, a new Web application leveraging
the pilot infrastructure of GlideinWMS to benchmark resources, and it shows how
to use the data collected and published by GlideinBenchmark to automate the
optimal selection of resources. An experiment can select the benchmark or the
set of benchmarks that most closely evaluate the performance of its workflows.
GlideinBenchmark, with the help of the GlideinWMS Factory, controls the
benchmark execution. Finally, a scheduler like HEPCloud's Decision Engine can
use the results to optimize resource provisioning.

</details>


### [7] [Bridging Cache-Friendliness and Concurrency: A Locality-Optimized In-Memory B-Skiplist](https://arxiv.org/abs/2507.21492)
*Yicong Luo,Senhe Hao,Brian Wheatman,Prashant Pandey,Helen Xu*

Main category: cs.DC

TL;DR: 论文提出了一种并发B-skiplist，通过提升缓存局部性显著提高了性能，同时保持了传统skiplist的简单性和并发控制机制。


<details>
  <summary>Details</summary>
Motivation: 传统skiplist因每个节点仅存储单一元素导致缓存局部性差，限制了性能。提升缓存局部性是优化内存索引性能的关键。

Method: 提出了一种自上而下、单遍插入算法和对应的并发控制方案，用于实现并发B-skiplist。

Result: 在128线程下，B-skiplist的吞吐量比现有并发skiplist高2x-9x，与缓存优化的树结构索引相比吞吐量相当（0.9x-1.7x），且99%延迟显著降低。

Conclusion: B-skiplist在保持简单性的同时，显著提升了性能和缓存局部性，适用于高性能内存索引场景。

Abstract: Skiplists are widely used for in-memory indexing in many key-value stores,
such as RocksDB and LevelDB, due to their ease of implementation and simple
concurrency control mechanisms. However, traditional skiplists suffer from poor
cache locality, as they store only a single element per node, leaving
performance on the table. Minimizing last-level cache misses is key to
maximizing in-memory index performance, making high cache locality essential.
In this paper, we present a practical concurrent B-skiplist that enhances cache
locality and performance while preserving the simplicity of traditional
skiplist structures and concurrency control schemes. Our key contributions
include a top-down, single-pass insertion algorithm for B-skiplists and a
corresponding simple and efficient top-down concurrency control scheme. On 128
threads, the proposed concurrent B-skiplist achieves between 2x-9x higher
throughput compared to state-of-the-art concurrent skiplist implementations,
including Facebook's concurrent skiplist from Folly and the Java
ConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves
competitive (0.9x-1.7x) throughput on point workloads compared to
state-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a
more complete picture of the performance, we also measure the latency of
skiplist and tree-based indices and find that the B-skiplist achieves between
3.5x-103x lower 99% latency compared to other concurrent skiplists and between
0.85x-64x lower 99% latency compared to tree-based indices on point workloads
with inserts.

</details>


### [8] [Collaborative State Machines: A Better Programming Model for the Cloud-Edge-IoT Continuum](https://arxiv.org/abs/2507.21685)
*Marlon Etheredge,Thomas Fahringer,Felix Erlacher,Elias Kohler,Stefan Pedratscher,Juan Aznar-Poveda,Nishant Saurabh,Adrien Lebre*

Main category: cs.DC

TL;DR: 论文提出了一种名为协作状态机（CSM）的编程模型，用于解决Cloud-Edge-IoT应用中动态和状态管理的复杂性，并通过实验验证了其性能优势。


<details>
  <summary>Details</summary>
Motivation: 现有编程模型难以有效管理Cloud-Edge-IoT应用的动态和状态特性，因此需要一种新的模型来支持反应式、事件驱动和状态化的应用开发。

Method: CSM模型通过协作状态机、事件驱动机制、数据封装和动作集成，实现了应用逻辑与计算服务的解耦，并支持本地和持久化数据处理。

Result: 实验表明，CSM在吞吐量、处理时间和生产力方面显著优于现有系统，如吞吐量提升12倍，图像处理时间减少2.3倍，智能工厂总处理时间减少55倍。

Conclusion: CSM是一种高效的编程模型，适用于Cloud-Edge-IoT连续体，能够显著提升应用性能和开发效率。

Abstract: The development of Cloud-Edge-IoT applications requires robust programming
models. Existing models often struggle to manage the dynamic and stateful
nature of these applications effectively. This paper introduces the
Collaborative State Machines (CSM) programming model to address these
complexities. CSM facilitates the development of reactive, event-driven, and
stateful applications targeting the Cloud-Edge-IoT continuum. Applications
built with CSM are composed of state machines that collaborate autonomously and
can be distributed across different layers of the continuum. Key features of
CSM include (i) a sophisticated collaboration mechanism among state machines
utilizing events and persistent data; (ii) encapsulation of state through the
inherent state of state machines and persistent data; (iii) integration of
actions and service invocations within states and state transitions, thereby
decoupling complex application logic from compute and data processing services;
and (iv) an advanced data model that supports the processing of local, static,
and persistent data with defined scope and lifetime. In addition to introducing
the CSM programming model, we present a runtime system and a comprehensive
evaluation of our approach. This evaluation is based on three use cases: a
stress test on a large-scale infrastructure, a surveillance system application,
and a complex smart factory scenario, all deployed on the Grid'5000 testbed.
Our results demonstrate a 12x increase in throughput through novel language
features in the stress test. Compared to Serverless Workflow, a
state-of-the-art baseline system, we show a 2.3x improvement in processing time
per processed image in a surveillance system use case, a 55x reduction in total
processing time for a smart factory use case, and an overall improvement in
productivity across these use cases.

</details>


### [9] [The Performance of Low-Synchronization Variants of Reorthogonalized Block Classical Gram--Schmidt](https://arxiv.org/abs/2507.21791)
*Erin Carson,Yuxin Ma*

Main category: cs.DC

TL;DR: 论文评估了两种低同步成本的BCGS变体（BCGSI+P-1S和BCGSI+P-2S）在分布式内存系统中的性能，发现它们在稳定性和速度上优于其他变体。


<details>
  <summary>Details</summary>
Motivation: 在分布式内存系统中，全局同步成本是性能瓶颈，因此需要低同步且稳定的正交化算法。

Method: 通过数值实验比较BCGSI+P-1S、BCGSI+P-2S与其他低同步BCGS变体的性能。

Result: BCGSI+P-1S和BCGSI+P-2S分别实现了4倍和2倍的速度提升，且稳定性优于同类变体。

Conclusion: BCGSI+P-1S和BCGSI+P-2S因其低同步和高稳定性，是分布式内存系统中经济QR分解的最佳选择。

Abstract: Numerous applications, such as Krylov subspace solvers, make extensive use of
the block classical Gram-Schmidt (BCGS) algorithm and its reorthogonalized
variants for orthogonalizing a set of vectors. For large-scale problems in
distributed memory settings, the communication cost, particularly the global
synchronization cost, is a major performance bottleneck. In recent years, many
low-synchronization BCGS variants have been proposed in an effort to reduce the
number of synchronization points. The work [E. Carson, Y. Ma, arXiv preprint
2411.07077] recently proposed stable one-synchronization and
two-synchronization variants of BCGS, i.e., BCGSI+P-1S and BCGSI+P-2S. In this
work, we evaluate the performance of BCGSI+P-1S and BCGSI+P-2S on a distributed
memory system compared to other well-known low-synchronization BCGS variants.
In comparison to the classical reorthogonalized BCGS algorithm (BCGSI+),
numerical experiments demonstrate that BCGSI+P-1S and BCGSI+P-2S can achieve up
to 4 times and 2 times speedups, respectively, and perform similarly to other
(less stable) one-synchronization and two-synchronization variants. BCGSI+P-1S
and BCGSI+P-2S are therefore recommended as the best choice in practice for
computing an economic QR factorization on distributed memory systems due to
their superior stability when compared to other variants with the same
synchronization cost.

</details>
