<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [On Propositional Program Equivalence (extended abstract)](https://arxiv.org/abs/2507.07480)
*Tobias Kappé*

Main category: cs.PL

TL;DR: 论文讨论了在命题等价性框架下程序等价性的可行性，基于(G)KAT理论的最新进展。


<details>
  <summary>Details</summary>
Motivation: 通用程序等价性不可判定，但通过抽象语句语义，命题等价性不仅可判定且实际可行。

Method: 利用Guarded Kleene Algebra with Tests (G)KAT理论分析命题程序等价性。

Result: 展示了命题等价性在程序分析中的实际应用和可行性。

Conclusion: (G)KAT为命题程序等价性提供了有效的理论框架，具有实际应用潜力。

Abstract: General program equivalence is undecidable. However, if we abstract away the
semantics of statements, then this problem becomes not just decidable, but
practically feasible. For instance, a program of the form "if $b$ then $e$ else
$f$" should be equivalent to "if not $b$ then $f$ else $e$" - no matter what
$b$, $e$ and $f$ are. This kind of equivalence is known as propositional
equivalence. In this extended abstract, we discuss recent developments in
propositional program equivalence from the perspective of (Guarded) Kleene
Algebra with Tests, or (G)KAT.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Distributed Training under Packet Loss](https://arxiv.org/abs/2507.07114)
*Erez Weintraub,Ron Banner,Ariel Orda*

Main category: cs.DC

TL;DR: 提出了一种新型分布式训练框架，可在不可靠连接下保持模型精度和收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有分布式框架依赖可靠连接，导致延迟和可扩展性问题，而不可靠连接可能牺牲模型精度。

Method: 采用两阶段防御机制：无偏梯度聚合和有界漂移参数广播。

Result: 在LLAMA2 7B模型上实验，容忍10%丢包率时困惑度变化不超过0.8%。

Conclusion: 该框架填补了高效通信协议与现代大模型训练需求之间的差距，支持在普通或广域网络上进行鲁棒学习。

Abstract: State-of-the-art language and vision models are routinely trained across
thousands of GPUs, often spanning multiple data-centers, yet today's
distributed frameworks still assume reliable connections (e.g., InfiniBand or
RoCE). The resulting acknowledgment traffic and retransmissions inflate tail
latencies and limit scalability. Leveraging unreliable connections will reduce
latency but may sacrifice model accuracy and convergence once packets are
dropped. A principled, end-to-end solution that preserves accuracy and
convergence guarantees under genuine packet loss has previously been missing.
We address this critical gap by introducing a novel distributed training
framework capable of operating over unreliable connections, offering unbiased
gradient aggregation and bounded parameter drift without modifying model code
or optimizers. The key insight is a two-stage defense against missing messages:
(i) Unbiased gradient aggregation: each worker reconstructs a consistent
gradient estimate from whatever packets arrive, guaranteeing expectation-level
correctness; and (ii) Bounded-drift parameter broadcasts: we prove the
inter-worker model discrepancy remains O(1) even after arbitrarily many
iterations, preventing the unbounded divergence typical of asynchronous setups.
Analytical bounds are matched by experiments on the LLAMA2 7B model with 64
GPUs: tolerating 10% random packet loss yields at most 0.8% perplexity change.
This work bridges the gap between communication-efficient datacenter protocols
and the accuracy and generalization guarantees demanded by modern large-model
training, enabling robust, high-throughput learning on commodity or wide-area
networks.

</details>


### [3] [Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces](https://arxiv.org/abs/2507.07116)
*Juan Cano-Benito,Andrea Cimmino,Sven Hertling,Heiko Paulheim,Raúl García-Castro*

Main category: cs.DC

TL;DR: 本文系统评估了不同类型的分布式账本技术（DLT）在语义数据存储中的表现，发现私有DLT效率最高，混合DLT在公开审计与操作效率间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 数据空间需要去中心化基础设施以实现语义互操作性，但现有DLT在语义数据存储方面存在效率不足的问题。

Method: 使用真实世界知识图谱对不同类型DLT（公有、私有、混合）进行性能、存储效率、资源消耗及语义数据更新与查询能力的比较。

Result: 私有DLT在语义内容存储和管理上效率最高，混合DLT在公开审计与操作效率间提供平衡。

Conclusion: 研究为去中心化数据生态系统选择合适DLT基础设施提供了依据，需根据数据主权需求权衡选择。

Abstract: Data spaces are emerging as decentralised infrastructures that enable
sovereign, secure, and trustworthy data exchange among multiple participants.
To achieve semantic interoperability within these environments, the use of
semantic web technologies and knowledge graphs has been proposed. Although
distributed ledger technologies (DLT) fit as the underlying infrastructure for
data spaces, there remains a significant gap in terms of the efficient storage
of semantic data on these platforms. This paper presents a systematic
evaluation of semantic data storage across different types of DLT (public,
private, and hybrid), using a real-world knowledge graph as an experimental
basis. The study compares performance, storage efficiency, resource
consumption, and the capabilities to update and query semantic data. The
results show that private DLTs are the most efficient for storing and managing
semantic content, while hybrid DLTs offer a balanced trade-off between public
auditability and operational efficiency. This research leads to a discussion on
the selection of the most appropriate DLT infrastructure based on the data
sovereignty requirements of decentralised data ecosystems.

</details>


### [4] [Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure](https://arxiv.org/abs/2507.07223)
*Myoungsoo Jung*

Main category: cs.DC

TL;DR: 论文提出了一种基于CXL的模块化数据中心架构，以解决现代AI工作负载（如大语言模型和检索增强生成）对内存、通信带宽和资源灵活性的高需求。通过优化互连技术和分层内存模型，提高了AI基础设施的可扩展性和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统GPU架构因GPU间通信开销增加而难以扩展，无法满足现代AI工作负载的需求。

Method: 提出基于CXL的模块化数据中心架构，结合XLink互连技术和分层内存模型，减少长距离数据传输并保持内存一致性。

Result: 评估表明，该设计显著提升了AI基础设施的可扩展性、吞吐量和灵活性。

Conclusion: 通过模块化设计和优化互连技术，能够有效支持现代AI工作负载的高需求。

Abstract: Modern AI workloads such as large language models (LLMs) and
retrieval-augmented generation (RAG) impose severe demands on memory,
communication bandwidth, and resource flexibility. Traditional GPU-centric
architectures struggle to scale due to growing inter-GPU communication
overheads. This report introduces key AI concepts and explains how Transformers
revolutionized data representation in LLMs. We analyze large-scale AI hardware
and data center designs, identifying scalability bottlenecks in hierarchical
systems. To address these, we propose a modular data center architecture based
on Compute Express Link (CXL) that enables disaggregated scaling of memory,
compute, and accelerators. We further explore accelerator-optimized
interconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink
Fusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance
data transfers while preserving memory coherence. We also propose a
hierarchical memory model that combines local and pooled memory, and evaluate
lightweight CXL implementations, HBM, and silicon photonics for efficient
scaling. Our evaluations demonstrate improved scalability, throughput, and
flexibility in AI infrastructure.

</details>


### [5] [Collective Communication Profiling of Modern-day Machine Learning Workloads](https://arxiv.org/abs/2507.07117)
*Jit Gupta,Andrew Li,Tarun Banka,Ariel Cohen,T. Sridhar,Raj Yavatkar*

Main category: cs.DC

TL;DR: 分析机器学习分布式系统中集体通信行为对网络性能的影响，并提出优化框架和拓扑结构的必要性。


<details>
  <summary>Details</summary>
Motivation: 机器学习任务中的集体通信操作可能导致网络拥塞和性能下降，需分析其行为以优化资源分配。

Method: 通过Nvidia Collective Communication Library记录和分析多种模型的通信行为，调整并行度、节点数和模型类型等参数。

Result: 展示了DeepSeek V3推理模型的通信行为数据，包括操作类型、传输大小和请求分布。

Conclusion: 需重新设计集体通信框架和网络拓扑以应对网络异常对机器学习任务的影响。

Abstract: Machine Learning jobs, carried out on large number of distributed high
performance systems, involve periodic communication using operations like
AllReduce, AllGather, and Broadcast. These operations may create high bandwidth
and bursty traffic patterns, leading to network congestion and packet loss,
thus impacting the performance of these jobs. Hence it is imperative to analyze
these patterns, which can be helpful in provisioning network resources
depending on the type of machine learning workloads. In this poster we carry
out extensive analysis of the collective communication behavior seen in a wide
variety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we
instrument Nvidia Collective Communication Library logging functionality for
richer context about the collectives and workloads. We adjust configuration
parameters that influence collective communication behavior, such as
parallelism, number of nodes, and model type. This overview presents and
discusses some of the results on the collective communication behavior for the
open source DeepSeek V3 inferencing model, which includes operation type and
count, transfer sizes per operation, and request size distribution. Our
analysis shows that it makes sense to rethink current collective communication
frameworks and network topologies so as to accommodate the effect of network
anomalies on the mentioned workloads.

</details>


### [6] [Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding](https://arxiv.org/abs/2507.07120)
*Nidhi Bhatia,Ankit More,Ritika Borkar,Tiyasa Mitra,Ramon Matas,Ritchie Zhao,Maximilian Golub,Dheevatsa Mudigere,Brian Pharris,Bita Darvish Rouhani*

Main category: cs.DC

TL;DR: Helix Parallelism是一种混合执行策略，通过KV并行和TP/EP并行优化LLM的推理效率，减少延迟并支持更大批次。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的KV历史增长，实时解码面临FFN权重访问和长KV缓存读取的瓶颈，现有方法如TP在注意力机制中效率不足。

Method: 引入Helix Parallelism，结合KV并行和TP/EP并行，通过轻量通信步骤保留精确注意力行为，并利用Helix HOP-B最小化通信开销。

Result: Helix在固定批次下减少TTL达1.5倍，支持32倍更大批次，提升DeepSeek-R1的吞吐-延迟Pareto效率。

Conclusion: Helix使超长序列实时推理成为可能，显著提升效率。

Abstract: As LLMs scale to multi-million-token KV histories, real-time autoregressive
decoding under tight Token-to-Token Latency (TTL) constraints faces growing
pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)
weights and reading long KV caches. While Tensor Parallelism (TP) helps
mitigate the cost of FFN weight reads, it does not scale well for attention.
When TP width exceeds the number of KV heads, it leads to inefficient KV
duplication, limits parallelism, and constrains batch size. Simultaneously,
DRAM reads for long KV histories scale linearly with batch size, further
capping efficiency.
  We introduce Helix Parallelism, a hybrid execution strategy that applies KV
parallelism during attention to shard KV caches across GPUs, then reuses the
same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN
computation. To preserve exact attention behavior, Helix includes a lightweight
communication step. To minimize the exposed communication cost, we introduce
Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through
batchwise overlap, preserving low TTL while improving GPU efficiency. Compared
to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at
fixed batch sizes and supports up to 32x larger batches under the same latency
budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on
Blackwell and making real-time inference with ultra-long-sequence practical.

</details>


### [7] [Ampere: Communication-Efficient and High-Accuracy Split Federated Learning](https://arxiv.org/abs/2507.07130)
*Zihan Zhang,Leon Wong,Blesson Varghese*

Main category: cs.DC

TL;DR: Ampere是一种新型的联邦学习系统，通过减少设备计算和通信开销，同时提高模型准确性，解决了Split Federated Learning (SFL)的问题。


<details>
  <summary>Details</summary>
Motivation: SFL虽然减少了设备计算负担，但引入了大量通信开销并降低了非独立同分布（non-IID）数据的模型准确性。Ampere旨在解决这些问题。

Method: Ampere采用单向块间训练和轻量级辅助网络生成方法，减少梯度传输和中间交换，同时通过整合设备块生成的激活来训练服务器块。

Result: 实验表明，Ampere在模型准确性、训练时间、通信开销和设备计算方面显著优于SFL基线系统。

Conclusion: Ampere在联邦学习中实现了高效、低开销和高准确性的训练，特别适用于非IID数据场景。

Abstract: A Federated Learning (FL) system collaboratively trains neural networks
across devices and a server but is limited by significant on-device computation
costs. Split Federated Learning (SFL) systems mitigate this by offloading a
block of layers of the network from the device to a server. However, in doing
so, it introduces large communication overheads due to frequent exchanges of
intermediate activations and gradients between devices and the server and
reduces model accuracy for non-IID data. We propose Ampere, a novel
collaborative training system that simultaneously minimizes on-device
computation and device-server communication while improving model accuracy.
Unlike SFL, which uses a global loss by iterative end-to-end training, Ampere
develops unidirectional inter-block training to sequentially train the device
and server block with a local loss, eliminating the transfer of gradients. A
lightweight auxiliary network generation method decouples training between the
device and server, reducing frequent intermediate exchanges to a single
transfer, which significantly reduces the communication overhead. Ampere
mitigates the impact of data heterogeneity by consolidating activations
generated by the trained device block to train the server block, in contrast to
SFL, which trains on device-specific, non-IID activations. Extensive
experiments on multiple CNNs and transformers show that, compared to
state-of-the-art SFL baseline systems, Ampere (i) improves model accuracy by up
to 13.26% while reducing training time by up to 94.6%, (ii) reduces
device-server communication overhead by up to 99.1% and on-device computation
by up to 93.13%, and (iii) reduces standard deviation of accuracy by 53.39% for
various non-IID degrees highlighting superior performance when faced with
heterogeneous data.

</details>


### [8] [M$^2$-MFP: A Multi-Scale and Multi-Level Memory Failure Prediction Framework for Reliable Cloud Infrastructure](https://arxiv.org/abs/2507.07144)
*Hongyi Xie,Min Zhou,Qiao Yu,Jialiang Yu,Zhenli Sheng,Hong Xie,Defu Lian*

Main category: cs.DC

TL;DR: M$^2$-MFP是一种多尺度分层内存故障预测框架，通过二进制空间特征提取器和双路径时间建模架构，显著提升了内存故障预测的性能。


<details>
  <summary>Details</summary>
Motivation: 内存故障对云基础设施的稳定性构成威胁，现有预测方法存在泛化能力差和性能不足的问题。

Method: 将可纠正错误转换为多级二进制矩阵表示，引入二进制空间特征提取器（BSFE），并开发双路径时间建模架构。

Result: 在基准数据集和实际部署中，M$^2$-MFP显著优于现有方法。

Conclusion: M$^2$-MFP有效提升了内存故障预测的准确性和可靠性，适用于云基础设施。

Abstract: As cloud services become increasingly integral to modern IT infrastructure,
ensuring hardware reliability is essential to sustain high-quality service.
Memory failures pose a significant threat to overall system stability, making
accurate failure prediction through the analysis of memory error logs (i.e.,
Correctable Errors) imperative. Existing memory failure prediction approaches
have notable limitations: rule-based expert models suffer from limited
generalizability and low recall rates, while automated feature extraction
methods exhibit suboptimal performance. To address these limitations, we
propose M$^2$-MFP: a Multi-scale and hierarchical memory failure prediction
framework designed to enhance the reliability and availability of cloud
infrastructure. M$^2$-MFP converts Correctable Errors (CEs) into multi-level
binary matrix representations and introduces a Binary Spatial Feature Extractor
(BSFE) to automatically extract high-order features at both DIMM-level and
bit-level. Building upon the BSFE outputs, we develop a dual-path temporal
modeling architecture: 1) a time-patch module that aggregates multi-level
features within observation windows, and 2) a time-point module that employs
interpretable rule-generation trees trained on bit-level patterns. Experiments
on both benchmark datasets and real-world deployment show the superiority of
M$^2$-MFP as it outperforms existing state-of-the-art methods by significant
margins. Code and data are available at this repository:
https://github.com/hwcloud-RAS/M2-MFP.

</details>


### [9] [Machine Learning-driven Multiscale MD Workflows: The Mini-MuMMI Experience](https://arxiv.org/abs/2507.07352)
*Loïc Pottier,Konstantia Georgouli,Timothy S. Carpenter,Fikret Aydin,Jeremy O. B. Tempkin,Dwight V. Nissley,Frederick H. Streitz,Thomas R. W. Scogland,Peer-Timo Bremer,Felice C. Lightstone,Helgi I. Ingólfsson*

Main category: cs.DC

TL;DR: 论文介绍了多尺度机器学习建模基础设施MuMMI及其轻量版mini-MuMMI，用于管理大规模并行多尺度分子动力学模拟，并展示了其在RAS-RAF膜相互作用中的应用。


<details>
  <summary>Details</summary>
Motivation: 多尺度模型在复杂现象建模中应用广泛，但跨尺度建模和并行计算管理具有挑战性。机器学习方法为解决这些问题提供了新思路。

Method: 提出了MuMMI及其轻量版mini-MuMMI，用于管理和协调大规模多尺度分子动力学模拟，支持从毫秒到纳秒的时间尺度。

Result: 展示了mini-MuMMI在RAS-RAF膜相互作用中的应用，验证了其在小规模计算系统上的实用性。

Conclusion: mini-MuMMI扩展了多尺度工作流的适用范围，为更广泛的应用提供了可能。

Abstract: Computational models have become one of the prevalent methods to model
complex phenomena. To accurately model complex interactions, such as detailed
biomolecular interactions, scientists often rely on multiscale models comprised
of several internal models operating at difference scales, ranging from
microscopic to macroscopic length and time scales. Bridging the gap between
different time and length scales has historically been challenging but the
advent of newer machine learning (ML) approaches has shown promise for tackling
that task. Multiscale models require massive amounts of computational power and
a powerful workflow management system. Orchestrating ML-driven multiscale
studies on parallel systems with thousands of nodes is challenging, the
workflow must schedule, allocate and control thousands of simulations operating
at different scales. Here, we discuss the massively parallel Multiscale
Machine-Learned Modeling Infrastructure (MuMMI), a multiscale workflow
management infrastructure, that can orchestrate thousands of molecular dynamics
(MD) simulations operating at different timescales, spanning from millisecond
to nanosecond. More specifically, we introduce a novel version of MuMMI called
"mini-MuMMI". Mini-MuMMI is a curated version of MuMMI designed to run on
modest HPC systems or even laptops whereas MuMMI requires larger HPC systems.
We demonstrate mini-MuMMI utility by exploring RAS-RAF membrane interactions
and discuss the different challenges behind the generalization of multiscale
workflows and how mini-MuMMI can be leveraged to target a broader range of
applications outside of MD and RAS-RAF interactions.

</details>


### [10] [KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows](https://arxiv.org/abs/2507.07400)
*Zaifeng Pan,Ajjkumar Patel,Zhengding Hu,Yipeng Shen,Yue Guan,Wan-Lu Li,Lianhui Qin,Yida Wang,Yufei Ding*

Main category: cs.DC

TL;DR: KVFlow是一种针对基于LLM的代理工作流程的KV缓存管理框架，通过预测代理的未来使用情况优化缓存策略，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有系统使用LRU策略管理KV缓存，无法预测代理的未来使用，导致频繁缓存未命中和计算开销。

Method: KVFlow通过Agent Step Graph抽象代理执行计划，并引入步骤到执行值指导细粒度缓存策略，同时支持重叠KV预取。

Result: KVFlow在单工作流程和多并发工作流程中分别实现了1.83倍和2.19倍的加速。

Conclusion: KVFlow通过智能缓存管理和预取机制，显著提升了代理工作流程的效率。

Abstract: Large language model (LLM) based agentic workflows have become a popular
paradigm for coordinating multiple specialized agents to solve complex tasks.
To improve serving efficiency, existing LLM systems employ prefix caching to
reuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby
avoiding redundant computation across repeated invocations. However, current
systems typically evict KV caches using a Least Recently Used (LRU) policy,
which fails to anticipate future agent usage and often discards KV caches
shortly before their reuse. This leads to frequent cache misses and substantial
recomputation or swapping overhead. We present KVFlow, a workflow-aware KV
cache management framework tailored for agentic workloads. KVFlow abstracts the
agent execution schedule as an Agent Step Graph and assigns each agent a
steps-to-execution value that estimates its temporal proximity to future
activation. These values guide a fine-grained eviction policy at the KV node
level, allowing KVFlow to preserve entries likely to be reused and efficiently
manage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a
fully overlapped KV prefetching mechanism, which proactively loads required
tensors from CPU to GPU in background threads for agents scheduled in the next
step, thereby avoiding cache miss stalls during generation. Compared to SGLang
with hierarchical radix cache, KVFlow achieves up to 1.83$\times$ speedup for
single workflows with large prompts, and up to 2.19$\times$ speedup for
scenarios with many concurrent workflows.

</details>


### [11] [Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems](https://arxiv.org/abs/2507.07671)
*Jovan Prodanov,Blaž Bertalanič,Carolina Fortuna,Shih-Kai Chou,Matjaž Branko Jurič,Ramon Sanchez-Iborra,Jernej Hribar*

Main category: cs.DC

TL;DR: 论文提出了一种基于多智能体强化学习的动态资源扩展引擎（MARLISE），用于解决边缘云系统中资源扩展效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 传统资源扩展方法依赖静态阈值和预定义规则，难以适应动态和分布式环境的需求，导致资源利用率和性能不佳。

Method: 采用两种深度强化学习算法（DQN和PPO）开发了MARLISE，实现动态、反应式的资源扩展。

Result: 实验表明，MARLISE在动态负载下能显著降低微服务响应时间，并提高资源效率，优于启发式方法。

Conclusion: MARLISE为边缘云系统提供了一种高效的动态资源扩展解决方案，提升了性能和适应性。

Abstract: Modern edge-cloud systems face challenges in efficiently scaling resources to
handle dynamic and unpredictable workloads. Traditional scaling approaches
typically rely on static thresholds and predefined rules, which are often
inadequate for optimizing resource utilization and maintaining performance in
distributed and dynamic environments. This inefficiency hinders the
adaptability and performance required in edge-cloud infrastructures, which can
only be achieved through the newly proposed in-place scaling. To address this
problem, we propose the Multi-Agent Reinforcement Learning-based In-place
Scaling Engine (MARLISE) that enables seamless, dynamic, reactive control with
in-place resource scaling. We develop our solution using two Deep Reinforcement
Learning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization
(PPO). We analyze each version of the proposed MARLISE solution using dynamic
workloads, demonstrating their ability to ensure low response times of
microservices and scalability. Our results show that MARLISE-based approaches
outperform heuristic method in managing resource elasticity while maintaining
microservice response times and achieving higher resource efficiency.

</details>


### [12] [KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based Auto-Scaling](https://arxiv.org/abs/2507.07932)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Qiang Guan,Hailong Jiang*

Main category: cs.DC

TL;DR: KIS-S框架结合GPU感知的Kubernetes推理模拟器（KISim）和基于PPO的自动扩展器（KIScaler），显著提升动态流量下的GPU推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决Kubernetes中GPU推理工作负载的自动扩展问题，尤其是动态和突发流量下默认机制（如HPA）的不足。

Method: 提出KIS-S框架，结合KISim（模拟器）和KIScaler（基于PPO的自动扩展器），在模拟中学习延迟感知和资源高效的扩展策略。

Result: 实验显示KIScaler平均奖励提升75.2%，P95延迟降低6.7倍，且无需重新训练即可泛化。

Conclusion: KIS-S填补了反应式自动扩展与智能编排之间的差距，适用于GPU加速环境。

Abstract: Autoscaling GPU inference workloads in Kubernetes remains challenging due to
the reactive and threshold-based nature of default mechanisms such as the
Horizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty
traffic patterns and lack integration with GPU-level metrics. We present KIS-S,
a unified framework that combines KISim, a GPU-aware Kubernetes Inference
Simulator, with KIScaler, a Proximal Policy Optimization (PPO)-based
autoscaler. KIScaler learns latency-aware and resource-efficient scaling
policies entirely in simulation, and is directly deployed without retraining.
Experiments across four traffic patterns show that KIScaler improves average
reward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and
generalizes without retraining. Our work bridges the gap between reactive
autoscaling and intelligent orchestration for scalable GPU-accelerated
environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [Accelerating Transposed Convolutions on FPGA-based Edge Devices](https://arxiv.org/abs/2507.07683)
*Jude Haris,José Cano*

Main category: cs.AR

TL;DR: 论文提出MM2IM加速器，通过结合矩阵乘法与col2IM优化转置卷积（TCONV）在边缘设备上的性能，显著提升速度和能效。


<details>
  <summary>Details</summary>
Motivation: 现有TCONV实现（如IOM）存在输出映射复杂、计算冗余等问题，导致边缘设备性能瓶颈，亟需高效解决方案。

Method: 提出硬件-软件协同设计的MM2IM加速器，结合矩阵乘法与col2IM方法，利用SECDA-TFLite工具包实现并评估。

Result: 在261种TCONV配置中平均提速1.9倍，在知名生成模型中最高提速4.2倍，能效提升显著。

Conclusion: MM2IM显著优化TCONV在边缘设备的性能，速度和能效均优于现有方案。

Abstract: Transposed Convolutions (TCONV) enable the up-scaling mechanism within
generative Artificial Intelligence (AI) models. However, the predominant
Input-Oriented Mapping (IOM) method for implementing TCONV has complex output
mapping, overlapping sums, and ineffectual computations. These inefficiencies
further exacerbate the performance bottleneck of TCONV and generative models on
resource-constrained edge devices. To address this problem, in this paper we
propose MM2IM, a hardware-software co-designed accelerator that combines Matrix
Multiplication (MatMul) with col2IM to process TCONV layers on
resource-constrained edge devices efficiently. Using the SECDA-TFLite design
toolkit, we implement MM2IM and evaluate its performance across 261 TCONV
problem configurations, achieving an average speedup of 1.9x against a
dual-thread ARM Neon optimized CPU baseline. We then evaluate the performance
of MM2IM on a range of TCONV layers from well-known generative models achieving
up to 4.2x speedup, and compare it against similar resource-constrained TCONV
accelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate
MM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x
energy reduction against the CPU baseline.

</details>
