<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Typing Fallback Functions: A Semantic Approach to Type Safe Smart Contracts](https://arxiv.org/abs/2512.04755)
*Stian Lybech,Daniele Gorla,Luca Aceto*

Main category: cs.PL

TL;DR: 该论文提出了一种在智能合约环境中使用语义类型来确保类型安全的方法，特别是针对像fallback函数这样的静态不可类型化语言构造。通过让合约创建者提供类型安全的形式化证明证书，用户只需验证证书有效性，这类似于区块链环境中的证明携带代码。


<details>
  <summary>Details</summary>
Motivation: 智能合约中某些语言构造（如fallback函数）无法进行静态类型检查，这可能导致类型安全问题。区块链的不可变性使得一旦部署错误合约难以修复，因此需要一种机制来确保这类代码的类型安全性。

Method: 采用语义类型方法，为TINYSOL语言（Solidity的简化版本）提供类型操作语义。通过共归纳定义的类型解释来表达安全证明，并使用类似于双相似性的up-to技术来紧凑表示证明证书。

Result: 成功开发了适用于区块链/智能合约环境的语义类型框架，能够确保信息流控制和非干扰性安全属性。该方法还可用于类型化基于fallback函数的典型指针到实现模式。

Conclusion: 论文的主要贡献不是安全定理本身，而是展示了在区块链/智能合约环境中实现这种方法所需的理论发展，为处理静态不可类型化构造提供了可行的解决方案。

Abstract: This paper develops semantic typing in a smart-contract setting to ensure type safety of code that uses statically untypable language constructs, such as the fallback function. The idea is that the creator of a contract on the blockchain equips code containing such constructs with a formal proof of its type safety, given in terms of the semantics of types. Then, a user of the contract only needs to check the validity of the provided `proof certificate' of type safety. This is a form of proof-carrying code, which naturally fits with the immutable nature of the blockchain environment.
  As a concrete application of our approach, we focus on ensuring information flow control and non-interference for the language TINYSOL, a distilled version of the Solidity language, through security types. We provide the semantics of types in terms of a typed operational semantics of TINYSOL, and a way for expressing the proofs of safety as coinductively-defined typing interpretations and for representing them compactly via up-to techniques, similar to those used for bisimilarity. We also show how our machinery can be used to type the typical pointer-to-implementation pattern based on the fallback function. However, our main contribution is not the safety theorem per se (and so security properties different from non-interference can be considered as well), but rather the presentation of the theoretical developments necessary to make this approach work in a blockchain/smart-contract setting.

</details>


### [2] [Optimizations and extensions for fair join pattern matching](https://arxiv.org/abs/2512.04876)
*Ioannis Karras*

Main category: cs.PL

TL;DR: 该论文优化了Haller等人的状态树匹配算法，在特定基准测试中实现高达10倍的性能提升，接近Rete算法在常规基准上的表现，同时保持对复杂条件守卫的优势。


<details>
  <summary>Details</summary>
Motivation: 虽然Haller等人提出了公平连接模式匹配的语义规范，但其状态树匹配算法在时间效率上表现不佳，特别是在常规基准测试中不如Rete算法。现有Rete算法需要大量手动适配才能应用于连接模式匹配问题。

Method: 增强和优化Haller等人的状态树匹配算法，改进基准测试套件（增加新功能、增强可扩展性和用户友好性），扩展连接模式实现（提供更少歧义的语法和动态模式切换）。

Result: 在特定基准测试中实现高达10倍的性能提升，接近Rete算法在常规基准上的表现，同时保持对复杂条件守卫的性能优势。提供了新的复杂模型用例，展示了连接模式在微服务Web架构中的适用性。

Conclusion: 通过优化状态树匹配算法，显著提升了连接模式匹配的时间效率，使其在实际应用中更具竞争力。扩展的实现和基准测试套件为连接模式的研究和应用提供了更好的工具支持。

Abstract: Join patterns are an underexplored approach for the programming of concurrent and distributed systems. When applied to the actor model, join patterns offer the novel capability of matching combinations of messages in the mailbox of an actor. Previous work by Philipp Haller et al. in the paper "Fair Join Pattern Matching for Actors" (ECOOP 2024) explored join patterns with conditional guards in an actor-based setting with a specification of fair and deterministic matching semantics. Nevertheless, the question of time efficiency in fair join pattern matching has remained underexplored. The stateful tree-based matching algorithm of Haller et al. performs worse than an implementation that adapts the Rete algorithm to the regular version of a join pattern matching benchmark, while outperforming on a variant with heavy conditional guards, which take longer to evaluate. Nevertheless, conforming Rete to the problem of join pattern matching requires heavy manual adaptation.
  In this thesis, we enhance and optimize the stateful tree-based matching algorithm of Haller et al. to achieve up to tenfold performance improvements on certain benchmarks, approaching the performance of Rete on regular benchmarks while maintaining the advantages of versatility and performance with heavy guards. We also enhance the benchmark suite, adding new features and enhancing its extensibility and user-friendliness. We extend the join pattern implementation with a less ambiguous syntax as well as dynamic pattern switching. Finally, we present a new complex model use case for join patterns, showing their applicability in a microservice web architecture.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Toward Sustainability-Aware LLM Inference on Edge Clusters](https://arxiv.org/abs/2512.04088)
*Kolichala Rajashekar,Nafiseh Sharghivand,Radu Prodan,Reza Farahani*

Main category: cs.DC

TL;DR: 本文提出了一种面向边缘集群的可持续性感知LLM推理框架，通过碳感知和延迟感知的路由策略，在NVIDIA Jetson Orin NX和Ada 2000设备上平衡推理延迟与碳足迹。


<details>
  <summary>Details</summary>
Motivation: LLM需要大量计算资源，导致显著的碳排放和运营成本。虽然训练能耗高，但长期环境负担来自推理过程，且因全球查询量巨大而加剧。云端推理存在延迟和带宽限制，边缘集群可实现本地化执行，但面临性能、能效和设备约束之间的权衡。

Method: 提出可持续性感知的LLM推理框架，用于包含NVIDIA Jetson Orin NX (8GB)和Nvidia Ada 2000 (16GB)设备的边缘集群。通过碳感知和延迟感知的路由策略，基于对不同提示和批量配置的能耗和执行时间的经验基准测试，平衡推理延迟和碳足迹。

Result: 实验评估表明，批量大小为4个提示时能在吞吐量和能效之间取得平衡，而更大的批量可能导致GPU内存饱和。比较了基线贪婪策略与碳感知和延迟感知策略在提示路由中的表现。

Conclusion: 通过碳感知和延迟感知的路由策略，可以在边缘集群上实现可持续的LLM推理，平衡延迟和碳足迹，批量大小4是最佳配置，为边缘环境中的高效LLM部署提供了实用解决方案。

Abstract: Large language models (LLMs) require substantial computational resources, leading to significant carbon emissions and operational costs. Although training is energy-intensive, the long-term environmental burden arises from inference, amplified by the massive global query volume. Cloud-based inference offers scalability but suffers from latency and bandwidth constraints due to centralized processing and continuous data transfer. Edge clusters instead can mitigate these limitations by enabling localized execution, yet they face trade-offs between performance, energy efficiency, and device constraints. This short paper presents a sustainability-aware LLM inference for edge clusters comprising NVIDIA Jetson Orin NX (8GB) and Nvidia Ada 2000 (16GB) devices. It aims to balance inference latency and carbon footprint through carbon- and latency-aware routing strategies, guided by empirical benchmarking of energy consumption and execution time across diverse prompts and batch (i.e., group of prompts) configurations. We compared baseline greedy strategies to carbon-aware and latency-aware strategies in prompt routing to specific hardware based on benchmarking information. Experimental evaluation shows that a batch size of four prompts achieves a trade-off between throughput, energy efficiency, while larger batches risk GPU memory saturation.

</details>


### [4] [Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud](https://arxiv.org/abs/2512.04089)
*Mario Colosi,Reza Farahani,Lauri Loven,Radu Prodan,Massimo Villari*

Main category: cs.DC

TL;DR: 本文评估了WebAssembly在浏览器、边缘节点和云服务器上执行无服务器工作流的性能，发现AOT编译和实例预热能显著降低启动延迟，小负载时浏览器性能有竞争力，大负载时边缘和云节点的AOT执行性能更优。


<details>
  <summary>Details</summary>
Motivation: WebAssembly（Wasm）作为一种二进制指令格式，具有跨平台、沙箱化和接近原生执行的特性，适合在浏览器、边缘节点和云服务器上执行无服务器工作流。但其性能和稳定性受启动开销、运行时执行模型（AOT/JIT编译）以及不同部署环境资源差异等因素影响，需要系统评估。

Method: 使用wasm32-wasi模块构建Wasm无服务器工作流，在浏览器中通过web worker执行，在边缘和云环境中通过HTTP shim将帧流式传输到Wasm运行时。测量冷启动/热启动延迟、每步延迟、工作流总时间、吞吐量以及CPU/内存利用率，以捕获跨环境的端到端行为。

Result: 结果显示：1）AOT编译和实例预热能显著降低启动延迟；2）对于小负载工作流，浏览器由于完全内存数据交换而具有竞争力性能；3）随着负载增大，工作流进入计算和内存密集型阶段，边缘和云节点的AOT执行性能明显超越浏览器。

Conclusion: Wasm无服务器工作流在不同环境中的性能表现存在显著差异。AOT编译和预热策略对性能提升至关重要，而环境选择应根据工作负载特性：小负载适合浏览器执行，大负载则更适合边缘和云节点的AOT执行。

Abstract: WebAssembly (Wasm) is a binary instruction format that enables portable, sandboxed, and near-native execution across heterogeneous platforms, making it well-suited for serverless workflow execution on browsers, edge nodes, and cloud servers. However, its performance and stability depend heavily on factors such as startup overhead, runtime execution model (e.g., Ahead-of-Time (AOT) and Just-in-Time (JIT) compilation), and resource variability across deployment contexts. This paper evaluates a Wasm-based serverless workflow executed consistently from the browser to edge and cloud instances. The setup uses wasm32-wasi modules: in the browser, execution occurs within a web worker, while on Edge and Cloud, an HTTP shim streams frames to the Wasm runtime. We measure cold- and warm-start latency, per-step delays, workflow makespan, throughput, and CPU/memory utilization to capture the end-to-end behavior across environments. Results show that AOT compilation and instance warming substantially reduce startup latency. For workflows with small payloads, the browser achieves competitive performance owing to fully in-memory data exchanges. In contrast, as payloads grow, the workflow transitions into a compute- and memory-intensive phase where AOT execution on edge and cloud nodes distinctly surpasses browser performance.

</details>


### [5] [Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions](https://arxiv.org/abs/2512.04093)
*Ali Akbar Vali,Sadoon Azizi,Mohammad Shojafar,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 对2020-2024年间136+篇研究的综述，系统分类了微服务雾/边缘计算中的资源管理策略，重点关注节能方案，识别研究空白并提出AI、量子计算等未来方向。


<details>
  <summary>Details</summary>
Motivation: 物联网设备激增需要高效响应服务，雾/边缘计算虽能降低延迟和能耗，但面临资源受限、异构性、动态负载和多样化QoS等资源管理挑战，需要系统梳理现有解决方案。

Method: 系统综述2020-2024年间136+项研究，按五个关键子领域分类：服务放置、资源供应、任务调度与卸载、资源分配、实例选择，基于优化技术、目标及优缺点进行分析。

Result: 建立了微服务雾/边缘计算资源管理的统一分类框架，识别了现有研究中基本资源管理组件缺乏协同的问题，并发现文献中的未解决挑战和研究空白。

Conclusion: 为研究人员和从业者提供了资源管理的统一节能视角，指出未来研究方向包括AI驱动优化、量子计算和Serverless计算，为更集成、高效、可持续的解决方案铺平道路。

Abstract: The exponential growth of Internet of Things (IoT) devices has intensified the demand for efficient and responsive services. To address this demand, fog and edge computing have emerged as distributed paradigms that bring computational resources closer to end users, reducing latency, bandwidth limitations, and energy consumption. However, these paradigms present challenges in resource management due to resource constraints, computational heterogeneity, dynamic workloads, and diverse Quality of Service (QoS) requirements. This paper presents a comprehensive survey of state-of-the-art resource management strategies in microservices-based fog and edge computing, focusing on energy-efficient solutions. We systematically review and classify more than 136 studies (2020-2024) into five key subdomains: service placement, resource provisioning, task scheduling and offloading, resource allocation, and instance selection. Our categorization is based on optimization techniques, targeted objectives, and the strengths and limitations of each approach. In addition, we examine existing surveys and identify unresolved challenges and gaps in the literature. By highlighting the lack of synergy among fundamental resource management components, we outline promising research directions leveraging AI-driven optimization, quantum computing, and serverless computing. This survey serves as a comprehensive reference for researchers and practitioners by providing a unified and energy-aware perspective on resource management in microservices-based fog and edge computing, paving the way for more integrated, efficient, and sustainable future solutions.

</details>


### [6] [Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale](https://arxiv.org/abs/2512.04096)
*Sushant Kumar Gupta,Anil Raghunath Iyer,Chang Yu,Neel Bagora,Olivier Pomerleau,Vivek Kumar,Prunthaban Kanthakumar*

Main category: cs.DC

TL;DR: Fast ACS是一个基于文件的顺序消息传递系统，通过结合RPC（远程过程调用）和RMA（远程内存访问）两种通信原语，实现跨集群的低延迟消息传递，支持数千消费者，达到Tbps级别的流量。


<details>
  <summary>Details</summary>
Motivation: 实时系统需要低延迟消息传递，数据需要从生产者传递到可能分布在城市和大陆边界的集群中的消费者。随着计算规模的扩大，可能有数千个消费者，需要强大的消息系统来保证顺序性和至少一次传递，同时避免消费者过载。

Method: 设计Fast ACS（Ads Copy Service），一个基于文件的顺序消息传递系统，利用两种通信原语的组合：双面（集群间）通信使用远程过程调用（RPC），单面（集群内）通信使用远程内存访问（RMA）。

Result: 系统已成功部署到数十个生产集群，每个集群可扩展到数千消费者，峰值时达到Tbps级别的集群内消费者流量。根据消息量和消费者规模，能够在几秒甚至亚秒级别（p99）将消息传递到全球消费者，资源成本低。

Conclusion: Fast ACS通过创新的文件基础架构和混合通信原语设计，实现了大规模、低延迟、高吞吐的消息传递系统，满足实时系统的严格要求。

Abstract: Low-latency message delivery is crucial for real-time systems. Data originating from a producer must be delivered to consumers, potentially distributed in clusters across metropolitan and continental boundaries. With the growing scale of computing, there can be several thousand consumers of the data. Such systems require a robust messaging system capable of transmitting messages containing data across clusters and efficiently delivering them to consumers. The system must offer guarantees like ordering and at-least-once delivery while avoiding overload on consumers, allowing them to consume messages at their own pace.
  This paper presents the design of Fast ACS (an abbreviation for Ads Copy Service), a file-based ordered message delivery system that leverages a combination of two-sided (inter-cluster) and one-sided (intra-cluster) communication primitives - namely, Remote Procedure Call and Remote Memory Access, respectively - to deliver messages. The system has been successfully deployed to dozens of production clusters and scales to accommodate several thousand consumers within each cluster, which amounts to Tbps-scale intra-cluster consumer traffic at peak. Notably, Fast ACS delivers messages to consumers across the globe within a few seconds or even sub-seconds (p99) based on the message volume and consumer scale, at a low resource cost.

</details>


### [7] [tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection](https://arxiv.org/abs/2512.04226)
*Ryan Swann,Muhammad Osama,Xiaohu Guo,Bryant Nelson,Lixun Zhang,Alex Brown,Yen Ong,Ali Yazdani,Sean Siddens,Ganesh Dasika,Alex Underwood*

Main category: cs.DC

TL;DR: tritonBLAS是一个基于架构参数（如缓存层次结构）的确定性分析模型，用于生成高性能GPU GEMM内核，无需运行时自动调优即可达到接近最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统GPU GEMM内核通常依赖耗时的运行时自动调优来获得高性能，这在实际生产环境中不实用。需要一种能够基于架构参数直接生成高性能内核的确定性方法。

Method: 开发了tritonBLAS分析模型，明确建模架构拓扑、矩阵形状和算法分块行为之间的关系。基于此模型，在Triton框架内实现了一个轻量级GEMM框架。

Result: 在现代GPU上评估多种GEMM问题规模，tritonBLAS实现了超过95%的自动调优解决方案性能，同时将自动调优时间降为零。

Conclusion: tritonBLAS是一个实用的即插即用替代方案，可用于生产环境中的高性能计算和机器学习工作负载，消除了传统自动调优的开销。

Abstract: We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.

</details>


### [8] [Scaling MPI Applications on Aurora](https://arxiv.org/abs/2512.04291)
*Huda Ibeid,Anthony-Trung Nguyen,Aditya Nishtala,Premanand Sakarda,Larry Kaplan,Nilakantan Mahadevan,Michael Woodacre,Victor Anisimov,Kalyan Kumaran,JaeHyuk Kwack,Vitali Morozov,Servesh Muralidharan,Scott Parker*

Main category: cs.DC

TL;DR: 本文介绍了Aurora超级计算机的系统设计，重点分析了其网络架构和验证方法，展示了该系统在多种基准测试和科学应用中的卓越性能。


<details>
  <summary>Details</summary>
Motivation: Aurora是2024年部署在阿贡国家实验室的百亿亿次超级计算机，目前是全球Top500榜单上三台百亿亿次机器之一。本文旨在详细介绍Aurora的系统设计，特别是其网络架构，并验证其能够支持大规模AI和HPC模拟，推动开放科学发展。

Method: 论文重点介绍了Aurora的网络架构设计，采用HPE Slingshot高性能互连网络，包含近85,000个Cassini网卡和5,600个Rosetta交换机，采用蜻蜓拓扑结构。通过MPI基准测试以及HPL、HPL-MxP、Graph500、HPCG等性能基准测试来验证系统性能，并在HACC、AMR-Wind、LAMMPS、FMM等多种科学应用上进行测试。

Result: Aurora在2024年6月的Top500榜单中排名第二，在HPL MxP基准测试中排名第一。系统展示了优异的吞吐量、延迟和带宽性能，能够支持应用程序扩展到大规模节点数量，为科学计算提供了新的能力水平。

Conclusion: Aurora超级计算机通过创新的硬件架构和先进的网络设计，成功实现了百亿亿次计算能力，成为全球最强大的AI和HPC系统之一，能够支持突破性的科学研究。

Abstract: The Aurora supercomputer, which was deployed at Argonne National Laboratory in 2024, is currently one of three Exascale machines in the world on the Top500 list. The Aurora system is composed of over ten thousand nodes each of which contains six Intel Data Center Max Series GPUs, Intel's first data center-focused discrete GPU, and two Intel Xeon Max Series CPUs, Intel's first Xeon processor to contain HBM memory. To achieve Exascale performance the system utilizes the HPE Slingshot high-performance fabric interconnect to connect the nodes. Aurora is currently the largest deployment of the Slingshot fabric to date with nearly 85,000 Cassini NICs and 5,600 Rosetta switches connected in a dragonfly topology. The combination of the Intel powered nodes and the Slingshot network enabled Aurora to become the second fastest system on the Top500 list in June of 2024 and the fastest system on the HPL MxP benchmark. The system is one of the most powerful systems in the world dedicated to AI and HPC simulations for open science. This paper presents details of the Aurora system design with a particular focus on the network fabric and the approach taken to validating it. The performance of the systems is demonstrated through the presentation of the results of MPI benchmarks as well as performance benchmarks including HPL, HPL-MxP, Graph500, and HPCG run on a large fraction of the system. Additionally results are presented for a diverse set of applications including HACC, AMR-Wind, LAMMPS, and FMM demonstrating that Aurora provides the throughput, latency, and bandwidth across system needed to allow applications to perform and scale to large node counts and providing new levels of capability and enabling breakthrough science.

</details>


### [9] [VLCs: Managing Parallelism with Virtualized Libraries](https://arxiv.org/abs/2512.04320)
*Yineng Yan,William Ruys,Hochan Lee,Ian Henriksen,Arthur Peters,Sean Stephens,Bozhi You,Henrique Fingler,Martin Burtscher,Milos Gligoric,Keshav Pingali,Mattan Erez,George Biros,Christopher J. Rossbach*

Main category: cs.DC

TL;DR: VLCs（虚拟库上下文）通过创建进程子单元来封装库和资源分配，无需修改库代码即可控制库的资源使用，解决并行库组合时的资源争用问题，实现最高2.85倍加速。


<details>
  <summary>Details</summary>
Motivation: 现代并行机器日益复杂，程序员依赖软件库组合来利用并行性。但许多库设计时未考虑组合使用，假设独占所有资源，导致并发使用时产生资源争用和性能下降。现有方案需要修改库或操作系统，通常不可行。

Method: 提出虚拟库上下文（VLCs）作为进程子单元，封装库集合及其资源分配。VLCs在不修改库代码的情况下控制库的资源使用，允许用户在库之间划分资源防止争用，或加载同一库的多个副本以支持并行执行线程不安全的代码。

Result: 开发并评估了C++和Python的VLCs原型。实验表明，在使用OpenMP、OpenBLAS和LibTorch等库的应用中，VLCs能实现最高2.85倍的加速。

Conclusion: VLCs提供了一种无需修改库代码的解决方案，有效解决了并行库组合时的资源争用问题，显著提升了并行应用的性能。

Abstract: As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible.
  We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process.
  In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.

</details>


### [10] [Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity](https://arxiv.org/abs/2512.04355)
*Gregory Bolet,Giorgis Georgakoudis,Konstantinos Parasyris,Harshitha Menon,Niranjan Hasabnis,Kirk W. Cameron,Gal Oren*

Main category: cs.DC

TL;DR: gpuFLOPBench是一个评估LLM预测GPU浮点运算次数的基准测试，包含577个CUDA内核，旨在测试模型在不运行代码的情况下进行前瞻性性能推理的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在代码生成方面进展迅速，但很少测试其在GPU性能预测方面的前瞻性推理能力。GPU软件开发需要开发者能够预测性能瓶颈，但现有代码助手无法内化硬件特定的微码效应。

Method: 创建gpuFLOPBench基准测试，包含从HeCBench提取的577个CUDA内核，每个内核都标注了真实性能分析和8个执行属性，区分可简单分析的代码和FLOPs依赖于编译器或运行时行为的复杂内核。

Result: 最新LLM在简单内核上能实现完美分类，但在涉及除法、内置数学函数或公共子表达式等隐含FLOPs的情况下，仍会出现多个数量级的错误，显示出对硬件特定微码效应理解不足的核心限制。

Conclusion: gpuFLOPBench为开发能够像经验丰富的GPU开发者一样严格推理性能的LLM工具提供了一个专注的测试平台，揭示了现有代码助手在硬件特定性能推理方面的局限性。

Abstract: Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to "count without running" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench

</details>


### [11] [A Structure-Aware Irregular Blocking Method for Sparse LU Factorization](https://arxiv.org/abs/2512.04389)
*Zhen Hu,Dongliang Xiong,Kai Huang,Changjun Wu,Xiaowen Jiang*

Main category: cs.DC

TL;DR: 提出一种基于对角线块特征的结构感知不规则分块方法，用于稀疏LU分解，通过调整块大小适应局部非零元分布，在GPU上实现显著加速


<details>
  <summary>Details</summary>
Motivation: 稀疏LU分解中，符号分解后的非零元倾向于分布在矩阵对角线和右下区域，这种非均匀分布导致常规2D分块方法产生工作负载不平衡问题，现有矩阵特征无法有效指导分块

Method: 提出对角线块特征来有效表征稀疏矩阵的局部非零元分布，基于此设计不规则分块方法，根据局部非零元分布调整块大小：在密集区域使用细粒度块，在稀疏区域使用粗粒度块，平衡依赖树中同一层级和跨层级的块非零元数量

Result: 在单块NVIDIA A100 GPU上，相比PanguLU和最新SuperLU_DIST分别获得平均1.50倍和3.32倍加速；在4块A100 GPU上，分别获得1.40倍和3.84倍加速

Conclusion: 提出的结构感知不规则分块方法能有效处理稀疏矩阵的非均匀分布，通过自适应调整块大小平衡工作负载，在GPU上实现显著性能提升

Abstract: In sparse LU factorization, nonzero elements after symbolic factorization tend to distribute in diagonal and right-bottom region of sparse matrices. However, regular 2D blocking on this non-uniform distribution structure may lead to workload imbalance across blocks. Besides, existing matrix features fail to guide us effectively in blocking. In this paper, we propose a structure-aware irregular blocking method for numerical factorization. A novel diagonal block-based feature is introduced to effectively characterize the local nonzero distribution of sparse matrices. Based on this, we further propose an irregular blocking method that adjusts block sizes according to the local distribution of nonzeros. The strategy utilizes fine-grained blocks in dense regions and coarse-grained blocks in sparse regions, adequately balancing the nonzeros of blocks both within the same level and across levels in the dependency tree. Experiments demonstrate that, on a single NVIDIA A100 GPU, our proposed irregular blocking method achieves average speedups of 1.50x and 3.32x over PanguLU and the latest SuperLU_DIST, respectively. In addition, it achieves speedups of 1.40x and 3.84x over PanguLU and SuperLU_DIST on 4 NVIDIA A100 GPUs.

</details>


### [12] [Offloading to CXL-based Computational Memory](https://arxiv.org/abs/2512.04449)
*Suyeon Lee,Kangkyu Park,Kwangsik Shin,Ada Gavrilovska*

Main category: cs.DC

TL;DR: KAI系统通过异步回传协议优化CXL计算内存的性能，减少端到端运行时间达50.4%


<details>
  <summary>Details</summary>
Motivation: 现有CXL计算内存的操作卸载机制无法充分利用不同CXL协议模型的权衡优势，导致数据移动成本高和系统效率低下

Method: 提出"异步回传"协议，在底层CXL协议上分层处理数据和控制传输操作；设计KAI系统实现异步数据移动和轻量级流水线的主机-CCM交互

Result: KAI将端到端运行时间减少高达50.4%，CCM和主机空闲时间分别平均减少22.11倍和3.85倍

Conclusion: 异步回传协议和KAI系统能有效利用CXL协议权衡，显著提升CXL计算内存系统的性能和效率

Abstract: CXL-based Computational Memory (CCM) enables near-memory processing within expanded remote memory, presenting opportunities to address data movement costs associated with disaggregated memory systems and to accelerate overall performance. However, existing operation offloading mechanisms are not capable of leveraging the trade-offs of different models based on different CXL protocols. This work first examines these tradeoffs and demonstrates their impact on end-to-end performance and system efficiency for workloads with diverse data and processing requirements. We propose a novel 'Asynchronous Back-Streaming' protocol by carefully layering data and control transfer operations on top of the underlying CXL protocols. We design KAI, a system that realizes the asynchronous back-streaming model that supports asynchronous data movement and lightweight pipelining in host-CCM interactions. Overall, KAI reduces end-to-end runtime by up to 50.4%, and CCM and host idle times by average 22.11x and 3.85x, respectively.

</details>


### [13] [Federated Learning for Terahertz Wireless Communication](https://arxiv.org/abs/2512.04984)
*O. Tansel Baydas,Ozgur B. Akan*

Main category: cs.DC

TL;DR: THz通信与联邦学习的结合面临宽带损伤的理论挑战，本文揭示了频谱空洞导致收敛误差的"多样性陷阱"，并提出SNR加权聚合策略来恢复收敛性。


<details>
  <summary>Details</summary>
Motivation: THz通信与联邦学习结合可实现超快速分布式学习，但现实宽带损伤（如波束倾斜、分子吸收等）对优化动态的影响缺乏理论分析，需要填补这一空白。

Method: 开发多载波随机框架，将本地梯度更新与频率选择性THz效应（波束倾斜、分子吸收、抖动）显式耦合，分析收敛误差底限与子载波SNR的关系。

Result: 发现"多样性陷阱"：标准无偏聚合下收敛误差底限由子载波SNR的调和均值驱动，单个频谱空洞可使整个带宽失效；识别带宽极限，超过临界点会因热噪声和增益崩溃而降低收敛性；证明SNR加权聚合策略可抑制方差奇点，在高倾斜区域恢复收敛。

Conclusion: THz-FL系统性能受物理层参数显著影响，标准平均聚合在严重波束倾斜下失效，需要SNR加权聚合来克服频谱空洞问题，确保可靠模型更新。

Abstract: The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [FLEX: Leveraging FPGA-CPU Synergy for Mixed-Cell-Height Legalization Acceleration](https://arxiv.org/abs/2512.04527)
*Xingyu Liu,Jiawei Liang,Linfeng Du,Yipu Zhang,Chaofang Ma,Hanwei Fan,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: FLEX：一种用于混合单元高度合法化任务的FPGA-CPU加速器，通过优化任务分配策略、多粒度流水线技术和计算密集型单元移位优化，实现显著加速和更好的合法化质量。


<details>
  <summary>Details</summary>
Motivation: 混合单元高度合法化在物理设计中计算密集且耗时，现有CPU-GPU和多线程CPU解决方案在性能和可扩展性方面仍有提升空间。需要利用FPGA和CPU的互补优势来加速这一关键设计步骤。

Method: 1. 优化任务分配策略，在FPGA和CPU之间进行高效任务划分；2. 采用多粒度流水线技术加速最耗时的步骤——寻找最优放置位置(FOP)；3. 针对FOP中计算密集的单元移位过程进行专门优化，使其与多粒度流水线框架无缝对齐。

Result: FLEX相比最先进的CPU-GPU和多线程CPU合法化器分别实现了18.3倍和5.4倍的加速，同时具有更好的可扩展性。在合法化质量方面分别提升了4%和1%。

Conclusion: FLEX通过创新的FPGA-CPU协同加速架构，显著提升了混合单元高度合法化的性能和效率，为物理设计中的这一关键步骤提供了高效的硬件加速解决方案。

Abstract: In this work, we present FLEX, an FPGA-CPU accelerator for mixed-cell-height legalization tasks. We address challenges from the following perspectives. First, we optimize the task assignment strategy and perform an efficient task partition between FPGA and CPU to exploit their complementary strengths. Second, a multi-granularity pipelining technique is employed to accelerate the most time-consuming step, finding optimal placement position (FOP), in legalization. At last, we particularly target the computationally intensive cell shifting process in FOP, optimizing the design to align it seamlessly with the multi-granularity pipelining framework for further speedup. Experimental results show that FLEX achieves up to 18.3x and 5.4x speedups compared to state-of-the-art CPU-GPU and multi-threaded CPU legalizers with better scalability, while improving legalization quality by 4% and 1%.

</details>


### [15] [Functional Stability of Software-Hardware Neural Network Implementation The NeuroComp Project](https://arxiv.org/abs/2512.04867)
*Bychkov Oleksii,Senysh Taras*

Main category: cs.AR

TL;DR: 提出一种通过神经元级硬件冗余确保神经网络功能稳定性的创新方法，使用独立微计算机实现每个神经元，确保在硬件故障时网络仍能运行


<details>
  <summary>Details</summary>
Motivation: 传统Dropout方法主要用于训练阶段的正则化，无法解决神经网络在运行时的硬件故障问题。需要一种能够在硬件层面确保神经网络功能稳定性的方法，特别是在分布式硬件系统中。

Method: 采用神经元级硬件冗余架构，每个神经元都在独立的微计算机（ESP32）上实现，形成分布式计算系统。当某个计算节点故障时，系统能够继续运行，确保神经网络的功能稳定性。

Result: 系统能够在单个计算节点故障时保持正常运行，实现了硬件层面的容错能力。相比传统Dropout方法，该方法专门针对运行时的硬件故障提供保护。

Conclusion: 通过神经元级硬件冗余的分布式架构，能够有效确保神经网络在硬件故障时的功能稳定性，为实际部署中的可靠性问题提供了新的解决方案。

Abstract: This paper presents an innovative approach to ensuring functional stability of neural networks through hardware redundancy at the individual neuron level. Unlike the classical Dropout method, which is used during training for regularization purposes, the proposed system ensures resilience to hardware failures during network operation. Each neuron is implemented on a separate microcomputer (ESP32), allowing the system to continue functioning even when individual computational nodes fail.

</details>


### [16] [Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming](https://arxiv.org/abs/2512.04910)
*Fang Li*

Main category: cs.AR

TL;DR: 使用ASP实现自动条板电路布局设计，通过两阶段求解方法生成紧凑、可制造的布局


<details>
  <summary>Details</summary>
Motivation: 解决条板电路布局自动化问题，为电子原型设计和教育提供实用工具，同时展示声明式编程在复杂设计自动化问题中的应用潜力

Method: 使用答案集编程（ASP）将布局问题形式化为综合和多目标优化任务，采用两阶段求解方法：先确保可行性，再优化布局质量

Result: 实验结果表明，该方法能为不同复杂度的电路生成紧凑、可制造的布局

Conclusion: 这项工作代表了条板电路布局自动化的重大进展，展示了声明式编程在解决复杂设计自动化问题中的强大能力

Abstract: This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.

</details>
