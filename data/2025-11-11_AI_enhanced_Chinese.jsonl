{"id": "2511.06117", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.06117", "abs": "https://arxiv.org/abs/2511.06117", "authors": ["Yacine Hakimi", "Riyadh Baghdadi"], "title": "A Data-driven Analysis of Code Optimizations", "comment": null, "summary": "As the demand for computational power grows, optimizing code through compilers becomes increasingly crucial. In this context, we focus on fully automatic code optimization techniques that automate the process of selecting and applying code transformations for better performance without manual intervention. Understanding how these transformations behave and interact is key to designing more effective optimization strategies. Compiler developers must make numerous design choices when constructing these heuristics. For instance, they may decide whether to allow transformations to be explored in any arbitrary order or to enforce a fixed sequence. While the former may theoretically offer the best performance gains, it significantly increases the search space. This raises an important question: Can a predefined, fixed order of applying transformations speed up the search without severely compromising optimization potential? In this paper, we address this and other related questions that arise in the design of automatic code optimization algorithms. Using a data-driven approach, we generate a large dataset of random programs, apply random optimization sequences, and record their execution times. Through statistical analysis, we provide insights that guide the development of more efficient automatic code optimization algorithms.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u81ea\u52a8\u4ee3\u7801\u4f18\u5316\u4e2d\u53d8\u6362\u987a\u5e8f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5206\u6790\u56fa\u5b9a\u987a\u5e8f\u4e0e\u4efb\u610f\u987a\u5e8f\u53d8\u6362\u7684\u6743\u8861\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u9700\u6c42\u589e\u957f\uff0c\u81ea\u52a8\u4ee3\u7801\u4f18\u5316\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u7f16\u8bd1\u5668\u5f00\u53d1\u8005\u9762\u4e34\u8bbe\u8ba1\u9009\u62e9\uff1a\u662f\u5141\u8bb8\u4efb\u610f\u987a\u5e8f\u7684\u4ee3\u7801\u53d8\u6362\uff08\u7406\u8bba\u4e0a\u6027\u80fd\u6700\u4f73\u4f46\u641c\u7d22\u7a7a\u95f4\u5927\uff09\uff0c\u8fd8\u662f\u91c7\u7528\u56fa\u5b9a\u987a\u5e8f\uff08\u53ef\u80fd\u52a0\u901f\u641c\u7d22\u4f46\u53ef\u80fd\u635f\u5931\u4f18\u5316\u6f5c\u529b\uff09\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff1a\u751f\u6210\u5927\u91cf\u968f\u673a\u7a0b\u5e8f\uff0c\u5e94\u7528\u968f\u673a\u4f18\u5316\u5e8f\u5217\uff0c\u8bb0\u5f55\u6267\u884c\u65f6\u95f4\uff0c\u5e76\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u4e3a\u81ea\u52a8\u4ee3\u7801\u4f18\u5316\u7b97\u6cd5\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u6027\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u4ee3\u7801\u4f18\u5316\u7b97\u6cd5\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\uff0c\u5e2e\u52a9\u5728\u641c\u7d22\u7a7a\u95f4\u548c\u4f18\u5316\u6f5c\u529b\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002"}}
{"id": "2511.05502", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.05502", "abs": "https://arxiv.org/abs/2511.05502", "authors": ["Varun Rajesh", "Om Jodhpurkar", "Pooja Anbuselvan", "Mantinder Singh", "Ashok Jallepali", "Shantanu Godbole", "Pradeep Kumar Sharma", "Hritvik Shrivastava"], "title": "Production-Grade Local LLM Inference on Apple Silicon: A Comparative Study of MLX, MLC-LLM, Ollama, llama.cpp, and PyTorch MPS", "comment": null, "summary": "We present a systematic, empirical evaluation of five local large language model (LLM) runtimes on Apple Silicon: MLX, MLC-LLM, llama.cpp, Ollama, and PyTorch MPS. Experiments were conducted on a Mac Studio equipped with an M2 Ultra processor and 192 GB of unified memory. Using the Qwen-2.5 model family across prompts ranging from a few hundred to 100,000 tokens, we measure time-to-first-token (TTFT), steady-state throughput, latency percentiles, long-context behavior (key-value and prompt caching), quantization support, streaming performance, batching and concurrency behavior, and deployment complexity.\n  Under our settings, MLX achieves the highest sustained generation throughput, while MLC-LLM delivers consistently lower TTFT for moderate prompt sizes and offers stronger out-of-the-box inference features. llama.cpp is highly efficient for lightweight single-stream use, Ollama emphasizes developer ergonomics but lags in throughput and TTFT, and PyTorch MPS remains limited by memory constraints on large models and long contexts.\n  All frameworks execute fully on-device with no telemetry, ensuring strong privacy guarantees. We release scripts, logs, and plots to reproduce all results. Our analysis clarifies the design trade-offs in Apple-centric LLM deployments and provides evidence-based recommendations for interactive and long-context processing. Although Apple Silicon inference frameworks still trail NVIDIA GPU-based systems such as vLLM in absolute performance, they are rapidly maturing into viable, production-grade solutions for private, on-device LLM inference.", "AI": {"tldr": "\u5bf9\u82f9\u679c\u82af\u7247\u4e0a\u4e94\u4e2a\u672c\u5730\u5927\u8bed\u8a00\u6a21\u578b\u8fd0\u884c\u65f6\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff1aMLX\u3001MLC-LLM\u3001llama.cpp\u3001Ollama\u548cPyTorch MPS\uff0c\u5728M2 Ultra\u82af\u7247\u4e0a\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\uff0c\u6db5\u76d6\u751f\u6210\u541e\u5410\u91cf\u3001\u9996token\u5ef6\u8fdf\u3001\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u7b49\u6307\u6807\u3002", "motivation": "\u8bc4\u4f30\u82f9\u679c\u82af\u7247\u4e0a\u4e0d\u540cLLM\u8fd0\u884c\u65f6\u7684\u5b9e\u9645\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u5f00\u53d1\u8005\u5728\u79c1\u6709\u3001\u8bbe\u5907\u7aefLLM\u63a8\u7406\u573a\u666f\u4e0b\u63d0\u4f9b\u57fa\u4e8e\u8bc1\u636e\u7684\u6846\u67b6\u9009\u62e9\u5efa\u8bae\u3002", "method": "\u5728\u914d\u5907M2 Ultra\u5904\u7406\u5668\u548c192GB\u7edf\u4e00\u5185\u5b58\u7684Mac Studio\u4e0a\uff0c\u4f7f\u7528Qwen-2.5\u6a21\u578b\u5bb6\u65cf\uff0c\u6d4b\u8bd5\u4ece\u51e0\u767e\u523010\u4e07token\u7684\u63d0\u793a\uff0c\u6d4b\u91cf\u9996token\u65f6\u95f4\u3001\u7a33\u6001\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u767e\u5206\u4f4d\u6570\u3001\u957f\u4e0a\u4e0b\u6587\u884c\u4e3a\u3001\u91cf\u5316\u652f\u6301\u7b49\u6307\u6807\u3002", "result": "MLX\u5b9e\u73b0\u6700\u9ad8\u6301\u7eed\u751f\u6210\u541e\u5410\u91cf\uff0cMLC-LLM\u5728\u4e2d\u7b49\u63d0\u793a\u5927\u5c0f\u4e0b\u63d0\u4f9b\u66f4\u4f4e\u7684\u9996token\u5ef6\u8fdf\uff0cllama.cpp\u5728\u8f7b\u91cf\u7ea7\u5355\u6d41\u4f7f\u7528\u4e2d\u9ad8\u6548\uff0cOllama\u6ce8\u91cd\u5f00\u53d1\u8005\u4f53\u9a8c\u4f46\u541e\u5410\u91cf\u548c\u9996token\u5ef6\u8fdf\u8f83\u5dee\uff0cPyTorch MPS\u53d7\u9650\u4e8e\u5927\u6a21\u578b\u548c\u957f\u4e0a\u4e0b\u6587\u7684\u5185\u5b58\u7ea6\u675f\u3002", "conclusion": "\u82f9\u679c\u82af\u7247\u63a8\u7406\u6846\u67b6\u867d\u7136\u7edd\u5bf9\u6027\u80fd\u4ecd\u843d\u540e\u4e8eNVIDIA GPU\u7cfb\u7edf\uff0c\u4f46\u6b63\u5feb\u901f\u6210\u719f\u4e3a\u79c1\u6709\u3001\u8bbe\u5907\u7aefLLM\u63a8\u7406\u7684\u53ef\u884c\u751f\u4ea7\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u6240\u6709\u6846\u67b6\u90fd\u5728\u8bbe\u5907\u4e0a\u5b8c\u5168\u6267\u884c\u4e14\u65e0\u9065\u6d4b\uff0c\u786e\u4fdd\u5f3a\u9690\u79c1\u4fdd\u8bc1\u3002"}}
{"id": "2511.05972", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.05972", "abs": "https://arxiv.org/abs/2511.05972", "authors": ["Guangyuan Liu", "Yinqiu Liu", "Ruichen Zhang", "Dusit Niyato", "Jiawen Kang", "Sumei Sun", "Abbas Jamalipour", "Ping Zhang"], "title": "DWM-RO: Decentralized World Models with Reasoning Offloading for SWIPT-enabled Satellite-Terrestrial HetNets", "comment": null, "summary": "Wireless networks are undergoing a paradigm shift toward massive connectivity with energy-efficient operation, driving the integration of satellite-terrestrial architectures with simultaneous wireless information and power transfer (SWIPT). Optimizing transmit beamforming and power splitting in such systems faces formidable challenges, e.g., time-varying channels and multi-tier interference, which create a complex decision landscape where conventional model-free multi-agent reinforcement learning (MARL) suffers from sample inefficiency due to rarely-encountered state transitions and poor coordination as decentralized agents act independently. This paper proposes the Decentralized World Model with Reasoning Offloading (DWM-RO) framework to address these fundamental limitations. Specifically, each agent employs a world model to learn compact predictive representations of environment dynamics, enabling imagination-based policy training that dramatically reduces required environment interactions. An uncertainty-aware offloading gate monitors local interference levels and model reconstruction errors to trigger selective edge coordination. When activated, a lightweight latent decorrelation mechanism at the edge refines agents' strategic representations, guiding them toward orthogonal actions that minimize resource conflicts. Extensive simulations demonstrate that DWM-RO converges 5 times faster than state-of-the-art baselines while achieving 34.7% higher spectral efficiency and reducing constraint violations by 40%. In dense network scenarios with 10 users, DWM-RO maintains violation rates below 20% while baselines exceed 70%, validating superior robustness.", "AI": {"tldr": "\u63d0\u51faDWM-RO\u6846\u67b6\u89e3\u51b3\u536b\u661f-\u5730\u9762\u7f51\u7edc\u4e2d\u7684SWIPT\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u548c\u63a8\u7406\u5378\u8f7d\u673a\u5236\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u534f\u8c03\u80fd\u529b", "motivation": "\u65e0\u7ebf\u7f51\u7edc\u5411\u5927\u89c4\u6a21\u8fde\u63a5\u548c\u80fd\u6548\u8fd0\u884c\u8f6c\u53d8\uff0c\u536b\u661f-\u5730\u9762\u67b6\u6784\u4e0eSWIPT\u96c6\u6210\u9762\u4e34\u65f6\u53d8\u4fe1\u9053\u548c\u591a\u5c42\u5e72\u6270\u7b49\u6311\u6218\uff0c\u4f20\u7edfMARL\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u548c\u534f\u8c03\u5dee\u7684\u95ee\u9898", "method": "\u6bcf\u4e2a\u4ee3\u7406\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u73af\u5883\u52a8\u6001\u7684\u7d27\u51d1\u9884\u6d4b\u8868\u793a\uff0c\u652f\u6301\u57fa\u4e8e\u60f3\u8c61\u7684\u7b56\u7565\u8bad\u7ec3\uff1b\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5378\u8f7d\u95e8\u76d1\u63a7\u672c\u5730\u5e72\u6270\u548c\u6a21\u578b\u91cd\u5efa\u8bef\u5dee\uff0c\u89e6\u53d1\u9009\u62e9\u6027\u8fb9\u7f18\u534f\u8c03\uff1b\u8fb9\u7f18\u8f7b\u91cf\u7ea7\u6f5c\u5728\u89e3\u76f8\u5173\u673a\u5236\u4f18\u5316\u4ee3\u7406\u7b56\u7565\u8868\u793a", "result": "DWM-RO\u6536\u655b\u901f\u5ea6\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5feb5\u500d\uff0c\u9891\u8c31\u6548\u7387\u63d0\u9ad834.7%\uff0c\u7ea6\u675f\u8fdd\u89c4\u51cf\u5c1140%\uff1b\u572810\u7528\u6237\u5bc6\u96c6\u573a\u666f\u4e2d\uff0c\u8fdd\u89c4\u7387\u4fdd\u6301\u572820%\u4ee5\u4e0b\uff0c\u800c\u57fa\u7ebf\u8d85\u8fc770%", "conclusion": "DWM-RO\u6846\u67b6\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u548c\u63a8\u7406\u5378\u8f7d\u6709\u6548\u89e3\u51b3\u4e86\u536b\u661f-\u5730\u9762SWIPT\u7f51\u7edc\u4e2d\u7684\u534f\u8c03\u548c\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u4f18\u52bf"}}
{"id": "2511.06052", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.06052", "abs": "https://arxiv.org/abs/2511.06052", "authors": ["Philipp Schaad", "Tal Ben-Nun", "Patrick Iff", "Torsten Hoefler"], "title": "Inductive Loop Analysis for Practical HPC Application Optimization", "comment": null, "summary": "Scientific computing applications heavily rely on multi-level loop nests operating on multidimensional arrays. This presents multiple optimization opportunities from exploiting parallelism to reducing data movement through prefetching and improved register usage. HPC frameworks often delegate fine-grained data movement optimization to compilers, but their low-level representations hamper analysis of common patterns, such as strided data accesses and loop-carried dependencies. In this paper, we introduce symbolic, inductive loop optimization (SILO), a novel technique that models data accesses and dependencies as functions of loop nest strides. This abstraction enables the automatic parallelization of sequentially-dependent loops, as well as data movement optimizations including software prefetching and pointer incrementation to reduce register spills. We demonstrate SILO on fundamental kernels from scientific applications with a focus on atmospheric models and numerical solvers, achieving up to 12$\\times$ speedup over the state of the art.", "AI": {"tldr": "SILO\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7b26\u53f7\u5f52\u7eb3\u5faa\u73af\u4f18\u5316\u6280\u672f\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u8bbf\u95ee\u548c\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u4e3a\u5faa\u73af\u6b65\u957f\u7684\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u987a\u5e8f\u4f9d\u8d56\u5faa\u73af\u7684\u81ea\u52a8\u5e76\u884c\u5316\u4ee5\u53ca\u6570\u636e\u79fb\u52a8\u4f18\u5316\uff0c\u5728\u79d1\u5b66\u8ba1\u7b97\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u79d1\u5b66\u8ba1\u7b97\u5e94\u7528\u4e25\u91cd\u4f9d\u8d56\u591a\u7ef4\u6570\u7ec4\u7684\u591a\u7ea7\u5faa\u73af\u5d4c\u5957\uff0c\u4f46\u73b0\u6709HPC\u6846\u67b6\u7684\u4f4e\u7ea7\u8868\u793a\u96be\u4ee5\u5206\u6790\u8de8\u6b65\u6570\u636e\u8bbf\u95ee\u548c\u5faa\u73af\u643a\u5e26\u4f9d\u8d56\u7b49\u5e38\u89c1\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5e76\u884c\u5316\u548c\u6570\u636e\u79fb\u52a8\u4f18\u5316\u7684\u6f5c\u529b\u3002", "method": "\u5f15\u5165\u7b26\u53f7\u5f52\u7eb3\u5faa\u73af\u4f18\u5316(SILO)\uff0c\u5c06\u6570\u636e\u8bbf\u95ee\u548c\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u4e3a\u5faa\u73af\u6b65\u957f\u7684\u51fd\u6570\uff0c\u652f\u6301\u81ea\u52a8\u5e76\u884c\u5316\u987a\u5e8f\u4f9d\u8d56\u5faa\u73af\uff0c\u5e76\u5b9e\u73b0\u8f6f\u4ef6\u9884\u53d6\u548c\u6307\u9488\u9012\u589e\u7b49\u6570\u636e\u79fb\u52a8\u4f18\u5316\u3002", "result": "\u5728\u79d1\u5b66\u8ba1\u7b97\u5e94\u7528\u7684\u57fa\u7840\u5185\u6838\u4e0a\uff0c\u7279\u522b\u662f\u5927\u6c14\u6a21\u578b\u548c\u6570\u503c\u6c42\u89e3\u5668\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u8fbe12\u500d\u7684\u52a0\u901f\u3002", "conclusion": "SILO\u901a\u8fc7\u9ad8\u7ea7\u62bd\u8c61\u6709\u6548\u89e3\u51b3\u4e86\u79d1\u5b66\u8ba1\u7b97\u4e2d\u5faa\u73af\u4f18\u5316\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u81ea\u52a8\u5e76\u884c\u5316\u548c\u6570\u636e\u79fb\u52a8\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.06249", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.06249", "abs": "https://arxiv.org/abs/2511.06249", "authors": ["Omin Kwon", "Kyungjun Oh", "Jaeyong Lee", "Myungsuk Kim", "Jihong Kim"], "title": "STAR: Improving Lifetime and Performance of High-Capacity Modern SSDs Using State-Aware Randomizer", "comment": "To appear in the Proceedings of the 2025 IEEE/ACM International Conference on Computer-Aided Design (ICCAD 2025)", "summary": "Although NAND flash memory has achieved continuous capacity improvements via advanced 3D stacking and multi-level cell technologies, these innovations introduce new reliability challenges, par- ticularly lateral charge spreading (LCS), absent in low-capacity 2D flash memory. Since LCS significantly increases retention errors over time, addressing this problem is essential to ensure the lifetime of modern SSDs employing high-capacity 3D flash memory. In this paper, we propose a novel data randomizer, STate-Aware Randomizer (STAR), which proactively eliminates the majority of weak data patterns responsible for retention errors caused by LCS. Unlike existing techniques that target only specific worst-case patterns, STAR effectively removes a broad spectrum of weak patterns, significantly enhancing reliability against LCS. By employing several optimization schemes, STAR can be efficiently integrated into the existing I/O datapath of an SSD controller with negligible timing overhead. To evaluate the proposed STAR scheme, we developed a STAR-aware SSD emulator based on characterization results from 160 real 3D NAND flash chips. Experimental results demonstrate that STAR improves SSD lifetime by up to 2.3x and reduces read latency by an average of 50% on real-world traces compared to conventional SSDs", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSTAR\u7684\u65b0\u578b\u6570\u636e\u968f\u673a\u5316\u5668\uff0c\u901a\u8fc7\u4e3b\u52a8\u6d88\u9664\u5bfc\u81f4\u6a2a\u5411\u7535\u8377\u6269\u6563(LCS)\u4fdd\u7559\u9519\u8bef\u7684\u5f31\u6570\u636e\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u9ad83D NAND\u95ea\u5b58\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u968f\u77403D\u5806\u53e0\u548c\u591a\u7ea7\u5355\u5143\u6280\u672f\u7684\u53d1\u5c55\uff0cNAND\u95ea\u5b58\u9762\u4e34\u65b0\u7684\u53ef\u9760\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u6a2a\u5411\u7535\u8377\u6269\u6563(LCS)\uff0c\u8fd9\u663e\u8457\u589e\u52a0\u4e86\u4fdd\u7559\u9519\u8bef\uff0c\u5f71\u54cd\u91c7\u7528\u9ad8\u5bb9\u91cf3D\u95ea\u5b58\u7684SSD\u5bff\u547d\u3002", "method": "\u5f00\u53d1\u4e86\u72b6\u6001\u611f\u77e5\u968f\u673a\u5316\u5668(STAR)\uff0c\u901a\u8fc7\u4e3b\u52a8\u6d88\u9664\u5bfc\u81f4LCS\u4fdd\u7559\u9519\u8bef\u7684\u5f31\u6570\u636e\u6a21\u5f0f\uff0c\u5e76\u91c7\u7528\u4f18\u5316\u65b9\u6848\u5c06\u5176\u9ad8\u6548\u96c6\u6210\u5230SSD\u63a7\u5236\u5668\u7684\u73b0\u6709I/O\u6570\u636e\u8def\u5f84\u4e2d\u3002", "result": "\u57fa\u4e8e160\u4e2a\u771f\u5b9e3D NAND\u95ea\u5b58\u82af\u7247\u7684\u8868\u5f81\u7ed3\u679c\uff0c\u5b9e\u9a8c\u663e\u793aSTAR\u53ef\u5c06SSD\u5bff\u547d\u63d0\u9ad8\u81f3\u591a2.3\u500d\uff0c\u5e76\u5728\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5e73\u5747\u51cf\u5c1150%\u7684\u8bfb\u53d6\u5ef6\u8fdf\u3002", "conclusion": "STAR\u65b9\u6848\u80fd\u6709\u6548\u5e94\u5bf93D NAND\u95ea\u5b58\u4e2d\u7684LCS\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347SSD\u53ef\u9760\u6027\u548c\u6027\u80fd\uff0c\u4e14\u5b9e\u73b0\u6210\u672c\u4f4e\u3002"}}
{"id": "2511.06159", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.06159", "abs": "https://arxiv.org/abs/2511.06159", "authors": ["Rasman Mubtasim Swargo", "Md Arifuzzaman"], "title": "Elastic Data Transfer Optimization with Hybrid Reinforcement Learning", "comment": null, "summary": "Modern scientific data acquisition generates petabytes of data that must be transferred to geographically distant computing clusters. Conventional tools either rely on preconfigured sessions, which are difficult to tune for users without domain expertise, or they adaptively optimize only concurrency while ignoring other important parameters. We present \\name, an adaptive data transfer method that jointly considers multiple parameters. Our solution incorporates heuristic-based parallelism, infinite pipelining, and a deep reinforcement learning based concurrency optimizer. To make agent training practical, we introduce a lightweight network simulator that reduces training time to less than four minutes and provides a $2750\\times$ speedup compared to online training. Experimental evaluation shows that \\name consistently outperforms existing methods across diverse datasets, achieving up to 9.5x higher throughput compared to state-of-the-art solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u6570\u636e\u4f20\u8f93\u65b9\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u591a\u4e2a\u53c2\u6570\uff0c\u7ed3\u5408\u542f\u53d1\u5f0f\u5e76\u884c\u3001\u65e0\u9650\u6d41\u6c34\u7ebf\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5e76\u53d1\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7f51\u7edc\u6a21\u62df\u5668\u5927\u5e45\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\uff0c\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u6570\u636e\u91c7\u96c6\u4ea7\u751fPB\u7ea7\u6570\u636e\u9700\u8981\u4f20\u8f93\u5230\u5730\u7406\u4e0a\u9065\u8fdc\u7684\u8ba1\u7b97\u96c6\u7fa4\uff0c\u73b0\u6709\u5de5\u5177\u8981\u4e48\u4f9d\u8d56\u9884\u914d\u7f6e\u4f1a\u8bdd\uff08\u5bf9\u975e\u4e13\u4e1a\u7528\u6237\u96be\u4ee5\u8c03\u4f18\uff09\uff0c\u8981\u4e48\u4ec5\u81ea\u9002\u5e94\u4f18\u5316\u5e76\u53d1\u800c\u5ffd\u7565\u5176\u4ed6\u91cd\u8981\u53c2\u6570\u3002", "method": "\u7ed3\u5408\u542f\u53d1\u5f0f\u5e76\u884c\u3001\u65e0\u9650\u6d41\u6c34\u7ebf\u548c\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5e76\u53d1\u4f18\u5316\u5668\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7f51\u7edc\u6a21\u62df\u5668\u5c06\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u52304\u5206\u949f\u4ee5\u5185\uff0c\u76f8\u6bd4\u5728\u7ebf\u8bad\u7ec3\u63d0\u4f9b2750\u500d\u52a0\u901f\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u9ad8\u8fbe9.5\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u8003\u8651\u591a\u4e2a\u53c2\u6570\u5e76\u63d0\u4f9b\u9ad8\u6548\u7684\u8bad\u7ec3\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u79d1\u5b66\u6570\u636e\u4f20\u8f93\u7684\u6027\u80fd\u3002"}}
{"id": "2511.06313", "categories": ["cs.AR", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.06313", "abs": "https://arxiv.org/abs/2511.06313", "authors": ["Stef Cuyckens", "Xiaoling Yi", "Robin Geens", "Joren Dumoulin", "Martin Wiesner", "Chao Fang", "Marian Verhelst"], "title": "Precision-Scalable Microscaling Datapaths with Optimized Reduction Tree for Efficient NPU Integration", "comment": "To appear in the 31st Asia and South Pacific Design Automation Conference (ASP-DAC 2026, Invited Paper)", "summary": "Emerging continual learning applications necessitate next-generation neural processing unit (NPU) platforms to support both training and inference operations. The promising Microscaling (MX) standard enables narrow bit-widths for inference and large dynamic ranges for training. However, existing MX multiply-accumulate (MAC) designs face a critical trade-off: integer accumulation requires expensive conversions from narrow floating-point products, while FP32 accumulation suffers from quantization losses and costly normalization. To address these limitations, we propose a hybrid precision-scalable reduction tree for MX MACs that combines the benefits of both approaches, enabling efficient mixed-precision accumulation with controlled accuracy relaxation. Moreover, we integrate an 8x8 array of these MACs into the state-of-the-art (SotA) NPU integration platform, SNAX, to provide efficient control and data transfer to our optimized precision-scalable MX datapath. We evaluate our design both on MAC and system level and compare it to the SotA. Our integrated system achieves an energy efficiency of 657, 1438-1675, and 4065 GOPS/W, respectively, for MXINT8, MXFP8/6, and MXFP4, with a throughput of 64, 256, and 512 GOPS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7cbe\u5ea6\u53ef\u6269\u5c55\u7684MX MAC\u8bbe\u8ba1\uff0c\u7ed3\u5408\u6574\u6570\u548c\u6d6e\u70b9\u7d2f\u52a0\u7684\u4f18\u52bf\uff0c\u5728SNAX NPU\u5e73\u53f0\u4e0a\u5b9e\u73b0\u9ad8\u6548\u6df7\u5408\u7cbe\u5ea6\u8ba1\u7b97", "motivation": "\u73b0\u6709MX MAC\u8bbe\u8ba1\u9762\u4e34\u5173\u952e\u6743\u8861\uff1a\u6574\u6570\u7d2f\u52a0\u9700\u8981\u6602\u8d35\u7684\u7a84\u6d6e\u70b9\u4e58\u79ef\u8f6c\u6362\uff0c\u800cFP32\u7d2f\u52a0\u5b58\u5728\u91cf\u5316\u635f\u5931\u548c\u6602\u8d35\u7684\u5f52\u4e00\u5316\u95ee\u9898", "method": "\u8bbe\u8ba1\u6df7\u5408\u7cbe\u5ea6\u53ef\u6269\u5c55\u7684\u5f52\u7ea6\u6811\u7528\u4e8eMX MAC\uff0c\u7ed3\u5408\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u9ad8\u6548\u6df7\u5408\u7cbe\u5ea6\u7d2f\u52a0\uff1b\u5c068x8 MAC\u9635\u5217\u96c6\u6210\u5230SNAX NPU\u5e73\u53f0", "result": "\u96c6\u6210\u7cfb\u7edf\u5728MXINT8\u3001MXFP8/6\u548cMXFP4\u4e0b\u5206\u522b\u5b9e\u73b0657\u30011438-1675\u548c4065 GOPS/W\u7684\u80fd\u6548\uff0c\u541e\u5410\u91cf\u5206\u522b\u4e3a64\u3001256\u548c512 GOPS", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u7cbe\u5ea6\u53ef\u6269\u5c55MX MAC\u8bbe\u8ba1\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728NPU\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6df7\u5408\u7cbe\u5ea6\u8ba1\u7b97\u6027\u80fd"}}
{"id": "2511.06187", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.06187", "abs": "https://arxiv.org/abs/2511.06187", "authors": ["Mathew Joseph", "Tanush Savadi", "Abel Souza"], "title": "LiteCast: A Lightweight Forecaster for Carbon Optimizations", "comment": null, "summary": "Over recent decades, electricity demand has experienced sustained growth through widespread electrification of transportation and the accelerated expansion of Artificial Intelligence (AI). Grids have managed the resulting surges by scaling generation capacity, incorporating additional resources such as solar and wind, and implementing demand-response mechanisms. Altogether, these policies influence a region's carbon intensity by affecting its energy mix. To mitigate the environmental impacts of consumption, carbon-aware optimizations often rely on long-horizon, high-accuracy forecasts of the grid's carbon intensity that typically use compute intensive models with extensive historical energy mix data. In addition to limiting scalability, accuracy improvements do not necessarily translate into proportional increases in savings. Highlighting the need for more efficient forecasting strategies, we argue that carbon forecasting solutions can achieve the majority of savings without requiring highly precise and complex predictions. Instead, it is the preservation of the ranking of forecasts relative to the ground-truth that drives realized savings. In this paper, we present LiteCast, a lightweight time series forecasting method capable of quickly modeling a region's energy mix to estimate its carbon intensity. LiteCast requires only a few days of historical energy and weather data, delivering fast forecasts that can quickly adapt to sudden changes in the electrical grid. Our evaluation in 50 worldwide regions under various real-world workloads shows that LiteCast outperforms state-of-the-art forecasters, delivering 20% higher savings with near-optimal performance, achieving 97% of the maximum attainable average savings, while remaining lightweight, efficient to run, and adaptive to new data.", "AI": {"tldr": "LiteCast\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u78b3\u5f3a\u5ea6\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ec5\u9700\u5c11\u91cf\u5386\u53f2\u6570\u636e\u5373\u53ef\u5feb\u901f\u5efa\u6a21\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u6392\u540d\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u8fd1\u6700\u4f18\u7684\u78b3\u51cf\u6392\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u78b3\u5f3a\u5ea6\u9884\u6d4b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u5386\u53f2\u6570\u636e\u548c\u590d\u6742\u6a21\u578b\uff0c\u4f46\u7cbe\u5ea6\u63d0\u5347\u5e76\u4e0d\u603b\u80fd\u8f6c\u5316\u4e3a\u76f8\u5e94\u51cf\u6392\u6548\u76ca\u3002\u7814\u7a76\u53d1\u73b0\u4fdd\u6301\u9884\u6d4b\u6392\u540d\u6bd4\u8ffd\u6c42\u7edd\u5bf9\u7cbe\u5ea6\u66f4\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u9884\u6d4b\u7b56\u7565\u3002", "method": "\u63d0\u51faLiteCast\u8f7b\u91cf\u7ea7\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ec5\u9700\u51e0\u5929\u5386\u53f2\u80fd\u6e90\u548c\u5929\u6c14\u6570\u636e\uff0c\u80fd\u591f\u5feb\u901f\u5efa\u6a21\u533a\u57df\u80fd\u6e90\u7ed3\u6784\u5e76\u4f30\u8ba1\u78b3\u5f3a\u5ea6\uff0c\u9002\u5e94\u7535\u7f51\u7a81\u53d8\u3002", "result": "\u572850\u4e2a\u5168\u7403\u533a\u57df\u7684\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u6d4b\u8bd5\u4e2d\uff0cLiteCast\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u5b9e\u73b020%\u66f4\u9ad8\u7684\u51cf\u6392\u6548\u76ca\uff0c\u8fbe\u5230\u6700\u5927\u53ef\u83b7\u5e73\u5747\u6548\u76ca\u768497%\uff0c\u540c\u65f6\u4fdd\u6301\u8f7b\u91cf\u9ad8\u6548\u3002", "conclusion": "LiteCast\u8bc1\u660e\u4e86\u78b3\u5f3a\u5ea6\u9884\u6d4b\u65e0\u9700\u9ad8\u5ea6\u7cbe\u786e\u7684\u590d\u6742\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u4fdd\u6301\u9884\u6d4b\u6392\u540d\u5373\u53ef\u5b9e\u73b0\u5927\u90e8\u5206\u51cf\u6392\u6548\u76ca\uff0c\u4e3a\u78b3\u611f\u77e5\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.06558", "categories": ["cs.AR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.06558", "abs": "https://arxiv.org/abs/2511.06558", "authors": ["Akshay Revankar", "Charan Renganathan", "Sartaj Wariah"], "title": "Offloading Data Center Tax", "comment": null, "summary": "The data centers of today are running diverse workloads sharing many common lower level functions called tax components. Any optimization to any tax component will lead to performance improvements across the data center fleet. Typically, performance enhancements in tax components are achieved by offloading them to accelerators, however, it is not practical to offload every tax component. The goal of this paper is to identify opportunities to offload more than one tax component together. We focus on MongoDB which is a common microservice used in a large number of applications in the datacenter. We profile MongoDB running as part of the DeathStarBench benchmark suite, identifying its tax components and their microarchitectural implications. We make observations and suggestions based on the inferences made to offload a few of the tax components in this application.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u6570\u636e\u4e2d\u5fc3\u4e2d\u8bc6\u522b\u5e76\u5171\u540c\u5378\u8f7d\u591a\u4e2atax\u7ec4\u4ef6\u7684\u673a\u4f1a\uff0c\u4ee5MongoDB\u4e3a\u4f8b\u8fdb\u884c\u6027\u80fd\u5206\u6790\u5e76\u63d0\u51fa\u5378\u8f7d\u5efa\u8bae\u3002", "motivation": "\u6570\u636e\u4e2d\u5fc3\u8fd0\u884c\u7740\u591a\u6837\u5316\u7684\u8d1f\u8f7d\uff0c\u8fd9\u4e9b\u8d1f\u8f7d\u5171\u4eab\u8bb8\u591a\u79f0\u4e3atax\u7ec4\u4ef6\u7684\u5e95\u5c42\u529f\u80fd\u3002\u4f18\u5316\u4efb\u4f55tax\u7ec4\u4ef6\u90fd\u80fd\u63d0\u9ad8\u6574\u4e2a\u6570\u636e\u4e2d\u5fc3\u96c6\u7fa4\u7684\u6027\u80fd\u3002\u867d\u7136\u901a\u5e38\u901a\u8fc7\u5c06tax\u7ec4\u4ef6\u5378\u8f7d\u5230\u52a0\u901f\u5668\u6765\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u5355\u72ec\u5378\u8f7d\u6bcf\u4e2a\u7ec4\u4ef6\u5e76\u4e0d\u73b0\u5b9e\u3002", "method": "\u4f7f\u7528DeathStarBench\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u5bf9MongoDB\u8fdb\u884c\u6027\u80fd\u5206\u6790\uff0c\u8bc6\u522b\u5176tax\u7ec4\u4ef6\u53ca\u5176\u5fae\u67b6\u6784\u5f71\u54cd\uff0c\u5e76\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\u63d0\u51fa\u5378\u8f7d\u5efa\u8bae\u3002", "result": "\u901a\u8fc7\u5206\u6790\u8bc6\u522b\u51fa\u4e86MongoDB\u4e2d\u7684tax\u7ec4\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u5171\u540c\u5378\u8f7d\u591a\u4e2atax\u7ec4\u4ef6\u7684\u5177\u4f53\u673a\u4f1a\u548c\u5efa\u8bae\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u5171\u540c\u5378\u8f7d\u591a\u4e2atax\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u5728\u6570\u636e\u4e2d\u5fc3\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u6027\u80fd\u4f18\u5316\uff0c\u800cMongoDB\u4f5c\u4e3a\u4e00\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5fae\u670d\u52a1\uff0c\u662f\u5b9e\u65bd\u8fd9\u79cd\u4f18\u5316\u7b56\u7565\u7684\u7406\u60f3\u6848\u4f8b\u3002"}}
{"id": "2511.06247", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.06247", "abs": "https://arxiv.org/abs/2511.06247", "authors": ["Cong Li", "Yuzhe Yang", "Xuegui Zheng", "Qifan Yang", "Yijin Guan", "Size Zheng", "Li-Wen Chang", "Shufan Liu", "Xin Liu", "Guangyu Sun"], "title": "Optimizing Long-context LLM Serving via Fine-grained Sequence Parallelism", "comment": null, "summary": "With the advancement of large language models (LLMs), their context windows have rapidly expanded. To meet diverse demands from varying-length requests in online services, existing state-of-the-art systems tune the sequence parallelism (SP) allocation. However, current dynamic SP allocation lacks flexibility to (1) support stage-specific parallelism requirements in LLM inference, (2) mitigate the global latency degradation from excessive SP allocation, and (3) exploit resource fragments arising from SP size variation.\n  To tackle this problem, we propose Chunkwise Dynamic Sequence Parallelism (CDSP), a fine-grained parallelism strategy that assigns SP sizes across \\textit{intra-request} token segments. Based on CDSP, we build Tetris, an LLM serving system that (1) efficiently integrates CDSP into disaggregated cluster to satisfy parallelism heterogeneity, (2) dynamically regulates SP size expansion based on real-time load conditions, and (3) adaptively explores chunking plans to utilize fragmented resources while meeting per-request demands. Compared with state-of-the-art systems, Tetris achieves up to 4.35$\\times$ lower time-to-first-token (TTFT) under max sustainable loads, reduces median time-between-tokens (TBT) by up to 40.1\\%, and increases the max request capacity by up to 45\\%.", "AI": {"tldr": "\u63d0\u51faChunkwise Dynamic Sequence Parallelism (CDSP)\u548cTetris\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u5e8f\u5217\u5e76\u884c\u5206\u914d\u7b56\u7565\uff0c\u5728LLM\u63a8\u7406\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8d44\u6e90\u5229\u7528\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u5e8f\u5217\u5e76\u884c\u5206\u914d\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u652f\u6301LLM\u63a8\u7406\u4e2d\u9636\u6bb5\u7279\u5b9a\u7684\u5e76\u884c\u9700\u6c42\uff0c\u96be\u4ee5\u7f13\u89e3\u8fc7\u5ea6\u5e76\u884c\u5206\u914d\u5bfc\u81f4\u7684\u5168\u5c40\u5ef6\u8fdf\u6076\u5316\uff0c\u4ee5\u53ca\u65e0\u6cd5\u6709\u6548\u5229\u7528\u56e0\u5e76\u884c\u89c4\u6a21\u53d8\u5316\u4ea7\u751f\u7684\u8d44\u6e90\u788e\u7247\u3002", "method": "\u63d0\u51faCDSP\u7b56\u7565\uff0c\u5728\u8bf7\u6c42\u5185\u90e8\u6309token\u6bb5\u5206\u914d\u5e8f\u5217\u5e76\u884c\u5927\u5c0f\uff1b\u6784\u5efaTetris\u7cfb\u7edf\uff0c\u5c06CDSP\u96c6\u6210\u5230\u89e3\u8026\u96c6\u7fa4\u4e2d\u6ee1\u8db3\u5e76\u884c\u5f02\u6784\u6027\uff0c\u57fa\u4e8e\u5b9e\u65f6\u8d1f\u8f7d\u52a8\u6001\u8c03\u8282\u5e76\u884c\u5927\u5c0f\u6269\u5c55\uff0c\u81ea\u9002\u5e94\u63a2\u7d22\u5206\u5757\u8ba1\u5212\u4ee5\u5229\u7528\u788e\u7247\u8d44\u6e90\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7cfb\u7edf\u76f8\u6bd4\uff0cTetris\u5728\u6700\u5927\u53ef\u6301\u7eed\u8d1f\u8f7d\u4e0b\u5b9e\u73b0\u9ad8\u8fbe4.35\u500d\u7684\u9996token\u65f6\u95f4\u964d\u4f4e\uff0c\u4e2d\u4f4dtoken\u95f4\u9694\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe40.1%\uff0c\u6700\u5927\u8bf7\u6c42\u5bb9\u91cf\u63d0\u5347\u9ad8\u8fbe45%\u3002", "conclusion": "CDSP\u548cTetris\u7cfb\u7edf\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u5e8f\u5217\u5e76\u884c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u670d\u52a1\u7684\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2511.06565", "categories": ["cs.AR", "cs.CL", "cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.06565", "abs": "https://arxiv.org/abs/2511.06565", "authors": ["Arnab A Purkayastha", "Jay Tharwani", "Shobhit Aggarwal"], "title": "FPGA or GPU? Analyzing comparative research for application-specific guidance", "comment": "7 pages", "summary": "The growing complexity of computational workloads has amplified the need for efficient and specialized hardware accelerators. Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) have emerged as prominent solutions, each excelling in specific domains. Although there is substantial research comparing FPGAs and GPUs, most of the work focuses primarily on performance metrics, offering limited insight into the specific types of applications that each accelerator benefits the most. This paper aims to bridge this gap by synthesizing insights from various research articles to guide users in selecting the appropriate accelerator for domain-specific applications. By categorizing the reviewed studies and analyzing key performance metrics, this work highlights the strengths, limitations, and ideal use cases for FPGAs and GPUs. The findings offer actionable recommendations, helping researchers and practitioners navigate trade-offs in performance, energy efficiency, and programmability.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7efc\u5408\u5206\u6790\u73b0\u6709\u7814\u7a76\uff0c\u4e3a\u9886\u57df\u7279\u5b9a\u5e94\u7528\u9009\u62e9\u5408\u9002\u7684\u786c\u4ef6\u52a0\u901f\u5668\uff08FPGA vs GPU\uff09\u63d0\u4f9b\u6307\u5bfc\uff0c\u91cd\u70b9\u5173\u6ce8\u5404\u81ea\u7684\u4f18\u52bf\u9886\u57df\u548c\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8FPGA\u548cGPU\u7684\u6027\u80fd\u6307\u6807\u6bd4\u8f83\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6bcf\u79cd\u52a0\u901f\u5668\u6700\u9002\u5408\u7684\u7279\u5b9a\u5e94\u7528\u7c7b\u578b\u7684\u6df1\u5165\u5206\u6790\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e2e\u52a9\u7528\u6237\u6839\u636e\u5177\u4f53\u5e94\u7528\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u786c\u4ef6\u52a0\u901f\u5668\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u6574\u7406\u5404\u79cd\u7814\u7a76\u6587\u732e\uff0c\u5206\u6790\u5173\u952e\u6027\u80fd\u6307\u6807\uff0c\u7cfb\u7edf\u8bc4\u4f30FPGA\u548cGPU\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u660e\u786e\u4e86FPGA\u548cGPU\u5404\u81ea\u7684\u4f18\u52bf\u3001\u5c40\u9650\u6027\u548c\u7406\u60f3\u4f7f\u7528\u573a\u666f\uff0c\u63d0\u4f9b\u4e86\u57fa\u4e8e\u6027\u80fd\u3001\u80fd\u6548\u548c\u53ef\u7f16\u7a0b\u6027\u6743\u8861\u7684\u5b9e\u9645\u5efa\u8bae\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6307\u5bfc\uff0c\u5e2e\u52a9\u4ed6\u4eec\u6839\u636e\u7279\u5b9a\u5e94\u7528\u9700\u6c42\u5728FPGA\u548cGPU\u4e4b\u95f4\u505a\u51fa\u660e\u667a\u7684\u9009\u62e9\u51b3\u7b56\u3002"}}
{"id": "2511.06345", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.06345", "abs": "https://arxiv.org/abs/2511.06345", "authors": ["Kelun Lei", "Hailong Yang", "Huaitao Zhang", "Xin You", "Kaige Zhang", "Zhongzhi Luan", "Yi Liu", "Depei Qian"], "title": "PRAGMA: A Profiling-Reasoned Multi-Agent Framework for Automatic Kernel Optimization", "comment": null, "summary": "Designing high-performance kernels requires expert-level tuning and a deep understanding of hardware characteristics. Recent advances in large language models (LLMs) have enabled automated kernel generation, yet most existing systems rely solely on correctness or execution time feedback, lacking the ability to reason about low-level performance bottlenecks. In this paper, we introduce PRAGMA, a profile-guided AI kernel generation framework that integrates execution feedback and fine-grained hardware profiling into the reasoning loop. PRAGMA enables LLMs to identify performance bottlenecks, preserve historical best versions, and iteratively refine code quality. We evaluate PRAGMA on KernelBench, covering GPU and CPU backends. Results show that PRAGMA consistently outperforms baseline AIKG without profiling enabled and achieves 2.81$\\times$ and 2.30$\\times$ averaged speedups against Torch on CPU and GPU platforms, respectively.", "AI": {"tldr": "PRAGMA\u662f\u4e00\u4e2a\u57fa\u4e8e\u6027\u80fd\u5206\u6790\u7684AI\u5185\u6838\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6267\u884c\u53cd\u9988\u548c\u7ec6\u7c92\u5ea6\u786c\u4ef6\u6027\u80fd\u5206\u6790\u6574\u5408\u5230\u63a8\u7406\u5faa\u73af\u4e2d\uff0c\u5e2e\u52a9LLM\u8bc6\u522b\u6027\u80fd\u74f6\u9888\u5e76\u8fed\u4ee3\u4f18\u5316\u4ee3\u7801\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709AI\u5185\u6838\u751f\u6210\u7cfb\u7edf\u4ec5\u4f9d\u8d56\u6b63\u786e\u6027\u6216\u6267\u884c\u65f6\u95f4\u53cd\u9988\uff0c\u7f3a\u4e4f\u5bf9\u5e95\u5c42\u6027\u80fd\u74f6\u9888\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u4e13\u5bb6\u7ea7\u8c03\u4f18\u548c\u6df1\u5ea6\u786c\u4ef6\u7406\u89e3\u3002", "method": "\u5f15\u5165PRAGMA\u6846\u67b6\uff0c\u96c6\u6210\u6267\u884c\u53cd\u9988\u548c\u7ec6\u7c92\u5ea6\u786c\u4ef6\u6027\u80fd\u5206\u6790\uff0c\u4f7fLLM\u80fd\u591f\u8bc6\u522b\u6027\u80fd\u74f6\u9888\u3001\u4fdd\u7559\u5386\u53f2\u6700\u4f73\u7248\u672c\u5e76\u8fed\u4ee3\u4f18\u5316\u4ee3\u7801\u3002", "result": "\u5728KernelBench\u4e0a\u8bc4\u4f30\uff0cPRAGMA\u59cb\u7ec8\u4f18\u4e8e\u672a\u542f\u7528\u6027\u80fd\u5206\u6790\u7684\u57fa\u7ebfAIKG\uff0c\u5728CPU\u548cGPU\u5e73\u53f0\u4e0a\u5206\u522b\u5b9e\u73b02.81\u500d\u548c2.30\u500d\u7684\u5e73\u5747\u52a0\u901f\u6bd4\u3002", "conclusion": "PRAGMA\u901a\u8fc7\u6574\u5408\u6027\u80fd\u5206\u6790\u5230AI\u5185\u6838\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u6838\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6027\u80fd\u5f15\u5bfc\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u5185\u6838\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.06679", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.06679", "abs": "https://arxiv.org/abs/2511.06679", "authors": ["Sangun Choi", "Yunho Oh"], "title": "EONSim: An NPU Simulator for On-Chip Memory and Embedding Vector Operations", "comment": null, "summary": "Embedding vector operations are a key component of modern deep neural network workloads. Unlike matrix operations with deterministic access patterns, embedding vector operations exhibit input data-dependent and non-deterministic memory accesses. Existing neural processing unit (NPU) simulators focus on matrix computations with simple double-buffered on-chip memory systems, lacking the modeling capability for realistic embedding behavior. Next-generation NPUs, however, call for more flexible on-chip memory architectures that can support diverse access and management schemes required by embedding workloads. To enable flexible exploration and design of emerging NPU architectures, we present EONSim, an NPU simulator that holistically models both matrix and embedding vector operations. EONSim integrates a validated performance model for matrix computations with detailed memory simulation for embedding accesses, supporting various on-chip memory management policies. Validated against TPUv6e, EONSim achieves an average inference time error of 1.4\\% and an average on-chip memory access count error of 2.2\\%.", "AI": {"tldr": "EONSim\u662f\u4e00\u4e2aNPU\u6a21\u62df\u5668\uff0c\u4e13\u95e8\u7528\u4e8e\u5efa\u6a21\u77e9\u9635\u548c\u5d4c\u5165\u5411\u91cf\u64cd\u4f5c\uff0c\u652f\u6301\u7075\u6d3b\u7684\u7247\u4e0a\u5185\u5b58\u67b6\u6784\u63a2\u7d22\uff0c\u9a8c\u8bc1\u663e\u793a\u5176\u51c6\u786e\u6027\u9ad8\u3002", "motivation": "\u73b0\u6709NPU\u6a21\u62df\u5668\u4e3b\u8981\u5173\u6ce8\u77e9\u9635\u8ba1\u7b97\uff0c\u7f3a\u4e4f\u5bf9\u5d4c\u5165\u5411\u91cf\u64cd\u4f5c\u4e2d\u6570\u636e\u4f9d\u8d56\u6027\u548c\u975e\u786e\u5b9a\u6027\u5185\u5b58\u8bbf\u95ee\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u800c\u4e0b\u4e00\u4ee3NPU\u9700\u8981\u652f\u6301\u5d4c\u5165\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7075\u6d3b\u5185\u5b58\u67b6\u6784\u3002", "method": "\u5f00\u53d1EONSim\u6a21\u62df\u5668\uff0c\u6574\u5408\u5df2\u9a8c\u8bc1\u7684\u77e9\u9635\u8ba1\u7b97\u6027\u80fd\u6a21\u578b\u548c\u8be6\u7ec6\u7684\u5d4c\u5165\u8bbf\u95ee\u5185\u5b58\u6a21\u62df\uff0c\u652f\u6301\u591a\u79cd\u7247\u4e0a\u5185\u5b58\u7ba1\u7406\u7b56\u7565\u3002", "result": "\u4e0eTPUv6e\u9a8c\u8bc1\u5bf9\u6bd4\uff0cEONSim\u5b9e\u73b0\u4e86\u5e73\u5747\u63a8\u7406\u65f6\u95f4\u8bef\u5dee1.4%\u548c\u5e73\u5747\u7247\u4e0a\u5185\u5b58\u8bbf\u95ee\u8ba1\u6570\u8bef\u5dee2.2%\u3002", "conclusion": "EONSim\u80fd\u591f\u5168\u9762\u5efa\u6a21\u77e9\u9635\u548c\u5d4c\u5165\u5411\u91cf\u64cd\u4f5c\uff0c\u4e3a\u65b0\u5174NPU\u67b6\u6784\u7684\u7075\u6d3b\u63a2\u7d22\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2511.06599", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.06599", "abs": "https://arxiv.org/abs/2511.06599", "authors": ["Siddharth Agarwal", "Maria A. Rodriguez", "Rajkumar Buyya"], "title": "Saarthi: An End-to-End Intelligent Platform for Optimising Distributed Serverless Workloads", "comment": "12 pages, 9 figures, 1 table, 2 algorithms", "summary": "FaaS offers significant advantages with its infrastructure abstraction, on-demand execution, and attractive no idle resource pricing for modern cloud applications. Despite these benefits, challenges such as startup latencies, static configurations, sub-optimal resource allocation and scheduling still exist due to coupled resource offering and workload-agnostic generic scheduling behaviour. These issues often lead to inconsistent function performance and unexpected operational costs for users and service providers. This paper introduces Saarthi, a novel, end-to-end serverless framework that intelligently manages the dynamic resource needs of function workloads, representing a significant step toward self-driving serverless platforms. Unlike platforms that rely on static resource configurations, Saarthi is input-aware, allowing it to intelligently anticipate resource requirements based on the characteristics of an incoming request payload. This input-driven approach reinforces function right-sizing and enables smart request orchestration across available function configurations. Saarthi further integrates a proactive fault-tolerant redundancy mechanism and employs a multi-objective Integer Linear Programming (ILP) model to maintain an optimal function quantity. This optimisation aims to maximise system throughput while simultaneously reducing overall operational costs. We validate the effectiveness of Saarthi by implementing it as a framework atop OpenFaaS. Our results demonstrate Saarthi's ability to achieve up to 1.45x better throughput, 1.84x reduced costs, while maintaining up to 98.3% service level targets with an overhead of up to 0.2 seconds as compared to the baseline OpenFaaS.", "AI": {"tldr": "Saarthi\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u65e0\u670d\u52a1\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u8f93\u5165\u611f\u77e5\u7684\u667a\u80fd\u8d44\u6e90\u7ba1\u7406\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u89e3\u51b3\u4e86FaaS\u5e73\u53f0\u4e2d\u7684\u542f\u52a8\u5ef6\u8fdf\u3001\u9759\u6001\u914d\u7f6e\u548c\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "FaaS\u5e73\u53f0\u867d\u7136\u5177\u6709\u57fa\u7840\u8bbe\u65bd\u62bd\u8c61\u3001\u6309\u9700\u6267\u884c\u548c\u65e0\u7a7a\u95f2\u8d44\u6e90\u5b9a\u4ef7\u7b49\u4f18\u52bf\uff0c\u4f46\u4ecd\u9762\u4e34\u542f\u52a8\u5ef6\u8fdf\u3001\u9759\u6001\u914d\u7f6e\u3001\u8d44\u6e90\u5206\u914d\u548c\u8c03\u5ea6\u4e0d\u4f18\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u51fd\u6570\u6027\u80fd\u4e0d\u4e00\u81f4\u548c\u610f\u5916\u8fd0\u8425\u6210\u672c\u3002", "method": "Saarthi\u91c7\u7528\u8f93\u5165\u611f\u77e5\u7684\u65b9\u6cd5\uff0c\u6839\u636e\u8bf7\u6c42\u8d1f\u8f7d\u7279\u5f81\u9884\u6d4b\u8d44\u6e90\u9700\u6c42\uff0c\u5b9e\u73b0\u51fd\u6570\u89c4\u6a21\u8c03\u6574\u548c\u667a\u80fd\u8bf7\u6c42\u7f16\u6392\u3002\u96c6\u6210\u4e86\u4e3b\u52a8\u5bb9\u9519\u5197\u4f59\u673a\u5236\uff0c\u5e76\u4f7f\u7528\u591a\u76ee\u6807\u6574\u6570\u7ebf\u6027\u89c4\u5212\u6a21\u578b\u6765\u7ef4\u6301\u6700\u4f18\u51fd\u6570\u6570\u91cf\u3002", "result": "\u5728OpenFaaS\u4e0a\u5b9e\u73b0\u7684Saarthi\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u4e861.45\u500d\u541e\u5410\u91cf\u63d0\u5347\u30011.84\u500d\u6210\u672c\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u630198.3%\u7684\u670d\u52a1\u6c34\u5e73\u76ee\u6807\uff0c\u5f00\u9500\u4ec5\u4e3a0.2\u79d2\u3002", "conclusion": "Saarthi\u4ee3\u8868\u4e86\u5411\u81ea\u9a71\u52a8\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u901a\u8fc7\u667a\u80fd\u8d44\u6e90\u7ba1\u7406\u6709\u6548\u89e3\u51b3\u4e86FaaS\u5e73\u53f0\u7684\u73b0\u6709\u6311\u6218\u3002"}}
{"id": "2511.06736", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2511.06736", "abs": "https://arxiv.org/abs/2511.06736", "authors": ["Arsalan Ali Malik", "John Buchanan", "Aydin Aysu"], "title": "Preemption-Enhanced Benchmark Suite for FPGAs", "comment": "13 Pages, 4 Figures, 4 Tables", "summary": "Field-Programmable Gate Arrays (FPGAs) have become essential in cloud computing due to their reconfigurability, energy efficiency, and ability to accelerate domain-specific workloads. As FPGA adoption grows, research into task scheduling and preemption techniques has intensified. However, the field lacks a standardized benchmarking framework for consistent and reproducible evaluation. Many existing studies propose innovative scheduling or preemption mechanisms but often rely on proprietary or synthetic benchmarks, limiting generalizability and making comparison difficult. This methodical fragmentation hinders effective evaluation of scheduling strategies and preemption in multi-tenant FPGA environments.\n  This paper presents the first open-source preemption-enabled benchmark suite for evaluating FPGA preemption strategies and testing new scheduling algorithms, without requiring users to create preemption workloads from scratch. The suite includes 27 diverse applications spanning cryptography, AI/ML, computation-intensive workloads, communication systems, and multimedia processing. Each benchmark integrates comprehensive context-saving and restoration mechanisms, facilitating reproducible research and consistent comparisons. Our suite not only simplifies testing FPGA scheduling policies but also benefits OS research by enabling the evaluation of scheduling fairness, resource allocation efficiency, and context-switching performance in multi-tenant FPGA systems, ultimately supporting the development of better operating systems and scheduling policies for FPGA-based environments. We also provide guidelines for adding new benchmarks, enabling future research to expand and refine FPGA preemption and scheduling evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5f00\u6e90FPGA\u62a2\u5360\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542b27\u4e2a\u591a\u6837\u5316\u5e94\u7528\uff0c\u652f\u6301\u62a2\u5360\u7b56\u7565\u8bc4\u4f30\u548c\u8c03\u5ea6\u7b97\u6cd5\u6d4b\u8bd5\uff0c\u65e0\u9700\u7528\u6237\u4ece\u5934\u521b\u5efa\u62a2\u5360\u5de5\u4f5c\u8d1f\u8f7d\u3002", "motivation": "FPGA\u5728\u4e91\u8ba1\u7b97\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u6765\u8bc4\u4f30\u4efb\u52a1\u8c03\u5ea6\u548c\u62a2\u5360\u6280\u672f\uff0c\u73b0\u6709\u7814\u7a76\u4f7f\u7528\u4e13\u6709\u6216\u5408\u6210\u57fa\u51c6\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u548c\u53ef\u6bd4\u6027\u3002", "method": "\u5f00\u53d1\u5305\u542b27\u4e2a\u5e94\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u6db5\u76d6\u5bc6\u7801\u5b66\u3001AI/ML\u3001\u8ba1\u7b97\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u3001\u901a\u4fe1\u7cfb\u7edf\u548c\u591a\u5a92\u4f53\u5904\u7406\u7b49\u9886\u57df\uff0c\u6bcf\u4e2a\u57fa\u51c6\u90fd\u96c6\u6210\u5b8c\u6574\u7684\u4e0a\u4e0b\u6587\u4fdd\u5b58\u548c\u6062\u590d\u673a\u5236\u3002", "result": "\u63d0\u4f9b\u4e86\u53ef\u91cd\u73b0\u7814\u7a76\u548c\u4e00\u81f4\u6bd4\u8f83\u7684\u57fa\u7840\uff0c\u7b80\u5316\u4e86FPGA\u8c03\u5ea6\u7b56\u7565\u6d4b\u8bd5\uff0c\u652f\u6301\u8bc4\u4f30\u591a\u79df\u6237FPGA\u7cfb\u7edf\u4e2d\u7684\u8c03\u5ea6\u516c\u5e73\u6027\u3001\u8d44\u6e90\u5206\u914d\u6548\u7387\u548c\u4e0a\u4e0b\u6587\u5207\u6362\u6027\u80fd\u3002", "conclusion": "\u8be5\u57fa\u51c6\u5957\u4ef6\u4e0d\u4ec5\u4fc3\u8fdbFPGA\u62a2\u5360\u548c\u8c03\u5ea6\u8bc4\u4f30\uff0c\u8fd8\u4e3a\u64cd\u4f5c\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u597d\u7684FPGA\u73af\u5883\u64cd\u4f5c\u7cfb\u7edf\u548c\u8c03\u5ea6\u7b56\u7565\uff0c\u5e76\u63d0\u4f9b\u4e86\u6dfb\u52a0\u65b0\u57fa\u51c6\u7684\u6307\u5357\u3002"}}
{"id": "2511.06605", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.06605", "abs": "https://arxiv.org/abs/2511.06605", "authors": ["Suchita Pati", "Mahzabeen Islam", "Shaizeen Aga", "Mohamed Assem Ibrahim"], "title": "DMA Collectives for Efficient ML Communication Offloads", "comment": null, "summary": "Offloading machine learning (ML) communication collectives to direct memory access (DMA) engines has emerged as an interesting and low-cost solution to efficiently overlap computation and communication in inference and training. Doing so delivers superior concurrent performance by freeing up all GPU cores for computation and also lowers interference in the memory sub-system (caches). While DMA collectives show strong promise, prior works have only studied them in limited context (bandwidth-bound transfer sizes only, performance-only).\n  To address this, we provide a comprehensive performance, power/energy and synchronization costs analysis of offloading ML communication collectives (all-gather, all-to-all) to DMA engines on state-of-the-art AMD Instinct MI300X GPUs. Our analysis reveals that, compared to the state-of-the-art RCCL communication collectives library, DMA collectives are at-par or better for large sizes (10s of MB to GB) in terms of both performance (16% better) and power (32% better). However, they significantly lag for latency-bound small sizes; 4.5X and 2.5X slower for all-gather and all-to-all, respectively. We provide a detailed latency breakdown of a DMA transfer and identify that DMA command scheduling and synchronization costs can limit DMA collective performance. To tackle this, we harness existing DMA architecture innovations, hitherto untapped, to build optimized DMA collectives and demonstrate their efficacy on real hardware. Our optimized implementations considerably close the performance gap for DMA collectives at smaller sizes (30% slower and 20% faster all-gather and all-to-all, respectively) and further improves performance (by 7%) and power savings at larger sizes (3-10%). Overall, this work represents a significant step toward making DMA collectives suitable for adoption in mainstream collective libraries.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5c06ML\u901a\u4fe1\u96c6\u5408\u64cd\u4f5c\u5378\u8f7d\u5230DMA\u5f15\u64ce\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u53d1\u73b0\u5728\u5927\u5c3a\u5bf8\u6570\u636e\u4f20\u8f93\u65f6DMA\u96c6\u5408\u64cd\u4f5c\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u4f46\u5728\u5c0f\u5c3a\u5bf8\u65f6\u6027\u80fd\u8f83\u5dee\u3002\u901a\u8fc7\u4f18\u5316\u5b9e\u73b0\u663e\u8457\u7f29\u5c0f\u4e86\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5c06ML\u901a\u4fe1\u96c6\u5408\u64cd\u4f5c\u5378\u8f7d\u5230DMA\u5f15\u64ce\u53ef\u4ee5\u9ad8\u6548\u91cd\u53e0\u8ba1\u7b97\u548c\u901a\u4fe1\uff0c\u91ca\u653eGPU\u6838\u5fc3\u7528\u4e8e\u8ba1\u7b97\u5e76\u51cf\u5c11\u5185\u5b58\u5b50\u7cfb\u7edf\u5e72\u6270\u3002\u4f46\u4e4b\u524d\u7684\u7814\u7a76\u4ec5\u5728\u6709\u9650\u80cc\u666f\u4e0b\u8fdb\u884c\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u5206\u6790\u3002", "method": "\u5728AMD Instinct MI300X GPU\u4e0a\u5bf9DMA\u96c6\u5408\u64cd\u4f5c\u8fdb\u884c\u6027\u80fd\u3001\u529f\u8017/\u80fd\u91cf\u548c\u540c\u6b65\u6210\u672c\u5206\u6790\uff0c\u5e76\u4e0eRCCL\u901a\u4fe1\u96c6\u5408\u5e93\u6bd4\u8f83\u3002\u8bc6\u522bDMA\u4f20\u8f93\u5ef6\u8fdf\u74f6\u9888\uff0c\u5229\u7528\u73b0\u6709DMA\u67b6\u6784\u521b\u65b0\u6784\u5efa\u4f18\u5316\u5b9e\u73b0\u3002", "result": "DMA\u96c6\u5408\u64cd\u4f5c\u5728\u5927\u5c3a\u5bf8\uff0810MB\u5230GB\uff09\u65f6\u6027\u80fd\u63d0\u534716%\uff0c\u529f\u8017\u964d\u4f4e32%\uff1b\u4f46\u5728\u5c0f\u5c3a\u5bf8\u65f6\u6027\u80fd\u8f83\u5dee\uff08all-gather\u61624.5\u500d\uff0call-to-all\u61622.5\u500d\uff09\u3002\u4f18\u5316\u5b9e\u73b0\u663e\u8457\u7f29\u5c0f\u4e86\u5c0f\u5c3a\u5bf8\u6027\u80fd\u5dee\u8ddd\uff08all-gather\u4ec5\u616230%\uff0call-to-all\u5feb20%\uff09\uff0c\u5927\u5c3a\u5bf8\u6027\u80fd\u8fdb\u4e00\u6b65\u6539\u55847%\uff0c\u529f\u8017\u8282\u77013-10%\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4f7fDMA\u96c6\u5408\u64cd\u4f5c\u66f4\u63a5\u8fd1\u4e3b\u6d41\u96c6\u5408\u5e93\u7684\u91c7\u7528\u6807\u51c6\uff0c\u901a\u8fc7\u4f18\u5316\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u548c\u529f\u8017\u8282\u7701\u3002"}}
{"id": "2511.06770", "categories": ["cs.AR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.06770", "abs": "https://arxiv.org/abs/2511.06770", "authors": ["Tamoghno Das", "Khanh Phan Vu", "Hanning Chen", "Hyunwoo Oh", "Mohsen Imani"], "title": "ASTER: Attention-based Spiking Transformer Engine for Event-driven Reasoning", "comment": "Submitted for review at conference", "summary": "The integration of spiking neural networks (SNNs) with transformer-based architectures has opened new opportunities for bio-inspired low-power, event-driven visual reasoning on edge devices. However, the high temporal resolution and binary nature of spike-driven computation introduce architectural mismatches with conventional digital hardware (CPU/GPU). Prior neuromorphic and Processing-in-Memory (PIM) accelerators struggle with high sparsity and complex operations prevalent in such models. To address these challenges, we propose a memory-centric hardware accelerator tailored for spiking transformers, optimized for deployment in real-time event-driven frameworks such as classification with both static and event-based input frames. Our design leverages a hybrid analog-digital PIM architecture with input sparsity optimizations, and a custom-designed dataflow to minimize memory access overhead and maximize data reuse under spatiotemporal sparsity, for compute and memory-efficient end-to-end execution of spiking transformers. We subsequently propose inference-time software optimizations for layer skipping, and timestep reduction, leveraging Bayesian Optimization with surrogate modeling to perform robust, efficient co-exploration of the joint algorithmic-microarchitectural design spaces under tight computational budgets. Evaluated on both image(ImageNet) and event-based (CIFAR-10 DVS, DVSGesture) classification, the accelerator achieves up to ~467x and ~1.86x energy reduction compared to edge GPU (Jetson Orin Nano) and previous PIM accelerators for spiking transformers, while maintaining competitive task accuracy on ImageNet dataset. This work enables a new class of intelligent ubiquitous edge AI, built using spiking transformer acceleration for low-power, real-time visual processing at the extreme edge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8109\u51b2\u53d8\u538b\u5668\u7684\u5185\u5b58\u4e2d\u5fc3\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u91c7\u7528\u6df7\u5408\u6a21\u62df-\u6570\u5b57PIM\u67b6\u6784\uff0c\u901a\u8fc7\u8f93\u5165\u7a00\u758f\u4f18\u5316\u548c\u5b9a\u5236\u6570\u636e\u6d41\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4f4e\u529f\u8017\u3001\u5b9e\u65f6\u89c6\u89c9\u5904\u7406\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u4e0e\u53d8\u538b\u5668\u67b6\u6784\u7ed3\u5408\u4e3a\u8fb9\u7f18\u8bbe\u5907\u63d0\u4f9b\u4e86\u751f\u7269\u542f\u53d1\u7684\u4f4e\u529f\u8017\u89c6\u89c9\u63a8\u7406\u673a\u4f1a\uff0c\u4f46\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u4e8c\u8fdb\u5236\u8ba1\u7b97\u7279\u6027\u4e0e\u4f20\u7edf\u6570\u5b57\u786c\u4ef6\u5b58\u5728\u67b6\u6784\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6a21\u62df-\u6570\u5b57PIM\u67b6\u6784\uff0c\u7ed3\u5408\u8f93\u5165\u7a00\u758f\u4f18\u5316\u548c\u5b9a\u5236\u6570\u636e\u6d41\uff0c\u6700\u5c0f\u5316\u5185\u5b58\u8bbf\u95ee\u5f00\u9500\uff1b\u63d0\u51fa\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u63a8\u7406\u65f6\u8f6f\u4ef6\u4f18\u5316\uff0c\u5305\u62ec\u5c42\u8df3\u8fc7\u548c\u65f6\u95f4\u6b65\u51cf\u5c11\u3002", "result": "\u5728\u56fe\u50cf\u548c\u4e8b\u4ef6\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u76f8\u6bd4\u8fb9\u7f18GPU\u548c\u5148\u524dPIM\u52a0\u901f\u5668\u5206\u522b\u5b9e\u73b0\u7ea6467\u500d\u548c1.86\u500d\u80fd\u8017\u964d\u4f4e\uff0c\u540c\u65f6\u5728ImageNet\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u7cbe\u5ea6\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u8109\u51b2\u53d8\u538b\u5668\u52a0\u901f\u5b9e\u73b0\u4e86\u65b0\u578b\u667a\u80fd\u8fb9\u7f18AI\uff0c\u4e3a\u6781\u7aef\u8fb9\u7f18\u7684\u4f4e\u529f\u8017\u5b9e\u65f6\u89c6\u89c9\u5904\u7406\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.06735", "categories": ["cs.DC", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.06735", "abs": "https://arxiv.org/abs/2511.06735", "authors": ["Raya Majid Alsharfa", "Mahmood Mohassel Feghhi", "Majid Hameed Majeed"], "title": "Wireless Sensor Networks Nodes Clustering and Optimization Based on Fuzzy C-Means and Water Strider Algorithms", "comment": "15 pages, 9 figures, Published in International Journal of Intelligent Engineering and Systems, 2025", "summary": "Wireless sensor networks (WSNs) face critical challenges in energy management and network lifetime optimization due to limited battery resources and communication overhead. This study introduces a novel hybrid clustering protocol that integrates the Water Strider Algorithm (WSA) with Fuzzy C-Means (FCM) clustering to achieve superior energy efficiency and network longevity. The proposed WSA-FCM method employs WSA for global optimization of cluster- head positions and FCM for refined node membership assignment with fuzzy boundaries. Through extensive experimentation across networks of 200-800 nodes with 10 independent simulation runs, the method demonstrates significant improvements: First Node Death (FND) delayed by 16.1% ($678\\pm12$ vs $584\\pm18$ rounds), Last Node Death (LND) extended by 11.9% ($1,262\\pm8$ vs $1,128\\pm11$ rounds), and 37.4% higher residual energy retention ($5.47\\pm0.09$ vs $3.98\\pm0.11$ J) compared to state-of-the-art hybrid methods. Intra-cluster distances are reduced by 19.4% with statistical significance (p < 0.001). Theoretical analysis proves convergence guarantees and complexity bounds of $O(n\\times c\\times T)$, while empirical scalability analysis demonstrates near-linear scaling behaviour. The method outperforms recent hybrid approaches including MOALO-FCM, MSSO-MST, Fuzzy+HHO, and GWO-FCM across all performance metrics with rigorous statistical validation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6c34\u9efe\u7b97\u6cd5\u548c\u6a21\u7ccaC\u5747\u503c\u805a\u7c7b\u7684\u6df7\u5408\u5206\u7c07\u534f\u8bae\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u7684\u80fd\u91cf\u6548\u7387\u548c\u7f51\u7edc\u5bff\u547d\u3002", "motivation": "\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u9762\u4e34\u80fd\u91cf\u7ba1\u7406\u548c\u7f51\u7edc\u5bff\u547d\u4f18\u5316\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u6709\u9650\u7684\u7535\u6c60\u8d44\u6e90\u548c\u901a\u4fe1\u5f00\u9500\u3002", "method": "\u4f7f\u7528\u6c34\u9efe\u7b97\u6cd5\u8fdb\u884c\u7c07\u5934\u4f4d\u7f6e\u7684\u5168\u5c40\u4f18\u5316\uff0c\u7ed3\u5408\u6a21\u7ccaC\u5747\u503c\u805a\u7c7b\u8fdb\u884c\u8282\u70b9\u6210\u5458\u5206\u914d\u7684\u7cbe\u7ec6\u5316\u5904\u7406\uff0c\u91c7\u7528\u6a21\u7cca\u8fb9\u754c\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6df7\u5408\u65b9\u6cd5\uff0c\u9996\u8282\u70b9\u6b7b\u4ea1\u5ef6\u8fdf16.1%\uff0c\u672b\u8282\u70b9\u6b7b\u4ea1\u5ef6\u957f11.9%\uff0c\u5269\u4f59\u80fd\u91cf\u4fdd\u7559\u63d0\u9ad837.4%\uff0c\u7c07\u5185\u8ddd\u79bb\u51cf\u5c1119.4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u80fd\u91cf\u6548\u7387\u3001\u7f51\u7edc\u5bff\u547d\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6df7\u5408\u65b9\u6cd5\uff0c\u5177\u6709\u6536\u655b\u4fdd\u8bc1\u548c\u8fd1\u7ebf\u6027\u6269\u5c55\u7279\u6027\u3002"}}
{"id": "2511.06838", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.06838", "abs": "https://arxiv.org/abs/2511.06838", "authors": ["Yuzong Chen", "Chao Fang", "Xilai Dai", "Yuheng Wu", "Thierry Tambe", "Marian Verhelst", "Mohamed S. Abdelfattah"], "title": "P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats", "comment": "Preprint. Under review", "summary": "The substantial memory bandwidth and computational demand of large language models (LLMs) present critical challenges for efficient inference. To tackle this, the literature has explored heterogeneous systems that combine neural processing units (NPUs) with DRAM-based processing-in-memory (PIM) for LLM acceleration. However, existing high-precision (e.g., FP16) PIM compute units incur significant area and power overhead in DRAM technology, limiting the effective computation throughput. In this paper, we introduce P3-LLM, a novel NPU-PIM integrated accelerator for LLM inference using hybrid numerical formats. Our approach is threefold: First, we propose a flexible mixed-precision quantization scheme, which leverages hybrid numerical formats to quantize different LLM operands with high compression efficiency and minimal accuracy loss. Second, we architect an efficient PIM accelerator co-design for P3-LLM, featuring lightweight compute units to support our hybrid numerical formats. The enhanced PIM compute units significantly boost the computation throughput under iso-area constraints. Third, we optimize the low-precision dataflow of different LLM modules by applying operator fusion to minimize the overhead of runtime dequantization. Our evaluation on a diverse set of representative LLMs and tasks demonstrates that P3-LLM achieves state-of-the-art quantization accuracy in terms of both KV-cache-only quantization and weight-activation quantization. Combining the proposed quantization scheme with PIM architecture co-design, P3-LLM yields an average of $4.9\\times$, $2.0\\times$, and $3.4\\times$ speedups over the state-of-the-art LLM accelerators HBM-PIM, Ecco, and Pimba, respectively. Our quantization code is available at https://github.com/yc2367/P3-LLM.git", "AI": {"tldr": "P3-LLM\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684NPU-PIM\u96c6\u6210\u52a0\u901f\u5668\uff0c\u91c7\u7528\u6df7\u5408\u6570\u503c\u683c\u5f0f\u548c\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8ba1\u7b97\u5355\u5143\u548c\u7b97\u5b50\u878d\u5408\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u9ad8\u7cbe\u5ea6PIM\u8ba1\u7b97\u5355\u5143\u5728DRAM\u6280\u672f\u4e2d\u4ea7\u751f\u663e\u8457\u7684\u9762\u79ef\u548c\u529f\u8017\u5f00\u9500\uff0c\u9650\u5236\u4e86\u6709\u6548\u7684\u8ba1\u7b97\u541e\u5410\u91cf\uff0c\u9700\u8981\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u5e26\u5bbd\u548c\u8ba1\u7b97\u9700\u6c42\u6311\u6218\u3002", "method": "1. \u63d0\u51fa\u7075\u6d3b\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6848\uff0c\u4f7f\u7528\u6df7\u5408\u6570\u503c\u683c\u5f0f\u91cf\u5316\u4e0d\u540cLLM\u64cd\u4f5c\u6570\uff1b2. \u8bbe\u8ba1\u9ad8\u6548\u7684PIM\u52a0\u901f\u5668\u534f\u540c\u8bbe\u8ba1\uff0c\u652f\u6301\u6df7\u5408\u6570\u503c\u683c\u5f0f\u7684\u8f7b\u91cf\u7ea7\u8ba1\u7b97\u5355\u5143\uff1b3. \u901a\u8fc7\u7b97\u5b50\u878d\u5408\u4f18\u5316\u4f4e\u7cbe\u5ea6\u6570\u636e\u6d41\uff0c\u51cf\u5c11\u8fd0\u884c\u65f6\u53cd\u91cf\u5316\u5f00\u9500\u3002", "result": "\u5728\u4ee3\u8868\u6027LLM\u548c\u4efb\u52a1\u4e0a\uff0cP3-LLM\u5728KV\u7f13\u5b58\u91cf\u5316\u548c\u6743\u91cd\u6fc0\u6d3b\u91cf\u5316\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u91cf\u5316\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u73b0\u6709LLM\u52a0\u901f\u5668HBM-PIM\u3001Ecco\u548cPimba\u5206\u522b\u5b9e\u73b04.9\u500d\u30012.0\u500d\u548c3.4\u500d\u7684\u5e73\u5747\u52a0\u901f\u3002", "conclusion": "P3-LLM\u901a\u8fc7\u6df7\u5408\u6570\u503c\u683c\u5f0f\u548cPIM\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u5e26\u5bbd\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2511.06824", "categories": ["cs.DC", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.06824", "abs": "https://arxiv.org/abs/2511.06824", "authors": ["Xin Yao", "Yang Liu", "Jin Jiang", "Yesen Chen", "Zhilong Chen", "Hongkang Dong", "Xiaofeng Wei", "Teng Zhang", "Dongyun Wang"], "title": "A GPU-boosted high-performance multi-working condition joint analysis framework for predicting dynamics of textured axial piston pump", "comment": null, "summary": "Accurate simulation to dynamics of axial piston pump (APP) is essential for its design, manufacture and maintenance. However, limited by computation capacity of CPU device and traditional solvers, conventional iteration methods are inefficient in complicated case with textured surface requiring refined mesh, and could not handle simulation during multiple periods. To accelerate Picard iteration for predicting dynamics of APP, a GPU-boosted high-performance Multi-working condition joint Analysis Framework (GMAF) is designed, which adopts Preconditioned Conjugate Gradient method (PCG) using Approximate Symmetric Successive Over-Relaxation preconditioner (ASSOR). GMAF abundantly utilizes GPU device via elevating computational intensity and expanding scale of massive parallel computation. Therefore, it possesses novel performance in analyzing dynamics of both smooth and textured APPs during multiple periods, as the establishment and solution to joint algebraic system for pressure field are accelerated magnificently, as well as numerical integral for force and moment due to oil flow. Compared with asynchronized convergence strategy pursuing local convergence, synchronized convergence strategy targeting global convergence is adopted in PCG solver for the joint system. Revealed by corresponding results, oil force in axial direction and moment in circumferential directly respond to input pressure, while other components evolve in sinusoidal patterns. Specifically, force and moment due to normal pressure instantly reach their steady state initially, while ones due to viscous shear stress evolve during periods. After simulating dynamics of APP and pressure distribution via GMAF, the promotion of pressure capacity and torsion resistance due to textured surface is revealed numerically, as several 'steps' exist in the pressure field corresponding to textures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u52a0\u901f\u7684\u9ad8\u6027\u80fd\u591a\u5de5\u51b5\u8054\u5408\u5206\u6790\u6846\u67b6\uff08GMAF\uff09\uff0c\u7528\u4e8e\u5feb\u901f\u6a21\u62df\u8f74\u5411\u67f1\u585e\u6cf5\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5177\u6709\u7eb9\u7406\u8868\u9762\u7684\u590d\u6742\u60c5\u51b5\u3002", "motivation": "\u4f20\u7edfCPU\u8fed\u4ee3\u65b9\u6cd5\u5728\u5904\u7406\u9700\u8981\u7cbe\u7ec6\u7f51\u683c\u7684\u7eb9\u7406\u8868\u9762\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u65e0\u6cd5\u8fdb\u884c\u591a\u5468\u671f\u6a21\u62df\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6c42\u89e3\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86GMAF\u6846\u67b6\uff0c\u91c7\u7528\u9884\u6761\u4ef6\u5171\u8f6d\u68af\u5ea6\u6cd5\uff08PCG\uff09\u548c\u8fd1\u4f3c\u5bf9\u79f0\u9010\u6b21\u8d85\u677e\u5f1b\u9884\u5904\u7406\u5668\uff08ASSOR\uff09\uff0c\u5145\u5206\u5229\u7528GPU\u7684\u8ba1\u7b97\u5f3a\u5ea6\u548c\u5e76\u884c\u8ba1\u7b97\u80fd\u529b\u3002", "result": "GMAF\u5728\u6a21\u62df\u5149\u6ed1\u548c\u7eb9\u7406\u8868\u9762\u8f74\u5411\u67f1\u585e\u6cf5\u7684\u591a\u5468\u671f\u52a8\u529b\u5b66\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u52a0\u901f\u4e86\u538b\u529b\u573a\u8054\u5408\u4ee3\u6570\u7cfb\u7edf\u7684\u5efa\u7acb\u548c\u6c42\u89e3\uff0c\u4ee5\u53ca\u6cb9\u6d41\u529b\u548c\u529b\u77e9\u7684\u6570\u503c\u79ef\u5206\u3002", "conclusion": "\u7eb9\u7406\u8868\u9762\u80fd\u591f\u63d0\u5347\u538b\u529b\u627f\u8f7d\u80fd\u529b\u548c\u6297\u626d\u6027\u80fd\uff0c\u538b\u529b\u573a\u4e2d\u51fa\u73b0\u4e0e\u7eb9\u7406\u5bf9\u5e94\u7684'\u53f0\u9636'\u7ed3\u6784\uff0c\u8bc1\u660e\u4e86\u7eb9\u7406\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.06907", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.06907", "abs": "https://arxiv.org/abs/2511.06907", "authors": ["Ilias Papalamprou", "Dimosthenis Masouros", "Ioannis Loudaros", "Francky Catthoor", "Dimitrios Soudris"], "title": "Optimizing GEMM for Energy and Performance on Versal ACAP Architectures", "comment": null, "summary": "General Matrix Multiplication (GEMM) is a fundamental operation in many scientific workloads, signal processing, and particularly deep learning. It is often a bottleneck for performance and energy efficiency, especially in edge environments with tight resource and power constraints. AMD's Versal ACAP offers heterogeneous components (AIEs, PL, PS) that can address these challenges, but mapping GEMM across them is complex, with prior works largely overlooking energy-performance trade-offs. In this paper, we propose an automated framework for Versal ACAP that generates GEMM mappings optimized for either performance or energy efficiency. Unlike prior analytical approaches, our method leverages a Machine Learning (ML) model, trained on approximately 6000 on-board experiments of different GEMM mappings, to guide Design Space Exploration, yielding more efficient designs. Evaluation on the Versal VCK190 shows geomean improvements of 1.23x (up to 2.5x) in throughput and 1.25x (up to 2.7x) in energy efficiency over state-of-the-art frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9AMD Versal ACAP\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f18\u5316GEMM\u6620\u5c04\uff0c\u5728\u6027\u80fd\u548c\u80fd\u6548\u65b9\u9762\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "GEMM\u662f\u6df1\u5ea6\u5b66\u4e60\u548c\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u5173\u952e\u64cd\u4f5c\uff0c\u5728\u8fb9\u7f18\u73af\u5883\u4e2d\u53d7\u9650\u4e8e\u6027\u80fd\u548c\u80fd\u6548\u3002AMD Versal ACAP\u7684\u5f02\u6784\u67b6\u6784\u867d\u80fd\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46GEMM\u6620\u5c04\u590d\u6742\u4e14\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u80fd\u6548-\u6027\u80fd\u6743\u8861\u7684\u8003\u8651\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5229\u7528\u5728\u7ea66000\u4e2a\u677f\u4e0a\u5b9e\u9a8c\u6570\u636e\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u6307\u5bfc\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u751f\u6210\u9488\u5bf9\u6027\u80fd\u6216\u80fd\u6548\u4f18\u5316\u7684GEMM\u6620\u5c04\u3002", "result": "\u5728Versal VCK190\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u6846\u67b6\uff0c\u541e\u5410\u91cf\u51e0\u4f55\u5e73\u5747\u63d0\u53471.23\u500d\uff08\u6700\u9ad82.5\u500d\uff09\uff0c\u80fd\u6548\u51e0\u4f55\u5e73\u5747\u63d0\u53471.25\u500d\uff08\u6700\u9ad82.7\u500d\uff09\u3002", "conclusion": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u6846\u67b6\u80fd\u6709\u6548\u4f18\u5316Versal ACAP\u4e0a\u7684GEMM\u6620\u5c04\uff0c\u5728\u6027\u80fd\u548c\u80fd\u6548\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2511.07202", "categories": ["cs.DC", "cs.AI", "cs.MA", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.07202", "abs": "https://arxiv.org/abs/2511.07202", "authors": ["Praveen Kumar Donta", "Alfreds Lapkovskis", "Enzo Mingozzi", "Schahram Dustdar"], "title": "Resilient by Design - Active Inference for Distributed Continuum Intelligence", "comment": null, "summary": "Failures are the norm in highly complex and heterogeneous devices spanning the distributed computing continuum (DCC), from resource-constrained IoT and edge nodes to high-performance computing systems. Ensuring reliability and global consistency across these layers remains a major challenge, especially for AI-driven workloads requiring real-time, adaptive coordination. This paper introduces a Probabilistic Active Inference Resilience Agent (PAIR-Agent) to achieve resilience in DCC systems. PAIR-Agent performs three core operations: (i) constructing a causal fault graph from device logs, (ii) identifying faults while managing certainties and uncertainties using Markov blankets and the free-energy principle, and (iii) autonomously healing issues through active inference. Through continuous monitoring and adaptive reconfiguration, the agent maintains service continuity and stability under diverse failure conditions. Theoretical validations confirm the reliability and effectiveness of the proposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u7387\u4e3b\u52a8\u63a8\u7406\u5f39\u6027\u4ee3\u7406(PAIR-Agent)\uff0c\u7528\u4e8e\u5728\u5206\u5e03\u5f0f\u8ba1\u7b97\u8fde\u7eed\u4f53\u4e2d\u5b9e\u73b0\u7cfb\u7edf\u5f39\u6027\uff0c\u901a\u8fc7\u6784\u5efa\u56e0\u679c\u6545\u969c\u56fe\u3001\u7ba1\u7406\u786e\u5b9a\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u81ea\u4e3b\u4fee\u590d\u6765\u5e94\u5bf9\u590d\u6742\u5f02\u6784\u8bbe\u5907\u4e2d\u7684\u6545\u969c\u3002", "motivation": "\u5728\u9ad8\u5ea6\u590d\u6742\u548c\u5f02\u6784\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u8fde\u7eed\u4f53\u8bbe\u5907\u4e2d\uff0c\u6545\u969c\u662f\u5e38\u6001\uff0c\u786e\u4fdd\u8de8\u5c42\u7684\u53ef\u9760\u6027\u548c\u5168\u5c40\u4e00\u81f4\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u5b9e\u65f6\u81ea\u9002\u5e94\u534f\u8c03\u7684AI\u9a71\u52a8\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "PAIR-Agent\u6267\u884c\u4e09\u4e2a\u6838\u5fc3\u64cd\u4f5c\uff1a(i)\u4ece\u8bbe\u5907\u65e5\u5fd7\u6784\u5efa\u56e0\u679c\u6545\u969c\u56fe\uff0c(ii)\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u6bef\u548c\u81ea\u7531\u80fd\u539f\u7406\u8bc6\u522b\u6545\u969c\u5e76\u7ba1\u7406\u786e\u5b9a\u6027\u4e0e\u4e0d\u786e\u5b9a\u6027\uff0c(iii)\u901a\u8fc7\u4e3b\u52a8\u63a8\u7406\u81ea\u4e3b\u4fee\u590d\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u6301\u7eed\u76d1\u63a7\u548c\u81ea\u9002\u5e94\u91cd\u65b0\u914d\u7f6e\uff0c\u8be5\u4ee3\u7406\u5728\u5404\u79cd\u6545\u969c\u6761\u4ef6\u4e0b\u4fdd\u6301\u670d\u52a1\u8fde\u7eed\u6027\u548c\u7a33\u5b9a\u6027\u3002\u7406\u8bba\u9a8c\u8bc1\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "PAIR-Agent\u6846\u67b6\u4e3a\u5206\u5e03\u5f0f\u8ba1\u7b97\u8fde\u7eed\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5f39\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u81ea\u4e3b\u5e94\u5bf9\u590d\u6742\u6545\u969c\u573a\u666f\uff0c\u786e\u4fdd\u7cfb\u7edf\u53ef\u9760\u8fd0\u884c\u3002"}}
{"id": "2511.06955", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.06955", "abs": "https://arxiv.org/abs/2511.06955", "authors": ["Arya Parameshwara", "Santosh Hanamappa Mokashi"], "title": "FPGA-Accelerated RISC-V ISA Extensions for Efficient Neural Network Inference on Edge Devices", "comment": "12 pages, 7 figures. Includes complete FPGA implementation on PYNQ-Z2 platform with hardware-validated results. Target applications: industrial inspection, agricultural sensing, warehouse robotics, and remote monitoring. Code and bitstreams available at https://github.com/aryapkar/fpga-riscv-nn-extensions", "summary": "Edge AI deployment faces critical challenges balancing computational performance, energy efficiency, and resource constraints. This paper presents FPGA-accelerated RISC-V instruction set architecture (ISA) extensions for efficient neural network inference on resource-constrained edge devices. We introduce a custom RISC-V core with four novel ISA extensions (FPGA.VCONV, FPGA.GEMM, FPGA.RELU, FPGA.CUSTOM) and integrated neural network accelerators, implemented and validated on the Xilinx PYNQ-Z2 platform. The complete system achieves 2.14x average latency speedup and 49.1% energy reduction versus an ARM Cortex-A9 software baseline across four benchmark models (MobileNet V2, ResNet-18, EfficientNet Lite, YOLO Tiny). Hardware implementation closes timing with +12.793 ns worst negative slack at 50 MHz while using 0.43% LUTs and 11.4% BRAM for the base core and 38.8% DSPs when accelerators are active. Hardware verification confirms successful FPGA deployment with verified 64 KB BRAM memory interface and AXI interconnect functionality. All performance metrics are obtained from physical hardware measurements. This work establishes a reproducible framework for ISA-guided FPGA acceleration that complements fixed-function ASICs by trading peak performance for programmability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FPGA\u52a0\u901f\u7684RISC-V ISA\u6269\u5c55\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff0c\u5728Xilinx PYNQ-Z2\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e862.14\u500d\u7684\u5e73\u5747\u5ef6\u8fdf\u52a0\u901f\u548c49.1%\u7684\u80fd\u8017\u964d\u4f4e\u3002", "motivation": "\u8fb9\u7f18AI\u90e8\u7f72\u9762\u4e34\u8ba1\u7b97\u6027\u80fd\u3001\u80fd\u6548\u548c\u8d44\u6e90\u7ea6\u675f\u4e4b\u95f4\u7684\u5e73\u8861\u6311\u6218\uff0c\u9700\u8981\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u5f00\u53d1\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5b9a\u5236\u7684RISC-V\u6838\u5fc3\uff0c\u5305\u542b\u56db\u4e2a\u65b0\u9896\u7684ISA\u6269\u5c55\uff08FPGA.VCONV\u3001FPGA.GEMM\u3001FPGA.RELU\u3001FPGA.CUSTOM\uff09\u548c\u96c6\u6210\u7684\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\uff0c\u5728Xilinx PYNQ-Z2\u5e73\u53f0\u4e0a\u5b9e\u73b0\u548c\u9a8c\u8bc1\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6a21\u578b\uff08MobileNet V2\u3001ResNet-18\u3001EfficientNet Lite\u3001YOLO Tiny\uff09\u4e0a\uff0c\u76f8\u6bd4ARM Cortex-A9\u8f6f\u4ef6\u57fa\u51c6\uff0c\u5b9e\u73b0\u4e862.14\u500d\u5e73\u5747\u5ef6\u8fdf\u52a0\u901f\u548c49.1%\u80fd\u8017\u964d\u4f4e\u3002\u786c\u4ef6\u5b9e\u73b0\u65f6\u949f\u9891\u738750MHz\uff0c\u4f7f\u75280.43% LUTs\u548c11.4% BRAM\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684ISA\u5f15\u5bfcFPGA\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u727a\u7272\u5cf0\u503c\u6027\u80fd\u6362\u53d6\u53ef\u7f16\u7a0b\u6027\uff0c\u8865\u5145\u4e86\u56fa\u5b9a\u529f\u80fd\u7684ASIC\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.07229", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.07229", "abs": "https://arxiv.org/abs/2511.07229", "authors": ["Jaehong Cho", "Hyunmin Choi", "Jongse Park"], "title": "LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and Serving Techniques in LLM Infrastructure", "comment": "4 pages, 3 figures", "summary": "This paper introduces LLMServingSim2.0, a system simulator designed for exploring heterogeneous hardware in large-scale LLM serving systems. LLMServingSim2.0 addresses two key limitations of its predecessor: (1) integrating hardware models into system-level simulators is non-trivial due to the lack of a clear abstraction, and (2) existing simulators support only a narrow subset of serving techniques, leaving no infrastructure that captures the breadth of approaches in modern LLM serving. To overcome these issues, LLMServingSim2.0 adopts trace-driven performance modeling, accompanied by an operator-level latency profiler, enabling the integration of new accelerators with a single command. It further embeds up-to-date serving techniques while exposing flexible interfaces for request routing, cache management, and scheduling policies. In a TPU case study, our profiler requires 18.5x fewer LoC and outperforms the predecessor's hardware-simulator integration, demonstrating LLMServingSim2.0's low-effort hardware extensibility. Our experiments further show that LLMServingSim2.0 reproduces GPU-based LLM serving with 1.9% error, while maintaining practical simulation time, making it a comprehensive platform for both hardware developers and LLM service providers.", "AI": {"tldr": "LLMServingSim2.0\u662f\u4e00\u4e2a\u7528\u4e8e\u63a2\u7d22\u5927\u89c4\u6a21LLM\u670d\u52a1\u7cfb\u7edf\u4e2d\u5f02\u6784\u786c\u4ef6\u7684\u7cfb\u7edf\u6a21\u62df\u5668\uff0c\u89e3\u51b3\u4e86\u786c\u4ef6\u6a21\u578b\u96c6\u6210\u56f0\u96be\u548c\u73b0\u6709\u6a21\u62df\u5668\u652f\u6301\u6280\u672f\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u524d\u4ee3\u7cfb\u7edf\u7684\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a(1) \u7531\u4e8e\u7f3a\u4e4f\u6e05\u6670\u7684\u62bd\u8c61\uff0c\u5c06\u786c\u4ef6\u6a21\u578b\u96c6\u6210\u5230\u7cfb\u7edf\u7ea7\u6a21\u62df\u5668\u4e2d\u975e\u5e38\u56f0\u96be\uff1b(2) \u73b0\u6709\u6a21\u62df\u5668\u4ec5\u652f\u6301\u4e00\u5c0f\u90e8\u5206\u670d\u52a1\u6280\u672f\uff0c\u7f3a\u4e4f\u80fd\u591f\u6355\u6349\u73b0\u4ee3LLM\u670d\u52a1\u5e7f\u6cdb\u65b9\u6cd5\u7684\u5de5\u5177\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8f68\u8ff9\u7684\u6027\u80fd\u5efa\u6a21\uff0c\u914d\u5907\u7b97\u5b50\u7ea7\u5ef6\u8fdf\u5206\u6790\u5668\uff0c\u652f\u6301\u901a\u8fc7\u5355\u4e00\u547d\u4ee4\u96c6\u6210\u65b0\u52a0\u901f\u5668\u3002\u5d4c\u5165\u6700\u65b0\u7684\u670d\u52a1\u6280\u672f\uff0c\u540c\u65f6\u63d0\u4f9b\u7075\u6d3b\u7684\u8def\u7531\u3001\u7f13\u5b58\u7ba1\u7406\u548c\u8c03\u5ea6\u7b56\u7565\u63a5\u53e3\u3002", "result": "\u5728TPU\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u5206\u6790\u5668\u9700\u8981\u51cf\u5c1118.5\u500d\u7684\u4ee3\u7801\u884c\u6570\uff0c\u6027\u80fd\u4f18\u4e8e\u524d\u4ee3\u7684\u786c\u4ef6\u6a21\u62df\u5668\u96c6\u6210\u3002\u5b9e\u9a8c\u663e\u793aLLMServingSim2.0\u80fd\u591f\u4ee51.9%\u7684\u8bef\u5dee\u91cd\u73b0\u57fa\u4e8eGPU\u7684LLM\u670d\u52a1\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u7684\u6a21\u62df\u65f6\u95f4\u3002", "conclusion": "LLMServingSim2.0\u662f\u4e00\u4e2a\u5168\u9762\u7684\u5e73\u53f0\uff0c\u4e3a\u786c\u4ef6\u5f00\u53d1\u8005\u548cLLM\u670d\u52a1\u63d0\u4f9b\u5546\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u7684\u786c\u4ef6\u6269\u5c55\u6027\u548c\u51c6\u786e\u7684\u6027\u80fd\u6a21\u62df\u80fd\u529b\u3002"}}
