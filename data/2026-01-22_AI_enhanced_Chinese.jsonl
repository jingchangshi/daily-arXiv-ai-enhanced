{"id": "2601.15167", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.15167", "abs": "https://arxiv.org/abs/2601.15167", "authors": ["Francesca Randone", "Romina Doz", "Mirco Tribastone", "Luca Bortolussi"], "title": "DeGAS: Gradient-Based Optimization of Probabilistic Programs without Sampling", "comment": null, "summary": "We present DeGAS, a differentiable Gaussian approximate semantics for loopless probabilistic programs that enables sample-free, gradient-based optimization in models with both continuous and discrete components. DeGAS evaluates programs under a Gaussian-mixture semantics and replaces measure-zero predicates and discrete branches with a vanishing smoothing, yielding closed-form expressions for posterior and path probabilities. We prove differentiability of these quantities with respect to program parameters, enabling end-to-end optimization via standard automatic differentiation, without Monte Carlo estimators. On thirteen benchmark programs, DeGAS achieves accuracy and runtime competitive with variational inference and MCMC. Importantly, it reliably tackles optimization problems where sampling-based baselines fail to converge due to conditioning involving continuous variables.", "AI": {"tldr": "DeGAS \u662f\u4e00\u79cd\u7528\u4e8e\u65e0\u5faa\u73af\u6982\u7387\u7a0b\u5e8f\u7684\u53ef\u5fae\u5206\u9ad8\u65af\u8fd1\u4f3c\u8bed\u4e49\uff0c\u652f\u6301\u5728\u5305\u542b\u8fde\u7eed\u548c\u79bb\u6563\u7ec4\u4ef6\u7684\u6a21\u578b\u4e2d\u8fdb\u884c\u65e0\u6837\u672c\u3001\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\uff08\u5982MCMC\uff09\u5728\u6d89\u53ca\u8fde\u7eed\u53d8\u91cf\u7684\u6761\u4ef6\u4f18\u5316\u95ee\u9898\u4e0a\u53ef\u80fd\u6536\u655b\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u6df7\u5408\u8fde\u7eed-\u79bb\u6563\u53d8\u91cf\u4e14\u652f\u6301\u68af\u5ea6\u4f18\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u8bed\u4e49\u8bc4\u4f30\u7a0b\u5e8f\uff0c\u5c06\u6d4b\u5ea6\u96f6\u8c13\u8bcd\u548c\u79bb\u6563\u5206\u652f\u66ff\u6362\u4e3a\u6e10\u8fd1\u5e73\u6ed1\uff0c\u5f97\u5230\u540e\u9a8c\u6982\u7387\u548c\u8def\u5f84\u6982\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u8bc1\u660e\u8fd9\u4e9b\u91cf\u5bf9\u7a0b\u5e8f\u53c2\u6570\u7684\u53ef\u5fae\u6027\uff0c\u4ece\u800c\u652f\u6301\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u5fae\u5206\u4f18\u5316\u3002", "result": "\u572813\u4e2a\u57fa\u51c6\u7a0b\u5e8f\u4e0a\uff0cDeGAS\u7684\u51c6\u786e\u6027\u548c\u8fd0\u884c\u65f6\u95f4\u4e0e\u53d8\u5206\u63a8\u65ad\u548cMCMC\u76f8\u5f53\uff0c\u5e76\u80fd\u53ef\u9760\u89e3\u51b3\u57fa\u4e8e\u91c7\u6837\u7684\u57fa\u7ebf\u65b9\u6cd5\u56e0\u8fde\u7eed\u53d8\u91cf\u6761\u4ef6\u800c\u65e0\u6cd5\u6536\u655b\u7684\u4f18\u5316\u95ee\u9898\u3002", "conclusion": "DeGAS\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u6837\u672c\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5305\u542b\u8fde\u7eed\u548c\u79bb\u6563\u7ec4\u4ef6\u7684\u6982\u7387\u7a0b\u5e8f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u5728\u6761\u4ef6\u4f18\u5316\u4e2d\u7684\u6536\u655b\u95ee\u9898\u3002"}}
{"id": "2601.15180", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.15180", "abs": "https://arxiv.org/abs/2601.15180", "authors": ["Pedro \u00c2ngelo", "Atsushi Igarashi", "Yuito Murase", "Vasco T. Vasconcelos"], "title": "Contextual Metaprogramming for Session Types", "comment": "36 pages, 14 figures, ESOP 2026", "summary": "We propose the integration of staged metaprogramming into a session-typed message passing functional language. We build on a model of contextual modal type theory with multi-level contexts, where contextual values, closing arbitrary terms over a series of variables, may be boxed and transmitted in messages. Once received, one such value may then be unboxed and locally applied before being run. To motivate this integration, we present examples of real-world use cases, for which our system would be suitable, such as servers preparing and shipping code on demand via session typed messages. We present a type system that distinguishes linear (used exactly once) from unrestricted (used an unbounded number of times) resources, and further define a type checker, suitable for a concrete implementation. We show type preservation, a progress result for sequential computations and absence of runtime errors for the concurrent runtime environment, as well as the correctness of the type checker.", "AI": {"tldr": "\u5c06\u5206\u9636\u6bb5\u5143\u7f16\u7a0b\u96c6\u6210\u5230\u4f1a\u8bdd\u7c7b\u578b\u7684\u6d88\u606f\u4f20\u9012\u51fd\u6570\u5f0f\u8bed\u8a00\u4e2d\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u6a21\u6001\u7c7b\u578b\u7406\u8bba\u548c\u591a\u7ea7\u4e0a\u4e0b\u6587\u5b9e\u73b0\u4ee3\u7801\u7684\u6253\u5305\u3001\u4f20\u8f93\u548c\u672c\u5730\u6267\u884c\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u670d\u52a1\u5668\u901a\u8fc7\u4f1a\u8bdd\u7c7b\u578b\u6d88\u606f\u6309\u9700\u51c6\u5907\u548c\u53d1\u9001\u4ee3\u7801\u7684\u9700\u6c42\uff0c\u63d0\u4f9b\u4e00\u79cd\u5b89\u5168\u53ef\u9760\u7684\u65b9\u5f0f\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u4f20\u8f93\u548c\u6267\u884c\u4ee3\u7801\u3002", "method": "\u57fa\u4e8e\u591a\u7ea7\u4e0a\u4e0b\u6587\u7684\u4e0a\u4e0b\u6587\u6a21\u6001\u7c7b\u578b\u7406\u8bba\u6a21\u578b\uff0c\u5176\u4e2d\u4e0a\u4e0b\u6587\u503c\uff08\u5305\u542b\u4efb\u610f\u9879\u5728\u4e00\u7cfb\u5217\u53d8\u91cf\u4e0a\u7684\u95ed\u5305\uff09\u53ef\u4ee5\u88ab\u6253\u5305\u5e76\u5728\u6d88\u606f\u4e2d\u4f20\u8f93\u3002\u63a5\u6536\u540e\u53ef\u4ee5\u89e3\u5305\u5e76\u5728\u672c\u5730\u5e94\u7528\u6267\u884c\u3002\u7c7b\u578b\u7cfb\u7edf\u533a\u5206\u7ebf\u6027\u8d44\u6e90\uff08\u6070\u597d\u4f7f\u7528\u4e00\u6b21\uff09\u548c\u65e0\u9650\u5236\u8d44\u6e90\uff08\u65e0\u9650\u6b21\u4f7f\u7528\uff09\u3002", "result": "\u5b9e\u73b0\u4e86\u7c7b\u578b\u4fdd\u6301\u6027\u3001\u987a\u5e8f\u8ba1\u7b97\u7684\u8fdb\u5c55\u7ed3\u679c\u3001\u5e76\u53d1\u8fd0\u884c\u65f6\u73af\u5883\u7684\u65e0\u8fd0\u884c\u65f6\u9519\u8bef\uff0c\u4ee5\u53ca\u7c7b\u578b\u68c0\u67e5\u5668\u7684\u6b63\u786e\u6027\u8bc1\u660e\u3002", "conclusion": "\u6210\u529f\u5c06\u5206\u9636\u6bb5\u5143\u7f16\u7a0b\u96c6\u6210\u5230\u4f1a\u8bdd\u7c7b\u578b\u6d88\u606f\u4f20\u9012\u8bed\u8a00\u4e2d\uff0c\u4e3a\u5206\u5e03\u5f0f\u4ee3\u7801\u4f20\u8f93\u548c\u6267\u884c\u63d0\u4f9b\u4e86\u7c7b\u578b\u5b89\u5168\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u670d\u52a1\u5668\u6309\u9700\u51c6\u5907\u548c\u53d1\u9001\u4ee3\u7801\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2601.14260", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14260", "abs": "https://arxiv.org/abs/2601.14260", "authors": ["Xiaoxuan Yang", "Peilin Chen", "Tergel Molom-Ochir", "Yiran Chen"], "title": "End-to-End Transformer Acceleration Through Processing-in-Memory Architectures", "comment": "ICM 2025", "summary": "Transformers have become central to natural language processing and large language models, but their deployment at scale faces three major challenges. First, the attention mechanism requires massive matrix multiplications and frequent movement of intermediate results between memory and compute units, leading to high latency and energy costs. Second, in long-context inference, the key-value cache (KV cache) can grow unpredictably and even surpass the model's weight size, creating severe memory and bandwidth bottlenecks. Third, the quadratic complexity of attention with respect to sequence length amplifies both data movement and compute overhead, making large-scale inference inefficient. To address these issues, this work introduces processing-in-memory solutions that restructure attention and feed-forward computation to minimize off-chip data transfers, dynamically compress and prune the KV cache to manage memory growth, and reinterpret attention as an associative memory operation to reduce complexity and hardware footprint. Moreover, we evaluate our processing-in-memory design against state-of-the-art accelerators and general-purpose GPUs, demonstrating significant improvements in energy efficiency and latency. Together, these approaches address computation overhead, memory scalability, and attention complexity, further enabling efficient, end-to-end acceleration of Transformer models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u5b58\u5185\u8ba1\u7b97\u7684Transformer\u52a0\u901f\u65b9\u6848\uff0c\u901a\u8fc7\u91cd\u6784\u6ce8\u610f\u529b\u673a\u5236\u3001\u52a8\u6001\u538b\u7f29KV\u7f13\u5b58\u3001\u5c06\u6ce8\u610f\u529b\u91cd\u65b0\u89e3\u91ca\u4e3a\u5173\u8054\u8bb0\u5fc6\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\u548c\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "Transformer\u5728\u5927\u89c4\u6a21\u90e8\u7f72\u65f6\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u6ce8\u610f\u529b\u673a\u5236\u9700\u8981\u5927\u91cf\u77e9\u9635\u4e58\u6cd5\u548c\u4e2d\u95f4\u7ed3\u679c\u5728\u5185\u5b58\u4e0e\u8ba1\u7b97\u5355\u5143\u95f4\u7684\u9891\u7e41\u79fb\u52a8\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u9ad8\u80fd\u8017\uff1b\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2dKV\u7f13\u5b58\u53ef\u80fd\u8d85\u8fc7\u6a21\u578b\u6743\u91cd\u5927\u5c0f\u9020\u6210\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\uff1b\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u52a0\u5267\u4e86\u6570\u636e\u79fb\u52a8\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u91c7\u7528\u5b58\u5185\u8ba1\u7b97\u65b9\u6848\uff1a1\uff09\u91cd\u6784\u6ce8\u610f\u529b\u548c\u524d\u9988\u8ba1\u7b97\u4ee5\u6700\u5c0f\u5316\u7247\u5916\u6570\u636e\u4f20\u8f93\uff1b2\uff09\u52a8\u6001\u538b\u7f29\u548c\u526a\u679dKV\u7f13\u5b58\u4ee5\u7ba1\u7406\u5185\u5b58\u589e\u957f\uff1b3\uff09\u5c06\u6ce8\u610f\u529b\u91cd\u65b0\u89e3\u91ca\u4e3a\u5173\u8054\u8bb0\u5fc6\u64cd\u4f5c\u4ee5\u964d\u4f4e\u590d\u6742\u5ea6\u548c\u786c\u4ef6\u5360\u7528\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u52a0\u901f\u5668\u548c\u901a\u7528GPU\u76f8\u6bd4\uff0c\u8be5\u5b58\u5185\u8ba1\u7b97\u8bbe\u8ba1\u5728\u80fd\u6548\u548c\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8fd9\u4e9b\u65b9\u6cd5\u5171\u540c\u89e3\u51b3\u4e86\u8ba1\u7b97\u5f00\u9500\u3001\u5185\u5b58\u53ef\u6269\u5c55\u6027\u548c\u6ce8\u610f\u529b\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u8fdb\u4e00\u6b65\u5b9e\u73b0\u4e86Transformer\u6a21\u578b\u7684\u9ad8\u6548\u7aef\u5230\u7aef\u52a0\u901f\u3002"}}
{"id": "2601.14466", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14466", "abs": "https://arxiv.org/abs/2601.14466", "authors": ["Roeland Wiersema"], "title": "JAXMg: A multi-GPU linear solver in JAX", "comment": null, "summary": "Solving large dense linear systems and eigenvalue problems is a core requirement in many areas of scientific computing, but scaling these operations beyond a single GPU remains challenging within modern programming frameworks. While highly optimized multi-GPU solver libraries exist, they are typically difficult to integrate into composable, just-in-time (JIT) compiled Python workflows. JAXMg provides multi-GPU dense linear algebra for JAX, enabling Cholesky-based linear solves and symmetric eigendecompositions for matrices that exceed single-GPU memory limits. By interfacing JAX with NVIDIA's cuSOLVERMg through an XLA Foreign Function Interface, JAXMg exposes distributed GPU solvers as JIT-compatible JAX primitives. This design allows scalable linear algebra to be embedded directly within JAX programs, preserving composability with JAX transformations and enabling multi-GPU execution in end-to-end scientific workflows.", "AI": {"tldr": "JAXMg \u662f\u4e00\u4e2a\u4e3a JAX \u63d0\u4f9b\u591a GPU \u7a20\u5bc6\u7ebf\u6027\u4ee3\u6570\u8fd0\u7b97\u7684\u5e93\uff0c\u901a\u8fc7\u96c6\u6210 NVIDIA cuSOLVERMg \u89e3\u51b3\u5355 GPU \u5185\u5b58\u4e0d\u8db3\u7684\u5927\u89c4\u6a21\u77e9\u9635\u8ba1\u7b97\u95ee\u9898\u3002", "motivation": "\u5927\u89c4\u6a21\u7a20\u5bc6\u7ebf\u6027\u7cfb\u7edf\u548c\u7279\u5f81\u503c\u95ee\u9898\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u5f88\u5e38\u89c1\uff0c\u4f46\u73b0\u6709\u591a GPU \u6c42\u89e3\u5668\u5e93\u96be\u4ee5\u96c6\u6210\u5230\u53ef\u7ec4\u5408\u7684 JIT \u7f16\u8bd1 Python \u5de5\u4f5c\u6d41\u4e2d\uff0c\u9650\u5236\u4e86 JAX \u6846\u67b6\u4e0b\u7684\u591a GPU \u8ba1\u7b97\u80fd\u529b\u3002", "method": "\u901a\u8fc7 XLA \u5916\u90e8\u51fd\u6570\u63a5\u53e3\u5c06 JAX \u4e0e NVIDIA cuSOLVERMg \u8fde\u63a5\uff0c\u5c06\u5206\u5e03\u5f0f GPU \u6c42\u89e3\u5668\u66b4\u9732\u4e3a JIT \u517c\u5bb9\u7684 JAX \u539f\u8bed\uff0c\u652f\u6301\u57fa\u4e8e Cholesky \u7684\u7ebf\u6027\u6c42\u89e3\u548c\u5bf9\u79f0\u7279\u5f81\u5206\u89e3\u3002", "result": "JAXMg \u5b9e\u73b0\u4e86\u8d85\u8d8a\u5355 GPU \u5185\u5b58\u9650\u5236\u7684\u77e9\u9635\u8ba1\u7b97\uff0c\u4f7f\u53ef\u6269\u5c55\u7ebf\u6027\u4ee3\u6570\u80fd\u591f\u76f4\u63a5\u5d4c\u5165 JAX \u7a0b\u5e8f\uff0c\u4fdd\u6301\u4e0e JAX \u53d8\u6362\u7684\u53ef\u7ec4\u5408\u6027\uff0c\u652f\u6301\u7aef\u5230\u7aef\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684\u591a GPU \u6267\u884c\u3002", "conclusion": "JAXMg \u6210\u529f\u89e3\u51b3\u4e86 JAX \u6846\u67b6\u4e2d\u591a GPU \u7a20\u5bc6\u7ebf\u6027\u4ee3\u6570\u7684\u96c6\u6210\u95ee\u9898\uff0c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u7ec4\u5408\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u94fe\u7684\u7a7a\u767d\u3002"}}
{"id": "2601.14347", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14347", "abs": "https://arxiv.org/abs/2601.14347", "authors": ["George Rafael Gourdoumanis", "Fotoini Oikonomou", "Maria Pantazi-Kypraiou", "Pavlos Stoikos", "Olympia Axelou", "Athanasios Tziouvaras", "Georgios Karakonstantis", "Tahani Aladwani", "Christos Anagnostopoulos", "Yixian Shen", "Anuj Pathania", "Alberto Garcia-Ortiz", "George Floros"], "title": "Multi-Partner Project: COIN-3D -- Collaborative Innovation in 3D VLSI Reliability", "comment": "DATE 2026", "summary": "As semiconductor manufacturing advances from the 3-nm process toward the sub-nanometer regime and transitions from FinFETs to gate-all-around field-effect transistors (GAAFETs), the resulting complexity and manufacturing challenges continue to increase. In this context, 3D chiplet-based approaches have emerged as key enablers to address these limitations while exploiting the expanded design space. Specifically, chiplets help address the lower yields typically associated with large monolithic designs. This paradigm enables the modular design of heterogeneous systems consisting of multiple chiplets (e.g., CPUs, GPUs, memory) fabricated using different technology nodes and processes. Consequently, it offers a capable and cost-effective strategy for designing heterogeneous systems. This paper introduces the Horizon Europe Twinning project COIN-3D (Collaborative Innovation in 3D VLSI Reliability), which aims to strengthen research excellence in 2.5D/3D VLSI systems reliability through collaboration between leading European institutions. More specifically, our primary scientific goal is the provision of novel open-source Electronic Design Automation (EDA) tools for reliability assessment of 3D systems, integrating advanced algorithms for physical- and system-level reliability analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86COIN-3D\u9879\u76ee\uff0c\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u5f00\u6e90EDA\u5de5\u5177\u6765\u52a0\u5f3a2.5D/3D VLSI\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u7814\u7a76\uff0c\u4ee5\u5e94\u5bf9\u534a\u5bfc\u4f53\u5236\u9020\u5411\u4e9a\u7eb3\u7c73\u5de5\u827a\u548cGAAFETs\u8f6c\u578b\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740\u534a\u5bfc\u4f53\u5236\u9020\u4ece3\u7eb3\u7c73\u5de5\u827a\u5411\u4e9a\u7eb3\u7c73\u9886\u57df\u53d1\u5c55\uff0c\u5e76\u4eceFinFETs\u8f6c\u5411\u5168\u73af\u7ed5\u6805\u6781\u573a\u6548\u5e94\u6676\u4f53\u7ba1(GAAFETs)\uff0c\u5236\u9020\u590d\u6742\u6027\u548c\u6311\u6218\u4e0d\u65ad\u589e\u52a0\u30023D\u5c0f\u82af\u7247\u65b9\u6cd5\u6210\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u5e76\u5229\u7528\u6269\u5c55\u8bbe\u8ba1\u7a7a\u95f4\u7684\u5173\u952e\u63a8\u52a8\u56e0\u7d20\uff0c\u7279\u522b\u662f\u5c0f\u82af\u7247\u6709\u52a9\u4e8e\u89e3\u51b3\u5927\u578b\u5355\u7247\u8bbe\u8ba1\u901a\u5e38\u5b58\u5728\u7684\u4f4e\u826f\u7387\u95ee\u9898\u3002", "method": "\u901a\u8fc7Horizon Europe Twinning\u9879\u76eeCOIN-3D\uff0c\u5728\u9886\u5148\u7684\u6b27\u6d32\u673a\u6784\u4e4b\u95f4\u5efa\u7acb\u5408\u4f5c\uff0c\u5f00\u53d1\u65b0\u9896\u7684\u5f00\u6e90\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316(EDA)\u5de5\u5177\uff0c\u7528\u4e8e3D\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u8bc4\u4f30\uff0c\u96c6\u6210\u7269\u7406\u7ea7\u548c\u7cfb\u7edf\u7ea7\u53ef\u9760\u6027\u5206\u6790\u7684\u5148\u8fdb\u7b97\u6cd5\u3002", "result": "\u8be5\u9879\u76ee\u65e8\u5728\u63d0\u4f9b\u5f00\u6e90EDA\u5de5\u5177\uff0c\u7528\u4e8e2.5D/3D VLSI\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u8bc4\u4f30\uff0c\u901a\u8fc7\u8de8\u673a\u6784\u5408\u4f5c\u52a0\u5f3a\u7814\u7a76\u5353\u8d8a\u6027\uff0c\u4e3a\u5f02\u6784\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u6210\u672c\u6548\u76ca\u9ad8\u7684\u7b56\u7565\u3002", "conclusion": "COIN-3D\u9879\u76ee\u901a\u8fc7\u5f00\u53d1\u5148\u8fdb\u7684\u53ef\u9760\u6027\u5206\u6790\u5de5\u5177\uff0c\u4e3a\u5e94\u5bf9\u534a\u5bfc\u4f53\u5236\u9020\u5411\u66f4\u5c0f\u5de5\u827a\u8282\u70b9\u548c3D\u96c6\u6210\u8f6c\u578b\u5e26\u6765\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u91cd\u8981\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u53ef\u9760\u3001\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u7684\u5f02\u6784\u7cfb\u7edf\u8bbe\u8ba1\u3002"}}
{"id": "2601.14608", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14608", "abs": "https://arxiv.org/abs/2601.14608", "authors": ["Torben R. Lahnor", "Mia Reitz", "Jonas Posner", "Patrick Diehl"], "title": "Exploring Performance-Productivity Trade-offs in AMT Runtimes: A Task Bench Study of Itoyori, ItoyoriFBC, HPX, and MPI", "comment": null, "summary": "Asynchronous Many-Task (AMT) runtimes offer a productive alternative to the Message Passing Interface (MPI). However, the diverse AMT landscape makes fair comparisons challenging. Task Bench, proposed by Slaughter et al., addresses this challenge through a parameterized framework for evaluating parallel programming systems. This work integrates two recent cluster AMTs, Itoyori and ItoyoriFBC, into Task Bench for comprehensive evaluation against MPI and HPX. Itoyori employs a Partitioned Global Address Space (PGAS) model with RDMA-based work stealing, while ItoyoriFBC extends it with futurebased synchronization.\n  We evaluate these systems in terms of both performance and programmer productivity. Performance is assessed across various configurations, including compute-bound kernels, weak scaling, and both imbalanced and communication-intensive patterns. Performance is quantified using application efficiency, i.e., the percentage of maximum performance achieved, and the Minimum Effective Task Granularity (METG), i.e., the smallest task duration before runtime overheads dominate. Programmer productivity is quantified using Lines of Code (LOC) and the Number of Library Constructs (NLC).\n  Our results reveal distinct trade-offs. MPI achieves the highest efficiency for regular, communication-light workloads but requires verbose, lowlevel code. HPX maintains stable efficiency under load imbalance across varying node counts, yet ranks last in productivity metrics, demonstrating that AMTs do not inherently guarantee improved productivity over MPI. Itoyori achieves the highest efficiency in communication-intensive configurations while leading in programmer productivity. ItoyoriFBC exhibits slightly lower efficiency than Itoyori, though its future-based synchronization offers potential for expressing irregular workloads.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06Itoyori\u548cItoyoriFBC\u4e24\u4e2a\u5f02\u6b65\u591a\u4efb\u52a1\u8fd0\u884c\u65f6\u96c6\u6210\u5230Task Bench\u6846\u67b6\u4e2d\uff0c\u4e0eMPI\u548cHPX\u8fdb\u884c\u7efc\u5408\u6027\u80fd\u4e0e\u751f\u4ea7\u529b\u5bf9\u6bd4\u8bc4\u4f30\u3002", "motivation": "\u5f02\u6b65\u591a\u4efb\u52a1\u8fd0\u884c\u65f6\u4f5c\u4e3aMPI\u7684\u66ff\u4ee3\u65b9\u6848\u5177\u6709\u751f\u4ea7\u529b\u4f18\u52bf\uff0c\u4f46\u591a\u6837\u5316\u7684AMT\u751f\u6001\u7cfb\u7edf\u4f7f\u5f97\u516c\u5e73\u6bd4\u8f83\u53d8\u5f97\u56f0\u96be\u3002\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e0d\u540c\u5e76\u884c\u7f16\u7a0b\u7cfb\u7edf\u7684\u6027\u80fd\u4e0e\u751f\u4ea7\u529b\u6743\u8861\u3002", "method": "\u4f7f\u7528Task Bench\u53c2\u6570\u5316\u6846\u67b6\u8bc4\u4f30\u56db\u4e2a\u7cfb\u7edf\uff1aMPI\u3001HPX\u3001Itoyori\uff08\u57fa\u4e8ePGAS\u548cRDMA\u5de5\u4f5c\u7a83\u53d6\uff09\u548cItoyoriFBC\uff08\u6269\u5c55\u4e86\u57fa\u4e8efuture\u7684\u540c\u6b65\uff09\u3002\u901a\u8fc7\u5e94\u7528\u6548\u7387\u3001\u6700\u5c0f\u6709\u6548\u4efb\u52a1\u7c92\u5ea6\u8bc4\u4f30\u6027\u80fd\uff0c\u901a\u8fc7\u4ee3\u7801\u884c\u6570\u548c\u5e93\u6784\u9020\u6570\u91cf\u8bc4\u4f30\u7a0b\u5e8f\u5458\u751f\u4ea7\u529b\u3002", "result": "MPI\u5728\u89c4\u5219\u3001\u901a\u4fe1\u8f7b\u91cf\u7ea7\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u6548\u7387\u6700\u9ad8\u4f46\u4ee3\u7801\u5197\u957f\uff1bHPX\u5728\u4e0d\u540c\u8282\u70b9\u6570\u4e0b\u8d1f\u8f7d\u4e0d\u5747\u8861\u65f6\u4fdd\u6301\u7a33\u5b9a\u6548\u7387\u4f46\u751f\u4ea7\u529b\u6307\u6807\u6700\u5dee\uff1bItoyori\u5728\u901a\u4fe1\u5bc6\u96c6\u578b\u914d\u7f6e\u4e2d\u6548\u7387\u6700\u9ad8\u4e14\u751f\u4ea7\u529b\u9886\u5148\uff1bItoyoriFBC\u6548\u7387\u7565\u4f4e\u4e8eItoyori\u4f46future\u540c\u6b65\u4e3a\u4e0d\u89c4\u5219\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u6f5c\u529b\u3002", "conclusion": "\u5f02\u6b65\u591a\u4efb\u52a1\u8fd0\u884c\u65f6\u5e76\u4e0d\u5929\u7136\u4fdd\u8bc1\u6bd4MPI\u66f4\u9ad8\u7684\u751f\u4ea7\u529b\uff0c\u4e0d\u540c\u7cfb\u7edf\u5728\u6027\u80fd\u4e0e\u751f\u4ea7\u529b\u95f4\u5b58\u5728\u660e\u663e\u6743\u8861\u3002Itoyori\u5728\u901a\u4fe1\u5bc6\u96c6\u578b\u573a\u666f\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800cItoyoriFBC\u7684future\u540c\u6b65\u4e3a\u4e0d\u89c4\u5219\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u8868\u8fbe\u6f5c\u529b\u3002"}}
{"id": "2601.15151", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.15151", "abs": "https://arxiv.org/abs/2601.15151", "authors": ["Jean Bruant", "Pierre-Henri Horrein", "Olivier Muller", "Fr\u00e9d\u00e9ric P\u00e9trot"], "title": "Pipeline Automation Framework for Reusable High-throughput Network Applications on FPGA", "comment": "29 pages, 10 listings, 5 tables", "summary": "In a context of ever-growing worldwide communication traffic, cloud service providers aim at deploying scalable infrastructures to address heterogeneous needs. Part of the network infrastructure, FPGAs are tailored to guarantee low-latency and high-throughput packet processing. However, slowness of the hardware design process impairs FPGA ability to be part of an agile infrastructure under constant evolution, from incident response to long-term transformation. Deploying and maintaining network functionalities across a wide variety of FPGAs raises the need to fine-tune hardware designs for several FPGA targets. To address this issue, we introduce PAF, an open-source architectural parameterization framework based on a pipeline-oriented design methodology. PAF (Pipeline Automation Framework) implementation is based on Chisel, a Scala-embedded Hardware Construction Language (HCL), that we leverage to interface with circuit elaboration. Applied to industrial network packet classification systems, PAF demonstrates efficient parameterization abilities, enabling to reuse and optimize the same pipelined design on several FPGAs. In addition, PAF focuses the pipeline description on the architectural intent, incidentally reducing the number of lines of code to express complex functionalities. Finally, PAF confirms that automation does not imply any loss of tight control on the architecture by achieving on par performance and resource usage with equivalent exhaustively described implementations.", "AI": {"tldr": "PAF\u662f\u4e00\u4e2a\u57fa\u4e8eChisel\u7684\u5f00\u6e90\u6d41\u6c34\u7ebf\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u53c2\u6570\u5316FPGA\u7f51\u7edc\u529f\u80fd\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u8de8\u591a\u79cdFPGA\u5e73\u53f0\u7684\u786c\u4ef6\u8bbe\u8ba1\u590d\u7528\u548c\u4f18\u5316\u3002", "motivation": "\u5728\u4e91\u670d\u52a1\u63d0\u4f9b\u5546\u90e8\u7f72\u53ef\u6269\u5c55\u57fa\u7840\u8bbe\u65bd\u7684\u80cc\u666f\u4e0b\uff0cFPGA\u4f5c\u4e3a\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u7684\u4e00\u90e8\u5206\uff0c\u867d\u7136\u80fd\u4fdd\u8bc1\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u91cf\u7684\u6570\u636e\u5305\u5904\u7406\uff0c\u4f46\u786c\u4ef6\u8bbe\u8ba1\u8fc7\u7a0b\u7f13\u6162\uff0c\u96be\u4ee5\u9002\u5e94\u5feb\u901f\u6f14\u8fdb\u7684\u654f\u6377\u57fa\u7840\u8bbe\u65bd\u9700\u6c42\u3002\u540c\u65f6\uff0c\u5728\u591a\u79cdFPGA\u5e73\u53f0\u4e0a\u90e8\u7f72\u548c\u7ef4\u62a4\u7f51\u7edc\u529f\u80fd\u9700\u8981\u5bf9\u786c\u4ef6\u8bbe\u8ba1\u8fdb\u884c\u7cbe\u7ec6\u8c03\u4f18\u3002", "method": "\u63d0\u51fa\u4e86PAF\uff08Pipeline Automation Framework\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u6d41\u6c34\u7ebf\u5bfc\u5411\u8bbe\u8ba1\u65b9\u6cd5\u7684\u5f00\u6e90\u67b6\u6784\u53c2\u6570\u5316\u6846\u67b6\u3002PAF\u57fa\u4e8eChisel\uff08\u4e00\u79cdScala\u5d4c\u5165\u5f0f\u786c\u4ef6\u6784\u9020\u8bed\u8a00\uff09\u5b9e\u73b0\uff0c\u5229\u7528\u8be5\u8bed\u8a00\u8fdb\u884c\u7535\u8def\u7ec6\u5316\u63a5\u53e3\u3002\u6846\u67b6\u4e13\u6ce8\u4e8e\u6d41\u6c34\u7ebf\u67b6\u6784\u610f\u56fe\u7684\u63cf\u8ff0\uff0c\u51cf\u5c11\u8868\u8fbe\u590d\u6742\u529f\u80fd\u6240\u9700\u7684\u4ee3\u7801\u91cf\u3002", "result": "\u5e94\u7528\u4e8e\u5de5\u4e1a\u7f51\u7edc\u6570\u636e\u5305\u5206\u7c7b\u7cfb\u7edf\u65f6\uff0cPAF\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u53c2\u6570\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5728\u591a\u79cdFPGA\u4e0a\u590d\u7528\u548c\u4f18\u5316\u76f8\u540c\u7684\u6d41\u6c34\u7ebf\u8bbe\u8ba1\u3002\u81ea\u52a8\u5316\u5e76\u672a\u5bfc\u81f4\u67b6\u6784\u63a7\u5236\u7684\u635f\u5931\uff0c\u5b9e\u73b0\u4e86\u4e0e\u8be6\u5c3d\u63cf\u8ff0\u5b9e\u73b0\u76f8\u5f53\u7684\u6027\u80fd\u548c\u8d44\u6e90\u4f7f\u7528\u6548\u7387\u3002", "conclusion": "PAF\u6846\u67b6\u901a\u8fc7\u67b6\u6784\u53c2\u6570\u5316\u548c\u6d41\u6c34\u7ebf\u81ea\u52a8\u5316\uff0c\u89e3\u51b3\u4e86FPGA\u786c\u4ef6\u8bbe\u8ba1\u8fc7\u7a0b\u7f13\u6162\u548c\u8de8\u5e73\u53f0\u90e8\u7f72\u56f0\u96be\u7684\u95ee\u9898\uff0c\u4f7fFPGA\u80fd\u591f\u66f4\u597d\u5730\u878d\u5165\u5feb\u901f\u6f14\u8fdb\u7684\u4e91\u57fa\u7840\u8bbe\u65bd\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u67b6\u6784\u7684\u7d27\u5bc6\u63a7\u5236\u3002"}}
{"id": "2601.14612", "categories": ["cs.DC", "cs.NI", "cs.PF", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.14612", "abs": "https://arxiv.org/abs/2601.14612", "authors": ["Neelkamal Bhuyan", "Randeep Bhatia", "Murali Kodialam", "TV Lakshman"], "title": "Exploiting Spot Instances for Time-Critical Cloud Workloads Using Optimal Randomized Strategies", "comment": "Accepted for publication in the 45th IEEE International Conference on Computer Communications (INFOCOM 2026). Copyright 2026 IEEE", "summary": "This paper addresses the challenge of deadline-aware online scheduling for jobs in hybrid cloud environments, where jobs may run on either cost-effective but unreliable spot instances or more expensive on-demand instances, under hard deadlines. We first establish a fundamental limit for existing (predominantly-) deterministic policies, proving a worst-case competitive ratio of $\u03a9(K)$, where $K$ is the cost ratio between on-demand and spot instances. We then present a novel randomized scheduling algorithm, ROSS, that achieves a provably optimal competitive ratio of $\\sqrt{K}$ under reasonable deadlines, significantly improving upon existing approaches. Extensive evaluations on real-world trace data from Azure and AWS demonstrate that ROSS effectively balances cost optimization and deadline guarantees, consistently outperforming the state-of-the-art by up to $30\\%$ in cost savings, across diverse spot market conditions.", "AI": {"tldr": "\u63d0\u51faROSS\u968f\u673a\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5728\u6df7\u5408\u4e91\u73af\u5883\u4e2d\u5b9e\u73b0\u6700\u4f18\u221aK\u7ade\u4e89\u6bd4\uff0c\u76f8\u6bd4\u73b0\u6709\u03a9(K)\u65b9\u6cd5\u663e\u8457\u6539\u8fdb\uff0c\u5b9e\u9645\u8bc4\u4f30\u663e\u793a\u6210\u672c\u8282\u7701\u63d0\u534730%", "motivation": "\u89e3\u51b3\u6df7\u5408\u4e91\u73af\u5883\u4e2d\u5177\u6709\u786c\u622a\u6b62\u65f6\u95f4\u7684\u4f5c\u4e1a\u8c03\u5ea6\u6311\u6218\uff0c\u9700\u8981\u5728\u6210\u672c\u6548\u76ca\u4f46\u4e0d\u53ef\u9760\u7684\u7ade\u4ef7\u5b9e\u4f8b\u4e0e\u6602\u8d35\u4f46\u53ef\u9760\u7684\u6309\u9700\u5b9e\u4f8b\u4e4b\u95f4\u505a\u51fa\u51b3\u7b56\uff0c\u73b0\u6709\u786e\u5b9a\u6027\u7b56\u7565\u5b58\u5728\u03a9(K)\u7684\u6700\u574f\u60c5\u51b5\u7ade\u4e89\u6bd4\u9650\u5236", "method": "\u63d0\u51faROSS\u968f\u673a\u8c03\u5ea6\u7b97\u6cd5\uff0c\u91c7\u7528\u968f\u673a\u5316\u7b56\u7565\u5728\u7ade\u4ef7\u5b9e\u4f8b\u548c\u6309\u9700\u5b9e\u4f8b\u4e4b\u95f4\u8fdb\u884c\u8c03\u5ea6\u51b3\u7b56\uff0c\u5728\u5408\u7406\u622a\u6b62\u65f6\u95f4\u7ea6\u675f\u4e0b\u5b9e\u73b0\u6700\u4f18\u7ade\u4e89\u6bd4", "result": "\u7406\u8bba\u8bc1\u660eROSS\u7b97\u6cd5\u8fbe\u5230\u221aK\u7684\u6700\u4f18\u7ade\u4e89\u6bd4\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u786e\u5b9a\u6027\u7b56\u7565\u7684\u03a9(K)\u4e0b\u754c\uff1b\u5728Azure\u548cAWS\u771f\u5b9e\u8ffd\u8e2a\u6570\u636e\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cROSS\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u8282\u7701\u9ad8\u8fbe30%\u7684\u6210\u672c", "conclusion": "ROSS\u7b97\u6cd5\u5728\u6df7\u5408\u4e91\u622a\u6b62\u65f6\u95f4\u611f\u77e5\u8c03\u5ea6\u4e2d\u5b9e\u73b0\u4e86\u7406\u8bba\u6700\u4f18\u6027\u80fd\uff0c\u6709\u6548\u5e73\u8861\u6210\u672c\u4f18\u5316\u548c\u622a\u6b62\u65f6\u95f4\u4fdd\u8bc1\uff0c\u5728\u4e0d\u540c\u7ade\u4ef7\u5e02\u573a\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u8272"}}
{"id": "2601.14642", "categories": ["cs.DC", "cs.LO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.14642", "abs": "https://arxiv.org/abs/2601.14642", "authors": ["Guillaume Ambal", "Max Stupple", "Brijesh Dongol", "Azalea Raad"], "title": "Specifying and Verifying RDMA Synchronisation (Extended Version)", "comment": "95 pages, extended version of ESOP 2026 paper", "summary": "Remote direct memory access (RDMA) allows a machine to directly read from and write to the memory of remote machine, enabling high-throughput, low-latency data transfer. Ensuring correctness of RDMA programs has only recently become possible with the formalisation of $\\text{RDMA}^\\text{TSO}$ semantics (describing the behaviour of RDMA networking over a TSO CPU). However, this semantics currently lacks a formalisation of remote synchronisation, meaning that the implementations of common abstractions such as locks cannot be verified. In this paper, we close this gap by presenting $\\text{RDMA}^{\\text{TSO}}_{\\text{RMW}}$, the first semantics for remote `read-modify-write' (RMW) instructions over TSO. It turns out that remote RMW operations are weak and only ensure atomicity against other remote RMWs. We therefore build a set of composable synchronisation abstractions starting with the $\\text{RDMA}^{\\text{WAIT}}_{\\text{RMW}}$ library. Underpinned by $\\text{RDMA}^{\\text{WAIT}}_{\\text{RMW}}$, we then specify, implement and verify three classes of remote locks that are suitable for different scenarios. Additionally, we develop the notion of a strong RDMA model, $\\text{RDMA}^{\\text{SC}}_{\\text{RMW}}$, which is akin to sequential consistency in shared memory architectures. Our libraries are built to be compatible with an existing set of high-performance libraries called LOCO, which ensures compositionality and verifiability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86RDMA\u5728TSO\u5185\u5b58\u6a21\u578b\u4e0b\u7684\u8fdc\u7a0bRMW\u64cd\u4f5c\u8bed\u4e49RDMA^TSO_RMW\uff0c\u89e3\u51b3\u4e86\u73b0\u6709RDMA\u8bed\u4e49\u7f3a\u4e4f\u8fdc\u7a0b\u540c\u6b65\u539f\u8bed\u7684\u95ee\u9898\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efa\u4e86\u53ef\u7ec4\u5408\u7684\u540c\u6b65\u62bd\u8c61\u5e93\u548c\u4e09\u7c7b\u8fdc\u7a0b\u9501\u3002", "motivation": "\u73b0\u6709\u7684RDMA^TSO\u8bed\u4e49\u867d\u7136\u63cf\u8ff0\u4e86RDMA\u5728TSO CPU\u4e0a\u7684\u884c\u4e3a\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fdc\u7a0b\u540c\u6b65\u64cd\u4f5c\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u5bfc\u81f4\u65e0\u6cd5\u9a8c\u8bc1\u9501\u7b49\u5e38\u89c1\u540c\u6b65\u62bd\u8c61\u7684\u5b9e\u73b0\u6b63\u786e\u6027\u3002", "method": "1. \u63d0\u51faRDMA^TSO_RMW\u8bed\u4e49\uff0c\u9996\u6b21\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86TSO\u67b6\u6784\u4e0b\u7684\u8fdc\u7a0b\"\u8bfb-\u4fee\u6539-\u5199\"\u64cd\u4f5c\uff1b2. \u6784\u5efaRDMA^WAIT_RMW\u5e93\u4f5c\u4e3a\u53ef\u7ec4\u5408\u540c\u6b65\u62bd\u8c61\u7684\u57fa\u7840\uff1b3. \u57fa\u4e8e\u6b64\u5e93\u8bbe\u8ba1\u3001\u5b9e\u73b0\u5e76\u9a8c\u8bc1\u4e09\u7c7b\u9002\u7528\u4e8e\u4e0d\u540c\u573a\u666f\u7684\u8fdc\u7a0b\u9501\uff1b4. \u63d0\u51fa\u7c7b\u4f3c\u987a\u5e8f\u4e00\u81f4\u6027\u7684\u5f3aRDMA\u6a21\u578bRDMA^SC_RMW\u3002", "result": "1. \u53d1\u73b0\u8fdc\u7a0bRMW\u64cd\u4f5c\u8f83\u5f31\uff0c\u4ec5\u80fd\u4fdd\u8bc1\u4e0e\u5176\u4ed6\u8fdc\u7a0bRMW\u64cd\u4f5c\u7684\u539f\u5b50\u6027\uff1b2. \u5f00\u53d1\u4e86\u4e0e\u73b0\u6709\u9ad8\u6027\u80fd\u5e93LOCO\u517c\u5bb9\u7684\u540c\u6b65\u5e93\uff0c\u786e\u4fdd\u53ef\u7ec4\u5408\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\uff1b3. \u63d0\u4f9b\u4e86\u4e09\u7c7b\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u8fdc\u7a0b\u9501\u5b9e\u73b0\u3002", "conclusion": "\u8bba\u6587\u586b\u8865\u4e86RDMA\u8bed\u4e49\u4e2d\u8fdc\u7a0b\u540c\u6b65\u539f\u8bed\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u5b9a\u4e49\u8fdc\u7a0bRMW\u64cd\u4f5c\u548c\u6784\u5efa\u540c\u6b65\u62bd\u8c61\u5e93\uff0c\u4f7f\u5f97\u5728RDMA\u7f51\u7edc\u4e0a\u5b9e\u73b0\u548c\u9a8c\u8bc1\u9501\u7b49\u540c\u6b65\u673a\u5236\u6210\u4e3a\u53ef\u80fd\uff0c\u4e3a\u6784\u5efa\u53ef\u9760\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.14735", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14735", "abs": "https://arxiv.org/abs/2601.14735", "authors": ["Varad Kulkarni", "Vaibhav Jha", "Nikhil Reddy", "Yogesh Simmhan"], "title": "Optimizing FaaS Platforms for MCP-enabled Agentic Workflows", "comment": null, "summary": "Agentic workflows that use autonomous AI Agents powered by Large Language Models (LLMs) and Model Context Protocol (MCP) servers is rapidly rising. This introduces challenges in scalable cloud deployment and state management. Traditional hosting on Virtual Machines (VMs) is resource-intensive and lacks elasticity. Functions-as-a-Service (FaaS) platforms offer modularity, autoscaling and cost efficiency but are inherently stateless. In this paper, we present the FAME, a FaaS-based architecture for orchestrating MCP-enabled agentic workflows. FAME decomposes agentic patterns such as ReAct into composable agents: Planner, Actor and Evaluator, that are each a FaaS function built using LangGraph and are orchestrated as a FaaS workflow. This enables modular composition as AWS Step Functions and avoids function timeouts seen for monolithic agentic workflows. To address context persistence across user requests in a conversation, FAME automates agent memory persistence and injection using DynamoDB. It also optimizes MCP server deployment through AWS Lambda wrappers, caches tool outputs in S3 and proposes function fusion strategies. We evaluate FAME on two representative applications, on research paper summarization and log analytics, under diverse memory and caching configurations. Results show up to 13x latency reduction, 88% fewer input tokens and 66% in cost savings, along with improved workflow completion rates. This demonstrates the viability of serverless platforms for hosting complex, multi-agent AI workflows at scale.", "AI": {"tldr": "FAME\u662f\u4e00\u4e2a\u57fa\u4e8eFaaS\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u7f16\u6392\u652f\u6301MCP\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u5c06\u667a\u80fd\u4f53\u6a21\u5f0f\u5206\u89e3\u4e3a\u53ef\u7ec4\u5408\u7684FaaS\u51fd\u6570\uff0c\u5b9e\u73b0\u6a21\u5757\u5316\u3001\u81ea\u52a8\u6269\u5c55\u548c\u6210\u672c\u6548\u76ca\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "motivation": "\u57fa\u4e8eLLM\u548cMCP\u7684\u81ea\u4e3bAI\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u5feb\u901f\u589e\u957f\uff0c\u4f46\u9762\u4e34\u4e91\u90e8\u7f72\u6269\u5c55\u6027\u548c\u72b6\u6001\u7ba1\u7406\u7684\u6311\u6218\u3002\u4f20\u7edfVM\u6258\u7ba1\u8d44\u6e90\u5bc6\u96c6\u4e14\u7f3a\u4e4f\u5f39\u6027\uff0c\u800cFaaS\u5e73\u53f0\u867d\u7136\u6a21\u5757\u5316\u3001\u53ef\u81ea\u52a8\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\uff0c\u4f46\u672c\u8d28\u4e0a\u662f\u65e0\u72b6\u6001\u7684\u3002", "method": "FAME\u5c06\u667a\u80fd\u4f53\u6a21\u5f0f\uff08\u5982ReAct\uff09\u5206\u89e3\u4e3a\u53ef\u7ec4\u5408\u7684\u667a\u80fd\u4f53\uff1aPlanner\u3001Actor\u548cEvaluator\uff0c\u6bcf\u4e2a\u90fd\u662f\u4f7f\u7528LangGraph\u6784\u5efa\u7684FaaS\u51fd\u6570\uff0c\u5e76\u4f5c\u4e3aFaaS\u5de5\u4f5c\u6d41\u7f16\u6392\u3002\u901a\u8fc7DynamoDB\u81ea\u52a8\u5316\u667a\u80fd\u4f53\u5185\u5b58\u6301\u4e45\u5316\u548c\u6ce8\u5165\uff0c\u4f7f\u7528AWS Lambda\u5305\u88c5\u5668\u4f18\u5316MCP\u670d\u52a1\u5668\u90e8\u7f72\uff0c\u5728S3\u4e2d\u7f13\u5b58\u5de5\u5177\u8f93\u51fa\uff0c\u5e76\u63d0\u51fa\u51fd\u6570\u878d\u5408\u7b56\u7565\u3002", "result": "\u5728\u4e24\u4e2a\u4ee3\u8868\u6027\u5e94\u7528\uff08\u7814\u7a76\u8bba\u6587\u6458\u8981\u548c\u65e5\u5fd7\u5206\u6790\uff09\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\uff1a\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe13\u500d\uff0c\u8f93\u5165\u4ee4\u724c\u51cf\u5c1188%\uff0c\u6210\u672c\u8282\u770166%\uff0c\u5de5\u4f5c\u6d41\u5b8c\u6210\u7387\u63d0\u9ad8\u3002", "conclusion": "FAME\u8bc1\u660e\u4e86\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u5728\u89c4\u6a21\u5316\u6258\u7ba1\u590d\u6742\u591a\u667a\u80fd\u4f53AI\u5de5\u4f5c\u6d41\u65b9\u9762\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7FaaS\u67b6\u6784\u89e3\u51b3\u4e86\u72b6\u6001\u7ba1\u7406\u548c\u6269\u5c55\u6027\u6311\u6218\u3002"}}
{"id": "2601.14912", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.14912", "abs": "https://arxiv.org/abs/2601.14912", "authors": ["Guangba Yu", "Genting Mai", "Rui Wang", "Ruipeng Li", "Pengfei Chen", "Long Pan", "Ruijie Xu"], "title": "AlertGuardian: Intelligent Alert Life-Cycle Management for Large-scale Cloud Systems", "comment": "Accepted by ASE 2025", "summary": "Alerts are critical for detecting anomalies in large-scale cloud systems, ensuring reliability and user experience. However, current systems generate overwhelming volumes of alerts, degrading operational efficiency due to ineffective alert life-cycle management. This paper details the efforts of Company-X to optimize alert life-cycle management, addressing alert fatigue in cloud systems. We propose AlertGuardian, a framework collaborating large language models (LLMs) and lightweight graph models to optimize the alert life-cycle through three phases: Alert Denoise uses graph learning model with virtual noise to filter noise, Alert Summary employs Retrieval Augmented Generation (RAG) with LLMs to create actionable summary, and Alert Rule Refinement leverages multi-agent iterative feedbacks to improve alert rule quality. Evaluated on four real-world datasets from Company-X's services, AlertGuardian significantly mitigates alert fatigue (94.8\\% alert reduction ratios) and accelerates fault diagnosis (90.5\\% diagnosis accuracy). Moreover, AlertGuardian improves 1,174 alert rules, with 375 accepted by SREs (32% acceptance rate). Finally, we share success stories and lessons learned about alert life-cycle management after the deployment of AlertGuardian in Company-X.", "AI": {"tldr": "AlertGuardian\u6846\u67b6\u7ed3\u5408LLM\u548c\u56fe\u6a21\u578b\u4f18\u5316\u4e91\u7cfb\u7edf\u544a\u8b66\u751f\u547d\u5468\u671f\u7ba1\u7406\uff0c\u901a\u8fc7\u53bb\u566a\u3001\u6458\u8981\u548c\u89c4\u5219\u4f18\u5316\u4e09\u4e2a\u9636\u6bb5\u663e\u8457\u51cf\u5c11\u544a\u8b66\u75b2\u52b3\u5e76\u63d0\u5347\u6545\u969c\u8bca\u65ad\u6548\u7387\u3002", "motivation": "\u4e91\u7cfb\u7edf\u4ea7\u751f\u5927\u91cf\u544a\u8b66\u5bfc\u81f4\u544a\u8b66\u75b2\u52b3\uff0c\u964d\u4f4e\u8fd0\u7ef4\u6548\u7387\uff0c\u9700\u8981\u4f18\u5316\u544a\u8b66\u751f\u547d\u5468\u671f\u7ba1\u7406\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faAlertGuardian\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8f7b\u91cf\u56fe\u6a21\u578b\uff1a1) Alert Denoise\u4f7f\u7528\u5e26\u865a\u62df\u566a\u58f0\u7684\u56fe\u5b66\u4e60\u6a21\u578b\u8fc7\u6ee4\u566a\u58f0\uff1b2) Alert Summary\u91c7\u7528RAG\u589e\u5f3a\u7684LLM\u751f\u6210\u53ef\u64cd\u4f5c\u6458\u8981\uff1b3) Alert Rule Refinement\u5229\u7528\u591a\u667a\u80fd\u4f53\u8fed\u4ee3\u53cd\u9988\u4f18\u5316\u544a\u8b66\u89c4\u5219\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff1a\u544a\u8b66\u51cf\u5c11\u7387\u8fbe94.8%\uff0c\u6545\u969c\u8bca\u65ad\u51c6\u786e\u7387\u8fbe90.5%\uff0c\u4f18\u5316\u4e861,174\u6761\u544a\u8b66\u89c4\u5219\uff0c\u5176\u4e2d375\u6761\u88abSRE\u63a5\u53d7\uff0832%\u63a5\u53d7\u7387\uff09\u3002", "conclusion": "AlertGuardian\u80fd\u6709\u6548\u7f13\u89e3\u4e91\u7cfb\u7edf\u544a\u8b66\u75b2\u52b3\uff0c\u63d0\u5347\u8fd0\u7ef4\u6548\u7387\uff0c\u5e76\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5206\u4eab\u4e86\u6210\u529f\u7ecf\u9a8c\u548c\u6559\u8bad\u3002"}}
{"id": "2601.14923", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14923", "abs": "https://arxiv.org/abs/2601.14923", "authors": ["Kaddour Sidi", "Daniel Balouek", "Baptiste Jonglez"], "title": "Application-level observability for adaptive Edge to Cloud continuum systems", "comment": "UCC 2025 - IEEE/ACM 18th International Conference on Utility and Cloud Computing, Dec 2025, NANTES, France", "summary": "Modern Edge-to-Cloud (E2C) systems require fine-grained observability to ensure adaptive behavior and compliance with performance objectives across heterogeneous and dynamic environments. This work introduces an application-level observability framework that integrates developer-driven instrumentation and SLO-aware feedback for autonomous adaptation. By combining OpenTelemetry, Prometheus, K3s, and Chaos Mesh, the framework enables real-time monitoring and adaptive control across the continuum. A video processing use case demonstrates how application-level metrics guide automatic adjustments to maintain target frame rate, latency, and detection accuracy under variable workloads and injected faults. Preliminary results highlight improved scalability, fault tolerance, and responsiveness, providing a practical foundation for adaptive, SLO-compliant E2C applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5e94\u7528\u7ea7\u53ef\u89c2\u6d4b\u6027\u6846\u67b6\uff0c\u7ed3\u5408\u5f00\u53d1\u8005\u9a71\u52a8\u7684\u4eea\u5668\u5316\u548cSLO\u611f\u77e5\u53cd\u9988\uff0c\u5b9e\u73b0\u8fb9\u7f18\u5230\u4e91\u7cfb\u7edf\u7684\u81ea\u4e3b\u9002\u5e94", "motivation": "\u73b0\u4ee3\u8fb9\u7f18\u5230\u4e91\u7cfb\u7edf\u9700\u8981\u7ec6\u7c92\u5ea6\u53ef\u89c2\u6d4b\u6027\uff0c\u4ee5\u786e\u4fdd\u5728\u5f02\u6784\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u884c\u4e3a\u548c\u6027\u80fd\u76ee\u6807\u5408\u89c4", "method": "\u96c6\u6210OpenTelemetry\u3001Prometheus\u3001K3s\u548cChaos Mesh\uff0c\u6784\u5efa\u5e94\u7528\u7ea7\u53ef\u89c2\u6d4b\u6027\u6846\u67b6\uff0c\u652f\u6301\u5b9e\u65f6\u76d1\u63a7\u548c\u81ea\u9002\u5e94\u63a7\u5236", "result": "\u89c6\u9891\u5904\u7406\u7528\u4f8b\u663e\u793a\u5e94\u7528\u7ea7\u6307\u6807\u80fd\u6307\u5bfc\u81ea\u52a8\u8c03\u6574\uff0c\u5728\u53ef\u53d8\u5de5\u4f5c\u8d1f\u8f7d\u548c\u6ce8\u5165\u6545\u969c\u4e0b\u7ef4\u6301\u76ee\u6807\u5e27\u7387\u3001\u5ef6\u8fdf\u548c\u68c0\u6d4b\u7cbe\u5ea6", "conclusion": "\u521d\u6b65\u7ed3\u679c\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u53ef\u6269\u5c55\u6027\u3001\u5bb9\u9519\u6027\u548c\u54cd\u5e94\u6027\uff0c\u4e3a\u81ea\u9002\u5e94\u3001SLO\u5408\u89c4\u7684\u8fb9\u7f18\u5230\u4e91\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840"}}
{"id": "2601.14980", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.14980", "abs": "https://arxiv.org/abs/2601.14980", "authors": ["Mengchun Xia", "Zhicheng Dong", "Donghong Cai", "Fang Fang", "Lisheng Fan", "Pingzhi Fan"], "title": "Parallel Collaborative ADMM Privacy Computing and Adaptive GPU Acceleration for Distributed Edge Networks", "comment": null, "summary": "Distributed computing has been widely applied in distributed edge networks for reducing the processing burden of high-dimensional data centralization, where a high-dimensional computational task is decomposed into multiple low-dimensional collaborative processing tasks or multiple edge nodes use distributed data to train a global model. However, the computing power of a single-edge node is limited, and collaborative computing will cause information leakage and excessive communication overhead. In this paper, we design a parallel collaborative distributed alternating direction method of multipliers (ADMM) and propose a three-phase parallel collaborative ADMM privacy computing (3P-ADMM-PC2) algorithm for distributed computing in edge networks, where the Paillier homomorphic encryption is utilized to protect data privacy during interactions. Especially, a quantization method is introduced, which maps the real numbers to a positive integer interval without affecting the homomorphic operations. To address the architectural mismatch between large-integer and Graphics Processing Unit (GPU) computing, we transform high-bitwidth computations into low-bitwidth matrix and vector operations. Thus the GPU can be utilized to implement parallel encryption and decryption computations with long keys. Finally, a GPU-accelerated 3P-ADMM-PC2 is proposed to optimize the collaborative computing tasks. Meanwhile, large-scale computational tasks are conducted in network topologies with varying numbers of edge nodes. Experimental results demonstrate that the proposed 3P-ADMM-PC2 has excellent mean square error performance, which is close to that of distributed ADMM without privacy-preserving. Compared to centralized ADMM and distributed ADMM implemented with Central Processing Unit (CPU) computation, the proposed scheme demonstrates a significant speedup ratio.", "AI": {"tldr": "\u63d0\u51fa3P-ADMM-PC2\u7b97\u6cd5\uff0c\u7ed3\u5408\u5e76\u884c\u534f\u4f5cADMM\u3001Paillier\u540c\u6001\u52a0\u5bc6\u548cGPU\u52a0\u901f\uff0c\u89e3\u51b3\u8fb9\u7f18\u7f51\u7edc\u4e2d\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898", "motivation": "\u8fb9\u7f18\u7f51\u7edc\u4e2d\u5206\u5e03\u5f0f\u8ba1\u7b97\u9762\u4e34\u5355\u8282\u70b9\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u3001\u534f\u4f5c\u8ba1\u7b97\u5bfc\u81f4\u4fe1\u606f\u6cc4\u9732\u548c\u901a\u4fe1\u5f00\u9500\u8fc7\u5927\u7684\u95ee\u9898\uff0c\u9700\u8981\u8bbe\u8ba1\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u9ad8\u6548\u8ba1\u7b97\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u8bbe\u8ba1\u5e76\u884c\u534f\u4f5c\u5206\u5e03\u5f0fADMM\u7b97\u6cd5\uff0c\u5f15\u5165Paillier\u540c\u6001\u52a0\u5bc6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\uff0c\u91c7\u7528\u91cf\u5316\u65b9\u6cd5\u5c06\u5b9e\u6570\u6620\u5c04\u5230\u6b63\u6574\u6570\u533a\u95f4\uff0c\u901a\u8fc7GPU\u52a0\u901f\u5b9e\u73b0\u957f\u5bc6\u94a5\u7684\u5e76\u884c\u52a0\u5bc6\u89e3\u5bc6\u8ba1\u7b97", "result": "3P-ADMM-PC2\u7b97\u6cd5\u5728\u5747\u65b9\u8bef\u5dee\u6027\u80fd\u4e0a\u63a5\u8fd1\u65e0\u9690\u79c1\u4fdd\u62a4\u7684\u5206\u5e03\u5f0fADMM\uff0c\u76f8\u6bd4CPU\u5b9e\u73b0\u7684\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0fADMM\u5177\u6709\u663e\u8457\u52a0\u901f\u6bd4", "conclusion": "\u63d0\u51fa\u7684GPU\u52a0\u901f3P-ADMM-PC2\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u8fb9\u7f18\u7f51\u7edc\u4e2d\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5177\u6709\u4f18\u5f02\u7684\u6027\u80fd\u548c\u52a0\u901f\u6548\u679c"}}
