<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.AR](#cs.AR) [Total: 8]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Bounded Model Checking of RISC-V Machine Code with Context-Free-Language Ordered Binary Decision Diagrams](https://arxiv.org/abs/2507.09539)
*Anna Bolotina,Christoph M. Kirsch,Stefanie Muroya Lei,Matthias Pleschinger*

Main category: cs.PL

TL;DR: 该论文探讨了符号执行在机器代码层面的可扩展性问题，提出了两种工具（rotor和bitme）来改进模型生成和有界模型检查，并通过BDDs技术优化SMT求解效率。


<details>
  <summary>Details</summary>
Motivation: 符号执行在控制流和数据流中存在状态爆炸问题，现有工具通常牺牲完整性或依赖SMT求解器。作者希望将推理完全下放到机器代码层面，并利用更高效的求解技术。

Method: 开发了rotor和bitme工具，分别用于模型生成和有界模型检查。rotor基于RISC-V整数算术建模，bitme采用BDDs（ADDs和CFLOBDDs）技术优化输入传播，减少SMT求解负担。

Result: 实验表明，现有SMT求解器表现不佳，但BDDs技术（尤其是CFLOBDDs）能显著提升求解效率，减少状态爆炸问题。

Conclusion: 通过将推理下放到机器代码层面并利用BDDs技术，可以提升符号执行的可扩展性，CFLOBDDs在减少状态爆炸方面具有潜力。

Abstract: Symbolic execution is a powerful technique for analyzing the behavior of
software yet scalability remains a challenge due to state explosion in control
and data flow. Existing tools typically aim at managing control flow
internally, often at the expense of completeness, while offloading reasoning
over data flow to SMT solvers. Moreover, reasoning typically happens on source
code or intermediate representation level to leverage structural information,
making machine code generation part of the trust base. We are interested in
changing the equation in two non-trivial ways: pushing reasoning down to
machine code level, and then offloading reasoning entirely into SMT solvers and
other, possibly more efficient solver technology. In more abstract terms, we
are asking if bit-precise reasoning technology can be made scalable on
software, and not just hardware. For this purpose, we developed two tools
called rotor and bitme for model generation and bounded model checking,
respectively. We chose RISC-V restricted to integer arithmetic as modeling
target for rotor since RISC-V integer semantics is essentially equivalent to
established SMT semantics over bitvectors and arrays of bitvectors. While
state-of-the-art SMT solvers struggle in our experiments, we have evidence that
there is potential for improvement. To show the potential, we have slightly
generalized and then implemented in bitme two types of binary decision diagrams
(BDDs): algebraic decision diagrams (ADDs) and context-free-language ordered
binary decision diagrams (CFLOBDDs). Bitme uses BDDs to propagate program input
through models, essentially generalizing constant propagation to domain
propagation. SMT solvers only get involved when model input cannot be
propagated, significanly speeding up SMT solving. We then study the impact on
state explosion of CFLOBDDs, which are potentially more scalable than ADDs.

</details>


### [2] [BeePL: Correct-by-compilation kernel extensions](https://arxiv.org/abs/2507.09883)
*Swarn Priya,Frédéric Besson,Connor Sughrue,Tim Steenvoorden,Jamie Fulford,Freek Verbeek,Binoy Ravindran*

Main category: cs.PL

TL;DR: BeePL是一种针对eBPF的领域特定语言，通过形式化验证的类型系统确保程序安全，解决了现有eBPF验证器的保守性和不健全问题。


<details>
  <summary>Details</summary>
Motivation: eBPF验证器在安全性和正确性上存在不足，既可能拒绝有效程序，也可能允许不安全行为。BeePL旨在通过形式化方法提供更可靠的解决方案。

Method: BeePL设计了形式化验证的类型系统，静态强制执行内存访问、指针使用、循环和控制流等安全属性，并在编译时插入运行时检查。

Result: BeePL的类型系统通过形式化证明确保了程序的安全性，并扩展了CompCert以生成可验证的BPF字节码。

Conclusion: BeePL为eBPF提供了一个端到端可验证的工具链，显著提升了内核扩展的安全性和可靠性。

Abstract: eBPF is a technology that allows developers to safely extend kernel
functionality without modifying kernel source code or developing loadable
kernel modules. Since the kernel governs critical system operations and
enforces isolation boundaries between user space and privileged data, any
mechanism that modifies its behavior must meet the highest standards of safety
and correctness. To this end, the eBPF toolchain includes a verifier, which
statically checks safety properties such as memory access validity, bounded
loops, and type correctness before loading the program into the kernel.
However, the existing verifier is both overly conservative in some
cases-rejecting valid programs-and unsound in others, permitting unsafe
behavior that violates the intended semantics of the kernel interface.
  To address these challenges, we introduce BeePL, a domain-specific language
for eBPF with a formally verified type system. The BeePL type system, along
with the language design, statically enforces key safety properties such as
type-correct memory access, safe pointer usage, absence of unbounded loops, and
structured control flow. These guarantees are backed by formal type soundness
proofs, ensuring that well-typed programs satisfy the safety invariants
required by the eBPF execution environment. BeePL also proves that well-typed
source programs meet critical eBPF-specific properties related to memory
safety, termination, and control flow, enabling high-level reasoning prior to
compilation. For properties not fully enforceable statically-such as dynamic
bounds and undefined behavior-BeePL inserts semantics-preserving runtime checks
during compilation. We develop a verified compilation strategy that extends
CompCert to generate BPF bytecode from BeePL programs, establishing a
principled foundation for an end-to-end verifiable toolchain for safe kernel
extensions.

</details>


### [3] [Rows and Capabilities as Modal Effects](https://arxiv.org/abs/2507.10301)
*Wenhao Tang,Sam Lindley*

Main category: cs.PL

TL;DR: 提出了一种统一的框架，用于编码、分析和比较基于行多态和能力的效果系统，通过模态效果类型解耦效果跟踪与函数。


<details>
  <summary>Details</summary>
Motivation: 理解基于行多态和能力的效果系统之间的精确关系，解决效果跟踪与其他特性（如函数）纠缠的问题。

Method: 利用模态效果类型，提出一个统一的框架，并通过宏翻译将现有系统编码到该框架中。

Result: 编码保留了类型和语义，揭示了不同效果系统中效果跟踪机制的本质，并提供了语言设计的见解。

Conclusion: 该框架为分析和比较效果系统提供了新视角，对语言设计有重要价值。

Abstract: Effect handlers allow programmers to model and compose computational effects
modularly. Effect systems statically guarantee that all effects are handled.
Several recent practical effect systems are based on either row polymorphism or
capabilities. However, there remains a gap in understanding the precise
relationship between effect systems with such disparate foundations. The main
difficulty is that in both row-based and capability-based systems, effect
tracking is typically entangled with other features such as functions.
  We propose a uniform framework for encoding, analysing, and comparing effect
systems. Our framework exploits and generalises modal effect types, a recent
novel effect system which decouples effect tracking from functions via
modalities. Modalities offer fine-grained control over when and how effects are
tracked, enabling us to express different strategies for effect tracking. We
give encodings as macro translations from existing row-based and
capability-based effect systems into our framework and show that these
encodings preserve types and semantics. Our encodings reveal the essence of
effect tracking mechanisms in different effect systems, enable a direct
analysis on their differences, and provide valuable insights on language
design.

</details>


### [4] [Orthologic Type Systems](https://arxiv.org/abs/2507.10482)
*Simon Guilloud,Viktor Kunčak*

Main category: cs.PL

TL;DR: 提出基于正交逻辑的类型系统设计，支持子类型假设下的交、并、否定类型，并扩展正交逻辑以支持单调和反单调函数，提供部分切割消除的证明系统。


<details>
  <summary>Details</summary>
Motivation: 设计支持复杂类型操作的类型系统，同时处理子类型假设。

Method: 扩展正交逻辑以支持函数符号，提出部分切割消除的证明系统，并开发子类型关系判定和类型规范化算法。

Result: 提出O(n²(1+m))的子类型关系判定算法和O(n²)的类型规范化算法。

Conclusion: 正交逻辑为复杂类型系统设计提供了有效基础，支持高效的类型操作和子类型处理。

Abstract: We propose to use orthologic as the basis for designing type systems
supporting intersection, union, and negation types in the presence of subtyping
assumptions. We show how to extend orthologic to support monotonic and
antimonotonic functions, supporting the use of type constructors in such type
systems. We present a proof system for orthologic with function symbols,
showing that it admits partial cut elimination. Using these insights, we
present an $\mathcal O(n^2(1+m))$ algorithm for deciding the subtyping relation
under $m$ assumptions. We also show $O(n^2)$ polynomial-time normalization
algorithm, allowing simplification of types to their minimal canonical form.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [MQFQ-Sticky: Fair Queueing For Serverless GPU Functions](https://arxiv.org/abs/2507.08954)
*Alexander Fuerst,Siddharth Anil,Vishakha Dixit,Purushottam,Kulkarni,Prateek Sharma*

Main category: cs.DC

TL;DR: 论文提出了一种支持GPU加速的无服务器计算（FaaS）系统，解决了现有FaaS框架因GPU与编程模型不匹配而无法提供GPU加速的问题。通过结合公平队列和GPU内存管理，显著降低了函数延迟。


<details>
  <summary>Details</summary>
Motivation: 当前FaaS框架（如OpenWhisk）无法支持GPU加速，而许多应用（如机器学习和科学计算）需要GPU加速。GPU与FaaS编程模型的不匹配以及动态异构的工作负载带来了挑战。

Method: 设计了MQFQ-Sticky方法，结合公平队列和GPU内存管理，平衡局部性、公平性和延迟。

Result: 实验表明，MQFQ-Sticky比现有GPU和CPU队列策略降低了2倍至20倍的函数延迟。

Conclusion: 论文提出的方法有效解决了FaaS中GPU加速的挑战，显著提升了性能。

Abstract: Hardware accelerators like GPUs are now ubiquitous in data centers, but are
not fully supported by common cloud abstractions such as Functions as a Service
(FaaS). Many popular and emerging FaaS applications such as machine learning
and scientific computing can benefit from GPU acceleration. However, FaaS
frameworks (such as OpenWhisk) are not capable of providing this acceleration
because of the impedance mismatch between GPUs and the FaaS programming model,
which requires virtualization and sandboxing of each function. The challenges
are amplified due to the highly dynamic and heterogeneous FaaS workloads. This
paper presents the design and implementation of a FaaS system for providing GPU
acceleration in a black-box manner (without modifying function code). Running
small functions in containerized sandboxes is challenging due to limited GPU
concurrency and high cold-start overheads, resulting in heavy queueing of
function invocations. We show how principles from I/O scheduling, such as fair
queuing and anticipatory scheduling, can be translated to function scheduling
on GPUs. We develop MQFQ-Sticky, an integrated fair queueing and GPU memory
management approach, which balances the tradeoffs between locality, fairness,
and latency. Empirical evaluation on a range of workloads shows that it reduces
function latency by 2x to 20x compared to existing GPU and CPU queueing
policies.

</details>


### [6] [Lightweight Federated Learning over Wireless Edge Networks](https://arxiv.org/abs/2507.09546)
*Xiangwang Hou,Jingjing Wang,Jun Du,Chunxiao Jiang,Yong Ren,Dusit Niyato*

Main category: cs.DC

TL;DR: 本文提出了一种轻量级联邦学习框架（LTFL），结合无线传输功率控制、模型剪枝和梯度量化，优化了联邦学习在无线网络中的部署。


<details>
  <summary>Details</summary>
Motivation: 随着智能设备连接无线网络的指数增长，数据量激增，传统集中式机器学习存在通信开销和隐私问题，联邦学习（FL）成为替代方案，但其在无线网络中的实际部署仍具挑战性。

Method: 提出LTFL框架，整合无线传输功率控制、模型剪枝和梯度量化，推导FL收敛间隙的闭式表达式，并基于此构建优化问题以最小化收敛间隙。

Result: 通过闭式解确定最优模型剪枝比例和梯度量化水平，并采用贝叶斯优化进行传输功率控制，实验表明LTFL优于现有方案。

Conclusion: LTFL框架有效解决了联邦学习在无线网络中的部署挑战，显著提升了性能。

Abstract: With the exponential growth of smart devices connected to wireless networks,
data production is increasing rapidly, requiring machine learning (ML)
techniques to unlock its value. However, the centralized ML paradigm raises
concerns over communication overhead and privacy. Federated learning (FL)
offers an alternative at the network edge, but practical deployment in wireless
networks remains challenging. This paper proposes a lightweight FL (LTFL)
framework integrating wireless transmission power control, model pruning, and
gradient quantization. We derive a closed-form expression of the FL convergence
gap, considering transmission error, model pruning error, and gradient
quantization error. Based on these insights, we formulate an optimization
problem to minimize the convergence gap while meeting delay and energy
constraints. To solve the non-convex problem efficiently, we derive closed-form
solutions for the optimal model pruning ratio and gradient quantization level,
and employ Bayesian optimization for transmission power control. Extensive
experiments on real-world datasets show that LTFL outperforms state-of-the-art
schemes.

</details>


### [7] [Intelligent Task Management via Dynamic Multi-region Division in LEO Satellite Networks](https://arxiv.org/abs/2507.09926)
*Zixuan Song,Zhishu Shen,Xiaoyu Zheng,Qiushi Zheng,Zheng Lei,Jiong Jin*

Main category: cs.DC

TL;DR: 本文提出了一种动态多区域划分框架，用于优化LEO卫星网络中的任务管理，通过遗传算法和MA-DDPG技术减少任务延迟并平衡资源利用。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络作为6G系统的关键组成部分，面临资源有限和任务分布不均的挑战，需要高效的任务管理方案。

Method: 提出动态多区域划分框架，结合遗传算法优化区域划分，并使用MA-DDPG进行自适应路由和任务分配。

Result: 仿真结果表明，该框架在任务延迟、能耗和完成率方面优于其他方法。

Conclusion: 该框架为LEO卫星网络的任务管理提供了高效解决方案，具有实际应用潜力。

Abstract: As a key complement to terrestrial networks and a fundamental component of
future 6G systems, Low Earth Orbit (LEO) satellite networks are expected to
provide high-quality communication services when integrated with ground-based
infrastructure, thereby attracting significant research interest. However, the
limited satellite onboard resources and the uneven distribution of
computational workloads often result in congestion along inter-satellite links
(ISLs) that degrades task processing efficiency. Effectively managing the
dynamic and large-scale topology of LEO networks to ensure balanced task
distribution remains a critical challenge. To this end, we propose a dynamic
multi-region division framework for intelligent task management in LEO
satellite networks. This framework optimizes both intra- and inter-region
routing to minimize task delay while balancing the utilization of computational
and communication resources. Based on this framework, we propose a dynamic
multi-region division algorithm based on the Genetic Algorithm (GA), which
adaptively adjusts the size of each region based on the workload status of
individual satellites. Additionally, we incorporate an adaptive routing
algorithm and a task splitting and offloading scheme based on Multi-Agent Deep
Deterministic Policy Gradient (MA-DDPG) to effectively accommodate the arriving
tasks. Simulation results demonstrate that our proposed framework outperforms
comparative methods in terms of the task delay, energy consumption per task,
and task completion rate.

</details>


### [8] [EAT: QoS-Aware Edge-Collaborative AIGC Task Scheduling via Attention-Guided Diffusion Reinforcement Learning](https://arxiv.org/abs/2507.10026)
*Zhifei Xu,Zhiqing Tang,Jiong Lou,Zhi Yao,Xuan Xie,Tian Wang,Yinglong Wang,Weijia Jia*

Main category: cs.DC

TL;DR: 论文提出了一种QoS感知的边缘协作AIGC任务调度算法（EAT），通过分段任务和强化学习优化边缘服务器的资源利用，显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 解决边缘服务器部署AIGC服务时资源利用不足、延迟与质量平衡不佳的问题。

Method: 1) 将AIGC任务分段并调度到不同边缘服务器；2) 提出基于强化学习的EAT算法，利用注意力层和扩散策略网络优化调度；3) 开发任务调度系统。

Result: 实验显示EAT算法相比基线方法可降低推理延迟高达56%。

Conclusion: EAT算法有效优化了边缘服务器的资源利用和任务调度性能。

Abstract: The growth of Artificial Intelligence (AI) and large language models has
enabled the use of Generative AI (GenAI) in cloud data centers for diverse
AI-Generated Content (AIGC) tasks. Models like Stable Diffusion introduce
unavoidable delays and substantial resource overhead, which are unsuitable for
users at the network edge with high QoS demands. Deploying AIGC services on
edge servers reduces transmission times but often leads to underutilized
resources and fails to optimally balance inference latency and quality. To
address these issues, this paper introduces a QoS-aware
\underline{E}dge-collaborative \underline{A}IGC \underline{T}ask scheduling
(EAT) algorithm. Specifically: 1) We segment AIGC tasks and schedule patches to
various edge servers, formulating it as a gang scheduling problem that balances
inference latency and quality while considering server heterogeneity, such as
differing model distributions and cold start issues. 2) We propose a
reinforcement learning-based EAT algorithm that uses an attention layer to
extract load and task queue information from edge servers and employs a
diffusion-based policy network for scheduling, efficiently enabling model
reuse. 3) We develop an AIGC task scheduling system that uses our EAT algorithm
to divide tasks and distribute them across multiple edge servers for
processing. Experimental results based on our system and large-scale
simulations show that our EAT algorithm can reduce inference latency by up to
56\% compared to baselines. We release our open-source code at
https://github.com/zzf1955/EAT.

</details>


### [9] [ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism](https://arxiv.org/abs/2507.10069)
*Zedong Liu,Shenggan Cheng,Guangming Tan,Yang You,Dingwen Tao*

Main category: cs.DC

TL;DR: 论文提出了一种名为ElasticMM的多模态大语言模型（MLLM）服务系统，通过弹性多模态并行（EMP）技术优化推理效率，显著降低首次令牌延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前紧密耦合的服务架构无法区分混合请求类型或适应不同推理阶段的并行策略，导致首次令牌延迟增加和资源利用率低下。

Method: 提出EMP范式，开发ElasticMM系统，包括模态感知负载均衡、弹性分区调度和统一多模态前缀缓存等技术。

Result: 实验表明，ElasticMM优于现有系统，首次令牌延迟降低4.2倍，吞吐量提高3.2-4.5倍。

Conclusion: ElasticMM通过弹性适应资源异构性，显著提升了MLLM的服务效率。

Abstract: Multimodal large language models (MLLMs) extend LLMs to handle images,
videos, and audio by incorporating feature extractors and projection modules.
However, these additional components -- combined with complex inference
pipelines and heterogeneous workloads -- introduce significant inference
overhead. Therefore, efficiently serving MLLMs remains a major challenge.
Current tightly coupled serving architectures struggle to distinguish between
mixed request types or adapt parallelism strategies to different inference
stages, leading to increased time-to-first-token (TTFT) latency and poor
resource utilization. To address this, we propose Elastic Multimodal
Parallelism (EMP), a new serving paradigm that elastically adapts to resource
heterogeneity across request types and inference stages. Building upon EMP, we
develop ElasticMM, an MLLM serving system that (1) separates requests into
independent modality groups with dynamic resource allocation via a
modality-aware load balancer; (2) decouples inference stages and enables
parallelism adjustment and adaptive scaling via elastic partition scheduling;
and (3) improves inference efficiency through unified multimodal prefix caching
and non-blocking encoding. Experiments on diverse real-world datasets show that
ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by
up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level
objectives (SLOs).

</details>


### [10] [Large-Scale Graph Building in Dynamic Environments: Low Latency and High Quality](https://arxiv.org/abs/2507.10139)
*Filipe Miguel Gonçalves de Almeida,CJ Carey,Hendrik Fichtenberger,Jonathan Halcrow,Silvio Lattanzi,André Linhares,Tao Meng,Ashkan Norouzi-Fard,Nikos Parotsidis,Bryan Perozzi,David Simcha*

Main category: cs.DC

TL;DR: 论文介绍了Dynamic Grale Using ScaNN（Dynamic GUS），一个继承Grale优势并支持动态更新的系统，适用于低延迟场景。


<details>
  <summary>Details</summary>
Motivation: 现有工具Grale适用于离线环境，但无法满足动态数据快速更新的需求；而现有的近似最近邻（ANN）系统又局限于单一嵌入相似性。

Method: 提出Dynamic GUS系统，结合Grale的质量和动态更新能力，实现每请求毫秒级延迟。

Result: 系统已在Google多个场景部署，例如Android安全与隐私领域，能4倍速度更快捕获有害应用。

Conclusion: Dynamic GUS成功解决了动态数据低延迟更新的需求，扩展了Grale的应用范围。

Abstract: Learning and constructing large-scale graphs has attracted attention in
recent decades, resulting in a rich literature that introduced various systems,
tools, and algorithms. Grale is one of such tools that is designed for offline
environments and is deployed in more than 50 different industrial settings at
Google. Grale is widely applicable because of its ability to efficiently learn
and construct a graph on datasets with multiple types of features. However, it
is often the case that applications require the underlying data to evolve
continuously and rapidly and the updated graph needs to be available with low
latency. Such setting make the use of Grale prohibitive. While there are
Approximate Nearest Neighbor (ANN) systems that handle dynamic updates with low
latency, they are mostly limited to similarities over a single embedding.
  In this work, we introduce a system that inherits the advantages and the
quality of Grale, and maintains a graph construction in a dynamic setting with
tens of milliseconds of latency per request. We call the system Dynamic Grale
Using ScaNN (Dynamic GUS). Our system has a wide range of applications with
over 10 deployments at Google. One of the applications is in Android Security
and Privacy, where Dynamic Grale Using ScaNN enables capturing harmful
applications 4 times faster, before they can reach users.

</details>


### [11] [Past-Future Scheduler for LLM Serving under SLA Guarantees](https://arxiv.org/abs/2507.10150)
*Ruihao Gong,Shihao Bai,Siyu Wu,Yunqian Fan,Zaijun Wang,Xiuhong Li,Hailong Yang,Xianglong Liu*

Main category: cs.DC

TL;DR: 论文提出了一种名为Past-Future的调度器，通过精确估计请求的内存需求，优化LLM服务框架的吞吐量，并开发了高性能框架LightLLM。


<details>
  <summary>Details</summary>
Motivation: 现有调度器因输出长度多样性导致内存估计不准确，影响吞吐量和SLA保证。

Method: 提出Past-Future调度器，结合历史输出长度分布和未来时间点内存占用计算，精确估计峰值内存需求。

Result: LightLLM框架实现该调度器，在重负载下吞吐量提升2-3倍。

Conclusion: Past-Future调度器有效平衡队列与驱逐，提升LLM服务性能，LightLLM开源促进研究。

Abstract: The exploration and application of Large Language Models (LLMs) is thriving.
To reduce deployment costs, continuous batching has become an essential feature
in current service frameworks. The effectiveness of continuous batching relies
on an accurate estimate of the memory requirements of requests. However, due to
the diversity in request output lengths, existing frameworks tend to adopt
aggressive or conservative schedulers, which often result in significant
overestimation or underestimation of memory consumption. Consequently, they
suffer from harmful request evictions or prolonged queuing times, failing to
achieve satisfactory throughput under strict Service Level Agreement (SLA)
guarantees (a.k.a. goodput), across various LLM application scenarios with
differing input-output length distributions. To address this issue, we propose
a novel Past-Future scheduler that precisely estimates the peak memory
resources required by the running batch via considering the historical
distribution of request output lengths and calculating memory occupancy at each
future time point. It adapts to applications with all types of input-output
length distributions, balancing the trade-off between request queuing and
harmful evictions, thereby consistently achieving better goodput. Furthermore,
to validate the effectiveness of the proposed scheduler, we developed a
high-performance LLM serving framework, LightLLM, that implements the
Past-Future scheduler. Compared to existing aggressive or conservative
schedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\times$
higher goodput than other schedulers under heavy loads. LightLLM is open source
to boost the research in such direction (https://github.com/ModelTC/lightllm).

</details>


### [12] [Cross-Timeslot Optimization for Distributed GPU Inference Using Reinforcement Learning](https://arxiv.org/abs/2507.10259)
*Chengze Du,Zhiwei Yu,Heng Xu,Haojie Wang,Bo liu,Jialong Li*

Main category: cs.DC

TL;DR: 论文提出了一种名为TORTA的时空调度框架，通过两层架构优化分布式GPU推理基础设施的资源调度，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有调度系统仅依赖当前系统状态做决策，忽视了任务需求和资源可用性的动态变化，导致GPU利用率低、任务迁移开销高和系统响应性差。

Method: TORTA采用两层设计：宏观调度器利用强化学习和最优传输协调跨区域任务分配；微观分配器优化区域内任务到服务器的分配以减少延迟和切换成本。

Result: 实验表明，TORTA在多种网络拓扑下平均推理响应时间降低15%，负载均衡提升4-5%，总运营成本减少10-20%。

Conclusion: TORTA通过时空调度框架有效解决了动态工作负载下的资源调度问题，显著提升了系统性能和经济性。

Abstract: The rapid growth of large language model (LLM) services imposes increasing
demands on distributed GPU inference infrastructure. Most existing scheduling
systems rely on the current system state to make decisions, without considering
how task demand and resource availability evolve over time. This lack of
temporal awareness leads to inefficient GPU utilization, high task migration
overhead, and poor system responsiveness under dynamic workloads. In this work,
we identify the fundamental limitations of these instantaneous-state-only
scheduling approaches and propose Temporal Optimal Resource scheduling via
Two-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling
framework that captures both long-term workload patterns and short-term
execution constraints. It adopts a two-layer design: a macro-level scheduler
leverages reinforcement learning and optimal transport to coordinate
inter-region task distribution, while a micro-level allocator refines
task-to-server assignments within each region to reduce latency and switching
costs. Experimental results across multiple network topologies show that TORTA
reduces average inference response time by up to 15\%, improves load balance by
approximately 4-5\%, and cuts total operational cost by 10-20\% compared to
state-of-the-art baseline methods.

</details>


### [13] [FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline](https://arxiv.org/abs/2507.10367)
*Jingwei Xu,Junbin Kang,Mingkai Dong,Mingyu Liu,Lu Zhang,Shaohong Guo,Ziyan Qiu,Mingzhen You,Ziyi Tian,Anqi Yu,Tianhong Ding,Xinwei Hu,Haibo Chen*

Main category: cs.DC

TL;DR: FalconFS是一种针对深度学习管道优化的分布式文件系统，采用无状态客户端架构，通过服务器端路径解析和并发请求合并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统客户端元数据缓存在深度学习管道中效率低下且占用内存资源，因此需要一种更高效的解决方案。

Method: FalconFS采用服务器端路径解析（混合元数据索引和惰性命名空间复制）和并发请求合并技术，同时提供VFS快捷部署。

Result: 相比CephFS和Lustre，FalconFS在小文件读写和深度学习模型训练中分别实现了5.72倍和12.81倍的吞吐量提升。

Conclusion: FalconFS在华为自动驾驶系统中成功部署并运行一年，验证了其高效性和实用性。

Abstract: Client-side metadata caching has long been considered an effective method for
accelerating metadata operations in distributed file systems (DFSs). However,
we have found that client-side state (e.g., caching) is not only ineffective
but also consumes valuable memory resources in the deep learning pipelines. We
thus propose FalconFS, a DFS optimized for deep learning pipelines with the
stateless-client architecture. Specifically, instead of performing client-side
path resolution and caching, FalconFS efficiently resolves paths on the server
side using hybrid metadata indexing and lazy namespace replication. FalconFS
also boosts server concurrency with concurrent request merging and provides
easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show
that FalconFS achieves up to 5.72$\times$ throughput for small file read/write
and up to 12.81$\times$ throughput for deep learning model training. FalconFS
has been running in Huawei autonomous driving system's production environment
with 10,000 NPUs for one year.

</details>


### [14] [Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters](https://arxiv.org/abs/2507.10392)
*Runsheng Benson Guo,Utkarsh Anand,Khuzaima Daudjee,Rathijit Sen*

Main category: cs.DC

TL;DR: Zorse系统通过整合流水线并行和数据并行，优化异构GPU集群上的LLM训练，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决异构GPU集群训练LLM时的负载均衡、内存优化和通信效率问题。

Method: 提出Zorse系统，结合流水线并行和数据并行，支持灵活分区和异构GPU集成，并配备自动配置规划器。

Result: Zorse在异构训练场景中显著优于现有系统。

Conclusion: Zorse为异构集群上的高效LLM训练提供了可行方案。

Abstract: Large language models (LLMs) require vast amounts of GPU compute to train,
but limited availability and high costs of GPUs make homogeneous clusters
impractical for many organizations. Instead, assembling heterogeneous clusters
by pooling together GPUs of different generations allows them to achieve higher
aggregate compute and make use of all available GPUs. However, training on
heterogeneous clusters presents several challenges, including load balancing
across GPUs, optimizing memory usage to accommodate varying memory capacities,
and ensuring communication-efficient training over diverse network
interconnects potentially spanning multiple datacenters. In this paper, we make
the case that efficient training on heterogeneous clusters requires (1) the
integration of pipeline parallelism and data parallelism in a manner that is
both communication- and memory-efficient, and (2) a more adaptable
configuration of pipeline and data parallelism, which includes the capability
to flexibly partition GPUs into asymmetric pipeline parallel stages and to
incorporate heterogeneous GPUs within the same data parallelism group. We
propose Zorse, the first system to unify all these capabilities while
incorporating a planner that automatically configures training strategies for a
given workload. Our evaluation shows that Zorse significantly outperforms
state-of-the-art systems in heterogeneous training scenarios.

</details>


### [15] [Consensus, Inconsistency, Emergence: what's paraconsistency got to do with it?](https://arxiv.org/abs/2507.10413)
*Gabriel Rocha*

Main category: cs.DC

TL;DR: 本文探讨了异步分布式系统中的共识问题，指出FLP不可能定理在广义计算定义下依然成立，并通过复杂系统理论分析了不一致性可能是共识的涌现特征。同时，研究了超一致逻辑中平凡性的涌现可能性，并讨论了开发支持超一致推理的共识算法的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究共识问题在异步分布式系统中的理论限制，特别是FLP不可能定理的扩展及其在复杂系统框架下的新视角。

Method: 通过广义计算定义验证FLP定理的普适性，利用复杂系统理论分析系统相变和不一致性的涌现，并探讨超一致逻辑的平凡性。

Result: FLP不可能定理在广义计算下依然成立；不一致性可能是共识的涌现特征；超一致逻辑中可能出现平凡性。

Conclusion: 共识问题在复杂系统中具有更深层次的理论挑战，开发支持超一致推理的算法可能是未来研究方向。

Abstract: The consensus problem, briefly stated, consists of having processes in an
asynchronous distributed system agree on a value. It is widely known that the
consensus problem does not have a deterministic solution that ensures both
termination and consistency, if there is at least one faulty process in the
system. This result, known as the FLP impossibility theorem, led to several
generalizations and developments in theoretical distributed computing. This
paper argues that the FLP impossibility theorem holds even under a generalized
definition of computation through oracles. Furthermore, using a theoretical
machinery from complex systems, this paper also posits that inconsistency may
be an emergent feature of consensus over distributed systems by examining how a
system transitions phases. Under the same complex systems framework, this paper
examines paraconsistent logics, arguing that while inconsistency is not an
emergent feature for these logics, triviality may be. Lastly, some attention is
given to the possibility of developing consensus algorithms capable of
paraconsistent reasoning.

</details>


### [16] [Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout](https://arxiv.org/abs/2507.10430)
*Ji Liu,Beichen Ma,Yang Zhou,Jingbo Zhou,Ruoming Jin,Dejing Dou,Huaiyu Dai,Haixun Wang,Patrick Valduriez*

Main category: cs.DC

TL;DR: FedDHAD框架通过动态异构模型聚合和自适应丢弃解决联邦学习中的数据分布不均和设备异构问题，显著提升准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据分布不均（非IID）和设备异构导致模型准确率下降和收敛速度慢，需要新的解决方案。

Method: 提出FedDH动态调整模型聚合权重以应对数据异构，FedAD通过神经元自适应操作提升效率和准确性。

Result: FedDHAD在准确性（最高提升6.7%）、效率（最高快2.02倍）和计算成本（最高降低15.0%）上优于现有方法。

Conclusion: FedDHAD框架有效解决了联邦学习中的异构问题，显著提升了性能。

Abstract: Federated Learning (FL) is a promising distributed machine learning approach
that enables collaborative training of a global model using multiple edge
devices. The data distributed among the edge devices is highly heterogeneous.
Thus, FL faces the challenge of data distribution and heterogeneity, where
non-Independent and Identically Distributed (non-IID) data across edge devices
may yield in significant accuracy drop. Furthermore, the limited computation
and communication capabilities of edge devices increase the likelihood of
stragglers, thus leading to slow model convergence. In this paper, we propose
the FedDHAD FL framework, which comes with two novel methods: Dynamic
Heterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH
dynamically adjusts the weights of each local model within the model
aggregation process based on the non-IID degree of heterogeneous data to deal
with the statistical data heterogeneity. FedAD performs neuron-adaptive
operations in response to heterogeneous devices to improve accuracy while
achieving superb efficiency. The combination of these two methods makes FedDHAD
significantly outperform state-of-the-art solutions in terms of accuracy (up to
6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to
15.0% smaller).

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [CEO-DC: An Actionable Framework to Close the Carbon Gap in HPC Data Centers](https://arxiv.org/abs/2507.08923)
*Rubén Rodríguez Álvarez,Denisa-Andreea Constantinescu,Miguel Peón-Quirós,David Atienza*

Main category: cs.AR

TL;DR: CEO-DC模型通过综合考虑成本、碳排放和计算需求，为数据中心提供可持续的采购和升级策略，但当前碳价格和经济激励不足，限制了其效果。


<details>
  <summary>Details</summary>
Motivation: 数据中心的快速扩张导致能源消耗和碳排放激增，现有研究未能全面考虑经济激励和计算需求增长，因此需要一种综合模型来优化碳和经济目标。

Method: 提出CEO-DC模型，结合成本、碳排放和计算需求，制定采购和升级策略，并通过AI案例研究验证其效果。

Result: 研究发现，4年周期升级旧设备可减少排放，但需经济激励；当前碳价格在多数国家不足以推动升级；优化能效可能增加碳价格需求。

Conclusion: CEO-DC为数据中心、设计者和政策制定者提供了升级时机、可持续增长限制和激励优化的实用建议。

Abstract: The rapid expansion of data centers (DCs) to support large-scale AI and
scientific workloads is driving unsustainable growth in energy consumption and
greenhouse gas emissions. While successive generations of hardware platforms
have improved performance and energy efficiency, the question remains whether
new, more efficient platforms can realistically offset the rising emissions
associated with increasing demand. Prior studies often overlook the complex
trade-offs in such transitions by failing to account for both the economic
incentives and the projected compute demand growth over the operational
lifetime of the devices. In response, we present CEO-DC, an integrated model
and decision-making methodology for Carbon and Economy Optimization in Data
Centers. CEO-DC models the competing forces of cost, carbon, and compute demand
to guide optimal platform procurement and replacement strategies. We propose
metrics to steer procurement, platform design, and policy decisions toward
sustainable DC technologies. Given current platform trends, our AI case study
using CEO-DC shows that upgrading legacy devices on a 4-year cycle reduces
total emissions. However, these upgrades fail to scale with DC demand growth
trends without increasing total emissions in over 44% of cases, and require
economic incentives for adoption in over 72%. Furthermore, current carbon
prices are insufficient to motivate upgrades in 9 out of the 14 countries with
the highest number of DCs globally. We also find that optimizing platforms for
energy efficiency at the expense of latency can increase the carbon price
required to justify their adoption. In summary, CEO-DC provides actionable
insights for DC architects, platform designers, and policymakers by timing
legacy platform upgrades, constraining DC growth to sustainable levels,
optimizing platform performance-to-cost ratios, and increasing incentives.

</details>


### [18] [Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference](https://arxiv.org/abs/2507.09010)
*Chun-Ting Chen,HanGyeol Mun,Jian Meng,Mohamed S. Abdelfattah,Jae-sun Seo*

Main category: cs.AR

TL;DR: 本文提出了一种用于边缘LLM推理的混合脉动阵列（HSA）加速器，通过优化数据流和量化技术，显著提高了推理效率和能效。


<details>
  <summary>Details</summary>
Motivation: 边缘LLM推理需要高面积效率和低外部内存访问（EMA），同时保持高能效。

Method: 采用混合脉动阵列（HSA）架构，结合MXINT4权重量化和优化数据流，减少EMA和去量化开销。

Result: 在1.3B LLM上实现了247/117（token/s/mm2），性能提升2.45x/13.5x，同时保持高能效。

Conclusion: 该加速器在边缘LLM推理中表现出色，显著提升了效率和性能。

Abstract: Edge inference for large language models (LLM) offers secure, low-latency,
and cost-effective inference solutions. We emphasize that an edge accelerator
should achieve high area efficiency and minimize external memory access (EMA)
during the memory-bound decode stage, while maintaining high energy efficiency
during the compute intensive prefill stage. This paper proposes an edge LLM
inference accelerator featuring a hybrid systolic array (HSA) architecture that
optimizes inference efficiency in both stages. To further reduce EMA, we adopt
MXINT4 weight quantization and propose an optimized dataflow tailored for HSA,
ensuring negligible dequantization overhead and achieving 100% hardware
utilization with minimal accuracy loss under edge DRAM bandwidth constraints.
For non-linear operations, we incorporate optimized root mean square
normalization (RMSNorm) and rotary position embedding (RoPE) units, reducing
their latency, area, and memory access overhead while enabling end-to-end
inference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while
running a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x
improvement over existing approaches, while maintaining superior energy
efficiency in token generation.

</details>


### [19] [SLIM: A Heterogeneous Accelerator for Edge Inference of Sparse Large Language Model via Adaptive Thresholding](https://arxiv.org/abs/2507.09201)
*Weihong Xu,Haein Choi,Po-kai Hsu,Shimeng Yu,Tajana Rosing*

Main category: cs.AR

TL;DR: SLIM是一种算法-硬件协同设计，通过利用LLM的稀疏性，显著降低了边缘设备上的资源消耗，实现了高效的LLM推理。


<details>
  <summary>Details</summary>
Motivation: 由于LLM模型体积大且内存操作密集，在资源受限的边缘设备上高效推理具有挑战性。现有加速器未能充分利用LLM操作中的稀疏性。

Method: SLIM采用自适应阈值算法动态配置稀疏性，结合近存储处理（NSP）和内存处理（PIM）的异构硬件架构。

Result: SLIM在吞吐量上比SSD-GPU系统提升13-18倍，能效比DRAM-GPU系统高9-10倍，同时保持低延迟。

Conclusion: SLIM为边缘计算环境提供了高效、低成本的LLM部署方案。

Abstract: Large language models (LLMs) have demonstrated exceptional proficiency in
understanding and generating human language, but efficient inference on
resource-constrained embedded devices remains challenging due to large model
sizes and memory-intensive operations in feedforward network (FFN) and
multi-head attention (MHA) layers. While existing accelerators offload LLM
inference to expensive heterogeneous computing systems, they fail to exploit
the significant sparsity inherent in LLM operations, leaving hardware resources
underutilized. We propose SLIM, an algorithm-hardware co-design optimized for
sparse LLM serving on edge devices. SLIM exploits LLM sparsity through an
adaptive thresholding algorithm that enables runtime-configurable sparsity with
negligible accuracy loss, fetching only activated neurons to dramatically
reduce data movement. Our heterogeneous hardware architecture strategically
combines near-storage processing (NSP) and processing-in-memory (PIM): FFN
weights are stored in high-density 3D NAND and computed using NSP units, while
memory-intensive MHA operations are processed in PIM modules. This design
significantly reduces memory footprint, data movement, and energy consumption.
Our comprehensive evaluation demonstrates SLIM's effectiveness, achieving
13-18x throughput improvements over SSD-GPU systems and 9-10x better energy
efficiency over DRAM-GPU systems while maintaining low latency, making
cost-effective LLM deployment viable for edge computing environments.

</details>


### [20] [Tools and Methodologies for System-Level Design](https://arxiv.org/abs/2507.09660)
*Shuvra S. Bhattacharyya,Marilyn Wolf*

Main category: cs.AR

TL;DR: 系统级设计对芯片设计至关重要，涉及建模、仿真、设计空间探索和验证，以视频和神经网络为例说明。


<details>
  <summary>Details</summary>
Motivation: 芯片设计周期长且容错性低，需要更全面的工具和方法来支持系统级设计。

Method: 通过建模、仿真、设计空间探索和验证工具，研究计算模型和硬件/软件协同设计。

Result: 系统级设计工具能够支持功能验证、性能分析和能效评估，视频和神经网络应用展示了其重要性。

Conclusion: 系统级设计工具和方法对复杂芯片设计至关重要，特别是在高性能应用领域。

Abstract: System-level design, once the province of board designers, has now become a
central concern for chip designers. Because chip design is a less forgiving
design medium -- design cycles are longer and mistakes are harder to correct --
system-on-chip designers need a more extensive tool suite than may be used by
board designers and a variety of tools and methodologies have been developed
for system-level design of systems-on-chips (SoCs). System-level design is less
amenable to synthesis than are logic or physical design. As a result,
system-level tools concentrate on modeling, simulation, design space
exploration, and design verification. The goal of modeling is to correctly
capture the system's operational semantics, which helps with both
implementation and verification. The study of models of computation provides a
framework for the description of digital systems. Not only do we need to
understand a particular style of computation, such as dataflow, but we also
need to understand how different models of computation can reliably communicate
with each other. Design space exploration tools, such as hardware/software
co-design, develop candidate designs to understand trade-offs. Simulation can
be used not only to verify functional correctness but also to supply
performance and power/energy information for design analysis. This chapter
employs two applications -- video and neural networks -- as examples. Both are
leading-edge applications that illustrate many important aspects of
system-level design.

</details>


### [21] [Efficient FRW Transitions via Stochastic Finite Differences for Handling Non-Stratified Dielectrics](https://arxiv.org/abs/2507.09730)
*Jiechen Huang,Wenjian Yu*

Main category: cs.AR

TL;DR: 本文提出了一种名为MicroWalk的算法，用于在复杂非分层介质中实现高精度的FRW电容提取，同时保持高效率。


<details>
  <summary>Details</summary>
Motivation: 先进技术中的复杂非分层介质结构对现有FRW过渡方案的准确性提出了挑战，需要一种新的方法来处理这些情况。

Method: 提出MicroWalk算法，结合有限差分法解决过渡概率问题，并开发了一种混合策略的3-D电容求解器。

Result: 实验表明，该求解器在保持高效率的同时，显著提高了准确性（速度提升802倍）。

Conclusion: MicroWalk算法在复杂介质中实现了高精度和高效率的电容提取，优于现有FRW求解器。

Abstract: The accuracy of floating-random-walk (FRW) based capacitance extraction
stands only when the recursive FRW transitions are sampled unbiasedly according
to surrounding dielectrics. Advanced technology profiles, featuring complicated
non-stratified dielectrics, challenge the accuracy of existing FRW transition
schemes that approximate dielectrics with stratified or eight-octant patterns.
In this work, we propose an algorithm named MicroWalk, enabling accurate FRW
transitions for arbitrary dielectrics while keeping high efficiency. It is
provably unbiased and equivalent to using transition probabilities solved by
finite difference method, but at orders of magnitude lower cost (802$\times$
faster). An enhanced 3-D capacitance solver is developed with a hybrid strategy
for complicated dielectrics, combining MicroWalk with the special treatment for
the first transition cube and the analytical algorithm for stratified cubes.
Experiments on real-world structures show that our solver achieves a
significant accuracy advantage over existing FRW solvers, while preserving high
efficiency.

</details>


### [22] [Low-Cost Fuel Dispenser Prototype Using STM32 and an H-bridge motor driver](https://arxiv.org/abs/2507.09774)
*MD Zobaer Hossain Bhuiyan,Abir Bin Faruque,Mahtab Newaz,Mohammad Abdul Qayum*

Main category: cs.AR

TL;DR: 设计并开发了一种基于STM32微控制器和L298N电机驱动的低成本燃油分配系统原型，适用于偏远或小规模环境。


<details>
  <summary>Details</summary>
Motivation: 为传统高成本系统不可行的场景提供经济实惠且可扩展的燃油分配解决方案。

Method: 使用STM32微控制器作为核心控制单元，管理4x4矩阵键盘输入和16x4 LCD显示，通过L298N电机驱动控制12V DC泵电机。

Result: 系统能够根据用户输入的燃油量精确控制电机运行时间，实现燃油分配。

Conclusion: 展示了嵌入式系统在构建低成本、用户友好和节能解决方案中的潜力，未来可扩展更多功能。

Abstract: This paper presents the design and development of a low-cost fuel dispensing
system prototype based on the STM32 microcontroller and L298N motor driver. The
system aims to provide an affordable and scalable solution for fuel delivery in
remote or small-scale environments where conventional, high-cost systems are
not feasible. The core control unit is built using an STM32 microcontroller,
which manages user input through a 4x4 matrix keypad and displays operational
data on a 16x4 LCD screen via I2C communication. A 12V DC pump motor is used to
simulate the fuel dispensing mechanism, precisely controlled via the dual
H-bridge L298N motor driver. The system is powered by a 11.1V battery and is
designed for ease of deployment and portability. The keypad allows users to
input the desired fuel amount, while the system ensures accurate motor runtime
corresponding to the volume to be dispensed. This project demonstrates how
embedded systems can be leveraged to build cost-effective, user-friendly, and
energy-efficient solutions. The proposed design can be further enhanced with
flow sensors, GSM connectivity, RFID cards, and payment integration for
real-world applications in fuel stations or agricultural use.

</details>


### [23] [BitParticle: Partializing Sparse Dual-Factors to Build Quasi-Synchronizing MAC Arrays for Energy-efficient DNNs](https://arxiv.org/abs/2507.09780)
*Feilong Qiaoyuan,Jihe Wang,Zhiyu Sun,Linying Wu,Yuanhua Xiao,Danghui Wang*

Main category: cs.AR

TL;DR: 论文提出了一种利用双因子稀疏性的MAC单元设计，通过部分积控制逻辑和准同步调度方案，提高了面积和能效。


<details>
  <summary>Details</summary>
Motivation: 量化DNN中的位级稀疏性为优化MAC操作提供了潜力，但现有方法无法同时利用双因子稀疏性且调度灵活性不足。

Method: 采用部分积控制逻辑解决部分积爆炸问题，并引入准同步调度方案以提升MAC单元利用率。

Result: 精确版设计面积效率提升29.2%，近似版能效提升7.5%。

Conclusion: 该设计在保持能效的同时显著提升了面积效率，为DNN加速提供了新思路。

Abstract: Bit-level sparsity in quantized deep neural networks (DNNs) offers
significant potential for optimizing Multiply-Accumulate (MAC) operations.
However, two key challenges still limit its practical exploitation. First,
conventional bit-serial approaches cannot simultaneously leverage the sparsity
of both factors, leading to a complete waste of one factor' s sparsity. Methods
designed to exploit dual-factor sparsity are still in the early stages of
exploration, facing the challenge of partial product explosion. Second, the
fluctuation of bit-level sparsity leads to variable cycle counts for MAC
operations. Existing synchronous scheduling schemes that are suitable for
dual-factor sparsity exhibit poor flexibility and still result in significant
underutilization of MAC units. To address the first challenge, this study
proposes a MAC unit that leverages dual-factor sparsity through the emerging
particlization-based approach. The proposed design addresses the issue of
partial product explosion through simple control logic, resulting in a more
area- and energy-efficient MAC unit. In addition, by discarding less
significant intermediate results, the design allows for further hardware
simplification at the cost of minor accuracy loss. To address the second
challenge, a quasi-synchronous scheme is introduced that adds cycle-level
elasticity to the MAC array, reducing pipeline stalls and thereby improving MAC
unit utilization. Evaluation results show that the exact version of the
proposed MAC array architecture achieves a 29.2% improvement in area efficiency
compared to the state-of-the-art bit-sparsity-driven architecture, while
maintaining comparable energy efficiency. The approximate variant further
improves energy efficiency by 7.5%, compared to the exact version. Index-Terms:
DNN acceleration, Bit-level sparsity, MAC unit

</details>


### [24] [Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving](https://arxiv.org/abs/2507.10178)
*Wonung Kim,Yubin Lee,Yoonsung Kim,Jinwoo Hwang,Seongryong Oh,Jiyong Jung,Aziz Huseynov,Woong Gyu Park,Chang Hyun Park,Divya Mahajan,Jongse Park*

Main category: cs.AR

TL;DR: 论文探讨了Transformer和post-transformer LLMs的性能特点，提出了一种统一的高效服务系统Pimba，通过状态更新处理单元（SPUs）优化内存带宽限制，显著提升了生成吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer和post-transformer LLMs在长上下文推理中的计算和内存成本问题，构建一个统一的高效服务系统。

Method: 分析了Transformer和post-transformer LLMs的性能特点，设计了基于状态更新处理单元（SPUs）的Pimba系统，采用MX量化算术优化状态更新和注意力操作。

Result: Pimba在生成吞吐量上比优化的GPU和GPU+PIM系统分别提升了3.2倍和2.1倍。

Conclusion: Pimba通过统一的硬件设计有效支持了Transformer和post-transformer LLMs，显著提升了性能。

Abstract: Transformers are the driving force behind today's Large Language Models
(LLMs), serving as the foundation for their performance and versatility. Yet,
their compute and memory costs grow with sequence length, posing scalability
challenges for long-context inferencing. In response, the algorithm community
is exploring alternative architectures, such as state space models (SSMs),
linear attention, and recurrent neural networks (RNNs), which we refer to as
post-transformers. This shift presents a key challenge: building a serving
system that efficiently supports both transformer and post-transformer LLMs
within a unified framework. To address this challenge, we analyze the
performance characteristics of transformer and post-transformer LLMs. Despite
their algorithmic differences, both are fundamentally limited by memory
bandwidth under batched inference due to attention in transformers and state
updates in post-transformers. Further analyses suggest two additional insights:
(1) state update operations, unlike attention, incur high hardware cost, making
per-bank PIM acceleration inefficient, and (2) different low-precision
arithmetic methods offer varying accuracy-area tradeoffs, while we identify
Microsoft's MX as the Pareto-optimal choice. Building on these insights, we
design Pimba as an array of State-update Processing Units (SPUs), each shared
between two banks to enable interleaved access to PIM. Each SPU includes a
State-update Processing Engine (SPE) that comprises element-wise multipliers
and adders using MX-based quantized arithmetic, enabling efficient execution of
state update and attention operations. Our evaluation shows that, compared to
LLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x
higher token generation throughput, respectively.

</details>
