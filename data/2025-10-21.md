<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 26]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Latency Based Tiling](https://arxiv.org/abs/2510.15912)
*Jack Cashman*

Main category: cs.PL

TL;DR: Latency Based Tiling是一种基于系统的方法，通过三角循环特征化机器失效率缩放来快速推导近似分块解，在保持快速编译时间的同时最大化局部性。


<details>
  <summary>Details</summary>
Motivation: 解决自动调优方法虽然有效但编译时间过长的问题，提供一种硬件无关、内存安全且可移植的分块策略。

Method: 使用三角循环避免预取器失真，通过失效率缩放捕捉数据访问延迟与工作集大小的关系，利用延迟显著增加确定L1、L2、L3内存大小的近似位置，结合观察到的每次迭代数据足迹进行分块。

Result: 实现了可忽略的编译时间开销，在Rust中实现硬件无关方法，结合缓存计时技术，可在任何支持Rust的系统上运行。

Conclusion: 该方法在多进程系统的共享缓存环境下，提供了一种快速、便携的内存安全分块解决方案，适用于多面体模型的子集。

Abstract: Latency Based Tiling provides a systems based approach to deriving
approximate tiling solution that maximizes locality while maintaining a fast
compile time. The method uses triangular loops to characterize miss ratio
scaling of a machine avoiding prefetcher distortion. Miss ratio scaling
captures the relationship between data access latency and working set size with
sharp increases in latency indicating the data footprint exceeds capacity from
a cache level. Through these noticeable increases in latency we can determine
an approximate location for L1, L2, and L3 memory sizes. These sizes are
expected to be under approximations of a systems true memory sizes which is in
line with our expectations given the shared nature of cache in a multi process
system as described in defensive loop tiling. Unlike auto tuning, which can be
effective but prohibitively slow, Latency Based Tiling achieves negligible
compile time overhead. The implementation in Rust enables a hardware agnostic
approach which combined with a cache timing based techniques, yields a
portable, memory safe system running wherever Rust is supported. The tiling
strategy is applied to a subset of the polyhedral model, where loop nestings
are tiled based on both the derived memory hierarchy and the observed data
footprint per iteration.

</details>


### [2] [Typing Strictness (Extended Version)](https://arxiv.org/abs/2510.16133)
*Daniel Sainati,Joseph W. Cutler,Benjamin C. Pierce,Stephanie Weirich*

Main category: cs.PL

TL;DR: 提出了一种新的严格性定义，通过更精确描述变量使用来改进传统定义，在按名调用和按值推送设置中建立了类型理论基础，并通过逻辑关系证明其准确性。


<details>
  <summary>Details</summary>
Motivation: 严格性分析对于非严格求值语言的高效实现至关重要，但源级别的严格性推理具有挑战性和反直觉性。

Method: 基于效应和共效应类型系统文献，在按名调用和按值推送设置中建立类型理论基础，使用逻辑关系证明严格性属性的准确性，并提供系统间转换。

Result: 开发了新的严格性定义和类型系统，能够更精确地描述变量使用，并在Rocq中机械化验证了所有结果。

Conclusion: 提出的严格性定义和类型系统为严格性分析提供了更精确的理论基础，有助于改善非严格求值语言的实现效率。

Abstract: Strictness analysis is critical to efficient implementation of languages with
non-strict evaluation, mitigating much of the performance overhead of laziness.
However, reasoning about strictness at the source level can be challenging and
unintuitive. We propose a new definition of strictness that refines the
traditional one by describing variable usage more precisely. We lay
type-theoretic foundations for this definition in both call-by-name and
call-by-push-value settings, drawing inspiration from the literature on type
systems tracking effects and coeffects. We prove via a logical relation that
the strictness attributes computed by our type systems accurately describe the
use of variables at runtime, and we offer a strictness-annotation-preserving
translation from the call-by-name system to the call-by-push-value one. All our
results are mechanized in Rocq.

</details>


### [3] [SimpliPy: A Source-Tracking Notional Machine for Simplified Python](https://arxiv.org/abs/2510.16594)
*Moida Praneeth Jain,Venkatesh Choppella*

Main category: cs.PL

TL;DR: SimpliPy是一个为Python子集设计的教学性概念机器，通过精确的操作语义和静态分析来澄清程序执行中的控制流和作用域概念，并提供了交互式调试工具。


<details>
  <summary>Details</summary>
Motivation: 解决新手程序员对程序执行的误解，特别是控制流和作用域概念的理解困难。

Method: 基于精确的操作语义（显式跟踪源代码行号），结合静态分析生成控制流图和识别词法作用域，并开发了交互式web调试器来可视化执行状态。

Result: 开发了SimpliPy系统，集成了形式语义、程序分析和可视化技术，为程序理解提供了教学方法和实际工具。

Conclusion: SimpliPy成功地将形式方法应用于程序理解教学，通过语义、分析和可视化的整合为新手程序员提供了清晰的学习工具。

Abstract: Misconceptions about program execution hinder many novice programmers. We
introduce SimpliPy, a notional machine designed around a carefully chosen
Python subset to clarify core control flow and scoping concepts. Its foundation
is a precise operational semantics that explicitly tracks source code line
numbers for each execution step, making the link between code and behavior
unambiguous. Complementing the dynamic semantics, SimpliPy uses static analysis
to generate Control Flow Graphs (CFGs) and identify lexical scopes, helping
students build a structural understanding before tracing. We also present an
interactive web-based debugger built on these principles. This tool embodies
the formal techniques, visualizing the operational state (environments, stack)
and using the static CFG to animate control flow directly on the graph during
step-by-step execution. SimpliPy thus integrates formal semantics, program
analysis, and visualization to offer both a pedagogical approach and a
practical demonstration of applying formal methods to program understanding.

</details>


### [4] [JAX Autodiff from a Linear Logic Perspective (Extended Version)](https://arxiv.org/abs/2510.16883)
*Giulia Giusti,Michele Pagani*

Main category: cs.PL

TL;DR: 本文提出了将自动微分系统编码到线性λ演算中的方法，该演算与Girard的线性逻辑具有Curry-Howard对应关系，证明了编码的定性和定量正确性，并发现反向传播中的unzipping变换是可选的。


<details>
  <summary>Details</summary>
Motivation: 现有的自动微分形式化虽然能表达主要程序变换，但类型系统过于特定，不清楚其子结构逻辑是否具有独立意义。

Method: 将Autodiff编码到线性λ演算中，该演算与线性逻辑具有Curry-Howard对应关系。

Result: 证明了编码在定性和定量上的正确性，并发现unzipping变换是可选的。

Conclusion: 提出的编码方法不仅保持了自动微分的语义和性能，还揭示了某些变换的非必要性，为自动微分提供了更简洁的理论基础。

Abstract: Autodiff refers to the core of the automatic differentiation systems
developed in projects like JAX and Dex. Autodiff has recently been formalised
in a linear typed calculus by Radul et al in arXiv:2204.10923. Although this
formalisation suffices to express the main program transformations of Autodiff,
the calculus is very specific to this task, and it is not clear whether the
type system yields a substructural logic that has interest on its own.
  We propose an encoding of Autodiff into a linear $\lambda$-calculus that
enjoys a Curry-Howard correspondence with Girard's linear logic. We prove that
the encoding is sound both qualitatively (the encoded terms are extensionally
equivalent to the original ones) and quantitatively (the encoding preserves the
original work cost as described in arXiv:2204.10923). As a byproduct, we show
that unzipping, one of the transformations used to implement backpropagation in
Autodiff, is, in fact, optional.

</details>


### [5] [Introducing Linear Implication Types to $λ_{GT}$ for Computing With Incomplete Graphs](https://arxiv.org/abs/2510.17429)
*Jin Sano,Naoki Yamamoto,Kazunori Ueda*

Main category: cs.PL

TL;DR: 该研究通过在λ_GT类型系统中引入线性蕴涵，解决了对不完整图的支持问题，并消除了模式匹配中的动态类型检查需求。


<details>
  <summary>Details</summary>
Motivation: 传统指针操作复杂易错，现有类型系统在支持复杂指针数据结构的高级声明式语言设计方面仍有不足。λ_GT语言虽然通过图抽象解决了部分问题，但其类型系统不支持不完整图，且依赖动态类型检查。

Method: 在λ_GT类型系统中引入线性蕴涵，并添加新的约束条件来确保类型系统的健全性。

Result: 成功扩展了λ_GT类型系统，使其能够支持不完整图，并消除了模式匹配中的动态类型检查需求。

Conclusion: 通过引入线性蕴涵和相应约束，显著提升了λ_GT类型系统的表达能力，使其能够更安全地处理复杂的图数据结构。

Abstract: Designing programming languages that enable intuitive and safe manipulation
of data structures is a critical research challenge. Conventional destructive
memory operations using pointers are complex and prone to errors. Existing type
systems, such as affine types and shape types, address this problem towards
safe manipulation of heaps and pointers, but design of high-level declarative
languages that allow us to manipulate complex pointer data structures at a
higher level of abstraction is largely an open problem. The $\lambda_{GT}$
language, a purely functional programming language that treats hypergraphs
(hereafter referred to as graphs) as primary data structures, addresses some of
these challenges. By abstracting data with shared references and cycles as
graphs, it enables declarative operations through pattern matching and
leverages its type system to guarantee safety of these operations.
Nevertheless, the previously proposed type system of $\lambda_{GT}$ leaves two
significant open challenges. First, the type system does not support
\emph{incomplete graphs}, that is, graphs in which some elements are missing
from the graphs of user-defined types. Second, the type system relies on
dynamic type checking during pattern matching. This study addresses these two
challenges by incorporating linear implication into the $\lambda_{GT}$ type
system, while introducing new constraints to ensure its soundness.

</details>


### [6] [Insum: Sparse GPU Kernels Simplified and Optimized with Indirect Einsums](https://arxiv.org/abs/2510.17505)
*Jaeyeon Won,Willow Ahrens,Joel S. Emer,Saman Amarasinghe*

Main category: cs.PL

TL;DR: 提出了一种新的稀疏计算表达方法，通过将格式无关的稀疏张量Einsums重写为格式感知的间接Einsums，并开发Insum编译器生成高效GPU代码，在稀疏GPU应用中实现1.14x到3.81x加速。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏编译器主要针对CPU设计，很少能生成高性能GPU代码，且在稀疏和密集区域混合计算时无法有效优化密集部分。

Method: 从格式无关的稀疏张量Einsums出发，重写为格式感知的间接Einsums，通过间接索引将稀疏数据和元数据映射到密集张量操作。开发Insum编译器生成GPU代码，并设计GroupCOO和BlockGroupCOO两种固定长度稀疏格式。

Result: 在多种稀疏GPU应用中实现1.14x到3.81x的加速，相比手写实现减少202x到4491x代码行数。

Conclusion: 该方法能有效生成高性能稀疏GPU内核，显著简化编程复杂度并提升性能。

Abstract: Programming high-performance sparse GPU kernels is notoriously difficult,
requiring both substantial effort and deep expertise. Sparse compilers aim to
simplify this process, but existing systems fall short in two key ways. First,
they are primarily designed for CPUs and rarely produce high-performance GPU
code. Second, when computations involve both sparse and dense regions, these
compilers often fail to optimize the dense portions effectively. In this paper,
we propose a new approach for expressing sparse computations. We start from
format-agnostic Einsums over sparse tensors and rewrite them into
format-conscious indirect Einsums, which explicitly encode format information
by mapping sparse data and metadata onto dense tensor operations through
indirect indexing. To execute indirect Einsums, we introduce the Insum
compiler, which generates efficient GPU code for these Einsums by lowering to
the PyTorch compiler, extended to better support Tensor Core-enabled indirect
Einsums. We also present two fixed-length sparse formats, GroupCOO and
BlockGroupCOO, designed to fit naturally with indirect Einsums. Our approach
achieves 1.14x to 3.81x speedups across a range of sparse GPU applications
while reducing lines of code by 202x to 4491x compared to hand-written
implementations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI](https://arxiv.org/abs/2510.16284)
*Di Zhang*

Main category: cs.DC

TL;DR: 提出了基于MPI的并行自助法算法，通过局部统计量聚合和同步伪随机数生成解决通信开销和内存限制问题，实现大规模系统的可扩展并行自助法。


<details>
  <summary>Details</summary>
Motivation: 传统自助法在大数据集或大量重采样时计算成本过高，需要解决分布式环境中的高通信开销和内存约束问题。

Method: 使用MPI开发并行自助法算法，提出两种策略：局部统计量聚合（传输充分统计量而非完整重采样数据集）和同步伪随机数生成（当整个数据集无法存储在单个进程中时实现分布式重采样）。

Result: 开发了通信和计算复杂度的分析模型，与朴素基线方法相比，所提方法显著减少了通信量和内存使用。

Conclusion: 所提出的方法在大规模系统上实现了可扩展的并行自助法，为大数据统计推断提供了有效的解决方案。

Abstract: Bootstrapping is a powerful statistical resampling technique for estimating
the sampling distribution of an estimator. However, its computational cost
becomes prohibitive for large datasets or a high number of resamples. This
paper presents a theoretical analysis and design of parallel bootstrapping
algorithms using the Message Passing Interface (MPI). We address two key
challenges: high communication overhead and memory constraints in distributed
environments. We propose two novel strategies: 1) Local Statistic Aggregation,
which drastically reduces communication by transmitting sufficient statistics
instead of full resampled datasets, and 2) Synchronized Pseudo-Random Number
Generation, which enables distributed resampling when the entire dataset cannot
be stored on a single process. We develop analytical models for communication
and computation complexity, comparing our methods against naive baseline
approaches. Our analysis demonstrates that the proposed methods offer
significant reductions in communication volume and memory usage, facilitating
scalable parallel bootstrapping on large-scale systems.

</details>


### [8] [MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization](https://arxiv.org/abs/2510.16415)
*Rizhen Hu,Yutong He,Ran Yan,Mou Sun,Binghang Yuan,Kun Yuan*

Main category: cs.DC

TL;DR: MeCeFO是一种内存和计算效率高的容错优化算法，用于分布式大语言模型训练，能在节点故障时无缝转移训练任务，同时最小化额外开销。


<details>
  <summary>Details</summary>
Motivation: 随着分布式优化扩展到满足大语言模型训练需求，硬件故障变得越来越不可忽视。现有的容错训练方法通常引入显著的计算或内存开销，需要额外资源。

Method: MeCeFO采用三种关键算法设计：(i)Skip-connection在反向传播时丢弃多头注意力模块；(ii)Recomputation减少前馈网络的激活内存；(iii)低秩梯度近似实现前馈网络权重矩阵梯度的高效估计。

Result: 理论上，MeCeFO与传统分布式训练具有相同的收敛速率O(1/√nT)。实证显示，在高故障率下MeCeFO保持稳健性能，吞吐量仅下降4.18%，比先前SOTA方法具有5.0-6.7倍的弹性。

Conclusion: MeCeFO提供了一种高效且稳健的容错训练解决方案，显著降低了分布式大语言模型训练中的故障恢复开销。

Abstract: As distributed optimization scales to meet the demands of Large Language
Model (LLM) training, hardware failures become increasingly non-negligible.
Existing fault-tolerant training methods often introduce significant
computational or memory overhead, demanding additional resources. To address
this challenge, we propose Memory- and Computation-efficient Fault-tolerant
Optimization (MeCeFO), a novel algorithm that ensures robust training with
minimal overhead. When a computing node fails, MeCeFO seamlessly transfers its
training task to a neighboring node while employing memory- and
computation-efficient algorithmic optimizations to minimize the extra workload
imposed on the neighboring node handling both tasks. MeCeFO leverages three key
algorithmic designs: (i) Skip-connection, which drops the multi-head attention
(MHA) module during backpropagation for memory- and computation-efficient
approximation; (ii) Recomputation, which reduces activation memory in
feedforward networks (FFNs); and (iii) Low-rank gradient approximation,
enabling efficient estimation of FFN weight matrix gradients. Theoretically,
MeCeFO matches the convergence rate of conventional distributed training, with
a rate of $\mathcal{O}(1/\sqrt{nT})$, where n is the data parallelism size and
T is the number of iterations. Empirically, MeCeFO maintains robust performance
under high failure rates, incurring only a 4.18% drop in throughput,
demonstrating 5.0$\times$ to 6.7$\times$ greater resilience than previous SOTA
approaches. Codes are available at https://github.com/pkumelon/MeCeFO.

</details>


### [9] [FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference](https://arxiv.org/abs/2510.16418)
*Jian Ma,Xinchen Lyu,Jun Jiang,Longhao Zou,Chenshan Ren,Qimei Cui,Xiaofeng Tao*

Main category: cs.DC

TL;DR: FourierCompress是一种基于傅里叶变换的LLM激活压缩框架，通过利用激活在频域的稀疏性，在边缘设备上实现高效的压缩，显著减少通信开销，同时保持接近无损的推理性能。


<details>
  <summary>Details</summary>
Motivation: 协作式LLM推理在资源受限的边缘设备上受到通信瓶颈的严重限制，因为高维中间激活的传输导致带宽消耗随输出长度线性增长，现有压缩方法难以同时实现高压缩比、低重构误差和计算效率。

Method: 利用LLM激活在频域的稀疏性，特别是第一层Transformer激活在低频域的能量集中特性，通过FFT将激活变换到频域，仅保留紧凑的低频系数块，在服务器端利用共轭对称性重构信号，实现硬件加速。

Result: 在Llama 3和Qwen2.5模型上的实验表明，FourierCompress在10个常识推理数据集上保持了接近未压缩基线的性能，平均减少激活大小7.6倍，平均准确率损失小于0.3%，压缩时间比Top-k方法减少32倍以上。

Conclusion: FourierCompress在通信效率、近无损推理和压缩速度之间取得了良好平衡，为边缘设备上的LLM推理提供了一种高效的解决方案。

Abstract: Collaborative large language model (LLM) inference enables real-time,
privacy-preserving AI services on resource-constrained edge devices by
partitioning computational workloads between client devices and edge servers.
However, this paradigm is severely hindered by communication bottlenecks caused
by the transmission of high-dimensional intermediate activations, exacerbated
by the autoregressive decoding structure of LLMs, where bandwidth consumption
scales linearly with output length. Existing activation compression methods
struggle to simultaneously achieve high compression ratios, low reconstruction
error, and computational efficiency. This paper proposes FourierCompress, a
novel, layer-aware activation compression framework that exploits the
frequency-domain sparsity of LLM activations. We rigorously demonstrate that
activations from the first Transformer layer exhibit strong smoothness and
energy concentration in the low-frequency domain, making them highly amenable
to near-lossless compression via the Fast Fourier Transform (FFT).
FourierCompress transforms activations into the frequency domain, retains only
a compact block of low-frequency coefficients, and reconstructs the signal at
the server using conjugate symmetry, enabling seamless hardware acceleration on
DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10
commonsense reasoning datasets demonstrate that FourierCompress preserves
performance remarkably close to the uncompressed baseline, outperforming Top-k,
QR, and SVD. FourierCompress bridges the gap between communication efficiency
(an average 7.6x reduction in activation size), near-lossless inference (less
than 0.3% average accuracy loss), and significantly faster compression
(achieving over 32x reduction in compression time compared to Top-k via
hardware acceleration) for edge-device LLM inference.

</details>


### [10] [Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages](https://arxiv.org/abs/2510.16497)
*Pacome Simon Mbonimpa,Diane Tuyizere,Azizuddin Ahmed Biyabani,Ozan K. Tonguz*

Main category: cs.DC

TL;DR: 提出了一个用于Kinyarwanda和Swahili语言的语音转录和合成框架，采用边缘-云并行架构提升处理速度和可访问性。


<details>
  <summary>Details</summary>
Motivation: 解决东非国家技术基础设施有限、缺乏强大语言处理工具的问题，为广泛使用的Kinyarwanda和Swahili语言提供语音处理能力。

Method: 使用Whisper和SpeechT5预训练模型，采用级联机制在边缘设备和云之间分配模型推理工作负载，减少延迟和资源使用。

Result: 在边缘设备上，SpeechT5模型内存使用压缩9.5%，Whisper模型压缩14%，最大内存使用149MB。在1.7GHz CPU和1MB/s网络带宽下，270字符文本处理时间少于1分钟。

Conclusion: 所提出的级联边缘-云架构可作为STT和TTS转录的优秀平台，具有良好的准确性和响应时间。

Abstract: This paper presents a novel framework for speech transcription and synthesis,
leveraging edge-cloud parallelism to enhance processing speed and accessibility
for Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful
language processing tools for these widely spoken languages in East African
countries with limited technological infrastructure. The framework utilizes the
Whisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and
text-to-speech (TTS) translation. The architecture uses a cascading mechanism
that distributes the model inference workload between the edge device and the
cloud, thereby reducing latency and resource usage, benefiting both ends. On
the edge device, our approach achieves a memory usage compression of 9.5% for
the SpeechT5 model and 14% for the Whisper model, with a maximum memory usage
of 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with
a 1 MB/s network bandwidth, the system can process a 270-character text in less
than a minute for both speech-to-text and text-to-speech transcription. Using
real-world survey data from Kenya, it is shown that the cascaded edge-cloud
architecture proposed could easily serve as an excellent platform for STT and
TTS transcription with good accuracy and response time.

</details>


### [11] [Reimagining RDMA Through the Lens of ML](https://arxiv.org/abs/2510.16606)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: Celeris是一种面向机器学习工作负载的专用RDMA传输协议，通过移除重传和顺序交付机制，利用ML对数据丢失的容忍性来显著降低尾延迟。


<details>
  <summary>Details</summary>
Motivation: 随着分布式ML工作负载扩展到数千个GPU，传统RDMA设计的严格可靠性和顺序交付机制在高性能互连中成为性能瓶颈，导致尾延迟问题。

Method: Celeris在RDMA NIC中移除重传和顺序交付，采用尽力而为传输，保留拥塞控制，通过软件级机制（自适应超时、数据优先级）和ML流水线进行丢失恢复（如Hadamard变换）。

Result: Celeris将99百分位延迟降低高达2.3倍，BRAM使用减少67%，NIC容错能力几乎翻倍。

Conclusion: Celeris为大规模ML集群提供了具有弹性、可扩展的专用传输协议，显著提升了系统性能。

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs
connected by ultra-high-speed inter-connects, tail latency in collective
communication has emerged as a primary bottleneck. Prior RDMA designs, like
RoCE, IRN, and SRNIC, enforce strict reliability and in-order delivery, relying
on retransmissions and packet sequencing to ensure correctness. While effective
for general-purpose workloads, these mechanisms introduce complexity and
latency that scale poorly, where even rare packet losses or delays can
consistently degrade system performance. We introduce Celeris, a
domain-specific RDMA transport that revisits traditional reliability guarantees
based on ML's tolerance for lost or partial data. Celeris removes
retransmissions and in-order delivery from the RDMA NIC, enabling best-effort
transport that exploits the robustness of ML workloads. It retains congestion
control (e.g., DCQCN) and manages communication with software-level mechanisms
such as adaptive timeouts and data prioritization, while shifting loss recovery
to the ML pipeline (e.g., using the Hadamard Transform). Early results show
that Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by
67%, and nearly doubles NIC resilience to faults -- delivering a resilient,
scalable transport tailored for ML at cluster scale.

</details>


### [12] [Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++](https://arxiv.org/abs/2510.16890)
*Jiří Klepl,Martin Kruliš,Matyáš Brabec*

Main category: cs.DC

TL;DR: 提出基于C++ Noarr库的MPI新抽象层，实现布局无关的MPI应用设计，性能与传统MPI C++绑定相当但设计更灵活


<details>
  <summary>Details</summary>
Motivation: 传统MPI的纯C接口缺乏现代语言特性（如类型检查、泛型编程），限制了分布式高性能计算应用的设计灵活性

Method: 扩展C++ Noarr库，遵循其范式（一等布局和遍历抽象），实现布局无关的MPI应用设计方法

Result: 通过分布式GEMM核作为案例研究，验证了该抽象的可用性和语法简洁性，性能与现有MPI C++绑定相当

Conclusion: 提出的MPI抽象层在保持性能的同时，显著提高了分布式应用设计的灵活性

Abstract: Message Passing Interface (MPI) has been a well-established technology in the
domain of distributed high-performance computing for several decades. However,
one of its greatest drawbacks is a rather ancient pure-C interface. It lacks
many useful features of modern languages (namely C++), like basic type-checking
or support for generic code design. In this paper, we propose a novel
abstraction for MPI, which we implemented as an extension of the C++ Noarr
library. It follows Noarr paradigms (first-class layout and traversal
abstraction) and offers layout-agnostic design of MPI applications. We also
implemented a layout-agnostic distributed GEMM kernel as a case study to
demonstrate the usability and syntax of the proposed abstraction. We show that
the abstraction achieves performance comparable to the state-of-the-art MPI C++
bindings while allowing for a more flexible design of distributed applications.

</details>


### [13] [FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems](https://arxiv.org/abs/2510.16896)
*Yiming Hu*

Main category: cs.DC

TL;DR: 提出了一种用于互连多核系统的集成容错架构，通过构建稳定性指标识别可靠机器并进行定期诊断，无需额外硬件即可实现永久故障隔离和自适应任务调度。


<details>
  <summary>Details</summary>
Motivation: 解决传统双相三模冗余(TMR)在永久故障下失效的问题，以及反应式TMR(R-TMR)因依赖额外硬件而增加系统复杂性和降低容错能力的问题。

Method: 构建稳定性指标来识别可靠机器，执行定期诊断，实现永久故障隔离和自适应任务调度，无需额外硬件。

Result: 相比基线TMR减少约30%的任务负载，实现了优异的故障覆盖率和隔离精度。

Conclusion: 该方法显著提高了互连多核系统的可靠性和能效，在无需额外硬件的情况下实现了对瞬态和永久故障的有效容错。

Abstract: Two-Phase Triple Modular Redundancy TMR divides redundancy operations into
two stages, omitting part of the computation during fault-free operation to
reduce energy consumption. However, it becomes ineffective under permanent
faults, limiting its reliability in critical systems. To address this,
Reactive-TMR (R-TMR) introduces permanent fault isolation mechanisms for faulty
cores, tolerating both transient and permanent faults. Yet, its reliance on
additional hardware increases system complexity and reduces fault tolerance
when multiple cores or auxiliary modules fail. This paper proposes an
integrated fault-tolerant architecture for interconnected multicore systems. By
constructing a stability metric to identify reliable machines and performing
periodic diagnostics, the method enables permanent fault isolation and adaptive
task scheduling without extra hardware. Experimental results show that it
reduces task workload by approximately 30% compared to baseline TMR and
achieves superior fault coverage and isolation accuracy, significantly
improving both reliability and energy efficiency.

</details>


### [14] [Tutoring LLM into a Better CUDA Optimizer](https://arxiv.org/abs/2510.16933)
*Matyáš Brabec,Jiří Klepl,Michal Töpfer,Martin Kruliš*

Main category: cs.DC

TL;DR: 评估大型语言模型在生成优化CUDA代码方面的能力，通过自动和人工评估发现LLMs需要指导才能达到并行计算专家的优化水平


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，编程工具（如GitHub Copilot）在代码生成、调试和性能优化方面取得了革命性进展。本文旨在研究最新推理模型为预定义任务生成优化CUDA代码的能力

Method: 通过自动评估（正确性和加速比）和人工评估（代码审查）来评估生成的解决方案，尝试了交互式方法让LLM修复之前的错误，并比较了有无详细提示指导的效果

Result: 结果表明LLMs是相当熟练的编码器，但需要指导才能达到并行计算专家提供的优化解决方案水平

Conclusion: 大型语言模型在代码生成方面表现出色，但需要通过提供详细提示和指导来达到专家级的优化水平，交互式方法有助于改进代码质量

Abstract: Recent leaps in large language models (LLMs) caused a revolution in
programming tools (like GitHub Copilot) that can help with code generation,
debugging, and even performance optimization. In this paper, we focus on the
capabilities of the most recent reasoning models to generate optimized CUDA
code for predefined, well-known tasks. Our objective is to determine which
types of code optimizations and parallel patterns the LLMs can perform by
themselves and whether they can be improved by tutoring (providing more
detailed hints and guidelines in the prompt). The generated solutions were
evaluated both automatically (for correctness and speedup) and manually (code
reviews) to provide a more detailed perspective. We also tried an interactive
approach where the LLM can fix its previous mistakes within a session. The
results indicate that LLMs are quite skilled coders; however, they require
tutoring to reach optimized solutions provided by parallel computing experts.

</details>


### [15] [Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU Infrastructure](https://arxiv.org/abs/2510.16946)
*Erfan Darzi,Aldo Pareja,Shreeanant Bharadwaj*

Main category: cs.DC

TL;DR: 提出基于eBPF的GPU尾延迟监控系统，通过关联主机指标和GPU内部事件，实现81-88%的诊断准确率，5秒内检测延迟峰值，6-8秒完成根因分析


<details>
  <summary>Details</summary>
Motivation: 现有监控工具在共享计算环境中缺乏细粒度根因分析能力，无法有效诊断GPU尾延迟峰值问题

Method: 开发eBPF遥测系统，统一监控GPU工作负载，关联eBPF主机指标和GPU内部事件，实现系统级可观测性

Result: 系统达到81-88%诊断准确率，5秒内检测延迟峰值，6-8秒完成根因分析，100Hz采样时CPU开销仅1.21%

Conclusion: 该系统能够识别NIC争用、PCIe压力和CPU干扰等根因，为多租户GPU基础设施提供操作调试能力，无需集群范围插装

Abstract: Diagnosing GPU tail latency spikes in cloud and HPC infrastructure is
critical for maintaining performance predictability and resource utilization,
yet existing monitoring tools lack the granularity for root cause analysis in
shared computing environments. We introduce an eBPF-based telemetry system that
provides unified host-side monitoring of GPU workloads, correlating
eBPF-derived host metrics with GPU-internal events for holistic system
observability. The system achieves 81--88\% diagnostic accuracy, detects spikes
within 5 seconds, and completes root cause analysis in 6--8 seconds, operating
with 1.21\% CPU overhead at 100Hz sampling. Evaluated on distributed learning
workloads, the system identifies root causes including NIC contention, PCIe
pressure, and CPU interference, enabling operational debugging for multi-tenant
GPU infrastructure without requiring cluster-wide instrumentation.

</details>


### [16] [Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization](https://arxiv.org/abs/2510.17158)
*Daniel Nichols,Konstantinos Parasyris,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.DC

TL;DR: 提出了一种训练语言模型的方法，使其能够在推理过程中与性能工具交互，以解决代码性能优化任务。


<details>
  <summary>Details</summary>
Motivation: 语言模型在软件工程中广泛使用，但在代码性能相关任务（如优化）中表现不佳，因为这些任务依赖于环境、硬件等复杂数据，而这些信息并未直接体现在源代码中。

Method: 训练语言模型在推理过程中与性能工具交互，从而理解环境如何与代码性能相互作用。

Result: 该方法被用于训练一个最先进的GPU内核优化模型。

Conclusion: 通过让语言模型在推理过程中与性能工具交互，可以显著提升其在代码性能优化任务中的表现。

Abstract: Language models are now prevalent in software engineering with many
developers using them to automate tasks and accelerate their development. While
language models have been tremendous at accomplishing complex software
engineering tasks, there are still many areas where they fail to deliver
desirable results, for instance code performance related tasks. Tasks like
optimization depend on many complex data from the environment, hardware, etc.
that are not directly represented in source code. Recent efforts have seen
large improvements in general code modeling tasks using chain-of-thought style
reasoning, but these models still fail to comprehend how the environment
interacts with code performance. In this paper we propose a methodology to
train language models that can interact with performance tools during their
reasoning process. We then demonstrate how this methodology can be used to
train a state-of-the-art GPU kernel optimization model.

</details>


### [17] [On the Universality of Round Elimination Fixed Points](https://arxiv.org/abs/2510.17639)
*Alkida Balliu,Sebastian Brandt,Ole Gabsdil,Dennis Olivetti,Jukka Suomela*

Main category: cs.DC

TL;DR: 本文证明了轮次消除固定点不能作为证明分布式图算法下界的通用技术，特别是对于带输入的问题。虽然解决了先前已知的障碍，但发现了新的障碍。


<details>
  <summary>Details</summary>
Motivation: 解决轮次消除固定点是否可以作为证明分布式图算法下界的通用技术这一开放性问题，特别是针对需要Ω(logn)轮次的局部可检查问题。

Method: 开发了基于三幂输入的新技术来系统构建轮次消除下界，并证明某些同态问题确实可以通过轮次消除固定点来证明下界。

Result: 消除了轮次消除通用性的已知障碍，但发现了新的障碍：某些带输入的问题无法通过松弛到非平凡轮次消除固定点来证明下界。

Conclusion: 轮次消除不能作为带输入问题的通用下界证明技术，但对于无输入问题可能是通用的。同时证明了首个适用于任何轮次消除固定点问题的通用下界定理。

Abstract: Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC
2020] has drawn attention to the following open question: are round elimination
fixed points a universal technique for proving lower bounds? That is, given a
locally checkable problem $\Pi$ that requires at least $\Omega(\log n)$ rounds
in the deterministic LOCAL model, can we always find a relaxation $\Pi'$ of
$\Pi$ that is a nontrivial fixed point for the round elimination technique [see
STOC 2016, PODC 2019]? If yes, then a key part of distributed computational
complexity would be also decidable.
  The key obstacle so far has been a certain family of homomorphism problems
[ITCS 2022], which require $\Omega(\log n)$ rounds, but the only known proof is
based on Marks' technique [J.AMS 2016].
  We develop a new technique for constructing round elimination lower bounds
systematically. Using so-called tripotent inputs we show that the
aforementioned homomorphism problems indeed admit a lower bound proof that is
based on round elimination fixed points. Hence we eliminate the only known
obstacle for the universality of round elimination.
  Yet we also present a new obstacle: we show that there are some problems with
inputs that require $\Omega(\log n)$ rounds, yet there is no proof that is
based on relaxations to nontrivial round elimination fixed points. Hence round
elimination cannot be a universal technique for problems with inputs (but it
might be universal for problems without inputs).
  We also prove the first fully general lower bound theorem that is applicable
to any problem, with or without inputs, that is a fixed point in round
elimination. Prior results of this form were only able to handle certain very
restricted inputs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [Multimodal Chip Physical Design Engineer Assistant](https://arxiv.org/abs/2510.15872)
*Yun-Da Tsai,Chang-Yu Chao,Liang-Yeh Shen,Tsung-Han Lin,Haoyu Yang,Mark Ho,Yi-Chen Lu,Wen-Hao Liu,Shou-De Lin,Haoxing Ren*

Main category: cs.AR

TL;DR: 提出了一种多模态大语言模型助手(MLLMA)，不仅能预测布线拥塞，还能提供可解释的设计建议，通过结合自动特征生成和可解释偏好学习框架，在CircuitNet基准测试中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现代芯片物理设计严重依赖EDA工具，但这些工具难以提供可解释的反馈或可操作的设计改进指导，需要填补这一空白。

Method: 结合MLLM引导的遗传提示进行自动特征生成，以及跨视觉、表格和文本输入的可解释偏好学习框架，建模与拥塞相关的权衡关系。

Result: 在CircuitNet基准测试中，该方法在准确性和可解释性方面均优于现有模型，设计建议与真实设计原则一致且对工程师具有可操作性。

Conclusion: 这项工作突显了MLLMs作为交互式助手在可解释和上下文感知的物理设计优化中的潜力。

Abstract: Modern chip physical design relies heavily on Electronic Design Automation
(EDA) tools, which often struggle to provide interpretable feedback or
actionable guidance for improving routing congestion. In this work, we
introduce a Multimodal Large Language Model Assistant (MLLMA) that bridges this
gap by not only predicting congestion but also delivering human-interpretable
design suggestions. Our method combines automated feature generation through
MLLM-guided genetic prompting with an interpretable preference learning
framework that models congestion-relevant tradeoffs across visual, tabular, and
textual inputs. We compile these insights into a "Design Suggestion Deck" that
surfaces the most influential layout features and proposes targeted
optimizations. Experiments on the CircuitNet benchmark demonstrate that our
approach outperforms existing models on both accuracy and explainability.
Additionally, our design suggestion guidance case study and qualitative
analyses confirm that the learned preferences align with real-world design
principles and are actionable for engineers. This work highlights the potential
of MLLMs as interactive assistants for interpretable and context-aware physical
design optimization.

</details>


### [19] [Putting the Context back into Memory](https://arxiv.org/abs/2510.15878)
*David A. Roberts*

Main category: cs.AR

TL;DR: 提出了一种在内存地址流中编码程序上下文的方法，使内存设备能够感知软件状态，从而改善内存优化和遥测能力。


<details>
  <summary>Details</summary>
Motivation: 解决硬件缓存预取、内存调度等导致的内存访问可观测性丧失问题，让内存设备能够获取程序上下文信息以支持更好的数据移动和分层优化。

Method: 通过在内存读取地址流中以非破坏性方式编码用户可见状态作为可检测的数据包，无需显著容量开销、驱动程序或特殊访问权限。构建了包含元数据注入的端到端系统原型。

Result: 成功实现了可从内存地址跟踪中可靠检测和解码的元数据注入系统，演示了精确代码执行标记和对象地址范围跟踪的用例。

Conclusion: 该方法使内存设备能够感知程序上下文，未来结合近内存计算可实现实时元数据解码，为用户提供定制化遥测统计或根据应用提示执行请求优先级排序、数据重映射等功能。

Abstract: Requests arriving at main memory are often different from what programmers
can observe or estimate by using CPU-based monitoring. Hardware cache
prefetching, memory request scheduling and interleaving cause a loss of
observability that limits potential data movement and tiering optimizations. In
response, memory-side telemetry hardware like page access heat map units (HMU)
and page prefetchers were proposed to inform Operating Systems with accurate
usage data. However, it is still hard to map memory activity to software
program functions and objects because of the decoupled nature of host
processors and memory devices. Valuable program context is stripped out from
the memory bus, leaving only commands, addresses and data. Programmers have
expert knowledge of future data accesses, priorities, and access to processor
state, which could be useful hints for runtime memory device optimization. This
paper makes context visible at memory devices by encoding any user-visible
state as detectable packets in the memory read address stream, in a
nondestructive manner without significant capacity overhead, drivers or special
access privileges. We prototyped an end-to-end system with metadata injection
that can be reliably detected and decoded from a memory address trace, either
by a host processor, or a memory module. We illustrate a use case with precise
code execution markers and object address range tracking. In the future, real
time metadata decoding with near-memory computing (NMC) could provide
customized telemetry and statistics to users, or act on application hints to
perform functions like prioritizing requests, remapping data and reconfiguring
devices.

</details>


### [20] [Opportunities and Challenges for 3D Systems and Their Design](https://arxiv.org/abs/2510.15880)
*Philip Emma,Eren Kurshan*

Main category: cs.AR

TL;DR: 3D集成技术面临设计、制造和测试的新挑战，需要在各层之间进行协同设计，确保空间对应关系正确，并实现独立测试和系统诊断。


<details>
  <summary>Details</summary>
Motivation: 随着光刻技术扩展面临挑战，3D集成技术因其能提高密度而受到广泛关注，但高功率密度和垂直集成带来了新的设计和制造难题。

Method: 通过在各层之间协同设计电路、通孔和宏单元的位置，确保组装时的空间对应关系，并实现各层的独立测试能力。

Result: 3D集成虽然能提高密度并加速扩展，但需要在设计、组装和测试方面解决新的技术挑战。

Conclusion: 要充分发挥3D集成的优势，需要明确其杠杆作用，并系统性地解决设计、组装和测试过程中出现的新挑战。

Abstract: Although it is not a new concept, 3D integration increasingly receives
widespread interest and focus as lithographic scaling becomes more challenging,
and as the ability to make miniature vias greatly improves. Like Moores law, 3D
integration improves density. With improvements in packaging density, however,
come the challenges associated with its inherently higher power density. And
though it acts somewhat as a scaling accelerator, the vertical integration also
poses new challenges to design and manufacturing technologies. The placement of
circuits, vias, and macros in the planes of a 3D stack must be co-designed
across layers (or must conform to new standards) so that, when assembled, they
have correct spatial correspondence. Each layer, although perhaps being a mere
functional slice through a system (and we can slice the system in many
different ways), must be independently testable so that we can systematically
test and diagnose subsystems before and after final assembly. When those layers
are assembled, they must come together in a way that enables a sensible yield
and facilitates testing the finished product. To make the most of 3D
integration, we should articulate the leverages of 3D systems (other
researchers offer a more complete treatment elsewhere). Then we can enumerate
and elucidate many of the new challenges posed by the design, assembly, and
test of 3D systems.

</details>


### [21] [FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern](https://arxiv.org/abs/2510.15882)
*Ao Shen,Rui Zhang,Junping Zhao*

Main category: cs.AR

TL;DR: FlexLink是一个创新的集体通信框架，通过聚合NVLink、PCIe和RDMA NICs等异构链路来解决LLM多节点部署中的通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，多节点部署成为必需，但现有通信库如NCCL仅使用单一互连（如NVLink），导致性能瓶颈，而其他硬件资源如PCIe和RDMA NICs在密集工作负载期间处于闲置状态。

Method: FlexLink采用两阶段自适应负载均衡策略，动态地将通信流量在所有可用链路上进行分区，确保更快的互连不会被较慢的互连所限制。

Result: 在8-GPU H800服务器上，FlexLink将AllReduce和AllGather等集体操作符的带宽分别比NCCL基线提高了26%和27%，通过将2-22%的总通信流量卸载到之前未充分利用的PCIe和RDMA NICs上实现。

Conclusion: FlexLink作为无损的即插即用替代方案，与NCCL API兼容，提供了显著的性能改进，同时确保易于采用。

Abstract: As large language models (LLMs) continue to scale, multi-node deployment has
become a necessity. Consequently, communication has become a critical
performance bottleneck. Current intra-node communication libraries, like NCCL,
typically make use of a single interconnect such as NVLink. This approach
creates performance ceilings, especially on hardware like the H800 GPU where
the primary interconnect's bandwidth can become a bottleneck, and leaves other
hardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable
Network Interface Cards (NICs) largely idle during intensive workloads. We
propose FlexLink, the first collective communication framework to the best of
our knowledge designed to systematically address this by aggregating these
heterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance
communication fabric. FlexLink employs an effective two-stage adaptive load
balancing strategy that dynamically partitions communication traffic across all
available links, ensuring that faster interconnects are not throttled by slower
ones. On an 8-GPU H800 server, our design improves the bandwidth of collective
operators such as AllReduce and AllGather by up to 26% and 27% over the NCCL
baseline, respectively. This gain is achieved by offloading 2-22% of the total
communication traffic to the previously underutilized PCIe and RDMA NICs.
FlexLink provides these improvements as a lossless, drop-in replacement
compatible with the NCCL API, ensuring easy adoption.

</details>


### [22] [Generalized Methodology for Determining Numerical Features of Hardware Floating-Point Matrix Multipliers: Part I](https://arxiv.org/abs/2510.15884)
*Faizan A Khattak,Mantas Mikaitis*

Main category: cs.AR

TL;DR: 提出了一种与架构无关的测试方案，用于分析消费级NVIDIA GPU中矩阵乘法器的数值特性，包括内部精度、舍入和归一化等特征。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于设备特定的硬编码常数或穷举搜索，无法广泛适用于各种混合精度格式。需要一种更通用的方法来分析消费级GPU的数值特性。

Method: 实现架构无关的测试方案，使用非穷举搜索且不依赖设备特定常数的测试向量生成方法，适用于各种输入输出精度格式组合。

Result: 在RTX-3060和Ada RTX-1000上验证了方案，发现RTX-3060的数值特性与数据中心GPU A100相同。支持binary16、TensorFloat32、bfloat16输入格式和binary16、binary32输出格式。

Conclusion: 该方法具有通用性，预计无需修改即可用于分析新一代NVIDIA GPU（如Hopper、Blackwell）和各种输入输出格式组合，包括最新的8位浮点格式。

Abstract: Numerical features of matrix multiplier hardware units in NVIDIA and AMD data
centre GPUs have recently been studied. Features such as rounding,
normalisation, and internal precision of the accumulators are of interest. In
this paper, we extend the methodology for analysing those features, to
consumer-grade NVIDIA GPUs by implementing an architecture-independent test
scheme for various input and output precision formats. Unlike current
approaches, the proposed test vector generation method neither performs an
exhaustive search nor relies on hard-coded {constants that are device-specific,
yet remains applicable to a wide range of mixed-precision formats. We have
applied the scheme to the RTX-3060 (Ampere architecture), and Ada RTX-1000 (Ada
Lovelace architecture) graphics cards and determined numerical features of
matrix multipliers for binary16, TensorFloat32, and bfloat16 input floating
point formats and binary16 and binary32 IEEE 754 output formats. Our
methodology allowed us to determine that} the numerical features of RTX-3060, a
consumer-grade GPU, are identical to those of the A100, a data centre GPU. We
do not expect our code to require any changes for performing analysis of matrix
multipliers on newer NVIDIA GPUs, Hopper or Blackwell, and their future
successors, and any input/output format combination, including the latest 8-bit
floating-point formats.

</details>


### [23] [ConZone+: Practical Zoned Flash Storage Emulation for Consumer Devices](https://arxiv.org/abs/2510.15885)
*Dingcui Yu,Zonghuan Yan,Jialin Liu,Yumiao Zhao,Yanyun Wang,Xinghui Duan,Yina Lv,Liang Shi*

Main category: cs.AR

TL;DR: ConZone+是一个模拟消费级分区闪存存储的仿真器，通过添加块接口支持解决了原版ConZone无法挂载文件系统的问题，提供了部署脚本和多项改进，帮助用户探索存储架构并集成优化。


<details>
  <summary>Details</summary>
Motivation: 为了理解和高效改进消费级分区闪存存储的软硬件设计，需要能够模拟其资源约束和架构特征的仿真工具。

Method: ConZone+在ConZone基础上扩展了块接口支持，提供了部署脚本，并包含有限的逻辑到物理映射缓存、受限写入缓冲区和混合闪存介质管理等消费级设备的典型组件。

Result: 通过对比代表性硬件架构和现有最优技术，验证了ConZone+的准确性；通过案例研究探索了分区存储设计和当前文件系统的不足。

Conclusion: ConZone+作为首个专门针对消费级分区闪存存储的仿真器，有效提升了此类存储系统的研究和优化能力。

Abstract: To facilitate the understanding and efficient enhancement of software and
hardware design for consumer-grade zoned flash storage, ConZone is proposed as
the first emulator designed to model the resource constraints and architectural
features typical of such systems. It incorporates essential components commonly
deployed in consumer-grade devices, including limited logical to physical
mapping caches, constrained write buffers, and hybrid flash media management.
However, ConZone cannot be mounted with the file system due to the lack of
in-place update capability, which is required by the metadata area of F2FS. To
improve the usability of the emulator, ConZone+ extends ConZone with support
for a block interface. We also provide a script to help the deployment and
introduces several enhancements over the original version. Users can explore
the internal architecture of consumer-grade zoned flash storage and integrate
their optimizations with system software using ConZone+. We validate the
accuracy of ConZone+ by comparing a hardware architecture representative of
consumer-grade zoned flash storage and comparing it with the state-of-the-art.
In addition, we conduct several case studies using ConZone+ to investigate the
design of zoned storage and explore the inadequacies of the current file
system.

</details>


### [24] [VeriGRAG: Enhancing LLM-Based Verilog Code Generation with Structure-Aware Soft Prompts](https://arxiv.org/abs/2510.15914)
*Jiayu Zhao,Song Chen*

Main category: cs.AR

TL;DR: VeriGRAG是一个通过图神经网络提取Verilog代码结构信息，并生成结构感知软提示来提升LLM生成Verilog代码正确性的框架。


<details>
  <summary>Details</summary>
Motivation: Verilog代码包含硬件电路的结构信息，但如何有效利用这些结构信息来提升LLM生成代码的功能和语法正确性仍是一个挑战。

Method: 使用GNN提取Verilog代码的结构图嵌入，通过多模态检索器选择相关嵌入，通过VeriFormer模块对齐代码模态生成结构感知软提示。

Result: 在VerilogEval和RTLLM基准测试中实现了最先进或更优的性能，显著提高了Verilog代码生成的正确性。

Conclusion: VeriGRAG框架通过利用Verilog代码的结构信息，有效提升了LLM生成硬件描述代码的质量和正确性。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
generating Verilog code from natural language descriptions. However, Verilog
code inherently encodes structural information of hardware circuits.
Effectively leveraging this structural information to enhance the functional
and syntactic correctness of LLM-generated Verilog code remains a significant
challenge. To address this challenge, we propose VeriGRAG , a novel framework
that extracts structural graph embeddings from Verilog code using graph neural
networks (GNNs). A multimodal retriever then selects the graph embeddings most
relevant to the given generation task, which are aligned with the code modality
through the VeriFormer module to generate structure-aware soft prompts. Our
experiments demonstrate that VeriGRAG substantially improves the correctness of
Verilog code generation, achieving state-of-the-art or superior performance
across both VerilogEval and RTLLM benchmarks.

</details>


### [25] [basic_RV32s: An Open-Source Microarchitectural Roadmap for RISC-V RV32I](https://arxiv.org/abs/2510.15887)
*Hyun Woo Kang,Ji Woong Choi*

Main category: cs.AR

TL;DR: BASIC_RV32s是一个开源的RISC-V RV32I架构框架，提供了从单周期核心到5级流水线设计的完整实现路径，包括完整的冒险转发、动态分支预测和异常处理，并在FPGA上验证。


<details>
  <summary>Details</summary>
Motivation: 解决RISC-V架构理论知识与硬件实现之间的差距，为开源硬件生态系统提供可复现的教学路径。

Method: 采用经典的Patterson和Hennessy方法学，从单周期核心逐步演进到5级流水线设计，集成到SoC中并通过UART通信在Xilinx Artix-7 FPGA上进行验证。

Result: 在50MHz频率下实现1.09 DMIPS/MHz的性能，所有RTL源代码、逻辑框图和发展日志以MIT许可证在GitHub上开源。

Conclusion: BASIC_RV32s为RISC-V教育提供了一个实用的微架构路线图，填补了理论与实践之间的空白，促进了开源硬件生态系统的发展。

Abstract: This paper introduces BASIC_RV32s, an open-source framework providing a
practical microarchitectural roadmap for the RISC-V RV32I architecture,
addressing the gap between theoretical knowledge and hardware implementation.
Following the classic Patterson and Hennessy methodology, the design evolves
from a basic single-cycle core to a 5-stage pipelined core design with full
hazard forwarding, dynamic branch prediction, and exception handling. For
verification, the final core design is integrated into a System-on-Chip (SoC)
with Universal Asynchronous Receiver-Transmitter (UART) communication
implemented on a Xilinx Artix-7 Field-Programmable Gate Array (FPGA), achieving
1.09 Dhrystone million instructions per second per megahertz (DMIPS/MHz) at 50
MHz. By releasing all Register-Transfer Level (RTL) source code, signal-level
logic block diagrams, and development logs under MIT license on GitHub,
BASIC_RV32s offers a reproducible instructional pathway for the open-source
hardware ecosystem.

</details>


### [26] [Limited Read-Write/Set Hardware Transactional Memory without modifying the ISA or the Coherence Protocol](https://arxiv.org/abs/2510.15888)
*Konstantinos Kafousis*

Main category: cs.AR

TL;DR: 提出了一种基于现有Load-Linked和Store-Conditional指令的硬件事务内存实现，无需修改缓存一致性协议，仅需修改L1数据缓存，适用于读写集不超过8个缓存行的小型事务。


<details>
  <summary>Details</summary>
Motivation: 现有HTM实现硬件复杂度高、需要ISA扩展和缓存协议修改，导致采用率低。本文旨在简化HTM实现，使其更易部署。

Method: 扩展Load-Linked和Store-Conditional指令语义实现HTM，限制事务读写集不超过8个缓存行，仅需修改L1数据缓存，提供两种基于重试检测的前进保证机制。

Result: 在Gem5中模拟验证，成功实现多个并发数据结构，在低竞争条件下性能优于TTS锁，跨节点竞争时中止率很低。

Conclusion: 提出的简化HTM设计可行且高效，特别适合小型事务应用场景，为HTM的广泛采用提供了更实用的解决方案。

Abstract: Hardware Transactional Memory (HTM) allows lock-free programming as easy as
with traditional coarse-grain locks or similar, while benefiting from the
performance advantages of fine-grained locking. Many HTM implementations have
been proposed, but they have not received widespread adoption because of their
high hardware complexity, their need for additions to the Instruction Set
Architecture (ISA), and often for modifications to the cache coherence
protocol.
  We show that HTM can be implemented without adding new instructions -- merely
by extending the semantics of two existing, Load-Linked and Store-Conditional.
Also, our proposed design does not modify or extend standard coherence
protocols. We further propose to drastically simplify the implementation of HTM
-- confined to modifications in the L1 Data Cache only -- by restricting it to
applications where the write set plus the read set of each transaction do not
exceed a small number of cache lines. We also propose two alternative
mechanisms to guarantee forward progress, both based on detecting retrial
attempts.
  We simulated our proposed design in Gem5, and we used it to implement several
popular concurrent data structures, showing that a maximum of eight (8) words
(cache lines) suffice for the write plus read sets. We provide a detailed
explanation of selected implementations, clarifying the intended usage of our
HTM from a programmer's perspective. We evaluated our HTM under varying
contention levels to explore its scalability limits. The results indicate that
our HTM provides good performance in concurrent data structures when contention
is spread across multiple nodes: in such cases, the percentage of aborts
relative to successful commits is very low. In the atomic fetch-and-increment
benchmark for multiple shared counters, the results show that, under
low-congestion, our HTM improves performance relative to the TTS lock.

</details>


### [27] [Accelerating Frontier MoE Training with 3D Integrated Optics](https://arxiv.org/abs/2510.15893)
*Mikhail Bernadskiy,Peter Carson,Thomas Graham,Taylor Groves,Ho John Lee,Eric Yeh*

Main category: cs.AR

TL;DR: 本文探讨了3D集成光互连技术在AI计算中的关键作用，通过光电共封装技术实现GPU集群的规模扩展，解决了传统电互连的距离限制问题。


<details>
  <summary>Details</summary>
Motivation: AI工作负载快速增长，传统半导体缩放放缓，高速互连成为新的缩放引擎。电互连的1米距离限制阻碍了跨机架的GPU集群扩展，需要新的光互连解决方案。

Method: 采用3D堆叠光学和逻辑技术，开发光电共封装(CPO)方案，在GPU封装和交换机中集成光互连，实现跨多个数据中心机架的GPU连接。

Result: 3D CPO技术使规模扩展能力提升8倍，支持万亿参数MoE模型训练，训练时间减少2.7倍，实现了前所未有的模型缩放。

Conclusion: 3D光电共封装技术是解决AI计算规模扩展瓶颈的关键，为大规模模型训练提供了高效的互连解决方案，推动了AI模型规模的突破。

Abstract: The unabated growth in AI workload demands is driving the need for concerted
advances in compute, memory, and interconnect performance. As traditional
semiconductor scaling slows, high-speed interconnects have emerged as the new
scaling engine, enabling the creation of larger logical GPUs by linking many
GPUs into a single, low-latency, high-bandwidth compute domain. While initial
scale-up fabrics leveraged copper interconnects for their power and cost
advantages, the maximum reach of passive electrical interconnects
(approximately 1 meter) effectively limits the scale-up domain to within a
single rack. The advent of 3D-stacked optics and logic offers a transformative,
power-efficient scale-up solution for connecting hundreds of GPU packages
(thousands of GPUs) across multiple data center racks. This work explores the
design tradeoffs of scale-up technologies and demonstrates how frontier LLMs
necessitate novel photonic solutions to achieve aggressive power and
performance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and
switches within the scale-up domain when training Frontier Mixture of Experts
(MoE) models exceeding one trillion parameters. Our results show that the
substantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X
increase in scale-up capability. This affords new opportunities for
multi-dimensional parallelism within the scale-up domain and results in a 2.7X
reduction in time-to-train, unlocking unprecedented model scaling.

</details>


### [28] [DiffPlace: A Conditional Diffusion Framework for Simultaneous VLSI Placement Beyond Sequential Paradigms](https://arxiv.org/abs/2510.15897)
*Kien Le Trung,Truong-Son Hy*

Main category: cs.AR

TL;DR: DiffPlace是一个基于条件去噪扩散过程的芯片布局框架，能够在不重新训练的情况下泛化到未见过的电路网表，实现可迁移的布局策略。


<details>
  <summary>Details</summary>
Motivation: 传统芯片布局方法（分析优化或强化学习）难以处理硬布局约束或需要为每个新电路设计进行昂贵的在线训练。DiffPlace旨在解决这些限制，提供可泛化的布局策略。

Method: 将芯片布局制定为条件去噪扩散过程，利用扩散模型的生成能力高效探索布局空间，同时基于电路连接性和相对质量指标进行条件约束，结合能量引导采样和约束流形扩散确保布局合法性。

Result: 在所有实验场景中实现了极低的重叠率，弥合了基于优化和基于学习方法之间的差距。

Conclusion: DiffPlace为现代VLSI设计提供了一条实现自动化、高质量芯片布局的实用路径。

Abstract: Chip placement, the task of determining optimal positions of circuit modules
on a chip canvas, is a critical step in the VLSI design flow that directly
impacts performance, power consumption, and routability. Traditional methods
rely on analytical optimization or reinforcement learning, which struggle with
hard placement constraints or require expensive online training for each new
circuit design. To address these limitations, we introduce DiffPlace, a
framework that formulates chip placement as a conditional denoising diffusion
process, enabling transferable placement policies that generalize to unseen
circuit netlists without retraining. DiffPlace leverages the generative
capabilities of diffusion models to efficiently explore the vast space of
placement while conditioning on circuit connectivity and relative quality
metrics to identify optimal solutions globally. Our approach combines
energy-guided sampling with constrained manifold diffusion to ensure placement
legality, achieving extremely low overlap across all experimental scenarios.
Our method bridges the gap between optimization-based and learning-based
approaches, offering a practical path toward automated, high-quality chip
placement for modern VLSI design. Our source code is publicly available at:
https://github.com/HySonLab/DiffPlace/

</details>


### [29] [Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding](https://arxiv.org/abs/2510.15917)
*Shai Bergman,Won Wook Song,Lukas Cavigelli,Konstantin Berestizshevsky,Ke Zhou,Ji Zhang*

Main category: cs.AR

TL;DR: 提出意图驱动存储系统(IDSS)，利用大语言模型从非结构化信号中推断工作负载意图，指导存储系统自适应和跨层参数重配置，在FileBench测试中IOPS提升最高达2.45倍。


<details>
  <summary>Details</summary>
Motivation: 现有存储系统缺乏对工作负载意图的理解，导致启发式方法脆弱且优化分散，无法适应现代大规模数据密集型应用的语义需求。

Method: 将大语言模型集成到存储控制回路中，通过四个设计原则和相应系统架构，让LLM从非结构化信号推断意图，在策略护栏内生成安全高效的配置决策。

Result: 在FileBench工作负载测试中，IDSS通过解释意图并为缓存、预取等存储组件生成可操作的配置，IOPS最高提升2.45倍。

Conclusion: 当受到护栏约束并嵌入结构化工作流时，大语言模型可以作为高层次语义优化器，弥合应用目标与低层系统控制之间的差距，推动存储系统向更自适应、自主和动态对齐的方向发展。

Abstract: Existing storage systems lack visibility into workload intent, limiting their
ability to adapt to the semantics of modern, large-scale data-intensive
applications. This disconnect leads to brittle heuristics and fragmented,
siloed optimizations. To address these limitations, we propose Intent-Driven
Storage Systems (IDSS), a vision for a new paradigm where large language models
(LLMs) infer workload and system intent from unstructured signals to guide
adaptive and cross-layer parameter reconfiguration. IDSS provides holistic
reasoning for competing demands, synthesizing safe and efficient decisions
within policy guardrails. We present four design principles for integrating
LLMs into storage control loops and propose a corresponding system
architecture. Initial results on FileBench workloads show that IDSS can improve
IOPS by up to 2.45X by interpreting intent and generating actionable
configurations for storage components such as caching and prefetching. These
findings suggest that, when constrained by guardrails and embedded within
structured workflows, LLMs can function as high-level semantic optimizers,
bridging the gap between application goals and low-level system control. IDSS
points toward a future in which storage systems are increasingly adaptive,
autonomous, and aligned with dynamic workload demands.

</details>


### [30] [LLM-VeriPPA: Power, Performance, and Area Optimization aware Verilog Code Generation with Large Language Models](https://arxiv.org/abs/2510.15899)
*Kiran Thorat,Jiahui Zhao,Yaotian Liu,Amit Hasan,Hongwu Peng,Xi Xie,Bin Lei,Caiwen Ding*

Main category: cs.AR

TL;DR: 提出VeriPPA框架，使用大语言模型优化芯片设计的功耗-性能-面积(PPA)并生成准确的Verilog代码，通过两阶段方法显著提升代码正确性和PPA优化效果。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型在芯片设计领域的潜力，解决Verilog代码生成的功能正确性、语法正确性以及PPA优化等关键挑战。

Method: 采用两阶段框架：第一阶段提升Verilog代码的功能和语法正确性，第二阶段优化代码以满足电路设计的PPA约束。

Result: 在RTLLM数据集上实现81.37%语法正确率和62.06%功能正确率；在VerilogEval数据集上达到99.56%语法正确率和43.79%功能正确率，均超越现有最佳方法。

Conclusion: 大语言模型在复杂技术领域具有巨大潜力，为芯片设计自动化提供了有前景的发展方向。

Abstract: Large Language Models (LLMs) are gaining prominence in various fields, thanks
to their ability to generate high- quality content from human instructions.
This paper delves into the field of chip design using LLMs, specifically in
Power- Performance-Area (PPA) optimization and the generation of accurate
Verilog codes for circuit designs. We introduce a novel framework VeriPPA
designed to optimize PPA and generate Verilog code using LLMs. Our method
includes a two-stage process where the first stage focuses on improving the
functional and syntactic correctness of the generated Verilog codes, while the
second stage focuses on optimizing the Verilog codes to meet PPA constraints of
circuit designs, a crucial element of chip design. Our framework achieves an
81.37% success rate in syntactic correctness and 62.06% in functional
correctness for code genera- tion, outperforming current state-of-the-art
(SOTA) methods. On the RTLLM dataset. On the VerilogEval dataset, our framework
achieves 99.56% syntactic correctness and 43.79% functional correctness, also
surpassing SOTA, which stands at 92.11% for syntactic correctness and 33.57%
for functional correctness. Furthermore, Our framework able to optimize the PPA
of the designs. These results highlight the potential of LLMs in handling
complex technical areas and indicate an encouraging development in the
automation of chip design processes.

</details>


### [31] [UPMEM Unleashed: Software Secrets for Speed](https://arxiv.org/abs/2510.15927)
*Krystian Chmielewski,Jarosław Ławnicki,Uladzislau Lukyanau,Tadeusz Kobus,Maciej Maciejewski*

Main category: cs.AR

TL;DR: 本文揭示了UPMEM PIM平台软件栈的低效问题，通过汇编优化、位串行处理和NUMA感知分配等方法，显著提升了整数运算和矩阵向量乘法的性能。


<details>
  <summary>Details</summary>
Motivation: PIM平台的软件开发工具包仍存在显著的性能优化空间，特别是在数据管理和并行编程方面。

Method: 修改UPMEM编译器生成的汇编代码，采用位串行处理低精度数据，扩展API以考虑NUMA架构。

Result: 整数加法速度提升1.6-2倍，乘法提升1.4-5.9倍；INT4位串行点积计算速度提升2.7倍；主机-PIM数据传输吞吐量提升2.9倍；优化后的INT8 GEMV内核性能提升3.5倍。

Conclusion: 通过简单的软件优化技术，可以在PIM平台上实现显著的性能提升，特别是在低精度计算场景下表现优异。

Abstract: Developing kernels for Processing-In-Memory (PIM) platforms poses unique
challenges in data management and parallel programming on limited processing
units. Although software development kits (SDKs) for PIM, such as the UPMEM
SDK, provide essential tools, these emerging platforms still leave significant
room for performance optimization. In this paper, we reveal surprising
inefficiencies in UPMEM software stack and play with non-standard programming
techniques. By making simple modifications to the assembly generated by the
UPMEM compiler, we achieve speedups of 1.6-2x in integer addition and 1.4-5.9x
in integer multiplication, depending on the data type. We also demonstrate that
bit-serial processing of low precision data is a viable option for UPMEM: in
INT4 bit-serial dot-product calculation, UPMEM can achieve over 2.7x speedup
over the baseline. Minor API extensions for PIM allocation that account for the
non-uniform memory access (NUMA) architecture of the server further improve the
consistency and throughput of host-PIM data transfers by up to 2.9x. Finally,
we show that, when the matrix is preloaded into PIM, our optimized kernels
outperform a dual-socket CPU server by over 3x for INT8 generalized
matrix-vector multiplication (GEMV) and by 10x for INT4 GEMV. Our optimized
INT8 GEMV kernel outperforms the baseline 3.5x.

</details>


### [32] [Fully Automated Verification Framework for Configurable IPs: From Requirements to Results](https://arxiv.org/abs/2510.15902)
*Shuhang Zhang,Jelena Radulovic,Thorsten Dworzak*

Main category: cs.AR

TL;DR: 提出全自动需求驱动功能验证框架，自动化vPlan生成、测试平台创建、回归执行和报告生成，大幅降低验证工作量


<details>
  <summary>Details</summary>
Motivation: 半导体行业竞争加剧，芯片价格压力增大，但需保持质量和可靠性。功能验证特别是可配置IP的验证成本高昂，复杂且资源密集

Method: 开发全自动化框架，包括需求管理工具中的vPlan自动生成、测试平台自动创建、回归自动执行和报告自动生成

Result: 大幅减少验证工作量，加速开发周期，最小化人为错误，提高覆盖率

Conclusion: 该框架为可配置IP验证挑战提供了可扩展且高效的解决方案

Abstract: The increasing competition in the semiconductor industry has created
significant pressure to reduce chip prices while maintaining quality and
reliability. Functional verification, particularly for configurable IPs, is a
major contributor to development costs due to its complexity and
resource-intensive nature. To address this, we propose a fully automated
framework for requirements driven functional verification. The framework
automates key processes, including vPlan generation, testbench creation,
regression execution, and reporting in a requirements management tool,
drastically reducing verification effort. This approach accelerates development
cycles, minimizes human error, and enhances coverage, offering a scalable and
efficient solution to the challenges of verifying configurable IPs.

</details>


### [33] [NVM-in-Cache: Repurposing Commodity 6T SRAM Cache into NVM Analog Processing-in-Memory Engine using a Novel Compute-on-Powerline Scheme](https://arxiv.org/abs/2510.15904)
*Subhradip Chakraborty,Ankur Singh,Xuming Chen,Gourav Datta,Akhilesh R. Jaiswal*

Main category: cs.AR

TL;DR: 提出了一种将RRAM集成到6T-SRAM中的NVM-in-Cache架构，形成6T-2R混合单元，支持内存内计算，在不增加面积的情况下提升存储密度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 深度学习神经网络工作负载快速增长，对片上SRAM容量需求大幅增加，SRAM阵列占据了芯片面积的很大部分，需要解决存储密度和计算效率的双重挑战。

Method: 将电阻式RAM（RRAM）器件集成到传统的6T-SRAM单元中，形成紧凑的6T-2R位单元，支持内存内计算模式，直接在缓存电源线上执行大规模并行乘累加操作。

Result: 在GlobalFoundries 22nm FDSOI技术中模拟显示，该设计实现了0.4 TOPS的吞吐量和491.78 TOPS/W的能效。使用Resnet-18神经网络进行CIFAR-10分类，在128行并行操作下达到91.27%的准确率。

Conclusion: NVM-in-Cache方法通过重新利用现有的6T SRAM缓存架构，为下一代AI加速器和通用处理器提供了一种可扩展、高能效的计算方法。

Abstract: The rapid growth of deep neural network (DNN) workloads has significantly
increased the demand for large-capacity on-chip SRAM in machine learning (ML)
applications, with SRAM arrays now occupying a substantial fraction of the
total die area. To address the dual challenges of storage density and
computation efficiency, this paper proposes an NVM-in-Cache architecture that
integrates resistive RAM (RRAM) devices into a conventional 6T-SRAM cell,
forming a compact 6T-2R bit-cell. This hybrid cell enables Processing-in-Memory
(PIM) mode, which performs massively parallel multiply-and-accumulate (MAC)
operations directly on cache power lines while preserving stored cache data. By
exploiting the intrinsic properties of the 6T-2R structure, the architecture
achieves additional storage capability, high computational throughput without
any bit-cell area overhead. Circuit- and array-level simulations in
GlobalFoundries 22nm FDSOI technology demonstrate that the proposed design
achieves a throughput of 0.4 TOPS and 491.78 TOPS/W. For 128 row-parallel
operations, the CIFAR-10 classification is demonstrated by mapping a Resnet-18
neural network, achieving an accuracy of 91.27%. These results highlight the
potential of the NVM-in-Cache approach to serve as a scalable, energy-efficient
computing method by re-purposing existing 6T SRAM cache architecture for
next-generation AI accelerators and general purpose processors.

</details>


### [34] [FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures](https://arxiv.org/abs/2510.15906)
*Yunsheng Bai,Ghaith Bany Hamad,Chia-Tung Ho,Syed Suhaib,Haoxing Ren*

Main category: cs.AR

TL;DR: FVDebug是一个自动化形式验证失败根因分析的系统，通过结合波形、RTL代码和设计规范，将失败跟踪转化为可操作的见解，并提供RTL修复建议。


<details>
  <summary>Details</summary>
Motivation: 形式验证失败调试是现代硬件设计流程中最耗时的瓶颈之一，工程师需要手动分析复杂的多周期反例，现有解决方案有限，无法处理设计意图与实现逻辑之间的复杂交互。

Method: 采用新颖的流水线方法：1) 因果图合成将失败跟踪构建为有向无环图；2) 图扫描器使用批量LLM分析和正反提示识别可疑节点；3) 洞察漫游器利用代理叙事探索生成高级因果解释；4) 修复生成器提供具体RTL修复。

Result: 在开放基准测试中，FVDebug实现了高假设质量和强Pass@k修复率，并在两个专有的生产级形式验证反例上报告了结果，证明其从学术基准到工业设计的适用性。

Conclusion: FVDebug通过智能系统自动化根因分析，显著提高了形式验证失败调试的效率，适用于从学术研究到工业应用的各种场景。

Abstract: Debugging formal verification (FV) failures represents one of the most
time-consuming bottlenecks in modern hardware design workflows. When properties
fail, engineers must manually trace through complex counter-examples spanning
multiple cycles, analyze waveforms, and cross-reference design specifications
to identify root causes - a process that can consume hours or days per bug.
Existing solutions are largely limited to manual waveform viewers or simple
automated tools that cannot reason about the complex interplay between design
intent and implementation logic. We present FVDebug, an intelligent system that
automates root-cause analysis by combining multiple data sources - waveforms,
RTL code, design specifications - to transform failure traces into actionable
insights. Our approach features a novel pipeline: (1) Causal Graph Synthesis
that structures failure traces into directed acyclic graphs, (2) Graph Scanner
using batched Large Language Model (LLM) analysis with for-and-against
prompting to identify suspicious nodes, and (3) Insight Rover leveraging
agentic narrative exploration to generate high-level causal explanations.
FVDebug further provides concrete RTL fixes through its Fix Generator.
Evaluated on open benchmarks, FVDebug attains high hypothesis quality and
strong Pass@k fix rates. We further report results on two proprietary,
production-scale FV counterexamples. These results demonstrate FVDebug's
applicability from academic benchmarks to industrial designs.

</details>


### [35] [Symbolic Timing Analysis of Digital Circuits Using Analytic Delay Functions](https://arxiv.org/abs/2510.15907)
*Era Thaqi,Dennis Eigner,Arman Ferdowsi,Ulrich Schmid*

Main category: cs.AR

TL;DR: 提出基于解析延迟公式的符号时序分析方法，通过计算内部信号转换时间的闭式表达式，实现无需仿真的时序分析和参数敏感性分析。


<details>
  <summary>Details</summary>
Motivation: 传统时序分析依赖仿真，无法提供解析的时序依赖关系分析。本文旨在开发一种基于解析延迟公式的符号分析方法，以支持更深入的时序特性研究。

Method: 利用Ferdowsi等人开发的2输入NOR、NAND和Muller-C门解析延迟公式，在固定信号转换顺序下，计算内部信号转换时间的符号表达式，依赖输入信号转换时间和门模型参数。

Result: 实现了基于SageMath的框架，应用于c17 slack基准电路的NOR门版本，成功生成符号延迟表达式，支持时序分析和参数敏感性分析。

Conclusion: 提出的符号时序分析方法能够有效计算延迟的闭式表达式，不仅支持无仿真的时序分析，还实现了对时序特性与参数依赖关系的解析研究。

Abstract: We propose a novel approach to symbolic timing analysis for digital
integrated circuits based on recently developed analytic delay formulas for
2-input NOR, NAND, and Muller-C gates by Ferdowsi et al. (NAHS 2025). Given a
fixed order of the transitions of all input and internal signals of a circuit,
our framework computes closed-form analytic delay expressions for all the
internal signal transition times that depend on (i) the symbolic transition
times of the relevant input signals and (ii) the model parameters of the
relevant gates. The resulting formulas facilitate per-transition timing
analysis without any simulation, by instantiating the symbolic input transition
times and the gate parameters. More importantly, however, they also enable an
\emph{analytic} study of the dependencies of certain timing properties on input
signals and gate parameters. For instance, differentiating a symbolic delay
expression with respect to a gate parameter or input transition time enables
sensitivity analysis. As a proof of concept, we implement our approach using
the computer algebra system SageMath and apply it to the NOR-gate version of
the c17 slack benchmark circuit.

</details>


### [36] [Belenos: Bottleneck Evaluation to Link Biomechanics to Novel Computing Optimizations](https://arxiv.org/abs/2510.15908)
*Hana Chitsaz,Johnson Umeike,Amirmahdi Namjoo,Babak N. Safa,Bahar Asgari*

Main category: cs.AR

TL;DR: Belenos项目对生物力学有限元模拟进行工作负载特征分析，发现小规模工作负载存在前端停顿问题，大规模工作负载则受后端瓶颈主导，需要通过架构感知的协同设计来优化性能。


<details>
  <summary>Details</summary>
Motivation: 当前生物力学有限元模拟在硬件和软件架构上存在效率限制，特别是在材料参数识别等迭代任务中，往往需要在精度和可处理性之间妥协。可重构硬件如FPGA提供了领域特定加速的潜力，但在生物力学中的应用尚未充分探索。

Method: 使用广泛采用的FEBio模拟器进行有限元生物力学工作负载特征分析，结合gem5敏感性研究和VTune分析，识别性能瓶颈和最优硬件配置。

Result: VTune分析显示小规模工作负载前端停顿约13.1%，大规模工作负载后端瓶颈占主导，后端绑定周期从59.9%到超过82.2%。gem5敏感性研究发现次优的流水线、内存或分支预测器设置可导致性能下降达37.1%。

Conclusion: 生物力学模拟工作负载需要架构感知的协同设计来有效支持，领域特定加速器的最优配置对性能至关重要。

Abstract: Finite element simulations are essential in biomechanics, enabling detailed
modeling of tissues and organs. However, architectural inefficiencies in
current hardware and software stacks limit performance and scalability,
especially for iterative tasks like material parameter identification. As a
result, workflows often sacrifice fidelity for tractability. Reconfigurable
hardware, such as FPGAs, offers a promising path to domain-specific
acceleration without the cost of ASICs, but its potential in biomechanics
remains underexplored. This paper presents Belenos, a comprehensive workload
characterization of finite element biomechanics using FEBio, a widely adopted
simulator, gem5 sensitivity studies, and VTune analysis. VTune results reveal
that smaller workloads experience moderate front-end stalls, typically around
13.1%, whereas larger workloads are dominated by significant back-end
bottlenecks, with backend-bound cycles ranging from 59.9% to over 82.2%.
Complementary gem5 sensitivity studies identify optimal hardware configurations
for Domain-Specific Accelerators (DSA), showing that suboptimal pipeline,
memory, or branch predictor settings can degrade performance by up to 37.1%.
These findings underscore the need for architecture-aware co-design to
efficiently support biomechanical simulation workloads.

</details>


### [37] [SoCks - Simplifying Firmware and Software Integration for Heterogeneous SoCs](https://arxiv.org/abs/2510.15910)
*Marvin Fuchs,Lukas Scheller,Timo Muscheid,Oliver Sander,Luis E. Ardila-Perez*

Main category: cs.AR

TL;DR: SoCks是一个灵活可扩展的构建框架，通过将SoC镜像划分为高层次单元（块）来降低复杂性，实现独立构建和最小化依赖，提高构建速度3倍。


<details>
  <summary>Details</summary>
Motivation: 现代异构SoC设备集成复杂组件，开发工具日益复杂但缺乏足够支持，导致学习曲线陡峭和故障排除困难。

Method: 将SoC镜像分区为块，每个固件和软件块以封装方式独立构建，通过标准化接口进行必要的信息交换，最小化依赖关系。

Result: SoCks可以比现有工具快3倍构建完整的SoC镜像，简化现有块实现的复用，支持无缝版本替换。

Conclusion: SoCks框架通过模块化方法有效降低了SoC开发的复杂性，支持去中心化和部分自动化的开发流程，显著提高了构建效率。

Abstract: Modern heterogeneous System-on-Chip (SoC) devices integrate advanced
components into a single package, offering powerful capabilities while also
introducing significant complexity. To manage these sophisticated devices,
firmware and software developers need powerful development tools. However, as
these tools become increasingly complex, they often lack adequate support,
resulting in a steep learning curve and challenging troubleshooting. To address
this, this work introduces System-on-Chip blocks (SoCks), a flexible and
expandable build framework that reduces complexity by partitioning the SoC
image into high-level units called blocks. SoCks builds each firmware and
software block in an encapsulated way, independently from other components of
the image, thereby reducing dependencies to a minimum. While some information
exchange between the blocks is unavoidable to ensure seamless runtime
integration, this interaction is standardized via interfaces. A small number of
dependencies and well-defined interfaces simplify the reuse of existing block
implementations and facilitate seamless substitution between versions-for
instance, when choosing root file systems for the embedded Linux operating
system. Additionally, this approach facilitates the establishment of a
decentralized and partially automated development flow through Continuous
Integration and Continuous Delivery (CI/CD). Measurement results demonstrate
that SoCks can build a complete SoC image up to three times faster than
established tools.

</details>


### [38] [TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs](https://arxiv.org/abs/2510.15926)
*Ye Qiao,Zhiheng Chen,Yifan Zhang,Yian Wang,Sitao Huang*

Main category: cs.AR

TL;DR: TeLLMe是一种基于查表的1.58位三元LLM加速器，专为低功耗边缘FPGA设计，支持预填充和自回归解码，在5W功耗下实现25 tokens/s的解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备和嵌入式系统的普及，在边缘平台上部署大型语言模型成为迫切需求，但受限于计算资源、内存需求和预填充阶段的长延迟。

Method: 采用表查找三元矩阵乘法引擎、细粒度URAM权重缓冲管理、流式数据流架构、反向重排序预填充注意力以及专用解码阶段注意力等技术。

Result: 在5W功耗预算下，TeLLMe提供高达25 tokens/s的解码吞吐量，64-128 token提示的首次token时间为0.45-0.96秒。

Conclusion: TeLLMe在边缘FPGA上的LLM推理实现了显著的能效提升，为资源受限的边缘设备部署LLM提供了可行方案。

Abstract: With the emergence of wearable devices and other embedded systems, deploying
large language models (LLMs) on edge platforms has become an urgent need.
However, this is challenging because of their high computational and memory
demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek)
compress weights to as low as 1.58~bits with minimal accuracy loss, edge
deployment is still constrained by limited on-chip resources, power budgets,
and the often-neglected long latency of the prefill stage. We present
\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for
low-power edge FPGAs that fully supports both prefill and autoregressive
decoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates
several novel techniques, including (1) a table-lookup-based ternary matrix
multiplication (TLMM) engine utilizing grouped activations and online
precomputation for low resource utilization and high throughput; (2) a
fine-grained analytic URAM-based weight buffer management scheme for efficient
loading and compute engine access; (3) a streaming dataflow architecture that
fuses floating-point element-wise operations with linear computations to hide
latency; (4) a reversed-reordered prefill stage attention with fused attention
operations for high memory efficiency; and (5) a resource-efficient specialized
decoding stage attention. Under a 5~W power budget, TeLLMe delivers up to
25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for
64--128 token prompts, marking a significant energy-efficiency advancement in
LLM inference on edge FPGAs.

</details>


### [39] [Implémentation Efficiente de Fonctions de Convolution sur FPGA à l'Aide de Blocs Paramétrables et d'Approximations Polynomiales](https://arxiv.org/abs/2510.15930)
*Philippe Magalhães,Virginie Fresse,Benoît Suffran,Olivier Alata*

Main category: cs.AR

TL;DR: 提出了一种可配置的卷积块库和FPGA资源利用预测模型，用于优化CNN在FPGA上的部署，解决硬件知识要求和长设计周期的问题。


<details>
  <summary>Details</summary>
Motivation: FPGA实现CNN具有低延迟、高能效和灵活性的优势，但开发复杂，需要硬件知识且设计周期长，限制了网络配置的快速探索和资源优化。

Method: 设计可配置的卷积块库以适应可用资源，并开发数学模型来预测FPGA资源利用率，通过参数相关性分析和误差指标进行验证。

Result: 设计的卷积块能够根据硬件约束调整卷积层，模型能够准确预测资源消耗，为FPGA选择和CNN优化部署提供了有用工具。

Conclusion: 该方法有效解决了FPGA上CNN实现的复杂性问题，实现了资源优化和快速部署，提升了设计效率。

Abstract: Implementing convolutional neural networks (CNNs) on field-programmable gate
arrays (FPGAs) has emerged as a promising alternative to GPUs, offering lower
latency, greater power efficiency and greater flexibility. However, this
development remains complex due to the hardware knowledge required and the long
synthesis, placement and routing stages, which slow down design cycles and
prevent rapid exploration of network configurations, making resource
optimisation under severe constraints particularly challenging. This paper
proposes a library of configurable convolution Blocks designed to optimize FPGA
implementation and adapt to available resources. It also presents a
methodological framework for developing mathematical models that predict FPGA
resources utilization. The approach is validated by analyzing the correlation
between the parameters, followed by error metrics. The results show that the
designed blocks enable adaptation of convolution layers to hardware
constraints, and that the models accurately predict resource consumption,
providing a useful tool for FPGA selection and optimized CNN deployment.

</details>


### [40] [Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing](https://arxiv.org/abs/2510.16040)
*Tianhua Xia,Sai Qian Zhang*

Main category: cs.AR

TL;DR: 提出了Kelle软件硬件协同设计解决方案，利用嵌入式DRAM(eDRAM)作为边缘设备上LLM服务的主要存储，通过细粒度内存驱逐、重计算和刷新控制算法，显著提升性能和能效。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上运行大型语言模型(LLM)对于降低延迟、改善实时处理和增强隐私至关重要，但KV缓存的内存占用和数据访问成本限制了在资源受限边缘设备上的部署。

Method: 使用eDRAM作为主要存储，结合细粒度内存驱逐、重计算和刷新控制算法，开发了Kelle软件硬件协同设计解决方案。

Result: 与现有基线解决方案相比，Kelle加速器实现了3.9倍的速度提升和4.5倍的能耗节省。

Conclusion: Kelle通过软件硬件协同设计有效解决了边缘设备上LLM部署的KV缓存管理问题，显著提升了系统性能和能效。

Abstract: Running Large Language Models (LLMs) on edge devices is crucial for reducing
latency, improving real-time processing, and enhancing privacy. By performing
inference directly on the device, data does not need to be sent to the cloud,
ensuring faster responses and reducing reliance on network connectivity.
However, implementing LLMs on edge devices presents challenges, particularly
with managing key-value (KV) caches, which plays a pivotal role in LLM serving.
As the input text lengthens, the size of the KV cache increases linearly with
the sequence length, leading to a significant memory footprint and data access
costs. On the other hand, edge devices have limited memory and computational
power, making it hard to store and efficiently access the large caches needed
for LLM inference.
  To mitigate the substantial overhead caused by KV cache, we propose using
embedded DRAM (eDRAM) as the primary storage for LLM serving in edge device,
which offers higher storage density compared to SRAM. However, to ensure data
integrity, eDRAM needs periodic refresh operations, which are power-intensive.
To reduce eDRAM costs and improve overall system performance, we
propose~\textit{Kelle}, a software-hardware co-design solution optimized for
deploying LLMs on eDRAM-based edge systems. Combined with our fine-grained
memory eviction, recomputation, and refresh control algorithms, the
\textit{Kelle} accelerator delivers a $3.9\times$ speedup and $4.5\times$
energy savings compared to existing baseline solutions.

</details>


### [41] [Architecture, Simulation and Software Stack to Support Post-CMOS Accelerators: The ARCHYTAS Project](https://arxiv.org/abs/2510.16487)
*Giovanni Agosta,Stefano Cherubin,Derek Christ,Francesco Conti,Asbjørn Djupdal,Matthias Jung,Georgios Keramidas,Roberto Passerone,Paolo Rech,Elisa Ricci,Philippe Velha,Flavio Vella,Kasim Sinan Yildirim,Nils Wilbert*

Main category: cs.AR

TL;DR: ARCHYTAS项目旨在设计和评估非传统硬件加速器（光电、易失性和非易失性内存处理、神经形态）来解决AI的功耗、效率和可扩展性瓶颈，重点关注国防应用。


<details>
  <summary>Details</summary>
Motivation: 解决AI在功耗、效率和可扩展性方面的瓶颈，特别是针对国防应用场景如自动驾驶车辆、监控无人机、海事和太空平台的需求。

Method: 开发系统架构和软件栈来集成和支持这些加速器，同时开发用于全系统及其组件早期原型设计的仿真软件。

Result: 提出了ARCHYTAS项目的整体架构方案，包括硬件加速器集成框架和仿真工具链。

Conclusion: ARCHYTAS通过非传统硬件加速器和配套软件栈的开发，为解决AI系统在国防应用中的关键性能瓶颈提供了系统化解决方案。

Abstract: ARCHYTAS aims to design and evaluate non-conventional hardware accelerators,
in particular, optoelectronic, volatile and non-volatile processing-in-memory,
and neuromorphic, to tackle the power, efficiency, and scalability bottlenecks
of AI with an emphasis on defense use cases (e.g., autonomous vehicles,
surveillance drones, maritime and space platforms). In this paper, we present
the system architecture and software stack that ARCHYTAS will develop to
integrate and support those accelerators, as well as the simulation software
needed for early prototyping of the full system and its components.

</details>


### [42] [Towards Intelligent Traffic Signaling in Dhaka City Based on Vehicle Detection and Congestion Optimization](https://arxiv.org/abs/2510.16622)
*Kazi Ababil Azam,Hasan Masum,Masfiqur Rahaman,A. B. M. Alim Al Islam*

Main category: cs.AR

TL;DR: 该研究为孟加拉国达卡市开发了一个智能交通信号系统，利用RTSP视频流、Raspberry Pi 4B处理和YOLO目标检测模型，结合NSGA-II多目标优化算法来优化信号配时，减少等待时间并提高车辆通行效率。


<details>
  <summary>Details</summary>
Motivation: 发展中国家如孟加拉国达卡市的交通密度高，导致严重拥堵。现有的智能交通信号系统主要针对发达国家的结构化交通，不适用于非车道化、异质化的交通环境，因此需要开发适合发展中国家背景的解决方案。

Method: 使用RTSP视频流采集数据，在Raspberry Pi 4B上进行低资源处理，采用基于YOLO的目标检测模型（在NHT-1071数据集上训练）检测和分类异质交通，然后应用NSGA-II多目标优化算法生成优化的信号配时方案。

Result: 在达卡市Palashi的五路交叉口进行测试，证明该系统能够显著改善交通管理，减少等待时间并提高车辆通行效率。

Conclusion: 开发的测试平台为具有复杂交通动态的发展中国家地区提供了更符合背景和有效的智能交通信号解决方案，为类似环境下的交通管理改进铺平了道路。

Abstract: The vehicular density in urbanizing cities of developing countries such as
Dhaka, Bangladesh result in a lot of traffic congestion, causing poor on-road
experiences. Traffic signaling is a key component in effective traffic
management for such situations, but the advancements in intelligent traffic
signaling have been exclusive to developed countries with structured traffic.
The non-lane-based, heterogeneous traffic of Dhaka City requires a contextual
approach. This study focuses on the development of an intelligent traffic
signaling system feasible in the context of developing countries such as
Bangladesh. We propose a pipeline leveraging Real Time Streaming Protocol
(RTSP) feeds, a low resources system Raspberry Pi 4B processing, and a state of
the art YOLO-based object detection model trained on the Non-lane-based and
Heterogeneous Traffic (NHT-1071) dataset to detect and classify heterogeneous
traffic. A multi-objective optimization algorithm, NSGA-II, then generates
optimized signal timings, minimizing waiting time while maximizing vehicle
throughput. We test our implementation in a five-road intersection at Palashi,
Dhaka, demonstrating the potential to significantly improve traffic management
in similar situations. The developed testbed paves the way for more contextual
and effective Intelligent Traffic Signaling (ITS) solutions for developing
areas with complicated traffic dynamics such as Dhaka City.

</details>


### [43] [SmaRTLy: RTL Optimization with Logic Inferencing and Structural Rebuilding](https://arxiv.org/abs/2510.17251)
*Chengxi Li,Yang Sun,Lei Chen,Yiwen Wang,Mingxuan Yuan,Evangeline F. Y. Young*

Main category: cs.AR

TL;DR: smaRTLy是一种针对RTL逻辑综合中多路选择器的优化技术，通过逻辑推断和结构重建，显著减少门数，在基准测试中比Yosys多减少8.95%的AIG面积，在工业级测试中多减少47.2%的面积。


<details>
  <summary>Details</summary>
Motivation: 传统工具如Yosys通过遍历多路选择器树并监控控制端口值来优化，但未能充分利用信号间的内在逻辑关系或结构优化潜力。

Method: 开发创新策略来移除冗余的多路选择器树并重构剩余部分，采用逻辑推断和结构重建技术。

Result: 在IWLS-2005和RISC-V基准测试中，相比Yosys额外减少8.95%的AIG面积；在百万门级的工业基准测试中，比Yosys多减少47.2%的AIG面积。

Conclusion: smaRTLy的逻辑推断和结构重建技术能有效增强RTL优化过程，实现更高效的硬件设计。

Abstract: This paper proposes smaRTLy: a new optimization technique for multiplexers in
Register-Transfer Level (RTL) logic synthesis. Multiplexer trees are very
common in RTL designs, and traditional tools like Yosys optimize them by
traversing the tree and monitoring control port values. However, this method
does not fully exploit the intrinsic logical relationships among signals or the
potential for structural optimization. To address these limitations, we develop
innovative strategies to remove redundant multiplexer trees and restructure the
remaining ones, significantly reducing the overall gate count. We evaluate
smaRTLy on the IWLS-2005 and RISC-V benchmarks, achieving an additional 8.95%
reduction in AIG area compared to Yosys. We also evaluate smaRTLy on an
industrial benchmark in the scale of millions of gates, results show that
smaRTLy can remove 47.2% more AIG area than Yosys. These results demonstrate
the effectiveness of our logic inferencing and structural rebuilding techniques
in enhancing the RTL optimization process, leading to more efficient hardware
designs.

</details>
