<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 9]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Tuning Random Generators: Property-Based Testing as Probabilistic Programming](https://arxiv.org/abs/2508.14394)
*Ryan Tjoa,Poorva Garg,Harrison Goldstein,Todd Millstein,Benjamin Pierce,Guy Van den Broeck*

Main category: cs.PL

TL;DR: 本文提出了一种自动离线调整属性基测试生成器权重的方法，通过符号权重和目标函数优化，实现更好的测试输入分布和更快的错误发现速度。


<details>
  <summary>Details</summary>
Motivation: 传统属性基测试中，用户需要手动调整生成器的随机选择权重来获得理想的测试输入分布，这个过程既繁琐又难以达到预期效果，限制了实际可实现的分布质量。

Method: 开发了Loaded Dice离散概率编程系统，支持微分和参数学习。给定带有未确定符号权重的生成器和目标函数，自动学习优化这些权重的值，以针对多样性、有效性和目标分布进行优化。

Result: 实验证明该方法能有效优化生成器分布，在针对多样性和有效性自动调优后，生成器在错误发现方面表现出3.1-7.4倍的加速效果。

Conclusion: 提出的自动调优技术能够显著改善属性基测试生成器的性能，通过优化权重选择实现更好的测试输入分布，大幅提升错误检测效率。

Abstract: Property-based testing validates software against an executable specification
by evaluating it on randomly generated inputs. The standard way that PBT users
generate test inputs is via generators that describe how to sample test inputs
through random choices. To achieve a good distribution over test inputs, users
must tune their generators, i.e., decide on the weights of these individual
random choices. Unfortunately, it is very difficult to understand how to choose
individual generator weights in order to achieve a desired distribution, so
today this process is tedious and limits the distributions that can be
practically achieved.
  In this paper, we develop techniques for the automatic and offline tuning of
generators. Given a generator with undetermined symbolic weights and an
objective function, our approach automatically learns values for these weights
that optimize for the objective. We describe useful objective functions that
allow users to (1) target desired distributions and (2) improve the diversity
and validity of their test cases. We have implemented our approach in a novel
discrete probabilistic programming system, Loaded Dice, that supports
differentiation and parameter learning, and use it as a language for
generators. We empirically demonstrate that our approach is effective at
optimizing generator distributions according to the specified objective
functions. We also perform a thorough evaluation on PBT benchmarks,
demonstrating that, when automatically tuned for diversity and validity, the
generators exhibit a 3.1-7.4x speedup in bug finding.

</details>


### [2] [Close is Good Enough: Component-Based Synthesis Modulo Logical Similarity](https://arxiv.org/abs/2508.14614)
*Ashish Mishra,Suresh Jagannathan*

Main category: cs.PL

TL;DR: 该论文提出了一种基于组件合成的新方法，通过逻辑相似性推理来优化搜索过程，使用精化类型规范和合格树自动机来避免探索语义相似的路径，显著提升了复杂查询的合成能力。


<details>
  <summary>Details</summary>
Motivation: 传统组件合成算法在面对精确约束和丰富规范时，可行解空间变得稀疏，搜索效率低下。需要一种能够推理路径间逻辑相似性的方法来避免冗余探索。

Method: 采用精化类型规范来丰富库方法，使用合格树自动机变体进行搜索，通过子类型约束来记录枚举项信息，实现候选解之间的相似性推理。

Result: 开发了名为\name的工具，在综合评估中证明其能够合成复杂CBS查询的解决方案，性能远超现有最先进技术。

Conclusion: 通过逻辑相似性推理和精化类型约束，显著提升了组件合成算法的效率和能力，为解决复杂合成问题提供了有效方法。

Abstract: Component-based synthesis (CBS) aims to generate loop-free programs from a
set of libraries whose methods are annotated with specifications and whose
output must satisfy a set of logical constraints, expressed as a query. The
effectiveness of a CBS algorithm critically depends on the severity of the
constraints imposed by the query. The more exact these constraints are, the
sparser the space of feasible solutions. This maxim also applies when we enrich
the expressiveness of the specifications affixed to library methods. In both
cases, the search must now contend with constraints that may only hold over a
small number of the possible execution paths that can be enumerated by a CBS
procedure.
  In this paper, we address this challenge by equipping CBS search with the
ability to reason about logical similarities among the paths it explores. Our
setting considers library methods equipped with refinement-type specifications
that enrich ordinary base types with a set of rich logical qualifiers to
constrain the set of values accepted by that type. We perform a search over a
tree automata variant called Qualified Tree Automata that intelligently records
information about enumerated terms, leveraging subtyping constraints over the
refinement types associated with these terms to enable reasoning about
similarity among candidate solutions as search proceeds, thereby avoiding
exploration of semantically similar paths.
  We present an implementation of this idea in a tool called \name, and provide
a comprehensive evaluation that demonstrates \name's ability to synthesize
solutions to complex CBS queries that go well-beyond the capabilities of the
existing state-of-the-art.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Time-optimal Asynchronous Minimal Vertex Covering by Myopic Robots](https://arxiv.org/abs/2508.14247)
*Saswata Jana,Subhajit Pramanick,Adri Bhattacharya,Partha Sarathi Mandal*

Main category: cs.DC

TL;DR: 本文研究有限可见度的发光机器人在异步调度下如何部署到图中形成最小顶点覆盖的问题，针对树和一般图提出了多种算法，实现了时间最优和内存最优的解决方案。


<details>
  <summary>Details</summary>
Motivation: 在有限可见度的自主机器人群体中，研究如何利用局部知识将机器人部署到满足特定属性（最小顶点覆盖）的顶点上，解决图覆盖问题。

Method: 使用有限可见范围的发光机器人，针对树结构提出了单门和多门算法，对一般图提出了需要额外O(logΔ)内存的算法，所有算法都在O(|E|)时段内运行。

Result: 建立了机器人可见范围和时间复杂度的下界（Ω(|E|)），为树结构提供了时间和内存最优的算法，为一般图提供了高效的解决方案。

Conclusion: 证明了有限可见度机器人能够在异步调度下有效解决最小顶点覆盖问题，特别是在树结构上能够实现最优解，为分布式机器人系统提供了理论基础和实用算法。

Abstract: In a connected graph with an autonomous robot swarm with limited visibility,
it is natural to ask whether the robots can be deployed to certain vertices
satisfying a given property using only local knowledge. This paper
affirmatively answers the question with a set of \emph{myopic} (finite
visibility range) luminous robots with the aim of \emph{filling a minimal
vertex cover} (MVC) of a given graph $G = (V, E)$. The graph has special
vertices, called \emph{doors}, through which robots enter sequentially.
Starting from the doors, the goal of the robots is to settle on a set of
vertices that forms a minimal vertex cover of $G$ under the asynchronous
($\mathcal{ASYNC}$) scheduler. We are also interested in achieving the
\emph{minimum vertex cover} (MinVC, which is NP-hard \cite{Karp1972} for
general graphs) for a specific graph class using the myopic robots. We
establish lower bounds on the visibility range for the robots and on the time
complexity (which is $\Omega(|E|)$). We present two algorithms for trees: one
for single door, which is both time and memory-optimal, and the other for
multiple doors, which is memory-optimal and achieves time-optimality when the
number of doors is a constant. Interestingly, our technique achieves MinVC on
trees with a single door. We then move to the general graph, where we present
two algorithms, one for the single door and the other for the multiple doors
with an extra memory of $O(\log \Delta)$ for the robots, where $\Delta$ is the
maximum degree of $G$. All our algorithms run in $O(|E|)$ epochs.

</details>


### [4] [Pure Data Spaces](https://arxiv.org/abs/2508.14271)
*Saul Youssef*

Main category: cs.DC

TL;DR: 本文提出了"纯数据"作为数学和计算的新公理化基础，以"有限序列"为核心概念而非逻辑或类型，发展了一套空间理论来研究数学对象的集合。


<details>
  <summary>Details</summary>
Motivation: 旨在建立基于"数据"而非传统逻辑或类型理论的数学基础框架，探索从纯数据中"有机生长"出经典数学对象的新途径。

Method: 通过定义"空间"作为基本集合概念，研究其自同态半环，从纯数据底物中以最小组合定义"有机生长"出各种数学结构。

Result: 成功从纯数据框架中衍生出自然数、整数、有理数、布尔空间、矩阵代数、高斯整数、四元数以及非结合代数如整数八元数等经典数学对象。

Conclusion: 该框架为数学和计算提供了新的基础视角，展示了从数据出发重建数学的可能性，并为理论和探索开辟了新方向。

Abstract: In a previous work, "pure data" is proposed as an axiomatic foundation for
mathematics and computing, based on "finite sequence" as the foundational
concept rather than based on logic or type. Within this framework, objects with
mathematical meaning are "data" and collections of mathematical objects must
then be associative data, called a "space." A space is then the basic
collection in this framework analogous to sets in Set Theory or objects in
Category Theory. A theory of spaces is developed,where spaces are studied via
their semiring of endomorphisms. To illustrate these concepts, and as a way of
exploring the implications of the framework, pure data spaces are "grown
organically" from the substrate of pure data with minimal combinatoric
definitions. Familiar objects from classical mathematics emerge this way,
including natural numbers, integers, rational numbers, boolean spaces, matrix
algebras, Gaussian Integers, Quaternions, and non-associative algebras like the
Integer Octonions. Insights from these examples are discussed with a view
towards new directions in theory and new exploration.

</details>


### [5] [SSSP-Del: Fully Dynamic Distributed Algorithm for Single-Source Shortest Path](https://arxiv.org/abs/2508.14319)
*Parshan Javanrood,Matei Ripeanu*

Main category: cs.DC

TL;DR: SSSP-Del是一个新的顶点中心、异步、完全分布式的动态单源最短路径算法，能够同时处理边插入和删除操作，在共享无架构中提供低延迟查询结果


<details>
  <summary>Details</summary>
Motivation: 现代图数据规模大且动态变化，传统SSSP算法无法高效处理拓扑变化，现有动态算法往往不能同时处理边增删、分布式内存操作和提供低延迟查询

Method: 提出SSSP-Del算法，采用顶点中心、异步、完全分布式架构，在共享无环境中处理边插入和删除流

Result: 在包含数百万顶点的大型真实世界和合成图上进行全面评估，分析了结果延迟、解决方案稳定性和吞吐量

Conclusion: SSSP-Del算法成功解决了动态图SSSP查询的挑战，能够高效处理大规模动态图中的边变化并提供低延迟查询

Abstract: Modern graphs are both large and dynamic, presenting significant challenges
for fundamental queries, such as the Single-Source Shortest Path (SSSP)
problem. Naively recomputing the SSSP tree after each topology change is
prohibitively expensive, causing on-demand computation to suffer from high
latency. Existing dynamic SSSP algorithms often cannot simultaneously handle
both edge additions and deletions, operate in distributed memory, and provide
low-latency query results. To address these challenges, this paper presents
SSSP-Del, a new vertex-centric, asynchronous, and fully distributed algorithm
for dynamic SSSP. Operating in a shared-nothing architecture, our algorithm
processes streams of both edge insertions and deletions. We conduct a
comprehensive evaluation on large real-world and synthetic graphs with millions
of vertices, and provide a thorough analysis by evaluating result latency,
solution stability, and throughput.

</details>


### [6] [A Hierarchical Sharded Blockchain Balancing Performance and Availability](https://arxiv.org/abs/2508.14457)
*Yongrae Jo,Chanik Park*

Main category: cs.DC

TL;DR: PyloChain是一个分层分片区块链系统，通过本地链和主链的层次结构，在保证可用性的同时提升性能，实现了比现有技术更高的吞吐量和更低延迟。


<details>
  <summary>Details</summary>
Motivation: 区块链网络在成员和交易量增长时面临可扩展性问题，现有分片技术往往以牺牲可用性为代价来追求性能，需要一种平衡可用性和性能的解决方案。

Method: 采用分层架构：多个本地链处理本地交易实现高并行度，主链使用DAG-based mempool保证本地块可用性，并通过BFT共识处理跨片交易，采用调度技术减少本地交易中止，提供细粒度审计机制。

Result: PyloChain实现了1.49倍更高的吞吐量和2.63倍更快的延迟，相比最先进的平衡分层分片区块链表现出更好的性能可扩展性。

Conclusion: PyloChain成功解决了区块链分片中的可用性与性能平衡问题，通过层次化设计和投机执行机制，为大规模区块链应用提供了可行的解决方案。

Abstract: Blockchain networks offer decentralization, transparency, and immutability
for managing critical data but encounter scalability problems as the number of
network members and transaction issuers grows. Sharding is considered a
promising solution to enhance blockchain scalability. However, most existing
blockchain sharding techniques prioritize performance at the cost of
availability (e.g., a failure in a few servers holding a shard leads to data
unavailability). In this paper, we propose PyloChain, a hierarchical sharded
blockchain that balances availability and performance. PyloChain consists of
multiple lower-level local chains and one higher-level main chain. Each local
chain speculatively executes local transactions to achieve high parallelism
across multiple local chains. The main chain leverages a directed-acyclic-graph
(DAG)-based mempool to guarantee local block availability and to enable
efficient Byzantine Fault Tolerance (BFT) consensus to execute global (or
cross-shard) transactions within a collocated sharding. PyloChain speculatively
executes local transactions across multiple local chains to achieve high
parallelism. In order to reduce the number of aborted local transactions,
PyloChain applies a simple scheduling technique to handle global transactions
in the main chain. PyloChain provides a fine-grained auditing mechanism to
mitigate faulty higher-level members by externalizing main chain operations to
lower-level local members. We implemented and evaluated PyloChain,
demonstrating its performance scalability with 1.49x higher throughput and
2.63x faster latency compared to the state-of-the-art balanced hierarchical
sharded blockchain.

</details>


### [7] [Auditable Shared Objects: From Registers to Synchronization Primitives](https://arxiv.org/abs/2508.14506)
*Hagit Attiya,Antonio Fernández Anta,Alessia Milani,Alexandre Rapetti,Corentin Travers*

Main category: cs.DC

TL;DR: 本文扩展了可审计性概念，从单写者寄存器扩展到多写者寄存器和其他共享对象，提供了n写者m读者可审计寄存器的实现，并展示了其与共识数的关系。


<details>
  <summary>Details</summary>
Motivation: 可审计性能够追踪共享对象上的操作，记录谁访问了哪些信息，让数据所有者更好地控制数据。最初研究的是单写者寄存器，本文旨在将可审计性扩展到其他共享对象。

Method: 提供了n写者m读者可审计读/写寄存器的实现，使用(m+n)-滑动寄存器（共识数为m+n），并证明该共识数是必要的。实现还扩展到支持可审计的LL/SC共享对象。

Result: 实现了具有O(n+m)步复杂度的可审计寄存器，展示了与共识数的关系，并通过可审计寄存器实现了防闪烁拒绝列表。

Conclusion: 成功将可审计性扩展到多写者寄存器和其他共享对象，建立了可审计寄存器与其他访问控制对象之间的联系，为数据审计提供了更强大的工具。

Abstract: Auditability allows to track operations performed on a shared object,
recording who accessed which information. This gives data owners more control
on their data. Initially studied in the context of single-writer registers,
this work extends the notion of auditability to other shared objects, and
studies their properties.
  We start by moving from single-writer to multi-writer registers, and provide
an implementation of an auditable $n$-writer $m$-reader read / write register,
with $O(n+m)$ step complexity. This implementation uses $(m+n)$-sliding
registers, which have consensus number $m+n$. We show that this consensus
number is necessary. The implementation extends naturally to support an
auditable load-linked / store-conditional (LL/SC) shared object. LL/SC is a
primitive that supports efficient implementation of many shared objects.
Finally, we relate auditable registers to other access control objects, by
implementing an anti-flickering deny list from auditable registers.

</details>


### [8] [Boosting Payment Channel Network Liquidity with Topology Optimization and Transaction Selection](https://arxiv.org/abs/2508.14524)
*Krishnendu Chatterjee,Jan Matyáš Křišťan,Stefan Schmid,Jakub Svoboda,Michelle Yeo*

Main category: cs.DC

TL;DR: 本文提出了一种支付通道网络(PCN)的近似算法，通过优化网络拓扑和交易路由决策来最小化通道创建成本和交易拒绝成本，实现了O(p)的近似比。


<details>
  <summary>Details</summary>
Motivation: 支付通道网络虽然能缓解区块链扩展性问题，但需要精心设计网络拓扑来最大化交易吞吐量，同时用户需要做出最优的交易转发决策来延长通道寿命。

Method: 考虑p个参与方的交易序列，设计PCN拓扑和通道容量，为每个交易输出接受或拒绝决策，提出O(p)近似算法，在特定交易分布假设下可提升至O(√p)。

Result: 理论分析证明了算法的近似性能，并通过在闪电网络中的实证研究验证了假设和方法的有效性。

Conclusion: 该算法为PCN网络优化提供了有效的解决方案，在理论和实践中都表现出良好的性能，有助于提升支付通道网络的效率和可扩展性。

Abstract: Payment channel networks (PCNs) are a promising technology that alleviates
blockchain scalability by shifting the transaction load from the blockchain to
the PCN. Nevertheless, the network topology has to be carefully designed to
maximise the transaction throughput in PCNs. Additionally, users in PCNs also
have to make optimal decisions on which transactions to forward and which to
reject to prolong the lifetime of their channels. In this work, we consider an
input sequence of transactions over $p$ parties. Each transaction consists of a
transaction size, source, and target, and can be either accepted or rejected
(entailing a cost). The goal is to design a PCN topology among the $p$
cooperating parties, along with the channel capacities, and then output a
decision for each transaction in the sequence to minimise the cost of creating
and augmenting channels, as well as the cost of rejecting transactions. Our
main contribution is an $\mathcal{O}(p)$ approximation algorithm for the
problem with $p$ parties. We further show that with some assumptions on the
distribution of transactions, we can reduce the approximation ratio to
$\mathcal{O}(\sqrt{p})$. We complement our theoretical analysis with an
empirical study of our assumptions and approach in the context of the Lightning
Network.

</details>


### [9] [A Systematic Evaluation of the Potential of Carbon-Aware Execution for Scientific Workflows](https://arxiv.org/abs/2508.14625)
*Kathleen West,Youssef Moawad,Fabian Lehmann,Vasilis Bountris,Ulf Leser,Yehia Elkhatib,Lauritz Thamsen*

Main category: cs.DC

TL;DR: 该研究分析了科学工作流的碳排放问题，展示了碳感知执行策略的潜力，包括时间转移、暂停恢复和资源缩放等方法，可显著减少碳排放达80%以上。


<details>
  <summary>Details</summary>
Motivation: 科学工作流通常计算密集且运行时间长，导致大量能源消耗和碳排放，但现有碳感知计算方法很少专门针对科学工作流，而科学工作流具有延迟容忍、可中断、可扩展和异构等特点，为碳感知计算提供了重要机会。

Method: 研究使用七个真实Nextflow工作流在不同集群基础设施上执行，采用平均和边际碳强度数据估算碳足迹，系统评估碳感知时间转移、暂停恢复策略，并应用工作流和任务级别的资源缩放。

Result: 时间转移策略能够减少超过80%的碳排放，资源缩放策略能够减少67%的碳排放，显示了碳感知工作流执行的巨大潜力。

Conclusion: 科学工作流执行存在显著的碳减排机会，通过碳感知的时间调度和资源优化策略可以大幅降低碳排放，为绿色科学计算提供了有效途径。

Abstract: Scientific workflows are widely used to automate scientific data analysis and
often involve computationally intensive processing of large datasets on compute
clusters. As such, their execution tends to be long-running and
resource-intensive, resulting in substantial energy consumption and, depending
on the energy mix, carbon emissions. Meanwhile, a wealth of carbon-aware
computing methods have been proposed, yet little work has focused specifically
on scientific workflows, even though they present a substantial opportunity for
carbon-aware computing because they are often significantly delay tolerant,
efficiently interruptible, highly scalable and widely heterogeneous. In this
study, we first exemplify the problem of carbon emissions associated with
running scientific workflows, and then show the potential for carbon-aware
workflow execution. For this, we estimate the carbon footprint of seven
real-world Nextflow workflows executed on different cluster infrastructures
using both average and marginal carbon intensity data. Furthermore, we
systematically evaluate the impact of carbon-aware temporal shifting, and the
pausing and resuming of the workflow. Moreover, we apply resource scaling to
workflows and workflow tasks. Finally, we report the potential reduction in
overall carbon emissions, with temporal shifting capable of decreasing
emissions by over 80%, and resource scaling capable of decreasing emissions by
67%.

</details>


### [10] [DAG it off: Latency Prefers No Common Coins](https://arxiv.org/abs/2508.14716)
*Amores-Sesar Ignacio,Grøndal Viktor,Holmgård Adam,Ottendal Mads*

Main category: cs.DC

TL;DR: Black Marlin是首个在部分同步设置中无需可靠广播和共同币原语的DAG拜占庭原子广播协议，具有3轮通信延迟（拜占庭故障下4.25轮）的最优性能


<details>
  <summary>Details</summary>
Motivation: 现有DAG拜占庭原子广播协议依赖可靠广播和共同币原语，存在性能瓶颈，需要开发更高效的协议

Method: 基于有向无环图(DAG)设计的新型拜占庭原子广播协议，摒弃了传统可靠广播和共同币原语，采用部分同步设置

Result: 协议实现3轮通信延迟的最优性能（拜占庭故障下4.25轮），保持最优通信和摊销通信复杂度，在吞吐量和延迟方面优于最先进的DAG协议

Conclusion: Black Marlin协议证明了无需可靠广播和共同币原语也能实现高性能拜占庭原子广播，为分布式系统提供了更高效的共识解决方案

Abstract: We introduce Black Marlin, the first Directed Acyclic Graph (DAG)-based
Byzantine atomic broadcast protocol in a partially synchronous setting that
successfully forgoes the reliable broadcast and common coin primitives. Black
Marlin achieves the optimal latency of 3 rounds of communication (4.25 with
Byzantine faults) while maintaining optimal communication and amortized
communication complexities. We present a formal security analysis of the
protocol, accompanied by empirical evidence that Black Marlin outperforms
state-of-the-art DAG-based protocols in both throughput and latency.

</details>


### [11] [MOHAF: A Multi-Objective Hierarchical Auction Framework for Scalable and Fair Resource Allocation in IoT Ecosystems](https://arxiv.org/abs/2508.14830)
*Kushagra Agrawal,Polat Goktas,Anjan Bandopadhyay,Debolina Ghosh,Junali Jasmine Jena,Mahendra Kumar Gourisaria*

Main category: cs.DC

TL;DR: 提出了MOHAF框架，一种分布式多目标拍卖机制，在IoT环境中联合优化成本、QoS、能效和公平性，相比传统方法显著提升了分配效率和公平性。


<details>
  <summary>Details</summary>
Motivation: IoT生态系统快速发展，传统集中式机制和单目标拍卖模型无法在动态分布式环境中提供平衡的系统性能，需要解决异构资源高效分配问题。

Method: 采用分层聚类降低计算复杂度，结合贪心次模优化策略保证(1-1/e)近似比，使用动态定价机制实时适应资源利用率。

Result: 在Google集群数据上测试显示，MOHAF分配效率(0.263)显著优于Greedy(0.185)、First-Price(0.138)和Random(0.101)方法，同时实现完美公平性(Jain指数=1.000)。

Conclusion: MOHAF具有近线性可扩展性、理论保证和强大的实证性能，为大规模IoT部署提供了实用且适应性强的解决方案，有效协调效率、公平性和可持续性。

Abstract: The rapid growth of Internet of Things (IoT) ecosystems has intensified the
challenge of efficiently allocating heterogeneous resources in highly dynamic,
distributed environments. Conventional centralized mechanisms and
single-objective auction models, focusing solely on metrics such as cost
minimization or revenue maximization, struggle to deliver balanced system
performance. This paper proposes the Multi-Objective Hierarchical Auction
Framework (MOHAF), a distributed resource allocation mechanism that jointly
optimizes cost, Quality of Service (QoS), energy efficiency, and fairness.
MOHAF integrates hierarchical clustering to reduce computational complexity
with a greedy, submodular optimization strategy that guarantees a (1-1/e)
approximation ratio. A dynamic pricing mechanism adapts in real time to
resource utilization, enhancing market stability and allocation quality.
Extensive experiments on the Google Cluster Data trace, comprising 3,553
requests and 888 resources, demonstrate MOHAF's superior allocation efficiency
(0.263) compared to Greedy (0.185), First-Price (0.138), and Random (0.101)
auctions, while achieving perfect fairness (Jain's index = 1.000). Ablation
studies reveal the critical influence of cost and QoS components in sustaining
balanced multi-objective outcomes. With near-linear scalability, theoretical
guarantees, and robust empirical performance, MOHAF offers a practical and
adaptable solution for large-scale IoT deployments, effectively reconciling
efficiency, equity, and sustainability in distributed resource coordination.

</details>


### [12] [Leveraging Hardware-Aware Computation in Mixed-Precision Matrix Multiply: A Tile-Centric Approach](https://arxiv.org/abs/2508.14848)
*Qiao Zhang,Rabab Alomairy,Dali Wang,Zhuowei Gu,Qinglei Cao*

Main category: cs.DC

TL;DR: 一种适应性混合精度GEMM框架，支持细粒度块级的不同精度格式，在多种硬件平台上实现良好的性能扩展性。


<details>
  <summary>Details</summary>
Motivation: 低精度算术优化硬件的出现需要重新评估数值算法，以利用混合精度计算提高性能和能源效率。

Method: 使用PaRSEC运行时系统，在细粒度的块/制图级别支持不同精度格式，平衡多种架构上的工作负载。

Result: 在ARM CPU基础的Fugaku超算、Nvidia GPU基础的A100 DGX和AMD GPU基础的Frontier超算上都实现了良好的性能扩展性。

Conclusion: 通过绑定算法进步与硬件创新，提高计算效率和精度，为各种应用领域推动变革性进步。

Abstract: General Matrix Multiplication (GEMM) is a critical operation underpinning a
wide range of applications in high-performance computing (HPC) and artificial
intelligence (AI). The emergence of hardware optimized for low-precision
arithmetic necessitates a reevaluation of numerical algorithms to leverage
mixed-precision computations, achieving improved performance and energy
efficiency. This research introduces an adaptive mixed-precision GEMM framework
that supports different precision formats at fine-grained tile/block levels. We
utilize the PaRSEC runtime system to balance workloads across various
architectures. The performance scales well on ARM CPU-based Fugaku
supercomputer, Nvidia GPU-based A100 DGX, and AMD GPU-based Frontier
supercomputer. This research aims to enhance computational efficiency and
accuracy by bridging algorithmic advancements and hardware innovations, driving
transformative progress in various applications.

</details>


### [13] [The Cost Advantage of Virtual Machine Migrations: Empirical Insights into Amazon's EC2 Marketspace](https://arxiv.org/abs/2508.14883)
*Benedikt Pittl,Werner Mach,Erich Schikuta*

Main category: cs.DC

TL;DR: 本文分析了云虚拟机组合的成本优化策略，通过使用亚马逊定价数据和Bitbrains数据中心实际使用数据，发现异构市场组合和运行时迁移能显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 随着云提供商推出多种虚拟机交易市场，消费者需要从多个市场和提供商购买虚拟机组合（云组合）。行业需要最佳实践来指导如何创建最优的云组合以实现成本优化。

Method: 使用亚马逊的定价数据和Bitbrains数据中心的真实虚拟机使用数据集进行成本分析，并利用第二个Bitbrains数据集进行结果验证。

Result: 研究发现：1）只有创建异构组合（从不同市场购买虚拟机）才能达到成本最优；2）运行时迁移虚拟机到不同市场对运行6小时至1年的虚拟机特别成本有效；3）大多数虚拟机资源未被充分利用，存在巨大成本优化潜力。

Conclusion: 云虚拟机组合的成本优化需要通过跨市场采购和运行时迁移策略来实现，未来还有很大潜力通过提高资源利用率来进一步降低成本。

Abstract: In recent years, cloud providers have introduced novel approaches for trading
virtual machines. For example, Virtustream introduced so-called muVMs to charge
cloud computing resources while other providers such as Google, Microsoft, or
Amazon re-invented their marketspaces. Today, the market leader Amazon runs six
marketspaces for trading virtual machines. Consumers can purchase bundles of
virtual machines, which are called cloud-portfolios, from multiple marketspaces
and providers. An industry-relevant field of research is to identify best
practices and guidelines on how such optimal portfolios are created. In the
paper at hand, a cost analysis of cloud portfolios is presented. Therefore,
pricing data from Amazon was used as well as a real virtual machine utilization
dataset from the Bitbrains datacenter. The results show that a cost optimum can
only be reached if heterogeneous portfolios are created where virtual machines
are purchased from different marketspaces. Additionally, the cost-benefit of
migrating virtual machines to different marketplaces during runtime is
presented. Such migrations are especially cost-effective for virtual machines
of cloud-portfolios which run between 6 hours and 1 year. The paper further
shows that most of the resources of virtual machines are never utilized by
consumers, which represents a significant future potential for cost
optimization. For the validation of the results, a second dataset of the
Bitbrains datacenter was used, which contains utility data of virtual machines
from a different domain of application.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [MAHL: Multi-Agent LLM-Guided Hierarchical Chiplet Design with Adaptive Debugging](https://arxiv.org/abs/2508.14053)
*Jinwei Tang,Jiayin Qin,Nuo Xu,Pragnya Sudershan Nalla,Yu Cao,Yang,Zhao,Caiwen Ding*

Main category: cs.AR

TL;DR: MAHL是一个基于大型语言模型的层次化chiplet设计生成框架，通过六个智能体协作实现AI算法-硬件映射，显著提高了chiplet设计的生成准确性和PPA优化效果。


<details>
  <summary>Details</summary>
Motivation: 随着AI等工作负载规模和算法复杂度的增加，高维度设计挑战日益突出。虽然LLM在HDL生成方面表现出色，但在2.5D集成和chiplet设计中面临扁平化设计、高验证成本和参数优化不精确等问题。

Method: 提出MAHL框架，包含六个协作智能体：层次化描述生成、检索增强代码生成、多样化流程验证和多粒度设计空间探索，共同实现高效的chiplet设计生成和PPA优化。

Result: 实验显示MAHL显著提高了简单RTL设计的生成准确性，在最佳情况下将真实chiplet设计的生成准确率（Pass@5）从0提升到0.72，相比最先进的CLARIE方法在某些优化目标下达到相当或更优的PPA结果。

Conclusion: MAHL框架成功解决了LLM驱动chiplet设计的关键挑战，为高效生成优化PPA的chiplet设计提供了有效解决方案，展示了层次化LLM方法在先进芯片设计中的潜力。

Abstract: As program workloads (e.g., AI) increase in size and algorithmic complexity,
the primary challenge lies in their high dimensionality, encompassing computing
cores, array sizes, and memory hierarchies. To overcome these obstacles,
innovative approaches are required. Agile chip design has already benefited
from machine learning integration at various stages, including logic synthesis,
placement, and routing. With Large Language Models (LLMs) recently
demonstrating impressive proficiency in Hardware Description Language (HDL)
generation, it is promising to extend their abilities to 2.5D integration, an
advanced technique that saves area overhead and development costs. However,
LLM-driven chiplet design faces challenges such as flatten design, high
validation cost and imprecise parameter optimization, which limit its chiplet
design capability. To address this, we propose MAHL, a hierarchical LLM-based
chiplet design generation framework that features six agents which
collaboratively enable AI algorithm-hardware mapping, including hierarchical
description generation, retrieval-augmented code generation, diverseflow-based
validation, and multi-granularity design space exploration. These components
together enhance the efficient generation of chiplet design with optimized
Power, Performance and Area (PPA). Experiments show that MAHL not only
significantly improves the generation accuracy of simple RTL design, but also
increases the generation accuracy of real-world chiplet design, evaluated by
Pass@5, from 0 to 0.72 compared to conventional LLMs under the best-case
scenario. Compared to state-of-the-art CLARIE (expert-based), MAHL achieves
comparable or even superior PPA results under certain optimization objectives.

</details>


### [15] [Revisit Choice Network for Synthesis and Technology Mapping](https://arxiv.org/abs/2508.14068)
*Chen Chen,Jiaqi Yin,Cunxi Yu*

Main category: cs.AR

TL;DR: Cristal是一种新的布尔选择网络构建方法，通过代表性逻辑锥搜索、结构变异和优先级选择等技术，生成更少但更高质量的选择节点，在技术映射后阶段显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的无损合成方法虽然能生成多个快照并合并功能等效节点，但往往忽视选择节点的质量，无法保证这些选择对技术映射真正有效。

Method: 提出Cristal框架，包含代表性逻辑锥搜索、通过等式饱和进行结构变异生成多样化选择结构、优先级排序选择以及选择网络构建和验证的新流程。

Result: 在延迟导向模式下平均减少3.85%/8.35%（面积/延迟），面积导向模式下减少0.11%/2.74%，大规模案例运行时间减少63.77%，在IWLS 2005、ISCAS'89和EPFL基准测试中表现优异。

Conclusion: Cristal通过构建更少但更高质量的选择节点，显著提升了布尔选择网络构建的效果，在技术映射后阶段超越了现有最先进方法。

Abstract: Choice network construction is a critical technique for alleviating
structural bias issues in Boolean optimization, equivalence checking, and
technology mapping. Previous works on lossless synthesis utilize independent
optimization to generate multiple snapshots, and use simulation and SAT solvers
to identify functionally equivalent nodes. These nodes are then merged into a
subject graph with choice nodes. However, such methods often neglect the
quality of these choices, raising the question of whether they truly contribute
to effective technology mapping.
  This paper introduces Cristal, a novel methodology and framework for
constructing Boolean choice networks. Specifically, Cristal introduces a new
flow of choice network-based synthesis and mapping, including representative
logic cone search, structural mutation for generating diverse choice structures
via equality saturation, and priority-ranking choice selection along with
choice network construction and validation. Through these techniques, Cristal
constructs fewer but higher-quality choices.
  Our experimental results demonstrate that Cristal outperforms the
state-of-the-art Boolean choice network construction implemented in ABC in the
post-mapping stage, achieving average reductions of 3.85%/8.35% (area/delay) in
delay-oriented mode, 0.11%/2.74% in area-oriented mode, and a 63.77% runtime
reduction on large-scale cases across a diverse set of combinational circuits
from the IWLS 2005, ISCAS'89, and EPFL benchmark suites.

</details>


### [16] [AI Agents for Photonic Integrated Circuit Design Automation](https://arxiv.org/abs/2508.14123)
*Ankita Sharma,YuQi Fu,Vahid Ansari,Rishabh Iyer,Fiona Kuang,Kashish Mistry,Raisa Islam Aishy,Sara Ahmad,Joaquin Matres,Dirk R. Englund,Joyce K. S. Poon*

Main category: cs.AR

TL;DR: PhIDO是一个多智能体框架，可将自然语言光子集成电路设计请求转换为布局掩模文件，在单器件设计中成功率高达91%，在15组件以下设计中最佳模型达到约57%的成功率。


<details>
  <summary>Details</summary>
Motivation: 解决光子集成电路设计中从自然语言描述到实际布局文件的自动化转换问题，提高设计效率和可访问性。

Method: 开发多智能体框架PhIDO，使用7种大型语言模型进行推理，在包含102个设计描述的测试平台上进行评估，涵盖从单器件到112组件的光子集成电路。

Result: 单器件设计成功率最高达91%；对于≤15组件的设计查询，o1、Gemini-2.5-pro和Claude Opus 4达到约57%的端到端pass@5成功率，其中Gemini-2.5-pro所需输出token最少且成本最低。

Conclusion: 该框架展示了自动化光子集成电路设计的潜力，未来发展方向包括标准化知识表示、扩展数据集、增强验证和机器人自动化。

Abstract: We present Photonics Intelligent Design and Optimization (PhIDO), a
multi-agent framework that converts natural-language photonic integrated
circuit (PIC) design requests into layout mask files. We compare 7 reasoning
large language models for PhIDO using a testbench of 102 design descriptions
that ranged from single devices to 112-component PICs. The success rate for
single-device designs was up to 91%. For design queries with less than or equal
to 15 components, o1, Gemini-2.5-pro, and Claude Opus 4 achieved the highest
end-to-end pass@5 success rates of approximately 57%, with Gemini-2.5-pro
requiring the fewest output tokens and lowest cost. The next steps toward
autonomous PIC development include standardized knowledge representations,
expanded datasets, extended verification, and robotic automation.

</details>


### [17] [Cross-Layer Design of Vector-Symbolic Computing: Bridging Cognition and Brain-Inspired Hardware Acceleration](https://arxiv.org/abs/2508.14245)
*Shuting Du,Mohamed Ibrahim,Zishen Wan,Luqi Zheng,Boheng Zhao,Zhenkun Fan,Che-Kai Liu,Tushar Krishna,Arijit Raychowdhury,Haitong Li*

Main category: cs.AR

TL;DR: 该论文旨在弥合向量符号架构(VSA)理论软件探索与高效硬件架构开发之间的鸿沟，从协同设计角度为研究人员提供见解，包括VSA原理、硬件技术分析、跨层设计方法，并提出了首个内存计算层次认知硬件系统。


<details>
  <summary>Details</summary>
Motivation: 尽管VSA在各种认知应用中广泛部署且硬件解决方案不断发展，但关于VSA硬件与算法融合的全面统一论述仍然有限，需要弥合理论软件探索与硬件架构开发之间的差距。

Method: 1) 介绍向量符号计算原理和核心数学操作；2) 深入分析VSA硬件技术(模拟、混合信号、数字电路)；3) 提出VSA跨层设计方法；4) 提出首个内存计算层次认知硬件系统作为具体演示。

Result: 通过详细性能特征和权衡分析比较了VSA硬件实现，提取了任意VSA公式开发的设计指南，展示了协同设计方法在效率、灵活性和可扩展性方面的优势。

Conclusion: 论文为VSA的硬件/软件协同设计提供了系统框架和具体实现，指出了未来研究面临的开放挑战，推动了VSA技术在认知计算领域的进一步发展。

Abstract: Vector Symbolic Architectures (VSAs) have been widely deployed in various
cognitive applications due to their simple and efficient operations. The
widespread adoption of VSAs has, in turn, spurred the development of numerous
hardware solutions aimed at optimizing their performance. Despite these
advancements, a comprehensive and unified discourse on the convergence of
hardware and algorithms in the context of VSAs remains somewhat limited. The
paper aims to bridge the gap between theoretical software-level explorations
and the development of efficient hardware architectures and emerging technology
fabrics for VSAs, providing insights from the co-design aspect for researchers
from either side. First, we introduce the principles of vector-symbolic
computing, including its core mathematical operations and learning paradigms.
Second, we provide an in-depth discussion on hardware technologies for VSAs,
analyzing analog, mixed-signal, and digital circuit design styles. We compare
hardware implementations of VSAs by carrying out detailed analysis of their
performance characteristics and tradeoffs, allowing us to extract design
guidelines for the development of arbitrary VSA formulations. Third, we discuss
a methodology for cross-layer design of VSAs that identifies synergies across
layers and explores key ingredients for hardware/software co-design of VSAs.
Finally, as a concrete demonstration of this methodology, we propose the first
in-memory computing hierarchical cognition hardware system, showcasing the
efficiency, flexibility, and scalability of this co-design approach. The paper
concludes with a discussion of open research challenges for future
explorations.

</details>


### [18] [Power Stabilization for AI Training Datacenters](https://arxiv.org/abs/2508.14318)
*Esha Choukse,Brijesh Warrier,Scot Heath,Luz Belmont,April Zhao,Hassan Ali Khan,Brian Harry,Matthew Kappel,Russell J. Hewett,Kushal Datta,Yu Pei,Caroline Lichtenberger,John Siegler,David Lukofsky,Zaid Kahn,Gurpreet Sahota,Andy Sullivan,Charles Frederick,Hien Thai,Rebecca Naughton,Daniel Jurnove,Justin Harp,Reid Carper,Nithish Mahalingam,Srini Varkala,Alok Gautam Kumbhare,Satyajit Desai,Venkatesh Ramamurthy,Praneeth Gottumukkala,Girish Bhatia,Kelsey Wildstone,Laurentiu Olariu,Mohammed Ayna,Mike Kendrick,Ricardo Bianchini,Aaron Hurst,Reza Zamani,Xin Li,Gene Oden,Rory Carmichael,Tom Li,Apoorv Gupta,Nilesh Dattani,Lawrence Marwong,Rob Nertney,Jeff Liott,Miro Enev,Divya Ramakrishnan,Ian Buck,Jonah Alben*

Main category: cs.AR

TL;DR: 大规模AI训练工作负载存在显著的功率波动问题，计算密集型阶段和通信阶段的功率需求差异巨大，可能导致电网基础设施损坏。论文提出了跨软件、GPU硬件和数据中心基础设施的多层次解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着AI训练规模扩展到数万个GPU，功率管理面临严峻挑战。计算阶段和通信阶段的功率需求差异导致大幅功率波动，这种波动的频率特性可能与电网关键频率共振，对电网基础设施造成物理损坏。

Method: 采用跨栈创新解决方案：软件层面优化、GPU硬件改进和数据中心基础设施调整。使用真实硬件和微软内部云功率模拟器进行严格测试，评估各种干预措施在实际条件下的效果。

Result: 通过多管齐下的方法有效稳定了AI训练工作负载的功率波动。测试结果表明，所提出的解决方案能够显著降低功率波动的幅度和频率风险。

Conclusion: 为了解决AI训练规模持续扩展的安全问题，需要采用综合性的功率稳定策略。跨软件、硬件和基础设施的多层次方法被证明是有效的，为未来更大规模的AI训练提供了可行的功率管理方案。

Abstract: Large Artificial Intelligence (AI) training workloads spanning several tens
of thousands of GPUs present unique power management challenges. These arise
due to the high variability in power consumption during the training. Given the
synchronous nature of these jobs, during every iteration there is a
computation-heavy phase, where each GPU works on the local data, and a
communication-heavy phase where all the GPUs synchronize on the data. Because
compute-heavy phases require much more power than communication phases, large
power swings occur. The amplitude of these power swings is ever increasing with
the increase in the size of training jobs. An even bigger challenge arises from
the frequency spectrum of these power swings which, if harmonized with critical
frequencies of utilities, can cause physical damage to the power grid
infrastructure. Therefore, to continue scaling AI training workloads safely, we
need to stabilize the power of such workloads. This paper introduces the
challenge with production data and explores innovative solutions across the
stack: software, GPU hardware, and datacenter infrastructure. We present the
pros and cons of each of these approaches and finally present a multi-pronged
approach to solving the challenge. The proposed solutions are rigorously tested
using a combination of real hardware and Microsoft's in-house cloud power
simulator, providing critical insights into the efficacy of these interventions
under real-world conditions.

</details>


### [19] [Computing-In-Memory Dataflow for Minimal Buffer Traffic](https://arxiv.org/abs/2508.14375)
*Choongseok Song,Doo Seok Jeong*

Main category: cs.AR

TL;DR: 该论文提出了一种新的存储计算数据流，专门优化深度卷积运算，在MobileNet和EfficientNet模型上实现了缓冲区交互量减少77.4-87.0%，总体能耗和延迟分别降低10.1-17.9%和15.6-27.8%。


<details>
  <summary>Details</summary>
Motivation: 解决存储计算宏在加速深度卷积时遇到的问题：存储器利用率低和缓冲区交互量大，后者对延迟和能耗有重大影响但被忽视。

Method: 提出一种新的存储计算数据流，通过最大化数据重用和改善存储利用来大幅减少缓冲区交互。该方法基于坚实的理论基础。

Result: 在MobileNet和EfficientNet模型上，新数据流实现了缓冲区交互量减少77.4-87.0%，总体数据交互能耗降低10.1-17.9%，延迟降低15.6-27.8%（与传统权重静态数据流相比）。

Conclusion: 该研究提出的新型存储计算数据流有效解决了深度卷积加速中的缓冲区性能瓶颈问题，为边缘AI设备提供了更高能效的解决方案。

Abstract: Computing-In-Memory (CIM) offers a potential solution to the memory wall
issue and can achieve high energy efficiency by minimizing data movement,
making it a promising architecture for edge AI devices. Lightweight models like
MobileNet and EfficientNet, which utilize depthwise convolution for feature
extraction, have been developed for these devices. However, CIM macros often
face challenges in accelerating depthwise convolution, including
underutilization of CIM memory and heavy buffer traffic. The latter, in
particular, has been overlooked despite its significant impact on latency and
energy consumption. To address this, we introduce a novel CIM dataflow that
significantly reduces buffer traffic by maximizing data reuse and improving
memory utilization during depthwise convolution. The proposed dataflow is
grounded in solid theoretical principles, fully demonstrated in this paper.
When applied to MobileNet and EfficientNet models, our dataflow reduces buffer
traffic by 77.4-87.0%, leading to a total reduction in data traffic energy and
latency by 10.1-17.9% and 15.6-27.8%, respectively, compared to the baseline
(conventional weight-stationary dataflow).

</details>


### [20] [Wit-HW: Bug Localization in Hardware Design Code via Witness Test Case Generation](https://arxiv.org/abs/2508.14414)
*Ruiyang Ma,Daikang Kuang,Ziqian Liu,Jiaxi Zhang,Ping Fan,Guojie Luo*

Main category: cs.AR

TL;DR: 本文提出Wit-HW框架，通过生成有效的见证测试用例来增强硬件bug定位，显著优于现有技术


<details>
  <summary>Details</summary>
Motivation: 现有硬件调试技术仅使用单个触发bug的测试用例，无法有效分析复杂硬件系统和定位深层bug的根源

Method: 将硬件bug定位问题转化为测试生成问题，定义有效见证测试用例标准，采用基于变异策略生成此类测试用例，通过频谱分析方法分析通过和失败测试用例的执行差异

Result: 在41个硬件bug上评估，Wit-HW在Top-1、Top-5、Top-10排名中分别有效定位49%、73%、88%的bug，显著优于最先进技术；在13个真实bug上也表现出鲁棒性能

Conclusion: Wit-HW框架通过生成见证测试用例有效提升了硬件bug定位能力，为解决复杂硬件系统的调试问题提供了有效方案

Abstract: Debugging hardware designs requires significant manual effort during hardware
development. After engineers identify a bug-triggering test case in
simulation-based hardware verification, they usually spend considerable time
analyzing the execution trace to localize the bug. Although numerous automated
hardware debugging techniques exist, they are not applicable to large designs
and deep bugs. A primary reason for their limitations is that these techniques
only utilize the information of a single bug-triggering test case for bug
localization, which prevents them from effectively analyzing intricate hardware
systems and figure out the root cause of bugs. To solve this problem, in this
paper, we transform the hardware bug localization problem into a test
generation problem, aiming to find a set of effective witness test cases beyond
the initial bug-triggering test case to enhance hardware bug localization.
Witness test cases refer to the cases that do not trigger the bug in the faulty
design. By analyzing the execution differences between passing and failing test
cases with spectrum-based method, we can eliminate innocent design statements
and localize the buggy ones. To further refine the suspicious area, we define
the criteria for effective witness test cases and use a mutation-based strategy
to generate such test cases. Based on this approach, we propose an automated
hardware bug localization framework named Wit-HW. We evaluate Wit-HW on 41 bugs
from various hardware designs. The experimental results show that Wit-HW
effectively localize 49%, 73%, 88% bugs within Top-1, Top-5, Top-10 ranks,
significantly outperforming state-of-the-art bug localization techniques.
Additionally, we evaluate Wit-HW on 13 real-world bugs collected from
open-source hardware projects, showcasing the robust performance of our method.

</details>


### [21] [An Open-Source HW-SW Co-Development Framework Enabling Efficient Multi-Accelerator Systems](https://arxiv.org/abs/2508.14582)
*Ryan Albert Antonio,Joren Dumoulin,Xiaoling Yi,Josse Van Delm,Yunhao Deng,Guilherme Paim,Marian Verhelst*

Main category: cs.AR

TL;DR: SNAX是一个开源的HW-SW集成框架，通过新颖的混合耦合方案实现高效的多加速器平台，在低功耗异构SoC中展示出卓越性能，神经网络性能提升10倍以上，加速器利用率超过90%。


<details>
  <summary>Details</summary>
Motivation: 当前异构加速器计算集群的集成策略存在数据移动效率低、硬件软件兼容性问题，缺乏统一的方法来平衡性能和易用性。

Method: 提出SNAX框架，采用混合耦合方案（松散耦合的异步控制和紧密耦合的数据访问），包含可重用硬件模块和基于MLIR的自定义编译器，自动化系统管理任务。

Result: 在低功耗异构SoC中，SNAX实现了>10倍的神经网络性能提升，同时保持>90%的加速器利用率，展示了高效的加速器集成和编程能力。

Conclusion: SNAX框架能够快速开发和部署定制的多加速器计算集群，有效解决了异构加速器平台集成中的效率和兼容性问题。

Abstract: Heterogeneous accelerator-centric compute clusters are emerging as efficient
solutions for diverse AI workloads. However, current integration strategies
often compromise data movement efficiency and encounter compatibility issues in
hardware and software. This prevents a unified approach that balances
performance and ease of use. To this end, we present SNAX, an open-source
integrated HW-SW framework enabling efficient multi-accelerator platforms
through a novel hybrid-coupling scheme, consisting of loosely coupled
asynchronous control and tightly coupled data access. SNAX brings reusable
hardware modules designed to enhance compute accelerator utilization, and its
customizable MLIR-based compiler to automate key system management tasks,
jointly enabling rapid development and deployment of customized
multi-accelerator compute clusters. Through extensive experimentation, we
demonstrate SNAX's efficiency and flexibility in a low-power heterogeneous SoC.
Accelerators can easily be integrated and programmed to achieve > 10x
improvement in neural network performance compared to other accelerator systems
while maintaining accelerator utilization of > 90% in full system operation.

</details>


### [22] [ListenToJESD204B: A Lightweight Open-Source JESD204B IP Core for FPGA-Based Ultrasound Acquisition systems](https://arxiv.org/abs/2508.14798)
*Soumyo Bhattacharjee,Federico Villani,Christian Vogt,Andrea Cossettini,Luca Benini*

Main category: cs.AR

TL;DR: 开源的JESD204B接收器IP核心，为超声系统提供高速、低延迟、低资源占用的解决方案


<details>
  <summary>Details</summary>
Motivation: 传统的低电压差分信号链路无法满足超声系统对数百个同步通道的需求，而商业JESD204B IP核心存在专利、费用高、资源占用大等问题

Method: 开发基于SystemVerilog的开源接收器IP核心，支持4个12.8Gb/s速率的GTH/GTY连路，采用模块化数据路径设计，包含每连路弹性缓冲器、SYSREF锁定的LMFC生成器和可选LFSR解码功能

Result: 该IP核心仅占用107个可配置逻辑块（约437个LUT），资源占用比商业IP减少79%，支持循环准确的AXI-Stream数据输出和确定性Subclass 1延迟，通过了半小时的稳定性测试

Conclusion: ListenToJESD204B为高速超声系统提供了一个高效、可扩展、开源的JESD204B接收方案，在保持协议兼容性的同时显著降低了资源占用和成本

Abstract: The demand for hundreds of tightly synchronized channels operating at tens of
MSPS in ultrasound systems exceeds conventional low-voltage differential
signaling links' bandwidth, pin count, and latency. Although the JESD204B
serial interface mitigates these limitations, commercial FPGA IP cores are
proprietary, costly, and resource-intensive. We present ListenToJESD204B, an
open-source receiver IP core released under a permissive Solderpad 0.51 license
for AMD Xilinx Zynq UltraScale+ devices. Written in synthesizable
SystemVerilog, the core supports four GTH/GTY lanes at 12.8 Gb/s and provides
cycle-accurate AXI-Stream data alongside deterministic Subclass~1 latency. It
occupies only 107 configurable logic blocks (approximately 437 LUTs),
representing a 79\% reduction compared to comparable commercially available IP.
A modular data path featuring per-lane elastic buffers, SYSREF-locked LMFC
generation, and optional LFSR descrambling facilitates scaling to high lane
counts. We verified protocol compliance through simulation against the Xilinx
JESD204C IP in JESD204B mode and on hardware using TI AFE58JD48 ADCs. Block
stability was verified by streaming 80 MSPS, 16-bit samples over two 12.8 Gb/s
links for 30 minutes with no errors.

</details>
