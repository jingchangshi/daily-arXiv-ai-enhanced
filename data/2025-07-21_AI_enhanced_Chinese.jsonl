{"id": "2507.13522", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.13522", "abs": "https://arxiv.org/abs/2507.13522", "authors": ["Ankit Bhardwaj", "Weiyang Wang", "Jeremy Carin", "Adam Belay", "Manya Ghobadi"], "title": "Checkmate: Zero-Overhead Model Checkpointing via Network Gradient Replication", "comment": "18 pages, 11 figures", "summary": "This paper presents Checkmate, a system that enables per-iteration\ncheckpointing in DNN training without any training slowdown. The traditional\napproach to checkpointing requires a pause in training to copy model states to\na separate location, allowing the state to be restored in the event of failure.\nThis approach fundamentally has a tradeoff between the frequency of checkpoints\nand the cost of a failure. We avoid this tradeoff; our key insight is that in\ndata-parallel training, all information necessary to create a checkpoint\nalready exists in the network as gradients. Our core contribution is a new\nmulticast abstraction that simultaneously delivers gradients to a separate\nCPU-based shadow cluster. The shadow maintains a checkpoint by applying those\ngradients to a copy of the model. Our evaluation shows that Checkmate performs\nper-iteration checkpointing with training throughput comparable to an ideal\nno-checkpoint baseline. Checkmate achieves 5 to 34.5x more frequent\ncheckpointing compared to state-of-the-art checkpointing systems, resulting in\n80% to 97.1% reduction in repeated work per failure. At the same checkpointing\nfrequency, Checkmate delivers 1.3x to 6.5x throughput compared to other\nsystems.", "AI": {"tldr": "Checkmate\u7cfb\u7edf\u5b9e\u73b0\u4e86DNN\u8bad\u7ec3\u4e2d\u7684\u6bcf\u8fed\u4ee3\u68c0\u67e5\u70b9\u529f\u80fd\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u68c0\u67e5\u70b9\u65b9\u6cd5\u7684\u8bad\u7ec3\u6682\u505c\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u68af\u5ea6\u4fe1\u606f\u5b9e\u73b0\u9ad8\u6548\u68c0\u67e5\u70b9\u3002", "motivation": "\u4f20\u7edf\u68c0\u67e5\u70b9\u65b9\u6cd5\u9700\u8981\u5728\u8bad\u7ec3\u6682\u505c\u65f6\u590d\u5236\u6a21\u578b\u72b6\u6001\uff0c\u5b58\u5728\u68c0\u67e5\u70b9\u9891\u7387\u4e0e\u6545\u969c\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u3002Checkmate\u65e8\u5728\u6d88\u9664\u8fd9\u79cd\u6743\u8861\u3002", "method": "\u5229\u7528\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u4e2d\u7684\u68af\u5ea6\u4fe1\u606f\uff0c\u901a\u8fc7\u65b0\u7684\u591a\u64ad\u62bd\u8c61\u5c06\u68af\u5ea6\u4f20\u9012\u5230\u57fa\u4e8eCPU\u7684\u5f71\u5b50\u96c6\u7fa4\uff0c\u5f71\u5b50\u96c6\u7fa4\u901a\u8fc7\u5e94\u7528\u68af\u5ea6\u7ef4\u62a4\u68c0\u67e5\u70b9\u3002", "result": "Checkmate\u5b9e\u73b0\u4e86\u6bcf\u8fed\u4ee3\u68c0\u67e5\u70b9\uff0c\u8bad\u7ec3\u541e\u5410\u91cf\u4e0e\u65e0\u68c0\u67e5\u70b9\u57fa\u7ebf\u76f8\u5f53\uff0c\u68c0\u67e5\u70b9\u9891\u7387\u63d0\u9ad85\u81f334.5\u500d\uff0c\u91cd\u590d\u5de5\u4f5c\u51cf\u5c1180%\u81f397.1%\u3002", "conclusion": "Checkmate\u5728\u4fdd\u6301\u9ad8\u68c0\u67e5\u70b9\u9891\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u541e\u5410\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002"}}
{"id": "2507.13601", "categories": ["cs.DC", "cs.ET", "cs.PF", "90B36, 90C27, 68M14, 68W40", "C.1.2; C.1.4; C.3.1; D.1.3; G.1.6"], "pdf": "https://arxiv.org/pdf/2507.13601", "abs": "https://arxiv.org/abs/2507.13601", "authors": ["Jorge Villarrubia", "Luis Costero", "Francisco D. Igual", "Katzalin Olcoz"], "title": "Leveraging Multi-Instance GPUs through moldable task scheduling", "comment": null, "summary": "NVIDIA MIG (Multi-Instance GPU) allows partitioning a physical GPU into\nmultiple logical instances with fully-isolated resources, which can be\ndynamically reconfigured. This work highlights the untapped potential of MIG\nthrough moldable task scheduling with dynamic reconfigurations. Specifically,\nwe propose a makespan minimization problem for multi-task execution under MIG\nconstraints. Our profiling shows that assuming monotonicity in task work with\nrespect to resources is not viable, as is usual in multicore scheduling.\nRelying on a state-of-the-art proposal that does not require such an\nassumption, we present FAR, a 3-phase algorithm to solve the problem. Phase 1\nof FAR builds on a classical task moldability method, phase 2 combines Longest\nProcessing Time First and List Scheduling with a novel repartitioning tree\nheuristic tailored to MIG constraints, and phase 3 employs local search via\ntask moves and swaps. FAR schedules tasks in batches offline, concatenating\ntheir schedules on the fly in an improved way that favors resource reuse.\nExcluding reconfiguration costs, the List Scheduling proof shows an\napproximation factor of 7/4 on the NVIDIA A30 model. We adapt the technique to\nthe particular constraints of an NVIDIA A100/H100 to obtain an approximation\nfactor of 2. Including the reconfiguration cost, our real-world experiments\nreveal a makespan with respect to the optimum no worse than 1.22x for a\nwell-known suite of benchmarks, and 1.10x for synthetic inputs inspired by real\nkernels. We obtain good experimental results for each batch of tasks, but also\nin the concatenation of batches, with large improvements over the\nstate-of-the-art and proposals without GPU reconfiguration. Beyond the\nalgorithm, the paper demonstrates the research potential of the MIG technology\nand suggests useful metrics, workload characterizations and evaluation\ntechniques for future work in this field.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFAR\u7684\u4e09\u9636\u6bb5\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728NVIDIA MIG\u6280\u672f\u4e0b\u5b9e\u73b0\u591a\u4efb\u52a1\u8c03\u5ea6\u7684\u6700\u5c0f\u5316\u5b8c\u6210\u65f6\u95f4\uff08makespan\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "NVIDIA MIG\u6280\u672f\u5141\u8bb8\u52a8\u6001\u5206\u533aGPU\u8d44\u6e90\uff0c\u4f46\u5176\u5728\u591a\u4efb\u52a1\u8c03\u5ea6\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u91cd\u914d\u7f6e\u4f18\u5316\u4efb\u52a1\u8c03\u5ea6\u3002", "method": "\u63d0\u51faFAR\u7b97\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u57fa\u4e8e\u7ecf\u5178\u4efb\u52a1\u53ef\u5851\u6027\u65b9\u6cd5\uff1b2\uff09\u7ed3\u5408\u6700\u957f\u5904\u7406\u65f6\u95f4\u4f18\u5148\u548c\u5217\u8868\u8c03\u5ea6\uff0c\u5e76\u5f15\u5165\u9488\u5bf9MIG\u7ea6\u675f\u7684\u91cd\u65b0\u5206\u533a\u6811\u542f\u53d1\u5f0f\uff1b3\uff09\u901a\u8fc7\u4efb\u52a1\u79fb\u52a8\u548c\u4ea4\u6362\u8fdb\u884c\u5c40\u90e8\u641c\u7d22\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u4e0d\u8003\u8651\u91cd\u914d\u7f6e\u6210\u672c\u65f6\uff0cFAR\u5728NVIDIA A30\u4e0a\u7684\u8fd1\u4f3c\u56e0\u5b50\u4e3a7/4\uff0c\u5728A100/H100\u4e0a\u4e3a2\u3002\u8003\u8651\u91cd\u914d\u7f6e\u6210\u672c\u540e\uff0c\u5b9e\u9645\u6027\u80fd\u63a5\u8fd1\u6700\u4f18\uff081.22x-1.10x\uff09\u3002", "conclusion": "FAR\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86MIG\u6280\u672f\u7684\u7814\u7a76\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u6307\u6807\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2507.13833", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.13833", "abs": "https://arxiv.org/abs/2507.13833", "authors": ["Zhixin Wang", "Tianyi Zhou", "Liming Liu", "Ao Li", "Jiarui Hu", "Dian Yang", "Jinlong Hou", "Siyuan Feng", "Yuan Cheng", "Yuan Qi"], "title": "DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training", "comment": null, "summary": "Reinforcement learning (RL) has become the pivotal post-training technique\nfor large language model. Effectively scaling reinforcement learning is now the\nkey to unlocking advanced reasoning capabilities and ensuring safe,\ngoal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually\nemploy a hybrid-controller architecture where a single-controller dispatches\nthe overall execution logic and manages overall data transfer and the\nmulti-controller executes distributed computation. For large-scale\nreinforcement learning, minor load imbalances can introduce significant\nbottlenecks, ultimately constraining the scalability of the system. To address\nthis limitation, we introduce DistFlow, a novel, fully distributed RL framework\ndesigned to break scaling barrier. We adopt a multi-controller paradigm that\ndispatches data transfer and execution tasks to all workers, which eliminates\nthe centralized node. This allows each worker to operate independently, leading\nto near-linear scalability up to thousands of GPUs and dramatic efficiency\ngains. Furthermore, our architecture decouples resource configuration from\nexecution logic, allowing each worker to have a unique execution flow, offering\nsignificant flexibility for rapid and cost-effective algorithmic\nexperimentation. Extensive experiments show that DistFlow achieves excellent\nlinear scalability and up to a 7x end-to-end throughput improvement over\nstate-of-the-art (SOTA) frameworks.", "AI": {"tldr": "DistFlow\u662f\u4e00\u79cd\u65b0\u578b\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u63a7\u5236\u5668\u67b6\u6784\u6d88\u9664\u4e2d\u5fc3\u8282\u70b9\uff0c\u5b9e\u73b0\u8fd1\u7ebf\u6027\u6269\u5c55\u548c\u9ad8\u6548\u8ba1\u7b97\u3002", "motivation": "\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u8d1f\u8f7d\u4e0d\u5e73\u8861\u4f1a\u5bfc\u81f4\u74f6\u9888\uff0c\u9650\u5236\u7cfb\u7edf\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u591a\u63a7\u5236\u5668\u8303\u5f0f\uff0c\u5c06\u6570\u636e\u4f20\u8f93\u548c\u6267\u884c\u4efb\u52a1\u5206\u914d\u7ed9\u6240\u6709\u5de5\u4f5c\u8282\u70b9\uff0c\u6d88\u9664\u4e2d\u5fc3\u8282\u70b9\uff0c\u5b9e\u73b0\u72ec\u7acb\u8fd0\u884c\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDistFlow\u5177\u6709\u4f18\u5f02\u7684\u7ebf\u6027\u6269\u5c55\u6027\uff0c\u541e\u5410\u91cf\u6bd4\u73b0\u6709\u6846\u67b6\u63d0\u53477\u500d\u3002", "conclusion": "DistFlow\u7a81\u7834\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u6269\u5c55\u9650\u5236\uff0c\u4e3a\u9ad8\u6548\u7b97\u6cd5\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u3002"}}
{"id": "2507.14069", "categories": ["cs.DC", "cs.AI", "cs.ET", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.14069", "abs": "https://arxiv.org/abs/2507.14069", "authors": ["Shuiguang Deng", "Di Yu", "Changze Lv", "Xin Du", "Linshan Jiang", "Xiaofan Zhao", "Wentao Tong", "Xiaoqing Zheng", "Weijia Fang", "Peng Zhao", "Gang Pan", "Schahram Dustdar", "Albert Y. Zomaya"], "title": "Edge Intelligence with Spiking Neural Networks", "comment": "This work has been submitted to Proceeding of IEEE for possible\n  publication", "summary": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u7684\u8fb9\u7f18\u667a\u80fd\uff08EdgeSNNs\uff09\uff0c\u63a2\u8ba8\u5176\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5305\u62ec\u4f4e\u529f\u8017\u8ba1\u7b97\u3001\u8bbe\u5907\u7aef\u5b66\u4e60\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u5b58\u5728\u5ef6\u8fdf\u3001\u5e26\u5bbd\u548c\u9690\u79c1\u95ee\u9898\uff0c\u800cSNNs\u901a\u8fc7\u6a21\u62df\u751f\u7269\u795e\u7ecf\u5143\u52a8\u6001\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u529f\u8017\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u7cfb\u7edf\u5206\u7c7b\u4e86EdgeSNNs\u7684\u57fa\u7840\uff08\u5982\u795e\u7ecf\u5143\u6a21\u578b\u3001\u5b66\u4e60\u7b97\u6cd5\u548c\u786c\u4ef6\u5e73\u53f0\uff09\uff0c\u5e76\u6df1\u5165\u8ba8\u8bba\u4e86\u8bbe\u5907\u7aef\u63a8\u7406\u3001\u8d44\u6e90\u611f\u77e5\u8bad\u7ec3\u548c\u9690\u79c1\u4fdd\u62a4\u7b49\u5b9e\u9645\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u4e86\u53cc\u8f68\u57fa\u51c6\u6d4b\u8bd5\u7b56\u7565\u4ee5\u652f\u6301\u516c\u5e73\u6bd4\u8f83\u548c\u786c\u4ef6\u4f18\u5316\uff0c\u5e76\u603b\u7ed3\u4e86\u5f53\u524d\u8fdb\u5c55\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u8111\u542f\u53d1\u5b66\u4e60\u4e0e\u8fb9\u7f18\u90e8\u7f72\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2507.13355", "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13355", "abs": "https://arxiv.org/abs/2507.13355", "authors": ["Riadul Islam", "Dhandeep Challagundla"], "title": "PGR-DRC: Pre-Global Routing DRC Violation Prediction Using Unsupervised Learning", "comment": null, "summary": "Leveraging artificial intelligence (AI)-driven electronic design and\nautomation (EDA) tools, high-performance computing, and parallelized algorithms\nare essential for next-generation microprocessor innovation, ensuring continued\nprogress in computing, AI, and semiconductor technology. Machine learning-based\ndesign rule checking (DRC) and lithography hotspot detection can improve\nfirst-pass silicon success. However, conventional ML and neural network\n(NN)-based models use supervised learning and require a large balanced dataset\n(in terms of positive and negative classes) and training time. This research\naddresses those key challenges by proposing the first-ever unsupervised DRC\nviolation prediction methodology. The proposed model can be built using any\nunbalanced dataset using only one class and set a threshold for it, then\nfitting any new data querying if they are within the boundary of the model for\nclassification. This research verified the proposed model by implementing\ndifferent computational cores using CMOS 28 nm technology and Synopsys Design\nCompiler and IC Compiler II tools. Then, layouts were divided into virtual\ngrids to collect about 60k data for analysis and verification. The proposed\nmethod has 99.95% prediction test accuracy, while the existing support vector\nmachine (SVM) and neural network (NN) models have 85.44\\% and 98.74\\% accuracy,\nrespectively. In addition, the proposed methodology has about 26.3x and up to\n6003x lower training times compared to SVM and NN-models, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684DRC\u8fdd\u89c4\u9884\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u5e73\u8861\u6570\u636e\u548c\u8bad\u7ec3\u65f6\u95f4\u7684\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u5e73\u8861\u6570\u636e\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u9650\u5236\u4e86DRC\u548c\u5149\u523b\u70ed\u70b9\u68c0\u6d4b\u7684\u6548\u7387\u3002", "method": "\u4f7f\u7528\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ec5\u9700\u4e00\u7c7b\u4e0d\u5e73\u8861\u6570\u636e\uff0c\u901a\u8fc7\u8bbe\u5b9a\u9608\u503c\u5bf9\u65b0\u6570\u636e\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u572828\u7eb3\u7c73CMOS\u6280\u672f\u4e2d\u9a8c\u8bc1\uff0c\u9884\u6d4b\u51c6\u786e\u7387\u8fbe99.95%\uff0c\u8bad\u7ec3\u65f6\u95f4\u6bd4SVM\u548cNN\u6a21\u578b\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86DRC\u9884\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u4e0b\u4e00\u4ee3\u5fae\u5904\u7406\u5668\u8bbe\u8ba1\u3002"}}
{"id": "2507.13494", "categories": ["cs.PL", "stat.CO"], "pdf": "https://arxiv.org/pdf/2507.13494", "abs": "https://arxiv.org/abs/2507.13494", "authors": ["Feras A. Saad", "Wonyeol Lee"], "title": "Random Variate Generation with Formal Guarantees", "comment": null, "summary": "This article introduces a new approach to principled and practical random\nvariate generation with formal guarantees. The key idea is to first specify the\ndesired probability distribution in terms of a finite-precision numerical\nprogram that defines its cumulative distribution function (CDF), and then\ngenerate exact random variates according to this CDF. We present a universal\nand fully automated method to synthesize exact random variate generators given\nany numerical CDF implemented in any binary number format, such as\nfloating-point, fixed-point, and posits. The method is guaranteed to operate\nwith the same precision used to specify the CDF, does not overflow, avoids\nexpensive arbitrary-precision arithmetic, and exposes a consistent API. The\nmethod rests on a novel space-time optimal implementation for the class of\ngenerators that attain the information-theoretically optimal Knuth and Yao\nentropy rate, consuming the least possible number of input random bits per\noutput variate. We develop a random variate generation library using our method\nin C and evaluate it on a diverse set of ``continuous'' and ``discrete''\ndistributions, showing competitive runtime with the state-of-the-art GNU\nScientific Library while delivering higher accuracy, entropy efficiency, and\nautomation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6709\u9650\u7cbe\u5ea6\u6570\u503c\u7a0b\u5e8f\u5b9a\u4e49CDF\u7684\u968f\u673a\u53d8\u91cf\u751f\u6210\u65b9\u6cd5\uff0c\u786e\u4fdd\u7cbe\u786e\u751f\u6210\u5e76\u907f\u514d\u9ad8\u7cbe\u5ea6\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u968f\u673a\u53d8\u91cf\u751f\u6210\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u81ea\u52a8\u5316\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u6570\u503cCDF\u7a0b\u5e8f\u5408\u6210\u7cbe\u786e\u968f\u673a\u53d8\u91cf\u751f\u6210\u5668\uff0c\u652f\u6301\u591a\u79cd\u4e8c\u8fdb\u5236\u683c\u5f0f\uff0c\u5b9e\u73b0\u7a7a\u95f4\u65f6\u95f4\u6700\u4f18\u3002", "result": "\u5728C\u8bed\u8a00\u5e93\u4e2d\u5b9e\u73b0\uff0c\u76f8\u6bd4GNU\u79d1\u5b66\u5e93\uff0c\u5177\u6709\u66f4\u9ad8\u7cbe\u5ea6\u3001\u71b5\u6548\u7387\u548c\u81ea\u52a8\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u968f\u673a\u53d8\u91cf\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u3001\u6548\u7387\u548c\u81ea\u52a8\u5316\u7684\u5e73\u8861\u3002"}}
{"id": "2507.14080", "categories": ["cs.DC", "D.2.4; C.2.4"], "pdf": "https://arxiv.org/pdf/2507.14080", "abs": "https://arxiv.org/abs/2507.14080", "authors": ["Derek Leung", "Nickolai Zeldovich", "Frans Kaashoek"], "title": "Shipwright: Proving liveness of distributed systems with Byzantine participants", "comment": "14 pages, 13 figures", "summary": "Ensuring liveness in a decentralized system, such as PBFT, is critical,\nbecause there may not be any single administrator that can restart the system\nif it encounters a liveness bug. At the same time, liveness is challenging to\nachieve because any single participant could be malicious, and yet the overall\nsystem must make forward progress. While verification is a promising approach\nfor ensuring the absence of bugs, no prior work has been able to verify\nliveness for an executable implementation of PBFT.\n  Shipwright is a verification framework for proving correctness and liveness\nof distributed systems where some participants might be malicious. Shipwright\nintroduces three techniques that enable formal reasoning about decentralized\nsettings with malicious participants, allow developers to decompose their\nsystem and proof in a modular fashion into sub-protocols and sub-proofs, and\nsupport sound reasoning about cryptographic signatures that may be embedded in\nmessages. We used Shipwright to implement and verify an initial prototype of\nagreement on a single log entry in PBFT (with a few limitations) and translate\nit to an executable implementation in Go. We experimentally demonstrate its\noperation and liveness both in the common case and in several failure\nscenarios.", "AI": {"tldr": "Shipwright\u662f\u4e00\u4e2a\u9a8c\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u8bc1\u660e\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6b63\u786e\u6027\u548c\u6d3b\u8dc3\u6027\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u6076\u610f\u53c2\u4e0e\u8005\u7684\u60c5\u51b5\u4e0b\u3002\u5b83\u901a\u8fc7\u6a21\u5757\u5316\u65b9\u6cd5\u548c\u5bc6\u7801\u5b66\u7b7e\u540d\u652f\u6301\uff0c\u5b9e\u73b0\u4e86\u5bf9PBFT\u534f\u8bae\u7684\u90e8\u5206\u9a8c\u8bc1\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\uff08\u5982PBFT\uff09\uff0c\u786e\u4fdd\u6d3b\u8dc3\u6027\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u7cfb\u7edf\u53ef\u80fd\u6ca1\u6709\u7ba1\u7406\u5458\u6765\u4fee\u590d\u6d3b\u8dc3\u6027\u6545\u969c\u3002\u7136\u800c\uff0c\u7531\u4e8e\u53ef\u80fd\u5b58\u5728\u6076\u610f\u53c2\u4e0e\u8005\uff0c\u6d3b\u8dc3\u6027\u96be\u4ee5\u4fdd\u8bc1\u3002", "method": "Shipwright\u5f15\u5165\u4e86\u4e09\u79cd\u6280\u672f\uff1a\u652f\u6301\u6076\u610f\u53c2\u4e0e\u8005\u7684\u5f62\u5f0f\u5316\u63a8\u7406\u3001\u6a21\u5757\u5316\u5206\u89e3\u7cfb\u7edf\u548c\u8bc1\u660e\u3001\u4ee5\u53ca\u5bf9\u5d4c\u5165\u6d88\u606f\u7684\u5bc6\u7801\u5b66\u7b7e\u540d\u8fdb\u884c\u5408\u7406\u63a8\u7406\u3002", "result": "Shipwright\u6210\u529f\u9a8c\u8bc1\u4e86PBFT\u534f\u8bae\u4e2d\u5355\u4e2a\u65e5\u5fd7\u6761\u76ee\u7684\u4e00\u81f4\u6027\uff08\u90e8\u5206\u529f\u80fd\uff09\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684Go\u5b9e\u73b0\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u5e38\u89c1\u60c5\u51b5\u548c\u6545\u969c\u573a\u666f\u4e0b\u7684\u6d3b\u8dc3\u6027\u3002", "conclusion": "Shipwright\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6076\u610f\u53c2\u4e0e\u8005\u548c\u6a21\u5757\u5316\u8bc1\u660e\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.13369", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13369", "abs": "https://arxiv.org/abs/2507.13369", "authors": ["Paul E. Calzada", "Zahin Ibnat", "Tanvir Rahman", "Kamal Kandula", "Danyu Lu", "Sujan Kumar Saha", "Farimah Farahmandi", "Mark Tehranipoor"], "title": "VerilogDB: The Largest, Highest-Quality Dataset with a Preprocessing Framework for LLM-based RTL Generation", "comment": null, "summary": "Large Language Models (LLMs) are gaining popularity for hardware design\nautomation, particularly through Register Transfer Level (RTL) code generation.\nIn this work, we examine the current literature on RTL generation using LLMs\nand identify key requirements for training and fine-tuning datasets. We\nconstruct a robust Verilog dataset through an automated three-pronged process\ninvolving database (DB) creation and management with PostgreSQL, data\ncollection from code hosting sites like OpenCores and GitHub, and data\npreprocessing to verify the codes' syntax, run logic synthesis, and extract\nrelevant module metadata. We implement a scalable and efficient DB\ninfrastructure to support analysis and detail our preprocessing pipeline to\nenforce high-quality data before DB insertion. The resulting dataset comprises\n20,392 Verilog samples, 751 MB of Verilog code data, which is the largest\nhigh-quality Verilog dataset for LLM fine-tuning to our knowledge. We further\nevaluate the dataset, address associated challenges, and explore potential\napplications for future research and development in LLM-based hardware\ngeneration.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u7684RTL\u4ee3\u7801\u751f\u6210\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684Verilog\u6570\u636e\u96c6\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5e94\u7528\u524d\u666f\u3002", "motivation": "LLMs\u5728\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684Verilog\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u548c\u5fae\u8c03\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u4e09\u9636\u6bb5\u6d41\u7a0b\uff08\u6570\u636e\u5e93\u521b\u5efa\u4e0e\u7ba1\u7406\u3001\u6570\u636e\u6536\u96c6\u3001\u6570\u636e\u9884\u5904\u7406\uff09\u6784\u5efaVerilog\u6570\u636e\u96c6\uff0c\u5e76\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u6570\u636e\u5e93\u57fa\u7840\u8bbe\u65bd\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b20,392\u4e2aVerilog\u6837\u672c\u3001751 MB\u4ee3\u7801\u6570\u636e\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u662f\u76ee\u524d\u6700\u5927\u7684\u7528\u4e8eLLM\u5fae\u8c03\u7684Verilog\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u57fa\u4e8eLLM\u7684\u786c\u4ef6\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.13533", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.13533", "abs": "https://arxiv.org/abs/2507.13533", "authors": ["Priyam Gupta"], "title": "Increasing the Expressiveness of a Gradual Verifier", "comment": "Presented at the 52nd ACM SIGPLAN Symposium on Principles of\n  Programming Languages (POPL 2025) Student Research Competition", "summary": "Static verification provides strong correctness guarantees for code; however,\nfully specifying programs for static verification is a complex, burdensome\nprocess for users. Gradual verification was introduced to make this process\neasier by supporting the verification of partially specified programs. The only\ncurrently working gradual verifier, Gradual C0, successfully verifies heap\nmanipulating programs, but lacks expressiveness in its specification language.\nThis paper describes the design and implementation of an extension to Gradual\nC0 that supports unfolding expressions, which allow more intuitive\nspecifications of recursive heap data structures.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5bf9Gradual C0\u7684\u6269\u5c55\u8bbe\u8ba1\uff0c\u901a\u8fc7\u652f\u6301\u5c55\u5f00\u8868\u8fbe\u5f0f\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u76f4\u89c2\u5730\u6307\u5b9a\u9012\u5f52\u5806\u6570\u636e\u7ed3\u6784\u3002", "motivation": "\u9759\u6001\u9a8c\u8bc1\u867d\u7136\u80fd\u63d0\u4f9b\u5f3a\u6b63\u786e\u6027\u4fdd\u8bc1\uff0c\u4f46\u5b8c\u5168\u6307\u5b9a\u7a0b\u5e8f\u7684\u8fc7\u7a0b\u590d\u6742\u4e14\u7e41\u7410\u3002\u9010\u6b65\u9a8c\u8bc1\uff08Gradual verification\uff09\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u4f46\u73b0\u6709\u7684Gradual C0\u5728\u89c4\u8303\u8bed\u8a00\u4e0a\u7f3a\u4e4f\u8868\u73b0\u529b\u3002", "method": "\u6269\u5c55Gradual C0\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\uff0c\u652f\u6301\u5c55\u5f00\u8868\u8fbe\u5f0f\u3002", "result": "\u6269\u5c55\u540e\u7684Gradual C0\u80fd\u591f\u66f4\u76f4\u89c2\u5730\u6307\u5b9a\u9012\u5f52\u5806\u6570\u636e\u7ed3\u6784\u3002", "conclusion": "\u901a\u8fc7\u652f\u6301\u5c55\u5f00\u8868\u8fbe\u5f0f\uff0cGradual C0\u7684\u8868\u8fbe\u80fd\u529b\u548c\u5b9e\u7528\u6027\u5f97\u5230\u4e86\u63d0\u5347\u3002"}}
{"id": "2507.13375", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.13375", "abs": "https://arxiv.org/abs/2507.13375", "authors": ["Chunyuan Zhao", "Zizheng Guo", "Zuodong Zhang", "Yibo Lin"], "title": "GAP-LA: GPU-Accelerated Performance-Driven Layer Assignment", "comment": null, "summary": "Layer assignment is critical for global routing of VLSI circuits. It converts\n2D routing paths into 3D routing solutions by determining the proper metal\nlayer for each routing segments to minimize congestion and via count. As\ndifferent layers have different unit resistance and capacitance, layer\nassignment also has significant impacts to timing and power. With growing\ndesign complexity, it becomes increasingly challenging to simultaneously\noptimize timing, power, and congestion efficiently. Existing studies are mostly\nlimited to a subset of objectives. In this paper, we propose a GPU-accelerated\nperformance-driven layer assignment framework, GAP-LA, for holistic\noptimization the aforementioned objectives. Experimental results demonstrate\nthat we can achieve 0.3%-9.9% better worst negative slack (WNS) and 2.0%-5.4%\nbetter total negative slack (TNS) while maintaining power and congestion with\ncompetitive runtime compared with ISPD 2025 contest winners, especially on\ndesigns with up to 12 millions of nets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdGPU\u52a0\u901f\u7684\u6027\u80fd\u9a71\u52a8\u5c42\u5206\u914d\u6846\u67b6GAP-LA\uff0c\u7528\u4e8e\u540c\u65f6\u4f18\u5316\u65f6\u5e8f\u3001\u529f\u8017\u548c\u62e5\u585e\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u8bbe\u8ba1\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u5982\u4f55\u5728\u5c42\u5206\u914d\u4e2d\u540c\u65f6\u4f18\u5316\u65f6\u5e8f\u3001\u529f\u8017\u548c\u62e5\u585e\u6210\u4e3a\u4e00\u4e2a\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5c40\u9650\u4e8e\u90e8\u5206\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u4e86GPU\u52a0\u901f\u7684\u6027\u80fd\u9a71\u52a8\u5c42\u5206\u914d\u6846\u67b6GAP-LA\uff0c\u8fdb\u884c\u5168\u5c40\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGAP-LA\u5728WNS\u548cTNS\u4e0a\u5206\u522b\u63d0\u5347\u4e860.3%-9.9%\u548c2.0%-5.4%\uff0c\u540c\u65f6\u4fdd\u6301\u529f\u8017\u548c\u62e5\u585e\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "GAP-LA\u5728\u590d\u6742\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5c42\u5206\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13774", "categories": ["cs.PL", "cs.LO", "D.3.1; F.3.2; F.4.1"], "pdf": "https://arxiv.org/pdf/2507.13774", "abs": "https://arxiv.org/abs/2507.13774", "authors": ["Arthur Adjedj", "Meven Lennon-Bertrand", "Thibaut Benjamin", "Kenji Maillard"], "title": "AdapTT: Functoriality for Dependent Type Casts", "comment": null, "summary": "The ability to cast values between related types is a leitmotiv of many\nflavors of dependent type theory, such as observational type theories,\nsubtyping, or cast calculi for gradual typing. These casts all exhibit a common\nstructural behavior that boils down to the pervasive functoriality of type\nformers. We propose and extensively study a type theory, called AdapTT, which\nmakes systematic and precise this idea of functorial type formers, with respect\nto an abstract notion of adapters relating types. Leveraging descriptions for\nfunctorial inductive types in AdapTT, we derive structural laws for type casts\non general inductive type formers.", "AI": {"tldr": "AdapTT \u662f\u4e00\u79cd\u7c7b\u578b\u7406\u8bba\uff0c\u901a\u8fc7\u62bd\u8c61\u9002\u914d\u5668\u5173\u7cfb\u7c7b\u578b\uff0c\u7cfb\u7edf\u5316\u5730\u7814\u7a76\u7c7b\u578b\u6784\u9020\u5668\u7684\u51fd\u5b50\u6027\uff0c\u5e76\u63a8\u5bfc\u51fa\u901a\u7528\u5f52\u7eb3\u7c7b\u578b\u6784\u9020\u5668\u7684\u7c7b\u578b\u8f6c\u6362\u7ed3\u6784\u89c4\u5f8b\u3002", "motivation": "\u7814\u7a76\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\u4e2d\u7c7b\u578b\u8f6c\u6362\u7684\u5171\u540c\u7ed3\u6784\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u7c7b\u578b\u6784\u9020\u5668\u7684\u51fd\u5b50\u6027\u3002", "method": "\u63d0\u51fa AdapTT \u7c7b\u578b\u7406\u8bba\uff0c\u5229\u7528\u62bd\u8c61\u9002\u914d\u5668\u5173\u7cfb\u7c7b\u578b\uff0c\u7cfb\u7edf\u5316\u5730\u7814\u7a76\u7c7b\u578b\u6784\u9020\u5668\u7684\u51fd\u5b50\u6027\u3002", "result": "\u63a8\u5bfc\u51fa\u901a\u7528\u5f52\u7eb3\u7c7b\u578b\u6784\u9020\u5668\u7684\u7c7b\u578b\u8f6c\u6362\u7ed3\u6784\u89c4\u5f8b\u3002", "conclusion": "AdapTT \u4e3a\u7c7b\u578b\u8f6c\u6362\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u7406\u8bba\u57fa\u7840\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\u573a\u666f\u3002"}}
{"id": "2507.13631", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.13631", "abs": "https://arxiv.org/abs/2507.13631", "authors": ["Fuyuki Kihara", "Seiji Uenohara", "Satoshi Awamura", "Naoko Misawa", "Chihiro Matsui", "Ken Takeuchi"], "title": "4T2R X-ReRAM CiM Array for Variation-tolerant, Low-power, Massively Parallel MAC Operation", "comment": "4 pages", "summary": "Computation-in-Memory (CiM) is attracting attention as a technology that can\nperform MAC calculations required for AI accelerators, at high speed with low\npower consumption. However, there is a problem regarding power consumption and\ndevice-derived errors that increase as row parallelism increases. In this\npaper, a 4T2R ReRAM cell and an 8T SRAM CiM suitable for CiM is proposed. It is\nshown that adopting the proposed 4T2R ReRAM cell reduces the errors due to\nvariation in ReRAM devices compared to conventional 4T4R ReRAM cells.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u8ba1\u7b97\u5185\u5b58\uff08CiM\uff09\u76844T2R ReRAM\u5355\u5143\u548c8T SRAM CiM\uff0c\u4ee5\u51cf\u5c11\u8bbe\u5907\u8bef\u5dee\u548c\u529f\u8017\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u5185\u5b58\u4e2d\u56e0\u884c\u5e76\u884c\u6027\u589e\u52a0\u5bfc\u81f4\u7684\u529f\u8017\u548c\u8bbe\u5907\u8bef\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51fa4T2R ReRAM\u5355\u5143\u548c8T SRAM CiM\u8bbe\u8ba1\uff0c\u5e76\u4e0e\u4f20\u7edf4T4R ReRAM\u5355\u5143\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "4T2R ReRAM\u5355\u5143\u51cf\u5c11\u4e86\u56e0ReRAM\u8bbe\u5907\u53d8\u5f02\u5f15\u8d77\u7684\u8bef\u5dee\u3002", "conclusion": "\u65b0\u8bbe\u8ba1\u76844T2R ReRAM\u5355\u5143\u5728CiM\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u964d\u4f4e\u4e86\u8bef\u5dee\u548c\u529f\u8017\u3002"}}
{"id": "2507.13792", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.13792", "abs": "https://arxiv.org/abs/2507.13792", "authors": ["Riccardo Bianchini", "Francesco Dagnino", "Paola Giannini", "Elena Zucca"], "title": "Don't exhaust, don't waste", "comment": "Submitted to JFP (Journal of Functional Programming)", "summary": "We extend the semantics and type system of a lambda calculus equipped with\ncommon constructs to be resource-aware. That is, the semantics keep tracks of\nthe usage of resources, and is stuck, besides in case of type errors, if either\na needed resource is exhausted, or a provided resource would be wasted. In such\nway, the type system guarantees, besides standard soundness, that for\nwell-typed programs there is a computation where no resource gets either\nexhausted or wasted.\n  The no-waste extension is parametric on an arbitrary grade algebra, modeling\nan arbitrary assortment of possible usages, and does not require ad-hoc changes\nto the underlying language. To this end, the semantics needs to be formalized\nin big-step style; as a consequence, expressing and proving (resource-aware)\nsoundness is challenging, and is achieved by applying recent techniques based\non coinductive reasoning.", "AI": {"tldr": "\u6269\u5c55\u4e86\u5e26\u5e38\u89c1\u6784\u9020\u7684lambda\u6f14\u7b97\u7684\u8bed\u4e49\u548c\u7c7b\u578b\u7cfb\u7edf\uff0c\u4f7f\u5176\u5177\u6709\u8d44\u6e90\u611f\u77e5\u80fd\u529b\uff0c\u786e\u4fdd\u8d44\u6e90\u4e0d\u88ab\u8017\u5c3d\u6216\u6d6a\u8d39\u3002", "motivation": "\u4e3a\u4e86\u5728\u8ba1\u7b97\u4e2d\u7cbe\u786e\u8ddf\u8e2a\u8d44\u6e90\u4f7f\u7528\u60c5\u51b5\uff0c\u907f\u514d\u8d44\u6e90\u8017\u5c3d\u6216\u6d6a\u8d39\uff0c\u63d0\u4f9b\u66f4\u5f3a\u7684\u7c7b\u578b\u4fdd\u8bc1\u3002", "method": "\u901a\u8fc7\u53c2\u6570\u5316\u7684\u7b49\u7ea7\u4ee3\u6570\u6269\u5c55\u8bed\u4e49\u548c\u7c7b\u578b\u7cfb\u7edf\uff0c\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u8bed\u8a00\uff0c\u91c7\u7528\u5927\u6b65\u8bed\u4e49\u5f62\u5f0f\u5316\u3002", "result": "\u7c7b\u578b\u7cfb\u7edf\u4fdd\u8bc1\u4e86\u8d44\u6e90\u4f7f\u7528\u7684\u5408\u7406\u6027\uff0c\u4e14\u9002\u7528\u4e8e\u4efb\u610f\u8d44\u6e90\u4f7f\u7528\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5171\u5f52\u7eb3\u63a8\u7406\u6280\u672f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u8d44\u6e90\u611f\u77e5\u7684\u8bed\u4e49\u548c\u7c7b\u578b\u7cfb\u7edf\u7684\u5f62\u5f0f\u5316\u4e0e\u9a8c\u8bc1\u3002"}}
