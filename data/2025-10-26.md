<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs](https://arxiv.org/abs/2510.19850)
*Mostapha Kalami Heris*

Main category: cs.PL

TL;DR: Prompt Decorators是一个声明式、可组合的语法框架，通过紧凑的控制标记（如+++Reasoning、+++Tone）来管理LLM的行为维度，实现任务意图与执行行为的解耦。


<details>
  <summary>Details</summary>
Motivation: 传统提示工程依赖冗长的自然语言指令，限制了可重现性、模块化和可解释性，用户缺乏对LLM推理和输出表达的一致控制。

Method: 提出Prompt Decorators框架，定义20个核心装饰器，分为认知与生成、表达与系统两个功能家族，包含统一的语法、作用域模型和确定性处理流水线。

Result: 使用案例显示提高了推理透明度、减少了提示复杂性，并在不同领域实现了标准化的模型行为。

Conclusion: 该框架为可扩展AI系统的互操作性、行为一致性和声明式接口开发提供了重要启示。

Abstract: Large Language Models (LLMs) are central to reasoning, writing, and
decision-support workflows, yet users lack consistent control over how they
reason and express outputs. Conventional prompt engineering relies on verbose
natural-language instructions, limiting reproducibility, modularity, and
interpretability. This paper introduces Prompt Decorators, a declarative,
composable syntax that governs LLM behavior through compact control tokens such
as +++Reasoning, +++Tone(style=formal), and +++Import(topic="Systems
Thinking"). Each decorator modifies a behavioral dimension, such as reasoning
style, structure, or tone, without changing task content. The framework
formalizes twenty core decorators organized into two functional families
(Cognitive & Generative and Expressive & Systemic), each further decomposed
into subcategories that govern reasoning, interaction, expression, and
session-control. It defines a unified syntax, scoping model, and deterministic
processing pipeline enabling predictable and auditable behavior composition. By
decoupling task intent from execution behavior, Prompt Decorators create a
reusable and interpretable interface for prompt design. Illustrative use cases
demonstrate improved reasoning transparency, reduced prompt complexity, and
standardized model behavior across domains. The paper concludes with
implications for interoperability, behavioral consistency, and the development
of declarative interfaces for scalable AI systems.

</details>


### [2] [A Specification's Realm: Characterizing the Knowledge Required for Executing a Given Algorithm Specification](https://arxiv.org/abs/2510.19853)
*Assaf Marron,David Harel*

Main category: cs.PL

TL;DR: 本文提出了算法规范的"领域"概念，即执行算法规范所需的前提知识集合，包括语法语义、领域知识、实体关系、因果规则等，并探讨了如何系统化生成这种领域文档以及评估执行忠实度的问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决算法规范在自然语言或伪代码中缺乏明确执行前提知识的问题，使算法规范能够作为独立实体被机械执行，而不依赖特定系统实现。

Method: 提出算法规范"领域"的概念，将其定义为执行规范所需的前提知识集合，包括语法语义、领域知识、实体关系、因果规则和操作指令等。提出通过系统化分析过程生成领域文档，并探讨利用大语言模型和现有文档重用的自动化方法。

Result: 建立了算法规范领域的初步特征描述，提出了领域文档的生成方法框架，并讨论了执行忠实度评估的问题。

Conclusion: 算法规范领域的特征化有助于算法规范在不同系统中的方法化实现和形式化验证，为机械执行提供了理论基础，同时提出了执行忠实度评估这一区别于正确性的重要问题。

Abstract: An algorithm specification in natural language or pseudocode is expected to
be clear and explicit enough to enable mechanical execution. In this position
paper we contribute an initial characterization of the knowledge that an
executing agent, human or machine, should possess in order to be able to carry
out the instructions of a given algorithm specification as a stand-alone
entity, independent of any system implementation. We argue that, for that
algorithm specification, such prerequisite knowledge, whether unique or shared
with other specifications, can be summarized in a document of practical size.
We term this document the realm of the algorithm specification. The generation
of such a realm is itself a systematic analytical process, significant parts of
which can be automated with the help of large language models and the reuse of
existing documents. The algorithm-specification's realm would consist of
specification language syntax and semantics, domain knowledge restricted to the
referenced entities, inter-entity relationships, relevant underlying
cause-and-effect rules, and detailed instructions and means for carrying out
certain operations. Such characterization of the realm can contribute to
methodological implementation of the algorithm specification in diverse systems
and to its formalization for mechanical verification. The paper also touches
upon the question of assessing execution faithfulness, which is distinct from
correctness: in the absence of a reference interpretation of natural language
or pseudocode specification with a given vocabulary, how can we determine if an
observed agent's execution indeed complies with the input specification.

</details>


### [3] [Deconstructed Proto-Quipper: A Rational Reconstruction](https://arxiv.org/abs/2510.20018)
*Ryan Kavanagh,Chuta Sano,Brigitte Pientka*

Main category: cs.PL

TL;DR: Proto-Quipper-A是对Proto-Quipper量子编程语言家族的重构，使用线性λ演算和伴随逻辑基础来简化量子电路的静态生成和推理。


<details>
  <summary>Details</summary>
Motivation: Proto-Quipper语言具有复杂的操作语义，依赖集合论操作和新鲜名称生成来操纵量子电路，这使得使用标准编程语言技术进行推理和机械化变得困难。

Method: 引入Proto-Quipper-A，使用线性λ演算描述量子电路，其范式与盒线电路图紧密对应。采用伴随逻辑基础将电路语言与线性/非线性函数语言集成，重构Proto-Quipper的电路编程抽象。

Result: Proto-Quipper-A具有简单的按值调用归约语义，并且是规范化的。使用标准逻辑关系证明了线性和子结构系统的规范化，避免了现有线性逻辑关系的复杂性。

Conclusion: Proto-Quipper-A为Proto-Quipper语言提供了一个更易处理的基础，简化了量子电路的静态生成和形式化推理。

Abstract: The Proto-Quipper family of programming languages aims to provide a formal
foundation for the Quipper quantum programming language. Unfortunately,
Proto-Quipper languages have complex operational semantics: they are inherently
effectful, and they rely on set-theoretic operations and fresh name generation
to manipulate quantum circuits. This makes them difficult to reason about using
standard programming language techniques and, ultimately, to mechanize. We
introduce Proto-Quipper-A, a rational reconstruction of Proto-Quipper languages
for static circuit generation. It uses a linear $\lambda$-calculus to describe
quantum circuits with normal forms that closely correspond to box-and-wire
circuit diagrams. Adjoint-logical foundations integrate this circuit language
with a linear/non-linear functional language and let us reconstruct
Proto-Quipper's circuit programming abstractions using more primitive
adjoint-logical operations. Proto-Quipper-A enjoys a simple call-by-value
reduction semantics, and to illustrate its tractability as a foundation for
Proto-Quipper languages, we show that it is normalizing. We show how to use
standard logical relations to prove normalization of linear and substructural
systems, thereby avoiding the inherent complexity of existing linear logical
relations.

</details>


### [4] [Deciding not to Decide: Sound and Complete Effect Inference in the Presence of Higher-Rank Polymorphism](https://arxiv.org/abs/2510.20532)
*Patrycja Balik,Szymon Jędras,Piotr Polesiuk*

Main category: cs.PL

TL;DR: 提出了一种类型与效应系统的效应推断算法，支持子类型、高阶多态性和直观的集合语义效应，通过将效应约束转换为命题逻辑公式来处理高阶多态性的作用域问题。


<details>
  <summary>Details</summary>
Motivation: 传统类型系统已发展出具有复杂推断算法的表达性变体并广泛应用，但类型与效应系统尚未广泛采用，因为现有算法在表达性、直观性和可判定性之间做出妥协。

Method: 开发效应推断算法，将效应约束转换为命题逻辑公式以延迟求解，处理高阶多态性的作用域问题。

Result: 算法相对于声明式类型与效应系统被证明是健全和完备的，已在Rocq证明助手中形式化，并在实际编程语言中成功实现。

Conclusion: 该工作为类型与效应系统提供了一种平衡表达性、直观性和可判定性的实用推断算法。

Abstract: Type-and-effect systems help the programmer to organize data and
computational effects in a program. While for traditional type systems
expressive variants with sophisticated inference algorithms have been developed
and widely used in programming languages, type-and-effect systems did not yet
gain widespread adoption. One reason for this is that type-and-effect systems
are more complex and the existing inference algorithms make compromises between
expressiveness, intuitiveness, and decidability. In this work, we present an
effect inference algorithm for a type-and-effect system with subtyping,
expressive higher-rank polymorphism, and intuitive set-like semantics of
effects. In order to deal with scoping issues of higher-rank polymorphism, we
delay solving of effect constraints by transforming them into formulae of
propositional logic. We prove soundness and completeness of our algorithm with
respect to a declarative type-and-effect system. All the presented results have
been formalized in the Rocq proof assistant, and the algorithm has been
successfully implemented in a realistic programming language.

</details>


### [5] [Compiling the Mimosa programming language to RTOS tasks](https://arxiv.org/abs/2510.20547)
*Nikolaus Huber,Susanne Graf,Philipp Rümmer,Wang Yi*

Main category: cs.PL

TL;DR: 提出了Mimosa编程语言的编译方案，基于MIMOS计算模型，将嵌入式系统软件描述为时间触发进程集合，通过FIFO队列通信，并适配Lustre编译方案到Mimosa语义。


<details>
  <summary>Details</summary>
Motivation: 为Mimosa编程语言开发编译方案，使其能够描述嵌入式系统软件的时间触发进程模型，并实现与实时操作系统原语的映射。

Method: 基于MIMOS计算模型，将嵌入式系统软件描述为时间触发进程集合，通过FIFO队列通信，并正式描述Lustre编译方案到Mimosa语义的适配。

Result: 成功开发了Mimosa语言的编译方案，能够将协调层映射到实时操作系统原语。

Conclusion: 提出的编译方案有效支持了Mimosa语言在嵌入式系统开发中的应用，实现了时间触发进程模型与实时操作系统的集成。

Abstract: This paper introduces a compilation scheme for programs written in the Mimosa
programming language, which builds upon the MIMOS model of computation. Mimosa
describes embedded systems software as a collection of time-triggered processes
which communicate through FIFO queues. We formally describe an adaptation of
the Lustre compilation scheme to the semantics of Mimosa and show how the
coordination layer can be mapped to real-time operating system primitives.

</details>


### [6] [SafeFFI: Efficient Sanitization at the Boundary Between Safe and Unsafe Code in Rust and Mixed-Language Applications](https://arxiv.org/abs/2510.20688)
*Oliver Braunsdorf,Tim Lange,Konrad Hohentanner,Julian Horsch,Johannes Kinder*

Main category: cs.PL

TL;DR: SafeFFI 是一个优化 Rust 二进制文件中内存安全检测的系统，通过在 unsafe 和 safe 代码边界处进行检查，将内存安全执行从 sanitizer 转移到 Rust 类型系统。


<details>
  <summary>Details</summary>
Motivation: Unsafe Rust 代码在与 C/C++ 库互操作和实现底层数据结构时是必要的，但可能导致内存安全违规。现有的 sanitizer 会引入大量不必要的检查。

Method: 在 unsafe 和 safe 代码边界处进行内存安全检查，将内存安全执行从 sanitizer 转移到 Rust 类型系统，避免昂贵的全程序分析。

Result: 减少了 98% 的 sanitizer 检查，编译时开销仅为 2.64 倍（相比之前方法的 8.83 倍），在流行的 Rust crate 和已知易受攻击代码上表现优异。

Conclusion: SafeFFI 在保持正确性和标记所有空间和时间内存安全违规的同时，实现了卓越的性能，显著减少了 sanitizer 检查的开销。

Abstract: Unsafe Rust code is necessary for interoperability with C/C++ libraries and
implementing low-level data structures, but it can cause memory safety
violations in otherwise memory-safe Rust programs. Sanitizers can catch such
memory errors at runtime, but introduce many unnecessary checks even for memory
accesses guaranteed safe by the Rust type system. We introduce SafeFFI, a
system for optimizing memory safety instrumentation in Rust binaries such that
checks occur at the boundary between unsafe and safe code, handing over the
enforcement of memory safety from the sanitizer to the Rust type system. Unlike
previous approaches, our design avoids expensive whole-program analysis and
adds much less compile-time overhead (2.64x compared to over 8.83x). On a
collection of popular Rust crates and known vulnerable Rust code, SafeFFI
achieves superior performance compared to state-of-the-art systems, reducing
sanitizer checks by up to 98%, while maintaining correctness and flagging all
spatial and temporal memory safety violations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [New Hardness Results for the LOCAL Model via a Simple Self-Reduction](https://arxiv.org/abs/2510.19972)
*Alkida Balliu,Filippo Casagrande,Francesco d'Amore,Dennis Olivetti*

Main category: cs.DC

TL;DR: 本文简化了Khoury和Schild的轮消除自约简技术，并应用该技术证明了最大b匹配问题和边着色问题的随机LOCAL算法下界。


<details>
  <summary>Details</summary>
Motivation: Khoury和Schild关于最大匹配问题的下界证明虽然优美但长达25页且技术复杂，难以消化和推广。历史上证明和技术的简化对理解图问题复杂性有重要意义。

Method: 提出轮消除自约简技术的简化版本，并应用该技术分析最大b匹配和边着色问题。

Result: 1. 最大b匹配问题需要Ω(min{log₁₊bΔ, logΔn})和Ω(√log₁₊bn)轮；2. 边着色问题需要Ω(min{logΔ, logΔn})和Ω(√logn)轮。

Conclusion: 成功简化了轮消除自约简技术，并获得了最大b匹配和边着色问题的新下界结果，为理解图问题复杂性提供了更简洁的证明方法。

Abstract: Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL
algorithm that solves maximal matching requires $\Omega(\min\{\log \Delta,
\log_\Delta n\})$ rounds, where $n$ is the number of nodes in the graph and
$\Delta$ is the maximum degree. This result is shown through a new technique,
called round elimination via self-reduction. The lower bound proof is beautiful
and presents very nice ideas. However, it spans more than 25 pages of technical
details, and hence it is hard to digest and generalize to other problems.
Historically, the simplification of proofs and techniques has marked an
important turning point in our understanding of the complexity of graph
problems. Our paper makes a step forward towards this direction, and provides
the following contributions.
  1. We present a short and simplified version of the round elimination via
self-reduction technique. The simplification of this technique enables us to
obtain the following two hardness results.
  2. We show that any randomized LOCAL algorithm that solves the maximal
$b$-matching problem requires $\Omega(\min\{\log_{1+b}\Delta, \log_\Delta n\})$
and $\Omega(\sqrt{\log_{1+b} n})$ rounds. We recall that the $b$-matching
problem is a generalization of the matching problem where each vertex can have
up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain
a short proof for the maximal matching lower bound shown by Khoury and Schild.
  3. Finally, we show that any randomized LOCAL algorithm that properly colors
the edges of a graph with $\Delta + k$ colors requires $\Omega(\min\{\log
\Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log n})$ rounds, for any $k\le
\Delta^{1-\varepsilon}$ and any constant $\varepsilon > 0$.

</details>


### [8] [AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training](https://arxiv.org/abs/2510.20111)
*Huawei Bai,Yifan Huang,Wenqi Shi,Ansheng You,Feifan Shao,Tengfei Han,Minghui Yu*

Main category: cs.DC

TL;DR: 提出AsyncHZP异步分层零并行方法，在保持内存效率的同时显著提升大规模语言模型训练性能


<details>
  <summary>Details</summary>
Motivation: 当前主流并行方法（如ND并行）复杂繁琐，而灵活方案如ZeRO又受通信开销限制，需要更高效的训练方法

Method: 自适应分片参数、梯度和优化器状态到不同副本组，采用多流异步调度在后台线程执行通信操作，重叠通信与计算

Result: 在密集和MoE模型上验证，AsyncHZP保持稳定扩展性，性能优于经典ND并行，无需复杂调优即达最优性能

Conclusion: AsyncHZP简化了高效大规模训练的路径，在保持内存效率的同时实现了最先进的性能

Abstract: The training efficiency and scalability of language models on massive
clusters currently remain a critical bottleneck. Mainstream approaches like ND
parallelism are often cumbersome and complex, while flexible alternatives such
as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by
communication overhead. In this paper, we propose Asynchronous Hierarchical
Zero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to
achieve superior performance while maintaining simplicity and memory
efficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding
that can lead to inefficient communication, AsyncHZP adaptively reshards
parameters, gradients, and optimizer states across different replica groups.
This strategy optimizes device memory utilization and significantly reduces
communication overhead. In addition, we also design a multi-stream asynchronous
scheduling method that executes parameter all-gather and gradient
reduce-scatter operations in dedicated background threads, effectively
overlapping communication with computation while incurring negligible memory
fragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)
models confirm that AsyncHZP maintains robust stability at scale. It
consistently outperforms classic ND parallelism, achieving state-of-the-art
performance without complex strategic tuning, thereby simplifying the path to
efficient large-scale training.

</details>


### [9] [A Full Stack Framework for High Performance Quantum-Classical Computing](https://arxiv.org/abs/2510.20128)
*Xin Zhan,K. Grace Johnson,Aniello Esposito,Barbara Chapman,Marco Fiorentino,Kirk M. Bresniker,Raymond G. Beausoleil,Masoud Mohseni*

Main category: cs.DC

TL;DR: 提出了一个HPC-QC全栈框架，通过模块化硬件/设备无关的软件集成方法，实现高性能计算与量子计算的混合工作负载开发。


<details>
  <summary>Details</summary>
Motivation: 解决可扩展的高性能计算与量子计算集成的需求，构建统一的量子-经典编程环境。

Method: 采用模块化硬件/设备无关的软件集成方法，开发可扩展的量子编程、调度和编译接口，利用Cray LLVM编译框架转换LLVM IR和Quantum IR，并开发自适应电路编织虚拟机来分割大型量子电路。

Result: 在HPE EX超级计算机上成功演示了多个混合HPC-QC多节点多CPU和GPU工作负载，包括求解线性方程组、量子优化和模拟量子相变。

Conclusion: 这项工作为基于经典HPC软件栈的统一量子-经典编程环境提供了框架。

Abstract: To address the growing needs for scalable High Performance Computing (HPC)
and Quantum Computing (QC) integration, we present our HPC-QC full stack
framework and its hybrid workload development capability with modular
hardware/device-agnostic software integration approach. The latest development
in extensible interfaces for quantum programming, dispatching, and compilation
within existing mature HPC programming environment are demonstrated. Our HPC-QC
full stack enables high-level, portable invocation of quantum kernels from
commercial quantum SDKs within HPC meta-program in compiled languages (C/C++
and Fortran) as well as Python through a quantum programming interface library
extension. An adaptive circuit knitting hypervisor is being developed to
partition large quantum circuits into sub-circuits that fit on smaller noisy
quantum devices and classical simulators. At the lower-level, we leverage Cray
LLVM-based compilation framework to transform and consume LLVM IR and Quantum
IR (QIR) from commercial quantum software frontends in a retargetable fashion
to different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU
and GPU workloads (including solving linear system of equations, quantum
optimization, and simulating quantum phase transitions) have been demonstrated
on HPE EX supercomputers to illustrate functionality and execution viability
for all three components developed so far. This work provides the framework for
a unified quantum-classical programming environment built upon classical HPC
software stack (compilers, libraries, parallel runtime and process scheduling).

</details>


### [10] [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)
*Min Si,Pavan Balaji,Yongzhou Chen,Ching-Hsiang Chu,Adi Gangidi,Saif Hasan,Subodh Iyengar,Dan Johnson,Bingzhe Liu,Jingliang Ren,Ashmitha Jeevaraj Shetty,Greg Steinbrecher,Xinfeng Xie,Yulun Wang,Bruce Wu,Jingyi Yang,Mingran Yang,Minlan Yu,Cen Zhao,Wes Bland,Denis Boyda,Suman Gumudavelli,Cristian Lumezanu,Rui Miao,Zhe Qu,Venkat Ramesh,Maxim Samoylov,Jan Seidel,Feng Tian,Qiye Tan,Shuqiang Zhang,Yimeng Zhao,Shengbao Zheng,Art Zhu,Hongyi Zeng*

Main category: cs.DC

TL;DR: 提出了NCCLX集体通信框架，针对超大规模LLM训练（超过10万GPU）优化通信性能，显著提升训练和推理效率


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断扩大，传统通信方法在数十万GPU规模下面临吞吐量和延迟限制，阻碍了最先进模型的开发和部署

Method: 开发了NCCLX集体通信框架，专门针对LLM全生命周期优化，支持超大规模集群的复杂工作负载，确保可靠、高吞吐、低延迟的数据交换

Result: 在Llama4模型上的实证评估显示通信效率显著提升，为下一代LLM在空前规模上运行提供了强大解决方案

Conclusion: NCCLX框架为解决超大规模LLM训练和推理的通信瓶颈提供了有效的技术方案，推动了更大规模模型的发展

Abstract: The increasing scale of large language models (LLMs) necessitates highly
efficient collective communication frameworks, particularly as training
workloads extend to hundreds of thousands of GPUs. Traditional communication
methods face significant throughput and latency limitations at this scale,
hindering both the development and deployment of state-of-the-art models. This
paper presents the NCCLX collective communication framework, developed at Meta,
engineered to optimize performance across the full LLM lifecycle, from the
synchronous demands of large-scale training to the low-latency requirements of
inference. The framework is designed to support complex workloads on clusters
exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency
data exchange. Empirical evaluation on the Llama4 model demonstrates
substantial improvements in communication efficiency. This research contributes
a robust solution for enabling the next generation of LLMs to operate at
unprecedented scales.

</details>


### [11] [FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services](https://arxiv.org/abs/2510.20388)
*Víctor Rampérez,Javier Soriano,David Lizcano,Juan A. Lara*

Main category: cs.DC

TL;DR: FLAS是一个结合主动和被动方法的自动扩缩容系统，通过预测高维指标趋势和基于资源使用指标的被动应急系统，为分布式服务提供最优扩缩容决策。


<details>
  <summary>Details</summary>
Motivation: 云计算弹性需求日益增长，现有自动扩缩容系统需要在不同情况下灵活结合主动预测和被动响应方法，以更少侵入性方式确保服务级别协议。

Method: 结合预测模型（预测高维SLA指标趋势）和被动应急系统（从资源使用指标估计高维指标），减少必要监控，可适配不同应用。

Result: 在多种测试场景下验证，包括预期使用场景和最坏情况，确保99%以上的时间满足性能要求。

Conclusion: FLAS是首个面向内容发布订阅分布式系统的自动扩缩容解决方案，通用性强，能有效保障服务性能。

Abstract: Cloud computing has established itself as the support for the vast majority
of emerging technologies, mainly due to the characteristic of elasticity it
offers. Auto-scalers are the systems that enable this elasticity by acquiring
and releasing resources on demand to ensure an agreed service level. In this
article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for
distributed services that combines the advantages of proactive and reactive
approaches according to the situation to decide the optimal scaling actions in
every moment. The main novelties introduced by FLAS are (i) a predictive model
of the high-level metrics trend which allows to anticipate changes in the
relevant SLA parameters (e.g. performance metrics such as response time or
throughput) and (ii) a reactive contingency system based on the estimation of
high-level metrics from resource use metrics, reducing the necessary
instrumentation (less invasive) and allowing it to be adapted agnostically to
different applications. We provide a FLAS implementation for the use case of a
content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone
of an event-driven architecture. To the best of our knowledge, this is the
first auto-scaling system for content-based publish-subscribe distributed
systems (although it is generic enough to fit any distributed service). Through
an evaluation based on several test cases recreating not only the expected
contexts of use, but also the worst possible scenarios (following the
Boundary-Value Analysis or BVA test methodology), we have validated our
approach and demonstrated the effectiveness of our solution by ensuring
compliance with performance requirements over 99% of the time.

</details>


### [12] [Accurate Performance Predictors for Edge Computing Applications](https://arxiv.org/abs/2510.20495)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 提出了一种在动态边缘环境中自动构建和评估性能预测器的方法，该方法同时优化预测准确性和推理时间，在电子显微镜工作流场景中达到90%准确率且推理时间小于往返时间的1%。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的动态边缘环境中，由于多应用共置和节点异构性，实现可预测的应用性能具有挑战性，这对有效的调度和资源管理至关重要。

Method: 自动构建和评估各种性能预测器，优先考虑准确性和推理时间，使用与应用程序性能最相关的监控指标历史状态进行训练，在动态共置场景中跨多个服务器进行评估。

Result: 预测器达到高达90%的准确率，同时保持推理时间小于往返时间的1%，在动态共置场景中经过多个服务器评估验证。

Conclusion: 需要系统化方法在动态共置场景中通过联合优化准确性和推理延迟来选择特定服务器的预测器，将此类预测器集成到边缘环境中可以提高资源利用率并实现可预测的性能。

Abstract: Accurate prediction of application performance is critical for enabling
effective scheduling and resource management in resource-constrained dynamic
edge environments. However, achieving predictable performance in such
environments remains challenging due to the co-location of multiple
applications and the node heterogeneity. To address this, we propose a
methodology that automatically builds and assesses various performance
predictors. This approach prioritizes both accuracy and inference time to
identify the most efficient model. Our predictors achieve up to 90% accuracy
while maintaining an inference time of less than 1% of the Round Trip Time.
These predictors are trained on the historical state of the most correlated
monitoring metrics to application performance and evaluated across multiple
servers in dynamic co-location scenarios. As usecase we consider electron
microscopy (EM) workflows, which have stringent real-time demands and diverse
resource requirements. Our findings emphasize the need for a systematic
methodology that selects server-specific predictors by jointly optimizing
accuracy and inference latency in dynamic co-location scenarios. Integrating
such predictors into edge environments can improve resource utilization and
result in predictable performance.

</details>


### [13] [Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing](https://arxiv.org/abs/2510.20506)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 该论文开发了轻量级RTT预测器，通过预测应用延迟来改进负载均衡，在Kubernetes管理的GPU集群中实现高达95%的预测准确率，显著降低应用RTT和资源浪费。


<details>
  <summary>Details</summary>
Motivation: 分布式应用对低端到端延迟的需求日益增长，传统负载均衡策略通常是反应式的，依赖过时或粗粒度的指标，导致次优路由决策和增加的尾部延迟。

Method: 开发基于时间序列监控数据的轻量级RTT预测器，利用高度相关的监控指标集，在Kubernetes管理的GPU集群上进行训练，保持低开销同时适应多样化的共置场景和异构硬件。

Result: 预测器达到高达95%的准确率，预测延迟保持在应用RTT的10%以内。确定了确保在资源受限集群中有效部署预测器所需的最小预测准确率阈值和关键系统级因素。

Conclusion: 基于仿真的评估表明，性能感知负载均衡能显著降低应用RTT并最小化资源浪费，突显了将预测性负载均衡集成到未来生产系统中的可行性。

Abstract: Distributed applications increasingly demand low end-to-end latency,
especially in edge and cloud environments where co-located workloads contend
for limited resources. Traditional load-balancing strategies are typically
reactive and rely on outdated or coarse-grained metrics, often leading to
suboptimal routing decisions and increased tail latencies. This paper
investigates the use of round-trip time (RTT) predictors to enhance request
routing by anticipating application latency. We develop lightweight and
accurate RTT predictors that are trained on time-series monitoring data
collected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of
highly correlated monitoring metrics, our approach maintains low overhead while
remaining adaptable to diverse co-location scenarios and heterogeneous
hardware. The predictors achieve up to 95% accuracy while keeping the
prediction delay within 10% of the application RTT. In addition, we identify
the minimum prediction accuracy threshold and key system-level factors required
to ensure effective predictor deployment in resource-constrained clusters.
Simulation-based evaluation demonstrates that performance-aware load balancing
can significantly reduce application RTT and minimize resource waste. These
results highlight the feasibility of integrating predictive load balancing into
future production systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [HALOC-AxA: An Area/-Energy-Efficient Approximate Adder for Image Processing Application](https://arxiv.org/abs/2510.20137)
*Hasnain A. Ziad,Ashiq A. Sakib*

Main category: cs.AR

TL;DR: 提出了一种新型近似加法器，比现有加法器更节能、面积更小，同时保持或提高计算精度，适用于图像处理等多媒体应用。


<details>
  <summary>Details</summary>
Motivation: 为计算密集型多媒体应用（如图像、音频、视频处理）设计更节能的硬件，平衡高性能、计算精度和能效之间的冲突需求。

Method: 开发了一种新型近似加法器设计，通过仿真验证其性能，并将其部署到图像处理任务中展示数字重建高质量图像的能力。

Result: 仿真结果表明，该加法器比现有加法器更节能、面积更小，同时实现改进或相当的精度。在图像处理任务中成功重建高质量图像。

Conclusion: 提出的近似加法器在能效、面积效率和计算精度方面优于现有设计，适用于多媒体处理应用。

Abstract: The design of approximate adders has been widely researched to advance
energy-efficient hardware for computation-intensive multimedia applications,
such as image, audio, or video processing. The design of approximate adders has
been widely researched to advance energy-efficient hardware for computation
intensive multimedia applications, such as image/audio/video processing.
Several static and dynamic approximate adders exist in the literature, each of
which endeavors to balance the conflicting demands of high performance,
computational accuracy, and energy efficiency. This work introduces a novel
approximate adder that is more energy- and area-efficient than existing adders,
while achieving improved or comparable accuracy, as demonstrated by simulation
results. The proposed adder's ability to digitally reconstruct high quality
images is further demonstrated by the deployment of the design for an image
processing task.

</details>


### [15] [In-DRAM True Random Number Generation Using Simultaneous Multiple-Row Activation: An Experimental Study of Real DRAM Chips](https://arxiv.org/abs/2510.20269)
*Ismail Emir Yuksel,Ataberk Olgun,F. Nisa Bostanci,Oguzhan Canpolat,Geraldo F. Oliveira,Mohammad Sadrosadati,Abdullah Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: 通过同时激活多行DRAM（SiMRA）在商用DDR4芯片中实现高速低延迟真随机数生成，相比现有技术吞吐量提升最高达1.99倍，且熵值随激活行数增加而提高。


<details>
  <summary>Details</summary>
Motivation: 利用商用DRAM芯片的物理特性开发高效真随机数生成器（TRNG），克服传统TRNG在吞吐量和延迟方面的限制。

Method: 在96个DDR4 DRAM芯片上实验表征SiMRA方法，研究不同同时激活行数（2、4、8、16、32）、数据模式、温度水平和空间变化对随机数生成的影响。

Result: 所有SiMRA基TRNG设计均通过NIST随机性测试；2、8、16、32行激活的吞吐量分别比现有最佳DRAM基TRNG高1.15x、1.99x、1.82x、1.39x；32行激活的平均熵比2行激活高2.51倍；温度从50°C升至90°C会使32行激活的熵降低1.53倍。

Conclusion: SiMRA是一种在商用DRAM中实现高效真随机数生成的有效方法，其性能受操作参数和条件显著影响，为未来TRNG研究提供了重要基础设施。

Abstract: In this work, we experimentally demonstrate that it is possible to generate
true random numbers at high throughput and low latency in commercial
off-the-shelf (COTS) DRAM chips by leveraging simultaneous multiple-row
activation (SiMRA) via an extensive characterization of 96 DDR4 DRAM chips. We
rigorously analyze SiMRA's true random generation potential in terms of
entropy, latency, and throughput for varying numbers of simultaneously
activated DRAM rows (i.e., 2, 4, 8, 16, and 32), data patterns, temperature
levels, and spatial variations. Among our 11 key experimental observations, we
highlight four key results. First, we evaluate the quality of our TRNG designs
using the commonly-used NIST statistical test suite for randomness and find
that all SiMRA-based TRNG designs successfully pass each test. Second, 2-, 8-,
16-, and 32-row activation-based TRNG designs outperform the state-of-theart
DRAM-based TRNG in throughput by up to 1.15x, 1.99x, 1.82x, and 1.39x,
respectively. Third, SiMRA's entropy tends to increase with the number of
simultaneously activated DRAM rows. Fourth, operational parameters and
conditions (e.g., data pattern and temperature) significantly affect entropy.
For example, for most of the tested modules, the average entropy of 32-row
activation is 2.51x higher than that of 2-row activation. For example,
increasing the temperature from 50{\deg}C to 90{\deg}C decreases SiMRA's
entropy by 1.53x for 32-row activation. To aid future research and development,
we open-source our infrastructure at https://github.com/CMU-SAFARI/SiMRA-TRNG.

</details>


### [16] [Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism on Dependency-Bound Kernels](https://arxiv.org/abs/2510.20400)
*Rubén Langarita,Jesús Alastruey-Benedé,Pablo Ibáñez-Marín,Santiago Marco-Sola,Miquel Moretó,Adrià Armejach*

Main category: cs.AR

TL;DR: Squire是一种通用加速器，专门用于解决具有复杂依赖模式的计算密集型内核的性能瓶颈，相比传统加速器能更有效地利用细粒度并行性。


<details>
  <summary>Details</summary>
Motivation: 传统通用加速器（如SIMD和GPGPU）在处理复杂数据依赖模式时存在局限性，而定制硬件（FPGA/ASIC）虽然性能好但设计复杂且缺乏灵活性。需要一种既能高效处理依赖绑定内核又保持通用性的加速方案。

Method: 每个Squire加速器包含一组低功耗顺序核心，这些核心能够快速相互通信并直接访问L2缓存。在典型多核系统中，每个核心集成一个Squire加速器，通过最小软件修改来加速并行任务中的依赖绑定内核。

Result: 在动态编程内核中获得了最高7.64倍的加速比，端到端应用整体加速达到3.66倍。能耗降低最高56%，面积开销仅为10.5%（相比Neoverse-N1基线）。

Conclusion: Squire提供了一种有效平衡性能、能效和灵活性的解决方案，能够显著加速依赖绑定内核，同时保持较低的硬件开销和软件修改成本。

Abstract: Multiple HPC applications are often bottlenecked by compute-intensive kernels
implementing complex dependency patterns (data-dependency bound). Traditional
general-purpose accelerators struggle to effectively exploit fine-grain
parallelism due to limitations in implementing convoluted data-dependency
patterns (like SIMD) and overheads due to synchronization and data transfers
(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved
performance and energy efficiency at a high cost in hardware design and
programming complexity and often lack the flexibility to process different
workloads. We propose Squire, a general-purpose accelerator designed to exploit
fine-grain parallelism effectively on dependency-bound kernels. Each Squire
accelerator has a set of general-purpose low-power in-order cores that can
rapidly communicate among themselves and directly access data from the L2
cache. Our proposal integrates one Squire accelerator per core in a typical
multicore system, allowing the acceleration of dependency-bound kernels within
parallel tasks with minimal software changes. As a case study, we evaluate
Squire's effectiveness by accelerating five kernels that implement complex
dependency patterns. We use three of these kernels to build an end-to-end
read-mapping tool that will be used to evaluate Squire. Squire obtains speedups
up to 7.64$\times$ in dynamic programming kernels. Overall, Squire provides an
acceleration for an end-to-end application of 3.66$\times$. In addition, Squire
reduces energy consumption by up to 56% with a minimal area overhead of 10.5%
compared to a Neoverse-N1 baseline.

</details>
