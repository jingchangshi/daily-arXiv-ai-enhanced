{"id": "2509.04936", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.04936", "abs": "https://arxiv.org/abs/2509.04936", "authors": ["Andrea Gilot", "Tobias Wrigstad", "Eva Darulova"], "title": "A Large-Scale Study of Floating-Point Usage in Statically Typed Languages", "comment": null, "summary": "Reasoning about floating-point arithmetic is notoriously hard. While static\nand dynamic analysis techniques or program repair have made significant\nprogress, more work is still needed to make them relevant to real-world code.\nOn the critical path to that goal is understanding what real-world\nfloating-point code looks like. To close that knowledge gap, this paper\npresents the first large-scale empirical study of floating-point arithmetic\nusage in statically typed languages across public GitHub repositories. We\nfollow state-of the art mining practices including random sampling and\nfiltering based on only intrinsic properties to avoid bias, and identify\nfloating-point usage by searching for keywords in the source code, and\nprogramming language constructs (e.g., loops) by parsing the code. Our\nevaluation supports the claim often made in papers that floating-point\narithmetic is widely used. Comparing statistics such as size and usage of\ncertain constructs and functions, we find that benchmarks used in literature to\nevaluate automated reasoning techniques for floating-point arithmetic are in\ncertain aspects representative of 'real-world' code, but not in all. We aim for\nour study and dataset to help future techniques for floating-point arithmetic\nto be designed and evaluated to match actual users' expectations."}
{"id": "2509.05160", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.05160", "abs": "https://arxiv.org/abs/2509.05160", "authors": ["Steven Smyth", "Daniel Busch", "Moez Ben Haj Hmida", "Edward A. Lee", "Bernhard Steffen"], "title": "AI-Assisted Modeling: DSL-Driven AI Interactions", "comment": "7 pages, 4 figures", "summary": "AI-assisted programming greatly increases software development performance.\nWe enhance this potential by integrating transparency through domain-specific\nmodeling techniques and providing instantaneous, graphical visualizations that\naccurately represent the semantics of AI-generated code. This approach\nfacilitates visual inspection and formal verification, such as model checking.\n  Formal models can be developed using programming, natural language prompts,\nvoice commands, and stage-wise refinement, with immediate feedback after each\ntransformation step. This support can be tailored to specific domains or\nintended purposes, improving both code generation and subsequent validation\nprocesses.\n  To demonstrate the effectiveness of this approach, we have developed a\nprototype as a Visual Studio Code extension for the Lingua Franca language.\nThis prototype showcases the potential for novel domain-specific modeling\npractices, offering an advancement in how models are created, visualized, and\nverified."}
{"id": "2509.05293", "categories": ["cs.PL", "cs.CL", "cs.SE", "D.3; F.3"], "pdf": "https://arxiv.org/pdf/2509.05293", "abs": "https://arxiv.org/abs/2509.05293", "authors": ["Julien Vanegue", "Jules Villard", "Peter O'Hearn", "Azalea Raad"], "title": "Non-Termination Proving: 100 Million LoC and Beyond", "comment": "14 pages, 4 figures", "summary": "We report on our tool, Pulse Infinite, that uses proof techniques to show\nnon-termination (divergence) in large programs. Pulse Infinite works\ncompositionally and under-approximately: the former supports scale, and the\nlatter ensures soundness for proving divergence. Prior work focused on small\nbenchmarks in the tens or hundreds of lines of code (LoC), and scale limits\ntheir practicality: a single company may have tens of millions, or even\nhundreds of millions of LoC or more. We report on applying Pulse Infinite to\nover a hundred million lines of open-source and proprietary software written in\nC, C++, and Hack, identifying over 30 previously unknown issues, establishing a\nnew state of the art for detecting divergence in real-world codebases."}
{"id": "2509.04719", "categories": ["cs.DC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04719", "abs": "https://arxiv.org/abs/2509.04719", "authors": ["Han Liang", "Jiahui Zhou", "Zicheng Zhou", "Xiaoxi Zhang", "Xu Chen"], "title": "STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs", "comment": null, "summary": "The escalating adoption of diffusion models for applications such as image\ngeneration demands efficient parallel inference techniques to manage their\nsubstantial computational cost. However, existing diffusion parallelism\ninference schemes often underutilize resources in heterogeneous multi-GPU\nenvironments, where varying hardware capabilities or background tasks cause\nworkload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion\nInference (STADI), a novel framework to accelerate diffusion model inference in\nsuch settings. At its core is a hybrid scheduler that orchestrates fine-grained\nparallelism across both temporal and spatial dimensions. Temporally, STADI\nintroduces a novel computation-aware step allocator applied after warmup\nphases, using a least-common-multiple-minimizing quantization technique to\nreduce denoising steps on slower GPUs and execution synchronization. To further\nminimize GPU idle periods, STADI executes an elastic patch parallelism\nmechanism that allocates variably sized image patches to GPUs according to\ntheir computational capability, ensuring balanced workload distribution through\na complementary spatial mechanism. Extensive experiments on both\nload-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy,\ndemonstrating improved load balancing and mitigation of performance\nbottlenecks. Compared to patch parallelism, a state-of-the-art diffusion\ninference framework, our method significantly reduces end-to-end inference\nlatency by up to 45% and significantly improves resource utilization on\nheterogeneous GPUs."}
{"id": "2509.04827", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.04827", "abs": "https://arxiv.org/abs/2509.04827", "authors": ["Jiahuan Yu", "Aryan Taneja", "Junfeng Lin", "Minjia Zhang"], "title": "VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving", "comment": null, "summary": "Modern Large Language Model (LLM) serving systems increasingly support\ninteractive applications, like real-time chat assistants, code generation\ntools, and agentic workflows. However, the soaring energy cost of LLM inference\npresents a growing challenge for sustainable and cost-effective deployment.\nThis paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM\nserving, built from a control theory perspective. VoltanaLLM co-designs\nfrequency scaling and request routing in emerging prefill/decode disaggregated\narchitectures, leveraging their decoupled execution to enable fine-grained\nphase-specific control. It consists of a feedback-driven frequency controller\nthat dynamically adapts GPU frequency for prefill and decode phases, and a\nstate-space router that explores routing decisions across frequency-scaled\ninstances to minimize energy under latency constraints. We implement VoltanaLLM\nin SGLang and evaluate its performance over multiple state-of-the-art LLMs and\nreal-world datasets. The results demonstrate that VoltanaLLM achieves up to\n36.3% energy savings while maintaining near-perfect SLO attainment rate, paving\nthe way for sustainable and intelligent LLM serving."}
{"id": "2509.05216", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.05216", "abs": "https://arxiv.org/abs/2509.05216", "authors": ["Mengjiao Han", "Andres Sewell", "Joseph Insley", "Janet Knowles", "Victor A. Mateevitsi", "Michael E. Papka", "Steve Petruzza", "Silvio Rizzi"], "title": "Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization", "comment": null, "summary": "We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS)\npipeline for scientific visualization. Building on previous work that\ndemonstrated high-fidelity isosurface reconstruction using Gaussian primitives,\nwe incorporate a multi-GPU training backend adapted from Grendel-GS to enable\nscalable processing of large datasets. By distributing optimization across\nGPUs, our method improves training throughput and supports high-resolution\nreconstructions that exceed single-GPU capacity. In our experiments, the system\nachieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs\ncompared to a single-GPU baseline, and successfully trains the Miranda dataset\n(18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays\nthe groundwork for integrating 3D-GS into HPC-based scientific workflows,\nenabling real-time post hoc and in situ visualization of complex simulations."}
{"id": "2509.05248", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.05248", "abs": "https://arxiv.org/abs/2509.05248", "authors": ["Iker Martín-Álvarez", "José I. Aliaga", "Maribel Castillo"], "title": "Dynamic reconfiguration for malleable applications using RMA", "comment": "Sumbitted in Workshop DynRes25. 12 pages, 6 Figures, 3 Algorithm, 1\n  Listing", "summary": "This paper investigates the novel one-sided communication methods based on\nremote memory access (RMA) operations in MPI for dynamic resizing of malleable\napplications, enabling data redistribution with minimal impact on application\nexecution. After their integration into the MaM library, these methods are\ncompared with traditional collective-based approaches. In addition, the\nexisting strategy Wait Drains is extended to support efficient background\nreconfiguration. Results show comparable performance, though high\ninitialization costs currently limit their advantage."}
{"id": "2509.05258", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05258", "abs": "https://arxiv.org/abs/2509.05258", "authors": ["Alexander Interrante-Grant", "Carla Varela-Rosa", "Suhaas Narayan", "Chris Connelly", "Albert Reuther"], "title": "Scaling Performance of Large Language Model Pretraining", "comment": null, "summary": "Large language models (LLMs) show best-in-class performance across a wide\nrange of natural language processing applications. Training these models is an\nextremely computationally expensive task; frontier Artificial Intelligence (AI)\nresearch companies are investing billions of dollars into supercomputing\ninfrastructure to train progressively larger models on increasingly massive\ndatasets. Unfortunately, information about the scaling performance and training\nconsiderations of these large training pipelines is scarce in public\nliterature. Working with large-scale datasets and models can be complex and\npractical recommendations are scarce in the public literature for tuning\ntraining performance when scaling up large language models. In this paper, we\naim to demystify the large language model pretraining pipeline somewhat - in\nparticular with respect to distributed training, managing large datasets across\nhundreds of nodes, and scaling up data parallelism with an emphasis on fully\nleveraging available GPU compute capacity."}
