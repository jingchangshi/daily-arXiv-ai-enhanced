<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Sharing State Between Prompts and Programs](https://arxiv.org/abs/2512.14805)
*Ellie Y. Cheng,Logan Weber,Tian Jin,Michael Carbin*

Main category: cs.PL

TL;DR: Nightjar编程系统引入共享程序状态抽象，让自然语言代码可以直接操作Python程序变量，减少手动集成工作，提高任务准确率并减少代码量，但会带来运行时开销。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的兴起，自然语言编程成为新范式，但自然语言代码与形式语言（如Python）之间的互操作性需要大量手动工作。需要一种方法让自然语言代码能直接操作程序状态，减少集成负担。

Method: 提出共享程序状态编程抽象，通过自然函数接口模式扩展编程系统以支持自然代码。在Nightjar系统中实现，允许Python程序包含能共享Python程序状态的自然代码。

Result: Nightjar程序相比手动实现的任务准确率相当或更高（+4-19%），平均减少39.6%的代码行数。代价是运行时开销为手动实现的0.4-4.3倍。

Conclusion: 共享程序状态抽象有效解决了自然语言代码与形式语言之间的互操作性问题，显著减少开发工作量，虽然带来运行时开销，但在准确率和代码简洁性方面有明显优势。

Abstract: The rise of large language models (LLMs) has introduced a new type of programming: natural language programming. By writing prompts that direct LLMs to perform natural language processing, code generation, reasoning, etc., users are writing code in natural language -- natural language code -- for the LLM to execute.
  An emerging area of research enables interoperability between natural language code and formal languages such as Python. We present a novel programming abstraction, shared program state, that removes the manual work required to enable interoperability between natural language code and program state. With shared program state, programmers can write natural code that directly writes program variables, computes with program objects, and implements control flow in the program. We present a schema for specifying natural function interfaces that extend programming systems to support natural code and leverage this schema to specify shared program state as a natural function interface.
  We implement shared program state in the Nightjar programming system. Nightjar enables programmers to write Python programs that contain natural code that shares the Python program state. We show that Nightjar programs achieve comparable or higher task accuracy than manually written implementations (+4-19%), while decreasing the lines of code by 39.6% on average. The tradeoff to using Nightjar is that it may incur runtime overhead (0.4-4.3x runtime of manual implementations).

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Reexamining Paradigms of End-to-End Data Movement](https://arxiv.org/abs/2512.15028)
*Chin Fang,Timothy Stitt,Michael J. McManus,Toshio Moriya*

Main category: cs.DC

TL;DR: 论文指出追求高性能数据传输不应仅关注网络带宽，而需采用硬件-软件协同设计的整体视角，解决从边缘到核心全链路中的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前高性能数据传输研究过度关注网络带宽，特别是100Gbps及以上国际链路，但仅关注网络带宽是不完整的，需要更全面的视角来理解实际可持续的数据传输能力。

Method: 研究六种常见范式：网络延迟、TCP拥塞控制算法等网络因素，以及CPU性能、虚拟化等主机端因素。使用支持延迟仿真的测试床进行高速广域网性能预测，并通过从资源受限边缘环境到100Gbps瑞士-加州链路的广泛生产测量进行验证。

Result: 主要瓶颈通常不在网络核心，而在网络之外的其他环节。硬件-软件协同设计能确保从1Gbps到100Gbps及以上数据传输的一致性能，有效缩小基准测试结果与复杂生产环境之间的差距。

Conclusion: 高性能数据传输需要超越网络中心的视角，采用硬件-软件协同设计的整体方法，才能在实际生产环境中实现可持续的高性能数据传输。

Abstract: The pursuit of high-performance data transfer often focuses on raw network bandwidth, and international links of 100 Gbps or higher are frequently considered the primary enabler. While necessary, this network-centric view is incomplete, equating provisioned link speeds with practical, sustainable data movement capabilities across the entire edge-to-core spectrum. This paper investigates six common paradigms, from the often-cited constraints of network latency and TCP congestion control algorithms to host-side factors such as CPU performance and virtualization that critically impact data movement workflows. We validated our findings using a latency-emulation-capable testbed for high-speed WAN performance prediction and through extensive production measurements from resource-constrained edge environments to a 100 Gbps operational link connecting Switzerland and California, U.S. These results show that the principal bottlenecks often reside outside the network core, and that a holistic hardware-software co-design ensures consistent performance, whether moving data at 1 Gbps or 100 Gbps and faster. This approach effectively closes the fidelity gap between benchmark results and diverse and complex production environments.

</details>


### [3] [LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs](https://arxiv.org/abs/2512.15306)
*Erik Schultheis,Dan Alistarh*

Main category: cs.DC

TL;DR: LLMQ是一个针对中等规模语言模型（3B-32B参数）在消费级GPU上训练的端到端CUDA/C++实现，通过优化内存和通信瓶颈，能在单张16GB游戏卡上训练7B模型，或在4张RTX 4090上训练32B模型。


<details>
  <summary>Details</summary>
Motivation: 消费级GPU内存有限且通信速度慢，难以训练中等规模语言模型。现有系统主要针对数据中心级GPU设计，需要为普通用户提供在消费级硬件上高效训练语言模型的解决方案。

Method: 采用激活检查点、卸载技术和基于复制引擎的集合通信等优化技术，针对内存和通信瓶颈进行优化。实现标准的8位训练流程，无需额外的算法近似。

Result: 在单张16GB中端游戏卡上训练7B模型，或在4张RTX 4090工作站上训练32B模型，同时保持约50%的FLOP利用率。效率可与在更昂贵的云级GPU上的生产级系统相媲美。

Conclusion: LLMQ证明了在消费级GPU上高效训练中等规模语言模型的可行性，为普通用户提供了经济实惠的解决方案，其性能可与昂贵的云级系统竞争。

Abstract: We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.

</details>


### [4] [Optimizing Bloom Filters for Modern GPU Architectures](https://arxiv.org/abs/2512.15595)
*Daniel Jünger,Kevin Kristensen,Yunsong Wang,Xiangyao Yu,Bertil Schmidt*

Main category: cs.DC

TL;DR: 本文探索了GPU上Bloom filter的设计空间，通过向量化、线程协作和计算延迟三个维度的优化，在保持高精度的同时实现了高吞吐量，性能超越现有方法11.35-15.4倍。


<details>
  <summary>Details</summary>
Motivation: Bloom filter是近似成员查询的基础数据结构，GPU因其大规模线程级并行性和高带宽内存，是加速Bloom filter的理想平台。然而，尽管CPU优化实现已得到充分研究，GPU设计仍未被充分探索，本文旨在填补这一空白。

Method: 从三个维度探索GPU上的设计空间：1) 向量化，2) 线程协作，3) 计算延迟。通过模块化的CUDA/C++实现，分析不同参数配置下硬件的响应，并测量性能趋势。

Result: 优化设计突破了速度与精度之间的传统权衡，在保持高精度配置优越准确性的同时，实现了通常仅限于高错误率变体的吞吐量。在相同错误率下，批量过滤器查找性能提升11.35倍，构建性能提升15.4倍，在B200 GPU上达到实际速度极限的92%以上。

Conclusion: 本文填补了GPU上Bloom filter设计的空白，展示了通过向量化、线程协作和计算延迟的优化组合可以显著提升性能，特别是在过滤器适合GPU缓存域时效果最佳。提出的方法在保持高精度的同时实现了高吞吐量，超越了现有技术水平。

Abstract: Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.
  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\times$ ($15.4\times$) for bulk filter lookup (construction), respectively, achieving above $92\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.

</details>


### [5] [LeaseGuard: Raft Leases Done Right](https://arxiv.org/abs/2512.15659)
*A. Jesse Jiryu Davis,Murat Demirbas,Lingzhi Deng*

Main category: cs.DC

TL;DR: LeaseGuard是一个基于Raft的新型租约算法，通过选举保证实现零网络往返的一致读取，显著提升读写性能


<details>
  <summary>Details</summary>
Motivation: Raft系统需要保证读取一致性，但现有方法要么通信开销大（安全检查），要么租约协议定义模糊且损害可用性，导致多数系统实现不正确或不实现租约

Method: 提出LeaseGuard算法，利用Raft选举的特定保证，在TLA+中严格规范，包含两个新颖优化：快速恢复写入吞吐量和提高读取可用性

Result: 在LogCabin中实现LeaseGuard，将一致读取开销从1个网络往返降为0，写入吞吐量从~1000提升到~10,000次/秒，新领导者能立即允许99%的读取成功

Conclusion: LeaseGuard提供了简单、严格规范的租约算法，显著改善Raft系统的读取一致性和整体性能，解决了传统租约的可用性问题

Abstract: Raft is a leading consensus algorithm for replicating writes in distributed databases. However, distributed databases also require consistent reads. To guarantee read consistency, a Raft-based system must either accept the high communication overhead of a safety check for each read, or implement leader leases. Prior lease protocols are vaguely specified and hurt availability, so most Raft systems implement them incorrectly or not at all. We introduce LeaseGuard, a novel lease algorithm that relies on guarantees specific to Raft elections. LeaseGuard is simple, rigorously specified in TLA+, and includes two novel optimizations that maximize availability during leader failover. The first optimization restores write throughput quickly, and the second improves read availability. We evaluate LeaseGuard with a simulation in Python and an implementation in LogCabin, the C++ reference implementation of Raft. By replacing LogCabin's default consistency mechanism (quorum checks), LeaseGuard reduces the overhead of consistent reads from one to zero network roundtrips. It also improves write throughput from ~1000 to ~10,000 writes per second, by eliminating contention between writes and quorum reads. Whereas traditional leases ban all reads on a new leader while it waits for a lease, in our LeaseGuard test the new leader instantly allows 99% of reads to succeed.

</details>


### [6] [Dynamic Rebatching for Efficient Early-Exit Inference with DREX](https://arxiv.org/abs/2512.15705)
*Xuting Liu,Daniel Alexander,Siva Kesava Reddy Kakarla,Behnaz Arzani,Vincent Liu*

Main category: cs.DC

TL;DR: DREX系统通过动态重组批处理来优化早期退出LLM推理，避免强制统一退出决策，在保持输出质量的同时提升吞吐量2-12%。


<details>
  <summary>Details</summary>
Motivation: 传统批处理框架不适合早期退出LLM架构，因为批处理中不同请求的退出时间不同。现有方案要么强制统一退出决策（错过早期退出机会），要么强制提前退出（降低输出质量）。

Method: 提出动态重组批处理：在早期退出点动态重组批处理，满足退出条件的请求立即处理，继续的请求放入缓冲区重新分组后转发到更深层。DREX系统实现此方案，包含无拷贝重组缓冲区和基于分析的调度器优化。

Result: DREX相比基线方法提升吞吐量2-12%，同时完全消除非自愿退出，保持早期退出模型预期的输出质量。

Conclusion: 动态重组批处理是解决早期退出LLM批处理问题的有效方案，DREX系统在保持输出质量的同时显著提升推理效率。

Abstract: Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [Implementation and Analysis of Thermometer Encoding in DWN FPGA Accelerators](https://arxiv.org/abs/2512.15251)
*Michael Mecik,Martin Kumm*

Main category: cs.AR

TL;DR: DWN硬件生成器包含温度计编码，实验显示编码成本在小型网络中占主导地位，可达LUT使用量的3.2倍


<details>
  <summary>Details</summary>
Motivation: FPGA上的全并行神经网络加速器面临硬件资源限制，DWN通过梯度训练优化资源使用，但依赖温度计编码且其硬件成本未充分评估

Method: 提出了包含温度计编码的DWN硬件生成器，在Jet Substructure Classification任务上进行实验评估

Result: 编码可使LUT使用量增加达3.20倍，在小型网络中成本占主导地位

Conclusion: DWN加速器需要编码感知的硬件设计，编码成本是重要考虑因素

Abstract: Fully parallel neural network accelerators on field-programmable gate arrays (FPGAs) offer high throughput for latency-critical applications but face hardware resource constraints. Weightless neural networks (WNNs) efficiently replace arithmetic with logic-based inference. Differential weightless neural networks (DWN) further optimize resource usage by learning connections between encoders and LUT layers via gradient-based training. However, DWNs rely on thermometer encoding, and the associated hardware cost has not been fully evaluated. We present a DWN hardware generator that includes thermometer encoding explicitly. Experiments on the Jet Substructure Classification (JSC) task show that encoding can increase LUT usage by up to 3.20$\times$, dominating costs in small networks and highlighting the need for encoding-aware hardware design in DWN accelerators.

</details>


### [8] [FAME: FPGA Acceleration of Secure Matrix Multiplication with Homomorphic Encryption](https://arxiv.org/abs/2512.15515)
*Zhihan Xu,Rajgopal Kannan,Viktor K. Prasanna*

Main category: cs.AR

TL;DR: 本文提出FAME，一种针对同态加密矩阵乘法优化的FPGA加速器，通过创新的数据路径设计减少内存带宽需求，实现比CPU方案平均221倍的加速。


<details>
  <summary>Details</summary>
Motivation: 同态加密（HE）虽然能实现加密数据的安全计算，但其高昂的计算成本，特别是矩阵乘法（MM）操作，阻碍了实际部署。加速同态加密矩阵乘法对于隐私保护机器学习等应用至关重要。

Method: 1. 开发成本模型评估给定HE参数和输入矩阵大小的片上内存需求；2. 设计新颖的同态线性变换（HLT）数据路径，实现细粒度数据重用，显著减少片外内存流量和片上内存需求；3. 基于此数据路径构建FAME加速器，支持任意矩阵形状和广泛的HE参数集。

Result: 在Alveo U280 FPGA上实现FAME，评估不同矩阵大小和形状的性能。实验结果显示，FAME相比最先进的CPU实现平均加速221倍，证明其在大规模连续HE MM和实际工作负载中的可扩展性和实用性。

Conclusion: FAME通过优化内存使用和数据重用，为同态加密矩阵乘法提供了高效、可扩展的FPGA加速解决方案，显著提升了HE在实际应用中的可行性。

Abstract: Homomorphic Encryption (HE) enables secure computation on encrypted data, addressing privacy concerns in cloud computing. However, the high computational cost of HE operations, particularly matrix multiplication (MM), remains a major barrier to its practical deployment. Accelerating homomorphic encrypted MM (HE MM) is therefore crucial for applications such as privacy-preserving machine learning.
  In this paper, we present a bandwidth-efficient FPGA implementation of HE MM. We first develop a cost model to evaluate the on-chip memory requirements for a given set of HE parameters and input matrix sizes. Our analysis shows that optimizing on-chip memory usage is critical for scalable and efficient HE MM. To this end, we design a novel datapath for Homomorphic Linear Transformation (HLT), the primary bottleneck in HE MM. The proposed datapath significantly reduces off-chip memory traffic and on-chip memory demand by enabling fine-grained data reuse. Leveraging this datapath, we introduce FAME, the first FPGA-based accelerator specifically tailored for HE MM. FAME supports arbitrary matrix shapes and is configurable across a wide range of HE parameter sets. We implement FAME on an Alveo U280 FPGA and evaluate its performance across diverse matrix sizes and shapes. Experimental results show that FAME achieves an average speedup of 221x over state-of-the-art CPU-based implementations, demonstrating its scalability and practicality for large-scale consecutive HE MM and real-world workloads.

</details>
