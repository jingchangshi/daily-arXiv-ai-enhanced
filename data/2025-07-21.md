<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Random Variate Generation with Formal Guarantees](https://arxiv.org/abs/2507.13494)
*Feras A. Saad,Wonyeol Lee*

Main category: cs.PL

TL;DR: 提出一种基于有限精度数值程序定义CDF的随机变量生成方法，确保精确生成并避免高精度计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决传统随机变量生成方法在精度、效率和自动化方面的不足。

Method: 通过数值CDF程序合成精确随机变量生成器，支持多种二进制格式，实现空间时间最优。

Result: 在C语言库中实现，相比GNU科学库，具有更高精度、熵效率和自动化。

Conclusion: 该方法在随机变量生成中实现了精度、效率和自动化的平衡。

Abstract: This article introduces a new approach to principled and practical random
variate generation with formal guarantees. The key idea is to first specify the
desired probability distribution in terms of a finite-precision numerical
program that defines its cumulative distribution function (CDF), and then
generate exact random variates according to this CDF. We present a universal
and fully automated method to synthesize exact random variate generators given
any numerical CDF implemented in any binary number format, such as
floating-point, fixed-point, and posits. The method is guaranteed to operate
with the same precision used to specify the CDF, does not overflow, avoids
expensive arbitrary-precision arithmetic, and exposes a consistent API. The
method rests on a novel space-time optimal implementation for the class of
generators that attain the information-theoretically optimal Knuth and Yao
entropy rate, consuming the least possible number of input random bits per
output variate. We develop a random variate generation library using our method
in C and evaluate it on a diverse set of ``continuous'' and ``discrete''
distributions, showing competitive runtime with the state-of-the-art GNU
Scientific Library while delivering higher accuracy, entropy efficiency, and
automation.

</details>


### [2] [Increasing the Expressiveness of a Gradual Verifier](https://arxiv.org/abs/2507.13533)
*Priyam Gupta*

Main category: cs.PL

TL;DR: 本文介绍了对Gradual C0的扩展设计，通过支持展开表达式，使其能够更直观地指定递归堆数据结构。


<details>
  <summary>Details</summary>
Motivation: 静态验证虽然能提供强正确性保证，但完全指定程序的过程复杂且繁琐。逐步验证（Gradual verification）旨在简化这一过程，但现有的Gradual C0在规范语言上缺乏表现力。

Method: 扩展Gradual C0的设计与实现，支持展开表达式。

Result: 扩展后的Gradual C0能够更直观地指定递归堆数据结构。

Conclusion: 通过支持展开表达式，Gradual C0的表达能力和实用性得到了提升。

Abstract: Static verification provides strong correctness guarantees for code; however,
fully specifying programs for static verification is a complex, burdensome
process for users. Gradual verification was introduced to make this process
easier by supporting the verification of partially specified programs. The only
currently working gradual verifier, Gradual C0, successfully verifies heap
manipulating programs, but lacks expressiveness in its specification language.
This paper describes the design and implementation of an extension to Gradual
C0 that supports unfolding expressions, which allow more intuitive
specifications of recursive heap data structures.

</details>


### [3] [AdapTT: Functoriality for Dependent Type Casts](https://arxiv.org/abs/2507.13774)
*Arthur Adjedj,Meven Lennon-Bertrand,Thibaut Benjamin,Kenji Maillard*

Main category: cs.PL

TL;DR: AdapTT 是一种类型理论，通过抽象适配器关系类型，系统化地研究类型构造器的函子性，并推导出通用归纳类型构造器的类型转换结构规律。


<details>
  <summary>Details</summary>
Motivation: 研究依赖类型理论中类型转换的共同结构行为，尤其是类型构造器的函子性。

Method: 提出 AdapTT 类型理论，利用抽象适配器关系类型，系统化地研究类型构造器的函子性。

Result: 推导出通用归纳类型构造器的类型转换结构规律。

Conclusion: AdapTT 为类型转换提供了一种系统化的理论基础，适用于多种依赖类型理论场景。

Abstract: The ability to cast values between related types is a leitmotiv of many
flavors of dependent type theory, such as observational type theories,
subtyping, or cast calculi for gradual typing. These casts all exhibit a common
structural behavior that boils down to the pervasive functoriality of type
formers. We propose and extensively study a type theory, called AdapTT, which
makes systematic and precise this idea of functorial type formers, with respect
to an abstract notion of adapters relating types. Leveraging descriptions for
functorial inductive types in AdapTT, we derive structural laws for type casts
on general inductive type formers.

</details>


### [4] [Don't exhaust, don't waste](https://arxiv.org/abs/2507.13792)
*Riccardo Bianchini,Francesco Dagnino,Paola Giannini,Elena Zucca*

Main category: cs.PL

TL;DR: 扩展了带常见构造的lambda演算的语义和类型系统，使其具有资源感知能力，确保资源不被耗尽或浪费。


<details>
  <summary>Details</summary>
Motivation: 为了在计算中精确跟踪资源使用情况，避免资源耗尽或浪费，提供更强的类型保证。

Method: 通过参数化的等级代数扩展语义和类型系统，无需修改底层语言，采用大步语义形式化。

Result: 类型系统保证了资源使用的合理性，且适用于任意资源使用模型。

Conclusion: 通过共归纳推理技术，成功实现了资源感知的语义和类型系统的形式化与验证。

Abstract: We extend the semantics and type system of a lambda calculus equipped with
common constructs to be resource-aware. That is, the semantics keep tracks of
the usage of resources, and is stuck, besides in case of type errors, if either
a needed resource is exhausted, or a provided resource would be wasted. In such
way, the type system guarantees, besides standard soundness, that for
well-typed programs there is a computation where no resource gets either
exhausted or wasted.
  The no-waste extension is parametric on an arbitrary grade algebra, modeling
an arbitrary assortment of possible usages, and does not require ad-hoc changes
to the underlying language. To this end, the semantics needs to be formalized
in big-step style; as a consequence, expressing and proving (resource-aware)
soundness is challenging, and is achieved by applying recent techniques based
on coinductive reasoning.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Checkmate: Zero-Overhead Model Checkpointing via Network Gradient Replication](https://arxiv.org/abs/2507.13522)
*Ankit Bhardwaj,Weiyang Wang,Jeremy Carin,Adam Belay,Manya Ghobadi*

Main category: cs.DC

TL;DR: Checkmate系统实现了DNN训练中的每迭代检查点功能，避免了传统检查点方法的训练暂停问题，通过利用梯度信息实现高效检查点。


<details>
  <summary>Details</summary>
Motivation: 传统检查点方法需要在训练暂停时复制模型状态，存在检查点频率与故障成本之间的权衡。Checkmate旨在消除这种权衡。

Method: 利用数据并行训练中的梯度信息，通过新的多播抽象将梯度传递到基于CPU的影子集群，影子集群通过应用梯度维护检查点。

Result: Checkmate实现了每迭代检查点，训练吞吐量与无检查点基线相当，检查点频率提高5至34.5倍，重复工作减少80%至97.1%。

Conclusion: Checkmate在保持高检查点频率的同时，显著提升了训练吞吐量，优于现有系统。

Abstract: This paper presents Checkmate, a system that enables per-iteration
checkpointing in DNN training without any training slowdown. The traditional
approach to checkpointing requires a pause in training to copy model states to
a separate location, allowing the state to be restored in the event of failure.
This approach fundamentally has a tradeoff between the frequency of checkpoints
and the cost of a failure. We avoid this tradeoff; our key insight is that in
data-parallel training, all information necessary to create a checkpoint
already exists in the network as gradients. Our core contribution is a new
multicast abstraction that simultaneously delivers gradients to a separate
CPU-based shadow cluster. The shadow maintains a checkpoint by applying those
gradients to a copy of the model. Our evaluation shows that Checkmate performs
per-iteration checkpointing with training throughput comparable to an ideal
no-checkpoint baseline. Checkmate achieves 5 to 34.5x more frequent
checkpointing compared to state-of-the-art checkpointing systems, resulting in
80% to 97.1% reduction in repeated work per failure. At the same checkpointing
frequency, Checkmate delivers 1.3x to 6.5x throughput compared to other
systems.

</details>


### [6] [Leveraging Multi-Instance GPUs through moldable task scheduling](https://arxiv.org/abs/2507.13601)
*Jorge Villarrubia,Luis Costero,Francisco D. Igual,Katzalin Olcoz*

Main category: cs.DC

TL;DR: 本文提出了一种名为FAR的三阶段算法，用于在NVIDIA MIG技术下实现多任务调度的最小化完成时间（makespan），并通过实验验证了其优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: NVIDIA MIG技术允许动态分区GPU资源，但其在多任务调度中的潜力尚未充分挖掘。本文旨在探索如何通过动态资源重配置优化任务调度。

Method: 提出FAR算法，包含三个阶段：1）基于经典任务可塑性方法；2）结合最长处理时间优先和列表调度，并引入针对MIG约束的重新分区树启发式；3）通过任务移动和交换进行局部搜索。

Result: 实验显示，在不考虑重配置成本时，FAR在NVIDIA A30上的近似因子为7/4，在A100/H100上为2。考虑重配置成本后，实际性能接近最优（1.22x-1.10x）。

Conclusion: FAR算法显著优于现有方法，展示了MIG技术的研究潜力，并为未来工作提供了有用的指标和评估方法。

Abstract: NVIDIA MIG (Multi-Instance GPU) allows partitioning a physical GPU into
multiple logical instances with fully-isolated resources, which can be
dynamically reconfigured. This work highlights the untapped potential of MIG
through moldable task scheduling with dynamic reconfigurations. Specifically,
we propose a makespan minimization problem for multi-task execution under MIG
constraints. Our profiling shows that assuming monotonicity in task work with
respect to resources is not viable, as is usual in multicore scheduling.
Relying on a state-of-the-art proposal that does not require such an
assumption, we present FAR, a 3-phase algorithm to solve the problem. Phase 1
of FAR builds on a classical task moldability method, phase 2 combines Longest
Processing Time First and List Scheduling with a novel repartitioning tree
heuristic tailored to MIG constraints, and phase 3 employs local search via
task moves and swaps. FAR schedules tasks in batches offline, concatenating
their schedules on the fly in an improved way that favors resource reuse.
Excluding reconfiguration costs, the List Scheduling proof shows an
approximation factor of 7/4 on the NVIDIA A30 model. We adapt the technique to
the particular constraints of an NVIDIA A100/H100 to obtain an approximation
factor of 2. Including the reconfiguration cost, our real-world experiments
reveal a makespan with respect to the optimum no worse than 1.22x for a
well-known suite of benchmarks, and 1.10x for synthetic inputs inspired by real
kernels. We obtain good experimental results for each batch of tasks, but also
in the concatenation of batches, with large improvements over the
state-of-the-art and proposals without GPU reconfiguration. Beyond the
algorithm, the paper demonstrates the research potential of the MIG technology
and suggests useful metrics, workload characterizations and evaluation
techniques for future work in this field.

</details>


### [7] [DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training](https://arxiv.org/abs/2507.13833)
*Zhixin Wang,Tianyi Zhou,Liming Liu,Ao Li,Jiarui Hu,Dian Yang,Jinlong Hou,Siyuan Feng,Yuan Cheng,Yuan Qi*

Main category: cs.DC

TL;DR: DistFlow是一种新型分布式强化学习框架，通过多控制器架构消除中心节点，实现近线性扩展和高效计算。


<details>
  <summary>Details</summary>
Motivation: 大规模强化学习中，负载不平衡会导致瓶颈，限制系统扩展性。

Method: 采用多控制器范式，将数据传输和执行任务分配给所有工作节点，消除中心节点，实现独立运行。

Result: 实验显示DistFlow具有优异的线性扩展性，吞吐量比现有框架提升7倍。

Conclusion: DistFlow突破了强化学习的扩展限制，为高效算法实验提供了灵活性。

Abstract: Reinforcement learning (RL) has become the pivotal post-training technique
for large language model. Effectively scaling reinforcement learning is now the
key to unlocking advanced reasoning capabilities and ensuring safe,
goal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually
employ a hybrid-controller architecture where a single-controller dispatches
the overall execution logic and manages overall data transfer and the
multi-controller executes distributed computation. For large-scale
reinforcement learning, minor load imbalances can introduce significant
bottlenecks, ultimately constraining the scalability of the system. To address
this limitation, we introduce DistFlow, a novel, fully distributed RL framework
designed to break scaling barrier. We adopt a multi-controller paradigm that
dispatches data transfer and execution tasks to all workers, which eliminates
the centralized node. This allows each worker to operate independently, leading
to near-linear scalability up to thousands of GPUs and dramatic efficiency
gains. Furthermore, our architecture decouples resource configuration from
execution logic, allowing each worker to have a unique execution flow, offering
significant flexibility for rapid and cost-effective algorithmic
experimentation. Extensive experiments show that DistFlow achieves excellent
linear scalability and up to a 7x end-to-end throughput improvement over
state-of-the-art (SOTA) frameworks.

</details>


### [8] [Edge Intelligence with Spiking Neural Networks](https://arxiv.org/abs/2507.14069)
*Shuiguang Deng,Di Yu,Changze Lv,Xin Du,Linshan Jiang,Xiaofan Zhao,Wentao Tong,Xiaoqing Zheng,Weijia Fang,Peng Zhao,Gang Pan,Schahram Dustdar,Albert Y. Zomaya*

Main category: cs.DC

TL;DR: 该论文综述了基于脉冲神经网络（SNNs）的边缘智能（EdgeSNNs），探讨其在资源受限设备上的应用潜力，包括低功耗计算、设备端学习和安全性问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在边缘计算中存在延迟、带宽和隐私问题，而SNNs通过模拟生物神经元动态提供了一种低功耗的替代方案。

Method: 论文系统分类了EdgeSNNs的基础（如神经元模型、学习算法和硬件平台），并深入讨论了设备端推理、资源感知训练和隐私保护等实际问题。

Result: 提出了双轨基准测试策略以支持公平比较和硬件优化，并总结了当前进展和未来研究方向。

Conclusion: 该研究填补了脑启发学习与边缘部署之间的空白，为研究人员提供了重要参考。

Abstract: The convergence of artificial intelligence and edge computing has spurred
growing interest in enabling intelligent services directly on
resource-constrained devices. While traditional deep learning models require
significant computational resources and centralized data management, the
resulting latency, bandwidth consumption, and privacy concerns have exposed
critical limitations in cloud-centric paradigms. Brain-inspired computing,
particularly Spiking Neural Networks (SNNs), offers a promising alternative by
emulating biological neuronal dynamics to achieve low-power, event-driven
computation. This survey provides a comprehensive overview of Edge Intelligence
based on SNNs (EdgeSNNs), examining their potential to address the challenges
of on-device learning, inference, and security in edge scenarios. We present a
systematic taxonomy of EdgeSNN foundations, encompassing neuron models,
learning algorithms, and supporting hardware platforms. Three representative
practical considerations of EdgeSNN are discussed in depth: on-device inference
using lightweight SNN models, resource-aware training and updating under
non-stationary data conditions, and secure and privacy-preserving issues.
Furthermore, we highlight the limitations of evaluating EdgeSNNs on
conventional hardware and introduce a dual-track benchmarking strategy to
support fair comparisons and hardware-aware optimization. Through this study,
we aim to bridge the gap between brain-inspired learning and practical edge
deployment, offering insights into current advancements, open challenges, and
future research directions. To the best of our knowledge, this is the first
dedicated and comprehensive survey on EdgeSNNs, providing an essential
reference for researchers and practitioners working at the intersection of
neuromorphic computing and edge intelligence.

</details>


### [9] [Shipwright: Proving liveness of distributed systems with Byzantine participants](https://arxiv.org/abs/2507.14080)
*Derek Leung,Nickolai Zeldovich,Frans Kaashoek*

Main category: cs.DC

TL;DR: Shipwright是一个验证框架，用于证明分布式系统的正确性和活跃性，特别是在存在恶意参与者的情况下。它通过模块化方法和密码学签名支持，实现了对PBFT协议的部分验证。


<details>
  <summary>Details</summary>
Motivation: 在去中心化系统中（如PBFT），确保活跃性至关重要，因为系统可能没有管理员来修复活跃性故障。然而，由于可能存在恶意参与者，活跃性难以保证。

Method: Shipwright引入了三种技术：支持恶意参与者的形式化推理、模块化分解系统和证明、以及对嵌入消息的密码学签名进行合理推理。

Result: Shipwright成功验证了PBFT协议中单个日志条目的一致性（部分功能），并将其转化为可执行的Go实现。实验验证了其在常见情况和故障场景下的活跃性。

Conclusion: Shipwright为分布式系统的形式化验证提供了新方法，特别是在处理恶意参与者和模块化证明方面具有潜力。

Abstract: Ensuring liveness in a decentralized system, such as PBFT, is critical,
because there may not be any single administrator that can restart the system
if it encounters a liveness bug. At the same time, liveness is challenging to
achieve because any single participant could be malicious, and yet the overall
system must make forward progress. While verification is a promising approach
for ensuring the absence of bugs, no prior work has been able to verify
liveness for an executable implementation of PBFT.
  Shipwright is a verification framework for proving correctness and liveness
of distributed systems where some participants might be malicious. Shipwright
introduces three techniques that enable formal reasoning about decentralized
settings with malicious participants, allow developers to decompose their
system and proof in a modular fashion into sub-protocols and sub-proofs, and
support sound reasoning about cryptographic signatures that may be embedded in
messages. We used Shipwright to implement and verify an initial prototype of
agreement on a single log entry in PBFT (with a few limitations) and translate
it to an executable implementation in Go. We experimentally demonstrate its
operation and liveness both in the common case and in several failure
scenarios.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [PGR-DRC: Pre-Global Routing DRC Violation Prediction Using Unsupervised Learning](https://arxiv.org/abs/2507.13355)
*Riadul Islam,Dhandeep Challagundla*

Main category: cs.AR

TL;DR: 提出了一种无监督的DRC违规预测方法，解决了传统方法需要大量平衡数据和训练时间的问题，验证了高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法需要大量平衡数据和训练时间，限制了DRC和光刻热点检测的效率。

Method: 使用无监督学习方法，仅需一类不平衡数据，通过设定阈值对新数据进行分类。

Result: 在28纳米CMOS技术中验证，预测准确率达99.95%，训练时间比SVM和NN模型显著降低。

Conclusion: 该方法显著提高了DRC预测的效率和准确性，适用于下一代微处理器设计。

Abstract: Leveraging artificial intelligence (AI)-driven electronic design and
automation (EDA) tools, high-performance computing, and parallelized algorithms
are essential for next-generation microprocessor innovation, ensuring continued
progress in computing, AI, and semiconductor technology. Machine learning-based
design rule checking (DRC) and lithography hotspot detection can improve
first-pass silicon success. However, conventional ML and neural network
(NN)-based models use supervised learning and require a large balanced dataset
(in terms of positive and negative classes) and training time. This research
addresses those key challenges by proposing the first-ever unsupervised DRC
violation prediction methodology. The proposed model can be built using any
unbalanced dataset using only one class and set a threshold for it, then
fitting any new data querying if they are within the boundary of the model for
classification. This research verified the proposed model by implementing
different computational cores using CMOS 28 nm technology and Synopsys Design
Compiler and IC Compiler II tools. Then, layouts were divided into virtual
grids to collect about 60k data for analysis and verification. The proposed
method has 99.95% prediction test accuracy, while the existing support vector
machine (SVM) and neural network (NN) models have 85.44\% and 98.74\% accuracy,
respectively. In addition, the proposed methodology has about 26.3x and up to
6003x lower training times compared to SVM and NN-models, respectively.

</details>


### [11] [VerilogDB: The Largest, Highest-Quality Dataset with a Preprocessing Framework for LLM-based RTL Generation](https://arxiv.org/abs/2507.13369)
*Paul E. Calzada,Zahin Ibnat,Tanvir Rahman,Kamal Kandula,Danyu Lu,Sujan Kumar Saha,Farimah Farahmandi,Mark Tehranipoor*

Main category: cs.AR

TL;DR: 本文研究了利用大型语言模型（LLMs）进行硬件设计自动化的RTL代码生成，构建了一个高质量的Verilog数据集，并探讨了其应用前景。


<details>
  <summary>Details</summary>
Motivation: LLMs在硬件设计自动化中的应用日益广泛，但目前缺乏高质量的Verilog数据集用于训练和微调。

Method: 通过自动化三阶段流程（数据库创建与管理、数据收集、数据预处理）构建Verilog数据集，并实现可扩展的数据库基础设施。

Result: 构建了包含20,392个Verilog样本、751 MB代码数据的高质量数据集，是目前最大的用于LLM微调的Verilog数据集。

Conclusion: 该数据集为基于LLM的硬件生成研究提供了重要资源，并指出了未来研究方向。

Abstract: Large Language Models (LLMs) are gaining popularity for hardware design
automation, particularly through Register Transfer Level (RTL) code generation.
In this work, we examine the current literature on RTL generation using LLMs
and identify key requirements for training and fine-tuning datasets. We
construct a robust Verilog dataset through an automated three-pronged process
involving database (DB) creation and management with PostgreSQL, data
collection from code hosting sites like OpenCores and GitHub, and data
preprocessing to verify the codes' syntax, run logic synthesis, and extract
relevant module metadata. We implement a scalable and efficient DB
infrastructure to support analysis and detail our preprocessing pipeline to
enforce high-quality data before DB insertion. The resulting dataset comprises
20,392 Verilog samples, 751 MB of Verilog code data, which is the largest
high-quality Verilog dataset for LLM fine-tuning to our knowledge. We further
evaluate the dataset, address associated challenges, and explore potential
applications for future research and development in LLM-based hardware
generation.

</details>


### [12] [GAP-LA: GPU-Accelerated Performance-Driven Layer Assignment](https://arxiv.org/abs/2507.13375)
*Chunyuan Zhao,Zizheng Guo,Zuodong Zhang,Yibo Lin*

Main category: cs.AR

TL;DR: 本文提出了一种GPU加速的性能驱动层分配框架GAP-LA，用于同时优化时序、功耗和拥塞，实验结果显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着设计复杂度的增加，如何在层分配中同时优化时序、功耗和拥塞成为一个挑战，现有研究多局限于部分目标。

Method: 提出了GPU加速的性能驱动层分配框架GAP-LA，进行全局优化。

Result: 实验表明，GAP-LA在WNS和TNS上分别提升了0.3%-9.9%和2.0%-5.4%，同时保持功耗和拥塞的竞争力。

Conclusion: GAP-LA在复杂设计中表现出色，为层分配问题提供了高效的解决方案。

Abstract: Layer assignment is critical for global routing of VLSI circuits. It converts
2D routing paths into 3D routing solutions by determining the proper metal
layer for each routing segments to minimize congestion and via count. As
different layers have different unit resistance and capacitance, layer
assignment also has significant impacts to timing and power. With growing
design complexity, it becomes increasingly challenging to simultaneously
optimize timing, power, and congestion efficiently. Existing studies are mostly
limited to a subset of objectives. In this paper, we propose a GPU-accelerated
performance-driven layer assignment framework, GAP-LA, for holistic
optimization the aforementioned objectives. Experimental results demonstrate
that we can achieve 0.3%-9.9% better worst negative slack (WNS) and 2.0%-5.4%
better total negative slack (TNS) while maintaining power and congestion with
competitive runtime compared with ISPD 2025 contest winners, especially on
designs with up to 12 millions of nets.

</details>


### [13] [4T2R X-ReRAM CiM Array for Variation-tolerant, Low-power, Massively Parallel MAC Operation](https://arxiv.org/abs/2507.13631)
*Fuyuki Kihara,Seiji Uenohara,Satoshi Awamura,Naoko Misawa,Chihiro Matsui,Ken Takeuchi*

Main category: cs.AR

TL;DR: 本文提出了一种适用于计算内存（CiM）的4T2R ReRAM单元和8T SRAM CiM，以减少设备误差和功耗问题。


<details>
  <summary>Details</summary>
Motivation: 解决计算内存中因行并行性增加导致的功耗和设备误差问题。

Method: 提出4T2R ReRAM单元和8T SRAM CiM设计，并与传统4T4R ReRAM单元进行比较。

Result: 4T2R ReRAM单元减少了因ReRAM设备变异引起的误差。

Conclusion: 新设计的4T2R ReRAM单元在CiM中表现更优，降低了误差和功耗。

Abstract: Computation-in-Memory (CiM) is attracting attention as a technology that can
perform MAC calculations required for AI accelerators, at high speed with low
power consumption. However, there is a problem regarding power consumption and
device-derived errors that increase as row parallelism increases. In this
paper, a 4T2R ReRAM cell and an 8T SRAM CiM suitable for CiM is proposed. It is
shown that adopting the proposed 4T2R ReRAM cell reduces the errors due to
variation in ReRAM devices compared to conventional 4T4R ReRAM cells.

</details>
