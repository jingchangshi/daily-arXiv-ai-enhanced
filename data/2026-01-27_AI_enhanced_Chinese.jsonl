{"id": "2601.17136", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17136", "abs": "https://arxiv.org/abs/2601.17136", "authors": ["Julian Bellavita", "Matthew Rubino", "Nakul Iyer", "Andrew Chang", "Aditya Devarakonda", "Flavio Vella", "Giulia Guidi"], "title": "Communication-Avoiding Linear Algebraic Kernel K-Means on GPUs", "comment": null, "summary": "Clustering is an important tool in data analysis, with K-means being popular for its simplicity and versatility. However, it cannot handle non-linearly separable clusters. Kernel K-means addresses this limitation but requires a large kernel matrix, making it computationally and memory intensive. Prior work has accelerated Kernel K-means by formulating it using sparse linear algebra primitives and implementing it on a single GPU. However, that approach cannot run on datasets with more than approximately 80,000 samples due to limited GPU memory.\n  In this work, we address this issue by presenting a suite of distributed-memory parallel algorithms for large-scale Kernel K-means clustering on multi-GPU systems. Our approach maps the most computationally expensive components of Kernel K-means onto communication-efficient distributed linear algebra primitives uniquely tailored for Kernel K-means, enabling highly scalable implementations that efficiently cluster million-scale datasets. Central to our work is the design of partitioning schemes that enable communication-efficient composition of the linear algebra primitives that appear in Kernel K-means.\n  Our 1.5D algorithm consistently achieves the highest performance, enabling Kernel K-means to scale to data one to two orders of magnitude larger than previously practical. On 256 GPUs, it achieves a geometric mean weak scaling efficiency of $79.7\\%$ and a geometric mean strong scaling speedup of $4.2\\times$. Compared to our 1D algorithm, the 1.5D approach achieves up to a $3.6\\times$ speedup on 256 GPUs and reduces clustering time from over an hour to under two seconds relative to a single-GPU sliding window implementation. Our results show that distributed algorithms designed with application-specific linear algebraic formulations can achieve substantial performance improvement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5206\u5e03\u5f0f\u5185\u5b58\u5e76\u884c\u7b97\u6cd5\uff0c\u5728\u591aGPU\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u5927\u89c4\u6a21\u6838K-means\u805a\u7c7b\uff0c\u80fd\u591f\u5904\u7406\u767e\u4e07\u7ea7\u6570\u636e\u96c6\uff0c\u76f8\u6bd4\u5355GPU\u65b9\u6cd5\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u4f20\u7edfK-means\u65e0\u6cd5\u5904\u7406\u975e\u7ebf\u6027\u53ef\u5206\u805a\u7c7b\uff0c\u6838K-means\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\u4f46\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5927\u3002\u73b0\u6709\u5355GPU\u65b9\u6cd5\u53d7\u9650\u4e8e\u5185\u5b58\uff0c\u65e0\u6cd5\u5904\u7406\u8d85\u8fc7\u7ea68\u4e07\u4e2a\u6837\u672c\u7684\u6570\u636e\u96c6\u3002", "method": "\u8bbe\u8ba1\u5206\u5e03\u5f0f\u5185\u5b58\u5e76\u884c\u7b97\u6cd5\uff0c\u5c06\u6838K-means\u8ba1\u7b97\u5bc6\u96c6\u578b\u7ec4\u4ef6\u6620\u5c04\u5230\u901a\u4fe1\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u7ebf\u6027\u4ee3\u6570\u539f\u8bed\u4e0a\uff0c\u91c7\u7528\u4e13\u95e8\u4e3a\u6838K-means\u5b9a\u5236\u7684\u5206\u533a\u65b9\u6848\uff0c\u5305\u62ec1D\u548c1.5D\u7b97\u6cd5\u3002", "result": "1.5D\u7b97\u6cd5\u6027\u80fd\u6700\u4f18\uff0c\u80fd\u5c06\u6838K-means\u6269\u5c55\u5230\u6bd4\u4e4b\u524d\u5b9e\u7528\u89c4\u6a21\u59271-2\u4e2a\u6570\u91cf\u7ea7\u7684\u6570\u636e\u3002\u5728256\u4e2aGPU\u4e0a\uff0c\u51e0\u4f55\u5e73\u5747\u5f31\u6269\u5c55\u6548\u7387\u8fbe79.7%\uff0c\u5f3a\u6269\u5c55\u52a0\u901f\u6bd4\u8fbe4.2\u500d\uff0c\u76f8\u6bd41D\u7b97\u6cd5\u6700\u9ad8\u52a0\u901f3.6\u500d\u3002", "conclusion": "\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\u8bbe\u8ba1\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\u7ed3\u5408\u7ebf\u6027\u4ee3\u6570\u516c\u5f0f\u5316\u80fd\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4f7f\u5927\u89c4\u6a21\u6838K-means\u805a\u7c7b\u53d8\u5f97\u5b9e\u7528\uff0c\u5c06\u805a\u7c7b\u65f6\u95f4\u4ece\u8d85\u8fc71\u5c0f\u65f6\u51cf\u5c11\u52302\u79d2\u4ee5\u5185\u3002"}}
{"id": "2601.17546", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.17546", "abs": "https://arxiv.org/abs/2601.17546", "authors": ["Ravi Kiran Kodali", "Vinoth Punniyamoorthy", "Akash Kumar Agarwal", "Bikesh Kumar", "Balakrishna Pothineni", "Aswathnarayan Muthukrishnan Kirubakaran", "Sumit Saha", "Nachiappan Chockalingam"], "title": "Push Down Optimization for Distributed Multi Cloud Data Integration", "comment": null, "summary": "Enterprises increasingly adopt multi cloud architectures to take advantage of diverse database engines, regional availability, and cost models. In these environments, ETL pipelines must process large, distributed datasets while minimizing latency and transfer cost. Push down optimization, which executes transformation logic within database engines rather than within the ETL tool, has proven highly effective in single cloud systems. However, when applied across multiple clouds, it faces challenges related to data movement, heterogeneous SQL engines, orchestration complexity, and fragmented security controls. This paper examines the feasibility of push down optimization in multi cloud ETL pipelines and analyzes its benefits and limitations. It evaluates localized push down, hybrid models, and data federation techniques that reduce cross cloud traffic while improving performance. A case study across Redshift and BigQuery demonstrates measurable gains, including lower end to end runtime, reduced transfer volume, and improved cost efficiency. The study highlights practical strategies that organizations can adopt to improve ETL scalability and reliability in distributed cloud environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u591a\u4e91\u73af\u5883\u4e2dETL\u7ba1\u9053\u7684\u4e0b\u63a8\u4f18\u5316\u53ef\u884c\u6027\uff0c\u5206\u6790\u4e86\u5176\u4f18\u52bf\u4e0e\u9650\u5236\uff0c\u5e76\u901a\u8fc7Redshift\u548cBigQuery\u7684\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u6027\u80fd\u63d0\u5347\u548c\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u4f01\u4e1a\u8d8a\u6765\u8d8a\u591a\u5730\u91c7\u7528\u591a\u4e91\u67b6\u6784\u4ee5\u5229\u7528\u4e0d\u540c\u7684\u6570\u636e\u5e93\u5f15\u64ce\u3001\u533a\u57df\u53ef\u7528\u6027\u548c\u6210\u672c\u6a21\u578b\u3002\u5728\u8fd9\u4e9b\u73af\u5883\u4e2d\uff0cETL\u7ba1\u9053\u9700\u8981\u5904\u7406\u5927\u91cf\u5206\u5e03\u5f0f\u6570\u636e\u96c6\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5ef6\u8fdf\u548c\u4f20\u8f93\u6210\u672c\u3002\u867d\u7136\u4e0b\u63a8\u4f18\u5316\u5728\u5355\u4e91\u7cfb\u7edf\u4e2d\u5df2\u88ab\u8bc1\u660e\u975e\u5e38\u6709\u6548\uff0c\u4f46\u5728\u591a\u4e91\u73af\u5883\u4e2d\u9762\u4e34\u6570\u636e\u79fb\u52a8\u3001\u5f02\u6784SQL\u5f15\u64ce\u3001\u7f16\u6392\u590d\u6742\u6027\u548c\u788e\u7247\u5316\u5b89\u5168\u63a7\u5236\u7b49\u6311\u6218\u3002", "method": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u4e91ETL\u7ba1\u9053\u4e2d\u7684\u4e0b\u63a8\u4f18\u5316\u53ef\u884c\u6027\uff0c\u8bc4\u4f30\u4e86\u672c\u5730\u5316\u4e0b\u63a8\u3001\u6df7\u5408\u6a21\u578b\u548c\u6570\u636e\u8054\u90a6\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u53ef\u4ee5\u51cf\u5c11\u8de8\u4e91\u6d41\u91cf\u540c\u65f6\u63d0\u9ad8\u6027\u80fd\u3002\u901a\u8fc7Redshift\u548cBigQuery\u7684\u6848\u4f8b\u7814\u7a76\u6765\u9a8c\u8bc1\u6548\u679c\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\u4e86\u53ef\u8861\u91cf\u7684\u6536\u76ca\uff0c\u5305\u62ec\u964d\u4f4e\u7aef\u5230\u7aef\u8fd0\u884c\u65f6\u95f4\u3001\u51cf\u5c11\u4f20\u8f93\u91cf\u4ee5\u53ca\u63d0\u9ad8\u6210\u672c\u6548\u7387\u3002\u7814\u7a76\u7a81\u51fa\u4e86\u7ec4\u7ec7\u53ef\u4ee5\u5728\u5206\u5e03\u5f0f\u4e91\u73af\u5883\u4e2d\u91c7\u7528\u7684\u5b9e\u9645\u7b56\u7565\uff0c\u4ee5\u6539\u5584ETL\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u591a\u4e91\u73af\u5883\u4e2d\u7684\u4e0b\u63a8\u4f18\u5316\u867d\u7136\u9762\u4e34\u6311\u6218\uff0c\u4f46\u901a\u8fc7\u9002\u5f53\u7684\u7b56\u7565\uff08\u5982\u672c\u5730\u5316\u4e0b\u63a8\u3001\u6df7\u5408\u6a21\u578b\u548c\u6570\u636e\u8054\u90a6\u6280\u672f\uff09\u53ef\u4ee5\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u6210\u672c\u8282\u7ea6\uff0c\u4e3a\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u6539\u5584ETL\u7ba1\u9053\u6548\u7387\u7684\u5b9e\u9645\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17578", "categories": ["cs.DC", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.17578", "abs": "https://arxiv.org/abs/2601.17578", "authors": ["Henrik Bengtsson"], "title": "A Unified Approach to Concurrent, Parallel Map-Reduce in R using Futures", "comment": "16 pages including 2.5 pages references, 1 figure", "summary": "The R ecosystem offers a rich variety of map-reduce application programming interfaces (APIs) for iterative computations, yet parallelizing code across these diverse frameworks requires learning multiple, often incompatible, parallel APIs. The futurize package addresses this challenge by providing a single function, futurize(), which transpiles sequential map-reduce expressions into their parallel equivalents in the future ecosystem, which performs all the heavy lifting. By leveraging R's native pipe operator, users can parallelize existing code with minimal refactoring -- often by simply appending `|> futurize()' to an expression. The package supports classical map-reduce functions from base R, purrr, crossmap, foreach, plyr, BiocParallel, e.g., lapply(xs, fcn) |> futurize() and map(xs, fcn) |> futurize(), as well as a growing set of domain-specific packages, e.g., boot, caret, glmnet, lme4, mgcv, and tm. By abstracting away the underlying parallel machinery, and unifying handling of future options, the package enables developers to declare what to parallelize via futurize(), and end-users to choose how via plan(). This article describes the philosophy, design, and implementation of futurize, demonstrates its usage across various map-reduce paradigms, and discusses its role in simplifying parallel computing in R.", "AI": {"tldr": "futurize\u5305\u63d0\u4f9b\u5355\u4e00\u51fd\u6570futurize()\uff0c\u5c06\u987a\u5e8fmap-reduce\u8868\u8fbe\u5f0f\u8f6c\u6362\u4e3afuture\u751f\u6001\u7cfb\u7edf\u7684\u5e76\u884c\u7248\u672c\uff0c\u7b80\u5316R\u4e2d\u5e76\u884c\u8ba1\u7b97", "motivation": "R\u751f\u6001\u7cfb\u7edf\u6709\u4e30\u5bcc\u7684map-reduce API\uff0c\u4f46\u5e76\u884c\u5316\u9700\u8981\u5b66\u4e60\u591a\u79cd\u4e0d\u517c\u5bb9\u7684\u5e76\u884cAPI\uff0c\u5bfc\u81f4\u4ee3\u7801\u79fb\u690d\u56f0\u96be", "method": "\u901a\u8fc7futurize()\u51fd\u6570\u5c06\u987a\u5e8fmap-reduce\u8868\u8fbe\u5f0f\u8f6c\u8bd1\u4e3a\u5e76\u884c\u7248\u672c\uff0c\u5229\u7528R\u539f\u751f\u7ba1\u9053\u64cd\u4f5c\u7b26\uff0c\u652f\u6301base R\u3001purrr\u3001crossmap\u3001foreach\u3001plyr\u3001BiocParallel\u7b49\u591a\u79cd\u6846\u67b6", "result": "\u7528\u6237\u53ea\u9700\u5728\u73b0\u6709\u4ee3\u7801\u540e\u6dfb\u52a0`|> futurize()`\u5373\u53ef\u5e76\u884c\u5316\uff0c\u652f\u6301\u591a\u79cd\u9886\u57df\u7279\u5b9a\u5305\u5982boot\u3001caret\u3001glmnet\u7b49\uff0c\u5f00\u53d1\u8005\u58f0\u660e\u5e76\u884c\u5185\u5bb9\uff0c\u7528\u6237\u901a\u8fc7plan()\u9009\u62e9\u5e76\u884c\u65b9\u5f0f", "conclusion": "futurize\u5305\u901a\u8fc7\u62bd\u8c61\u5e95\u5c42\u5e76\u884c\u673a\u5236\uff0c\u7edf\u4e00\u5904\u7406future\u9009\u9879\uff0c\u7b80\u5316\u4e86R\u4e2d\u7684\u5e76\u884c\u8ba1\u7b97\uff0c\u4f7f\u5e76\u884c\u5316\u53d8\u5f97\u7b80\u5355\u76f4\u89c2"}}
{"id": "2601.17279", "categories": ["cs.AR", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.17279", "abs": "https://arxiv.org/abs/2601.17279", "authors": ["Sonu Kumar", "Lavanya Vinnakota", "Mukul Lokhande", "Santosh Kumar Vishvakarma", "Adam Teman"], "title": "SPADE: A SIMD Posit-enabled compute engine for Accelerating DNN Efficiency", "comment": null, "summary": "The growing demand for edge-AI systems requires arithmetic units that balance numerical precision, energy efficiency, and compact hardware while supporting diverse formats. Posit arithmetic offers advantages over floating- and fixed-point representations through its tapered precision, wide dynamic range, and improved numerical robustness. This work presents SPADE, a unified multi-precision SIMD Posit-based multiplyaccumulate (MAC) architecture supporting Posit (8,0), Posit (16,1), and Posit (32,2) within a single framework. Unlike prior single-precision or floating/fixed-point SIMD MACs, SPADE introduces a regime-aware, lane-fused SIMD Posit datapath that hierarchically reuses Posit-specific submodules (LOD, complementor, shifter, and multiplier) across 8/16/32-bit precisions without datapath replication. FPGA implementation on a Xilinx Virtex-7 shows 45.13% LUT and 80% slice reduction for Posit (8,0), and up to 28.44% and 17.47% improvement for Posit (16,1) and Posit (32,2) over prior work, with only 6.9% LUT and 14.9% register overhead for multi-precision support. ASIC results across TSMC nodes achieve 1.38 GHz at 6.1 mW (28 nm). Evaluation on MNIST, CIFAR-10/100, and alphabet datasets confirms competitive inference accuracy.", "AI": {"tldr": "SPADE\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u7cbe\u5ea6SIMD Posit MAC\u67b6\u6784\uff0c\u652f\u6301Posit(8,0)\u3001(16,1)\u3001(32,2)\u683c\u5f0f\uff0c\u901a\u8fc7\u5206\u5c42\u590d\u7528\u5b50\u6a21\u5757\u51cf\u5c11\u786c\u4ef6\u5f00\u9500\uff0c\u5728FPGA\u548cASIC\u4e0a\u5b9e\u73b0\u9ad8\u6548\u80fd\u8fb9\u7f18AI\u8ba1\u7b97\u3002", "motivation": "\u8fb9\u7f18AI\u7cfb\u7edf\u9700\u8981\u5e73\u8861\u6570\u503c\u7cbe\u5ea6\u3001\u80fd\u6548\u548c\u786c\u4ef6\u7d27\u51d1\u6027\u7684\u7b97\u672f\u5355\u5143\u3002Posit\u7b97\u672f\u76f8\u6bd4\u6d6e\u70b9\u548c\u5b9a\u70b9\u8868\u793a\u5177\u6709\u9525\u5f62\u7cbe\u5ea6\u3001\u5bbd\u52a8\u6001\u8303\u56f4\u548c\u66f4\u597d\u7684\u6570\u503c\u9c81\u68d2\u6027\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u65b9\u6848\u591a\u4e3a\u5355\u7cbe\u5ea6\u6216\u6d6e\u70b9/\u5b9a\u70b9SIMD MAC\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u591a\u7cbe\u5ea6Posit\u652f\u6301\u3002", "method": "\u63d0\u51faSPADE\u67b6\u6784\uff1a1) \u91c7\u7528\u57fa\u4e8eregime-aware\u7684lane-fused SIMD Posit\u6570\u636e\u901a\u8def\uff1b2) \u5206\u5c42\u590d\u7528Posit\u4e13\u7528\u5b50\u6a21\u5757(LOD\u3001\u8865\u7801\u5668\u3001\u79fb\u4f4d\u5668\u3001\u4e58\u6cd5\u5668)\u8de88/16/32\u4f4d\u7cbe\u5ea6\uff1b3) \u907f\u514d\u6570\u636e\u901a\u8def\u590d\u5236\uff0c\u652f\u6301\u4e09\u79cdPosit\u683c\u5f0f\u7684\u7edf\u4e00\u6846\u67b6\u3002", "result": "FPGA\u5b9e\u73b0\uff1aXilinx Virtex-7\u4e0a\uff0cPosit(8,0)\u51cf\u5c1145.13% LUT\u548c80% slice\uff1bPosit(16,1)\u548c(32,2)\u5206\u522b\u63d0\u534728.44%\u548c17.47%\uff1b\u591a\u7cbe\u5ea6\u652f\u6301\u4ec5\u589e\u52a06.9% LUT\u548c14.9%\u5bc4\u5b58\u5668\u5f00\u9500\u3002ASIC\uff1a28nm TSMC\u8282\u70b9\u8fbe1.38GHz@6.1mW\u3002MNIST\u3001CIFAR-10/100\u7b49\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u7ade\u4e89\u6027\u63a8\u7406\u7cbe\u5ea6\u3002", "conclusion": "SPADE\u901a\u8fc7\u521b\u65b0\u7684\u7edf\u4e00\u591a\u7cbe\u5ea6SIMD Posit MAC\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u6570\u503c\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u786c\u4ef6\u8d44\u6e90\uff0c\u4e3a\u8fb9\u7f18AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7d27\u51d1\u7684\u7b97\u672f\u5355\u5143\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u7cbe\u5ea6\u3001\u80fd\u6548\u548c\u786c\u4ef6\u6210\u672c\u7684\u9700\u6c42\u3002"}}
{"id": "2601.17589", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17589", "abs": "https://arxiv.org/abs/2601.17589", "authors": ["Thomas Sandholm", "Bernardo A. Huberman", "Klas Segeljakt", "Paris Carbone"], "title": "Lightspeed Data Compute for the Space Era", "comment": null, "summary": "While thousands of satellites photograph Earth every day, most of that data never makes it to the ground because downlink bandwidth simply cannot keep up. Processing data in the Low Earth Orbit (LEO) zone offers promising capabilities to overcome this limitation. We propose SpaceCoMP, a MapReduce-inspired processing model for LEO satellite mesh networks. Ground stations submit queries over an area of interest; satellites collect sensor data, process it cooperatively at light-speed using inter-satellite laser links, and return only the results. Our compute model leverages space physics to accelerate computations on LEO megaconstellations. Our distance-aware routing protocol exploits orbital geometry. In addition, our bipartite match scheduling strategy places map and reduce tasks within orbital regions while minimizing aggregation costs. We have simulated constellations of 1,000-10,000 satellites showcasing 61-79% improvement in map placement efficiency over baselines, 18-28% over greedy allocation, and 67-72% reduction in aggregation cost. SpaceCoMP demonstrates that the orbital mesh is not merely useful as a communication relay, as seen today, but can provide the foundations for faster data processing above the skies.", "AI": {"tldr": "SpaceCoMP\uff1a\u4e00\u79cd\u53d7MapReduce\u542f\u53d1\u7684LEO\u536b\u661f\u7f51\u7edc\u5904\u7406\u6a21\u578b\uff0c\u5229\u7528\u661f\u95f4\u6fc0\u5149\u94fe\u8def\u5728\u8f68\u9053\u4e0a\u8fdb\u884c\u534f\u540c\u6570\u636e\u5904\u7406\uff0c\u4ec5\u8fd4\u56de\u7ed3\u679c\u800c\u975e\u539f\u59cb\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u4e0b\u884c\u5e26\u5bbd\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u6bcf\u5929\u6570\u5343\u9897\u536b\u661f\u62cd\u6444\u5730\u7403\u6570\u636e\uff0c\u4f46\u7531\u4e8e\u4e0b\u884c\u5e26\u5bbd\u9650\u5236\uff0c\u5927\u90e8\u5206\u6570\u636e\u65e0\u6cd5\u4f20\u56de\u5730\u9762\u3002\u5728\u4f4e\u5730\u7403\u8f68\u9053\u533a\u57df\u8fdb\u884c\u6570\u636e\u5904\u7406\u6709\u671b\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51faSpaceCoMP\u5904\u7406\u6a21\u578b\uff1a\u5730\u9762\u7ad9\u63d0\u4ea4\u611f\u5174\u8da3\u533a\u57df\u67e5\u8be2\uff1b\u536b\u661f\u6536\u96c6\u4f20\u611f\u5668\u6570\u636e\uff0c\u5229\u7528\u661f\u95f4\u6fc0\u5149\u94fe\u8def\u8fdb\u884c\u534f\u540c\u5904\u7406\uff1b\u91c7\u7528\u8ddd\u79bb\u611f\u77e5\u8def\u7531\u534f\u8bae\u5229\u7528\u8f68\u9053\u51e0\u4f55\u7279\u6027\uff1b\u4f7f\u7528\u4e8c\u5206\u56fe\u5339\u914d\u8c03\u5ea6\u7b56\u7565\u5728\u8f68\u9053\u533a\u57df\u5185\u653e\u7f6emap\u548creduce\u4efb\u52a1\u4ee5\u6700\u5c0f\u5316\u805a\u5408\u6210\u672c\u3002", "result": "\u57281000-10000\u9897\u536b\u661f\u7684\u661f\u5ea7\u6a21\u62df\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5map\u653e\u7f6e\u6548\u7387\u63d0\u534761-79%\uff0c\u76f8\u6bd4\u8d2a\u5a6a\u5206\u914d\u63d0\u534718-28%\uff0c\u805a\u5408\u6210\u672c\u964d\u4f4e67-72%\u3002", "conclusion": "SpaceCoMP\u8bc1\u660e\u8f68\u9053\u7f51\u72b6\u7f51\u7edc\u4e0d\u4ec5\u53ef\u4f5c\u4e3a\u901a\u4fe1\u4e2d\u7ee7\uff08\u5982\u5f53\u524d\u5e94\u7528\uff09\uff0c\u8fd8\u80fd\u4e3a\u5929\u7a7a\u4e4b\u4e0a\u7684\u66f4\u5feb\u6570\u636e\u5904\u7406\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2601.17615", "categories": ["cs.AR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.17615", "abs": "https://arxiv.org/abs/2601.17615", "authors": ["Rahul Bera", "Zhenrong Lang", "Caroline Hengartner", "Konstantinos Kanellopoulos", "Rakesh Kumar", "Mohammad Sadrosadati", "Onur Mutlu"], "title": "Athena: Synergizing Data Prefetching and Off-Chip Prediction via Online Reinforcement Learning", "comment": null, "summary": "Prefetching and off-chip prediction are two techniques proposed to hide long memory access latencies in high-performance processors. In this work, we demonstrate that: (1) prefetching and off-chip prediction often provide complementary performance benefits, yet (2) naively combining them often fails to realize their full performance potential, and (3) existing prefetcher control policies leave significant room for performance improvement behind.\n  Our goal is to design a holistic framework that can autonomously learn to coordinate an off-chip predictor with multiple prefetchers employed at various cache levels. To this end, we propose a new technique called Athena, which models the coordination between prefetchers and off-chip predictor (OCP) as a reinforcement learning (RL) problem. Athena acts as the RL agent that observes multiple system-level features (e.g., prefetcher/OCP accuracy, bandwidth usage) over an epoch of program execution, and uses them as state information to select a coordination action (i.e., enabling the prefetcher and/or OCP, and adjusting prefetcher aggressiveness). At the end of every epoch, Athena receives a numerical reward that measures the change in multiple system-level metrics (e.g., number of cycles taken to execute an epoch). Athena uses this reward to autonomously and continuously learn a policy to coordinate prefetchers with OCP.\n  Our extensive evaluation using a diverse set of memory-intensive workloads shows that Athena consistently outperforms prior state-of-the-art coordination policies across a wide range of system configurations with various combinations of underlying prefetchers, OCPs, and main memory bandwidths, while incurring only modest storage overhead. Athena is freely available at https://github.com/CMU-SAFARI/Athena.", "AI": {"tldr": "Athena\uff1a\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u534f\u8c03\u591a\u7ea7\u7f13\u5b58\u9884\u53d6\u5668\u548c\u7247\u5916\u9884\u6d4b\u5668\uff0c\u4ee5\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u6027\u80fd", "motivation": "\u9884\u53d6\u548c\u7247\u5916\u9884\u6d4b\u662f\u9690\u85cf\u5185\u5b58\u8bbf\u95ee\u5ef6\u8fdf\u7684\u4e24\u79cd\u6280\u672f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u534f\u8c03\u8fd9\u4e24\u79cd\u6280\u672f\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u5b83\u4eec\u7684\u6027\u80fd\u6f5c\u529b\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u534f\u8c03\u673a\u5236", "method": "\u5c06\u9884\u53d6\u5668\u548c\u7247\u5916\u9884\u6d4b\u5668\u7684\u534f\u8c03\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0cAthena\u4f5c\u4e3aRL\u4ee3\u7406\uff0c\u89c2\u5bdf\u7cfb\u7edf\u7ea7\u7279\u5f81\uff08\u5982\u51c6\u786e\u7387\u3001\u5e26\u5bbd\u4f7f\u7528\uff09\uff0c\u9009\u62e9\u534f\u8c03\u52a8\u4f5c\uff08\u542f\u7528/\u7981\u7528\u7ec4\u4ef6\u3001\u8c03\u6574\u9884\u53d6\u5668\u6fc0\u8fdb\u6027\uff09\uff0c\u901a\u8fc7\u5956\u52b1\u4fe1\u53f7\u6301\u7eed\u5b66\u4e60\u4f18\u5316\u7b56\u7565", "result": "Athena\u5728\u591a\u79cd\u5185\u5b58\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u534f\u8c03\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u7cfb\u7edf\u914d\u7f6e\uff08\u4e0d\u540c\u9884\u53d6\u5668\u3001OCP\u3001\u5185\u5b58\u5e26\u5bbd\u7ec4\u5408\uff09\uff0c\u4e14\u5b58\u50a8\u5f00\u9500\u9002\u4e2d", "conclusion": "Athena\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u548c\u534f\u8c03\u9884\u53d6\u5668\u4e0e\u7247\u5916\u9884\u6d4b\u5668\uff0c\u663e\u8457\u63d0\u5347\u5185\u5b58\u8bbf\u95ee\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2601.17670", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.17670", "abs": "https://arxiv.org/abs/2601.17670", "authors": ["Roberto Rossi", "Steven D. Prestwich"], "title": "Grammar-Aware Literate Generative Mathematical Programming with Compiler-in-the-Loop", "comment": "18 pages, 10 figures", "summary": "This work investigates generative mathematical programming through the lens of Algebraic Modelling Languages (AMLs) and compiler-guided model synthesis. By leveraging PyOPL, an OPL-like AML compiler that provides detailed syntax diagnostics, we introduce SyntAGM, an end-to-end system that translates natural language problem descriptions into PyOPL models via a generate--compile--assess--revise loop. SyntAGM is grammar-aware thanks to in-context exposure to the PyOPL BNF grammar, and benefits from few-shot retrieval of literate PyOPL model exemplars. To obtain a valid PyOPL model that matches the problem description, SyntAGM mobilises compiler feedback and an LLM-based alignment judge. In a comparative study against established prompting baselines SyntAGM achieves competitive accuracy with superior token, cost, and latency profiles.", "AI": {"tldr": "SyntAGM\u662f\u4e00\u4e2a\u901a\u8fc7\u751f\u6210-\u7f16\u8bd1-\u8bc4\u4f30-\u4fee\u8ba2\u5faa\u73af\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u8f6c\u6362\u4e3aPyOPL\u6570\u5b66\u89c4\u5212\u6a21\u578b\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u5229\u7528\u7f16\u8bd1\u5668\u53cd\u9988\u548cLLM\u5bf9\u9f50\u5224\u65ad\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u5f0f\u6570\u5b66\u7f16\u7a0b\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u4ee3\u6570\u5efa\u6a21\u8bed\u8a00\u548c\u7f16\u8bd1\u5668\u5f15\u5bfc\u7684\u6a21\u578b\u5408\u6210\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u6570\u5b66\u89c4\u5212\u6a21\u578b\uff0c\u63d0\u9ad8\u5efa\u6a21\u6548\u7387\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u5f00\u53d1SyntAGM\u7cfb\u7edf\uff0c\u57fa\u4e8ePyOPL\u4ee3\u6570\u5efa\u6a21\u8bed\u8a00\u7f16\u8bd1\u5668\uff0c\u91c7\u7528\u751f\u6210-\u7f16\u8bd1-\u8bc4\u4f30-\u4fee\u8ba2\u5faa\u73af\uff0c\u7ed3\u5408\u8bed\u6cd5\u611f\u77e5\uff08\u901a\u8fc7BNF\u8bed\u6cd5\u4e0a\u4e0b\u6587\u5b66\u4e60\uff09\u3001\u5c11\u6837\u672c\u68c0\u7d22\u793a\u4f8b\u3001\u7f16\u8bd1\u5668\u53cd\u9988\u548cLLM\u5bf9\u9f50\u5224\u65ad\u6765\u751f\u6210\u548c\u4f18\u5316\u6a21\u578b\u3002", "result": "\u4e0e\u73b0\u6709\u63d0\u793a\u57fa\u51c6\u76f8\u6bd4\uff0cSyntAGM\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5728\u4ee4\u724c\u4f7f\u7528\u3001\u6210\u672c\u548c\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "SyntAGM\u901a\u8fc7\u7ed3\u5408\u7f16\u8bd1\u5668\u53cd\u9988\u548cLLM\u5bf9\u9f50\u5224\u65ad\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u5230\u6570\u5b66\u89c4\u5212\u6a21\u578b\u7684\u81ea\u52a8\u8f6c\u6362\uff0c\u4e3a\u751f\u6210\u5f0f\u6570\u5b66\u7f16\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17606", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17606", "abs": "https://arxiv.org/abs/2601.17606", "authors": ["Shannon Kinkead", "Jackson Wesley", "Whit Schonbein", "David DeBonis", "Matthew G. F. Dosanjh", "Amanda Bienz"], "title": "Scaling All-to-all Operations Across Emerging Many-Core Supercomputers", "comment": null, "summary": "Performant all-to-all collective operations in MPI are critical to fast Fourier transforms, transposition, and machine learning applications. There are many existing implementations for all-to-all exchanges on emerging systems, with the achieved performance dependent on many factors, including message size, process count, architecture, and parallel system partition. This paper presents novel all-to-all algorithms for emerging many-core systems. Further, the paper presents a performance analysis against existing algorithms and system MPI, with novel algorithms achieving up to 3x speedup over system MPI at 32 nodes of state-of-the-art Sapphire Rapids systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u65b0\u5174\u4f17\u6838\u7cfb\u7edf\u7684\u521b\u65b0all-to-all\u7b97\u6cd5\uff0c\u5728Sapphire Rapids\u7cfb\u7edf\u4e0a\u76f8\u6bd4\u7cfb\u7edfMPI\u5b9e\u73b0\u4e86\u6700\u9ad83\u500d\u7684\u6027\u80fd\u52a0\u901f\u3002", "motivation": "MPI\u4e2d\u7684\u9ad8\u6027\u80fdall-to-all\u96c6\u4f53\u64cd\u4f5c\u5bf9\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u3001\u8f6c\u7f6e\u548c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u5b9e\u73b0\u5728\u65b0\u5174\u7cfb\u7edf\u4e0a\u7684\u6027\u80fd\u53d7\u6d88\u606f\u5927\u5c0f\u3001\u8fdb\u7a0b\u6570\u3001\u67b6\u6784\u548c\u5e76\u884c\u7cfb\u7edf\u5206\u533a\u7b49\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u9700\u8981\u9488\u5bf9\u65b0\u5174\u4f17\u6838\u7cfb\u7edf\u8fdb\u884c\u4f18\u5316\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u65b0\u5174\u4f17\u6838\u7cfb\u7edf\u7684\u521b\u65b0all-to-all\u7b97\u6cd5\uff0c\u5e76\u5bf9\u73b0\u6709\u7b97\u6cd5\u548c\u7cfb\u7edfMPI\u8fdb\u884c\u4e86\u6027\u80fd\u5206\u6790\u6bd4\u8f83\u3002", "result": "\u572832\u4e2a\u8282\u70b9\u7684\u5148\u8fdbSapphire Rapids\u7cfb\u7edf\u4e0a\uff0c\u65b0\u7b97\u6cd5\u76f8\u6bd4\u7cfb\u7edfMPI\u5b9e\u73b0\u4e86\u6700\u9ad83\u500d\u7684\u6027\u80fd\u52a0\u901f\u3002", "conclusion": "\u9488\u5bf9\u65b0\u5174\u4f17\u6838\u7cfb\u7edf\u8bbe\u8ba1\u7684\u4e13\u7528all-to-all\u7b97\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347MPI\u96c6\u4f53\u64cd\u4f5c\u7684\u6027\u80fd\uff0c\u5bf9\u79d1\u5b66\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.17633", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17633", "abs": "https://arxiv.org/abs/2601.17633", "authors": ["Rakesh Nadig", "Vamanan Arulchelvan", "Mayank Kabra", "Harshita Gupta", "Rahul Bera", "Nika Mansouri Ghiasi", "Nanditha Rao", "Qingcai Jiang", "Andreas Kosmas Kakolyris", "Yu Liang", "Mohammad Sadrosadati", "Onur Mutlu"], "title": "Conduit: Programmer-Transparent Near-Data Processing Using Multiple Compute-Capable Resources in Solid State Drives", "comment": "To appear in IEEE International Symposium on High-Performance Computer Architecture (HPCA) 2026", "summary": "Solid-state drives (SSDs) are well suited for near-data processing (NDP) because they: (1) store large application datasets, and (2) support three NDP paradigms: in-storage processing (ISP), processing using DRAM in the SSD (PuD-SSD), and in-flash processing (IFP). A large body of prior SSD-based NDP techniques operate in isolation, mapping computations to only one or two NDP paradigms (i.e., ISP, PuD-SSD, or IFP) within the SSD. These techniques (1) are tailored to specific workloads or kernels, (2) do not exploit the full computational potential of an SSD, and (3) lack programmer-transparency. While several prior works propose techniques to partition computation between the host and near-memory accelerators, adapting these techniques to SSDs has limited benefits because they (1) ignore the heterogeneity of the SSD resources, and (2) make offloading decisions based on limited factors such as bandwidth utilization, or data movement cost. We propose Conduit, a general-purpose, programmer-transparent NDP framework for SSDs that leverages multiple SSD computation resources. At compile time, Conduit executes a custom compiler (e.g., LLVM) pass that (i) vectorizes suitable application code segments into SIMD operations that align with the SSD's page layout, and (ii) embeds metadata (e.g., operation type, operand sizes) into the vectorized instructions to guide runtime offloading decisions. At runtime, within the SSD, Conduit performs instruction-granularity offloading by evaluating six key features, and uses a cost function to select the most suitable SSD resource. We evaluate Conduit and two prior NDP offloading techniques using an in-house event-driven SSD simulator on six data-intensive workloads. Conduit outperforms the best-performing prior offloading policy by 1.8x and reduces energy consumption by 46%.", "AI": {"tldr": "Conduit\u662f\u4e00\u4e2a\u901a\u7528\u7684\u3001\u5bf9\u7a0b\u5e8f\u5458\u900f\u660e\u7684SSD\u8fd1\u6570\u636e\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528SSD\u7684\u591a\u79cd\u8ba1\u7b97\u8d44\u6e90\uff0c\u5b9e\u73b0\u6307\u4ee4\u7c92\u5ea6\u7684\u5378\u8f7d\u51b3\u7b56\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u6027\u80fd\u63d0\u53471.8\u500d\uff0c\u80fd\u8017\u964d\u4f4e46%\u3002", "motivation": "\u73b0\u6709SSD\u8fd1\u6570\u636e\u5904\u7406\u6280\u672f\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u53ea\u9488\u5bf9\u7279\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u6216\u5185\u6838\uff1b2\uff09\u672a\u80fd\u5145\u5206\u5229\u7528SSD\u7684\u5168\u90e8\u8ba1\u7b97\u6f5c\u529b\uff1b3\uff09\u7f3a\u4e4f\u5bf9\u7a0b\u5e8f\u5458\u7684\u900f\u660e\u5ea6\u3002\u540c\u65f6\uff0c\u73b0\u6709\u4e3b\u673a\u4e0e\u8fd1\u5185\u5b58\u52a0\u901f\u5668\u4e4b\u95f4\u7684\u8ba1\u7b97\u5206\u533a\u6280\u672f\u4e5f\u4e0d\u9002\u7528\u4e8eSSD\uff0c\u56e0\u4e3a\u5b83\u4eec\u5ffd\u7565\u4e86SSD\u8d44\u6e90\u7684\u5f02\u6784\u6027\uff0c\u4e14\u5378\u8f7d\u51b3\u7b56\u57fa\u4e8e\u6709\u9650\u56e0\u7d20\u3002", "method": "Conduit\u91c7\u7528\u7f16\u8bd1\u65f6\u548c\u8fd0\u884c\u65f6\u76f8\u7ed3\u5408\u7684\u65b9\u6848\u3002\u7f16\u8bd1\u65f6\u901a\u8fc7\u81ea\u5b9a\u4e49\u7f16\u8bd1\u5668\uff08\u5982LLVM\uff09\u5c06\u5408\u9002\u7684\u5e94\u7528\u4ee3\u7801\u6bb5\u5411\u91cf\u5316\u4e3a\u4e0eSSD\u9875\u9762\u5e03\u5c40\u5bf9\u9f50\u7684SIMD\u64cd\u4f5c\uff0c\u5e76\u5728\u5411\u91cf\u5316\u6307\u4ee4\u4e2d\u5d4c\u5165\u5143\u6570\u636e\u3002\u8fd0\u884c\u65f6\u5728SSD\u5185\u90e8\u57fa\u4e8e\u516d\u4e2a\u5173\u952e\u7279\u5f81\u8bc4\u4f30\u6307\u4ee4\u7c92\u5ea6\u5378\u8f7d\uff0c\u4f7f\u7528\u6210\u672c\u51fd\u6570\u9009\u62e9\u6700\u5408\u9002\u7684SSD\u8d44\u6e90\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u8bc4\u4f30\uff0cConduit\u76f8\u6bd4\u6700\u4f73\u73b0\u6709\u5378\u8f7d\u7b56\u7565\u6027\u80fd\u63d0\u53471.8\u500d\uff0c\u80fd\u8017\u964d\u4f4e46%\u3002", "conclusion": "Conduit\u901a\u8fc7\u5145\u5206\u5229\u7528SSD\u7684\u591a\u79cd\u8ba1\u7b97\u8d44\u6e90\uff0c\u5b9e\u73b0\u4e86\u901a\u7528\u7684\u3001\u5bf9\u7a0b\u5e8f\u5458\u900f\u660e\u7684\u8fd1\u6570\u636e\u5904\u7406\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u80fd\u6548\u3002"}}
{"id": "2601.17957", "categories": ["cs.PL", "cs.DC", "cs.FL", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.17957", "abs": "https://arxiv.org/abs/2601.17957", "authors": ["Ehud Shapiro"], "title": "Types for Grassroots Logic Programs", "comment": null, "summary": "Grassroots Logic Programs (GLP) is a concurrent logic programming language in which logic variables are partitioned into paired readers and writers. An assignment is produced at most once via a writer and consumed at most once via its paired reader, and may contain additional readers and/or writers. This enables the concise expression of rich multidirectional communication modalities.\n  ``Logic Programs as Types for Logic Programs'' (LICS'91) defined types as regular sets of paths over derivable ground atoms. Here, we define types to be regular sets of moded paths, where a mode captures directionality of communication -- whether a subterm is consumed from or produced to the environment -- enabling the typing of interactive partial computations including those that eventually deadlock or fail, or never terminate. We provide a syntactic definition of well-typing and prove that a program is well-typed iff the path abstraction of its moded-atom semantics satisfies covariance and contravariance conditions with respect to its type.\n  The GLP type system was implemented in Dart by AI, starting from a mathematical specification of Typed GLP (this paper), deriving from it an English spec (written by AI), and from the spec deriving Dart code (by AI). While GLP is naturally untyped, the motivation for Typed GLP comes from programming with AI: Asking AI to program complex communication modalities in GLP (and in general) and hoping for the best is a tenuous strategy. The emerging discipline we advocate and employ is for the human designer and AI to jointly develop and agree upon (1)~GLP types; (2)~GLP procedure type declarations; (3)~informal (English) descriptions of the procedures; and only then let AI attempt to write (4)~GLP code based on those.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Typed GLP\u7cfb\u7edf\uff0c\u4e3aGrassroots Logic Programs\u6dfb\u52a0\u7c7b\u578b\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u5f0f\u5316\u8def\u5f84\u5b9a\u4e49\u7c7b\u578b\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u90e8\u5206\u8ba1\u7b97\u7684\u7c7b\u578b\u68c0\u67e5\uff0c\u5e76\u5c55\u793a\u4e86AI\u8f85\u52a9\u5f00\u53d1\u6d41\u7a0b\u3002", "motivation": "GLP\u4f5c\u4e3a\u65e0\u7c7b\u578b\u5e76\u53d1\u903b\u8f91\u7f16\u7a0b\u8bed\u8a00\uff0c\u5728\u8868\u8fbe\u590d\u6742\u901a\u4fe1\u6a21\u5f0f\u65f6\u5b58\u5728\u6311\u6218\u3002\u7279\u522b\u662f\u5f53\u4f7f\u7528AI\u7f16\u7a0b\u65f6\uff0c\u7f3a\u4e4f\u7c7b\u578b\u7cfb\u7edf\u4f7f\u5f97\u7f16\u5199\u6b63\u786e\u4ee3\u7801\u53d8\u5f97\u56f0\u96be\u3002\u9700\u8981\u7c7b\u578b\u7cfb\u7edf\u6765\u786e\u4fddAI\u751f\u6210\u7684\u4ee3\u7801\u7b26\u5408\u9884\u671f\u901a\u4fe1\u6a21\u5f0f\u3002", "method": "\u5b9a\u4e49\u7c7b\u578b\u4e3a\u6a21\u5f0f\u5316\u8def\u5f84\u7684\u6b63\u5219\u96c6\u5408\uff0c\u5176\u4e2d\u6a21\u5f0f\u6355\u83b7\u901a\u4fe1\u65b9\u5411\u6027\uff08\u6d88\u8d39\u6216\u4ea7\u751f\uff09\u3002\u63d0\u4f9b\u8bed\u6cd5\u4e0a\u7684\u826f\u7c7b\u578b\u5b9a\u4e49\uff0c\u5e76\u8bc1\u660e\u7a0b\u5e8f\u826f\u7c7b\u578b\u5f53\u4e14\u4ec5\u5f53\u5176\u6a21\u5f0f\u539f\u5b50\u8bed\u4e49\u7684\u8def\u5f84\u62bd\u8c61\u6ee1\u8db3\u534f\u53d8\u548c\u9006\u53d8\u6761\u4ef6\u3002", "result": "\u5b9e\u73b0\u4e86GLP\u7c7b\u578b\u7cfb\u7edf\uff0c\u901a\u8fc7AI\u4ece\u6570\u5b66\u89c4\u8303\u751f\u6210\u82f1\u6587\u89c4\u8303\uff0c\u518d\u751f\u6210Dart\u4ee3\u7801\u3002\u5efa\u7acb\u4e86\u4eba\u673a\u534f\u4f5c\u5f00\u53d1\u6d41\u7a0b\uff1a\u5148\u5b9a\u4e49GLP\u7c7b\u578b\u3001\u8fc7\u7a0b\u7c7b\u578b\u58f0\u660e\u3001\u975e\u6b63\u5f0f\u63cf\u8ff0\uff0c\u518d\u8ba9AI\u7f16\u5199\u4ee3\u7801\u3002", "conclusion": "Typed GLP\u4e3a\u5e76\u53d1\u903b\u8f91\u7f16\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7c7b\u578b\u7cfb\u7edf\uff0c\u7279\u522b\u9002\u5408AI\u8f85\u52a9\u7f16\u7a0b\u573a\u666f\u3002\u901a\u8fc7\u7c7b\u578b\u7cfb\u7edf\u7ea6\u675fAI\u751f\u6210\u7684\u4ee3\u7801\uff0c\u63d0\u9ad8\u4e86\u590d\u6742\u901a\u4fe1\u6a21\u5f0f\u7f16\u7a0b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2601.17707", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17707", "abs": "https://arxiv.org/abs/2601.17707", "authors": ["Mekala Kiran", "Apurba Das", "Suman Banerjee", "Tathagata Ray"], "title": "Multi-core & GPU-based Balanced Butterfly Counting in Signed Bipartite Graphs", "comment": null, "summary": "Balanced butterfly counting, corresponding to counting balanced (2, 2)-bicliques, is a fundamental primitive in the analysis of signed bipartite graphs and provides a basis for studying higher-order structural properties such as clustering coefficients and community structure. Although prior work has proposed an efficient CPU-based serial method for counting balanced (2, k)-bicliques. The computational cost of balanced butterfly counting remains a major bottleneck on large-scale graphs. In this work, we present the highly parallel implementations for balanced butterfly counting for both multicore CPUs and GPUs. The proposed multi-core algorithm (M-BBC) employs fine-grained vertex-level parallelism to accelerate wedge-based counting while eliminating the generation of unbalanced substructures. To improve scalability, we develop a GPU-based method (G-BBC) that uses a tile-based parallel approach to effectively leverage shared memory while handling large vertex sets. We then present an improved variation, G-BBC++, which integrates dynamic scheduling to mitigate workload imbalance and maximize throughput. We conduct an experimental assessment of the proposed methods across 15 real-world datasets. Experimental results exhibit that M-BBC achieves speedups of up to 71.13x (average 38.13x) over the sequential baseline BB2K. The GPU-based algorithms deliver even greater improvements, achieving up to 13,320x speedup (average 2,600x) over BB2K and outperforming M-BBC by up to 186x (average 50x). These results indicate the substantial scalability and efficiency of our parallel algorithms and establish a robust foundation for high-performance signed motif analysis on massive bipartite graphs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9488\u5bf9\u5e73\u8861\u8774\u8776\u8ba1\u6570\u7684\u5e76\u884c\u7b97\u6cd5\uff0c\u5305\u62ec\u591a\u6838CPU\u7684M-BBC\u548cGPU\u7684G-BBC/G-BBC++\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u5e73\u8861\u8774\u8776\u8ba1\u6570\u662f\u5206\u6790\u7b26\u53f7\u4e8c\u5206\u56fe\u7684\u57fa\u7840\u539f\u8bed\uff0c\u7528\u4e8e\u7814\u7a76\u9ad8\u9636\u7ed3\u6784\u7279\u6027\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u56fe\u4e0a\u7684\u8ba1\u7b97\u6210\u672c\u4ecd\u7136\u662f\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u6838CPU\u7b97\u6cd5M-BBC\uff08\u7ec6\u7c92\u5ea6\u9876\u70b9\u7ea7\u5e76\u884c\uff09\u548cGPU\u7b97\u6cd5G-BBC\uff08\u57fa\u4e8etile\u7684\u5e76\u884c\u65b9\u6cd5\uff09\uff0c\u4ee5\u53ca\u6539\u8fdb\u7248G-BBC++\uff08\u96c6\u6210\u52a8\u6001\u8c03\u5ea6\uff09\u3002", "result": "M-BBC\u76f8\u6bd4\u987a\u5e8f\u57fa\u7ebfBB2K\u6700\u9ad8\u52a0\u901f71.13\u500d\uff08\u5e73\u574738.13\u500d\uff09\uff1bGPU\u7b97\u6cd5\u6700\u9ad8\u52a0\u901f13,320\u500d\uff08\u5e73\u57472,600\u500d\uff09\uff0c\u6bd4M-BBC\u6700\u9ad8\u5feb186\u500d\uff08\u5e73\u574750\u500d\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u5e76\u884c\u7b97\u6cd5\u5177\u6709\u663e\u8457\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u4e8c\u5206\u56fe\u4e0a\u7684\u9ad8\u6027\u80fd\u7b26\u53f7\u6a21\u4f53\u5206\u6790\u5efa\u7acb\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2601.17940", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.17940", "abs": "https://arxiv.org/abs/2601.17940", "authors": ["Luca Colagrande", "Luca Benini"], "title": "Late Breaking Results: Boosting Efficient Dual-Issue Execution on Lightweight RISC-V Cores", "comment": "Accepted at DATE 2026", "summary": "Large-scale ML accelerators rely on large numbers of PEs, imposing strict bounds on the area and energy budget of each PE. Prior work demonstrates that limited dual-issue capabilities can be efficiently integrated into a lightweight in-order open-source RISC-V core (Snitch), with a geomean IPC boost of 1.6x and a geomean energy efficiency gain of 1.3x, obtained by concurrently executing integer and FP instructions. Unfortunately, this required a complex and error-prone low level programming model (COPIFT). We introduce COPIFTv2 which augments Snitch with lightweight queues enabling direct, fine-grained communication and synchronization between integer and FP threads. By eliminating the tiling and software pipelining steps of COPIFT, we can remove much of its complexity and software overheads. As a result, COPIFTv2 achieves up to a 1.49x speedup and a 1.47x energy-efficiency gain over COPIFT, and a peak IPC of 1.81. Overall, COPIFTv2 significantly enhances the efficiency and programmability of dual-issue execution on lightweight cores. Our implementation is fully open source and performance experiments are reproducible using free software.", "AI": {"tldr": "COPIFTv2\u5728Snitch RISC-V\u6838\u5fc3\u4e0a\u5f15\u5165\u8f7b\u91cf\u7ea7\u961f\u5217\uff0c\u6539\u8fdb\u4e86\u53cc\u53d1\u5c04\u6267\u884c\uff0c\u76f8\u6bd4COPIFT\u83b7\u5f971.49\u500d\u52a0\u901f\u548c1.47\u500d\u80fd\u6548\u63d0\u5347\uff0c\u7b80\u5316\u4e86\u7f16\u7a0b\u6a21\u578b\u3002", "motivation": "\u5927\u89c4\u6a21ML\u52a0\u901f\u5668\u9700\u8981\u5927\u91cfPE\uff0c\u6bcf\u4e2aPE\u7684\u9762\u79ef\u548c\u80fd\u8017\u9884\u7b97\u4e25\u683c\u3002\u73b0\u6709COPIFT\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u5347\u6027\u80fd\uff08IPC 1.6x\uff0c\u80fd\u65481.3x\uff09\uff0c\u4f46\u7f16\u7a0b\u6a21\u578b\u590d\u6742\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u9700\u8981\u7e41\u7410\u7684\u5e73\u94fa\u548c\u8f6f\u4ef6\u6d41\u6c34\u7ebf\u6b65\u9aa4\u3002", "method": "\u5728Snitch RISC-V\u6838\u5fc3\u4e0a\u5f15\u5165\u8f7b\u91cf\u7ea7\u961f\u5217\uff0c\u5b9e\u73b0\u6574\u6570\u548c\u6d6e\u70b9\u7ebf\u7a0b\u95f4\u7684\u76f4\u63a5\u7ec6\u7c92\u5ea6\u901a\u4fe1\u548c\u540c\u6b65\u3002\u6d88\u9664COPIFT\u4e2d\u7684\u5e73\u94fa\u548c\u8f6f\u4ef6\u6d41\u6c34\u7ebf\u6b65\u9aa4\uff0c\u7b80\u5316\u7f16\u7a0b\u6a21\u578b\u3002", "result": "\u76f8\u6bd4COPIFT\u83b7\u5f971.49\u500d\u52a0\u901f\u548c1.47\u500d\u80fd\u6548\u63d0\u5347\uff0c\u5cf0\u503cIPC\u8fbe\u52301.81\u3002\u663e\u8457\u63d0\u5347\u4e86\u8f7b\u91cf\u7ea7\u6838\u5fc3\u4e0a\u53cc\u53d1\u5c04\u6267\u884c\u7684\u6548\u7387\u548c\u53ef\u7f16\u7a0b\u6027\u3002", "conclusion": "COPIFTv2\u901a\u8fc7\u8f7b\u91cf\u7ea7\u961f\u5217\u673a\u5236\u663e\u8457\u6539\u8fdb\u4e86\u53cc\u53d1\u5c04\u6267\u884c\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u63d0\u5347\u7684\u540c\u65f6\u5927\u5927\u7b80\u5316\u4e86\u7f16\u7a0b\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5f00\u6e90\u5b9e\u73b0\u548c\u53ef\u590d\u73b0\u7684\u6027\u80fd\u5b9e\u9a8c\u3002"}}
{"id": "2601.18793", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.18793", "abs": "https://arxiv.org/abs/2601.18793", "authors": ["Michael Lee", "Ningning Xie", "Oleg Kiselyov", "Jeremy Yallop"], "title": "Handling Scope Checks (Extended Version)", "comment": "Extended version of Handling Scope Checks (POPL'26): includes appendices, fixes minor typos, and tweaks phrasing for readability", "summary": "Metaprogramming and effect handlers interact in unexpected, and sometimes undesirable, ways. One example is scope extrusion: the generation of ill-scoped code. Scope extrusion can either be preemptively prevented, via static type systems, or retroactively detected, via dynamic checks. Static type systems exist in theory, but struggle with a range of implementation and usability problems in practice. In contrast, dynamic checks exist in practice (e.g. in MetaOCaml), but are understudied in theory. Designers of metalanguages are thus given little guidance regarding the design and implementation of checks. We present the first formal study of dynamic scope extrusion checks, introducing a calculus ($\u03bb_{\\langle\\langle\\text{op}\\rangle\\rangle}$) for describing and evaluating checks. Further, we introduce a novel dynamic check $\\unicode{x2014}$ the \"Cause-for-Concern\" check $\\unicode{x2014}$ which we prove correct, characterise without reference to its implementation, and argue combines the advantages of existing dynamic checks. Finally, we extend our framework with refined environment classifiers, which statically prevent scope extrusion, and compare their expressivity with the dynamic checks.", "AI": {"tldr": "\u9996\u6b21\u5f62\u5f0f\u5316\u7814\u7a76\u52a8\u6001\u4f5c\u7528\u57df\u5916\u6cc4\u68c0\u67e5\uff0c\u63d0\u51fa\u03bb\u27e8\u27e8op\u27e9\u27e9\u6f14\u7b97\u63cf\u8ff0\u548c\u8bc4\u4f30\u68c0\u67e5\uff0c\u5f15\u5165\u65b0\u7684\"Cause-for-Concern\"\u52a8\u6001\u68c0\u67e5\u5e76\u8bc1\u660e\u5176\u6b63\u786e\u6027\uff0c\u540c\u65f6\u6269\u5c55\u6846\u67b6\u5305\u542b\u9759\u6001\u4f5c\u7528\u57df\u5206\u7c7b\u5668\u3002", "motivation": "\u5143\u7f16\u7a0b\u548c\u6548\u5e94\u5904\u7406\u5668\u7684\u4ea4\u4e92\u4f1a\u4ea7\u751f\u610f\u5916\u95ee\u9898\uff0c\u5982\u4f5c\u7528\u57df\u5916\u6cc4\u3002\u9759\u6001\u7c7b\u578b\u7cfb\u7edf\u5b58\u5728\u5b9e\u73b0\u548c\u53ef\u7528\u6027\u95ee\u9898\uff0c\u800c\u52a8\u6001\u68c0\u67e5\u867d\u5728\u5b9e\u8df5\u4e2d\u5b58\u5728\uff08\u5982MetaOCaml\uff09\u4f46\u7f3a\u4e4f\u7406\u8bba\u7814\u7a76\uff0c\u91d1\u5c5e\u8bed\u8a00\u8bbe\u8ba1\u8005\u7f3a\u4e4f\u8bbe\u8ba1\u548c\u5b9e\u73b0\u68c0\u67e5\u7684\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u03bb\u27e8\u27e8op\u27e9\u27e9\u6f14\u7b97\u5f62\u5f0f\u5316\u6846\u67b6\u6765\u63cf\u8ff0\u548c\u8bc4\u4f30\u52a8\u6001\u4f5c\u7528\u57df\u5916\u6cc4\u68c0\u67e5\uff1b\u5f15\u5165\u65b0\u7684\"Cause-for-Concern\"\u52a8\u6001\u68c0\u67e5\u65b9\u6cd5\uff1b\u6269\u5c55\u6846\u67b6\u5305\u542b\u7cbe\u5316\u7684\u73af\u5883\u5206\u7c7b\u5668\u7528\u4e8e\u9759\u6001\u9632\u6b62\u4f5c\u7528\u57df\u5916\u6cc4\u3002", "result": "\u8bc1\u660e\"Cause-for-Concern\"\u68c0\u67e5\u7684\u6b63\u786e\u6027\uff0c\u65e0\u9700\u53c2\u8003\u5176\u5b9e\u73b0\u5373\u53ef\u63cf\u8ff0\u5176\u7279\u5f81\uff0c\u8bba\u8bc1\u5176\u7ed3\u5408\u4e86\u73b0\u6709\u52a8\u6001\u68c0\u67e5\u7684\u4f18\u70b9\uff1b\u6bd4\u8f83\u52a8\u6001\u68c0\u67e5\u4e0e\u9759\u6001\u73af\u5883\u5206\u7c7b\u5668\u7684\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "\u9996\u6b21\u5bf9\u52a8\u6001\u4f5c\u7528\u57df\u5916\u6cc4\u68c0\u67e5\u8fdb\u884c\u5f62\u5f0f\u5316\u7814\u7a76\uff0c\u4e3a\u91d1\u5c5e\u8bed\u8a00\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u548c\u5b9e\u73b0\u68c0\u67e5\u7684\u7406\u8bba\u6307\u5bfc\uff0c\u5c55\u793a\u4e86\u52a8\u6001\u68c0\u67e5\u4e0e\u9759\u6001\u65b9\u6cd5\u7684\u4e92\u8865\u5173\u7cfb\u3002"}}
{"id": "2601.17754", "categories": ["cs.DC", "cs.ET", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.17754", "abs": "https://arxiv.org/abs/2601.17754", "authors": ["Nicolai Stawinoga", "David Katz", "Anton Lydike", "Justs Zarins", "Nick Brown", "George Bisbas", "Tobias Grosser"], "title": "An MLIR Lowering Pipeline for Stencils at Wafer-Scale", "comment": "Paper in ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS '26)", "summary": "The Cerebras Wafer-Scale Engine (WSE) delivers performance at an unprecedented scale of over 900,000 compute units, all connected via a single-wafer on-chip interconnect. Initially designed for AI, the WSE architecture is also well-suited for High Performance Computing (HPC). However, its distributed asynchronous programming model diverges significantly from the simple sequential or bulk-synchronous programs that one would typically derive for a given mathematical program description. Targeting the WSE requires a bespoke re-implementation when porting existing code. The absence of WSE support in compilers such as MLIR, meant that there was little hope for automating this process.\n  Stencils are ubiquitous in HPC, and in this paper we explore the hypothesis that domain specific information about stencils can be leveraged by the compiler to automatically target the WSE without requiring application-level code changes. We present a compiler pipeline that transforms stencil-based kernels into highly optimized CSL code for the WSE, bridging the semantic gap between the mathematical representation of the problem and the WSE's asynchronous execution model. Based upon five benchmarks across three HPC programming technologies, running on both the Cerebras WSE2 and WSE3, our approach delivers comparable, if not slightly better, performance than manually optimized code. Furthermore, without requiring any application level code changes, performance on the WSE3 is around 14 times faster than 128 Nvidia A100 GPUs and 20 times faster than 128 nodes of a CPU-based Cray-EX supercomputer when using our approach.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7f16\u8bd1\u5668\u6d41\u6c34\u7ebf\uff0c\u80fd\u591f\u81ea\u52a8\u5c06\u57fa\u4e8e\u6a21\u677f\u7684\u8ba1\u7b97\u5185\u6838\u8f6c\u6362\u4e3a\u9488\u5bf9Cerebras\u6676\u5706\u7ea7\u5f15\u64ce\u4f18\u5316\u7684\u4ee3\u7801\uff0c\u65e0\u9700\u4fee\u6539\u5e94\u7528\u5c42\u4ee3\u7801\uff0c\u5728WSE3\u4e0a\u6027\u80fd\u6bd4128\u4e2aNVIDIA A100 GPU\u5feb\u7ea614\u500d\u3002", "motivation": "Cerebras\u6676\u5706\u7ea7\u5f15\u64ce(WSE)\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u5176\u5206\u5e03\u5f0f\u5f02\u6b65\u7f16\u7a0b\u6a21\u578b\u4e0e\u4f20\u7edf\u987a\u5e8f\u6216\u6279\u91cf\u540c\u6b65\u7f16\u7a0b\u6a21\u578b\u5dee\u5f02\u5de8\u5927\uff0c\u79fb\u690d\u73b0\u6709\u4ee3\u7801\u9700\u8981\u5b9a\u5236\u5316\u91cd\u5b9e\u73b0\u3002\u7f3a\u4e4f\u7f16\u8bd1\u5668\u652f\u6301\u4f7f\u5f97\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\u51e0\u4e4e\u4e0d\u53ef\u80fd\uff0c\u800c\u6a21\u677f\u8ba1\u7b97\u5728HPC\u4e2d\u666e\u904d\u5b58\u5728\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7f16\u8bd1\u5668\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u6a21\u677f\u8ba1\u7b97\u7684\u9886\u57df\u7279\u5b9a\u4fe1\u606f\uff0c\u81ea\u52a8\u5c06\u57fa\u4e8e\u6a21\u677f\u7684\u5185\u6838\u8f6c\u6362\u4e3a\u9ad8\u5ea6\u4f18\u5316\u7684CSL\u4ee3\u7801\uff0c\u5f25\u5408\u95ee\u9898\u6570\u5b66\u8868\u793a\u4e0eWSE\u5f02\u6b65\u6267\u884c\u6a21\u578b\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u3002", "result": "\u57fa\u4e8e\u4e09\u4e2aHPC\u7f16\u7a0b\u6280\u672f\u7684\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728Cerebras WSE2\u548cWSE3\u4e0a\u8fd0\u884c\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e0e\u624b\u52a8\u4f18\u5316\u4ee3\u7801\u76f8\u5f53\u751a\u81f3\u7565\u4f18\u7684\u6027\u80fd\u3002\u65e0\u9700\u5e94\u7528\u5c42\u4ee3\u7801\u4fee\u6539\uff0cWSE3\u4e0a\u7684\u6027\u80fd\u6bd4128\u4e2aNVIDIA A100 GPU\u5feb\u7ea614\u500d\uff0c\u6bd4128\u4e2aCPU\u8282\u70b9\u7684Cray-EX\u8d85\u7ea7\u8ba1\u7b97\u673a\u5feb20\u500d\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u6a21\u677f\u8ba1\u7b97\u7684\u9886\u57df\u7279\u5b9a\u4fe1\u606f\uff0c\u7f16\u8bd1\u5668\u53ef\u4ee5\u81ea\u52a8\u4e3a\u76ee\u6807\u786c\u4ef6\u751f\u6210\u9ad8\u6548\u4ee3\u7801\uff0c\u65e0\u9700\u5e94\u7528\u5c42\u4ee3\u7801\u4fee\u6539\uff0c\u4e3a\u5728Cerebras WSE\u7b49\u65b0\u578b\u67b6\u6784\u4e0a\u90e8\u7f72HPC\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.18007", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.18007", "abs": "https://arxiv.org/abs/2601.18007", "authors": ["Duckgyu Shin", "Naoya Onizawa", "Warren J. Gross", "Takahiro Hanyu"], "title": "Memory-Efficient FPGA Implementation of Stochastic Simulated Annealing", "comment": "11 pages", "summary": "Simulated annealing (SA) is a well-known algorithm for solving combinatorial optimization problems. However, the computation time of SA increases rapidly, as the size of the problem grows. Recently, a stochastic simulated annealing (SSA) algorithm that converges faster than conventional SA has been reported. In this paper, we present a hardware-aware SSA (HA- SSA) algorithm for memory-efficient FPGA implementations. HA-SSA can reduce the memory usage of storing intermediate results while maintaining the computing speed of SSA. For evaluation purposes, the proposed algorithm is compared with the conventional SSA and SA approaches on maximum cut combinatorial optimization problems. HA-SSA achieves a convergence speed that is up to 114-times faster than that of the conventional SA algorithm depending on the maximum cut problem selected from the G-set which is a dataset of the maximum cut problems. HA-SSA is implemented on a field-programmable gate array (FPGA) (Xilinx Kintex-7), and it achieves up to 6-times the memory efficiency of conventional SSA while maintaining high solution quality for optimization problems.", "AI": {"tldr": "\u63d0\u51fa\u786c\u4ef6\u611f\u77e5\u7684\u968f\u673a\u6a21\u62df\u9000\u706b\u7b97\u6cd5\uff08HA-SSA\uff09\uff0c\u5728FPGA\u4e0a\u5b9e\u73b0\u5185\u5b58\u9ad8\u6548\u5b9e\u73b0\uff0c\u76f8\u6bd4\u4f20\u7edfSA\u52a0\u901f114\u500d\uff0c\u76f8\u6bd4SSA\u5185\u5b58\u6548\u7387\u63d0\u53476\u500d\u3002", "motivation": "\u6a21\u62df\u9000\u706b\uff08SA\uff09\u7b97\u6cd5\u5728\u89e3\u51b3\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u65f6\uff0c\u968f\u7740\u95ee\u9898\u89c4\u6a21\u589e\u5927\u8ba1\u7b97\u65f6\u95f4\u6025\u5267\u589e\u52a0\u3002\u867d\u7136\u968f\u673a\u6a21\u62df\u9000\u706b\uff08SSA\uff09\u6bd4\u4f20\u7edfSA\u6536\u655b\u66f4\u5feb\uff0c\u4f46\u5728FPGA\u5b9e\u73b0\u4e2d\u9700\u8981\u5b58\u50a8\u4e2d\u95f4\u7ed3\u679c\uff0c\u5185\u5b58\u4f7f\u7528\u6548\u7387\u4e0d\u9ad8\u3002", "method": "\u63d0\u51fa\u786c\u4ef6\u611f\u77e5\u7684\u968f\u673a\u6a21\u62df\u9000\u706b\u7b97\u6cd5\uff08HA-SSA\uff09\uff0c\u9488\u5bf9FPGA\u5b9e\u73b0\u8fdb\u884c\u4f18\u5316\uff0c\u51cf\u5c11\u5b58\u50a8\u4e2d\u95f4\u7ed3\u679c\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301SSA\u7684\u8ba1\u7b97\u901f\u5ea6\u3002\u5728\u6700\u5927\u5272\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4f7f\u7528G-set\u6570\u636e\u96c6\u3002", "result": "HA-SSA\u5728\u6700\u5927\u5272\u95ee\u9898\u4e0a\u6bd4\u4f20\u7edfSA\u7b97\u6cd5\u6536\u655b\u901f\u5ea6\u5feb114\u500d\uff08\u53d6\u51b3\u4e8e\u5177\u4f53\u95ee\u9898\uff09\u3002\u5728Xilinx Kintex-7 FPGA\u4e0a\u5b9e\u73b0\uff0c\u76f8\u6bd4\u4f20\u7edfSSA\u5185\u5b58\u6548\u7387\u63d0\u53476\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89e3\u8d28\u91cf\u3002", "conclusion": "HA-SSA\u7b97\u6cd5\u5728FPGA\u4e0a\u5b9e\u73b0\u4e86\u5185\u5b58\u9ad8\u6548\u7684\u6a21\u62df\u9000\u706b\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\uff0c\u4e3a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u786c\u4ef6\u52a0\u901f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.17774", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.17774", "abs": "https://arxiv.org/abs/2601.17774", "authors": ["Zizhao Zhang", "Yihan Xue", "Haotian Zhu", "Sijia Li", "Zhijun Wang", "Yujie Xiao"], "title": "CondenseGraph: Communication-Efficient Distributed GNN Training via On-the-Fly Graph Condensation", "comment": null, "summary": "Distributed Graph Neural Network (GNN) training suffers from substantial communication overhead due to the inherent neighborhood dependency in graph-structured data. This neighbor explosion problem requires workers to frequently exchange boundary node features across partitions, creating a communication bottleneck that severely limits training scalability. Existing approaches rely on static graph partitioning strategies that cannot adapt to dynamic network conditions. In this paper, we propose CondenseGraph, a novel communication-efficient framework for distributed GNN training. Our key innovation is an on-the-fly graph condensation mechanism that dynamically compresses boundary node features into compact super nodes before transmission. To compensate for the information loss introduced by compression, we develop a gradient-based error feedback mechanism that maintains convergence guarantees while reducing communication volume by 40-60%. Extensive experiments on four benchmark datasets demonstrate that CondenseGraph achieves comparable accuracy to full-precision baselines while significantly reducing communication costs and training time.", "AI": {"tldr": "CondenseGraph\uff1a\u4e00\u79cd\u901a\u8fc7\u52a8\u6001\u56fe\u538b\u7f29\u548c\u8bef\u5dee\u53cd\u9988\u673a\u5236\u51cf\u5c11\u5206\u5e03\u5f0fGNN\u8bad\u7ec3\u901a\u4fe1\u5f00\u9500\u7684\u9ad8\u6548\u6846\u67b6", "motivation": "\u5206\u5e03\u5f0f\u56fe\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5b58\u5728\u4e25\u91cd\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u7531\u4e8e\u56fe\u6570\u636e\u7684\u90bb\u57df\u4f9d\u8d56\u6027\uff0c\u9700\u8981\u9891\u7e41\u4ea4\u6362\u8fb9\u754c\u8282\u70b9\u7279\u5f81\uff0c\u73b0\u6709\u9759\u6001\u56fe\u5212\u5206\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u7f51\u7edc\u6761\u4ef6", "method": "\u63d0\u51faCondenseGraph\u6846\u67b6\uff0c\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a1) \u52a8\u6001\u56fe\u538b\u7f29\u673a\u5236\uff0c\u5c06\u8fb9\u754c\u8282\u70b9\u7279\u5f81\u538b\u7f29\u4e3a\u7d27\u51d1\u7684\u8d85\u8282\u70b9\u540e\u518d\u4f20\u8f93\uff1b2) \u57fa\u4e8e\u68af\u5ea6\u7684\u8bef\u5dee\u53cd\u9988\u673a\u5236\uff0c\u8865\u507f\u538b\u7f29\u5e26\u6765\u7684\u4fe1\u606f\u635f\u5931\u5e76\u4fdd\u8bc1\u6536\u655b\u6027", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCondenseGraph\u5728\u4fdd\u6301\u4e0e\u5168\u7cbe\u5ea6\u57fa\u7ebf\u76f8\u5f53\u51c6\u786e\u5ea6\u7684\u540c\u65f6\uff0c\u901a\u4fe1\u91cf\u51cf\u5c1140-60%\uff0c\u8bad\u7ec3\u65f6\u95f4\u663e\u8457\u964d\u4f4e", "conclusion": "CondenseGraph\u901a\u8fc7\u52a8\u6001\u7279\u5f81\u538b\u7f29\u548c\u8bef\u5dee\u53cd\u9988\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0fGNN\u8bad\u7ec3\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u901a\u4fe1\u6548\u7387\u4e0e\u6a21\u578b\u7cbe\u5ea6\u7684\u826f\u597d\u5e73\u8861"}}
{"id": "2601.18070", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.18070", "abs": "https://arxiv.org/abs/2601.18070", "authors": ["Jinwu Chen", "Yuhui Shi", "He Wang", "Zhe Jiang", "Jun Yang", "Xin Si", "Zhenhua Zhu"], "title": "CIM-Tuner: Balancing the Compute and Storage Capacity of SRAM-CIM Accelerator via Hardware-mapping Co-exploration", "comment": null, "summary": "As an emerging type of AI computing accelerator, SRAM Computing-In-Memory (CIM) accelerators feature high energy efficiency and throughput. However, various CIM designs and under-explored mapping strategies impede the full exploration of compute and storage balancing in SRAM-CIM accelerator, potentially leading to significant performance degradation. To address this issue, we propose CIM-Tuner, an automatic tool for hardware balancing and optimal mapping strategy under area constraint via hardware-mapping co-exploration. It ensures universality across various CIM designs through a matrix abstraction of CIM macros and a generalized accelerator template. For efficient mapping with different hardware configurations, it employs fine-grained two-level strategies comprising accelerator-level scheduling and macro-level tiling. Compared to prior CIM mapping, CIM-Tuner's extended strategy space achieves 1.58$\\times$ higher energy efficiency and 2.11$\\times$ higher throughput. Applied to SOTA CIM accelerators with identical area budget, CIM-Tuner also delivers comparable improvements. The simulation accuracy is silicon-verified and CIM-Tuner tool is open-sourced at https://github.com/champloo2878/CIM-Tuner.git.", "AI": {"tldr": "CIM-Tuner\u662f\u4e00\u4e2a\u81ea\u52a8\u5de5\u5177\uff0c\u901a\u8fc7\u786c\u4ef6-\u6620\u5c04\u534f\u540c\u63a2\u7d22\uff0c\u5728\u9762\u79ef\u7ea6\u675f\u4e0b\u5b9e\u73b0SRAM-CIM\u52a0\u901f\u5668\u7684\u786c\u4ef6\u5e73\u8861\u548c\u6700\u4f18\u6620\u5c04\u7b56\u7565", "motivation": "SRAM\u8ba1\u7b97\u5185\u5b58(CIM)\u52a0\u901f\u5668\u5177\u6709\u9ad8\u80fd\u6548\u548c\u9ad8\u541e\u5410\u91cf\uff0c\u4f46\u5404\u79cdCIM\u8bbe\u8ba1\u548c\u672a\u5145\u5206\u63a2\u7d22\u7684\u6620\u5c04\u7b56\u7565\u963b\u788d\u4e86\u8ba1\u7b97\u548c\u5b58\u50a8\u5e73\u8861\u7684\u5168\u9762\u63a2\u7d22\uff0c\u53ef\u80fd\u5bfc\u81f4\u663e\u8457\u7684\u6027\u80fd\u4e0b\u964d", "method": "\u901a\u8fc7CIM\u5b8f\u7684\u77e9\u9635\u62bd\u8c61\u548c\u901a\u7528\u52a0\u901f\u5668\u6a21\u677f\u786e\u4fdd\u8de8\u5404\u79cdCIM\u8bbe\u8ba1\u7684\u901a\u7528\u6027\uff1b\u91c7\u7528\u7ec6\u7c92\u5ea6\u4e24\u7ea7\u7b56\u7565\uff08\u52a0\u901f\u5668\u7ea7\u8c03\u5ea6\u548c\u5b8f\u7ea7\u5206\u5757\uff09\u5b9e\u73b0\u4e0d\u540c\u786c\u4ef6\u914d\u7f6e\u4e0b\u7684\u9ad8\u6548\u6620\u5c04", "result": "\u76f8\u6bd4\u5148\u524d\u7684CIM\u6620\u5c04\uff0cCIM-Tuner\u7684\u6269\u5c55\u7b56\u7565\u7a7a\u95f4\u5b9e\u73b0\u4e861.58\u500d\u66f4\u9ad8\u7684\u80fd\u6548\u548c2.11\u500d\u66f4\u9ad8\u7684\u541e\u5410\u91cf\uff1b\u5e94\u7528\u4e8e\u5177\u6709\u76f8\u540c\u9762\u79ef\u9884\u7b97\u7684SOTA CIM\u52a0\u901f\u5668\u65f6\u4e5f\u63d0\u4f9b\u4e86\u76f8\u5f53\u7684\u6539\u8fdb", "conclusion": "CIM-Tuner\u662f\u4e00\u4e2a\u6709\u6548\u7684\u81ea\u52a8\u5de5\u5177\uff0c\u901a\u8fc7\u786c\u4ef6-\u6620\u5c04\u534f\u540c\u63a2\u7d22\u89e3\u51b3\u4e86SRAM-CIM\u52a0\u901f\u5668\u7684\u786c\u4ef6\u5e73\u8861\u548c\u6620\u5c04\u4f18\u5316\u95ee\u9898\uff0c\u4eff\u771f\u7cbe\u5ea6\u7ecf\u8fc7\u7845\u9a8c\u8bc1\uff0c\u5de5\u5177\u5df2\u5f00\u6e90"}}
{"id": "2601.17855", "categories": ["cs.DC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.17855", "abs": "https://arxiv.org/abs/2601.17855", "authors": ["Zixi Chen", "Tianci Bu", "Chendong Song", "Xin Lu", "Yinyu Ye", "Zijie Zhou"], "title": "A Universal Load Balancing Principle and Its Application to Large Language Model Serving", "comment": null, "summary": "Load balancing-the allocation of work across parallel resources to reduce delay, energy and cost-is a pervasive challenge in science and engineering, from large-scale simulation and data processing to cloud and manufacturing operations. Motivated by the emerging bottleneck in large language model (LLM) serving, we study a particularly stringent regime of load balancing that arises in barrier-synchronized, stateful systems: work cannot be freely migrated and progress is gated by the slowest participant at each step, so heterogeneity and temporal drift in workloads create persistent stragglers and substantial idle time. LLM serving under data-parallel decoding provides a prominent modern instance: in production traces, barrier-induced idle can exceed 40% of compute time per decode step. Here we develop a universal load-balancing principle, which admits a step-wise finite-horizon integer-optimization formulation and yields worst-case guarantees: across LLM decode models and a broader class of non-decreasing workload drift processes, it reduces long-run imbalance by a factor that grows with batch size and system scale. Extensive experiments corroborate the theory, showing substantial improvements in throughput and latency together with reductions in energy consumption. These results provide a general, theoretically grounded framework for load balancing, with immediate implications for sustainable LLM serving and broad relevance to other synchronization-gated resource-allocation problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u8d1f\u8f7d\u5747\u8861\u539f\u5219\uff0c\u9488\u5bf9\u5c4f\u969c\u540c\u6b65\u7cfb\u7edf\u4e2d\u7684\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u6f02\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u9010\u6b65\u6709\u9650\u65f6\u57df\u6574\u6570\u4f18\u5316\u51cf\u5c11\u7a7a\u95f2\u65f6\u95f4\uff0c\u5728LLM\u670d\u52a1\u7b49\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u670d\u52a1\u4e2d\u51fa\u73b0\u7684\u5c4f\u969c\u540c\u6b65\u3001\u6709\u72b6\u6001\u7cfb\u7edf\u8d1f\u8f7d\u5747\u8861\u74f6\u9888\uff1a\u5de5\u4f5c\u65e0\u6cd5\u81ea\u7531\u8fc1\u79fb\uff0c\u8fdb\u5ea6\u53d7\u6700\u6162\u53c2\u4e0e\u8005\u9650\u5236\uff0c\u5f02\u6784\u6027\u548c\u5de5\u4f5c\u8d1f\u8f7d\u7684\u65f6\u95f4\u6f02\u79fb\u5bfc\u81f4\u6301\u7eed\u843d\u540e\u8005\u548c\u5927\u91cf\u7a7a\u95f2\u65f6\u95f4\u3002\u751f\u4ea7\u8ddf\u8e2a\u663e\u793a\u5c4f\u969c\u5f15\u8d77\u7684\u7a7a\u95f2\u65f6\u95f4\u53ef\u8d85\u8fc7\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u8ba1\u7b97\u65f6\u95f4\u768440%\u3002", "method": "\u5f00\u53d1\u4e86\u901a\u7528\u8d1f\u8f7d\u5747\u8861\u539f\u5219\uff0c\u91c7\u7528\u9010\u6b65\u6709\u9650\u65f6\u57df\u6574\u6570\u4f18\u5316\u516c\u5f0f\uff0c\u4e3aLLM\u89e3\u7801\u6a21\u578b\u548c\u66f4\u5e7f\u6cdb\u7684\u975e\u9012\u51cf\u5de5\u4f5c\u8d1f\u8f7d\u6f02\u79fb\u8fc7\u7a0b\u63d0\u4f9b\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\uff0c\u51cf\u5c11\u957f\u671f\u4e0d\u5e73\u8861\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u51cf\u5c11\u957f\u671f\u4e0d\u5e73\u8861\uff0c\u51cf\u5c11\u56e0\u5b50\u968f\u6279\u5904\u7406\u5927\u5c0f\u548c\u7cfb\u7edf\u89c4\u6a21\u589e\u957f\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u7406\u8bba\uff0c\u663e\u793a\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u663e\u8457\u6539\u5584\uff0c\u540c\u65f6\u80fd\u8017\u964d\u4f4e\u3002", "conclusion": "\u4e3a\u8d1f\u8f7d\u5747\u8861\u63d0\u4f9b\u4e86\u901a\u7528\u3001\u7406\u8bba\u57fa\u7840\u7684\u6846\u67b6\uff0c\u5bf9\u53ef\u6301\u7eedLLM\u670d\u52a1\u6709\u76f4\u63a5\u610f\u4e49\uff0c\u5e76\u5bf9\u5176\u4ed6\u540c\u6b65\u95e8\u63a7\u8d44\u6e90\u5206\u914d\u95ee\u9898\u5177\u6709\u5e7f\u6cdb\u76f8\u5173\u6027\u3002"}}
{"id": "2601.18140", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.18140", "abs": "https://arxiv.org/abs/2601.18140", "authors": ["Yan Zhu", "Boru Chen", "Christopher W. Fletcher", "Nandeeka Nayak"], "title": "RTeAAL Sim: Using Tensor Algebra to Represent and Accelerate RTL Simulation (Extended Version)", "comment": null, "summary": "RTL simulation on CPUs remains a persistent bottleneck in hardware design. State-of-the-art simulators embed the circuit directly into the simulation binary, resulting in long compilation times and execution that is fundamentally CPU frontend-bound, with severe instruction-cache pressure.\n  This work proposes RTeAAL Sim, which reformulates RTL simulation as a sparse tensor algebra problem. By representing RTL circuits as tensors and simulation as a sparse tensor algebra kernel, RTeAAL Sim decouples simulation behavior from binary size and makes RTL simulation amenable to well-studied tensor algebra optimizations. We demonstrate that a prototype of our tensor-based simulator, even with a subset of these optimizations, already mitigates the compilation overhead and frontend pressure and achieves performance competitive with the highly optimized Verilator simulator across multiple CPUs and ISAs.", "AI": {"tldr": "RTeAAL Sim\u5c06RTL\u4eff\u771f\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7a00\u758f\u5f20\u91cf\u4ee3\u6570\u95ee\u9898\uff0c\u901a\u8fc7\u5f20\u91cf\u8868\u793a\u7535\u8def\u548c\u4eff\u771f\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4eff\u771f\u5668\u7684\u7f16\u8bd1\u65f6\u95f4\u957f\u548cCPU\u524d\u7aef\u74f6\u9888\u95ee\u9898\uff0c\u6027\u80fd\u53ef\u4e0e\u9ad8\u5ea6\u4f18\u5316\u7684Verilator\u7ade\u4e89\u3002", "motivation": "\u4f20\u7edfRTL\u4eff\u771f\u5668\u5c06\u7535\u8def\u76f4\u63a5\u5d4c\u5165\u4eff\u771f\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u5bfc\u81f4\u7f16\u8bd1\u65f6\u95f4\u957f\uff0c\u6267\u884c\u65f6\u53d7CPU\u524d\u7aef\u9650\u5236\u4e25\u91cd\uff0c\u5b58\u5728\u6307\u4ee4\u7f13\u5b58\u538b\u529b\u5927\u7684\u95ee\u9898\u3002", "method": "\u5c06RTL\u4eff\u771f\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7a00\u758f\u5f20\u91cf\u4ee3\u6570\u95ee\u9898\uff1a\u5c06RTL\u7535\u8def\u8868\u793a\u4e3a\u5f20\u91cf\uff0c\u4eff\u771f\u8fc7\u7a0b\u4f5c\u4e3a\u7a00\u758f\u5f20\u91cf\u4ee3\u6570\u5185\u6838\uff0c\u4f7f\u4eff\u771f\u884c\u4e3a\u4e0e\u4e8c\u8fdb\u5236\u5927\u5c0f\u89e3\u8026\uff0c\u5e76\u5e94\u7528\u6210\u719f\u7684\u5f20\u91cf\u4ee3\u6570\u4f18\u5316\u6280\u672f\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u5373\u4f7f\u53ea\u5e94\u7528\u90e8\u5206\u4f18\u5316\uff0c\u5df2\u80fd\u7f13\u89e3\u7f16\u8bd1\u5f00\u9500\u548c\u524d\u7aef\u538b\u529b\uff0c\u5728\u4e0d\u540cCPU\u548cISA\u4e0a\u6027\u80fd\u4e0e\u9ad8\u5ea6\u4f18\u5316\u7684Verilator\u4eff\u771f\u5668\u76f8\u5f53\u3002", "conclusion": "\u5f20\u91cf\u4ee3\u6570\u65b9\u6cd5\u4e3aRTL\u4eff\u771f\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u7f16\u8bd1\u548c\u6027\u80fd\u74f6\u9888\uff0c\u5c55\u73b0\u4e86\u5728\u786c\u4ef6\u4eff\u771f\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.18158", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.18158", "abs": "https://arxiv.org/abs/2601.18158", "authors": ["Karame Mohammadiporshokooh", "Panagiotis Syskakis", "Hartmut Kaiser"], "title": "An Initial Evaluation of Distributed Graph Algorithms using NWGraph and HPX", "comment": "Initial technical report. Extended version of work under submission", "summary": "Graphs are central to modeling relationships in scientific computing, data analysis, and AI/ML, but their growing scale can exceed the memory and compute capacity of single nodes, requiring distributed solutions. Existing distributed graph framework, however, face fundamental challenges: graph algorithms are latency-bound, suffer from irregular memory access, and often impose synchronization costs that limit scalability and efficiency. In this work, we present a distributed implementation of the NWGraph library integrated with the HPX runtime system. By leveraging HPX's asynchronous many-task model, our approach aims to reduce synchronization overhead, improve load balance, and provide a foundation for distributed graph analytics. We evaluate this approach using two representative algorithms: Breadth-First-Search (BFS) and (PageRank). Our initial results show that BFS achieves better performance than the distributed Boost Graph Library (BGL), while PageRank remains more challenging, with current implementation not yet outperforming BGL. These findings highlight both the promise and the open challenges of applying asynchronous task-based runtimes to graph processing, and point to opportunities for future optimizations and extensions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06NWGraph\u5e93\u4e0eHPX\u8fd0\u884c\u65f6\u7cfb\u7edf\u7ed3\u5408\u7684\u5206\u5e03\u5f0f\u56fe\u5904\u7406\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u5f02\u6b65\u591a\u4efb\u52a1\u6a21\u578b\u51cf\u5c11\u540c\u6b65\u5f00\u9500\u3001\u6539\u5584\u8d1f\u8f7d\u5747\u8861\uff0c\u4e3a\u5206\u5e03\u5f0f\u56fe\u5206\u6790\u63d0\u4f9b\u57fa\u7840\u3002\u5b9e\u9a8c\u663e\u793aBFS\u6027\u80fd\u4f18\u4e8e\u5206\u5e03\u5f0fBGL\uff0c\u4f46PageRank\u5c1a\u672a\u8d85\u8d8aBGL\u3002", "motivation": "\u968f\u7740\u56fe\u6570\u636e\u89c4\u6a21\u4e0d\u65ad\u589e\u957f\uff0c\u5355\u8282\u70b9\u5185\u5b58\u548c\u8ba1\u7b97\u80fd\u529b\u5df2\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u9700\u8981\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\u3002\u73b0\u6709\u5206\u5e03\u5f0f\u56fe\u6846\u67b6\u9762\u4e34\u5ef6\u8fdf\u53d7\u9650\u3001\u5185\u5b58\u8bbf\u95ee\u4e0d\u89c4\u5219\u3001\u540c\u6b65\u6210\u672c\u9ad8\u7b49\u6311\u6218\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002", "method": "\u5c06NWGraph\u5e93\u4e0eHPX\u8fd0\u884c\u65f6\u7cfb\u7edf\u96c6\u6210\uff0c\u5229\u7528HPX\u7684\u5f02\u6b65\u591a\u4efb\u52a1\u6a21\u578b\u6765\u51cf\u5c11\u540c\u6b65\u5f00\u9500\u3001\u6539\u5584\u8d1f\u8f7d\u5747\u8861\uff0c\u4e3a\u5206\u5e03\u5f0f\u56fe\u5206\u6790\u63d0\u4f9b\u57fa\u7840\u6846\u67b6\u3002\u4f7f\u7528BFS\u548cPageRank\u4e24\u79cd\u4ee3\u8868\u6027\u7b97\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "BFS\u7b97\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5206\u5e03\u5f0fBoost Graph Library (BGL)\uff0c\u4f46PageRank\u7b97\u6cd5\u76ee\u524d\u5c1a\u672a\u8d85\u8d8aBGL\uff0c\u663e\u793a\u51fa\u5f02\u6b65\u4efb\u52a1\u8fd0\u884c\u65f6\u5728\u56fe\u5904\u7406\u4e2d\u7684\u6f5c\u529b\u548c\u6311\u6218\u3002", "conclusion": "\u5f02\u6b65\u4efb\u52a1\u8fd0\u884c\u65f6\u5728\u56fe\u5904\u7406\u4e2d\u5177\u6709\u5e94\u7528\u524d\u666f\uff0c\u4f46\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\u3002BFS\u7684\u6210\u529f\u548cPageRank\u7684\u6311\u6218\u8868\u660e\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u6269\u5c55\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2601.18159", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.18159", "abs": "https://arxiv.org/abs/2601.18159", "authors": ["Zizhen Liu", "Fangzhiyi Wang", "Mengdi Wang", "Jing Ye", "Hayden Kwok-Hay So", "Cheng Liu", "Huawei Li"], "title": "Lifecycle Cost-Effectiveness Modeling for Redundancy-Enhanced Multi-Chiplet Architectures", "comment": null, "summary": "The growing demand for compute-intensive applications has made multi-chiplet architectures a promising alternative to monolithic designs, offering improved scalability and manufacturing flexibility. However, effectively managing the economic effectiveness remains challenging. Existing cost models either overlook the amortization of compute value over a chip's operational lifetime or fail to evaluate how redundancy strategies, which are widely adopted to enhance yield and fault tolerance, impact long-term cost efficiency. This paper presents a comprehensive cost-effectiveness framework for multi-chiplet architectures, introducing a novel Lifecycle Cost Effectiveness (LCE) metric that evaluates amortized compute costs by jointly optimizing manufacturing expenses and operational lifetime. Our approach uniquely integrates: (1) redundancy-aware cost modeling spanning both intra- and inter-chiplet levels, (2) reliability-driven lifetime estimation, and (3) quantitative analysis of how redundancy configurations on overall economic effectiveness. Extensive trade-off and multi-objective optimization studies demonstrate the effectiveness of the model and reveal essential co-optimization strategies between module and chiplet-level redundancy to achieve cost-efficient multi-chiplet architecture designs.", "AI": {"tldr": "\u63d0\u51fa\u591a\u82af\u7247\u67b6\u6784\u751f\u547d\u5468\u671f\u6210\u672c\u6548\u76ca\u6846\u67b6\uff0c\u5f15\u5165LCE\u6307\u6807\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5236\u9020\u6210\u672c\u548c\u8fd0\u884c\u5bff\u547d\u6765\u8bc4\u4f30\u644a\u9500\u8ba1\u7b97\u6210\u672c\uff0c\u63ed\u793a\u6a21\u5757\u4e0e\u82af\u7247\u7ea7\u5197\u4f59\u534f\u540c\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u8ba1\u7b97\u5bc6\u96c6\u578b\u5e94\u7528\u9700\u6c42\u589e\u957f\u4f7f\u591a\u82af\u7247\u67b6\u6784\u6210\u4e3a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u6210\u672c\u6a21\u578b\u8981\u4e48\u5ffd\u7565\u82af\u7247\u8fd0\u884c\u5bff\u547d\u5185\u7684\u8ba1\u7b97\u4ef7\u503c\u644a\u9500\uff0c\u8981\u4e48\u672a\u80fd\u8bc4\u4f30\u5197\u4f59\u7b56\u7565\u5bf9\u957f\u671f\u6210\u672c\u6548\u76ca\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u5168\u9762\u7684\u6210\u672c\u6548\u76ca\u6846\u67b6\uff0c\u5f15\u5165\u751f\u547d\u5468\u671f\u6210\u672c\u6548\u76ca(LCE)\u6307\u6807\uff0c\u96c6\u6210\uff1a(1)\u8de8\u82af\u7247\u5185\u548c\u82af\u7247\u95f4\u7ea7\u522b\u7684\u5197\u4f59\u611f\u77e5\u6210\u672c\u5efa\u6a21\uff0c(2)\u53ef\u9760\u6027\u9a71\u52a8\u7684\u5bff\u547d\u4f30\u8ba1\uff0c(3)\u5197\u4f59\u914d\u7f6e\u5bf9\u6574\u4f53\u7ecf\u6d4e\u6548\u76ca\u7684\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u5e7f\u6cdb\u7684\u6743\u8861\u7814\u7a76\u548c\u591a\u76ee\u6807\u4f18\u5316\u8bc1\u660e\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u6a21\u5757\u7ea7\u548c\u82af\u7247\u7ea7\u5197\u4f59\u4e4b\u95f4\u7684\u5fc5\u8981\u534f\u540c\u4f18\u5316\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u9ad8\u7684\u591a\u82af\u7247\u67b6\u6784\u8bbe\u8ba1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u82af\u7247\u67b6\u6784\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6210\u672c\u6548\u76ca\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5236\u9020\u6210\u672c\u548c\u8fd0\u884c\u5bff\u547d\uff0c\u4ee5\u53ca\u5197\u4f59\u914d\u7f6e\u7684\u534f\u540c\u4f18\u5316\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u7ecf\u6d4e\u6709\u6548\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2601.18400", "categories": ["cs.DC", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.18400", "abs": "https://arxiv.org/abs/2601.18400", "authors": ["Andrei Lebedev", "Vincent Gramoli"], "title": "On the Bandwidth Consumption of Blockchains", "comment": "11 pages, 6 figures", "summary": "With the advent of blockchain technology, the number of proposals has boomed. The network traffic imposed by these blockchain proposals increases the cost of hosting nodes. Unfortunately, as of today, we are not aware of any comparative study of the bandwidth consumption of blockchains.\n  In this paper, we propose the first empirical comparison of blockchain bandwidth consumption. To this end, we measure the network traffic of blockchain network nodes of five blockchain protocols: Algorand, Aptos, Avalanche, Redbelly and Solana. We study the variation over time, differentiate the receiving and sending traffic and analyze how this traffic varies with the number of nodes and validators.\n  We conclude that the transport protocol is the main factor impacting the network traffic, segregating node roles helps reduce traffic and different blockchains are differently impacted by the network size.", "AI": {"tldr": "\u9996\u6b21\u5bf9\u4e94\u79cd\u533a\u5757\u94fe\u534f\u8bae\uff08Algorand\u3001Aptos\u3001Avalanche\u3001Redbelly\u3001Solana\uff09\u8fdb\u884c\u5e26\u5bbd\u6d88\u8017\u7684\u5b9e\u8bc1\u6bd4\u8f83\uff0c\u53d1\u73b0\u4f20\u8f93\u534f\u8bae\u662f\u5f71\u54cd\u7f51\u7edc\u6d41\u91cf\u7684\u4e3b\u8981\u56e0\u7d20", "motivation": "\u533a\u5757\u94fe\u63d0\u6848\u6570\u91cf\u6fc0\u589e\u5bfc\u81f4\u8282\u70b9\u6258\u7ba1\u6210\u672c\u589e\u52a0\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u533a\u5757\u94fe\u5e26\u5bbd\u6d88\u8017\u7684\u6bd4\u8f83\u7814\u7a76\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d", "method": "\u6d4b\u91cf\u4e94\u79cd\u533a\u5757\u94fe\u534f\u8bae\u7f51\u7edc\u8282\u70b9\u7684\u7f51\u7edc\u6d41\u91cf\uff0c\u7814\u7a76\u65f6\u95f4\u53d8\u5316\u3001\u63a5\u6536\u4e0e\u53d1\u9001\u6d41\u91cf\u5dee\u5f02\uff0c\u5206\u6790\u6d41\u91cf\u5982\u4f55\u968f\u8282\u70b9\u548c\u9a8c\u8bc1\u8005\u6570\u91cf\u53d8\u5316", "result": "\u4f20\u8f93\u534f\u8bae\u662f\u5f71\u54cd\u7f51\u7edc\u6d41\u91cf\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u8282\u70b9\u89d2\u8272\u5206\u79bb\u6709\u52a9\u4e8e\u51cf\u5c11\u6d41\u91cf\uff0c\u4e0d\u540c\u533a\u5757\u94fe\u53d7\u7f51\u7edc\u89c4\u6a21\u5f71\u54cd\u7a0b\u5ea6\u4e0d\u540c", "conclusion": "\u9996\u6b21\u63d0\u4f9b\u4e86\u533a\u5757\u94fe\u5e26\u5bbd\u6d88\u8017\u7684\u5b9e\u8bc1\u6bd4\u8f83\uff0c\u4e3a\u533a\u5757\u94fe\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u7279\u522b\u662f\u4f20\u8f93\u534f\u8bae\u9009\u62e9\u548c\u8282\u70b9\u89d2\u8272\u5206\u79bb\u7684\u91cd\u8981\u6027"}}
