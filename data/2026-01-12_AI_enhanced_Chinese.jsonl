{"id": "2601.05972", "categories": ["cs.PL", "math.CT"], "pdf": "https://arxiv.org/pdf/2601.05972", "abs": "https://arxiv.org/abs/2601.05972", "authors": ["Jack Carlisle", "Jay Shah", "Reuben Stern", "Paul VanKoughnett"], "title": "Categorical Foundations for CuTe Layouts", "comment": "174 pages", "summary": "NVIDIA's CUTLASS library provides a robust and expressive set of methods for describing and manipulating multi-dimensional tensor data on the GPU. These methods are conceptually grounded in the abstract notion of a CuTe layout and a rich algebra of such layouts, including operations such as composition, logical product, and logical division. In this paper, we present a categorical framework for understanding this layout algebra by focusing on a naturally occurring class of tractable layouts. To this end, we define two categories Tuple and Nest whose morphisms give rise to layouts. We define a suite of operations on morphisms in these categories and prove their compatibility with the corresponding layout operations. Moreover, we give a complete characterization of the layouts which arise from our construction. Finally, we provide a Python implementation of our categorical constructions, along with tests that demonstrate alignment with CUTLASS behavior. This implementation can be found at our git repository https://github.com/ColfaxResearch/layout-categories.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3aNVIDIA CUTLASS\u5e93\u7684\u5e03\u5c40\u4ee3\u6570\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8303\u7574\u8bba\u6846\u67b6\uff0c\u5b9a\u4e49\u4e86Tuple\u548cNest\u4e24\u4e2a\u8303\u7574\uff0c\u5176\u6001\u5c04\u4ea7\u751f\u5e03\u5c40\uff0c\u5e76\u5b9e\u73b0\u4e86Python\u9a8c\u8bc1\u5b9e\u73b0\u3002", "motivation": "CUTLASS\u5e93\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u591a\u7ef4\u5f20\u91cf\u5e03\u5c40\u64cd\u4f5c\uff0c\u4f46\u5176\u5e95\u5c42\u4ee3\u6570\u7f3a\u4e4f\u5f62\u5f0f\u5316\u7406\u8bba\u57fa\u7840\u3002\u8bba\u6587\u65e8\u5728\u4e3a\u8fd9\u4e9b\u5e03\u5c40\u64cd\u4f5c\u5efa\u7acb\u4e25\u683c\u7684\u8303\u7574\u8bba\u6846\u67b6\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u5f62\u5f0f\u5316\u5e03\u5c40\u4ee3\u6570\u3002", "method": "\u5b9a\u4e49\u4e86\u4e24\u4e2a\u8303\u7574Tuple\u548cNest\uff0c\u5176\u6001\u5c04\u4ea7\u751f\u5e03\u5c40\u3002\u5728\u8fd9\u4e9b\u8303\u7574\u7684\u6001\u5c04\u4e0a\u5b9a\u4e49\u4e86\u4e00\u5957\u64cd\u4f5c\uff0c\u5e76\u8bc1\u660e\u8fd9\u4e9b\u64cd\u4f5c\u4e0eCUTLASS\u4e2d\u76f8\u5e94\u5e03\u5c40\u64cd\u4f5c\u7684\u517c\u5bb9\u6027\u3002\u5b8c\u5168\u523b\u753b\u4e86\u4ece\u8be5\u6784\u9020\u4ea7\u751f\u7684\u5e03\u5c40\u7c7b\u578b\u3002", "result": "\u5efa\u7acb\u4e86CUTLASS\u5e03\u5c40\u4ee3\u6570\u7684\u8303\u7574\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u8303\u7574\u64cd\u4f5c\u4e0e\u5e03\u5c40\u64cd\u4f5c\u7684\u517c\u5bb9\u6027\uff0c\u5b8c\u6574\u63cf\u8ff0\u4e86\u53ef\u751f\u6210\u7684\u5e03\u5c40\u7c7b\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86Python\u5b9e\u73b0\u9a8c\u8bc1\u4e0eCUTLASS\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u8303\u7574\u8bba\u6846\u67b6\u4e3aCUTLASS\u5e03\u5c40\u4ee3\u6570\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u3001\u5206\u6790\u548c\u4f18\u5316GPU\u5f20\u91cf\u5e03\u5c40\uff0c\u5b9e\u73b0\u4ee3\u7801\u53ef\u5728GitHub\u4ed3\u5e93\u83b7\u53d6\u3002"}}
{"id": "2601.05569", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.05569", "abs": "https://arxiv.org/abs/2601.05569", "authors": ["Zixuan Li", "Chuanzhen Wang", "Haotian Sun"], "title": "Self-Evolving Distributed Memory Architecture for Scalable AI Systems", "comment": "21 pages", "summary": "Distributed AI systems face critical memory management challenges across computation, communication, and deployment layers. RRAM based in memory computing suffers from scalability limitations due to device non idealities and fixed array sizes. Decentralized AI frameworks struggle with memory efficiency across NAT constrained networks due to static routing that ignores computational load. Multi agent deployment systems tightly couple application logic with execution environments, preventing adaptive memory optimization. These challenges stem from a fundamental lack of coordinated memory management across architectural layers. We introduce Self Evolving Distributed Memory Architecture for Scalable AI Systems, a three layer framework that unifies memory management across computation, communication, and deployment. Our approach features (1) memory guided matrix processing with dynamic partitioning based on device characteristics, (2) memory aware peer selection considering network topology and computational capacity, and (3) runtime adaptive deployment optimization through continuous reconfiguration. The framework maintains dual memory systems tracking both long term performance patterns and short term workload statistics. Experiments on COCO 2017, ImageNet, and SQuAD show that our method achieves 87.3 percent memory utilization efficiency and 142.5 operations per second compared to Ray Distributed at 72.1 percent and 98.7 operations per second, while reducing communication latency by 30.2 percent to 171.2 milliseconds and improving resource utilization to 82.7 percent. Our contributions include coordinated memory management across three architectural layers, workload adaptive resource allocation, and a dual memory architecture enabling dynamic system optimization.", "AI": {"tldr": "\u63d0\u51faSelf-Evolving Distributed Memory Architecture (SEDMA)\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u5c42\u7edf\u4e00\u5185\u5b58\u7ba1\u7406\u89e3\u51b3\u5206\u5e03\u5f0fAI\u7cfb\u7edf\u7684\u5185\u5b58\u6548\u7387\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u5185\u5b58\u5229\u7528\u7387\u3001\u8ba1\u7b97\u901f\u5ea6\u548c\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u5206\u5e03\u5f0fAI\u7cfb\u7edf\u9762\u4e34\u8de8\u8ba1\u7b97\u3001\u901a\u4fe1\u548c\u90e8\u7f72\u5c42\u7684\u4e25\u91cd\u5185\u5b58\u7ba1\u7406\u6311\u6218\uff1aRRAM\u5185\u5b58\u8ba1\u7b97\u53d7\u9650\u4e8e\u8bbe\u5907\u975e\u7406\u60f3\u6027\u548c\u56fa\u5b9a\u9635\u5217\u5927\u5c0f\uff1b\u53bb\u4e2d\u5fc3\u5316AI\u6846\u67b6\u5728NAT\u7ea6\u675f\u7f51\u7edc\u4e2d\u56e0\u9759\u6001\u8def\u7531\u800c\u5185\u5b58\u6548\u7387\u4f4e\u4e0b\uff1b\u591a\u667a\u80fd\u4f53\u90e8\u7f72\u7cfb\u7edf\u5c06\u5e94\u7528\u903b\u8f91\u4e0e\u6267\u884c\u73af\u5883\u7d27\u5bc6\u8026\u5408\uff0c\u963b\u788d\u81ea\u9002\u5e94\u5185\u5b58\u4f18\u5316\u3002\u8fd9\u4e9b\u95ee\u9898\u7684\u6839\u6e90\u5728\u4e8e\u7f3a\u4e4f\u8de8\u67b6\u6784\u5c42\u7684\u534f\u8c03\u5185\u5b58\u7ba1\u7406\u3002", "method": "\u63d0\u51fa\u4e09\u5c42\u6846\u67b6SEDMA\uff1a1) \u57fa\u4e8e\u8bbe\u5907\u7279\u6027\u7684\u5185\u5b58\u5f15\u5bfc\u77e9\u9635\u5904\u7406\u4e0e\u52a8\u6001\u5206\u533a\uff1b2) \u8003\u8651\u7f51\u7edc\u62d3\u6251\u548c\u8ba1\u7b97\u5bb9\u91cf\u7684\u5185\u5b58\u611f\u77e5\u5bf9\u7b49\u8282\u70b9\u9009\u62e9\uff1b3) \u901a\u8fc7\u6301\u7eed\u91cd\u914d\u7f6e\u5b9e\u73b0\u8fd0\u884c\u65f6\u81ea\u9002\u5e94\u90e8\u7f72\u4f18\u5316\u3002\u6846\u67b6\u7ef4\u62a4\u53cc\u5185\u5b58\u7cfb\u7edf\uff0c\u540c\u65f6\u8ddf\u8e2a\u957f\u671f\u6027\u80fd\u6a21\u5f0f\u548c\u77ed\u671f\u5de5\u4f5c\u8d1f\u8f7d\u7edf\u8ba1\u3002", "result": "\u5728COCO 2017\u3001ImageNet\u548cSQuAD\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u523087.3%\u7684\u5185\u5b58\u5229\u7528\u6548\u7387\u548c142.5 ops/s\uff0c\u4f18\u4e8eRay Distributed\u768472.1%\u548c98.7 ops/s\uff1b\u901a\u4fe1\u5ef6\u8fdf\u964d\u4f4e30.2%\u81f3171.2\u6beb\u79d2\uff0c\u8d44\u6e90\u5229\u7528\u7387\u63d0\u5347\u81f382.7%\u3002", "conclusion": "\u8d21\u732e\u5305\u62ec\uff1a\u8de8\u4e09\u5c42\u67b6\u6784\u7684\u534f\u8c03\u5185\u5b58\u7ba1\u7406\u3001\u5de5\u4f5c\u8d1f\u8f7d\u81ea\u9002\u5e94\u8d44\u6e90\u5206\u914d\u3001\u4ee5\u53ca\u652f\u6301\u52a8\u6001\u7cfb\u7edf\u4f18\u5316\u7684\u53cc\u5185\u5b58\u67b6\u6784\u3002\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5206\u5e03\u5f0fAI\u7cfb\u7edf\u7684\u5185\u5b58\u6548\u7387\u548c\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2601.05668", "categories": ["cs.AR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.05668", "abs": "https://arxiv.org/abs/2601.05668", "authors": ["Ram\u00f3n Beivide", "Crist\u00f3bal Camarero", "Carmen Mart\u00ednez", "Enrique Vallejo", "Mateo Valero"], "title": "LACIN: Linearly Arranged Complete Interconnection Networks", "comment": "5 pages, 4 figures", "summary": "Several interconnection networks are based on the complete graph topology. Networks with a moderate size can be based on a single complete graph. However, large-scale networks such as Dragonfly and HyperX use, respectively, a hierarchical or a multi-dimensional composition of complete graphs.\n  The number of links in these networks is huge and grows rapidly with their size. This paper introduces LACIN, a set of complete graph implementations that use identically indexed ports to link switches. This way of implementing the network reduces the complexity of its cabling and its routing. LACIN eases the deployment of networks for parallel computers of different scales, from VLSI systems to the largest supercomputers.", "AI": {"tldr": "LACIN\u662f\u4e00\u79cd\u4f7f\u7528\u76f8\u540c\u7d22\u5f15\u7aef\u53e3\u8fde\u63a5\u4ea4\u6362\u673a\u7684\u5b8c\u6574\u56fe\u7f51\u7edc\u5b9e\u73b0\u65b9\u6cd5\uff0c\u53ef\u964d\u4f4e\u5927\u89c4\u6a21\u7f51\u7edc\u7684\u5e03\u7ebf\u548c\u8def\u7531\u590d\u6742\u6027", "motivation": "\u57fa\u4e8e\u5b8c\u6574\u56fe\u62d3\u6251\u7684\u7f51\u7edc\uff08\u5982Dragonfly\u548cHyperX\uff09\u5728\u5927\u578b\u7cfb\u7edf\u4e2d\u94fe\u63a5\u6570\u91cf\u5de8\u5927\u4e14\u5feb\u901f\u589e\u957f\uff0c\u5bfc\u81f4\u5e03\u7ebf\u590d\u6742\u548c\u8def\u7531\u56f0\u96be", "method": "\u63d0\u51faLACIN\u5b9e\u73b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u76f8\u540c\u7d22\u5f15\u7684\u7aef\u53e3\u8fde\u63a5\u4ea4\u6362\u673a\uff0c\u7b80\u5316\u7f51\u7edc\u5b9e\u73b0", "result": "LACIN\u51cf\u5c11\u4e86\u7f51\u7edc\u7684\u5e03\u7ebf\u590d\u6742\u6027\u548c\u8def\u7531\u590d\u6742\u6027\uff0c\u4fbf\u4e8e\u4e0d\u540c\u89c4\u6a21\u5e76\u884c\u8ba1\u7b97\u673a\u7f51\u7edc\u7684\u90e8\u7f72", "conclusion": "LACIN\u4e3a\u4eceVLSI\u7cfb\u7edf\u5230\u6700\u5927\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u5404\u79cd\u89c4\u6a21\u5e76\u884c\u8ba1\u7b97\u673a\u63d0\u4f9b\u4e86\u66f4\u6613\u90e8\u7f72\u7684\u7f51\u7edc\u5b9e\u73b0\u65b9\u6848"}}
{"id": "2601.05816", "categories": ["cs.DC", "hep-lat"], "pdf": "https://arxiv.org/pdf/2601.05816", "abs": "https://arxiv.org/abs/2601.05816", "authors": ["Shiting Long", "Gustavo Ramirez-Hidalgo", "Stepan Nassyr", "Jose Jimenez-Merchan", "Andreas Frommer", "Dirk Pleiter"], "title": "Performance-Portable Optimization and Analysis of Multiple Right-Hand Sides in a Lattice QCD Solver", "comment": "Accepted for publication in 2025 IEEE 32nd International Conference on High Performance Computing, Data, and Analytics (HiPC)", "summary": "Managing the high computational cost of iterative solvers for sparse linear systems is a known challenge in scientific computing. Moreover, scientific applications often face memory bandwidth constraints, making it critical to optimize data locality and enhance the efficiency of data transport. We extend the lattice QCD solver DD-$\u03b1$AMG to incorporate multiple right-hand sides (rhs) for both the Wilson-Dirac operator evaluation and the GMRES solver, with and without odd-even preconditioning. To optimize auto-vectorization, we introduce a flexible interface that supports various data layouts and implement a new data layout for better SIMD utilization. We evaluate our optimizations on both x86 and Arm clusters, demonstrating performance portability with similar speedups. A key contribution of this work is the performance analysis of our optimizations, which reveals the complexity introduced by architectural constraints and compiler behavior. Additionally, we explore different implementations leveraging a new matrix instruction set for Arm called SME and provide an early assessment of its potential benefits.", "AI": {"tldr": "\u6269\u5c55DD-\u03b1AMG\u6c42\u89e3\u5668\u4ee5\u652f\u6301\u591a\u53f3\u7aef\u9879\uff0c\u4f18\u5316\u6570\u636e\u5e03\u5c40\u63d0\u5347SIMD\u5229\u7528\uff0c\u5728x86\u548cArm\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u6027\u80fd\u53ef\u79fb\u690d\u6027\uff0c\u5e76\u8bc4\u4f30Arm SME\u6307\u4ee4\u96c6\u7684\u6f5c\u529b\u3002", "motivation": "\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u8fed\u4ee3\u6c42\u89e3\u5668\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u79d1\u5b66\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u5185\u5b58\u5e26\u5bbd\u9650\u5236\uff0c\u9700\u8981\u4f18\u5316\u6570\u636e\u5c40\u90e8\u6027\u548c\u6570\u636e\u4f20\u8f93\u6548\u7387\u3002", "method": "\u6269\u5c55DD-\u03b1AMG\u6c42\u89e3\u5668\u652f\u6301\u591a\u53f3\u7aef\u9879\uff08\u6709/\u65e0\u5947\u5076\u9884\u5904\u7406\uff09\uff0c\u5f15\u5165\u7075\u6d3b\u63a5\u53e3\u652f\u6301\u591a\u79cd\u6570\u636e\u5e03\u5c40\u4ee5\u4f18\u5316\u81ea\u52a8\u5411\u91cf\u5316\uff0c\u5b9e\u73b0\u65b0\u7684\u6570\u636e\u5e03\u5c40\u63d0\u5347SIMD\u5229\u7528\u7387\uff0c\u5728x86\u548cArm\u96c6\u7fa4\u4e0a\u8bc4\u4f30\uff0c\u5e76\u63a2\u7d22Arm SME\u6307\u4ee4\u96c6\u5b9e\u73b0\u3002", "result": "\u5728x86\u548cArm\u96c6\u7fa4\u4e0a\u5c55\u793a\u4e86\u6027\u80fd\u53ef\u79fb\u690d\u6027\u548c\u76f8\u4f3c\u7684\u52a0\u901f\u6548\u679c\uff0c\u6027\u80fd\u5206\u6790\u63ed\u793a\u4e86\u67b6\u6784\u7ea6\u675f\u548c\u7f16\u8bd1\u5668\u884c\u4e3a\u5e26\u6765\u7684\u590d\u6742\u6027\uff0c\u5bf9Arm SME\u6307\u4ee4\u96c6\u63d0\u4f9b\u4e86\u65e9\u671f\u8bc4\u4f30\u3002", "conclusion": "\u901a\u8fc7\u591a\u53f3\u7aef\u9879\u652f\u6301\u548c\u6570\u636e\u5e03\u5c40\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86DD-\u03b1AMG\u6c42\u89e3\u5668\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u8de8\u67b6\u6784\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6027\uff0c\u5e76\u4e3aArm SME\u6307\u4ee4\u96c6\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2601.05955", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.05955", "abs": "https://arxiv.org/abs/2601.05955", "authors": ["Yuliang Chen", "Xi Lin", "Jun Wu", "Xiangrui Cai", "Qiaolun Zhang", "Xichun Fan", "Jiapeng Xu", "Xiu Su"], "title": "Multi-Modal Style Transfer-based Prompt Tuning for Efficient Federated Domain Generalization", "comment": null, "summary": "Federated Domain Generalization (FDG) aims to collaboratively train a global model across distributed clients that can generalize well on unseen domains. However, existing FDG methods typically struggle with cross-client data heterogeneity and incur significant communication and computation overhead. To address these challenges, this paper presents a new FDG framework, dubbed FaST-PT, which facilitates local feature augmentation and efficient unseen domain adaptation in a distributed manner. First, we propose a lightweight Multi-Modal Style Transfer (MST) method to transform image embedding under text supervision, which could expand the training data distribution and mitigate domain shift. We then design a dual-prompt module that decomposes the prompt into global and domain prompts. Specifically, global prompts capture general knowledge from augmented embedding across clients, while domain prompts capture domain-specific knowledge from local data. Besides, Domain-aware Prompt Generation (DPG) is introduced to adaptively generate suitable prompts for each sample, which facilitates unseen domain adaptation through knowledge fusion. Extensive experiments on four cross-domain benchmark datasets, e.g., PACS and DomainNet, demonstrate the superior performance of FaST-PT over SOTA FDG methods such as FedDG-GA and DiPrompt. Ablation studies further validate the effectiveness and efficiency of FaST-PT.", "AI": {"tldr": "FaST-PT\u662f\u4e00\u4e2a\u8054\u90a6\u57df\u6cdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u98ce\u683c\u8f6c\u79fb\u8fdb\u884c\u672c\u5730\u7279\u5f81\u589e\u5f3a\uff0c\u5e76\u91c7\u7528\u53cc\u63d0\u793a\u6a21\u5757\uff08\u5168\u5c40\u63d0\u793a\u548c\u57df\u63d0\u793a\uff09\u6765\u6355\u83b7\u901a\u7528\u548c\u7279\u5b9a\u9886\u57df\u77e5\u8bc6\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u57df\u6cdb\u5316\u65b9\u6cd5\u9762\u4e34\u8de8\u5ba2\u6237\u7aef\u6570\u636e\u5f02\u6784\u6027\u6311\u6218\uff0c\u4e14\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u8f83\u5927\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\u53c8\u80fd\u9ad8\u6548\u9002\u5e94\u672a\u89c1\u57df\u7684\u65b0\u6846\u67b6\u3002", "method": "1. \u63d0\u51fa\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u98ce\u683c\u8f6c\u79fb\u65b9\u6cd5\uff0c\u5728\u6587\u672c\u76d1\u7763\u4e0b\u8f6c\u6362\u56fe\u50cf\u5d4c\u5165\u4ee5\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff1b2. \u8bbe\u8ba1\u53cc\u63d0\u793a\u6a21\u5757\uff0c\u5c06\u63d0\u793a\u5206\u89e3\u4e3a\u5168\u5c40\u63d0\u793a\uff08\u6355\u83b7\u8de8\u5ba2\u6237\u7aef\u901a\u7528\u77e5\u8bc6\uff09\u548c\u57df\u63d0\u793a\uff08\u6355\u83b7\u672c\u5730\u7279\u5b9a\u9886\u57df\u77e5\u8bc6\uff09\uff1b3. \u5f15\u5165\u57df\u611f\u77e5\u63d0\u793a\u751f\u6210\u673a\u5236\uff0c\u4e3a\u6bcf\u4e2a\u6837\u672c\u81ea\u9002\u5e94\u751f\u6210\u5408\u9002\u63d0\u793a\uff0c\u901a\u8fc7\u77e5\u8bc6\u878d\u5408\u4fc3\u8fdb\u672a\u89c1\u57df\u9002\u5e94\u3002", "result": "\u5728PACS\u548cDomainNet\u7b49\u56db\u4e2a\u8de8\u57df\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFaST-PT\u4f18\u4e8eFedDG-GA\u548cDiPrompt\u7b49SOTA\u8054\u90a6\u57df\u6cdb\u5316\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "FaST-PT\u901a\u8fc7\u672c\u5730\u7279\u5f81\u589e\u5f3a\u548c\u9ad8\u6548\u7684\u53cc\u63d0\u793a\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u57df\u6cdb\u5316\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u5728\u672a\u89c1\u57df\u9002\u5e94\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
