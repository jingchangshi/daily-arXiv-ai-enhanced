<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Pushing Tensor Accelerators Beyond MatMul in a User-Schedulable Language](https://arxiv.org/abs/2512.02371)
*Yihong Zhang,Derek Gerstmann,Andrew Adams,Maaz Bin Safeer Ahmad*

Main category: cs.PL

TL;DR: 提出基于编译器的技术，使用Halide语言和等式饱和的指令选择器，让张量加速器能应用于图像处理等传统ML之外的应用，实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 张量加速器在现代CPU/GPU中占比增加，但难以编程，只能通过厂商提供的核库使用，限制了其在传统ML和科学计算之外的应用。图像处理等应用本质上是矩阵线性变换，理论上可以利用张量加速器，但编程困难阻碍了这种应用。

Method: 使用Halide用户可调度语言简洁表达算法，实现基于等式饱和的灵活张量指令选择器，支持CPU和GPU附加的张量加速器，并与现有调度操作（如生产者-消费者融合）协同工作。

Result: 实现了多个图像处理流水线（滤波、重采样、去噪等），相比非加速器基线获得显著加速。例如，在Nvidia RTX 4070 GPU上，下采样例程通过使用Tensor Cores实现了6.1倍加速。

Conclusion: 通过编译器技术，张量加速器可以扩展到传统领域之外的应用，使开发者能够用少量代码编写多样化的加速器利用应用，展示了张量加速器在更广泛场景中的潜力。

Abstract: Tensor accelerators now represent a growing share of compute resources in modern CPUs and GPUs. However, they are hard to program, leading developers to use vendor-provided kernel libraries that support tensor accelerators. As a result, the usage of tensor accelerators is limited to the provided interface, mainly designed for traditional ML and scientific computing workloads.
  In this paper, we show that tensor accelerators can improve the performance of applications beyond simple variants of MatMul. For example, many image processing pipelines are linear transformations over matrices in disguise and can therefore utilize such specialized hardware. This is nonetheless hindered by the difficulties in programming tensor accelerators. We tackle this problem with compiler-based techniques. We use the Halide user-schedulable language and express operations as Halide algorithms succinctly. To this end, we implement a flexible tensor instruction selector based on equality saturation. The tensor instruction selector supports both CPU- and GPU-attached tensor accelerators and works with existing scheduling operations (e.g., producer-consumer fusion). Together, this enables developers to write diverse accelerator-leveraging applications in a few dozen lines.
  Using our system, we demonstrate the potential of tensor accelerators beyond their traditional domains. We implement several image processing pipelines (e.g., filtering, resampling, and denoising) in our system and evaluate them against non-accelerator-leveraging baselines. We show that these pipelines can achieve significant speedups. For example, a downsampling routine is sped up by $6.1\times$ by utilizing Tensor Cores on an Nvidia RTX 4070 GPU.

</details>


### [2] [Probabilistic energy profiler for statically typed JVM-based programming languages](https://arxiv.org/abs/2512.02738)
*Joel Nyholm,Wojciech Mostowski,Christoph Reichenbach*

Main category: cs.PL

TL;DR: 提出基于贝叶斯统计的JVM字节码能耗预测方法，通过分析字节码模式、数据类型、操作和设备因素，构建统计分布模型而非点估计，提高能耗预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有能耗分析方法主要关注CPU能耗的点估计，忽略了其他硬件影响，且缺乏统计推理和可解释性。需要一种能更全面预测软件能耗的方法，特别是针对静态类型JVM语言（如Java和Scala）。

Method: 测量字节码模式的能耗，构建基于贝叶斯统计的统计模型。模型包含四个因素：数据大小、数据类型、操作（从代码静态获取）以及执行硬件平台（设备）。通过Java实现验证方法。

Result: 所有四个因素都对能耗有显著影响，特别是同型号设备间存在能耗差异，操作和数据类型也会导致能耗差异。实验显示模型对未见程序的能耗预测与实际能耗高度吻合。

Conclusion: 提出了一种构建能耗模型的新方法学，该方法可为未来验证工具等提供能耗估计基础，解决了现有方法仅关注CPU点估计的局限性，提高了预测的统计可靠性和可解释性。

Abstract: Energy consumption is a growing concern in several fields, from mobile devices to large data centers. Developers need detailed data on the energy consumption of their software to mitigate consumption issues. Previous approaches have a broader focus, such as on specific functions or programs, rather than source code statements. They primarily focus on estimating the CPU's energy consumption using point estimates, thereby disregarding other hardware effects and limiting their use for statistical reasoning and explainability. We developed a novel methodology to address the limitations of measuring only the CPU's consumption and using point estimates, focusing on predicting the energy usage of statically typed JVM-based programming languages, such as Java and Scala. We measure the energy consumption of Bytecode patterns, the translation from the programming language's source code statement to their Java Bytecode representation. With the energy measurements, we construct a statistical model using Bayesian statistics, which allows us to predict the energy consumption through statistical distributions and analyze individual factors. The model includes three factors we obtain statically from the code: data size, data type, operation, and one factor about the hardware platform the code executes on: device. To validate our methodology, we implemented it for Java and evaluated its energy predictions on unseen programs. We observe that all four factors are influential, notably that two devices of the same model may differ in energy consumption and that the operations and data types cause consumption differences. The experiments also show that the energy prediction of programs closely follows the program's real energy consumption, validating our approach. Our work presents a methodology for constructing an energy model that future work, such as verification tools, can use for their energy estimates.

</details>


### [3] [Lumos: Let there be Language Model System Certification](https://arxiv.org/abs/2512.02966)
*Isha Chaudhary,Vedaant Jain,Avaljot Singh,Kavya Sachdeva,Sayan Ranu,Gagandeep Singh*

Main category: cs.PL

TL;DR: Lumos是一个用于规范和形式化认证语言模型系统行为的概率编程框架，支持通过图结构表示提示分布，并能发现最先进视觉语言模型在自动驾驶场景中的严重安全风险。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统化、可扩展的框架来规范和形式化认证语言模型系统的行为，特别是在面对快速演变的威胁环境时，需要一种能够灵活修改规范并保持认证有效性的方法。

Method: Lumos是一个基于图的命令式概率编程领域特定语言，通过图结构表示提示分布，从采样子图生成随机提示，提供混合语义解释规范，并与统计认证器集成来认证任意提示分布下的语言模型行为。

Result: Lumos能够编码现有的语言模型规范，包括复杂的关系和时间规范，并首次为自动驾驶场景中的视觉语言模型制定了安全规范。实验显示，最先进的视觉语言模型Qwen-VL在雨天右转场景中至少有90%的概率产生错误和不安全的响应，暴露出重大安全风险。

Conclusion: Lumos是第一个系统化、可扩展的基于语言的框架，用于规范和认证语言模型系统行为，为更广泛采用语言模型认证铺平了道路，其模块化结构使规范易于修改，能够跟上快速演变的威胁环境。

Abstract: We introduce the first principled framework, Lumos, for specifying and formally certifying Language Model System (LMS) behaviors. Lumos is an imperative probabilistic programming DSL over graphs, with constructs to generate independent and identically distributed prompts for LMS. It offers a structured view of prompt distributions via graphs, forming random prompts from sampled subgraphs. Lumos supports certifying LMS for arbitrary prompt distributions via integration with statistical certifiers. We provide hybrid (operational and denotational) semantics for Lumos, providing a rigorous way to interpret the specifications. Using only a small set of composable constructs, Lumos can encode existing LMS specifications, including complex relational and temporal specifications. It also facilitates specifying new properties - we present the first safety specifications for vision-language models (VLMs) in autonomous driving scenarios developed with Lumos. Using these, we show that the state-of-the-art VLM Qwen-VL exhibits critical safety failures, producing incorrect and unsafe responses with at least 90% probability in right-turn scenarios under rainy driving conditions, revealing substantial safety risks. Lumos's modular structure allows easy modification of the specifications, enabling LMS certification to stay abreast with the rapidly evolving threat landscape. We further demonstrate that specification programs written in Lumos enable finding specific failure cases exhibited by state-of-the-art LMS. Lumos is the first systematic and extensible language-based framework for specifying and certifying LMS behaviors, paving the way for a wider adoption of LMS certification.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Fantasy: Efficient Large-scale Vector Search on GPU Clusters with GPUDirect Async](https://arxiv.org/abs/2512.02278)
*Yi Liu,Chen Qian*

Main category: cs.DC

TL;DR: Fantasy是一个用于大规模向量相似性搜索的GPU集群系统，通过GPUDirect Async技术实现计算与网络通信的流水线化，解决单GPU内存不足问题，提升搜索吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着AI应用（如大语言模型）中向量数量持续增长，图索引大小迅速超过单GPU内存容量，无法在单GPU上存储和处理整个索引。现有CPU-GPU架构中数据加载步骤会阻塞GPU计算，影响性能。

Method: 提出Fantasy系统，在GPU集群中利用GPUDirect Async技术实现向量搜索和数据传输的流水线化，重叠计算和网络通信，支持大规模图索引和大量查询批处理。

Result: 显著提高了大规模图索引的搜索吞吐量，能够处理更大的查询批大小，解决了单GPU内存限制问题。

Conclusion: Fantasy通过GPU集群中的计算-通信重叠机制，为大规模向量相似性搜索提供了高效的解决方案，克服了单GPU内存限制和现有CPU-GPU架构的性能瓶颈。

Abstract: Vector similarity search has become a critical component in AI-driven applications such as large language models (LLMs). To achieve high recall and low latency, GPUs are utilized to exploit massive parallelism for faster query processing. However, as the number of vectors continues to grow, the graph size quickly exceeds the memory capacity of a single GPU, making it infeasible to store and process the entire index on a single GPU. Recent work uses CPU-GPU architectures to keep vectors in CPU memory or SSDs, but the loading step stalls GPU computation. We present Fantasy, an efficient system that pipelines vector search and data transfer in a GPU cluster with GPUDirect Async. Fantasy overlaps computation and network communication to significantly improve search throughput for large graphs and deliver large query batch sizes.

</details>


### [5] [DOLMA: A Data Object Level Memory Disaggregation Framework for HPC Applications](https://arxiv.org/abs/2512.02300)
*Haoyu Zheng,Shouwei Gao,Jie Ren,Wenqian Dong*

Main category: cs.DC

TL;DR: DOLMA是一个面向HPC应用的数据对象级内存解聚框架，通过智能识别和卸载数据对象到远程内存，在平均减少63%本地内存使用的同时，将性能下降控制在16%以内。


<details>
  <summary>Details</summary>
Motivation: 内存解聚技术有望扩展HPC系统的内存容量并提高利用率，但远程内存访问的性能开销对计算密集型HPC应用构成重大挑战，因为这类应用的执行时间对数据局部性高度敏感。

Method: DOLMA框架智能识别并将数据对象卸载到远程内存，提供定量分析以确定合适的本地内存大小。利用HPC应用典型可预测的内存访问模式，通过双缓冲区设计实现远程内存预取，并仔细平衡本地和远程内存使用，同时保持多线程并发性。

Result: 在八个HPC工作负载和计算内核的评估中，DOLMA将性能下降限制在16%以内，同时将本地内存使用量平均减少了63%。

Conclusion: DOLMA为HPC领域利用解聚内存提供了一个灵活高效的解决方案，在最小化影响应用性能的同时，显著减少了本地内存使用。

Abstract: Memory disaggregation is promising to scale memory capacity and improves utilization in HPC systems. However, the performance overhead of accessing remote memory poses a significant challenge, particularly for compute-intensive HPC applications where execution times are highly sensitive to data locality. In this work, we present DOLMA, a Data Object Level M emory dis Aggregation framework designed for HPC applications. DOLMA intelligently identifies and offloads data objects to remote memory, while providing quantitative analysis to decide a suitable local memory size. Furthermore, DOLMA leverages the predictable memory access patterns typical in HPC applications and enables remote memory prefetch via a dual-buffer design. By carefully balancing local and remote memory usage and maintaining multi-thread concurrency, DOLMA provides a flexible and efficient solution for leveraging disaggregated memory in HPC domains while minimally compromising application performance. Evaluating with eight HPC workloads and computational kernels, DOLMA limits performance degradation to less than 16% while reducing local memory usage by up to 63%, on average.

</details>


### [6] [Solutions for Distributed Memory Access Mechanism on HPC Clusters](https://arxiv.org/abs/2512.02546)
*Jan Meizner,Maciej Malawski*

Main category: cs.DC

TL;DR: 论文评估了分布式系统中基于两个HPC集群的远程内存访问机制，比较了基于共享存储和MPI（通过Infiniband和Slingshot）的解决方案与本地内存访问的性能，发现基于MPI的远程访问结果与本地内存访问相似。


<details>
  <summary>Details</summary>
Motivation: 研究分布式系统中远程内存访问机制的性能，特别是在HPC集群环境下，为医疗等应用场景提供高效的远程内存访问解决方案。

Method: 基于两个不同的HPC集群，比较了三种内存访问机制：1) 基于共享存储的远程访问，2) 基于MPI（通过Infiniband和Slingshot网络）的远程访问，3) 本地内存访问作为基准对比。

Result: 研究发现，特别是基于MPI的远程内存访问性能与本地内存访问相似，表明在分布式系统中可以实现接近本地性能的远程内存访问。

Conclusion: 基于MPI的远程内存访问机制在HPC集群中表现出色，性能接近本地访问，特别适用于医疗等需要高效远程数据访问的应用场景。

Abstract: Paper presents and evaluates various mechanisms for remote access to memory in distributed systems based on two distinct HPC clusters. We are comparing solutions based on the shared storage and MPI (over Infiniband and Slingshot) to the local memory access. This paper also mentions medical use-cases that would mostly benefit from the described solution. We have found out that results for remote access esp. backed by MPI are similar to local memory access.

</details>


### [7] [Offloading Artificial Intelligence Workloads across the Computing Continuum by means of Active Storage Systems](https://arxiv.org/abs/2512.02646)
*Alex Barceló,Sebastián A. Cajas Ordoñez,Jaydeep Samanta,Andrés L. Suárez-Cetrulo,Romila Ghosh,Ricardo Simón Carbajo,Anna Queralt*

Main category: cs.DC

TL;DR: 本文提出了一种基于主动存储的计算连续体软件架构，用于优化AI工作负载分布，通过将计算嵌入存储架构减少数据传输开销，提高内存效率和训练速度。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载在多样化计算环境中的需求增长，传统云架构在处理AI驱动数据时面临存储、计算和数据移动效率低下的问题。现有框架缺乏适应计算连续体所需的灵活性和适应性，无法应对设备异构性和快速变化的算法模型。

Method: 提出一种软件架构，利用主流Python库和active storage平台dataClay，将AI工作负载无缝分布到计算连续体中。通过将计算嵌入存储架构（主动存储），减少数据传输开销，优化资源利用。

Result: 实验评估显示，通过主动存储卸载工作负载显著提高了内存效率（减少内存消耗）和训练速度，同时保持准确性。在不同设备上展示了内存消耗、存储需求、训练时间和执行效率方面的优势与权衡。

Conclusion: 主动存储有潜力彻底改变AI工作负载管理，使分布式AI部署更具可扩展性和资源效率，同时为领域专家和应用开发者提供极低的学习门槛。该方法为处理AI工作负载的异构分布式环境提供了有效的解决方案。

Abstract: The increasing demand for artificial intelligence (AI) workloads across diverse computing environments has driven the need for more efficient data management strategies. Traditional cloud-based architectures struggle to handle the sheer volume and velocity of AI-driven data, leading to inefficiencies in storage, computation, and data movement. This paper explores the integration of active storage systems within the computing continuum to optimize AI workload distribution.
  By embedding computation directly into storage architectures, active storage is able to reduce data transfer overhead, enhancing performance and improving resource utilization. Other existing frameworks and architectures offer mechanisms to distribute certain AI processes across distributed environments; however, they lack the flexibility and adaptability that the continuum requires, both regarding the heterogeneity of devices and the rapid-changing algorithms and models being used by domain experts and researchers.
  This article proposes a software architecture aimed at seamlessly distributing AI workloads across the computing continuum, and presents its implementation using mainstream Python libraries and dataClay, an active storage platform. The evaluation shows the benefits and trade-offs regarding memory consumption, storage requirements, training times, and execution efficiency across different devices. Experimental results demonstrate that the process of offloading workloads through active storage significantly improves memory efficiency and training speeds while maintaining accuracy. Our findings highlight the potential of active storage to revolutionize AI workload management, making distributed AI deployments more scalable and resource-efficient with a very low entry barrier for domain experts and application developers.

</details>


### [8] [Distributed and Autonomic Minimum Spanning Trees](https://arxiv.org/abs/2512.02683)
*Luiz A. Rodrigues,Elias P. Duarte,Luciana Arantes*

Main category: cs.DC

TL;DR: 提出一种自组织算法，使分布式系统中的n个进程构建和维护连接自身的生成树，每个节点的入度和树深度不超过log₂n，支持动态创建、故障恢复，并基于此实现两种广播算法。


<details>
  <summary>Details</summary>
Motivation: 传统的一对多广播策略不可扩展，会给发送者带来沉重负载。需要一种能够自动构建和维护生成树的方法，以支持可扩展的广播通信。

Method: 使用VCube虚拟拓扑作为故障检测器，提出自组织算法动态构建和维护生成树。算法确保每个顶点的入度和树深度不超过log₂n，支持从任意源进程创建，并能透明地重建以应对进程故障和恢复。

Result: 算法能容忍最多n-1个进程故障，正确进程仍通过可扩展的功能性生成树保持连接。当所有进程正常时，每个进程的度恰好为log₂n。基于此提出了两种广播算法：尽力而为广播和可靠广播，并通过仿真与其他方案进行了比较。

Conclusion: 该自组织生成树算法为分布式系统提供了可扩展的广播解决方案，能够动态适应系统变化，支持大规模系统中的高效通信。

Abstract: The most common strategy for enabling a process in a distributed system to broadcast a message is one-to-all communication. However, this approach is not scalable, as it places a heavy load on the sender. This work presents an autonomic algorithm that enables the $n$ processes in a distributed system to build and maintain a spanning tree connecting themselves. In this context, processes are the vertices of the spanning tree. By definition, a spanning tree connects all processes without forming cycles. The proposed algorithm ensures that every vertex in the spanning tree has both an in-degree and the tree depth of at most $log_2 n$. When all processes are correct, the degree of each process is exactly $log_2 n$. A spanning tree is dynamically created from any source process and is transparently reconstructed as processes fail or recover. Up to $n-1$ processes can fail, and the correct processes remain connected through a scalable, functioning spanning tree. To build and maintain the tree, processes use the VCube virtual topology, which also serves as a failure detector. Two broadcast algorithms based on the autonomic spanning tree algorithm are presented: one for best-effort broadcast and one for reliable broadcast. Simulation results are provided, including comparisons with other alternatives.

</details>


### [9] [Designing FAIR Workflows at OLCF: Building Scalable and Reusable Ecosystems for HPC Science](https://arxiv.org/abs/2512.02818)
*Sean R. Wilkinson,Patrick Widener,Sarp Oral,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 论文提出HPC中心应更积极地在跨学科研究中培育FAIR生态系统，通过基于组件的FAIR方法而非完整工作流，提升计算组件的可发现性、共享和重用。


<details>
  <summary>Details</summary>
Motivation: HPC中心的基础设施、软件环境和安全要求与用户本地系统差异大，导致用户开发与特定HPC中心紧密耦合的数字制品，造成重复劳动。虽然FAIR原则已在各研究社区推广，但领域特定性形成了信息孤岛，限制了跨学科合作。

Method: 基于欧洲开放科学云EOSC-Life FAIR工作流协作平台的架构，提出针对HPC需求定制的模型。强调使单个工作流组件FAIR化，而非整个工作流，采用基于组件的方法来支持HPC用户的多样化需求。

Result: 提出HPC中心应设计支持研究人员更有效发现、共享和重用计算组件的基础设施，通过组件级FAIR化最大化HPC用户工作的长期价值。

Conclusion: HPC中心应发挥更积极作用，培育跨学科的FAIR生态系统，通过组件化FAIR方法解决HPC环境中的重复劳动和跨学科协作问题，提升科学研究的效率和可持续性。

Abstract: High Performance Computing (HPC) centers provide advanced infrastructure that enables scientific research at extreme scale. These centers operate with hardware configurations, software environments, and security requirements that differ substantially from most users' local systems. As a result, users often develop customized digital artifacts that are tightly coupled to a given HPC center. This practice can lead to significant duplication of effort as multiple users independently create similar solutions to common problems. The FAIR Principles offer a framework to address these challenges. Initially designed to improve data stewardship, the FAIR approach has since been extended to encompass software, workflows, models, and infrastructure. By encouraging the use of rich metadata and community standards, FAIR practices aim to make digital artifacts easier to share and reuse, both within and across scientific domains. Many FAIR initiatives have emerged within individual research communities, often aligned by discipline (e.g. bioinformatics, earth sciences). These communities have made progress in adopting FAIR practices, but their domain-specific nature can lead to silos that limit broader collaboration. Thus, we propose that HPC centers play a more active role in fostering FAIR ecosystems that support research across multiple disciplines. This requires designing infrastructure that enables researchers to discover, share, and reuse computational components more effectively. Here, we build on the architecture of the European Open Science Cloud (EOSC) EOSC-Life FAIR Workflows Collaboratory to propose a model tailored to the needs of HPC. Rather than focusing on entire workflows, we emphasize the importance of making individual workflow components FAIR. This component-based approach better supports the diverse and evolving needs of HPC users while maximizing the long-term value of their work.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis](https://arxiv.org/abs/2512.02189)
*Aaron Jarmusch,Sunita Chandrasekaran*

Main category: cs.AR

TL;DR: 本文提出开源微基准测试套件，系统评估NVIDIA Blackwell (B200) GPU架构创新，包括第5代张量核心、张量内存、解压缩引擎等特性，并与H200对比，展示B200在混合精度吞吐量提升1.56倍、能效提升42%、内存访问延迟降低58%等优势。


<details>
  <summary>Details</summary>
Motivation: 随着GPU架构快速发展以满足百亿亿次计算和机器学习需求，架构创新对不同工作负载的性能影响缺乏系统理解。NVIDIA Blackwell (B200)引入多项重要架构创新，但量化这些改进的系统方法滞后于硬件开发周期。

Method: 开发开源微基准测试套件，系统评估Blackwell GPU并与H200对比，分析内存子系统、张量核心流水线和浮点精度（FP32、FP16、FP8、FP6、FP4）。评估密集/稀疏GEMM、Transformer推理和训练工作负载。

Result: B200的张量核心增强实现比H200高1.56倍的混合精度吞吐量和42%的能效提升。内存分析显示缓存未命中时内存访问延迟降低58%，从根本上改变了最优算法设计策略。

Conclusion: 开源微基准测试套件为应用开发者提供实用见解，帮助充分利用现代GPU架构特性，支持明智的架构决策，并指导未来GPU设计方向。

Abstract: As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.
  Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies.

</details>


### [11] [Near-Memory Architecture for Threshold-Ordinal Surface-Based Corner Detection of Event Cameras](https://arxiv.org/abs/2512.02346)
*Hongyang Shang,An Guo,Shuai Dong,Junyi Yang,Ye Ke,Arindam Basu*

Main category: cs.AR

TL;DR: 提出NM-TOS近存架构，通过8T SRAM单元、流水线优化和DVFS技术，显著降低TOS角点检测算法的延迟和能耗，适用于事件相机的边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 事件相机在监控和自动驾驶中应用广泛，但TOS等角点检测算法在资源受限的边缘设备上存在高延迟问题，限制了事件相机的高速低功耗优势。

Method: 提出NM-TOS近存架构，采用读写解耦的8T SRAM单元，通过流水线优化补丁更新速度，结合硬件-软件协同优化的外围电路和动态电压频率缩放(DVFS)技术。

Result: 相比传统数字实现，在1.2V电压下延迟/能耗降低24.7倍/1.2倍，在0.6V电压下降低1.93倍/6.6倍。蒙特卡洛仿真显示在0.62V以上零比特错误率，角点检测精度损失很小。

Conclusion: NM-TOS架构有效解决了事件相机角点检测算法在边缘设备上的延迟问题，实现了显著的延迟和能耗优化，同时保持了良好的检测精度。

Abstract: Event-based Cameras (EBCs) are widely utilized in surveillance and autonomous driving applications due to their high speed and low power consumption. Corners are essential low-level features in event-driven computer vision, and novel algorithms utilizing event-based representations, such as Threshold-Ordinal Surface (TOS), have been developed for corner detection. However, the implementation of these algorithms on resource-constrained edge devices is hindered by significant latency, undermining the advantages of EBCs. To address this challenge, a near-memory architecture for efficient TOS updates (NM-TOS) is proposed. This architecture employs a read-write decoupled 8T SRAM cell and optimizes patch update speed through pipelining. Hardware-software co-optimized peripheral circuits and dynamic voltage and frequency scaling (DVFS) enable power and latency reductions. Compared to traditional digital implementations, our architecture reduces latency/energy by 24.7x/1.2x at Vdd = 1.2 V or 1.93x/6.6x at Vdd = 0.6 V based on 65nm CMOS process. Monte Carlo simulations confirm robust circuit operation, demonstrating zero bit error rate at operating voltages above 0.62 V, with only 0.2% at 0.61 V and 2.5% at 0.6 V. Corner detection evaluation using precision-recall area under curve (AUC) metrics reveals minor AUC reductions of 0.027 and 0.015 at 0.6 V for two popular EBC datasets.

</details>


### [12] [Monomorphism-based CGRA Mapping via Space and Time Decoupling](https://arxiv.org/abs/2512.02859)
*Cristian Tirelli,Rodrigo Otoni,Laura Pozzi*

Main category: cs.AR

TL;DR: 提出一种新的CGRA映射方法，通过解耦时间和空间维度并分别探索，显著提升编译速度，特别适用于大型CGRA


<details>
  <summary>Details</summary>
Motivation: 现有CGRA编译技术在处理大规模阵列时存在可扩展性问题，难以将代码高效映射到大型CGRA上

Method: 采用时间和空间维度解耦的方法：首先使用SMT公式遍历时间维度，然后基于单态搜索寻找有效的空间解决方案

Result: 在保持与最先进技术相同映射质量的同时，显著减少编译时间，特别是在大型CGRA上效果更明显。在20×20 CGRA上实现了约10^5倍的编译速度提升

Conclusion: 提出的解耦时间和空间维度的映射方法有效解决了CGRA编译的可扩展性问题，为大型CGRA的高效编译提供了可行方案

Abstract: Coarse-Grain Reconfigurable Arrays (CGRAs) provide flexibility and energy efficiency in accelerating compute-intensive loops. Existing compilation techniques often struggle with scalability, unable to map code onto large CGRAs. To address this, we propose a novel approach to the mapping problem where the time and space dimensions are decoupled and explored separately. We leverage an SMT formulation to traverse the time dimension first, and then perform a monomorphism-based search to find a valid spatial solution. Experimental results show that our approach achieves the same mapping quality of state-of-the-art techniques while significantly reducing compilation time, with this reduction being particularly tangible when compiling for large CGRAs. We achieve approximately $10^5\times$ average compilation speedup for the benchmarks evaluated on a $20\times 20$ CGRA.

</details>


### [13] [SAT-MapIt: A SAT-based Modulo Scheduling Mapper for Coarse Grain Reconfigurable Architectures](https://arxiv.org/abs/2512.02875)
*Cristian Tirelli,Lorenzo Ferretti,Laura Pozzi*

Main category: cs.AR

TL;DR: SAT-MapIt：使用SAT求解器解决CGRA映射问题的新方法，相比现有技术能更有效地探索解空间，在47.72%的基准测试中获得更好结果。


<details>
  <summary>Details</summary>
Motivation: CGRA的加速效果严重依赖于映射质量，现有技术使用模调度和图算法（如最大团枚举）进行编译，但解空间探索不够有效。

Method: 提出基于SAT公式化的映射方法：1）引入核移动调度（KMS）；2）结合数据流图和CGRA架构信息生成布尔约束；3）使用SAT求解器高效导航解空间；4）采用迭代过程，如果当前II无有效映射则增加II重新求解。

Result: SAT-MapIt在47.72%的基准测试中优于现有技术：有时能找到更低的II，有时能在现有技术找不到有效映射的情况下找到有效映射。

Conclusion: 基于SAT的映射方法比现有模调度技术能更有效地探索解空间，为CGRA编译提供了更好的解决方案。

Abstract: Coarse-Grain Reconfigurable Arrays (CGRAs) are emerging low-power architectures aimed at accelerating compute-intensive application loops. The acceleration that a CGRA can ultimately provide, however, heavily depends on the quality of the mapping, i.e. on how effectively the loop is compiled onto the given platform. State of the Art compilation techniques achieve mapping through modulo scheduling, a strategy which attempts to minimize the II (Iteration Interval) needed to execute a loop, and they do so usually through well known graph algorithms, such as Max-Clique Enumeration.
  We address the mapping problem through a SAT formulation, instead, and thus explore the solution space more effectively than current SoA tools. To formulate the SAT problem, we introduce an ad-hoc schedule called the \textit{kernel mobility schedule} (KMS), which we use in conjunction with the data-flow graph and the architectural information of the CGRA in order to create a set of boolean statements that describe all constraints to be obeyed by the mapping for a given II. We then let the SAT solver efficiently navigate this complex space. As in other SoA techniques, the process is iterative: if a valid mapping does not exist for the given II, the II is increased and a new KMS and set of constraints is generated and solved.
  Our experimental results show that SAT-MapIt obtains better results compared to SoA alternatives in $47.72\%$ of the benchmarks explored: sometimes finding a lower II, and others even finding a valid mapping when none could previously be found.

</details>


### [14] [Mapping code on Coarse Grained Reconfigurable Arrays using a SAT solver](https://arxiv.org/abs/2512.02884)
*Cristian Tirelli,Laura Pozzi*

Main category: cs.AR

TL;DR: 提出基于SAT的CGRA映射方法，使用Kernel Mobility Schedule编码所有可能的映射，显著减少编译时间并提高映射质量


<details>
  <summary>Details</summary>
Motivation: CGRA作为协处理器加速计算密集型循环工作负载，但现有编译技术难以找到最优映射，需要改进编译过程以找到给定拓扑结构下的最低迭代间隔

Method: 提出基于可满足性(SAT)的映射问题公式化方法，引入Kernel Mobility Schedule来编码给定数据流图和迭代间隔的所有可能映射，结合CGRA架构信息生成约束条件

Result: 实验结果表明，该方法不仅平均减少了编译时间，而且相比现有最先进技术获得了更高质量的映射

Conclusion: 提出的基于SAT的CGRA映射方法通过Kernel Mobility Schedule有效解决了映射问题，在编译时间和映射质量方面均优于现有技术

Abstract: Emerging low-powered architectures like Coarse-Grain Reconfigurable Arrays (CGRAs) are becoming more common. Often included as co-processors, they are used to accelerate compute-intensive workloads like loops. The speedup obtained is defined by the hardware design of the accelerator and by the quality of the compilation. State of the art (SoA) compilation techniques leverage modulo scheduling to minimize the Iteration Interval (II), exploit the architecture parallelism and, consequentially, reduce the execution time of the accelerated workload. In our work, we focus on improving the compilation process by finding the lowest II for any given topology, through a satisfiability (SAT) formulation of the mapping problem. We introduce a novel schedule, called Kernel Mobility Schedule, to encode all the possible mappings for a given Data Flow Graph (DFG) and for a given II. The schedule is used together with the CGRA architectural information to generate all the constraints necessary to find a valid mapping. Experimental results demonstrate that our method not only reduces compilation time on average but also achieves higher quality mappings compared to existing SoA techniques.

</details>
