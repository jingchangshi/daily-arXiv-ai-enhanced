{"id": "2507.05308", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05308", "abs": "https://arxiv.org/abs/2507.05308", "authors": ["Zehuan Chen", "Xiangwei Lai"], "title": "High Order Collaboration-Oriented Federated Graph Neural Network for Accurate QoS Prediction", "comment": null, "summary": "Predicting Quality of Service (QoS) data crucial for cloud service selection,\nwhere user privacy is a critical concern. Federated Graph Neural Networks\n(FGNNs) can perform QoS data prediction as well as maintaining user privacy.\nHowever, existing FGNN-based QoS predictors commonly implement on-device\ntraining on scattered explicit user-service graphs, thereby failing to utilize\nthe implicit user-user interactions. To address this issue, this study proposes\na high order collaboration-oriented federated graph neural network (HC-FGNN) to\nobtain accurate QoS prediction with privacy preservation. Concretely, it\nmagnifies the explicit user-service graphs following the principle of attention\nmechanism to obtain the high order collaboration, which reflects the implicit\nuser-user interactions. Moreover, it utilizes a lightweight-based message\naggregation way to improve the computational efficiency. The extensive\nexperiments on two QoS datasets from real application indicate that the\nproposed HC-FGNN possesses the advantages of high prediction accurate and\nprivacy protection.", "AI": {"tldr": "HC-FGNN\u5229\u7528\u9ad8\u9636\u534f\u4f5c\u548c\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347QoS\u9884\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "motivation": "\u73b0\u6709FGNN\u65b9\u6cd5\u672a\u80fd\u5229\u7528\u9690\u5f0f\u7528\u6237\u4ea4\u4e92\uff0c\u5f71\u54cd\u9884\u6d4b\u7cbe\u5ea6\u3002", "method": "\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u6269\u5c55\u663e\u5f0f\u7528\u6237-\u670d\u52a1\u56fe\uff0c\u6355\u6349\u9ad8\u9636\u534f\u4f5c\uff0c\u5e76\u91c7\u7528\u8f7b\u91cf\u7ea7\u6d88\u606f\u805a\u5408\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9eQoS\u6570\u636e\u96c6\u4e0a\uff0cHC-FGNN\u8868\u73b0\u51fa\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u9690\u79c1\u4fdd\u62a4\u4f18\u52bf\u3002", "conclusion": "HC-FGNN\u6709\u6548\u89e3\u51b3\u4e86\u9690\u5f0f\u7528\u6237\u4ea4\u4e92\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u5e73\u8861\u3002"}}
{"id": "2507.05653", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.05653", "abs": "https://arxiv.org/abs/2507.05653", "authors": ["Guilin Zhang", "Srinivas Vippagunta", "Raghavendra Nandagopal", "Suchitra Raman", "Jeff Xu", "Marcus Pfeiffer", "Shree Chatterjee", "Ziqi Tan", "Wulan Guo", "Hailong Jiang"], "title": "Archetype-Aware Predictive Autoscaling with Uncertainty Quantification for Serverless Workloads on Kubernetes", "comment": "6 pages, 4 figures, 1 table. First three authors contributed equally.\n  Correspondence to Hailong Jiang", "summary": "High-performance extreme computing (HPEC) platforms increasingly adopt\nserverless paradigms, yet face challenges in efficiently managing highly\ndynamic workloads while maintaining service-level objectives (SLOs). We propose\n**AAPA**, an archetype-aware predictive autoscaling system that leverages weak\nsupervision to automatically classify 300\\,000\\,+ workload windows into four\narchetypes (PERIODIC, SPIKE, RAMP, STATIONARY\\_NOISY) with 99.8\\% accuracy.\nEvaluation on publicly available Azure Functions traces shows that AAPA reduces\nSLO violations by up to 50\\%, improves response time by 40\\%, albeit with a\n2--8\\,$\\times$ increase in resource cost under spike-heavy loads.", "AI": {"tldr": "AAPA\u662f\u4e00\u79cd\u57fa\u4e8e\u539f\u578b\u611f\u77e5\u7684\u9884\u6d4b\u81ea\u52a8\u6269\u5c55\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f31\u76d1\u7763\u5c06\u5de5\u4f5c\u8d1f\u8f7d\u5206\u7c7b\u4e3a\u56db\u79cd\u539f\u578b\uff0c\u663e\u8457\u51cf\u5c11SLO\u8fdd\u89c4\u5e76\u63d0\u5347\u54cd\u5e94\u65f6\u95f4\uff0c\u4f46\u8d44\u6e90\u6210\u672c\u6709\u6240\u589e\u52a0\u3002", "motivation": "\u9ad8\u6027\u80fd\u6781\u7aef\u8ba1\u7b97\u5e73\u53f0\u5728\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u7ba1\u7406\u548cSLO\u7ef4\u62a4\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "AAPA\u5229\u7528\u5f31\u76d1\u7763\u81ea\u52a8\u5206\u7c7b\u5de5\u4f5c\u8d1f\u8f7d\u4e3a\u56db\u79cd\u539f\u578b\uff08\u5468\u671f\u6027\u3001\u5cf0\u503c\u3001\u659c\u5761\u3001\u5e73\u7a33\u566a\u58f0\uff09\uff0c\u51c6\u786e\u7387\u8fbe99.8%\u3002", "result": "\u5728Azure Functions\u6d4b\u8bd5\u4e2d\uff0cAAPA\u51cf\u5c11SLO\u8fdd\u89c450%\uff0c\u54cd\u5e94\u65f6\u95f4\u63d0\u534740%\uff0c\u4f46\u5cf0\u503c\u8d1f\u8f7d\u4e0b\u8d44\u6e90\u6210\u672c\u589e\u52a02-8\u500d\u3002", "conclusion": "AAPA\u6709\u6548\u4f18\u5316\u4e86\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u7ba1\u7406\uff0c\u4f46\u9700\u6743\u8861\u8d44\u6e90\u6210\u672c\u4e0e\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.05704", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.05704", "abs": "https://arxiv.org/abs/2507.05704", "authors": ["Qianpiao Ma", "Junlong Zhou", "Xiangpeng Hou", "Jianchun Liu", "Hongli Xu", "Jianeng Miao", "Qingmin Jia"], "title": "Air-FedGA: A Grouping Asynchronous Federated Learning Mechanism Exploiting Over-the-air Computation", "comment": "This paper has been accepted by IEEE International Parallel &\n  Distributed Processing Symposium (IPDPS), 2025", "summary": "Federated learning (FL) is a new paradigm to train AI models over distributed\nedge devices (i.e., workers) using their local data, while confronting various\nchallenges including communication resource constraints, edge heterogeneity and\ndata Non-IID. Over-the-air computation (AirComp) is a promising technique to\nachieve efficient utilization of communication resource for model aggregation\nby leveraging the superposition property of a wireless multiple access channel\n(MAC). However, AirComp requires strict synchronization among edge devices,\nwhich is hard to achieve in heterogeneous scenarios. In this paper, we propose\nan AirComp-based grouping asynchronous federated learning mechanism\n(Air-FedGA), which combines the advantages of AirComp and asynchronous FL to\naddress the communication and heterogeneity challenges. Specifically, Air-FedGA\norganizes workers into groups and performs over-the-air aggregation within each\ngroup, while groups asynchronously communicate with the parameter server to\nupdate the global model. In this way, Air-FedGA accelerates the FL model\ntraining by over-the-air aggregation, while relaxing the synchronization\nrequirement of this aggregation technology. We theoretically prove the\nconvergence of Air-FedGA. We formulate a training time minimization problem for\nAir-FedGA and propose the power control and worker grouping algorithm to solve\nit, which jointly optimizes the power scaling factors at edge devices, the\ndenoising factors at the parameter server, as well as the worker grouping\nstrategy. We conduct experiments on classical models and datasets, and the\nresults demonstrate that our proposed mechanism and algorithm can speed up FL\nmodel training by 29.9%-71.6% compared with the state-of-the-art solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7a\u4e2d\u8ba1\u7b97\u7684\u5f02\u6b65\u8054\u90a6\u5b66\u4e60\u673a\u5236\uff08Air-FedGA\uff09\uff0c\u901a\u8fc7\u5206\u7ec4\u548c\u5f02\u6b65\u901a\u4fe1\u4f18\u5316\u8d44\u6e90\u5229\u7528\u548c\u540c\u6b65\u95ee\u9898\uff0c\u663e\u8457\u52a0\u901f\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u5728\u901a\u4fe1\u8d44\u6e90\u53d7\u9650\u3001\u8fb9\u7f18\u8bbe\u5907\u5f02\u6784\u548c\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08Non-IID\uff09\u60c5\u51b5\u4e0b\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u7a7a\u4e2d\u8ba1\u7b97\uff08AirComp\uff09\u548c\u5f02\u6b65\u8054\u90a6\u5b66\u4e60\uff0c\u5c06\u8bbe\u5907\u5206\u7ec4\u8fdb\u884c\u7a7a\u4e2d\u805a\u5408\uff0c\u5f02\u6b65\u66f4\u65b0\u5168\u5c40\u6a21\u578b\uff0c\u5e76\u4f18\u5316\u529f\u7387\u63a7\u5236\u548c\u5206\u7ec4\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAir-FedGA\u6bd4\u73b0\u6709\u65b9\u6848\u52a0\u901f\u8bad\u7ec329.9%-71.6%\uff0c\u5e76\u7406\u8bba\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u3002", "conclusion": "Air-FedGA\u6709\u6548\u89e3\u51b3\u4e86\u540c\u6b65\u548c\u8d44\u6e90\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u6548\u7387\u3002"}}
{"id": "2507.06011", "categories": ["cs.DC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06011", "abs": "https://arxiv.org/abs/2507.06011", "authors": ["Daghash K. Alqahtani", "Maria A. Rodriguez", "Muhammad Aamir Cheema", "Hamid Rezatofighi", "Adel N. Toosi"], "title": "ECORE: Energy-Conscious Optimized Routing for Deep Learning Models at the Edge", "comment": null, "summary": "Edge computing enables data processing closer to the source, significantly\nreducing latency an essential requirement for real-time vision-based analytics\nsuch as object detection in surveillance and smart city environments. However,\nthese tasks place substantial demands on resource constrained edge devices,\nmaking the joint optimization of energy consumption and detection accuracy\ncritical. To address this challenge, we propose ECORE, a framework that\nintegrates multiple dynamic routing strategies including estimation based\ntechniques and a greedy selection algorithm to direct image processing requests\nto the most suitable edge device-model pair. ECORE dynamically balances energy\nefficiency and detection performance based on object characteristics. We\nevaluate our approach through extensive experiments on real-world datasets,\ncomparing the proposed routers against widely used baseline techniques. The\nevaluation leverages established object detection models (YOLO, SSD,\nEfficientDet) and diverse edge platforms, including Jetson Orin Nano, Raspberry\nPi 4 and 5, and TPU accelerators. Results demonstrate that our proposed\ncontext-aware routing strategies can reduce energy consumption and latency by\n45% and 49%, respectively, while incurring only a 2% loss in detection accuracy\ncompared to accuracy-centric methods.", "AI": {"tldr": "ECORE\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8def\u7531\u7b56\u7565\u4f18\u5316\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5bf9\u8c61\u68c0\u6d4b\u4efb\u52a1\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u5b9e\u65f6\u89c6\u89c9\u5206\u6790\u4efb\u52a1\u5bf9\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u9ad8\u9700\u6c42\uff0c\u9700\u8981\u5e73\u8861\u80fd\u8017\u4e0e\u68c0\u6d4b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faECORE\u6846\u67b6\uff0c\u6574\u5408\u52a8\u6001\u8def\u7531\u7b56\u7565\uff08\u4f30\u8ba1\u6280\u672f\u548c\u8d2a\u5a6a\u9009\u62e9\u7b97\u6cd5\uff09\uff0c\u6839\u636e\u5bf9\u8c61\u7279\u5f81\u52a8\u6001\u5206\u914d\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cECORE\u80fd\u8017\u964d\u4f4e45%\uff0c\u5ef6\u8fdf\u51cf\u5c1149%\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u4ec5\u4e0b\u964d2%\u3002", "conclusion": "ECORE\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u80fd\u8017\u4e0e\u68c0\u6d4b\u7cbe\u5ea6\u7684\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2507.05556", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.05556", "abs": "https://arxiv.org/abs/2507.05556", "authors": ["Jumin Kim", "Seungmin Baek", "Minbok Wi", "Hwayong Nam", "Michael Jaemin Kim", "Sukhan Lee", "Kyomin Sohn", "Jung Ho Ahn"], "title": "Per-Row Activation Counting on Real Hardware: Demystifying Performance Overheads", "comment": "4 pages, 4 figures, to appear at IEEE Computer Architecture Letters", "summary": "Per-Row Activation Counting (PRAC), a DRAM read disturbance mitigation\nmethod, modifies key DRAM timing parameters, reportedly causing significant\nperformance overheads in simulator-based studies. However, given known\ndiscrepancies between simulators and real hardware, real-machine experiments\nare vital for accurate PRAC performance estimation. We present the first\nreal-machine performance analysis of PRAC. After verifying timing modifications\non the latest CPUs using microbenchmarks, our analysis shows that PRAC's\naverage and maximum overheads are just 1.06% and 3.28% for the SPEC CPU2017\nworkloads -- up to 9.15x lower than simulator-based reports. Further, we show\nthat the close page policy minimizes this overhead by effectively hiding the\nelongated DRAM row precharge operations due to PRAC from the critical path.", "AI": {"tldr": "PRAC\u662f\u4e00\u79cdDRAM\u8bfb\u53d6\u5e72\u6270\u7f13\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6539DRAM\u65f6\u5e8f\u53c2\u6570\u5b9e\u73b0\u3002\u7814\u7a76\u53d1\u73b0\u5176\u5b9e\u9645\u6027\u80fd\u5f00\u9500\u8fdc\u4f4e\u4e8e\u6a21\u62df\u5668\u9884\u6d4b\uff0c\u5e73\u5747\u548c\u6700\u5927\u5f00\u9500\u4ec5\u4e3a1.06%\u548c3.28%\u3002", "motivation": "\u6a21\u62df\u5668\u4e0e\u771f\u5b9e\u786c\u4ef6\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u901a\u8fc7\u771f\u5b9e\u673a\u5668\u5b9e\u9a8c\u51c6\u786e\u8bc4\u4f30PRAC\u7684\u6027\u80fd\u5f00\u9500\u3002", "method": "\u5728\u6700\u65b0CPU\u4e0a\u9a8c\u8bc1\u65f6\u5e8f\u4fee\u6539\uff0c\u5e76\u4f7f\u7528SPEC CPU2017\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u6027\u80fd\u5206\u6790\u3002", "result": "PRAC\u7684\u5e73\u5747\u548c\u6700\u5927\u6027\u80fd\u5f00\u9500\u5206\u522b\u4e3a1.06%\u548c3.28%\uff0c\u6bd4\u6a21\u62df\u5668\u9884\u6d4b\u4f4e9.15\u500d\u3002\u5173\u95ed\u9875\u7b56\u7565\u53ef\u8fdb\u4e00\u6b65\u51cf\u5c11\u5f00\u9500\u3002", "conclusion": "PRAC\u7684\u5b9e\u9645\u6027\u80fd\u5f00\u9500\u663e\u8457\u4f4e\u4e8e\u6a21\u62df\u5668\u9884\u6d4b\uff0c\u5173\u95ed\u9875\u7b56\u7565\u80fd\u6709\u6548\u9690\u85cf\u5176\u5e26\u6765\u7684\u65f6\u5e8f\u5ef6\u8fdf\u3002"}}
{"id": "2507.06031", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06031", "abs": "https://arxiv.org/abs/2507.06031", "authors": ["Juncheng Jia", "Ji Liu", "Chao Huo", "Yihui Shen", "Yang Zhou", "Huaiyu Dai", "Dejing Dou"], "title": "Efficient Federated Learning with Timely Update Dissemination", "comment": "38 pages, to appear in Knowledge and Information Systems (KAIS)", "summary": "Federated Learning (FL) has emerged as a compelling methodology for the\nmanagement of distributed data, marked by significant advancements in recent\nyears. In this paper, we propose an efficient FL approach that capitalizes on\nadditional downlink bandwidth resources to ensure timely update dissemination.\nInitially, we implement this strategy within an asynchronous framework,\nintroducing the Asynchronous Staleness-aware Model Update (FedASMU), which\nintegrates both server-side and device-side methodologies. On the server side,\nwe present an asynchronous FL system model that employs a dynamic model\naggregation technique, which harmonizes local model updates with the global\nmodel to enhance both accuracy and efficiency. Concurrently, on the device\nside, we propose an adaptive model adjustment mechanism that integrates the\nlatest global model with local models during training to further elevate\naccuracy. Subsequently, we extend this approach to a synchronous context,\nreferred to as FedSSMU. Theoretical analyses substantiate the convergence of\nour proposed methodologies. Extensive experiments, encompassing six models and\nfive public datasets, demonstrate that FedASMU and FedSSMU significantly\nsurpass baseline methods in terms of both accuracy (up to 145.87%) and\nefficiency (up to 97.59%).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff08FedASMU\u548cFedSSMU\uff09\uff0c\u5229\u7528\u989d\u5916\u4e0b\u884c\u5e26\u5bbd\u8d44\u6e90\uff0c\u901a\u8fc7\u5f02\u6b65\u548c\u540c\u6b65\u6846\u67b6\u63d0\u5347\u6a21\u578b\u66f4\u65b0\u7684\u53ca\u65f6\u6027\u3001\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u5206\u5e03\u5f0f\u6570\u636e\u7ba1\u7406\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u66f4\u65b0\u4f20\u64ad\u7684\u53ca\u65f6\u6027\u548c\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u670d\u52a1\u5668\u7aef\u548c\u8bbe\u5907\u7aef\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5f02\u6b65\u6846\u67b6FedASMU\uff08\u52a8\u6001\u6a21\u578b\u805a\u5408\u548c\u81ea\u9002\u5e94\u6a21\u578b\u8c03\u6574\uff09\u548c\u540c\u6b65\u6846\u67b6FedSSMU\uff0c\u7ed3\u5408\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedASMU\u548cFedSSMU\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u51c6\u786e\u6027\u63d0\u5347\u9ad8\u8fbe145.87%\uff0c\u6548\u7387\u63d0\u5347\u9ad8\u8fbe97.59%\uff09\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u5206\u5e03\u5f0f\u6570\u636e\u7ba1\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05681", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05681", "abs": "https://arxiv.org/abs/2507.05681", "authors": ["Muhammad Hadir Khan", "Matthew Guthaus"], "title": "GATMesh: Clock Mesh Timing Analysis using Graph Neural Networks", "comment": null, "summary": "Clock meshes are essential in high-performance VLSI systems for minimizing\nskew and handling PVT variations, but analyzing them is difficult due to\nreconvergent paths, multi-source driving, and input mesh buffer skew. SPICE\nsimulations are accurate but slow; yet simplified models miss key effects like\nslew and input skew. We propose GATMesh, a Graph Neural Network (GNN)-based\nframework that models the clock mesh as a graph with augmented structural and\nphysical features. Trained on SPICE data, GATMesh achieves high accuracy with\naverage delay error of 5.27ps on unseen benchmarks, while achieving speed-ups\nof 47146x over multi-threaded SPICE simulation.", "AI": {"tldr": "GATMesh\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u5206\u6790\u65f6\u949f\u7f51\u683c\uff0c\u663e\u8457\u63d0\u5347\u901f\u5ea6\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u65f6\u949f\u7f51\u683c\u5728\u9ad8\u6027\u80fdVLSI\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\uff08\u5982SPICE\u4eff\u771f\uff09\u901f\u5ea6\u6162\uff0c\u7b80\u5316\u6a21\u578b\u53c8\u5ffd\u7565\u5173\u952e\u6548\u5e94\u3002", "method": "\u63d0\u51faGATMesh\uff0c\u5c06\u65f6\u949f\u7f51\u683c\u5efa\u6a21\u4e3a\u5e26\u6709\u589e\u5f3a\u7ed3\u6784\u548c\u7269\u7406\u7279\u5f81\u7684\u56fe\uff0c\u5229\u7528GNN\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u5ef6\u8fdf\u8bef\u5dee\u4e3a5.27ps\uff0c\u901f\u5ea6\u6bd4\u591a\u7ebf\u7a0bSPICE\u4eff\u771f\u5feb47146\u500d\u3002", "conclusion": "GATMesh\u5728\u65f6\u949f\u7f51\u683c\u5206\u6790\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.06107", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.06107", "abs": "https://arxiv.org/abs/2507.06107", "authors": ["Junaid Ahmed Khan", "Andrea Bartolini"], "title": "A Unified Ontology for Scalable Knowledge Graph-Driven Operational Data Analytics in High-Performance Computing Systems", "comment": "12 pages", "summary": "Modern high-performance computing (HPC) systems generate massive volumes of\nheterogeneous telemetry data from millions of sensors monitoring compute,\nmemory, power, cooling, and storage subsystems. As HPC infrastructures scale to\nsupport increasingly complex workloads-including generative AI-the need for\nefficient, reliable, and interoperable telemetry analysis becomes critical.\nOperational Data Analytics (ODA) has emerged to address these demands; however,\nthe reliance on schema-less storage solutions limits data accessibility and\nsemantic integration. Ontologies and knowledge graphs (KG) provide an effective\nway to enable efficient and expressive data querying by capturing domain\nsemantics, but they face challenges such as significant storage overhead and\nthe limited applicability of existing ontologies, which are often tailored to\nspecific HPC systems only. In this paper, we present the first unified ontology\nfor ODA in HPC systems, designed to enable semantic interoperability across\nheterogeneous data centers. Our ontology models telemetry data from the two\nlargest publicly available ODA datasets-M100 (Cineca, Italy) and F-DATA\n(Fugaku, Japan)-within a single data model. The ontology is validated through\n36 competency questions reflecting real-world stakeholder requirements, and we\nintroduce modeling optimizations that reduce knowledge graph (KG) storage\noverhead by up to 38.84% compared to a previous approach, with an additional\n26.82% reduction depending on the desired deployment configuration. This work\npaves the way for scalable ODA KGs and supports not only analysis within\nindividual systems, but also cross-system analysis across heterogeneous HPC\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684HPC\u7cfb\u7edfODA\u672c\u4f53\uff0c\u652f\u6301\u5f02\u6784\u6570\u636e\u4e2d\u5fc3\u95f4\u7684\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\uff0c\u4f18\u5316\u5b58\u50a8\u5f00\u9500\u5e76\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3HPC\u7cfb\u7edf\u4e2d\u5f02\u6784\u9065\u6d4b\u6570\u636e\u7684\u8bed\u4e49\u96c6\u6210\u548c\u9ad8\u6548\u67e5\u8be2\u95ee\u9898\uff0c\u514b\u670d\u73b0\u6709\u672c\u4f53\u5b58\u50a8\u5f00\u9500\u5927\u548c\u9002\u7528\u6027\u6709\u9650\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684ODA\u672c\u4f53\uff0c\u57fa\u4e8e\u4e24\u4e2a\u5927\u578b\u516c\u5f00\u6570\u636e\u96c6\uff08M100\u548cF-DATA\uff09\uff0c\u5f15\u5165\u5efa\u6a21\u4f18\u5316\u4ee5\u51cf\u5c11\u5b58\u50a8\u5f00\u9500\u3002", "result": "\u5b58\u50a8\u5f00\u9500\u51cf\u5c1138.84%\uff08\u4e0e\u4e4b\u524d\u65b9\u6cd5\u76f8\u6bd4\uff09\uff0c\u6839\u636e\u914d\u7f6e\u53ef\u989d\u5916\u51cf\u5c1126.82%\uff0c\u652f\u6301\u8de8\u7cfb\u7edf\u5206\u6790\u3002", "conclusion": "\u8be5\u672c\u4f53\u4e3a\u53ef\u6269\u5c55\u7684ODA\u77e5\u8bc6\u56fe\u8c31\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u652f\u6301\u5f02\u6784HPC\u7cfb\u7edf\u7684\u8de8\u7cfb\u7edf\u5206\u6790\u3002"}}
{"id": "2507.06069", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.06069", "abs": "https://arxiv.org/abs/2507.06069", "authors": ["Atiyeh Gheibi-Fetrat", "Amirsaeed Ahmadi-Tonekaboni", "Farzam Koohi-Ronaghi", "Pariya Hajipour", "Sana Babayan-Vanestan", "Fatemeh Fotouhi", "Elahe Mortazavian-Farsani", "Pouria Khajehpour-Dezfouli", "Sepideh Safari", "Shaahin Hessabi", "Hamid Sarbazi-Azad"], "title": "RTGPU: Real-Time Computing with Graphics Processing Units", "comment": "This document provides a concise summary of the book RTGPU, submitted\n  to Synthesis Lectures on Computer Architecture. Due to copyright\n  restrictions, the full content is not reproduced here; readers are referred\n  to the complete book for more comprehensive details", "summary": "In this work, we survey the role of GPUs in real-time systems. Originally\ndesigned for parallel graphics workloads, GPUs are now widely used in\ntime-critical applications such as machine learning, autonomous vehicles, and\nrobotics due to their high computational throughput. Their parallel\narchitecture is well-suited for accelerating complex tasks under strict timing\nconstraints. However, their integration into real-time systems presents several\nchallenges, including non-preemptive execution, execution time variability, and\nresource contention; factors that can lead to unpredictable delays and deadline\nviolations. We examine existing solutions that address these challenges,\nincluding scheduling algorithms, resource management techniques, and\nsynchronization methods, and highlight open research directions to improve GPU\npredictability and performance in real-time environments.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86GPU\u5728\u5b9e\u65f6\u7cfb\u7edf\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u6311\u6218\uff0c\u63a2\u8ba8\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "GPU\u56e0\u5176\u9ad8\u8ba1\u7b97\u541e\u5410\u91cf\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5b9e\u65f6\u7cfb\u7edf\uff0c\u4f46\u5176\u96c6\u6210\u5e26\u6765\u975e\u62a2\u5360\u5f0f\u6267\u884c\u3001\u6267\u884c\u65f6\u95f4\u53ef\u53d8\u6027\u7b49\u6311\u6218\u3002", "method": "\u901a\u8fc7\u8c03\u5ea6\u7b97\u6cd5\u3001\u8d44\u6e90\u7ba1\u7406\u6280\u672f\u548c\u540c\u6b65\u65b9\u6cd5\u7b49\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u5206\u6790\u3002", "result": "\u603b\u7ed3\u4e86GPU\u5728\u5b9e\u65f6\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u9ad8GPU\u5728\u5b9e\u65f6\u73af\u5883\u4e2d\u7684\u53ef\u9884\u6d4b\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2507.06127", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06127", "abs": "https://arxiv.org/abs/2507.06127", "authors": ["Dongsheng Zuo", "Jiadong Zhu", "Yang Luo", "Yuzhe Ma"], "title": "PrefixAgent: An LLM-Powered Design Framework for Efficient Prefix Adder Optimization", "comment": null, "summary": "Prefix adders are fundamental arithmetic circuits, but their design space\ngrows exponentially with bit-width, posing significant optimization challenges.\nPrevious works face limitations in performance, generalization, and\nscalability. To address these challenges, we propose PrefixAgent, a large\nlanguage model (LLM)-powered framework that enables efficient prefix adder\noptimization. Specifically, PrefixAgent reformulates the problem into subtasks\nincluding backbone synthesis and structure refinement, which effectively\nreduces the search space. More importantly, this new design perspective enables\nus to efficiently collect enormous high-quality data and reasoning traces with\nE-graph, which further results in an effective fine-tuning of LLM. Experimental\nresults show that PrefixAgent synthesizes prefix adders with consistently\nsmaller areas compared to baseline methods, while maintaining scalability and\ngeneralization in commercial EDA flows.", "AI": {"tldr": "PrefixAgent\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u4f18\u5316\u524d\u7f00\u52a0\u6cd5\u5668\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u548c\u5229\u7528E-graph\u6570\u636e\uff0c\u663e\u8457\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u524d\u7f00\u52a0\u6cd5\u5668\u7684\u8bbe\u8ba1\u7a7a\u95f4\u968f\u4f4d\u5bbd\u6307\u6570\u589e\u957f\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u6cdb\u5316\u548c\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "PrefixAgent\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff08\u5982\u4e3b\u5e72\u5408\u6210\u548c\u7ed3\u6784\u4f18\u5316\uff09\uff0c\u5229\u7528E-graph\u6536\u96c6\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u63a8\u7406\u8f68\u8ff9\uff0c\u5e76\u5fae\u8c03LLM\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPrefixAgent\u751f\u6210\u7684\u524d\u7f00\u52a0\u6cd5\u5668\u9762\u79ef\u66f4\u5c0f\uff0c\u540c\u65f6\u5728\u5546\u4e1aEDA\u6d41\u7a0b\u4e2d\u4fdd\u6301\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PrefixAgent\u901a\u8fc7LLM\u548c\u4efb\u52a1\u5206\u89e3\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u524d\u7f00\u52a0\u6cd5\u5668\u4f18\u5316\u65b9\u6848\u3002"}}
