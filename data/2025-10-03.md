<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Odontoceti: Ultra-Fast DAG Consensus with Two Round Commitment](https://arxiv.org/abs/2510.01216)
*Preston Vander Vos*

Main category: cs.DC

TL;DR: Odontoceti是一种基于DAG的共识协议，通过将故障容错从33%降低到20%，在2轮通信内完成确认，实现300毫秒中位延迟和10,000 TPS的高性能。


<details>
  <summary>Details</summary>
Motivation: 解决区块链用户对可扩展性的需求，追求低延迟和高吞吐量，通过降低故障容错来提升性能。

Method: 采用n = 5f + 1验证者架构，构建未认证DAG并使用新颖的决策规则提交区块，包含针对崩溃故障的优化机制。

Result: 在现实网络条件下实现300毫秒中位延迟和10,000 TPS，相比现有生产协议延迟降低20-25%。

Conclusion: 证明了低故障容错共识协议在区块链中的实际可行性，两轮通信的波长度显著提升了性能。

Abstract: Users of blockchains value scalability, expecting fast confirmations and
immediate transaction processing. Odontoceti, the latest in DAG-based
consensus, addresses these concerns by prioritizing low latency and high
throughput, making a strategic trade-off in security by operating with a 20%
fault tolerance instead of the established 33% level. It is the first DAG-based
protocol to achieve commitment in just two communication rounds, delivering
median latency of 300 milliseconds while processing 10,000 transactions per
second under realistic network conditions. Odontoceti operates with n = 5f + 1
validators and creates an uncertified DAG with a novel decision rule for
committing blocks. The protocol includes an optimization that advances progress
when participants are slow, benefiting crash fault scenarios which are more
common in practice than Byzantine faults. Evaluation results demonstrate 20-25%
latency improvements compared to an existing production protocol, validating
that reducing wave length from three rounds to two rounds yields meaningful
performance benefits. This paper establishes the practical viability of lower
fault tolerance consensus protocols for blockchains.

</details>


### [2] [Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters](https://arxiv.org/abs/2510.01256)
*Lingling Zeng,Gen Zhang,Jialin Peng,Xiang Xu,Yuan Xu,Lijun Ma*

Main category: cs.DC

TL;DR: Kant是一个高效统一的大规模AI容器集群调度平台，支持训练和推理任务的协同调度，通过Backfill和增强Binpack等策略显著提升资源利用率和调度效率。


<details>
  <summary>Details</summary>
Motivation: 随着AI集群规模扩大和大语言模型训练/推理需求快速增长，传统调度系统在资源利用率、调度效率和服务质量方面面临挑战。

Method: 基于Kant系统的实际实现，采用Backfill和增强Binpack等调度策略，定义了一套AI集群关键评估指标（GAR、SOR、GFR、JWTD、JTTED）。

Result: 实验结果表明Kant在数百到数万GPU规模的集群中表现优异，显著提高资源利用率、减少资源碎片化和分布式训练通信开销。

Conclusion: Kant系统已在多个AI数据中心集群部署，稳定支持大规模智能计算工作负载，为构建高性能、高可用的AI原生调度基础设施提供了实用工程方案。

Abstract: As AI cluster sizes continue to expand and the demand for
large-language-model (LLM) training and inference workloads grows rapidly,
traditional scheduling systems face significant challenges in balancing
resource utilization, scheduling efficiency, and service quality. This paper
presents and evaluates Kant: an efficient unified scheduling platform designed
for large-scale AI container clusters, supporting the co-scheduling of both
training and inference jobs. Based on the practical implementation of the Kant
system, we systematically define a set of key evaluation metrics for AI
clusters, including GPU Allocation Ratio (GAR), Scheduling Occupancy Rate
(SOR), GPU Node Fragmentation Ratio (GFR), Job Waiting Time Distribution
(JWTD), and Job Training Time Estimation Distribution (JTTED), providing a
foundation for quantitative performance analysis. Experimental results
demonstrate that Kant achieves exceptional performance in clusters ranging from
hundreds to tens of thousands of GPUs. By leveraging scheduling strategies such
as Backfill and Enhanced Binpack (E-Binpack), the system significantly improves
resource utilization and scheduling efficiency, while effectively reducing
resource fragmentation and communication overhead in distributed training. The
system has been deployed in multiple AI data center clusters, where it stably
supports large-scale intelligent computing workloads. This work provides a
practical engineering approach for building high-performance, highly available,
AI-native scheduling infrastructure.

</details>


### [3] [IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol](https://arxiv.org/abs/2510.01260)
*Ningyuan Yang,Guanliang Lyu,Mingchen Ma,Yiyi Lu,Yiming Li,Zhihui Gao,Hancheng Ye,Jianyi Zhang,Tingjun Chen,Yiran Chen*

Main category: cs.DC

TL;DR: 提出了IoT-MCP框架，通过边缘部署的MCP服务器连接LLM与物联网设备，并创建了首个包含114个基础任务和1140个复杂任务的IoT-MCP Bench基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型与物联网系统集成时面临的硬件异构性和控制复杂性挑战。

Method: 使用模型上下文协议(MCP)作为标准化通信机制，通过边缘部署的服务器桥接LLM和物联网生态系统。

Result: 在22种传感器类型和6种微控制器单元上验证，实现了100%任务成功率、205ms平均响应时间和74KB峰值内存占用。

Conclusion: 提供了一个开源集成框架和标准化评估方法，用于LLM-IoT系统的开发和评估。

Abstract: The integration of Large Language Models (LLMs) with Internet-of-Things (IoT)
systems faces significant challenges in hardware heterogeneity and control
complexity. The Model Context Protocol (MCP) emerges as a critical enabler,
providing standardized communication between LLMs and physical devices. We
propose IoT-MCP, a novel framework that implements MCP through edge-deployed
servers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we
introduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g.,
``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel
so hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation
across 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100%
task success rate to generate tool calls that fully meet expectations and
obtain completely accurate results, 205ms average response time, and 74KB peak
memory footprint. This work delivers both an open-source integration framework
(https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized
evaluation methodology for LLM-IoT systems.

</details>


### [4] [QScale: Probabilistic Chained Consensus for Moderate-Scale Systems](https://arxiv.org/abs/2510.01536)
*Hasan Heydari,Alysson Bessani,Kartik Nayak*

Main category: cs.DC

TL;DR: QScale是一个针对中等规模分布式账本设计的协议，通过子线性通信复杂度和低延迟实现高效共识，填补了PBFT和Algorand之间的空白。


<details>
  <summary>Details</summary>
Motivation: 现有分布式账本协议要么通信复杂度高（如PBFT），要么需要大量进程（如Algorand），都不适合几百到一千个进程的中等规模系统。

Method: 提出QScale协议，采用子线性通信复杂度设计，每个进程每块通信复杂度为$\widetilde{O}(\kappa \sqrt{n})$，总通信复杂度为$\widetilde{O}(n\kappa)$。

Result: 协议在最佳情况下具有$O(\kappa)$轮延迟，同时以压倒性概率保证安全性和活性。

Conclusion: QScale为中等规模分布式账本系统提供了可行的解决方案，平衡了通信复杂度和系统规模的要求。

Abstract: Existing distributed ledger protocols either incur a high communication
complexity and are thus suited to systems with a small number of processes
(e.g., PBFT), or rely on committee-sampling-based approaches that only work for
a very large number of processes (e.g., Algorand). Neither of these lines of
work is well-suited for moderate-scale distributed ledgers ranging from a few
hundred to a thousand processes, which are common in production (e.g, Redbelly,
Sui). The goal of this work is to design a distributed ledger with sub-linear
communication complexity per process, sub-quadratic total communication
complexity, and low latency for finalizing a block into the ledger, such that
it can be used for moderate-scale systems. We propose QScale, a protocol in
which every process incurs only $\widetilde{O}(\kappa \sqrt{n})$ communication
complexity per-block in expectation, $\widetilde{O}(n\kappa)$ total
communication complexity per-block in expectation, and a best-case latency of
$O(\kappa)$ rounds while ensuring safety and liveness with overwhelming
probability, with $\kappa$ being a small security parameter.

</details>


### [5] [Accuracy vs Performance: An abstraction model for deadline constrained offloading at the mobile-edge](https://arxiv.org/abs/2510.01885)
*Jamie Cotter,Ignacio Castineiras,Victor Cionca*

Main category: cs.DC

TL;DR: 提出了一种用于移动边缘设备上低延迟、截止时间约束的DNN卸载调度算法，通过轻量级网络状态表示、动态带宽估计和优先级感知抢占机制来减少延迟。


<details>
  <summary>Details</summary>
Motivation: 解决移动边缘设备上DNN任务卸载时的低延迟和截止时间约束问题，特别是在高负载情况下需要提高任务吞吐量。

Method: 设计了包含设备可用性、网络链路通信、优先级感知抢占和任务截止时间的调度算法，采用资源可用性表示、网络离散化和动态带宽估计机制。

Result: 在由四个树莓派2组成的移动边缘设备系统上测试，与先前方法相比，在高负载下表现出更好的性能，动态带宽估计有助于任务放置并提高资源稀缺时的任务吞吐量。

Conclusion: 新的低延迟抽象模型在高负载工作负载下表现更好，动态带宽估计机制在资源稀缺时能有效提升任务吞吐量。

Abstract: In this paper, we present a solution for low-latency deadline-constrained DNN
offloading on mobile edge devices. We design a scheduling algorithm with
lightweight network state representation, considering device availability,
communication on the network link, priority-aware pre-emption, and task
deadlines. The scheduling algorithm aims to reduce latency by designing a
resource availability representation, as well as a network discretisation and a
dynamic bandwidth estimation mechanism. We implement the scheduling algorithm
into a system composed of four Raspberry Pi 2 (model Bs) mobile edge devices,
sampling a waste classification conveyor belt at a set frame rate. The system
is evaluated and compared to a previous approach of ours, which was proven to
outcompete work-stealers and a non-pre-emption based scheduling heuristic under
the aforementioned waste classification scenario. Our findings show the novel
lower latency abstraction models yield better performance under high-volume
workloads, with the dynamic bandwidth estimation assisting the task placement
while, ultimately, increasing task throughput in times of resource scarcity.

</details>


### [6] [Programming RISC-V accelerators via Fortran](https://arxiv.org/abs/2510.02170)
*Nick Brown,Jake Davies,Felix LeClair*

Main category: cs.DC

TL;DR: 提出一种通过Fortran驱动RISC-V加速器的方法，避免为新型加速器重新开发科学计算代码


<details>
  <summary>Details</summary>
Motivation: RISC-V加速器具有HPC潜力，但需要专门的编程模型，而科学计算中大量复杂代码使用Fortran编写，重写不现实

Method: 开发一种方法，使Fortran代码能够直接驱动RISC-V加速器架构

Result: 该方法避免了代码重新开发，保持了现有Fortran代码的可用性

Conclusion: 该方法为在RISC-V加速器上运行现有Fortran科学计算代码提供了可行途径

Abstract: A range of RISC-V based accelerators are available and coming to market, and
there is strong potential for these to be used for High Performance Computing
(HPC) workloads. However, such accelerators tend to provide bespoke programming
models and APIs that require codes to be rewritten. In scientific computing,
where many of the simulation code are highly complex, extensive, and written in
Fortran, this is not realistic. In this extended abstract we present an
approach that enables driving such architectures via Fortran, avoiding code
redevelopment.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [Edge GPU Aware Multiple AI Model Pipeline for Accelerated MRI Reconstruction and Analysis](https://arxiv.org/abs/2510.01730)
*Ashiyana Abdul Majeed,Mahmoud Meribout,Safa Mohammed Sali*

Main category: cs.AR

TL;DR: 提出了一种硬件加速的多模型系统，用于从CT图像同时进行MRI重建和诊断，通过优化硬件引擎分配和模型微调，在NVIDIA边缘GPU上实现了近150帧/秒的实时性能，且准确率提升5%。


<details>
  <summary>Details</summary>
Motivation: AI在医学影像处理中取得了显著进展，但很少有人研究如何优化具有硬件加速的多模型系统。随着专用边缘设备的出现，有效利用其加速器变得越来越重要。

Method: 利用现代NVIDIA边缘GPU中的硬件引擎（GPU和DLA）以及调度技术，优化多个AI模型中不同层的硬件分配，减少硬件引擎间的理想时间。对GAN模型进行微调，避免回退到GPU引擎执行。

Result: 在Jetson AGX Xavier和Orin设备上实现了近150帧/秒的吞吐量，微调后的边缘GPU感知AI模型准确率提升了5%，两个微调模型的硬件分配使性能比原始模型翻倍。

Conclusion: 结果表明，在医学图像分析和诊断中采用硬件感知并行模型是有效的。

Abstract: Advancements in AI have greatly enhanced the medical imaging process, making
it quicker to diagnose patients. However, very few have investigated the
optimization of a multi-model system with hardware acceleration. As specialized
edge devices emerge, the efficient use of their accelerators is becoming
increasingly crucial. This paper proposes a hardware-accelerated method for
simultaneous reconstruction and diagnosis of \ac{MRI} from \ac{CT} images.
Real-time performance of achieving a throughput of nearly 150 frames per second
was achieved by leveraging hardware engines available in modern NVIDIA edge
GPU, along with scheduling techniques. This includes the GPU and the \ac{DLA}
available in both Jetson AGX Xavier and Jetson AGX Orin, which were considered
in this paper. The hardware allocation of different layers of the multiple AI
models was done in such a way that the ideal time between the hardware engines
is reduced. In addition, the AI models corresponding to the \ac{GAN} model were
fine-tuned in such a way that no fallback execution into the GPU engine is
required without compromising accuracy. Indeed, the accuracy corresponding to
the fine-tuned edge GPU-aware AI models exhibited an accuracy enhancement of
5\%. A further hardware allocation of two fine-tuned GPU-aware GAN models
proves they can double the performance over the original model, leveraging
adequate partitioning on the NVIDIA Jetson AGX Xavier and Orin devices. The
results prove the effectiveness of employing hardware-aware models in parallel
for medical image analysis and diagnosis.

</details>


### [8] [Multiplier-free In-Memory Vector-Matrix Multiplication Using Distributed Arithmetic](https://arxiv.org/abs/2510.02099)
*Felix Zeller,John Reuben,Dietmar Fey*

Main category: cs.AR

TL;DR: 本文提出了一种基于分布式算法(DA)的向量矩阵乘法(VMM)方法，通过在ReRAM内存中存储权重和，使用移位加法电路实现VMM，完全消除了功耗高的ADC需求。


<details>
  <summary>Details</summary>
Motivation: 传统内存计算中的VMM需要大量ADC/DAC，消耗显著功率和面积。分布式算法可以在不使用硬件乘法器的情况下实现向量内积，特别适用于其中一个向量是常数的情况。

Method: 扩展DA技术实现输入向量与常数矩阵的乘法，在ReRAM内存中存储权重和，使用外围的移位加法电路完成VMM计算。

Result: 通过晶体管级仿真验证，相比传统的位切片方法，延迟减少4.5倍，能耗减少12倍，完全消除了功耗高的ADC需求。

Conclusion: 基于DA的VMM方法在延迟、能耗和面积方面显著优于传统内存计算方法，特别适合神经网络推理应用。

Abstract: Vector-Matrix Multiplication (VMM) is the fundamental and frequently required
computation in inference of Neural Networks (NN). Due to the large data
movement required during inference, VMM can benefit greatly from in-memory
computing. However, ADC/DACs required for in-memory VMM consume significant
power and area. `Distributed Arithmetic (DA)', a technique in computer
architecture prevalent in 1980s was used to achieve inner product or dot
product of two vectors without using a hard-wired multiplier when one of the
vectors is a constant. In this work, we extend the DA technique to multiply an
input vector with a constant matrix. By storing the sum of the weights in
memory, DA achieves VMM using shift-and-add circuits in the periphery of ReRAM
memory. We verify functional and also estimate non-functional properties
(latency, energy, area) by performing transistor-level simulations. Using
energy-efficient sensing and fine grained pipelining, our approach achieves 4.5
x less latency and 12 x less energy than VMM performed in memory conventionally
by bit slicing. Furthermore, DA completely eliminated the need for power-hungry
ADCs which are the main source of area and energy consumption in the current
VMM implementations in memory.

</details>
