{"id": "2508.21256", "categories": ["cs.PL", "cs.CL", "cs.GR", "68N20, 68N15, 68W10", "D.3.4; D.3.2; D.1.3"], "pdf": "https://arxiv.org/pdf/2508.21256", "abs": "https://arxiv.org/abs/2508.21256", "authors": ["Nripesh Niketan", "Vaatsalya Shrivastva"], "title": "CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation", "comment": "15 Pages, 5 Figures, 1 Table. Introduces CrossTL, a universal\n  programming language translator enabling bidirectional translation between 8\n  programming languages (CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan\n  SPIR-V, Rust, Mojo) through a unified intermediate representation called\n  CrossGL. Includes comprehensive evaluation with complex real-world examples", "summary": "We present CrossTL, a universal programming language translator enabling\nbidirectional translation between multiple languages through a unified\nintermediate representation called CrossGL. Traditional approaches require\nseparate translators for each language pair, leading to exponential complexity\ngrowth. CrossTL uses a single universal IR to facilitate translations between\nCUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,\nwith Slang support in development. Our system consists of: language-specific\nlexers/parsers converting source code to ASTs, bidirectional CrossGL\ntranslation modules implementing ToCrossGLConverter classes for importing code\nand CodeGen classes for target generation, and comprehensive backend\nimplementations handling full translation pipelines. We demonstrate\neffectiveness through comprehensive evaluation across programming domains,\nachieving successful compilation and execution across all supported backends.\nThe universal IR design enables adding new languages with minimal effort,\nrequiring only language-specific frontend/backend components. Our contributions\ninclude: (1) a unified IR capturing semantics of multiple programming\nparadigms, (2) a modular architecture enabling extensibility, (3) a\ncomprehensive framework supporting GPU compute, graphics programming, and\nsystems languages, and (4) empirical validation demonstrating practical\nviability of universal code translation. CrossTL represents a significant step\ntoward language-agnostic programming, enabling write-once, deploy-everywhere\ndevelopment.", "AI": {"tldr": "CrossTL\u662f\u4e00\u4e2a\u901a\u7528\u7684\u7f16\u7a0b\u8bed\u8a00\u7ffb\u8bd1\u5668\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u4e2d\u95f4\u8868\u793aCrossGL\u5b9e\u73b0\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u4e4b\u95f4\u7684\u53cc\u5411\u7ffb\u8bd1\uff0c\u652f\u6301CUDA\u3001HIP\u3001Metal\u3001HLSL\u3001GLSL\u3001SPIR-V\u3001Rust\u548cMojo\u7b49\u8bed\u8a00\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u5bf9\u8bed\u8a00\u5355\u72ec\u5f00\u53d1\u7ffb\u8bd1\u5668\uff0c\u5bfc\u81f4\u590d\u6742\u5ea6\u5448\u6307\u6570\u7ea7\u589e\u957f\u3002CrossTL\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u7684\u4e2d\u95f4\u8868\u793a\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5b9e\u73b0\"\u4e00\u6b21\u7f16\u5199\uff0c\u5230\u5904\u90e8\u7f72\"\u7684\u5f00\u53d1\u6a21\u5f0f\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u8bed\u8a00\u7279\u5b9a\u7684\u8bcd\u6cd5\u5206\u6790\u5668/\u89e3\u6790\u5668\u5c06\u6e90\u4ee3\u7801\u8f6c\u6362\u4e3aAST\uff0c\u53cc\u5411CrossGL\u7ffb\u8bd1\u6a21\u5757\uff08ToCrossGLConverter\u7c7b\u7528\u4e8e\u5bfc\u5165\u4ee3\u7801\uff0cCodeGen\u7c7b\u7528\u4e8e\u76ee\u6807\u751f\u6210\uff09\uff0c\u4ee5\u53ca\u5904\u7406\u5b8c\u6574\u7ffb\u8bd1\u6d41\u7a0b\u7684\u540e\u7aef\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u8de8\u7f16\u7a0b\u9886\u57df\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u8bc1\u660e\u7cfb\u7edf\u5728\u6240\u6709\u652f\u6301\u7684\u540e\u7aef\u4e0a\u90fd\u80fd\u6210\u529f\u7f16\u8bd1\u548c\u6267\u884c\uff0c\u9a8c\u8bc1\u4e86\u901a\u7528\u4ee3\u7801\u7ffb\u8bd1\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "CrossTL\u4ee3\u8868\u4e86\u5411\u8bed\u8a00\u65e0\u5173\u7f16\u7a0b\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u7edf\u4e00\u7684IR\u8bbe\u8ba1\u4f7f\u5f97\u6dfb\u52a0\u65b0\u8bed\u8a00\u53ea\u9700\u6700\u5c11\u7684\u52aa\u529b\uff0c\u4ec5\u9700\u8981\u8bed\u8a00\u7279\u5b9a\u7684\u524d\u7aef/\u540e\u7aef\u7ec4\u4ef6\u3002"}}
{"id": "2508.21593", "categories": ["cs.PL", "cs.MS", "math.HO"], "pdf": "https://arxiv.org/pdf/2508.21593", "abs": "https://arxiv.org/abs/2508.21593", "authors": ["Anne Baanen", "Matthew Robert Ballard", "Johan Commelin", "Bryan Gin-ge Chen", "Michael Rothgang", "Damiano Testa"], "title": "Growing Mathlib: maintenance of a large scale mathematical library", "comment": "21 pages, 1 figure. To appear at Conference on Intelligent Computer\n  Mathematics (CICM) 2025", "summary": "The Lean mathematical library Mathlib is one of the fastest-growing libraries\nof formalised mathematics. We describe various strategies to manage this\ngrowth, while allowing for change and avoiding maintainer overload. This\nincludes dealing with breaking changes via a deprecation system, using code\nquality analysis tools (linters) to provide direct user feedback about common\npitfalls, speeding up compilation times through conscious library (re-)design,\ndealing with technical debt as well as writing custom tooling to help with the\nreview and triage of new contributions.", "AI": {"tldr": "Mathlib\u662f\u589e\u957f\u6700\u5feb\u7684\u6570\u5b66\u5f62\u5f0f\u5316\u5e93\u4e4b\u4e00\uff0c\u672c\u6587\u63cf\u8ff0\u4e86\u7ba1\u7406\u5176\u589e\u957f\u3001\u5141\u8bb8\u53d8\u66f4\u5e76\u907f\u514d\u7ef4\u62a4\u8005\u8fc7\u8f7d\u7684\u5404\u79cd\u7b56\u7565\u3002", "motivation": "\u968f\u7740Mathlib\u6570\u5b66\u5e93\u7684\u5feb\u901f\u589e\u957f\uff0c\u9700\u8981\u6709\u6548\u7684\u7b56\u7565\u6765\u7ba1\u7406\u5e93\u7684\u6269\u5c55\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7801\u8d28\u91cf\u548c\u7ef4\u62a4\u6548\u7387\uff0c\u907f\u514d\u7ef4\u62a4\u8005\u8d1f\u62c5\u8fc7\u91cd\u3002", "method": "\u91c7\u7528\u591a\u79cd\u7b56\u7565\uff1a\u901a\u8fc7\u5f03\u7528\u7cfb\u7edf\u5904\u7406\u7834\u574f\u6027\u53d8\u66f4\u3001\u4f7f\u7528\u4ee3\u7801\u8d28\u91cf\u5206\u6790\u5de5\u5177\uff08linter\uff09\u63d0\u4f9b\u7528\u6237\u53cd\u9988\u3001\u901a\u8fc7\u6709\u610f\u8bc6\u7684\u5e93\uff08\u91cd\u65b0\uff09\u8bbe\u8ba1\u52a0\u901f\u7f16\u8bd1\u65f6\u95f4\u3001\u5904\u7406\u6280\u672f\u503a\u52a1\u4ee5\u53ca\u7f16\u5199\u81ea\u5b9a\u4e49\u5de5\u5177\u5e2e\u52a9\u5ba1\u67e5\u548c\u5206\u7c7b\u65b0\u8d21\u732e\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u5957\u7efc\u5408\u7684\u5e93\u7ba1\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u5927\u578b\u6570\u5b66\u5f62\u5f0f\u5316\u5e93\u7684\u589e\u957f\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u7ef4\u62a4\u6548\u7387\u548c\u4ee3\u7801\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u7ba1\u7406\u7b56\u7565\u548c\u5de5\u5177\u652f\u6301\uff0cMathlib\u80fd\u591f\u5728\u5feb\u901f\u589e\u957f\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u548c\u53ef\u7ef4\u62a4\u6027\uff0c\u4e3a\u5927\u578b\u5f62\u5f0f\u5316\u6570\u5b66\u5e93\u7684\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.21265", "categories": ["cs.AR", "cond-mat.supr-con", "cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.21265", "abs": "https://arxiv.org/abs/2508.21265", "authors": ["Sasan Razmkhah", "Mingye Li", "Zeming Cheng", "Robert S. Aviles", "Kyle Jackman", "Joey Delport", "Lieze Schindler", "Wenhui Luo", "Takuya Suzuki", "Mehdi Kamal", "Christopher L. Ayala", "Coenrad J. Fourie", "Nabuyuki Yoshikawa", "Peter A. Beerel", "Sandeep Gupta", "Massoud Pedram"], "title": "SCE-NTT: A Hardware Accelerator for Number Theoretic Transform Using Superconductor Electronics", "comment": "13 pages, 22 figures", "summary": "This research explores the use of superconductor electronics (SCE) for\naccelerating fully homomorphic encryption (FHE), focusing on the\nNumber-Theoretic Transform (NTT), a key computational bottleneck in FHE\nschemes. We present SCE-NTT, a dedicated hardware accelerator based on\nsuperconductive single flux quantum (SFQ) logic and memory, targeting high\nperformance and energy efficiency beyond the limits of conventional CMOS. To\naddress SFQ constraints such as limited dense RAM and restricted fanin/fanout,\nwe propose a deeply pipelined NTT-128 architecture using shift register memory\n(SRM). Designed for N=128 32-bit coefficients, NTT-128 comprises log2(N)=7\nprocessing elements (PEs), each featuring a butterfly unit (BU), dual\ncoefficient memories operating in ping-pong mode via FIFO-based SRM queues, and\ntwiddle factor buffers. The BU integrates a Shoup modular multiplier optimized\nfor a small area, leveraging precomputed twiddle factors. A new RSFQ cell\nlibrary with over 50 parameterized cells, including compound logic units, was\ndeveloped for implementation. Functional and timing correctness were validated\nusing JoSIM analog simulations and Verilog models. A multiphase clocking scheme\nwas employed to enhance robustness and reduce path-balancing overhead,\nimproving circuit reliability. Fabricated results show the NTT-128 unit\nachieves 531 million NTT/sec at 34 GHz, over 100x faster than state-of-the-art\nCMOS equivalents. We also project that the architecture can scale to larger\nsizes, such as a 2^14-point NTT in approximately 482 ns. Key-switch throughput\nis estimated at 1.63 million operations/sec, significantly exceeding existing\nhardware. These results demonstrate the strong potential of SCE-based\naccelerators for scalable, energy-efficient secure computation in the\npost-quantum era, with further gains anticipated through advances in\nfabrication.", "AI": {"tldr": "\u57fa\u4e8e\u8d85\u5bfc\u7535\u5b50\u5b66(SCE)\u7684\u4e13\u7528\u786c\u4ef6\u52a0\u901f\u5668SCE-NTT\uff0c\u9488\u5bf9\u5168\u540c\u6001\u52a0\u5bc6\u4e2d\u7684\u6570\u8bba\u53d8\u6362(NTT)\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86531\u767e\u4e07\u6b21NTT/\u79d2\u7684\u9ad8\u6027\u80fd\uff0c\u6bd4\u6700\u5148\u8fdbCMOS\u65b9\u6848\u5feb100\u500d\u4ee5\u4e0a", "motivation": "\u5168\u540c\u6001\u52a0\u5bc6(FHE)\u4e2d\u7684\u6570\u8bba\u53d8\u6362(NTT)\u662f\u8ba1\u7b97\u74f6\u9888\uff0c\u4f20\u7edfCMOS\u6280\u672f\u9762\u4e34\u6027\u80fd\u9650\u5236\uff0c\u9700\u8981\u63a2\u7d22\u8d85\u5bfc\u7535\u5b50\u5b66\u6765\u7a81\u7834\u6027\u80fd\u8fb9\u754c\u5e76\u5b9e\u73b0\u9ad8\u80fd\u6548", "method": "\u63d0\u51fa\u57fa\u4e8e\u8d85\u5bfc\u5355\u78c1\u901a\u91cf\u5b50(SFQ)\u903b\u8f91\u548c\u5b58\u50a8\u5668\u7684\u4e13\u7528\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u91c7\u7528\u6df1\u5ea6\u6d41\u6c34\u7ebf\u67b6\u6784\uff0c\u4f7f\u7528\u79fb\u4f4d\u5bc4\u5b58\u5668\u5185\u5b58(SRM)\u89e3\u51b3SFQ\u7ea6\u675f\uff0c\u5f00\u53d1\u4e86\u5305\u542b50\u591a\u4e2a\u53c2\u6570\u5316\u5355\u5143\u7684\u65b0RSFQ\u5355\u5143\u5e93", "result": "NTT-128\u5355\u5143\u572834GHz\u9891\u7387\u4e0b\u8fbe\u5230531\u767e\u4e07\u6b21NTT/\u79d2\uff0c\u6bd4\u73b0\u6709CMOS\u65b9\u6848\u5feb100\u500d\u4ee5\u4e0a\uff1b\u53ef\u6269\u5c55\u81f32^14\u70b9NTT\u7ea6482ns\u5b8c\u6210\uff1b\u5bc6\u94a5\u5207\u6362\u541e\u5410\u91cf\u4f30\u8ba1\u4e3a163\u4e07\u6b21\u64cd\u4f5c/\u79d2", "conclusion": "SCE\u57fa\u52a0\u901f\u5668\u5728\u540e\u91cf\u5b50\u65f6\u4ee3\u5177\u6709\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u9ad8\u80fd\u6548\u5b89\u5168\u8ba1\u7b97\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u968f\u7740\u5236\u9020\u6280\u672f\u7684\u8fdb\u6b65\u9884\u8ba1\u5c06\u83b7\u5f97\u66f4\u5927\u6536\u76ca"}}
{"id": "2508.21230", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.21230", "abs": "https://arxiv.org/abs/2508.21230", "authors": ["Brian Curless", "Michael Gowanlock"], "title": "Fast and Scalable Mixed Precision Euclidean Distance Calculations Using GPU Tensor Cores", "comment": "To appear in the proceedings of the International Conference on\n  Parallel Processing 2025", "summary": "Modern GPUs are equipped with tensor cores (TCs) that are commonly used for\nmatrix multiplication in artificial intelligence workloads. However, because\nthey have high computational throughput, they can lead to significant\nperformance gains in other algorithms if they can be successfully exploited. We\nexamine using TCs to compute Euclidean distance calculations, which are used in\nmany data analytics applications. Prior work has only investigated using 64 bit\nfloating point (FP64) data for computation; however, TCs can operate on lower\nprecision floating point data (i.e., 16 bit matrix multiplication and 32 bit\naccumulation), which we refer to as FP16-32. FP16-32 TC peak throughput is so\nhigh that TCs are easily starved of data. We propose a Fast and Scalable Tensor\ncore Euclidean Distance (FaSTED) algorithm. To achieve high computational\nthroughput, we design FaSTED for significant hierarchical reuse of data and\nmaximize memory utilization at every level (global memory, shared memory, and\nregisters). We apply FaSTED to the application of similarity searches, which\ntypically employ an indexing data structure to eliminate superfluous Euclidean\ndistance calculations. We compare to the state-of-the-art (SOTA) TC Euclidean\ndistance algorithm in the literature that employs FP64, as well as to two\nsingle precision (FP32) CUDA core algorithms that both employ an index. We find\nthat across four real-world high-dimensional datasets spanning 128-960\ndimensions, the mixed-precision brute force approach achieves a speedup over\nthe SOTA algorithms of 2.5-51x. We also quantify the accuracy loss of our mixed\nprecision algorithm to be less than <0.06% when compared to the FP64 baseline.", "AI": {"tldr": "\u63d0\u51faFaSTED\u7b97\u6cd5\uff0c\u5229\u7528GPU\u5f20\u91cf\u6838\u5fc3\u7684FP16-32\u6df7\u5408\u7cbe\u5ea6\u8ba1\u7b97\u6b27\u6c0f\u8ddd\u79bb\uff0c\u76f8\u6bd4\u73b0\u6709FP64\u7b97\u6cd5\u83b7\u5f972.5-51\u500d\u52a0\u901f\uff0c\u7cbe\u5ea6\u635f\u5931\u5c0f\u4e8e0.06%", "motivation": "\u73b0\u4ee3GPU\u5f20\u91cf\u6838\u5fc3\u5177\u6709\u9ad8\u8ba1\u7b97\u541e\u5410\u91cf\uff0c\u4f46\u4e4b\u524d\u7814\u7a76\u4ec5\u4f7f\u7528FP64\u7cbe\u5ea6\u3002\u5229\u7528FP16-32\u6df7\u5408\u7cbe\u5ea6\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6b27\u6c0f\u8ddd\u79bb\u8ba1\u7b97\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\u4e2d", "method": "\u8bbe\u8ba1FaSTED\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u6570\u636e\u91cd\u7528\u548c\u6700\u5927\u5316\u5404\u7ea7\u5185\u5b58\uff08\u5168\u5c40\u5185\u5b58\u3001\u5171\u4eab\u5185\u5b58\u3001\u5bc4\u5b58\u5668\uff09\u5229\u7528\u7387\u6765\u5b9e\u73b0\u9ad8\u8ba1\u7b97\u541e\u5410\u91cf\uff0c\u5e94\u7528\u4e8e\u76f8\u4f3c\u6027\u641c\u7d22\u573a\u666f", "result": "\u57284\u4e2a\u771f\u5b9e\u9ad8\u7ef4\u6570\u636e\u96c6\uff08128-960\u7ef4\uff09\u4e0a\uff0c\u6df7\u5408\u7cbe\u5ea6\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709FP64\u7b97\u6cd5\u83b7\u5f972.5-51\u500d\u52a0\u901f\uff0c\u7cbe\u5ea6\u635f\u5931\u5c0f\u4e8e0.06%", "conclusion": "\u5229\u7528\u5f20\u91cf\u6838\u5fc3\u7684FP16-32\u6df7\u5408\u7cbe\u5ea6\u8ba1\u7b97\u6b27\u6c0f\u8ddd\u79bb\u662f\u9ad8\u6548\u53ef\u884c\u7684\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u6570\u636e\u5206\u6790\u548c\u76f8\u4f3c\u6027\u641c\u7d22\u5e94\u7528"}}
{"id": "2508.21267", "categories": ["cs.AR", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.21267", "abs": "https://arxiv.org/abs/2508.21267", "authors": ["Devon Lister", "Prabhu Vellaisamy", "John Paul Shen", "Di Wu"], "title": "Catwalk: Unary Top-K for Efficient Ramp-No-Leak Neuron Design for Temporal Neural Networks", "comment": "Amar Mukherjee Best Paper Award of ISVLSI 2025", "summary": "Temporal neural networks (TNNs) are neuromorphic neural networks that utilize\nbit-serial temporal coding. TNNs are composed of columns, which in turn employ\nneurons as their building blocks. Each neuron processes volleys of input\nspikes, modulated by associated synaptic weights, on its dendritic inputs.\nRecently proposed neuron implementation in CMOS employs a Spike Response Model\n(SRM) with a ramp-no-leak (RNL) response function and assumes all the inputs\ncan carry spikes. However, in actual spike volleys, only a small subset of the\ndendritic inputs actually carry spikes in each compute cycle. This form of\nsparsity can be exploited to achieve better hardware efficiency. In this paper,\nwe propose a Catwalk neuron implementation by relocating spikes in a spike\nvolley as a sorted subset cluster via unary top-k. Such relocation can\nsignificantly reduce the cost of the subsequent parallel counter (PC) for\naccumulating the response functions from the spiking inputs. This can lead to\nimprovements on area and power efficiency in RNL neuron implementation.\nPlace-and-route results show Catwalk is 1.39x and 1.86x better in area and\npower, respectively, as compared to existing SRM0-RNL neurons.", "AI": {"tldr": "\u63d0\u51faCatwalk\u795e\u7ecf\u5143\u5b9e\u73b0\uff0c\u901a\u8fc7\u91cd\u6392\u8109\u51b2\u5e8f\u5217\u4e2d\u7684\u7a00\u758f\u8109\u51b2\u6765\u964d\u4f4e\u786c\u4ef6\u6210\u672c\uff0c\u5728\u9762\u79ef\u548c\u529f\u8017\u6548\u7387\u4e0a\u76f8\u6bd4\u73b0\u6709SRM0-RNL\u795e\u7ecf\u5143\u5206\u522b\u63d0\u53471.39\u500d\u548c1.86\u500d", "motivation": "\u73b0\u6709SRM-RNL\u795e\u7ecf\u5143\u5b9e\u73b0\u5047\u8bbe\u6240\u6709\u8f93\u5165\u90fd\u643a\u5e26\u8109\u51b2\uff0c\u4f46\u5b9e\u9645\u8109\u51b2\u5e8f\u5217\u4e2d\u53ea\u6709\u5c11\u91cf\u8f93\u5165\u5728\u8ba1\u7b97\u5468\u671f\u5185\u771f\u6b63\u643a\u5e26\u8109\u51b2\uff0c\u8fd9\u79cd\u7a00\u758f\u6027\u53ef\u4ee5\u88ab\u5229\u7528\u6765\u63d0\u9ad8\u786c\u4ef6\u6548\u7387", "method": "\u901a\u8fc7\u4e00\u5143top-k\u65b9\u6cd5\u5c06\u8109\u51b2\u5e8f\u5217\u4e2d\u7684\u8109\u51b2\u91cd\u6392\u4e3a\u6392\u5e8f\u540e\u7684\u5b50\u96c6\u7c07\uff0c\u663e\u8457\u964d\u4f4e\u540e\u7eed\u5e76\u884c\u8ba1\u6570\u5668(PC)\u79ef\u7d2f\u54cd\u5e94\u51fd\u6570\u7684\u6210\u672c", "result": "\u5e03\u5c40\u5e03\u7ebf\u7ed3\u679c\u663e\u793a\uff0cCatwalk\u5728\u9762\u79ef\u548c\u529f\u8017\u65b9\u9762\u5206\u522b\u6bd4\u73b0\u6709SRM0-RNL\u795e\u7ecf\u5143\u597d1.39\u500d\u548c1.86\u500d", "conclusion": "Catwalk\u795e\u7ecf\u5143\u5b9e\u73b0\u901a\u8fc7\u5229\u7528\u8109\u51b2\u5e8f\u5217\u7684\u7a00\u758f\u6027\uff0c\u6709\u6548\u63d0\u9ad8\u4e86RNL\u795e\u7ecf\u5143\u5b9e\u73b0\u7684\u9762\u79ef\u548c\u529f\u7387\u6548\u7387"}}
{"id": "2508.21286", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.21286", "abs": "https://arxiv.org/abs/2508.21286", "authors": ["Changheng Wang", "Zhiqing Wei", "Lizhe Liu", "Qiao Deng", "Yingda Wu", "Yangyang Niu", "Yashan Pang", "Zhiyong Feng"], "title": "Decentralized Federated Averaging via Random Walk", "comment": null, "summary": "Federated Learning (FL) is a communication-efficient distributed machine\nlearning method that allows multiple devices to collaboratively train models\nwithout sharing raw data. FL can be categorized into centralized and\ndecentralized paradigms. The centralized paradigm relies on a central server to\naggregate local models, potentially resulting in single points of failure,\ncommunication bottlenecks, and exposure of model parameters. In contrast, the\ndecentralized paradigm, which does not require a central server, provides\nimproved robustness and privacy. The essence of federated learning lies in\nleveraging multiple local updates for efficient communication. However, this\napproach may result in slower convergence or even convergence to suboptimal\nmodels in the presence of heterogeneous and imbalanced data. To address this\nchallenge, we study decentralized federated averaging via random walk (DFedRW),\nwhich replaces multiple local update steps on a single device with random walk\nupdates. Traditional Federated Averaging (FedAvg) and its decentralized\nversions commonly ignore stragglers, which reduces the amount of training data\nand introduces sampling bias. Therefore, we allow DFedRW to aggregate partial\nrandom walk updates, ensuring that each computation contributes to the model\nupdate. To further improve communication efficiency, we also propose a\nquantized version of DFedRW. We demonstrate that (quantized) DFedRW achieves\nconvergence upper bound of order $\\mathcal{O}(\\frac{1}{k^{1-q}})$ under convex\nconditions. Furthermore, we propose a sufficient condition that reveals when\nquantization balances communication and convergence. Numerical analysis\nindicates that our proposed algorithms outperform (decentralized) FedAvg in\nboth convergence rate and accuracy, achieving a 38.3\\% and 37.5\\% increase in\ntest accuracy under high levels of heterogeneities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u6e38\u8d70\u7684\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5e73\u5747\u7b97\u6cd5DFedRW\uff0c\u901a\u8fc7\u968f\u673a\u6e38\u8d70\u66f4\u65b0\u66ff\u4ee3\u591a\u6b65\u672c\u5730\u66f4\u65b0\uff0c\u89e3\u51b3\u4e86\u5f02\u6784\u6570\u636e\u4e0b\u7684\u6536\u655b\u95ee\u9898\uff0c\u5e76\u652f\u6301\u90e8\u5206\u805a\u5408\u548c\u91cf\u5316\u4ee5\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5728\u5f02\u6784\u548c\u4e0d\u5e73\u8861\u6570\u636e\u4e0b\u5b58\u5728\u6536\u655b\u6162\u548c\u6b21\u4f18\u6a21\u578b\u95ee\u9898\uff0c\u4e14\u96c6\u4e2d\u5f0f\u67b6\u6784\u6709\u5355\u70b9\u6545\u969c\u548c\u9690\u79c1\u98ce\u9669\u3002\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u867d\u80fd\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u9690\u79c1\u6027\uff0c\u4f46\u4ecd\u9762\u4e34\u6536\u655b\u6548\u7387\u6311\u6218\u3002", "method": "\u63d0\u51faDFedRW\u7b97\u6cd5\uff0c\u7528\u968f\u673a\u6e38\u8d70\u66f4\u65b0\u66ff\u4ee3\u591a\u6b65\u672c\u5730\u66f4\u65b0\uff1b\u5141\u8bb8\u805a\u5408\u90e8\u5206\u968f\u673a\u6e38\u8d70\u66f4\u65b0\u4ee5\u907f\u514d\u5ffd\u7565\u6389\u961f\u8005\uff1b\u8fdb\u4e00\u6b65\u63d0\u51fa\u91cf\u5316\u7248\u672c\u4ee5\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u51f8\u6761\u4ef6\u4e0b\u8fbe\u5230O(1/k^(1-q))\u6536\u655b\u4e0a\u754c\uff1b\u6570\u503c\u5206\u6790\u663e\u793a\u5728\u9ad8\u5f02\u6784\u6027\u4e0b\u6d4b\u8bd5\u51c6\u786e\u7387\u6bd4\u4f20\u7edfFedAvg\u63d0\u9ad838.3%\u548c37.5%\u3002", "conclusion": "DFedRW\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u5728\u5f02\u6784\u6570\u636e\u4e0b\u7684\u6536\u655b\u95ee\u9898\uff0c\u540c\u65f6\u901a\u8fc7\u91cf\u5316\u5e73\u8861\u4e86\u901a\u4fe1\u6548\u7387\u548c\u6536\u655b\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2508.21493", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.21493", "abs": "https://arxiv.org/abs/2508.21493", "authors": ["Yaman Umuroglu", "Christoph Berganski", "Felix Jentzsch", "Michal Danilowicz", "Tomasz Kryjak", "Charalampos Bezaitis", "Magnus Sjalander", "Ian Colbert", "Thomas Preusser", "Jakoba Petri-Koenig", "Michaela Blott"], "title": "SIRA: Scaled-Integer Range Analysis for Optimizing FPGA Dataflow Neural Network Accelerators", "comment": "Submitted to ACM TRETS Special Issue on Open-Source Tools for\n  Reconfigurable Devices and Systems", "summary": "While neural network quantization effectively reduces the cost of matrix\nmultiplications, aggressive quantization can expose non-matrix-multiply\noperations as significant performance and resource bottlenecks on embedded\nsystems. Addressing such bottlenecks requires a comprehensive approach to\ntailoring the precision across operations in the inference computation. To this\nend, we introduce scaled-integer range analysis (SIRA), a static analysis\ntechnique employing interval arithmetic to determine the range, scale, and bias\nfor tensors in quantized neural networks. We show how this information can be\nexploited to reduce the resource footprint of FPGA dataflow neural network\naccelerators via tailored bitwidth adaptation for accumulators and downstream\noperations, aggregation of scales and biases, and conversion of consecutive\nelementwise operations to thresholding operations. We integrate SIRA-driven\noptimizations into the open-source FINN framework, then evaluate their\neffectiveness across a range of quantized neural network workloads and compare\nimplementation alternatives for non-matrix-multiply operations. We demonstrate\nan average reduction of 17% for LUTs, 66% for DSPs, and 22% for accumulator\nbitwidths with SIRA optimizations, providing detailed benchmark analysis and\nanalytical models to guide the implementation style for non-matrix layers.\nFinally, we open-source SIRA to facilitate community exploration of its\nbenefits across various applications and hardware platforms.", "AI": {"tldr": "SIRA\u662f\u4e00\u79cd\u9759\u6001\u5206\u6790\u6280\u672f\uff0c\u901a\u8fc7\u533a\u95f4\u8fd0\u7b97\u786e\u5b9a\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u4e2d\u5f20\u91cf\u7684\u8303\u56f4\u3001\u5c3a\u5ea6\u548c\u504f\u7f6e\uff0c\u7528\u4e8e\u4f18\u5316FPGA\u6570\u636e\u6d41\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u7684\u8d44\u6e90\u5360\u7528\u3002", "motivation": "\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u6fc0\u8fdb\u7684\u91cf\u5316\u4f1a\u66b4\u9732\u975e\u77e9\u9635\u4e58\u6cd5\u64cd\u4f5c\u4f5c\u4e3a\u663e\u8457\u7684\u6027\u80fd\u548c\u8d44\u6e90\u74f6\u9888\uff0c\u9700\u8981\u5168\u9762\u7684\u7cbe\u5ea6\u5b9a\u5236\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f15\u5165scaled-integer range analysis (SIRA)\u9759\u6001\u5206\u6790\u6280\u672f\uff0c\u4f7f\u7528\u533a\u95f4\u8fd0\u7b97\u786e\u5b9a\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u4e2d\u5f20\u91cf\u7684\u8303\u56f4\u3001\u5c3a\u5ea6\u548c\u504f\u7f6e\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u8fdb\u884c\u4f4d\u5bbd\u9002\u914d\u3001\u5c3a\u5ea6\u548c\u504f\u7f6e\u805a\u5408\u3001\u8fde\u7eed\u5143\u7d20\u64cd\u4f5c\u8f6c\u6362\u4e3a\u9608\u503c\u64cd\u4f5c\u7b49\u4f18\u5316\u3002", "result": "\u5e73\u5747\u51cf\u5c1117%\u7684LUT\u300166%\u7684DSP\u548c22%\u7684\u7d2f\u52a0\u5668\u4f4d\u5bbd\uff0c\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u57fa\u51c6\u5206\u6790\u548c\u5206\u6790\u6a21\u578b\u6765\u6307\u5bfc\u975e\u77e9\u9635\u5c42\u7684\u5b9e\u73b0\u98ce\u683c\u3002", "conclusion": "SIRA\u4f18\u5316\u80fd\u663e\u8457\u51cf\u5c11FPGA\u52a0\u901f\u5668\u7684\u8d44\u6e90\u5360\u7528\uff0c\u5df2\u5f00\u6e90SIRA\u4ee5\u4fbf\u793e\u533a\u5728\u5404\u79cd\u5e94\u7528\u548c\u786c\u4ef6\u5e73\u53f0\u4e0a\u63a2\u7d22\u5176\u4f18\u52bf\u3002"}}
{"id": "2508.21289", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.21289", "abs": "https://arxiv.org/abs/2508.21289", "authors": ["Val\u00e9rie Hayot-Sasson", "Nathaniel Hudson", "Andr\u00e9 Bauer", "Maxime Gonthier", "Ian Foster", "Kyle Chard"], "title": "Addressing Reproducibility Challenges in HPC with Continuous Integration", "comment": null, "summary": "The high-performance computing (HPC) community has adopted incentive\nstructures to motivate reproducible research, with major conferences awarding\nbadges to papers that meet reproducibility requirements. Yet, many papers do\nnot meet such requirements. The uniqueness of HPC infrastructure and software,\ncoupled with strict access requirements, may limit opportunities for\nreproducibility. In the absence of resource access, we believe that regular\ndocumented testing, through continuous integration (CI), coupled with complete\nprovenance information, can be used as a substitute. Here, we argue that better\nHPC-compliant CI solutions will improve reproducibility of applications. We\npresent a survey of reproducibility initiatives and describe the barriers to\nreproducibility in HPC. To address existing limitations, we present a GitHub\nAction, CORRECT, that enables secure execution of tests on remote HPC\nresources. We evaluate CORRECT's usability across three different types of HPC\napplications, demonstrating the effectiveness of using CORRECT for automating\nand documenting reproducibility evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCORRECT\u5de5\u5177\uff0c\u901a\u8fc7GitHub Action\u5b9e\u73b0HPC\u5e94\u7528\u7684\u8fdc\u7a0b\u5b89\u5168\u6d4b\u8bd5\u6267\u884c\uff0c\u4ee5\u89e3\u51b3\u9ad8\u6027\u80fd\u8ba1\u7b97\u9886\u57df\u56e0\u57fa\u7840\u8bbe\u65bd\u9650\u5236\u5bfc\u81f4\u7684\u590d\u73b0\u6027\u6311\u6218", "motivation": "HPC\u793e\u533a\u867d\u7136\u8bbe\u7acb\u4e86\u590d\u73b0\u6027\u6fc0\u52b1\u673a\u5236\uff0c\u4f46\u7531\u4e8e\u72ec\u7279\u7684\u57fa\u7840\u8bbe\u65bd\u548c\u4e25\u683c\u8bbf\u95ee\u9650\u5236\uff0c\u8bb8\u591a\u8bba\u6587\u96be\u4ee5\u6ee1\u8db3\u590d\u73b0\u6027\u8981\u6c42\u3002\u7f3a\u4e4f\u8d44\u6e90\u8bbf\u95ee\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u66ff\u4ee3\u65b9\u6848\u6765\u786e\u4fdd\u7814\u7a76\u53ef\u590d\u73b0", "method": "\u5f00\u53d1\u4e86CORRECT GitHub Action\u5de5\u5177\uff0c\u652f\u6301\u5728\u8fdc\u7a0bHPC\u8d44\u6e90\u4e0a\u5b89\u5168\u6267\u884c\u6d4b\u8bd5\uff0c\u7ed3\u5408\u6301\u7eed\u96c6\u6210(CI)\u548c\u5b8c\u6574\u7684\u6eaf\u6e90\u4fe1\u606f\u6765\u66ff\u4ee3\u76f4\u63a5\u8d44\u6e90\u8bbf\u95ee", "result": "\u5728\u4e09\u79cd\u4e0d\u540c\u7c7b\u578b\u7684HPC\u5e94\u7528\u4e0a\u8bc4\u4f30\u4e86CORRECT\u7684\u53ef\u7528\u6027\uff0c\u8bc1\u660e\u8be5\u5de5\u5177\u80fd\u6709\u6548\u81ea\u52a8\u5316\u5e76\u8bb0\u5f55\u590d\u73b0\u6027\u8bc4\u4f30\u8fc7\u7a0b", "conclusion": "\u66f4\u597d\u7684HPC\u517c\u5bb9CI\u89e3\u51b3\u65b9\u6848\u5c06\u63d0\u9ad8\u5e94\u7528\u7a0b\u5e8f\u7684\u590d\u73b0\u6027\uff0cCORRECT\u5de5\u5177\u4e3a\u89e3\u51b3HPC\u590d\u73b0\u6027\u969c\u788d\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848"}}
{"id": "2508.21524", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21524", "abs": "https://arxiv.org/abs/2508.21524", "authors": ["Wenyong Zhou", "Zhengwu Liu", "Yuan Ren", "Ngai Wong"], "title": "Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators", "comment": "5 pages, 6 figures", "summary": "Compute-in-memory (CIM) accelerators have emerged as a promising way for\nenhancing the energy efficiency of convolutional neural networks (CNNs).\nDeploying CNNs on CIM platforms generally requires quantization of network\nweights and activations to meet hardware constraints. However, existing\napproaches either prioritize hardware efficiency with binary weight and\nactivation quantization at the cost of accuracy, or utilize multi-bit weights\nand activations for greater accuracy but limited efficiency. In this paper, we\nintroduce a novel binary weight multi-bit activation (BWMA) method for CNNs on\nCIM-based accelerators. Our contributions include: deriving closed-form\nsolutions for weight quantization in each layer, significantly improving the\nrepresentational capabilities of binarized weights; and developing a\ndifferentiable function for activation quantization, approximating the ideal\nmulti-bit function while bypassing the extensive search for optimal settings.\nThrough comprehensive experiments on CIFAR-10 and ImageNet datasets, we show\nthat BWMA achieves notable accuracy improvements over existing methods,\nregistering gains of 1.44\\%-5.46\\% and 0.35\\%-5.37\\% on respective datasets.\nMoreover, hardware simulation results indicate that 4-bit activation\nquantization strikes the optimal balance between hardware cost and model\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4e8c\u8fdb\u5236\u6743\u91cd\u591a\u6bd4\u7279\u6fc0\u6d3b(BWMA)\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b58\u5185\u8ba1\u7b97\u52a0\u901f\u5668\u4e0a\u7684CNN\uff0c\u5728\u4fdd\u6301\u786c\u4ef6\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u7cbe\u5ea6", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f7f\u7528\u4e8c\u8fdb\u5236\u6743\u91cd\u548c\u6fc0\u6d3b\u4ee5\u786c\u4ef6\u6548\u7387\u4e3a\u4ee3\u4ef7\u727a\u7272\u7cbe\u5ea6\uff0c\u8981\u4e48\u4f7f\u7528\u591a\u6bd4\u7279\u6743\u91cd\u548c\u6fc0\u6d3b\u83b7\u5f97\u66f4\u9ad8\u7cbe\u5ea6\u4f46\u6548\u7387\u6709\u9650\uff0c\u9700\u8981\u627e\u5230\u5e73\u8861\u65b9\u6848", "method": "\u63a8\u5bfc\u6bcf\u5c42\u6743\u91cd\u91cf\u5316\u7684\u95ed\u5f0f\u89e3\u4ee5\u63d0\u5347\u4e8c\u503c\u5316\u6743\u91cd\u8868\u793a\u80fd\u529b\uff1b\u5f00\u53d1\u53ef\u5fae\u5206\u6fc0\u6d3b\u91cf\u5316\u51fd\u6570\u8fd1\u4f3c\u7406\u60f3\u591a\u6bd4\u7279\u51fd\u6570\uff0c\u907f\u514d\u5927\u91cf\u641c\u7d22\u6700\u4f18\u8bbe\u7f6e", "result": "\u5728CIFAR-10\u548cImageNet\u6570\u636e\u96c6\u4e0a\u5206\u522b\u83b7\u5f971.44%-5.46%\u548c0.35%-5.37%\u7684\u7cbe\u5ea6\u63d0\u5347\uff1b\u786c\u4ef6\u4eff\u771f\u663e\u793a4\u4f4d\u6fc0\u6d3b\u91cf\u5316\u5728\u786c\u4ef6\u6210\u672c\u548c\u6a21\u578b\u6027\u80fd\u95f4\u8fbe\u5230\u6700\u4f18\u5e73\u8861", "conclusion": "BWMA\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5b58\u5185\u8ba1\u7b97\u52a0\u901f\u5668\u4e2d\u91cf\u5316\u7cbe\u5ea6\u4e0e\u786c\u4ef6\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548CNN\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.21328", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.21328", "abs": "https://arxiv.org/abs/2508.21328", "authors": ["Zhiyu Wang", "Mohammad Goudarzi", "Mingming Gong", "Rajkumar Buyya"], "title": "A Knowledge Distillation-empowered Adaptive Federated Reinforcement Learning Framework for Multi-Domain IoT Applications Scheduling", "comment": null, "summary": "The rapid proliferation of Internet of Things (IoT) applications across\nheterogeneous Cloud-Edge-IoT environments presents significant challenges in\ndistributed scheduling optimization. Existing approaches face issues, including\nfixed neural network architectures that are incompatible with computational\nheterogeneity, non-Independent and Identically Distributed (non-IID) data\ndistributions across IoT scheduling domains, and insufficient cross-domain\ncollaboration mechanisms. This paper proposes KD-AFRL, a Knowledge\nDistillation-empowered Adaptive Federated Reinforcement Learning framework that\naddresses multi-domain IoT application scheduling through three core\ninnovations. First, we develop a resource-aware hybrid architecture generation\nmechanism that creates dual-zone neural networks enabling heterogeneous devices\nto participate in collaborative learning while maintaining optimal resource\nutilization. Second, we propose a privacy-preserving environment-clustered\nfederated learning approach that utilizes differential privacy and K-means\nclustering to address non-IID challenges and facilitate effective collaboration\namong compatible domains. Third, we introduce an environment-oriented\ncross-architecture knowledge distillation mechanism that enables efficient\nknowledge transfer between heterogeneous models through temperature-regulated\nsoft targets. Comprehensive experiments with real Cloud-Edge-IoT infrastructure\ndemonstrate KD-AFRL's effectiveness using diverse IoT applications. Results\nshow significant improvements over the best baseline, with 21% faster\nconvergence and 15.7%, 10.8%, and 13.9% performance gains in completion time,\nenergy consumption, and weighted cost, respectively. Scalability experiments\nreveal that KD-AFRL achieves 3-5 times better performance retention compared to\nexisting solutions as the number of domains increases.", "AI": {"tldr": "\u63d0\u51fa\u4e86KD-AFRL\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u81ea\u9002\u5e94\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3IoT\u591a\u57df\u8c03\u5ea6\u95ee\u9898\uff0c\u5728\u6536\u655b\u901f\u5ea6\u3001\u5b8c\u6210\u65f6\u95f4\u3001\u80fd\u8017\u548c\u6210\u672c\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "IoT\u5e94\u7528\u5728\u4e91\u8fb9\u7aef\u5f02\u6784\u73af\u5883\u4e2d\u9762\u4e34\u5206\u5e03\u5f0f\u8c03\u5ea6\u4f18\u5316\u7684\u6311\u6218\uff0c\u5305\u62ec\u56fa\u5b9a\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e0e\u8ba1\u7b97\u5f02\u6784\u6027\u4e0d\u517c\u5bb9\u3001\u975eIID\u6570\u636e\u5206\u5e03\u4ee5\u53ca\u8de8\u57df\u534f\u4f5c\u673a\u5236\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "1. \u8d44\u6e90\u611f\u77e5\u7684\u6df7\u5408\u67b6\u6784\u751f\u6210\u673a\u5236\uff0c\u521b\u5efa\u53cc\u533a\u57df\u795e\u7ecf\u7f51\u7edc\uff1b2. \u9690\u79c1\u4fdd\u62a4\u7684\u57fa\u4e8e\u73af\u5883\u805a\u7c7b\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u5dee\u5206\u9690\u79c1\u548cK-means\u805a\u7c7b\uff1b3. \u73af\u5883\u5bfc\u5411\u7684\u8de8\u67b6\u6784\u77e5\u8bc6\u84b8\u998f\u673a\u5236\uff0c\u901a\u8fc7\u6e29\u5ea6\u8c03\u8282\u8f6f\u76ee\u6807\u5b9e\u73b0\u5f02\u6784\u6a21\u578b\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6bd4\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u6536\u655b\u901f\u5ea6\u5feb21%\uff0c\u5728\u5b8c\u6210\u65f6\u95f4\u3001\u80fd\u8017\u548c\u52a0\u6743\u6210\u672c\u65b9\u9762\u5206\u522b\u63d0\u534715.7%\u300110.8%\u548c13.9%\u3002\u53ef\u6269\u5c55\u6027\u5b9e\u9a8c\u4e2d\uff0c\u968f\u7740\u57df\u6570\u91cf\u589e\u52a0\uff0c\u6027\u80fd\u4fdd\u6301\u80fd\u529b\u6bd4\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u597d3-5\u500d\u3002", "conclusion": "KD-AFRL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86IoT\u591a\u57df\u8c03\u5ea6\u4e2d\u7684\u5f02\u6784\u6027\u3001\u975eIID\u6570\u636e\u548c\u8de8\u57df\u534f\u4f5c\u6311\u6218\uff0c\u5728\u771f\u5b9e\u4e91\u8fb9\u7aef\u57fa\u7840\u8bbe\u65bd\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.21473", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.21473", "abs": "https://arxiv.org/abs/2508.21473", "authors": ["Daniil Vostrikov", "Yash Madhwal", "Andrey Seoev", "Anastasiia Smirnova", "Yury Yanovich", "Alexey Smirnov", "Vladimir Gorgadze"], "title": "Unpacking Maximum Extractable Value on Polygon: A Study on Atomic Arbitrage", "comment": null, "summary": "The evolution of blockchain technology, from its origins as a decentralized\nledger for cryptocurrencies to its broader applications in areas like\ndecentralized finance (DeFi), has significantly transformed financial\necosystems while introducing new challenges such as Maximum Extractable Value\n(MEV). This paper explores MEV on the Polygon blockchain, with a particular\nfocus on Atomic Arbitrage (AA) transactions. We establish criteria for\nidentifying AA transactions and analyze key factors such as searcher behavior,\nbidding dynamics, and token usage. Utilizing a dataset spanning 22 months and\ncovering 23 million blocks, we examine MEV dynamics with a focus on Spam-based\nand Auction-based backrunning strategies. Our findings reveal that while\nSpam-based transactions are more prevalent, Auction-based transactions\ndemonstrate greater profitability. Through detailed examples and analysis, we\ninvestigate the interactions between network architecture, transaction\nsequencing, and MEV extraction, offering comprehensive insights into the\nevolution and challenges of MEV in decentralized ecosystems. These results\nemphasize the need for robust transaction ordering mechanisms and highlight the\nimplications of emerging MEV strategies for blockchain networks.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86Polygon\u533a\u5757\u94fe\u4e0a\u7684\u6700\u5927\u53ef\u63d0\u53d6\u4ef7\u503c(MEV)\uff0c\u7279\u522b\u5173\u6ce8\u539f\u5b50\u5957\u5229\u4ea4\u6613\uff0c\u53d1\u73b0\u5783\u573e\u90ae\u4ef6\u5f0f\u4ea4\u6613\u66f4\u666e\u904d\u4f46\u62cd\u5356\u5f0f\u4ea4\u6613\u66f4\u76c8\u5229\u3002", "motivation": "\u968f\u7740\u533a\u5757\u94fe\u6280\u672f\u4ece\u52a0\u5bc6\u8d27\u5e01\u6269\u5c55\u5230DeFi\u7b49\u66f4\u5e7f\u6cdb\u5e94\u7528\uff0cMEV\u6210\u4e3a\u91cd\u8981\u6311\u6218\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u5176\u5728Polygon\u7b49\u7f51\u7edc\u4e2d\u7684\u8868\u73b0\u548c\u5f71\u54cd\u3002", "method": "\u4f7f\u752822\u4e2a\u6708\u30012300\u4e07\u4e2a\u533a\u5757\u7684\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u539f\u5b50\u5957\u5229\u4ea4\u6613\u8bc6\u522b\u6807\u51c6\uff0c\u5206\u6790\u641c\u7d22\u8005\u884c\u4e3a\u3001\u7ade\u4ef7\u52a8\u6001\u548c\u4ee3\u5e01\u4f7f\u7528\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u91cd\u70b9\u7814\u7a76\u5783\u573e\u90ae\u4ef6\u5f0f\u548c\u62cd\u5356\u5f0f\u56de\u8dd1\u7b56\u7565\u3002", "result": "\u5783\u573e\u90ae\u4ef6\u5f0f\u4ea4\u6613\u66f4\u666e\u904d\uff0c\u4f46\u62cd\u5356\u5f0f\u4ea4\u6613\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u76c8\u5229\u80fd\u529b\u3002\u7814\u7a76\u63ed\u793a\u4e86\u7f51\u7edc\u67b6\u6784\u3001\u4ea4\u6613\u6392\u5e8f\u548cMEV\u63d0\u53d6\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u9700\u8981\u5f3a\u5927\u7684\u4ea4\u6613\u6392\u5e8f\u673a\u5236\uff0c\u5e76\u6307\u51fa\u4e86\u65b0\u5174MEV\u7b56\u7565\u5bf9\u533a\u5757\u94fe\u7f51\u7edc\u7684\u5f71\u54cd\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u751f\u6001\u7cfb\u7edf\u4e2dMEV\u7684\u6f14\u53d8\u548c\u6311\u6218\u63d0\u4f9b\u4e86\u5168\u9762\u89c1\u89e3\u3002"}}
{"id": "2508.21613", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.21613", "abs": "https://arxiv.org/abs/2508.21613", "authors": ["Yuhang Zhou", "Zhibin Wang", "Peng Jiang", "Haoran Xia", "Junhe Lu", "Qianyu Jiang", "Rong Gu", "Hengxi Xu", "Xinjing Huang", "Guanghuan Fang", "Zhiheng Hu", "Jingyi Zhang", "Yongjin Cai", "Jian He", "Chen Tian"], "title": "Odyssey: Adaptive Policy Selection for Resilient Distributed Training", "comment": null, "summary": "Training large language models faces frequent interruptions due to various\nfaults, demanding robust fault-tolerance. Existing backup-free methods, such as\nredundant computation, dynamic parallelism, and data rerouting, each incur\nperformance penalties, whether from ongoing overhead, lengthy reconfigurations,\nor post-recovery inefficiencies. We propose Odyssey, an adaptive fault-tolerant\nsystem that intelligently selects optimal recovery strategies when a failure\noccurs. Odyssey achieves this through a unified performance model, expedient\nexecution plan search, accurate performance estimation, and efficient\ncommunication optimizations. Experiments on a 32-card cluster show that Odyssey\nmaintains a performance gap of within 11.00% between post-recovery and\nfailure-free training, while preserving model convergence and efficient memory\nusage. Compared to state-of-the-art methods, Odyssey achieves up to 1.229x and\n1.355x higher average throughput than Oobleck and Recycle, respectively.", "AI": {"tldr": "Odyssey\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u5bb9\u9519\u7cfb\u7edf\uff0c\u901a\u8fc7\u667a\u80fd\u9009\u62e9\u6700\u4f18\u6062\u590d\u7b56\u7565\u6765\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u4e2d\u65ad\u5f71\u54cd\uff0c\u572832\u5361\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u6062\u590d\u540e\u6027\u80fd\u4e0e\u65e0\u6545\u969c\u8bad\u7ec3\u4ec511%\u7684\u5dee\u8ddd", "motivation": "\u73b0\u6709\u65e0\u5907\u4efd\u65b9\u6cd5\uff08\u5197\u4f59\u8ba1\u7b97\u3001\u52a8\u6001\u5e76\u884c\u3001\u6570\u636e\u91cd\u8def\u7531\uff09\u90fd\u5b58\u5728\u6027\u80fd\u635f\u5931\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6545\u969c\u6062\u590d\u65b9\u6848", "method": "\u4f7f\u7528\u7edf\u4e00\u6027\u80fd\u6a21\u578b\u3001\u5feb\u901f\u6267\u884c\u8ba1\u5212\u641c\u7d22\u3001\u51c6\u786e\u6027\u80fd\u4f30\u8ba1\u548c\u9ad8\u6548\u901a\u4fe1\u4f18\u5316\u6765\u667a\u80fd\u9009\u62e9\u6700\u4f18\u6062\u590d\u7b56\u7565", "result": "\u572832\u5361\u96c6\u7fa4\u4e0a\uff0c\u6062\u590d\u540e\u8bad\u7ec3\u6027\u80fd\u4e0e\u65e0\u6545\u969c\u8bad\u7ec3\u4ec5\u5dee11.00%\uff0c\u5e73\u5747\u541e\u5410\u91cf\u6bd4Oobleck\u548cRecycle\u5206\u522b\u9ad81.229\u500d\u548c1.355\u500d", "conclusion": "Odyssey\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5904\u7406\u8bad\u7ec3\u4e2d\u65ad\uff0c\u4fdd\u6301\u6a21\u578b\u6536\u655b\u6027\u548c\u5185\u5b58\u4f7f\u7528\u6548\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5"}}
{"id": "2508.21706", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.21706", "abs": "https://arxiv.org/abs/2508.21706", "authors": ["Zhibin Wang", "Zhonghui Zhang", "Yuhang Zhou", "Zibo Wang", "Mo Zhou", "Peng Jiang", "Weilin Cai", "Chengying Huan", "Rong Gu", "Sheng Zhong", "Chen Tian"], "title": "Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency with Speculative Decoding", "comment": null, "summary": "Recent advancements in Mixture of Experts (MoE) models have significantly\nincreased their parameter scale as well as model performance. Extensive\noffloading techniques have been proposed to address the GPU memory limitations\nof MoE inference. However, due to the I/O bottleneck and sparse computation of\nMoE models, existing offloading techniques still suffer from low hardware\nutilization. To fully utilize the hardware resources, we propose SpecMoEOff,\nwhich employs the speculative decoding technique to enlarge the workload of\neach expert. SpecMoEOff orchestrates the GPU and CPU by both theoretical and\nempirical roofline analysis. In addition, we develop a dedicated CPU chunked\nattention verification kernel to fit the speculative decoding in offloading\nscenarios as well as minimizing the additional overhead led by draft models.\nSpecMoEOff further integrates an optimizer to automatically tune the\nhyperparameters of speculative decoding under given hardware and workload.\nExperimental results show that SpecMoEOff achieves up to 2.5x decode throughput\nimprovement over the state-of-the-art MoE offloading techniques.", "AI": {"tldr": "SpecMoEOff\u5229\u7528\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u6269\u5927\u4e13\u5bb6\u6a21\u578b\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u7ecf\u9a8c\u5206\u6790\u534f\u8c03GPU\u548cCPU\u8d44\u6e90\uff0c\u5f00\u53d1\u4e13\u7528CPU\u6ce8\u610f\u529b\u9a8c\u8bc1\u5185\u6838\uff0c\u5e76\u96c6\u6210\u81ea\u52a8\u4f18\u5316\u5668\u8c03\u53c2\uff0c\u5728MoE\u6a21\u578b\u63a8\u7406\u4e2d\u5b9e\u73b0\u6700\u9ad82.5\u500d\u7684\u89e3\u7801\u541e\u5410\u91cf\u63d0\u5347", "motivation": "\u73b0\u6709\u7684MoE\u6a21\u578b\u5378\u8f7d\u6280\u672f\u7531\u4e8eI/O\u74f6\u9888\u548c\u7a00\u758f\u8ba1\u7b97\u95ee\u9898\uff0c\u5bfc\u81f4\u786c\u4ef6\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u5145\u5206\u5229\u7528\u786c\u4ef6\u8d44\u6e90", "method": "\u63d0\u51faSpecMoEOff\u65b9\u6cd5\uff0c\u91c7\u7528\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u6269\u5927\u4e13\u5bb6\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u7ecf\u9a8c\u5206\u6790\u534f\u8c03GPU\u548cCPU\uff0c\u5f00\u53d1\u4e13\u7528CPU\u5206\u5757\u6ce8\u610f\u529b\u9a8c\u8bc1\u5185\u6838\uff0c\u5e76\u96c6\u6210\u81ea\u52a8\u4f18\u5316\u5668\u8c03\u53c2", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSpecMoEOff\u76f8\u6bd4\u6700\u5148\u8fdb\u7684MoE\u5378\u8f7d\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad82.5\u500d\u7684\u89e3\u7801\u541e\u5410\u91cf\u63d0\u5347", "conclusion": "SpecMoEOff\u901a\u8fc7\u63a8\u6d4b\u89e3\u7801\u548c\u786c\u4ef6\u8d44\u6e90\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86MoE\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u786c\u4ef6\u5229\u7528\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd"}}
