<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 20]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Mechanizing a Proof-Relevant Logical Relation for Timed Message-Passing Protocols](https://arxiv.org/abs/2511.19521)
*Tesla Zhang,Asher Kornfeld,Rui Li,Sonya Simkin,Yue Yao,Stephanie Balzer*

Main category: cs.PL

TL;DR: 本文在Rocq定理证明器中实现了Yao等人关于定时消息传递协议验证的语义类型系统的机械化，包括逻辑关系、可计算轨迹的代数和基本定理。


<details>
  <summary>Details</summary>
Motivation: 定时消息传递协议在物联网和实时系统中很常见，语义类型系统能同时支持类型化和非类型化组件。Yao等人的工作缺乏机械化实现，这限制了其可扩展性和实际应用。

Method: 在Rocq定理证明器中机械化Yao等人的证明相关逻辑关系，处理轨迹的交错、分区和连接操作，解决内涵类型理论中等式支持的局限性。

Result: 成功实现了逻辑关系、可计算轨迹代数及相关引理、逻辑关系基本定理的机械化证明。

Conclusion: 机械化不仅为系统提供了机器证明，还为未来的扩展和应用提供了更好的可扩展性基础。

Abstract: Semantic typing has become a powerful tool for program verification, applying the technique of logical relations as not only a proof method, but also a device for prescribing program behavior. In recent work, Yao et al. scaled semantic typing to the verification of timed message-passing protocols, which are prevalent in, e.g., IoT and real-time systems applications. The appeal of semantic typing in this context is precisely because of its ability to support typed and untyped program components alike -- including physical objects -- which caters to the heterogeneity of these applications. Another demand inherent to these applications is timing: constraining the time or time window within which a message exchange must happen. Yao et al. equipped their logical relation not only with temporal predicates, but also with computable trajectories, to supply the evidence that an inhabitant can step from one time point to another one. While Yao et al. provide the formalization for such a verification tool, it lacks a mechanization. Mechanizing the system would not only provide a machine proof for it, but also facilitate scalability for future extensions and applications.
  This paper tackles the challenge of mechanizing the resulting proof-relevant logical relation in a proof assistant. allowing trajectories to be interleaved, partitioned, and concatenated, while the intended equality on trajectories is the equality of their graphs when seen as processes indexed by time. Unfortunately, proof assistants based on intensional type theory only have modest support for such equations, forcing a prolific use of transports. This paper reports on the process of mechanizing Yao et al.'s results, comprising the logical relation, the algebra of computable trajectories with supporting lemmas, and the fundamental theorem of the logical relation, in the Rocq theorem prover.

</details>


### [2] [Understanding Accelerator Compilers via Performance Profiling](https://arxiv.org/abs/2511.19764)
*Ayaka Yorihiro,Griffin Berlstein,Pedro Pontes García,Kevin Laeufer,Adrian Sampson*

Main category: cs.PL

TL;DR: Petal是一个针对Calyx中间语言的周期级分析工具，通过探测和跟踪分析来帮助加速器设计者理解编译器决策对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 加速器设计语言(ADL)编译器存在性能不可预测性问题，需要工具来帮助程序员理解编译器决策如何影响性能。

Method: Petal在Calyx代码中插入探针，通过寄存器传输级仿真分析跟踪数据，将跟踪事件映射回高级控制结构以追踪每个结构活跃的时钟周期。

Result: 案例研究表明Petal的周期级分析能够识别现有加速器设计中的性能问题，并指导开发者进行编译器无法自动执行的优化，其中一个应用的总周期数减少了46.9%。

Conclusion: Petal为ADL程序员提供了理解编译器决策对性能影响的工具，弥补了编译器不完美的问题，帮助识别和解决性能瓶颈。

Abstract: Accelerator design languages (ADLs), high-level languages that compile to hardware units, help domain experts quickly design efficient application-specific hardware. ADL compilers optimize datapaths and convert software-like control flow constructs into control paths. Such compilers are necessarily complex and often unpredictable: they must bridge the wide semantic gap between high-level semantics and cycle-level schedules, and they typically rely on advanced heuristics to optimize circuits. The resulting performance can be difficult to control, requiring guesswork to find and resolve performance problems in the generated hardware. We conjecture that ADL compilers will never be perfect: some performance unpredictability is endemic to the problem they solve.
  In lieu of compiler perfection, we argue for compiler understanding tools that give ADL programmers insight into how the compiler's decisions affect performance. We introduce Petal, a cycle-level Petal for the Calyx intermediate language (IL). Petal instruments the Calyx code with probes and then analyzes the trace from a register-transfer-level simulation. It maps the events in the trace back to high-level control constructs in the Calyx code to track the clock cycles when each construct was active. Using case studies, we demonstrate that Petal's cycle-level profiles can identify performance problems in existing accelerator designs. We show that these insights can also guide developers toward optimizations that the compiler was unable to perform automatically, including a reduction by 46.9\% of total cycles for one application.

</details>


### [3] [The Ghosts of Empires: Extracting Modularity from Interleaving-Based Proofs (Extended Version)](https://arxiv.org/abs/2511.20369)
*Frank Schüssele,Matthias Zumkeller,Miriam Lagunes-Rochin,Dominik Klumpp*

Main category: cs.PL

TL;DR: 将基于交错执行的正确性证明转换为线程模块化证明，通过自动合成幽灵变量来捕获相关交错信息，生成紧凑的正确性证书。


<details>
  <summary>Details</summary>
Motivation: 算法软件验证器中的实现错误威胁验证结果的可靠性。为正确程序生成正确性证书可以独立验证验证结果，从而帮助发现这些错误。

Method: 将基于交错执行的正确性证明转换为Owicki-Gries风格的线程模块化证明，自动合成幽灵变量来捕获相关交错信息，并抽象掉不相关细节。

Result: 评估表明该方法在实践中高效，与基线相比生成了紧凑的证明。

Conclusion: 提出的方法能够有效生成并发程序的紧凑正确性证明，有助于提高验证器的可靠性。

Abstract: Implementation bugs threaten the soundness of algorithmic software verifiers. Generating correctness certificates for correct programs allows for efficient independent validation of verification results, and thus helps to reveal such bugs. Automatic generation of small, compact correctness proofs for concurrent programs is challenging, as the correctness arguments may depend on the particular interleaving, which can lead to exponential explosion. We present an approach that converts an interleaving-based correctness proof, as generated by many algorithmic verifiers, into a thread-modular correctness proof in the style of Owicki and Gries. We automatically synthesize ghost variables that capture the relevant interleaving information, and abstract away irrelevant details. Our evaluation shows that the approach is efficient in practice and generates compact proofs, compared to a baseline.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Opt4GPTQ: Co-Optimizing Memory and Computation for 4-bit GPTQ Quantized LLM Inference on Heterogeneous Platforms](https://arxiv.org/abs/2511.19438)
*Yaozheng Zhang,Wei Wang,Jie Kong,Jiehan Zhou,Huanqing Cui*

Main category: cs.DC

TL;DR: Opt4GPTQ是一个针对4位GPTQ量化大语言模型在异构AI加速器上的推理优化方法，通过三种平台级优化策略显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在异构计算平台上的广泛应用，实现高效的推理面临重大挑战，特别是在异构平台上推理效率低下的问题。

Method: 基于vLLM服务系统，集成三种平台级优化策略：共享内存缓冲优化(SMB-Opt)、向量化内存加载优化(VML-Opt)和内联汇编优化(ILA-Opt)。

Result: 实验结果显示，Opt4GPTQ在不同模型上有效提升推理性能，最高实现84.42%的吞吐量提升和51.35%的延迟降低。

Conclusion: 这项工作强调了平台级工程优化在异构AI加速架构上实现高效大语言模型推理的关键作用，为未来异构平台适配提供了宝贵的部署经验和方法论。

Abstract: The increasing adoption of large language model (LLMs) on heterogeneous computing platforms poses significant challenges for achieving high inference efficiency. To address the low inference efficiency of LLMs across diverse heterogeneous platforms, this paper proposes a practical optimization method, Opt4GPTQ, designed for 4-bit GPTQ quantized LLMs inference on heterogeneous AI accelerators. Built upon the vLLM serving system, Opt4GPTQ integrates three platform-level optimization strategies: Shared Memory Buffering optimization (SMB-Opt), which caches data in shared memory and employs single-threaded writes; Vectorized Memory Loading optimization (VML-Opt), which utilizes vectorized memory operations for efficient data loading; and Inline Assembly optimization (ILAOpt), which directly leverages hardware-native vector halfprecision addition and fused multiply-accumulate instructions for efficient execution. Experimental results show that Opt4GPTQ effectively improves inference performance across different models, achieving up to 84.42% throughput improvement and up to 51.35% latency reduction. This work highlights the critical role of platform-level engineering optimizations in enabling efficient LLMs inference on emerging heterogeneous AI acceleration architectures and provides valuable deployment experience and methodologies for future heterogeneous platform adaptation.

</details>


### [5] [Asynchronous Cooperative Optimization of a Capacitated Vehicle Routing Problem Solution](https://arxiv.org/abs/2511.19445)
*Luca Accorsi,Demetrio Laganà,Federico Michelotto,Roberto Musmanno,Daniele Vigo*

Main category: cs.DC

TL;DR: 提出了FILO2^x并行共享内存方案，用于协同优化容量约束车辆路径问题，无需显式分解且同步开销最小


<details>
  <summary>Details</summary>
Motivation: 利用FILO2算法的局部性特性，通过并行异步优化多个可能不相关的解区域，更好地利用计算资源

Method: 设计FILO2^x作为FILO2算法的单轨迹并行适配，采用迭代级并行化，多个求解器同时优化同一基础解

Result: 相比原始方法显著减少求解时间，同时保持从数百到数十万客户实例的最终解质量相似

Conclusion: FILO2^x通过充分利用可用计算资源，在保持解质量的同时大幅提升求解效率

Abstract: We propose a parallel shared-memory schema to cooperatively optimize the solution of a Capacitated Vehicle Routing Problem instance with minimal synchronization effort and without the need for an explicit decomposition. To this end, we design FILO2$^x$ as a single-trajectory parallel adaptation of the FILO2 algorithm originally proposed for extremely large-scale instances and described in Accorsi and Vigo (2024). Using the locality of the FILO2 optimization applications, in FILO2$^x$ several possibly unrelated solution areas are concurrently asynchronously optimized. The overall search trajectory emerges as an iteration-based parallelism obtained by the simultaneous optimization of the same underlying solution performed by several solvers. Despite the high efficiency exhibited by the single-threaded FILO2 algorithm, the computational results show that, by better exploiting the available computing resources, FILO2$^x$ can greatly enhance the resolution time compared to the original approach, still maintaining a similar final solution quality for instances ranging from hundreds to hundreds of thousands customers.

</details>


### [6] [AI-driven Predictive Shard Allocation for Scalable Next Generation Blockchains](https://arxiv.org/abs/2511.19450)
*M. Zeeshan Haider,Tayyaba Noreen,M. D. Assuncao,Kaiwen Zhang*

Main category: cs.DC

TL;DR: PSAP是一种动态智能分片分配协议，通过预测性工作负载分配解决区块链分片中的负载不均问题，显著提升吞吐量和降低延迟


<details>
  <summary>Details</summary>
Motivation: 静态或启发式分片分配导致工作负载倾斜、拥塞和跨分片通信过多，削弱了分片的可扩展性优势

Method: 集成时间工作负载预测模型和安全约束强化学习控制器，实现多区块前瞻预测和自适应分片重构，通过同步量化运行时和安全门确保确定性推理

Result: 在异构数据集上实验显示，相比现有动态分片基线，吞吐量提升2倍，延迟降低35%，跨分片开销减少20%

Conclusion: 预测性、确定性和安全感知的分片分配是下一代可扩展区块链系统的有前景方向

Abstract: Sharding has emerged as a key technique to address blockchain scalability by partitioning the ledger into multiple shards that process transactions in parallel. Although this approach improves throughput, static or heuristic shard allocation often leads to workload skew, congestion, and excessive cross-shard communication diminishing the scalability benefits of sharding. To overcome these challenges, we propose the Predictive Shard Allocation Protocol (PSAP), a dynamic and intelligent allocation framework that proactively assigns accounts and transactions to shards based on workload forecasts. PSAP integrates a Temporal Workload Forecasting (TWF) model with a safety-constrained reinforcement learning (Safe-PPO) controller, jointly enabling multi-block-ahead prediction and adaptive shard reconfiguration. The protocol enforces deterministic inference across validators through a synchronized quantized runtime and a safety gate that limits stake concentration, migration gas, and utilization thresholds. By anticipating hotspot formation and executing bounded, atomic migrations, PSAP achieves stable load balance while preserving Byzantine safety. Experimental evaluation on heterogeneous datasets, including Ethereum, NEAR, and Hyperledger Fabric mapped via address-clustering heuristics, demonstrates up to 2x throughput improvement, 35\% lower latency, and 20\% reduced cross-shard overhead compared to existing dynamic sharding baselines. These results confirm that predictive, deterministic, and security-aware shard allocation is a promising direction for next-generation scalable blockchain systems.

</details>


### [7] [AVS: A Computational and Hierarchical Storage System for Autonomous Vehicles](https://arxiv.org/abs/2511.19453)
*Yuxin Wang,Yuankai He,Weisong Shi*

Main category: cs.DC

TL;DR: AVS是一个专为自动驾驶车辆设计的存储系统，通过分层布局、模态感知压缩和热冷数据分层，实现了高效的数据存储和检索，显著减少了存储占用并支持实时数据摄取。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆产生海量异构数据（如每天14TB），现有车载存储系统无法提供高效的数据存储和检索能力，需要支持第三方应用的可查询存储系统。

Method: 采用计算与存储协同设计的分层架构：模态感知的数据缩减和压缩、热冷数据分层与每日归档、轻量级元数据层索引，并在嵌入式硬件上使用真实L4自动驾驶轨迹进行验证。

Result: 原型系统在适度资源预算下实现了可预测的实时数据摄取、快速选择性检索和显著的存储占用减少。

Conclusion: 存储应作为自动驾驶堆栈中的一等组件，该工作为更可扩展和长期部署提供了观察和下一步方向。

Abstract: Autonomous vehicles (AVs) are evolving into mobile computing platforms, equipped with powerful processors and diverse sensors that generate massive heterogeneous data, for example 14 TB per day. Supporting emerging third-party applications calls for a general-purpose, queryable onboard storage system. Yet today's data loggers and storage stacks in vehicles fail to deliver efficient data storage and retrieval. This paper presents AVS, an Autonomous Vehicle Storage system that co-designs computation with a hierarchical layout: modality-aware reduction and compression, hot-cold tiering with daily archival, and a lightweight metadata layer for indexing. The design is grounded with system-level benchmarks on AV data that cover SSD and HDD filesystems and embedded indexing, and is validated on embedded hardware with real L4 autonomous driving traces. The prototype delivers predictable real-time ingest, fast selective retrieval, and substantial footprint reduction under modest resource budgets. The work also outlines observations and next steps toward more scalable and longer deployments to motivate storage as a first-class component in AV stacks.

</details>


### [8] [Optimizations on Graph-Level for Domain Specific Computations in Julia and Application to QED](https://arxiv.org/abs/2511.19456)
*Anton Reinhard,Simeon Ehrig,René Widera,Michael Bussmann,Uwe Hernandez Acosta*

Main category: cs.DC

TL;DR: 提出了一个Julia软件框架，能自动动态生成静态调度和编译代码，通过增强DAG调度理论实现优化，并以量子电动力学中多粒子散射过程的矩阵元计算为例验证。


<details>
  <summary>Details</summary>
Motivation: 科学计算中的复杂问题通常包含具有不同计算需求的子任务，需要针对每个子任务分析并在最适合的硬件上调度，同时考虑并行性、任务依赖性和设备间数据传输速度。

Method: 使用有向无环图表示计算问题，在现有DAG调度概念基础上添加领域特定计算信息，开发Julia框架自动动态生成静态调度和编译代码。

Result: 实现了理论框架和软件系统，能够有效利用机器上的硬件资源，在量子电动力学多粒子散射矩阵元计算中验证了方法的有效性。

Conclusion: 通过增强DAG调度理论并添加领域特定信息，实现了原本不可能的优化，为复杂科学计算问题提供了高效的自动调度解决方案。

Abstract: Complex computational problems in science often consist of smaller parts that can have largely distinct compute requirements from one another. For optimal efficiency, analyzing each subtask and scheduling it on the best-suited hardware would be necessary. Other considerations must be taken into account, too, such as parallelism, dependencies between different subtasks, and data transfer speeds between devices. To achieve this, directed acyclic graphs are often employed to represent these problems and enable utilizing as much hardware as possible on a given machine. In this paper, we present a software framework written in Julia capable of automatically and dynamically producing statically scheduled and compiled code. We lay theoretical foundations and add domain-specific information about the computation to the existing concepts of DAG scheduling, enabling optimizations that would otherwise be impossible. To illustrate the theory we implement an example application: the computation of matrix elements for scattering processes with many external particles in quantum electrodynamics.

</details>


### [9] [SparOA: Sparse and Operator-aware Hybrid Scheduling for Edge DNN Inference](https://arxiv.org/abs/2511.19457)
*Ziyang Zhang,Jie Liu,Luca Mottola*

Main category: cs.DC

TL;DR: SparOA是一个CPU-GPU混合推理框架，通过利用稀疏性和计算强度来优化算子调度，在资源受限的边缘设备上实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在边缘设备部署时面临资源限制问题，现有解决方案如模型压缩会牺牲精度，而专用硬件成本高且不灵活。现有混合推理方法通常忽略算子特性对性能的影响。

Method: SparOA包含三个关键组件：阈值预测器准确确定最优稀疏性和计算强度阈值；基于强化学习的调度器根据实时硬件状态动态优化资源分配；混合推理引擎通过异步执行和批量大小优化提高效率。

Result: SparOA相比所有基线方法平均加速1.22-1.31倍，比CPU-only方法最高加速50.7倍。在能耗方面，比最先进的协同执行基线节省7%-16%的能耗。

Conclusion: SparOA通过有效利用稀疏性和计算强度特性，结合智能调度策略，在边缘设备上实现了显著的性能提升和能耗优化。

Abstract: The resource demands of deep neural network (DNN) models introduce significant performance challenges, especially when deployed on resource-constrained edge devices. Existing solutions like model compression often sacrifice accuracy, while specialized hardware remains costly and inflexible. Hybrid inference methods, however, typically overlook how operator characteristics impact performance. In this work, we present SparOA, a CPU-GPU hybrid inference framework, which leverages both sparsity and computational intensity to optimize operator scheduling. SparOA embraces aforementioned challenges through three key components: (1) a threshold predictor that accurately determines optimal sparsity and computational intensity thresholds; (2) a reinforcement learning-based scheduler that dynamically optimizes resource allocation based on real-time hardware states; and (3) a hybrid inference engine that enhances efficiency through asynchronous execution and batch size optimization.Extensive results show that SparOA achieves an average speedup of 1.22-1.31x compared to all baselines, and outperforms the CPU-Only by up to 50.7x. Also, SparOA achieves optimal energy-per-inference, consuming 7\%-16\% less energy than the SOTA co-execution baseline.

</details>


### [10] [Systemic approach for modeling a generic smart grid](https://arxiv.org/abs/2511.19460)
*Sofiane Ben Amor,Guillaume Guerard,Loup-Noé Levy*

Main category: cs.DC

TL;DR: 提出了一种智能电网的骨干模型，用于测试电网的替代场景，通过分布式子系统优化实现生产和消费调度。


<details>
  <summary>Details</summary>
Motivation: 智能电网技术进步带来了复杂的跨学科建模问题，传统计算方法难以解决，需要系统性的集成建模方法。

Method: 开发智能电网骨干模型，采用分布式子系统优化方法，模拟不同系统以验证假设。

Result: 实现了生产和消费调度，同时保持了灵活性和可扩展性。

Conclusion: 该工具为智能电网提供了有效的测试平台，能够在构建实际规模模型前验证各种假设和场景。

Abstract: Smart grid technological advances present a recent class of complex interdisciplinary modeling and increasingly difficult simulation problems to solve using traditional computational methods. To simulate a smart grid requires a systemic approach to integrated modeling of power systems, energy markets, demand-side management, and much other resources and assets that are becoming part of the current paradigm of the power grid. This paper presents a backbone model of a smart grid to test alternative scenarios for the grid. This tool simulates disparate systems to validate assumptions before the human scale model. Thanks to a distributed optimization of subsystems, the production and consumption scheduling is achieved while maintaining flexibility and scalability.

</details>


### [11] [Urban Buildings Energy Consumption Estimation Using HPC: A Case Study of Bologna](https://arxiv.org/abs/2511.19463)
*Aldo Canfora,Eleonora Bergamaschi,Riccardo Mioli,Federico Battini,Mirko Degli Esposti,Giorgio Pedrazzi,Chiara Dellacasa*

Main category: cs.DC

TL;DR: 开发了一个集成EnergyPlus模拟、高性能计算和开放地理数据集的UBEM管道，用于估算意大利博洛尼亚建筑能耗需求


<details>
  <summary>Details</summary>
Motivation: 城市建筑能源建模在理解和预测城市尺度能耗方面具有重要作用

Method: 结合EnergyPlus模拟、HPC和开放地理数据集，从博洛尼亚开放数据门户获取几何信息，从区域建筑法规和TABULA数据库获取非几何属性

Result: 在Leonardo超级计算机上成功模拟了约25,000栋建筑，耗时不到30分钟

Conclusion: 该UBEM管道能够高效准确地估算城市尺度建筑能耗需求

Abstract: Urban Building Energy Modeling (UBEM) plays a central role in understanding and forecasting energy consumption at the city scale. In this work, we present a UBEM pipeline that integrates EnergyPlus simulations, high-performance computing (HPC), and open geospatial datasets to estimate the energy demand of buildings in Bologna, Italy. Geometric information including building footprints and heights was obtained from the Bologna Open Data portal and enhanced with aerial LiDAR measurements. Non-geometric attributes such as construction materials, insulation characteristics, and window performance were derived from regional building regulations and the European TABULA database. The computation was carried out on Leonardo, the Cineca-hosted supercomputer, enabling the simulation of approximately 25,000 buildings in under 30 minutes.

</details>


### [12] [Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments](https://arxiv.org/abs/2511.19464)
*Marcio Pohlmann,Alex Severo,Gefté Almeida,Diego Kreutz,Tiago Heinrich,Lourenço Pereira*

Main category: cs.DC

TL;DR: 评估本地运行的小型语言模型(SLMs)在事件分类任务中的表现，以替代基于云的LLMs，解决成本、延迟和机密性问题


<details>
  <summary>Details</summary>
Motivation: SOCs和CSIRTs面临自动化事件分类的压力，但使用基于云的LLMs存在成本、延迟和机密性风险

Method: 评估21个参数范围从1B到20B的模型，改变温度超参数，测量执行时间和精度，比较两种不同架构

Result: 温度对性能影响很小，而参数数量和GPU容量是决定性因素

Conclusion: 本地执行的SLMs能够满足事件分类的挑战，参数数量和GPU能力是关键因素

Abstract: SOCs and CSIRTs face increasing pressure to automate incident categorization, yet the use of cloud-based LLMs introduces costs, latency, and confidentiality risks. We investigate whether locally executed SLMs can meet this challenge. We evaluated 21 models ranging from 1B to 20B parameters, varying the temperature hyperparameter and measuring execution time and precision across two distinct architectures. The results indicate that temperature has little influence on performance, whereas the number of parameters and GPU capacity are decisive factors.

</details>


### [13] [Towards a future space-based, highly scalable AI infrastructure system design](https://arxiv.org/abs/2511.19468)
*Blaise Agüera y Arcas,Travis Beals,Maria Biggs,Jessica V. Bloom,Thomas Fischbacher,Konstantin Gromov,Urs Köster,Rishiraj Pravahan,James Manyika*

Main category: cs.DC

TL;DR: 该论文探讨了在太空中构建可扩展的机器学习计算系统，使用配备太阳能电池板、光学卫星间链路和TPU芯片的卫星群，利用太阳能在轨道上进行AI计算。


<details>
  <summary>Details</summary>
Motivation: 随着AI计算需求的持续增长，需要寻找可持续的能源来源。太阳是太阳系中最大的能源，因此考虑如何让未来的AI基础设施最有效地利用太阳能。

Method: 使用配备太阳能电池板、光学卫星间链路和Google TPU加速器芯片的卫星群，卫星在近距离编队飞行，通过81颗卫星组成1公里半径的集群，使用高精度ML模型控制大规模星座。

Result: Trillium TPU经过辐射测试，在相当于5年任务寿命的总电离剂量下没有永久性故障，并进行了位翻转错误特性分析。发射成本分析显示到2030年代中期LEO发射成本可能降至≤200美元/公斤。

Conclusion: 在太空中构建基于太阳能的AI计算基础设施是可行的，通过卫星群编队飞行和光学通信可以实现高带宽低延迟的计算系统，发射成本的下降使这一方案更具经济可行性。

Abstract: If AI is a foundational general-purpose technology, we should anticipate that demand for AI compute -- and energy -- will continue to grow. The Sun is by far the largest energy source in our solar system, and thus it warrants consideration how future AI infrastructure could most efficiently tap into that power. This work explores a scalable compute system for machine learning in space, using fleets of satellites equipped with solar arrays, inter-satellite links using free-space optics, and Google tensor processing unit (TPU) accelerator chips. To facilitate high-bandwidth, low-latency inter-satellite communication, the satellites would be flown in close proximity. We illustrate the basic approach to formation flight via a 81-satellite cluster of 1 km radius, and describe an approach for using high-precision ML-based models to control large-scale constellations. Trillium TPUs are radiation tested. They survive a total ionizing dose equivalent to a 5 year mission life without permanent failures, and are characterized for bit-flip errors. Launch costs are a critical part of overall system cost; a learning curve analysis suggests launch to low-Earth orbit (LEO) may reach $\lesssim$\$200/kg by the mid-2030s.

</details>


### [14] [Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments](https://arxiv.org/abs/2511.19479)
*Sangam Ghimire,Paribartan Timalsina,Nirjal Bhurtel,Bishal Neupane,Bigyan Byanju Shrestha,Subarna Bhattarai,Prajwal Gaire,Jessica Thapa,Sudan Jha*

Main category: cs.DC

TL;DR: 提出了一个在混合高性能计算和云环境中高效运行的联邦学习框架，解决了系统异构性、通信开销和资源调度等关键挑战，同时保持模型准确性和数据隐私。


<details>
  <summary>Details</summary>
Motivation: 随着对可扩展和隐私感知AI系统的需求增长，联邦学习成为有前景的解决方案，允许去中心化模型训练而无需移动原始数据。同时，高性能计算和云基础设施的结合提供了巨大计算能力，但在处理异构硬件、通信限制和非均匀数据时引入了新的复杂性。

Method: 构建了一个在混合HPC和云环境中运行的联邦学习框架，通过解决系统异构性、通信开销和资源调度等关键挑战来优化性能。

Result: 在混合测试平台上的实验表明，即使在非独立同分布数据分布和不同硬件条件下，系统在可扩展性、容错性和收敛性方面表现出色。

Conclusion: 这些结果突显了联邦学习作为在现代分布式计算环境中构建可扩展AI系统的实用方法的潜力。

Abstract: As the demand grows for scalable and privacy-aware AI systems, Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training without moving raw data. At the same time, the combination of high-performance computing (HPC) and cloud infrastructure offers vast computing power but introduces new complexities, especially when dealing with heterogeneous hardware, communication limits, and non-uniform data. In this work, we present a federated learning framework built to run efficiently across mixed HPC and cloud environments. Our system addresses key challenges such as system heterogeneity, communication overhead, and resource scheduling, while maintaining model accuracy and data privacy. Through experiments on a hybrid testbed, we demonstrate strong performance in terms of scalability, fault tolerance, and convergence, even under non-Independent and Identically Distributed (non-IID) data distributions and varied hardware. These results highlight the potential of federated learning as a practical approach to building scalable Artificial Intelligence (AI) systems in modern, distributed computing settings.

</details>


### [15] [Enabling Scientific Workflow Scheduling Research in Non-Uniform Memory Access Architectures](https://arxiv.org/abs/2511.19832)
*Aurelio Vivas,Harold Castro*

Main category: cs.DC

TL;DR: nFlows是一个NUMA感知的工作流执行运行时系统，用于在NUMA架构的HPC系统上建模、执行、模拟和验证数据密集型工作流的调度算法。


<details>
  <summary>Details</summary>
Motivation: 现代HPC系统普遍采用NUMA架构，包含多个NUMA域、异构内存区域（HBM和DRAM）以及加速器，这增加了数据访问延迟的变异性，使任务和数据放置复杂化。然而，大多数工作流调度策略最初是为Grid或Cloud环境开发的，很少考虑NUMA感知。

Method: 开发了nFlows系统，支持构建仿真模型并在物理系统上直接执行，能够研究NUMA对调度的影响、设计NUMA感知算法、分析数据移动行为、识别性能瓶颈以及探索内存内工作流执行。

Result: 提出了nFlows系统的设计、实现和验证方法，该系统能够模拟和验证NUMA感知的调度算法在数据密集型工作流上的性能。

Conclusion: nFlows填补了NUMA感知工作流调度在HPC系统中的空白，为研究NUMA架构对数据密集型工作流性能的影响提供了有效工具。

Abstract: Data-intensive scientific workflows increasingly rely on high-performance computing (HPC) systems, complementing traditional Grid and Cloud platforms. However, workflow scheduling on HPC infrastructures remains challenging due to the prevalence of non-uniform memory access (NUMA) architectures. These systems require schedulers to account for data locality not only across distributed environments but also within each node. Modern HPC nodes integrate multiple NUMA domains and heterogeneous memory regions, such as high-bandwidth memory (HBM) and DRAM, and frequently attach accelerators (GPUs or FPGAs) and network interface cards (NICs) to specific NUMA nodes. This design increases the variability of data-access latency and complicates the placement of both tasks and data. Despite these constraints, most workflow scheduling strategies were originally developed for Grid or Cloud environments and rarely incorporate NUMA-aware considerations. To address this gap, this work introduces nFlows, a NUMA-aware Workflow Execution Runtime System that enables the modeling, bare-metal execution, simulation, and validation of scheduling algorithms for data-intensive workflows on NUMA-based HPC systems. The system's design, implementation, and validation methodology are presented. nFlows supports the construction of simulation models and their direct execution on physical systems, enabling studies of NUMA effects on scheduling, the design of NUMA-aware algorithms, the analysis of data-movement behavior, the identification of performance bottlenecks, and the exploration of in-memory workflow execution.

</details>


### [16] [Batch Denoising for AIGC Service Provisioning in Wireless Edge Networks](https://arxiv.org/abs/2511.19847)
*Jinghang Xu,Kun Guo,Wei Teng,Chenxi Liu,Wei Feng*

Main category: cs.DC

TL;DR: 提出了一种用于无线边缘网络中AIGC服务供应的批量去噪框架，联合优化内容生成和传输，在端到端延迟约束下最大化平均服务质量。


<details>
  <summary>Details</summary>
Motivation: 基于两个经验观察：(1)批量去噪通过增强并行性有效减少每步去噪延迟；(2)早期去噪步骤对生成质量的影响大于后期步骤。

Method: 开发了STACKING算法优化批量去噪，不依赖于特定质量函数形式，计算复杂度低。在批量解决方案基础上进一步优化AIGC服务间的带宽分配。

Result: 仿真结果表明该算法在提供高质量、低延迟AIGC服务方面具有优越性能。

Conclusion: 提出的批量去噪框架和联合优化方法能够有效提升无线边缘网络中AIGC服务的质量和效率。

Abstract: Artificial intelligence-generated content (AIGC) service provisioning in wireless edge networks involves two phases: content generation on edge servers and content transmission to mobile devices. In this paper, we take image generation as a representative application and propose a batch denoising framework, followed by a joint optimization of content generation and transmission, with the objective of maximizing the average AIGC service quality under an end-to-end service delay constraint. Motivated by the empirical observations that (i) batch denoising effectively reduces per-step denoising delay by enhancing parallelism and (ii) early denoising steps have a greater impact on generation quality than later steps, we develop the STACKING algorithm to optimize batch denoising. The STACKING operates independently of any specific form of the content quality function and achieves lower computational complexity. Building on the batch solution, we further optimize bandwidth allocation across AIGC services. Simulation results demonstrate the superior performance of our algorithm in delivering high-quality, lower-latency AIGC services.

</details>


### [17] [Improved Linear-Time Construction of Minimal Dominating Set via Mobile Agents](https://arxiv.org/abs/2511.19880)
*Prabhat Kumar Chand,Anisur Rahaman Molla*

Main category: cs.DC

TL;DR: 本文提出了在匿名图中使用移动代理计算最小支配集(mDS)的线性时间算法，在同步设置下仅需O(n)轮次和O(log n)位内存，无需全局参数先验知识。


<details>
  <summary>Details</summary>
Motivation: 移动代理作为分布式环境中解决图问题的强大框架，需要高效算法来解决经典图问题。最小支配集是图论中的基本问题，现有算法在复杂度和内存使用方面有待改进。

Method: 基于最近提出的最优分散算法，设计了两种新算法：1) 在根配置下使用同步移动代理模型；2) 在任意配置下工作。算法利用代理的局部计算能力和图遍历能力，构建生成树并选举唯一领导者。

Result: 在连通n节点图上，无论初始配置是根配置还是任意配置，都能在O(n)轮次内计算最小支配集，每个代理仅需O(log n)位内存，且无需全局参数知识。算法同时构建生成树和选举领导者。

Conclusion: 提出的算法在同步移动代理模型中实现了最小支配集计算的线性时间解决方案，显著改进了文献中已知的最佳复杂度结果，同时生成树构建和领导者选举作为自然副产品也具有独立研究价值。

Abstract: Mobile agents have emerged as a powerful framework for solving fundamental graph problems in distributed settings in recent times. These agents, modelled as autonomous physical or software entities, possess local computation power, finite memory and have the ability to traverse a graph, offering efficient solutions to a range of classical problems. In this work, we focus on the problem of computing a \emph{minimal dominating set} (mDS) in anonymous graphs using mobile agents. Building on the recently proposed optimal dispersion algorithm on the synchronous mobile agent model, we design two new algorithms that achieve a \emph{linear-time} solution for this problem in the synchronous setting. Specifically, given a connected $n$-node graph with $n$ agents initially placed in either rooted or arbitrary configurations, we show that an mDS can be computed in $O(n)$ rounds using only $O(\log n)$ bits of memory per agent, without using any prior knowledge of any global parameters. This improves upon the best-known complexity results in the literature over the same model. In addition, as natural by-products of our methodology, our algorithms also construct a spanning tree and elect a unique leader in $O(n)$ rounds, which are also important results of independent interest in the mobile-agent framework.

</details>


### [18] [PolarStore: High-Performance Data Compression for Large-Scale Cloud-Native Databases](https://arxiv.org/abs/2511.19949)
*Qingda Hu,Xinjun Yang,Feifei Li,Junru Li,Ya Lin,Yuqi Zhou,Yicong Zhu,Junwei Zhang,Rongbiao Xie,Ling Zhou,Bin Wu,Wenchao Zhou*

Main category: cs.DC

TL;DR: PolarStore是一个用于云原生关系数据库的压缩共享存储系统，通过软硬件结合的压缩机制实现高压缩比和低性能开销。


<details>
  <summary>Details</summary>
Motivation: 云原生RDBMS虽然提供弹性计算资源，但存储成本仍是关键问题。现有压缩方法存在性能开销大或灵活性不足的权衡问题。

Method: 采用双层级压缩机制：在PolarCSD硬件中进行存储压缩，同时在软件中进行轻量级压缩。结合数据库导向优化、硬件改进和压缩感知调度方案。

Result: 在PolarDB中部署数千台存储服务器，管理超过100PB数据，实现3.55的压缩比，存储成本降低约60%，同时保持与未压缩集群相当的性能。

Conclusion: PolarStore成功解决了云原生RDBMS中压缩存储的成本与性能权衡问题，通过软硬件协同设计实现了高效的数据压缩。

Abstract: In recent years, resource elasticity and cost optimization have become essential for RDBMSs. While cloud-native RDBMSs provide elastic computing resources via disaggregated computing and storage, storage costs remain a critical user concern. Consequently, data compression emerges as an effective strategy to reduce storage costs. However, existing compression approaches in RDBMSs present a stark trade-off: software-based approaches incur significant performance overheads, while hardware-based alternatives lack the flexibility required for diverse database workloads. In this paper, we present PolarStore, a compressed shared storage system for cloud-native RDBMSs. PolarStore employs a dual-layer compression mechanism that combines in-storage compression in PolarCSD hardware with lightweight compression in software. This design leverages the strengths of both approaches. PolarStore also incorporates database-oriented optimizations to maintain high performance on critical I/O paths. Drawing from large-scale deployment experiences, we also introduce hardware improvements for PolarCSD to ensure host-level stability and propose a compression-aware scheduling scheme to improve cluster-level space efficiency. PolarStore is currently deployed on thousands of storage servers within PolarDB, managing over 100 PB of data. It achieves a compression ratio of 3.55 and reduces storage costs by approximately 60%. Remarkably, these savings are achieved while maintaining performance comparable to uncompressed clusters.

</details>


### [19] [SwitchDelta: Asynchronous Metadata Updating for Distributed Storage with In-Network Data Visibility](https://arxiv.org/abs/2511.19978)
*Junru Li,Qing Wang,Zhe Yang,Shuo Liu,Jiwu Shu,Youyou Lu*

Main category: cs.DC

TL;DR: SwitchDelta通过将元数据更新移出关键路径来加速分布式存储系统中的有序写入，利用可编程交换机缓冲元数据更新，在网络中实现数据可见性并保持强一致性。


<details>
  <summary>Details</summary>
Motivation: 传统分布式存储系统采用有序写入来保持强一致性，但这种方式存在性能瓶颈。需要一种方法来加速有序写入过程。

Method: 1) 在可编程交换机中缓冲飞行中的元数据更新；2) 使用尽力而为的数据平面设计克服交换机资源限制；3) 设计新颖的元数据更新协议利用网络内数据可见性的优势。

Result: 在三种分布式内存存储系统（日志结构键值存储、文件系统和二级索引）中评估，SwitchDelta将写入操作延迟降低高达52.4%，在写入密集型工作负载下吞吐量提升高达126.9%。

Conclusion: SwitchDelta通过将元数据更新移出关键路径，有效加速了分布式存储系统的有序写入性能，同时保持了强一致性。

Abstract: Distributed storage systems typically maintain strong consistency between data nodes and metadata nodes by adopting ordered writes: 1) first installing data; 2) then updating metadata to make data visible.We propose SwitchDelta to accelerate ordered writes by moving metadata updates out of the critical path. It buffers in-flight metadata updates in programmable switches to enable data visibility in the network and retain strong consistency. SwitchDelta uses a best-effort data plane design to overcome the resource limitation of switches and designs a novel metadata update protocol to exploit the benefits of in-network data visibility. We evaluate SwitchDelta in three distributed in-memory storage systems: log-structured key-value stores, file systems, and secondary indexes. The evaluation shows that SwitchDelta reduces the latency of write operations by up to 52.4% and boosts the throughput by up to 126.9% under write-heavy workloads.

</details>


### [20] [QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation](https://arxiv.org/abs/2511.20100)
*Xinguo Zhu,Shaohui Peng,Jiaming Guo,Yunji Chen,Qi Guo,Yuanbo Wen,Hang Qin,Ruizhi Chen,Qirui Zhou,Ke Gao,Yanjun Wu,Chen Zhao,Ling Li*

Main category: cs.DC

TL;DR: MTMC是一个分层框架，通过将优化策略与实现细节解耦来解决GPU内核生成的正确性和效率问题。它使用强化学习指导轻量级LLM学习语义优化策略，再利用通用LLM逐步实现优化方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法直接生成完整优化程序，需要在优化策略和实现代码的极大空间中探索，导致正确性和效率的冲突。需要一种更有效的方法来导航这个复杂空间。

Method: 提出Macro Thinking Micro Coding框架：Macro Thinking使用强化学习指导轻量级LLM学习最大化硬件利用率的优化策略；Micro Coding利用通用LLM逐步实现优化提案，避免全内核生成错误。

Result: 在KernelBench上，MTMC在Level 1-2达到近100%准确率，Level 3达到70%准确率，比现有最佳方法提升50%以上，速度比LLM快7.3倍，比专家优化的PyTorch Eager内核快2.2倍。在TritonBench上达到59.64%准确率和34倍加速。

Conclusion: MTMC通过分层解耦优化策略和实现细节，有效解决了LLM在GPU内核生成中的正确性和效率问题，展示了在复杂优化空间中的卓越性能。

Abstract: Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.

</details>


### [21] [Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management](https://arxiv.org/abs/2511.20172)
*Xinjun Yang,Qingda Hu,Junru Li,Feifei Li,Yuqi Zhou,Yicong Zhu,Qiuru Lin,Jian Dai,Yang Kong,Jiayu Zhang,Guoqiang Xu,Qiang Liu*

Main category: cs.DC

TL;DR: Beluga是一种基于CXL技术的新型内存架构，通过CXL交换机让GPU和CPU共享大规模内存池，显著降低LLM推理中的内存访问延迟，相比RDMA方案减少89.6%的首令牌时间并提升7.35倍吞吐量。


<details>
  <summary>Details</summary>
Motivation: LLM模型规模快速增长和对长上下文推理的需求使得内存成为GPU加速服务系统的关键瓶颈。现有RDMA分解内存池方案存在高延迟、复杂通信协议和同步开销等问题。

Method: 提出Beluga内存架构，支持GPU和CPU通过CXL交换机以原生load/store语义访问共享的大规模内存池，并基于此设计Beluga-KVCache系统专门管理LLM推理中的大规模KVCache。

Result: 在vLLM推理引擎中，相比RDMA方案，Beluga-KVCache实现了89.6%的首令牌时间减少和7.35倍的吞吐量提升。

Conclusion: Beluga是首个通过CXL交换机让GPU直接访问大规模内存池的系统，为GPU低延迟共享访问海量内存资源迈出了重要一步。

Abstract: The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.

</details>


### [22] [Interactive Visualization of Proof-of-Work Consensus Protocol on Raspberry Pi](https://arxiv.org/abs/2511.20391)
*Anton Ivashkevich,Matija Piškorec,Claudio J. Tessone*

Main category: cs.DC

TL;DR: 开发了一个基于多个树莓派的完整以太坊PoW区块链网络原型，用于教育和演示区块链概念


<details>
  <summary>Details</summary>
Motivation: 创建一个易于设置、完全独立的区块链系统，通过可视化界面向广泛受众展示区块链基本原理，特别适合教育用途

Method: 使用多个树莓派计算机构建以太坊PoW区块链网络，通过本地WiFi路由器连接，配备LCD屏幕显示本地区块链状态，提供基于Web的配置界面

Result: 成功实现了功能完整的PoW共识机制原型系统，能够可视化展示区块链状态变化、共识过程以及网络拓扑和延迟等因素对共识的影响

Conclusion: 该原型系统有效演示了区块链核心概念，特别适合教育环境，能够直观展示PoW共识机制及其在各种网络条件下的行为表现

Abstract: We describe a prototype of a fully capable Ethereum Proof-of-Work (PoW) blockchain network running on multiple Raspberry Pi (RPi) computers. The prototype is easy to set up and is intended to function as a completely standalone system, using a local WiFi router for connectivity. It features LCD screens for visualization of the local state of blockchain ledgers on each RPi, making it ideal for educational purposes and to demonstrate fundamental blockchain concepts to a wide audience. For example, a functioning PoW consensus is easily visible from the LCD screens, as well as consensus degradation which might arise from various factors, including peer-to-peer topology and communication latency - all parameters which can be configured from the central web-based interface.

</details>


### [23] [Efficient Parallel Implementation of the Pilot Assignment Problem in Massive MIMO Systems](https://arxiv.org/abs/2511.20511)
*Eman Alqudah,Ashfaq Khokhar*

Main category: cs.DC

TL;DR: 提出了一种优化的混合K-means聚类和遗传算法(SK-means GA)用于大规模MIMO系统中的导频分配，通过并行实现(PK-means GA)在FPGA上实现3.5毫秒的收敛时间，比传统遗传算法快29.3%。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO系统中导频序列分配是一个关键挑战，多个用户共享相同导频序列会导致干扰，降低信道估计精度，这直接影响自动驾驶和工业物联网等实时应用。

Method: 提出优化的混合K-means聚类和遗传算法(SK-means GA)，并在FPGA上使用Vivado高级综合工具开发并行实现(PK-means GA)，应用循环展开、流水线和函数内联等优化技术。

Result: SK-means GA比传统GA收敛时间减少29.3%(82秒 vs 116秒)，并行实现PK-means GA在FPGA上加速到3.5毫秒收敛。

Conclusion: PK-means GA在执行速度上的显著改进使其非常适合低延迟实时无线网络(6G)应用。

Abstract: The assignment of the pilot sequence is a critical challenge in massive MIMO systems, as sharing the same pilot sequence among multiple users causes interference, which degrades the accuracy of the channel estimation. This problem, equivalent to the NP-hard graph coloring problem, directly impacts real-time applications such as autonomous driving and industrial IoT, where minimizing channel estimation time is crucial. This paper proposes an optimized hybrid K-means clustering and Genetic Algorithm (SK-means GA) to improve the pilot assignment efficiency, achieving a 29.3% reduction in convergence time (82s vs. 116s for conventional GA). A parallel implementation (PK-means GA) is developed on an FPGA using Vivado High-Level Synthesis Tools (HLST) to further enhance the run-time performance, accelerating convergence to 3.5 milliseconds. Within Vivado implementation, different optimization techniques such as loop unrolling, pipelining, and function inlining are applied to realize the reported speedup. This significant improvement of PK-means GA in execution speed makes it highly suitable for low-latency real-time wireless networks (6G)

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [24] [CAMformer: Associative Memory is All You Need](https://arxiv.org/abs/2511.19740)
*Tergel Molom-Ochir,Benjamin F. Morris,Mark Horton,Chiyue Wei,Cong Guo,Brady Taylor,Peter Liu,Shan X. Wang,Deliang Fan,Hai Helen Li,Yiran Chen*

Main category: cs.AR

TL;DR: CAMformer是一种新型Transformer加速器，通过将注意力重新解释为关联记忆操作，使用电压域二进制注意力内容可寻址存储器(BA-CAM)计算注意力分数，实现恒定时间相似性搜索，显著提升能效和吞吐量。


<details>
  <summary>Details</summary>
Motivation: Transformer由于注意力机制中的密集相似性计算导致二次计算成本，面临可扩展性挑战。

Method: 采用BA-CAM通过模拟电荷共享计算注意力分数，用物理相似性感知替代数字算术；集成层次化两阶段top-k过滤、流水线执行和高精度上下文化。

Result: 在BERT和Vision Transformer工作负载上，CAMformer相比最先进加速器实现10倍以上能效提升、最高4倍吞吐量提升和6-8倍面积降低，同时保持近乎无损的精度。

Conclusion: CAMformer通过重新设计注意力计算为关联记忆操作，在保持算法精度的同时实现了显著的架构效率提升。

Abstract: Transformers face scalability challenges due to the quadratic cost of attention, which involves dense similarity computations between queries and keys. We propose CAMformer, a novel accelerator that reinterprets attention as an associative memory operation and computes attention scores using a voltage-domain Binary Attention Content Addressable Memory (BA-CAM). This enables constant-time similarity search through analog charge sharing, replacing digital arithmetic with physical similarity sensing. CAMformer integrates hierarchical two-stage top-k filtering, pipelined execution, and high-precision contextualization to achieve both algorithmic accuracy and architectural efficiency. Evaluated on BERT and Vision Transformer workloads, CAMformer achieves over 10x energy efficiency, up to 4x higher throughput, and 6-8x lower area compared to state-of-the-art accelerators--while maintaining near-lossless accuracy.

</details>


### [25] [Pickle Prefetcher: Programmable and Scalable Last-Level Cache Prefetcher](https://arxiv.org/abs/2511.19973)
*Hoa Nguyen,Pongstorn Maidee,Jason Lowe-Power,Alireza Kaviani*

Main category: cs.AR

TL;DR: Pickle Prefetcher是一个可编程的LLC预取器，通过软件定义预取策略来处理不规则内存访问模式，相比传统预取技术显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代高性能架构使用大型LLC，但对于不规则内存访问模式会增加延迟。现有基于预测的预取器难以处理现代应用中普遍存在的不规则访问模式。

Method: 采用可编程方法，允许软件通过简单编程接口定义自己的预取策略，无需扩展ISA。将硬件预测的逻辑复杂性转化为软件可编程性。

Result: 在gem5全系统模拟中，Pickle Prefetcher显著优于传统预取技术。在GAPBS BFS实现上比基线系统提速1.74倍，与私有缓存预取器结合时比仅使用私有缓存预取器的系统提速1.40倍。

Conclusion: 通过软件可编程性替代复杂硬件预测，Pickle Prefetcher能够有效处理不规则内存访问模式，在保持硬件资源效率的同时实现显著性能提升。

Abstract: Modern high-performance architectures employ large last-level caches (LLCs). While large LLCs can reduce average memory access latency for workloads with a high degree of locality, they can also increase latency for workloads with irregular memory access patterns. Prefetchers are widely used to reduce memory latency by prefetching data into the cache hierarchy before it is accessed by the core. However, existing prediction-based prefetchers often struggle with irregular memory access patterns, which are especially prevalent in modern applications. This paper introduces the Pickle Prefetcher, a programmable and scalable LLC prefetcher designed to handle independent irregular memory access patterns effectively. Instead of relying on static heuristics or complex prediction algorithms, Pickle Prefetcher allows software to define its own prefetching strategies using a simple programming interface without expanding the instruction set architecture (ISA). By trading the logic complexity of hardware prediction for software programmability, Pickle Prefetcher can adapt to a wide range of access patterns without requiring extensive hardware resources for prediction. This allows the prefetcher to dedicate its resources to scheduling and issuing timely prefetch requests. Graph applications are an example where the memory access pattern is irregular but easily predictable by software. Through extensive evaluations of the Pickle Prefetcher on gem5 full-system simulations, we demonstrate tha Pickle Prefetcher significantly outperforms traditional prefetching techniques. Our results show that Pickle Prefetcher achieves speedups of up to 1.74x on the GAPBS breadth-first search (BFS) implementation over a baseline system. When combined with private cache prefetchers, Pickle Prefetcher provides up to a 1.40x speedup over systems using only private cache prefetchers.

</details>


### [26] [R3A: Reliable RTL Repair Framework with Multi-Agent Fault Localization and Stochastic Tree-of-Thoughts Patch Generation](https://arxiv.org/abs/2511.20090)
*Zizhang Luo,Fan Cui,Kexing Zhou,Runlin Guo,Mile Xia,Hongyuan Hou,Yun Lian*

Main category: cs.AR

TL;DR: R3A是一个基于大语言模型的RTL程序自动修复框架，通过随机树搜索和多智能体故障定位方法，显著提高了RTL bug修复的可靠性和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统自动程序修复方法依赖固定模板，只能处理有限bug；而现有LLM方法由于随机性和RTL代码波形长上下文问题，修复结果不可靠。

Method: 提出R3A框架：1）随机树搜索方法控制补丁生成智能体探索验证解决方案；2）多智能体故障定位方法寻找故障候选点作为补丁生成起点。

Result: 在RTL-repair数据集中，R3A在给定时间内修复了90.6%的bug，比传统方法和其他LLM方法多覆盖45%的bug，平均pass@5率达到86.7%。

Conclusion: R3A框架通过结合随机树搜索和多智能体故障定位，显著提高了基于LLM的RTL程序修复的可靠性和有效性。

Abstract: Repairing RTL bugs is crucial for hardware design and verification. Traditional automatic program repair (APR) methods define dedicated search spaces to locate and fix bugs with program synthesis. However, they heavily rely on fixed templates and can only deal with limited bugs. As an alternative, Large Language Models with the ability to understand code semantics can be explored for RTL repair. However, they suffer from unreliable outcomes due to inherent randomness and long input contexts of RTL code and waveform. To address these challenges, we propose R3A, an LLM-based automatic RTL program repair framework upon the basic model to improve reliability. R3A proposes the stochastic Tree-Of-Thoughts method to control a patch generation agent to explore a validated solution for the bug. The algorithm samples search states according to a heuristic function to balance between exploration and exploitation for a reliable outcome. Besides, R3A proposes a multi-agent fault localization method to find fault candidates as the starting points for the patch generation agent, further increasing the reliability. Experiments show R3A can fix 90.6% of bugs in the RTL-repair dataset within a given time limit, which covers 45% more bugs than traditional methods and other LLM-based approaches, while achieving an 86.7% pass@5 rate on average, showing a high reliability.

</details>
