{"id": "2602.02892", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.02892", "abs": "https://arxiv.org/abs/2602.02892", "authors": ["Zhuolun Xiang", "Andrei Tonkikh", "Alexander Spiegelman"], "title": "Prefix Consensus For Censorship Resistant BFT", "comment": null, "summary": "Despite broad use of BFT consensus in blockchains, censorship resistance is weak: leaders can exclude transactions, a growing concern for trading and DeFi.\n  We address this by introducing a new abstraction and protocol stack. First, we introduce \\emph{Prefix Consensus}, where parties input vectors and output $(v^{\\sf low},v^{\\sf high})$ that (i) extend the maximum common prefix of honest inputs and (ii) satisfy $v_i^{\\sf low}\\preceq v_j^{\\sf high}$ for all honest $i,j$. Unlike classical consensus, no single output is required. We show Prefix Consensus is solvable asynchronously and give tight round-complexity bounds.\n  We then define \\emph{Strong Prefix Consensus}, requiring agreement on the \\emph{high} output. Our protocol is leaderless and partially synchronous: one Prefix Consensus instance decides (possibly different) lows, and additional instances yield a unique safe-to-extend high, even if an adversary can suspend one party per round.\n  We lift this to a leaderless, multi-proposer, censorship-resistant BFT SMR protocol: per slot, all parties broadcast proposals, deterministically rank them, and run one Strong Prefix Consensus on proposal hashes, committing honest proposals in \\emph{four rounds}. A deterministic demotion rule updates the ranking when a party's proposal is excluded, implying that after GST at most $f$ slots can miss an honest proposal while progress remains leaderless under suspension and up to $f{-}1$ Byzantine faults.\n  Finally, we connect Prefix Consensus to graded and binary/validated consensus: we obtain an optimal-latency graded consensus (3 message delays) and leaderless Binary/Validated Consensus with worst-case message complexity $O(n^3)$ and communication $O(n^4)$.", "AI": {"tldr": "\u63d0\u51faPrefix Consensus\u65b0\u62bd\u8c61\u548c\u534f\u8bae\u6808\uff0c\u89e3\u51b3BFT\u5171\u8bc6\u4e2d\u7684\u5ba1\u67e5\u62b5\u6297\u95ee\u9898\uff0c\u5b9e\u73b0\u65e0\u9886\u5bfc\u8005\u3001\u591a\u63d0\u8bae\u8005\u7684\u6297\u5ba1\u67e5BFT SMR\u534f\u8bae", "motivation": "\u73b0\u6709BFT\u5171\u8bc6\u5728\u533a\u5757\u94fe\u4e2d\u5ba1\u67e5\u62b5\u6297\u80fd\u529b\u5f31\uff0c\u9886\u5bfc\u8005\u53ef\u4ee5\u6392\u9664\u4ea4\u6613\uff0c\u8fd9\u5bf9\u4ea4\u6613\u548cDeFi\u6784\u6210\u65e5\u76ca\u589e\u957f\u7684\u5a01\u80c1", "method": "1. \u5f15\u5165Prefix Consensus\u62bd\u8c61\uff0c\u5404\u65b9\u8f93\u5165\u5411\u91cf\u5e76\u8f93\u51fa\u9ad8\u4f4e\u8fb9\u754c\uff1b2. \u5b9a\u4e49Strong Prefix Consensus\uff0c\u8981\u6c42\u5728\u9ad8\u8f93\u51fa\u4e0a\u8fbe\u6210\u4e00\u81f4\uff1b3. \u6784\u5efa\u65e0\u9886\u5bfc\u8005\u3001\u591a\u63d0\u8bae\u8005\u7684\u6297\u5ba1\u67e5BFT SMR\u534f\u8bae\uff0c\u4f7f\u7528\u786e\u5b9a\u6027\u6392\u540d\u548c\u964d\u7ea7\u89c4\u5219", "result": "1. Prefix Consensus\u53ef\u5728\u5f02\u6b65\u73af\u5883\u4e0b\u89e3\u51b3\u5e76\u7ed9\u51fa\u7d27\u7684\u8f6e\u590d\u6742\u5ea6\u754c\uff1b2. Strong Prefix Consensus\u534f\u8bae\u65e0\u9886\u5bfc\u8005\u4e14\u90e8\u5206\u540c\u6b65\uff1b3. \u6297\u5ba1\u67e5BFT SMR\u534f\u8bae\u5728\u56db\u8f6e\u5185\u63d0\u4ea4\u8bda\u5b9e\u63d0\u6848\uff1b4. \u5c06Prefix Consensus\u8fde\u63a5\u5230\u5206\u7ea7\u548c\u4e8c\u8fdb\u5236/\u9a8c\u8bc1\u5171\u8bc6\uff0c\u83b7\u5f97\u6700\u4f18\u5ef6\u8fdf\u7684\u5206\u7ea7\u5171\u8bc6", "conclusion": "\u901a\u8fc7Prefix Consensus\u65b0\u62bd\u8c61\u6784\u5efa\u4e86\u6709\u6548\u7684\u6297\u5ba1\u67e5BFT\u5171\u8bc6\u534f\u8bae\u6808\uff0c\u89e3\u51b3\u4e86\u533a\u5757\u94fe\u4e2d\u9886\u5bfc\u8005\u5ba1\u67e5\u4ea4\u6613\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u9886\u5bfc\u8005\u3001\u591a\u63d0\u8bae\u8005\u7684\u5b89\u5168\u72b6\u6001\u673a\u590d\u5236"}}
{"id": "2602.02987", "categories": ["cs.DC", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.02987", "abs": "https://arxiv.org/abs/2602.02987", "authors": ["Ruihan Lin", "Zezhen Ding", "Zean Han", "Jiheng Zhang"], "title": "Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control", "comment": null, "summary": "Large Language Models (LLMs) are rapidly becoming critical infrastructure for enterprise applications, driving unprecedented demand for GPU-based inference services. A key operational challenge arises from the two-phase nature of LLM inference: a compute-intensive \\emph{prefill} phase that processes user input, followed by a memory-bound \\emph{decode} phase that generates output tokens. When these phases share GPU resources, prefill tasks throttle the processing speed of concurrent decodes, creating state-dependent contention. This contention is further complicated by workload heterogeneity, as different applications exhibit vastly different input and output lengths. We develop a stochastic control framework for scheduling heterogeneous LLM workloads across large GPU clusters. We formulate LLM inference as a multiclass many-server queueing network with state-dependent service rates, grounded in empirical iteration-time measurements. We analyze the fluid approximation of this system and solve steady-state linear programs that characterize optimal resource allocation. We design gate-and-route policies that regulate prefill admission and decode routing, and prove that they are asymptotically optimal in the many-GPU limit under both bundled and separate token-pricing schemes. We further extend the framework to incorporate Service Level Indicators (SLIs) such as latency and fairness, providing a general approach to constrained scheduling. Numerical experiments calibrated to empirical iteration-time data demonstrate that our policies outperform standard serving heuristics.", "AI": {"tldr": "\u9488\u5bf9LLM\u63a8\u7406\u4e2dprefill\u548cdecode\u9636\u6bb5\u8d44\u6e90\u4e89\u7528\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u6392\u961f\u7f51\u7edc\u7684\u968f\u673a\u63a7\u5236\u6846\u67b6\uff0c\u8bbe\u8ba1\u95e8\u63a7\u8def\u7531\u7b56\u7565\u5b9e\u73b0\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6700\u4f18\u8c03\u5ea6", "motivation": "LLM\u63a8\u7406\u7684\u4e24\u9636\u6bb5\u7279\u6027\uff08\u8ba1\u7b97\u5bc6\u96c6\u7684prefill\u9636\u6bb5\u548c\u5185\u5b58\u53d7\u9650\u7684decode\u9636\u6bb5\uff09\u5728\u5171\u4eabGPU\u8d44\u6e90\u65f6\u4ea7\u751f\u72b6\u6001\u4f9d\u8d56\u7684\u8d44\u6e90\u4e89\u7528\uff0c\u52a0\u4e0a\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5f02\u6784\u6027\uff08\u4e0d\u540c\u5e94\u7528\u8f93\u5165\u8f93\u51fa\u957f\u5ea6\u5dee\u5f02\u5927\uff09\uff0c\u4f7f\u5f97\u8c03\u5ea6\u6210\u4e3a\u5173\u952e\u6311\u6218", "method": "\u5c06LLM\u63a8\u7406\u5efa\u6a21\u4e3a\u5177\u6709\u72b6\u6001\u4f9d\u8d56\u670d\u52a1\u901f\u7387\u7684\u591a\u7c7b\u591a\u670d\u52a1\u5668\u6392\u961f\u7f51\u7edc\uff0c\u57fa\u4e8e\u7ecf\u9a8c\u8fed\u4ee3\u65f6\u95f4\u6d4b\u91cf\uff0c\u5206\u6790\u6d41\u4f53\u8fd1\u4f3c\u5e76\u6c42\u89e3\u7a33\u6001\u7ebf\u6027\u89c4\u5212\uff0c\u8bbe\u8ba1\u8c03\u8282prefill\u51c6\u5165\u548cdecode\u8def\u7531\u7684\u95e8\u63a7\u8def\u7531\u7b56\u7565", "result": "\u5728\u5927\u91cfGPU\u9650\u5236\u4e0b\uff0c\u8bc1\u660e\u6240\u63d0\u7b56\u7565\u5728\u6346\u7ed1\u548c\u5206\u79bbtoken\u5b9a\u4ef7\u65b9\u6848\u4e0b\u90fd\u662f\u6e10\u8fd1\u6700\u4f18\u7684\uff0c\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u6807\u51c6\u670d\u52a1\u542f\u53d1\u5f0f\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u968f\u673a\u63a7\u5236\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3LLM\u63a8\u7406\u4e2d\u7684\u8d44\u6e90\u4e89\u7528\u95ee\u9898\uff0c\u652f\u6301\u670d\u52a1\u7ea7\u522b\u6307\u6807\u7ea6\u675f\uff0c\u4e3a\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u63d0\u4f9b\u901a\u7528\u65b9\u6cd5"}}
{"id": "2602.03081", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.03081", "abs": "https://arxiv.org/abs/2602.03081", "authors": ["Mohammadali Khodabandehlou", "Jared Coleman", "Niranjan Suri", "Bhaskar Krishnamachari"], "title": "Studying the Effect of Schedule Preemption on Dynamic Task Graph Scheduling", "comment": null, "summary": "Dynamic scheduling of task graphs is often addressed without revisiting prior task allocations, with a primary focus on minimizing makespan. We study controlled schedule preemption, introducing the Last-K Preemption model, which selectively reschedules recent task graphs while preserving earlier allocations. Using synthetic, RIoTBench, WFCommons, and adversarial workloads, we compare preemptive, non-preemptive, and partial-preemptive strategies across makespan, fairness, utilization, and runtime. Results show moderate preemption can match most makespan and utilization gains of full preemption while maintaining fairness and low overhead.", "AI": {"tldr": "\u7814\u7a76\u52a8\u6001\u4efb\u52a1\u56fe\u8c03\u5ea6\u4e2d\u7684\u53d7\u63a7\u62a2\u5360\u6a21\u578b\uff0c\u63d0\u51faLast-K\u62a2\u5360\u65b9\u6cd5\uff0c\u9009\u62e9\u6027\u91cd\u8c03\u5ea6\u8fd1\u671f\u4efb\u52a1\u56fe\uff0c\u5728\u4fdd\u6301\u65e9\u671f\u5206\u914d\u7684\u540c\u65f6\u4f18\u5316\u6027\u80fd\u6307\u6807", "motivation": "\u4f20\u7edf\u52a8\u6001\u4efb\u52a1\u56fe\u8c03\u5ea6\u901a\u5e38\u4e0d\u91cd\u65b0\u5ba1\u89c6\u5148\u524d\u7684\u4efb\u52a1\u5206\u914d\uff0c\u4e3b\u8981\u5173\u6ce8\u6700\u5c0f\u5316\u5b8c\u6210\u65f6\u95f4\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u53d7\u63a7\u7684\u8c03\u5ea6\u62a2\u5360\uff0c\u5728\u4fdd\u6301\u516c\u5e73\u6027\u548c\u4f4e\u5f00\u9500\u7684\u540c\u65f6\u83b7\u5f97\u6027\u80fd\u63d0\u5347", "method": "\u63d0\u51faLast-K\u62a2\u5360\u6a21\u578b\uff0c\u9009\u62e9\u6027\u91cd\u8c03\u5ea6\u6700\u8fd1\u7684\u4efb\u52a1\u56fe\uff0c\u540c\u65f6\u4fdd\u7559\u65e9\u671f\u5206\u914d\u3002\u6bd4\u8f83\u62a2\u5360\u5f0f\u3001\u975e\u62a2\u5360\u5f0f\u548c\u90e8\u5206\u62a2\u5360\u5f0f\u7b56\u7565\uff0c\u4f7f\u7528\u5408\u6210\u3001RIoTBench\u3001WFCommons\u548c\u5bf9\u6297\u6027\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u8bc4\u4f30", "result": "\u7ed3\u679c\u663e\u793a\u9002\u5ea6\u7684\u62a2\u5360\u53ef\u4ee5\u5728\u5339\u914d\u5b8c\u5168\u62a2\u5360\u7684\u5927\u90e8\u5206\u5b8c\u6210\u65f6\u95f4\u548c\u5229\u7528\u7387\u6536\u76ca\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u516c\u5e73\u6027\u548c\u4f4e\u8fd0\u884c\u65f6\u5f00\u9500", "conclusion": "\u53d7\u63a7\u7684\u8c03\u5ea6\u62a2\u5360\u7b56\u7565\uff08\u5982Last-K\u6a21\u578b\uff09\u80fd\u591f\u5728\u6027\u80fd\u6307\u6807\u548c\u7cfb\u7edf\u5f00\u9500\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u52a8\u6001\u4efb\u52a1\u56fe\u8c03\u5ea6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.03246", "categories": ["cs.DC", "cs.NI", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.03246", "abs": "https://arxiv.org/abs/2602.03246", "authors": ["Tamoghna Sarkar", "Bhaskar Krishnamachari"], "title": "Joint Network-and-Server Congestion in Multi-Source Traffic Allocation: A Convex Formulation and Price-Based Decentralization", "comment": "10pages, 7 figures, submitted a version conference", "summary": "This paper studies an important rate allocation problem that arises in many networked and distributed systems: steady-state traffic rate allocation from multiple sources to multiple service nodes when both (i) the access-path delay on each source-node route is rate-dependent (capacity-constrained) and convex, and (ii) each service node (also capacity-constrained) experiences a load-dependent queueing delay driven by aggregate load from all sources. We show that the resulting flow-weighted end-to-end delay minimization is a convex program, yielding a global system-optimal solution characterized by KKT conditions that equalize total marginal costs (a path marginal access term plus a node congestion price) across all utilized routes. This condition admits a Wardrop-type interpretation: for each source, all utilized options equalize total marginal cost, while any option with strictly larger total marginal cost receives no flow. Building on this structure, we develop a lightweight distributed pricing-based algorithm in which each service node locally computes and broadcasts a scalar congestion price from its observed aggregate load, while each source updates its traffic split by solving a small separable convex allocation problem under the advertised prices. Numerical illustrations demonstrate convergence of the distributed iteration to the centralized optimum and highlight the trade-offs induced by jointly modeling access and service congestion.", "AI": {"tldr": "\u7814\u7a76\u591a\u6e90\u591a\u670d\u52a1\u8282\u70b9\u7684\u7a33\u6001\u6d41\u91cf\u5206\u914d\u95ee\u9898\uff0c\u8003\u8651\u8def\u5f84\u8bbf\u95ee\u5ef6\u8fdf\u548c\u670d\u52a1\u8282\u70b9\u6392\u961f\u5ef6\u8fdf\uff0c\u63d0\u51fa\u5206\u5e03\u5f0f\u5b9a\u4ef7\u7b97\u6cd5\u5b9e\u73b0\u7cfb\u7edf\u6700\u4f18", "motivation": "\u7f51\u7edc\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5b58\u5728\u91cd\u8981\u7684\u901f\u7387\u5206\u914d\u95ee\u9898\uff1a\u5f53\u591a\u4e2a\u6e90\u8282\u70b9\u5411\u591a\u4e2a\u670d\u52a1\u8282\u70b9\u53d1\u9001\u6d41\u91cf\u65f6\uff0c\u9700\u8981\u8003\u8651\u8def\u5f84\u8bbf\u95ee\u5ef6\u8fdf\uff08\u4e0e\u901f\u7387\u76f8\u5173\u4e14\u51f8\uff09\u548c\u670d\u52a1\u8282\u70b9\u6392\u961f\u5ef6\u8fdf\uff08\u4e0e\u603b\u8d1f\u8f7d\u76f8\u5173\uff09\u3002\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5355\u72ec\u5904\u7406\u8fd9\u4e24\u79cd\u5ef6\u8fdf\uff0c\u9700\u8981\u8054\u5408\u5efa\u6a21\u4ee5\u83b7\u5f97\u7cfb\u7edf\u6700\u4f18\u5206\u914d\u3002", "method": "\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u6700\u5c0f\u5316\u5efa\u6a21\u4e3a\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u57fa\u4e8eKKT\u6761\u4ef6\u63a8\u5bfc\u51fa\u6700\u4f18\u6761\u4ef6\uff1a\u6240\u6709\u4f7f\u7528\u8def\u5f84\u7684\u603b\u8fb9\u9645\u6210\u672c\uff08\u8def\u5f84\u8fb9\u9645\u8bbf\u95ee\u9879+\u8282\u70b9\u62e5\u585e\u4ef7\u683c\uff09\u76f8\u7b49\u3002\u63d0\u51fa\u8f7b\u91cf\u7ea7\u5206\u5e03\u5f0f\u5b9a\u4ef7\u7b97\u6cd5\uff1a\u670d\u52a1\u8282\u70b9\u6839\u636e\u89c2\u6d4b\u8d1f\u8f7d\u8ba1\u7b97\u5e76\u5e7f\u64ad\u62e5\u585e\u4ef7\u683c\uff0c\u6e90\u8282\u70b9\u6839\u636e\u4ef7\u683c\u66f4\u65b0\u6d41\u91cf\u5206\u914d\u3002", "result": "\u8bc1\u660e\u7aef\u5230\u7aef\u5ef6\u8fdf\u6700\u5c0f\u5316\u662f\u51f8\u89c4\u5212\u95ee\u9898\uff0c\u5b58\u5728\u5168\u5c40\u6700\u4f18\u89e3\u3002\u5206\u5e03\u5f0f\u7b97\u6cd5\u80fd\u591f\u6536\u655b\u5230\u96c6\u4e2d\u5f0f\u6700\u4f18\u89e3\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6536\u655b\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u8054\u5408\u5efa\u6a21\u8bbf\u95ee\u5ef6\u8fdf\u548c\u670d\u52a1\u62e5\u585e\u7684\u6d41\u91cf\u5206\u914d\u95ee\u9898\u5177\u6709\u51f8\u7ed3\u6784\uff0c\u53ef\u901a\u8fc7\u5206\u5e03\u5f0f\u5b9a\u4ef7\u7b97\u6cd5\u6709\u6548\u6c42\u89e3\u3002\u6700\u4f18\u6761\u4ef6\u5177\u6709Wardrop\u578b\u89e3\u91ca\uff0c\u4e3a\u5b9e\u9645\u7f51\u7edc\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u548c\u5b9e\u7528\u7b97\u6cd5\u3002"}}
{"id": "2602.03033", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.03033", "abs": "https://arxiv.org/abs/2602.03033", "authors": ["Haoxuan Yin", "Andrzej S. Murawski", "C. -H. Luke Ong"], "title": "Layered Modal ML: Syntax and Full Abstraction", "comment": "22 pages, 6 figures", "summary": "MetaML-style metaprogramming languages allow programmers to construct, manipulate and run code. In the presence of higher-order references for code, ensuring type safety is challenging, as free variables can escape their binders. In this paper, we present Layered Modal ML (LMML), \\textit{the first metaprogramming language that supports storing and running open code under a strong type safety guarantee}. The type system utilises contextual modal types to track and reason about free variables in code explicitly.\n  A crucial concern in metaprogramming-based program optimisations is whether the optimised program preserves the meaning of the original program. Addressing this question requires a notion of program equivalence and techniques to reason about it. In this paper, we provide a semantic model that captures contextual equivalence for LMML, establishing \\textit{the first full abstraction result for an imperative MetaML-style language}. Our model is based on traces derived via operational game semantics, where the meaning of a program is modelled by its possible interactions with the environment. We also establish a novel closed instances of use theorem that accounts for both call-by-value and call-by-name closing substitutions.", "AI": {"tldr": "LMML\u662f\u9996\u4e2a\u652f\u6301\u5728\u5f3a\u7c7b\u578b\u5b89\u5168\u4fdd\u8bc1\u4e0b\u5b58\u50a8\u548c\u8fd0\u884c\u5f00\u653e\u4ee3\u7801\u7684\u5143\u7f16\u7a0b\u8bed\u8a00\uff0c\u5e76\u5efa\u7acb\u4e86\u9996\u4e2a\u547d\u4ee4\u5f0fMetaML\u98ce\u683c\u8bed\u8a00\u7684\u5b8c\u5168\u62bd\u8c61\u8bed\u4e49\u6a21\u578b\u3002", "motivation": "\u5143\u7f16\u7a0b\u8bed\u8a00\u4e2d\uff0c\u5728\u5b58\u5728\u9ad8\u9636\u4ee3\u7801\u5f15\u7528\u65f6\u786e\u4fdd\u7c7b\u578b\u5b89\u5168\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u81ea\u7531\u53d8\u91cf\u53ef\u80fd\u9003\u9038\u5176\u7ed1\u5b9a\u5668\u3002\u540c\u65f6\uff0c\u57fa\u4e8e\u5143\u7f16\u7a0b\u7684\u7a0b\u5e8f\u4f18\u5316\u9700\u8981\u9a8c\u8bc1\u4f18\u5316\u7a0b\u5e8f\u662f\u5426\u4fdd\u6301\u539f\u59cb\u7a0b\u5e8f\u7684\u610f\u4e49\uff0c\u8fd9\u9700\u8981\u7a0b\u5e8f\u7b49\u4ef7\u6027\u7684\u6982\u5ff5\u548c\u63a8\u7406\u6280\u672f\u3002", "method": "\u63d0\u51faLayered Modal ML (LMML)\uff0c\u4f7f\u7528\u4e0a\u4e0b\u6587\u6a21\u6001\u7c7b\u578b\u663e\u5f0f\u8ddf\u8e2a\u548c\u63a8\u7406\u4ee3\u7801\u4e2d\u7684\u81ea\u7531\u53d8\u91cf\u3002\u57fa\u4e8e\u64cd\u4f5c\u535a\u5f08\u8bed\u4e49\u7684\u8ff9\u5efa\u7acb\u8bed\u4e49\u6a21\u578b\uff0c\u6355\u83b7\u4e0a\u4e0b\u6587\u7b49\u4ef7\u6027\uff0c\u5e76\u5efa\u7acb\u65b0\u9896\u7684\u5c01\u95ed\u5b9e\u4f8b\u4f7f\u7528\u5b9a\u7406\u3002", "result": "LMML\u662f\u9996\u4e2a\u652f\u6301\u5728\u5f3a\u7c7b\u578b\u5b89\u5168\u4fdd\u8bc1\u4e0b\u5b58\u50a8\u548c\u8fd0\u884c\u5f00\u653e\u4ee3\u7801\u7684\u5143\u7f16\u7a0b\u8bed\u8a00\u3002\u5efa\u7acb\u4e86\u9996\u4e2a\u547d\u4ee4\u5f0fMetaML\u98ce\u683c\u8bed\u8a00\u7684\u5b8c\u5168\u62bd\u8c61\u7ed3\u679c\uff0c\u8bed\u4e49\u6a21\u578b\u80fd\u591f\u6355\u83b7\u4e0a\u4e0b\u6587\u7b49\u4ef7\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u7c7b\u578b\u5b89\u5168\u7684\u5143\u7f16\u7a0b\u8bed\u8a00LMML\u53ca\u5176\u5b8c\u5168\u62bd\u8c61\u8bed\u4e49\u6a21\u578b\uff0c\u4e3a\u5143\u7f16\u7a0b\u4e2d\u7684\u7a0b\u5e8f\u4f18\u5316\u548c\u7b49\u4ef7\u6027\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.03444", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.03444", "abs": "https://arxiv.org/abs/2602.03444", "authors": ["Arivarasan Karmegam", "Lucianna Kiffer", "Antonio Fern\u00e1ndez Anta"], "title": "Exploiting Multi-Core Parallelism in Blockchain Validation and Construction", "comment": null, "summary": "Blockchain validators can reduce block processing time by exploiting multi-core CPUs, but deterministic execution must preserve a given total order while respecting transaction conflicts and per-block runtime limits. This paper systematically examines how validators can exploit multi-core parallelism during both block construction and execution without violating blockchain semantics. We formalize two validator-side optimization problems: (i) executing an already ordered block on \\(p\\) cores to minimize makespan while ensuring equivalence to sequential execution; and (ii) selecting and scheduling a subset of mempool transactions under a runtime limit \\(B\\) to maximize validator reward. For both, we develop exact Mixed-Integer Linear Programming (MILP) formulations that capture conflict, order, and capacity constraints, and propose fast deterministic heuristics that scale to realistic workloads. Using Ethereum mainnet traces and including a Solana-inspired declared-access baseline (Sol) for ordered-block scheduling and a simple reward-greedy baseline (RG) for block construction, we empirically quantify the trade-offs between optimality and runtime.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u533a\u5757\u94fe\u9a8c\u8bc1\u5668\u5982\u4f55\u5229\u7528\u591a\u6838CPU\u5e76\u884c\u5904\u7406\u533a\u5757\uff0c\u540c\u65f6\u4fdd\u6301\u786e\u5b9a\u6027\u6267\u884c\u548c\u533a\u5757\u94fe\u8bed\u4e49\u3002\u63d0\u51fa\u4e86\u4e24\u4e2a\u4f18\u5316\u95ee\u9898\uff1a\u6709\u5e8f\u533a\u5757\u5e76\u884c\u6267\u884c\u548c\u533a\u5757\u6784\u5efa\u4ea4\u6613\u9009\u62e9\uff0c\u5e76\u5f00\u53d1\u4e86MILP\u6a21\u578b\u548c\u9ad8\u6548\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "motivation": "\u533a\u5757\u94fe\u9a8c\u8bc1\u5668\u53ef\u4ee5\u5229\u7528\u591a\u6838CPU\u51cf\u5c11\u533a\u5757\u5904\u7406\u65f6\u95f4\uff0c\u4f46\u5fc5\u987b\u4fdd\u6301\u786e\u5b9a\u6027\u6267\u884c\uff0c\u540c\u65f6\u5c0a\u91cd\u4ea4\u6613\u51b2\u7a81\u548c\u6bcf\u4e2a\u533a\u5757\u7684\u8fd0\u884c\u65f6\u95f4\u9650\u5236\u3002\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u5982\u4f55\u5728\u533a\u5757\u6784\u5efa\u548c\u6267\u884c\u4e2d\u5229\u7528\u591a\u6838\u5e76\u884c\u6027\u800c\u4e0d\u8fdd\u53cd\u533a\u5757\u94fe\u8bed\u4e49\u3002", "method": "\u5f62\u5f0f\u5316\u4e86\u4e24\u4e2a\u9a8c\u8bc1\u5668\u7aef\u4f18\u5316\u95ee\u9898\uff1a1) \u5728p\u4e2a\u6838\u5fc3\u4e0a\u6267\u884c\u5df2\u6392\u5e8f\u533a\u5757\u4ee5\u6700\u5c0f\u5316\u5b8c\u6210\u65f6\u95f4\uff0c\u540c\u65f6\u786e\u4fdd\u4e0e\u987a\u5e8f\u6267\u884c\u7b49\u4ef7\uff1b2) \u5728\u8fd0\u884c\u65f6\u9650\u5236B\u4e0b\u9009\u62e9\u548c\u8c03\u5ea6\u5185\u5b58\u6c60\u4ea4\u6613\u5b50\u96c6\u4ee5\u6700\u5927\u5316\u9a8c\u8bc1\u5668\u5956\u52b1\u3002\u4e3a\u4e24\u8005\u5f00\u53d1\u4e86\u7cbe\u786e\u7684\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212(MILP)\u516c\u5f0f\uff0c\u5e76\u63d0\u51fa\u53ef\u6269\u5c55\u5230\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5feb\u901f\u786e\u5b9a\u6027\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "result": "\u4f7f\u7528\u4ee5\u592a\u574a\u4e3b\u7f51\u8ffd\u8e2a\u6570\u636e\uff0c\u5305\u62ecSolana\u542f\u53d1\u7684\u58f0\u660e\u8bbf\u95ee\u57fa\u7ebf(Sol)\u7528\u4e8e\u6709\u5e8f\u533a\u5757\u8c03\u5ea6\u548c\u7b80\u5355\u5956\u52b1\u8d2a\u5a6a\u57fa\u7ebf(RG)\u7528\u4e8e\u533a\u5757\u6784\u5efa\uff0c\u5b9e\u8bc1\u91cf\u5316\u4e86\u6700\u4f18\u6027\u548c\u8fd0\u884c\u65f6\u95f4\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u533a\u5757\u94fe\u9a8c\u8bc1\u5668\u5728\u591a\u6838\u73af\u5883\u4e0b\u7684\u5e76\u884c\u5904\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7MILP\u6a21\u578b\u548c\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u4fdd\u6301\u533a\u5757\u94fe\u8bed\u4e49\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6027\u80fd\u4f18\u5316\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u65b9\u6cd5\u57fa\u7840\u3002"}}
{"id": "2602.03777", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.03777", "abs": "https://arxiv.org/abs/2602.03777", "authors": ["Federico Bruzzone", "Walter Cazzola", "Luca Favalli"], "title": "From Separate Compilation to Sound Language Composition", "comment": "43 pages, 1 figure, 5 Listing", "summary": "The development of programming languages involves complex theoretical and practical challenges, particularly when addressing modularity and reusability through language extensions. While language workbenches aim to enable modular development under the constraints of the language extension problem, one critical constraint -- separate compilation -- is often relaxed due to its complexity. However, this relaxation undermines artifact reusability and integration with common dependency systems. A key difficulty under separate compilation arises from managing attribute grammars, as extensions may introduce new attributes that invalidate previously generated abstract syntax tree structures. Existing approaches, such as the use of dynamic maps in the Neverlang workbench, favor flexibility at the cost of compile-time correctness, leading to potential runtime errors due to undefined attributes. This work addresses this issue by introducing nlgcheck, a theoretically sound static analysis tool based on data-flow analysis for the Neverlang language workbench. nlgcheck detects potential runtime errors -- such as undefined attribute accesses -- at compile time, preserving separate compilation while maintaining strong static correctness guarantees. Experimental evaluation using mutation testing on Neverlang-based projects demonstrates that nlgcheck effectively enhances robustness without sacrificing modularity or flexibility and with a level of performance that does not impede its adoption in daily development activities.", "AI": {"tldr": "nlgcheck\u662f\u4e00\u4e2a\u57fa\u4e8e\u6570\u636e\u6d41\u5206\u6790\u7684\u9759\u6001\u5206\u6790\u5de5\u5177\uff0c\u7528\u4e8eNeverlang\u8bed\u8a00\u5de5\u4f5c\u53f0\uff0c\u80fd\u5728\u7f16\u8bd1\u65f6\u68c0\u6d4b\u6f5c\u5728\u8fd0\u884c\u65f6\u9519\u8bef\uff08\u5982\u672a\u5b9a\u4e49\u5c5e\u6027\u8bbf\u95ee\uff09\uff0c\u5728\u4fdd\u6301\u5355\u72ec\u7f16\u8bd1\u7684\u540c\u65f6\u63d0\u4f9b\u5f3a\u9759\u6001\u6b63\u786e\u6027\u4fdd\u8bc1\u3002", "motivation": "\u7f16\u7a0b\u8bed\u8a00\u5f00\u53d1\u4e2d\uff0c\u8bed\u8a00\u5de5\u4f5c\u53f0\u4e3a\u4e86\u652f\u6301\u6a21\u5757\u5316\u5f00\u53d1\uff0c\u5e38\u5e38\u653e\u677e\u5355\u72ec\u7f16\u8bd1\u8fd9\u4e00\u5173\u952e\u7ea6\u675f\uff0c\u4f46\u8fd9\u4f1a\u635f\u5bb3\u5de5\u4ef6\u53ef\u91cd\u7528\u6027\u548c\u4e0e\u4f9d\u8d56\u7cfb\u7edf\u7684\u96c6\u6210\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982Neverlang\u4e2d\u7684\u52a8\u6001\u6620\u5c04\uff09\u4ee5\u7075\u6d3b\u6027\u4e3a\u4ee3\u4ef7\u727a\u7272\u7f16\u8bd1\u65f6\u6b63\u786e\u6027\uff0c\u5bfc\u81f4\u6f5c\u5728\u8fd0\u884c\u65f6\u9519\u8bef\u3002", "method": "\u63d0\u51fanlgcheck\u5de5\u5177\uff0c\u57fa\u4e8e\u6570\u636e\u6d41\u5206\u6790\u8fdb\u884c\u9759\u6001\u5206\u6790\uff0c\u4e13\u95e8\u9488\u5bf9Neverlang\u8bed\u8a00\u5de5\u4f5c\u53f0\u3002\u8be5\u5de5\u5177\u5728\u7f16\u8bd1\u65f6\u68c0\u6d4b\u672a\u5b9a\u4e49\u5c5e\u6027\u8bbf\u95ee\u7b49\u6f5c\u5728\u8fd0\u884c\u65f6\u9519\u8bef\uff0c\u4fdd\u6301\u5355\u72ec\u7f16\u8bd1\u7684\u540c\u65f6\u63d0\u4f9b\u9759\u6001\u6b63\u786e\u6027\u4fdd\u8bc1\u3002", "result": "\u901a\u8fc7\u57fa\u4e8eNeverlang\u9879\u76ee\u7684\u7a81\u53d8\u6d4b\u8bd5\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cnlgcheck\u80fd\u6709\u6548\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u4e0d\u727a\u7272\u6a21\u5757\u5316\u6216\u7075\u6d3b\u6027\uff0c\u4e14\u6027\u80fd\u6c34\u5e73\u4e0d\u4f1a\u963b\u788d\u65e5\u5e38\u5f00\u53d1\u6d3b\u52a8\u4e2d\u7684\u91c7\u7528\u3002", "conclusion": "nlgcheck\u89e3\u51b3\u4e86\u8bed\u8a00\u6269\u5c55\u4e2d\u5355\u72ec\u7f16\u8bd1\u4e0e\u9759\u6001\u6b63\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u8bed\u8a00\u5de5\u4f5c\u53f0\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0a\u53ef\u9760\u7684\u9759\u6001\u5206\u6790\u5de5\u5177\uff0c\u5728\u4fdd\u6301\u6a21\u5757\u5316\u5f00\u53d1\u4f18\u52bf\u7684\u540c\u65f6\u9632\u6b62\u8fd0\u884c\u65f6\u9519\u8bef\u3002"}}
{"id": "2602.03474", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.03474", "abs": "https://arxiv.org/abs/2602.03474", "authors": ["Shachar Meir", "David Peleg"], "title": "Recursive Energy Efficient Agreement", "comment": null, "summary": "Agreement is a foundational problem in distributed computing that have been studied extensively for over four decades. Recently, Meir, Mirault, Peleg and Robinson introduced the notion of \\emph{Energy Efficient Agreement}, where the goal is to solve Agreement while minimizing the number of round a party participates in, thereby reducing the energy cost per participant. We show a recursive Agreement algorithm that has $O(\\log f)$ active rounds per participant, where $f<n$ represents the maximum number of crash faults in the system.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9012\u5f52\u5f0f\u5171\u8bc6\u7b97\u6cd5\uff0c\u4f7f\u6bcf\u4e2a\u53c2\u4e0e\u8005\u4ec5\u9700\u53c2\u4e0eO(log f)\u8f6e\u6d3b\u8dc3\u901a\u4fe1\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017", "motivation": "\u4f20\u7edf\u5206\u5e03\u5f0f\u5171\u8bc6\u7b97\u6cd5\u4e2d\u6240\u6709\u8282\u70b9\u9700\u8981\u53c2\u4e0e\u6240\u6709\u8f6e\u6b21\u901a\u4fe1\uff0c\u80fd\u8017\u9ad8\u3002\u8fd1\u671f\u7814\u7a76\u63d0\u51fa\u80fd\u91cf\u9ad8\u6548\u5171\u8bc6\u6982\u5ff5\uff0c\u65e8\u5728\u51cf\u5c11\u6bcf\u4e2a\u53c2\u4e0e\u8005\u9700\u8981\u53c2\u4e0e\u7684\u8f6e\u6b21\u6570\uff0c\u4ece\u800c\u964d\u4f4e\u80fd\u8017\u6210\u672c", "method": "\u8bbe\u8ba1\u9012\u5f52\u5f0f\u5171\u8bc6\u7b97\u6cd5\uff0c\u901a\u8fc7\u9012\u5f52\u7ed3\u6784\u5c06\u53c2\u4e0e\u8005\u7684\u6d3b\u8dc3\u901a\u4fe1\u8f6e\u6b21\u9650\u5236\u5728O(log f)\u7ea7\u522b\uff0c\u5176\u4e2df<n\u8868\u793a\u7cfb\u7edf\u4e2d\u6700\u5927\u5d29\u6e83\u6545\u969c\u6570", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86\u6bcf\u4e2a\u53c2\u4e0e\u8005\u4ec5\u9700O(log f)\u8f6e\u6d3b\u8dc3\u53c2\u4e0e\uff0c\u76f8\u6bd4\u4f20\u7edf\u7b97\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u80fd\u8017", "conclusion": "\u8be5\u9012\u5f52\u7b97\u6cd5\u4e3a\u80fd\u91cf\u9ad8\u6548\u5171\u8bc6\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u8bc1\u5171\u8bc6\u6b63\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u53c2\u4e0e\u8005\u7684\u80fd\u91cf\u6d88\u8017"}}
{"id": "2602.03495", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.03495", "abs": "https://arxiv.org/abs/2602.03495", "authors": ["Zeyu Zhu", "Gang Li", "Peisong Wang", "Zitao Mo", "Minnan Pei", "Zhuoran Song", "Xiaoyao Liang", "Jian Cheng"], "title": "DALI: A Workload-Aware Offloading Framework for Efficient MoE Inference on Local PCs", "comment": null, "summary": "Mixture of Experts (MoE) architectures significantly enhance the capacity of LLMs without proportional increases in computation, but at the cost of a vast parameter size. Offloading MoE expert parameters to host memory and leveraging both CPU and GPU computation has recently emerged as a promising direction to support such models on resourceconstrained local PC platforms. While promising, we notice that existing approaches mismatch the dynamic nature of expert workloads, which leads to three fundamental inefficiencies: (1) Static expert assignment causes severe CPUGPU load imbalance, underutilizing CPU and GPU resources; (2) Existing prefetching techniques fail to accurately predict high-workload experts, leading to costly inaccurate prefetches; (3) GPU cache policies neglect workload dynamics, resulting in poor hit rates and limited effectiveness. To address these challenges, we propose DALI, a workloaDAware offLoadIng framework for efficient MoE inference on local PCs. To fully utilize hardware resources, DALI first dynamically assigns experts to CPU or GPU by modeling assignment as a 0-1 integer optimization problem and solving it efficiently using a Greedy Assignment strategy at runtime. To improve prefetching accuracy, we develop a Residual-Based Prefetching method leveraging inter-layer residual information to accurately predict high-workload experts. Additionally, we introduce a Workload-Aware Cache Replacement policy that exploits temporal correlation in expert activations to improve GPU cache efficiency. By evaluating across various MoE models and settings, DALI achieves significant speedups in the both prefill and decoding phases over the state-of-the-art offloading frameworks.", "AI": {"tldr": "DALI\uff1a\u4e00\u79cd\u9762\u5411\u672c\u5730PC\u4e0aMoE\u63a8\u7406\u7684\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u5378\u8f7d\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u5206\u914d\u3001\u6b8b\u5dee\u9884\u53d6\u548c\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7f13\u5b58\u66ff\u6362\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "MoE\u67b6\u6784\u867d\u7136\u80fd\u5927\u5e45\u63d0\u5347LLM\u5bb9\u91cf\u800c\u4e0d\u6210\u6bd4\u4f8b\u589e\u52a0\u8ba1\u7b97\u91cf\uff0c\u4f46\u53c2\u6570\u89c4\u6a21\u5e9e\u5927\u3002\u73b0\u6709\u5378\u8f7d\u65b9\u6cd5\u65e0\u6cd5\u5339\u914d\u4e13\u5bb6\u5de5\u4f5c\u8d1f\u8f7d\u7684\u52a8\u6001\u7279\u6027\uff0c\u5bfc\u81f4CPU-GPU\u8d1f\u8f7d\u4e0d\u5e73\u8861\u3001\u9884\u53d6\u4e0d\u51c6\u786e\u548cGPU\u7f13\u5b58\u6548\u7387\u4f4e\u4e0b\u7b49\u95ee\u9898\u3002", "method": "1) \u5c06\u4e13\u5bb6\u5206\u914d\u5efa\u6a21\u4e3a0-1\u6574\u6570\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u8d2a\u5fc3\u5206\u914d\u7b56\u7565\u52a8\u6001\u5206\u914d\u4e13\u5bb6\u5230CPU\u6216GPU\uff1b2) \u57fa\u4e8e\u6b8b\u5dee\u7684\u9884\u53d6\u65b9\u6cd5\uff0c\u5229\u7528\u5c42\u95f4\u6b8b\u5dee\u4fe1\u606f\u51c6\u786e\u9884\u6d4b\u9ad8\u5de5\u4f5c\u8d1f\u8f7d\u4e13\u5bb6\uff1b3) \u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7684\u7f13\u5b58\u66ff\u6362\u7b56\u7565\uff0c\u5229\u7528\u4e13\u5bb6\u6fc0\u6d3b\u7684\u65f6\u95f4\u76f8\u5173\u6027\u63d0\u5347GPU\u7f13\u5b58\u6548\u7387\u3002", "result": "\u5728\u5404\u79cdMoE\u6a21\u578b\u548c\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\uff0cDALI\u5728\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5378\u8f7d\u6846\u67b6\u90fd\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002", "conclusion": "DALI\u901a\u8fc7\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7684\u5378\u8f7d\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86MoE\u63a8\u7406\u4e2d\u7684\u5173\u952e\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u672c\u5730PC\u5e73\u53f0\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684MoE\u6a21\u578b\u652f\u6301\u65b9\u6848\u3002"}}
{"id": "2602.03802", "categories": ["cs.DC", "cs.AI", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.03802", "abs": "https://arxiv.org/abs/2602.03802", "authors": ["Grigory Begunov", "Alexander Tyurin"], "title": "Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods", "comment": null, "summary": "Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization. We revisit Synchronous SGD and its robust variant, called $m$-Synchronous SGD, and theoretically show that they are nearly optimal in many heterogeneous computation scenarios, which is somewhat unexpected. We analyze the synchronous methods under random computation times and adversarial partial participation of workers, and prove that their time complexities are optimal in many practical regimes, up to logarithmic factors. While synchronous methods are not universal solutions and there exist tasks where asynchronous methods may be necessary, we show that they are sufficient for many modern heterogeneous computation scenarios.", "AI": {"tldr": "\u540c\u6b65SGD\u53ca\u5176\u53d8\u4f53m-Synchronous SGD\u5728\u5f02\u6784\u8ba1\u7b97\u573a\u666f\u4e2d\u63a5\u8fd1\u6700\u4f18\uff0c\u5c3d\u7ba1\u5f02\u6b65\u4f18\u5316\u6709\u8fdb\u5c55\u4f46\u540c\u6b65\u65b9\u6cd5\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\u8db3\u591f\u6709\u6548", "motivation": "\u5c3d\u7ba1\u5f02\u6b65\u4f18\u5316\u65b9\u6cd5\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u4ee3\u5206\u5e03\u5f0f\u4f18\u5316\u4ecd\u4e3b\u8981\u4f9d\u8d56\u4f20\u7edf\u540c\u6b65\u65b9\u6cd5\u3002\u4f5c\u8005\u91cd\u65b0\u5ba1\u89c6\u540c\u6b65\u65b9\u6cd5\uff0c\u7814\u7a76\u5176\u5728\u5f02\u6784\u8ba1\u7b97\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u6311\u6218\u4e86\u5f02\u6b65\u65b9\u6cd5\u5fc5\u7136\u66f4\u4f18\u7684\u666e\u904d\u8ba4\u77e5\u3002", "method": "\u5206\u6790Synchronous SGD\u53ca\u5176\u9c81\u68d2\u53d8\u4f53m-Synchronous SGD\uff0c\u5728\u968f\u673a\u8ba1\u7b97\u65f6\u95f4\u548c\u5bf9\u6297\u6027\u90e8\u5206\u53c2\u4e0e\u7684\u5de5\u4f5c\u8282\u70b9\u6761\u4ef6\u4e0b\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u8bc4\u4f30\u5176\u65f6\u95f4\u590d\u6742\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u540c\u6b65\u65b9\u6cd5\u5728\u8bb8\u591a\u5f02\u6784\u8ba1\u7b97\u573a\u666f\u4e2d\u63a5\u8fd1\u6700\u4f18\uff0c\u5176\u65f6\u95f4\u590d\u6742\u6027\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\u8fbe\u5230\u6700\u4f18\uff08\u6700\u591a\u76f8\u5dee\u5bf9\u6570\u56e0\u5b50\uff09\u3002\u540c\u6b65\u65b9\u6cd5\u867d\u7136\u4e0d\u662f\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5728\u8bb8\u591a\u73b0\u4ee3\u5f02\u6784\u8ba1\u7b97\u573a\u666f\u4e2d\u8db3\u591f\u6709\u6548\u3002", "conclusion": "\u540c\u6b65SGD\u65b9\u6cd5\u5728\u5f02\u6784\u8ba1\u7b97\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u9884\u671f\uff0c\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\u63a5\u8fd1\u6700\u4f18\uff0c\u6311\u6218\u4e86\u5f02\u6b65\u65b9\u6cd5\u5fc5\u7136\u66f4\u4f18\u7684\u5047\u8bbe\uff0c\u4e3a\u5206\u5e03\u5f0f\u4f18\u5316\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u4f9d\u636e\u3002"}}
