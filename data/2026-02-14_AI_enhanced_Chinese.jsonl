{"id": "2602.11481", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11481", "abs": "https://arxiv.org/abs/2602.11481", "authors": ["Minda Li", "Bhaskar Krishnamachari"], "title": "Compiler-Guided Inference-Time Adaptation: Improving GPT-5 Programming Performance in Idris", "comment": null, "summary": "GPT-5, a state of the art large language model from OpenAI, demonstrates strong performance in widely used programming languages such as Python, C++, and Java; however, its ability to operate in low resource or less commonly used languages remains underexplored. This work investigates whether GPT-5 can effectively acquire proficiency in an unfamiliar functional programming language, Idris, through iterative, feedback driven prompting. We first establish a baseline showing that with zero shot prompting the model solves only 22 out of 56 Idris exercises using the platform Exercism, substantially underperforming relative to higher resource languages (45 out of 50 in Python and 35 out of 47 in Erlang). We then evaluate several refinement strategies, including iterative prompting based on platform feedback, augmenting prompts with documentation and error classification guides, and iterative prompting using local compilation errors and failed test cases. Among these approaches, incorporating local compilation errors yields the most substantial improvements. Using this structured, error guided refinement loop, GPT-5 performance increased to an impressive 54 solved problems out of 56. These results suggest that while large language models may initially struggle in low resource settings, structured compiler level feedback can play a critical role in unlocking their capabilities.", "AI": {"tldr": "GPT-5\u5728\u4f4e\u8d44\u6e90\u51fd\u6570\u5f0f\u7f16\u7a0b\u8bed\u8a00Idris\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u57fa\u4e8e\u7f16\u8bd1\u9519\u8bef\u7684\u8fed\u4ee3\u63d0\u793a\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4ece22/56\u63d0\u5347\u523054/56", "motivation": "\u7814\u7a76GPT-5\u5728\u4f4e\u8d44\u6e90\u6216\u4e0d\u5e38\u7528\u7f16\u7a0b\u8bed\u8a00\uff08\u5982Idris\uff09\u4e2d\u7684\u80fd\u529b\uff0c\u8fd9\u4e9b\u8bed\u8a00\u76f8\u5bf9\u4e8ePython\u3001C++\u3001Java\u7b49\u9ad8\u8d44\u6e90\u8bed\u8a00\u7814\u7a76\u4e0d\u8db3", "method": "\u4f7f\u7528Exercism\u5e73\u53f0\u7684Idris\u7ec3\u4e60\u8fdb\u884c\u8bc4\u4f30\uff0c\u6bd4\u8f83\u591a\u79cd\u7cbe\u70bc\u7b56\u7565\uff1a\u57fa\u4e8e\u5e73\u53f0\u53cd\u9988\u7684\u8fed\u4ee3\u63d0\u793a\u3001\u6dfb\u52a0\u6587\u6863\u548c\u9519\u8bef\u5206\u7c7b\u6307\u5357\u3001\u4f7f\u7528\u672c\u5730\u7f16\u8bd1\u9519\u8bef\u548c\u5931\u8d25\u6d4b\u8bd5\u7528\u4f8b\u7684\u8fed\u4ee3\u63d0\u793a", "result": "\u96f6\u6b21\u63d0\u793a\u4ec5\u89e3\u51b322/56\u4e2a\u95ee\u9898\uff0c\u8fdc\u4f4e\u4e8ePython\uff0845/50\uff09\u548cErlang\uff0835/47\uff09\u3002\u4f7f\u7528\u672c\u5730\u7f16\u8bd1\u9519\u8bef\u7684\u8fed\u4ee3\u63d0\u793a\u6548\u679c\u6700\u597d\uff0c\u5c06\u6027\u80fd\u63d0\u5347\u523054/56\u4e2a\u95ee\u9898", "conclusion": "\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u53ef\u80fd\u521d\u59cb\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u7ed3\u6784\u5316\u7684\u7f16\u8bd1\u5668\u7ea7\u522b\u53cd\u9988\u5728\u91ca\u653e\u5176\u80fd\u529b\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528"}}
{"id": "2602.11357", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.11357", "abs": "https://arxiv.org/abs/2602.11357", "authors": ["Xiaoling Yi", "Ryan Antonio", "Yunhao Deng", "Fanchen Kong", "Joren Dumoulin", "Jun Yin", "Marian Verhelst"], "title": "A 16 nm 1.60TOPS/W High Utilization DNN Accelerator with 3D Spatial Data Reuse and Efficient Shared Memory Access", "comment": "Accepted at ISCAS 2026 (2026 IEEE International Symposium on Circuits and Systems)", "summary": "Achieving high compute utilization across a wide range of AI workloads is crucial for the efficiency of versatile DNN accelerators. This paper presents the Voltra chip and its utilization-optimised DNN accelerator architecture, which leverages 3-Dimensional (3D) spatial data reuse along with efficient and flexible shared memory access. The 3D spatial dataflow enables balanced spatial data reuse across three dimensions, improving spatial utilization by up to 2.0x compared to a conventional 2D design. Inside the shared memory access architecture, Voltra incorporates flexible data streamers that enable mixed-grained hardware data pre-fetching and dynamic memory allocation, further improving the temporal utilization by 2.12-2.94x and achieving 1.15-2.36x total latency speedup compared with the non-prefetching and separated memory architecture, respectively. Fabricated in 16nm technology, our chip achieves 1.60 TOPS/W peak system energy efficiency and 1.25 TOPS/mm2 system area efficiency, which is competitive with state-of-the-art solutions while achieving high utilization across diverse workloads.", "AI": {"tldr": "Voltra\u82af\u7247\u91c7\u75283D\u7a7a\u95f4\u6570\u636e\u6d41\u548c\u7075\u6d3b\u5171\u4eab\u5185\u5b58\u67b6\u6784\uff0c\u901a\u8fc73D\u7a7a\u95f4\u6570\u636e\u590d\u7528\u548c\u52a8\u6001\u5185\u5b58\u5206\u914d\uff0c\u76f8\u6bd4\u4f20\u7edf2D\u8bbe\u8ba1\u63d0\u5347\u7a7a\u95f4\u5229\u7528\u73872.0\u500d\uff0c\u65f6\u95f4\u5229\u7528\u73872.12-2.94\u500d\uff0c\u572816nm\u5de5\u827a\u4e0b\u5b9e\u73b01.60 TOPS/W\u80fd\u6548\u548c1.25 TOPS/mm\u00b2\u9762\u79ef\u6548\u7387\u3002", "motivation": "\u63d0\u9ad8AI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8ba1\u7b97\u5229\u7528\u7387\u5bf9\u4e8e\u901a\u7528DNN\u52a0\u901f\u5668\u7684\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u8bbe\u8ba1\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u5229\u7528\u7387\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u65b0\u7684\u67b6\u6784\u6765\u4f18\u5316\u6570\u636e\u590d\u7528\u548c\u5185\u5b58\u8bbf\u95ee\u3002", "method": "\u63d0\u51faVoltra\u82af\u7247\u67b6\u6784\uff0c\u91c7\u75283D\u7a7a\u95f4\u6570\u636e\u6d41\u5b9e\u73b0\u4e09\u7ef4\u5e73\u8861\u6570\u636e\u590d\u7528\uff0c\u7ed3\u5408\u7075\u6d3b\u7684\u6570\u636e\u6d41\u5904\u7406\u5668\u652f\u6301\u6df7\u5408\u7c92\u5ea6\u786c\u4ef6\u9884\u53d6\u548c\u52a8\u6001\u5185\u5b58\u5206\u914d\uff0c\u4f18\u5316\u5171\u4eab\u5185\u5b58\u8bbf\u95ee\u3002", "result": "3D\u8bbe\u8ba1\u76f8\u6bd4\u4f20\u7edf2D\u8bbe\u8ba1\u63d0\u5347\u7a7a\u95f4\u5229\u7528\u73872.0\u500d\uff1b\u7075\u6d3b\u5185\u5b58\u67b6\u6784\u63d0\u5347\u65f6\u95f4\u5229\u7528\u73872.12-2.94\u500d\uff0c\u603b\u5ef6\u8fdf\u52a0\u901f1.15-2.36\u500d\uff1b16nm\u5de5\u827a\u4e0b\u5b9e\u73b01.60 TOPS/W\u5cf0\u503c\u7cfb\u7edf\u80fd\u6548\u548c1.25 TOPS/mm\u00b2\u7cfb\u7edf\u9762\u79ef\u6548\u7387\u3002", "conclusion": "Voltra\u82af\u7247\u901a\u8fc7\u521b\u65b0\u76843D\u7a7a\u95f4\u6570\u636e\u6d41\u548c\u7075\u6d3b\u5171\u4eab\u5185\u5b58\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u65b9\u6848\u7ade\u4e89\u529b\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u8de8\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9ad8\u8ba1\u7b97\u5229\u7528\u7387\u3002"}}
{"id": "2602.11362", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.11362", "abs": "https://arxiv.org/abs/2602.11362", "authors": ["Reginald Frank", "Soujanya Ponnapalli", "Octavio Lomeli", "Neil Giridharan", "Marcos K Aguilera", "Natacha Crooks"], "title": "Real Life Is Uncertain. Consensus Should Be Too!", "comment": "HotOS '25: Proceedings of the 2025 Workshop on Hot Topics in Operating Systems", "summary": "Modern distributed systems rely on consensus protocols to build a fault-tolerant-core upon which they can build applications. Consensus protocols are correct under a specific failure model, where up to $f$ machines can fail. We argue that this $f$-threshold failure model oversimplifies the real world and limits potential opportunities to optimize for cost or performance. We argue instead for a probabilistic failure model that captures the complex and nuanced nature of faults observed in practice. Probabilistic consensus protocols can explicitly leverage individual machine \\textit{failure curves} and explore side-stepping traditional bottlenecks such as majority quorum intersection, enabling systems that are more reliable, efficient, cost-effective, and sustainable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7528\u6982\u7387\u6027\u6545\u969c\u6a21\u578b\u66ff\u4ee3\u4f20\u7edf\u7684f\u9608\u503c\u6545\u969c\u6a21\u578b\uff0c\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\uff0c\u5e76\u4f18\u5316\u5171\u8bc6\u534f\u8bae\u7684\u6210\u672c\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5171\u8bc6\u534f\u8bae\u57fa\u4e8ef\u9608\u503c\u6545\u969c\u6a21\u578b\uff08\u6700\u591af\u53f0\u673a\u5668\u6545\u969c\uff09\uff0c\u8fd9\u79cd\u6a21\u578b\u8fc7\u5ea6\u7b80\u5316\u4e86\u73b0\u5b9e\u4e16\u754c\uff0c\u9650\u5236\u4e86\u5728\u6210\u672c\u548c\u6027\u80fd\u65b9\u9762\u7684\u4f18\u5316\u673a\u4f1a\u3002\u73b0\u5b9e\u4e2d\u7684\u6545\u969c\u5177\u6709\u590d\u6742\u6027\u548c\u7ec6\u5fae\u5dee\u522b\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u6a21\u578b\u6765\u6355\u6349\u3002", "method": "\u63d0\u51fa\u6982\u7387\u6027\u6545\u969c\u6a21\u578b\uff0c\u5229\u7528\u4e2a\u4f53\u673a\u5668\u7684\"\u6545\u969c\u66f2\u7ebf\"\uff0c\u7ed5\u8fc7\u4f20\u7edf\u74f6\u9888\uff08\u5982\u591a\u6570\u4ef2\u88c1\u4ea4\u96c6\uff09\uff0c\u8bbe\u8ba1\u66f4\u53ef\u9760\u3001\u9ad8\u6548\u3001\u7ecf\u6d4e\u3001\u53ef\u6301\u7eed\u7684\u7cfb\u7edf\u3002", "result": "\u6982\u7387\u6027\u5171\u8bc6\u534f\u8bae\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u5efa\u6a21\u73b0\u5b9e\u6545\u969c\uff0c\u63d0\u4f9b\u6bd4\u4f20\u7edf\u9608\u503c\u6a21\u578b\u66f4\u597d\u7684\u53ef\u9760\u6027\u3001\u6548\u7387\u3001\u6210\u672c\u6548\u76ca\u548c\u53ef\u6301\u7eed\u6027\u3002", "conclusion": "\u6982\u7387\u6027\u6545\u969c\u6a21\u578b\u6bd4\u4f20\u7edf\u7684f\u9608\u503c\u6a21\u578b\u66f4\u80fd\u53cd\u6620\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5b9e\u9645\u6545\u969c\u7279\u6027\uff0c\u4e3a\u5171\u8bc6\u534f\u8bae\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411\uff0c\u6709\u671b\u6784\u5efa\u66f4\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\u3002"}}
{"id": "2602.11461", "categories": ["cs.AR", "cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.11461", "abs": "https://arxiv.org/abs/2602.11461", "authors": ["Yilun Huang", "Asal Mehradfar", "Salman Avestimehr", "Hamidreza Aghasi"], "title": "EM-Aware Physical Synthesis: Neural Inductor Modeling and Intelligent Placement & Routing for RF Circuits", "comment": "Accepted at the 2026 IEEE International Symposium on Circuits and Systems (ISCAS 2026)", "summary": "This paper presents an ML-driven framework for automated RF physical synthesis that transforms circuit netlists into manufacturable GDSII layouts. While recent ML approaches demonstrate success in topology selection and parameter optimization, they fail to produce manufacturable layouts due to oversimplified component models and lack of routing capabilities. Our framework addresses these limitations through three key innovations: (1) a neural network framework trained on 18,210 inductor geometries with frequency sweeps from 1-100 GHz, generating 7.5 million training samples, that predicts inductor Q-factor with less than 2% error and enables fast gradient-based layout optimization with a 93.77% success rate in producing high-Q layouts; (2) an intelligent P-Cell optimizer that reduces layout area while maintaining design-rule-check (DRC) compliance; and (3) a complete placement and routing engine with frequency-dependent EM spacing rules and DRC-aware synthesis. The neural inductor model demonstrates superior accuracy across 1-100 GHz, enabling EM-accurate component synthesis with real-time inference. The framework successfully generates DRC-aware GDSII layouts for RF circuits, representing a significant step toward automated RF physical design.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2aML\u9a71\u52a8\u7684RF\u7269\u7406\u5408\u6210\u6846\u67b6\uff0c\u5c06\u7535\u8def\u7f51\u8868\u8f6c\u6362\u4e3a\u53ef\u5236\u9020\u7684GDSII\u5e03\u5c40\uff0c\u89e3\u51b3\u4e86\u73b0\u6709ML\u65b9\u6cd5\u65e0\u6cd5\u751f\u6210\u53ef\u5236\u9020\u5e03\u5c40\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709ML\u65b9\u6cd5\u5728\u62d3\u6251\u9009\u62e9\u548c\u53c2\u6570\u4f18\u5316\u65b9\u9762\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u7531\u4e8e\u7ec4\u4ef6\u6a21\u578b\u8fc7\u4e8e\u7b80\u5316\u4e14\u7f3a\u4e4f\u5e03\u7ebf\u80fd\u529b\uff0c\u65e0\u6cd5\u751f\u6210\u53ef\u5236\u9020\u7684\u5e03\u5c40\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316RF\u7269\u7406\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u57fa\u4e8e18,210\u4e2a\u7535\u611f\u51e0\u4f55\u7ed3\u6784\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u9884\u6d4bQ\u56e0\u5b50\u8bef\u5dee\u5c0f\u4e8e2%\uff1b2) \u667a\u80fdP-Cell\u4f18\u5316\u5668\uff0c\u51cf\u5c11\u5e03\u5c40\u9762\u79ef\u540c\u65f6\u4fdd\u6301DRC\u5408\u89c4\uff1b3) \u5b8c\u6574\u7684\u5e03\u5c40\u5e03\u7ebf\u5f15\u64ce\uff0c\u5177\u6709\u9891\u7387\u76f8\u5173\u7684EM\u95f4\u8ddd\u89c4\u5219\u548cDRC\u611f\u77e5\u5408\u6210\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u57281-100 GHz\u8303\u56f4\u5185\u8868\u73b0\u51fa\u5353\u8d8a\u7cbe\u5ea6\uff0c\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u7684EM\u7cbe\u786e\u7ec4\u4ef6\u5408\u6210\u3002\u6846\u67b6\u6210\u529f\u751f\u6210DRC\u611f\u77e5\u7684RF\u7535\u8defGDSII\u5e03\u5c40\uff0c\u9ad8Q\u5e03\u5c40\u6210\u529f\u738793.77%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4ee3\u8868\u4e86\u5411\u81ea\u52a8\u5316RF\u7269\u7406\u8bbe\u8ba1\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u80fd\u591f\u751f\u6210\u53ef\u5236\u9020\u7684GDSII\u5e03\u5c40\uff0c\u89e3\u51b3\u4e86\u73b0\u6709ML\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.11456", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.11456", "abs": "https://arxiv.org/abs/2602.11456", "authors": ["Chaoyi Ruan", "Geng Luo", "Xinyi Wan", "Long Zhao", "Qinghe Wang", "Jiaan Zhu", "Duling Xu", "Guanbin Xu", "Dehui Wei", "Xiang Liu", "Cheng Li", "Haifeng Sun", "Congcong Miao", "Jialin Li"], "title": "RL over Commodity Networks: Overcoming the Bandwidth Barrier with Lossless Sparse Deltas", "comment": null, "summary": "LLM post-training with reinforcement learning (RL) requires frequent synchronization of large model parameters between the trainer and distributed rollout actors. High-throughput RL post-training therefore relies on dedicated RDMA HPC clusters, an infrastructure cost most organizations cannot absorb. A natural alternative is to aggregate loosely-coupled GPUs over standard Ethernet and WAN links, but this commodity connectivity cannot sustain full-weight broadcasts: synchronizing an 8B model can take over 100~seconds on bandwidth-limited links, while rollout generation typically takes tens of seconds.\n  Toward making RL practical in this regime, we observe that RL fine-tuning yields highly sparse per-step updates, with only around 1\\% of parameter elements changing. Atop this insight, we present SparrowRL, a novel high-performance RL training system that preserves bit-exact updates without dropping or quantizing information, designed for commodity-networked, loosely-coupled GPU resources. SparrowRL represents each step as a sparse delta checkpoint, pipelines delta extraction with multi-stream transmission, overlaps transfer with rollout generation, and coordinates heterogeneous workers with throughput- and bandwidth-aware scheduling plus lease-based fault tolerance. On Qwen3 models from 4B to 14B deployed across up to four geographic regions, SparrowRL reduces per-step transfer payload by 79$\\times$ for Qwen3-8B and improves throughput by 2.4--9.5$\\times$ over full-weight broadcast across WAN, narrowing the throughput gap relative to an ideal RDMA single-datacenter baseline to within 8.91\\%. By leveraging on-demand, cross-cloud GPUs over commodity links, SparrowRL delivers 1.21--1.59$\\times$ higher tokens per dollar than reserved RDMA clusters at comparable throughput.", "AI": {"tldr": "SparrowRL\uff1a\u9488\u5bf9\u677e\u6563\u8026\u5408GPU\u96c6\u7fa4\u7684\u7a00\u758f\u589e\u91cf\u4f20\u8f93RL\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u4ec5\u4f20\u8f931%\u53d8\u5316\u7684\u53c2\u6570\uff0c\u5728\u666e\u901a\u7f51\u7edc\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u6548RL\u5fae\u8c03", "motivation": "\u4f20\u7edfRL\u8bad\u7ec3\u9700\u8981\u9891\u7e41\u540c\u6b65\u5927\u6a21\u578b\u53c2\u6570\uff0c\u4f9d\u8d56\u6602\u8d35\u7684RDMA HPC\u96c6\u7fa4\u3002\u666e\u901a\u4ee5\u592a\u7f51/WAN\u8fde\u63a5\u65e0\u6cd5\u627f\u53d7\u5168\u6743\u91cd\u5e7f\u64ad\uff08\u540c\u6b658B\u6a21\u578b\u9700100+\u79d2\uff09\uff0c\u9650\u5236\u4e86RL\u5728\u677e\u6563\u8026\u5408GPU\u8d44\u6e90\u4e0a\u7684\u5b9e\u7528\u6027", "method": "\u57fa\u4e8eRL\u5fae\u8c03\u4ea7\u751f\u7a00\u758f\u66f4\u65b0\u7684\u89c2\u5bdf\uff08\u4ec5\u7ea61%\u53c2\u6570\u53d8\u5316\uff09\uff0c\u8bbe\u8ba1SparrowRL\u7cfb\u7edf\uff1a1\uff09\u5c06\u6bcf\u4e00\u6b65\u8868\u793a\u4e3a\u7a00\u758f\u589e\u91cf\u68c0\u67e5\u70b9\uff1b2\uff09\u6d41\u6c34\u7ebf\u5316\u589e\u91cf\u63d0\u53d6\u4e0e\u591a\u6d41\u4f20\u8f93\uff1b3\uff09\u4f20\u8f93\u4e0erollout\u751f\u6210\u91cd\u53e0\uff1b4\uff09\u541e\u5410\u91cf\u548c\u5e26\u5bbd\u611f\u77e5\u8c03\u5ea6\u52a0\u79df\u7ea6\u5bb9\u9519\u673a\u5236", "result": "\u5728Qwen3\u6a21\u578b\uff084B-14B\uff09\u8de8\u6700\u591a4\u4e2a\u5730\u7406\u533a\u57df\u7684\u90e8\u7f72\u4e2d\uff1a1\uff09Qwen3-8B\u6bcf\u6b65\u4f20\u8f93\u8d1f\u8f7d\u51cf\u5c1179\u500d\uff1b2\uff09WAN\u4e0a\u541e\u5410\u91cf\u6bd4\u5168\u6743\u91cd\u5e7f\u64ad\u63d0\u53472.4-9.5\u500d\uff1b3\uff09\u4e0e\u7406\u60f3RDMA\u5355\u6570\u636e\u4e2d\u5fc3\u57fa\u7ebf\u7684\u541e\u5410\u5dee\u8ddd\u7f29\u5c0f\u52308.91%\u4ee5\u5185\uff1b4\uff09\u6309\u9700\u8de8\u4e91GPU\u6bd4\u9884\u7559RDMA\u96c6\u7fa4\u6bcf\u7f8e\u5143token\u6570\u9ad81.21-1.59\u500d", "conclusion": "SparrowRL\u901a\u8fc7\u7a00\u758f\u589e\u91cf\u4f20\u8f93\u673a\u5236\uff0c\u4f7fRL\u8bad\u7ec3\u80fd\u5728\u666e\u901a\u7f51\u7edc\u8fde\u63a5\u7684\u677e\u6563\u8026\u5408GPU\u8d44\u6e90\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u663e\u8457\u964d\u4f4e\u6210\u672c\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684RL\u5e94\u7528\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2602.11521", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.11521", "abs": "https://arxiv.org/abs/2602.11521", "authors": ["Lian Liu", "Shixin Zhao", "Yutian Zhou", "Yintao He", "Mengdi Wang", "Yinhe Han", "Ying Wang"], "title": "PAM: Processing Across Memory Hierarchy for Efficient KV-centric LLM Serving System", "comment": "15 pages, 13 figures", "summary": "The widespread adoption of Large Language Models (LLMs) has exponentially increased the demand for efficient serving systems. With growing requests and context lengths, key-value (KV)-related operations, including attention computation and KV cache storage, have emerged as critical bottlenecks. They require massive memory bandwidth and capacity. Unfortunately, existing LLM serving systems, optimized for compute-bound workloads, fail to handle these memory-intensive operations effectively. Even with Processing-In-Memory (PIM) technology, current single-level memory designs cannot simultaneously satisfy the bandwidth and capacity requirements.\n  To address these challenges, we propose Processing Across Memory (PAM), a KV-centric LLM serving system that coordinates heterogeneous PIM-enabled memory devices within a hierarchical architecture. PAM introduces a novel computing paradigm to balance high memory bandwidth with scalable capacity. First, PAM exploits the inherent context locality in KV access patterns to intelligently distribute KV tokens across the memory hierarchy. Second, to further exploit context locality, it introduces the PAMattention algorithm, enabling fine-grained parallel attention computation across heterogeneous PIM devices. Finally, PAM incorporates an intra-device KV mapping, inter-device KV migration interface, and an inter-device online KV scheduling algorithm to dynamically balance computational workloads. By addressing both bandwidth and capacity demands simultaneously, PAM significantly enhances the efficiency and scalability of LLM serving systems, paving the way for cost-effective, high-performance solutions in the era of large-scale AI.", "AI": {"tldr": "PAM\u662f\u4e00\u4e2aKV\u4e2d\u5fc3\u7684LLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u534f\u8c03\u5f02\u6784PIM\u5185\u5b58\u8bbe\u5907\u7684\u5206\u5c42\u67b6\u6784\uff0c\u5e73\u8861\u9ad8\u5185\u5b58\u5e26\u5bbd\u4e0e\u53ef\u6269\u5c55\u5bb9\u91cf\uff0c\u89e3\u51b3KV\u76f8\u5173\u64cd\u4f5c\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u968f\u7740LLM\u7684\u5e7f\u6cdb\u91c7\u7528\uff0cKV\u76f8\u5173\u64cd\u4f5c\uff08\u6ce8\u610f\u529b\u8ba1\u7b97\u548cKV\u7f13\u5b58\u5b58\u50a8\uff09\u6210\u4e3a\u5173\u952e\u74f6\u9888\uff0c\u9700\u8981\u5927\u91cf\u5185\u5b58\u5e26\u5bbd\u548c\u5bb9\u91cf\u3002\u73b0\u6709LLM\u670d\u52a1\u7cfb\u7edf\u9488\u5bf9\u8ba1\u7b97\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4f18\u5316\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u5185\u5b58\u5bc6\u96c6\u578b\u64cd\u4f5c\uff0c\u5373\u4f7f\u4f7f\u7528PIM\u6280\u672f\uff0c\u5f53\u524d\u7684\u5355\u7ea7\u5185\u5b58\u8bbe\u8ba1\u4e5f\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u5e26\u5bbd\u548c\u5bb9\u91cf\u9700\u6c42\u3002", "method": "1. \u5229\u7528KV\u8bbf\u95ee\u6a21\u5f0f\u4e2d\u7684\u4e0a\u4e0b\u6587\u5c40\u90e8\u6027\uff0c\u667a\u80fd\u5730\u5c06KV\u4ee4\u724c\u5206\u5e03\u5728\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u4e2d\uff1b2. \u5f15\u5165PAMattention\u7b97\u6cd5\uff0c\u652f\u6301\u8de8\u5f02\u6784PIM\u8bbe\u5907\u7684\u7ec6\u7c92\u5ea6\u5e76\u884c\u6ce8\u610f\u529b\u8ba1\u7b97\uff1b3. \u5305\u542b\u8bbe\u5907\u5185KV\u6620\u5c04\u3001\u8bbe\u5907\u95f4KV\u8fc1\u79fb\u63a5\u53e3\u548c\u8bbe\u5907\u95f4\u5728\u7ebfKV\u8c03\u5ea6\u7b97\u6cd5\uff0c\u52a8\u6001\u5e73\u8861\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u3002", "result": "PAM\u901a\u8fc7\u540c\u65f6\u89e3\u51b3\u5e26\u5bbd\u548c\u5bb9\u91cf\u9700\u6c42\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u670d\u52a1\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u5927\u89c4\u6a21AI\u65f6\u4ee3\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "PAM\u901a\u8fc7\u534f\u8c03\u5f02\u6784PIM\u5185\u5b58\u8bbe\u5907\u7684\u5206\u5c42\u67b6\u6784\uff0c\u5e73\u8861\u9ad8\u5185\u5b58\u5e26\u5bbd\u4e0e\u53ef\u6269\u5c55\u5bb9\u91cf\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u670d\u52a1\u4e2d\u7684KV\u76f8\u5173\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21AI\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11544", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.11544", "abs": "https://arxiv.org/abs/2602.11544", "authors": ["Yiming Zhou", "Kaiping Xue", "Enhong Chen"], "title": "Differentially Private Perturbed Push-Sum Protocol and Its Application in Non-Convex Optimization", "comment": null, "summary": "In decentralized networks, nodes cannot ensure that their shared information will be securely preserved by their neighbors, making privacy vulnerable to inference by curious nodes. Adding calibrated random noise before communication to satisfy differential privacy offers a proven defense; however, most existing methods are tailored to specific downstream tasks and lack a general, protocol-level privacy-preserving solution. To bridge this gap, we propose Differentially Private Perturbed Push-Sum (DPPS), a lightweight differential privacy protocol for decentralized communication. Since protocol-level differential privacy introduces the unique challenge of obtaining the sensitivity for each communication round, DPPS introduces a novel sensitivity estimation mechanism that requires each node to compute and broadcast only one scalar per round, enabling rigorous differential privacy guarantees. This design allows DPPS to serve as a plug-and-play, low-cost privacy-preserving solution for downstream applications built on it. To provide a concrete instantiation of DPPS and better balance the privacy-utility trade-off, we design PartPSP, a privacy-preserving decentralized algorithm for non-convex optimization that integrates a partial communication mechanism. By partitioning model parameters into local and shared components and applying DPPS only to the shared parameters, PartPSP reduces the dimensionality of consensus data, thereby lowering the magnitude of injected noise and improving optimization performance. We theoretically prove that PartPSP converges under non-convex objectives and, with partial communication, achieves better optimization performance under the same privacy budget. Experimental results validate the effectiveness of DPPS's privacy-preserving and demonstrate that PartPSP outperforms existing privacy-preserving decentralized optimization algorithms.", "AI": {"tldr": "\u63d0\u51faDPPS\u534f\u8bae\u7ea7\u5dee\u5206\u9690\u79c1\u65b9\u6848\u548cPartPSP\u975e\u51f8\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u654f\u611f\u6027\u4f30\u8ba1\u548c\u90e8\u5206\u901a\u4fe1\u673a\u5236\u5e73\u8861\u9690\u79c1\u4e0e\u6027\u80fd", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u7f51\u7edc\u4e2d\u8282\u70b9\u65e0\u6cd5\u786e\u4fdd\u90bb\u5c45\u5b89\u5168\u4fdd\u5b58\u5171\u4eab\u4fe1\u606f\uff0c\u73b0\u6709\u5dee\u5206\u9690\u79c1\u65b9\u6cd5\u591a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff0c\u7f3a\u4e4f\u901a\u7528\u534f\u8bae\u7ea7\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faDPPS\u8f7b\u91cf\u7ea7\u5dee\u5206\u9690\u79c1\u534f\u8bae\uff0c\u5305\u542b\u65b0\u9896\u7684\u654f\u611f\u6027\u4f30\u8ba1\u673a\u5236\uff1b\u8bbe\u8ba1PartPSP\u7b97\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u5206\u533a\uff08\u672c\u5730/\u5171\u4eab\uff09\u548c\u90e8\u5206\u901a\u4fe1\u673a\u5236\u964d\u4f4e\u566a\u58f0\u5f71\u54cd", "result": "DPPS\u63d0\u4f9b\u4e25\u683c\u5dee\u5206\u9690\u79c1\u4fdd\u8bc1\uff0cPartPSP\u5728\u975e\u51f8\u76ee\u6807\u4e0b\u6536\u655b\uff0c\u76f8\u540c\u9690\u79c1\u9884\u7b97\u4e0b\u901a\u8fc7\u90e8\u5206\u901a\u4fe1\u83b7\u5f97\u66f4\u597d\u4f18\u5316\u6027\u80fd", "conclusion": "DPPS\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u6709\u6548\uff0cPartPSP\u5728\u9690\u79c1-\u6548\u7528\u6743\u8861\u4e0a\u4f18\u4e8e\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u53bb\u4e2d\u5fc3\u5316\u4f18\u5316\u7b97\u6cd5"}}
{"id": "2602.11580", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.11580", "abs": "https://arxiv.org/abs/2602.11580", "authors": ["Hao Zhen", "Qingxuan Kang", "Yungang Bao", "Trevor E. Carlson"], "title": "Benchmarking for Single Feature Attribution with Microarchitecture Cliffs", "comment": "12 pages, 14 figures, 4 tables", "summary": "Architectural simulators play a critical role in early microarchitectural exploration due to their flexibility and high productivity. However, their effectiveness is often constrained by fidelity: simulators may deviate from the behavior of the final RTL, leading to unreliable performance estimates. Consequently, model calibration, which aligns simulator behavior with the RTL as the ground-truth microarchitecture, becomes essential for achieving accurate performance modeling.\n  To facilitate model calibration accuracy, we propose Microarchitecture Cliffs, a benchmark generation methodology designed to expose mismatches in microarchitectural behavior between the simulator and RTL. After identifying the key architectural components that require calibration, the Cliff methodology enables precise attribution of microarchitectural differences to a single microarchitectural feature through a set of benchmarks. In addition, we develop a set of automated tools to improve the efficiency of the Cliff workflow.\n  We apply the Cliff methodology to calibrate the XiangShan version of gem5 (XS-GEM5) against the XiangShan open-source CPU (XS-RTL). We reduce the performance error of XS-GEM5 from 59.2% to just 1.4% on the Cliff benchmarks. Meanwhile, the calibration guided by Cliffs effectively reduces the relative error of a representative tightly coupled microarchitectural feature by 48.03%. It also substantially lowers the absolute performance error, with reductions of 15.1% and 21.0% on SPECint2017 and SPECfp2017, respectively.", "AI": {"tldr": "\u63d0\u51faMicroarchitecture Cliffs\u57fa\u51c6\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u66b4\u9732\u6a21\u62df\u5668\u4e0eRTL\u4e4b\u95f4\u7684\u5fae\u67b6\u6784\u884c\u4e3a\u5dee\u5f02\uff0c\u5e2e\u52a9\u6821\u51c6\u6a21\u62df\u5668\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u67b6\u6784\u6a21\u62df\u5668\u5728\u65e9\u671f\u5fae\u67b6\u6784\u63a2\u7d22\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u6709\u6548\u6027\u53d7\u9650\u4e8e\u4fdd\u771f\u5ea6\uff1a\u6a21\u62df\u5668\u53ef\u80fd\u4e0e\u6700\u7ec8RTL\u884c\u4e3a\u5b58\u5728\u504f\u5dee\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u6027\u80fd\u4f30\u8ba1\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5c06\u6a21\u62df\u5668\u884c\u4e3a\u4e0eRTL\u4f5c\u4e3a\u771f\u5b9e\u5fae\u67b6\u6784\u8fdb\u884c\u6821\u51c6\uff0c\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u6027\u80fd\u5efa\u6a21\u3002", "method": "\u63d0\u51faCliff\u57fa\u51c6\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u8bc6\u522b\u9700\u8981\u6821\u51c6\u7684\u5173\u952e\u67b6\u6784\u7ec4\u4ef6\u540e\uff0c\u901a\u8fc7\u4e00\u7ec4\u57fa\u51c6\u6d4b\u8bd5\u5c06\u5fae\u67b6\u6784\u5dee\u5f02\u7cbe\u786e\u5f52\u56e0\u4e8e\u5355\u4e2a\u5fae\u67b6\u6784\u7279\u5f81\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4e00\u5957\u81ea\u52a8\u5316\u5de5\u5177\u6765\u63d0\u9ad8Cliff\u5de5\u4f5c\u6d41\u7a0b\u7684\u6548\u7387\u3002", "result": "\u5c06Cliff\u65b9\u6cd5\u5e94\u7528\u4e8e\u6821\u51c6XiangShan\u7248\u672c\u7684gem5\uff08XS-GEM5\uff09\u4e0eXiangShan\u5f00\u6e90CPU\uff08XS-RTL\uff09\u3002\u5728Cliff\u57fa\u51c6\u4e0a\uff0c\u5c06XS-GEM5\u7684\u6027\u80fd\u8bef\u5dee\u4ece59.2%\u964d\u4f4e\u5230\u4ec51.4%\u3002\u540c\u65f6\uff0c\u6821\u51c6\u4f7f\u4ee3\u8868\u6027\u7d27\u5bc6\u8026\u5408\u5fae\u67b6\u6784\u7279\u5f81\u7684\u76f8\u5bf9\u8bef\u5dee\u964d\u4f4e\u4e8648.03%\uff0c\u5e76\u5728SPECint2017\u548cSPECfp2017\u4e0a\u5206\u522b\u964d\u4f4e\u4e8615.1%\u548c21.0%\u7684\u7edd\u5bf9\u6027\u80fd\u8bef\u5dee\u3002", "conclusion": "Microarchitecture Cliffs\u65b9\u6cd5\u80fd\u6709\u6548\u66b4\u9732\u6a21\u62df\u5668\u4e0eRTL\u4e4b\u95f4\u7684\u5fae\u67b6\u6784\u884c\u4e3a\u5dee\u5f02\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u5dee\u5f02\u5f52\u56e0\u548c\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u663e\u8457\u63d0\u9ad8\u6a21\u62df\u5668\u6821\u51c6\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u53ef\u9760\u7684\u6027\u80fd\u5efa\u6a21\u63d0\u4f9b\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2602.11686", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11686", "abs": "https://arxiv.org/abs/2602.11686", "authors": ["Xinyi Liu", "Yujie Wang", "Fangcheng Fu", "Xuefeng Xiao", "Huixia Li", "Jiashi Li", "Bin Cui"], "title": "LAER-MoE: Load-Adaptive Expert Re-layout for Efficient Mixture-of-Experts Training", "comment": "19 pages, 12 figures, the paper will be presented at ASPLOS 2026", "summary": "Expert parallelism is vital for effectively training Mixture-of-Experts (MoE) models, enabling different devices to host distinct experts, with each device processing different input data. However, during expert parallel training, dynamic routing results in significant load imbalance among experts: a handful of overloaded experts hinder overall iteration, emerging as a training bottleneck.\n  In this paper, we introduce LAER-MoE, an efficient MoE training framework. The core of LAER-MoE is a novel parallel paradigm, Fully Sharded Expert Parallel (FSEP), which fully partitions each expert parameter by the number of devices and restores partial experts at expert granularity through All-to-All communication during training. This allows for flexible re-layout of expert parameters during training to enhance load balancing. In particular, we perform fine-grained scheduling of communication operations to minimize communication overhead. Additionally, we develop a load balancing planner to formulate re-layout strategies of experts and routing schemes for tokens during training. We perform experiments on an A100 cluster, and the results indicate that our system achieves up to 1.69x acceleration compared to the current state-of-the-art training systems. Source code available at https://github.com/PKU-DAIR/Hetu-Galvatron/tree/laer-moe.", "AI": {"tldr": "LAER-MoE\u662f\u4e00\u4e2a\u9ad8\u6548\u7684MoE\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7FSEP\u5e76\u884c\u8303\u5f0f\u89e3\u51b3\u4e13\u5bb6\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u6700\u9ad81.69\u500d\u52a0\u901f", "motivation": "\u4e13\u5bb6\u5e76\u884c\u8bad\u7ec3\u4e2d\uff0c\u52a8\u6001\u8def\u7531\u5bfc\u81f4\u4e13\u5bb6\u8d1f\u8f7d\u4e25\u91cd\u4e0d\u5747\u8861\uff0c\u5c11\u6570\u8fc7\u8f7d\u4e13\u5bb6\u6210\u4e3a\u8bad\u7ec3\u74f6\u9888\uff0c\u9700\u8981\u89e3\u51b3\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898", "method": "\u63d0\u51faFSEP\u5e76\u884c\u8303\u5f0f\uff0c\u5c06\u4e13\u5bb6\u53c2\u6570\u5b8c\u5168\u5206\u7247\u5230\u6240\u6709\u8bbe\u5907\uff0c\u901a\u8fc7All-to-All\u901a\u4fe1\u5728\u8bad\u7ec3\u65f6\u6309\u4e13\u5bb6\u7c92\u5ea6\u6062\u590d\u90e8\u5206\u4e13\u5bb6\uff1b\u91c7\u7528\u7ec6\u7c92\u5ea6\u901a\u4fe1\u8c03\u5ea6\u548c\u8d1f\u8f7d\u5747\u8861\u89c4\u5212\u5668\u4f18\u5316\u4e13\u5bb6\u5e03\u5c40\u548ctoken\u8def\u7531", "result": "\u5728A100\u96c6\u7fa4\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8bad\u7ec3\u7cfb\u7edf\uff0cLAER-MoE\u5b9e\u73b0\u4e86\u6700\u9ad81.69\u500d\u7684\u52a0\u901f", "conclusion": "LAER-MoE\u901a\u8fc7\u521b\u65b0\u7684FSEP\u5e76\u884c\u8303\u5f0f\u548c\u8d1f\u8f7d\u5747\u8861\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86MoE\u8bad\u7ec3\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387"}}
{"id": "2602.11614", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.11614", "abs": "https://arxiv.org/abs/2602.11614", "authors": ["Yousuf Choudhary", "Tosiron Adegbija"], "title": "Device-Circuit Co-Design of Variation-Resilient Read and Write Drivers for Antiferromagnetic Tunnel Junction (AFMTJ) Memories", "comment": "International VLSI Symposium on Technology, Systems and Applications (VLSI-TSA) 2026", "summary": "Antiferromagnetic Tunnel Junctions (AFMTJs) offer picosecond switching and high integration density for in-memory computing, but their ultrafast dynamics and low tunnel magnetoresistance (TMR) make state-of-the-art MRAM interfaces unreliable. This work develops a device-circuit co-designed read/write interface optimized for AFMTJ behavior. Using a calibrated SPICE AFMTJ model as a baseline, we identify the limitations of conventional drivers and propose an asymmetric pulse driver (PD) for deterministic picosecond switching and a self-timed sense amplifier (STSA) with dynamic trip-point tuning for low-TMR sensing. Our experiments using SPICE and Monte Carlo evaluations demonstrate that the proposed circuits preserve AFMTJ latency and energy benefits while achieving robust read/write yield under realistic PVT and 3D integration parasitics, outperforming standard MRAM front-ends under the same conditions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u53cd\u94c1\u78c1\u96a7\u9053\u7ed3\uff08AFMTJ\uff09\u7684\u5668\u4ef6-\u7535\u8def\u534f\u540c\u8bbe\u8ba1\u8bfb\u5199\u63a5\u53e3\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u8109\u51b2\u9a71\u52a8\u5668\u548c\u81ea\u5b9a\u65f6\u611f\u6d4b\u653e\u5927\u5668\u89e3\u51b3\u4e86AFMTJ\u8d85\u5feb\u52a8\u6001\u548c\u4f4eTMR\u5e26\u6765\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "AFMTJ\u5177\u6709\u76ae\u79d2\u7ea7\u5f00\u5173\u901f\u5ea6\u548c\u9ad8\u5ea6\u96c6\u6210\u5bc6\u5ea6\uff0c\u9002\u7528\u4e8e\u5185\u5b58\u8ba1\u7b97\uff0c\u4f46\u5176\u8d85\u5feb\u52a8\u6001\u7279\u6027\u548c\u4f4e\u96a7\u9053\u78c1\u963b\uff08TMR\uff09\u4f7f\u5f97\u73b0\u6709MRAM\u63a5\u53e3\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u4e13\u95e8\u4f18\u5316\u7684\u8bfb\u5199\u63a5\u53e3\u3002", "method": "\u4f7f\u7528\u6821\u51c6\u7684SPICE AFMTJ\u6a21\u578b\u4f5c\u4e3a\u57fa\u51c6\uff0c\u5206\u6790\u4e86\u4f20\u7edf\u9a71\u52a8\u5668\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u975e\u5bf9\u79f0\u8109\u51b2\u9a71\u52a8\u5668\u5b9e\u73b0\u786e\u5b9a\u6027\u76ae\u79d2\u5f00\u5173\uff0c\u4ee5\u53ca\u5177\u6709\u52a8\u6001\u89e6\u53d1\u70b9\u8c03\u8c10\u7684\u81ea\u5b9a\u65f6\u611f\u6d4b\u653e\u5927\u5668\u7528\u4e8e\u4f4eTMR\u68c0\u6d4b\u3002", "result": "SPICE\u548c\u8499\u7279\u5361\u6d1b\u8bc4\u4f30\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u7535\u8def\u5728\u4fdd\u6301AFMTJ\u5ef6\u8fdf\u548c\u80fd\u8017\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u5728\u73b0\u5b9ePVT\u548c3D\u96c6\u6210\u5bc4\u751f\u53c2\u6570\u4e0b\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u8bfb\u5199\u826f\u7387\uff0c\u4f18\u4e8e\u6807\u51c6MRAM\u524d\u7aef\u7535\u8def\u3002", "conclusion": "\u901a\u8fc7\u5668\u4ef6-\u7535\u8def\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u6210\u529f\u5f00\u53d1\u4e86\u9488\u5bf9AFMTJ\u7279\u6027\u7684\u4f18\u5316\u8bfb\u5199\u63a5\u53e3\uff0c\u89e3\u51b3\u4e86\u8d85\u5feb\u52a8\u6001\u548c\u4f4eTMR\u5e26\u6765\u7684\u53ef\u9760\u6027\u6311\u6218\uff0c\u4e3aAFMTJ\u5728\u5185\u5b58\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11741", "categories": ["cs.DC", "cs.DB", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.11741", "abs": "https://arxiv.org/abs/2602.11741", "authors": ["Bo Guan"], "title": "Designing Scalable Rate Limiting Systems: Algorithms, Architecture, and Distributed Solutions", "comment": "27 pages, 8 figures, 2 tables", "summary": "Designing a rate limiter that is simultaneously accurate, available, and scalable presents a fundamental challenge in distributed systems, primarily due to the trade-offs between algorithmic precision, availability, consistency, and partition tolerance. This article presents a concrete architecture for a distributed rate limiting system in a production-grade environment. Our design chooses the in-memory cache database, the Redis, along with its Sorted Set data structure, which provides $O(log (N))$ time complexity operation for the key-value pair dataset with efficiency and low latency, and maintains precision. The core contribution is quantifying the accuracy and memory cost trade-off of the chosen Rolling Window as the implemented rate limiting algorithm against the Token Bucket and Fixed Window algorithms. In addition, we explain how server-side Lua scripting is critical to bundling cleanup, counting, and insertion into a single atomic operation, thereby eliminating race conditions in concurrent environments. In the system architecture, we propose a three-layer architecture that manages the storage and updating of the limit rules. Through script load by hashing the rule parameters, rules can be changed without modifying the cached scripts. Furthermore, we analyze the deployment of this architecture on a Redis Cluster, which provides the availability and scalability by data sharding and replication. We explain the acceptance of AP (Availability and Partition Tolerance) from the CAP theorem as the pragmatic engineering trade-off for this use case.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRedis Sorted Set\u7684\u5206\u5e03\u5f0f\u9650\u6d41\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7Rolling Window\u7b97\u6cd5\u5728\u51c6\u786e\u6027\u548c\u5185\u5b58\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u91c7\u7528\u4e09\u5c42\u67b6\u6784\u548cLua\u811a\u672c\u4fdd\u8bc1\u539f\u5b50\u6027\uff0c\u5e76\u5728Redis Cluster\u4e0a\u90e8\u7f72\u4ee5\u5b9e\u73b0\u53ef\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u540c\u65f6\u5177\u5907\u51c6\u786e\u6027\u3001\u53ef\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u5206\u5e03\u5f0f\u9650\u6d41\u7cfb\u7edf\u9762\u4e34\u6839\u672c\u6027\u6311\u6218\uff0c\u4e3b\u8981\u6e90\u4e8e\u7b97\u6cd5\u7cbe\u5ea6\u3001\u53ef\u7528\u6027\u3001\u4e00\u81f4\u6027\u548c\u5206\u533a\u5bb9\u5fcd\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002\u9700\u8981\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u6784\u5efa\u4e00\u4e2a\u5b9e\u7528\u7684\u5206\u5e03\u5f0f\u9650\u6d41\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u9009\u62e9Redis\u5185\u5b58\u7f13\u5b58\u6570\u636e\u5e93\u53ca\u5176Sorted Set\u6570\u636e\u7ed3\u6784\uff0c\u63d0\u4f9bO(log(N))\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u64cd\u4f5c\uff1b2. \u91c7\u7528Rolling Window\u4f5c\u4e3a\u9650\u6d41\u7b97\u6cd5\uff0c\u5e76\u4e0eToken Bucket\u548cFixed Window\u7b97\u6cd5\u8fdb\u884c\u5bf9\u6bd4\uff1b3. \u4f7f\u7528\u670d\u52a1\u5668\u7aefLua\u811a\u672c\u5c06\u6e05\u7406\u3001\u8ba1\u6570\u548c\u63d2\u5165\u64cd\u4f5c\u6346\u7ed1\u4e3a\u539f\u5b50\u64cd\u4f5c\uff1b4. \u63d0\u51fa\u4e09\u5c42\u67b6\u6784\u7ba1\u7406\u9650\u6d41\u89c4\u5219\u7684\u5b58\u50a8\u548c\u66f4\u65b0\uff1b5. \u901a\u8fc7\u811a\u672c\u54c8\u5e0c\u5b9e\u73b0\u89c4\u5219\u53d8\u66f4\u65e0\u9700\u4fee\u6539\u7f13\u5b58\u811a\u672c\uff1b6. \u5728Redis Cluster\u4e0a\u90e8\u7f72\uff0c\u901a\u8fc7\u6570\u636e\u5206\u7247\u548c\u590d\u5236\u63d0\u4f9b\u53ef\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "1. \u91cf\u5316\u4e86Rolling Window\u7b97\u6cd5\u5728\u51c6\u786e\u6027\u548c\u5185\u5b58\u6210\u672c\u65b9\u9762\u7684\u6743\u8861\uff1b2. \u901a\u8fc7Lua\u811a\u672c\u539f\u5b50\u64cd\u4f5c\u6d88\u9664\u4e86\u5e76\u53d1\u73af\u5883\u4e2d\u7684\u7ade\u6001\u6761\u4ef6\uff1b3. \u4e09\u5c42\u67b6\u6784\u652f\u6301\u7075\u6d3b\u7684\u89c4\u5219\u7ba1\u7406\uff1b4. Redis Cluster\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u53ef\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff1b5. \u63a5\u53d7CAP\u5b9a\u7406\u4e2d\u7684AP\uff08\u53ef\u7528\u6027\u548c\u5206\u533a\u5bb9\u5fcd\u6027\uff09\u4f5c\u4e3a\u5b9e\u9645\u5de5\u7a0b\u6743\u8861\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u5206\u5e03\u5f0f\u9650\u6d41\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7Redis Sorted Set\u548cRolling Window\u7b97\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u91c7\u7528\u539f\u5b50\u64cd\u4f5c\u548c\u4e09\u5c42\u67b6\u6784\u8bbe\u8ba1\uff0c\u5e76\u5728Redis Cluster\u4e0a\u5b9e\u73b0\u9ad8\u53ef\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5206\u5e03\u5f0f\u9650\u6d41\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.11966", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.11966", "abs": "https://arxiv.org/abs/2602.11966", "authors": ["Jiahong Bi", "Lars Sch\u00fctze", "Jeronimo Castrillon"], "title": "MING: An Automated CNN-to-Edge MLIR HLS framework", "comment": null, "summary": "Driven by the increasing demand for low-latency and real-time processing, machine learning applications are steadily migrating toward edge computing platforms, where Field-Programmable Gate Arrays (FPGAs) are widely adopted for their energy efficiency compared to CPUs and GPUs. To generate high-performance and low-power FPGA designs, several frameworks built upon High Level Synthesis (HLS) vendor tools have been proposed, among which MLIR-based frameworks are gaining significant traction due to their extensibility and ease of use. However, existing state-of-the-art frameworks often overlook the stringent resource constraints of edge devices. To address this limitation, we propose MING, an Multi-Level Intermediate Representation (MLIR)-based framework that abstracts and automates the HLS design process. Within this framework, we adopt a streaming architecture with carefully managed buffers, specifically designed to handle resource constraints while ensuring low-latency. In comparison with recent frameworks, our approach achieves on average 15x speedup for standard Convolutional Neural Network (CNN) kernels with up to four layers, and up to 200x for single-layer kernels. For kernels with larger input sizes, MING is capable of generating efficient designs that respect hardware resource constraints, whereas state-of-the-art frameworks struggle to meet.", "AI": {"tldr": "MING\u662f\u4e00\u4e2a\u57fa\u4e8eMLIR\u7684FPGA HLS\u6846\u67b6\uff0c\u9488\u5bf9\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u7ea6\u675f\u4f18\u5316\uff0c\u901a\u8fc7\u6d41\u5f0f\u67b6\u6784\u548c\u7f13\u51b2\u7ba1\u7406\uff0c\u76f8\u6bd4\u73b0\u6709\u6846\u67b6\u5728CNN\u5185\u6838\u4e0a\u5b9e\u73b015-200\u500d\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u5e94\u7528\u5411\u8fb9\u7f18\u8ba1\u7b97\u8fc1\u79fb\uff0cFPGA\u56e0\u5176\u80fd\u6548\u4f18\u52bf\u88ab\u5e7f\u6cdb\u91c7\u7528\u3002\u73b0\u6709\u57fa\u4e8eMLIR\u7684HLS\u6846\u67b6\u5f80\u5f80\u5ffd\u89c6\u8fb9\u7f18\u8bbe\u5907\u7684\u4e25\u683c\u8d44\u6e90\u7ea6\u675f\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faMING\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8eMLIR\u7684\u62bd\u8c61\u548c\u81ea\u52a8\u5316HLS\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u91c7\u7528\u6d41\u5f0f\u67b6\u6784\u914d\u5408\u7cbe\u5fc3\u7ba1\u7406\u7684\u7f13\u51b2\u533a\uff0c\u4e13\u95e8\u9488\u5bf9\u8d44\u6e90\u7ea6\u675f\u4f18\u5316\u5e76\u786e\u4fdd\u4f4e\u5ef6\u8fdf\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6846\u67b6\uff0cMING\u5728\u6807\u51c6CNN\u5185\u6838\uff08\u6700\u591a\u56db\u5c42\uff09\u4e0a\u5e73\u5747\u5b9e\u73b015\u500d\u52a0\u901f\uff0c\u5355\u5c42\u5185\u6838\u53ef\u8fbe200\u500d\u52a0\u901f\u3002\u5bf9\u4e8e\u5927\u8f93\u5165\u5c3a\u5bf8\u7684\u5185\u6838\uff0cMING\u80fd\u751f\u6210\u6ee1\u8db3\u786c\u4ef6\u8d44\u6e90\u7ea6\u675f\u7684\u9ad8\u6548\u8bbe\u8ba1\uff0c\u800c\u73b0\u6709\u6846\u67b6\u96be\u4ee5\u5b9e\u73b0\u3002", "conclusion": "MING\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8fb9\u7f18\u8bbe\u5907\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684FPGA HLS\u8bbe\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u6d41\u5f0f\u67b6\u6784\u548c\u7f13\u51b2\u7ba1\u7406\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u3002"}}
{"id": "2602.11998", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2602.11998", "abs": "https://arxiv.org/abs/2602.11998", "authors": ["Ramakant kumar"], "title": "An Auction-Based Mechanism for Optimal Task Allocation and Resource Aware Containerization", "comment": null, "summary": "Distributed computing has enabled cooperation between multiple computing devices for the simultaneous execution of resource-hungry tasks. Such execution also plays a pivotal role in the parallel execution of numerous tasks in the Internet of Things (IoT) environment. Leveraging the computing resources of multiple devices, the offloading and processing of computationintensive tasks can be carried out more efficiently. However, managing resources and optimizing costs remain challenging for successfully executing tasks in cloud-based containerization for IoT. This paper proposes AUC-RAC, an auction-based mechanism for efficient offloading of computation tasks among multiple local servers in the context of IoT devices. The approach leverages the concept of Docker swarm, which connects multiple local servers in the form of Manager Node (MN) and Worker Nodes (WNs). It uses Docker containerization to execute tasks simultaneously. In this system, IoT devices send tasks to the MN, which then sends the task details to all its WNs to participate in the auction-based bidding process. The auctionbased bidding process optimizes the allocation of computation tasks among multiple systems, considering their resource sufficiency. The experimental analysis establishes that the approach offers improved offloading and computation-intensive services for IoT devices by enabling cooperation between local servers.", "AI": {"tldr": "\u63d0\u51faAUC-RAC\u62cd\u5356\u673a\u5236\uff0c\u901a\u8fc7Docker Swarm\u8fde\u63a5\u591a\u4e2a\u672c\u5730\u670d\u52a1\u5668\uff0c\u4f18\u5316IoT\u8bbe\u5907\u8ba1\u7b97\u4efb\u52a1\u5378\u8f7d\uff0c\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u6548\u7387", "motivation": "\u5728IoT\u73af\u5883\u4e2d\uff0c\u5206\u5e03\u5f0f\u8ba1\u7b97\u9700\u8981\u591a\u4e2a\u8bbe\u5907\u534f\u540c\u6267\u884c\u8d44\u6e90\u5bc6\u96c6\u578b\u4efb\u52a1\uff0c\u4f46\u4e91\u5bb9\u5668\u5316\u73af\u5883\u4e2d\u7684\u8d44\u6e90\u7ba1\u7406\u548c\u6210\u672c\u4f18\u5316\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027", "method": "\u63d0\u51faAUC-RAC\u62cd\u5356\u673a\u5236\uff0c\u57fa\u4e8eDocker Swarm\u67b6\u6784\u8fde\u63a5Manager Node\u548cWorker Nodes\uff0c\u4f7f\u7528Docker\u5bb9\u5668\u5316\u6280\u672f\uff0c\u901a\u8fc7\u62cd\u5356\u7ade\u4ef7\u8fc7\u7a0b\u4f18\u5316\u8ba1\u7b97\u4efb\u52a1\u5206\u914d", "result": "\u5b9e\u9a8c\u5206\u6790\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4fc3\u8fdb\u672c\u5730\u670d\u52a1\u5668\u4e4b\u95f4\u7684\u534f\u4f5c\uff0c\u4e3aIoT\u8bbe\u5907\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u4efb\u52a1\u5378\u8f7d\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u670d\u52a1", "conclusion": "AUC-RAC\u62cd\u5356\u673a\u5236\u80fd\u591f\u6709\u6548\u4f18\u5316IoT\u73af\u5883\u4e2d\u8ba1\u7b97\u4efb\u52a1\u7684\u5206\u914d\uff0c\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u6548\u7387\uff0c\u6539\u5584\u8ba1\u7b97\u5bc6\u96c6\u578b\u670d\u52a1\u7684\u6027\u80fd"}}
{"id": "2602.12070", "categories": ["cs.DC", "math.PR"], "pdf": "https://arxiv.org/pdf/2602.12070", "abs": "https://arxiv.org/abs/2602.12070", "authors": ["Zixi Cai", "Kuowen Chen", "Shengquan Du", "Tsvi Kopelowitz", "Seth Pettie", "Ben Plosk"], "title": "Contention Resolution, With and Without a Global Clock", "comment": null, "summary": "In the Contention Resolution problem $n$ parties each wish to have exclusive use of a shared resource for one unit of time. The problem has been studied since the early 1970s, under a variety of assumptions on feedback given to the parties, how the parties wake up, knowledge of $n$, and so on. The most consistent assumption is that parties do not have access to a global clock, only their local time since wake-up. This is surprising because the assumption of a global clock is both technologically realistic and algorithmically interesting. It enriches the problem, and opens the door to entirely new techniques. Our primary results are: [1] We design a new Contention Resolution protocol that guarantees latency $$O\\left(\\left(n\\log\\log n\\log^{(3)} n\\log^{(4)} n\\cdots \\log^{(\\log^* n)} n\\right)\\cdot 2^{\\log^* n}\\right) \\le n(\\log\\log n)^{1+o(1)}$$ in expectation and with high probability. This already establishes at least a roughly $\\log n$ complexity gap between randomized protocols in GlobalClock and LocalClock. [2] Prior analyses of randomized ContentionResolution protocols in LocalClock guaranteed a certain latency with high probability, i.e., with probability $1-1/\\text{poly}(n)$. We observe that it is just as natural to measure expected latency, and prove a $\\log n$-factor complexity gap between the two objectives for memoryless protocols. The In-Expectation complexity is $\u0398(n \\log n/\\log\\log n)$ whereas the With-High-Probability latency is $\u0398(n\\log^2 n/\\log\\log n)$. Three of these four upper and lower bounds are new. [3] Given the complexity separation above, one would naturally want a ContentionResolution protocol that is optimal under both the In-Expectation and With-High-Probability metrics. This is impossible! It is even impossible to achieve In-Expectation latency $o(n\\log^2 n/(\\log\\log n)^2)$ and With-High-Probability latency $n\\log^{O(1)} n$ simultaneously.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u5168\u5c40\u65f6\u949f\u7684\u7ade\u4e89\u89e3\u51b3\u534f\u8bae\uff0c\u8bbe\u8ba1\u4e86\u65b0\u7684\u534f\u8bae\u83b7\u5f97\u66f4\u4f4e\u7684\u5ef6\u8fdf\uff0c\u8bc1\u660e\u4e86\u968f\u673a\u534f\u8bae\u5728\u5168\u5c40\u65f6\u949f\u548c\u672c\u5730\u65f6\u949f\u4e0b\u7684\u590d\u6742\u5ea6\u5dee\u8ddd\uff0c\u4ee5\u53ca\u671f\u671b\u5ef6\u8fdf\u548c\u9ad8\u6982\u7387\u5ef6\u8fdf\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u7ade\u4e89\u89e3\u51b3\u662f\u5206\u5e03\u5f0f\u8ba1\u7b97\u4e2d\u7684\u7ecf\u5178\u95ee\u9898\uff0c\u4f20\u7edf\u7814\u7a76\u5047\u8bbe\u53c2\u4e0e\u8005\u53ea\u80fd\u8bbf\u95ee\u672c\u5730\u65f6\u949f\u3002\u672c\u6587\u5f15\u5165\u5168\u5c40\u65f6\u949f\u5047\u8bbe\uff0c\u8fd9\u5728\u6280\u672f\u4e0a\u66f4\u73b0\u5b9e\u4e14\u7b97\u6cd5\u4e0a\u66f4\u6709\u8da3\uff0c\u80fd\u4e30\u5bcc\u95ee\u9898\u5e76\u5f00\u542f\u65b0\u6280\u672f\u3002", "method": "\u8bbe\u8ba1\u4e86\u65b0\u7684\u7ade\u4e89\u89e3\u51b3\u534f\u8bae\uff0c\u5206\u6790\u4e86\u968f\u673a\u534f\u8bae\u5728\u5168\u5c40\u65f6\u949f\u548c\u672c\u5730\u65f6\u949f\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u7814\u7a76\u4e86\u671f\u671b\u5ef6\u8fdf\u548c\u9ad8\u6982\u7387\u5ef6\u8fdf\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u63d0\u4f9b\u4e86\u65b0\u7684\u4e0a\u754c\u548c\u4e0b\u754c\u5206\u6790\u3002", "result": "1. \u8bbe\u8ba1\u4e86\u65b0\u534f\u8bae\uff0c\u5ef6\u8fdf\u4e3aO(n(log log n)^{1+o(1)})\uff0c\u5efa\u7acb\u4e86\u5168\u5c40\u65f6\u949f\u548c\u672c\u5730\u65f6\u949f\u4e0b\u968f\u673a\u534f\u8bae\u7684\u590d\u6742\u5ea6\u5dee\u8ddd\uff1b2. \u8bc1\u660e\u4e86\u65e0\u8bb0\u5fc6\u534f\u8bae\u7684\u671f\u671b\u5ef6\u8fdf\u548c\u9ad8\u6982\u7387\u5ef6\u8fdf\u4e4b\u95f4\u5b58\u5728log n\u56e0\u5b50\u5dee\u8ddd\uff1b3. \u8bc1\u660e\u4e86\u4e0d\u53ef\u80fd\u540c\u65f6\u83b7\u5f97\u6700\u4f18\u7684\u671f\u671b\u5ef6\u8fdf\u548c\u9ad8\u6982\u7387\u5ef6\u8fdf\u3002", "conclusion": "\u5168\u5c40\u65f6\u949f\u5047\u8bbe\u663e\u8457\u6539\u53d8\u4e86\u7ade\u4e89\u89e3\u51b3\u95ee\u9898\u7684\u590d\u6742\u5ea6\u683c\u5c40\uff0c\u5e26\u6765\u4e86\u65b0\u7684\u7b97\u6cd5\u53ef\u80fd\u6027\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u671f\u671b\u5ef6\u8fdf\u548c\u9ad8\u6982\u7387\u5ef6\u8fdf\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\uff0c\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.12151", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.12151", "abs": "https://arxiv.org/abs/2602.12151", "authors": ["Youhe Jiang", "Fangcheng Fu", "Taiyi Wang", "Guoliang He", "Eiko Yoneki"], "title": "OServe: Accelerating LLM Serving via Spatial-Temporal Workload Orchestration", "comment": null, "summary": "Serving Large Language Models (LLMs) can benefit immensely from parallelizing both the model and input requests across multiple devices, but incoming workloads exhibit substantial spatial and temporal heterogeneity. Spatially, workloads comprise heterogeneous requests with varying compute and memory demands. Temporally, workload composition varies over time. Nevertheless, existing systems typically assume spatially uniform and temporally stable workloads, employing a homogeneous, static model deployment. This mismatch between the assumption and real-world spatial-temporal heterogeneity results in suboptimal performance. We present OServe, an LLM serving system with heterogeneous and flexible model deployment that addresses both spatial and temporal heterogeneity. First, OServe introduces a novel workload-aware scheduling algorithm that optimizes heterogeneous model deployments according to real-time workload characteristics. Second, OServe proposes an efficient workload-adaptive switching method that migrates model deployments in response to predicted workload changes. Experiments on real-world traces show that OServe improves performance by up to 2$\\times$ (average: 1.5$\\times$) compared to state-of-the-art serving systems.", "AI": {"tldr": "OServe\u662f\u4e00\u4e2a\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7684\u5f02\u6784\u7075\u6d3b\u90e8\u7f72\u7cfb\u7edf\uff0c\u901a\u8fc7\u611f\u77e5\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8c03\u5ea6\u7b97\u6cd5\u548c\u81ea\u9002\u5e94\u5207\u6362\u673a\u5236\uff0c\u89e3\u51b3\u670d\u52a1\u4e2d\u7684\u65f6\u7a7a\u5f02\u6784\u6027\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u7cfb\u7edf\u6027\u80fd\u63d0\u5347\u6700\u9ad82\u500d\u3002", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u7cfb\u7edf\u901a\u5e38\u5047\u8bbe\u5de5\u4f5c\u8d1f\u8f7d\u5728\u7a7a\u95f4\u4e0a\u5747\u5300\u3001\u65f6\u95f4\u4e0a\u7a33\u5b9a\uff0c\u91c7\u7528\u540c\u8d28\u5316\u9759\u6001\u6a21\u578b\u90e8\u7f72\u3002\u4f46\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u5b58\u5728\u663e\u8457\u7684\u7a7a\u95f4\u5f02\u6784\u6027\uff08\u4e0d\u540c\u8bf7\u6c42\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u4e0d\u540c\uff09\u548c\u65f6\u95f4\u5f02\u6784\u6027\uff08\u5de5\u4f5c\u8d1f\u8f7d\u7ec4\u6210\u968f\u65f6\u95f4\u53d8\u5316\uff09\uff0c\u8fd9\u79cd\u5047\u8bbe\u4e0e\u73b0\u5b9e\u4e0d\u5339\u914d\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "1. \u5f15\u5165\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u7b97\u6cd5\uff0c\u6839\u636e\u5b9e\u65f6\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\u4f18\u5316\u5f02\u6784\u6a21\u578b\u90e8\u7f72\uff1b2. \u63d0\u51fa\u9ad8\u6548\u7684\u5de5\u4f5c\u8d1f\u8f7d\u81ea\u9002\u5e94\u5207\u6362\u65b9\u6cd5\uff0c\u6839\u636e\u9884\u6d4b\u7684\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\u8fc1\u79fb\u6a21\u578b\u90e8\u7f72\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u8ddf\u8e2a\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOServe\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u670d\u52a1\u7cfb\u7edf\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe2\u500d\uff08\u5e73\u57471.5\u500d\uff09\u3002", "conclusion": "OServe\u901a\u8fc7\u5f02\u6784\u7075\u6d3b\u7684\u6a21\u578b\u90e8\u7f72\u6709\u6548\u89e3\u51b3\u4e86LLM\u670d\u52a1\u4e2d\u7684\u65f6\u7a7a\u5f02\u6784\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u670d\u52a1\u6027\u80fd\uff0c\u4e3a\u5904\u7406\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u7684\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
