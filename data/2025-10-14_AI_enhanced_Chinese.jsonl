{"id": "2510.09847", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.09847", "abs": "https://arxiv.org/abs/2510.09847", "authors": ["Said Muhammad", "Lahlou Laaziz", "Nadjia Kara", "Phat Tan Nguyen", "Timothy Murphy"], "title": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling", "comment": "Accepted and presented at the 13th IEEE International Conference on\n  Intelligent Mobile Computing 2025 (IMC), CISOSE 2025 in Tucson, Arizona, USA.\n  This is the author's accepted manuscript (AAM). The final published version\n  will appear in the IEEE conference proceedings", "summary": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability.", "AI": {"tldr": "\u63d0\u51faTHEAS\u7b97\u6cd5\u7528\u4e8e\u52a8\u6001\u8c03\u6574\u8d44\u6e90\u7ea7\u522b\uff0c\u5728\u5f02\u6784\u7cfb\u7edf\u4e2d\u5e73\u8861\u6027\u80fd\u4e0e\u529f\u8017\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u6ce2\u52a8\u5927\u7684\u573a\u666f\u3002", "motivation": "\u5728\u5f02\u6784\u7cfb\u7edf\u4e2d\uff0c\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\u5206\u5e03\u4e0d\u5747\uff0c\u9700\u8981\u52a8\u6001\u8d44\u6e90\u8c03\u6574\u6765\u63d0\u9ad8\u80fd\u6548\u540c\u65f6\u4fdd\u6301\u5fc5\u8981\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u90e8\u7f72THEAS\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u8d44\u6e90\u7ea7\u522b\u6765\u5e73\u8861\u6027\u80fd\u548c\u529f\u8017\uff0c\u5e76\u4e0eCFS\u3001EAS\u3001HeteroSched\u548cUtility-Based Scheduling\u7b49\u8c03\u5ea6\u6280\u672f\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "result": "THEAS\u7b97\u6cd5\u5728\u9002\u5e94\u6027\u3001\u6838\u5fc3\u9009\u62e9\u6807\u51c6\u3001\u6027\u80fd\u6269\u5c55\u3001\u7f13\u5b58\u611f\u77e5\u3001\u5f00\u9500\u548c\u5b9e\u65f6\u9002\u7528\u6027\u7b49\u65b9\u9762\u4e0e\u5176\u4ed6\u8c03\u5ea6\u65b9\u6848\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "THEAS\u7b97\u6cd5\u80fd\u591f\u5728\u5f02\u6784\u7cfb\u7edf\u4e2d\u6709\u6548\u5e73\u8861\u6027\u80fd\u548c\u529f\u8017\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u5b9e\u65f6\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.09851", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.09851", "abs": "https://arxiv.org/abs/2510.09851", "authors": ["Haci Ismail Aslan", "Syed Muhammad Mahmudul Haque", "Joel Witzke", "Odej Kao"], "title": "QONNECT: A QoS-Aware Orchestration System for Distributed Kubernetes Clusters", "comment": "Accepted at the International Conference on Service-Oriented\n  Computing (ICSOC) 2025", "summary": "Modern applications increasingly span across cloud, fog, and edge\nenvironments, demanding orchestration systems that can adapt to diverse\ndeployment contexts while meeting Quality-of-Service (QoS) requirements.\nStandard Kubernetes schedulers do not account for user-defined objectives such\nas energy efficiency, cost optimization, and global performance, often leaving\noperators to make manual, cluster-by-cluster placement decisions. To address\nthis need, we present QONNECT, a vendor-agnostic orchestration framework that\nenables declarative, QoS-driven application deployment across heterogeneous\nKubernetes and K3s clusters. QONNECT introduces a distributed architecture\ncomposed of a central Knowledge Base, Raft-replicated Resource Lead Agents, and\nlightweight Resource Agents in each cluster. Through a minimal YAML-based\ninterface, users specify high-level QoS goals, which the system translates into\nconcrete placement and migration actions. Our implementation is evaluated on a\nfederated testbed of up to nine cloud-fog-edge clusters using the Istio\nBookinfo microservice application. The system demonstrates dynamic,\npolicy-driven microservice placement, automated failover, QoS-compliant\nrescheduling, and leader re-election after node failure, all without manual\nintervention. By bridging the gap between declarative deployment models and\noperational QoS goals, QONNECT transforms the cloud-edge continuum into a\nunified, self-optimizing platform.", "AI": {"tldr": "QONNECT\u662f\u4e00\u4e2a\u4f9b\u5e94\u5546\u65e0\u5173\u7684\u7f16\u6392\u6846\u67b6\uff0c\u652f\u6301\u5728\u5f02\u6784Kubernetes\u548cK3s\u96c6\u7fa4\u4e0a\u8fdb\u884c\u58f0\u660e\u5f0f\u3001QoS\u9a71\u52a8\u7684\u5e94\u7528\u90e8\u7f72\uff0c\u89e3\u51b3\u4e86\u6807\u51c6Kubernetes\u8c03\u5ea6\u5668\u65e0\u6cd5\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49\u76ee\u6807\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u5e94\u7528\u8d8a\u6765\u8d8a\u591a\u5730\u8de8\u8d8a\u4e91\u3001\u96fe\u548c\u8fb9\u7f18\u73af\u5883\uff0c\u9700\u8981\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u90e8\u7f72\u73af\u5883\u5e76\u6ee1\u8db3QoS\u8981\u6c42\u7684\u7f16\u6392\u7cfb\u7edf\u3002\u6807\u51c6Kubernetes\u8c03\u5ea6\u5668\u65e0\u6cd5\u8003\u8651\u7528\u6237\u5b9a\u4e49\u7684\u76ee\u6807\uff0c\u5982\u80fd\u6548\u3001\u6210\u672c\u4f18\u5316\u548c\u5168\u5c40\u6027\u80fd\uff0c\u5bfc\u81f4\u64cd\u4f5c\u5458\u9700\u8981\u624b\u52a8\u8fdb\u884c\u96c6\u7fa4\u7ea7\u90e8\u7f72\u51b3\u7b56\u3002", "method": "QONNECT\u91c7\u7528\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u5305\u62ec\u4e2d\u592e\u77e5\u8bc6\u5e93\u3001Raft\u590d\u5236\u7684\u8d44\u6e90\u9886\u5bfc\u4ee3\u7406\u548c\u6bcf\u4e2a\u96c6\u7fa4\u4e2d\u7684\u8f7b\u91cf\u7ea7\u8d44\u6e90\u4ee3\u7406\u3002\u7528\u6237\u901a\u8fc7\u57fa\u4e8eYAML\u7684\u63a5\u53e3\u6307\u5b9a\u9ad8\u7ea7QoS\u76ee\u6807\uff0c\u7cfb\u7edf\u5c06\u5176\u8f6c\u6362\u4e3a\u5177\u4f53\u7684\u653e\u7f6e\u548c\u8fc1\u79fb\u64cd\u4f5c\u3002", "result": "\u5728\u5305\u542b\u591a\u8fbe\u4e5d\u4e2a\u4e91-\u96fe-\u8fb9\u7f18\u96c6\u7fa4\u7684\u8054\u5408\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u4f7f\u7528Istio Bookinfo\u5fae\u670d\u52a1\u5e94\u7528\u8fdb\u884c\u8bc4\u4f30\uff0c\u7cfb\u7edf\u5c55\u793a\u4e86\u52a8\u6001\u3001\u7b56\u7565\u9a71\u52a8\u7684\u5fae\u670d\u52a1\u653e\u7f6e\u3001\u81ea\u52a8\u6545\u969c\u8f6c\u79fb\u3001QoS\u5408\u89c4\u7684\u91cd\u8c03\u5ea6\u4ee5\u53ca\u8282\u70b9\u6545\u969c\u540e\u7684\u9886\u5bfc\u8005\u91cd\u65b0\u9009\u4e3e\uff0c\u6240\u6709\u8fd9\u4e9b\u90fd\u65e0\u9700\u624b\u52a8\u5e72\u9884\u3002", "conclusion": "\u901a\u8fc7\u5f25\u5408\u58f0\u660e\u5f0f\u90e8\u7f72\u6a21\u578b\u548c\u64cd\u4f5cQoS\u76ee\u6807\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0cQONNECT\u5c06\u4e91-\u8fb9\u7f18\u8fde\u7eed\u4f53\u8f6c\u53d8\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u81ea\u4f18\u5316\u7684\u5e73\u53f0\u3002"}}
{"id": "2510.10126", "categories": ["cs.DC", "F.2.2, I.2.7"], "pdf": "https://arxiv.org/pdf/2510.10126", "abs": "https://arxiv.org/abs/2510.10126", "authors": ["Sehar Zehra", "Hassan Jamil Syed", "Ummay Faseeha"], "title": "FedMon: Federated eBPF Monitoring for Distributed Anomaly Detection in Multi-Cluster Cloud Environments", "comment": "7 pages , 6 figures , 1 table and it is a conference paper", "summary": "Kubernetes multi-cluster deployments demand scalable and privacy-preserving\nanomaly detection. Existing eBPF-based monitors provide low-overhead system and\nnetwork visibility but are limited to single clusters, while centralized\napproaches incur bandwidth, privacy, and heterogeneity challenges. We propose\nFedMon, a federated eBPF framework that unifies kernel-level telemetry with\nfederated learning (FL) for cross-cluster anomaly detection. Lightweight eBPF\nagents capture syscalls and network events, extract local statistical and\nsequence features, and share only model updates with a global server. A hybrid\ndetection engine combining Variational Autoencoders (VAEs) with Isolation\nForests enables both temporal pattern modeling and outlier detection. Deployed\nacross three Kubernetes clusters, FedMon achieves 94% precision, 91% recall,\nand an F1-score of 0.92, while cutting bandwidth usage by 60% relative to\ncentralized baselines. Results demonstrate that FedMon enhances accuracy,\nscalability, and privacy, providing an effective defense for large-scale,\nmulti-tenant cloud-native environments.", "AI": {"tldr": "FedMon\u662f\u4e00\u4e2a\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u548ceBPF\u7684\u591a\u96c6\u7fa4\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7eBPF\u4ee3\u7406\u6536\u96c6\u7cfb\u7edf\u8c03\u7528\u548c\u7f51\u7edc\u4e8b\u4ef6\uff0c\u4ec5\u5171\u4eab\u6a21\u578b\u66f4\u65b0\uff0c\u5728\u4e09\u4e2aKubernetes\u96c6\u7fa4\u4e2d\u5b9e\u73b0\u4e8694%\u7cbe\u786e\u7387\u300191%\u53ec\u56de\u7387\u548c0.92 F1\u5206\u6570\uff0c\u540c\u65f6\u6bd4\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u51cf\u5c1160%\u5e26\u5bbd\u4f7f\u7528\u3002", "motivation": "\u89e3\u51b3Kubernetes\u591a\u96c6\u7fa4\u90e8\u7f72\u4e2d\u53ef\u6269\u5c55\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u5f02\u5e38\u68c0\u6d4b\u9700\u6c42\u3002\u73b0\u6709eBPF\u76d1\u63a7\u5668\u4ec5\u9650\u4e8e\u5355\u96c6\u7fa4\uff0c\u800c\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u9762\u4e34\u5e26\u5bbd\u3001\u9690\u79c1\u548c\u5f02\u6784\u6027\u6311\u6218\u3002", "method": "\u7ed3\u5408\u8054\u90a6\u5b66\u4e60(FL)\u548ceBPF\u6280\u672f\uff0c\u8f7b\u91cf\u7ea7eBPF\u4ee3\u7406\u6355\u83b7\u7cfb\u7edf\u8c03\u7528\u548c\u7f51\u7edc\u4e8b\u4ef6\uff0c\u63d0\u53d6\u672c\u5730\u7edf\u8ba1\u548c\u5e8f\u5217\u7279\u5f81\uff0c\u4ec5\u4e0e\u5168\u5c40\u670d\u52a1\u5668\u5171\u4eab\u6a21\u578b\u66f4\u65b0\u3002\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u548c\u5b64\u7acb\u68ee\u6797\u7684\u6df7\u5408\u68c0\u6d4b\u5f15\u64ce\u8fdb\u884c\u65f6\u95f4\u6a21\u5f0f\u5efa\u6a21\u548c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5728\u4e09\u4e2aKubernetes\u96c6\u7fa4\u90e8\u7f72\u4e2d\uff0c\u8fbe\u523094%\u7cbe\u786e\u7387\u300191%\u53ec\u56de\u7387\u548c0.92 F1\u5206\u6570\uff0c\u76f8\u6bd4\u96c6\u4e2d\u5f0f\u57fa\u7ebf\u51cf\u5c1160%\u5e26\u5bbd\u4f7f\u7528\u3002", "conclusion": "FedMon\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u9690\u79c1\u4fdd\u62a4\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u79df\u6237\u4e91\u539f\u751f\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2510.10166", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10166", "abs": "https://arxiv.org/abs/2510.10166", "authors": ["Suhrid Gupta", "Muhammed Tawfiqul Islam", "Rajkumar Buyya"], "title": "Proactive and Reactive Autoscaling Techniques for Edge Computing", "comment": null, "summary": "Edge computing allows for the decentralization of computing resources. This\ndecentralization is achieved through implementing microservice architectures,\nwhich require low latencies to meet stringent service level agreements (SLA)\nsuch as performance, reliability, and availability metrics. While cloud\ncomputing offers the large data storage and computation resources necessary to\nhandle peak demands, a hybrid cloud and edge environment is required to ensure\nSLA compliance. Several auto-scaling algorithms have been proposed to try to\nachieve these compliance challenges, but they suffer from performance issues\nand configuration complexity. This chapter provides a brief overview of edge\ncomputing architecture, its uses, benefits, and challenges for resource\nscaling. We then introduce Service Level Agreements, and existing research on\ndevising algorithms used in edge computing environments to meet these\nagreements, along with their benefits and drawbacks.", "AI": {"tldr": "\u672c\u7ae0\u6982\u8ff0\u4e86\u8fb9\u7f18\u8ba1\u7b97\u67b6\u6784\u53ca\u5176\u5728\u6ee1\u8db3SLA\u8981\u6c42\u65b9\u9762\u7684\u6311\u6218\uff0c\u4ecb\u7ecd\u4e86\u73b0\u6709\u7684\u81ea\u52a8\u6269\u5c55\u7b97\u6cd5\u53ca\u5176\u4f18\u7f3a\u70b9\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u901a\u8fc7\u5fae\u670d\u52a1\u67b6\u6784\u5b9e\u73b0\u8ba1\u7b97\u8d44\u6e90\u53bb\u4e2d\u5fc3\u5316\uff0c\u9700\u8981\u4f4e\u5ef6\u8fdf\u6765\u6ee1\u8db3\u4e25\u683c\u7684SLA\u8981\u6c42\uff0c\u4f46\u73b0\u6709\u7684\u81ea\u52a8\u6269\u5c55\u7b97\u6cd5\u5b58\u5728\u6027\u80fd\u95ee\u9898\u548c\u914d\u7f6e\u590d\u6742\u6027\u3002", "method": "\u63d0\u4f9b\u8fb9\u7f18\u8ba1\u7b97\u67b6\u6784\u7684\u7b80\u8981\u6982\u8ff0\uff0c\u4ecb\u7ecdSLA\u6982\u5ff5\uff0c\u5e76\u7efc\u8ff0\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u7528\u4e8e\u6ee1\u8db3SLA\u7684\u7b97\u6cd5\u7814\u7a76\u3002", "result": "\u5206\u6790\u4e86\u8fb9\u7f18\u8ba1\u7b97\u5728\u8d44\u6e90\u6269\u5c55\u65b9\u9762\u7684\u4f7f\u7528\u3001\u4f18\u52bf\u548c\u6311\u6218\uff0c\u4ee5\u53ca\u73b0\u6709\u7b97\u6cd5\u5728\u6ee1\u8db3SLA\u65b9\u9762\u7684\u8868\u73b0\u3002", "conclusion": "\u6df7\u5408\u4e91\u548c\u8fb9\u7f18\u73af\u5883\u5bf9\u4e8e\u786e\u4fddSLA\u5408\u89c4\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9700\u8981\u6539\u8fdb\u73b0\u6709\u7684\u81ea\u52a8\u6269\u5c55\u7b97\u6cd5\u4ee5\u89e3\u51b3\u6027\u80fd\u548c\u914d\u7f6e\u590d\u6742\u6027\u95ee\u9898\u3002"}}
{"id": "2510.10225", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10225", "abs": "https://arxiv.org/abs/2510.10225", "authors": ["Jialin Sun", "Yuchen Hu", "Dean You", "Yushu Du", "Hui Wang", "Xinwei Fang", "Weiwei Shan", "Nan Guan", "Zhe Jiang"], "title": "ISAAC: Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism", "comment": null, "summary": "Functional verification is a critical bottleneck in integrated circuit\ndevelopment, with CPU verification being especially time-intensive and\nlabour-consuming. Industrial practice relies on differential testing for CPU\nverification, yet faces bottlenecks at nearly each stage of the framework\npipeline: front-end stimulus generation lacks micro-architectural awareness,\nyielding low-quality and redundant tests that impede coverage closure and miss\ncorner cases. Meanwhile, back-end simulation infrastructure, even with FPGA\nacceleration, often stalls on long-running tests and offers limited visibility,\ndelaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a\nfull-stack, Large Language Model (LLM)-aided CPU verification framework with\nFPGA parallelism, from bug categorisation and stimulus generation to simulation\ninfrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC's\nfront-end, infused with micro-architectural knowledge and historical bug\npatterns, generating highly targeted tests that rapidly achieve coverage goals\nand capture elusive corner cases. In ISAAC's back-end, we introduce a\nlightweight forward-snapshot mechanism and a decoupled co-simulation\narchitecture between the Instruction Set Simulator (ISS) and the Design Under\nTest (DUT), enabling a single ISS to drive multiple DUTs in parallel. By\neliminating long-tail test bottlenecks and exploiting FPGA parallelism, the\nsimulation throughput is significantly improved. As a demonstration, we used\nISAAC to verify a mature CPU that has undergone multiple successful tape-outs.\nResults show up to 17,536x speed-up over software RTL simulation, while\ndetecting several previously unknown bugs, two of which are reported in this\npaper.", "AI": {"tldr": "ISAAC\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684CPU\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u6fc0\u52b1\u751f\u6210\u548cFPGA\u5e76\u884c\u4eff\u771f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9a8c\u8bc1\u6548\u7387\u5e76\u53d1\u73b0\u4e86\u65b0\u7684bug\u3002", "motivation": "\u4f20\u7edfCPU\u9a8c\u8bc1\u65b9\u6cd5\u5b58\u5728\u74f6\u9888\uff1a\u524d\u7aef\u6fc0\u52b1\u751f\u6210\u7f3a\u4e4f\u5fae\u67b6\u6784\u610f\u8bc6\u5bfc\u81f4\u6d4b\u8bd5\u8d28\u91cf\u4f4e\uff0c\u540e\u7aef\u4eff\u771f\u57fa\u7840\u8bbe\u65bd\u5373\u4f7f\u4f7f\u7528FPGA\u52a0\u901f\u4e5f\u9762\u4e34\u957f\u8fd0\u884c\u6d4b\u8bd5\u548c\u6709\u9650\u53ef\u89c1\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faISAAC\u5168\u6808\u6846\u67b6\uff1a\u524d\u7aef\u4f7f\u7528\u6ce8\u5165\u5fae\u67b6\u6784\u77e5\u8bc6\u548c\u5386\u53f2bug\u6a21\u5f0f\u7684\u591a\u4ee3\u7406\u6fc0\u52b1\u5f15\u64ce\uff1b\u540e\u7aef\u91c7\u7528\u8f7b\u91cf\u7ea7\u524d\u5411\u5feb\u7167\u673a\u5236\u548c\u89e3\u8026\u534f\u540c\u4eff\u771f\u67b6\u6784\uff0c\u5b9e\u73b0\u5355\u4e2aISS\u9a71\u52a8\u591a\u4e2aDUT\u5e76\u884c\u8fd0\u884c\u3002", "result": "\u5728\u6210\u719fCPU\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u8f6f\u4ef6RTL\u4eff\u771f\u83b7\u5f97\u9ad8\u8fbe17,536\u500d\u7684\u52a0\u901f\uff0c\u5e76\u68c0\u6d4b\u5230\u591a\u4e2a\u5148\u524d\u672a\u77e5\u7684bug\u3002", "conclusion": "ISAAC\u901a\u8fc7\u6d88\u9664\u957f\u5c3e\u6d4b\u8bd5\u74f6\u9888\u548c\u5229\u7528FPGA\u5e76\u884c\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eff\u771f\u541e\u5410\u91cf\uff0c\u4e3aCPU\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09726", "categories": ["cs.PL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09726", "abs": "https://arxiv.org/abs/2510.09726", "authors": ["Tilman Hinnerichs", "Reuben Gardos Reid", "Jaap de Jong", "Bart Swinkels", "Pamela Wochner", "Nicolae Filat", "Tudor Magurescu", "Issa Hanou", "Sebastijan Dumancic"], "title": "Herb.jl: A Unifying Program Synthesis Library", "comment": null, "summary": "Program synthesis -- the automatic generation of code given a specification\n-- is one of the most fundamental tasks in artificial intelligence (AI) and\nmany programmers' dream. Numerous synthesizers have been developed to tackle\nprogram synthesis, manifesting different ideas to approach the exponentially\ngrowing program space. While numerous smart program synthesis tools exist,\nreusing and remixing previously developed methods is tedious and\ntime-consuming. We propose Herb.jl, a unifying program synthesis library\nwritten in the Julia programming language, to address these issues. Since\ncurrent methods rely on similar building blocks, we aim to modularize the\nunderlying synthesis algorithm into communicating and fully extendable\nsub-compartments, allowing for straightforward reapplication of these modules.\nTo demonstrate the benefits of using Herb.jl, we show three common use cases:\n1. how to implement a simple problem and grammar, and how to solve it, 2. how\nto implement a previously developed synthesizer with just a few lines of code,\nand 3. how to run a synthesizer against a benchmark.", "AI": {"tldr": "Herb.jl\u662f\u4e00\u4e2a\u7528Julia\u7f16\u7a0b\u8bed\u8a00\u7f16\u5199\u7684\u7edf\u4e00\u7a0b\u5e8f\u5408\u6210\u5e93\uff0c\u65e8\u5728\u6a21\u5757\u5316\u5408\u6210\u7b97\u6cd5\uff0c\u4fbf\u4e8e\u91cd\u7528\u548c\u6269\u5c55\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u7a0b\u5e8f\u5408\u6210\u5de5\u5177\u867d\u7136\u4f17\u591a\uff0c\u4f46\u91cd\u7528\u548c\u6df7\u5408\u5148\u524d\u5f00\u53d1\u7684\u65b9\u6cd5\u65e2\u7e41\u7410\u53c8\u8017\u65f6\u3002", "method": "\u5c06\u5e95\u5c42\u5408\u6210\u7b97\u6cd5\u6a21\u5757\u5316\u4e3a\u53ef\u901a\u4fe1\u548c\u5b8c\u5168\u53ef\u6269\u5c55\u7684\u5b50\u7ec4\u4ef6\uff0c\u5141\u8bb8\u76f4\u63a5\u91cd\u7528\u8fd9\u4e9b\u6a21\u5757\u3002", "result": "\u5c55\u793a\u4e86\u4e09\u4e2a\u5e38\u89c1\u7528\u4f8b\uff1a\u5b9e\u73b0\u7b80\u5355\u95ee\u9898\u548c\u8bed\u6cd5\u5e76\u89e3\u51b3\u3001\u7528\u5c11\u91cf\u4ee3\u7801\u5b9e\u73b0\u5148\u524d\u5f00\u53d1\u7684\u5408\u6210\u5668\u3001\u4ee5\u53ca\u9488\u5bf9\u57fa\u51c6\u8fd0\u884c\u5408\u6210\u5668\u3002", "conclusion": "Herb.jl\u901a\u8fc7\u6a21\u5757\u5316\u65b9\u6cd5\u89e3\u51b3\u4e86\u7a0b\u5e8f\u5408\u6210\u4e2d\u65b9\u6cd5\u91cd\u7528\u56f0\u96be\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\u3002"}}
{"id": "2510.10302", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10302", "abs": "https://arxiv.org/abs/2510.10302", "authors": ["Liangkun Chen", "Zijian Wen", "Tian Wu", "Xiaoxi Zhang", "Chuan Wu"], "title": "SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference", "comment": null, "summary": "The Mixture-of-Experts (MoE) architecture has been widely adopted in large\nlanguage models (LLMs) to reduce computation cost through model sparsity.\nEmploying speculative decoding (SD) can further accelerate MoE inference by\ndrafting multiple tokens per step and verifying them in parallel. However,\ncombining MoE with SD inflates GPU memory and aggravates CPU-GPU bandwidth\ncontention during multi-token verification. Existing MoE offloading systems are\nSD-agnostic and do not address this bottleneck. We present SP-MoE, the first\nSD-aware expert-offloading and compute-communication pipelining framework.\nSP-MoE introduces: (1) speculative expert prefetching that exploits structural\ncorrespondence between the draft and target models to prefetch likely experts\nahead of verification; (2) a cutoff-layer policy that bounds per-layer prefetch\ndepth based on empirical profiles and an analytical latency model, guaranteeing\njust-in-time availability without overfetch; and (3) a pipelined runtime with\nasynchronous prefetch threads and batched I/O to hide loading latency.\nExtensive experiments demonstrate that SP-MoE achieves a 1.07-3.5 times TPOT\nspeedup over state-of-the-art methods across diverse datasets, environments,\nand MoE-based models.", "AI": {"tldr": "SP-MoE\u662f\u4e00\u4e2a\u7ed3\u5408\u63a8\u6d4b\u89e3\u7801\u548c\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u7684\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u6d4b\u6027\u4e13\u5bb6\u9884\u53d6\u3001\u622a\u6b62\u5c42\u7b56\u7565\u548c\u6d41\u6c34\u7ebf\u8fd0\u884c\u65f6\u6765\u4f18\u5316GPU\u5185\u5b58\u548c\u5e26\u5bbd\u4f7f\u7528\u3002", "motivation": "MoE\u67b6\u6784\u4e0e\u63a8\u6d4b\u89e3\u7801\u7ed3\u5408\u4f1a\u52a0\u5267GPU\u5185\u5b58\u538b\u529b\u548cCPU-GPU\u5e26\u5bbd\u7ade\u4e89\uff0c\u73b0\u6709MoE\u5378\u8f7d\u7cfb\u7edf\u672a\u80fd\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u63a8\u6d4b\u6027\u4e13\u5bb6\u9884\u53d6\u3001\u57fa\u4e8e\u7ecf\u9a8c\u914d\u7f6e\u548c\u5206\u6790\u5ef6\u8fdf\u6a21\u578b\u7684\u622a\u6b62\u5c42\u7b56\u7565\uff0c\u4ee5\u53ca\u5f02\u6b65\u9884\u53d6\u7ebf\u7a0b\u548c\u6279\u5904\u7406I/O\u7684\u6d41\u6c34\u7ebf\u8fd0\u884c\u65f6\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u3001\u73af\u5883\u548cMoE\u6a21\u578b\u4e0a\uff0cSP-MoE\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e861.07-3.5\u500d\u7684TPOT\u52a0\u901f\u3002", "conclusion": "SP-MoE\u662f\u9996\u4e2aSD\u611f\u77e5\u7684\u4e13\u5bb6\u5378\u8f7d\u548c\u8ba1\u7b97\u901a\u4fe1\u6d41\u6c34\u7ebf\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86MoE\u4e0e\u63a8\u6d4b\u89e3\u7801\u7ed3\u5408\u65f6\u7684\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2510.10623", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10623", "abs": "https://arxiv.org/abs/2510.10623", "authors": ["Ahmed J. Abdelmaksoud", "Cristian Sestito", "Shiwei Wang", "Themis Prodromakis"], "title": "ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration", "comment": null, "summary": "Transformers are at the core of modern AI nowadays. They rely heavily on\nmatrix multiplication and require efficient acceleration due to their\nsubstantial memory and computational requirements. Quantization plays a vital\nrole in reducing memory usage, and can be exploited for computations by\ndesigning reconfigurable architectures that enhance matrix multiplication by\ndynamically adjusting the precision. This paper proposes ADiP, a novel\nadaptive-precision systolic array architecture designed for efficient matrix\nmultiplication acceleration.The proposed architecture consists of NxN\nadaptive-precision processing elements (PEs) and shared accumulators. ADiP\nsupports multiple computation modes, including symmetric single-matrix\nmultiplication as well as asymmetric multi-matrix multiplication with a shared\ninput matrix, thereby improving data-reuse and PE utilization. In addition,\nADiP maximizes the computational density by adapting to different precisions,\nsuch as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed\nfor ADiP architecture, including latency and throughput for versatile\narchitecture configurations. A comprehensive hardware design space exploration\nis demonstrated using 22nm commercial technology, achieving up to a 4x higher\ncomputational throughput. Furthermore, ADiP is evaluated on different\ntransformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,\ndelivering latency improvement up to 53.6%, and energy improvement up to 24.4%\nfor BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a\npeak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,\n8bitx4bit, and 8bitx2bit operations, respectively.", "AI": {"tldr": "ADiP\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u9002\u5e94\u7cbe\u5ea6\u8109\u52a8\u9635\u5217\u67b6\u6784\uff0c\u4e13\u95e8\u4e3a\u9ad8\u6548\u77e9\u9635\u4e58\u6cd5\u52a0\u901f\u800c\u8bbe\u8ba1\uff0c\u652f\u6301\u591a\u79cd\u8ba1\u7b97\u6a21\u5f0f\u548c\u7cbe\u5ea6\u914d\u7f6e\uff0c\u5728Transformer\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5ef6\u8fdf\u548c\u80fd\u6548\u6539\u8fdb\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u73b0\u4ee3AI\u4e2d\u5360\u636e\u6838\u5fc3\u5730\u4f4d\uff0c\u4f46\u5176\u77e9\u9635\u4e58\u6cd5\u8fd0\u7b97\u5177\u6709\u5de8\u5927\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u3002\u91cf\u5316\u6280\u672f\u53ef\u4ee5\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\uff0c\u4f46\u9700\u8981\u53ef\u91cd\u6784\u67b6\u6784\u6765\u52a8\u6001\u8c03\u6574\u7cbe\u5ea6\u4ee5\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "method": "\u63d0\u51faADiP\u67b6\u6784\uff0c\u5305\u542bNxN\u81ea\u9002\u5e94\u7cbe\u5ea6\u5904\u7406\u5355\u5143\u548c\u5171\u4eab\u7d2f\u52a0\u5668\uff0c\u652f\u6301\u5bf9\u79f0\u5355\u77e9\u9635\u4e58\u6cd5\u548c\u975e\u5bf9\u79f0\u591a\u77e9\u9635\u4e58\u6cd5\uff0c\u80fd\u591f\u9002\u5e948bit\u00d78bit\u30018bit\u00d74bit\u30018bit\u00d72bit\u7b49\u4e0d\u540c\u7cbe\u5ea6\u914d\u7f6e\u3002", "result": "\u572822nm\u5546\u7528\u6280\u672f\u4e0b\uff0cADiP\u5b9e\u73b0\u4e86\u9ad8\u8fbe4\u500d\u7684\u8ba1\u7b97\u541e\u5410\u91cf\u63d0\u5347\u3002\u5728GPT-2 Medium\u3001BERT Large\u548cBitNet-1.58B\u6a21\u578b\u4e0a\uff0c\u5ef6\u8fdf\u6539\u8fdb\u6700\u9ad8\u8fbe53.6%\uff0cBitNet-1.58B MHA\u5de5\u4f5c\u8d1f\u8f7d\u7684\u80fd\u6548\u6539\u8fdb\u6700\u9ad8\u8fbe24.4%\u300264\u00d764\u89c4\u6a21\u4e0b\u5cf0\u503c\u541e\u5410\u91cf\u5206\u522b\u4e3a8.192 TOPS\u300116.384 TOPS\u548c32.768 TOPS\u3002", "conclusion": "ADiP\u67b6\u6784\u901a\u8fc7\u81ea\u9002\u5e94\u7cbe\u5ea6\u548c\u591a\u79cd\u8ba1\u7b97\u6a21\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u77e9\u9635\u4e58\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u548c\u80fd\u6548\uff0c\u7279\u522b\u9002\u5408\u73b0\u4ee3Transformer\u6a21\u578b\u7684\u52a0\u901f\u9700\u6c42\u3002"}}
{"id": "2510.09932", "categories": ["cs.PL", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.09932", "abs": "https://arxiv.org/abs/2510.09932", "authors": ["Devansh Jain", "Akash Pardeshi", "Marco Frigo", "Krut Patel", "Kaustubh Khulbe", "Jai Arora", "Charith Mendis"], "title": "ACT: Automatically Generating Compiler Backends from Tensor Accelerator ISA Descriptions", "comment": null, "summary": "Tensor compilers play a key role in enabling high-performance implementations\nof deep learning workloads. These compilers rely on existing CPU and GPU code\ngeneration backends to generate device-specific code. Recently, many tensor\naccelerators (neural processing units) have been proposed to further accelerate\nthese workloads. Compared to commodity hardware, however, most of the proposed\ntensor accelerators do not have compiler backends with code generation support.\nMoreover, the accelerator designs are subject to fast iteration cycles, making\nit difficult to manually develop compiler backends similar to commodity\nhardware platforms. Therefore, to increase adoption and enable faster software\ndevelopment cycles for novel tensor accelerator designs, we need to make the\ncompiler backend construction process more agile.\n  To address this gap, we introduce ACT, a compiler backend generator that\nautomatically generates compiler backends for tensor accelerators, given just\nthe instruction set architecture (ISA) descriptions. We first formally specify\nthe compiler backend generation problem that introduces a novel specification\nfor describing tensor accelerator ISAs. Next, we design ACT such that it\nsupports user-programmable memories and complex parameterized instructions that\nare prevalent in tensor accelerators. ACT uses a novel parameterized equality\nsaturation-based instruction selection phase and a constraint programming-based\nmemory allocation phase. We prove that compiler backends generated by ACT are\nsound and complete. Finally, we generate compiler backends for three\naccelerator platforms from industry and academia, and show that they match or\noutperform code written using hand-optimized kernel libraries while maintaining\nlow compilation overheads.", "AI": {"tldr": "ACT\u662f\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u5f20\u91cf\u52a0\u901f\u5668\u7f16\u8bd1\u5668\u540e\u7aef\u7684\u5de5\u5177\uff0c\u4ec5\u9700\u6307\u4ee4\u96c6\u67b6\u6784\u63cf\u8ff0\u5373\u53ef\u751f\u6210\u9ad8\u6027\u80fd\u4ee3\u7801\uff0c\u89e3\u51b3\u4e86\u65b0\u578b\u5f20\u91cf\u52a0\u901f\u5668\u7f3a\u4e4f\u7f16\u8bd1\u5668\u652f\u6301\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5f20\u91cf\u52a0\u901f\u5668\u7f3a\u4e4f\u7f16\u8bd1\u5668\u540e\u7aef\u652f\u6301\uff0c\u4e14\u8bbe\u8ba1\u8fed\u4ee3\u901f\u5ea6\u5feb\uff0c\u624b\u52a8\u5f00\u53d1\u7f16\u8bd1\u5668\u540e\u7aef\u6210\u672c\u9ad8\u3001\u5468\u671f\u957f\uff0c\u963b\u788d\u4e86\u65b0\u578b\u52a0\u901f\u5668\u7684\u91c7\u7528\u548c\u8f6f\u4ef6\u5f00\u53d1\u3002", "method": "ACT\u901a\u8fc7\u5f62\u5f0f\u5316\u5b9a\u4e49\u7f16\u8bd1\u5668\u540e\u7aef\u751f\u6210\u95ee\u9898\uff0c\u652f\u6301\u7528\u6237\u53ef\u7f16\u7a0b\u5185\u5b58\u548c\u590d\u6742\u53c2\u6570\u5316\u6307\u4ee4\uff0c\u91c7\u7528\u53c2\u6570\u5316\u7b49\u5f0f\u9971\u548c\u6307\u4ee4\u9009\u62e9\u548c\u57fa\u4e8e\u7ea6\u675f\u89c4\u5212\u7684\u5185\u5b58\u5206\u914d\u65b9\u6cd5\u3002", "result": "\u4e3a\u4e09\u4e2a\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u7684\u52a0\u901f\u5668\u5e73\u53f0\u751f\u6210\u4e86\u7f16\u8bd1\u5668\u540e\u7aef\uff0c\u751f\u6210\u7684\u4ee3\u7801\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u624b\u5de5\u4f18\u5316\u5185\u6838\u5e93\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u7f16\u8bd1\u5f00\u9500\u3002", "conclusion": "ACT\u80fd\u591f\u81ea\u52a8\u3001\u9ad8\u6548\u5730\u751f\u6210\u5f20\u91cf\u52a0\u901f\u5668\u7684\u7f16\u8bd1\u5668\u540e\u7aef\uff0c\u8bc1\u660e\u5176\u751f\u6210\u7684\u540e\u7aef\u662f\u6b63\u786e\u4e14\u5b8c\u6574\u7684\uff0c\u6709\u52a9\u4e8e\u52a0\u901f\u65b0\u578b\u5f20\u91cf\u52a0\u901f\u5668\u7684\u8f6f\u4ef6\u5f00\u53d1\u751f\u6001\u5efa\u8bbe\u3002"}}
{"id": "2510.10380", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10380", "abs": "https://arxiv.org/abs/2510.10380", "authors": ["Shouxu Lin", "Zimeng Pan", "Yuhang Yao", "Haeyoung Noh", "Pei Zhang", "Carlee Joe-Wong"], "title": "FLAMMABLE: A Multi-Model Federated Learning Framework with Multi-Model Engagement and Adaptive Batch Sizes", "comment": null, "summary": "Multi-Model Federated Learning (MMFL) is an emerging direction in Federated\nLearning (FL) where multiple models are trained in parallel, generally on\nvarious datasets. Optimizing the models' accuracies and training times in the\nMMFL setting requires adapting to data and system heterogeneity across clients\nas in single-model FL; these challenges are amplified in the MMFL setting due\nto additional heterogeneity across models. Neither existing solutions nor\nna\\\"ive extensions of single-model FL frameworks efficiently address these\nchallenges. To bridge this gap, we propose FLAMMABLE, a comprehensive MMFL\ntraining framework. FLAMMABLE optimizes model training by intelligently\nadapting client batch sizes while engaging them to train multiple carefully\nchosen models, depending on their system capabilities, in each training round.\nTo evaluate FLAMMABLE, we develop the first benchmark platform for the MMFL\nsetting, which may enable future reproducible MMFL research. Extensive\nevaluations on multiple datasets and models show that FLAMMABLE boosts the MMFL\ntime-to-accuracy performance by 1.1$\\sim$10.0$\\times$ while improving the final\nmodel accuracy by 1.3$\\sim$5.4\\% compared to several known baselines.", "AI": {"tldr": "FLAMMABLE\u662f\u4e00\u4e2a\u591a\u6a21\u578b\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u8c03\u6574\u5ba2\u6237\u7aef\u6279\u91cf\u5927\u5c0f\u548c\u9009\u62e9\u5408\u9002\u6a21\u578b\u6765\u4f18\u5316\u8bad\u7ec3\u6548\u7387\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u65f6\u95f4\u6548\u7387\u548c\u6a21\u578b\u7cbe\u5ea6\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u591a\u6a21\u578b\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u6570\u636e\u5f02\u6784\u3001\u7cfb\u7edf\u5f02\u6784\u4ee5\u53ca\u6a21\u578b\u95f4\u5f02\u6784\u7684\u6311\u6218\uff0c\u73b0\u6709\u5355\u6a21\u578b\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u8fd9\u4e9b\u590d\u6742\u95ee\u9898\uff0c\u9700\u8981\u4e13\u95e8\u7684\u591a\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u63d0\u51faFLAMMABLE\u6846\u67b6\uff0c\u667a\u80fd\u8c03\u6574\u5ba2\u6237\u7aef\u6279\u91cf\u5927\u5c0f\uff0c\u6839\u636e\u5ba2\u6237\u7aef\u7cfb\u7edf\u80fd\u529b\u9009\u62e9\u5408\u9002\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5f00\u53d1\u4e86\u9996\u4e2a\u591a\u6a21\u578b\u8054\u90a6\u5b66\u4e60\u57fa\u51c6\u5e73\u53f0\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cFLAMMABLE\u5c06\u65f6\u95f4\u6548\u7387\u63d0\u5347\u4e861.1-10.0\u500d\uff0c\u6700\u7ec8\u6a21\u578b\u7cbe\u5ea6\u63d0\u9ad8\u4e861.3-5.4%\u3002", "conclusion": "FLAMMABLE\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u578b\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5f02\u6784\u6027\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u53ef\u590d\u73b0\u7684\u591a\u6a21\u578b\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.10676", "categories": ["cs.AR", "cs.CL", "cs.RO", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10676", "abs": "https://arxiv.org/abs/2510.10676", "authors": ["Mukul Lokhande", "Tanushree Dewangan", "Mohd Sharik Mansoori", "Tejas Chaudhari", "Akarsh J.", "Damayanti Lokhande", "Adam Teman", "Santosh Kumar Vishvakarma"], "title": "Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation", "comment": null, "summary": "This paper introduces Bhasha-Rupantarika, a light and efficient multilingual\ntranslation system tailored through algorithm-hardware codesign for\nresource-limited settings. The method investigates model deployment at\nsub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental\nresults indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in\ninference speed, which correlates with an increased throughput of 66 tokens/s\n(improvement by 4.8x). This underscores the importance of ultra-low precision\nquantization for real-time deployment in IoT devices using FPGA accelerators,\nachieving performance on par with expectations. Our evaluation covers\nbidirectional translation between Indian and international languages,\nshowcasing its adaptability in low-resource linguistic contexts. The FPGA\ndeployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,\nresulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x\nenhancement compared to HPTA. Overall, the evaluation provides a viable\nsolution based on quantisation-aware translation along with hardware efficiency\nsuitable for deployable multilingual AI systems. The entire codes\n[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for\nreproducibility are publicly available, facilitating rapid integration and\nfurther development by researchers.", "AI": {"tldr": "Bhasha-Rupantarika\u662f\u4e00\u4e2a\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u9ad8\u6548\u591a\u8bed\u8a00\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u4e13\u95e8\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4f18\u5316\u3002\u8be5\u7cfb\u7edf\u91c7\u7528\u4e9a\u5b57\u8282\u7cbe\u5ea6\u91cf\u5316\uff08FP8/INT8/INT4/FP4\uff09\uff0c\u5728FPGA\u4e0a\u90e8\u7f72\u5b9e\u73b0\u4e864.1\u500d\u6a21\u578b\u538b\u7f29\u548c4.2\u500d\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\uff0c\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u5b9e\u73b0\u5b9e\u65f6\u90e8\u7f72\u548c\u4f4e\u529f\u8017\u8fd0\u884c\u3002", "method": "\u91c7\u7528\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7814\u7a76\u4e9a\u5b57\u8282\u7cbe\u5ea6\u91cf\u5316\uff08FP8\u3001INT8\u3001INT4\u3001FP4\uff09\uff0c\u5728FPGA\u52a0\u901f\u5668\u4e0a\u90e8\u7f72\u591a\u8bed\u8a00\u7ffb\u8bd1\u6a21\u578b\u3002", "result": "FP4\u91cf\u5316\u5b9e\u73b04.1\u500d\u6a21\u578b\u538b\u7f29\u548c4.2\u500d\u63a8\u7406\u52a0\u901f\uff0c\u541e\u5410\u91cf\u63d0\u5347\u81f366 tokens/s\uff084.8\u500d\u63d0\u5347\uff09\u3002FPGA\u90e8\u7f72\u51cf\u5c111.96\u500dLUTs\u548c1.65\u500dFFs\uff0c\u76f8\u6bd4OPU\u548cHPTA\u5206\u522b\u63d0\u53472.2\u500d\u548c4.6\u500d\u541e\u5410\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u8d85\u4f4e\u7cbe\u5ea6\u91cf\u5316\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u53ef\u90e8\u7f72\u7684\u591a\u8bed\u8a00AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u4e8e\u91cf\u5316\u611f\u77e5\u7ffb\u8bd1\u548c\u786c\u4ef6\u6548\u7387\u7684\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10015", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.10015", "abs": "https://arxiv.org/abs/2510.10015", "authors": ["Jinhua Wu", "Yuting Wang", "Liukun Yu", "Linglong Meng"], "title": "End-to-end Compositional Verification of Program Safety through Verified and Verifying Compilation", "comment": null, "summary": "Program safety (i.e., absence of undefined behaviors) is critical for correct\noperation of computer systems. It is usually verified at the source level\n(e.g., by separation logics) and preserved to the target by verified compilers\n(e.g., CompCert), thereby achieving end-to-end verification of safety. However,\nmodern safe programming languages like Rust pose new problems in achieving\nend-to-end safety. Because not all functionalities can be implemented in the\nsafe language, mixing safe and unsafe modules is needed. Therefore, verified\ncompilation must preserve a modular notion of safety which can be composed at\nthe target level. Furthermore, certain classes of errors (e.g., memory errors)\nare automatically excluded by verifying compilation (e.g., borrow checking) for\nmodules written in safe languages. As a result, verified compilation needs to\ncooperate with verifying compilation to ensure end-to-end safety.\n  To address the above problems, we propose a modular and generic definition of\nsafety called open safety based on program semantics described as open labeled\ntransition systems (LTS). Open safety is composable at the boundary of modules\nand can be modularly preserved by verified compositional compilation. Those\nproperties enable separate verification of safety for heterogeneous modules and\ncomposition of the safety results at the target level. Open safety can be\ngeneralized to partial safety (i.e., only a certain class of errors can occur).\nBy this we formalized the correctness of verifying compilation as derivation of\ntotal safety from partial safety. We demonstrate how our framework can combine\nverified and verifying compilation by developing a verified compiler for an\nownership language (called Owlang) inspired by Rust. We evaluate our approach\non the compositional safety verification using a hash map implemented by Owlang\nand C.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f00\u653e\u6807\u7b7e\u8f6c\u79fb\u7cfb\u7edf\u7684\u5f00\u653e\u5b89\u5168\u6027\u6982\u5ff5\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u4ee3\u5b89\u5168\u7f16\u7a0b\u8bed\u8a00\uff08\u5982Rust\uff09\u4e2d\u6df7\u5408\u5b89\u5168\u4e0e\u4e0d\u5b89\u5168\u6a21\u5757\u65f6\u7684\u7aef\u5230\u7aef\u5b89\u5168\u9a8c\u8bc1\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u5b89\u5168\u7f16\u7a0b\u8bed\u8a00\u9700\u8981\u6df7\u5408\u5b89\u5168\u548c\u4e0d\u5b89\u5168\u6a21\u5757\u6765\u5b9e\u73b0\u6240\u6709\u529f\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u6a21\u5757\u5316\u7684\u5b89\u5168\u6027\u5b9a\u4e49\uff0c\u80fd\u591f\u5728\u76ee\u6807\u7ea7\u522b\u7ec4\u5408\uff0c\u5e76\u4e0e\u9a8c\u8bc1\u7f16\u8bd1\u534f\u4f5c\u786e\u4fdd\u7aef\u5230\u7aef\u5b89\u5168\u3002", "method": "\u57fa\u4e8e\u5f00\u653e\u6807\u7b7e\u8f6c\u79fb\u7cfb\u7edf\u5b9a\u4e49\u5f00\u653e\u5b89\u5168\u6027\u6982\u5ff5\uff0c\u652f\u6301\u6a21\u5757\u5316\u7ec4\u5408\u548c\u9a8c\u8bc1\u7ec4\u5408\u7f16\u8bd1\uff0c\u53ef\u63a8\u5e7f\u5230\u90e8\u5206\u5b89\u5168\u6027\uff0c\u5e76\u5c06\u9a8c\u8bc1\u7f16\u8bd1\u7684\u6b63\u786e\u6027\u5f62\u5f0f\u5316\u4e3a\u4ece\u90e8\u5206\u5b89\u5168\u6027\u63a8\u5bfc\u51fa\u5b8c\u5168\u5b89\u5168\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u6240\u6709\u6743\u8bed\u8a00Owlang\u7684\u9a8c\u8bc1\u7f16\u8bd1\u5668\uff0c\u5e76\u5728\u7531Owlang\u548cC\u5b9e\u73b0\u7684\u54c8\u5e0c\u6620\u5c04\u4e0a\u8bc4\u4f30\u4e86\u7ec4\u5408\u5b89\u5168\u9a8c\u8bc1\u65b9\u6cd5\u3002", "conclusion": "\u5f00\u653e\u5b89\u5168\u6027\u6846\u67b6\u80fd\u591f\u7ed3\u5408\u9a8c\u8bc1\u7f16\u8bd1\u548c\u9a8c\u8bc1\u7f16\u8bd1\uff0c\u652f\u6301\u5f02\u6784\u6a21\u5757\u7684\u5355\u72ec\u5b89\u5168\u9a8c\u8bc1\u548c\u5728\u76ee\u6807\u7ea7\u522b\u7684\u5b89\u5168\u7ed3\u679c\u7ec4\u5408\u3002"}}
{"id": "2510.10620", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10620", "abs": "https://arxiv.org/abs/2510.10620", "authors": ["Chenyu Jiang", "Zhenkun Cai", "Ye Tian", "Zhen Jia", "Yida Wang", "Chuan Wu"], "title": "DCP: Addressing Input Dynamism In Long-Context Training via Dynamic Context Parallelism", "comment": "16 pages, 22 figures", "summary": "Context parallelism has emerged as a key technique to support long-context\ntraining, a growing trend in generative AI for modern large models. However,\nexisting context parallel methods rely on static parallelization configurations\nthat overlook the dynamic nature of training data, specifically, the\nvariability in sequence lengths and token relationships (i.e., attention\npatterns) across samples. As a result, these methods often suffer from\nunnecessary communication overhead and imbalanced computation. In this paper,\nwe present DCP, a dynamic context parallel training framework that introduces\nfine-grained blockwise partitioning of both data and computation. By enabling\nflexible mapping of data and computation blocks to devices, DCP can adapt to\nvarying sequence characteristics, effectively reducing communication and\nimproving memory and computation balance. Micro-benchmarks demonstrate that DCP\naccelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under\nsparse attention patterns. Additionally, we observe up to 0.94x~1.16x\nend-to-end training speed-up for causal masks, and 1.00x~1.46x for sparse\nmasks.", "AI": {"tldr": "DCP\u662f\u4e00\u4e2a\u52a8\u6001\u4e0a\u4e0b\u6587\u5e76\u884c\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u6570\u636e\u5757\u5212\u5206\u548c\u8ba1\u7b97\u6620\u5c04\u6765\u9002\u5e94\u4e0d\u540c\u5e8f\u5217\u7279\u5f81\uff0c\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u5e76\u6539\u5584\u8ba1\u7b97\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u4e0a\u4e0b\u6587\u5e76\u884c\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u5e76\u884c\u914d\u7f6e\uff0c\u65e0\u6cd5\u9002\u5e94\u8bad\u7ec3\u6570\u636e\u4e2d\u5e8f\u5217\u957f\u5ea6\u548c\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u52a8\u6001\u53d8\u5316\uff0c\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u901a\u4fe1\u5f00\u9500\u548c\u8ba1\u7b97\u4e0d\u5e73\u8861\u3002", "method": "\u5f15\u5165\u7ec6\u7c92\u5ea6\u7684\u6570\u636e\u5757\u5212\u5206\u548c\u8ba1\u7b97\u5206\u533a\uff0c\u5b9e\u73b0\u6570\u636e\u548c\u8ba1\u7b97\u5757\u5230\u8bbe\u5907\u7684\u7075\u6d3b\u6620\u5c04\uff0c\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u5e8f\u5217\u7279\u5f81\u3002", "result": "\u5728\u56e0\u679c\u63a9\u7801\u4e0b\u6ce8\u610f\u529b\u8ba1\u7b97\u52a0\u901f1.19x~2.45x\uff0c\u7a00\u758f\u6ce8\u610f\u529b\u6a21\u5f0f\u4e0b\u52a0\u901f2.15x~3.77x\uff1b\u7aef\u5230\u7aef\u8bad\u7ec3\u5728\u56e0\u679c\u63a9\u7801\u4e0b\u52a0\u901f0.94x~1.16x\uff0c\u7a00\u758f\u63a9\u7801\u4e0b\u52a0\u901f1.00x~1.46x\u3002", "conclusion": "DCP\u901a\u8fc7\u52a8\u6001\u4e0a\u4e0b\u6587\u5e76\u884c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u901a\u4fe1\u548c\u8ba1\u7b97\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2510.10872", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.10872", "abs": "https://arxiv.org/abs/2510.10872", "authors": ["Sumukh Pinge", "Ashkan Moradifirouzabadi", "Keming Fan", "Prasanna Venkatesan Ravindran", "Tanvir H. Pantha", "Po-Kai Hsu", "Zheyu Li", "Weihong Xu", "Zihan Xia", "Flavio Ponzina", "Winston Chern", "Taeyoung Song", "Priyankka Ravikumar", "Mengkun Tian", "Lance Fernandes", "Huy Tran", "Hari Jayasankar", "Hang Chen", "Chinsung Park", "Amrit Garlapati", "Kijoon Kim", "Jongho Woo", "Suhwan Lim", "Kwangsoo Kim", "Wanki Kim", "Daewon Ha", "Duygu Kuzum", "Shimeng Yu", "Sourav Dutta", "Asif Khan", "Tajana Rosing", "Mingu Kang"], "title": "FeNOMS: Enhancing Open Modification Spectral Library Search with In-Storage Processing on Ferroelectric NAND (FeNAND) Flash", "comment": null, "summary": "The rapid expansion of mass spectrometry (MS) data, now exceeding hundreds of\nterabytes, poses significant challenges for efficient, large-scale library\nsearch - a critical component for drug discovery. Traditional processors\nstruggle to handle this data volume efficiently, making in-storage computing\n(ISP) a promising alternative. This work introduces an ISP architecture\nleveraging a 3D Ferroelectric NAND (FeNAND) structure, providing significantly\nhigher density, faster speeds, and lower voltage requirements compared to\ntraditional NAND flash. Despite its superior density, the NAND structure has\nnot been widely utilized in ISP applications due to limited throughput\nassociated with row-by-row reads from serially connected cells. To overcome\nthese limitations, we integrate hyperdimensional computing (HDC), a\nbrain-inspired paradigm that enables highly parallel processing with simple\noperations and strong error tolerance. By combining HDC with the proposed\ndual-bound approximate matching (D-BAM) distance metric, tailored to the FeNAND\nstructure, we parallelize vector computations to enable efficient MS spectral\nlibrary search, achieving 43x speedup and 21x higher energy efficiency over\nstate-of-the-art 3D NAND methods, while maintaining comparable accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u94c1\u7535NAND\u548c\u8d85\u7ef4\u8ba1\u7b97\u7684\u5b58\u50a8\u5185\u8ba1\u7b97\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u8d28\u8c31\u6570\u636e\u641c\u7d22\uff0c\u76f8\u6bd4\u73b0\u67093D NAND\u65b9\u6cd5\u5b9e\u73b043\u500d\u52a0\u901f\u548c21\u500d\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u8d28\u8c31\u6570\u636e\u5feb\u901f\u589e\u957f\u81f3\u6570\u767eTB\u89c4\u6a21\uff0c\u4f20\u7edf\u5904\u7406\u5668\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u5e93\u641c\u7d22\uff0c\u5b58\u50a8\u5185\u8ba1\u7b97\u6210\u4e3a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7ed3\u54083D\u94c1\u7535NAND\u7ed3\u6784\u548c\u8d85\u7ef4\u8ba1\u7b97\uff0c\u91c7\u7528\u53cc\u8fb9\u754c\u8fd1\u4f3c\u5339\u914d\u8ddd\u79bb\u5ea6\u91cf\uff0c\u5728FeNAND\u7ed3\u6784\u4e2d\u5e76\u884c\u5316\u5411\u91cf\u8ba1\u7b97\u3002", "result": "\u5b9e\u73b0\u4e8643\u500d\u901f\u5ea6\u63d0\u5347\u548c21\u500d\u80fd\u6548\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684\u51c6\u786e\u5ea6\u3002", "conclusion": "\u8be5\u67b6\u6784\u6210\u529f\u89e3\u51b3\u4e86NAND\u7ed3\u6784\u5728\u5b58\u50a8\u5185\u8ba1\u7b97\u4e2d\u7684\u541e\u5410\u91cf\u9650\u5236\uff0c\u4e3a\u5927\u89c4\u6a21\u8d28\u8c31\u6570\u636e\u641c\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.10209", "categories": ["cs.PL", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.10209", "abs": "https://arxiv.org/abs/2510.10209", "authors": ["Massinissa Merouani", "Afif Boudaoud", "Riyadh Baghdadi"], "title": "LOOPerSet: A Large-Scale Dataset for Data-Driven Polyhedral Compiler Optimization", "comment": null, "summary": "The advancement of machine learning for compiler optimization, particularly\nwithin the polyhedral model, is constrained by the scarcity of large-scale,\npublic performance datasets. This data bottleneck forces researchers to\nundertake costly data generation campaigns, slowing down innovation and\nhindering reproducible research learned code optimization. To address this gap,\nwe introduce LOOPerSet, a new public dataset containing 28 million labeled data\npoints derived from 220,000 unique, synthetically generated polyhedral\nprograms. Each data point maps a program and a complex sequence of\nsemantics-preserving transformations (such as fusion, skewing, tiling, and\nparallelism)to a ground truth performance measurement (execution time). The\nscale and diversity of LOOPerSet make it a valuable resource for training and\nevaluating learned cost models, benchmarking new model architectures, and\nexploring the frontiers of automated polyhedral scheduling. The dataset is\nreleased under a permissive license to foster reproducible research and lower\nthe barrier to entry for data-driven compiler optimization.", "AI": {"tldr": "LOOPerSet\u662f\u4e00\u4e2a\u5305\u542b2800\u4e07\u4e2a\u6807\u8bb0\u6570\u636e\u70b9\u7684\u65b0\u516c\u5171\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u7f16\u8bd1\u5668\u4f18\u5316\u7814\u7a76\uff0c\u7279\u522b\u662f\u591a\u9762\u4f53\u6a21\u578b\u4e2d\u7684\u6027\u80fd\u9884\u6d4b\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u7f16\u8bd1\u5668\u4f18\u5316\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u5927\u89c4\u6a21\u516c\u5f00\u6027\u80fd\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u9650\u5236\uff0c\u8fd9\u8feb\u4f7f\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u6602\u8d35\u7684\u6570\u636e\u751f\u6210\uff0c\u51cf\u7f13\u4e86\u521b\u65b0\u5e76\u963b\u788d\u4e86\u53ef\u91cd\u73b0\u7814\u7a76\u3002", "method": "\u4ece22\u4e07\u4e2a\u72ec\u7279\u7684\u5408\u6210\u751f\u6210\u7684\u591a\u9762\u4f53\u7a0b\u5e8f\u4e2d\u63d0\u53d62800\u4e07\u4e2a\u6570\u636e\u70b9\uff0c\u6bcf\u4e2a\u6570\u636e\u70b9\u5c06\u7a0b\u5e8f\u548c\u590d\u6742\u7684\u8bed\u4e49\u4fdd\u6301\u8f6c\u6362\u5e8f\u5217\uff08\u5982\u878d\u5408\u3001\u503e\u659c\u3001\u5e73\u94fa\u548c\u5e76\u884c\u5316\uff09\u6620\u5c04\u5230\u771f\u5b9e\u6027\u80fd\u6d4b\u91cf\uff08\u6267\u884c\u65f6\u95f4\uff09\u3002", "result": "\u521b\u5efa\u4e86LOOPerSet\u6570\u636e\u96c6\uff0c\u5176\u89c4\u6a21\u548c\u591a\u6837\u6027\u4f7f\u5176\u6210\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30\u5b66\u4e60\u6210\u672c\u6a21\u578b\u3001\u57fa\u51c6\u6d4b\u8bd5\u65b0\u6a21\u578b\u67b6\u6784\u4ee5\u53ca\u63a2\u7d22\u81ea\u52a8\u5316\u591a\u9762\u4f53\u8c03\u5ea6\u524d\u6cbf\u7684\u5b9d\u8d35\u8d44\u6e90\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u5728\u5bbd\u677e\u8bb8\u53ef\u4e0b\u53d1\u5e03\uff0c\u65e8\u5728\u4fc3\u8fdb\u53ef\u91cd\u73b0\u7814\u7a76\u5e76\u964d\u4f4e\u6570\u636e\u9a71\u52a8\u7f16\u8bd1\u5668\u4f18\u5316\u7684\u5165\u95e8\u95e8\u69db\u3002"}}
{"id": "2510.10747", "categories": ["cs.DC", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.10747", "abs": "https://arxiv.org/abs/2510.10747", "authors": ["Chirag Shetty", "Sarthak Chakraborty", "Hubertus Franke", "Larisa Shwartz", "Chandra Narayanaswami", "Indranil Gupta", "Saurabh Jha"], "title": "CPU-Limits kill Performance: Time to rethink Resource Control", "comment": "Vision Paper accepted to SoCC 2025", "summary": "Research in compute resource management for cloud-native applications is\ndominated by the problem of setting optimal CPU limits -- a fundamental OS\nmechanism that strictly restricts a container's CPU usage to its specified\nCPU-limits . Rightsizing and autoscaling works have innovated on\nallocation/scaling policies assuming the ubiquity and necessity of CPU-limits .\nWe question this. Practical experiences of cloud users indicate that CPU-limits\nharms application performance and costs more than it helps. These observations\nare in contradiction to the conventional wisdom presented in both academic\nresearch and industry best practices. We argue that this indiscriminate\nadoption of CPU-limits is driven by erroneous beliefs that CPU-limits is\nessential for operational and safety purposes. We provide empirical evidence\nmaking a case for eschewing CPU-limits completely from latency-sensitive\napplications. This prompts a fundamental rethinking of auto-scaling and billing\nparadigms and opens new research avenues. Finally, we highlight specific\nscenarios where CPU-limits can be beneficial if used in a well-reasoned way\n(e.g. background jobs).", "AI": {"tldr": "\u672c\u6587\u8d28\u7591CPU\u9650\u5236\u7684\u5fc5\u8981\u6027\uff0c\u8ba4\u4e3a\u5bf9\u4e8e\u5ef6\u8fdf\u654f\u611f\u5e94\u7528\u5e94\u5b8c\u5168\u907f\u514d\u4f7f\u7528CPU\u9650\u5236\uff0c\u8fd9\u9700\u8981\u91cd\u65b0\u601d\u8003\u81ea\u52a8\u6269\u5c55\u548c\u8ba1\u8d39\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3aCPU\u9650\u5236\u5bf9\u4e91\u539f\u751f\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645\u7ecf\u9a8c\u8868\u660eCPU\u9650\u5236\u4f1a\u635f\u5bb3\u5e94\u7528\u6027\u80fd\u5e76\u589e\u52a0\u6210\u672c\uff0c\u8fd9\u4e0e\u5b66\u672f\u7814\u7a76\u548c\u884c\u4e1a\u6700\u4f73\u5b9e\u8df5\u76f8\u77db\u76fe\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u8bc1\u636e\u5206\u6790CPU\u9650\u5236\u5bf9\u5ef6\u8fdf\u654f\u611f\u5e94\u7528\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u8ba8\u7279\u5b9a\u573a\u666f\u4e0bCPU\u9650\u5236\u7684\u5408\u7406\u4f7f\u7528\u65b9\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0CPU\u9650\u5236\u5bf9\u5ef6\u8fdf\u654f\u611f\u5e94\u7528\u5f0a\u5927\u4e8e\u5229\uff0c\u5e94\u8be5\u5b8c\u5168\u907f\u514d\u4f7f\u7528\uff0c\u4f46\u80cc\u666f\u4f5c\u4e1a\u7b49\u7279\u5b9a\u573a\u666f\u4ecd\u53ef\u53d7\u76ca\u3002", "conclusion": "\u9700\u8981\u4ece\u6839\u672c\u4e0a\u91cd\u65b0\u601d\u8003\u81ea\u52a8\u6269\u5c55\u548c\u8ba1\u8d39\u8303\u5f0f\uff0c\u4e3aCPU\u9650\u5236\u7684\u4f7f\u7528\u63d0\u4f9b\u66f4\u7ec6\u81f4\u7684\u6307\u5bfc\u539f\u5219\u3002"}}
{"id": "2510.11192", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11192", "abs": "https://arxiv.org/abs/2510.11192", "authors": ["Jo\u00e3o Paulo Cardoso de Lima", "Marc Dietrich", "Jeronimo Castrillon", "Asif Ali Khan"], "title": "Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs", "comment": "8 pages, to appear in IEEE Cross-disciplinary Conference on\n  Memory-Centric Computing (CCMCC)", "summary": "Structured sparsity enables deploying large language models (LLMs) on\nresource-constrained systems. Approaches like dense-to-sparse fine-tuning are\nparticularly compelling, achieving remarkable structured sparsity by reducing\nthe model size by over 6.7x, while still maintaining acceptable accuracy.\nDespite this reduction, LLM inference, especially the decode stage being\ninherently memory-bound, is extremely expensive on conventional Von-Neumann\narchitectures. Compute-in-memory (CIM) architectures mitigate this by\nperforming computations directly in memory, and when paired with sparse LLMs,\nenable storing and computing the entire model in memory, eliminating the data\nmovement on the off-chip bus and improving efficiency. Nonetheless, naively\nmapping sparse matrices onto CIM arrays leads to poor array utilization and\ndiminished computational efficiency. In this paper, we present an automated\nframework with novel mapping and scheduling strategies to accelerate sparse LLM\ninference on CIM accelerators. By exploiting block-diagonal sparsity, our\napproach improves CIM array utilization by over 50%, achieving more than 4x\nreduction in both memory footprint and the number of required floating-point\noperations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u6620\u5c04\u548c\u8c03\u5ea6\u7b56\u7565\u6765\u52a0\u901f\u7a00\u758f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5185\u5b58\u8ba1\u7b97\u52a0\u901f\u5668\u4e0a\u7684\u63a8\u7406\uff0c\u5229\u7528\u5757\u5bf9\u89d2\u7a00\u758f\u6027\u63d0\u9ad8\u9635\u5217\u5229\u7528\u738750%\u4ee5\u4e0a\u3002", "motivation": "\u89e3\u51b3\u7a00\u758f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f20\u7edf\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u4e0a\u63a8\u7406\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u5728\u5185\u5b58\u8ba1\u7b97\u67b6\u6784\u4e0a\u76f4\u63a5\u6620\u5c04\u7a00\u758f\u77e9\u9635\u5bfc\u81f4\u7684\u9635\u5217\u5229\u7528\u7387\u4f4e\u548c\u8ba1\u7b97\u6548\u7387\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u91c7\u7528\u65b0\u9896\u7684\u6620\u5c04\u548c\u8c03\u5ea6\u7b56\u7565\uff0c\u5229\u7528\u5757\u5bf9\u89d2\u7a00\u758f\u6027\u6765\u4f18\u5316\u7a00\u758f\u77e9\u9635\u5728\u5185\u5b58\u8ba1\u7b97\u9635\u5217\u4e0a\u7684\u5e03\u5c40\u3002", "result": "\u5185\u5b58\u8ba1\u7b97\u9635\u5217\u5229\u7528\u7387\u63d0\u9ad8\u8d85\u8fc750%\uff0c\u5185\u5b58\u5360\u7528\u548c\u6d6e\u70b9\u8fd0\u7b97\u6b21\u6570\u5747\u51cf\u5c114\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5185\u5b58\u8ba1\u7b97\u52a0\u901f\u5668\u4e0a\u7684\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2510.10216", "categories": ["cs.PL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10216", "abs": "https://arxiv.org/abs/2510.10216", "authors": ["Zhechong Huang", "Zhao Zhang", "Ruyi Ji", "Tingxuan Xia", "Qihao Zhu", "Qinxiang Cao", "Zeyu Sun", "Yingfei Xiong"], "title": "Learning to Guarantee Type Correctness in Code Generation through Type-Guided Program Synthesis", "comment": null, "summary": "Language models have shown remarkable proficiency in code generation;\nnevertheless, ensuring type correctness remains a challenge. Although\ntraditional methods, such as constrained decoding, alleviate this problem by\nexternally rejecting untypable code, the model itself does not effectively\nlearn type reasoning internally, which ultimately limits its overall\nperformance. This paper introduces TyFlow, a novel system that internalizes\ntype reasoning within code generation to guide the model to learn the type\nsystem. The core of our approach is a novel type-guided program synthesis\nsystem that maintains an isomorphism between type derivation trees and\nsynthesis derivation trees, enabling a new code representation based on\nsynthesis decision sequences rather than traditional text-based token\nsequences. By offloading the complexity of type system learning to the\nrepresentation itself, models can redirect their computational resources toward\nhigher-level program semantics. Our evaluation shows that TyFlow not only\neliminates type errors but also significantly improves functional correctness,\nhighlighting the importance of aligning LMs with type systems internally.", "AI": {"tldr": "TyFlow\u901a\u8fc7\u5c06\u7c7b\u578b\u63a8\u7406\u5185\u5316\u5230\u4ee3\u7801\u751f\u6210\u4e2d\uff0c\u63d0\u51fa\u57fa\u4e8e\u5408\u6210\u51b3\u7b56\u5e8f\u5217\u7684\u65b0\u578b\u4ee3\u7801\u8868\u793a\u65b9\u6cd5\uff0c\u5728\u6d88\u9664\u7c7b\u578b\u9519\u8bef\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u529f\u80fd\u6b63\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982\u7ea6\u675f\u89e3\u7801\u53ea\u80fd\u5916\u90e8\u62d2\u7edd\u4e0d\u53ef\u7c7b\u578b\u5316\u7684\u4ee3\u7801\uff0c\u4f46\u6a21\u578b\u672c\u8eab\u672a\u80fd\u6709\u6548\u5b66\u4e60\u7c7b\u578b\u63a8\u7406\uff0c\u9650\u5236\u4e86\u6574\u4f53\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7c7b\u578b\u5f15\u5bfc\u7684\u7a0b\u5e8f\u5408\u6210\u7cfb\u7edf\uff0c\u4fdd\u6301\u7c7b\u578b\u63a8\u5bfc\u6811\u4e0e\u5408\u6210\u63a8\u5bfc\u6811\u4e4b\u95f4\u7684\u540c\u6784\u5173\u7cfb\uff0c\u521b\u5efa\u57fa\u4e8e\u5408\u6210\u51b3\u7b56\u5e8f\u5217\u800c\u975e\u4f20\u7edf\u6587\u672c\u6807\u8bb0\u5e8f\u5217\u7684\u4ee3\u7801\u8868\u793a\u3002", "result": "TyFlow\u4e0d\u4ec5\u6d88\u9664\u4e86\u7c7b\u578b\u9519\u8bef\uff0c\u8fd8\u663e\u8457\u63d0\u9ad8\u4e86\u529f\u80fd\u6b63\u786e\u6027\u3002", "conclusion": "\u5c06\u8bed\u8a00\u6a21\u578b\u4e0e\u7c7b\u578b\u7cfb\u7edf\u5185\u90e8\u5bf9\u9f50\u5bf9\u4e8e\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.10818", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10818", "abs": "https://arxiv.org/abs/2510.10818", "authors": ["Kevin Chalmers", "Jan B\u00e6kgaard Pedersen"], "title": "Fair Kernel-Lock-Free Claim/Release Protocol for Shared Object Access in Cooperatively Scheduled Runtimes", "comment": null, "summary": "We present the first spin-free, kernel-lock-free mutex that cooperates with\nuser-mode schedulers and is formally proven FIFO-fair and linearizable using\nCSP/FDR. Our fairness oracle and stability-based proof method are reusable\nacross coroutine runtime designs. We designed the claim/release protocol for a\nprocess-oriented language -- ProcessJ -- to manage the race for claiming shared\ninter-process communication channels. Internally, we use a lock-free queue to\npark waiting processes for gaining access to a shared object, such as exclusive\naccess to a shared channel to read from or write to. The queue ensures control\nand fairness for processes wishing to access a shared resource, as the protocol\nhandles claim requests in the order they are inserted into the queue. We\nproduce CSP models of our protocol and a mutex specification, demonstrating\nwith FDR that our protocol behaves as a locking mutex.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u65e0\u81ea\u65cb\u3001\u65e0\u5185\u6838\u9501\u7684\u4e92\u65a5\u9501\uff0c\u4e0e\u7528\u6237\u6a21\u5f0f\u8c03\u5ea6\u5668\u534f\u4f5c\uff0c\u4f7f\u7528CSP/FDR\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4e3aFIFO\u516c\u5e73\u4e14\u7ebf\u6027\u5316\u3002", "motivation": "\u4e3aProcessJ\u8bed\u8a00\u8bbe\u8ba1\u58f0\u660e/\u91ca\u653e\u534f\u8bae\uff0c\u7ba1\u7406\u5171\u4eab\u8fdb\u7a0b\u95f4\u901a\u4fe1\u901a\u9053\u7684\u7ade\u4e89\uff0c\u786e\u4fdd\u5bf9\u5171\u4eab\u8d44\u6e90\u7684\u63a7\u5236\u548c\u516c\u5e73\u8bbf\u95ee\u3002", "method": "\u4f7f\u7528\u65e0\u9501\u961f\u5217\u6765\u505c\u653e\u7b49\u5f85\u83b7\u53d6\u5171\u4eab\u5bf9\u8c61\u8bbf\u95ee\u6743\u9650\u7684\u8fdb\u7a0b\uff0c\u534f\u8bae\u6309\u7167\u961f\u5217\u63d2\u5165\u987a\u5e8f\u5904\u7406\u58f0\u660e\u8bf7\u6c42\u3002", "result": "\u6784\u5efa\u4e86\u534f\u8bae\u7684CSP\u6a21\u578b\u548c\u4e92\u65a5\u9501\u89c4\u8303\uff0c\u901a\u8fc7FDR\u9a8c\u8bc1\u534f\u8bae\u884c\u4e3a\u7b26\u5408\u9501\u5b9a\u4e92\u65a5\u9501\u7684\u8981\u6c42\u3002", "conclusion": "\u5f00\u53d1\u4e86\u53ef\u91cd\u7528\u7684\u516c\u5e73\u6027\u9884\u8a00\u673a\u548c\u57fa\u4e8e\u7a33\u5b9a\u6027\u7684\u8bc1\u660e\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u534f\u7a0b\u8fd0\u884c\u65f6\u8bbe\u8ba1\u3002"}}
{"id": "2510.10219", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.10219", "abs": "https://arxiv.org/abs/2510.10219", "authors": ["Ruihao Li", "Lizy K. John", "Neeraja J. Yadwadkar"], "title": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc", "comment": null, "summary": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively.", "AI": {"tldr": "Exgen-Malloc\u662f\u4e00\u4e2a\u4e13\u4e3a\u5355\u7ebf\u7a0b\u5e94\u7528\u8bbe\u8ba1\u7684\u5185\u5b58\u5206\u914d\u5668\uff0c\u901a\u8fc7\u7b80\u5316\u5143\u6570\u636e\u548c\u63a7\u5236\u6d41\uff0c\u5728SPEC CPU2017\u3001redis-benchmark\u548cmimalloc-bench\u6d4b\u8bd5\u4e2d\u5206\u522b\u5b9e\u73b0\u4e861.17x\u30011.10x\u548c1.93x\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u4ee5\u53ca6.2%\u30010.1%\u548c25.2%\u7684\u5185\u5b58\u8282\u7701\u3002", "motivation": "\u73b0\u4ee3\u5185\u5b58\u5206\u914d\u5668\u4e3a\u591a\u7ebf\u7a0b\u73af\u5883\u4f18\u5316\uff0c\u4f46\u590d\u6742\u7684\u5143\u6570\u636e\u548c\u63a7\u5236\u903b\u8f91\u5e26\u6765\u4e86\u663e\u8457\u5f00\u9500\u3002\u5728\u5355\u7ebf\u7a0b\u573a\u666f\u4e2d\uff0c\u8fd9\u4e9b\u5f00\u9500\u53ef\u4ee5\u907f\u514d\uff0c\u4ece\u800c\u63d0\u5347\u5206\u914d\u6548\u7387\u548c\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u3002", "method": "\u8bbe\u8ba1\u4e13\u4e3a\u5355\u7ebf\u7a0b\u7684Exgen-Malloc\uff0c\u91c7\u7528\u96c6\u4e2d\u5f0f\u5806\u3001\u5355\u4e00\u7a7a\u95f2\u5757\u5217\u8868\u3001\u5e73\u8861\u7684\u5185\u5b58\u63d0\u4ea4\u548c\u91cd\u5b9a\u4f4d\u7b56\u7565\uff0c\u5e76\u501f\u9274\u73b0\u4ee3\u591a\u7ebf\u7a0b\u5206\u914d\u5668\u7684\u8bbe\u8ba1\u539f\u5219\u3002", "result": "\u5728\u4e24\u4e2aIntel Xeon\u5e73\u53f0\u4e0a\u6d4b\u8bd5\uff0cExgen-Malloc\u5728SPEC CPU2017\u3001redis-benchmark\u548cmimalloc-bench\u4e0a\u5206\u522b\u6bd4dlmalloc\u5feb1.17x\u30011.10x\u548c1.93x\uff0c\u6bd4mimalloc\u8282\u77016.2%\u30010.1%\u548c25.2%\u5185\u5b58\u3002", "conclusion": "\u4e13\u4e3a\u5355\u7ebf\u7a0b\u5e94\u7528\u4f18\u5316\u7684\u5185\u5b58\u5206\u914d\u5668\u80fd\u591f\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u5185\u5b58\u6d88\u8017\uff0c\u8bc1\u660e\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u7b80\u5316\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.10833", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.10833", "abs": "https://arxiv.org/abs/2510.10833", "authors": ["Mehdi Zekriyapanah Gashti"], "title": "FIDRS: A Novel Framework for Integrated Distributed Reliable Systems", "comment": null, "summary": "In this paper we represent a new framework for integrated distributed and\nreliable systems. In the proposed framework we have used three parts to\nincrease Satisfaction and Performance of this framework. At first we analyze\nprevious frameworks related to integrated systems, then represent new proposed\nframework in order to improving previous framework, and we discuss its\ndifferent phases. Finally we compare the results of simulation of the new\nframework with previous ones. In FIDRS framework, the technique of\nheterogeneous distributed data base is used to improve Performance and speed in\nresponding to users and in this way we can improve dependability and\nreliability of framework simultaneously. In extraction phase of the new\nframework we have used RMSD algorithm that decreases responding time in big\ndatabase. Finally by using FDIRS framework we succeeded to increase Efficiency,\nPerformance and reliability of integrated systems and remove some of previous\nframeworks problems.", "AI": {"tldr": "\u63d0\u51faFIDRS\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6784\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u548cRMSD\u7b97\u6cd5\u63d0\u9ad8\u96c6\u6210\u7cfb\u7edf\u7684\u6027\u80fd\u3001\u6548\u7387\u548c\u53ef\u9760\u6027", "motivation": "\u6539\u8fdb\u73b0\u6709\u96c6\u6210\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u89e3\u51b3\u54cd\u5e94\u65f6\u95f4\u6162\u548c\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u7f3a\u9677", "method": "\u4f7f\u7528\u5f02\u6784\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u6280\u672f\u63d0\u9ad8\u54cd\u5e94\u901f\u5ea6\uff0c\u91c7\u7528RMSD\u7b97\u6cd5\u51cf\u5c11\u5927\u6570\u636e\u73af\u5883\u4e0b\u7684\u54cd\u5e94\u65f6\u95f4", "result": "\u65b0\u6846\u67b6\u6210\u529f\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6548\u7387\u3001\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u89e3\u51b3\u4e86\u4e4b\u524d\u6846\u67b6\u7684\u4e00\u4e9b\u95ee\u9898", "conclusion": "FIDRS\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u96c6\u6210\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6574\u4f53\u8868\u73b0\uff0c\u5728\u6027\u80fd\u548c\u53ef\u9760\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u8fdb"}}
{"id": "2510.10410", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10410", "abs": "https://arxiv.org/abs/2510.10410", "authors": ["Hui Xu"], "title": "A Trace-based Approach for Code Safety Analysis", "comment": null, "summary": "Rust is a memory-safe programming language that disallows undefined behavior.\nIts safety guarantees have been extensively examined by the community through\nempirical studies, which has led to its remarkable success. However, unsafe\ncode remains a critical concern in Rust. By reviewing the safety design of Rust\nand analyzing real-world Rust projects, this paper establishes a systematic\nframework for understanding unsafe code and undefined behavior, and summarizes\nthe soundness criteria for Rust code. It further derives actionable guidance\nfor achieving sound encapsulation.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86Rust\u8bed\u8a00\u4e2d\u7684\u4e0d\u5b89\u5168\u4ee3\u7801\u548c\u672a\u5b9a\u4e49\u884c\u4e3a\uff0c\u5efa\u7acb\u4e86\u7406\u89e3\u6846\u67b6\u5e76\u603b\u7ed3\u4e86Rust\u4ee3\u7801\u7684\u5065\u5168\u6027\u6807\u51c6\uff0c\u4e3a\u5b89\u5168\u5c01\u88c5\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u6307\u5bfc\u3002", "motivation": "Rust\u4f5c\u4e3a\u5185\u5b58\u5b89\u5168\u7f16\u7a0b\u8bed\u8a00\u867d\u7136\u7981\u6b62\u672a\u5b9a\u4e49\u884c\u4e3a\uff0c\u4f46\u4e0d\u5b89\u5168\u4ee3\u7801\u4ecd\u7136\u662f\u5173\u952e\u95ee\u9898\u3002\u901a\u8fc7\u5ba1\u67e5Rust\u7684\u5b89\u5168\u8bbe\u8ba1\u548c\u5206\u6790\u5b9e\u9645\u9879\u76ee\uff0c\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u6027\u6846\u67b6\u6765\u7406\u89e3\u4e0d\u5b89\u5168\u4ee3\u7801\u548c\u672a\u5b9a\u4e49\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u5ba1\u67e5Rust\u7684\u5b89\u5168\u8bbe\u8ba1\u5e76\u5206\u6790\u771f\u5b9e\u4e16\u754c\u7684Rust\u9879\u76ee\uff0c\u5efa\u7acb\u7406\u89e3\u4e0d\u5b89\u5168\u4ee3\u7801\u548c\u672a\u5b9a\u4e49\u884c\u4e3a\u7684\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u603b\u7ed3Rust\u4ee3\u7801\u7684\u5065\u5168\u6027\u6807\u51c6\u3002", "result": "\u5efa\u7acb\u4e86\u7406\u89e3\u4e0d\u5b89\u5168\u4ee3\u7801\u548c\u672a\u5b9a\u4e49\u884c\u4e3a\u7684\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u603b\u7ed3\u4e86Rust\u4ee3\u7801\u7684\u5065\u5168\u6027\u6807\u51c6\uff0c\u5e76\u63a8\u5bfc\u51fa\u5b9e\u73b0\u5b89\u5168\u5c01\u88c5\u7684\u53ef\u64cd\u4f5c\u6307\u5bfc\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aRust\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u7406\u89e3\u4e0d\u5b89\u5168\u4ee3\u7801\u7684\u7cfb\u7edf\u6027\u65b9\u6cd5\u548c\u5b9e\u73b0\u5b89\u5168\u5c01\u88c5\u7684\u5177\u4f53\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8Rust\u4ee3\u7801\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2510.11189", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11189", "abs": "https://arxiv.org/abs/2510.11189", "authors": ["Yangyang Wen", "Paul Townend", "Per-Olov \u00d6stberg", "Abel Souza", "Cl\u00e9ment Courageux-Sudan"], "title": "A Decentralized Microservice Scheduling Approach Using Service Mesh in Cloud-Edge Systems", "comment": "9 pages, 4 figures. Accepted at the 2025 IEEE Joint Cloud Computing\n  (JCC) track of IEEE CISOSE 2025. Conference: IEEE JCC 2025, 16th IEEE\n  International Conference on Joint Cloud Computing, Tucson, Arizona, USA, from\n  July 21 to 24, 2025", "summary": "As microservice-based systems scale across the cloud-edge continuum,\ntraditional centralized scheduling mechanisms increasingly struggle with\nlatency, coordination overhead, and fault tolerance. This paper presents a new\narchitectural direction: leveraging service mesh sidecar proxies as\ndecentralized, in-situ schedulers to enable scalable, low-latency coordination\nin large-scale, cloud-native environments. We propose embedding lightweight,\nautonomous scheduling logic into each sidecar, allowing scheduling decisions to\nbe made locally without centralized control. This approach leverages the\ngrowing maturity of service mesh infrastructures, which support programmable\ndistributed traffic management. We describe the design of such an architecture\nand present initial results demonstrating its scalability potential in terms of\nresponse time and latency under varying request rates. Rather than delivering a\nfinalized scheduling algorithm, this paper presents a system-level\narchitectural direction and preliminary evidence to support its scalability\npotential.", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528\u670d\u52a1\u7f51\u683c\u8fb9\u8f66\u4ee3\u7406\u4f5c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u8c03\u5ea6\u5668\u7684\u65b0\u67b6\u6784\u65b9\u5411\uff0c\u4ee5\u89e3\u51b3\u4e91\u8fb9\u73af\u5883\u4e2d\u4f20\u7edf\u96c6\u4e2d\u5f0f\u8c03\u5ea6\u7684\u5ef6\u8fdf\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5fae\u670d\u52a1\u7cfb\u7edf\u5728\u4e91\u8fb9\u73af\u5883\u4e2d\u6269\u5c55\uff0c\u4f20\u7edf\u96c6\u4e2d\u5f0f\u8c03\u5ea6\u673a\u5236\u9762\u4e34\u5ef6\u8fdf\u3001\u534f\u8c03\u5f00\u9500\u548c\u5bb9\u9519\u6027\u7b49\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u8f7b\u91cf\u7ea7\u3001\u81ea\u4e3b\u7684\u8c03\u5ea6\u903b\u8f91\u5d4c\u5165\u5230\u6bcf\u4e2a\u670d\u52a1\u7f51\u683c\u8fb9\u8f66\u4ee3\u7406\u4e2d\uff0c\u5b9e\u73b0\u672c\u5730\u5316\u8c03\u5ea6\u51b3\u7b56\uff0c\u65e0\u9700\u96c6\u4e2d\u63a7\u5236\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u8be5\u67b6\u6784\u5728\u54cd\u5e94\u65f6\u95f4\u548c\u5ef6\u8fdf\u65b9\u9762\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u8bf7\u6c42\u901f\u7387\u4e0b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7ea7\u67b6\u6784\u65b9\u5411\uff0c\u5e76\u63d0\u4f9b\u4e86\u521d\u6b65\u8bc1\u636e\u652f\u6301\u5176\u5728\u5927\u578b\u4e91\u539f\u751f\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u6f5c\u529b\u3002"}}
{"id": "2510.10517", "categories": ["cs.PL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.10517", "abs": "https://arxiv.org/abs/2510.10517", "authors": ["Su-Hyeon Kim", "Joonghyuk Hahn", "Sooyoung Cha", "Yo-Sub Han"], "title": "ECO: Enhanced Code Optimization via Performance-Aware Prompting for Code-LLMs", "comment": null, "summary": "Code runtime optimization-the task of rewriting a given code to a faster\none-remains challenging, as it requires reasoning about performance trade-offs\ninvolving algorithmic and structural choices. Recent approaches employ\ncode-LLMs with slow-fast code pairs provided as optimization guidance, but such\npair-based methods obscure the causal factors of performance gains and often\nlead to superficial pattern imitation rather than genuine performance\nreasoning. We introduce ECO, a performance-aware prompting framework for code\noptimization. ECO first distills runtime optimization instructions (ROIs) from\nreference slow-fast code pairs; Each ROI describes root causes of inefficiency\nand the rationales that drive performance improvements. For a given input code,\nECO in parallel employs (i) a symbolic advisor to produce a bottleneck\ndiagnosis tailored to the code, and (ii) an ROI retriever to return related\nROIs. These two outputs are then composed into a performance-aware prompt,\nproviding actionable guidance for code-LLMs. ECO's prompts are model-agnostic,\nrequire no fine-tuning, and can be easily prepended to any code-LLM prompt. Our\nempirical studies highlight that ECO prompting significantly improves\ncode-LLMs' ability to generate efficient code, achieving speedups of up to\n7.81x while minimizing correctness loss.", "AI": {"tldr": "ECO\u662f\u4e00\u4e2a\u6027\u80fd\u611f\u77e5\u7684\u4ee3\u7801\u4f18\u5316\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6027\u80fd\u74f6\u9888\u548c\u4f18\u5316\u539f\u7406\uff0c\u6307\u5bfc\u4ee3\u7801LLM\u751f\u6210\u66f4\u9ad8\u6548\u7684\u4ee3\u7801\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u9ad8\u8fbe7.81\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6162\u5feb\u4ee3\u7801\u5bf9\u7684\u4f18\u5316\u65b9\u6cd5\u5f80\u5f80\u5bfc\u81f4\u8868\u9762\u6a21\u5f0f\u6a21\u4eff\u800c\u975e\u771f\u6b63\u7684\u6027\u80fd\u63a8\u7406\uff0c\u65e0\u6cd5\u63ed\u793a\u6027\u80fd\u63d0\u5347\u7684\u56e0\u679c\u56e0\u7d20\u3002", "method": "ECO\u6846\u67b6\u9996\u5148\u4ece\u53c2\u8003\u4ee3\u7801\u5bf9\u4e2d\u63d0\u53d6\u8fd0\u884c\u65f6\u4f18\u5316\u6307\u4ee4(ROI)\uff0c\u7136\u540e\u5e76\u884c\u4f7f\u7528\u7b26\u53f7\u987e\u95ee\u751f\u6210\u74f6\u9888\u8bca\u65ad\u548cROI\u68c0\u7d22\u5668\u68c0\u7d22\u76f8\u5173\u4f18\u5316\u6307\u4ee4\uff0c\u6700\u540e\u7ec4\u5408\u6210\u6027\u80fd\u611f\u77e5\u63d0\u793a\u6765\u6307\u5bfc\u4ee3\u7801LLM\u3002", "result": "ECO\u63d0\u793a\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801LLM\u751f\u6210\u9ad8\u6548\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe7.81\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u6b63\u786e\u6027\u635f\u5931\u3002", "conclusion": "ECO\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u3001\u65e0\u9700\u5fae\u8c03\u7684\u4ee3\u7801\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6027\u80fd\u611f\u77e5\u63d0\u793a\u6709\u6548\u6307\u5bfc\u4ee3\u7801LLM\u8fdb\u884c\u6df1\u5ea6\u6027\u80fd\u63a8\u7406\u800c\u975e\u8868\u9762\u6a21\u5f0f\u6a21\u4eff\u3002"}}
{"id": "2510.11211", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11211", "abs": "https://arxiv.org/abs/2510.11211", "authors": ["Sheikh Azizul Hakim", "Saem Hasan"], "title": "An Explorative Study on Distributed Computing Techniques in Training and Inference of Large Language Models", "comment": null, "summary": "Large language models (LLM) are advanced AI systems trained on extensive\ntextual data, leveraging deep learning techniques to understand and generate\nhuman-like language. Today's LLMs with billions of parameters are so huge that\nhardly any single computing node can train, fine-tune, or infer from them.\nTherefore, several distributed computing techniques are being introduced in the\nliterature to properly utilize LLMs. We have explored the application of\ndistributed computing techniques in LLMs from two angles.\n  \\begin{itemize}\n  \\item We study the techniques that democratize the LLM, that is, how large\nmodels can be run on consumer-grade computers. Here, we also implement a novel\nmetaheuristics-based modification to an existing system.\n  \\item We perform a comparative study on three state-of-the-art LLM serving\ntechniques. \\end{itemize}", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ece\u4e24\u4e2a\u89d2\u5ea6\u63a2\u8ba8\u4e86\u5206\u5e03\u5f0f\u8ba1\u7b97\u6280\u672f\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u4e2d\u7684\u5e94\u7528\uff1a\u4e00\u662f\u7814\u7a76\u5982\u4f55\u5728\u6d88\u8d39\u7ea7\u8ba1\u7b97\u673a\u4e0a\u8fd0\u884c\u5927\u578b\u6a21\u578b\uff0c\u4e8c\u662f\u5bf9\u4e09\u79cd\u6700\u5148\u8fdb\u7684LLM\u670d\u52a1\u6280\u672f\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u62e5\u6709\u6570\u5341\u4ebf\u53c2\u6570\u7684LLM\u8fc7\u4e8e\u5e9e\u5927\uff0c\u5355\u4e2a\u8ba1\u7b97\u8282\u70b9\u96be\u4ee5\u8fdb\u884c\u8bad\u7ec3\u3001\u5fae\u8c03\u6216\u63a8\u7406\uff0c\u56e0\u6b64\u9700\u8981\u5f15\u5165\u5206\u5e03\u5f0f\u8ba1\u7b97\u6280\u672f\u6765\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u6a21\u578b\u3002", "method": "1. \u7814\u7a76\u4f7fLLM\u6c11\u4e3b\u5316\u7684\u6280\u672f\uff0c\u5373\u5728\u6d88\u8d39\u7ea7\u8ba1\u7b97\u673a\u4e0a\u8fd0\u884c\u5927\u578b\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5e76\u5bf9\u73b0\u6709\u7cfb\u7edf\u8fdb\u884c\u4e86\u57fa\u4e8e\u5143\u542f\u53d1\u5f0f\u7684\u65b0\u9896\u6539\u8fdb\u30022. \u5bf9\u4e09\u79cd\u6700\u5148\u8fdb\u7684LLM\u670d\u52a1\u6280\u672f\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\u3002", "result": "\u8bba\u6587\u63a2\u7d22\u4e86\u5206\u5e03\u5f0f\u8ba1\u7b97\u5728LLM\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u90e8\u7f72\u65b9\u6848\u548c\u4e0d\u540c\u670d\u52a1\u6280\u672f\u7684\u6027\u80fd\u6bd4\u8f83\u3002", "conclusion": "\u5206\u5e03\u5f0f\u8ba1\u7b97\u6280\u672f\u5bf9\u4e8e\u89e3\u51b3LLM\u89c4\u6a21\u8fc7\u5927\u3001\u96be\u4ee5\u5728\u5355\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u7684\u95ee\u9898\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u4ece\u6a21\u578b\u90e8\u7f72\u548c\u670d\u52a1\u6548\u7387\u4e24\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u4f18\u5316\u3002"}}
{"id": "2510.10531", "categories": ["cs.PL", "cs.DC", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.10531", "abs": "https://arxiv.org/abs/2510.10531", "authors": ["Guillaume Ambal", "George Hodgkins", "Mark Madler", "Gregory Chockler", "Brijesh Dongol", "Joseph Izraelevitz", "Azalea Raad", "Viktor Vafeiadis"], "title": "A Verified High-Performance Composable Object Library for Remote Direct Memory Access (Extended Version)", "comment": null, "summary": "Remote Direct Memory Access (RDMA) is a memory technology that allows remote\ndevices to directly write to and read from each other's memory, bypassing\ncomponents such as the CPU and operating system. This enables low-latency\nhigh-throughput networking, as required for many modern data centres, HPC\napplications and AI/ML workloads. However, baseline RDMA comprises a highly\npermissive weak memory model that is difficult to use in practice and has only\nrecently been formalised. In this paper, we introduce the Library of Composable\nObjects (LOCO), a formally verified library for building multi-node objects on\nRDMA, filling the gap between shared memory and distributed system programming.\nLOCO objects are well-encapsulated and take advantage of the strong locality\nand the weak consistency characteristics of RDMA. They have performance\ncomparable to custom RDMA systems (e.g. distributed maps), but with a far\nsimpler programming model amenable to formal proofs of correctness. To support\nverification, we develop a novel modular declarative verification framework,\ncalled Mowgli, that is flexible enough to model multinode objects and is\nindependent of a memory consistency model. We instantiate Mowgli with the RDMA\nmemory model, and use it to verify correctness of LOCO libraries.", "AI": {"tldr": "LOCO\u662f\u4e00\u4e2a\u7ecf\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684RDMA\u591a\u8282\u70b9\u5bf9\u8c61\u5e93\uff0c\u586b\u8865\u4e86\u5171\u4eab\u5185\u5b58\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u7f16\u7a0b\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u9ad8\u6027\u80fd\u4e14\u6613\u4e8e\u9a8c\u8bc1\u7684\u6b63\u786e\u6027\u4fdd\u8bc1\u3002", "motivation": "RDMA\u867d\u7136\u63d0\u4f9b\u4f4e\u5ef6\u8fdf\u9ad8\u541e\u5410\u7684\u7f51\u7edc\u6027\u80fd\uff0c\u4f46\u5176\u5f31\u5185\u5b58\u6a21\u578b\u96be\u4ee5\u5728\u5b9e\u9645\u4e2d\u4f7f\u7528\uff0c\u4e14\u6700\u8fd1\u624d\u88ab\u5f62\u5f0f\u5316\uff0c\u9700\u8981\u7b80\u5316\u7f16\u7a0b\u6a21\u578b\u5e76\u786e\u4fdd\u6b63\u786e\u6027\u3002", "method": "\u5f00\u53d1LOCO\u5e93\u6784\u5efa\u591a\u8282\u70b9\u5bf9\u8c61\uff0c\u5229\u7528RDMA\u7684\u5f3a\u5c40\u90e8\u6027\u548c\u5f31\u4e00\u81f4\u6027\u7279\u6027\uff1b\u521b\u5efaMowgli\u6a21\u5757\u5316\u58f0\u660e\u5f0f\u9a8c\u8bc1\u6846\u67b6\uff0c\u72ec\u7acb\u4e8e\u5185\u5b58\u4e00\u81f4\u6027\u6a21\u578b\uff0c\u5e76\u5b9e\u4f8b\u5316\u5230RDMA\u5185\u5b58\u6a21\u578b\u3002", "result": "LOCO\u5bf9\u8c61\u6027\u80fd\u4e0e\u5b9a\u5236RDMA\u7cfb\u7edf\u76f8\u5f53\uff0c\u4f46\u7f16\u7a0b\u6a21\u578b\u66f4\u7b80\u5355\uff0c\u9002\u5408\u5f62\u5f0f\u5316\u6b63\u786e\u6027\u8bc1\u660e\uff1bMowgli\u6846\u67b6\u6210\u529f\u9a8c\u8bc1\u4e86LOCO\u5e93\u7684\u6b63\u786e\u6027\u3002", "conclusion": "LOCO\u4e3aRDMA\u63d0\u4f9b\u4e86\u6613\u4e8e\u4f7f\u7528\u4e14\u53ef\u9a8c\u8bc1\u7684\u591a\u8282\u70b9\u5bf9\u8c61\u7f16\u7a0b\u62bd\u8c61\uff0cMowgli\u6846\u67b6\u4e3a\u591a\u8282\u70b9\u5bf9\u8c61\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.11513", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.11513", "abs": "https://arxiv.org/abs/2510.11513", "authors": ["Alex Elwood", "Tom Deakin", "Justin Lovegrove", "Chris Nelson"], "title": "An Asynchronous Many-Task Algorithm for Unstructured $S_{N}$ Transport on Shared Memory Systems", "comment": null, "summary": "Discrete ordinates $S_N$ transport solvers on unstructured meshes pose a\nchallenge to scale due to complex data dependencies, memory access patterns and\na high-dimensional domain. In this paper, we review the performance bottlenecks\nwithin the shared memory parallelization scheme of an existing transport solver\non modern many-core architectures with high core counts. With this analysis, we\nthen survey the performance of this solver across a variety of compute\nhardware. We then present a new Asynchronous Many-Task (AMT) algorithm for\nshared memory parallelism, present results showing an increase in computational\nperformance over the existing method, and evaluate why performance is improved.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u975e\u7ed3\u6784\u5316\u7f51\u683c\u4e0a\u79bb\u6563\u7eb5\u6807\u6cd5\u4f20\u8f93\u6c42\u89e3\u5668\u7684\u6027\u80fd\u74f6\u9888\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f02\u6b65\u591a\u4efb\u52a1\u7b97\u6cd5\u6765\u63d0\u9ad8\u8ba1\u7b97\u6027\u80fd\u3002", "motivation": "\u975e\u7ed3\u6784\u5316\u7f51\u683c\u4e0a\u7684\u79bb\u6563\u7eb5\u6807\u6cd5\u4f20\u8f93\u6c42\u89e3\u5668\u7531\u4e8e\u590d\u6742\u7684\u6570\u636e\u4f9d\u8d56\u5173\u7cfb\u3001\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u548c\u9ad8\u7ef4\u57df\u7279\u6027\uff0c\u5728\u591a\u6838\u67b6\u6784\u4e0a\u96be\u4ee5\u6709\u6548\u6269\u5c55\u3002", "method": "\u9996\u5148\u5206\u6790\u73b0\u6709\u4f20\u8f93\u6c42\u89e3\u5668\u5728\u5171\u4eab\u5185\u5b58\u5e76\u884c\u5316\u65b9\u6848\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u7136\u540e\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5f02\u6b65\u591a\u4efb\u52a1\uff08AMT\uff09\u7b97\u6cd5\u7528\u4e8e\u5171\u4eab\u5185\u5b58\u5e76\u884c\u3002", "result": "\u65b0\u7b97\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6027\u80fd\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u5e76\u8bc4\u4f30\u4e86\u6027\u80fd\u6539\u8fdb\u7684\u539f\u56e0\u3002", "conclusion": "\u5f02\u6b65\u591a\u4efb\u52a1\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u7f51\u683c\u4f20\u8f93\u6c42\u89e3\u5668\u5728\u591a\u6838\u67b6\u6784\u4e0a\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6027\u80fd\u3002"}}
{"id": "2510.11007", "categories": ["cs.PL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2510.11007", "abs": "https://arxiv.org/abs/2510.11007", "authors": ["Antonina Nepeivoda", "Ilya Afanasyev"], "title": "Abstract String Domain Defined with Word Equations as a Reduced Product (Extended Version)", "comment": null, "summary": "We introduce a string-interval abstract domain, where string intervals are\ncharacterized by systems of word equations (encoding lower bounds on string\nvalues) and word disequalities (encoding upper bounds). Building upon the\nlattice structure of string intervals, we define an abstract string object as a\nreduced product on a string property semilattice, determined by\nlength-non-increasing morphisms. We consider several reduction strategies for\nabstract string objects and show that upon these strategies the string object\ndomain forms a lattice. We define basic abstract string operations on the\ndomain, aiming to minimize computational overheads on the reduction, and show\nhow the domain can be used to analyse properties of JavaScript string\nmanipulating programs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b57\u7b26\u4e32\u533a\u95f4\u62bd\u8c61\u57df\uff0c\u901a\u8fc7\u5b57\u65b9\u7a0b\u548c\u5b57\u4e0d\u7b49\u5f0f\u6765\u8868\u5f81\u5b57\u7b26\u4e32\u8fb9\u754c\uff0c\u6784\u5efa\u4e86\u57fa\u4e8e\u957f\u5ea6\u975e\u9012\u589e\u6001\u5c04\u7684\u62bd\u8c61\u5b57\u7b26\u4e32\u5bf9\u8c61\u683c\u7ed3\u6784\uff0c\u5e76\u5e94\u7528\u4e8eJavaScript\u5b57\u7b26\u4e32\u7a0b\u5e8f\u5206\u6790\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3JavaScript\u7b49\u8bed\u8a00\u4e2d\u5b57\u7b26\u4e32\u64cd\u4f5c\u7a0b\u5e8f\u7684\u5206\u6790\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7cbe\u786e\u8868\u5f81\u5b57\u7b26\u4e32\u8fb9\u754c\u548c\u7ea6\u675f\u7684\u62bd\u8c61\u57df\u3002", "method": "\u5b9a\u4e49\u5b57\u7b26\u4e32\u533a\u95f4\u62bd\u8c61\u57df\uff0c\u4f7f\u7528\u5b57\u65b9\u7a0b\u8868\u793a\u4e0b\u754c\u3001\u5b57\u4e0d\u7b49\u5f0f\u8868\u793a\u4e0a\u754c\uff1b\u6784\u5efa\u57fa\u4e8e\u957f\u5ea6\u975e\u9012\u589e\u6001\u5c04\u7684\u5b57\u7b26\u4e32\u5c5e\u6027\u534a\u683c\u4e0a\u7684\u7b80\u5316\u79ef\uff1b\u8bbe\u8ba1\u591a\u79cd\u7ea6\u7b80\u7b56\u7565\u5f62\u6210\u683c\u7ed3\u6784\uff1b\u5b9a\u4e49\u57fa\u672c\u62bd\u8c61\u5b57\u7b26\u4e32\u64cd\u4f5c\u4ee5\u6700\u5c0f\u5316\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u6784\u5efa\u4e86\u5b57\u7b26\u4e32\u5bf9\u8c61\u57df\u7684\u683c\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u5bf9JavaScript\u5b57\u7b26\u4e32\u64cd\u4f5c\u7a0b\u5e8f\u7684\u6709\u6548\u5206\u6790\u3002", "conclusion": "\u8be5\u5b57\u7b26\u4e32\u533a\u95f4\u62bd\u8c61\u57df\u80fd\u591f\u6709\u6548\u5206\u6790\u5b57\u7b26\u4e32\u64cd\u4f5c\u7a0b\u5e8f\u7684\u5c5e\u6027\uff0c\u4e3a\u7a0b\u5e8f\u9a8c\u8bc1\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.11697", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.11697", "abs": "https://arxiv.org/abs/2510.11697", "authors": ["Matteo Mordacchini", "Emanuele Carlini", "Patrizio Dazzi"], "title": "A Fast-Converging Decentralized Approach to the Weighted Minimum Vertex Cover Problem", "comment": null, "summary": "We address the problem of computing a Minimum Weighted Vertex Cover (MWVC) in\na decentralized network. MWVC, a classical NP-hard problem, is foundational in\napplications such as network monitoring and resource placement. We propose a\nfully decentralized protocol where each node makes decisions using only local\nknowledge and communicates with its neighbors. The method is adaptive,\ncommunication-efficient, and avoids centralized coordination. We evaluate the\nprotocol on real-world and synthetic graphs, comparing it to both centralized\nand decentralized baselines. Our results demonstrate competitive solution\nquality with reduced communication overhead, highlighting the feasibility of\nMWVC computation in decentralized environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u6700\u5c0f\u52a0\u6743\u9876\u70b9\u8986\u76d6\u8ba1\u7b97\u534f\u8bae\uff0c\u4ec5\u4f7f\u7528\u672c\u5730\u77e5\u8bc6\u548c\u90bb\u5c45\u901a\u4fe1\uff0c\u5728\u771f\u5b9e\u548c\u5408\u6210\u56fe\u4e0a\u9a8c\u8bc1\u4e86\u5176\u7ade\u4e89\u6027\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u548c\u4f4e\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u6700\u5c0f\u52a0\u6743\u9876\u70b9\u8986\u76d6\u662fNP\u96be\u95ee\u9898\uff0c\u5728\u7f51\u7edc\u76d1\u63a7\u548c\u8d44\u6e90\u653e\u7f6e\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u57fa\u7840\u6027\u4f5c\u7528\uff0c\u9700\u8981\u53bb\u4e2d\u5fc3\u5316\u89e3\u51b3\u65b9\u6848\u4ee5\u907f\u514d\u96c6\u4e2d\u534f\u8c03\u3002", "method": "\u8bbe\u8ba1\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u534f\u8bae\uff0c\u6bcf\u4e2a\u8282\u70b9\u4ec5\u57fa\u4e8e\u672c\u5730\u77e5\u8bc6\u548c\u90bb\u5c45\u901a\u4fe1\u505a\u51fa\u51b3\u7b56\uff0c\u5177\u6709\u81ea\u9002\u5e94\u6027\u548c\u901a\u4fe1\u6548\u7387\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u56fe\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u96c6\u4e2d\u5f0f\u548c\u53bb\u4e2d\u5fc3\u5316\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u534f\u8bae\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5728\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u8ba1\u7b97\u6700\u5c0f\u52a0\u6743\u9876\u70b9\u8986\u76d6\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.11420", "categories": ["cs.PL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.11420", "abs": "https://arxiv.org/abs/2510.11420", "authors": ["Mark Koch", "Agust\u00edn Borgna", "Seyon Sivarajah", "Alan Lawrence", "Alec Edgington", "Douglas Wilson", "Craig Roy", "Luca Mondada", "Lukas Heidemann", "Ross Duncan"], "title": "HUGR: A Quantum-Classical Intermediate Representation", "comment": "8 pages, extended abstract submitted to PlanQC25", "summary": "We introduce the Hierarchical Unified Graph Representation (HUGR): a novel\ngraph based intermediate representation for mixed quantum-classical programs.\nHUGR's design features high expressivity and extensibility to capture the\ncapabilities of near-term and forthcoming quantum computing devices, as well as\nnew and evolving abstractions from novel quantum programming paradigms. The\ngraph based structure is machine-friendly and supports powerful pattern\nmatching based compilation techniques. Inspired by MLIR, HUGR's extensibility\nfurther allows compilation tooling to reason about programs at multiple levels\nof abstraction, lowering smoothly between them. Safety guarantees in the\nstructure including strict, static typing and linear quantum types allow rapid\ndevelopment of compilation tooling without fear of program invalidation. A full\nspecification of HUGR and reference implementation are open-source and\navailable online.", "AI": {"tldr": "\u63d0\u51fa\u4e86HUGR\uff1a\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u65b0\u578b\u4e2d\u95f4\u8868\u793a\uff0c\u7528\u4e8e\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u7a0b\u5e8f\uff0c\u5177\u6709\u9ad8\u8868\u8fbe\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u652f\u6301\u591a\u7ea7\u62bd\u8c61\u7f16\u8bd1\u548c\u5b89\u5168\u6027\u4fdd\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u6355\u6349\u8fd1\u91cf\u5b50\u8ba1\u7b97\u8bbe\u5907\u7684\u80fd\u529b\u548c\u65b0\u5174\u91cf\u5b50\u7f16\u7a0b\u8303\u5f0f\u7684\u62bd\u8c61\uff0c\u9700\u8981\u4e00\u4e2a\u673a\u5668\u53cb\u597d\u4e14\u652f\u6301\u5f3a\u5927\u7f16\u8bd1\u6280\u672f\u7684\u4e2d\u95f4\u8868\u793a\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u56fe\u7684\u4e2d\u95f4\u8868\u793aHUGR\uff0c\u53d7MLIR\u542f\u53d1\uff0c\u652f\u6301\u591a\u7ea7\u62bd\u8c61\u3001\u4e25\u683c\u9759\u6001\u7c7b\u578b\u548c\u7ebf\u6027\u91cf\u5b50\u7c7b\u578b\u7b49\u5b89\u5168\u4fdd\u8bc1\u3002", "result": "\u5f00\u53d1\u4e86\u5b8c\u6574\u7684HUGR\u89c4\u8303\u548c\u5f00\u6e90\u53c2\u8003\u5b9e\u73b0\uff0c\u652f\u6301\u6a21\u5f0f\u5339\u914d\u7f16\u8bd1\u6280\u672f\u548c\u5e73\u6ed1\u7684\u62bd\u8c61\u5c42\u7ea7\u8f6c\u6362\u3002", "conclusion": "HUGR\u4e3a\u91cf\u5b50\u7f16\u8bd1\u5de5\u5177\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u53d1\u5c55\u7684\u91cf\u5b50\u8ba1\u7b97\u8bbe\u5907\u548c\u7f16\u7a0b\u8303\u5f0f\u3002"}}
{"id": "2510.11573", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.11573", "abs": "https://arxiv.org/abs/2510.11573", "authors": ["Santiago Arranz-Olmos", "Gilles Barthe", "Lionel Blatter", "Xingyu Xie", "Zhiyuan Zhang"], "title": "(Dis)Proving Spectre Security with Speculation-Passing Style", "comment": null, "summary": "Constant-time (CT) verification tools are commonly used for detecting\npotential side-channel vulnerabilities in cryptographic libraries. Recently, a\nnew class of tools, called speculative constant-time (SCT) tools, has also been\nused for detecting potential Spectre vulnerabilities. In many cases, these SCT\ntools have emerged as liftings of CT tools. However, these liftings are seldom\ndefined precisely and are almost never analyzed formally. The goal of this\npaper is to address this gap, by developing formal foundations for these\nliftings, and to demonstrate that these foundations can yield practical\nbenefits.\n  Concretely, we introduce a program transformation, coined Speculation-Passing\nStyle (SPS), for reducing SCT verification to CT verification. Essentially, the\ntransformation instruments the program with a new input that corresponds to\nattacker-controlled predictions and modifies the program to follow them. This\napproach is sound and complete, in the sense that a program is SCT if and only\nif its SPS transform is CT. Thus, we can leverage existing CT verification\ntools to prove SCT; we illustrate this by combining SPS with three standard\nmethodologies for CT verification, namely reducing it to non-interference,\nassertion safety and dynamic taint analysis. We realize these combinations with\nthree existing tools, EasyCrypt, BINSEC, and ctgrind, and we evaluate them on\nKocher's benchmarks for Spectre-v1. Our results focus on Spectre-v1 in the\nstandard CT leakage model; however, we also discuss applications of our method\nto other variants of Spectre and other leakage models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Speculation-Passing Style (SPS)\u7a0b\u5e8f\u8f6c\u6362\u65b9\u6cd5\uff0c\u5c06\u63a8\u6d4b\u6027\u5e38\u6570\u65f6\u95f4(SCT)\u9a8c\u8bc1\u7b80\u5316\u4e3a\u5e38\u6570\u65f6\u95f4(CT)\u9a8c\u8bc1\uff0c\u4e3aSCT\u5de5\u5177\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u63a8\u6d4b\u6027\u5e38\u6570\u65f6\u95f4(SCT)\u9a8c\u8bc1\u5de5\u5177\u901a\u5e38\u662f\u4ece\u5e38\u6570\u65f6\u95f4(CT)\u5de5\u5177\u63d0\u5347\u800c\u6765\uff0c\u4f46\u8fd9\u79cd\u63d0\u5347\u7f3a\u4e4f\u7cbe\u786e\u7684\u5b9a\u4e49\u548c\u5f62\u5f0f\u5316\u5206\u6790\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u8fd9\u4e9b\u63d0\u5347\u63d0\u4f9b\u5f62\u5f0f\u5316\u57fa\u7840\u3002", "method": "\u5f15\u5165Speculation-Passing Style (SPS)\u7a0b\u5e8f\u8f6c\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u7a0b\u5e8f\u6dfb\u52a0\u653b\u51fb\u8005\u63a7\u5236\u7684\u9884\u6d4b\u8f93\u5165\u5e76\u4fee\u6539\u7a0b\u5e8f\u6765\u9075\u5faa\u8fd9\u4e9b\u9884\u6d4b\uff0c\u4ece\u800c\u5c06SCT\u9a8c\u8bc1\u8f6c\u5316\u4e3aCT\u9a8c\u8bc1\u3002", "result": "SPS\u8f6c\u6362\u662f\u5b8c\u5907\u7684\uff0c\u5373\u7a0b\u5e8f\u662fSCT\u5f53\u4e14\u4ec5\u5f53\u5176SPS\u8f6c\u6362\u662fCT\u3002\u901a\u8fc7\u4e0eEasyCrypt\u3001BINSEC\u548cctgrind\u4e09\u79cdCT\u9a8c\u8bc1\u5de5\u5177\u7ed3\u5408\uff0c\u5728Spectre-v1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SPS\u65b9\u6cd5\u4e3aSCT\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u57fa\u7840\uff0c\u80fd\u591f\u5229\u7528\u73b0\u6709CT\u9a8c\u8bc1\u5de5\u5177\u8bc1\u660eSCT\u5c5e\u6027\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u5176\u4ed6Spectre\u53d8\u4f53\u548c\u6cc4\u6f0f\u6a21\u578b\u3002"}}
