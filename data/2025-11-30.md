<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Optimism in Equality Saturation](https://arxiv.org/abs/2511.20782)
*Russel Arbore,Alvin Cheung,Max Willsey*

Main category: cs.PL

TL;DR: 提出了一种基于抽象解释的乐观分析方法，用于在等式饱和过程中精确分析循环程序，特别是在SSA形式下的程序。


<details>
  <summary>Details</summary>
Motivation: 现有的e-class分析对循环程序的分析是悲观的，在分析SSA形式等循环程序时效果不佳。

Method: 开发了基于抽象解释的算法，在等式饱和过程中精确分析循环，使用新的SSA语义构建原型抽象解释器。

Result: 原型系统能够比clang和gcc更精确地分析简单示例程序。

Conclusion: 该方法实现了乐观分析和非破坏性重写的统一算法，提高了循环程序分析的精确性。

Abstract: Equality saturation is a technique for program optimization based on non-destructive rewriting and a form of program analysis called e-class analysis. The current form of e-class analysis is pessimistic and therefore ineffective at analyzing cyclic programs, such as those in SSA form. We propose an abstract interpretation algorithm that can precisely analyze cycles during equality saturation. This results in a unified algorithm for optimistic analysis and non-destructive rewriting. We instantiate this approach on a prototype abstract interpreter for SSA programs using a new semantics of SSA. Our prototype can analyze simple example programs more precisely than clang and gcc.

</details>


### [2] [Towards Computational UIP in Cubical Agda](https://arxiv.org/abs/2511.21209)
*Yee-Jian Tan,Andreas Nuyts,Dominique Devriese*

Main category: cs.PL

TL;DR: 本文探讨了在Cubical Agda中实现h-Set立方类型论的方法，分析了UIP的不同表述形式及其计算规则，并实现了一个无Glue的Cubical Agda变体。


<details>
  <summary>Details</summary>
Motivation: 立方类型理论具有商归纳类型和函数外延性等优势，但高阶等式层次可能使形式化变得复杂。虽然截断到h-Set层次可以保留这些特性，但目前在Cubical Agda中实现h-Set立方类型论的方法存在不足。

Method: 分析UIP的不同表述形式及其计算规则，评估它们在Cubical Agda中实现的适用性，并实现一个无Glue的Cubical Agda变体。

Result: 提出了UIP的多种表述形式及其计算规则，并成功实现了一个与假设UIP兼容的无Glue Cubical Agda变体。

Conclusion: 为Cubical Agda中未来实现UIP提供了理论基础和实现路径，使h-Set立方类型论能够更有效地在证明助手中实现。

Abstract: Some advantages of Cubical Type Theory, as implemented by Cubical Agda, over intensional Martin-Löf Type Theory include Quotient Inductive Types (QITs), which exist as instances of Higher Inductive Types, and functional extensionality, which is provable in Cubical Type Theory. However, HoTT features an infinite hierarchy of equalities that may become unwieldy in formalisations. Fortunately, QITs and functional extensionality are both preserved even if the equality levels of Cubical Type Theory are truncated to only homotopical Sets (h-Sets). In other words, removing the univalence axiom from Cubical Type Theory and instead postulating a conflicting axiom: the Uniqueness of Identity Proofs (UIP) postulate. Since univalence is proved in Cubical Type Theory from the so-called Glue Types, therefore, it is known that one can first remove the Glue Types (thus removing univalence) and then set-truncate all equalities (essentially assuming UIP), à la XTT. The result is a "h-Set Cubical Type Theory" that retains features such as functional extensionality and QITs.
  However, in Cubical Agda, there are currently only two unsatisfying ways to achieve h-Set Cubical Type Theory. The first is to give up on the canonicity of the theory and simply postulate the UIP axiom, while the second way is to use a standard result stating "type formers preserve h-levels" to manually prove UIP for every defined type. The latter is, however, laborious work best suited for an automatic implementation by the proof assistant. In this project, we analyse formulations of UIP and detail their computation rules for Cubical Agda, and evaluate their suitability for implementation. We also implement a variant of Cubical Agda without Glue, which is already compatible with postulated UIP, in anticipation of a future implementation of UIP in Cubical Agda.

</details>


### [3] [SV-LIB 1.0: A Standard Exchange Format for Software-Verification Tasks](https://arxiv.org/abs/2511.21509)
*Dirk Beyer,Gidon Ernst,Martin Jonáš,Marian Lingsch-Rosenfeld*

Main category: cs.PL

TL;DR: SV-LIB是一个用于软件验证任务的交换格式和中间语言，基于命令式编程语言概念，使用SMT-LIB表示表达式和类型，支持验证见证格式。


<details>
  <summary>Details</summary>
Motivation: 解决不同编程语言验证工具之间的互操作性问题，使语言无关的验证方法能够跨语言复用，促进技术转移。

Method: 设计基于命令式编程语言概念和SMT-LIB的中间语言格式，定义程序、规范和验证见证的表示方法，支持正确和错误程序的见证格式。

Result: 提出了SV-LIB 1.0版本格式，包括设计目标、语法和非形式化语义，支持独立见证验证器开发和验证器复用。

Conclusion: SV-LIB为软件验证工具提供了统一的交换格式，未来计划增加形式化语义和并发扩展。

Abstract: In the past two decades, significant research and development effort went into the development of verification tools for individual languages, such asC, C++, and Java. Many of the used verification approaches are in fact language-agnostic and it would be beneficial for the technology transfer to allow for using the implementations also for other programming and modeling languages. To address the problem, we propose SV-LIB, an exchange format and intermediate language for software-verification tasks, including programs, specifications, and verification witnesses. SV-LIBis based on well-known concepts from imperative programming languages and uses SMT-LIB to represent expressions and sorts used in the program. This makes it easy to parse and to build into existing infrastructure, since many verification tools are based on SMT solvers already. Furthermore, SV-LIBdefines a witness format for both correct and incorrect SV-LIB programs, together with means for specifying witness-validation tasks. This makes it possible both to implement independent witness validators and to reuse some verifiers also as validators for witnesses. This paper presents version 1.0 of the SV-LIBformat, including its design goals, the syntax, and informal semantics. Formal semantics and further extensions to concurrency are planned for future versions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures](https://arxiv.org/abs/2511.20780)
*Alison Silva,Gustavo Callou*

Main category: cs.DC

TL;DR: 本文提出了一种基于随机Petri网的方法来分析私有云环境中Nextcloud文件服务器的可用性，评估了四种冗余策略对系统可用性的影响。


<details>
  <summary>Details</summary>
Motivation: 随着云存储平台在学术和商业环境中的普及，可靠性成为关键需求，特别是对于寻求公共云替代方案的组织。评估这些系统的可靠性至关重要。

Method: 使用随机Petri网(SPNs)建模方法，在Apache CloudStack托管的私有云环境中分析Nextcloud文件服务器的可用性，评估了四种架构配置：基线、主机级冗余、虚拟机冗余以及两者组合。

Result: 结果表明，在主机和虚拟机级别同时实施冗余策略能显著提高可用性并减少预期停机时间。

Conclusion: 所提出的方法为评估私有云可用性和支持基础设施设计决策提供了一种有效途径。

Abstract: Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions.

</details>


### [5] [Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks](https://arxiv.org/abs/2511.20834)
*Dionysios Adamopoulos,Anastasia Poulopoulou,Georgios Goumas,Christina Giannoula*

Main category: cs.DC

TL;DR: Spira是一个基于GPU的稀疏卷积引擎，通过利用体素坐标的整数性、有界性和几何连续性等特性，显著提升了3D点云网络的处理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏卷积引擎未能充分利用体素坐标的三个关键特性（整数值、有限空间范围、几何连续性），导致在核映射构建过程中存在高预处理和后处理开销。

Method: Spira设计了四种关键技术：(1) 一次性搜索算法构建核映射；(2) 打包原生处理方案低成本访问体素坐标；(3) 双数据流执行机制适应不同层特性；(4) 网络级并行化策略同时构建所有层的核映射。

Result: Spira在端到端推理中平均性能提升1.71倍（最高2.31倍），在逐层执行中平均提升2.13倍（最高3.32倍）。

Conclusion: Spira通过充分利用体素坐标的特性，显著提升了稀疏卷积在GPU上的执行效率，为3D点云处理提供了高效的解决方案。

Abstract: Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work identifies three key properties of voxel coordinates: they are integer-valued, bounded within a limited spatial range, and geometrically continuous-neighboring voxels on the same object surface are highly likely to exist at small spatial offsets from each other. Prior SpC engines do not fully exploit these properties and suffer from high pre-processing and post-processing overheads during kernel map construction. To address this, we design Spira, the first voxel-property-aware SpC engine for GPUs. Spira proposes: (i) a high-performance one-shot search algorithm that builds the kernel map with no preprocessing and high memory locality, (ii) an effective packed-native processing scheme that accesses packed voxel coordinates at low cost, (iii) a flexible dual-dataflow execution mechanism that efficiently computes output feature vectors by adapting to layer characteristics, and (iv) a network-wide parallelization strategy that builds kernel maps for all SpC layers concurrently at network start. Our evaluation shows that Spira significantly outperforms prior SpC engines by 1.71x on average and up to 2.31x for end-to-end inference, and by 2.13x on average and up to 3.32x for layer-wise execution across diverse layer configurations.

</details>


### [6] [Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows](https://arxiv.org/abs/2511.20975)
*Yinwei Dai,Zhuofu Chen,Anand Iyer,Ravi Netravali*

Main category: cs.DC

TL;DR: Aragog是一个动态配置选择系统，能够在工作流执行过程中根据运行时动态调整LLM配置，以降低计算成本并提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的代理工作流配置选择方法在请求执行前就固定配置，无法适应系统负载的快速变化，导致配置很快变得不优化。

Method: Aragog将问题解耦为两个核心组件：一次性路由步骤识别所有保持精度的配置，以及基于最新系统观察的廉价每阶段调度器。

Result: 在多样化工作流和模型家族中，Aragog将最大服务吞吐量提高了50.0-217.0%，在峰值请求率下将中位延迟降低了32.5-78.9%，同时保持与最昂贵配置相当的精度。

Conclusion: Aragog通过动态配置调整有效解决了代理工作流服务中的成本效率问题，显著提升了系统性能。

Abstract: Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\% and reduces median latency by 32.5--78.9\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.

</details>


### [7] [A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving](https://arxiv.org/abs/2511.20982)
*Junhan Liao,Minxian Xu,Wanyi Zheng,Yan Wang,Kejiang Ye,Rajkumar Buyya,Chengzhong Xu*

Main category: cs.DC

TL;DR: DOPD是一个动态LLM推理系统，通过实时监控负载动态调整预填充和解码实例分配比例，解决异构工作负载下的生产者-消费者不平衡问题，显著提升系统吞吐量和响应性能。


<details>
  <summary>Details</summary>
Motivation: 当代LLM将预填充和解码阶段解耦到不同GPU上以缓解各自瓶颈，但异构工作负载导致这种分离架构中两种实例类型间的生产者-消费者不平衡。

Method: 提出DOPD系统，基于实时负载监控动态调整实例分配以达到最优预填充/解码比例，结合适当的请求调度策略解决实例不平衡和资源分配不匹配问题。

Result: 相比vLLM和DistServe，DOPD将系统吞吐量提升1.5倍，P90首token时间降低67.5%，P90每输出token时间降低22.8%。

Conclusion: 动态P/D调整技术基于历史负载进行主动重配置，在使用更少额外资源的同时实现了超过99%的SLO达成率。

Abstract: To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.

</details>


### [8] [Handling of Memory Page Faults during Virtual-Address RDMA](https://arxiv.org/abs/2511.21018)
*Antonis Psistakis*

Main category: cs.DC

TL;DR: 该论文实现了一种与DMA引擎集成的页面错误处理机制，通过SMMU检测故障并采用硬件-软件解决方案进行解决，避免了传统RDMA技术中内存固定的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统RDMA技术无法容忍页面错误，通常采用内存固定技术，但这带来了编程复杂性、内存使用限制和效率低下的问题。现代操作系统如Linux默认启用的透明大页等优化机制使得固定无法完全防止页面错误。

Method: 在ExaNeSt项目的DMA引擎中集成页面错误处理机制，通过ARM SMMU检测故障，采用硬件-软件解决方案请求重传。需要修改Linux SMMU驱动、开发新软件库、调整DMA引擎硬件和调度逻辑。

Result: 在ExaNeSt的QFDB平台上进行了实验评估，比较了该机制与内存固定和预错误处理等替代方案的性能。

Conclusion: 提出的页面错误处理机制相比传统内存固定方法具有优势，能够更好地处理页面错误问题。

Abstract: Nowadays, avoiding system calls during cluster communication (e.g., in Data Centers and High Performance Computing) in modern high-speed interconnection networks has become a necessity, due to the high overhead of multiple data copies between kernel and user space. User-level zero-copy Remote Direct Memory Access (RDMA) technologies address this problem by improving performance and reducing system energy consumption. However, traditional RDMA engines cannot tolerate page faults and therefore use various techniques to avoid them.
  State-of-the-art RDMA approaches typically rely on pinning address spaces or multiple pages per application. This method introduces long-term disadvantages due to increased programming complexity (pinning and unpinning buffers), limits on how much memory can be pinned, and inefficient memory utilization. In addition, pinning does not fully prevent page faults because modern operating systems apply internal optimization mechanisms, such as Transparent Huge Pages (THP), which are enabled by default in Linux.
  This thesis implements a page-fault handling mechanism integrated with the DMA engine of the ExaNeSt project. Faults are detected by the ARM System Memory Management Unit (SMMU) and resolved through a hardware-software solution that can request retransmission when needed. This mechanism required modifications to the Linux SMMU driver, the development of a new software library, changes to the DMA engine hardware, and adjustments to the DMA scheduling logic. Experiments were conducted on the Quad-FPGA Daughter Board (QFDB) of ExaNeSt, which uses Xilinx Zynq UltraScale+ MPSoCs.
  Finally, we evaluate our mechanism and compare it against alternatives such as pinning and pre-faulting, and discuss the advantages of our approach.

</details>


### [9] [Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM](https://arxiv.org/abs/2511.21413)
*Tim Trappen,Robert Keßler,Roland Pabel,Viktor Achter,Stefan Wesner*

Main category: cs.DC

TL;DR: 提出了一种在超级计算机RAMSES上集成vLLM、Slurm和Kubernetes来服务LLM的解决方案，能够高效处理100-1000个并发请求，延迟开销仅约500ms。


<details>
  <summary>Details</summary>
Motivation: 由于AI推理需求增长，特别是在高等教育领域，需要利用现有基础设施的新解决方案。传统HPC操作模型不适合同步、面向用户的动态AI应用工作负载。

Method: 在超级计算机RAMSES上集成vLLM、Slurm和Kubernetes来服务大语言模型(LLM)。

Result: 初步基准测试表明，该架构在100、500和1000个并发请求下都能高效扩展，端到端延迟仅增加约500毫秒的开销。

Conclusion: 该集成方案成功解决了传统HPC在AI推理应用中的适应性挑战，为高等教育领域的AI服务提供了可行的技术路径。

Abstract: Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.

</details>


### [10] [MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training](https://arxiv.org/abs/2511.21431)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Yueqiang Chen,Baoguo He,Hongfeng Sun,Ziqing Yin,Shangchao Su,Zhiyan Cui,Liang Dong,Xiyuan Li,Lingbin Wang,Jianwei He,Jiesong Ma,Weikang Huang,Jianglei Tong,Dongdong Gao,Jian Zhang,Hong Tian*

Main category: cs.DC

TL;DR: MemFine是一个内存感知的细粒度调度框架，通过分块重计算策略解决MoE训练中的内存瓶颈问题，在内存受限的GPU上实现稳定的大规模MoE训练。


<details>
  <summary>Details</summary>
Motivation: 大规模MoE模型训练面临严重的内存瓶颈，动态token路由导致的负载不平衡会造成GPU内存溢出，限制了模型的可扩展性。现有负载均衡方法会牺牲模型精度，且在内存受限硬件上失效。

Method: 将token分布和专家计算分解为可管理的块，采用分块重计算策略，通过理论内存模型动态优化内存效率和吞吐量之间的平衡。

Result: 相比基于完全重计算的基线方法，MemFine减少了48.03%的激活内存，提高了4.42%的吞吐量。

Conclusion: MemFine能够在内存受限的GPU上实现稳定的大规模MoE训练，有效解决了内存瓶颈问题。

Abstract: The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.

</details>


### [11] [Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation](https://arxiv.org/abs/2511.21535)
*Morteza Sadeghi*

Main category: cs.DC

TL;DR: 通过引入数据冗余改善MLFMA中近场算子的内存局部性，在GPU上实现最高7倍的内核加速，但受限于数据重组开销，端到端应用加速仅为1.04倍。


<details>
  <summary>Details</summary>
Motivation: MLFMA中的近场算子在GPU上由于内存局部性差成为性能瓶颈，需要改进内存访问模式。

Method: 引入数据冗余减少内存访问分散，提出基于局部性度量的分析模型预测性能趋势，并在两个MLFMA应用中验证。

Result: 内核加速最高达7倍，但端到端应用加速仅1.04倍，模型能可靠预测不同问题规模和密度下的性能趋势。

Conclusion: 数据冗余可提升GPU上P2P算子性能，但需权衡局部性收益与数据移动成本，该技术可最小化代码修改注入现有实现。

Abstract: The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.

</details>


### [12] [Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases](https://arxiv.org/abs/2511.21612)
*Shahir Abdullah,Syed Rohit Zaman*

Main category: cs.DC

TL;DR: 论文提出了二维扩展平面模型，将水平扩展和垂直扩展结合考虑，通过DIAGONALSCALE算法计算最优的斜向扩展路径，相比传统单一维度的扩展方式能显著降低延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 现代云数据库将扩展视为二元决策：水平扩展（增加节点）或垂直扩展（增加单节点资源）。这种一维视图限制了性能优化，因为数据库性能、成本和协调开销是水平弹性和单节点资源共同作用的结果。

Method: 引入扩展平面二维模型，将分布式数据库配置表示为点(H,V)，其中H是节点数，V是资源向量。提出DIAGONALSCALE算法，通过局部搜索评估水平、垂直和斜向移动，选择最小化多目标函数的配置。

Result: 斜向扩展相比仅水平或仅垂直的自动扩展，能将p95延迟降低高达40%，每查询成本降低高达37%，重平衡次数减少2-5倍。

Conclusion: 研究结果强调了多维扩展模型的必要性，为下一代云数据库系统的自动扩展提供了基础。

Abstract: Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems.

</details>


### [13] [AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI](https://arxiv.org/abs/2511.21661)
*Beth Plale,Neelesh Karthikeyan,Isuru Gamage,Joe Stubbs,Sachith Withana*

Main category: cs.DC

TL;DR: 该研究评估了在Patra模型卡服务器中使用模型上下文协议(MCP)作为接口的效益与权衡，包括定量性能开销和定性使用适配性分析。


<details>
  <summary>Details</summary>
Motivation: 传统AI/ML模型卡在训练时的一次性评估无法反映模型在实际使用中的动态表现，需要研究模型卡作为动态对象的使用方式。

Method: 通过嵌入ICICLE AI研究所软件生态系统的Patra模型卡，研究MCP作为接口与REST接口的性能对比，并分析MCP支持的活跃会话使用情况。

Result: 定量评估显示MCP相比REST接口存在性能开销，但核心价值在于支持动态模型卡的活跃会话功能。

Conclusion: MCP作为接口在动态模型卡场景中具有使用适配性优势，尽管存在性能开销，但支持活跃会话功能是其核心价值。

Abstract: AI/ML model cards can contain a benchmarked evaluation of an AI/ML model against intended use but a one time assessment during model training does not get at how and where a model is actually used over its lifetime. Through Patra Model Cards embedded in the ICICLE AI Institute software ecosystem we study model cards as dynamic objects. The study reported here assesses the benefits and tradeoffs of adopting the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative assessment shows the overhead of MCP as compared to a REST interface. The core question however is of active sessions enabled by MCP; this is a qualitative question of fit and use in the context of dynamic model cards that we address as well.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI](https://arxiv.org/abs/2511.21232)
*Muhammed Yildirim,Ozcan Ozturk*

Main category: cs.AR

TL;DR: 提出一种融合像素数据流硬件加速器，消除深度可分离卷积中间缓冲区，减少87%数据传输，在FPGA上实现59.3倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI和TinyML应用中深度可分离卷积逐层执行时中间特征图传输带来的高能耗和延迟问题，突破内存墙限制。

Method: 设计基于RISC-V处理器的定制功能单元，采用融合像素数据流，将扩展、深度卷积和投影阶段流水线化处理单个输出像素，无需中间缓冲区。

Result: 在Xilinx Artix-7 FPGA上实现59.3倍加速，ASIC合成显示28nm工艺下0.284mm²面积、910mW功耗，40nm工艺下1.20mm²面积、233mW功耗。

Conclusion: 证实了在TinyML资源约束下实现零缓冲区数据流的可行性，为边缘AI加速器克服内存墙提供了有效策略。

Abstract: The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.

</details>


### [15] [Bombyx: OpenCilk Compilation for FPGA Hardware Acceleration](https://arxiv.org/abs/2511.21346)
*Mohamed Shahawy,Julien de Castelnau,Paolo Ienne*

Main category: cs.AR

TL;DR: Bombyx是一个编译器工具链，将OpenCilk程序转换为Cilk-1风格的中间表示，使CPU导向的任务级并行应用能高效映射到FPGA空间架构上。


<details>
  <summary>Details</summary>
Motivation: 现有系统在FPGA上支持任务级并行时，OpenCilk的隐式任务模型需要昂贵的硬件上下文切换，而Cilk-1的显式延续传递模型更适合FPGA的流式特性。

Method: 开发Bombyx编译器工具链，支持多种编译目标：OpenCilk兼容运行时和可综合的PE生成器，并引入解耦访问-执行优化来提升内存-计算重叠。

Result: 能够自动生成高性能处理单元，改善内存-计算重叠和整体吞吐量。

Conclusion: Bombyx成功将CPU导向的任务级并行应用映射到FPGA空间架构，通过显式延续传递模型和解耦优化实现了高效执行。

Abstract: Task-level parallelism (TLP) is a widely used approach in software where independent tasks are dynamically created and scheduled at runtime. Recent systems have explored architectural support for TLP on field-programmable gate arrays (FPGAs), often leveraging high-level synthesis (HLS) to create processing elements (PEs). In this paper, we present Bombyx, a compiler toolchain that lowers OpenCilk programs into a Cilk-1-inspired intermediate representation, enabling efficient mapping of CPU-oriented TLP applications to spatial architectures on FPGAs. Unlike OpenCilk's implicit task model, which requires costly context switching in hardware, Cilk-1 adopts explicit continuation-passing - a model that better aligns with the streaming nature of FPGAs. Bombyx supports multiple compilation targets: one is an OpenCilk-compatible runtime for executing Cilk-1-style code using the OpenCilk backend, and another is a synthesizable PE generator designed for HLS tools like Vitis HLS. Additionally, we introduce a decoupled access-execute optimization that enables automatic generation of high-performance PEs, improving memory-compute overlap and overall throughput.

</details>


### [16] [A Jammer-Resilient 2.87 mm$^2$ 1.28 MS/s 310 mW Multi-Antenna Synchronization ASIC in 65 nm](https://arxiv.org/abs/2511.21451)
*Flurin Arquint,Oscar Castañeda,Gian Marti,Christoph Studer*

Main category: cs.AR

TL;DR: 首个抗干扰多天线时间同步ASIC实现，支持单天线发射器与16天线接收器同步，可抵御最多2天线智能干扰器。


<details>
  <summary>Details</summary>
Motivation: 解决无线通信系统中时间同步信号易受干扰攻击的问题，特别是针对智能干扰器的威胁。

Method: 采用多天线处理算法，在65nm工艺下实现ASIC设计，支持1.28 MS/s采样率。

Result: 芯片核心面积2.87 mm²，功耗310 mW，成功实现抗干扰时间同步功能。

Conclusion: 该ASIC证明了多天线处理在硬件层面实现抗干扰时间同步的可行性，为安全无线通信系统提供了硬件基础。

Abstract: We present the first ASIC implementation of jammer-resilient multi-antenna time synchronization. The ASIC implements a recent algorithm that mitigates jamming attacks on synchronization signals using multi-antenna processing. Our design supports synchronization between a single-antenna transmitter and a 16-antenna receiver while mitigating smart jammers with up to two transmit antennas. The fabricated 65 nm ASIC has a core area of 2.87 mm$^2$, consumes a power of 310 mW, and supports a sampling rate of 1.28 mega-samples per second (MS/s).

</details>


### [17] [A 0.32 mm$^2$ 100 Mb/s 223 mW ASIC in 22FDX for Joint Jammer Mitigation, Channel Estimation, and SIMO Data Detection](https://arxiv.org/abs/2511.21461)
*Jonas Elmiger,Fabian Stuber,Oscar Castañeda,Gian Marti,Christoph Studer*

Main category: cs.AR

TL;DR: 提出了首个单输入多输出(SIMO)接收器ASIC，联合执行干扰抑制、信道估计和数据检测，采用MAED算法，在22nm FD-SOI工艺下实现0.32mm²核心面积，吞吐量100Mb/s，功耗223mW。


<details>
  <summary>Details</summary>
Motivation: 现有系统在面对智能干扰器和扫频干扰器时性能受限，需要开发能够同时处理干扰抑制、信道估计和数据检测的集成解决方案。

Method: 采用MAED算法，通过非线性优化问题统一干扰器估计与抑制、信道估计和数据检测，使用空间滤波技术来抑制智能干扰器。

Result: 支持8个接收天线，在22nm FD-SOI工艺下实现0.32mm²核心面积，吞吐量100Mb/s，功耗223mW，相比现有抗干扰检测器，每用户吞吐量提高3倍，面积效率提高4.5倍。

Conclusion: 该ASIC设计成功实现了联合干扰抑制、信道估计和数据检测的功能，在抗干扰性能和系统效率方面显著优于现有技术。

Abstract: We present the first single-input multiple-output (SIMO) receiver ASIC that jointly performs jammer mitigation, channel estimation, and data detection. The ASIC implements a recent algorithm called siMultaneous mitigAtion, Estimation, and Detection (MAED). MAED mitigates smart jammers via spatial filtering using a nonlinear optimization problem that unifies jammer estimation and nulling, channel estimation, and data detection to achieve state-of-the-art error-rate performance under jamming. The design supports eight receive antennas and enables mitigation of smart jammers as well as of barrage jammers. The ASIC is fabricated in 22 nm FD-SOI, has a core area of 0.32 mm$^2$, and achieves a throughput of 100 Mb/s at 223 mW, thus delivering 3$\times$ higher per-user throughput and 4.5$\times$ higher area efficiency than the state-of-the-art jammer-resilient detector.

</details>


### [18] [Modeling and Optimizing Performance Bottlenecks for Neuromorphic Accelerators](https://arxiv.org/abs/2511.21549)
*Jason Yik,Walter Gallego Gomez,Andrew Cheng,Benedetto Leto,Alessandro Pierro,Noah Pacik-Nelson,Korneel Van den Berghe,Vittorio Fra,Andreea Danielescu,Gianvito Urgese,Vijay Janapa Reddi*

Main category: cs.AR

TL;DR: 本文首次对神经形态加速器进行全面的性能边界和瓶颈分析，揭示了传统指标（如网络级稀疏性和操作计数）的不足，提出了floorline性能模型来识别性能边界，并开发了结合稀疏感知训练和floorline指导分区的优化方法，在保持准确率的同时实现了3.86倍运行时间提升和3.38倍能耗降低。


<details>
  <summary>Details</summary>
Motivation: 神经形态加速器利用事件驱动、空间扩展架构自然利用非结构化稀疏性，但其独特的架构特性产生了与传统加速器根本不同的性能动态。现有优化方法依赖聚合网络级稀疏性和操作计数，但这些指标对实际部署性能的改善程度未知。

Method: 对三种真实神经形态加速器（Brainchip AKD1000、Synsense Speck、Intel Loihi 2）进行理论分析建模和广泛实证表征，建立三种瓶颈状态（内存受限、计算受限、流量受限），提出floorline性能模型识别性能边界，开发结合稀疏感知训练和floorline指导分区的优化方法。

Result: 识别了三种不同的加速器瓶颈状态，建立了floorline性能模型来识别性能边界，优化方法在保持准确率的同时实现了最高3.86倍的运行时间提升和3.38倍的能耗降低。

Conclusion: 传统基于网络级稀疏性和操作计数的指标不足以优化神经形态加速器性能，floorline性能模型能有效识别性能边界和瓶颈，结合稀疏感知训练的分区优化方法能显著提升性能效率。

Abstract: Neuromorphic accelerators offer promising platforms for machine learning (ML) inference by leveraging event-driven, spatially-expanded architectures that naturally exploit unstructured sparsity through co-located memory and compute. However, their unique architectural characteristics create performance dynamics that differ fundamentally from conventional accelerators. Existing workload optimization approaches for neuromorphic accelerators rely on aggregate network-wide sparsity and operation counting, but the extent to which these metrics actually improve deployed performance remains unknown. This paper presents the first comprehensive performance bound and bottleneck analysis of neuromorphic accelerators, revealing the shortcomings of the conventional metrics and offering an understanding of what facets matter for workload performance. We present both theoretical analytical modeling and extensive empirical characterization of three real neuromorphic accelerators: Brainchip AKD1000, Synsense Speck, and Intel Loihi 2. From these, we establish three distinct accelerator bottleneck states, memory-bound, compute-bound, and traffic-bound, and identify which workload configuration features are likely to exhibit these bottleneck states. We synthesize all of our insights into the floorline performance model, a visual model that identifies performance bounds and informs how to optimize a given workload, based on its position on the model. Finally, we present an optimization methodology that combines sparsity-aware training with floorline-informed partitioning. Our methodology achieves substantial performance improvements at iso-accuracy: up to 3.86x runtime improvement and 3.38x energy reduction compared to prior manually-tuned configurations.

</details>
