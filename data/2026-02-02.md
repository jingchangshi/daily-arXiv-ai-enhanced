<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Recursive Mutexes in Separation Logic](https://arxiv.org/abs/2601.22557)
*Ke Du,William Mansky,Paolo G. Giarrusso,Gregory Malecha*

Main category: cs.PL

TL;DR: 论文为递归互斥锁（recursive mutex）开发了分离逻辑规范，类似于普通互斥锁的两种规范风格：保护不变式或原子改变锁状态。


<details>
  <summary>Details</summary>
Motivation: 递归互斥锁在C++、Java等面向对象语言中很常见，但现有的分离逻辑规范主要针对普通互斥锁。需要为递归互斥锁开发类似的规范，使其能够被同一线程多次获取，同时保持规范的简洁性。

Method: 为递归互斥锁开发两种风格的分离逻辑规范：1）基于保护不变式的规范；2）基于原子状态变化的规范。规范设计使得所有获取/释放操作都被统一处理，客户端只需在访问锁不变式时判断自己是否持有该锁。

Result: 成功为递归互斥锁开发了分离逻辑规范，这些规范能够处理同一线程多次获取锁的情况，同时保持了与普通互斥锁规范类似的结构和简洁性。

Conclusion: 递归互斥锁可以在分离逻辑中得到与普通互斥锁类似的规范处理，通过统一的获取/释放操作处理，简化了客户端的使用，只需在访问锁保护的数据时判断锁的持有状态。

Abstract: Mutexes (i.e., locks) are well understood in separation logic, and can be specified in terms of either protecting an invariant or atomically changing the state of the lock. In this abstract, we develop the same styles of specifications for \emph{recursive} mutexes, a common variant of mutexes in object-oriented languages such as C++ and Java. A recursive mutex can be acquired any number of times by the same thread, and our specifications treat all acquires/releases uniformly, with clients only needing to determine whether they hold the mutex when accessing the lock invariant.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Towards Resiliency in Large Language Model Serving with KevlarFlow](https://arxiv.org/abs/2601.22438)
*Shangshu Qian,Kipling Liu,P. C. Sruthi,Lin Tan,Yongle Zhang*

Main category: cs.DC

TL;DR: KevlarFlow是一个用于大语言模型服务的容错架构，通过解耦模型并行初始化、动态流量重路由和后台KV缓存复制，显著降低故障恢复时间并提升服务性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务系统在超大规模集群中面临硬件故障频繁的问题，导致服务中断。现有恢复机制过慢（需要长达10分钟重新初始化资源和加载模型权重），无法满足高可用性需求。

Method: 提出KevlarFlow架构，采用三种关键技术：1) 解耦模型并行初始化，2) 动态流量重路由，3) 后台KV缓存复制，以在部分故障时维持高吞吐量。

Result: KevlarFlow将平均恢复时间(MTTR)降低20倍，在故障条件下：平均延迟提升3.1倍，p99延迟提升2.8倍，平均首次令牌时间(TTFT)提升378.9倍，p99 TTFT提升574.6倍，运行时开销可忽略。

Conclusion: KevlarFlow有效解决了LLM服务系统的容错问题，显著提升了服务可用性和性能，为超大规模集群中的可靠LLM服务提供了实用解决方案。

Abstract: Large Language Model (LLM) serving systems remain fundamentally fragile, where frequent hardware faults in hyperscale clusters trigger disproportionate service outages in the software stack. Current recovery mechanisms are prohibitively slow, often requiring up to 10 minutes to reinitialize resources and reload massive model weights. We introduce KevlarFlow, a fault tolerant serving architecture designed to bridge the gap between hardware unreliability and service availability. KevlarFlow leverages 1) decoupled model parallelism initialization, 2) dynamic traffic rerouting, and 3) background KV cache replication to maintain high throughput during partial failures. Our evaluation demonstrates that KevlarFlow reduces mean-time-to-recovery (MTTR) by 20x and, under failure conditions, improves average latency by 3.1x, 99th percentile (p99) latency by 2.8x, average time-to-first-token (TTFT) by 378.9x, and p99 TTFT by 574.6x with negligible runtime overhead in comparison to state-of-the-art LLM serving systems.

</details>


### [3] [Coordinating Power Grid Frequency Regulation Service with Data Center Load Flexibility](https://arxiv.org/abs/2601.22487)
*Ali Jahanshahi,Sara Rashidi Golrouye,Osten Anderson,Nanpeng Yu,Daniel Wong*

Main category: cs.DC

TL;DR: 数据中心参与电网频率调节可减少化石燃料备用需求，从而降低碳排放


<details>
  <summary>Details</summary>
Motivation: AI/ML数据中心能耗增长导致碳排放增加，而可再生能源转型和数据中心需求增长可能破坏电网稳定。电网依赖化石燃料发电厂进行频率调节，这会产生隐藏碳排放。

Method: 提出"外源性碳排放"指标量化数据中心参与调节服务带来的电网侧碳减排；开发EcoCenter框架最大化GPU数据中心提供频率调节的能力，减少化石燃料备用需求。

Result: 数据中心参与频率调节产生的外源性碳节约通常超过其运行碳排放，表明数据中心可作为电网调节资源减少整体碳排放。

Conclusion: GPU数据中心与电网协调参与频率调节服务可显著减少对化石燃料调节备用的需求，降低整体碳排放，实现数据中心从能源消耗者向电网稳定贡献者的转变。

Abstract: AI/ML data center growth have led to higher energy consumption and carbon emissions. The shift to renewable energy and growing data center energy demands can destabilize the power grid. Power grids rely on frequency regulation reserves, typically fossil-fueled power plants, to stabilize and balance the supply and demand of electricity. This paper sheds light on the hidden carbon emissions of frequency regulation service. Our work explores how modern GPU data centers can coordinate with power grids to reduce the need for fossil-fueled frequency regulation reserves. We first introduce a novel metric, Exogenous Carbon, to quantify grid-side carbon emission reductions resulting from data center participation in regulation service. We additionally introduce EcoCenter, a framework to maximize the amount of frequency regulation provision that GPU data centers can provide, and thus, reduce the amount of frequency regulation reserves necessary. We demonstrate that data center participation in frequency regulation can result in Exogenous carbon savings that oftentimes outweigh Operational carbon emissions.

</details>


### [4] [HetCCL: Accelerating LLM Training with Heterogeneous GPUs](https://arxiv.org/abs/2601.22585)
*Heehoon Kim,Jaehwan Lee,Taejeoung Kim,Jongwon Park,Jinpyo Kim,Pyongwon Suh,Ryan H. Choi,Sangwoo Lee,Jaejin Lee*

Main category: cs.DC

TL;DR: HetCCL是一个异构GPU集体通信库，支持跨NVIDIA和AMD GPU的高性能通信，无需修改现有深度学习应用


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，组织需要扩展GPU集群，但现有深度学习框架缺乏对异构GPU集体通信的支持，导致效率低下和成本增加

Method: HetCCL统一了厂商特定的后端，支持基于RDMA的跨GPU通信，无需驱动修改。引入两种新颖机制，在利用优化的厂商库（NVIDIA NCCL和AMD RCCL）的同时实现跨厂商通信

Result: 在多厂商GPU集群上的评估显示，HetCCL在同类设置中与NCCL和RCCL性能相当，在异构环境中具有独特的扩展能力

Conclusion: HetCCL实现了实用、高性能的NVIDIA和AMD GPU混合训练，无需修改现有深度学习应用

Abstract: The rapid growth of large language models is driving organizations to expand their GPU clusters, often with GPUs from multiple vendors. However, current deep learning frameworks lack support for collective communication across heterogeneous GPUs, leading to inefficiency and higher costs. We present HetCCL, a collective communication library that unifies vendor-specific backends and enables RDMA-based communication across GPUs without requiring driver modifications. HetCCL introduces two novel mechanisms that enable cross-vendor communication while leveraging optimized vendor libraries, NVIDIA NCCL and AMD RCCL. Evaluations on a multi-vendor GPU cluster show that HetCCL matches NCCL and RCCL performance in homogeneous setups while uniquely scaling in heterogeneous environments, enabling practical, high-performance training with both NVIDIA and AMD GPUs without changes to existing deep learning applications.

</details>


### [5] [CONCUR: High-Throughput Agentic Batch Inference of LLM via Congestion-Based Concurrency Control](https://arxiv.org/abs/2601.22705)
*Qiaoling Chen,Zhisheng Ye,Tian Tang,Peng Sun,Boyu Tian,Guoteng Wang,Shenggui Li,Yonggang Wen,Zhenhua Han,Tianwei Zhang*

Main category: cs.DC

TL;DR: CONCUR是一个轻量级控制层，通过基于反馈的代理准入控制来防止KV缓存中间阶段抖动，显著提升批量推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 代理工作负载的批量推理会对GPU键值缓存造成持续累积的压力，导致严重的吞吐量下降，这种现象被称为中间阶段抖动，是现有缓存管理方法未能充分解决的新问题。

Method: CONCUR采用分布式系统中拥塞控制的思路，将KV缓存视为共享资源，通过基于反馈的调节机制实现代理级别的准入控制。它使用缓存感知控制算法，根据运行时缓存信号动态调整活跃代理数量。

Result: 在大型模型和实际代理工作负载上，CONCUR防止了中间阶段抖动，在Qwen3-32B上提升批量推理吞吐量达4.09倍，在DeepSeek-V3上提升1.9倍，且与现有LLM服务系统兼容。

Conclusion: KV缓存管理需要从反应式的请求级控制转向主动的代理级准入控制，CONCUR通过反馈驱动的调节机制有效解决了中间阶段抖动问题，显著提升了批量推理性能。

Abstract: Batch inference for agentic workloads stresses the GPU key-value (KV) cache in a sustained and cumulative manner, often causing severe throughput degradation well before memory capacity is exhausted. We identify this phenomenon as middle-phase thrashing, a previously under-characterized pathology in which cache efficiency collapses as long-lived agents accumulate state over time.
  We argue that mitigating this pathology requires moving beyond reactive, request-level cache management to proactive, agent-level admission control. Drawing inspiration from congestion control in distributed systems, we view the KV cache as a shared resource whose efficient utilization depends on feedback-driven regulation. Based on this insight, we present CONCUR, a lightweight control layer that regulates agent admission to bound aggregate cache pressure while preserving execution continuity. CONCUR adapts a cache-aware control algorithm to dynamically adjust the number of active agents using runtime cache signals.
  Across large models and real-world agent workloads, CONCUR prevents middle-phase thrashing and improves batch inference throughput by up to 4.09x on Qwen3-32B and 1.9x on DeepSeek-V3, while remaining compatible with existing LLM serving systems.

</details>


### [6] [AscendCraft: Automatic Ascend NPU Kernel Generation via DSL-Guided Transcompilation](https://arxiv.org/abs/2601.22760)
*Zhongzhen Wen,Shudi Shao,Zhong Li,Yu Ge,Tongtong Xu,Yuanyi Lin,Tian Zhang*

Main category: cs.DC

TL;DR: AscendCraft：一种DSL引导的自动AscendC内核生成方法，通过轻量级DSL抽象复杂性并建模Ascend执行语义，实现高编译成功率和功能正确性，部分内核性能超越PyTorch。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型性能依赖于高效内核实现，但为专用加速器（如NPU）开发高性能内核耗时且需要专业知识。虽然LLM能生成正确的GPU内核，但NPU内核生成因领域特定编程模型、有限公开示例和稀疏文档而未被充分探索，直接生成AscendC内核的正确率极低。

Method: 提出AscendCraft方法：1）引入轻量级DSL，抽象非必要复杂性并显式建模Ascend特定执行语义；2）使用类别特定专家示例在DSL中生成内核；3）通过结构化、约束驱动的LLM降低传递将DSL转编译为AscendC。

Result: 在MultiKernelBench的七个算子类别上评估：达到98.1%编译成功率和90.4%功能正确性；46.2%生成的内核匹配或超越PyTorch eager执行性能；成功为新提出的mHC架构生成两个正确内核，性能大幅超越PyTorch。

Conclusion: DSL引导的转编译方法能使LLM生成既正确又具有竞争力的NPU内核，解决了NPU内核生成的挑战，展示了方法的通用性和有效性。

Abstract: The performance of deep learning models critically depends on efficient kernel implementations, yet developing high-performance kernels for specialized accelerators remains time-consuming and expertise-intensive. While recent work demonstrates that large language models (LLMs) can generate correct and performant GPU kernels, kernel generation for neural processing units (NPUs) remains largely underexplored due to domain-specific programming models, limited public examples, and sparse documentation. Consequently, directly generating AscendC kernels with LLMs yields extremely low correctness, highlighting a substantial gap between GPU and NPU kernel generation.
  We present AscendCraft, a DSL-guided approach for automatic AscendC kernel generation. AscendCraft introduces a lightweight DSL that abstracts non-essential complexity while explicitly modeling Ascend-specific execution semantics. Kernels are first generated in the DSL using category-specific expert examples and then transcompiled into AscendC through structured, constraint-driven LLM lowering passes. Evaluated on MultiKernelBench across seven operator categories, AscendCraft achieves 98.1% compilation success and 90.4% functional correctness. Moreover, 46.2% of generated kernels match or exceed PyTorch eager execution performance, demonstrating that DSL-guided transcompilation can enable LLMs to generate both correct and competitive NPU kernels. Beyond benchmarks, AscendCraft further demonstrates its generality by successfully generating two correct kernels for newly proposed mHC architecture, achieving performance that substantially surpasses PyTorch eager execution.

</details>


### [7] [ERA: Epoch-Resolved Arbitration for Duelling Admins in Group Management CRDTs](https://arxiv.org/abs/2601.22963)
*Kegan Dougal*

Main category: cs.DC

TL;DR: CRDTs在分区时可能出现状态"回滚"，导致权限管理问题，如"决斗管理员"场景。文章提出通过异步"纪元事件"仲裁建立有界全序，增强一致性。


<details>
  <summary>Details</summary>
Motivation: CRDTs在强最终一致性下，当节点以不同顺序累积事件时，物化视图可能出现"回滚"现象。在权限管理场景（如即时通讯应用）中，这会导致意外行为，如"决斗管理员"问题：两个管理员同时撤销对方权限时，谁获胜？拜占庭管理员可能利用并发性赢得决斗。

Method: 提出通过外部仲裁器仲裁并发事件间的不可变happens-before关系。仲裁通过可选的"纪元事件"异步批量进行，保持可用性。这引入了纪元内的有界全序，产生"最终性"。

Result: 该方法在保持CRDTs可用性的同时，通过纪元事件仲裁提高了CRDTs所能提供的一致性水平，解决了并发事件导致的权限管理问题。

Conclusion: CRDTs在权限管理等敏感场景需要更强的顺序保证。通过异步纪元事件仲裁建立有界全序，可以在保持可用性的同时提供更好的最终性，防止拜占庭管理员利用并发性。

Abstract: Conflict-Free Replicated Data Types (CRDTs) are used in a range of fields for their coordination-free replication with strong eventual consistency. By prioritising availability over consistency under partition, nodes accumulate events in different orders, and rely on an associative, commutative and idempotent merge function to present a materialised view of the CRDT. Under some circumstances, the state of the materialised view over time can appear to ''roll back'' previously applied events. When the materialised view is used to manage group permissions such as ones found in instant messaging applications, this can lead to surprising behaviour. This can occur when there are multiple concurrent events, such as in the Duelling Admins problem where two equally permissioned admins concurrently revoke each other's permissions. Who wins? This article argues that a Byzantine admin can exploit concurrency to win the duel. As a result, an external arbiter is required to arbitrate an immutable happens-before relation between concurrent events. Arbitration occurs asynchronously in batches via optional ''epoch events'', preserving availability. This introduces a bounded total order within epochs, and the resulting ''finality'' improves on the level of consistency CRDTs can provide.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [RulePlanner: All-in-One Reinforcement Learner for Unifying Design Rules in 3D Floorplanning](https://arxiv.org/abs/2601.22476)
*Ruizhe Zhong,Xingbo Du,Junchi Yan*

Main category: cs.AR

TL;DR: 提出基于深度强化学习的统一框架，用于处理3D芯片布局中的复杂设计规则约束，通过矩阵表示、动作空间约束和奖励信号实现自动化布局优化。


<details>
  <summary>Details</summary>
Motivation: 随着技术节点缩小，3D多层堆叠芯片布局面临复杂设计规则约束的挑战，现有方法只能处理有限规则，其他规则违反需要专家手动调整，导致后处理工作繁重耗时。

Method: 提出统一的深度强化学习框架，包含三个关键组件：1) 设计规则的矩阵表示；2) 动作空间约束以过滤导致规则违反的无效动作；3) 约束满足度的定量分析作为奖励信号。

Result: 在公共基准测试中验证了方法的有效性和正确性，在未见电路上展示了良好的可迁移性，框架可扩展以适应新设计规则。

Conclusion: 该框架为未来芯片设计中的新兴挑战提供了灵活解决方案，能够统一处理多种硬件设计规则，减少人工干预，提高布局效率。

Abstract: Floorplanning determines the coordinate and shape of each module in Integrated Circuits. With the scaling of technology nodes, in floorplanning stage especially 3D scenarios with multiple stacked layers, it has become increasingly challenging to adhere to complex hardware design rules. Current methods are only capable of handling specific and limited design rules, while violations of other rules require manual and meticulous adjustment. This leads to labor-intensive and time-consuming post-processing for expert engineers. In this paper, we propose an all-in-one deep reinforcement learning-based approach to tackle these challenges, and design novel representations for real-world IC design rules that have not been addressed by previous approaches. Specifically, the processing of various hardware design rules is unified into a single framework with three key components: 1) novel matrix representations to model the design rules, 2) constraints on the action space to filter out invalid actions that cause rule violations, and 3) quantitative analysis of constraint satisfaction as reward signals. Experiments on public benchmarks demonstrate the effectiveness and validity of our approach. Furthermore, transferability is well demonstrated on unseen circuits. Our framework is extensible to accommodate new design rules, thus providing flexibility to address emerging challenges in future chip design. Code will be available at: https://github.com/Thinklab-SJTU/EDA-AI

</details>


### [9] [Design of a GPU with Heterogeneous Cores for Graphics](https://arxiv.org/abs/2601.22862)
*Aurora Tomás,Juan Luis Aragón,Joan Manuel Parcerisa,Antonio González*

Main category: cs.AR

TL;DR: KHEPRI提出了一种异构GPU架构，包含计算专用和内存专用两种核心，通过智能调度器根据图块特性动态分配任务，在保持数据局部性的同时提升图形应用性能。


<details>
  <summary>Details</summary>
Motivation: 图形应用中的场景具有多样性，不同区域的物体复杂度差异导致计算强度和内存带宽需求显著不同。传统同构GPU无法有效处理这种变异性，需要异构架构来优化不同类型的工作负载。

Method: 提出KHEPRI异构GPU架构，包含两种核心：优化高ILP的计算专用核心和容忍更多缓存未命中的内存专用核心。关键创新是智能调度器，利用帧间连贯性预测每个图块特性，动态分配任务到最合适的核心，同时保持数据局部性。

Result: 相比传统同构GPU，KHEPRI在商业动画图形应用中平均性能提升9.2%，帧率提升7.3%，GPU总能耗降低4.8%，且无需额外硬件开销。

Conclusion: 异构GPU架构能有效处理图形应用中的工作负载多样性，通过智能调度实现性能提升和能耗降低，证明了异构性在GPU设计中的价值。

Abstract: Heterogeneous architectures can deliver higher performance and energy efficiency than symmetric counterparts by using multiple architectures tuned to different types of workloads. While previous works focused on CPUs, this work extends the concept of heterogeneity to GPUs by proposing KHEPRI, a heterogeneous GPU architecture for graphics applications. Scenes in graphics applications showcase diversity, as they consist of many objects with varying levels of complexity. As a result, computational intensity and memory bandwidth requirements differ significantly across different regions of each scene. To address this variability, our proposal includes two types of cores: cores optimized for high ILP (compute-specialized) and cores that tolerate a higher number of simultaneously outstanding cache misses (memory-specialized). A key component of the proposed architecture is a novel work scheduler that dynamically assigns each part of a frame (i.e., a tile) to the most suitable core. Designing this scheduler is particularly challenging, as it must preserve data locality; otherwise, the benefits of heterogeneity may be offset by the penalty of additional cache misses. Additionally, the scheduler requires knowledge of each tile's characteristics before rendering it. For this purpose, KHEPRI leverages frame-to-frame coherence to predict the behavior of each tile based on that of the corresponding tile in the previous frame. Evaluations across a wide range of commercial animated graphics applications show that, compared to a traditional homogeneous GPU, KHEPRI achieves an average performance improvement of 9.2%, a throughput increase (frames per second) of 7.3%, and a total GPU energy reduction of 4.8%. Importantly, these benefits are achieved without any hardware overhead.

</details>


### [10] [Machine Learning for Energy-Performance-aware Scheduling](https://arxiv.org/abs/2601.23134)
*Zheyuan Hu,Yifei Shi*

Main category: cs.AR

TL;DR: 提出基于高斯过程的贝叶斯优化框架，用于在异构多核架构上自动搜索最优调度配置，平衡能耗与延迟的权衡，并通过敏感性分析提供模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 在后登纳德时代，嵌入式系统优化需要在能耗效率和延迟之间进行复杂权衡。传统启发式调优在高维、非平滑的优化空间中效率低下。

Method: 使用高斯过程的贝叶斯优化框架，通过近似帕累托前沿处理多目标优化问题，结合敏感性分析（fANOVA）比较不同协方差核函数（如Matérn与RBF），为黑盒模型提供物理可解释性。

Result: 框架能够自动搜索异构多核架构上的最优调度配置，揭示驱动系统性能的主导硬件参数，提供模型可解释性。

Conclusion: 提出的贝叶斯优化框架能够有效解决嵌入式系统在多维优化空间中的调度配置问题，通过敏感性分析增强了模型的可解释性，有助于理解硬件参数对系统性能的影响。

Abstract: In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gaussian Processes to automate the search for optimal scheduling configurations on heterogeneous multi-core architectures. We explicitly address the multi-objective nature of the problem by approximating the Pareto Frontier between energy and time. Furthermore, by incorporating Sensitivity Analysis (fANOVA) and comparing different covariance kernels (e.g., Matérn vs. RBF), we provide physical interpretability to the black-box model, revealing the dominant hardware parameters driving system performance.

</details>


### [11] [Toward Digital Twins in 3D IC Packaging: A Critical Review of Physics, Data, and Hybrid Architectures](https://arxiv.org/abs/2601.23226)
*Gourab Datta,Sarah Safura Sharif,Yaser Mike Banad*

Main category: cs.AR

TL;DR: 该综述论文澄清了3D IC数字孪生的概念层次，提出了结合物理建模、数据驱动和原位传感的统一混合架构，并制定了标准化路线图。


<details>
  <summary>Details</summary>
Motivation: 3D IC封装和异构集成面临热热点、翘曲应力和互连老化等多物理耦合挑战，需要超越传统离线计量学的实时可靠性管理。现有数字孪生文献存在概念模糊，混淆了静态多物理仿真与真正动态闭环孪生的区别。

Method: 通过三个具体贡献：1) 澄清数字孪生层次结构（数字模型、数字影子、数字孪生）；2) 综合三大使能技术（物理建模、数据驱动范式、原位传感）；3) 提出统一混合数字孪生架构，利用物理信息机器学习解决数据稀缺和延迟约束。

Result: 提出了一个标准化的路线图，整合IEEE 1451和UCIe协议，加速从被动数字影子向自主自优化数字孪生的转变，用于3D IC制造和现场操作。

Conclusion: 该综述通过澄清概念、综合技术和提出统一架构，为3D IC数字孪生提供了系统框架，旨在实现从被动监控到主动优化的转变，提升3D IC的可靠性和性能管理。

Abstract: Three-dimensional integrated circuit (3D IC) pack-aging and heterogeneous integration have emerged as central pillars of contemporary semiconductor scaling. Yet, the multi-physics coupling inherent to stacked architectures manifesting as thermal hot spots, warpage-induced stresses, and interconnect aging demands monitoring and control capabilities that surpass traditional offline metrology. Although Digital Twin (DT) technology provides a principled route to real-time reliability management, the existing literature remains fragmented and frequently blurs the distinction between static multiphysics simulation workflows and truly dynamic, closed-loop twins. This critical review distinguishes itself by addressing these deficiencies through three specific contributions. First, we clarify the Digital Twin hierarchy to resolve terminological ambiguity between digital models, shadows, and twins. Second, we synthesize three foundational enabling technologies: (1) physics-based modeling, emphasizing the shift from computationally intensive finite-element analysis (FEA) to real-time surrogate models; (2) data-driven paradigms, highlighting virtual metrology (VM) for inferring latent metrics; and (3) in-situ sensing, the nervous system coupling the physical stack to its virtual counterpart. Third, beyond a descriptive survey, we propose a unified hybrid DT architecture that leverages physics-informed machine learning (e.g., PINNs) to reconcile data scarcity with latency constraints. Finally, we outline a standards-aligned roadmap incorporating IEEE 1451 and UCIe protocols to accelerate the transition from passive digital shadows to autonomous, self-optimizing Digital Twins for 3D IC manufacturing and field operation.

</details>
