{"id": "2509.10711", "categories": ["cs.DC", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10711", "abs": "https://arxiv.org/abs/2509.10711", "authors": ["Subhajit Pramanick", "Saswata Jana", "Partha Sarathi Mandal", "Gokarna Sharma"], "title": "Asynchronous Gathering of Opaque Robots with Mobility Faults", "comment": "38 pages, 26 figures, and 1 table", "summary": "We consider the fundamental benchmarking problem of gathering in an\n$(N,f)$-fault system consisting of $N$ robots, of which at most $f$ might fail\nat any execution, under asynchrony. Two seminal results established\nimpossibility of a solution in the oblivious robot (OBLOT) model in a\n$(2,0)$-fault system under semi-synchrony and in a $(3,1)$-Byzantine fault\nsystem under asynchrony. Recently, a breakthrough result circumvented the first\nimpossibility result by giving a deterministic algorithm in a $(2,0)$-fault\nsystem under asynchrony in the luminous robot (LUMI) model using 2-colored\nlights. However, a breakthrough result established impossibility of gathering\nin a $(2,1)$-crash system in the LUMI model under semi-synchrony. In this\npaper, we consider a {\\em mobility fault} model in which a robot crash only\nimpacts it mobility but not the operation of the light.\n  We establish four results under asynchrony in LUMI with the mobility fault\nmodel. We show that it is impossible to solve gathering in a $(2,1)$-mobility\nfault system using 2-colored lights, and then give a solution using 3-colored\nlights, which is optimal w.r.t. the number of colors. We then consider an\n$(N,f)$-mobility fault system, $f<N$, both $N,f$ not known, and give two\ndeterministic algorithms that exhibit a nice time-color trade-off: The first\nwith time $O(N)$ using 7-colored lights and the second with time\n$O(\\max\\{\\ell,f\\})$ using 26-colored lights, where $\\ell< N$ is the number of\ndistinct convex layers of robot positions in the initial configuration.\nInterestingly, for $l, f = O(1)$, our result is optimal. Our algorithms for an\n$(N,f)$-mobility fault system are the first to be analysed time complexity, can\nwithstand obstructed visibility (opaque robot model) and asynchronous\nscheduling.", "AI": {"tldr": "\u672c\u6587\u5728\u5f02\u6b65LUMI\u6a21\u578b\u4e2d\u7814\u7a76\u79fb\u52a8\u6545\u969c\u4e0b\u7684\u673a\u5668\u4eba\u805a\u96c6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u989c\u8272\u6570\u91cf\u7684\u6700\u4f18\u7b97\u6cd5\u548c\u65f6\u95f4-\u989c\u8272\u6743\u8861\u7b97\u6cd5", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6545\u969c\u6a21\u578b\u4e0b\u673a\u5668\u4eba\u805a\u96c6\u7684\u4e0d\u53ef\u884c\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728(2,1)-\u6545\u969c\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u5f15\u5165\u79fb\u52a8\u6545\u969c\u6a21\u578b\u6765\u7a81\u7834\u73b0\u6709\u9650\u5236", "method": "\u4f7f\u7528LUMI\u6a21\u578b\uff08\u5e26\u706f\u5149\u7684\u673a\u5668\u4eba\uff09\uff0c\u63d0\u51fa\u4e24\u79cd\u786e\u5b9a\u6027\u7b97\u6cd5\uff1a\u4f7f\u75283\u8272\u706f\u7684\u6700\u4f18\u7b97\u6cd5\uff0c\u4ee5\u53ca\u4f7f\u75287\u8272\u548c26\u8272\u706f\u7684\u65f6\u95f4-\u989c\u8272\u6743\u8861\u7b97\u6cd5", "result": "\u8bc1\u660e\u4e86\u5728(2,1)-\u79fb\u52a8\u6545\u969c\u7cfb\u7edf\u4e2d\u4f7f\u75282\u8272\u706f\u4e0d\u53ef\u884c\uff0c\u4f46\u4f7f\u75283\u8272\u706f\u53ef\u884c\u4e14\u6700\u4f18\uff1b\u63d0\u51fa\u4e86\u5728(N,f)-\u7cfb\u7edf\u4e2d\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u5206\u522b\u4e3aO(N)\u548cO(max{l,f})", "conclusion": "\u79fb\u52a8\u6545\u969c\u6a21\u578b\u4e3a\u673a\u5668\u4eba\u805a\u96c6\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u9014\u5f84\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u989c\u8272\u6570\u91cf\u548c\u65f6\u95f4\u590d\u6742\u5ea6\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u6700\u4f18\u6216\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd"}}
{"id": "2509.10712", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10712", "abs": "https://arxiv.org/abs/2509.10712", "authors": ["Rahma Nouaji", "Stella Bitchebe", "Ricardo Macedo", "Oana Balmau"], "title": "MinatoLoader: Accelerating Machine Learning Training Through Efficient Data Preprocessing", "comment": "Paper accepted at EuroSys 2026 (will be updated after the\n  camera-ready)", "summary": "Data loaders are used by Machine Learning (ML) frameworks like PyTorch and\nTensorFlow to apply transformations to data before feeding it into the\naccelerator. This operation is called data preprocessing. Data preprocessing\nplays an important role in the ML training workflow because if it is\ninefficiently pipelined with the training, it can yield high GPU idleness,\nresulting in important training delays. Unfortunately, existing data loaders\nturn out to waste GPU resources, with $76\\%$ GPU idleness when using the\nPyTorch data loader, for example. One key source of inefficiency is the\nvariability in preprocessing time across samples within the same dataset.\nExisting data loaders are oblivious to this variability, and they construct\nbatches without any consideration of slow or fast samples. In this case, the\nentire batch is delayed by a single slow sample, stalling the training pipeline\nand resulting in head-of-line blocking.\n  To address these inefficiencies, we present MinatoLoader, a general-purpose\ndata loader for PyTorch that accelerates training and improves GPU utilization.\nMinatoLoader is designed for a single-server setup, containing multiple GPUs.\nIt continuously prepares data in the background and actively constructs batches\nby prioritizing fast-to-preprocess samples, while slower samples are processed\nin parallel.\n  We evaluate MinatoLoader on servers with V100 and A100 GPUs. On a machine\nwith four A100 GPUs, MinatoLoader improves the training time of a wide range of\nworkloads by up to $7.5\\times$ ($3.6\\times$ on average) over PyTorch DataLoader\nand Pecan, and up to $3\\times$ ($2.2\\times$ on average) over DALI. It also\nincreases average GPU utilization from 46.4\\% with PyTorch to 90.45\\%, while\npreserving model accuracy and enabling faster convergence.", "AI": {"tldr": "MinatoLoader\u662f\u4e00\u4e2a\u9488\u5bf9PyTorch\u7684\u901a\u7528\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u901a\u8fc7\u4f18\u5148\u5904\u7406\u9884\u5904\u7406\u901f\u5ea6\u5feb\u7684\u6837\u672c\u5e76\u5e76\u884c\u5904\u7406\u6162\u6837\u672c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u52a0\u8f7d\u5668\u56e0\u9884\u5904\u7406\u65f6\u95f4\u5dee\u5f02\u5bfc\u81f4\u7684GPU\u7a7a\u95f2\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u901f\u5ea6\u548cGPU\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u52a0\u8f7d\u5668\uff08\u5982PyTorch DataLoader\uff09\u5728\u5904\u7406\u5177\u6709\u9884\u5904\u7406\u65f6\u95f4\u5dee\u5f02\u7684\u6570\u636e\u6837\u672c\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u4f1a\u5bfc\u81f4\u9ad8\u8fbe76%\u7684GPU\u7a7a\u95f2\u65f6\u95f4\uff0c\u56e0\u4e3a\u6574\u4e2a\u6279\u6b21\u7684\u5904\u7406\u4f1a\u88ab\u5355\u4e2a\u6162\u6837\u672c\u5ef6\u8fdf\uff0c\u9020\u6210\u8bad\u7ec3\u7ba1\u9053\u963b\u585e\u3002", "method": "MinatoLoader\u5728\u5355\u670d\u52a1\u5668\u591aGPU\u8bbe\u7f6e\u4e2d\uff0c\u901a\u8fc7\u5728\u540e\u53f0\u6301\u7eed\u51c6\u5907\u6570\u636e\u5e76\u4e3b\u52a8\u6784\u5efa\u6279\u6b21\uff0c\u4f18\u5148\u5904\u7406\u9884\u5904\u7406\u901f\u5ea6\u5feb\u7684\u6837\u672c\uff0c\u540c\u65f6\u5e76\u884c\u5904\u7406\u8f83\u6162\u7684\u6837\u672c\uff0c\u4ee5\u907f\u514d\u5934\u90e8\u963b\u585e\u95ee\u9898\u3002", "result": "\u5728\u914d\u5907\u56db\u4e2aA100 GPU\u7684\u673a\u5668\u4e0a\uff0cMinatoLoader\u76f8\u6bd4PyTorch DataLoader\u548cPecan\u5c06\u8bad\u7ec3\u65f6\u95f4\u63d0\u5347\u4e86\u6700\u9ad87.5\u500d\uff08\u5e73\u57473.6\u500d\uff09\uff0c\u76f8\u6bd4DALI\u63d0\u5347\u4e86\u6700\u9ad83\u500d\uff08\u5e73\u57472.2\u500d\uff09\uff0c\u5e76\u5c06\u5e73\u5747GPU\u5229\u7528\u7387\u4ece46.4%\u63d0\u5347\u81f390.45%\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u5e76\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u3002", "conclusion": "MinatoLoader\u901a\u8fc7\u667a\u80fd\u7684\u6837\u672c\u4f18\u5148\u7ea7\u8c03\u5ea6\u548c\u5e76\u884c\u5904\u7406\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u9884\u5904\u7406\u4e2d\u7684\u6548\u7387\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u7684\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2509.10719", "categories": ["cs.DC", "cs.AR", "cs.LG", "cs.PF", "68M20, 68T05", "C.1.2; C.1.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.10719", "abs": "https://arxiv.org/abs/2509.10719", "authors": ["Mohammed Humaid Siddiqui", "Fernando Guzman", "Yufei Wu", "Ruishu Ann"], "title": "Coordinated Reinforcement Learning Prefetching Architecture for Multicore Systems", "comment": "47 pages, 12 figures, technical report prepared at Fairleigh\n  Dickinson University", "summary": "Hardware prefetching is critical to fill the performance gap between CPU\nspeeds and slower memory accesses. With multicore architectures becoming\ncommonplace, traditional prefetchers are severely challenged. Independent core\noperation creates significant redundancy (up to 20% of prefetch requests are\nduplicates), causing unnecessary memory bus traffic and wasted bandwidth.\nFurthermore, cutting-edge prefetchers such as Pythia suffer from about a 10%\nperformance loss when scaling from a single-core to a four-core system. To\nsolve these problems, we propose CRL-Pythia, a coordinated reinforcement\nlearning based prefetcher specifically designed for multicore systems. In this\nwork, CRL-Pythia addresses these issues by enabling cross-core sharing of\ninformation and cooperative prefetching decisions, which greatly reduces\nredundant prefetch requests and improves learning convergence across cores. Our\nexperiments demonstrate that CRL-Pythia outperforms single Pythia\nconfigurations in all cases, with approximately 12% IPC (instructions per\ncycle) improvement for bandwidth-constrained workloads, while imposing moderate\nhardware overhead. Our sensitivity analyses also verify its robustness and\nscalability, thereby making CRL-Pythia a practical and efficient solution to\ncontemporary multicore systems.", "AI": {"tldr": "CRL-Pythia\u662f\u4e00\u4e2a\u57fa\u4e8e\u534f\u8c03\u5f3a\u5316\u5b66\u4e60\u7684\u786c\u4ef6\u9884\u53d6\u5668\uff0c\u4e13\u95e8\u4e3a\u591a\u6838\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8de8\u6838\u4fe1\u606f\u5171\u4eab\u548c\u534f\u4f5c\u9884\u53d6\u51b3\u7b56\uff0c\u663e\u8457\u51cf\u5c11\u5197\u4f59\u9884\u53d6\u8bf7\u6c42\u5e76\u63d0\u9ad8\u5b66\u4e60\u6536\u655b\u6027\u3002", "motivation": "\u591a\u6838\u67b6\u6784\u4e0b\u4f20\u7edf\u9884\u53d6\u5668\u9762\u4e34\u4e25\u91cd\u6311\u6218\uff1a\u72ec\u7acb\u6838\u5fc3\u64cd\u4f5c\u5bfc\u81f4\u5927\u91cf\u5197\u4f59\u9884\u53d6\u8bf7\u6c42\uff08\u9ad8\u8fbe20%\u7684\u91cd\u590d\u8bf7\u6c42\uff09\uff0c\u9020\u6210\u4e0d\u5fc5\u8981\u7684\u5185\u5b58\u603b\u7ebf\u6d41\u91cf\u548c\u5e26\u5bbd\u6d6a\u8d39\uff1b\u5148\u8fdb\u9884\u53d6\u5668\u5982Pythia\u5728\u4ece\u5355\u6838\u6269\u5c55\u5230\u56db\u6838\u7cfb\u7edf\u65f6\u6027\u80fd\u635f\u5931\u7ea610%\u3002", "method": "\u63d0\u51faCRL-Pythia\u534f\u8c03\u5f3a\u5316\u5b66\u4e60\u9884\u53d6\u5668\uff0c\u901a\u8fc7\u8de8\u6838\u4fe1\u606f\u5171\u4eab\u548c\u534f\u4f5c\u9884\u53d6\u51b3\u7b56\u673a\u5236\uff0c\u51cf\u5c11\u5197\u4f59\u9884\u53d6\u8bf7\u6c42\u5e76\u6539\u5584\u591a\u6838\u95f4\u7684\u5b66\u4e60\u6536\u655b\u6027\u3002", "result": "CRL-Pythia\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u4f18\u4e8e\u5355Pythia\u914d\u7f6e\uff0c\u5728\u5e26\u5bbd\u53d7\u9650\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5b9e\u73b0\u7ea612%\u7684IPC\uff08\u6bcf\u5468\u671f\u6307\u4ee4\u6570\uff09\u63d0\u5347\uff0c\u540c\u65f6\u786c\u4ef6\u5f00\u9500\u9002\u4e2d\u3002\u654f\u611f\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "CRL-Pythia\u662f\u5f53\u4ee3\u591a\u6838\u7cfb\u7edf\u7684\u4e00\u4e2a\u5b9e\u7528\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u534f\u8c03\u5b66\u4e60\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6838\u73af\u5883\u4e0b\u7684\u9884\u53d6\u5197\u4f59\u548c\u6027\u80fd\u635f\u5931\u95ee\u9898\u3002"}}
{"id": "2509.10803", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.10803", "abs": "https://arxiv.org/abs/2509.10803", "authors": ["Nafees Iqbal", "Jed Brown"], "title": "Enhancing Type Safety in MPI with Rust: A Statically Verified Approach for RSMPI", "comment": null, "summary": "The Message Passing Interface (MPI) is a fundamental tool for building\nhigh-performance computing (HPC) applications, enabling efficient communication\nacross distributed systems. Despite its widespread adoption, MPI's low-level\ninterface and lack of built-in type safety make it prone to runtime errors,\nundefined behavior, and debugging challenges, especially in large-scale\napplications. Rust, a modern systems programming language, offers a compelling\nsolution with its strong type system, which enforces memory and type safety at\ncompile time without compromising performance. This paper introduces a\ntype-safe communication framework for MPI, built on the RSMPI library, to\naddress the limitations of traditional MPI programming. At its core is the\nTypedCommunicator, an abstraction that enforces static type safety in\npoint-to-point communication operations. By leveraging Rust's Equivalence\ntrait, our framework guarantees that only compatible types can participate in\ncommunication, catching mismatches either at compile time or through runtime\nvalidation. The framework supports both single-value and slice-based\ncommunication, providing an intuitive API for diverse data structures. Our\nimplementation demonstrates that this approach eliminates common MPI errors,\nimproves developer productivity, and maintains performance, adhering to Rust's\nprinciple of zero-cost abstractions. This work lays the foundation for\nextending type safety to collective operations, advancing the robustness of\nparallel computing in Rust.", "AI": {"tldr": "\u57fa\u4e8eRust\u8bed\u8a00\u7684RSMPI\u5e93\u6784\u5efa\u7c7b\u578b\u5b89\u5168\u7684MPI\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7TypedCommunicator\u5f3a\u5236\u9759\u6001\u7c7b\u578b\u5b89\u5168\uff0c\u6d88\u9664MPI\u7f16\u7a0b\u4e2d\u7684\u8fd0\u884c\u65f6\u9519\u8bef\u548c\u7c7b\u578b\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3MPI\u4f4e\u7ea7\u63a5\u53e3\u7f3a\u4e4f\u7c7b\u578b\u5b89\u5168\u5bfc\u81f4\u7684\u8fd0\u884c\u65f6\u9519\u8bef\u3001\u672a\u5b9a\u4e49\u884c\u4e3a\u548c\u8c03\u8bd5\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u3002", "method": "\u57fa\u4e8eRSMPI\u5e93\u6784\u5efaTypedCommunicator\u62bd\u8c61\uff0c\u5229\u7528Rust\u7684Equivalence trait\u5f3a\u5236\u7c7b\u578b\u5b89\u5168\uff0c\u652f\u6301\u5355\u503c\u548c\u5207\u7247\u901a\u4fe1\uff0c\u63d0\u4f9b\u76f4\u89c2API\u3002", "result": "\u8be5\u6846\u67b6\u6d88\u9664\u4e86\u5e38\u89c1MPI\u9519\u8bef\uff0c\u63d0\u9ad8\u5f00\u53d1\u8005\u751f\u4ea7\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u9075\u5faa\u96f6\u6210\u672c\u62bd\u8c61\u539f\u5219\u3002", "conclusion": "\u4e3a\u6269\u5c55\u7c7b\u578b\u5b89\u5168\u5230\u96c6\u4f53\u64cd\u4f5c\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u8fdbRust\u5e76\u884c\u8ba1\u7b97\u7684\u7a33\u5065\u6027\u3002"}}
{"id": "2509.10627", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.10627", "abs": "https://arxiv.org/abs/2509.10627", "authors": ["Yu-Hong Lai", "Chieh-Lin Tsai", "Wen Sheng Lim", "Han-Wen Hu", "Tei-Wei Kuo", "Yuan-Hao Chang"], "title": "ReCross: Efficient Embedding Reduction Scheme for In-Memory Computing using ReRAM-Based Crossbar", "comment": null, "summary": "Deep learning-based recommendation models (DLRMs) are widely deployed in\ncommercial applications to enhance user experience. However, the large and\nsparse embedding layers in these models impose substantial memory bandwidth\nbottlenecks due to high memory access costs and irregular access patterns,\nleading to increased inference time and energy consumption. While resistive\nrandom access memory (ReRAM) based crossbars offer a fast and energy-efficient\nsolution through in-memory embedding reduction operations, naively mapping\nembeddings onto crossbar arrays leads to poor crossbar utilization and thus\ndegrades performance. We present ReCross, an efficient ReRAM-based in-memory\ncomputing (IMC) scheme designed to minimize execution time and enhance energy\nefficiency in DLRM embedding reduction. ReCross co-optimizes embedding access\npatterns and ReRAM crossbar characteristics by intelligently grouping and\nmapping co-occurring embeddings, replicating frequently accessed embeddings\nacross crossbars, and dynamically selecting in-memory processing operations\nusing a newly designed dynamic switch ADC circuit that considers runtime energy\ntrade-offs. Experimental results demonstrate that ReCross achieves a 3.97x\nreduction in execution time and a 6.1x improvement in energy efficiency\ncompared to state-of-the-art IMC approaches.", "AI": {"tldr": "ReCross\u662f\u4e00\u79cd\u57fa\u4e8eReRAM\u7684\u5185\u5b58\u8ba1\u7b97\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u5d4c\u5165\u8bbf\u95ee\u6a21\u5f0f\u548c\u4ea4\u53c9\u9635\u5217\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u63a8\u8350\u6a21\u578b\u7684\u5d4c\u5165\u51cf\u5c11\u64cd\u4f5c\u6027\u80fd", "motivation": "\u6df1\u5ea6\u63a8\u8350\u6a21\u578b\u4e2d\u7684\u5927\u578b\u7a00\u758f\u5d4c\u5165\u5c42\u5b58\u5728\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\uff0c\u5bfc\u81f4\u63a8\u7406\u65f6\u95f4\u589e\u52a0\u548c\u80fd\u8017\u4e0a\u5347\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u901a\u8fc7\u667a\u80fd\u5206\u7ec4\u548c\u6620\u5c04\u5171\u73b0\u5d4c\u5165\u3001\u5728\u4ea4\u53c9\u9635\u5217\u95f4\u590d\u5236\u9891\u7e41\u8bbf\u95ee\u7684\u5d4c\u5165\u3001\u4f7f\u7528\u52a8\u6001\u5f00\u5173ADC\u7535\u8def\u52a8\u6001\u9009\u62e9\u5185\u5b58\u5904\u7406\u64cd\u4f5c", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5185\u5b58\u8ba1\u7b97\u65b9\u6cd5\uff0c\u6267\u884c\u65f6\u95f4\u51cf\u5c113.97\u500d\uff0c\u80fd\u6548\u63d0\u53476.1\u500d", "conclusion": "ReCross\u901a\u8fc7\u534f\u540c\u4f18\u5316\u5d4c\u5165\u8bbf\u95ee\u6a21\u5f0f\u548cReRAM\u4ea4\u53c9\u9635\u5217\u7279\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u63a8\u8350\u6a21\u578b\u5d4c\u5165\u51cf\u5c11\u64cd\u4f5c\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898"}}
{"id": "2509.11418", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.11418", "abs": "https://arxiv.org/abs/2509.11418", "authors": ["Runming Li", "Yue Yao", "Robert Harper"], "title": "Mechanizing Synthetic Tait Computability in Istari", "comment": null, "summary": "Categorical gluing is a powerful technique for proving meta-theorems of type\ntheories such as canonicity and normalization. Synthetic Tait Computability\n(STC) provides an abstract treatment of the complex gluing models by\ninternalizing the gluing category into a modal dependent type theory with a\nphase distinction. This work presents a mechanization of STC in the Istari\nproof assistant. Istari is a Martin-L\\\"{o}f-style extensional type theory with\nequality reflection. Equality reflection eliminates the nuisance of transport\nreasoning typically found in intensional proof assistants. This work develops a\nreusable library for synthetic phase distinction, including modalities,\nextension types, and strict glue types, and applies it to two case studies: (1)\na canonicity model for dependent type theory with dependent products and\nbooleans with large elimination, and (2) a Kripke canonicity model for the\ncost-aware logical framework. Our results demonstrate that the core STC\nconstructions can be formalized essentially verbatim in Istari, preserving the\nelegance of the on-paper arguments while ensuring machine-checked correctness.", "AI": {"tldr": "\u5728Istari\u8bc1\u660e\u52a9\u624b\u4e2d\u5b9e\u73b0\u4e86\u5408\u6210Tait\u53ef\u8ba1\u7b97\u6027(STC)\u7684\u5f62\u5f0f\u5316\uff0c\u5305\u62ec\u6a21\u6001\u3001\u6269\u5c55\u7c7b\u578b\u548c\u4e25\u683c\u80f6\u5408\u7c7b\u578b\u7b49\u53ef\u91cd\u7528\u5e93\uff0c\u5e76\u5e94\u7528\u4e8e\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\uff1a\u5e26\u5927\u6d88\u9664\u7684\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\u7684\u5178\u8303\u6027\u6a21\u578b\u548c\u6210\u672c\u611f\u77e5\u903b\u8f91\u6846\u67b6\u7684Kripke\u5178\u8303\u6027\u6a21\u578b", "motivation": "\u8303\u7574\u80f6\u5408\u662f\u8bc1\u660e\u7c7b\u578b\u7406\u8bba\u5143\u5b9a\u7406(\u5982\u5178\u8303\u6027\u548c\u6b63\u89c4\u5316)\u7684\u5f3a\u5927\u6280\u672f\uff0cSTC\u901a\u8fc7\u5c06\u80f6\u5408\u8303\u7574\u5185\u5316\u5230\u5177\u6709\u76f8\u4f4d\u533a\u5206\u7684\u6a21\u6001\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\u4e2d\uff0c\u4e3a\u590d\u6742\u80f6\u5408\u6a21\u578b\u63d0\u4f9b\u62bd\u8c61\u5904\u7406\u3002\u672c\u7814\u7a76\u65e8\u5728\u5728Istari\u4e2d\u673a\u68b0\u5316STC", "method": "\u5728Istari\u8bc1\u660e\u52a9\u624b\u4e2d\u5f00\u53d1\u53ef\u91cd\u7528\u7684\u5408\u6210\u76f8\u4f4d\u533a\u5206\u5e93\uff0c\u5305\u62ec\u6a21\u6001\u3001\u6269\u5c55\u7c7b\u578b\u548c\u4e25\u683c\u80f6\u5408\u7c7b\u578b\uff0c\u7136\u540e\u5c06STC\u6838\u5fc3\u6784\u9020\u5f62\u5f0f\u5316\uff0c\u5e76\u5e94\u7528\u4e8e\u4e24\u4e2a\u5177\u4f53\u6848\u4f8b\u7814\u7a76", "result": "\u6210\u529f\u5728Istari\u4e2d\u5b9e\u73b0\u4e86STC\u7684\u5f62\u5f0f\u5316\uff0c\u6838\u5fc3\u6784\u9020\u53ef\u4ee5\u51e0\u4e4e\u9010\u5b57\u5730\u5f62\u5f0f\u5316\uff0c\u4fdd\u6301\u4e86\u7eb8\u4e0a\u8bba\u8bc1\u7684\u4f18\u96c5\u6027\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u673a\u5668\u68c0\u67e5\u7684\u6b63\u786e\u6027", "conclusion": "Istari\u7684\u7b49\u5f0f\u53cd\u5c04\u7279\u6027\u6d88\u9664\u4e86\u5185\u6db5\u8bc1\u660e\u52a9\u624b\u4e2d\u5178\u578b\u7684\u4f20\u8f93\u63a8\u7406\u9ebb\u70e6\uff0c\u4f7f\u5f97STC\u7684\u673a\u68b0\u5316\u65e2\u4fdd\u6301\u4e86\u6570\u5b66\u4f18\u96c5\u6027\u53c8\u83b7\u5f97\u4e86\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u53ef\u9760\u6027"}}
{"id": "2509.11076", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.11076", "abs": "https://arxiv.org/abs/2509.11076", "authors": ["Zibo Wang", "Yuhang Zhou", "Zhibin Wang", "Shipeng Li", "Xinjing Huang", "Chendong Cai", "Bingxu Mu", "Yuqing Sun", "Zhiheng Hu", "Bin She", "Shu You", "Guanghuan Fang", "Rong Gu", "Wanchun Dou", "Guihai Chen", "Chen Tian"], "title": "Chameleon: Taming Dynamic Operator Sequences for Memory-Intensive LLM Training", "comment": null, "summary": "The increasing size of large language models (LLMs) has led to a surge in\nmemory requirements during training, often exceeding the capacity of\nhigh-bandwidth memory (HBM). Swap-based memory optimization incurs neither\naccuracy loss nor additional end-to-end overhead when effectively overlapped,\nthus being an attractive solution. However, existing swap methods assume\nconsistent operator sequences, which is impractical in Eager Mode, where\noperator sequences can vary during change.\n  We propose Chameleon, which redesigns the end-to-end process of swap-based\nmemory optimization and is the first work to consider varying operator\nsequences in Eager Mode. Chameleon (i) introduces a lightweight online profiler\nto enable continuous profiling for monitoring operator sequences, (ii)\ngenerates effective swap policies with limited operator information, and (iii)\noptimizes the policy execution module for accurate policy application and\nbetter performance. Experimental results demonstrate that Chameleon reduces\nprofiling overhead by 84.25%, enables training models up to 4x larger than\nhardware memory while adapting to changes in operator sequences, improves\nperformance by up to 38.94% compared to recomputation or high-degree\nparallelism.", "AI": {"tldr": "Chameleon\u662f\u4e00\u79cd\u9488\u5bf9Eager Mode\u4e0b\u53d8\u957f\u64cd\u4f5c\u5e8f\u5217\u7684swap\u5185\u5b58\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5728\u7ebf\u5206\u6790\u5668\u548c\u4f18\u5316\u7b56\u7565\u6267\u884c\u6a21\u5757\uff0c\u663e\u8457\u964d\u4f4e\u5206\u6790\u5f00\u9500\u5e76\u63d0\u5347\u6027\u80fd", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65f6\u5185\u5b58\u9700\u6c42\u6fc0\u589e\uff0c\u73b0\u6709swap\u65b9\u6cd5\u5047\u8bbe\u64cd\u4f5c\u5e8f\u5217\u4e00\u81f4\uff0c\u4f46\u5728Eager Mode\u4e2d\u64cd\u4f5c\u5e8f\u5217\u4f1a\u53d8\u5316\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u91cd\u65b0\u8bbe\u8ba1swap\u5185\u5b58\u4f18\u5316\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7\u5728\u7ebf\u5206\u6790\u5668\u8fdb\u884c\u6301\u7eed\u76d1\u63a7\uff0c\u751f\u6210\u6709\u9650\u64cd\u4f5c\u4fe1\u606f\u4e0b\u7684\u6709\u6548swap\u7b56\u7565\uff0c\u4f18\u5316\u7b56\u7565\u6267\u884c\u6a21\u5757", "result": "\u5206\u6790\u5f00\u9500\u964d\u4f4e84.25%\uff0c\u652f\u6301\u8bad\u7ec3\u6bd4\u786c\u4ef6\u5185\u5b58\u59274\u500d\u7684\u6a21\u578b\uff0c\u9002\u5e94\u64cd\u4f5c\u5e8f\u5217\u53d8\u5316\uff0c\u6027\u80fd\u6bd4\u91cd\u8ba1\u7b97\u6216\u9ad8\u5ea6\u5e76\u884c\u5316\u63d0\u534738.94%", "conclusion": "Chameleon\u662f\u9996\u4e2a\u8003\u8651Eager Mode\u53d8\u957f\u64cd\u4f5c\u5e8f\u5217\u7684swap\u4f18\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u5b58\u4f18\u5316\u6548\u679c"}}
{"id": "2509.10702", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10702", "abs": "https://arxiv.org/abs/2509.10702", "authors": ["Charles Hong", "Qijing Huang", "Grace Dinh", "Mahesh Subedar", "Yakun Sophia Shao"], "title": "DOSA: Differentiable Model-Based One-Loop Search for DNN Accelerators", "comment": "Published at MICRO 2023", "summary": "In the hardware design space exploration process, it is critical to optimize\nboth hardware parameters and algorithm-to-hardware mappings. Previous work has\nlargely approached this simultaneous optimization problem by separately\nexploring the hardware design space and the mapspace - both individually large\nand highly nonconvex spaces - independently. The resulting combinatorial\nexplosion has created significant difficulties for optimizers.\n  In this paper, we introduce DOSA, which consists of differentiable\nperformance models and a gradient descent-based optimization technique to\nsimultaneously explore both spaces and identify high-performing design points.\nExperimental results demonstrate that DOSA outperforms random search and\nBayesian optimization by 2.80x and 12.59x, respectively, in improving DNN model\nenergy-delay product, given a similar number of samples. We also demonstrate\nthe modularity and flexibility of DOSA by augmenting our analytical model with\na learned model, allowing us to optimize buffer sizes and mappings of a real\nDNN accelerator and attain a 1.82x improvement in energy-delay product.", "AI": {"tldr": "DOSA\u662f\u4e00\u4e2a\u7528\u4e8e\u786c\u4ef6\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u7684\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u6027\u80fd\u6a21\u578b\u540c\u65f6\u4f18\u5316\u786c\u4ef6\u53c2\u6570\u548c\u7b97\u6cd5\u5230\u786c\u4ef6\u7684\u6620\u5c04\uff0c\u663e\u8457\u4f18\u4e8e\u968f\u673a\u641c\u7d22\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u786c\u4ef6\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u9700\u8981\u540c\u65f6\u4f18\u5316\u786c\u4ef6\u53c2\u6570\u548c\u7b97\u6cd5\u5230\u786c\u4ef6\u7684\u6620\u5c04\uff0c\u4f46\u8fd9\u4e24\u4e2a\u7a7a\u95f4\u90fd\u662f\u9ad8\u5ea6\u975e\u51f8\u7684\uff0c\u4f20\u7edf\u65b9\u6cd5\u5206\u522b\u63a2\u7d22\u5bfc\u81f4\u7ec4\u5408\u7206\u70b8\u95ee\u9898\uff0c\u7ed9\u4f18\u5316\u5668\u5e26\u6765\u5de8\u5927\u56f0\u96be\u3002", "method": "\u63d0\u51faDOSA\u6846\u67b6\uff0c\u5305\u542b\u53ef\u5fae\u5206\u6027\u80fd\u6a21\u578b\u548c\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u4f18\u5316\u6280\u672f\uff0c\u80fd\u591f\u540c\u65f6\u63a2\u7d22\u786c\u4ef6\u8bbe\u8ba1\u7a7a\u95f4\u548c\u6620\u5c04\u7a7a\u95f4\uff0c\u8fd8\u652f\u6301\u901a\u8fc7\u589e\u5f3a\u5b66\u4e60\u6a21\u578b\u6765\u4f18\u5316\u7f13\u51b2\u533a\u5927\u5c0f\u548c\u6620\u5c04\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDOSA\u5728\u6539\u5584DNN\u6a21\u578b\u80fd\u91cf\u5ef6\u8fdf\u79ef\u65b9\u9762\u6bd4\u968f\u673a\u641c\u7d22\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u5206\u522b\u63d0\u53472.80\u500d\u548c12.59\u500d\u3002\u901a\u8fc7\u589e\u5f3a\u5b66\u4e60\u6a21\u578b\u4f18\u5316\u771f\u5b9eDNN\u52a0\u901f\u5668\uff0c\u5b9e\u73b0\u4e861.82\u500d\u7684\u80fd\u91cf\u5ef6\u8fdf\u79ef\u6539\u8fdb\u3002", "conclusion": "DOSA\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u540c\u65f6\u4f18\u5316\u786c\u4ef6\u8bbe\u8ba1\u548c\u6620\u5c04\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u5efa\u6a21\u548c\u68af\u5ea6\u4e0b\u964d\u6280\u672f\u89e3\u51b3\u4e86\u7ec4\u5408\u7206\u70b8\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u6a21\u5757\u5316\u548c\u7075\u6d3b\u6027\uff0c\u5728\u786c\u4ef6\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2509.11901", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.11901", "abs": "https://arxiv.org/abs/2509.11901", "authors": ["Kentaro Kobayashi", "Yukiyoshi Kameyama"], "title": "Expressive Power of One-Shot Control Operators and Coroutines", "comment": "Full version of the paper accepted at APLAS 2025. Includes appendices\n  with proofs. 59 pages", "summary": "Control operators, such as exceptions and effect handlers, provide a means of\nrepresenting computational effects in programs abstractly and modularly. While\nmost theoretical studies have focused on multi-shot control operators, one-shot\ncontrol operators -- which restrict the use of captured continuations to at\nmost once -- are gaining attention for their balance between expressiveness and\nefficiency. This study aims to fill the gap. We present a mathematically\nrigorous comparison of the expressive power among one-shot control operators,\nincluding effect handlers, delimited continuations, and even asymmetric\ncoroutines. Following previous studies on multi-shot control operators, we\nadopt Felleisen's macro-expressiveness as our measure of expressiveness. We\nverify the folklore that one-shot effect handlers and one-shot\ndelimited-control operators can be macro-expressed by asymmetric coroutines,\nbut not vice versa. We explain why a previous informal argument fails, and how\nto revise it to make a valid macro-translation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u6570\u5b66\u4e25\u683c\u7684\u5b8f\u8868\u8fbe\u529b\u5206\u6790\uff0c\u8bc1\u5b9e\u4e86\u4e00\u952e\u63a7\u5236\u64cd\u4f5c\u7b26\u4e2d\u4e0d\u5bf9\u79f0\u534f\u7a0b\u6bd4\u6548\u679c\u5904\u7406\u5668\u548c\u754c\u5b9a\u7eed\u65bd\u66f4\u5f3a\u5927\u7684\u6c1b\u95f4\u5047\u8bf4", "motivation": "\u586b\u8865\u4e00\u952e\u63a7\u5236\u64cd\u4f5c\u7b26\u5728\u8868\u8fbe\u529b\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5bf9\u6bd4\u4e0d\u540c\u4e00\u952e\u63a7\u5236\u64cd\u4f5c\u7b26\u7684\u8868\u8fbe\u529b\u5f3a\u5ea6", "method": "\u91c7\u7528Felleisen\u7684\u5b8f\u8868\u8fbe\u529b\u4f5c\u4e3a\u8868\u8fbe\u529b\u5ea6\u8861\u91cf\u6807\u51c6\uff0c\u5bf9\u4e00\u952e\u6548\u679c\u5904\u7406\u5668\u3001\u754c\u5b9a\u7eed\u65bd\u548c\u4e0d\u5bf9\u79f0\u534f\u7a0b\u8fdb\u884c\u6570\u5b66\u4e25\u683c\u7684\u5bf9\u6bd4\u5206\u6790", "result": "\u9a8c\u8bc1\u4e86\u4e0d\u5bf9\u79f0\u534f\u7a0b\u53ef\u4ee5\u5b8f\u8868\u8fbe\u4e00\u952e\u6548\u679c\u5904\u7406\u5668\u548c\u754c\u5b9a\u7eed\u65bd\uff0c\u4f46\u53cd\u4e4b\u5219\u4e0d\u884c\uff0c\u5e76\u4fee\u6b63\u4e86\u4e4b\u524d\u975e\u6b63\u5f0f\u8bba\u8bc1\u7684\u9519\u8bef", "conclusion": "\u4e00\u952e\u63a7\u5236\u64cd\u4f5c\u7b26\u5728\u8868\u8fbe\u529b\u4e0a\u5b58\u5728\u660e\u663e\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u4e0d\u5bf9\u79f0\u534f\u7a0b\u5177\u6709\u6700\u5f3a\u7684\u8868\u8fbe\u529b\uff0c\u8fd9\u4e3a\u7a0b\u5e8f\u8bed\u8a00\u8bbe\u8ba1\u548c\u6548\u679c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u57fa\u7840"}}
{"id": "2509.11134", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.11134", "abs": "https://arxiv.org/abs/2509.11134", "authors": ["Jiaang Duan", "Shenglin Xu", "Shiyou Qian", "Dingyu Yang", "Kangjin Wang", "Chenzhi Liao", "Yinghao Yu", "Qin Hua", "Hanwen Hu", "Qi Wang", "Wenchao Wu", "Dongqing Bao", "Tianyu Lu", "Jian Cao", "Guangtao Xue", "Guodong Yang", "Liping Zhang", "Gang Chen"], "title": "GFS: A Preemption-aware Scheduling Framework for GPU Clusters with Predictive Spot Instance Management", "comment": "This paper has been accepted to the 31st ACM International Conference\n  on Architectural Support for Programming Languages and Operating Systems\n  (ASPLOS 2026)", "summary": "The surge in large language models (LLMs) has fundamentally reshaped the\nlandscape of GPU usage patterns, creating an urgent need for more efficient\nmanagement strategies. While cloud providers employ spot instances to reduce\ncosts for low-priority (LP) tasks, existing schedulers still grapple with high\neviction rates and lengthy queuing times. To address these limitations, we\npresent GFS, a novel preemptive scheduling framework that enhances\nservice-level objective (SLO) compliance for high-priority (HP) tasks while\nminimizing preemptions to LP tasks. Firstly, GFS utilizes a lightweight\nforecasting model that predicts GPU demand among different tenants, enabling\nproactive resource management. Secondly, GFS employs a dynamic allocation\nmechanism to adjust the spot quota for LP tasks with guaranteed durations.\nLastly, GFS incorporates a preemptive scheduling policy that prioritizes HP\ntasks while minimizing the impact on LP tasks. We demonstrate the effectiveness\nof GFS through both real-world implementation and simulations. The results show\nthat GFS reduces eviction rates by 33.0\\%, and cuts queuing delays by 44.1\\%\nfor LP tasks. Furthermore, GFS enhances the GPU allocation rate by up to 22.8\\%\nin real production clusters. In a production cluster of more than 10,000 GPUs,\nGFS yields roughly \\$459,715 in monthly benefits.", "AI": {"tldr": "GFS\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u62a2\u5360\u5f0f\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9884\u6d4b\u6a21\u578b\u3001\u52a8\u6001\u5206\u914d\u673a\u5236\u548c\u62a2\u5360\u5f0f\u8c03\u5ea6\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1\u7684\u9a71\u9010\u7387\u548c\u6392\u961f\u5ef6\u8fdf\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86GPU\u5206\u914d\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\u6539\u53d8\u4e86GPU\u4f7f\u7528\u6a21\u5f0f\uff0c\u73b0\u6709\u8c03\u5ea6\u5668\u9762\u4e34\u9ad8\u9a71\u9010\u7387\u548c\u957f\u6392\u961f\u65f6\u95f4\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7ba1\u7406\u7b56\u7565\u6765\u5e73\u8861\u9ad8\u4f18\u5148\u7ea7\u548c\u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1\u7684\u9700\u6c42\u3002", "method": "1) \u4f7f\u7528\u8f7b\u91cf\u7ea7\u9884\u6d4b\u6a21\u578b\u9884\u6d4b\u4e0d\u540c\u79df\u6237\u7684GPU\u9700\u6c42\uff1b2) \u91c7\u7528\u52a8\u6001\u5206\u914d\u673a\u5236\u8c03\u6574\u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1\u7684spot\u914d\u989d\u5e76\u4fdd\u8bc1\u6301\u7eed\u65f6\u95f4\uff1b3) \u5b9e\u65bd\u62a2\u5360\u5f0f\u8c03\u5ea6\u7b56\u7565\uff0c\u4f18\u5148\u5904\u7406\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "GFS\u5c06\u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1\u7684\u9a71\u9010\u7387\u964d\u4f4e33.0%\uff0c\u6392\u961f\u5ef6\u8fdf\u51cf\u5c1144.1%\uff0cGPU\u5206\u914d\u7387\u63d0\u9ad822.8%\uff0c\u5728\u8d85\u8fc710,000\u4e2aGPU\u7684\u751f\u4ea7\u96c6\u7fa4\u4e2d\u6bcf\u6708\u4ea7\u751f\u7ea6459,715\u7f8e\u5143\u7684\u7ecf\u6d4e\u6548\u76ca\u3002", "conclusion": "GFS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86GPU\u8d44\u6e90\u8c03\u5ea6\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1SLO\u5408\u89c4\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u4f18\u5148\u7ea7\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6574\u4f53\u8d44\u6e90\u5229\u7528\u7387\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.10751", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.10751", "abs": "https://arxiv.org/abs/2509.10751", "authors": ["Lucas M. Leipnitz de Fraga", "Cl\u00e1udio Machado Diniz"], "title": "Design and Analysis of Approximate Hardware Accelerators for VVC Intra Angular Prediction", "comment": "Accepted SBCCI 2025", "summary": "The Versatile Video Coding (VVC) standard significantly improves compression\nefficiency over its predecessor, HEVC, but at the cost of substantially higher\ncomputational complexity, particularly in intra-frame prediction. This stage\nemploys various directional modes, each requiring multiple multiplications\nbetween reference samples and constant coefficients. To optimize these\noperations at hardware accelerators, multiplierless constant multiplication\n(MCM) blocks offer a promising solution. However, VVC's interpolation filters\nhave more than fifty distinct coefficients, making MCM implementations\nresource-intensive. This work proposes an approximation method to reduce the\nnumber of interpolation coefficients by averaging fixed subsets of them,\ntherefore decreasing MCM block size and potentially lowering circuit area and\npower consumption. Six different MCM block architectures for angular intra\nprediction are introduced, in which five use the approximation method\nintroduced in this work, and evaluate the trade-off between coefficient\nreduction and coding efficiency compared with a conventional multiplier\narchitecture. Experimental results in ten videos demonstrate that only two MCM\nimplementations exceed a 4% BD-Rate increase and 2.6% on average in the worst\ncase, while two of the MCM implementations have circuit area reduction of 20%\nand 44%. For three of the architectures, parallel sample prediction modules\nwere synthesized, showing a reduction of 30% gate area compared to single\nsample processing units, and a reduction in energy consumption for two of the\nimplementations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u7cfb\u6570\u5e73\u5747\u8fd1\u4f3c\u65b9\u6cd5\u6765\u51cf\u5c11VVC\u5e27\u5185\u9884\u6d4b\u63d2\u503c\u6ee4\u6ce2\u5668\u7cfb\u6570\u6570\u91cf\u7684\u6280\u672f\uff0c\u4ee5\u964d\u4f4e\u786c\u4ef6\u5b9e\u73b0\u590d\u6742\u5ea6\u548c\u529f\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u7f16\u7801\u6548\u7387\u3002", "motivation": "VVC\u6807\u51c6\u76f8\u6bd4HEVC\u663e\u8457\u63d0\u9ad8\u4e86\u538b\u7f29\u6548\u7387\uff0c\u4f46\u5e26\u6765\u4e86\u66f4\u9ad8\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u7279\u522b\u662f\u5728\u5e27\u5185\u9884\u6d4b\u9636\u6bb5\u3002\u63d2\u503c\u6ee4\u6ce2\u5668\u670950\u591a\u4e2a\u4e0d\u540c\u7cfb\u6570\uff0c\u4f7f\u5f97\u65e0\u4e58\u6cd5\u5668\u5e38\u6570\u4e58\u6cd5(MCM)\u5757\u5b9e\u73b0\u8d44\u6e90\u5bc6\u96c6\u3002", "method": "\u901a\u8fc7\u5e73\u5747\u56fa\u5b9a\u5b50\u96c6\u7684\u63d2\u503c\u7cfb\u6570\u6765\u51cf\u5c11\u7cfb\u6570\u6570\u91cf\uff0c\u4ece\u800c\u51cf\u5c0fMCM\u5757\u89c4\u6a21\u3002\u63d0\u51fa\u4e86\u516d\u79cd\u4e0d\u540c\u7684MCM\u5757\u67b6\u6784\uff0c\u5176\u4e2d\u4e94\u79cd\u4f7f\u7528\u8be5\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4f20\u7edf\u4e58\u6cd5\u5668\u67b6\u6784\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u53ea\u6709\u4e24\u79cdMCM\u5b9e\u73b0\u8d85\u8fc74%\u7684BD-Rate\u589e\u52a0\uff0c\u6700\u574f\u60c5\u51b5\u4e0b\u5e73\u5747\u589e\u52a02.6%\u3002\u4e24\u79cdMCM\u5b9e\u73b0\u5206\u522b\u51cf\u5c11\u4e8620%\u548c44%\u7684\u7535\u8def\u9762\u79ef\u3002\u4e09\u79cd\u67b6\u6784\u7684\u5e76\u884c\u6837\u672c\u9884\u6d4b\u6a21\u5757\u76f8\u6bd4\u5355\u6837\u672c\u5904\u7406\u5355\u5143\u51cf\u5c11\u4e8630%\u7684\u95e8\u9762\u79ef\uff0c\u4e24\u79cd\u5b9e\u73b0\u964d\u4f4e\u4e86\u80fd\u8017\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7cfb\u6570\u8fd1\u4f3c\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u964d\u4f4eVVC\u5e27\u5185\u9884\u6d4b\u7684\u786c\u4ef6\u5b9e\u73b0\u590d\u6742\u5ea6\uff0c\u5728\u4fdd\u6301\u53ef\u63a5\u53d7\u7684\u7f16\u7801\u6548\u7387\u635f\u5931\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u7535\u8def\u9762\u79ef\u548c\u529f\u8017\u3002"}}
{"id": "2509.11152", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.11152", "abs": "https://arxiv.org/abs/2509.11152", "authors": ["Wajih Boukaram", "David Keyes", "Sherry Li", "Yang Liu", "George Turkiyyah"], "title": "Linear Complexity $\\mathcal{H}^2$ Direct Solver for Fine-Grained Parallel Architectures", "comment": null, "summary": "We present factorization and solution phases for a new linear complexity\ndirect solver designed for concurrent batch operations on fine-grained parallel\narchitectures, for matrices amenable to hierarchical representation. We focus\non the strong-admissibility-based $\\mathcal{H}^2$ format, where strong\nrecursive skeletonization factorization compresses remote interactions. We\nbuild upon previous implementations of $\\mathcal{H}^2$ matrix construction for\nefficient factorization and solution algorithm design, which are illustrated\ngraphically in stepwise detail. The algorithms are ``blackbox'' in the sense\nthat the only inputs are the matrix and right-hand side, without analytical or\ngeometrical information about the origin of the system. We demonstrate linear\ncomplexity scaling in both time and memory on four representative families of\ndense matrices up to one million in size. Parallel scaling up to 16 threads is\nenabled by a multi-level matrix graph coloring and avoidance of dynamic memory\nallocations thanks to prefix-sum memory management. An experimental backward\nerror analysis is included. We break down the timings of different phases,\nidentify phases that are memory-bandwidth limited, and discuss alternatives for\nphases that may be sensitive to the trend to employ lower precisions for\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ebf\u6027\u590d\u6742\u5ea6\u76f4\u63a5\u6c42\u89e3\u5668\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u5e76\u884c\u67b6\u6784\u4e0a\u7684\u5e76\u53d1\u6279\u5904\u7406\u64cd\u4f5c\uff0c\u9002\u7528\u4e8e\u5c42\u6b21\u5316\u8868\u793a\u7684\u77e9\u9635\u3002", "motivation": "\u9488\u5bf9\u5c42\u6b21\u5316\u77e9\u9635\uff08\u7279\u522b\u662f\u5f3a\u53ef\u5bb9\u8bb8\u6027H\u00b2\u683c\u5f0f\uff09\u8bbe\u8ba1\u9ad8\u6548\u5e76\u884c\u76f4\u63a5\u6c42\u89e3\u5668\uff0c\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u5185\u5b58\u590d\u6742\u5ea6\uff0c\u652f\u6301\u5927\u89c4\u6a21\u7a20\u5bc6\u77e9\u9635\u6c42\u89e3\u3002", "method": "\u57fa\u4e8e\u5f3a\u9012\u5f52\u9aa8\u67b6\u5316\u5206\u89e3\u538b\u7f29\u8fdc\u7a0b\u4ea4\u4e92\uff0c\u91c7\u7528\u591a\u7ea7\u77e9\u9635\u56fe\u7740\u8272\u548c\u524d\u7f00\u548c\u5185\u5b58\u7ba1\u7406\u907f\u514d\u52a8\u6001\u5185\u5b58\u5206\u914d\uff0c\u5b9e\u73b016\u7ebf\u7a0b\u5e76\u884c\u6269\u5c55\u3002", "result": "\u5728\u56db\u4e2a\u4ee3\u8868\u6027\u7a20\u5bc6\u77e9\u9635\u65cf\u4e0a\u9a8c\u8bc1\u4e86\u7ebf\u6027\u590d\u6742\u5ea6\u6269\u5c55\uff08\u6700\u5927\u89c4\u6a21100\u4e07\uff09\uff0c\u5305\u542b\u5b9e\u9a8c\u6027\u540e\u5411\u8bef\u5dee\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u5185\u5b58\u5e26\u5bbd\u53d7\u9650\u9636\u6bb5\u3002", "conclusion": "\u8be5\u9ed1\u76d2\u6c42\u89e3\u5668\u4ec5\u9700\u8f93\u5165\u77e9\u9635\u548c\u53f3\u7aef\u9879\uff0c\u65e0\u9700\u7cfb\u7edf\u6765\u6e90\u7684\u89e3\u6790\u6216\u51e0\u4f55\u4fe1\u606f\uff0c\u4e3a\u4f4e\u7cbe\u5ea6\u8ba1\u7b97\u8d8b\u52bf\u63d0\u4f9b\u4e86\u66ff\u4ee3\u65b9\u6848\u8ba8\u8bba\u3002"}}
{"id": "2509.11503", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.11503", "abs": "https://arxiv.org/abs/2509.11503", "authors": ["Rishab Parthasarathy", "Akshay Attaluri", "Gilford Ting"], "title": "always_comm: An FPGA-based Hardware Accelerator for Audio/Video Compression and Transmission", "comment": "8 pages, 8 figures, 1 table, equal contribution", "summary": "We present a design for an extensible video conferencing stack implemented\nentirely in hardware on a Nexys4 DDR FPGA, which uses the M-JPEG codec to\ncompress video and a UDP networking stack to communicate between the FPGA and\nthe receiving computer. This networking stack accepts real-time updates from\nboth the video codec and the audio controller, which means that video will be\nable to be streamed at 30 FPS from the FPGA to a computer. On the computer\nside, a Python script reads the Ethernet packets and decodes the packets into\nthe video and the audio for real time playback. We evaluate this architecture\nusing both functional, simulation-driven verification in Cocotb and by\nsynthesizing SystemVerilog RTL code using Vivado for deployment on our Nexys4\nDDR FPGA, where we evaluate both end-to-end latency and throughput of video\ntransmission.", "AI": {"tldr": "\u57fa\u4e8eFPGA\u786c\u4ef6\u5b9e\u73b0\u7684\u53ef\u6269\u5c55\u89c6\u9891\u4f1a\u8bae\u7cfb\u7edf\uff0c\u91c7\u7528M-JPEG\u7f16\u7801\u548cUDP\u7f51\u7edc\u6808\uff0c\u652f\u630130FPS\u5b9e\u65f6\u89c6\u9891\u6d41\u4f20\u8f93", "motivation": "\u5b9e\u73b0\u5168\u786c\u4ef6\u5316\u7684\u89c6\u9891\u4f1a\u8bae\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u6027\u80fd\u548c\u5b9e\u65f6\u6027\uff0c\u652f\u6301\u5b9e\u65f6\u89c6\u9891\u6d41\u4f20\u8f93", "method": "\u5728Nexys4 DDR FPGA\u4e0a\u5b9e\u73b0M-JPEG\u89c6\u9891\u538b\u7f29\u7f16\u7801\u548cUDP\u7f51\u7edc\u6808\uff0c\u901a\u8fc7Python\u811a\u672c\u89e3\u7801\u6536\u53d6\u7684\u4ee5\u592a\u7f51\u6570\u636e\u5305\u8fdb\u884c\u5b9e\u65f6\u64ad\u653e", "result": "\u7cfb\u7edf\u901a\u8fc7Cocotb\u6a21\u62df\u9a8c\u8bc1\u548cVivado\u5408\u6210\u90e8\u7f72\uff0c\u80fd\u591f\u5728FPGA\u4e0a\u5b9e\u73b0\u7a33\u5b9a\u768430FPS\u89c6\u9891\u6d41\u4f20\u8f93", "conclusion": "\u8fd9\u79cd\u5168\u786c\u4ef6\u5316\u8bbe\u8ba1\u65b9\u6848\u6709\u6548\u5730\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u5b9e\u65f6\u89c6\u9891\u4f1a\u8bae\u529f\u80fd\uff0c\u4e3a\u5d4c\u5165\u5f0f\u89c6\u9891\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.11156", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.11156", "abs": "https://arxiv.org/abs/2509.11156", "authors": ["Suvarthi Sarkar", "Aadarshraj Sah", "Poddutoori Sweeya Reddy", "Aryabartta Sahu"], "title": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud", "comment": null, "summary": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94K\u6253\u5305\u7f13\u5b58\u7b97\u6cd5(AKPC)\uff0c\u5c06\u4f20\u7edf\u76842\u9879\u6253\u5305\u6269\u5c55\u5230\u53ef\u53d8\u5927\u5c0f\u7684K\u9879\u6253\u5305\uff0c\u901a\u8fc7\u52a8\u6001\u5f62\u6210\u3001\u5408\u5e76\u548c\u5206\u5272\u6570\u636e\u7c07\u6765\u4f18\u5316\u7f13\u5b58\u6548\u7387\uff0c\u5728Netflix\u548cSpotify\u6570\u636e\u96c6\u4e0a\u5206\u522b\u51cf\u5c11\u4e8663%\u548c55%\u7684\u603b\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u9650\u4e8e2\u9879\u6210\u5bf9\u6253\u5305\uff0c\u9650\u5236\u4e86\u7f13\u5b58\u6548\u7387\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\u7528\u6237\u8bbf\u95ee\u6a21\u5f0f\u9884\u6d4b\u6280\u672f\u7684\u53d1\u5c55\u4f7f\u5f97\u6253\u5305\u591a\u4e2a\u76f8\u5173\u6570\u636e\u9879\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u8fc7\u6253\u5305\u589e\u52a0\u6210\u672c\u3001\u6b20\u6253\u5305\u9519\u8fc7\u5171\u4eab\u673a\u4f1a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5728\u7ebf\u7b97\u6cd5AKPC\uff0c\u4eceCDN\u8fd0\u8425\u5546\u89d2\u5ea6\u5236\u5b9aK PackCache\u95ee\u9898\uff0c\u6700\u5c0f\u5316\u5305\u542b\u4f20\u8f93\u6210\u672c\u548c\u5185\u5b58\u79df\u8d41\u6210\u672c\u7684\u603b\u6210\u672c\u3002\u7b97\u6cd5\u57fa\u4e8e\u7528\u6237\u8bbf\u95ee\u6a21\u5f0f\u548c\u5185\u5bb9\u76f8\u5173\u6027\u52a8\u6001\u5f62\u6210\u3001\u5408\u5e76\u548c\u5206\u5272\u6570\u636e\u7c07\uff0c\u652f\u6301\u6279\u91cf\u8bf7\u6c42\u548c\u8fd1\u4f3c\u7c07\u5408\u5e76\u3002", "result": "\u5728Netflix\u548cSpotify\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0cAKPC\u76f8\u6bd4\u5728\u7ebf\u57fa\u7ebf\u5206\u522b\u51cf\u5c11\u4e8663%\u548c55%\u7684\u603b\u6210\u672c\uff0c\u6027\u80fd\u8fbe\u5230\u6700\u4f18\u89e3\u768415%\u548c13%\u8303\u56f4\u5185\u3002", "conclusion": "AKPC\u7b97\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u7f13\u5b58\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u9002\u5e94K\u9879\u6253\u5305\u663e\u8457\u63d0\u5347\u4e86\u7f13\u5b58\u6027\u80fd\u3002"}}
{"id": "2509.11529", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.11529", "abs": "https://arxiv.org/abs/2509.11529", "authors": ["Rishab Parthasarathy"], "title": "SuperUROP: An FPGA-Based Spatial Accelerator for Sparse Matrix Operations", "comment": "7 pages, 6 figures, work done as the Citadel Undergraduate Research\n  Scholar in the MIT SuperUROP program", "summary": "Solving sparse systems of linear equations is a fundamental problem in the\nfield of numerical methods, with applications spanning from circuit design to\nurban planning. These problems can have millions of constraints, such as when\nlaying out transistors on a circuit, or trying to optimize traffic light\ntimings, making fast sparse solvers extremely important. However, existing\nstate-of-the-art software-level solutions for solving sparse linear systems,\ntermed iterative solvers, are extremely inefficient on current hardware. This\ninefficiency can be attributed to two key reasons: (1) poor short-term data\nreuse, which causes frequent, irregular memory accesses, and (2) complex data\ndependencies, which limit parallelism. Hence, in this paper, we present an FPGA\nimplementation of the existing Azul accelerator, an SRAM-only hardware\naccelerator that achieves both high memory bandwidth utilization and arithmetic\nintensity. Azul features a grid of tiles, each of which is composed of a\nprocessing element (PE) and a small independent SRAM memory, which are all\nconnected over a network on chip (NoC). We implement Azul on FPGA using simple\nRISC-V CPU cores connected to a memory hierarchy of different FPGA memory\nmodules. We utilize custom RISC-V ISA augmentations to implement a task-based\nprogramming model for the various PEs, allowing communication over the NoC.\nFinally, we design simple distributed test cases so that we can functionally\nverify the FPGA implementation, verifying equivalent performance to an\narchitectural simulation of the Azul framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Azul\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u5668\u7684FPGA\u5b9e\u73b0\uff0c\u901a\u8fc7SRAM-only\u786c\u4ef6\u52a0\u901f\u5668\u89e3\u51b3\u73b0\u6709\u8fed\u4ee3\u6c42\u89e3\u5668\u5728\u5185\u5b58\u8bbf\u95ee\u548c\u5e76\u884c\u6027\u65b9\u9762\u7684\u6548\u7387\u95ee\u9898", "motivation": "\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u5728\u6570\u503c\u65b9\u6cd5\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8f6f\u4ef6\u7ea7\u8fed\u4ee3\u6c42\u89e3\u5668\u5728\u5f53\u524d\u786c\u4ef6\u4e0a\u6548\u7387\u4f4e\u4e0b\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5185\u5b58\u8bbf\u95ee\u4e0d\u89c4\u5219\u548c\u6570\u636e\u4f9d\u8d56\u590d\u6742\u9650\u5236\u4e86\u5e76\u884c\u6027", "method": "\u5728FPGA\u4e0a\u5b9e\u73b0Azul\u52a0\u901f\u5668\uff0c\u91c7\u7528RISC-V CPU\u6838\u5fc3\u8fde\u63a5\u4e0d\u540cFPGA\u5185\u5b58\u6a21\u5757\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49RISC-V ISA\u6269\u5c55\u5b9e\u73b0\u57fa\u4e8e\u4efb\u52a1\u7684\u7f16\u7a0b\u6a21\u578b\uff0c\u652f\u6301NoC\u901a\u4fe1", "result": "\u8bbe\u8ba1\u4e86\u5206\u5e03\u5f0f\u6d4b\u8bd5\u7528\u4f8b\u8fdb\u884c\u529f\u80fd\u9a8c\u8bc1\uff0c\u786e\u8ba4FPGA\u5b9e\u73b0\u4e0eAzul\u6846\u67b6\u67b6\u6784\u4eff\u771f\u5177\u6709\u7b49\u6548\u6027\u80fd", "conclusion": "FPGA\u5b9e\u73b0\u7684Azul\u52a0\u901f\u5668\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u4e2d\u7684\u5185\u5b58\u5e26\u5bbd\u5229\u7528\u548c\u7b97\u672f\u5f3a\u5ea6\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u7ea6\u675f\u95ee\u9898\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.11162", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.11162", "abs": "https://arxiv.org/abs/2509.11162", "authors": ["Chuanchao Gao", "Arvind Easwaran"], "title": "Energy-Efficient Joint Offloading and Resource Allocation for Deadline-Constrained Tasks in Multi-Access Edge Computing", "comment": null, "summary": "This paper addresses the deadline-constrained task offloading and resource\nallocation problem in multi-access edge computing. We aim to determine where\neach task is offloaded and processed, as well as corresponding communication\nand computation resource allocations, to maximize the total saved energy for\nIoT devices, while considering task deadline and system resource constraints.\nEspecially, our system allows each task to be offloaded to one of its\naccessible access points (APs) and processed on a server that is not co-located\nwith its offloading AP. We formulate this problem as an Integer Nonlinear\nProgramming problem and show it is NP-Hard. To address this problem, we propose\na Graph-Matching-based Approximation Algorithm ($\\mathtt{GMA}$), the first\napproximation algorithm of its kind. $\\mathtt{GMA}$ leverages linear\nrelaxation, tripartite graph construction, and a Linear Programming rounding\ntechnique. We prove that $\\mathtt{GMA}$ is a\n$\\frac{1-\\alpha}{2+\\epsilon}$-approximation algorithm, where $\\epsilon$ is a\nsmall positive value, and $\\alpha$ ($0$$\\le$$\\alpha$$<$$1$) is a system\nparameter that ensures the resource allocated to any task by an AP or a server\ncannot exceed $\\alpha$ times its resource capacity. Experiments show that, in\npractice, $\\mathtt{GMA}$'s energy saving achieves $97\\%$ of the optimal value\non average.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u5339\u914d\u7684\u8fd1\u4f3c\u7b97\u6cd5\uff08GMA\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u8bbf\u95ee\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u4efb\u52a1\u5378\u8f7d\u548c\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5728\u6ee1\u8db3\u4efb\u52a1\u622a\u6b62\u671f\u548c\u7cfb\u7edf\u8d44\u6e90\u7ea6\u675f\u7684\u524d\u63d0\u4e0b\u6700\u5927\u5316IoT\u8bbe\u5907\u7684\u8282\u80fd\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u591a\u8bbf\u95ee\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u5378\u8f7d\u548c\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u8fd9\u662f\u4e00\u4e2aNP\u96be\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8fd1\u4f3c\u7b97\u6cd5\u6765\u6700\u5927\u5316IoT\u8bbe\u5907\u7684\u80fd\u91cf\u8282\u7701\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u56fe\u5339\u914d\u7684\u8fd1\u4f3c\u7b97\u6cd5\uff08GMA\uff09\uff0c\u5229\u7528\u7ebf\u6027\u677e\u5f1b\u3001\u4e09\u90e8\u56fe\u6784\u5efa\u548c\u7ebf\u6027\u89c4\u5212\u56db\u820d\u4e94\u5165\u6280\u672f\u6765\u8fdb\u884c\u95ee\u9898\u6c47\u6c47\u3002", "result": "\u7b97\u6cd5\u5728\u7406\u8bba\u4e0a\u5177\u6709\u8fd1\u4f3c\u6bd4\u4f8b\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u8282\u80fd\u6548\u679c\u5e73\u5747\u8fbe\u5230\u6700\u4f18\u89e3\u768497%\u3002", "conclusion": "GMA\u7b97\u6cd5\u80fd\u591f\u9ad8\u6548\u5730\u89e3\u51b3\u591a\u8bbf\u95ee\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u4efb\u52a1\u5378\u8f7d\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.12053", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12053", "abs": "https://arxiv.org/abs/2509.12053", "authors": ["Yujun Lin", "Zhekai Zhang", "Song Han"], "title": "LEGO: Spatial Accelerator Generation and Optimization for Tensor Applications", "comment": "The first two authors have equal contributions; Published as a\n  conference paper in HPCA 2025; 13 pages, 14 figures", "summary": "Modern tensor applications, especially foundation models and generative AI\napplications require multiple input modalities (both vision and language),\nwhich increases the demand for flexible accelerator architecture. Existing\nframeworks suffer from the trade-off between design flexibility and\nproductivity of RTL generation: either limited to very few hand-written\ntemplates or cannot automatically generate the RTL. To address this challenge,\nwe propose the LEGO framework, which targets tensor applications and\nautomatically generates spatial architecture design and outputs synthesizable\nRTL code without handwritten RTL design templates. Leveraging the\naffine-transformation-based architecture representation, LEGO front end finds\ninterconnections between function units, synthesizes the memory system, and\nfuses different spatial dataflow designs based on data reuse analysis. LEGO\nback end then translates the hardware in a primitive-level graph to perform\nlower-level optimizations, and applies a set of linear-programming algorithms\nto optimally insert pipeline registers and reduce the overhead of unused logic\nwhen switching spatial dataflows. Our evaluation demonstrates that LEGO can\nachieve 3.2x speedup and 2.4x energy efficiency compared to previous work\nGemmini, and can generate one architecture for diverse modern foundation models\nin generative AI applications.", "AI": {"tldr": "LEGO\u6846\u67b6\u81ea\u52a8\u751f\u6210\u9762\u5411\u5f20\u91cf\u5e94\u7528\u7684\u7a7a\u95f4\u67b6\u6784\u8bbe\u8ba1\u548c\u53ef\u7efc\u5408RTL\u4ee3\u7801\uff0c\u65e0\u9700\u624b\u5199RTL\u6a21\u677f\uff0c\u76f8\u6bd4Gemmini\u5b9e\u73b03.2\u500d\u52a0\u901f\u548c2.4\u500d\u80fd\u6548\u63d0\u5347", "motivation": "\u73b0\u4ee3\u5f20\u91cf\u5e94\u7528\uff08\u7279\u522b\u662f\u57fa\u7840\u6a21\u578b\u548c\u751f\u6210\u5f0fAI\uff09\u9700\u8981\u591a\u6a21\u6001\u8f93\u5165\uff0c\u73b0\u6709\u6846\u67b6\u5728\u8bbe\u8ba1\u7075\u6d3b\u6027\u548cRTL\u751f\u6210\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u8981\u4e48\u5c40\u9650\u4e8e\u5c11\u6570\u624b\u5199\u6a21\u677f\uff0c\u8981\u4e48\u65e0\u6cd5\u81ea\u52a8\u751f\u6210RTL", "method": "\u57fa\u4e8e\u4eff\u5c04\u53d8\u6362\u7684\u67b6\u6784\u8868\u793a\uff0c\u524d\u7aef\u5206\u6790\u529f\u80fd\u5355\u5143\u4e92\u8fde\u3001\u5408\u6210\u5b58\u50a8\u7cfb\u7edf\u3001\u57fa\u4e8e\u6570\u636e\u91cd\u7528\u5206\u6790\u878d\u5408\u4e0d\u540c\u7a7a\u95f4\u6570\u636e\u6d41\u8bbe\u8ba1\uff1b\u540e\u7aef\u901a\u8fc7\u56fe\u8868\u793a\u8fdb\u884c\u4f4e\u7ea7\u4f18\u5316\uff0c\u4f7f\u7528\u7ebf\u6027\u89c4\u5212\u7b97\u6cd5\u4f18\u5316\u6d41\u6c34\u7ebf\u5bc4\u5b58\u5668\u63d2\u5165\u548c\u51cf\u5c11\u672a\u4f7f\u7528\u903b\u8f91\u5f00\u9500", "result": "\u76f8\u6bd4Gemmini\u5b9e\u73b03.2\u500d\u901f\u5ea6\u63d0\u5347\u548c2.4\u500d\u80fd\u6548\u63d0\u5347\uff0c\u80fd\u591f\u4e3a\u751f\u6210\u5f0fAI\u5e94\u7528\u4e2d\u7684\u591a\u6837\u5316\u73b0\u4ee3\u57fa\u7840\u6a21\u578b\u751f\u6210\u7edf\u4e00\u67b6\u6784", "conclusion": "LEGO\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5f20\u91cf\u52a0\u901f\u5668\u8bbe\u8ba1\u4e2d\u7684\u7075\u6d3b\u6027\u4e0e\u751f\u4ea7\u529b\u77db\u76fe\uff0c\u4e3a\u591a\u6a21\u6001AI\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u786c\u4ef6\u751f\u6210\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.11396", "categories": ["cs.DC", "cs.SY", "eess.SY", "C.2.4"], "pdf": "https://arxiv.org/pdf/2509.11396", "abs": "https://arxiv.org/abs/2509.11396", "authors": ["Adam Janiak", "Damian Kowalczyk", "Maciej Lichtenstein"], "title": "Parallel/Distributed Tabu Search for Scheduling Microprocessor Tasks in Hybrid Flowshop", "comment": "authors listed in alphabetical order", "summary": "The paper deals with the makespan minimization in the hybrid flow shop\nscheduling problem with multiprocessor tasks. The hybrid flow shop (HFS)\ngeneralizes the classical flow shop processor configuration by replacing each\nprocessor (processing stage) by some number of identical parallel processors.\nSimilarly, the multiprocessor tasks generalize the classical assumption, by\nallowing a task to require more than one processor simultaneously for its\nprocessing. In this work we present the algorithm for solving the problem based\non the tabu search technique. The proposed algorithm uses parallel and\ndistributed mechanisms for neighborhood evaluation and well balances\nheterogeneous network environment.", "AI": {"tldr": "\u57fa\u4e8e\u7981\u5fcc\u641c\u7d22\u7684\u6df7\u5408\u6d41\u6c34\u8f66\u95f4\u8c03\u5ea6\u7b97\u6cd5\uff0c\u89e3\u51b3\u591a\u5904\u7406\u5668\u4efb\u52a1\u7684\u6700\u5c0f\u5316\u5b8c\u5de5\u65f6\u95f4\u95ee\u9898", "motivation": "\u6df7\u5408\u6d41\u6c34\u8f66\u95f4(HFS)\u6269\u5c55\u4e86\u7ecf\u5178\u6d41\u6c34\u8f66\u95f4\uff0c\u6bcf\u4e2a\u5904\u7406\u9636\u6bb5\u7531\u591a\u4e2a\u76f8\u540c\u5e76\u884c\u5904\u7406\u5668\u7ec4\u6210\uff1b\u591a\u5904\u7406\u5668\u4efb\u52a1\u5141\u8bb8\u4efb\u52a1\u540c\u65f6\u9700\u8981\u591a\u4e2a\u5904\u7406\u5668\u5904\u7406\u3002\u9700\u8981\u9ad8\u6548\u7b97\u6cd5\u89e3\u51b3\u8fd9\u7c7b\u590d\u6742\u8c03\u5ea6\u95ee\u9898", "method": "\u91c7\u7528\u7981\u5fcc\u641c\u7d22\u6280\u672f\uff0c\u4f7f\u7528\u5e76\u884c\u548c\u5206\u5e03\u5f0f\u673a\u5236\u8fdb\u884c\u90bb\u57df\u8bc4\u4f30\uff0c\u5e76\u826f\u597d\u5e73\u8861\u5f02\u6784\u7f51\u7edc\u73af\u5883", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u7b97\u6cd5\u6846\u67b6\u6765\u5904\u7406\u6df7\u5408\u6d41\u6c34\u8f66\u95f4\u4e2d\u591a\u5904\u7406\u5668\u4efb\u52a1\u7684\u8c03\u5ea6\u4f18\u5316", "conclusion": "\u8be5\u7981\u5fcc\u641c\u7d22\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5177\u6709\u591a\u5904\u7406\u5668\u4efb\u52a1\u7684\u6df7\u5408\u6d41\u6c34\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898\uff0c\u7279\u522b\u9002\u5408\u5f02\u6784\u7f51\u7edc\u73af\u5883\u4e0b\u7684\u5e76\u884c\u8ba1\u7b97"}}
{"id": "2509.11512", "categories": ["cs.DC", "cs.AI", "cs.LG", "68T05, 68M14, 68W10"], "pdf": "https://arxiv.org/pdf/2509.11512", "abs": "https://arxiv.org/abs/2509.11512", "authors": ["Tasnuva Chowdhury", "Tadashi Maeno", "Fatih Furkan Akman", "Joseph Boudreau", "Sankha Dutta", "Shengyu Feng", "Adolfy Hoisie", "Kuan-Chieh Hsu", "Raees Khan", "Jaehyung Kim", "Ozgur O. Kilic", "Scott Klasky", "Alexei Klimentov", "Tatiana Korchuganova", "Verena Ingrid Martinez Outschoorn", "Paul Nilsson", "David K. Park", "Norbert Podhorszki", "Yihui Ren", "John Rembrandt Steele", "Fr\u00e9d\u00e9ric Suter", "Sairam Sri Vatsavai", "Torre Wenaus", "Wei Yang", "Yiming Yang", "Shinjae Yoo"], "title": "Machine Learning-Driven Predictive Resource Management in Complex Science Workflows", "comment": null, "summary": "The collaborative efforts of large communities in science experiments, often\ncomprising thousands of global members, reflect a monumental commitment to\nexploration and discovery. Recently, advanced and complex data processing has\ngained increasing importance in science experiments. Data processing workflows\ntypically consist of multiple intricate steps, and the precise specification of\nresource requirements is crucial for each step to allocate optimal resources\nfor effective processing. Estimating resource requirements in advance is\nchallenging due to a wide range of analysis scenarios, varying skill levels\namong community members, and the continuously increasing spectrum of computing\noptions. One practical approach to mitigate these challenges involves initially\nprocessing a subset of each step to measure precise resource utilization from\nactual processing profiles before completing the entire step. While this\ntwo-staged approach enables processing on optimal resources for most of the\nworkflow, it has drawbacks such as initial inaccuracies leading to potential\nfailures and suboptimal resource usage, along with overhead from waiting for\ninitial processing completion, which is critical for fast-turnaround analyses.\nIn this context, our study introduces a novel pipeline of machine learning\nmodels within a comprehensive workflow management system, the Production and\nDistributed Analysis (PanDA) system. These models employ advanced machine\nlearning techniques to predict key resource requirements, overcoming challenges\nposed by limited upfront knowledge of characteristics at each step. Accurate\nforecasts of resource requirements enable informed and proactive\ndecision-making in workflow management, enhancing the efficiency of handling\ndiverse, complex workflows across heterogeneous resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b0\u9898\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u9884\u6d4b\u79d1\u5b66\u5b9e\u9a8c\u6570\u636e\u5904\u7406\u6d41\u7a0b\u7684\u8d44\u6e90\u9700\u6c42\uff0c\u89e3\u51b3\u4f20\u7edf\u4e24\u9636\u6bb5\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u5904\u7406\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u79d1\u5b66\u5b9e\u9a8c\u6570\u636e\u5904\u7406\u6d41\u7a0b\u8d8b\u5411\u590d\u6742\u5316\uff0c\u4f46\u9884\u5148\u4f30\u7b97\u8d44\u6e90\u9700\u6c42\u5f88\u56f0\u96be\u3002\u4f20\u7edf\u7684\u4e24\u9636\u6bb5\u5904\u7406\u65b9\u6cd5\u5b58\u5728\u521d\u59cb\u51c6\u786e\u6027\u4f4e\u3001\u8d44\u6e90\u4f7f\u7528\u4e0d\u4f18\u3001\u7b49\u5f85\u65f6\u95f4\u8fc7\u957f\u7b49\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u5feb\u901f\u56de\u8f6c\u5206\u6790\u7684\u573a\u666f\u3002", "method": "\u5728PanDA\u7cfb\u7edf\u4e2d\u96c6\u6210\u4e86\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6d41\u6c34\u7ebf\uff0c\u91c7\u7528\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\u6765\u9884\u6d4b\u5173\u952e\u8d44\u6e90\u9700\u6c42\uff0c\u514b\u670d\u6bcf\u4e2a\u6b65\u9aa4\u521d\u59cb\u7279\u5f81\u77e5\u8bc6\u6709\u9650\u7684\u6311\u6218\u3002", "result": "\u51c6\u786e\u7684\u8d44\u6e90\u9700\u6c42\u9884\u6d4b\u4f7f\u5f97\u6d41\u7a0b\u7ba1\u7406\u80fd\u591f\u8fdb\u884c\u660e\u667a\u7684\u9884\u9635\u51b3\u7b56\uff0c\u63d0\u9ad8\u4e86\u5728\u5f02\u6784\u8d44\u6e90\u4e0a\u5904\u7406\u591a\u6837\u5316\u590d\u6742\u6d41\u7a0b\u7684\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u79d1\u5b66\u5b9e\u9a8c\u6570\u636e\u5904\u7406\u4e2d\u7684\u8d44\u6e90\u9884\u6d4b\u6311\u6218\uff0c\u4e3a\u5927\u89c4\u6a21\u79d1\u5b66\u5408\u4f5c\u9879\u76ee\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u6d41\u7a0b\u7ba1\u7406\u65b9\u6848\u3002"}}
{"id": "2509.11697", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.11697", "abs": "https://arxiv.org/abs/2509.11697", "authors": ["Cheng Zhang", "Wan-Lei Zhao", "Shihai Xiao", "Jiajie Yao", "Xuecang Zhang"], "title": "Towards the Distributed Large-scale k-NN Graph Construction by Graph Merge", "comment": "14 pages, 14 figures", "summary": "In order to support the real-time interaction with LLMs and the instant\nsearch or the instant recommendation on social media, it becomes an imminent\nproblem to build k-NN graph or indexing graph for the massive number of\nvectorized multimedia data. In such scenarios, the scale of the data or the\nscale of the graph may exceed the processing capacity of a single machine. This\npaper aims to address the graph construction problem of such scale via\nefficient graph merge. For the graph construction on a single node, two generic\nand highly parallelizable algorithms, namely Two-way Merge and Multi-way Merge\nare proposed to merge subgraphs into one. For the graph construction across\nmultiple nodes, a multi-node procedure based on Two-way Merge is presented. The\nprocedure makes it feasible to construct a large-scale k-NN graph/indexing\ngraph on either a single node or multiple nodes when the data size exceeds the\nmemory capacity of one node. Extensive experiments are conducted on both\nlarge-scale k-NN graph and indexing graph construction. For the k-NN graph\nconstruction, the large-scale and high-quality k-NN graphs are constructed by\ngraph merge in parallel. Typically, a billion-scale k-NN graph can be built in\napproximately 17h when only three nodes are employed. For the indexing graph\nconstruction, similar NN search performance as the original indexing graph is\nachieved with the merged indexing graphs while requiring much less time of\nconstruction.", "AI": {"tldr": "\u901a\u8fc7\u9ad8\u6548\u7684\u56fe\u5408\u5e76\u7b97\u6cd5\uff08\u53cc\u5411\u5408\u5e76\u548c\u591a\u8def\u5408\u5e76\uff09\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u5411\u91cf\u6570\u636e\u7684k\u8fd1\u90bb\u56fe\u548c\u7d22\u5f15\u56fe\u6784\u5efa\u95ee\u9898\uff0c\u652f\u6301\u5355\u673a\u548c\u5206\u5e03\u5f0f\u573a\u666f", "motivation": "\u4e3a\u4e86\u652f\u6301LLM\u5b9e\u65f6\u4ea4\u4e92\u4ee5\u53ca\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u5373\u65f6\u641c\u7d22/\u63a8\u8350\uff0c\u9700\u8981\u4e3a\u5927\u89c4\u6a21\u5411\u91cf\u5316\u591a\u5a92\u4f53\u6570\u636e\u6784\u5efak-NN\u56fe\u6216\u7d22\u5f15\u56fe\uff0c\u800c\u5355\u673a\u5904\u7406\u80fd\u529b\u53ef\u80fd\u4e0d\u8db3", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u9ad8\u5e76\u884c\u5316\u7684\u56fe\u5408\u5e76\u7b97\u6cd5\uff1a\u53cc\u5411\u5408\u5e76\u548c\u591a\u8def\u5408\u5e76\uff0c\u4ee5\u53ca\u57fa\u4e8e\u53cc\u5411\u5408\u5e76\u7684\u591a\u8282\u70b9\u6784\u5efa\u6d41\u7a0b", "result": "\u5728\u4e09\u4e2a\u8282\u70b9\u4e0a\u7ea617\u5c0f\u65f6\u5b8c\u6210\u5341\u4ebf\u7ea7k-NN\u56fe\u6784\u5efa\uff0c\u5408\u5e76\u540e\u7684\u7d22\u5f15\u56fe\u4fdd\u6301\u4e86\u7c7b\u4f3c\u7684\u8fd1\u90bb\u641c\u7d22\u6027\u80fd\u4f46\u6784\u5efa\u65f6\u95f4\u5927\u5927\u7f29\u77ed", "conclusion": "\u901a\u8fc7\u56fe\u5408\u5e76\u6280\u672f\u53ef\u4ee5\u9ad8\u6548\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684k-NN\u56fe\u548c\u7d22\u5f15\u56fe\uff0c\u89e3\u51b3\u5355\u673a\u5185\u5b58\u9650\u5236\u95ee\u9898"}}
{"id": "2509.11754", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.11754", "abs": "https://arxiv.org/abs/2509.11754", "authors": ["Zhiyuan Ren", "Mingxuan Lu", "Wenchi Cheng"], "title": "A Uniqueness Theorem for Distributed Computation under Physical Constraint", "comment": null, "summary": "Foundational models of computation often abstract away physical hardware\nlimitations. However, in extreme environments like In-Network Computing (INC),\nthese limitations become inviolable laws, creating an acute trilemma among\ncommunication efficiency, bounded memory, and robust scalability. Prevailing\ndistributed paradigms, while powerful in their intended domains, were not\ndesigned for this stringent regime and thus face fundamental challenges. This\npaper demonstrates that resolving this trilemma requires a shift in perspective\n- from seeking engineering trade-offs to deriving solutions from logical\nnecessity. We establish a rigorous axiomatic system that formalizes these\nphysical constraints and prove that for the broad class of computations\nadmitting an idempotent merge operator, there exists a unique, optimal\nparadigm. Any system satisfying these axioms must converge to a single normal\nform: Self-Describing Parallel Flows (SDPF), a purely data-centric model where\nstateless executors process flows that carry their own control logic. We\nfurther prove this unique paradigm is convergent, Turing-complete, and minimal.\nIn the same way that the CAP theorem established a boundary for what is\nimpossible in distributed state management, our work provides a constructive\ndual: a uniqueness theorem that reveals what is \\textit{inevitable} for\ndistributed computation flows under physical law.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u8303\u5f0fSDPF\uff0c\u901a\u8fc7\u516c\u7406\u5316\u7269\u7406\u7ea6\u675f\u8bc1\u660e\u5bf9\u4e8e\u5177\u6709\u5e73\u51e1\u5408\u5e76\u8fd0\u7b97\u7b26\u7684\u8ba1\u7b97\u7c7b\uff0c\u5b58\u5728\u552f\u4e00\u7684\u6700\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u7f51\u7edc\u5185\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u901a\u4fe1\u6548\u7387\u3001\u5185\u5b58\u9650\u5236\u548c\u53ef\u6269\u5c55\u6027\u7684\u4e09\u96be\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8ba1\u7b97\u6a21\u578b\u5ffd\u7565\u4e86\u7269\u7406\u786c\u4ef6\u7684\u6781\u7aef\u9650\u5236\uff0c\u800c\u5728\u7f51\u7edc\u5185\u8ba1\u7b97\u8fd9\u79cd\u6781\u7aef\u73af\u5883\u4e0b\uff0c\u901a\u4fe1\u6548\u7387\u3001\u6709\u9650\u5185\u5b58\u548c\u53ef\u6269\u5c55\u6027\u6784\u6210\u4e86\u4e0d\u53ef\u8c03\u548c\u7684\u4e09\u96be\u95ee\u9898\uff0c\u9700\u8981\u627e\u5230\u4ece\u903b\u8f91\u5fc5\u8981\u6027\u51fa\u53d1\u7684\u6839\u672c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u516c\u7406\u5316\u7cfb\u7edf\u6765\u5f62\u5f0f\u5316\u8fd9\u4e9b\u7269\u7406\u7ea6\u675f\uff0c\u5e76\u8bc1\u660e\u5bf9\u4e8e\u5177\u6709\u5e73\u51e1\u5408\u5e76\u8fd0\u7b97\u7b26\u7684\u8ba1\u7b97\u7c7b\uff0c\u5b58\u5728\u552f\u4e00\u7684\u6700\u4f18\u8303\u5f0f\uff1a\u81ea\u63cf\u8ff0\u5e76\u884c\u6d41(SDPF)\uff0c\u8fd9\u662f\u4e00\u79cd\u7eaf\u7cb9\u6570\u636e\u4e2d\u5fc3\u7684\u6a21\u578b\uff0c\u65e0\u72b6\u6001\u6267\u884c\u5668\u5904\u7406\u643a\u5e26\u81ea\u8eab\u63a7\u5236\u903b\u8f91\u7684\u6570\u636e\u6d41\u3002", "result": "\u8bc1\u660e\u4e86SDPF\u8303\u5f0f\u662f\u6536\u655b\u7684\u3001\u56fe\u7075\u5b8c\u5907\u7684\u548c\u6700\u5c0f\u7684\u3002\u7c7b\u4f3c\u4e8eCAP\u5b9a\u7406\u4e3a\u5206\u5e03\u5f0f\u72b6\u6001\u7ba1\u7406\u8bbe\u5b9a\u4e86\u4e0d\u53ef\u80fd\u6027\u8fb9\u754c\uff0c\u672c\u6587\u5219\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6784\u9020\u6027\u7684\u5bf9\u5076\uff1a\u4e00\u4e2a\u552f\u4e00\u6027\u5b9a\u7406\uff0c\u63ed\u793a\u4e86\u5728\u7269\u7406\u6cd5\u5219\u4e0b\u5206\u5e03\u5f0f\u8ba1\u7b97\u6d41\u7684\u5fc5\u7136\u6027\u3002", "conclusion": "\u89e3\u51b3\u7f51\u7edc\u5185\u8ba1\u7b97\u4e2d\u7684\u4e09\u96be\u95ee\u9898\u9700\u8981\u4ece\u5de5\u7a0b\u6298\u8870\u8f6c\u5411\u903b\u8f91\u5fc5\u8981\u6027\u7684\u601d\u7ef4\u65b9\u5f0f\u53d8\u9769\uff0cSDPF\u8303\u5f0f\u4f5c\u4e3a\u552f\u4e00\u7684\u6700\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u6781\u7aef\u73af\u5883\u4e0b\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5357\u3002"}}
{"id": "2509.11904", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.11904", "abs": "https://arxiv.org/abs/2509.11904", "authors": ["Julien Dallot", "Caio Caldeira", "Arash Pourdamghani", "Olga Goussevskaia", "Stefan Schmid"], "title": "LASLiN: A Learning-Augmented Peer-to-Peer Network", "comment": null, "summary": "We introduce a learning-augmented peer-to-peer (P2P) network design that\nleverages the predictions of traffic patterns to optimize the network's\ntopology. While keeping formal guarantees on the standard P2P metrics (routing\npath length, maximum degree), we optimize the network in a demand-aware manner\nand minimize the path lengths weighted by the peer-to-peer communication\ndemands. Our protocol is learning-augmented, meaning that each node receives an\nindividual, possibly inaccurate prediction about the future traffic patterns,\nwith the goal of improving the network's performances. We strike a trade-off\nbetween significantly improved performances when the predictions are correct\n(consistency) and polylogarithmic performances when the predictions are\narbitrary (robustness).\n  We have two main contributions. First, we consider the centralized setting\nand show that the problem of constructing an optimum static skip list network\n(SLN) is solvable in polynomial time and can be computed via dynamic\nprogramming. This problem is the natural demand-aware extension of the optimal\nskip list problem.\n  Second, we introduce the Uniform P2P protocol which generalizes skip list\nnetworks (SLN) by relaxing the node's heights from discrete to continuous. We\nshow that Uniform achieves state-of-the-art performances: logarithmic routing\nand maximum degree, both with high probability. We then use Uniform to build a\nlearning-augmented P2P protocol in order to incorporate demand-awareness,\nleading to our main contribution, LASLiN. We prove that the performances of\nLASLiN are consistent with those of an optimum static SLN with correct\npredictions (given via our dynamic programming approach), and are at most a\nlogarithmic factor off the state-of-the-art P2P protocols if the predictions\nare arbitrary wrong. For the special case of highly sparse demands, we show\nthat LASLiN achieves improved performances.", "AI": {"tldr": "\u57fa\u4e8e\u9884\u6d4b\u4ea4\u901a\u6a21\u5f0f\u7684\u5b66\u4e60\u589e\u5f3a\u578bP2P\u7f51\u7edc\u534f\u8baeLASLiN\uff0c\u5728\u4fdd\u6301\u6807\u51c6P2P\u6307\u6807\u7684\u540c\u65f6\u4f18\u5316\u9700\u6c42\u611f\u77e5\u7684\u8def\u7531\u6027\u80fd\uff0c\u5e73\u8861\u4e86\u9884\u6d4b\u51c6\u786e\u65f6\u7684\u9ad8\u6027\u80fd\u548c\u9884\u6d4b\u9519\u8bef\u65f6\u7684\u7a33\u5065\u6027", "motivation": "\u4f20\u7edfP2P\u7f51\u7edc\u8bbe\u8ba1\u5ffd\u89c6\u4e86\u8282\u70b9\u95f4\u7684\u5dee\u5f02\u5316\u901a\u4fe1\u9700\u6c42\uff0c\u65e0\u6cd5\u5229\u7528\u4ea4\u901a\u6a21\u5f0f\u9884\u6d4b\u6765\u4f18\u5316\u7f51\u7edc\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u5f62\u5f0f\u4fdd\u8bc1\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u5b66\u4e60\u9884\u6d4b\u6765\u63d0\u5347\u9700\u6c42\u611f\u77e5\u7684\u7f51\u7edc\u6027\u80fd", "method": "\u9996\u5148\u5728\u96c6\u4e2d\u5f0f\u73af\u5883\u4e2d\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u6c42\u89e3\u6700\u4f18\u9759\u6001\u8df3\u8868\u7f51\u7edc\u6784\u9020\u95ee\u9898\u3002\u7136\u540e\u63d0\u51faUniform P2P\u534f\u8bae\uff0c\u5c06\u8df3\u8868\u7f51\u7edc\u4e2d\u8282\u70b9\u9ad8\u5ea6\u4ece\u79bb\u6563\u653e\u677e\u4e3a\u8fde\u7eed\u3002\u6700\u7ec8\u57fa\u4e8eUniform\u6784\u5efa\u5b66\u4e60\u589e\u5f3a\u578bP2P\u534f\u8baeLASLiN\uff0c\u901a\u8fc7\u5404\u8282\u70b9\u7684\u4e2a\u4f53\u5316\u9884\u6d4b\u6765\u4f18\u5316\u7f51\u7edc\u62d3\u6251", "result": "\u8bc1\u660e\u4e86LASLiN\u5728\u9884\u6d4b\u51c6\u786e\u65f6\u80fd\u591f\u8fbe\u5230\u4e0e\u6700\u4f18\u9759\u6001\u8df3\u8868\u7f51\u7edc\u76f8\u4e00\u81f4\u7684\u6027\u80fd\uff0c\u800c\u5728\u9884\u6d4b\u5b8c\u5168\u9519\u8bef\u65f6\u4ecd\u80fd\u4fdd\u6301\u5bf9\u6570\u7ea7\u522b\u7684\u6027\u80fd\u3002\u5bf9\u4e8e\u9ad8\u5ea6\u7a00\u758f\u7684\u9700\u6c42\u6a21\u5f0f\uff0cLASLiN\u80fd\u591f\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0", "conclusion": "LASLiN\u534f\u8bae\u6210\u529f\u5b9e\u73b0\u4e86\u5b66\u4e60\u589e\u5f3a\u578bP2P\u7f51\u7edc\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u6807\u51c6P2P\u6307\u6807\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u5229\u7528\u4ea4\u901a\u6a21\u5f0f\u9884\u6d4b\u6765\u663e\u8457\u63d0\u5347\u7f51\u7edc\u6027\u80fd\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u4e00\u81f4\u6027\u548c\u7a33\u5065\u6027"}}
{"id": "2509.12136", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.12136", "abs": "https://arxiv.org/abs/2509.12136", "authors": ["Tomer Bitan", "Tal Kadosh", "Erel Kaplan", "Shira Meiri", "Le Chen", "Peter Morales", "Niranjan Hasabnis", "Gal Oren"], "title": "UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code Translation in HPC", "comment": "Accepted to IEEE HPEC conference 2025. 9 pages, incl references", "summary": "Translating programs between various parallel programming languages is an\nimportant problem in the high-performance computing (HPC) community. Existing\ntools for this problem are either too narrow in scope and/or outdated. Recent\nexplosive growth in the popularity of large language models (LLMs) and their\nability to generate and translate code offers a potential alternative approach.\nToward that end, we first need to systematically evaluate the ability of LLMs\nto translate between parallel languages.\n  In this work, we introduce UniPar, a systematic evaluation framework for\nLLM-based parallel code translation. Specifically, in this work, we target\ntranslations between serial code, CUDA, and OpenMP. Our goal is to assess how\nwell current instruction-tuned LLMs -- specifically GPT-4o-mini and\nLLaMA-3.3-70B-Instruct -- can be used out of the box or enhanced through known\nstrategies. We evaluated four major usage modes: hyperparameter optimization\nfor decoding, zero- and few-shot prompting, supervised fine-tuning, and\niterative feedback through compiler-based repair. As a part of the evaluation,\nwe construct a new dataset called PARATRANS, covering both serial-to-parallel\ntranslation and cross-paradigm transformations.\n  Our findings reveal that while off-the-shelf models struggle under the\ndefault settings (e.g., GPT-4o-mini achieves only 46% compilation and 15%\nfunctional correctness), our UniPar methodology -- combining fine-tuning,\nhyperparameter tuning, and compiler-guided repair -- improves performance by up\nto 2X (69% compilation and 33% correctness). We believe that our findings will\nprovide useful insights for researchers to further improve LLMs for the\nparallel language translation problem.\n  UniPar source code and PARATRANS dataset are available at our GitHub\nrepository https://github.com/Scientific-Computing-Lab/UniPar_AI.", "AI": {"tldr": "UniPar\u662f\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30LLM\u5e76\u884c\u4ee3\u7801\u7ffb\u8bd1\u80fd\u529b\u7684\u6846\u67b6\uff0c\u9488\u5bf9\u4e32\u884c\u4ee3\u7801\u3001CUDA\u548cOpenMP\u4e4b\u95f4\u7684\u7ffb\u8bd1\uff0c\u901a\u8fc7\u5fae\u8c03\u3001\u8d85\u53c2\u6570\u4f18\u5316\u548c\u7f16\u8bd1\u5668\u5f15\u5bfc\u4fee\u590d\u7b49\u65b9\u6cd5\uff0c\u5c06GPT-4o-mini\u7684\u6027\u80fd\u4ece46%\u7f16\u8bd1\u7387\u548c15%\u529f\u80fd\u6b63\u786e\u7387\u63d0\u5347\u523069%\u548c33%\u3002", "motivation": "\u73b0\u6709\u5e76\u884c\u7f16\u7a0b\u8bed\u8a00\u7ffb\u8bd1\u5de5\u5177\u8303\u56f4\u6709\u9650\u4e14\u8fc7\u65f6\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5e76\u884c\u4ee3\u7801\u7ffb\u8bd1\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1UniPar\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b\u56db\u79cd\u4e3b\u8981\u4f7f\u7528\u6a21\u5f0f\uff1a\u89e3\u7801\u8d85\u53c2\u6570\u4f18\u5316\u3001\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\u3001\u76d1\u7763\u5fae\u8c03\u3001\u4ee5\u53ca\u57fa\u4e8e\u7f16\u8bd1\u5668\u7684\u8fed\u4ee3\u4fee\u590d\u3002\u6784\u5efa\u4e86PARATRANS\u6570\u636e\u96c6\u3002", "result": "\u73b0\u6210\u6a21\u578b\u5728\u9ed8\u8ba4\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4e0d\u4f73\uff08GPT-4o-mini\u4ec546%\u7f16\u8bd1\u7387\u548c15%\u529f\u80fd\u6b63\u786e\u7387\uff09\uff0c\u4f46UniPar\u65b9\u6cd5\u5c06\u6027\u80fd\u63d0\u5347\u81f3\u591a2\u500d\uff0869%\u7f16\u8bd1\u7387\u548c33%\u6b63\u786e\u7387\uff09\u3002", "conclusion": "UniPar\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u5e76\u884c\u8bed\u8a00\u7ffb\u8bd1\u65b9\u9762\u7684\u6027\u80fd\uff0c\u4e3a\u7814\u7a76\u8005\u8fdb\u4e00\u6b65\u6539\u8fdbLLM\u63d0\u4f9b\u4e86\u6709\u7528\u89c1\u89e3\u3002"}}
{"id": "2509.12138", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.12138", "abs": "https://arxiv.org/abs/2509.12138", "authors": ["Mengjiao Han", "Andres Sewell", "Joseph Insley", "Janet Knowles", "Victor A. Mateevitsi", "Michael E. Papka", "Steve Petruzza", "Silvio Rizzi"], "title": "Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization", "comment": null, "summary": "3D Gaussian Splatting (3D-GS) has recently emerged as a powerful technique\nfor real-time, photorealistic rendering by optimizing anisotropic Gaussian\nprimitives from view-dependent images. While 3D-GS has been extended to\nscientific visualization, prior work remains limited to single-GPU settings,\nrestricting scalability for large datasets on high-performance computing (HPC)\nsystems. We present a distributed 3D-GS pipeline tailored for HPC. Our approach\npartitions data across nodes, trains Gaussian splats in parallel using\nmulti-nodes and multi-GPUs, and merges splats for global rendering. To\neliminate artifacts, we add ghost cells at partition boundaries and apply\nbackground masks to remove irrelevant pixels. Benchmarks on the\nRichtmyer-Meshkov datasets (about 106.7M Gaussians) show up to 3X speedup\nacross 8 nodes on Polaris while preserving image quality. These results\ndemonstrate that distributed 3D-GS enables scalable visualization of\nlarge-scale scientific data and provide a foundation for future in situ\napplications.", "AI": {"tldr": "\u5206\u5e03\u5f0f3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u7528\u4e8e\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7684\u5927\u89c4\u6a21\u79d1\u5b66\u6570\u636e\u53ef\u89c6\u5316\uff0c\u901a\u8fc7\u591a\u8282\u70b9\u591aGPU\u5e76\u884c\u8bad\u7ec3\u5b9e\u73b03\u500d\u52a0\u901f", "motivation": "\u73b0\u67093D-GS\u6280\u672f\u4ec5\u9650\u4e8e\u5355GPU\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u4e0a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u53ef\u6269\u5c55\u6027\u9700\u6c42", "method": "\u63d0\u51fa\u5206\u5e03\u5f0f3D-GS\u6d41\u6c34\u7ebf\uff1a\u8de8\u8282\u70b9\u6570\u636e\u5206\u533a\u3001\u591a\u8282\u70b9\u591aGPU\u5e76\u884c\u8bad\u7ec3\u9ad8\u65af\u6cfc\u6e85\u3001\u6dfb\u52a0\u8fb9\u754c\u5e7d\u7075\u5355\u5143\u548c\u80cc\u666f\u63a9\u7801\u6d88\u9664\u4f2a\u5f71\u3001\u5408\u5e76\u6cfc\u6e85\u8fdb\u884c\u5168\u5c40\u6e32\u67d3", "result": "\u5728Polaris\u7cfb\u7edf8\u4e2a\u8282\u70b9\u4e0a\u5bf9Richtmyer-Meshkov\u6570\u636e\u96c6\uff08\u7ea61.067\u4ebf\u9ad8\u65af\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b9e\u73b0\u9ad8\u8fbe3\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf", "conclusion": "\u5206\u5e03\u5f0f3D-GS\u80fd\u591f\u5b9e\u73b0\u5927\u89c4\u6a21\u79d1\u5b66\u6570\u636e\u7684\u53ef\u6269\u5c55\u53ef\u89c6\u5316\uff0c\u4e3a\u672a\u6765\u539f\u4f4d\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2509.12141", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.12141", "abs": "https://arxiv.org/abs/2509.12141", "authors": ["Weihao Zhu", "Long Shi", "Kang Wei", "Zhen Mei", "Zhe Wang", "Jiaheng Wang", "Jun Li"], "title": "When MoE Meets Blockchain: A Trustworthy Distributed Framework of Large Models", "comment": null, "summary": "As an enabling architecture of Large Models (LMs), Mixture of Experts (MoE)\nhas become prevalent thanks to its sparsely-gated mechanism, which lowers\ncomputational overhead while maintaining learning performance comparable to\ndense LMs. The essence of MoE lies in utilizing a group of neural networks\n(called experts) with each specializing in different types of tasks, along with\na trainable gating network that selectively activates a subset of these experts\nto handle specific tasks. Traditional cloud-based MoE encounters challenges\nsuch as prolonged response latency, high bandwidth consumption, and data\nprivacy leakage. To address these issues, researchers have proposed to deploy\nMoE over distributed edge networks. However, a key concern of distributed MoE\nframeworks is the lack of trust in data interactions among distributed experts\nwithout the surveillance of any trusted authority, and thereby prone to\npotential attacks such as data manipulation. In response to the security issues\nof traditional distributed MoE, we propose a blockchain-aided trustworthy MoE\n(B-MoE) framework that consists of three layers: the edge layer, the blockchain\nlayer, and the storage layer. In this framework, the edge layer employs the\nactivated experts downloaded from the storage layer to process the learning\ntasks, while the blockchain layer functions as a decentralized trustworthy\nnetwork to trace, verify, and record the computational results of the experts\nfrom the edge layer. The experimental results demonstrate that B-MoE is more\nrobust to data manipulation attacks than traditional distributed MoE during\nboth the training and inference processes.", "AI": {"tldr": "\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u53ef\u4fe1\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08B-MoE\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5206\u5e03\u5f0fMoE\u5728\u6570\u636e\u4ea4\u4e92\u4e2d\u7684\u4fe1\u4efb\u95ee\u9898\u548c\u6570\u636e\u64cd\u7eb5\u653b\u51fb\u98ce\u9669", "motivation": "\u4f20\u7edf\u4e91\u7aefMoE\u9047\u5230\u54cd\u5e94\u5ef6\u8fdf\u3001\u5e26\u5bbd\u6d88\u8017\u9ad8\u3001\u6570\u636e\u9690\u79c1\u6cc4\u6f0f\u95ee\u9898\uff0c\u800c\u5206\u5e03\u5f0f\u8fb9\u7f18MoE\u7f3a\u4e4f\u4fe1\u4efb\u673a\u5236\uff0c\u5bb9\u6613\u53d7\u5230\u6570\u636e\u64cd\u7eb5\u653b\u51fb", "method": "\u8bbe\u8ba1\u4e09\u5c42B-MoE\u6846\u67b6\uff1a\u8fb9\u7f18\u5c42\u4e0b\u8f7d\u5e76\u8fd0\u884c\u4e13\u5bb6\uff0c\u533a\u5757\u94fe\u5c42\u4f5c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u4fe1\u4efb\u7f51\u7edc\u8ffd\u8e2a\u3001\u9a8c\u8bc1\u548c\u8bb0\u5f55\u8ba1\u7b97\u7ed3\u679c\uff0c\u5b58\u50a8\u5c62\u4f9b\u5e94\u4e13\u5bb6", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aB-MoE\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6bd4\u4f20\u7edf\u5206\u5e03\u5f0fMoE\u66f4\u52a0\u8010\u53d7\u6570\u636e\u64cd\u7eb5\u653b\u51fb", "conclusion": "\u533a\u5757\u94fe\u6280\u672f\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5206\u5e03\u5f0fMoE\u6846\u67b6\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u5927\u6a21\u578b\u5728\u8fb9\u7f18\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
