<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 5]
- [cs.DC](#cs.DC) [Total: 20]
- [cs.AR](#cs.AR) [Total: 10]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Towards Automatic Error Recovery in Parsing Expression](https://arxiv.org/abs/2507.03629)
*Sérgio Queiroz de Medeiros,Fabio Mascarenhas*

Main category: cs.PL

TL;DR: 本文提出了一种自动为PEG（解析表达式文法）添加标签和恢复表达式的算法，以改进IDE中解析器的错误恢复能力。


<details>
  <summary>Details</summary>
Motivation: 在IDE中，解析器需要为语法无效的程序构建AST以支持重构和代码完成功能，因此错误恢复是关键。

Method: 通过算法自动为PEG添加标签和恢复表达式，减少手动标注的工作量。

Result: 在Titan编程语言的解析器中应用该算法，结果表明只需少量手动干预即可为大多数不相交的PEG生成错误恢复解析器。

Conclusion: 该算法有效简化了PEG的错误恢复机制实现，适用于IDE中的解析器。

Abstract: Error recovery is an essential feature for a parser that should be plugged in
Integrated Development Environments (IDEs), which must build Abstract Syntax
Trees (ASTs) even for syntactically invalid programs in order to offer features
such as automated refactoring and code completion.
  Parsing Expressions Grammars (PEGs) are a formalism that naturally describes
recursive top-down parsers using a restricted form of backtracking. Labeled
failures are a conservative extension of PEGs that adds an error reporting
mechanism for PEG parsers, and these labels can also be associated with
recovery expressions to also be an error recovery mechanism. These expressions
can use the full expressivity of PEGs to recover from syntactic errors.
  Manually annotating a large grammar with labels and recovery expressions can
be difficult. In this work, we present an algorithm that automatically
annotates a PEG with labels, and builds their corresponding recovery
expressions. We evaluate this algorithm by adding error recovery to the parser
of the Titan programming language. The results shown that with a small amount
of manual intervention our algorithm can be used to produce error recovering
parsers for PEGs where most of the alternatives are disjoint.

</details>


### [2] [Semantically Separating Nominal Wyvern for Usability and Decidability](https://arxiv.org/abs/2507.03867)
*Yu Xiang Zhu,Amos Robinson,Sophia Roshal,Timothy Mou,Julian Mackay,Jonathan Aldrich,Alex Potanin*

Main category: cs.PL

TL;DR: Nominal Wyvern 是一种类似 DOT 的依赖类型系统，通过名义声明和结构精化的分离，解决了 DOT 中子类型可判定性问题，同时保持了表达力。


<details>
  <summary>Details</summary>
Motivation: DOT 计算法结合了函数式和面向对象特性，但牺牲了子类型的可判定性。现有方法要么降低表达力，要么牺牲易用性。本文旨在解决这一问题。

Method: 引入 Nominal Wyvern，通过名义声明和结构精化的分离，避免不可判定的递归结构类型，并采用 material/shape 分离技术保证可判定性。

Result: 设计了一个语法和结构对 OOP 用户友好的类型系统，实现了子类型可判定性，同时保留了 F-有界多态性和模块系统的表达力。

Conclusion: Nominal Wyvern 提供了一种平衡表达力和可判定性的解决方案，适用于实际编程需求。

Abstract: The Dependent Object Types (DOT) calculus incorporates concepts from
functional languages (e.g. modules) with traditional object-oriented features
(e.g. objects, subtyping) to achieve greater expressivity (e.g. F-bounded
polymorphism). However, this merger of paradigms comes at the cost of subtype
decidability. Recent work on bringing decidability to DOT has either sacrificed
expressiveness or ease of use. The unrestricted construction of recursive types
and type bounds has made subtype decidability a much harder problem than in
traditional object-oriented programming.
  Recognizing this, our paper introduces Nominal Wyvern, a DOT-like dependent
type system that takes an alternative approach: instead of having a uniform
structural syntax like DOT, Nominal Wyvern is designed around a "semantic
separation" between the nominal declaration of recursive types on the one hand,
and the structural refinement of those types when they are used on the other.
This design naturally guides the user to avoid writing undecidably recursive
structural types.
  From a technical standpoint, this separation also makes guaranteeing
decidability possible by allowing for an intuitive adaptation of material/shape
separation, a technique for achieving subtype decidability by separating types
responsible for subtyping constraints from types that represent concrete data.
The result is a type system with syntax and structure familiar to OOP users
that achieves decidability without compromising the expressiveness of F-bounded
polymorphism and module systems as they are used in practice.

</details>


### [3] [CCR 2.0: High-level Reasoning for Conditional Refinements](https://arxiv.org/abs/2507.04298)
*Youngju Song,Minki Cho*

Main category: cs.PL

TL;DR: 论文提出了一种改进的CCR 2.0模型，结合了细化与分离逻辑的优点，提供了更好的组合性定理和隐藏模型细节的证明技术。


<details>
  <summary>Details</summary>
Motivation: 为了融合细化与分离逻辑的互补优势，提升形式验证的实用性和证明复用性。

Method: 提出CCR 2.0模型，改进组合性定理并设计隐藏模型细节的证明技术。

Result: CCR 2.0在Coq中形式化，实现了更好的组合性和用户友好性。

Conclusion: CCR 2.0为低层系统形式验证提供了更高效和直观的工具。

Abstract: In recent years, great progress has been made in the field of formal
verification for low-level systems. Many of them are based on one of two
popular approaches: refinement or separation logic. These two approaches are
very different in nature and offer complementary benefits in terms of
compositionality. Recently, to fuse these benefits in a unified mechanism, a
new approach called Conditional Contextual Refinement (CCR 1.0 for short) was
proposed. In this paper, we advance the model of CCR 1.0 and provide novel and
intuitive reasoning principles, resulting in: CCR 2.0. Specifically, CCR 2.0
(i) comes with a better compositionality theorem, having the practical benefit
of facilitating more proof reuse, and (ii) provides a proof technique that
hides model-level (i.e., resources of the separation logic) details from the
user. Achieving this goal was challenging due to non-trivial counterexamples
which necessitated us to devise novel notions. Our results are formalized in
Coq.

</details>


### [4] [Retargeting an Abstract Interpreter for a New Language by Partial Evaluation](https://arxiv.org/abs/2507.04316)
*Jay Lee*

Main category: cs.PL

TL;DR: 提出了一种通过部分评估技术自动从现有抽象解释器生成新语言抽象解释器的方法，减少开发时间。


<details>
  <summary>Details</summary>
Motivation: 开发静态分析器耗时且复杂，需要一种方法来自动化这一过程。

Method: 利用部分评估技术，将源语言的抽象解释器根据目标语言的语义进行特化。

Result: 方法能有效将一个语言的抽象解释器转换为另一个语言的正确分析器。

Conclusion: 该技术避免了从头开发新目标分析器的需求，显著提升了效率。

Abstract: It is well-known that abstract interpreters can be systematically derived
from their concrete counterparts using a "recipe," but developing sound static
analyzers remains a time-consuming task. Reducing the effort required and
mechanizing the process of developing analyzers continues to be a significant
challenge. Is it possible to automatically retarget an existing abstract
interpreter for a new language?
  We propose a novel technique to automatically derive abstract interpreters
for various languages from an existing abstract interpreter. By leveraging
partial evaluation, we specialize an abstract interpreter for a source
language. The specialization is performed using the semantics of target
languages written in the source language. Our approach eliminates the need to
develop analyzers for new targets from scratch. We show that our method can
effectively retarget an abstract interpreter for one language into a correct
analyzer for another language.

</details>


### [5] [React-tRace: A Semantics for Understanding React Hooks](https://arxiv.org/abs/2507.05234)
*Jay Lee,Joongwon Ahn,Kwangkeun Yi*

Main category: cs.PL

TL;DR: 本文通过形式化React Hooks的语义，提出了React-tRace框架，帮助开发者更清晰地理解其行为，并通过理论和实证验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: React Hooks的语义对开发者来说不够透明，容易导致UI错误，因此需要一种更清晰的形式化方法来解释其行为。

Method: 提出React-tRace框架，形式化Hooks的语义，并通过定义解释器和测试套件验证其准确性。

Result: 理论和实证表明，React-tRace能准确捕捉Hooks的行为，并提供了一个可视化工具帮助开发者理解。

Conclusion: React-tRace为开发者提供了更清晰的Hooks语义理解，减少了UI错误的可能性。

Abstract: React has become the most widely used web front-end framework, enabling the
creation of user interfaces in a declarative and compositional manner. Hooks
are a set of APIs that manage side effects in functional components in React.
However, their semantics are often seen as opaque to developers, leading to UI
bugs. In this paper, we formalize the semantics of the essence of React Hooks
we name React-tRace, providing a framework that clarifies their behavior. We
demonstrate that our model captures the behavior of React, by theoretically
showing that it embodies essential properties of Hooks and empirically
comparing our React-tRace-definitional interpreter against a test suite.
Furthermore, we showcase a practical visualization tool based on the
formalization to demonstrate how developers can better understand the semantics
of Hooks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [ZettaLith: An Architectural Exploration of Extreme-Scale AI Inference Acceleration](https://arxiv.org/abs/2507.02871)
*Kia Silverbrook*

Main category: cs.DC

TL;DR: ZettaLith是一种新型计算架构，旨在将AI推理的成本和功耗降低1000倍以上，相比当前GPU系统。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统的高计算成本和功耗限制了其广泛部署和扩展，现有硬件存在效率瓶颈。

Method: ZettaLith通过放弃通用GPU应用，采用多项协同设计的架构创新，基于现有数字电子技术实现高效推理。

Result: 预计到2027年，单个ZettaLith机架可实现1.507 zettaFLOPS，性能提升1047倍，功耗效率提升1490倍，成本效益提升2325倍。

Conclusion: ZettaLith为AI推理提供了一种高效、低成本且可扩展的解决方案，适用于从桌面到移动设备的多种规模。

Abstract: The high computational cost and power consumption of current and anticipated
AI systems present a major challenge for widespread deployment and further
scaling. Current hardware approaches face fundamental efficiency limits. This
paper introduces ZettaLith, a scalable computing architecture designed to
reduce the cost and power of AI inference by over 1,000x compared to current
GPU-based systems. Based on architectural analysis and technology projections,
a single ZettaLith rack could potentially achieve 1.507 zettaFLOPS in 2027 -
representing a theoretical 1,047x improvement in inference performance, 1,490x
better power efficiency, and could be 2,325x more cost-effective than current
leading GPU racks for FP4 transformer inference. The ZettaLith architecture
achieves these gains by abandoning general purpose GPU applications, and via
the multiplicative effect of numerous co-designed architectural innovations
using established digital electronic technologies, as detailed in this paper.
ZettaLith's core architectural principles scale down efficiently to exaFLOPS
desktop systems and petaFLOPS mobile chips, maintaining their roughly 1,000x
advantage. ZettaLith presents a simpler system architecture compared to the
complex hierarchy of current GPU clusters. ZettaLith is optimized exclusively
for AI inference and is not applicable for AI training.

</details>


### [7] [Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications](https://arxiv.org/abs/2507.03114)
*Seonho Lee,Jihwan Oh,Junkyum Kim,Seokjin Go,Jongse Park,Divya Mahajan*

Main category: cs.DC

TL;DR: 该论文深入研究了GPU加速系统中计算与通信重叠的影响，发现重叠策略虽能减少通信瓶颈，但可能导致计算速度下降18.9%，同时在某些配置下增加功耗。


<details>
  <summary>Details</summary>
Motivation: 由于模型规模庞大，分布式训练中计算与通信的重叠策略对性能至关重要，但当前共识认为应积极重叠以减轻开销，本研究旨在验证其实际效果。

Method: 通过系统评估最新GPU硬件特性（如数值精度、专用核心和功耗限制）对分布式训练负载的影响，并分析重叠策略的性能与功耗表现。

Result: 重叠计算与通信平均导致18.9%的计算速度下降，但比顺序执行快10.2%；专用数据路径和优化数值精度可缓解部分问题，但可能引发资源竞争和功耗增加。

Conclusion: 需平衡策略以优化能效和训练吞吐量，硬件特性与重叠策略的权衡是关键。

Abstract: This paper provides an in-depth characterization of GPU-accelerated systems,
to understand the interplay between overlapping computation and communication
which is commonly employed in distributed training settings. Due to the large
size of models, distributing them across multiple devices is required.
Overlapping strategies, which enable concurrent computation and communication,
are critical for mitigating communication bottlenecks and maximizing GPU
utilization. However, the current consensus is that we should always and
aggressively overlap compute and communication to mitigate the overhead of
distribution. By systematically evaluating state-of-the-art GPUs, this study
investigates the impact of hardware features such as numeric precision,
specialized cores, and power capping on distributed training workloads.
Comprehensive experiments and studies showcase the effects of overlapping
strategies on performance and power consumption across varying scenarios. We
observe that overlapping computation and communication can result in an average
computational slowdown of 18.9%, with a maximum of 40.0% slowdown. This
slowdown is in comparison to the scenario when no communication was happening
with the compute. We consider this an ideal execution scenario, where the
communication in parallel has not impact on the compute time. However,
performing computation and communication sequentially is, on average, 10.2%
slower than overlapped execution, with a maximum slowdown of 26.6%. We further
observe, while specialized datapath and optimized numeric precision mitigate
certain slowdowns, overlapping execution can lead to resource contention and
also increase power consumption under specific configurations. The analysis
also uncovers trade-offs introduced by power and frequency capping, emphasizing
the importance of balanced strategies to optimize energy efficiency and
training throughput.

</details>


### [8] [Symbiosis: Multi-Adapter Inference and Fine-Tuning](https://arxiv.org/abs/2507.03220)
*Saransh Gupta,Umesh Deshpande,Travis Janssen,Swami Sundararaman*

Main category: cs.DC

TL;DR: Symbiosis提出了一种参数高效微调（PEFT）的框架，解决了现有框架在多适配器推理和微调中的资源管理、隐私保护和性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT框架在多适配器场景下存在资源浪费、灵活性不足和隐私泄露问题，Symbiosis旨在解决这些问题。

Method: 通过共享基础模型层和分离执行技术，Symbiosis实现了多任务间的资源高效共享和灵活管理。

Result: 在Llama2-13B上的实验表明，Symbiosis在相同GPU资源下可微调4倍数量的适配器。

Conclusion: Symbiosis为多适配器场景提供了高效、灵活且隐私保护的解决方案。

Abstract: Parameter-efficient fine-tuning (PEFT) allows model builders to capture the
task specific parameters into adapters, which are a fraction of the size of the
original base model. Popularity of PEFT technique for fine-tuning has led to
creation of a large number of adapters for popular Large Language Models
(LLMs). However, existing frameworks fall short in supporting inference or
fine-tuning with multiple adapters in the following ways. 1) For fine-tuning,
each job needs to deploy its dedicated base model instance, which results in
excessive GPU memory consumption and poor GPU utilization. 2) While popular
inference platforms can serve multiple PEFT adapters, they do not allow
independent resource management or mixing of different PEFT methods. 3) They
cannot share resources (such as base model instance) between inference and
fine-tuning jobs. 4) They do not provide privacy to users who may not wish to
expose their fine-tuned parameters to service providers. In Symbiosis, we
address the above problems by enabling as-a-service deployment of base model.
The base model layers can be shared across multiple inference or fine-tuning
processes. Our split-execution technique decouples the execution of
client-specific adapters and layers from the frozen base model layers offering
them flexibility to manage their resources, to select their fine-tuning method,
to achieve their performance goals. Our approach is transparent to models and
works out-of-the-box for most models in the transformers library. Our
evaluation on Llama2-13B shows the compared to baseline, Symbiosis can
fine-tune 4X more adapters on the same set of GPUs in the same amount of time.

</details>


### [9] [Analysis and Optimized CXL-Attached Memory Allocation for Long-Context LLM Fine-Tuning](https://arxiv.org/abs/2507.03305)
*Yong-Cheng Liaw,Shuo-Han Chen*

Main category: cs.DC

TL;DR: 研究探讨了利用CXL内存扩展CPU内存以支持更大模型和更长上下文长度的LLM微调，提出了CXL感知分配策略以减少性能开销。


<details>
  <summary>Details</summary>
Motivation: 解决LLM微调中CPU内存容量不足的问题，探索CXL内存作为扩展方案的可行性。

Method: 通过基准测试量化CXL内存与CPU、GPU间数据传输的性能开销，提出CXL感知分配策略优化工作负载分区。

Result: 实验表明，CXL感知分配和多AIC使用显著减少带宽竞争，提升长上下文LLM微调效率。

Conclusion: CXL是解锁CPU卸载在长上下文LLM微调中潜力的有前景方案。

Abstract: The growing prevalence of Large Language Models (LLMs) and their substantial
memory requirements have prompted renewed interest in CPU offloading as a
method to compensate for limited GPU memory. In particular, when CPU memory is
leveraged to temporarily store intermediate states of LLMs, CPU memory becomes
a new bottleneck and soon reaches the capacity limitation of commodity CPUs. In
this work, we investigate the effectiveness of Compute Express Link (CXL)
add-in card (AIC) memory as an extension to CPU memory, enabling larger model
sizes and longer context lengths during fine-tuning. Through extensive
benchmarking, this study quantifies the performance overhead introduced by
transferring data between CXL memory, CPU, and GPUs, focusing on how
concurrency and data volume influence bandwidth utilization and latency. This
study also compares CPUbased optimizer steps when model parameters, gradients,
and optimizer states reside in local memory versus CXL memory, revealing that
naive adoption of CXL often degrades performance during the optimizer phase. To
overcome these challenges, this study proposes a CXL-aware allocation to
strategically partition CPU offloading workloads across both local and CXL
memory. This study further demonstrates that employing multiple AICs
significantly reduces bandwidth contention, thus improving scalability.
Experimental results show that these optimizations enable efficient
long-context LLM fine-tuning, underscoring CXL as a promising avenue for
unlocking the full potential of CPU offloading in long-context LLM fine-tuning.

</details>


### [10] [A Distributed Consensus Algorithm for Autonomous Vehicles Deciding Entering Orders on Intesections without Traffic Signals](https://arxiv.org/abs/2507.03486)
*Younjeong Lee,Young Yoon*

Main category: cs.DC

TL;DR: 提出了一种基于投票的分布式共识算法，用于无人驾驶车辆（CAVs）与非自动驾驶车辆（HVs）共存的无信号交叉口通行优先级确定。


<details>
  <summary>Details</summary>
Motivation: 解决CAVs与HVs在无信号交叉口共存时的通行优先级问题，提高交通效率。

Method: 采用类似Raft的投票分布式共识算法，结合候选人和领导者选举过程，确保安全性和活跃性。

Result: 实验表明，算法在混合交通条件下平均30-40毫秒内达成共识。

Conclusion: 该算法能有效提升无信号交叉口的交通流效率，尤其在混合交通和通信不可靠情况下。

Abstract: We propose a methodology for connected autonomous vehicles (CAVs) to
determine their passing priority at unsignalized intersections where they
coexist with human-driven vehicles (HVs). Assuming that CAVs can perceive the
entry order of surrounding vehicles using computer vision technology and are
capable of avoiding collisions, we introduce a voting-based distributed
consensus algorithm inspired by Raft to resolve tie-breaking among
simultaneously arriving CAVs. The algorithm is structured around the candidate
and leader election processes and incorporates a minimal consensus quorum to
ensure both safety and liveness among CAVs under typical asynchronous
communication conditions. Assuming CAVs to be SAE (Society of Automotive
Engineers) Level-4 or higher autonomous vehicles, we implemented the proposed
distributed consensus algorithm using gRPC. By adjusting variables such as the
CAV-to-HV ratio, intersection scale, and the processing time of computer vision
modules, we demonstrated that stable consensus can be achieved even under
mixed-traffic conditions involving HVs without adequate functionalities to
interact with CAVs. Experimental results show that the proposed algorithm
reached consensus at a typical unsignalized four-way, two-lane intersection in
approximately 30-40 ms on average. A secondary vision-based system is employed
to complete the crossing priorities based on the recognized lexicographical
order of the license plate numbers in case the consensus procedure times out on
an unreliable vehicle-to-vehicle communication network. The significance of
this study lies in its ability to improve traffic flow at unsignalized
intersections by enabling rapid determination of passing priority through
distributed consensus even under mixed traffic with faulty vehicles.

</details>


### [11] [On Optimizing Resource Utilization in Distributed Connected Components](https://arxiv.org/abs/2507.03695)
*Mohsen Koohi Esfahani*

Main category: cs.DC

TL;DR: 论文提出两种分布式连通分量算法SiskinCC和RobinCC，优化内存和网络带宽利用，最高提速58.5倍。


<details>
  <summary>Details</summary>
Motivation: 加速分布式连通分量计算，优化内存和网络带宽利用。

Method: 基于Jayanti-Tarjan并查集算法，设计SiskinCC和RobinCC，分别优化共享内存访问和网络带宽利用。

Result: 在5000亿边和117亿顶点的图上，2048核环境下，提速达58.5倍。

Conclusion: SiskinCC和RobinCC显著提升分布式连通分量计算效率。

Abstract: Connected Components (CC) is a core graph problem with numerous applications.
This paper investigates accelerating distributed CC by optimizing memory and
network bandwidth utilization. We present two novel distributed CC algorithms,
SiskinCC and RobinCC, which are built upon the Jayanti-Tarjan disjoint set
union algorithm. To optimize memory utilization, SiskinCC and RobinCC are
designed to facilitate efficient access to a shared array for all cores running
in a machine. This allows execution of faster algorithms with larger memory
bounds. SiskinCC leverages the continuous inter-machine communication during
the computation phase to reduce the final communication overhead and RobinCC
leverages the structural properties of real-world graphs to optimize network
bandwidth utilization. Our evaluation against state-of-the-art CC algorithms,
using real-world and synthetic graphs with up to 500 billion edges and 11.7
billion vertices, and on up to 2048 CPU cores, demonstrates that SiskinCC and
RobinCC achieve up to 58.5 times speedup.

</details>


### [12] [On Fault Tolerance of Data Storage Systems: A Holistic Perspective](https://arxiv.org/abs/2507.03849)
*Mai Zheng,Duo Zhang,Ahmed Dajani*

Main category: cs.DC

TL;DR: 本文概述了现代数据存储系统的架构、组件及故障检测与容错技术，并探讨了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 随着数据量的激增，存储系统的故障容错能力变得至关重要，但复杂的软硬件交互可能导致潜在问题。

Method: 介绍存储系统架构与组件，分析跨层故障检测与容错技术。

Result: 总结了影响系统恢复与数据完整性的关键问题。

Conclusion: 提出了未来研究中的开放挑战与方向。

Abstract: Data storage systems serve as the foundation of digital society. The enormous
data generated by people on a daily basis make the fault tolerance of data
storage systems increasingly important. Unfortunately, modern storage systems
consist of complicated hardware and software layers interacting with each
other, which may contain latent bugs that elude extensive testing and lead to
data corruption, system downtime, or even unrecoverable data loss in practice.
In this chapter, we take a holistic view to introduce the typical architecture
and major components of modern data storage systems (e.g., solid state drives,
persistent memories, local file systems, and distributed storage management at
scale). Next, we discuss a few representative bug detection and fault tolerance
techniques across layers with a focus on issues that affect system recovery and
data integrity. Finally, we conclude with open challenges and future work.

</details>


### [13] [FedFog: Resource-Aware Federated Learning in Edge and Fog Networks](https://arxiv.org/abs/2507.03952)
*Somayeh Sobati-M*

Main category: cs.DC

TL;DR: FedFog是一个结合边缘/雾计算与联邦学习的仿真框架，优化了模型收敛、延迟和能效。


<details>
  <summary>Details</summary>
Motivation: 当前仿真工具未能有效整合无服务器架构与隐私保护的联邦学习，限制了边缘智能系统的研究。

Method: 扩展FogFaaS环境，引入自适应FL调度器、隐私数据流和资源感知编排，模拟动态物联网场景。

Result: 在基准数据集上，FedFog相比传统FL或FaaS，加速了模型收敛、降低了延迟并提高了能效。

Conclusion: FedFog是研究可扩展智能边缘系统的有力工具。

Abstract: As edge and fog computing become central to modern distributed systems,
there's growing interest in combining serverless architectures with
privacy-preserving machine learning techniques like federated learning (FL).
However, current simulation tools fail to capture this integration effectively.
In this paper, we introduce FedFog, a simulation framework that extends the
FogFaaS environment to support FL-aware serverless execution across edge-fog
infrastructures. FedFog incorporates an adaptive FL scheduler,
privacy-respecting data flow, and resource-aware orchestration to emulate
realistic, dynamic conditions in IoT-driven scenarios. Through extensive
simulations on benchmark datasets, we demonstrate that FedFog accelerates model
convergence, reduces latency, and improves energy efficiency compared to
conventional FL or FaaS setups-making it a valuable tool for researchers
exploring scalable, intelligent edge systems.

</details>


### [14] [One-Bit Model Aggregation for Differentially Private and Byzantine-Robust Personalized Federated Learning](https://arxiv.org/abs/2507.03973)
*Muhang Lan,Song Xiao,Wenyi Zhang*

Main category: cs.DC

TL;DR: PRoBit+是一种基于模型正则化的个性化联邦学习框架，通过一比特随机量化和最大似然估计解决通信开销、拜占庭攻击和隐私泄露问题。


<details>
  <summary>Details</summary>
Motivation: 随着联邦学习系统规模的扩大，通信开销、拜占庭攻击和隐私泄露等问题日益突出，需要一种高效且安全的解决方案。

Method: PRoBit+采用一比特随机量化和最大似然估计进行参数聚合，动态调整参数更新步长，提升深度神经网络在低通信开销和异构数据分布下的训练稳定性。

Result: 理论分析证明了PRoBit+的拜占庭鲁棒性、差分隐私和收敛性上限，实验表明其在拜占庭攻击下优于现有比特传输方案，隐私保护性能损失最小。

Conclusion: PRoBit+在传输精度、安全保证和收敛速率之间实现了平衡，性能随客户端数量增加而提升，适用于异构环境下的联邦学习。

Abstract: As the scale of federated learning (FL) systems expands, their inherent
performance limitations like communication overhead, Byzantine vulnerability,
and privacy leakage have become increasingly critical. This paper considers a
personalized FL framework based on model regularization, and proposes a model
aggregation algorithm named PRoBit+ to concurrently overcome these limitations.
PRoBit+ employs one-bit stochastic quantization and maximum likelihood
estimation for parameter aggregation, and dynamically adjusts the step size of
parameter updates, improving training stability of deep neural networks under
low communication overhead and heterogeneous data distributions. PRoBit+'s
statistical analysis is then conducted and its Byzantine robustness is proved.
The $(\epsilon,0)$-differential privacy and a convergence upper bound of the
PRoBit+ based FL are also theoretically established in heterogeneous contexts.
The analysis illustrates the trade-off among transmission accuracy, security
guarantees, and convergence rates, and also indicates that the performance
degradation caused by transmission errors and privacy protection can be
progressively eliminated at a rate of $\mathcal{O}(1/M)$ as the number of
uploading clients $M$ increases. Comprehensive numerical experiments are
conducted to assess PRoBit+ in comparison to benchmark methods across different
Byzantine attacks and varying proportions of malicious clients. The
experimental results demonstrate that PRoBit+ exhibits improved Byzantine
robustness over existing bit-based transmission schemes, minimal performance
degradation related to privacy protection, and nearly identical performance to
full-precision FedAvg in a secure environment.

</details>


### [15] [Gathering Teams of Bounded Memory Agents on a Line](https://arxiv.org/abs/2507.04172)
*Younan Gao,Andrzej Pelc*

Main category: cs.DC

TL;DR: 研究了多个移动代理在无限直线上的同步聚集问题，代理具有不同标签和团队结构，聚集的可行性和时间取决于团队大小。


<details>
  <summary>Details</summary>
Motivation: 探讨在同步环境下，不同团队结构的移动代理如何在无限直线上高效聚集。

Method: 使用确定性自动机模型，分析代理在不同团队大小和直线方向性（有向/无向）下的聚集策略。

Result: 团队大小和直线方向性影响聚集可行性和时间：有向直线需团队大小>1，时间O(D)；无向直线团队大小≥3时时间Θ(D)，团队大小=2时时间Θ(D log L)。

Conclusion: 团队大小和直线方向性是聚集问题的关键因素，不同情况下存在最优时间复杂度的解决方案。

Abstract: Several mobile agents, modelled as deterministic automata, navigate in an
infinite line in synchronous rounds. All agents start in the same round. In
each round, an agent can move to one of the two neighboring nodes, or stay
idle. Agents have distinct labels which are integers from the set $\{1,\dots,
L\}$. They start in teams, and all agents in a team have the same starting
node. The adversary decides the compositions of teams, and their starting
nodes. Whenever an agent enters a node, it sees the entry port number and the
states of all collocated agents; this information forms the input of the agent
on the basis of which it transits to the next state and decides the current
action. The aim is for all agents to gather at the same node and stop.
Gathering is feasible, if this task can be accomplished for any decisions of
the adversary, and its time is the worst-case number of rounds from the start
till gathering.
  We consider the feasibility and time complexity of gathering teams of agents,
and give a complete solution of this problem. It turns out that both
feasibility and complexity of gathering depend on the sizes of teams. We first
concentrate on the case when all teams have the same size $x$. For the oriented
line, gathering is impossible if $x=1$, and it can be accomplished in time
$O(D)$, for $x>1$, where $D$ is the distance between the starting nodes of the
most distant teams. This complexity is of course optimal. For the unoriented
line, the situation is different. For $x=1$, gathering is also impossible, but
for $x=2$, the optimal time of gathering is $\Theta(D\log L)$, and for $x\geq
3$, the optimal time of gathering is $\Theta(D)$. In the case when there are
teams of different sizes, we show that gathering is always possible in time
$O(D)$, even for the unoriented line. This complexity is of course optimal.

</details>


### [16] [Static Analysis for Detecting Transaction Conflicts in Ethereum Smart Contracts](https://arxiv.org/abs/2507.04357)
*Zareh Chahoki Atefeh,Roveri Marco*

Main category: cs.DC

TL;DR: 提出一种静态分析方法，用于检测以太坊智能合约中潜在的交易冲突，以提高区块链的吞吐量和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的并发执行方法因运行时冲突检测和回滚机制导致高开销，而静态冲突检测方法缺乏全面研究。

Method: 通过分析Solidity合约中的状态变量访问模式，识别交易对之间的读写、写写和函数调用冲突。

Result: 在真实以太坊智能合约数据集上验证，该方法能高精度识别潜在冲突。

Conclusion: 该工具支持设计交易调度策略，减少运行时失败，提升验证者吞吐量，促进区块链可扩展性。

Abstract: Ethereum smart contracts operate in a concurrent environment where multiple
transactions can be submitted simultaneously. However, the Ethereum Virtual
Machine (EVM) enforces sequential execution of transactions within each block
to prevent conflicts arising from concurrent access to the same state
variables. Although this approach guarantees correct behavior, it limits the
ability of validators to leverage multi-core architectures for faster
transaction processing, thus restricting throughput. Existing solutions
introduce concurrency by allowing simultaneous transaction execution combined
with runtime conflict detection and rollback mechanisms to maintain
correctness. However, these methods incur significant overhead due to
continuous conflict tracking and transaction reversion. Recently, alternative
approaches have emerged that aim to predict conflicts statically, before
execution, by analyzing smart contract code for potential transaction
interactions. Despite their promise, there is a lack of comprehensive studies
that examine static conflict detection and its broader implications in specific
smart contracts. This paper fills this important gap by proposing a novel
static analysis method to detect potential transaction conflicts in Ethereum
smart contracts. Our method identifies read-write, write-write, and function
call conflicts between transaction pairs by analyzing state variable access
patterns in Solidity contracts. We implement a tool that parses contract code
and performs conflict detection. Evaluation on a dataset of real-world Ethereum
smart contracts demonstrates that our approach achieves high precision in
identifying potential conflicts. By enabling proactive conflict detection, our
tool supports further design of transaction scheduling strategies that reduce
runtime failures, enhance validator throughput, and contribute to blockchain
scalability.

</details>


### [17] [Skipper: Maximal Matching with a Single Pass over Edges](https://arxiv.org/abs/2507.04420)
*Mohsen Koohi Esfahani*

Main category: cs.DC

TL;DR: Skipper是一种增量异步最大匹配算法，通过单次处理边、跳过大部分边并最小化内存使用，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有并行最大匹配算法需要多次处理边且内存需求高，限制了效率。

Method: Skipper算法确定性单次处理边，跳过大量边，仅需单字节内存每顶点。

Result: 在1610亿边图上，Skipper仅处理1.2%的边，速度提升47.1倍，输出质量达88.6%。

Conclusion: Skipper在性能和内存效率上显著优于现有算法，适用于大规模图处理。

Abstract: Maximal Matching (MM) is a fundamental graph problem with diverse
applications. However, state-of-the-art parallel MM algorithms are limited by
their need to process graph edges repeatedly over multiple iterations.
Furthermore, optimized algorithms often require additional memory for graph
contraction or edge filtering. In this paper, we introduce Skipper, an
incremental asynchronous MM algorithm that (i) processes each edge
deterministically and only once, (ii) skips a large fraction of edges during
processing, and (iii) minimizes memory space utilization. Notably, Skipper
requires (a) a single pass over the edges, and (b) only a single byte of memory
space per vertex. Our evaluation of Skipper, using both real-world and
synthetic graphs with up to 161 billion edges, and across three different
computer architectures, shows that Skipper processes only 1.2% of the edges and
delivers a 47.1 times average speedup (geometric mean). Moreover, Skipper's
output quality is highly competitive, with an average size of 88.6% relative to
the output of the Lim-Chung algorithm as a state-of-the-art MM algorithm with
the largest output size.

</details>


### [18] [Agentic Distributed Computing](https://arxiv.org/abs/2507.04459)
*Ajay D. Kshemkalyani,Manish Kumar,Anisur Rahaman Molla,Gokarna Sharma*

Main category: cs.DC

TL;DR: 本文提出了一种新的分布式计算模型——代理模型，扩展了传统的消息传递模型，研究了领导者选举和最小生成树问题，并提出了优化的确定性算法。


<details>
  <summary>Details</summary>
Motivation: 传统的消息传递模型假设计算设备是静态的，而代理模型引入了移动计算设备（代理），以更灵活的方式处理分布式任务，填补了k≤n情况下的研究空白。

Method: 提出了两种确定性算法分别处理k<n和k=n的领导者选举问题，并基于此设计了最小生成树算法，优化了时间和内存复杂度。

Result: 在代理模型中实现了高效的领导者选举和最小生成树构造，首次解决了k≤n的情况。

Conclusion: 代理模型为分布式计算提供了新的研究方向，特别是在移动计算设备场景下，具有重要的理论和实践意义。

Abstract: The most celebrated and extensively studied model of distributed computing is
the {\em message-passing model,} in which each vertex/node of the (distributed
network) graph corresponds to a static computational device that communicates
with other devices through passing messages. In this paper, we consider the
{\em agentic model} of distributed computing which extends the message-passing
model in a new direction. In the agentic model, computational devices are
modeled as relocatable or mobile computational devices (called agents in this
paper), i.e., each vertex/node of the graph serves as a container for the
devices, and hence communicating with another device requires relocating to the
same node. We study two fundamental graph level tasks, leader election, and
minimum spanning tree, in the agentic model, which will enhance our
understanding of distributed computation across paradigms. The objective is to
minimize both time and memory complexities. Following the literature, we
consider the synchronous setting in which each agent performs its operations
synchronously with others, and hence the time complexity can be measured in
rounds. In this paper, we present two deterministic algorithms for leader
election: one for the case of $k<n$ and another for the case of $k=n$,
minimizing both time and memory complexities, where $k$ and $n$, respectively,
are the number of agents and number of nodes of the graph. Using these leader
election results, we develop deterministic algorithms for agents to construct a
minimum spanning tree of the graph, minimizing both time and memory
complexities. To the best of our knowledge, this is the first study of
distributed graph level tasks in the agentic model with $k\leq n$. Previous
studies only considered the case of $k=n$.

</details>


### [19] [RAPTOR: Practical Numerical Profiling of Scientific Applications](https://arxiv.org/abs/2507.04647)
*Faveo Hoerold,Ivan R. Ivanov,Akash Dhruv,William S. Moses,Anshu Dubey,Mohamed Wahib,Jens Domke*

Main category: cs.DC

TL;DR: RAPTOR是一种数值分析工具，帮助科学家评估代码中降低精度的可行性，支持从FP64到FP16的转换，并通过LLVM实现透明替换或模拟用户定义精度。


<details>
  <summary>Details</summary>
Motivation: 现代高性能架构中低精度单元的普及增加了科学家的负担，传统FP64能力停滞或减少，迫使科学家重新评估代码精度需求。

Method: 利用LLVM透明替换高精度计算为低精度单元或模拟用户定义精度，开发了RAPTOR工具。

Result: 在四个真实多物理Flash-X应用中验证了RAPTOR的功能，展示了其易用性和实用性。

Conclusion: RAPTOR为科学家提供了一种新颖、功能丰富的方法，用于分析和调整数值需求及不稳定性。

Abstract: The proliferation of low-precision units in modern high-performance
architectures increasingly burdens domain scientists. Historically, the choice
in HPC was easy: can we get away with 32 bit floating-point operations and
lower bandwidth requirements, or is FP64 necessary? Driven by Artificial
Intelligence, vendors introduced novel low-precision units for vector and
tensor operations, and FP64 capabilities stagnate or are reduced. This is
forcing scientists to re-evaluate their codes, but a trivial search-and-replace
approach to go from FP64 to FP16 will not suffice. We introduce RAPTOR: a
numerical profiling tool to guide scientists in their search for code regions
where precision lowering is feasible. Using LLVM, we transparently replace
high-precision computations using low-precision units, or emulate a
user-defined precision. RAPTOR is a novel, feature-rich approach -- with focus
on ease of use -- to change, profile, and reason about numerical requirements
and instabilities, which we demonstrate with four real-world multi-physics
Flash-X applications.

</details>


### [20] [Communication Round and Computation Efficient Exclusive Prefix-Sums Algorithms (for MPI_Exscan)](https://arxiv.org/abs/2507.04785)
*Jesper Larsson Träff*

Main category: cs.DC

TL;DR: 论文提出了一种新的简单算法，用于在消息传递系统中计算独占前缀和，优化了通信轮次和操作次数，并与现有方法进行了比较。


<details>
  <summary>Details</summary>
Motivation: 现有独占前缀和算法在通信轮次或操作次数上存在不足，需要更高效的解决方案。

Method: 提出了一种新算法，通过优化通信轮次和操作次数（q=⌈log2(p−1)+log2(4/3)⌉轮次，q−1次操作）来计算独占前缀和。

Result: 新算法在36节点集群上的MPI实现中表现优于现有方法，展示了标准实现的改进潜力。

Conclusion: 新算法在独占前缀和计算中更高效，适用于小输入向量场景，为MPI库提供了优化方向。

Abstract: Parallel scan primitives compute element-wise inclusive or exclusive prefix
sums of input vectors contributed by $p$ consecutively ranked processors under
an associative, binary operator $\oplus$. In message-passing systems with
bounded, one-ported communication capabilities, at least $\lceil\log_2 p\rceil$
or $\lceil\log_2 (p-1)\rceil$ communication rounds are required to perform the
scans. While there are well-known, simple algorithms for the inclusive scan
that solve the problem in $\lceil\log_2 p\rceil$ communication rounds with
$\lceil\log_2 p\rceil$ applications of $\oplus$ (which could be expensive), the
exclusive scan appears more difficult. Conventionally, the problem is solved
with either $\lceil\log_2 (p-1)\rceil+1$ communication rounds (e.g., by
shifting the input vectors), or in $\lceil\log_2 p\rceil$ communication rounds
with $2\lceil\log_2 p\rceil-1$ applications of $\oplus$ (by a modified
inclusive scan algorithm). We give a new, simple algorithm that computes the
exclusive prefix sums in $q=\lceil\log_2 (p-1)+\log_2\frac{4}{3}\rceil$
simultaneous send-receive communication rounds with $q-1$ applications of
$\oplus$. We compare the three algorithms implemented in MPI against the MPI
library native MPI\_Exscan primitive on a small, $36$-node cluster with a
state-of-the-art MPI library, indicating possible and worthwhile improvements
to standard implementations. The algorithms assume input vectors to be small so
that performance is dominated by the number of communication rounds. For large
input vectors, other (pipelined, fixed-degree tree) algorithms must be used.

</details>


### [21] [Demystifying NCCL: An In-depth Analysis of GPU Communication Protocols and Algorithms](https://arxiv.org/abs/2507.04786)
*Zhiyi Hu,Siyuan Shen,Tommaso Bonato,Sylvain Jeaugey,Cedell Alexander,Eric Spada,Jeff Hammond,Torsten Hoefler*

Main category: cs.DC

TL;DR: 本文对NVIDIA Collective Communication Library (NCCL) 进行了全面分析，揭示了其内部通信协议、数据移动机制及集体通信算法，并开发了ATLAHS工具链以模拟大规模AI训练中的通信模式。


<details>
  <summary>Details</summary>
Motivation: NCCL虽然是开源的，但其内部设计不透明，难以分析性能或识别瓶颈，因此需要深入理解其工作机制。

Method: 研究分析了NCCL的通信协议变体（Simple、LL和LL128）、节点内和节点间数据移动机制，以及基于环和树的集体通信算法。

Result: 通过研究开发了ATLAHS工具链，能够准确模拟大规模AI训练中的NCCL通信模式。

Conclusion: 该研究为系统研究人员和性能工程师提供了优化或模拟大规模集体通信的指导。

Abstract: The NVIDIA Collective Communication Library (NCCL) is a critical software
layer enabling high-performance collectives on large-scale GPU clusters.
Despite being open source with a documented API, its internal design remains
largely opaque. The orchestration of communication channels, selection of
protocols, and handling of memory movement across devices and nodes are not
well understood, making it difficult to analyze performance or identify
bottlenecks. This paper presents a comprehensive analysis of NCCL, focusing on
its communication protocol variants (Simple, LL, and LL128), mechanisms
governing intra-node and inter-node data movement, and ring- and tree-based
collective communication algorithms. The insights obtained from this study
serve as the foundation for ATLAHS, an application-trace-driven network
simulation toolchain capable of accurately reproducing NCCL communication
patterns in large-scale AI training workloads. By demystifying NCCL's internal
architecture, this work provides guidance for system researchers and
performance engineers working to optimize or simulate collective communication
at scale.

</details>


### [22] [Distributed Approximation Algorithms for Minimum Dominating Set in Locally Nice Graphs](https://arxiv.org/abs/2507.04960)
*Marthe Bonamy,Cyril Gavoille,Timothé Picavet,Alexandra Wesolek*

Main category: cs.DC

TL;DR: 本文提出了一种新的分布式算法，用于在欧拉亏格为g的曲面上嵌入的图中近似求解最小支配集（MDS），改进并简化了现有方法。


<details>
  <summary>Details</summary>
Motivation: 改进现有MDS算法的近似比和复杂度，特别是在欧拉亏格曲面上的图。

Method: 基于LOCAL模型，利用图的渐近维数和平面图上的MDS算法，无需预先嵌入图。

Result: 近似比从24g+O(1)改进至34+ε，且在可定向曲面上优于91+ε。

Conclusion: 算法适用于任何渐近维数有界的图类，扩展了应用范围。

Abstract: We give a new, short proof that graphs embeddable in a given Euler genus-$g$
surface admit a simple $f(g)$-round $\alpha$-approximation distributed
algorithm for Minimum Dominating Set (MDS), where the approximation ratio
$\alpha \le 906$. Using tricks from Heydt et al. [European Journal of
Combinatorics (2025)], we in fact derive that $\alpha \le 34 +\varepsilon$,
therefore improving upon the current state of the art of $24g+O(1)$ due to
Amiri et al. [ACM Transactions on Algorithms (2019)]. It also improves the
approximation ratio of $91+\varepsilon$ due to Czygrinow et al. [Theoretical
Computer Science (2019)] in the particular case of orientable surfaces.
  All our distributed algorithms work in the deterministic LOCAL model. They do
not require any preliminary embedding of the graph and only rely on two things:
a LOCAL algorithm for MDS on planar graphs with ``uniform'' approximation
guarantees and the knowledge that graphs embeddable in bounded Euler genus
surfaces have asymptotic dimension $2$.
  More generally, our algorithms work in any graph class of bounded asymptotic
dimension where ``most vertices'' are locally in a graph class that admits a
LOCAL algorithm for MDS with uniform approximation guarantees.

</details>


### [23] [Silent Failures in Stateless Systems: Rethinking Anomaly Detection for Serverless Computing](https://arxiv.org/abs/2507.04969)
*Chanh Nguyen,Erik Elmroth,Monowar Bhuyan*

Main category: cs.DC

TL;DR: 论文探讨了无服务器计算中异常检测的挑战，提出了针对动态、短暂和无状态环境的下一代检测框架研究议程。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算虽然提升了开发敏捷性和可扩展性，但其动态和短暂特性使得传统异常检测方法失效，需要新的解决方案。

Method: 系统分析了无服务器环境的独特挑战（如无持久状态、监控粒度不一致等），并研究了多种威胁（如DoS、DoW等），提出了下一代检测框架的研究方向。

Result: 明确了无服务器环境中异常检测的关键挑战和威胁，为下一代框架提供了设计原则和研究方向。

Conclusion: 论文为无服务器生态系统的异常检测奠定了基础，提出了需要实时、轻量级、隐私保护且适应边缘云的研究方向。

Abstract: Serverless computing has redefined cloud application deployment by
abstracting infrastructure and enabling on-demand, event-driven execution,
thereby enhancing developer agility and scalability. However, maintaining
consistent application performance in serverless environments remains a
significant challenge. The dynamic and transient nature of serverless functions
makes it difficult to distinguish between benign and anomalous behavior, which
in turn undermines the effectiveness of traditional anomaly detection methods.
These conventional approaches, designed for stateful and long-running services,
struggle in serverless settings where executions are short-lived, functions are
isolated, and observability is limited.
  In this first comprehensive vision paper on anomaly detection for serverless
systems, we systematically explore the unique challenges posed by this
paradigm, including the absence of persistent state, inconsistent monitoring
granularity, and the difficulty of correlating behaviors across distributed
functions. We further examine a range of threats that manifest as anomalies,
from classical Denial-of-Service (DoS) attacks to serverless-specific threats
such as Denial-of-Wallet (DoW) and cold start amplification. Building on these
observations, we articulate a research agenda for next-generation detection
frameworks that address the need for context-aware, multi-source data fusion,
real-time, lightweight, privacy-preserving, and edge-cloud adaptive
capabilities.
  Through the identification of key research directions and design principles,
we aim to lay the foundation for the next generation of anomaly detection in
cloud-native, serverless ecosystems.

</details>


### [24] [MoLink: Distributed and Efficient Serving Framework for Large Models](https://arxiv.org/abs/2507.05043)
*Lewei Jin,Yongqi Chen,Kui Zhang,Yifan Zhuo,Yi Gao,Bowei Yang,Zhengong Cai,Wei Dong*

Main category: cs.DC

TL;DR: MoLink是一个分布式LLM服务系统，旨在通过消费级GPU实现高效、低成本的大模型服务，解决了网络限制和系统异构性两大挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务成本高昂，消费级GPU提供了低成本替代方案，但面临网络条件和系统异构性的挑战。

Method: 提出MoLink系统，采用多项关键技术，支持在异构和弱连接的消费级GPU上高效运行LLM。

Result: 实验显示，MoLink的吞吐量提升高达458%，成本利润边际提升高达151%。

Conclusion: MoLink为消费级GPU上的LLM服务提供了高效、低成本的解决方案，支持多种主流开源大模型架构。

Abstract: Large language models represent a groundbreaking shift in generative AI. Yet,
these advances come with a significant challenge: the high cost of model
serving. To mitigate these costs, consumer-grade GPUs emerge as a more
affordable alternative. This presents an opportunity for more cost-efficient
LLM serving by leveraging these GPUs.
  However, it is non-trivial to achieve high-efficiency LLM serving on
consumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often
deployed in limited network conditions; 2) these GPUs often exhibit
heterogeneity in host systems. To address these challenges, we present MoLink,
a distributed LLM serving system for large models. It incorporates several key
techniques, enabling efficient LLM serving on heterogeneous and weakly
connected consumer-grade GPUs. Our experiments demonstrate that it achieves
throughput improvements of up to 458\% and cost-profit margin improvements of
up to 151\%, compared to state-of-the-art systems. MoLink allows users on
Windows, Linux, and containerized VMs to seamlessly integrate GPUs with just a
few lines of code over Ethernet or public networks. Currently, it supports 18
mainstream architectures of open-source large language models.

</details>


### [25] [Cooperative Gradient Coding](https://arxiv.org/abs/2507.05230)
*Shudi Weng,Ming Xiao,Chao Ren,Mikael Skoglund*

Main category: cs.DC

TL;DR: 论文提出了一种名为CoGC的梯度编码框架，用于解决分布式训练中的不可靠通信问题，并进一步提出GC$^+$解码机制以提升系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究梯度编码在不可靠通信环境下的分布式训练问题，旨在消除数据集复制的需求，提高通信和计算效率，适用于联邦学习。

Method: 提出CoGC框架，利用客户端间的协作通信，并结合标准GC解码机制；进一步提出GC$^+$解码机制，利用失败解码中的信息提升可靠性。

Result: CoGC在客户端通信良好时表现出强韧性和最优性，但在通信差时效率低；GC$^+$显著提升了系统可靠性，全局模型恢复占主导。

Conclusion: 论文为CoGC和GC$^+$建立了理论框架，分析了中断对GC矩阵的影响，并通过仿真验证了其有效性。

Abstract: This work studies gradient coding (GC) in the context of distributed training
problems with unreliable communication. We propose cooperative GC (CoGC), a
novel gradient-sharing-based GC framework that leverages cooperative
communication among clients. This approach ultimately eliminates the need for
dataset replication, making it both communication- and computation-efficient
and suitable for federated learning (FL). By employing the standard GC decoding
mechanism, CoGC yields strictly binary outcomes: either the global model is
exactly recovered, or the decoding fails entirely, with no intermediate
results. This characteristic ensures the optimality of the training and
demonstrates strong resilience to client-to-server communication failures when
the communication channels among clients are in good condition. However, it may
also result in communication inefficiency and hinder convergence due to its
lack of flexibility, especially when communication channels among clients are
in poor condition. To overcome this limitation and further harness the
potential of GC matrices, we propose a complementary decoding mechanism, termed
GC$^+$, which leverages information that would otherwise be discarded during GC
decoding failures. This approach significantly improves system reliability
under unreliable communication, as the full recovery of the global model
typically dominates in GC$^+$. To conclude, this work establishes solid
theoretical frameworks for both CoGC and GC$^+$. We provide complete outage
analyses for each decoding mechanism, along with a rigorous investigation of
how outages affect the structure and performance of GC matrices. Building on
these analyses, we derive convergence bounds for both decoding mechanisms.
Finally, the effectiveness of CoGC and GC$^+$ is validated through extensive
simulations.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [26] [ForgeHLS: A Large-Scale, Open-Source Dataset for High-Level Synthesis](https://arxiv.org/abs/2507.03255)
*Zedong Peng,Zeju Li,Mingzhe Gao,Qiang Xu,Chen Zhang,Jieru Zhao*

Main category: cs.AR

TL;DR: ForgeEDA是一个开源的综合电路数据集，包含多种电路表示形式，支持EDA算法的基准测试和AI模型训练，旨在推动IC设计和EDA领域的创新。


<details>
  <summary>Details</summary>
Motivation: 现有数据集存在局限性，ForgeEDA旨在填补这一空白，提供多样化的电路表示形式，以支持全面的分析和开发。

Method: ForgeEDA包含RTL代码、PM网表、AIG和布局网表等多种电路表示形式，并通过EDA算法的PPA优化任务展示其实用性。

Result: ForgeEDA能够揭示性能差距，推动EDA算法的进步，并提升AI模型在EDA任务中的性能和泛化能力。

Conclusion: ForgeEDA通过其规模和多样性，有望推动现代IC设计的突破，并支持EDA领域的下一代创新。

Abstract: We introduce ForgeEDA, an open-source comprehensive circuit dataset across
various categories. ForgeEDA includes diverse circuit representations such as
Register Transfer Level (RTL) code, Post-mapping (PM) netlists, And-Inverter
Graphs (AIGs), and placed netlists, enabling comprehensive analysis and
development. We demonstrate ForgeEDA's utility by benchmarking state-of-the-art
EDA algorithms on critical tasks such as Power, Performance, and Area (PPA)
optimization, highlighting its ability to expose performance gaps and drive
advancements. Additionally, ForgeEDA's scale and diversity facilitate the
training of AI models for EDA tasks, demonstrating its potential to improve
model performance and generalization. By addressing limitations in existing
datasets, ForgeEDA aims to catalyze breakthroughs in modern IC design and
support the next generation of innovations in EDA.

</details>


### [27] [Hummingbird: A Smaller and Faster Large Language Model Accelerator on Embedded FPGA](https://arxiv.org/abs/2507.03308)
*Jindong Li,Tenglong Li,Ruiqi Chen,Guobin Shen,Dongcheng Zhao,Qian Zhang,Yi Zeng*

Main category: cs.AR

TL;DR: Hummingbird是一种专为嵌入式FPGA设计的LLM推理加速器，具有更小、更强、更快的特点，显著提升了性能和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs的高计算和内存需求与嵌入式设备资源有限的矛盾，现有FPGA部署方案多依赖昂贵硬件且仅适用于小型LLM，限制了实际应用。

Method: 提出Hummingbird，针对嵌入式FPGA优化，通过卸载策略突破内存限制，并提升资源利用率和推理速度。

Result: 在KV260和ZCU104上，Hummingbird分别实现4.8和8.6 tokens/s的速度，资源节省显著，且支持更大模型LLaMA3-8B。

Conclusion: Hummingbird为边缘计算提供了经济高效的LLM解决方案，展示了工业应用的潜力。

Abstract: Deploying large language models (LLMs) on embedded devices remains a
significant research challenge due to the high computational and memory demands
of LLMs and the limited hardware resources available in such environments.
While embedded FPGAs have demonstrated performance and energy efficiency in
traditional deep neural networks, their potential for LLM inference remains
largely unexplored. Recent efforts to deploy LLMs on FPGAs have primarily
relied on large, expensive cloud-grade hardware and have only shown promising
results on relatively small LLMs, limiting their real-world applicability. In
this work, we present Hummingbird, a novel FPGA accelerator designed
specifically for LLM inference on embedded FPGAs. Hummingbird is smaller,
targeting embedded FPGAs such as the KV260 and ZCU104 with 67% LUT, 39% DSP,
and 42% power savings over existing research. Hummingbird is stronger,
targeting LLaMA3-8B and supporting longer contexts, overcoming the typical 4GB
memory constraint of embedded FPGAs through offloading strategies. Finally,
Hummingbird is faste, achieving 4.8 tokens/s and 8.6 tokens/s for LLaMA3-8B on
the KV260 and ZCU104 respectively, with 93-94% model bandwidth utilization,
outperforming the prior 4.9 token/s for LLaMA2-7B with 84% bandwidth
utilization baseline. We further demonstrate the viability of industrial
applications by deploying Hummingbird on a cost-optimized Spartan UltraScale
FPGA, paving the way for affordable LLM solutions at the edge.

</details>


### [28] [A Flexible Instruction Set Architecture for Efficient GEMMs](https://arxiv.org/abs/2507.03522)
*Alexandre de Limas Santana,Adrià Armejach,Francesc Martinez,Erich Focht,Marc Casas*

Main category: cs.AR

TL;DR: 论文提出了一种名为MTE的新型矩阵指令集架构（ISA），解决了现有矩阵ISA在运行GEMM工作负载时的性能不足问题，尤其是在处理卷积和Transformer模型时。


<details>
  <summary>Details</summary>
Motivation: 现有矩阵ISA虽然比SIMD/向量ISA在运行GEMM时吞吐量更大，但缺乏灵活性，无法动态适应数据格式等应用特定需求，导致性能不佳。

Method: 提出了MTE，一种完全解耦指令集架构与微架构的矩阵ISA，仅需少量额外指令和一个64位控制状态寄存器（CSR）。MTE支持三维向量化GEMM、利用现有向量寄存器文件，并解耦瓦片形状与微架构。

Result: MTE比现有最佳矩阵ISA性能提升1.35倍。

Conclusion: MTE是一种高效且灵活的矩阵ISA，显著提升了GEMM工作负载的性能，尤其适用于卷积和Transformer模型。

Abstract: GEneral Matrix Multiplications (GEMMs) are recurrent in high-performance
computing and deep learning workloads. Typically, high-end CPUs accelerate GEMM
workloads with Single-Instruction Multiple Data (SIMD) or vector Instruction
Set Architectures (ISAs). Since these ISAs face significant issues when running
GEMM workloads, particularly when dealing with small, tall, or skinny matrices,
matrix ISAs have been proposed and implemented by major hardware vendors in the
last years. Although these matrix ISAs deliver larger throughput when running
GEMMs than their SIMD/vector counterparts, they are rigid solutions unable to
dynamically adapt themselves to application-specific aspects like the data
format. This paper demonstrates that the state-of-the-art matrix ISAs deliver
suboptimal performance when running the most commonly used convolution and
transformer models.
  This paper proposes the Matrix Tile Extension (MTE), the first matrix ISA
that completely decouples the instruction set architecture from the
microarchitecture and seamlessly interacts with existing vector ISAs. MTE
incurs minimal implementation overhead since it only requires a few additional
instructions and a 64-bit Control Status Register (CSR) to keep its state.
Specifically, MTE can i) vectorize GEMMs across the three dimensions M, N, and
K; ii) leverage the capacity of the existing vector register file; and iii)
decouple the tile shape from the underlying microarchitecture. MTE achieves
speed-ups of 1.35x over the best state-of-the-art matrix ISA.

</details>


### [29] [FIXME: Towards End-to-End Benchmarking of LLM-Aided Design Verification](https://arxiv.org/abs/2507.04276)
*Gwok-Waa Wan,Shengchu Su,Ruihu Wang,Qixiang Chen,Sam-Zaak Wong,Mengnv Xing,Hefei Feng,Yubo Wang,Yinan Zhu,Jingyi Zhang,Jianmin Ye,Xinlai Wan,Tao Ni,Qiang Xu,Nan Guan,Zhe Jiang,Xi Wang,Yang Jun*

Main category: cs.AR

TL;DR: FIXME是一个开源的多模型评估框架，用于评估大型语言模型（LLM）在硬件功能验证（FV）中的性能，填补了当前研究的空白。


<details>
  <summary>Details</summary>
Motivation: 硬件功能验证是现代设计方法中的主要瓶颈，但当前研究主要集中在RTL生成和基本调试上，缺乏对LLM在该领域能力的全面评估。

Method: FIXME通过三级难度层次结构、六个验证子领域和180个多样化任务，结合AI与人类协作的方法，构建了一个高质量的数据集，并提升了功能覆盖率45.57%。

Result: 通过评估GPT-4、Claude3和LlaMA3等先进LLM，识别了改进的关键领域，并提出了未来研究方向。

Conclusion: FIXME为LLM在硬件设计验证中的潜力提供了深入分析，并开源了基准测试，推动了该领域的进一步发展。

Abstract: Despite the transformative potential of Large Language Models (LLMs) in
hardware design, a comprehensive evaluation of their capabilities in design
verification remains underexplored. Current efforts predominantly focus on RTL
generation and basic debugging, overlooking the critical domain of functional
verification, which is the primary bottleneck in modern design methodologies
due to the rapid escalation of hardware complexity. We present FIXME, the first
end-to-end, multi-model, and open-source evaluation framework for assessing LLM
performance in hardware functional verification (FV) to address this crucial
gap. FIXME introduces a structured three-level difficulty hierarchy spanning
six verification sub-domains and 180 diverse tasks, enabling in-depth analysis
across the design lifecycle. Leveraging a collaborative AI-human approach, we
construct a high-quality dataset using 100% silicon-proven designs, ensuring
comprehensive coverage of real-world challenges. Furthermore, we enhance the
functional coverage by 45.57% through expert-guided optimization. By rigorously
evaluating state-of-the-art LLMs such as GPT-4, Claude3, and LlaMA3, we
identify key areas for improvement and outline promising research directions to
unlock the full potential of LLM-driven automation in hardware design
verification. The benchmark is available at
https://github.com/ChatDesignVerification/FIXME.

</details>


### [30] [HLStrans: Dataset for LLM-Driven C-to-HLS Hardware Code Synthesis](https://arxiv.org/abs/2507.04315)
*Qingyun Zou,Nuo Chen,Yao Chen,Bingsheng He,WengFei Wong*

Main category: cs.AR

TL;DR: 论文介绍了HLStrans数据集，用于填补现有开源数据集在复杂性和优化多样性上的不足，以支持LLMs生成高性能HLS代码的研究。


<details>
  <summary>Details</summary>
Motivation: 现有开源数据集缺乏足够的复杂性和优化多样性，限制了LLMs在自动生成高性能HLS代码中的应用。

Method: 通过构建HLStrans数据集，包含137个真实程序及其多种C-to-HLS转换，生成23K标记设计变体，涵盖广泛的优化和pragma。

Result: 数据集为LLMs提供了丰富的优化案例，支持其生成可合成的高性能HLS代码。

Conclusion: HLStrans数据集填补了研究空白，未来将扩展规模和程序多样性，推动AI与硬件合成交叉领域的研究。

Abstract: High-level synthesis (HLS) enables software developers to describe and
implement hardware at a higher level of abstraction by using C/C++ instead of
traditional hardware description languages to automatically generate FPGA-ready
designs. However, generating HLS code significantly differs from standard
C/C++: it disallows certain coding idioms, relies on specialized libraries, and
critically requires fine-grained transformations and the insertion of
optimization directives (pragmas) to achieve high performance. Large language
models (LLMs) have shown promise in automating such transformations, yet
existing open-source datasets lack sufficient complexity and optimization
diversity. To address this gap, we introduce the HLStrans dataset, a
comprehensive collection of 137 distinct real word programs, each annotated
with a variety of C-to-HLS transformations that yield over 23K labeled design
variants. These include a broad spectrum of pragmas and code-level
optimizations. We benchmark state-of-the-art LLMs on this dataset to evaluate
their ability to generate synthesizable, high-performance HLS code. As part of
an ongoing effort, we plan to expand the HLStrans dataset in both scale and
program variety, further empowering research at the intersection of AI and
hardware synthesis.

</details>


### [31] [da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs](https://arxiv.org/abs/2507.04535)
*Chang Sun,Zhiqiang Que,Vladimir Loncar,Wayne Luk,Maria Spiropulu*

Main category: cs.AR

TL;DR: 提出了一种基于分布式算术（DA）的高效算法，用于在FPGA上实现恒定矩阵向量乘法（CMVM），优化了面积利用和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决在FPGA上部署神经网络时面积利用率低和延迟高的问题，特别是在CERN大型强子对撞机等微秒级延迟要求的场景。

Method: 采用分布式算术（DA）实现CMVM，优化算法以减少资源占用和计算时间。

Result: 算法在保持与现有技术相似的资源减少的同时，显著提高了计算速度，资源占用减少高达三分之一。

Conclusion: 该算法成功降低了FPGA上的资源需求和延迟，使之前不可行的网络部署成为可能，并已集成到开源库hls4ml中。

Abstract: Neural networks with a latency requirement on the order of microseconds, like
the ones used at the CERN Large Hadron Collider, are typically deployed on
FPGAs fully unrolled and pipelined. A bottleneck for the deployment of such
neural networks is area utilization, which is directly related to the required
constant matrix-vector multiplication (CMVM) operations. In this work, we
propose an efficient algorithm for implementing CMVM operations with
distributed arithmetic (DA) on FPGAs that simultaneously optimizes for area
consumption and latency. The algorithm achieves resource reduction similar to
state-of-the-art algorithms while being significantly faster to compute. The
proposed algorithm is open-sourced and integrated into the \texttt{hls4ml}
library, a free and open-source library for running real-time neural network
inference on FPGAs. We show that the proposed algorithm can reduce on-chip
resources by up to a third for realistic, highly quantized neural networks
while simultaneously reducing latency, enabling the implementation of
previously infeasible networks.

</details>


### [32] [NeuroPDE: A Neuromorphic PDE Solver Based on Spintronic and Ferroelectric Devices](https://arxiv.org/abs/2507.04677)
*Siqing Fu,Lizhou Wu,Tiejun Li,Chunyuan Zhang,Sheng Ma,Jianmin Zhang,Yuhan Tang,Jixuan Tang*

Main category: cs.AR

TL;DR: NeuroPDE是一种基于新兴自旋电子和铁电器件的神经形态PDE求解器硬件设计，通过利用物理随机性实现高效求解。


<details>
  <summary>Details</summary>
Motivation: 传统冯·诺依曼架构缺乏硬件固有随机性，限制了PDE求解器的性能。

Method: 采用自旋神经元模拟随机游走，铁电突触非易失性存储连续权重。

Result: 求解扩散方程时方差小于1e-2，性能提升3.48x至315x，能耗优势2.7x至29.8x。

Conclusion: 研究为未来概率神经形态计算系统开辟了新途径。

Abstract: In recent years, new methods for solving partial differential equations
(PDEs) such as Monte Carlo random walk methods have gained considerable
attention. However, due to the lack of hardware-intrinsic randomness in the
conventional von Neumann architecture, the performance of PDE solvers is
limited. In this paper, we introduce NeuroPDE, a hardware design for
neuromorphic PDE solvers that utilizes emerging spintronic and ferroelectric
devices. NeuroPDE incorporates spin neurons that are capable of probabilistic
transmission to emulate random walks, along with ferroelectric synapses that
store continuous weights non-volatilely. The proposed NeuroPDE achieves a
variance of less than 1e-2 compared to analytical solutions when solving
diffusion equations, demonstrating a performance advantage of 3.48x to 315x
speedup in execution time and an energy consumption advantage of 2.7x to 29.8x
over advanced CMOS-based neuromorphic chips. By leveraging the inherent
physical stochasticity of emerging devices, this study paves the way for future
probabilistic neuromorphic computing systems.

</details>


### [33] [Jack Unit: An Area- and Energy-Efficient Multiply-Accumulate (MAC) Unit Supporting Diverse Data Formats](https://arxiv.org/abs/2507.04772)
*Seock-Hwan Noh,Sungju Kim,Seohyun Kim,Daehoon Kim,Jaeha Kung,Yeseong Kim*

Main category: cs.AR

TL;DR: 提出了一种名为Jack单元的多功能乘加单元，支持多种数据格式，通过优化硬件设计显著提升了面积和能效。


<details>
  <summary>Details</summary>
Motivation: 现有MAC单元在处理多种数据格式时效率不足，需要一种更灵活且高效的解决方案。

Method: 采用精度可调的CSM、基于指数差的调整方法以及2D子字并行技术。

Result: Jack单元面积缩小1.17~2.01倍，功耗降低1.05~1.84倍；AI加速器能效提升1.32~5.41倍。

Conclusion: Jack单元在多种数据格式下均表现出卓越的硬件效率和能效优势。

Abstract: In this work, we introduce an area- and energy-efficient multiply-accumulate
(MAC) unit, named Jack unit, that is a jack-of-all-trades, supporting various
data formats such as integer (INT), floating point (FP), and microscaling data
format (MX). It provides bit-level flexibility and enhances hardware efficiency
by i) replacing the carry-save multiplier (CSM) in the FP multiplier with a
precision-scalable CSM, ii) performing the adjustment of significands based on
the exponent differences within the CSM, and iii) utilizing 2D sub-word
parallelism. To assess effectiveness, we implemented the layout of the Jack
unit and three baseline MAC units. Additionally, we designed an AI accelerator
equipped with our Jack units to compare with a state-of-the-art AI accelerator
supporting various data formats. The proposed MAC unit occupies 1.17~2.01x
smaller area and consumes 1.05~1.84x lower power compared to the baseline MAC
units. On five AI benchmarks, the accelerator designed with our Jack units
improves energy efficiency by 1.32~5.41x over the baseline across various data
formats.

</details>


### [34] [Optimizing Scalable Multi-Cluster Architectures for Next-Generation Wireless Sensing and Communication](https://arxiv.org/abs/2507.05012)
*Samuel Riedel,Yichao Zhang,Marco Bertuletti,Luca Benini*

Main category: cs.AR

TL;DR: 论文分析了多集群架构中不同集群规模对同步、数据移动和可编程性的影响，提出了一种新型双缓冲屏障技术，并验证了大规模集群在性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 下一代无线技术需要高度并行的架构处理海量数据，但多集群架构中集群规模的选择缺乏优化指导。

Method: 扩展开源共享内存集群MemPool为多集群架构，并提出双缓冲屏障技术以解耦处理器和DMA。

Result: 256核单集群比16个16核集群在内存密集型任务中快2倍，计算密集型任务中快24%。

Conclusion: 大规模集群能显著减少同步和通信开销，提升性能。

Abstract: Next-generation wireless technologies (for immersive-massive communication,
joint communication and sensing) demand highly parallel architectures for
massive data processing. A common architectural template scales up by grouping
tens to hundreds of cores into shared-memory clusters, which are then scaled
out as multi-cluster manycore systems. This hierarchical design, used in GPUs
and accelerators, requires a balancing act between fewer large clusters and
more smaller clusters, affecting design complexity, synchronization,
communication efficiency, and programmability. While all multi-cluster
architectures must balance these trade-offs, there is limited insight into
optimal cluster sizes. This paper analyzes various cluster configurations,
focusing on synchronization, data movement overhead, and programmability for
typical wireless sensing and communication workloads. We extend the open-source
shared-memory cluster MemPool into a multi-cluster architecture and propose a
novel double-buffering barrier that decouples processor and DMA. Our results
show a single 256-core cluster can be twice as fast as 16 16-core clusters for
memory-bound kernels and up to 24% faster for compute-bound kernels due to
reduced synchronization and communication overheads.

</details>


### [35] [ViPSN 2.0: A Reconfigurable Battery-free IoT Platform for Vibration Energy Harvesting](https://arxiv.org/abs/2507.05081)
*Xin Li,Mianxin Xiao,Xi Shen,Jiaqing Chu,Weifeng Huang,Jiashun Li,Yaoyi Li,Mingjing Cai,Jiaming Chen,Xinming Zhang,Daxing Zhang,Congsi Wang,Hong Tang,Bao Zhao,Qitao Lu,Yilong Wang,Jianjun Wang,Minyi Xu,Shitong Fang,Xuanyu Huang. Chaoyang Zhao,Zicheng Liu,Yaowen Yang,Guobiao Hu,Junrui Liang,Wei-Hsin Liao*

Main category: cs.AR

TL;DR: ViPSN2.0是一个模块化、可重构的物联网平台，支持多种振动能量收集器，并通过标准化热插拔接口适应不同应用需求，有效管理能量波动。


<details>
  <summary>Details</summary>
Motivation: 环境振动的不稳定性限制了振动能量收集的效率，导致能量供应不稳定和适应性差。

Method: 提出ViPSN2.0平台，支持多种能量收集器，并采用能量指示电源管理框架，适应不同应用任务。

Result: 通过三个应用验证了平台的灵活性和鲁棒性，包括低功耗信标传输、高功率远程通信和间歇性图像捕获。

Conclusion: ViPSN2.0能在能量受限条件下可靠满足无电池物联网部署的多样化需求。

Abstract: Vibration energy harvesting is a promising solution for powering battery-free
IoT systems; however, the instability of ambient vibrations presents
significant challenges, such as limited harvested energy, intermittent power
supply, and poor adaptability to various applications. To address these
challenges, this paper proposes ViPSN2.0, a modular and reconfigurable IoT
platform that supports multiple vibration energy harvesters (piezoelectric,
electromagnetic, and triboelectric) and accommodates sensing tasks with varying
application requirements through standardized hot-swappable interfaces.
ViPSN~2.0 incorporates an energy-indication power management framework tailored
to various application demands, including light-duty discrete sampling,
heavy-duty high-power sensing, and complex-duty streaming tasks, thereby
effectively managing fluctuating energy availability. The platform's
versatility and robustness are validated through three representative
applications: ViPSN-Beacon, enabling ultra-low-power wireless beacon
transmission from a single transient fingertip press; ViPSN-LoRa, supporting
high-power, long-range wireless communication powered by wave vibrations in
actual marine environments; and ViPSN-Cam, enabling intermittent image capture
and wireless transfer. Experimental results demonstrate that ViPSN~2.0 can
reliably meet a wide range of requirements in practical battery-free IoT
deployments under energy-constrained conditions.

</details>
