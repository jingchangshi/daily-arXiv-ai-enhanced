<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Soteria: Efficient Symbolic Execution as a Functional Library](https://arxiv.org/abs/2511.08729)
*Sacha-Élie Ayoun,Opale Sjöstedt,Azalea Raad*

Main category: cs.PL

TL;DR: Soteria是一个轻量级的OCaml库，用于直接为源语言构建符号执行引擎，避免了中间语言的性能、准确性和语言特性支持方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的符号执行工具依赖中间语言来支持多种编程语言，但这在性能、准确性和语言特性支持方面引入了权衡。作者认为直接为每种源语言构建符号执行引擎更简单有效。

Method: 开发了Soteria库，采用函数式编程风格构建符号执行引擎，直接操作源语言语义，提供可配置性、组合推理和易于实现的特性。

Result: 基于Soteria开发了Soteria^Rust（首个支持Rust Tree Borrows别名模型的符号执行引擎）和Soteria^C（C语言的组合符号执行引擎），在性能和bug检测数量上优于或媲美Kani、Pulse、CBMC和Gillian-C等最先进工具。

Conclusion: 证明了无需中间语言的妥协，也能实现声音、高效、准确和表达力强的符号执行，并形式化了Soteria的理论基础并证明了其声音性。

Abstract: Symbolic execution (SE) tools often rely on intermediate languages (ILs) to support multiple programming languages, promising reusability and efficiency. In practice, this approach introduces trade-offs between performance, accuracy, and language feature support. We argue that building SE engines \emph{directly} for each source language is both simpler and more effective. We present Soteria, a lightweight OCaml library for writing SE engines in a functional style, without compromising on performance, accuracy or feature support. Soteria enables developers to construct SE engines that operate directly over source-language semantics, offering \emph{configurability}, compositional reasoning, and ease of implementation. Using Soteria, we develop Soteria$^{\text{Rust}}$, the \emph{first} Rust SE engine supporting Tree Borrows (the intricate aliasing model of Rust), and Soteria$^{\text{C}}$, a compositional SE engine for C. Both tools are competitive with or outperform state-of-the-art tools such as Kani, Pulse, CBMC and Gillian-C in performance and the number of bugs detected. We formalise the theoretical foundations of Soteria and prove its soundness, demonstrating that sound, efficient, accurate, and expressive SE can be achieved without the compromises of ILs.

</details>


### [2] [Galois Slicing as Automatic Differentiation](https://arxiv.org/abs/2511.09203)
*Robert Atkey,Roly Perera*

Main category: cs.PL

TL;DR: 本文通过CHAD自动微分方法重新构建Galois切片，建立其与可微分编程的类比关系，并扩展了定量区间分析的应用。


<details>
  <summary>Details</summary>
Motivation: 探索Galois切片与可微分编程之间的类比关系，将前向和后向切片视为一种自动微分过程，以澄清现有方法中的隐式选择。

Method: 使用Vákár等人提出的CHAD自动微分方法，通过范畴语义重新构建Galois切片，并扩展到定量区间分析。

Result: 成功建立了Galois切片与自动微分的理论联系，澄清了现有方法中的选择，并实现了对定量区间分析的扩展。

Conclusion: Galois切片可以有效地通过自动微分框架重新构建，这种视角不仅澄清了现有方法，还启发了新的应用扩展。

Abstract: Galois slicing is a technique for program slicing for provenance, developed by Perera and collaborators. Galois slicing aims to explain program executions by demonstrating how to track approximations of the input and output forwards and backwards along a particular execution. In this paper, we explore an analogy between Galois slicing and differentiable programming, seeing the implementation of forwards and backwards slicing as a kind of automatic differentiation. Using the CHAD approach to automatic differentiation due to Vákár and collaborators, we reformulate Galois slicing via a categorical semantics. In doing so, we are able to explore extensions of the Galois slicing idea to quantitative interval analysis, and to clarify the implicit choices made in existing instantiations of this approach.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [An MLIR pipeline for offloading Fortran to FPGAs via OpenMP](https://arxiv.org/abs/2511.08713)
*Gabriel Rodriguez-Canal,David Katz,Nick Brown*

Main category: cs.DC

TL;DR: 在MLIR中通过OpenMP目标指令实现选择性代码卸载到FPGA的首个实现，结合OpenMP方言和HLS方言提供可移植的FPGA编译流程。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律放缓，FPGA等异构计算平台在加速HPC工作负载方面受到关注，需要更灵活的FPGA加速方法。

Method: 将MLIR OpenMP方言与高级综合(HLS)方言结合，支持任何MLIR兼容前端(如Flang)，利用现有MLIR构建块减少开发工作量。

Result: 实现了基于指令的FPGA加速，支持通过标准OpenMP指令手动优化卸载内核，展示了MLIR生态系统的可组合性优势。

Conclusion: 建立了一个灵活可扩展的路径，将基于指令的FPGA加速集成到MLIR生态系统中。

Abstract: With the slowing of Moore's Law, heterogeneous computing platforms such as Field Programmable Gate Arrays (FPGAs) have gained increasing interest for accelerating HPC workloads. In this work we present, to the best of our knowledge, the first implementation of selective code offloading to FPGAs via the OpenMP target directive within MLIR. Our approach combines the MLIR OpenMP dialect with a High-Level Synthesis (HLS) dialect to provide a portable compilation flow targeting FPGAs. Unlike prior OpenMP FPGA efforts that rely on custom compilers, by contrast we integrate with MLIR and so support any MLIR-compatible front end, demonstrated here with Flang. Building upon a range of existing MLIR building blocks significantly reduces the effort required and demonstrates the composability benefits of the MLIR ecosystem. Our approach supports manual optimisation of offloaded kernels through standard OpenMP directives, and this work establishes a flexible and extensible path for directive-based FPGA acceleration integrated within the MLIR ecosystem.

</details>


### [4] [Distribution and Management of Datacenter Load Decoupling](https://arxiv.org/abs/2511.08936)
*Liuzixuan Lin,Andrew A. Chien*

Main category: cs.DC

TL;DR: 数据中心通过能源资源解耦功率容量与电网负载，实现灵活性以降低碳排放，优化分布和管理策略可显著提升可再生能源吸收和经济效益。


<details>
  <summary>Details</summary>
Motivation: AI和云数据中心能耗爆炸式增长加剧了碳足迹问题，数据中心恒定电力需求与波动性可再生能源的矛盾需要通过负载适应性来解决。

Method: 定义并计算数据中心负载解耦的功率和能量需求，评估优化分布和管理方法，包括站点差异和电网合作策略。

Result: 优化分布可实现98%的潜在电网碳减排，仅需70%的总解耦需求；电网合作管理使碳减排效果提升1.4倍；经济上平均收益大于本地成本。

Conclusion: 数据中心负载解耦是降低碳排放的有效方法，但站点间差异可能需要电网干预以确保经济可行性。

Abstract: The exploding power consumption of AI and cloud datacenters (DCs) intensifies the long-standing concerns about their carbon footprint, especially because DCs' need for constant power clashes with volatile renewable generation needed for grid decarbonization. DC flexibility (a.k.a. load adaptation) is a key to reducing DC carbon emissions by improving grid renewable absorption.
  DC flexibility can be created, without disturbing datacenter capacity by decoupling a datacenter's power capacity and grid load with a collection of energy resources. Because decoupling can be costly, we study how to best distribute and manage decoupling to maximize benefits for all. Key considerations include site variation and datacenter-grid cooperation.
  We first define and compute the power and energy needs of datacenter load decoupling, and then we evaluate designed distribution and management approaches. Evaluation shows that optimized distribution can deliver >98% of the potential grid carbon reduction with 70% of the total decoupling need. For management, DC-grid cooperation (2-way sharing and control vs. 1-way info sharing) enables 1.4x grid carbon reduction. Finally, we show that decoupling may be economically viable, as on average datacenters can get power cost and carbon emissions benefits greater than their local costs of decoupling. However, skew across sites suggests grid intervention may be required.

</details>


### [5] [Evaluating HPC-Style CPU Performance and Cost in Virtualized Cloud Infrastructures](https://arxiv.org/abs/2511.08948)
*Jay Tharwani,Shobhit Aggarwal,Arnab A Purkayastha*

Main category: cs.DC

TL;DR: 本文评估了四种主要云提供商（AWS、Azure、GCP、OCI）在虚拟化云基础设施中的HPC风格CPU性能和成本，使用SPEC ACCEL套件中的OpenMP工作负载，比较了Intel、AMD和ARM实例类型在按需和折扣定价下的表现。


<details>
  <summary>Details</summary>
Motivation: 研究云环境中不同CPU架构实例的性能和成本差异，为HPC工作负载选择最优云实例提供指导。

Method: 使用SPEC ACCEL套件中的OpenMP工作负载，在AWS、Azure、GCP和OCI四种云平台上测试Intel、AMD和ARM实例类型，比较按需和一年折扣定价下的性能和成本。

Result: AWS在所有三种实例类型中运行时间最短但收费最高；OCI是最经济的选择但运行较慢；Azure表现中等；GCP从Intel切换到AMD时性能显著提升，但其ARM实例比自身AMD实例慢两倍且更贵；AWS的ARM实例比其Intel和AMD实例快达49%。

Conclusion: 实例选择和云提供商选择会显著影响运行时间和成本，应根据工作负载优先级（原始速度或成本最小化）来指导实例类型决策。

Abstract: This paper evaluates HPC-style CPU performance and cost in virtualized cloud infrastructures using a subset of OpenMP workloads in the SPEC ACCEL suite. Four major cloud providers by market share AWS, Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI) are compared across Intel, AMD, and ARM general purpose instance types under both on-demand and one-year discounted pricing. AWS consistently delivers the shortest runtime in all three instance types, yet charges a premium, especially for on-demand usage. OCI emerges as the most economical option across all CPU families, although it generally runs workloads more slowly than AWS. Azure often exhibits mid-range performance and cost, while GCP presents a mixed profile: it sees a notable boost when moving from Intel to AMD. On the other hand, its ARM instance is more than twice as slow as its own AMD offering and remains significantly more expensive. AWS's internal comparisons reveal that its ARM instance can outperform its Intel and AMD siblings by up to 49 percent in runtime. These findings highlight how instance choices and provider selection can yield substantial variations in both runtime and price, indicating that workload priorities, whether raw speed or cost minimization, should guide decisions on instance types.

</details>


### [6] [SPADA: A Spatial Dataflow Architecture Programming Language](https://arxiv.org/abs/2511.09447)
*Lukas Gianinazzi,Tal Ben-Nun,Torsten Hoefler*

Main category: cs.DC

TL;DR: SPADA是一种面向空间数据流架构的编程语言，通过抽象底层架构细节，提供对数据放置、数据流模式和异步操作的精确控制，显著简化编程复杂度。


<details>
  <summary>Details</summary>
Motivation: 空间数据流架构（如Cerebras晶圆级引擎）在AI和科学计算中表现出色，但编程困难，需要显式协调数据移动和异步计算。现有FPGA和CGRA编程模型未能充分利用其独特能力。

Method: 提出SPADA编程语言，建立严格的数据流语义框架，定义路由正确性、数据竞争和死锁，并设计针对Cerebras CSL的多级编译器。

Result: SPADA可将复杂并行模式（如流水线归约和多维模板）的代码量减少6-8倍，在三个数量级上实现接近理想的弱扩展性能。

Conclusion: SPADA通过统一空间数据流架构的编程模型，推进了这些新兴高性能计算平台的理论基础和实践可用性。

Abstract: Spatial dataflow architectures like the Cerebras Wafer-Scale Engine achieve exceptional performance in AI and scientific applications by leveraging distributed memory across processing elements (PEs) and localized computation. However, programming these architectures remains challenging due to the need for explicit orchestration of data movement through reconfigurable networks-on-chip and asynchronous computation triggered by data arrival. Existing FPGA and CGRA programming models emphasize loop scheduling but overlook the unique capabilities of spatial dataflow architectures, particularly efficient dataflow over regular grids and intricate routing management.
  We present SPADA, a programming language that provides precise control over data placement, dataflow patterns, and asynchronous operations while abstracting architecture-specific low-level details. We introduce a rigorous dataflow semantics framework for SPADA that defines routing correctness, data races, and deadlocks. Additionally, we design and implement a compiler targeting Cerebras CSL with multi-level lowering.
  SPADA serves as both a high-level programming interface and an intermediate representation for domain-specific languages (DSLs), which we demonstrate with the GT4Py stencil DSL. SPADA enables developers to express complex parallel patterns -- including pipelined reductions and multi-dimensional stencils -- in 6--8x less code than CSL with near-ideal weak scaling across three orders of magnitude. By unifying programming for spatial dataflow architectures under a single model, SPADA advances both the theoretical foundations and practical usability of these emerging high-performance computing platforms.

</details>


### [7] [Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science](https://arxiv.org/abs/2511.08998)
*Zilinghan Li,Aditya Sinha,Yijiang Li,Kyle Chard,Kibaek Kim,Ravi Madduri*

Main category: cs.DC

TL;DR: 提出了企业级隐私保护联邦学习框架APPFL的设计愿景，旨在解决从本地原型到分布式部署的规模化挑战，支持跨异构计算环境的无缝扩展。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能在不共享数据的情况下实现协作训练，但构建既用户友好又具备企业级可扩展性和隐私保护能力的框架仍然困难，特别是在连接本地原型和异构客户端部署方面存在挑战。

Method: 基于APPFL框架开发经验，提出企业级隐私保护FL框架的关键能力：可扩展本地仿真、无缝部署过渡、跨异构基础设施部署、多级抽象、综合隐私安全技术（差分隐私、安全聚合、认证、机密计算）。

Result: 提出了实现企业级联邦学习框架的架构设计，能够弥合研究原型与企业级部署之间的差距。

Conclusion: 该框架旨在实现可扩展、可靠且隐私保护的科学AI，为联邦学习从研究走向企业应用提供完整解决方案。

Abstract: Federated learning (FL) is a promising approach to enabling collaborative model training without centralized data sharing, a crucial requirement in scientific domains where data privacy, ownership, and compliance constraints are critical. However, building user-friendly enterprise-level FL frameworks that are both scalable and privacy-preserving remains challenging, especially when bridging the gap between local prototyping and distributed deployment across heterogeneous client computing infrastructures. In this paper, based on our experiences building the Advanced Privacy-Preserving Federated Learning (APPFL) framework, we present our vision for an enterprise-grade, privacy-preserving FL framework designed to scale seamlessly across computing environments. We identify several key capabilities that such a framework must provide: (1) Scalable local simulation and prototyping to accelerate experimentation and algorithm design; (2) seamless transition from simulation to deployment; (3) distributed deployment across diverse, real-world infrastructures, from personal devices to cloud clusters and HPC systems; (4) multi-level abstractions that balance ease of use and research flexibility; and (5) comprehensive privacy and security through techniques such as differential privacy, secure aggregation, robust authentication, and confidential computing. We further discuss architectural designs to realize these goals. This framework aims to bridge the gap between research prototypes and enterprise-scale deployment, enabling scalable, reliable, and privacy-preserving AI for science.

</details>


### [8] [Flex-MIG: Enabling Distributed Execution on MIG](https://arxiv.org/abs/2511.09143)
*Myungsu Kim,Ikjun Yeom,Younghoon Kim*

Main category: cs.DC

TL;DR: Flex-MIG是一个软件框架，通过将MIG从一对一分配模型改为一对多分配模型，并支持跨MIG实例的主机共享内存集合操作，解决了GPU集群中的碎片化和利用率低的问题。


<details>
  <summary>Details</summary>
Motivation: 多租户环境中的GPU集群经常存在利用率不足的问题，而NVIDIA MIG虽然提供硬件级隔离，但其硬件刚性和传统的一对一分配模型会导致严重的碎片化和集群范围的利用率低下。

Method: Flex-MIG采用纯软件方法，用一对多分配模型替代一对一模型，并实现跨MIG实例的主机共享内存集合操作，无需硬件修改。

Result: Flex-MIG消除了需要排空的重配置过程，减少了碎片化，在不同跟踪数据下将makespan提高了高达17%。

Conclusion: 将MIG的操作模型重新设计为软件协调层，可以显著提高集群效率。

Abstract: GPU clusters in multi-tenant settings often suffer from underutilization, making GPU-sharing technologies essential for efficient resource use. Among them, NVIDIA Multi-Instance GPU (MIG) has gained traction for providing hardware-level isolation that enables concurrent workloads without interference. However, MIG's hardware rigidity and the conventional one-to-one allocation model jointly lead to severe fragmentation and cluster-wide underutilization. We present Flex-MIG, a software-only framework that replaces one-to-one with a one-to-many allocation model and enables host-shared-memory collectives across MIG instances without hardware modification. Flex-MIG eliminates drain-required reconfiguration, reduces fragmentation, and improves makespan by up to 17% across diverse traces, showing that rethinking MIG's operational model as a software-coordinated layer substantially improves cluster efficiency.

</details>


### [9] [Minimize Your Critical Path with Combine-and-Exchange Locks](https://arxiv.org/abs/2511.09194)
*Simon König,Lukas Epple,Christian Becker*

Main category: cs.DC

TL;DR: 论文提出了一种新的用户空间任务调度方法CES，通过组合交换调度来优化协程同步，避免了传统用户空间同步原语带来的不必要延迟，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现代编程语言广泛支持协程用于高并行或异步应用，用户空间同步避免了重量级系统调用，但现有用户空间同步原语仍从内核级调度视角处理同步，引入了不必要的延迟并限制了吞吐量。

Method: 开发了Combine-and-Exchange Scheduling (CES)方法，确保竞争临界区保持在同一个执行线程上，同时将可并行工作均匀分布到其他线程。

Result: 该方法可应用于多种现有语言和库，在应用基准测试中实现了3倍性能提升，在微基准测试中实现了8倍性能提升。

Conclusion: CES为完全在用户空间调度的任务重新设计了同步方法，显著提高了协程同步的性能和吞吐量。

Abstract: Coroutines are experiencing a renaissance as many modern programming languages support the use of cooperative multitasking for highly parallel or asynchronous applications. One of the greatest advantages of this is that concurrency and synchronization is manged entirely in the userspace, omitting heavy-weight system calls. However, we find that state-of-the-art userspace synchronization primitives approach synchronization in the userspace from the perspective of kernel-level scheduling. This introduces unnecessary delays on the critical path of the application, limiting throughput. In this paper, we re-think synchronization for tasks that are scheduled entirely in the userspace (e.g., coroutines, fibers, etc.). We develop Combine-and-Exchange Scheduling (CES), a novel scheduling approach that ensures contended critical sections stay on the same thread of execution while parallelizable work is evenly spread across the remaining threads. We show that our approach can be applied to many existing languages and libraries, resulting in 3-fold performance improvements in application benchmarks as well as 8-fold performance improvements in microbenchmarks.

</details>


### [10] [No Cords Attached: Coordination-Free Concurrent Lock-Free Queues](https://arxiv.org/abs/2511.09410)
*Yusuf Motiwala*

Main category: cs.DC

TL;DR: 提出了Cyclic Memory Protection (CMP)无锁队列，通过有界保护窗口实现严格FIFO语义、无界容量和协调自由，在高度并发场景下性能优于现有方案1.72-4倍。


<details>
  <summary>Details</summary>
Motivation: 现有无锁队列实现过于复杂，协调机制开销远大于队列操作本身，特别是在AI时代需要数百到数千并发线程的场景下，这种开销变得不可接受。

Method: 使用循环内存保护(CMP)机制，通过有界保护窗口提供实际的内存回收保证，避免无限保护带来的复杂性和开销。

Result: CMP在高度竞争条件下性能比最先进的无锁队列提升1.72-4倍，能够扩展到数百个线程，同时保持严格FIFO语义。

Conclusion: 高度并发的队列可以回归其基本简单性，而无需削弱队列语义，有界保护比无限保护更实用和高效。

Abstract: The queue is conceptually one of the simplest data structures-a basic FIFO container. However, ensuring correctness in the presence of concurrency makes existing lock-free implementations significantly more complex than their original form. Coordination mechanisms introduced to prevent hazards such as ABA, use-after-free, and unsafe reclamation often dominate the design, overshadowing the queue itself. Many schemes compromise strict FIFO ordering, unbounded capacity, or lock-free progress to mask coordination overheads. Yet the true source of complexity lies in the pursuit of infinite protection against reclamation hazards--theoretically sound but impractical and costly. This pursuit not only drives unnecessary complexity but also creates a protection paradox where excessive protection reduces system resilience rather than improving it. While such costs may be tolerable in conventional workloads, the AI era has shifted the paradigm: training and inference pipelines involve hundreds to thousands of concurrent threads per node, and at this scale, protection and coordination overheads dominate, often far heavier than the basic queue operations themselves.
  This paper introduces Cyclic Memory Protection (CMP), a coordination-free queue that preserves strict FIFO semantics, unbounded capacity, and lock-free progress while restoring simplicity. CMP reclaims the strict FIFO that other approaches sacrificed through bounded protection windows that provide practical reclamation guarantees. We prove strict FIFO and safety via linearizability and bounded reclamation analysis, and show experimentally that CMP outperforms state-of-the-art lock-free queues by up to 1.72-4x under high contention while maintaining scalability to hundreds of threads. Our work demonstrates that highly concurrent queues can return to their fundamental simplicity without weakening queue semantics.

</details>


### [11] [Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links](https://arxiv.org/abs/2511.09485)
*Miroslav Popovic,Marko Popovic,Pavle Vasiljevic,Miodrag Djukic*

Main category: cs.DC

TL;DR: 本文使用CSP过程代数对联邦学习Python测试平台的第三个通用算法（TDM通信）进行形式化验证，分两阶段：第一阶段构建CSP模型，第二阶段使用PAT模型检查器验证其无死锁和成功终止。


<details>
  <summary>Details</summary>
Motivation: 对联邦学习框架中的第三个通用算法（TDM通信）进行形式化验证，确保其正确性，此前已对前两个算法进行了验证。

Method: 采用CSP过程代数构建模型，分两阶段：1）构建与Python代码对应的CSP模型；2）使用PAT模型检查器验证算法的死锁自由性（安全性）和成功终止性（活性）。

Result: PAT模型检查器自动证明了第三个通用算法的正确性，验证了其无死锁和成功终止。

Conclusion: 成功完成了对联邦学习Python测试平台中TDM通信算法的形式化验证，证明了该算法的安全性和活性。

Abstract: The Python Testbed for Federated Learning Algorithms is a simple FL framework targeting edge systems, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the universal TDM communication in the current time slot. The first two were formally verified in a previous paper using the CSP process algebra, and in this paper, we use the same approach to formally verify the third one, in two phases. In the first phase, we construct the CSP model as a faithful representation of the real Python code. In the second phase, the model checker PAT automatically proves correctness of the third generic algorithm by proving its deadlock freeness (safety property) and successful termination (liveness property).

</details>


### [12] [LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication](https://arxiv.org/abs/2511.09557)
*Prajwal Singhania,Siddharth Singh,Lannie Dalton Hough,Akarsh Srivastava,Harshitha Menon,Charles Fredrick Jekel,Abhinav Bhatele*

Main category: cs.DC

TL;DR: 本文研究了多节点分布式推理的性能，开发了基于NVSHMEM的分层all-reduce算法NVRAR，在Llama 3.1 405B模型上实现了最高1.72倍的延迟降低。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断增大，分布式推理变得越来越重要，需要跨多个GPU和节点进行高效扩展。

Method: 使用多个先进推理引擎和YALIS原型引擎进行实验，分析不同模型并行方案的强扩展性，并开发了基于递归倍增和NVSHMEM的NVRAR分层all-reduce算法。

Result: NVRAR在HPE Slingshot和InfiniBand互连上，对于128KB到2MB的消息大小，比NCCL延迟降低1.9x-3.6x；在Llama 3.1 405B模型的多节点解码密集型工作负载中，端到端批量延迟最高降低1.72倍。

Conclusion: NVRAR算法有效解决了all-reduce操作这一常见性能瓶颈，显著提升了多节点分布式推理的效率。

Abstract: As large language models (LLMs) continue to grow in size, distributed inference has become increasingly important. Model-parallel strategies must now efficiently scale not only across multiple GPUs but also across multiple nodes. In this work, we present a detailed performance study of multi-node distributed inference using LLMs on GPU-based supercomputers. We conduct experiments with several state-of-the-art inference engines alongside YALIS, a research-oriented prototype engine designed for controlled experimentation. We analyze the strong-scaling behavior of different model-parallel schemes and identify key bottlenecks. Since all-reduce operations are a common performance bottleneck, we develop NVRAR, a hierarchical all-reduce algorithm based on recursive doubling with NVSHMEM. NVRAR achieves up to 1.9x-3.6x lower latency than NCCL for message sizes between 128 KB and 2 MB on HPE Slingshot and InfiniBand interconnects. Integrated into YALIS, NVRAR achieves up to a 1.72x reduction in end-to-end batch latency for the Llama 3.1 405B model in multi-node decode-heavy workloads using tensor parallelism.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [3D Guard-Layer: An Integrated Agentic AI Safety System for Edge Artificial Intelligence](https://arxiv.org/abs/2511.08842)
*Eren Kurshan,Yuan Xie,Paul Franzon*

Main category: cs.AR

TL;DR: 提出了一种基于3D集成的代理AI安全架构，通过专用安全层动态学习和缓解针对AI系统的攻击，利用边缘计算硬件的共位优势持续监控、检测和主动缓解威胁。


<details>
  <summary>Details</summary>
Motivation: 尽管实施了护栏和安全机制，边缘AI领域的安全漏洞和挑战日益普遍，成为AI系统实际部署和安全性的重要障碍。

Method: 采用代理AI安全架构，通过3D集成专用安全层，构建自适应AI安全基础设施，利用边缘计算硬件的共位优势进行持续监控和威胁缓解。

Result: 系统增强了对抗新兴网络攻击的韧性，同时提高了系统可靠性、模块化和性能，且成本和3D集成开销最小。

Conclusion: 该架构为边缘AI系统提供了有效的安全保护，解决了实际部署中的安全挑战。

Abstract: AI systems have found a wide range of real-world applications in recent years. The adoption of edge artificial intelligence, embedding AI directly into edge devices, is rapidly growing. Despite the implementation of guardrails and safety mechanisms, security vulnerabilities and challenges have become increasingly prevalent in this domain, posing a significant barrier to the practical deployment and safety of AI systems. This paper proposes an agentic AI safety architecture that leverages 3D to integrate a dedicated safety layer. It introduces an adaptive AI safety infrastructure capable of dynamically learning and mitigating attacks against the AI system. The system leverages the inherent advantages of co-location with the edge computing hardware to continuously monitor, detect and proactively mitigate threats to the AI system. The integration of local processing and learning capabilities enhances resilience against emerging network-based attacks while simultaneously improving system reliability, modularity, and performance, all with minimal cost and 3D integration overhead.

</details>


### [14] [FsimNNs: An Open-Source Graph Neural Network Platform for SEU Simulation-based Fault Injection](https://arxiv.org/abs/2511.09131)
*Li Lu,Jianan Wen,Milos Krstic*

Main category: cs.AR

TL;DR: 开发了一个基于时空图神经网络的开源平台，用于加速单粒子翻转故障模拟，解决了传统模拟方法计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 基于模拟的故障注入方法在评估电路对单粒子翻转的脆弱性时，随着电路复杂度的增加，计算成本显著增长。

Method: 引入了三种包含空洞空间金字塔池化和注意力机制的时空图神经网络架构，改进了时空特征提取能力。

Result: 在六个不同复杂度的开源电路上构建了SEU故障模拟数据集，评估了STGNN模型的预测能力，并讨论了其泛化能力。

Conclusion: 开发的开源平台和数据集支持可重复性研究，为加速SEU故障模拟提供了有效解决方案。

Abstract: Simulation-based fault injection is a widely adopted methodology for assessing circuit vulnerability to Single Event Upsets (SEUs); however, its computational cost grows significantly with circuit complexity. To address this limitation, this work introduces an open-source platform that exploits Spatio-Temporal Graph Neural Networks (STGNNs) to accelerate SEU fault simulation. The platform includes three STGNN architectures incorporating advanced components such as Atrous Spatial Pyramid Pooling (ASPP) and attention mechanisms, thereby improving spatio-temporal feature extraction. In addition, SEU fault simulation datasets are constructed from six open-source circuits with varying levels of complexity, providing a comprehensive benchmark for performance evaluation. The predictive capability of the STGNN models is analyzed and compared on these datasets. Moreover, to further investigate the efficiency of the approach, we evaluate the predictive capability of STGNNs across multiple test cases and discuss their generalization capability. The developed platform and datasets are released as open-source to support reproducibility and further research on https://github.com/luli2021/FsimNNs.

</details>
