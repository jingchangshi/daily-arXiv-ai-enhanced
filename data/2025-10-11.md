<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Type, Ability, and Effect Systems: Perspectives on Purity, Semantics, and Expressiveness](https://arxiv.org/abs/2510.07582)
*Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 该论文提出了基于上下文等价性的纯度语义定义，并建议用完整性来衡量表达能力。研究表明最小效应系统和能力系统在表达能力上不可比较，因此提出了结合类型、能力和效应系统的综合方案。


<details>
  <summary>Details</summary>
Motivation: 现有强制执行纯计算和效应交互分离的方法（如单子、类型-效应系统、能力系统）在精确性和可用性之间存在张力，各有优缺点。论文旨在为评估这类系统设立更高标准。

Method: 首先提出基于上下文等价性的纯度语义定义；其次用完整性来衡量表达能力；然后分析最小效应系统和能力系统的表达能力；最后提出综合方案并提供逻辑关系来辅助纯度证明。

Result: 研究表明最小效应系统和能力系统在表达能力上是不可比较的，即没有一种系统在表达能力上完全包含另一种。

Conclusion: 类型、能力和效应系统的综合方案能够结合各自优势并避免弱点，论文提供了形式化模型和逻辑关系来支持各种效应类型系统的纯度证明。

Abstract: Programming benefits from a clear separation between pure, mathematical
computation and impure, effectful interaction with the world. Existing
approaches to enforce this separation include monads, type-and-effect systems,
and capability systems. All share a tension between precision and usability,
and each one has non-obvious strengths and weaknesses.
  This paper aims to raise the bar in assessing such systems. First, we propose
a semantic definition of purity, inspired by contextual equivalence, as a
baseline independent of any specific typing discipline. Second, we propose that
expressiveness should be measured by the degree of completeness, i.e., how many
semantically pure terms can be typed as pure. Using this measure, we focus on
minimal meaningful effect and capability systems and show that they are
incomparable, i.e., neither subsumes the other in terms of expressiveness.
  Based on this result, we propose a synthesis and show that type, ability, and
effect systems combine their respective strengths while avoiding their
weaknesses. As part of our formal model, we provide a logical relation to
facilitate proofs of purity and other properties for a variety of effect typing
disciplines.

</details>


### [2] [The Functional Machine Calculus III: Control](https://arxiv.org/abs/2510.07851)
*Willem Heijltjes*

Main category: cs.PL

TL;DR: Functional Machine Calculus扩展了lambda演算，统一了函数式和命令式编程范式，支持计算效果、控制流操作，并保证汇合归约和类型终止性。


<details>
  <summary>Details</summary>
Motivation: 为了统一函数式和命令式编程范式，在保持lambda演算核心特性的同时嵌入计算效果、评估策略和控制流操作。

Method: 扩展简化的Krivine机器，使用多个操作数栈建模效果，使用延续栈建模顺序、分支和循环计算，定义简单操作语义。

Result: 实现了汇合归约关系，简单类型系统保证机器终止和强规范化（无迭代时），成功嵌入完整的命令式语言。

Conclusion: 提供了一个统一的功能-命令式计算模型，支持简单类型、直观操作语义和汇合归约语义。

Abstract: The Functional Machine Calculus (Heijltjes 2022) is a new approach to
unifying the imperative and functional programming paradigms. It extends the
lambda-calculus, preserving the key features of confluent reduction and typed
termination, to embed computational effects, evaluation strategies, and control
flow operations. The first instalment modelled sequential higher-order
computation with global store, input/output, probabilities, and
non-determinism, and embedded both the call-by-name and call-by-value
lambda-calculus, as well as Moggi's computational metalanguage and Levy's
call-by-push-value. The present paper extends the calculus from sequential to
branching and looping control flow. This allows the faithful embedding of a
minimal but complete imperative language, including conditionals, exception
handling, and iteration, as well as constants and algebraic data types.
  The calculus is defined through a simple operational semantics, extending the
(simplified) Krivine machine for the lambda-calculus with multiple operand
stacks to model effects and a continuation stack to model sequential,
branching, and looping computation. It features a confluent reduction relation
and a system of simple types that guarantees termination of the machine and
strong normalization of reduction (in the absence of iteration). These
properties carry over to the embedded imperative language, providing a unified
functional-imperative model of computation that supports simple types, a direct
and intuitive operational semantics, and a confluent reduction semantics.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Adaptive Execution Scheduler for DataDios SmartDiff](https://arxiv.org/abs/2510.07811)
*Aryan Poduri*

Main category: cs.DC

TL;DR: 提出了一种用于单机差分引擎的自适应调度器，通过动态调整批处理大小和线程/工作器数量，在固定CPU和内存预算下最小化p95延迟。


<details>
  <summary>Details</summary>
Motivation: 现有调度方法在内存使用和延迟优化方面存在不足，需要一种能够自适应调整资源分配以避免内存溢出同时优化延迟的解决方案。

Method: 使用轻量级预分析器估计每行字节数和I/O速率，在线成本/内存模型修剪不安全操作，采用带防护的爬山策略优化延迟，并包含背压和慢任务缓解机制。

Result: 在合成和公开表格基准测试中，相比调优的热身启发式方法，p95延迟降低23-28%；相比固定网格基线降低35-40%，峰值内存降低16-22%，且无内存溢出问题。

Conclusion: 该自适应调度器在保持可比吞吐量的同时，显著降低了延迟和内存使用，证明了其在实际应用中的有效性。

Abstract: We present an adaptive scheduler for a single differencing engine (SmartDiff)
with two execution modes: (i) in-memory threads and (ii) Dask based
parallelism. The scheduler continuously tunes batch size and worker/thread
count within fixed CPU and memory budgets to minimize p95 latency. A
lightweight preflight profiler estimates bytes/row and I/O rate; an online
cost/memory model prunes unsafe actions; and a guarded hill-climb policy favors
lower latency with backpressure and straggler mitigation. Backend selection is
gated by a conservative working-set estimate so that in-memory execution is
chosen when safe, otherwise Dask is used. Across synthetic and public tabular
benchmarks, the scheduler reduces p95 latency by 23 to 28 percent versus a
tuned warm-up heuristic (and by 35 to 40 percent versus fixed grid baselines),
while lowering peak memory by 16 to 22 percent (25 to 32 percent vs. fixed)
with zero OOMs and comparable throughput.

</details>


### [4] [A Multi-Simulation Bridge for IoT Digital Twins](https://arxiv.org/abs/2510.08164)
*Marco Picone,Samuele Burattini,Marco Melloni,Prasad Talasila,Davide Ziglioli,Matteo Martinelli,Nicola Bicocchi,Alessandro Ricci,Peter Gorm Larsen*

Main category: cs.DC

TL;DR: 提出了DT Simulation Bridge框架，支持数字孪生与仿真平台之间的双向交互，提升工业物联网系统的设计灵活性和实时操作能力。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生在物联网和工业物联网中能力的增强，需要与仿真平台无缝集成以支持系统设计、验证和实时操作。

Method: 设计并实现了DT Simulation Bridge软件框架，支持数字孪生与仿真环境之间的多样化交互模式，通过双向数据交换实现动态模型更新和实时反馈。

Result: 实验结果表明，该框架增强了设计灵活性，促进了虚拟调试，并在真实条件下支持实时行为分析，在多种工业场景中表现出有效性。

Conclusion: DT Simulation Bridge为数字孪生与仿真平台的集成提供了有效的解决方案，支持从开发到实时操作的全生命周期管理。

Abstract: The increasing capabilities of Digital Twins (DTs) in the context of the
Internet of Things (IoT) and Industrial IoT (IIoT) call for seamless
integration with simulation platforms to support system design, validation, and
real-time operation. This paper introduces the concept, design, and
experimental evaluation of the DT Simulation Bridge - a software framework that
enables diverse interaction patterns between active DTs and simulation
environments. The framework supports both the DT development lifecycle and the
incorporation of simulations during active operation. Through bidirectional
data exchange, simulations can update DT models dynamically, while DTs provide
real-time feedback to adapt simulation parameters. We describe the
architectural design and core software components that ensure flexible
interoperability and scalable deployment. Experimental results show that the DT
Simulation Bridge enhances design agility, facilitates virtual commissioning,
and supports live behavioral analysis under realistic conditions, demonstrating
its effectiveness across a range of industrial scenarios.

</details>


### [5] [Towards Energy-Efficient Serverless Computing with Hardware Isolation](https://arxiv.org/abs/2510.08180)
*Natalie Carl,Tobias Pfandzelter,David Bermbach*

Main category: cs.DC

TL;DR: 提出重新设计无服务器硬件架构，使用硬件隔离替代软件隔离，为每个函数分配独立处理器，可显著降低能耗


<details>
  <summary>Details</summary>
Motivation: 当前无服务器平台在传统服务器硬件上运行数千个函数，需要昂贵的软件隔离机制和高度的资源过度配置，导致能效低下

Method: 使用硬件隔离技术，为每个函数分配独立处理器，构建仅在实际工作时消耗能源的无服务器硬件栈

Result: 初步评估显示可减少90.63%的能耗开销，平均节省70.8MW

Conclusion: 硬件隔离的无服务器架构能显著提升能效，更好地满足无服务器软件的需求

Abstract: Serverless computing provides just-in-time infrastructure provisioning with
rapid elasticity and a finely-grained pricing model. As full control of
resource allocation is in the hands of the cloud provider and applications only
consume resources when they actually perform work, we believe that serverless
computing is uniquely positioned to maximize energy efficiency.
  However, the focus of current serverless platforms is to run hundreds or
thousands of serverless functions from different tenants on traditional server
hardware, requiring expensive software isolation mechanisms and a high degree
of overprovisioning, i.e., idle servers, to anticipate load spikes. With shared
caches, high clock frequencies, and many-core architectures, servers today are
optimized for large, singular workloads but not to run thousands of isolated
functions.
  We propose rethinking the serverless hardware architecture to align it with
the requirements of serverless software. Specifically, we propose using
hardware isolation with individual processors per function instead of software
isolation resulting in a serverless hardware stack that consumes energy only
when an application actually performs work. In preliminary evaluation with real
hardware and a typical serverless workload we find that this could reduce
energy consumption overheads by 90.63% or an average 70.8MW.

</details>


### [6] [Distributed Resource Selection for Self-Organising Cloud-Edge Systems](https://arxiv.org/abs/2510.08228)
*Quentin Renau,Amjad Ullah,Emma Hart*

Main category: cs.DC

TL;DR: 提出了一种分布式资源选择机制，用于云边环境中的动态资源分配，通过分布式决策避免集中式协调的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 在高度动态的云边环境中，集中式协调成为性能瓶颈，需要分布式、自组织的编排系统来支持复杂分布式应用的实时部署和适应。

Method: 采用基于共识的机制，利用本地知识和代理间协作，实现无需中央控制器的分布式资源分配决策。

Result: 计算时间是影响分配决策的关键因素。该方法在不牺牲最优性或增加成本的情况下实现快速分配，在集中式启发式算法运行慢30倍的情况下仍能及时获得结果。

Conclusion: 该分布式资源选择机制为分布式编排系统提供了可行的核心组件，能够在云边环境中实现高效、可扩展且具有弹性的资源管理。

Abstract: This paper presents a distributed resource selection mechanism for diverse
cloud-edge environments, enabling dynamic and context-aware allocation of
resources to meet the demands of complex distributed applications. By
distributing the decision-making process, our approach ensures efficiency,
scalability, and resilience in highly dynamic cloud-edge environments where
centralised coordination becomes a bottleneck. The proposed mechanism aims to
function as a core component of a broader, distributed, and self-organising
orchestration system that facilitates the intelligent placement and adaptation
of applications in real-time. This work leverages a consensus-based mechanism
utilising local knowledge and inter-agent collaboration to achieve efficient
results without relying on a central controller, thus paving the way for
distributed orchestration. Our results indicate that computation time is the
key factor influencing allocation decisions. Our approach consistently delivers
rapid allocations without compromising optimality or incurring additional cost,
achieving timely results at scale where exhaustive search is infeasible and
centralised heuristics run up to 30 times slower.

</details>


### [7] [Energy-Efficient Maximal Independent Sets in Radio Networks](https://arxiv.org/abs/2510.08244)
*Dominick Banasik,Varsha Dani,Fabien Dufoulon,Aayush Gupta,Thomas P. Hayes,Gopal Pandurangan*

Main category: cs.DC

TL;DR: 本文提出了无线网络中更节能的最大独立集（MIS）算法，在碰撞检测（CD）模型中达到O(log n)能量复杂度（最优），在无碰撞检测（no-CD）模型中达到O(log²n log log n)能量复杂度，显著优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 无线网络通常由电池供电，能量是宝贵资源。需要设计能量复杂度尽可能低的分布式算法，其中只有唤醒轮次消耗能量。

Method: 针对CD和no-CD两种无线网络模型，设计了随机分布式MIS算法。CD模型通过优化算法结构实现低能耗，no-CD模型采用更复杂的技术处理无碰撞检测的挑战。

Result: CD模型：能量复杂度O(log n)（最优），轮复杂度O(log² n)，失败概率1/poly(n)。no-CD模型：能量复杂度O(log²n log log n)，轮复杂度O(log³ n log Δ)，显著优于现有O(log³ n)的算法。

Conclusion: 本文在无线网络MIS问题上实现了显著的能量效率提升，特别是在CD模型中达到了理论最优，为能量受限的无线网络提供了实用的分布式算法解决方案。

Abstract: The maximal independent set (MIS) is one of the most fundamental problems in
distributed computing, and it has been studied intensively for over four
decades. This paper focuses on the MIS problem in the Radio Network model, a
standard model widely used to model wireless networks, particularly ad hoc
wireless and sensor networks. Energy is a premium resource in these networks,
which are typically battery-powered. Hence, designing distributed algorithms
that use as little energy as possible is crucial. We use the well-established
energy model where a node can be sleeping or awake in a round, and only the
awake rounds (when it can send or listen) determine the energy complexity of
the algorithm, which we want to minimize.
  We present new, more energy-efficient MIS algorithms in radio networks with
arbitrary and unknown graph topology. We present algorithms for two popular
variants of the radio model -- with collision detection (CD) and without
collision detection (no-CD). Specifically, we obtain the following results:
  1. CD model: We present a randomized distributed MIS algorithm with energy
complexity $O(\log n)$, round complexity $O(\log^2 n)$, and failure probability
$1 / poly(n)$, where $n$ is the network size. We show that our energy
complexity is optimal by showing a matching $\Omega(\log n)$ lower bound.
  2. no-CD model: In the more challenging no-CD model, we present a randomized
distributed MIS algorithm with energy complexity $O(\log^2n \log \log n)$,
round complexity $O(\log^3 n \log \Delta)$, and failure probability $1 /
poly(n)$. The energy complexity of our algorithm is significantly lower than
the round (and energy) complexity of $O(\log^3 n)$ of the best known
distributed MIS algorithm of Davies [PODC 2023] for arbitrary graph topology.

</details>


### [8] [Investigating Matrix Repartitioning to Address the Over- and Undersubscription Challenge for a GPU-based CFD Solver](https://arxiv.org/abs/2510.08536)
*Gregor Olenik,Marcel Koch,Hartwig Anzt*

Main category: cs.DC

TL;DR: 提出了一种重新分区策略来改进OpenFOAM中基于插件的GPU加速方法，通过平衡CPU矩阵组装和GPU线性求解，缓解资源过度订阅问题


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算日益依赖GPU，但在复杂科学框架如OpenFOAM中集成GPU加速仍面临挑战。现有方法要么完全重构代码库，要么使用基于插件的GPU求解器，在性能和开发工作量之间存在权衡

Method: 提出重新分区策略，包括详细的计算模型、新颖的矩阵重新分区和更新过程，在异构CPU-GPU环境中更好地平衡CPU矩阵组装和GPU线性求解

Result: 该方法显著缓解了过度订阅问题，在大型CFD模拟中提高了求解器性能和资源利用率

Conclusion: 所提出的重新分区方法有效改进了OpenFOAM中基于插件的GPU加速，为异构计算环境提供了更好的性能解决方案

Abstract: Modern high-performance computing (HPC) increasingly relies on GPUs, but
integrating GPU acceleration into complex scientific frameworks like OpenFOAM
remains a challenge. Existing approaches either fully refactor the codebase or
use plugin-based GPU solvers, each facing trade-offs between performance and
development effort. In this work, we address the limitations of plugin-based
GPU acceleration in OpenFOAM by proposing a repartitioning strategy that better
balances CPU matrix assembly and GPU-based linear solves. We present a detailed
computational model, describe a novel matrix repartitioning and update
procedure, and evaluate its performance on large-scale CFD simulations. Our
results show that the proposed method significantly mitigates oversubscription
issues, improving solver performance and resource utilization in heterogeneous
CPU-GPU environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [How long can you sleep? Idle Time System Inefficiencies and Opportunities](https://arxiv.org/abs/2510.07449)
*Georgia Antoniou,Haris Volos,Jawad Haj Yahya,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: 提出了一个基于排队模型的框架，用于揭示运行延迟关键应用的现代服务器的空闲机会，发现实际系统与理论模型相比存在显著的空闲状态利用不足问题。


<details>
  <summary>Details</summary>
Motivation: 现代服务器在运行延迟关键应用时存在空闲机会未被充分利用的问题，需要量化分析这种机会并识别效率低下的原因。

Method: 使用三种排队模型（M/M/1、cxM/M/1和M/M/c）来估计CPU核心和系统级别的理论空闲时间分布，并将实际服务器空闲情况与理论模型进行比较。

Result: 比较发现实际服务器与理论模型之间存在显著差异，存在大量进入深度空闲状态的机会被错过，主要原因是空闲管理器的准确性和传统深度空闲状态转换延迟问题。

Conclusion: 该框架为早期设计探索提供了方法，能够洞察不同服务器系统配置和负载下的空闲时间行为和机会。

Abstract: This work introduces a model-based framework that reveals the idle
opportunity of modern servers running latency-critical applications.
Specifically, three queuing models, M/M/1, cxM/M/1, and M/M/c, are used to
estimate the theoretical idle time distribution at the CPU core and system
(package) level. A comparison of the actual idleness of a real server and that
from the theoretical models reveals significant missed opportunities to enter
deep idle states. This inefficiency is attributed to the idle-governor
inaccuracy and the high latency to transition to/from legacy deep-idle states.
The proposed methodology offers the means for an early-stage design exploration
and insights into idle time behavior and opportunities for varying server
system configurations and load.

</details>


### [10] [DL-PIM: Improving Data Locality in Processing-in-Memory Systems](https://arxiv.org/abs/2510.07719)
*Parker Hao Tian,Zahra Yousefijamarani,Alaa Alameldeen*

Main category: cs.AR

TL;DR: DL-PIM是一种新型PIM架构，通过动态检测数据移动开销并主动将数据移动到本地内存的保留区域来改善数据局部性，使用分布式地址间接硬件查找表重定向流量。


<details>
  <summary>Details</summary>
Motivation: 传统PIM架构虽然能减少处理器与内存间的数据传输成本，但其优势依赖于数据与处理单元的邻近性。数据移动开销会降低PIM的性能和能效，因为需要将数据从远程内存位置移动到内存内的处理单元进行计算。

Method: 提出DL-PIM架构，动态检测数据移动开销，主动将数据移动到请求处理单元的本地内存保留区域。使用分布式地址间接硬件查找表重定向流量到当前数据位置。在HMC和HBM两种3D堆叠内存上实现，并采用自适应机制评估间接访问的成本和收益，动态启用或禁用以防止对某些工作负载产生负面影响。

Result: DL-PIM将HMC中每个请求的平均内存延迟降低54%，HBM中降低50%。对于具有大量数据重用的工作负载，HMC性能提升15%，HBM提升5%。所有代表性工作负载在HMC中实现6%加速，HBM中实现3%加速。

Conclusion: DL-PIM通过增强数据局部性有效提高了整体系统性能，特别是在3D堆叠内存架构中显著减少了数据移动开销和内存访问延迟。

Abstract: PIM architectures aim to reduce data transfer costs between processors and
memory by integrating processing units within memory layers. Prior PIM
architectures have shown potential to improve energy efficiency and
performance. However, such advantages rely on data proximity to the processing
units performing computations. Data movement overheads can degrade PIM's
performance and energy efficiency due to the need to move data between a
processing unit and a distant memory location. %they face challenges due to the
overhead of transferring data from remote memory locations to processing units
inside memory for computation. In this paper, we demonstrate that a large
fraction of PIM's latency per memory request is attributed to data transfers
and queuing delays from remote memory accesses. To improve PIM's data locality,
we propose DL-PIM, a novel architecture that dynamically detects the overhead
of data movement, and proactively moves data to a reserved area in the local
memory of the requesting processing unit. DL-PIM uses a distributed
address-indirection hardware lookup table to redirect traffic to the current
data location. We propose DL-PIM implementations on two 3D stacked memories:
HMC and HBM. While some workloads benefit from DL-PIM, others are negatively
impacted by the additional latency due to indirection accesses. Therefore, we
propose an adaptive mechanism that assesses the cost and benefit of indirection
and dynamically enables or disables it to prevent degrading workloads that
suffer from indirection. Overall, DL-PIM reduces the average memory latency per
request by 54% in HMC and 50% in HBM which resulted in performance improvement
of 15% for workloads with substantial data reuse in HMC and 5% in HBM. For all
representative workloads, DL-PIM achieved a 6% speedup in HMC and a 3% speedup
in HBM, showing that DL-PIM enhances data locality and overall system
performance.

</details>


### [11] [A Scalable FPGA Architecture With Adaptive Memory Utilization for GEMM-Based Operations](https://arxiv.org/abs/2510.08137)
*Anastasios Petropoulos,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: 提出基于FPGA的可配置DNN推理加速器，采用脉动阵列、高带宽内存和UltraRAM，支持多种处理单元配置，通过启发式权重传输调度实现高效吞吐量，并可扩展模拟存内计算设备。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络推理对专用硬件计算效率需求的增长，需要开发灵活可配置的加速器架构来适应不同模型和未来FPGA设计。

Method: 设计FPGA动态可配置加速器，包含脉动阵列、高带宽内存和UltraRAM，提供两种不同计算能力的处理单元配置，使用相同接口和外围模块，通过多处理单元实例化和启发式权重传输调度。

Result: 该架构相比先前工作实现了显著的吞吐效率提升，并能扩展模拟模拟存内计算设备，用于研究设备级噪声行为。

Conclusion: 该工作提出了一个适用于各种模型和未来FPGA设计的通用DNN推理加速架构，具有高度适应性和可扩展性。

Abstract: Deep neural network (DNN) inference relies increasingly on specialized
hardware for high computational efficiency. This work introduces a
field-programmable gate array (FPGA)-based dynamically configurable accelerator
featuring systolic arrays, high-bandwidth memory, and UltraRAMs. We present two
processing unit (PU) configurations with different computing capabilities using
the same interfaces and peripheral blocks. By instantiating multiple PUs and
employing a heuristic weight transfer schedule, the architecture achieves
notable throughput efficiency over prior works. Moreover, we outline how the
architecture can be extended to emulate analog in-memory computing (AIMC)
devices to aid next-generation heterogeneous AIMC chip designs and investigate
device-level noise behavior. Overall, this brief presents a versatile DNN
inference acceleration architecture adaptable to various models and future FPGA
designs.

</details>


### [12] [FMCache: File-System Metadata Caching in Programmable Switches](https://arxiv.org/abs/2510.08351)
*Qingxiu Liu,Jiazhen Cai,Siyuan Sheng,Yuhui Chen,Lu Tang,Zhirong Shen,Patrick P. C. Lee*

Main category: cs.AR

TL;DR: FMCache是一个基于可编程交换机的文件系统元数据缓存框架，直接在交换机数据平面处理元数据请求，显著提升分布式文件系统性能


<details>
  <summary>Details</summary>
Motivation: 分布式文件系统中多客户端访问元数据时，客户端缓存会带来显著的缓存一致性维护开销和复杂性，需要更高效的元数据管理方案

Method: 利用可编程交换机在数据平面直接处理文件系统元数据请求，解决文件系统特有的路径依赖问题，并在严格交换机资源约束下实现高效缓存

Result: 在Tofino交换机测试平台上，FMCache相比原生HDFS吞吐量提升最高181.6%，与客户端缓存结合时额外提升139.6%，同时保持低延迟和有限交换机资源使用

Conclusion: FMCache证明了在交换机层面实现文件系统元数据缓存的可行性，能够有效提升分布式文件系统性能并补充客户端缓存方案

Abstract: Fast and scalable metadata management across multiple metadata servers is
crucial for distributed file systems to handle numerous files and directories.
Client-side caching of frequently accessed metadata can mitigate server loads,
but incurs significant overhead and complexity in maintaining cache consistency
when the number of clients increases. We propose FMCache, an in-switch
file-system metadata caching framework that leverages programmable switches to
serve file-system metadata requests from multiple clients directly in the
switch data plane. Unlike prior in-switch key-value caching approaches, FMCache
addresses file-system-specific path dependencies under stringent switch
resource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on
a Tofino-switch testbed using real-world file-system metadata workloads.
FMCache achieves up to 181.6% higher throughput than vanilla HDFS and
complements client-side caching with additional throughput gains of up to
139.6%. It also incurs low latencies and limited switch resource usage.

</details>


### [13] [SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM Inference](https://arxiv.org/abs/2510.08544)
*Hengrui Zhang,Pratyush Patel,August Ning,David Wentzlaff*

Main category: cs.AR

TL;DR: SPAD提出专门针对LLM推理的两个阶段（预填充和解码）设计专用芯片，通过预填充芯片和解码芯片的分离设计，相比传统GPU/TPU能显著降低硬件成本和功耗。


<details>
  <summary>Details</summary>
Motivation: 传统GPU/TPU采用"越多越好"的设计理念，导致在LLM推理的两个阶段（计算密集的预填充阶段和内存密集的解码阶段）都存在资源利用不足的问题，增加了服务成本。

Method: 采用"少即是多"方法设计专用芯片：预填充芯片使用更大的脉动阵列和成本效益高的GDDR内存；解码芯片保持高内存带宽但减少计算容量。

Result: 相比H100，预填充芯片性能提高8%，硬件成本降低52%；解码芯片达到97%性能，TDP降低28%。端到端模拟显示硬件成本降低19%-41%，TDP降低2%-17%。

Conclusion: SPAD设计具有长期适用性，即使模型和工作负载变化，通过重新分配芯片类型仍能实现11%-43%的硬件成本降低。

Abstract: Large Language Models (LLMs) have gained popularity in recent years, driving
up the demand for inference. LLM inference is composed of two phases with
distinct characteristics: a compute-bound prefill phase followed by a
memory-bound decode phase. To efficiently serve LLMs, prior work proposes
prefill-decode disaggregation to run each phase on separate hardware. However,
existing hardware poorly matches the different requirements of each phase.
Current datacenter GPUs and TPUs follow a more-is-better design philosophy that
maximizes compute and memory resources, causing memory bandwidth
underutilization in the prefill phase and compute underutilization in the
decode phase. Such underutilization directly translates into increased serving
costs.
  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting
a less-is-more methodology to design specialized chips tailored to the distinct
characteristics of prefill and decode phases. The proposed Prefill Chips have
larger systolic arrays and use cost-effective GDDR memory, whereas the proposed
Decode Chips retain high memory bandwidth but reduce compute capacity. Compared
to modeled H100s, simulations show that the proposed Prefill Chips deliver 8%
higher prefill performance on average at 52% lower hardware cost, while the
proposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.
  End-to-end simulations on production traces show that SPAD reduces hardware
cost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while
offering the same performance. Even when models and workloads change, SPAD can
reallocate either type of chip to run either phase and still achieve 11%-43%
lower hardware costs, demonstrating the longevity of the SPAD design.

</details>
