{"id": "2507.02871", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02871", "abs": "https://arxiv.org/abs/2507.02871", "authors": ["Kia Silverbrook"], "title": "ZettaLith: An Architectural Exploration of Extreme-Scale AI Inference Acceleration", "comment": "53 pages, 15 figures, 23 tables", "summary": "The high computational cost and power consumption of current and anticipated\nAI systems present a major challenge for widespread deployment and further\nscaling. Current hardware approaches face fundamental efficiency limits. This\npaper introduces ZettaLith, a scalable computing architecture designed to\nreduce the cost and power of AI inference by over 1,000x compared to current\nGPU-based systems. Based on architectural analysis and technology projections,\na single ZettaLith rack could potentially achieve 1.507 zettaFLOPS in 2027 -\nrepresenting a theoretical 1,047x improvement in inference performance, 1,490x\nbetter power efficiency, and could be 2,325x more cost-effective than current\nleading GPU racks for FP4 transformer inference. The ZettaLith architecture\nachieves these gains by abandoning general purpose GPU applications, and via\nthe multiplicative effect of numerous co-designed architectural innovations\nusing established digital electronic technologies, as detailed in this paper.\nZettaLith's core architectural principles scale down efficiently to exaFLOPS\ndesktop systems and petaFLOPS mobile chips, maintaining their roughly 1,000x\nadvantage. ZettaLith presents a simpler system architecture compared to the\ncomplex hierarchy of current GPU clusters. ZettaLith is optimized exclusively\nfor AI inference and is not applicable for AI training."}
{"id": "2507.03114", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.03114", "abs": "https://arxiv.org/abs/2507.03114", "authors": ["Seonho Lee", "Jihwan Oh", "Junkyum Kim", "Seokjin Go", "Jongse Park", "Divya Mahajan"], "title": "Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications", "comment": null, "summary": "This paper provides an in-depth characterization of GPU-accelerated systems,\nto understand the interplay between overlapping computation and communication\nwhich is commonly employed in distributed training settings. Due to the large\nsize of models, distributing them across multiple devices is required.\nOverlapping strategies, which enable concurrent computation and communication,\nare critical for mitigating communication bottlenecks and maximizing GPU\nutilization. However, the current consensus is that we should always and\naggressively overlap compute and communication to mitigate the overhead of\ndistribution. By systematically evaluating state-of-the-art GPUs, this study\ninvestigates the impact of hardware features such as numeric precision,\nspecialized cores, and power capping on distributed training workloads.\nComprehensive experiments and studies showcase the effects of overlapping\nstrategies on performance and power consumption across varying scenarios. We\nobserve that overlapping computation and communication can result in an average\ncomputational slowdown of 18.9%, with a maximum of 40.0% slowdown. This\nslowdown is in comparison to the scenario when no communication was happening\nwith the compute. We consider this an ideal execution scenario, where the\ncommunication in parallel has not impact on the compute time. However,\nperforming computation and communication sequentially is, on average, 10.2%\nslower than overlapped execution, with a maximum slowdown of 26.6%. We further\nobserve, while specialized datapath and optimized numeric precision mitigate\ncertain slowdowns, overlapping execution can lead to resource contention and\nalso increase power consumption under specific configurations. The analysis\nalso uncovers trade-offs introduced by power and frequency capping, emphasizing\nthe importance of balanced strategies to optimize energy efficiency and\ntraining throughput."}
{"id": "2507.03220", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03220", "abs": "https://arxiv.org/abs/2507.03220", "authors": ["Saransh Gupta", "Umesh Deshpande", "Travis Janssen", "Swami Sundararaman"], "title": "Symbiosis: Multi-Adapter Inference and Fine-Tuning", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) allows model builders to capture the\ntask specific parameters into adapters, which are a fraction of the size of the\noriginal base model. Popularity of PEFT technique for fine-tuning has led to\ncreation of a large number of adapters for popular Large Language Models\n(LLMs). However, existing frameworks fall short in supporting inference or\nfine-tuning with multiple adapters in the following ways. 1) For fine-tuning,\neach job needs to deploy its dedicated base model instance, which results in\nexcessive GPU memory consumption and poor GPU utilization. 2) While popular\ninference platforms can serve multiple PEFT adapters, they do not allow\nindependent resource management or mixing of different PEFT methods. 3) They\ncannot share resources (such as base model instance) between inference and\nfine-tuning jobs. 4) They do not provide privacy to users who may not wish to\nexpose their fine-tuned parameters to service providers. In Symbiosis, we\naddress the above problems by enabling as-a-service deployment of base model.\nThe base model layers can be shared across multiple inference or fine-tuning\nprocesses. Our split-execution technique decouples the execution of\nclient-specific adapters and layers from the frozen base model layers offering\nthem flexibility to manage their resources, to select their fine-tuning method,\nto achieve their performance goals. Our approach is transparent to models and\nworks out-of-the-box for most models in the transformers library. Our\nevaluation on Llama2-13B shows the compared to baseline, Symbiosis can\nfine-tune 4X more adapters on the same set of GPUs in the same amount of time."}
{"id": "2507.03305", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.03305", "abs": "https://arxiv.org/abs/2507.03305", "authors": ["Yong-Cheng Liaw", "Shuo-Han Chen"], "title": "Analysis and Optimized CXL-Attached Memory Allocation for Long-Context LLM Fine-Tuning", "comment": "9 pages, 10 figures, 2 tables", "summary": "The growing prevalence of Large Language Models (LLMs) and their substantial\nmemory requirements have prompted renewed interest in CPU offloading as a\nmethod to compensate for limited GPU memory. In particular, when CPU memory is\nleveraged to temporarily store intermediate states of LLMs, CPU memory becomes\na new bottleneck and soon reaches the capacity limitation of commodity CPUs. In\nthis work, we investigate the effectiveness of Compute Express Link (CXL)\nadd-in card (AIC) memory as an extension to CPU memory, enabling larger model\nsizes and longer context lengths during fine-tuning. Through extensive\nbenchmarking, this study quantifies the performance overhead introduced by\ntransferring data between CXL memory, CPU, and GPUs, focusing on how\nconcurrency and data volume influence bandwidth utilization and latency. This\nstudy also compares CPUbased optimizer steps when model parameters, gradients,\nand optimizer states reside in local memory versus CXL memory, revealing that\nnaive adoption of CXL often degrades performance during the optimizer phase. To\novercome these challenges, this study proposes a CXL-aware allocation to\nstrategically partition CPU offloading workloads across both local and CXL\nmemory. This study further demonstrates that employing multiple AICs\nsignificantly reduces bandwidth contention, thus improving scalability.\nExperimental results show that these optimizations enable efficient\nlong-context LLM fine-tuning, underscoring CXL as a promising avenue for\nunlocking the full potential of CPU offloading in long-context LLM fine-tuning."}
{"id": "2507.03629", "categories": ["cs.PL", "cs.FL", "F.4.3; D.3.1; D.3.4"], "pdf": "https://arxiv.org/pdf/2507.03629", "abs": "https://arxiv.org/abs/2507.03629", "authors": ["SÃ©rgio Queiroz de Medeiros", "Fabio Mascarenhas"], "title": "Towards Automatic Error Recovery in Parsing Expression", "comment": "arXiv admin note: substantial text overlap with arXiv:1905.02145", "summary": "Error recovery is an essential feature for a parser that should be plugged in\nIntegrated Development Environments (IDEs), which must build Abstract Syntax\nTrees (ASTs) even for syntactically invalid programs in order to offer features\nsuch as automated refactoring and code completion.\n  Parsing Expressions Grammars (PEGs) are a formalism that naturally describes\nrecursive top-down parsers using a restricted form of backtracking. Labeled\nfailures are a conservative extension of PEGs that adds an error reporting\nmechanism for PEG parsers, and these labels can also be associated with\nrecovery expressions to also be an error recovery mechanism. These expressions\ncan use the full expressivity of PEGs to recover from syntactic errors.\n  Manually annotating a large grammar with labels and recovery expressions can\nbe difficult. In this work, we present an algorithm that automatically\nannotates a PEG with labels, and builds their corresponding recovery\nexpressions. We evaluate this algorithm by adding error recovery to the parser\nof the Titan programming language. The results shown that with a small amount\nof manual intervention our algorithm can be used to produce error recovering\nparsers for PEGs where most of the alternatives are disjoint."}
{"id": "2507.03255", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03255", "abs": "https://arxiv.org/abs/2507.03255", "authors": ["Zedong Peng", "Zeju Li", "Mingzhe Gao", "Qiang Xu", "Chen Zhang", "Jieru Zhao"], "title": "ForgeHLS: A Large-Scale, Open-Source Dataset for High-Level Synthesis", "comment": null, "summary": "We introduce ForgeEDA, an open-source comprehensive circuit dataset across\nvarious categories. ForgeEDA includes diverse circuit representations such as\nRegister Transfer Level (RTL) code, Post-mapping (PM) netlists, And-Inverter\nGraphs (AIGs), and placed netlists, enabling comprehensive analysis and\ndevelopment. We demonstrate ForgeEDA's utility by benchmarking state-of-the-art\nEDA algorithms on critical tasks such as Power, Performance, and Area (PPA)\noptimization, highlighting its ability to expose performance gaps and drive\nadvancements. Additionally, ForgeEDA's scale and diversity facilitate the\ntraining of AI models for EDA tasks, demonstrating its potential to improve\nmodel performance and generalization. By addressing limitations in existing\ndatasets, ForgeEDA aims to catalyze breakthroughs in modern IC design and\nsupport the next generation of innovations in EDA."}
{"id": "2507.03486", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.03486", "abs": "https://arxiv.org/abs/2507.03486", "authors": ["Younjeong Lee", "Young Yoon"], "title": "A Distributed Consensus Algorithm for Autonomous Vehicles Deciding Entering Orders on Intesections without Traffic Signals", "comment": "10 pages, 6 figures", "summary": "We propose a methodology for connected autonomous vehicles (CAVs) to\ndetermine their passing priority at unsignalized intersections where they\ncoexist with human-driven vehicles (HVs). Assuming that CAVs can perceive the\nentry order of surrounding vehicles using computer vision technology and are\ncapable of avoiding collisions, we introduce a voting-based distributed\nconsensus algorithm inspired by Raft to resolve tie-breaking among\nsimultaneously arriving CAVs. The algorithm is structured around the candidate\nand leader election processes and incorporates a minimal consensus quorum to\nensure both safety and liveness among CAVs under typical asynchronous\ncommunication conditions. Assuming CAVs to be SAE (Society of Automotive\nEngineers) Level-4 or higher autonomous vehicles, we implemented the proposed\ndistributed consensus algorithm using gRPC. By adjusting variables such as the\nCAV-to-HV ratio, intersection scale, and the processing time of computer vision\nmodules, we demonstrated that stable consensus can be achieved even under\nmixed-traffic conditions involving HVs without adequate functionalities to\ninteract with CAVs. Experimental results show that the proposed algorithm\nreached consensus at a typical unsignalized four-way, two-lane intersection in\napproximately 30-40 ms on average. A secondary vision-based system is employed\nto complete the crossing priorities based on the recognized lexicographical\norder of the license plate numbers in case the consensus procedure times out on\nan unreliable vehicle-to-vehicle communication network. The significance of\nthis study lies in its ability to improve traffic flow at unsignalized\nintersections by enabling rapid determination of passing priority through\ndistributed consensus even under mixed traffic with faulty vehicles."}
{"id": "2507.03867", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.03867", "abs": "https://arxiv.org/abs/2507.03867", "authors": ["Yu Xiang Zhu", "Amos Robinson", "Sophia Roshal", "Timothy Mou", "Julian Mackay", "Jonathan Aldrich", "Alex Potanin"], "title": "Semantically Separating Nominal Wyvern for Usability and Decidability", "comment": null, "summary": "The Dependent Object Types (DOT) calculus incorporates concepts from\nfunctional languages (e.g. modules) with traditional object-oriented features\n(e.g. objects, subtyping) to achieve greater expressivity (e.g. F-bounded\npolymorphism). However, this merger of paradigms comes at the cost of subtype\ndecidability. Recent work on bringing decidability to DOT has either sacrificed\nexpressiveness or ease of use. The unrestricted construction of recursive types\nand type bounds has made subtype decidability a much harder problem than in\ntraditional object-oriented programming.\n  Recognizing this, our paper introduces Nominal Wyvern, a DOT-like dependent\ntype system that takes an alternative approach: instead of having a uniform\nstructural syntax like DOT, Nominal Wyvern is designed around a \"semantic\nseparation\" between the nominal declaration of recursive types on the one hand,\nand the structural refinement of those types when they are used on the other.\nThis design naturally guides the user to avoid writing undecidably recursive\nstructural types.\n  From a technical standpoint, this separation also makes guaranteeing\ndecidability possible by allowing for an intuitive adaptation of material/shape\nseparation, a technique for achieving subtype decidability by separating types\nresponsible for subtyping constraints from types that represent concrete data.\nThe result is a type system with syntax and structure familiar to OOP users\nthat achieves decidability without compromising the expressiveness of F-bounded\npolymorphism and module systems as they are used in practice."}
{"id": "2507.03308", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.03308", "abs": "https://arxiv.org/abs/2507.03308", "authors": ["Jindong Li", "Tenglong Li", "Ruiqi Chen", "Guobin Shen", "Dongcheng Zhao", "Qian Zhang", "Yi Zeng"], "title": "Hummingbird: A Smaller and Faster Large Language Model Accelerator on Embedded FPGA", "comment": "Accepted by ICCAD2025", "summary": "Deploying large language models (LLMs) on embedded devices remains a\nsignificant research challenge due to the high computational and memory demands\nof LLMs and the limited hardware resources available in such environments.\nWhile embedded FPGAs have demonstrated performance and energy efficiency in\ntraditional deep neural networks, their potential for LLM inference remains\nlargely unexplored. Recent efforts to deploy LLMs on FPGAs have primarily\nrelied on large, expensive cloud-grade hardware and have only shown promising\nresults on relatively small LLMs, limiting their real-world applicability. In\nthis work, we present Hummingbird, a novel FPGA accelerator designed\nspecifically for LLM inference on embedded FPGAs. Hummingbird is smaller,\ntargeting embedded FPGAs such as the KV260 and ZCU104 with 67% LUT, 39% DSP,\nand 42% power savings over existing research. Hummingbird is stronger,\ntargeting LLaMA3-8B and supporting longer contexts, overcoming the typical 4GB\nmemory constraint of embedded FPGAs through offloading strategies. Finally,\nHummingbird is faste, achieving 4.8 tokens/s and 8.6 tokens/s for LLaMA3-8B on\nthe KV260 and ZCU104 respectively, with 93-94% model bandwidth utilization,\noutperforming the prior 4.9 token/s for LLaMA2-7B with 84% bandwidth\nutilization baseline. We further demonstrate the viability of industrial\napplications by deploying Hummingbird on a cost-optimized Spartan UltraScale\nFPGA, paving the way for affordable LLM solutions at the edge."}
{"id": "2507.03695", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.03695", "abs": "https://arxiv.org/abs/2507.03695", "authors": ["Mohsen Koohi Esfahani"], "title": "On Optimizing Resource Utilization in Distributed Connected Components", "comment": null, "summary": "Connected Components (CC) is a core graph problem with numerous applications.\nThis paper investigates accelerating distributed CC by optimizing memory and\nnetwork bandwidth utilization. We present two novel distributed CC algorithms,\nSiskinCC and RobinCC, which are built upon the Jayanti-Tarjan disjoint set\nunion algorithm. To optimize memory utilization, SiskinCC and RobinCC are\ndesigned to facilitate efficient access to a shared array for all cores running\nin a machine. This allows execution of faster algorithms with larger memory\nbounds. SiskinCC leverages the continuous inter-machine communication during\nthe computation phase to reduce the final communication overhead and RobinCC\nleverages the structural properties of real-world graphs to optimize network\nbandwidth utilization. Our evaluation against state-of-the-art CC algorithms,\nusing real-world and synthetic graphs with up to 500 billion edges and 11.7\nbillion vertices, and on up to 2048 CPU cores, demonstrates that SiskinCC and\nRobinCC achieve up to 58.5 times speedup."}
{"id": "2507.04298", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.04298", "abs": "https://arxiv.org/abs/2507.04298", "authors": ["Youngju Song", "Minki Cho"], "title": "CCR 2.0: High-level Reasoning for Conditional Refinements", "comment": null, "summary": "In recent years, great progress has been made in the field of formal\nverification for low-level systems. Many of them are based on one of two\npopular approaches: refinement or separation logic. These two approaches are\nvery different in nature and offer complementary benefits in terms of\ncompositionality. Recently, to fuse these benefits in a unified mechanism, a\nnew approach called Conditional Contextual Refinement (CCR 1.0 for short) was\nproposed. In this paper, we advance the model of CCR 1.0 and provide novel and\nintuitive reasoning principles, resulting in: CCR 2.0. Specifically, CCR 2.0\n(i) comes with a better compositionality theorem, having the practical benefit\nof facilitating more proof reuse, and (ii) provides a proof technique that\nhides model-level (i.e., resources of the separation logic) details from the\nuser. Achieving this goal was challenging due to non-trivial counterexamples\nwhich necessitated us to devise novel notions. Our results are formalized in\nCoq."}
{"id": "2507.03522", "categories": ["cs.AR", "cs.LG", "C.1.0"], "pdf": "https://arxiv.org/pdf/2507.03522", "abs": "https://arxiv.org/abs/2507.03522", "authors": ["Alexandre de Limas Santana", "AdriÃ  Armejach", "Francesc Martinez", "Erich Focht", "Marc Casas"], "title": "A Flexible Instruction Set Architecture for Efficient GEMMs", "comment": null, "summary": "GEneral Matrix Multiplications (GEMMs) are recurrent in high-performance\ncomputing and deep learning workloads. Typically, high-end CPUs accelerate GEMM\nworkloads with Single-Instruction Multiple Data (SIMD) or vector Instruction\nSet Architectures (ISAs). Since these ISAs face significant issues when running\nGEMM workloads, particularly when dealing with small, tall, or skinny matrices,\nmatrix ISAs have been proposed and implemented by major hardware vendors in the\nlast years. Although these matrix ISAs deliver larger throughput when running\nGEMMs than their SIMD/vector counterparts, they are rigid solutions unable to\ndynamically adapt themselves to application-specific aspects like the data\nformat. This paper demonstrates that the state-of-the-art matrix ISAs deliver\nsuboptimal performance when running the most commonly used convolution and\ntransformer models.\n  This paper proposes the Matrix Tile Extension (MTE), the first matrix ISA\nthat completely decouples the instruction set architecture from the\nmicroarchitecture and seamlessly interacts with existing vector ISAs. MTE\nincurs minimal implementation overhead since it only requires a few additional\ninstructions and a 64-bit Control Status Register (CSR) to keep its state.\nSpecifically, MTE can i) vectorize GEMMs across the three dimensions M, N, and\nK; ii) leverage the capacity of the existing vector register file; and iii)\ndecouple the tile shape from the underlying microarchitecture. MTE achieves\nspeed-ups of 1.35x over the best state-of-the-art matrix ISA."}
{"id": "2507.03849", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.03849", "abs": "https://arxiv.org/abs/2507.03849", "authors": ["Mai Zheng", "Duo Zhang", "Ahmed Dajani"], "title": "On Fault Tolerance of Data Storage Systems: A Holistic Perspective", "comment": null, "summary": "Data storage systems serve as the foundation of digital society. The enormous\ndata generated by people on a daily basis make the fault tolerance of data\nstorage systems increasingly important. Unfortunately, modern storage systems\nconsist of complicated hardware and software layers interacting with each\nother, which may contain latent bugs that elude extensive testing and lead to\ndata corruption, system downtime, or even unrecoverable data loss in practice.\nIn this chapter, we take a holistic view to introduce the typical architecture\nand major components of modern data storage systems (e.g., solid state drives,\npersistent memories, local file systems, and distributed storage management at\nscale). Next, we discuss a few representative bug detection and fault tolerance\ntechniques across layers with a focus on issues that affect system recovery and\ndata integrity. Finally, we conclude with open challenges and future work."}
{"id": "2507.04316", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.04316", "abs": "https://arxiv.org/abs/2507.04316", "authors": ["Jay Lee"], "title": "Retargeting an Abstract Interpreter for a New Language by Partial Evaluation", "comment": "Presented at the Student Research Competition (SRC) at PLDI 2025\n  (https://pldi25.sigplan.org/details/pldi-2025-src/1/)", "summary": "It is well-known that abstract interpreters can be systematically derived\nfrom their concrete counterparts using a \"recipe,\" but developing sound static\nanalyzers remains a time-consuming task. Reducing the effort required and\nmechanizing the process of developing analyzers continues to be a significant\nchallenge. Is it possible to automatically retarget an existing abstract\ninterpreter for a new language?\n  We propose a novel technique to automatically derive abstract interpreters\nfor various languages from an existing abstract interpreter. By leveraging\npartial evaluation, we specialize an abstract interpreter for a source\nlanguage. The specialization is performed using the semantics of target\nlanguages written in the source language. Our approach eliminates the need to\ndevelop analyzers for new targets from scratch. We show that our method can\neffectively retarget an abstract interpreter for one language into a correct\nanalyzer for another language."}
{"id": "2507.04276", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.04276", "abs": "https://arxiv.org/abs/2507.04276", "authors": ["Gwok-Waa Wan", "Shengchu Su", "Ruihu Wang", "Qixiang Chen", "Sam-Zaak Wong", "Mengnv Xing", "Hefei Feng", "Yubo Wang", "Yinan Zhu", "Jingyi Zhang", "Jianmin Ye", "Xinlai Wan", "Tao Ni", "Qiang Xu", "Nan Guan", "Zhe Jiang", "Xi Wang", "Yang Jun"], "title": "FIXME: Towards End-to-End Benchmarking of LLM-Aided Design Verification", "comment": null, "summary": "Despite the transformative potential of Large Language Models (LLMs) in\nhardware design, a comprehensive evaluation of their capabilities in design\nverification remains underexplored. Current efforts predominantly focus on RTL\ngeneration and basic debugging, overlooking the critical domain of functional\nverification, which is the primary bottleneck in modern design methodologies\ndue to the rapid escalation of hardware complexity. We present FIXME, the first\nend-to-end, multi-model, and open-source evaluation framework for assessing LLM\nperformance in hardware functional verification (FV) to address this crucial\ngap. FIXME introduces a structured three-level difficulty hierarchy spanning\nsix verification sub-domains and 180 diverse tasks, enabling in-depth analysis\nacross the design lifecycle. Leveraging a collaborative AI-human approach, we\nconstruct a high-quality dataset using 100% silicon-proven designs, ensuring\ncomprehensive coverage of real-world challenges. Furthermore, we enhance the\nfunctional coverage by 45.57% through expert-guided optimization. By rigorously\nevaluating state-of-the-art LLMs such as GPT-4, Claude3, and LlaMA3, we\nidentify key areas for improvement and outline promising research directions to\nunlock the full potential of LLM-driven automation in hardware design\nverification. The benchmark is available at\nhttps://github.com/ChatDesignVerification/FIXME."}
{"id": "2507.03952", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.03952", "abs": "https://arxiv.org/abs/2507.03952", "authors": ["Somayeh Sobati-M"], "title": "FedFog: Resource-Aware Federated Learning in Edge and Fog Networks", "comment": null, "summary": "As edge and fog computing become central to modern distributed systems,\nthere's growing interest in combining serverless architectures with\nprivacy-preserving machine learning techniques like federated learning (FL).\nHowever, current simulation tools fail to capture this integration effectively.\nIn this paper, we introduce FedFog, a simulation framework that extends the\nFogFaaS environment to support FL-aware serverless execution across edge-fog\ninfrastructures. FedFog incorporates an adaptive FL scheduler,\nprivacy-respecting data flow, and resource-aware orchestration to emulate\nrealistic, dynamic conditions in IoT-driven scenarios. Through extensive\nsimulations on benchmark datasets, we demonstrate that FedFog accelerates model\nconvergence, reduces latency, and improves energy efficiency compared to\nconventional FL or FaaS setups-making it a valuable tool for researchers\nexploring scalable, intelligent edge systems."}
{"id": "2507.05234", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.05234", "abs": "https://arxiv.org/abs/2507.05234", "authors": ["Jay Lee", "Joongwon Ahn", "Kwangkeun Yi"], "title": "React-tRace: A Semantics for Understanding React Hooks", "comment": "Conditionally accepted to OOPSLA 2025", "summary": "React has become the most widely used web front-end framework, enabling the\ncreation of user interfaces in a declarative and compositional manner. Hooks\nare a set of APIs that manage side effects in functional components in React.\nHowever, their semantics are often seen as opaque to developers, leading to UI\nbugs. In this paper, we formalize the semantics of the essence of React Hooks\nwe name React-tRace, providing a framework that clarifies their behavior. We\ndemonstrate that our model captures the behavior of React, by theoretically\nshowing that it embodies essential properties of Hooks and empirically\ncomparing our React-tRace-definitional interpreter against a test suite.\nFurthermore, we showcase a practical visualization tool based on the\nformalization to demonstrate how developers can better understand the semantics\nof Hooks."}
{"id": "2507.04315", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.04315", "abs": "https://arxiv.org/abs/2507.04315", "authors": ["Qingyun Zou", "Nuo Chen", "Yao Chen", "Bingsheng He", "WengFei Wong"], "title": "HLStrans: Dataset for LLM-Driven C-to-HLS Hardware Code Synthesis", "comment": null, "summary": "High-level synthesis (HLS) enables software developers to describe and\nimplement hardware at a higher level of abstraction by using C/C++ instead of\ntraditional hardware description languages to automatically generate FPGA-ready\ndesigns. However, generating HLS code significantly differs from standard\nC/C++: it disallows certain coding idioms, relies on specialized libraries, and\ncritically requires fine-grained transformations and the insertion of\noptimization directives (pragmas) to achieve high performance. Large language\nmodels (LLMs) have shown promise in automating such transformations, yet\nexisting open-source datasets lack sufficient complexity and optimization\ndiversity. To address this gap, we introduce the HLStrans dataset, a\ncomprehensive collection of 137 distinct real word programs, each annotated\nwith a variety of C-to-HLS transformations that yield over 23K labeled design\nvariants. These include a broad spectrum of pragmas and code-level\noptimizations. We benchmark state-of-the-art LLMs on this dataset to evaluate\ntheir ability to generate synthesizable, high-performance HLS code. As part of\nan ongoing effort, we plan to expand the HLStrans dataset in both scale and\nprogram variety, further empowering research at the intersection of AI and\nhardware synthesis."}
{"id": "2507.03973", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.03973", "abs": "https://arxiv.org/abs/2507.03973", "authors": ["Muhang Lan", "Song Xiao", "Wenyi Zhang"], "title": "One-Bit Model Aggregation for Differentially Private and Byzantine-Robust Personalized Federated Learning", "comment": null, "summary": "As the scale of federated learning (FL) systems expands, their inherent\nperformance limitations like communication overhead, Byzantine vulnerability,\nand privacy leakage have become increasingly critical. This paper considers a\npersonalized FL framework based on model regularization, and proposes a model\naggregation algorithm named PRoBit+ to concurrently overcome these limitations.\nPRoBit+ employs one-bit stochastic quantization and maximum likelihood\nestimation for parameter aggregation, and dynamically adjusts the step size of\nparameter updates, improving training stability of deep neural networks under\nlow communication overhead and heterogeneous data distributions. PRoBit+'s\nstatistical analysis is then conducted and its Byzantine robustness is proved.\nThe $(\\epsilon,0)$-differential privacy and a convergence upper bound of the\nPRoBit+ based FL are also theoretically established in heterogeneous contexts.\nThe analysis illustrates the trade-off among transmission accuracy, security\nguarantees, and convergence rates, and also indicates that the performance\ndegradation caused by transmission errors and privacy protection can be\nprogressively eliminated at a rate of $\\mathcal{O}(1/M)$ as the number of\nuploading clients $M$ increases. Comprehensive numerical experiments are\nconducted to assess PRoBit+ in comparison to benchmark methods across different\nByzantine attacks and varying proportions of malicious clients. The\nexperimental results demonstrate that PRoBit+ exhibits improved Byzantine\nrobustness over existing bit-based transmission schemes, minimal performance\ndegradation related to privacy protection, and nearly identical performance to\nfull-precision FedAvg in a secure environment."}
{"id": "2507.04535", "categories": ["cs.AR", "cs.LG", "hep-ex", "B.2.4; B.6"], "pdf": "https://arxiv.org/pdf/2507.04535", "abs": "https://arxiv.org/abs/2507.04535", "authors": ["Chang Sun", "Zhiqiang Que", "Vladimir Loncar", "Wayne Luk", "Maria Spiropulu"], "title": "da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs", "comment": null, "summary": "Neural networks with a latency requirement on the order of microseconds, like\nthe ones used at the CERN Large Hadron Collider, are typically deployed on\nFPGAs fully unrolled and pipelined. A bottleneck for the deployment of such\nneural networks is area utilization, which is directly related to the required\nconstant matrix-vector multiplication (CMVM) operations. In this work, we\npropose an efficient algorithm for implementing CMVM operations with\ndistributed arithmetic (DA) on FPGAs that simultaneously optimizes for area\nconsumption and latency. The algorithm achieves resource reduction similar to\nstate-of-the-art algorithms while being significantly faster to compute. The\nproposed algorithm is open-sourced and integrated into the \\texttt{hls4ml}\nlibrary, a free and open-source library for running real-time neural network\ninference on FPGAs. We show that the proposed algorithm can reduce on-chip\nresources by up to a third for realistic, highly quantized neural networks\nwhile simultaneously reducing latency, enabling the implementation of\npreviously infeasible networks."}
{"id": "2507.04172", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.04172", "abs": "https://arxiv.org/abs/2507.04172", "authors": ["Younan Gao", "Andrzej Pelc"], "title": "Gathering Teams of Bounded Memory Agents on a Line", "comment": null, "summary": "Several mobile agents, modelled as deterministic automata, navigate in an\ninfinite line in synchronous rounds. All agents start in the same round. In\neach round, an agent can move to one of the two neighboring nodes, or stay\nidle. Agents have distinct labels which are integers from the set $\\{1,\\dots,\nL\\}$. They start in teams, and all agents in a team have the same starting\nnode. The adversary decides the compositions of teams, and their starting\nnodes. Whenever an agent enters a node, it sees the entry port number and the\nstates of all collocated agents; this information forms the input of the agent\non the basis of which it transits to the next state and decides the current\naction. The aim is for all agents to gather at the same node and stop.\nGathering is feasible, if this task can be accomplished for any decisions of\nthe adversary, and its time is the worst-case number of rounds from the start\ntill gathering.\n  We consider the feasibility and time complexity of gathering teams of agents,\nand give a complete solution of this problem. It turns out that both\nfeasibility and complexity of gathering depend on the sizes of teams. We first\nconcentrate on the case when all teams have the same size $x$. For the oriented\nline, gathering is impossible if $x=1$, and it can be accomplished in time\n$O(D)$, for $x>1$, where $D$ is the distance between the starting nodes of the\nmost distant teams. This complexity is of course optimal. For the unoriented\nline, the situation is different. For $x=1$, gathering is also impossible, but\nfor $x=2$, the optimal time of gathering is $\\Theta(D\\log L)$, and for $x\\geq\n3$, the optimal time of gathering is $\\Theta(D)$. In the case when there are\nteams of different sizes, we show that gathering is always possible in time\n$O(D)$, even for the unoriented line. This complexity is of course optimal."}
{"id": "2507.04677", "categories": ["cs.AR", "B.7.1"], "pdf": "https://arxiv.org/pdf/2507.04677", "abs": "https://arxiv.org/abs/2507.04677", "authors": ["Siqing Fu", "Lizhou Wu", "Tiejun Li", "Chunyuan Zhang", "Sheng Ma", "Jianmin Zhang", "Yuhan Tang", "Jixuan Tang"], "title": "NeuroPDE: A Neuromorphic PDE Solver Based on Spintronic and Ferroelectric Devices", "comment": "9 pages, 12 figures, accepted at ICCAD 2025 (The 2025 IEEE/ACM\n  International Conference on Computer-Aided Design)", "summary": "In recent years, new methods for solving partial differential equations\n(PDEs) such as Monte Carlo random walk methods have gained considerable\nattention. However, due to the lack of hardware-intrinsic randomness in the\nconventional von Neumann architecture, the performance of PDE solvers is\nlimited. In this paper, we introduce NeuroPDE, a hardware design for\nneuromorphic PDE solvers that utilizes emerging spintronic and ferroelectric\ndevices. NeuroPDE incorporates spin neurons that are capable of probabilistic\ntransmission to emulate random walks, along with ferroelectric synapses that\nstore continuous weights non-volatilely. The proposed NeuroPDE achieves a\nvariance of less than 1e-2 compared to analytical solutions when solving\ndiffusion equations, demonstrating a performance advantage of 3.48x to 315x\nspeedup in execution time and an energy consumption advantage of 2.7x to 29.8x\nover advanced CMOS-based neuromorphic chips. By leveraging the inherent\nphysical stochasticity of emerging devices, this study paves the way for future\nprobabilistic neuromorphic computing systems."}
{"id": "2507.04357", "categories": ["cs.DC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.04357", "abs": "https://arxiv.org/abs/2507.04357", "authors": ["Zareh Chahoki Atefeh", "Roveri Marco"], "title": "Static Analysis for Detecting Transaction Conflicts in Ethereum Smart Contracts", "comment": null, "summary": "Ethereum smart contracts operate in a concurrent environment where multiple\ntransactions can be submitted simultaneously. However, the Ethereum Virtual\nMachine (EVM) enforces sequential execution of transactions within each block\nto prevent conflicts arising from concurrent access to the same state\nvariables. Although this approach guarantees correct behavior, it limits the\nability of validators to leverage multi-core architectures for faster\ntransaction processing, thus restricting throughput. Existing solutions\nintroduce concurrency by allowing simultaneous transaction execution combined\nwith runtime conflict detection and rollback mechanisms to maintain\ncorrectness. However, these methods incur significant overhead due to\ncontinuous conflict tracking and transaction reversion. Recently, alternative\napproaches have emerged that aim to predict conflicts statically, before\nexecution, by analyzing smart contract code for potential transaction\ninteractions. Despite their promise, there is a lack of comprehensive studies\nthat examine static conflict detection and its broader implications in specific\nsmart contracts. This paper fills this important gap by proposing a novel\nstatic analysis method to detect potential transaction conflicts in Ethereum\nsmart contracts. Our method identifies read-write, write-write, and function\ncall conflicts between transaction pairs by analyzing state variable access\npatterns in Solidity contracts. We implement a tool that parses contract code\nand performs conflict detection. Evaluation on a dataset of real-world Ethereum\nsmart contracts demonstrates that our approach achieves high precision in\nidentifying potential conflicts. By enabling proactive conflict detection, our\ntool supports further design of transaction scheduling strategies that reduce\nruntime failures, enhance validator throughput, and contribute to blockchain\nscalability."}
{"id": "2507.04772", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.04772", "abs": "https://arxiv.org/abs/2507.04772", "authors": ["Seock-Hwan Noh", "Sungju Kim", "Seohyun Kim", "Daehoon Kim", "Jaeha Kung", "Yeseong Kim"], "title": "Jack Unit: An Area- and Energy-Efficient Multiply-Accumulate (MAC) Unit Supporting Diverse Data Formats", "comment": "Accepted for publication at the 30th ACM/IEEE International Symposium\n  on Low Power Electronics and Design (ISLPED 2025)", "summary": "In this work, we introduce an area- and energy-efficient multiply-accumulate\n(MAC) unit, named Jack unit, that is a jack-of-all-trades, supporting various\ndata formats such as integer (INT), floating point (FP), and microscaling data\nformat (MX). It provides bit-level flexibility and enhances hardware efficiency\nby i) replacing the carry-save multiplier (CSM) in the FP multiplier with a\nprecision-scalable CSM, ii) performing the adjustment of significands based on\nthe exponent differences within the CSM, and iii) utilizing 2D sub-word\nparallelism. To assess effectiveness, we implemented the layout of the Jack\nunit and three baseline MAC units. Additionally, we designed an AI accelerator\nequipped with our Jack units to compare with a state-of-the-art AI accelerator\nsupporting various data formats. The proposed MAC unit occupies 1.17~2.01x\nsmaller area and consumes 1.05~1.84x lower power compared to the baseline MAC\nunits. On five AI benchmarks, the accelerator designed with our Jack units\nimproves energy efficiency by 1.32~5.41x over the baseline across various data\nformats."}
{"id": "2507.04420", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.04420", "abs": "https://arxiv.org/abs/2507.04420", "authors": ["Mohsen Koohi Esfahani"], "title": "Skipper: Maximal Matching with a Single Pass over Edges", "comment": null, "summary": "Maximal Matching (MM) is a fundamental graph problem with diverse\napplications. However, state-of-the-art parallel MM algorithms are limited by\ntheir need to process graph edges repeatedly over multiple iterations.\nFurthermore, optimized algorithms often require additional memory for graph\ncontraction or edge filtering. In this paper, we introduce Skipper, an\nincremental asynchronous MM algorithm that (i) processes each edge\ndeterministically and only once, (ii) skips a large fraction of edges during\nprocessing, and (iii) minimizes memory space utilization. Notably, Skipper\nrequires (a) a single pass over the edges, and (b) only a single byte of memory\nspace per vertex. Our evaluation of Skipper, using both real-world and\nsynthetic graphs with up to 161 billion edges, and across three different\ncomputer architectures, shows that Skipper processes only 1.2% of the edges and\ndelivers a 47.1 times average speedup (geometric mean). Moreover, Skipper's\noutput quality is highly competitive, with an average size of 88.6% relative to\nthe output of the Lim-Chung algorithm as a state-of-the-art MM algorithm with\nthe largest output size."}
{"id": "2507.05012", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.05012", "abs": "https://arxiv.org/abs/2507.05012", "authors": ["Samuel Riedel", "Yichao Zhang", "Marco Bertuletti", "Luca Benini"], "title": "Optimizing Scalable Multi-Cluster Architectures for Next-Generation Wireless Sensing and Communication", "comment": "6 pages, 8 figures, accepted at IWASI 2025", "summary": "Next-generation wireless technologies (for immersive-massive communication,\njoint communication and sensing) demand highly parallel architectures for\nmassive data processing. A common architectural template scales up by grouping\ntens to hundreds of cores into shared-memory clusters, which are then scaled\nout as multi-cluster manycore systems. This hierarchical design, used in GPUs\nand accelerators, requires a balancing act between fewer large clusters and\nmore smaller clusters, affecting design complexity, synchronization,\ncommunication efficiency, and programmability. While all multi-cluster\narchitectures must balance these trade-offs, there is limited insight into\noptimal cluster sizes. This paper analyzes various cluster configurations,\nfocusing on synchronization, data movement overhead, and programmability for\ntypical wireless sensing and communication workloads. We extend the open-source\nshared-memory cluster MemPool into a multi-cluster architecture and propose a\nnovel double-buffering barrier that decouples processor and DMA. Our results\nshow a single 256-core cluster can be twice as fast as 16 16-core clusters for\nmemory-bound kernels and up to 24% faster for compute-bound kernels due to\nreduced synchronization and communication overheads."}
{"id": "2507.04459", "categories": ["cs.DC", "cs.DS", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04459", "abs": "https://arxiv.org/abs/2507.04459", "authors": ["Ajay D. Kshemkalyani", "Manish Kumar", "Anisur Rahaman Molla", "Gokarna Sharma"], "title": "Agentic Distributed Computing", "comment": "42 pages, 3 figures,3 tables, 8 pseudocodes; some overlaps with\n  arXiv:2403.13716v2", "summary": "The most celebrated and extensively studied model of distributed computing is\nthe {\\em message-passing model,} in which each vertex/node of the (distributed\nnetwork) graph corresponds to a static computational device that communicates\nwith other devices through passing messages. In this paper, we consider the\n{\\em agentic model} of distributed computing which extends the message-passing\nmodel in a new direction. In the agentic model, computational devices are\nmodeled as relocatable or mobile computational devices (called agents in this\npaper), i.e., each vertex/node of the graph serves as a container for the\ndevices, and hence communicating with another device requires relocating to the\nsame node. We study two fundamental graph level tasks, leader election, and\nminimum spanning tree, in the agentic model, which will enhance our\nunderstanding of distributed computation across paradigms. The objective is to\nminimize both time and memory complexities. Following the literature, we\nconsider the synchronous setting in which each agent performs its operations\nsynchronously with others, and hence the time complexity can be measured in\nrounds. In this paper, we present two deterministic algorithms for leader\nelection: one for the case of $k<n$ and another for the case of $k=n$,\nminimizing both time and memory complexities, where $k$ and $n$, respectively,\nare the number of agents and number of nodes of the graph. Using these leader\nelection results, we develop deterministic algorithms for agents to construct a\nminimum spanning tree of the graph, minimizing both time and memory\ncomplexities. To the best of our knowledge, this is the first study of\ndistributed graph level tasks in the agentic model with $k\\leq n$. Previous\nstudies only considered the case of $k=n$."}
{"id": "2507.05081", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.05081", "abs": "https://arxiv.org/abs/2507.05081", "authors": ["Xin Li", "Mianxin Xiao", "Xi Shen", "Jiaqing Chu", "Weifeng Huang", "Jiashun Li", "Yaoyi Li", "Mingjing Cai", "Jiaming Chen", "Xinming Zhang", "Daxing Zhang", "Congsi Wang", "Hong Tang", "Bao Zhao", "Qitao Lu", "Yilong Wang", "Jianjun Wang", "Minyi Xu", "Shitong Fang", "Xuanyu Huang. Chaoyang Zhao", "Zicheng Liu", "Yaowen Yang", "Guobiao Hu", "Junrui Liang", "Wei-Hsin Liao"], "title": "ViPSN 2.0: A Reconfigurable Battery-free IoT Platform for Vibration Energy Harvesting", "comment": null, "summary": "Vibration energy harvesting is a promising solution for powering battery-free\nIoT systems; however, the instability of ambient vibrations presents\nsignificant challenges, such as limited harvested energy, intermittent power\nsupply, and poor adaptability to various applications. To address these\nchallenges, this paper proposes ViPSN2.0, a modular and reconfigurable IoT\nplatform that supports multiple vibration energy harvesters (piezoelectric,\nelectromagnetic, and triboelectric) and accommodates sensing tasks with varying\napplication requirements through standardized hot-swappable interfaces.\nViPSN~2.0 incorporates an energy-indication power management framework tailored\nto various application demands, including light-duty discrete sampling,\nheavy-duty high-power sensing, and complex-duty streaming tasks, thereby\neffectively managing fluctuating energy availability. The platform's\nversatility and robustness are validated through three representative\napplications: ViPSN-Beacon, enabling ultra-low-power wireless beacon\ntransmission from a single transient fingertip press; ViPSN-LoRa, supporting\nhigh-power, long-range wireless communication powered by wave vibrations in\nactual marine environments; and ViPSN-Cam, enabling intermittent image capture\nand wireless transfer. Experimental results demonstrate that ViPSN~2.0 can\nreliably meet a wide range of requirements in practical battery-free IoT\ndeployments under energy-constrained conditions."}
{"id": "2507.04647", "categories": ["cs.DC", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.04647", "abs": "https://arxiv.org/abs/2507.04647", "authors": ["Faveo Hoerold", "Ivan R. Ivanov", "Akash Dhruv", "William S. Moses", "Anshu Dubey", "Mohamed Wahib", "Jens Domke"], "title": "RAPTOR: Practical Numerical Profiling of Scientific Applications", "comment": "12 pages, 8 figures, to be published in SC'25", "summary": "The proliferation of low-precision units in modern high-performance\narchitectures increasingly burdens domain scientists. Historically, the choice\nin HPC was easy: can we get away with 32 bit floating-point operations and\nlower bandwidth requirements, or is FP64 necessary? Driven by Artificial\nIntelligence, vendors introduced novel low-precision units for vector and\ntensor operations, and FP64 capabilities stagnate or are reduced. This is\nforcing scientists to re-evaluate their codes, but a trivial search-and-replace\napproach to go from FP64 to FP16 will not suffice. We introduce RAPTOR: a\nnumerical profiling tool to guide scientists in their search for code regions\nwhere precision lowering is feasible. Using LLVM, we transparently replace\nhigh-precision computations using low-precision units, or emulate a\nuser-defined precision. RAPTOR is a novel, feature-rich approach -- with focus\non ease of use -- to change, profile, and reason about numerical requirements\nand instabilities, which we demonstrate with four real-world multi-physics\nFlash-X applications."}
{"id": "2507.02871", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.02871", "abs": "https://arxiv.org/abs/2507.02871", "authors": ["Kia Silverbrook"], "title": "ZettaLith: An Architectural Exploration of Extreme-Scale AI Inference Acceleration", "comment": "53 pages, 15 figures, 23 tables", "summary": "The high computational cost and power consumption of current and anticipated\nAI systems present a major challenge for widespread deployment and further\nscaling. Current hardware approaches face fundamental efficiency limits. This\npaper introduces ZettaLith, a scalable computing architecture designed to\nreduce the cost and power of AI inference by over 1,000x compared to current\nGPU-based systems. Based on architectural analysis and technology projections,\na single ZettaLith rack could potentially achieve 1.507 zettaFLOPS in 2027 -\nrepresenting a theoretical 1,047x improvement in inference performance, 1,490x\nbetter power efficiency, and could be 2,325x more cost-effective than current\nleading GPU racks for FP4 transformer inference. The ZettaLith architecture\nachieves these gains by abandoning general purpose GPU applications, and via\nthe multiplicative effect of numerous co-designed architectural innovations\nusing established digital electronic technologies, as detailed in this paper.\nZettaLith's core architectural principles scale down efficiently to exaFLOPS\ndesktop systems and petaFLOPS mobile chips, maintaining their roughly 1,000x\nadvantage. ZettaLith presents a simpler system architecture compared to the\ncomplex hierarchy of current GPU clusters. ZettaLith is optimized exclusively\nfor AI inference and is not applicable for AI training."}
{"id": "2507.04785", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.04785", "abs": "https://arxiv.org/abs/2507.04785", "authors": ["Jesper Larsson TrÃ¤ff"], "title": "Communication Round and Computation Efficient Exclusive Prefix-Sums Algorithms (for MPI_Exscan)", "comment": null, "summary": "Parallel scan primitives compute element-wise inclusive or exclusive prefix\nsums of input vectors contributed by $p$ consecutively ranked processors under\nan associative, binary operator $\\oplus$. In message-passing systems with\nbounded, one-ported communication capabilities, at least $\\lceil\\log_2 p\\rceil$\nor $\\lceil\\log_2 (p-1)\\rceil$ communication rounds are required to perform the\nscans. While there are well-known, simple algorithms for the inclusive scan\nthat solve the problem in $\\lceil\\log_2 p\\rceil$ communication rounds with\n$\\lceil\\log_2 p\\rceil$ applications of $\\oplus$ (which could be expensive), the\nexclusive scan appears more difficult. Conventionally, the problem is solved\nwith either $\\lceil\\log_2 (p-1)\\rceil+1$ communication rounds (e.g., by\nshifting the input vectors), or in $\\lceil\\log_2 p\\rceil$ communication rounds\nwith $2\\lceil\\log_2 p\\rceil-1$ applications of $\\oplus$ (by a modified\ninclusive scan algorithm). We give a new, simple algorithm that computes the\nexclusive prefix sums in $q=\\lceil\\log_2 (p-1)+\\log_2\\frac{4}{3}\\rceil$\nsimultaneous send-receive communication rounds with $q-1$ applications of\n$\\oplus$. We compare the three algorithms implemented in MPI against the MPI\nlibrary native MPI\\_Exscan primitive on a small, $36$-node cluster with a\nstate-of-the-art MPI library, indicating possible and worthwhile improvements\nto standard implementations. The algorithms assume input vectors to be small so\nthat performance is dominated by the number of communication rounds. For large\ninput vectors, other (pipelined, fixed-degree tree) algorithms must be used."}
{"id": "2507.04786", "categories": ["cs.DC", "C.2"], "pdf": "https://arxiv.org/pdf/2507.04786", "abs": "https://arxiv.org/abs/2507.04786", "authors": ["Zhiyi Hu", "Siyuan Shen", "Tommaso Bonato", "Sylvain Jeaugey", "Cedell Alexander", "Eric Spada", "Jeff Hammond", "Torsten Hoefler"], "title": "Demystifying NCCL: An In-depth Analysis of GPU Communication Protocols and Algorithms", "comment": null, "summary": "The NVIDIA Collective Communication Library (NCCL) is a critical software\nlayer enabling high-performance collectives on large-scale GPU clusters.\nDespite being open source with a documented API, its internal design remains\nlargely opaque. The orchestration of communication channels, selection of\nprotocols, and handling of memory movement across devices and nodes are not\nwell understood, making it difficult to analyze performance or identify\nbottlenecks. This paper presents a comprehensive analysis of NCCL, focusing on\nits communication protocol variants (Simple, LL, and LL128), mechanisms\ngoverning intra-node and inter-node data movement, and ring- and tree-based\ncollective communication algorithms. The insights obtained from this study\nserve as the foundation for ATLAHS, an application-trace-driven network\nsimulation toolchain capable of accurately reproducing NCCL communication\npatterns in large-scale AI training workloads. By demystifying NCCL's internal\narchitecture, this work provides guidance for system researchers and\nperformance engineers working to optimize or simulate collective communication\nat scale."}
{"id": "2507.04960", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2507.04960", "abs": "https://arxiv.org/abs/2507.04960", "authors": ["Marthe Bonamy", "Cyril Gavoille", "TimothÃ© Picavet", "Alexandra Wesolek"], "title": "Distributed Approximation Algorithms for Minimum Dominating Set in Locally Nice Graphs", "comment": null, "summary": "We give a new, short proof that graphs embeddable in a given Euler genus-$g$\nsurface admit a simple $f(g)$-round $\\alpha$-approximation distributed\nalgorithm for Minimum Dominating Set (MDS), where the approximation ratio\n$\\alpha \\le 906$. Using tricks from Heydt et al. [European Journal of\nCombinatorics (2025)], we in fact derive that $\\alpha \\le 34 +\\varepsilon$,\ntherefore improving upon the current state of the art of $24g+O(1)$ due to\nAmiri et al. [ACM Transactions on Algorithms (2019)]. It also improves the\napproximation ratio of $91+\\varepsilon$ due to Czygrinow et al. [Theoretical\nComputer Science (2019)] in the particular case of orientable surfaces.\n  All our distributed algorithms work in the deterministic LOCAL model. They do\nnot require any preliminary embedding of the graph and only rely on two things:\na LOCAL algorithm for MDS on planar graphs with ``uniform'' approximation\nguarantees and the knowledge that graphs embeddable in bounded Euler genus\nsurfaces have asymptotic dimension $2$.\n  More generally, our algorithms work in any graph class of bounded asymptotic\ndimension where ``most vertices'' are locally in a graph class that admits a\nLOCAL algorithm for MDS with uniform approximation guarantees."}
{"id": "2507.04969", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.04969", "abs": "https://arxiv.org/abs/2507.04969", "authors": ["Chanh Nguyen", "Erik Elmroth", "Monowar Bhuyan"], "title": "Silent Failures in Stateless Systems: Rethinking Anomaly Detection for Serverless Computing", "comment": "12 pages, 6 figures, IEEE CISOSE 2025", "summary": "Serverless computing has redefined cloud application deployment by\nabstracting infrastructure and enabling on-demand, event-driven execution,\nthereby enhancing developer agility and scalability. However, maintaining\nconsistent application performance in serverless environments remains a\nsignificant challenge. The dynamic and transient nature of serverless functions\nmakes it difficult to distinguish between benign and anomalous behavior, which\nin turn undermines the effectiveness of traditional anomaly detection methods.\nThese conventional approaches, designed for stateful and long-running services,\nstruggle in serverless settings where executions are short-lived, functions are\nisolated, and observability is limited.\n  In this first comprehensive vision paper on anomaly detection for serverless\nsystems, we systematically explore the unique challenges posed by this\nparadigm, including the absence of persistent state, inconsistent monitoring\ngranularity, and the difficulty of correlating behaviors across distributed\nfunctions. We further examine a range of threats that manifest as anomalies,\nfrom classical Denial-of-Service (DoS) attacks to serverless-specific threats\nsuch as Denial-of-Wallet (DoW) and cold start amplification. Building on these\nobservations, we articulate a research agenda for next-generation detection\nframeworks that address the need for context-aware, multi-source data fusion,\nreal-time, lightweight, privacy-preserving, and edge-cloud adaptive\ncapabilities.\n  Through the identification of key research directions and design principles,\nwe aim to lay the foundation for the next generation of anomaly detection in\ncloud-native, serverless ecosystems."}
{"id": "2507.05043", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.05043", "abs": "https://arxiv.org/abs/2507.05043", "authors": ["Lewei Jin", "Yongqi Chen", "Kui Zhang", "Yifan Zhuo", "Yi Gao", "Bowei Yang", "Zhengong Cai", "Wei Dong"], "title": "MoLink: Distributed and Efficient Serving Framework for Large Models", "comment": null, "summary": "Large language models represent a groundbreaking shift in generative AI. Yet,\nthese advances come with a significant challenge: the high cost of model\nserving. To mitigate these costs, consumer-grade GPUs emerge as a more\naffordable alternative. This presents an opportunity for more cost-efficient\nLLM serving by leveraging these GPUs.\n  However, it is non-trivial to achieve high-efficiency LLM serving on\nconsumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often\ndeployed in limited network conditions; 2) these GPUs often exhibit\nheterogeneity in host systems. To address these challenges, we present MoLink,\na distributed LLM serving system for large models. It incorporates several key\ntechniques, enabling efficient LLM serving on heterogeneous and weakly\nconnected consumer-grade GPUs. Our experiments demonstrate that it achieves\nthroughput improvements of up to 458\\% and cost-profit margin improvements of\nup to 151\\%, compared to state-of-the-art systems. MoLink allows users on\nWindows, Linux, and containerized VMs to seamlessly integrate GPUs with just a\nfew lines of code over Ethernet or public networks. Currently, it supports 18\nmainstream architectures of open-source large language models."}
{"id": "2507.05230", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.05230", "abs": "https://arxiv.org/abs/2507.05230", "authors": ["Shudi Weng", "Ming Xiao", "Chao Ren", "Mikael Skoglund"], "title": "Cooperative Gradient Coding", "comment": null, "summary": "This work studies gradient coding (GC) in the context of distributed training\nproblems with unreliable communication. We propose cooperative GC (CoGC), a\nnovel gradient-sharing-based GC framework that leverages cooperative\ncommunication among clients. This approach ultimately eliminates the need for\ndataset replication, making it both communication- and computation-efficient\nand suitable for federated learning (FL). By employing the standard GC decoding\nmechanism, CoGC yields strictly binary outcomes: either the global model is\nexactly recovered, or the decoding fails entirely, with no intermediate\nresults. This characteristic ensures the optimality of the training and\ndemonstrates strong resilience to client-to-server communication failures when\nthe communication channels among clients are in good condition. However, it may\nalso result in communication inefficiency and hinder convergence due to its\nlack of flexibility, especially when communication channels among clients are\nin poor condition. To overcome this limitation and further harness the\npotential of GC matrices, we propose a complementary decoding mechanism, termed\nGC$^+$, which leverages information that would otherwise be discarded during GC\ndecoding failures. This approach significantly improves system reliability\nunder unreliable communication, as the full recovery of the global model\ntypically dominates in GC$^+$. To conclude, this work establishes solid\ntheoretical frameworks for both CoGC and GC$^+$. We provide complete outage\nanalyses for each decoding mechanism, along with a rigorous investigation of\nhow outages affect the structure and performance of GC matrices. Building on\nthese analyses, we derive convergence bounds for both decoding mechanisms.\nFinally, the effectiveness of CoGC and GC$^+$ is validated through extensive\nsimulations."}
