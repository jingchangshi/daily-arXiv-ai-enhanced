{"id": "2511.00403", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.00403", "abs": "https://arxiv.org/abs/2511.00403", "authors": ["Wentao Peng", "Ruyi Ji", "Yingfei Xiong"], "title": "Equality Saturation Guided by Large Language Models", "comment": "presented at EGRAPHS 2025", "summary": "One critical issue with large language models (LLMs) is their inability to\nguarantee correctness. Although this problem can be addressed by applying LLMs\nto formal rewrite systems, current LLMs are still far from adequate to generate\nsound rewrite chains. To bridge this gap, this paper proposes LLM-guided\nequality saturation, dubbed LGuess, by incorporating e-graphs as an\nintermediate layer between LLMs and rewrite systems. LGuess queries LLMs only\nfor high-level rewrite checkpoints and uses e-graphs to supply low-level\nrewrite chains between these checkpoints. The key technical challenge in this\nprocedure lies in effectively extracting a suitable checkpoint from a saturated\ne-graph, which LGuess addresses by learning a probabilistic model from the LLM.\nThe model predicts probable checkpoints while remaining simple enough for\neffective extraction. We implement a prototype of LGuess and evaluate it on the\nproblem of factorizing multivariable polynomials. The results demonstrate a\nsignificant advantage of LGuess compared to both straightforward equality\nsaturation and the approach that queries the LLM directly for the rewrite\nchain.", "AI": {"tldr": "LGuess\u901a\u8fc7\u5c06e-graphs\u4f5c\u4e3aLLMs\u548c\u91cd\u5199\u7cfb\u7edf\u4e4b\u95f4\u7684\u4e2d\u95f4\u5c42\uff0c\u89e3\u51b3\u4e86LLMs\u65e0\u6cd5\u4fdd\u8bc1\u6b63\u786e\u6027\u7684\u95ee\u9898\u3002\u5b83\u53ea\u5411LLM\u67e5\u8be2\u9ad8\u5c42\u6b21\u7684\u91cd\u5199\u68c0\u67e5\u70b9\uff0c\u5e76\u4f7f\u7528e-graphs\u63d0\u4f9b\u8fd9\u4e9b\u68c0\u67e5\u70b9\u4e4b\u95f4\u7684\u4f4e\u5c42\u6b21\u91cd\u5199\u94fe\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u4fdd\u8bc1\u6b63\u786e\u6027\uff0c\u867d\u7136\u53ef\u4ee5\u5c06\u5176\u5e94\u7528\u4e8e\u5f62\u5f0f\u91cd\u5199\u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u5f53\u524dLLMs\u5728\u751f\u6210\u53ef\u9760\u91cd\u5199\u94fe\u65b9\u9762\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u63d0\u51faLLM\u5f15\u5bfc\u7684\u7b49\u5f0f\u9971\u548c\u65b9\u6cd5LGuess\uff0c\u901a\u8fc7e-graphs\u4f5c\u4e3a\u4e2d\u95f4\u5c42\uff0c\u5b66\u4e60LLM\u7684\u6982\u7387\u6a21\u578b\u6765\u9884\u6d4b\u53ef\u80fd\u7684\u68c0\u67e5\u70b9\uff0c\u4ece\u9971\u548ce-graph\u4e2d\u6709\u6548\u63d0\u53d6\u5408\u9002\u7684\u68c0\u67e5\u70b9\u3002", "result": "\u5728\u591a\u53d8\u91cf\u591a\u9879\u5f0f\u56e0\u5f0f\u5206\u89e3\u95ee\u9898\u4e0a\uff0cLGuess\u76f8\u6bd4\u76f4\u63a5\u7b49\u5f0f\u9971\u548c\u548c\u76f4\u63a5\u67e5\u8be2LLM\u91cd\u5199\u94fe\u7684\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "LGuess\u901a\u8fc7\u7ed3\u5408LLMs\u548ce-graphs\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u5f62\u5f0f\u91cd\u5199\u7cfb\u7edf\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5728\u591a\u9879\u5f0f\u56e0\u5f0f\u5206\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.00488", "categories": ["cs.PL", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00488", "abs": "https://arxiv.org/abs/2511.00488", "authors": ["Jun Gao", "Yun Peng", "Xiaoxue Ren"], "title": "\\texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress in\ncode-related tasks. Despite their advancement, empirical evidence reveals that\nthey still struggle with \\emph{deductive code reasoning}, the ability to reason\nabout the program execution process. While prior studies have recognized this\nlimitation, the underlying causes remain largely underexplored. In this paper,\nwe begin by presenting a comprehensive empirical study that reveals three key\nchallenges undermining deductive code reasoning: (1) an intrinsic gap between\ngeneration and reasoning abilities, (2) a consistent bias towards code sources,\nand (3) weak zero-shot generalization on complex benchmarks. In light of these\nchallenges, we propose \\texttt{ReMind}, a multi-agent framework composed of\n\\texttt{Mutator}, \\texttt{Executor}, and \\texttt{Inspector}. The\n\\texttt{Mutator} generates code variants to mitigate bias towards code sources,\nthe \\texttt{Executor} traces variable states step-by-step to expose\ninconsistency, and the \\texttt{Inspector} identifies problematic reasoning\nsteps and provides control-flow refinement to bridge the intrinsic reasoning\ngap. Through their coordinated collaboration, \\texttt{ReMind} systematically\nidentifies and refines reasoning flaws, achieving outstanding performance and\nenabling robust zero-shot generalization. Extensive experiments on two\nbenchmarks with five LLMs demonstrate the superior advantages of\n\\texttt{ReMind} compared to baseline approaches in deductive code reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ReMind\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7Mutator\u3001Executor\u548cInspector\u7684\u534f\u4f5c\u6765\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6f14\u7ece\u4ee3\u7801\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u548c\u96f6\u6837\u672c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\u5b83\u4eec\u5728\u6f14\u7ece\u4ee3\u7801\u63a8\u7406\uff08\u7406\u89e3\u7a0b\u5e8f\u6267\u884c\u8fc7\u7a0b\uff09\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u4e14\u6839\u672c\u539f\u56e0\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faReMind\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1aMutator\u751f\u6210\u4ee3\u7801\u53d8\u4f53\u4ee5\u51cf\u5c11\u5bf9\u4ee3\u7801\u6e90\u7684\u504f\u89c1\uff1bExecutor\u9010\u6b65\u8ffd\u8e2a\u53d8\u91cf\u72b6\u6001\u4ee5\u66b4\u9732\u4e0d\u4e00\u81f4\u6027\uff1bInspector\u8bc6\u522b\u6709\u95ee\u9898\u7684\u63a8\u7406\u6b65\u9aa4\u5e76\u63d0\u4f9b\u63a7\u5236\u6d41\u4f18\u5316\u3002\u4e09\u8005\u534f\u540c\u5de5\u4f5c\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u548c\u4f18\u5316\u63a8\u7406\u7f3a\u9677\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e94\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cReMind\u5728\u6f14\u7ece\u4ee3\u7801\u63a8\u7406\u65b9\u9762\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u51fa\u8272\u7684\u6027\u80fd\u548c\u9c81\u68d2\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002", "conclusion": "ReMind\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6f14\u7ece\u4ee3\u7801\u63a8\u7406\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff0c\u4e3a\u63d0\u5347\u4ee3\u7801\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00592", "categories": ["cs.PL", "cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.00592", "abs": "https://arxiv.org/abs/2511.00592", "authors": ["Massinissa Merouani", "Islem Kara Bernou", "Riyadh Baghdadi"], "title": "Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization", "comment": "Accepted at the 34th International Conference on Parallel\n  Architectures and Compilation Techniques (PACT 2025). 12 pages, plus appendix", "summary": "Automatic code optimization remains a difficult challenge, particularly for\ncomplex loop nests on modern hardware. This paper investigates a novel approach\nto code optimization where Large Language Models (LLMs) guide the process\nthrough a closed-loop interaction with a compiler. We present ComPilot, an\nexperimental framework that leverages off-the-shelf LLMs, without any\ntask-specific fine-tuning, as interactive optimization agents. ComPilot\nestablishes a feedback loop where an LLM proposes transformations for a given\nloop nest to a compiler. The compiler attempts the transformations, reporting\nback legality status and measured speedup or slowdown. The LLM utilizes this\nconcrete feedback to iteratively refine its optimization strategy. Our\nextensive evaluation across the PolyBench benchmark suite demonstrates the\neffectiveness of this zero-shot approach. ComPilot achieves geometric mean\nspeedups of 2.66x (single run) and 3.54x (best-of-5 runs) over the original\ncode. Furthermore, ComPilot demonstrates competitive performance against the\nstate-of-the-art Pluto polyhedral optimizer, outperforming it in many cases.\nThis experimental study demonstrates that general-purpose LLMs can effectively\nguide the code optimization process when grounded by compiler feedback, opening\npromising research directions for agentic AI in code optimization.", "AI": {"tldr": "ComPilot\u662f\u4e00\u4e2a\u5229\u7528\u73b0\u6210LLM\u4f5c\u4e3a\u4ea4\u4e92\u5f0f\u4f18\u5316\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e0e\u7f16\u8bd1\u5668\u5efa\u7acb\u53cd\u9988\u5faa\u73af\u6765\u4f18\u5316\u4ee3\u7801\uff0c\u5728PolyBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e862.66-3.54\u500d\u7684\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u3002", "motivation": "\u73b0\u4ee3\u786c\u4ef6\u4e0a\u590d\u6742\u5faa\u73af\u5d4c\u5957\u7684\u81ea\u52a8\u4ee3\u7801\u4f18\u5316\u4ecd\u7136\u662f\u4e00\u4e2a\u56f0\u96be\u6311\u6218\uff0c\u9700\u8981\u63a2\u7d22\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u73b0\u6210\u7684LLM\u4f5c\u4e3a\u4ea4\u4e92\u5f0f\u4f18\u5316\u4ee3\u7406\uff0c\u4e0e\u7f16\u8bd1\u5668\u5efa\u7acb\u53cd\u9988\u5faa\u73af\uff1aLLM\u63d0\u51fa\u8f6c\u6362\u5efa\u8bae\uff0c\u7f16\u8bd1\u5668\u5c1d\u8bd5\u8f6c\u6362\u5e76\u53cd\u9988\u5408\u6cd5\u6027\u548c\u6027\u80fd\u6570\u636e\uff0cLLM\u636e\u6b64\u8fed\u4ee3\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728PolyBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cComPilot\u5b9e\u73b0\u4e86\u5355\u6b21\u8fd0\u884c2.66\u500d\u548c\u6700\u4f735\u6b21\u8fd0\u884c3.54\u500d\u7684\u51e0\u4f55\u5e73\u5747\u52a0\u901f\uff0c\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684Pluto\u591a\u9762\u4f53\u4f18\u5316\u5668\u3002", "conclusion": "\u901a\u7528LLM\u5728\u7f16\u8bd1\u5668\u53cd\u9988\u7684\u6307\u5bfc\u4e0b\u53ef\u4ee5\u6709\u6548\u5f15\u5bfc\u4ee3\u7801\u4f18\u5316\u8fc7\u7a0b\uff0c\u4e3a\u4ee3\u7801\u4f18\u5316\u4e2d\u7684\u667a\u80fd\u4ee3\u7406AI\u5f00\u8f9f\u4e86\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.00740", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.00740", "abs": "https://arxiv.org/abs/2511.00740", "authors": ["Igor Engel", "Ekaterina Verbitskaia"], "title": "Typed Embedding of miniKanren for Functional Conversion", "comment": null, "summary": "Relational programming enables program synthesis through a verifier-to-solver\napproach. An earlier paper introduced a functional conversion that mitigated\nsome of the inherent performance overhead. However, the conversion was\ninelegant: it was oblivious to types, demanded determinism annotations, and\nimplicit generator threading. In this paper, we address these issues by\nproviding a typed tagless-final embedding of miniKanren into Haskell. This\nimprovement significantly reduces boilerplate while preserving, and sometimes\nenhancing, earlier speedups.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06miniKanren\u5d4c\u5165Haskell\u7684\u7c7b\u578b\u5316\u65e0\u6807\u7b7e\u6700\u7ec8\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4e4b\u524d\u8f6c\u6362\u65b9\u6cd5\u4e2d\u7684\u7c7b\u578b\u4e0d\u654f\u611f\u3001\u9700\u8981\u786e\u5b9a\u6027\u6807\u6ce8\u548c\u9690\u5f0f\u751f\u6210\u5668\u7ebf\u7a0b\u7b49\u95ee\u9898\u3002", "motivation": "\u4e4b\u524d\u7684\u51fd\u6570\u5f0f\u8f6c\u6362\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u5f00\u9500\u95ee\u9898\uff0c\u4e14\u5b9e\u73b0\u4e0d\u591f\u4f18\u96c5\uff1a\u5bf9\u7c7b\u578b\u4e0d\u654f\u611f\u3001\u9700\u8981\u786e\u5b9a\u6027\u6807\u6ce8\u3001\u5b58\u5728\u9690\u5f0f\u751f\u6210\u5668\u7ebf\u7a0b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7c7b\u578b\u5316\u65e0\u6807\u7b7e\u6700\u7ec8\u5d4c\u5165\u65b9\u6cd5\u5c06miniKanren\u5d4c\u5165\u5230Haskell\u4e2d\uff0c\u51cf\u5c11\u4e86\u6837\u677f\u4ee3\u7801\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u6837\u677f\u4ee3\u7801\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e86\u4e4b\u524d\u7684\u6027\u80fd\u52a0\u901f\u6548\u679c\u3002", "conclusion": "\u7c7b\u578b\u5316\u65e0\u6807\u7b7e\u6700\u7ec8\u5d4c\u5165\u65b9\u6cd5\u4e3a\u5173\u7cfb\u5f0f\u7f16\u7a0b\u63d0\u4f9b\u4e86\u66f4\u4f18\u96c5\u7684\u5b9e\u73b0\u65b9\u6848\uff0c\u5728\u51cf\u5c11\u4ee3\u7801\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2511.00038", "categories": ["cs.DC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00038", "abs": "https://arxiv.org/abs/2511.00038", "authors": ["Suman Raj", "Radhika Mittal", "Rajiv Mayani", "Pawel Zuk", "Anirban Mandal", "Michael Zink", "Yogesh Simmhan", "Ewa Deelman"], "title": "AeroResQ: Edge-Accelerated UAV Framework for Scalable, Resilient and Collaborative Escape Route Planning in Wildfire Scenarios", "comment": "26 pages, 11 figures", "summary": "Drone fleets equipped with onboard cameras, computer vision, and Deep Neural\nNetwork (DNN) models present a powerful paradigm for real-time spatio-temporal\ndecision-making. In wildfire response, such drones play a pivotal role in\nmonitoring fire dynamics, supporting firefighter coordination, and facilitating\nsafe evacuation. In this paper, we introduce AeroResQ, an edge-accelerated UAV\nframework designed for scalable, resilient, and collaborative escape route\nplanning during wildfire scenarios. AeroResQ adopts a multi-layer orchestration\narchitecture comprising service drones (SDs) and coordinator drones (CDs), each\nperforming specialized roles. SDs survey fire-affected areas, detect stranded\nindividuals using onboard edge accelerators running fire detection and human\npose identification DNN models, and issue requests for assistance. CDs,\nequipped with lightweight data stores such as Apache IoTDB, dynamically\ngenerate optimal ground escape routes and monitor firefighter movements along\nthese routes. The framework proposes a collaborative path-planning approach\nbased on a weighted A* search algorithm, where CDs compute context-aware escape\npaths. AeroResQ further incorporates intelligent load-balancing and resilience\nmechanisms: CD failures trigger automated data redistribution across IoTDB\nreplicas, while SD failures initiate geo-fenced re-partitioning and\nreassignment of spatial workloads to operational SDs. We evaluate AeroResQ\nusing realistic wildfire emulated setup modeled on recent Southern California\nwildfires. Experimental results demonstrate that AeroResQ achieves a nominal\nend-to-end latency of <=500ms, much below the 2s request interval, while\nmaintaining over 98% successful task reassignment and completion, underscoring\nits feasibility for real-time, on-field deployment in emergency response and\nfirefighter safety operations.", "AI": {"tldr": "AeroResQ\u662f\u4e00\u4e2a\u8fb9\u7f18\u52a0\u901f\u7684\u65e0\u4eba\u673a\u6846\u67b6\uff0c\u7528\u4e8e\u91ce\u706b\u573a\u666f\u4e0b\u7684\u53ef\u6269\u5c55\u3001\u5f39\u6027\u548c\u534f\u4f5c\u5f0f\u9003\u751f\u8def\u7ebf\u89c4\u5212\uff0c\u91c7\u7528\u591a\u5c42\u7f16\u6392\u67b6\u6784\uff0c\u5305\u542b\u670d\u52a1\u65e0\u4eba\u673a\u548c\u534f\u8c03\u65e0\u4eba\u673a\uff0c\u5b9e\u73b0\u5b9e\u65f6\u51b3\u7b56\u548c\u4efb\u52a1\u534f\u8c03\u3002", "motivation": "\u5728\u91ce\u706b\u54cd\u5e94\u4e2d\uff0c\u914d\u5907\u673a\u8f7d\u6444\u50cf\u5934\u548cDNN\u6a21\u578b\u7684\u65e0\u4eba\u673a\u7fa4\u5bf9\u4e8e\u5b9e\u65f6\u65f6\u7a7a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u76d1\u63a7\u706b\u707e\u52a8\u6001\u3001\u652f\u6301\u6d88\u9632\u5458\u534f\u8c03\u548c\u4fc3\u8fdb\u5b89\u5168\u758f\u6563\u3002", "method": "\u91c7\u7528\u591a\u5c42\u7f16\u6392\u67b6\u6784\uff0c\u670d\u52a1\u65e0\u4eba\u673a\u8d1f\u8d23\u8c03\u67e5\u706b\u707e\u533a\u57df\u3001\u68c0\u6d4b\u88ab\u56f0\u4eba\u5458\uff0c\u534f\u8c03\u65e0\u4eba\u673a\u4f7f\u7528\u52a0\u6743A*\u641c\u7d22\u7b97\u6cd5\u52a8\u6001\u751f\u6210\u6700\u4f18\u5730\u9762\u9003\u751f\u8def\u7ebf\uff0c\u5e76\u5305\u542b\u667a\u80fd\u8d1f\u8f7d\u5e73\u8861\u548c\u5f39\u6027\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aAeroResQ\u5b9e\u73b0\u4e86\u2264500ms\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u8fdc\u4f4e\u4e8e2\u79d2\u8bf7\u6c42\u95f4\u9694\uff0c\u540c\u65f6\u4fdd\u6301\u8d85\u8fc798%\u7684\u4efb\u52a1\u91cd\u65b0\u5206\u914d\u548c\u5b8c\u6210\u6210\u529f\u7387\u3002", "conclusion": "AeroResQ\u8bc1\u660e\u4e86\u5176\u5728\u7d27\u6025\u54cd\u5e94\u548c\u6d88\u9632\u5458\u5b89\u5168\u64cd\u4f5c\u4e2d\u5b9e\u65f6\u73b0\u573a\u90e8\u7f72\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u91ce\u706b\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9003\u751f\u8def\u7ebf\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00075", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00075", "abs": "https://arxiv.org/abs/2511.00075", "authors": ["Qianhui Li", "Weiya Wang", "Qianqi Zhao", "Tong Qu", "Jing He", "Xuhong Qiang", "Jingwen Hou", "Ke Chen", "Bao Zhang", "Qi Wang"], "title": "PDA-LSTM: Knowledge-driven page data arrangement based on LSTM for LCM supression in QLC 3D NAND flash memories", "comment": null, "summary": "Quarter level cell (QLC) 3D NAND flash memory is emerging as the predominant\nstorage solution in the era of artificial intelligence. QLC 3D NAND flash\nstores 4 bit per cell to expand the storage density, resulting in narrower read\nmargins. Constrained to read margins, QLC always suffers from lateral charge\nmigration (LCM), which caused by non-uniform charge density across adjacent\nmemory cells. To suppress charge density gap between cells, there are some\nalgorithm in form of intra-page data mapping such as WBVM, DVDS. However, we\nobserve inter-page data arrangements also approach the suppression. Thus, we\nproposed an intelligent model PDA-LSTM to arrange intra-page data for LCM\nsuppression, which is a physics-knowledge-driven neural network model. PDA-LSTM\napplies a long-short term memory (LSTM) neural network to compute a data\narrangement probability matrix from input page data pattern. The arrangement is\nto minimize the global impacts derived from the LCM among wordlines. Since each\npage data can be arranged only once, we design a transformation from output\nmatrix of LSTM network to non-repetitive sequence generation probability matrix\nto assist training process. The arranged data pattern can decrease the bit\nerror rate (BER) during data retention. In addition, PDA-LSTM do not need extra\nflag bits to record data transport of 3D NAND flash compared with WBVM, DVDS.\nThe experiment results show that the PDA-LSTM reduces the average BER by 80.4%\ncompared with strategy without data arrangement, and by 18.4%, 15.2% compared\nrespectively with WBVM and DVDS with code-length 64.", "AI": {"tldr": "\u63d0\u51faPDA-LSTM\u6a21\u578b\uff0c\u901a\u8fc7LSTM\u795e\u7ecf\u7f51\u7edc\u4f18\u53163D NAND\u95ea\u5b58\u4e2d\u7684\u6570\u636e\u6392\u5217\uff0c\u6291\u5236\u6a2a\u5411\u7535\u8377\u8fc1\u79fb(LCM)\uff0c\u964d\u4f4e\u6bd4\u7279\u9519\u8bef\u7387(BER)\uff0c\u76f8\u6bd4\u65e0\u6570\u636e\u6392\u5217\u7b56\u7565\u5e73\u5747BER\u964d\u4f4e80.4%\u3002", "motivation": "QLC 3D NAND\u95ea\u5b58\u56e0\u5b58\u50a8\u5bc6\u5ea6\u63d0\u5347\u5bfc\u81f4\u8bfb\u53d6\u88d5\u5ea6\u53d8\u7a84\uff0c\u6613\u53d7\u6a2a\u5411\u7535\u8377\u8fc1\u79fb(LCM)\u5f71\u54cd\u3002\u73b0\u6709\u7b97\u6cd5\u4ec5\u5173\u6ce8\u9875\u5185\u6570\u636e\u6620\u5c04\uff0c\u800c\u9875\u95f4\u6570\u636e\u6392\u5217\u4e5f\u80fd\u6709\u6548\u6291\u5236LCM\u3002", "method": "\u8bbe\u8ba1PDA-LSTM\u6a21\u578b\uff0c\u4f7f\u7528LSTM\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u6570\u636e\u6392\u5217\u6982\u7387\u77e9\u9635\uff0c\u5c06\u8f93\u51fa\u77e9\u9635\u8f6c\u6362\u4e3a\u975e\u91cd\u590d\u5e8f\u5217\u751f\u6210\u6982\u7387\u77e9\u9635\u4ee5\u8f85\u52a9\u8bad\u7ec3\uff0c\u4f18\u5316\u9875\u95f4\u6570\u636e\u6392\u5217\u4ee5\u6700\u5c0f\u5316LCM\u7684\u5168\u5c40\u5f71\u54cd\u3002", "result": "PDA-LSTM\u76f8\u6bd4\u65e0\u6570\u636e\u6392\u5217\u7b56\u7565\u5e73\u5747BER\u964d\u4f4e80.4%\uff0c\u76f8\u6bd4WBVM\u548cDVDS\uff08\u7801\u957f64\uff09\u5206\u522b\u964d\u4f4e18.4%\u548c15.2%\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6807\u5fd7\u4f4d\u3002", "conclusion": "PDA-LSTM\u901a\u8fc7\u667a\u80fd\u6570\u636e\u6392\u5217\u6709\u6548\u6291\u5236LCM\uff0c\u663e\u8457\u964d\u4f4eBER\uff0c\u4e3aQLC 3D NAND\u95ea\u5b58\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u9519\u8bef\u7387\u63a7\u5236\u65b9\u6848\u3002"}}
{"id": "2511.01736", "categories": ["cs.PL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.01736", "abs": "https://arxiv.org/abs/2511.01736", "authors": ["Charles Yuan"], "title": "Cobble: Compiling Block Encodings for Quantum Computational Linear Algebra", "comment": "20 pages, 12 figures", "summary": "Quantum algorithms for computational linear algebra promise up to exponential\nspeedups for applications such as simulation and regression, making them prime\ncandidates for hardware realization. But these algorithms execute in a model\nthat cannot efficiently store matrices in memory like a classical algorithm\ndoes, instead requiring developers to implement complex expressions for matrix\narithmetic in terms of correct and efficient quantum circuits. Among the\nchallenges for the developer is navigating a cost model in which conventional\noptimizations for linear algebra, such as subexpression reuse, can be\ninapplicable or unprofitable.\n  In this work, we present Cobble, a language for programming with quantum\ncomputational linear algebra. Cobble enables developers to express and\nmanipulate the quantum representations of matrices, known as block encodings,\nusing high-level notation that automatically compiles to correct quantum\ncircuits. Cobble features analyses that estimate leading factors in time and\nspace usage of programs, as well as optimizations that reduce overhead and\ngenerate efficient circuits using leading techniques such as the quantum\nsingular value transformation. We evaluate Cobble on benchmark kernels for\nsimulation, regression, search, and other applications, showing 2.6x-25.4x\nspeedups not achieved by existing circuit optimizers on these benchmarks.", "AI": {"tldr": "Cobble\u662f\u4e00\u79cd\u7528\u4e8e\u91cf\u5b50\u8ba1\u7b97\u7ebf\u6027\u4ee3\u6570\u7f16\u7a0b\u7684\u8bed\u8a00\uff0c\u901a\u8fc7\u9ad8\u7ea7\u7b26\u53f7\u81ea\u52a8\u7f16\u8bd1\u4e3a\u6b63\u786e\u7684\u91cf\u5b50\u7535\u8def\uff0c\u63d0\u4f9b\u6027\u80fd\u5206\u6790\u548c\u4f18\u5316\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b02.6x-25.4x\u7684\u52a0\u901f\u3002", "motivation": "\u91cf\u5b50\u7ebf\u6027\u4ee3\u6570\u7b97\u6cd5\u867d\u7136\u627f\u8bfa\u6307\u6570\u7ea7\u52a0\u901f\uff0c\u4f46\u5f00\u53d1\u4eba\u5458\u9700\u8981\u5c06\u590d\u6742\u7684\u77e9\u9635\u8fd0\u7b97\u8868\u8fbe\u5f0f\u8f6c\u5316\u4e3a\u6b63\u786e\u9ad8\u6548\u7684\u91cf\u5b50\u7535\u8def\uff0c\u9762\u4e34\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u4e0d\u9002\u7528\u7b49\u6311\u6218\u3002", "method": "\u5f00\u53d1Cobble\u8bed\u8a00\uff0c\u652f\u6301\u7528\u9ad8\u7ea7\u7b26\u53f7\u8868\u8fbe\u548c\u64cd\u4f5c\u77e9\u9635\u7684\u91cf\u5b50\u8868\u793a\uff08\u5757\u7f16\u7801\uff09\uff0c\u81ea\u52a8\u7f16\u8bd1\u4e3a\u91cf\u5b50\u7535\u8def\uff0c\u5305\u542b\u6027\u80fd\u5206\u6790\u548c\u4f18\u5316\u6280\u672f\u5982\u91cf\u5b50\u5947\u5f02\u503c\u53d8\u6362\u3002", "result": "\u5728\u6a21\u62df\u3001\u56de\u5f52\u3001\u641c\u7d22\u7b49\u5e94\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCobble\u5b9e\u73b0\u4e862.6x-25.4x\u7684\u52a0\u901f\uff0c\u4f18\u4e8e\u73b0\u6709\u7535\u8def\u4f18\u5316\u5668\u3002", "conclusion": "Cobble\u4e3a\u91cf\u5b50\u8ba1\u7b97\u7ebf\u6027\u4ee3\u6570\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7f16\u7a0b\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u7b80\u5316\u4e86\u5f00\u53d1\u8fc7\u7a0b\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2511.00263", "categories": ["cs.DC", "cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2511.00263", "abs": "https://arxiv.org/abs/2511.00263", "authors": ["Jinyuan Chen"], "title": "COOL Is Optimal in Error-Free Asynchronous Byzantine Agreement", "comment": "25 pages", "summary": "COOL (Chen'21) is an error-free, information-theoretically secure Byzantine\nagreement (BA) protocol proven to achieve BA consensus in the synchronous\nsetting for an $\\ell$-bit message, with a total communication complexity of\n$O(\\max\\{n\\ell, nt \\log q\\})$ bits, four communication rounds in the worst\ncase, and a single invocation of a binary BA, under the optimal resilience\nassumption $n \\geq 3t + 1$ in a network of $n$ nodes, where up to $t$ nodes may\nbehave dishonestly. Here, $q$ denotes the alphabet size of the error correction\ncode used in the protocol.\n  In this work, we present an adaptive variant of COOL, called OciorACOOL,\nwhich achieves error-free, information-theoretically secure BA consensus in the\nasynchronous setting with total $O(\\max\\{n\\ell, n t \\log q\\})$ communication\nbits, $O(1)$ rounds, and a single invocation of an asynchronous binary BA\nprotocol, still under the optimal resilience assumption $n \\geq 3t + 1$.\nMoreover, OciorACOOL retains the same low-complexity, traditional $(n, k)$\nerror-correction encoding and decoding as COOL, with $k=t/3$.", "AI": {"tldr": "OciorACOOL\u662fCOOL\u534f\u8bae\u7684\u5f02\u6b65\u81ea\u9002\u5e94\u7248\u672c\uff0c\u5728\u5f02\u6b65\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u65e0\u9519\u8bef\u3001\u4fe1\u606f\u8bba\u5b89\u5168\u7684\u62dc\u5360\u5ead\u5171\u8bc6\uff0c\u5177\u6709\u6700\u4f18\u5f39\u6027n\u22653t+1\u3001O(max{n\u2113, nt log q})\u901a\u4fe1\u590d\u6742\u5ea6\u548cO(1)\u8f6e\u6b21\u3002", "motivation": "\u5c06COOL\u534f\u8bae\u4ece\u540c\u6b65\u8bbe\u7f6e\u6269\u5c55\u5230\u5f02\u6b65\u8bbe\u7f6e\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4f4e\u901a\u4fe1\u590d\u6742\u5ea6\u548c\u4fe1\u606f\u8bba\u5b89\u5168\u6027\uff0c\u89e3\u51b3\u5f02\u6b65\u73af\u5883\u4e0b\u7684\u62dc\u5360\u5ead\u5171\u8bc6\u95ee\u9898\u3002", "method": "\u57fa\u4e8eCOOL\u534f\u8bae\u8bbe\u8ba1\u81ea\u9002\u5e94\u53d8\u4f53\uff0c\u4f7f\u7528\u4f20\u7edf\u7684(n,k)\u7ea0\u9519\u7f16\u7801\uff08k=t/3\uff09\uff0c\u4ec5\u9700\u8c03\u7528\u4e00\u6b21\u5f02\u6b65\u4e8c\u8fdb\u5236BA\u534f\u8bae\u3002", "result": "\u5728\u5f02\u6b65\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u4e0eCOOL\u76f8\u540c\u7684\u901a\u4fe1\u590d\u6742\u5ea6O(max{n\u2113, nt log q})\u3001O(1)\u8f6e\u6b21\uff0c\u540c\u65f6\u4fdd\u6301\u65e0\u9519\u8bef\u548c\u4fe1\u606f\u8bba\u5b89\u5168\u6027\u3002", "conclusion": "OciorACOOL\u6210\u529f\u5c06COOL\u534f\u8bae\u6269\u5c55\u5230\u5f02\u6b65\u73af\u5883\uff0c\u5728\u4fdd\u6301\u6240\u6709\u4f18\u70b9\u7684\u540c\u65f6\u9002\u5e94\u4e86\u5f02\u6b65\u7f51\u7edc\u6761\u4ef6\u3002"}}
{"id": "2511.00295", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.00295", "abs": "https://arxiv.org/abs/2511.00295", "authors": ["Kosmas Alexandridis", "Giorgos Dimitrakopoulos"], "title": "H-FA: A Hybrid Floating-Point and Logarithmic Approach to Hardware Accelerated FlashAttention", "comment": "Accepted for publication at IEEE Transactions on Circuits and Systems\n  for Artificial Intelligence", "summary": "Transformers have significantly advanced AI and machine learning through\ntheir powerful attention mechanism. However, computing attention on long\nsequences can become a computational bottleneck. FlashAttention mitigates this\nby fusing the softmax and matrix operations into a tiled computation pattern\nthat decouples performance from sequence length. Though designed for GPUs, its\nsimplicity also makes it well suited for direct hardware acceleration. To\nimprove hardware implementation, we compute FlashAttention using a mixture of\nfloating-point and fixed-point logarithm domain representations. Floating-point\nis used to compute attention scores from query and key matrices, while\nlogarithmic computation simplifies the fused computation of softmax\nnormalization and the multiplication with the value matrix. This\ntransformation, called H-FA, replaces vector-wide floating-point multiplication\nand division operations by additions and subtractions implemented efficiently\nwith fixed-point arithmetic in the logarithm domain. Exponential function\nevaluations are effectively omitted and fused with the rest operations, and the\nfinal result is directly returned to floating-point arithmetic without any\nadditional hardware overhead. Hardware implementation results at 28nm\ndemonstrate that H-FA achieves a 26.5% reduction in area and a 23.4% reduction\nin power, on average, compared to FlashAttention parallel hardware\narchitectures built solely with floating-point datapaths, without hindering\nperformance.", "AI": {"tldr": "H-FA\u662f\u4e00\u79cd\u6539\u8fdb\u7684FlashAttention\u786c\u4ef6\u5b9e\u73b0\uff0c\u901a\u8fc7\u6df7\u5408\u4f7f\u7528\u6d6e\u70b9\u6570\u548c\u5b9a\u70b9\u6570\u5bf9\u6570\u57df\u8868\u793a\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u786c\u4ef6\u9762\u79ef\u548c\u529f\u8017\u3002", "motivation": "Transformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u5b58\u5728\u8ba1\u7b97\u74f6\u9888\uff0cFlashAttention\u867d\u7136\u901a\u8fc7\u5206\u5757\u8ba1\u7b97\u7f13\u89e3\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u5176\u786c\u4ef6\u5b9e\u73b0\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u5728\u964d\u4f4e\u9762\u79ef\u548c\u529f\u8017\u65b9\u9762\u3002", "method": "\u4f7f\u7528\u6d6e\u70b9\u6570\u8ba1\u7b97\u67e5\u8be2\u548c\u952e\u77e9\u9635\u7684\u6ce8\u610f\u529b\u5206\u6570\uff0c\u540c\u65f6\u5728\u5bf9\u6570\u57df\u4e2d\u4f7f\u7528\u5b9a\u70b9\u6570\u7b97\u672f\u7b80\u5316softmax\u5f52\u4e00\u5316\u548c\u4e0e\u503c\u77e9\u9635\u4e58\u6cd5\u7684\u878d\u5408\u8ba1\u7b97\uff0c\u5c06\u5411\u91cf\u7ea7\u7684\u6d6e\u70b9\u4e58\u9664\u64cd\u4f5c\u66ff\u6362\u4e3a\u5b9a\u70b9\u6570\u7684\u52a0\u51cf\u64cd\u4f5c\u3002", "result": "\u572828nm\u5de5\u827a\u4e0b\uff0cH-FA\u76f8\u6bd4\u4ec5\u4f7f\u7528\u6d6e\u70b9\u6570\u636e\u8def\u5f84\u7684FlashAttention\u5e76\u884c\u786c\u4ef6\u67b6\u6784\uff0c\u5e73\u5747\u51cf\u5c11\u4e8626.5%\u7684\u9762\u79ef\u548c23.4%\u7684\u529f\u8017\uff0c\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "H-FA\u901a\u8fc7\u6df7\u5408\u6d6e\u70b9-\u5b9a\u70b9\u5bf9\u6570\u57df\u8ba1\u7b97\u65b9\u6cd5\uff0c\u6210\u529f\u5b9e\u73b0\u4e86FlashAttention\u786c\u4ef6\u5b9e\u73b0\u7684\u9762\u79ef\u548c\u529f\u8017\u4f18\u5316\uff0c\u4e3a\u957f\u5e8f\u5217Transformer\u7684\u9ad8\u6548\u786c\u4ef6\u52a0\u901f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.00294", "categories": ["cs.DC", "cs.NI", "68M14", "C.2.4"], "pdf": "https://arxiv.org/pdf/2511.00294", "abs": "https://arxiv.org/abs/2511.00294", "authors": ["Lucas Almeida", "Maycon Peixoto"], "title": "Tetris: An SLA-aware Application Placement Strategy in the Edge-Cloud Continuum", "comment": "10 pages, 7 sections, 12 figures, 9 tables", "summary": "An Edge-Cloud Continuum integrates edge and cloud resources to provide a\nflexible and scalable infrastructure. This paradigm can minimize latency by\nprocessing data closer to the source at the edge while leveraging the vast\ncomputational power of the cloud for more intensive tasks. In this context,\nmodule application placement requires strategic allocation plans that align\nuser demands with infrastructure constraints, aiming for efficient resource\nuse. Therefore, we propose Tetris, an application placement strategy that\nutilizes a heuristic algorithm to distribute computational services across edge\nand cloud resources efficiently. Tetris prioritizes services based on SLA\nurgencies and resource efficiency to avoid system overloading. Our results\ndemonstrate that Tetris reduces SLA violations by approximately 76% compared to\nthe baseline method, which serves as a reference point for benchmarking\nperformance in this scenario. Therefore, Tetris offers an effective placement\napproach for managing latency-sensitive applications in Edge-Cloud Continuum\nenvironments, enhancing Quality of Service (QoS) for users.", "AI": {"tldr": "\u63d0\u51faTetris\u5e94\u7528\u653e\u7f6e\u7b56\u7565\uff0c\u5728\u8fb9\u7f18-\u4e91\u8fde\u7eed\u4f53\u4e2d\u901a\u8fc7\u542f\u53d1\u5f0f\u7b97\u6cd5\u5206\u914d\u8ba1\u7b97\u670d\u52a1\uff0c\u4f18\u5148\u8003\u8651SLA\u7d27\u6025\u6027\u548c\u8d44\u6e90\u6548\u7387\uff0c\u51cf\u5c1176%\u7684SLA\u8fdd\u89c4\u3002", "motivation": "\u8fb9\u7f18-\u4e91\u8fde\u7eed\u4f53\u6574\u5408\u8fb9\u7f18\u548c\u4e91\u8d44\u6e90\u63d0\u4f9b\u7075\u6d3b\u57fa\u7840\u8bbe\u65bd\uff0c\u9700\u8981\u5728\u7528\u6237\u9700\u6c42\u4e0e\u57fa\u7840\u8bbe\u65bd\u7ea6\u675f\u95f4\u8fdb\u884c\u6218\u7565\u6027\u5e94\u7528\u653e\u7f6e\u4ee5\u5b9e\u73b0\u9ad8\u6548\u8d44\u6e90\u5229\u7528\u3002", "method": "\u4f7f\u7528\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u8fb9\u7f18\u548c\u4e91\u8d44\u6e90\u95f4\u5206\u914d\u8ba1\u7b97\u670d\u52a1\uff0c\u57fa\u4e8eSLA\u7d27\u6025\u6027\u548c\u8d44\u6e90\u6548\u7387\u8fdb\u884c\u4f18\u5148\u7ea7\u6392\u5e8f\u4ee5\u907f\u514d\u7cfb\u7edf\u8fc7\u8f7d\u3002", "result": "\u4e0e\u57fa\u51c6\u65b9\u6cd5\u76f8\u6bd4\uff0cTetris\u51cf\u5c11\u4e86\u7ea676%\u7684SLA\u8fdd\u89c4\uff0c\u63d0\u5347\u4e86\u670d\u52a1\u8d28\u91cf\u3002", "conclusion": "Tetris\u4e3a\u8fb9\u7f18-\u4e91\u8fde\u7eed\u4f53\u73af\u5883\u4e2d\u7684\u5ef6\u8fdf\u654f\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u653e\u7f6e\u65b9\u6cd5\uff0c\u663e\u8457\u6539\u5584\u4e86\u7528\u6237\u670d\u52a1\u8d28\u91cf\u3002"}}
{"id": "2511.00321", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00321", "abs": "https://arxiv.org/abs/2511.00321", "authors": ["Dowon Kim", "MinJae Lee", "Janghyeon Kim", "HyuckSung Kwon", "Hyeonggyu Jeong", "Sang-Soo Park", "Minyong Yoon", "Si-Dong Roh", "Yongsuk Kwon", "Jinin So", "Jungwook Choi"], "title": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits", "comment": null, "summary": "The expansion of context windows in large language models (LLMs) to\nmulti-million tokens introduces severe memory and compute bottlenecks,\nparticularly in managing the growing Key-Value (KV) cache. While Compute\nExpress Link (CXL) enables non-eviction frameworks that offload the full\nKV-cache to scalable external memory, these frameworks still suffer from costly\ndata transfers when recalling non-resident KV tokens to limited GPU memory as\ncontext lengths increase. This work proposes scalable Processing-Near-Memory\n(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that\ncoordinates memory and computation beyond GPU limits. Our design offloads token\npage selection to a PNM accelerator within CXL memory, eliminating costly\nrecalls and enabling larger GPU batch sizes. We further introduce a hybrid\nparallelization strategy and a steady-token selection mechanism to enhance\ncompute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM\nsystem, our solution delivers consistent performance gains for LLMs with up to\n405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)\nand GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x\nthroughput improvement, up to 60x lower energy per token, and up to 7.3x better\ntotal cost efficiency than the baseline, demonstrating that CXL-enabled\nmulti-PNM architectures can serve as a scalable backbone for future\nlong-context LLM inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCXL\u7684PNM KV\u7f13\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fd1\u5185\u5b58\u5904\u7406\u6280\u672f\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u6269\u5c55\u5230\u6570\u767e\u4e07token\uff0cKV\u7f13\u5b58\u7ba1\u7406\u9762\u4e34\u4e25\u91cd\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u73b0\u6709\u7684CXL\u975e\u9a71\u9010\u6846\u67b6\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u4ecd\u5b58\u5728\u6602\u8d35\u7684\u6570\u636e\u4f20\u8f93\u6210\u672c\u3002", "method": "\u8bbe\u8ba1CXL\u542f\u7528\u7684KV\u7f13\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u5c06token\u9875\u9762\u9009\u62e9\u5378\u8f7d\u5230CXL\u5185\u5b58\u4e2d\u7684PNM\u52a0\u901f\u5668\uff0c\u63d0\u51fa\u6df7\u5408\u5e76\u884c\u5316\u7b56\u7565\u548c\u7a33\u6001token\u9009\u62e9\u673a\u5236\u3002", "result": "\u5728405B\u53c2\u6570\u548c1M token\u4e0a\u4e0b\u6587\u7684LLM\u4e0a\u5b9e\u73b0\u4e86\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0cPNM-KV\u548cPnG-KV\u65b9\u6848\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad821.9\u500d\u541e\u5410\u91cf\u63d0\u5347\u300160\u500d\u6bcftoken\u80fd\u8017\u964d\u4f4e\u548c7.3\u500d\u603b\u6210\u672c\u6548\u7387\u63d0\u5347\u3002", "conclusion": "CXL\u542f\u7528\u7684\u591aPNM\u67b6\u6784\u53ef\u4ee5\u4f5c\u4e3a\u672a\u6765\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u7684\u53ef\u6269\u5c55\u57fa\u7840\u67b6\u6784\u3002"}}
{"id": "2511.00603", "categories": ["cs.DC", "cs.AI", "cs.NI", "68T05", "I.2.11"], "pdf": "https://arxiv.org/pdf/2511.00603", "abs": "https://arxiv.org/abs/2511.00603", "authors": ["Yubo Wang", "Yubo Cui", "Tuo Shi", "Danyang Li", "Wenxin Li", "Lide Suo", "Tao Wang", "Xin Xie"], "title": "EPARA: Parallelizing Categorized AI Inference in Edge Clouds", "comment": "15 pages,20 figures", "summary": "With the increasing adoption of AI applications such as large language models\nand computer vision AI, the computational demands on AI inference systems are\ncontinuously rising, making the enhancement of task processing capacity using\nexisting hardware a primary objective in edge clouds. We propose EPARA, an\nend-to-end AI parallel inference framework in edge, aimed at enhancing the edge\nAI serving capability. Our key idea is to categorize tasks based on their\nsensitivity to latency/frequency and requirement for GPU resources, thereby\nachieving both request-level and service-level task-resource allocation. EPARA\nconsists of three core components: 1) a task-categorized parallelism allocator\nthat decides the parallel mode of each task, 2) a distributed request handler\nthat performs the calculation for the specific request, and 3) a state-aware\nscheduler that periodically updates service placement in edge clouds. We\nimplement a EPARA prototype and conduct a case study on the EPARA operation for\nLLMs and segmentation tasks. Evaluation through testbed experiments involving\nedge servers, embedded devices, and microcomputers shows that EPARA achieves up\nto 2.1$\\times$ higher goodput in production workloads compared to prior\nframeworks, while adapting to various edge AI inference tasks.", "AI": {"tldr": "EPARA\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u8fb9\u7f18AI\u5e76\u884c\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u7c7b\u548c\u8d44\u6e90\u5206\u914d\u63d0\u5347\u8fb9\u7f18AI\u670d\u52a1\u80fd\u529b\uff0c\u5728\u8fb9\u7f18\u670d\u52a1\u5668\u4e0a\u5b9e\u73b0\u6bd4\u73b0\u6709\u6846\u67b6\u9ad82.1\u500d\u7684\u751f\u4ea7\u8d1f\u8f7d\u541e\u5410\u91cf\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8ba1\u7b97\u673a\u89c6\u89c9AI\u5e94\u7528\u7684\u5e7f\u6cdb\u91c7\u7528\uff0cAI\u63a8\u7406\u7cfb\u7edf\u7684\u8ba1\u7b97\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u9700\u8981\u5728\u73b0\u6709\u786c\u4ef6\u57fa\u7840\u4e0a\u63d0\u5347\u8fb9\u7f18\u4e91\u7684\u4efb\u52a1\u5904\u7406\u80fd\u529b\u3002", "method": "EPARA\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1)\u4efb\u52a1\u5206\u7c7b\u5e76\u884c\u5206\u914d\u5668\u51b3\u5b9a\u6bcf\u4e2a\u4efb\u52a1\u7684\u5e76\u884c\u6a21\u5f0f\uff1b2)\u5206\u5e03\u5f0f\u8bf7\u6c42\u5904\u7406\u5668\u6267\u884c\u5177\u4f53\u8bf7\u6c42\u8ba1\u7b97\uff1b3)\u72b6\u6001\u611f\u77e5\u8c03\u5ea6\u5668\u5b9a\u671f\u66f4\u65b0\u8fb9\u7f18\u4e91\u4e2d\u7684\u670d\u52a1\u90e8\u7f72\u3002\u901a\u8fc7\u57fa\u4e8e\u4efb\u52a1\u5bf9\u5ef6\u8fdf/\u9891\u7387\u654f\u611f\u5ea6\u548cGPU\u8d44\u6e90\u9700\u6c42\u7684\u5206\u7c7b\uff0c\u5b9e\u73b0\u8bf7\u6c42\u7ea7\u548c\u670d\u52a1\u7ea7\u7684\u4efb\u52a1-\u8d44\u6e90\u5206\u914d\u3002", "result": "\u5728\u5305\u542b\u8fb9\u7f18\u670d\u52a1\u5668\u3001\u5d4c\u5165\u5f0f\u8bbe\u5907\u548c\u5fae\u8ba1\u7b97\u673a\u7684\u6d4b\u8bd5\u5e73\u53f0\u5b9e\u9a8c\u4e2d\uff0cEPARA\u5728\u751f\u4ea7\u8d1f\u8f7d\u4e2d\u76f8\u6bd4\u73b0\u6709\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.1\u500d\u7684\u9ad8\u541e\u5410\u91cf\uff0c\u5e76\u80fd\u9002\u5e94\u5404\u79cd\u8fb9\u7f18AI\u63a8\u7406\u4efb\u52a1\u3002", "conclusion": "EPARA\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8fb9\u7f18AI\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u667a\u80fd\u7684\u4efb\u52a1\u5206\u7c7b\u548c\u8d44\u6e90\u5206\u914d\u673a\u5236\uff0c\u5728\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2511.01244", "categories": ["cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.01244", "abs": "https://arxiv.org/abs/2511.01244", "authors": ["Wajid Ali", "Ayaz Akram", "Deepak Shankar"], "title": "Simulation-Driven Evaluation of Chiplet-Based Architectures Using VisualSim", "comment": null, "summary": "This paper focuses on the simulation of multi-die System-on-Chip (SoC)\narchitectures using VisualSim, emphasiz- ing chiplet-based system modeling and\nperformance analysis. Chiplet technology presents a promising alternative to\ntraditional monolithic chips, which face increasing challenges in manufactur-\ning costs, power efficiency, and performance scaling. By integrat- ing multiple\nsmall modular silicon units into a single package, chiplet-based architectures\noffer greater flexibility and scalability at a lower overall cost. In this\nstudy, we developed a detailed sim- ulation model of a chiplet-based system,\nincorporating multicore ARM processor clusters interconnected through a ARM\nCMN600 network-on-chip (NoC) for efficient communication [4], [7]. The\nsimulation framework in VisualSim enables the evaluation of critical system\nmetrics, including inter-chiplet communication latency, memory access\nefficiency, workload distribution, and the power-performance tradeoff under\nvarious workloads. Through simulation-driven insights, this research highlights\nkey factors influencing chiplet system performance and provides a foundation\nfor optimizing future chiplet-based semiconductor designs.", "AI": {"tldr": "\u4f7f\u7528VisualSim\u4eff\u771f\u591a\u82af\u7247\u7cfb\u7edf\u67b6\u6784\uff0c\u91cd\u70b9\u7814\u7a76\u57fa\u4e8e\u5c0f\u82af\u7247\u7684\u7cfb\u7edf\u5efa\u6a21\u548c\u6027\u80fd\u5206\u6790\uff0c\u8bc4\u4f30\u901a\u4fe1\u5ef6\u8fdf\u3001\u5185\u5b58\u8bbf\u95ee\u6548\u7387\u7b49\u5173\u952e\u6307\u6807\u3002", "motivation": "\u4f20\u7edf\u5355\u7247\u82af\u7247\u9762\u4e34\u5236\u9020\u6210\u672c\u3001\u80fd\u6548\u548c\u6027\u80fd\u6269\u5c55\u7684\u6311\u6218\uff0c\u5c0f\u82af\u7247\u6280\u672f\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u6a21\u5757\u5316\u7845\u5355\u5143\u63d0\u4f9b\u66f4\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u4e14\u6210\u672c\u66f4\u4f4e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5c0f\u82af\u7247\u7cfb\u7edf\u7684\u8be6\u7ec6\u4eff\u771f\u6a21\u578b\uff0c\u5305\u542b\u591a\u6838ARM\u5904\u7406\u5668\u96c6\u7fa4\uff0c\u901a\u8fc7ARM CMN600\u7247\u4e0a\u7f51\u7edc\u5b9e\u73b0\u9ad8\u6548\u901a\u4fe1\uff0c\u4f7f\u7528VisualSim\u6846\u67b6\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u4eff\u771f\u63ed\u793a\u4e86\u5f71\u54cd\u5c0f\u82af\u7247\u7cfb\u7edf\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5305\u62ec\u82af\u7247\u95f4\u901a\u4fe1\u5ef6\u8fdf\u3001\u5185\u5b58\u8bbf\u95ee\u6548\u7387\u548c\u529f\u8017\u6027\u80fd\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f18\u5316\u672a\u6765\u57fa\u4e8e\u5c0f\u82af\u7247\u7684\u534a\u5bfc\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u4eff\u771f\u9a71\u52a8\u5206\u6790\u5728\u5c0f\u82af\u7247\u7cfb\u7edf\u5f00\u53d1\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.00796", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00796", "abs": "https://arxiv.org/abs/2511.00796", "authors": ["Ran Yan", "Youhe Jiang", "Tianyuan Wu", "Jiaxuan Gao", "Zhiyu Mei", "Wei Fu", "Haohui Mai", "Wei Wang", "Yi Wu", "Binhang Yuan"], "title": "AReaL-Hex: Accommodating Asynchronous RL Training over Heterogeneous GPUs", "comment": null, "summary": "Maximizing training throughput and cost-efficiency of RL for LLMs is\nessential to democratize this advanced technique. One promising but challenging\napproach is to deploy such a computational workflow over heterogeneous GPUs.\nUnlike conventional large-scale LLM pretraining, RL training generally\ndecomposes into three coupled stages, i.e., rollout generation, reward\ncomputation, and policy/value updates, which exhibit markedly different compute\nintensities, memory footprints, and communication patterns. Recent research\nshows that fully asynchronous RL training can disaggregate these stages across\ndisjoint hardware pools without sacrificing training stability, creating a\ngreat opportunity for real-world heterogeneous deployment. To this end, we\npresent AReaL-Hex, a heterogeneity-aware asynchronous RL training system that\neffectively schedules how to execute rollout generation and policy model\ntraining over heterogeneous GPUs while enforcing data staleness bounds.\nConcretely, we use a two-phase scheduler: (i) a constrained search with MILP to\nselect per-stage parallelization strategies and workload assignments given a\nresource budget, and (ii) a graph-partitioning step that allocates\nheterogeneous GPUs and interconnects to maximize end-to-end throughput. Built\natop a fully asynchronous RL architecture, AReaL-Hex maps HBM-I/O-bound\ngeneration and compute-bound optimization to more cost-efficient resources and\nbalances their producer-consumer interactions to avoid both idleness and stale\nrollout trajectories. On the mathematical reasoning task with various model\nscales (1.5B, 7B, and 14B), compared to homogeneous deployments of\nstate-of-the-art asynchronous RL systems: (i) When maintaining the same total\nbudgets, AReaL-Hex delivers up to 1.50x higher training throughput; (ii) When\nachieving the same training throughput, AReaL-Hex results in up to 1.46x\nreduction in training cost.", "AI": {"tldr": "AReaL-Hex\u662f\u4e00\u4e2a\u9762\u5411\u5f02\u6784GPU\u7684\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8c03\u5ea6\u5668\u5728\u4fdd\u6301\u6570\u636e\u65b0\u9c9c\u5ea6\u7ea6\u675f\u7684\u540c\u65f6\uff0c\u6700\u5927\u5316\u5f02\u6784\u73af\u5883\u4e0b\u7684\u8bad\u7ec3\u541e\u5410\u91cf\u548c\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u4e3a\u4e86\u964d\u4f4eLLM\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u95e8\u69db\uff0c\u9700\u8981\u6700\u5927\u5316\u8bad\u7ec3\u541e\u5410\u91cf\u548c\u6210\u672c\u6548\u76ca\u3002\u5f02\u6784GPU\u90e8\u7f72\u662f\u4e00\u4e2a\u6709\u524d\u666f\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3aRL\u8bad\u7ec3\u5305\u542b\u4e09\u4e2a\u8ba1\u7b97\u7279\u6027\u4e0d\u540c\u7684\u9636\u6bb5\u3002", "method": "\u4f7f\u7528\u4e24\u9636\u6bb5\u8c03\u5ea6\u5668\uff1a1\uff09\u901a\u8fc7\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u9009\u62e9\u5e76\u884c\u5316\u7b56\u7565\u548c\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\uff1b2\uff09\u901a\u8fc7\u56fe\u5206\u533a\u5206\u914d\u5f02\u6784GPU\u548c\u4e92\u8fde\u4ee5\u6700\u5927\u5316\u7aef\u5230\u7aef\u541e\u5410\u91cf\u3002\u7cfb\u7edf\u5c06\u5185\u5b58I/O\u5bc6\u96c6\u578b\u751f\u6210\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u4f18\u5316\u6620\u5c04\u5230\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u7684\u8d44\u6e90\u4e0a\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\uff0c\u76f8\u6bd4\u540c\u6784\u90e8\u7f72\u7684\u6700\u5148\u8fdb\u5f02\u6b65RL\u7cfb\u7edf\uff1a1\uff09\u76f8\u540c\u9884\u7b97\u4e0b\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u53471.50\u500d\uff1b2\uff09\u76f8\u540c\u541e\u5410\u91cf\u4e0b\u8bad\u7ec3\u6210\u672c\u964d\u4f4e1.46\u500d\u3002", "conclusion": "AReaL-Hex\u8bc1\u660e\u4e86\u5728\u5f02\u6784GPU\u73af\u5883\u4e2d\u6709\u6548\u8c03\u5ea6RL\u8bad\u7ec3\u9636\u6bb5\u7684\u53ef\u884c\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2511.00807", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.00807", "abs": "https://arxiv.org/abs/2511.00807", "authors": ["Xuan He", "Zequan Fang", "Jinzhao Lian", "Danny H. K. Tsang", "Baosen Zhang", "Yize Chen"], "title": "FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving on Heterogeneous GPUs", "comment": "In Submission, code available at\n  https://github.com/AndrewFangZequan/LLM_Serving_FREESH", "summary": "The ever-increasing computation and energy demand for LLM and AI agents call\nfor holistic and efficient optimization of LLM serving systems. In practice,\nheterogeneous GPU clusters can be deployed in a geographically distributed\nmanner, while LLM load also observes diversity in terms of both query traffic\nand serving patterns. LLM queries running on advanced GPUs during a\nhigh-emission hour at one location can lead to significantly higher carbon\nfootprints versus same queries running on mid-level GPUs at a low-emission time\nand location. By observing LLM serving requirements and leveraging\nspatiotemporal computation flexibility, we consider the joint routing and\nscheduling problem, and propose FREESH to cooperatively run a group of data\ncenters while minimizing user-specified carbon or energy objectives. FREESH\nidentifies the optimal configurations of balanced load serving by matching\ndistinct GPU instance's power-throughput characteristics with predictable LLM\nquery length and workloads. To ensure both latency and fairness requirements,\nFREESH identifies optimized parallelism and query routing schedules together\nwith dynamic GPU frequency scaling for power saving, and Least-Laxity-First\n(LLF) serving strategy for query scheduling. During the 1-hour serving on\nproduction workloads, FREESH reduces energy by 28.6% and emissions by 45.45%\ntogether with improvements in SLO attainment and fairness.", "AI": {"tldr": "FREESH\u662f\u4e00\u4e2a\u8054\u5408\u8def\u7531\u548c\u8c03\u5ea6\u7684LLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u7a7a\u8ba1\u7b97\u7075\u6d3b\u6027\uff0c\u5728\u5f02\u6784GPU\u96c6\u7fa4\u4e2d\u4f18\u5316\u80fd\u6e90\u6d88\u8017\u548c\u78b3\u6392\u653e\uff0c\u540c\u65f6\u6ee1\u8db3\u5ef6\u8fdf\u548c\u516c\u5e73\u6027\u8981\u6c42\u3002", "motivation": "\u968f\u7740LLM\u548cAI\u4ee3\u7406\u7684\u8ba1\u7b97\u548c\u80fd\u6e90\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u9700\u8981\u5168\u9762\u4f18\u5316LLM\u670d\u52a1\u7cfb\u7edf\u3002\u5f02\u6784GPU\u96c6\u7fa4\u5728\u5730\u7406\u4e0a\u5206\u5e03\u90e8\u7f72\uff0c\u800cLLM\u8d1f\u8f7d\u5728\u67e5\u8be2\u6d41\u91cf\u548c\u670d\u52a1\u6a21\u5f0f\u4e0a\u5177\u6709\u591a\u6837\u6027\uff0c\u8fd9\u4e3a\u901a\u8fc7\u65f6\u7a7a\u8ba1\u7b97\u7075\u6d3b\u6027\u6765\u4f18\u5316\u80fd\u6e90\u548c\u78b3\u6392\u653e\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "FREESH\u901a\u8fc7\u5339\u914d\u4e0d\u540cGPU\u5b9e\u4f8b\u7684\u529f\u8017-\u541e\u5410\u91cf\u7279\u6027\u4e0e\u53ef\u9884\u6d4b\u7684LLM\u67e5\u8be2\u957f\u5ea6\u548c\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\u670d\u52a1\u3002\u7cfb\u7edf\u7ed3\u5408\u4f18\u5316\u5e76\u884c\u5ea6\u548c\u67e5\u8be2\u8def\u7531\u8c03\u5ea6\uff0c\u52a8\u6001GPU\u9891\u7387\u8c03\u8282\u4ee5\u8282\u7701\u529f\u8017\uff0c\u4ee5\u53caLeast-Laxity-First\uff08LLF\uff09\u670d\u52a1\u7b56\u7565\u8fdb\u884c\u67e5\u8be2\u8c03\u5ea6\u3002", "result": "\u5728\u751f\u4ea7\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u8fdb\u884c1\u5c0f\u65f6\u670d\u52a1\u6d4b\u8bd5\uff0cFREESH\u5b9e\u73b0\u4e86\u80fd\u6e90\u6d88\u8017\u964d\u4f4e28.6%\uff0c\u78b3\u6392\u653e\u51cf\u5c1145.45%\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86SLO\u8fbe\u6210\u7387\u548c\u516c\u5e73\u6027\u3002", "conclusion": "FREESH\u901a\u8fc7\u8054\u5408\u8def\u7531\u548c\u8c03\u5ea6\u4f18\u5316\uff0c\u5728\u5f02\u6784\u5206\u5e03\u5f0fGPU\u96c6\u7fa4\u4e2d\u6709\u6548\u964d\u4f4e\u4e86LLM\u670d\u52a1\u7684\u80fd\u6e90\u6d88\u8017\u548c\u78b3\u6392\u653e\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u670d\u52a1\u8d28\u91cf\u548c\u516c\u5e73\u6027\uff0c\u4e3a\u53ef\u6301\u7eed\u7684AI\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.01001", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.01001", "abs": "https://arxiv.org/abs/2511.01001", "authors": ["Johansell Villalobos", "Daniel Caviedes-Voulli\u00e8me", "Silvio Rizzi", "Esteban Meneses"], "title": "Towards Portability at Scale: A Cross-Architecture Performance Evaluation of a GPU-enabled Shallow Water Solver", "comment": "Conference: SBAC-PAD 2025", "summary": "Current climate change has posed a grand challenge in the field of numerical\nmodeling due to its complex, multiscale dynamics. In hydrological modeling, the\nincreasing demand for high-resolution, real-time simulations has led to the\nadoption of GPU-accelerated platforms and performance portable programming\nframeworks such as Kokkos. In this work, we present a comprehensive performance\nstudy of the SERGHEI-SWE solver, a shallow water equations code, across four\nstate-of-the-art heterogeneous HPC systems: Frontier (AMD MI250X), JUWELS\nBooster (NVIDIA A100), JEDI (NVIDIA H100), and Aurora (Intel Max 1550). We\nassess strong scaling up to 1024 GPUs and weak scaling upwards of 2048 GPUs,\ndemonstrating consistent scalability with a speedup of 32 and an efficiency\nupwards of 90\\% for most almost all the test range. Roofline analysis reveals\nthat memory bandwidth is the dominant performance bottleneck, with key solver\nkernels residing in the memory-bound region. To evaluate performance\nportability, we apply both harmonic and arithmetic mean-based metrics while\nvarying problem size. Results indicate that while SERGHEI-SWE achieves\nportability across devices with tuned problem sizes (<70\\%), there is room for\nkernel optimization within the solver with more granular control of the\narchitecture specifically by using Kokkos teams and architecture specific\ntunable parameters. These findings position SERGHEI-SWE as a robust, scalable,\nand portable simulation tool for large-scale geophysical applications under\nevolving HPC architectures with potential to enhance its performance.", "AI": {"tldr": "\u672c\u6587\u5bf9SERGHEI-SWE\u6d45\u6c34\u65b9\u7a0b\u6c42\u89e3\u5668\u5728\u56db\u79cd\u5f02\u6784HPC\u7cfb\u7edf\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u53ef\u79fb\u690d\u6027\uff0c\u540c\u65f6\u6307\u51fa\u5185\u5b58\u5e26\u5bbd\u662f\u4e3b\u8981\u6027\u80fd\u74f6\u9888\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u5bf9\u6570\u503c\u5efa\u6a21\u63d0\u51fa\u91cd\u5927\u6311\u6218\uff0c\u6c34\u6587\u5efa\u6a21\u5bf9\u9ad8\u5206\u8fa8\u7387\u5b9e\u65f6\u6a21\u62df\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4fc3\u4f7f\u91c7\u7528GPU\u52a0\u901f\u5e73\u53f0\u548c\u6027\u80fd\u53ef\u79fb\u690d\u7f16\u7a0b\u6846\u67b6\u3002", "method": "\u5728\u56db\u79cd\u5f02\u6784HPC\u7cfb\u7edf\uff08Frontier\u3001JUWELS Booster\u3001JEDI\u3001Aurora\uff09\u4e0a\u8bc4\u4f30SERGHEI-SWE\u6c42\u89e3\u5668\u7684\u5f3a\u6269\u5c55\u548c\u5f31\u6269\u5c55\u6027\u80fd\uff0c\u4f7f\u7528\u5c4b\u9876\u7ebf\u5206\u6790\u8bc6\u522b\u6027\u80fd\u74f6\u9888\uff0c\u5e94\u7528\u8c03\u548c\u4e0e\u7b97\u672f\u5747\u503c\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u53ef\u79fb\u690d\u6027\u3002", "result": "\u57281024\u4e2aGPU\u4e0a\u5b9e\u73b032\u500d\u52a0\u901f\uff0c\u6548\u7387\u8d85\u8fc790%\uff1b\u5c4b\u9876\u7ebf\u5206\u6790\u663e\u793a\u5185\u5b58\u5e26\u5bbd\u662f\u4e3b\u8981\u74f6\u9888\uff1b\u6027\u80fd\u53ef\u79fb\u690d\u6027\u5728\u8c03\u6574\u95ee\u9898\u5927\u5c0f\u65f6\u53ef\u8fbe70%\u3002", "conclusion": "SERGHEI-SWE\u662f\u4e00\u4e2a\u7a33\u5065\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u79fb\u690d\u7684\u6a21\u62df\u5de5\u5177\uff0c\u4f46\u5728\u5185\u6838\u4f18\u5316\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u53ef\u901a\u8fc7Kokkos\u56e2\u961f\u548c\u67b6\u6784\u7279\u5b9a\u53c2\u6570\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2511.01127", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01127", "abs": "https://arxiv.org/abs/2511.01127", "authors": ["Fabio Diniz Rossi"], "title": "Neuro-Inspired Task Offloading in Edge-IoT Networks Using Spiking Neural Networks", "comment": "5 pages, 2 figures, 1 table", "summary": "Traditional task offloading strategies in edge computing often rely on static\nheuristics or data-intensive machine learning models, which are not always\nsuitable for highly dynamic and resource-constrained environments. In this\npaper, we propose a novel task-offloading framework based on Spiking Neural\nNetworks inspired by the efficiency and adaptability of biological neural\nsystems. Our approach integrates an SNN-based decision module into edge nodes\nto perform real-time, energy-efficient task orchestration. We evaluate the\nmodel under various IoT workload scenarios using a hybrid simulation\nenvironment composed of YAFS and Brian2. The results demonstrate that our\nSNN-based framework significantly reduces task processing latency and energy\nconsumption while improving task success rates. Compared to traditional\nheuristic and ML-based strategies, our model achieves up to 26% lower latency,\n32% less energy consumption, and 25\\% higher success rate under high-load\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u8fb9\u7f18\u8ba1\u7b97\u4efb\u52a1\u5378\u8f7d\u6846\u67b6\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548c\u80fd\u8017", "motivation": "\u4f20\u7edf\u4efb\u52a1\u5378\u8f7d\u7b56\u7565\u5728\u9ad8\u5ea6\u52a8\u6001\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5c06SNN\u51b3\u7b56\u6a21\u5757\u96c6\u6210\u5230\u8fb9\u7f18\u8282\u70b9\uff0c\u4f7f\u7528YAFS\u548cBrian2\u6df7\u5408\u4eff\u771f\u73af\u5883\u8bc4\u4f30", "result": "\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u5ef6\u8fdf\u964d\u4f4e26%\uff0c\u80fd\u8017\u51cf\u5c1132%\uff0c\u9ad8\u8d1f\u8f7d\u4e0b\u6210\u529f\u7387\u63d0\u9ad825%", "conclusion": "SNN\u6846\u67b6\u5728\u8fb9\u7f18\u8ba1\u7b97\u4efb\u52a1\u5378\u8f7d\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u80fd\u6548"}}
{"id": "2511.01235", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01235", "abs": "https://arxiv.org/abs/2511.01235", "authors": ["Shruthi Kannappan", "Ashwina Kumar", "Rupesh Nasre"], "title": "Scalable Maxflow Processing for Dynamic Graphs", "comment": null, "summary": "The Maximum Flow (Max-Flow) problem is a cornerstone in graph theory and\ncombinatorial optimization, aiming to determine the largest possible flow from\na designated source node to a sink node within a capacitated flow network. It\nhas extensive applications across diverse domains such as computer networking,\ntransportation systems, and image segmentation. The objective is to maximize\nthe total throughput while respecting edge capacity constraints and maintaining\nflow conservation at all intermediate vertices.\n  Among the various algorithms proposed for solving the Max-Flow problem, the\nPush--Relabel algorithm is particularly notable for its efficiency and\nsuitability for parallelization, owing to its localized vertex-based\noperations. This property has motivated extensive research into GPU-accelerated\nMax-Flow computation, leveraging the high degree of parallelism inherent to\nmodern GPU architectures.\n  In this paper, we present a novel GPU-parallel Max-Flow algorithm capable of\nincrementally recomputing the maximum flow of a dynamic graph following a batch\nof edge updates. In addition, we introduce a high-performance static GPU\nalgorithm designed for efficiently computing the initial Max-Flow on static\ngraphs. We further describe a series of CUDA-specific implementation\noptimizations that enhance performance, scalability, and memory efficiency on\nGPU platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684GPU\u5e76\u884c\u6700\u5927\u6d41\u7b97\u6cd5\uff0c\u80fd\u591f\u52a8\u6001\u66f4\u65b0\u56fe\u7684\u6700\u5927\u6d41\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9ad8\u6027\u80fd\u7684\u9759\u6001GPU\u7b97\u6cd5\u548cCUDA\u4f18\u5316\u65b9\u6848\u3002", "motivation": "\u6700\u5927\u6d41\u95ee\u9898\u662f\u56fe\u8bba\u548c\u7ec4\u5408\u4f18\u5316\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u5728\u8ba1\u7b97\u673a\u7f51\u7edc\u3001\u4ea4\u901a\u7cfb\u7edf\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u3002Push-Relabel\u7b97\u6cd5\u56e0\u5176\u5c40\u90e8\u9876\u70b9\u64cd\u4f5c\u7279\u6027\u9002\u5408\u5e76\u884c\u5316\uff0c\u8fd9\u6fc0\u53d1\u4e86\u5229\u7528GPU\u5e76\u884c\u67b6\u6784\u8fdb\u884c\u6700\u5927\u6d41\u8ba1\u7b97\u7684\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86GPU\u5e76\u884c\u6700\u5927\u6d41\u7b97\u6cd5\uff0c\u652f\u6301\u52a8\u6001\u56fe\u7684\u6279\u91cf\u8fb9\u66f4\u65b0\u540e\u589e\u91cf\u91cd\u8ba1\u7b97\u6700\u5927\u6d41\uff1b\u8bbe\u8ba1\u4e86\u9ad8\u6027\u80fd\u9759\u6001GPU\u7b97\u6cd5\u8ba1\u7b97\u521d\u59cb\u6700\u5927\u6d41\uff1b\u5b9e\u73b0\u4e86\u4e00\u7cfb\u5217CUDA\u7279\u5b9a\u7684\u4f18\u5316\u6280\u672f\u3002", "result": "\u7b97\u6cd5\u5728GPU\u5e73\u53f0\u4e0a\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3001\u53ef\u6269\u5c55\u6027\u548c\u5185\u5b58\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684GPU\u5e76\u884c\u6700\u5927\u6d41\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u52a8\u6001\u56fe\u7684\u6700\u5927\u6d41\u8ba1\u7b97\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.01255", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01255", "abs": "https://arxiv.org/abs/2511.01255", "authors": ["He Chen", "ZiHua Zheng", "JingHua Sun"], "title": "Design of quasi phase matching crystal based on differential gray wolf algorithm", "comment": null, "summary": "This paper focuses on the key problem in the development of nonlinear optical\ntechnology, the performance optimization of aperiodically polarized crystals.\nThe performance of the crystal depends on the precise control of the micro\ndistribution of crystal domains, but its optimization belongs to the\nhigh-dimensional discrete combination \"NP hard\" problem. The traditional\nalgorithm has the bottleneck of slow convergence and easy to fall into local\noptimization, while the heuristic methods such as genetic algorithm are limited\nby the CPU serial calculation and inefficient. In order to solve the above\nchallenges, this paper proposes the fusion scheme of hwsda hybrid optimization\nalgorithm and GPU parallel acceleration technology: the differential evolution\nalgorithm (DE) is used to realize the global search, and the gray wolf\noptimization algorithm (GWO) is used to strengthen the local search and\nconvergence speed, and the two coordinate to balance the global and local\noptimization requirements; At the same time, it relies on GPU multi-core\narchitecture to realize thread level parallel computing and improve\noptimization efficiency. This scheme effectively breaks through the\noptimization problem of high-dimensional discrete space, improves the accuracy\nof crystal domain control, improves the efficiency of quasi phase matching\ndesign by hundreds to thousands of times compared with traditional CPU serial\ncomputing, provides a new paradigm for the design of complex nonlinear optical\ndevices, and helps promote the performance breakthrough and industrial\napplication of related devices in the fields of quantum optics and laser\nprocessing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u6df7\u5408\u4f18\u5316\u7b97\u6cd5\u548cGPU\u5e76\u884c\u52a0\u901f\u6280\u672f\u7684\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u5468\u671f\u6781\u5316\u6676\u4f53\u6027\u80fd\u4f18\u5316\u8fd9\u4e00\u9ad8\u7ef4\u79bb\u6563\u7ec4\u5408NP\u96be\u9898\u3002", "motivation": "\u4f20\u7edf\u7b97\u6cd5\u5728\u975e\u5468\u671f\u6781\u5316\u6676\u4f53\u4f18\u5316\u4e2d\u5b58\u5728\u6536\u655b\u6162\u3001\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\uff0c\u800c\u542f\u53d1\u5f0f\u65b9\u6cd5\u53d7\u9650\u4e8eCPU\u4e32\u884c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u91c7\u7528\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5(DE)\u8fdb\u884c\u5168\u5c40\u641c\u7d22\uff0c\u7070\u72fc\u4f18\u5316\u7b97\u6cd5(GWO)\u52a0\u5f3a\u5c40\u90e8\u641c\u7d22\u548c\u6536\u655b\u901f\u5ea6\uff0c\u4e24\u8005\u534f\u540c\u5e73\u8861\u5168\u5c40\u548c\u5c40\u90e8\u4f18\u5316\u9700\u6c42\uff0c\u5e76\u5229\u7528GPU\u591a\u6838\u67b6\u6784\u5b9e\u73b0\u7ebf\u7a0b\u7ea7\u5e76\u884c\u8ba1\u7b97\u3002", "result": "\u8be5\u65b9\u6848\u6709\u6548\u7a81\u7834\u4e86\u9ad8\u7ef4\u79bb\u6563\u7a7a\u95f4\u7684\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6676\u4f53\u57df\u63a7\u5236\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edfCPU\u4e32\u884c\u8ba1\u7b97\uff0c\u51c6\u76f8\u4f4d\u5339\u914d\u8bbe\u8ba1\u6548\u7387\u63d0\u5347\u4e86\u6570\u767e\u81f3\u6570\u5343\u500d\u3002", "conclusion": "\u4e3a\u590d\u6742\u975e\u7ebf\u6027\u5149\u5b66\u5668\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u91cf\u5b50\u5149\u5b66\u548c\u6fc0\u5149\u52a0\u5de5\u7b49\u9886\u57df\u76f8\u5173\u5668\u4ef6\u7684\u6027\u80fd\u7a81\u7834\u548c\u5de5\u4e1a\u5e94\u7528\u3002"}}
{"id": "2511.01333", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01333", "abs": "https://arxiv.org/abs/2511.01333", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Hassan Rizwan", "Sagnik Bhattacharya", "Muhammad Ali Jamshed", "John M. Cioffi"], "title": "Transformer-Based Sparse CSI Estimation for Non-Stationary Channels", "comment": "ICC 2026", "summary": "Accurate and efficient estimation of Channel State Information (CSI) is\ncritical for next-generation wireless systems operating under non-stationary\nconditions, where user mobility, Doppler spread, and multipath dynamics rapidly\nalter channel statistics. Conventional pilot aided estimators incur substantial\noverhead, while deep learning approaches degrade under dynamic pilot patterns\nand time varying fading. This paper presents a pilot-aided Flash-Attention\nTransformer framework that unifies model-driven pilot acquisition with data\ndriven CSI reconstruction through patch-wise self-attention and a physics aware\ncomposite loss function enforcing phase alignment, correlation consistency, and\ntime frequency smoothness. Under a standardized 3GPP NR configuration, the\nproposed framework outperforms LMMSE and LSTM baselines by approximately 13 dB\nin phase invariant normalized mean-square error (NMSE) with markedly lower\nbit-error rate (BER), while reducing pilot overhead by 16 times. These results\ndemonstrate that attention based architectures enable reliable CSI recovery and\nenhanced spectral efficiency without compromising link quality, addressing a\nfundamental bottleneck in adaptive, low-overhead channel estimation for\nnon-stationary 5G and beyond-5G networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFlash-Attention Transformer\u7684CSI\u4f30\u8ba1\u6846\u67b6\uff0c\u5728\u975e\u5e73\u7a33\u4fe1\u9053\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u5bfc\u9891\u5f00\u9500\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u5728\u975e\u5e73\u7a33\u6761\u4ef6\u4e0b\u8fd0\u884c\uff0c\u4f20\u7edf\u5bfc\u9891\u8f85\u52a9\u4f30\u8ba1\u5668\u5f00\u9500\u5927\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u52a8\u6001\u5bfc\u9891\u6a21\u5f0f\u548c\u65f6\u53d8\u8870\u843d\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u9a71\u52a8\u5bfc\u9891\u83b7\u53d6\u4e0e\u6570\u636e\u9a71\u52a8CSI\u91cd\u5efa\uff0c\u4f7f\u7528\u8865\u4e01\u7ea7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u7269\u7406\u611f\u77e5\u590d\u5408\u635f\u5931\u51fd\u6570\uff0c\u786e\u4fdd\u76f8\u4f4d\u5bf9\u9f50\u3001\u76f8\u5173\u6027\u4e00\u81f4\u6027\u548c\u65f6\u9891\u5e73\u6ed1\u6027\u3002", "result": "\u57283GPP NR\u914d\u7f6e\u4e0b\uff0c\u6bd4LMMSE\u548cLSTM\u57fa\u7ebf\u5728\u76f8\u4f4d\u4e0d\u53d8\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\u4e0a\u63d0\u5347\u7ea613dB\uff0c\u8bef\u7801\u7387\u663e\u8457\u964d\u4f4e\uff0c\u540c\u65f6\u5bfc\u9891\u5f00\u9500\u51cf\u5c1116\u500d\u3002", "conclusion": "\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u67b6\u6784\u80fd\u591f\u5728\u4fdd\u8bc1\u94fe\u8def\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u53ef\u9760\u7684CSI\u6062\u590d\u548c\u589e\u5f3a\u7684\u9891\u8c31\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u975e\u5e73\u7a335G\u53ca\u540e5G\u7f51\u7edc\u4e2d\u81ea\u9002\u5e94\u3001\u4f4e\u5f00\u9500\u4fe1\u9053\u4f30\u8ba1\u7684\u57fa\u672c\u74f6\u9888\u3002"}}
{"id": "2511.01420", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01420", "abs": "https://arxiv.org/abs/2511.01420", "authors": ["Christoph Lenzen"], "title": "Gradient Clock Synchronization with Practically Constant Local Skew", "comment": "38 pages, no figures, submitted to STOC 2026", "summary": "Gradient Clock Synchronization (GCS) is the task of minimizing the local\nskew, i.e., the clock offset between neighboring clocks, in a larger network.\nWhile asymptotically optimal bounds are known, from a practical perspective\nthey have crucial shortcomings:\n  - Local skew bounds are determined by upper bounds on offset estimation that\nneed to be guaranteed throughout the entire lifetime of the system.\n  - Worst-case frequency deviations of local oscillators from their nominal\nrate are assumed, yet frequencies tend to be much more stable in the (relevant)\nshort term.\n  State-of-the-art deployed synchronization methods adapt to the true offset\nmeasurement and frequency errors, but achieve no non-trivial guarantees on the\nlocal skew.\n  In this work, we provide a refined model and novel analysis of existing\ntechniques for solving GCS in this model. By requiring only stability of\nmeasurement and frequency errors, we can circumvent existing lower bounds,\nleading to dramatic improvements under very general conditions. For example, if\nlinks exhibit a uniform worst-case estimation error of $\\Delta$ and a change in\nestimation errors of $\\delta\\ll \\Delta$ on relevant time scales, we bound the\nlocal skew by $O(\\Delta+\\delta \\log D)$ for networks of diameter $D$,\neffectively ``breaking'' the established $\\Omega(\\Delta\\log D)$ lower bound,\nwhich holds when $\\delta=\\Delta$. Similarly, we show how to limit the influence\nof local oscillators on $\\delta$ to scale with the change of frequency of an\nindividual oscillator on relevant time scales, rather than a worst-case bound\nover all oscillators and the lifetime of the system.\n  Moreover, we show how to ensure self-stabilization in this challenging\nsetting. Last, but not least, we extend all of our results to the scenario of\nexternal synchronization, at the cost of a limited increase in stabilization\ntime.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u68af\u5ea6\u65f6\u949f\u540c\u6b65\u6a21\u578b\uff0c\u901a\u8fc7\u8003\u8651\u6d4b\u91cf\u8bef\u5dee\u548c\u9891\u7387\u8bef\u5dee\u7684\u7a33\u5b9a\u6027\u800c\u975e\u6700\u574f\u60c5\u51b5\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u7684\u4e0b\u754c\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u540c\u6b65\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GCS\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u672c\u5730\u504f\u79fb\u754c\u9650\u4f9d\u8d56\u4e8e\u6574\u4e2a\u7cfb\u7edf\u751f\u547d\u5468\u671f\u5185\u7684\u504f\u79fb\u4f30\u8ba1\u4e0a\u754c\uff1b2) \u5047\u8bbe\u672c\u5730\u632f\u8361\u5668\u9891\u7387\u5b58\u5728\u6700\u574f\u60c5\u51b5\u504f\u5dee\uff0c\u800c\u5b9e\u9645\u4e2d\u9891\u7387\u5728\u77ed\u671f\u5185\u66f4\u7a33\u5b9a\u3002\u73b0\u6709\u90e8\u7f72\u65b9\u6cd5\u867d\u7136\u9002\u5e94\u771f\u5b9e\u8bef\u5dee\uff0c\u4f46\u65e0\u6cd5\u63d0\u4f9b\u975e\u5e73\u51e1\u7684\u672c\u5730\u504f\u79fb\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u6a21\u578b\u548c\u73b0\u6709\u6280\u672f\u7684\u91cd\u65b0\u5206\u6790\uff0c\u4ec5\u8981\u6c42\u6d4b\u91cf\u548c\u9891\u7387\u8bef\u5dee\u7684\u7a33\u5b9a\u6027\uff0c\u4ece\u800c\u89c4\u907f\u73b0\u6709\u4e0b\u754c\u3002\u901a\u8fc7\u8003\u8651\u76f8\u5173\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u8bef\u5dee\u53d8\u5316\u800c\u975e\u6700\u574f\u60c5\u51b5\uff0c\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "result": "\u5728\u94fe\u8def\u5177\u6709\u7edf\u4e00\u6700\u574f\u60c5\u51b5\u4f30\u8ba1\u8bef\u5dee\u0394\u548c\u76f8\u5173\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8bef\u5dee\u53d8\u5316\u03b4\u226a\u0394\u65f6\uff0c\u672c\u5730\u504f\u79fb\u754c\u9650\u4e3aO(\u0394+\u03b4 log D)\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u7684\u03a9(\u0394 log D)\u4e0b\u754c\u3002\u540c\u65f6\u9650\u5236\u4e86\u672c\u5730\u632f\u8361\u5668\u5bf9\u03b4\u7684\u5f71\u54cd\uff0c\u4f7f\u5176\u4ec5\u4e0e\u5355\u4e2a\u632f\u8361\u5668\u5728\u76f8\u5173\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u9891\u7387\u53d8\u5316\u76f8\u5173\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u4fdd\u8bc1\u81ea\u7a33\u5b9a\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68af\u5ea6\u65f6\u949f\u540c\u6b65\u6027\u80fd\uff0c\u5e76\u53ef\u5c06\u7ed3\u679c\u6269\u5c55\u5230\u5916\u90e8\u540c\u6b65\u573a\u666f\uff0c\u4ec5\u9700\u6709\u9650\u7684\u7a33\u5b9a\u65f6\u95f4\u589e\u52a0\u3002"}}
{"id": "2511.01573", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01573", "abs": "https://arxiv.org/abs/2511.01573", "authors": ["Melanie Tonarelli", "Simone Riva", "Pietro Benedusi", "Fabrizio Ferrandi", "Rolf Krause"], "title": "Adaptive Multidimensional Quadrature on Multi-GPU Systems", "comment": "9 pages, 8 figures. Submitted to the proceedings of the 29th\n  International Conference on Domain Decomposition Methods (DD29)", "summary": "We introduce a distributed adaptive quadrature method that formulates\nmultidimensional integration as a hierarchical domain decomposition problem on\nmulti-GPU architectures. The integration domain is recursively partitioned into\nsubdomains whose refinement is guided by local error estimators. Each subdomain\nevolves independently on a GPU, which exposes a significant load imbalance as\nthe adaptive process progresses. To address this challenge, we introduce a\ndecentralised load redistribution schemes based on a cyclic round-robin policy.\nThis strategy dynamically rebalance subdomains across devices through\nnon-blocking, CUDA-aware MPI communication that overlaps with computation. The\nproposed strategy has two main advantages compared to a state-of-the-art\nGPU-tailored package: higher efficiency in high dimensions; and improved\nrobustness w.r.t the integrand regularity and the target accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u81ea\u9002\u5e94\u6c42\u79ef\u65b9\u6cd5\uff0c\u5c06\u591a\u7ef4\u79ef\u5206\u8f6c\u5316\u4e3a\u591aGPU\u67b6\u6784\u4e0a\u7684\u5206\u5c42\u57df\u5206\u89e3\u95ee\u9898\uff0c\u901a\u8fc7\u5faa\u73af\u8f6e\u8be2\u7b56\u7565\u5b9e\u73b0\u8d1f\u8f7d\u91cd\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u591a\u7ef4\u79ef\u5206\u8ba1\u7b97\u4e2d\u968f\u7740\u81ea\u9002\u5e94\u8fc7\u7a0b\u8fdb\u5c55\u800c\u51fa\u73b0\u7684\u663e\u8457\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u9ad8\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u57df\u5206\u89e3\u65b9\u6cd5\uff0c\u5c06\u79ef\u5206\u57df\u9012\u5f52\u5212\u5206\u4e3a\u5b50\u57df\uff0c\u57fa\u4e8e\u5c40\u90e8\u8bef\u5dee\u4f30\u8ba1\u5668\u6307\u5bfc\u7ec6\u5316\uff0c\u4f7f\u7528\u57fa\u4e8e\u5faa\u73af\u8f6e\u8be2\u7b56\u7565\u7684\u5206\u6563\u8d1f\u8f7d\u91cd\u5206\u914d\u65b9\u6848\uff0c\u901a\u8fc7\u975e\u963b\u585e\u7684CUDA\u611f\u77e5MPI\u901a\u4fe1\u4e0e\u8ba1\u7b97\u91cd\u53e0\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684GPU\u5b9a\u5236\u5305\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u5177\u6709\u66f4\u9ad8\u6548\u7387\uff0c\u5bf9\u79ef\u5206\u51fd\u6570\u6b63\u5219\u6027\u548c\u76ee\u6807\u7cbe\u5ea6\u7684\u9c81\u68d2\u6027\u66f4\u597d\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u81ea\u9002\u5e94\u6c42\u79ef\u65b9\u6cd5\u5728\u591aGPU\u67b6\u6784\u4e0a\u6709\u6548\u89e3\u51b3\u4e86\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u9ad8\u7ef4\u79ef\u5206\u8ba1\u7b97\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.01843", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.01843", "abs": "https://arxiv.org/abs/2511.01843", "authors": ["Andrew Goodng", "Kevin Porter", "Thomas Lopatic", "Ashish Shinde", "Sunil Sayyaparaju", "Srinivasan Seshadri", "V. Srinivasan"], "title": "LARK - Linearizability Algorithms for Replicated Keys in Aerospike", "comment": "Submitted to Industry Track of a Database Conference", "summary": "We present LARK (Linearizability Algorithms for Replicated Keys), a\nsynchronous replication protocol that achieves linearizability while minimizing\nlatency and infrastructure cost, at significantly higher availability than\ntraditional quorum-log consensus. LARK introduces Partition Availability\nConditions (PAC) that reason over the entire database cluster rather than fixed\nreplica sets, improving partition availability under independent failures by\nroughly 3x when tolerating one failure and 10x when tolerating two. Unlike\nRaft, Paxos, and Viewstamped Replication, LARK eliminates ordered logs,\nenabling immediate partition readiness after leader changes -- with at most a\nper-key duplicate-resolution round trip when the new leader lacks the latest\ncopy. Under equal storage budgets -- where both systems maintain only f+1 data\ncopies to tolerate f failures -- LARK continues committing through data-node\nfailures while log-based protocols must pause commits for replica rebuilding.\nThese properties also enable zero-downtime rolling restarts even when\nmaintaining only two copies. We provide formal safety arguments and a TLA+\nspecification, and we demonstrate through analysis and experiments that LARK\nachieves significant availability gains.", "AI": {"tldr": "LARK\u662f\u4e00\u79cd\u540c\u6b65\u590d\u5236\u534f\u8bae\uff0c\u901a\u8fc7\u6d88\u9664\u6709\u5e8f\u65e5\u5fd7\u548c\u5f15\u5165\u5206\u533a\u53ef\u7528\u6027\u6761\u4ef6\uff0c\u5728\u7ebf\u6027\u4e00\u81f4\u6027\u4e0b\u663e\u8457\u63d0\u9ad8\u53ef\u7528\u6027\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u65e5\u5fd7\u7684\u5171\u8bc6\u534f\u8bae\uff08\u5982Raft\u3001Paxos\uff09\u5728\u6545\u969c\u6062\u590d\u65f6\u9700\u8981\u6682\u505c\u63d0\u4ea4\uff0c\u4e14\u53ef\u7528\u6027\u53d7\u9650\u3002LARK\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7684\u53ef\u7528\u6027\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002", "method": "\u5f15\u5165\u5206\u533a\u53ef\u7528\u6027\u6761\u4ef6\uff08PAC\uff09\uff0c\u57fa\u4e8e\u6574\u4e2a\u6570\u636e\u5e93\u96c6\u7fa4\u800c\u975e\u56fa\u5b9a\u526f\u672c\u96c6\u8fdb\u884c\u63a8\u7406\uff1b\u6d88\u9664\u6709\u5e8f\u65e5\u5fd7\uff0c\u4f7f\u5206\u533a\u5728\u9886\u5bfc\u8005\u53d8\u66f4\u540e\u7acb\u5373\u53ef\u7528\uff1b\u4ec5\u9700f+1\u6570\u636e\u526f\u672c\u5373\u53ef\u5bb9\u5fcdf\u6b21\u6545\u969c\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u534f\u8bae\uff0c\u5728\u5bb9\u5fcd1\u6b21\u6545\u969c\u65f6\u53ef\u7528\u6027\u63d0\u9ad8\u7ea63\u500d\uff0c\u5bb9\u5fcd2\u6b21\u6545\u969c\u65f6\u63d0\u9ad8\u7ea610\u500d\uff1b\u5728\u6570\u636e\u8282\u70b9\u6545\u969c\u65f6\u4ecd\u80fd\u7ee7\u7eed\u63d0\u4ea4\uff1b\u652f\u6301\u96f6\u505c\u673a\u6eda\u52a8\u91cd\u542f\u3002", "conclusion": "LARK\u5728\u7ebf\u6027\u4e00\u81f4\u6027\u4fdd\u8bc1\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u53ef\u7528\u6027\uff0c\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u57fa\u7840\u8bbe\u65bd\u6210\u672c\uff0c\u4e3a\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u590d\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
