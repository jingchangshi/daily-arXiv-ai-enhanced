<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 5]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Verifying Correctness of Shared Channels in a Cooperatively Scheduled Process-Oriented Language](https://arxiv.org/abs/2510.11751)
*Jan Pedersen,Kevin Chalmers*

Main category: cs.PL

TL;DR: 使用FDR工具分析协作调度运行时中共享通信通道的行为，验证ProcessJ语言中通道实现的正确性，发现正确行为依赖于充足的执行资源


<details>
  <summary>Details</summary>
Motivation: 理解并发组件在特定条件下的行为很重要，需要分析协作调度运行时中共享通信通道的行为

Method: 使用FDR精化检查和建模工具，开发共享通道的行为规范和ProcessJ语言中这些通道的实现模型

Result: 虽然可以实现正确的通道行为，但结果取决于是否有足够资源来执行所有相关进程

Conclusion: 建模并发组件的运行时环境对于确保组件在现实世界中按规范行为是必要的

Abstract: Correct concurrent behaviour is important in understanding how components
will act within certain conditions. In this work. we analyse the behaviour of
shared communicating channels within a coorporatively scheduled runtime. We use
the refinement checking and modelling tool FDR to develop both specifications
of how such shared channels should behave and models of the implementations of
these channels in the cooperatively scheduled language ProcessJ. Our results
demonstrate that although we can certainly implement the correct behaviour of
such channels, the outcome is dependant on having adequate resources available
to execute all processes involved. We conclude that modelling the runtime
environment of concurrent components is necessary to ensure components behave
as specified in the real world.

</details>


### [2] [AwareCompiler: Agentic Context-Aware Compiler Optimization via a Synergistic Knowledge-Data Driven Framework](https://arxiv.org/abs/2510.11759)
*Hongyu Lin,Haolin Pan,Haoran Luo,Yuchen Li,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.PL

TL;DR: AwareCompiler是一个基于LLM的编译器优化代理框架，通过结构化知识集成、知识驱动自适应pass生成和数据驱动混合训练来解决语义对齐、交互效率和奖励稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 自动化编译器优化面临三大挑战：(1)抽象程序表示与具体优化pass之间的语义不对齐，(2)代理与编译器环境间的低效交互机制，(3)大优化空间中决策过程的奖励稀疏性。

Method: 提出三个关键创新：结构化知识集成与数据集构建、知识驱动自适应pass生成、数据驱动混合训练管道。

Result: 在标准基准测试中，AwareCompiler在性能和效率上显著优于现有基线方法。

Conclusion: 该框架展示了知识-数据驱动协同方法的有效性，为编译器优化自动化提供了新思路。

Abstract: Compiler optimization is crucial for enhancing program performance by
transforming the sequence of optimization passes while maintaining correctness.
Despite the promising potential of large language models (LLMs)-based agent for
software optimization, automating compiler optimization remains challenging due
to: (1) semantic misalignment between abstract program representations and
concrete optimization passes, (2) inefficient interaction mechanisms between
agents and compiler environments, and (3) reward sparsity from the extensive
decision-making process within large optimization spaces. This paper introduces
\textbf{AwareCompiler}, an agentic framework for compiler optimization that
addresses these challenges through three key innovations: structured knowledge
integration and dataset construction, knowledge-driven adaptive pass
generation, and data-driven hybrid training pipeline. Experimental results on
standard benchmarks demonstrate that AwareCompiler significantly outperforms
existing baselines in both performance and efficiency, highlighting the
effectiveness of our synergistic knowledge-data-driven approach. Our code is
publicly available at https://github.com/LHY-24/AwareCompiler.

</details>


### [3] [Functional Reasoning for Distributed Systems with Failures](https://arxiv.org/abs/2510.12131)
*Haobin Ni,Robbert van Renesse,Greg Morrisett*

Main category: cs.PL

TL;DR: 本文通过设计Sync和Async两种语言，为分布式系统的非形式化Hoare风格推理提供了形式化基础，并证明了在Sync中证明的安全属性在编译到Async后仍然保持。


<details>
  <summary>Details</summary>
Motivation: 分布式系统理论中使用的非形式化Hoare风格推理虽然直观，但可能存在错误，且与形式化证明的对应关系不明确，需要建立形式化基础。

Method: 设计Sync和Async两种语言：Sync作为同步数据并行程序，具有函数式指称语义；Async作为异步交互式程序集合，具有基于轨迹的操作语义。Sync可编译到Async，并证明安全属性在编译过程中保持。

Result: 在Rocq中实现了这两种语言，并验证了两个容错共识协议（BOSCO和SeqPaxos）的安全属性。

Conclusion: 该方法为分布式系统的组合式形式推理提供了理论基础，将直观的非形式化推理与标准形式化方法连接起来，适用于包括拜占庭故障在内的各类分布式系统。

Abstract: Distributed system theory literature often argues for correctness using an
informal, Hoare-like style of reasoning. While these arguments are intuitive,
they have not all been foolproof, and whether they directly correspond to
formal proofs is in question. We formally ground this kind of reasoning and
connect it to standard formal approaches through language design and
meta-analysis, which leads to a functional style of compositional formal
reasoning for a class of distributed systems, including cases involving
Byzantine faults. The core of our approach is twin languages: Sync and Async,
which formalize the insight from distributed system theory that an asynchronous
system can be reduced to a synchronous system for more straightforward
reasoning under certain conditions. Sync describes a distributed system as a
single, synchronous, data-parallel program. It restricts programs syntactically
and has a functional denotational semantics suitable for Hoare-style formal
reasoning. Async models a distributed system as a collection of interacting
monadic programs, one for each non-faulty node in the system. It has a standard
trace-based operational semantics, modeling asynchrony with interleaving. Sync
compiles to Async and can then be extracted to yield executable code. We prove
that any safety property proven for a Sync program in its denotational
semantics is preserved in the operational semantics of its compiled Async
programs. We implement the twin languages in Rocq and verify the safety
properties of two fault-tolerant consensus protocols: BOSCO and SeqPaxos.

</details>


### [4] [Operational methods in semantics](https://arxiv.org/abs/2510.12295)
*Roberto M. Amadio*

Main category: cs.PL

TL;DR: 这些讲义笔记聚焦于编程语言操作语义的抽象模型、基本思想和结果，采用从程序计算步骤的抽象描述出发，逐步构建语义等价性、规范语言和静态分析的方法。


<details>
  <summary>Details</summary>
Motivation: 操作语义方法需要适度的数学复杂度，能够很好地扩展到各种编程特性，是构建可移植语言实现、指定和测试程序属性的合适框架。

Method: 从程序计算步骤的抽象描述开始，逐步构建语义等价性、规范语言和静态分析。

Result: 操作语义方法在实践中被证明是有效的，能够处理从基本语言实现到编译器正确性证明等更复杂的任务。

Conclusion: 操作语义是编程语言语义学中特别有效的方法，它平衡了数学严谨性和实际应用性，适用于各种规模的编程语言分析和实现任务。

Abstract: The focus of these lecture notes is on abstract models and basic ideas and
results that relate to the operational semantics of programming languages
largely conceived. The approach is to start with an abstract description of the
computation steps of programs and then to build on top semantic equivalences,
specification languages, and static analyses. While other approaches to the
semantics of programming languages are possible, it appears that the
operational one is particularly effective in that it requires a moderate level
of mathematical sophistication and scales reasonably well to a large variety of
programming features. In practice, operational semantics is a suitable
framework to build portable language implementations and to specify and test
program properties. It is also used routinely to tackle more ambitious tasks
such as proving the correctness of a compiler or a static analyzer.

</details>


### [5] [GUPPY: Pythonic Quantum-Classical Programming](https://arxiv.org/abs/2510.12582)
*Mark Koch,Alan Lawrence,Kartik Singhal,Seyon Sivarajah,Ross Duncan*

Main category: cs.PL

TL;DR: Guppy是一个嵌入Python的领域特定语言，用于编写具有复杂控制流的高层次混合量子程序，目标是能在真实量子硬件上运行。


<details>
  <summary>Details</summary>
Motivation: 量子编程需要处理复杂控制流，现有工具难以在Pythonic语法下实现高层次混合量子程序的编写和硬件执行。

Method: 开发嵌入Python的领域特定语言Guppy，支持Pythonic语法编写混合量子程序，包含复杂控制流功能。

Result: 提出了Guppy语言框架，能够表达高层次混合量子程序，为在真实量子硬件上执行量子代码提供了解决方案。

Conclusion: Guppy作为嵌入式DSL，为量子编程提供了更自然和强大的表达方式，有助于推动量子计算的实际应用。

Abstract: We present ongoing work on Guppy, a domain-specific language embedded in
Python that allows users to write high-level hybrid quantum programs with
complex control flow in Pythonic syntax, aiming to run them on actual quantum
hardware.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters](https://arxiv.org/abs/2510.11938)
*Yanying Lin,Shijie Peng,Chengzhi Lu,Chengzhong Xu,Kejiang Ye*

Main category: cs.DC

TL;DR: FlexPipe通过动态重构流水线架构，在运行时根据请求模式调整流水线粒度，解决了LLM服务中的资源碎片化和效率问题


<details>
  <summary>Details</summary>
Motivation: 现有系统依赖静态流水线配置，无法适应动态工作负载，导致资源效率低下和GPU碎片化问题

Method: 将模型分解为细粒度阶段，基于实时请求模式分析智能调整流水线粒度，包含三个关键创新：细粒度模型分区、飞行中流水线重构和拓扑感知资源分配

Result: 在82-GPU集群上评估，FlexPipe实现了8.5倍资源效率提升，延迟降低38.3%，GPU预留需求从峰值容量的75%降至30%

Conclusion: FlexPipe通过动态流水线重构显著提升了LLM服务的资源效率和性能，有效解决了生产环境中的资源碎片化问题

Abstract: Serving Large Language Models (LLMs) in production faces significant
challenges from highly variable request patterns and severe resource
fragmentation in serverless clusters. Current systems rely on static pipeline
configurations that struggle to adapt to dynamic workload conditions, leading
to substantial inefficiencies. We present FlexPipe, a novel system that
dynamically reconfigures pipeline architectures during runtime to address these
fundamental limitations. FlexPipe decomposes models into fine-grained stages
and intelligently adjusts pipeline granularity based on real-time request
pattern analysis, implementing three key innovations: fine-grained model
partitioning with preserved computational graph constraints, inflight pipeline
refactoring with consistent cache transitions, and topology-aware resource
allocation that navigates GPU fragmentation. Comprehensive evaluation on an
82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource
efficiency while maintaining 38.3% lower latency compared to state-of-the-art
systems, reducing GPU reservation requirements from 75% to 30% of peak
capacity.

</details>


### [7] [Comparing Cross-Platform Performance via Node-to-Node Scaling Studies](https://arxiv.org/abs/2510.12166)
*Kenneth Weiss,Thomas M. Stitt,Daryl Hawkins,Olga Pearce,Stephanie Brink,Robert N. Rieben*

Main category: cs.DC

TL;DR: 本文提出以单计算节点作为跨平台性能比较的基础单元，并提供了节点到节点扩展研究的设置、运行和分析指导。


<details>
  <summary>Details</summary>
Motivation: 由于高性能计算架构日益多样化，研究人员和实践者越来越需要在不同平台上比较代码的性能和可扩展性，但缺乏如何进行此类跨平台研究的指导。

Method: 将单计算节点作为跨平台比较的自然基础单元，提供节点到节点扩展研究的设置、运行和分析指导，包括结果展示模板。

Result: 通过多个案例研究展示了这种方法的优势，证明了以节点为基础进行跨平台性能比较的有效性。

Conclusion: 以单计算节点为基准进行跨平台扩展研究是可行且有效的方法，为不同架构间的性能比较提供了实用指导。

Abstract: Due to the increasing diversity of high-performance computing architectures,
researchers and practitioners are increasingly interested in comparing a code's
performance and scalability across different platforms. However, there is a
lack of available guidance on how to actually set up and analyze such
cross-platform studies. In this paper, we contend that the natural base unit of
computing for such studies is a single compute node on each platform and offer
guidance in setting up, running, and analyzing node-to-node scaling studies. We
propose templates for presenting scaling results of these studies and provide
several case studies highlighting the benefits of this approach.

</details>


### [8] [GPU-Accelerated Algorithms for Process Mapping](https://arxiv.org/abs/2510.12196)
*Petr Samoldekin,Christian Schulz,Henning Woydt*

Main category: cs.DC

TL;DR: 提出了两种GPU加速的进程映射算法，用于在超级计算机上分配任务图的顶点，以平衡计算负载并最小化通信成本。


<details>
  <summary>Details</summary>
Motivation: 受GPU图像分割器近期成功的启发，开发GPU加速的进程映射算法以提高性能。

Method: 第一种算法采用分层多分割，沿超级计算机层次结构分割任务图；第二种算法将进程映射直接集成到现代多级图分割流程中，利用GPU并行性加速关键阶段。

Result: 两种方法相比最先进的CPU算法实现了超过300倍的加速。第一种算法通信成本平均增加约10%，第二种算法速度更快但解决方案质量较低。

Conclusion: 这是首个基于GPU的进程映射算法，在保持竞争力的同时显著提升了性能。

Abstract: Process mapping asks to assign vertices of a task graph to processing
elements of a supercomputer such that the computational workload is balanced
while the communication cost is minimized. Motivated by the recent success of
GPU-based graph partitioners, we propose two GPU-accelerated algorithms for
this optimization problem. The first algorithm employs hierarchical
multisection, which partitions the task graph alongside the hierarchy of the
supercomputer. The method utilizes GPU-based graph partitioners to accelerate
the mapping process. The second algorithm integrates process mapping directly
into the modern multilevel graph partitioning pipeline. Vital phases like
coarsening and refinement are accelerated by exploiting the parallelism of
GPUs. In our experiments, both methods achieve speedups exceeding 300 when
compared to state-of-the-art CPU-based algorithms. The first algorithm has, on
average, about 10 percent greater communication costs and thus remains
competitive to CPU algorithms. The second approach is much faster, with a
geometric mean speedup of 77.6 and peak speedup of 598 at the cost of lower
solution quality. To our knowledge, these are the first GPU-based algorithms
for process mapping.

</details>


### [9] [Metronome: Efficient Scheduling for Periodic Traffic Jobs with Network and Priority Awareness](https://arxiv.org/abs/2510.12274)
*Hao Jiang,Meng Qin,Ruijie Kuai,Dandan Liang*

Main category: cs.DC

TL;DR: Metronome是一个面向云原生网络的网络感知和优先级感知调度机制，通过时间分复用和多目标优化策略，显著提升了分布式训练作业的性能和集群带宽利用率。


<details>
  <summary>Details</summary>
Motivation: 随着计算需求快速增长，云原生网络需要解决高效资源协调问题，特别是应对集群中网络带宽动态波动的挑战，支持具有周期性流量模式和动态带宽需求的分布式训练作业。

Method: 采用时间分复用方法，利用作业流量特征构建弹性网络资源分配模型；结合多目标优化策略，同时考虑延迟和作业优先级；通过监控集群执行重配置操作来适应动态环境。

Result: 在13个常见机器学习模型上的实验表明，相比现有Kubernetes调度机制，Metronome可将作业完成时间最多减少19.50%，平均带宽利用率最多提升23.20%。

Conclusion: Metronome能够有效提升集群资源利用率同时保证服务性能，为云原生网络中的分布式训练作业提供了高效的调度解决方案。

Abstract: With the rapid growth in computing power demand, cloud native networks have
emerged as a promising solution to address the challenges of efficient resource
coordination, particularly in coping with the dynamic fluctuations of network
bandwidth in clusters. We propose Metronome, a network-aware and priority-aware
scheduling mechanism for cloud native networks. This mechanism is designed to
support jobs that exhibit periodic traffic patterns and dynamic bandwidth
demands, particularly in the context of distributed training. Specifically,
Metronome employs a time-division multiplexing approach that leverages job
traffic characteristics to construct an elastic network resource allocation
model, enabling efficient bandwidth sharing across multiple jobs. In addition,
it incorporates a multi-objective optimization strategy, jointly considering
latency and job priorities to achieve globally optimal as well as dynamic
resource allocation. Finally, Metronome adapts to the dynamic environment by
monitoring the cluster and performing reconfiguration operations. Extensive
experiments with 13 common machine learning models demonstrate that Metronome
can enhance cluster resource utilization while guaranteeing service
performance. Compared with the existing Kubernetes scheduling mechanisms across
multiple scenarios, Metronome reduces job completion time by up to 19.50% while
improving average bandwidth utilization by up to 23.20%.

</details>


### [10] [A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines](https://arxiv.org/abs/2510.12354)
*Sepideh Masoudi,Mark Edward Michael Daly,Jannis Kiesel,Stefan Tai*

Main category: cs.DC

TL;DR: 提出基于Kubernetes的工具，支持在数据共享管道中延迟、非侵入式地应用云设计模式，无需修改服务源代码，同时收集能耗指标。


<details>
  <summary>Details</summary>
Motivation: 在联邦环境中构建消费者特定数据共享管道时，传统云设计模式的预定义和嵌入会损害模块化、降低可重用性，与管道动态、消费者驱动的特性冲突。

Method: 开发基于Kubernetes的工具，支持自动化模式注入，允许延迟应用选定的云设计模式，无需修改服务源代码。

Result: 该工具能够非侵入式地应用云设计模式，同时收集能耗指标，使开发者能够在保持管道灵活可组合结构的同时做出能耗感知决策。

Conclusion: 该方法解决了在可重用数据共享管道中集成云设计模式时的模块化和可重用性挑战，同时支持能耗优化。

Abstract: As data mesh architectures gain traction in federated environments,
organizations are increasingly building consumer-specific data-sharing
pipelines using modular, cloud-native transformation services. Prior work has
shown that structuring these pipelines with reusable transformation stages
enhances both scalability and energy efficiency. However, integrating
traditional cloud design patterns into such pipelines poses a challenge:
predefining and embedding patterns can compromise modularity, reduce
reusability, and conflict with the pipelines dynamic, consumer-driven nature.
To address this, we introduce a Kubernetes-based tool that enables the deferred
and non-intrusive application of selected cloud design patterns without
requiring changes to service source code. The tool supports automated pattern
injection and collects energy consumption metrics, allowing developers to make
energy-aware decisions while preserving the flexible, composable structure of
reusable data-sharing pipelines.

</details>


### [11] [TALP-Pages: An easy-to-integrate continuous performance monitoring framework](https://arxiv.org/abs/2510.12436)
*Valentin Seitz,Jordy Trilaksono,Marta Garcia-Gasulla*

Main category: cs.DC

TL;DR: TALP-Pages是一个易于集成的框架，为开发者提供代码性能的快速反馈，通过TALP工具收集性能指标并生成HTML报告，帮助在开发过程中早期检测性能退化。


<details>
  <summary>Details</summary>
Motivation: HPC代码开发中需要早期检测性能退化，并深入了解应用扩展行为，但现有工具在开销和后处理方面存在不足。

Method: 基于TALP工具实时收集性能指标，使用适合CI的文件夹结构存储数据，生成包含性能因子回归可视化和扩展效率表的HTML报告。

Result: 与基于追踪的工具相比，TALP-Pages在更严格的资源约束下能更快生成扩展效率表，在GENE-X CI设置中只需最小改动即可集成。

Conclusion: TALP-Pages提供了低开销、易集成的性能监控方案，能有效检测和解释性能变化，适合集成到开发工作流中。

Abstract: Ensuring good performance is a key aspect in the development of codes that
target HPC machines. As these codes are under active development, the necessity
to detect performance degradation early in the development process becomes
apparent. In addition, having meaningful insight into application scaling
behavior tightly coupled to the development workflow is helpful. In this paper,
we introduce TALP-Pages, an easy-to-integrate framework that enables developers
to get fast and in-repository feedback about their code performance using
established fundamental performance and scaling factors. The framework relies
on TALP, which enables the on-the-fly collection of these metrics. Based on a
folder structure suited for CI which contains the files generated by TALP,
TALP-Pages generates an HTML report with visualizations of the performance
factor regression as well as scaling-efficiency tables. We compare TALP-Pages
to tracing-based tools in terms of overhead and post-processing requirements
and find that TALP-Pages can produce the scaling-efficiency tables faster and
under tighter resource constraints. To showcase the ease of use and
effectiveness of this approach, we extend the current CI setup of GENE-X with
only minimal changes required and showcase the ability to detect and explain a
performance improvement.

</details>


### [12] [Low Latency, High Bandwidth Streaming of Experimental Data with EJFAT](https://arxiv.org/abs/2510.12597)
*Ilya Baldin,Michael Goodrich,Vardan Gyurjyan,Graham Heyes,Derek Howard,Yatish Kumar,David Lawrence,Brad Sawatzky,Stacey Sheldon,Carl Timmer*

Main category: cs.DC

TL;DR: EJFAT架构通过FPGA加速实现边缘到计算集群的负载均衡，支持实验数据流的直接处理，为JLab科学项目和未来数据中心提供高吞吐低延迟解决方案


<details>
  <summary>Details</summary>
Motivation: 解决实验数据流处理中的高吞吐量和低延迟需求，支持JLab科学项目及未来数据中心的实时数据采集和处理工作流

Method: 采用FPGA加速技术，实现压缩、分片、UDP包目标重定向(NAT)、解压缩和重组等功能，无缝集成边缘和集群计算

Result: 成功构建EJFAT负载均衡架构，与ESnet、LBNL等DOE设施协同工作，在实际数据源上验证了系统性能

Conclusion: EJFAT架构为时间关键型数据采集系统和数据中心工作流提供了有效的解决方案，具有推广应用价值

Abstract: Thomas Jefferson National Accelerator Facility (JLab) has partnered with
Energy Sciences Network (ESnet) to define and implement an edge to compute
cluster computational load balancing acceleration architecture. The ESnet-JLab
FPGA Accelerated Transport (EJFAT) architecture focuses on FPGA acceleration to
address compression, fragmentation, UDP packet destination redirection (Network
Address Translation (NAT)) and decompression and reassembly.
  EJFAT seamlessly integrates edge and cluster computing to support direct
processing of streamed experimental data. This will directly benefit the JLab
science program as well as data centers of the future that require high
throughput and low latency for both time-critical data acquisition systems and
data center workflows.
  The EJFAT project will be presented along with how it is synergistic with
other DOE activities such as an Integrated Research Infrastructure (IRI), and
recent results using data sources at JLab, an EJFAT LB at ESnet, and
computational cluster resources at Lawrence Berkeley National Laboratory
(LBNL).

</details>


### [13] [A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization of Banded Matrices](https://arxiv.org/abs/2510.12705)
*Evelyne Ringoot,Rabab Alomairy,Alan Edelman*

Main category: cs.DC

TL;DR: 本文提出了首个GPU算法，用于将带状矩阵简化为双对角形式，作为SVD计算的关键步骤。该算法克服了内存带宽限制，在GPU上实现了比CPU实现快100倍以上的性能。


<details>
  <summary>Details</summary>
Motivation: 带状矩阵到双对角形式的约简是SVD计算的关键步骤，传统上被认为不适合GPU计算，因为它是内存带宽受限的。但随着GPU硬件发展，特别是L1内存的增加，使得GPU实现成为可能。

Method: 基于CPU多核并行缓存高效凸起追逐算法，适配优化GPU吞吐量。使用Julia语言的数组抽象和KernelAbstractions，实现跨NVIDIA、AMD、Intel和Apple Metal GPU的硬件和数据精度无关函数。

Result: GPU算法在1024x1024矩阵尺寸上超越多线程CPU高性能库PLASMA和SLATE，在32k x 32k矩阵上性能提升超过100倍。算法性能随矩阵带宽线性增长。

Conclusion: 这项工作突破了内存带宽和矩阵带宽的限制，在GPU上实现了数量级更快的带状矩阵到双对角形式的约简算法。

Abstract: The reduction of a banded matrix to a bidiagonal form is a crucial step in
the Singular Value Decomposition (SVD), a cornerstone of scientific computing
and AI. Despite being a highly parallel algorithm, it was previously believed
to be unsuitable for GPU computation because it is memory bandwidth-bound.
Recent developments in GPU hardware, including larger L1 memory per Streaming
Multiprocessor/Compute Unit, have changed that. We present the first GPU
algorithm for reducing a banded matrix to bidiagonal form as part of the
NextLA.jl open-source software package. Our algorithm is based on previous
CPU-based multicore parallel cache-efficient bulge chasing algorithms and
adapted to optimize for GPU throughput. We leverage Julia Language's Array
abstractions and KernelAbstractions to implement a single hardware- and data
precision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for
half, single, and double precision, and examine performance optimization across
hardware architectures and data precision. We also develop a hardware-aware
performance model and identify key hyperparameters, such as inner tilewidth and
block concurrency, that govern optimal GPU execution for bandwidth-bound
workloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU
can outperform CPU-based implementations: the GPU algorithm outperforms
multithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size
1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,
the performance of the algorithm increases linearly with matrix bandwidth size,
making faster reduction of larger matrix bandwidths now also possible. With
this work, we break memory bandwidth barriers, as well as matrix bandwidth
barriers, resulting in orders-of-magnitude faster algorithms for the reduction
of banded matrices to bidiagonal form on the GPU.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [A Direct Memory Access Controller (DMAC) for Irregular Data Transfers on RISC-V Linux Systems](https://arxiv.org/abs/2510.12277)
*Thomas Benz,Axel Vanoni,Michael Rogenmoser,Luca Benini*

Main category: cs.AR

TL;DR: 提出了一种优化的描述符DMA控制器，专门用于高效处理小单元大小的任意传输，通过轻量级描述符格式和推测性预取方案显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习应用驱动的计算系统异构性增长，内存系统面临处理任意且要求更高的传输效率的压力。传统描述符DMA在处理小单元传输时效率低下，存在描述符过大和串行处理导致的静态开销问题。

Method: 在AXI4 DMA中实现轻量级描述符格式，采用低开销推测性描述符预取方案（即使预测错误也无额外延迟），集成到64位Linux兼容RISC-V SoC中并在Kintex FPGA上进行仿真。

Result: 相比商用DMA IP，传输启动延迟降低1.66倍，64字节传输时总线利用率提升2.5倍（理想内存系统）或3.6倍（深度内存系统），资源使用减少11%查找表、23%触发器且无需块RAM。在GF12LP+工艺下实现1.44GHz时钟频率，面积仅49.5kGE。

Conclusion: 所提出的优化DMA控制器能有效解决小单元传输效率问题，在性能和资源效率方面均显著优于传统方案。

Abstract: With the ever-growing heterogeneity in computing systems, driven by modern
machine learning applications, pressure is increasing on memory systems to
handle arbitrary and more demanding transfers efficiently. Descriptor-based
direct memory access controllers (DMACs) allow such transfers to be executed by
decoupling memory transfers from processing units. Classical descriptor-based
DMACs are inefficient when handling arbitrary transfers of small unit sizes.
Excessive descriptor size and the serialized nature of processing descriptors
employed by the DMAC lead to large static overheads when setting up transfers.
To tackle this inefficiency, we propose a descriptor-based DMAC optimized to
efficiently handle arbitrary transfers of small unit sizes. We implement a
lightweight descriptor format in an AXI4-based DMAC. We further increase
performance by implementing a low-overhead speculative descriptor prefetching
scheme without additional latency penalties in the case of a misprediction. Our
DMAC is integrated into a 64-bit Linux-capable RISC-V SoC and emulated on a
Kintex FPGA to evaluate its performance. Compared to an off-the-shelf
descriptor-based DMAC IP, we achieve 1.66x less latency launching transfers,
increase bus utilization up to 2.5x in an ideal memory system with
64-byte-length transfers while requiring 11% fewer lookup tables, 23% fewer
flip-flops, and no block RAMs. We can extend our lead in bus utilization to
3.6x with 64-byte-length transfers in deep memory systems. We synthesized our
DMAC in GlobalFoundries' GF12LP+ node, achieving a clock frequency of over 1.44
GHz while occupying only 49.5 kGE.

</details>
