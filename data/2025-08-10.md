<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Consistent Updates for Scalable Microservices](https://arxiv.org/abs/2508.04829)
*Devora Chait-Roth,Kedar S. Namjoshi,Thomas Wies*

Main category: cs.PL

TL;DR: 本文提出了首个保证混合模式更新一致性的算法，利用服务操作的语义属性（如交换性），并证明语义感知是避免不一致的必要条件。


<details>
  <summary>Details</summary>
Motivation: 在线服务通常采用可扩展的微服务架构，但动态修改服务功能时，混合模式操作可能导致不一致性问题，现有方法效率低下或风险高。

Method: 提出基于服务动作语义属性的算法，并开发理论框架，形式化混合模式更新的原子性要求。

Result: 证明了语义感知是避免不一致的必要条件，并提出了新的算法确保更新原子性。

Conclusion: 通过语义感知和理论框架，解决了混合模式更新的不一致性问题，为动态服务更新提供了可靠方法。

Abstract: Online services are commonly implemented with a scalable microservice
architecture, where isomorphic worker processes service client requests,
recording persistent state in a backend data store. To maintain service, any
modifications to the service functionality must be made on the fly -- i.e., as
the service continues to process client requests -- but doing so is
challenging. The central difficulty is that of avoiding potential
inconsistencies caused by ''mixed mode'' operation, where workers of current
and new versions are concurrently active and interact via the data store. Some
update methods avoid mixed mode altogether, but only at the cost of substantial
inefficiency -- by doubling resources (memory and compute), or by halving
throughput. The alternative is a so-called ''rolling'' update, which is
uncontrolled and runs the risk of serious service failures arising from
inconsistent mixed-mode behavior.
  In this paper, we present the first algorithms that guarantee consistency for
mixed mode updates. The algorithms rely on semantic properties of service
actions, such as commutativity. We show that semantic awareness is required, by
proving that any semantically oblivious, mixed-mode update method cannot avoid
inconsistencies. Ideally, it should appear to every client that a service
update takes effect atomically; this ensures that a client is not exposed to
inconsistent mixed-mode behavior. We introduce a framework that formalizes this
intuition and develop foundational theory for reasoning about the consistency
of mixed-mode updates, applying that theory to derive the new algorithms and
establish their correctness.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [OPTIMUMP2P: Fast and Reliable Gossiping in P2P Networks](https://arxiv.org/abs/2508.04833)
*Nicolas Nicolaou,Onyeka Obi,Aayush Rajasekaran,Alejandro Bergasov,Aleksandr Bezobchuk,Kishori M. Konwar,Michael Meier,Santiago Paiva,Har Preet Singh,Swarnabha Sinha*

Main category: cs.DC

TL;DR: OPTIMUMP2P是一种新型的Gossip算法，利用RLNC技术提升libp2p的性能和可靠性，优于Gossipsub协议。


<details>
  <summary>Details</summary>
Motivation: 提升去中心化系统中信息传播的性能和可靠性，特别是在区块链协议中。

Method: 引入OPTIMUMP2P算法，利用随机线性网络编码（RLNC）技术加速信息传播并确保可靠交付。

Result: 在模拟和真实环境中评估，OPTIMUMP2P在性能上优于Gossipsub协议。

Conclusion: OPTIMUMP2P显著提升了信息传播效率，适用于对抗恶意行为的场景。

Abstract: Gossip algorithms are pivotal in the dissemination of information within
decentralized systems. Consequently, numerous gossip libraries have been
developed and widely utilized especially in blockchain protocols for the
propagation of blocks and transactions. A well-established library is libp2p,
which provides two gossip algorithms: floodsup and gossibsup. These algorithms
enable the delivery of published messages to a set of peers. In this work we
aim to enhance the performance and reliability of libp2p by introducing
OPTIMUMP2P, a novel gossip algorithm that leverages the capabilities of Random
Linear Network Coding (RLNC) to expedite the dissemination of information in a
peer-to-peer (P2P) network while ensuring reliable delivery, even in the
presence of malicious actors capable of corrupting the transmitted data.
Preliminary research from the Ethereum Foundation has demonstrated the use of
RLNC in the significant improvement in the block propagation time [14]. Here we
present extensive evaluation results both in simulation and real-world
environments that demonstrate the performance gains of OPTIMUMP2P over the
Gossipsub protocol.

</details>


### [3] [Linear Search for Capturing an Oblivious Mobile Target in the Sender/Receiver Model](https://arxiv.org/abs/2508.04870)
*Khaled Jawhar,Evangelos Kranakis*

Main category: cs.DC

TL;DR: 研究两个通信能力不同的自主机器人在线性搜索中捕获移动目标的算法，分析了不同场景下的竞争比。


<details>
  <summary>Details</summary>
Motivation: 探讨不对称通信对线性搜索竞争比的影响，特别是在机器人通信能力不同的情况下如何优化捕获时间。

Method: 设计了新的线性搜索算法，考虑了目标移动速度、方向（远离或接近原点）以及机器人对环境的了解程度。

Result: 分析了不同场景下的竞争比，展示了不对称通信对搜索效率的影响。

Conclusion: 研究为理解通信不对称性对线性搜索性能的影响提供了新见解。

Abstract: We consider linear search for capturing an oblivious moving target by two
autonomous robots with different communicating abilities. Both robots can
communicate Face-to-Face (F2F) when co-located but in addition one robot is a
Sender (can also send messages wirelessly) and the other also a Receiver (can
also receive messages wirelessly). This is known as Sender/Receiver (S/R, for
short) communication model. The robots can move with max speed $1$. The moving
target starts at distance $d$ from the origin and can move either with speed
$v<1$ away from the origin in the ``away'' model or with speed $v \geq 0$
toward the origin in the ``toward'' model. We assume that the direction of
motion of the target (i.e., whether it is the away or toward model) is known to
the robots in advance. To capture the target the two robots must be co-located
with it.
  We design new linear search algorithms and analyze the competitive ratio of
the time required to capture the target. The approach takes into account
various scenarios related to what the robots know about the search environment
(e.g., starting distance or speed of the mobile, away or toward model, or a
combination thereof). Our study contributes to understanding how asymmetric
communication affects the competitive ratio of linear search.

</details>


### [4] [Managing, Analyzing and Sharing Research Data with Gen3 Data Commons](https://arxiv.org/abs/2508.04944)
*Craig Barnes,Kyle Burton,Michael S. Fitzsimons,Hara Prasad Juvvala,Brienna Larrick,Christopher Meyer,Pauline Ribeyre,Ao Liu,Clint Malson,Noah Metoki-Shlubsky,Andrii Prokhorenkov,Jawad Qureshi,Radhika Reddy,L. Philip Schumm,Mingfei Shao,Trevar Simmons,Alexander VanTol,Peter Vassilatos,Aarti Venkat,Robert L. Grossman*

Main category: cs.DC

TL;DR: Gen3是一个开源数据平台，用于构建数据共享空间，支持数据管理、分析和共享，已管理超过28 PB数据和6400万FAIR数据对象。


<details>
  <summary>Details</summary>
Motivation: 提供一个云基础的数据平台，促进研究社区内的数据管理和共享。

Method: 通过定义数据模型自动生成数据门户（搜索、提交）和FAIR API，基于标准化软件服务构建。

Result: 已支持构建十几个数据共享空间，管理大量数据。

Conclusion: Gen3是一个灵活、可互操作的数据平台，适用于构建和管理数据共享空间。

Abstract: Gen3 is an open-source data platform for building data commons. A data
commons is a cloud-based data platform for managing, analyzing, and sharing
data with a research community. Gen3 has been used to build over a dozen data
commons that in aggregate contain over 28 PB of data and 64 million FAIR data
objects. To set up a Gen3 data commons, you first define a data model. Gen3
then autogenerates 1) a data portal for searching and exploring data in the
commons; 2) a data portal for submitting data to the commons; and 3) FAIR APIs
for accessing the data programmatically. Gen3 is built over a small number of
standards-based software services, which are designed to support current and
future Gen3 components so that Gen3 can interoperate with other data platforms
and data ecosystems.

</details>


### [5] [Tesserae: Scalable Placement Policies for Deep Learning Workloads](https://arxiv.org/abs/2508.04953)
*Song Bian,Saurabh Agarwal,Md. Tareq Mahmood,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 论文提出了一种基于图匹配的调度策略，用于优化深度学习集群的资源利用率，显著提升了作业完成时间和集群效率。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型训练已成为数据中心的主要工作负载，但现有调度策略要么性能次优，要么扩展性差。

Method: 将调度约束建模为图匹配问题，设计了新的放置策略以减少作业迁移开销和优化作业打包。

Result: 实验表明，Tesserae调度器将平均作业完成时间提升至1.62倍，集群总完成时间提升至1.15倍。

Conclusion: 图匹配方法为深度学习集群调度提供了可扩展且高效的解决方案。

Abstract: Training deep learning (DL) models has become a dominant workload in
data-centers and improving resource utilization is a key goal of DL cluster
schedulers. In order to do this, schedulers typically incorporate placement
policies that govern where jobs are placed on the cluster. Existing placement
policies are either designed as ad-hoc heuristics or incorporated as
constraints within a complex optimization problem and thus either suffer from
suboptimal performance or poor scalability. Our key insight is that many
placement constraints can be formulated as graph matching problems and based on
that we design novel placement policies for minimizing job migration overheads
and job packing. We integrate these policies into Tesserae and describe how our
design leads to a scalable and effective GPU cluster scheduler. Our
experimental results show that Tesserae improves average JCT by up to 1.62x and
the Makespan by up to 1.15x compared with the existing schedulers.

</details>


### [6] [Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations](https://arxiv.org/abs/2508.05020)
*Anjiang Wei,Hang Song,Mert Hidayetoglu,Elliott Slaughter,Sanjiva K. Lele,Alex Aiken*

Main category: cs.DC

TL;DR: 开发了一种基于AMR的高阶数值求解器，解决了Regent语言中的动态数据结构、网格有效性等挑战，任务融合和GPU内核生成显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 高阶求解器在科学计算中至关重要，而AMR能有效降低计算成本，但实现AMR在Regent语言中存在挑战。

Method: 使用Regent语言开发AMR求解器，解决动态数据结构、网格有效性、任务融合等问题，并通过GPU内核生成优化性能。

Result: 任务融合实现18倍加速，GPU内核生成带来9.7倍加速，成功模拟了欧拉方程的两个典型问题。

Conclusion: 该方法在Regent中高效实现了AMR，显著提升了计算性能，适用于复杂流问题。

Abstract: High-order solvers for compressible flows are vital in scientific
applications. Adaptive mesh refinement (AMR) is a key technique for reducing
computational cost by concentrating resolution in regions of interest. In this
work, we develop an AMR-based numerical solver using Regent, a high-level
programming language for the Legion programming model. We address several
challenges associated with implementing AMR in Regent. These include dynamic
data structures for patch refinement/coarsening, mesh validity enforcement, and
reducing task launch overhead via task fusion. Experimental results show that
task fusion achieves 18x speedup, while automated GPU kernel generation via
simple annotations yields 9.7x speedup for the targeted kernel. We demonstrate
our approach through simulations of two canonical compressible flow problems
governed by the Euler equations.

</details>


### [7] [Theseus: A Distributed and Scalable GPU-Accelerated Query Processing Platform Optimized for Efficient Data Movement](https://arxiv.org/abs/2508.05029)
*Felipe Aramburú,William Malpica,Kaouther Abrougui,Amin Aramoon,Romulo Auccapuclla,Claude Brisson,Matthijs Brobbel,Colby Farrell,Pradeep Garigipati,Joost Hoozemans,Supun Kamburugamuve,Akhil Nair,Alexander Ocsa,Johan Peltenburg,Rubén Quesada López,Deepak Sihag,Ahmet Uyar,Dhruv Vats,Michael Wendt,Jignesh M. Patel,Rodrigo Aramburú*

Main category: cs.DC

TL;DR: Theseus是一个分布式加速器原生查询引擎，旨在优化数据移动、内存利用和计算，显著提升大规模数据分析性能。


<details>
  <summary>Details</summary>
Motivation: 为了降低大规模数据分析的成本并提高吞吐量，利用GPU等加速器，解决数据移动和系统优化的挑战。

Method: Theseus采用异步控制机制和固定大小的页锁定主机内存分配，优化网络通信、数据预加载、内存管理和GPU计算任务。

Result: 在TPC-H基准测试中，Theseus性能优于Databricks Photon高达4倍，且能以2个DGX A100节点处理100TB规模的数据。

Conclusion: Theseus展示了在分布式加速器环境中高效处理大规模数据分析的潜力，为生产级应用提供了可行的解决方案。

Abstract: Online analytical processing of queries on datasets in the many-terabyte
range is only possible with costly distributed computing systems. To decrease
the cost and increase the throughput, systems can leverage accelerators such as
GPUs, which are now ubiquitous in the compute infrastructure. This introduces
many challenges, the majority of which are related to when, where, and how to
best move data around the system. We present Theseus -- a production-ready
enterprise-scale distributed accelerator-native query engine designed to
balance data movement, memory utilization, and computation in an
accelerator-based system context. Specialized asynchronous control mechanisms
are tightly coupled to the hardware resources for the purpose of network
communication, data pre-loading, data spilling across memories and storage, and
GPU compute tasks. The memory subsystem contains a mechanism for fixed-size
page-locked host memory allocations to increase throughput and reduce memory
fragmentation. For the TPC-H benchmarks at scale factors ranging from 1k to 30k
on cloud infrastructure, Theseus outperforms Databricks Photon by up to
$4\times$ at cost parity. Theseus is capable of processing all queries of the
TPC-H and TPC-DS benchmarks at scale factor 100k (100 TB scale) with as few as
2 DGX A100 640GB nodes.

</details>


### [8] [Simulating LLM training workloads for heterogeneous compute and network infrastructure](https://arxiv.org/abs/2508.05370)
*Sumit Kumar,Arjun Temura,Naman Sharma,Ramanjeet Singh,Meet Dadhania,Praveen Tammana,Satananda Burla,Abed Mohammad Kamaluddin,Rinku Shah*

Main category: cs.DC

TL;DR: 论文提出了一种异构感知的分布式LLM模拟器，用于预测训练时间并支持自定义设备配置，解决了现有模拟器假设同质基础设施的不足。


<details>
  <summary>Details</summary>
Motivation: 大规模GPU集群的需求增长带来了模型优化和系统级改进的挑战，现有LLM训练模拟器假设基础设施同质化，但实践中设备异构性不可避免。

Method: 设计了异构感知的分布式LLM模拟器，支持自定义设备组配置和非均匀工作负载分区。

Result: 初步模拟结果显示异构性对模型计算和通信时间的影响。

Conclusion: 异构感知模拟器填补了现有技术与实际需求之间的差距，为分布式训练提供了更准确的预测工具。

Abstract: The growing demand for large-scale GPU clusters in distributed model training
presents a significant barrier to innovation, particularly in model
optimization, performance tuning, and system-level enhancements. To address
this challenge, LLM training simulators are employed to estimate training time
and guide design decisions. However, the state-of-the-art LLM training
simulators assume homogeneous compute and network infrastructure. In practice,
device heterogeneity is inevitable due to resource sharing in cloud
environments, frequent shifts in device generations, and inherent intra-chip
interconnect heterogeneity. To address the gap between state-of-the-art and
practical requirements, we propose the design of a heterogeneity-aware
distributed LLM simulator capable of predicting training time while enabling
abstractions to specify custom configurations for device groups and
device-to-parallelism mapping. We present the design requirements and
challenges in building a heterogeneity-aware distributed ML training simulator,
and design components such as non-uniform workload partitioning. Our initial
simulation results demonstrate the impact of heterogeneity on the model
computation and communication time.

</details>


### [9] [Adaptive Parallel Downloader for Large Genomic Datasets](https://arxiv.org/abs/2508.05511)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: FastBioDL是一种用于大型生物数据集的并行文件下载工具，通过自适应并发控制器动态调整并发流数量，显著提升下载速度。


<details>
  <summary>Details</summary>
Motivation: 现有下载工具因静态并发设置无法适应动态网络条件，导致带宽利用低效和下载时间延长。

Method: 将下载过程建模为在线优化问题，利用效用函数和梯度下降实时动态调整并发流数量。

Result: 在公共基因组数据集上，FastBioDL比现有工具快4倍；高速网络实验中快2.1倍。

Conclusion: FastBioDL为大规模基因组数据获取提供了高效解决方案，无需专用商业软件或协议。

Abstract: Modern next-generation sequencing (NGS) projects routinely generate terabytes
of data, which researchers commonly download from public repositories such as
SRA or ENA. Existing download tools often employ static concurrency settings,
leading to inefficient bandwidth utilization and prolonged download times due
to their inability to adapt to dynamic network conditions. We introduce
FastBioDL, a parallel file downloader designed for large biological datasets,
featuring an adaptive concurrency controller. FastBioDL frames the download
process as an online optimization problem, utilizing a utility function and
gradient descent to adjust the number of concurrent socket streams in real-time
dynamically. This approach maximizes download throughput while minimizing
resource overhead. Comprehensive evaluations on public genomic datasets
demonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art
tools. Moreover, in high-speed network experiments, its adaptive design was up
to $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP
or FTP downloads on the client side, FastBioDL provides a robust and efficient
solution for large-scale genomic data acquisition, democratizing
high-performance data retrieval for researchers without requiring specialized
commercial software or protocols.

</details>


### [10] [Modular Architecture for High-Performance and Low Overhead Data Transfers](https://arxiv.org/abs/2508.05546)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: AutoMDT是一种基于深度强化学习的模块化数据传输架构，通过优化并发级别显著提升数据传输性能。


<details>
  <summary>Details</summary>
Motivation: 高性能应用需要快速可靠地传输大规模数据，传统工具因配置固定或优化方法单一导致资源利用不足和不稳定。

Method: 采用深度强化学习（PPO）代理优化并发级别，结合轻量级网络系统模拟器进行离线训练。

Result: 在测试中，AutoMDT实现了8倍收敛速度和68%传输时间减少。

Conclusion: AutoMDT通过模块化设计和强化学习显著提升了数据传输效率和适应性。

Abstract: High-performance applications necessitate rapid and dependable transfer of
massive datasets across geographically dispersed locations. Traditional file
transfer tools often suffer from resource underutilization and instability
because of fixed configurations or monolithic optimization methods. We propose
AutoMDT, a novel modular data transfer architecture that employs a deep
reinforcement learning based agent to simultaneously optimize concurrency
levels for read, network, and write operations. Our solution incorporates a
lightweight network-system simulator, enabling offline training of a Proximal
Policy Optimization (PPO) agent in approximately 45 minutes on average, thereby
overcoming the impracticality of lengthy online training in production
networks. AutoMDT's modular design decouples I/O and network tasks, allowing
the agent to capture complex buffer dynamics precisely and to adapt quickly to
changing system and network conditions. Evaluations on production-grade
testbeds show that AutoMDT achieves up to 8x faster convergence and a 68%
reduction in transfer completion times compared with state-of-the-art
solutions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [Understanding and Mitigating Errors of LLM-Generated RTL Code](https://arxiv.org/abs/2508.05266)
*Jiazheng Zhang,Cheng Liu,Huawei Li*

Main category: cs.AR

TL;DR: 论文针对基于大语言模型（LLM）的RTL代码生成成功率低的问题，通过错误分析和针对性技术改进，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在RTL代码生成中错误率高，主要源于RTL编程知识不足、电路概念理解差、设计描述模糊或多模态输入误解。

Method: 采用错误分析、领域知识库构建、检索增强生成（RAG）、设计描述规则检查、多模态输入转换及迭代调试循环等技术。

Result: 改进后的框架在VerilogEval基准测试中达到91.0%准确率，比基线方法提升32.7%。

Conclusion: 通过针对性错误修正技术，显著提升了LLM在RTL代码生成中的性能。

Abstract: Despite the promising potential of large language model (LLM) based
register-transfer-level (RTL) code generation, the overall success rate remains
unsatisfactory. Errors arise from various factors, with limited understanding
of specific failure causes hindering improvement. To address this, we conduct a
comprehensive error analysis and manual categorization. Our findings reveal
that most errors stem not from LLM reasoning limitations, but from insufficient
RTL programming knowledge, poor understanding of circuit concepts, ambiguous
design descriptions, or misinterpretation of complex multimodal inputs.
Leveraging in-context learning, we propose targeted error correction
techniques. Specifically, we construct a domain-specific knowledge base and
employ retrieval-augmented generation (RAG) to supply necessary RTL knowledge.
To mitigate ambiguity errors, we introduce design description rules and
implement a rule-checking mechanism. For multimodal misinterpretation, we
integrate external tools to convert inputs into LLM-compatible meta-formats.
For remaining errors, we adopt an iterative debugging loop (simulation-error
localization-correction). Integrating these techniques into an LLM-based
framework significantly improves performance. We incorporate these error
correction techniques into a foundational LLM-based RTL code generation
framework, resulting in significantly improved performance. Experimental
results show that our enhanced framework achieves 91.0\% accuracy on the
VerilogEval benchmark, surpassing the baseline code generation approach by
32.7\%, demonstrating the effectiveness of our methods.

</details>


### [12] [relOBI: A Reliable Low-latency Interconnect for Tightly-Coupled On-chip Communication](https://arxiv.org/abs/2508.05354)
*Michael Rogenmoser,Angelo Garofalo,Luca Benini*

Main category: cs.AR

TL;DR: 论文提出了一种名为relOBI的扩展方案，结合TMR和ECC技术，显著提高了SoC互连的可靠性，将故障注入的脆弱性从34.85%降至0%。


<details>
  <summary>Details</summary>
Motivation: 在辐射密集环境中，SoC互连的软错误可能导致整个系统失效，因此需要高可靠性的解决方案。

Method: 扩展OBI协议，结合TMR保护关键握手信号，并使用ECC保护其他信号。

Result: 实现的全可靠交叉开关将故障脆弱性降至0%，面积增加2.6倍，时序影响1.4倍，面积开销比文献中的细粒度三重化方案低1.8倍。

Conclusion: relOBI是一种高效的可靠性增强方案，适用于辐射密集环境中的SoC互连。

Abstract: On-chip communication is a critical element of modern systems-on-chip (SoCs),
allowing processor cores to interact with memory and peripherals. Interconnects
require special care in radiation-heavy environments, as any soft error within
the SoC interconnect is likely to cause a functional failure of the whole SoC.
This work proposes relOBI, an extension to Open Bus Interface (OBI) combining
triple modular redundancy (TMR) for critical handshake signals with error
correction codes (ECC) protection on other signals for complete reliability.
Implementing and testing a fully reliable crossbar shows improved reliability
to injected faults from a vulnerability of 34.85 % to 0 % compared to a
reference design, with an area increase of 2.6x and 1.4x timing impact. The
area overhead is 1.8x lower than that reported in the literature for
fine-grained triplication and voting.

</details>
