<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 5]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Cyclotron: Compilation of Recurrences to Distributed and Systolic Architectures](https://arxiv.org/abs/2511.09987)
*Shiv Sundram,Akhilesh Balasingam,Nathan Zhang,Kunle Olukotun,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: Cyclotron是一个使用递归方程表达流式数据流算法的框架和编译器，可将算法可移植地编译到分布式处理器拓扑上。


<details>
  <summary>Details</summary>
Motivation: 为了解决流式数据流算法在分布式硬件上的高效执行问题，提供一种能够优化内存访问、实现流水线执行的编译框架。

Method: 使用逻辑张量上的递归方程作为输入语言，通过中间表示层逐步降低到处理器特定的发送、接收和计算操作，并提供调度语言控制数据流。

Result: 在可重构模拟器和分布式CPU集群上验证了框架的可移植性，生成的矩阵乘法实现与ScaLAPACK性能相当。

Conclusion: Cyclotron框架成功实现了流式数据流算法在分布式硬件上的高效编译和执行，为硬件设计空间探索提供了有力工具。

Abstract: We present Cyclotron, a framework and compiler for using recurrence equations to express streaming dataflow algorithms, which then get portably compiled to distributed topologies of interlinked processors. Our framework provides an input language of recurrences over logical tensors, which then gets lowered into an intermediate language of recurrences over logical iteration spaces, and finally into programs of send, receive, and computation operations specific to each individual processor. In Cyclotron's IR, programs are optimized such that external memory interactions are confined to the boundaries of the iteration space. Within inner iteration spaces, all data accesses become local: data accesses target values residing in local fast memory or on neighboring processing units, avoiding costly memory movement. We provide a scheduling language allowing users to define how data gets streamed and broadcasted between processors, enabling pipelined execution of computation kernels over distributed topologies of processing elements. We demonstrate the portability of our approach by compiling our IR to a reconfigurable simulator of systolic arrays and chiplet style distributed hardware, as well as to distributed-memory CPU clusters. In the simulated reconfigurable setting, we use our compiler for hardware design space exploration in which link costs and latencies can be specified. In the distributed CPU setting, we show how to use recurrences and our scheduling language to express various matrix multiplication routines (Cannon, SUMMA, PUMMA, weight stationary) and solvers (Triangular solve and Cholesky). For matrix multiplication and the triangular solve, we generate distributed implementations competitive with ScaLAPACK.

</details>


### [2] [Omnidirectional type inference for ML: principality any way](https://arxiv.org/abs/2511.10343)
*Alistair O'Brien,Didier Rémy,Gabriel Scherer*

Main category: cs.PL

TL;DR: 本文提出了全向类型推断方法，通过动态信息流和挂起匹配约束来解决ML类型系统扩展中的主要性丧失问题，比现有方法更灵活且表达能力更强。


<details>
  <summary>Details</summary>
Motivation: ML类型系统的许多扩展（如GADTs、高阶多态、静态重载）会破坏主要性，导致类型推断变得脆弱。现有的双向类型推断方法采用固定的推断顺序，经常拒绝本应类型正确的程序。

Method: 提出全向类型推断，允许类型信息以动态顺序流动，使用挂起匹配约束在需要已知类型信息时暂停推断，并在信息可用时恢复。引入增量实例化机制来处理let泛化问题。

Result: 该方法在OCaml的两个不同特性（记录标签和数据构造器的静态重载、半显式一等多态）上验证了有效性，获得了比OCaml当前类型检查器表达能力更强的主要类型推断算法。

Conclusion: 全向性为在脆弱特性存在时恢复主要性提供了一个通用框架，能够处理ML类型系统扩展带来的挑战，提供更灵活和表达能力更强的类型推断。

Abstract: The Damas-Hindley-Milner (ML) type system owes its success to principality, the property that every well-typed expression has a unique most general type. This makes inference predictable and efficient. Unfortunately, many extensions of ML (GADTs, higher-rank polymorphism, and static overloading) endanger princpality by introducing _fragile_ constructs that resist principal inference. Existing approaches recover principality through directional inference algorithms, which propagate _known_ type information in a fixed (or static) order (e.g. as in bidirectional typing) to disambiguate such constructs. However, the rigidity of a static inference order often causes otherwise well-typed programs to be rejected.
  We propose _omnidirectional_ type inference, where type information flows in a dynamic order. Typing constraints may be solved in any order, suspending when progress requires known type information and resuming once it becomes available, using _suspended match constraints_. This approach is straightforward for simply typed systems, but extending it to ML is challenging due to let-generalization. Existing ML inference algorithms type let-bindings (let x = e1 in e2) in a fixed order: type e1, generalize its type, and then type e2. To overcome this, we introduce _incremental instantiation_, allowing partially solved type schemes containing suspended constraints to be instantiated, with a mechanism to incrementally update instances as the scheme is refined.
  Omnidirectionality provides a general framework for restoring principality in the presence of fragile features. We demonstrate its versatility on two fundamentally different features of OCaml: static overloading of record labels and datatype constructors and semi-explicit first-class polymorphism. In both cases, we obtain a principal type inference algorithm that is more expressive than OCaml's current typechecker.

</details>


### [3] [Lazy Linearity for a Core Functional Language](https://arxiv.org/abs/2511.10361)
*Rodrigo Mesquita,Bernardo Toninho*

Main category: cs.PL

TL;DR: 提出了Linear Core系统，在惰性求值语言中静态接受线性类型的惰性语义，解决了Haskell优化编译器中语法线性性被破坏但语义保持的问题。


<details>
  <summary>Details</summary>
Motivation: 传统线性类型语言中，消耗线性资源等同于程序中的语法出现。但在非严格求值下，线性性可以从语义角度理解，语法出现不一定意味着执行时实际使用资源。Haskell优化编译器会重写程序，破坏语法线性性但保持语义，这促使需要新的系统来处理这种差异。

Method: 引入Linear Core系统，该系统接受线性类型的惰性语义，适用于惰性语言如GHC的Core中间语言。证明系统是健全的，保证线性资源使用，并且多个优化转换在Linear Core中保持线性性而在Core中失败。

Result: 实现了Linear Core作为编译器插件，验证了系统在线性性重的库（包括linear-base）中的有效性。

Conclusion: Linear Core系统成功解决了惰性语言中线性类型语义与语法不匹配的问题，为Haskell等惰性语言的线性类型实现提供了可行的解决方案。

Abstract: Traditionally, in linearly typed languages, consuming a linear resource is synonymous with its syntactic occurrence in the program. However, under the lens of non-strict evaluation, linearity can be further understood semantically, where a syntactic occurrence of a resource does not necessarily entail using that resource when the program is executed. While this distinction has been largely unexplored, it turns out to be inescapable in Haskell's optimising compiler, which heavily rewrites the source program in ways that break syntactic linearity but preserve the program's semantics. We introduce Linear Core, a novel system which accepts the lazy semantics of linearity statically and is suitable for lazy languages such as the Core intermediate language of the Glasgow Haskell Compiler. We prove that Linear Core is sound, guaranteeing linear resource usage, and that multiple optimising transformations preserve linearity in Linear Core while failing to do so in Core. We have implemented Linear Core as a compiler plugin to validate the system against linearity-heavy libraries, including linear-base.

</details>


### [4] [Modeling Layout Abstractions Using Integer Set Relations](https://arxiv.org/abs/2511.10374)
*Somashekaracharya G Bhaskaracharya,Aravind Acharya,Bastian Hagedorn,Vinod Grover*

Main category: cs.PL

TL;DR: 本文提出使用整数集库(ISL)为CuTe布局和Triton线性布局创建统一的数学表示，通过整数集关系实现形式化分析、正确性验证和跨系统优化基础。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习编译器依赖布局抽象来管理逻辑张量结构与物理内存安排的复杂映射。CuTe布局和Triton线性布局是广泛采用的行业标准，但各自具有不同的数学基础，阻碍了统一的形式化分析和跨系统推理。

Method: 使用整数集库(ISL)建模：对CuTe布局通过整数集关系编码从多维坐标到线性索引的转换（包括步长计算和位级操作的swizzle操作）；对Triton线性布局建模二进制向量空间变换，其中算术运算遵循有限域F_2规则。实现完整的布局操作算法套件。

Result: 实验评估表明，该系统能够处理从基本恒等变换到具有复杂步长配置和swizzle模式的多维张量安排的完整布局复杂度谱系，验证了跨不同布局范式的数学建模方法。

Conclusion: 该方法成功桥接了CuTe和Triton布局系统之间的差距，为统一形式化分析、正确性验证和未来跨系统优化策略奠定了基础。

Abstract: Modern deep learning compilers rely on layout abstractions to manage the complex mapping between logical tensor structures and physical memory arrangements. CuTe layouts and Triton linear layouts are widely adopted industry standards. However, these layout systems operate independently with distinct mathematical underpinnings, preventing unified formal analysis and cross-system reasoning. We bridge this gap by introducing a novel approach that leverages the Integer Set Library (ISL) to create a unified mathematical representation for both layout systems through integer set relations, thereby enabling rigorous formal analysis, correctness verification, and the foundation for future cross-system optimization strategies. Our approach models CuTe layouts through integer set relations that encode the transformation from multi-dimensional coordinates to linear indices using stride-based calculations, including sophisticated swizzle operations that perform bit-level manipulations for enhanced memory access patterns. For Triton linear layouts, we construct integer set relations that model the binary vector space transformations where arithmetic operations follow finite field F_2 rules. We implement a complete suite of layout manipulation algorithms for composition, inversion, complement using built-in operations in ISL to ensure mathematical correctness and preserve layout semantics. Experimental evaluation shows that the system handles the full spectrum of layout complexity, from elementary identity transformations to sophisticated multi-dimensional tensor arrangements with complex stride configurations and swizzle patterns, validating the mathematical modeling approach across different layout paradigms.

</details>


### [5] [zkStruDul: Programming zkSNARKs with Structural Duality](https://arxiv.org/abs/2511.10565)
*Rahul Krishnan,Ashley Samuelson,Emily Yao,Ethan Cecchetti*

Main category: cs.PL

TL;DR: zkStruDul是一个统一输入转换和谓词定义的语言，通过单一抽象消除重复代码和安全隐患，构建在现有NIZK技术之上。


<details>
  <summary>Details</summary>
Motivation: 现有NIZK工具将输入转换和谓词定义分开实现，导致逻辑重复和潜在的安全漏洞，需要统一抽象来避免这些问题。

Method: 开发zkStruDul语言，将输入转换和谓词定义统一为单一抽象，编译器可从中投影出两个过程，消除代码重复。

Result: zkStruDul提供了高层抽象，支持递归证明等重要特性，并证明了源级语义与投影语义的行为一致性。

Conclusion: zkStruDul通过统一抽象解决了NIZK应用中输入转换和谓词定义的分离问题，提高了安全性和开发效率。

Abstract: Non-Interactive Zero Knowledge (NIZK) proofs, such as zkSNARKS, let one prove knowledge of private data without revealing it or interacting with a verifier. While existing tooling focuses on specifying the predicate to be proven, real-world applications optimize predicate definitions to minimize proof generation overhead, but must correspondingly transform predicate inputs. Implementing these two steps separately duplicates logic that must precisely match to avoid catastrophic security flaws. We address this shortcoming with zkStruDul, a language that unifies input transformations and predicate definitions into a single combined abstraction from which a compiler can project both procedures, eliminating duplicate code and problematic mismatches. zkStruDul provides a high-level abstraction to layer on top of existing NIZK technology and supports important features like recursive proofs. We provide a source-level semantics and prove its behavior is identical to the projected semantics, allowing straightforward standard reasoning.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Ksurf-Drone: Attention Kalman Filter for Contextual Bandit Optimization in Cloud Resource Allocation](https://arxiv.org/abs/2511.09766)
*Michael Dang'ana,Yuqiu Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: Ksurf是一种针对高可变云数据的方差最小化估计方法，在基于估计的资源编排任务中显著降低了延迟方差和资源使用量。


<details>
  <summary>Details</summary>
Motivation: 云数据中心中容器基础设施的资源编排和配置参数搜索面临大配置空间和云不确定性的挑战，现有技术如Drone编排器在处理高度可变的云环境时准确性不足。

Method: 将Ksurf作为上下文多臂老虎机的目标函数模型，应用于Drone编排器，用于处理高度可变工作负载的云场景。

Result: Ksurf实现了p95延迟方差降低41%，p99延迟方差降低47%，CPU使用量减少4%，主节点内存使用减少7MB，在VarBench Kubernetes基准测试中平均工作pod数量节省7%成本。

Conclusion: Ksurf在高可变云环境下能够显著提升资源编排的准确性和效率，降低延迟方差和资源消耗。

Abstract: Resource orchestration and configuration parameter search are key concerns for container-based infrastructure in cloud data centers. Large configuration search space and cloud uncertainties are often mitigated using contextual bandit techniques for resource orchestration including the state-of-the-art Drone orchestrator. Complexity in the cloud provider environment due to varying numbers of virtual machines introduces variability in workloads and resource metrics, making orchestration decisions less accurate due to increased nonlinearity and noise. Ksurf, a state-of-the-art variance-minimizing estimator method ideal for highly variable cloud data, enables optimal resource estimation under conditions of high cloud variability.
  This work evaluates the performance of Ksurf on estimation-based resource orchestration tasks involving highly variable workloads when employed as a contextual multi-armed bandit objective function model for cloud scenarios using Drone. Ksurf enables significantly lower latency variance of $41\%$ at p95 and $47\%$ at p99, demonstrates a $4\%$ reduction in CPU usage and 7 MB reduction in master node memory usage on Kubernetes, resulting in a $7\%$ cost savings in average worker pod count on VarBench Kubernetes benchmark.

</details>


### [7] [A Poly-Log Approximation for Transaction Scheduling in Fog-Cloud Computing and Beyond](https://arxiv.org/abs/2511.09776)
*Ramesh Adhikari,Costas Busch,Pavan Poudel*

Main category: cs.DC

TL;DR: 本文研究雾-云计算网络中事务调度问题，目标是优化共享资源分配，最小化调度总成本。针对常数倍维网络，提出了单对象和多对象事务的近似调度算法。


<details>
  <summary>Details</summary>
Motivation: 在分布式雾-云计算环境中，事务和共享对象可以在网络中移动，需要高效调度来无冲突地分配共享资源。现有研究缺乏对移动对象和事务的联合调度优化。

Method: 针对常数倍维网络，提出集中式和分布式调度算法。单对象事务使用O(log n·log D)近似算法，多对象事务扩展到O(k·log n·log D)近似。分布式版本无需全局事务知识。

Result: 算法在理论上证明了近似比保证：单对象为O(log n·log D)，多对象为O(k·log n·log D)，其中n为节点数，D为网络直径，k为每个事务访问的最大对象数。

Conclusion: 提出的调度算法能有效处理雾-云网络中移动对象和事务的调度问题，提供理论性能保证，且支持分布式实现，适用于实际部署。

Abstract: Transaction scheduling is crucial to efficiently allocate shared resources in a conflict-free manner in distributed systems. We investigate the efficient scheduling of transactions in a network of fog-cloud computing model, where transactions and their associated shared objects can move within the network. The schedule may require objects to move to transaction nodes, or the transactions to move to the object nodes. Moreover, the schedule may determine intermediate nodes where both objects and transactions meet. Our goal is to minimize the total combined cost of the schedule. We focus on networks of constant doubling dimension, which appear frequently in practice. We consider a batch problem where an arbitrary set of nodes has transactions that need to be scheduled. First, we consider a single shared object required by all the transactions and present a scheduling algorithm that gives an $O(\log n \cdot \log D)$ approximation of the optimal schedule, where $n$ is the number of nodes and $D$ is the diameter of the network. Later, we consider transactions accessing multiple shared objects (at most $k$ objects per transaction) and provide a scheduling algorithm that gives an $O(k \cdot \log n \cdot \log D)$ approximation. We also provide a fully distributed version of the scheduling algorithms where the nodes do not need global knowledge of transactions.

</details>


### [8] [MoFa: A Unified Performance Modeling Framework for LLM Pretraining](https://arxiv.org/abs/2511.09837)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Shangchao Su,Ziqing Yin,Zhiyan Cui,Hongfeng Sun,Baoguo He,Yueqiang Chen,Liang Dong,Xiyuan Li,Lingbin Wang,Lijun Ma,Qiang Huang,Ting Liu,Chong Wang,Can Wei*

Main category: cs.DC

TL;DR: MoFa是一个用于大规模语言模型预训练的性能建模框架，统一考虑了多维优化特征和容错机制，提供高精度的性能预测和调优指导。


<details>
  <summary>Details</summary>
Motivation: 随着LLM参数规模从数十亿增长到数万亿，分布式预训练需要在大规模集群上进行。传统手动调优成本高昂，现有性能建模方法无法全面考虑优化特征和容错机制的开销。

Method: 提出MoFa框架，包含增强的成本模型来准确捕捉关键优化效果，并基于历史集群可靠性数据集成容错模型。开发了基于MoFa的调优系统来探索最优预训练性能和潜在瓶颈。

Result: 广泛的建模评估表明MoFa在各种场景下都能实现高预测精度。调优实验系统揭示了不同配置下影响预训练性能的关键因素。

Conclusion: MoFa为LLM预训练系统设计和部署提供了可靠的事前指导，能够有效解决大规模分布式预训练的性能优化挑战。

Abstract: The exponential growth in LLM scales, with parameters soaring from billions to trillions, has necessitated distributed pretraining across large clusters comprising thousands to tens of thousands of devices. While hybrid parallelization strategies enable such pretraining, the vast combinatorial strategy space introduces significant optimization challenges. Traditional manual tuning methods incur prohibitive trial-and-error costs, and existing performance modeling approaches exhibit critical limitations: they fail to comprehensively account for prevalent optimization features and ignore the substantial overhead imposed by essential fault tolerance mechanisms like checkpoint recovery in long-duration pretraining. To address these gaps, we propose MoFa, a novel pretraining performance modeling framework that unifies multi-dimensional optimization features and fault tolerance. MoFa incorporates an enhanced cost model to accurately capture the effects of key optimizations and integrates a fault tolerance model based on historical cluster reliability data. Besides, a MoFa-based tuning system is developed to explore optimal pretraining performance and potential bottlenecks in various scenarios. Extensive modeling evaluations demonstrate that MoFa can achieve high prediction accuracy across various scenarios. In addition, through comprehensive tuning experiments, our framework systematically reveals the key factors influencing pretraining performance under different configurations, which provides solid a priori guidance for LLM pretraining system design and deployment.

</details>


### [9] [Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs](https://arxiv.org/abs/2511.09861)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: GPU系统在数据中心中广泛使用，但存在性能变异问题。研究发现热不平衡导致的Lit Silicon效应是主要原因，通过检测和缓解技术可实现6%性能提升和4%功耗改善。


<details>
  <summary>Details</summary>
Motivation: GPU系统在大规模数据中心中普遍存在性能变异问题，严重影响HPC和AI工作负载（如大语言模型训练）的性能。需要理解并解决这一问题。

Method: 分析单节点多GPU系统运行LLM训练的性能，提出Lit Silicon效应理论，建立性能功耗分析模型，设计检测缓解技术，评估三种电源管理方案。

Result: 在AMD MI300X GPU系统上测试，观察到最高6%性能提升和4%功耗改善，可为数据中心节省数亿美元成本。

Conclusion: Lit Silicon效应是GPU性能变异的关键因素，提出的解决方案成本低廉，可作为新的节点级电源管理层轻松部署到数据中心。

Abstract: GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, such as cutting-edge large language models (LLMs). We analyze the performance of a single-node multi-GPU system running LLM training, and observe that the kernel-level performance variation is highly correlated with concurrent computation communication (C3), a technique to overlap computation and communication across GPUs for performance gains. We then take a further step to reason that thermally induced straggling coupling with C3 impacts performance variation, coined as the Lit Silicon effect. Lit Silicon describes that in a multi-GPU node, thermal imbalance across GPUs introduces node-level straggler GPUs, which in turn slow down the leader GPUs. Lit Silicon leads to node-level performance variation and inefficiency, impacting the entire datacenter from the bottom up. We propose analytical performance and power models for Lit Silicon, to understand the potential system-level gains. We further design simple detection and mitigation techniques to effectively address the Lit Silicon problem, and evaluate three different power management solutions, including power optimization under GPU thermal design power, performance optimization under node-level GPU power capping, and performance optimization under node-level CPU power sloshing. We conduct experiments on two workloads on two AMD InstinctTM MI300X GPU systems under two LLM training frameworks, and observe up to 6% performance and 4% power improvements, potentially saving hundreds of millions of dollars in datacenters. Our solution is almost free lunch and can be effortlessly adopted in datacenters as a new node-level power management layer.

</details>


### [10] [Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction](https://arxiv.org/abs/2511.09956)
*Mani Tofigh,Edward Guo,Weiwei Jia,Xiaoning Ding,Jianchen Shan*

Main category: cs.DC

TL;DR: CacheX：一种在云虚拟机中探测缓存抽象的新方法，通过驱逐集技术实现细粒度缓存感知，无需硬件或虚拟机监控程序支持，可提升云环境中缓存利用率。


<details>
  <summary>Details</summary>
Motivation: 云虚拟机中的缓存优化通常无效，因为虚拟机无法获知和控制底层缓存配置细节，包括CPU缓存分区、共享情况以及内存到缓存的映射关系。

Method: 提出CacheX解决方案，使用驱逐集技术在虚拟机内部探测准确且细粒度的缓存抽象信息，无需硬件或虚拟机监控程序支持，并开发了LLC争用感知任务调度和虚拟颜色感知页面缓存管理两种新技术。

Result: 在x86 Linux内核中实现的CacheX评估表明，该方法能有效提高公共云虚拟机中各种工作负载的缓存利用率。

Conclusion: CacheX为云虚拟机提供了一种实用的缓存抽象探测方法，能够显著改善缓存利用效率，解决了云环境中缓存优化面临的可见性和控制性挑战。

Abstract: This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.

</details>


### [11] [Dynamic Edge Server Selection in Time-Varying Environments: A Reliability-Aware Predictive Approach](https://arxiv.org/abs/2511.10146)
*Jaime Sebastian Burbano,Arnova Abdullah,Eldiyar Zhantileuov,Mohan Liyanage,Rolf Schuster*

Main category: cs.DC

TL;DR: MO-HAN是一种轻量级边缘服务器选择方法，融合延迟预测、自适应可靠性和基于滞后的切换，在降低延迟的同时减少50%的切换次数。


<details>
  <summary>Details</summary>
Motivation: 多服务器架构中的动态网络拥塞对延迟敏感的嵌入式应用提出了边缘服务器选择的挑战。

Method: 使用被动测量（到达率、利用率、负载大小）和指数调制有理延迟模型，计算平衡预测延迟和可靠性的分数，确保只有在预期增益有意义时才进行切换。

Result: MO-HAN在降低平均和尾部延迟方面始终优于静态和公平分布基线，同时相比纯机会选择减少了近50%的切换次数。

Conclusion: MO-HAN无需侵入式检测或重型学习基础设施，适用于资源受限的嵌入式设备。

Abstract: Latency-sensitive embedded applications increasingly rely on edge computing, yet dynamic network congestion in multi-server architectures challenges proper edge server selection. This paper proposes a lightweight server-selection method for edge applications that fuses latency prediction with adaptive reliability and hysteresis-based handover. Using passive measurements (arrival rate, utilization, payload size) and an exponentially modulated rational delay model, the proposed Moderate Handover (MO-HAN) method computes a score that balances predicted latency and reliability to ensure handovers occur only when the expected gain is meaningful and maintain reduced end-to-end latency. Results show that MO-HAN consistently outperforms static and fair-distribution baselines by lowering mean and tail latencies, while reducing handovers by nearly 50% compared to pure opportunistic selection. These gains arise without intrusive instrumentation or heavy learning infrastructure, making MO-HAN practical for resource-constrained embedded devices.

</details>


### [12] [Selection of Supervised Learning-based Sparse Matrix Reordering Algorithms](https://arxiv.org/abs/2511.10180)
*Tao Tang,Youfu Jiang,Yingbo Cui,Jianbin Fang,Peng Zhang,Lin Peng,Chun Huang*

Main category: cs.DC

TL;DR: 提出基于监督学习的稀疏矩阵重排序算法选择模型，相比单独使用AMD算法可减少55.37%求解时间，平均加速比为1.45


<details>
  <summary>Details</summary>
Motivation: 传统稀疏矩阵排序算法选择依赖暴力搜索或经验知识，缺乏对不同稀疏矩阵结构的自适应能力

Method: 使用监督学习模型学习矩阵特征与常用重排序算法之间的关联，实现自动化智能选择

Result: 在Florida稀疏矩阵数据集上实验表明，模型能准确预测最优重排序算法

Conclusion: 该模型实现了稀疏矩阵重排序算法的智能选择，显著提升了求解效率

Abstract: Sparse matrix ordering is a vital optimization technique often employed for solving large-scale sparse matrices. Its goal is to minimize the matrix bandwidth by reorganizing its rows and columns, thus enhancing efficiency. Conventional methods for algorithm selection usually depend on brute-force search or empirical knowledge, lacking the ability to adjust to diverse sparse matrix structures.As a result, we have introduced a supervised learning-based model for choosing sparse matrix reordering algorithms. This model grasps the correlation between matrix characteristics and commonly utilized reordering algorithms, facilitating the automated and intelligent selection of the suitable sparse matrix reordering algorithm. Experiments conducted on the Florida sparse matrix dataset reveal that our model can accurately predict the optimal reordering algorithm for various matrices, leading to a 55.37% reduction in solution time compared to solely using the AMD reordering algorithm, with an average speedup ratio of 1.45.

</details>


### [13] [Workload Schedulers -- Genesis, Algorithms and Differences](https://arxiv.org/abs/2511.10258)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: 本文提出了一种现代工作负载调度器的分类方法，描述了操作系统进程调度器、集群系统作业调度器和大数据调度器三类调度器的演变历程、算法使用和特性，并讨论了它们之间的差异和相似之处。


<details>
  <summary>Details</summary>
Motivation: 随着计算系统从单机到分布式集群再到大数据平台的发展，工作负载调度器经历了显著的演变。本文旨在系统性地分类和描述不同类型的现代调度器，揭示它们在算法设计和功能特性上的发展轨迹。

Method: 通过分析三类调度器（操作系统进程调度器、集群系统作业调度器、大数据调度器）的历史演变，比较它们使用的算法和功能特性，建立分类框架并讨论其发展脉络。

Result: 构建了一个现代工作负载调度器的分类体系，揭示了从早期实现到现代实现的发展路径，识别了不同类型调度器在算法应用和功能设计上的差异与共性。

Conclusion: 尽管调度器应用于不同规模的系统（从本地到分布式），但在调度策略设计上存在显著的相似性，这反映了调度问题的本质特征在不同系统层级上的共通性。

Abstract: This paper presents a novel approach to categorization of modern workload schedulers. We provide descriptions of three classes of schedulers: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers and Big Data Schedulers. We describe their evolution from early adoptions to modern implementations, considering both the use and features of algorithms. In summary, we discuss differences between all presented classes of schedulers and discuss their chronological development. In conclusion we highlight similarities in the focus of scheduling strategies design, applicable to both local and distributed systems.

</details>


### [14] [FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing](https://arxiv.org/abs/2511.10442)
*Aarush Agarwal,Raymond He,Jan Kieseler,Matteo Cremonesi,Shah Rukh Qasim*

Main category: cs.DC

TL;DR: FastGraph是一种专为低维空间优化的GPU加速k近邻算法，用于加速图神经网络中的图构建，在2-10维空间中相比现有方法实现20-40倍加速且几乎无内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在图构建阶段存在性能瓶颈，特别是在低维空间（2-10维）中，需要高效的k近邻算法来加速图构建过程。

Method: 采用GPU驻留的箱分区方法，支持完整梯度流和自适应参数调优，提升计算和内存效率。

Result: 在维度小于10的情况下，相比FAISS、ANNOY和SCANN等先进库实现20-40倍加速，几乎无内存开销。

Conclusion: FastGraph显著提升了GNN工作流程的性能，特别适用于高能物理中的粒子聚类、视觉目标跟踪和图聚类等计算密集型低维应用。

Abstract: We introduce FastGraph, a novel GPU-optimized k-nearest neighbor algorithm specifically designed to accelerate graph construction in low-dimensional spaces (2-10 dimensions), critical for high-performance graph neural networks. Our method employs a GPU-resident, bin-partitioned approach with full gradient-flow support and adaptive parameter tuning, significantly enhancing both computational and memory efficiency. Benchmarking demonstrates that FastGraph achieves a 20-40x speedup over state-of-the-art libraries such as FAISS, ANNOY, and SCANN in dimensions less than 10 with virtually no memory overhead. These improvements directly translate into substantial performance gains for GNN-based workflows, particularly benefiting computationally intensive applications in low dimensions such as particle clustering in high-energy physics, visual object tracking, and graph clustering.

</details>


### [15] [Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs](https://arxiv.org/abs/2511.10480)
*Changhai Man,Joongun Park,Hanjiang Wu,Huan Xu,Srinivas Sridharan,Tushar Krishna*

Main category: cs.DC

TL;DR: STAGE是一个合成高保真执行轨迹的框架，用于准确建模大语言模型工作负载，支持大规模分布式系统配置的探索。


<details>
  <summary>Details</summary>
Motivation: 由于大规模AI基础设施主要限于大型云提供商，且现有平台的执行轨迹难以适应未来更大规模系统配置的研究需求，因此需要开发能够合成高保真执行轨迹的框架。

Method: 引入符号张量图生成器(STAGE)框架，支持全面的并行化策略，允许用户系统性地探索各种LLM架构和系统配置。

Result: STAGE展示了其可扩展性，能够合成覆盖超过32K GPU的高保真LLM轨迹，同时在计算、内存和通信方面保持张量级精度。

Conclusion: STAGE将公开可用，以促进分布式机器学习系统的进一步研究。

Abstract: Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [16] [History-Aware Trajectory k-Anonymization Using an FPGA-Based Hardware Accelerator for Real-Time Location Services](https://arxiv.org/abs/2511.09688)
*Hiroshi Nakano,Hiroaki Nishi*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA的实时轨迹k-匿名化方法，通过结合历史轨迹搜索和最短路径计算，在保护隐私的同时提高数据实用性。


<details>
  <summary>Details</summary>
Motivation: 解决先前仅依赖最短路径计算的轨迹匿名化方法无法捕捉真实旅行行为的问题，提高匿名化数据的实用性。

Method: 开发了新型FPGA硬件架构，集成并行历史轨迹搜索和传统最短路径查找，使用自定义定点计数模块准确加权历史数据贡献。

Result: 实现超过6,000条记录/秒的实时吞吐量，数据保留率相比先前仅最短路径设计提高1.2%，更有效地保留主要干道。

Conclusion: 这是关键进展，能够在LBS严格延迟约束下实现高保真、历史感知的匿名化，同时保护隐私和行为准确性。

Abstract: Our previous work established the feasibility of FPGA-based real-time trajectory anonymization, a critical task for protecting user privacy in modern location-based services (LBS). However, that pioneering approach relied exclusively on shortest-path computations, which can fail to capture re- alistic travel behavior and thus reduce the utility of the anonymized data. To address this limitation, this paper introduces a novel, history-aware trajectory k-anonymization methodology and presents an advanced FPGA-based hardware architecture to implement it. Our proposed architecture uniquely integrates par- allel history-based trajectory searches with conventional shortest- path finding, using a custom fixed-point counting module to ac- curately weigh contributions from historical data. This approach enables the system to prioritize behaviorally common routes over geometrically shorter but less-traveled paths. The FPGA implementation demonstrates that our new architecture achieves a real-time throughput of over 6,000 records/s, improves data retention by up to 1.2% compared to our previous shortest-path- only design, and preserves major arterial roads more effectively. These results signify a key advancement, enabling high-fidelity, history-aware anonymization that preserves both privacy and behavioral accuracy under the strict latency constraints of LBS.

</details>


### [17] [AssertMiner: Module-Level Spec Generation and Assertion Mining using Static Analysis Guided LLMs](https://arxiv.org/abs/2511.10007)
*Hongqin Lyu,Yonghao Wang,Jiaxin Zhou,Zhiteng Chao,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: AssertMiner是一个模块级断言生成框架，利用AST静态信息辅助LLM挖掘断言，提升验证覆盖率和错误检测能力


<details>
  <summary>Details</summary>
Motivation: 现有基于设计规范的断言生成方法通常只产生顶层断言，忽略了微架构级别模块中更频繁出现设计错误的实现细节验证需求

Method: 通过AST结构提取获取模块调用图、I/O表和数据流图，指导LLM生成模块级规范并挖掘模块级断言

Result: AssertMiner在生成模块高质量断言方面优于AssertLLM和Spec2Assertion方法，集成后可显著提升结构覆盖率和错误检测能力

Conclusion: AssertMiner能够实现更全面高效的验证过程，解决了模块级断言生成的局限性

Abstract: Assertion-based verification (ABV) is a key approach to checking whether a logic design complies with its architectural specifications. Existing assertion generation methods based on design specifications typically produce only top-level assertions, overlooking verification needs on the implementation details in the modules at the micro-architectural level, where design errors occur more frequently. To address this limitation, we present AssertMiner, a module-level assertion generation framework that leverages static information generated from abstract syntax tree (AST) to assist LLMs in mining assertions. Specifically, it performs AST-based structural extraction to derive the module call graph, I/O table, and dataflow graph, guiding the LLM to generate module-level specifications and mine module-level assertions. Our evaluation demonstrates that AssertMiner outperforms existing methods such as AssertLLM and Spec2Assertion in generating high-quality assertions for modules. When integrated with these methods, AssertMiner can enhance the structural coverage and significantly improve the error detection capability, enabling a more comprehensive and efficient verification process.

</details>


### [18] [The Role of Advanced Computer Architectures in Accelerating Artificial Intelligence Workloads](https://arxiv.org/abs/2511.10010)
*Shahid Amin,Syed Pervez Hussnain Shah*

Main category: cs.AR

TL;DR: 本文系统回顾了AI与计算机架构的协同演进，分析了GPU、ASIC、FPGA等主流AI加速器架构的设计理念、性能权衡，以及数据流优化、内存层次、稀疏化和量化等核心原则，并展望了存内计算和神经形态计算等新兴技术。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型（特别是深度神经网络）复杂性增加，其巨大的计算需求已超出传统架构的极限，需要专门研究AI加速器架构来应对这一挑战。

Method: 通过结构化回顾方法，分析主流AI加速器架构（GPU、ASIC、FPGA）的设计哲学、关键特征和性能权衡，并基于行业标准基准测试的定量性能数据进行综合评估。

Result: 研究表明不同的架构范式在性能、能效和灵活性方面存在显著差异，数据流优化、内存层次设计等核心原则对AI加速器的性能至关重要。

Conclusion: AI与计算机架构处于共生关系，硬件-软件协同设计不再是优化选项，而是未来计算进步的必要条件。

Abstract: The remarkable progress in Artificial Intelligence (AI) is foundation-ally linked to a concurrent revolution in computer architecture. As AI models, particularly Deep Neural Networks (DNNs), have grown in complexity, their massive computational demands have pushed traditional architectures to their limits. This paper provides a structured review of this co-evolution, analyzing the architectural landscape designed to accelerate modern AI workloads. We explore the dominant architectural paradigms Graphics Processing Units (GPUs), Appli-cation-Specific Integrated Circuits (ASICs), and Field-Programmable Gate Ar-rays (FPGAs) by breaking down their design philosophies, key features, and per-formance trade-offs. The core principles essential for performance and energy efficiency, including dataflow optimization, advanced memory hierarchies, spar-sity, and quantization, are analyzed. Furthermore, this paper looks ahead to emerging technologies such as Processing-in-Memory (PIM) and neuromorphic computing, which may redefine future computation. By synthesizing architec-tural principles with quantitative performance data from industry-standard benchmarks, this survey presents a comprehensive picture of the AI accelerator landscape. We conclude that AI and computer architecture are in a symbiotic relationship, where hardware-software co-design is no longer an optimization but a necessity for future progress in computing.

</details>


### [19] [Combined power management and congestion control in High-Speed Ethernet-based Networks for Supercomputers and Data Centers](https://arxiv.org/abs/2511.10159)
*Miguel Sánchez de la Rosa,Francisco J. andújar,Jesus Escudero-Sahuquillo,José L. Sánchez,Francisco J. Alfaro-Cortés*

Main category: cs.AR

TL;DR: 该论文研究数据中心和超级计算机网络的拥塞控制和节能管理，以及两者之间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 随着数据中心和超级计算机的普及，网络互连成为关键瓶颈。需要解决高负载下的拥塞问题和空闲时的能耗问题。

Method: 探索网络的两个方面：拥塞控制（防止高负载下性能下降）和电源管理（空闲时节能），并研究两者之间的相互作用。

Result: 未在摘要中明确说明具体结果。

Conclusion: 网络拥塞控制和节能管理是大型计算系统性能优化的两个关键方面，需要综合考虑它们的相互作用。

Abstract: The demand for computer in our daily lives has led to the proliferation of Datacenters that power indispensable many services. On the other hand, computing has become essential for some research for various scientific fields, that require Supercomputers with vast computing capabilities to produce results in reasonable time. The scale and complexity of these systems, compared to our day-to-day devices, are like comparing a cell to a living organism. To make them work properly, we need state-of-the-art technology and engineering, not just raw resources. Interconnecting the different computer nodes that make up a whole is a delicate task, as it can become the bottleneck for the whole infrastructure. In this work, we explore two aspects of the network: how to prevent degradation under heavy use with congestion control, and how to save energy when idle with power management; and how the two may interact.

</details>


### [20] [Beamspace Equalization for mmWave Massive MIMO: Algorithms and VLSI Implementations](https://arxiv.org/abs/2511.10563)
*Seyed Hadi Mirfarshbafan,Christoph Studer*

Main category: cs.AR

TL;DR: 本文提出了新的波束空间数据检测算法和VLSI架构，用于降低毫米波大规模MIMO系统中的数据检测功耗。


<details>
  <summary>Details</summary>
Motivation: 大规模多用户MIMO和毫米波通信是未来无线系统的关键技术，但其部署会带来高昂的基带处理硬件成本和功耗。波束空间处理利用毫米波频段的信道稀疏性来降低基带处理复杂度。

Method: 提出了复杂稀疏自适应均衡器(CSPADE)算法及其对应的VLSI架构，包括完全并行化实现和基于顺序乘累加(MAC)的架构。

Result: 在22nm FDSOI工艺中的VLSI实现结果表明，完全并行化的CSPADE相比天线域均衡可节省高达54%的功耗，而基于MAC的架构可节省高达66%的功耗。

Conclusion: 所提出的架构在现有大规模MIMO数据检测器中实现了最高的吞吐量，同时具有更好的能量和面积效率。

Abstract: Massive multiuser multiple-input multiple-output (MIMO) and millimeter-wave (mmWave) communication are key physical layer technologies in future wireless systems. Their deployment, however, is expected to incur excessive baseband processing hardware cost and power consumption. Beamspace processing leverages the channel sparsity at mmWave frequencies to reduce baseband processing complexity. In this paper, we review existing beamspace data detection algorithms and propose new algorithms as well as corresponding VLSI architectures that reduce data detection power. We present VLSI implementation results for the proposed architectures in a 22nm FDSOI process. Our results demonstrate that a fully-parallelized implementation of the proposed complex sparsity-adaptive equalizer (CSPADE) achieves up to 54% power savings compared to antenna-domain equalization. Furthermore, our fully-parallelized designs achieve the highest reported throughput among existing massive MIMO data detectors, while achieving better energy and area efficiency. We also present a sequential multiply-accumulate (MAC)-based architecture for CSPADE, which enables even higher power savings, i.e., up to 66%, compared to a MAC-based antenna-domain equalizer.

</details>
