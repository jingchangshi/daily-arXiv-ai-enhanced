<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling](https://arxiv.org/abs/2511.16947)
*Chenqi Zhao,Wenfei Wu,Linhai Song,Yuchen Xu*

Main category: cs.DC

TL;DR: MicroEP是一种新颖的并行化策略，通过在MoE系统中实现细粒度负载均衡来优化训练效率。


<details>
  <summary>Details</summary>
Motivation: MoE的动态特性导致专家间负载不平衡，严重影响训练效率，现有解决方案要么牺牲模型精度，要么引入额外系统开销。

Method: 提出MicroEP并行化策略，通过跨GPU的高效token调度实现每个微批次的最优负载均衡，并构建了MicroMoE分布式训练系统。

Result: 实验结果显示，MicroMoE相比最先进系统将端到端训练吞吐量提升高达47.6%，几乎始终实现GPU间最优负载均衡。

Conclusion: MicroEP能够有效解决MoE系统中的负载均衡问题，显著提升训练效率。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising approach to scale up deep learning models due to its significant reduction in computational resources. However, the dynamic nature of MoE leads to load imbalance among experts, severely impacting training efficiency. While previous research has attempted to address the load balancing challenge, existing solutions either compromise model accuracy or introduce additional system overhead. As a result, they fail to achieve fine-grained load balancing, which is crucial to optimizing training efficiency.
  We propose MicroEP, a novel parallelization strategy to achieve fine-grained load balancing in MoE systems. MicroEP is capable of achieving optimal load balancing in every micro-batch through efficient token scheduling across GPUs. Furthermore, we propose MicroMoE, an efficient distributed MoE training system with MicroEP's load balancing capabilities. Our experimental results demonstrate that MicroMoE improves the end-to-end training throughput by up to 47.6% compared with the state-of-the-art system, and almost consistently achieves optimal load balance among GPUs.

</details>


### [2] [Modeling Anomaly Detection in Cloud Services: Analysis of the Properties that Impact Latency and Resource Consumption](https://arxiv.org/abs/2511.17119)
*Gabriel Job Antunes Grabher,Fumio Machida,Thomas Ropars*

Main category: cs.DC

TL;DR: 本文研究性能异常检测器的特性如何优化云服务性能与成本之间的权衡，发现高精度和高召回率并非总是必要，检测频率会影响精度和召回率的重要性。


<details>
  <summary>Details</summary>
Motivation: 云服务中性能异常的检测和解决对维持性能目标至关重要，但异常检测器会出错，需要研究哪些检测特性对优化性能与成本权衡最重要。

Method: 使用随机奖励网对由性能异常检测器监控的云服务进行建模，分析检测器特性（精度、召回率和检查频率）对平均延迟和资源消耗的影响。

Result: 结果显示，如果检测可以频繁运行，高精度足以获得良好的性能成本权衡；但如果检测器运行不频繁，召回率变得最重要。高精度和高召回率并非总是必要。

Conclusion: 性能异常检测器的优化应根据检测频率来权衡精度和召回率的重要性，频繁检测时精度更重要，不频繁检测时召回率更关键。

Abstract: Detecting and resolving performance anomalies in Cloud services is crucial for maintaining desired performance objectives. Scaling actions triggered by an anomaly detector help achieve target latency at the cost of extra resource consumption. However, performance anomaly detectors make mistakes. This paper studies which characteristics of performance anomaly detection are important to optimize the trade-off between performance and cost. Using Stochastic Reward Nets, we model a Cloud service monitored by a performance anomaly detector. Using our model, we study the impact of detector characteristics, namely precision, recall and inspection frequency, on the average latency and resource consumption of the monitored service. Our results show that achieving a high precision and a high recall is not always necessary. If detection can be run frequently, a high precision is enough to obtain a good performance-to-cost trade-off, but if the detector is run infrequently, recall becomes the most important.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [3] [Vorion: A RISC-V GPU with Hardware-Accelerated 3D Gaussian Rendering and Training](https://arxiv.org/abs/2511.16831)
*Yipeng Wang,Mengtian Yang,Chieh-pu Lo,Jaydeep P. Kulkarni*

Main category: cs.AR

TL;DR: Vorion是首个支持硬件加速3D高斯泼溅渲染和训练的GPGPU原型，通过可扩展架构、最小硬件改动、z-tiling和混合数据流实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术在实时神经渲染中表现出色，但其计算需求巨大，导致在边缘设备上无法实现实时渲染，在工作站上无法实现实时4D重建，因此需要专用硬件加速。

Method: 提出Vorion架构，采用可扩展设计，对传统光栅化器进行最小硬件改动，引入z-tiling增加并行性，使用高斯/像素中心混合数据流，在TSMC 16nm工艺上实现原型系统。

Result: 最小系统（8个SIMT核心，2个高斯光栅化器）实现19 FPS渲染性能；扩展设计（16个光栅化器）实现38.6次迭代/秒的训练速度。

Conclusion: Vorion证明了硬件加速3D高斯泼溅的可行性，为下一代GPU图形流水线中的专用硬件提供了有力案例。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a foundational technique for real-time neural rendering, 3D scene generation, volumetric video (4D) capture. However, its rendering and training impose massive computation, making real-time rendering on edge devices and real-time 4D reconstruction on workstations currently infeasible. Given its fixed-function nature and similarity with traditional rasterization, 3DGS presents a strong case for dedicated hardware in the graphics pipeline of next-generation GPUs. This work, Vorion, presents the first GPGPU prototype with hardware-accelerated 3DGS rendering and training. Vorion features scalable architecture, minimal hardware change to traditional rasterizers, z-tiling to increase parallelism, and Gaussian/pixel-centric hybrid dataflow. We prototype the minimal system (8 SIMT cores, 2 Gaussian rasterizer) using TSMC 16nm FinFET technology, which achieves 19 FPS for rendering. The scaled design with 16 rasterizers achieves 38.6 iterations/s for training.

</details>


### [4] [Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration](https://arxiv.org/abs/2511.17123)
*Jiaxun Fang,Li Zhang,Shaoyi Huang*

Main category: cs.AR

TL;DR: 提出了一种面向能量感知的层间压缩框架，通过结合MAC单元能量特性和层间能量特征，在量化感知训练中进行能量-精度协同优化的权重选择，实现CNN模型在脉动阵列加速器上的能量优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用全局激活模型、粗糙能量代理或层无关策略，限制了在真实硬件上的有效性。需要更精确的层间能量模型和压缩策略来优化CNN在脉动阵列加速器上的能量消耗。

Method: 1) 构建层感知MAC能量模型，结合每层激活统计和22位部分和转换的MSB-汉明距离分组；2) 在量化感知训练中引入能量-精度协同优化的权重选择算法；3) 提出能量优先的层间调度策略，在全局精度约束下更积极地压缩高能耗层。

Result: 在不同CNN模型上实验，实现了高达58.6%的能量降低，精度下降仅为2-3%，优于现有的功率感知基准方法。

Conclusion: 该框架通过精确的层间能量建模和能量优先的压缩策略，有效降低了脉动阵列加速器中CNN模型的能量消耗，同时保持了较高的精度。

Abstract: Systolic array accelerators execute CNNs with energy dominated by the switching activity of multiply accumulate (MAC) units. Although prior work exploits weight dependent MAC power for compression, existing methods often use global activation models, coarse energy proxies, or layer-agnostic policies, which limits their effectiveness on real hardware. We propose an energy aware, layer-wise compression framework that explicitly leverages MAC and layer level energy characteristics. First, we build a layer-aware MAC energy model that combines per-layer activation statistics with an MSB-Hamming distance grouping of 22-bit partial sum transitions, and integrate it with a tile-level systolic mapping to estimate convolution-layer energy. On top of this model, we introduce an energy accuracy co-optimized weight selection algorithm within quantization aware training and an energy-prioritized layer-wise schedule that compresses high energy layers more aggressively under a global accuracy constraint. Experiments on different CNN models demonstrate up to 58.6\% energy reduction with 2-3\% accuracy drop, outperforming a state-of-the-art power-aware baseline.

</details>


### [5] [NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices](https://arxiv.org/abs/2511.17235)
*Rohit Prasad*

Main category: cs.AR

TL;DR: NX-CGRA是一种针对边缘计算中Transformer推理的可编程硬件加速器，采用粗粒度可重构阵列架构，支持线性和非线性函数，在性能、能效和架构灵活性之间实现平衡。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中Transformer工作负载的多样性和复杂性日益增加，需要在性能、能效和架构灵活性之间找到平衡点，而固定功能加速器无法适应广泛的用例。

Method: 采用粗粒度可重构阵列架构，具有软件驱动的可编程性，能够高效执行各种内核模式，支持线性和非线性Transformer推理算法。

Result: 使用真实Transformer模型的代表性基准测试进行评估，展示了高整体效率和在不同操作类别上有利的能耗-面积权衡。

Conclusion: NX-CGRA作为可扩展和适应性强的硬件解决方案，在受限的功率和硅预算下具有边缘Transformer部署的潜力。

Abstract: The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets.

</details>


### [6] [DISCA: A Digital In-memory Stochastic Computing Architecture Using A Compressed Bent-Pyramid Format](https://arxiv.org/abs/2511.17265)
*Shady Agwa,Yikang Shen,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: 提出了一种名为DISCA的新型数字内存随机计算架构，采用压缩的准随机Bent-Pyramid数据格式，在保持数字系统可扩展性和可靠性的同时，实现了接近模拟计算的计算简单性。


<details>
  <summary>Details</summary>
Motivation: 传统冯·诺依曼架构面临内存墙和摩尔定律终结的挑战，而现有内存计算架构（包括模拟和数字）由于设计限制导致性能大幅下降。AI应用向边缘迁移对硬件预算提出了更多约束。

Method: 使用压缩的准随机Bent-Pyramid数据格式的数字内存随机计算架构DISCA，结合了模拟计算的计算简单性和数字系统的可扩展性、生产性和可靠性。

Result: 在商用180nm CMOS技术下，DISCA在500MHz频率下实现了每比特3.59 TOPS/W的能效，相比同类架构将矩阵乘法工作负载的能效提高了数个数量级。

Conclusion: DISCA架构通过创新的数据格式和计算方式，有效解决了内存墙问题，为边缘AI应用提供了高能效的计算解决方案。

Abstract: Nowadays, we are witnessing an Artificial Intelligence revolution that dominates the technology landscape in various application domains, such as healthcare, robotics, automotive, security, and defense. Massive-scale AI models, which mimic the human brain's functionality, typically feature millions and even billions of parameters through data-intensive matrix multiplication tasks. While conventional Von-Neumann architectures struggle with the memory wall and the end of Moore's Law, these AI applications are migrating rapidly towards the edge, such as in robotics and unmanned aerial vehicles for surveillance, thereby adding more constraints to the hardware budget of AI architectures at the edge. Although in-memory computing has been proposed as a promising solution for the memory wall, both analog and digital in-memory computing architectures suffer from substantial degradation of the proposed benefits due to various design limitations. We propose a new digital in-memory stochastic computing architecture, DISCA, utilizing a compressed version of the quasi-stochastic Bent-Pyramid data format. DISCA inherits the same computational simplicity of analog computing, while preserving the same scalability, productivity, and reliability of digital systems. Post-layout modeling results of DISCA show an energy efficiency of 3.59 TOPS/W per bit at 500 MHz using a commercial 180nm CMOS technology. Therefore, DISCA significantly improves the energy efficiency for matrix multiplication workloads by orders of magnitude if scaled and compared to its counterpart architectures.

</details>


### [7] [MemIntelli: A Generic End-to-End Simulation Framework for Memristive Intelligent Computing](https://arxiv.org/abs/2511.17418)
*Houji Zhou,Ling Yang,Zhiwei Zhou,Yi Li,Xiangshui Miao*

Main category: cs.AR

TL;DR: 提出了MemIntelli，一个支持灵活可变精度计算的内存计算端到端仿真框架，用于在忆阻器件上预验证各种智能应用。


<details>
  <summary>Details</summary>
Motivation: 忆阻内存计算存在电路与算法耦合问题，计算可靠性易受器件和外围电路非理想效应影响，需要高效的软硬件协同仿真工具将器件和电路模型嵌入算法。

Method: 在器件和电路层面使用数学函数通过等效电路建模抽象器件和电路；在架构层面实现支持整数和浮点数据表示的灵活可变精度内存计算；兼容NumPy和PyTorch实现与应用的集成。

Result: 通过方程求解、数据聚类、小波变换、神经网络训练和推理等智能算法验证了MemIntelli的强大处理能力。

Conclusion: 该研究提供了一个全面的仿真工具，促进了从器件到应用的内存计算系统协同设计。

Abstract: Memristive in-memory computing (IMC) has emerged as a promising solution for addressing the bottleneck in the Von Neumann architecture. However, the couplingbetweenthecircuitandalgorithm in IMC makes computing reliability susceptible to non-ideal effects in devices and peripheral circuits. In this respect, efficient softwarehardwareco-simulationtoolsarehighlydesiredtoembedthedevice and circuit models into the algorithms. In this paper, for the first time, we proposed an end-to-end simulation framework supporting flexible variable-precision computing, named MemIntelli, to realize the pre-verification of diverse intelligent applications on memristive devices. At the device and circuit level, mathematical functions are employed to abstract the devices and circuits through meticulous equivalent circuit modeling. On the architecture level, MemIntelli achieves flexible variable-precision IMC supporting integer and floating data representation with bit-slicing. Moreover, MemIntelli is compatible with NumPy and PyTorch for seamless integration with applications. To demonstrate its capabilities, diverse intelligent algorithms, such as equation solving, data clustering, wavelet transformation, and neural network training and inference, were employed to showcase the robust processing ability of MemIntelli. This research presents a comprehensive simulation tool that facilitates the co-design of the IMC system, spanning from device to application.

</details>
