{"id": "2509.15782", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.15782", "abs": "https://arxiv.org/abs/2509.15782", "authors": ["Evgenii Rezunov", "Niko Zurstraßen", "Lennart M. Reimann", "Rainer Leupers"], "title": "Automatic Microarchitecture-Aware Custom Instruction Design for RISC-V Processors", "comment": "PREPRINT - Accepted for publication at the 2025 IEEE/ACM\n  International Conference On Computer-Aided Design (ICCAD)", "summary": "An Application-Specific Instruction Set Processor(ASIP) is a specialized\nmicroprocessor that provides a trade-off between the programmability of a\nGeneral Purpose Processor (GPP) and the performance and energy-efficiency of\ndedicated hardware accelerators. ASIPs are often derived from off-the-shelf\nGPPs extended by custom instructions tailored towards a specific software\nworkload. One of the most important challenges of designing an ASIP is to find\nsaid custom instructions that help to increase performance without being too\ncostly in terms of area and power consumption. To date, solving this challenge\nis relatively labor-intensive and typically performed manually. Addressing the\nlack of automation, we present Custom Instruction Designer for RISC-V\nExtensions (CIDRE), a front-to-back tool for ASIP design. CIDRE automatically\nanalyzes hotspots in RISC-V applications and generates custom instruction\nsuggestions with a corresponding nML description. The nML description can be\nused with other electronic design automation tools to accurately assess the\ncost and benefits of the found suggestions. In a RISC-V benchmark study, we\nwere able to accelerate embedded benchmarks from Embench and MiBench by up to\n2.47x with less than 24% area increase. The entire process was conducted\ncompletely automatically."}
{"id": "2509.15834", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.15834", "abs": "https://arxiv.org/abs/2509.15834", "authors": ["Shardul Chiplunkar", "Clément Pit-Claudel"], "title": "Automatic layout of railroad diagrams", "comment": "24 pages (+2 appendix, +3 references); 22 figures (+4 appendix); 3\n  tables", "summary": "Railroad diagrams (also called \"syntax diagrams\") are a common, intuitive\nvisualization of grammars, but limited tooling and a lack of formal attention\nto their layout mostly confines them to hand-drawn documentation. We present\nthe first formal treatment of railroad diagram layout along with a principled,\npractical implementation. We characterize the problem as compiling a *diagram\nlanguage* (specifying conceptual components and how they connect and compose)\nto a *layout language* (specifying basic graphical shapes and their sizes and\npositions). We then implement a compiler that performs *line wrapping* to meet\na target width, as well as vertical *alignment* and horizontal *justification*\nper user-specified policies. We frame line wrapping as an optimization problem,\nwhere we describe principled dimensions of optimality and implement\ncorresponding heuristics. For front-end evaluation, we show that our diagram\nlanguage is well-suited for common applications by describing how regular\nexpressions and Backus-Naur form can be compiled to it. For back-end\nevaluation, we argue that our compiler is practical by comparing its output to\ndiagrams laid out by hand and by other tools."}
{"id": "2509.15450", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.15450", "abs": "https://arxiv.org/abs/2509.15450", "authors": ["Abhishek Vijaya Kumar", "Arjun Devraj", "Rachee Singh"], "title": "PCCL: Photonic circuit-switched collective communication for distributed ML", "comment": null, "summary": "Modern distributed ML suffers from a fundamental gap between the theoretical\nand realized performance of collective communication algorithms due to\ncongestion and hop-count induced dilation in practical GPU clusters. We present\nPCCL, a Photonic Collective Communication Library that reconfigures the network\ntopology to match the communication patterns of collective algorithms, thereby\neliminating congestion and dilation by creating direct, contention-free\ncircuits between communicating GPUs. Unlike prior approaches that synthesize\nalgorithms for specific network topologies and collectives, PCCL generalizes to\nany collective primitive and any topology by adapting the network to match each\nalgorithm's communication pattern. PCCL's key innovation lies in its\nhardware-agnostic optimization framework that intelligently decides when to\nreconfigure based on the trade-off between network reconfiguration delay and\ncongestion/dilation costs, making it practical across different optical\nhardware with varying switching speeds. Our evaluation demonstrates that PCCL\nachieves up to 3X speedup over state-of-the-art algorithms on 128 GPUs across\nvarious workloads, buffer sizes, and topologies, translating to a 1.3X speedup\nin end-to-end training throughput."}
{"id": "2509.15847", "categories": ["cs.DC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.15847", "abs": "https://arxiv.org/abs/2509.15847", "authors": ["Qianyu Yu", "Giuliano Losa", "Nibesh Shrestha", "Xuechao Wang"], "title": "Angelfish: Consensus with Optimal Throughput and Latency Across the Leader-DAG Spectrum", "comment": null, "summary": "To maximize performance, many modern blockchain systems rely on\neventually-synchronous, Byzantine fault-tolerant (BFT) consensus protocols. Two\nprotocol designs have emerged in this space: protocols that minimize latency\nusing a leader that drives both data dissemination and consensus, and protocols\nthat maximize throughput using a separate, asynchronous data dissemination\nlayer. Recent protocols such as Partially-Synchronous Bullshark and Sailfish\ncombine elements of both approaches by using a DAG to enable parallel data\ndissemination and a leader that paces DAG formation. This improves latency\nwhile achieving state-of-the-art throughput. Yet the latency of leader-based\nprotocols is still better under moderate loads.\n  We present Angelfish, a hybrid protocol that adapts smoothly across this\ndesign space, from leader-based to Sailfish-like DAG-based consensus. Angelfish\nlets a dynamically-adjusted subset of parties use best-effort broadcast to\nissue lightweight votes instead of reliably broadcasting costlier DAG vertices.\nThis reduces communication, helps lagging nodes catch up, and lowers latency in\npractice compared to prior DAG-based protocols. Our empirical evaluation shows\nthat Angelfish attains state-of-the-art peak throughput while matching the\nlatency of leader-based protocols under moderate throughput, delivering the\nbest of both worlds."}
{"id": "2509.15940", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.15940", "abs": "https://arxiv.org/abs/2509.15940", "authors": ["Guoliang He", "Youhe Jiang", "Wencong Xiao", "Kaihua Jiang", "Shuguang Wang", "Jun Wang", "Zixian Du", "Zhuo Jiang", "Xinlei Zhang", "Binhang Yuan", "Eiko Yoneki"], "title": "Efficient Pre-Training of LLMs via Topology-Aware Communication Alignment on More Than 9600 GPUs", "comment": "NeurIPS 2025", "summary": "The scaling law for large language models (LLMs) depicts that the path\ntowards machine intelligence necessitates training at large scale. Thus,\ncompanies continuously build large-scale GPU clusters, and launch training jobs\nthat span over thousands of computing nodes. However, LLM pre-training presents\nunique challenges due to its complex communication patterns, where GPUs\nexchange data in sparse yet high-volume bursts within specific groups.\nInefficient resource scheduling exacerbates bandwidth contention, leading to\nsuboptimal training performance. This paper presents Arnold, a scheduling\nsystem summarizing our experience to effectively align LLM communication\npatterns with data center topology at scale. An in-depth characteristic study\nis performed to identify the impact of physical network topology to LLM\npre-training jobs. Based on the insights, we develop a scheduling algorithm to\neffectively align communication patterns with the physical network topology in\nmodern data centers. Through simulation experiments, we show the effectiveness\nof our algorithm in reducing the maximum spread of communication groups by up\nto $1.67$x. In production training, our scheduling system improves the\nend-to-end performance by $10.6\\%$ when training with more than $9600$ GPUs, a\nsignificant improvement for our training pipeline."}
