<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 17]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Mechanizing Synthetic Tait Computability in Istari](https://arxiv.org/abs/2509.11418)
*Runming Li,Yue Yao,Robert Harper*

Main category: cs.PL

TL;DR: 在Istari证明助手中实现了合成Tait可计算性(STC)的形式化，包括模态、扩展类型和严格胶合类型等可重用库，并应用于两个案例研究：带大消除的依赖类型理论的典范性模型和成本感知逻辑框架的Kripke典范性模型


<details>
  <summary>Details</summary>
Motivation: 范畴胶合是证明类型理论元定理(如典范性和正规化)的强大技术，STC通过将胶合范畴内化到具有相位区分的模态依赖类型理论中，为复杂胶合模型提供抽象处理。本研究旨在在Istari中机械化STC

Method: 在Istari证明助手中开发可重用的合成相位区分库，包括模态、扩展类型和严格胶合类型，然后将STC核心构造形式化，并应用于两个具体案例研究

Result: 成功在Istari中实现了STC的形式化，核心构造可以几乎逐字地形式化，保持了纸上论证的优雅性，同时确保了机器检查的正确性

Conclusion: Istari的等式反射特性消除了内涵证明助手中典型的传输推理麻烦，使得STC的机械化既保持了数学优雅性又获得了形式化验证的可靠性

Abstract: Categorical gluing is a powerful technique for proving meta-theorems of type
theories such as canonicity and normalization. Synthetic Tait Computability
(STC) provides an abstract treatment of the complex gluing models by
internalizing the gluing category into a modal dependent type theory with a
phase distinction. This work presents a mechanization of STC in the Istari
proof assistant. Istari is a Martin-L\"{o}f-style extensional type theory with
equality reflection. Equality reflection eliminates the nuisance of transport
reasoning typically found in intensional proof assistants. This work develops a
reusable library for synthetic phase distinction, including modalities,
extension types, and strict glue types, and applies it to two case studies: (1)
a canonicity model for dependent type theory with dependent products and
booleans with large elimination, and (2) a Kripke canonicity model for the
cost-aware logical framework. Our results demonstrate that the core STC
constructions can be formalized essentially verbatim in Istari, preserving the
elegance of the on-paper arguments while ensuring machine-checked correctness.

</details>


### [2] [Expressive Power of One-Shot Control Operators and Coroutines](https://arxiv.org/abs/2509.11901)
*Kentaro Kobayashi,Yukiyoshi Kameyama*

Main category: cs.PL

TL;DR: 这篇论文通过数学严格的宏表达力分析，证实了一键控制操作符中不对称协程比效果处理器和界定续施更强大的氛间假说


<details>
  <summary>Details</summary>
Motivation: 填补一键控制操作符在表达力方面的研究空白，对比不同一键控制操作符的表达力强度

Method: 采用Felleisen的宏表达力作为表达力度衡量标准，对一键效果处理器、界定续施和不对称协程进行数学严格的对比分析

Result: 验证了不对称协程可以宏表达一键效果处理器和界定续施，但反之则不行，并修正了之前非正式论证的错误

Conclusion: 一键控制操作符在表达力上存在明显的层次结构，不对称协程具有最强的表达力，这为程序语言设计和效果系统提供了重要的理论基础

Abstract: Control operators, such as exceptions and effect handlers, provide a means of
representing computational effects in programs abstractly and modularly. While
most theoretical studies have focused on multi-shot control operators, one-shot
control operators -- which restrict the use of captured continuations to at
most once -- are gaining attention for their balance between expressiveness and
efficiency. This study aims to fill the gap. We present a mathematically
rigorous comparison of the expressive power among one-shot control operators,
including effect handlers, delimited continuations, and even asymmetric
coroutines. Following previous studies on multi-shot control operators, we
adopt Felleisen's macro-expressiveness as our measure of expressiveness. We
verify the folklore that one-shot effect handlers and one-shot
delimited-control operators can be macro-expressed by asymmetric coroutines,
but not vice versa. We explain why a previous informal argument fails, and how
to revise it to make a valid macro-translation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Asynchronous Gathering of Opaque Robots with Mobility Faults](https://arxiv.org/abs/2509.10711)
*Subhajit Pramanick,Saswata Jana,Partha Sarathi Mandal,Gokarna Sharma*

Main category: cs.DC

TL;DR: 本文在异步LUMI模型中研究移动故障下的机器人聚集问题，提出了基于颜色数量的最优算法和时间-颜色权衡算法


<details>
  <summary>Details</summary>
Motivation: 解决传统故障模型下机器人聚集的不可行性问题，特别是在(2,1)-故障系统中，通过引入移动故障模型来突破现有限制

Method: 使用LUMI模型（带灯光的机器人），提出两种确定性算法：使用3色灯的最优算法，以及使用7色和26色灯的时间-颜色权衡算法

Result: 证明了在(2,1)-移动故障系统中使用2色灯不可行，但使用3色灯可行且最优；提出了在(N,f)-系统中的高效算法，时间复杂度分别为O(N)和O(max{l,f})

Conclusion: 移动故障模型为机器人聚集问题提供了新的解决途径，提出的算法在颜色数量和时间复杂度方面都达到了最优或接近最优的性能

Abstract: We consider the fundamental benchmarking problem of gathering in an
$(N,f)$-fault system consisting of $N$ robots, of which at most $f$ might fail
at any execution, under asynchrony. Two seminal results established
impossibility of a solution in the oblivious robot (OBLOT) model in a
$(2,0)$-fault system under semi-synchrony and in a $(3,1)$-Byzantine fault
system under asynchrony. Recently, a breakthrough result circumvented the first
impossibility result by giving a deterministic algorithm in a $(2,0)$-fault
system under asynchrony in the luminous robot (LUMI) model using 2-colored
lights. However, a breakthrough result established impossibility of gathering
in a $(2,1)$-crash system in the LUMI model under semi-synchrony. In this
paper, we consider a {\em mobility fault} model in which a robot crash only
impacts it mobility but not the operation of the light.
  We establish four results under asynchrony in LUMI with the mobility fault
model. We show that it is impossible to solve gathering in a $(2,1)$-mobility
fault system using 2-colored lights, and then give a solution using 3-colored
lights, which is optimal w.r.t. the number of colors. We then consider an
$(N,f)$-mobility fault system, $f<N$, both $N,f$ not known, and give two
deterministic algorithms that exhibit a nice time-color trade-off: The first
with time $O(N)$ using 7-colored lights and the second with time
$O(\max\{\ell,f\})$ using 26-colored lights, where $\ell< N$ is the number of
distinct convex layers of robot positions in the initial configuration.
Interestingly, for $l, f = O(1)$, our result is optimal. Our algorithms for an
$(N,f)$-mobility fault system are the first to be analysed time complexity, can
withstand obstructed visibility (opaque robot model) and asynchronous
scheduling.

</details>


### [4] [MinatoLoader: Accelerating Machine Learning Training Through Efficient Data Preprocessing](https://arxiv.org/abs/2509.10712)
*Rahma Nouaji,Stella Bitchebe,Ricardo Macedo,Oana Balmau*

Main category: cs.DC

TL;DR: MinatoLoader是一个针对PyTorch的通用数据加载器，通过优先处理预处理速度快的样本并并行处理慢样本，解决了现有数据加载器因预处理时间差异导致的GPU空闲问题，显著提升了训练速度和GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 现有数据加载器（如PyTorch DataLoader）在处理具有预处理时间差异的数据样本时效率低下，会导致高达76%的GPU空闲时间，因为整个批次的处理会被单个慢样本延迟，造成训练管道阻塞。

Method: MinatoLoader在单服务器多GPU设置中，通过在后台持续准备数据并主动构建批次，优先处理预处理速度快的样本，同时并行处理较慢的样本，以避免头部阻塞问题。

Result: 在配备四个A100 GPU的机器上，MinatoLoader相比PyTorch DataLoader和Pecan将训练时间提升了最高7.5倍（平均3.6倍），相比DALI提升了最高3倍（平均2.2倍），并将平均GPU利用率从46.4%提升至90.45%，同时保持模型精度并实现更快收敛。

Conclusion: MinatoLoader通过智能的样本优先级调度和并行处理机制，有效解决了数据预处理中的效率瓶颈问题，显著提升了深度学习训练的性能和资源利用率。

Abstract: Data loaders are used by Machine Learning (ML) frameworks like PyTorch and
TensorFlow to apply transformations to data before feeding it into the
accelerator. This operation is called data preprocessing. Data preprocessing
plays an important role in the ML training workflow because if it is
inefficiently pipelined with the training, it can yield high GPU idleness,
resulting in important training delays. Unfortunately, existing data loaders
turn out to waste GPU resources, with $76\%$ GPU idleness when using the
PyTorch data loader, for example. One key source of inefficiency is the
variability in preprocessing time across samples within the same dataset.
Existing data loaders are oblivious to this variability, and they construct
batches without any consideration of slow or fast samples. In this case, the
entire batch is delayed by a single slow sample, stalling the training pipeline
and resulting in head-of-line blocking.
  To address these inefficiencies, we present MinatoLoader, a general-purpose
data loader for PyTorch that accelerates training and improves GPU utilization.
MinatoLoader is designed for a single-server setup, containing multiple GPUs.
It continuously prepares data in the background and actively constructs batches
by prioritizing fast-to-preprocess samples, while slower samples are processed
in parallel.
  We evaluate MinatoLoader on servers with V100 and A100 GPUs. On a machine
with four A100 GPUs, MinatoLoader improves the training time of a wide range of
workloads by up to $7.5\times$ ($3.6\times$ on average) over PyTorch DataLoader
and Pecan, and up to $3\times$ ($2.2\times$ on average) over DALI. It also
increases average GPU utilization from 46.4\% with PyTorch to 90.45\%, while
preserving model accuracy and enabling faster convergence.

</details>


### [5] [Coordinated Reinforcement Learning Prefetching Architecture for Multicore Systems](https://arxiv.org/abs/2509.10719)
*Mohammed Humaid Siddiqui,Fernando Guzman,Yufei Wu,Ruishu Ann*

Main category: cs.DC

TL;DR: CRL-Pythia是一个基于协调强化学习的硬件预取器，专门为多核系统设计，通过跨核信息共享和协作预取决策，显著减少冗余预取请求并提高学习收敛性。


<details>
  <summary>Details</summary>
Motivation: 多核架构下传统预取器面临严重挑战：独立核心操作导致大量冗余预取请求（高达20%的重复请求），造成不必要的内存总线流量和带宽浪费；先进预取器如Pythia在从单核扩展到四核系统时性能损失约10%。

Method: 提出CRL-Pythia协调强化学习预取器，通过跨核信息共享和协作预取决策机制，减少冗余预取请求并改善多核间的学习收敛性。

Result: CRL-Pythia在所有情况下都优于单Pythia配置，在带宽受限工作负载下实现约12%的IPC（每周期指令数）提升，同时硬件开销适中。敏感性分析验证了其鲁棒性和可扩展性。

Conclusion: CRL-Pythia是当代多核系统的一个实用高效解决方案，通过协调学习机制有效解决了多核环境下的预取冗余和性能损失问题。

Abstract: Hardware prefetching is critical to fill the performance gap between CPU
speeds and slower memory accesses. With multicore architectures becoming
commonplace, traditional prefetchers are severely challenged. Independent core
operation creates significant redundancy (up to 20% of prefetch requests are
duplicates), causing unnecessary memory bus traffic and wasted bandwidth.
Furthermore, cutting-edge prefetchers such as Pythia suffer from about a 10%
performance loss when scaling from a single-core to a four-core system. To
solve these problems, we propose CRL-Pythia, a coordinated reinforcement
learning based prefetcher specifically designed for multicore systems. In this
work, CRL-Pythia addresses these issues by enabling cross-core sharing of
information and cooperative prefetching decisions, which greatly reduces
redundant prefetch requests and improves learning convergence across cores. Our
experiments demonstrate that CRL-Pythia outperforms single Pythia
configurations in all cases, with approximately 12% IPC (instructions per
cycle) improvement for bandwidth-constrained workloads, while imposing moderate
hardware overhead. Our sensitivity analyses also verify its robustness and
scalability, thereby making CRL-Pythia a practical and efficient solution to
contemporary multicore systems.

</details>


### [6] [Enhancing Type Safety in MPI with Rust: A Statically Verified Approach for RSMPI](https://arxiv.org/abs/2509.10803)
*Nafees Iqbal,Jed Brown*

Main category: cs.DC

TL;DR: 基于Rust语言的RSMPI库构建类型安全的MPI通信框架，通过TypedCommunicator强制静态类型安全，消除MPI编程中的运行时错误和类型不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 解决MPI低级接口缺乏类型安全导致的运行时错误、未定义行为和调试困难，特别是在大规模应用中。

Method: 基于RSMPI库构建TypedCommunicator抽象，利用Rust的Equivalence trait强制类型安全，支持单值和切片通信，提供直观API。

Result: 该框架消除了常见MPI错误，提高开发者生产力，同时保持性能，遵循零成本抽象原则。

Conclusion: 为扩展类型安全到集体操作奠定了基础，推进Rust并行计算的稳健性。

Abstract: The Message Passing Interface (MPI) is a fundamental tool for building
high-performance computing (HPC) applications, enabling efficient communication
across distributed systems. Despite its widespread adoption, MPI's low-level
interface and lack of built-in type safety make it prone to runtime errors,
undefined behavior, and debugging challenges, especially in large-scale
applications. Rust, a modern systems programming language, offers a compelling
solution with its strong type system, which enforces memory and type safety at
compile time without compromising performance. This paper introduces a
type-safe communication framework for MPI, built on the RSMPI library, to
address the limitations of traditional MPI programming. At its core is the
TypedCommunicator, an abstraction that enforces static type safety in
point-to-point communication operations. By leveraging Rust's Equivalence
trait, our framework guarantees that only compatible types can participate in
communication, catching mismatches either at compile time or through runtime
validation. The framework supports both single-value and slice-based
communication, providing an intuitive API for diverse data structures. Our
implementation demonstrates that this approach eliminates common MPI errors,
improves developer productivity, and maintains performance, adhering to Rust's
principle of zero-cost abstractions. This work lays the foundation for
extending type safety to collective operations, advancing the robustness of
parallel computing in Rust.

</details>


### [7] [Chameleon: Taming Dynamic Operator Sequences for Memory-Intensive LLM Training](https://arxiv.org/abs/2509.11076)
*Zibo Wang,Yuhang Zhou,Zhibin Wang,Shipeng Li,Xinjing Huang,Chendong Cai,Bingxu Mu,Yuqing Sun,Zhiheng Hu,Bin She,Shu You,Guanghuan Fang,Rong Gu,Wanchun Dou,Guihai Chen,Chen Tian*

Main category: cs.DC

TL;DR: Chameleon是一种针对Eager Mode下变长操作序列的swap内存优化方法，通过轻量级在线分析器和优化策略执行模块，显著降低分析开销并提升性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练时内存需求激增，现有swap方法假设操作序列一致，但在Eager Mode中操作序列会变化，需要新的解决方案

Method: 重新设计swap内存优化的端到端流程，引入轻量级在线分析器进行持续监控，生成有限操作信息下的有效swap策略，优化策略执行模块

Result: 分析开销降低84.25%，支持训练比硬件内存大4倍的模型，适应操作序列变化，性能比重计算或高度并行化提升38.94%

Conclusion: Chameleon是首个考虑Eager Mode变长操作序列的swap优化方法，有效解决了现有方法的局限性，显著提升了内存优化效果

Abstract: The increasing size of large language models (LLMs) has led to a surge in
memory requirements during training, often exceeding the capacity of
high-bandwidth memory (HBM). Swap-based memory optimization incurs neither
accuracy loss nor additional end-to-end overhead when effectively overlapped,
thus being an attractive solution. However, existing swap methods assume
consistent operator sequences, which is impractical in Eager Mode, where
operator sequences can vary during change.
  We propose Chameleon, which redesigns the end-to-end process of swap-based
memory optimization and is the first work to consider varying operator
sequences in Eager Mode. Chameleon (i) introduces a lightweight online profiler
to enable continuous profiling for monitoring operator sequences, (ii)
generates effective swap policies with limited operator information, and (iii)
optimizes the policy execution module for accurate policy application and
better performance. Experimental results demonstrate that Chameleon reduces
profiling overhead by 84.25%, enables training models up to 4x larger than
hardware memory while adapting to changes in operator sequences, improves
performance by up to 38.94% compared to recomputation or high-degree
parallelism.

</details>


### [8] [GFS: A Preemption-aware Scheduling Framework for GPU Clusters with Predictive Spot Instance Management](https://arxiv.org/abs/2509.11134)
*Jiaang Duan,Shenglin Xu,Shiyou Qian,Dingyu Yang,Kangjin Wang,Chenzhi Liao,Yinghao Yu,Qin Hua,Hanwen Hu,Qi Wang,Wenchao Wu,Dongqing Bao,Tianyu Lu,Jian Cao,Guangtao Xue,Guodong Yang,Liping Zhang,Gang Chen*

Main category: cs.DC

TL;DR: GFS是一个新颖的抢占式调度框架，通过轻量级预测模型、动态分配机制和抢占式调度策略，显著降低了低优先级任务的驱逐率和排队延迟，同时提高了GPU分配率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的兴起改变了GPU使用模式，现有调度器面临高驱逐率和长排队时间的问题，需要更高效的管理策略来平衡高优先级和低优先级任务的需求。

Method: 1) 使用轻量级预测模型预测不同租户的GPU需求；2) 采用动态分配机制调整低优先级任务的spot配额并保证持续时间；3) 实施抢占式调度策略，优先处理高优先级任务同时最小化对低优先级任务的影响。

Result: GFS将低优先级任务的驱逐率降低33.0%，排队延迟减少44.1%，GPU分配率提高22.8%，在超过10,000个GPU的生产集群中每月产生约459,715美元的经济效益。

Conclusion: GFS框架有效解决了GPU资源调度中的关键问题，在保证高优先级任务SLO合规的同时，显著提升了低优先级任务的性能和整体资源利用率，具有重要的实际应用价值。

Abstract: The surge in large language models (LLMs) has fundamentally reshaped the
landscape of GPU usage patterns, creating an urgent need for more efficient
management strategies. While cloud providers employ spot instances to reduce
costs for low-priority (LP) tasks, existing schedulers still grapple with high
eviction rates and lengthy queuing times. To address these limitations, we
present GFS, a novel preemptive scheduling framework that enhances
service-level objective (SLO) compliance for high-priority (HP) tasks while
minimizing preemptions to LP tasks. Firstly, GFS utilizes a lightweight
forecasting model that predicts GPU demand among different tenants, enabling
proactive resource management. Secondly, GFS employs a dynamic allocation
mechanism to adjust the spot quota for LP tasks with guaranteed durations.
Lastly, GFS incorporates a preemptive scheduling policy that prioritizes HP
tasks while minimizing the impact on LP tasks. We demonstrate the effectiveness
of GFS through both real-world implementation and simulations. The results show
that GFS reduces eviction rates by 33.0\%, and cuts queuing delays by 44.1\%
for LP tasks. Furthermore, GFS enhances the GPU allocation rate by up to 22.8\%
in real production clusters. In a production cluster of more than 10,000 GPUs,
GFS yields roughly \$459,715 in monthly benefits.

</details>


### [9] [Linear Complexity $\mathcal{H}^2$ Direct Solver for Fine-Grained Parallel Architectures](https://arxiv.org/abs/2509.11152)
*Wajih Boukaram,David Keyes,Sherry Li,Yang Liu,George Turkiyyah*

Main category: cs.DC

TL;DR: 提出了一种新的线性复杂度直接求解器，支持细粒度并行架构上的并发批处理操作，适用于层次化表示的矩阵。


<details>
  <summary>Details</summary>
Motivation: 针对层次化矩阵（特别是强可容许性H²格式）设计高效并行直接求解器，实现线性时间复杂度和内存复杂度，支持大规模稠密矩阵求解。

Method: 基于强递归骨架化分解压缩远程交互，采用多级矩阵图着色和前缀和内存管理避免动态内存分配，实现16线程并行扩展。

Result: 在四个代表性稠密矩阵族上验证了线性复杂度扩展（最大规模100万），包含实验性后向误差分析，识别了内存带宽受限阶段。

Conclusion: 该黑盒求解器仅需输入矩阵和右端项，无需系统来源的解析或几何信息，为低精度计算趋势提供了替代方案讨论。

Abstract: We present factorization and solution phases for a new linear complexity
direct solver designed for concurrent batch operations on fine-grained parallel
architectures, for matrices amenable to hierarchical representation. We focus
on the strong-admissibility-based $\mathcal{H}^2$ format, where strong
recursive skeletonization factorization compresses remote interactions. We
build upon previous implementations of $\mathcal{H}^2$ matrix construction for
efficient factorization and solution algorithm design, which are illustrated
graphically in stepwise detail. The algorithms are ``blackbox'' in the sense
that the only inputs are the matrix and right-hand side, without analytical or
geometrical information about the origin of the system. We demonstrate linear
complexity scaling in both time and memory on four representative families of
dense matrices up to one million in size. Parallel scaling up to 16 threads is
enabled by a multi-level matrix graph coloring and avoidance of dynamic memory
allocations thanks to prefix-sum memory management. An experimental backward
error analysis is included. We break down the timings of different phases,
identify phases that are memory-bandwidth limited, and discuss alternatives for
phases that may be sensitive to the trend to employ lower precisions for
performance.

</details>


### [10] [Adaptive K-PackCache: Cost-Centric Data Caching in Cloud](https://arxiv.org/abs/2509.11156)
*Suvarthi Sarkar,Aadarshraj Sah,Poddutoori Sweeya Reddy,Aryabartta Sahu*

Main category: cs.DC

TL;DR: 该论文提出了一种自适应K打包缓存算法(AKPC)，将传统的2项打包扩展到可变大小的K项打包，通过动态形成、合并和分割数据簇来优化缓存效率，在Netflix和Spotify数据集上分别减少了63%和55%的总成本。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅限于2项成对打包，限制了缓存效率的进一步提升。用户访问模式预测技术的发展使得打包多个相关数据项成为可能，但需要解决过打包增加成本、欠打包错过共享机会的问题。

Method: 提出在线算法AKPC，从CDN运营商角度制定K PackCache问题，最小化包含传输成本和内存租赁成本的总成本。算法基于用户访问模式和内容相关性动态形成、合并和分割数据簇，支持批量请求和近似簇合并。

Result: 在Netflix和Spotify数据集上的广泛评估显示，AKPC相比在线基线分别减少了63%和55%的总成本，性能达到最优解的15%和13%范围内。

Conclusion: AKPC算法具有可扩展性和有效性，适用于现实世界的缓存系统，通过自适应K项打包显著提升了缓存性能。

Abstract: Recent advances in data analytics have enabled the accurate prediction of
user access patterns, giving rise to the idea of packed caching delivering
multiple co accessed data items together as a bundle. This improves caching
efficiency, as accessing one item often implies the need for others. Prior work
has explored only 2 item pairwise packing. In this paper, we extend the concept
to general K packing, allowing variable size bundles for improved flexibility
and performance. We formulate the K PackCache problem from a content delivery
network CDN operator perspective, aiming to minimize total cost comprising two
components: transfer cost modeled as a base cost plus a linearly increasing
term with the number of items packed, and memory rental cost for caching, which
depends on how long and how much is stored. Overpacking increases cost due to
low utility, underpacking leads to missed sharing opportunities. We propose an
online algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,
and splits data cliques based on user access patterns and content correlation.
Our approach supports batch requests, enables approximate clique merging, and
offers a formal competitive guarantee. Through extensive evaluation on the
Netflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55
percentage over online baselines, respectively, and achieves performance within
15 and 13 percentage of the optimal. This demonstrates its scalability and
effectiveness for real world caching systems.

</details>


### [11] [Energy-Efficient Joint Offloading and Resource Allocation for Deadline-Constrained Tasks in Multi-Access Edge Computing](https://arxiv.org/abs/2509.11162)
*Chuanchao Gao,Arvind Easwaran*

Main category: cs.DC

TL;DR: 这篇论文提出了一种基于图匹配的近似算法（GMA），用于解决多访问边缘计算中的任务卸载和资源分配问题，在满足任务截止期和系统资源约束的前提下最大化IoT设备的节能效果。


<details>
  <summary>Details</summary>
Motivation: 解决多访问边缘计算环境中的任务卸载和资源分配问题，这是一个NP难问题，需要高效的近似算法来最大化IoT设备的能量节省。

Method: 提出了基于图匹配的近似算法（GMA），利用线性松弛、三部图构建和线性规划四舍五入技术来进行问题汇汇。

Result: 算法在理论上具有近似比例保证，实验结果显示其节能效果平均达到最优解的97%。

Conclusion: GMA算法能够高效地解决多访问边缘计算中的任务卸载问题，并在实践中展现出良好的性能。

Abstract: This paper addresses the deadline-constrained task offloading and resource
allocation problem in multi-access edge computing. We aim to determine where
each task is offloaded and processed, as well as corresponding communication
and computation resource allocations, to maximize the total saved energy for
IoT devices, while considering task deadline and system resource constraints.
Especially, our system allows each task to be offloaded to one of its
accessible access points (APs) and processed on a server that is not co-located
with its offloading AP. We formulate this problem as an Integer Nonlinear
Programming problem and show it is NP-Hard. To address this problem, we propose
a Graph-Matching-based Approximation Algorithm ($\mathtt{GMA}$), the first
approximation algorithm of its kind. $\mathtt{GMA}$ leverages linear
relaxation, tripartite graph construction, and a Linear Programming rounding
technique. We prove that $\mathtt{GMA}$ is a
$\frac{1-\alpha}{2+\epsilon}$-approximation algorithm, where $\epsilon$ is a
small positive value, and $\alpha$ ($0$$\le$$\alpha$$<$$1$) is a system
parameter that ensures the resource allocated to any task by an AP or a server
cannot exceed $\alpha$ times its resource capacity. Experiments show that, in
practice, $\mathtt{GMA}$'s energy saving achieves $97\%$ of the optimal value
on average.

</details>


### [12] [Parallel/Distributed Tabu Search for Scheduling Microprocessor Tasks in Hybrid Flowshop](https://arxiv.org/abs/2509.11396)
*Adam Janiak,Damian Kowalczyk,Maciej Lichtenstein*

Main category: cs.DC

TL;DR: 基于禁忌搜索的混合流水车间调度算法，解决多处理器任务的最小化完工时间问题


<details>
  <summary>Details</summary>
Motivation: 混合流水车间(HFS)扩展了经典流水车间，每个处理阶段由多个相同并行处理器组成；多处理器任务允许任务同时需要多个处理器处理。需要高效算法解决这类复杂调度问题

Method: 采用禁忌搜索技术，使用并行和分布式机制进行邻域评估，并良好平衡异构网络环境

Result: 提出了一个有效的算法框架来处理混合流水车间中多处理器任务的调度优化

Conclusion: 该禁忌搜索算法能够有效解决具有多处理器任务的混合流水车间调度问题，特别适合异构网络环境下的并行计算

Abstract: The paper deals with the makespan minimization in the hybrid flow shop
scheduling problem with multiprocessor tasks. The hybrid flow shop (HFS)
generalizes the classical flow shop processor configuration by replacing each
processor (processing stage) by some number of identical parallel processors.
Similarly, the multiprocessor tasks generalize the classical assumption, by
allowing a task to require more than one processor simultaneously for its
processing. In this work we present the algorithm for solving the problem based
on the tabu search technique. The proposed algorithm uses parallel and
distributed mechanisms for neighborhood evaluation and well balances
heterogeneous network environment.

</details>


### [13] [Machine Learning-Driven Predictive Resource Management in Complex Science Workflows](https://arxiv.org/abs/2509.11512)
*Tasnuva Chowdhury,Tadashi Maeno,Fatih Furkan Akman,Joseph Boudreau,Sankha Dutta,Shengyu Feng,Adolfy Hoisie,Kuan-Chieh Hsu,Raees Khan,Jaehyung Kim,Ozgur O. Kilic,Scott Klasky,Alexei Klimentov,Tatiana Korchuganova,Verena Ingrid Martinez Outschoorn,Paul Nilsson,David K. Park,Norbert Podhorszki,Yihui Ren,John Rembrandt Steele,Frédéric Suter,Sairam Sri Vatsavai,Torre Wenaus,Wei Yang,Yiming Yang,Shinjae Yoo*

Main category: cs.DC

TL;DR: 本文提出了一种基于机器学习的新题流水线，通过预测科学实验数据处理流程的资源需求，解决传统两阶段方法的问题，提高处理效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 科学实验数据处理流程趋向复杂化，但预先估算资源需求很困难。传统的两阶段处理方法存在初始准确性低、资源使用不优、等待时间过长等问题，特别是对于需要快速回转分析的场景。

Method: 在PanDA系统中集成了一个机器学习模型流水线，采用先进的机器学习技术来预测关键资源需求，克服每个步骤初始特征知识有限的挑战。

Result: 准确的资源需求预测使得流程管理能够进行明智的预阵决策，提高了在异构资源上处理多样化复杂流程的效率。

Conclusion: 该研究通过机器学习方法有效解决了科学实验数据处理中的资源预测挑战，为大规模科学合作项目提供了更高效的流程管理方案。

Abstract: The collaborative efforts of large communities in science experiments, often
comprising thousands of global members, reflect a monumental commitment to
exploration and discovery. Recently, advanced and complex data processing has
gained increasing importance in science experiments. Data processing workflows
typically consist of multiple intricate steps, and the precise specification of
resource requirements is crucial for each step to allocate optimal resources
for effective processing. Estimating resource requirements in advance is
challenging due to a wide range of analysis scenarios, varying skill levels
among community members, and the continuously increasing spectrum of computing
options. One practical approach to mitigate these challenges involves initially
processing a subset of each step to measure precise resource utilization from
actual processing profiles before completing the entire step. While this
two-staged approach enables processing on optimal resources for most of the
workflow, it has drawbacks such as initial inaccuracies leading to potential
failures and suboptimal resource usage, along with overhead from waiting for
initial processing completion, which is critical for fast-turnaround analyses.
In this context, our study introduces a novel pipeline of machine learning
models within a comprehensive workflow management system, the Production and
Distributed Analysis (PanDA) system. These models employ advanced machine
learning techniques to predict key resource requirements, overcoming challenges
posed by limited upfront knowledge of characteristics at each step. Accurate
forecasts of resource requirements enable informed and proactive
decision-making in workflow management, enhancing the efficiency of handling
diverse, complex workflows across heterogeneous resources.

</details>


### [14] [Towards the Distributed Large-scale k-NN Graph Construction by Graph Merge](https://arxiv.org/abs/2509.11697)
*Cheng Zhang,Wan-Lei Zhao,Shihai Xiao,Jiajie Yao,Xuecang Zhang*

Main category: cs.DC

TL;DR: 通过高效的图合并算法（双向合并和多路合并）来解决大规模向量数据的k近邻图和索引图构建问题，支持单机和分布式场景


<details>
  <summary>Details</summary>
Motivation: 为了支持LLM实时交互以及社交媒体上的即时搜索/推荐，需要为大规模向量化多媒体数据构建k-NN图或索引图，而单机处理能力可能不足

Method: 提出了两种高并行化的图合并算法：双向合并和多路合并，以及基于双向合并的多节点构建流程

Result: 在三个节点上约17小时完成十亿级k-NN图构建，合并后的索引图保持了类似的近邻搜索性能但构建时间大大缩短

Conclusion: 通过图合并技术可以高效构建大规模高质量的k-NN图和索引图，解决单机内存限制问题

Abstract: In order to support the real-time interaction with LLMs and the instant
search or the instant recommendation on social media, it becomes an imminent
problem to build k-NN graph or indexing graph for the massive number of
vectorized multimedia data. In such scenarios, the scale of the data or the
scale of the graph may exceed the processing capacity of a single machine. This
paper aims to address the graph construction problem of such scale via
efficient graph merge. For the graph construction on a single node, two generic
and highly parallelizable algorithms, namely Two-way Merge and Multi-way Merge
are proposed to merge subgraphs into one. For the graph construction across
multiple nodes, a multi-node procedure based on Two-way Merge is presented. The
procedure makes it feasible to construct a large-scale k-NN graph/indexing
graph on either a single node or multiple nodes when the data size exceeds the
memory capacity of one node. Extensive experiments are conducted on both
large-scale k-NN graph and indexing graph construction. For the k-NN graph
construction, the large-scale and high-quality k-NN graphs are constructed by
graph merge in parallel. Typically, a billion-scale k-NN graph can be built in
approximately 17h when only three nodes are employed. For the indexing graph
construction, similar NN search performance as the original indexing graph is
achieved with the merged indexing graphs while requiring much less time of
construction.

</details>


### [15] [A Uniqueness Theorem for Distributed Computation under Physical Constraint](https://arxiv.org/abs/2509.11754)
*Zhiyuan Ren,Mingxuan Lu,Wenchi Cheng*

Main category: cs.DC

TL;DR: 这篇论文提出了一种新的分布式计算范式SDPF，通过公理化物理约束证明对于具有平凡合并运算符的计算类，存在唯一的最优解决方案，解决了网络内计算环境中的通信效率、内存限制和可扩展性的三难问题。


<details>
  <summary>Details</summary>
Motivation: 传统计算模型忽略了物理硬件的极端限制，而在网络内计算这种极端环境下，通信效率、有限内存和可扩展性构成了不可调和的三难问题，需要找到从逻辑必要性出发的根本解决方案。

Method: 建立了一个严格的公理化系统来形式化这些物理约束，并证明对于具有平凡合并运算符的计算类，存在唯一的最优范式：自描述并行流(SDPF)，这是一种纯粹数据中心的模型，无状态执行器处理携带自身控制逻辑的数据流。

Result: 证明了SDPF范式是收敛的、图灵完备的和最小的。类似于CAP定理为分布式状态管理设定了不可能性边界，本文则提供了一个构造性的对偶：一个唯一性定理，揭示了在物理法则下分布式计算流的必然性。

Conclusion: 解决网络内计算中的三难问题需要从工程折衰转向逻辑必要性的思维方式变革，SDPF范式作为唯一的最优解决方案，为极端环境下的分布式计算提供了理论基础和实践指南。

Abstract: Foundational models of computation often abstract away physical hardware
limitations. However, in extreme environments like In-Network Computing (INC),
these limitations become inviolable laws, creating an acute trilemma among
communication efficiency, bounded memory, and robust scalability. Prevailing
distributed paradigms, while powerful in their intended domains, were not
designed for this stringent regime and thus face fundamental challenges. This
paper demonstrates that resolving this trilemma requires a shift in perspective
- from seeking engineering trade-offs to deriving solutions from logical
necessity. We establish a rigorous axiomatic system that formalizes these
physical constraints and prove that for the broad class of computations
admitting an idempotent merge operator, there exists a unique, optimal
paradigm. Any system satisfying these axioms must converge to a single normal
form: Self-Describing Parallel Flows (SDPF), a purely data-centric model where
stateless executors process flows that carry their own control logic. We
further prove this unique paradigm is convergent, Turing-complete, and minimal.
In the same way that the CAP theorem established a boundary for what is
impossible in distributed state management, our work provides a constructive
dual: a uniqueness theorem that reveals what is \textit{inevitable} for
distributed computation flows under physical law.

</details>


### [16] [LASLiN: A Learning-Augmented Peer-to-Peer Network](https://arxiv.org/abs/2509.11904)
*Julien Dallot,Caio Caldeira,Arash Pourdamghani,Olga Goussevskaia,Stefan Schmid*

Main category: cs.DC

TL;DR: 基于预测交通模式的学习增强型P2P网络协议LASLiN，在保持标准P2P指标的同时优化需求感知的路由性能，平衡了预测准确时的高性能和预测错误时的稳健性


<details>
  <summary>Details</summary>
Motivation: 传统P2P网络设计忽视了节点间的差异化通信需求，无法利用交通模式预测来优化网络性能。需要一种方法在保持形式保证的前提下，通过学习预测来提升需求感知的网络性能

Method: 首先在集中式环境中通过动态规划求解最优静态跳表网络构造问题。然后提出Uniform P2P协议，将跳表网络中节点高度从离散放松为连续。最终基于Uniform构建学习增强型P2P协议LASLiN，通过各节点的个体化预测来优化网络拓扑

Result: 证明了LASLiN在预测准确时能够达到与最优静态跳表网络相一致的性能，而在预测完全错误时仍能保持对数级别的性能。对于高度稀疏的需求模式，LASLiN能够实现更好的性能表现

Conclusion: LASLiN协议成功实现了学习增强型P2P网络设计，在保持标准P2P指标的前提下，通过利用交通模式预测来显著提升网络性能，并具有良好的一致性和稳健性

Abstract: We introduce a learning-augmented peer-to-peer (P2P) network design that
leverages the predictions of traffic patterns to optimize the network's
topology. While keeping formal guarantees on the standard P2P metrics (routing
path length, maximum degree), we optimize the network in a demand-aware manner
and minimize the path lengths weighted by the peer-to-peer communication
demands. Our protocol is learning-augmented, meaning that each node receives an
individual, possibly inaccurate prediction about the future traffic patterns,
with the goal of improving the network's performances. We strike a trade-off
between significantly improved performances when the predictions are correct
(consistency) and polylogarithmic performances when the predictions are
arbitrary (robustness).
  We have two main contributions. First, we consider the centralized setting
and show that the problem of constructing an optimum static skip list network
(SLN) is solvable in polynomial time and can be computed via dynamic
programming. This problem is the natural demand-aware extension of the optimal
skip list problem.
  Second, we introduce the Uniform P2P protocol which generalizes skip list
networks (SLN) by relaxing the node's heights from discrete to continuous. We
show that Uniform achieves state-of-the-art performances: logarithmic routing
and maximum degree, both with high probability. We then use Uniform to build a
learning-augmented P2P protocol in order to incorporate demand-awareness,
leading to our main contribution, LASLiN. We prove that the performances of
LASLiN are consistent with those of an optimum static SLN with correct
predictions (given via our dynamic programming approach), and are at most a
logarithmic factor off the state-of-the-art P2P protocols if the predictions
are arbitrary wrong. For the special case of highly sparse demands, we show
that LASLiN achieves improved performances.

</details>


### [17] [UniPar: A Unified LLM-Based Framework for Parallel and Accelerated Code Translation in HPC](https://arxiv.org/abs/2509.12136)
*Tomer Bitan,Tal Kadosh,Erel Kaplan,Shira Meiri,Le Chen,Peter Morales,Niranjan Hasabnis,Gal Oren*

Main category: cs.DC

TL;DR: UniPar是一个系统评估LLM并行代码翻译能力的框架，针对串行代码、CUDA和OpenMP之间的翻译，通过微调、超参数优化和编译器引导修复等方法，将GPT-4o-mini的性能从46%编译率和15%功能正确率提升到69%和33%。


<details>
  <summary>Details</summary>
Motivation: 现有并行编程语言翻译工具范围有限且过时，需要系统评估大型语言模型在并行代码翻译方面的能力。

Method: 开发UniPar评估框架，包含四种主要使用模式：解码超参数优化、零样本和少样本提示、监督微调、以及基于编译器的迭代修复。构建了PARATRANS数据集。

Result: 现成模型在默认设置下表现不佳（GPT-4o-mini仅46%编译率和15%功能正确率），但UniPar方法将性能提升至多2倍（69%编译率和33%正确率）。

Conclusion: UniPar方法显著提升了LLM在并行语言翻译方面的性能，为研究者进一步改进LLM提供了有用见解。

Abstract: Translating programs between various parallel programming languages is an
important problem in the high-performance computing (HPC) community. Existing
tools for this problem are either too narrow in scope and/or outdated. Recent
explosive growth in the popularity of large language models (LLMs) and their
ability to generate and translate code offers a potential alternative approach.
Toward that end, we first need to systematically evaluate the ability of LLMs
to translate between parallel languages.
  In this work, we introduce UniPar, a systematic evaluation framework for
LLM-based parallel code translation. Specifically, in this work, we target
translations between serial code, CUDA, and OpenMP. Our goal is to assess how
well current instruction-tuned LLMs -- specifically GPT-4o-mini and
LLaMA-3.3-70B-Instruct -- can be used out of the box or enhanced through known
strategies. We evaluated four major usage modes: hyperparameter optimization
for decoding, zero- and few-shot prompting, supervised fine-tuning, and
iterative feedback through compiler-based repair. As a part of the evaluation,
we construct a new dataset called PARATRANS, covering both serial-to-parallel
translation and cross-paradigm transformations.
  Our findings reveal that while off-the-shelf models struggle under the
default settings (e.g., GPT-4o-mini achieves only 46% compilation and 15%
functional correctness), our UniPar methodology -- combining fine-tuning,
hyperparameter tuning, and compiler-guided repair -- improves performance by up
to 2X (69% compilation and 33% correctness). We believe that our findings will
provide useful insights for researchers to further improve LLMs for the
parallel language translation problem.
  UniPar source code and PARATRANS dataset are available at our GitHub
repository https://github.com/Scientific-Computing-Lab/UniPar_AI.

</details>


### [18] [Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization](https://arxiv.org/abs/2509.12138)
*Mengjiao Han,Andres Sewell,Joseph Insley,Janet Knowles,Victor A. Mateevitsi,Michael E. Papka,Steve Petruzza,Silvio Rizzi*

Main category: cs.DC

TL;DR: 分布式3D高斯泼溅技术，用于高性能计算系统的大规模科学数据可视化，通过多节点多GPU并行训练实现3倍加速


<details>
  <summary>Details</summary>
Motivation: 现有3D-GS技术仅限于单GPU设置，无法满足高性能计算系统上大规模数据集的可扩展性需求

Method: 提出分布式3D-GS流水线：跨节点数据分区、多节点多GPU并行训练高斯泼溅、添加边界幽灵单元和背景掩码消除伪影、合并泼溅进行全局渲染

Result: 在Polaris系统8个节点上对Richtmyer-Meshkov数据集（约1.067亿高斯）进行基准测试，实现高达3倍加速，同时保持图像质量

Conclusion: 分布式3D-GS能够实现大规模科学数据的可扩展可视化，为未来原位应用奠定了基础

Abstract: 3D Gaussian Splatting (3D-GS) has recently emerged as a powerful technique
for real-time, photorealistic rendering by optimizing anisotropic Gaussian
primitives from view-dependent images. While 3D-GS has been extended to
scientific visualization, prior work remains limited to single-GPU settings,
restricting scalability for large datasets on high-performance computing (HPC)
systems. We present a distributed 3D-GS pipeline tailored for HPC. Our approach
partitions data across nodes, trains Gaussian splats in parallel using
multi-nodes and multi-GPUs, and merges splats for global rendering. To
eliminate artifacts, we add ghost cells at partition boundaries and apply
background masks to remove irrelevant pixels. Benchmarks on the
Richtmyer-Meshkov datasets (about 106.7M Gaussians) show up to 3X speedup
across 8 nodes on Polaris while preserving image quality. These results
demonstrate that distributed 3D-GS enables scalable visualization of
large-scale scientific data and provide a foundation for future in situ
applications.

</details>


### [19] [When MoE Meets Blockchain: A Trustworthy Distributed Framework of Large Models](https://arxiv.org/abs/2509.12141)
*Weihao Zhu,Long Shi,Kang Wei,Zhen Mei,Zhe Wang,Jiaheng Wang,Jun Li*

Main category: cs.DC

TL;DR: 基于区块链的可信混合专家模型（B-MoE）框架，解决了分布式MoE在数据交互中的信任问题和数据操纵攻击风险


<details>
  <summary>Details</summary>
Motivation: 传统云端MoE遇到响应延迟、带宽消耗高、数据隐私泄漏问题，而分布式边缘MoE缺乏信任机制，容易受到数据操纵攻击

Method: 设计三层B-MoE框架：边缘层下载并运行专家，区块链层作为去中心化信任网络追踪、验证和记录计算结果，存储屢供应专家

Result: 实验结果显示B-MoE在训练和推理过程中比传统分布式MoE更加耐受数据操纵攻击

Conclusion: 区块链技术可以有效提升分布式MoE框架的安全性和可信度，为大模型在边缘环境中的安全部署提供了新的解决方案

Abstract: As an enabling architecture of Large Models (LMs), Mixture of Experts (MoE)
has become prevalent thanks to its sparsely-gated mechanism, which lowers
computational overhead while maintaining learning performance comparable to
dense LMs. The essence of MoE lies in utilizing a group of neural networks
(called experts) with each specializing in different types of tasks, along with
a trainable gating network that selectively activates a subset of these experts
to handle specific tasks. Traditional cloud-based MoE encounters challenges
such as prolonged response latency, high bandwidth consumption, and data
privacy leakage. To address these issues, researchers have proposed to deploy
MoE over distributed edge networks. However, a key concern of distributed MoE
frameworks is the lack of trust in data interactions among distributed experts
without the surveillance of any trusted authority, and thereby prone to
potential attacks such as data manipulation. In response to the security issues
of traditional distributed MoE, we propose a blockchain-aided trustworthy MoE
(B-MoE) framework that consists of three layers: the edge layer, the blockchain
layer, and the storage layer. In this framework, the edge layer employs the
activated experts downloaded from the storage layer to process the learning
tasks, while the blockchain layer functions as a decentralized trustworthy
network to trace, verify, and record the computational results of the experts
from the edge layer. The experimental results demonstrate that B-MoE is more
robust to data manipulation attacks than traditional distributed MoE during
both the training and inference processes.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [20] [ReCross: Efficient Embedding Reduction Scheme for In-Memory Computing using ReRAM-Based Crossbar](https://arxiv.org/abs/2509.10627)
*Yu-Hong Lai,Chieh-Lin Tsai,Wen Sheng Lim,Han-Wen Hu,Tei-Wei Kuo,Yuan-Hao Chang*

Main category: cs.AR

TL;DR: ReCross是一种基于ReRAM的内存计算方案，通过优化嵌入访问模式和交叉阵列特性，显著提升深度推荐模型的嵌入减少操作性能


<details>
  <summary>Details</summary>
Motivation: 深度推荐模型中的大型稀疏嵌入层存在内存带宽瓶颈，导致推理时间增加和能耗上升，需要更高效的解决方案

Method: 通过智能分组和映射共现嵌入、在交叉阵列间复制频繁访问的嵌入、使用动态开关ADC电路动态选择内存处理操作

Result: 相比最先进的内存计算方法，执行时间减少3.97倍，能效提升6.1倍

Conclusion: ReCross通过协同优化嵌入访问模式和ReRAM交叉阵列特性，有效解决了深度推荐模型嵌入减少操作中的性能瓶颈问题

Abstract: Deep learning-based recommendation models (DLRMs) are widely deployed in
commercial applications to enhance user experience. However, the large and
sparse embedding layers in these models impose substantial memory bandwidth
bottlenecks due to high memory access costs and irregular access patterns,
leading to increased inference time and energy consumption. While resistive
random access memory (ReRAM) based crossbars offer a fast and energy-efficient
solution through in-memory embedding reduction operations, naively mapping
embeddings onto crossbar arrays leads to poor crossbar utilization and thus
degrades performance. We present ReCross, an efficient ReRAM-based in-memory
computing (IMC) scheme designed to minimize execution time and enhance energy
efficiency in DLRM embedding reduction. ReCross co-optimizes embedding access
patterns and ReRAM crossbar characteristics by intelligently grouping and
mapping co-occurring embeddings, replicating frequently accessed embeddings
across crossbars, and dynamically selecting in-memory processing operations
using a newly designed dynamic switch ADC circuit that considers runtime energy
trade-offs. Experimental results demonstrate that ReCross achieves a 3.97x
reduction in execution time and a 6.1x improvement in energy efficiency
compared to state-of-the-art IMC approaches.

</details>


### [21] [DOSA: Differentiable Model-Based One-Loop Search for DNN Accelerators](https://arxiv.org/abs/2509.10702)
*Charles Hong,Qijing Huang,Grace Dinh,Mahesh Subedar,Yakun Sophia Shao*

Main category: cs.AR

TL;DR: DOSA是一个用于硬件设计空间探索的梯度下降优化框架，通过可微分性能模型同时优化硬件参数和算法到硬件的映射，显著优于随机搜索和贝叶斯优化方法。


<details>
  <summary>Details</summary>
Motivation: 硬件设计空间探索需要同时优化硬件参数和算法到硬件的映射，但这两个空间都是高度非凸的，传统方法分别探索导致组合爆炸问题，给优化器带来巨大困难。

Method: 提出DOSA框架，包含可微分性能模型和基于梯度下降的优化技术，能够同时探索硬件设计空间和映射空间，还支持通过增强学习模型来优化缓冲区大小和映射。

Result: 实验结果显示，DOSA在改善DNN模型能量延迟积方面比随机搜索和贝叶斯优化分别提升2.80倍和12.59倍。通过增强学习模型优化真实DNN加速器，实现了1.82倍的能量延迟积改进。

Conclusion: DOSA提供了一种有效的同时优化硬件设计和映射的方法，通过可微分建模和梯度下降技术解决了组合爆炸问题，展现了模块化和灵活性，在硬件设计空间探索中具有显著优势。

Abstract: In the hardware design space exploration process, it is critical to optimize
both hardware parameters and algorithm-to-hardware mappings. Previous work has
largely approached this simultaneous optimization problem by separately
exploring the hardware design space and the mapspace - both individually large
and highly nonconvex spaces - independently. The resulting combinatorial
explosion has created significant difficulties for optimizers.
  In this paper, we introduce DOSA, which consists of differentiable
performance models and a gradient descent-based optimization technique to
simultaneously explore both spaces and identify high-performing design points.
Experimental results demonstrate that DOSA outperforms random search and
Bayesian optimization by 2.80x and 12.59x, respectively, in improving DNN model
energy-delay product, given a similar number of samples. We also demonstrate
the modularity and flexibility of DOSA by augmenting our analytical model with
a learned model, allowing us to optimize buffer sizes and mappings of a real
DNN accelerator and attain a 1.82x improvement in energy-delay product.

</details>


### [22] [Design and Analysis of Approximate Hardware Accelerators for VVC Intra Angular Prediction](https://arxiv.org/abs/2509.10751)
*Lucas M. Leipnitz de Fraga,Cláudio Machado Diniz*

Main category: cs.AR

TL;DR: 提出一种通过系数平均近似方法来减少VVC帧内预测插值滤波器系数数量的技术，以降低硬件实现复杂度和功耗，同时保持编码效率。


<details>
  <summary>Details</summary>
Motivation: VVC标准相比HEVC显著提高了压缩效率，但带来了更高的计算复杂度，特别是在帧内预测阶段。插值滤波器有50多个不同系数，使得无乘法器常数乘法(MCM)块实现资源密集。

Method: 通过平均固定子集的插值系数来减少系数数量，从而减小MCM块规模。提出了六种不同的MCM块架构，其中五种使用该近似方法，并与传统乘法器架构进行对比评估。

Result: 实验结果显示，只有两种MCM实现超过4%的BD-Rate增加，最坏情况下平均增加2.6%。两种MCM实现分别减少了20%和44%的电路面积。三种架构的并行样本预测模块相比单样本处理单元减少了30%的门面积，两种实现降低了能耗。

Conclusion: 所提出的系数近似方法能够有效降低VVC帧内预测的硬件实现复杂度，在保持可接受的编码效率损失的同时，显著减少电路面积和功耗。

Abstract: The Versatile Video Coding (VVC) standard significantly improves compression
efficiency over its predecessor, HEVC, but at the cost of substantially higher
computational complexity, particularly in intra-frame prediction. This stage
employs various directional modes, each requiring multiple multiplications
between reference samples and constant coefficients. To optimize these
operations at hardware accelerators, multiplierless constant multiplication
(MCM) blocks offer a promising solution. However, VVC's interpolation filters
have more than fifty distinct coefficients, making MCM implementations
resource-intensive. This work proposes an approximation method to reduce the
number of interpolation coefficients by averaging fixed subsets of them,
therefore decreasing MCM block size and potentially lowering circuit area and
power consumption. Six different MCM block architectures for angular intra
prediction are introduced, in which five use the approximation method
introduced in this work, and evaluate the trade-off between coefficient
reduction and coding efficiency compared with a conventional multiplier
architecture. Experimental results in ten videos demonstrate that only two MCM
implementations exceed a 4% BD-Rate increase and 2.6% on average in the worst
case, while two of the MCM implementations have circuit area reduction of 20%
and 44%. For three of the architectures, parallel sample prediction modules
were synthesized, showing a reduction of 30% gate area compared to single
sample processing units, and a reduction in energy consumption for two of the
implementations.

</details>


### [23] [always_comm: An FPGA-based Hardware Accelerator for Audio/Video Compression and Transmission](https://arxiv.org/abs/2509.11503)
*Rishab Parthasarathy,Akshay Attaluri,Gilford Ting*

Main category: cs.AR

TL;DR: 基于FPGA硬件实现的可扩展视频会议系统，采用M-JPEG编码和UDP网络栈，支持30FPS实时视频流传输


<details>
  <summary>Details</summary>
Motivation: 实现全硬件化的视频会议解决方案，提高性能和实时性，支持实时视频流传输

Method: 在Nexys4 DDR FPGA上实现M-JPEG视频压缩编码和UDP网络栈，通过Python脚本解码收取的以太网数据包进行实时播放

Result: 系统通过Cocotb模拟验证和Vivado合成部署，能够在FPGA上实现稳定的30FPS视频流传输

Conclusion: 这种全硬件化设计方案有效地实现了高性能实时视频会议功能，为嵌入式视频通信系统提供了可扩展的硬件解决方案

Abstract: We present a design for an extensible video conferencing stack implemented
entirely in hardware on a Nexys4 DDR FPGA, which uses the M-JPEG codec to
compress video and a UDP networking stack to communicate between the FPGA and
the receiving computer. This networking stack accepts real-time updates from
both the video codec and the audio controller, which means that video will be
able to be streamed at 30 FPS from the FPGA to a computer. On the computer
side, a Python script reads the Ethernet packets and decodes the packets into
the video and the audio for real time playback. We evaluate this architecture
using both functional, simulation-driven verification in Cocotb and by
synthesizing SystemVerilog RTL code using Vivado for deployment on our Nexys4
DDR FPGA, where we evaluate both end-to-end latency and throughput of video
transmission.

</details>


### [24] [SuperUROP: An FPGA-Based Spatial Accelerator for Sparse Matrix Operations](https://arxiv.org/abs/2509.11529)
*Rishab Parthasarathy*

Main category: cs.AR

TL;DR: 本文提出了Azul稀疏线性系统求解器的FPGA实现，通过SRAM-only硬件加速器解决现有迭代求解器在内存访问和并行性方面的效率问题


<details>
  <summary>Details</summary>
Motivation: 稀疏线性系统求解在数值方法中至关重要，但现有软件级迭代求解器在当前硬件上效率低下，主要原因是内存访问不规则和数据依赖复杂限制了并行性

Method: 在FPGA上实现Azul加速器，采用RISC-V CPU核心连接不同FPGA内存模块的层次结构，通过自定义RISC-V ISA扩展实现基于任务的编程模型，支持NoC通信

Result: 设计了分布式测试用例进行功能验证，确认FPGA实现与Azul框架架构仿真具有等效性能

Conclusion: FPGA实现的Azul加速器能够有效解决稀疏线性系统求解中的内存带宽利用和算术强度问题，为大规模约束问题提供高效解决方案

Abstract: Solving sparse systems of linear equations is a fundamental problem in the
field of numerical methods, with applications spanning from circuit design to
urban planning. These problems can have millions of constraints, such as when
laying out transistors on a circuit, or trying to optimize traffic light
timings, making fast sparse solvers extremely important. However, existing
state-of-the-art software-level solutions for solving sparse linear systems,
termed iterative solvers, are extremely inefficient on current hardware. This
inefficiency can be attributed to two key reasons: (1) poor short-term data
reuse, which causes frequent, irregular memory accesses, and (2) complex data
dependencies, which limit parallelism. Hence, in this paper, we present an FPGA
implementation of the existing Azul accelerator, an SRAM-only hardware
accelerator that achieves both high memory bandwidth utilization and arithmetic
intensity. Azul features a grid of tiles, each of which is composed of a
processing element (PE) and a small independent SRAM memory, which are all
connected over a network on chip (NoC). We implement Azul on FPGA using simple
RISC-V CPU cores connected to a memory hierarchy of different FPGA memory
modules. We utilize custom RISC-V ISA augmentations to implement a task-based
programming model for the various PEs, allowing communication over the NoC.
Finally, we design simple distributed test cases so that we can functionally
verify the FPGA implementation, verifying equivalent performance to an
architectural simulation of the Azul framework.

</details>


### [25] [LEGO: Spatial Accelerator Generation and Optimization for Tensor Applications](https://arxiv.org/abs/2509.12053)
*Yujun Lin,Zhekai Zhang,Song Han*

Main category: cs.AR

TL;DR: LEGO框架自动生成面向张量应用的空间架构设计和可综合RTL代码，无需手写RTL模板，相比Gemmini实现3.2倍加速和2.4倍能效提升


<details>
  <summary>Details</summary>
Motivation: 现代张量应用（特别是基础模型和生成式AI）需要多模态输入，现有框架在设计灵活性和RTL生成效率之间存在权衡，要么局限于少数手写模板，要么无法自动生成RTL

Method: 基于仿射变换的架构表示，前端分析功能单元互连、合成存储系统、基于数据重用分析融合不同空间数据流设计；后端通过图表示进行低级优化，使用线性规划算法优化流水线寄存器插入和减少未使用逻辑开销

Result: 相比Gemmini实现3.2倍速度提升和2.4倍能效提升，能够为生成式AI应用中的多样化现代基础模型生成统一架构

Conclusion: LEGO框架成功解决了张量加速器设计中的灵活性与生产力矛盾，为多模态AI应用提供了高效的自动化硬件生成解决方案

Abstract: Modern tensor applications, especially foundation models and generative AI
applications require multiple input modalities (both vision and language),
which increases the demand for flexible accelerator architecture. Existing
frameworks suffer from the trade-off between design flexibility and
productivity of RTL generation: either limited to very few hand-written
templates or cannot automatically generate the RTL. To address this challenge,
we propose the LEGO framework, which targets tensor applications and
automatically generates spatial architecture design and outputs synthesizable
RTL code without handwritten RTL design templates. Leveraging the
affine-transformation-based architecture representation, LEGO front end finds
interconnections between function units, synthesizes the memory system, and
fuses different spatial dataflow designs based on data reuse analysis. LEGO
back end then translates the hardware in a primitive-level graph to perform
lower-level optimizations, and applies a set of linear-programming algorithms
to optimally insert pipeline registers and reduce the overhead of unused logic
when switching spatial dataflows. Our evaluation demonstrates that LEGO can
achieve 3.2x speedup and 2.4x energy efficiency compared to previous work
Gemmini, and can generate one architecture for diverse modern foundation models
in generative AI applications.

</details>
