<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Doc2Spec: Synthesizing Formal Programming Specifications from Natural Language via Grammar Induction](https://arxiv.org/abs/2602.04892)
*Shihao Xia,Mengting He,Haomin Jia,Linhai Song*

Main category: cs.PL

TL;DR: Doc2Spec：一个多智能体框架，使用LLM从自然语言规则自动推导规范语法，然后生成形式化规范，在三个编程语言的七个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 确保API实现和使用符合自然语言编程规则对软件正确性、安全性和可靠性至关重要。形式化验证能提供强保证但需要精确规范，而手动编写规范既困难又昂贵。

Method: 提出Doc2Spec多智能体框架，使用LLM从自然语言规则自动推导规范语法，然后基于推导出的语法生成形式化规范。该语法捕获关键领域知识，约束规范空间，并确保表示一致性。

Result: 在三个编程语言的七个基准测试中，Doc2Spec优于无语法推导的基线方法，并与使用手动构建语法的技术取得竞争性结果，证明了自动语法推导在形式化自然语言规则方面的有效性。

Conclusion: Doc2Spec通过自动推导规范语法，显著提高了生成形式化规范的可靠性和质量，为解决形式化验证中规范编写困难的问题提供了有效方案。

Abstract: Ensuring that API implementations and usage comply with natural language programming rules is critical for software correctness, security, and reliability. Formal verification can provide strong guarantees but requires precise specifications, which are difficult and costly to write manually. To address this challenge, we present Doc2Spec, a multi-agent framework that uses LLMs to automatically induce a specification grammar from natural-language rules and then generates formal specifications guided by the induced grammar. The grammar captures essential domain knowledge, constrains the specification space, and enforces consistent representations, thereby improving the reliability and quality of generated specifications. Evaluated on seven benchmarks across three programming languages, Doc2Spec outperforms a baseline without grammar induction and achieves competitive results against a technique with a manually crafted grammar, demonstrating the effectiveness of automated grammar induction for formalizing natural-language rules.

</details>


### [2] [Strong Normalisation for Asynchronous Effects](https://arxiv.org/abs/2602.05528)
*Danel Ahman,Ilja Sobolev*

Main category: cs.PL

TL;DR: 该论文研究了Ahman和Pretnar提出的异步效应演算的规范化性质，证明了在移除一般递归后，该演算（包括顺序和并行部分）是强规范化的，并进一步证明了在引入受控的中断驱动递归行为后，顺序片段仍保持强规范化。


<details>
  <summary>Details</summary>
Motivation: Ahman和Pretnar的异步效应演算为代数计算效应提供了异步处理能力，能够自然地建模抢占式多线程、可取消远程函数调用、多方应用等场景。然而，该演算的规范化性质尚未得到充分研究，特别是其终止性保证需要深入分析。

Method: 采用Lindley和Stark基于⊤⊤-提升的方法的扩展版本，结构化地构建强规范化证明。首先证明移除一般递归后的完整演算（包括顺序和并行部分）是强规范化的，然后证明在引入受控中断驱动递归行为后，顺序片段仍保持强规范化。所有结果均在Agda中形式化验证。

Result: 1. 移除一般递归后，完整的异步效应演算（包括顺序和并行部分）是强规范化的；2. 在顺序片段中重新引入受控的中断驱动递归行为后，该片段仍保持强规范化；3. 所有证明均在Agda中形式化验证。

Conclusion: 该研究为异步效应演算提供了坚实的规范化理论基础，证明了在适当限制下（移除一般递归或控制递归行为），该演算具有良好的终止性。这为异步效应编程语言的设计和实现提供了理论保证，扩展了Lindley和Stark的规范化证明方法到异步场景。

Abstract: Asynchronous effects of Ahman and Pretnar complement the conventional synchronous treatment of algebraic computational effects with asynchrony based on decoupling the execution of algebraic operation calls into signalling that an operation's implementation needs to be executed, and into interrupting a running computation with the operation's result, to which the computation can react by installing matching interrupt handlers. Beyond providing asynchrony for algebraic effects, the resulting core calculus also naturally models examples such as pre-emptive multi-threading, (cancellable) remote function calls, multi-party applications, and even a parallel variant of runners of algebraic effects. In this paper, we study the normalisation properties of this calculus. We prove that if one removes general recursion from the original calculus, then the remaining calculus is strongly normalising, including both its sequential and parallel parts. However, this only guarantees termination for very simple asynchronous examples. To improve on this result, we also prove that the sequential fragment of the calculus remains strongly normalising when a controlled amount of interrupt-driven recursive behaviour is reintroduced. Our strong normalisation proofs are structured compositionally as a natural extension of Lindley and Stark's $\top\top$-lifting based approach for proving strong normalisation of effectful languages. All our results are also formalised in Agda.

</details>


### [3] [An Equational Axiomatization of Dynamic Threads via Algebraic Effects: Presheaves on Finite Relations, Labelled Posets, and Parameterized Algebraic Theories](https://arxiv.org/abs/2602.05850)
*Ohad Kammar,Jack Liell-Cock,Sam Lindley,Cristina Matache,Sam Staton*

Main category: cs.PL

TL;DR: 使用代数效应理论为动态线程提供完整的等式公理化，基于参数化代数理论，通过fork和wait原语构建并发程序，证明模型完备性和语法完备性。


<details>
  <summary>Details</summary>
Motivation: 为动态线程提供完整的等式公理化框架，使用代数效应理论来形式化并发编程中的线程创建和同步操作。

Method: 基于参数化代数理论，构建包含fork（创建子线程）和wait（等待线程完成）原语的代数理论，结合基本原子操作和结合律等公理。

Result: 证明了两个完备性：1) 模型完备性：闭表达式完全捕获带标签偏序集（pomsets）的相等性；2) 语法完备性：所有闭替换下相等的开表达式都可证明相等。

Conclusion: 代数效应方法使语义分析能专注于fork和wait的代数操作，该方法可扩展到简单并发编程语言，提供操作和指称语义，证明其健全、充分且一阶完全抽象。

Abstract: We use the theory of algebraic effects to give a complete equational axiomatization for dynamic threads. Our method is based on parameterized algebraic theories, which give a concrete syntax for strong monads on functor categories, and are a convenient framework for names and binding. Our programs are built from the key primitives `fork' and `wait'. `Fork' creates a child thread and passes its name (thread ID) to the parent thread. `Wait' allows us to wait for given child threads to finish. We provide a parameterized algebraic theory built from fork and wait, together with basic atomic actions and laws such as associativity of `fork'. Our equational axiomatization is complete in two senses. First, for closed expressions, it completely captures equality of labelled posets (pomsets), an established model of concurrency: model complete. Second, any two open expressions are provably equal if they are equal under all closing substitutions: syntactically complete. The benefit of algebraic effects is that the semantic analysis can focus on the algebraic operations of fork and wait. We then extend the analysis to a simple concurrent programming language by giving operational and denotational semantics. The denotational semantics is built using the methods of parameterized algebraic theories and we show that it is sound, adequate, and fully abstract at first order for labelled-poset observations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [A novel scalable high performance diffusion solver for multiscale cell simulations](https://arxiv.org/abs/2602.05017)
*Jose-Luis Estragues-Muñoz,Carlos Alvarez,Arnau Montagud,Daniel Jimenez-Gonzalez,Alfonso Valencia*

Main category: cs.DC

TL;DR: 提出一个可扩展的HPC解决方案BioFVM，用于分子扩散建模，相比现有方法实现近200倍加速和36%内存使用减少


<details>
  <summary>Details</summary>
Motivation: 将细胞分辨率模型扩展到真实规模的肿瘤模拟是数字孪生疾病模型开发的关键挑战，需要处理万亿级操作的高性能计算

Method: 采用高效实现的最先进有限体积法框架，开发了可扩展的生物有限体积法库BioFVM，并进行系统性能评估

Result: HPC方案相比现有最先进解决方案实现近200倍加速，内存使用减少高达36%

Conclusion: 提出的可扩展HPC解决方案为高效计算下一代生物问题铺平了道路

Abstract: Agent-based cellular models simulate tissue evolution by capturing the behavior of individual cells, their interactions with neighboring cells, and their responses to the surrounding microenvironment. An important challenge in the field is scaling cellular resolution models to real-scale tumor simulations, which is critical for the development of digital twin models of diseases and requires the use of High-Performance Computing (HPC) since every time step involves trillions of operations. We hereby present a scalable HPC solution for the molecular diffusion modeling using an efficient implementation of state-of-the-art Finite Volume Method (FVM) frameworks. The paper systematically evaluates a novel scalable Biological Finite Volume Method (BioFVM) library and presents an extensive performance analysis of the available solutions. Results shows that our HPC proposal reach almost 200x speedup and up to 36% reduction in memory usage over the current state-of-the-art solutions, paving the way to efficiently compute the next generation of biological problems.

</details>


### [5] [Towards Advancing Research with Workflows: A perspective from the Workflows Community Summit -- Amsterdam, 2025](https://arxiv.org/abs/2602.05131)
*Irene Bonati,Silvina Caino-Lores,Tainã Coleman,Sagar Dolas,Sandro Fiore,Venkatesh Kannan,Marco Verdicchio,Sean R. Wilkinson,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 2025年工作流社区峰会识别了科学工作流采用的关键障碍，并提出了技术、政策和社区层面的解决方案，强调从计算性能转向科学影响评估，建立标准化模式，培养国际社区，以及投资人力资本。


<details>
  <summary>Details</summary>
Motivation: 科学工作流在现代研究中对于协调分布式计算、管理大数据和确保可重复性至关重要，但面临采用障碍。2025年工作流社区峰会旨在召集国际专家探讨该领域的挑战和机遇，推动科学发现。

Method: 通过2025年6月6日在阿姆斯特丹举行的国际专家峰会，识别科学工作流采用的关键障碍，包括系统通用性与领域特定性之间的张力、可持续性问题、开发者认可不足以及标准化、资金、培训和跨学科合作方面的差距。

Result: 峰会提出了跨越技术、政策和社区维度的行动路线：从原始计算性能转向衡量真实科学影响的评估指标；形式化工作流模式和社区驱动的基准测试以提高透明度、可重复性和可用性；培养一个凝聚的国际工作流社区；投资人力资本，包括专门的工作流工程角色、职业路径和教育整合。

Conclusion: 通过解决技术、政策和社区层面的系统性挑战，科学工作流可以更好地支持现代研究，促进科学发现。峰会提出的行动路线为构建更可持续、可重复和高效的科学计算生态系统提供了框架。

Abstract: Scientific workflows have become essential for orchestrating complex computational processes across distributed resources, managing large datasets, and ensuring reproducibility in modern research. The Workflows Community Summit 2025, held in Amsterdam on June 6th, 2025, convened international experts to examine emerging challenges and opportunities in this domain. Participants identified key barriers to workflow adoption, including tensions between system generality and domain-specific utility, concerns over long-term sustainability of workflow systems and services, insufficient recognition for those who develop and maintain reproducible workflows, and gaps in standardization, funding, training, and cross-disciplinary collaboration. To address these challenges, the summit proposed action lines spanning technology, policy, and community dimensions: shifting evaluation metrics from raw computational performance toward measuring genuine scientific impact; formalizing workflow patterns and community-driven benchmarks to improve transparency, reproducibility, and usability; cultivating a cohesive international workflows community that engages funding bodies and research stakeholders; and investing in human capital through dedicated workflow engineering roles, career pathways, and integration of workflow concepts into educational curricula and long-term training initiatives. This document presents the summit's findings, beginning with an overview of the current computing ecosystem and the rationale for workflow-centric approaches, followed by a discussion of identified challenges and recommended action lines for advancing scientific discovery through workflows.

</details>


### [6] [ORACL: Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices](https://arxiv.org/abs/2602.05292)
*Haoyu Bai,Muhammed Tawfiqul Islam,Minxian Xu,Rajkumar Buyya*

Main category: cs.DC

TL;DR: ORACL：利用大语言模型进行微服务自动扩缩的通用少样本资源分配框架，通过思维链推理诊断性能问题并推荐资源分配，无需特定部署训练。


<details>
  <summary>Details</summary>
Motivation: 现有微服务自动扩缩策略存在局限性：基于学习的模型需要大量特定部署训练，而手动调整的规则脆弱且难以泛化。需要一种能够适应快速演变的微服务部署的通用资源分配方法。

Method: 提出ORACL框架，将运行时遥测数据（pod、副本、CPU/内存使用率、延迟、SLO、故障信号）转换为语义自然语言状态描述，利用LLM进行思维链推理，识别根本原因、剪枝动作空间，并在策略约束下做出安全的分配决策。

Result: 在代表性开源微服务工作负载上的实验表明：ORACL将根本原因识别准确率提高15%，训练加速高达24倍，短期场景下服务质量提升6%，且无需特定部署的重新训练。

Conclusion: 大语言模型可以作为通用的少样本资源分配器，适应快速演变的微服务部署。ORACL通过思维链推理提供可解释的决策，在保持安全约束的同时提高性能和效率。

Abstract: Applications are moving away from monolithic designs to microservice and serverless architectures, where fleets of lightweight and independently deployable components run on public clouds. Autoscaling serves as the primary control mechanism for balancing resource utilization and quality of service, yet existing policies are either opaque learned models that require substantial per-deployment training or brittle hand-tuned rules that fail to generalize. We investigate whether large language models can act as universal few-shot resource allocators that adapt across rapidly evolving microservice deployments.
  We propose ORACL, Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices, a framework that leverages prior knowledge and chain-of-thought reasoning to diagnose performance regressions and recommend resource allocations. ORACL transforms runtime telemetry, including pods, replicas, CPU and memory usage, latency, service-level objectives, and fault signals, into semantic natural-language state descriptions and invokes an LLM to produce an interpretable intermediate reasoning trace. This reasoning identifies likely root causes, prunes the action space, and issues safe allocation decisions under policy constraints. Experiments on representative open-source microservice workloads show that ORACL improves root-cause identification accuracy by 15 percent, accelerates training by up to 24x, and improves quality of service by 6 percent in short-term scenarios, without deployment-specific retraining.

</details>


### [7] [Proteus: Append-Only Ledgers for (Mostly) Trusted Execution Environments](https://arxiv.org/abs/2602.05346)
*Shubham Mishra,João Gonçalves,Chawinphat Tankuranand,Neil Giridharan,Natacha Crooks,Heidi Howard,Chris Jensen*

Main category: cs.DC

TL;DR: Proteus是一个分布式共识协议，在CFT协议中嵌入BFT协议，谨慎信任TEE硬件，在TEE可能被攻击的情况下仍能保证数据完整性。


<details>
  <summary>Details</summary>
Motivation: 分布式账本依赖硬件TEE提供可信保障，但TEE可能受到攻击，这会破坏分布式账本精心设计的保证。需要一种在TEE可能被攻破时仍能保证完整性的解决方案。

Method: 通过精心重构CFT和BFT协议使它们结构对齐，将BFT协议嵌入到CFT协议中，无需额外消息。谨慎信任TEE的保证，在TEE平台可能被攻破时仍能保证完整性。

Result: Proteus在保持与常规TEE增强共识协议相当性能的同时，能够在TEE平台被攻破时保证数据完整性。

Conclusion: Proteus提供了一种平衡方法，既利用TEE的性能优势，又通过BFT协议提供额外的安全保证，在TEE可能被攻击的情况下仍能确保分布式账本的完整性。

Abstract: Distributed ledgers are increasingly relied upon by industry to provide trustworthy accountability, strong integrity protection, and high availability for critical data without centralizing trust. Recently, distributed append-only logs are opting for a layered approach, combining crash-fault-tolerant (CFT) consensus with hardware-based Trusted Execution Environments (TEEs) for greater resiliency. Unfortunately, hardware TEEs can be subject to (rare) attacks, undermining the very guarantees that distributed ledgers are carefully designed to achieve. In response, we present Proteus, a new distributed consensus protocol that cautiously trusts the guarantees of TEEs. Proteus carefully embeds a Byzantine fault-tolerant (BFT) protocol inside of a CFT protocol with no additional messages. This is made possible through careful refactoring of both the CFT and BFT protocols such that their structure aligns. Proteus achieves performance in line with regular TEE-enabled consensus protocols, while guaranteeing integrity in the face of TEE platform compromises.

</details>


### [8] [Reaching Univalency with Subquadratic Communication](https://arxiv.org/abs/2602.05356)
*Andrew Lewis-Pye*

Main category: cs.DC

TL;DR: 论文证明Dolev-Reischuk下界中的二次通信成本主要来自结果传播阶段，而非达成一致性决策阶段。通过引入ε-BA松弛协议，可以在O(n log n)通信复杂度内达成"单值性"，然后通过一轮全交换完成完整BA。


<details>
  <summary>Details</summary>
Motivation: Dolev-Reischuk下界表明确定性拜占庭容错协议需要Ω(f²+n)消息，但这一二次成本的具体来源不明确。论文旨在探究该下界是源于达成一致性决策的困难，还是仅仅源于将结果传播给所有处理器的过程。

Method: 引入ε-BA松弛协议，允许ε比例的正确处理器输出错误结果。证明当f < n(1/3 - ε)时，ε-BA可以通过确定性协议以O(n log n)通信复杂度解决。任何ε-BA协议都可以作为完整BA协议的第一阶段，第二阶段只需一轮全交换和多数投票即可完成BA。

Result: 1. 达成单值性（协议结果已确定）不需要二次通信，可以在O(n log n)内完成。2. Dolev-Reischuk下界的二次成本完全来自结果传播阶段。3. 在认证设置中定义可提取BA，证明其通信复杂度为O(f log f)。

Conclusion: Dolev-Reischuk下界中的二次通信成本主要源于将已确定的结果传播给所有处理器的过程，而非达成一致性决策本身。这一发现深化了对拜占庭共识协议通信复杂性的理解，并为设计更高效的协议提供了新思路。

Abstract: The Dolev-Reischuk lower bound establishes that any deterministic Byzantine Agreement (BA) protocol for $n$ processors tolerating $f$ faults requires $Ω(f^2+n)$ messages. But what exactly does this quadratic cost pay for? Even the minimal requirement that every correct processor \emph{receive at least one message} already necessitates $Ω(f^2 + n)$ messages. This raises a fundamental question: is the Dolev-Reischuk bound about the difficulty of \emph{reaching univalency} -- the point at which the protocol's outcome is determined -- or merely about \emph{disseminating} the outcome to all processors afterward?
  We resolve this question by showing that reaching univalency does \emph{not} require quadratic communication. Specifically, we introduce $ε$-BA, a relaxation allowing an $ε$-fraction of correct processors to output incorrectly, and prove it can be solved deterministically with $O(n \log n)$ communication complexity when $f < n(1/3 - ε)$. Crucially, any $ε$-BA protocol can serve as the first phase of a full BA protocol: after $ε$-BA, a single all-to-all exchange and majority vote completes BA. Since the outcome is already determined after $ε$-BA, this demonstrates that the quadratic cost in Dolev-Reischuk stems entirely from dissemination, rather than from reaching univalency. We also define Extractable BA for authenticated settings, capturing when processors collectively hold enough signed messages to determine the agreed value, and show it can be solved with communication complexity $O(f \log f)$.

</details>


### [9] [TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism](https://arxiv.org/abs/2602.05754)
*Seonghye Cho,Jaemin Han,Hyunjin Kim,Euisoo Jung,Jae-Gil Lee*

Main category: cs.DC

TL;DR: TimelyFreeze通过建模流水线调度为有向无环图并求解线性规划，在精度约束下计算最优冻结比例，减少流水线气泡，提升训练吞吐量达40%


<details>
  <summary>Details</summary>
Motivation: 流水线并行虽然能训练超出单设备内存的模型，但实际吞吐量受限于流水线气泡。现有参数冻结方法虽然能通过自适应跳过反向计算来提高训练吞吐量，但往往过度冻结参数，导致不必要的精度下降。

Method: 将流水线调度建模为有向无环图，通过求解线性规划问题来计算最优冻结比例，在精度约束下最小化批次执行时间。

Result: 在LLaMA-8B上实现了高达40%的训练吞吐量提升，同时保持可比的精度。能够在不影响收敛的情况下加速大规模模型训练，并适用于多种流水线并行设置。

Conclusion: TimelyFreeze通过优化参数冻结策略，有效解决了流水线并行中的气泡问题，实现了训练吞吐量的显著提升，同时避免了不必要的精度损失，具有广泛的适用性。

Abstract: Pipeline parallelism enables training models that exceed single-device memory, but practical throughput remains limited by pipeline bubbles. Although parameter freezing can improve training throughput by adaptively skipping backward computation, existing methods often over-freeze parameters, resulting in unnecessary accuracy degradation. To address this issue, we propose TimelyFreeze, which models the pipeline schedule as a directed acyclic graph and solves a linear program to compute optimal freeze ratios that minimize batch execution time under accuracy constraints. Experiments show that TimelyFreeze achieves up to 40% training throughput improvement on LLaMA-8B with comparable accuracy. Overall, it enables faster large-scale model training without compromising convergence and generalizes across diverse pipeline-parallel settings.

</details>


### [10] [Location-Aware Dispersion on Anonymous Graphs](https://arxiv.org/abs/2602.05948)
*Himani,Supantha Pandit,Gokarna Sharma*

Main category: cs.DC

TL;DR: 本文提出了位置感知分散问题，这是经典分散问题的新扩展，要求机器人根据颜色匹配移动到对应颜色的节点上，并设计了确定性算法，给出了时间与内存的界限，同时证明了不可能性和下界。


<details>
  <summary>Details</summary>
Motivation: 经典分散问题假设机器人可以占据任意空闲节点，但在实际应用中，机器人可能需要移动到特定类型的节点（如充电站、特定资源点等）。位置感知分散问题通过引入颜色标签，使机器人必须移动到与其颜色匹配的节点，这更贴近现实世界的应用场景。

Method: 提出了位置感知分散问题的形式化定义，开发了多个确定性算法，这些算法在匿名、连通、无向图上运行，每个节点和机器人都有颜色标签。算法旨在最小化时间和内存需求，同时给出了算法的时间复杂度和内存使用界限。

Result: 证明了位置感知分散问题的算法可行性，设计了具有保证时间与内存界限的确定性算法。同时给出了任何确定性算法的不可能性和下界结果，表明该问题相比经典分散问题更具挑战性。

Conclusion: 位置感知分散问题是经典分散问题的有意义的扩展，虽然在匿名网络中算法可行，但获得高效解决方案比经典分散问题更具挑战性，需要更多时间和内存资源。

Abstract: The well-studied DISPERSION problem is a fundamental coordination problem in distributed robotics, where a set of mobile robots must relocate so that each occupies a distinct node of a network. DISPERSION assumes that a robot can settle at any node as long as no other robot settles on that node. In this work, we introduce LOCATION-AWARE DISPERSION, a novel generalization of DISPERSION that incorporates location awareness: Let $G = (V, E)$ be an anonymous, connected, undirected graph with $n = |V|$ nodes, each labeled with a color $\sf{col}(v) \in C = \{c_1, \dots, c_t\}, t\leq n$. A set $R = \{r_1, \dots, r_k\}$ of $k \leq n$ mobile robots is given, where each robot $r_i$ has an associated color $\mathsf{col}(r_i) \in C$. Initially placed arbitrarily on the graph, the goal is to relocate the robots so that each occupies a distinct node of the same color. When $|C|=1$, LOCATION-AWARE DISPERSION reduces to DISPERSION. There is a solution to DISPERSION in graphs with any $k\leq n$ without knowing $k,n$.
  Like DISPERSION, the goal is to solve LOCATION-AWARE DISPERSION minimizing both time and memory requirement at each agent. We develop several deterministic algorithms with guaranteed bounds on both time and memory requirement. We also give an impossibility and a lower bound for any deterministic algorithm for LOCATION-AWARE DISPERSION. To the best of our knowledge, the presented results collectively establish the algorithmic feasibility of LOCATION-AWARE DISPERSION in anonymous networks and also highlight the challenges on getting an efficient solution compared to the solutions for DISPERSION.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [CVA6-CFI: A First Glance at RISC-V Control-Flow Integrity Extensions](https://arxiv.org/abs/2602.04991)
*Simone Manoni,Emanuele Parisi,Riccardo Tedeschi,Davide Rossi,Andrea Acquaviva,Andrea Bartolini*

Main category: cs.AR

TL;DR: 本文首次设计、集成并评估了RISC-V控制流完整性(CFI)标准扩展Zicfiss和Zicfilp，通过硬件实现影子栈和着陆垫原语来保护程序免受控制流劫持攻击。


<details>
  <summary>Details</summary>
Motivation: 保护易受攻击的程序免受控制流劫持攻击，为RISC-V架构提供标准化的控制流完整性硬件扩展。

Method: 设计了两个独立可配置的硬件单元（分别用于前向边和后向边控制流保护），完全集成到开源CVA6核心中，实现了基于影子栈和着陆垫原语的安全机制。

Result: 在22nm FDX技术中合成时仅产生1.0%的面积开销，使用MiBench汽车基准子集评估时性能开销最高为15.6%，并将完整实现开源发布。

Conclusion: 成功实现了首个RISC-V CFI标准扩展的硬件设计，以较小的硬件开销提供了有效的控制流保护，为RISC-V生态系统的安全增强做出了贡献。

Abstract: This work presents the first design, integration, and evaluation of the standard RISC-V extensions for Control-Flow Integrity (CFI). The Zicfiss and Zicfilp extensions aim at protecting the execution of a vulnerable program from control-flow hijacking attacks through the implementation of security mechanisms based on shadow stack and landing pad primitives. We introduce two independent and configurable hardware units implementing forward-edge and backward-edge control-flow protection, fully integrated into the open-source CVA6 core. Our design incurs in only 1.0% area overhead when synthesized in 22 nm FDX technology, and up to 15.6% performance overhead based on evaluation with the MiBench automotive benchmark subset. We release the complete implementation as open source.

</details>


### [12] [COFFEE: A Carbon-Modeling and Optimization Framework for HZO-based FeFET eNVMs](https://arxiv.org/abs/2602.05018)
*Hongbang Wu,Xuesi Chen,Shubham Jadhav,Amit Lal,Lillian Pentecost,Udit Gupta*

Main category: cs.AR

TL;DR: COFFEE是首个针对HZO基FeFET eNVM的碳建模框架，涵盖硬件制造（体现碳）到使用阶段（运营碳）的全生命周期分析，发现相比SRAM，HZO-FeFET eNVM在单位MB体现碳降低4.3倍，在边缘ML加速器中可减少体现碳42.3%和运营碳70%。


<details>
  <summary>Details</summary>
Motivation: 信息通信技术对全球环境影响日益显著，新兴非易失性存储器（eNVM）虽能提供能效计算方案，但其端到端碳足迹尚未被充分理解。理解硬件系统全生命周期环境影响是实现可持续计算的第一步。

Method: 提出COFFEE框架，基于真实半导体工厂数据和器件制造工艺估算体现碳，利用架构级eNVM设计空间探索工具量化使用阶段性能和能耗，以HZO基FeFET eNVM为例进行详细研究。

Result: 2MB容量下，HZO-FeFETs的单位面积体现碳比CMOS基线高11%，但单位MB体现碳始终比SRAM低4.3倍。边缘ML加速器案例中，用HZO-FeFET eNVM替换SRAM权重缓冲器可减少体现碳42.3%，运营碳最多减少70%。

Conclusion: COFFEE框架为eNVM碳足迹评估提供了首个系统方法，证明HZO-FeFET eNVM在可持续计算中具有显著优势，特别是能大幅降低边缘AI应用的碳足迹。

Abstract: Information and communication technologies account for a growing portion of global environmental impacts. While emerging technologies, such as emerging non-volatile memories (eNVM), offer a promising solution to energy efficient computing, their end-to-end footprint is not well understood. Understanding the environmental impact of hardware systems over their life cycle is the first step to realizing sustainable computing. This work conducts a detailed study of one example eNVM device: hafnium-zirconium-oxide (HZO)-based ferroelectric field-effect transistors (FeFETs). We present COFFEE, the first carbon modeling framework for HZO-based FeFET eNVMs across life cycle, from hardware manufacturing (embodied carbon) to use (operational carbon). COFFEE builds on data gathered from a real semiconductor fab and device fabrication recipes to estimate embodied carbon, and architecture level eNVM design space exploration tools to quantify use-phase performance and energy. Our evaluation shows that, at 2 MB capacity, the embodied carbon per unit area overhead of HZO-FeFETs can be up to 11% higher than the CMOS baseline, while the embodied carbon per MB remains consistently about 4.3x lower than SRAM across different memory capacity. A further case study applies COFFEE to an edge ML accelerator, showing that replacing the SRAM-based weight buffer with HZO-based FeFET eNVMs reduces embodied carbon by 42.3% and operational carbon by up to 70%.

</details>


### [13] [Balancing FP8 Computation Accuracy and Efficiency on Digital CIM via Shift-Aware On-the-fly Aligned-Mantissa Bitwidth Prediction](https://arxiv.org/abs/2602.05743)
*Liang Zhao,Kunming Shao,Zhipeng Liao,Xijie Huang,Tim Kwang-Ting Cheng,Chi-Ying Tsui,Yi Zou*

Main category: cs.AR

TL;DR: 提出一种灵活的FP8数字计算内存加速器，通过动态位宽预测和FIFO对齐单元，支持可变FP8对齐尾数位宽，在28nm工艺下实现20.4 TFLOPS/W能效，比现有工作高2.8倍。


<details>
  <summary>Details</summary>
Motivation: 现有数字计算内存架构难以支持可变FP8对齐尾数位宽，统一的校准策略和固定精度MAC单元无法处理分布多样的输入数据，限制了FP8格式在Transformer推理和训练中的应用。

Method: 提出三个创新：1) 动态移位感知位宽预测，实时预测输入并自适应调整权重和输入的对齐尾数精度；2) FIFO输入对齐单元，用基于指针的控制替代复杂桶形移位器；3) 精度可扩展的INT MAC阵列，以最小开销实现灵活权重精度。

Result: 在28nm CMOS工艺中实现64×96 CIM阵列，达到20.4 TFLOPS/W能效，比之前工作高2.8倍，支持所有FP8格式。在Llama-7b上的实验显示，DSBP在相同精度水平下比固定位宽模式效率更高。

Conclusion: 该工作提出了一种灵活的FP8 DCIM加速器架构，通过动态位宽预测和高效对齐机制，成功解决了可变FP8格式支持问题，在保持精度的同时显著提升了能效，为Transformer应用提供了更好的硬件支持。

Abstract: FP8 low-precision formats have gained significant adoption in Transformer inference and training. However, existing digital compute-in-memory (DCIM) architectures face challenges in supporting variable FP8 aligned-mantissa bitwidths, as unified alignment strategies and fixed-precision multiply-accumulate (MAC) units struggle to handle input data with diverse distributions. This work presents a flexible FP8 DCIM accelerator with three innovations: (1) a dynamic shift-aware bitwidth prediction (DSBP) with on-the-fly input prediction that adaptively adjusts weight (2/4/6/8b) and input (2$\sim$12b) aligned-mantissa precision; (2) a FIFO-based input alignment unit (FIAU) replacing complex barrel shifters with pointer-based control; and (3) a precision-scalable INT MAC array achieving flexible weight precision with minimal overhead. Implemented in 28nm CMOS with a 64$\times$96 CIM array, the design achieves 20.4 TFLOPS/W for fixed E5M7, demonstrating 2.8$\times$ higher FP8 efficiency than previous work while supporting all FP8 formats. Results on Llama-7b show that the DSBP achieves higher efficiency than fixed bitwidth mode at the same accuracy level on both BoolQ and Winogrande datasets, with configurable parameters enabling flexible accuracy-efficiency trade-offs.

</details>
