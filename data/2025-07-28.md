<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Decompiling Rust: An Empirical Study of Compiler Optimizations and Reverse Engineering Challenges](https://arxiv.org/abs/2507.18792)
*Zixu Zhou*

Main category: cs.PL

TL;DR: 论文评估了Rust二进制文件反编译的挑战，发现泛型、特质方法和错误处理结构显著降低反编译质量，尤其是在发布构建中。


<details>
  <summary>Details</summary>
Motivation: Rust的丰富类型系统、编译器优化和高层抽象使得反编译其二进制文件具有挑战性，需要系统评估其影响。

Method: 通过基准测试驱动的评估，分析了核心Rust功能和编译器构建模式对反编译质量的影响，并采用自动化评分框架。

Result: 泛型类型、特质方法和错误处理结构显著降低了反编译质量，尤其是在发布构建中。

Conclusion: 研究结果为工具开发者提供了实用见解，并强调了需要针对Rust的反编译策略。

Abstract: Decompiling Rust binaries is challenging due to the language's rich type
system, aggressive compiler optimizations, and widespread use of high-level
abstractions. In this work, we conduct a benchmark-driven evaluation of
decompilation quality across core Rust features and compiler build modes. Our
automated scoring framework shows that generic types, trait methods, and error
handling constructs significantly reduce decompilation quality, especially in
release builds. Through representative case studies, we analyze how specific
language constructs affect control flow, variable naming, and type information
recovery. Our findings provide actionable insights for tool developers and
highlight the need for Rust-aware decompilation strategies.

</details>


### [2] [IsaMini: Redesigned Isabelle Proof Lanugage for Machine Learning](https://arxiv.org/abs/2507.18885)
*Qiyuan Xu,Renxi Wang,Haonan Li,David Sanan,Conrad Watt*

Main category: cs.PL

TL;DR: 论文提出了一种改进的证明语言MiniLang，用于提升神经定理证明（NTP）在Isabelle/HOL中的表现，实验显示其显著提高了LLMs在PISA基准上的成功率。


<details>
  <summary>Details</summary>
Motivation: 通过改进证明语言，利用LLMs的能力，降低证明工程的劳动力和计算成本，推动形式验证和软件工程的发展。

Method: 设计了MiniLang，一种改进的证明语言，并结合了增强版的Sledgehammer工具，用于Isabelle/HOL。

Result: MiniLang使两种微调LLMs在PISA基准上的成功率提升至69.1%（pass@1），超过之前Baldur的65.7%（pass@64）；pass@8达到79.2%，超过当前最佳Magnushammer的71.0%。

Conclusion: MiniLang通过优化证明语言显著提升了NTP的性能，展示了LLMs在自动化定理证明中的潜力。

Abstract: Neural Theorem Proving (NTP) employs deep learning methods, particularly
Large Language Models (LLMs), to automate formal proofs in proof assistants.
This approach holds promise for reducing the dramatic labor costs or
computation costs required in proof engineering, which is fundamental to formal
verification and other software engineering methods. The paper explores the
potential of improving NTP by redesigning the proof language, given that LLMs'
capabilities depend highly on representations. We introduce \emph{MiniLang}, a
redesigned proof language for Isabelle/HOL incorporating an improved version of
Sledgehammer. Experiments show MiniLang benefits two fine-tuned LLMs by
improving the success rate on the PISA benchmark by up to 29\% in comparison to
generation of Isar proof script. The success rate under one attempt (so-called
\emph{pass@1}) reaches 69.1\%, exceeding the previous Baldur's pass@64
(65.7\%); The pass@8 reaches 79.2\%, exceeding the state-of-the-art on PISA
(71.0\%) achieved by Magnushammer.

</details>


### [3] [An Enumerative Embedding of the Python Type System in ACL2s](https://arxiv.org/abs/2507.19015)
*Samuel Xifaras,Panagiotis Manolios,Andrew T. Walter,William Robertson*

Main category: cs.PL

TL;DR: 该论文提出了一种在ACL2s中嵌入Python类型系统子集的方法，用于生成输入以模糊测试Python程序，并识别未被现有类型检查器发现的错误。


<details>
  <summary>Details</summary>
Motivation: Python作为行业标准语言，其代码的静态分析和错误检测仍存在不足，尤其是类型检查器无法覆盖的某些错误。

Method: 在ACL2s中嵌入Python类型系统子集，定义类型和生成输入，用于模糊测试Python程序。

Result: 在四个开源仓库中测试，代码覆盖率为68%至80%，并识别了影响覆盖率的代码模式。

Conclusion: 该方法有效识别了Python代码中的错误，并提出了未来改进的建议。

Abstract: Python is a high-level interpreted language that has become an industry
standard in a wide variety of applications. In this paper, we take a first step
towards using ACL2s to reason about Python code by developing an embedding of a
subset of the Python type system in ACL2s. The subset of Python types we
support includes many of the most commonly used type annotations as well as
user-defined types comprised of supported types. We provide ACL2s definitions
of these types, as well as defdata enumerators that are customized to provide
code coverage and identify errors in Python programs. Using the ACL2s
embedding, we can generate instances of types that can then be used as inputs
to fuzz Python programs, which allows us to identify bugs in Python code that
are not detected by state-of-the-art Python type checkers. We evaluate our work
against four open-source repositories, extracting their type information and
generating inputs for fuzzing functions with type signatures that are in the
supported subset of Python types. Note that we only use the type signatures of
functions to generate inputs and treat the bodies of functions as black boxes.
We measure code coverage, which ranges from about 68% to more than 80%, and
identify code patterns that hinder coverage such as complex branch conditions
and external file system dependencies. We conclude with a discussion of the
results and recommendations for future work.

</details>


### [4] [A Programming Language for Feasible Solutions](https://arxiv.org/abs/2507.19176)
*Weijun Chen,Yuxi Fu,Huan Long*

Main category: cs.PL

TL;DR: 论文提出了一种新型命令式编程语言，其静态类型系统确保所有程序在多项式时间内运行，且所有多项式时间可解问题均可由该语言解决。


<details>
  <summary>Details</summary>
Motivation: 为解决程序验证中的运行时效率和终止性问题，避免临时性处理，设计一个能保证这些性质的稳健框架。

Method: 设计一种基于静态类型系统的命令式编程语言，确保程序运行时间多项式化且问题可解性等价。

Result: 理论证明了等价性定理，实践上实现了语言解释器，验证了方法的可行性。

Conclusion: 该语言为程序分析和验证提供了高效且可靠的方法，兼具理论和实践意义。

Abstract: Runtime efficiency and termination are crucial properties in the studies of
program verification. Instead of dealing with these issues in an ad hoc manner,
it would be useful to develop a robust framework in which such properties are
guaranteed by design. This paper introduces a new imperative programming
language whose design is grounded in a static type system that ensures the
following equivalence property: All definable programs are guaranteed to run in
polynomial time; Conversely, all problems solvable in polynomial time can be
solved by some programs of the language. The contribution of this work is
twofold. On the theoretical side, the foundational equivalence property is
established, and the proof of the equivalence theorem is non-trivial. On the
practical side, a programming approach is proposed that can streamline program
analysis and verification for feasible computations. An interpreter for the
language has been implemented, demonstrating the feasibility of the approach in
practice.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [CUTHERMO: Understanding GPU Memory Inefficiencies with Heat Map Profiling](https://arxiv.org/abs/2507.18729)
*Yanbo Zhao,Jinku Cui,Zecheng Li,Shuyin Jiao,Xu Liu,Jiajia Li*

Main category: cs.DC

TL;DR: cuThermo是一个轻量级的GPU内存分析工具，通过热图识别内存低效问题，并提供优化指导，实验显示性能提升显著。


<details>
  <summary>Details</summary>
Motivation: GPU内存子系统的效率对高性能计算至关重要，但缺乏全面的运行时和细粒度内存分析工具。

Method: cuThermo通过分析GPU二进制文件，生成基于访问模式的热图，识别内存低效问题。

Result: 在六个应用中识别出五种内存访问模式，优化后性能提升高达721.79%。

Conclusion: cuThermo是一种无需修改硬件或代码的实用工具，能有效提升GPU内存性能。

Abstract: GPUs have become indispensable in high-performance computing, machine
learning, and many other domains. Efficiently utilizing the memory subsystem on
GPUs is critical for maximizing computing power through massive parallelism.
Analyzing memory access patterns has proven to be an effective method for
understanding memory bottlenecks in applications. However, comprehensive
runtime and fine-grained memory profiling support is lacking on GPU
architectures. In this work, we introduce cuThermo, a lightweight and practical
profiling tool for GPU memory analysis. It operates on GPU binaries without
requiring any modifications to hardware, operating system, or application
source code. Given a CUDA application, cuThermo identifies memory
inefficiencies at runtime via a heat map based on distinct visited warp counts
to represent word-sector-level data sharing and provides optimization guidance
in performance tuning iterations. Through our experiments on six applications,
we identified five memory access patterns that are portable across different
GPU architectures. By evaluating optimization on two GPUs, cuThermo achieves up
to $721.79\%$ performance improvement.

</details>


### [6] [PPipe: Efficient Video Analytics Serving on Heterogeneous GPU Clusters via Pool-Based Pipeline Parallelism](https://arxiv.org/abs/2507.18748)
*Z. Jonny Kong,Qiang Xu,Y. Charlie Hu*

Main category: cs.DC

TL;DR: 论文提出了一种利用管道并行技术（PPipe）在异构GPU集群上高效服务延迟敏感型模型推理的方法，显著提升了低端GPU的利用率和服务吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着GPU的快速创新，异构GPU集群在公共云和本地数据中心中越来越普遍。研究旨在探索如何利用管道并行技术优化延迟敏感型模型推理服务。

Method: 提出PPipe系统，结合基于MILP的控制平面和基于资源预留的自适应批处理数据平面，利用低端和高端GPU的协同效应实现管道并行。

Result: 在18个CNN模型上的评估显示，PPipe显著提升了低端GPU的利用率（41.1%-65.5%），同时保持高端GPU的高利用率，服务吞吐量提高了32.2%-75.1%。

Conclusion: PPipe通过管道并行技术有效利用了异构GPU集群的资源，为延迟敏感型推理服务提供了高效解决方案。

Abstract: With the rapid innovation of GPUs, heterogeneous GPU clusters in both public
clouds and on-premise data centers have become increasingly commonplace. In
this paper, we demonstrate how pipeline parallelism, a technique wellstudied
for throughput-oriented deep learning model training, can be used effectively
for serving latency-bound model inference, e.g., in video analytics systems, on
heterogeneous GPU clusters. Our work exploits the synergy between diversity in
model layers and diversity in GPU architectures, which results in comparable
inference latency for many layers when running on low-class and high-class
GPUs. We explore how such overlooked capability of low-class GPUs can be
exploited using pipeline parallelism and present a novel inference serving
system, PPipe, that employs pool-based pipeline parallelism via an MILP-based
control plane and a data plane that performs resource reservation-based
adaptive batching. Evaluation results on diverse workloads (18 CNN models) show
that PPipe achieves 41.1% - 65.5% higher utilization of low-class GPUs while
maintaining high utilization of high-class GPUs, leading to 32.2% - 75.1%
higher serving throughput compared to various baselines.

</details>


### [7] [Deadline-Aware Joint Task Scheduling and Offloading in Mobile Edge Computing Systems](https://arxiv.org/abs/2507.18864)
*Ngoc Hung Nguyen,Van-Dinh Nguyen,Anh Tuan Nguyen,Nguyen Van Thieu,Hoang Nam Nguyen,Symeon Chatzinotas*

Main category: cs.DC

TL;DR: 本文提出了一种优化任务调度算法，旨在满足移动边缘计算和云系统中的低延迟需求，并通过快速检测机制应对随机到达任务的不确定性。


<details>
  <summary>Details</summary>
Motivation: 随着移动边缘计算和云系统对交互服务质量要求的提高，需要优化任务调度以满足严格的延迟或截止期限要求。

Method: 提出了最优任务调度算法，确定任务的最优顺序，并开发了在线方法以快速检测和处理随机到达的任务。

Result: 算法具有线性和线性对数时间复杂度（O(n)和O(nlogn)），并通过数值结果验证了其在服务比例和调度成本上的有效性。

Conclusion: 该算法在满足低延迟需求的同时，显著提高了调度效率和用户决策能力。

Abstract: The demand for stringent interactive quality-of-service has intensified in
both mobile edge computing (MEC) and cloud systems, driven by the imperative to
improve user experiences. As a result, the processing of computation-intensive
tasks in these systems necessitates adherence to specific deadlines or
achieving extremely low latency. To optimize task scheduling performance,
existing research has mainly focused on reducing the number of late jobs whose
deadlines are not met. However, the primary challenge with these methods lies
in the total search time and scheduling efficiency. In this paper, we present
the optimal job scheduling algorithm designed to determine the optimal task
order for a given set of tasks. In addition, users are enabled to make informed
decisions for offloading tasks based on the information provided by servers.
The details of performance analysis are provided to show its optimality and low
complexity with the linearithmic time O(nlogn), where $n$ is the number of
tasks. To tackle the uncertainty of the randomly arriving tasks, we further
develop an online approach with fast outage detection that achieves rapid
acceptance times with time complexity of O(n). Extensive numerical results are
provided to demonstrate the effectiveness of the proposed algorithm in terms of
the service ratio and scheduling cost.

</details>


### [8] [GPUnion: Autonomous GPU Sharing on Campus](https://arxiv.org/abs/2507.18928)
*Yufang Li,Yuanbo Zhang,Hanlong Liao,Guoming Tang,Deke Guo*

Main category: cs.DC

TL;DR: GPUnion是一个校园级GPU共享平台，通过自愿参与和保留提供者自主权的方式，提高GPU资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决校园内GPU资源分配不均的问题，同时避免传统集中式管理对学术资源自主性的冲突。

Method: 采用容器化任务调度、资源提供者优先架构和弹性执行机制（如自动检查点和迁移）。

Result: 在多个校园场景中，GPU利用率提升30%，交互会话增加40%，工作负载迁移成功率达94%。

Conclusion: GPUnion证明提供者自主权与平台可靠性可共存，挑战传统集中式模式，促进校园计算资源的民主化访问。

Abstract: A pronounced imbalance in GPU resources exists on campus, where some
laboratories own underutilized servers while others lack the compute needed for
AI research. GPU sharing can alleviate this disparity, while existing platforms
typically rely on centralized oversight and persistent allocation models,
conflicting with the voluntary and autonomous nature of academic resource
ownership. We present GPUnion, a campus-scale GPU sharing platform enabling
voluntary participation while preserving full provider autonomy. GPUnion
incorporates three core mechanisms: i) container-based task dispatching and
execution, ii) resource provider-first architecture, and iii) resilient
execution featuring automatic checkpointing and migration. GPUnion also
supports custom data storage and integrates the non-root execution and image
attestation for isolation and security improvement for containerization. Case
studies across multiple campus scenarios demonstrate 30% more GPU utilization
improvement, 40% increase in interactive sessions, and 94% successful workload
migration during provider departures. GPUnion demonstrates that provider
autonomy and platform reliability can coexist, challenging conventional
centralized paradigms and democratizing access to computational resources
within campus networks.

</details>


### [9] [The Case for Time-Shared Computing Resources](https://arxiv.org/abs/2507.19287)
*Pierre Jacquet,Adrien Luxey-Bitri*

Main category: cs.DC

TL;DR: 论文探讨了ICT行业的环境影响，提出通过改进租户间资源共享来减少物理资源使用，从而实现能源效率和集群规模缩减。


<details>
  <summary>Details</summary>
Motivation: ICT行业的环境影响日益显著，资源有限性要求更严格的资源管理，而当前租户间资源共享不足。

Method: 提出从硬件层面的传统时间共享转向更高抽象层次的资源共享，以更少的物理资源实现服务。

Result: 通过资源共享和性能权衡，可以减少集群规模并提高能源效率。

Conclusion: 改进资源共享是减少ICT环境影响的可行方向，需进一步研究挑战和机遇。

Abstract: The environmental impact of Information and Communication Technologies (ICT)
continues to grow, driven notably by increasing usage, rebound effects, and
emerging demands. However, despite the virtual nature of its services, the
sector remains inherently constrained by its materiality and cannot rely on an
infinite pool of resources. As a result, the wide variety of supported services
may need to be managed under stricter limits within hosting facilities in the
future. Contrary to common assumptions, we show that tenants typically do not
share computing resources, even in environments commonly perceived as
mutualized, such as cloud platforms. Time-sharing has been progressively phased
out for reasons of performance, security, predictability, and, perhaps more
importantly, due to the decreasing cost of computing resources. This paper
advocates for managing fewer physical resources by improving resource sharing
between tenants. It represents a paradigm shift, moving beyond traditional
time-sharing at the hardware level to a higher abstraction. This approach
entails "doing with fewer resources" under conditions of "reduced performance".
Nonetheless, enhancing the mutualization of infrastructure can reduce cluster
sizes (through consolidation) and improve energy efficiency, with gains related
to the accepted performance trade-off, a situation potentially more socially
acceptable than eliminating services. We review the current state of the art,
identify challenges and opportunities, propose interpretations of Time-Shared
Computing, and outline key research directions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [RailX: A Flexible, Scalable, and Low-Cost Network Architecture for Hyper-Scale LLM Training Systems](https://arxiv.org/abs/2507.18889)
*Yinxiao Feng,Tiancheng Chen,Yuchen Wei,Siyuan Shen,Shiju Wang,Wei Li,Kaisheng Ma,Torsten Hoefler*

Main category: cs.AR

TL;DR: RailX是一种基于节点内直接连接和节点间电路交换的可重构网络架构，解决了传统互联网络的可扩展性和成本问题。


<details>
  <summary>Details</summary>
Motivation: 传统互联网络架构（如Rail-optimized和Torus）在超大规模AI工作负载下既不可扩展也不经济高效。

Method: RailX通过基于Hamiltonian分解理论的互联方法，将独立的环形拓扑组织成全互联拓扑，同时优化环形集体通信和全互联通信。

Result: RailX能以低成本（10% Fat-Tree）实现超大规模芯片互联，直径仅2~4跳，支持1.8TB带宽。

Conclusion: RailX是一种高效、低成本且灵活的互联架构，适用于超大规模AI工作负载和MLaaS场景。

Abstract: Increasingly large AI workloads are calling for hyper-scale infrastructure;
however, traditional interconnection network architecture is neither scalable
nor cost-effective enough. Tree-based topologies such as the
\textit{Rail-optimized} network are extremely expensive, while direct
topologies such as \textit{Torus} have insufficient bisection bandwidth and
flexibility. In this paper, we propose \textit{RailX}, a reconfigurable network
architecture based on intra-node direct connectivity and inter-node circuit
switching. Nodes and optical switches are physically 2D-organized, achieving
better scalability than existing centralized circuit switching networks. We
propose a novel interconnection method based on \textit{Hamiltonian
Decomposition} theory to organize separate rail-based rings into
\textit{all-to-all} topology, simultaneously optimizing ring-collective and
all-to-all communication. More than $100$K chips with hyper bandwidth can be
interconnected with a flat switching layer, and the diameter is only $2\sim4$
inter-node hops. The network cost per injection/All-Reduce bandwidth of
\textit{RailX} is less than $10\%$ of the Fat-Tree, and the cost per
bisection/All-to-All bandwidth is less than $50\%$ of the Fat-Tree.
Specifically, only $\sim$\$$1.3$B is required to interconnect 200K chips with
1.8TB bandwidth. \textit{RailX} can also be used in the ML-as-a-service (MLaaS)
scenario, where single or multiple training workloads with various shapes,
scales, and parallelism strategies can be flexibly mapped, and failures can be
worked around.

</details>


### [11] [3DGauCIM: Accelerating Static/Dynamic 3D Gaussian Splatting via Digital CIM for High Frame Rate Real-Time Edge Rendering](https://arxiv.org/abs/2507.19133)
*Wei-Hsing Huang,Cheng-Jhih Shih,Jian-Wei Su,Samuel Wade Wang,Vaidehi Garg,Yuyao Kong,Jen-Chun Tien,Nealson Li,Arijit Raychowdhury,Meng-Fan Chang,Yingyan,Lin,Shimeng Yu*

Main category: cs.AR

TL;DR: 动态3D高斯泼溅（3DGS）在边缘设备上实现时面临能耗和性能挑战，本文通过算法-硬件协同设计优化，实现高帧率实时渲染。


<details>
  <summary>Details</summary>
Motivation: 解决动态3DGS在边缘设备上因高能耗和低效率而难以实现实时渲染的问题。

Method: 提出算法优化（如减少DRAM访问、自适应分块分组、改进排序）和硬件设计（DCIM友好计算流）。

Result: 实验显示，静态和动态场景分别实现200 FPS以上，功耗仅为0.28 W和0.63 W。

Conclusion: 成功解决了在资源受限的边缘设备上实现高效动态3DGS的挑战。

Abstract: Dynamic 3D Gaussian splatting (3DGS) extends static 3DGS to render dynamic
scenes, enabling AR/VR applications with moving objects. However, implementing
dynamic 3DGS on edge devices faces challenges: (1) Loading all Gaussian
parameters from DRAM for frustum culling incurs high energy costs. (2)
Increased parameters for dynamic scenes elevate sorting latency and energy
consumption. (3) Limited on-chip buffer capacity with higher parameters reduces
buffer reuse, causing frequent DRAM access. (4) Dynamic 3DGS operations are not
readily compatible with digital compute-in-memory (DCIM). These challenges
hinder real-time performance and power efficiency on edge devices, leading to
reduced battery life or requiring bulky batteries. To tackle these challenges,
we propose algorithm-hardware co-design techniques. At the algorithmic level,
we introduce three optimizations: (1) DRAM-access reduction frustum culling to
lower DRAM access overhead, (2) Adaptive tile grouping to enhance on-chip
buffer reuse, and (3) Adaptive interval initialization Bucket-Bitonic sort to
reduce sorting latency. At the hardware level, we present a DCIM-friendly
computation flow that is evaluated using the measured data from a 16nm DCIM
prototype chip. Our experimental results on Large-Scale Real-World
Static/Dynamic Datasets demonstrate the ability to achieve high frame rate
real-time rendering exceeding 200 frame per second (FPS) with minimal power
consumption, merely 0.28 W for static Large-Scale Real-World scenes and 0.63 W
for dynamic Large-Scale Real-World scenes. This work successfully addresses the
significant challenges of implementing static/dynamic 3DGS technology on
resource-constrained edge devices.

</details>


### [12] [A3D-MoE: Acceleration of Large Language Models with Mixture of Experts via 3D Heterogeneous Integration](https://arxiv.org/abs/2507.19142)
*Wei-Hsing Huang,Janak Sharda,Cheng-Jhih Shih,Yuyao Kong,Faaiq Waqar,Pin-Jun Chen,Yingyan,Lin,Shimeng Yu*

Main category: cs.AR

TL;DR: 论文提出A3D-MoE系统，通过3D异构集成和优化调度，解决MoE架构LLM的硬件利用率低、延迟高和能耗大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统LLM和MoE架构在推理时存在硬件利用率低、能耗高和延迟问题，亟需优化。

Method: 采用3D异构集成技术、自适应GEMV-GEMM比阵列、硬件资源感知调度和MoE分数感知HBM访问减少。

Result: A3D-MoE显著提升性能，延迟降低1.8x-2x，能耗减少2x-4x，吞吐量提高1.44x-1.8x。

Conclusion: A3D-MoE为高效LLM推理提供了可行的解决方案，显著优化了性能和能耗。

Abstract: Conventional large language models (LLMs) are equipped with dozens of GB to
TB of model parameters, making inference highly energy-intensive and costly as
all the weights need to be loaded to onboard processing elements during
computation. Recently, the Mixture-of-Experts (MoE) architecture has emerged as
an efficient alternative, promising efficient inference with less activated
weights per token. Nevertheless, fine-grained MoE-based LLMs face several
challenges: 1) Variable workloads during runtime create arbitrary GEMV-GEMM
ratios that reduce hardware utilization, 2) Traditional MoE-based scheduling
for LLM serving cannot fuse attention operations with MoE operations, leading
to increased latency and decreased hardware utilization, and 3) Despite being
more efficient than conventional LLMs, loading experts from DRAM still consumes
significant energy and requires substantial DRAM bandwidth. Addressing these
challenges, we propose: 1) A3D-MoE, a 3D Heterogeneous Integration system that
employs state-of-the-art vertical integration technology to significantly
enhance memory bandwidth while reducing Network-on-Chip (NoC) overhead and
energy consumption. 2) A 3D-Adaptive GEMV-GEMM-ratio systolic array with
V-Cache efficient data reuse and a novel unified 3D dataflow to solve the
problem of reduced hardware utilization caused by arbitrary GEMV-GEMM ratios
from different workloads, 3) A Hardware resource-aware operation fusion
scheduler that fuses attention operations with MoE operations to enhance
hardware performance, and 4) MoE Score-Aware HBM access reduction with even-odd
expert placement that reduces DRAM access and bandwidth requirements. Our
evaluation results indicate that A3D-MoE delivers significant performance
enhancements, reducing latency by a factor of 1.8x to 2x and energy consumption
by 2x to 4x, while improving throughput by 1.44x to 1.8x compared to the
state-of-the-art.

</details>
