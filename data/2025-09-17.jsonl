{"id": "2509.12208", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.12208", "abs": "https://arxiv.org/abs/2509.12208", "authors": ["Boran Zhao", "Zihang Yuan", "Yanbin Hu", "Haiming Zhai", "Haoruo Zhang", "Wenzhe Zhao", "Tian Xia", "Pengju Ren"], "title": "IsoSched: Preemptive Tile Cascaded Scheduling of Multi-DNN via Subgraph Isomorphism", "comment": null, "summary": "Deploying deep neural network (DNN) accelerators with Layer Temporal\nScheduling (LTS) often incurs significant overheads (e.g., energy and latency),\nas intermediate activations must be cached in DRAM. To alleviate this, Tile\nSpatial Scheduling (TSS) reduces such costs by fragmenting inter-layer data\ninto smaller tiles communicated via on-chip links.However, many emerging\napplications require concurrent execution of multiple DNNs with complex\ntopologies, where critical tasks must preempt others to meet stringent latency\nrequirements (e.g., in autonomous driving, obstacle detection must complete\nwithin tens of milliseconds). Existing TSS works lack support for preemption,\nwhile prior preemption schemes rely on LTS and thus inherit its overheads. This\nhighlights the need for preemptive and efficient TSS-based frameworks. Yet,\nrealizing such systems is challenging due to the complexity of enabling\npreemption in graphs with large-scale topologies (e.g., modern large language\nmodels may contain tens of thousands of edges). To tackle this, we present\nIsoSched, the first framework enabling preemptive multi-DNN scheduling on TSS\narchitecture. IsoSched first formulates scheduling of complex-topology graphs\nas an integer-linear program (ILP) and subgraph isomorphism problem; second, it\napplies Layer Concatenate and Split (LCS) for load balancing in tile pipelines;\nthird, it employs an Ullmann-based algorithm enhanced by Monte Carlo Tree\nSearch (MCTS) to accelerate subgraph matching, and uses compact matrix encoding\n(i.e., Compressed Sparse Row, CSR) to reduce memory usage. IsoSched outperforms\nLTS-PRM approaches (i.e., PREMA, Planaria, CD-MSA, MoCA) in Latency-Bound\nThroughput (LBT), speedup, and energy efficiency, and achieves higher critical\ntask satisfaction than TSS-NPRM (i.e., HASP) across varying task complexities."}
{"id": "2509.12210", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.12210", "abs": "https://arxiv.org/abs/2509.12210", "authors": ["Masaru Dobashi", "Kohei Toshimitsu", "Hirotsugu Seike", "Miki Kanno", "Genki Horie", "Noboru Koshizuka"], "title": "A Proposal for High-Level Architectural Model Capable of Expressing Various Data Collaboration Platform and Data Space Concepts", "comment": "Comments: 16 pages, 3 figures, uses amsmath, amssymb, amsfonts,\n  graphicx, authblk", "summary": "This paper proposes \"Data Space High-Level Architecture Model\" (DS-HLAM) for\nexpressing diverse data collaboration platforms across regional\nimplementations. The framework introduces mathematically rigorous definitions\nwith success conditions formalized through finite state automata theory,\nenabling interoperability while preserving digital sovereignty requirements."}
{"id": "2509.12211", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12211", "abs": "https://arxiv.org/abs/2509.12211", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "TinyServe: Query-Aware Cache Selection for Efficient LLM Serving", "comment": "Accepted to ACM MM as Oral Paper, also accepted to ICML MOSS\n  workshop, publicly available as https://openreview.net/forum?id=sOdtl4jLci", "summary": "Serving large language models (LLMs) efficiently remains challenging due to\nthe high memory and latency overhead of key-value (KV) cache access during\nautoregressive decoding. We present \\textbf{TinyServe}, a lightweight and\nextensible serving system for deploying tiny LLMs (e.g., TinyLLaMA, GPT2-345M)\nwith support for structured KV sparsity, plugin-based token selection, and\nhardware-efficient attention kernels. Unlike prior simulation frameworks,\nTinyServe executes real-time decoding with configurable sparsity strategies and\nfine-grained instrumentation.\n  To reduce decoding cost, we introduce a \\textit{query-aware page selection}\nmechanism that leverages bounding-box metadata to estimate attention relevance\nbetween the query and KV cache blocks. This enables selective KV loading with\nminimal overhead and no model modifications. Our fused CUDA kernel integrates\npage scoring, sparse memory access, and masked attention in a single pass.\n  Experiments show that TinyServe achieves up to \\textbf{3.4x} speedup and over\n\\textbf{2x} memory savings with negligible accuracy drop. Additional analysis\nof cache reuse, page hit rate, and multi-GPU scaling confirms its practicality\nas an efficient system-level design for LLM training and inference research on\nresource-constrained hardware."}
{"id": "2509.12231", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.12231", "abs": "https://arxiv.org/abs/2509.12231", "authors": ["Jian Hou"], "title": "Research on fault diagnosis and root cause analysis based on full stack observability", "comment": null, "summary": "With the rapid development of cloud computing and ultra-large-scale data\ncenters, the scale and complexity of systems have increased significantly,\nleading to frequent faults that often show cascading propagation. How to\nachieve efficient, accurate, and interpretable Root Cause Analysis (RCA) based\non observability data (metrics, logs, traces) has become a core issue in AIOps.\nThis paper reviews two mainstream research threads in top conferences and\njournals over the past five years: FaultInsight[1] focusing on dynamic causal\ndiscovery and HolisticRCA[2] focusing on multi-modal/cross-level fusion, and\nanalyzes the advantages and disadvantages of existing methods. A KylinRCA\nframework integrating the ideas of both is proposed, which depicts the\npropagation chain through temporal causal discovery, realizes global root cause\nlocalization and type identification through cross-modal graph learning, and\noutputs auditable evidence chains combined with mask-based explanation methods.\nA multi-dimensional experimental scheme is designed, evaluation indicators are\nclarified, and engineering challenges are discussed, providing an effective\nsolution for fault diagnosis under full-stack observability."}
{"id": "2509.12593", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.12593", "abs": "https://arxiv.org/abs/2509.12593", "authors": ["Yimin Zhang", "Mario de Sousa"], "title": "Converting IEC 61131-3 LD into SFC Using Large Language Model: Dataset and Testing", "comment": null, "summary": "In the domain of Programmable Logic Controller (PLC) programming, converting\na Ladder Diagram (LD) into a Sequential Function Chart (SFC) is an inherently\nchallenging problem, primarily due to the lack of domain-specific knowledge and\nthe issue of state explosion in existing algorithms. However, the rapid\ndevelopment of Artificial Intelligence (AI) - especially Large Language Model\n(LLM) - offers a promising new approach.\n  Despite this potential, data-driven approaches in this field have been\nhindered by a lack of suitable datasets. To address this gap, we constructed\nseveral datasets consisting of paired textual representations of SFC and LD\nprograms that conform to the IEC 61131-3 standard.\n  Based on these datasets, we explored the feasibility of automating the LD-SFC\nconversion using LLM. Our preliminary experiments show that a fine-tuned LLM\nmodel achieves up to 91% accuracy on certain dataset, with the lowest observed\naccuracy being 79%, suggesting that with proper training and representation,\nLLMs can effectively support LD-SFC conversion. These early results highlight\nthe viability and future potential of this approach."}
{"id": "2509.12676", "categories": ["cs.AR", "cs.CR", "C.3; E.3; C.1"], "pdf": "https://arxiv.org/pdf/2509.12676", "abs": "https://arxiv.org/abs/2509.12676", "authors": ["Jiaao Ma", "Ceyu Xu", "Lisa Wu Wills"], "title": "A Scalable Architecture for Efficient Multi-bit Fully Homomorphic Encryption", "comment": "13 pages, 16 figures", "summary": "In the era of cloud computing, privacy-preserving computation offloading is\ncrucial for safeguarding sensitive data. Fully Homomorphic Encryption (FHE)\nenables secure processing of encrypted data, but the inherent computational\ncomplexity of FHE operations introduces significant computational overhead on\nthe server side. FHE schemes often face a tradeoff between efficiency and\nversatility. While the CKKS scheme is highly efficient for polynomial\noperations, it lacks the flexibility of the binary TFHE (Torus-FHE) scheme,\nwhich offers greater versatility but at the cost of efficiency. The recent\nmulti-bit TFHE extension offers greater flexibility and performance by\nsupporting native non-polynomial operations and efficient integer processing.\nHowever, current implementations of multi-bit TFHE are constrained by its\nnarrower numeric representation, which prevents its adoption in applications\nrequiring wider numeric representations.\n  To address this challenge, we introduce Taurus, a hardware accelerator\ndesigned to enhance the efficiency of multi-bit TFHE computations. Taurus\nsupports ciphertexts up to 10 bits by leveraging novel FFT units and optimizing\nmemory bandwidth through key reuse strategies. We also propose a compiler with\noperation deduplication to improve memory utilization. Our experiment results\ndemonstrate that Taurus achieves up to 2600x speedup over a CPU, 1200x speedup\nover a GPU, and up to 7x faster compared to the previous state-of-the-art TFHE\naccelerator. Moreover, Taurus is the first accelerator to demonstrate\nprivacy-preserving inference with large language models such as GPT-2. These\nadvancements enable more practical and scalable applications of\nprivacy-preserving computation in cloud environments."}
{"id": "2509.12232", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.12232", "abs": "https://arxiv.org/abs/2509.12232", "authors": ["Gianmarco Accordi", "Jens Domke", "Theresa Pollinger", "Davide Gadioli", "Gianluca Palermo"], "title": "Towards High-Performance and Portable Molecular Docking on CPUs through Vectorization", "comment": null, "summary": "Recent trends in the HPC field have introduced new CPU architectures with\nimproved vectorization capabilities that require optimization to achieve peak\nperformance and thus pose challenges for performance portability. The\ndeployment of high-performing scientific applications for CPUs requires\nadapting the codebase and optimizing for performance. Evaluating these\napplications provides insights into the complex interactions between code,\ncompilers, and hardware. We evaluate compiler auto-vectorization and explicit\nvectorization to achieve performance portability across modern CPUs with long\nvectors. We select a molecular docking application as a case study, as it\nrepresents computational patterns commonly found across HPC workloads. We\nreport insights into the technical challenges, architectural trends, and\noptimization strategies relevant to the future development of scientific\napplications for HPC. Our results show which code transformations enable\nportable auto-vectorization, reaching performance similar to explicit\nvectorization. Experimental data confirms that x86 CPUs typically achieve\nhigher execution performance than ARM CPUs, primarily due to their wider\nvectorization units. However, ARM architectures demonstrate competitive energy\nconsumption and cost-effectiveness."}
{"id": "2509.13006", "categories": ["cs.PL", "cs.MS", "math.OC", "90C05, 90c06, 90c10", "D.3.4; G.4"], "pdf": "https://arxiv.org/pdf/2509.13006", "abs": "https://arxiv.org/abs/2509.13006", "authors": ["Shermin Khosravi", "David Bremner"], "title": "Efficient Compilation of Algorithms into Compact Linear Programs", "comment": "Preliminary version will appear in CASCON 2025", "summary": "Linear Programming (LP) is widely applied in industry and is a key component\nof various other mathematical problem-solving techniques. Recent work\nintroduced an LP compiler translating polynomial-time, polynomial-space\nalgorithms into polynomial-size LPs using intuitive high-level programming\nlanguages, offering a promising alternative to manually specifying each set of\nconstraints through Algebraic Modeling Languages (AMLs). However, the resulting\nLPs, while polynomial in size, are often extremely large, posing challenges for\nexisting LP solvers. In this paper, we propose a novel approach for generating\nsubstantially smaller LPs from algorithms. Our goal is to establish\nminimum-size compact LP formulations for problems in P having natural\nformulations with exponential extension complexities. Our broader vision is to\nenable the systematic generation of Compact Integer Programming (CIP)\nformulations for problems with exponential-size IPs having polynomial-time\nseparation oracles. To this end, we introduce a hierarchical linear pipelining\ntechnique that decomposes nested program structures into synchronized regions\nwith well-defined execution transitions -- functions of compile-time\nparameters. This decomposition allows us to localize LP constraints and\nvariables within each region, significantly reducing LP size without the loss\nof generality, ensuring the resulting LP remains valid for all inputs of size\n$n$. We demonstrate the effectiveness of our method on two benchmark problems\n-- the makespan problem, which has exponential extension complexity, and the\nweighted minimum spanning tree problem -- both of which have exponential-size\nnatural LPs. Our results show up to a $25$-fold reduction in LP size and\nsubstantial improvements in solver performance across both commercial and\nnon-commercial LP solvers."}
{"id": "2509.12993", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.12993", "abs": "https://arxiv.org/abs/2509.12993", "authors": ["Cenlin Duan", "Jianlei Yang", "Rubing Yang", "Yikun Wang", "Yiou Wang", "Lingkun Long", "Yingjie Qi", "Xiaolin He", "Ao Zhou", "Xueyan Wang", "Weisheng Zhao"], "title": "HPIM: Heterogeneous Processing-In-Memory-based Accelerator for Large Language Models Inference", "comment": null, "summary": "The deployment of large language models (LLMs) presents significant\nchallenges due to their enormous memory footprints, low arithmetic intensity,\nand stringent latency requirements, particularly during the autoregressive\ndecoding stage. Traditional compute-centric accelerators, such as GPUs, suffer\nfrom severe resource underutilization and memory bandwidth bottlenecks in these\nmemory-bound workloads. To overcome these fundamental limitations, we propose\nHPIM, the first memory-centric heterogeneous Processing-In-Memory (PIM)\naccelerator that integrates SRAM-PIM and HBM-PIM subsystems designed\nspecifically for LLM inference. HPIM employs a software-hardware co-design\napproach that combines a specialized compiler framework with a heterogeneous\nhardware architecture. It intelligently partitions workloads based on their\ncharacteristics: latency-critical attention operations are mapped to the\nSRAM-PIM subsystem to exploit its ultra-low latency and high computational\nflexibility, while weight-intensive GEMV computations are assigned to the\nHBM-PIM subsystem to leverage its high internal bandwidth and large storage\ncapacity. Furthermore, HPIM introduces a tightly coupled pipeline strategy\nacross SRAM-PIM and HBM-PIM subsystems to maximize intra-token parallelism,\nthereby significantly mitigating serial dependency of the autoregressive\ndecoding stage. Comprehensive evaluations using a cycle-accurate simulator\ndemonstrate that HPIM significantly outperforms state-of-the-art accelerators,\nachieving a peak speedup of up to 22.8x compared to the NVIDIA A100 GPU.\nMoreover, HPIM exhibits superior performance over contemporary PIM-based\naccelerators, highlighting its potential as a highly practical and scalable\nsolution for accelerating large-scale LLM inference."}
{"id": "2509.12252", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.12252", "abs": "https://arxiv.org/abs/2509.12252", "authors": ["Foteini Stathopoulou", "Aggelos Ferikoglou", "Manolis Katsaragakis", "Dimosthenis Masouros", "Sotirios Xydis", "Dimitrios Soudris"], "title": "SynergAI: Edge-to-Cloud Synergy for Architecture-Driven High-Performance Orchestration for AI Inference", "comment": null, "summary": "The rapid evolution of Artificial Intelligence (AI) and Machine Learning (ML)\nhas significantly heightened computational demands, particularly for\ninference-serving workloads. While traditional cloud-based deployments offer\nscalability, they face challenges such as network congestion, high energy\nconsumption, and privacy concerns. In contrast, edge computing provides\nlow-latency and sustainable alternatives but is constrained by limited\ncomputational resources. In this work, we introduce SynergAI, a novel framework\ndesigned for performance- and architecture-aware inference serving across\nheterogeneous edge-to-cloud infrastructures. Built upon a comprehensive\nperformance characterization of modern inference engines, SynergAI integrates a\ncombination of offline and online decision-making policies to deliver\nintelligent, lightweight, and architecture-aware scheduling. By dynamically\nallocating workloads across diverse hardware architectures, it effectively\nminimizes Quality of Service (QoS) violations. We implement SynergAI within a\nKubernetes-based ecosystem and evaluate its efficiency. Our results demonstrate\nthat architecture-driven inference serving enables optimized and\narchitecture-aware deployments on emerging hardware platforms, achieving an\naverage reduction of 2.4x in QoS violations compared to a State-of-the-Art\n(SotA) solution."}
{"id": "2509.13019", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.13019", "abs": "https://arxiv.org/abs/2509.13019", "authors": ["Frédéric Fort", "David Nowak", "Vlad Rusu"], "title": "Pleasant Imperative Program Proofs with GallinaC", "comment": "In Proceedings FROM 2025, arXiv:2509.11877", "summary": "Even with the increase of popularity of functional programming, imperative\nprogramming remains a key programming paradigm, especially for programs\noperating at lower levels of abstraction. When such software offers key\ncomponents of a Trusted Computing Base (TCB), e.g. an operating system kernel,\nit becomes desirable to provide mathematical correctness proofs.\n  However, current real-world imperative programming languages possess\n\"expressive\", i.e. overly permissive, semantics. Thus, producing correctness\nproofs of such programs becomes tedious and error-prone, requiring to take care\nof numerous \"administrative\" details. Ideally, a proof-oriented imperative\nlanguage should feature well-behaved semantics while allowing imperative\nidioms.\n  To obtain a high-degree of confidence in the correctness of such a language,\nits tools should be developed inside a proof-assistant such that program proofs\nare machine checked.\n  We present GallinaC, a shallow embedding of a Turing-complete imperative\nlanguage directly inside the functional programming language of the Rocq proof\nassistant, Gallina. In particular, it features a truly generic and unbounded\nwhile loop. Having a functional core means proofs about GallinaC programs may\nuse the same tactics as proofs about pure functional ones.\n  Work on GallinaC is still under progress, but we present first promising\nresults. A prototype implementation has shown the viability of GallinaC with\nthe correctness proof of a list reversal procedure for linked-lists of unknown\nsize. We currently focus on the forward simulation between the GallinaC\nintermediate representation (IR) and Cminor, the entry language of the CompCert\nback-end."}
{"id": "2509.13029", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13029", "abs": "https://arxiv.org/abs/2509.13029", "authors": ["Yi Ren", "Baokang Peng", "Chenhao Xue", "Kairong Guo", "Yukun Wang", "Guoyao Cheng", "Yibo Lin", "Lining Zhang", "Guangyu Sun"], "title": "Orthrus: Dual-Loop Automated Framework for System-Technology Co-Optimization", "comment": "Accepted by ICCAD 2025", "summary": "With the diminishing return from Moore's Law, system-technology\nco-optimization (STCO) has emerged as a promising approach to sustain the\nscaling trends in the VLSI industry. By bridging the gap between system\nrequirements and technology innovations, STCO enables customized optimizations\nfor application-driven system architectures. However, existing research lacks\nsufficient discussion on efficient STCO methodologies, particularly in\naddressing the information gap across design hierarchies and navigating the\nexpansive cross-layer design space. To address these challenges, this paper\npresents Orthrus, a dual-loop automated framework that synergizes system-level\nand technology-level optimizations. At the system level, Orthrus employs a\nnovel mechanism to prioritize the optimization of critical standard cells using\nsystem-level statistics. It also guides technology-level optimization via the\nnormal directions of the Pareto frontier efficiently explored by Bayesian\noptimization. At the technology level, Orthrus leverages system-aware insights\nto optimize standard cell libraries. It employs a neural network-assisted\nenhanced differential evolution algorithm to efficiently optimize technology\nparameters. Experimental results on 7nm technology demonstrate that Orthrus\nachieves 12.5% delay reduction at iso-power and 61.4% power savings at\niso-delay over the baseline approaches, establishing new Pareto frontiers in\nSTCO."}
{"id": "2509.12256", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.12256", "abs": "https://arxiv.org/abs/2509.12256", "authors": ["Temitayo Adefemi"], "title": "The Entropy of Parallel Systems", "comment": "6 pages, 1 figure", "summary": "Ever since Claude Shannon used entropy for his \"Mathematical Theory of\nCommunication\", entropy has become a buzzword in research circles with\nscientists applying entropy to describe any phenomena that are reminiscent of\ndisorder. In this paper, we used entropy to describe the incompatibility\nbetween components in the computer, which can cause noise and disorder within\nthe parallel cluster. We develop a mathematical theory, primarily based on\ngraph theory and logarithms, to quantify the entropy of a parallel cluster by\naccounting for the entropy of each system within the cluster. We proceed using\nthis model to calculate the entropy of the Top 10 supercomputers in the Top500\nlist. Our entropy framework reveals a statistically significant negative\ncorrelation between system entropy and computational performance across the\nworld's fastest supercomputers. Most notably, the LINPACK benchmark\ndemonstrates a strong negative correlation (r = -0.7832, p = 0.0077) with our\nentropy measure, indicating that systems with lower entropy consistently\nachieve higher computational efficiency, this Relationship is further supported\nby moderate correlations with MLPerf mixed-precision benchmarks (r = -0.6234)\nand HPCC composite scores (r = -0.5890), suggesting the framework's\napplicability extends beyond traditional dense linear algebra workloads."}
{"id": "2509.13022", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.13022", "abs": "https://arxiv.org/abs/2509.13022", "authors": ["Andrei Nacu", "Dorel Lucanu"], "title": "Navigating the Python Type Jungle", "comment": "In Proceedings FROM 2025, arXiv:2509.11877", "summary": "Python's typing system has evolved pragmatically into a powerful but\ntheoretically fragmented system, with scattered specifications. This paper\nproposes a formalization to address this fragmentation. The central\ncontribution is a formal foundation that uses concepts from type theory to\ndemonstrate that Python's type system can be elegantly described. This work\naims to serve as a crucial first step toward the future development of type\ninference tools."}
{"id": "2509.12296", "categories": ["cs.DC", "cs.AI", "cs.CE", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12296", "abs": "https://arxiv.org/abs/2509.12296", "authors": ["Vijay Kumar Butte", "Sujata Butte"], "title": "An End to End Edge to Cloud Data and Analytics Strategy", "comment": null, "summary": "There is an exponential growth of connected Internet of Things (IoT) devices.\nThese have given rise to applications that rely on real time data to make\ncritical decisions quickly. Enterprises today are adopting cloud at a rapid\npace. There is a critical need to develop secure and efficient strategy and\narchitectures to best leverage capabilities of cloud and edge assets. This\npaper provides an end to end secure edge to cloud data and analytics strategy.\nTo enable real life implementation, the paper provides reference architectures\nfor device layer, edge layer and cloud layer."}
{"id": "2509.13128", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13128", "abs": "https://arxiv.org/abs/2509.13128", "authors": ["Raphaël Monat"], "title": "Try-Mopsa: Relational Static Analysis in Your Pocket", "comment": null, "summary": "Static analyzers are complex pieces of software with large dependencies. They\ncan be difficult to install, which hinders adoption and creates barriers for\nstudents learning static analysis. This work introduces Try-Mopsa: a\nscaled-down version of the Mopsa static analysis platform, compiled into\nJavaScript to run purely as a client-side application in web browsers.\nTry-Mopsa provides a responsive interface that works on both desktop and mobile\ndevices. Try-Mopsa features all the core components of Mopsa. In particular, it\nsupports relational numerical domains. We present the interface, changes and\nadaptations required to have a pure JavaScript version of Mopsa. We envision\nTry-Mopsa as a convenient platform for onboarding or teaching purposes."}
{"id": "2509.12384", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.12384", "abs": "https://arxiv.org/abs/2509.12384", "authors": ["Seth Ockerman", "Amal Gueroudji", "Song Young Oh", "Robert Underwood", "Nicholas Chia", "Kyle Chard", "Robert Ross", "Shivaram Venkataraman"], "title": "Exploring Distributed Vector Databases Performance on HPC Platforms: A Study with Qdrant", "comment": "To appear in the SC'25 Workshop Frontiers in Generative AI for HPC\n  Science and Engineering: Foundations, Challenges, and Opportunities", "summary": "Vector databases have rapidly grown in popularity, enabling efficient\nsimilarity search over data such as text, images, and video. They now play a\ncentral role in modern AI workflows, aiding large language models by grounding\nmodel outputs in external literature through retrieval-augmented generation.\nDespite their importance, little is known about the performance characteristics\nof vector databases in high-performance computing (HPC) systems that drive\nlarge-scale science. This work presents an empirical study of distributed\nvector database performance on the Polaris supercomputer in the Argonne\nLeadership Computing Facility. We construct a realistic biological-text\nworkload from BV-BRC and generate embeddings from the peS2o corpus using\nQwen3-Embedding-4B. We select Qdrant to evaluate insertion, index construction,\nand query latency with up to 32 workers. Informed by practical lessons from our\nexperience, this work takes a first step toward characterizing vector database\nperformance on HPC platforms to guide future research and optimization."}
{"id": "2509.13261", "categories": ["cs.PL", "D.3.4"], "pdf": "https://arxiv.org/pdf/2509.13261", "abs": "https://arxiv.org/abs/2509.13261", "authors": ["Noé De Santo", "Stephanie Weirich"], "title": "Rebound: Efficient, Expressive, and Well-Scoped Binding", "comment": "15 pages, 5 figures, 3 tables. To be published in Proceedings of the\n  18th ACM SIGPLAN International Haskell Symposium (Haskell 2025)", "summary": "We introduce the Rebound library that supports well-scoped term\nrepresentations in Haskell and automates the definition of substitution,\nalpha-equivalence, and other operations that work with binding structures. The\nkey idea of our design is the use of first-class environments that map\nvariables to expressions in some new scope. By statically tracking scopes,\nusers of this library gain confidence that they have correctly maintained the\nsubtle invariants that stem from using de Bruijn indices. Behind the scenes,\nRebound uses environments to optimize the application of substitutions, while\nproviding explicit access to these data structures when desired. We demonstrate\nthat this library is expressive by using it to implement a wide range of\nlanguage features with sophisticated uses of binding and several different\noperations that use this abstract syntax. Our examples include pi-forall, a\ntutorial implementation of a type checker for a dependently-typed programming\nlanguage. Finally, we benchmark Rebound to understand its performance\ncharacteristics and find that it produces faster code than competing libraries."}
{"id": "2509.12849", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12849", "abs": "https://arxiv.org/abs/2509.12849", "authors": ["Pedro Garcia Lopez", "Daniel Barcelona Pons", "Marcin Copik", "Torsten Hoefler", "Eduardo Quiñones", "Maciej Malawski", "Peter Pietzutch", "Alberto Marti", "Thomas Ohlson Timoudas", "Aleksander Slominski"], "title": "AI Factories: It's time to rethink the Cloud-HPC divide", "comment": null, "summary": "The strategic importance of artificial intelligence is driving a global push\ntoward Sovereign AI initiatives. Nationwide governments are increasingly\ndeveloping dedicated infrastructures, called AI Factories (AIF), to achieve\ntechnological autonomy and secure the resources necessary to sustain robust\nlocal digital ecosystems.\n  In Europe, the EuroHPC Joint Undertaking is investing hundreds of millions of\neuros into several AI Factories, built atop existing high-performance computing\n(HPC) supercomputers. However, while HPC systems excel in raw performance, they\nare not inherently designed for usability, accessibility, or serving as\npublic-facing platforms for AI services such as inference or agentic\napplications. In contrast, AI practitioners are accustomed to cloud-native\ntechnologies like Kubernetes and object storage, tools that are often difficult\nto integrate within traditional HPC environments.\n  This article advocates for a dual-stack approach within supercomputers:\nintegrating both HPC and cloud-native technologies. Our goal is to bridge the\ndivide between HPC and cloud computing by combining high performance and\nhardware acceleration with ease of use and service-oriented front-ends. This\nconvergence allows each paradigm to amplify the other. To this end, we will\nstudy the cloud challenges of HPC (Serverless HPC) and the HPC challenges of\ncloud technologies (High-performance Cloud)."}
{"id": "2509.12930", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.12930", "abs": "https://arxiv.org/abs/2509.12930", "authors": ["Xuefeng Han", "Wen Chen", "Jun Li", "Ming Ding", "Qingqing Wu", "Kang Wei", "Xiumei Deng", "Yumeng Shao", "Qiong Wu"], "title": "Analysis and Optimization of Wireless Multimodal Federated Learning on Modal Heterogeneity", "comment": null, "summary": "Multimodal federated learning (MFL) is a distributed framework for training\nmultimodal models without uploading local multimodal data of clients, thereby\neffectively protecting client privacy. However, multimodal data is commonly\nheterogeneous across diverse clients, where each client possesses only a subset\nof all modalities, renders conventional analysis results and optimization\nmethods in unimodal federated learning inapplicable. In addition, fixed latency\ndemand and limited communication bandwidth pose significant challenges for\ndeploying MFL in wireless scenarios. To optimize the wireless MFL performance\non modal heterogeneity, this paper proposes a joint client scheduling and\nbandwidth allocation (JCSBA) algorithm based on a decision-level fusion\narchitecture with adding a unimodal loss function. Specifically, with the\ndecision results, the unimodal loss functions are added to both the training\nobjective and local update loss functions to accelerate multimodal convergence\nand improve unimodal performance. To characterize MFL performance, we derive a\nclosed-form upper bound related to client and modality scheduling and minimize\nthe derived bound under the latency, energy, and bandwidth constraints through\nJCSBA. Experimental results on multimodal datasets demonstrate that the JCSBA\nalgorithm improves the multimodal accuracy and the unimodal accuracy by 4.06%\nand 2.73%, respectively, compared to conventional algorithms."}
{"id": "2509.12942", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.12942", "abs": "https://arxiv.org/abs/2509.12942", "authors": ["Michael Senn", "Christian Cachin"], "title": "Asymmetric Grid Quorum Systems for Heterogeneous Processes", "comment": null, "summary": "Quorum systems are a common way to formalize failure assumptions in\ndistributed systems. Traditionally, these assumptions are shared by all\ninvolved processes. More recently, systems have emerged which allow processes\nsome freedom in choosing their own, subjective or asymmetric, failure\nassumptions. For such a system to work, individual processes' assumptions must\nbe compatible. However, this leads to a Catch-22-style scenario: How can\nprocesses collaborate to agree on compatible failure assumptions when they have\nno compatible failure assumptions to start with?\n  We introduce asymmetric grid quorum systems that allow a group of processes\nto specify heterogeneous trust assumptions independently of each other and\nwithout coordination. They are based on qualitative attributes describing how\nthe processes differ. Each process may select a quorum system from this class\nthat aligns best with its subjective view. The available choices are designed\nto be compatible by definition, thereby breaking the cycling dependency.\nAsymmetric grid quorum systems have many applications that range from cloud\nplatforms to blockchain networks."}
{"id": "2509.13157", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13157", "abs": "https://arxiv.org/abs/2509.13157", "authors": ["Guillermo Toyos-Marfurt", "Petr Kuznetsov"], "title": "Space-Time Trade-off in Bounded Iterated Memory", "comment": "To be presented at the 27th International Symposium on Stabilization,\n  Safety, and Security of Distributed Systems (SSS 2025)", "summary": "The celebrated asynchronous computability theorem (ACT) characterizes tasks\nsolvable in the read-write shared-memory model using the unbounded\nfull-information protocol, where in every round of computation, each process\nshares its complete knowledge of the system with the other processes.\nTherefore, ACT assumes shared-memory variables of unbounded capacity. It has\nbeen recently shown that boundedvariables can achieve the same computational\npower at the expense of extra rounds. However, the exact relationship between\nthe bit capacity of the shared memory and the number of rounds required in\norder to implement one round of the full-information protocol remained unknown.\n  In this paper, we focus on the asymptotic round complexity of bounded\niterated shared-memory algorithms that simulate, up to isomorphism, the\nunbounded full-information protocol. We relate the round complexity to the\nnumber of processes $n$, the number of iterations of the full information\nprotocol $r$, and the bit size per shared-memory entry $b$. By analyzing the\ncorresponding protocol complex, a combinatorial structure representing\nreachable states, we derive necessary conditions and present a bounded\nfull-information algorithm tailored to the bits available $b$ per shared memory\nentry. We show that for $n>2$, the round complexity required to implement the\nfull-information protocol satisfies $\\Omega((n!)^{r-1} \\cdot 2^{n-b})$. Our\nresults apply to a range of iterated shared-memory models, from regular\nread-write registers to atomic and immediate snapshots. Moreover, our bounded\nfull-information algorithm is asymptotically optimal for the iterated collect\nmodel and within a linear factor $n$ of optimal for the snapshot-based models."}
{"id": "2509.13201", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13201", "abs": "https://arxiv.org/abs/2509.13201", "authors": ["Thanh Son Phung", "Douglas Thain"], "title": "Scaling Up Throughput-oriented LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management", "comment": "10 pages", "summary": "The widespread growth in LLM developments increasingly demands more\ncomputational power from clusters than what they can supply. Traditional LLM\napplications inherently require huge static resource allocations, which force\nusers to either wait in a long job queue and accept progress delay, or buy\nexpensive hardware to fulfill their needs and exacerbate the demand-supply\nproblem. However, not all LLM applications are latency-sensitive and can\ninstead be executed in a throughput-oriented way. This throughput orientation\nallows a dynamic allocation that opportunistically pools available resources\nover time, avoiding both the long queue and expensive GPU purchases.\nEffectively utilizing opportunistic resources brings numerous challenges\nnevertheless. Our solution, pervasive context management, exploits the common\ncomputational context in LLM applications and provides mechanisms and policies\nthat allow seamless context reuse on opportunistic resources. Our evaluation\nshows an LLM application with pervasive context management on opportunistic\nresources reduces its execution time by 98.1%."}
