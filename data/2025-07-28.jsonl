{"id": "2507.18729", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.18729", "abs": "https://arxiv.org/abs/2507.18729", "authors": ["Yanbo Zhao", "Jinku Cui", "Zecheng Li", "Shuyin Jiao", "Xu Liu", "Jiajia Li"], "title": "CUTHERMO: Understanding GPU Memory Inefficiencies with Heat Map Profiling", "comment": null, "summary": "GPUs have become indispensable in high-performance computing, machine\nlearning, and many other domains. Efficiently utilizing the memory subsystem on\nGPUs is critical for maximizing computing power through massive parallelism.\nAnalyzing memory access patterns has proven to be an effective method for\nunderstanding memory bottlenecks in applications. However, comprehensive\nruntime and fine-grained memory profiling support is lacking on GPU\narchitectures. In this work, we introduce cuThermo, a lightweight and practical\nprofiling tool for GPU memory analysis. It operates on GPU binaries without\nrequiring any modifications to hardware, operating system, or application\nsource code. Given a CUDA application, cuThermo identifies memory\ninefficiencies at runtime via a heat map based on distinct visited warp counts\nto represent word-sector-level data sharing and provides optimization guidance\nin performance tuning iterations. Through our experiments on six applications,\nwe identified five memory access patterns that are portable across different\nGPU architectures. By evaluating optimization on two GPUs, cuThermo achieves up\nto $721.79\\%$ performance improvement."}
{"id": "2507.18748", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.18748", "abs": "https://arxiv.org/abs/2507.18748", "authors": ["Z. Jonny Kong", "Qiang Xu", "Y. Charlie Hu"], "title": "PPipe: Efficient Video Analytics Serving on Heterogeneous GPU Clusters via Pool-Based Pipeline Parallelism", "comment": null, "summary": "With the rapid innovation of GPUs, heterogeneous GPU clusters in both public\nclouds and on-premise data centers have become increasingly commonplace. In\nthis paper, we demonstrate how pipeline parallelism, a technique wellstudied\nfor throughput-oriented deep learning model training, can be used effectively\nfor serving latency-bound model inference, e.g., in video analytics systems, on\nheterogeneous GPU clusters. Our work exploits the synergy between diversity in\nmodel layers and diversity in GPU architectures, which results in comparable\ninference latency for many layers when running on low-class and high-class\nGPUs. We explore how such overlooked capability of low-class GPUs can be\nexploited using pipeline parallelism and present a novel inference serving\nsystem, PPipe, that employs pool-based pipeline parallelism via an MILP-based\ncontrol plane and a data plane that performs resource reservation-based\nadaptive batching. Evaluation results on diverse workloads (18 CNN models) show\nthat PPipe achieves 41.1% - 65.5% higher utilization of low-class GPUs while\nmaintaining high utilization of high-class GPUs, leading to 32.2% - 75.1%\nhigher serving throughput compared to various baselines."}
{"id": "2507.18864", "categories": ["cs.DC", "cs.CC", "C.2.4; I.2.8"], "pdf": "https://arxiv.org/pdf/2507.18864", "abs": "https://arxiv.org/abs/2507.18864", "authors": ["Ngoc Hung Nguyen", "Van-Dinh Nguyen", "Anh Tuan Nguyen", "Nguyen Van Thieu", "Hoang Nam Nguyen", "Symeon Chatzinotas"], "title": "Deadline-Aware Joint Task Scheduling and Offloading in Mobile Edge Computing Systems", "comment": "14 pages, 13 figures. Accepted for publication in IEEE Internet of\n  Things Journal (JIOT)", "summary": "The demand for stringent interactive quality-of-service has intensified in\nboth mobile edge computing (MEC) and cloud systems, driven by the imperative to\nimprove user experiences. As a result, the processing of computation-intensive\ntasks in these systems necessitates adherence to specific deadlines or\nachieving extremely low latency. To optimize task scheduling performance,\nexisting research has mainly focused on reducing the number of late jobs whose\ndeadlines are not met. However, the primary challenge with these methods lies\nin the total search time and scheduling efficiency. In this paper, we present\nthe optimal job scheduling algorithm designed to determine the optimal task\norder for a given set of tasks. In addition, users are enabled to make informed\ndecisions for offloading tasks based on the information provided by servers.\nThe details of performance analysis are provided to show its optimality and low\ncomplexity with the linearithmic time O(nlogn), where $n$ is the number of\ntasks. To tackle the uncertainty of the randomly arriving tasks, we further\ndevelop an online approach with fast outage detection that achieves rapid\nacceptance times with time complexity of O(n). Extensive numerical results are\nprovided to demonstrate the effectiveness of the proposed algorithm in terms of\nthe service ratio and scheduling cost."}
{"id": "2507.18928", "categories": ["cs.DC", "C.2.4; C.2.1"], "pdf": "https://arxiv.org/pdf/2507.18928", "abs": "https://arxiv.org/abs/2507.18928", "authors": ["Yufang Li", "Yuanbo Zhang", "Hanlong Liao", "Guoming Tang", "Deke Guo"], "title": "GPUnion: Autonomous GPU Sharing on Campus", "comment": "7 pages, 3 figures, 1 table. Submitted to the ACM Workshop on Hot\n  Topics in Networks (HOTNETS) 2025", "summary": "A pronounced imbalance in GPU resources exists on campus, where some\nlaboratories own underutilized servers while others lack the compute needed for\nAI research. GPU sharing can alleviate this disparity, while existing platforms\ntypically rely on centralized oversight and persistent allocation models,\nconflicting with the voluntary and autonomous nature of academic resource\nownership. We present GPUnion, a campus-scale GPU sharing platform enabling\nvoluntary participation while preserving full provider autonomy. GPUnion\nincorporates three core mechanisms: i) container-based task dispatching and\nexecution, ii) resource provider-first architecture, and iii) resilient\nexecution featuring automatic checkpointing and migration. GPUnion also\nsupports custom data storage and integrates the non-root execution and image\nattestation for isolation and security improvement for containerization. Case\nstudies across multiple campus scenarios demonstrate 30% more GPU utilization\nimprovement, 40% increase in interactive sessions, and 94% successful workload\nmigration during provider departures. GPUnion demonstrates that provider\nautonomy and platform reliability can coexist, challenging conventional\ncentralized paradigms and democratizing access to computational resources\nwithin campus networks."}
{"id": "2507.18889", "categories": ["cs.AR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.18889", "abs": "https://arxiv.org/abs/2507.18889", "authors": ["Yinxiao Feng", "Tiancheng Chen", "Yuchen Wei", "Siyuan Shen", "Shiju Wang", "Wei Li", "Kaisheng Ma", "Torsten Hoefler"], "title": "RailX: A Flexible, Scalable, and Low-Cost Network Architecture for Hyper-Scale LLM Training Systems", "comment": "25 pages, 21 figures, 6 tables", "summary": "Increasingly large AI workloads are calling for hyper-scale infrastructure;\nhowever, traditional interconnection network architecture is neither scalable\nnor cost-effective enough. Tree-based topologies such as the\n\\textit{Rail-optimized} network are extremely expensive, while direct\ntopologies such as \\textit{Torus} have insufficient bisection bandwidth and\nflexibility. In this paper, we propose \\textit{RailX}, a reconfigurable network\narchitecture based on intra-node direct connectivity and inter-node circuit\nswitching. Nodes and optical switches are physically 2D-organized, achieving\nbetter scalability than existing centralized circuit switching networks. We\npropose a novel interconnection method based on \\textit{Hamiltonian\nDecomposition} theory to organize separate rail-based rings into\n\\textit{all-to-all} topology, simultaneously optimizing ring-collective and\nall-to-all communication. More than $100$K chips with hyper bandwidth can be\ninterconnected with a flat switching layer, and the diameter is only $2\\sim4$\ninter-node hops. The network cost per injection/All-Reduce bandwidth of\n\\textit{RailX} is less than $10\\%$ of the Fat-Tree, and the cost per\nbisection/All-to-All bandwidth is less than $50\\%$ of the Fat-Tree.\nSpecifically, only $\\sim$\\$$1.3$B is required to interconnect 200K chips with\n1.8TB bandwidth. \\textit{RailX} can also be used in the ML-as-a-service (MLaaS)\nscenario, where single or multiple training workloads with various shapes,\nscales, and parallelism strategies can be flexibly mapped, and failures can be\nworked around."}
{"id": "2507.18792", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18792", "abs": "https://arxiv.org/abs/2507.18792", "authors": ["Zixu Zhou"], "title": "Decompiling Rust: An Empirical Study of Compiler Optimizations and Reverse Engineering Challenges", "comment": null, "summary": "Decompiling Rust binaries is challenging due to the language's rich type\nsystem, aggressive compiler optimizations, and widespread use of high-level\nabstractions. In this work, we conduct a benchmark-driven evaluation of\ndecompilation quality across core Rust features and compiler build modes. Our\nautomated scoring framework shows that generic types, trait methods, and error\nhandling constructs significantly reduce decompilation quality, especially in\nrelease builds. Through representative case studies, we analyze how specific\nlanguage constructs affect control flow, variable naming, and type information\nrecovery. Our findings provide actionable insights for tool developers and\nhighlight the need for Rust-aware decompilation strategies."}
{"id": "2507.19287", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.19287", "abs": "https://arxiv.org/abs/2507.19287", "authors": ["Pierre Jacquet", "Adrien Luxey-Bitri"], "title": "The Case for Time-Shared Computing Resources", "comment": "Post-proceedings paper presented at LIMITS 2025: 11th Workshop on\n  Computing within Limits, 2025-06-26/27, Online", "summary": "The environmental impact of Information and Communication Technologies (ICT)\ncontinues to grow, driven notably by increasing usage, rebound effects, and\nemerging demands. However, despite the virtual nature of its services, the\nsector remains inherently constrained by its materiality and cannot rely on an\ninfinite pool of resources. As a result, the wide variety of supported services\nmay need to be managed under stricter limits within hosting facilities in the\nfuture. Contrary to common assumptions, we show that tenants typically do not\nshare computing resources, even in environments commonly perceived as\nmutualized, such as cloud platforms. Time-sharing has been progressively phased\nout for reasons of performance, security, predictability, and, perhaps more\nimportantly, due to the decreasing cost of computing resources. This paper\nadvocates for managing fewer physical resources by improving resource sharing\nbetween tenants. It represents a paradigm shift, moving beyond traditional\ntime-sharing at the hardware level to a higher abstraction. This approach\nentails \"doing with fewer resources\" under conditions of \"reduced performance\".\nNonetheless, enhancing the mutualization of infrastructure can reduce cluster\nsizes (through consolidation) and improve energy efficiency, with gains related\nto the accepted performance trade-off, a situation potentially more socially\nacceptable than eliminating services. We review the current state of the art,\nidentify challenges and opportunities, propose interpretations of Time-Shared\nComputing, and outline key research directions."}
{"id": "2507.19133", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.19133", "abs": "https://arxiv.org/abs/2507.19133", "authors": ["Wei-Hsing Huang", "Cheng-Jhih Shih", "Jian-Wei Su", "Samuel Wade Wang", "Vaidehi Garg", "Yuyao Kong", "Jen-Chun Tien", "Nealson Li", "Arijit Raychowdhury", "Meng-Fan Chang", "Yingyan", "Lin", "Shimeng Yu"], "title": "3DGauCIM: Accelerating Static/Dynamic 3D Gaussian Splatting via Digital CIM for High Frame Rate Real-Time Edge Rendering", "comment": null, "summary": "Dynamic 3D Gaussian splatting (3DGS) extends static 3DGS to render dynamic\nscenes, enabling AR/VR applications with moving objects. However, implementing\ndynamic 3DGS on edge devices faces challenges: (1) Loading all Gaussian\nparameters from DRAM for frustum culling incurs high energy costs. (2)\nIncreased parameters for dynamic scenes elevate sorting latency and energy\nconsumption. (3) Limited on-chip buffer capacity with higher parameters reduces\nbuffer reuse, causing frequent DRAM access. (4) Dynamic 3DGS operations are not\nreadily compatible with digital compute-in-memory (DCIM). These challenges\nhinder real-time performance and power efficiency on edge devices, leading to\nreduced battery life or requiring bulky batteries. To tackle these challenges,\nwe propose algorithm-hardware co-design techniques. At the algorithmic level,\nwe introduce three optimizations: (1) DRAM-access reduction frustum culling to\nlower DRAM access overhead, (2) Adaptive tile grouping to enhance on-chip\nbuffer reuse, and (3) Adaptive interval initialization Bucket-Bitonic sort to\nreduce sorting latency. At the hardware level, we present a DCIM-friendly\ncomputation flow that is evaluated using the measured data from a 16nm DCIM\nprototype chip. Our experimental results on Large-Scale Real-World\nStatic/Dynamic Datasets demonstrate the ability to achieve high frame rate\nreal-time rendering exceeding 200 frame per second (FPS) with minimal power\nconsumption, merely 0.28 W for static Large-Scale Real-World scenes and 0.63 W\nfor dynamic Large-Scale Real-World scenes. This work successfully addresses the\nsignificant challenges of implementing static/dynamic 3DGS technology on\nresource-constrained edge devices."}
{"id": "2507.18885", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.18885", "abs": "https://arxiv.org/abs/2507.18885", "authors": ["Qiyuan Xu", "Renxi Wang", "Haonan Li", "David Sanan", "Conrad Watt"], "title": "IsaMini: Redesigned Isabelle Proof Lanugage for Machine Learning", "comment": null, "summary": "Neural Theorem Proving (NTP) employs deep learning methods, particularly\nLarge Language Models (LLMs), to automate formal proofs in proof assistants.\nThis approach holds promise for reducing the dramatic labor costs or\ncomputation costs required in proof engineering, which is fundamental to formal\nverification and other software engineering methods. The paper explores the\npotential of improving NTP by redesigning the proof language, given that LLMs'\ncapabilities depend highly on representations. We introduce \\emph{MiniLang}, a\nredesigned proof language for Isabelle/HOL incorporating an improved version of\nSledgehammer. Experiments show MiniLang benefits two fine-tuned LLMs by\nimproving the success rate on the PISA benchmark by up to 29\\% in comparison to\ngeneration of Isar proof script. The success rate under one attempt (so-called\n\\emph{pass@1}) reaches 69.1\\%, exceeding the previous Baldur's pass@64\n(65.7\\%); The pass@8 reaches 79.2\\%, exceeding the state-of-the-art on PISA\n(71.0\\%) achieved by Magnushammer."}
{"id": "2507.18889", "categories": ["cs.AR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.18889", "abs": "https://arxiv.org/abs/2507.18889", "authors": ["Yinxiao Feng", "Tiancheng Chen", "Yuchen Wei", "Siyuan Shen", "Shiju Wang", "Wei Li", "Kaisheng Ma", "Torsten Hoefler"], "title": "RailX: A Flexible, Scalable, and Low-Cost Network Architecture for Hyper-Scale LLM Training Systems", "comment": "25 pages, 21 figures, 6 tables", "summary": "Increasingly large AI workloads are calling for hyper-scale infrastructure;\nhowever, traditional interconnection network architecture is neither scalable\nnor cost-effective enough. Tree-based topologies such as the\n\\textit{Rail-optimized} network are extremely expensive, while direct\ntopologies such as \\textit{Torus} have insufficient bisection bandwidth and\nflexibility. In this paper, we propose \\textit{RailX}, a reconfigurable network\narchitecture based on intra-node direct connectivity and inter-node circuit\nswitching. Nodes and optical switches are physically 2D-organized, achieving\nbetter scalability than existing centralized circuit switching networks. We\npropose a novel interconnection method based on \\textit{Hamiltonian\nDecomposition} theory to organize separate rail-based rings into\n\\textit{all-to-all} topology, simultaneously optimizing ring-collective and\nall-to-all communication. More than $100$K chips with hyper bandwidth can be\ninterconnected with a flat switching layer, and the diameter is only $2\\sim4$\ninter-node hops. The network cost per injection/All-Reduce bandwidth of\n\\textit{RailX} is less than $10\\%$ of the Fat-Tree, and the cost per\nbisection/All-to-All bandwidth is less than $50\\%$ of the Fat-Tree.\nSpecifically, only $\\sim$\\$$1.3$B is required to interconnect 200K chips with\n1.8TB bandwidth. \\textit{RailX} can also be used in the ML-as-a-service (MLaaS)\nscenario, where single or multiple training workloads with various shapes,\nscales, and parallelism strategies can be flexibly mapped, and failures can be\nworked around."}
{"id": "2507.19142", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.19142", "abs": "https://arxiv.org/abs/2507.19142", "authors": ["Wei-Hsing Huang", "Janak Sharda", "Cheng-Jhih Shih", "Yuyao Kong", "Faaiq Waqar", "Pin-Jun Chen", "Yingyan", "Lin", "Shimeng Yu"], "title": "A3D-MoE: Acceleration of Large Language Models with Mixture of Experts via 3D Heterogeneous Integration", "comment": null, "summary": "Conventional large language models (LLMs) are equipped with dozens of GB to\nTB of model parameters, making inference highly energy-intensive and costly as\nall the weights need to be loaded to onboard processing elements during\ncomputation. Recently, the Mixture-of-Experts (MoE) architecture has emerged as\nan efficient alternative, promising efficient inference with less activated\nweights per token. Nevertheless, fine-grained MoE-based LLMs face several\nchallenges: 1) Variable workloads during runtime create arbitrary GEMV-GEMM\nratios that reduce hardware utilization, 2) Traditional MoE-based scheduling\nfor LLM serving cannot fuse attention operations with MoE operations, leading\nto increased latency and decreased hardware utilization, and 3) Despite being\nmore efficient than conventional LLMs, loading experts from DRAM still consumes\nsignificant energy and requires substantial DRAM bandwidth. Addressing these\nchallenges, we propose: 1) A3D-MoE, a 3D Heterogeneous Integration system that\nemploys state-of-the-art vertical integration technology to significantly\nenhance memory bandwidth while reducing Network-on-Chip (NoC) overhead and\nenergy consumption. 2) A 3D-Adaptive GEMV-GEMM-ratio systolic array with\nV-Cache efficient data reuse and a novel unified 3D dataflow to solve the\nproblem of reduced hardware utilization caused by arbitrary GEMV-GEMM ratios\nfrom different workloads, 3) A Hardware resource-aware operation fusion\nscheduler that fuses attention operations with MoE operations to enhance\nhardware performance, and 4) MoE Score-Aware HBM access reduction with even-odd\nexpert placement that reduces DRAM access and bandwidth requirements. Our\nevaluation results indicate that A3D-MoE delivers significant performance\nenhancements, reducing latency by a factor of 1.8x to 2x and energy consumption\nby 2x to 4x, while improving throughput by 1.44x to 1.8x compared to the\nstate-of-the-art."}
{"id": "2507.19015", "categories": ["cs.PL", "cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19015", "abs": "https://arxiv.org/abs/2507.19015", "authors": ["Samuel Xifaras", "Panagiotis Manolios", "Andrew T. Walter", "William Robertson"], "title": "An Enumerative Embedding of the Python Type System in ACL2s", "comment": "In Proceedings ACL2 2025, arXiv:2507.18567", "summary": "Python is a high-level interpreted language that has become an industry\nstandard in a wide variety of applications. In this paper, we take a first step\ntowards using ACL2s to reason about Python code by developing an embedding of a\nsubset of the Python type system in ACL2s. The subset of Python types we\nsupport includes many of the most commonly used type annotations as well as\nuser-defined types comprised of supported types. We provide ACL2s definitions\nof these types, as well as defdata enumerators that are customized to provide\ncode coverage and identify errors in Python programs. Using the ACL2s\nembedding, we can generate instances of types that can then be used as inputs\nto fuzz Python programs, which allows us to identify bugs in Python code that\nare not detected by state-of-the-art Python type checkers. We evaluate our work\nagainst four open-source repositories, extracting their type information and\ngenerating inputs for fuzzing functions with type signatures that are in the\nsupported subset of Python types. Note that we only use the type signatures of\nfunctions to generate inputs and treat the bodies of functions as black boxes.\nWe measure code coverage, which ranges from about 68% to more than 80%, and\nidentify code patterns that hinder coverage such as complex branch conditions\nand external file system dependencies. We conclude with a discussion of the\nresults and recommendations for future work."}
{"id": "2507.19176", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.19176", "abs": "https://arxiv.org/abs/2507.19176", "authors": ["Weijun Chen", "Yuxi Fu", "Huan Long"], "title": "A Programming Language for Feasible Solutions", "comment": null, "summary": "Runtime efficiency and termination are crucial properties in the studies of\nprogram verification. Instead of dealing with these issues in an ad hoc manner,\nit would be useful to develop a robust framework in which such properties are\nguaranteed by design. This paper introduces a new imperative programming\nlanguage whose design is grounded in a static type system that ensures the\nfollowing equivalence property: All definable programs are guaranteed to run in\npolynomial time; Conversely, all problems solvable in polynomial time can be\nsolved by some programs of the language. The contribution of this work is\ntwofold. On the theoretical side, the foundational equivalence property is\nestablished, and the proof of the equivalence theorem is non-trivial. On the\npractical side, a programming approach is proposed that can streamline program\nanalysis and verification for feasible computations. An interpreter for the\nlanguage has been implemented, demonstrating the feasibility of the approach in\npractice."}
