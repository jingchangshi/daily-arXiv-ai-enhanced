<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [DeGAS: Gradient-Based Optimization of Probabilistic Programs without Sampling](https://arxiv.org/abs/2601.15167)
*Francesca Randone,Romina Doz,Mirco Tribastone,Luca Bortolussi*

Main category: cs.PL

TL;DR: DeGAS 是一种用于无循环概率程序的可微分高斯近似语义，支持在包含连续和离散组件的模型中进行无样本、基于梯度的优化。


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的方法（如MCMC）在涉及连续变量的条件优化问题上可能收敛困难，需要一种能够处理混合连续-离散变量且支持梯度优化的方法。

Method: 使用高斯混合语义评估程序，将测度零谓词和离散分支替换为渐近平滑，得到后验概率和路径概率的闭式表达式，证明这些量对程序参数的可微性，从而支持端到端的自动微分优化。

Result: 在13个基准程序上，DeGAS的准确性和运行时间与变分推断和MCMC相当，并能可靠解决基于采样的基线方法因连续变量条件而无法收敛的优化问题。

Conclusion: DeGAS提供了一种有效的无样本梯度优化方法，特别适用于包含连续和离散组件的概率程序，解决了传统采样方法在条件优化中的收敛问题。

Abstract: We present DeGAS, a differentiable Gaussian approximate semantics for loopless probabilistic programs that enables sample-free, gradient-based optimization in models with both continuous and discrete components. DeGAS evaluates programs under a Gaussian-mixture semantics and replaces measure-zero predicates and discrete branches with a vanishing smoothing, yielding closed-form expressions for posterior and path probabilities. We prove differentiability of these quantities with respect to program parameters, enabling end-to-end optimization via standard automatic differentiation, without Monte Carlo estimators. On thirteen benchmark programs, DeGAS achieves accuracy and runtime competitive with variational inference and MCMC. Importantly, it reliably tackles optimization problems where sampling-based baselines fail to converge due to conditioning involving continuous variables.

</details>


### [2] [Contextual Metaprogramming for Session Types](https://arxiv.org/abs/2601.15180)
*Pedro Ângelo,Atsushi Igarashi,Yuito Murase,Vasco T. Vasconcelos*

Main category: cs.PL

TL;DR: 将分阶段元编程集成到会话类型的消息传递函数式语言中，通过上下文模态类型理论和多级上下文实现代码的打包、传输和本地执行。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中服务器通过会话类型消息按需准备和发送代码的需求，提供一种安全可靠的方式在分布式系统中传输和执行代码。

Method: 基于多级上下文的上下文模态类型理论模型，其中上下文值（包含任意项在一系列变量上的闭包）可以被打包并在消息中传输。接收后可以解包并在本地应用执行。类型系统区分线性资源（恰好使用一次）和无限制资源（无限次使用）。

Result: 实现了类型保持性、顺序计算的进展结果、并发运行时环境的无运行时错误，以及类型检查器的正确性证明。

Conclusion: 成功将分阶段元编程集成到会话类型消息传递语言中，为分布式代码传输和执行提供了类型安全的框架，适用于服务器按需准备和发送代码等实际应用场景。

Abstract: We propose the integration of staged metaprogramming into a session-typed message passing functional language. We build on a model of contextual modal type theory with multi-level contexts, where contextual values, closing arbitrary terms over a series of variables, may be boxed and transmitted in messages. Once received, one such value may then be unboxed and locally applied before being run. To motivate this integration, we present examples of real-world use cases, for which our system would be suitable, such as servers preparing and shipping code on demand via session typed messages. We present a type system that distinguishes linear (used exactly once) from unrestricted (used an unbounded number of times) resources, and further define a type checker, suitable for a concrete implementation. We show type preservation, a progress result for sequential computations and absence of runtime errors for the concurrent runtime environment, as well as the correctness of the type checker.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [JAXMg: A multi-GPU linear solver in JAX](https://arxiv.org/abs/2601.14466)
*Roeland Wiersema*

Main category: cs.DC

TL;DR: JAXMg 是一个为 JAX 提供多 GPU 稠密线性代数运算的库，通过集成 NVIDIA cuSOLVERMg 解决单 GPU 内存不足的大规模矩阵计算问题。


<details>
  <summary>Details</summary>
Motivation: 大规模稠密线性系统和特征值问题在科学计算中很常见，但现有多 GPU 求解器库难以集成到可组合的 JIT 编译 Python 工作流中，限制了 JAX 框架下的多 GPU 计算能力。

Method: 通过 XLA 外部函数接口将 JAX 与 NVIDIA cuSOLVERMg 连接，将分布式 GPU 求解器暴露为 JIT 兼容的 JAX 原语，支持基于 Cholesky 的线性求解和对称特征分解。

Result: JAXMg 实现了超越单 GPU 内存限制的矩阵计算，使可扩展线性代数能够直接嵌入 JAX 程序，保持与 JAX 变换的可组合性，支持端到端科学工作流的多 GPU 执行。

Conclusion: JAXMg 成功解决了 JAX 框架中多 GPU 稠密线性代数的集成问题，为科学计算提供了可组合、可扩展的解决方案，填补了现有工具链的空白。

Abstract: Solving large dense linear systems and eigenvalue problems is a core requirement in many areas of scientific computing, but scaling these operations beyond a single GPU remains challenging within modern programming frameworks. While highly optimized multi-GPU solver libraries exist, they are typically difficult to integrate into composable, just-in-time (JIT) compiled Python workflows. JAXMg provides multi-GPU dense linear algebra for JAX, enabling Cholesky-based linear solves and symmetric eigendecompositions for matrices that exceed single-GPU memory limits. By interfacing JAX with NVIDIA's cuSOLVERMg through an XLA Foreign Function Interface, JAXMg exposes distributed GPU solvers as JIT-compatible JAX primitives. This design allows scalable linear algebra to be embedded directly within JAX programs, preserving composability with JAX transformations and enabling multi-GPU execution in end-to-end scientific workflows.

</details>


### [4] [Exploring Performance-Productivity Trade-offs in AMT Runtimes: A Task Bench Study of Itoyori, ItoyoriFBC, HPX, and MPI](https://arxiv.org/abs/2601.14608)
*Torben R. Lahnor,Mia Reitz,Jonas Posner,Patrick Diehl*

Main category: cs.DC

TL;DR: 该研究将Itoyori和ItoyoriFBC两个异步多任务运行时集成到Task Bench框架中，与MPI和HPX进行综合性能与生产力对比评估。


<details>
  <summary>Details</summary>
Motivation: 异步多任务运行时作为MPI的替代方案具有生产力优势，但多样化的AMT生态系统使得公平比较变得困难。需要系统性地评估不同并行编程系统的性能与生产力权衡。

Method: 使用Task Bench参数化框架评估四个系统：MPI、HPX、Itoyori（基于PGAS和RDMA工作窃取）和ItoyoriFBC（扩展了基于future的同步）。通过应用效率、最小有效任务粒度评估性能，通过代码行数和库构造数量评估程序员生产力。

Result: MPI在规则、通信轻量级工作负载中效率最高但代码冗长；HPX在不同节点数下负载不均衡时保持稳定效率但生产力指标最差；Itoyori在通信密集型配置中效率最高且生产力领先；ItoyoriFBC效率略低于Itoyori但future同步为不规则工作负载提供潜力。

Conclusion: 异步多任务运行时并不天然保证比MPI更高的生产力，不同系统在性能与生产力间存在明显权衡。Itoyori在通信密集型场景中表现最佳，而ItoyoriFBC的future同步为不规则工作负载提供了表达潜力。

Abstract: Asynchronous Many-Task (AMT) runtimes offer a productive alternative to the Message Passing Interface (MPI). However, the diverse AMT landscape makes fair comparisons challenging. Task Bench, proposed by Slaughter et al., addresses this challenge through a parameterized framework for evaluating parallel programming systems. This work integrates two recent cluster AMTs, Itoyori and ItoyoriFBC, into Task Bench for comprehensive evaluation against MPI and HPX. Itoyori employs a Partitioned Global Address Space (PGAS) model with RDMA-based work stealing, while ItoyoriFBC extends it with futurebased synchronization.
  We evaluate these systems in terms of both performance and programmer productivity. Performance is assessed across various configurations, including compute-bound kernels, weak scaling, and both imbalanced and communication-intensive patterns. Performance is quantified using application efficiency, i.e., the percentage of maximum performance achieved, and the Minimum Effective Task Granularity (METG), i.e., the smallest task duration before runtime overheads dominate. Programmer productivity is quantified using Lines of Code (LOC) and the Number of Library Constructs (NLC).
  Our results reveal distinct trade-offs. MPI achieves the highest efficiency for regular, communication-light workloads but requires verbose, lowlevel code. HPX maintains stable efficiency under load imbalance across varying node counts, yet ranks last in productivity metrics, demonstrating that AMTs do not inherently guarantee improved productivity over MPI. Itoyori achieves the highest efficiency in communication-intensive configurations while leading in programmer productivity. ItoyoriFBC exhibits slightly lower efficiency than Itoyori, though its future-based synchronization offers potential for expressing irregular workloads.

</details>


### [5] [Exploiting Spot Instances for Time-Critical Cloud Workloads Using Optimal Randomized Strategies](https://arxiv.org/abs/2601.14612)
*Neelkamal Bhuyan,Randeep Bhatia,Murali Kodialam,TV Lakshman*

Main category: cs.DC

TL;DR: 提出ROSS随机调度算法，在混合云环境中实现最优√K竞争比，相比现有Ω(K)方法显著改进，实际评估显示成本节省提升30%


<details>
  <summary>Details</summary>
Motivation: 解决混合云环境中具有硬截止时间的作业调度挑战，需要在成本效益但不可靠的竞价实例与昂贵但可靠的按需实例之间做出决策，现有确定性策略存在Ω(K)的最坏情况竞争比限制

Method: 提出ROSS随机调度算法，采用随机化策略在竞价实例和按需实例之间进行调度决策，在合理截止时间约束下实现最优竞争比

Result: 理论证明ROSS算法达到√K的最优竞争比，显著优于现有确定性策略的Ω(K)下界；在Azure和AWS真实追踪数据上的评估显示，ROSS比现有最优方法节省高达30%的成本

Conclusion: ROSS算法在混合云截止时间感知调度中实现了理论最优性能，有效平衡成本优化和截止时间保证，在不同竞价市场条件下均表现出色

Abstract: This paper addresses the challenge of deadline-aware online scheduling for jobs in hybrid cloud environments, where jobs may run on either cost-effective but unreliable spot instances or more expensive on-demand instances, under hard deadlines. We first establish a fundamental limit for existing (predominantly-) deterministic policies, proving a worst-case competitive ratio of $Ω(K)$, where $K$ is the cost ratio between on-demand and spot instances. We then present a novel randomized scheduling algorithm, ROSS, that achieves a provably optimal competitive ratio of $\sqrt{K}$ under reasonable deadlines, significantly improving upon existing approaches. Extensive evaluations on real-world trace data from Azure and AWS demonstrate that ROSS effectively balances cost optimization and deadline guarantees, consistently outperforming the state-of-the-art by up to $30\%$ in cost savings, across diverse spot market conditions.

</details>


### [6] [Specifying and Verifying RDMA Synchronisation (Extended Version)](https://arxiv.org/abs/2601.14642)
*Guillaume Ambal,Max Stupple,Brijesh Dongol,Azalea Raad*

Main category: cs.DC

TL;DR: 论文提出了RDMA在TSO内存模型下的远程RMW操作语义RDMA^TSO_RMW，解决了现有RDMA语义缺乏远程同步原语的问题，并在此基础上构建了可组合的同步抽象库和三类远程锁。


<details>
  <summary>Details</summary>
Motivation: 现有的RDMA^TSO语义虽然描述了RDMA在TSO CPU上的行为，但缺乏对远程同步操作的形式化定义，导致无法验证锁等常见同步抽象的实现正确性。

Method: 1. 提出RDMA^TSO_RMW语义，首次形式化定义了TSO架构下的远程"读-修改-写"操作；2. 构建RDMA^WAIT_RMW库作为可组合同步抽象的基础；3. 基于此库设计、实现并验证三类适用于不同场景的远程锁；4. 提出类似顺序一致性的强RDMA模型RDMA^SC_RMW。

Result: 1. 发现远程RMW操作较弱，仅能保证与其他远程RMW操作的原子性；2. 开发了与现有高性能库LOCO兼容的同步库，确保可组合性和可验证性；3. 提供了三类经过验证的远程锁实现。

Conclusion: 论文填补了RDMA语义中远程同步原语的空白，通过形式化定义远程RMW操作和构建同步抽象库，使得在RDMA网络上实现和验证锁等同步机制成为可能，为构建可靠的分布式系统提供了理论基础。

Abstract: Remote direct memory access (RDMA) allows a machine to directly read from and write to the memory of remote machine, enabling high-throughput, low-latency data transfer. Ensuring correctness of RDMA programs has only recently become possible with the formalisation of $\text{RDMA}^\text{TSO}$ semantics (describing the behaviour of RDMA networking over a TSO CPU). However, this semantics currently lacks a formalisation of remote synchronisation, meaning that the implementations of common abstractions such as locks cannot be verified. In this paper, we close this gap by presenting $\text{RDMA}^{\text{TSO}}_{\text{RMW}}$, the first semantics for remote `read-modify-write' (RMW) instructions over TSO. It turns out that remote RMW operations are weak and only ensure atomicity against other remote RMWs. We therefore build a set of composable synchronisation abstractions starting with the $\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$ library. Underpinned by $\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$, we then specify, implement and verify three classes of remote locks that are suitable for different scenarios. Additionally, we develop the notion of a strong RDMA model, $\text{RDMA}^{\text{SC}}_{\text{RMW}}$, which is akin to sequential consistency in shared memory architectures. Our libraries are built to be compatible with an existing set of high-performance libraries called LOCO, which ensures compositionality and verifiability.

</details>


### [7] [Optimizing FaaS Platforms for MCP-enabled Agentic Workflows](https://arxiv.org/abs/2601.14735)
*Varad Kulkarni,Vaibhav Jha,Nikhil Reddy,Yogesh Simmhan*

Main category: cs.DC

TL;DR: FAME是一个基于FaaS的架构，用于编排支持MCP的智能体工作流，通过将智能体模式分解为可组合的FaaS函数，实现模块化、自动扩展和成本效益，显著降低延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 基于LLM和MCP的自主AI智能体工作流快速增长，但面临云部署扩展性和状态管理的挑战。传统VM托管资源密集且缺乏弹性，而FaaS平台虽然模块化、可自动扩展且成本效益高，但本质上是无状态的。

Method: FAME将智能体模式（如ReAct）分解为可组合的智能体：Planner、Actor和Evaluator，每个都是使用LangGraph构建的FaaS函数，并作为FaaS工作流编排。通过DynamoDB自动化智能体内存持久化和注入，使用AWS Lambda包装器优化MCP服务器部署，在S3中缓存工具输出，并提出函数融合策略。

Result: 在两个代表性应用（研究论文摘要和日志分析）上评估，结果显示：延迟降低高达13倍，输入令牌减少88%，成本节省66%，工作流完成率提高。

Conclusion: FAME证明了无服务器平台在规模化托管复杂多智能体AI工作流方面的可行性，通过FaaS架构解决了状态管理和扩展性挑战。

Abstract: Agentic workflows that use autonomous AI Agents powered by Large Language Models (LLMs) and Model Context Protocol (MCP) servers is rapidly rising. This introduces challenges in scalable cloud deployment and state management. Traditional hosting on Virtual Machines (VMs) is resource-intensive and lacks elasticity. Functions-as-a-Service (FaaS) platforms offer modularity, autoscaling and cost efficiency but are inherently stateless. In this paper, we present the FAME, a FaaS-based architecture for orchestrating MCP-enabled agentic workflows. FAME decomposes agentic patterns such as ReAct into composable agents: Planner, Actor and Evaluator, that are each a FaaS function built using LangGraph and are orchestrated as a FaaS workflow. This enables modular composition as AWS Step Functions and avoids function timeouts seen for monolithic agentic workflows. To address context persistence across user requests in a conversation, FAME automates agent memory persistence and injection using DynamoDB. It also optimizes MCP server deployment through AWS Lambda wrappers, caches tool outputs in S3 and proposes function fusion strategies. We evaluate FAME on two representative applications, on research paper summarization and log analytics, under diverse memory and caching configurations. Results show up to 13x latency reduction, 88% fewer input tokens and 66% in cost savings, along with improved workflow completion rates. This demonstrates the viability of serverless platforms for hosting complex, multi-agent AI workflows at scale.

</details>


### [8] [AlertGuardian: Intelligent Alert Life-Cycle Management for Large-scale Cloud Systems](https://arxiv.org/abs/2601.14912)
*Guangba Yu,Genting Mai,Rui Wang,Ruipeng Li,Pengfei Chen,Long Pan,Ruijie Xu*

Main category: cs.DC

TL;DR: AlertGuardian框架结合LLM和图模型优化云系统告警生命周期管理，通过去噪、摘要和规则优化三个阶段显著减少告警疲劳并提升故障诊断效率。


<details>
  <summary>Details</summary>
Motivation: 云系统产生大量告警导致告警疲劳，降低运维效率，需要优化告警生命周期管理来解决这一问题。

Method: 提出AlertGuardian框架，结合大语言模型和轻量图模型：1) Alert Denoise使用带虚拟噪声的图学习模型过滤噪声；2) Alert Summary采用RAG增强的LLM生成可操作摘要；3) Alert Rule Refinement利用多智能体迭代反馈优化告警规则。

Result: 在四个真实数据集上评估：告警减少率达94.8%，故障诊断准确率达90.5%，优化了1,174条告警规则，其中375条被SRE接受（32%接受率）。

Conclusion: AlertGuardian能有效缓解云系统告警疲劳，提升运维效率，并在实际部署中分享了成功经验和教训。

Abstract: Alerts are critical for detecting anomalies in large-scale cloud systems, ensuring reliability and user experience. However, current systems generate overwhelming volumes of alerts, degrading operational efficiency due to ineffective alert life-cycle management. This paper details the efforts of Company-X to optimize alert life-cycle management, addressing alert fatigue in cloud systems. We propose AlertGuardian, a framework collaborating large language models (LLMs) and lightweight graph models to optimize the alert life-cycle through three phases: Alert Denoise uses graph learning model with virtual noise to filter noise, Alert Summary employs Retrieval Augmented Generation (RAG) with LLMs to create actionable summary, and Alert Rule Refinement leverages multi-agent iterative feedbacks to improve alert rule quality. Evaluated on four real-world datasets from Company-X's services, AlertGuardian significantly mitigates alert fatigue (94.8\% alert reduction ratios) and accelerates fault diagnosis (90.5\% diagnosis accuracy). Moreover, AlertGuardian improves 1,174 alert rules, with 375 accepted by SREs (32% acceptance rate). Finally, we share success stories and lessons learned about alert life-cycle management after the deployment of AlertGuardian in Company-X.

</details>


### [9] [Application-level observability for adaptive Edge to Cloud continuum systems](https://arxiv.org/abs/2601.14923)
*Kaddour Sidi,Daniel Balouek,Baptiste Jonglez*

Main category: cs.DC

TL;DR: 提出一个应用级可观测性框架，结合开发者驱动的仪器化和SLO感知反馈，实现边缘到云系统的自主适应


<details>
  <summary>Details</summary>
Motivation: 现代边缘到云系统需要细粒度可观测性，以确保在异构动态环境中的自适应行为和性能目标合规

Method: 集成OpenTelemetry、Prometheus、K3s和Chaos Mesh，构建应用级可观测性框架，支持实时监控和自适应控制

Result: 视频处理用例显示应用级指标能指导自动调整，在可变工作负载和注入故障下维持目标帧率、延迟和检测精度

Conclusion: 初步结果展示了改进的可扩展性、容错性和响应性，为自适应、SLO合规的边缘到云应用提供了实用基础

Abstract: Modern Edge-to-Cloud (E2C) systems require fine-grained observability to ensure adaptive behavior and compliance with performance objectives across heterogeneous and dynamic environments. This work introduces an application-level observability framework that integrates developer-driven instrumentation and SLO-aware feedback for autonomous adaptation. By combining OpenTelemetry, Prometheus, K3s, and Chaos Mesh, the framework enables real-time monitoring and adaptive control across the continuum. A video processing use case demonstrates how application-level metrics guide automatic adjustments to maintain target frame rate, latency, and detection accuracy under variable workloads and injected faults. Preliminary results highlight improved scalability, fault tolerance, and responsiveness, providing a practical foundation for adaptive, SLO-compliant E2C applications.

</details>


### [10] [Parallel Collaborative ADMM Privacy Computing and Adaptive GPU Acceleration for Distributed Edge Networks](https://arxiv.org/abs/2601.14980)
*Mengchun Xia,Zhicheng Dong,Donghong Cai,Fang Fang,Lisheng Fan,Pingzhi Fan*

Main category: cs.DC

TL;DR: 提出3P-ADMM-PC2算法，结合并行协作ADMM、Paillier同态加密和GPU加速，解决边缘网络中分布式计算的隐私保护和计算效率问题


<details>
  <summary>Details</summary>
Motivation: 边缘网络中分布式计算面临单节点计算能力有限、协作计算导致信息泄露和通信开销过大的问题，需要设计既能保护隐私又能高效计算的解决方案

Method: 设计并行协作分布式ADMM算法，引入Paillier同态加密保护数据隐私，采用量化方法将实数映射到正整数区间，通过GPU加速实现长密钥的并行加密解密计算

Result: 3P-ADMM-PC2算法在均方误差性能上接近无隐私保护的分布式ADMM，相比CPU实现的集中式和分布式ADMM具有显著加速比

Conclusion: 提出的GPU加速3P-ADMM-PC2算法能有效解决边缘网络中分布式计算的隐私保护和计算效率问题，具有优异的性能和加速效果

Abstract: Distributed computing has been widely applied in distributed edge networks for reducing the processing burden of high-dimensional data centralization, where a high-dimensional computational task is decomposed into multiple low-dimensional collaborative processing tasks or multiple edge nodes use distributed data to train a global model. However, the computing power of a single-edge node is limited, and collaborative computing will cause information leakage and excessive communication overhead. In this paper, we design a parallel collaborative distributed alternating direction method of multipliers (ADMM) and propose a three-phase parallel collaborative ADMM privacy computing (3P-ADMM-PC2) algorithm for distributed computing in edge networks, where the Paillier homomorphic encryption is utilized to protect data privacy during interactions. Especially, a quantization method is introduced, which maps the real numbers to a positive integer interval without affecting the homomorphic operations. To address the architectural mismatch between large-integer and Graphics Processing Unit (GPU) computing, we transform high-bitwidth computations into low-bitwidth matrix and vector operations. Thus the GPU can be utilized to implement parallel encryption and decryption computations with long keys. Finally, a GPU-accelerated 3P-ADMM-PC2 is proposed to optimize the collaborative computing tasks. Meanwhile, large-scale computational tasks are conducted in network topologies with varying numbers of edge nodes. Experimental results demonstrate that the proposed 3P-ADMM-PC2 has excellent mean square error performance, which is close to that of distributed ADMM without privacy-preserving. Compared to centralized ADMM and distributed ADMM implemented with Central Processing Unit (CPU) computation, the proposed scheme demonstrates a significant speedup ratio.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [End-to-End Transformer Acceleration Through Processing-in-Memory Architectures](https://arxiv.org/abs/2601.14260)
*Xiaoxuan Yang,Peilin Chen,Tergel Molom-Ochir,Yiran Chen*

Main category: cs.AR

TL;DR: 本文提出基于存内计算的Transformer加速方案，通过重构注意力机制、动态压缩KV缓存、将注意力重新解释为关联记忆操作，显著提升能效和降低延迟。


<details>
  <summary>Details</summary>
Motivation: Transformer在大规模部署时面临三大挑战：注意力机制需要大量矩阵乘法和中间结果在内存与计算单元间的频繁移动导致高延迟高能耗；长上下文推理中KV缓存可能超过模型权重大小造成内存带宽瓶颈；注意力机制的二次复杂度加剧了数据移动和计算开销。

Method: 采用存内计算方案：1）重构注意力和前馈计算以最小化片外数据传输；2）动态压缩和剪枝KV缓存以管理内存增长；3）将注意力重新解释为关联记忆操作以降低复杂度和硬件占用。

Result: 与最先进的加速器和通用GPU相比，该存内计算设计在能效和延迟方面表现出显著改进。

Conclusion: 这些方法共同解决了计算开销、内存可扩展性和注意力复杂度问题，进一步实现了Transformer模型的高效端到端加速。

Abstract: Transformers have become central to natural language processing and large language models, but their deployment at scale faces three major challenges. First, the attention mechanism requires massive matrix multiplications and frequent movement of intermediate results between memory and compute units, leading to high latency and energy costs. Second, in long-context inference, the key-value cache (KV cache) can grow unpredictably and even surpass the model's weight size, creating severe memory and bandwidth bottlenecks. Third, the quadratic complexity of attention with respect to sequence length amplifies both data movement and compute overhead, making large-scale inference inefficient. To address these issues, this work introduces processing-in-memory solutions that restructure attention and feed-forward computation to minimize off-chip data transfers, dynamically compress and prune the KV cache to manage memory growth, and reinterpret attention as an associative memory operation to reduce complexity and hardware footprint. Moreover, we evaluate our processing-in-memory design against state-of-the-art accelerators and general-purpose GPUs, demonstrating significant improvements in energy efficiency and latency. Together, these approaches address computation overhead, memory scalability, and attention complexity, further enabling efficient, end-to-end acceleration of Transformer models.

</details>


### [12] [Multi-Partner Project: COIN-3D -- Collaborative Innovation in 3D VLSI Reliability](https://arxiv.org/abs/2601.14347)
*George Rafael Gourdoumanis,Fotoini Oikonomou,Maria Pantazi-Kypraiou,Pavlos Stoikos,Olympia Axelou,Athanasios Tziouvaras,Georgios Karakonstantis,Tahani Aladwani,Christos Anagnostopoulos,Yixian Shen,Anuj Pathania,Alberto Garcia-Ortiz,George Floros*

Main category: cs.AR

TL;DR: 该论文介绍了COIN-3D项目，旨在通过开发开源EDA工具来加强2.5D/3D VLSI系统的可靠性研究，以应对半导体制造向亚纳米工艺和GAAFETs转型带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着半导体制造从3纳米工艺向亚纳米领域发展，并从FinFETs转向全环绕栅极场效应晶体管(GAAFETs)，制造复杂性和挑战不断增加。3D小芯片方法成为解决这些限制并利用扩展设计空间的关键推动因素，特别是小芯片有助于解决大型单片设计通常存在的低良率问题。

Method: 通过Horizon Europe Twinning项目COIN-3D，在领先的欧洲机构之间建立合作，开发新颖的开源电子设计自动化(EDA)工具，用于3D系统的可靠性评估，集成物理级和系统级可靠性分析的先进算法。

Result: 该项目旨在提供开源EDA工具，用于2.5D/3D VLSI系统的可靠性评估，通过跨机构合作加强研究卓越性，为异构系统设计提供成本效益高的策略。

Conclusion: COIN-3D项目通过开发先进的可靠性分析工具，为应对半导体制造向更小工艺节点和3D集成转型带来的挑战提供了重要解决方案，有助于实现更可靠、成本效益更高的异构系统设计。

Abstract: As semiconductor manufacturing advances from the 3-nm process toward the sub-nanometer regime and transitions from FinFETs to gate-all-around field-effect transistors (GAAFETs), the resulting complexity and manufacturing challenges continue to increase. In this context, 3D chiplet-based approaches have emerged as key enablers to address these limitations while exploiting the expanded design space. Specifically, chiplets help address the lower yields typically associated with large monolithic designs. This paradigm enables the modular design of heterogeneous systems consisting of multiple chiplets (e.g., CPUs, GPUs, memory) fabricated using different technology nodes and processes. Consequently, it offers a capable and cost-effective strategy for designing heterogeneous systems. This paper introduces the Horizon Europe Twinning project COIN-3D (Collaborative Innovation in 3D VLSI Reliability), which aims to strengthen research excellence in 2.5D/3D VLSI systems reliability through collaboration between leading European institutions. More specifically, our primary scientific goal is the provision of novel open-source Electronic Design Automation (EDA) tools for reliability assessment of 3D systems, integrating advanced algorithms for physical- and system-level reliability analysis.

</details>


### [13] [Pipeline Automation Framework for Reusable High-throughput Network Applications on FPGA](https://arxiv.org/abs/2601.15151)
*Jean Bruant,Pierre-Henri Horrein,Olivier Muller,Frédéric Pétrot*

Main category: cs.AR

TL;DR: PAF是一个基于Chisel的开源流水线自动化框架，用于参数化FPGA网络功能设计，实现跨多种FPGA平台的硬件设计复用和优化。


<details>
  <summary>Details</summary>
Motivation: 在云服务提供商部署可扩展基础设施的背景下，FPGA作为网络基础设施的一部分，虽然能保证低延迟和高吞吐量的数据包处理，但硬件设计过程缓慢，难以适应快速演进的敏捷基础设施需求。同时，在多种FPGA平台上部署和维护网络功能需要对硬件设计进行精细调优。

Method: 提出了PAF（Pipeline Automation Framework），这是一个基于流水线导向设计方法的开源架构参数化框架。PAF基于Chisel（一种Scala嵌入式硬件构造语言）实现，利用该语言进行电路细化接口。框架专注于流水线架构意图的描述，减少表达复杂功能所需的代码量。

Result: 应用于工业网络数据包分类系统时，PAF展示了高效的参数化能力，能够在多种FPGA上复用和优化相同的流水线设计。自动化并未导致架构控制的损失，实现了与详尽描述实现相当的性能和资源使用效率。

Conclusion: PAF框架通过架构参数化和流水线自动化，解决了FPGA硬件设计过程缓慢和跨平台部署困难的问题，使FPGA能够更好地融入快速演进的云基础设施中，同时保持对架构的紧密控制。

Abstract: In a context of ever-growing worldwide communication traffic, cloud service providers aim at deploying scalable infrastructures to address heterogeneous needs. Part of the network infrastructure, FPGAs are tailored to guarantee low-latency and high-throughput packet processing. However, slowness of the hardware design process impairs FPGA ability to be part of an agile infrastructure under constant evolution, from incident response to long-term transformation. Deploying and maintaining network functionalities across a wide variety of FPGAs raises the need to fine-tune hardware designs for several FPGA targets. To address this issue, we introduce PAF, an open-source architectural parameterization framework based on a pipeline-oriented design methodology. PAF (Pipeline Automation Framework) implementation is based on Chisel, a Scala-embedded Hardware Construction Language (HCL), that we leverage to interface with circuit elaboration. Applied to industrial network packet classification systems, PAF demonstrates efficient parameterization abilities, enabling to reuse and optimize the same pipelined design on several FPGAs. In addition, PAF focuses the pipeline description on the architectural intent, incidentally reducing the number of lines of code to express complex functionalities. Finally, PAF confirms that automation does not imply any loss of tight control on the architecture by achieving on par performance and resource usage with equivalent exhaustively described implementations.

</details>
