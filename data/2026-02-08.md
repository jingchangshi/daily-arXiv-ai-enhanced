<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Doc2Spec: Synthesizing Formal Programming Specifications from Natural Language via Grammar Induction](https://arxiv.org/abs/2602.04892)
*Shihao Xia,Mengting He,Haomin Jia,Linhai Song*

Main category: cs.PL

TL;DR: Doc2Spec：使用多智能体框架和LLM从自然语言规则自动推导规范语法，然后生成形式化规范，提高API实现和使用的合规性验证效率。


<details>
  <summary>Details</summary>
Motivation: 确保API实现和使用符合自然语言编程规则对软件正确性、安全性和可靠性至关重要。形式化验证能提供强保证但需要精确规范，而手动编写规范既困难又昂贵。

Method: 提出Doc2Spec多智能体框架，使用LLM从自然语言规则自动推导规范语法，然后基于推导出的语法生成形式化规范。语法捕获关键领域知识，约束规范空间，并确保表示一致性。

Result: 在三种编程语言的七个基准测试上评估，Doc2Spec优于无语法推导的基线方法，并与使用手动构建语法的技术取得竞争性结果，证明了自动语法推导在形式化自然语言规则方面的有效性。

Conclusion: Doc2Spec通过自动推导规范语法，显著提高了形式化规范的生成质量和可靠性，为解决手动编写规范的成本和难度问题提供了有效方案。

Abstract: Ensuring that API implementations and usage comply with natural language programming rules is critical for software correctness, security, and reliability. Formal verification can provide strong guarantees but requires precise specifications, which are difficult and costly to write manually. To address this challenge, we present Doc2Spec, a multi-agent framework that uses LLMs to automatically induce a specification grammar from natural-language rules and then generates formal specifications guided by the induced grammar. The grammar captures essential domain knowledge, constrains the specification space, and enforces consistent representations, thereby improving the reliability and quality of generated specifications. Evaluated on seven benchmarks across three programming languages, Doc2Spec outperforms a baseline without grammar induction and achieves competitive results against a technique with a manually crafted grammar, demonstrating the effectiveness of automated grammar induction for formalizing natural-language rules.

</details>


### [2] [Strong Normalisation for Asynchronous Effects](https://arxiv.org/abs/2602.05528)
*Danel Ahman,Ilja Sobolev*

Main category: cs.PL

TL;DR: 该论文研究了Ahman和Pretnar提出的异步效应演算的规范化性质，证明了移除一般递归后整个演算（包括顺序和并行部分）是强规范化的，并进一步证明了在重新引入受控的中断驱动递归行为后，顺序片段仍保持强规范化。


<details>
  <summary>Details</summary>
Motivation: Ahman和Pretnar的异步效应演算扩展了传统的同步代数计算效应，为代数操作调用提供了异步处理能力，能够自然建模抢占式多线程、可取消远程函数调用、多方应用等场景。然而，原演算包含一般递归，其规范化性质尚未得到研究。

Method: 采用Lindley和Stark基于⊤⊤提升的方法来证明强规范化性质，首先证明移除一般递归后整个演算的强规范化，然后证明在顺序片段中重新引入受控的中断驱动递归行为后仍保持强规范化。所有证明都在Agda中形式化验证。

Result: 1. 移除一般递归后，整个异步效应演算（包括顺序和并行部分）是强规范化的；2. 在顺序片段中重新引入受控的中断驱动递归行为后，该片段仍保持强规范化；3. 所有结果都在Agda中得到了形式化验证。

Conclusion: 该研究为异步效应演算提供了重要的规范化保证，证明了在适当限制递归行为的情况下，异步效应演算可以保持强规范化性质，这为异步效应系统的可靠性和可预测性提供了理论基础。

Abstract: Asynchronous effects of Ahman and Pretnar complement the conventional synchronous treatment of algebraic computational effects with asynchrony based on decoupling the execution of algebraic operation calls into signalling that an operation's implementation needs to be executed, and into interrupting a running computation with the operation's result, to which the computation can react by installing matching interrupt handlers. Beyond providing asynchrony for algebraic effects, the resulting core calculus also naturally models examples such as pre-emptive multi-threading, (cancellable) remote function calls, multi-party applications, and even a parallel variant of runners of algebraic effects. In this paper, we study the normalisation properties of this calculus. We prove that if one removes general recursion from the original calculus, then the remaining calculus is strongly normalising, including both its sequential and parallel parts. However, this only guarantees termination for very simple asynchronous examples. To improve on this result, we also prove that the sequential fragment of the calculus remains strongly normalising when a controlled amount of interrupt-driven recursive behaviour is reintroduced. Our strong normalisation proofs are structured compositionally as a natural extension of Lindley and Stark's $\top\top$-lifting based approach for proving strong normalisation of effectful languages. All our results are also formalised in Agda.

</details>


### [3] [An Equational Axiomatization of Dynamic Threads via Algebraic Effects: Presheaves on Finite Relations, Labelled Posets, and Parameterized Algebraic Theories](https://arxiv.org/abs/2602.05850)
*Ohad Kammar,Jack Liell-Cock,Sam Lindley,Cristina Matache,Sam Staton*

Main category: cs.PL

TL;DR: 使用代数效应理论为动态线程提供完整的等式公理化，基于参数化代数理论，以fork和wait为基本原语，实现模型完备性和语法完备性。


<details>
  <summary>Details</summary>
Motivation: 为动态线程提供形式化的等式公理化，使用代数效应理论来简化并发程序的语义分析，专注于fork和wait等代数操作。

Method: 基于参数化代数理论，构建以fork和wait为基本原语的代数理论，包含基本原子操作和结合律等定律，为并发编程语言提供操作语义和指称语义。

Result: 实现了两个完备性：1) 模型完备性：闭表达式完全捕获标记偏序集(pomsets)的相等性；2) 语法完备性：开表达式在所有闭化替换下相等时可证明相等。指称语义在标记偏序集观察下是健全、充分且一阶完全抽象的。

Conclusion: 代数效应理论为动态线程提供了有效的等式公理化框架，参数化代数理论是处理名称和绑定的便捷工具，能够为并发编程语言提供形式化的语义基础。

Abstract: We use the theory of algebraic effects to give a complete equational axiomatization for dynamic threads. Our method is based on parameterized algebraic theories, which give a concrete syntax for strong monads on functor categories, and are a convenient framework for names and binding. Our programs are built from the key primitives `fork' and `wait'. `Fork' creates a child thread and passes its name (thread ID) to the parent thread. `Wait' allows us to wait for given child threads to finish. We provide a parameterized algebraic theory built from fork and wait, together with basic atomic actions and laws such as associativity of `fork'. Our equational axiomatization is complete in two senses. First, for closed expressions, it completely captures equality of labelled posets (pomsets), an established model of concurrency: model complete. Second, any two open expressions are provably equal if they are equal under all closing substitutions: syntactically complete. The benefit of algebraic effects is that the semantic analysis can focus on the algebraic operations of fork and wait. We then extend the analysis to a simple concurrent programming language by giving operational and denotational semantics. The denotational semantics is built using the methods of parameterized algebraic theories and we show that it is sound, adequate, and fully abstract at first order for labelled-poset observations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [A novel scalable high performance diffusion solver for multiscale cell simulations](https://arxiv.org/abs/2602.05017)
*Jose-Luis Estragues-Muñoz,Carlos Alvarez,Arnau Montagud,Daniel Jimenez-Gonzalez,Alfonso Valencia*

Main category: cs.DC

TL;DR: 提出了一种可扩展的高性能计算解决方案BioFVM，用于分子扩散建模，相比现有方法实现了近200倍加速和36%内存使用减少。


<details>
  <summary>Details</summary>
Motivation: 基于代理的细胞模型在模拟组织演化时需要处理个体细胞行为、细胞间相互作用和微环境响应。将细胞分辨率模型扩展到真实规模的肿瘤模拟是数字孪生疾病模型发展的关键挑战，这需要高性能计算，因为每个时间步涉及数万亿次操作。

Method: 开发了一个可扩展的高性能计算解决方案，采用有限体积法框架的高效实现，系统地评估了新型可扩展生物有限体积法库BioFVM，并对现有解决方案进行了广泛的性能分析。

Result: 提出的高性能计算方案相比当前最先进解决方案实现了近200倍的加速，内存使用减少了高达36%，为高效计算下一代生物问题铺平了道路。

Conclusion: BioFVM库为大规模生物模拟提供了高效的可扩展解决方案，显著提升了计算性能和内存效率，有助于推进数字孪生疾病模型的发展。

Abstract: Agent-based cellular models simulate tissue evolution by capturing the behavior of individual cells, their interactions with neighboring cells, and their responses to the surrounding microenvironment. An important challenge in the field is scaling cellular resolution models to real-scale tumor simulations, which is critical for the development of digital twin models of diseases and requires the use of High-Performance Computing (HPC) since every time step involves trillions of operations. We hereby present a scalable HPC solution for the molecular diffusion modeling using an efficient implementation of state-of-the-art Finite Volume Method (FVM) frameworks. The paper systematically evaluates a novel scalable Biological Finite Volume Method (BioFVM) library and presents an extensive performance analysis of the available solutions. Results shows that our HPC proposal reach almost 200x speedup and up to 36% reduction in memory usage over the current state-of-the-art solutions, paving the way to efficiently compute the next generation of biological problems.

</details>


### [5] [Towards Advancing Research with Workflows: A perspective from the Workflows Community Summit -- Amsterdam, 2025](https://arxiv.org/abs/2602.05131)
*Irene Bonati,Silvina Caino-Lores,Tainã Coleman,Sagar Dolas,Sandro Fiore,Venkatesh Kannan,Marco Verdicchio,Sean R. Wilkinson,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 2025年科学工作流社区峰会识别了工作流采用的关键障碍，并提出技术、政策和社区层面的行动方案，以推动工作流在科学研究中的应用。


<details>
  <summary>Details</summary>
Motivation: 科学工作流在现代研究中对于协调分布式计算、管理大数据和确保可重复性变得至关重要，但存在采用障碍。2025年工作流社区峰会旨在识别这些挑战并制定解决方案。

Method: 通过2025年6月6日在阿姆斯特丹举行的国际工作流社区峰会，汇集专家讨论新兴挑战和机遇，识别关键障碍并提出行动方案。

Result: 识别了四大关键障碍：系统通用性与领域特定性之间的张力、工作流系统长期可持续性问题、工作流开发者缺乏认可、标准化/资金/培训/跨学科合作不足。提出了技术、政策和社区层面的行动方案。

Conclusion: 需要从单纯计算性能评估转向科学影响力衡量，建立正式工作流模式和社区基准，培养国际工作流社区，投资人力资本，以通过工作流推动科学发现。

Abstract: Scientific workflows have become essential for orchestrating complex computational processes across distributed resources, managing large datasets, and ensuring reproducibility in modern research. The Workflows Community Summit 2025, held in Amsterdam on June 6th, 2025, convened international experts to examine emerging challenges and opportunities in this domain. Participants identified key barriers to workflow adoption, including tensions between system generality and domain-specific utility, concerns over long-term sustainability of workflow systems and services, insufficient recognition for those who develop and maintain reproducible workflows, and gaps in standardization, funding, training, and cross-disciplinary collaboration. To address these challenges, the summit proposed action lines spanning technology, policy, and community dimensions: shifting evaluation metrics from raw computational performance toward measuring genuine scientific impact; formalizing workflow patterns and community-driven benchmarks to improve transparency, reproducibility, and usability; cultivating a cohesive international workflows community that engages funding bodies and research stakeholders; and investing in human capital through dedicated workflow engineering roles, career pathways, and integration of workflow concepts into educational curricula and long-term training initiatives. This document presents the summit's findings, beginning with an overview of the current computing ecosystem and the rationale for workflow-centric approaches, followed by a discussion of identified challenges and recommended action lines for advancing scientific discovery through workflows.

</details>


### [6] [ORACL: Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices](https://arxiv.org/abs/2602.05292)
*Haoyu Bai,Muhammed Tawfiqul Islam,Minxian Xu,Rajkumar Buyya*

Main category: cs.DC

TL;DR: ORACL是一个利用大语言模型进行微服务资源自动扩缩的框架，通过思维链推理诊断性能问题并推荐资源分配，无需针对特定部署进行训练。


<details>
  <summary>Details</summary>
Motivation: 当前微服务和Serverless架构中，自动扩缩策略要么是需要大量训练的不透明学习模型，要么是难以泛化的手工调优规则，缺乏能够快速适应不断演化的微服务部署的通用解决方案。

Method: ORACL将运行时遥测数据（pod、副本、CPU/内存使用率、延迟、SLO、故障信号）转换为语义自然语言状态描述，利用LLM的思维链推理生成可解释的中间推理轨迹，识别根本原因、剪枝动作空间，并在策略约束下做出安全的分配决策。

Result: 在代表性开源微服务工作负载上的实验表明，ORACL将根本原因识别准确率提高15%，训练速度提升高达24倍，短期场景下服务质量改善6%，且无需针对特定部署进行重新训练。

Conclusion: 大语言模型可以作为通用的少样本资源分配器，通过思维链推理适应快速演化的微服务部署，提供可解释、高效的自动扩缩解决方案。

Abstract: Applications are moving away from monolithic designs to microservice and serverless architectures, where fleets of lightweight and independently deployable components run on public clouds. Autoscaling serves as the primary control mechanism for balancing resource utilization and quality of service, yet existing policies are either opaque learned models that require substantial per-deployment training or brittle hand-tuned rules that fail to generalize. We investigate whether large language models can act as universal few-shot resource allocators that adapt across rapidly evolving microservice deployments.
  We propose ORACL, Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices, a framework that leverages prior knowledge and chain-of-thought reasoning to diagnose performance regressions and recommend resource allocations. ORACL transforms runtime telemetry, including pods, replicas, CPU and memory usage, latency, service-level objectives, and fault signals, into semantic natural-language state descriptions and invokes an LLM to produce an interpretable intermediate reasoning trace. This reasoning identifies likely root causes, prunes the action space, and issues safe allocation decisions under policy constraints. Experiments on representative open-source microservice workloads show that ORACL improves root-cause identification accuracy by 15 percent, accelerates training by up to 24x, and improves quality of service by 6 percent in short-term scenarios, without deployment-specific retraining.

</details>


### [7] [Proteus: Append-Only Ledgers for (Mostly) Trusted Execution Environments](https://arxiv.org/abs/2602.05346)
*Shubham Mishra,João Gonçalves,Chawinphat Tankuranand,Neil Giridharan,Natacha Crooks,Heidi Howard,Chris Jensen*

Main category: cs.DC

TL;DR: Proteus是一种新的分布式共识协议，通过将BFT协议嵌入到CFT协议中，在保持高性能的同时增强了对TEE硬件攻击的抵御能力。


<details>
  <summary>Details</summary>
Motivation: 分布式账本依赖硬件可信执行环境(TEEs)提供高可用性，但TEEs可能受到攻击，这会破坏分布式账本的设计保证。需要一种既能利用TEEs优势又能抵御其潜在攻击的解决方案。

Method: Proteus协议谨慎地信任TEEs保证，通过重构CFT和BFT协议使它们的结构对齐，将BFT协议嵌入到CFT协议中，无需额外消息。

Result: Proteus在保持与常规TEE增强共识协议相当性能的同时，能够在TEE平台被攻破时保证数据完整性。

Conclusion: Proteus提供了一种平衡方法，既利用TEEs的性能优势，又通过BFT机制增强对硬件攻击的抵御能力，为分布式账本提供了更强的安全保障。

Abstract: Distributed ledgers are increasingly relied upon by industry to provide trustworthy accountability, strong integrity protection, and high availability for critical data without centralizing trust. Recently, distributed append-only logs are opting for a layered approach, combining crash-fault-tolerant (CFT) consensus with hardware-based Trusted Execution Environments (TEEs) for greater resiliency. Unfortunately, hardware TEEs can be subject to (rare) attacks, undermining the very guarantees that distributed ledgers are carefully designed to achieve. In response, we present Proteus, a new distributed consensus protocol that cautiously trusts the guarantees of TEEs. Proteus carefully embeds a Byzantine fault-tolerant (BFT) protocol inside of a CFT protocol with no additional messages. This is made possible through careful refactoring of both the CFT and BFT protocols such that their structure aligns. Proteus achieves performance in line with regular TEE-enabled consensus protocols, while guaranteeing integrity in the face of TEE platform compromises.

</details>


### [8] [Reaching Univalency with Subquadratic Communication](https://arxiv.org/abs/2602.05356)
*Andrew Lewis-Pye*

Main category: cs.DC

TL;DR: 论文证明了Dolev-Reischuk下界中的二次通信成本主要来自结果传播阶段，而非达成一致性决策阶段。通过引入ε-BA松弛协议，展示了在达成一致性决策阶段只需O(n log n)通信复杂度。


<details>
  <summary>Details</summary>
Motivation: Dolev-Reischuk下界表明拜占庭容错协议需要Ω(f²+n)消息，但这一二次成本的具体来源不明确。论文旨在探究这一成本是来自达成一致性决策的困难，还是仅仅来自将结果传播给所有处理器的过程。

Method: 引入ε-BA松弛协议，允许ε比例的正确处理器输出错误结果。证明当f < n(1/3 - ε)时，ε-BA可以用O(n log n)通信复杂度确定性解决。任何ε-BA协议都可以作为完整BA协议的第一阶段，第二阶段通过单次全交换和多数投票完成BA。

Result: 达成一致性决策（univalency）不需要二次通信，ε-BA可以在O(n log n)通信复杂度下解决。完整的BA协议中，二次成本完全来自结果传播阶段，而非决策阶段。在认证设置中，Extractable BA可以用O(f log f)通信复杂度解决。

Conclusion: Dolev-Reischuk下界中的二次通信成本完全来自结果传播需求，而非达成一致性决策的难度。这一发现澄清了拜占庭协议通信复杂性的本质，为设计更高效的协议提供了理论基础。

Abstract: The Dolev-Reischuk lower bound establishes that any deterministic Byzantine Agreement (BA) protocol for $n$ processors tolerating $f$ faults requires $Ω(f^2+n)$ messages. But what exactly does this quadratic cost pay for? Even the minimal requirement that every correct processor \emph{receive at least one message} already necessitates $Ω(f^2 + n)$ messages. This raises a fundamental question: is the Dolev-Reischuk bound about the difficulty of \emph{reaching univalency} -- the point at which the protocol's outcome is determined -- or merely about \emph{disseminating} the outcome to all processors afterward?
  We resolve this question by showing that reaching univalency does \emph{not} require quadratic communication. Specifically, we introduce $ε$-BA, a relaxation allowing an $ε$-fraction of correct processors to output incorrectly, and prove it can be solved deterministically with $O(n \log n)$ communication complexity when $f < n(1/3 - ε)$. Crucially, any $ε$-BA protocol can serve as the first phase of a full BA protocol: after $ε$-BA, a single all-to-all exchange and majority vote completes BA. Since the outcome is already determined after $ε$-BA, this demonstrates that the quadratic cost in Dolev-Reischuk stems entirely from dissemination, rather than from reaching univalency. We also define Extractable BA for authenticated settings, capturing when processors collectively hold enough signed messages to determine the agreed value, and show it can be solved with communication complexity $O(f \log f)$.

</details>


### [9] [TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism](https://arxiv.org/abs/2602.05754)
*Seonghye Cho,Jaemin Han,Hyunjin Kim,Euisoo Jung,Jae-Gil Lee*

Main category: cs.DC

TL;DR: TimelyFreeze通过建模流水线调度为有向无环图并求解线性规划，在精度约束下计算最优冻结比例，减少流水线气泡，提升训练吞吐量40%


<details>
  <summary>Details</summary>
Motivation: 流水线并行虽然能训练超出单设备内存的模型，但实际吞吐量受限于流水线气泡。现有参数冻结方法虽然能通过自适应跳过反向计算提高训练吞吐量，但往往过度冻结参数，导致不必要的精度下降。

Method: 将流水线调度建模为有向无环图，通过求解线性规划问题来计算最优的冻结比例，在满足精度约束的条件下最小化批次执行时间。

Result: 在LLaMA-8B模型上实现了高达40%的训练吞吐量提升，同时保持可比的精度。能够在不影响收敛的情况下加速大规模模型训练，并适用于多种流水线并行设置。

Conclusion: TimelyFreeze通过优化参数冻结策略，有效解决了流水线并行中的气泡问题，实现了训练吞吐量的显著提升而不损害模型精度，为大规模模型训练提供了高效解决方案。

Abstract: Pipeline parallelism enables training models that exceed single-device memory, but practical throughput remains limited by pipeline bubbles. Although parameter freezing can improve training throughput by adaptively skipping backward computation, existing methods often over-freeze parameters, resulting in unnecessary accuracy degradation. To address this issue, we propose TimelyFreeze, which models the pipeline schedule as a directed acyclic graph and solves a linear program to compute optimal freeze ratios that minimize batch execution time under accuracy constraints. Experiments show that TimelyFreeze achieves up to 40% training throughput improvement on LLaMA-8B with comparable accuracy. Overall, it enables faster large-scale model training without compromising convergence and generalizes across diverse pipeline-parallel settings.

</details>


### [10] [Location-Aware Dispersion on Anonymous Graphs](https://arxiv.org/abs/2602.05948)
*Himani,Supantha Pandit,Gokarna Sharma*

Main category: cs.DC

TL;DR: 本文提出了位置感知分散问题（LOCATION-AWARE DISPERSION），这是经典分散问题的新扩展，要求机器人根据颜色匹配移动到相同颜色的节点上，当颜色集大小为1时退化为传统分散问题。


<details>
  <summary>Details</summary>
Motivation: 传统分散问题只要求机器人占据不同节点，不考虑节点属性。实际应用中，机器人可能需要根据位置特征（如颜色标签）进行分配。位置感知分散问题引入了颜色匹配约束，使问题更具实际意义和挑战性。

Method: 提出了几种确定性算法，保证时间和内存需求的界限。同时给出了位置感知分散问题的不可行性证明和确定性算法的下界分析。

Result: 建立了位置感知分散问题在匿名网络中的算法可行性，同时揭示了相比传统分散问题解决方案，获得高效解所面临的挑战。

Conclusion: 位置感知分散问题是分散问题的有意义扩展，虽然算法上可行，但相比传统分散问题需要更复杂的解决方案，时间和内存效率方面面临更大挑战。

Abstract: The well-studied DISPERSION problem is a fundamental coordination problem in distributed robotics, where a set of mobile robots must relocate so that each occupies a distinct node of a network. DISPERSION assumes that a robot can settle at any node as long as no other robot settles on that node. In this work, we introduce LOCATION-AWARE DISPERSION, a novel generalization of DISPERSION that incorporates location awareness: Let $G = (V, E)$ be an anonymous, connected, undirected graph with $n = |V|$ nodes, each labeled with a color $\sf{col}(v) \in C = \{c_1, \dots, c_t\}, t\leq n$. A set $R = \{r_1, \dots, r_k\}$ of $k \leq n$ mobile robots is given, where each robot $r_i$ has an associated color $\mathsf{col}(r_i) \in C$. Initially placed arbitrarily on the graph, the goal is to relocate the robots so that each occupies a distinct node of the same color. When $|C|=1$, LOCATION-AWARE DISPERSION reduces to DISPERSION. There is a solution to DISPERSION in graphs with any $k\leq n$ without knowing $k,n$.
  Like DISPERSION, the goal is to solve LOCATION-AWARE DISPERSION minimizing both time and memory requirement at each agent. We develop several deterministic algorithms with guaranteed bounds on both time and memory requirement. We also give an impossibility and a lower bound for any deterministic algorithm for LOCATION-AWARE DISPERSION. To the best of our knowledge, the presented results collectively establish the algorithmic feasibility of LOCATION-AWARE DISPERSION in anonymous networks and also highlight the challenges on getting an efficient solution compared to the solutions for DISPERSION.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [CVA6-CFI: A First Glance at RISC-V Control-Flow Integrity Extensions](https://arxiv.org/abs/2602.04991)
*Simone Manoni,Emanuele Parisi,Riccardo Tedeschi,Davide Rossi,Andrea Acquaviva,Andrea Bartolini*

Main category: cs.AR

TL;DR: 首次设计、集成并评估了RISC-V控制流完整性扩展标准Zicfiss和Zicfilp，通过硬件实现影子栈和着陆垫机制保护程序免受控制流劫持攻击，在CVA6核心中集成仅带来1.0%面积开销和最高15.6%性能开销。


<details>
  <summary>Details</summary>
Motivation: 保护易受攻击程序免受控制流劫持攻击，通过硬件实现控制流完整性机制，为RISC-V架构提供标准化的安全扩展。

Method: 设计了两个独立可配置的硬件单元：Zicfiss（影子栈）用于后向边保护，Zicfilp（着陆垫）用于前向边保护，完全集成到开源CVA6核心中。

Result: 在22nm FDX工艺下仅增加1.0%的面积开销，使用MiBench汽车基准测试子集评估显示最高15.6%的性能开销，完整实现已开源。

Conclusion: 成功实现了首个RISC-V控制流完整性扩展标准，以较小的硬件开销提供了有效的控制流保护，为RISC-V生态系统贡献了重要的安全增强。

Abstract: This work presents the first design, integration, and evaluation of the standard RISC-V extensions for Control-Flow Integrity (CFI). The Zicfiss and Zicfilp extensions aim at protecting the execution of a vulnerable program from control-flow hijacking attacks through the implementation of security mechanisms based on shadow stack and landing pad primitives. We introduce two independent and configurable hardware units implementing forward-edge and backward-edge control-flow protection, fully integrated into the open-source CVA6 core. Our design incurs in only 1.0% area overhead when synthesized in 22 nm FDX technology, and up to 15.6% performance overhead based on evaluation with the MiBench automotive benchmark subset. We release the complete implementation as open source.

</details>


### [12] [COFFEE: A Carbon-Modeling and Optimization Framework for HZO-based FeFET eNVMs](https://arxiv.org/abs/2602.05018)
*Hongbang Wu,Xuesi Chen,Shubham Jadhav,Amit Lal,Lillian Pentecost,Udit Gupta*

Main category: cs.AR

TL;DR: COFFEE是首个针对HZO基FeFET eNVM的碳建模框架，涵盖硬件制造（体现碳）到使用阶段（运营碳）的全生命周期分析，显示相比SRAM可显著降低碳足迹。


<details>
  <summary>Details</summary>
Motivation: 信息和通信技术对全球环境影响日益增长，新兴非易失性存储器（eNVM）虽能提高能效，但其端到端碳足迹尚不明确。理解硬件系统全生命周期环境影响是实现可持续计算的第一步。

Method: 提出COFFEE框架，基于真实半导体工厂数据和器件制造工艺估算体现碳，利用架构级eNVM设计空间探索工具量化使用阶段性能和能耗，以HZO基FeFET为例进行详细研究。

Result: 2MB容量下，HZO-FeFET的单位面积体现碳比CMOS基线高11%，但每MB体现碳始终比SRAM低4.3倍。边缘ML加速器案例中，用HZO-FeFET eNVM替换SRAM权重缓冲器，体现碳减少42.3%，运营碳最多减少70%。

Conclusion: COFFEE框架为eNVM碳足迹评估提供了系统方法，证明HZO基FeFET eNVM在降低碳足迹方面具有显著潜力，有助于推动可持续计算发展。

Abstract: Information and communication technologies account for a growing portion of global environmental impacts. While emerging technologies, such as emerging non-volatile memories (eNVM), offer a promising solution to energy efficient computing, their end-to-end footprint is not well understood. Understanding the environmental impact of hardware systems over their life cycle is the first step to realizing sustainable computing. This work conducts a detailed study of one example eNVM device: hafnium-zirconium-oxide (HZO)-based ferroelectric field-effect transistors (FeFETs). We present COFFEE, the first carbon modeling framework for HZO-based FeFET eNVMs across life cycle, from hardware manufacturing (embodied carbon) to use (operational carbon). COFFEE builds on data gathered from a real semiconductor fab and device fabrication recipes to estimate embodied carbon, and architecture level eNVM design space exploration tools to quantify use-phase performance and energy. Our evaluation shows that, at 2 MB capacity, the embodied carbon per unit area overhead of HZO-FeFETs can be up to 11% higher than the CMOS baseline, while the embodied carbon per MB remains consistently about 4.3x lower than SRAM across different memory capacity. A further case study applies COFFEE to an edge ML accelerator, showing that replacing the SRAM-based weight buffer with HZO-based FeFET eNVMs reduces embodied carbon by 42.3% and operational carbon by up to 70%.

</details>


### [13] [Balancing FP8 Computation Accuracy and Efficiency on Digital CIM via Shift-Aware On-the-fly Aligned-Mantissa Bitwidth Prediction](https://arxiv.org/abs/2602.05743)
*Liang Zhao,Kunming Shao,Zhipeng Liao,Xijie Huang,Tim Kwang-Ting Cheng,Chi-Ying Tsui,Yi Zou*

Main category: cs.AR

TL;DR: 提出一种灵活的FP8数字存内计算加速器，通过动态位宽预测、FIFO对齐单元和可扩展INT MAC阵列，支持所有FP8格式，在28nm工艺下实现20.4 TFLOPS/W能效，比现有工作高2.8倍。


<details>
  <summary>Details</summary>
Motivation: 现有数字存内计算架构难以支持可变FP8对齐尾数位宽，统一对齐策略和固定精度乘累加单元无法处理分布多样的输入数据，限制了FP8格式在Transformer推理和训练中的应用。

Method: 1) 动态移位感知位宽预测(DSBP)：在线预测输入，自适应调整权重(2/4/6/8b)和输入(2~12b)对齐尾数精度；2) FIFO输入对齐单元(FIAU)：用基于指针的控制替代复杂桶形移位器；3) 精度可扩展INT MAC阵列：以最小开销实现灵活权重精度。

Result: 28nm CMOS工艺下实现64×96存内计算阵列，固定E5M7格式达到20.4 TFLOPS/W能效，FP8效率比先前工作高2.8倍，支持所有FP8格式。在Llama-7b模型上，DSBP在BoolQ和Winogrande数据集上比固定位宽模式在相同精度水平下获得更高效率。

Conclusion: 该工作提出的灵活FP8数字存内计算加速器通过创新的动态位宽预测、高效对齐单元和可扩展计算架构，成功解决了可变FP8格式支持问题，实现了显著的能效提升和灵活的精度-效率权衡。

Abstract: FP8 low-precision formats have gained significant adoption in Transformer inference and training. However, existing digital compute-in-memory (DCIM) architectures face challenges in supporting variable FP8 aligned-mantissa bitwidths, as unified alignment strategies and fixed-precision multiply-accumulate (MAC) units struggle to handle input data with diverse distributions. This work presents a flexible FP8 DCIM accelerator with three innovations: (1) a dynamic shift-aware bitwidth prediction (DSBP) with on-the-fly input prediction that adaptively adjusts weight (2/4/6/8b) and input (2$\sim$12b) aligned-mantissa precision; (2) a FIFO-based input alignment unit (FIAU) replacing complex barrel shifters with pointer-based control; and (3) a precision-scalable INT MAC array achieving flexible weight precision with minimal overhead. Implemented in 28nm CMOS with a 64$\times$96 CIM array, the design achieves 20.4 TFLOPS/W for fixed E5M7, demonstrating 2.8$\times$ higher FP8 efficiency than previous work while supporting all FP8 formats. Results on Llama-7b show that the DSBP achieves higher efficiency than fixed bitwidth mode at the same accuracy level on both BoolQ and Winogrande datasets, with configurable parameters enabling flexible accuracy-efficiency trade-offs.

</details>
