<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 12]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Permuting Transactions in Ethereum Blocks: An Empirical Study](https://arxiv.org/abs/2509.25415)
*Jan Droll*

Main category: cs.DC

TL;DR: 研究分析了以太坊随机交易排序的技术可行性，发现约22%的区块排列因协议违规而无效，主要涉及私有挖矿交易和接近gas限制的区块，但大部分区块的gas消耗偏差不超过10%。


<details>
  <summary>Details</summary>
Motivation: 评估随机交易排序在以太坊生态系统中的技术可部署性，以缓解中心化效应和提高公平性，同时考虑gas限制和协议规则的影响。

Method: 对超过335,000个以太坊主网区块的交易进行多次排列和执行，量化协议违规、执行错误和gas消耗偏差。

Result: 22%的区块排列因协议违规而无效；几乎所有在排列中显示执行错误但在原始顺序中正常的交易都是私有挖矿交易；仅6%的交易显示gas消耗偏差，98%的区块排列gas消耗偏差不超过10%。

Conclusion: 从技术角度看，如果交易选择处理得当，随机交易排序可能是可行的。

Abstract: Several recent proposals implicitly or explicitly suggest making use of
randomized transaction ordering within a block to mitigate centralization
effects and to improve fairness in the Ethereum ecosystem. However,
transactions and blocks are subject to gas limits and protocol rules. In a
randomized transaction order, the behavior of transactions may change depending
on other transactions in the same block, leading to invalid blocks and varying
gas consumptions. In this paper, we quantify and characterize protocol
violations, execution errors and deviations in gas consumption of blocks and
transactions to examine technical deployability. For that, we permute and
execute the transactions of over 335,000 Ethereum Mainnet blocks multiple
times. About 22% of block permutations are invalid due to protocol violations
caused by privately mined transactions or blocks close to their gas limit.
Also, almost all transactions which show execution errors under permutation but
not in the original order are privately mined transactions. Only 6% of
transactions show deviations in gas consumption and 98% of block permutations
deviate at most 10% from their original gas consumption. From a technical
perspective, these results suggest that randomized transaction ordering may be
feasible if transaction selection is handled carefully.

</details>


### [2] [Enabling Time-Aware Priority Traffic Management over Distributed FPGA Nodes](https://arxiv.org/abs/2509.26043)
*Alberto Scionti,Paolo Savio,Francesco Lubrano,Federico Stirano,Antonino Nespola,Olivier Terzo,Corrado De Sio,Luca Sterpone*

Main category: cs.DC

TL;DR: 本文扩展了开源NIC实现Corundum的功能，在硬件层面实现了时间感知的流量管理，通过控制不同优先级队列的传输带宽来管理流量类别。


<details>
  <summary>Details</summary>
Motivation: 现代网络接口卡(NIC)已从简单的网络流量传输设备演变为复杂的异构系统，能够卸载主机CPU的复杂任务。FPGA也从纯可编程设备发展为包含通用处理器和专用引擎的异构系统。作者旨在利用现代FPGA实现智能NIC，并通过硬件级时间感知流量管理来精确控制不同流量类别的带宽分配。

Method: 扩展Corundum开源NIC实现，在AXI总线上暴露专用控制寄存器，使NIC驱动程序能够配置不同优先级队列的传输带宽。每个控制寄存器对应特定传输队列，设置队列在传输窗口内获得输出端口访问权限的时间比例。通过Linux QDISC机制将队列与不同流量类别关联并设置优先级。

Result: 实验评估表明，该方法能够有效管理不同传输流预留的带宽，实现了对多队列传输的精确带宽控制。

Conclusion: 通过在硬件层面实现时间感知的流量管理，成功扩展了Corundum NIC的功能，为不同流量类别提供了精确的带宽控制能力，证明了现代FPGA在实现智能NIC方面的可行性。

Abstract: Network Interface Cards (NICs) greatly evolved from simple basic devices
moving traffic in and out of the network to complex heterogeneous systems
offloading host CPUs from performing complex tasks on in-transit packets. These
latter comprise different types of devices, ranging from NICs accelerating
fixed specific functions (e.g., on-the-fly data compression/decompression,
checksum computation, data encryption, etc.) to complex Systems-on-Chip (SoC)
equipped with both general purpose processors and specialized engines
(Smart-NICs). Similarly, Field Programmable Gate Arrays (FPGAs) moved from pure
reprogrammable devices to modern heterogeneous systems comprising
general-purpose processors, real-time cores and even AI-oriented engines.
Furthermore, the availability of high-speed network interfaces (e.g., SFPs)
makes modern FPGAs a good choice for implementing Smart-NICs. In this work, we
extended the functionalities offered by an open-source NIC implementation
(Corundum) by enabling time-aware traffic management in hardware, and using
this feature to control the bandwidth associated with different traffic
classes. By exposing dedicated control registers on the AXI bus, the driver of
the NIC can easily configure the transmission bandwidth of different
prioritized queues. Basically, each control register is associated with a
specific transmission queue (Corundum can expose up to thousands of
transmission and receiving queues), and sets up the fraction of time in a
transmission window which the queue is supposed to get access the output port
and transmit the packets. Queues are then prioritized and associated to
different traffic classes through the Linux QDISC mechanism. Experimental
evaluation demonstrates that the approach allows to properly manage the
bandwidth reserved to the different transmission flows.

</details>


### [3] [Enhancing Split Learning with Sharded and Blockchain-Enabled SplitFed Approaches](https://arxiv.org/abs/2509.25555)
*Amirreza Sokhankhosh,Khalid Hassan,Sara Rouhani*

Main category: cs.DC

TL;DR: 提出两种新颖框架SSFL和BSFL，解决SplitFed Learning的可扩展性、性能和安全性问题。SSFL通过分片技术提升性能85.2%，BSFL结合区块链技术增强安全性，抵御数据中毒攻击62.7%。


<details>
  <summary>Details</summary>
Motivation: 联邦学习(FL)和分割学习(SL)在隐私关键领域有应用前景，但FL计算需求大，SL训练时间长。SplitFed Learning(SFL)结合两者优势但仍存在可扩展性、性能和安全性问题。

Method: SSFL通过将SL服务器工作负载和通信开销分布到多个并行分片来解决可扩展性问题；BSFL在SSFL基础上用基于区块链的架构替代集中式服务器，采用委员会驱动的共识机制，包含评估机制排除中毒模型更新。

Result: 实验评估显示SSFL相比基线SL和SFL方法，性能和可扩展性分别提升31.2%和85.2%；BSFL对数据中毒攻击的抵御能力提升62.7%，在正常操作条件下保持优越性能。

Conclusion: BSFL是首个实现端到端去中心化SplitFed Learning系统的区块链赋能框架，有效解决了SFL的可扩展性、性能和安全性限制。

Abstract: Collaborative and distributed learning techniques, such as Federated Learning
(FL) and Split Learning (SL), hold significant promise for leveraging sensitive
data in privacy-critical domains. However, FL and SL suffer from key
limitations -- FL imposes substantial computational demands on clients, while
SL leads to prolonged training times. To overcome these challenges, SplitFed
Learning (SFL) was introduced as a hybrid approach that combines the strengths
of FL and SL. Despite its advantages, SFL inherits scalability, performance,
and security issues from SL. In this paper, we propose two novel frameworks:
Sharded SplitFed Learning (SSFL) and Blockchain-enabled SplitFed Learning
(BSFL). SSFL addresses the scalability and performance constraints of SFL by
distributing the workload and communication overhead of the SL server across
multiple parallel shards. Building upon SSFL, BSFL replaces the centralized
server with a blockchain-based architecture that employs a committee-driven
consensus mechanism to enhance fairness and security. BSFL incorporates an
evaluation mechanism to exclude poisoned or tampered model updates, thereby
mitigating data poisoning and model integrity attacks. Experimental evaluations
against baseline SL and SFL approaches show that SSFL improves performance and
scalability by 31.2% and 85.2%, respectively. Furthermore, BSFL increases
resilience to data poisoning attacks by 62.7% while maintaining superior
performance under normal operating conditions. To the best of our knowledge,
BSFL is the first blockchain-enabled framework to implement an end-to-end
decentralized SplitFed Learning system.

</details>


### [4] [LAPIS: A Performance Portable, High Productivity Compiler Framework](https://arxiv.org/abs/2509.25605)
*Brian Kelley,Sivasankaran Rajamanickam*

Main category: cs.DC

TL;DR: LAPIS是一个基于MLIR的编译器，解决了编程模型在可移植性、性能和生产力三个维度上的挑战，能够自动降低计算科学和AI中的稀疏和稠密线性代数内核，并促进PyTorch和Kokkos之间的代码集成。


<details>
  <summary>Details</summary>
Motivation: 当前编程模型在可移植性、性能和生产力这三个关键维度上存在不足，计算科学和机器学习领域的不同框架需要手动集成代码，且现有框架缺乏对新计算机架构的易扩展性。

Method: 开发基于MLIR的LAPIS编译器，创建一个基于Kokkos生态系统原则的方言，支持自动降低稀疏和稠密线性代数内核，并实现PyTorch和Kokkos之间的代码集成。

Result: LAPIS能够在多种架构上实现可移植性，其内核性能与默认MLIR实现相当，同时支持框架向新架构的扩展。

Conclusion: LAPIS成功解决了编程模型在三个关键维度上的挑战，为计算科学和机器学习的融合提供了有效的编译器解决方案。

Abstract: Portability, performance, and productivity are three critical dimensions for
evaluating a programming model or compiler infrastructure. Several modern
programming models for computational science focus on performance and
portability. On the other end, several machine learning focused programming
models focus on portability and productivity. A clear solution that is strong
in all three dimensions has yet to emerge. A second related problem arises when
use cases from computational science converge with machine learning. The
disparate popular frameworks of these fields require programmers to manually
integrate codes written in different frameworks. Finally, several programming
frameworks lack easy options for extensibility as any new computer architecture
change require complex changes to the programming models. We present LAPIS, an
MLIR-based compiler that addresses all three of these challenges. We
demonstrate that LAPIS can automatically lower sparse and dense linear algebra
kernels from computational science and artificial intelligence use cases. We
also show how LAPIS facilitates the integration of codes between PyTorch and
Kokkos. We compare kernel performance with the default MLIR implementations on
diverse architectures to demonstrate portability. By developing a dialect that
is built on the principles of the Kokkos ecosystem, LAPIS also allows
extensibility of the framework to new architectures.

</details>


### [5] [PAST: Pilot and Adaptive Orchestration for Timely and Resilient Service Delivery in Edge-Assisted UAV Networks under Spatio-Temporal Dynamics](https://arxiv.org/abs/2509.25700)
*Houyi Qi,Minghui Liwang,Liqun Fu,Sai Zou,Xinlei Yi,Wei Ni,Huaiyu Dai*

Main category: cs.DC

TL;DR: PAST框架结合PilotAO和AdaptAO机制，通过风险感知的早期决策和智能自适应更新，在动态无人机-边缘网络中实现稳定灵活的资源交易。


<details>
  <summary>Details</summary>
Motivation: 传统现货交易存在协商延迟和高能耗问题，传统期货交易难以适应动态不确定的无人机-边缘环境，需要解决激励驱动的资源交易挑战。

Method: 提出PAST框架，包含PilotAO（带超额预订的风险感知早期决策模块）和AdaptAO（基于无人机移动性、供需变化和协议性能动态更新协议和超额预订率的智能适应模块）。

Result: 在真实数据集上的实验表明，PAST在决策开销、任务完成延迟、资源利用率和社会福利方面持续优于基准方法。

Conclusion: 通过结合预测规划和实时调整，PAST为提升低空任务性能提供了稳健自适应的实践参考。

Abstract: Incentive-driven resource trading is essential for UAV applications with
intensive, time-sensitive computing demands. Traditional spot trading suffers
from negotiation delays and high energy costs, while conventional futures
trading struggles to adapt to the dynamic, uncertain UAV-edge environment. To
address these challenges, we propose PAST (pilot-and-adaptive stable trading),
a novel framework for edge-assisted UAV networks with spatio-temporal dynamism.
PAST integrates two complementary mechanisms: PilotAO (pilot trading agreements
with overbooking), a risk-aware, overbooking-enabled early-stage
decision-making module that establishes long-term, mutually beneficial
agreements and boosts resource utilization; and AdaptAO (adaptive trading
agreements with overbooking rate update), an intelligent adaptation module that
dynamically updates agreements and overbooking rates based on UAV mobility,
supply-demand variations, and agreement performance. Together, these mechanisms
enable both stability and flexibility, guaranteeing individual rationality,
strong stability, competitive equilibrium, and weak Pareto optimality.
Extensive experiments on real-world datasets show that PAST consistently
outperforms benchmark methods in decision-making overhead, task completion
latency, resource utilization, and social welfare. By combining predictive
planning with real-time adjustments, PAST offers a valuable reference on robust
and adaptive practice for improving low-altitude mission performance.

</details>


### [6] [Accelerating LLM Inference with Precomputed Query Storage](https://arxiv.org/abs/2509.25919)
*Jay H. Park,Youngju Cho,Choungsol Lee,Moonwook Oh,Euiseong Seo*

Main category: cs.DC

TL;DR: StorInfer是一个存储辅助的LLM推理系统，通过预计算和存储可预测的查询-响应对来加速推理，在查询语义匹配时绕过GPU推理直接返回存储的响应，显著降低延迟和计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在资源受限环境（如设备端或边缘部署）中推理延迟高的问题，通过存储预计算结果来优化推理效率。

Method: 使用LLM驱动的生成器自适应生成多样化和去重的查询，采用自适应查询掩码防止相似查询再生，以及自适应采样动态调整生成参数以促进语义多样性。生成的查询-响应对通过磁盘支持的向量数据库进行嵌入和索引，实现快速的相似性检索。

Result: 生成了15万个独特的预计算对（占用830MB存储空间），实现了高达17.3%的延迟降低，且响应质量无损失。在多个QA数据集上的评估证明了该方法的实用性和可扩展性。

Conclusion: StorInfer展示了利用存储作为高效、低延迟LLM部署主要推动因素的有前景方向，特别适用于具有可预测查询分布的场景。

Abstract: Large language model (LLM) inference often suffers from high latency,
particularly in resource-constrained environments such as on-device or edge
deployments. To address this challenge, we present StorInfer, a novel
storage-assisted LLM inference system that accelerates response time by
precomputing and storing predictable query-response pairs offline. When a user
query semantically matches a precomputed query, StorInfer bypasses expensive
GPU inference and instantly returns the stored response, significantly reducing
latency and compute costs. To maximize coverage and effectiveness, StorInfer
employs an LLM-driven generator that adaptively produces diverse and
deduplicated queries based on a given knowledge base. This is achieved via two
techniques: adaptive query masking, which prevents regeneration of similar
queries, and adaptive sampling, which dynamically tunes generation parameters
to promote semantic diversity. The resulting query-response pairs are embedded
and indexed using a disk-backed vector database to enable fast,
similarity-based retrieval at runtime. Using this approach, we generated 150K
unique precomputed pairs (taking up to 830 MB of storage space), achieving up
to 17.3% latency reduction with no loss in response quality. Our evaluation
across multiple QA datasets demonstrates the practicality and scalability of
storage-assisted inference, especially in scenarios with predictable query
distributions. StorInfer highlights a promising direction in leveraging storage
as a primary enabler for efficient, low-latency LLM deployment.

</details>


### [7] [Efficient Distributed Training via Dual Batch Sizes and Cyclic Progressive Learning](https://arxiv.org/abs/2509.26092)
*Kuan-Wei Lu,Ding-Yong Hong,Pangfeng Liu,Jan-Jan Wu*

Main category: cs.DC

TL;DR: 提出了一种结合双批次大小学习和循环渐进学习的混合分布式训练方法，通过同时使用大、小批次大小来平衡训练效率和模型泛化能力，并使用渐进分辨率调整来减少时间开销。


<details>
  <summary>Details</summary>
Motivation: 当前分布式机器学习主要依赖增加硬件资源和大批次训练来加速，但大批次训练会导致泛化能力下降和准确率降低。需要一种方法既能保持训练效率又能提升模型泛化能力。

Method: 基于参数服务器框架的双批次大小学习方案：同时使用硬件支持的最大批次大小和较小批次大小；结合循环渐进学习方案：在训练过程中从低到高渐进调整图像分辨率。

Result: 在ResNet-18上的实验结果显示：在CIFAR-100上准确率提升3.3%，训练时间减少10.6%；在ImageNet上准确率提升0.1%，训练时间减少35.7%。

Conclusion: 该混合方法有效解决了大批次训练导致的泛化能力下降问题，在保持训练效率的同时显著提升了模型准确率，为分布式机器学习提供了有效的解决方案。

Abstract: Distributed machine learning is critical for training deep learning models on
large datasets and with numerous parameters. Current research primarily focuses
on leveraging additional hardware resources and powerful computing units to
accelerate the training process. As a result, larger batch sizes are often
employed to speed up training. However, training with large batch sizes can
lead to lower accuracy due to poor generalization. To address this issue, we
propose the dual batch size learning scheme, a distributed training method
built on the parameter server framework. This approach maximizes training
efficiency by utilizing the largest batch size that the hardware can support
while incorporating a smaller batch size to enhance model generalization. By
using two different batch sizes simultaneously, this method reduces testing
loss and enhances generalization, with minimal extra training time.
Additionally, to mitigate the time overhead caused by dual batch size learning,
we propose the cyclic progressive learning scheme. This technique gradually
adjusts image resolution from low to high during training, significantly
boosting training speed. By combining cyclic progressive learning with dual
batch size learning, our hybrid approach improves both model generalization and
training efficiency. Experimental results using ResNet-18 show that, compared
to conventional training methods, our method can improve accuracy by 3.3% while
reducing training time by 10.6% on CIFAR-100, and improve accuracy by 0.1%
while reducing training time by 35.7% on ImageNet.

</details>


### [8] [AGOCS -- Accurate Google Cloud Simulator Framework](https://arxiv.org/abs/2509.26120)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: AGOCS是一个基于真实Google集群工作负载轨迹的高保真云工作负载模拟器，可在桌面机器上用于日常研究，提供精确的作业、任务和节点参数分析。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够基于真实工作负载轨迹进行高保真云工作负载模拟的工具，方便研究人员在桌面环境中进行日常研究。

Method: 基于Google集群12.5K节点一个月的工作负载轨迹，使用Scala语言实现，注重并行执行和易于扩展的设计概念。

Result: 实现了能够揭示作业、任务和节点精确参数的框架，提供实际资源使用统计，并通过开源GitHub仓库提供。

Conclusion: AGOCS是一个实用的高保真云工作负载模拟器，为云研究提供了便利工具，并提出了未来性能增强的替代方法。

Abstract: This paper presents the Accurate Google Cloud Simulator (AGOCS) - a novel
high-fidelity Cloud workload simulator based on parsing real workload traces,
which can be conveniently used on a desktop machine for day-to-day research.
Our simulation is based on real-world workload traces from a Google Cluster
with 12.5K nodes, over a period of a calendar month. The framework is able to
reveal very precise and detailed parameters of the executed jobs, tasks and
nodes as well as to provide actual resource usage statistics. The system has
been implemented in Scala language with focus on parallel execution and an
easy-to-extend design concept. The paper presents the detailed structural
framework for AGOCS and discusses our main design decisions, whilst also
suggesting alternative and possibly performance enhancing future approaches.
The framework is available via the Open Source GitHub repository.

</details>


### [9] [Parallax: Efficient LLM Inference Service over Decentralized Environment](https://arxiv.org/abs/2509.26182)
*Chris Tong,Youhe Jiang,Gufeng Chen,Tianyi Zhao,Sibian Lu,Wenjie Qu,Eric Yang,Lynn Ai,Binhang Yuan*

Main category: cs.DC

TL;DR: Parallax是一个去中心化的LLM服务系统，通过两阶段调度器将异构GPU池转化为高效推理平台，解决了GPU异构性和网络带宽限制的挑战。


<details>
  <summary>Details</summary>
Motivation: 集中式LLM推理服务成本高昂，依赖专用GPU集群和高带宽互联。利用去中心化GPU池是替代方案，但GPU异构性、有限网络带宽和动态可用性使高效调度成为核心挑战。

Method: 采用两阶段调度器：(1)模型分配阶段，将每个副本的层分配到不同GPU上，在内存和链路带宽约束下联合优化延迟和吞吐量；(2)请求时GPU流水线选择阶段，将不同副本的层拼接成端到端执行链，平衡负载并适应当前条件。

Result: 在真实志愿者节点上部署开源LLM进行评估，Parallax相比去中心化基线持续降低延迟并提高吞吐量。

Conclusion: 原则性调度可以使志愿者计算成为LLM推理的实用且经济可行的基础。

Abstract: Deploying a large language model (LLM) inference service remains costly
because centralized serving depends on specialized GPU clusters and
high-bandwidth interconnects in datacenters. An appealing alternative is to
leverage collaborative decentralized GPU pools. However, heterogeneity in GPU
and limited interconnected network bandwidth, along with potentially dynamic
availability, make efficient scheduling the central challenge in this scenario.
In this paper, we present Parallax, a decentralized LLM serving system that
turns a pool of heterogeneous GPUs into an efficient inference platform via a
two-phase scheduler. Parallax decomposes planning into (i) model allocation,
which places layers of each replica across diverse GPUs to jointly optimize
latency and throughput under memory and link-bandwidth constraints, and (ii)
request-time GPU pipeline selection, which stitches layers from different
replicas into end-to-end execution chains that balance load and adapt to
current conditions. We implement Parallax and evaluate it on open-source LLMs
deployed over real volunteer nodes. Parallax consistently reduces latency and
increases throughput relative to decentralized baselines, demonstrating that
principled scheduling can make volunteer compute a practical, affordable
substrate for LLM inference.
  Github Repo at: https://github.com/GradientHQ/parallax.

</details>


### [10] [I Like To Move It -- Computation Instead of Data in the Brain](https://arxiv.org/abs/2509.26193)
*Fabian Czappa,Marvin Kaster,Felix Wolf*

Main category: cs.DC

TL;DR: 提出一种新算法，通过移动计算而非数据来显著减少通信开销，将连接性更新时间缩短6倍，尖峰交换时间缩短两个数量级。


<details>
  <summary>Details</summary>
Motivation: 大脑模拟面临计算需求挑战，特别是在处理约10^11个神经元和10^14个突触的连接组时。结构可塑性（突触形成和删除）对记忆形成和学习至关重要，但现有的连接性更新和尖峰交换方法存在通信瓶颈。

Method: 使用Barnes-Hut近似算法将计算复杂度从O(n^2)降低到O(n log n)，并提出新算法通过移动计算而非数据来减少通信开销。

Result: 新算法使连接性更新时间缩短6倍，尖峰交换时间缩短超过两个数量级。

Conclusion: 通过优化计算和通信策略，显著提升了大规模脑模拟的可扩展性和效率。

Abstract: The detailed functioning of the human brain is still poorly understood. Brain
simulations are a well-established way to complement experimental research, but
must contend with the computational demands of the approximately $10^{11}$
neurons and the $10^{14}$ synapses connecting them, the network of the latter
referred to as the connectome. Studies suggest that changes in the connectome
(i.e., the formation and deletion of synapses, also known as structural
plasticity) are essential for critical tasks such as memory formation and
learning. The connectivity update can be efficiently computed using a
Barnes-Hut-inspired approximation that lowers the computational complexity from
$O(n^2)$ to $O(n log n)$, where n is the number of neurons. However, updating
synapses, which relies heavily on RMA, and the spike exchange between neurons,
which requires all-to-all communication at every time step, still hinder
scalability. We present a new algorithm that significantly reduces the
communication overhead by moving computation instead of data. This shrinks the
time it takes to update connectivity by a factor of six and the time it takes
to exchange spikes by more than two orders of magnitude.

</details>


### [11] [Efficient Construction of Large Search Spaces for Auto-Tuning](https://arxiv.org/abs/2509.26253)
*Floris-Jan Willemsen,Rob V. van Nieuwpoort,Ben van Werkhoven*

Main category: cs.DC

TL;DR: 将约束式自动调优的搜索空间构建重新表述为约束满足问题，通过优化CSP求解器显著提升构建效率。


<details>
  <summary>Details</summary>
Motivation: 自动调优中的搜索空间构建由于组合数量庞大和约束复杂，可能耗时数分钟到数天，成为主要瓶颈。现有方法缺乏理论基础且仅适用于特定子集。

Method: 将搜索空间构建重新表述为CSP问题，开发运行时解析器将用户约束转换为求解器优化表达式，优化求解器以利用自动调优约束的常见结构。

Result: 优化求解器相比暴力枚举减少4个数量级构建时间，相比未优化CSP求解器减少3个数量级，相比基于链式树的领先框架减少1-2个数量级。

Conclusion: 消除了自动调优的关键可扩展性障碍，为探索先前无法达到的问题规模提供了即插即用解决方案。

Abstract: Automatic performance tuning, or auto-tuning, accelerates high-performance
codes by exploring vast spaces of code variants. However, due to the large
number of possible combinations and complex constraints, constructing these
search spaces can be a major bottleneck. Real-world applications have been
encountered where the search space construction takes minutes to hours or even
days. Current state-of-the-art techniques for search space construction, such
as chain-of-trees, lack a formal foundation and only perform adequately on a
specific subset of search spaces.
  We show that search space construction for constraint-based auto-tuning can
be reformulated as a Constraint Satisfaction Problem (CSP). Building on this
insight with a CSP solver, we develop a runtime parser that translates
user-defined constraint functions into solver-optimal expressions, optimize the
solver to exploit common structures in auto-tuning constraints, and integrate
these and other advances in open-source tools. These contributions
substantially improve performance and accessibility while preserving
flexibility.
  We evaluate our approach using a diverse set of benchmarks, demonstrating
that our optimized solver reduces construction time by four orders of magnitude
versus brute-force enumeration, three orders of magnitude versus an unoptimized
CSP solver, and one to two orders of magnitude versus leading auto-tuning
frameworks built on chain-of-trees. We thus eliminate a critical scalability
barrier for auto-tuning and provide a drop-in solution that enables the
exploration of previously unattainable problem scales in auto-tuning and
related domains.

</details>


### [12] [CSnake: Detecting Self-Sustaining Cascading Failure via Causal Stitching of Fault Propagations](https://arxiv.org/abs/2509.26529)
*Shangshu Qian,Lin Tan,Yongle Zhang*

Main category: cs.DC

TL;DR: CSnake是一个故障注入框架，通过因果拼接技术模拟复杂的故障传播链，来暴露分布式系统中的自维持级联故障。


<details>
  <summary>Details</summary>
Motivation: 现有故障检测技术难以在部署前暴露自维持级联故障，因为这些故障需要特定条件的复杂组合才能触发，涉及多个故障传播序列。

Method: 采用因果拼接技术将不同测试中的多个单故障注入因果连接，设计故障因果分析(FCA)来识别故障传播链，使用三阶段测试预算分配协议和局部兼容性检查来优化搜索。

Result: 在五个系统中检测到15个导致自维持级联故障的bug，其中5个已确认，2个已修复。

Conclusion: CSnake框架能有效暴露分布式系统中的自维持级联故障，通过因果拼接和优化搜索策略提高了故障检测效率。

Abstract: Recent studies have revealed that self-sustaining cascading failures in
distributed systems frequently lead to widespread outages, which are
challenging to contain and recover from. Existing failure detection techniques
struggle to expose such failures prior to deployment, as they typically require
a complex combination of specific conditions to be triggered. This challenge
stems from the inherent nature of cascading failures, as they typically involve
a sequence of fault propagations, each activated by distinct conditions.
  This paper presents CSnake, a fault injection framework to expose
self-sustaining cascading failures in distributed systems. CSnake uses the
novel idea of causal stitching, which causally links multiple single-fault
injections in different tests to simulate complex fault propagation chains. To
identify these chains, CSnake designs a counterfactual causality analysis of
fault propagations - fault causality analysis (FCA): FCA compares the execution
trace of a fault injection run with its corresponding profile run (i.e., same
test w/o the injection) and identifies any additional faults triggered, which
are considered to have a causal relationship with the injected fault.
  To address the large search space of fault and workload combinations, CSnake
employs a three-phase allocation protocol of test budget that prioritizes
faults with unique and diverse causal consequences, increasing the likelihood
of uncovering conditional fault propagations. Furthermore, to avoid incorrectly
connecting fault propagations from workloads with incompatible conditions,
CSnake performs a local compatibility check that approximately checks the
compatibility of the path constraints associated with connected fault
propagations with low overhead.
  CSnake detected 15 bugs that cause self-sustaining cascading failures in five
systems, five of which have been confirmed with two fixed.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [smallNet: Implementation of a convolutional layer in tiny FPGAs](https://arxiv.org/abs/2509.25391)
*Fernanda Zapata Bascuñán,Alan Ezequiel Fuster*

Main category: cs.AR

TL;DR: 开发了一个完全用Verilog实现的卷积层，无需Python库和IP核，基于滤波器多项式结构，可在低成本FPGA、SoM、SoC和ASIC上部署。相比计算机版本实现5.1倍加速，81%分类准确率，功耗仅1.5W。


<details>
  <summary>Details</summary>
Motivation: 当前Xilinx和VLSI中的神经网络开发系统需要与Python库协同开发，限制了在嵌入式设备上的部署。

Method: 完全用Verilog手写设计卷积层，基于滤波器多项式结构，避免使用IP核，在单核Cora Z7上验证。

Result: 实现5.1倍速度提升，超过81%的分类准确率，总功耗仅1.5W。

Conclusion: 该架构证明了在资源受限的嵌入式应用中实现实时神经网络处理的可行性。

Abstract: Since current neural network development systems in Xilinx and VLSI require
codevelopment with Python libraries, the first stage of a convolutional network
has been implemented by developing a convolutional layer entirely in Verilog.
This handcoded design, free of IP cores and based on a filter polynomial like
structure, enables straightforward deployment not only on low cost FPGAs but
also on SoMs, SoCs, and ASICs. We analyze the limitations of numerical
representations and compare our implemented architecture, smallNet, with its
computer based counterpart, demonstrating a 5.1x speedup, over 81%
classification accuracy, and a total power consumption of just 1.5 W. The
algorithm is validated on a single-core Cora Z7, demonstrating its feasibility
for real time, resource-constrained embedded applications.

</details>


### [14] [LLM-Powered Code Analysis and Optimization for Gaussian Splatting Kernels](https://arxiv.org/abs/2509.25626)
*Yi Hu,Huiyang Zhou*

Main category: cs.AR

TL;DR: 使用大型语言模型(LLMs)优化3D高斯泼溅(3DGS)的GPU内核，实现了显著性能提升，并展示了LLMs与领域专家协作的潜力。


<details>
  <summary>Details</summary>
Motivation: 3DGS在新型视图合成和实时渲染中具有重要应用，但GPU架构复杂性和性能调优参数搜索空间大，手动优化需要专业知识且耗时易错。

Method: 利用LLMs分析和优化高斯泼溅内核，结合性能分析器提供额外信息，并与领域专家手动优化版本进行对比。

Result: 在MipNeRF360数据集上，LLMs优化使原始3DGS代码获得19-24%加速；结合性能分析器信息后提升至平均38%、最高42%；手动优化可达平均39%、最高48%。在优化后的Seele框架上，LLMs仍能进一步提升6%性能。

Conclusion: LLMs在GPU内核优化方面表现出强大能力，但仍存在手动优化超越LLMs的情况，表明LLMs与领域专家协作具有巨大潜力。

Abstract: 3D Gaussian splatting (3DGS) is a transformative technique with profound
implications on novel view synthesis and real-time rendering. Given its
importance, there have been many attempts to improve its performance. However,
with the increasing complexity of GPU architectures and the vast search space
of performance-tuning parameters, it is a challenging task. Although manual
optimizations have achieved remarkable speedups, they require domain expertise
and the optimization process can be highly time consuming and error prone. In
this paper, we propose to exploit large language models (LLMs) to analyze and
optimize Gaussian splatting kernels. To our knowledge, this is the first work
to use LLMs to optimize highly specialized real-world GPU kernels. We reveal
the intricacies of using LLMs for code optimization and analyze the code
optimization techniques from the LLMs. We also propose ways to collaborate with
LLMs to further leverage their capabilities. For the original 3DGS code on the
MipNeRF360 datasets, LLMs achieve significant speedups, 19% with Deepseek and
24% with GPT-5, demonstrating the different capabilities of different LLMs. By
feeding additional information from performance profilers, the performance
improvement from LLM-optimized code is enhanced to up to 42% and 38% on
average. In comparison, our best-effort manually optimized version can achieve
a performance improvement up to 48% and 39% on average, showing that there are
still optimizations beyond the capabilities of current LLMs. On the other hand,
even upon a newly proposed 3DGS framework with algorithmic optimizations,
Seele, LLMs can still further enhance its performance by 6%, showing that there
are optimization opportunities missed by domain experts. This highlights the
potential of collaboration between domain experts and LLMs.

</details>


### [15] [SAIL: SRAM-Accelerated LLM Inference System with Lookup-Table-based GEMV](https://arxiv.org/abs/2509.25853)
*Jingyao Zhang,Jaewoo Park,Jongeun Lee,Elaheh Sadredini*

Main category: cs.AR

TL;DR: SAIL是一种基于CPU的LLM推理解决方案，通过SRAM加速和查找表技术，支持任意比特精度，在CPU上实现高效的大语言模型推理。


<details>
  <summary>Details</summary>
Motivation: CPU推理对于AI民主化至关重要，但面临两个挑战：现有CPU架构难以处理量化模型的低精度运算，以及token生成阶段的内存瓶颈问题。

Method: 提出SAIL架构，包含三个关键技术：基于SRAM的批处理LUT-GEMV、模式感知LUT优化和内存内类型转换算法，仅需2%硬件开销和一条新指令。

Result: 实验评估显示，SAIL相比ARM Neoverse-N1 CPU基线实现10.7倍加速和19.9倍每美元token数提升，比NVIDIA V100 GPU成本效率高7.04倍。

Conclusion: SAIL为基于CPU的高效LLM推理提供了实用路径，在保持低硬件开销的同时显著提升性能。

Abstract: Large Language Model (LLM) inference requires substantial computational
resources, yet CPU-based inference remains essential for democratizing AI due
to the widespread availability of CPUs compared to specialized accelerators.
However, efficient LLM inference on CPUs faces two fundamental challenges: (1)
existing CPU architectures struggle with low-precision arithmetic required by
quantized models, where optimal bit precision varies across models and layers;
and (2) the memory-bound nature of the token generation phase creates severe
performance bottlenecks. To address these challenges, we propose SAIL
(SRAM-Accelerated Inference of LLMs), a CPU-based inference solution that
efficiently supports arbitrary bit precisions with minimal overhead. SAIL
integrates three key innovations: First, we introduce Batched LUT-based General
Matrix-Vector Multiplication (LUT-GEMV) with SRAM-based processing-in-memory,
enabling high data reuse through lookup tables and reducing memory movement.
Second, our Pattern-Aware LUT optimization identifies and exploits redundancy
in input activation patterns, reducing computation cycles by 13.8\%. Third, we
develop an in-memory type conversion algorithm that leverages PIM's parallelism
for efficient de-/quantization operations, alleviating pressure on CPU's vector
units. Our architecture requires only 2\% hardware overhead and a single new
instruction, while maintaining dual functionality as both compute and storage
units. Experimental evaluations using a modified gem5 simulator demonstrate
that SAIL achieves up to 10.7x speedup and 19.9x higher tokens per dollar
compared to ARM Neoverse-N1 CPU baselines, and up to 7.04x better cost
efficiency than NVIDIA V100 GPUs, establishing a practical path for efficient
CPU-based LLM inference.

</details>


### [16] [Runtime Energy Monitoring for RISC-V Soft-Cores](https://arxiv.org/abs/2509.26065)
*Alberto Scionti,Paolo Savio,Francesco Lubrano,Olivier Terzo,Marco Ferretti,Florin Apopei,Juri Bellucci,Ennio Spano,Luca Carriere*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA的运行时能耗监控方法，无需复杂架构模型，可扩展至多节点集群监控


<details>
  <summary>Details</summary>
Motivation: 传统能耗监控方法需要复杂的架构模型和持续调优，限制了设计空间探索的效率

Method: 使用测量板与FPGA系统模块结合，捕获电流电压值并通过特定内存区域暴露，运行服务读取并计算能耗统计

Result: 实现了无需消耗FPGA额外资源的实时能耗监控，支持数十个测量点

Conclusion: 该框架可用于航空设计应用中的性能与能耗优化，特别是在RISC-V软核上的浅层人工神经网络

Abstract: Energy efficiency is one of the major concern in designing advanced computing
infrastructures. From single nodes to large-scale systems (data centers),
monitoring the energy consumption of the computing system when applications run
is a critical task. Designers and application developers often rely on software
tools and detailed architectural models to extract meaningful information and
determine the system energy consumption. However, when a design space
exploration is required, designers may incur in continuous tuning of the models
to match with the system under evaluation. To overcome such limitations, we
propose a holistic approach to monitor energy consumption at runtime without
the need of running complex (micro-)architectural models. Our approach is based
on a measurement board coupled with a FPGA-based System-on-Module. The
measuring board captures currents and voltages (up to tens measuring points)
driving the FPGA and exposes such values through a specific memory region. A
running service reads and computes energy consumption statistics without
consuming extra resources on the FPGA device. Our approach is also scalable to
monitoring of multi-nodes infrastructures (clusters). We aim to leverage this
framework to perform experiments in the context of an aeronautical design
application; specifically, we will look at optimizing performance and energy
consumption of a shallow artificial neural network on RISC-V based soft-cores.

</details>
