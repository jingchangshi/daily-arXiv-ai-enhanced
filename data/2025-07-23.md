<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [RightTyper: Effective and Efficient Type Annotation for Python](https://arxiv.org/abs/2507.16051)
*Juan Altmayer Pizzorno,Emery D. Berger*

Main category: cs.PL

TL;DR: RightTyper是一种新颖的Python类型注释生成方法，通过基于采样的动态分析克服了现有方法的局限性，能够生成精确的类型注释并将类型检查转化为异常检测，同时保持较低的性能开销。


<details>
  <summary>Details</summary>
Motivation: 现有的Python类型注释方法存在多种问题：静态方法难以处理动态特性且推断类型过于宽泛；AI方法本质上不可靠且可能遗漏罕见或用户定义的类型；动态方法会造成高达270倍的运行时开销，甚至可能推断出错误类型导致运行时错误。此外，所有先前工作都隐含假设待注释的代码已经是正确的，这一假设通常不成立。

Method: RightTyper采用基于采样的方法，结合自我分析指导、统计过滤以及类型信息的仔细解析和聚合。该方法通过有原则且普遍的采样技术来分析实际程序行为，生成精确的类型注释。

Result: RightTyper相比于先前方法提高了类型检查的召回率，能够生成基于实际程序行为的精确类型注释。同时，它将类型检查转化为异常检测，能够识别程序员可以审查的边界情况。在性能方面，RightTyper平均仅造成30%的性能开销，具有快速且空间高效的特点。

Conclusion: RightTyper成功解决了现有Python类型注释方法的主要缺陷，通过创新的采样策略实现了精确性、效率和实用性的平衡，为Python代码的类型注释提供了一种更优的解决方案。

Abstract: Python type annotations bring the benefits of static type checking to the
language. However, manually writing annotations can be time-consuming and
tedious. The result is that most real-world Python code remains largely
untyped. Past approaches to annotating types in Python code fall short in a
number of ways. Static approaches struggle with dynamic features and infer
overly broad types. AI-based methods are inherently unsound and can miss rare
or user-defined types. Dynamic methods can impose extreme runtime overheads,
degrading performance by up to 270x, abort execution as they exhaust resources,
and even infer incorrect types that lead to runtime errors. Crucially, all
prior work assumes implicitly that the code to be annotated is already correct.
This assumption is generally unwarranted, especially for large codebases that
have been untyped.
  This paper presents RightTyper, a novel approach for Python that overcomes
these disadvantages. RightTyper not only generates precise type annotations
based on actual program behavior, improving recall in type checking relative to
prior approaches. It also turns type checking into anomaly detection, allowing
the type checker to identify corner cases that the programmer can audit for
unintended behavior. RightTyper is also fast and space-efficient, imposing just
30% performance overhead on average. RightTyper achieves these characteristics
by a principled yet pervasive use of sampling--guided by self-profiling--along
with statistical filtering and careful resolution and aggregation of type
information.

</details>


### [2] [Understanding Haskell-style Overloading via Open Data and Open Functions](https://arxiv.org/abs/2507.16086)
*Andrew Marmaduke,Apoorv Ingle,J. Garrett Morris*

Main category: cs.PL

TL;DR: 本文提出了一种新的核心语言System F_D，为Haskell风格的重载提供统一语义，通过开放数据类型和开放函数实现，并在Lean4中机械化验证了其元理论


<details>
  <summary>Details</summary>
Motivation: 现有的Haskell类型类系统语义存在局限性，需要额外的类型等式公理，且表达能力不足。因此需要一种新的、更统一和表达力更强的语义来处理Haskell风格的重载

Method: 设计了一种新的核心语言System F_D，其特点是具有开放数据类型和开放函数，这些都通过实例集合而非单一定义来给出。使用Lean4交互式定理证明器对该系统的元理论进行机械化验证

Result: System F_D能够编码Haskell类型类系统的高级特性，比现有的这些特性语义更具表达力，且无需假设额外的类型等式公理

Conclusion: System F_D为Haskell风格重载提供了一种新的统一语义，通过开放数据类型和开放函数的设计，实现了比现有方法更强的表达能力，同时避免了对额外类型等式公理的依赖

Abstract: We present a new, uniform semantics for Haskell-style overloading. We realize
our approach in a new core language, System F$_\mathrm{D}$, whose metatheory we
mechanize in the Lean4 interactive theorem prover. System F$_\mathrm{D}$ is
distinguished by its open data types and open functions, each given by a
collection of instances rather than by a single definition. We show that System
F$_\mathrm{D}$ can encode advanced features of Haskell's of type class systems,
more expressively than current semantics of these features, and without
assuming additional type equality axioms.

</details>


### [3] [Querying Graph-Relational Data](https://arxiv.org/abs/2507.16089)
*Michael J. Sullivan,Zhibo Chen,Elvis Pranskevichus,Robert J. Simmons,Victor Petrovykh,Aljaž Mur Eržen,Yury Selivanov*

Main category: cs.PL

TL;DR: 本文提出了图关系数据库模型，通过EdgeQL查询语言和Gel系统解决了关系数据库扁平化表示与应用程序嵌套数据需求之间的不匹配问题，在保持高效性的同时提供了灵活的对象形状数据操作。


<details>
  <summary>Details</summary>
Motivation: 关系数据库的扁平化数据模型与应用程序期望接收的深度嵌套信息之间存在阻抗不匹配问题，即"对象-关系不匹配"问题。现有的对象关系映射(ORM)技术虽然能提供对象形状的数据操作，但效率低下。

Method: 正式定义了图关系数据库模型，为查询提供了静态和动态语义。开发了EdgeQL这一通用的SQL风格查询语言，以及Gel系统来实现该模型。Gel系统将EdgeQL模式和查询编译成PostgreSQL查询。

Result: Gel系统能够高效地实现对象形状的数据操作，其性能接近直接编写复杂PostgreSQL查询的效率，同时避免了传统ORM技术的低效问题。

Conclusion: 图关系数据库模型通过EdgeQL和Gel系统提供了一个灵活、可组合且强类型的解决方案，有效解决了对象-关系不匹配问题，在保持数据操作便利性的同时实现了高效的查询性能。

Abstract: For applications that store structured data in relational databases, there is
an impedance mismatch between the flat representations encouraged by relational
data models and the deeply nested information that applications expect to
receive. In this work, we present the graph-relational database model, which
provides a flexible, compositional, and strongly-typed solution to this
"object-relational mismatch." We formally define the graph-relational database
model and present a static and dynamic semantics for queries. In addition, we
discuss the realization of the graph-relational database model in EdgeQL, a
general-purpose SQL-style query language, and the Gel system, which compiles
EdgeQL schemas and queries into PostgreSQL queries. Gel facilitates the kind of
object-shaped data manipulation that is frequently provided inefficiently by
object-relational mapping (ORM) technologies, while achieving most of the
efficiency that comes from require writing complex PostgreSQL queries directly.

</details>


### [4] [Enhancing Compiler Optimization Efficiency through Grammatical Decompositions of Control-Flow Graphs](https://arxiv.org/abs/2507.16660)
*Xuran Cai*

Main category: cs.PL

TL;DR: 本文提出了SPL（Series-Parallel-Loop）分解框架，用于解决编译器优化中的寄存器分配、LOSPRE和银行选择等问题，相比传统树分解算法能够更好地处理控制流图的稀疏性特征并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统的编译器优化方法（如寄存器分配和LOSPRE）使用树分解算法时忽略了控制流图的重要稀疏性特征，导致计算成本过高，需要一种新的框架来解决这些挑战。

Method: 提出SPL（Series-Parallel-Loop）分解框架，将图结构中的部分约束满足问题（PCSPs）进行通用化求解，并将其应用于三个优化问题：寄存器分配、LOSPRE优化和银行选择指令放置优化。

Result: SPL分解在寄存器分配中能够准确建模变量干扰图，实现高效的寄存器分配；在LOSPRE中有效识别和消除程序执行中的冗余；在银行选择指令放置中提高数据检索效率并减少延迟。实验表明相比现有方法有显著的性能提升。

Conclusion: SPL分解被证明是处理复杂编译器优化问题的强大工具，包括寄存器分配、LOSPRE和银行选择等，为编译器优化提供了新的有效解决方案。

Abstract: This thesis addresses the complexities of compiler optimizations, such as
register allocation and Lifetime-optimal Speculative Partial Redundancy
Elimination (LOSPRE), which are often handled using tree decomposition
algorithms. However, these methods frequently overlook important sparsity
aspects of Control Flow Graphs (CFGs) and result in high computational costs.
We introduce the SPL (Series-Parallel-Loop) decomposition, a novel framework
that offers optimal solutions to these challenges. A key contribution is the
formulation of a general solution for Partial Constraint Satisfaction Problems
(PCSPs) within graph structures, applied to three optimization problems. First,
SPL decomposition enhances register allocation by accurately modeling variable
interference graphs, leading to efficient register assignments and improved
performance across benchmarks. Second, it optimizes LOSPRE by effectively
identifying and eliminating redundancies in program execution. Finally, the
thesis focuses on optimizing the placement of bank selection instructions to
enhance data retrieval efficiency and reduce latency. Extensive experimentation
demonstrates significant performance improvements over existing methods,
establishing SPL decomposition as a powerful tool for complex compiler
optimizations, including register allocation, LOSPRE, and bank selection.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Resilience Evaluation of Kubernetes in Cloud-Edge Environments via Failure Injection](https://arxiv.org/abs/2507.16109)
*Zihao Chen,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: 本研究构建了一个新颖的弹性评估框架，集成多种故障注入工具对混合云边缘Kubernetes环境进行系统性弹性测试，创建了首个混合云边缘Kubernetes部署的综合弹性数据集，发现云边缘部署在网络延迟和分区条件下响应稳定性优于云部署80%，而云部署在带宽限制下弹性表现更佳47%。


<details>
  <summary>Details</summary>
Motivation: 随着Kubernetes在关键任务微服务中的应用日益增长，需要在真实故障条件下评估系统弹性变得至关重要，但目前对混合云边缘环境中Kubernetes的系统性弹性评估研究有限，缺乏全面的评估框架和数据集。

Method: 设计了一个新颖的弹性评估框架，集成了Chaos Mesh、Gremlin和ChaosBlade等主流故障注入工具，结合真实流量模拟工具，实现复杂故障场景的自动化编排。通过该框架系统性地针对云和云边缘环境中的节点级、Pod级和网络故障进行综合实验。

Result: 创建了首个混合云边缘Kubernetes部署的综合弹性数据集，包含超过30GB的性能数据，来自11,965个故障注入场景，涵盖响应时间、故障率和错误模式。实验结果显示云边缘部署在网络延迟和分区条件下的响应稳定性比云部署优80%，而云部署在带宽限制下的弹性表现比云边缘部署好47%。

Conclusion: 该研究为混合云边缘Kubernetes环境的弹性评估提供了系统性的方法和全面的数据集，量化分析结果为云边缘部署的架构决策提供了定量指导，填补了该领域研究的空白。

Abstract: Kubernetes has emerged as an essential platform for deploying containerised
applications across cloud and edge infrastructures. As Kubernetes gains
increasing adoption for mission-critical microservices, evaluating system
resilience under realistic fault conditions becomes crucial. However,
systematic resilience assessments of Kubernetes in hybrid cloud-edge
environments are currently limited in research. To address this gap, a novel
resilience evaluation framework integrates mainstream fault injection tools
with automated workload generation for comprehensive cloud-edge Kubernetes
testing. Multiple fault injection platforms, including Chaos Mesh, Gremlin, and
ChaosBlade are combined with realistic traffic simulation tools, enabling
automated orchestration of complex failure scenarios. Through this framework,
comprehensive experiments are conducted that systematically target node-level,
pod-level, and network failures across cloud and cloud-edge environments. The
first comprehensive resilience dataset for hybrid cloud-edge Kubernetes
deployments is created, comprising over 30 GB of performance data from 11,965
fault injection scenarios including response times, failure rates, and error
patterns. Analysis reveals that cloud-edge deployments demonstrate 80% superior
response stability under network delay and partition conditions, while cloud
deployments exhibit 47% better resilience under bandwidth limitations,
providing quantitative guidance for architectural decision-making in cloud-edge
deployments.

</details>


### [6] [Parallel Ray Tracing of Black Hole Images Using the Schwarzschild Metric](https://arxiv.org/abs/2507.16165)
*Liam Naddell,Marcelo Ponce*

Main category: cs.DC

TL;DR: 这篇论文描述了一个并行开源程序的实现，该程序能够在黑洞几何环境中进行光线追踪渲染图像，结合了数学近似、科学库使用和并行计算技术。


<details>
  <summary>Details</summary>
Motivation: 黑洞图像渲染在科学和天体物理可视化中很常见，而通用光线追踪技术在计算机图形学中广泛应用。需要开发一个能够处理黑洞几何环境下光线追踪的并行程序。

Method: 结合多种并行科学计算技术，包括数学近似方法、科学库的使用、共享内存和分布式内存并行计算技术，实现一个开源的并行光线追踪程序。

Result: 成功实现了一个能够在黑洞几何环境中进行光线追踪的并行开源程序，该程序结合了多种并行计算技术。

Conclusion: 通过结合数学近似、科学库使用和并行计算技术，成功开发了一个专门用于黑洞几何环境下光线追踪的开源并行程序，为科学和天体物理可视化提供了有效工具。

Abstract: Rendering images of black holes by utilizing ray tracing techniques is a
common methodology employed in many aspects of scientific and astrophysical
visualizations. Similarly, general ray tracing techniques are widely used in
areas related to computer graphics. In this work we describe the implementation
of a parallel open-source program that can ray trace images in the presence of
a black hole geometry. We do this by combining a couple of different techniques
usually present in parallel scientific computing, such as, mathematical
approximations, utilization of scientific libraries, shared-memory and
distributed-memory parallelism.

</details>


### [7] [Autonomous Dominant Resource Fairness for Blockchain Ecosystems](https://arxiv.org/abs/2507.16350)
*Serdar Metin*

Main category: cs.DC

TL;DR: 本研究提出了自主主导资源公平性算法(Autonomous Dominant Resource Fairness)，这是一种适用于区块链智能合约的多资源分配算法，能够高效处理异构资源需求的任务分配问题。


<details>
  <summary>Details</summary>
Motivation: 尽管区块链在资源管理方面应用广泛，但现有研究主要集中在单一资源分配场景，即使涉及多种资源类型，也通常将其视为单一资源类型处理。缺乏针对异构资源需求任务的多资源类型分配解决方案，特别是适应区块链环境中gas限制的算法。

Method: 采用预计算主导资源公平性(Precomputed Dominant Resource Fairness)算法的智能合约适配版本，该算法能够在不使用循环迭代的情况下近似主导资源公平性，使其更适合区块链环境中的gas限制约束。

Result: 实验数据显示，自主主导资源公平性算法是一种gas成本高效的算法，能够为无限数量的用户管理数百种资源类型，有效解决了区块链环境下的多资源分配问题。

Conclusion: 自主主导资源公平性算法成功解决了区块链环境下多资源类型分配的技术挑战，为区块链在资源管理领域的应用提供了更加完善和实用的解决方案，具有良好的可扩展性和成本效率。

Abstract: Blockchain systems have been a part of mainstream academic research, and a
hot topic at that. It has spread to almost every subfield in the computer
science literature, as well as economics and finance. Especially in a world
where digital trust is much sought for, blockchains offer a rich variety of
desired properties, such as immutability, public auditing, decentralised record
keeping, among others. Not only has it been a research topic of its own, the
integration of blockchains into other systems has been proposed as solutions in
many areas, ranging from grid computing, cloud and fog computing, to internet
of things, self driving vehicles , and smart cities. In many cases the primary
function attributed to blockchains in these contexts is resource management.
Although much attention is paid to this topic, the focus is on single resource
allocation scenarios. Even the cases where multiple resource types are to be
allocated, are treated as single resource type scenarios, and problems are
formulated as allocating standardised bundles consisting of a fixed amount of
each of them, such as virtual machines. The present study addresses the problem
of allocating multiple resource types among tasks with heterogeneous resource
demands with a smart contract adaptation of Precomputed Dominant Resource
Fairness; an algorithm that approximates Dominant Resource Fairness, without
loop iterations, which makes it preferable in the blockchain context because of
the block gas limit. We present the resulting algorithm, Autonomous Dominant
Resource Fairness, along with the empirical data collected from the tests run
on the algorithm. The results show that Autonomous Dominant Resource Fairness
is a gas-cost efficient algorithm, which can be used to manage hundreds of
resource types for unlimited number of users.

</details>


### [8] [FOGNITE: Federated Learning-Enhanced Fog-Cloud Architecture](https://arxiv.org/abs/2507.16668)
*Somayeh Sobati-M*

Main category: cs.DC

TL;DR: 本文提出FOGNITE框架，结合联邦学习、强化学习和数字孪生技术，为智能电网提供边缘计算解决方案，实现负载均衡准确率提升93.7%，能耗浪费减少63.2%


<details>
  <summary>Details</summary>
Motivation: 现代智能电网需要快速、智能且节能的边缘计算来管理实时波动并确保可靠运行，现有架构无法满足分布式能源系统的自主性、韧性和效率要求

Method: FOGNITE框架包含三个核心组件：1）联邦学习-各雾节点在私有能耗数据上训练CNN-LSTM模型，通过联邦聚合保护数据隐私；2）强化学习-智能体根据系统负载和能源条件动态调度任务；3）数字孪生验证-分层数字孪生层在部署前模拟潜在行动

Result: 在树莓派设备真实测试平台上评估显示，相比传统架构，负载均衡准确率提升93.7%，能耗浪费减少63.2%，显著降低执行错误和能源浪费

Conclusion: FOGNITE将智能电网控制从被动纠正转向主动优化，代表了向更智能、适应性更强、更可持续的能源基础设施迈进的重要一步

Abstract: Modern smart grids demand fast, intelligent, and energy-aware computing at
the edge to manage real time fluctuations and ensure reliable operation. This
paper introduces FOGNITE Fog-based Grid In intelligence with Neural Integration
and Twin based Execution a next-generation fog cloud framework designed to
enhance autonomy, resilience, and efficiency in distributed energy systems.
FOGNITE combines three core components: federated learning, reinforcement
learning, and digital twin validation. Each fog node trains a local CNN LSTM
model on private energy consumption data, enabling predictive intelligence
while preserving data privacy through federated aggregation. A reinforcement
learning agent dynamically schedules tasks based on current system load and
energy conditions, optimizing for performance under uncertainty.
  To prevent unsafe or inefficient decisions, a hierarchical digital twin layer
simulates potential actions before deployment, significantly reducing execution
errors and energy waste. We evaluate FOGNITE on a real world testbed of
Raspberry Pi devices, showing up to a 93.7% improvement in load balancing
accuracy and a 63.2% reduction in energy waste compared to conventional
architectures. By shifting smart grid control from reactive correction to
proactive optimization, FOGNITE represents a step toward more intelligent,
adaptive, and sustainable energy infrastructures

</details>


### [9] [AcceleratedKernels.jl: Cross-Architecture Parallel Algorithms from a Unified, Transpiled Codebase](https://arxiv.org/abs/2507.16710)
*Andrei-Leonard Nicusan,Dominik Werner,Simon Branford,Simon Hartley,Andrew J. Morris,Kit Windows-Yule*

Main category: cs.DC

TL;DR: AcceleratedKernels.jl是一个后端无关的Julia并行计算库，通过独特的转译架构原生支持NVIDIA、AMD、Intel和Apple加速器，在英国HPC集群上使用200个NVIDIA A100 GPU实现了世界级的538-855 GB/s排序吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的并行计算解决方案存在实现和使用复杂性高的问题，需要一个统一的、后端无关的库来简化跨不同硬件加速器的并行编程，同时保持高性能。

Method: 开发了基于独特转译架构的AcceleratedKernels.jl库，采用统一紧凑的代码库设计，原生支持多种硬件加速器(NVIDIA、AMD、Intel、Apple)，实现CPU-GPU协同处理，并支持硬件专用的MPI实现。

Result: 算术密集型内核的基准测试显示性能与C和OpenMP多线程CPU实现相当，Julia在数值性能上有时比传统C编译器更一致和可预测。在Baskerville Tier 2英国HPC集群上实现了538-855 GB/s的世界级排序吞吐量，使用直接NVLink GPU间互连平均获得4.93倍加速。

Conclusion: AcceleratedKernels.jl成功提供了高性能的跨平台并行计算解决方案，证明了Julia在HPC领域的潜力。通信密集型HPC任务只有在使用GPUDirect互连时在GPU上才具有经济可行性，这为未来HPC系统设计提供了重要指导。

Abstract: AcceleratedKernels.jl is introduced as a backend-agnostic library for
parallel computing in Julia, natively targeting NVIDIA, AMD, Intel, and Apple
accelerators via a unique transpilation architecture. Written in a unified,
compact codebase, it enables productive parallel programming with minimised
implementation and usage complexities. Benchmarks of arithmetic-heavy kernels
show performance on par with C and OpenMP-multithreaded CPU implementations,
with Julia sometimes offering more consistent and predictable numerical
performance than conventional C compilers. Exceptional composability is
highlighted as simultaneous CPU-GPU co-processing is achievable - such as
CPU-GPU co-sorting - with transparent use of hardware-specialised MPI
implementations. Tests on the Baskerville Tier 2 UK HPC cluster achieved
world-class sorting throughputs of 538-855 GB/s using 200 NVIDIA A100 GPUs,
comparable to the highest literature-reported figure of 900 GB/s achieved on
262,144 CPU cores. The use of direct NVLink GPU-to-GPU interconnects resulted
in a 4.93x speedup on average; normalised by a combined capital, running and
environmental cost, communication-heavy HPC tasks only become economically
viable on GPUs if GPUDirect interconnects are employed.

</details>


### [10] [Collaborative Inference and Learning between Edge SLMs and Cloud LLMs: A Survey of Algorithms, Execution, and Open Challenges](https://arxiv.org/abs/2507.16731)
*Senyao Li,Haozhao Wang,Wenchao Xu,Rui Zhang,Song Guo,Jingling Yuan,Xian Zhong,Tianwei Zhang,Ruixuan Li*

Main category: cs.DC

TL;DR: 这是一篇关于大语言模型(LLM)与小语言模型(SLM)边缘-云端协作的综述论文，系统性地分析了推理和训练阶段的协作策略，为构建高效、可扩展且可信的边缘-云端智能系统提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，单纯的云端部署或边缘设备压缩已无法满足需求，存在延迟、隐私、成本和个性化等问题。因此需要探索云端LLM与边缘SLM的协作范式来解决这些挑战。

Method: 提出了边缘-云端协作策略的统一分类法。推理方面包括任务分配、任务分解和混合协作三类方法，涵盖自适应调度、资源感知卸载、推测解码和模块化路由。训练方面涵盖分布式适应技术，包括参数对齐、剪枝、双向蒸馏和小模型引导优化。

Result: 系统总结了数据集、基准测试和部署案例，重点介绍了隐私保护方法和垂直应用场景，为LLM-SLM协作提供了首个系统性的理论框架。

Conclusion: 该综述为LLM-SLM协作建立了系统性基础，通过系统和算法的协同设计，能够实现高效、可扩展且可信的边缘-云端智能系统，为未来的研究和应用提供了重要指导。

Abstract: As large language models (LLMs) evolve, deploying them solely in the cloud or
compressing them for edge devices has become inadequate due to concerns about
latency, privacy, cost, and personalization. This survey explores a
collaborative paradigm in which cloud-based LLMs and edge-deployed small
language models (SLMs) cooperate across both inference and training. We present
a unified taxonomy of edge-cloud collaboration strategies. For inference, we
categorize approaches into task assignment, task division, and mixture-based
collaboration at both task and token granularity, encompassing adaptive
scheduling, resource-aware offloading, speculative decoding, and modular
routing. For training, we review distributed adaptation techniques, including
parameter alignment, pruning, bidirectional distillation, and
small-model-guided optimization. We further summarize datasets, benchmarks, and
deployment cases, and highlight privacy-preserving methods and vertical
applications. This survey provides the first systematic foundation for LLM-SLM
collaboration, bridging system and algorithm co-design to enable efficient,
scalable, and trustworthy edge-cloud intelligence.

</details>


### [11] [Cooling Matters: Benchmarking Large Language Models and Vision-Language Models on Liquid-Cooled Versus Air-Cooled H100 GPU Systems](https://arxiv.org/abs/2507.16781)
*Imran Latif,Muhammad Ali Shafique,Hayat Ullah,Alex C. Newkirk,Xi Yu,Arslan Munir*

Main category: cs.DC

TL;DR: 本研究比较了液冷和风冷系统在运行大语言模型和视觉语言模型时的性能差异，发现液冷系统在温度控制、计算性能和能效方面均显著优于风冷系统。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载（特别是大语言模型和视觉语言模型）的快速增长，数据中心面临着日益严峻的功耗和散热挑战，需要评估不同冷却方案的效果以优化数据中心的能效和可持续性。

Method: 使用两个HGX节点（每个配备8块NVIDIA H100 GPU）分别采用液冷和风冷方案，通过GPU Burn、Weights and Biases和IPMItool等工具收集详细的热力、功耗和计算数据，对大语言模型和视觉语言模型进行基准测试。

Result: 液冷系统GPU温度维持在41-50°C之间，而风冷系统在负载下温度波动在54-72°C；液冷系统性能提升17%（每GPU 54 TFLOPs vs 46 TFLOPs），具有更好的每瓦性能、更低的能耗开销和更高的系统效率。

Conclusion: 液冷系统的热稳定性带来了显著的性能和能效优势，为超大规模数据中心提供了在能源效率和可持续性方面的优化路径，证明了液冷技术在AI工作负载中的重要价值。

Abstract: The unprecedented growth in artificial intelligence (AI) workloads, recently
dominated by large language models (LLMs) and vision-language models (VLMs),
has intensified power and cooling demands in data centers. This study
benchmarks LLMs and VLMs on two HGX nodes, each with 8x NVIDIA H100 graphics
processing units (GPUs), using liquid and air cooling. Leveraging GPU Burn,
Weights and Biases, and IPMItool, we collect detailed thermal, power, and
computation data. Results show that the liquid-cooled systems maintain GPU
temperatures between 41-50 degrees Celsius, while the air-cooled counterparts
fluctuate between 54-72 degrees Celsius under load. This thermal stability of
liquid-cooled systems yields 17 percent higher performance (54 TFLOPs per GPU
vs. 46 TFLOPs per GPU), improved performance per watt, reduced energy overhead,
and greater system efficiency than the air-cooled counterparts. These findings
underscore the energy and sustainability benefits of liquid cooling, offering a
compelling path forward for hyperscale data centers s

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [A Sparsity-Aware Autonomous Path Planning Accelerator with HW/SW Co-Design and Multi-Level Dataflow Optimization](https://arxiv.org/abs/2507.16177)
*Yifan Zhang,Xiaoyu Niu,Hongzheng Tian,Yanjun Zhang,Bo Yu,Shaoshan Liu,Sitao Huang*

Main category: cs.AR

TL;DR: 本文提出了一个基于FPGA的端到端加速框架，用于自动驾驶路径规划中的二次规划问题，通过硬件友好的ADMM算法和多级数据流优化策略，在AMD ZCU102平台上实现了相比CPU、GPU和现有FPGA设计的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶路径规划的计算密集性对资源受限的硬件平台构成重大挑战，需要针对路径规划核心的二次规划问题开发高效的硬件加速解决方案。

Method: 采用硬件友好的交替方向乘子法(ADMM)求解二次规划问题，使用可并行化的预条件共轭梯度法(PCG)处理线性系统；通过分析稀疏矩阵模式设计定制化存储方案和高效稀疏矩阵乘法单元；实施多级数据流优化策略，包括算子内并行化和流水线、算子间细粒度流水线以及CPU-FPGA系统级任务映射。

Result: 在AMD ZCU102平台上实现了最先进的延迟和能效表现：相比最佳FPGA设计快1.48倍，相比Intel i7-11800H CPU快2.89倍，相比ARM Cortex-A57嵌入式CPU快5.62倍，相比最先进GPU解决方案快1.56倍，吞吐量相比现有FPGA设计提升2.05倍。

Conclusion: 该FPGA加速框架成功解决了自动驾驶路径规划的计算瓶颈问题，通过算法优化和硬件架构设计的协同，在多个性能指标上超越了现有的CPU、GPU和FPGA解决方案，为资源受限的自动驾驶系统提供了高效的路径规划加速方案。

Abstract: Path planning is critical for autonomous driving, generating smooth,
collision-free, feasible paths based on perception and localization inputs.
However, its computationally intensive nature poses significant challenges for
resource-constrained autonomous driving hardware. This paper presents an
end-to-end FPGA-based acceleration framework targeting the quadratic
programming (QP), core of optimization-based path planning. We employ a
hardware-friendly alternating direction method of multipliers (ADMM) for QP
solving and a parallelizable preconditioned conjugate gradient (PCG) method for
linear systems. By analyzing sparse matrix patterns, we propose customized
storage schemes and efficient sparse matrix multiplication units, significantly
reducing resource usage and accelerating matrix operations. Our multi-level
dataflow optimization strategy incorporates intra-operator parallelization and
pipelining, inter-operator fine-grained pipelining, and CPU-FPGA system-level
task mapping. Implemented on the AMD ZCU102 platform, our framework achieves
state-of-the-art latency and energy efficiency, including 1.48x faster
performance than the best FPGA-based design, 2.89x over an Intel i7-11800H CPU,
5.62x over an ARM Cortex-A57 embedded CPU, and 1.56x over a state-of-the-art
GPU solution, along with a 2.05x throughput improvement over existing
FPGA-based designs.

</details>


### [13] [Hourglass Sorting: A novel parallel sorting algorithm and its implementation](https://arxiv.org/abs/2507.16326)
*Daniel Bascones,Borja Morcillo*

Main category: cs.AR

TL;DR: 本文提出了一种新颖的并行排序器，专门针对输入并行但输出串行的场景，在FPGA上实现并验证，实现了log n的首元素输出延迟和n+log n的总排序时间，时钟频率不随n退化，资源与输入大小线性扩展。


<details>
  <summary>Details</summary>
Motivation: 传统串行排序算法受O(n log n)复杂度限制，虽然并行化技术可以突破这一限制，但实现成本高，且随着数组规模增大，数据移动成为瓶颈，可能比排序本身更耗时。因此需要设计适用于特定场景（输入并行，输出串行）的高效并行排序器。

Method: 设计了一种专门针对输入并行但输出串行场景的新型并行排序器，并在FPGA上实现该设计，在量子LDPC解码器的背景下进行验证。该方法通过特殊的并行架构设计，避免了传统并行排序方法的缺陷。

Result: 实现了log n的首元素输出延迟，总排序时间为n+log n。与其他并行排序方法不同，时钟频率不会随着n的增加而降低，硬件资源消耗与输入大小呈线性关系。在FPGA上的实现验证了设计的有效性。

Conclusion: 提出的并行排序器成功解决了输入并行输出串行场景下的排序问题，实现了优异的时间性能（log n首元素延迟，n+log n总时间），同时保持了良好的可扩展性（线性资源扩展，时钟频率不退化），为特定应用场景提供了高效的排序解决方案。

Abstract: Sorting is one of the fundamental problems in computer science. Playing a
role in many processes, it has a lower complexity bound imposed by
$\mathcal{O}(n\log{n})$ when executing on a sequential machine. This limit can
be brought down to sub-linear times thanks to parallelization techniques that
increase the number of comparisons done in parallel. This, however, increases
the cost of implementation, which limits the application of such techniques.
Moreover, as the size of the arrays increases, a bottleneck arises in moving
the vast quantities of data required at the input, and generated at the output
of such sorter. This might impose time requirements much stricter than those of
the sorting itself. In this paper, a novel parallel sorter is proposed for the
specific case where the input is parallel, but the output is serial. The design
is then implemented and verified on an FPGA within the context of a quantum
LDPC decoder. A latency of $\log{n}$ is achieved for the output of the first
element, after which the rest stream out for a total sorting time of
$n+\log{n}$. Contrary to other parallel sorting methods, clock speed does not
degrade with $n$, and resources scale linearly with input size.

</details>


### [14] [ApproxGNN: A Pretrained GNN for Parameter Prediction in Design Space Exploration for Approximate Computing](https://arxiv.org/abs/2507.16379)
*Ondrej Vlcek,Vojtech Mrazek*

Main category: cs.AR

TL;DR: 本文提出ApproxGNN，一个基于图神经网络的预训练模型，用于预测近似加速器的质量和硬件成本，通过学习嵌入的组件特征提取方法，在设计空间探索中显著提高预测精度并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法需要为每个新电路配置重新训练，计算成本高且耗时；预测由近似组件构成的电路精度而无需完整综合仍是挑战性问题；需要一种高效且可迁移的方法来自动化近似计算的设计空间探索。

Method: 提出ApproxGNN构建方法，使用预训练图神经网络模型预测近似加速器的QoR和硬件成本；引入基于学习嵌入的新颖组件特征提取方法，替代传统误差度量；支持少量近似组件训练、多预测任务迁移、预计算嵌入提高效率。

Result: 在图像卷积滤波器数据集上，所提出的嵌入方法比传统方法的预测精度（均方误差）提高50%；整体预测精度比统计机器学习方法在无微调情况下提高30%，在快速微调情况下提高54%。

Conclusion: ApproxGNN通过基于学习嵌入的特征提取和图神经网络架构，成功解决了近似计算设计空间探索中的预测精度和计算效率问题，为近似加速器的自动化设计提供了有效解决方案。

Abstract: Approximate computing offers promising energy efficiency benefits for
error-tolerant applications, but discovering optimal approximations requires
extensive design space exploration (DSE). Predicting the accuracy of circuits
composed of approximate components without performing complete synthesis
remains a challenging problem. Current machine learning approaches used to
automate this task require retraining for each new circuit configuration,
making them computationally expensive and time-consuming. This paper presents
ApproxGNN, a construction methodology for a pre-trained graph neural network
model predicting QoR and HW cost of approximate accelerators employing
approximate adders from a library. This approach is applicable in DSE for
assignment of approximate components to operations in accelerator. Our approach
introduces novel component feature extraction based on learned embeddings
rather than traditional error metrics, enabling improved transferability to
unseen circuits. ApproxGNN models can be trained with a small number of
approximate components, supports transfer to multiple prediction tasks,
utilizes precomputed embeddings for efficiency, and significantly improves
accuracy of the prediction of approximation error. On a set of image
convolutional filters, our experimental results demonstrate that the proposed
embeddings improve prediction accuracy (mean square error) by 50% compared to
conventional methods. Furthermore, the overall prediction accuracy is 30%
better than statistical machine learning approaches without fine-tuning and 54%
better with fast finetuning.

</details>


### [15] [Ironman: Accelerating Oblivious Transfer Extension for Privacy-Preserving AI with Near-Memory Processing](https://arxiv.org/abs/2507.16391)
*Chenqi Lin,Kang Yang,Tianshi Xu,Ling Liang,Yufei Wang,Zhaohui Chen,Runsheng Wang,Mingyu Gao,Meng Li*

Main category: cs.AR

TL;DR: 本文提出了名为Ironman的OT加速器，通过针对SPCOT和LPN操作的不同特点设计专门的硬件加速方案，显著提升了隐私保护机器学习框架中不经意传输的效率，实现了39.2-237.4倍的OT吞吐量提升和2.1-3.4倍的端到端延迟降低。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习的广泛应用，用户数据的隐私问题日益突出。基于密码学原语的隐私保护机器学习(PPML)是一个有前景的解决方案，但现有PPML框架严重依赖不经意传输(OT)原语来计算非线性函数，而OT主要在通用CPU上计算，成为了现代PPML框架的延迟瓶颈。

Method: 提出Ironman OT加速器，针对OT中的两个主要操作采用不同的优化策略：(1)对于计算密集型的SPCOT操作，设计了硬件友好的SPCOT算法和定制化加速器；(2)对于因内存访问模式不规则而受内存带宽限制的LPN操作，采用近内存处理(NMP)架构，配备内存侧缓存和索引排序来提高有效内存带宽。

Result: 实验结果显示，在不同NMP配置下，Ironman相比全线程CPU实现在OT吞吐量上取得了39.2-237.4倍的提升。在不同PPML框架中，对于CNN和Transformer模型，Ironman实现了2.1-3.4倍的端到端延迟降低。

Conclusion: Ironman通过针对OT操作中不同组件的特定优化策略，成功解决了PPML框架中的延迟瓶颈问题，为隐私保护机器学习的实际应用提供了高效的硬件加速解决方案。

Abstract: With the wide application of machine learning (ML), privacy concerns arise
with user data as they may contain sensitive information. Privacy-preserving ML
(PPML) based on cryptographic primitives has emerged as a promising solution in
which an ML model is directly computed on the encrypted data to provide a
formal privacy guarantee. However, PPML frameworks heavily rely on the
oblivious transfer (OT) primitive to compute nonlinear functions. OT mainly
involves the computation of single-point correlated OT (SPCOT) and learning
parity with noise (LPN) operations. As OT is still computed extensively on
general-purpose CPUs, it becomes the latency bottleneck of modern PPML
frameworks.
  In this paper, we propose a novel OT accelerator, dubbed Ironman, to
significantly increase the efficiency of OT and the overall PPML framework. We
observe that SPCOT is computation-bounded, and thus propose a hardware-friendly
SPCOT algorithm with a customized accelerator to improve SPCOT computation
throughput. In contrast, LPN is memory-bandwidth-bounded due to irregular
memory access patterns. Hence, we further leverage the near-memory processing
(NMP) architecture equipped with memory-side cache and index sorting to improve
effective memory bandwidth. With extensive experiments, we demonstrate Ironman
achieves a 39.2-237.4 times improvement in OT throughput across different NMP
configurations compared to the full-thread CPU implementation. For different
PPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end
latency for both CNN and Transformer models.

</details>


### [16] [Augmenting Von Neumann's Architecture for an Intelligent Future](https://arxiv.org/abs/2507.16628)
*Rajpreet Singh,Vidhi Kothari*

Main category: cs.AR

TL;DR: 本文提出了一种扩展冯·诺伊曼模型的新型计算机架构，通过添加专用推理单元(RU)来实现原生人工通用智能能力，该架构将推理、学习和适应作为内在执行属性而非软件抽象。


<details>
  <summary>Details</summary>
Motivation: 传统计算架构无法原生支持人工通用智能所需的符号推理、多智能体协调和混合符号-神经计算能力，需要一种能够在硬件层面直接支持认知计算的新型架构。

Method: 设计了一个扩展冯·诺伊曼模型的新架构，核心是专用推理单元(RU)作为协处理器，配合推理专用指令集、并行符号处理管道、智能体感知的内核抽象，以及统一的内存层次结构，通过硬件、操作系统和智能体运行时的系统化协同设计实现。

Result: 该架构能够让自主智能体在系统级别直接执行目标导向规划、动态知识操作和内省推理，实现了认知和数值计算负载的无缝集成，使推理、学习和适应成为内在的执行属性。

Conclusion: 通过硬件嵌入的推理能力，该架构为开发通用智能机器建立了计算基础，有潜力实现真正的人工通用智能系统，其中认知能力不再是软件层面的抽象，而是计算基础设施的固有特性。

Abstract: This work presents a novel computer architecture that extends the Von Neumann
model with a dedicated Reasoning Unit (RU) to enable native artificial general
intelligence capabilities. The RU functions as a specialized co-processor that
executes symbolic inference, multi-agent coordination, and hybrid
symbolic-neural computation as fundamental architectural primitives. This
hardware-embedded approach allows autonomous agents to perform goal-directed
planning, dynamic knowledge manipulation, and introspective reasoning directly
within the computational substrate at system scale. The architecture
incorporates a reasoning-specific instruction set architecture, parallel
symbolic processing pipelines, agent-aware kernel abstractions, and a unified
memory hierarchy that seamlessly integrates cognitive and numerical workloads.
Through systematic co-design across hardware, operating system, and agent
runtime layers, this architecture establishes a computational foundation where
reasoning, learning, and adaptation emerge as intrinsic execution properties
rather than software abstractions, potentially enabling the development of
general-purpose intelligent machines.

</details>


### [17] [MTU: The Multifunction Tree Unit in zkSpeed for Accelerating HyperPlonk](https://arxiv.org/abs/2507.16793)
*Jianqiao Mo,Alhad Daftardar,Joey Ah-kiow,Kaiyue Guo,Benedikt Bünz,Siddharth Garg,Brandon Reagen*

Main category: cs.AR

TL;DR: 本文研究了零知识证明(ZKP)中基于二叉树计算模式的硬件加速优化，提出了混合遍历策略和多功能树单元(MTU)硬件加速器，在DDR带宽下实现了相比CPU高达1478倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 零知识证明对隐私保护和可验证计算至关重要，其核心算法如SumCheck协议和Merkle树承诺具有平衡二叉树计算模式，适合硬件加速。然而，针对如何最优利用底层树结构模式来提高硬件效率的专门研究仍然有限。

Method: 对基于树的工作负载在不同遍历策略下进行系统性评估，分析多线程CPU和硬件加速器(多功能树单元MTU)的性能表现。提出了一种硬件友好的混合遍历方法，用于改善二叉树的并行性和可扩展性，同时显著减少硬件上的内存流量。

Result: MTU在DDR级别带宽下相比CPU实现了高达1478倍的加速比，提出的混合遍历方法相比独立方法的性能提升高达3倍。这些结果为具有二叉树结构的ZKP工作负载设计高效硬件加速器提供了实用指导。

Conclusion: 通过系统性研究二叉树遍历策略和专用硬件设计，成功实现了零知识证明工作负载的显著性能提升，为ZKP硬件加速器设计提供了重要的理论指导和实践经验。

Abstract: Zero-Knowledge Proofs (ZKPs) are critical for privacy preservation and
verifiable computation. Many ZKPs rely on kernels such as the SumCheck protocol
and Merkle Tree commitments, which enable their security properties. These
kernels exhibit balanced binary tree computational patterns, which enable
efficient hardware acceleration. Prior work has investigated accelerating these
kernels as part of an overarching ZKP protocol; however, a focused study of how
to best exploit the underlying tree pattern for hardware efficiency remains
limited. We conduct a systematic evaluation of these tree-based workloads under
different traversal strategies, analyzing performance on multi-threaded CPUs
and a hardware accelerator, the Multifunction Tree Unit (MTU). We introduce a
hardware-friendly Hybrid Traversal for binary tree that improves parallelism
and scalability while significantly reducing memory traffic on hardware. Our
results show that MTU achieves up to 1478$\times$ speedup over CPU at DDR-level
bandwidth and that our hybrid traversal outperforms as standalone approach by
up to 3$\times$. These findings offer practical guidance for designing
efficient hardware accelerators for ZKP workloads with binary tree structures.

</details>
