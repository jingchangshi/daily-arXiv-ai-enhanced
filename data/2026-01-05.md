<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Word Frequency Counting Based on Serverless MapReduce](https://arxiv.org/abs/2601.00380)
*Hanzhe Li,Bingchen Lin,Mengyuan Xu*

Main category: cs.DC

TL;DR: 本文结合Serverless计算和MapReduce模型优化词频统计任务，通过实验确定最优的Map和Reduce函数数量以提高执行效率


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算需求增长，Serverless计算成为研究热点，而MapReduce作为大数据处理模型已广泛应用。本文旨在结合Serverless的FaaS框架和MapReduce的高并发、鲁棒性优势，优化词频统计任务的执行时间和效率

Method: 采用基于Serverless计算平台的MapReduce编程模型，针对特定任务（词频统计）进行实验，探索最优的Map函数和Reduce函数数量配置

Result: 实验表明，随着Map和Reduce函数数量的增加，相同工作负载下的执行时间减少，程序整体效率以不同速率提升。通过实验发现了最优的Map和Reduce函数配置

Conclusion: 本文提出的Serverless MapReduce优化方法能够有效提高词频统计任务的执行效率，最优Map/Reduce函数数量的发现有助于企业和程序员找到最优解决方案

Abstract: With the increasing demand for high-performance and high-efficiency computing, cloud computing, especially serverless computing, has gradually become a research hotspot in recent years, attracting numerous research attention. Meanwhile, MapReduce, which is a popular big data processing model in the industry, has been widely applied in various fields. Inspired by the serverless framework of Function as a Service and the high concurrency and robustness of MapReduce programming model, this paper focus on combining them to reduce the time span and increase the efficiency when executing the word frequency counting task. In this case, the paper use a MapReduce programming model based on a serverless computing platform to figure out the most optimized number of Map functions and Reduce functions for a particular task. For the same amount of workload, extensive experiments show both execution time reduces and the overall efficiency of the program improves at different rates as the number of map functions and reduce functions increases. This paper suppose the discovery of the most optimized number of map and reduce functions can help cooperations and programmers figure out the most optimized solutions.

</details>


### [2] [Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving](https://arxiv.org/abs/2601.00397)
*Amey Agrawal,Mayank Yadav,Sukrit Kumar,Anirudha Agrawal,Garv Ghai,Souradeep Bera,Elton Pinto,Sirish Gambhira,Mohammad Adain,Kasra Sohrab,Chus Antonanzas,Alexey Tumanov*

Main category: cs.DC

TL;DR: Revati是一个时间扭曲仿真器，通过直接执行真实服务系统代码实现性能建模，无需物理GPU，比真实GPU执行快5-17倍，预测误差小于5%


<details>
  <summary>Details</summary>
Motivation: 部署LLM需要测试数百种服务配置，但在GPU集群上评估每个配置需要数小时和数千美元成本。离散事件仿真器虽然更快更便宜，但需要重新实现服务系统的控制逻辑，随着框架演进负担加重。

Method: Revati是一个时间扭曲仿真器，通过拦截CUDA API调用来虚拟化设备管理，允许服务框架在没有物理GPU的情况下运行。系统不执行GPU内核，而是执行时间跳跃——通过预测的内核持续时间快速推进虚拟时间。提出了一种协调协议，在分布式进程中同步这些时间跳跃，同时保持因果关系。

Result: 在vLLM和SGLang上，Revati在多个模型和并行配置下实现了小于5%的预测误差，同时运行速度比真实GPU执行快5-17倍。

Conclusion: Revati通过直接执行真实服务系统代码实现快速准确的性能建模，解决了传统仿真器需要重新实现控制逻辑的问题，为LLM服务配置优化提供了高效的工具。

Abstract: Deploying LLMs efficiently requires testing hundreds of serving configurations, but evaluating each one on a GPU cluster takes hours and costs thousands of dollars. Discrete-event simulators are faster and cheaper, but they require re-implementing the serving system's control logic -- a burden that compounds as frameworks evolve.
  We present Revati, a time-warp emulator that enables performance modeling by directly executing real serving system code at simulation-like speed. The system intercepts CUDA API calls to virtualize device management, allowing serving frameworks to run without physical GPUs. Instead of executing GPU kernels, it performs time jumps -- fast-forwarding virtual time by predicted kernel durations. We propose a coordination protocol that synchronizes these jumps across distributed processes while preserving causality. On vLLM and SGLang, Revati achieves less than 5% prediction error across multiple models and parallelism configurations, while running 5-17x faster than real GPU execution.

</details>


### [3] [Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure](https://arxiv.org/abs/2601.00530)
*Ravi Teja Pagidoju*

Main category: cs.DC

TL;DR: 本文提出了一种系统、可重复的云POS系统性能比较方法，在GCP和Azure上测试零售工作负载，发现GCP响应时间快23%，Azure成本效率高71.9%。


<details>
  <summary>Details</summary>
Motivation: 零售业数字化转型加速了基于云的POS系统采用，但缺乏针对零售工作负载的平台特定性能实证研究，特别是小型零售商和研究人员可用的透明评估方法。

Method: 使用免费层云资源，通过实时API端点和开源基准测试代码，在GCP和Azure上部署POS工作负载，测量响应延迟、吞吐量、可扩展性等性能指标，并根据实际资源使用和当前公有云定价估算运营成本。

Result: GCP在基准负载下实现23.0%更快的响应时间，而Azure在稳态操作中显示71.9%更高的成本效率。所有表格和图表直接从代码输出生成，确保实验数据与报告结果一致。

Conclusion: 本研究建立了强大的零售云应用开放基准测试方法，首次提供了跨领先云平台的POS系统特有工作负载的全面、代码驱动比较，为商家考虑云POS实施提供了有用框架。

Abstract: Althoughthereislittleempiricalresearchonplatform-specific performance for retail workloads, the digital transformation of the retail industry has accelerated the adoption of cloud-based Point-of-Sale (POS) systems. This paper presents a systematic, repeatable comparison of POS workload deployments on Google Cloud Platform (GCP) and Microsoft Azure using real-time API endpoints and open-source benchmarking code. Using free-tier cloud resources, we offer a transparent methodology for POS workload evaluation that small retailers and researchers can use. Our approach measures important performance metrics like response latency, throughput, and scalability while estimating operational costs based on actual resource usage and current public cloud pricing because there is no direct billing under free-tier usage. All the tables and figures in this study are generated directly from code outputs, ensuring that the experimental data and the reported results are consistent. Our analysis shows that GCP achieves 23.0% faster response times at baseline load, while Azure shows 71.9% higher cost efficiency for steady-state operations. We look at the architectural components that lead to these differences and provide a helpful framework for merchants considering cloud point-of-sale implementation. This study establishes a strong, open benchmarking methodology for retail cloud applications and offers the first comprehensive, code-driven comparison of workloads unique to point-of-sale systems across leading cloud platforms.

</details>


### [4] [FlexSpec: Frozen Drafts Meet Evolving Targets in Edge-Cloud Collaborative LLM Speculative Decoding](https://arxiv.org/abs/2601.00644)
*Yuchen Li,Rui Kong,Zhonghao Lyu,Qiyang Li,Xinran Chen,Hengyi Cai,Lingyong Yan,Shuaiqiang Wang,Jiashu Zhao,Guangxu Zhu,Linghe Kong,Guihai Chen,Haoyi Xiong,Dawei Yin*

Main category: cs.DC

TL;DR: FlexSpec：一种面向边缘-云协同推理的通信高效框架，通过共享主干架构和信道感知自适应推测机制，解决传统推测解码中模型同步开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 在移动和边缘计算环境中部署大语言模型面临资源有限、无线带宽稀缺和模型频繁更新的挑战。现有的边缘-云协同推理框架基于推测解码技术，但存在边缘与云端模型紧密耦合的问题，导致模型同步带来过多通信开销，增加端到端延迟，限制了推测解码在边缘环境中的可扩展性。

Method: 提出FlexSpec框架，核心设计包括：1）共享主干架构，使单个静态的边缘侧草稿模型能与一系列演化的云端目标模型保持兼容；2）信道感知自适应推测机制，根据实时信道状态信息和设备能量预算动态调整推测草稿长度。

Result: 大量实验表明，FlexSpec在推理效率方面优于传统的推测解码方法，显著减少了通信和维护成本，消除了边缘侧重新训练或重复模型下载的需求。

Conclusion: FlexSpec通过解耦边缘部署与云端模型更新，有效解决了边缘-云协同推理中的通信开销问题，为演化边缘-云系统提供了高效、可扩展的解决方案。

Abstract: Deploying large language models (LLMs) in mobile and edge computing environments is constrained by limited on-device resources, scarce wireless bandwidth, and frequent model evolution. Although edge-cloud collaborative inference with speculative decoding (SD) can reduce end-to-end latency by executing a lightweight draft model at the edge and verifying it with a cloud-side target model, existing frameworks fundamentally rely on tight coupling between the two models. Consequently, repeated model synchronization introduces excessive communication overhead, increasing end-to-end latency, and ultimately limiting the scalability of SD in edge environments. To address these limitations, we propose FlexSpec, a communication-efficient collaborative inference framework tailored for evolving edge-cloud systems. The core design of FlexSpec is a shared-backbone architecture that allows a single and static edge-side draft model to remain compatible with a large family of evolving cloud-side target models. By decoupling edge deployment from cloud-side model updates, FlexSpec eliminates the need for edge-side retraining or repeated model downloads, substantially reducing communication and maintenance costs. Furthermore, to accommodate time-varying wireless conditions and heterogeneous device constraints, we develop a channel-aware adaptive speculation mechanism that dynamically adjusts the speculative draft length based on real-time channel state information and device energy budgets. Extensive experiments demonstrate that FlexSpec achieves superior performance compared to conventional SD approaches in terms of inference efficiency.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [5] [Enhancing Reliability of STT-MRAM Caches by Eliminating Read Disturbance Accumulation](https://arxiv.org/abs/2601.00450)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asadi*

Main category: cs.AR

TL;DR: 提出REAP-cache方案，通过消除并行访问时的读干扰累积，显著提升STT-MRAM缓存的可靠性，将MTTF提高171倍，仅增加少量面积和能耗。


<details>
  <summary>Details</summary>
Motivation: STT-MRAM作为SRAM替代品具有高密度、低功耗等优势，但存在高读干扰错误率问题。传统ECC方案在并行访问缓存时，未请求的块在ECC检查前会累积读干扰错误，严重降低缓存可靠性。

Method: 提出REAP-cache方案，通过防止读干扰累积来完全消除该问题。方案简单有效，在不影响缓存性能的前提下，解决了并行访问时未检查ECC块的错误累积问题。

Result: REAP-cache将缓存平均故障时间（MTTF）提高171倍，同时仅增加不到1%的缓存面积和2.7%的能耗消耗。

Conclusion: REAP-cache方案能有效解决STT-MRAM缓存中的读干扰累积问题，显著提升可靠性，且硬件开销极小，具有实际应用价值。

Abstract: Spin-Transfer Torque Magnetic RAM (STT-MRAM) as one of the most promising replacements for SRAMs in on-chip cache memories benefits from higher density and scalability, near-zero leakage power, and non-volatility, but its reliability is threatened by high read disturbance error rate. Error-Correcting Codes (ECCs) are conventionally suggested to overcome the read disturbance errors in STT-MRAM caches. By employing aggressive ECCs and checking out a cache block on every read access, a high level of cache reliability is achieved. However, to minimize the cache access time in modern processors, all blocks in the target cache set are simultaneously read in parallel for tags comparison operation and only the requested block is sent out, if any, after checking its ECC. These extra cache block reads without checking their ECCs until requesting the blocks by the processor cause the accumulation of read disturbance error, which significantly degrade the cache reliability. In this paper, we first introduce and formulate the read disturbance accumulation phenomenon and reveal that this accumulation due to conventional parallel accesses of cache blocks significantly increases the cache error rate. Then, we propose a simple yet effective scheme, so-called Read Error Accumulation Preventer cache (REAP-cache), to completely eliminate the accumulation of read disturbances without compromising the cache performance. Our evaluations show that the proposed REAP-cache extends the cache Mean Time To Failure (MTTF) by 171x, while increases the cache area by less than 1% and energy consumption by only 2.7%.

</details>


### [6] [ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches](https://arxiv.org/abs/2601.00456)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asadi*

Main category: cs.AR

TL;DR: 针对STT-MRAM缓存中数据依赖性错误模式，提出ROBIN ECC配置方案，相比传统ECC显著提升纠错能力


<details>
  <summary>Details</summary>
Motivation: STT-MRAM作为片上缓存有前景，但高错误率是主要限制因素。传统ECC因数据依赖性错误模式而效率降低，需要改进纠错能力

Method: 首先全面分析传统ECC效率下降原因，然后提出名为ROBIN的高效ECC配置方案，专门针对数据依赖性错误模式进行优化

Result: 评估显示传统ECC低效使缓存错误率平均增加151.7%，而ROBIN将此值降低超过28.6倍，显著提升纠错能力

Conclusion: ROBIN ECC配置能有效解决STT-MRAM中数据依赖性错误问题，显著降低缓存错误率，为STT-MRAM在片上缓存应用提供可行解决方案

Abstract: Spin-Transfer Torque Magnetic RAM} (STT-MRAM) is a promising alternative for SRAMs in on-chip cache memories. Besides all its advantages, high error rate in STT-MRAM is a major limiting factor for on-chip cache memories. In this paper, we first present a comprehensive analysis that reveals that the conventional Error-Correcting Codes (ECCs) lose their efficiency due to data-dependent error patterns, and then propose an efficient ECC configuration, so-called ROBIN, to improve the correction capability. The evaluations show that the inefficiency of conventional ECC increases the cache error rate by an average of 151.7% while ROBIN reduces this value by more than 28.6x.

</details>
