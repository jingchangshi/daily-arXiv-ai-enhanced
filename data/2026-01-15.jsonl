{"id": "2601.09114", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09114", "abs": "https://arxiv.org/abs/2601.09114", "authors": ["Yufan Xia", "Marco De La Pierre", "Amanda S. Barnard", "Giuseppe Maria Junior Barca"], "title": "A Machine Learning Approach Towards Runtime Optimisation of Matrix Multiplication", "comment": "2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "summary": "The GEneral Matrix Multiplication (GEMM) is one of the essential algorithms in scientific computing. Single-thread GEMM implementations are well-optimised with techniques like blocking and autotuning. However, due to the complexity of modern multi-core shared memory systems, it is challenging to determine the number of threads that minimises the multi-thread GEMM runtime. We present a proof-of-concept approach to building an Architecture and Data-Structure Aware Linear Algebra (ADSALA) software library that uses machine learning to optimise the runtime performance of BLAS routines. More specifically, our method uses a machine learning model on-the-fly to automatically select the optimal number of threads for a given GEMM task based on the collected training data. Test results on two different HPC node architectures, one based on a two-socket Intel Cascade Lake and the other on a two-socket AMD Zen 3, revealed a 25 to 40 per cent speedup compared to traditional GEMM implementations in BLAS when using GEMM of memory usage within 100 MB."}
{"id": "2601.09146", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.09146", "abs": "https://arxiv.org/abs/2601.09146", "authors": ["Lingkang Shangguan"], "title": "Transaction-Driven Dynamic Reconfiguration for Certificate-Based Payment Systems", "comment": "draft initial version", "summary": "We present a transaction-driven dynamic reconfiguration protocol in Modern payment systems based on Byzantine Consistent Broadcast which can achieve high performance by avoiding global transaction ordering. We demonstrate the fundamental paradigm of modern payment systems, which combines user nonce based transactions ordering with periodic system-wide consensus mechanisms. Building on this foundation, we design PDCC(Payment Dynamic Config Change), which can lead a smooth reconfiguration process without impacting the original system's performance."}
{"id": "2601.09184", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.09184", "abs": "https://arxiv.org/abs/2601.09184", "authors": ["Yifei Xie", "Btissam Er-Rahmadi", "Xiao Chen", "Tiejun Ma", "Jane Hillston"], "title": "Optimizing View Change for Byzantine Fault Tolerance in Parallel Consensus", "comment": null, "summary": "The parallel Byzantine Fault Tolerant (BFT) protocol is viewed as a promising solution to address the consensus scalability issue of the permissioned blockchain. One of the main challenges in parallel BFT is the view change process that happens when the leader node fails, which can lead to performance bottlenecks. Existing parallel BFT protocols typically rely on passive view change mechanisms with blind leader rotation. Such approaches frequently select unavailable or slow nodes as leaders, resulting in degraded performance. To address these challenges, we propose a View Change Optimization (VCO) model based on mixed integer programming that optimizes leader selection and follower reassignment across parallel committees by considering communication delays and failure scenarios. We applied a decomposition method with efficient subproblems and improved benders cuts to solve the VCO model. Leveraging the results of improved decomposition solution method, we propose an efficient iterative backup leader selection algorithm as views proceed. By performing experiments in Microsoft Azure cloud environments, we demonstrate that the VCO-driven parallel BFT outperforms existing configuration methods under both normal operation and faulty condition. The results show that the VCO model is effective as network size increases, making it a suitable solution for high-performance parallel BFT systems."}
{"id": "2601.09002", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.09002", "abs": "https://arxiv.org/abs/2601.09002", "authors": ["Peter M. Kogge"], "title": "Annotated PIM Bibliography", "comment": "Initial version. Will be updated with more references and detail in future releases", "summary": "Processing in Memory (PIM) and similar terms such as Compute In Memory (CIM), Logic in Memory (LIM), In Memory Computing (IMC), and Near Memory Computing (NMC) have gained attention recently as a potentially ``revolutionary new'' technique. The truth, however, is that many examples of the technology go back over 60 years. This document attempts to provide an annotated bibliography of PIM technology that attempts to cover the whole time-frame, and is organized to augment a forth-coming article."}
{"id": "2601.09258", "categories": ["cs.DC", "cs.LG", "cs.OS"], "pdf": "https://arxiv.org/pdf/2601.09258", "abs": "https://arxiv.org/abs/2601.09258", "authors": ["Du Yin", "Jiayi Ren", "Xiayu Sun", "Tianyao Zhou", "Haizhu Zhou", "Ruiyan Ma", "Danyang Zhang"], "title": "LatencyPrism: Online Non-intrusive Latency Sculpting for SLO-Guaranteed LLM Inference", "comment": "12 pages, 6 figures", "summary": "LLM inference latency critically determines user experience and operational costs, directly impacting throughput under SLO constraints. Even brief latency spikes degrade service quality despite acceptable average performance. However, distributed inference environments featuring diverse software frameworks and XPU architectures combined with dynamic workloads make latency analysis challenging. Constrained by intrusive designs that necessitate service restarts or even suspension, and by hardware-bound implementations that fail to adapt to heterogeneous inference environments, existing AI profiling methods are often inadequate for real-time production analysis.\n  We present LatencyPrism, the first zero-intrusion multi-platform latency sculpting system. It aims to break down the inference latency across pipeline, proactively alert on inference latency anomalies, and guarantee adherence to SLOs, all without requiring code modifications or service restarts. LatencyPrism has been deployed across thousands of XPUs for over six months. It enables low-overhead real-time monitoring at batch level with alerts triggered in milliseconds. This approach distinguishes between workload-driven latency variations and anomalies indicating underlying issues with an F1-score of 0.98. We also conduct extensive experiments and investigations into root cause analysis to demonstrate LatencyPrism's capability."}
{"id": "2601.09217", "categories": ["cs.PL", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.09217", "abs": "https://arxiv.org/abs/2601.09217", "authors": ["Izumi Tanaka", "Ken Sakayori", "Shinya Takamaeda-Yamazaki", "Naoki Kobayashi"], "title": "Relational Hoare Logic for High-Level Synthesis of Hardware Accelerators", "comment": "An extended version of the paper to appear in Proceedings of ESOP 2026", "summary": "High-level synthesis (HLS) is a powerful tool for developing efficient hardware accelerators that rely on specialized memory systems to achieve sufficient on-chip data reuse and off-chip bandwidth utilization. However, even with HLS, designing such systems still requires careful manual tuning, as automatic optimizations provided by existing tools are highly sensitive to programming style and often lack transparency. To address these issues, we present a formal translation framework based on relational Hoare logic, which enables robust and transparent transformations. Our method recognizes complex memory access patterns in na√Øve HLS programs and automatically transforms them by inserting on-chip buffers to enforce linear access to off-chip memory, and by replacing non-sequential processing with stream processing, while preserving program semantics. Experiments using our prototype translator, combined with an off-the-shelf HLS compiler and a real FPGA board, have demonstrated significant performance improvements."}
{"id": "2601.09334", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.09334", "abs": "https://arxiv.org/abs/2601.09334", "authors": ["Valerio Besozzi", "Matteo Della Bartola", "Patrizio Dazzi", "Marco Danelutto"], "title": "High-Performance Serverless Computing: A Systematic Literature Review on Serverless for HPC, AI, and Big Data", "comment": null, "summary": "The widespread deployment of large-scale, compute-intensive applications such as high-performance computing, artificial intelligence, and big data is leading to convergence between cloud and high-performance computing infrastructures. Cloud providers are increasingly integrating high-performance computing capabilities in their infrastructures, such as hardware accelerators and high-speed interconnects, while researchers in the high-performance computing community are starting to explore cloud-native paradigms to improve scalability, elasticity, and resource utilization. In this context, serverless computing emerges as a promising execution model to efficiently handle highly dynamic, parallel, and distributed workloads. This paper presents a comprehensive systematic literature review of 122 research articles published between 2018 and early 2025, exploring the use of the serverless paradigm to develop, deploy, and orchestrate compute-intensive applications across cloud, high-performance computing, and hybrid environments. From these, a taxonomy comprising eight primary research directions and nine targeted use case domains is proposed, alongside an analysis of recent publication trends and collaboration networks among authors, highlighting the growing interest and interconnections within this emerging research field. Overall, this work aims to offer a valuable foundation for both new researchers and experienced practitioners, guiding the development of next-generation serverless solutions for parallel compute-intensive applications."}
{"id": "2601.09583", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.09583", "abs": "https://arxiv.org/abs/2601.09583", "authors": ["Berke Ates", "Philipp Schaad", "Timo Schneider", "Alexandru Calotoiu", "Torsten Hoefler"], "title": "MLIR-Forge: A Modular Framework for Language Smiths", "comment": null, "summary": "Optimizing compilers are essential for the efficient and correct execution of software across various scientific fields. Domain-specific languages (DSL) typically use higher level intermediate representations (IR) in their compiler pipelines for domain-specific optimizations. As these IRs add to complexity, it is crucial to test them thoroughly. Random program generators have proven to be an effective tool to test compilers through differential and fuzz testing. However, developing specialized program generators for compiler IRs is not straightforward and demands considerable resources. We introduce MLIR-Forge, a novel random program generator framework that leverages the flexibility of MLIR, aiming to simplify the creation of specialized program generators. MLIR-Forge achieves this by splitting the generation process into fundamental building blocks that are language specific, and reusable program creation logic that constructs random programs from these building blocks. This hides complexity and furthermore, even the language specific components can be defined using a set of common tools. We demonstrate MLIR-Forge's capabilities by generating MLIR with built-in dialects, WebAssembly, and a data-centric program representation, DaCe -- requiring less than a week of development time in total for each of them. Using the generated programs we conduct differential testing and find 9 MLIR, 15 WebAssembly, and 774 DaCe groups of bugs with the corresponding program generators, after running them until the rate of new bugs stagnates."}
