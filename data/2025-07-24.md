<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Hiord: An Approach to the Specification and Verification of Higher-Order (C)LP Programs](https://arxiv.org/abs/2507.17233)
*Marco Ciccalè,Daniel Jurjo-Rivas,Jose F. Morales,Pedro López-García,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: 本文提出了一种用于静态验证高阶(C)LP程序和高阶断言的新方法，通过谓词属性描述高阶参数，并将其简化为一阶属性来进行编译时验证。


<details>
  <summary>Details</summary>
Motivation: 高阶程序构造可以让代码更具表达力和简洁性，但在约束逻辑编程(CLP)中，高阶断言的编译时验证相对缺乏研究。现有的研究主要集中在运行时验证，而静态验证高阶程序和高阶断言的方法仍然不够成熟。

Method: 使用Ciao断言语言，提出谓词属性来描述高阶参数；完善谓词属性的语法和语义；引入基于语义顺序关系的抽象准则来确定编译时的谓词属性一致性；通过将谓词属性简化为一阶属性，使得现有的基于抽象解释的静态分析器能够处理高阶断言。

Result: 开发了原型实现并在Ciao系统中通过多个示例进行了评估，证明了该方法的可行性和有效性。该方法能够将高阶断言验证问题转化为已有的一阶断言分析框架可以处理的问题。

Conclusion: 提出的方法成功实现了高阶(C)LP程序的静态验证，通过谓词属性和语义顺序关系建立了编译时验证高阶断言的理论基础，并通过简化为一阶属性的方式使其能够在现有分析框架中实现，为高阶程序的静态分析提供了新的解决方案。

Abstract: Higher-order constructs enable more expressive and concise code by allowing
procedures to be parameterized by other procedures. Assertions allow expressing
partial program specifications, which can be verified either at compile time
(statically) or run time (dynamically). In higher-order programs, assertions
can also describe higher-order arguments. While in the context of (C)LP,
run-time verification of higher-order assertions has received some attention,
compile-time verification remains relatively unexplored. We propose a novel
approach for statically verifying higher-order (C)LP programs with higher-order
assertions. Although we use the Ciao assertion language for illustration, our
approach is quite general and we believe is applicable to similar contexts.
Higher-order arguments are described using predicate properties -- a special
kind of property which exploits the (Ciao) assertion language. We refine the
syntax and semantics of these properties and introduce an abstract criterion to
determine conformance to a predicate property at compile time, based on a
semantic order relation comparing the predicate property with the predicate
assertions. We then show how to handle these properties using an abstract
interpretation-based static analyzer for programs with first-order assertions
by reducing predicate properties to first-order properties. Finally, we report
on a prototype implementation and evaluate it through various examples within
the Ciao system.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Mapple: A Domain-Specific Language for Mapping Distributed Heterogeneous Parallel Programs](https://arxiv.org/abs/2507.17087)
*Anjiang Wei,Rohan Yadav,Hang Song,Wonchan Lee,Ke Wang,Alex Aiken*

Main category: cs.DC

TL;DR: 本文介绍了Mapple，一个用于分布式应用映射的高级声明式编程接口，通过提供转换原语（特别是decompose原语）来解决迭代空间和处理器空间之间的维度不匹配问题，显著简化了mapper代码开发并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式异构系统并行程序优化复杂且需要大量代码修改，基于任务的编程系统虽然提高了模块化程度，但其映射接口往往过于底层，缺乏高级抽象，导致开发高性能mapper困难。

Method: 设计并实现了Mapple，一个高级声明式编程接口，提供转换原语来解决迭代空间和处理器空间的维度不匹配问题，特别是decompose原语用于最小化通信量。在Legion运行时上实现Mapple，将Mapple映射器转换为底层C++接口。

Result: 在九个应用（包括六种矩阵乘法算法和三个科学计算工作负载）上的实验表明，Mapple将mapper代码大小减少了14倍，相比专家编写的C++ mapper实现了高达1.34倍的性能提升。decompose原语相比现有维度解决启发式方法实现了高达1.83倍的改进。

Conclusion: Mapple通过提供高级声明式接口和有效的转换原语，成功简化了分布式应用高性能mapper的开发过程，同时保持或提升了性能，证明了高级抽象在并行编程中的有效性。

Abstract: Optimizing parallel programs for distributed heterogeneous systems remains a
complex task, often requiring significant code modifications. Task-based
programming systems improve modularity by separating performance decisions from
core application logic, but their mapping interfaces are often too low-level.
In this work, we introduce Mapple, a high-level, declarative programming
interface for mapping distributed applications. Mapple provides transformation
primitives to resolve dimensionality mismatches between iteration and processor
spaces, including a key primitive, decompose, that helps minimize communication
volume. We implement Mapple on top of the Legion runtime by translating Mapple
mappers into its low-level C++ interface. Across nine applications, including
six matrix multiplication algorithms and three scientific computing workloads,
Mapple reduces mapper code size by 14X and enables performance improvements of
up to 1.34X over expert-written C++ mappers. In addition, the decompose
primitive achieves up to 1.83X improvement over existing
dimensionality-resolution heuristics. These results demonstrate that Mapple
simplifies the development of high-performance mappers for distributed
applications.

</details>


### [3] [PathWeaver: A High-Throughput Multi-GPU System for Graph-Based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2507.17094)
*Sukjin Kim,Seongyeon Park,Si Ung Noh,Junguk Hong,Taehee Kwon,Hunseong Lim,Jinho Lee*

Main category: cs.DC

TL;DR: 本文提出PathWeaver，一个新颖的多GPU框架，通过流水线路径扩展、幽灵暂存和方向引导选择三项技术，显著提升大规模数据集上基于图的近似最近邻搜索性能，相比现有多GPU方案实现3.24倍几何平均加速和最高5.30倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有GPU加速的近似最近邻搜索方法在设计时忽略了多GPU可扩展性，仅将额外GPU视为扩展内存容量的手段，采用朴素的数据集分割方法在各GPU上独立搜索，导致效率低下。随着数据集规模不断增长，迫切需要高效的多GPU解决方案。

Method: 提出PathWeaver多GPU框架，包含三个核心技术：1）基于流水线的路径扩展-利用GPU间通信减少冗余搜索迭代的GPU感知流水线机制；2）幽灵暂存-利用代表性数据集识别最优查询起始点，减少困难查询的搜索空间；3）方向引导选择-在搜索过程早期过滤无关点的数据选择技术，最小化不必要的内存访问和距离计算。

Result: 在多个数据集上的综合评估显示，PathWeaver相比最先进的多GPU近似最近邻搜索框架实现了3.24倍的几何平均加速，在95%召回率下最高达到5.30倍加速。

Conclusion: PathWeaver成功解决了现有多GPU近似最近邻搜索方法的可扩展性问题，通过三项创新技术显著提升了大规模数据集上的搜索性能，为推荐系统、自然语言处理和计算机视觉等应用中的近似最近邻搜索提供了高效的多GPU解决方案。

Abstract: Graph-based Approximate Nearest Neighbor Search (ANNS) is widely adopted in
numerous applications, such as recommendation systems, natural language
processing, and computer vision. While recent works on GPU-based acceleration
have significantly advanced ANNS performance, the ever-growing scale of
datasets now demands efficient multi-GPU solutions. However, the design of
existing works overlooks multi-GPU scalability, resulting in naive approaches
that treat additional GPUs as a means to extend memory capacity for large
datasets. This inefficiency arises from partitioning the dataset and
independently searching for data points similar to the queries in each GPU. We
therefore propose PathWeaver, a novel multi-GPU framework designed to scale and
accelerate ANNS for large datasets. First, we propose pipelining-based path
extension, a GPU-aware pipelining mechanism that reduces prior work's redundant
search iterations by leveraging GPU-to-GPU communication. Second, we design
ghost staging that leverages a representative dataset to identify optimal query
starting points, reducing the search space for challenging queries. Finally, we
introduce direction-guided selection, a data selection technique that filters
irrelevant points early in the search process, minimizing unnecessary memory
accesses and distance computations. Comprehensive evaluations across diverse
datasets demonstrate that PathWeaver achieves 3.24$\times$ geomean speedup and
up to 5.30$\times$ speedup on 95% recall rate over state-of-the-art
multi-GPU-based ANNS frameworks.

</details>


### [4] [BucketServe: Bucket-Based Dynamic Batching for Smart and Efficient LLM Inference Serving](https://arxiv.org/abs/2507.17120)
*Wanyi Zheng,Minxian Xu,Shengye Song,Kejiang Ye*

Main category: cs.DC

TL;DR: 本文提出了BucketServe，一个基于桶的动态批处理框架，通过将请求按序列长度分组到同质化桶中来优化大语言模型推理性能，显著提升吞吐量并减少延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统使用静态或连续批处理策略，在异构工作负载下会导致GPU内存利用率低效和延迟增加，难以适应动态工作负载波动，造成吞吐量次优和SLO违反问题。

Method: 设计BucketServe框架，根据序列长度将请求分组到大小同质的桶中，通过实时批大小调整优化GPU内存使用，引入自适应桶分割/合并和优先级感知调度机制。

Result: BucketServe在吞吐量方面显著优于UELLM，提升高达3.58倍；在80% SLO达成率下能处理比DistServe多1.93倍的请求负载；系统负载容量比UELLM高1.975倍。

Conclusion: BucketServe通过基于桶的动态批处理有效解决了LLM推理中的资源利用和延迟问题，在多个性能指标上都实现了显著改进，为LLM服务系统提供了更优的解决方案。

Abstract: Large language models (LLMs) have become increasingly popular in various
areas, traditional business gradually shifting from rule-based systems to
LLM-based solutions. However, the inference of LLMs is resource-intensive or
latency-sensitive, posing significant challenges for serving systems. Existing
LLM serving systems often use static or continuous batching strategies, which
can lead to inefficient GPU memory utilization and increased latency,
especially under heterogeneous workloads. These methods may also struggle to
adapt to dynamic workload fluctuations, resulting in suboptimal throughput and
potential service level objective (SLO) violations. In this paper, we introduce
BucketServe, a bucket-based dynamic batching framework designed to optimize LLM
inference performance. By grouping requests into size-homogeneous buckets based
on sequence length, BucketServe minimizes padding overhead and optimizes GPU
memory usage through real-time batch size adjustments preventing out-of-memory
(OOM) errors. It introduces adaptive bucket splitting/merging and
priority-aware scheduling to mitigate resource fragmentation and ensure SLO
compliance. Experiment shows that BucketServe significantly outperforms UELLM
in throughput, achieving up to 3.58x improvement. It can also handle 1.93x more
request load under the SLO attainment of 80% compared with DistServe and
demonstrates 1.975x higher system load capacity compared to the UELLM.

</details>


### [5] [Auto-scaling Approaches for Cloud-native Applications: A Survey and Taxonomy](https://arxiv.org/abs/2507.17128)
*Minxian Xu,Linfeng Wen,Junhan Liao,Huaming Wu,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 本文系统性回顾了2020年以来云原生应用自动扩缩容的最新方法，提出了从基础设施、架构、扩缩容方法、优化目标和行为建模五个维度的分类法，并识别了当前研究空白和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 云原生应用具有复杂的交互关系、服务数量和负载不断变化，对自动扩缩容方法提出了更高要求，涉及微服务依赖分析、性能分析、异常检测、工作负载特征化和任务协同定位等多项挑战，需要系统性梳理现有研究进展。

Method: 系统性文献综述方法，从2020年开始收集云原生应用自动扩缩容的最新研究，提出五维度分类法（基础设施、架构、扩缩容方法、优化目标、行为建模），对各种方法进行全面比较和深入讨论，分析其关键特征、优势、局限性和应用场景。

Result: 建立了云原生应用自动扩缩容方法的详细分类体系，全面比较了各种方法在不同环境和条件下的性能表现，识别了当前研究领域的空白和未解决的挑战，特别是在大模型应用、微服务依赖管理和元学习技术方面。

Conclusion: 总结了该领域的研究现状，强调了未来有前景的探索方向，特别是大模型的应用、微服务依赖管理以及元学习技术的使用，以增强模型在不同环境中的适用性和适应性。

Abstract: The interactions within cloud-native applications are complex, with a
constantly changing number of services and loads, posing higher demands on
auto-scaling approach. This mainly involves several challenges such as
microservices dependency analysis, performance profiling, anomaly detection,
workload characterization and task co-location. Therefore, some advanced
algorithms have been investigated into auto-scaling cloud-native applications
to optimize system and application performance. These algorithms can learn from
historical data and appropriately adjust resource allocation based on the
current environment and load conditions to optimize resource utilization and
system performance. In this paper, we systematically review the literature on
state-of-the-art auto-scaling approaches for cloud-native applications from
2020, and further explore the technological evolution. Additionally, we propose
a detailed taxonomy to categorize current research from five perspectives,
including infrastructure, architecture, scaling methods, optimization
objectives, and behavior modeling. Then, we provide a comprehensive comparison
and in-depth discussion of the key features, advantages, limitations, and
application scenarios of each approach, considering their performance in
diverse environments and under various conditions. Finally, we summarize the
current state of research in this field, identify the gaps and unresolved
challenges, and emphasize promising directions for future exploration,
particularly in areas such as the application of large models, microservice
dependency management, and the use of meta-learning techniques to enhance model
applicability and adaptability across different environments.

</details>


### [6] [BrownoutServe: SLO-Aware Inference Serving under Bursty Workloads for MoE-based LLMs](https://arxiv.org/abs/2507.17133)
*Jianmin Hu,Minxian Xu,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 本文提出了BrownoutServe框架，通过"联合专家"和动态brownout机制优化MoE大语言模型的推理效率，在突发流量下实现了最高2.07倍的吞吐量提升和90.28%的SLO违规减少。


<details>
  <summary>Details</summary>
Motivation: 现有MoE架构的大语言模型服务系统存在静态模型部署和缺乏动态工作负载适应性的问题，导致资源利用率低下、延迟增加，特别是在突发请求期间表现不佳。

Method: 提出BrownoutServe框架，包含两个核心技术：1）"联合专家"机制，整合多个专家的知识以减少专家访问次数和推理延迟；2）动态brownout机制，自适应调整特定token的处理过程，在保证服务水平目标的同时优化推理性能。

Result: 实验结果显示BrownoutServe在各种工作负载下表现出色：相比vLLM实现了最高2.07倍的吞吐量提升，SLO违规减少了90.28%，在突发流量下展现出强大的鲁棒性，同时保持了可接受的推理准确性。

Conclusion: BrownoutServe成功解决了MoE大语言模型在动态计算需求和流量条件下的推理效率和服务可靠性问题，为MoE架构的实际部署提供了有效的解决方案。

Abstract: In recent years, the Mixture-of-Experts (MoE) architecture has been widely
applied to large language models (LLMs), providing a promising solution that
activates only a subset of the model's parameters during computation, thereby
reducing overall memory requirements and allowing for faster inference compared
to dense models. Despite these advantages, existing systems still face issues
of low efficiency due to static model placement and lack of dynamic workloads
adaptation. This leads to suboptimal resource utilization and increased
latency, especially during bursty requests periods.
  To address these challenges, this paper introduces BrownoutServe, a novel
serving framework designed to optimize inference efficiency and maintain
service reliability for MoE-based LLMs under dynamic computational demands and
traffic conditions. BrownoutServe introduces "united experts" that integrate
knowledge from multiple experts, reducing the times of expert access and
inference latency. Additionally, it proposes a dynamic brownout mechanism to
adaptively adjust the processing of certain tokens, optimizing inference
performance while guaranteeing service level objectives (SLOs) are met. Our
evaluations show the effectiveness of BrownoutServe under various workloads: it
achieves up to 2.07x throughput improvement compared to vLLM and reduces SLO
violations by 90.28%, showcasing its robustness under bursty traffic while
maintaining acceptable inference accuracy.

</details>


### [7] [Efficient Column-Wise N:M Pruning on RISC-V CPU](https://arxiv.org/abs/2507.17301)
*Chi-Wei Chu,Ding-Yong Hong,Jan-Jan Wu*

Main category: cs.DC

TL;DR: 本文提出了一种基于RISC-V向量架构的列级N:M剪枝策略，通过修改XNNPACK框架和融合im2col与数据打包操作，实现了ResNet推理吞吐量4.0倍的提升，同时保持ImageNet准确率损失在2.1%以内。


<details>
  <summary>Details</summary>
Motivation: 深度学习框架中权重剪枝技术对提高计算效率至关重要，特别是对于作为CNN性能瓶颈的卷积算子。然而，不同的剪枝实现方法会显著影响计算性能和内存占用，因此需要开发更高效的剪枝策略和执行框架。

Method: 提出了tile级别的列级N:M剪枝策略，修改XNNPACK以支持RISC-V向量架构上的高效剪枝模型执行，融合im2col和数据打包操作以减少冗余内存访问，并结合AITemplate的性能分析技术为每个卷积算子选择最优实现。

Result: 该方法使ResNet推理吞吐量提升高达4.0倍，同时ImageNet top-1准确率相比稠密基线模型仅下降2.1%以内，有效平衡了性能提升和精度保持。

Conclusion: 通过在RISC-V向量架构上实现高效的列级N:M剪枝策略，结合框架优化和算子融合技术，成功实现了显著的推理性能提升，为边缘设备上的深度学习模型部署提供了有效的解决方案。

Abstract: In deep learning frameworks, weight pruning is a widely used technique for
improving computational efficiency by reducing the size of large models. This
is especially critical for convolutional operators, which often act as
performance bottlenecks in convolutional neural networks (CNNs). However, the
effectiveness of pruning heavily depends on how it is implemented, as different
methods can significantly impact both computational performance and memory
footprint. In this work, we propose a column-wise N:M pruning strategy applied
at the tile level and modify XNNPACK to enable efficient execution of pruned
models on the RISC-V vector architecture. Additionally, we propose fusing the
operations of im2col and data packing to minimize redundant memory accesses and
memory overhead. To further optimize performance, we incorporate AITemplate's
profiling technique to identify the optimal implementation for each
convolutional operator. Our proposed approach effectively increases ResNet
inference throughput by as much as 4.0x, and preserves ImageNet top-1 accuracy
within 2.1\% of the dense baseline.

</details>


### [8] [Multiprocessor Scheduling with Memory Constraints: Fundamental Properties and Finding Optimal Solutions](https://arxiv.org/abs/2507.17411)
*Pál András Papp,Toni Böhnlein,A. N. Yzelman*

Main category: cs.DC

TL;DR: 本文研究了在双层内存层次结构的多处理器系统上调度计算DAG的问题，提出了基于整数线性规划(ILP)的整体调度算法，能够同时优化负载平衡、通信和数据移动，性能显著优于传统的分离式优化方法。


<details>
  <summary>Details</summary>
Motivation: 现有的并行化和内存管理分离优化方法在实际应用中表现不佳，可能产生与最优解相差线性倍数的结果。需要一种能够同时考虑工作负载平衡、通信开销和缓存大小限制导致的数据移动的整体调度方法。

Method: 采用整数线性规划(ILP)方法来表示和求解调度问题，开发了一种基于ILP的整体调度算法，能够统一处理并行化调度和内存管理，而不是将它们分别优化。

Result: 实验结果表明，基于ILP的方法能够找到比结合经典调度算法和内存管理策略的基准方法显著更好的解决方案，验证了整体优化方法的有效性。

Conclusion: 在双层内存层次结构的多处理器系统上，整体调度方法优于分离式优化方法。基于ILP的调度算法能够有效处理复杂的约束条件，为计算DAG调度问题提供了一种新的解决思路。

Abstract: We study the problem of scheduling a general computational DAG on multiple
processors in a 2-level memory hierarchy. This setting is a natural
generalization of several prominent models in the literature, and it
simultaneously captures workload balancing, communication, and data movement
due to cache size limitations. We first analyze the fundamental properties of
this problem from a theoretical perspective, such as its computational
complexity. We also prove that optimizing parallelization and memory management
separately, as done in many applications, can result in a solution that is a
linear factor away from the optimum.
  On the algorithmic side, we discuss a natural technique to represent and
solve the problem as an Integer Linear Program (ILP). We develop a holistic
scheduling algorithm based on this approach, and we experimentally study its
performance and properties on a small benchmark of computational tasks. Our
results confirm that the ILP-based method can indeed find considerably better
solutions than a baseline which combines classical scheduling algorithms and
memory management policies.

</details>


### [9] [Distributed P2P quantile tracking with relative value error](https://arxiv.org/abs/2507.17458)
*Marco Pulimeno,Italo Epicoco,Massimo Cafaro*

Main category: cs.DC

TL;DR: 本文提出了DUDDSketch算法，这是UDDSketch算法的分布式版本，用于在非结构化P2P网络中准确追踪分位数，采用完全去中心化的gossip协议实现


<details>
  <summary>Details</summary>
Motivation: 现有的分位数追踪算法主要是集中式的，需要开发一种能够在分布式环境下，特别是在非结构化P2P网络中准确追踪分位数的算法

Method: 设计了DUDDSketch算法，这是一个完全去中心化的、基于gossip的分布式协议，能够在非结构化P2P网络环境中工作，并对算法进行了形式化正确性证明

Result: 通过大量实验验证，DUDDSketch算法能够收敛到与顺序算法相同的结果，证明了算法的有效性和可靠性

Conclusion: DUDDSketch算法成功实现了分布式环境下的准确分位数追踪，具有完全去中心化和收敛性保证的特点，为分布式系统中的分位数计算提供了可行的解决方案

Abstract: In this paper we present \textsc{DUDDSketch}, a distributed version of the
\textsc{UDDSketch} algorithm for accurate tracking of quantiles. The algorithm
is a fully decentralized, gossip-based distributed protocol working in the
context of unstructured P2P networks. We discuss the algorithm's design and
formally prove its correctness. We also show, through extensive experimental
results, that the algorithm converges to the results provided by the sequential
algorithm, which is a fundamental and highly desirable property.

</details>
