{"id": "2508.10303", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.10303", "abs": "https://arxiv.org/abs/2508.10303", "authors": ["Arkapravo Ghosh", "Abhishek Moitra", "Abhiroop Bhattacharjee", "Ruokai Yin", "Priyadarshini Panda"], "title": "DiffAxE: Diffusion-driven Hardware Accelerator Generation and Design Space Exploration", "comment": "14 pages, 24 figures, 8 tables, 54 references", "summary": "Design space exploration (DSE) is critical for developing optimized hardware\narchitectures, especially for AI workloads such as deep neural networks (DNNs)\nand large language models (LLMs), which require specialized acceleration. As\nmodel complexity grows, accelerator design spaces have expanded to O(10^17),\nbecoming highly irregular, non-convex, and exhibiting many-to-one mappings from\ndesign configurations to performance metrics. This complexity renders direct\ninverse derivation infeasible and necessitates heuristic or sampling-based\noptimization. Conventional methods - including Bayesian optimization, gradient\ndescent, reinforcement learning, and genetic algorithms - depend on iterative\nsampling, resulting in long runtimes and sensitivity to initialization. Deep\nlearning-based approaches have reframed DSE as classification using\nrecommendation models, but remain limited to small-scale (O(10^3)), less\ncomplex design spaces. To overcome these constraints, we propose a generative\napproach that models hardware design as 1-D image synthesis conditioned on\ntarget performance, enabling efficient learning of non-differentiable,\nnon-bijective hardware-performance mappings. Our framework achieves 0.86% lower\ngeneration error than Bayesian optimization with a 17000x speedup, and\noutperforms GANDSE with 30% lower error at only 1.83x slower search. We further\nextend the method to a structured DSE setting, attaining 9.8% lower\nenergy-delay product (EDP) and 6% higher performance, with up to 145.6x and\n1312x faster search compared to existing optimization methods on O(10^17)\ndesign spaces. For LLM inference, our method achieves 3.37x and 7.75x lower EDP\non a 32nm ASIC and Xilinx Ultrascale+ VPU13 FPGA, respectively, compared to the\nstate-of-the-art DOSA framework."}
{"id": "2508.10409", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10409", "abs": "https://arxiv.org/abs/2508.10409", "authors": ["Zihao Chen", "Ji Zhuang", "Jinyi Shen", "Xiaoyue Ke", "Xinyi Yang", "Mingjie Zhou", "Zhuoyao Du", "Xu Yan", "Zhouyang Wu", "Zhenyu Xu", "Jiangli Huang", "Li Shang", "Xuan Zeng", "Fan Yang"], "title": "AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design", "comment": null, "summary": "In this paper, we propose AnalogSeeker, an effort toward an open-source\nfoundation language model for analog circuit design, with the aim of\nintegrating domain knowledge and giving design assistance. To overcome the\nscarcity of data in this field, we employ a corpus collection strategy based on\nthe domain knowledge framework of analog circuits. High-quality, accessible\ntextbooks across relevant subfields are systematically curated and cleaned into\na textual domain corpus. To address the complexity of knowledge of analog\ncircuits, we introduce a granular domain knowledge distillation method. Raw,\nunlabeled domain corpus is decomposed into typical, granular learning nodes,\nwhere a multi-agent framework distills implicit knowledge embedded in\nunstructured text into question-answer data pairs with detailed reasoning\nprocesses, yielding a fine-grained, learnable dataset for fine-tuning. To\naddress the unexplored challenges in training analog circuit foundation models,\nwe explore and share our training methods through both theoretical analysis and\nexperimental validation. We finally establish a fine-tuning-centric training\nparadigm, customizing and implementing a neighborhood self-constrained\nsupervised fine-tuning algorithm. This approach enhances training outcomes by\nconstraining the perturbation magnitude between the model's output\ndistributions before and after training. In practice, we train the\nQwen2.5-32B-Instruct model to obtain AnalogSeeker, which achieves 85.04%\naccuracy on AMSBench-TQA, the analog circuit knowledge evaluation benchmark,\nwith a 15.67% point improvement over the original model and is competitive with\nmainstream commercial models. Furthermore, AnalogSeeker also shows\neffectiveness in the downstream operational amplifier design task. AnalogSeeker\nis open-sourced at https://huggingface.co/analogllm/analogseeker for research\nuse."}
{"id": "2508.10691", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.10691", "abs": "https://arxiv.org/abs/2508.10691", "authors": ["Alish Kanani", "Lukas Pfromm", "Harsh Sharma", "Janardhan Rao Doppa", "Partha Pratim Pande", "Umit Y. Ogras"], "title": "THERMOS: Thermally-Aware Multi-Objective Scheduling of AI Workloads on Heterogeneous Multi-Chiplet PIM Architectures", "comment": "Paper accepted at ESWEEK 2025 (CODES+ISSS) conference", "summary": "Chiplet-based integration enables large-scale systems that combine diverse\ntechnologies, enabling higher yield, lower costs, and scalability, making them\nwell-suited to AI workloads. Processing-in-Memory (PIM) has emerged as a\npromising solution for AI inference, leveraging technologies such as ReRAM,\nSRAM, and FeFET, each offering unique advantages and trade-offs. A\nheterogeneous chiplet-based PIM architecture can harness the complementary\nstrengths of these technologies to enable higher performance and energy\nefficiency. However, scheduling AI workloads across such a heterogeneous system\nis challenging due to competing performance objectives, dynamic workload\ncharacteristics, and power and thermal constraints. To address this need, we\npropose THERMOS, a thermally-aware, multi-objective scheduling framework for AI\nworkloads on heterogeneous multi-chiplet PIM architectures. THERMOS trains a\nsingle multi-objective reinforcement learning (MORL) policy that is capable of\nachieving Pareto-optimal execution time, energy, or a balanced objective at\nruntime, depending on the target preferences. Comprehensive evaluations show\nthat THERMOS achieves up to 89% faster average execution time and 57% lower\naverage energy consumption than baseline AI workload scheduling algorithms with\nonly 0.14% runtime and 0.022% energy overhead."}
{"id": "2508.10781", "categories": ["cs.PL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.10781", "abs": "https://arxiv.org/abs/2508.10781", "authors": ["Abtin Molavi", "Amanda Xu", "Ethan Cecchetti", "Swamit Tannu", "Aws Albarghouthi"], "title": "Generating Compilers for Qubit Mapping and Routing", "comment": null, "summary": "Quantum computers promise to solve important problems faster than classical\ncomputers, potentially unlocking breakthroughs in materials science, chemistry,\nand beyond. Optimizing compilers are key to realizing this potential, as they\nminimize expensive resource usage and limit error rates. A critical compilation\nstep is qubit mapping and routing (QMR), which finds mappings from circuit\nqubits to qubits on a target device and plans instruction execution while\nsatisfying the device's connectivity constraints. The challenge is that the\nlandscape of quantum architectures is incredibly diverse and fast-evolving.\nGiven this diversity, hundreds of papers have addressed the QMR problem for\ndifferent qubit hardware, connectivity constraints, and quantum error\ncorrection schemes.\n  We present an approach for automatically generating qubit mapping and routing\ncompilers for arbitrary quantum architectures. Though each QMR problem is\ndifferent, we identify a common core structure-device state machine-that we use\nto formulate an abstract QMR problem. Our formulation naturally leads to a\ndomain-specific language, Marol, for specifying QMR problems-for example, the\nwell-studied NISQ mapping and routing problem requires only 12 lines of Marol.\nWe demonstrate that QMR problems, defined in Marol, can be solved with a\npowerful parametric solver that can be instantiated for any Marol program. We\nevaluate our approach through case studies of important QMR problems from prior\nand recent work, covering noisy and fault-tolerant quantum architectures on all\nmajor hardware platforms. Our thorough evaluation shows that generated\ncompilers are competitive with handwritten, specialized compilers in terms of\nruntime and solution quality. We envision that our approach will simplify\ndevelopment of future quantum compilers as new quantum architectures continue\nto emerge."}
{"id": "2508.10141", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.10141", "abs": "https://arxiv.org/abs/2508.10141", "authors": ["Laura Lawniczak", "Tobias Distler"], "title": "Hard Shell, Reliable Core: Improving Resilience in Replicated Systems with Selective Hybridization", "comment": "Extended version of publication in 44th International Symposium on\n  Reliable Distributed Systems (SRDS '25)", "summary": "Hybrid fault models are known to be an effective means for enhancing the\nrobustness of consensus-based replicated systems. However, existing\nhybridization approaches suffer from limited flexibility with regard to the\ncomposition of crash-tolerant and Byzantine fault-tolerant system parts and/or\nare associated with a significant diversification overhead. In this paper we\naddress these issues with ShellFT, a framework that leverages the concept of\nmicro replication to allow system designers to freely choose the parts of the\nreplication logic that need to be resilient against Byzantine faults. As a key\nbenefit, such a selective hybridization makes it possible to develop hybrid\nsolutions that are tailored to the specific characteristics and requirements of\nindividual use cases. To illustrate this flexibility, we present three custom\nShellFT protocols and analyze the complexity of their implementations. Our\nevaluation shows that compared with traditional hybridization approaches,\nShellFT is able to decrease diversification costs by more than 70%."}
{"id": "2508.10202", "categories": ["cs.DC", "cs.NA", "cs.PF", "math.NA", "65Y20, 65Y05, 65Y10, 68Q25, 68W40, 65M32, 5B05", "F.2; G.4; C.4"], "pdf": "https://arxiv.org/pdf/2508.10202", "abs": "https://arxiv.org/abs/2508.10202", "authors": ["Sreeram Venkat", "Kasia Swirydowicz", "Noah Wolfe", "Omar Ghattas"], "title": "Mixed-Precision Performance Portability of FFT-Based GPU-Accelerated Algorithms for Block-Triangular Toeplitz Matrices", "comment": null, "summary": "The hardware diversity displayed in leadership-class computing facilities,\nalongside the immense performance boosts exhibited by today's GPUs when\ncomputing in lower precision, provide a strong incentive for scientific HPC\nworkflows to adopt mixed-precision algorithms and performance portability\nmodels. We present an on-the-fly framework using Hipify for performance\nportability and apply it to FFTMatvec-an HPC application that computes\nmatrix-vector products with block-triangular Toeplitz matrices. Our approach\nenables FFTMatvec, initially a CUDA-only application, to run seamlessly on AMD\nGPUs with excellent observed performance. Performance optimizations for AMD\nGPUs are integrated directly into the open-source rocBLAS library, keeping the\napplication code unchanged. We then present a dynamic mixed-precision framework\nfor FFTMatvec; a Pareto front analysis determines the optimal mixed-precision\nconfiguration for a desired error tolerance. Results are shown for AMD Instinct\nMI250X, MI300X, and the newly launched MI355X GPUs. The performance-portable,\nmixed-precision FFTMatvec is scaled to 2,048 GPUs on the OLCF Frontier\nsupercomputer."}
{"id": "2508.10305", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.10305", "abs": "https://arxiv.org/abs/2508.10305", "authors": ["Ruoyu Li", "Yafan Huang", "Longtao Zhang", "Zhuoxun Yang", "Sheng Di", "Jiajun Huang", "Jinyang Liu", "Jiannan Tian", "Xin Liang", "Guanpeng Li", "Hanqi Guo", "Franck Cappello", "Kai Zhao"], "title": "GPZ: GPU-Accelerated Lossy Compressor for Particle Data", "comment": null, "summary": "Particle-based simulations and point-cloud applications generate massive,\nirregular datasets that challenge storage, I/O, and real-time analytics.\nTraditional compression techniques struggle with irregular particle\ndistributions and GPU architectural constraints, often resulting in limited\nthroughput and suboptimal compression ratios. In this paper, we present GPZ, a\nhigh-performance, error-bounded lossy compressor designed specifically for\nlarge-scale particle data on modern GPUs. GPZ employs a novel four-stage\nparallel pipeline that synergistically balances high compression efficiency\nwith the architectural demands of massively parallel hardware. We introduce a\nsuite of targeted optimizations for computation, memory access, and GPU\noccupancy that enables GPZ to achieve near-hardware-limit throughput. We\nconduct an extensive evaluation on three distinct GPU architectures\n(workstation, data center, and edge) using six large-scale, real-world\nscientific datasets from five distinct domains. The results demonstrate that\nGPZ consistently and significantly outperforms five state-of-the-art GPU\ncompressors, delivering up to 8x higher end-to-end throughput while\nsimultaneously achieving superior compression ratios and data quality."}
{"id": "2508.10349", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10349", "abs": "https://arxiv.org/abs/2508.10349", "authors": ["Tianjun Yuan", "Jiaxiang Geng", "Pengchao Han", "Xianhao Chen", "Bing Luo"], "title": "Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models", "comment": "10 pages, Submitted to INFOCOM2026", "summary": "Fine-tuning foundation models is critical for superior performance on\npersonalized downstream tasks, compared to using pre-trained models.\nCollaborative learning can leverage local clients' datasets for fine-tuning,\nbut limited client data and heterogeneous data distributions hinder effective\ncollaboration. To address the challenge, we propose a flexible personalized\nfederated learning paradigm that enables clients to engage in collaborative\nlearning while maintaining personalized objectives. Given the limited and\nheterogeneous computational resources available on clients, we introduce\n\\textbf{flexible personalized split federated learning (FlexP-SFL)}. Based on\nsplit learning, FlexP-SFL allows each client to train a portion of the model\nlocally while offloading the rest to a server, according to resource\nconstraints. Additionally, we propose an alignment strategy to improve\npersonalized model performance on global data. Experimental results show that\nFlexP-SFL outperforms baseline models in personalized fine-tuning efficiency\nand final accuracy."}
{"id": "2508.10481", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.10481", "abs": "https://arxiv.org/abs/2508.10481", "authors": ["Adrien Cassagne", "No√© Amiot", "Manuel Bouyer"], "title": "Dalek: An Unconventional and Energy-Aware Heterogeneous Cluster", "comment": null, "summary": "Dalek is an experimental compute cluster designed to evaluate the performance\nof heterogeneous, consumer-grade hardware for software design, prototyping, and\nalgorithm development. In contrast to traditional computing centers that rely\non costly, server-class components, Dalek integrates CPUs and GPUs typically\nfound in mini-PCs, laptops, and gaming desktops, providing a cost-effective yet\nversatile platform. This document details the cluster's architecture and\nsoftware stack, and presents results from synthetic benchmarks. Furthermore, it\nintroduces a custom energy monitoring platform capable of delivering 1000\naveraged samples per second with milliwatt-level resolution. This\nhigh-precision monitoring capability enables a wide range of energy-aware\nresearch experiments in applied Computer Science."}
{"id": "2508.10854", "categories": ["cs.DC", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.10854", "abs": "https://arxiv.org/abs/2508.10854", "authors": ["Oliver Thomson Brown", "Mateusz Meller", "James Richings"], "title": "Introducing CQ: A C-like API for Quantum Accelerated HPC", "comment": "8 pages, 1 figure. Submitted to the 1st International Workshop for\n  Software Frameworks and Workload Management on Quantum and HPC Ecosystems at\n  SC25", "summary": "In this paper we present CQ, a specification for a C-like API for quantum\naccelerated HPC, as well as CQ-SimBE, a reference implementation of CQ written\nin C99, and built on top of the statevector simulator QuEST. CQ focuses on\nenabling the incremental integration of quantum computing into classical HPC\ncodes by supporting runtime offloading from languages such as C and Fortran. It\nprovides a way of describing and offloading quantum computations which is\ncompatible with strictly and strongly typed compiled languages, and gives the\nprogrammer fine-grained control over classical data movement. The CQ Simulated\nBackend (CQ-SimBE) provides both a way to demonstrate the usage and utility of\nCQ, and a space to experiment with new features such as support for analogue\nquantum computing. Both the CQ specification and CQ-SimBE are open-source, and\navailable in public repositories."}
{"id": "2508.10862", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.10862", "abs": "https://arxiv.org/abs/2508.10862", "authors": ["Brendan Kobayashi Chou", "Andrew Lewis-Pye", "Patrick O'Grady"], "title": "Minimmit: Fast Finality with Even Faster Blocks", "comment": null, "summary": "Minimmit is a new protocol for State-Machine-Replication (SMR) that extends\nthe '2-round finality' approach of protocols such as Alpenglow to further\nreduce latency, by allowing for faster progression through 'views'. This\npreliminary draft provides motivation and pseudocode, together with proofs of\nconsistency and liveness. An updated draft with a proof of optimistic\nresponsiveness, suggested optimizations, and experiments, is to follow."}
