<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Dependent Session Types for Verified Concurrent Programming](https://arxiv.org/abs/2510.19129)
*Qiancheng Fu,Hongwei Xi,Ankush Das*

Main category: cs.PL

TL;DR: TLLC扩展了TLL类型系统，增加了基于会话的并发性和依赖会话类型，支持关系验证和并发程序正确性验证


<details>
  <summary>Details</summary>
Motivation: 将依赖类型与会话类型结合，使并发程序能够继承顺序程序的正确性证明，为队列等数据结构和map-reduce等并发算法提供内在验证

Method: 开发了直觉会话类型的新形式化方法，扩展TLL类型系统，实现依赖会话类型，构建原型编译器生成并发C代码

Result: 建立了TLLC的元理论，证明了其作为项演算和进程演算的可靠性，并通过原型实现进行了广泛评估

Conclusion: TLLC成功将依赖类型与会话类型集成，为并发程序验证提供了强大工具，其会话类型形式化方法具有广泛适用性

Abstract: We present TLLC which extends the Two-Level Linear dependent type theory
(TLL) with session-based concurrency. Equipped with Martin-L\"{o}f style
dependency, the session types of TLLC allow protocols to specify properties of
communicated messages. When used in conjunction with the dependent type
machinery already present in TLL, dependent session types facilitate a form of
relational verification by relating concurrent programs with their idealized
sequential counterparts. Correctness properties proven for sequential programs
can be easily lifted to their corresponding concurrent implementations. TLLC
makes session types a powerful tool for intrinsically verifying the correctness
of data structures such as queues and concurrent algorithms such as map-reduce.
To extend TLL with session types, we develop a novel formulation of
intuitionistic session type which we believe to be widely applicable for
integrating session types into other type systems beyond the context of TLLC.
We study the meta-theory of our language, proving its soundness as both a term
calculus and a process calculus. To demonstrate the practicality of TLLC, we
have implemented a prototype compiler that translates TLLC programs into
concurrent C code, which has been extensively evaluated.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Comparative analysis of large data processing in Apache Spark using Java, Python and Scala](https://arxiv.org/abs/2510.19012)
*Ivan Borodii,Illia Fedorovych,Halyna Osukhivska,Diana Velychko,Roman Butsii*

Main category: cs.DC

TL;DR: 该研究比较了在Apache Spark平台上使用Java、Python和Scala处理大数据集的性能差异。结果显示编程语言对性能有显著影响：Python在小数据集处理上表现最佳，而Scala和Java在复杂操作和大数据处理方面更高效。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单个处理阶段，缺乏对完整ETL工作流程在不同编程语言下的全面比较，特别是使用Apache Iceberg时的性能差异。

Method: 通过执行多个操作进行对比分析，包括从CSV文件下载数据、转换数据并加载到Apache Iceberg分析表中。测试了不同大小的数据集（5MB和1.6GB）以及复杂的数据合并操作。

Result: 处理5MB文件时Python最快（6.71秒），Scala（9.13秒）和Java（9.62秒）较慢。处理1.6GB文件时三者性能相近，Python仍最快（46.34秒）。复杂合并操作中Scala表现最佳（374.42秒），Python最慢（398.32秒）。

Conclusion: 编程语言显著影响Apache Spark数据处理效率。Python适合小数据集，Scala和Java更适合大数据量和复杂操作。这些发现有助于根据具体性能需求和数据量优化数据处理流程。

Abstract: During the study, the results of a comparative analysis of the process of
handling large datasets using the Apache Spark platform in Java, Python, and
Scala programming languages were obtained. Although prior works have focused on
individual stages, comprehensive comparisons of full ETL workflows across
programming languages using Apache Iceberg remain limited. The analysis was
performed by executing several operations, including downloading data from CSV
files, transforming and loading it into an Apache Iceberg analytical table. It
was found that the performance of the Spark algorithm varies significantly
depending on the amount of data and the programming language used. When
processing a 5-megabyte CSV file, the best result was achieved in Python: 6.71
seconds, which is superior to Scala's score of 9.13 seconds and Java's time of
9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming
languages demonstrated similar results: the fastest performance was showed in
Python: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56
seconds, respectively. When performing a more complex operation that involved
combining two CSV files into a single dataset for further loading into an
Apache Iceberg table, Scala demonstrated the highest performance, at 374.42
seconds. Java processing was completed in 379.8 seconds, while Python was the
least efficient, with a runtime of 398.32 seconds. It follows that the
programming language significantly affects the efficiency of data processing by
the Apache Spark algorithm, with Scala and Java being more productive for
processing large amounts of data and complex operations, while Python
demonstrates an advantage in working with small amounts of data. The results
obtained can be useful for optimizing data handling processes depending on
specific performance requirements and the amount of information being
processed.

</details>


### [3] [CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation](https://arxiv.org/abs/2510.18893)
*Sergey Pugachev*

Main category: cs.DC

TL;DR: CodeCRDT提出了一种基于观察驱动的协调模式，使用CRDT实现无锁、无冲突的并发代码生成，在部分任务上获得21.1%加速，但某些任务有39.4%减速，100%收敛且无合并失败。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM系统由于昂贵的协调成本而无法实现并行加速，需要更高效的协调机制。

Method: 使用冲突无关复制数据类型（CRDTs），通过监控共享状态的可观察更新和确定性收敛来实现协调，而非显式消息传递。

Result: 在600次试验中，部分任务获得21.1%加速，部分任务有39.4%减速，100%收敛且无合并失败，语义冲突率为5-10%。

Conclusion: 研究形式化了观察驱动协调模式，揭示了语义冲突率和质量-性能权衡，并基于任务结构实证分析了并行协调的成功与失败条件。

Abstract: Multi-agent LLM systems fail to realize parallel speedups due to costly
coordination. We present CodeCRDT, an observation-driven coordination pattern
where agents coordinate by monitoring a shared state with observable updates
and deterministic convergence, rather than explicit message passing. Using
Conflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,
conflict-free concurrent code generation with strong eventual consistency.
Evaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits
and trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on
others, and 100% convergence with zero merge failures. The study formalizes
observation-driven coordination for stochastic LLM agents, revealing semantic
conflict rates (5-10%) and quality-performance tradeoffs, and provides
empirical characterization of when parallel coordination succeeds versus fails
based on task structure.

</details>


### [4] [AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators](https://arxiv.org/abs/2510.18897)
*Jacopo Tagliabue*

Main category: cs.DC

TL;DR: 结合LLM随机代码生成与确定性验证，通过生成-验证循环自动设计分布式系统调度策略，在保持可解释性的同时探索大规模设计空间。


<details>
  <summary>Details</summary>
Motivation: 探索AI驱动的分布式系统策略设计方法，将LLM的创造性生成能力与模拟器的确定性验证相结合，以自动化方式优化系统性能。

Method: 使用生成-验证循环：LLM生成Python调度策略，模拟器在标准化trace上评估，结构化反馈指导后续生成。基于Bauplan FaaS运行时和Eudoxia模拟器进行案例研究。

Result: 在多模型上实现了吞吐量提升的初步结果，证明了该方法在优化系统性能方面的有效性。

Conclusion: 该方法在保持策略可解释性的同时有效探索设计空间，未来AI将在扩展该方法论中发挥关键作用，特别是帮助引导新模拟器的开发。

Abstract: We explore AI-driven distributed-systems policy design by combining
stochastic code generation from large language models (LLMs) with deterministic
verification in a domain-specific simulator. Using a Function-as-a-Service
runtime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we
frame scheduler design as an iterative generate-and-verify loop: an LLM
proposes a Python policy, the simulator evaluates it on standardized traces,
and structured feedback steers subsequent generations. This setup preserves
interpretability while enabling targeted search over a large design space. We
detail the system architecture and report preliminary results on throughput
improvements across multiple models. Beyond early gains, we discuss the limits
of the current setup and outline next steps; in particular, we conjecture that
AI will be crucial for scaling this methodology by helping to bootstrap new
simulators.

</details>


### [5] [On the Randomized Locality of Matching Problems in Regular Graphs](https://arxiv.org/abs/2510.19151)
*Seri Khoury,Manish Purohit,Aaron Schild,Joshua Wang*

Main category: cs.DC

TL;DR: 本文研究了正则图中匹配问题的局部性，证明了(1+ε)-近似匹配是真正局部的，其局部性仅依赖于ε而与图参数无关。同时揭示了最大匹配在节点平均复杂度和最坏情况复杂度之间的强分离。


<details>
  <summary>Details</summary>
Motivation: 研究分布式对称性破坏问题中的局部性，特别是正则图中匹配问题的局部性，这是建立对称性破坏问题局部性下界和分类结果的主要基准。

Method: 开发了随机化算法，并采用基于鞅的新颖分析方法来分析Luby已有40年历史的算法，证明在Δ-正则图的线图上应用一轮Luby算法会产生几乎Δ/2-正则的图。

Result: 证明了(1+ε)-近似匹配在正则图中是真正局部的，局部性仅依赖于ε；当Δ≥poly(1/ε)时，这种依赖关系仅为1/ε的对数；最大匹配的节点平均复杂度仅为O(1)。

Conclusion: 正则图中的近似匹配具有真正局部性，而最大匹配在节点平均复杂度和最坏情况复杂度之间存在强分离，这为分布式对称性破坏问题的局部性研究提供了重要见解。

Abstract: The main goal in distributed symmetry-breaking is to understand the locality
of problems; i.e., the radius of the neighborhood that a node needs to explore
in order to arrive at its part of a global solution. In this work, we study the
locality of matching problems in the family of regular graphs, which is one of
the main benchmarks for establishing lower bounds on the locality of
symmetry-breaking problems, as well as for obtaining classification results.
For approximate matching, we develop randomized algorithms to show that $(1 +
\epsilon)$-approximate matching in regular graphs is truly local; i.e., the
locality depends only on $\epsilon$ and is independent of all other graph
parameters. Furthermore, as long as the degree $\Delta$ is not very small
(namely, as long as $\Delta \geq \text{poly}(1/\epsilon)$), this dependence is
only logarithmic in $1/\epsilon$. This stands in sharp contrast to maximal
matching in regular graphs which requires some dependence on the number of
nodes $n$ or the degree $\Delta$. We show matching lower bounds for both
results. For maximal matching, our techniques further allow us to establish a
strong separation between the node-averaged complexity and worst-case
complexity of maximal matching in regular graphs, by showing that the former is
only $O(1)$. Central to our main technical contribution is a novel
martingale-based analysis for the $\approx 40$-year-old algorithm by Luby. In
particular, our analysis shows that applying one round of Luby's algorithm on
the line graph of a $\Delta$-regular graph results in an almost
$\Delta/2$-regular graph.

</details>


### [6] [RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs](https://arxiv.org/abs/2510.19225)
*Yongji Wu,Xueshen Liu,Haizhong Zheng,Juncheng Gu,Beidi Chen,Z. Morley Mao,Arvind Krishnamurthy,Ion Stoica*

Main category: cs.DC

TL;DR: RLBoost是一个利用可抢占GPU资源进行高效强化学习训练的系统，通过混合架构和三项关键技术，在提升训练吞吐量的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有RL框架在资源利用上存在矛盾：rollout阶段需要大量独立GPU实例，而训练阶段需要紧密耦合的GPU。同时，云上的可抢占GPU资源提供了显著的成本节省机会，但缺乏高效利用方案。

Method: 采用混合架构，包含三项关键技术：(1)自适应rollout卸载动态调整工作负载；(2)基于拉取的权重传输快速配置新实例；(3)令牌级响应收集和迁移实现高效抢占处理和负载均衡。

Result: 实验显示RLBoost将训练吞吐量提升1.51x-1.97x，同时将成本效率提高28%-49%。

Conclusion: RLBoost通过有效利用可抢占GPU资源，解决了RL训练中的资源利用矛盾，实现了显著的性能和成本效益提升。

Abstract: Reinforcement learning (RL) has become essential for unlocking advanced
reasoning capabilities in large language models (LLMs). RL workflows involve
interleaving rollout and training stages with fundamentally different resource
requirements. Rollout typically dominates overall execution time, yet scales
efficiently through multiple independent instances. In contrast, training
requires tightly-coupled GPUs with full-mesh communication. Existing RL
frameworks fall into two categories: co-located and disaggregated
architectures. Co-located ones fail to address this resource tension by forcing
both stages to share the same GPUs. Disaggregated architectures, without
modifications of well-established RL algorithms, suffer from resource
under-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances
on public clouds and spare capacity in production clusters, present significant
cost-saving opportunities for accelerating RL workflows, if efficiently
harvested for rollout.
  In this paper, we present RLBoost, a systematic solution for cost-efficient
RL training that harvests preemptible GPU resources. Our key insight is that
rollout's stateless and embarrassingly parallel nature aligns perfectly with
preemptible and often fragmented resources. To efficiently utilize these
resources despite frequent and unpredictable availability changes, RLBoost
adopts a hybrid architecture with three key techniques: (1) adaptive rollout
offload to dynamically adjust workloads on the reserved (on-demand) cluster,
(2) pull-based weight transfer that quickly provisions newly available
instances, and (3) token-level response collection and migration for efficient
preemption handling and continuous load balancing. Extensive experiments show
RLBoost increases training throughput by 1.51x-1.97x while improving cost
efficiency by 28%-49% compared to using only on-demand GPU resources.

</details>


### [7] [RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training](https://arxiv.org/abs/2510.19262)
*Heng Xu,Zhiwei Yu,Chengze Du,Ying Zhou,Letian Li,Haojie Wang,Weiqiang Cheng,Jialong Li*

Main category: cs.DC

TL;DR: RailS是一个分布式负载均衡框架，通过利用Rail架构的拓扑对称性，将全局协调转化为本地调度，使用LPT喷洒调度器主动平衡流量，显著提升MoE训练中的all-to-all通信性能。


<details>
  <summary>Details</summary>
Motivation: MoE模型训练中的稀疏且高度不平衡的all-to-all通信主导了迭代时间，传统负载均衡方法无法充分利用Rail架构的多NIC带宽。

Method: RailS利用Rail拓扑的对称性证明均匀发送确保均匀接收，每个节点独立执行LPT喷洒调度器进行本地调度，激活N条并行rail进行细粒度、拓扑感知的多路径传输。

Result: 在合成和真实MoE工作负载中，RailS将总线带宽提升20%-78%，完成时间减少17%-78%。对于Mixtral工作负载，迭代时间缩短18%-40%，实现接近最优的负载均衡。

Conclusion: RailS能够充分利用分布式训练中的架构并行性，显著提升MoE训练效率。

Abstract: Training Mixture-of-Experts (MoE) models introduces sparse and highly
imbalanced all-to-all communication that dominates iteration time. Conventional
load-balancing methods fail to exploit the deterministic topology of Rail
architectures, leaving multi-NIC bandwidth underutilized. We present RailS, a
distributed load-balancing framework that minimizes all-to-all completion time
in MoE training. RailS leverages the Rail topology's symmetry to prove that
uniform sending ensures uniform receiving, transforming global coordination
into local scheduling. Each node independently executes a Longest Processing
Time First (LPT) spraying scheduler to proactively balance traffic using local
information. RailS activates N parallel rails for fine-grained, topology-aware
multipath transmission. Across synthetic and real-world MoE workloads, RailS
improves bus bandwidth by 20%--78% and reduces completion time by 17%--78%. For
Mixtral workloads, it shortens iteration time by 18%--40% and achieves
near-optimal load balance, fully exploiting architectural parallelism in
distributed training.

</details>


### [8] [FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems](https://arxiv.org/abs/2510.19301)
*Ziheng Deng,Xue Liu,Jiantong Jiang,Yankai Li,Qingxu Deng,Xiaochun Yang*

Main category: cs.DC

TL;DR: FLASH Viterbi是一种快速、轻量级、自适应且硬件友好的Viterbi解码算子，通过非递归分治策略、剪枝和并行化技术提升时间和内存效率，适用于资源受限的数据系统。


<details>
  <summary>Details</summary>
Motivation: 随着工作负载迁移到资源受限的边缘平台，标准Viterbi解码仍然内存密集且计算不灵活，现有方法通常在解码时间和空间效率之间权衡，但存在显著的运行时开销和缺乏适应性。

Method: 结合非递归分治策略与剪枝和并行化技术，提出FLASH-BS Viterbi变体使用内存高效数据结构解耦空间复杂度与隐藏状态空间大小，两种算法都能通过动态调整内部参数适应不同部署场景。

Result: 广泛实验表明，所提算法在解码时间和内存效率方面始终优于现有基线，同时保持适应性和硬件友好特性，开发了基于FPGA的硬件加速器，展示高吞吐量和低资源使用。

Conclusion: FLASH Viterbi算法在资源受限的数据系统中表现出色，提供高效、自适应且硬件友好的Viterbi解码解决方案，所有代码已公开。

Abstract: The Viterbi algorithm is a key operator for structured sequence inference in
modern data systems, with applications in trajectory analysis, online
recommendation, and speech recognition. As these workloads increasingly migrate
to resource-constrained edge platforms, standard Viterbi decoding remains
memory-intensive and computationally inflexible. Existing methods typically
trade decoding time for space efficiency, but often incur significant runtime
overhead and lack adaptability to various system constraints. This paper
presents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly
Viterbi decoding operator that enhances adaptability and resource efficiency.
FLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning
and parallelization techniques to enhance both time and memory efficiency,
making it well-suited for resource-constrained data systems.To further decouple
space complexity from the hidden state space size, we present FLASH-BS Viterbi,
a dynamic beam search variant built on a memory-efficient data structure. Both
proposed algorithms exhibit strong adaptivity to diverse deployment scenarios
by dynamically tuning internal parameters.To ensure practical deployment on
edge devices, we also develop FPGA-based hardware accelerators for both
algorithms, demonstrating high throughput and low resource usage. Extensive
experiments show that our algorithms consistently outperform existing baselines
in both decoding time and memory efficiency, while preserving adaptability and
hardware-friendly characteristics essential for modern data systems. All codes
are publicly available at https://github.com/Dzh-16/FLASH-Viterbi.

</details>


### [9] [HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission](https://arxiv.org/abs/2510.19470)
*Weihao Yang,Hao Huang,Donglei Wu,Ningke Li,Yanqi Pan,Qiyang Zheng,Wen Xia,Shiyi Li,Qiang Wang*

Main category: cs.DC

TL;DR: HybridEP是一个针对混合专家模型跨数据中心训练的优化框架，通过动态调整专家放置位置来减少通信开销，在受限带宽下性能提升可达5.6倍


<details>
  <summary>Details</summary>
Motivation: 随着混合专家模型规模快速增长，单数据中心训练已无法满足需求，转向跨数据中心训练。但在低带宽环境下，专家并行面临严重的可扩展性问题，现有重叠通信和计算的优化方法效果有限

Method: 提出HybridEP框架，核心思想是动态转换专家的空间放置以减少通信流量和频率。构建基于流的模型确定最优传输比例，结合域基分区和参数高效迁移技术优化GPU级通信拓扑

Result: 实验结果显示HybridEP在受限带宽下比现有MoE训练系统性能提升高达5.6倍。大规模仿真中，在1000个数据中心下不同带宽条件下实现1.45倍加速

Conclusion: HybridEP可视为具有更好可扩展性的通用专家并行方法，有效解决了跨数据中心混合专家模型训练中的通信瓶颈问题

Abstract: Mixture-of-Experts (MoE) has become a popular architecture for scaling large
models. However, the rapidly growing scale outpaces model training on a single
DC, driving a shift toward a more flexible, cross-DC training paradigm. Under
this, Expert Parallelism (EP) of MoE faces significant scalability issues due
to the limited cross-DC bandwidth. Specifically, existing EP optimizations
attempt to overlap data communication and computation, which has little benefit
in low-bandwidth scenarios due to a much longer data communication time.
Therefore, the trends of cross-DC EP scaling is fast becoming a critical
roadblock to the continued growth of MoE models.
  To address this, we propose HybridEP, a modeling-guided framework to optimize
EP under constrained bandwidth. Our key idea is to dynamically transform the
spatial placement of experts to reduce data communication traffic and
frequency, thereby minimizing EP's communication overheads. However, it is
non-trivial to find the optimal solution because it complicates the original
communication pattern by mixing data and expert communication. We therefore
build a stream-based model to determine the optimal transmission ratio. Guided
by this, we incorporate two techniques: (1) domain-based partition to construct
the mapping between hybrid patterns and specific communication topology at GPU
level, and (2) parameter-efficient migration to further refine this topology by
reducing expert transmission overhead and enlarging the domain size. Combining
all these designs, HybridEP can be considered as a more general EP with better
scalability. Experimental results show that HybridEP outperforms existing
state-of-the-art MoE training systems by up to 5.6x under constrained
bandwidth. We further compare HybridEP and EP on large-scale simulations.
HybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths.

</details>


### [10] [Propius: A Platform for Collaborative Machine Learning across the Edge and the Cloud](https://arxiv.org/abs/2510.19617)
*Eric Ding*

Main category: cs.DC

TL;DR: Propius是一个用于协作机器学习的可扩展资源管理系统，包含控制平面和数据平面，能够适应客户端机器的异构性，并高效管理ML作业与边缘资源之间的计算流。


<details>
  <summary>Details</summary>
Motivation: 协作机器学习面临数据隐私、通信开销和模型异构性等挑战，现有系统多为特定用例构建，缺乏可扩展性和可复用性。随着协作ML规模扩大，需要可扩展、高效且支持多租户的资源管理系统。

Method: Propius系统由控制平面和数据平面组成。控制平面支持多个协作ML作业之间的高效资源共享和多种资源共享策略；数据平面提高协作ML模型共享和结果收集的可扩展性。

Result: 评估显示Propius在资源利用率（最高1.88倍）、吞吐量（最高2.76倍）和作业完成时间（最高1.26倍）方面优于现有资源管理技术。

Conclusion: Propius是一个能够适应客户端异构性、高效管理计算流并支持多租户资源共享的协作ML资源管理系统，在可扩展性和性能方面表现优异。

Abstract: Collaborative Machine Learning is a paradigm in the field of distributed
machine learning, designed to address the challenges of data privacy,
communication overhead, and model heterogeneity. There have been significant
advancements in optimization and communication algorithm design and ML hardware
that enables fair, efficient and secure collaborative ML training. However,
less emphasis is put on collaborative ML infrastructure development. Developers
and researchers often build server-client systems for a specific collaborative
ML use case, which is not scalable and reusable. As the scale of collaborative
ML grows, the need for a scalable, efficient, and ideally multi-tenant resource
management system becomes more pressing. We propose a novel system, Propius,
that can adapt to the heterogeneity of client machines, and efficiently manage
and control the computation flow between ML jobs and edge resources in a
scalable fashion. Propius is comprised of a control plane and a data plane. The
control plane enables efficient resource sharing among multiple collaborative
ML jobs and supports various resource sharing policies, while the data plane
improves the scalability of collaborative ML model sharing and result
collection. Evaluations show that Propius outperforms existing resource
management techniques and frameworks in terms of resource utilization (up to
$1.88\times$), throughput (up to $2.76$), and job completion time (up to
$1.26\times$).

</details>


### [11] [Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation](https://arxiv.org/abs/2510.19689)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Srinivas Vippagunta,Suchitra Raman,Shreeshankar Chatterjee,Ju Lin,Shang Liu,Mary Schladenhauffen,Jeffrey Luo,Hailong Jiang*

Main category: cs.DC

TL;DR: 提出了一种面向生产的BDaaS蓝图，集成单节点无服务器GPU运行时与TabNet，在受监管环境中实现高吞吐、低延迟、低成本且可解释的表格数据分析。


<details>
  <summary>Details</summary>
Motivation: 传统分布式框架如Spark和Flink在大规模分析中有效，但协调复杂性和审计开销不适合中等规模、延迟敏感的推理场景。无服务器GPU和可解释模型如TabNet为受监管环境提供了新的部署方案。

Method: 设计集成单节点无服务器GPU运行时与TabNet的BDaaS蓝图，利用GPU加速提高吞吐量，无服务器弹性降低成本，特征掩码可解释性满足合规要求。

Result: 在HR、Adult和BLS数据集上的基准测试显示，GPU流水线相比Spark基线实现4.5倍吞吐量提升、98倍延迟降低和90%成本降低，合规机制仅增加约5.7ms延迟。

Conclusion: 该方案为受监管的企业和政府环境提供了安全、可解释、成本效益高的无服务器GPU分析实践方案，包含合规基准、可复现蓝图和决策框架。

Abstract: Industrial and government organizations increasingly depend on data-driven
analytics for workforce, finance, and regulated decision processes, where
timeliness, cost efficiency, and compliance are critical. Distributed
frameworks such as Spark and Flink remain effective for massive-scale batch or
streaming analytics but introduce coordination complexity and auditing
overheads that misalign with moderate-scale, latency-sensitive inference.
Meanwhile, cloud providers now offer serverless GPUs, and models such as TabNet
enable interpretable tabular ML, motivating new deployment blueprints for
regulated environments. In this paper, we present a production-oriented Big
Data as a Service (BDaaS) blueprint that integrates a single-node serverless
GPU runtime with TabNet. The design leverages GPU acceleration for throughput,
serverless elasticity for cost reduction, and feature-mask interpretability for
IL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,
comparing our approach against Spark and CPU baselines. Our results show that
GPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%
lower cost per 1K inferences compared to Spark baselines, while compliance
mechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains
stable under peak load, ensuring reliable auditability. Taken together, these
findings provide a compliance-aware benchmark, a reproducible Helm-packaged
blueprint, and a decision framework that demonstrate the practicality of
secure, interpretable, and cost-efficient serverless GPU analytics for
regulated enterprise and government settings.

</details>


### [12] [CommonSense: Efficient Set Intersection (SetX) Protocol Based on Compressed Sensing](https://arxiv.org/abs/2510.19725)
*Jingfan Meng,Tianji Yang,Jun Xu*

Main category: cs.DC

TL;DR: 本文开发了一种多轮集合交集协议(SetX)，通过压缩感知技术显著降低了通信成本，比基于IBLT的集合协调协议减少了8-10倍的通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有的集合交集解决方案通常重用集合协调协议，但存在误解认为两者成本相当。实际上集合交集本质上比集合协调更便宜，因此需要专门的优化协议。

Method: 使用多轮压缩感知草图交换协议：Alice发送A的CS草图给Bob，Bob识别其独特元素；如果A不是B的子集，Bob将无法解码的元素发送回Alice，Alice解码其独特元素。双方通过迭代更新直到达成交集共识。

Result: 在真实数据集上的实验表明，该SetX协议比基于IBLT的SetR协议减少了8-10倍的通信成本。

Conclusion: 集合交集问题在通信成本上比集合协调问题更便宜，通过专门的压缩感知多轮协议可以显著优化性能，突破了集合协调问题的信息论下界。

Abstract: In the set reconciliation (\textsf{SetR}) problem, two parties Alice and Bob,
holding sets $\mathsf{A}$ and $\mathsf{B}$, communicate to learn the symmetric
difference $\mathsf{A} \Delta \mathsf{B}$. In this work, we study a related but
under-explored problem: set intersection (\textsf{SetX})~\cite{Ozisik2019},
where both parties learn $\mathsf{A} \cap \mathsf{B}$ instead. However,
existing solutions typically reuse \textsf{SetR} protocols due to the absence
of dedicated \textsf{SetX} protocols and the misconception that \textsf{SetR}
and \textsf{SetX} have comparable costs. Observing that \textsf{SetX} is
fundamentally cheaper than \textsf{SetR}, we developed a multi-round
\textsf{SetX} protocol that outperforms the information-theoretic lower bound
of \textsf{SetR} problem. In our \textsf{SetX} protocol, Alice sends Bob a
compressed sensing (CS) sketch of $\mathsf{A}$ to help Bob identify his unique
elements (those in $\mathsf{B \setminus A}$). This solves the \textsf{SetX}
problem, if $\mathsf{A} \subseteq \mathsf{B}$. Otherwise, Bob sends a CS sketch
of the residue (a set of elements he cannot decode) back to Alice for her to
decode her unique elements (those in $\mathsf{A \setminus B}$). As such, Alice
and Bob communicate back and forth %with a set membership filter (SMF) of
estimated $\mathsf{B \setminus A}$. Alice updates $\mathsf{A}$ and
communication repeats until both parties agrees on $\mathsf{A} \cap
\mathsf{B}$. On real world datasets, experiments show that our $\mathsf{SetX}$
protocol reduces the communication cost by 8 to 10 times compared to the
IBLT-based $\mathsf{SetR}$ protocol.

</details>


### [13] [Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and Beyond](https://arxiv.org/abs/2510.19805)
*Carl-Johan Fauvelle Munck af Rosensch"old,Feras M. Awaysheh,Ahmad Awad*

Main category: cs.DC

TL;DR: 对Redis替代品Valkey、KeyDB和Garnet在Kubernetes环境下的性能基准测试，评估吞吐量、延迟、资源效率和迁移复杂性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对内存键值数据库最新工具的实证评估，需要解决可扩展性、兼容性和可持续性限制。

Method: 在Kubernetes部署中使用真实工作负载对Valkey、KeyDB和Garnet进行系统化基准测试。

Result: 结果显示被测试的数据系统之间存在明显的权衡取舍，包括性能、兼容性和长期可行性方面的差异。

Conclusion: 研究提供了新兴内存键值存储的全面性能和可行性评估，强调了性能、兼容性和项目成熟度之间的权衡。

Abstract: In-memory key-value datastores have become indispensable building blocks of
modern cloud-native infrastructures, yet their evolution faces scalability,
compatibility, and sustainability constraints. The current literature lacks an
experimental evaluation of state-of-the-art tools in the domain. This study
addressed this timely gap by benchmarking Redis alternatives and systematically
evaluating Valkey, KeyDB, and Garnet under realistic workloads within
Kubernetes deployments. The results demonstrate clear trade-offs among the
benchmarked data systems. Our study presents a comprehensive performance and
viability assessment of the emerging in-memory key-value stores. Metrics
include throughput, tail latency, CPU and memory efficiency, and migration
complexity. We highlight trade-offs between performance, compatibility, and
long-term viability, including project maturity, community support, and
sustained development.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [Res-DPU: Resource-shared Digital Processing-in-memory Unit for Edge-AI Workloads](https://arxiv.org/abs/2510.19260)
*Mukul Lokhande,Narendra Singh Dhakad,Seema Chouhan,Akash Sankhe,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: Res-DPU是一种资源共享的数字内存处理单元，采用双端口5T SRAM锁存器和共享2T AND计算逻辑，将每比特乘法成本降至5.25T，比现有技术减少56%晶体管数量，并通过TRAIT加法树和CIA2M方法实现能效提升和精度-延迟权衡。


<details>
  <summary>Details</summary>
Motivation: 解决现有数字内存处理方法因使用庞大比特单元和晶体管密集型加法树导致的低计算密度、宏可扩展性受限和能效低下的问题。

Method: 提出Res-DPU架构，包含双端口5T SRAM锁存器、共享2T AND计算逻辑、TRAIT加法树（使用FA-7T和PG-FA-26T），以及CIA2M运行时精度-延迟权衡方法。

Result: 16KB REP-DPIM宏在TSMC 65nm工艺下实现0.43 TOPS吞吐量和87.22 TOPS/W能效，在CIFAR-10数据集上对ResNet-18和VGG-16（含30%剪枝）达到96.85%质量结果。

Conclusion: Res-DPU为高度可扩展和能效优化的实时边缘AI加速器提供了一个有效的解决方案。

Abstract: Processing-in-memory (PIM) has emerged as the go to solution for addressing
the von Neumann bottleneck in edge AI accelerators. However, state-of-the-art
(SoTA) digital PIM approaches suffer from low compute density, primarily due to
the use of bulky bit cells and transistor-heavy adder trees, which impose
limitations on macro scalability and energy efficiency. This work introduces
Res-DPU, a resource-shared digital PIM unit, with a dual-port 5T SRAM latch and
shared 2T AND compute logic. This reflects the per-bit multiplication cost to
just 5.25T and reduced the transistor count of the PIM array by up to 56% over
the SoTA works. Furthermore, a Transistor-Reduced 2D Interspersed Adder Tree
(TRAIT) with FA-7T and PG-FA-26T helps reduce the power consumption of the
adder tree by up to 21.35% and leads to improved energy efficiency by 59%
compared to conventional 28T RCA designs. We propose a Cycle-controlled
Iterative Approximate-Accurate Multiplication (CIA2M) approach, enabling
run-time accuracy-latency trade-offs without requiring error-correction
circuitry. The 16 KB REP-DPIM macro achieves 0.43 TOPS throughput and 87.22
TOPS/W energy efficiency in TSMC 65nm CMOS, with 96.85% QoR for ResNet-18 or
VGG-16 on CIFAR-10, including 30% pruning. The proposed results establish a
Res-DPU module for highly scalable and energy-efficient real-time edge AI
accelerators.

</details>


### [15] [gem5 Co-Pilot: AI Assistant Agent for Architectural Design Space Exploration](https://arxiv.org/abs/2510.19577)
*Zuoming Fu,Alex Manley,Mohammad Alian*

Main category: cs.AR

TL;DR: 开发了gem5 Co-Pilot，一个基于大语言模型的AI助手，用于自动化gem5的设计空间探索，通过网页GUI、设计空间数据库和检索增强生成系统来优化计算机架构参数配置。


<details>
  <summary>Details</summary>
Motivation: 计算机架构设计空间探索复杂耗时，需要分析大量参数设置和仿真统计数据，而大语言模型能够加速长文本分析和智能决策，这正是成功设计空间探索所需的关键功能。

Method: 构建gem5 Co-Pilot AI助手，包含网页GUI用于用户交互、自动化代理和结果总结；实现设计空间探索语言和设计空间数据库(DSDB)；基于DSDB实现检索增强生成系统。

Result: 在四个成本约束范围内进行成本约束优化实验，与两个基线模型比较，结果显示gem5 Co-Pilot能够基于性能和成本快速识别特定设计约束下的最优参数，且用户交互有限。

Conclusion: gem5 Co-Pilot成功利用大语言模型能力，有效加速了计算机架构设计空间探索过程，能够在有限用户交互下快速找到最优参数配置。

Abstract: Generative AI is increasing the productivity of software and hardware
development across many application domains. In this work, we utilize the power
of Large Language Models (LLMs) to develop a co-pilot agent for assisting gem5
users with automating design space exploration. Computer architecture design
space exploration is complex and time-consuming, given that numerous parameter
settings and simulation statistics must be analyzed before improving the
current design. The emergence of LLMs has significantly accelerated the
analysis of long-text data as well as smart decision making, two key functions
in a successful design space exploration task. In this project, we first build
gem5 Co-Pilot, an AI agent assistant for gem5, which comes with a webpage-GUI
for smooth user interaction, agent automation, and result summarization. We
also implemented a language for design space exploration, as well as a Design
Space Database (DSDB). With DSDB, gem5 Co-Pilot effectively implements a
Retrieval Augmented Generation system for gem5 design space exploration. We
experiment on cost-constraint optimization with four cost ranges and compare
our results with two baseline models. Results show that gem5 Co-Pilot can
quickly identify optimal parameters for specific design constraints based on
performance and cost, with limited user interaction.

</details>
