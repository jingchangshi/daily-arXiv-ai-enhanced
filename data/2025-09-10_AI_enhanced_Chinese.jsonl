{"id": "2509.07157", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.07157", "abs": "https://arxiv.org/abs/2509.07157", "authors": ["Guanzhou Hu", "Yiwei Chen", "Andrea Arpaci-Dusseau", "Remzi Arpaci-Dusseau"], "title": "Crossword: Adaptive Consensus for Dynamic Data-Heavy Workloads", "comment": "13 pages, 16 figures, 1 table, contains appendix", "summary": "We present Crossword, a flexible consensus protocol for dynamic data-heavy\nworkloads, a rising challenge in the cloud where replication payload sizes span\na wide spectrum and introduce sporadic bandwidth stress. Crossword applies\nper-instance erasure coding and distributes coded shards intelligently to\nreduce critical-path data transfer significantly when desirable. Unlike\nprevious approaches that statically assign shards to servers, Crossword enables\nan adaptive tradeoff between the assignment of shards and quorum size in\nreaction to dynamic workloads and network conditions, while always retaining\nthe availability guarantee of classic protocols. Crossword handles leader\nfailover gracefully by employing a lazy follower gossiping mechanism that\nincurs minimal impact on critical-path performance. We implement Crossword\n(along with relevant protocols) in Gazette, a distributed, replicated, and\nprotocol-generic key-value store written in async Rust. We evaluate Crossword\ncomprehensively to show that it matches the best performance among previous\nprotocols (MultiPaxos, Raft, RSPaxos, and CRaft) in static scenarios, and\noutperforms them by up to 2.3x under dynamic workloads and network conditions.\nOur integration of Crossword with CockroachDB brings 1.32x higher aggregate\nthroughput to TPC-C under 5-way replication. We will open-source Gazette upon\npublication.", "AI": {"tldr": "Crossword\u662f\u4e00\u79cd\u7075\u6d3b\u7684\u5171\u8bc6\u534f\u8bae\uff0c\u9488\u5bf9\u52a8\u6001\u6570\u636e\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u8bbe\u8ba1\uff0c\u901a\u8fc7\u667a\u80fd\u5206\u7247\u5206\u914d\u548c\u64e6\u9664\u7f16\u7801\u663e\u8457\u51cf\u5c11\u5173\u952e\u8def\u5f84\u6570\u636e\u4f20\u8f93\uff0c\u5728\u52a8\u6001\u573a\u666f\u4e0b\u6027\u80fd\u63d0\u53472.3\u500d", "motivation": "\u4e91\u73af\u5883\u4e2d\u590d\u5236\u8d1f\u8f7d\u5927\u5c0f\u8de8\u5ea6\u5927\u4e14\u5e26\u6765\u95f4\u6b47\u6027\u5e26\u5bbd\u538b\u529b\uff0c\u73b0\u6709\u534f\u8bae\u9759\u6001\u5206\u7247\u5206\u914d\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u548c\u7f51\u7edc\u6761\u4ef6", "method": "\u91c7\u7528\u6bcf\u5b9e\u4f8b\u64e6\u9664\u7f16\u7801\uff0c\u667a\u80fd\u5206\u53d1\u7f16\u7801\u5206\u7247\uff0c\u652f\u6301\u5206\u7247\u5206\u914d\u548c\u4ef2\u88c1\u5927\u5c0f\u7684\u81ea\u9002\u5e94\u6743\u8861\uff0c\u4f7f\u7528\u60f0\u6027\u8ddf\u968f\u8005gossip\u673a\u5236\u5904\u7406\u9886\u5bfc\u8005\u6545\u969c\u8f6c\u79fb", "result": "\u5728\u9759\u6001\u573a\u666f\u4e0b\u4e0e\u73b0\u6709\u534f\u8bae\u6027\u80fd\u76f8\u5f53\uff0c\u5728\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u548c\u7f51\u7edc\u6761\u4ef6\u4e0b\u6027\u80fd\u63d0\u5347\u8fbe2.3\u500d\uff0c\u4e0eCockroachDB\u96c6\u6210\u4f7fTPC-C\u805a\u5408\u541e\u5410\u91cf\u63d0\u9ad81.32\u500d", "conclusion": "Crossword\u534f\u8bae\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u6570\u636e\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5171\u8bc6\u6311\u6218\uff0c\u5728\u4fdd\u6301\u7ecf\u5178\u534f\u8bae\u53ef\u7528\u6027\u4fdd\u8bc1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd"}}
{"id": "2509.07158", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.07158", "abs": "https://arxiv.org/abs/2509.07158", "authors": ["Guanzhou Hu", "Andrea Arpaci-Dusseau", "Remzi Arpaci-Dusseau"], "title": "Bodega: Serving Linearizable Reads Locally from Anywhere at Anytime via Roster Leases", "comment": "12 pages, 16 figures, 1 table, contains appendix", "summary": "We present Bodega, the first consensus protocol that serves linearizable\nreads locally from any desired node, regardless of interfering writes. Bodega\nachieves this via a novel roster leases algorithm that safeguards the roster, a\nnew notion of cluster metadata. The roster is a generalization of leadership;\nit tracks arbitrary subsets of replicas as responder nodes for local reads. A\nconsistent agreement on the roster is established through roster leases, an\nall-to-all leasing mechanism that generalizes existing all-to-one leasing\napproaches (Leader Leases, Quorum Leases), unlocking a new point in the\nprotocol design space. Bodega further employs optimistic holding and early\naccept notifications to minimize interruption from interfering writes, and\nincorporates smart roster coverage and lightweight heartbeats to maximize\npracticality. Bodega is a non-intrusive extension to classic consensus; it\nimposes no special requirements on writes other than a responder-covering\nquorum. We implement Bodega and related works in Vineyard, a protocol-generic\nreplicated key-value store written in async Rust. We compare it to previous\nprotocols (Leader Leases, EPaxos, PQR, and Quorum Leases) and two production\ncoordination services (etcd and ZooKeeper). Bodega speeds up average client\nread requests by 5.6x-13.1x on real WAN clusters versus previous approaches\nunder moderate write interference, delivers comparable write performance,\nsupports fast proactive roster changes as well as fault tolerance via leases,\nand closely matches the performance of sequentially-consistent etcd and\nZooKeeper deployments across all YCSB workloads. We will open-source Vineyard\nupon publication.", "AI": {"tldr": "Bodega\u662f\u9996\u4e2a\u80fd\u5728\u4efb\u4f55\u8282\u70b9\u672c\u5730\u63d0\u4f9b\u7ebf\u6027\u5316\u8bfb\u53d6\u7684\u5171\u8bc6\u534f\u8bae\uff0c\u901a\u8fc7\u65b0\u578broster leases\u7b97\u6cd5\u5b9e\u73b0\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728WAN\u96c6\u7fa4\u4e2d\u8bfb\u53d6\u901f\u5ea6\u63d0\u53475.6-13.1\u500d", "motivation": "\u89e3\u51b3\u73b0\u6709\u5171\u8bc6\u534f\u8bae\u65e0\u6cd5\u5728\u4efb\u4f55\u8282\u70b9\u672c\u5730\u63d0\u4f9b\u7ebf\u6027\u5316\u8bfb\u53d6\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u5199\u5165\u5e72\u6270\u7684\u60c5\u51b5\u4e0b", "method": "\u91c7\u7528roster leases\u7b97\u6cd5\uff08\u5168\u5bf9\u5168\u79df\u8d41\u673a\u5236\uff09\u3001optimistic holding\u3001early accept notifications\u3001smart roster coverage\u548clightweight heartbeats\u7b49\u6280\u672f", "result": "\u5728\u771f\u5b9eWAN\u96c6\u7fa4\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u4e2d\u7b49\u5199\u5165\u5e72\u6270\u4e0b\u5e73\u5747\u5ba2\u6237\u7aef\u8bfb\u53d6\u8bf7\u6c42\u901f\u5ea6\u63d0\u53475.6-13.1\u500d\uff0c\u5199\u5165\u6027\u80fd\u76f8\u5f53\uff0c\u652f\u6301\u5feb\u901f\u4e3b\u52a8roster\u53d8\u66f4\u548c\u5bb9\u9519", "conclusion": "Bodega\u901a\u8fc7\u521b\u65b0\u7684roster leases\u673a\u5236\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u4efb\u4f55\u8282\u70b9\u672c\u5730\u63d0\u4f9b\u7ebf\u6027\u5316\u8bfb\u53d6\u7684\u76ee\u6807\uff0c\u662f\u5171\u8bc6\u534f\u8bae\u8bbe\u8ba1\u7a7a\u95f4\u7684\u65b0\u7a81\u7834"}}
{"id": "2509.07199", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.07199", "abs": "https://arxiv.org/abs/2509.07199", "authors": ["Anjus George", "Michael J. Brim", "Christopher Zimmer", "Tyler J. Skluzacek", "A. J. Ruckman", "Gustav R. Jansen", "Sarp Oral"], "title": "A Study on Messaging Trade-offs in Data Streaming for Scientific Workflows", "comment": "10 pages, 8 figures", "summary": "Memory-to-memory data streaming is essential for modern scientific workflows\nthat require near real-time data analysis, experimental steering, and informed\ndecision-making during experiment execution. It eliminates the latency\nbottlenecks associated with file-based transfers to parallel storage, enabling\nrapid data movement between experimental facilities and HPC systems. These\ntightly coupled experimental-HPC workflows demand low latency, high throughput,\nand reliable data delivery to support on-the-fly analysis and timely feedback\nfor experimental control. Off-the-shelf messaging frameworks are increasingly\nconsidered viable solutions for enabling such direct memory streaming due to\ntheir maturity, broad adoption, and ability to abstract core messaging and\nreliability functionalities from the application layer. However, effectively\nmeeting the workflows' requirements depends on utilizing the framework's\ncapabilities and carefully tuning its configurations.\n  In this paper, we present a study that investigates the messaging parameters,\nand their configuration choices that impact the streaming requirements of two\nrepresentative scientific workflows. We specifically characterize throughput\ntrade-offs associated with reliable message transmission for these workflows.\nOur study is conducted through streaming simulations using synthetic workloads\nderived from the Deleria and LCLS workflows, employing the RabbitMQ messaging\nframework within the context of the Data Streaming to HPC infrastructure at\nOLCF. Our simulations reveal several key observations and practical insights\nthat help users understand which configurations best meet the needs of their\nstreaming workloads.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u6d88\u606f\u4f20\u9012\u53c2\u6570\u914d\u7f6e\u5bf9\u79d1\u5b66\u5de5\u4f5c\u6d41\u5185\u5b58\u6570\u636e\u6d41\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u901a\u8fc7RabbitMQ\u6846\u67b6\u6a21\u62dfDeleria\u548cLCLS\u5de5\u4f5c\u6d41\uff0c\u63ed\u793a\u4e86\u53ef\u9760\u6d88\u606f\u4f20\u8f93\u7684\u541e\u5410\u91cf\u6743\u8861\u548c\u6700\u4f73\u914d\u7f6e\u5b9e\u8df5", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u5de5\u4f5c\u6d41\u9700\u8981\u8fd1\u5b9e\u65f6\u6570\u636e\u5206\u6790\u548c\u5b9e\u9a8c\u63a7\u5236\uff0c\u5185\u5b58\u5230\u5185\u5b58\u6570\u636e\u6d41\u53ef\u6d88\u9664\u6587\u4ef6\u4f20\u8f93\u5ef6\u8fdf\uff0c\u4f46\u9700\u8981\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u91cf\u7684\u53ef\u9760\u6570\u636e\u4f20\u8f93\uff0c\u73b0\u6709\u6d88\u606f\u6846\u67b6\u9700\u8981\u5408\u7406\u914d\u7f6e\u624d\u80fd\u6ee1\u8db3\u9700\u6c42", "method": "\u4f7f\u7528RabbitMQ\u6d88\u606f\u6846\u67b6\uff0c\u57fa\u4e8eOLCF\u7684HPC\u6570\u636e\u6d41\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u5408\u6210\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u62dfDeleria\u548cLCLS\u4e24\u79cd\u4ee3\u8868\u6027\u79d1\u5b66\u5de5\u4f5c\u6d41\uff0c\u7814\u7a76\u6d88\u606f\u53c2\u6570\u914d\u7f6e\u5bf9\u6d41\u6027\u80fd\u7684\u5f71\u54cd", "result": "\u6a21\u62df\u5b9e\u9a8c\u63ed\u793a\u4e86\u591a\u4e2a\u5173\u952e\u89c2\u5bdf\u7ed3\u679c\u548c\u5b9e\u8df5\u89c1\u89e3\uff0c\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u54ea\u4e9b\u914d\u7f6e\u6700\u80fd\u6ee1\u8db3\u5176\u6d41\u5de5\u4f5c\u8d1f\u8f7d\u9700\u6c42\uff0c\u7279\u522b\u662f\u53ef\u9760\u6d88\u606f\u4f20\u8f93\u76f8\u5173\u7684\u541e\u5410\u91cf\u6743\u8861", "conclusion": "\u6d88\u606f\u6846\u67b6\u7684\u6709\u6548\u4f7f\u7528\u9700\u8981\u4ed4\u7ec6\u8c03\u6574\u914d\u7f6e\u53c2\u6570\uff0c\u7814\u7a76\u4e3a\u79d1\u5b66\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u914d\u7f6e\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u5185\u5b58\u6570\u636e\u6d41\u6027\u80fd"}}
{"id": "2509.07378", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.07378", "abs": "https://arxiv.org/abs/2509.07378", "authors": ["Mohammad Sadegh Sirjani", "Somayeh Sobati-Moghadam"], "title": "Optimizing Task Scheduling in Fog Computing with Deadline Awareness", "comment": null, "summary": "The rise of Internet of Things (IoT) devices has led to the development of\nnumerous applications that require quick responses and low latency. Fog\ncomputing has emerged as a solution for processing these IoT applications, but\nit faces challenges such as resource allocation and job scheduling. Therefore,\nit is crucial to determine how to assign and schedule tasks on Fog nodes. A\nwell-designed job scheduling algorithm can help decrease energy usage and\nimprove response times for application requests. This work aims to schedule\ntasks in IoT while minimizing the total energy consumption of nodes and\nenhancing the Quality of Service (QoS) requirements of IoT tasks, taking into\naccount task deadlines. Initially, this paper classifies the Fog nodes into two\ncategories based on their traffic level: low and high. It schedules\nlow-deadline tasks on low-traffic-level nodes using an Improved Golden Eagle\nOptimization (IGEO) algorithm, an enhancement of the Golden Eagle Optimization\nAlgorithm that utilizes genetic operators for discretization. High-deadline\ntasks are processed on high-traffic nodes using reinforcement learning (RL).\nThis combined approach is called the Reinforcement Improved Golden Eagle\nOptimization (RIGEO) algorithm. Experimental results demonstrate that the\nproposed algorithms optimize system response time, total deadline violation\ntime, and resource and system energy consumption compared to other\nstate-of-the-art algorithms.", "AI": {"tldr": "\u63d0\u51faRIGEO\u7b97\u6cd5\uff0c\u7ed3\u5408\u6539\u8fdb\u91d1\u9e70\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u96fe\u8ba1\u7b97\u73af\u5883\u4e2d\u4f18\u5316IoT\u4efb\u52a1\u8c03\u5ea6\uff0c\u964d\u4f4e\u80fd\u8017\u5e76\u63d0\u5347\u670d\u52a1\u8d28\u91cf", "motivation": "IoT\u8bbe\u5907\u589e\u957f\u9700\u8981\u4f4e\u5ef6\u8fdf\u54cd\u5e94\uff0c\u96fe\u8ba1\u7b97\u9762\u4e34\u8d44\u6e90\u5206\u914d\u548c\u4efb\u52a1\u8c03\u5ea6\u6311\u6218\uff0c\u9700\u8981\u8bbe\u8ba1\u9ad8\u6548\u7b97\u6cd5\u6765\u964d\u4f4e\u80fd\u8017\u5e76\u6ee1\u8db3\u4efb\u52a1\u622a\u6b62\u65f6\u95f4\u8981\u6c42", "method": "\u5c06\u96fe\u8282\u70b9\u6309\u6d41\u91cf\u5206\u4e3a\u9ad8\u4f4e\u4e24\u7c7b\uff1a\u4f4e\u6d41\u91cf\u8282\u70b9\u4f7f\u7528\u6539\u8fdb\u91d1\u9e70\u4f18\u5316\u7b97\u6cd5(IGEO)\u5904\u7406\u4f4e\u622a\u6b62\u65f6\u95f4\u4efb\u52a1\uff0c\u9ad8\u6d41\u91cf\u8282\u70b9\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60(RL)\u5904\u7406\u9ad8\u622a\u6b62\u65f6\u95f4\u4efb\u52a1\uff0c\u7ec4\u5408\u6210RIGEO\u7b97\u6cd5", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u5148\u8fdb\u7b97\u6cd5\uff0cRIGEO\u5728\u7cfb\u7edf\u54cd\u5e94\u65f6\u95f4\u3001\u622a\u6b62\u65f6\u95f4\u8fdd\u53cd\u603b\u91cf\u3001\u8d44\u6e90\u548c\u7cfb\u7edf\u80fd\u8017\u65b9\u9762\u90fd\u6709\u4f18\u5316", "conclusion": "RIGEO\u7b97\u6cd5\u901a\u8fc7\u5206\u7c7b\u8c03\u5ea6\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u96fe\u8ba1\u7b97\u73af\u5883\u4e2d\u7684IoT\u4efb\u52a1\u8c03\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u80fd\u8017\u4f18\u5316\u548c\u670d\u52a1\u8d28\u91cf\u63d0\u5347"}}
{"id": "2509.07003", "categories": ["cs.PL", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.07003", "abs": "https://arxiv.org/abs/2509.07003", "authors": ["Youjie Li", "Cheng Wan", "Zhiqi Lin", "Hongyu Zhu", "Jiacheng Yang", "Ziang Song", "Xinyi Di", "Jiawei Wu", "Huiyao Shu", "Wenlei Bao", "Yanghua Peng", "Haibin Lin", "Li-Wen Chang"], "title": "veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD", "comment": "21 pages, 16 figures, 5 tables", "summary": "Large Language Models (LLMs) have scaled rapidly in size and complexity,\nrequiring increasingly intricate parallelism for distributed training, such as\n3D parallelism. This sophistication motivates a shift toward simpler, more\ndebuggable programming paradigm like Single Program Multiple Data (SPMD).\nHowever, SPMD in eager execution introduces two key challenges: ensuring\nconsistency with single-device execution and achieving high performance at\nscale. In this paper, we introduce veScale, an eager-mode training system that\nfully embraces SPMD paradigm to democratize distributed tensor programming.\nveScale addresses the prevalent issue of inconsistent results in systems like\nPyTorch by introducing a novel algorithm of distributed Random Number\nGeneration (RNG) compatible with arbitrary sharded operators. veScale also\nsignificantly boosts training performance by reducing PyTorch primitive's\noverhead and improving communication efficiency. Evaluations show that veScale\ndelivers up to 2.2x speedup over the state-of-the-art training systems, like\nTorchTitan, and cuts code complexity by 78.4%, while preserving\nsingle-device-equivalent results.", "AI": {"tldr": "veScale\u662f\u4e00\u4e2a\u57fa\u4e8eSPMD\u8303\u5f0f\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86LLM\u8bad\u7ec3\u4e2d\u7684\u4e00\u81f4\u6027\u548c\u6027\u80fd\u95ee\u9898\uff0c\u6bd4\u73b0\u6709\u7cfb\u7edf\u5feb2.2\u500d\uff0c\u4ee3\u7801\u590d\u6742\u5ea6\u964d\u4f4e78.4%", "motivation": "\u968f\u7740LLM\u89c4\u6a21\u548c\u590d\u6742\u5ea6\u7684\u5feb\u901f\u589e\u957f\uff0c\u73b0\u6709\u5206\u5e03\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff08\u59823D\u5e76\u884c\uff09\u53d8\u5f97\u8fc7\u4e8e\u590d\u6742\uff0c\u9700\u8981\u8f6c\u5411\u66f4\u7b80\u5355\u3001\u6613\u8c03\u8bd5\u7684SPMD\u7f16\u7a0b\u8303\u5f0f\uff0c\u4f46SPMD\u5728eager\u6267\u884c\u4e2d\u5b58\u5728\u7ed3\u679c\u4e00\u81f4\u6027\u548c\u6027\u80fd\u6269\u5c55\u7684\u6311\u6218", "method": "\u5f15\u5165veScale\u7cfb\u7edf\uff0c\u91c7\u7528\u5b8c\u5168SPMD\u8303\u5f0f\uff0c\u63d0\u51fa\u5206\u5e03\u5f0f\u968f\u673a\u6570\u751f\u6210\u7b97\u6cd5\u786e\u4fdd\u4efb\u610f\u5206\u7247\u7b97\u5b50\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u51cf\u5c11PyTorch\u539f\u8bed\u5f00\u9500\u548c\u63d0\u5347\u901a\u4fe1\u6548\u7387\u6765\u4f18\u5316\u6027\u80fd", "result": "\u76f8\u6bd4TorchTitan\u7b49\u5148\u8fdb\u8bad\u7ec3\u7cfb\u7edf\uff0cveScale\u5b9e\u73b0\u4e86\u6700\u9ad82.2\u500d\u7684\u52a0\u901f\uff0c\u4ee3\u7801\u590d\u6742\u5ea6\u964d\u4f4e78.4%\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5355\u8bbe\u5907\u6267\u884c\u7b49\u6548\u7684\u7ed3\u679c", "conclusion": "veScale\u6210\u529f\u8bc1\u660e\u4e86SPMD\u8303\u5f0f\u5728LLM\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u3001\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.07690", "categories": ["cs.AR", "cs.DC", "cs.MS", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.07690", "abs": "https://arxiv.org/abs/2509.07690", "authors": ["Xiaoming Chen"], "title": "HYLU: Hybrid Parallel Sparse LU Factorization", "comment": null, "summary": "This article introduces HYLU, a hybrid parallel LU factorization-based\ngeneral-purpose solver designed for efficiently solving sparse linear systems\n(Ax=b) on multi-core shared-memory architectures. The key technical feature of\nHYLU is the integration of hybrid numerical kernels so that it can adapt to\nvarious sparsity patterns of coefficient matrices. Tests on 34 sparse matrices\nfrom SuiteSparse Matrix Collection reveal that HYLU outperforms Intel MKL\nPARDISO in the numerical factorization phase by geometric means of 1.74X (for\none-time solving) and 2.26X (for repeated solving). HYLU can be downloaded from\nhttps://github.com/chenxm1986/hylu.", "AI": {"tldr": "HYLU\u662f\u4e00\u4e2a\u6df7\u5408\u5e76\u884cLU\u5206\u89e3\u6c42\u89e3\u5668\uff0c\u5728\u5171\u4eab\u5185\u5b58\u591a\u6838\u67b6\u6784\u4e0a\u9ad8\u6548\u6c42\u89e3\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\uff0c\u76f8\u6bd4Intel MKL PARDISO\u5728\u6570\u503c\u5206\u89e3\u9636\u6bb5\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u9488\u5bf9\u591a\u6838\u5171\u4eab\u5185\u5b58\u67b6\u6784\u8bbe\u8ba1\u9ad8\u6548\u7684\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u5668\uff0c\u9700\u8981\u9002\u5e94\u4e0d\u540c\u7a00\u758f\u6a21\u5f0f\u7684\u7cfb\u6570\u77e9\u9635", "method": "\u91c7\u7528\u6df7\u5408\u6570\u503c\u6838\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u5e76\u884cLU\u5206\u89e3\u6280\u672f\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5904\u7406\u5404\u79cd\u7a00\u758f\u6a21\u5f0f", "result": "\u5728SuiteSparse\u77e9\u9635\u96c6\u768434\u4e2a\u7a00\u758f\u77e9\u9635\u6d4b\u8bd5\u4e2d\uff0c\u6570\u503c\u5206\u89e3\u9636\u6bb5\u76f8\u6bd4Intel MKL PARDISO\u67091.74\u500d\uff08\u5355\u6b21\u6c42\u89e3\uff09\u548c2.26\u500d\uff08\u91cd\u590d\u6c42\u89e3\uff09\u7684\u51e0\u4f55\u5e73\u5747\u6027\u80fd\u63d0\u5347", "conclusion": "HYLU\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u901a\u7528\u6c42\u89e3\u5668\uff0c\u5728\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90"}}
{"id": "2509.07379", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.07379", "abs": "https://arxiv.org/abs/2509.07379", "authors": ["Yuning Zhang", "Grant Pinkert", "Nan Yang", "Yanli Li", "Dong Yuan"], "title": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for Efficient MoE LLM Inference", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize.", "AI": {"tldr": "DuoServe-MoE\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9MoE\u6a21\u578b\u63a8\u7406\u7684\u4f18\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u79bbprefill\u548cdecode\u9636\u6bb5\u5e76\u91c7\u7528\u4e0d\u540c\u7684\u4e13\u5bb6\u8c03\u5ea6\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86GPU\u5185\u5b58\u4f7f\u7528\u5e76\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "MoE\u6a21\u578b\u867d\u7136\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u4e13\u5bb6\u5206\u652f\u4fdd\u6301\u4e86\u63a8\u7406\u6548\u7387\uff0c\u4f46\u5927\u91cf\u4e13\u5bb6\u6743\u91cd\u5e26\u6765\u4e86\u5de8\u5927\u7684GPU\u5185\u5b58\u538b\u529b\uff0c\u7279\u522b\u662f\u5728\u5355GPU\u670d\u52a1\u5668\u7b49\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u3002\u540c\u65f6\uff0cMoE\u63a8\u7406\u5305\u542bprefill\uff08\u5bc6\u96c6\u6fc0\u6d3b\uff09\u548cdecode\uff08\u7a00\u758f\u6fc0\u6d3b\uff09\u4e24\u4e2a\u4e0d\u540c\u9636\u6bb5\uff0c\u7edf\u4e00\u8c03\u5ea6\u7b56\u7565\u4f1a\u5bfc\u81f4\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51faDuoServe-MoE\u7cfb\u7edf\uff1a1\uff09\u660e\u786e\u5206\u79bbprefill\u548cdecode\u9636\u6bb5\uff1b2\uff09prefill\u9636\u6bb5\u4f7f\u7528\u53cc\u6d41CUDA\u7ba1\u9053\uff0c\u91cd\u53e0\u4e13\u5bb6\u6743\u91cd\u9884\u53d6\u548c\u975eMoE\u5c42\u8ba1\u7b97\uff1b3\uff09decode\u9636\u6bb5\u4f7f\u7528\u79bb\u7ebf\u8bad\u7ec3\u7684\u8f7b\u91cf\u7ea7\u5c42\u7ea7\u9884\u6d4b\u5668\u9884\u53d6\u6700\u53ef\u80fd\u6fc0\u6d3b\u7684\u4e13\u5bb6\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u3002", "result": "\u57284\u4f4dMixtral-8x7B\u548c8x22B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cDuoServe-MoE\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u63d0\u5347\u4e861.42\u52307.54\u500d\uff0c\u540c\u65f6\u5c06\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u4fdd\u6301\u5728\u5b8c\u6574\u6a21\u578b\u5927\u5c0f\u7684\u4ec515%\u3002", "conclusion": "DuoServe-MoE\u901a\u8fc7\u9488\u5bf9MoE\u63a8\u7406\u4e0d\u540c\u9636\u6bb5\u7684\u7279\u6027\u8bbe\u8ba1\u4e13\u95e8\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u5b58\u538b\u529b\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684MoE\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07551", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.07551", "abs": "https://arxiv.org/abs/2509.07551", "authors": ["Sean Bocirnea", "William J. Bowman"], "title": "Fast and Extensible Hybrid Embeddings with Micros", "comment": "13 pages", "summary": "Macro embedding is a popular approach to defining extensible shallow\nembeddings of object languages in Scheme like host languages. While macro\nembedding has even been shown to enable implementing extensible typed languages\nin systems like Racket, it comes at a cost: compile-time performance. In this\npaper, we revisit micros - syntax to intermediate representation (IR)\ntransformers, rather than source syntax to source syntax transformers (macros).\nMicro embedding enables stopping at an IR, producing a deep embedding and\nenabling high performance compile-time functions over an efficient IR, before\nshallowly embedding the IR back into source syntax. Combining micros with\nseveral design patterns to enable the IR and functions over it to be\nextensible, we achieve extensible hybrid embedding of statically typed\nlanguages with significantly improved compile-time compared to macro-embedding\napproaches. We describe our design patterns and propose new abstractions\npackaging these patterns.", "AI": {"tldr": "\u5c0f\u578b\u5d4c\u5165\u6280\u672f\u901a\u8fc7\u8bed\u6cd5\u5230\u4e2d\u95f4\u8868\u793a\u8f6c\u6362\u5668\u63d0\u9ad8\u7f16\u8bd1\u6027\u80fd\uff0c\u5145\u5206\u5229\u7528\u6df1\u5d4c\u5165\u7684\u9ad8\u6548\u80fd\u548c\u6d45\u5d4c\u5165\u7684\u6269\u5c55\u6027\uff0c\u5728\u4fdd\u6301\u7c7b\u578b\u8bed\u8a00\u6269\u5c55\u6027\u7684\u540c\u65f6\u663e\u8457\u7f29\u77ed\u7f16\u8bd1\u65f6\u95f4\u3002", "motivation": "\u5b8f\u5d4c\u5165\u6280\u672f\u867d\u7136\u652f\u6301\u6269\u5c55\u6027\u7c7b\u578b\u8bed\u8a00\u5b9e\u73b0\uff0c\u4f46\u5bfc\u81f4\u7f16\u8bd1\u65f6\u6027\u80fd\u95ee\u9898\uff0c\u9700\u8981\u627e\u5230\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u6269\u5c55\u6027\u7684\u540c\u65f6\u63d0\u5347\u7f16\u8bd1\u6548\u7387\u3002", "method": "\u91c7\u7528\u5fae\u5d4c\u5165\u6280\u672f\uff0c\u5c06\u8bed\u6cd5\u8f6c\u6362\u4e3a\u4e2d\u95f4\u8868\u793a\uff08IR\uff09\uff0c\u5728IR\u5c42\u9762\u8fdb\u884c\u9ad8\u6548\u7684\u7f16\u8bd1\u65f6\u51fd\u6570\u8fd0\u7b97\uff0c\u6700\u540e\u518d\u6d45\u5d4c\u5165\u56de\u6e90\u7801\u8bed\u6cd5\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u8bbe\u8ba1\u6a21\u5f0f\u4fdd\u8bc1IR\u548c\u5176\u51fd\u6570\u7684\u6269\u5c55\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u6df7\u5408\u5d4c\u5165\u9759\u6001\u7c7b\u578b\u8bed\u8a00\uff0c\u4e0e\u5b8f\u5d4c\u5165\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7f16\u8bd1\u65f6\u95f4\u5f97\u5230\u663e\u8457\u7f29\u77ed\u3002", "conclusion": "\u5fae\u5d4c\u5165\u6280\u672f\u901a\u8fc7\u7ed3\u5408\u6df1\u5d4c\u5165\u7684\u9ad8\u6548\u80fd\u548c\u6d45\u5d4c\u5165\u7684\u6269\u5c55\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5b8f\u5d4c\u5165\u5728\u7f16\u8bd1\u6027\u80fd\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6784\u5efa\u9ad8\u6548\u53ef\u6269\u5c55\u7c7b\u578b\u8bed\u8a00\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2509.07425", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.07425", "abs": "https://arxiv.org/abs/2509.07425", "authors": ["Sanyam Kaul", "Manaswini Piduguralla", "Gayathri Shreeya Patnala", "Sathya Peri"], "title": "Dependency-Aware Execution Mechanism in Hyperledger Fabric Architecture", "comment": null, "summary": "Hyperledger Fabric is a leading permissioned blockchain framework for\nenterprise use, known for its modular design and privacy features. While it\nstrongly supports configurable consensus and access control, Fabric can face\nchallenges in achieving high transaction throughput and low rejection rates\nunder heavy workloads. These performance limitations are often attributed to\nendorsement, ordering, and validation bottlenecks. Further, optimistic\nconcurrency control and deferred validation in Fabric may lead to resource\ninefficiencies and contention, as conflicting transactions are identified only\nduring the commit phase. To address these challenges, we propose a\ndependency-aware execution model for Hyperledger Fabric. Our approach includes:\n(a) a dependency flagging system during endorsement, marking transactions as\nindependent or dependent using a hashmap; (b) an optimized block construction\nin the ordering service that prioritizes independent transactions; (c) the\nincorporation of a Directed Acyclic Graph (DAG) within each block to represent\ndependencies; and (d) parallel execution of independent transactions at the\ncommitter, with dependent transactions processed according to DAG order.\nIncorporated in Hyperledger Fabric v2.5, our framework was tested on workloads\nwith varying dependency levels and system loads. Results show up to 40% higher\nthroughput and significantly reduced rejection rates in high-contention\nscenarios. This demonstrates that dependency-aware scheduling and DAG-based\nexecution can substantially enhance Fabric's scalability while remaining\ncompatible with its existing consensus and smart contract layers.", "AI": {"tldr": "\u4e3aHyperledger Fabric\u63d0\u51fa\u4f9d\u8d56\u611f\u77e5\u6267\u884c\u6a21\u578b\uff0c\u901a\u8fc7\u4f9d\u8d56\u6807\u8bb0\u3001\u4f18\u5316\u533a\u5757\u6784\u5efa\u3001DAG\u4f9d\u8d56\u8868\u793a\u548c\u5e76\u884c\u6267\u884c\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u548c\u964d\u4f4e\u62d2\u7edd\u7387", "motivation": "\u89e3\u51b3Fabric\u5728\u9ad8\u8d1f\u8f7d\u4e0b\u4ea4\u6613\u541e\u5410\u91cf\u4f4e\u548c\u62d2\u7edd\u7387\u9ad8\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u4e50\u89c2\u5e76\u53d1\u63a7\u5236\u548c\u5ef6\u8fdf\u9a8c\u8bc1\u5bfc\u81f4\u7684\u8d44\u6e90\u4f4e\u6548\u548c\u7ade\u4e89\u95ee\u9898", "method": "\u4f9d\u8d56\u6807\u8bb0\u7cfb\u7edf\u3001\u4f18\u5316\u533a\u5757\u6784\u5efa\u3001DAG\u4f9d\u8d56\u8868\u793a\u3001\u5e76\u884c\u6267\u884c\u72ec\u7acb\u4ea4\u6613\u3001\u6309DAG\u987a\u5e8f\u5904\u7406\u4f9d\u8d56\u4ea4\u6613", "result": "\u5728Hyperledger Fabric v2.5\u4e2d\u6d4b\u8bd5\u663e\u793a\uff0c\u9ad8\u7ade\u4e89\u573a\u666f\u4e0b\u541e\u5410\u91cf\u63d0\u534740%\uff0c\u62d2\u7edd\u7387\u663e\u8457\u964d\u4f4e", "conclusion": "\u4f9d\u8d56\u611f\u77e5\u8c03\u5ea6\u548cDAG\u6267\u884c\u80fd\u5927\u5e45\u63d0\u5347Fabric\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u73b0\u6709\u5171\u8bc6\u548c\u667a\u80fd\u5408\u7ea6\u5c42\u7684\u517c\u5bb9\u6027"}}
{"id": "2509.07609", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.07609", "abs": "https://arxiv.org/abs/2509.07609", "authors": ["Yichen Xu", "Oliver Bra\u010devac", "Cao Nguyen Pham", "Martin Odersky"], "title": "What's in the Box: Ergonomic and Expressive Capture Tracking over Generic Data Structures (Extended Version)", "comment": null, "summary": "Capturing types in Scala unify static effect and resource tracking with\nobject capabilities, enabling lightweight effect polymorphism with minimal\nnotational overhead. However, their expressiveness has been insufficient for\ntracking capabilities embedded in generic data structures, preventing them from\nscaling to the standard collections library -- an essential prerequisite for\nbroader adoption. This limitation stems from the inability to name capabilities\nwithin the system's notion of box types.\n  This paper develops System Capless, a new foundation for capturing types that\nprovides the theoretical basis for reach capabilities (rcaps), a novel\nmechanism for naming \"what's in the box.\" The calculus refines the universal\ncapability notion into a new scheme with existential and universal capture set\nquantification. Intuitively, rcaps witness existentially quantified capture\nsets inside the boxes of generic types in a way that does not require exposing\nexistential capture types in the surface language. We have fully mechanized the\nformal metatheory of System Capless in Lean, including proofs of type soundness\nand scope safety. System Capless supports the same lightweight notation of\ncapturing types plus rcaps, as certified by a type-preserving translation, and\nalso enables fully optional explicit capture-set quantification to increase\nexpressiveness.\n  Finally, we present a full reimplementation of capture checking in Scala 3\nbased on System Capless and migrate the entire Scala collections library and an\nasynchronous programming library to evaluate its practicality and ergonomics.\nOur results demonstrate that reach capabilities enable the adoption of capture\nchecking in production code with minimal changes and minimal-to-zero notational\noverhead in a vast majority of cases.", "AI": {"tldr": "System Capless\u901a\u8fc7\u5f15\u5165reach capabilities\u673a\u5236\uff0c\u89e3\u51b3\u4e86Scala\u6355\u83b7\u7c7b\u578b\u5728\u6cdb\u578b\u6570\u636e\u7ed3\u6784\u4e2d\u80fd\u529b\u8ddf\u8e2a\u7684\u9650\u5236\uff0c\u4f7f\u6355\u83b7\u68c0\u67e5\u80fd\u591f\u6269\u5c55\u5230\u6807\u51c6\u96c6\u5408\u5e93\uff0c\u5b9e\u73b0\u4e86\u6700\u5c0f\u5316\u8bed\u6cd5\u5f00\u9500\u7684\u6548\u679c\u591a\u6001\u6027\u3002", "motivation": "\u73b0\u6709\u7684Scala\u6355\u83b7\u7c7b\u578b\u5728\u8ddf\u8e2a\u5d4c\u5165\u6cdb\u578b\u6570\u636e\u7ed3\u6784\u4e2d\u7684\u80fd\u529b\u65b9\u9762\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6269\u5c55\u5230\u6807\u51c6\u96c6\u5408\u5e93\uff0c\u8fd9\u963b\u788d\u4e86\u5176\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u4e3b\u8981\u9650\u5236\u5728\u4e8e\u65e0\u6cd5\u5728\u7cfb\u7edf\u7684box\u7c7b\u578b\u6982\u5ff5\u4e2d\u547d\u540d\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86System Capless\u8ba1\u7b97\u7cfb\u7edf\uff0c\u5f15\u5165reach capabilities\u673a\u5236\u6765\u547d\u540d\"\u76d2\u5b50\u91cc\u7684\u5185\u5bb9\"\u3002\u8be5\u7cfb\u7edf\u5c06\u901a\u7528\u80fd\u529b\u6982\u5ff5\u7ec6\u5316\u4e3a\u5177\u6709\u5b58\u5728\u6027\u548c\u901a\u7528\u6027\u6355\u83b7\u96c6\u91cf\u5316\u7684\u65b0\u65b9\u6848\uff0c\u652f\u6301\u53ef\u9009\u663e\u5f0f\u6355\u83b7\u96c6\u91cf\u5316\u4ee5\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5728Scala 3\u4e2d\u57fa\u4e8eSystem Capless\u5b8c\u5168\u91cd\u65b0\u5b9e\u73b0\u4e86\u6355\u83b7\u68c0\u67e5\uff0c\u5e76\u8fc1\u79fb\u4e86\u6574\u4e2aScala\u96c6\u5408\u5e93\u548c\u5f02\u6b65\u7f16\u7a0b\u5e93\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660ereach capabilities\u80fd\u591f\u5728\u751f\u4ea7\u4ee3\u7801\u4e2d\u4ee5\u6700\u5c0f\u66f4\u6539\u548c\u6700\u5c0f\u5230\u96f6\u7684\u8bed\u6cd5\u5f00\u9500\u5b9e\u73b0\u6355\u83b7\u68c0\u67e5\u3002", "conclusion": "System Capless\u4e3a\u6355\u83b7\u7c7b\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7reach capabilities\u673a\u5236\u6210\u529f\u89e3\u51b3\u4e86\u6cdb\u578b\u6570\u636e\u7ed3\u6784\u4e2d\u80fd\u529b\u8ddf\u8e2a\u7684\u9650\u5236\uff0c\u4f7f\u6355\u83b7\u68c0\u67e5\u80fd\u591f\u5b9e\u9645\u5e94\u7528\u4e8e\u751f\u4ea7\u73af\u5883\uff0c\u5177\u6709\u5f88\u597d\u7684\u5b9e\u7528\u6027\u548c\u4eba\u673a\u5de5\u7a0b\u5b66\u7279\u6027\u3002"}}
{"id": "2509.07497", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.07497", "abs": "https://arxiv.org/abs/2509.07497", "authors": ["Hai Dinh-Tuan", "Tien Hung Nguyen", "Sanjeet Raj Pandey"], "title": "DREAMS: Decentralized Resource Allocation and Service Management across the Compute Continuum Using Service Affinity", "comment": null, "summary": "Modern manufacturing systems require adaptive computing infrastructures that\ncan respond to highly dynamic workloads and increasingly customized production\ndemands. The compute continuum emerges as a promising solution, enabling\nflexible deployment of microservices across distributed, heterogeneous domains.\nHowever, this paradigm also requires a novel approach to resource allocation\nand service placement, as traditional centralized solutions struggle to scale\neffectively, suffer from latency bottlenecks, and introduce single points of\nfailure. In this paper, we present DREAMS, a decentralized framework that\noptimizes microservice placement decisions collaboratively across different\ncomputational domains. At its core, DREAMS introduces agents that operate\nautonomously within each domain while coordinating globally through a\nRaft-based consensus algorithm and cost-benefit voting. This decentralized\narchitecture enables responsive, privacy-preserving, and fault-tolerant\ncoordination, making it particularly suitable given the growing prevalence of\nmulti-stakeholder scenarios across the compute continuum. In particular, within\nmodern manufacturing environments, DREAMS achieves globally optimized service\nplacements while maintaining high fault tolerance. Further evaluations\ndemonstrate that key coordination operations, such as Local Domain Manager\n(LDM) registration and migration voting, scale sub-linearly with the number of\ndomains, confirming the efficiency and scalability of our proposal.", "AI": {"tldr": "DREAMS\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u5fae\u670d\u52a1\u90e8\u7f72\u6846\u67b6\uff0c\u901a\u8fc7Raft\u5171\u8bc6\u7b97\u6cd5\u548c\u6210\u672c\u6548\u76ca\u6295\u7968\u5b9e\u73b0\u8de8\u8ba1\u7b97\u57df\u7684\u534f\u4f5c\u4f18\u5316\uff0c\u7279\u522b\u9002\u7528\u4e8e\u73b0\u4ee3\u5236\u9020\u4e1a\u7684\u52a8\u6001\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u73b0\u4ee3\u5236\u9020\u7cfb\u7edf\u9700\u8981\u9002\u5e94\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u548c\u5b9a\u5236\u5316\u751f\u4ea7\u9700\u6c42\uff0c\u4f20\u7edf\u96c6\u4e2d\u5f0f\u8d44\u6e90\u5206\u914d\u65b9\u6848\u5b58\u5728\u6269\u5c55\u6027\u5dee\u3001\u5ef6\u8fdf\u74f6\u9888\u548c\u5355\u70b9\u6545\u969c\u7b49\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u53bb\u4e2d\u5fc3\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faDREAMS\u6846\u67b6\uff0c\u5728\u6bcf\u4e2a\u8ba1\u7b97\u57df\u90e8\u7f72\u81ea\u4e3b\u4ee3\u7406\uff0c\u901a\u8fc7Raft\u5171\u8bc6\u7b97\u6cd5\u8fdb\u884c\u5168\u5c40\u534f\u8c03\uff0c\u91c7\u7528\u6210\u672c\u6548\u76ca\u6295\u7968\u673a\u5236\u5b9e\u73b0\u534f\u4f5c\u5f0f\u5fae\u670d\u52a1\u90e8\u7f72\u51b3\u7b56\u3002", "result": "DREAMS\u5728\u73b0\u4ee3\u5236\u9020\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5168\u5c40\u4f18\u5316\u7684\u670d\u52a1\u90e8\u7f72\uff0c\u5177\u6709\u9ad8\u5bb9\u9519\u6027\u3002\u5173\u952e\u534f\u8c03\u64cd\u4f5c\uff08\u5982LDM\u6ce8\u518c\u548c\u8fc1\u79fb\u6295\u7968\uff09\u968f\u57df\u6570\u91cf\u5448\u4e9a\u7ebf\u6027\u6269\u5c55\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "DREAMS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u54cd\u5e94\u8fc5\u901f\u3001\u4fdd\u62a4\u9690\u79c1\u3001\u5bb9\u9519\u6027\u5f3a\u7684\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u6846\u67b6\uff0c\u7279\u522b\u9002\u5408\u8ba1\u7b97\u8fde\u7eed\u4f53\u4e2d\u591a\u5229\u76ca\u76f8\u5173\u65b9\u573a\u666f\u7684\u5fae\u670d\u52a1\u90e8\u7f72\u4f18\u5316\u3002"}}
{"id": "2509.07506", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.07506", "abs": "https://arxiv.org/abs/2509.07506", "authors": ["Anjiang Wei", "Tianran Sun", "Yogesh Seenichamy", "Hang Song", "Anne Ouyang", "Azalia Mirhoseini", "Ke Wang", "Alex Aiken"], "title": "Astra: A Multi-Agent System for GPU Kernel Performance Optimization", "comment": null, "summary": "GPU kernel optimization has long been a central challenge at the intersection\nof high-performance computing and machine learning. Efficient kernels are\ncrucial for accelerating large language model (LLM) training and serving, yet\nattaining high performance typically requires extensive manual tuning.\nCompiler-based systems reduce some of this burden, but still demand substantial\nmanual design and engineering effort. Recently, researchers have explored using\nLLMs for GPU kernel generation, though prior work has largely focused on\ntranslating high-level PyTorch modules into CUDA code. In this work, we\nintroduce Astra, the first LLM-based multi-agent system for GPU kernel\noptimization. Unlike previous approaches, Astra starts from existing CUDA\nimplementations extracted from SGLang, a widely deployed framework for serving\nLLMs, rather than treating PyTorch modules as the specification. Within Astra,\nspecialized LLM agents collaborate through iterative code generation, testing,\nprofiling, and planning to produce kernels that are both correct and\nhigh-performance. On kernels from SGLang, Astra achieves an average speedup of\n1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study\nfurther demonstrates that LLMs can autonomously apply loop transformations,\noptimize memory access patterns, exploit CUDA intrinsics, and leverage fast\nmath operations to yield substantial performance gains. Our work highlights\nmulti-agent LLM systems as a promising new paradigm for GPU kernel\noptimization.", "AI": {"tldr": "Astra\u662f\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8eGPU\u5185\u6838\u4f18\u5316\uff0c\u4ece\u73b0\u6709CUDA\u4ee3\u7801\u51fa\u53d1\u800c\u975ePyTorch\u6a21\u5757\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b9e\u73b0\u5e73\u57471.32\u500d\u52a0\u901f", "motivation": "GPU\u5185\u6838\u4f18\u5316\u5bf9LLM\u8bad\u7ec3\u548c\u670d\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u624b\u52a8\u8c03\u4f18\uff0c\u73b0\u6709\u7f16\u8bd1\u5668\u7cfb\u7edf\u4ecd\u9700\u5927\u91cf\u4eba\u5de5\u8bbe\u8ba1\uff0c\u9700\u8981\u66f4\u81ea\u52a8\u5316\u7684\u4f18\u5316\u65b9\u6848", "method": "\u6784\u5efa\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\uff0c\u5404\u667a\u80fd\u4f53\u901a\u8fc7\u8fed\u4ee3\u4ee3\u7801\u751f\u6210\u3001\u6d4b\u8bd5\u3001\u6027\u80fd\u5206\u6790\u548c\u89c4\u5212\u534f\u4f5c\uff0c\u4eceSGLang\u6846\u67b6\u63d0\u53d6\u7684CUDA\u5b9e\u73b0\u5f00\u59cb\u4f18\u5316", "result": "\u5728SGLang\u5185\u6838\u4e0a\u4f7f\u7528OpenAI o4-mini\u96f6\u6837\u672c\u63d0\u793a\u5b9e\u73b0\u5e73\u57471.32\u500d\u52a0\u901f\uff0c\u80fd\u591f\u81ea\u4e3b\u5e94\u7528\u5faa\u73af\u53d8\u6362\u3001\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u3001\u5229\u7528CUDA\u5185\u7f6e\u51fd\u6570\u548c\u5feb\u901f\u6570\u5b66\u8fd0\u7b97", "conclusion": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e3aGPU\u5185\u6838\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b0\u8303\u5f0f\uff0c\u80fd\u591f\u81ea\u52a8\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347"}}
{"id": "2509.07567", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.07567", "abs": "https://arxiv.org/abs/2509.07567", "authors": ["Peter Arzt", "Felix Wolf"], "title": "Navigating Energy Doldrums: Modeling the Impact of Energy Price Volatility on HPC Cost of Ownership", "comment": "8 pages, 6 figures, 1 table", "summary": "Energy costs are a major factor in the total cost of ownership (TCO) for\nhigh-performance computing (HPC) systems. The rise of intermittent green energy\nsources and reduced reliance on fossil fuels have introduced volatility into\nelectricity markets, complicating energy budgeting. This paper explores\nvariable capacity as a strategy for managing HPC energy costs - dynamically\nadjusting compute resources in response to fluctuating electricity prices.\nWhile this approach can lower energy expenses, it risks underutilizing costly\nhardware. To evaluate this trade-off, we present a simple model that helps\noperators estimate the TCO impact of variable capacity strategies using key\nsystem parameters. We apply this model to real data from a university HPC\ncluster and assess how different scenarios could affect the cost-effectiveness\nof this approach in the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u6a21\u578b\u6765\u8bc4\u4f30HPC\u7cfb\u7edf\u4e2d\u53ef\u53d8\u5bb9\u91cf\u7b56\u7565\u5bf9\u603b\u62e5\u6709\u6210\u672c\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u6765\u5e94\u5bf9\u7535\u4ef7\u6ce2\u52a8\u3002", "motivation": "\u80fd\u6e90\u6210\u672c\u662f\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u603b\u62e5\u6709\u6210\u672c\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u7eff\u8272\u80fd\u6e90\u7684\u95f4\u6b47\u6027\u548c\u7535\u4ef7\u6ce2\u52a8\u4f7f\u5f97\u80fd\u6e90\u9884\u7b97\u590d\u6742\u5316\uff0c\u9700\u8981\u5bfb\u627e\u6709\u6548\u7684\u6210\u672c\u7ba1\u7406\u7b56\u7565\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7b80\u5355\u6a21\u578b\uff0c\u4f7f\u7528\u5173\u952e\u7cfb\u7edf\u53c2\u6570\u5e2e\u52a9\u8fd0\u8425\u5546\u4f30\u7b97\u53ef\u53d8\u5bb9\u91cf\u7b56\u7565\u5bf9TCO\u7684\u5f71\u54cd\uff0c\u5e76\u5c06\u8be5\u6a21\u578b\u5e94\u7528\u4e8e\u5927\u5b66HPC\u96c6\u7fa4\u7684\u5b9e\u9645\u6570\u636e\u3002", "result": "\u6a21\u578b\u80fd\u591f\u8bc4\u4f30\u4e0d\u540c\u60c5\u666f\u4e0b\u53ef\u53d8\u5bb9\u91cf\u7b56\u7565\u7684\u6210\u672c\u6548\u76ca\uff0c\u4e3aHPC\u7cfb\u7edf\u8fd0\u8425\u5546\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u3002", "conclusion": "\u53ef\u53d8\u5bb9\u91cf\u7b56\u7565\u662f\u7ba1\u7406HPC\u80fd\u6e90\u6210\u672c\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u5728\u8282\u80fd\u548c\u786c\u4ef6\u5229\u7528\u7387\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0c\u8be5\u6a21\u578b\u4e3a\u672a\u6765\u6210\u672c\u6548\u76ca\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2509.07595", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.07595", "abs": "https://arxiv.org/abs/2509.07595", "authors": ["Shiva Sai Krishna Anand Tokal", "Vaibhav Jha", "Anand Eswaran", "Praveen Jayachandran", "Yogesh Simmhan"], "title": "AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with FaaS-hosted MCP Services", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) has rapidly transformed various\nfields including code generation, text summarization, image generation and so\non. Agentic AI is a recent evolution that further advances this by coupling the\ndecision making and generative capabilities of LLMs with actions that can be\nperformed using tools. While seemingly powerful, Agentic systems often struggle\nwhen faced with numerous tools, complex multi-step tasks,and long-context\nmanagement to track history and avoid hallucinations. Workflow patterns such as\nChain-of-Thought (CoT) and ReAct help address this. Here, we define a novel\nagentic workflow pattern, AgentX, composed of stage designer, planner, and\nexecutor agents that is competitive or better than the state-of-the-art agentic\npatterns. We also leverage Model Context Protocol (MCP) tools, and propose two\nalternative approaches for deploying MCP servers as cloud Functions as a\nService (FaaS). We empirically evaluate the success rate, latency and cost for\nAgentX and two contemporary agentic patterns, ReAct and Magentic One, using\nthese the FaaS and local MCP server alternatives for three practical\napplications. This highlights the opportunities and challenges of designing and\ndeploying agentic workflows.", "AI": {"tldr": "AgentX\u662f\u4e00\u79cd\u65b0\u578b\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u6a21\u5f0f\uff0c\u7531\u9636\u6bb5\u8bbe\u8ba1\u5668\u3001\u89c4\u5212\u5668\u548c\u6267\u884c\u5668\u667a\u80fd\u4f53\u7ec4\u6210\uff0c\u5728\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u6a21\u5f0f\u3002", "motivation": "\u751f\u6210\u5f0fAI\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u9762\u5bf9\u4f17\u591a\u5de5\u5177\u3001\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u548c\u957f\u4e0a\u4e0b\u6587\u7ba1\u7406\u65f6\u7ecf\u5e38\u9047\u5230\u56f0\u96be\uff0c\u9700\u8981\u66f4\u597d\u7684\u5de5\u4f5c\u6d41\u6a21\u5f0f\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faAgentX\u5de5\u4f5c\u6d41\u6a21\u5f0f\uff0c\u5305\u542b\u9636\u6bb5\u8bbe\u8ba1\u5668\u3001\u89c4\u5212\u5668\u548c\u6267\u884c\u5668\u4e09\u4e2a\u667a\u80fd\u4f53\u7ec4\u4ef6\uff0c\u5e76\u5229\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae(MCP)\u5de5\u5177\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u5c06MCP\u670d\u52a1\u5668\u90e8\u7f72\u4e3a\u4e91\u51fd\u6570\u5373\u670d\u52a1(FaaS)\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9645\u5e94\u7528\u7684\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e86AgentX\u4e0eReAct\u3001Magentic One\u4e24\u79cd\u5f53\u4ee3\u667a\u80fd\u4f53\u6a21\u5f0f\u7684\u6210\u529f\u7387\u3001\u5ef6\u8fdf\u548c\u6210\u672c\uff0c\u8bc1\u660e\u4e86AgentX\u7684\u7ade\u4e89\u4f18\u52bf\u3002", "conclusion": "AgentX\u5de5\u4f5c\u6d41\u6a21\u5f0f\u5728\u8bbe\u8ba1\u548c\u90e8\u7f72\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u65b9\u9762\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u9047\u548c\u6311\u6218\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.07781", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.07781", "abs": "https://arxiv.org/abs/2509.07781", "authors": ["Lorenzo Martignetti", "Eli\u00e3 Batista", "Gianpaolo Cugola", "Fernando Pedone"], "title": "Scaling atomic ordering in shared memory", "comment": "21 pages, 16 figures", "summary": "Atomic multicast is a communication primitive used in dependable systems to\nensure consistent ordering of messages delivered to a set of replica groups.\nThis primitive enables critical services to integrate replication and sharding\n(i.e., state partitioning) to achieve fault tolerance and scalability. While\nseveral atomic multicast protocols have been developed for message-passing\nsystems, only a few are designed for the shared memory system model. This paper\nintroduces TRAM, an atomic multicast protocol specifically designed for shared\nmemory systems, leveraging an overlay tree architecture. Due to its simple and\npractical design, TRAM delivers exceptional performance, increasing throughput\nby more than 3$\\times$ and reducing latency by more than 2.3$\\times$ compared\nto state-of-the-art shared memory-based protocols. Additionally, it\nsignificantly outperforms message-passing-based protocols, boosting throughput\nby up to 5.9$\\times$ and reducing latency by up to 106$\\times$.", "AI": {"tldr": "TRAM\u662f\u4e00\u4e2a\u4e13\u4e3a\u5171\u4eab\u5185\u5b58\u7cfb\u7edf\u8bbe\u8ba1\u7684\u539f\u5b50\u591a\u64ad\u534f\u8bae\uff0c\u91c7\u7528\u8986\u76d6\u6811\u67b6\u6784\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u534f\u8bae", "motivation": "\u539f\u5b50\u591a\u64ad\u662f\u53ef\u9760\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u901a\u4fe1\u539f\u8bed\uff0c\u4f46\u73b0\u6709\u534f\u8bae\u4e3b\u8981\u9488\u5bf9\u6d88\u606f\u4f20\u9012\u7cfb\u7edf\uff0c\u5171\u4eab\u5185\u5b58\u7cfb\u7edf\u7684\u9ad8\u6027\u80fd\u539f\u5b50\u591a\u64ad\u534f\u8bae\u8f83\u5c11", "method": "\u57fa\u4e8e\u8986\u76d6\u6811\u67b6\u6784\u8bbe\u8ba1TRAM\u534f\u8bae\uff0c\u4e13\u95e8\u9488\u5bf9\u5171\u4eab\u5185\u5b58\u7cfb\u7edf\u8fdb\u884c\u4f18\u5316", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5171\u4eab\u5185\u5b58\u534f\u8bae\uff0c\u541e\u5410\u91cf\u63d0\u53473\u500d\u4ee5\u4e0a\uff0c\u5ef6\u8fdf\u964d\u4f4e2.3\u500d\u4ee5\u4e0a\uff1b\u76f8\u6bd4\u6d88\u606f\u4f20\u9012\u534f\u8bae\uff0c\u541e\u5410\u91cf\u63d0\u5347\u6700\u9ad85.9\u500d\uff0c\u5ef6\u8fdf\u964d\u4f4e\u6700\u9ad8106\u500d", "conclusion": "TRAM\u534f\u8bae\u901a\u8fc7\u7b80\u5355\u5b9e\u7528\u7684\u8bbe\u8ba1\uff0c\u5728\u5171\u4eab\u5185\u5b58\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u96c6\u6210\u590d\u5236\u548c\u5206\u7247\u7684\u5173\u952e\u670d\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u539f\u5b50\u591a\u64ad\u89e3\u51b3\u65b9\u6848"}}
