<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Optimism in Equality Saturation](https://arxiv.org/abs/2511.20782)
*Russel Arbore,Alvin Cheung,Max Willsey*

Main category: cs.PL

TL;DR: 提出一种基于抽象解释的算法，能够精确分析等式饱和中的循环程序，在SSA程序上实现比clang和gcc更精确的分析


<details>
  <summary>Details</summary>
Motivation: 当前e-class分析对循环程序分析悲观且无效，特别是在SSA形式的程序中

Method: 使用抽象解释算法，结合新的SSA语义，在等式饱和过程中进行乐观分析和非破坏性重写

Result: 原型系统在简单示例程序上的分析精度超过了clang和gcc

Conclusion: 该算法统一了乐观分析和非破坏性重写，能够更精确地分析循环程序

Abstract: Equality saturation is a technique for program optimization based on non-destructive rewriting and a form of program analysis called e-class analysis. The current form of e-class analysis is pessimistic and therefore ineffective at analyzing cyclic programs, such as those in SSA form. We propose an abstract interpretation algorithm that can precisely analyze cycles during equality saturation. This results in a unified algorithm for optimistic analysis and non-destructive rewriting. We instantiate this approach on a prototype abstract interpreter for SSA programs using a new semantics of SSA. Our prototype can analyze simple example programs more precisely than clang and gcc.

</details>


### [2] [Towards Computational UIP in Cubical Agda](https://arxiv.org/abs/2511.21209)
*Yee-Jian Tan,Andreas Nuyts,Dominique Devriese*

Main category: cs.PL

TL;DR: 本文探讨了在Cubical Agda中实现h-Set Cubical Type Theory的方法，分析了UIP的不同表述形式及其计算规则，并实现了一个无Glue类型的Cubical Agda变体。


<details>
  <summary>Details</summary>
Motivation: Cubical Type Theory具有QIT和函数外延性等优势，但HoTT的无限等式层级在形式化中可能变得繁琐。虽然可以通过截断到h-Set级别来保留这些特性，但目前在Cubical Agda中只有两种不令人满意的方法来实现h-Set Cubical Type Theory。

Method: 分析UIP的不同表述形式及其计算规则，评估它们在Cubical Agda中实现的适用性，并实现一个无Glue类型的Cubical Agda变体。

Result: 开发了与假设UIP兼容的无Glue Cubical Agda变体，为未来在Cubical Agda中实现UIP奠定了基础。

Conclusion: 本文为在Cubical Agda中实现h-Set Cubical Type Theory提供了理论基础和初步实现，解决了当前实现方法的局限性。

Abstract: Some advantages of Cubical Type Theory, as implemented by Cubical Agda, over intensional Martin-Löf Type Theory include Quotient Inductive Types (QITs), which exist as instances of Higher Inductive Types, and functional extensionality, which is provable in Cubical Type Theory. However, HoTT features an infinite hierarchy of equalities that may become unwieldy in formalisations. Fortunately, QITs and functional extensionality are both preserved even if the equality levels of Cubical Type Theory are truncated to only homotopical Sets (h-Sets). In other words, removing the univalence axiom from Cubical Type Theory and instead postulating a conflicting axiom: the Uniqueness of Identity Proofs (UIP) postulate. Since univalence is proved in Cubical Type Theory from the so-called Glue Types, therefore, it is known that one can first remove the Glue Types (thus removing univalence) and then set-truncate all equalities (essentially assuming UIP), à la XTT. The result is a "h-Set Cubical Type Theory" that retains features such as functional extensionality and QITs.
  However, in Cubical Agda, there are currently only two unsatisfying ways to achieve h-Set Cubical Type Theory. The first is to give up on the canonicity of the theory and simply postulate the UIP axiom, while the second way is to use a standard result stating "type formers preserve h-levels" to manually prove UIP for every defined type. The latter is, however, laborious work best suited for an automatic implementation by the proof assistant. In this project, we analyse formulations of UIP and detail their computation rules for Cubical Agda, and evaluate their suitability for implementation. We also implement a variant of Cubical Agda without Glue, which is already compatible with postulated UIP, in anticipation of a future implementation of UIP in Cubical Agda.

</details>


### [3] [SV-LIB 1.0: A Standard Exchange Format for Software-Verification Tasks](https://arxiv.org/abs/2511.21509)
*Dirk Beyer,Gidon Ernst,Martin Jonáš,Marian Lingsch-Rosenfeld*

Main category: cs.PL

TL;DR: SV-LIB是一种用于软件验证任务的交换格式和中间语言，基于命令式编程语言概念，使用SMT-LIB表示表达式和类型，支持验证见证格式。


<details>
  <summary>Details</summary>
Motivation: 现有验证工具多为特定语言开发，但许多验证方法实际上是语言无关的。为促进技术转移，需要一种通用格式让验证工具实现可以跨语言复用。

Method: 设计SV-LIB格式，基于命令式编程语言概念，使用SMT-LIB表示表达式和类型，定义验证见证格式和见证验证任务规范。

Result: 提出了SV-LIB 1.0版本，包括设计目标、语法和非形式化语义，支持程序、规范和验证见证的表示。

Conclusion: SV-LIB为软件验证提供了通用的交换格式，便于验证工具集成和见证验证，未来计划扩展形式化语义和并发支持。

Abstract: In the past two decades, significant research and development effort went into the development of verification tools for individual languages, such asC, C++, and Java. Many of the used verification approaches are in fact language-agnostic and it would be beneficial for the technology transfer to allow for using the implementations also for other programming and modeling languages. To address the problem, we propose SV-LIB, an exchange format and intermediate language for software-verification tasks, including programs, specifications, and verification witnesses. SV-LIBis based on well-known concepts from imperative programming languages and uses SMT-LIB to represent expressions and sorts used in the program. This makes it easy to parse and to build into existing infrastructure, since many verification tools are based on SMT solvers already. Furthermore, SV-LIBdefines a witness format for both correct and incorrect SV-LIB programs, together with means for specifying witness-validation tasks. This makes it possible both to implement independent witness validators and to reuse some verifiers also as validators for witnesses. This paper presents version 1.0 of the SV-LIBformat, including its design goals, the syntax, and informal semantics. Formal semantics and further extensions to concurrency are planned for future versions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures](https://arxiv.org/abs/2511.20780)
*Alison Silva,Gustavo Callou*

Main category: cs.DC

TL;DR: 本文提出了一种基于随机Petri网的方法来分析私有云环境中Nextcloud文件服务器的可用性，评估了四种冗余策略对系统可用性的影响。


<details>
  <summary>Details</summary>
Motivation: 随着云存储平台在学术和商业环境中的普及，可靠性成为关键需求。特别是对于寻求公共云服务替代方案的组织，评估这些系统的可靠性至关重要。

Method: 使用随机Petri网(SPNs)建模方法，在Apache CloudStack托管的私有云环境中分析Nextcloud文件服务器的可用性，评估了四种架构配置：基线、主机级冗余、虚拟机冗余以及两者组合。

Result: 结果显示，在主机和虚拟机级别同时实施冗余策略能显著提高系统可用性并减少预期停机时间。

Conclusion: 所提出的方法为评估私有云可用性提供了有效工具，并支持基础设施设计决策。

Abstract: Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions.

</details>


### [5] [Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks](https://arxiv.org/abs/2511.20834)
*Dionysios Adamopoulos,Anastasia Poulopoulou,Georgios Goumas,Christina Giannoula*

Main category: cs.DC

TL;DR: Spira是一个基于GPU的稀疏卷积引擎，通过利用体素坐标的整数性、空间有界性和几何连续性特性，显著提升了3D点云网络的性能。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏卷积引擎未能充分利用体素坐标的三个关键特性（整数值、空间有界性、几何连续性），导致核映射构建时的预处理和后处理开销过高。

Method: 提出Spira引擎，包含：一次性搜索算法构建核映射、打包原生处理方案访问体素坐标、双数据流执行机制适应层特性、网络级并行化策略并发构建所有层核映射。

Result: Spira在端到端推理中平均加速1.71倍（最高2.31倍），在逐层执行中平均加速2.13倍（最高3.32倍）。

Conclusion: Spira通过充分利用体素坐标特性，显著提升了稀疏卷积的性能，为3D点云网络在自动驾驶和AR/VR应用提供了高效解决方案。

Abstract: Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work identifies three key properties of voxel coordinates: they are integer-valued, bounded within a limited spatial range, and geometrically continuous-neighboring voxels on the same object surface are highly likely to exist at small spatial offsets from each other. Prior SpC engines do not fully exploit these properties and suffer from high pre-processing and post-processing overheads during kernel map construction. To address this, we design Spira, the first voxel-property-aware SpC engine for GPUs. Spira proposes: (i) a high-performance one-shot search algorithm that builds the kernel map with no preprocessing and high memory locality, (ii) an effective packed-native processing scheme that accesses packed voxel coordinates at low cost, (iii) a flexible dual-dataflow execution mechanism that efficiently computes output feature vectors by adapting to layer characteristics, and (iv) a network-wide parallelization strategy that builds kernel maps for all SpC layers concurrently at network start. Our evaluation shows that Spira significantly outperforms prior SpC engines by 1.71x on average and up to 2.31x for end-to-end inference, and by 2.13x on average and up to 3.32x for layer-wise execution across diverse layer configurations.

</details>


### [6] [Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows](https://arxiv.org/abs/2511.20975)
*Yinwei Dai,Zhuofu Chen,Anand Iyer,Ravi Netravali*

Main category: cs.DC

TL;DR: Aragog系统通过动态调整工作流配置来优化LLM推理成本，在保持精度的同时显著提升吞吐量和降低延迟


<details>
  <summary>Details</summary>
Motivation: 现有的工作流配置选择方法在请求执行前固定配置，无法适应系统负载的动态变化，导致成本效率低下

Method: 将问题分解为一次性路由步骤识别精度保持配置，和基于实时系统观察的廉价每阶段调度器选择配置

Result: 在多样化工作流和模型家族中，Aragog将最大服务吞吐量提升50.0-217.0%，峰值请求率下中位延迟降低32.5-78.9%

Conclusion: Aragog通过运行时动态配置调整，实现了工作流服务的高效扩展，在保持精度的同时大幅提升性能

Abstract: Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\% and reduces median latency by 32.5--78.9\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.

</details>


### [7] [A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving](https://arxiv.org/abs/2511.20982)
*Junhan Liao,Minxian Xu,Wanyi Zheng,Yan Wang,Kejiang Ye,Rajkumar Buyya,Chengzhong Xu*

Main category: cs.DC

TL;DR: DOPD是一个动态LLM推理系统，通过实时监控负载动态调整预填充和解码实例分配比例，解决异构工作负载下的生产者-消费者不平衡问题，显著提升系统吞吐量和响应性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理系统将预填充和解码阶段解耦到不同GPU上，但异构工作负载导致两个实例类型间的生产者-消费者不平衡，造成资源分配不匹配。

Method: 提出DOPD系统，基于实时负载监控动态调整预填充/解码实例分配比例，结合适当的请求调度策略，解决混合长度请求下的资源分配不匹配问题。

Result: 相比vLLM和DistServe，DOPD将系统吞吐量提升1.5倍，P90首token延迟降低67.5%，P90每输出token时间降低22.8%，SLO达成率超过99%且使用更少额外资源。

Conclusion: DOPD通过动态P/D比例调整技术，基于历史负载进行主动重新配置，有效解决了LLM推理系统中的资源不平衡问题，显著提升了系统性能和资源利用率。

Abstract: To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.

</details>


### [8] [Handling of Memory Page Faults during Virtual-Address RDMA](https://arxiv.org/abs/2511.21018)
*Antonis Psistakis*

Main category: cs.DC

TL;DR: 该论文实现了一种与DMA引擎集成的页错误处理机制，通过ARM SMMU检测错误并通过硬件-软件解决方案解决，避免了传统RDMA技术中内存固定的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RDMA技术需要固定内存地址空间来避免页错误，但这带来了编程复杂性、内存使用限制和效率低下等问题，且无法完全防止页错误。

Method: 在ExaNeSt项目中，通过修改Linux SMMU驱动、开发新软件库、改变DMA引擎硬件和调整DMA调度逻辑，实现了一个硬件-软件的页错误处理机制。

Result: 在ExaNeSt的QFDB平台上进行了实验，评估了该机制并与内存固定和预错误处理等替代方案进行了比较。

Conclusion: 提出的页错误处理机制相比传统方法具有优势，能够更有效地处理RDMA通信中的页错误问题。

Abstract: Nowadays, avoiding system calls during cluster communication (e.g., in Data Centers and High Performance Computing) in modern high-speed interconnection networks has become a necessity, due to the high overhead of multiple data copies between kernel and user space. User-level zero-copy Remote Direct Memory Access (RDMA) technologies address this problem by improving performance and reducing system energy consumption. However, traditional RDMA engines cannot tolerate page faults and therefore use various techniques to avoid them.
  State-of-the-art RDMA approaches typically rely on pinning address spaces or multiple pages per application. This method introduces long-term disadvantages due to increased programming complexity (pinning and unpinning buffers), limits on how much memory can be pinned, and inefficient memory utilization. In addition, pinning does not fully prevent page faults because modern operating systems apply internal optimization mechanisms, such as Transparent Huge Pages (THP), which are enabled by default in Linux.
  This thesis implements a page-fault handling mechanism integrated with the DMA engine of the ExaNeSt project. Faults are detected by the ARM System Memory Management Unit (SMMU) and resolved through a hardware-software solution that can request retransmission when needed. This mechanism required modifications to the Linux SMMU driver, the development of a new software library, changes to the DMA engine hardware, and adjustments to the DMA scheduling logic. Experiments were conducted on the Quad-FPGA Daughter Board (QFDB) of ExaNeSt, which uses Xilinx Zynq UltraScale+ MPSoCs.
  Finally, we evaluate our mechanism and compare it against alternatives such as pinning and pre-faulting, and discuss the advantages of our approach.

</details>


### [9] [Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM](https://arxiv.org/abs/2511.21413)
*Tim Trappen,Robert Keßler,Roland Pabel,Viktor Achter,Stefan Wesner*

Main category: cs.DC

TL;DR: 提出了一种在超级计算机RAMSES上集成vLLM、Slurm和Kubernetes来服务LLM的解决方案，能够高效处理100-1000个并发请求，端到端延迟仅增加约500毫秒。


<details>
  <summary>Details</summary>
Motivation: 由于AI推理需求增长，特别是在高等教育领域，需要利用现有基础设施的新解决方案。传统HPC操作模式不适合同步、面向用户的动态AI应用工作负载。

Method: 在超级计算机RAMSES上集成vLLM、Slurm和Kubernetes来服务大型语言模型(LLM)。

Result: 初始基准测试表明，该架构能够高效扩展到100、500和1000个并发请求，端到端延迟仅增加约500毫秒。

Conclusion: 提出的解决方案成功解决了传统HPC在AI推理应用中的适应性挑战，实现了高效可扩展的LLM服务。

Abstract: Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.

</details>


### [10] [MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training](https://arxiv.org/abs/2511.21431)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Yueqiang Chen,Baoguo He,Hongfeng Sun,Ziqing Yin,Shangchao Su,Zhiyan Cui,Liang Dong,Xiyuan Li,Lingbin Wang,Jianwei He,Jiesong Ma,Weikang Huang,Jianglei Tong,Dongdong Gao,Jian Zhang,Hong Tian*

Main category: cs.DC

TL;DR: MemFine是一个内存感知的细粒度调度框架，通过分块重计算策略解决MoE模型训练中的内存瓶颈问题，在保证吞吐量的同时显著降低内存使用。


<details>
  <summary>Details</summary>
Motivation: 大规模MoE模型训练面临严重的内存瓶颈，动态token路由导致的负载不平衡使得GPU内存溢出，限制了模型的可扩展性。现有的负载均衡方法会损害模型精度且在内存受限硬件上失效。

Method: 将token分布和专家计算分解为可管理的块，采用分块重计算策略，通过理论内存模型动态优化内存效率和吞吐量之间的平衡。

Result: 实验表明，MemFine相比全重计算基线减少了48.03%的激活内存，提升了4.42%的吞吐量，能够在内存受限的GPU上实现稳定的大规模MoE训练。

Conclusion: MemFine框架有效解决了MoE训练中的内存瓶颈问题，为在内存受限硬件上训练大规模MoE模型提供了可行的解决方案。

Abstract: The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.

</details>


### [11] [Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation](https://arxiv.org/abs/2511.21535)
*Morteza Sadeghi*

Main category: cs.DC

TL;DR: 通过引入数据冗余来改善MLFMA中近场算子的GPU内存局部性，提出基于局部性度量的分析模型预测性能趋势，在电磁求解器和恒星动力学代码中验证，获得最高7倍内核加速但端到端应用加速有限。


<details>
  <summary>Details</summary>
Motivation: MLFMA中的近场算子在GPU上由于内存局部性差成为性能瓶颈，需要改善内存访问模式来提升性能。

Method: 引入数据冗余减少内存访问分散，提出结合数据量和访问分散度的局部性度量分析模型，在DBIM-MLFMA和PhotoNs-2.0两个应用中验证。

Result: 内核速度最高提升7倍，但由于数据重构开销增加，端到端应用加速仅1.04倍。模型能可靠捕捉不同问题规模和密度下的性能趋势。

Conclusion: 数据冗余可提升GPU上P2P算子性能，前提是局部性收益超过数据移动成本，该技术可最小化代码修改注入现有实现。

Abstract: The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.

</details>


### [12] [Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases](https://arxiv.org/abs/2511.21612)
*Shahir Abdullah,Syed Rohit Zaman*

Main category: cs.DC

TL;DR: 提出了一个二维的伸缩平面模型，将节点数量与单节点资源向量结合起来，通过联合的水平与垂直伸缩（对角线路径）来优化数据库性能、成本和协调开销。


<details>
  <summary>Details</summary>
Motivation: 现代云数据库将伸缩视为二元决策（水平伸缩或垂直伸缩），这种一维视图限制了性能优化，因为数据库性能、成本和协调开销是水平弹性与单节点资源（CPU、内存、网络带宽、存储IOPS）共同作用的结果。

Method: 引入伸缩平面模型，每个分布式数据库配置表示为点(H,V)，其中H是节点数，V是资源向量。提出DIAGONALSCALE算法，通过离散局部搜索评估水平、垂直和对角线移动，选择满足SLA约束下最小化多目标函数的配置。

Result: 对角线伸缩相比仅水平或仅垂直自动伸缩，可将p95延迟降低高达40%，每查询成本降低高达37%，并减少2到5倍的重新平衡。

Conclusion: 研究结果强调了多维伸缩模型的必要性，并为下一代云数据库系统的自动伸缩提供了基础。

Abstract: Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems.

</details>


### [13] [AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI](https://arxiv.org/abs/2511.21661)
*Beth Plale,Neelesh Karthikeyan,Isuru Gamage,Joe Stubbs,Sachith Withana*

Main category: cs.DC

TL;DR: 该研究评估了在Patra模型卡服务器中采用模型上下文协议(MCP)作为接口的效益与权衡，比较了MCP与REST接口的开销，并探讨了MCP在动态模型卡环境中的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统AI/ML模型卡在训练时的一次性评估无法反映模型在其生命周期中的实际使用情况，需要将模型卡作为动态对象进行研究。

Method: 通过在ICICLE AI Institute软件生态系统中嵌入Patra模型卡，研究模型卡作为动态对象，并评估采用MCP作为接口的效益与权衡。

Result: 定量评估显示MCP相比REST接口存在一定开销，但核心问题是MCP启用的活跃会话，这是关于动态模型卡环境中适用性的定性问题。

Conclusion: MCP作为模型卡服务器接口在动态模型卡环境中具有潜力，但需要权衡其开销与启用的活跃会话能力。

Abstract: AI/ML model cards can contain a benchmarked evaluation of an AI/ML model against intended use but a one time assessment during model training does not get at how and where a model is actually used over its lifetime. Through Patra Model Cards embedded in the ICICLE AI Institute software ecosystem we study model cards as dynamic objects. The study reported here assesses the benefits and tradeoffs of adopting the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative assessment shows the overhead of MCP as compared to a REST interface. The core question however is of active sessions enabled by MCP; this is a qualitative question of fit and use in the context of dynamic model cards that we address as well.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI](https://arxiv.org/abs/2511.21232)
*Muhammed Yildirim,Ozcan Ozturk*

Main category: cs.AR

TL;DR: 提出一种用于深度可分离卷积的零缓冲硬件加速器架构，通过像素级数据流消除中间特征图存储，显著减少数据移动和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI和TinyML应用中深度可分离卷积的多阶段设计带来的内存瓶颈问题，减少中间特征图传输的高能耗和延迟。

Method: 设计基于融合像素级数据流的硬件加速器架构，作为RISC-V处理器的定制功能单元，通过紧密耦合流水线直接计算所有DSC阶段的单个输出像素。

Result: 相比传统逐层执行减少87%数据移动，在FPGA上实现59.3倍加速，ASIC合成显示紧凑面积（0.284-1.20 mm²）和低功耗（233-910 mW）。

Conclusion: 证实了在TinyML资源约束下实现零缓冲数据流的可行性，为边缘AI加速器克服内存墙提供了有效策略。

Abstract: The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.

</details>


### [15] [Bombyx: OpenCilk Compilation for FPGA Hardware Acceleration](https://arxiv.org/abs/2511.21346)
*Mohamed Shahawy,Julien de Castelnau,Paolo Ienne*

Main category: cs.AR

TL;DR: Bombyx是一个编译器工具链，将OpenCilk程序转换为Cilk-1风格的中间表示，使CPU任务级并行应用能高效映射到FPGA空间架构上。


<details>
  <summary>Details</summary>
Motivation: 现有系统探索了FPGA上任务级并行的架构支持，但OpenCilk的隐式任务模型需要昂贵的硬件上下文切换，不适合FPGA的流式特性。

Method: 采用Cilk-1的显式延续传递模型，支持多种编译目标：OpenCilk兼容运行时和可综合处理单元生成器，并引入解耦访问执行优化。

Result: 实现了自动生成高性能处理单元，改善了内存计算重叠和整体吞吐量。

Conclusion: Bombyx成功将CPU导向的任务级并行应用有效映射到FPGA空间架构，克服了传统模型的局限性。

Abstract: Task-level parallelism (TLP) is a widely used approach in software where independent tasks are dynamically created and scheduled at runtime. Recent systems have explored architectural support for TLP on field-programmable gate arrays (FPGAs), often leveraging high-level synthesis (HLS) to create processing elements (PEs). In this paper, we present Bombyx, a compiler toolchain that lowers OpenCilk programs into a Cilk-1-inspired intermediate representation, enabling efficient mapping of CPU-oriented TLP applications to spatial architectures on FPGAs. Unlike OpenCilk's implicit task model, which requires costly context switching in hardware, Cilk-1 adopts explicit continuation-passing - a model that better aligns with the streaming nature of FPGAs. Bombyx supports multiple compilation targets: one is an OpenCilk-compatible runtime for executing Cilk-1-style code using the OpenCilk backend, and another is a synthesizable PE generator designed for HLS tools like Vitis HLS. Additionally, we introduce a decoupled access-execute optimization that enables automatic generation of high-performance PEs, improving memory-compute overlap and overall throughput.

</details>


### [16] [A Jammer-Resilient 2.87 mm$^2$ 1.28 MS/s 310 mW Multi-Antenna Synchronization ASIC in 65 nm](https://arxiv.org/abs/2511.21451)
*Flurin Arquint,Oscar Castañeda,Gian Marti,Christoph Studer*

Main category: cs.AR

TL;DR: 首个抗干扰多天线时间同步ASIC实现，支持单天线发射器与16天线接收器同步，可抵御最多2天线智能干扰器。


<details>
  <summary>Details</summary>
Motivation: 解决无线通信系统中时间同步信号易受干扰攻击的问题，特别是针对智能干扰器的威胁。

Method: 采用多天线处理算法，在65nm工艺下实现ASIC设计，支持1.28 MS/s采样率。

Result: 芯片核心面积2.87 mm²，功耗310 mW，成功实现了抗干扰时间同步功能。

Conclusion: 该ASIC证明了多天线处理在硬件层面实现抗干扰时间同步的可行性，为安全无线通信系统提供了硬件基础。

Abstract: We present the first ASIC implementation of jammer-resilient multi-antenna time synchronization. The ASIC implements a recent algorithm that mitigates jamming attacks on synchronization signals using multi-antenna processing. Our design supports synchronization between a single-antenna transmitter and a 16-antenna receiver while mitigating smart jammers with up to two transmit antennas. The fabricated 65 nm ASIC has a core area of 2.87 mm$^2$, consumes a power of 310 mW, and supports a sampling rate of 1.28 mega-samples per second (MS/s).

</details>


### [17] [A 0.32 mm$^2$ 100 Mb/s 223 mW ASIC in 22FDX for Joint Jammer Mitigation, Channel Estimation, and SIMO Data Detection](https://arxiv.org/abs/2511.21461)
*Jonas Elmiger,Fabian Stuber,Oscar Castañeda,Gian Marti,Christoph Studer*

Main category: cs.AR

TL;DR: 首款SIMO接收器ASIC芯片，集成抗干扰、信道估计和数据检测功能，采用MAED算法，在22nm FD-SOI工艺下实现100Mb/s吞吐量，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决智能干扰器对无线通信系统的威胁，传统方法无法有效应对动态变化的智能干扰，需要开发能同时处理干扰抑制、信道估计和数据检测的集成解决方案。

Method: 采用MAED算法，通过非线性优化问题统一处理干扰器估计与消除、信道估计和数据检测，支持8个接收天线，能对抗智能干扰器和连续波干扰器。

Result: 在22nm FD-SOI工艺下实现，核心面积0.32mm²，功耗223mW，吞吐量100Mb/s，相比现有抗干扰检测器，单用户吞吐量提升3倍，面积效率提升4.5倍。

Conclusion: 该ASIC成功展示了MAED算法的硬件实现可行性，为抗干扰无线通信系统提供了高效集成的解决方案，在吞吐量和能效方面显著优于现有技术。

Abstract: We present the first single-input multiple-output (SIMO) receiver ASIC that jointly performs jammer mitigation, channel estimation, and data detection. The ASIC implements a recent algorithm called siMultaneous mitigAtion, Estimation, and Detection (MAED). MAED mitigates smart jammers via spatial filtering using a nonlinear optimization problem that unifies jammer estimation and nulling, channel estimation, and data detection to achieve state-of-the-art error-rate performance under jamming. The design supports eight receive antennas and enables mitigation of smart jammers as well as of barrage jammers. The ASIC is fabricated in 22 nm FD-SOI, has a core area of 0.32 mm$^2$, and achieves a throughput of 100 Mb/s at 223 mW, thus delivering 3$\times$ higher per-user throughput and 4.5$\times$ higher area efficiency than the state-of-the-art jammer-resilient detector.

</details>


### [18] [Modeling and Optimizing Performance Bottlenecks for Neuromorphic Accelerators](https://arxiv.org/abs/2511.21549)
*Jason Yik,Walter Gallego Gomez,Andrew Cheng,Benedetto Leto,Alessandro Pierro,Noah Pacik-Nelson,Korneel Van den Berghe,Vittorio Fra,Andreea Danielescu,Gianvito Urgese,Vijay Janapa Reddi*

Main category: cs.AR

TL;DR: 本文首次对神经形态加速器进行了全面的性能边界和瓶颈分析，揭示了传统指标的不足，提出了floorline性能模型和优化方法，在保持准确率的同时实现了3.86倍运行时间改进和3.38倍能耗降低。


<details>
  <summary>Details</summary>
Motivation: 神经形态加速器具有独特的架构特性，其性能动态与传统加速器有根本差异。现有优化方法依赖聚合稀疏度和操作计数，但这些指标对实际部署性能的提升效果未知，需要深入理解影响工作负载性能的关键因素。

Method: 采用理论分析建模和实证表征相结合的方法，分析三个真实神经形态加速器（Brainchip AKD1000、Synsense Speck、Intel Loihi 2），建立三种瓶颈状态（内存受限、计算受限、流量受限），并开发floorline性能模型和结合稀疏感知训练与floorline指导分区的优化方法。

Result: 识别出三种不同的加速器瓶颈状态，确定了可能表现出这些瓶颈状态的工作负载配置特征。提出的优化方法在保持准确率不变的情况下，相比先前手动调优配置实现了3.86倍运行时间改进和3.38倍能耗降低。

Conclusion: 神经形态加速器的性能优化需要超越传统指标，理解具体的瓶颈状态。floorline性能模型能够有效识别性能边界并指导工作负载优化，结合稀疏感知训练的分区方法能够显著提升性能效率。

Abstract: Neuromorphic accelerators offer promising platforms for machine learning (ML) inference by leveraging event-driven, spatially-expanded architectures that naturally exploit unstructured sparsity through co-located memory and compute. However, their unique architectural characteristics create performance dynamics that differ fundamentally from conventional accelerators. Existing workload optimization approaches for neuromorphic accelerators rely on aggregate network-wide sparsity and operation counting, but the extent to which these metrics actually improve deployed performance remains unknown. This paper presents the first comprehensive performance bound and bottleneck analysis of neuromorphic accelerators, revealing the shortcomings of the conventional metrics and offering an understanding of what facets matter for workload performance. We present both theoretical analytical modeling and extensive empirical characterization of three real neuromorphic accelerators: Brainchip AKD1000, Synsense Speck, and Intel Loihi 2. From these, we establish three distinct accelerator bottleneck states, memory-bound, compute-bound, and traffic-bound, and identify which workload configuration features are likely to exhibit these bottleneck states. We synthesize all of our insights into the floorline performance model, a visual model that identifies performance bounds and informs how to optimize a given workload, based on its position on the model. Finally, we present an optimization methodology that combines sparsity-aware training with floorline-informed partitioning. Our methodology achieves substantial performance improvements at iso-accuracy: up to 3.86x runtime improvement and 3.38x energy reduction compared to prior manually-tuned configurations.

</details>
