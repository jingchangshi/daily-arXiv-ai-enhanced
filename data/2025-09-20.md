<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [DeliverC: Teaching Pointers through GenAI-Powered Game-Based Learning](https://arxiv.org/abs/2509.14496)
*Wyatt Petula,Anushcka Joshi,Peggy Tu,Amrutha Somasundar,Suman Saha*

Main category: cs.PL

TL;DR: DeliverC是一个集成GPT-4-mini的GenAI增强游戏，为C语言指针学习提供实时自适应支持，通过个性化提示和动态生成挑战来提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 虽然基于游戏的学习在编程教育中广泛应用，但很少有工具能为复杂主题（如C指针）提供自适应、实时的支持。

Method: 开发了DeliverC游戏系统，集成GPT-4-mini提供个性化提示和动态生成指针相关挑战。通过25名本科生的试点研究，收集游戏数据和15项问卷调查（涵盖动机、自我效能、元认知和反馈质量等构念）。

Result: 大多数学生在使用工具后感到更自信和反思性更强，错误率随着学生在支架式关卡中的进展而降低。但参与度随任务难度增加而下降，部分学生反映收到不清晰或模糊的反馈。

Conclusion: DeliverC可以增强系统编程中的参与度和理解，但AI生成反馈仍需改进。研究强调了将GenAI与基于游戏的学习相结合，在传统挑战性编程领域中支持个性化和交互式实践的潜力。

Abstract: While game-based learning is widely used in programming education, few tools
offer adaptive, real-time support for complex topics, such as C pointers. We
present DeliverC, a GenAI-enhanced game that integrates GPT-4-mini to provide
personalized hints and generate pointer-related challenges on the fly. In a
pilot study involving 25 undergraduate students, we investigated the impact of
the system on learning through gameplay data and a 15-item survey that covered
constructs such as motivation, self-efficacy, metacognition, and feedback
quality. Results show that most students felt more confident and reflective
after using the tool, and error rates decreased as students progressed through
scaffolded levels. However, participation decreased with task difficulty, and
some students reported receiving unclear or vague feedback. These findings
suggest that DeliverC can enhance engagement and understanding in systems
programming, although refinement in AI-generated feedback is still needed. Our
study highlights the potential of combining GenAI with game-based learning to
support personalized and interactive practice in traditionally challenging
programming domains.

</details>


### [2] [Refinement-Types Driven Development: A study](https://arxiv.org/abs/2509.15005)
*Facundo Domínguez,Arnaud Spiwack*

Main category: cs.PL

TL;DR: 本文主张将SMT求解器更广泛地应用于日常编程，通过精化类型和Liquid Haskell实现编译器静态检查的无缝集成，提升普通类型检查器的程序组合能力。


<details>
  <summary>Details</summary>
Motivation: 挑战SMT求解器仅用于形式化方法和验证的传统观念，探索其在日常编程任务中的应用潜力，使普通编程更简单有趣。

Method: 采用精化类型（以Liquid Haskell为代表）将SMT求解器集成到编译器静态检查中，并通过处理编译器中的绑定器作用域案例研究进行验证。

Result: 开发了Liquid Haskell求解器的有限映射理论原型实现来支持案例研究，展示了SMT求解器在日常编程中的实际应用价值。

Conclusion: 精化类型和SMT求解器的结合为日常编程提供了新的可能性，有望使普通编程变得更简单和愉快。

Abstract: This paper advocates for the broader application of SMT solvers in everyday
programming, challenging the conventional wisdom that these tools are solely
for formal methods and verification. We claim that SMT solvers, when seamlessly
integrated into a compiler's static checks, significantly enhance the
capabilities of ordinary type checkers in program composition. Specifically, we
argue that refinement types, as embodied by Liquid Haskell, enable the use of
SMT solvers in mundane programming tasks. Through a case study on handling
binder scopes in compilers, we envision a future where ordinary programming is
made simpler and more enjoyable with the aid of refinement types and SMT
solvers. As a secondary contribution, we present a prototype implementation of
a theory of finite maps for Liquid Haskell's solver, developed to support our
case study.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless and GPU-Based Training Architectures](https://arxiv.org/abs/2509.14920)
*Amine Barrak,Fabio Petrillo,Fehmi Jaafar*

Main category: cs.DC

TL;DR: 本文对SPIRT、ScatterReduce、AllReduce和MLLess等无服务器分布式机器学习架构进行了比较分析，发现SPIRT在训练时间效率、通信开销和容错能力方面表现优异，虽然初始设置成本较高但具有长期经济优势。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习对可扩展且经济高效的训练解决方案需求日益增长，无服务器计算通过提供动态可扩展性和资源高效执行来应对这些挑战。

Method: 比较分析了SPIRT、ScatterReduce、AllReduce和MLLess等多种无服务器分布式ML架构，重点关注训练时间效率、成本效益、通信开销和容错能力等关键指标。

Result: SPIRT通过RedisAI支持的并行批处理和数据库内操作策略，显著减少了训练时间和通信开销。传统架构存在可扩展性挑战，对故障和对抗性攻击的脆弱性各不相同。

Conclusion: SPIRT在性能和经济性方面具有优势，研究为结合现有系统最有效特性的新模型开发奠定了基础，指明了未来研究方向。

Abstract: The field of distributed machine learning (ML) faces increasing demands for
scalable and cost-effective training solutions, particularly in the context of
large, complex models. Serverless computing has emerged as a promising paradigm
to address these challenges by offering dynamic scalability and
resource-efficient execution. Building upon our previous work, which introduced
the Serverless Peer Integrated for Robust Training (SPIRT) architecture, this
paper presents a comparative analysis of several serverless distributed ML
architectures. We examine SPIRT alongside established architectures like
ScatterReduce, AllReduce, and MLLess, focusing on key metrics such as training
time efficiency, cost-effectiveness, communication overhead, and fault
tolerance capabilities. Our findings reveal that SPIRT provides significant
improvements in reducing training times and communication overhead through
strategies such as parallel batch processing and in-database operations
facilitated by RedisAI. However, traditional architectures exhibit scalability
challenges and varying degrees of vulnerability to faults and adversarial
attacks. The cost analysis underscores the long-term economic benefits of SPIRT
despite its higher initial setup costs. This study not only highlights the
strengths and limitations of current serverless ML architectures but also sets
the stage for future research aimed at developing new models that combine the
most effective features of existing systems.

</details>


### [4] [Conditional Prior-based Non-stationary Channel Estimation Using Accelerated Diffusion Models](https://arxiv.org/abs/2509.15182)
*Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Umer,Asad Aali,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 提出条件先验扩散方法用于非平稳无线信道估计，通过历史条件化学习和时间编码器捕捉信道瞬时相干性，在3GPP基准测试中优于多种基线方法


<details>
  <summary>Details</summary>
Motivation: 移动丰富的城市微小区环境中无线信道具有非平稳特性，传统和深度学习估计器性能会因信道分布随时间变化而下降

Method: 使用条件先验扩散学习历史条件化评分函数来去噪信道快照，采用跨时间注意力机制的时间编码器，SNR匹配初始化和几何间隔缩短调度

Result: 在3GPP基准测试中在所有SNR范围内均获得比LMMSE、GMM、LSTM和LDAMP基线更低的NMSE，表现出稳定性能和强高SNR保真度

Conclusion: 条件先验扩散方法能有效处理非平稳信道估计问题，通过时间上下文学习和优化扩散过程实现优于现有方法的性能

Abstract: Wireless channels in motion-rich urban microcell (UMi) settings are
non-stationary; mobility and scatterer dynamics shift the distribution over
time, degrading classical and deep estimators. This work proposes conditional
prior diffusion for channel estimation, which learns a history-conditioned
score to denoise noisy channel snapshots. A temporal encoder with cross-time
attention compresses a short observation window into a context vector, which
captures the channel's instantaneous coherence and steers the denoiser via
feature-wise modulation. In inference, an SNR-matched initialization selects
the diffusion step whose marginal aligns with the measured input SNR, and the
process follows a shortened, geometrically spaced schedule, preserving the
signal-to-noise trajectory with far fewer iterations. Temporal
self-conditioning with the previous channel estimate and a training-only
smoothness penalty further stabilizes evolution without biasing the test-time
estimator. Evaluations on a 3GPP benchmark show lower NMSE across all SNRs than
LMMSE, GMM, LSTM, and LDAMP baselines, demonstrating stable performance and
strong high SNR fidelity.

</details>


### [5] [Channel Prediction under Network Distribution Shift Using Continual Learning-based Loss Regularization](https://arxiv.org/abs/2509.15192)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ibtsaam Qadir,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 通过控制损失正则化的持续学习框架，解决异构网络环境下频道预测的分布偏移问题，SI方法在性能和资源消耗方面都显著优于标准EWC。


<details>
  <summary>Details</summary>
Motivation: 现代无线网络中，移动用户穿越异构网络配置时遇到频道预测性能治思问题，传统预测器在分布偏移情况下NMSE上升37.5%，需要解决灾难忘记问题。

Method: 提出基于损失正则化的持续学习框架，通过添加惩罚项选择性保保对之前配置关键的网络参数，同时适应新环境。研究了弹性权重固化(EWC)和突触智能(SI)两种正则化策略。

Result: 在3GPP场景和多种架构下，SI将高SNR情况下的NMSE底值降低了1.8dB(约32-34%)，EWC达到了1.4dB(约17-28%)。SI保持O(M)内存复杂度，而标准EWC需要O(MK)复杂度。

Conclusion: 通过损失正则化的持续学习框架有效解决了异构网络环境下频道预测的分布偏移问题，SI方法在性能和资源效率方面都显示出优势，适合资源受限的无线基础设施。

Abstract: Modern wireless networks face critical challenges when mobile users traverse
heterogeneous network configurations with varying antenna layouts, carrier
frequencies, and scattering statistics. Traditional predictors degrade under
distribution shift, with NMSE rising by 37.5\% during cross-configuration
handovers. This work addresses catastrophic forgetting in channel prediction by
proposing a continual learning framework based on loss regularization. The
approach augments standard training objectives with penalty terms that
selectively preserve network parameters essential for previous configurations
while enabling adaptation to new environments. Two prominent regularization
strategies are investigated: Elastic Weight Consolidation (EWC) and Synaptic
Intelligence (SI). Across 3GPP scenarios and multiple architectures, SI lowers
the high-SNR NMSE floor by up to 1.8 dB ($\approx$32--34\%), while EWC achieves
up to 1.4 dB ($\approx$17--28\%). Notably, standard EWC incurs
$\mathcal{O}(MK)$ complexity (storing $M$ Fisher diagonal entries and
corresponding parameter snapshots across $K$ tasks) unless consolidated,
whereas SI maintains $\mathcal{O}(M)$ memory complexity (storing $M$ model
parameters), independent of task sequence length, making it suitable for
resource-constrained wireless infrastructure

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations](https://arxiv.org/abs/2509.14388)
*Lennart Bamberg,Filippo Minnella,Roberto Bosio,Fabrizio Ottati,Yuebin Wang,Jongmin Lee,Luciano Lavagno,Adam Fuks*

Main category: cs.AR

TL;DR: eIQ Neutron NPU通过灵活的数据驱动架构和编译器优化，在相同TOPS和内存资源下比领先的嵌入式NPU性能提升1.8倍，最高可达4倍。


<details>
  <summary>Details</summary>
Motivation: 传统NPU的峰值TOPS指标不能真实反映实际性能，且通常与更高的硅成本相关。需要在保持灵活性的同时最大化计算利用率。

Method: 采用灵活的数据驱动架构设计，配合编译器使用约束编程方法根据工作负载特性优化计算和数据移动。

Result: 在标准AI基准测试中，相比领先的嵌入式NPU和编译器堆栈，在相同TOPS和内存资源下平均加速1.8倍（峰值4倍）。即使面对计算和内存资源翻倍的NPU，仍能提供最高3.3倍的性能提升。

Conclusion: eIQ Neutron NPU通过架构和编译器的协同设计，在保持灵活性的同时显著提升了计算效率，为边缘AI推理提供了高效的解决方案。

Abstract: Neural Processing Units (NPUs) are key to enabling efficient AI inference in
resource-constrained edge environments. While peak tera operations per second
(TOPS) is often used to gauge performance, it poorly reflects real-world
performance and typically rather correlates with higher silicon cost. To
address this, architects must focus on maximizing compute utilization, without
sacrificing flexibility. This paper presents the eIQ Neutron efficient-NPU,
integrated into a commercial flagship MPU, alongside co-designed compiler
algorithms. The architecture employs a flexible, data-driven design, while the
compiler uses a constrained programming approach to optimize compute and data
movement based on workload characteristics. Compared to the leading embedded
NPU and compiler stack, our solution achieves an average speedup of 1.8x (4x
peak) at equal TOPS and memory resources across standard AI-benchmarks. Even
against NPUs with double the compute and memory resources, Neutron delivers up
to 3.3x higher performance.

</details>


### [7] [Shift-Left Techniques in Electronic Design Automation: A Survey](https://arxiv.org/abs/2509.14551)
*Xinyue Wu,Zixuan Li,Fan Hu,Ting Lin,Xiaotian Zhao,Runxi Wang,Xinfei Guo*

Main category: cs.AR

TL;DR: 本文对EDA领域中的Shift-Left方法进行了全面调研，分析了其在芯片设计流程中的进展、挑战和未来方向，重点关注AI技术和开源设计流程对预测建模能力的增强作用。


<details>
  <summary>Details</summary>
Motivation: 随着芯片设计复杂度增加，传统串行设计流程效率低下。Shift-Left方法通过创建数字孪生和融合多个设计步骤，使设计师能够更早建立关联并优化设计，但准确复制下游行为和确定采用时机仍存在挑战。

Method: 采用文献综述方法，对EDA和更广泛设计生态系统中的现有和新兴Shift-Left研究范式进行全面调研，整理相关论文并组织在GitHub仓库中。

Result: 研究发现AI技术和开源设计流程的兴起显著增强了预测和建模能力，使数据驱动方法在EDA社区中日益重要，从而增强了当前工具中的Shift-Left功能。

Conclusion: Shift-Left方法在EDA领域取得了显著进展，但仍面临挑战。随着智能EDA工具和技术的发展，数据驱动方法和AI技术将继续推动Shift-Left范式的发展，为芯片设计带来新的机遇。

Abstract: The chip design process involves numerous steps, beginning with defining
product requirements and progressing through architectural planning,
system-level design, and the physical layout of individual circuit blocks. As
the enablers of large-scale chip development, Electronic Design Automation
(EDA) tools play a vital role in helping designers achieve high-quality
results. The Shift-Left methodology introduces a pathway toward creating
digital twins and fusing multiple design steps, thereby transitioning
traditionally sequential, physically-aware processes into virtual design
environments. This shift allows designers to establish stronger correlations
earlier and optimize designs more effectively. However, challenges remain,
especially in accurately replicating downstream behaviors and determining the
right scope and timing for adoption. These challenges, in turn, have revealed
new opportunities for EDA vendors, physical designers, and logic designers
alike. As the industry advances toward intelligent EDA tools and techniques, it
is timely to reflect on Shift-Left progress made and the challenges that
remain. The rise of AI techniques and the momentum of open-source design flows
have significantly strengthened prediction and modeling capabilities, making
data-driven methods increasingly relevant to the EDA community. This, in turn,
enhances the ''Shift-Left'' features embedded in current tools. In this paper,
we present a comprehensive survey of existing and emerging paradigms in
Shift-Left research within EDA and the broader design ecosystem. Our goal is to
provide a unique perspective on the state of the field and its future
directions. Relevant papers mentioned are organized in
https://github.com/iCAS-SJTU/Shift-Left-EDA-Papers.

</details>


### [8] [DeepAssert: An LLM-Aided Verification Framework with Fine-Grained Assertion Generation for Modules with Extracted Module Specifications](https://arxiv.org/abs/2509.14668)
*Yonghao Wang,Jiaxin Zhou,Hongqin Lyu,Zhiteng Chao,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: DeepAssert是一个基于LLM的验证框架，能够分析模块间的调用关系并提取模块级规范，自动生成细粒度的深度断言，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有断言生成方法要么依赖设计规范（只能生成顶层断言），要么需要黄金RTL模型（难以获得），无法生成针对模块内部功能的深度断言。

Method: 提出DeepAssert框架：分析模块调用关系，提取各模块的I/O端口信息和独立规范，然后指导LLM自动生成模块级细粒度深度断言。

Result: DeepAssert在生成高质量模块深度断言方面显著优于AssertLLM和Spec2Assertion等方法，且能提升这些方法的整体断言质量。

Conclusion: DeepAssert提供了一个更全面有效的验证流程，能够解决现有方法在生成深度断言方面的局限性。

Abstract: Assertion-Based Verification (ABV) is a crucial method for ensuring that
logic designs conform to their architectural specifications. However, existing
assertion generation methods primarily rely on information either from the
design specification, or register-transfer level (RTL) code. The former methods
are typically limited to generating assertions for the top-level design. As the
top-level design is composed of different modules without module-level
specifications, they are unable to generate deep assertions that target the
internal functionality of modules. The latter methods often rely on a golden
RTL model, which is difficult to obtain. To address the above limitations, this
paper presents a novel large language model (LLM)-aided verification framework
named DeepAssert. DeepAssert is capable of analyzing the invocation
relationships between modules and extracting independent specifications for
each module with its I/O port information. These extracted specifications are
subsequently used to guide LLMs to automatically generate fine-grained deep
assertions for these modules. Our evaluation demonstrates that DeepAssert
significantly outperforms existing methods such as AssertLLM and Spec2Assertion
in generating high-quality deep assertions for modules. Furthermore, when
integrated with these methods, DeepAssert can enhance the overall quality of
the assertions generated. This allows for a more comprehensive and effective
verification process.

</details>


### [9] [LEAP: LLM Inference on Scalable PIM-NoC Architecture with Balanced Dataflow and Fine-Grained Parallelism](https://arxiv.org/abs/2509.14781)
*Yimin Wang,Yue Jiet Chong,Xuanyao Fong*

Main category: cs.AR

TL;DR: LEAP是一种非冯·诺依曼架构的LLM推理加速器，通过PIM和NoC协同设计，在Llama模型上相比A100 GPU实现了2.55倍吞吐量提升和71.94倍能效提升


<details>
  <summary>Details</summary>
Motivation: LLM推理需求日益增长，但其大张量尺寸和计算复杂度给内存、计算和数据总线带来了巨大挑战，需要新的硬件架构来解决这些问题

Method: 提出计算/内存/通信协同设计的非冯·诺依曼加速器LEAP，结合PIM和计算NoC，根据数据动态性将矩阵乘法分配到PIM或NoC以最大化数据局部性，采用启发式设计空间探索优化模型划分和映射，使用细粒度并行和分块技术实现高吞吐数据流

Result: 在Llama 1B/8B/13B模型上评估，相比A100 GPU实现了约2.55倍的吞吐量(tokens/sec)提升和约71.94倍的能效(tokens/Joule)提升

Conclusion: LEAP架构通过PIM和NoC的协同设计，有效解决了LLM推理中的内存、计算和通信瓶颈，显著提升了推理性能和能效

Abstract: Large language model (LLM) inference has been a prevalent demand in daily
life and industries. The large tensor sizes and computing complexities in LLMs
have brought challenges to memory, computing, and databus. This paper proposes
a computation/memory/communication co-designed non-von Neumann accelerator by
aggregating processing-in-memory (PIM) and computational network-on-chip (NoC),
termed LEAP. The matrix multiplications in LLMs are assigned to PIM or NoC
based on the data dynamicity to maximize data locality. Model partition and
mapping are optimized by heuristic design space exploration. Dedicated
fine-grained parallelism and tiling techniques enable high-throughput dataflow
across the distributed resources in PIM and NoC. The architecture is evaluated
on Llama 1B/8B/13B models and shows $\sim$2.55$\times$ throughput (tokens/sec)
improvement and $\sim$71.94$\times$ energy efficiency (tokens/Joule) boost
compared to the A100 GPU.

</details>


### [10] [NEURAL: An Elastic Neuromorphic Architecture with Hybrid Data-Event Execution and On-the-fly Attention Dataflow](https://arxiv.org/abs/2509.15036)
*Yuehai Chen,Farhad Merchant*

Main category: cs.AR

TL;DR: NEURAL是一种基于混合数据-事件执行范式的神经形态架构，通过解耦稀疏感知处理和神经元计算，使用弹性FIFO，实现低延迟高能效的脉冲神经网络处理。


<details>
  <summary>Details</summary>
Motivation: 现有SNN硬件实现受限于脉冲稀疏性和多时间步执行，导致延迟增加和能效降低，需要新的架构设计来解决这些问题。

Method: 提出NEURAL架构，采用混合数据-事件执行范式，解耦稀疏感知处理与神经元计算；集成W2TTFS机制替代平均池化；使用基于知识蒸馏的训练框架构建单时间步SNN模型。

Result: 在算法层面，VGG-11模型在CIFAR-10和CIFAR-100上的准确率分别提升3.20%和5.13%；在架构层面，相比现有SNN加速器，资源利用率降低50%，能效提升1.97倍。

Conclusion: NEURAL架构通过创新的混合执行范式和训练方法，有效解决了SNN硬件实现的延迟和能效问题，为高效神经形态计算提供了可行方案。

Abstract: Spiking neural networks (SNNs) have emerged as a promising alternative to
artificial neural networks (ANNs), offering improved energy efficiency by
leveraging sparse and event-driven computation. However, existing hardware
implementations of SNNs still suffer from the inherent spike sparsity and
multi-timestep execution, which significantly increase latency and reduce
energy efficiency. This study presents NEURAL, a novel neuromorphic
architecture based on a hybrid data-event execution paradigm by decoupling
sparsity-aware processing from neuron computation and using elastic
first-in-first-out (FIFO). NEURAL supports on-the-fly execution of spiking
QKFormer by embedding its operations within the baseline computing flow without
requiring dedicated hardware units. It also integrates a novel
window-to-time-to-first-spike (W2TTFS) mechanism to replace average pooling and
enable full-spike execution. Furthermore, we introduce a knowledge distillation
(KD)-based training framework to construct single-timestep SNN models with
competitive accuracy. NEURAL is implemented on a Xilinx Virtex-7 FPGA and
evaluated using ResNet-11, QKFResNet-11, and VGG-11. Experimental results
demonstrate that, at the algorithm level, the VGG-11 model trained with KD
improves accuracy by 3.20% on CIFAR-10 and 5.13% on CIFAR-100. At the
architecture level, compared to existing SNN accelerators, NEURAL achieves a
50% reduction in resource utilization and a 1.97x improvement in energy
efficiency.

</details>


### [11] [Voyager: An End-to-End Framework for Design-Space Exploration and Generation of DNN Accelerators](https://arxiv.org/abs/2509.15205)
*Kartik Prabhu,Jeffrey Yu,Xinyuan Allen Pan,Zhouhua Xie,Abigail Aleshire,Zihan Chen,Ammar Ali Ratnani,Priyanka Raina*

Main category: cs.AR

TL;DR: Voyager是一个基于HLS的DNN加速器自动生成框架，支持多种数据类型和量化方案，能够快速进行设计空间探索并生成高性能的tapeout-ready设计。


<details>
  <summary>Details</summary>
Motivation: 现有DNN加速器设计方法自动化程度低、参数化有限、不支持多种数据类型和量化方案，且缺乏端到端的软件编译器支持，设计过程耗时且难以扩展。

Method: 基于高层次综合(HLS)框架，提供跨技术节点、时钟频率和规模的可配置参数，支持浮点数、posit和整数等多种数据类型，以及用户自定义格式和量化方案，并集成PyTorch编译器进行端到端网络映射。

Result: 在最新视觉和语言模型上验证，设计利用率高达99.8%，相比现有生成器延迟降低61%、面积减少56%，性能与手工优化加速器相当但自动化程度更高。

Conclusion: Voyager框架成功解决了DNN加速器设计自动化难题，提供了高效的设计空间探索和硬件生成能力，支持多种数据类型和量化方案，显著提升了设计效率。

Abstract: While deep neural networks (DNNs) have achieved state-of-the-art performance
in fields from computer vision to natural language processing, efficiently
running these computationally demanding models requires hardware accelerators.
However, designing these accelerators is a time-consuming, labor-intensive
process that does not scale well. While prior efforts have sought to automate
DNN accelerator generation, they offer limited parameterization, cannot produce
high-performance, tapeout-ready designs, provide limited support for datatypes
and quantization schemes, and lack an integrated, end-to-end software compiler.
This work proposes Voyager, a high-level synthesis (HLS)-based framework for
design space exploration (DSE) and generation of DNN accelerators. Voyager
overcomes the limitations of prior work by offering extensive configurability
across technology nodes, clock frequencies, and scales, with customizable
parameters such as number of processing elements, on-chip buffer sizes, and
external memory bandwidth. Voyager supports a wider variety of datatypes and
quantization schemes versus prior work, including both built-in floating-point,
posit and integer formats, as well as user-defined formats with both per-tensor
scaling and microscaling quantization. Voyager's PyTorch-based compiler
efficiently maps networks end-to-end on the generated hardware, with support
for quantization, fusion, and tiling. We evaluate Voyager on state-of-the-art
vision and language models. Voyager enables fast DSE with full-dataset accuracy
evaluation for datatypes and quantization schemes. Generated designs achieve a
high utilization across models and scales, up to 99.8%, and outperform prior
generators with up to 61% lower latency and 56% lower area. Compared to
hand-optimized accelerators, Voyager achieves comparable performance, while
offering much greater automation in design and workload mapping.

</details>
