{"id": "2512.20073", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.20073", "abs": "https://arxiv.org/abs/2512.20073", "authors": ["Hongyang Shang", "Shuai Dong", "Ye Ke", "Arindam Basu"], "title": "3D Stack In-Sensor-Computing (3DS-ISC): Accelerating Time-Surface Construction for Neuromorphic Event Cameras", "comment": null, "summary": "This work proposes a 3D Stack In-Sensor-Computing (3DS-ISC) architecture for efficient event-based vision processing. A real-time normalization method using an exponential decay function is introduced to construct the time-surface, reducing hardware usage while preserving temporal information. The circuit design utilizes the leakage characterization of Dynamic Random Access Memory(DRAM) for timestamp normalization. Custom interdigitated metal-oxide-metal capacitor (MOMCAP) is used to store the charge and low leakage switch (LL switch) is used to extend the effective charge storage time. The 3DS-ISC architecture integrates sensing, memory, and computation to overcome the memory wall problem, reducing power, latency, and reducing area by 69x, 2.2x and 1.9x, respectively, compared with its 2D counterpart. Moreover, compared to works using a 16-bit SRAM to store timestamps, the ISC analog array can reduce power consumption by three orders of magnitude. In real computer vision (CV) tasks, we applied the spatial-temporal correlation filter (STCF) for denoise, and 3D-ISC achieved almost equivalent accuracy compared to the digital implementation using high precision timestamps. As for the image classification, time-surface constructed by 3D-ISC is used as the input of GoogleNet, achieving 99% on N-MNIST, 85% on N-Caltech101, 78% on CIFAR10-DVS, and 97% on DVS128 Gesture, comparable with state-of-the-art results on each dataset. Additionally, the 3D-ISC method is also applied to image reconstruction using the DAVIS240C dataset, achieving the highest average SSIM (0.62) among three methods. This work establishes a foundation for real-time, resource-efficient event-based processing and points to future integration of advanced computational circuits for broader applications.", "AI": {"tldr": "\u63d0\u51fa3D\u5806\u53e0\u4f20\u611f\u5668\u5185\u8ba1\u7b97\u67b6\u6784\uff0c\u5229\u7528DRAM\u6f0f\u7535\u7279\u6027\u5b9e\u73b0\u65f6\u95f4\u8868\u9762\u5b9e\u65f6\u5f52\u4e00\u5316\uff0c\u663e\u8457\u964d\u4f4e\u529f\u8017\u548c\u9762\u79ef\uff0c\u5728\u4e8b\u4ef6\u89c6\u89c9\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u4f20\u7edf\u4e8b\u4ef6\u89c6\u89c9\u5904\u7406\u9762\u4e34\u5185\u5b58\u5899\u95ee\u9898\uff0c\u529f\u8017\u548c\u5ef6\u8fdf\u8f83\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f20\u611f\u5668\u5185\u8ba1\u7b97\u67b6\u6784\u6765\u652f\u6301\u5b9e\u65f6\u5904\u7406", "method": "\u91c7\u75283D\u5806\u53e0\u96c6\u6210\u4f20\u611f\u3001\u5b58\u50a8\u548c\u8ba1\u7b97\uff0c\u5229\u7528DRAM\u6f0f\u7535\u7279\u6027\u5b9e\u73b0\u6307\u6570\u8870\u51cf\u65f6\u95f4\u5f52\u4e00\u5316\uff0c\u4f7f\u7528MOMCAP\u5b58\u50a8\u7535\u8377\u548c\u4f4e\u6f0f\u7535\u5f00\u5173\u5ef6\u957f\u5b58\u50a8\u65f6\u95f4", "result": "\u76f8\u6bd42D\u67b6\u6784\u964d\u4f4e\u529f\u801769\u500d\u3001\u5ef6\u8fdf2.2\u500d\u3001\u9762\u79ef1.9\u500d\uff1b\u76f8\u6bd416\u4f4dSRAM\u964d\u4f4e\u529f\u8017\u4e09\u4e2a\u6570\u91cf\u7ea7\uff1b\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u5206\u7c7b\u7cbe\u5ea6\uff0c\u56fe\u50cf\u91cd\u5efaSSIM\u6700\u9ad8", "conclusion": "3D\u4f20\u611f\u5668\u5185\u8ba1\u7b97\u67b6\u6784\u4e3a\u5b9e\u65f6\u9ad8\u6548\u4e8b\u4ef6\u89c6\u89c9\u5904\u7406\u5960\u5b9a\u57fa\u7840\uff0c\u672a\u6765\u53ef\u96c6\u6210\u66f4\u5148\u8fdb\u8ba1\u7b97\u7535\u8def\u6269\u5c55\u5e94\u7528\u8303\u56f4"}}
{"id": "2512.20198", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.20198", "abs": "https://arxiv.org/abs/2512.20198", "authors": ["Huizheng Wang", "Taiquan Wei", "Hongbin Wang", "Zichuan Wang", "Xinru Tang", "Zhiheng Yue", "Shaojun Wei", "Yang Hu", "Shouyi Yin"], "title": "Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling", "comment": "Accepted for publication in IEEE Transactions on Computers", "summary": "Large language models (LLMs) rely on self-attention for contextual understanding, demanding high-throughput inference and large-scale token parallelism (LTPP). Existing dynamic sparsity accelerators falter under LTPP scenarios due to stage-isolated optimizations. Revisiting the end-to-end sparsity acceleration flow, we identify an overlooked opportunity: cross-stage coordination can substantially reduce redundant computation and memory access. We propose STAR, a cross-stage compute- and memory-efficient algorithm-hardware co-design tailored for Transformer inference under LTPP. STAR introduces a leading-zero-based sparsity prediction using log-domain add-only operations to minimize prediction overhead. It further employs distributed sorting and a sorted updating FlashAttention mechanism, guided by a coordinated tiling strategy that enables fine-grained stage interaction for improved memory efficiency and latency. These optimizations are supported by a dedicated STAR accelerator architecture, achieving up to 9.2$\\times$ speedup and 71.2$\\times$ energy efficiency over A100, and surpassing SOTA accelerators by up to 16.1$\\times$ energy and 27.1$\\times$ area efficiency gains. Further, we deploy STAR onto a multi-core spatial architecture, optimizing dataflow and execution orchestration for ultra-long sequence processing. Architectural evaluation shows that, compared to the baseline design, Spatial-STAR achieves a 20.1$\\times$ throughput improvement.", "AI": {"tldr": "STAR\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u957f\u5e8f\u5217\u5e76\u884c\u5904\u7406\u7684\u8de8\u9636\u6bb5\u534f\u540c\u7a00\u758f\u52a0\u901f\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u9884\u6d4b\u7a00\u758f\u6027\u3001\u5206\u5e03\u5f0f\u6392\u5e8f\u548c\u534f\u8c03\u5206\u5757\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u7a00\u758f\u52a0\u901f\u5668\u5728\u957f\u5e8f\u5217\u5e76\u884c\u5904\u7406\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u91c7\u7528\u9636\u6bb5\u9694\u79bb\u7684\u4f18\u5316\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\u8de8\u9636\u6bb5\u534f\u540c\u53ef\u4ee5\u5927\u5e45\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u548c\u5185\u5b58\u8bbf\u95ee\uff0c\u8fd9\u662f\u88ab\u5ffd\u89c6\u7684\u673a\u4f1a\u3002", "method": "1) \u57fa\u4e8e\u524d\u5bfc\u96f6\u7684\u7a00\u758f\u9884\u6d4b\uff0c\u4f7f\u7528\u5bf9\u6570\u57df\u4ec5\u52a0\u6cd5\u64cd\u4f5c\u6700\u5c0f\u5316\u9884\u6d4b\u5f00\u9500\uff1b2) \u5206\u5e03\u5f0f\u6392\u5e8f\u548c\u6392\u5e8f\u66f4\u65b0FlashAttention\u673a\u5236\uff1b3) \u534f\u8c03\u5206\u5757\u7b56\u7565\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u9636\u6bb5\u4ea4\u4e92\uff1b4) \u4e13\u7528STAR\u52a0\u901f\u5668\u67b6\u6784\uff1b5) \u591a\u6838\u7a7a\u95f4\u67b6\u6784\u90e8\u7f72\u4f18\u5316\u6570\u636e\u6d41\u548c\u6267\u884c\u7f16\u6392\u3002", "result": "STAR\u76f8\u6bd4A100\u5b9e\u73b09.2\u500d\u52a0\u901f\u548c71.2\u500d\u80fd\u6548\u63d0\u5347\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u52a0\u901f\u5668\u5b9e\u73b016.1\u500d\u80fd\u6548\u548c27.1\u500d\u9762\u79ef\u6548\u7387\u63d0\u5347\u3002\u7a7a\u95f4\u67b6\u6784\u7248\u672c\u76f8\u6bd4\u57fa\u7ebf\u8bbe\u8ba1\u5b9e\u73b020.1\u500d\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "STAR\u901a\u8fc7\u8de8\u9636\u6bb5\u534f\u540c\u7684\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u5e76\u884c\u5904\u7406\u573a\u666f\u4e0b\u7684Transformer\u63a8\u7406\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u8d85\u957f\u5e8f\u5217\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20495", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.20495", "abs": "https://arxiv.org/abs/2512.20495", "authors": ["He Zhu", "Zheng Liu", "Xingyang Li", "Anbang Wu", "Jieru Zhao", "Fangxin Liu", "Yiming Gan", "Jingwen Leng", "Yu Feng"], "title": "Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization", "comment": null, "summary": "3D Gaussian splatting (3DGS) has drawn significant attention in the architectural community recently. However, current architectural designs often overlook the 3DGS scalability, making them fragile for extremely large-scale 3DGS. Meanwhile, the VR bandwidth requirement makes it impossible to deliver high-fidelity and smooth VR content from the cloud.\n  We present Nebula, a coherent acceleration framework for large-scale 3DGS collaborative rendering. Instead of streaming videos, Nebula streams intermediate results after the LoD search, reducing 1925% data communication between the cloud and the client. To further enhance the motion-to-photon experience, we introduce a temporal-aware LoD search in the cloud that tames the irregular memory access and reduces redundant data access by exploiting temporal coherence across frames. On the client side, we propose a novel stereo rasterization that enables two eyes to share most computations during the stereo rendering with bit-accurate quality. With minimal hardware augmentations, Nebula achieves 2.7$\\times$ motion-to-photon speedup and reduces 1925% bandwidth over lossy video streaming.", "AI": {"tldr": "Nebula\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a213D\u9ad8\u65af\u6cfc\u6e85\u534f\u540c\u6e32\u67d3\u7684\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5f0f\u4f20\u8f93\u4e2d\u95f4\u7ed3\u679c\u800c\u975e\u89c6\u9891\uff0c\u663e\u8457\u964d\u4f4e\u4e91\u5ba2\u6237\u7aef\u5e26\u5bbd\u9700\u6c42\uff0c\u5e76\u63d0\u5347VR\u4f53\u9a8c\u3002", "motivation": "\u5f53\u524d3D\u9ad8\u65af\u6cfc\u6e85\u67b6\u6784\u8bbe\u8ba1\u5ffd\u89c6\u53ef\u6269\u5c55\u6027\uff0c\u96be\u4ee5\u5904\u7406\u8d85\u5927\u89c4\u6a21\u573a\u666f\uff1b\u540c\u65f6VR\u5e26\u5bbd\u9700\u6c42\u4f7f\u5f97\u4e91\u7aef\u65e0\u6cd5\u4f20\u8f93\u9ad8\u4fdd\u771f\u3001\u6d41\u7545\u7684VR\u5185\u5bb9\u3002", "method": "1) \u6d41\u5f0f\u4f20\u8f93LoD\u641c\u7d22\u540e\u7684\u4e2d\u95f4\u7ed3\u679c\u800c\u975e\u89c6\u9891\uff1b2) \u4e91\u7aef\u5f15\u5165\u65f6\u95f4\u611f\u77e5LoD\u641c\u7d22\uff0c\u5229\u7528\u5e27\u95f4\u65f6\u95f4\u4e00\u81f4\u6027\u51cf\u5c11\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\uff1b3) \u5ba2\u6237\u7aef\u63d0\u51fa\u65b0\u9896\u7684\u7acb\u4f53\u5149\u6805\u5316\uff0c\u8ba9\u53cc\u773c\u5171\u4eab\u5927\u90e8\u5206\u8ba1\u7b97\uff1b4) \u6700\u5c0f\u786c\u4ef6\u589e\u5f3a\u3002", "result": "\u76f8\u6bd4\u6709\u635f\u89c6\u9891\u6d41\uff0cNebula\u5b9e\u73b02.7\u500d\u8fd0\u52a8\u5230\u5149\u5b50\u5ef6\u8fdf\u52a0\u901f\uff0c\u51cf\u5c111925%\u5e26\u5bbd\u9700\u6c42\u3002", "conclusion": "Nebula\u901a\u8fc7\u521b\u65b0\u7684\u534f\u540c\u6e32\u67d3\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a213D\u9ad8\u65af\u6cfc\u6e85\u6e32\u67d3\u7684\u6269\u5c55\u6027\u548cVR\u5e26\u5bbd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2512.20571", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.20571", "abs": "https://arxiv.org/abs/2512.20571", "authors": ["Brennan Romero", "D. G. Perera"], "title": "Composing Mini Oscilloscope on Embedded Systems", "comment": "22 pages, 11 figures", "summary": "In this paper, our goal is to reproduce the basic functionalities of a regular oscilloscope, using the Nuvoton NUC-140 embedded systems development platform as the front-end and display method. A custom-built daughter board connects the NUC-140 to a variety of peripherals, including two BNC scope-probe connections, an external nine-button keypad, and a calibration signal. The LCD of the NUC-140 development board serves as the waveform display. From the experimental results, it is demonstrated that our proposed system became a very competent debugging tool. It implements 90% of the features we typically use on original oscilloscopes, including: automatic, edge-triggered, and single modes; waveform visualization using vertical and horizontal scaling; probe calibration.", "AI": {"tldr": "\u4f7f\u7528Nuvoton NUC-140\u5d4c\u5165\u5f0f\u5e73\u53f0\u5b9e\u73b0\u57fa\u672c\u793a\u6ce2\u5668\u529f\u80fd\uff0c\u5305\u62ec\u81ea\u52a8\u3001\u8fb9\u6cbf\u89e6\u53d1\u3001\u5355\u6b21\u6a21\u5f0f\uff0c\u6ce2\u5f62\u663e\u793a\u548c\u63a2\u5934\u6821\u51c6", "motivation": "\u5229\u7528\u5d4c\u5165\u5f0f\u7cfb\u7edf\u5e73\u53f0\u5b9e\u73b0\u5e38\u89c4\u793a\u6ce2\u5668\u7684\u57fa\u672c\u529f\u80fd\uff0c\u521b\u5efa\u4e00\u4e2a\u7ecf\u6d4e\u5b9e\u7528\u7684\u8c03\u8bd5\u5de5\u5177", "method": "\u4f7f\u7528Nuvoton NUC-140\u4f5c\u4e3a\u524d\u7aef\u548c\u663e\u793a\u5e73\u53f0\uff0c\u901a\u8fc7\u5b9a\u5236\u5b50\u677f\u8fde\u63a5BNC\u63a2\u5934\u63a5\u53e3\u3001\u5916\u90e8\u4e5d\u952e\u952e\u76d8\u548c\u6821\u51c6\u4fe1\u53f7\uff0c\u5229\u7528\u5f00\u53d1\u677fLCD\u663e\u793a\u6ce2\u5f62", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e8690%\u7684\u5e38\u7528\u793a\u6ce2\u5668\u529f\u80fd\uff0c\u5305\u62ec\u81ea\u52a8\u3001\u8fb9\u6cbf\u89e6\u53d1\u3001\u5355\u6b21\u6a21\u5f0f\uff0c\u5782\u76f4\u548c\u6c34\u5e73\u7f29\u653e\u6ce2\u5f62\u53ef\u89c6\u5316\uff0c\u4ee5\u53ca\u63a2\u5934\u6821\u51c6\uff0c\u6210\u4e3a\u4e00\u4e2a\u975e\u5e38\u79f0\u804c\u7684\u8c03\u8bd5\u5de5\u5177", "conclusion": "\u57fa\u4e8eNUC-140\u7684\u5d4c\u5165\u5f0f\u793a\u6ce2\u5668\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u5e38\u89c4\u793a\u6ce2\u5668\u7684\u6838\u5fc3\u529f\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5d4c\u5165\u5f0f\u5e73\u53f0\u5728\u6d4b\u8bd5\u6d4b\u91cf\u4eea\u5668\u5f00\u53d1\u4e2d\u7684\u53ef\u884c\u6027"}}
{"id": "2512.20214", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.20214", "abs": "https://arxiv.org/abs/2512.20214", "authors": ["Philipp Schr\u00f6er", "Darion Haase", "Joost-Pieter Katoen"], "title": "Error Localization, Certificates, and Hints for Probabilistic Program Verification via Slicing (Extended Version)", "comment": null, "summary": "This paper focuses on effective user diagnostics generated during the deductive verification of probabilistic programs. Our key principle is based on providing slices for (1) error reporting, (2) proof simplification, and (3) preserving successful verification results. By formally defining these different notions on HeyVL, an existing quantitative intermediate verification language (IVL), our concepts (and implementation) can be used to obtain diagnostics for a range of probabilistic programming languages. Slicing for error reporting is a novel notion of error localization for quantitative assertions. We demonstrate slicing-based diagnostics on a variety of proof rules such as quantitative versions of the specification statement and invariant-based loop rules, and formally prove the correctness of specialized error messages and verification hints.\n  We implemented our user diagnostics into the deductive verifier Caesar. Our novel implementation -- called \\emph{Brutus} -- can search for slices which do or do not verify, corresponding to each of the three diagnostic notions. For error reporting (1), it exploits a binary search-based algorithm that minimizes error-witnessing slices. To solve for slices that verify (2 and 3), we empirically compare different algorithms based on unsatisfiable cores, minimal unsatisfiable subset enumeration, and a direct SMT encoding of the slicing problem. Our empirical evaluation of Brutus on existing and new benchmarks shows that we can find slices that are both small and informative.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u5207\u7247\u6280\u672f\u7684\u6982\u7387\u7a0b\u5e8f\u6f14\u7ece\u9a8c\u8bc1\u7528\u6237\u8bca\u65ad\u65b9\u6cd5\uff0c\u5305\u62ec\u9519\u8bef\u62a5\u544a\u3001\u8bc1\u660e\u7b80\u5316\u548c\u9a8c\u8bc1\u7ed3\u679c\u4fdd\u6301\u4e09\u79cd\u8bca\u65ad\u6982\u5ff5\uff0c\u5e76\u5728Caesar\u9a8c\u8bc1\u5668\u4e2d\u5b9e\u73b0\u4e3aBrutus\u5de5\u5177\u3002", "motivation": "\u6982\u7387\u7a0b\u5e8f\u9a8c\u8bc1\u4e2d\u9700\u8981\u6709\u6548\u7684\u7528\u6237\u8bca\u65ad\u6765\u5e2e\u52a9\u7406\u89e3\u9a8c\u8bc1\u5931\u8d25\u539f\u56e0\u3001\u7b80\u5316\u8bc1\u660e\u8fc7\u7a0b\uff0c\u5e76\u4fdd\u6301\u5df2\u9a8c\u8bc1\u7684\u90e8\u5206\u7ed3\u679c\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9488\u5bf9\u5b9a\u91cf\u65ad\u8a00\u7684\u9519\u8bef\u5b9a\u4f4d\u673a\u5236\u3002", "method": "\u57fa\u4e8eHeyVL\u5b9a\u91cf\u4e2d\u95f4\u9a8c\u8bc1\u8bed\u8a00\uff0c\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e09\u79cd\u8bca\u65ad\u5207\u7247\u6982\u5ff5\uff1a\u9519\u8bef\u62a5\u544a\u5207\u7247\u3001\u8bc1\u660e\u7b80\u5316\u5207\u7247\u548c\u9a8c\u8bc1\u7ed3\u679c\u4fdd\u6301\u5207\u7247\u3002\u5b9e\u73b0Brutus\u5de5\u5177\uff0c\u91c7\u7528\u4e8c\u5206\u641c\u7d22\u7b97\u6cd5\u6700\u5c0f\u5316\u9519\u8bef\u89c1\u8bc1\u5207\u7247\uff0c\u5e76\u6bd4\u8f83\u57fa\u4e8e\u4e0d\u53ef\u6ee1\u8db3\u6838\u3001\u6700\u5c0f\u4e0d\u53ef\u6ee1\u8db3\u5b50\u96c6\u679a\u4e3e\u548c\u76f4\u63a5SMT\u7f16\u7801\u7684\u591a\u79cd\u7b97\u6cd5\u3002", "result": "\u5728\u73b0\u6709\u548c\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cBrutus\u80fd\u591f\u627e\u5230\u65e2\u5c0f\u53c8\u4fe1\u606f\u4e30\u5bcc\u7684\u5207\u7247\u3002\u9519\u8bef\u62a5\u544a\u5207\u7247\u4e3a\u5b9a\u91cf\u65ad\u8a00\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u9519\u8bef\u5b9a\u4f4d\u673a\u5236\uff0c\u652f\u6301\u591a\u79cd\u8bc1\u660e\u89c4\u5219\u5982\u5b9a\u91cf\u89c4\u8303\u8bed\u53e5\u548c\u57fa\u4e8e\u4e0d\u53d8\u91cf\u7684\u5faa\u73af\u89c4\u5219\u3002", "conclusion": "\u63d0\u51fa\u7684\u5207\u7247\u8bca\u65ad\u65b9\u6cd5\u4e3a\u6982\u7387\u7a0b\u5e8f\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7528\u6237\u8bca\u65ad\u5de5\u5177\uff0c\u80fd\u591f\u751f\u6210\u4e13\u95e8\u7684\u9519\u8bef\u6d88\u606f\u548c\u9a8c\u8bc1\u63d0\u793a\uff0c\u5e76\u5f62\u5f0f\u5316\u8bc1\u660e\u4e86\u5176\u6b63\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u6982\u7387\u7f16\u7a0b\u8bed\u8a00\u7684\u8bca\u65ad\u3002"}}
{"id": "2512.19842", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.19842", "abs": "https://arxiv.org/abs/2512.19842", "authors": ["Andrea Sordello", "Marco Mellia", "Idilio Drago", "Rodolfo Valentim", "Francesco Musumeci", "Massimo Tornatore", "Federico Cerutti", "Martino Trevisan", "Alessio Botta", "Willen Borges Coelho"], "title": "Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform", "comment": null, "summary": "The complexity and scale of Internet attacks call for distributed, cooperative observatories capable of monitoring malicious traffic across diverse networks. Holoscope is a lightweight, cloud-native platform designed to simplify the deployment and management of distributed telescope (passive) and honeypot (active) sensors, used to collect and analyse attack traffic by exposing or simulating vulnerable systems. Built upon K3s and WireGuard, Holoscope offers secure connectivity, automated node onboarding, and resilient operation even in resource-constrained environments. Through modular design and Infrastructure-as-Code principles, it supports dynamic sensor orchestration, automated recovery and processing. We build, deploy and operate Holoscope across multiple institutions and cloud networks in Europe and Brazil, enabling unified visibility into large-scale attack phenomena while maintaining ease of integration and security compliance.", "AI": {"tldr": "Holoscope\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e91\u539f\u751f\u5e73\u53f0\uff0c\u7528\u4e8e\u7b80\u5316\u5206\u5e03\u5f0f\u671b\u8fdc\u955c\u548c\u871c\u7f50\u4f20\u611f\u5668\u7684\u90e8\u7f72\u4e0e\u7ba1\u7406\uff0c\u652f\u6301\u8de8\u591a\u673a\u6784\u7f51\u7edc\u7684\u5b89\u5168\u653b\u51fb\u76d1\u63a7\u3002", "motivation": "\u4e92\u8054\u7f51\u653b\u51fb\u7684\u590d\u6742\u6027\u548c\u89c4\u6a21\u9700\u8981\u80fd\u591f\u8de8\u591a\u6837\u5316\u7f51\u7edc\u76d1\u63a7\u6076\u610f\u6d41\u91cf\u7684\u5206\u5e03\u5f0f\u3001\u534f\u4f5c\u5f0f\u89c2\u6d4b\u5e73\u53f0\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u90e8\u7f72\u3001\u7ba1\u7406\u548c\u5b89\u5168\u8fde\u63a5\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u57fa\u4e8eK3s\u548cWireGuard\u6784\u5efa\u7684\u4e91\u539f\u751f\u5e73\u53f0\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\u539f\u5219\uff0c\u63d0\u4f9b\u5b89\u5168\u8fde\u63a5\u3001\u81ea\u52a8\u5316\u8282\u70b9\u52a0\u5165\u3001\u5f39\u6027\u64cd\u4f5c\uff0c\u652f\u6301\u52a8\u6001\u4f20\u611f\u5668\u7f16\u6392\u3001\u81ea\u52a8\u5316\u6062\u590d\u548c\u5904\u7406\u3002", "result": "\u5728\u6b27\u6d32\u548c\u5df4\u897f\u7684\u591a\u4e2a\u673a\u6784\u548c\u4e91\u7f51\u7edc\u4e2d\u6210\u529f\u90e8\u7f72\u548c\u8fd0\u884cHoloscope\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5927\u89c4\u6a21\u653b\u51fb\u73b0\u8c61\u7684\u7edf\u4e00\u53ef\u89c1\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6613\u96c6\u6210\u6027\u548c\u5b89\u5168\u5408\u89c4\u6027\u3002", "conclusion": "Holoscope\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5206\u5e03\u5f0f\u653b\u51fb\u76d1\u63a7\u5e73\u53f0\uff0c\u7b80\u5316\u4e86\u4f20\u611f\u5668\u90e8\u7f72\u7ba1\u7406\uff0c\u652f\u6301\u8de8\u673a\u6784\u534f\u4f5c\uff0c\u4e3a\u5927\u89c4\u6a21\u7f51\u7edc\u653b\u51fb\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.19849", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.19849", "abs": "https://arxiv.org/abs/2512.19849", "authors": ["Ziming Mao", "Yihan Zhang", "Chihan Cui", "Kaichao You", "Zhongjie Chen", "Zhiying Xu", "Scott Shenker", "Costin Raiciu", "Yang Zhou", "Ion Stoica"], "title": "UCCL-EP: Portable Expert-Parallel Communication", "comment": null, "summary": "Mixture-of-Experts (MoE) workloads rely on expert parallelism (EP) to achieve high GPU efficiency. State-of-the-art EP communication systems such as DeepEP demonstrate strong performance but exhibit poor portability across heterogeneous GPU and NIC platforms. The poor portability is rooted in architecture: GPU-initiated token-level RDMA communication requires tight vertical integration between GPUs and NICs, e.g., GPU writes to NIC driver/MMIO interfaces.\n  We present UCCL-EP, a portable EP communication system that delivers DeepEP-level performance across heterogeneous GPU and NIC hardware. UCCL-EP replaces GPU-initiated RDMA with a high-throughput GPU-CPU control channel: compact token-routing commands are transferred to multithreaded CPU proxies, which then issue GPUDirect RDMA operations on behalf of GPUs. UCCL-EP further emulates various ordering semantics required by specialized EP communication modes using RDMA immediate data, enabling correctness on NICs that lack such ordering, e.g., AWS EFA. We implement UCCL-EP on NVIDIA and AMD GPUs with EFA and Broadcom NICs. On EFA, it outperforms the best existing EP solution by up to $2.1\\times$ for dispatch and combine throughput. On NVIDIA-only platform, UCCL-EP achieves comparable performance to the original DeepEP. UCCL-EP also improves token throughput on SGLang by up to 40% on the NVIDIA+EFA platform, and improves DeepSeek-V3 training throughput over the AMD Primus/Megatron-LM framework by up to 45% on a 16-node AMD+Broadcom platform.", "AI": {"tldr": "UCCL-EP\uff1a\u4e00\u79cd\u53ef\u79fb\u690d\u7684\u4e13\u5bb6\u5e76\u884c\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7GPU-CPU\u63a7\u5236\u901a\u9053\u66ff\u4ee3GPU\u53d1\u8d77\u7684RDMA\uff0c\u5728\u5f02\u6784GPU\u548cNIC\u5e73\u53f0\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e13\u5bb6\u5e76\u884c\u901a\u4fe1\u7cfb\u7edf\uff08\u5982DeepEP\uff09\u867d\u7136\u6027\u80fd\u4f18\u79c0\uff0c\u4f46\u8de8\u5f02\u6784GPU\u548cNIC\u5e73\u53f0\u7684\u79fb\u690d\u6027\u5dee\uff0c\u56e0\u4e3a\u5176GPU\u53d1\u8d77\u7684\u4ee4\u724c\u7ea7RDMA\u901a\u4fe1\u9700\u8981GPU\u4e0eNIC\u4e4b\u95f4\u7684\u7d27\u5bc6\u5782\u76f4\u96c6\u6210\u3002", "method": "1) \u7528\u9ad8\u541e\u5410\u91cf\u7684GPU-CPU\u63a7\u5236\u901a\u9053\u66ff\u4ee3GPU\u53d1\u8d77\u7684RDMA\uff0c\u5c06\u7d27\u51d1\u7684\u4ee4\u724c\u8def\u7531\u547d\u4ee4\u4f20\u8f93\u7ed9\u591a\u7ebf\u7a0bCPU\u4ee3\u7406\uff0c\u7531\u4ee3\u7406\u4ee3\u8868GPU\u6267\u884cGPUDirect RDMA\u64cd\u4f5c\uff1b2) \u4f7f\u7528RDMA\u5373\u65f6\u6570\u636e\u6a21\u62df\u5404\u79cd\u6392\u5e8f\u8bed\u4e49\uff0c\u786e\u4fdd\u5728\u7f3a\u4e4f\u6b64\u7c7b\u6392\u5e8f\u652f\u6301\u7684NIC\uff08\u5982AWS EFA\uff09\u4e0a\u7684\u6b63\u786e\u6027\u3002", "result": "\u5728EFA\u4e0a\uff0cUCCL-EP\u7684\u8c03\u5ea6\u548c\u7ec4\u5408\u541e\u5410\u91cf\u6bd4\u73b0\u6709\u6700\u4f73EP\u89e3\u51b3\u65b9\u6848\u63d0\u5347\u9ad8\u8fbe2.1\u500d\uff1b\u5728NVIDIA\u5e73\u53f0\u4e0a\uff0c\u6027\u80fd\u4e0e\u539f\u59cbDeepEP\u76f8\u5f53\uff1b\u5728NVIDIA+EFA\u5e73\u53f0\u4e0a\uff0cSGLang\u4ee4\u724c\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe40%\uff1b\u572816\u8282\u70b9AMD+Broadcom\u5e73\u53f0\u4e0a\uff0cDeepSeek-V3\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe45%\u3002", "conclusion": "UCCL-EP\u901a\u8fc7\u521b\u65b0\u7684GPU-CPU\u63a7\u5236\u901a\u9053\u8bbe\u8ba1\u548c\u6392\u5e8f\u8bed\u4e49\u6a21\u62df\uff0c\u5b9e\u73b0\u4e86\u8de8\u5f02\u6784\u786c\u4ef6\u5e73\u53f0\u7684\u9ad8\u6027\u80fd\u4e13\u5bb6\u5e76\u884c\u901a\u4fe1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u79fb\u690d\u6027\u5dee\u7684\u95ee\u9898\u3002"}}
{"id": "2512.19851", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.19851", "abs": "https://arxiv.org/abs/2512.19851", "authors": ["Aditya Bhosale", "Laxmikant Kale"], "title": "An Adaptive Distributed Stencil Abstraction for GPUs", "comment": null, "summary": "The scientific computing ecosystem in Python is largely confined to single-node parallelism, creating a gap between high-level prototyping in NumPy and high-performance execution on modern supercomputers. The increasing prevalence of hardware accelerators and the need for energy efficiency have made resource adaptivity a critical requirement, yet traditional HPC abstractions remain rigid. To address these challenges, we present an adaptive, distributed abstraction for stencil computations on multi-node GPUs. This abstraction is built using CharmTyles, a framework based on the adaptive Charm++ runtime, and features a familiar NumPy-like syntax to minimize the porting effort from prototype to production code. We showcase the resource elasticity of our abstraction by dynamically rescaling a running application across a different number of nodes and present a performance analysis of the associated overheads. Furthermore, we demonstrate that our abstraction achieves significant performance improvements over both a specialized, high-performance stencil DSL and a generalized NumPy replacement.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCharm++\u8fd0\u884c\u65f6\u7684\u81ea\u9002\u5e94\u5206\u5e03\u5f0f\u62bd\u8c61CharmTyles\uff0c\u7528\u4e8e\u591a\u8282\u70b9GPU\u4e0a\u7684\u6a21\u677f\u8ba1\u7b97\uff0c\u63d0\u4f9bNumPy-like\u8bed\u6cd5\uff0c\u652f\u6301\u52a8\u6001\u8d44\u6e90\u5f39\u6027\u4f38\u7f29\uff0c\u6027\u80fd\u4f18\u4e8e\u4e13\u7528DSL\u548c\u901a\u7528NumPy\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "Python\u79d1\u5b66\u8ba1\u7b97\u751f\u6001\u7cfb\u7edf\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u8282\u70b9\u5e76\u884c\uff0cNumPy\u539f\u578b\u4e0e\u8d85\u7ea7\u8ba1\u7b97\u673a\u9ad8\u6027\u80fd\u6267\u884c\u4e4b\u95f4\u5b58\u5728\u9e3f\u6c9f\u3002\u786c\u4ef6\u52a0\u901f\u5668\u666e\u53ca\u548c\u80fd\u6548\u9700\u6c42\u4f7f\u5f97\u8d44\u6e90\u81ea\u9002\u5e94\u6027\u6210\u4e3a\u5173\u952e\u9700\u6c42\uff0c\u4f46\u4f20\u7edfHPC\u62bd\u8c61\u4ecd\u7136\u50f5\u5316\u3002", "method": "\u57fa\u4e8e\u81ea\u9002\u5e94Charm++\u8fd0\u884c\u65f6\u6784\u5efaCharmTyles\u6846\u67b6\uff0c\u521b\u5efa\u652f\u6301\u591a\u8282\u70b9GPU\u7684\u5206\u5e03\u5f0f\u6a21\u677f\u8ba1\u7b97\u62bd\u8c61\uff0c\u91c7\u7528\u719f\u6089\u7684NumPy-like\u8bed\u6cd5\u4ee5\u51cf\u5c11\u4ece\u539f\u578b\u5230\u751f\u4ea7\u4ee3\u7801\u7684\u79fb\u690d\u5de5\u4f5c\u3002", "result": "\u5c55\u793a\u4e86\u8d44\u6e90\u5f39\u6027\u80fd\u529b\uff1a\u52a8\u6001\u8c03\u6574\u8fd0\u884c\u5e94\u7528\u7a0b\u5e8f\u5728\u4e0d\u540c\u8282\u70b9\u6570\u95f4\u7684\u4f38\u7f29\uff1b\u6027\u80fd\u5206\u6790\u663e\u793a\u76f8\u5173\u5f00\u9500\u53ef\u63a7\uff1b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4e13\u7528\u9ad8\u6027\u80fd\u6a21\u677fDSL\u548c\u901a\u7528NumPy\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "CharmTyles\u586b\u8865\u4e86Python\u79d1\u5b66\u8ba1\u7b97\u4e2d\u5355\u8282\u70b9\u539f\u578b\u4e0e\u591a\u8282\u70b9\u9ad8\u6027\u80fd\u6267\u884c\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u5e03\u5f0f\u62bd\u8c61\u548cNumPy-like\u8bed\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8d44\u6e90\u5f39\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2512.19972", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.19972", "abs": "https://arxiv.org/abs/2512.19972", "authors": ["Pengchao Han", "Xi Huang", "Yi Fang", "Guojun Han"], "title": "Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions", "comment": "Published in IEEE TNSE", "summary": "Collaborative learning has emerged as a key paradigm in large-scale intelligent systems, enabling distributed agents to cooperatively train their models while addressing their privacy concerns. Central to this paradigm is knowledge distillation (KD), a technique that facilitates efficient knowledge transfer among agents. However, the underlying mechanisms by which KD leverages memory and knowledge across agents remain underexplored. This paper aims to bridge this gap by offering a comprehensive review of KD in collaborative learning, with a focus on the roles of memory and knowledge. We define and categorize memory and knowledge within the KD process and explore their interrelationships, providing a clear understanding of how knowledge is extracted, stored, and shared in collaborative settings. We examine various collaborative learning patterns, including distributed, hierarchical, and decentralized structures, and provide insights into how memory and knowledge dynamics shape the effectiveness of KD in collaborative learning. Particularly, we emphasize task heterogeneity in distributed learning pattern covering federated learning (FL), multi-agent domain adaptation (MADA), federated multi-modal learning (FML), federated continual learning (FCL), federated multi-task learning (FMTL), and federated graph knowledge embedding (FKGE). Additionally, we highlight model heterogeneity, data heterogeneity, resource heterogeneity, and privacy concerns of these tasks. Our analysis categorizes existing work based on how they handle memory and knowledge. Finally, we discuss existing challenges and propose future directions for advancing KD techniques in the context of collaborative learning.", "AI": {"tldr": "\u672c\u6587\u5bf9\u534f\u4f5c\u5b66\u4e60\u4e2d\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u91cd\u70b9\u5173\u6ce8\u8bb0\u5fc6\u4e0e\u77e5\u8bc6\u5728KD\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u673a\u5236\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u534f\u4f5c\u5b66\u4e60\u6a21\u5f0f\u4e0b\u7684\u77e5\u8bc6\u63d0\u53d6\u3001\u5b58\u50a8\u4e0e\u5171\u4eab\uff0c\u5e76\u63a2\u8ba8\u4e86\u4efb\u52a1\u5f02\u8d28\u6027\u7b49\u6311\u6218\u3002", "motivation": "\u534f\u4f5c\u5b66\u4e60\u5df2\u6210\u4e3a\u5927\u89c4\u6a21\u667a\u80fd\u7cfb\u7edf\u7684\u5173\u952e\u8303\u5f0f\uff0c\u4f46\u77e5\u8bc6\u84b8\u998f\u5728\u534f\u4f5c\u5b66\u4e60\u4e2d\u5982\u4f55\u5229\u7528\u8bb0\u5fc6\u548c\u77e5\u8bc6\u7684\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6df1\u5165\u7406\u89e3\u77e5\u8bc6\u5728\u534f\u4f5c\u73af\u5883\u4e2d\u7684\u63d0\u53d6\u3001\u5b58\u50a8\u548c\u5171\u4eab\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u548c\u5206\u7c7bKD\u8fc7\u7a0b\u4e2d\u7684\u8bb0\u5fc6\u4e0e\u77e5\u8bc6\uff0c\u63a2\u7d22\u5176\u76f8\u4e92\u5173\u7cfb\uff1b\u5206\u6790\u5206\u5e03\u5f0f\u3001\u5206\u5c42\u548c\u53bb\u4e2d\u5fc3\u5316\u7b49\u4e0d\u540c\u534f\u4f5c\u5b66\u4e60\u6a21\u5f0f\uff1b\u7279\u522b\u5173\u6ce8\u5206\u5e03\u5f0f\u5b66\u4e60\u6a21\u5f0f\u4e2d\u7684\u4efb\u52a1\u5f02\u8d28\u6027\uff0c\u6db5\u76d6\u8054\u90a6\u5b66\u4e60\u3001\u591a\u667a\u80fd\u4f53\u57df\u9002\u5e94\u3001\u8054\u90a6\u591a\u6a21\u6001\u5b66\u4e60\u7b49\u591a\u79cd\u573a\u666f\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u534f\u4f5c\u5b66\u4e60\u4e2dKD\u6280\u672f\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u6846\u67b6\uff0c\u660e\u786e\u4e86\u8bb0\u5fc6\u4e0e\u77e5\u8bc6\u5728\u77e5\u8bc6\u8f6c\u79fb\u4e2d\u7684\u4f5c\u7528\u673a\u5236\uff1b\u5206\u6790\u4e86\u6a21\u578b\u5f02\u8d28\u6027\u3001\u6570\u636e\u5f02\u8d28\u6027\u3001\u8d44\u6e90\u5f02\u8d28\u6027\u548c\u9690\u79c1\u95ee\u9898\u7b49\u5173\u952e\u6311\u6218\uff1b\u5bf9\u73b0\u6709\u5de5\u4f5c\u57fa\u4e8e\u8bb0\u5fc6\u4e0e\u77e5\u8bc6\u5904\u7406\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u3002", "conclusion": "\u672c\u6587\u4e3a\u534f\u4f5c\u5b66\u4e60\u4e2d\u77e5\u8bc6\u84b8\u998f\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7406\u8bba\u57fa\u7840\u548c\u5206\u6790\u6846\u67b6\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8KD\u6280\u672f\u5728\u534f\u4f5c\u5b66\u4e60\u4e2d\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2512.20017", "categories": ["cs.DC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.20017", "abs": "https://arxiv.org/abs/2512.20017", "authors": ["Hexu Zhao", "Xiaoteng Liu", "Xiwen Min", "Jianhao Huang", "Youming Deng", "Yanfei Li", "Ang Li", "Jinyang Li", "Aurojit Panda"], "title": "Scaling Point-based Differentiable Rendering for Large-scale Reconstruction", "comment": "13 pages main text, plus appendix", "summary": "Point-based Differentiable Rendering (PBDR) enables high-fidelity 3D scene reconstruction, but scaling PBDR to high-resolution and large scenes requires efficient distributed training systems. Existing systems are tightly coupled to a specific PBDR method. And they suffer from severe communication overhead due to poor data locality. In this paper, we present Gaian, a general distributed training system for PBDR. Gaian provides a unified API expressive enough to support existing PBDR methods, while exposing rich data-access information, which Gaian leverages to optimize locality and reduce communication. We evaluated Gaian by implementing 4 PBDR algorithms. Our implementations achieve high performance and resource efficiency: across six datasets and up to 128 GPUs, it reduces communication by up to 91% and improves training throughput by 1.50x-3.71x.", "AI": {"tldr": "Gaian\u662f\u4e00\u4e2a\u7528\u4e8e\u70b9\u57fa\u53ef\u5fae\u6e32\u67d3\u7684\u901a\u7528\u5206\u5e03\u5f0f\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u5c40\u90e8\u6027\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387", "motivation": "\u73b0\u6709\u7684\u70b9\u57fa\u53ef\u5fae\u6e32\u67d3\u5206\u5e03\u5f0f\u8bad\u7ec3\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u4e0e\u7279\u5b9a\u65b9\u6cd5\u7d27\u5bc6\u8026\u5408\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\uff1b2) \u6570\u636e\u5c40\u90e8\u6027\u5dee\u5bfc\u81f4\u4e25\u91cd\u7684\u901a\u4fe1\u5f00\u9500\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u65e2\u80fd\u652f\u6301\u591a\u79cdPBDR\u65b9\u6cd5\u53c8\u80fd\u4f18\u5316\u901a\u4fe1\u6548\u7387\u7684\u7cfb\u7edf\u3002", "method": "Gaian\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684API\uff0c\u8db3\u591f\u8868\u8fbe\u73b0\u6709PBDR\u65b9\u6cd5\uff0c\u540c\u65f6\u66b4\u9732\u4e30\u5bcc\u7684\u6570\u636e\u8bbf\u95ee\u4fe1\u606f\u3002\u7cfb\u7edf\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u6765\u4f18\u5316\u6570\u636e\u5c40\u90e8\u6027\uff0c\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002\u901a\u8fc7\u5b9e\u73b04\u79cdPBDR\u7b97\u6cd5\u6765\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u57286\u4e2a\u6570\u636e\u96c6\u548c\u6700\u591a128\u4e2aGPU\u4e0a\uff0cGaian\u51cf\u5c11\u4e86\u9ad8\u8fbe91%\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\u4e861.50\u500d\u52303.71\u500d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u3002", "conclusion": "Gaian\u662f\u4e00\u4e2a\u901a\u7528\u7684\u5206\u5e03\u5f0fPBDR\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u7edf\u4e00\u7684API\u548c\u4f18\u5316\u7684\u6570\u636e\u5c40\u90e8\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\u5e76\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u9ad8\u5206\u8fa8\u73873D\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20064", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20064", "abs": "https://arxiv.org/abs/2512.20064", "authors": ["Yaojian Chen", "Si-Qiu Gong", "Lin Gan", "Yanfei Liu", "An Yang", "Yinuo Wang", "Chao-yang Lu", "Guangwen Yang"], "title": "FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling", "comment": "12 pages, 13 figures", "summary": "Matrix Product State (MPS) is a versatile tensor network representation widely applied in quantum physics, quantum chemistry, and machine learning, etc. MPS sampling serves as a critical fundamental operation in these fields. As the problems become more complex, the scale of MPS is rapidly increasing. Traditional data parallelism is limited by memory and heavy I/O in large-scale MPS. Model parallelism that can handle large-scale MPS imposes rigid process bindings and lacks scalability. This work proposes Fast-MPS, a multi-level parallel framework for scalable MPS sampling. Our design combines data parallelism across samples with tensor parallelism along bond dimensions. We eliminate memory and I/O pressure through compression and overlapping, and revive data parallel in large-scale MPS sampling. We evaluate our approach on Gaussian Boson Sampling, a representative and demanding application. Fast-MPS achieves over 10x speedup compared to existing simulators, scales to thousands of processes, and enables simulations with 8,176 sites and bond dimension chi = 10^4, significantly outperforming the state of the art. Fast-MPS has demonstrated great potential in high-performance tensor network applications.", "AI": {"tldr": "Fast-MPS\u662f\u4e00\u4e2a\u7528\u4e8e\u53ef\u6269\u5c55MPS\u91c7\u6837\u7684\u591a\u7ea7\u5e76\u884c\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u8de8\u6837\u672c\u7684\u6570\u636e\u5e76\u884c\u548c\u6cbf\u952e\u7ef4\u5ea6\u7684\u5f20\u91cf\u5e76\u884c\uff0c\u5728Gaussian Boson Sampling\u4e2d\u5b9e\u73b0\u4e8610\u500d\u4ee5\u4e0a\u7684\u52a0\u901f\uff0c\u53ef\u6269\u5c55\u5230\u6570\u5343\u4e2a\u8fdb\u7a0b\u3002", "motivation": "\u968f\u7740\u95ee\u9898\u590d\u6742\u5ea6\u589e\u52a0\uff0cMPS\u89c4\u6a21\u8fc5\u901f\u6269\u5927\u3002\u4f20\u7edf\u6570\u636e\u5e76\u884c\u53d7\u9650\u4e8e\u5185\u5b58\u548cI/O\uff0c\u800c\u6a21\u578b\u5e76\u884c\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\uff0c\u9700\u8981\u65b0\u7684\u5e76\u884c\u6846\u67b6\u6765\u5904\u7406\u5927\u89c4\u6a21MPS\u91c7\u6837\u3002", "method": "\u63d0\u51faFast-MPS\u591a\u7ea7\u5e76\u884c\u6846\u67b6\uff0c\u7ed3\u5408\u8de8\u6837\u672c\u7684\u6570\u636e\u5e76\u884c\u548c\u6cbf\u952e\u7ef4\u5ea6\u7684\u5f20\u91cf\u5e76\u884c\uff0c\u901a\u8fc7\u538b\u7f29\u548c\u91cd\u53e0\u6280\u672f\u6d88\u9664\u5185\u5b58\u548cI/O\u538b\u529b\uff0c\u5728\u5927\u89c4\u6a21MPS\u91c7\u6837\u4e2d\u91cd\u65b0\u542f\u7528\u6570\u636e\u5e76\u884c\u3002", "result": "\u5728Gaussian Boson Sampling\u4e2d\u5b9e\u73b0\u8d85\u8fc710\u500d\u52a0\u901f\uff0c\u53ef\u6269\u5c55\u5230\u6570\u5343\u4e2a\u8fdb\u7a0b\uff0c\u652f\u63018,176\u4e2a\u4f4d\u70b9\u548c\u952e\u7ef4\u5ea6chi=10^4\u7684\u6a21\u62df\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "Fast-MPS\u5728\u9ad8\u6027\u80fd\u5f20\u91cf\u7f51\u7edc\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21MPS\u91c7\u6837\u7684\u5e76\u884c\u5316\u6311\u6218\u3002"}}
{"id": "2512.20163", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20163", "abs": "https://arxiv.org/abs/2512.20163", "authors": ["Leszek G\u0105sieniec", "Tytus Grodzicki", "Tomasz Jurdzi\u0144ski", "Jakub Kowalski", "Grzegorz Stachowiak"], "title": "Population Protocols Revisited: Parity and Beyond", "comment": null, "summary": "For nearly two decades, population protocols have been extensively studied, yielding efficient solutions for central problems in distributed computing, including leader election, and majority computation, a predicate type in Presburger Arithmetic closely tied to population protocols. Surprisingly, no protocols have achieved both time- and space-efficiency for congruency predicates, such as parity computation, which are complementary in this arithmetic framework. This gap highlights a significant challenge in the field. To address this gap, we explore the parity problem, where agents are tasked with computing the parity of the given sub-population size. Then we extend the solution for parity to compute congruences modulo an arbitrary $m$.\n  Previous research on efficient population protocols has focused on protocols that minimise both stabilisation time and state utilisation for specific problems. In contrast, this work slightly relaxes this expectation, permitting protocols to place less emphasis on full optimisation and more on universality, robustness, and probabilistic guarantees. This allows us to propose a novel computing paradigm that integrates population weights (or simply weights), a robust clocking mechanism, and efficient anomaly detection coupled with a switching mechanism (which ensures slow but always correct solutions). This paradigm facilitates universal design of efficient multistage stable population protocols. Specifically, the first efficient parity and congruence protocols introduced here use both $O(\\log^3 n)$ states and achieve silent stabilisation in $O(\\log^3 n)$ time. We conclude by discussing the impact of implicit conversion between unary and binary representations enabled by the weight system, with applications to other problems, including the computation and representation of (sub-)population sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u4e0a\u90fd\u9ad8\u6548\u7684\u5947\u5076\u6027\u548c\u540c\u4f59\u6027\u7fa4\u4f53\u534f\u8bae\uff0c\u4f7f\u7528O(log\u00b3 n)\u72b6\u6001\u5728O(log\u00b3 n)\u65f6\u95f4\u5185\u5b9e\u73b0\u7a33\u5b9a\u8ba1\u7b97\u3002", "motivation": "\u7fa4\u4f53\u534f\u8bae\u7814\u7a76\u8fd120\u5e74\u6765\uff0c\u867d\u7136\u5bf9\u9886\u5bfc\u8005\u9009\u4e3e\u548c\u591a\u6570\u8ba1\u7b97\u7b49\u95ee\u9898\u6709\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5bf9\u5947\u5076\u6027\u548c\u540c\u4f59\u6027\u7b49Presburger\u7b97\u672f\u4e2d\u7684\u4e92\u8865\u8c13\u8bcd\uff0c\u4e00\u76f4\u6ca1\u6709\u540c\u65f6\u5b9e\u73b0\u65f6\u95f4\u548c\u7a7a\u95f4\u9ad8\u6548\u7684\u534f\u8bae\u3002\u8fd9\u4e00\u7a7a\u767d\u6784\u6210\u4e86\u8be5\u9886\u57df\u7684\u91cd\u8981\u6311\u6218\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u8ba1\u7b97\u8303\u5f0f\uff0c\u6574\u5408\u7fa4\u4f53\u6743\u91cd\u7cfb\u7edf\u3001\u9c81\u68d2\u7684\u65f6\u949f\u673a\u5236\u3001\u9ad8\u6548\u5f02\u5e38\u68c0\u6d4b\u548c\u5207\u6362\u673a\u5236\u3002\u91c7\u7528\u591a\u9636\u6bb5\u7a33\u5b9a\u7fa4\u4f53\u534f\u8bae\u8bbe\u8ba1\uff0c\u5141\u8bb8\u534f\u8bae\u4e0d\u5b8c\u5168\u4f18\u5316\uff0c\u66f4\u6ce8\u91cd\u901a\u7528\u6027\u3001\u9c81\u68d2\u6027\u548c\u6982\u7387\u4fdd\u8bc1\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5947\u5076\u6027\u548c\u540c\u4f59\u6027\u534f\u8bae\uff0c\u4f7f\u7528O(log\u00b3 n)\u72b6\u6001\u5728O(log\u00b3 n)\u65f6\u95f4\u5185\u5b9e\u73b0\u9759\u9ed8\u7a33\u5b9a\u3002\u6743\u91cd\u7cfb\u7edf\u8fd8\u652f\u6301\u4e00\u5143\u548c\u4e8c\u8fdb\u5236\u8868\u793a\u4e4b\u95f4\u7684\u9690\u5f0f\u8f6c\u6362\uff0c\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u653e\u677e\u5b8c\u5168\u4f18\u5316\u7684\u8981\u6c42\uff0c\u4e13\u6ce8\u4e8e\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\uff0c\u6210\u529f\u586b\u8865\u4e86\u7fa4\u4f53\u534f\u8bae\u4e2d\u5947\u5076\u6027\u548c\u540c\u4f59\u6027\u8ba1\u7b97\u7684\u65f6\u95f4-\u7a7a\u95f4\u6548\u7387\u7a7a\u767d\uff0c\u4e3a\u5176\u4ed6\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u8ba1\u7b97\u8303\u5f0f\u3002"}}
{"id": "2512.20178", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.20178", "abs": "https://arxiv.org/abs/2512.20178", "authors": ["Chen Zhuang", "Lingqi Zhang", "Benjamin Brock", "Du Wu", "Peng Chen", "Toshio Endo", "Satoshi Matsuoka", "Mohamed Wahib"], "title": "SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication", "comment": "Under Review", "summary": "Distributed Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental operation in numerous high-performance computing and deep learning applications. The major performance bottleneck in distributed SpMM lies in the substantial communication overhead, which limits both performance and scalability. In this paper, we identify and analyze sources of inefficient communication in existing distributed SpMM implementations at two levels and address these inefficiencies by proposing: (1) a fine-grained, sparsity-aware communication strategy that reduces communication overhead by exploiting the sparsity pattern of the sparse matrix, and (2) a hierarchical communication strategy that integrates the sparsity-aware strategy with the common two-tier network architectures in GPU-accelerated systems, to reduce redundant communication across slow network links. We implement these optimizations in a comprehensive distributed SpMM framework, \\method{}. Extensive evaluations on real-world datasets show that our framework demonstrates strong scalability up to 128 GPUs, achieving geometric mean speedups of 221.5$\\times$, 56.0$\\times$, 23.4$\\times$, and 8.8$\\times$ over four state-of-the-art baselines (CAGNET, SPA, BCL, and CoLa, respectively) at this scale.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5e03\u5f0f\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7a00\u758f\u611f\u77e5\u901a\u4fe1\u7b56\u7565\u548c\u5206\u5c42\u901a\u4fe1\u7b56\u7565\uff0c\u5728128\u4e2aGPU\u4e0a\u5b9e\u73b0\u6700\u9ad8221.5\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5206\u5e03\u5f0f\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u662f\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u57fa\u7840\u64cd\u4f5c\uff0c\u73b0\u6709\u5b9e\u73b0\u5b58\u5728\u663e\u8457\u7684\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "1) \u7ec6\u7c92\u5ea6\u7a00\u758f\u611f\u77e5\u901a\u4fe1\u7b56\u7565\uff1a\u5229\u7528\u7a00\u758f\u77e9\u9635\u7684\u7a00\u758f\u6a21\u5f0f\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff1b2) \u5206\u5c42\u901a\u4fe1\u7b56\u7565\uff1a\u5c06\u7a00\u758f\u611f\u77e5\u7b56\u7565\u4e0eGPU\u52a0\u901f\u7cfb\u7edf\u4e2d\u5e38\u89c1\u7684\u4e24\u5c42\u7f51\u7edc\u67b6\u6784\u7ed3\u5408\uff0c\u51cf\u5c11\u8de8\u6162\u901f\u7f51\u7edc\u94fe\u8def\u7684\u5197\u4f59\u901a\u4fe1\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6846\u67b6\u5728128\u4e2aGPU\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\uff0c\u76f8\u5bf9\u4e8e\u56db\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff08CAGNET\u3001SPA\u3001BCL\u3001CoLa\uff09\u5206\u522b\u5b9e\u73b0\u4e86221.5\u500d\u300156.0\u500d\u300123.4\u500d\u548c8.8\u500d\u7684\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u5206\u5e03\u5f0fSpMM\u5b9e\u73b0\u4e2d\u7684\u901a\u4fe1\u4f4e\u6548\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u9488\u5bf9\u6027\u7684\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5e03\u5f0f\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2512.20184", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20184", "abs": "https://arxiv.org/abs/2512.20184", "authors": ["Chaoyi Ruan", "Yiliang Wang", "Ziji Shi", "Jialin Li"], "title": "Reaching Agreement Among Reasoning LLM Agents", "comment": null, "summary": "Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.\n  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faAegean\uff0c\u4e00\u4e2a\u4e3a\u968f\u673a\u63a8\u7406\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u5171\u8bc6\u534f\u8bae\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cbe\u70bc\u95ee\u9898\uff0c\u901a\u8fc7\u589e\u91cf\u4ef2\u88c1\u68c0\u6d4b\u5b9e\u73b0\u65e9\u671f\u7ec8\u6b62\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u540c\u65f6\u4fdd\u6301\u7b54\u6848\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7f16\u6392\u4f9d\u8d56\u9759\u6001\u542f\u53d1\u5f0f\u5de5\u4f5c\u6d41\uff08\u5982\u56fa\u5b9a\u5faa\u73af\u9650\u5236\u548c\u5c4f\u969c\u540c\u6b65\uff09\uff0c\u8fd9\u4e9b\u4e34\u65f6\u65b9\u6cd5\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\u3001\u56e0\u843d\u540e\u8282\u70b9\u4ea7\u751f\u9ad8\u5ef6\u8fdf\uff0c\u5e76\u53ef\u80fd\u6700\u7ec8\u5316\u77ac\u65f6\u534f\u8bae\u3002\u53ef\u9760\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u9700\u8981\u7c7b\u4f3c\u7ecf\u5178\u5206\u5e03\u5f0f\u5171\u8bc6\u95ee\u9898\u7684\u5f62\u5f0f\u5316\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u7cbe\u70bc\u95ee\u9898\u7684\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u5305\u62ec\u6b63\u786e\u6027\u4fdd\u8bc1\u7684\u5b9a\u4e49\u548c\u667a\u80fd\u4f53\u63a8\u7406\u7684\u5f62\u5f0f\u8bed\u4e49\u3002\u7136\u540e\u5f15\u5165Aegean\u2014\u2014\u4e3a\u968f\u673a\u63a8\u7406\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u5171\u8bc6\u534f\u8bae\uff0c\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cbe\u70bc\u95ee\u9898\u3002\u5728Aegean-Serve\u4e2d\u5b9e\u73b0\u8be5\u534f\u8bae\uff0c\u8fd9\u662f\u4e00\u4e2a\u5171\u8bc6\u611f\u77e5\u7684\u670d\u52a1\u5f15\u64ce\uff0c\u5728\u5e76\u53d1\u667a\u80fd\u4f53\u6267\u884c\u4e2d\u8fdb\u884c\u589e\u91cf\u4ef2\u88c1\u68c0\u6d4b\uff0c\u5b9e\u73b0\u5145\u5206\u667a\u80fd\u4f53\u6536\u655b\u65f6\u7684\u65e9\u671f\u7ec8\u6b62\u3002", "result": "\u5728\u56db\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAegean\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u6027\u548c\u6d3b\u6027\u4fdd\u8bc1\uff0c\u540c\u65f6\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u964d\u4f4e\u5ef6\u8fdf1.2-20\u500d\uff0c\u4fdd\u6301\u7b54\u6848\u8d28\u91cf\u57282.5%\u4ee5\u5185\u3002\u5728\u672c\u5730GPU\u90e8\u7f72\u548c\u5546\u4e1aAPI\u63d0\u4f9b\u5546\u4e0a\u7684\u4e00\u81f4\u589e\u76ca\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u5171\u8bc6\u7684\u7f16\u6392\u6d88\u9664\u4e86\u843d\u540e\u8282\u70b9\u5ef6\u8fdf\u800c\u4e0d\u727a\u7272\u6b63\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u667a\u80fd\u4f53\u63a8\u7406\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u57fa\u7840\uff0c\u901a\u8fc7\u5171\u8bc6\u534f\u8baeAegean\u89e3\u51b3\u4e86\u73b0\u6709\u9759\u6001\u5de5\u4f5c\u6d41\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5ef6\u8fdf\u663e\u8457\u964d\u4f4e\u548c\u8d44\u6e90\u6548\u7387\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u7b54\u6848\u8d28\u91cf\uff0c\u4e3a\u53ef\u9760\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.20210", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20210", "abs": "https://arxiv.org/abs/2512.20210", "authors": ["Yinan Ni", "Xiao Yang", "Yuqi Tang", "Zhimin Qiu", "Chen Wang", "Tingzhou Yuan"], "title": "Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs", "comment": null, "summary": "The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.", "AI": {"tldr": "P-LoRA\uff1a\u4e00\u79cd\u9762\u5411LoRA\u5fae\u8c03LLM\u7684\u670d\u52a1\u5668\u65e0\u611f\u77e5\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u6d4b\u6027\u9002\u914d\u5668\u9884\u53d6\u548c\u9875\u9762\u5f0f\u5185\u5b58\u7ba1\u7406\uff0c\u663e\u8457\u964d\u4f4e\u51b7\u542f\u52a8\u5ef6\u8fdf\u5e76\u63d0\u5347\u541e\u5410\u91cf\u3002", "motivation": "\u5728\u670d\u52a1\u5668\u65e0\u611f\u77e5\u73af\u5883\u4e2d\u90e8\u7f72\u57fa\u4e8eLoRA\u7684LLM\u63a8\u7406\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u53cd\u5e94\u5f0f\u9002\u914d\u5668\u52a0\u8f7d\u5bfc\u81f4\u663e\u8457\u7684\u51b7\u542f\u52a8\u5ef6\u8fdf\uff0c\u4ee5\u53ca\u9891\u7e41\u7684\u9002\u914d\u5668\u4ea4\u6362\u9020\u6210\u4e25\u91cd\u7684GPU\u5185\u5b58\u788e\u7247\u5316\u3002", "method": "\u63d0\u51faP-LoRA\u7cfb\u7edf\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6280\u672f\uff1a1\uff09\u8f7b\u91cf\u7ea7LSTM\u6d41\u91cf\u9884\u6d4b\u5668\uff0c\u9884\u6d4b\u9002\u914d\u5668\u9700\u6c42\u5e76\u4e3b\u52a8\u4ece\u4e3b\u673a\u5185\u5b58\u9884\u53d6\u5230GPU\uff1b2\uff09\u53d7\u64cd\u4f5c\u7cfb\u7edf\u865a\u62df\u5185\u5b58\u542f\u53d1\u7684\u9875\u9762\u5f0f\u9002\u914d\u5668\u5185\u5b58\u7ba1\u7406\u673a\u5236\uff0c\u4fdd\u6301\u9ad8GPU\u5185\u5b58\u5229\u7528\u7387\u3002", "result": "\u5b9e\u9a8c\u4f7f\u7528Azure Functions\u8ddf\u8e2a\u7684\u751f\u4ea7\u7ea7\u5de5\u4f5c\u8d1f\u8f7d\uff0cP-LoRA\u76f8\u6bd4S-LoRA\u5b9e\u73b01.52\u500d\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5728\u9ad8\u5e76\u53d1\u573a\u666f\u4e0b\u5e73\u5747TTFT\u964d\u4f4e35%\uff0c\u51b7\u542f\u52a8\u5ef6\u8fdf\u6700\u9ad8\u51cf\u5c1168%\uff0cGPU\u5185\u5b58\u5229\u7528\u7387\u4fdd\u6301\u572887%\u4ee5\u4e0a\u3002", "conclusion": "P-LoRA\u901a\u8fc7\u9884\u6d4b\u6027\u9002\u914d\u5668\u9884\u53d6\u548c\u667a\u80fd\u5185\u5b58\u7ba1\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u670d\u52a1\u5668\u65e0\u611f\u77e5\u73af\u5883\u4e2dLoRA-based LLM\u63a8\u7406\u7684\u51b7\u542f\u52a8\u548c\u5185\u5b58\u788e\u7247\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2512.20394", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20394", "abs": "https://arxiv.org/abs/2512.20394", "authors": ["Mohammad Walid Charrwi", "Zaid Hussain"], "title": "Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults", "comment": null, "summary": "As Network-on-Chip (NoC) and Wireless Sensor Network architectures continue to scale, the topology of the underlying network becomes a critical factor in performance. Gaussian Interconnected Networks based on the arithmetic of Gaussian integers, offer attractive properties regarding diameter and symmetry. Despite their attractive theoretical properties, adaptive routing techniques in these networks are vulnerable to node and link faults, leading to rapid degradation in communication reliability. Node failures (particularly those following Gaussian distributions, such as thermal hotspots or physical damage clusters) pose severe challenges to traditional deterministic routing. This paper proposes a fault-aware Reinforcement Learning (RL) routing scheme tailored for Gaussian Interconnected Networks. By utilizing a PPO (Proximal Policy Optimization) agent with a specific reward structure designed to penalize fault proximity, the system dynamically learns to bypass faulty regions. We compare our proposed RL-based routing protocol against a greedy adaptive shortest-path routing algorithm. Experimental results demonstrate that the RL agent significantly outperforms the adaptive routing sustaining a Packet Delivery Ratio (PDR) of 0.95 at 40% fault density compared to 0.66 for the greedy. Furthermore, the RL approach exhibits effective delivery rates compared to the greedy adaptive routing, particularly under low network load of 20% at 0.57 vs. 0.43, showing greater proficiency in managing congestion, validating its efficacy in stochastic, fault-prone topologies", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bb9\u9519\u8def\u7531\u65b9\u6848\uff0c\u7528\u4e8e\u9ad8\u65af\u4e92\u8fde\u7f51\u7edc\uff0c\u901a\u8fc7PPO\u4ee3\u7406\u5b66\u4e60\u7ed5\u8fc7\u6545\u969c\u533a\u57df\uff0c\u663e\u8457\u63d0\u5347\u5305\u6295\u9012\u7387", "motivation": "\u968f\u7740NoC\u548c\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u89c4\u6a21\u6269\u5927\uff0c\u7f51\u7edc\u62d3\u6251\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u9ad8\u65af\u4e92\u8fde\u7f51\u7edc\u5177\u6709\u76f4\u5f84\u548c\u5bf9\u79f0\u6027\u4f18\u52bf\uff0c\u4f46\u81ea\u9002\u5e94\u8def\u7531\u6280\u672f\u6613\u53d7\u8282\u70b9\u548c\u94fe\u8def\u6545\u969c\u5f71\u54cd\uff0c\u5bfc\u81f4\u901a\u4fe1\u53ef\u9760\u6027\u5feb\u901f\u4e0b\u964d\u3002\u7279\u522b\u662f\u9075\u5faa\u9ad8\u65af\u5206\u5e03\u7684\u8282\u70b9\u6545\u969c\uff08\u5982\u70ed\u70ed\u70b9\u6216\u7269\u7406\u635f\u574f\u96c6\u7fa4\uff09\u5bf9\u4f20\u7edf\u786e\u5b9a\u6027\u8def\u7531\u6784\u6210\u4e25\u91cd\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u9ad8\u65af\u4e92\u8fde\u7f51\u7edc\u7684\u6545\u969c\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u8def\u7531\u65b9\u6848\u3002\u4f7f\u7528PPO\uff08\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff09\u4ee3\u7406\uff0c\u8bbe\u8ba1\u7279\u5b9a\u7684\u5956\u52b1\u7ed3\u6784\u6765\u60e9\u7f5a\u63a5\u8fd1\u6545\u969c\u533a\u57df\uff0c\u7cfb\u7edf\u52a8\u6001\u5b66\u4e60\u7ed5\u8fc7\u6545\u969c\u533a\u57df\u3002\u4e0e\u8d2a\u5a6a\u81ea\u9002\u5e94\u6700\u77ed\u8def\u5f84\u8def\u7531\u7b97\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRL\u4ee3\u7406\u663e\u8457\u4f18\u4e8e\u81ea\u9002\u5e94\u8def\u7531\uff1a\u572840%\u6545\u969c\u5bc6\u5ea6\u4e0b\uff0cRL\u7ef4\u63010.95\u7684\u5305\u6295\u9012\u7387\uff0c\u800c\u8d2a\u5a6a\u7b97\u6cd5\u4ec5\u4e3a0.66\u3002\u572820%\u4f4e\u7f51\u7edc\u8d1f\u8f7d\u4e0b\uff0cRL\u6295\u9012\u7387\u4e3a0.57\uff0c\u8d2a\u5a6a\u7b97\u6cd5\u4e3a0.43\uff0c\u663e\u793aRL\u5728\u7ba1\u7406\u62e5\u585e\u65b9\u9762\u66f4\u6709\u6548\u3002", "conclusion": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8def\u7531\u65b9\u6848\u5728\u968f\u673a\u3001\u6613\u6545\u969c\u7684\u62d3\u6251\u7ed3\u6784\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5904\u7406\u9ad8\u65af\u4e92\u8fde\u7f51\u7edc\u6545\u969c\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u9ad8\u6545\u969c\u5bc6\u5ea6\u548c\u4f4e\u7f51\u7edc\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u8d2a\u5a6a\u81ea\u9002\u5e94\u8def\u7531\u3002"}}
{"id": "2512.20485", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.20485", "abs": "https://arxiv.org/abs/2512.20485", "authors": ["Tanisha Fonseca", "Gengrui Zhang"], "title": "WOC: Dual-Path Weighted Object Consensus Made Efficient", "comment": null, "summary": "Modern distributed systems face a critical challenge: existing consensus protocols optimize for either node heterogeneity or workload independence, but not both. For example, Cabinet leverages weighted quorums to handle node heterogeneity but serializes all operations through a global leader, limiting parallelism. EPaxos enables parallel execution for independent operations but treats all nodes uniformly, ignoring performance differences. To tackle this problem, we present WOC, a dual-path consensus protocol that dynamically routes operations into two paths based on their access patterns. Independent operations execute through a fast path that uses object-specific weighted quorums and completes in one network round-trip. Conflicting or shared objects route through a leader-coordinated slow path employing node-weighted consensus. Our evaluation demonstrates that WOC achieves up to 4X higher throughput than Cabinet for workloads with >70% independent objects, while maintaining equivalent performance under high contention.", "AI": {"tldr": "WOC\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8def\u5f84\u5171\u8bc6\u534f\u8bae\uff0c\u6839\u636e\u64cd\u4f5c\u8bbf\u95ee\u6a21\u5f0f\u52a8\u6001\u8def\u7531\uff1a\u72ec\u7acb\u64cd\u4f5c\u8d70\u5feb\u901f\u8def\u5f84\uff08\u5bf9\u8c61\u7279\u5b9a\u52a0\u6743\u4ef2\u88c1\uff0c1\u4e2a\u7f51\u7edc\u5f80\u8fd4\uff09\uff0c\u51b2\u7a81/\u5171\u4eab\u5bf9\u8c61\u8d70\u6162\u901f\u8def\u5f84\uff08\u8282\u70b9\u52a0\u6743\u5171\u8bc6\uff09\uff0c\u5728\u4fdd\u6301\u9ad8\u7ade\u4e89\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5bf9>70%\u72ec\u7acb\u5bf9\u8c61\u7684\u5de5\u4f5c\u8d1f\u8f7d\u5b9e\u73b0\u6bd4Cabinet\u9ad84\u500d\u7684\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u5171\u8bc6\u534f\u8bae\u8981\u4e48\u9488\u5bf9\u8282\u70b9\u5f02\u6784\u6027\u4f18\u5316\uff08\u5982Cabinet\u4f7f\u7528\u52a0\u6743\u4ef2\u88c1\u4f46\u4e32\u884c\u5316\u6240\u6709\u64cd\u4f5c\uff09\uff0c\u8981\u4e48\u9488\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u72ec\u7acb\u6027\u4f18\u5316\uff08\u5982EPaxos\u652f\u6301\u5e76\u884c\u6267\u884c\u4f46\u5ffd\u7565\u8282\u70b9\u6027\u80fd\u5dee\u5f02\uff09\uff0c\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u8282\u70b9\u5f02\u6784\u6027\u548c\u5de5\u4f5c\u8d1f\u8f7d\u72ec\u7acb\u6027\u3002", "method": "WOC\u91c7\u7528\u53cc\u8def\u5f84\u5171\u8bc6\u534f\u8bae\uff1a1\uff09\u5feb\u901f\u8def\u5f84\u5904\u7406\u72ec\u7acb\u64cd\u4f5c\uff0c\u4f7f\u7528\u5bf9\u8c61\u7279\u5b9a\u52a0\u6743\u4ef2\u88c1\uff0c1\u4e2a\u7f51\u7edc\u5f80\u8fd4\u5b8c\u6210\uff1b2\uff09\u6162\u901f\u8def\u5f84\u5904\u7406\u51b2\u7a81\u6216\u5171\u4eab\u5bf9\u8c61\uff0c\u91c7\u7528\u8282\u70b9\u52a0\u6743\u5171\u8bc6\uff0c\u7531\u9886\u5bfc\u8005\u534f\u8c03\u3002\u6839\u636e\u64cd\u4f5c\u8bbf\u95ee\u6a21\u5f0f\u52a8\u6001\u8def\u7531\u5230\u76f8\u5e94\u8def\u5f84\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff1a\u5bf9\u4e8e\u72ec\u7acb\u5bf9\u8c61\u6bd4\u4f8b>70%\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0cWOC\u6bd4Cabinet\u5b9e\u73b0\u9ad8\u8fbe4\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff1b\u5728\u9ad8\u7ade\u4e89\u573a\u666f\u4e0b\uff0cWOC\u4fdd\u6301\u4e0eCabinet\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "WOC\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u5171\u8bc6\u534f\u8bae\u65e0\u6cd5\u540c\u65f6\u4f18\u5316\u8282\u70b9\u5f02\u6784\u6027\u548c\u5de5\u4f5c\u8d1f\u8f7d\u72ec\u7acb\u6027\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u73b0\u4ee3\u5206\u5e03\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u5171\u8bc6\u89e3\u51b3\u65b9\u6848\u3002"}}
