{"id": "2509.09774", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.09774", "abs": "https://arxiv.org/abs/2509.09774", "authors": ["Doru Thom Popovici", "Mario Vega", "Angelos Ioannou", "Fabien Chaix", "Dania Mosuli", "Blair Reasoner", "Tan Nguyen", "Xiaokun Yang", "John Shalf"], "title": "Towards An Approach to Identify Divergences in Hardware Designs for HPC Workloads", "comment": "9 pages, 8 figures", "summary": "Developing efficient hardware accelerators for mathematical kernels used in\nscientific applications and machine learning has traditionally been a\nlabor-intensive task. These accelerators typically require low-level\nprogramming in Verilog or other hardware description languages, along with\nsignificant manual optimization effort. Recently, to alleviate this challenge,\nhigh-level hardware design tools like Chisel and High-Level Synthesis have\nemerged. However, as with any compiler, some of the generated hardware may be\nsuboptimal compared to expert-crafted designs. Understanding where these\ninefficiencies arise is crucial, as it provides valuable insights for both\nusers and tool developers. In this paper, we propose a methodology to\nhierarchically decompose mathematical kernels - such as Fourier transforms,\nmatrix multiplication, and QR factorization - into a set of common building\nblocks or primitives. Then the primitives are implemented in the different\nprogramming environments, and the larger algorithms get assembled. Furthermore,\nwe employ an automatic approach to investigate the achievable frequency and\nrequired resources. Performing this experimentation at each level will provide\nfairer comparisons between designs and offer guidance for both tool developers\nand hardware designers to adopt better practices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5206\u89e3\u6570\u5b66\u5185\u6838\u4e3a\u57fa\u672c\u539f\u8bed\u7684\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u7f16\u7a0b\u73af\u5883\u4e2d\u5b9e\u73b0\u8fd9\u4e9b\u539f\u8bed\u5e76\u7ec4\u88c5\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5206\u6790\u9891\u7387\u548c\u8d44\u6e90\u4f7f\u7528\u6765\u6bd4\u8f83\u4e0d\u540c\u8bbe\u8ba1\u65b9\u6cd5\u7684\u6548\u7387", "motivation": "\u4f20\u7edf\u786c\u4ef6\u52a0\u901f\u5668\u5f00\u53d1\u9700\u8981\u4f4e\u5c42Verilog\u7f16\u7a0b\u548c\u5927\u91cf\u624b\u52a8\u4f18\u5316\uff0c\u73b0\u6709\u9ad8\u7ea7\u8bbe\u8ba1\u5de5\u5177\u751f\u6210\u7684\u786c\u4ef6\u53ef\u80fd\u4e0d\u5982\u4e13\u5bb6\u8bbe\u8ba1\u4f18\u5316\uff0c\u9700\u8981\u7406\u89e3\u6548\u7387\u5dee\u8ddd\u7684\u6765\u6e90", "method": "\u5c06\u6570\u5b66\u5185\u6838\uff08\u5085\u91cc\u53f6\u53d8\u6362\u3001\u77e9\u9635\u4e58\u6cd5\u3001QR\u5206\u89e3\u7b49\uff09\u5206\u5c42\u5206\u89e3\u4e3a\u901a\u7528\u6784\u5efa\u5757/\u539f\u8bed\uff0c\u5728\u4e0d\u540c\u7f16\u7a0b\u73af\u5883\u4e2d\u5b9e\u73b0\u8fd9\u4e9b\u539f\u8bed\u5e76\u7ec4\u88c5\u7b97\u6cd5\uff0c\u91c7\u7528\u81ea\u52a8\u5316\u65b9\u6cd5\u5206\u6790\u53ef\u8fbe\u9891\u7387\u548c\u6240\u9700\u8d44\u6e90", "result": "\u901a\u8fc7\u5728\u5404\u4e2a\u5c42\u7ea7\u8fdb\u884c\u5b9e\u9a8c\uff0c\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u95f4\u66f4\u516c\u5e73\u7684\u6bd4\u8f83\uff0c\u5e76\u4e3a\u5de5\u5177\u5f00\u53d1\u8005\u548c\u786c\u4ef6\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u6539\u8fdb\u5b9e\u8df5\u7684\u6307\u5bfc", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u8bc6\u522b\u4e0d\u540c\u8bbe\u8ba1\u65b9\u6cd5\u4e2d\u7684\u6548\u7387\u5dee\u8ddd\uff0c\u4e3a\u786c\u4ef6\u8bbe\u8ba1\u5de5\u5177\u548c\u65b9\u6cd5\u7684\u6539\u8fdb\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3"}}
{"id": "2509.10051", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.10051", "abs": "https://arxiv.org/abs/2509.10051", "authors": ["Tianwei Pan", "Tianao Dai", "Jianlei Yang", "Hongbin Jing", "Yang Su", "Zeyu Hao", "Xiaotao Jia", "Chunming Hu", "Weisheng Zhao"], "title": "Finesse: An Agile Design Framework for Pairing-based Cryptography via Software/Hardware Co-Design", "comment": "Published on 52nd Annual International Symposium on Computer\n  Architecture (ISCA'25)", "summary": "Pairing-based cryptography (PBC) is crucial in modern cryptographic\napplications. With the rapid advancement of adversarial research and the\ngrowing diversity of application requirements, PBC accelerators need regular\nupdates in algorithms, parameter configurations, and hardware design. However,\ntraditional design methodologies face significant challenges, including\nprolonged design cycles, difficulties in balancing performance and flexibility,\nand insufficient support for potential architectural exploration.\n  To address these challenges, we introduce Finesse, an agile design framework\nbased on co-design methodology. Finesse leverages a co-optimization cycle\ndriven by a specialized compiler and a multi-granularity hardware simulator,\nenabling both optimized performance metrics and effective design space\nexploration. Furthermore, Finesse adopts a modular design flow to significantly\nshorten design cycles, while its versatile abstraction ensures flexibility\nacross various curve families and hardware architectures.\n  Finesse offers flexibility, efficiency, and rapid prototyping, comparing with\nprevious frameworks. With compilation times reduced to minutes, Finesse enables\nfaster iteration cycles and streamlined hardware-software co-design.\nExperiments on popular curves demonstrate its effectiveness, achieving\n$34\\times$ improvement in throughput and $6.2\\times$ increase in area\nefficiency compared to previous flexible frameworks, while outperforming\nstate-of-the-art non-flexible ASIC designs with a $3\\times$ gain in throughput\nand $3.2\\times$ improvement in area efficiency.", "AI": {"tldr": "Finesse\u662f\u4e00\u4e2a\u57fa\u4e8e\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u7684\u654f\u6377\u8bbe\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u914d\u5bf9\u5bc6\u7801\u5b66\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u4e13\u7528\u7f16\u8bd1\u5668\u548c\u591a\u7c92\u5ea6\u786c\u4ef6\u6a21\u62df\u5668\u5b9e\u73b0\u6027\u80fd\u4f18\u5316\u548c\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u663e\u8457\u7f29\u77ed\u8bbe\u8ba1\u5468\u671f\u5e76\u63d0\u9ad8\u7075\u6d3b\u6027\u3002", "motivation": "\u4f20\u7edf\u914d\u5bf9\u5bc6\u7801\u5b66\u52a0\u901f\u5668\u8bbe\u8ba1\u65b9\u6cd5\u9762\u4e34\u8bbe\u8ba1\u5468\u671f\u957f\u3001\u6027\u80fd\u4e0e\u7075\u6d3b\u6027\u96be\u4ee5\u5e73\u8861\u3001\u67b6\u6784\u63a2\u7d22\u652f\u6301\u4e0d\u8db3\u7b49\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u654f\u6377\u8bbe\u8ba1\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e13\u7528\u7f16\u8bd1\u5668\u548c\u591a\u7c92\u5ea6\u786c\u4ef6\u6a21\u62df\u5668\u8fdb\u884c\u534f\u540c\u4f18\u5316\u5faa\u73af\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u6d41\u7a0b\u548c\u901a\u7528\u62bd\u8c61\u6765\u652f\u6301\u4e0d\u540c\u66f2\u7ebf\u65cf\u548c\u786c\u4ef6\u67b6\u6784\u3002", "result": "\u7f16\u8bd1\u65f6\u95f4\u7f29\u77ed\u81f3\u5206\u949f\u7ea7\uff0c\u5728\u6d41\u884c\u66f2\u7ebf\u4e0a\u5b9e\u73b034\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c6.2\u500d\u9762\u79ef\u6548\u7387\u63d0\u5347\uff0c\u76f8\u6bd4\u975e\u7075\u6d3bASIC\u8bbe\u8ba1\u4e5f\u67093\u500d\u541e\u5410\u91cf\u548c3.2\u500d\u9762\u79ef\u6548\u7387\u4f18\u52bf\u3002", "conclusion": "Finesse\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u914d\u5bf9\u5bc6\u7801\u5b66\u52a0\u901f\u5668\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u3001\u9ad8\u6548\u6027\u548c\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u548cASIC\u8bbe\u8ba1\u3002"}}
{"id": "2509.10372", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.10372", "abs": "https://arxiv.org/abs/2509.10372", "authors": ["Huizheng Wang", "Zichuan Wang", "Zhiheng Yue", "Yousheng Long", "Taiquan Wei", "Jianxun Yang", "Yang Wang", "Chao Li", "Shaojun Wei", "Yang Hu", "Shouyi Yin"], "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging Bit-Slice-enabled Sparsity and Repetitiveness", "comment": null, "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively.", "AI": {"tldr": "MCBP\u662f\u4e00\u79cd\u57fa\u4e8e\u6bd4\u7279\u7c92\u5ea6\u7684\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5229\u7528\u6bd4\u7279\u5207\u7247\u7684\u91cd\u8907\u6027\u548c\u7a00\u758f\u6027\u6765\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u65b9\u9762\u5b9e\u73b0\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u9762\u4e34GEMM\u64cd\u4f5c\u3001\u6743\u91cd\u8bbf\u95ee\u548cKV\u7f13\u5b58\u8bbf\u95ee\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u73b0\u6709Transformer\u52a0\u901f\u5668\u96be\u4ee5\u540c\u65f6\u4f18\u5316\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u7ec6\u7c92\u5ea6\u7684\u534f\u540c\u4f18\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u5173\u952e\u6280\u672f\uff1a1) BS\u91cd\u8907\u6027\u542f\u7528\u7684\u8ba1\u7b97\u51cf\u5c11(BRCR) - \u901a\u8fc7\u6bd4\u7279\u5207\u7247\u5411\u91cf\u95f4\u7684\u5197\u4f59\u6d88\u9664GEMM\u8ba1\u7b97\uff1b2) BS\u7a00\u758f\u6027\u542f\u7528\u7684\u4e24\u6001\u7f16\u7801(BSTC) - \u5229\u7528\u9ad8\u4f4d\u6bd4\u7279\u5207\u7247\u6743\u91cd\u4e2d\u7684\u7a00\u758f\u6027\u51cf\u5c11\u6743\u91cd\u8bbf\u95ee\uff1b3) \u6bd4\u7279\u7c92\u5ea6\u6e10\u8fdb\u9884\u6d4b(BGPP) - \u57fa\u4e8e\u63d0\u524d\u7ec8\u6b62\u7684\u6bd4\u7279\u7c92\u5ea6\u9884\u6d4b\u51cf\u5c11KV\u7f13\u5b58\u8bbf\u95ee\u3002", "result": "\u572826\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMCBP\u76f8\u6bd4Nvidia A100 GPU\u5b9e\u73b09.43\u500d\u52a0\u901f\u548c31.1\u500d\u80fd\u6548\u63d0\u5347\uff1b\u76f8\u6bd4SOTA Transformer\u52a0\u901f\u5668Spatten\u3001FACT\u548cSOFA\uff0c\u5206\u522b\u5b9e\u73b035\u500d\u30015.2\u500d\u548c3.2\u500d\u7684\u8282\u80fd\u6548\u679c\u3002", "conclusion": "MCBP\u901a\u8fc7\u6bd4\u7279\u7c92\u5ea6\u7684\u8ba1\u7b97-\u5185\u5b58\u9ad8\u6548\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2d\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u573a\u666f\u4e0b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10400", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.10400", "abs": "https://arxiv.org/abs/2509.10400", "authors": ["Yang Zhong", "Haoran Wu", "Xueqi Li", "Sa Wang", "David Boland", "Yungang Bao", "Kan Shi"], "title": "TurboFuzz: FPGA Accelerated Hardware Fuzzing for Processor Agile Verification", "comment": null, "summary": "Verification is a critical process for ensuring the correctness of modern\nprocessors. The increasing complexity of processor designs and the emergence of\nnew instruction set architectures (ISAs) like RISC-V have created demands for\nmore agile and efficient verification methodologies, particularly regarding\nverification efficiency and faster coverage convergence. While simulation-based\napproaches now attempt to incorporate advanced software testing techniques such\nas fuzzing to improve coverage, they face significant limitations when applied\nto processor verification, notably poor performance and inadequate test case\nquality. Hardware-accelerated solutions using FPGA or ASIC platforms have tried\nto address these issues, yet they struggle with challenges including host-FPGA\ncommunication overhead, inefficient test pattern generation, and suboptimal\nimplementation of the entire multi-step verification process.\n  In this paper, we present TurboFuzz, an end-to-end hardware-accelerated\nverification framework that implements the entire Test\nGeneration-Simulation-Coverage Feedback loop on a single FPGA for modern\nprocessor verification. TurboFuzz enhances test quality through optimized test\ncase (seed) control flow, efficient inter-seed scheduling, and hybrid fuzzer\nintegration, thereby improving coverage and execution efficiency. Additionally,\nit employs a feedback-driven generation mechanism to accelerate coverage\nconvergence. Experimental results show that TurboFuzz achieves up to 2.23x more\ncoverage collection than software-based fuzzers within the same time budget,\nand up to 571x performance speedup when detecting real-world issues, while\nmaintaining full visibility and debugging capabilities with moderate area\noverhead.", "AI": {"tldr": "TurboFuzz\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u786c\u4ef6\u52a0\u901f\u9a8c\u8bc1\u6846\u67b6\uff0c\u5728\u5355\u4e2aFPGA\u4e0a\u5b9e\u73b0\u5b8c\u6574\u7684\u6d4b\u8bd5\u751f\u6210-\u4eff\u771f-\u8986\u76d6\u7387\u53cd\u9988\u5faa\u73af\uff0c\u76f8\u6bd4\u8f6f\u4ef6\u6a21\u7cca\u6d4b\u8bd5\u5668\u5728\u76f8\u540c\u65f6\u95f4\u5185\u5b9e\u73b02.23\u500d\u8986\u76d6\u7387\u63d0\u5347\uff0c\u68c0\u6d4b\u771f\u5b9e\u95ee\u9898\u65f6\u8fbe\u5230571\u500d\u6027\u80fd\u52a0\u901f\u3002", "motivation": "\u73b0\u4ee3\u5904\u7406\u5668\u8bbe\u8ba1\u590d\u6742\u6027\u589e\u52a0\u548cRISC-V\u7b49\u65b0ISA\u7684\u51fa\u73b0\uff0c\u9700\u8981\u66f4\u654f\u6377\u9ad8\u6548\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002\u4f20\u7edf\u4eff\u771f\u65b9\u6cd5\u6027\u80fd\u5dee\u3001\u6d4b\u8bd5\u7528\u4f8b\u8d28\u91cf\u4e0d\u8db3\uff0c\u786c\u4ef6\u52a0\u901f\u65b9\u6848\u5b58\u5728\u901a\u4fe1\u5f00\u9500\u5927\u3001\u6d4b\u8bd5\u6a21\u5f0f\u751f\u6210\u6548\u7387\u4f4e\u7b49\u95ee\u9898\u3002", "method": "\u5728\u5355\u4e2aFPGA\u4e0a\u5b9e\u73b0\u5b8c\u6574\u7684\u6d4b\u8bd5\u751f\u6210-\u4eff\u771f-\u8986\u76d6\u7387\u53cd\u9988\u5faa\u73af\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u6d4b\u8bd5\u7528\u4f8b\u63a7\u5236\u6d41\u3001\u9ad8\u6548\u7684\u79cd\u5b50\u95f4\u8c03\u5ea6\u548c\u6df7\u5408\u6a21\u7cca\u5668\u96c6\u6210\u6765\u63d0\u9ad8\u6d4b\u8bd5\u8d28\u91cf\u548c\u6267\u884c\u6548\u7387\uff0c\u91c7\u7528\u53cd\u9988\u9a71\u52a8\u7684\u751f\u6210\u673a\u5236\u52a0\u901f\u8986\u76d6\u7387\u6536\u655b\u3002", "result": "\u5728\u76f8\u540c\u65f6\u95f4\u9884\u7b97\u5185\u6bd4\u8f6f\u4ef6\u6a21\u7cca\u6d4b\u8bd5\u5668\u591a\u6536\u96c62.23\u500d\u8986\u76d6\u7387\uff0c\u68c0\u6d4b\u771f\u5b9e\u95ee\u9898\u65f6\u8fbe\u5230571\u500d\u6027\u80fd\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u5168\u7684\u53ef\u89c6\u5316\u548c\u8c03\u8bd5\u80fd\u529b\uff0c\u9762\u79ef\u5f00\u9500\u9002\u4e2d\u3002", "conclusion": "TurboFuzz\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u5668\u9a8c\u8bc1\u7684\u8986\u76d6\u7387\u548c\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u8c03\u8bd5\u80fd\u529b\u3002"}}
{"id": "2509.09795", "categories": ["cs.DC", "cs.DB", "cs.DS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.09795", "abs": "https://arxiv.org/abs/2509.09795", "authors": ["Arivarasan Karmegam", "Gabina Luz Bianchi", "Margarita Capretto", "Mart\u00edn Ceresa", "Antonio Fern\u00e1ndez Anta", "C\u00e9sar S\u00e1nchez"], "title": "Setchain Algorithms for Blockchain Scalability", "comment": null, "summary": "Setchain has been proposed to increase blockchain scalability by relaxing the\nstrict total order requirement among transactions. Setchain organizes elements\ninto a sequence of sets, referred to as epochs, so that elements within each\nepoch are unordered. In this paper, we propose and evaluate three distinct\nSetchain algorithms, that leverage an underlying block-based ledger. Vanilla is\na basic implementation that serves as a reference point. Compresschain\naggregates elements into batches, and compresses these batches before appending\nthem as epochs in the ledger. Hashchain converts batches into fixed-length\nhashes which are appended as epochs in the ledger. This requires Hashchain to\nuse a distributed service to obtain the batch contents from its hash. To allow\nlight clients to safely interact with only one server, the proposed algorithms\nmaintain, as part of the Setchain, proofs for the epochs. An epoch-proof is the\nhash of the epoch, cryptographically signed by a server. A client can verify\nthe correctness of an epoch with $f+1$ epoch-proofs (where $f$ is the maximum\nnumber of Byzantine servers assumed). All three Setchain algorithms are\nimplemented on top of the CometBFT blockchain application platform. We\nconducted performance evaluations across various configurations, using clusters\nof four, seven, and ten servers. Our results show that the Setchain algorithms\nreach orders of magnitude higher throughput than the underlying blockchain, and\nachieve finality with latency below 4 seconds.", "AI": {"tldr": "Setchain\u901a\u8fc7\u653e\u677e\u4ea4\u6613\u4e25\u683c\u5168\u5e8f\u8981\u6c42\u6765\u63d0\u9ad8\u533a\u5757\u94fe\u53ef\u6269\u5c55\u6027\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u57fa\u4e8e\u5e95\u5c42\u533a\u5757\u94fe\u7684\u7b97\u6cd5\uff1aVanilla\u3001Compresschain\u548cHashchain\uff0c\u5b9e\u73b0\u4e86\u6bd4\u5e95\u5c42\u533a\u5757\u94fe\u9ad8\u51e0\u4e2a\u6570\u91cf\u7ea7\u7684\u541e\u5410\u91cf\u3002", "motivation": "\u89e3\u51b3\u533a\u5757\u94fe\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u653e\u677e\u4ea4\u6613\u4e25\u683c\u5168\u5e8f\u8981\u6c42\u6765\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "method": "\u63d0\u51fa\u4e09\u79cdSetchain\u7b97\u6cd5\uff1aVanilla\uff08\u57fa\u7840\u5b9e\u73b0\uff09\u3001Compresschain\uff08\u6279\u91cf\u538b\u7f29\uff09\u548cHashchain\uff08\u54c8\u5e0c\u6279\u5904\u7406\uff09\uff0c\u57fa\u4e8eCometBFT\u5e73\u53f0\u5b9e\u73b0\uff0c\u4f7f\u7528epoch-proof\u673a\u5236\u786e\u4fdd\u5b89\u5168\u6027\u3002", "result": "\u57284\u30017\u300110\u4e2a\u670d\u52a1\u5668\u7684\u96c6\u7fa4\u6d4b\u8bd5\u4e2d\uff0cSetchain\u7b97\u6cd5\u8fbe\u5230\u6bd4\u5e95\u5c42\u533a\u5757\u94fe\u9ad8\u51e0\u4e2a\u6570\u91cf\u7ea7\u7684\u541e\u5410\u91cf\uff0c\u6700\u7ec8\u6027\u5ef6\u8fdf\u4f4e\u4e8e4\u79d2\u3002", "conclusion": "Setchain\u7b97\u6cd5\u901a\u8fc7\u653e\u677e\u6392\u5e8f\u8981\u6c42\u6709\u6548\u63d0\u9ad8\u4e86\u533a\u5757\u94fe\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e09\u79cd\u7b97\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u90fd\u6709\u826f\u597d\u8868\u73b0\uff0cHashchain\u7279\u522b\u9002\u5408\u8f7b\u5ba2\u6237\u7aef\u5e94\u7528\u3002"}}
{"id": "2509.09868", "categories": ["cs.DC", "cs.CR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.09868", "abs": "https://arxiv.org/abs/2509.09868", "authors": ["Yunhao Zhang", "Haobin Ni", "Soumya Basu", "Shir Cohen", "Maofan Yin", "Lorenzo Alvisi", "Robbert van Renesse", "Qi Chen", "Lidong Zhou"], "title": "Ordered Consensus with Equal Opportunity", "comment": null, "summary": "The specification of state machine replication (SMR) has no requirement on\nthe final total order of commands. In blockchains based on SMR, however, order\nmatters, since different orders could provide their clients with different\nfinancial rewards. Ordered consensus augments the specification of SMR to\ninclude specific guarantees on such order, with a focus on limiting the\ninfluence of Byzantine nodes. Real-world ordering manipulations, however, can\nand do happen even without Byzantine replicas, typically because of factors,\nsuch as faster networks or closer proximity to the blockchain infrastructure,\nthat give some clients an unfair advantage. To address this challenge, this\npaper proceeds to extend ordered consensus by requiring it to also support\nequal opportunity, a concrete notion of fairness, widely adopted in social\nsciences. Informally, equal opportunity requires that two candidates who,\naccording to a set of criteria deemed to be relevant, are equally qualified for\na position (in our case, a specific slot in the SMR total order), should have\nan equal chance of landing it. We show how randomness can be leveraged to keep\nbias in check, and, to this end, introduce the secret random oracle (SRO), a\nsystem component that generates randomness in a fault-tolerant manner. We\ndescribe two SRO designs based, respectively, on trusted hardware and threshold\nverifiable random functions, and instantiate them in Bercow, a new ordered\nconsensus protocol that, by approximating equal opportunity up to within a\nconfigurable factor, can effectively mitigate well-known ordering attacks in\nSMR-based blockchains.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6392\u5e8f\u5171\u8bc6\u534f\u8baeBercow\uff0c\u901a\u8fc7\u5bc6\u5bc6\u968f\u673a\u795e\u8c08(SRO)\u673a\u5236\u5b9e\u73b0\u5e73\u7b49\u673a\u4f1a\u7684\u516c\u5e73\u6027\u8981\u6c42\uff0c\u6709\u6548\u9632\u8303SMR\u57fa\u7840\u7684\u533a\u5757\u94fe\u4e2d\u7684\u6392\u5e8f\u653b\u51fb\u3002", "motivation": "\u4f20\u7edf\u72b6\u6001\u673a\u590d\u5236(SMR)\u534f\u8bae\u5bf9\u547d\u4ee4\u6700\u7ec8\u6392\u5e8f\u6ca1\u6709\u8981\u6c42\uff0c\u4f46\u5728\u533a\u5757\u94fe\u4e2d\u6392\u5e8f\u81f4\u5173\u91d1\u878d\u5956\u52b1\u3002\u867d\u7136\u6709\u6392\u5e8f\u5171\u8bc6\u6765\u9650\u5236\u5e0c\u814a\u8282\u70b9\u5f71\u54cd\uff0c\u4f46\u5b9e\u9645\u4e2d\u5373\u4f7f\u6ca1\u6709\u5e0c\u814a\u8282\u70b9\uff0c\u901a\u8fc7\u66f4\u5feb\u7f51\u7edc\u6216\u66f4\u8fd1\u8ddd\u79bb\u7b49\u56e0\u7d20\uff0c\u67d0\u4e9b\u5ba2\u6237\u4ecd\u53ef\u83b7\u5f97\u4e0d\u516c\u5e73\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u6269\u5c55\u6392\u5e8f\u5171\u8bc6\u8981\u6c42\u652f\u6301\u5e73\u7b49\u673a\u4f1a\u7684\u516c\u5e73\u6027\u6982\u5ff5\uff0c\u5e76\u4ecb\u7ecd\u4e86\u5bc6\u5bc6\u968f\u673a\u795e\u8c08(SRO)\u7cfb\u7edf\u7ec4\u4ef6\uff0c\u80fd\u591f\u4ee5\u6545\u969c\u5bb9\u9519\u65b9\u5f0f\u751f\u6210\u968f\u673a\u6570\u3002\u63d0\u51fa\u4e86\u57fa\u4e8e\u4fe1\u4efb\u786c\u4ef6\u548c\u9608\u503c\u53ef\u9a8c\u8bc1\u968f\u673a\u51fd\u6570\u7684\u4e24\u79cdSRO\u8bbe\u8ba1\uff0c\u5e76\u5728Bercow\u534f\u8bae\u4e2d\u5b9e\u73b0\u3002", "result": "\u5f00\u53d1\u4e86Bercow\u534f\u8bae\uff0c\u80fd\u591f\u5728\u53ef\u914d\u7f6e\u56e0\u5b50\u8303\u56f4\u5185\u8fd1\u4f3c\u5b9e\u73b0\u5e73\u7b49\u673a\u4f1a\uff0c\u6709\u6548\u51cf\u7f13SMR\u57fa\u7840\u533a\u5757\u94fe\u4e2d\u7684\u77e5\u540d\u6392\u5e8f\u653b\u51fb\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5e73\u7b49\u673a\u4f1a\u516c\u5e73\u6027\u6982\u5ff5\u548c\u968f\u673a\u6027\u673a\u5236\uff0c\u8fd9\u9879\u7814\u7a76\u4e3aSMR\u57fa\u7840\u533a\u5757\u94fe\u63d0\u4f9b\u4e86\u66f4\u516c\u5e73\u7684\u6392\u5e8f\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u5171\u8bc6\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u5e94\u5bf9\u5b9e\u9645\u4e2d\u7684\u6392\u5e8f\u6b3a\u8bc8\u95ee\u9898\u3002"}}
{"id": "2509.10371", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10371", "abs": "https://arxiv.org/abs/2509.10371", "authors": ["Seokjin Go", "Joongun Park", "Spandan More", "Hanjiang Wu", "Irene Wang", "Aaron Jezghani", "Tushar Krishna", "Divya Mahajan"], "title": "Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective", "comment": null, "summary": "The rapid scaling of Large Language Models (LLMs) has pushed training\nworkloads far beyond the limits of single-node analysis, demanding a deeper\nunderstanding of how these models behave across large-scale, multi-GPU systems.\nIn this paper, we present a comprehensive characterization of LLM training\nacross diverse real-world workloads and hardware platforms, including NVIDIA\nH100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various\nparallelism strategies -- tensor, pipeline, data, and expert -- and evaluate\ntheir effects on hardware utilization, power consumption, and thermal behavior.\nWe further evaluate the effectiveness of optimizations such as activation\nrecomputation and compute-communication overlap. Our findings show that\nperformance is not determined solely by scaling hardware capacity. Scale-up\nsystems with fewer, higher-memory GPUs can outperform scale-out systems in\ncommunication-bound regimes, but only under carefully tuned configurations; in\nother cases, scale-out deployments achieve superior throughput. We also show\nthat certain parallelism combinations, such as tensor with pipeline, lead to\nbandwidth underutilization due to inefficient data chunking, while increasing\nmicrobatch sizes beyond a certain point induces bursty execution and peak power\nexcursions that worsen thermal throttling. These insights reveal how training\nperformance is shaped by complex interactions between hardware, system\ntopology, and model execution. We conclude by offering recommendations for\nsystem and hardware design to improve the scalability and reliability of future\nLLM systems and workloads. The source code of this project is available at\nhttps://github.com/sitar-lab/CharLLM-PPT.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u5728\u591aGPU\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\u7279\u5f81\u5316\u5206\u6790\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u548c\u5e76\u884c\u7b56\u7565\u5bf9\u786c\u4ef6\u5229\u7528\u7387\u3001\u529f\u8017\u548c\u70ed\u884c\u4e3a\u7684\u5f71\u54cd", "motivation": "\u968f\u7740LLM\u8bad\u7ec3\u8d1f\u8377\u8d8a\u6765\u8d8a\u8d85\u51fa\u5355\u8282\u70b9\u80fd\u529b\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u8fd9\u4e9b\u6a21\u578b\u5728\u5927\u89c4\u6a21\u591aGPU\u7cfb\u7edf\u4e2d\u7684\u884c\u4e3a\u7279\u5f81", "method": "\u5728NVIDIA H100/H200\u548cAMD MI250 GPU\u5e73\u53f0\u4e0a\uff0c\u5bf9\u5bc6\u96c6\u548c\u7a00\u758f\u6a21\u578b\u8fdb\u884c\u7efc\u5408\u6027\u80fd\u5206\u6790\uff0c\u6d4b\u8bd5\u4e86\u5f20\u91cf\u5e76\u884c\u3001\u6c34\u7ebf\u5e76\u884c\u3001\u6570\u636e\u5e76\u884c\u548c\u4e13\u5bb6\u5e76\u884c\u7b56\u7565\uff0c\u8bc4\u4f30\u6fc0\u6d3b\u91cd\u8ba1\u7b97\u548c\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\u7b49\u4f18\u5316\u6280\u672f", "result": "\u53d1\u73b0\u6027\u80fd\u4e0d\u4ec5\u4ec5\u53d6\u51b3\u4e8e\u786c\u4ef6\u89c4\u6a21\uff0c\u8c03\u8bd5\u597d\u7684\u5c11\u6570\u9ad8\u5185\u5b58GPU\u7cfb\u7edf\u5728\u901a\u4fe1\u7f1a\u53e3\u60c5\u51b5\u4e0b\u66f4\u4f18\uff0c\u67d0\u4e9b\u5e76\u884c\u7ec4\u5408\u5bfc\u81f4\u5e26\u5bbd\u5229\u7528\u7387\u4f4e\uff0c\u8fc7\u5927\u5fae\u6279\u91cf\u4f1a\u5bfc\u81f4\u529f\u8017\u5cf0\u503c\u548c\u70ed\u9650\u5236\u95ee\u9898", "conclusion": "\u8bad\u7ec3\u6027\u80fd\u7531\u786c\u4ef6\u3001\u7cfb\u7edf\u62d3\u6251\u7ed3\u6784\u548c\u6a21\u578b\u6267\u884c\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u51b3\u5b9a\uff0c\u5efa\u8bae\u4ece\u7cfb\u7edf\u548c\u786c\u4ef6\u8bbe\u8ba1\u65b9\u9762\u8fdb\u884c\u4f18\u5316\u4ee5\u63d0\u5347\u672a\u6765LLM\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027"}}
