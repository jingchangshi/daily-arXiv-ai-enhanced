<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [If-T: A Benchmark for Type Narrowing](https://arxiv.org/abs/2508.03830)
*Hanwen Guo,Ben Greenman*

Main category: cs.PL

TL;DR: If-T是一个语言无关的设计基准，用于评估类型缩窄系统的能力，通过简单程序揭示其优缺点，帮助研究者和语言设计者在精度与性能之间找到平衡。


<details>
  <summary>Details</summary>
Motivation: 动态类型程序的设计需要类型缩窄机制，但目前缺乏统一标准衡量其行为优劣，且现有系统复杂度高，收益不明确。

Method: 基于文献、语言文档和实验，If-T提出核心维度，每个维度包含正反示例程序，用于评估类型缩窄系统的能力。

Result: If-T成功应用于五种类型检查器（如TypeScript、Flow等），揭示了它们在逻辑推理和用户定义谓词等方面的差异。

Conclusion: If-T为类型缩窄系统提供了公平评估标准，帮助未来设计在精度、性能和注解负担之间取得更好平衡。

Abstract: **Context:** The design of static type systems that can validate
dynamically-typed programs (**gradually**) is an ongoing challenge. A key
difficulty is that dynamic code rarely follows datatype-driven design. Programs
instead use runtime tests to narrow down the proper usage of incoming data.
Type systems for dynamic languages thus need a **type narrowing** mechanism
that refines the type environment along individual control paths based on
dominating tests, a form of flow-sensitive typing. In order to express
refinements, the type system must have some notion of sets and subsets. Since
set-theoretic types are computationally and ergonomically complex, the need for
type narrowing raises design questions about how to balance precision and
performance. **Inquiry:** To date, the design of type narrowing systems has
been driven by intuition, past experience, and examples from users in various
language communities. There is no standard that captures desirable and
undesirable behaviors. Prior formalizations of narrowing are also significantly
more complex than a standard type system, and it is unclear how the extra
complexity pays off in terms of concrete examples. This paper addresses the
problems through If-T, a language-agnostic **design benchmark** for type
narrowing that characterizes the abilities of implementations using simple
programs that draw attention to fundamental questions. Unlike a traditional
performance-focused benchmark, If-T measures a narrowing system's ability to
validate correct code and reject incorrect code. Unlike a test suite, systems
are not required to fully conform to If-T. Deviations are acceptable provided
they are justified by well-reasoned design considerations, such as compile-time
performance. **Approach:** If-T is guided by the literature on type narrowing,
the documentation of gradual languages such as TypeScript, and experiments with
typechecker implementations. We have identified a set of core technical
dimensions for type narrowing. For each dimension, the benchmark contains a set
of topics and (at least) two characterizing programs per topic: one that should
typecheck and one that should not typecheck. **Knowledge:** If-T provides a
baseline to measure type narrowing systems. For researchers, it provides
criteria to categorize future designs via its collection of positive and
negative examples. For language designers, the benchmark demonstrates the
payoff of typechecker complexity in terms of concrete examples. Designers can
use the examples to decide whether supporting a particular example is
worthwhile. Both the benchmark and its implementations are freely available
online. **Grounding:** We have implemented the benchmark for five typecheckers:
TypeScript, Flow, Typed Racket, mypy, and Pyright. The results highlight
important differences, such as the ability to track logical implications among
program variables and typechecking for user-defined narrowing predicates.
**Importance:** Type narrowing is essential for gradual type systems, but the
tradeoffs between systems with different complexity have been unclear. If-T
clarifies these tradeoffs by illustrating the benefits and limitations of each
level of complexity. With If-T as a way to assess implementations in a fair,
cross-language manner, future type system designs can strive for a better
balance among precision, annotation burden, and performance.

</details>


### [2] [A Type System for Data Privacy Compliance in Active Object Languages](https://arxiv.org/abs/2508.03831)
*Chinmayi Prabhu Baramashetru,Paola Giannini,Silvia Lizeth Tapia Tarifa,Olaf Owe*

Main category: cs.PL

TL;DR: 本文提出了一种基于语言的隐私集成方法，通过静态和运行时技术结合，确保个人数据符合GDPR要求。


<details>
  <summary>Details</summary>
Motivation: GDPR等数据保护法规要求系统在设计时嵌入隐私保护，但将抽象原则转化为具体方法仍具挑战性。

Method: 采用基于类型检查和类型推断的主动对象语言框架，跟踪授权数据流并自动生成运行时约束。

Result: 通过类型系统整合合规检查和用户同意变更，验证了方法的可行性，并展示了如何满足GDPR常见要求。

Conclusion: 该工作为隐私感知系统设计提供了系统化、自动化的方法，对医疗和金融等领域的可信系统构建具有重要意义。

Abstract: Data protection laws such as GDPR aim to give users unprecedented control
over their personal data. Compliance with these regulations requires
systematically considering information flow and interactions among entities
handling sensitive data. Privacy-by-design principles advocate embedding data
protection into system architectures as a default. However, translating these
abstract principles into concrete, explicit methods remains a significant
challenge. This paper addresses this gap by proposing a language-based approach
to privacy integration, combining static and runtime techniques. By employing
type checking and type inference in an active object language, the framework
enables the tracking of authorised data flows and the automatic generation of
constraints checked at runtime based on user consent. This ensures that
personal data is processed in compliance with GDPR constraints. The key
contribution of this work is a type system that gather the compliance checks
and the changes to users consent and integrates data privacy compliance
verification into system execution. The paper demonstrates the feasibility of
this approach through a soundness proof and several examples, illustrating how
the proposed language addresses common GDPR requirements, such as user consent,
purpose limitation, and data subject rights. This work advances the state of
the art in privacy-aware system design by offering a systematic and automated
method for integrating GDPR compliance into programming languages. This
capability has implications for building trustworthy systems in domains such as
healthcare or finance, where data privacy is crucial.

</details>


### [3] [Generating Inputs for Grammar Mining using Dynamic Symbolic Execution](https://arxiv.org/abs/2508.03832)
*Andreas Pointner,Josef Pichler,Herbert Prähofer*

Main category: cs.PL

TL;DR: 论文提出了一种自动生成输入的方法，用于改进语法挖掘的完整性，解决了现有方法因输入数据不足而遗漏边缘案例的问题。


<details>
  <summary>Details</summary>
Motivation: 软件组件在演化过程中可能偏离原始规范，导致无法准确确定其接受的输入。现有语法挖掘方法依赖输入数据的完整性，但实际中难以获取足够覆盖的输入数据。

Method: 结合动态符号执行（DSE）和两种新机制（迭代扩展输入和三阶段输入生成），提出了一种全自动输入生成方法。

Result: 在11个基准应用中验证了方法的有效性，其提取的语法在精确度和召回率上接近现有最佳方法，并能发现传统方法遗漏的边缘案例。

Conclusion: 该方法为软件工程提供了一种自动化、可扩展且精确的语法挖掘解决方案，显著提升了提取语法的全面性和鲁棒性。

Abstract: A vast number of software systems include components that parse and process
structured input. In addition to programming languages, which are analyzed by
compilers or interpreters, there are numerous components that process
standardized or proprietary data formats of varying complexity. Even if such
components were initially developed and tested based on a specification, such
as a grammar, numerous modifications and adaptations over the course of
software evolution can make it impossible to precisely determine which inputs
they actually accept. In this situation, grammar mining can be used to
reconstruct the specification in the form of a grammar. Established approaches
already produce useful results, provided that sufficient input data is
available to fully cover the input language. However, achieving this
completeness is a major challenge. In practice, only input data recorded during
the operation of the software systems is available. If this data is used for
grammar mining, the resulting grammar reflects only the actual processed inputs
but not the complete grammar of the input language accepted by the software
component. As a result, edge cases or previously supported features that no
longer appear in the available input data are missing from the generated
grammar. This work addresses this challenge by introducing a novel approach for
the automatic generation of inputs for grammar mining. Although input
generators have already been used for fuzz testing, it remains unclear whether
they are also suitable for grammar miners. Building on the grammar miner Mimid,
this work presents a fully automated approach to input generation. The approach
leverages Dynamic Symbolic Execution (DSE) and extends it with two mechanisms
to overcome the limitations of DSE regarding structured input parsers. First,
the search for new inputs is guided by an iterative expansion that starts with
a single-character input and gradually extends it. Second, input generation is
structured into a novel three-phase approach, which separates the generation of
inputs for parser functions. The proposed method was evaluated against a
diverse set of eleven benchmark applications from the existing literature.
Results demonstrate that the approach achieves precision and recall for
extracted grammars close to those derived from state-of-the-art grammar miners
such as Mimid. Notably, it successfully uncovers subtle features and edge cases
in parsers that are typically missed by such grammar miners. The effectiveness
of the method is supported by empirical evidence, showing that it can achieve
high performance in various domains without requiring prior input samples. This
contribution is significant for researchers and practitioners in software
engineering, offering an automated, scalable, and precise solution for grammar
mining. By eliminating the need for manual input generation, the approach not
only reduces workload but also enhances the robustness and comprehensiveness of
the extracted grammars. Following this approach, software engineers can
reconstruct specification from existing (legacy) parsers.

</details>


### [4] [Weak Memory Model Formalisms: Introduction and Survey](https://arxiv.org/abs/2508.04115)
*Roger C. Su,Robert J. Colvin*

Main category: cs.PL

TL;DR: 本文综述了弱内存模型的规范化研究，包括其定义、对执行的影响、推理工具及系统，并介绍了两种常见的形式化表示方法。


<details>
  <summary>Details</summary>
Motivation: 由于微架构特性或编译器优化，程序顺序无法可靠指示执行顺序，弱内存模型使并发编程更具挑战性。规范化的弱内存模型对开发安全关键软件至关重要。

Method: 通过操作语义和公理语义两种形式化方法，结合简化版的Intel x86架构，探讨弱内存模型的规范化和推理工具。

Result: 综述了弱内存模型的历史发展、硬件特性、理论成果及计算复杂性，并展望了未来研究方向。

Conclusion: 弱内存模型的规范化是解决并发编程问题的关键，未来研究需进一步优化工具和理论。

Abstract: Memory consistency models define the order in which accesses to shared memory
in a concurrent system may be observed to occur. Such models are a necessity
since program order is not a reliable indicator of execution order, due to
microarchitectural features or compiler transformations. Concurrent
programming, already a challenging task, is thus made even harder when weak
memory effects must be addressed. A rigorous specification of weak memory
models is therefore essential to make this problem tractable for developers of
safety- and security-critical, low-level software.
  In this paper we survey the field of formalisations of weak memory models,
including their specification, their effects on execution, and tools and
inference systems for reasoning about code. To assist the discussion we also
provide an introduction to two styles of formal representation found commonly
in the literature (using a much simplified version of Intel's x86 as the
example): a step-by-step construction of traces of the system (operational
semantics); and with respect to relations between memory events (axiomatic
semantics). The survey covers some long-standing hardware features that lead to
observable weak behaviours, a description of historical developments in
practice and in theory, an overview of computability and complexity results,
and outlines current and future directions in the field.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [FlashCommunication V2: Bit Splitting and Spike Reserving for Any Bit Communication](https://arxiv.org/abs/2508.03760)
*Qingyuan Li,Bo Zhang,Hui Kang,Tianhao Xu,Yulei Qian,Yuchen Xie,Lin Ma*

Main category: cs.DC

TL;DR: FlashCommunication V2 是一种新的通信范式，通过位拆分和峰值保留技术，支持任意位宽的跨GPU高效传输，显著提升通信系统的灵活性和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 分布式训练和部署大型语言模型（LLMs）时，通信瓶颈成为关键挑战，需要高效的跨GPU传输解决方案。

Method: 提出位拆分技术将不规则位宽分解为基本单元，确保硬件兼容性；峰值保留技术保留数值异常值（最小和最大值）为浮点数，缩小动态数值范围，支持低至2位的量化。

Result: 在NVLink和PCIe架构下，实现了AllReduce最高3.2倍加速和All2All通信2倍加速。

Conclusion: FlashCommunication V2通过软硬件协同设计，显著提升了通信效率和性能，适用于大规模LLM训练和部署。

Abstract: Nowadays, communication bottlenecks have emerged as a critical challenge in
the distributed training and deployment of large language models (LLMs). This
paper introduces FlashCommunication V2, a novel communication paradigm enabling
efficient cross-GPU transmission at arbitrary bit widths. Its core innovations
lie in the proposed bit splitting and spike reserving techniques, which address
the challenges of low-bit quantization. Bit splitting decomposes irregular bit
widths into basic units, ensuring compatibility with hardware capabilities and
thus enabling transmission at any bit width. Spike reserving, on the other
hand, retains numerical outliers (i.e., minima and maxima) as floating-point
numbers, which shrinks the dynamic numerical range and pushes the quantization
limits to 2-bit with acceptable losses. FlashCommunication V2 significantly
enhances the flexibility and resource utilization of communication systems.
Through meticulous software-hardware co-design, it delivers robust performance
and reduced overhead across both NVLink-based and PCIe-based architectures,
achieving a maximum 3.2$\times$ speedup in AllReduce and 2$\times$ in All2All
communication.

</details>


### [6] [Two-dimensional Sparse Parallelism for Large Scale Deep Learning Recommendation Model Training](https://arxiv.org/abs/2508.03854)
*Xin Zhang,Quanyu Zhu,Liangbei Xu,Zain Huda,Wang Zhou,Jin Fang,Dennis van der Staay,Yuxi Hu,Jade Nie,Jiyan Yang,Chunzhi Yang*

Main category: cs.DC

TL;DR: 提出了一种新的二维稀疏并行方法，解决了大规模深度学习推荐模型（DLRM）训练中的内存和通信问题，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 随着DLRM模型复杂度的增加，传统全并行策略在扩展性上面临挑战，如内存限制、通信开销和性能损失。

Method: 结合数据并行和模型并行，提出二维稀疏并行方法，并开发动量缩放的行级AdaGrad算法以减少性能损失。

Result: 实验表明，该方法在4K GPU上实现了近乎线性的训练速度扩展，同时保持模型性能。

Conclusion: 该方法为推荐模型训练设定了新的技术标杆，显著提升了效率和扩展性。

Abstract: The increasing complexity of deep learning recommendation models (DLRM) has
led to a growing need for large-scale distributed systems that can efficiently
train vast amounts of data. In DLRM, the sparse embedding table is a crucial
component for managing sparse categorical features. Typically, these tables in
industrial DLRMs contain trillions of parameters, necessitating model
parallelism strategies to address memory constraints. However, as training
systems expand with massive GPUs, the traditional fully parallelism strategies
for embedding table post significant scalability challenges, including
imbalance and straggler issues, intensive lookup communication, and heavy
embedding activation memory. To overcome these limitations, we propose a novel
two-dimensional sparse parallelism approach. Rather than fully sharding tables
across all GPUs, our solution introduces data parallelism on top of model
parallelism. This enables efficient all-to-all communication and reduces peak
memory consumption. Additionally, we have developed the momentum-scaled
row-wise AdaGrad algorithm to mitigate performance losses associated with the
shift in training paradigms. Our extensive experiments demonstrate that the
proposed approach significantly enhances training efficiency while maintaining
model performance parity. It achieves nearly linear training speed scaling up
to 4K GPUs, setting a new state-of-the-art benchmark for recommendation model
training.

</details>


### [7] [Reputation-based partition scheme for IoT security](https://arxiv.org/abs/2508.03981)
*Zhikui Chen,Muhammad Zeeshan Haider,Naiwen Luo,Shuo Yu,Xu Yuan,Yaochen Zhang,Tayyaba Noreen*

Main category: cs.DC

TL;DR: 本文提出了一种基于信誉的分区方案（RSPC），以解决众包感知中的安全和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 随着智能终端的普及，众包感知成为数据聚合的重要范式，但集中式管理带来安全漏洞和可扩展性问题。

Method: RSPC通过结合节点信誉值计算最优分区大小，并定期重组网络以避免分区攻击，同时提出四阶段确认协议确保跨分区交易安全高效。

Result: 实验表明，RSPC提高了众包感知的可扩展性、低延迟和高吞吐量。

Conclusion: RSPC为众包感知提供了一种有效的安全和可扩展性解决方案。

Abstract: With the popularity of smart terminals, such as the Internet of Things,
crowdsensing is an emerging data aggregation paradigm, which plays a pivotal
role in data-driven applications. There are some key issues in the development
of crowdsensing such as platform security and privacy protection. As the
crowdsensing is usually managed by a centralized platform, centralized
management will bring various security vulnerabilities and scalability issues.
To solve these issues, an effective reputation-based partition scheme (RSPC) is
proposed in this article. The partition scheme calculates the optimal partition
size by combining the node reputation value and divides the node into several
disjoint partitions according to the node reputation value. By selecting the
appropriate partition size, RSPC provides a mechanism to ensure that each
partition is valid, as long as themaximum permissible threshold for the failed
node is observed. At the same time, the RSPC reorganizes the network
periodically to avoid partition attacks. In addition, for cross-partition
transactions, this paper innovatively proposes a four-stage confirmation
protocol to ensure the efficient and safe completion of cross-partition
transactions. Finally, experiments show that RSPC improves scalability, low
latency, and high throughput for crowdsensing.

</details>


### [8] [High-Performance and Power-Efficient Emulation of Matrix Multiplication using INT8 Matrix Engines](https://arxiv.org/abs/2508.03984)
*Yuki Uchino,Katsuhisa Ozaki,Toshiyuki Imamura*

Main category: cs.DC

TL;DR: 论文提出了一种利用低精度矩阵引擎模拟高精度矩阵乘法（SGEMM和DGEMM）的方法，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 深度学习中对低精度矩阵乘法的需求推动了高精度模拟技术的发展，以充分利用高性能低精度引擎。

Method: 提出了一种新的模拟方法，利用低精度矩阵引擎实现高精度矩阵乘法。

Result: 在GH200 Grace Hopper Superchip上，DGEMM模拟性能提升1.4倍，能效提升43%；SGEMM模拟性能提升3.0倍，能效提升154%。

Conclusion: 新方法在性能和能效上显著优于传统模拟方法和原生实现，适用于大规模问题。

Abstract: Recent architectures integrate high-performance and power-efficient matrix
engines. These engines demonstrate remarkable performance in low-precision
matrix multiplication, which is crucial in deep learning. Several techniques
have been proposed to emulate single- and double-precision general
matrix-matrix multiplication (SGEMM and DGEMM, respectively) by leveraging such
low-precision matrix engines. In this study, we present emulation methods that
significantly outperforms conventional approaches. On a GH200 Grace Hopper
Superchip, the proposed DGEMM emulation achieves a 1.4x speedup and a 43\%
improvement in power efficiency compared to native DGEMM for sufficiently large
problems. The proposed SGEMM emulation achieves a 3.0x speedup and a 154\%
improvement in power efficiency compared to native SGEMM for sufficiently large
problems. Furthermore, compared to conventional emulation methods, the proposed
emulation achieves more than 2x higher performance and superior power
efficiency.

</details>


### [9] [Advanced DAG-Based Ranking (ADR) Protocol for Blockchain Scalability](https://arxiv.org/abs/2508.04000)
*Tayyaba Noreen,Qiufen Xia,Muhammad Zeeshan Haider*

Main category: cs.DC

TL;DR: 本文提出了一种基于DAG的ADR协议，旨在解决区块链系统的吞吐量、可扩展性和延迟问题，特别适用于物联网应用。


<details>
  <summary>Details</summary>
Motivation: 当前区块链系统在共识机制和节点身份管理方面存在局限性，导致吞吐量低、可扩展性差和高延迟，限制了其在物联网等领域的应用。

Method: ADR协议采用DAG结构，通过三步法（节点验证、DAG账本构建和排名算法）提升网络安全性、吞吐量和可扩展性。

Result: 在Amazon EC2集群上的实验表明，ADR显著提升了交易吞吐量和网络活跃度，优于现有的DAG区块链如IOTA和ByteBall。

Conclusion: ADR协议通过创新的DAG结构和排名算法，为区块链在物联网等领域的应用提供了高效、可扩展的解决方案。

Abstract: In the past decade, blockchain has emerged as a promising solution for
building secure distributed ledgers and has attracted significant attention.
However, current blockchain systems suffer from limited throughput, poor
scalability, and high latency. Due to limitations in consensus mechanisms,
especially in managing node identities, blockchain is often considered
unsuitable for applications such as the Internet of Things (IoT). This paper
proposes the Advanced DAG-based Ranking (ADR) protocol to enhance blockchain
scalability and throughput. ADR employs a directed acyclic graph (DAG)
structure where nodes are positioned based on their rankings. Unlike
traditional chains, ADR allows honest nodes to write blocks and verify
transactions using a DAG-based topology. The protocol follows a three-step
approach to secure the network against double-spending and enhance performance.
First, it verifies nodes using their public and private keys before granting
entry. Second, it builds an advanced DAG ledger enabling block production and
transaction validation. Third, a ranking algorithm filters out malicious nodes,
ranks the remaining nodes based on performance, and arranges them
topologically. This process increases throughput and ensures robust
scalability. We evaluated ADR on Amazon EC2 clusters with over 100 nodes,
including scenarios with injected malicious nodes. Simulation results
demonstrate that ADR significantly improves transaction throughput and network
liveness compared to existing DAG-based blockchains such as IOTA and ByteBall,
making it well-suited for IoT applications.

</details>


### [10] [High-Performance Statistical Computing (HPSC): Challenges, Opportunities, and Future Directions](https://arxiv.org/abs/2508.04013)
*Sameh Abdulah,Mary Lai O. Salvana,Ying Sun,David E. Keyes,Marc G. Genton*

Main category: cs.DC

TL;DR: 论文探讨了统计计算（SC）与高性能计算（HPC）的融合，提出了高性能统计计算（HPSC）的愿景，并分析了挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 统计计算社区在HPC领域缺席，而其他学科（如模拟科学和AI）已广泛参与HPC。论文旨在推动SC与HPC的结合，以加速统计应用的发展。

Method: 通过回顾SC历史，提出HPSC的愿景，并分析技术挑战与社区适应需求。

Result: 提出了HPSC的发展路线图，强调社区合作与技术创新的重要性。

Conclusion: 通过SC与HPC的深度融合，可以推动高性能统计计算的发展，为统计科学带来新的机遇。

Abstract: We recognize the emergence of a statistical computing community focused on
working with large computing platforms and producing software and applications
that exemplify high-performance statistical computing (HPSC). The statistical
computing (SC) community develops software that is widely used across
disciplines. However, it remains largely absent from the high-performance
computing (HPC) landscape, particularly on platforms such as those featured on
the Top500 or Green500 lists. Many disciplines already participate in HPC,
mostly centered around simulation science, although data-focused efforts under
the artificial intelligence (AI) label are gaining popularity. Bridging this
gap requires both community adaptation and technical innovation to align
statistical methods with modern HPC technologies. We can accelerate progress in
fast and scalable statistical applications by building strong connections
between the SC and HPC communities. We present a brief history of SC, a vision
for how its strengths can contribute to statistical science in the HPC
environment (such as HPSC), the challenges that remain, and the opportunities
currently available, culminating in a possible roadmap toward a thriving HPSC
community.

</details>


### [11] [SelectiveShield: Lightweight Hybrid Defense Against Gradient Leakage in Federated Learning](https://arxiv.org/abs/2508.04265)
*Borui Li,Li Yan,Jianmin Liu*

Main category: cs.DC

TL;DR: SelectiveShield是一种轻量级混合防御框架，结合选择性同态加密和差分隐私，保护联邦学习中的梯度泄漏风险，同时保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在分散数据上协作训练模型，但易受梯度泄漏攻击，现有防御机制（如差分隐私和同态加密）在隐私、模型效用和系统开销之间存在权衡。

Method: SelectiveShield利用Fisher信息量化参数敏感性，通过协作协商协议选择敏感参数进行同态加密保护，非关键参数采用差分隐私噪声保护。

Result: 实验表明，SelectiveShield在显著降低梯度泄漏风险的同时，保持了强模型效用。

Conclusion: SelectiveShield为实际联邦学习部署提供了实用且可扩展的防御机制。

Abstract: Federated Learning (FL) enables collaborative model training on decentralized
data but remains vulnerable to gradient leakage attacks that can reconstruct
sensitive user information. Existing defense mechanisms, such as differential
privacy (DP) and homomorphic encryption (HE), often introduce a trade-off
between privacy, model utility, and system overhead, a challenge that is
exacerbated in heterogeneous environments with non-IID data and varying client
capabilities. To address these limitations, we propose SelectiveShield, a
lightweight hybrid defense framework that adaptively integrates selective
homomorphic encryption and differential privacy. SelectiveShield leverages
Fisher information to quantify parameter sensitivity, allowing clients to
identify critical parameters locally. Through a collaborative negotiation
protocol, clients agree on a shared set of the most sensitive parameters for
protection via homomorphic encryption. Parameters that are uniquely important
to individual clients are retained locally, fostering personalization, while
non-critical parameters are protected with adaptive differential privacy noise.
Extensive experiments demonstrate that SelectiveShield maintains strong model
utility while significantly mitigating gradient leakage risks, offering a
practical and scalable defense mechanism for real-world federated learning
deployments.

</details>


### [12] [S2M3: Split-and-Share Multi-Modal Models for Distributed Multi-Task Inference on the Edge](https://arxiv.org/abs/2508.04271)
*JinYi Yoon,JiHo Lee,Ting He,Nakjung Choi,Bo Ji*

Main category: cs.DC

TL;DR: S2M3是一种用于边缘设备的多模态多任务推理架构，通过模块拆分和共享减少资源使用，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态AI服务依赖云端存在带宽、延迟、隐私等问题，而边缘设备支持多任务面临资源挑战。

Method: 提出S2M3架构，拆分多模态模型的功能模块并共享通用模块，采用贪婪模块级放置和并行路由策略。

Result: 实验显示S2M3在单任务和多任务下分别减少内存使用50%和62%，延迟降低56.9%，且不影响准确性。

Conclusion: S2M3在资源受限设备上高效支持多模态多任务推理，优于云端AI。

Abstract: With the advancement of Artificial Intelligence (AI) towards multiple
modalities (language, vision, speech, etc.), multi-modal models have
increasingly been used across various applications (e.g., visual question
answering or image generation/captioning). Despite the success of AI as a
service for multi-modal applications, it relies heavily on clouds, which are
constrained by bandwidth, latency, privacy concerns, and unavailability under
network or server failures. While on-device AI becomes popular, supporting
multiple tasks on edge devices imposes significant resource challenges. To
address this, we introduce S2M3, a split-and-share multi-modal architecture for
multi-task inference on edge devices. Inspired by the general-purpose nature of
multi-modal models, which are composed of multiple modules (encoder, decoder,
classifier, etc.), we propose to split multi-modal models at functional-level
modules; and then share common modules to reuse them across tasks, thereby
reducing resource usage. To address cross-model dependency arising from module
sharing, we propose a greedy module-level placement with per-request parallel
routing by prioritizing compute-intensive modules. Through experiments on a
testbed consisting of 14 multi-modal models across 5 tasks and 10 benchmarks,
we demonstrate that S2M3 can reduce memory usage by up to 50% and 62% in
single-task and multi-task settings, respectively, without sacrificing
accuracy. Furthermore, S2M3 achieves optimal placement in 89 out of 95
instances (93.7%) while reducing inference latency by up to 56.9% on
resource-constrained devices, compared to cloud AI.

</details>


### [13] [Optimizing Microgrid Composition for Sustainable Data Centers](https://arxiv.org/abs/2508.04284)
*Julius Irion,Philipp Wiesner,Jonathan Bader,Odej Kao*

Main category: cs.DC

TL;DR: 本文提出了一种优化框架，结合计算与能源系统模拟器Vessim和NREL的SAM模型，用于评估数据中心微电网的长期可持续性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着数据中心能源需求增长和电网基础设施不足，微电网的可再生能源和储能配置对可持续性和可靠性的影响缺乏评估工具。

Method: 扩展Vessim模拟器，整合NREL的SAM模型，模拟计算负载、可再生能源生产和储能的交互，并采用多阶段黑盒优化方法。

Result: 框架能够评估微电网配置对运营和隐含碳排放的影响，帮助数据中心运营商做出更明智的能源规划决策。

Conclusion: 该优化框架为数据中心微电网的可持续性和可靠性提供了实用的评估工具。

Abstract: As computing energy demand continues to grow and electrical grid
infrastructure struggles to keep pace, an increasing number of data centers are
being planned with colocated microgrids that integrate on-site renewable
generation and energy storage. However, while existing research has examined
the tradeoffs between operational and embodied carbon emissions in the context
of renewable energy certificates, there is a lack of tools to assess how the
sizing and composition of microgrid components affects long-term sustainability
and power reliability.
  In this paper, we present a novel optimization framework that extends the
computing and energy system co-simulator Vessim with detailed renewable energy
generation models from the National Renewable Energy Laboratory's (NREL) System
Advisor Model (SAM). Our framework simulates the interaction between computing
workloads, on-site renewable production, and energy storage, capturing both
operational and embodied emissions. We use a multi-horizon black-box
optimization to explore efficient microgrid compositions and enable operators
to make more informed decisions when planning energy systems for data centers.

</details>


### [14] [Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in Cloud Computing](https://arxiv.org/abs/2508.04334)
*Noor Islam S. Mohammad*

Main category: cs.DC

TL;DR: 提出了一种结合深度强化学习和蚁群优化的混合调度算法，显著提升了IoT云环境中的任务调度效率。


<details>
  <summary>Details</summary>
Motivation: IoT设备快速增长产生的异构数据流需要高效调度以满足延迟、能耗和QoS需求，现有方法难以适应动态负载和网络变化。

Method: 结合深度强化学习（模型无关策略梯度）和蚁群优化，前者适应实时负载变化，后者全局优化资源分配。

Result: 实验显示，相比现有方法，平均响应时间减少18.4%，资源利用率提升12.7%，能耗降低9.3%，且满足SLA要求。

Conclusion: 深度强化学习与群体智能结合为IoT云平台提供了高效、可扩展的调度方案。

Abstract: The rapid growth of Internet of Things (IoT) devices produces massive,
heterogeneous data streams, demanding scalable and efficient scheduling in
cloud environments to meet latency, energy, and Quality-of-Service (QoS)
requirements. Existing scheduling methods often lack adaptability to dynamic
workloads and network variability inherent in IoT-cloud systems. This paper
presents a novel hybrid scheduling algorithm combining deep Reinforcement
Learning (RL) and Ant Colony Optimization (ACO) to address these challenges.
The deep RL agent utilizes a model-free policy-gradient approach to learn
adaptive task allocation policies responsive to real-time workload fluctuations
and network states. Simultaneously, the ACO metaheuristic conducts a global
combinatorial search to optimize resource distribution, mitigate congestion,
and balance load across distributed cloud nodes. Extensive experiments on
large-scale synthetic IoT datasets, reflecting diverse workloads and QoS
constraints, demonstrate that the proposed method achieves up to 18.4%
reduction in average response time, 12.7% improvement in resource utilization,
and 9.3% decrease in energy consumption compared to leading heuristics and
RL-only baselines. Moreover, the algorithm ensures strict Service Level
Agreement (SLA) compliance through deadline-aware scheduling and dynamic
prioritization. The results confirm the effectiveness of integrating model-free
RL with swarm intelligence for scalable, energy-efficient IoT data scheduling,
offering a promising approach for next-generation IoT-cloud platforms.

</details>


### [15] [Edge-assisted Parallel Uncertain Skyline Processing for Low-latency IoE Analysis](https://arxiv.org/abs/2508.04596)
*Chuan-Chi Lai,Yan-Lin Chen,Bo-Xin Liu,Chuan-Ming Liu*

Main category: cs.DC

TL;DR: 论文提出了一种边缘辅助并行不确定天际线（EPUS）算法，用于处理IoE大数据分析，通过边缘计算减少数据传输和云端资源需求。


<details>
  <summary>Details</summary>
Motivation: 随着IoE数据的快速增长，传统云计算环境下数据传输和资源需求成本高昂，边缘计算成为解决方案。

Method: 利用天际线候选集在并行边缘计算节点上修剪数据，仅发送必要信息到云端更新全局天际线。

Result: 仿真结果显示，EPUS方法在二维数据上减少50%以上延迟，高维数据表现也优于现有方法。

Conclusion: EPUS算法有效降低了数据传输量和云端资源需求，适用于低延迟IoE分析应用。

Abstract: Due to the Internet of Everything (IoE), data generated in our life become
larger. As a result, we need more effort to analyze the data and extract
valuable information. In the cloud computing environment, all data analysis is
done in the cloud, and the client only needs less computing power to handle
some simple tasks. However, with the rapid increase in data volume, sending all
data to the cloud via the Internet has become more expensive. The required
cloud computing resources have also become larger. To solve this problem, edge
computing is proposed. Edge is granted with more computation power to process
data before sending it to the cloud. Therefore, the data transmitted over the
Internet and the computing resources required by the cloud can be effectively
reduced. In this work, we proposed an Edge-assisted Parallel Uncertain Skyline
(EPUS) algorithm for emerging low-latency IoE analytic applications. We use the
concept of skyline candidate set to prune data that are less likely to become
the skyline data on the parallel edge computing nodes. With the candidate
skyline set, each edge computing node only sends the information required to
the server for updating the global skyline, which reduces the amount of data
that transfer over the internet. According to the simulation results, the
proposed method is better than two comparative methods, which reduces the
latency of processing two-dimensional data by more than 50%. For
high-dimensional data, the proposed EPUS method also outperforms the other
existing methods.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [16] [Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent Memory Subsystems](https://arxiv.org/abs/2508.03837)
*Davide Zoni,Andrea Galimberti,Adriano Guarisco*

Main category: cs.AR

TL;DR: Rhea是一个统一框架，用于设计和验证RTL缓存一致性内存子系统，支持多核SoC架构的开发。


<details>
  <summary>Details</summary>
Motivation: 设计和验证高效的缓存一致性内存子系统是现代多核SoC开发中的关键但复杂的任务。

Method: Rhea生成可综合、高度可配置的RTL，并集成Verilator和gem5的仿真功能，支持实际RTL测试。

Result: Rhea设计的MSI内存子系统在16核规模下性能介于gem5的MI和MOESI模型之间，仿真开销为1.6-2.7倍。

Conclusion: Rhea能高效支持RTL缓存一致性内存子系统的快速开发和验证，具有可扩展性。

Abstract: Designing and validating efficient cache-coherent memory subsystems is a
critical yet complex task in the development of modern multi-core
system-on-chip architectures. Rhea is a unified framework that streamlines the
design and system-level validation of RTL cache-coherent memory subsystems. On
the design side, Rhea generates synthesizable, highly configurable RTL
supporting various architectural parameters. On the validation side, Rhea
integrates Verilator's cycle-accurate RTL simulation with gem5's full-system
simulation, allowing realistic workloads and operating systems to run alongside
the actual RTL under test. We apply Rhea to design MSI-based RTL memory
subsystems with one and two levels of private caches and scaling up to sixteen
cores. Their evaluation with 22 applications from state-of-the-art benchmark
suites shows intermediate performance relative to gem5 Ruby's MI and MOESI
models. The hybrid gem5-Verilator co-simulation flow incurs a moderate
simulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher
fidelity by simulating real RTL hardware. This overhead decreases with scale,
down to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's
effectiveness and scalability in enabling fast development of RTL
cache-coherent memory subsystem designs.

</details>


### [17] [FlashVault: Versatile In-NAND Self-Encryption with Zero Area Overhead](https://arxiv.org/abs/2508.03866)
*Seock-Hwan Noh,Hoyeon Lee,Junkyum Kim,Junsu Im,Jay H. Park,Sungjin Lee,Sam H. Noh,Yeseong Kim,Jaeha Kung*

Main category: cs.AR

TL;DR: FlashVault是一种嵌入4D V-NAND结构中的自加密架构，支持多种加密算法，无需外置加密模块，性能优于CPU和近核处理架构。


<details>
  <summary>Details</summary>
Motivation: 解决传统SSD加密需要外置模块的问题，同时满足多样化的加密需求。

Method: 在NAND芯片的闲置硅区域嵌入可重构加密引擎，支持块密码、公钥和后量子算法，并通过RTL实现和P&R评估。

Result: FlashVault性能优于CPU加密（1.46~3.45倍）和近核架构（1.02~2.01倍），满足多种加密标准。

Conclusion: FlashVault是一种高效且安全的SSD架构，适用于多样化的加密需求。

Abstract: We present FlashVault, an in-NAND self-encryption architecture that embeds a
reconfigurable cryptographic engine into the unused silicon area of a
state-of-the-art 4D V-NAND structure. FlashVault supports not only block
ciphers for data encryption but also public-key and post-quantum algorithms for
digital signatures, all within the NAND flash chip. This design enables each
NAND chip to operate as a self-contained enclave without incurring area
overhead, while eliminating the need for off-chip encryption. We implement
FlashVault at the register-transfer level (RTL) and perform place-and-route
(P&R) for accurate power/area evaluation. Our analysis shows that the power
budget determines the number of cryptographic engines per NAND chip. We
integrate this architectural choice into a full-system simulation and evaluate
its performance on a wide range of cryptographic algorithms. Our results show
that FlashVault consistently outperforms both CPU-based encryption (1.46~3.45x)
and near-core processing architecture (1.02~2.01x), demonstrating its
effectiveness as a secure SSD architecture that meets diverse cryptographic
requirements imposed by regulatory standards and enterprise policies.

</details>


### [18] [TROOP: At-the-Roofline Performance for Vector Processors on Low Operational Intensity Workloads](https://arxiv.org/abs/2508.03900)
*Navaneeth Kunhi Purayil,Diyou Shen,Matteo Perotti,Luca Benini*

Main category: cs.AR

TL;DR: TROOP是一组硬件优化技术，旨在提升向量处理单元（VPE）的L1内存带宽利用率，使其接近理论极限（at-the-roofline），适用于数据重用率较低的工作负载。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的VPE在L1内存带宽上受限，仅对数据重用率高的工作负载（如GEMM）高效，而对GEMV等低数据重用工作负载表现不佳。

Method: TROOP通过解耦的加载-存储接口、改进的向量链式操作、影子缓冲区和地址扰乱技术优化VPE微架构。

Result: 在12nm FinFET工艺中实现，TROOP对GEMV、DOTP和AXPY等内存密集型核心分别提速1.5倍、2.2倍和2.6倍，能效提升高达45%。

Conclusion: TROOP在不显著增加面积开销（<7%）的情况下，实现了接近理论极限的性能和能效提升。

Abstract: The fast evolution of Machine Learning (ML) models requires flexible and
efficient hardware solutions as hardwired accelerators face rapid obsolescence.
Vector processors are fully programmable and achieve high energy efficiencies
by exploiting data parallelism, amortizing instruction fetch and decoding
costs. Hence, a promising design choice is to build accelerators based on
shared L1-memory clusters of streamlined Vector Processing Elements (VPEs).
However, current state-of-the-art VPEs are limited in L1 memory bandwidth and
achieve high efficiency only for computational kernels with high data reuse in
the Vector Register File (VRF), such as General Matrix Multiplication (GEMM).
Performance is suboptimal for workloads with lower data reuse like General
Matrix-Vector Multiplication (GEMV). To fully exploit available bandwidth at
the L1 memory interface, the VPE micro-architecture must be optimized to
achieve near-ideal utilization, i.e., to be as close as possible to the L1
memory roofline (at-the-roofline). In this work, we propose TROOP, a set of
hardware optimizations that include decoupled load-store interfaces, improved
vector chaining, shadow buffers to hide VRF conflicts, and address scrambling
techniques to achieve at-the-roofline performance for VPEs without compromising
their area and energy efficiency. We implement TROOP on an open-source
streamlined vector processor in a 12nm FinFET technology. TROOP achieves
significant speedups of 1.5x, 2.2x, and 2.6x, respectively, for key
memory-intensive kernels such as GEMV, DOTP and AXPY, achieving at-the-roofline
performance. Additionally, TROOP enhances the energy efficiency by up to 45%,
reaching 38 DP-GFLOPs/W (1 GHz, TT, 0.8V) for DOTP while maintaining a high
energy efficiency of 61 DP-GFLOPs/W for GEMMs, incurring only a minor area
overhead of less than 7%.

</details>


### [19] [OpenYield: An Open-Source SRAM Yield Analysis and Optimization Benchmark Suite](https://arxiv.org/abs/2508.04106)
*Shan Shen,Xingyang Li,Zhuohua Liu,Yikai Wang,Yiheng Wu,Junhao Ma,Yuquan Sun,Wei W. Xing*

Main category: cs.AR

TL;DR: OpenYield是一个开源生态系统，旨在解决SRAM产量分析中学术模型与工业现实之间的脱节问题，提供真实电路生成器、标准化评估和优化平台。


<details>
  <summary>Details</summary>
Motivation: 解决学术模型与工业实践之间的脱节问题，促进可重复研究和学术界与工业界的合作。

Method: 通过三个核心贡献实现：真实SRAM电路生成器、标准化评估平台和优化平台。

Result: OpenYield为SRAM设计提供了全面的基准测试，增强了设计的鲁棒性和效率。

Conclusion: OpenYield为学术界与工业界的合作奠定了基础，加速了内存设计的创新。

Abstract: Static Random-Access Memory (SRAM) yield analysis is essential for
semiconductor innovation, yet research progress faces a critical challenge: the
significant disconnect between simplified academic models and complex
industrial realities. The absence of open, realistic benchmarks has created a
reproducibility crisis, where promising academic techniques often fail to
translate to industrial practice. We present \textit{OpenYield}, a
comprehensive open-source ecosystem designed to address this critical gap
through three core contributions: (1) A realistic SRAM circuit generator that
uniquely incorporates critical second-order-effect parasitics, inter-cell
leakage coupling, and peripheral circuit variations, which are typically
omitted in academic studies but decisive in industrial designs. (2) A
standardized evaluation platform with a simple interface and implemented
baseline yield analysis algorithms, enabling fair comparisons and reproducible
research. (3) A standardized SRAM optimization platform, demonstrating
OpenYield's utility in enhancing SRAM design robustness and efficiency,
providing a comprehensive benchmark for optimization algorithms. OpenYield
creates a foundation for meaningful academia-industry collaboration,
accelerating innovation in memory design. The framework is publicly available
on \href{https://github.com/ShenShan123/OpenYield}{OpenYield:URL}

</details>


### [20] [ECOLogic: Enabling Circular, Obfuscated, and Adaptive Logic via eFPGA-Augmented SoCs](https://arxiv.org/abs/2508.04516)
*Ishraq Tashdid,Dewan Saiham,Nafisa Anjum,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.AR

TL;DR: ECOLogic是一种混合设计范式，通过在ASIC中嵌入轻量级eFPGA结构，实现高性能、可更新且环保的计算。


<details>
  <summary>Details</summary>
Motivation: 传统硬件平台（ASIC和FPGA）在性能、灵活性和可持续性之间存在权衡。ASIC高效但不灵活，FPGA可重构但资源开销大。ECOLogic旨在结合两者的优势。

Method: 提出ECOLogic架构，嵌入eFPGA结构，并引入ECOScore评分框架，指导RTL分区。

Result: 在六个SoC模块中，ECOLogic平均保留90%的ASIC性能，功耗降低480倍，碳排放减少99.7%。

Conclusion: ECOLogic是一种高性能、安全且环保的可重构系统解决方案。

Abstract: Traditional hardware platforms - ASICs and FPGAs - offer competing trade-offs
among performance, flexibility, and sustainability. ASICs provide high
efficiency but are inflexible post-fabrication, require costly re-spins for
updates, and expose IPs to piracy risks. FPGAs offer reconfigurability and
reuse, yet suffer from substantial area, power, and performance overheads,
resulting in higher carbon footprints. We present ECOLogic, a hybrid design
paradigm that embeds lightweight eFPGA fabric within ASICs to enable secure,
updatable, and resource-aware computation. Central to this architecture is
ECOScore, a quantitative scoring framework that evaluates IPs based on
adaptability, piracy threat, performance tolerance, and resource fit to guide
RTL partitioning. Evaluated across six diverse SoC modules, ECOLogic retains an
average of 90 percent ASIC-level performance (up to 2 GHz), achieves 9.8 ns
timing slack (versus 5.1 ns in FPGA), and reduces power by 480 times on
average. Moreover, sustainability analysis shows a 99.7 percent reduction in
deployment carbon footprint and 300 to 500 times lower emissions relative to
FPGA-only implementations. These results position ECOLogic as a
high-performance, secure, and environmentally sustainable solution for
next-generation reconfigurable systems.

</details>


### [21] [Near instantaneous O(1) Analog Solver Circuit for Linear Symmetric Positive-Definite Systems](https://arxiv.org/abs/2508.04609)
*Osama Abdelaleim,Arun Prakash,Ayhan Irfanoglu,Veljko Milutinovic*

Main category: cs.AR

TL;DR: 本文提出了一种通用模拟直接求解电路，用于加速正定对称线性方程组的求解，利用负电阻电路实现O(1)复杂度。


<details>
  <summary>Details</summary>
Motivation: 线性方程组求解在科学模拟、数据分析和机器学习中至关重要，需要加速解决。

Method: 采用非反相运算放大器配置设计负电阻电路，模拟对称系统，并优化系统架构。

Result: 对于对角占优对称矩阵，实现O(1)复杂度；非对角占优系统则依赖矩阵特性，但与矩阵大小无关。

Conclusion: 该电路设计高效且通用，适用于多种对称正定线性系统的快速求解。

Abstract: Accelerating the solution of linear systems of equations is critical due to
their central role in numerous applications, such as scientific simulations,
data analytics, and machine learning. This paper presents a general-purpose
analog direct solver circuit designed to accelerate the solution of positive
definite symmetric linear systems of equations. The proposed design leverages
non-inverting operational amplifier configurations to create a negative
resistance circuit, effectively modeling any symmetric system. The paper
details the principles behind the design, optimizations of the system
architecture, and numerical results that demonstrate the robustness of the
design. The findings reveal that the proposed system solves diagonally dominant
symmetric matrices with O(1) complexity, achieving the theoretical maximum
speed as the circuit relies solely on resistors. For non-diagonally dominant
symmetric positive-definite systems, the solution speed depends on matrix
properties such as eigenvalues and the maximum off-diagonal term, but remains
independent of matrix size.

</details>
