{"id": "2508.08396", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.08396", "abs": "https://arxiv.org/abs/2508.08396", "authors": ["Fanchen Kong", "Yunhao Deng", "Xiaoling Yi", "Ryan Antonio", "Marian Verhelst"], "title": "XDMA: A Distributed, Extensible DMA Architecture for Layout-Flexible Data Movements in Heterogeneous Multi-Accelerator SoCs", "comment": "4 pages, 6 figures, Proceeded by The 43rd IEEE International\n  Conference on Computer Design (ICCD 2025)", "summary": "As modern AI workloads increasingly rely on heterogeneous accelerators,\nensuring high-bandwidth and layout-flexible data movements between accelerator\nmemories has become a pressing challenge. Direct Memory Access (DMA) engines\npromise high bandwidth utilization for data movements but are typically optimal\nonly for contiguous memory access, thus requiring additional software loops for\ndata layout transformations. This, in turn, leads to excessive control overhead\nand underutilized on-chip interconnects. To overcome this inefficiency, we\npresent XDMA, a distributed and extensible DMA architecture that enables\nlayout-flexible data movements with high link utilization. We introduce three\nkey innovations: (1) a data streaming engine as XDMA Frontend, replacing\nsoftware address generators with hardware ones; (2) a distributed DMA\narchitecture that maximizes link utilization and separates configuration from\ndata transfer; (3) flexible plugins for XDMA enabling on-the-fly data\nmanipulation during data transfers. XDMA demonstrates up to 151.2x/8.2x higher\nlink utilization than software-based implementations in synthetic workloads and\nachieves 2.3x average speedup over accelerators with SoTA DMA in real-world\napplications. Our design incurs <2% area overhead over SoTA DMA solutions while\nconsuming 17% of system power. XDMA proves that co-optimizing memory access,\nlayout transformation, and interconnect protocols is key to unlocking\nheterogeneous multi-accelerator SoC performance.", "AI": {"tldr": "XDMA\u662f\u4e00\u79cd\u5206\u5e03\u5f0f\u53ef\u6269\u5c55DMA\u67b6\u6784\uff0c\u901a\u8fc7\u786c\u4ef6\u4f18\u5316\u5b9e\u73b0\u9ad8\u5e26\u5bbd\u548c\u7075\u6d3b\u5e03\u5c40\u7684\u6570\u636e\u4f20\u8f93\uff0c\u663e\u8457\u63d0\u5347\u5f02\u6784\u52a0\u901f\u5668\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3AI\u5de5\u4f5c\u8d1f\u8f7d\u4f9d\u8d56\u5f02\u6784\u52a0\u901f\u5668\uff0c\u4f46\u73b0\u6709DMA\u5f15\u64ce\u4ec5\u9002\u7528\u4e8e\u8fde\u7eed\u5185\u5b58\u8bbf\u95ee\uff0c\u5bfc\u81f4\u63a7\u5236\u5f00\u9500\u9ad8\u548c\u94fe\u8def\u5229\u7528\u7387\u4f4e\u3002", "method": "\u63d0\u51faXDMA\u67b6\u6784\uff0c\u5305\u542b\u786c\u4ef6\u5730\u5740\u751f\u6210\u5668\u3001\u5206\u5e03\u5f0fDMA\u8bbe\u8ba1\u53ca\u7075\u6d3b\u63d2\u4ef6\uff0c\u4f18\u5316\u6570\u636e\u4f20\u8f93\u548c\u5e03\u5c40\u8f6c\u6362\u3002", "result": "\u5728\u5408\u6210\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u94fe\u8def\u5229\u7528\u7387\u63d0\u5347151.2\u500d\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u901f\u5ea6\u63d0\u53472.3\u500d\uff0c\u9762\u79ef\u5f00\u9500\u4ec52%\uff0c\u529f\u8017\u4e3a17%\u3002", "conclusion": "XDMA\u901a\u8fc7\u534f\u540c\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u3001\u5e03\u5c40\u8f6c\u6362\u548c\u4e92\u8054\u534f\u8bae\uff0c\u663e\u8457\u63d0\u5347\u5f02\u6784\u591a\u52a0\u901f\u5668SoC\u6027\u80fd\u3002"}}
{"id": "2508.08457", "categories": ["cs.AR", "cs.ET", "C.1.3; B.3.1"], "pdf": "https://arxiv.org/pdf/2508.08457", "abs": "https://arxiv.org/abs/2508.08457", "authors": ["Ming-Yen Lee", "Faaiq Waqar", "Hanchen Yang", "Muhammed Ahosan Ul Karim", "Harsono Simka", "Shimeng Yu"], "title": "Architecting Long-Context LLM Acceleration with Packing-Prefetch Scheduler and Ultra-Large Capacity On-Chip Memories", "comment": "7 pages, 8 figures, 2 tables", "summary": "Long-context Large Language Model (LLM) inference faces increasing compute\nbottlenecks as attention calculations scale with context length, primarily due\nto the growing KV-cache transfer overhead that saturates High Bandwidth Memory\n(HBM). While prefetching techniques mitigate cache misses by fetching KV data\nin advance, their spatial and temporal benefits present new opportunities to\nexploit. This work proposes a packing-prefetch scheduling architecture with\nmonolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with\nultra-large on-chip capacity to accelerate long-context LLM inference. Our\noptimizations demonstrate 8.06x decode speedup and 1.83x overall latency\nreduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL\nmemories over the serial execution. Evaluations of multi-request workloads on\nTPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM\nbandwidth reduction compared to packing-only methods on Llama3.1-8B and\nLlama3.1-70B models. With the co-design of packing, prefetching, and BEOL\nmemories, our approach alleviates HBM constraints and enables efficient\nlong-context LLM inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6253\u5305\u9884\u53d6\u8c03\u5ea6\u548c3D\u5d4c\u5165\u5f0f\u5b58\u50a8\u5668\u7684\u67b6\u6784\uff0c\u663e\u8457\u52a0\u901f\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u4e2d\u56e0KV\u7f13\u5b58\u4f20\u8f93\u5bfc\u81f4\u7684HBM\u5e26\u5bbd\u74f6\u9888\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6253\u5305\u9884\u53d6\u8c03\u5ea6\u548c3D\u5d4c\u5165\u5f0f\u5b58\u50a8\u5668\u4f18\u5316KV\u7f13\u5b58\u7ba1\u7406\u3002", "result": "\u5728Llama3.1-8B\u4e0a\u5b9e\u73b08.06\u500d\u89e3\u7801\u52a0\u901f\u548c1.83\u500d\u5ef6\u8fdf\u964d\u4f4e\uff0c\u591a\u8bf7\u6c42\u541e\u5410\u63d0\u53471.7-2.4\u500d\u3002", "conclusion": "\u901a\u8fc7\u6253\u5305\u3001\u9884\u53d6\u548c3D\u5b58\u50a8\u5668\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u7f13\u89e3HBM\u9650\u5236\uff0c\u63d0\u5347\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2508.08503", "categories": ["cs.AR", "cs.DB", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.08503", "abs": "https://arxiv.org/abs/2508.08503", "authors": ["Sabiha Tajdari", "Anastasia Ailamaki", "Sandhya Dwarkadas"], "title": "JSPIM: A Skew-Aware PIM Accelerator for High-Performance Databases Join and Select Operations", "comment": null, "summary": "Database applications are increasingly bottlenecked by memory bandwidth and\nlatency due to the memory wall and the limited scalability of DRAM. Join\nqueries, central to analytical workloads, require intensive memory access and\nare particularly vulnerable to inefficiencies in data movement. While\nProcessing-in-Memory (PIM) offers a promising solution, existing designs\ntypically reuse CPU-oriented join algorithms, limiting parallelism and\nincurring costly inter-chip communication. Additionally, data skew, a main\nchallenge in CPU-based joins, remains unresolved in current PIM architectures.\n  We introduce JSPIM, a PIM module that accelerates hash join and, by\nextension, corresponding select queries through algorithm-hardware co-design.\nJSPIM deploys parallel search engines within each subarray and redesigns hash\ntables to achieve O(1) lookups, fully exploiting PIM's fine-grained\nparallelism. To mitigate skew, our design integrates subarray-level parallelism\nwith rank-level processing, eliminating redundant off-chip transfers.\nEvaluations show JSPIM delivers 400x to 1000x speedup on join queries versus\nDuckDB. When paired with DuckDB for the full SSB benchmark, JSPIM achieves an\noverall 2.5x throughput improvement (individual query gains of 1.1x to 28x), at\njust a 7% data overhead and 2.1% per-rank PIM-enabled chip area increase.", "AI": {"tldr": "JSPIM\u662f\u4e00\u79cd\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u52a0\u901f\u54c8\u5e0c\u8fde\u63a5\u7684PIM\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u5e93\u67e5\u8be2\u6027\u80fd\u3002", "motivation": "\u6570\u636e\u5e93\u5e94\u7528\u53d7\u9650\u4e8e\u5185\u5b58\u5e26\u5bbd\u548c\u5ef6\u8fdf\uff0c\u7279\u522b\u662f\u8fde\u63a5\u67e5\u8be2\u6548\u7387\u4f4e\u4e0b\uff0c\u73b0\u6709PIM\u8bbe\u8ba1\u672a\u80fd\u89e3\u51b3\u6570\u636e\u503e\u659c\u95ee\u9898\u3002", "method": "JSPIM\u5728\u5b50\u9635\u5217\u5185\u90e8\u7f72\u5e76\u884c\u641c\u7d22\u5f15\u64ce\uff0c\u91cd\u65b0\u8bbe\u8ba1\u54c8\u5e0c\u8868\u4ee5\u5b9e\u73b0O(1)\u67e5\u627e\uff0c\u5e76\u5229\u7528\u5b50\u9635\u5217\u7ea7\u5e76\u884c\u6027\u548crank\u7ea7\u5904\u7406\u7f13\u89e3\u6570\u636e\u503e\u659c\u3002", "result": "JSPIM\u5728\u8fde\u63a5\u67e5\u8be2\u4e0a\u6bd4DuckDB\u5feb400x\u81f31000x\uff0cSSB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6574\u4f53\u541e\u5410\u91cf\u63d0\u53472.5\u500d\u3002", "conclusion": "JSPIM\u901a\u8fc7\u9ad8\u6548\u5229\u7528PIM\u7684\u7ec6\u7c92\u5ea6\u5e76\u884c\u6027\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u5e93\u67e5\u8be2\u6027\u80fd\uff0c\u540c\u65f6\u786c\u4ef6\u5f00\u9500\u6781\u5c0f\u3002"}}
{"id": "2508.08822", "categories": ["cs.AR", "cs.AI", "cs.ET", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.08822", "abs": "https://arxiv.org/abs/2508.08822", "authors": ["Shady Agwa", "Yihan Pan", "Georgios Papandroulidakis", "Themis Prodromakis"], "title": "OISMA: On-the-fly In-memory Stochastic Multiplication Architecture for Matrix-Multiplication Workloads", "comment": "12 pages, 13 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Artificial Intelligence models are currently driven by a significant\nup-scaling of their complexity, with massive matrix multiplication workloads\nrepresenting the major computational bottleneck. In-memory computing\narchitectures are proposed to avoid the Von Neumann bottleneck. However, both\ndigital/binary-based and analogue in-memory computing architectures suffer from\nvarious limitations, which significantly degrade the performance and energy\nefficiency gains. This work proposes OISMA, a novel in-memory computing\narchitecture that utilizes the computational simplicity of a quasi-stochastic\ncomputing domain (Bent-Pyramid system), while keeping the same efficiency,\nscalability, and productivity of digital memories. OISMA converts normal memory\nread operations into in-situ stochastic multiplication operations with a\nnegligible cost. An accumulation periphery then accumulates the output\nmultiplication bitstreams, achieving the matrix multiplication functionality.\nExtensive matrix multiplication benchmarking was conducted to analyze the\naccuracy of the Bent-Pyramid system, using matrix dimensions ranging from 4x4\nto 512x512. The accuracy results show a significant decrease in the average\nrelative Frobenius error, from 9.42% (for 4x4) to 1.81% (for 512x512), compared\nto 64-bit double precision floating-point format. A 1T1R OISMA array of 4 KB\ncapacity was implemented using a commercial 180nm technology node and in-house\nRRAM technology. At 50 MHz, OISMA achieves 0.891 TOPS/W and 3.98 GOPS/mm2 for\nenergy and area efficiency, respectively, occupying an effective computing area\nof 0.804241 mm2. Scaling OISMA from 180nm to 22nm technology shows a\nsignificant improvement of two orders of magnitude in energy efficiency and one\norder of magnitude in area efficiency, compared to dense matrix multiplication\nin-memory computing architectures.", "AI": {"tldr": "OISMA\u662f\u4e00\u79cd\u65b0\u578b\u5185\u5b58\u8ba1\u7b97\u67b6\u6784\uff0c\u5229\u7528\u51c6\u968f\u673a\u8ba1\u7b97\u57df\uff08Bent-Pyramid\u7cfb\u7edf\uff09\u7b80\u5316\u8ba1\u7b97\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u5b57\u5b58\u50a8\u7684\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e9\u9635\u4e58\u6cd5\u7684\u51c6\u786e\u6027\u548c\u80fd\u6548\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u7684\u8ba1\u7b97\u74f6\u9888\u5728\u4e8e\u5927\u89c4\u6a21\u77e9\u9635\u4e58\u6cd5\uff0c\u800c\u73b0\u6709\u5185\u5b58\u8ba1\u7b97\u67b6\u6784\u5b58\u5728\u6027\u80fd\u9650\u5236\u3002OISMA\u65e8\u5728\u901a\u8fc7\u51c6\u968f\u673a\u8ba1\u7b97\u57df\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "OISMA\u5c06\u666e\u901a\u5185\u5b58\u8bfb\u53d6\u64cd\u4f5c\u8f6c\u6362\u4e3a\u539f\u4f4d\u968f\u673a\u4e58\u6cd5\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7\u7d2f\u52a0\u5916\u56f4\u7535\u8def\u5b9e\u73b0\u77e9\u9635\u4e58\u6cd5\u529f\u80fd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cOISMA\u57284x4\u5230512x512\u77e9\u9635\u4e0a\u7684\u5e73\u5747\u76f8\u5bf9Frobenius\u8bef\u5dee\u663e\u8457\u964d\u4f4e\uff0c\u80fd\u6548\u548c\u9762\u79ef\u6548\u7387\u5206\u522b\u4e3a0.891 TOPS/W\u548c3.98 GOPS/mm\u00b2\u3002", "conclusion": "OISMA\u5728\u80fd\u6548\u548c\u9762\u79ef\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u8ba1\u7b97\u67b6\u6784\uff0c\u672a\u6765\u6280\u672f\u8282\u70b9\u7f29\u653e\u5c06\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u6027\u80fd\u3002"}}
{"id": "2508.08430", "categories": ["cs.DC", "cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.08430", "abs": "https://arxiv.org/abs/2508.08430", "authors": ["Abhinaba Chakraborty", "Wouter Tavernier", "Akis Kourtis", "Mario Pickavet", "Andreas Oikonomakis", "Didier Colle"], "title": "Profiling Concurrent Vision Inference Workloads on NVIDIA Jetson -- Extended", "comment": null, "summary": "The proliferation of IoT devices and advancements in network technologies\nhave intensified the demand for real-time data processing at the network edge.\nTo address these demands, low-power AI accelerators, particularly GPUs, are\nincreasingly deployed for inference tasks, enabling efficient computation while\nmitigating cloud-based systems' latency and bandwidth limitations. Despite\ntheir growing deployment, GPUs remain underutilised even in computationally\nintensive workloads. This underutilisation stems from the limited understanding\nof GPU resource sharing, particularly in edge computing scenarios. In this\nwork, we conduct a detailed analysis of both high- and low-level metrics,\nincluding GPU utilisation, memory usage, streaming multiprocessor (SM)\nutilisation, and tensor core usage, to identify bottlenecks and guide\nhardware-aware optimisations. By integrating traces from multiple profiling\ntools, we provide a comprehensive view of resource behaviour on NVIDIA Jetson\nedge devices under concurrent vision inference workloads. Our findings indicate\nthat while GPU utilisation can reach $100\\%$ under specific optimisations,\ncritical low-level resources, such as SMs and tensor cores, often operate only\nat $15\\%$ to $30\\%$ utilisation. Moreover, we observe that certain CPU-side\nevents, such as thread scheduling, context switching, etc., frequently emerge\nas bottlenecks, further constraining overall GPU performance. We provide\nseveral key observations for users of vision inference workloads on NVIDIA edge\ndevices.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u8fb9\u7f18\u8ba1\u7b97\u4e2dGPU\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5de5\u5177\u8ffd\u8e2a\u63ed\u793a\u4e86\u4f4e\u5c42\u6b21\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u548cCPU\u4e8b\u4ef6\u6210\u4e3a\u74f6\u9888\u7684\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u5efa\u8bae\u3002", "motivation": "\u968f\u7740IoT\u8bbe\u5907\u548c\u7f51\u7edc\u6280\u672f\u7684\u53d1\u5c55\uff0c\u8fb9\u7f18\u8ba1\u7b97\u5bf9\u5b9e\u65f6\u6570\u636e\u5904\u7406\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46GPU\u5728\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5bf9\u8d44\u6e90\u5171\u4eab\u7684\u7406\u89e3\u6709\u9650\u3002", "method": "\u901a\u8fc7\u5206\u6790\u9ad8\u3001\u4f4e\u5c42\u6b21\u6307\u6807\uff08\u5982GPU\u5229\u7528\u7387\u3001\u5185\u5b58\u4f7f\u7528\u3001SM\u548cTensor\u6838\u5fc3\u5229\u7528\u7387\uff09\uff0c\u7ed3\u5408\u591a\u5de5\u5177\u8ffd\u8e2a\uff0c\u5168\u9762\u8bc4\u4f30NVIDIA Jetson\u8bbe\u5907\u5728\u5e76\u53d1\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8d44\u6e90\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1GPU\u5229\u7528\u7387\u53ef\u8fbe100%\uff0c\u4f46SM\u548cTensor\u6838\u5fc3\u5229\u7528\u7387\u4ec5\u4e3a15%-30%\uff0c\u4e14CPU\u4e8b\u4ef6\uff08\u5982\u7ebf\u7a0b\u8c03\u5ea6\uff09\u5e38\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002", "conclusion": "\u8bba\u6587\u4e3aNVIDIA\u8fb9\u7f18\u8bbe\u5907\u7684\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u7528\u6237\u63d0\u4f9b\u4e86\u5173\u952e\u89c2\u5bdf\u7ed3\u679c\uff0c\u6307\u5bfc\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u3002"}}
{"id": "2508.08479", "categories": ["cs.DC", "cs.LG", "14J60", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.08479", "abs": "https://arxiv.org/abs/2508.08479", "authors": ["Yuvraj Dutta", "Soumyajit Chatterjee", "Sandip Chakraborty", "Basabdatta Palit"], "title": "Benchmarking Federated Learning for Throughput Prediction in 5G Live Streaming Applications", "comment": "14 pages, 24 figures, submitted to IEEE TNET", "summary": "Accurate and adaptive network throughput prediction is essential for\nlatency-sensitive and bandwidth-intensive applications in 5G and emerging 6G\nnetworks. However, most existing methods rely on centralized training with\nuniformly collected data, limiting their applicability in heterogeneous mobile\nenvironments with non-IID data distributions. This paper presents the first\ncomprehensive benchmarking of federated learning (FL) strategies for throughput\nprediction in realistic 5G edge scenarios. We evaluate three aggregation\nalgorithms - FedAvg, FedProx, and FedBN - across four time-series\narchitectures: LSTM, CNN, CNN+LSTM, and Transformer, using five diverse\nreal-world datasets. We systematically analyze the effects of client\nheterogeneity, cohort size, and history window length on prediction\nperformance. Our results reveal key trade-offs among model complexities,\nconvergence rates, and generalization. It is found that FedBN consistently\ndelivers robust performance under non-IID conditions. On the other hand, LSTM\nand Transformer models outperform CNN-based baselines by up to 80% in R2\nscores. Moreover, although Transformers converge in half the rounds of LSTM,\nthey require longer history windows to achieve a high R2, indicating higher\ncontext dependence. LSTM is, therefore, found to achieve a favorable balance\nbetween accuracy, rounds, and temporal footprint. To validate the end-to-end\napplicability of the framework, we have integrated our FL-based predictors into\na live adaptive streaming pipeline. It is seen that FedBN-based LSTM and\nTransformer models improve mean QoE scores by 11.7% and 11.4%, respectively,\nover FedAvg, while also reducing the variance. These findings offer actionable\ninsights for building scalable, privacy-preserving, and edge-aware throughput\nprediction systems in next-generation wireless networks.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7b56\u7565\u57285G\u8fb9\u7f18\u573a\u666f\u4e2d\u7684\u541e\u5410\u91cf\u9884\u6d4b\u6027\u80fd\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u805a\u5408\u7b97\u6cd5\u548c\u56db\u79cd\u65f6\u95f4\u5e8f\u5217\u67b6\u6784\uff0c\u53d1\u73b0FedBN\u5728\u975eIID\u6570\u636e\u4e0b\u8868\u73b0\u7a33\u5065\uff0cLSTM\u548cTransformer\u6a21\u578b\u4f18\u4e8eCNN\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b35G\u548c6G\u7f51\u7edc\u4e2d\u5f02\u6784\u79fb\u52a8\u73af\u5883\u4e0b\u975eIID\u6570\u636e\u5206\u5e03\u5bf9\u541e\u5410\u91cf\u9884\u6d4b\u7684\u9650\u5236\uff0c\u63a2\u7d22\u8054\u90a6\u5b66\u4e60\u7684\u9002\u7528\u6027\u3002", "method": "\u8bc4\u4f30\u4e86FedAvg\u3001FedProx\u548cFedBN\u4e09\u79cd\u805a\u5408\u7b97\u6cd5\uff0c\u4ee5\u53caLSTM\u3001CNN\u3001CNN+LSTM\u548cTransformer\u56db\u79cd\u65f6\u95f4\u5e8f\u5217\u67b6\u6784\uff0c\u4f7f\u7528\u4e94\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u5206\u6790\u6027\u80fd\u3002", "result": "FedBN\u5728\u975eIID\u6761\u4ef6\u4e0b\u8868\u73b0\u7a33\u5065\uff0cLSTM\u548cTransformer\u6a21\u578b\u5728R2\u5206\u6570\u4e0a\u6bd4CNN\u57fa\u7ebf\u9ad880%\uff0cTransformer\u6536\u655b\u66f4\u5feb\u4f46\u9700\u8981\u66f4\u957f\u5386\u53f2\u7a97\u53e3\u3002", "conclusion": "FedBN\u7ed3\u5408LSTM\u6216Transformer\u6a21\u578b\u5728\u541e\u5410\u91cf\u9884\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08525", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.08525", "abs": "https://arxiv.org/abs/2508.08525", "authors": ["Xiaopei Zhang", "Xingang Wang", "Xin Wang"], "title": "A Reinforcement Learning-Driven Task Scheduling Algorithm for Multi-Tenant Distributed Systems", "comment": null, "summary": "This paper addresses key challenges in task scheduling for multi-tenant\ndistributed systems, including dynamic resource variation, heterogeneous tenant\ndemands, and fairness assurance. An adaptive scheduling method based on\nreinforcement learning is proposed. By modeling the scheduling process as a\nMarkov decision process, the study defines the state space, action space, and\nreward function. A scheduling policy learning framework is designed using\nProximal Policy Optimization (PPO) as the core algorithm. This enables dynamic\nperception of complex system states and real-time decision-making. Under a\nmulti-objective reward mechanism, the scheduler jointly optimizes task latency,\nresource utilization, and tenant fairness. The coordination between the policy\nnetwork and the value network continuously refines the scheduling strategy.\nThis enhances overall system performance. To validate the effectiveness of the\nproposed method, a series of experiments were conducted in multi-scenario\nenvironments built using a real-world public dataset. The experiments evaluated\ntask latency control, resource efficiency, policy stability, and fairness. The\nresults show that the proposed method outperforms existing scheduling\napproaches across multiple evaluation metrics. It demonstrates strong stability\nand generalization ability. The proposed scheduling framework provides\npractical and engineering value in policy design, dynamic resource modeling,\nand multi-tenant service assurance. It effectively improves scheduling\nefficiency and resource management in distributed systems under complex\nconditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u4efb\u52a1\u8c03\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u79df\u6237\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u52a8\u6001\u8d44\u6e90\u53d8\u5316\u3001\u5f02\u6784\u79df\u6237\u9700\u6c42\u548c\u516c\u5e73\u6027\u4fdd\u969c\u95ee\u9898\u3002", "motivation": "\u591a\u79df\u6237\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u4efb\u52a1\u8c03\u5ea6\u9762\u4e34\u52a8\u6001\u8d44\u6e90\u53d8\u5316\u3001\u5f02\u6784\u79df\u6237\u9700\u6c42\u548c\u516c\u5e73\u6027\u4fdd\u969c\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5c06\u8c03\u5ea6\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5b9a\u4e49\u4e86\u72b6\u6001\u7a7a\u95f4\u3001\u52a8\u4f5c\u7a7a\u95f4\u548c\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u91c7\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7b97\u6cd5\u8bbe\u8ba1\u8c03\u5ea6\u7b56\u7565\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u5ef6\u8fdf\u63a7\u5236\u3001\u8d44\u6e90\u6548\u7387\u3001\u7b56\u7565\u7a33\u5b9a\u6027\u548c\u516c\u5e73\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u8c03\u5ea6\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u8c03\u5ea6\u6846\u67b6\u5728\u7b56\u7565\u8bbe\u8ba1\u3001\u52a8\u6001\u8d44\u6e90\u5efa\u6a21\u548c\u591a\u79df\u6237\u670d\u52a1\u4fdd\u969c\u65b9\u9762\u5177\u6709\u5b9e\u7528\u548c\u5de5\u7a0b\u4ef7\u503c\uff0c\u80fd\u6709\u6548\u63d0\u5347\u590d\u6742\u6761\u4ef6\u4e0b\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u8c03\u5ea6\u6548\u7387\u548c\u8d44\u6e90\u7ba1\u7406\u3002"}}
{"id": "2508.09035", "categories": ["cs.DC", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09035", "abs": "https://arxiv.org/abs/2508.09035", "authors": ["Yibo Jin", "Yixu Xu", "Yue Chen", "Chengbin Wang", "Tao Wang", "Jiaqi Huang", "Rongfei Zhang", "Yiming Dong", "Yuting Yan", "Ke Cheng", "Yingjie Zhu", "Shulan Wang", "Qianqian Tang", "Shuaishuai Meng", "Guanxin Cheng", "Ze Wang", "Shuyan Miao", "Ketao Wang", "Wen Liu", "Yifan Yang", "Tong Zhang", "Anran Wang", "Chengzhou Lu", "Tiantian Dong", "Yongsheng Zhang", "Zhe Wang", "Hefei Guo", "Hongjie Liu", "Wei Lu", "Zhengyong Zhang"], "title": "P/D-Device: Disaggregated Large Language Model between Cloud and Devices", "comment": null, "summary": "Serving disaggregated large language models has been widely adopted in\nindustrial practice for enhanced performance. However, too many tokens\ngenerated in decoding phase, i.e., occupying the resources for a long time,\nessentially hamper the cloud from achieving a higher throughput. Meanwhile, due\nto limited on-device resources, the time to first token (TTFT), i.e., the\nlatency of prefill phase, increases dramatically with the growth on prompt\nlength. In order to concur with such a bottleneck on resources, i.e., long\noccupation in cloud and limited on-device computing capacity, we propose to\nseparate large language model between cloud and devices. That is, the cloud\nhelps a portion of the content for each device, only in its prefill phase.\nSpecifically, after receiving the first token from the cloud, decoupling with\nits own prefill, the device responds to the user immediately for a lower TTFT.\nThen, the following tokens from cloud are presented via a speed controller for\nsmoothed TPOT (the time per output token), until the device catches up with the\nprogress. On-device prefill is then amortized using received tokens while the\nresource usage in cloud is controlled. Moreover, during cloud prefill, the\nprompt can be refined, using those intermediate data already generated, to\nfurther speed up on-device inference. We implement such a scheme P/D-Device,\nand confirm its superiority over other alternatives. We further propose an\nalgorithm to decide the best settings. Real-trace experiments show that TTFT\ndecreases at least 60%, maximum TPOT is about tens of milliseconds, and cloud\nthroughput increases by up to 15x.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u79bb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e91\u7aef\u548c\u8bbe\u5907\u7aef\u7684\u65b9\u6cd5\uff08P/D-Device\uff09\uff0c\u4ee5\u89e3\u51b3\u4e91\u7aef\u8d44\u6e90\u5360\u7528\u548c\u8bbe\u5907\u7aef\u8ba1\u7b97\u80fd\u529b\u9650\u5236\u7684\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9996\u6b21\u54cd\u5e94\u65f6\u95f4\uff08TTFT\uff09\u5e76\u63d0\u9ad8\u4e86\u4e91\u7aef\u541e\u5410\u91cf\u3002", "motivation": "\u5de5\u4e1a\u5b9e\u8df5\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u9636\u6bb5\u751f\u6210\u8fc7\u591a\u4ee4\u724c\u4f1a\u957f\u671f\u5360\u7528\u4e91\u7aef\u8d44\u6e90\uff0c\u964d\u4f4e\u541e\u5410\u91cf\uff1b\u540c\u65f6\uff0c\u8bbe\u5907\u7aef\u8d44\u6e90\u6709\u9650\uff0c\u63d0\u793a\u957f\u5ea6\u589e\u52a0\u4f1a\u5bfc\u81f4\u9996\u6b21\u54cd\u5e94\u65f6\u95f4\uff08TTFT\uff09\u663e\u8457\u4e0a\u5347\u3002", "method": "\u901a\u8fc7\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u79bb\u5230\u4e91\u7aef\u548c\u8bbe\u5907\u7aef\uff0c\u4e91\u7aef\u4ec5\u5728\u9884\u586b\u5145\u9636\u6bb5\u8f85\u52a9\u8bbe\u5907\u7aef\u3002\u8bbe\u5907\u7aef\u5728\u63a5\u6536\u5230\u4e91\u7aef\u9996\u4e2a\u4ee4\u724c\u540e\u7acb\u5373\u54cd\u5e94\u7528\u6237\uff0c\u964d\u4f4eTTFT\uff1b\u540e\u7eed\u4ee4\u724c\u901a\u8fc7\u901f\u5ea6\u63a7\u5236\u5668\u5e73\u6ed1\u8f93\u51fa\uff0c\u8bbe\u5907\u7aef\u9010\u6b65\u8ffd\u8d76\u8fdb\u5ea6\u3002\u4e91\u7aef\u9884\u586b\u5145\u671f\u95f4\u8fd8\u53ef\u4f18\u5316\u63d0\u793a\u4ee5\u52a0\u901f\u8bbe\u5907\u7aef\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cTTFT\u964d\u4f4e\u81f3\u5c1160%\uff0c\u6700\u5927TPOT\u4e3a\u51e0\u5341\u6beb\u79d2\uff0c\u4e91\u7aef\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe15\u500d\u3002", "conclusion": "P/D-Device\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u8d44\u6e90\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
