<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A Case Study on the Effectiveness of LLMs in Verification with Proof Assistants](https://arxiv.org/abs/2508.18587)
*Barış Bayazıt,Yao Li,Xujie Si*

Main category: cs.PL

TL;DR: 这篇论文通过对Rocq项目的案例研究，评估了大语言模型在证明助手中自动生成证明的效果，发现外部依赖和上下文对证明生成有显著帮助，LLMs在小规模证明上表现优异但也会出现奇怪错误


<details>
  <summary>Details</summary>
Motivation: 识别大语言模型在证明助手中自动化证明生成的实际效果和局限性，以确定其在形式化验证中的应用价值

Method: 基于两个成熟的Rocq项目(hs-to-coq工具和Verdi)进行案例研究，通过定量和定性分析来评估LLMs生成证明的效果

Result: 研究发现：(1)外部依赖和同源文件中的上下文能显著帮助证明生成；(2)LLMs在小规模证明上表现优异但也能生成大规模证明；(3)不同验证项目中LLMs表现不同；(4)LLMs能生成简洁智能的证明，将经典技术应用于新定义，但也会出现奇怪错误

Conclusion: 大语言模型在证明助手中具有自动化证明生成的潜力，但需要考虑上下文依赖和项目特定因素，其能力在不同规模和类型的证明中存在差异

Abstract: Large language models (LLMs) can potentially help with verification using
proof assistants by automating proofs. However, it is unclear how effective
LLMs are in this task. In this paper, we perform a case study based on two
mature Rocq projects: the hs-to-coq tool and Verdi. We evaluate the
effectiveness of LLMs in generating proofs by both quantitative and qualitative
analysis. Our study finds that: (1) external dependencies and context in the
same source file can significantly help proof generation; (2) LLMs perform
great on small proofs but can also generate large proofs; (3) LLMs perform
differently on different verification projects; and (4) LLMs can generate
concise and smart proofs, apply classical techniques to new definitions, but
can also make odd mistakes.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Experiences with Model Context Protocol Servers for Science and High Performance Computing](https://arxiv.org/abs/2508.18489)
*Haochen Pan,Ryan Chard,Reid Mello,Christopher Grams,Tanjin He,Alexander Brace,Owen Price Skelly,Will Engler,Hayden Holbrook,Song Young Oh,Maxime Gonthier,Michael Papka,Ben Blaiszik,Kyle Chard,Ian Foster*

Main category: cs.DC

TL;DR: 这篇论文探讨了使用Model Context Protocol (MCP)作为统一接口来解决大语言模型代理在科学工作流中遇到的异构API和安全障碍问题。


<details>
  <summary>Details</summary>
Motivation: 研究计算基础设施提供的异质API和安全模型经常阻碍大语言模型代理进行科学工作流的规划和执行，需要一种统一的接口来实现资源的发现、调用和组合。

Method: 采用Model Context Protocol (MCP)作为统一接口，在成熟服务（如Globus Transfer、Compute、Search，计算设施状态API，Octopus事件架构，以及Garden、Galaxy等领域特定工具）上实现薄层MCP服务器。

Result: 通过计算化学、生物信息学、量子化学和文件系统监控等案例研究，证明了MCP导向架构在实践中的可行性和效果。

Conclusion: 论文总结了使用MCP接口的经验教训，并提出了在代理导向科学中面临的评估和信任问题等开放性挑战。

Abstract: Large language model (LLM)-powered agents are increasingly used to plan and
execute scientific workflows, yet most research cyberinfrastructure (CI)
exposes heterogeneous APIs and implements security models that present barriers
for use by agents. We report on our experience using the Model Context Protocol
(MCP) as a unifying interface that makes research capabilities discoverable,
invokable, and composable. Our approach is pragmatic: we implement thin MCP
servers over mature services, including Globus Transfer, Compute, and Search;
status APIs exposed by computing facilities; Octopus event fabric; and
domain-specific tools such as Garden and Galaxy. We use case studies in
computational chemistry, bioinformatics, quantum chemistry, and filesystem
monitoring to illustrate how this MCP-oriented architecture can be used in
practice. We distill lessons learned and outline open challenges in evaluation
and trust for agent-led science.

</details>


### [3] [Managing Multi Instance GPUs for High Throughput and Energy Savings](https://arxiv.org/abs/2508.18556)
*Abhijeet Saraha,Yuanbo Li,Chris Porter,Santosh Pande*

Main category: cs.DC

TL;DR: 本文针对现代GPU（A100等）开发了分区和调度方案，通过动态内存估计、分区融合与分裂等技术，显著提升了科学计算和机器学习工作负载的吞吐量和能效。


<details>
  <summary>Details</summary>
Motivation: 现代GPU虽然提供性能和安全隔离功能，但由于芯片分区复杂约束，充分利用其并发性具有挑战性。需要开发有效的分区调度方案来优化各类工作负载的执行效率。

Method: 开发了多种分区调度方案，包括动态内存估计、分区融合、分区分裂等技术，并支持进程重启以处理内存不足错误，以及早期重启优化。

Result: 通用工作负载获得6.20倍吞吐量提升和5.93倍能效改进；ML工作负载在A100 GPU上获得1.59倍吞吐量和1.12倍能效提升；LLM工作负载获得1.43倍吞吐量提升和1.11倍节能。

Conclusion: 提出的GPU分区调度方案能有效提升各类工作负载的性能和能效，特别是在处理复杂约束条件下实现更好的资源利用率。

Abstract: Modern GPUs such as the Ampere series (A30, A100) as well as the Hopper
series (H100, H200) offer performance as well as security isolation features.
They also support a good amount of concurrency, but taking advantage of it can
be quite challenging due to the complex constraints on partitioning the chip.
  In this work, we develop partitioning and scheduling schemes for a variety of
workloads, ranging from scientific to modern ML workloads, including LLMs. We
develop several schemes involving dynamic memory estimation, partition fusion
and partition fission. We also support process restart to recover from
out-of-memory errors for workloads and early restart as an optimization. This
approach yields up to 6.20x throughput and 5.93x energy improvements for
general workloads; and we see 1.59x and 1.12x improvement to throughput and
energy, respectively, for ML workloads on an A100 GPU. We leverage this
technique on LLM workloads and show good improvements, including up to 1.43x
throughput improvement and 1.11x energy savings.

</details>


### [4] [Strata: Hierarchical Context Caching for Long Context Language Model Serving](https://arxiv.org/abs/2508.18572)
*Zhiqiang Xie,Ziyi Xu,Mark Zhao,Yuwei An,Vikram Sharma Mailthody,Scott Mahlke,Michael Garland,Christos Kozyrakis*

Main category: cs.DC

TL;DR: Strata是一个分层上下文缓存框架，通过GPU辅助I/O和缓存感知调度，解决了长上下文LLM服务中的KV缓存存储和传输瓶颈问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM上下文窗口不断扩大，KV缓存的存储占用迅速超过GPU内存容量，导致生产系统需要采用分层缓存。但传输大缓存上下文回GPU时存在严重的性能瓶颈：分页布局导致碎片化I/O无法充分利用带宽，现有调度器未考虑缓存加载延迟。

Method: Strata引入GPU辅助I/O来对抗KV缓存碎片化，解耦GPU和CPU内存布局，并采用缓存感知请求调度来平衡计算与I/O延迟，将不可避免的停顿与补充任务重叠执行。

Result: 基于SGLang构建并在生产中部署，Strata在长上下文基准测试中相比vLLM + LMCache实现了5倍更低的首次令牌时间(TTFT)，比NVIDIA TensorRT-LLM快3.75倍，且不降低短上下文性能。

Conclusion: Strata框架有效解决了长上下文LLM服务中的缓存管理挑战，通过创新的I/O优化和调度策略显著提升了系统性能，为大规模LLM部署提供了可行的解决方案。

Abstract: Large Language Models (LLMs) with expanding context windows face significant
performance hurdles. While caching key-value (KV) states is critical for
avoiding redundant computation, the storage footprint of long-context caches
quickly exceeds GPU memory capacity, forcing production systems to adopt
hierarchical caching across memory hierarchies. However, transferring large
cached contexts back to the GPU introduces severe performance bottlenecks:
fragmented I/O from paged layouts prevents full bandwidth utilization, and
existing schedulers fail to account for cache-loading delays, leaving systems
loading-bound rather than compute-bound. We present Strata, a hierarchical
context caching framework designed for efficient long context LLM serving.
Strata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling
GPU and CPU memory layouts and employs cache-aware request scheduling to
balance compute with I/O latency and overlapping unavoidable stalls with
complementary tasks. Built on SGLang and deployed in production, Strata
achieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache
and 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without
degrading short-context performance.

</details>


### [5] [Examining MPI and its Extensions for Asynchronous Multithreaded Communication](https://arxiv.org/abs/2508.18667)
*Jiakun Yan,Marc Snir,Yanfei Guo*

Main category: cs.DC

TL;DR: 本文评估了MPI的两个扩展（VCI和Continuation）在AMT运行时HPX中的性能表现，发现虽然能提升性能但仍需改进，特别是在多VCI设置下的多线程消息速率和单VCI每线程模式的有效性方面。


<details>
  <summary>Details</summary>
Motivation: 随着HPC架构复杂化和不规则科学算法的普及，需要高效支持异步多线程通信，特别是对于异步多任务(AMT)系统，而原始MPI规范并未考虑这种通信模式。

Method: 使用基于HPX低级通信机制的MPI级微基准测试测量扩展的峰值性能潜力，然后将这些扩展集成到HPX中以评估实际场景中的有效性。

Result: 结果显示这些扩展相比标准MPI能提升性能，但当前continuation提案限制了多VCI设置中的最大多线程消息速率，且推荐的每线程单VCI模式因注意力问题在实际系统中效果不佳。

Conclusion: 需要提高VCI内部线程效率以实现可扩展的多线程通信，并充分发挥近期MPI扩展的效益。

Abstract: The increasing complexity of HPC architectures and the growing adoption of
irregular scientific algorithms demand efficient support for asynchronous,
multithreaded communication. This need is especially pronounced with
Asynchronous Many-Task (AMT) systems. This communication pattern was not a
consideration during the design of the original MPI specification. The MPI
community has recently introduced several extensions to address these evolving
requirements. This work evaluates two such extensions, the Virtual
Communication Interface (VCI) and the Continuation extensions, in the context
of an established AMT runtime HPX. We begin by using an MPI-level
microbenchmark, modeled from HPX's low-level communication mechanism, to
measure the peak performance potential of these extensions. We then integrate
them into HPX to evaluate their effectiveness in real-world scenarios. Our
results show that while these extensions can enhance performance compared to
standard MPI, areas for improvement remain. The current continuation proposal
limits the maximum multithreaded message rate achievable in the multi-VCI
setting. Furthermore, the recommended one-VCI-per-thread mode proves
ineffective in real-world systems due to the attentiveness problem. These
findings underscore the importance of improving intra-VCI threading efficiency
to achieve scalable multithreaded communication and fully realize the benefits
of recent MPI extensions.

</details>


### [6] [ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive](https://arxiv.org/abs/2508.18850)
*Xinhao Luo,Zihan Liu,Yangjie Zhou,Shihan Fang,Ziyu Huang,Yu Feng,Chen Zhang,Shixuan Sun,Zhenzhe Zheng,Jingwen Leng,Minyi Guo*

Main category: cs.DC

TL;DR: ClusterFusion通过引入集群级通信原语ClusterReduce和ClusterGather，将LLM解码中的多个阶段融合为单个内核，显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: LLM解码存在高延迟问题，主要原因是算子执行碎片化和对片外内存的重度依赖，限制了融合机会并产生大量内存流量和内核启动开销。

Method: 设计ClusterReduce和ClusterGather两个集群级通信原语，构建ClusterFusion执行框架，将QKV投影、注意力机制和输出投影等解码阶段融合为单个内核。

Result: 在H100 GPU上评估显示，ClusterFusion相比最先进推理框架平均端到端延迟提升1.61倍。

Conclusion: 通过结构化集群通信抽象和联合调度通信计算，ClusterFusion有效扩展了算子融合范围，显著提升了LLM解码性能。

Abstract: Large language model (LLM) decoding suffers from high latency due to
fragmented execution across operators and heavy reliance on off-chip memory for
data exchange and reduction. This execution model limits opportunities for
fusion and incurs significant memory traffic and kernel launch overhead. While
modern architectures such as NVIDIA Hopper provide distributed shared memory
and low-latency intra-cluster interconnects, they expose only low-level data
movement instructions, lacking structured abstractions for collective on-chip
communication. To bridge this software-hardware gap, we introduce two
cluster-level communication primitives, ClusterReduce and ClusterGather, which
abstract common communication patterns and enable structured, high-speed data
exchange and reduction between thread blocks within a cluster, allowing
intermediate results to be on-chip without involving off-chip memory. Building
on these abstractions, we design ClusterFusion, an execution framework that
schedules communication and computation jointly to expand operator fusion scope
by composing decoding stages such as QKV Projection, Attention, and Output
Projection into a single fused kernels. Evaluations on H100 GPUs show that
ClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on
average in end-to-end latency across different models and configurations. The
source code is available at https://github.com/xinhao-luo/ClusterFusion.

</details>


### [7] [SIREN: Software Identification and Recognition in HPC Systems](https://arxiv.org/abs/2508.18950)
*Thomas Jakobsche,Fredrik Robertsén,Jessica R. Jones,Utz-Uwe Haus,Florina M. Ciorba*

Main category: cs.DC

TL;DR: SIREN是一个HPC系统进程级数据收集框架，通过模糊哈希技术实现软件识别和重复执行检测，解决了传统基于作业/文件名方法不可靠的问题


<details>
  <summary>Details</summary>
Motivation: HPC系统需要应用特定的洞察来分析日益复杂和多样化的工作负载，传统基于作业或文件名的识别方法对用户自定义名称不可靠

Method: 开发SIREN框架，收集进程元数据、环境信息和可执行文件模糊哈希，通过模糊哈希技术检测相似性而不受版本或编译方式变化影响

Result: 在LUMI系统上的首次部署显示，SIREN能够提供软件使用洞察、识别已知应用的重复执行，以及基于相似性识别未知应用

Conclusion: SIREN框架提高了HPC系统的可观测性，为系统优化和安全改进提供了有效工具，克服了传统识别方法的局限性

Abstract: HPC systems use monitoring and operational data analytics to ensure
efficiency, performance, and orderly operations. Application-specific insights
are crucial for analyzing the increasing complexity and diversity of HPC
workloads, particularly through the identification of unknown software and
recognition of repeated executions, which facilitate system optimization and
security improvements. However, traditional identification methods using job or
file names are unreliable for arbitrary user-provided names (a.out). Fuzzy
hashing of executables detects similarities despite changes in executable
version or compilation approach while preserving privacy and file integrity,
overcoming these limitations. We introduce SIREN, a process-level data
collection framework for software identification and recognition. SIREN
improves observability in HPC by enabling analysis of process metadata,
environment information, and executable fuzzy hashes. Findings from a first
opt-in deployment campaign on LUMI show SIREN's ability to provide insights
into software usage, recognition of repeated executions of known applications,
and similarity-based identification of unknown applications.

</details>


### [8] [Deep Learning-Enabled Supercritical Flame Simulation at Detailed Chemistry and Real-Fluid Accuracy Towards Trillion-Cell Scale](https://arxiv.org/abs/2508.18969)
*Zhuoqiang Guo,Runze Mao,Lijun Liu,Guangming Tan,Weile Jia,Zhi X. Chen*

Main category: cs.DC

TL;DR: 优化DeepFlame超临界火焰模拟软件，在神威和富岳超算上实现6180亿和1540亿网格的LOX/CH4湍流燃烧模拟，计算能力提升3个数量级


<details>
  <summary>Details</summary>
Motivation: 传统超临界火焰模拟受限于百万级网格规模，无法满足火箭发动机燃烧等实际工程应用的高分辨率需求

Method: 从并行计算、计算效率和I/O性能三个角度优化DeepFlame软件，集成深度神经网络同时保持真实流体力学和化学精度

Result: 在神威(98304节点)和富岳(73728节点)超算上分别达到439/1186和187/316 PFlop/s性能，实现前所未有的求解时间

Conclusion: 突破性进展使高保真超临界火焰建模成为下一代火箭推进和超高能量密度系统的关键设计工具

Abstract: For decades, supercritical flame simulations incorporating detailed chemistry
and real-fluid transport have been limited to millions of cells, constraining
the resolved spatial and temporal scales of the physical system. We optimize
the supercritical flame simulation software DeepFlame -- which incorporates
deep neural networks while retaining the real-fluid mechanical and chemical
accuracy -- from three perspectives: parallel computing, computational
efficiency, and I/O performance. Our highly optimized DeepFlame achieves
supercritical liquid oxygen/methane (LOX/\ce{CH4}) turbulent combustion
simulation of up to 618 and 154 billion cells with unprecedented
time-to-solution, attaining 439/1186 and 187/316 PFlop/s (32.3\%/21.8\% and
37.4\%/31.8\% of the peak) in FP32/mixed-FP16 precision on Sunway (98,304
nodes) and Fugaku (73,728 nodes) supercomputers, respectively. This
computational capability surpasses existing capacities by three orders of
magnitude, enabling the first practical simulation of rocket engine combustion
with >100 LOX/\ce{CH4} injectors. This breakthrough establishes high-fidelity
supercritical flame modeling as a critical design tool for next-generation
rocket propulsion and ultra-high energy density systems.

</details>


### [9] [CARMA: Collocation-Aware Resource Manager with GPU Memory Estimator](https://arxiv.org/abs/2508.19073)
*Ehsan Yousefzadeh-Asl-Miandoab,Reza Karimzadeh,Bulat Ibragimov,Florina M. Ciorba,Pınar Tözün*

Main category: cs.DC

TL;DR: CARMA是一个服务器级的任务级协同定位感知资源管理系统，通过ML-based GPU内存估计和协同定位策略来解决DL训练任务在GPU上的内存不足和资源干扰问题，显著提升GPU利用率、减少执行时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 企业级基础设施研究表明GPU在深度学习训练中经常被严重低效利用。任务协同定位虽然能提高利用率，但会导致内存不足崩溃和资源干扰导致的性能下降，影响系统鲁棒性和服务质量。

Method: 提出CARMA系统，包含GPUMemNet（基于ML的GPU内存估计框架）来最小化内存错误，引入协同定位策略限制GPU利用率以减少干扰，并提供任务崩溃后的稳健重启恢复方法。

Result: 在真实DL训练任务跟踪的评估中，CARMA使GPU时间利用率提升39.3%，端到端执行时间减少约26.7%，GPU能耗降低约14.2%。

Conclusion: CARMA系统有效解决了DL训练任务在GPU协同定位中的内存和干扰问题，显著提高了资源利用率、性能效率和能源效率。

Abstract: Studies conducted on enterprise-scale infrastructure have shown that GPUs --
the core computational resource for deep learning (DL) training -- are often
significantly underutilized. DL task collocation on GPUs is an opportunity to
address this challenge. However, it may result in (1) out-of-memory crashes for
the subsequently arriving task and (2) slowdowns for all tasks sharing the GPU
due to resource interference. The former challenge poses a threat to
robustness, while the latter affects the quality of service and energy
efficiency.
  We propose CARMA, a server-scale task-level collocation-aware resource
management system that handles both collocation challenges. CARMA encompasses
GPUMemNet, a novel ML-based GPU memory estimator framework for DL training
tasks, to minimize out-of-memory errors and introduces collocation policies
that cap GPU utilization to minimize interference. Furthermore, CARMA
introduces a recovery method to ensure robust restart of tasks that crash. Our
evaluation on traces modeled after real-world DL training task traces shows
that CARMA increases the GPU utilization over time by 39.3\%, decreases the
end-to-end execution time by $\sim$26.7\%, and reduces the GPU energy use by
$\sim$14.2\%.

</details>


### [10] [Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices](https://arxiv.org/abs/2508.19078)
*Fahao Chen,Jie Wan,Peng Li,Zhou Su,Dongxiao Yu*

Main category: cs.DC

TL;DR: FLUX是一个联邦学习系统，专门用于在资源受限设备上高效微调MoE大语言模型，通过量化分析、专家合并和动态角色分配实现4.75倍加速


<details>
  <summary>Details</summary>
Motivation: 现有方法无法在资源受限环境下有效微调MoE大语言模型，存在不实际的系统假设和缺乏MoE特性考虑的问题

Method: 提出三个关键技术：1) 基于量化的本地分析估计专家激活；2) 自适应层感知专家合并减少资源消耗；3) 探索-利用策略动态分配专家角色

Result: 在LLaMA-MoE和DeepSeek-MoE上的实验表明，FLUX显著优于现有方法，时间到精度加速比达到4.75倍

Conclusion: FLUX系统成功解决了资源受限环境下MoE模型联邦微调的挑战，为边缘设备部署大模型提供了可行方案

Abstract: Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models
(LLMs) is challenging due to their massive computational requirements and the
resource constraints of participants. Existing working attempts to fill this
gap through model quantization, computation offloading, or expert pruning.
However, they cannot achieve desired performance due to impractical system
assumptions and a lack of consideration for MoE-specific characteristics. In
this paper, we propose FLUX, a system designed to enable federated fine-tuning
of MoE-based LLMs across participants with constrained computing resources
(e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX
introduces three key innovations: (1) quantization-based local profiling to
estimate expert activation with minimal overhead, (2) adaptive layer-aware
expert merging to reduce resource consumption while preserving accuracy, and
(3) dynamic expert role assignment using an exploration-exploitation strategy
to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE
and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX
significantly outperforms existing methods, achieving up to 4.75X speedup in
time-to-accuracy.

</details>


### [11] [Ab-initio Quantum Transport with the GW Approximation, 42,240 Atoms, and Sustained Exascale Performance](https://arxiv.org/abs/2508.19138)
*Nicolas Vetsch,Alexander Maeder,Vincent Maillou,Anders Winka,Jiang Cao,Grzegorz Kwasniewski,Leonard Deuschle,Torsten Hoefler,Alexandros Nikolaos Ziogas,Mathieu Luisier*

Main category: cs.DC

TL;DR: 提出了首个能够处理实验尺度纳米带场效应晶体管(NRFET)的NEGF+GW计算方案QuaTrEx，通过创新的空间域分解方法实现了84,480原子规模的计算，在超级计算机上展现出优异的扩展性和exascale性能


<details>
  <summary>Details</summary>
Motivation: 随着器件尺寸缩小到几纳米级别，强电子-电子相互作用变得至关重要，传统的DFT+NEGF方法需要扩展到GW近似来准确描述这些量子效应，但计算量巨大

Method: 开发了QuaTrEx软件包，采用新颖的空间域分解方案，实现了NEGF+GW计算框架，能够处理实验尺度的NRFET几何结构

Result: 成功处理了84,480原子规模的器件，在Alps和Frontier超级计算机上实现了>80%的弱扩展效率，在42,240原子上达到了1.15 Eflop/s的exascale性能

Conclusion: QuaTrEx是首个能够处理实验尺度NRFET器件的NEGF+GW实现，为解决纳米电子器件中的强关联效应提供了强大的计算工具，具有优异的可扩展性和计算性能

Abstract: Designing nanoscale electronic devices such as the currently manufactured
nanoribbon field-effect transistors (NRFETs) requires advanced modeling tools
capturing all relevant quantum mechanical effects. State-of-the-art approaches
combine the non-equilibrium Green's function (NEGF) formalism and density
functional theory (DFT). However, as device dimensions do not exceed a few
nanometers anymore, electrons are confined in ultra-small volumes, giving rise
to strong electron-electron interactions. To account for these critical
effects, DFT+NEGF solvers should be extended with the GW approximation, which
massively increases their computational intensity. Here, we present the first
implementation of the NEGF+GW scheme capable of handling NRFET geometries with
dimensions comparable to experiments. This package, called QuaTrEx, makes use
of a novel spatial domain decomposition scheme, can treat devices made of up to
84,480 atoms, scales very well on the Alps and Frontier supercomputers (>80%
weak scaling efficiency), and sustains an exascale FP64 performance on 42,240
atoms (1.15 Eflop/s).

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [SeDA: Secure and Efficient DNN Accelerators with Hardware/Software Synergy](https://arxiv.org/abs/2508.18924)
*Wei Xuan,Zhongrui Wang,Lang Feng,Ning Lin,Zihao Xuan,Rongliang Fu,Tsung-Yi Ho,Yuzhong Jiao,Luhong Liang*

Main category: cs.AR

TL;DR: SeDA是一种高效的DNN加速器安全框架，通过带宽感知加密、最优块粒度和多级完整性验证机制，显著降低性能开销和内存访问开销


<details>
  <summary>Details</summary>
Motivation: 当前DNN加速器安全方案需要大量硬件资源并产生显著的片外内存访问开销，需要更高效的解决方案

Method: 1) 带宽感知加密机制提高硬件资源效率 2) 通过层内和层间分块模式实现最优块粒度 3) 多级完整性验证机制最小化内存访问开销

Result: 实验结果显示SeDA将服务器和边缘NPU的性能开销降低了12%以上，同时确保强大的可扩展性

Conclusion: SeDA为DNN加速器提供了高效的安全保护方案，在保持安全性的同时显著降低了性能开销

Abstract: Ensuring the confidentiality and integrity of DNN accelerators is paramount
across various scenarios spanning autonomous driving, healthcare, and finance.
However, current security approaches typically require extensive hardware
resources, and incur significant off-chip memory access overheads. This paper
introduces SeDA, which utilizes 1) a bandwidth-aware encryption mechanism to
improve hardware resource efficiency, 2) optimal block granularity through
intra-layer and inter-layer tiling patterns, and 3) a multi-level integrity
verification mechanism that minimizes, or even eliminates, memory access
overheads. Experimental results show that SeDA decreases performance overhead
by over 12% for both server and edge neural processing units (NPUs), while
ensuring robust scalability.

</details>


### [13] [TaiBai: A fully programmable brain-inspired processor with topology-aware efficiency](https://arxiv.org/abs/2508.18961)
*Qianpeng Li,Yu Song,Xin Liu,Wenna Song,Boshi Zhao,Zhichao Wang,Aoxin Chen,Tielin Zhang,Liang Chen*

Main category: cs.AR

TL;DR: TaiBai是一款事件驱动的可编程脑启发处理器，通过利用时空脉冲稀疏性实现超高效能，比NVIDIA RTX 3090 GPU能效高200倍以上


<details>
  <summary>Details</summary>
Motivation: 解决现有脑启发芯片网络拓扑结构僵化和神经元可编程性有限的问题，提升芯片的适应性和灵活性

Method: 采用分层拓扑编码方案支持任意网络架构，多粒度指令集实现神经元和突触可编程，配合协同设计的编译器栈进行优化映射

Result: 在语音识别、ECG分类和脑机接口解码等任务中，实现比RTX 3090 GPU高200倍以上的能效，同时保持相当的准确率

Conclusion: TaiBai芯片展示了作为可扩展、可编程和超高效解决方案的巨大潜力，适用于多尺度脑模拟和脑启发计算

Abstract: Brain-inspired computing has emerged as a promising paradigm to overcome the
energy-efficiency limitations of conventional intelligent systems by emulating
the brain's partitioned architecture and event-driven sparse computation.
However, existing brain-inspired chips often suffer from rigid network topology
constraints and limited neuronal programmability, hindering their adaptability.
To address these challenges, we present TaiBai, an event-driven, programmable
many-core brain-inspired processor that leverages temporal and spatial spike
sparsity to minimize bandwidth and computational overhead. TaiBai chip contains
three key features: First, a brain-inspired hierarchical topology encoding
scheme is designed to flexibly support arbitrary network architectures while
slashing storage overhead for large-scale networks; Second, a multi-granularity
instruction set enables programmability of brain-like spiking neuron or
synapses with various dynamics and on-chip learning rules; Third, a co-designed
compiler stack optimizes task mapping and resource allocation. After evaluating
across various tasks, such as speech recognition, ECG classification, and
cross-day brain-computer interface decoding, we found spiking neural networks
embedded on the TaiBai chip could achieve more than 200 times higher energy
efficiency than a standard NVIDIA RTX 3090 GPU at a comparable accuracy. These
results demonstrated its high potentiation as a scalable, programmable, and
ultra-efficient solution for both multi-scale brain simulation and
brain-inspired computation.

</details>


### [14] [Building an Open CGRA Ecosystem for Agile Innovation](https://arxiv.org/abs/2508.19090)
*Rohan Juneja,Pranav Dangi,Thilini Kaushalya Bandara,Zhaoying Li,Dhananjaya Wijerathne,Li-Shiuan Peh,Tulika Mitra*

Main category: cs.AR

TL;DR: 本文提出了一个开放的粗粗细程度可重构架构（CGRA）生态系统，包括HyCUBE CGRA、PACE SoC和Morpher设计框架，以支持硬件软件协同设计和跨层优化。


<details>
  <summary>Details</summary>
Motivation: 现代计算工作负荷（特别是AI和边缘应用）需要硬件软件协同设计来实现高性能和低能耗目标，而开放平台和灵活生态系统可以替代闭源垂直集成开发模式。

Method: 开发了一个完整的开源CGRA生态系统：1) HyCUBE - 具有可重构单周期多跳互联的CGRA；2) PACE - 将高效能HyCUBE嵌入RISC-V SoC的边缘计算方案；3) Morpher - 支持设计空间探索、编译、模拟和验证的开源CGRA设计框架。

Result: 构建了一个完整的开源CGRA生态系统，能够支持从架构探索到实际部署的全流程开发，为灵活硬件开发提供基础。

Conclusion: 提倡在每个层面实施开放性，以降低创新门槛、支持可复现研究。并呼吁为CGRAs和空间加速器建立统一的抽象层，实现硬件专门化与软件开发的解耦，从而开启架构可移植性、编译器创新和空间计算的可扩展开放基础。

Abstract: Modern computing workloads, particularly in AI and edge applications, demand
hardware-software co-design to meet aggressive performance and energy targets.
Such co-design benefits from open and agile platforms that replace closed,
vertically integrated development with modular, community-driven ecosystems.
Coarse-Grained Reconfigurable Architectures (CGRAs), with their unique balance
of flexibility and efficiency are particularly well-suited for this paradigm.
When built on open-source hardware generators and software toolchains, CGRAs
provide a compelling foundation for architectural exploration, cross-layer
optimization, and real-world deployment. In this paper, we will present an open
CGRA ecosystem that we have developed to support agile innovation across the
stack. Our contributions include HyCUBE, a CGRA with a reconfigurable
single-cycle multi-hop interconnect for efficient data movement; PACE, which
embeds a power-efficient HyCUBE within a RISC-V SoC targeting edge computing;
and Morpher, a fully open-source, architecture-adaptive CGRA design framework
that supports design space exploration, compilation, simulation, and
validation. By embracing openness at every layer, we aim to lower barriers to
innovation, enable reproducible research, and demonstrate how CGRAs can anchor
the next wave of agile hardware development. We will conclude with a call for a
unified abstraction layer for CGRAs and spatial accelerators, one that
decouples hardware specialization from software development. Such a
representation would unlock architectural portability, compiler innovation, and
a scalable, open foundation for spatial computing.

</details>
