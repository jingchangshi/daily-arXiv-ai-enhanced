{"id": "2509.13429", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13429", "abs": "https://arxiv.org/abs/2509.13429", "authors": ["Anthony Arnold", "Mark Marron"], "title": "Catalpa: GC for a Low-Variance Software Stack", "comment": null, "summary": "The performance of an application/runtime is usually conceptualized as a\ncontinuous function where, the lower the amount of memory/time used on a given\nworkload, then the better the compiler/runtime is. However, in practice, good\nperformance of an application is viewed as more of a binary function - either\nthe application responds in under, say 100 ms, and is fast enough for a user to\nbarely notice, or it takes a noticeable amount of time, leaving the user\nwaiting and potentially abandoning the task. Thus, performance really means how\noften the application is fast enough to be usable, leading industrial\ndevelopers to focus on the 95th and 99th percentile tail-latencies as heavily,\nor moreso, than average response time. Our vision is to create a software stack\nthat actively supports these needs via programming language and runtime system\ndesign. In this paper we present a novel garbage-collector design, the Catalpa\ncollector, for the Bosque programming language and runtime. This allocator is\ndesigned to minimize latency and variability while maintaining high-throughput\nand incurring small memory overheads. To achieve these goals we leverage\nvarious features of the Bosque language, including immutability and\nreference-cycle freedom, to construct a collector that has bounded collection\npauses, incurs fixed-constant memory overheads, and does not require any\nbarriers or synchronization with application code.", "AI": {"tldr": "\u63d0\u51fa\u4e86Catalpa\u5783\u573e\u6536\u96c6\u5668\uff0c\u901a\u8fc7\u5229\u7528Bosque\u8bed\u8a00\u7684\u4e0d\u53ef\u53d8\u6027\u548c\u65e0\u5f15\u7528\u5faa\u73af\u7279\u6027\uff0c\u5b9e\u73b0\u6709\u754c\u6536\u96c6\u6682\u505c\u3001\u56fa\u5b9a\u5185\u5b58\u5f00\u9500\u548c\u65e0\u5c4f\u969c\u540c\u6b65\u7684\u9ad8\u6027\u80fd\u5783\u573e\u56de\u6536", "motivation": "\u5b9e\u9645\u5e94\u7528\u4e2d\u6027\u80fd\u5f80\u5f80\u662f\u4e8c\u5143\u7684\uff08\u8981\u4e48\u8db3\u591f\u5feb\u7528\u6237\u65e0\u611f\u77e5\uff0c\u8981\u4e48\u660e\u663e\u5ef6\u8fdf\uff09\uff0c\u5de5\u4e1a\u754c\u66f4\u5173\u6ce895%\u548c99%\u767e\u5206\u4f4d\u5c3e\u5ef6\u8fdf\u800c\u975e\u5e73\u5747\u54cd\u5e94\u65f6\u95f4", "method": "\u8bbe\u8ba1Catalpa\u6536\u96c6\u5668\uff0c\u5229\u7528Bosque\u8bed\u8a00\u7684\u4e0d\u53ef\u53d8\u6027\u548c\u65e0\u5f15\u7528\u5faa\u73af\u7279\u6027\uff0c\u6784\u5efa\u65e0\u9700\u5c4f\u969c\u6216\u540c\u6b65\u7684\u6709\u754c\u6682\u505c\u6536\u96c6\u5668", "result": "\u5b9e\u73b0\u4e86\u6700\u5c0f\u5316\u5ef6\u8fdf\u548c\u53ef\u53d8\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u541e\u5410\u91cf\u548c\u5c0f\u5185\u5b58\u5f00\u9500\u7684\u5783\u573e\u6536\u96c6\u5668", "conclusion": "\u901a\u8fc7\u7f16\u7a0b\u8bed\u8a00\u548c\u8fd0\u884c\u65f6\u7cfb\u7edf\u8bbe\u8ba1\u53ef\u4ee5\u4e3b\u52a8\u652f\u6301\u5de5\u4e1a\u7ea7\u6027\u80fd\u9700\u6c42\uff0cCatalpa\u6536\u96c6\u5668\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.13489", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.13489", "abs": "https://arxiv.org/abs/2509.13489", "authors": ["Chester J. F. Gould", "William J. Bowman"], "title": "Extended Abstract: Towards a Performance Comparison of Syntax and Type-Directed NbE", "comment": "Submitted to TyDe 2025", "summary": "A key part of any dependent type-checker is the method for checking whether\ntwo types are equal. A common claim is that syntax-directed equality is more\nperformant, although type-directed equality is more expressive. However, this\nclaim is difficult to make precise, since implementations choose only one or\nthe other approach, making a direct comparison impossible. We present some\nwork-in-progress developing a realistic platform for direct, apples-to-apples,\ncomparison of the two approaches, quantifying how much slower type-directed\nequality checking is, and analyzing why and how it can be improved.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4e2a\u8fdb\u884c\u4e2d\u7684\u5de5\u4f5c\uff0c\u7814\u7a76\u5982\u4f55\u5728\u4f9d\u8d56\u7c7b\u578b\u68c0\u67e5\u5668\u4e2d\u76f4\u63a5\u6bd4\u8f83\u8bed\u6cd5\u6307\u5bfc\u548c\u7c7b\u578b\u6307\u5bfc\u7684\u7c7b\u578b\u76f8\u7b49\u6027\u68c0\u67e5\u65b9\u6cd5\u7684\u6027\u80fd\u548c\u8868\u8fbe\u529b\u3002", "motivation": "\u867d\u7136\u5e38\u89c1\u8bf4\u6cd5\u8ba4\u4e3a\u8bed\u6cd5\u6307\u5bfc\u65b9\u6cd5\u6027\u80fd\u66f4\u597d\u800c\u7c7b\u578b\u6307\u5bfc\u65b9\u6cd5\u8868\u8fbe\u529b\u66f4\u5f3a\uff0c\u4f46\u7531\u4e8e\u5b9e\u73b0\u901a\u5e38\u53ea\u9009\u62e9\u5176\u4e2d\u4e00\u79cd\u65b9\u6cd5\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8fdb\u884c\u76f4\u63a5\u7684\u5e73\u7b49\u6bd4\u8f83\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u5b9e\u9645\u7684\u5e73\u53f0\uff0c\u8fdb\u884c\u76f4\u63a5\u7684\u5e73\u7b49\u6bd4\u8f83\uff0c\u91cf\u5316\u7c7b\u578b\u6307\u5bfc\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u5206\u6790\u5176\u539f\u56e0\u548c\u6539\u8fdb\u65b9\u6cd5\u3002", "result": "\u8fd9\u662f\u4e00\u4e2a\u8fdb\u884c\u4e2d\u7684\u7814\u7a76\uff0c\u5c1a\u672a\u63d0\u4f9b\u5177\u4f53\u7684\u5b9e\u9a8c\u7ed3\u679c\u548c\u6570\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u76ee\u6807\u662f\u4e3a\u4e86\u63d0\u4f9b\u4e00\u4e2a\u53ef\u9760\u7684\u5e73\u53f0\u6765\u8bc4\u4f30\u4e24\u79cd\u7c7b\u578b\u76f8\u7b49\u6027\u68c0\u67e5\u65b9\u6cd5\u7684\u771f\u5b9e\u6027\u80fd\u5dee\u5f02\u548c\u8868\u8fbe\u529b\u4f18\u52bf\u3002"}}
{"id": "2509.13982", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.13982", "abs": "https://arxiv.org/abs/2509.13982", "authors": ["Boyu Zhang", "Ping He", "Tianyu Du", "Xuhong Zhang", "Lei Yun", "Kingsum Chow", "Jianwei Yin"], "title": "CLMTracing: Black-box User-level Watermarking for Code Language Model Tracing", "comment": null, "summary": "With the widespread adoption of open-source code language models (code LMs),\nintellectual property (IP) protection has become an increasingly critical\nconcern. While current watermarking techniques have the potential to identify\nthe code LM to protect its IP, they have limitations when facing the more\npractical and complex demand, i.e., offering the individual user-level tracing\nin the black-box setting. This work presents CLMTracing, a black-box code LM\nwatermarking framework employing the rule-based watermarks and\nutility-preserving injection method for user-level model tracing. CLMTracing\nfurther incorporates a parameter selection algorithm sensitive to the robust\nwatermark and adversarial training to enhance the robustness against watermark\nremoval attacks. Comprehensive evaluations demonstrate CLMTracing is effective\nacross multiple state-of-the-art (SOTA) code LMs, showing significant harmless\nimprovements compared to existing SOTA baselines and strong robustness against\nvarious removal attacks.", "AI": {"tldr": "CLMTracing\u662f\u4e00\u4e2a\u9ed1\u76d2\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u89c4\u5219\u6c34\u5370\u548c\u4fdd\u6301\u5b9e\u7528\u6027\u7684\u6ce8\u5165\u65b9\u6cd5\u5b9e\u73b0\u7528\u6237\u7ea7\u8ffd\u8e2a\uff0c\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740\u5f00\u6e90\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u91c7\u7528\uff0c\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u73b0\u6709\u6c34\u5370\u6280\u672f\u5728\u9762\u5bf9\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u7684\u7528\u6237\u7ea7\u8ffd\u8e2a\u9700\u6c42\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u6c34\u5370\u548c\u4fdd\u6301\u5b9e\u7528\u6027\u7684\u6ce8\u5165\u65b9\u6cd5\uff0c\u7ed3\u5408\u5bf9\u9c81\u68d2\u6c34\u5370\u654f\u611f\u7684\u53c2\u6570\u9009\u62e9\u7b97\u6cd5\u548c\u5bf9\u6297\u8bad\u7ec3\u6765\u589e\u5f3a\u6297\u653b\u51fb\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u6700\u5148\u8fdb\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cCLMTracing\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u6709\u663e\u8457\u65e0\u5bb3\u6539\u8fdb\uff0c\u5bf9\u5404\u79cd\u79fb\u9664\u653b\u51fb\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "CLMTracing\u4e3a\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9ed1\u76d2\u7528\u6237\u7ea7\u8ffd\u8e2a\u6c34\u5370\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u9700\u6c42\u3002"}}
{"id": "2509.14092", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.14092", "abs": "https://arxiv.org/abs/2509.14092", "authors": ["Michele Boreale", "Luisa Collodi"], "title": "Parallelizable Feynman-Kac Models for Universal Probabilistic Programming", "comment": "In Proceedings GandALF 2025, arXiv:2509.13258", "summary": "We study provably correct and efficient instantiations of Sequential Monte\nCarlo (SMC) inference in the context of formal operational semantics of\nProbabilistic Programs (PPs). We focus on universal PPs featuring sampling from\narbitrary measures and conditioning/reweighting in unbounded loops. We first\nequip Probabilistic Program Graphs (PPGs), an automata-theoretic description\nformat of PPs, with an expectation-based semantics over infinite execution\ntraces, which also incorporates trace weights. We then prove a finite\napproximation theorem that provides bounds to this semantics based on\nexpectations taken over finite, fixed-length traces. This enables us to frame\nour semantics within a Feynman-Kac (FK) model, and ensures the consistency of\nthe Particle Filtering (PF) algorithm, an instance of SMC, with respect to our\nsemantics. Building on these results, we introduce VPF, a vectorized version of\nthe PF algorithm tailored to PPGs and our semantics. Experiments conducted with\na proof-of-concept implementation of VPF show very promising results compared\nto state-of-the-art PP inference tools.", "AI": {"tldr": "\u672c\u6587\u4e3a\u6982\u7387\u7a0b\u5e8f\u5f00\u53d1\u4e86\u57fa\u4e8eSequential Monte Carlo\u7684\u5411\u91cf\u5316\u7c92\u5b50\u6ee4\u6ce2\u7b97\u6cd5VPF\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u65e0\u9650\u6267\u884c\u8f68\u8ff9\u4e0a\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u6709\u9650\u8fd1\u4f3c\u6027\u3002", "motivation": "\u7814\u7a76\u6982\u7387\u7a0b\u5e8f\u7684\u6b63\u5f0f\u64cd\u4f5c\u8bed\u4e49\uff0c\u9700\u8981\u5904\u7406\u4ece\u4efb\u610f\u6d4b\u5ea6\u91c7\u6837\u548c\u5728\u65e0\u754c\u5faa\u73af\u4e2d\u6761\u4ef6\u5316/\u91cd\u52a0\u6743\u7684\u901a\u7528\u6982\u7387\u7a0b\u5e8f\uff0c\u4e3a\u8fd9\u7c7b\u7a0b\u5e8f\u63d0\u4f9b\u53ef\u8bc1\u660e\u6b63\u786e\u4e14\u9ad8\u6548\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u4e3a\u6982\u7387\u7a0b\u5e8f\u56fe(PPGs)\u5efa\u7acb\u57fa\u4e8e\u65e0\u9650\u6267\u884c\u8f68\u8ff9\u7684\u671f\u671b\u8bed\u4e49\uff0c\u8bc1\u660e\u6709\u9650\u8fd1\u4f3c\u5b9a\u7406\uff1b\u7136\u540e\u6784\u5efaFeynman-Kac\u6a21\u578b\u6846\u67b6\uff0c\u786e\u4fdd\u7c92\u5b50\u6ee4\u6ce2\u7b97\u6cd5\u4e0e\u8bed\u4e49\u7684\u4e00\u81f4\u6027\uff1b\u6700\u540e\u63d0\u51fa\u9488\u5bf9PPGs\u7684\u5411\u91cf\u5316\u7c92\u5b50\u6ee4\u6ce2\u7b97\u6cd5VPF\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eVPF\u76f8\u6bd4\u73b0\u6709\u6982\u7387\u7a0b\u5e8f\u63a8\u7406\u5de5\u5177\u8868\u73b0\u51fa\u975e\u5e38\u6709\u524d\u666f\u7684\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u901a\u7528\u6982\u7387\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684SMC\u63a8\u7406\u6846\u67b6\uff0c\u63d0\u51fa\u7684VPF\u7b97\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u6982\u7387\u7a0b\u5e8f\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.13557", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13557", "abs": "https://arxiv.org/abs/2509.13557", "authors": ["Zesong Jiang", "Yuqi Sun", "Qing Zhong", "Mahathi Krishna", "Deepak Patil", "Cheng Tan", "Sriram Krishnamoorthy", "Jeff Zhang"], "title": "MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for CGRAs", "comment": null, "summary": "Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing\narchitecture that can deliver high-performance, energy-efficient acceleration\nacross diverse domains. By supporting reconfiguration at the functional unit\nlevel, CGRAs efficiently adapt to varying computational patterns and optimize\nresource utilization. However, designing CGRAs is highly challenging due to the\nvast design space, independent architectural parameters, and the time-consuming\nnature of manual design. Fortunately, the rapid advancement of large language\nmodels (LLMs) presents new opportunities to automate this process.\n  In this work, we propose MACO -- an open-source multi-agent LLM-based\nframework for Hardware/Software (HW/SW) co-design of CGRAs. The framework\nemploys LLM reasoning to generate CGRAs across four stages: HW/SW co-design,\nDesign error correction, Best design selection, and Evaluation & Feedback.\nFurthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent\nreasoning and feedback to achieve higher PPA (that is, power, performance, and\narea) design points for a given domain. In addition, we introduce an LLM\nself-learning mechanism that employs LLM-driven decision making to select the\noptimal CGRA to accelerate the design process.\n  We evaluate the framework with state-of-the-art LLM-based methods and manual\nCGRA design, in terms of performance, power consumption, and area. Experimental\nresults show that MACO efficiently generates high-quality CGRA architectures,\nsignificantly reducing manual design effort and demonstrating the potential of\nour framework for real-world CGRA design.", "AI": {"tldr": "MACO\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8eCGRA\u7684\u786c\u4ef6/\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7LLM\u63a8\u7406\u81ea\u52a8\u751f\u6210\u548c\u4f18\u5316CGRA\u67b6\u6784\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u8bbe\u8ba1\u5de5\u4f5c\u91cf\u3002", "motivation": "CGRA\u8bbe\u8ba1\u9762\u4e34\u8bbe\u8ba1\u7a7a\u95f4\u5de8\u5927\u3001\u67b6\u6784\u53c2\u6570\u72ec\u7acb\u3001\u4eba\u5de5\u8bbe\u8ba1\u8017\u65f6\u7b49\u6311\u6218\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u4e3a\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u9047\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u9636\u6bb5\u8fdb\u884cHW/SW\u534f\u540c\u8bbe\u8ba1\uff1a\u786c\u4ef6/\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u3001\u8bbe\u8ba1\u9519\u8bef\u4fee\u6b63\u3001\u6700\u4f73\u8bbe\u8ba1\u9009\u62e9\u3001\u8bc4\u4f30\u4e0e\u53cd\u9988\uff0c\u5e76\u5f15\u5165LLM\u81ea\u5b66\u4e60\u673a\u5236\u9009\u62e9\u6700\u4f18CGRA\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMACO\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684CGRA\u67b6\u6784\uff0c\u5728\u6027\u80fd\u3001\u529f\u8017\u548c\u9762\u79ef\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u548c\u4eba\u5de5CGRA\u8bbe\u8ba1\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5728\u5b9e\u9645CGRA\u8bbe\u8ba1\u4e2d\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u548c\u667a\u80fd\u4f53\u63a8\u7406\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684PPA\u8bbe\u8ba1\u70b9\u3002"}}
{"id": "2509.13325", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13325", "abs": "https://arxiv.org/abs/2509.13325", "authors": ["Matteo Zanotto", "Leonardo Vicentini", "Redi Vreto", "Francesco Lumpp", "Diego Braga", "Sandro Fiore"], "title": "A User-centric Kubernetes-based Architecture for Green Cloud Computing", "comment": null, "summary": "To meet the increasing demand for cloud computing services, the scale and\nnumber of data centers keeps increasing worldwide. This growth comes at the\ncost of increased electricity consumption, which directly correlates to CO2\nemissions, the main driver of climate change. As such, researching ways to\nreduce cloud computing emissions is more relevant than ever. However, although\ncloud providers are reportedly already working near optimal power efficiency,\nthey fail in providing precise sustainability reporting. This calls for further\nimprovements on the cloud computing consumer's side. To this end, in this paper\nwe propose a user-centric, Kubernetes-based architecture for green cloud\ncomputing. We implement a carbon intensity forecaster and we use it to schedule\nworkloads based on the availability of green energy, exploiting both regional\nand temporal variations to minimize emissions. We evaluate our system using\nreal-world traces of cloud workloads execution comparing the achieved carbon\nemission savings against a baseline round-robin scheduler. Our findings\nindicate that our system can achieve up to a 13% reduction in emissions in a\nstrict scenario with heavy limitations on the available resources.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eKubernetes\u7684\u7528\u6237\u4e2d\u5fc3\u7eff\u8272\u4e91\u8ba1\u7b97\u67b6\u6784\uff0c\u901a\u8fc7\u78b3\u5f3a\u5ea6\u9884\u6d4b\u548c\u7eff\u8272\u80fd\u6e90\u8c03\u5ea6\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u5b9e\u73b013%\u7684\u78b3\u6392\u653e\u51cf\u5c11", "motivation": "\u6570\u636e\u4e2d\u5fc3\u89c4\u6a21\u589e\u957f\u5bfc\u81f4\u7535\u529b\u6d88\u8017\u548cCO2\u6392\u653e\u589e\u52a0\uff0c\u4e91\u63d0\u4f9b\u5546\u867d\u63a5\u8fd1\u6700\u4f18\u80fd\u6548\u4f46\u7f3a\u4e4f\u7cbe\u786e\u53ef\u6301\u7eed\u6027\u62a5\u544a\uff0c\u9700\u8981\u5728\u7528\u6237\u4fa7\u8fdb\u4e00\u6b65\u6539\u8fdb", "method": "\u5b9e\u73b0\u78b3\u5f3a\u5ea6\u9884\u6d4b\u5668\uff0c\u5229\u7528\u533a\u57df\u548c\u65f6\u95f4\u53d8\u5316\u6027\uff0c\u57fa\u4e8e\u7eff\u8272\u80fd\u6e90\u53ef\u7528\u6027\u8c03\u5ea6\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4f7f\u7528Kubernetes\u67b6\u6784", "result": "\u4f7f\u7528\u771f\u5b9e\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u8ddf\u8e2a\u8bc4\u4f30\uff0c\u4e0e\u8f6e\u8be2\u8c03\u5ea6\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728\u8d44\u6e90\u4e25\u683c\u53d7\u9650\u573a\u666f\u4e0b\u53ef\u5b9e\u73b0\u9ad8\u8fbe13%\u7684\u6392\u653e\u51cf\u5c11", "conclusion": "\u7528\u6237\u4e2d\u5fc3\u7684\u7eff\u8272\u4e91\u8ba1\u7b97\u67b6\u6784\u80fd\u6709\u6548\u51cf\u5c11\u78b3\u6392\u653e\uff0c\u78b3\u611f\u77e5\u8c03\u5ea6\u662f\u964d\u4f4e\u4e91\u8ba1\u7b97\u73af\u5883\u5f71\u54cd\u7684\u53ef\u884c\u65b9\u6848"}}
{"id": "2509.13694", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13694", "abs": "https://arxiv.org/abs/2509.13694", "authors": ["Hanchen Ye", "Deming Chen"], "title": "StreamTensor: Make Tensors Stream in Dataflow Accelerators for LLMs", "comment": "Accepted by MICRO'25", "summary": "Efficient execution of deep learning workloads on dataflow architectures is\ncrucial for overcoming memory bottlenecks and maximizing performance. While\nstreaming intermediate results between computation kernels can significantly\nimprove efficiency, existing approaches struggle with inter-kernel\ncorrelations, external memory access management, and buffer optimization. In\nthis work, we propose StreamTensor, a compiler framework that automatically\nconstructs and optimizes stream-based dataflow accelerators. StreamTensor\nintroduces a novel iterative tensor type system to explicitly encode stream\nlayouts, enabling seamless kernel fusion, buffer allocation, and memory\noptimization. By systematically exploring three hierarchical design spaces,\nincluding tensor tiling, kernel fusion, and resource allocation, StreamTensor\nbalances computational intensity, memory efficiency, and data streaming to\nmaximize performance. Based on FPGA evaluations on Large Language Models (LLM),\nStreamTensor achieves up to 0.76x and 0.64x lower latency compared to the\nstate-of-the-art FPGA LLM accelerators and GPUs, and up to 1.99x higher energy\nefficiency compared to GPUs, making it a promising approach for scalable\ndataflow-based deep learning acceleration.", "AI": {"tldr": "StreamTensor\u662f\u4e00\u4e2a\u7f16\u8bd1\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u8fed\u4ee3\u5f20\u91cf\u7c7b\u578b\u7cfb\u7edf\u548c\u5206\u5c42\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u81ea\u52a8\u6784\u5efa\u548c\u4f18\u5316\u57fa\u4e8e\u6d41\u7684\u6570\u636e\u6d41\u52a0\u901f\u5668\uff0c\u5728FPGA\u4e0a\u5b9e\u73b0\u6bd4\u73b0\u6709FPGA\u52a0\u901f\u5668\u548cGPU\u66f4\u4f4e\u7684\u5ef6\u8fdf\u548c\u66f4\u9ad8\u7684\u80fd\u6548", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u5728\u6570\u636e\u6d41\u67b6\u6784\u4e0a\u6267\u884c\u65f6\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5185\u6838\u95f4\u76f8\u5173\u6027\u3001\u5916\u90e8\u5185\u5b58\u8bbf\u95ee\u7ba1\u7406\u548c\u7f13\u51b2\u533a\u4f18\u5316\u65b9\u9762\u5b58\u5728\u56f0\u96be", "method": "\u63d0\u51faStreamTensor\u7f16\u8bd1\u5668\u6846\u67b6\uff0c\u5f15\u5165\u8fed\u4ee3\u5f20\u91cf\u7c7b\u578b\u7cfb\u7edf\u663e\u5f0f\u7f16\u7801\u6d41\u5e03\u5c40\uff0c\u7cfb\u7edf\u63a2\u7d22\u5f20\u91cf\u5e73\u94fa\u3001\u5185\u6838\u878d\u5408\u548c\u8d44\u6e90\u5206\u914d\u4e09\u4e2a\u5206\u5c42\u8bbe\u8ba1\u7a7a\u95f4", "result": "\u5728FPGA\u4e0a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e2d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684FPGA LLM\u52a0\u901f\u5668\u5ef6\u8fdf\u964d\u4f4e0.76\u500d\uff0c\u76f8\u6bd4GPU\u5ef6\u8fdf\u964d\u4f4e0.64\u500d\uff0c\u80fd\u6548\u6bd4GPU\u9ad81.99\u500d", "conclusion": "StreamTensor\u4e3a\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u6570\u636e\u6d41\u7684\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5"}}
{"id": "2509.13575", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13575", "abs": "https://arxiv.org/abs/2509.13575", "authors": ["Benjamin Wilfong", "Anand Radhakrishnan", "Henry A. Le Berre", "Tanush Prathi", "Stephen Abbott", "Spencer H. Bryngelson"], "title": "Testing and benchmarking emerging supercomputers via the MFC flow solver", "comment": "9 pages, 3 figures", "summary": "Deploying new supercomputers requires testing and evaluation via application\ncodes. Portable, user-friendly tools enable evaluation, and the Multicomponent\nFlow Code (MFC), a computational fluid dynamics (CFD) code, addresses this\nneed. MFC is adorned with a toolchain that automates input generation,\ncompilation, batch job submission, regression testing, and benchmarking. The\ntoolchain design enables users to evaluate compiler-hardware combinations for\ncorrectness and performance with limited software engineering experience. As\nwith other PDE solvers, wall time per spatially discretized grid point serves\nas a figure of merit. We present MFC benchmarking results for five generations\nof NVIDIA GPUs, three generations of AMD GPUs, and various CPU architectures,\nutilizing Intel, Cray, NVIDIA, AMD, and GNU compilers. These tests have\nrevealed compiler bugs and regressions on recent machines such as Frontier and\nEl Capitan. MFC has benchmarked approximately 50 compute devices and 5 flagship\nsupercomputers.", "AI": {"tldr": "MFC\u662f\u4e00\u4e2a\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u4ee3\u7801\uff0c\u914d\u5907\u81ea\u52a8\u5316\u5de5\u5177\u94fe\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u7f16\u8bd1\u5668-\u786c\u4ef6\u7ec4\u5408\u7684\u6027\u80fd\u548c\u6b63\u786e\u6027\uff0c\u5df2\u6d4b\u8bd5\u7ea650\u79cd\u8ba1\u7b97\u8bbe\u5907\u548c5\u53f0\u8d85\u7ea7\u8ba1\u7b97\u673a\u3002", "motivation": "\u90e8\u7f72\u65b0\u8d85\u7ea7\u8ba1\u7b97\u673a\u9700\u8981\u901a\u8fc7\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u8fdb\u884c\u6d4b\u8bd5\u548c\u8bc4\u4f30\uff0c\u9700\u8981\u4fbf\u643a\u3001\u7528\u6237\u53cb\u597d\u7684\u5de5\u5177\u6765\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528MFC\u4ee3\u7801\u53ca\u5176\u81ea\u52a8\u5316\u5de5\u5177\u94fe\uff0c\u5305\u62ec\u8f93\u5165\u751f\u6210\u3001\u7f16\u8bd1\u3001\u6279\u5904\u7406\u4f5c\u4e1a\u63d0\u4ea4\u3001\u56de\u5f52\u6d4b\u8bd5\u548c\u57fa\u51c6\u6d4b\u8bd5\u529f\u80fd\uff0c\u6d4b\u8bd5\u4e0d\u540cGPU\u548cCPU\u67b6\u6784\u4ee5\u53ca\u591a\u79cd\u7f16\u8bd1\u5668\u3002", "result": "\u6d4b\u8bd5\u4e86\u4e94\u4ee3NVIDIA GPU\u3001\u4e09\u4ee3AMD GPU\u548c\u5404\u79cdCPU\u67b6\u6784\uff0c\u53d1\u73b0\u4e86Frontier\u548cEl Capitan\u7b49\u65b0\u673a\u5668\u4e0a\u7684\u7f16\u8bd1\u5668\u9519\u8bef\u548c\u56de\u5f52\u95ee\u9898\u3002", "conclusion": "MFC\u5de5\u5177\u94fe\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u7f16\u8bd1\u5668-\u786c\u4ef6\u7ec4\u5408\uff0c\u5e2e\u52a9\u53d1\u73b0\u7cfb\u7edf\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u6027\u80fd\u6d4b\u8bd5\u548c\u9a8c\u8bc1\u3002"}}
{"id": "2509.13710", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13710", "abs": "https://arxiv.org/abs/2509.13710", "authors": ["Hongyi Li", "Songchen Ma", "Huanyu Qu", "Weihao Zhang", "Jia Chen", "Junfeng Lin", "Fengbin Tu", "Rong Zhao"], "title": "CompAir: Synergizing Complementary PIMs and In-Transit NoC Computation for Efficient LLM Acceleration", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has revolutionized\nvarious aspects of human life, yet their immense computational and energy\ndemands pose significant challenges for efficient inference. The memory wall,\nthe growing processor-memory speed disparity, remains a critical bottleneck for\nLLM. Process-In-Memory (PIM) architectures overcome limitations by co-locating\ncompute units with memory, leveraging 5-20$\\times$ higher internal bandwidth\nand enabling greater energy efficiency than GPUs. However, existing PIMs\nstruggle to balance flexibility, performance, and cost-efficiency for LLMs'\ndynamic memory-compute patterns and operator diversity. DRAM-PIM suffers from\ninter-bank communication overhead despite its vector parallelism. SRAM-PIM\noffers sub-10ns latency for matrix operation but is constrained by limited\ncapacity. This work introduces CompAir, a novel PIM architecture that\nintegrates DRAM-PIM and SRAM-PIM with hybrid bonding, enabling efficient linear\ncomputations while unlocking multi-granularity data pathways. We further\ndevelop CompAir-NoC, an advanced network-on-chip with an embedded arithmetic\nlogic unit that performs non-linear operations during data movement,\nsimultaneously reducing communication overhead and area cost. Finally, we\ndevelop a hierarchical Instruction Set Architecture that ensures both\nflexibility and programmability of the hybrid PIM. Experimental results\ndemonstrate that CompAir achieves 1.83-7.98$\\times$ prefill and\n1.95-6.28$\\times$ decode improvement over the current state-of-the-art fully\nPIM architecture. Compared to the hybrid A100 and HBM-PIM system, CompAir\nachieves 3.52$\\times$ energy consumption reduction with comparable throughput.\nThis work represents the first systematic exploration of hybrid DRAM-PIM and\nSRAM-PIM architectures with in-network computation capabilities, offering a\nhigh-efficiency solution for LLM.", "AI": {"tldr": "CompAir\u662f\u4e00\u79cd\u65b0\u578b\u6df7\u5408PIM\u67b6\u6784\uff0c\u901a\u8fc7DRAM-PIM\u548cSRAM-PIM\u7684\u5f02\u6784\u96c6\u6210\u4ee5\u53ca\u521b\u65b0\u7684\u7247\u4e0a\u7f51\u7edc\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u548c\u80fd\u6548", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u548c\u80fd\u8017\u9700\u6c42\u5de8\u5927\uff0c\u73b0\u6709PIM\u67b6\u6784\u96be\u4ee5\u5e73\u8861\u7075\u6d3b\u6027\u3001\u6027\u80fd\u548c\u6210\u672c\u6548\u7387\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406LLM\u7684\u52a8\u6001\u5185\u5b58\u8ba1\u7b97\u6a21\u5f0f\u548c\u7b97\u5b50\u591a\u6837\u6027", "method": "\u63d0\u51faCompAir\u6df7\u5408PIM\u67b6\u6784\uff1a1\uff09\u901a\u8fc7\u6df7\u5408\u952e\u5408\u96c6\u6210DRAM-PIM\u548cSRAM-PIM\uff1b2\uff09\u5f00\u53d1CompAir-NoC\u7247\u4e0a\u7f51\u7edc\uff0c\u5728\u6570\u636e\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u6267\u884c\u975e\u7ebf\u6027\u8fd0\u7b97\uff1b3\uff09\u8bbe\u8ba1\u5206\u5c42\u6307\u4ee4\u96c6\u67b6\u6784\u786e\u4fdd\u7075\u6d3b\u6027\u548c\u53ef\u7f16\u7a0b\u6027", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5168PIM\u67b6\u6784\uff0c\u9884\u586b\u5145\u6027\u80fd\u63d0\u53471.83-7.98\u500d\uff0c\u89e3\u7801\u6027\u80fd\u63d0\u53471.95-6.28\u500d\uff1b\u76f8\u6bd4\u6df7\u5408A100\u548cHBM-PIM\u7cfb\u7edf\uff0c\u80fd\u8017\u964d\u4f4e3.52\u500d\u4e14\u541e\u5410\u91cf\u76f8\u5f53", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7cfb\u7edf\u63a2\u7d22\u6df7\u5408DRAM-PIM\u548cSRAM-PIM\u67b6\u6784\u5e76\u5177\u5907\u7f51\u7edc\u5185\u8ba1\u7b97\u80fd\u529b\u7684\u5de5\u4f5c\uff0c\u4e3aLLM\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.13583", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13583", "abs": "https://arxiv.org/abs/2509.13583", "authors": ["Varsha Rao", "Andrew A. Chien"], "title": "Modeling the Carbon Footprint of HPC: The Top 500 and EasyC", "comment": "15 pages, 11 figures", "summary": "Climate change is a critical concern for HPC systems, but GHG protocol\ncarbon-emission accounting methodologies are difficult for a single system, and\neffectively infeasible for a collection of systems. As a result, there is no\nHPC-wide carbon reporting, and even the largest HPC sites do not do GHG\nprotocol reporting.\n  We assess the carbon footprint of HPC, focusing on the Top 500 systems. The\nkey challenge lies in modeling the carbon footprint with limited data\navailability.\n  With the disclosed Top500.org data, and using a new tool, EasyC, we were able\nto model the operational carbon of 391 HPC systems and the embodied carbon of\n283 HPC systems. We further show how this coverage can be enhanced by\nexploiting additional public information. With improved coverage, then\ninterpolation is used to produce the first carbon footprint estimates of the\nTop 500 HPC systems. They are 1,393.7 million MT CO2e operational carbon (1\nYear) and 1,881.8 million MT CO2e embodied carbon. We also project how the Top\n500's carbon footprint will increase through 2030.\n  A key enabler is the EasyC tool which models carbon footprint with only a few\ndata metrics. We explore availability of data and enhancement, showing that\ncoverage can be increased to 98% of Top 500 systems for operational and 80.8%\nof the systems for embodied emissions.", "AI": {"tldr": "\u4f7f\u7528EasyC\u5de5\u5177\u5206\u6790Top 500\u8d85\u7b97\u7cfb\u7edf\u7684\u78b3\u8e39\u8ff9\uff0c\u4f30\u7b97\u51fa\u8fd0\u8425\u78b3\u6392\u653e1393.7\u4e07\u5428CO2e\uff0c\u4f53\u73b0\u78b3\u6392\u653e1881.8\u4e07\u5428CO2e\uff0c\u5e76\u9884\u6d4b2030\u5e74\u524d\u589e\u957f\u8d8b\u52bf", "motivation": "HPC\u7cfb\u7edf\u78b3\u6392\u653e\u91cf\u8ba1\u7b97\u65b9\u6cd5\u590d\u6742\u4e14\u7f3a\u4e4f\u7edf\u4e00\u62a5\u544a\uff0c\u9700\u8981\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\u6765\u4f30\u7b97\u5168\u7403HPC\u7cfb\u7edf\u7684\u78b3\u8e39\u8ff9", "method": "\u5f00\u53d1EasyC\u5de5\u5177\uff0c\u5229\u7528Top500.org\u6570\u636e\u548c\u516c\u5f00\u4fe1\u606f\uff0c\u901a\u8fc7\u63d2\u503c\u6cd5\u5efa\u6a21\u8d85\u7b97\u7cfb\u7edf\u7684\u8fd0\u8425\u78b3\u548c\u4f53\u73b0\u78b3\u6392\u653e", "result": "\u6210\u529f\u6a21\u578b\u4e86391\u4e2a\u7cfb\u7edf\u7684\u8fd0\u8425\u78b3\u6392\u653e\u548c283\u4e2a\u7cfb\u7edf\u7684\u4f53\u73b0\u78b3\u6392\u653e\uff0c\u603b\u4f30\u7b97\u51faTop 500\u7cfb\u7edf\u5e74\u78b3\u6392\u653e\u91cf\u8fbe3275.5\u4e07\u5428CO2e", "conclusion": "EasyC\u5de5\u5177\u80fd\u591f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u4f30\u7b97HPC\u7cfb\u7edf\u78b3\u8e39\u8ff9\uff0c\u4e3aHPC\u884c\u4e1a\u78b3\u6392\u653e\u7ba1\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6cd5\uff0c\u5e76\u9884\u89c8\u4e86\u672a\u6765\u78b3\u6392\u653e\u589e\u957f\u8d8b\u52bf"}}
{"id": "2509.13765", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13765", "abs": "https://arxiv.org/abs/2509.13765", "authors": ["Zhirui Huang", "Rui Ma", "Shijie Cao", "Ran Shu", "Ian Wang", "Ting Cao", "Chixiao Chen", "Yongqiang Xiong"], "title": "TENET: An Efficient Sparsity-Aware LUT-Centric Architecture for Ternary LLM Inference On Edge", "comment": null, "summary": "Ternary quantization has emerged as a powerful technique for reducing both\ncomputational and memory footprint of large language models (LLM), enabling\nefficient real-time inference deployment without significantly compromising\nmodel accuracy. Conventional LLM inference platforms (e.g GPUs) cannot\ncapitalize on its benefits, as they (i) lack native support for ternary\narithmetic and memory specialization and (ii) remain severely under-utilized in\nlow-batch, real-time scenarios. In this work, we propose TENET, a sparse-aware\nLUT-centric architecture that co-optimizes algorithm, compute, and memory for\nternary LLM inference. To maximize the efficiency of Ternary Linear layer,\nTENET introduces a Sparse Ternary LUT (STL) core that optimizes ternary\nmixed-precision GEMM using a symmetric precompute lookup table. It also\nfeatures Dynamic Activation N:M Sparsity to exploit the sparsity within the\nactivation of each token. Additionally, we propose a LUT-based 64B:80B ternary\nweight decompression module to fully exploit the memory efficiency of ternary\nvalues. At the system level, we design a heterogeneous TENET accelerator with\nfull programmability that integrates STL cores with high-precision cores. An\nassociated Linear-Projection-aware Sparse Attention dataflow is introduced to\noptimize memory access and hardware utilization. We implement TENET accelerator\nprototype on both FPGA and ASIC platforms. Experiments across various model\nsizes and workloads demonstrate that TENET-FPGA and TENET-ASIC improve energy\nefficiency by 4.3$\\times$ and 21.1$\\times$, respectively, compared to the A100\nGPU. Furthermore, TENET-ASIC achieves a 2.7$\\times$ average speedup compared to\nthe A100 GPU in end-to-end inference latency.", "AI": {"tldr": "TENET\u662f\u4e00\u4e2a\u9488\u5bf9\u4e09\u5143\u91cf\u5316LLM\u63a8\u7406\u7684\u7a00\u758f\u611f\u77e5LUT\u4e2d\u5fc3\u67b6\u6784\uff0c\u901a\u8fc7\u7b97\u6cd5\u3001\u8ba1\u7b97\u548c\u5185\u5b58\u534f\u540c\u4f18\u5316\uff0c\u5728FPGA\u548cASIC\u5e73\u53f0\u4e0a\u5206\u522b\u5b9e\u73b04.3\u500d\u548c21.1\u500d\u7684\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u4f20\u7edfGPU\u5e73\u53f0\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e09\u5143\u91cf\u5316\u7684\u4f18\u52bf\uff0c\u7f3a\u4e4f\u5bf9\u4e09\u5143\u7b97\u672f\u548c\u5185\u5b58\u4e13\u4e1a\u5316\u7684\u539f\u751f\u652f\u6301\uff0c\u5728\u4f4e\u6279\u6b21\u5b9e\u65f6\u63a8\u7406\u573a\u666f\u4e0b\u5229\u7528\u7387\u4e25\u91cd\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSparse Ternary LUT (STL)\u6838\u5fc3\u4f18\u5316\u4e09\u5143\u6df7\u5408\u7cbe\u5ea6GEMM\uff0c\u91c7\u7528\u52a8\u6001\u6fc0\u6d3bN:M\u7a00\u758f\u6027\uff0c\u8bbe\u8ba1\u57fa\u4e8eLUT\u768464B:80B\u6743\u91cd\u89e3\u538b\u7f29\u6a21\u5757\uff0c\u6784\u5efa\u5f02\u6784\u53ef\u7f16\u7a0b\u52a0\u901f\u5668\u3002", "result": "TENET-FPGA\u548cTENET-ASIC\u76f8\u6bd4A100 GPU\u5206\u522b\u5b9e\u73b04.3\u500d\u548c21.1\u500d\u7684\u80fd\u6548\u63d0\u5347\uff0cTENET-ASIC\u5728\u7aef\u5230\u7aef\u63a8\u7406\u5ef6\u8fdf\u4e0a\u5b9e\u73b02.7\u500d\u5e73\u5747\u52a0\u901f\u3002", "conclusion": "TENET\u67b6\u6784\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e09\u5143\u91cf\u5316LLM\u63a8\u7406\u5728\u4f20\u7edf\u786c\u4ef6\u4e0a\u7684\u6548\u7387\u74f6\u9888\uff0c\u4e3a\u5b9e\u65f6\u63a8\u7406\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.13703", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.13703", "abs": "https://arxiv.org/abs/2509.13703", "authors": ["Sriram Srinivasan", "Hamdan Alabsi", "Rand Obeidat", "Nithisha Ponnala", "Azene Zenebe"], "title": "GPU Programming for AI Workflow Development on AWS SageMaker: An Instructional Approach", "comment": null, "summary": "We present the design, implementation, and comprehensive evaluation of a\nspecialized course on GPU architecture, GPU programming, and how these are used\nfor developing AI agents. This course is offered to undergraduate and graduate\nstudents during Fall 2024 and Spring 2025. The course began with foundational\nconcepts in GPU/CPU hardware and parallel computing and progressed to develop\nRAG and optimizing them using GPUs. Students gained experience provisioning and\nconfiguring cloud-based GPU instances, implementing parallel algorithms, and\ndeploying scalable AI solutions. We evaluated learning outcomes through\nassessments, course evaluations, and anonymous surveys. The results reveal that\n(1) AWS served as an effective and economical platform for practical GPU\nprogramming, (2) experiential learning significantly enhanced technical\nproficiency and engagement, and (3) the course strengthened students'\nproblem-solving and critical thinking skills through tools such as TensorBoard\nand HPC profilers, which exposed performance bottlenecks and scaling issues.\nOur findings underscore the pedagogical value of integrating parallel computing\ninto STEM education. We advocate for broader adoption of similar electives\nacross STEM curricula to prepare students for the demands of modern,\ncompute-intensive fields.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u95e8\u4e13\u95e8\u8bb2\u6388GPU\u67b6\u6784\u3001GPU\u7f16\u7a0b\u548cAI\u5e94\u7528\u7684\u8bfe\u7a0b\uff0c\u901a\u8fc7AWS\u4e91\u5e73\u53f0\u8fdb\u884c\u5b9e\u8df5\u6559\u5b66\uff0c\u8bc1\u660e\u4e86\u5b9e\u8df5\u5b66\u4e60\u5bf9\u63d0\u5347\u6280\u672f\u6280\u80fd\u548c\u601d\u7ef4\u80fd\u529b\u7684\u6548\u679c", "motivation": "\u4e3a\u4e86\u51c6\u5907STEM\u5b66\u751f\u6ee1\u8db3\u73b0\u4ee3\u8ba1\u7b97\u5bc6\u96c6\u9886\u57df\u7684\u9700\u6c42\uff0c\u6574\u5408\u5e76\u884c\u8ba1\u7b97\u5230\u6559\u80b2\u4e2d\uff0c\u63d0\u5347\u5b66\u751f\u7684\u6280\u672f\u7cbe\u901a\u5ea6\u548c\u89e3\u51b3\u95ee\u9898\u80fd\u529b", "method": "\u8bbe\u8ba1\u4e86\u4eceGPU/CPU\u786c\u4ef6\u57fa\u7840\u3001\u5e76\u884c\u8ba1\u7b97\u5230RAG\u5f00\u53d1\u7684\u8fdb\u9636\u8bfe\u7a0b\uff0c\u4f7f\u7528AWS\u4e91GPU\u5b9e\u4f8b\u8fdb\u884c\u5b9e\u8df5\uff0c\u901a\u8fc7\u8bc4\u4f30\u3001\u8bfe\u7a0b\u8bc4\u4ef7\u548c\u533f\u540d\u8c03\u67e5\u8bc4\u4f30\u5b66\u4e60\u6548\u679c", "result": "\u7ed3\u679c\u663e\u793a\uff1a(1)AWS\u662f\u9ad8\u6548\u7ecf\u6d4e\u7684GPU\u7f16\u7a0b\u5e73\u53f0\uff1b(2)\u5b9e\u8df5\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u6280\u672f\u6280\u80fd\u548c\u53c2\u4e0e\u5ea6\uff1b(3)\u8bfe\u7a0b\u589e\u5f3a\u4e86\u5b66\u751f\u7684\u95ee\u9898\u89e3\u51b3\u548c\u601d\u7ef4\u80fd\u529b", "conclusion": "\u5e76\u884c\u8ba1\u7b97\u6574\u5408\u5230STEM\u6559\u80b2\u4e2d\u5177\u6709\u91cd\u8981\u6559\u80b2\u4ef7\u503c\uff0c\u5efa\u8bae\u66f4\u5e7f\u6cdb\u91c7\u7528\u7c7b\u4f3c\u9009\u4fee\u8bfe\u7a0b\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u8ba1\u7b97\u5bc6\u96c6\u9886\u57df\u7684\u9700\u6c42"}}
{"id": "2509.13997", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.13997", "abs": "https://arxiv.org/abs/2509.13997", "authors": ["Yu Zhu", "Aditya Dhakal", "Pedro Bruel", "Gourav Rattihalli", "Yunming Xiao", "Johann Lombardi", "Dejan Milojicic"], "title": "An RDMA-First Object Storage System with SmartNIC Offload", "comment": null, "summary": "AI training and inference impose sustained, fine-grain I/O that stresses\nhost-mediated, TCP-based storage paths. Motivated by kernel-bypass networking\nand user-space storage stacks, we revisit POSIX-compatible object storage for\nGPU-centric pipelines. We present ROS2, an RDMA-first object storage system\ndesign that offloads the DAOS client to an NVIDIA BlueField-3 SmartNIC while\nleaving the DAOS I/O engine unchanged on the storage server. ROS2 separates a\nlightweight control plane (gRPC for namespace and capability exchange) from a\nhigh-throughput data plane (UCX/libfabric over RDMA or TCP) and removes host\nmediation from the data path.\n  Using FIO/DFS across local and remote configurations, we find that on\nserver-grade CPUs RDMA consistently outperforms TCP for both large sequential\nand small random I/O. When the RDMA-driven DAOS client is offloaded to\nBlueField-3, end-to-end performance is comparable to the host, demonstrating\nthat SmartNIC offload preserves RDMA efficiency while enabling DPU-resident\nfeatures such as multi-tenant isolation and inline services (e.g.,\nencryption/decryption) close to the NIC. In contrast, TCP on the SmartNIC lags\nhost performance, underscoring the importance of RDMA for offloaded\ndeployments.\n  Overall, our results indicate that an RDMA-first, SmartNIC-offloaded\nobject-storage stack is a practical foundation for scaling data delivery in\nmodern LLM training environments; integrating optional GPU-direct placement for\nLLM tasks is left for future work.", "AI": {"tldr": "ROS2\u662f\u4e00\u4e2a\u57fa\u4e8eRDMA\u7684\u667a\u80fd\u7f51\u5361\u5378\u8f7d\u5bf9\u8c61\u5b58\u50a8\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u79bb\u63a7\u5236\u5e73\u9762\u548c\u6570\u636e\u5e73\u9762\uff0c\u5728GPU\u4e2d\u5fc3\u5316AI\u8bad\u7ec3\u7ba1\u9053\u4e2d\u63d0\u4f9b\u9ad8\u6027\u80fd\u5b58\u50a8\u8bbf\u95ee\uff0cRDMA\u6027\u80fd\u4f18\u4e8eTCP\u4e14\u667a\u80fd\u7f51\u5361\u5378\u8f7d\u4fdd\u6301\u6548\u7387", "motivation": "AI\u8bad\u7ec3\u548c\u63a8\u7406\u4ea7\u751f\u6301\u7eed\u3001\u7ec6\u7c92\u5ea6\u7684I/O\u8d1f\u8f7d\uff0c\u7ed9\u57fa\u4e8eTCP\u7684\u4e3b\u673a\u4e2d\u4ecb\u5b58\u50a8\u8def\u5f84\u5e26\u6765\u538b\u529b\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6POSIX\u517c\u5bb9\u7684\u5bf9\u8c61\u5b58\u50a8\u65b9\u6848", "method": "\u8bbe\u8ba1ROS2\u7cfb\u7edf\uff0c\u5c06DAOS\u5ba2\u6237\u7aef\u5378\u8f7d\u5230NVIDIA BlueField-3\u667a\u80fd\u7f51\u5361\uff0c\u4fdd\u6301\u5b58\u50a8\u670d\u52a1\u5668\u4e0a\u7684DAOS I/O\u5f15\u64ce\u4e0d\u53d8\uff0c\u5206\u79bb\u8f7b\u91cf\u7ea7\u63a7\u5236\u5e73\u9762(gRPC)\u548c\u9ad8\u541e\u5410\u6570\u636e\u5e73\u9762(UCX/libfabric over RDMA/TCP)", "result": "RDMA\u5728\u670d\u52a1\u5668\u7ea7CPU\u4e0a\u59cb\u7ec8\u4f18\u4e8eTCP\uff0c\u667a\u80fd\u7f51\u5361\u5378\u8f7d\u7684RDMA\u9a71\u52a8DAOS\u5ba2\u6237\u7aef\u7aef\u5230\u7aef\u6027\u80fd\u4e0e\u4e3b\u673a\u76f8\u5f53\uff0c\u800cTCP\u5728\u667a\u80fd\u7f51\u5361\u4e0a\u6027\u80fd\u843d\u540e", "conclusion": "RDMA\u4f18\u5148\u3001\u667a\u80fd\u7f51\u5361\u5378\u8f7d\u7684\u5bf9\u8c61\u5b58\u50a8\u5806\u6808\u662f\u73b0\u4ee3LLM\u8bad\u7ec3\u73af\u5883\u4e2d\u6269\u5c55\u6570\u636e\u4ea4\u4ed8\u7684\u5b9e\u7528\u57fa\u7840"}}
{"id": "2509.13978", "categories": ["cs.DC", "cs.AI", "cs.DB", "68M14, 68M20, 68T07", "C.2.4; D.1.3; I.2.0"], "pdf": "https://arxiv.org/pdf/2509.13978", "abs": "https://arxiv.org/abs/2509.13978", "authors": ["Renan Souza", "Timothy Poteet", "Brian Etz", "Daniel Rosendo", "Amal Gueroudji", "Woong Shin", "Prasanna Balaprakash", "Rafael Ferreira da Silva"], "title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology", "comment": "Paper accepted in the proceedings of the ACM/IEEE Supercomputing\n  Conference (SC). Cite it as Renan Souza, Timothy Poteet, Brian Etz, Daniel\n  Rosendo, Amal Gueroudji, Woong Shin, Prasanna Balaprakash, and Rafael\n  Ferreira da Silva. 2025. LLM Agents for Interactive Workflow Provenance:\n  Reference Architecture and Evaluation Methodology. In SC Workshops (WORKS)", "summary": "Modern scientific discovery increasingly relies on workflows that process\ndata across the Edge, Cloud, and High Performance Computing (HPC) continuum.\nComprehensive and in-depth analyses of these data are critical for hypothesis\nvalidation, anomaly detection, reproducibility, and impactful findings.\nAlthough workflow provenance techniques support such analyses, at large scale,\nthe provenance data become complex and difficult to analyze. Existing systems\ndepend on custom scripts, structured queries, or static dashboards, limiting\ndata interaction. In this work, we introduce an evaluation methodology,\nreference architecture, and open-source implementation that leverages\ninteractive Large Language Model (LLM) agents for runtime data analysis. Our\napproach uses a lightweight, metadata-driven design that translates natural\nlanguage into structured provenance queries. Evaluations across LLaMA, GPT,\nGemini, and Claude, covering diverse query classes and a real-world chemistry\nworkflow, show that modular design, prompt tuning, and Retrieval-Augmented\nGeneration (RAG) enable accurate and insightful LLM agent responses beyond\nrecorded provenance.", "AI": {"tldr": "\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u4ea4\u4e92\u5f0f\u6d41\u7a0b\u6267\u884c\u6570\u636e\u5206\u6790\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u67e5\u8be2\uff0c\u89e3\u51b3\u5927\u89c4\u6a21\u6d41\u7a0b\u6267\u884c\u8ffd\u6eaf\u6570\u636e\u5206\u6790\u590d\u6742\u6027\u95ee\u9898", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u53d1\u73b0\u4f9d\u8d56\u4e8e\u5728Edge\u3001Cloud\u548cHPC\u8fde\u7eed\u4f53\u4e0a\u5904\u7406\u6570\u636e\u7684\u6d41\u7a0b\u3002\u867d\u7136\u6d41\u7a0b\u6267\u884c\u8ffd\u6eaf\u6280\u672f\u652f\u6301\u6df1\u5165\u5206\u6790\uff0c\u4f46\u5927\u89c4\u6a21\u4e0b\u6267\u884c\u8ffd\u6eaf\u6570\u636e\u53d8\u5f97\u590d\u6742\u96be\u4ee5\u5206\u6790\uff0c\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u81ea\u5b9a\u4e49\u811a\u672c\u3001\u7ed3\u6784\u5316\u67e5\u8be2\u6216\u9759\u6001\u4eea\u8868\u677f\uff0c\u9650\u5236\u4e86\u6570\u636e\u4ea4\u4e92\u80fd\u529b", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\u3001\u53c2\u8003\u67b6\u6784\u548c\u5f00\u6e90\u5b9e\u73b0\uff0c\u5229\u7528\u4ea4\u4e92\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u8fdb\u884c\u8fd0\u884c\u65f6\u6570\u636e\u5206\u6790\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u3001\u5143\u6570\u636e\u9a71\u52a8\u7684\u8bbe\u8ba1\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u7ffb\u8bd1\u4e3a\u7ed3\u6784\u5316\u6267\u884c\u8ffd\u6eaf\u67e5\u8be2", "result": "\u5728LLaMA\u3001GPT\u3001Gemini\u548cClaude\u7b49\u591a\u4e2a\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u6d89\u53ca\u591a\u79cd\u67e5\u8be2\u7c7b\u578b\u548c\u771f\u5b9e\u5316\u5b66\u6d41\u7a0b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u3001\u63d0\u793a\u8c03\u6574\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u4e14\u6709\u6df1\u5ea6\u7684LLM\u4ee3\u7406\u54cd\u5e94\uff0c\u8d85\u8d8a\u4e86\u8bb0\u5f55\u7684\u6267\u884c\u8ffd\u6eaf\u8303\u56f4", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u6d41\u7a0b\u6267\u884c\u8ffd\u6eaf\u6570\u636e\u5206\u6790\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u63d0\u4f9b\u4e86\u66f4\u52a0\u7075\u6d3b\u548c\u6df1\u5165\u7684\u6570\u636e\u5206\u6790\u80fd\u529b\uff0c\u4e3a\u79d1\u5b66\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u65b9\u6cd5"}}
{"id": "2509.14041", "categories": ["cs.AR", "cs.CL", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.14041", "abs": "https://arxiv.org/abs/2509.14041", "authors": ["Henry Kao", "Nikhil Sreekumar", "Prabhdeep Singh Soni", "Ali Sedaghati", "Fang Su", "Bryan Chan", "Maziar Goudarzi", "Reza Azimi"], "title": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval Prediction For Instruction Caching", "comment": null, "summary": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO.", "AI": {"tldr": "TRRIP\u662f\u4e00\u79cd\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u8bd1\u5668\u5206\u6790\u4ee3\u7801\u6e29\u5ea6\uff08\u70ed/\u51b7\uff09\u5e76\u5229\u7528\u64cd\u4f5c\u7cfb\u7edf\u63a5\u53e3\u5411\u786c\u4ef6\u63d0\u4f9b\u6e29\u5ea6\u4fe1\u606f\uff0c\u4f18\u5316\u6307\u4ee4\u7f13\u5b58\u66ff\u6362\u7b56\u7565\uff0c\u51cf\u5c11\u70ed\u4ee3\u7801\u7684\u9a71\u9010\u7387\uff0c\u5728\u79fb\u52a8CPU\u4e0a\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u79fb\u52a8CPU\u8f6f\u4ef6\u7531\u4e8e\u590d\u6742\u7684\u8fd0\u884c\u65f6\u884c\u4e3a\u548c\u6307\u4ee4\u91cd\u7528\u8ddd\u79bb\u5927\uff0c\u5bfc\u81f4\u4f20\u7edf\u6307\u4ee4\u7f13\u5b58\u66ff\u6362\u7b56\u7565\u6548\u7387\u4f4e\u4e0b\uff0c\u524d\u7aef\u505c\u987f\u4e25\u91cd\uff0c\u4e14\u4ee3\u7801\u590d\u6742\u5ea6\u589e\u957f\u5feb\u4e8e\u7247\u4e0a\u5185\u5b58\u5bb9\u91cf\u589e\u957f\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff1a\u7f16\u8bd1\u5668\u5206\u6790\u4ee3\u7801\u6e29\u5ea6\u5e76\u5206\u7c7b\u8f6c\u6362\uff0c\u901a\u8fc7OS\u63a5\u53e3\u63d0\u4f9b\u6e29\u5ea6\u4fe1\u606f\uff1b\u786c\u4ef6\u8f7b\u91cf\u7ea7\u6269\u5c55\u5229\u7528\u6e29\u5ea6\u5c5e\u6027\u4f18\u5316\u7f13\u5b58\u66ff\u6362\u7b56\u7565\u3002", "result": "L2\u6307\u4ee4MPKI\u964d\u4f4e26.5%\uff0c\u5728\u5df2\u4f7f\u7528PGO\u4f18\u5316\u7684\u79fb\u52a8\u4ee3\u7801\u4e0a\u5b9e\u73b03.9%\u7684\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u6bd4\u3002", "conclusion": "TRRIP\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u91c7\u7eb3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u4f18\u5316\u79fb\u52a8\u7cfb\u7edf\u7684\u6307\u4ee4\u7f13\u5b58\u7ba1\u7406\uff0c\u63d0\u5347\u6027\u80fd\u3002"}}
