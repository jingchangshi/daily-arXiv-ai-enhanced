<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Hey Pentti, We Did It!: A Fully Vector-Symbolic Lisp](https://arxiv.org/abs/2510.17889)
*Eilene Tomkins-Flanagan,Mary A. Kelly*

Main category: cs.PL

TL;DR: 该论文展示了如何在向量符号架构中实现完整的Lisp语言，包括五个基本函数、lambda表达式等，使用全息简化表示和查找表清理内存。


<details>
  <summary>Details</summary>
Motivation: 验证Kanerva(2014)的假设，证明向量符号架构能够构建完整的Lisp语言，并探讨其数学意义和架构规范的重要性。

Method: 使用全息简化表示(Plate, 1995)和查找表清理内存，构建Lisp 1.5规范中五个基本函数、lambda表达式及其他辅助函数的向量符号表示。

Result: 成功实现了向量符号架构下的完整Lisp语言，证明了该架构的笛卡尔闭包性质。

Conclusion: 向量符号架构具备构建图灵完备语言的能力，清理内存应明确包含在架构规范中，这对理解向量符号计算的数学基础具有重要意义。

Abstract: Kanerva (2014) suggested that it would be possible to construct a complete
Lisp out of a vector-symbolic architecture. We present the general form of a
vector-symbolic representation of the five Lisp elementary functions, lambda
expressions, and other auxiliary functions, found in the Lisp 1.5 specification
McCarthy (1960), which is near minimal and sufficient for Turing-completeness.
Our specific implementation uses holographic reduced representations Plate
(1995), with a lookup table cleanup memory. Lisp, as all Turing-complete
languages, is a Cartesian closed category, unusual in its proximity to the
mathematical abstraction. We discuss the mathematics, the purpose, and the
significance of demonstrating vector-symbolic architectures' Cartesian-closure,
as well as the importance of explicitly including cleanup memories in the
specification of the architecture.

</details>


### [2] [ZipLex: Verified Invertible Lexing with Memoized Derivatives and Zippers](https://arxiv.org/abs/2510.18479)
*Samuel Chassot,Viktor Kunčak*

Main category: cs.PL

TL;DR: ZipLex是一个经过验证的可逆词法分析框架，不仅保证正则表达式语义和最大匹配属性，还确保词法分析和打印互为逆操作。


<details>
  <summary>Details</summary>
Motivation: 传统验证词法分析器只关注正则表达式语义和最大匹配属性，但缺乏对词法分析和打印互为逆操作的保证。

Method: 采用新的token序列抽象来捕获token可分离性，结合验证数据结构（Huet拉链和记忆化导数）进行优化。

Result: ZipLex在Scala中实现并通过Stainless验证器验证正确性，支持JSON处理和编程语言词法分析等实际应用。

Conclusion: 验证可逆性可以在不付出过高代价的情况下实现，ZipLex比Coqlex慢4倍但比Verbatim++快两个数量级。

Abstract: We present ZipLex, a verified framework for invertible lexical analysis.
Unlike past verified lexers that focus only on satisfying the semantics of
regular expressions and the maximal munch property, ZipLex also guarantees that
lexing and printing are mutual inverses. Our design relies on two sets of
ideas: (1) a new abstraction of token sequences that captures the separability
of tokens in a sequence while supporting their efficient manipulation, and (2)
a combination of verified data structures and optimizations, including Huet's
zippers and memoized derivatives, to achieve practical performance. We
implemented ZipLex in Scala and verified its correctness, including
invertibility, using the Stainless verifier. Our evaluation demonstrates that
ZipLex supports realistic applications such as JSON processing and lexers of
programming languages. In comparison to other verified lexers (which do not
enforce invertibility), ZipLex is 4x slower than Coqlex and two orders of
magnitude faster than Verbatim++, showing that verified invertibility can be
achieved without prohibitive cost.

</details>


### [3] [CPSLint: A Domain-Specific Language Providing Data Validation and Sanitisation for Industrial Cyber-Physical Systems](https://arxiv.org/abs/2510.18651)
*Uraz Odyurt,Ömer Sayilir,Mariëlle Stoelinga,Vadim Zaytsev*

Main category: cs.PL

TL;DR: CPSLint是一个针对工业信息物理系统的领域特定语言，用于数据预处理，支持类型检查、约束验证、缺失数据填补等操作，为机器学习工作流准备数据。


<details>
  <summary>Details</summary>
Motivation: 工业CPS系统产生大量时间序列数据，这些原始数据通常过于庞大且非结构化，需要数据预处理才能用于机器学习解决方案，如故障检测和识别工作流。

Method: 开发CPSLint领域特定语言，提供类型检查、约束验证和修复功能，支持列级和行级的数据结构推断，包括缺失数据填补和描述性执行阶段提取。

Result: 通过概念验证实现展示了CPSLint的功能，能够有效处理CPS原始数据，为ML辅助的故障检测和识别工作流准备数据。

Conclusion: CPSLint为工业CPS数据预处理提供了一个有效的领域特定解决方案，能够处理相似但存在差异的数据准备需求，支持机器学习工作流的数据消费。

Abstract: Raw datasets are often too large and unstructured to work with directly, and
require a data preparation process. The domain of industrial Cyber-Physical
Systems (CPS) is no exception, as raw data typically consists of large amounts
of time-series data logging the system's status in regular time intervals. Such
data has to be sanity checked and preprocessed to be consumable by data-centric
workflows. We introduce CPSLint, a Domain-Specific Language designed to provide
data preparation for industrial CPS. We build up on the fact that many raw data
collections in the CPS domain require similar actions to render them suitable
for Machine-Learning (ML) solutions, e.g., Fault Detection and Identification
(FDI) workflows, yet still vary enough to hope for one universally applicable
solution.
  CPSLint's main features include type checking and enforcing constraints
through validation and remediation for data columns, such as imputing missing
data from surrounding rows. More advanced features cover inference of extra
CPS-specific data structures, both column-wise and row-wise. For instance, as
row-wise structures, descriptive execution phases are an effective method of
data compartmentalisation are extracted and prepared for ML-assisted FDI
workflows. We demonstrate CPSLint's features through a proof of concept
implementation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis](https://arxiv.org/abs/2510.17852)
*Yuze Sun,Wentao Luo,Yanfei Xiang,Jiancheng Pan,Jiahao Li,Quan Zhang,Xiaomeng Huang*

Main category: cs.DC

TL;DR: 该论文提出了一个将大气和海洋AI模型从PyTorch迁移到MindSpore并针对中国芯片优化的框架，旨在减少对GPU的依赖，提高硬件独立性。


<details>
  <summary>Details</summary>
Motivation: 当前AI气候和天气研究模型（如FourCastNet和AI-GOMS）严重依赖GPU，限制了硬件独立性，特别是对中国国产硬件和框架的支持不足。

Method: 开发了一个专注于软硬件适配、内存优化和并行化的迁移框架，将模型从PyTorch迁移到MindSpore，并针对中国芯片进行优化。

Result: 实验结果表明，迁移和优化过程保持了模型的原始精度，同时显著减少了系统依赖，通过利用中国芯片作为科学计算的可替代方案提高了运行效率。

Conclusion: 这项工作为在大气和海洋AI模型开发中利用中国国产芯片和框架提供了有价值的见解和实践指导，为实现更大的技术独立性提供了途径。

Abstract: With the growing role of artificial intelligence in climate and weather
research, efficient model training and inference are in high demand. Current
models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware
independence, especially for Chinese domestic hardware and frameworks. To
address this issue, we present a framework for migrating large-scale
atmospheric and oceanic models from PyTorch to MindSpore and optimizing for
Chinese chips, and evaluating their performance against GPUs. The framework
focuses on software-hardware adaptation, memory optimization, and parallelism.
Furthermore, the model's performance is evaluated across multiple metrics,
including training speed, inference speed, model accuracy, and energy
efficiency, with comparisons against GPU-based implementations. Experimental
results demonstrate that the migration and optimization process preserves the
models' original accuracy while significantly reducing system dependencies and
improving operational efficiency by leveraging Chinese chips as a viable
alternative for scientific computing. This work provides valuable insights and
practical guidance for leveraging Chinese domestic chips and frameworks in
atmospheric and oceanic AI model development, offering a pathway toward greater
technological independence.

</details>


### [5] [Efficient Multi-Worker Selection based Distributed Swarm Learning via Analog Aggregation](https://arxiv.org/abs/2510.18152)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.DC

TL;DR: 提出了一种用于分布式群体学习的空中模拟聚合方法DSL-OTA，通过多工作者选择和空中聚合提高通信效率、实现有效合作并确保隐私保护。


<details>
  <summary>Details</summary>
Motivation: 解决分布式学习中有限传输资源和复杂通信环境导致的边缘设备间协作效率低下的问题，特别是在大规模网络中。

Method: 结合多工作者选择策略与空中模拟聚合，使基于单一最佳工作者的标准DSL变得更加联邦化，同时保护聚合过程免受数据泄露风险。

Result: 理论分析验证了DSL-OTA在快速收敛和低通信成本方面的优势，仿真结果显示在异构和同构数据集设置下均优于现有方法。

Conclusion: DSL-OTA方法有效提升了分布式群体学习的通信效率和协作效果，同时确保了隐私保护。

Abstract: Recent advances in distributed learning systems have introduced effective
solutions for implementing collaborative artificial intelligence techniques in
wireless communication networks. Federated learning approaches provide a
model-aggregation mechanism among edge devices to achieve collaborative
training, while ensuring data security, communication efficiency, and sharing
computational overheads. On the other hand, limited transmission resources and
complex communication environments remain significant bottlenecks to the
efficient collaborations among edge devices, particularly within large-scale
networks. To address such issues, this paper proposes an over-the-air (OTA)
analog aggregation method designed for the distributed swarm learning (DSL),
termed DSL-OTA, aiming to enhance communication efficiency, enable effective
cooperation, and ensure privacy preserving. Incorporating multi-worker
selection strategy with over-the-air aggregation not only makes the standard
DSL based on single best worker contributing to global model update to become
more federated, but also secures the aggregation from potential risks of data
leakage. Our theoretical analyses verify the advantages of the proposed DSL-OTA
algorithm in terms of fast convergence rate and low communication costs.
Simulation results reveal that our DSL-OTA outperforms the other existing
methods by achieving better learning performance under both homogeneous and
heterogeneous dataset settings.

</details>


### [6] [A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces](https://arxiv.org/abs/2510.18300)
*Ankur Lahiry,Ayush Pokharel,Banooqa Banday,Seth Ockerman,Amal Gueroudji,Mohammad Zaeed,Tanzima Z. Islam,Line Pouchard*

Main category: cs.DC

TL;DR: 提出了一个端到端的并行性能分析框架，用于高效处理多个大规模GPU跟踪数据，通过并行处理和因果图方法显著提升分析效率。


<details>
  <summary>Details</summary>
Motivation: 大规模GPU跟踪数据对于识别异构高性能计算架构中的性能瓶颈至关重要，但单一跟踪数据的庞大体积和复杂性使得性能分析计算成本高且耗时。

Method: 开发了一个端到端并行性能分析框架，通过并发分区和处理跟踪数据，采用因果图方法和并行协调图来揭示执行流中的性能变异性和依赖关系。

Result: 实验结果显示在可扩展性方面实现了67%的改进，证明了该流水线在独立分析多个跟踪时的有效性。

Conclusion: 该并行性能分析框架能够高效处理大规模GPU跟踪数据，显著提升分析效率，为异构HPC架构的性能优化提供了有效工具。

Abstract: Large-scale GPU traces play a critical role in identifying performance
bottlenecks within heterogeneous High-Performance Computing (HPC)
architectures. However, the sheer volume and complexity of a single trace of
data make performance analysis both computationally expensive and
time-consuming. To address this challenge, we present an end-to-end parallel
performance analysis framework designed to handle multiple large-scale GPU
traces efficiently. Our proposed framework partitions and processes trace data
concurrently and employs causal graph methods and parallel coordinating chart
to expose performance variability and dependencies across execution flows.
Experimental results demonstrate a 67% improvement in terms of scalability,
highlighting the effectiveness of our pipeline for analyzing multiple traces
independently.

</details>


### [7] [SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices](https://arxiv.org/abs/2510.18544)
*Pan Zhou,Yiming Lei,Ling Liu,Xiaoqiong Xu,Ying Cai,Daji Ergu,Hongfang Yu,Yueyue Dai*

Main category: cs.DC

TL;DR: SLICE是一种针对边缘计算场景的LLM推理调度解决方案，通过结合效用最大化请求调度算法和动态迭代生成速率控制机制，显著提高了SLO达成率。


<details>
  <summary>Details</summary>
Motivation: 现有调度系统仅以最大化输出token吞吐量为优化目标，无法满足边缘设备对TTFT、TPOT和端到端延迟等多样化SLO要求，导致SLO违规率居高不下。

Method: 提出SLICE调度方案，结合效用最大化请求调度算法和动态迭代生成速率控制机制。

Result: 相比最先进解决方案Orca和FastServe，SLICE实现了高达35倍的SLO达成率提升和3.4倍的任务完成时间优势。

Conclusion: SLICE能有效解决边缘计算场景中LLM推理服务的多样化SLO需求问题，显著提升服务质量。

Abstract: Large Language Models (LLMs), as the foundational architecture for
next-generation interactive AI applications, not only power intelligent
dialogue systems but also drive the evolution of embodied intelligence on edge
devices, including humanoid robots, smart vehicles, and other scenarios. The
applications running on these edge devices impose differentiated Service Level
Objectives (SLO) requirements on LLM services, specifically manifested as
distinct constraints on Time to First Token (TTFT) and Time Per Output Token
(TPOT) as well as end-to-end latency. Notably, edge devices typically handle
real-time tasks that are extremely sensitive to latency, such as machine
control and navigation planning. However, existing scheduling service systems
still prioritize maximizing output token throughput as the sole optimization
objective, failing to adequately address the diversity of SLO requirements.
This ultimately results in persistently high violation rates for end-to-end
latency or TPOT related SLOs.
  This paper proposes SLICE, an innovative scheduling solution designed for
edge computing scenarios with differentiated SLO requirements. By combining a
utility-maximizing request scheduling algorithm with a dynamic iterative
control mechanism for generation rates, SLICE significantly improves LLM
inference service SLO attainment. Experimental results demonstrate that
compared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to
35x higher SLO attainment and 3.4x advantage in task completion time than the
other two solutions.

</details>


### [8] [Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications](https://arxiv.org/abs/2510.18586)
*Zhuohang Bian,Feiyang Wu,Teng Ma,Youwei Zhuo*

Main category: cs.DC

TL;DR: Tokencake是一个针对多智能体应用中KV缓存优化的服务框架，通过空间调度和时间调度解决缓存争用和内存闲置问题，显著降低延迟并提高GPU内存利用率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多智能体应用中广泛使用外部函数调用，导致KV缓存面临严重性能挑战：空间争用导致关键智能体缓存被驱逐，时间利用不足导致等待函数调用的智能体缓存闲置在GPU内存中。

Method: Tokencake采用智能体感知设计，空间调度器使用动态内存分区保护关键智能体免受争用，时间调度器采用主动卸载和预测性上传机制，在函数调用停滞期间重新利用GPU内存。

Result: 在代表性多智能体基准测试中，Tokencake相比vLLM可将端到端延迟降低超过47.06%，有效GPU内存利用率提高达16.9%。

Conclusion: Tokencake通过协同优化调度和内存管理的智能体感知设计，有效解决了多智能体应用中KV缓存的性能瓶颈问题。

Abstract: Large Language Models (LLMs) are increasingly deployed in complex multi-agent
applications that use external function calls. This workload creates severe
performance challenges for the KV Cache: space contention leads to the eviction
of critical agents' caches and time underutilization leaves the cache of agents
stalled on long-running tool calls idling in GPU memory. We present Tokencake,
a KV-Cache-centric serving framework that co-optimizes scheduling and memory
management with an agent-aware design. Tokencake's Space Scheduler uses dynamic
memory partitioning to shield critical agents from contention, while its Time
Scheduler employs a proactive offload and predictive upload mechanism to
repurpose GPU memory during function call stalls. Our evaluation on
representative multi-agent benchmarks shows that Tokencake can reduce
end-to-end latency by over 47.06%, improve effective GPU memory utilization by
up to 16.9% compared to vLLM.

</details>


### [9] [Distributed Interactive Proofs for Planarity with Log-Star Communication](https://arxiv.org/abs/2510.18592)
*Yuval Gil,Merav Parter*

Main category: cs.DC

TL;DR: 提出了新的通信高效分布式交互证明协议用于平面性验证，实现了O(log*n)轮交互和O(1)证明大小的嵌入式平面性证明。


<details>
  <summary>Details</summary>
Motivation: 解决分布式系统中平面性验证的通信效率问题，减少证明者与验证者之间的通信开销。

Method: 设计分布式交互证明协议，通过多轮交互和优化的证明大小来验证图的平面性特性。

Result: 实现了嵌入式平面性验证的O(log*n)轮交互和O(1)证明大小，以及一般平面性验证的O(⌈logΔ/log*n⌉)证明大小。

Conclusion: 该协议显著提高了分布式平面性验证的通信效率，为分布式图性质验证提供了新的解决方案。

Abstract: We provide new communication-efficient distributed interactive proofs for
planarity. The notion of a \emph{distributed interactive proof (DIP)} was
introduced by Kol, Oshman, and Saxena (PODC 2018). In a DIP, the \emph{prover}
is a single centralized entity whose goal is to prove a certain claim regarding
an input graph $G$. To do so, the prover communicates with a distributed
\emph{verifier} that operates concurrently on all $n$ nodes of $G$. A DIP is
measured by the amount of prover-verifier communication it requires. Namely,
the goal is to design a DIP with a small number of interaction rounds and a
small \emph{proof size}, i.e., a small amount of communication per round. Our
main result is an $O(\log ^{*}n)$-round DIP protocol for embedded planarity and
planarity with a proof size of $O(1)$ and $O(\lceil\log \Delta/\log
^{*}n\rceil)$, respectively. In fact, this result can be generalized as
follows. For any $1\leq r\leq \log^{*}n$, there exists an $O(r)$-round protocol
for embedded planarity and planarity with a proof size of $O(\log ^{(r)}n)$ and
$O(\log ^{(r)}n+\log \Delta /r)$, respectively.

</details>


### [10] [Towards an Optimized Benchmarking Platform for CI/CD Pipelines](https://arxiv.org/abs/2510.18640)
*Nils Japke,Sebastian Koch,Helmut Lukasczyk,David Bermbach*

Main category: cs.DC

TL;DR: 论文指出性能基准测试优化在CI/CD系统中应用不足，提出了三个关键挑战：优化策略组合性、结果自动评估、实际应用复杂性，并提出了云基准测试框架概念。


<details>
  <summary>Details</summary>
Motivation: 大型软件系统的性能回归会导致资源效率低下，需要频繁基准测试来检测，但现有基准测试方法资源消耗大、耗时长，难以集成到CI/CD流水线中。

Method: 识别了基准测试优化的三个核心挑战：优化策略的组合性、基准测试结果的自动评估、在CI/CD系统中应用策略的可用性和复杂性，并提出了云基准测试框架概念。

Result: 提出了基准测试优化领域的关键开放问题，旨在推动研究使CI/CD系统中的性能回归检测更加实用和有效。

Conclusion: 基准测试优化领域在关键方面仍未被充分探索，阻碍了其广泛应用，需要进一步研究解决组合性、自动评估和实际应用复杂性等挑战。

Abstract: Performance regressions in large-scale software systems can lead to
substantial resource inefficiencies, making their early detection critical.
Frequent benchmarking is essential for identifying these regressions and
maintaining service-level agreements (SLAs). Performance benchmarks, however,
are resource-intensive and time-consuming, which is a major challenge for
integration into Continuous Integration / Continuous Deployment (CI/CD)
pipelines. Although numerous benchmark optimization techniques have been
proposed to accelerate benchmark execution, there is currently no practical
system that integrates these optimizations seamlessly into real-world CI/CD
pipelines. In this vision paper, we argue that the field of benchmark
optimization remains under-explored in key areas that hinder its broader
adoption. We identify three central challenges to enabling frequent and
efficient benchmarking: (a) the composability of benchmark optimization
strategies, (b) automated evaluation of benchmarking results, and (c) the
usability and complexity of applying these strategies as part of CI/CD systems
in practice. We also introduce a conceptual cloud-based benchmarking framework
handling these challenges transparently. By presenting these open problems, we
aim to stimulate research toward making performance regression detection in
CI/CD systems more practical and effective.

</details>


### [11] [PCMS: Parallel Coupler For Multimodel Simulations](https://arxiv.org/abs/2510.18838)
*Jacob S. Merson,Cameron W. Smith,Mark S. Shephard,Fuad Hasan,Abhiyan Paudel,Angel Castillo-Crooke,Joyal Mathew,Mohammad Elahi*

Main category: cs.DC

TL;DR: PCMS是一个新的GPU加速通用耦合框架，用于在超级计算机上耦合模拟代码，支持高达五维的分布式控制和场映射方法。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在领导级超级计算机上高效耦合多个模拟代码的框架，以支持复杂的多物理场模拟。

Method: PCMS采用GPU加速的分布式控制和场映射方法，利用离散化和场信息来满足物理约束，支持高达五维的耦合。

Result: 成功演示了XGC与DEGAS2的耦合，以及GNET与GTC的5D分布函数耦合，在Frontier的2,080个GPU上实现了85%的弱扩展效率。

Conclusion: PCMS是一个高效的多模型耦合框架，能够在大型GPU集群上实现高性能的复杂模拟代码耦合。

Abstract: This paper presents the Parallel Coupler for Multimodel Simulations (PCMS), a
new GPU accelerated generalized coupling framework for coupling simulation
codes on leadership class supercomputers. PCMS includes distributed control and
field mapping methods for up to five dimensions. For field mapping PCMS can
utilize discretization and field information to accommodate physics
constraints. PCMS is demonstrated with a coupling of the gyrokinetic
microturbulence code XGC with a Monte Carlo neutral transport code DEGAS2 and
with a 5D distribution function coupling of an energetic particle transport
code (GNET) to a gyrokinetic microturbulence code (GTC). Weak scaling is also
demonstrated on up to 2,080 GPUs of Frontier with a weak scaling efficiency of
85%.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [From Quarter to All: Accelerating Speculative LLM Decoding via Floating-Point Exponent Remapping and Parameter Sharing](https://arxiv.org/abs/2510.18525)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: SPEQ是一种算法-硬件协同设计的推测解码方法，使用全模型的部分权重位形成量化草稿模型，无需额外训练或存储开销，在15个LLM和任务上实现了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能优异，但由于参数规模大导致推理延迟高。量化可以减少模型大小但会带来性能下降，而推测解码虽然无损但通常会产生额外开销。

Method: 提出SPEQ算法-硬件协同设计方法，使用全模型的部分权重位构建量化草稿模型，通过可重构处理单元阵列高效执行草稿和验证过程。

Result: 在15个LLM和任务上的实验结果显示，SPEQ相比FP16、Olive和Tender分别实现了2.07倍、1.53倍和1.45倍的加速。

Conclusion: SPEQ通过算法-硬件协同设计，在不增加额外训练或存储开销的情况下，有效降低了大型语言模型的推理延迟，实现了显著的性能提升。

Abstract: Large language models achieve impressive performance across diverse tasks but
exhibit high inference latency due to their large parameter sizes. While
quantization reduces model size, it often leads to performance degradation
compared to the full model. Speculative decoding remains lossless but typically
incurs extra overheads. We propose SPEQ, an algorithm-hardware co-designed
speculative decoding method that uses part of the full-model weight bits to
form a quantized draft model, thereby eliminating additional training or
storage overhead. A reconfigurable processing element array enables efficient
execution of both the draft and verification passes. Experimental results
across 15 LLMs and tasks demonstrate that SPEQ achieves speedups of 2.07x,
1.53x, and 1.45x compared over FP16, Olive, and Tender, respectively.

</details>
