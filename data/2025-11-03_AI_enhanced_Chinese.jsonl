{"id": "2510.26913", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.26913", "abs": "https://arxiv.org/abs/2510.26913", "authors": ["Junyi Shen", "Noppanat Wadlom", "Lingfeng Zhou", "Dequan Wang", "Xu Miao", "Lei Fang", "Yao Lu"], "title": "FlowMesh: A Service Fabric for Composable LLM Workflows", "comment": null, "summary": "AI deployment increasingly resembles a pipeline of data transformation,\nfine-tuning, and agent interactions rather than a monolithic LLM job; recent\nexamples include RLHF/RLAIF training and agentic workflows. To cope with this\nshift, we propose FlowMesh, a multi-tenant service fabric that executes and\noptimizes these workloads as one shared service instead of isolated pipelines.\nIt decomposes workflows into fine-grained operators with recorded lineage,\nenabling de-duplication of work across users and batching requests on the same\nhardware while preserving per-workflow provenance. A global control plane\nmaintains a cluster-wide pool of ready operators and uses a single utility\nfunction to pick both the batch and the worker, balancing throughput, cost, and\ndata locality on heterogeneous GPUs. The data plane is an elastic fleet of\nstateless workers backed by a content-addressable store, enabling rapid,\nautomatic scale-out, safe retry after preemption, and portability across\nmanaged clusters such as Kubernetes and geo-distributed GPU marketplaces such\nas Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost\nreduction and 2.0x lower energy usage, provides a similar or better latency\nprofile, and remains efficient under dynamic and failure-prone conditions.", "AI": {"tldr": "FlowMesh\u662f\u4e00\u4e2a\u591a\u79df\u6237\u670d\u52a1\u67b6\u6784\uff0c\u5c06AI\u5de5\u4f5c\u6d41\u4f5c\u4e3a\u5171\u4eab\u670d\u52a1\u800c\u975e\u72ec\u7acb\u7ba1\u9053\u6267\u884c\u548c\u4f18\u5316\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7b97\u5b50\u5206\u89e3\u3001\u53bb\u91cd\u548c\u6279\u5904\u7406\u5b9e\u73b0\u6210\u672c\u964d\u4f4e\u548c\u80fd\u6548\u63d0\u5347\u3002", "motivation": "AI\u90e8\u7f72\u6b63\u4ece\u5355\u4e00LLM\u4efb\u52a1\u8f6c\u5411\u6570\u636e\u8f6c\u6362\u3001\u5fae\u8c03\u548c\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u6d41\u6c34\u7ebf\u6a21\u5f0f\uff0c\u9700\u8981\u5e94\u5bf9\u8fd9\u79cd\u8f6c\u53d8\u5e26\u6765\u7684\u6548\u7387\u548c\u6210\u672c\u6311\u6218\u3002", "method": "\u5c06\u5de5\u4f5c\u6d41\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u7b97\u5b50\u5e76\u8bb0\u5f55\u8840\u7f18\u5173\u7cfb\uff0c\u901a\u8fc7\u5168\u5c40\u63a7\u5236\u5e73\u9762\u7ba1\u7406\u7b97\u5b50\u6c60\uff0c\u4f7f\u7528\u5355\u4e00\u6548\u7528\u51fd\u6570\u9009\u62e9\u6279\u6b21\u548c\u5de5\u4f5c\u8282\u70b9\uff0c\u6570\u636e\u5e73\u9762\u91c7\u7528\u65e0\u72b6\u6001\u5de5\u4f5c\u8282\u70b9\u548c\u5185\u5bb9\u5bfb\u5740\u5b58\u50a8\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6848\uff0cFlowMesh\u5b9e\u73b0\u9ad8\u8fbe3.8\u500d\u6210\u672c\u964d\u4f4e\u548c2.0\u500d\u80fd\u8017\u964d\u4f4e\uff0c\u63d0\u4f9b\u76f8\u4f3c\u6216\u66f4\u597d\u7684\u5ef6\u8fdf\u6027\u80fd\uff0c\u5728\u52a8\u6001\u548c\u6613\u6545\u969c\u73af\u5883\u4e0b\u4fdd\u6301\u9ad8\u6548\u3002", "conclusion": "FlowMesh\u901a\u8fc7\u5171\u4eab\u670d\u52a1\u67b6\u6784\u6709\u6548\u4f18\u5316\u4e86\u73b0\u4ee3AI\u5de5\u4f5c\u6d41\uff0c\u5728\u6210\u672c\u3001\u80fd\u8017\u548c\u6027\u80fd\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.27039", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27039", "abs": "https://arxiv.org/abs/2510.27039", "authors": ["Zhuo Zheng", "Lingran Meng", "Ziyu Lin"], "title": "A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration", "comment": null, "summary": "Accurate traffic flow forecasting is essential for the development of\nintelligent transportation systems (ITS), supporting tasks such as traffic\nsignal optimization, congestion management, and route planning. Traditional\nmodels often fail to effectively capture complex spatial-temporal dependencies\nin large-scale road networks, especially under the influence of external\nfactors such as weather, holidays, and traffic accidents. To address this\nchallenge, this paper proposes a cloud-based hybrid model that integrates\nSpatio-Temporal Graph Neural Networks (ST-GNN) with a Transformer architecture\nfor traffic flow prediction. The model leverages the strengths of GNNs in\nmodeling spatial correlations across road networks and the Transformers'\nability to capture long-term temporal dependencies. External contextual\nfeatures are incorporated via feature fusion to enhance predictive accuracy.\nThe proposed model is deployed on a cloud computing platform to achieve\nscalability and real-time adaptability. Experimental evaluation of the dataset\nshows that our model outperforms baseline methods (LSTM, TCN, GCN, pure\nTransformer) with an RMSE of only 17.92 and a MAE of only 10.53. These findings\nsuggest that the hybrid GNN-Transformer approach provides an effective and\nscalable solution for cloud-based ITS applications, offering methodological\nadvancements for traffic flow forecasting and practical implications for\ncongestion mitigation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e91\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u67b6\u6784\u8fdb\u884c\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\uff0c\u5728\u4e91\u5e73\u53f0\u4e0a\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u65f6\u9002\u5e94\u6027", "motivation": "\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u6709\u6548\u6355\u6349\u5927\u89c4\u6a21\u8def\u7f51\u4e2d\u7684\u590d\u6742\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5728\u5929\u6c14\u3001\u8282\u5047\u65e5\u3001\u4ea4\u901a\u4e8b\u6545\u7b49\u5916\u90e8\u56e0\u7d20\u5f71\u54cd\u4e0b", "method": "\u96c6\u6210\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\uff08ST-GNN\uff09\u548cTransformer\u67b6\u6784\uff0c\u5229\u7528GNN\u5efa\u6a21\u9053\u8def\u7f51\u7edc\u7a7a\u95f4\u76f8\u5173\u6027\uff0cTransformer\u6355\u6349\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u878d\u5408\u6574\u5408\u5916\u90e8\u4e0a\u4e0b\u6587\u7279\u5f81", "result": "\u5728\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6a21\u578b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08LSTM\u3001TCN\u3001GCN\u3001\u7eafTransformer\uff09\uff0cRMSE\u4ec5\u4e3a17.92\uff0cMAE\u4ec5\u4e3a10.53", "conclusion": "\u6df7\u5408GNN-Transformer\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u4e91\u7684\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u8fdb\u5c55\uff0c\u5e76\u4e3a\u62e5\u5835\u7f13\u89e3\u63d0\u4f9b\u4e86\u5b9e\u9645\u610f\u4e49"}}
{"id": "2510.27257", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27257", "abs": "https://arxiv.org/abs/2510.27257", "authors": ["Mengshi Qi", "Jiaxuan Peng", "Jie Zhang", "Juan Zhu", "Yong Li", "Huadong Ma"], "title": "Synergistic Tensor and Pipeline Parallelism", "comment": null, "summary": "In the machine learning system, the hybrid model parallelism combining tensor\nparallelism (TP) and pipeline parallelism (PP) has become the dominant solution\nfor distributed training of Large Language Models~(LLMs) and Multimodal LLMs\n(MLLMs). However, TP introduces significant collective communication overheads,\nwhile PP suffers from synchronization inefficiencies such as pipeline bubbles.\nExisting works primarily address these challenges from isolated perspectives,\nfocusing either on overlapping TP communication or on flexible PP scheduling to\nmitigate pipeline bubbles. In this paper, we propose a new synergistic tensor\nand pipeline parallelism schedule that simultaneously reduces both types of\nbubbles. Our proposed schedule decouples the forward and backward passes in PP\ninto fine-grained computation units, which are then braided to form a composite\ncomputation sequence. This compositional structure enables near-complete\nelimination of TP-related bubbles. Building upon this structure, we further\ndesign the PP schedule to minimize PP bubbles. Experimental results demonstrate\nthat our approach improves training throughput by up to 12% for LLMs and 16%\nfor MLLMs compared to existing scheduling methods. Our source code is avaiable\nat https://github.com/MICLAB-BUPT/STP.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u534f\u540c\u7684\u5f20\u91cf\u5e76\u884c\u548c\u6d41\u6c34\u7ebf\u5e76\u884c\u8c03\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u524d\u5411\u548c\u540e\u5411\u4f20\u64ad\u89e3\u8026\u4e3a\u7ec6\u7c92\u5ea6\u8ba1\u7b97\u5355\u5143\u5e76\u4ea4\u7ec7\u7f16\u6392\uff0c\u540c\u65f6\u51cf\u5c11\u4e24\u79cd\u5e76\u884c\u65b9\u5f0f\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\u548c\u6d41\u6c34\u7ebf\u6c14\u6ce1\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u5e76\u884c\u8bad\u7ec3\u65b9\u6cd5\u4e2d\uff0c\u5f20\u91cf\u5e76\u884c\u5f15\u5165\u663e\u8457\u7684\u96c6\u4f53\u901a\u4fe1\u5f00\u9500\uff0c\u6d41\u6c34\u7ebf\u5e76\u884c\u5b58\u5728\u540c\u6b65\u6548\u7387\u4f4e\u4e0b\u7684\u6d41\u6c34\u7ebf\u6c14\u6ce1\u95ee\u9898\uff0c\u800c\u73b0\u6709\u5de5\u4f5c\u5f80\u5f80\u53ea\u4ece\u5b64\u7acb\u89d2\u5ea6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5c06\u6d41\u6c34\u7ebf\u5e76\u884c\u7684\u524d\u5411\u548c\u540e\u5411\u4f20\u64ad\u89e3\u8026\u4e3a\u7ec6\u7c92\u5ea6\u8ba1\u7b97\u5355\u5143\uff0c\u7136\u540e\u4ea4\u7ec7\u7f16\u6392\u5f62\u6210\u590d\u5408\u8ba1\u7b97\u5e8f\u5217\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u8bbe\u8ba1\u6d41\u6c34\u7ebf\u5e76\u884c\u8c03\u5ea6\u4ee5\u6700\u5c0f\u5316\u6d41\u6c34\u7ebf\u6c14\u6ce1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u8c03\u5ea6\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5bf9LLM\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\u6700\u9ad8\u8fbe12%\uff0c\u5bf9MLLM\u63d0\u5347\u6700\u9ad8\u8fbe16%\u3002", "conclusion": "\u63d0\u51fa\u7684\u534f\u540c\u8c03\u5ea6\u65b9\u6cd5\u80fd\u6709\u6548\u540c\u65f6\u51cf\u5c11\u5f20\u91cf\u5e76\u884c\u548c\u6d41\u6c34\u7ebf\u5e76\u884c\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\u548c\u540c\u6b65\u5ef6\u8fdf\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2510.27289", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27289", "abs": "https://arxiv.org/abs/2510.27289", "authors": ["Zhengchang Hua", "Panagiotis Oikonomou", "Karim Djemame", "Nikos Tziritas", "Georgios Theodoropoulos"], "title": "A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination", "comment": "16 pages, 8 figures. Accepted by the 25th International Conference on\n  Algorithms and Architectures for Parallel Processing (ICA3PP'25)", "summary": "The coordination of large-scale, decentralised systems, such as a fleet of\nElectric Vehicles (EVs) in a Vehicle-to-Grid (V2G) network, presents a\nsignificant challenge for modern control systems. While collaborative Digital\nTwins have been proposed as a solution to manage such systems without\ncompromising the privacy of individual agents, deriving globally optimal\ncontrol policies from the high-level information they share remains an open\nproblem. This paper introduces Digital Twin Assisted Multi-Agent Deep\nDeterministic Policy Gradient (DT-MADDPG) algorithm, a novel hybrid\narchitecture that integrates a multi-agent reinforcement learning framework\nwith a collaborative DT network. Our core contribution is a simulation-assisted\nlearning algorithm where the centralised critic is enhanced by a predictive\nglobal model that is collaboratively built from the privacy-preserving data\nshared by individual DTs. This approach removes the need for collecting\nsensitive raw data at a centralised entity, a requirement of traditional\nmulti-agent learning algorithms. Experimental results in a simulated V2G\nenvironment demonstrate that DT-MADDPG can achieve coordination performance\ncomparable to the standard MADDPG algorithm while offering significant\nadvantages in terms of data privacy and architectural decentralisation. This\nwork presents a practical and robust framework for deploying intelligent,\nlearning-based coordination in complex, real-world cyber-physical systems.", "AI": {"tldr": "\u63d0\u51faDT-MADDPG\u7b97\u6cd5\uff0c\u5c06\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e0e\u534f\u4f5c\u6570\u5b57\u5b6a\u751f\u7f51\u7edc\u7ed3\u5408\uff0c\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u7535\u52a8\u6c7d\u8f66V2G\u7f51\u7edc\u7684\u534f\u8c03\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\uff08\u5982\u7535\u52a8\u6c7d\u8f66V2G\u7f51\u7edc\uff09\u7684\u534f\u8c03\u63a7\u5236\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u4e2a\u4f53\u667a\u80fd\u4f53\u7684\u9690\u79c1\uff0c\u907f\u514d\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u7b97\u6cd5\u9700\u8981\u96c6\u4e2d\u6536\u96c6\u654f\u611f\u539f\u59cb\u6570\u636e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6570\u5b57\u5b6a\u751f\u8f85\u52a9\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\uff08DT-MADDPG\uff09\uff0c\u901a\u8fc7\u534f\u4f5c\u6784\u5efa\u7684\u9884\u6d4b\u5168\u5c40\u6a21\u578b\u589e\u5f3a\u96c6\u4e2d\u8bc4\u8bba\u5bb6\uff0c\u4f7f\u7528\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u800c\u975e\u539f\u59cb\u6570\u636e\u3002", "result": "\u5728\u6a21\u62dfV2G\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDT-MADDPG\u80fd\u8fbe\u5230\u4e0e\u6807\u51c6MADDPG\u7b97\u6cd5\u76f8\u5f53\u7684\u534f\u8c03\u6027\u80fd\uff0c\u540c\u65f6\u5728\u6570\u636e\u9690\u79c1\u548c\u67b6\u6784\u53bb\u4e2d\u5fc3\u5316\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5728\u590d\u6742\u73b0\u5b9e\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u4e2d\u90e8\u7f72\u57fa\u4e8e\u5b66\u4e60\u7684\u667a\u80fd\u534f\u8c03\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u9c81\u68d2\u7684\u6846\u67b6\u3002"}}
{"id": "2510.26944", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.26944", "abs": "https://arxiv.org/abs/2510.26944", "authors": ["Hoa Nguyen", "Pongstorn Maidee", "Jason Lowe-Power", "Alireza Kaviani"], "title": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache Hierarchies", "comment": null, "summary": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios.", "AI": {"tldr": "Choreographer\u662f\u4e00\u4e2a\u7528\u4e8e\u7ec6\u7c92\u5ea6\u52a0\u901f\u5668\u7cfb\u7edf\u7ea7\u8bc4\u4f30\u7684\u4eff\u771f\u6846\u67b6\uff0c\u80fd\u591f\u51c6\u786e\u6355\u6349\u786c\u4ef6\u8f6f\u4ef6\u4ea4\u4e92\u5f00\u9500\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u65e0\u6cd5\u5168\u9762\u6355\u6349\u6838\u5fc3-\u52a0\u901f\u5668\u548c\u7f13\u5b58-\u52a0\u901f\u5668\u4ea4\u4e92\u4e2d\u7684\u786c\u4ef6\u8f6f\u4ef6\u5f00\u9500\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u7cfb\u7edf\u7ea7\u8bc4\u4f30\u5de5\u5177\u6765\u4f18\u5316\u5ef6\u8fdf\u654f\u611f\u4efb\u52a1\u7684\u52a0\u901f\u5668\u8bbe\u8ba1\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8egem5\u7684\u8be6\u7ec6\u786c\u4ef6\u6808\uff08\u542bAMBA CHI\u7f51\u72b6\u7f51\u7edc\uff09\u548c\u5b8c\u6574Linux\u8f6f\u4ef6\u6808\uff0c\u63d0\u4f9bC++ API\u548c\u6a21\u5757\u5316\u914d\u7f6e\u9009\u9879\uff0c\u5305\u542b\u7cbe\u786e\u7684\u7f13\u5b58\u6a21\u578b\u6765\u6355\u6349\u6027\u80fd\u53d8\u5316\u3002", "result": "\u6570\u636e\u611f\u77e5\u9884\u53d6\u5668\u5728\u56fe\u5f62\u5206\u6790\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u5b9e\u73b01.08x-1.88x\u52a0\u901f\uff0c\u5feb\u901f\u6392\u5e8f\u52a0\u901f\u5668\u5b9e\u73b0\u8d85\u8fc72x\u52a0\u901f\u4e14\u5730\u5740\u8f6c\u6362\u5f00\u9500\u6700\u5c0f\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "Choreographer\u80fd\u591f\u51c6\u786e\u5efa\u6a21\u590d\u6742\u786c\u4ef6\u8f6f\u4ef6\u4ea4\u4e92\uff0c\u5728\u5c0f\u4efb\u52a1\u5378\u8f7d\u573a\u666f\u4e2d\u4f18\u5316\u6027\u80fd\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u52a0\u901f\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7cfb\u7edf\u7ea7\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2510.27317", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27317", "abs": "https://arxiv.org/abs/2510.27317", "authors": ["Shuyi Chen", "Panagiotis Oikonomou", "Zhengchang Hua", "Nikos Tziritas", "Karim Djemame", "Nan Zhang", "Georgios Theodoropoulos"], "title": "Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing", "comment": "Accepted by the 21st IEEE International Conference on Green Computing\n  and Communications (GreenCom 2025)", "summary": "Multi-access Edge Computing (MEC) delivers low-latency services by hosting\napplications near end-users. To promote sustainability, these systems are\nincreasingly integrated with renewable Energy Harvesting (EH) technologies,\nenabling operation where grid electricity is unavailable. However, balancing\nthe intermittent nature of harvested energy with dynamic user demand presents a\nsignificant resource allocation challenge. This work proposes an online\nstrategy for an MEC system powered exclusively by EH to address this trade-off.\nOur strategy dynamically schedules computational tasks with dependencies and\ngoverns energy consumption through real-time decisions on server frequency\nscaling and service module migration. Experiments using real-world datasets\ndemonstrate our algorithm's effectiveness in efficiently utilizing harvested\nenergy while maintaining low service latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u53ef\u518d\u751f\u80fd\u6e90\u4f9b\u7535\u7684\u591a\u63a5\u5165\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\u7684\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u5ea6\u8ba1\u7b97\u4efb\u52a1\u548c\u63a7\u5236\u80fd\u8017\u6765\u5e73\u8861\u95f4\u6b47\u6027\u53ef\u518d\u751f\u80fd\u6e90\u4e0e\u52a8\u6001\u7528\u6237\u9700\u6c42\u3002", "motivation": "\u591a\u63a5\u5165\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u53ef\u518d\u751f\u80fd\u6e90\u6536\u96c6\u6280\u672f\uff0c\u4f46\u5728\u6ca1\u6709\u7535\u7f51\u4f9b\u7535\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u5e73\u8861\u95f4\u6b47\u6027\u6536\u96c6\u80fd\u6e90\u4e0e\u52a8\u6001\u7528\u6237\u9700\u6c42\u6210\u4e3a\u91cd\u8981\u7684\u8d44\u6e90\u5206\u914d\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5728\u7ebf\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u5ea6\u5177\u6709\u4f9d\u8d56\u5173\u7cfb\u7684\u8ba1\u7b97\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5b9e\u65f6\u51b3\u7b56\u670d\u52a1\u5668\u9891\u7387\u7f29\u653e\u548c\u670d\u52a1\u6a21\u5757\u8fc1\u79fb\u6765\u63a7\u5236\u80fd\u8017\u6d88\u8017\u3002", "result": "\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u6709\u6548\u5229\u7528\u6536\u96c6\u80fd\u6e90\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u670d\u52a1\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u7b56\u7565\u6210\u529f\u89e3\u51b3\u4e86\u53ef\u518d\u751f\u80fd\u6e90\u4f9b\u7535\u7684\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u8d44\u6e90\u5206\u914d\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u80fd\u6548\u548c\u6027\u80fd\u7684\u5e73\u8861\u3002"}}
{"id": "2510.26985", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.26985", "abs": "https://arxiv.org/abs/2510.26985", "authors": ["Mostafa Darvishi"], "title": "Practical Timing Closure in FPGA and ASIC Designs: Methods, Challenges, and Case Studies", "comment": "5 figures, 3 tables", "summary": "This paper presents an in-depth analysis of timing closure challenges and\nconstraints in Field Programmable Gate Arrays (FPGAs) and Application Specific\nIntegrated Circuits (ASICs). We examine core timing principles, architectural\ndistinctions, and design methodologies influencing timing behavior in both\ntechnologies. A case study comparing the Xilinx Kintex UltraScale+ FPGA\n(XCKU040) with a 7nm ASIC highlights practical timing analysis and performance\ntrade-offs. Experimental results show ASICs achieve superior timing of 45ps\nsetup and 35ps hold, while modern FPGAs remain competitive with 180ps setup and\n120ps hold times, validating their suitability for high-performance designs.", "AI": {"tldr": "\u672c\u6587\u6df1\u5165\u5206\u6790FPGA\u548cASIC\u4e2d\u7684\u65f6\u5e8f\u6536\u655b\u6311\u6218\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u6bd4\u8f83Xilinx Kintex UltraScale+ FPGA\u4e0e7nm ASIC\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u663e\u793aASIC\u5728\u65f6\u5e8f\u6027\u80fd\u4e0a\u66f4\u4f18\u4f46FPGA\u4ecd\u5177\u7ade\u4e89\u529b\u3002", "motivation": "\u7814\u7a76FPGA\u548cASIC\u5728\u65f6\u5e8f\u6536\u655b\u65b9\u9762\u7684\u6838\u5fc3\u6311\u6218\u548c\u7ea6\u675f\uff0c\u7406\u89e3\u4e24\u79cd\u6280\u672f\u5728\u65f6\u5e8f\u884c\u4e3a\u4e0a\u7684\u5dee\u5f02\uff0c\u4e3a\u9ad8\u6027\u80fd\u8bbe\u8ba1\u9009\u62e9\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u5206\u6790\u6838\u5fc3\u65f6\u5e8f\u539f\u7406\u3001\u67b6\u6784\u5dee\u5f02\u548c\u8bbe\u8ba1\u65b9\u6cd5\u5b66\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u6bd4\u8f83XCKU040 FPGA\u4e0e7nm ASIC\u7684\u5b9e\u9645\u65f6\u5e8f\u5206\u6790\u548c\u6027\u80fd\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aASIC\u5b9e\u73b045ps\u5efa\u7acb\u65f6\u95f4\u548c35ps\u4fdd\u6301\u65f6\u95f4\u7684\u4f18\u8d8a\u65f6\u5e8f\u6027\u80fd\uff0c\u800c\u73b0\u4ee3FPGA\u8fbe\u5230180ps\u5efa\u7acb\u65f6\u95f4\u548c120ps\u4fdd\u6301\u65f6\u95f4\uff0c\u8bc1\u660eFPGA\u4ecd\u9002\u7528\u4e8e\u9ad8\u6027\u80fd\u8bbe\u8ba1\u3002", "conclusion": "ASIC\u5728\u65f6\u5e8f\u6027\u80fd\u4e0a\u4f18\u4e8eFPGA\uff0c\u4f46\u73b0\u4ee3FPGA\u7684\u65f6\u5e8f\u8868\u73b0\u4ecd\u5177\u6709\u7ade\u4e89\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u9ad8\u6027\u80fd\u8bbe\u8ba1\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2510.27067", "categories": ["cs.PL", "cs.PF", "B.8.2; D.3.4"], "pdf": "https://arxiv.org/pdf/2510.27067", "abs": "https://arxiv.org/abs/2510.27067", "authors": ["Marouane Benbetka", "Merwan Bekkar", "Riyadh Baghdadi", "Martin Kong"], "title": "Dependence-Driven, Scalable Quantum Circuit Mapping with Affine Abstractions", "comment": "To appear in the Proceedings of the 2026 International Symposium on\n  Code Generation and Optimization (CGO 2026)", "summary": "Qubit Mapping is a critical task in Quantum Compilation, as modern Quantum\nProcessing Units (QPUs) are constrained to nearest-neighbor interactions\ndefined by a qubit coupling graph. This compiler pass repairs the connectivity\nof two-qubit gates whose operands are not adjacent by inserting SWAP gates that\nmove the state of qubits between directly connected qubits. Deciding when to\nintroduce SWAPs while minimizing their count is critical because the error in\nquantum programs increases exponentially with the circuit latency, measured in\nnumber of gates along the critical path of the circuit. Prior work for this\nproblem relied on heuristics and exact methods that partition the circuit into\ntwo or more layers, but failed to exploit valuable dependence information in\nany form.\n  This paper introduces a novel qubit mapping algorithm based on the weight of\ntransitive dependences. The introduced mapper models quantum circuits with\naffine abstractions thereby yielding the ability to compute transitive\ndependences. In turn, the newfound information is used to partition circuits by\ndependence distances and compute, efficiently, distinct weights for each layer.\nWe evaluate the efficiency of our mapper on IBM and Rigetti QPUs, using the\nlarge datasets from the QUEKO and QASMBench benchmark suites, and against four\nbaseline tools (QMAP, Sabre, Cirq and TKET), demonstrating notable improvements\nin circuit depth and swap count while delivering competitive scalability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f20\u9012\u4f9d\u8d56\u6743\u91cd\u7684\u91cf\u5b50\u6bd4\u7279\u6620\u5c04\u7b97\u6cd5\uff0c\u901a\u8fc7\u4eff\u5c04\u62bd\u8c61\u5efa\u6a21\u91cf\u5b50\u7535\u8def\u6765\u8ba1\u7b97\u4f20\u9012\u4f9d\u8d56\uff0c\u4ece\u800c\u4f18\u5316SWAP\u95e8\u63d2\u5165\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u7535\u8def\u6df1\u5ea6\u548cSWAP\u95e8\u6570\u91cf\u3002", "motivation": "\u73b0\u4ee3\u91cf\u5b50\u5904\u7406\u5668\u53ea\u652f\u6301\u76f8\u90bb\u91cf\u5b50\u6bd4\u7279\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u9700\u8981\u63d2\u5165SWAP\u95e8\u6765\u4fee\u590d\u975e\u76f8\u90bb\u91cf\u5b50\u6bd4\u7279\u95f4\u7684\u8fde\u901a\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4f9d\u8d56\u4fe1\u606f\uff0c\u5bfc\u81f4SWAP\u95e8\u6570\u91cf\u8fc7\u591a\uff0c\u589e\u52a0\u4e86\u7535\u8def\u9519\u8bef\u7387\u3002", "method": "\u4f7f\u7528\u4eff\u5c04\u62bd\u8c61\u5efa\u6a21\u91cf\u5b50\u7535\u8def\uff0c\u8ba1\u7b97\u4f20\u9012\u4f9d\u8d56\u5173\u7cfb\uff0c\u6839\u636e\u4f9d\u8d56\u8ddd\u79bb\u5bf9\u7535\u8def\u8fdb\u884c\u5206\u5c42\uff0c\u5e76\u4e3a\u6bcf\u5c42\u8ba1\u7b97\u4e0d\u540c\u7684\u6743\u91cd\u6765\u4f18\u5316SWAP\u95e8\u63d2\u5165\u7b56\u7565\u3002", "result": "\u5728IBM\u548cRigetti\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u4f7f\u7528QUEKO\u548cQASMBench\u57fa\u51c6\u6d4b\u8bd5\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u76f8\u6bd4QMAP\u3001Sabre\u3001Cirq\u548cTKET\u7b49\u57fa\u7ebf\u5de5\u5177\uff0c\u5728\u7535\u8def\u6df1\u5ea6\u548cSWAP\u95e8\u6570\u91cf\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u57fa\u4e8e\u4f20\u9012\u4f9d\u8d56\u6743\u91cd\u7684\u6620\u5c04\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u4f9d\u8d56\u4fe1\u606f\uff0c\u663e\u8457\u4f18\u5316\u91cf\u5b50\u7535\u8def\u7684\u7f16\u8bd1\u8d28\u91cf\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u7f16\u8bd1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.27351", "categories": ["cs.DC", "65Y05, 65Y10, 90C59, 68T20"], "pdf": "https://arxiv.org/pdf/2510.27351", "abs": "https://arxiv.org/abs/2510.27351", "authors": ["Milena Veneva"], "title": "ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method", "comment": "10 pages, 6 figures, 4 tables, DLCP conference 2025, Moscow, Russia", "summary": "This paper presents a machine learning (ML)-based heuristic for finding the\noptimum sub-system size for the CUDA implementation of the parallel partition\nalgorithm. Computational experiments for different system of linear algebraic\nequation (SLAE) sizes are conducted, and the optimum sub-system size for each\nof them is found empirically. To estimate a model for the sub-system size, we\nperform the k-nearest neighbors (kNN) classification method. Statistical\nanalysis of the results is done. By comparing the predicted values with the\nactual data, the algorithm is deemed to be acceptably good. Next, the heuristic\nis expanded to work for the recursive parallel partition algorithm as well. An\nalgorithm for determining the optimum sub-system size for each recursive step\nis formulated. A kNN model for predicting the optimum number of recursive steps\nfor a particular SLAE size is built.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bfb\u627e\u5e76\u884c\u5206\u533a\u7b97\u6cd5CUDA\u5b9e\u73b0\u7684\u6700\u4f73\u5b50\u7cfb\u7edf\u5927\u5c0f\u3002\u901a\u8fc7kNN\u5206\u7c7b\u65b9\u6cd5\u9884\u6d4b\u6700\u4f73\u5b50\u7cfb\u7edf\u5927\u5c0f\uff0c\u5e76\u5c06\u8be5\u65b9\u6cd5\u6269\u5c55\u5230\u9012\u5f52\u5e76\u884c\u5206\u533a\u7b97\u6cd5\u3002", "motivation": "\u5bfb\u627e\u5e76\u884c\u5206\u533a\u7b97\u6cd5\u5728CUDA\u5b9e\u73b0\u4e2d\u7684\u6700\u4f73\u5b50\u7cfb\u7edf\u5927\u5c0f\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u4f7f\u7528kNN\u5206\u7c7b\u65b9\u6cd5\u9884\u6d4b\u6700\u4f73\u5b50\u7cfb\u7edf\u5927\u5c0f\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u4e0e\u5b9e\u9645\u6570\u636e\u6bd4\u8f83\u9a8c\u8bc1\u7b97\u6cd5\u6548\u679c\u3002\u5c06\u542f\u53d1\u5f0f\u65b9\u6cd5\u6269\u5c55\u5230\u9012\u5f52\u5e76\u884c\u5206\u533a\u7b97\u6cd5\uff0c\u6784\u5efa\u9884\u6d4b\u6700\u4f73\u9012\u5f52\u6b65\u6570\u7684kNN\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u6bd4\u8f83\u9884\u6d4b\u503c\u4e0e\u5b9e\u9645\u6570\u636e\uff0c\u7b97\u6cd5\u8868\u73b0\u826f\u597d\u3002\u6210\u529f\u6784\u5efa\u4e86\u9884\u6d4b\u6700\u4f73\u5b50\u7cfb\u7edf\u5927\u5c0f\u548c\u9012\u5f52\u6b65\u6570\u7684\u6a21\u578b\u3002", "conclusion": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9884\u6d4b\u5e76\u884c\u5206\u533a\u7b97\u6cd5\u7684\u6700\u4f73\u53c2\u6570\u914d\u7f6e\uff0c\u4e3aCUDA\u5b9e\u73b0\u63d0\u4f9b\u4e86\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2510.27070", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.27070", "abs": "https://arxiv.org/abs/2510.27070", "authors": ["Dong Tong"], "title": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review", "comment": null, "summary": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u57fa\u4e8e\u63cf\u8ff0\u7b26\u7684\u5bf9\u8c61\u611f\u77e5\u5185\u5b58\u7cfb\u7edf\u8fdb\u884c\u4e86\u5168\u9762\u8c03\u67e5\uff0c\u8fd9\u79cd\u67b6\u6784\u8303\u5f0f\u65e8\u5728\u5f25\u5408\u786c\u4ef6/\u8f6f\u4ef6\u63a5\u53e3\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u901a\u8fc7\u5c06\u63cf\u8ff0\u7b26\u63d0\u5347\u4e3a\u4e00\u7ea7\u67b6\u6784\u62bd\u8c61\uff0c\u4f7f\u786c\u4ef6\u80fd\u591f\u52a8\u6001\u83b7\u53d6\u548c\u6267\u884c\u8f6f\u4ef6\u5b9a\u4e49\u5bf9\u8c61\u7684\u4e30\u5bcc\u8bed\u4e49\u3002", "motivation": "\u73b0\u4ee3\u8ba1\u7b97\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u56e0\u7f3a\u4e4f\u539f\u751f\u67b6\u6784\u673a\u5236\u6765\u4f20\u64ad\u9ad8\u7ea7\u7a0b\u5e8f\u8bed\u4e49\uff08\u5982\u5bf9\u8c61\u8eab\u4efd\u3001\u8fb9\u754c\u548c\u751f\u547d\u5468\u671f\uff09\u800c\u53d7\u5230\u6839\u672c\u6027\u635f\u5bb3\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u7ed8\u5236\u4e86\u8be5\u65b9\u6cd5\u7684\u6f14\u8fdb\u548c\u73b0\u72b6\uff0c\u5efa\u7acb\u4e86\u5185\u5b58\u5bf9\u8c61\u548c\u63cf\u8ff0\u7b26\u7684\u57fa\u7840\u6982\u5ff5\uff0c\u5e76\u5f15\u5165\u4e86\u63cf\u8ff0\u7b26\u5bfb\u5740\u6a21\u5f0f\u7684\u65b0\u5206\u7c7b\u6cd5\uff0c\u4e3a\u5206\u6790\u548c\u6bd4\u8f83\u4e0d\u540c\u5b9e\u73b0\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6846\u67b6\u3002", "result": "\u7edf\u4e00\u5206\u6790\u63ed\u793a\u4e86\u8be5\u8303\u5f0f\u5982\u4f55\u6574\u4f53\u89e3\u51b3\u5185\u5b58\u4fdd\u62a4\u3001\u7ba1\u7406\u548c\u5904\u7406\u7684\u76f8\u4e92\u5173\u8054\u6311\u6218\u3002\u901a\u8fc7CentroID\u6a21\u578b\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u5176\u6df7\u5408\u6807\u8bb0\u6307\u9488\u7f16\u7801\u548c\u63cf\u8ff0\u7b26\u5904\u7406\u673a\u5236\u5982\u4f55\u4f53\u73b0\u5b9e\u7528\u9ad8\u6548\u7684\u5bf9\u8c61\u611f\u77e5\u8bbe\u8ba1\u8def\u5f84\u3002", "conclusion": "\u660e\u786e\u7684\u5bf9\u8c61\u8bed\u4e49\u8de8\u5c42\u901a\u4fe1\u4e3a\u4e0b\u4e00\u4ee3\u7f13\u5b58\u5c42\u6b21\u7ed3\u6784\u3001\u7edf\u4e00\u865a\u62df\u5185\u5b58\u751a\u81f3128\u4f4d\u67b6\u6784\u63d0\u4f9b\u4e86\u57fa\u7840\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.27656", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.27656", "abs": "https://arxiv.org/abs/2510.27656", "authors": ["Nandor Licker", "Kevin Hu", "Vladimir Zaytsev", "Lequn Chen"], "title": "RDMA Point-to-Point Communication for LLM Systems", "comment": null, "summary": "Emerging Large Language Model (LLM) system patterns, such as disaggregated\ninference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement\nfine-tuning, require flexible point-to-point communication beyond simple\ncollectives. Existing implementations are locked to specific Network Interface\nControllers (NICs), hindering integration into inference engines and\nportability across hardware providers. We present TransferEngine, which bridges\nthe functionality of common NICs to expose a uniform interface. TransferEngine\nexposes one-sided WriteImm operations with a ImmCounter primitive for\ncompletion notification, without ordering assumptions of network transport,\ntransparently managing multiple NICs per GPU. We demonstrate peak throughput of\n400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We\nshowcase TransferEngine through three production systems: (1) KvCache transfer\nfor disaggregated inference with dynamic scaling, (2) RL weight updates\nachieving 1.3 seconds for trillion-parameter models, and (3) MoE\ndispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,\nwith the first viable latencies on EFA. We demonstrate that our portable\npoint-to-point communication complements collectives while avoiding lock-in.", "AI": {"tldr": "TransferEngine\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7f51\u7edc\u63a5\u53e3\u62bd\u8c61\u5c42\uff0c\u89e3\u51b3\u4e86LLM\u7cfb\u7edf\u4e2d\u70b9\u5bf9\u70b9\u901a\u4fe1\u7684\u786c\u4ef6\u9501\u5b9a\u95ee\u9898\uff0c\u652f\u6301\u591a\u79cdNIC\u5e76\u63d0\u4f9b\u9ad8\u6027\u80fd\u901a\u4fe1\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u7cfb\u7edf\u5b9e\u73b0\u88ab\u9501\u5b9a\u5728\u7279\u5b9a\u7f51\u7edc\u63a5\u53e3\u63a7\u5236\u5668(NIC)\u4e0a\uff0c\u963b\u788d\u4e86\u63a8\u7406\u5f15\u64ce\u7684\u96c6\u6210\u548c\u8de8\u786c\u4ef6\u63d0\u4f9b\u5546\u7684\u79fb\u690d\u6027\u3002", "method": "TransferEngine\u6865\u63a5\u5e38\u89c1NIC\u529f\u80fd\uff0c\u66b4\u9732\u7edf\u4e00\u7684\u5355\u8fb9WriteImm\u64cd\u4f5c\u548cImmCounter\u539f\u8bed\u8fdb\u884c\u5b8c\u6210\u901a\u77e5\uff0c\u900f\u660e\u7ba1\u7406\u6bcf\u4e2aGPU\u7684\u591a\u4e2aNIC\u3002", "result": "\u5728NVIDIA ConnectX-7\u548cAWS EFA\u4e0a\u5b9e\u73b0400Gbps\u5cf0\u503c\u541e\u5410\u91cf\uff1b\u652f\u6301\u52a8\u6001\u6269\u5c55\u7684KvCache\u4f20\u8f93\u3001\u4e07\u4ebf\u53c2\u6570\u6a21\u578b1.3\u79d2RL\u6743\u91cd\u66f4\u65b0\u3001\u4ee5\u53ca\u4f18\u4e8eDeepEP\u7684MoE\u8c03\u5ea6/\u7ec4\u5408\u5ef6\u8fdf\u3002", "conclusion": "TransferEngine\u7684\u53ef\u79fb\u690d\u70b9\u5bf9\u70b9\u901a\u4fe1\u8865\u5145\u4e86\u96c6\u5408\u64cd\u4f5c\uff0c\u540c\u65f6\u907f\u514d\u4e86\u786c\u4ef6\u9501\u5b9a\u95ee\u9898\u3002"}}
{"id": "2510.27107", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.27107", "abs": "https://arxiv.org/abs/2510.27107", "authors": ["Zhipeng Liao", "Kunming Shao", "Jiangnan Yu", "Liang Zhao", "Tim Kwang-Ting Cheng", "Chi-Ying Tsui", "Jie Yang", "Mohamad Sawan"], "title": "A Memory-Efficient Retrieval Architecture for RAG-Enabled Wearable Medical LLMs-Agents", "comment": "Accepted by BioCAS2025", "summary": "With powerful and integrative large language models (LLMs), medical AI agents\nhave demonstrated unique advantages in providing personalized medical\nconsultations, continuous health monitoring, and precise treatment plans.\nRetrieval-Augmented Generation (RAG) integrates personal medical documents into\nLLMs by an external retrievable database to address the costly retraining or\nfine-tuning issues in deploying customized agents. While deploying medical\nagents in edge devices ensures privacy protection, RAG implementations impose\nsubstantial memory access and energy consumption during the retrieval stage.\nThis paper presents a hierarchical retrieval architecture for edge RAG,\nleveraging a two-stage retrieval scheme that combines approximate retrieval for\ncandidate set generation, followed by high-precision retrieval on pre-selected\ndocument embeddings. The proposed architecture significantly reduces energy\nconsumption and external memory access while maintaining retrieval accuracy.\nSimulation results show that, under TSMC 28nm technology, the proposed\nhierarchical retrieval architecture has reduced the overall memory access by\nnearly 50% and the computation by 75% compared to pure INT8 retrieval, and the\ntotal energy consumption for 1 MB data retrieval is 177.76 {\\mu}J/query.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8fb9\u7f18RAG\u7684\u5206\u5c42\u68c0\u7d22\u67b6\u6784\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u68c0\u7d22\u65b9\u6848\u663e\u8457\u964d\u4f4e\u80fd\u8017\u548c\u5185\u5b58\u8bbf\u95ee\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u7d22\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u533b\u7597AI\u4ee3\u7406\u65f6\uff0cRAG\u5b9e\u73b0\u4e2d\u68c0\u7d22\u9636\u6bb5\u5e26\u6765\u7684\u9ad8\u5185\u5b58\u8bbf\u95ee\u548c\u80fd\u8017\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u3002", "method": "\u91c7\u7528\u5206\u5c42\u68c0\u7d22\u67b6\u6784\uff0c\u7ed3\u5408\u8fd1\u4f3c\u68c0\u7d22\u751f\u6210\u5019\u9009\u96c6\uff0c\u7136\u540e\u5728\u9884\u9009\u6587\u6863\u5d4c\u5165\u4e0a\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u68c0\u7d22\u7684\u4e24\u9636\u6bb5\u65b9\u6848\u3002", "result": "\u5728TSMC 28nm\u6280\u672f\u4e0b\uff0c\u76f8\u6bd4\u7eafINT8\u68c0\u7d22\uff0c\u603b\u4f53\u5185\u5b58\u8bbf\u95ee\u51cf\u5c11\u8fd150%\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c1175%\uff0c1MB\u6570\u636e\u68c0\u7d22\u7684\u603b\u80fd\u8017\u4e3a177.76\u03bcJ/\u67e5\u8be2\u3002", "conclusion": "\u8be5\u5206\u5c42\u68c0\u7d22\u67b6\u6784\u80fd\u6709\u6548\u964d\u4f4e\u8fb9\u7f18RAG\u7684\u80fd\u8017\u548c\u5185\u5b58\u8bbf\u95ee\uff0c\u540c\u65f6\u7ef4\u6301\u68c0\u7d22\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u533b\u7597AI\u4ee3\u7406\u7684\u90e8\u7f72\u3002"}}
