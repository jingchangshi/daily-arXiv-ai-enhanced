<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Intrinsically Correct Algorithms and Recursive Coalgebras](https://arxiv.org/abs/2512.10748)
*Cass Alexandru,Henning Urbat,Thorsten Wißmann*

Main category: cs.PL

TL;DR: 提出基于良基函子的框架，自动保证递归性，简化递归算法的形式化验证


<details>
  <summary>Details</summary>
Motivation: 递归余代数虽能优雅建模递归算法，但证明其递归性通常复杂且特设，阻碍在证明助手中的形式化

Method: 引入良基函子概念，在良基关系索引的族范畴上工作，证明良基函子的每个余代数都是递归的

Result: 主要理论结果：良基函子的所有余代数自动递归；涵盖排序函数等传统技术；案例研究包括快速排序、欧几里得算法、CYK解析

Conclusion: 提供内在保证递归性的框架，简化递归算法形式化，已在Cubical Agda中实现理论和案例验证

Abstract: Recursive coalgebras provide an elegant categorical tool for modelling recursive algorithms and analysing their termination and correctness. By considering coalgebras over categories of suitably indexed families, the correctness of the corresponding algorithms follows intrinsically just from the type of the computed maps. However, proving recursivity of the underlying coalgebras is non-trivial, and proofs are typically ad hoc. This layer of complexity impedes the formalization of coalgebraically defined recursive algorithms in proof assistants. We introduce a framework for constructing coalgebras which are intrinsically recursive in the sense that the type of the coalgebra guarantees recursivity from the outset. Our approach is based on the novel concept of a well-founded functor on a category of families indexed by a well-founded relation. We show as our main result that every coalgebra for a well-founded functor is recursive, and demonstrate that well-known techniques for proving recursivity and termination such as ranking functions are subsumed by this abstract setup. We present a number of case studies, including Quicksort, the Euclidian algorithm, and CYK parsing. Both the main theoretical result and selected case studies have been formalized in Cubical Agda.

</details>


### [2] [Towards Cumulative Abstract Semantics via Handlers](https://arxiv.org/abs/2512.10861)
*Cade Lueker,Andrew Fox,Bor-Yuh Evan Chang*

Main category: cs.PL

TL;DR: 提出基于作用域效应的累积抽象语义框架，实现模块化控制流分析，支持多种路径/流敏感性和方向/近似策略。


<details>
  <summary>Details</summary>
Motivation: 现有抽象解释框架缺乏真正的灵活性，无法支持不同的路径敏感性、流敏感性、分析方向和近似策略。大多数解释器将语法和语义紧密耦合，导致实现难以模块化。当前模块化设计方法需要复杂的数据结构（如单子变换器），虽然提供模块化但通常笨重难用。

Method: 利用作用域效应在解释器中积累语义片段，定义累积抽象语义。将效应分为两类：语法消除处理程序和领域语义引入处理程序。通过这种分类实现模块化设计，允许从单一解释器创建多个动态求值器和静态分析器。

Result: 展示了使用效应作为工具设计清晰、优雅、模块化抽象解释框架的优势。实现了语法和语义的分离，支持灵活组合不同的分析策略。

Conclusion: 基于作用域效应的累积抽象语义方法为构建模块化抽象解释框架提供了有效途径，能够创建灵活、可组合的分析工具，同时保持代码的简洁性和可维护性。

Abstract: We consider the problem of modularizing control flow in a generic abstract interpretation framework. A generic abstract interpretation framework is not truly flexible if it does not allow interpreting with different path- and flow-sensitivities, by going forwards or backwards, and over- or under-approximately. Most interpreters inherently intertwine syntax and semantics, making the implementation antagonistic to modularity. Current approaches to modular designs require the use of complex data structures (e.g., monad transformers), providing modularity but often proving unwieldy (e.g., lifts). We observe that leveraging scoped effects within an interpreter facilitates the accumulation of semantic fragments against a fixed syntax. In this paper, we define cumulative abstract semantics, illustrating the potential for creating multiple dynamic evaluators and static analyses from one interpreter. This modularity is achieved by grouping effects into two categories: syntax elimination and domain-semantic introduction handlers. Our contribution shows the benefits of using effects as an instrument for designing a clean, elegant, and modular abstract interpretation framework.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [A study of the spectrum resource leasing method based on ERC4907 extension](https://arxiv.org/abs/2512.09942)
*Zhiming Liang,Bin Chen,Litao Ye,Chen Sun,Shuo Wang,Zhe Peng*

Main category: cs.DC

TL;DR: M-ERC4907扩展ERC4907标准，支持多时间槽批量配置和多用户同时授权，解决单用户单时间槽限制，显著降低链上交易和Gas消耗。


<details>
  <summary>Details</summary>
Motivation: ERC4907标准仅支持单用户单时间槽授权，在去中心化多槽调度场景中适用性和效率受限，需要扩展功能以支持更复杂的资源分配需求。

Method: 提出M-ERC4907扩展方法，引入批量配置多个时间槽和同时授权多个用户的新功能，消除ERC4907的刚性顺序授权约束，在Remix开发平台进行实验验证。

Result: 实验结果显示M-ERC4907方法显著减少链上交易和总体Gas消耗，提升了可扩展性和资源分配效率。

Conclusion: M-ERC4907有效解决了ERC4907在多槽调度场景中的局限性，为可租赁NFT提供了更灵活高效的授权机制，适用于复杂的去中心化资源分配应用。

Abstract: The ERC4907 standard enables rentable Non-Fungible Tokens (NFTs) but is limited to single-user, single-time-slot authorization, which severely limits its applicability and efficiency in decentralized multi-slot scheduling scenarios. To address this limitation, this paper proposes Multi-slot ERC4907 (M-ERC4907) extension method. The M-ERC4907 method introduces novel functionalities to support the batch configuration of multiple time slots and simultaneous authorization of multiple users, thereby effectively eliminating the rigid sequential authorization constraint of ERC4907. The experiment was conducted on the Remix development platform. Experimental results show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency.

</details>


### [4] [ELANA: A Simple Energy and Latency Analyzer for LLMs](https://arxiv.org/abs/2512.09946)
*Hung-Yueh Chiang,Bokun Wang,Diana Marculescu*

Main category: cs.DC

TL;DR: ELANA是一个开源的轻量级LLM性能分析工具，用于评估模型大小、KV缓存、延迟（TTFT、TPOT、TTLT）和能耗，支持Hugging Face所有公开模型，适用于多GPU和边缘GPU平台。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在移动边缘设备到云GPU集群等各种硬件平台上的延迟和功耗是主要限制因素，需要基准测试工具来优化模型部署效率和下一代模型开发。

Method: 开发了一个轻量级、学术友好的性能分析工具ELANA，支持分析模型大小、KV缓存大小、预填充延迟（TTFT）、生成延迟（TPOT）和端到端延迟（TTLT），兼容Hugging Face API，提供命令行界面和可选能耗日志。

Result: 开源了ELANA工具，支持所有Hugging Face公开模型，可轻松定制或适配压缩或低比特模型，适用于高效LLM研究或小规模概念验证研究。

Conclusion: ELANA是一个简单实用的LLM性能分析工具，有助于优化模型部署效率和促进高效LLM研究，特别适合学术研究和小规模验证场景。

Abstract: The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.

</details>


### [5] [CloudFix: Automated Policy Repair for Cloud Access Control Policies Using Large Language Models](https://arxiv.org/abs/2512.09957)
*Bethel Hall,Owen Ungaro,William Eiers*

Main category: cs.DC

TL;DR: CloudFix：首个结合形式化方法与LLM的云访问控制策略自动修复框架，通过故障定位、LLM生成修复、SMT验证实现高效策略修复


<details>
  <summary>Details</summary>
Motivation: 云环境中访问控制策略的手动编写和更新容易出错且耗时，可能导致安全漏洞。现有符号分析方法在云访问控制场景下泛化能力有限，而LLM在程序修复中的应用尚未探索用于云访问控制策略修复。

Method: CloudFix结合形式化方法与LLM：1）使用形式化方法进行故障定位，识别策略中的错误语句；2）利用LLM生成潜在修复方案；3）通过SMT求解器验证修复的正确性。

Result: 在282个真实AWS访问控制策略数据集上评估，CloudFix在不同请求规模下均优于基线实现，提高了修复准确率。这是首次将LLM应用于策略修复，展示了LLM在访问控制中的有效性。

Conclusion: CloudFix是首个结合形式化方法与LLM的云访问控制策略自动修复框架，能够高效自动修复云访问控制策略，工具和数据集已公开。

Abstract: Access control policies are vital for securing modern cloud computing, where organizations must manage access to sensitive data across thousands of users in distributed system settings. Cloud administrators typically write and update policies manually, which can be an error-prone and time-consuming process and can potentially lead to security vulnerabilities. Existing approaches based on symbolic analysis have demonstrated success in automated debugging and repairing access control policies; however, their generalizability is limited in the context of cloud-based access control. Conversely, Large Language Models (LLMs) have been utilized for automated program repair; however, their applicability to repairing cloud access control policies remains unexplored. In this work, we introduce CloudFix, the first automated policy repair framework for cloud access control that combines formal methods with LLMs. Given an access control policy and a specification of allowed and denied access requests, CloudFix employs Formal Methods-based Fault Localization to identify faulty statements in the policy and leverages LLMs to generate potential repairs, which are then verified using SMT solvers. To evaluate CloudFix, we curated a dataset of 282 real-world AWS access control policies extracted from forum posts and augmented them with synthetically generated request sets based on real scenarios. Our experimental results show that CloudFix improves repair accuracy over a Baseline implementation across varying request sizes. Our work is the first to leverage LLMs for policy repair, showcasing the effectiveness of LLMs for access control and enabling efficient and automated repair of cloud access control policies. We make our tool Cloudfix and AWS dataset publicly available.

</details>


### [6] [TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0](https://arxiv.org/abs/2512.09961)
*Jinyu Chen,Long Shi,Taotao Wang,Jiaheng Wang,Wei Zhang*

Main category: cs.DC

TL;DR: 本文提出TDC-Cache框架，通过两层架构、深度强化学习缓存优化和PoCL共识机制，解决Web3.0去中心化数据访问中的效率和安全问题。


<details>
  <summary>Details</summary>
Motivation: Web3.0从中心化向去中心化转型，赋予用户数据自主权，但面临冗余数据复制导致的效率问题和数据不一致带来的安全漏洞挑战。

Method: 开发TDC-Cache框架，采用两层架构：DON层作为可信中介平台；提出DRL-DC方法动态优化分布式预言机缓存策略；设计PoCL共识机制维护缓存决策一致性。

Result: 实验结果显示：相比现有方法，平均访问延迟降低20%，缓存命中率最高提升18%，平均共识成功率提高10%。

Conclusion: 本文首次探索了Web3.0去中心化缓存框架和策略，通过TDC-Cache有效解决了效率和安全问题，为Web3.0基础设施提供了重要参考。

Abstract: The rapid growth of Web3.0 is transforming the Internet from a centralized structure to decentralized, which empowers users with unprecedented self-sovereignty over their own data. However, in the context of decentralized data access within Web3.0, it is imperative to cope with efficiency concerns caused by the replication of redundant data, as well as security vulnerabilities caused by data inconsistency. To address these challenges, we develop a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0 to ensure efficient caching and enhance system resilience against adversarial threats. This framework features a two-layer architecture, wherein the Decentralized Oracle Network (DON) layer serves as a trusted intermediary platform for decentralized caching, bridging the contents from decentralized storage and the content requests from users. In light of the complexity of Web3.0 network topologies and data flows, we propose a Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC) for TDC-Cache to dynamically optimize caching strategies of distributed oracles. Furthermore, we develop a Proof of Cooperative Learning (PoCL) consensus to maintain the consistency of decentralized caching decisions within DON. Experimental results show that, compared with existing approaches, the proposed framework reduces average access latency by 20%, increases the cache hit rate by at most 18%, and improves the average success consensus rate by 10%. Overall, this paper serves as a first foray into the investigation of decentralized caching framework and strategy for Web3.0.

</details>


### [7] [Design Space Exploration of DMA based Finer-Grain Compute Communication Overlap](https://arxiv.org/abs/2512.10236)
*Shagnik Pal,Shaizeen Aga,Suchita Pati,Mahzabeen Islam,Lizy K. John*

Main category: cs.DC

TL;DR: 提出FiCCO（细粒度计算-通信重叠）方法，通过比分片级更细粒度的重叠来提升分布式ML训练性能，设计启发式调度策略，实现最高1.6倍加速。


<details>
  <summary>Details</summary>
Motivation: 当前分布式ML训练中，数据依赖的通信和计算操作存在高达1.7倍的性能损失，现有粗粒度分片级重叠方法无法充分利用网络拓扑和细粒度数据流。

Method: 提出FiCCO细粒度计算-通信重叠方法，分析操作分解的效率损失，设计FiCCO调度空间，开发启发式调度选择策略，并利用GPU DMA引擎卸载通信以减少竞争。

Result: 在真实ML部署场景中，定制化FiCCO调度实现最高1.6倍加速，启发式策略在81%的未见场景中提供准确指导。

Conclusion: FiCCO通过细粒度重叠扩展了调度设计空间，结合效率损失分析和启发式调度选择，能显著提升分布式ML训练性能。

Abstract: As both ML training and inference are increasingly distributed, parallelization techniques that shard (divide) ML model across GPUs of a distributed system, are often deployed. With such techniques, there is a high prevalence of data-dependent communication and computation operations where communication is exposed, leaving as high as 1.7x ideal performance on the table. Prior works harness the fact that ML model state and inputs are already sharded, and employ careful overlap of individual computation/communication shards. While such coarse-grain overlap is promising, in this work, we instead make a case for finer-grain compute-communication overlap which we term FiCCO, where we argue for finer-granularity, one-level deeper overlap than at shard-level, to unlock compute/communication overlap for a wider set of network topologies, finer-grain dataflow and more. We show that FiCCO opens up a wider design space of execution schedules than possible at shard-level alone. At the same time, decomposition of ML operations into smaller operations (done in both shard-based and finer-grain techniques) causes operation-level inefficiency losses. To balance the two, we first present a detailed characterization of these inefficiency losses, then present a design space of FiCCO schedules, and finally overlay the schedules with concomitant inefficiency signatures. Doing so helps us design heuristics that frameworks and runtimes can harness to select bespoke FiCCO schedules based on the nature of underlying ML operations. Finally, to further minimize contention inefficiencies inherent with operation overlap, we offload communication to GPU DMA engines. We evaluate several scenarios from realistic ML deployments and demonstrate that our proposed bespoke schedules deliver up to 1.6x speedup and our heuristics provide accurate guidance in 81% of unseen scenarios.

</details>


### [8] [GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference](https://arxiv.org/abs/2512.09963)
*Phuong Tran,Tzu-Hao Liu,Long Tan Le,Tung-Anh Nguyen,Van Quan La,Eason Yu,Han Shu,Choong Seon Hong,Nguyen H. Tran*

Main category: cs.DC

TL;DR: GOODSPEED是一个分布式推理框架，通过自适应推测解码优化多服务器LLM推理的吞吐量和公平性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型计算需求高，实时推理面临挑战。推测解码虽能加速推理，但在多服务器环境中难以同时保证高吞吐量和服务器间的公平性。

Method: 采用中心验证服务器协调异构草稿服务器的框架，使用梯度调度算法动态分配令牌验证任务，最大化对数效用函数以确保比例公平性。

Result: 通过流体样本路径分析证明，GOODSPEED在稳态条件下收敛到最优吞吐量分配，在动态工作负载下保持接近最优性能且有界误差。

Conclusion: GOODSPEED为分布式LLM推理系统中的多服务器推测解码提供了可扩展、公平且高效的解决方案。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet their high computational demands pose significant challenges for real-time inference, especially in multi-user server speculative decoding and resource-constrained environments. Speculative decoding has emerged as a promising technique to accelerate LLM inference by using lightweight draft models to generate candidate tokens, which are subsequently verified by a larger, more accurate model. However, ensuring both high goodput (the effective rate of accepted tokens) and fairness across multiple draft servers cooperating with a central verification server remains an open challenge. This paper introduces GOODSPEED, a novel distributed inference framework that optimizes goodput through adaptive speculative decoding. GOODSPEED employs a central verification server that coordinates a set of heterogeneous draft servers, each running a small language model to generate speculative tokens. To manage resource allocation effectively, GOODSPEED incorporates a gradient scheduling algorithm that dynamically assigns token verification tasks, maximizing a logarithmic utility function to ensure proportional fairness across servers. By processing speculative outputs from all draft servers in parallel, the framework enables efficient collaboration between the verification server and distributed draft generators, streamlining both latency and throughput. Through rigorous fluid sample path analysis, we show that GOODSPEED converges to the optimal goodput allocation in steady-state conditions and maintains near-optimal performance with provably bounded error under dynamic workloads. These results demonstrate that GOODSPEED provides a scalable, fair and efficient solution for multi-server speculative decoding in distributed LLM inference systems.

</details>


### [9] [Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters](https://arxiv.org/abs/2512.10271)
*Shruti Dongare,Redwan Ibne Seraj Khan,Hadeel Albahar,Nannan Zhao,Diego Melendez Maita,Ali R. Butt*

Main category: cs.DC

TL;DR: RLTune是一个基于强化学习的调度框架，用于在异构GPU集群上动态调度深度学习作业，无需作业特定分析即可优化作业完成时间、排队延迟和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现代云平台承载大规模深度学习工作负载，需要高吞吐、低延迟的GPU调度。然而，GPU集群日益增长的异构性以及对应用特性的有限可见性，给现有调度器带来了重大挑战，这些调度器通常依赖离线分析或应用特定假设。

Method: RLTune集成了强化学习驱动的优先级排序和基于混合整数线性规划（MILP）的作业到节点映射，以优化系统级目标。该框架在微软Philly、Helios和阿里巴巴的大规模生产跟踪数据上进行训练。

Result: RLTune将GPU利用率提高达20%，排队延迟减少达81%，作业完成时间缩短达70%。该框架能够泛化到多样化工作负载，无需每个作业的分析。

Conclusion: RLTune为云提供商提供了一个实用的、可扩展的解决方案，用于更高效、公平和可持续的深度学习工作负载管理，无需作业特定分析即可跨不同工作负载泛化。

Abstract: Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.

</details>


### [10] [High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments](https://arxiv.org/abs/2512.10312)
*Julian Rodriguez,Piotr Lopez,Emiliano Lerma,Rafael Medrano,Jacobo Hernandez*

Main category: cs.DC

TL;DR: 该文档报告了大数据课程中实施的实践和方法序列，包括Epsilon数据集处理、文本分析分类、电影特征分析以及Apache Spark分布式集群的技术实现。


<details>
  <summary>Details</summary>
Motivation: 展示大数据课程中完整的实践流程，从数据处理到分布式计算集群的构建，体现大数据处理的全栈技术应用。

Method: 采用分阶段方法：1) Epsilon数据集的小组和个人处理策略；2) RestMex文本分析与分类；3) IMDb电影特征分析；4) 基于Linux的Apache Spark分布式计算集群技术实现，使用Scala编程语言。

Result: 文档详细记录了大数据处理的全流程实践，包括数据预处理、文本分析、特征工程和分布式计算环境的搭建，形成了完整的大数据技术应用案例。

Conclusion: 该课程项目成功展示了从数据采集处理到分布式计算系统部署的完整大数据技术栈，为学习者提供了实践性的大数据处理经验。

Abstract: This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.

</details>


### [11] [Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability](https://arxiv.org/abs/2512.10425)
*Fan Yu,Guodong Li,Si Wu,Weijun Fang,Sihuang Hu*

Main category: cs.DC

TL;DR: CP-LRCs是一种新型宽条带局部可修复编码，通过将全局奇偶校验块分解到所有局部奇偶校验块中，实现奇偶校验块间的结构化依赖，从而在保持MDS级容错的同时降低单节点和多节点修复带宽。


<details>
  <summary>Details</summary>
Motivation: 现有局部可修复编码在宽条带设置中存在结构限制：扩大的局部组增加单节点修复成本，多节点故障频繁触发昂贵的全局修复，可靠性急剧下降。根本原因是局部和全局奇偶校验块独立设计，无法在修复过程中协同工作。

Method: 提出级联奇偶校验LRCs（CP-LRCs），通过将全局奇偶校验块分解到所有局部奇偶校验块中，在奇偶校验块间嵌入结构化依赖，形成级联奇偶校验组。提供通用系数生成框架，开发利用级联特性的修复算法，并实例化为CP-Azure和CP-Uniform两种实现。

Result: 在阿里云上的评估显示，CP-LRCs相比现有方法，单节点故障修复时间减少高达41%，两节点故障修复时间减少26%，同时保持MDS级容错能力。

Conclusion: CP-LRCs通过奇偶校验块间的结构化依赖解决了宽条带LRCs的关键限制，实现了修复带宽的显著降低，为大规模存储系统提供了更高效的纠删码方案。

Abstract: Erasure coding with wide stripes is increasingly adopted to reduce storage overhead in large-scale storage systems. However, existing Locally Repairable Codes (LRCs) exhibit structural limitations in this setting: inflated local groups increase single-node repair cost, multi-node failures frequently trigger expensive global repair, and reliability degrades sharply. We identify a key root cause: local and global parity blocks are designed independently, preventing them from cooperating during repair. We present Cascaded Parity LRCs (CP-LRCs), a new family of wide stripe LRCs that embed structured dependency between parity blocks by decomposing a global parity block across all local parity blocks. This creates a cascaded parity group that preserves MDS-level fault tolerance while enabling low-bandwidth single-node and multi-node repairs. We provide a general coefficient-generation framework, develop repair algorithms exploiting cascading, and instantiate the design with CP-Azure and CP-Uniform. Evaluations on Alibaba Cloud show reductions in repair time of up to 41% for single-node failures and 26% for two-node failures.

</details>


### [12] [Clustered Federated Learning with Hierarchical Knowledge Distillation](https://arxiv.org/abs/2512.10443)
*Sabtain Ahmad,Meerzhan Kanatbekova,Ivona Brandic,Atakan Aral*

Main category: cs.DC

TL;DR: 提出CFLHKD方法，通过分层聚类联邦学习和多教师知识蒸馏，在保护隐私的同时提升异构IoT环境中的模型性能


<details>
  <summary>Details</summary>
Motivation: 传统聚类联邦学习存在学习碎片化问题，无法充分利用跨集群的集体知识，需要一种既能保持集群个性化又能实现知识共享的方法

Method: 提出CFLHKD方法，采用分层联邦学习架构进行双层聚合（边缘集群特定模型+云端统一全局模型），结合多教师知识蒸馏实现跨集群知识共享

Result: 在标准基准数据集上，CFLHKD在集群特定模型和全局模型准确率上均优于代表性基线方法，性能提升3.32-7.57%

Conclusion: 分层聚类联邦学习结合知识蒸馏能有效解决传统方法的局限性，在保持隐私的同时提升训练效率和模型性能

Abstract: Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\%.

</details>


### [13] [ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp](https://arxiv.org/abs/2512.10576)
*Xinhang Chen,Chao Zhang,Jiahuan He,Wei Liu,Jianming Zhang,Wenlong Zhou,Xiao Li,Pai Zeng,Shiyong Li,Yuanpan Qian,Dong Li,Zhaogeng Li*

Main category: cs.DC

TL;DR: ESS系统通过将Latent-Cache卸载到CPU内存，解决了DeepSeek-V3.2-Exp在长上下文推理中的GPU内存瓶颈，显著提升了解码阶段吞吐量。


<details>
  <summary>Details</summary>
Motivation: DeepSeek-V3.2-Exp虽然通过稀疏注意力机制降低了长上下文推理延迟，但解码阶段的PD解耦仍然是主要瓶颈。Latent-Cache随序列长度线性增长与GPU内存容量有限之间的冲突，限制了批处理大小，从而抑制了解码阶段吞吐量。

Method: 提出ESS（Extended Sparse Server）系统，采用卸载中心的设计方法，选择性地将Latent-Cache卸载到CPU内存，同时将延迟关键组件保留在GPU上。通过释放GPU内存，ESS使批处理大小扩展与GPU内存约束解耦。

Result: 高保真模拟显示，ESS在32K上下文长度下提供69.4%的吞吐量提升，在128K上下文长度下最高可达123%的吞吐量提升，有效改善了大上下文推理工作负载的性能。

Conclusion: ESS是一个实用且可扩展的解决方案，通过优化内存管理显著提升长上下文LLM服务的解码阶段吞吐量，从而降低实际部署成本。

Abstract: DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.
  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.
  Our high-fidelity simulations show that ESS delivers 69.4\% throughput improvement at 32K context length and up to 123\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [Algorithm-Driven On-Chip Integration for High Density and Low Cost](https://arxiv.org/abs/2512.10089)
*Jeongeun Kim,Sabrina Yarzada,Paul Chen,Christopher Torng*

Main category: cs.AR

TL;DR: 提出了一种用于大规模半导体晶圆流片的新方法，通过自动化设计打包、窄区域互连架构和片上电源域技术，实现高达13倍面积优化的可扩展多项目集成方案。


<details>
  <summary>Details</summary>
Motivation: 随着半导体人才培养需求的增长，需要支持大量独立硬件设计的研究和培训平台。传统的多项目晶圆服务在项目数量增加时扩展性受限，现有方法虽然尝试通过共享die集成小型设计，但缺乏系统性的设计布局、连接和验证原则。

Method: 提出三种关键技术：1）建立结构化设计空间公式化，实现自动化算法驱动的项目打包；2）引入仅利用站点间窄区域进行片外通信和其他共享需求的架构；3）提供片上电源域实用方法，支持在标准实验室工作台上进行每个项目的功耗特性分析。

Result: 实验结果显示，该方法相比最先进的纯物理聚合方法实现了高达13倍的面积优化，为大规模流片环境提供了可扩展且经济高效的解决方案。

Conclusion: 该工作填补了密集集成设计站点布局、连接和验证的系统性研究空白，通过自动化打包、窄区域互连架构和片上电源域技术，为大规模半导体教育和研究提供了可扩展的流片平台。

Abstract: Growing interest in semiconductor workforce development has generated demand for platforms capable of supporting large numbers of independent hardware designs for research and training without imposing high per-project overhead. Traditional multi-project wafer (MPW) services based solely on physical co-placement have historically met this need, yet their scalability breaks down as project counts rise. Recent efforts towards scalable chip tapeouts mitigate these limitations by integrating many small designs within a shared die and attempt to amortize costly resources such as IO pads and memory macros. However, foundational principles for arranging, linking, and validating such densely integrated design sites have received limited systematic investigation. This work presents a new approach with three key techniques to address this gap. First, we establish a structured formulation of the design space that enables automated, algorithm-driven packing of many projects, replacing manual layout practices. Second, we introduce an architecture that exploits only the narrow-area regions between sites to deliver on off-chip communication and other shared needs. Third, we provide a practical approach for on-chip power domains enabling per-project power characterization at a standard laboratory bench and requiring no expertise in low-power ASIC design. Experimental results show that our approach achieves substantial area reductions of up to 13x over state-of-the-art physical-only aggregation methods, offering a scalable and cost-effective path forward for large-scale tapeout environments.

</details>


### [15] [A Vertically Integrated Framework for Templatized Chip Design](https://arxiv.org/abs/2512.10155)
*Jeongeun Kim,Christopher Torng*

Main category: cs.AR

TL;DR: 该论文提出了一种从高级面向对象软件规范生成芯片的方法，通过软件对象到芯片区域的直接映射，降低软件开发者参与芯片设计的门槛。


<details>
  <summary>Details</summary>
Motivation: 软件开发者通常难以将定制硬件集成到应用中，尽管专用芯片能为机器学习和AI等领域带来显著优势。现有芯片设计流程对软件开发者来说过于复杂，需要降低参与门槛。

Method: 采用模块化构建策略：1）软件对象直接映射为芯片上的对应区域；2）使用基于序列的形式类型系统验证硬件模块间的通信模式；3）开发适合对象对齐设计风格的布局技术；4）研究硬件互连策略来组合多个模块。

Result: 该方法为入门级芯片设计学习者提供了从软件到芯片设计的思维连续性，实现了实用的布局生成，显著降低了软件开发者参与芯片创建所需的专业知识。

Conclusion: 通过保持软件抽象到硬件实现的直接映射，该工作成功降低了软件开发者参与芯片设计的门槛，使更多开发者能够利用定制硬件优势，同时满足轻量级性能需求。

Abstract: Developers who primarily engage with software often struggle to incorporate custom hardware into their applications, even though specialized silicon can provide substantial benefits to machine learning and AI, as well as to the application domains that they enable. This work investigates how a chip can be generated from a high-level object-oriented software specification, targeting introductory-level chip design learners with only very light performance requirements, while maintaining mental continuity between the chip layout and the software source program. In our approach, each software object is represented as a corresponding region on the die, producing a one-to-one structural mapping that preserves these familiar abstractions throughout the design flow. To support this mapping, we employ a modular construction strategy in which vertically composed IP blocks implement the behavioral protocols expressed in software. A direct syntactic translation, however, cannot meet hardware-level efficiency or communication constraints. For this reason, we leverage formal type systems based on sequences that check whether interactions between hardware modules adhere to the communication patterns described in the software model. We further examine hardware interconnect strategies for composing many such modules and develop layout techniques suited to this object-aligned design style. Together, these contributions preserve mental continuity from software to chip design for new learners and enables practical layout generation, ultimately reducing the expertise required for software developers to participate in chip creation.

</details>


### [16] [Neuromorphic Processor Employing FPGA Technology with Universal Interconnections](https://arxiv.org/abs/2512.10180)
*Pracheta Harlikar,Abdel-Hameed A. Badawy,Prasanna Date*

Main category: cs.AR

TL;DR: 基于FPGA的低成本开源神经形态处理器，支持可配置连接和LIF神经元模型，通过UART接口实现运行时重配置，在Iris和MNIST基准测试中验证了能效和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 神经形态计算具有超低功耗和实时推理的潜力，但缺乏灵活的开源平台阻碍了广泛采用和实验。本文旨在提供一个低成本、可访问的神经形态处理器平台。

Method: 在Xilinx Zynq-7000 FPGA平台上实现神经形态处理器，支持全连接可配置连接，采用泄漏积分发放(LIF)神经元模型，具有阈值、突触权重、不应期等可定制参数，通过UART接口与主机通信实现运行时重配置。

Result: 使用Iris分类和MNIST数字识别基准数据集验证了架构有效性，综合后结果显示了设计的能效和可扩展性，证明了其作为研究级神经形态平台的可行性。

Conclusion: 该实现提供了一个既易于访问又适应实际脉冲神经网络应用的研究级神经形态平台，将在项目完成后作为开源发布。

Abstract: Neuromorphic computing, inspired by biological neural systems, holds immense promise for ultra-low-power and real-time inference applications. However, limited access to flexible, open-source platforms continues to hinder widespread adoption and experimentation. In this paper, we present a low-cost neuromorphic processor implemented on a Xilinx Zynq-7000 FPGA platform. The processor supports all-to-all configurable connectivity and employs the leaky integrate-and-fire (LIF) neuron model with customizable parameters such as threshold, synaptic weights, and refractory period. Communication with the host system is handled via a UART interface, enabling runtime reconfiguration without hardware resynthesis. The architecture was validated using benchmark datasets including the Iris classification and MNIST digit recognition tasks. Post-synthesis results highlight the design's energy efficiency and scalability, establishing its viability as a research-grade neuromorphic platform that is both accessible and adaptable for real-world spiking neural network applications. This implementation will be released as open source following project completion.

</details>


### [17] [SemanticBBV: A Semantic Signature for Cross-Program Knowledge Reuse in Microarchitecture Simulation](https://arxiv.org/abs/2512.10231)
*Zhenguo Liu,Chengao Shi,Chen Ding,Jiang Xu*

Main category: cs.AR

TL;DR: SemanticBBV：一个两阶段框架，通过语义编码和集合变换器生成性能感知的程序签名，实现跨程序仿真重用，相比传统BBV方法获得7143倍加速


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的微架构仿真使用基本块向量(BBV)作为程序表示，但BBV存在顺序依赖ID（阻碍跨程序知识重用）和缺乏语义内容（难以预测硬件性能）的根本限制，有巨大的优化潜力未被挖掘

Method: 1. 轻量级RWKV语义编码器将汇编基本块转换为丰富的BBE嵌入，捕获深度功能语义；2. 顺序不变的集合变换器聚合BBE（按执行频率加权）生成最终签名，通过三元组损失（签名区分性）和CPI回归任务（性能敏感性）双重目标共同训练

Result: 仅模拟14个通用程序点就能估计10个SPEC CPU基准测试的性能，平均准确率达86.3%，实现7143倍仿真加速；签名对新微架构有强适应性，只需少量微调

Conclusion: SemanticBBV不仅匹配传统BBV的单程序准确性，还实现了前所未有的跨程序分析能力，为微架构仿真开辟了新的优化方向

Abstract: For decades, sampling-based techniques have been the de facto standard for accelerating microarchitecture simulation, with the Basic Block Vector (BBV) serving as the cornerstone program representation. Yet, the BBV's fundamental limitations: order-dependent IDs that prevent cross-program knowledge reuse and a lack of semantic content predictive of hardware performance have left a massive potential for optimization untapped.
  To address these gaps, we introduce SemanticBBV, a novel, two-stage framework that generates robust, performance-aware signatures for cross-program simulation reuse. First, a lightweight RWKV-based semantic encoder transforms assembly basic blocks into rich Basic Block Embeddings (BBEs), capturing deep functional semantics. Second, an order-invariant Set Transformer aggregates these BBEs, weighted by execution frequency, into a final signature. Crucially, this stage is co-trained with a dual objective: a triplet loss for signature distinctiveness and a Cycles Per Instruction (CPI) regression task, directly imbuing the signature with performance sensitivity. Our evaluation demonstrates that SemanticBBV not only matches traditional BBVs in single-program accuracy but also enables unprecedented cross-program analysis. By simulating just 14 universal program points, we estimated the performance of ten SPEC CPU benchmarks with 86.3% average accuracy, achieving a 7143x simulation speedup. Furthermore, the signature shows strong adaptability to new microarchitectures with minimal fine-tuning.

</details>
