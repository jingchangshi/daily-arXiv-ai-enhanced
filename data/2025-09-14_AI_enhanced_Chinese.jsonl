{"id": "2509.08969", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.08969", "abs": "https://arxiv.org/abs/2509.08969", "authors": ["Nima Karimian Kakolaki"], "title": "A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems", "comment": null, "summary": "Distributed systems require robust, scalable identifier schemes to ensure\ndata uniqueness and efficient indexing across multiple nodes. This paper\npresents a comprehensive analysis of the evolution of distributed identifiers,\ncomparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We\ncombine mathematical calculation of collision probabilities with empirical\nexperiments measuring generation speed and network transmission overhead in a\nsimulated distributed environment. Results demonstrate that ULIDs significantly\noutperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing\ngeneration speed by 97.32%. statistical analysis further shows ULIDs offer a\n98.42% lower collision risk compared to UUIDv7, while maintaining negligible\ncollision probabilities even at high generation rates. These findings highlight\nULIDs as an optimal choice for high-performance distributed systems, providing\nefficient, time-ordered, and lexicographically sortable identifiers suitable\nfor scalable applications. All source code, datasets, and analysis scripts\nutilized in this research are publicly available in our dedicated repository at\nhttps://github.com/nimakarimiank/uids-comparison. This repository contains\ncomprehensive documentation of the experimental setup, including configuration\nfiles for the distributed environment, producer and consumer implementations,\nand message broker integration. Additionally, it provides the data scripts and\ndatasets. Researchers and practitioners are encouraged to explore the\nrepository for full reproducibility of the experiments and to facilitate\nfurther investigation or extension of the presented work.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u5206\u5e03\u5f0f\u6807\u8bc6\u7b26\u65b9\u6848\u8fdb\u884c\u4e86\u7efc\u5408\u5206\u6790\uff0c\u901a\u8fc7\u7406\u8bba\u8ba1\u7b97\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660eULIDs\u5728\u7f51\u7edc\u5f00\u9500\u3001\u751f\u6210\u901f\u5ea6\u548c\u51b2\u7a81\u98ce\u9669\u65b9\u9762\u663e\u8457\u4f18\u4e8eUUIDv4\u548cUUIDv7\uff0c\u662f\u9ad8\u6027\u80fd\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6700\u4f73\u9009\u62e9\u3002", "motivation": "\u5206\u5e03\u5f0f\u7cfb\u7edf\u9700\u8981\u7a81\u7834\u6027\u3001\u53ef\u6269\u5c55\u7684\u6807\u8bc6\u7b26\u65b9\u6848\u6765\u786e\u4fdd\u6570\u636e\u552f\u4e00\u6027\u548c\u5728\u591a\u8282\u70b9\u4e0a\u7684\u9ad8\u6548\u7d22\u5f15\u3002", "method": "\u7ed3\u5408\u51b2\u7a81\u6982\u7387\u7684\u6570\u5b66\u8ba1\u7b97\u4e0e\u5728\u6a21\u62df\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u6d4b\u91cf\u751f\u6210\u901f\u5ea6\u548c\u7f51\u7edc\u4f20\u8f93\u5f00\u9500\u7684\u5b9e\u9a8c\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u4f20\u7edf\u81ea\u589e\u952e\u3001UUIDv4\u3001UUIDv7\u548cULIDs\u3002", "result": "ULIDs\u663e\u8457\u8d85\u8fc7UUIDv4\u548cUUIDv7\uff0c\u7f51\u7edc\u5f00\u9500\u51cf\u5c1183.7%\uff0c\u751f\u6210\u901f\u5ea6\u63d0\u9ad897.32%\uff0c\u51b2\u7a81\u98ce\u9669\u6bd4UUIDv7\u4f4e98.42%\uff0c\u5373\u4f7f\u5728\u9ad8\u751f\u6210\u901f\u7387\u4e0b\u4e5f\u4fdd\u6301\u53ef\u5ffd\u7565\u7684\u51b2\u7a81\u6982\u7387\u3002", "conclusion": "ULIDs\u662f\u9ad8\u6027\u80fd\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6700\u4f73\u9009\u62e9\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u65f6\u95f4\u6392\u5e8f\u548c\u5b57\u5178\u6392\u5e8f\u80fd\u529b\u7684\u6807\u8bc6\u7b26\uff0c\u9002\u7528\u4e8e\u53ef\u6269\u5c55\u5e94\u7528\u3002"}}
{"id": "2509.09058", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.09058", "abs": "https://arxiv.org/abs/2509.09058", "authors": ["Ajay Kumar", "Praveen Rao", "Peter Sanders"], "title": "Optimizing the Variant Calling Pipeline Execution on Human Genomes Using GPU-Enabled Machines", "comment": "To appear in 14th International Workshop on Parallel and AI-based\n  Bioinformatics and Biomedicine (ParBio), Philadelphia, 2025", "summary": "Variant calling is the first step in analyzing a human genome and aims to\ndetect variants in an individual's genome compared to a reference genome. Due\nto the computationally-intensive nature of variant calling, genomic data are\nincreasingly processed in cloud environments as large amounts of compute and\nstorage resources can be acquired with the pay-as-you-go pricing model. In this\npaper, we address the problem of efficiently executing a variant calling\npipeline for a workload of human genomes on graphics processing unit\n(GPU)-enabled machines. We propose a novel machine learning (ML)-based approach\nfor optimizing the workload execution to minimize the total execution time. Our\napproach encompasses two key techniques: The first technique employs ML to\npredict the execution times of different stages in a variant calling pipeline\nbased on the characteristics of a genome sequence. Using the predicted times,\nthe second technique generates optimal execution plans for the machines by\ndrawing inspiration from the flexible job shop scheduling problem. The plans\nare executed via careful synchronization across different machines. We\nevaluated our approach on a workload of publicly available genome sequences\nusing a testbed with different types of GPU hardware. We observed that our\napproach was effective in predicting the execution times of variant calling\npipeline stages using ML on features such as sequence size, read quality,\npercentage of duplicate reads, and average read length. In addition, our\napproach achieved 2X speedup (on an average) over a greedy approach that also\nused ML for predicting the execution times on the tested workload of sequences.\nFinally, our approach achieved 1.6X speedup (on an average) over a dynamic\napproach that executed the workload based on availability of resources without\nusing any ML-based time predictions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u53d8\u5f02\u68c0\u6d4b\u6d41\u6c34\u7ebf\u5404\u9636\u6bb5\u6267\u884c\u65f6\u95f4\u548c\u4f18\u5316\u8c03\u5ea6\uff0c\u5728GPU\u673a\u5668\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u57fa\u56e0\u7ec4\u53d8\u5f02\u68c0\u6d4b\u5de5\u4f5c\u8d1f\u8377\u6267\u884c\u3002", "motivation": "\u7535\u5b50\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u53d8\u5f02\u68c0\u6d4b\u5728\u4e91\u73af\u5883\u4e2d\u5904\u7406\u65f6\uff0c\u9700\u8981\u6709\u6548\u5229\u7528GPU\u8d44\u6e90\u5e76\u6700\u5c0f\u5316\u603b\u6267\u884c\u65f6\u95f4\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u53d8\u5f02\u68c0\u6d4b\u6d41\u6c34\u7ebf\u5404\u9636\u6bb5\u6267\u884c\u65f6\u95f4\uff0c\u57fa\u4e8e\u57fa\u56e0\u7ec4\u5e8f\u5217\u7279\u5f81\uff08\u5982\u5e8f\u5217\u5927\u5c0f\u3001\u8bfb\u8d28\u91cf\u7b49\uff09\uff0c\u7136\u540e\u4f7f\u7528\u7075\u6d3b\u4f5c\u4e1a\u5e97\u8c03\u5ea6\u95ee\u9898\u7684\u601d\u60f3\u751f\u6210\u6700\u4f18\u6267\u884c\u8ba1\u5212\uff0c\u901a\u8fc7\u7cbe\u5fc3\u540c\u6b65\u6267\u884c\u3002", "result": "\u5728\u516c\u5f00\u57fa\u56e0\u7ec4\u5e8f\u5217\u5de5\u4f5c\u8d1f\u8377\u4e0a\uff0c\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u6267\u884c\u65f6\u95f4\uff0c\u76f8\u6bd4\u8d2a\u5fc3\u65b9\u6cd5\u83b7\u5f972\u500d\u52a0\u901f\uff0c\u76f8\u6bd4\u52a8\u6001\u8c03\u5ea6\u65b9\u6cd5\u83b7\u5f971.6\u500d\u52a0\u901f\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u7ed3\u5408\u4f18\u5316\u8c03\u5ea6\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u57fa\u56e0\u7ec4\u53d8\u5f02\u68c0\u6d4b\u5728GPU\u73af\u5883\u4e0b\u7684\u6267\u884c\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u57fa\u56e0\u7ec4\u6570\u636e\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09094", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.09094", "abs": "https://arxiv.org/abs/2509.09094", "authors": ["Guochu Xiong", "Xiangzhong Luo", "Weichen Liu"], "title": "Coherence-Aware Task Graph Modeling for Realistic Application", "comment": "Accepted by MEMOCODE'25, 10 pages", "summary": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis.", "AI": {"tldr": "CoTAM\u662f\u4e00\u4e2a\u7f13\u5b58\u4e00\u81f4\u6027\u611f\u77e5\u7684\u4efb\u52a1\u56fe\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u4e00\u81f4\u6027\u5f71\u54cd\u3001\u91cf\u5316\u5176\u6743\u91cd\u5e76\u63a8\u65ad\u4efb\u52a1\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e3a\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u6784\u5efa\u7edf\u4e00\u7684\u4efb\u52a1\u56fe\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u9759\u6001\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u52a8\u6001\u6570\u636e\u4f9d\u8d56\u884c\u4e3a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u591a\u6838\u7cfb\u7edf\u6269\u5c55\uff0c\u7f13\u5b58\u4e00\u81f4\u6027\u6210\u4e3a\u7cfb\u7edf\u6027\u80fd\u5173\u952e\u56e0\u7d20\u3002\u4f20\u7edf\u4efb\u52a1\u56fe\u5efa\u6a21\u4f9d\u8d56\u9884\u5b9a\u4e49\u56fe\uff0c\u4f46\u73b0\u5b9e\u5e94\u7528\u7f3a\u4e4f\u663e\u5f0f\u56fe\u4e14\u5177\u6709\u52a8\u6001\u6570\u636e\u4f9d\u8d56\u884c\u4e3a\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f7f\u7528\u9690\u5f0f\u6280\u672f\u4e0d\u751f\u6210\u663e\u5f0f\u56fe\uff0c\u8981\u4e48\u751f\u6210\u9488\u5bf9\u56fa\u5b9a\u8c03\u5ea6\u6a21\u578b\u7684\u56fe\uff0c\u4e14\u5f80\u5f80\u5ffd\u7565\u4e00\u81f4\u6027\u4ea4\u4e92\uff0c\u5bfc\u81f4\u8bbe\u8ba1\u5047\u8bbe\u4e0e\u5b9e\u9645\u8fd0\u884c\u65f6\u884c\u4e3a\u5b58\u5728\u5dee\u8ddd\u3002", "method": "CoTAM\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\uff1a1) \u89e3\u8026\u7f13\u5b58\u4e00\u81f4\u6027\u5f71\u54cd\u4e0e\u6574\u4f53\u6267\u884c\uff1b2) \u901a\u8fc7\u5b66\u4e60\u6743\u91cd\u65b9\u6848\u91cf\u5316\u4e00\u81f4\u6027\u5f71\u54cd\uff1b3) \u63a8\u65ad\u4efb\u52a1\u95f4\u4f9d\u8d56\u5173\u7cfb\u4ee5\u751f\u6210\u4e00\u81f4\u6027\u611f\u77e5\u7684\u4efb\u52a1\u56fe\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCoTAM\u4f18\u4e8e\u9690\u5f0f\u65b9\u6cd5\uff0c\u5f25\u5408\u4e86\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u884c\u4e3a\u4e0e\u73b0\u6709\u8bbe\u8ba1\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u8bc1\u660e\u4e86\u5c06\u7f13\u5b58\u4e00\u81f4\u6027\u7eb3\u5165\u4efb\u52a1\u56fe\u5efa\u6a21\u5bf9\u4e8e\u51c6\u786e\u548c\u53ef\u6cdb\u5316\u7684\u7cfb\u7edf\u7ea7\u5206\u6790\u7684\u91cd\u8981\u6027\u3002", "conclusion": "CoTAM\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u4efb\u52a1\u56fe\u5efa\u6a21\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u663e\u5f0f\u8003\u8651\u7f13\u5b58\u4e00\u81f4\u6027\u4ea4\u4e92\uff0c\u4e3a\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u901a\u7528\u7684\u7cfb\u7edf\u7ea7\u5206\u6790\u57fa\u7840\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6838\u7cfb\u7edf\u6027\u80fd\u5206\u6790\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2509.09400", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.09400", "abs": "https://arxiv.org/abs/2509.09400", "authors": ["Valerio Besozzi", "Enrico Fiasco", "Marco Danelutto", "Patrizio Dazzi"], "title": "WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge", "comment": "Accepted at VHPC25", "summary": "Serverless computing at the edge requires lightweight execution environments\nto minimize cold start latency, especially in Urgent Edge Computing (UEC). This\npaper compares WebAssembly and unikernel-based MicroVMs for serverless\nworkloads. We present Limes, a WebAssembly runtime built on Wasmtime, and\nevaluate it against the Firecracker-based environment used in SPARE. Results\nshow that WebAssembly offers lower cold start times for lightweight functions\nbut suffers with complex workloads, while Firecracker provides higher, but\nstable, cold starts and better execution performance, particularly for\nI/O-heavy tasks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u6bd4\u8f83\u4e86WebAssembly\u5488unikernel\u57fa\u7684MicroVM\u5728\u8fb9\u7f18\u670d\u52a1\u5668\u65e0\u8ba1\u7b97\u4e2d\u7684\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793aWebAssembly\u5728\u8f7b\u91cf\u51fd\u6570\u4e0a\u542b\u6709\u66f4\u4f4e\u7684\u51b7\u542f\u52a8\u5ef6\u8fdf\uff0c\u800cFirecracker\u5728\u590d\u6742\u5de5\u4f5c\u8d1f\u8f7d\u5488I/O\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u8fb9\u7f18\u670d\u52a1\u5668\u65e0\u8ba1\u7b97\u9700\u8981\u8f7b\u91cf\u6267\u884c\u73af\u5883\u6765\u6700\u5c0f\u5316\u51b7\u542f\u52a8\u5ef6\u8fdf\uff0c\u7279\u522b\u662f\u5728\u7d27\u6025\u8fb9\u7f18\u8ba1\u7b97(UEC)\u573a\u666f\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86Limes\uff0c\u4e00\u4e2a\u57fa\u4e8eWasmtime\u7684WebAssembly\u8fd0\u884c\u65f6\uff0c\u5e76\u5c06\u5176\u4e0e\u57fa\u4e8eFirecracker\u7684SPARE\u73af\u5883\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u5206\u6790\u3002", "result": "WebAssembly\u5728\u8f7b\u91cf\u51fd\u6570\u4e0a\u63d0\u4f9b\u66f4\u4f4e\u7684\u51b7\u542f\u52a8\u65f6\u95f4\uff0c\u4f46\u5728\u590d\u6742\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u8868\u73b0\u5dee\u5f3a\uff1bFirecracker\u63d0\u4f9b\u66f4\u9ad8\u4f46\u7a33\u5b9a\u7684\u51b7\u542f\u52a8\u5488\u66f4\u597d\u7684\u6267\u884c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728I/O\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u4e24\u79cd\u6280\u672f\u5404\u6709\u4f18\u52bf\uff1aWebAssembly\u9002\u7528\u4e8e\u8f7b\u91cf\u51fd\u6570\u7684\u5feb\u901f\u542f\u52a8\uff0c\u800cFirecracker\u66f4\u9002\u5408\u590d\u6742\u5488I/O\u5bc6\u96c6\u578b\u7684\u670d\u52a1\u5668\u65e0\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u3002"}}
{"id": "2509.09019", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.09019", "abs": "https://arxiv.org/abs/2509.09019", "authors": ["Mohit Tekriwal", "John Sarracino"], "title": "Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs", "comment": null, "summary": "Scientific computing programs often undergo aggressive compiler optimization\nto achieve high performance and efficient resource utilization. While\nperformance is critical, we also need to ensure that these optimizations are\ncorrect. In this paper, we focus on a specific class of optimizations,\nfloating-point optimizations, notably due to fast math, at the LLVM IR level.\nWe present a preliminary work, which leverages the Verified LLVM framework in\nthe Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)\noptimization for a basic block implementing the arithmetic expression $a * b +\nc$ . We then propose ways to extend this preliminary results by adding more\nprogram features and fast math floating-point optimizations.", "AI": {"tldr": "\u901a\u8fc7\u9a8c\u8bc1LLVM\u6846\u67b6\u5728Rocq\u5b9a\u7406\u8bc1\u660e\u5668\u4e2d\u8bc1\u660e\u6d6e\u70b9\u6570FMA\u4f18\u5316\u7684\u6b63\u786e\u6027\uff0c\u5e76\u63d0\u51fa\u6269\u5c55\u66f4\u591a\u7a0b\u5e8f\u7279\u5f81\u548c\u5feb\u901f\u6570\u5b66\u4f18\u5316\u7684\u65b9\u6848", "motivation": "\u79d1\u5b66\u8ba1\u7b97\u7a0b\u5e8f\u9700\u8981\u7ec8\u6781\u4f18\u5316\u6765\u83b7\u5f97\u9ad8\u6027\u80fd\uff0c\u4f46\u540c\u65f6\u5fc5\u987b\u786e\u4fdd\u8fd9\u4e9b\u4f18\u5316\u7684\u6b63\u786e\u6027\uff0c\u7279\u522b\u662f\u6d6e\u70b9\u6570\u8fd0\u7b97\u4f18\u5316", "method": "\u5229\u7528Rocq\u5b9a\u7406\u8bc1\u660e\u5668\u4e2d\u7684Verified LLVM\u6846\u67b6\uff0c\u8bc1\u660eFMA\u4f18\u5316\u5728\u57fa\u672c\u5757a*b+c\u7b97\u672f\u8868\u8fbe\u5f0f\u4e2d\u7684\u6b63\u786e\u6027", "result": "\u5b8c\u6210\u4e86FMA\u4f18\u5316\u6b63\u786e\u6027\u7684\u521d\u6b65\u8bc1\u660e\u5de5\u4f5c", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9a8c\u8bc1\u6d6e\u70b9\u6570\u4f18\u5316\u6b63\u786e\u6027\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u63d0\u51fa\u4e86\u5411\u66f4\u590d\u6742\u7a0b\u5e8f\u7279\u5f81\u548c\u66f4\u591a\u5feb\u901f\u6570\u5b66\u4f18\u5316\u6269\u5c55\u7684\u65b9\u5411"}}
{"id": "2509.09178", "categories": ["cs.AR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.09178", "abs": "https://arxiv.org/abs/2509.09178", "authors": ["Ayan Biswas", "Jimmy Jin"], "title": "Implementation of a 8-bit Wallace Tree Multiplier", "comment": null, "summary": "Wallace tree multipliers are a parallel digital multiplier architecture\ndesigned to minimize the worst-case time complexity of the circuit depth\nrelative to the input size [1]. In particular, it seeks to perform long\nmultiplication in the binary sense, reducing as many partial products per stage\nas possible through full and half adders circuits, achieving O(log(n)) where n\n= bit length of input. This paper provides an overview of the design, progress\nand methodology in the final project of ECE 55900, consisting of the schematic\nand layout of a Wallace tree 8-bit input multiplier on the gpdk45 technology in\nCadence Virtuoso, as well as any design attempts prior to the final product.\nThis also includes our endeavors in designing the final MAC (Multiply\nAccumulate) unit with undefined targets, which we chose to implement as a 16\nbit combinational multiply-add.", "AI": {"tldr": "8\u4f4dWallace\u6811\u4e58\u6cd5\u5668\u5728gpdk45\u5de5\u827a\u4e0a\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\uff0c\u5305\u62ec\u7535\u8def\u56fe\u3001\u7248\u56fe\u4ee5\u53caMAC\u5355\u5143\u7684\u8bbe\u8ba1\u5c1d\u8bd5", "motivation": "\u8bbe\u8ba1\u5e76\u884c\u6570\u5b57\u4e58\u6cd5\u5668\u67b6\u6784\u4ee5\u6700\u5c0f\u5316\u7535\u8def\u6df1\u5ea6\u7684\u6700\u574f\u60c5\u51b5\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0O(log(n))\u7684\u65f6\u95f4\u590d\u6742\u5ea6", "method": "\u4f7f\u7528Cadence Virtuoso\u5728gpdk45\u5de5\u827a\u4e0a\u8bbe\u8ba1Wallace\u68118\u4f4d\u4e58\u6cd5\u5668\u7684\u7535\u8def\u56fe\u548c\u7248\u56fe\uff0c\u901a\u8fc7\u5168\u52a0\u5668\u548c\u534a\u52a0\u5668\u7535\u8def\u51cf\u5c11\u6bcf\u7ea7\u90e8\u5206\u79ef", "result": "\u6210\u529f\u5b9e\u73b0\u4e868\u4f4dWallace\u6811\u4e58\u6cd5\u5668\u7684\u8bbe\u8ba1\uff0c\u5e76\u5c1d\u8bd5\u8bbe\u8ba1\u4e8616\u4f4d\u7ec4\u5408\u4e58\u6cd5\u7d2f\u52a0(MAC)\u5355\u5143", "conclusion": "Wallace\u6811\u4e58\u6cd5\u5668\u67b6\u6784\u80fd\u6709\u6548\u964d\u4f4e\u4e58\u6cd5\u64cd\u4f5c\u7684\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u5728\u6570\u5b57\u7535\u8def\u8bbe\u8ba1\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c"}}
{"id": "2509.09435", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.09435", "abs": "https://arxiv.org/abs/2509.09435", "authors": ["Houming Qiu", "Kun Zhu", "Dusit Niyato", "Nguyen Cong Luong", "Changyan Yi", "Chen Dai"], "title": "Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing", "comment": null, "summary": "Collaborative mobile edge computing (MEC) has emerged as a promising paradigm\nto enable low-capability edge nodes to cooperatively execute\ncomputation-intensive tasks. However, straggling edge nodes (stragglers)\nsignificantly degrade the performance of MEC systems by prolonging computation\nlatency. While coded distributed computing (CDC) as an effective technique is\nwidely adopted to mitigate straggler effects, existing CDC schemes exhibit two\ncritical limitations: (i) They cannot successfully decode the final result\nunless the number of received results reaches a fixed recovery threshold, which\nseriously restricts their flexibility; (ii) They suffer from inherent poles in\ntheir encoding/decoding functions, leading to decoding inaccuracies and\nnumerical instability in the computational results. To address these\nlimitations, this paper proposes an approximated CDC scheme based on\nbarycentric rational interpolation. The proposed CDC scheme offers several\noutstanding advantages. Firstly, it can decode the final result leveraging any\nreturned results from workers. Secondly, it supports computations over both\nfinite and real fields while ensuring numerical stability. Thirdly, its\nencoding/decoding functions are free of poles, which not only enhances\napproximation accuracy but also achieves flexible accuracy tuning. Fourthly, it\nintegrates a novel BRI-based gradient coding algorithm accelerating the\ntraining process while providing robustness against stragglers. Finally,\nexperimental results reveal that the proposed scheme is superior to existing\nCDC schemes in both waiting time and approximate accuracy.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u91cd\u5fc3\u6709\u7406\u63d2\u503c\u7684\u8fd1\u4f3c\u7f16\u7801\u5206\u5e03\u5f0f\u8ba1\u7b97\u65b9\u6848\uff0c\u89e3\u51b3\u73b0\u6709CDC\u65b9\u6848\u6062\u590d\u9608\u503c\u56fa\u5b9a\u548c\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u652f\u6301\u4efb\u610f\u7ed3\u679c\u89e3\u7801\u5e76\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027", "motivation": "\u73b0\u6709\u7f16\u7801\u5206\u5e03\u5f0f\u8ba1\u7b97(CDC)\u65b9\u6848\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u9700\u8981\u56fa\u5b9a\u6062\u590d\u9608\u503c\u624d\u80fd\u89e3\u7801\uff0c\u4ee5\u53ca\u7f16\u7801/\u89e3\u7801\u51fd\u6570\u5b58\u5728\u6781\u70b9\u5bfc\u81f4\u6570\u503c\u4e0d\u7a33\u5b9a\u548c\u7cbe\u5ea6\u95ee\u9898", "method": "\u57fa\u4e8e\u91cd\u5fc3\u6709\u7406\u63d2\u503c(BRI)\u8bbe\u8ba1\u8fd1\u4f3cCDC\u65b9\u6848\uff0c\u5f00\u53d1BRI-based\u68af\u5ea6\u7f16\u7801\u7b97\u6cd5\uff0c\u652f\u6301\u6709\u9650\u57df\u548c\u5b9e\u6570\u57df\u8ba1\u7b97", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6848\u5728\u7b49\u5f85\u65f6\u95f4\u548c\u8fd1\u4f3c\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709CDC\u65b9\u6848\uff0c\u80fd\u591f\u5229\u7528\u4efb\u610f\u8fd4\u56de\u7ed3\u679c\u89e3\u7801\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u7cbe\u5ea6\u8c03\u8282", "conclusion": "\u6240\u63d0\u51fa\u7684BRI-based CDC\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u3001\u6570\u503c\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u7cbe\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\u7684\u6027\u80fd"}}
{"id": "2509.09059", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.09059", "abs": "https://arxiv.org/abs/2509.09059", "authors": ["Paulette Koronkevich", "William J. Bowman"], "title": "Dependent-Type-Preserving Memory Allocation", "comment": "Submitted and received second place at the Student Research\n  Competition at Principles of Programming Languages 2022", "summary": "Dependently typed programming languages such as Coq, Agda, Idris, and F*,\nallow programmers to write detailed specifications of their programs and prove\ntheir programs meet these specifications. However, these specifications can be\nviolated during compilation since they are erased after type checking. External\nprograms linked with the compiled program can violate the specifications of the\noriginal program and change the behavior of the compiled program -- even when\ncompiled with a verified compiler. For example, since Coq does not allow\nexplicitly allocating memory, a programmer might link their Coq program with a\nC program that can allocate memory. Even if the Coq program is compiled with a\nverified compiler, the external C program can still violate the memory-safe\nspecification of the Coq program by providing an uninitialized pointer to\nmemory. This error could be ruled out by type checking in a language expressive\nenough to indicate whether memory is initialized versus uninitialized. Linking\nwith a program with an uninitialized pointer could be considered ill-typed, and\nour linking process could prevent linking with ill-typed programs. To\nfacilitate type checking during linking, we can use type-preserving\ncompilation, which preserves the types through the compilation process. In this\nongoing work, we develop a typed intermediate language that supports dependent\nmemory allocation, as well as a dependent-type-preserving compiler pass for\nmemory allocation.", "AI": {"tldr": "\u4f9d\u8d56\u7c7b\u578b\u8bed\u8a00\u5728\u7f16\u8bd1\u540e\u7c7b\u578b\u89c4\u8303\u4f1a\u88ab\u64e6\u9664\uff0c\u5bfc\u81f4\u5916\u90e8\u94fe\u63a5\u7a0b\u5e8f\u53ef\u80fd\u8fdd\u53cd\u539f\u59cb\u7a0b\u5e8f\u89c4\u8303\u3002\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u7c7b\u578b\u4fdd\u6301\u7f16\u8bd1\u548c\u4f9d\u8d56\u5185\u5b58\u5206\u914d\u7684\u4e2d\u95f4\u8bed\u8a00\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u4f9d\u8d56\u7c7b\u578b\u8bed\u8a00\u5982Coq\u3001Agda\u7b49\u5141\u8bb8\u7f16\u5199\u8be6\u7ec6\u7a0b\u5e8f\u89c4\u8303\uff0c\u4f46\u8fd9\u4e9b\u89c4\u8303\u5728\u7f16\u8bd1\u540e\u88ab\u64e6\u9664\uff0c\u5916\u90e8\u94fe\u63a5\u7a0b\u5e8f\u53ef\u80fd\u8fdd\u53cd\u539f\u59cb\u7a0b\u5e8f\u7684\u5185\u5b58\u5b89\u5168\u7b49\u89c4\u8303\uff0c\u5373\u4f7f\u4f7f\u7528\u9a8c\u8bc1\u7f16\u8bd1\u5668\u4e5f\u65e0\u6cd5\u907f\u514d\u3002", "method": "\u5f00\u53d1\u652f\u6301\u4f9d\u8d56\u5185\u5b58\u5206\u914d\u7684\u7c7b\u578b\u5316\u4e2d\u95f4\u8bed\u8a00\uff0c\u4ee5\u53ca\u4f9d\u8d56\u7c7b\u578b\u4fdd\u6301\u7684\u5185\u5b58\u5206\u914d\u7f16\u8bd1\u5668\u4f20\u9012\uff0c\u901a\u8fc7\u5728\u94fe\u63a5\u65f6\u8fdb\u884c\u7c7b\u578b\u68c0\u67e5\u6765\u9632\u6b62\u4e0e\u7c7b\u578b\u4e0d\u6b63\u786e\u7684\u7a0b\u5e8f\u94fe\u63a5\u3002", "result": "\u8fd9\u662f\u4e00\u9879\u8fdb\u884c\u4e2d\u7684\u5de5\u4f5c\uff0c\u63d0\u51fa\u4e86\u7c7b\u578b\u4fdd\u6301\u7f16\u8bd1\u7684\u65b9\u6cd5\u8bba\u6846\u67b6\u6765\u89e3\u51b3\u4f9d\u8d56\u7c7b\u578b\u7a0b\u5e8f\u7f16\u8bd1\u540e\u7684\u89c4\u8303\u8fdd\u53cd\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u7c7b\u578b\u4fdd\u6301\u7f16\u8bd1\u548c\u4f9d\u8d56\u5185\u5b58\u5206\u914d\u7684\u4e2d\u95f4\u8bed\u8a00\uff0c\u53ef\u4ee5\u5728\u94fe\u63a5\u65f6\u4fdd\u6301\u7c7b\u578b\u4fe1\u606f\uff0c\u9632\u6b62\u5916\u90e8\u7a0b\u5e8f\u8fdd\u53cd\u539f\u59cb\u7a0b\u5e8f\u7684\u4f9d\u8d56\u7c7b\u578b\u89c4\u8303\u3002"}}
{"id": "2509.09505", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.09505", "abs": "https://arxiv.org/abs/2509.09505", "authors": ["Haoran Wu", "Can Xiao", "Jiayi Nie", "Xuan Guo", "Binglei Lou", "Jeffrey T. H. Wong", "Zhiwen Mo", "Cheng Zhang", "Przemyslaw Forys", "Wayne Luk", "Hongxiang Fan", "Jianyi Cheng", "Timothy M. Jones", "Rika Antonova", "Robert Mullins", "Aaron Zhao"], "title": "Combating the Memory Walls: Optimization Pathways for Long-Context Agentic LLM Inference", "comment": null, "summary": "LLMs now form the backbone of AI agents for a diverse array of applications,\nincluding tool use, command-line agents, and web or computer use agents. These\nagentic LLM inference tasks are fundamentally different from chatbot-focused\ninference -- they often have much larger context lengths to capture complex,\nprolonged inputs, such as entire webpage DOMs or complicated tool call\ntrajectories. This, in turn, generates significant off-chip memory traffic for\nthe underlying hardware at the inference stage and causes the workload to be\nconstrained by two memory walls, namely the bandwidth and capacity memory\nwalls, preventing the on-chip compute units from achieving high utilization.\n  In this paper, we introduce PLENA, a hardware-software co-designed system\nthat applies three core optimization pathways to tackle these challenges. PLENA\nincludes an efficient hardware implementation of compute and memory units\nsupporting an asymmetric quantization scheme. PLENA also features a novel\nflattened systolic array architecture that has native support for\nFlashAttention to tackle these memory walls in the scenario of inference\nserving for long-context LLMs. Additionally, PLENA is developed with a complete\nstack, including a custom ISA, a compiler, a cycle-emulated simulator, and an\nautomated design space exploration flow. The simulated results show that PLENA\nachieves up to 8.5x higher utilization than existing accelerators, and delivers\n2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the\nTPU v6e, under the same multiplier count and memory settings. The full PLENA\nsystem will also be open-sourced.", "AI": {"tldr": "PLENA\u662f\u4e00\u4e2a\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u7cfb\u7edf\uff0c\u4e13\u95e8\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u5e26\u5bbd\u548c\u5bb9\u91cf\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u91cf\u5316\u65b9\u6848\u3001\u6241\u5e73\u5316\u8109\u52a8\u9635\u5217\u67b6\u6784\u548c\u5b8c\u6574\u5de5\u5177\u94fe\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u52a0\u901f\u5668\u6700\u9ad88.5\u500d\u7684\u5229\u7528\u7387\u63d0\u5347\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u5e94\u7528\uff08\u5982\u7f51\u9875DOM\u5904\u7406\u3001\u590d\u6742\u5de5\u5177\u8c03\u7528\uff09\u9700\u8981\u5904\u7406\u8d85\u957f\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u63a8\u7406\u9636\u6bb5\u4ea7\u751f\u5927\u91cf\u7247\u5916\u5185\u5b58\u8bbf\u95ee\uff0c\u53d7\u5230\u5185\u5b58\u5e26\u5bbd\u548c\u5bb9\u91cf\u4e24\u4e2a\u5185\u5b58\u5899\u7684\u9650\u5236\uff0c\u4f7f\u5f97\u8ba1\u7b97\u5355\u5143\u5229\u7528\u7387\u4f4e\u4e0b\u3002", "method": "PLENA\u91c7\u7528\u4e09\u4e2a\u6838\u5fc3\u4f18\u5316\u8def\u5f84\uff1a1\uff09\u652f\u6301\u975e\u5bf9\u79f0\u91cf\u5316\u65b9\u6848\u7684\u9ad8\u6548\u786c\u4ef6\u5b9e\u73b0\uff1b2\uff09\u5177\u6709FlashAttention\u539f\u751f\u652f\u6301\u7684\u6241\u5e73\u5316\u8109\u52a8\u9635\u5217\u67b6\u6784\uff1b3\uff09\u5b8c\u6574\u7684\u5de5\u5177\u94fe\u5305\u62ec\u81ea\u5b9a\u4e49ISA\u3001\u7f16\u8bd1\u5668\u3001\u5468\u671f\u6a21\u62df\u4eff\u771f\u5668\u548c\u81ea\u52a8\u5316\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u6d41\u7a0b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cPLENA\u76f8\u6bd4\u73b0\u6709\u52a0\u901f\u5668\u5b9e\u73b0\u6700\u9ad88.5\u500d\u7684\u5229\u7528\u7387\u63d0\u5347\uff0c\u5728\u76f8\u540c\u4e58\u6cd5\u5668\u6570\u91cf\u548c\u5185\u5b58\u914d\u7f6e\u4e0b\uff0c\u541e\u5410\u91cf\u6bd4A100 GPU\u9ad82.24\u500d\uff0c\u6bd4TPU v6e\u9ad83.85\u500d\u3002", "conclusion": "PLENA\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u5899\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u786c\u4ef6\u5229\u7528\u7387\u548c\u63a8\u7406\u541e\u5410\u91cf\uff0c\u8be5\u7cfb\u7edf\u5c06\u5f00\u6e90\u53d1\u5e03\u3002"}}
{"id": "2509.09493", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.09493", "abs": "https://arxiv.org/abs/2509.09493", "authors": ["Ignacio Amores-Sesar", "Christian Cachin", "Juan Villacis"], "title": "Weaker Assumptions for Asymmetric Trust", "comment": null, "summary": "In distributed systems with asymmetric trust, each participant is free to\nmake its own trust assumptions about others, captured by an asymmetric quorum\nsystem. This contrasts with ordinary, symmetric quorum systems and threshold\nmodels, where trust assumptions are uniformly shared among participants.\nFundamental problems like reliable broadcast and consensus are unsolvable in\nthe asymmetric model if quorum systems satisfy only the classical properties of\nconsistency and availability. Existing approaches overcome this by introducing\nstronger assumptions. We show that some of these assumptions are overly\nrestrictive, so much so that they effectively eliminate the benefits of\nasymmetric trust. To address this, we propose a new approach to characterize\nasymmetric problems and, building upon it, present algorithms for reliable\nbroadcast and consensus that require weaker assumptions than previous\nsolutions. Our methods are general and can be extended to other core problems\nin systems with asymmetric trust.", "AI": {"tldr": "\u5728\u4e0d\u5bf9\u79f0\u4fe1\u4efb\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u63d0\u51fa\u4e86\u66f4\u5f31\u5047\u8bbe\u7684\u53ef\u9760\u5e7f\u64ad\u548c\u5171\u8bc6\u7b97\u6cd5\uff0c\u5145\u5206\u5229\u7528\u4e0d\u5bf9\u79f0\u4fe1\u4efb\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u4e0d\u5bf9\u79f0\u4fe1\u4efb\u6a21\u578b\u4e2d\u7684\u89e3\u51b3\u65b9\u6848\u5047\u8bbe\u8fc7\u4e8e\u4e25\u683c\uff0c\u5f71\u54cd\u4e86\u4e0d\u5bf9\u79f0\u4fe1\u4efb\u7684\u4f18\u52bf\uff0c\u9700\u8981\u66f4\u5f31\u5047\u8bbe\u7684\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u5bf9\u79f0\u95ee\u9898\u7279\u5f81\u5316\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u53ef\u9760\u5e7f\u64ad\u548c\u5171\u8bc6\u7b97\u6cd5\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u7684\u5047\u8bbe\u8981\u6bd4\u73b0\u6709\u65b9\u6848\u66f4\u5f31\u3002", "result": "\u65b0\u7b97\u6cd5\u5728\u66f4\u5f31\u7684\u5047\u8bbe\u4e0b\u5b9e\u73b0\u4e86\u53ef\u9760\u5e7f\u64ad\u548c\u5171\u8bc6\uff0c\u4fdd\u6301\u4e86\u4e0d\u5bf9\u79f0\u4fe1\u4efb\u7684\u4f18\u52bf\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u6838\u5fc3\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e0d\u5bf9\u79f0\u4fe1\u4efb\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u5047\u8bbe\u8981\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.09525", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2509.09525", "abs": "https://arxiv.org/abs/2509.09525", "authors": ["Jialiang Huang", "Teng Ma", "Zheng Liu", "Sixing Lin", "Kang Chen", "Jinlei Jiang", "Xia Liao", "Yingdi Shan", "Yongwei Wu", "Ning Zhang", "Mengting Lu", "Tao Ma", "Haifeng Gong", "Mingxing Zhang"], "title": "TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes", "comment": "38 pages", "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.", "AI": {"tldr": "TrEnv\u662f\u4e00\u4e2a\u4e13\u4e3aLLM\u4ee3\u7406\u4f18\u5316\u7684\u65e0\u670d\u52a1\u5668\u5e73\u53f0\uff0c\u901a\u8fc7\u53ef\u91cd\u7528\u7684\u6c99\u7bb1\u548c\u5185\u5b58\u6a21\u677f\u7b49\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u542f\u52a8\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u5728\u5bb9\u5668\u548cVM\u73af\u5883\u4e2d\u90fd\u6bd4\u73b0\u6709\u7cfb\u7edf\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u5bf9\u4e8eLLM\u4ee3\u7406\u7b49\u65b0\u5174\u5de5\u4f5c\u8d1f\u8f7d\u5b58\u5728\u74f6\u9888\uff0c\u5176\u5f00\u9500\u53ef\u80fd\u8fbe\u5230LLM API\u8c03\u7528\u6210\u672c\u768470%\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u9ad8\u5bc6\u5ea6\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u3002", "method": "TrEnv\u91c7\u7528\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u652f\u6301\u5bb9\u5668\u548cVM\u73af\u5883\uff0c\u901a\u8fc7\u53ef\u91cd\u7528\u7684\u6c99\u7bb1\u3001\u5185\u5b58\u6a21\u677f\u3001\u6d4f\u89c8\u5668\u5171\u4eab\u548c\u9875\u9762\u7f13\u5b58\u7ed5\u8fc7\u673a\u5236\u6765\u4f18\u5316\u6267\u884c\u73af\u5883\u3002", "result": "\u8bc4\u4f30\u663e\u793aTrEnv\u5728\u5bb9\u5668\u73af\u5883\u4e2d\u5c06P99\u5ef6\u8fdf\u964d\u4f4e7\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1148%\uff1b\u5728VM\u73af\u5883\u4e2dP99\u5ef6\u8fdf\u964d\u4f4e58%\uff0c\u5185\u5b58\u8282\u770161%\u3002", "conclusion": "TrEnv\u901a\u8fc7\u4e13\u95e8\u9488\u5bf9LLM\u4ee3\u7406\u9700\u6c42\u7684\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u5728\u652f\u6301\u65b0\u5174\u5de5\u4f5c\u8d1f\u8f7d\u65f6\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
