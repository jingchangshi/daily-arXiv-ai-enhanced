{"id": "2509.21527", "categories": ["cs.DC", "cs.PF", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.21527", "abs": "https://arxiv.org/abs/2509.21527", "authors": ["Mahesh Doijade", "Andrey Alekseenko", "Ania Brown", "Alan Gray", "Szil\u00e1rd P\u00e1ll"], "title": "Redesigning GROMACS Halo Exchange: Improving Strong Scaling with GPU-initiated NVSHMEM", "comment": "17 pages, 8 figures, submitted to PAW-ATM Workshop, SC 2025", "summary": "Improving time-to-solution in molecular dynamics simulations often requires\nstrong scaling due to fixed-sized problems. GROMACS is highly\nlatency-sensitive, with peak iteration rates in the sub-millisecond, making\nscalability on heterogeneous supercomputers challenging. MPI's CPU-centric\nnature introduces additional latencies on GPU-resident applications' critical\npath, hindering GPU utilization and scalability. To address these limitations,\nwe present an NVSHMEM-based GPU kernel-initiated redesign of the GROMACS domain\ndecomposition halo-exchange algorithm. Highly tuned GPU kernels fuse data\npacking and communication, leveraging hardware latency-hiding for fine-grained\noverlap. We employ kernel fusion across overlapped data forwarding\ncommunication phases and utilize the asynchronous copy engine over NVLink to\noptimize latency and bandwidth. Our GPU-resident formulation greatly increases\ncommunication-computation overlap, improving GROMACS strong scaling performance\nacross NVLink by up to 1.5x (intra-node) and 2x (multi-node), and up to 1.3x\nmulti-node over NVLink+InfiniBand. This demonstrates the profound benefits of\nGPU-initiated communication for strong-scaling a broad range of\nlatency-sensitive applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNVSHMEM\u7684GPU\u5185\u6838\u542f\u52a8\u7684GROMACS\u57df\u5206\u89e3halo\u4ea4\u6362\u7b97\u6cd5\u91cd\u8bbe\u8ba1\uff0c\u901a\u8fc7GPU\u5185\u6838\u878d\u5408\u6570\u636e\u6253\u5305\u548c\u901a\u4fe1\uff0c\u5229\u7528\u786c\u4ef6\u5ef6\u8fdf\u9690\u85cf\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u91cd\u53e0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u7684\u5f3a\u6269\u5c55\u6027\u80fd\u3002", "motivation": "\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u9700\u8981\u5f3a\u6269\u5c55\u6027\u6765\u89e3\u51b3\u56fa\u5b9a\u5927\u5c0f\u95ee\u9898\uff0c\u4f46GROMACS\u5bf9\u5ef6\u8fdf\u9ad8\u5ea6\u654f\u611f\uff0cMPI\u7684CPU\u4e2d\u5fc3\u7279\u6027\u5728GPU\u9a7b\u7559\u5e94\u7528\u4e2d\u5f15\u5165\u4e86\u989d\u5916\u5ef6\u8fdf\uff0c\u963b\u788d\u4e86GPU\u5229\u7528\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528NVSHMEM\u7684GPU\u5185\u6838\u542f\u52a8\u91cd\u8bbe\u8ba1\uff0c\u9ad8\u5ea6\u8c03\u4f18\u7684GPU\u5185\u6838\u878d\u5408\u6570\u636e\u6253\u5305\u548c\u901a\u4fe1\uff0c\u5229\u7528\u786c\u4ef6\u5ef6\u8fdf\u9690\u85cf\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u91cd\u53e0\uff0c\u5728\u91cd\u53e0\u6570\u636e\u8f6c\u53d1\u901a\u4fe1\u9636\u6bb5\u8fdb\u884c\u5185\u6838\u878d\u5408\uff0c\u5e76\u901a\u8fc7NVLink\u4f7f\u7528\u5f02\u6b65\u590d\u5236\u5f15\u64ce\u4f18\u5316\u5ef6\u8fdf\u548c\u5e26\u5bbd\u3002", "result": "GPU\u9a7b\u7559\u65b9\u6848\u5927\u5e45\u589e\u52a0\u4e86\u901a\u4fe1\u8ba1\u7b97\u91cd\u53e0\uff0c\u5728NVLink\u4e0aGROMACS\u5f3a\u6269\u5c55\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe1.5\u500d\uff08\u8282\u70b9\u5185\uff09\u548c2\u500d\uff08\u591a\u8282\u70b9\uff09\uff0c\u5728NVLink+InfiniBand\u591a\u8282\u70b9\u73af\u5883\u4e0b\u63d0\u5347\u8fbe1.3\u500d\u3002", "conclusion": "GPU\u542f\u52a8\u901a\u4fe1\u5bf9\u4e8e\u5f3a\u6269\u5c55\u5404\u79cd\u5ef6\u8fdf\u654f\u611f\u5e94\u7528\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2509.21841", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.21841", "abs": "https://arxiv.org/abs/2509.21841", "authors": ["Chang Chen", "Tiancheng Chen", "Jiangfei Duan", "Qianchao Zhu", "Zerui Wang", "Qinghao Hu", "Peng Sun", "Xiuhong Li", "Chao Yang", "Torsten Hoefler"], "title": "Zeppelin: Balancing Variable-length Workloads in Data Parallel Large Model Training", "comment": null, "summary": "Training large language models (LLMs) with increasingly long and varying\nsequence lengths introduces severe load imbalance challenges in large-scale\ndata-parallel training. Recent frameworks attempt to mitigate these issues\nthrough data reorganization or hybrid parallel strategies. However, they often\noverlook how computational and communication costs scale with sequence length,\nresulting in suboptimal performance. We identify three critical challenges: (1)\nvarying computation-to-communication ratios across sequences of different\nlengths in distributed attention, (2) mismatch between static NIC-GPU affinity\nand dynamic parallel workloads, and (3) distinct optimal partitioning\nstrategies required for quadratic attention versus linear components. To\naddress these challenges, we present Zeppelin, a novel training system that\nintegrates three key techniques: (1) a hierarchical sequence partitioning\nmethod for the attention module that reduces communication overhead and\nbalances computation, supported by an efficient attention engine that applies\ndivergent parallel strategies; (2) a routing layer that orchestrates inter-node\ntransfers to fully utilize NIC bandwidth; and (3) a remapping layer that\ntransforms sequence layouts between attention and linear modules, ensuring high\ncomputational efficiency across both. Comprehensive evaluations across diverse\nconfigurations show that Zeppelin delivers an average 2.80x speedup over\nstate-of-the-art methods.", "AI": {"tldr": "Zeppelin\u662f\u4e00\u4e2a\u9488\u5bf9\u957f\u5e8f\u5217LLM\u8bad\u7ec3\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u5e8f\u5217\u5206\u533a\u3001\u8def\u7531\u5c42\u548c\u91cd\u6620\u5c04\u5c42\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u901a\u4fe1\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e862.80\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540c\u957f\u5ea6\u5e8f\u5217\u65f6\u5b58\u5728\u8ba1\u7b97\u901a\u4fe1\u6210\u672c\u6bd4\u4f8b\u53d8\u5316\u3001\u9759\u6001NIC-GPU\u4eb2\u548c\u6027\u4e0e\u52a8\u6001\u5e76\u884c\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5339\u914d\u3001\u4ee5\u53ca\u6ce8\u610f\u529b\u6a21\u5757\u548c\u7ebf\u6027\u6a21\u5757\u9700\u8981\u4e0d\u540c\u5206\u533a\u7b56\u7565\u7684\u95ee\u9898\u3002", "method": "1. \u6ce8\u610f\u529b\u6a21\u5757\u7684\u5c42\u6b21\u5316\u5e8f\u5217\u5206\u533a\u65b9\u6cd5\uff0c\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u5e76\u5e73\u8861\u8ba1\u7b97\uff1b2. \u8def\u7531\u5c42\u534f\u8c03\u8282\u70b9\u95f4\u4f20\u8f93\u4ee5\u5145\u5206\u5229\u7528NIC\u5e26\u5bbd\uff1b3. \u91cd\u6620\u5c04\u5c42\u5728\u6ce8\u610f\u529b\u548c\u7ebf\u6027\u6a21\u5757\u95f4\u8f6c\u6362\u5e8f\u5217\u5e03\u5c40\u3002", "result": "\u5728\u5404\u79cd\u914d\u7f6e\u4e0b\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0cZeppelin\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5e73\u57472.80\u500d\u7684\u52a0\u901f\u3002", "conclusion": "Zeppelin\u901a\u8fc7\u96c6\u6210\u4e09\u79cd\u5173\u952e\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5e8f\u5217LLM\u8bad\u7ec3\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2509.22068", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.22068", "abs": "https://arxiv.org/abs/2509.22068", "authors": ["Sebastian Werner", "Mathis K\u00e4hler", "Alireza Hakamian"], "title": "Code once, Run Green: Automated Green Code Translation in Serverless Computing", "comment": "Accepted at IC2E 2025", "summary": "The rapid digitization and the increasing use of emerging technologies such\nas AI models have significantly contributed to the emissions of computing\ninfrastructure. Efforts to mitigate this impact typically focus on the\ninfrastructure level such as powering data centers with renewable energy, or\nthrough the specific design of energy-efficient software. However, both\nstrategies rely on stakeholder intervention, making their adoption in legacy\nand already-deployed systems unlikely. As a result, past architectural and\nimplementation decisions continue to incur additional energy usage - a\nphenomenon we refer to as energy debt.\n  Hence, in this paper, we investigate the potential of serverless computing\nplatforms to automatically reduce energy debt by leveraging the unique access\nto function source code. Specifically, we explore whether large language models\n(LLMs) can translate serverless functions into more energy-efficient\nprogramming languages while preserving functional correctness. To this end, we\ndesign and implement ReFaaS and integrate it into the Fission serverless\nframework. We evaluate multiple LLMs on their ability to perform such code\ntranslations and analyze their impact on energy consumption.\n  Our preliminary results indicate that translated functions can reduce\ninvocation energy by up to 70%, achieving net energy savings after\napproximately 3,000 to 5,000 invocations, depending on the LLM used.\nNonetheless, the approach faces several challenges: not all functions are\nsuitable for translation, and for some, the amortization threshold is\nsignificantly higher or unreachable. Despite these limitations, we identify\nfour key research challenges whose resolution could unlock long-term, automated\nmitigation of energy debt in serverless computing.", "AI": {"tldr": "\u5229\u7528LLMs\u5c06\u65e0\u670d\u52a1\u5668\u51fd\u6570\u7ffb\u8bd1\u4e3a\u66f4\u8282\u80fd\u7684\u7f16\u7a0b\u8bed\u8a00\uff0c\u53ef\u51cf\u5c11\u9ad8\u8fbe70%\u7684\u8c03\u7528\u80fd\u8017\uff0c\u4f46\u9762\u4e34\u51fd\u6570\u9002\u7528\u6027\u548c\u644a\u9500\u9608\u503c\u7b49\u6311\u6218\u3002", "motivation": "\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u7684\u78b3\u6392\u653e\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u4f9d\u8d56\u5229\u76ca\u76f8\u5173\u8005\u5e72\u9884\uff0c\u96be\u4ee5\u5728\u9057\u7559\u7cfb\u7edf\u4e2d\u5b9e\u65bd\uff0c\u5bfc\u81f4\u80fd\u6e90\u503a\u52a1\u6301\u7eed\u7d2f\u79ef\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0ReFaaS\u7cfb\u7edf\uff0c\u96c6\u6210\u5230Fission\u65e0\u670d\u52a1\u5668\u6846\u67b6\u4e2d\uff0c\u5229\u7528\u591a\u79cdLLMs\u5c06\u51fd\u6570\u4ee3\u7801\u7ffb\u8bd1\u4e3a\u66f4\u8282\u80fd\u7684\u7f16\u7a0b\u8bed\u8a00\uff0c\u540c\u65f6\u4fdd\u6301\u529f\u80fd\u6b63\u786e\u6027\u3002", "result": "\u7ffb\u8bd1\u540e\u7684\u51fd\u6570\u53ef\u51cf\u5c11\u9ad8\u8fbe70%\u7684\u8c03\u7528\u80fd\u8017\uff0c\u5728\u7ea63000-5000\u6b21\u8c03\u7528\u540e\u5b9e\u73b0\u51c0\u8282\u80fd\uff0c\u4f46\u5e76\u975e\u6240\u6709\u51fd\u6570\u90fd\u9002\u5408\u7ffb\u8bd1\u3002", "conclusion": "\u867d\u7136\u9762\u4e34\u6311\u6218\uff0c\u4f46\u8bc6\u522b\u51fa\u56db\u4e2a\u5173\u952e\u7814\u7a76\u6311\u6218\uff0c\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u53ef\u4e3a\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u4e2d\u7684\u80fd\u6e90\u503a\u52a1\u63d0\u4f9b\u957f\u671f\u81ea\u52a8\u5316\u7f13\u89e3\u65b9\u6848\u3002"}}
{"id": "2509.22117", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22117", "abs": "https://arxiv.org/abs/2509.22117", "authors": ["Lucio Anderlini", "Giulio Bianchini", "Diego Ciangottini", "Stefano Dal Pra", "Diego Michelotto", "Rosa Petrini", "Daniele Spiga"], "title": "The AI_INFN Platform: Artificial Intelligence Development in the Cloud", "comment": "To be published in SciPost Physics Proceedings for European AI for\n  Fundamental Physics Conference (EuCAIFCon 2025)", "summary": "Machine Learning (ML) is driving a revolution in the way scientists design,\ndevelop, and deploy data-intensive software. However, the adoption of ML\npresents new challenges for the computing infrastructure, particularly in terms\nof provisioning and orchestrating access to hardware accelerators for\ndevelopment, testing, and production. The INFN-funded project AI_INFN\n(Artificial Intelligence at INFN) aims at fostering the adoption of ML\ntechniques within INFN use cases by providing support on multiple aspects,\nincluding the provisioning of AI-tailored computing resources. It leverages\ncloud-native solutions in the context of INFN Cloud, to share hardware\naccelerators as effectively as possible, ensuring the diversity of the\nInstitute's research activities is not compromised. In this contribution, we\nprovide an update on the commissioning of a Kubernetes platform designed to\nease the development of GPU-powered data analysis workflows and their\nscalability on heterogeneous distributed computing resources, also using the\noffloading mechanism with Virtual Kubelet and InterLink API. This setup can\nmanage workflows across different resource providers, including sites of the\nWorldwide LHC Computing Grid and supercomputers such as CINECA Leonardo,\nproviding a model for use cases requiring dedicated infrastructures for\ndifferent parts of the workload. Initial test results, emerging case studies,\nand integration scenarios will be presented with functional tests and\nbenchmarks.", "AI": {"tldr": "AI_INFN\u9879\u76ee\u901a\u8fc7Kubernetes\u5e73\u53f0\u548c\u4e91\u539f\u751f\u89e3\u51b3\u65b9\u6848\uff0c\u4e3aINFN\u7684\u673a\u5668\u5b66\u4e60\u7528\u4f8b\u63d0\u4f9bGPU\u52a0\u901f\u5668\u8d44\u6e90\u7ba1\u7406\uff0c\u652f\u6301\u8de8\u4e0d\u540c\u8ba1\u7b97\u8d44\u6e90\uff08\u5305\u62ecLHC\u8ba1\u7b97\u7f51\u683c\u548c\u8d85\u7ea7\u8ba1\u7b97\u673a\uff09\u7684\u5de5\u4f5c\u6d41\u5f00\u53d1\u548c\u6269\u5c55\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u79d1\u5b66\u8f6f\u4ef6\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u5bf9\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\uff08\u7279\u522b\u662f\u786c\u4ef6\u52a0\u901f\u5668\u7684\u914d\u7f6e\u548c\u7f16\u6392\uff09\u63d0\u51fa\u4e86\u65b0\u6311\u6218\u3002AI_INFN\u9879\u76ee\u65e8\u5728\u4fc3\u8fdbINFN\u5185\u90e8ML\u6280\u672f\u7684\u91c7\u7528\uff0c\u901a\u8fc7\u63d0\u4f9bAI\u5b9a\u5236\u7684\u8ba1\u7b97\u8d44\u6e90\u652f\u6301\u3002", "method": "\u5229\u7528\u4e91\u539f\u751f\u89e3\u51b3\u65b9\u6848\u548cKubernetes\u5e73\u53f0\uff0c\u7ed3\u5408Virtual Kubelet\u548cInterLink API\u7684\u5378\u8f7d\u673a\u5236\uff0c\u7ba1\u7406\u8de8\u4e0d\u540c\u8d44\u6e90\u63d0\u4f9b\u5546\uff08\u5305\u62ecLHC\u8ba1\u7b97\u7f51\u683c\u548c\u8d85\u7ea7\u8ba1\u7b97\u673a\uff09\u7684GPU\u52a0\u901f\u5de5\u4f5c\u6d41\u3002", "result": "\u521d\u6b65\u6d4b\u8bd5\u7ed3\u679c\u3001\u6848\u4f8b\u7814\u7a76\u548c\u96c6\u6210\u573a\u666f\u901a\u8fc7\u529f\u80fd\u6d4b\u8bd5\u548c\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u8be5\u5e73\u53f0\u5728\u5f02\u6784\u5206\u5e03\u5f0f\u8ba1\u7b97\u8d44\u6e90\u4e0a\u6709\u6548\u7ba1\u7406GPU\u52a0\u901f\u5de5\u4f5c\u6d41\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4e3a\u9700\u8981\u4e3a\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u540c\u90e8\u5206\u63d0\u4f9b\u4e13\u7528\u57fa\u7840\u8bbe\u65bd\u7684\u7528\u4f8b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u5171\u4eab\u786c\u4ef6\u52a0\u901f\u5668\u8d44\u6e90\uff0c\u540c\u65f6\u786e\u4fdd\u7814\u7a76\u6240\u591a\u6837\u5316\u7684\u7814\u7a76\u6d3b\u52a8\u4e0d\u53d7\u5f71\u54cd\u3002"}}
{"id": "2509.21762", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.21762", "abs": "https://arxiv.org/abs/2509.21762", "authors": ["Ian McDougall", "Michael Davies", "Rahul Chatterjee", "Somesh Jha", "Karthikeyan Sankaralingam"], "title": "Privacy-Preserving Performance Profiling of In-The-Wild GPUs", "comment": "26 pages, 10 figures", "summary": "GPUs are the dominant platform for many important applications today\nincluding deep learning, accelerated computing, and scientific simulation.\nHowever, as the complexity of both applications and hardware increases, GPU\nchip manufacturers face a significant challenge: how to gather comprehensive\nperformance characteristics and value profiles from GPUs deployed in real-world\nscenarios. Such data, encompassing the types of kernels executed and the time\nspent in each, is crucial for optimizing chip design and enhancing application\nperformance. Unfortunately, despite the availability of low-level tools like\nNSYS and NCU, current methodologies fall short, offering data collection\ncapabilities only on an individual user basis rather than a broader, more\ninformative fleet-wide scale. This paper takes on the problem of realizing a\nsystem that allows planet-scale real-time GPU performance profiling of\nlow-level hardware characteristics. The three fundamental problems we solve\nare: i) user experience of achieving this with no slowdown; ii) preserving user\nprivacy, so that no 3rd party is aware of what applications any user runs; iii)\nefficacy in showing we are able to collect data and assign it applications even\nwhen run on 1000s of GPUs. Our results simulate a 100,000 size GPU deployment,\nrunning applications from the Torchbench suite, showing our system addresses\nall 3 problems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u884c\u661f\u7ea7\u5b9e\u65f6GPU\u6027\u80fd\u5206\u6790\u7cfb\u7edf\uff0c\u89e3\u51b3\u5927\u89c4\u6a21GPU\u90e8\u7f72\u4e2d\u7684\u6027\u80fd\u6570\u636e\u6536\u96c6\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u8bc1\u96f6\u6027\u80fd\u635f\u5931\u548c\u7528\u6237\u9690\u79c1\u4fdd\u62a4", "motivation": "\u968f\u7740GPU\u5e94\u7528\u548c\u786c\u4ef6\u590d\u6742\u6027\u589e\u52a0\uff0c\u5236\u9020\u5546\u9700\u8981\u4ece\u771f\u5b9e\u90e8\u7f72\u73af\u5883\u4e2d\u6536\u96c6\u5168\u9762\u7684\u6027\u80fd\u6570\u636e\u6765\u4f18\u5316\u82af\u7247\u8bbe\u8ba1\u548c\u5e94\u7528\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u53ea\u80fd\u63d0\u4f9b\u5355\u4e2a\u7528\u6237\u7ea7\u522b\u7684\u6570\u636e\u6536\u96c6", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u6570\u5343\u4e2aGPU\u4e0a\u5b9e\u65f6\u6536\u96c6\u4f4e\u5c42\u7ea7\u786c\u4ef6\u6027\u80fd\u7279\u5f81\u6570\u636e\uff0c\u540c\u65f6\u786e\u4fdd\u96f6\u6027\u80fd\u635f\u5931\u548c\u7528\u6237\u9690\u79c1\u4fdd\u62a4", "result": "\u5728\u6a21\u62df\u768410\u4e07\u4e2aGPU\u90e8\u7f72\u73af\u5883\u4e2d\u8fd0\u884cTorchbench\u5957\u4ef6\u5e94\u7528\uff0c\u8bc1\u660e\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6240\u6709\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u884c\u661f\u7ea7GPU\u6027\u80fd\u5206\u6790\uff0c\u4e3aGPU\u5236\u9020\u5546\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u90e8\u7f72\u73af\u5883\u4e2d\u7684\u5b9d\u8d35\u6027\u80fd\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u62a4\u4e86\u7528\u6237\u9690\u79c1"}}
{"id": "2509.21629", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21629", "abs": "https://arxiv.org/abs/2509.21629", "authors": ["Anjiang Wei", "Tarun Suresh", "Tianran Sun", "Haoze Wu", "Ke Wang", "Alex Aiken"], "title": "InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?", "comment": null, "summary": "Program verification relies on loop invariants, yet automatically discovering\nstrong invariants remains a long-standing challenge. We introduce a principled\nframework for evaluating LLMs on invariant synthesis. Our approach uses a\nverifier-based decision procedure with a formal soundness guarantee and\nassesses not only correctness but also the speedup that invariants provide in\nverification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based\nverifiers against the traditional solver UAutomizer. While LLM-based verifiers\nrepresent a promising direction, they do not yet offer a significant advantage\nover UAutomizer. Model capability also proves critical, as shown by sharp\ndifferences in speedups across models, and our benchmark remains an open\nchallenge for current LLMs. Finally, we show that supervised fine-tuning and\nBest-of-N sampling can improve performance: fine-tuning on 3589 instances\nraises the percentage of speedup cases for Qwen3-Coder-480B from 8% to 29.2%,\nand Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30LLMs\u5728\u5faa\u73af\u4e0d\u53d8\u5f0f\u5408\u6210\u4e0a\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5668\u51b3\u7b56\u7a0b\u5e8f\u8bc4\u4f30\u6b63\u786e\u6027\u548c\u9a8c\u8bc1\u901f\u5ea6\u63d0\u5347\uff0c\u53d1\u73b0\u5f53\u524dLLM\u9a8c\u8bc1\u5668\u76f8\u6bd4\u4f20\u7edf\u6c42\u89e3\u5668UAutomizer\u5c1a\u65e0\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u76d1\u7763\u5fae\u8c03\u548cBest-of-N\u91c7\u6837\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7a0b\u5e8f\u9a8c\u8bc1\u4f9d\u8d56\u5faa\u73af\u4e0d\u53d8\u5f0f\uff0c\u4f46\u81ea\u52a8\u53d1\u73b0\u5f3a\u4e0d\u53d8\u5f0f\u4ecd\u662f\u957f\u671f\u6311\u6218\uff0c\u9700\u8981\u5efa\u7acb\u539f\u5219\u6027\u6846\u67b6\u6765\u8bc4\u4f30LLMs\u5728\u4e0d\u53d8\u5f0f\u5408\u6210\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u9a8c\u8bc1\u5668\u7684\u51b3\u7b56\u7a0b\u5e8f\uff0c\u5177\u6709\u5f62\u5f0f\u5316\u6b63\u786e\u6027\u4fdd\u8bc1\uff0c\u8bc4\u4f30\u4e0d\u53d8\u5f0f\u6b63\u786e\u6027\u548c\u9a8c\u8bc1\u901f\u5ea6\u63d0\u5347\u3002\u8bc4\u4f30\u4e867\u4e2a\u6700\u5148\u8fdbLLMs\u548c\u73b0\u6709LLM\u9a8c\u8bc1\u5668\uff0c\u5e76\u4e0e\u4f20\u7edf\u6c42\u89e3\u5668UAutomizer\u5bf9\u6bd4\u3002", "result": "LLM\u9a8c\u8bc1\u5668\u76ee\u524d\u76f8\u6bd4UAutomizer\u65e0\u663e\u8457\u4f18\u52bf\uff0c\u6a21\u578b\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4e0d\u540c\u6a21\u578b\u7684\u901f\u5ea6\u63d0\u5347\u5dee\u5f02\u660e\u663e\u3002\u76d1\u7763\u5fae\u8c03\uff083589\u4e2a\u5b9e\u4f8b\uff09\u4f7fQwen3-Coder-480B\u7684\u901f\u5ea6\u63d0\u5347\u6848\u4f8b\u4ece8%\u63d0\u9ad8\u523029.2%\uff0cBest-of-N\u91c7\u6837\uff08N=16\uff09\u4f7fClaude-sonnet-4\u4ece8.8%\u63d0\u5347\u523022.1%\u3002", "conclusion": "LLM\u9a8c\u8bc1\u5668\u662f\u6709\u524d\u666f\u7684\u65b9\u5411\u4f46\u5c1a\u672a\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff0c\u5f53\u524d\u57fa\u51c6\u5bf9LLMs\u4ecd\u662f\u5f00\u653e\u6311\u6218\uff0c\u76d1\u7763\u5fae\u8c03\u548c\u91c7\u6837\u6280\u672f\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2509.22233", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2509.22233", "abs": "https://arxiv.org/abs/2509.22233", "authors": ["Thomas Boudier", "Filippo Casagrande", "Avinandan Das", "Massimo Equi", "Henrik Lievonen", "Augusto Modanese", "Ronja Stimpert"], "title": "Orientation does not help with 3-coloring a grid in online-LOCAL", "comment": "16 pages, 3 figures", "summary": "The online-LOCAL and SLOCAL models are extensions of the LOCAL model where\nnodes are processed in a sequential but potentially adversarial order. So far,\nthe only problem we know of where the global memory of the online-LOCAL model\nhas an advantage over SLOCAL is 3-coloring bipartite graphs. Recently, Chang et\nal. [PODC 2024] showed that even in grids, 3-coloring requires $\\Omega(\\log n)$\nlocality in deterministic online-LOCAL. This result was subsequently extended\nby Akbari et al. [STOC 2025] to also hold in randomized online-LOCAL. However,\nboth proofs heavily rely on the assumption that the algorithm does not have\naccess to the orientation of the underlying grid. In this paper, we show how to\nlift this requirement and obtain the same lower bound (against either model)\neven when the algorithm is explicitly given a globally consistent orientation\nof the grid.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u5373\u4f7f\u5728\u7b97\u6cd5\u660e\u786e\u83b7\u5f97\u7f51\u683c\u7684\u5168\u5c40\u4e00\u81f4\u65b9\u5411\u7684\u60c5\u51b5\u4e0b\uff0c3-\u7740\u8272\u7f51\u683c\u95ee\u9898\u5728\u786e\u5b9a\u6027online-LOCAL\u548c\u968f\u673a\u5316online-LOCAL\u6a21\u578b\u4e2d\u4ecd\u7136\u9700\u8981\u03a9(log n)\u7684\u5c40\u90e8\u6027\u3002", "motivation": "\u5148\u524d\u7684\u7814\u7a76\u8868\u660e3-\u7740\u8272\u7f51\u683c\u5728online-LOCAL\u6a21\u578b\u4e2d\u9700\u8981\u03a9(log n)\u5c40\u90e8\u6027\uff0c\u4f46\u8fd9\u4e9b\u8bc1\u660e\u90fd\u4f9d\u8d56\u4e8e\u7b97\u6cd5\u65e0\u6cd5\u8bbf\u95ee\u5e95\u5c42\u7f51\u683c\u65b9\u5411\u7684\u5047\u8bbe\u3002\u672c\u6587\u65e8\u5728\u79fb\u9664\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u65b0\u7684\u6280\u672f\u65b9\u6cd5\uff0c\u4f7f\u5f97\u5373\u4f7f\u7b97\u6cd5\u88ab\u660e\u786e\u63d0\u4f9b\u4e86\u7f51\u683c\u7684\u5168\u5c40\u4e00\u81f4\u65b9\u5411\uff0c\u4ecd\u7136\u80fd\u591f\u8bc1\u660e\u76f8\u540c\u7684\u4e0b\u754c\u6210\u7acb\u3002", "result": "\u6210\u529f\u8bc1\u660e\u4e86\u5728\u786e\u5b9a\u6027online-LOCAL\u548c\u968f\u673a\u5316online-LOCAL\u6a21\u578b\u4e2d\uff0c3-\u7740\u8272\u7f51\u683c\u95ee\u9898\u5373\u4f7f\u6709\u5168\u5c40\u65b9\u5411\u4fe1\u606f\uff0c\u4ecd\u7136\u9700\u8981\u03a9(log n)\u7684\u5c40\u90e8\u6027\u3002", "conclusion": "\u7f51\u683c\u65b9\u5411\u4fe1\u606f\u5e76\u4e0d\u80fd\u5e2e\u52a9\u964d\u4f4e3-\u7740\u8272\u95ee\u9898\u5728online-LOCAL\u6a21\u578b\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u8fd9\u5f3a\u5316\u4e86\u8be5\u95ee\u9898\u7684\u56fa\u6709\u96be\u5ea6\u3002"}}
{"id": "2509.22410", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22410", "abs": "https://arxiv.org/abs/2509.22410", "authors": ["Shayne Wadle", "Yanxin Zhang", "Vikas Singh", "Karthikeyan Sankaralingam"], "title": "NeuroScalar: A Deep Learning Framework for Fast, Accurate, and In-the-Wild Cycle-Level Performance Prediction", "comment": null, "summary": "The evaluation of new microprocessor designs is constrained by slow,\ncycle-accurate simulators that rely on unrepresentative benchmark traces. This\npaper introduces a novel deep learning framework for high-fidelity,\n``in-the-wild'' simulation on production hardware. Our core contribution is a\nDL model trained on microarchitecture-independent features to predict\ncycle-level performance for hypothetical processor designs. This unique\napproach allows the model to be deployed on existing silicon to evaluate future\nhardware. We propose a complete system featuring a lightweight hardware trace\ncollector and a principled sampling strategy to minimize user impact. This\nsystem achieves a simulation speed of 5 MIPS on a commodity GPU, imposing a\nmere 0.1% performance overhead. Furthermore, our co-designed Neutrino on-chip\naccelerator improves performance by 85x over the GPU. We demonstrate that this\nframework enables accurate performance analysis and large-scale hardware A/B\ntesting on a massive scale using real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5fae\u5904\u7406\u5668\u6027\u80fd\u6a21\u62df\u6846\u67b6\uff0c\u53ef\u5728\u73b0\u6709\u786c\u4ef6\u4e0a\u8bc4\u4f30\u672a\u6765\u5904\u7406\u5668\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u3001\u5927\u89c4\u6a21\u786c\u4ef6A/B\u6d4b\u8bd5", "motivation": "\u4f20\u7edf\u5fae\u5904\u7406\u5668\u8bbe\u8ba1\u8bc4\u4f30\u53d7\u9650\u4e8e\u7f13\u6162\u7684\u5468\u671f\u7cbe\u786e\u6a21\u62df\u5668\uff0c\u4e14\u4f9d\u8d56\u4e0d\u5177\u4ee3\u8868\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u8f68\u8ff9", "method": "\u4f7f\u7528\u5fae\u67b6\u6784\u65e0\u5173\u7279\u5f81\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u5468\u671f\u7ea7\u6027\u80fd\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u786c\u4ef6\u8f68\u8ff9\u6536\u96c6\u5668\u548c\u91c7\u6837\u7b56\u7565", "result": "\u5728\u5546\u7528GPU\u4e0a\u5b9e\u73b05 MIPS\u6a21\u62df\u901f\u5ea6\uff0c\u4ec50.1%\u6027\u80fd\u5f00\u9500\uff1b\u4e13\u7528\u52a0\u901f\u5668Neutrino\u6bd4GPU\u63d0\u534785\u500d\u6027\u80fd", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u771f\u5b9e\u5e94\u7528\u4e0a\u5b9e\u73b0\u51c6\u786e\u7684\u6027\u80fd\u5206\u6790\u548c\u5927\u89c4\u6a21\u786c\u4ef6A/B\u6d4b\u8bd5"}}
{"id": "2509.21793", "categories": ["cs.PL", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.21793", "abs": "https://arxiv.org/abs/2509.21793", "authors": ["Jianhong Zhao", "Everett Hildenbrandt", "Juan Conejero", "Yongwang Zhao"], "title": "Compiling by Proving: Language-Agnostic Automatic Optimization from Formal Semantics", "comment": null, "summary": "Verification proofs encode complete program behavior, yet we discard them\nafter checking correctness. We present compiling by proving, a paradigm that\ntransforms these proofs into optimized execution rules. By constructing\nAll-Path Reachability Proofs through symbolic execution and compiling their\ngraph structure, we consolidate many semantic rewrites into single rules while\npreserving correctness by construction. We implement this as a\nlanguage-agnostic extension to the K framework. Evaluation demonstrates\nperformance improvements across different compilation scopes: opcode-level\noptimizations show consistent speedups, while whole-program compilation\nachieves orders of magnitude greater performance gains.", "AI": {"tldr": "\u63d0\u51fa\"\u901a\u8fc7\u8bc1\u660e\u7f16\u8bd1\"\u7684\u65b0\u8303\u5f0f\uff0c\u5c06\u9a8c\u8bc1\u8bc1\u660e\u8f6c\u5316\u4e3a\u4f18\u5316\u7684\u6267\u884c\u89c4\u5219\uff0c\u901a\u8fc7\u7b26\u53f7\u6267\u884c\u6784\u5efa\u5168\u8def\u5f84\u53ef\u8fbe\u6027\u8bc1\u660e\u5e76\u7f16\u8bd1\u5176\u56fe\u7ed3\u6784\uff0c\u5728\u4fdd\u6301\u6b63\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u9a8c\u8bc1\u8bc1\u660e\u7f16\u7801\u4e86\u5b8c\u6574\u7684\u7a0b\u5e8f\u884c\u4e3a\uff0c\u4f46\u5728\u68c0\u67e5\u6b63\u786e\u6027\u540e\u5c31\u88ab\u4e22\u5f03\u4e86\uff0c\u8fd9\u4e9b\u8bc1\u660e\u4e2d\u8574\u542b\u7684\u8bed\u4e49\u4fe1\u606f\u53ef\u4ee5\u88ab\u5229\u7528\u6765\u4f18\u5316\u7a0b\u5e8f\u6267\u884c\u3002", "method": "\u6784\u5efa\u5168\u8def\u5f84\u53ef\u8fbe\u6027\u8bc1\u660e\uff0c\u901a\u8fc7\u7b26\u53f7\u6267\u884c\u548c\u7f16\u8bd1\u8bc1\u660e\u7684\u56fe\u7ed3\u6784\uff0c\u5c06\u591a\u4e2a\u8bed\u4e49\u91cd\u5199\u5408\u5e76\u4e3a\u5355\u4e00\u89c4\u5219\uff0c\u5728K\u6846\u67b6\u4e2d\u5b9e\u73b0\u8bed\u8a00\u65e0\u5173\u7684\u6269\u5c55\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u5728\u4e0d\u540c\u7f16\u8bd1\u8303\u56f4\u5185\u90fd\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\uff1a\u64cd\u4f5c\u7801\u7ea7\u4f18\u5316\u663e\u793a\u4e00\u81f4\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u800c\u5168\u7a0b\u5e8f\u7f16\u8bd1\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u66f4\u9ad8\u7684\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "\u901a\u8fc7\u8bc1\u660e\u7f16\u8bd1\u662f\u4e00\u79cd\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5229\u7528\u9a8c\u8bc1\u8bc1\u660e\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u751f\u6210\u4f18\u5316\u7684\u6267\u884c\u89c4\u5219\uff0c\u5728\u4fdd\u6301\u6b63\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2509.22512", "categories": ["cs.AR", "n/a"], "pdf": "https://arxiv.org/pdf/2509.22512", "abs": "https://arxiv.org/abs/2509.22512", "authors": ["Soroush Ahadi", "Mehdi Modarressi", "Masoud Daneshtalab"], "title": "AxLLM: accelerator architecture for large language models with computation reuse capability", "comment": "7 pages, 9 figures", "summary": "Large language models demand massive computational power and memory\nresources, posing significant challenges for efficient deployment. While\nquantization has been widely explored to reduce model size and computation,\nthis paper demonstrates an additional benefit: quantization increases parameter\nlocality, creating opportunities for computation reuse. Building on this\ninsight, we propose AxLLM, a hardware accelerator architecture designed for\nquantized models. Axllm introduces a novel redundancy elimination technique\nthat caches and reuses multiplication results for repeated weight values,\nsubstantially reducing redundant operations. The architecture features dual\nmultiply and reuse pipelines, efficiently supporting both base models and LoRA\nfine-tuned models without altering parameters, retraining, or requiring offline\npreprocessing. Experimental results show that AxLLM achieves up to 90%\nreduction in computations, delivering 28% lower energy consumption and a 1.7x\nspeedup over baseline execution. These results highlight Axllm as a scalable\nand efficient solution for accelerating LLMs on specialized hardware.", "AI": {"tldr": "AxLLM\u662f\u4e00\u79cd\u9488\u5bf9\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u786c\u4ef6\u52a0\u901f\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u6d88\u9664\u5197\u4f59\u8ba1\u7b97\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe90%\u7684\u8ba1\u7b97\u91cf\u51cf\u5c11\u300128%\u7684\u80fd\u8017\u964d\u4f4e\u548c1.7\u500d\u7684\u52a0\u901f\u6bd4\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5de8\u5927\u7684\u8ba1\u7b97\u80fd\u529b\u548c\u5185\u5b58\u8d44\u6e90\uff0c\u90e8\u7f72\u6548\u7387\u9762\u4e34\u6311\u6218\u3002\u91cf\u5316\u4e0d\u4ec5\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\uff0c\u8fd8\u80fd\u589e\u52a0\u53c2\u6570\u5c40\u90e8\u6027\uff0c\u4e3a\u8ba1\u7b97\u91cd\u7528\u521b\u9020\u673a\u4f1a\u3002", "method": "\u63d0\u51faAxLLM\u786c\u4ef6\u52a0\u901f\u5668\u67b6\u6784\uff0c\u91c7\u7528\u65b0\u9896\u7684\u5197\u4f59\u6d88\u9664\u6280\u672f\uff0c\u7f13\u5b58\u5e76\u91cd\u7528\u91cd\u590d\u6743\u91cd\u503c\u7684\u4e58\u6cd5\u7ed3\u679c\u3002\u67b6\u6784\u5305\u542b\u53cc\u4e58\u6cd5\u548c\u91cd\u7528\u6d41\u6c34\u7ebf\uff0c\u652f\u6301\u57fa\u7840\u6a21\u578b\u548cLoRA\u5fae\u8c03\u6a21\u578b\uff0c\u65e0\u9700\u53c2\u6570\u4fee\u6539\u3001\u91cd\u65b0\u8bad\u7ec3\u6216\u79bb\u7ebf\u9884\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cAxLLM\u5b9e\u73b0\u4e86\u9ad8\u8fbe90%\u7684\u8ba1\u7b97\u91cf\u51cf\u5c11\uff0c\u80fd\u8017\u964d\u4f4e28%\uff0c\u901f\u5ea6\u63d0\u53471.7\u500d\u3002", "conclusion": "AxLLM\u4e3a\u5728\u4e13\u7528\u786c\u4ef6\u4e0a\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22614", "categories": ["cs.PL", "D.3.1; F.3.2; D.3.2; D.3.3"], "pdf": "https://arxiv.org/pdf/2509.22614", "abs": "https://arxiv.org/abs/2509.22614", "authors": ["Dmitri Volkov", "Yafei Yang", "Chung-chieh Shan"], "title": "Committing to the bit: Relational programming with semiring arrays and SAT solving", "comment": "12 pages, for associated repo see\n  https://github.com/sporkl/semiringkanren", "summary": "We propose semiringKanren, a relational programming language where each\nrelation expression denotes a semiring array. We formalize a type system that\nrestricts the arrays to finite size. We then define a semantics that is\nparameterized by the semiring that the arrays draw their elements from. We\ncompile semiringKanren types to bitstring representations. For the Boolean\nsemiring, this compilation enables us to use an SAT solver to run\nsemiringKanren programs efficiently. We compare the performance of\nsemiringKanren and faster miniKanren for solving Sudoku puzzles. Our experiment\nshows that semiringKanren can be a more efficient variant of miniKanren.", "AI": {"tldr": "\u63d0\u51fa\u4e86semiringKanren\u5173\u7cfb\u7f16\u7a0b\u8bed\u8a00\uff0c\u5c06\u5173\u7cfb\u8868\u8fbe\u5f0f\u8868\u793a\u4e3a\u534a\u73af\u6570\u7ec4\uff0c\u901a\u8fc7\u7c7b\u578b\u7cfb\u7edf\u9650\u5236\u6570\u7ec4\u5927\u5c0f\uff0c\u5e76\u7f16\u8bd1\u4e3a\u4f4d\u4e32\u8868\u793a\u3002\u5bf9\u4e8e\u5e03\u5c14\u534a\u73af\uff0c\u53ef\u4f7f\u7528SAT\u6c42\u89e3\u5668\u9ad8\u6548\u8fd0\u884c\uff0c\u5728\u89e3\u51b3\u6570\u72ec\u95ee\u9898\u65f6\u6bd4miniKanren\u66f4\u9ad8\u6548\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e\u534a\u73af\u7684\u5173\u7cfb\u7f16\u7a0b\u8bed\u8a00\uff0c\u901a\u8fc7\u7c7b\u578b\u7cfb\u7edf\u548c\u4f4d\u4e32\u7f16\u8bd1\u5b9e\u73b0\u9ad8\u6548\u6267\u884c\uff0c\u7279\u522b\u662f\u5229\u7528SAT\u6c42\u89e3\u5668\u63d0\u5347\u5e03\u5c14\u534a\u73af\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u5b9a\u4e49semiringKanren\u8bed\u8a00\uff0c\u5173\u7cfb\u8868\u8fbe\u5f0f\u8868\u793a\u534a\u73af\u6570\u7ec4\uff1b\u5efa\u7acb\u7c7b\u578b\u7cfb\u7edf\u9650\u5236\u6570\u7ec4\u5927\u5c0f\uff1b\u8bbe\u8ba1\u53c2\u6570\u5316\u8bed\u4e49\uff1b\u5c06\u7c7b\u578b\u7f16\u8bd1\u4e3a\u4f4d\u4e32\u8868\u793a\uff1b\u5bf9\u5e03\u5c14\u534a\u73af\u4f7f\u7528SAT\u6c42\u89e3\u5668\u3002", "result": "\u5b9e\u9a8c\u6bd4\u8f83\u663e\u793a\uff0c\u5728\u89e3\u51b3\u6570\u72ec\u95ee\u9898\u65f6\uff0csemiringKanren\u6bd4miniKanren\u8868\u73b0\u66f4\u9ad8\u6548\u3002", "conclusion": "semiringKanren\u53ef\u4ee5\u6210\u4e3aminiKanren\u7684\u66f4\u9ad8\u6548\u53d8\u4f53\uff0c\u7279\u522b\u662f\u5728\u5229\u7528SAT\u6c42\u89e3\u5668\u5904\u7406\u5e03\u5c14\u534a\u73af\u65f6\u3002"}}
