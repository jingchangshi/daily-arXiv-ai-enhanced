{"id": "2511.10760", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.10760", "abs": "https://arxiv.org/abs/2511.10760", "authors": ["Emad Haque", "Pragnya Sudershan Nalla", "Jeff Zhang", "Sachin S. Sapatnekar", "Chaitali Chakrabarti", "Yu Cao"], "title": "Tiny Chiplets Enabled by Packaging Scaling: Opportunities in ESD Protection and Signal Integrity", "comment": null, "summary": "The scaling of advanced packaging technologies provides abundant interconnection resources for 2.5D/3D heterogeneous integration (HI), thereby enabling the construction of larger-scale VLSI systems with higher energy efficiency in data movement. However, conventional I/O circuitry, including electrostatic discharge (ESD) protection and signaling, introduces significant area overhead. Prior studies have identified this overhead as a major constraint in reducing chiplet size below 100 mm2. In this study, we revisit reliability requirements from the perspective of chiplet interface design. Through parasitic extraction and SPICE simulations, we demonstrate that ESD protection and inter-chiplet signaling can be substantially simplified in future 2.5D/3D packaging technologies. Such simplification, in turn, paves the road for further chiplet miniaturization and improves the composability and reusability of tiny chiplets.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u8bc4\u4f30\u4e862.5D/3D\u5f02\u6784\u96c6\u6210\u4e2d\u82af\u7247\u63a5\u53e3\u8bbe\u8ba1\u7684\u53ef\u9760\u6027\u8981\u6c42\uff0c\u63d0\u51fa\u53ef\u4ee5\u5927\u5e45\u7b80\u5316ESD\u4fdd\u62a4\u548c\u82af\u7247\u95f4\u4fe1\u53f7\u4f20\u8f93\uff0c\u4e3a\u66f4\u5c0f\u5c3a\u5bf8\u7684chiplet\u94fa\u5e73\u9053\u8def\u3002", "motivation": "\u4f20\u7edfI/O\u7535\u8def\uff08\u5305\u62ecESD\u4fdd\u62a4\u548c\u4fe1\u53f7\u4f20\u8f93\uff09\u5728\u5148\u8fdb\u5c01\u88c5\u6280\u672f\u4e2d\u5f15\u5165\u4e86\u663e\u8457\u7684\u9762\u79ef\u5f00\u9500\uff0c\u6210\u4e3a\u5c06chiplet\u5c3a\u5bf8\u51cf\u5c0f\u5230100mm\u00b2\u4ee5\u4e0b\u7684\u4e3b\u8981\u9650\u5236\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u5bc4\u751f\u63d0\u53d6\u548cSPICE\u4eff\u771f\uff0c\u4ece\u82af\u7247\u63a5\u53e3\u8bbe\u8ba1\u7684\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u53ef\u9760\u6027\u8981\u6c42\u3002", "result": "\u7814\u7a76\u8868\u660e\u5728\u672a\u6765\u76842.5D/3D\u5c01\u88c5\u6280\u672f\u4e2d\uff0cESD\u4fdd\u62a4\u548c\u82af\u7247\u95f4\u4fe1\u53f7\u4f20\u8f93\u53ef\u4ee5\u5927\u5e45\u7b80\u5316\u3002", "conclusion": "\u8fd9\u79cd\u7b80\u5316\u4e3a\u8fdb\u4e00\u6b65chiplet\u5c0f\u578b\u5316\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u63d0\u9ad8\u4e86\u5fae\u5c0fchiplet\u7684\u53ef\u7ec4\u5408\u6027\u548c\u53ef\u91cd\u7528\u6027\u3002"}}
{"id": "2511.10909", "categories": ["cs.AR", "cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.10909", "abs": "https://arxiv.org/abs/2511.10909", "authors": ["Peichen Xie", "Yang Wang", "Fan Yang", "Mao Yang"], "title": "MMA-Sim: Bit-Accurate Reference Model of Tensor Cores and Matrix Cores", "comment": null, "summary": "The rapidly growing computation demands of deep neural networks (DNNs) have driven hardware vendors to integrate matrix multiplication accelerators (MMAs), such as NVIDIA Tensor Cores and AMD Matrix Cores, into modern GPUs. However, due to distinct and undocumented arithmetic specifications for floating-point matrix multiplication, some MMAs can lead to numerical imprecision and inconsistency that can compromise the stability and reproducibility of DNN training and inference.\n  This paper presents MMA-Sim, the first bit-accurate reference model that reveals the detailed arithmetic behaviors of the MMAs from ten GPU architectures (eight from NVIDIA and two from AMD). By dissecting the MMAs using a combination of targeted and randomized tests, our methodology derives nine arithmetic algorithms to simulate the floating-point matrix multiplication of the MMAs. Large-scale validation confirms bitwise equivalence between MMA-Sim and the real hardware. Using MMA-Sim, we investigate arithmetic behaviors that affect DNN training stability, and identify undocumented behaviors that could lead to significant errors.", "AI": {"tldr": "MMA-Sim\u662f\u9996\u4e2a\u7cbe\u786e\u5230\u6bd4\u7279\u7ea7\u522b\u7684\u53c2\u8003\u6a21\u578b\uff0c\u63ed\u793a\u4e8610\u79cdGPU\u67b6\u6784\u4e2d\u77e9\u9635\u4e58\u6cd5\u52a0\u901f\u5668\u7684\u8be6\u7ec6\u7b97\u672f\u884c\u4e3a\uff0c\u901a\u8fc7\u76ee\u6807\u6d4b\u8bd5\u548c\u968f\u673a\u6d4b\u8bd5\u63a8\u5bfc\u51fa9\u79cd\u7b97\u6cd5\u6765\u6a21\u62df\u6d6e\u70b9\u77e9\u9635\u4e58\u6cd5\u3002", "motivation": "\u73b0\u4ee3GPU\u4e2d\u7684\u77e9\u9635\u4e58\u6cd5\u52a0\u901f\u5668\u7531\u4e8e\u4e0d\u540c\u7684\u672a\u6587\u6863\u5316\u6d6e\u70b9\u77e9\u9635\u4e58\u6cd5\u7b97\u672f\u89c4\u8303\uff0c\u53ef\u80fd\u5bfc\u81f4\u6570\u503c\u4e0d\u7cbe\u786e\u548c\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cdDNN\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u91cd\u73b0\u6027\u3002", "method": "\u7ed3\u5408\u76ee\u6807\u6d4b\u8bd5\u548c\u968f\u673a\u6d4b\u8bd5\u7684\u65b9\u6cd5\uff0c\u5256\u6790\u77e9\u9635\u4e58\u6cd5\u52a0\u901f\u5668\u7684\u7b97\u672f\u884c\u4e3a\uff0c\u63a8\u5bfc\u51fa9\u79cd\u7b97\u6cd5\u6765\u6a21\u62df\u6d6e\u70b9\u77e9\u9635\u4e58\u6cd5\u3002", "result": "\u5927\u89c4\u6a21\u9a8c\u8bc1\u786e\u8ba4MMA-Sim\u4e0e\u771f\u5b9e\u786c\u4ef6\u5728\u6bd4\u7279\u7ea7\u522b\u5b8c\u5168\u7b49\u4ef7\uff0c\u5e76\u8bc6\u522b\u51fa\u5f71\u54cdDNN\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u7b97\u672f\u884c\u4e3a\u548c\u53ef\u80fd\u5bfc\u81f4\u663e\u8457\u9519\u8bef\u7684\u672a\u6587\u6863\u5316\u884c\u4e3a\u3002", "conclusion": "MMA-Sim\u63d0\u4f9b\u4e86\u5bf9GPU\u77e9\u9635\u4e58\u6cd5\u52a0\u901f\u5668\u7b97\u672f\u884c\u4e3a\u7684\u6df1\u5165\u7406\u89e3\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3DNN\u8bad\u7ec3\u4e2d\u7684\u6570\u503c\u7a33\u5b9a\u6027\u548c\u53ef\u91cd\u73b0\u6027\u95ee\u9898\u3002"}}
{"id": "2511.11248", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.11248", "abs": "https://arxiv.org/abs/2511.11248", "authors": ["Jianyu Wei", "Qingtao Li", "Shijie Cao", "Lingxiao Ma", "Zixu Hao", "Yanyong Zhang", "Xiaoyan Hu", "Ting Cao"], "title": "T-MAN: Enabling End-to-End Low-Bit LLM Inference on NPUs via Unified Table Lookup", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed on customer devices. To support them, current devices are adopting SoCs (System on Chip) with NPUs (Neural Processing Unit) installed. Although high performance is expected, LLM inference on NPUs is slower than its CPU counterpart. The reason is that NPUs have poor performance on computations other than GEMM, like dequantization. Current works either disaggregate prefill on the NPUs and decoding on the CPUs, or put both on the NPUs but with an accuracy loss. To solve this issue, based on the insight that low-bit can enable target computation encoded within an acceptably sized table, we propose table lookup to subsume hardware operations otherwise unsupported. To realize this, we overcome the conflicting hardware behavior of prefill and decoding to design a unified table layout and tiling through (1) fused two-level table-based dequantization and (2) concurrency-hierarchy-guided tiling. Based on that, we implement the prefill phase by three-stage pipeline and map the table-lookup-based decoding to NPU's vector units. Results show 1.4x and 3.1x speedup for prefill and decoding respectively, and 84% energy savings compared to the baseline NPU methods. The code is available at https://github.com/microsoft/T-MAC/tree/main/t-man.", "AI": {"tldr": "\u63d0\u51faT-MAC\u65b9\u6cd5\uff0c\u901a\u8fc7\u67e5\u8868\u6280\u672f\u89e3\u51b3NPU\u4e0aLLM\u63a8\u7406\u6027\u80fd\u5dee\u7684\u95ee\u9898\uff0c\u5728\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u5206\u522b\u5b9e\u73b01.4\u500d\u548c3.1\u500d\u52a0\u901f\uff0c\u8282\u770184%\u80fd\u8017\u3002", "motivation": "\u5f53\u524dNPU\u5728LLM\u63a8\u7406\u4e2d\u6027\u80fd\u4e0d\u5982CPU\uff0c\u4e3b\u8981\u56e0\u4e3aNPU\u5728\u975eGEMM\u8fd0\u7b97\uff08\u5982\u53cd\u91cf\u5316\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5206\u79bb\u9884\u586b\u5145\u548c\u89e3\u7801\u5230\u4e0d\u540c\u786c\u4ef6\uff0c\u8981\u4e48\u5728NPU\u4e0a\u8fd0\u884c\u4f46\u635f\u5931\u7cbe\u5ea6\u3002", "method": "\u5229\u7528\u4f4e\u6bd4\u7279\u4f4d\u5c06\u76ee\u6807\u8ba1\u7b97\u7f16\u7801\u5230\u53ef\u63a5\u53d7\u5927\u5c0f\u7684\u8868\u4e2d\uff0c\u901a\u8fc7\u67e5\u8868\u66ff\u4ee3\u786c\u4ef6\u4e0d\u652f\u6301\u7684\u8fd0\u7b97\u3002\u5177\u4f53\u5305\u62ec\uff1a(1)\u878d\u5408\u4e24\u7ea7\u8868\u57fa\u53cd\u91cf\u5316\uff1b(2)\u5e76\u53d1\u5c42\u6b21\u5f15\u5bfc\u7684\u5e73\u94fa\uff1b(3)\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\u5b9e\u73b0\u9884\u586b\u5145\uff1b(4)\u5c06\u57fa\u4e8e\u67e5\u8868\u7684\u89e3\u7801\u6620\u5c04\u5230NPU\u5411\u91cf\u5355\u5143\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebfNPU\u65b9\u6cd5\uff0c\u9884\u586b\u5145\u9636\u6bb5\u52a0\u901f1.4\u500d\uff0c\u89e3\u7801\u9636\u6bb5\u52a0\u901f3.1\u500d\uff0c\u80fd\u8017\u8282\u770184%\u3002", "conclusion": "T-MAC\u65b9\u6cd5\u901a\u8fc7\u67e5\u8868\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86NPU\u4e0aLLM\u63a8\u7406\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u80fd\u6548\u3002"}}
{"id": "2511.10753", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.10753", "abs": "https://arxiv.org/abs/2511.10753", "authors": ["Jiamin Li", "Lei Qu", "Tao Zhang", "Grigory Chirkov", "Shuotao Xu", "Peng Cheng", "Lidong Zhou"], "title": "FengHuang: Next-Generation Memory Orchestration for AI Inferencing", "comment": null, "summary": "This document presents a vision for a novel AI infrastructure design that has been initially validated through inference simulations on state-of-the-art large language models. Advancements in deep learning and specialized hardware have driven the rapid growth of large language models (LLMs) and generative AI systems. However, traditional GPU-centric architectures face scalability challenges for inference workloads due to limitations in memory capacity, bandwidth, and interconnect scaling. To address these issues, the FengHuang Platform, a disaggregated AI infrastructure platform, is proposed to overcome memory and communication scaling limits for AI inference. FengHuang features a multi-tier shared-memory architecture combining high-speed local memory with centralized disaggregated remote memory, enhanced by active tensor paging and near-memory compute for tensor operations. Simulations demonstrate that FengHuang achieves up to 93% local memory capacity reduction, 50% GPU compute savings, and 16x to 70x faster inter-GPU communication compared to conventional GPU scaling. Across workloads such as GPT-3, Grok-1, and QWEN3-235B, FengHuang enables up to 50% GPU reductions while maintaining end-user performance, offering a scalable, flexible, and cost-effective solution for AI inference infrastructure. FengHuang provides an optimal balance as a rack-level AI infrastructure scale-up solution. Its open, heterogeneous design eliminates vendor lock-in and enhances supply chain flexibility, enabling significant infrastructure and power cost reductions.", "AI": {"tldr": "\u63d0\u51fa\u51e4\u51f0\u5e73\u53f0\uff0c\u4e00\u79cd\u5206\u89e3\u5f0fAI\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u591a\u7ea7\u5171\u4eab\u5185\u5b58\u67b6\u6784\u89e3\u51b3GPU\u5185\u5b58\u548c\u901a\u4fe1\u6269\u5c55\u9650\u5236\uff0c\u5b9e\u73b0\u663e\u8457\u7684\u5185\u5b58\u5bb9\u91cf\u51cf\u5c11\u3001GPU\u8ba1\u7b97\u8282\u7701\u548c\u66f4\u5feb\u7684\u901a\u4fe1\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edfGPU\u4e2d\u5fc3\u67b6\u6784\u5728\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u9762\u4e34\u5185\u5b58\u5bb9\u91cf\u3001\u5e26\u5bbd\u548c\u4e92\u8fde\u6269\u5c55\u7684\u9650\u5236\uff0c\u9700\u8981\u65b0\u7684\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\u6765\u652f\u6301\u5927\u89c4\u6a21AI\u63a8\u7406\u3002", "method": "\u51e4\u51f0\u5e73\u53f0\u91c7\u7528\u591a\u7ea7\u5171\u4eab\u5185\u5b58\u67b6\u6784\uff0c\u7ed3\u5408\u9ad8\u901f\u672c\u5730\u5185\u5b58\u548c\u96c6\u4e2d\u5f0f\u5206\u89e3\u8fdc\u7a0b\u5185\u5b58\uff0c\u901a\u8fc7\u4e3b\u52a8\u5f20\u91cf\u5206\u9875\u548c\u8fd1\u5185\u5b58\u8ba1\u7b97\u4f18\u5316\u5f20\u91cf\u64cd\u4f5c\u3002", "result": "\u6a21\u62df\u663e\u793a\u51e4\u51f0\u5e73\u53f0\u5b9e\u73b0\u9ad8\u8fbe93%\u672c\u5730\u5185\u5b58\u5bb9\u91cf\u51cf\u5c11\u300150%GPU\u8ba1\u7b97\u8282\u7701\uff0c\u4ee5\u53ca16\u500d\u523070\u500d\u66f4\u5feb\u7684GPU\u95f4\u901a\u4fe1\u901f\u5ea6\uff0c\u5728GPT-3\u3001Grok-1\u7b49\u6a21\u578b\u4e0a\u53ef\u51cf\u5c1150%GPU\u6570\u91cf\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u51e4\u51f0\u5e73\u53f0\u4f5c\u4e3a\u673a\u67b6\u7ea7AI\u57fa\u7840\u8bbe\u65bd\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u7075\u6d3b\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684AI\u63a8\u7406\u57fa\u7840\u8bbe\u65bd\uff0c\u6d88\u9664\u4f9b\u5e94\u5546\u9501\u5b9a\u5e76\u663e\u8457\u964d\u4f4e\u57fa\u7840\u8bbe\u65bd\u548c\u7535\u529b\u6210\u672c\u3002"}}
{"id": "2511.11055", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.11055", "abs": "https://arxiv.org/abs/2511.11055", "authors": ["Michael Schwarz", "Julian Erhard"], "title": "Data Race Detection by Digest-Driven Abstract Interpretation (Extended Version)", "comment": "Extended of paper accepted to appear at VMCAI'26; 29 Pages, including 2 Appendices", "summary": "Sound static analysis can prove the absence of data races by establishing that no two conflicting memory accesses can occur at the same time. We repurpose the concept of digests -- summaries of computational histories originally introduced to bring tunable concurrency-sensitivity to thread-modular value analysis by abstract interpretation, extending this idea to race detection: We use digests to capture the conditions under which conflicting accesses may not happen in parallel. To formalize this, we give a definition of data races in the thread-modular local trace semantics and show how exclusion criteria for potential conflicts can be expressed as digests. We report on our implementation of digest-driven data race detection in the static analyzer Goblint, and evaluate it on the SV-COMP benchmark suite. Combining the lockset digest with digests reasoning on thread ids and thread joins increases the number of correctly solved tasks by more than a factor of five compared to lockset reasoning alone.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6458\u8981\u7684\u9759\u6001\u6570\u636e\u7ade\u4e89\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6458\u8981\u6355\u83b7\u51b2\u7a81\u8bbf\u95ee\u4e0d\u80fd\u5e76\u884c\u53d1\u751f\u7684\u6761\u4ef6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u7ade\u4e89\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u9759\u6001\u5206\u6790\u867d\u7136\u80fd\u8bc1\u660e\u6570\u636e\u7ade\u4e89\u7684\u7f3a\u5931\uff0c\u4f46\u9700\u8981\u66f4\u7cbe\u786e\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u54ea\u4e9b\u51b2\u7a81\u5185\u5b58\u8bbf\u95ee\u4e0d\u4f1a\u540c\u65f6\u53d1\u751f\u3002\u672c\u6587\u65e8\u5728\u5c06\u6458\u8981\u6982\u5ff5\u91cd\u65b0\u7528\u4e8e\u6570\u636e\u7ade\u4e89\u68c0\u6d4b\uff0c\u4ee5\u6355\u83b7\u51b2\u7a81\u8bbf\u95ee\u4e0d\u80fd\u5e76\u884c\u53d1\u751f\u7684\u6761\u4ef6\u3002", "method": "\u5728\u5c40\u90e8\u8ff9\u8bed\u4e49\u4e2d\u5b9a\u4e49\u6570\u636e\u7ade\u4e89\uff0c\u5c06\u51b2\u7a81\u6392\u9664\u6761\u4ef6\u8868\u793a\u4e3a\u6458\u8981\u3002\u5728\u9759\u6001\u5206\u6790\u5668Goblint\u4e2d\u5b9e\u73b0\u4e86\u6458\u8981\u9a71\u52a8\u7684\u6570\u636e\u7ade\u4e89\u68c0\u6d4b\uff0c\u7ed3\u5408\u4e86\u9501\u96c6\u6458\u8981\u3001\u7ebf\u7a0bID\u548c\u7ebf\u7a0b\u8fde\u63a5\u63a8\u7406\u3002", "result": "\u5728SV-COMP\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u5408\u9501\u96c6\u6458\u8981\u4e0e\u7ebf\u7a0bID\u548c\u7ebf\u7a0b\u8fde\u63a5\u63a8\u7406\uff0c\u76f8\u6bd4\u5355\u72ec\u4f7f\u7528\u9501\u96c6\u63a8\u7406\uff0c\u6b63\u786e\u89e3\u51b3\u7684\u4efb\u52a1\u6570\u91cf\u589e\u52a0\u4e86\u4e94\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u6458\u8981\u9a71\u52a8\u7684\u6570\u636e\u7ade\u4e89\u68c0\u6d4b\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9759\u6001\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5c06\u6458\u8981\u6982\u5ff5\u6269\u5c55\u5230\u7ade\u4e89\u68c0\u6d4b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.11070", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.11070", "abs": "https://arxiv.org/abs/2511.11070", "authors": ["Sangho Lim", "Hyoungjin Lim", "Wonyeol Lee", "Xavier Rival", "Hongseok Yang"], "title": "Optimising Density Computations in Probabilistic Programs via Automatic Loop Vectorisation", "comment": "70 pages, 19 figures, the first two authors contributed equally to this work, accepted at POPL'26", "summary": "Probabilistic programming languages (PPLs) are a popular tool for high-level modelling across many fields. They provide a range of algorithms for probabilistic inference, which analyse models by learning their parameters from a dataset or estimating their posterior distributions. However, probabilistic inference is known to be very costly. One of the bottlenecks of probabilistic inference stems from the iteration over entries of a large dataset or a long series of random samples. Vectorisation can mitigate this cost, but manual vectorisation is error-prone, and existing automatic techniques are often ad-hoc and limited, unable to handle general repetition structures, such as nested loops and loops with data-dependent control flow, without significant user intervention. To address this bottleneck, we propose a sound and effective method for automatically vectorising loops in probabilistic programs. Our method achieves high throughput using speculative parallel execution of loop iterations, while preserving the semantics of the original loop through a fixed-point check. We formalise our method as a translation from an imperative PPL into a lower-level target language with primitives geared towards vectorisation. We implemented our method for the Pyro PPL and evaluated it on a range of probabilistic models. Our experiments show significant performance gains against an existing vectorisation baseline, achieving $1.1$--$6\\times$ speedups and reducing GPU memory usage in many cases. Unlike the baseline, which is limited to a subset of models, our method effectively handled all the tested models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5411\u91cf\u5316\u6982\u7387\u7a0b\u5e8f\u4e2d\u5faa\u73af\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u6d4b\u6027\u5e76\u884c\u6267\u884c\u63d0\u9ad8\u6027\u80fd\uff0c\u5728Pyro PPL\u4e2d\u5b9e\u73b0\u5e76\u53d6\u5f97\u663e\u8457\u52a0\u901f\u6548\u679c", "motivation": "\u6982\u7387\u63a8\u7406\u6210\u672c\u9ad8\u6602\uff0c\u5176\u4e2d\u4e00\u4e2a\u74f6\u9888\u5728\u4e8e\u9700\u8981\u8fed\u4ee3\u5927\u578b\u6570\u636e\u96c6\u6216\u957f\u5e8f\u5217\u968f\u673a\u6837\u672c\u3002\u73b0\u6709\u5411\u91cf\u5316\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5904\u7406\u901a\u7528\u5faa\u73af\u7ed3\u6784", "method": "\u4f7f\u7528\u63a8\u6d4b\u6027\u5e76\u884c\u6267\u884c\u5faa\u73af\u8fed\u4ee3\uff0c\u901a\u8fc7\u4e0d\u52a8\u70b9\u68c0\u67e5\u4fdd\u6301\u539f\u59cb\u5faa\u73af\u8bed\u4e49\uff0c\u5c06\u547d\u4ee4\u5f0fPPL\u8f6c\u6362\u4e3a\u652f\u6301\u5411\u91cf\u5316\u7684\u4f4e\u7ea7\u76ee\u6807\u8bed\u8a00", "result": "\u5728\u591a\u79cd\u6982\u7387\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u83b7\u5f971.1-6\u500d\u52a0\u901f\uff0c\u51cf\u5c11GPU\u5185\u5b58\u4f7f\u7528\uff0c\u4e14\u80fd\u5904\u7406\u6240\u6709\u6d4b\u8bd5\u6a21\u578b", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u81ea\u52a8\u5411\u91cf\u5316\u6982\u7387\u7a0b\u5e8f\u4e2d\u7684\u5faa\u73af\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2511.10860", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10860", "abs": "https://arxiv.org/abs/2511.10860", "authors": ["Rabimba Karanjai", "Lei Xu", "Weidong Shi"], "title": "HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation", "comment": "Accepted in AIWare 2025", "summary": "Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow where specialized LLM agents (Recipe Agent and Test Agent) iteratively generate and refine test cases through a critique loop. This architecture enables the generation of context-aware unit tests that specifically target parallel execution constructs, complex communication patterns, and hierarchical parallelism. We demonstrate HPCAgentTester's ability to produce compilable and functionally correct tests for OpenMP and MPI primitives, effectively identifying subtle bugs that are often missed by conventional techniques. Our evaluation shows that HPCAgentTester significantly improves test compilation rates and correctness compared to standalone LLMs, offering a more robust and scalable solution for ensuring the reliability of parallel software systems.", "AI": {"tldr": "HPCAgentTester\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53LLM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u751f\u6210HPC\u8f6f\u4ef6\u7684\u5355\u5143\u6d4b\u8bd5\uff0c\u7279\u522b\u9488\u5bf9OpenMP\u548cMPI\u5e76\u884c\u7a0b\u5e8f\uff0c\u901a\u8fc7\u534f\u4f5c\u5de5\u4f5c\u6d41\u63d0\u9ad8\u6d4b\u8bd5\u8d28\u91cf\u548c\u8986\u76d6\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406HPC\u4e2d\u7684\u975e\u786e\u5b9a\u6027\u884c\u4e3a\u548c\u540c\u6b65\u95ee\u9898\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u6765\u786e\u4fdd\u5e76\u884c\u8f6f\u4ef6\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53LLM\u67b6\u6784\uff0c\u5305\u542b\u914d\u65b9\u667a\u80fd\u4f53\u548c\u6d4b\u8bd5\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u8fed\u4ee3\u5f0f\u7684\u6279\u8bc4\u5faa\u73af\u534f\u4f5c\u751f\u6210\u548c\u4f18\u5316\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4e13\u95e8\u9488\u5bf9\u5e76\u884c\u6267\u884c\u6784\u9020\u548c\u590d\u6742\u901a\u4fe1\u6a21\u5f0f\u3002", "result": "\u80fd\u591f\u4e3aOpenMP\u548cMPI\u539f\u8bed\u751f\u6210\u53ef\u7f16\u8bd1\u4e14\u529f\u80fd\u6b63\u786e\u7684\u6d4b\u8bd5\uff0c\u6709\u6548\u8bc6\u522b\u4f20\u7edf\u6280\u672f\u7ecf\u5e38\u9057\u6f0f\u7684\u7ec6\u5faebug\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u7f16\u8bd1\u7387\u548c\u6b63\u786e\u6027\u3002", "conclusion": "HPCAgentTester\u76f8\u6bd4\u72ec\u7acbLLM\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5e76\u884c\u8f6f\u4ef6\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u4fdd\u969c\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2511.11264", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.11264", "abs": "https://arxiv.org/abs/2511.11264", "authors": ["Tobias Kapp\u00e9", "Alexandra Silva", "Jana Wagemaker"], "title": "Kleene Algebra", "comment": null, "summary": "This booklet serves as an introduction to Kleene Algebra (KA), a set of laws that can be used to study general equivalences between programs. It discusses how general programs can be modeled using regular expressions, how those expressions correspond to automata, and how this correspondence can be exploited to obtain the central result of KA, namely that an equivalence of regular expressions is true if and only if it can be proved using the laws of KA. Each chapter closes with a set of exercises to further build intuition and understanding, and there is an optional chapter that develops automata theory through the lens of coalgebra.", "AI": {"tldr": "This booklet introduces Kleene Algebra (KA) as a framework for studying program equivalences using regular expressions and automata, with exercises to build understanding and an optional coalgebra-based approach to automata theory.", "motivation": "To provide a foundational understanding of Kleene Algebra and its application in verifying program equivalences through the correspondence between regular expressions and automata.", "method": "Modeling programs with regular expressions, establishing their correspondence to automata, and leveraging this to prove equivalences using KA laws.", "result": "The central result is that an equivalence of regular expressions holds if and only if it can be proven using the laws of Kleene Algebra.", "conclusion": "The booklet effectively introduces KA, reinforces concepts with exercises, and offers an alternative coalgebraic perspective on automata theory for deeper insight."}}
{"id": "2511.11332", "categories": ["cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11332", "abs": "https://arxiv.org/abs/2511.11332", "authors": ["Chaoyun Zhang", "Liqun Li", "He Huang", "Chiming Ni", "Bo Qiao", "Si Qin", "Yu Kang", "Minghua Ma", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "UFO$^3$: Weaving the Digital Agent Galaxy", "comment": "We developed UFO$^3$ as a fully engineered system with over 73K lines of code, encompassing agent implementations and integrations for Windows, Linux, and Android mobile devices. The entire project is open-sourced at https://github.com/microsoft/UFO/, accompanied by detailed documentation and tutorials at https://microsoft.github.io/UFO/", "summary": "Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO$^3$, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO$^3$ models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.\n  We evaluate UFO$^3$ on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO$^3$ achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO$^3$ achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.", "AI": {"tldr": "UFO$^3$\u662f\u4e00\u4e2a\u8de8\u8bbe\u5907\u4efb\u52a1\u7f16\u6392\u7cfb\u7edf\uff0c\u5c06\u5f02\u6784\u8bbe\u5907\u7edf\u4e00\u4e3a\u5355\u4e00\u7f16\u6392\u7ed3\u6784\uff0c\u901a\u8fc7TaskConstellation\u6a21\u578b\u5b9e\u73b0\u5f02\u6b65\u6267\u884c\u3001\u81ea\u9002\u5e94\u6062\u590d\u548c\u52a8\u6001\u4f18\u5316\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u6846\u67b6\u5927\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u64cd\u4f5c\u7cfb\u7edf\u6216\u8bbe\u5907\uff0c\u5bfc\u81f4\u8de8\u8bbe\u5907\u5de5\u4f5c\u6d41\u7a0b\u8106\u5f31\u4e14\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u64cd\u4f5c\uff0c\u9700\u8981\u7edf\u4e00\u7684\u8de8\u8bbe\u5907\u7f16\u6392\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528TaskConstellation\u6a21\u578b\uff0c\u5c06\u7528\u6237\u8bf7\u6c42\u5efa\u6a21\u4e3a\u53ef\u53d8\u7684\u5206\u5e03\u5f0fDAG\uff0c\u5305\u542b\u539f\u5b50\u5b50\u4efb\u52a1\u548c\u663e\u5f0f\u63a7\u5236\u6570\u636e\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7Constellation Orchestrator\u548cAgent Interaction Protocol\u5b9e\u73b0\u5b89\u5168\u5f02\u6b65\u6267\u884c\u3002", "result": "\u5728\u5305\u542b55\u4e2a\u8de8\u8bbe\u5907\u4efb\u52a1\u7684NebulaBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUFO$^3$\u5b9e\u73b0\u4e8683.3%\u7684\u5b50\u4efb\u52a1\u5b8c\u6210\u7387\u300170.9%\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u884c\u5ea6\u5e73\u5747\u5bbd\u5ea61.72\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u6bd4\u987a\u5e8f\u57fa\u7ebf\u51cf\u5c1131%\u3002", "conclusion": "UFO$^3$\u5b9e\u73b0\u4e86\u8de8\u5f02\u6784\u8bbe\u5907\u7684\u51c6\u786e\u3001\u9ad8\u6548\u548c\u5f39\u6027\u4efb\u52a1\u7f16\u6392\uff0c\u5c06\u5b64\u7acb\u4ee3\u7406\u7edf\u4e00\u4e3a\u8fde\u8d2f\u7684\u81ea\u9002\u5e94\u8ba1\u7b97\u7ed3\u6784\uff0c\u6269\u5c55\u4e86\u666e\u9002\u8ba1\u7b97\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2511.11292", "categories": ["cs.PL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11292", "abs": "https://arxiv.org/abs/2511.11292", "authors": ["Santiago Arranz-Olmos", "Gilles Barthe", "Lionel Blatter", "Benjamin Gr\u00e9goire", "Vincent Laporte", "Paolo Torrini"], "title": "The Jasmin Compiler Preserves Cryptographic Security", "comment": null, "summary": "Jasmin is a programming and verification framework for developing efficient, formally verified, cryptographic implementations. A main component of the framework is the Jasmin compiler, which empowers programmers to write efficient implementations of state-of-the-art cryptographic primitives, including post-quantum cryptographic standards. The Jasmin compiler is proven functionally correct in the Rocq prover. However, this functional correctness statement does not apply to nonterminating or probabilistic computations, which are essential features in cryptography.\n  In this paper, we significantly enhance the guarantees of the compiler by showing, in the Rocq prover, that its front-end (25 out of 30 passes) preserves cryptographic security. To this end, we first define a Relational Hoare Logic tailored for compiler correctness proofs. We prove the soundness of our logic w.r.t. a new denotational semantics of Jasmin programs based on interaction trees. Secondly, we use our program logic to prove the functional correctness of the (unmodified) Jasmin compiler w.r.t. said semantics. Lastly, we formalize cryptographic security -- focusing on IND-CCA -- with interaction trees and prove that the Jasmin compiler preserves cryptographic security.", "AI": {"tldr": "\u672c\u6587\u663e\u8457\u589e\u5f3a\u4e86Jasmin\u7f16\u8bd1\u5668\uff08\u7528\u4e8e\u5f00\u53d1\u9ad8\u6548\u3001\u5f62\u5f0f\u9a8c\u8bc1\u7684\u5bc6\u7801\u5b66\u5b9e\u73b0\uff09\u7684\u5b89\u5168\u4fdd\u8bc1\uff0c\u8bc1\u660e\u5176\u524d\u7aef25\u4e2a\u7f16\u8bd1\u8fc7\u7a0b\u80fd\u591f\u4fdd\u6301\u5bc6\u7801\u5b66\u5b89\u5168\u6027\uff0c\u5305\u62ecIND-CCA\u5b89\u5168\u6027\u3002", "motivation": "Jasmin\u7f16\u8bd1\u5668\u867d\u7136\u5df2\u5728Rocq\u8bc1\u660e\u5668\u4e2d\u8bc1\u660e\u529f\u80fd\u6b63\u786e\uff0c\u4f46\u8fd9\u79cd\u6b63\u786e\u6027\u58f0\u660e\u4e0d\u9002\u7528\u4e8e\u5bc6\u7801\u5b66\u4e2d\u5fc5\u9700\u7684\u975e\u7ec8\u6b62\u6216\u6982\u7387\u8ba1\u7b97\uff0c\u56e0\u6b64\u9700\u8981\u589e\u5f3a\u5176\u5bc6\u7801\u5b66\u5b89\u5168\u6027\u4fdd\u8bc1\u3002", "method": "\u9996\u5148\u5b9a\u4e49\u9002\u7528\u4e8e\u7f16\u8bd1\u5668\u6b63\u786e\u6027\u8bc1\u660e\u7684\u5173\u7cfbHoare\u903b\u8f91\uff0c\u5e76\u57fa\u4e8e\u4ea4\u4e92\u6811\u7684\u65b0\u6307\u79f0\u8bed\u4e49\u8bc1\u660e\u5176\u53ef\u9760\u6027\uff1b\u7136\u540e\u4f7f\u7528\u8be5\u7a0b\u5e8f\u903b\u8f91\u8bc1\u660eJasmin\u7f16\u8bd1\u5668\u76f8\u5bf9\u4e8e\u8be5\u8bed\u4e49\u7684\u529f\u80fd\u6b63\u786e\u6027\uff1b\u6700\u540e\u7528\u4ea4\u4e92\u6811\u5f62\u5f0f\u5316\u5bc6\u7801\u5b66\u5b89\u5168\u6027\u5e76\u8bc1\u660e\u7f16\u8bd1\u5668\u4fdd\u6301\u5bc6\u7801\u5b66\u5b89\u5168\u3002", "result": "\u6210\u529f\u8bc1\u660eJasmin\u7f16\u8bd1\u5668\u524d\u7aef25\u4e2a\u7f16\u8bd1\u8fc7\u7a0b\u80fd\u591f\u4fdd\u6301\u5bc6\u7801\u5b66\u5b89\u5168\u6027\uff0c\u5305\u62ecIND-CCA\u5b89\u5168\u6027\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u7f16\u8bd1\u5668\u7684\u5b89\u5168\u4fdd\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u65b0\u7684\u7a0b\u5e8f\u903b\u8f91\u548c\u8bed\u4e49\u6846\u67b6\uff0c\u6210\u529f\u6269\u5c55\u4e86Jasmin\u7f16\u8bd1\u5668\u7684\u9a8c\u8bc1\u8303\u56f4\uff0c\u4f7f\u5176\u4e0d\u4ec5\u4fdd\u8bc1\u529f\u80fd\u6b63\u786e\u6027\uff0c\u8fd8\u80fd\u4fdd\u6301\u5bc6\u7801\u5b66\u5b89\u5168\u6027\uff0c\u4e3a\u5f00\u53d1\u5f62\u5f0f\u9a8c\u8bc1\u7684\u5bc6\u7801\u5b66\u5b9e\u73b0\u63d0\u4f9b\u4e86\u66f4\u5f3a\u4fdd\u969c\u3002"}}
{"id": "2511.11542", "categories": ["cs.DC", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2511.11542", "abs": "https://arxiv.org/abs/2511.11542", "authors": ["Tomas Oppelstrup", "Nicholas Giamblanco", "Delyan Z. Kalchev", "Ilya Sharapov", "Mark Taylor", "Dirk Van Essendelft", "Sivasankaran Rajamanickam", "Michael James"], "title": "Beyond Exascale: Dataflow Domain Translation on a Cerebras Cluster", "comment": "19 pages, 11 figures. Accepted for HPC/Asia 2026", "summary": "Simulation of physical systems is essential in many scientific and engineering domains. Commonly used domain decomposition methods are unable to deliver high simulation rate or high utilization in network computing environments. In particular, Exascale systems deliver only a small fraction their peak performance for these workloads. This paper introduces the novel \\algorithmpropernoun{} algorithm, designed to overcome these limitations. We apply this method and show simulations running in excess of 1.6 million time steps per second and simulations achieving 84 PFLOP/s. Our implementation can achieve 90\\% of peak performance in both single-node and clustered environments. We illustrate the method by applying the shallow-water equations to model a tsunami following an asteroid impact at 460m-resolution on a planetary scale running on a cluster of 64 Cerebras CS-3 systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u7269\u7406\u7cfb\u7edf\u6a21\u62df\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5728\u5206\u5e03\u5f0f\u8ba1\u7b97\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6781\u9ad8\u7684\u6a21\u62df\u901f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u57df\u5206\u89e3\u65b9\u6cd5\u5728\u5206\u5e03\u5f0f\u8ba1\u7b97\u73af\u5883\u4e2d\u65e0\u6cd5\u5b9e\u73b0\u9ad8\u6a21\u62df\u901f\u7387\u6216\u9ad8\u5229\u7528\u7387\uff0c\u7279\u522b\u662f\u5728Exascale\u7cfb\u7edf\u4e2d\u53ea\u80fd\u53d1\u6325\u5cf0\u503c\u6027\u80fd\u7684\u4e00\u5c0f\u90e8\u5206\u3002", "method": "\u5f15\u5165\u4e86\u65b0\u9896\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u572864\u4e2aCerebras CS-3\u7cfb\u7edf\u96c6\u7fa4\u4e0a\u5e94\u7528\u6d45\u6c34\u65b9\u7a0b\u6765\u6a21\u62df\u884c\u661f\u5c3a\u5ea6\u7684\u6d77\u5578\u3002", "result": "\u5b9e\u73b0\u4e86\u8d85\u8fc7160\u4e07\u65f6\u95f4\u6b65/\u79d2\u7684\u6a21\u62df\u901f\u5ea6\uff0c\u8fbe\u523084 PFLOP/s\u7684\u8ba1\u7b97\u6027\u80fd\uff0c\u5728\u5355\u8282\u70b9\u548c\u96c6\u7fa4\u73af\u5883\u4e2d\u90fd\u80fd\u8fbe\u5230\u5cf0\u503c\u6027\u80fd\u768490%\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u7269\u7406\u7cfb\u7edf\u6a21\u62df\u4e2d\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
