<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [PLSEMANTICSBENCH: Large Language Models As Programming Language Interpreters](https://arxiv.org/abs/2510.03415)
*Aditya Thimmaiah,Jiyang Zhang,Jayanth Srinivasa,Junyi Jessy Li,Milos Gligoric*

Main category: cs.PL

TL;DR: 研究大型语言模型能否基于形式语义作为编程语言解释器，使用IMP语言和不同语义形式化方法进行实验，发现模型在标准语义下表现良好但在非标准语义下性能下降，表明其语义理解不够鲁棒。


<details>
  <summary>Details</summary>
Motivation: 探索LLM能否仅基于编程语言的形式语义来执行程序，这将有助于快速原型化新编程语言和语言特性。

Method: 使用IMP语言（C的子集），通过小步操作语义和基于重写的操作语义进行形式化。创建三个评估集（人工编写、LLM翻译、模糊生成），控制代码复杂度。评估三个任务：最终状态预测、语义规则预测、执行轨迹预测。定义非标准语义来区分预训练记忆和语义能力。

Result: 强代码/推理LLM在标准语义下表现良好，但在非标准语义下性能显著下降。模型在不同复杂度程序上表现模式化：在简单程序上形式语义有帮助，但在复杂程序上反而有害。推理模型在粗粒度任务上表现优异，能处理嵌套循环深度超过5的复杂程序。

Conclusion: LLM有潜力作为编程语言解释器，但其语义理解缺乏鲁棒性。发布了基准测试和支持代码。

Abstract: As large language models (LLMs) excel at code reasoning, a natural question
arises: can an LLM execute programs (i.e., act as an interpreter) purely based
on a programming language's formal semantics? If so, it will enable rapid
prototyping of new programming languages and language features. We study this
question using the imperative language IMP (a subset of C), formalized via
small-step operational semantics (SOS) and rewriting-based operational
semantics (K-semantics). We introduce three evaluation sets-Human-Written,
LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by
code-complexity metrics spanning the size, control-flow, and data-flow axes.
Given a program and its semantics formalized with SOS/K-semantics, models are
evaluated on three tasks ranging from coarse to fine: (1) final-state
prediction, (2) semantic rule prediction, and (3) execution trace prediction.
To distinguish pretraining memorization from semantic competence, we define two
nonstandard semantics obtained through systematic mutations of the standard
rules. Across strong code/reasoning LLMs, performance drops under nonstandard
semantics despite high performance under the standard one. We further find that
(i) there are patterns to different model failures, (ii) most reasoning models
perform exceptionally well on coarse grained tasks involving reasoning about
highly complex programs often containing nested loop depths beyond five, and
surprisingly, (iii) providing formal semantics helps on simple programs but
often hurts on more complex ones. Overall, the results show a promise that LLMs
could serve as programming language interpreters, but points to the lack of
their robust semantics understanding. We release the benchmark and the
supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.

</details>


### [2] [Encoding Numeric Computations and Infusing Heuristic Knowledge Using Integrity Constraints in stableKanren](https://arxiv.org/abs/2510.04049)
*Xiangyu Guo,Ajay Bansal*

Main category: cs.PL

TL;DR: 本文展示了在stableKanren中使用完整性约束进行数值计算以解决问题的示例，并介绍了多种注入启发式知识以减少求解时间的方法。


<details>
  <summary>Details</summary>
Motivation: 扩展关系型编程语言以支持数值计算，并平衡符号与数值计算，避免将所有数字转换为符号，同时简化语法。

Method: 使用stableKanren（miniKanren的扩展）构建约束存储来实现完整性约束，支持组合搜索问题的声明式生成和测试范式。

Result: 展示了在SEND+MORE=MONEY谜题中，不同编程或查询方法对求解器性能的影响，随着更多启发式知识的注入，性能逐渐提升。

Conclusion: stableKanren通过约束存储构造提供了直接的数值表示，平衡了符号和数值计算，并支持通过外部函数实现混合解决方案。

Abstract: This paper presents examples of using integrity constraints in stableKanren
to encode numeric computations for problem solving. Then, we use one of the
examples to introduce multiple ways to infuse heuristic knowledge and reduce
solving time. stableKanren is an extension of miniKanren that supports normal
logic programs under stable model semantics. stableKanren further supports
numeric computation by constructing a constraint store for integrity
constraints. There are three ways to extend a relational programming language
with numeric computations: relational number representation, grounding numbers
to symbols, and constraint store construction. We demonstrate that the numeric
computations in stableKanren have a straightforward numerical representation
compared to relational number representations. More importantly, stableKanren
balances symbolic and numeric computation in relational programming by avoiding
the grounding of all numbers to symbols. Lastly, it also has simpler syntax
compared to other constraint store construction approaches. stableKanren
supports combinatorial search problem solving under a declarative generate and
test paradigm. Such a paradigm generates all possible combinations of solutions
to the problem, then applies a set of constraints to prune out the unwanted
solutions. We demonstrate that different approaches to writing programs or
queries affect the solver's performance in the SEND+MORE=MONEY puzzle. The
performance gradually improves as more heuristic knowledge is infused through
the programs or queries. Additionally, we show how to use an external function
to achieve a hybrid solution.

</details>


### [3] [Retrofitting Control Flow Graphs in LLVM IR for Auto Vectorization](https://arxiv.org/abs/2510.04890)
*Shihan Fang,Wenxin Zheng*

Main category: cs.PL

TL;DR: 提出了一种新的向量化流水线，通过SIR和VIR两种专用IR扩展来改进控制流分析和指令依赖分析，显著提升了自动向量化的范围和效率。


<details>
  <summary>Details</summary>
Motivation: 现代处理器依赖SIMD指令集提升性能，但现有编译器如LLVM和GCC由于向量化过程分散且扩展性有限，无法充分利用向量化机会。控制流分析和向量化机会识别仍是挑战。

Method: 引入包含SIR和VIR两种IR扩展的新型向量化流水线。SIR编码高层次结构信息，VIR通过数据依赖分析显式表示指令依赖关系。基于VIR提供的详细依赖信息，构建灵活可扩展的向量化框架。

Result: 实验评估显示，与LLVM和GCC相比，提出的向量化流水线分别实现了高达53%和58%的性能加速。

Conclusion: 通过专门的IR扩展和依赖分析，提出的向量化框架显著改善了向量化过程间的互操作性，扩展了同构指令的搜索空间，有效提升了自动向量化的范围和效率。

Abstract: Modern processors increasingly rely on SIMD instruction sets, such as AVX and
RVV, to significantly enhance parallelism and computational performance.
However, production-ready compilers like LLVM and GCC often fail to fully
exploit available vectorization opportunities due to disjoint vectorization
passes and limited extensibility. Although recent attempts in heuristics and
intermediate representation (IR) designs have attempted to address these
problems, efficiently simplifying control flow analysis and accurately
identifying vectorization opportunities remain challenging tasks.
  To address these issues, we introduce a novel vectorization pipeline
featuring two specialized IR extensions: SIR, which encodes high-level
structural information, and VIR, which explicitly represents instruction
dependencies through data dependency analysis. Leveraging the detailed
dependency information provided by VIR, we develop a flexible and extensible
vectorization framework. This approach substantially improves interoperability
across vectorization passes and expands the search space for identifying
isomorphic instructions, ultimately enhancing both the scope and efficiency of
automatic vectorization. Experimental evaluations demonstrate that our proposed
vectorization pipeline achieves significant performance improvements,
delivering speedups of up to 53% and 58% compared to LLVM and GCC,
respectively.

</details>


### [4] [concurrentKanren: miniKanren for parallel execution](https://arxiv.org/abs/2510.04994)
*Sjoerd Dost*

Main category: cs.PL

TL;DR: 提出了一个用Go语言实现的并行miniKanren系统，展示了其可行性及性能提升潜力


<details>
  <summary>Details</summary>
Motivation: 虽然并发逻辑编程早于miniKanren，但miniKanren的并发实现仍未被充分探索

Method: 利用隐式并行性，允许遗留程序从并行执行中受益，并讨论了实现策略

Result: 评估了并行性的影响，证明了并行实现的可行性

Conclusion: 为未来语言无关的模型奠定了基础

Abstract: Concurrent logic programming predates miniKanren, but concurrent
implementations of miniKanren have remained largely unexplored. In this work we
present a parallel implementation of miniKanren in Go, demonstrating its
feasibility and potential for performance improvements. Our approach leverages
implicit parallelism allowing legacy programs to benefit from parallel
execution. We discuss implementation strategies and evaluate the impact of
parallelism, laying groundwork for future language-agnostic models.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Cosmological Hydrodynamics at Exascale: A Trillion-Particle Leap in Capability](https://arxiv.org/abs/2510.03557)
*Nicholas Frontiere,J. D. Emberson,Michael Buehlmann,Esteban M. Rangel,Salman Habib,Katrin Heitmann,Patricia Larsen,Vitali Morozov,Adrian Pope,Claude-André Faucher-Giguère,Antigoni Georgiadou,Damien Lebrun-Grandié,Andrey Prokopenko*

Main category: cs.DC

TL;DR: CRK-HACC是一个宇宙学流体动力学代码，执行了Frontier-E模拟：一个包含4万亿粒子的全天空模拟，比以往努力大一个数量级，峰值性能达513.1 PFLOPs，在一周多时间内处理了466亿粒子/秒并写入超过100PB数据。


<details>
  <summary>Details</summary>
Motivation: 解决宇宙学中最基本的问题需要与下一代天空巡天相匹配的模拟，要求处理详细气体动力学和天体物理效应，与引力自洽地建模结构形成。

Method: 使用尺度分离技术、GPU驻留树求解器、原位分析管道和多层I/O，构建了CRK-HACC宇宙学流体动力学代码。

Result: 执行了Frontier-E模拟：4万亿粒子全天空模拟，峰值性能513.1 PFLOPs，处理速度46.6亿粒子/秒，写入数据超过100PB。

Conclusion: 百亿亿次计算使得能够在巡天尺度体积中运行模拟，同时纳入塑造复杂宇宙结构的关键子网格过程，这是实现所需真实感的重要步骤。

Abstract: Resolving the most fundamental questions in cosmology requires simulations
that match the scale, fidelity, and physical complexity demanded by
next-generation sky surveys. To achieve the realism needed for this critical
scientific partnership, detailed gas dynamics, along with a host of
astrophysical effects, must be treated self-consistently with gravity for
end-to-end modeling of structure formation. As an important step on this
roadmap, exascale computing enables simulations that span survey-scale volumes
while incorporating key subgrid processes that shape complex cosmic structures.
We present results from CRK-HACC, a cosmological hydrodynamics code built for
the extreme scalability requirements set by modern cosmological surveys. Using
separation-of-scale techniques, GPU-resident tree solvers, in situ analysis
pipelines, and multi-tiered I/O, CRK-HACC executed Frontier-E: a four trillion
particle full-sky simulation, over an order of magnitude larger than previous
efforts. The run achieved 513.1 PFLOPs peak performance, processing 46.6
billion particles per second and writing more than 100 PB of data in just over
one week of runtime.

</details>


### [6] [Datacenter Energy Optimized Power Profiles](https://arxiv.org/abs/2510.03872)
*Sreedhar Narayanaswamy,Pratikkumar Dilipkumar Patel,Ian Karlin,Apoorv Gupta,Sudhir Saripalli,Janey Guo*

Main category: cs.DC

TL;DR: NVIDIA推出Blackwell B200的数据中心电源配置文件功能，通过智能电源管理和HPC/AI工作负载优化，在功率受限设施中实现高达15%的节能和13%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 解决数据中心在严格功率限制下如何平衡能源效率和性能的问题，特别是在HPC和AI工作负载场景中。

Method: 利用Blackwell架构的硬件和软件创新，提供粗粒度的用户控制，结合HPC和AI工作负载的领域知识进行智能电源管理。

Result: Blackwell第一阶段实现高达15%的节能，关键应用性能保持在97%以上，在功率受限设施中总体吞吐量提升高达13%。

Conclusion: 数据中心电源配置文件功能为功率受限环境中的HPC和AI工作负载提供了有效的能源效率优化方案。

Abstract: This paper presents datacenter power profiles, a new NVIDIA software feature
released with Blackwell B200, aimed at improving energy efficiency and/or
performance. The initial feature provides coarse-grain user control for HPC and
AI workloads leveraging hardware and software innovations for intelligent power
management and domain knowledge of HPC and AI workloads. The resulting
workload-aware optimization recipes maximize computational throughput while
operating within strict facility power constraints. The phase-1 Blackwell
implementation achieves up to 15% energy savings while maintaining performance
levels above 97% for critical applications, enabling an overall throughput
increase of up to 13% in a power-constrained facility.
  KEYWORDS GPU power management, energy efficiency, power profile, HPC
optimization, Max-Q, Blackwell architecture

</details>


### [7] [Toward Co-adapting Machine Learning Job Shape and Cluster Topology](https://arxiv.org/abs/2510.03891)
*Shawn Shuoshuo Chen,Daiyaan Arfeen,Minlan Yu,Peter Steenkiste,Srinivasan Seshan*

Main category: cs.DC

TL;DR: RFold通过在运行时同时调整作业形状和集群拓扑，解决了分布式机器学习作业调度中网络争用最小化和集群利用率最大化之间的固有矛盾。


<details>
  <summary>Details</summary>
Motivation: 在多租户环面拓扑集群中分配资源时，现有调度器通常只能优化一个目标（最小化网络争用或最大化集群利用率），而牺牲另一个目标。

Method: RFold结合两种技术：(1)识别支持作业通信需求的同构作业形状；(2)重新配置支持光学电路交换的拓扑以支持更多样化的作业形状。

Result: 在4096节点环面集群模拟器上的初步评估显示，RFold相比现有方法可将绝对集群利用率提高57%，作业完成时间最多减少11倍。

Conclusion: 研究表明，网络争用最小化和集群利用率最大化这两个目标可以同时实现，通过动态调整作业形状和集群拓扑来达成这一目标。

Abstract: Allocating resources to distributed machine learning jobs in multi-tenant
torus-topology clusters must meet each job's specific placement and
communication requirements, which are typically described using shapes. There
is an inherent tension between minimizing network contention and maximizing
cluster utilization when placing various-shaped jobs. While existing schedulers
typically optimize for one objective at the expense of the other, we
demonstrate that both can be achieved simultaneously.
  Our proposed approach, RFold, adapts both job shapes and the underlying
cluster topology at runtime. This is accomplished by combining two techniques:
(1) identifying homomorphic job shapes that support the jobs communication
needs, and (2) reconfiguring the optical circuit switch-enabled topology to
support more diverse job shapes. Preliminary evaluation performed on a
4096-node torus cluster simulator indicates that RFold can improve absolute
cluster utilization by 57% and reduce job completion time by up to 11x relative
to existing methods

</details>


### [8] [Towards Carbon-Aware Container Orchestration: Predicting Workload Energy Consumption with Federated Learning](https://arxiv.org/abs/2510.03970)
*Zainab Saad,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.DC

TL;DR: 提出基于联邦学习的能耗预测方法，在保护数据隐私的同时实现比集中式方法更低的预测误差（MAE降低11.7%），解决了Kepler和CASPER等系统中隐私与能效预测的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗增加导致碳足迹增长，现有Kubernetes调度优化方法依赖集中式机器学习模型，存在隐私泄露风险且难以跨环境泛化。

Method: 扩展Kubernetes Efficient Power Level Exporter (Kepler)，使用Flower的FedXgbBagging聚合策略，在分布式客户端上协作训练XGBoost模型，无需集中共享数据。

Result: 在SPECPower基准数据集上的实验表明，联邦学习方法比集中式基线的平均绝对误差降低了11.7%。

Conclusion: 该工作为企业在不牺牲运营隐私的前提下实现可持续云计算提供了可行路径，解决了先前系统中数据隐私与能耗预测效率之间的未解权衡。

Abstract: The growing reliance on large-scale data centers to run resource-intensive
workloads has significantly increased the global carbon footprint, underscoring
the need for sustainable computing solutions. While container orchestration
platforms like Kubernetes help optimize workload scheduling to reduce carbon
emissions, existing methods often depend on centralized machine learning models
that raise privacy concerns and struggle to generalize across diverse
environments. In this paper, we propose a federated learning approach for
energy consumption prediction that preserves data privacy by keeping sensitive
operational data within individual enterprises. By extending the Kubernetes
Efficient Power Level Exporter (Kepler), our framework trains XGBoost models
collaboratively across distributed clients using Flower's FedXgbBagging
aggregation using a bagging strategy, eliminating the need for centralized data
sharing. Experimental results on the SPECPower benchmark dataset show that our
FL-based approach achieves 11.7 percent lower Mean Absolute Error compared to a
centralized baseline. This work addresses the unresolved trade-off between data
privacy and energy prediction efficiency in prior systems such as Kepler and
CASPER and offers enterprises a viable pathway toward sustainable cloud
computing without compromising operational privacy.

</details>


### [9] [From Patchwork to Network: A Comprehensive Framework for Demand Analysis and Fleet Optimization of Urban Air Mobility](https://arxiv.org/abs/2510.04186)
*Xuan Jiang,Xuanyu Zhou,Yibo Zhao,Shangqing Cao,Jinhua Zhao,Mark Hansen,Raja Sengupta*

Main category: cs.DC

TL;DR: 提出一个利用现有区域机场和优化异构机队的UAM网络模型，通过LPSim并行仿真框架同时优化需求、运营和地面交通交互，在旧金山湾区案例中可为23万次出行节省20分钟以上时间。


<details>
  <summary>Details</summary>
Motivation: 解决城市空中交通基础设施成本高和运营复杂性的挑战，实现UAM的实际应用。

Method: 开发LPSim大规模并行仿真框架，使用多GPU计算同时优化UAM需求、机队运营和地面交通交互，扩展均衡搜索算法预测需求和确定最优机队组成。

Result: 在旧金山湾区案例研究中，该UAM模型可为23万次选定出行节省超过20分钟旅行时间。

Conclusion: UAM系统的成功关键依赖于与地面交通的无缝集成和动态调度。

Abstract: Urban Air Mobility (UAM) presents a transformative vision for metropolitan
transportation, but its practical implementation is hindered by substantial
infrastructure costs and operational complexities. We address these challenges
by modeling a UAM network that leverages existing regional airports and
operates with an optimized, heterogeneous fleet of aircraft. We introduce
LPSim, a Large-Scale Parallel Simulation framework that utilizes multi-GPU
computing to co-optimize UAM demand, fleet operations, and ground
transportation interactions simultaneously. Our equilibrium search algorithm is
extended to accurately forecast demand and determine the most efficient fleet
composition. Applied to a case study of the San Francisco Bay Area, our results
demonstrate that this UAM model can yield over 20 minutes' travel time savings
for 230,000 selected trips. However, the analysis also reveals that system-wide
success is critically dependent on seamless integration with ground access and
dynamic scheduling.

</details>


### [10] [Beyond Canonical Rounds: Communication Abstractions for Optimal Byzantine Resilience](https://arxiv.org/abs/2510.04310)
*Hagit Attiya,Itay Flam,Jennifer L. Welch*

Main category: cs.DC

TL;DR: 本文研究了异步拜占庭容错系统中通信抽象模式的局限性，发现在关键弹性范围3f < n ≤ 5f内，经典的异步轮次和通信闭包模式存在固有局限，而gather抽象提供了更好的模块化设计基础。


<details>
  <summary>Details</summary>
Motivation: 探索异步拜占庭容错系统中通信抽象模式的适用性，解释为何在最优弹性(n > 3f)下，传统的轮次框架难以实现某些关键任务。

Method: 通过理论分析证明经典异步轮次和通信闭包模式在3f < n ≤ 5f范围内的局限性，并提出gather抽象作为替代方案，展示了如何将连接共识问题归约到gather。

Result: 发现近似共识、十字军共识、可靠广播和gather等关键任务无法通过有限轮次的规范轮算法解决，在通信闭包条件下不可解。同时展示了gather抽象在最优弹性下可实现常数时间解。

Conclusion: 轮次抽象虽然分析方便，但掩盖了拜占庭容错算法的真实复杂性，而更丰富的通信模式如gather为模块化最优弹性设计提供了更好的基础。

Abstract: We study communication abstractions for asynchronous Byzantine fault
tolerance with optimal failure resilience, where $n > 3f$. Two classic patterns
-- canonical asynchronous rounds and communication-closed layers -- have long
been considered as general frameworks for designing distributed algorithms,
making asynchronous executions appear synchronous and enabling modular
reasoning.
  We show that these patterns are inherently limited in the critical resilience
regime $3f < n \le 5f$. Several key tasks -- such as approximate and crusader
agreement, reliable broadcast and gather -- cannot be solved by bounded-round
canonical-round algorithms, and are unsolvable if communication closure is
imposed. These results explain the historical difficulty of achieving
optimal-resilience algorithms within round-based frameworks.
  On the positive side, we show that the gather abstraction admits
constant-time solutions with optimal resilience ($n > 3f$), and supports
modular reductions. Specifically, we present the first optimally-resilient
algorithm for connected consensus by reducing it to gather.
  Our results demonstrate that while round-based abstractions are analytically
convenient, they obscure the true complexity of Byzantine fault-tolerant
algorithms. Richer communication patterns such as gather provide a better
foundation for modular, optimal-resilience design.

</details>


### [11] [Next-Generation Event-Driven Architectures: Performance, Scalability, and Intelligent Orchestration Across Messaging Frameworks](https://arxiv.org/abs/2510.04404)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Ahsan Habib Tareq*

Main category: cs.DC

TL;DR: 本文提出了首个全面的消息系统基准测试框架，评估12个系统在三种典型工作负载下的性能，并引入AIEO智能编排系统，显著优化延迟、资源利用率和成本。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统需要低延迟、容错的事件处理，但缺乏对主流消息系统的统一标准化比较研究。

Method: 开发基准测试框架评估12个消息系统，引入AIEO系统（采用机器学习预测缩放、强化学习动态资源分配和多目标优化）。

Result: 发现各系统性能权衡：Kafka吞吐量最高但运维复杂，Pulsar性能均衡支持多租户，无服务器方案弹性好但延迟较高。AIEO实现34%延迟降低、28%资源利用率提升和42%成本优化。

Conclusion: 贡献了标准化基准测试方法、开源智能编排系统和基于证据的决策指南，为下一代分布式系统设计奠定基础。

Abstract: Modern distributed systems demand low-latency, fault-tolerant event
processing that exceeds traditional messaging architecture limits. While
frameworks including Apache Kafka, RabbitMQ, Apache Pulsar, NATS JetStream, and
serverless event buses have matured significantly, no unified comparative study
evaluates them holistically under standardized conditions. This paper presents
the first comprehensive benchmarking framework evaluating 12 messaging systems
across three representative workloads: e-commerce transactions, IoT telemetry
ingestion, and AI inference pipelines. We introduce AIEO (AI-Enhanced Event
Orchestration), employing machine learning-driven predictive scaling,
reinforcement learning for dynamic resource allocation, and multi-objective
optimization. Our evaluation reveals fundamental trade-offs: Apache Kafka
achieves peak throughput (1.2M messages/sec, 18ms p95 latency) but requires
substantial operational expertise; Apache Pulsar provides balanced performance
(950K messages/sec, 22ms p95) with superior multi-tenancy; serverless solutions
offer elastic scaling for variable workloads despite higher baseline latency
(80-120ms p95). AIEO demonstrates 34\% average latency reduction, 28\% resource
utilization improvement, and 42% cost optimization across all platforms. We
contribute standardized benchmarking methodologies, open-source intelligent
orchestration, and evidence-based decision guidelines. The evaluation
encompasses 2,400+ experimental configurations with rigorous statistical
analysis, providing comprehensive performance characterization and establishing
foundations for next-generation distributed system design.

</details>


### [12] [The R(1)W(1) Communication Model for Self-Stabilizing Distributed Algorithms](https://arxiv.org/abs/2510.04644)
*Hirotsugu Kakugawa,Sayaka Kamei,Masahiro Shibata,Fukuhito Ooshita*

Main category: cs.DC

TL;DR: 提出新的R(1)W(1)通信执行模型，设计自稳定分布式算法解决最大匹配、最小k支配集和最大k依赖集问题，并提供基于随机距离二局部互斥的转换器来在同步消息传递模型中模拟R(1)W(1)模型算法。


<details>
  <summary>Details</summary>
Motivation: 自稳定是设计容错分布式算法的重要方法，能够从任何类型和有限数量的瞬时故障中自动恢复，特别适用于大规模分布式系统。

Method: 提出R(1)W(1)模型，每个进程可以在单步中读写自身和邻居的局部变量；设计自稳定分布式算法；开发基于随机距离二局部互斥的转换器。

Result: 成功设计了在R(1)W(1)模型下的自稳定算法，解决了最大匹配、最小k支配集和最大k依赖集问题，并实现了向同步消息传递模型的转换。

Conclusion: R(1)W(1)模型为自稳定分布式算法设计提供了新框架，所提出的算法和转换器扩展了自稳定理论在分布式系统中的应用范围。

Abstract: Self-stabilization is a versatile methodology in the design of fault-tolerant
distributed algorithms for transient faults. A self-stabilizing system
automatically recovers from any kind and any finite number of transient faults.
This property is specifically useful in modern distributed systems with a large
number of components. In this paper, we propose a new communication and
execution model named the R(1)W(1) model in which each process can read and
write its own and neighbors' local variables in a single step. We propose
self-stabilizing distributed algorithms in the R(1)W(1) model for the problems
of maximal matching, minimal k-dominating set and maximal k-dependent set.
Finally, we propose an example transformer, based on randomized distance-two
local mutual exclusion, to simulate algorithms designed for the R(1)W(1) model
in the synchronous message passing model with synchronized clocks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [A Dense and Efficient Instruction Set Architecture Encoding](https://arxiv.org/abs/2510.04158)
*Emad Jacob Maroun*

Main category: cs.AR

TL;DR: Scry指令集架构通过前向时间引用和内部标记技术，在2字节指令空间内实现了与RISC-V RV64IMC 4字节指令相当的功能，指令密度显著提高。


<details>
  <summary>Details</summary>
Motivation: 指令集设计应最大化指令密度和编码效率，因为这两者直接影响处理器性能，而实现细节通常影响性能、功耗和面积。

Method: 采用前向时间引用作为数据流机制，指令引用未来消耗其输出的指令；使用内部标记技术，处理器内部跟踪数据类型以减少指令数量并提高灵活性。

Result: Scry仅用2字节指令就实现了RISC-V RV64IMC的功能，占用28%的2字节编码空间，而RISC-V占用68%的4字节编码空间。手编译的Scry静态指令密度在小函数中与RV64IMC相当，随函数规模增大而提高。

Conclusion: Scry指令集通过创新的设计方法显著提高了指令密度和编码效率，为现代处理器实现提供了更优化的指令集架构。

Abstract: Instruction density and encoding efficiency are some of the few things
directly affected by an instruction set architecture's design. In contrast, a
processor's implementation often significantly influences performance, power
efficiency, and area usage. Therefore, a major goal of instruction set design
should be maximizing instruction density and encoding efficiency. This paper
introduces the design elements of the Scry instruction set architecture that
most significantly affect instruction density and encoding efficiency. Scry is
a novel and experimental instruction set that revisits first principles to
design an instruction set fit for modern processor implementations. Scry uses
forward-temporal referencing as a means of data flow, where instructions refer
to which future instructions consume their outputs. It also uses internal
tagging, where the processors track data types internally, to reduce the number
of instructions needed and increase flexibility. Combining these two methods,
Scry achieves instruction-feature parity with RISC-V's RV64IMC using only
2-byte instructions compared to RISC-V's 4 bytes. Scry's instructions occupy
only 28% of the 2-byte encoding space, where RV64IMC instructions occupy 68% of
the 4-byte encoding space. We show that hand-compiled Scry's static instruction
density is comparable to RV64IMC for small functions and improves as functions
grow in size.

</details>
