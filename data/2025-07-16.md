<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 8]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Stream programs are monoid homomorphisms with state](https://arxiv.org/abs/2507.10799)
*Tyler Hou,Michael Arntzenius,Max Willsey*

Main category: cs.PL

TL;DR: 论文定义了一类确定性流函数，并证明其可作为状态幺半群同态实现。同态定律比先前的流程序优化语义框架更简单，同时支持丰富的等式推理。


<details>
  <summary>Details</summary>
Motivation: 旨在简化流程序优化的语义框架，同时保留对复杂数据流程序（如顺序组合、并行组合和反馈）的等式推理支持。

Method: 通过将确定性流函数实现为状态幺半群的同态，简化同态定律。

Result: 展示了该方法在分区数据库连接、分层否定和简化TCP模型中的应用。

Conclusion: 该方法简化了流程序优化的语义框架，同时支持丰富的等式推理，适用于多种数据流程序。

Abstract: We define a broad class of deterministic stream functions and show they can
be implemented as homomorphisms into a "state" monoid. The homomorphism laws
are simpler than the conditions of previous semantic frameworks for stream
program optimization, yet retain support for rich equational reasoning over
expressive dataflow programs, including sequential composition, parallel
composition, and feedback. We demonstrate this using examples of partitioned
database joins, stratified negation, and a simplified model of TCP.

</details>


### [2] [The downgrading semantics of memory safety](https://arxiv.org/abs/2507.11282)
*René Rydhof Hansen,Andreas Stenbæk Larsen,Aslan Askarov*

Main category: cs.PL

TL;DR: 本文提出了一种称为“渐进分配器独立性”的概念，用于准确捕捉内存安全的分配器特定方面。


<details>
  <summary>Details</summary>
Motivation: 传统的内存安全定义通常基于负面事件，被认为缺乏原则性。本文旨在通过非干扰理论提供更语义化的内存安全定义。

Method: 研究使用低级语言和平面内存模型，通过malloc和free原语操作指针（视为整数），提出渐进分配器独立性的概念，并利用信息流技术处理内存不足和指针到整数的转换。

Result: 渐进分配器独立性成功捕捉了内存安全的分配器特定方面，并通过信息流技术解决了内存不足和指针转换的问题。

Conclusion: 本文提出的渐进分配器独立性为内存安全提供了更语义化的定义，并展示了信息流技术的适用性。

Abstract: Memory safety is traditionally characterized in terms of bad things that
cannot happen, an approach that is often criticized as unprincipled. Prior work
suggest a connection between memory safety and noninterference, but no
satisfactory semantic notion of memory safety is currently known.
  This work proposes a notion of gradual allocator independence that accurately
captures many allocator-specific aspects of memory safety. We consider a
low-level language with access to an allocator that provides malloc and free
primitives in a flat memory model. Pointers are just integers, and as such it
is trivial to write memory-unsafe programs. The basic intuition of gradual
allocator independence is that of noninterference, namely that allocators must
not influence program execution. This intuition is refined in two important
ways to account for the allocators running out-of-memory and for programs to
have pointer-to-integer casts. The key insight of the definition is to treat
these extensions as forms of downgrading and give them satisfactory technical
treatment using the state-of-the-art information flow machinery.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [FAFO: Over 1 million TPS on a single node running EVM while still Merkleizing every block](https://arxiv.org/abs/2507.10757)
*Ryan Zarick,Isaac Zhang,Daniel Wong,Thomas Kim,Bryan Pellegrino,Mignon Li,Kelvin Wong*

Main category: cs.DC

TL;DR: FAFO是一种区块链交易调度器，通过重新排序交易以提高并行性，解决了数据竞争导致的吞吐量限制问题。


<details>
  <summary>Details</summary>
Motivation: 当前区块链执行吞吐量受限于数据竞争，降低了执行层的并行性。

Method: FAFO使用CPU优化的Bloom过滤器检测冲突，并高效调度并行交易执行。

Result: 在单节点上实现了每秒110万次原生ETH转账和50万次ERC20转账，成本降低91%。

Conclusion: FAFO证明通过优化执行层和交易调度器设计，可以实现支持未来去中心化应用的高吞吐量。

Abstract: Current blockchain execution throughput is limited by data contention,
reducing execution layer parallelism. Fast Ahead-of-Formation Optimization
(FAFO) is the first blockchain transaction scheduler to address this problem by
reordering transactions before block formation for maximum concurrency. FAFO
uses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts
and schedule parallel transaction execution at high throughput and low
overhead.
  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1
million native ETH transfers per second and over half a million ERC20 transfers
per second on a single node (Table 1), with 91% lower cost compared to
state-of-the-art sharded execution. Unlike many other existing high throughput
blockchain execution clients, FAFO uses QMDB to Merkleize world state after
every block, enabling light clients and stateless validation for ZK-based
vApps. FAFO scales with minimal synchronization overhead, scaling linearly with
additional CPU resources until it fully exploits the maximum parallelism of the
underlying transaction flow. FAFO proves that the high throughput necessary to
support future decentralized applications can be achieved with a streamlined
execution layer and innovations in blockchain transaction scheduler design.
FAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.

</details>


### [4] [Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks](https://arxiv.org/abs/2507.10789)
*Aaron Jarmusch,Nathan Graddon,Sunita Chandrasekaran*

Main category: cs.DC

TL;DR: 本文对NVIDIA Blackwell架构进行了微架构分析，通过微基准测试揭示了其关键子系统，并与Hopper架构进行了比较。


<details>
  <summary>Details</summary>
Motivation: 科学研究的快速发展需要更强的计算能力，GPU是解决方案之一。本文旨在深入分析Blackwell架构的性能特征，为开发者提供优化依据。

Method: 通过微基准测试研究Blackwell架构的延迟、吞吐量、缓存行为和调度细节，并与Hopper架构进行比较。

Result: 揭示了Blackwell架构的关键设计细节，包括第5代张量核心支持FP4和FP6精度，并展示了与Hopper架构的性能对比。

Conclusion: 研究结果为开发者优化Blackwell平台上的工作负载提供了实用见解，并为GPU架构研究贡献了新数据。

Abstract: The rapid development in scientific research provides a need for more compute
power, which is partly being solved by GPUs. This paper presents a
microarchitectural analysis of the modern NVIDIA Blackwell architecture by
studying GPU performance
  features with thought through microbenchmarks. We unveil key subsystems,
including the memory hierarchy, SM execution
  pipelines, and the SM sub-core units, including the 5th generation tensor
cores supporting FP4 and FP6 precisions.
  To understand the different key features of the NVIDIA GPU, we study latency,
throughput, cache behavior, and scheduling
  details, revealing subtle tuning metrics in the design of Blackwell. To
develop a comprehensive analysis, we compare the
  Blackwell architecture with the previous Hopper architecture by using the
GeForce RTX 5080 and H100 PCIe, respectively. We
  evaluate and compare results, presenting both generational improvements and
performance regressions. Additionally, we
  investigate the role of power efficiency and energy consumption under varied
workloads. Our findings provide actionable insights
  for application developers, compiler writers, and performance engineers to
optimize workloads on Blackwell-based platforms,
  and contribute new data to the growing research on GPU architectures.

</details>


### [5] [MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix Unit](https://arxiv.org/abs/2507.11067)
*Yinuo Wang,Tianqi Mao,Lin Gan,Wubing Wan,Zeyu Song,Jiayu Fu,Lanke He,Wenqiang Wang,Zekun Yin,Wei Xue,Guangwen Yang*

Main category: cs.DC

TL;DR: MMStencil通过矩阵加速策略、SIMD和矩阵单元优化、内存优化及多线程并行，显著提升了3D高阶模板计算的性能，超越了现有库和工业级GPU实现。


<details>
  <summary>Details</summary>
Motivation: 研究3D高阶模板计算在HPC中的应用不足，利用多核CPU上的矩阵单元探索加速策略。

Method: 结合SIMD和矩阵单元优化算法，解决内存访问问题；提出内存优化和多线程并行范式；采用DMA-based NUMA通信。

Result: MMStencil在Nvidia A100 GPU上性能提升2.1倍，实际HPC应用中RTM速度提升1.8倍。

Conclusion: MMStencil通过综合优化实现了高性能的3D高阶模板计算，适用于实际HPC场景。

Abstract: Matrix-accelerated stencil computation is a hot research topic, yet its
application to three-dimensional (3D) high-order stencils and HPC remains
underexplored. With the emergence of matrix units on multicore CPUs, we analyze
matrix-based acceleration strategies and tailor an optimal approach for 3D
high-order stencils. We introduce algorithmic optimizations based on SIMD and
matrix units to address strided memory accesses, alignment conflicts, and
redundant accesses. We propose memory optimizations to boost on-package memory
efficiency, and a novel multi-thread parallelism paradigm to overcome
data-sharing challenges caused by the absence of shared data caches. MMStencil
sustains consistently high hardware utilization across diverse stencil shapes
and dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA
effects and MPI limitations in hybrid parallelism. Combining all the
innovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100
GPGPU by up to 2.1x. Moreover, the performance improvements translate directly
to real-world HPC applications and enable RTM applications to yield 1.8x
speedup versus a highly optimized industrial Nvidia A100 GPGPU version.

</details>


### [6] [Generating Dynamic Graph Algorithms for Multiple Backends for a Graph DSL](https://arxiv.org/abs/2507.11094)
*Nibedita Behera,Ashwina Kumar,Atharva Chougule,Mohammed Shan P S,Rushabh Nirdosh Lalwani,Rupesh Nasre*

Main category: cs.DC

TL;DR: 提出了一种抽象方案和运行时优化，用于高效处理动态图算法，通过DSL自动生成并行代码，并在多核、分布式和多核环境中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着非结构化和半结构化数据的快速增长，并行化图算法变得至关重要，但动态图的高效和正确同步代码生成仍是一个挑战。

Method: 引入抽象方案和运行时优化，通过DSL表达动态处理逻辑，并自动生成针对多核、分布式和多核环境的并行代码。

Result: 在十种大型图和三种常用算法（最短路径、PageRank和三角形计数）上验证了方法的有效性。

Conclusion: 该方案为动态图算法的高效并行处理提供了可行解决方案。

Abstract: With the rapid growth of unstructured and semistructured data, parallelizing
graph algorithms has become essential for efficiency. However, due to the
inherent irregularity in computation, memory access patterns, and
communication, graph algorithms are notoriously difficult to parallelize. To
address this challenge, several libraries, frameworks, and domain-specific
languages (DSLs) have been proposed to ease the parallel programming burden for
domain experts. Existing frameworks partially or fully abstract away
parallelism intricacies, provide intuitive scheduling mnemonics, and employ
program analysis to identify data races and generate synchronization code.
Despite these advances, most frameworks are limited in their abstractions and
runtime optimizations, especially when dealing with static graphs. In contrast,
many real-world graphs are inherently dynamic, with evolving structures over
time through insertions, deletions, and modifications of vertices, edges, and
attributes. Generating efficient and correctly synchronized code for such
dynamic graph algorithms remains a significant challenge.
  In this work, we introduce an abstraction scheme and runtime optimizations
for the efficient processing of morph algorithms. Specifically, given an
initial graph G and a set of updates $\Delta$G involving edge insertions and
deletions, we express the dynamic processing logic through a DSL and
automatically generate parallel code targeting multicore, distributed, and
many-core environments. We demonstrate the effectiveness of our approach by
applying the DSL-generated code to ten large graphs with diverse
characteristics and three widely used algorithms: Shortest Paths, PageRank, and
Triangle Counting.

</details>


### [7] [Boosting Scientific Error-Bounded Lossy Compression through Optimized Synergistic Lossy-Lossless Orchestration](https://arxiv.org/abs/2507.11165)
*Shixun Wu,Jinwen Pan,Jinyang Liu,Jiannan Tian,Ziwei Qiu,Jiajun Huang,Kai Zhao,Xin Liang,Sheng Di,Zizhong Chen,Franck Cappello*

Main category: cs.DC

TL;DR: cuSZ-Hi是一种优化的GPU科学数据压缩器，支持高压缩比和低延迟，相比现有方法提升显著。


<details>
  <summary>Details</summary>
Motivation: 高性能计算架构发展下，科学计算工作流需要高效的数据压缩解决方案。

Method: 优化并行插值数据预测方案，探索无损编码技术，系统评估性能。

Result: cuSZ-Hi在相同误差下压缩比提升249%，相同PSNR下提升215%。

Conclusion: cuSZ-Hi在压缩比和性能上优于现有方法，适用于多种数据特征。

Abstract: As high-performance computing architectures evolve, more scientific computing
workflows are being deployed on advanced computing platforms such as GPUs.
These workflows can produce raw data at extremely high throughputs, requiring
urgent high-ratio and low-latency error-bounded data compression solutions. In
this paper, we propose cuSZ-Hi, an optimized high-ratio GPU-based scientific
error-bounded lossy compressor with a flexible, domain-irrelevant, and fully
open-source framework design. Our novel contributions are: 1) We maximally
optimize the parallelized interpolation-based data prediction scheme on GPUs,
enabling the full functionalities of interpolation-based scientific data
prediction that are adaptive to diverse data characteristics; 2) We thoroughly
explore and investigate lossless data encoding techniques, then craft and
incorporate the best-fit lossless encoding pipelines for maximizing the
compression ratio of cuSZ-Hi; 3) We systematically evaluate cuSZ-Hi on
benchmarking datasets together with representative baselines. Compared to
existing state-of-the-art scientific lossy compressors, with comparative or
better throughput than existing high-ratio scientific error-bounded lossy
compressors on GPUs, cuSZ-Hi can achieve up to 249% compression ratio
improvement under the same error bound, and up to 215% compression ratio
improvement under the same decompression data PSNR.

</details>


### [8] [Cyclic Data Streaming on GPUs for Short Range Stencils Applied to Molecular Dynamics](https://arxiv.org/abs/2507.11289)
*Martin Rose,Simon Homes,Lukas Ramsperger,Jose Gracia,Christoph Niethammer,Jadran Vrabec*

Main category: cs.DC

TL;DR: 提出了一种基于GPU集群的高带宽通信框架，用于高性能科学计算，实现了显式算法的线性性能扩展。


<details>
  <summary>Details</summary>
Motivation: 追求科学计算的最高性能，解决GPU集群中高效通信和并行化的问题。

Method: 框架通过环形通信在GPU间传递数据切片，实现时间并行化，用户只需编写GPU内核而无需了解底层并行策略。

Result: 在分子动力学模拟中，该框架在强扩展性能上优于LAMMPS。

Conclusion: 该框架为高性能科学计算提供了一种高效且易于使用的解决方案。

Abstract: In the quest for highest performance in scientific computing, we present a
novel framework that relies on high-bandwidth communication between GPUs in a
compute cluster. The framework offers linear scaling of performance for
explicit algorithms that is only limited by the size of the dataset and the
number of GPUs. Slices of the dataset propagate in a ring of processes (GPUs)
from one GPU, where they are processed, to the next, which results in a
parallel-in-time parallelization. The user of the framework has to write GPU
kernels that implement the algorithm and provide slices of the dataset.
Knowledge about the underlying parallelization strategy is not required because
the communication between processes is carried out by the framework. As a case
study, molecular dynamics simulation based on the Lennard-Jones potential is
implemented to measure the performance for a homogeneous fluid. Single node
performance and strong scaling behavior of this framework is compared to
LAMMPS, which is outperformed in the strong scaling case.

</details>


### [9] [A new Dune grid for scalable dynamic adaptivity based on the p4est software library](https://arxiv.org/abs/2507.11386)
*Carsten Burstedde,Mikhail Kirilin,Robert Klöfkorn*

Main category: cs.DC

TL;DR: 本文扩展了Dune求解器库，新增了对开源p4est软件的网格接口，以继承其卓越的MPI可扩展性和多块网格支持。


<details>
  <summary>Details</summary>
Motivation: 利用p4est的无限MPI可扩展性、轻量数据结构和原生多块网格支持，弥补Dune现有网格接口的不足。

Method: 通过Dune-Grid接口实现与p4est的耦合，并与基于Dune-ALUGrid的现有实现进行并行环境下的性能对比。

Result: 新实现的可扩展性优于Dune-ALUGrid，并提出了改进的平衡策略，性能优于p4est原有策略。

Conclusion: 新接口显著提升了Dune的可扩展性和性能，适用于复杂网格拓扑的并行计算。

Abstract: In this work we extend the Dune solver library with another grid interface to
the open-source p4est software. While Dune already supports about a dozen
different mesh implementations through its mesh interface Dune-Grid, we
undertake this new coupling effort in order to inherit p4est's practically
unlimited MPI scalability as well as its relatively thin data structures, and
its native support for multi-block (forest) mesh topologies in both 2D and 3D.
  The presented implementation is compared to an existing implementation based
on Dune-ALUGrid for a variety of challenging test examples in a parallel
environment. The numerical experiments show that the implementation presented
here is outperforming Dune-ALUGrid in terms of scalability. In addition, an
alternative balancing strategy is presented to ensure 2:1 balancing across
element faces showing improved performance compared to the existing p4est
balance strategy in the numerical examples considered in this work.

</details>


### [10] [Quantifying the Energy Consumption and Carbon Emissions of LLM Inference via Simulations](https://arxiv.org/abs/2507.11417)
*Miray Özcan,Philipp Wiesner,Philipp Weiß,Odej Kao*

Main category: cs.DC

TL;DR: 论文提出了一种模拟框架，用于评估LLM推理在不同部署设置下的能源和碳排放影响，填补了现有框架的空白。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的环境影响日益显著，尤其是推理阶段的碳排放占其总生命周期的一半以上，但现有模拟框架缺乏对能源的考量。

Method: 扩展了高保真LLM推理模拟器，加入GPU功耗模型，并集成到能源系统协同模拟环境中，以量化碳排放并探索碳感知调度的潜力。

Result: 框架揭示了推理参数对能源需求和碳排放的影响，展示了一个案例中69.2%的可再生能源抵消潜力。

Conclusion: 该框架为未来碳感知推理基础设施设计提供了基础。

Abstract: The environmental impact of Large Language Models (LLMs) is rising
significantly, with inference now accounting for more than half of their total
lifecycle carbon emissions. However, existing simulation frameworks, which are
increasingly used to determine efficient LLM deployments, lack any concept of
power and, therefore, cannot accurately estimate inference-related emissions.
We present a simulation framework to assess the energy and carbon implications
of LLM inference under varying deployment setups. First, we extend a
high-fidelity LLM inference simulator with a GPU power model that estimates
power consumption based on utilization metrics, enabling analysis across
configurations like batch size, sequence length, and model parallelism. Second,
we integrate simulation outputs into an energy system co-simulation environment
to quantify carbon emissions under specific grid conditions and explore the
potential of carbon-aware scheduling. Through scenario-based analysis, our
framework reveals how inference parameters affect energy demand and carbon
footprint, demonstrates a renewable offset potential of up to 69.2% in an
illustrative deployment case, and provides a foundation for future carbon-aware
inference infrastructure design.

</details>


### [11] [FLsim: A Modular and Library-Agnostic Simulation Framework for Federated Learning](https://arxiv.org/abs/2507.11430)
*Arnab Mukherjee,Raju Halder,Joydeep Chandra*

Main category: cs.DC

TL;DR: FLsim是一个模块化、可扩展且高效的联邦学习模拟框架，支持多样化的实验需求。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习研究中新方法与现有技术对比的挑战。

Method: 开发FLsim框架，支持自定义数据分布、学习算法、网络拓扑、模型聚合及区块链集成。

Result: 实验证明FLsim能有效模拟多种先进联邦学习实验。

Conclusion: FLsim为研究人员和实践者提供了前所未有的灵活性和功能性。

Abstract: Federated Learning (FL) has undergone significant development since its
inception in 2016, advancing from basic algorithms to complex methodologies
tailored to address diverse challenges and use cases. However, research and
benchmarking of novel FL techniques against a plethora of established
state-of-the-art solutions remain challenging. To streamline this process, we
introduce FLsim, a comprehensive FL simulation framework designed to meet the
diverse requirements of FL workflows in the literature. FLsim is characterized
by its modularity, scalability, resource efficiency, and controlled
reproducibility of experimental outcomes. Its easy to use interface allows
users to specify customized FL requirements through job configuration, which
supports: (a) customized data distributions, ranging from non-independent and
identically distributed (non-iid) data to independent and identically
distributed (iid) data, (b) selection of local learning algorithms according to
user preferences, with complete agnosticism to ML libraries, (c) choice of
network topology illustrating communication patterns among nodes, (d)
definition of model aggregation and consensus algorithms, and (e) pluggable
blockchain support for enhanced robustness. Through a series of experimental
evaluations, we demonstrate the effectiveness and versatility of FLsim in
simulating a diverse range of state-of-the-art FL experiments. We envisage that
FLsim would mark a significant advancement in FL simulation frameworks,
offering unprecedented flexibility and functionality for researchers and
practitioners alike.

</details>


### [12] [Uniting the World by Dividing it: Federated Maps to Enable Spatial Applications](https://arxiv.org/abs/2507.11437)
*Sagar Bharadwaj,Srinivasan Seshan,Anthony Rowe*

Main category: cs.DC

TL;DR: 论文提出了一种联邦空间命名系统，以解决现有集中式地图基础设施在覆盖范围和隐私方面的不足，支持室内外空间应用。


<details>
  <summary>Details</summary>
Motivation: 现有空间命名系统（如Google和Apple地图）由少数大公司控制，主要覆盖室外公共空间，无法满足新兴应用（如增强现实）对室内外详细地图的需求。

Method: 提出联邦空间命名系统，允许多方管理和提供自己的地图，实现地图管理的可扩展性和隐私保护。

Result: 讨论了支持联邦地图的基本服务和实际实现方法。

Conclusion: 联邦空间命名系统能够解决集中式地图的局限性，支持更广泛的空间应用。

Abstract: The emergence of the Spatial Web -- the Web where content is tied to
real-world locations has the potential to improve and enable many applications
such as augmented reality, navigation, robotics, and more. The Spatial Web is
missing a key ingredient that is impeding its growth -- a spatial naming system
to resolve real-world locations to names. Today's spatial naming systems are
digital maps such as Google and Apple maps. These maps and the location-based
services provided on top of these maps are primarily controlled by a few large
corporations and mostly cover outdoor public spaces. Emerging classes of
applications, such as persistent world-scale augmented reality, require
detailed maps of both outdoor and indoor spaces. Existing centralized mapping
infrastructures are proving insufficient for such applications because of the
scale of cartography efforts required and the privacy of indoor map data.
  In this paper, we present a case for a federated spatial naming system, or in
other words, a federated mapping infrastructure. This enables disparate parties
to manage and serve their own maps of physical regions and unlocks scalability
of map management, isolation and privacy of maps. Map-related services such as
address-to-location mapping, location-based search, and routing needs
re-architecting to work on federated maps. We discuss some essential services
and practicalities of enabling these services.

</details>


### [13] [Scaling the memory wall using mixed-precision -- HPG-MxP on an exascale machine](https://arxiv.org/abs/2507.11512)
*Aditya Kashi,Nicholson Koukpaizan,Hao Lu,Michael Matheson,Sarp Oral,Feiyi Wang*

Main category: cs.DC

TL;DR: 混合精度算法在科学计算中应用，HPG-MxP基准测试首次在GPU超级计算机上实现1.6倍加速。


<details>
  <summary>Details</summary>
Motivation: 探索混合精度算法在科学计算中的实际增益，尤其是针对稀疏矩阵应用。

Method: 提出并优化HPG-MxP基准测试，结合双精度和单精度计算。

Result: 在现代GPU超级计算机上实现1.6倍的加速。

Conclusion: 混合精度算法在稀疏矩阵应用中具有潜力，但实际增益需进一步研究。

Abstract: Mixed-precision algorithms have been proposed as a way for scientific
computing to benefit from some of the gains seen for artificial intelligence
(AI) on recent high performance computing (HPC) platforms. A few applications
dominated by dense matrix operations have seen substantial speedups by
utilizing low precision formats such as FP16. However, a majority of scientific
simulation applications are memory bandwidth limited. Beyond preliminary
studies, the practical gain from using mixed-precision algorithms on a given
HPC system is largely unclear.
  The High Performance GMRES Mixed Precision (HPG-MxP) benchmark has been
proposed to measure the useful performance of a HPC system on sparse
matrix-based mixed-precision applications. In this work, we present a highly
optimized implementation of the HPG-MxP benchmark for an exascale system and
describe our algorithm enhancements. We show for the first time a speedup of
1.6x using a combination of double- and single-precision on modern GPU-based
supercomputers.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [Device-Level Optimization Techniques for Solid-State Drives: A Survey](https://arxiv.org/abs/2507.10573)
*Tianyu Ren,Yajuan Du,Jinhua Cui,Yina Lv,Qiao Li,Chun Jason Xue*

Main category: cs.AR

TL;DR: 本文综述了固态硬盘（SSD）的架构、关键挑战及优化技术，探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着存储需求的增长，SSD在可扩展性、耐久性、延迟和安全性方面面临挑战，需要深入分析和优化。

Method: 通过分析SSD的基本组件（如NAND闪存结构、控制器功能、主机接口协议）和主要挑战（如可靠性、耐久性、延迟、安全性），探讨了优化技术（如纠错机制、FTL增强、新兴架构）。

Result: 总结了SSD当前的技术现状和优化方法，并提出了未来研究方向（如QLC/PLC NAND扩展性、AI/LLM工作负载优化）。

Conclusion: 本文为下一代SSD的开发提供了指导，旨在平衡性能、寿命和安全性。

Abstract: Solid-state drives (SSDs) have revolutionized data storage with their high
performance, energy efficiency, and reliability. However, as storage demands
grow, SSDs face critical challenges in scalability, endurance, latency, and
security. This survey provides a comprehensive analysis of SSD architecture,
key challenges, and device-level optimization techniques. We first examine the
fundamental components of SSDs, including NAND flash memory structures, SSD
controller functionalities (e.g., address mapping, garbage collection, wear
leveling), and host interface protocols (SATA, SAS, NVMe). Next, we discuss
major challenges such as reliability degradation, endurance limitations,
latency variations, and security threats (e.g., secure deletion, ransomware
defense). We then explore advanced optimization techniques, including error
correction mechanisms, flash translation layer (FTL) enhancements, and emerging
architectures like zoned namespace (ZNS) SSDs and flexible data placement
(FDP). Finally, we highlight open research challenges, such as QLC/PLC NAND
scalability, performance-reliability trade-offs, and SSD optimizations for
AI/LLM workloads. This survey aims to guide future research in developing
next-generation SSDs that balance performance, longevity, and security in
evolving storage ecosystems.

</details>


### [15] [SPICEAssistant: LLM using SPICE Simulation Tools for Schematic Design of Switched-Mode Power Supplies](https://arxiv.org/abs/2507.10639)
*Simon Nau,Jan Krummenauer,André Zimmermann*

Main category: cs.AR

TL;DR: 论文提出SPICEAssistant框架，通过工具接口让LLM与SPICE模拟器交互，提升其在SMPS设计中的性能，比GPT-4o表现高38%。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在电子设计自动化（EDA）中的应用潜力，特别是SMPS设计中的挑战，如模拟工具结果解析和多步骤设计过程。

Method: 提出SPICEAssistant框架，为LLM提供工具接口，使其能与SPICE模拟器灵活交互，优化电路设计。

Result: 在256个问题的基准测试中，SPICEAssistant显著提升LLM性能，模拟反馈和多轮迭代增强设计能力。

Conclusion: SPICEAssistant框架有效解决了LLM在SMPS设计中的局限性，性能提升显著。

Abstract: State-of-the-art large language models (LLMs) show high performance across a
wide range of tasks in many domains of science. In the field of electronic
design automation (EDA), it is yet to be determined to what extent they are
capable to understand, adapt, and dimension electronic circuits. This paper
focuses on the application of LLMs to switched-mode power supply (SMPS) design
on printed circuit boards (PCBs). Particular challenges for LLMs in this
context include their limited ability to interpret results from key simulation
tools like SPICE and the multi-step design process. To address these
challenges, we suggest SPICEAssistant, a framework that provides a broad
selection of tools to an LLM. The tools serve as an interface to SPICE,
allowing the LLM to interact flexibly with the simulator to estimate the impact
of its modifications to the circuit. To evaluate the performance of
SPICEAssistant, we defined a benchmark consisting of 256 questions testing the
ability to adapt circuit netlists to fulfil different SMPS design tasks. The
benchmarking results show that simulation feedback effectively improves SMPS
design capabilities of LLMs. An increasing number of simulation iterations
leads to enhanced performance. The SPICEAssistant framework significantly
outperforms the standalone LLM GPT-4o on the benchmark by approximately 38%.

</details>


### [16] [LASANA: Large-scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration](https://arxiv.org/abs/2507.10748)
*Jason Ho,James A. Boyle,Linshen Liu,Andreas Gerstlauer*

Main category: cs.AR

TL;DR: LASANA是一种利用机器学习快速建模和模拟神经形态系统中模拟子块的新方法，显著提升了速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 神经形态系统需要更高效的AI工作负载处理，但现有工具在快速探索和协同设计方面存在不足。

Method: 通过SPICE级模拟训练机器学习模型，预测电路能量、性能和模拟/数字接口行为。

Result: 在MNIST和脉冲MNIST上，LASANA比SPICE快三个数量级，能量、延迟和行为误差分别低于7%、8%和2%。

Conclusion: LASANA为神经形态架构的快速探索和协同设计提供了高效工具。

Abstract: Neuromorphic systems using in-memory or event-driven computing are motivated
by the need for more energy-efficient processing of artificial intelligence
workloads. Emerging neuromorphic architectures aim to combine traditional
digital designs with the computational efficiency of analog computing and novel
device technologies. A crucial problem in the rapid exploration and co-design
of such architectures is the lack of tools for fast and accurate modeling and
simulation. Typical mixed-signal design tools integrate a digital simulator
with an analog solver like SPICE, which is prohibitively slow for large
systems. By contrast, behavioral modeling of analog components is faster, but
existing approaches are fixed to specific architectures with limited energy and
performance modeling. In this paper, we propose LASANA, a novel approach that
leverages machine learning to derive data-driven surrogate models of analog
sub-blocks in a digital backend architecture. LASANA uses SPICE-level
simulations of a circuit to train ML models that predict circuit energy,
performance, and behavior at analog/digital interfaces. Such models can provide
energy and performance annotation on top of existing behavioral models or
function as replacements to analog simulation. We apply LASANA to an analog
crossbar array and a spiking neuron circuit. Running MNIST and spiking MNIST,
LASANA surrogates demonstrate up to three orders of magnitude speedup over
SPICE, with energy, latency, and behavioral error less than 7%, 8%, and 2%,
respectively.

</details>


### [17] [OpenGCRAM: An Open-Source Gain Cell Compiler Enabling Design-Space Exploration for AI Workloads](https://arxiv.org/abs/2507.10849)
*Xinxin Wang,Lixian Yan,Shuhan Liu,Luke Upton,Zhuoqi Cai,Yiming Tan,Shengman Li,Koustav Jana,Peijing Li,Jesse Cirimelli-Low,Thierry Tambe,Matthew Guthaus,H. -S. Philip Wong*

Main category: cs.AR

TL;DR: OpenGCRAM是一个开源GCRAM编译器，用于快速生成优化的GCRAM电路设计和布局，支持多样化的应用需求。


<details>
  <summary>Details</summary>
Motivation: GCRAM因其高密度、低功耗和可调保留时间成为加速器片上内存的理想选择，但设计和优化过程耗时。

Method: 通过用户指定的配置（如字大小和字数），生成电路设计、DRC/LVS清洁布局，并提供面积、延迟和功耗模拟。

Result: OpenGCRAM实现了快速、准确、可定制和优化的GCRAM块生成，减少了设计时间并确保工艺合规。

Conclusion: OpenGCRAM为多样化应用需求提供了性能定制的内存块，显著提升了GCRAM的设计效率。

Abstract: Gain Cell memory (GCRAM) offers higher density and lower power than SRAM,
making it a promising candidate for on-chip memory in domain-specific
accelerators. To support workloads with varying traffic and lifetime metrics,
GCRAM also offers high bandwidth, ultra low leakage power and a wide range of
retention times, which can be adjusted through transistor design (like
threshold voltage and channel material) and on-the-fly by changing the
operating voltage. However, designing and optimizing GCRAM sub-systems can be
time-consuming. In this paper, we present OpenGCRAM, an open-source GCRAM
compiler capable of generating GCRAM bank circuit designs and DRC- and
LVS-clean layouts for commercially available foundry CMOS, while also providing
area, delay, and power simulations based on user-specified configurations
(e.g., word size and number of words). OpenGCRAM enables fast, accurate,
customizable, and optimized GCRAM block generation, reduces design time, ensure
process compliance, and delivers performance-tailored memory blocks that meet
diverse application requirements.

</details>


### [18] [Mapping Fusion: Improving FPGA Technology Mapping with ASIC Mapper](https://arxiv.org/abs/2507.10912)
*Cunxi Yu*

Main category: cs.AR

TL;DR: FuseMap框架利用强化学习改进FPGA的LUT映射，结合ASIC技术映射，提升性能和面积效率。


<details>
  <summary>Details</summary>
Motivation: ASIC技术映射可能提升LUT映射性能，两者可协同工作。

Method: 提出FuseMap框架，利用强化学习进行设计特定选择。

Result: 在多个基准测试中，FuseMap提高了映射精度，减少了延迟和面积。

Conclusion: FuseMap为FPGA设计流程提供了更高效的LUT映射解决方案。

Abstract: LUT (Look-Up Table) mapping is a critical step in FPGA logic synthesis, where
a logic network is transformed into a form that can be directly implemented
using the FPGA's LUTs. An FPGA LUT is a flexible digital memory structure that
can implement any logic function of a limited number of inputs, typically 4 to
6 inputs, depending on the FPGA architecture. The goal of LUT mapping is to map
the Boolean network into LUTs, where each LUT can implement any function with a
fixed number of inputs. In parallel to FPGA technology mapping, ASIC technology
mapping maps the Boolean network to user-defined standard cells, which has
traditionally been developed separately from LUT mapping algorithms. However,
in this work, our motivating examples demonstrate that ASIC technology mappers
can potentially improve the performance of LUT mappers, such that standard cell
mapping and LUT mapping work in an incremental manner.
  Therefore, we propose the FuseMap framework, which explores this opportunity
to improve LUT mapping in the FPGA design flow by utilizing reinforcement
learning to make design-specific choices during cell selection. The
effectiveness of FuseMap is evaluated on a wide range of benchmarks, different
technology libraries, and technology mappers. The experimental results
demonstrate that FuseMap achieves higher mapping accuracy while reducing delay
and area across diverse circuit designs collected from ISCAS 85/89, ITC/ISCAS
99, VTR 8.0, and EPFL benchmarks.

</details>


### [19] [Security Enclave Architecture for Heterogeneous Security Primitives for Supply-Chain Attacks](https://arxiv.org/abs/2507.10971)
*Kshitij Raj,Atri Chatterjee,Patanjali SLPSK,Swarup Bhunia,Sandip Ray*

Main category: cs.AR

TL;DR: CITADEL是一个模块化安全框架，旨在简化SoC安全架构的设计，通过可配置的IP块构建定制安全机制，资源开销极小。


<details>
  <summary>Details</summary>
Motivation: SoC安全架构设计复杂且易出错，需要高效解决方案。

Method: 提出CITADEL框架，采用模块化、可配置的IP块，适应多种威胁。

Result: CITADEL资源开销小，适用于不同ASIC技术。

Conclusion: CITADEL是增强SoC安全性的实用解决方案。

Abstract: Designing secure architectures for system-on-chip (SoC) platforms is a highly
intricate and time-intensive task, often requiring months of development and
meticulous verification. Even minor architectural oversights can lead to
critical vulnerabilities that undermine the security of the entire chip. In
response to this challenge, we introduce CITADEL, a modular security framework
aimed at streamlining the creation of robust security architectures for SoCs.
CITADEL offers a configurable, plug-and-play subsystem composed of custom
intellectual property (IP) blocks, enabling the construction of diverse
security mechanisms tailored to specific threats. As a concrete demonstration,
we instantiate CITADEL to defend against supply-chain threats, illustrating how
the framework adapts to one of the most pressing concerns in hardware security.
This paper explores the range of obstacles encountered when building a unified
security architecture capable of addressing multiple attack vectors and
presents CITADEL's strategies for overcoming them. Through several real-world
case studies, we showcase the practical implementation of CITADEL and present a
thorough evaluation of its impact on silicon area and power consumption across
various ASIC technologies. Results indicate that CITADEL introduces only
minimal resource overhead, making it a practical solution for enhancing SoC
security.

</details>


### [20] [SystolicAttention: Fusing FlashAttention within a Single Systolic Array](https://arxiv.org/abs/2507.11331)
*Jiawei Lin,Guokai Chen,Yuanlong Li,Thomas Bourgeat*

Main category: cs.AR

TL;DR: 论文提出了一种增强型脉动阵列架构FSA，以解决现有脉动阵列在执行FlashAttention时的低效问题，通过SystolicAttention调度算法显著提升了阵列利用率。


<details>
  <summary>Details</summary>
Motivation: 当前脉动阵列在执行FlashAttention时因频繁的数据交换和非矩阵操作导致利用率低下，FSA旨在消除对外部向量单元的依赖，提升性能。

Method: 提出FSA架构和SystolicAttention调度算法，将FlashAttention操作映射到脉动阵列上，实现细粒度的元素级重叠执行。

Result: FSA在注意力FLOPs/s利用率上比AWS NeuronCore-v2和Google TPUv5e分别高出1.77倍和4.83倍，面积开销仅约10%。

Conclusion: FSA通过优化脉动阵列架构和调度算法，显著提升了FlashAttention的执行效率，为Transformer模型的加速提供了新方案。

Abstract: Transformer models rely heavily on scaled dot-product attention (SDPA),
typically implemented using the FlashAttention algorithm. However, current
systolic-array-based accelerators face significant challenges when executing
FlashAttention. Systolic arrays can only achieve high utilization for
consecutive and large matrix multiplications. In contrast, FlashAttention
requires frequently interleaved matrix multiplications and softmax operations.
  The frequent data swaps between the systolic array and external vector units
result in low systolic array utilization. This is further exacerbated by the
fact that softmax involves numerous non-matrix operations, which are not
well-suited for systolic arrays. Moreover, the concurrent execution of matrix
multiplication on systolic arrays and softmax on vector units leads to register
file and SRAM port contention, further degrading performance.
  To overcome these limitations, we propose FSA, an enhanced systolic array
architecture that enables the entire FlashAttention algorithm to run entirely
within a single systolic array, eliminating the need for external vector units.
At the core of FSA is SystolicAttention, a novel scheduling algorithm that maps
FlashAttention operations onto systolic arrays with fine-grained, element-wise
overlap. This significantly improves array utilization while preserving the
original floating-point operation order to maintain numerical stability.
  We implement FSA in synthesizable RTL and evaluate its performance against
state-of-the-art commercial accelerators. Our results show that FSA achieves
1.77x and 4.83x higher attention FLOPs/s utilization compared to AWS
NeuronCore-v2 and Google TPUv5e, respectively, with only about 10% area
overhead.

</details>


### [21] [Elk: Exploring the Efficiency of Inter-core Connected AI Chips with Deep Learning Compiler Techniques](https://arxiv.org/abs/2507.11506)
*Yiqi Liu,Yuqi Xue,Noelle Crawford,Jilong Xue,Jian Huang*

Main category: cs.AR

TL;DR: Elk是一个深度学习编译器框架，旨在通过联合优化计算、通信和I/O性能，最大化ICCA芯片的效率。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型需求的增长，ICCA芯片面临计算、通信和I/O之间的性能权衡问题，需要一种系统化的优化方法。

Method: Elk通过可配置参数构建全局权衡空间，采用新的归纳运算符调度策略和成本感知的片上内存分配算法，生成全局优化的执行计划。

Result: Elk在ICCA芯片上实现了平均94%的理想性能，支持大型DL模型，并展示了架构设计空间探索的能力。

Conclusion: Elk通过联合优化性能因素，显著提升了ICCA芯片的效率，并为新芯片开发提供了设计空间探索工具。

Abstract: To meet the increasing demand of deep learning (DL) models, AI chips are
employing both off-chip memory (e.g., HBM) and high-bandwidth low-latency
interconnect for direct inter-core data exchange. However, it is not easy to
explore the efficiency of these inter-core connected AI (ICCA) chips, due to a
fundamental tussle among compute (per-core execution), communication
(inter-core data exchange), and I/O (off-chip data access).
  In this paper, we develop Elk, a DL compiler framework to maximize the
efficiency of ICCA chips by jointly trading off all the three performance
factors discussed above. Elk structures these performance factors into
configurable parameters and forms a global trade-off space in the DL compiler.
To systematically explore this space and maximize overall efficiency, Elk
employs a new inductive operator scheduling policy and a cost-aware on-chip
memory allocation algorithm. It generates globally optimized execution plans
that best overlap off-chip data loading and on-chip execution. To examine the
efficiency of Elk, we build a full-fledged emulator based on a real ICCA chip
IPU-POD4, and an ICCA chip simulator for sensitivity analysis with different
interconnect network topologies. Elk achieves 94% of the ideal roofline
performance of ICCA chips on average, showing the benefits of supporting large
DL models on ICCA chips. We also show Elk's capability of enabling architecture
design space exploration for new ICCA chip development.

</details>
