<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Optimism in Equality Saturation](https://arxiv.org/abs/2511.20782)
*Russel Arbore,Alvin Cheung,Max Willsey*

Main category: cs.PL

TL;DR: 提出了一种用于等式饱和的抽象解释算法，能够精确分析循环程序，在SSA程序上实现更精确的分析。


<details>
  <summary>Details</summary>
Motivation: 现有的e-class分析对循环程序分析悲观且无效，特别是在SSA形式的程序中。

Method: 基于抽象解释算法，结合非破坏性重写和乐观分析，在SSA程序上实现原型抽象解释器。

Result: 原型系统能够比clang和gcc更精确地分析简单示例程序。

Conclusion: 该算法统一了乐观分析和非破坏性重写，在循环程序分析方面表现出更好的精确性。

Abstract: Equality saturation is a technique for program optimization based on non-destructive rewriting and a form of program analysis called e-class analysis. The current form of e-class analysis is pessimistic and therefore ineffective at analyzing cyclic programs, such as those in SSA form. We propose an abstract interpretation algorithm that can precisely analyze cycles during equality saturation. This results in a unified algorithm for optimistic analysis and non-destructive rewriting. We instantiate this approach on a prototype abstract interpreter for SSA programs using a new semantics of SSA. Our prototype can analyze simple example programs more precisely than clang and gcc.

</details>


### [2] [Towards Computational UIP in Cubical Agda](https://arxiv.org/abs/2511.21209)
*Yee-Jian Tan,Andreas Nuyts,Dominique Devriese*

Main category: cs.PL

TL;DR: 本文探讨了在立方类型论中实现h-集合立方类型论的方法，分析了UIP（身份证明唯一性）的不同表述及其计算规则，并实现了一个无Glue类型的立方Agda变体。


<details>
  <summary>Details</summary>
Motivation: 立方类型论具有商归纳类型和函数外延性等优点，但HoTT的无限等式层次在形式化中可能变得繁琐。虽然截断到h-集合可以保留这些特性，但目前在立方Agda中实现h-集合立方类型论的方法都不够理想。

Method: 分析UIP的不同表述及其计算规则，评估它们在立方Agda中实现的适用性，并实现一个无Glue类型的立方Agda变体。

Result: 开发了与假设UIP兼容的无Glue立方Agda变体，为未来在立方Agda中实现UIP奠定了基础。

Conclusion: 本文为在立方Agda中实现h-集合立方类型论提供了理论基础和初步实现，解决了当前实现方法的不足。

Abstract: Some advantages of Cubical Type Theory, as implemented by Cubical Agda, over intensional Martin-Löf Type Theory include Quotient Inductive Types (QITs), which exist as instances of Higher Inductive Types, and functional extensionality, which is provable in Cubical Type Theory. However, HoTT features an infinite hierarchy of equalities that may become unwieldy in formalisations. Fortunately, QITs and functional extensionality are both preserved even if the equality levels of Cubical Type Theory are truncated to only homotopical Sets (h-Sets). In other words, removing the univalence axiom from Cubical Type Theory and instead postulating a conflicting axiom: the Uniqueness of Identity Proofs (UIP) postulate. Since univalence is proved in Cubical Type Theory from the so-called Glue Types, therefore, it is known that one can first remove the Glue Types (thus removing univalence) and then set-truncate all equalities (essentially assuming UIP), à la XTT. The result is a "h-Set Cubical Type Theory" that retains features such as functional extensionality and QITs.
  However, in Cubical Agda, there are currently only two unsatisfying ways to achieve h-Set Cubical Type Theory. The first is to give up on the canonicity of the theory and simply postulate the UIP axiom, while the second way is to use a standard result stating "type formers preserve h-levels" to manually prove UIP for every defined type. The latter is, however, laborious work best suited for an automatic implementation by the proof assistant. In this project, we analyse formulations of UIP and detail their computation rules for Cubical Agda, and evaluate their suitability for implementation. We also implement a variant of Cubical Agda without Glue, which is already compatible with postulated UIP, in anticipation of a future implementation of UIP in Cubical Agda.

</details>


### [3] [SV-LIB 1.0: A Standard Exchange Format for Software-Verification Tasks](https://arxiv.org/abs/2511.21509)
*Dirk Beyer,Gidon Ernst,Martin Jonáš,Marian Lingsch-Rosenfeld*

Main category: cs.PL

TL;DR: SV-LIB是一个用于软件验证任务的交换格式和中间语言，基于命令式编程语言概念，使用SMT-LIB表示表达式和类型，支持验证见证格式。


<details>
  <summary>Details</summary>
Motivation: 现有验证工具多为特定语言开发，但许多验证方法本质上是语言无关的。为了促进技术转移和工具复用，需要一种通用的交换格式。

Method: 提出SV-LIB格式，基于命令式编程语言概念，使用SMT-LIB表示表达式和类型，定义验证见证格式和见证验证任务规范。

Result: 开发了SV-LIB 1.0版本，包括设计目标、语法和非正式语义，支持正确和错误程序的验证见证。

Conclusion: SV-LIB为软件验证工具提供了语言无关的交换格式，便于工具集成和见证验证，并发扩展和形式语义将在未来版本中实现。

Abstract: In the past two decades, significant research and development effort went into the development of verification tools for individual languages, such asC, C++, and Java. Many of the used verification approaches are in fact language-agnostic and it would be beneficial for the technology transfer to allow for using the implementations also for other programming and modeling languages. To address the problem, we propose SV-LIB, an exchange format and intermediate language for software-verification tasks, including programs, specifications, and verification witnesses. SV-LIBis based on well-known concepts from imperative programming languages and uses SMT-LIB to represent expressions and sorts used in the program. This makes it easy to parse and to build into existing infrastructure, since many verification tools are based on SMT solvers already. Furthermore, SV-LIBdefines a witness format for both correct and incorrect SV-LIB programs, together with means for specifying witness-validation tasks. This makes it possible both to implement independent witness validators and to reuse some verifiers also as validators for witnesses. This paper presents version 1.0 of the SV-LIBformat, including its design goals, the syntax, and informal semantics. Formal semantics and further extensions to concurrency are planned for future versions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures](https://arxiv.org/abs/2511.20780)
*Alison Silva,Gustavo Callou*

Main category: cs.DC

TL;DR: 本文提出了一种基于随机Petri网的方法来分析私有云环境中Nextcloud文件服务器的可用性，评估了四种冗余策略对系统可靠性的影响。


<details>
  <summary>Details</summary>
Motivation: 随着云存储平台在学术和商业环境中的普及，可靠性成为关键需求，特别是对于寻求公共云服务替代方案的组织。评估这些系统的可靠性至关重要。

Method: 使用随机Petri网（SPNs）建模方法，在Apache CloudStack私有云环境中分析Nextcloud文件服务器的可用性，评估了四种架构配置：基线、主机级冗余、虚拟机冗余以及两者组合。

Result: 结果显示，在主机和虚拟机层面同时实施冗余策略能显著提高可用性并减少预期停机时间。

Conclusion: 所提出的方法为评估私有云可用性提供了有效手段，并支持基础设施设计决策。

Abstract: Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions.

</details>


### [5] [Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks](https://arxiv.org/abs/2511.20834)
*Dionysios Adamopoulos,Anastasia Poulopoulou,Georgios Goumas,Christina Giannoula*

Main category: cs.DC

TL;DR: Spira是一个专为GPU设计的稀疏卷积引擎，通过利用体素坐标的整数性、有界性和几何连续性等特性，显著提升了3D点云网络的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏卷积引擎未能充分利用体素坐标的特性，在核映射构建过程中存在高预处理和后处理开销。

Method: 提出一次性搜索算法构建核映射、打包原生处理方案、双数据流执行机制和网络级并行化策略。

Result: Spira在端到端推理中平均加速1.71倍，最高2.31倍；在逐层执行中平均加速2.13倍，最高3.32倍。

Conclusion: Spira通过充分利用体素坐标特性，显著提升了稀疏卷积的计算效率。

Abstract: Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work identifies three key properties of voxel coordinates: they are integer-valued, bounded within a limited spatial range, and geometrically continuous-neighboring voxels on the same object surface are highly likely to exist at small spatial offsets from each other. Prior SpC engines do not fully exploit these properties and suffer from high pre-processing and post-processing overheads during kernel map construction. To address this, we design Spira, the first voxel-property-aware SpC engine for GPUs. Spira proposes: (i) a high-performance one-shot search algorithm that builds the kernel map with no preprocessing and high memory locality, (ii) an effective packed-native processing scheme that accesses packed voxel coordinates at low cost, (iii) a flexible dual-dataflow execution mechanism that efficiently computes output feature vectors by adapting to layer characteristics, and (iv) a network-wide parallelization strategy that builds kernel maps for all SpC layers concurrently at network start. Our evaluation shows that Spira significantly outperforms prior SpC engines by 1.71x on average and up to 2.31x for end-to-end inference, and by 2.13x on average and up to 3.32x for layer-wise execution across diverse layer configurations.

</details>


### [6] [Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows](https://arxiv.org/abs/2511.20975)
*Yinwei Dai,Zhuofu Chen,Anand Iyer,Ravi Netravali*

Main category: cs.DC

TL;DR: Aragog系统通过动态调整工作流配置来优化多阶段Agent工作流的服务效率，在保持精度的同时显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的固定配置方法无法适应工作流执行期间系统负载的快速变化，导致配置很快变得次优，造成计算资源浪费。

Method: 将问题分解为一次性路由步骤识别所有保持精度的配置，以及基于实时系统观察的廉价每阶段调度器选择配置，并引入新策略加速每个步骤。

Result: 在多样化工作流和模型家族中，Aragog在峰值请求率下将最大服务吞吐量提高50.0-217.0%，中位延迟降低32.5-78.9%，同时保持与最昂贵配置相当的精度。

Conclusion: Aragog通过运行时动态配置调整有效解决了Agent工作流服务中的成本效率问题，实现了性能与精度的良好平衡。

Abstract: Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\% and reduces median latency by 32.5--78.9\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.

</details>


### [7] [A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving](https://arxiv.org/abs/2511.20982)
*Junhan Liao,Minxian Xu,Wanyi Zheng,Yan Wang,Kejiang Ye,Rajkumar Buyya,Chengzhong Xu*

Main category: cs.DC

TL;DR: DOPD是一个动态LLM推理系统，通过实时监控负载动态调整预填充和解码实例分配比例，解决异构工作负载下的生产者-消费者不平衡问题，显著提升系统吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当代LLM将预填充和解码阶段解耦到不同GPU以缓解各自瓶颈，但异构工作负载导致这种解耦架构中两种实例类型间的生产者-消费者不平衡。

Method: 提出DOPD系统，基于实时负载监控动态调整实例分配以实现最优的预填充/解码比例，结合适当的请求调度策略解决实例不平衡和资源分配不匹配问题。

Result: 相比vLLM和DistServe，DOPD将系统吞吐量提升最高1.5倍，P90首token时间降低67.5%，P90每输出token时间降低22.8%。

Conclusion: 动态P/D调整技术基于历史负载进行主动重配置，使用更少额外资源实现超过99%的SLO达成率。

Abstract: To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.

</details>


### [8] [Handling of Memory Page Faults during Virtual-Address RDMA](https://arxiv.org/abs/2511.21018)
*Antonis Psistakis*

Main category: cs.DC

TL;DR: 这篇论文实现了一个与DMA引擎集成的页面错误处理机制，通过硬件-软件协同方案解决RDMA通信中的页面错误问题，避免了传统内存固定方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统RDMA技术无法容忍页面错误，通常采用内存固定技术，但这带来了编程复杂性、内存使用限制和效率低下等问题，且无法完全防止页面错误。

Method: 在ExaNeSt项目中，通过ARM SMMU检测页面错误，开发硬件-软件解决方案，包括修改Linux SMMU驱动、开发新软件库、调整DMA引擎硬件和调度逻辑。

Result: 在ExaNeSt的QFDB平台上进行了实验评估，与内存固定和预错误处理等替代方案进行了比较。

Conclusion: 提出的页面错误处理机制相比传统方法具有优势，能够更有效地处理RDMA通信中的内存管理问题。

Abstract: Nowadays, avoiding system calls during cluster communication (e.g., in Data Centers and High Performance Computing) in modern high-speed interconnection networks has become a necessity, due to the high overhead of multiple data copies between kernel and user space. User-level zero-copy Remote Direct Memory Access (RDMA) technologies address this problem by improving performance and reducing system energy consumption. However, traditional RDMA engines cannot tolerate page faults and therefore use various techniques to avoid them.
  State-of-the-art RDMA approaches typically rely on pinning address spaces or multiple pages per application. This method introduces long-term disadvantages due to increased programming complexity (pinning and unpinning buffers), limits on how much memory can be pinned, and inefficient memory utilization. In addition, pinning does not fully prevent page faults because modern operating systems apply internal optimization mechanisms, such as Transparent Huge Pages (THP), which are enabled by default in Linux.
  This thesis implements a page-fault handling mechanism integrated with the DMA engine of the ExaNeSt project. Faults are detected by the ARM System Memory Management Unit (SMMU) and resolved through a hardware-software solution that can request retransmission when needed. This mechanism required modifications to the Linux SMMU driver, the development of a new software library, changes to the DMA engine hardware, and adjustments to the DMA scheduling logic. Experiments were conducted on the Quad-FPGA Daughter Board (QFDB) of ExaNeSt, which uses Xilinx Zynq UltraScale+ MPSoCs.
  Finally, we evaluate our mechanism and compare it against alternatives such as pinning and pre-faulting, and discuss the advantages of our approach.

</details>


### [9] [Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM](https://arxiv.org/abs/2511.21413)
*Tim Trappen,Robert Keßler,Roland Pabel,Viktor Achter,Stefan Wesner*

Main category: cs.DC

TL;DR: 提出了一种在超级计算机RAMSES上集成vLLM、Slurm和Kubernetes来服务LLM的解决方案，该架构在100、500和1000个并发请求下能高效扩展，端到端延迟仅增加约500毫秒。


<details>
  <summary>Details</summary>
Motivation: 由于AI推理需求增长，特别是在高等教育领域，需要利用现有基础设施的新解决方案。传统HPC操作模型不适用于同步、面向用户的动态AI应用工作负载。

Method: 在超级计算机RAMSES上集成vLLM、Slurm和Kubernetes来服务大型语言模型。

Result: 初始基准测试表明，该架构在100、500和1000个并发请求下能高效扩展，端到端延迟仅增加约500毫秒。

Conclusion: 提出的解决方案成功解决了传统HPC模型在AI推理应用中的局限性，实现了高效可扩展的LLM服务部署。

Abstract: Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.

</details>


### [10] [MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training](https://arxiv.org/abs/2511.21431)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Yueqiang Chen,Baoguo He,Hongfeng Sun,Ziqing Yin,Shangchao Su,Zhiyan Cui,Liang Dong,Xiyuan Li,Lingbin Wang,Jianwei He,Jiesong Ma,Weikang Huang,Jianglei Tong,Dongdong Gao,Jian Zhang,Hong Tian*

Main category: cs.DC

TL;DR: MemFine是一个内存感知的细粒度调度框架，通过分块重计算策略解决MoE训练中的内存瓶颈问题，在内存受限的GPU上实现稳定的大规模MoE训练。


<details>
  <summary>Details</summary>
Motivation: 大规模MoE模型训练面临严重的内存瓶颈，动态token路由导致的负载不均衡会造成GPU内存溢出，限制了模型的可扩展性。现有的负载均衡方法会损害模型精度且在内存受限硬件上失效。

Method: 将token分布和专家计算分解为可管理的块，采用分块重计算策略，通过理论内存模型动态优化以平衡内存效率和吞吐量。

Result: 相比完全重计算基线，MemFine减少激活内存48.03%，提高吞吐量4.42%，在内存受限GPU上实现稳定的大规模MoE训练。

Conclusion: MemFine框架有效解决了MoE训练的内存瓶颈问题，为在内存受限硬件上进行大规模MoE训练提供了可行方案。

Abstract: The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.

</details>


### [11] [Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation](https://arxiv.org/abs/2511.21535)
*Morteza Sadeghi*

Main category: cs.DC

TL;DR: 通过引入数据冗余改善MLFMA中P2P算子的GPU内存局部性，提出基于局部性度量的分析模型预测性能趋势，在电磁求解器和恒星动力学代码中验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: MLFMA中的近场(P2P)算子在GPU上由于内存局部性差成为性能瓶颈，需要改善内存访问模式。

Method: 引入数据冗余减少内存访问分散度，提出结合数据量和访问分散度的局部性度量分析模型，无需硬件特定分析即可预测加速趋势。

Result: 内核加速最高达7倍，但由于数据重构开销增加，端到端应用加速限制在1.04倍。模型能可靠捕捉不同问题规模和密度下的性能趋势。

Conclusion: 数据冗余可以提升GPU上P2P算子的性能，前提是局部性收益超过数据移动成本，该方法可最小化代码修改注入现有实现。

Abstract: The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.

</details>


### [12] [Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases](https://arxiv.org/abs/2511.21612)
*Shahir Abdullah,Syed Rohit Zaman*

Main category: cs.DC

TL;DR: 提出了Scaling Plane二维缩放模型，通过联合水平（节点数）和垂直（单节点资源）调整的"对角线缩放"策略，相比传统单向缩放能显著降低延迟、成本和重新平衡开销。


<details>
  <summary>Details</summary>
Motivation: 现代云数据库将缩放视为二元决策（水平或垂直），这种一维视图限制了性能优化，因为数据库性能、成本和协调开销来自水平和垂直资源的联合交互。

Method: 引入Scaling Plane二维模型，将数据库配置表示为(H,V)点，定义延迟、吞吐量、协调开销和成本的平滑近似；提出DIAGONALSCALE算法，在缩放平面中评估水平、垂直和对角线移动，选择满足SLA约束的最优配置。

Result: 对角线缩放相比仅水平或垂直自动缩放，可将p95延迟降低40%，每查询成本降低37%，重新平衡减少2-5倍。

Conclusion: 研究结果强调了对多维缩放模型的需求，为下一代云数据库系统自动缩放提供了基础。

Abstract: Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems.

</details>


### [13] [AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI](https://arxiv.org/abs/2511.21661)
*Beth Plale,Neelesh Karthikeyan,Isuru Gamage,Joe Stubbs,Sachith Withana*

Main category: cs.DC

TL;DR: 该研究评估了在Patra模型卡系统中采用模型上下文协议(MCP)作为接口的效益与权衡，比较了MCP与REST接口的开销，并探讨了MCP在动态模型卡上下文中的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统的AI/ML模型卡在训练时进行一次性评估，无法反映模型在实际使用过程中的动态变化。研究旨在探索模型卡作为动态对象在ICICLE AI研究所软件生态系统中的应用。

Method: 通过Patra模型卡系统，研究采用模型上下文协议(MCP)作为接口，并与REST接口进行定量比较，同时定性评估MCP在动态模型卡上下文中的适用性和使用情况。

Result: 定量评估显示MCP相比REST接口存在一定的开销。核心发现是MCP能够支持活跃会话，这在动态模型卡上下文中具有重要价值。

Conclusion: MCP作为Patra模型卡服务器的接口，虽然在性能上存在一定开销，但在支持动态模型卡的活跃会话方面具有优势，适合在需要持续监控模型使用情况的场景中应用。

Abstract: AI/ML model cards can contain a benchmarked evaluation of an AI/ML model against intended use but a one time assessment during model training does not get at how and where a model is actually used over its lifetime. Through Patra Model Cards embedded in the ICICLE AI Institute software ecosystem we study model cards as dynamic objects. The study reported here assesses the benefits and tradeoffs of adopting the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative assessment shows the overhead of MCP as compared to a REST interface. The core question however is of active sessions enabled by MCP; this is a qualitative question of fit and use in the context of dynamic model cards that we address as well.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI](https://arxiv.org/abs/2511.21232)
*Muhammed Yildirim,Ozcan Ozturk*

Main category: cs.AR

TL;DR: 提出一种用于边缘AI的硬件加速器架构，采用融合像素级数据流，消除中间缓冲区需求，在RISC-V处理器上实现高达59.3倍加速和87%数据移动减少。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI应用中深度可分离卷积的多阶段设计带来的内存墙问题，即中间特征图传输的高能耗和延迟成本。

Method: 设计基于融合像素级数据流的硬件加速器架构，作为RISC-V处理器的定制功能单元，通过紧密耦合流水线在DSC所有阶段（扩展、深度卷积、投影）中完成单个输出像素计算，无需写入内存。

Result: 在Xilinx Artix-7 FPGA上实现59.3倍加速；ASIC合成显示28nm工艺下0.284mm²面积、910mW功耗（2GHz），40nm工艺下1.20mm²面积、233mW功耗（300MHz）。

Conclusion: 验证了在TinyML资源约束内实现零缓冲区数据流的可行性，为边缘AI加速器克服内存墙提供了新颖有效的策略。

Abstract: The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.

</details>


### [15] [Bombyx: OpenCilk Compilation for FPGA Hardware Acceleration](https://arxiv.org/abs/2511.21346)
*Mohamed Shahawy,Julien de Castelnau,Paolo Ienne*

Main category: cs.AR

TL;DR: Bombyx是一个编译器工具链，将OpenCilk程序转换为Cilk-1风格的中间表示，使CPU导向的任务级并行应用能高效映射到FPGA空间架构上。


<details>
  <summary>Details</summary>
Motivation: 现有系统在FPGA上支持任务级并行时，OpenCilk的隐式任务模型需要昂贵的硬件上下文切换，而Cilk-1的显式延续传递模型更适合FPGA的流式特性。

Method: 开发Bombyx编译器工具链，支持多个编译目标：OpenCilk兼容运行时和可综合处理单元生成器，并引入解耦访问执行优化。

Result: 能够自动生成高性能处理单元，改善内存计算重叠和整体吞吐量。

Conclusion: Bombyx成功将CPU导向的任务级并行应用映射到FPGA空间架构，通过Cilk-1模型和优化技术提升了性能。

Abstract: Task-level parallelism (TLP) is a widely used approach in software where independent tasks are dynamically created and scheduled at runtime. Recent systems have explored architectural support for TLP on field-programmable gate arrays (FPGAs), often leveraging high-level synthesis (HLS) to create processing elements (PEs). In this paper, we present Bombyx, a compiler toolchain that lowers OpenCilk programs into a Cilk-1-inspired intermediate representation, enabling efficient mapping of CPU-oriented TLP applications to spatial architectures on FPGAs. Unlike OpenCilk's implicit task model, which requires costly context switching in hardware, Cilk-1 adopts explicit continuation-passing - a model that better aligns with the streaming nature of FPGAs. Bombyx supports multiple compilation targets: one is an OpenCilk-compatible runtime for executing Cilk-1-style code using the OpenCilk backend, and another is a synthesizable PE generator designed for HLS tools like Vitis HLS. Additionally, we introduce a decoupled access-execute optimization that enables automatic generation of high-performance PEs, improving memory-compute overlap and overall throughput.

</details>


### [16] [A Jammer-Resilient 2.87 mm$^2$ 1.28 MS/s 310 mW Multi-Antenna Synchronization ASIC in 65 nm](https://arxiv.org/abs/2511.21451)
*Flurin Arquint,Oscar Castañeda,Gian Marti,Christoph Studer*

Main category: cs.AR

TL;DR: 首个抗干扰多天线时间同步ASIC实现，支持单天线发射器与16天线接收器同步，可抵御最多2天线智能干扰器。


<details>
  <summary>Details</summary>
Motivation: 解决无线通信系统中时间同步信号易受智能干扰攻击的问题，确保在干扰环境下保持可靠的时间同步。

Method: 采用多天线处理算法，在65nm工艺上设计ASIC芯片，实现抗干扰同步功能，支持1.28 MS/s采样率。

Result: 芯片核心面积2.87 mm²，功耗310 mW，成功实现抗干扰时间同步功能。

Conclusion: 该ASIC证明了多天线处理在硬件层面实现抗干扰时间同步的可行性，为安全无线通信系统提供了硬件基础。

Abstract: We present the first ASIC implementation of jammer-resilient multi-antenna time synchronization. The ASIC implements a recent algorithm that mitigates jamming attacks on synchronization signals using multi-antenna processing. Our design supports synchronization between a single-antenna transmitter and a 16-antenna receiver while mitigating smart jammers with up to two transmit antennas. The fabricated 65 nm ASIC has a core area of 2.87 mm$^2$, consumes a power of 310 mW, and supports a sampling rate of 1.28 mega-samples per second (MS/s).

</details>


### [17] [A 0.32 mm$^2$ 100 Mb/s 223 mW ASIC in 22FDX for Joint Jammer Mitigation, Channel Estimation, and SIMO Data Detection](https://arxiv.org/abs/2511.21461)
*Jonas Elmiger,Fabian Stuber,Oscar Castañeda,Gian Marti,Christoph Studer*

Main category: cs.AR

TL;DR: 首个SIMO接收器ASIC芯片，集成抗干扰、信道估计和数据检测功能，采用MAED算法对抗智能干扰器，在22nm FD-SOI工艺下实现100Mb/s吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统接收器在智能干扰器攻击下性能严重下降，需要开发能同时处理干扰抑制、信道估计和数据检测的集成解决方案。

Method: 采用MAED算法，通过非线性优化问题实现空间滤波，统一处理干扰器估计与消除、信道估计和数据检测。设计支持8个接收天线。

Result: 在22nm FD-SOI工艺下实现，核心面积0.32mm²，功耗223mW，吞吐量100Mb/s。相比现有抗干扰检测器，用户吞吐量提升3倍，面积效率提升4.5倍。

Conclusion: 该ASIC成功展示了MAED算法的硬件实现可行性，为抗干扰通信系统提供了高效的集成解决方案。

Abstract: We present the first single-input multiple-output (SIMO) receiver ASIC that jointly performs jammer mitigation, channel estimation, and data detection. The ASIC implements a recent algorithm called siMultaneous mitigAtion, Estimation, and Detection (MAED). MAED mitigates smart jammers via spatial filtering using a nonlinear optimization problem that unifies jammer estimation and nulling, channel estimation, and data detection to achieve state-of-the-art error-rate performance under jamming. The design supports eight receive antennas and enables mitigation of smart jammers as well as of barrage jammers. The ASIC is fabricated in 22 nm FD-SOI, has a core area of 0.32 mm$^2$, and achieves a throughput of 100 Mb/s at 223 mW, thus delivering 3$\times$ higher per-user throughput and 4.5$\times$ higher area efficiency than the state-of-the-art jammer-resilient detector.

</details>


### [18] [Modeling and Optimizing Performance Bottlenecks for Neuromorphic Accelerators](https://arxiv.org/abs/2511.21549)
*Jason Yik,Walter Gallego Gomez,Andrew Cheng,Benedetto Leto,Alessandro Pierro,Noah Pacik-Nelson,Korneel Van den Berghe,Vittorio Fra,Andreea Danielescu,Gianvito Urgese,Vijay Janapa Reddi*

Main category: cs.AR

TL;DR: 本文首次对神经形态加速器进行全面的性能边界和瓶颈分析，揭示了传统指标的不足，提出了floorline性能模型和优化方法，在保持准确率不变的情况下实现了3.86倍运行时间改进和3.38倍能耗降低。


<details>
  <summary>Details</summary>
Motivation: 神经形态加速器具有独特的架构特性，其性能动态与传统加速器有根本差异。现有的工作负载优化方法依赖聚合网络稀疏度和操作计数，但这些指标对实际部署性能的改善程度未知。

Method: 采用理论分析建模和实证表征相结合的方法，研究了三种真实的神经形态加速器：Brainchip AKD1000、Synsense Speck和Intel Loihi 2。建立了floorline性能模型，并提出结合稀疏感知训练和floorline指导分区的优化方法。

Result: 确定了三种不同的加速器瓶颈状态：内存受限、计算受限和流量受限，并识别了可能表现出这些瓶颈状态的工作负载配置特征。优化方法在保持准确率不变的情况下，实现了最高3.86倍的运行时间改进和3.38倍的能耗降低。

Conclusion: 神经形态加速器的性能优化需要超越传统的稀疏度和操作计数指标，floorline性能模型能够有效识别性能边界并指导工作负载优化，为神经形态计算的实际部署提供了重要指导。

Abstract: Neuromorphic accelerators offer promising platforms for machine learning (ML) inference by leveraging event-driven, spatially-expanded architectures that naturally exploit unstructured sparsity through co-located memory and compute. However, their unique architectural characteristics create performance dynamics that differ fundamentally from conventional accelerators. Existing workload optimization approaches for neuromorphic accelerators rely on aggregate network-wide sparsity and operation counting, but the extent to which these metrics actually improve deployed performance remains unknown. This paper presents the first comprehensive performance bound and bottleneck analysis of neuromorphic accelerators, revealing the shortcomings of the conventional metrics and offering an understanding of what facets matter for workload performance. We present both theoretical analytical modeling and extensive empirical characterization of three real neuromorphic accelerators: Brainchip AKD1000, Synsense Speck, and Intel Loihi 2. From these, we establish three distinct accelerator bottleneck states, memory-bound, compute-bound, and traffic-bound, and identify which workload configuration features are likely to exhibit these bottleneck states. We synthesize all of our insights into the floorline performance model, a visual model that identifies performance bounds and informs how to optimize a given workload, based on its position on the model. Finally, we present an optimization methodology that combines sparsity-aware training with floorline-informed partitioning. Our methodology achieves substantial performance improvements at iso-accuracy: up to 3.86x runtime improvement and 3.38x energy reduction compared to prior manually-tuned configurations.

</details>
