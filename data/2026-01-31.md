<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Optimal Software Pipelining using an SMT-Solver](https://arxiv.org/abs/2601.21842)
*Jan-Willem Roorda*

Main category: cs.PL

TL;DR: 基于SMT求解器的最优软件流水线方法，显著优于启发式算法和手工优化，并能提供不可行性反馈


<details>
  <summary>Details</summary>
Motivation: 软件流水线是VLIW处理器的重要循环优化技术，传统方法使用启发式算法，但存在优化效果有限的问题，需要更优的解决方案

Method: 使用可满足性模理论(SMT)求解器构建最优软件流水线调度器，将调度问题转化为约束求解问题

Result: 该方法显著优于传统启发式算法和手工优化，并能向程序员和处理器设计者提供关于特定启动间隔不可行的反馈信息

Conclusion: 基于SMT求解器的最优软件流水线方法是有效的，不仅能获得更好的性能，还能提供有价值的调试和设计反馈

Abstract: Software Pipelining is a classic and important loop-optimization for VLIW processors. It improves instruction-level parallelism by overlapping multiple iterations of a loop and executing them in parallel. Typically, it is implemented using heuristics. In this paper, we present an optimal software pipeliner based on a Satisfiability Modulo Theories (SMT) Solver. We show that our approach significantly outperforms heuristic algorithms and hand-optimization. Furthermore, we show how the solver can be used to give feedback to programmers and processor designers on why a software pipelined schedule of a certain initiation interval is not feasible.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Deep Reinforcement Learning for Fault-Adaptive Routing in Eisenstein-Jacobi Interconnection Topologies](https://arxiv.org/abs/2601.21090)
*Mohammad Walid Charrwi,Zaid Hussain*

Main category: cs.DC

TL;DR: 该论文评估了三种路由算法在故障EJ网络中的表现：贪婪自适应路由、Dijkstra算法和强化学习方法。强化学习方法在故障环境下表现最佳，达到94%可达性和91%数据包投递率，接近理论最优性能。


<details>
  <summary>Details</summary>
Motivation: 随着多核架构密度增加，需要高性能且容错的互连网络。EJ网络具有优越的拓扑特性，但在故障条件下传统路由启发式方法面临挑战，需要评估不同路由方法在故障环境下的表现。

Method: 评估三种路由范式：确定性贪婪自适应路由、理论最优的Dijkstra算法，以及基于强化学习的方法。使用多目标奖励函数惩罚接近故障节点并奖励路径效率，RL代理学习在故障集群周围导航。

Result: 在9个故障节点情况下，贪婪路由性能严重下降至10%可达性和数据包投递率；Dijkstra算法证明52-54%是拓扑最优值；RL方法达到94%可达性和91%数据包投递率。吞吐量评估显示RL在所有负载下维持超过90%的归一化吞吐量。

Conclusion: 基于强化学习的自适应策略是实用解决方案，在贪婪路由的效率和Dijkstra算法的最优性之间架起桥梁，为故障易发互连网络提供鲁棒的自愈通信，无需全局拓扑知识或最优算法的计算开销。

Abstract: The increasing density of many-core architectures necessitates interconnection networks that are both high-performance and fault-resilient. Eisenstein-Jacobi (EJ) networks, with their symmetric 6-regular topology, offer superior topological properties but challenge traditional routing heuristics under fault conditions. This paper evaluates three routing paradigms in faulty EJ environments: deterministic Greedy Adaptive Routing, theoretically optimal Dijkstra's algorithm, and a reinforcement learning (RL)-based approach. Using a multi-objective reward function to penalize fault proximity and reward path efficiency, the RL agent learns to navigate around clustered failures that typically induce dead-ends in greedy geometric routing. Dijkstra's algorithm establishes the theoretical performance ceiling by computing globally optimal paths with complete topology knowledge, revealing the true connectivity limits of faulty networks. Quantitative analysis at nine faulty nodes shows greedy routing catastrophically degrades to 10% effective reachability and packet delivery, while Dijkstra proves 52-54% represents the topological optimum. The RL agent achieves 94% effective reachability and 91% packet delivery, making it suitable for distributed deployment. Furthermore, throughput evaluations demonstrate that RL sustains over 90% normalized throughput across all loads, actually outperforming Dijkstra under congestion through implicit load balancing strategies. These results establish RL-based adaptive policies as a practical solution that bridges the gap between greedy's efficiency and Dijkstra's optimality, providing robust, self-healing communication in fault-prone interconnection networks without requiring the global topology knowledge or computational overhead of optimal algorithms.

</details>


### [3] [Maxwait: A Generalized Mechanism for Distributed Time-Sensitive Systems](https://arxiv.org/abs/2601.21146)
*Francesco Paladino,Shulu Li,Edward A. Lee*

Main category: cs.DC

TL;DR: maxwait是一种简单的协调机制，在分布式时间敏感系统中平衡时序要求和一致性，涵盖多种经典分布式系统方法并支持实时行为。


<details>
  <summary>Details</summary>
Motivation: 分布式时间敏感系统需要在通信延迟和同步不确定性的情况下，平衡时序要求（可用性）和一致性。现有方法缺乏统一的语义框架来明确配置这些权衡。

Method: 提出maxwait协调机制，作为Lingua Franca协调语言的扩展实现。该机制在通信延迟有界时强制逻辑时间一致性，在边界被违反时提供结构化故障处理。

Result: maxwait机制能够涵盖PTIDES、Chandy-and-Misra（带或不带空消息）、Jefferson's Time-Warp、Lamport时间故障检测等经典方法，并能实现LET、发布订阅、actor、CRDT、RPC with futures等常见分布式模式。

Conclusion: maxwait提供了一个统一的语义框架，不仅包含多种现有分布式系统方法，还增加了更好的时序控制、有界时间故障检测和确定性选项，适用于分布式信息物理应用。

Abstract: Distributed time-sensitive systems must balance timing requirements (availability) and consistency in the presence of communication delays and synchronization uncertainty. This paper presents maxwait, a simple coordination mechanism with surprising generality that makes these tradeoffs explicit and configurable. We demonstrate that this mechanism subsumes classical distributed system methods such as PTIDES, Chandy-and-Misra with or without null messages, Jefferson's Time-Warp, and Lamport's time-based fault detection, while enabling real-time behavior in distributed cyber-physical applications. The mechanism can also realize many commonly used distributed system patterns, including logical execution time (LET), publish and subscribe, actors, conflict-free replicated data types (CRDTs), and remote procedure calls with futures. More importantly, it adds to these mechanisms better control over timing, bounded time fault detection, and the option of making them more deterministic, all within a single semantic framework. Implemented as an extension of the Lingua Franca coordination language, maxwait enforces logical-time consistency when communication latencies are bounded and provides structured fault handling when bounds are violated.

</details>


### [4] [ZipMoE: Efficient On-Device MoE Serving via Lossless Compression and Cache-Affinity Scheduling](https://arxiv.org/abs/2601.21198)
*Yuchen Yang,Yaru Zhao,Pu Yang,Shaowei Wang,Zhi-Hua Zhou*

Main category: cs.DC

TL;DR: ZipMoE：一种高效的语义无损边缘设备MoE服务系统，通过缓存调度协同设计将推理从I/O瓶颈转为计算中心工作流，显著降低延迟并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然增强了大型语言模型的表达能力，但其巨大的内存占用严重阻碍了在资源受限的边缘设备上的实际部署，尤其是在需要保持模型行为而不依赖有损量化的情况下。

Method: ZipMoE通过缓存调度协同设计，利用边缘设备硬件特性与MoE参数统计冗余之间的协同作用，将边缘设备MoE推理范式从I/O瓶颈转变为支持高效并行化的计算中心工作流。

Result: 在代表性边缘计算平台上使用开源MoE模型和真实工作负载进行实验，ZipMoE相比最先进系统实现了高达72.77%的推理延迟降低和高达6.76倍的吞吐量提升。

Conclusion: ZipMoE通过创新的缓存调度协同设计，有效解决了边缘设备上MoE模型部署的内存瓶颈问题，实现了语义无损的高效推理服务。

Abstract: While Mixture-of-Experts (MoE) architectures substantially bolster the expressive power of large-language models, their prohibitive memory footprint severely impedes the practical deployment on resource-constrained edge devices, especially when model behavior must be preserved without relying on lossy quantization. In this paper, we present ZipMoE, an efficient and semantically lossless on-device MoE serving system. ZipMoE exploits the synergy between the hardware properties of edge devices and the statistical redundancy inherent to MoE parameters via a caching-scheduling co-design with provable performance guarantee. Fundamentally, our design shifts the paradigm of on-device MoE inference from an I/O-bound bottleneck to a compute-centric workflow that enables efficient parallelization. We implement a prototype of ZipMoE and conduct extensive experiments on representative edge computing platforms using popular open-source MoE models and real-world workloads. Our evaluation reveals that ZipMoE achieves up to $72.77\%$ inference latency reduction and up to $6.76\times$ higher throughput than the state-of-the-art systems.

</details>


### [5] [Ira: Efficient Transaction Replay for Distributed Systems](https://arxiv.org/abs/2601.21286)
*Adithya Bhat,Harshal Bhadreshkumar Shah,Mohsen Minaei*

Main category: cs.DC

TL;DR: Ira框架通过传输紧凑提示来加速主备复制中的备份重放，在以太坊案例中实现25倍加速


<details>
  <summary>Details</summary>
Motivation: 主备复制中，共识延迟受限于备份节点重放主节点交易的时间。主节点已执行交易，拥有未来访问模式的知识，这正是最优重放所需的信息。

Method: 提出Ira框架，传输紧凑提示与交易批次一起。具体实现Ira-L协议，主节点提供包含以太坊区块中使用的键工作集和每个键一字节元数据的提示，备份使用这些提示进行高效区块重放。

Result: 提示紧凑，每个区块中位数增加47KB压缩数据（约5%区块负载）。主节点开销28.6%，其中直接提示成本10.9%。备份端实现中位数每区块25倍加速，16个预取线程下总重放时间从6.5小时降至16分钟（23.6倍加速）。

Conclusion: Ira框架通过传输主节点执行知识作为提示，显著加速备份重放，在以太坊环境中证明有效，为共识协议性能提升提供新途径。

Abstract: In primary-backup replication, consensus latency is bounded by the time for backup nodes to replay (re-execute) transactions proposed by the primary. In this work, we present Ira, a framework to accelerate backup replay by transmitting compact \emph{hints} alongside transaction batches. Our key insight is that the primary, having already executed transactions, possesses knowledge of future access patterns which is exactly the information needed for optimal replay.
  We use Ethereum for our case study and present a concrete protocol, Ira-L, within our framework to improve cache management of Ethereum block execution. The primaries implementing Ira-L provide hints that consist of the working set of keys used in an Ethereum block and one byte of metadata per key indicating the table to read from, and backups use these hints for efficient block replay.
  We evaluated Ira-L against the state-of-the-art Ethereum client reth over two weeks of Ethereum mainnet activity ($100,800$ blocks containing over $24$ million transactions). Our hints are compact, adding a median of $47$ KB compressed per block ($\sim5\%$ of block payload). We observe that the sequential hint generation and block execution imposes a $28.6\%$ wall-time overhead on the primary, though the direct cost from hints is $10.9\%$ of execution time; all of which can be pipelined and parallelized in production deployments. On the backup side, we observe that Ira-L achieves a median per-block speedup of $25\times$ over baseline reth. With $16$ prefetch threads, aggregate replay time drops from $6.5$ hours to $16$ minutes ($23.6\times$ wall-time speedup).

</details>


### [6] [EWSJF: An Adaptive Scheduler with Hybrid Partitioning for Mixed-Workload LLM Inference](https://arxiv.org/abs/2601.21758)
*Bronislav Sidik,Chaya Levi,Joseph Kampeas*

Main category: cs.DC

TL;DR: EWSJF：一种基于有效工作负载的自适应最短作业优先调度器，用于混合工作负载下的LLM服务，通过实时学习工作负载结构，显著提升吞吐量和降低短请求延迟


<details>
  <summary>Details</summary>
Motivation: 在混合工作负载（短延迟敏感交互查询+长吞吐量导向批处理请求）下服务LLM存在调度挑战，标准的FCFS策略存在严重的队头阻塞问题，导致高尾延迟和硬件利用率不足

Method: EWSJF包含四个组件：1) Refine-and-Prune无监督分区算法发现性能同质请求组；2) Dynamic Queue Routing将请求分配到这些组；3) Density-Weighted Scoring上下文感知优先级函数平衡紧急性和公平性；4) Bayesian Meta-Optimization基于实时性能反馈持续调优评分和分区参数

Result: 在vLLM中实现，相比FCFS，端到端吞吐量提升超过30%，短请求的平均首次令牌时间减少高达4倍

Conclusion: 自适应、基于学习的请求调度是高效响应LLM服务的关键缺失层，EWSJF通过实时学习工作负载结构，在公平性和吞吐量方面实现联合改进

Abstract: Serving Large Language Models (LLMs) under mixed workloads--short, latency-sensitive interactive queries alongside long, throughput-oriented batch requests--poses a fundamental scheduling challenge. Standard First-Come, First-Served (FCFS) policies suffer from severe head-of-line blocking, leading to high tail latency and underutilized hardware. We introduce EWSJF (Effective Workload-based Shortest Job First), an adaptive request-level scheduler that learns workload structure in real time to jointly improve fairness and throughput. EWSJF operates upstream of execution-level schedulers and integrates four components: (1) Refine-and-Prune, an unsupervised partitioning algorithm that discovers performance-homogeneous request groups; (2) Dynamic Queue Routing for assigning requests to these groups; (3) Density-Weighted Scoring, a context-aware prioritization function balancing urgency and fairness; and (4) Bayesian Meta-Optimization, which continuously tunes scoring and partitioning parameters based on live performance feedback. Implemented in vLLM, EWSJF improves end-to-end throughput by over 30% and reduces average Time-To-First-Token for short requests by up to 4x compared to FCFS. These results demonstrate that adaptive, learning-based request scheduling is a critical missing layer for efficient and responsive LLM serving. Implementation available at https://anonymous.4open.science/r/vllm_0110-32D8.

</details>


### [7] [Self-Adaptive Probabilistic Skyline Query Processing in Distributed Edge Computing via Deep Reinforcement Learning](https://arxiv.org/abs/2601.21855)
*Chuan-Chi Lai*

Main category: cs.DC

TL;DR: SA-PSKY是一个用于边缘云协作系统的自适应概率天际线查询框架，通过深度强化学习动态优化过滤阈值，显著降低通信开销和响应时间。


<details>
  <summary>Details</summary>
Motivation: 在万物互联时代，边缘传感器数据爆炸式增长使得高效的概率天际线查询处理成为关键挑战。传统分布式方法依赖静态阈值过滤，无法适应边缘计算环境的高度动态和异构特性，导致通信瓶颈或计算延迟问题。

Method: 提出SA-PSKY框架，将动态阈值调整问题形式化为连续马尔可夫决策过程，利用深度确定性策略梯度智能体实时自主优化过滤强度。通过分析多维系统状态（数据到达率、不确定性分布、资源可用性）来最小化计算和通信成本的联合目标函数。

Result: 实验评估表明SA-PSKY持续优于最先进的静态和启发式基线方法，通信开销降低高达60%，总响应时间减少40%，并在不同数据分布下保持稳健的可扩展性。

Conclusion: SA-PSKY通过自适应阈值调整有效解决了边缘计算环境中资源冲突问题，为分布式概率天际线查询提供了高效、可扩展的解决方案。

Abstract: In the era of the Internet of Everything (IoE), the exponential growth of sensor-generated data at the network edge renders efficient Probabilistic Skyline Query (PSKY) processing a critical challenge. Traditional distributed PSKY methodologies predominantly rely on pre-defined static thresholds to filter local candidates. However, these rigid approaches are fundamentally ill-suited for the highly volatile and heterogeneous nature of edge computing environments, often leading to either severe communication bottlenecks or excessive local computational latency. To resolve this resource conflict, this paper presents SA-PSKY, a novel Self-Adaptive framework designed for distributed edge-cloud collaborative systems. We formalize the dynamic threshold adjustment problem as a continuous Markov Decision Process (MDP) and leverage a Deep Deterministic Policy Gradient (DDPG) agent to autonomously optimize filtering intensities in real-time. By intelligently analyzing multi-dimensional system states, including data arrival rates, uncertainty distributions, and instantaneous resource availability, our framework effectively minimizes a joint objective function of computation and communication costs. Comprehensive experimental evaluations demonstrate that SA-PSKY consistently outperforms state-of-the-art static and heuristic baselines. Specifically, it achieves a reduction of up to 60\% in communication overhead and 40\% in total response time, while ensuring robust scalability across diverse data distributions.

</details>


### [8] [Belief Propagation Converges to Gaussian Distributions in Sparsely-Connected Factor Graphs](https://arxiv.org/abs/2601.21935)
*Tom Yates,Yuzhou Cheng,Ignacio Alzugaray,Danyal Akarca,Pedro A. M. Mediano,Andrew J. Davison*

Main category: cs.DC

TL;DR: 论文为高斯信念传播（GBP）在高度非高斯、稀疏连接因子图中提供理论保证，证明在满足4个关键假设下，变量信念会收敛到高斯分布。


<details>
  <summary>Details</summary>
Motivation: GBP在实际应用中表现出色（如计算机视觉、传感器网络），即使处理非高斯问题时也有效，但缺乏理论支持。需要为GBP在高度非高斯、稀疏连接因子图中的有效性提供理论保证。

Method: 利用中心极限定理（CLT）进行数学证明，证明在满足4个关键假设的复杂环状因子图中，BP下的变量信念会收敛到高斯分布。并通过立体深度估计任务进行实验验证。

Result: 理论证明：在满足4个关键假设的稀疏连接因子图中，变量信念会收敛到高斯分布。实验验证：在立体深度估计任务中，仅经过几次BP迭代后，变量信念就变得越来越高斯。

Conclusion: 为GBP在高度非高斯、稀疏连接因子图中的有效性提供了理论保证，解释了为什么GBP在实际应用中即使处理非高斯问题也能表现良好，填补了理论与实证之间的差距。

Abstract: Belief Propagation (BP) is a powerful algorithm for distributed inference in probabilistic graphical models, however it quickly becomes infeasible for practical compute and memory budgets. Many efficient, non-parametric forms of BP have been developed, but the most popular is Gaussian Belief Propagation (GBP), a variant that assumes all distributions are locally Gaussian. GBP is widely used due to its efficiency and empirically strong performance in applications like computer vision or sensor networks - even when modelling non-Gaussian problems. In this paper, we seek to provide a theoretical guarantee for when Gaussian approximations are valid in highly non-Gaussian, sparsely-connected factor graphs performing BP (common in spatial AI). We leverage the Central Limit Theorem (CLT) to prove mathematically that variables' beliefs under BP converge to a Gaussian distribution in complex, loopy factor graphs obeying our 4 key assumptions. We then confirm experimentally that variable beliefs become increasingly Gaussian after just a few BP iterations in a stereo depth estimation task.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [FireFly-P: FPGA-Accelerated Spiking Neural Network Plasticity for Robust Adaptive Control](https://arxiv.org/abs/2601.21222)
*Tenglong Li,Jindong Li,Guobin Shen,Dongcheng Zhao,Qian Zhang,Yi Zeng*

Main category: cs.AR

TL;DR: FireFly-P是一个基于FPGA的硬件加速器，实现了新型可塑性算法，用于实时自适应控制，具有8μs端到端延迟和0.713W功耗。


<details>
  <summary>Details</summary>
Motivation: 利用脉冲神经网络（SNNs）的生物可塑性机制实现无监督自适应，避免反向传播的计算开销，为机器人提供实时自适应控制能力。

Method: 设计FPGA硬件加速器FireFly-P，实现新型可塑性算法，利用片上可塑性增强网络泛化能力，在Cmod A7-35T FPGA上实现。

Result: 实现了8μs的端到端延迟（推理和可塑性更新），功耗仅0.713W，占用约10K LUTs，在动态非结构化环境中表现鲁棒。

Conclusion: 硬件加速的SNN可塑性是实现自适应、低延迟、高能效控制系统的可行路径，特别适合资源受限的嵌入式机器人平台。

Abstract: Spiking Neural Networks (SNNs) offer a biologically plausible learning mechanism through synaptic plasticity, enabling unsupervised adaptation without the computational overhead of backpropagation. To harness this capability for robotics, this paper presents FireFly-P, an FPGA-based hardware accelerator that implements a novel plasticity algorithm for real-time adaptive control. By leveraging on-chip plasticity, our architecture enhances the network's generalization, ensuring robust performance in dynamic and unstructured environments. The hardware design achieves an end-to-end latency of just 8~$μ$s for both inference and plasticity updates, enabling rapid adaptation to unseen scenarios. Implemented on a tiny Cmod A7-35T FPGA, FireFly-P consumes only 0.713~W and $\sim$10K~LUTs, making it ideal for power- and resource-constrained embedded robotic platforms. This work demonstrates that hardware-accelerated SNN plasticity is a viable path toward enabling adaptive, low-latency, and energy-efficient control systems.

</details>


### [10] [Frequency as Aperture: Enabling Embeddable Near-Field Sensing for 6G Wireless Radios](https://arxiv.org/abs/2601.21584)
*Pin-Han Ho,Limei Peng,Yiming Miao,Xu Fan,Kairan Liang,Haoran Mei,Wei Duan*

Main category: cs.AR

TL;DR: FaA是一种无线优先的感知范式，通过将频率敏捷性转化为虚拟感知孔径，实现近场感知，显著降低射频前端复杂度，为6G ISAC提供硬件高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前毫米波感知大多依赖专用雷达硬件，与成本、功耗受限的无线节点不兼容。未来6G需要原生支持ISAC，但现有方案在硬件效率和成本方面存在挑战。

Method: 提出频率即孔径(FaA)方法：利用单个射频链和频率扫描漏波天线，将本地振荡器频率扫描（已用于宽带通信）重用于二维空间感知，将空间采样从天线域转移到频域。

Result: FaA在相同物理和频谱约束下，比传统多通道MIMO感知具有更高的架构效率，提供精细的角度和距离分辨能力，同时保持低功耗和单位成本。

Conclusion: 近场感知可以无缝集成到频率敏捷的无线射频中，为智能家居、可穿戴设备和工业边缘部署提供硬件高效、可嵌入且保护隐私的ISAC节点。

Abstract: Integrated sensing and communication (ISAC) is expected to be natively supported by future 6G wireless radios, yet most mmWave sensing solutions still rely on dedicated radar hardware incompatible with cost and power constrained wireless nodes. This article introduces Frequency-as-Aperture (FaA), a wireless-first sensing paradigm that repurposes inherent frequency agility into a virtual sensing aperture, enabling near-field perception with minimal RF front end complexity. Using a single RF chain and a frequency-scanning leaky-wave antenna, FaA achieves two dimensional spatial sensing by reusing the local oscillator (LO) frequency sweep already employed for wideband communication. From a wireless-system perspective, this shifts spatial sampling from the antenna domain to the frequency domain, embedding radar-grade spatial fingerprints directly into the communication RF chain. A case study shows that FaA provides fine angular and range discrimination with low power consumption and unit cost, demonstrating significantly higher architectural efficiency than conventional multi-channel MIMO based sensing under identical physical and spectral constraints. These results indicate that near-field sensing can be seamlessly integrated into frequency-agile wireless radios, enabling hardware-efficient, embeddable, and privacy-preserving ISAC nodes for smart homes, wearables, and industrial edge deployments.

</details>
