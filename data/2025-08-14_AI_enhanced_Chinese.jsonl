{"id": "2508.09505", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09505", "abs": "https://arxiv.org/abs/2508.09505", "authors": ["Zhanghan Wang", "Ding Ding", "Hang Zhu", "Haibin Lin", "Aurojit Panda"], "title": "Verify Distributed Deep Learning Model Implementation Refinement with Iterative Relation Inference", "comment": null, "summary": "Distributed machine learning training and inference is common today because\ntoday's large models require more memory and compute than can be provided by a\nsingle GPU. Distributed models are generally produced by programmers who take a\nsequential model specification and apply several distribution strategies to\ndistribute state and computation across GPUs. Unfortunately, bugs can be\nintroduced in the process, and a distributed model implementation's outputs\nmight differ from the sequential model's outputs. In this paper, we describe an\napproach to statically identify such bugs by checking model refinement, that\nis, can the sequential model's outputs be reconstructed from the distributed\nmodel's outputs? Our approach, implemented in GraphGuard, uses iterative\nrewriting to prove model refinement. Our approach can scale to today's large\nmodels and deployments: we evaluate it using GPT and Llama-3. Further, it\nprovides actionable output that aids in bug localization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9759\u6001\u68c0\u6d4b\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0e\u987a\u5e8f\u6a21\u578b\u8f93\u51fa\u5dee\u5f02\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u67e5\u6a21\u578b\u7ec6\u5316\u6765\u8bc6\u522b\u6f5c\u5728\u9519\u8bef\u3002", "motivation": "\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5b9e\u73b0\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u5f15\u5165\u9519\u8bef\uff0c\u5bfc\u81f4\u8f93\u51fa\u4e0e\u987a\u5e8f\u6a21\u578b\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u4e00\u79cd\u9759\u6001\u5206\u6790\u65b9\u6cd5\u6765\u8bc6\u522b\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528GraphGuard\u5de5\u5177\uff0c\u901a\u8fc7\u8fed\u4ee3\u91cd\u5199\u6280\u672f\u9a8c\u8bc1\u6a21\u578b\u7ec6\u5316\uff0c\u5373\u80fd\u5426\u4ece\u5206\u5e03\u5f0f\u6a21\u578b\u8f93\u51fa\u91cd\u5efa\u987a\u5e8f\u6a21\u578b\u8f93\u51fa\u3002", "result": "\u65b9\u6cd5\u9002\u7528\u4e8e\u5927\u578b\u6a21\u578b\uff08\u5982GPT\u548cLlama-3\uff09\uff0c\u5e76\u80fd\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u9519\u8bef\u5b9a\u4f4d\u4fe1\u606f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u5206\u5e03\u5f0f\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u9519\u8bef\uff0c\u5e76\u5e2e\u52a9\u5f00\u53d1\u8005\u5b9a\u4f4d\u95ee\u9898\u3002"}}
{"id": "2508.09591", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09591", "abs": "https://arxiv.org/abs/2508.09591", "authors": ["Wenxiang Lin", "Xinglin Pan", "Lin Zhang", "Shaohuai Shi", "Xuan Wang", "Xiaowen Chu"], "title": "HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap", "comment": null, "summary": "The sparsely activated mixture-of-experts (MoE) transformer has become a\ncommon architecture for large language models (LLMs) due to its sparsity, which\nrequires fewer computational demands while easily scaling the model size. In\nMoE models, each MoE layer requires to dynamically choose tokens to activate\nparticular experts for computation while the activated experts may not be\nlocated in the same device or GPU as the token. However, this leads to\nsubstantial communication and load imbalances across all GPUs, which obstructs\nthe scalability of distributed systems within a GPU cluster. To this end, we\nintroduce HierMoE to accelerate the training of MoE models by two\ntopology-aware techniques: 1) token deduplication to reduce the communication\ntraffic, and 2) expert swap to balance the workloads among all GPUs. To enable\nthe above two proposed approaches to be more general, we build theoretical\nmodels aimed at achieving the best token duplication and expert swap strategy\nunder different model configurations and hardware environments. We implement\nour prototype HierMoE system atop Megatron-LM and conduct experiments on a\n32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results\nshow that our HierMoE achieves $1.55\\times$ to $3.32\\times$ faster\ncommunication and delivers $1.18\\times$ to $1.27\\times$ faster end-to-end\ntraining compared to state-of-the-art MoE training systems, Tutel-2DH,\nSmartMoE, and Megatron-LM.", "AI": {"tldr": "HierMoE\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u6280\u672f\uff08\u4ee4\u724c\u53bb\u91cd\u548c\u4e13\u5bb6\u4ea4\u6362\uff09\u52a0\u901fMoE\u6a21\u578b\u8bad\u7ec3\uff0c\u63d0\u5347\u901a\u4fe1\u548c\u8d1f\u8f7d\u5747\u8861\u3002", "motivation": "\u89e3\u51b3MoE\u6a21\u578b\u4e2d\u56e0\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u5bfc\u81f4\u7684\u901a\u4fe1\u548c\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u63d0\u5347\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u62d3\u6251\u611f\u77e5\u6280\u672f\uff1a\u4ee4\u724c\u53bb\u91cd\u51cf\u5c11\u901a\u4fe1\u6d41\u91cf\uff0c\u4e13\u5bb6\u4ea4\u6362\u5e73\u8861GPU\u8d1f\u8f7d\uff0c\u5e76\u5efa\u7acb\u7406\u8bba\u6a21\u578b\u4f18\u5316\u7b56\u7565\u3002", "result": "\u572832-GPU\u96c6\u7fa4\u4e0a\u5b9e\u9a8c\uff0cHierMoE\u901a\u4fe1\u901f\u5ea6\u63d0\u53471.55\u00d7\u81f33.32\u00d7\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53471.18\u00d7\u81f31.27\u00d7\u3002", "conclusion": "HierMoE\u663e\u8457\u63d0\u5347MoE\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002"}}
{"id": "2508.09663", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.09663", "abs": "https://arxiv.org/abs/2508.09663", "authors": ["Philipp A. Friese", "Ahmed Eleliemy", "Utz-Uwe Haus", "Martin Schulz"], "title": "Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for Kubernetes", "comment": "10 pages, 12 figures, 1 table, 3 listings, to be published in IEEE\n  Cluster 2025", "summary": "Converged HPC-Cloud computing is an emerging computing paradigm that aims to\nsupport increasingly complex and multi-tenant scientific workflows. These\nsystems require reconciliation of the isolation requirements of native cloud\nworkloads and the performance demands of HPC applications. In this context,\nnetworking hardware is a critical boundary component: it is the conduit for\nhigh-throughput, low-latency communication and enables isolation across\ntenants. HPE Slingshot is a high-speed network interconnect that provides up to\n200 Gbps of throughput per port and targets high-performance computing (HPC)\nsystems. The Slingshot host software, including hardware drivers and network\nmiddleware libraries, is designed to meet HPC deployments, which predominantly\nuse single-tenant access modes. Hence, the Slingshot stack is not suited for\nsecure use in multi-tenant deployments, such as converged HPC-Cloud\ndeployments. In this paper, we design and implement an extension to the\nSlingshot stack targeting converged deployments on the basis of Kubernetes. Our\nintegration provides secure, container-granular, and multi-tenant access to\nSlingshot RDMA networking capabilities at minimal overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9HPE Slingshot\u7f51\u7edc\u7684\u6269\u5c55\u8bbe\u8ba1\uff0c\u4ee5\u652f\u6301\u591a\u79df\u6237\u7684HPC-\u4e91\u8ba1\u7b97\u878d\u5408\u90e8\u7f72\uff0c\u901a\u8fc7Kubernetes\u5b9e\u73b0\u5b89\u5168\u4e14\u4f4e\u5f00\u9500\u7684\u5bb9\u5668\u7ea7\u8bbf\u95ee\u3002", "motivation": "HPC-\u4e91\u8ba1\u7b97\u878d\u5408\u9700\u8981\u6ee1\u8db3\u9ad8\u6027\u80fd\u548c\u9694\u79bb\u6027\u9700\u6c42\uff0c\u4f46\u73b0\u6709Slingshot\u7f51\u7edc\u6808\u4ec5\u652f\u6301\u5355\u79df\u6237\u6a21\u5f0f\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u79df\u6237\u73af\u5883\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u57fa\u4e8eKubernetes\u7684Slingshot\u7f51\u7edc\u6808\u6269\u5c55\uff0c\u63d0\u4f9b\u5bb9\u5668\u7ea7\u7684\u591a\u79df\u6237\u5b89\u5168\u8bbf\u95ee\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9Slingshot RDMA\u7f51\u7edc\u529f\u80fd\u7684\u5b89\u5168\u3001\u591a\u79df\u6237\u8bbf\u95ee\uff0c\u4e14\u5f00\u9500\u6781\u4f4e\u3002", "conclusion": "\u8be5\u6269\u5c55\u6210\u529f\u89e3\u51b3\u4e86HPC-\u4e91\u8ba1\u7b97\u878d\u5408\u4e2d\u7f51\u7edc\u9694\u79bb\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2508.09856", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.09856", "abs": "https://arxiv.org/abs/2508.09856", "authors": ["Mathieu Boespflug", "Arnaud Spiwack"], "title": "Invertible Syntax without the Tuples (Functional Pearl)", "comment": null, "summary": "In the seminal paper Functional unparsing, Olivier Danvy used continuation\npassing to reanalyse printf-like format strings as combinators. In the\nintervening decades, the conversation shifted towards a concurrent line of work\n-- applicative, monadic or arrow-based combinator libraries -- in an effort to\nfind combinators for invertible syntax descriptions that simultaneously\ndetermine a parser as well as a printer, and with more expressive power, able\nto handle inductive structures such as lists and trees. Along the way,\ncontinuation passing got lost. This paper argues that Danvy's insight remains\nas relevant to the general setting as it was to the restricted setting of his\noriginal paper. Like him, we present three solutions that exploit\ncontinuation-passing style as an alternative to both dependent types and\nmonoidal aggregation via nested pairs, in our case to parse and print\nstructured data with increasing expressive power.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u63a2\u8ba8\u4e86Danvy\u7684\u5ef6\u7eed\u4f20\u9012\u98ce\u683c\uff08CPS\uff09\u5728\u89e3\u6790\u548c\u6253\u5370\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86CPS\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u91cd\u65b0\u8bc4\u4f30Danvy\u7684CPS\u65b9\u6cd5\u5728\u73b0\u4ee3\u7ec4\u5408\u5e93\uff08\u5982\u5e94\u7528\u5f0f\u3001\u5355\u5b50\u6216\u7bad\u5934\uff09\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u53ef\u9006\u8bed\u6cd5\u63cf\u8ff0\u65f6\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u57fa\u4e8eCPS\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u6790\u548c\u6253\u5370\u7ed3\u6784\u5316\u6570\u636e\uff0c\u907f\u514d\u4e86\u4f9d\u8d56\u7c7b\u578b\u548c\u5355\u5b50\u805a\u5408\u7684\u590d\u6742\u6027\u3002", "result": "\u5c55\u793a\u4e86CPS\u5728\u5904\u7406\u5217\u8868\u548c\u6811\u7b49\u5f52\u7eb3\u7ed3\u6784\u65f6\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u73b0\u4ee3\u7ec4\u5408\u5e93\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "CPS\u65b9\u6cd5\u5728\u89e3\u6790\u548c\u6253\u5370\u7ed3\u6784\u5316\u6570\u636e\u4e2d\u4ecd\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4e3a\u73b0\u4ee3\u7ec4\u5408\u5e93\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u6d01\u4e14\u5f3a\u5927\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.09570", "categories": ["cs.AR", "B.3.0; B.6.0"], "pdf": "https://arxiv.org/pdf/2508.09570", "abs": "https://arxiv.org/abs/2508.09570", "authors": ["Xiangfeng Liu", "Zhe Jiang", "Anzhen Zhu", "Xiaomeng Han", "Mingsong Lyu", "Qingxu Deng", "Nan Guan"], "title": "Re-thinking Memory-Bound Limitations in CGRAs", "comment": "25 pages, 18 figures, CODES+ISSS 2025", "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7c97\u7c92\u5ea6\u53ef\u91cd\u6784\u9635\u5217\uff08CGRA\uff09\u7684\u6539\u8fdb\u5185\u5b58\u5b50\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fd0\u884c\u524d\u6267\u884c\u673a\u5236\u548c\u7f13\u5b58\u91cd\u914d\u7f6e\u6280\u672f\uff0c\u6709\u6548\u5904\u7406\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CGRA\u7814\u7a76\u901a\u5e38\u5047\u8bbe\u5185\u6838\u53ef\u4ee5\u4eceScratchpad Memory\uff08SPM\uff09\u8bbf\u95ee\u6240\u6709\u6570\u636e\uff0c\u4f46\u590d\u6742\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5982\u56fe\u5206\u6790\u3001\u4e0d\u89c4\u5219\u6570\u636e\u5e93\u64cd\u4f5c\uff09\u7684\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u5bfc\u81f4CGRA\u5229\u7528\u7387\u6781\u4f4e\uff0c\u751a\u81f3\u4f4e\u4e8e1.5%\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u5185\u5b58\u5b50\u7cfb\u7edf\u5e76\u4f18\u5316\u5185\u5b58\u6a21\u578b\uff0c\u7ed3\u5408\u8fd0\u884c\u524d\u6267\u884c\u673a\u5236\u548c\u7f13\u5b58\u91cd\u914d\u7f6e\u6280\u672f\uff0c\u89e3\u51b3\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\u95ee\u9898\u3002", "result": "\u6539\u8fdb\u540e\u7684\u7cfb\u7edf\u6027\u80fd\u4e0e\u539f\u59cbSPM-only\u7cfb\u7edf\u76f8\u5f53\uff0c\u4ec5\u97001.27%\u7684\u5b58\u50a8\u7a7a\u95f4\uff1b\u8fd0\u884c\u524d\u6267\u884c\u673a\u5236\u5e73\u5747\u63d0\u901f3.04\u500d\uff08\u6700\u9ad86.91\u500d\uff09\uff0c\u7f13\u5b58\u91cd\u914d\u7f6e\u6280\u672f\u989d\u5916\u63d0\u53476.02%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86CGRA\u5728\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u4e0b\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u7684\u5c40\u9650\u6027\u3002"}}
