{"id": "2601.00450", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.00450", "abs": "https://arxiv.org/abs/2601.00450", "authors": ["Elham Cheshmikhani", "Hamed Farbeh", "Hossein Asadi"], "title": "Enhancing Reliability of STT-MRAM Caches by Eliminating Read Disturbance Accumulation", "comment": null, "summary": "Spin-Transfer Torque Magnetic RAM (STT-MRAM) as one of the most promising replacements for SRAMs in on-chip cache memories benefits from higher density and scalability, near-zero leakage power, and non-volatility, but its reliability is threatened by high read disturbance error rate. Error-Correcting Codes (ECCs) are conventionally suggested to overcome the read disturbance errors in STT-MRAM caches. By employing aggressive ECCs and checking out a cache block on every read access, a high level of cache reliability is achieved. However, to minimize the cache access time in modern processors, all blocks in the target cache set are simultaneously read in parallel for tags comparison operation and only the requested block is sent out, if any, after checking its ECC. These extra cache block reads without checking their ECCs until requesting the blocks by the processor cause the accumulation of read disturbance error, which significantly degrade the cache reliability. In this paper, we first introduce and formulate the read disturbance accumulation phenomenon and reveal that this accumulation due to conventional parallel accesses of cache blocks significantly increases the cache error rate. Then, we propose a simple yet effective scheme, so-called Read Error Accumulation Preventer cache (REAP-cache), to completely eliminate the accumulation of read disturbances without compromising the cache performance. Our evaluations show that the proposed REAP-cache extends the cache Mean Time To Failure (MTTF) by 171x, while increases the cache area by less than 1% and energy consumption by only 2.7%.", "AI": {"tldr": "\u63d0\u51faREAP-cache\u65b9\u6848\uff0c\u901a\u8fc7\u6d88\u9664\u5e76\u884c\u8bbf\u95ee\u65f6\u7684\u8bfb\u5e72\u6270\u7d2f\u79ef\uff0c\u663e\u8457\u63d0\u5347STT-MRAM\u7f13\u5b58\u7684\u53ef\u9760\u6027\uff0c\u5c06MTTF\u63d0\u9ad8171\u500d\uff0c\u4ec5\u589e\u52a0\u5c11\u91cf\u9762\u79ef\u548c\u80fd\u8017\u3002", "motivation": "STT-MRAM\u4f5c\u4e3aSRAM\u66ff\u4ee3\u54c1\u5177\u6709\u9ad8\u5bc6\u5ea6\u3001\u4f4e\u529f\u8017\u7b49\u4f18\u52bf\uff0c\u4f46\u5b58\u5728\u9ad8\u8bfb\u5e72\u6270\u9519\u8bef\u7387\u95ee\u9898\u3002\u4f20\u7edfECC\u65b9\u6848\u5728\u5e76\u884c\u8bbf\u95ee\u7f13\u5b58\u65f6\uff0c\u672a\u8bf7\u6c42\u7684\u5757\u5728ECC\u68c0\u67e5\u524d\u4f1a\u7d2f\u79ef\u8bfb\u5e72\u6270\u9519\u8bef\uff0c\u4e25\u91cd\u964d\u4f4e\u7f13\u5b58\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faREAP-cache\u65b9\u6848\uff0c\u901a\u8fc7\u9632\u6b62\u8bfb\u5e72\u6270\u7d2f\u79ef\u6765\u5b8c\u5168\u6d88\u9664\u8be5\u95ee\u9898\u3002\u65b9\u6848\u7b80\u5355\u6709\u6548\uff0c\u5728\u4e0d\u5f71\u54cd\u7f13\u5b58\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u89e3\u51b3\u4e86\u5e76\u884c\u8bbf\u95ee\u65f6\u672a\u68c0\u67e5ECC\u5757\u7684\u9519\u8bef\u7d2f\u79ef\u95ee\u9898\u3002", "result": "REAP-cache\u5c06\u7f13\u5b58\u5e73\u5747\u6545\u969c\u65f6\u95f4\uff08MTTF\uff09\u63d0\u9ad8171\u500d\uff0c\u540c\u65f6\u4ec5\u589e\u52a0\u4e0d\u52301%\u7684\u7f13\u5b58\u9762\u79ef\u548c2.7%\u7684\u80fd\u8017\u6d88\u8017\u3002", "conclusion": "REAP-cache\u65b9\u6848\u80fd\u6709\u6548\u89e3\u51b3STT-MRAM\u7f13\u5b58\u4e2d\u7684\u8bfb\u5e72\u6270\u7d2f\u79ef\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u53ef\u9760\u6027\uff0c\u4e14\u786c\u4ef6\u5f00\u9500\u6781\u5c0f\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.00456", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.00456", "abs": "https://arxiv.org/abs/2601.00456", "authors": ["Elham Cheshmikhani", "Hamed Farbeh", "Hossein Asadi"], "title": "ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches", "comment": null, "summary": "Spin-Transfer Torque Magnetic RAM} (STT-MRAM) is a promising alternative for SRAMs in on-chip cache memories. Besides all its advantages, high error rate in STT-MRAM is a major limiting factor for on-chip cache memories. In this paper, we first present a comprehensive analysis that reveals that the conventional Error-Correcting Codes (ECCs) lose their efficiency due to data-dependent error patterns, and then propose an efficient ECC configuration, so-called ROBIN, to improve the correction capability. The evaluations show that the inefficiency of conventional ECC increases the cache error rate by an average of 151.7% while ROBIN reduces this value by more than 28.6x.", "AI": {"tldr": "\u9488\u5bf9STT-MRAM\u7f13\u5b58\u4e2d\u6570\u636e\u4f9d\u8d56\u6027\u9519\u8bef\u6a21\u5f0f\uff0c\u63d0\u51faROBIN ECC\u914d\u7f6e\u65b9\u6848\uff0c\u76f8\u6bd4\u4f20\u7edfECC\u663e\u8457\u63d0\u5347\u7ea0\u9519\u80fd\u529b", "motivation": "STT-MRAM\u4f5c\u4e3a\u7247\u4e0a\u7f13\u5b58\u6709\u524d\u666f\uff0c\u4f46\u9ad8\u9519\u8bef\u7387\u662f\u4e3b\u8981\u9650\u5236\u56e0\u7d20\u3002\u4f20\u7edfECC\u56e0\u6570\u636e\u4f9d\u8d56\u6027\u9519\u8bef\u6a21\u5f0f\u800c\u6548\u7387\u964d\u4f4e\uff0c\u9700\u8981\u6539\u8fdb\u7ea0\u9519\u80fd\u529b", "method": "\u9996\u5148\u5168\u9762\u5206\u6790\u4f20\u7edfECC\u6548\u7387\u4e0b\u964d\u539f\u56e0\uff0c\u7136\u540e\u63d0\u51fa\u540d\u4e3aROBIN\u7684\u9ad8\u6548ECC\u914d\u7f6e\u65b9\u6848\uff0c\u4e13\u95e8\u9488\u5bf9\u6570\u636e\u4f9d\u8d56\u6027\u9519\u8bef\u6a21\u5f0f\u8fdb\u884c\u4f18\u5316", "result": "\u8bc4\u4f30\u663e\u793a\u4f20\u7edfECC\u4f4e\u6548\u4f7f\u7f13\u5b58\u9519\u8bef\u7387\u5e73\u5747\u589e\u52a0151.7%\uff0c\u800cROBIN\u5c06\u6b64\u503c\u964d\u4f4e\u8d85\u8fc728.6\u500d\uff0c\u663e\u8457\u63d0\u5347\u7ea0\u9519\u80fd\u529b", "conclusion": "ROBIN ECC\u914d\u7f6e\u80fd\u6709\u6548\u89e3\u51b3STT-MRAM\u4e2d\u6570\u636e\u4f9d\u8d56\u6027\u9519\u8bef\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u7f13\u5b58\u9519\u8bef\u7387\uff0c\u4e3aSTT-MRAM\u5728\u7247\u4e0a\u7f13\u5b58\u5e94\u7528\u63d0\u4f9b\u53ef\u884c\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.00380", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00380", "abs": "https://arxiv.org/abs/2601.00380", "authors": ["Hanzhe Li", "Bingchen Lin", "Mengyuan Xu"], "title": "Word Frequency Counting Based on Serverless MapReduce", "comment": "6 pages, 4 figures, International Conference on Engineering Management, Information Technology and Intelligence (EMITI 2024)", "summary": "With the increasing demand for high-performance and high-efficiency computing, cloud computing, especially serverless computing, has gradually become a research hotspot in recent years, attracting numerous research attention. Meanwhile, MapReduce, which is a popular big data processing model in the industry, has been widely applied in various fields. Inspired by the serverless framework of Function as a Service and the high concurrency and robustness of MapReduce programming model, this paper focus on combining them to reduce the time span and increase the efficiency when executing the word frequency counting task. In this case, the paper use a MapReduce programming model based on a serverless computing platform to figure out the most optimized number of Map functions and Reduce functions for a particular task. For the same amount of workload, extensive experiments show both execution time reduces and the overall efficiency of the program improves at different rates as the number of map functions and reduce functions increases. This paper suppose the discovery of the most optimized number of map and reduce functions can help cooperations and programmers figure out the most optimized solutions.", "AI": {"tldr": "\u672c\u6587\u7ed3\u5408Serverless\u8ba1\u7b97\u548cMapReduce\u6a21\u578b\u4f18\u5316\u8bcd\u9891\u7edf\u8ba1\u4efb\u52a1\uff0c\u901a\u8fc7\u5b9e\u9a8c\u786e\u5b9a\u6700\u4f18\u7684Map\u548cReduce\u51fd\u6570\u6570\u91cf\u4ee5\u63d0\u9ad8\u6267\u884c\u6548\u7387", "motivation": "\u968f\u7740\u9ad8\u6027\u80fd\u8ba1\u7b97\u9700\u6c42\u589e\u957f\uff0cServerless\u8ba1\u7b97\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\uff0c\u800cMapReduce\u4f5c\u4e3a\u5927\u6570\u636e\u5904\u7406\u6a21\u578b\u5df2\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408Serverless\u7684FaaS\u6846\u67b6\u548cMapReduce\u7684\u9ad8\u5e76\u53d1\u3001\u9c81\u68d2\u6027\u4f18\u52bf\uff0c\u4f18\u5316\u8bcd\u9891\u7edf\u8ba1\u4efb\u52a1\u7684\u6267\u884c\u65f6\u95f4\u548c\u6548\u7387", "method": "\u91c7\u7528\u57fa\u4e8eServerless\u8ba1\u7b97\u5e73\u53f0\u7684MapReduce\u7f16\u7a0b\u6a21\u578b\uff0c\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff08\u8bcd\u9891\u7edf\u8ba1\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u63a2\u7d22\u6700\u4f18\u7684Map\u51fd\u6570\u548cReduce\u51fd\u6570\u6570\u91cf\u914d\u7f6e", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u968f\u7740Map\u548cReduce\u51fd\u6570\u6570\u91cf\u7684\u589e\u52a0\uff0c\u76f8\u540c\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u6267\u884c\u65f6\u95f4\u51cf\u5c11\uff0c\u7a0b\u5e8f\u6574\u4f53\u6548\u7387\u4ee5\u4e0d\u540c\u901f\u7387\u63d0\u5347\u3002\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u4e86\u6700\u4f18\u7684Map\u548cReduce\u51fd\u6570\u914d\u7f6e", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Serverless MapReduce\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u8bcd\u9891\u7edf\u8ba1\u4efb\u52a1\u7684\u6267\u884c\u6548\u7387\uff0c\u6700\u4f18Map/Reduce\u51fd\u6570\u6570\u91cf\u7684\u53d1\u73b0\u6709\u52a9\u4e8e\u4f01\u4e1a\u548c\u7a0b\u5e8f\u5458\u627e\u5230\u6700\u4f18\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.00397", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00397", "abs": "https://arxiv.org/abs/2601.00397", "authors": ["Amey Agrawal", "Mayank Yadav", "Sukrit Kumar", "Anirudha Agrawal", "Garv Ghai", "Souradeep Bera", "Elton Pinto", "Sirish Gambhira", "Mohammad Adain", "Kasra Sohrab", "Chus Antonanzas", "Alexey Tumanov"], "title": "Revati: Transparent GPU-Free Time-Warp Emulation for LLM Serving", "comment": null, "summary": "Deploying LLMs efficiently requires testing hundreds of serving configurations, but evaluating each one on a GPU cluster takes hours and costs thousands of dollars. Discrete-event simulators are faster and cheaper, but they require re-implementing the serving system's control logic -- a burden that compounds as frameworks evolve.\n  We present Revati, a time-warp emulator that enables performance modeling by directly executing real serving system code at simulation-like speed. The system intercepts CUDA API calls to virtualize device management, allowing serving frameworks to run without physical GPUs. Instead of executing GPU kernels, it performs time jumps -- fast-forwarding virtual time by predicted kernel durations. We propose a coordination protocol that synchronizes these jumps across distributed processes while preserving causality. On vLLM and SGLang, Revati achieves less than 5% prediction error across multiple models and parallelism configurations, while running 5-17x faster than real GPU execution.", "AI": {"tldr": "Revati\u662f\u4e00\u4e2a\u65f6\u95f4\u626d\u66f2\u4eff\u771f\u5668\uff0c\u901a\u8fc7\u76f4\u63a5\u6267\u884c\u771f\u5b9e\u670d\u52a1\u7cfb\u7edf\u4ee3\u7801\u5b9e\u73b0\u6027\u80fd\u5efa\u6a21\uff0c\u65e0\u9700\u7269\u7406GPU\uff0c\u6bd4\u771f\u5b9eGPU\u6267\u884c\u5feb5-17\u500d\uff0c\u9884\u6d4b\u8bef\u5dee\u5c0f\u4e8e5%", "motivation": "\u90e8\u7f72LLM\u9700\u8981\u6d4b\u8bd5\u6570\u767e\u79cd\u670d\u52a1\u914d\u7f6e\uff0c\u4f46\u5728GPU\u96c6\u7fa4\u4e0a\u8bc4\u4f30\u6bcf\u4e2a\u914d\u7f6e\u9700\u8981\u6570\u5c0f\u65f6\u548c\u6570\u5343\u7f8e\u5143\u6210\u672c\u3002\u79bb\u6563\u4e8b\u4ef6\u4eff\u771f\u5668\u867d\u7136\u66f4\u5feb\u66f4\u4fbf\u5b9c\uff0c\u4f46\u9700\u8981\u91cd\u65b0\u5b9e\u73b0\u670d\u52a1\u7cfb\u7edf\u7684\u63a7\u5236\u903b\u8f91\uff0c\u968f\u7740\u6846\u67b6\u6f14\u8fdb\u8d1f\u62c5\u52a0\u91cd\u3002", "method": "Revati\u662f\u4e00\u4e2a\u65f6\u95f4\u626d\u66f2\u4eff\u771f\u5668\uff0c\u901a\u8fc7\u62e6\u622aCUDA API\u8c03\u7528\u6765\u865a\u62df\u5316\u8bbe\u5907\u7ba1\u7406\uff0c\u5141\u8bb8\u670d\u52a1\u6846\u67b6\u5728\u6ca1\u6709\u7269\u7406GPU\u7684\u60c5\u51b5\u4e0b\u8fd0\u884c\u3002\u7cfb\u7edf\u4e0d\u6267\u884cGPU\u5185\u6838\uff0c\u800c\u662f\u6267\u884c\u65f6\u95f4\u8df3\u8dc3\u2014\u2014\u901a\u8fc7\u9884\u6d4b\u7684\u5185\u6838\u6301\u7eed\u65f6\u95f4\u5feb\u901f\u63a8\u8fdb\u865a\u62df\u65f6\u95f4\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u8c03\u534f\u8bae\uff0c\u5728\u5206\u5e03\u5f0f\u8fdb\u7a0b\u4e2d\u540c\u6b65\u8fd9\u4e9b\u65f6\u95f4\u8df3\u8dc3\uff0c\u540c\u65f6\u4fdd\u6301\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u5728vLLM\u548cSGLang\u4e0a\uff0cRevati\u5728\u591a\u4e2a\u6a21\u578b\u548c\u5e76\u884c\u914d\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u5c0f\u4e8e5%\u7684\u9884\u6d4b\u8bef\u5dee\uff0c\u540c\u65f6\u8fd0\u884c\u901f\u5ea6\u6bd4\u771f\u5b9eGPU\u6267\u884c\u5feb5-17\u500d\u3002", "conclusion": "Revati\u901a\u8fc7\u76f4\u63a5\u6267\u884c\u771f\u5b9e\u670d\u52a1\u7cfb\u7edf\u4ee3\u7801\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u7684\u6027\u80fd\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4eff\u771f\u5668\u9700\u8981\u91cd\u65b0\u5b9e\u73b0\u63a7\u5236\u903b\u8f91\u7684\u95ee\u9898\uff0c\u4e3aLLM\u670d\u52a1\u914d\u7f6e\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2601.00530", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.00530", "abs": "https://arxiv.org/abs/2601.00530", "authors": ["Ravi Teja Pagidoju"], "title": "Cost-Performance Analysis of Cloud-Based Retail Point-of-Sale Systems: A Comparative Study of Google Cloud Platform and Microsoft Azure", "comment": "Accepted at 38th International Conference on Computer Applications in Industry and Engineering (CAINE 2025)", "summary": "Althoughthereislittleempiricalresearchonplatform-specific performance for retail workloads, the digital transformation of the retail industry has accelerated the adoption of cloud-based Point-of-Sale (POS) systems. This paper presents a systematic, repeatable comparison of POS workload deployments on Google Cloud Platform (GCP) and Microsoft Azure using real-time API endpoints and open-source benchmarking code. Using free-tier cloud resources, we offer a transparent methodology for POS workload evaluation that small retailers and researchers can use. Our approach measures important performance metrics like response latency, throughput, and scalability while estimating operational costs based on actual resource usage and current public cloud pricing because there is no direct billing under free-tier usage. All the tables and figures in this study are generated directly from code outputs, ensuring that the experimental data and the reported results are consistent. Our analysis shows that GCP achieves 23.0% faster response times at baseline load, while Azure shows 71.9% higher cost efficiency for steady-state operations. We look at the architectural components that lead to these differences and provide a helpful framework for merchants considering cloud point-of-sale implementation. This study establishes a strong, open benchmarking methodology for retail cloud applications and offers the first comprehensive, code-driven comparison of workloads unique to point-of-sale systems across leading cloud platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u3001\u53ef\u91cd\u590d\u7684\u4e91POS\u7cfb\u7edf\u6027\u80fd\u6bd4\u8f83\u65b9\u6cd5\uff0c\u5728GCP\u548cAzure\u4e0a\u6d4b\u8bd5\u96f6\u552e\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u53d1\u73b0GCP\u54cd\u5e94\u65f6\u95f4\u5feb23%\uff0cAzure\u6210\u672c\u6548\u7387\u9ad871.9%\u3002", "motivation": "\u96f6\u552e\u4e1a\u6570\u5b57\u5316\u8f6c\u578b\u52a0\u901f\u4e86\u57fa\u4e8e\u4e91\u7684POS\u7cfb\u7edf\u91c7\u7528\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u96f6\u552e\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5e73\u53f0\u7279\u5b9a\u6027\u80fd\u5b9e\u8bc1\u7814\u7a76\uff0c\u7279\u522b\u662f\u5c0f\u578b\u96f6\u552e\u5546\u548c\u7814\u7a76\u4eba\u5458\u53ef\u7528\u7684\u900f\u660e\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u514d\u8d39\u5c42\u4e91\u8d44\u6e90\uff0c\u901a\u8fc7\u5b9e\u65f6API\u7aef\u70b9\u548c\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u4ee3\u7801\uff0c\u5728GCP\u548cAzure\u4e0a\u90e8\u7f72POS\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u6d4b\u91cf\u54cd\u5e94\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u3001\u53ef\u6269\u5c55\u6027\u7b49\u6027\u80fd\u6307\u6807\uff0c\u5e76\u6839\u636e\u5b9e\u9645\u8d44\u6e90\u4f7f\u7528\u548c\u5f53\u524d\u516c\u6709\u4e91\u5b9a\u4ef7\u4f30\u7b97\u8fd0\u8425\u6210\u672c\u3002", "result": "GCP\u5728\u57fa\u51c6\u8d1f\u8f7d\u4e0b\u5b9e\u73b023.0%\u66f4\u5feb\u7684\u54cd\u5e94\u65f6\u95f4\uff0c\u800cAzure\u5728\u7a33\u6001\u64cd\u4f5c\u4e2d\u663e\u793a71.9%\u66f4\u9ad8\u7684\u6210\u672c\u6548\u7387\u3002\u6240\u6709\u8868\u683c\u548c\u56fe\u8868\u76f4\u63a5\u4ece\u4ee3\u7801\u8f93\u51fa\u751f\u6210\uff0c\u786e\u4fdd\u5b9e\u9a8c\u6570\u636e\u4e0e\u62a5\u544a\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u5f3a\u5927\u7684\u96f6\u552e\u4e91\u5e94\u7528\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u9996\u6b21\u63d0\u4f9b\u4e86\u8de8\u9886\u5148\u4e91\u5e73\u53f0\u7684POS\u7cfb\u7edf\u7279\u6709\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5168\u9762\u3001\u4ee3\u7801\u9a71\u52a8\u6bd4\u8f83\uff0c\u4e3a\u5546\u5bb6\u8003\u8651\u4e91POS\u5b9e\u65bd\u63d0\u4f9b\u4e86\u6709\u7528\u6846\u67b6\u3002"}}
{"id": "2601.00644", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.00644", "abs": "https://arxiv.org/abs/2601.00644", "authors": ["Yuchen Li", "Rui Kong", "Zhonghao Lyu", "Qiyang Li", "Xinran Chen", "Hengyi Cai", "Lingyong Yan", "Shuaiqiang Wang", "Jiashu Zhao", "Guangxu Zhu", "Linghe Kong", "Guihai Chen", "Haoyi Xiong", "Dawei Yin"], "title": "FlexSpec: Frozen Drafts Meet Evolving Targets in Edge-Cloud Collaborative LLM Speculative Decoding", "comment": null, "summary": "Deploying large language models (LLMs) in mobile and edge computing environments is constrained by limited on-device resources, scarce wireless bandwidth, and frequent model evolution. Although edge-cloud collaborative inference with speculative decoding (SD) can reduce end-to-end latency by executing a lightweight draft model at the edge and verifying it with a cloud-side target model, existing frameworks fundamentally rely on tight coupling between the two models. Consequently, repeated model synchronization introduces excessive communication overhead, increasing end-to-end latency, and ultimately limiting the scalability of SD in edge environments. To address these limitations, we propose FlexSpec, a communication-efficient collaborative inference framework tailored for evolving edge-cloud systems. The core design of FlexSpec is a shared-backbone architecture that allows a single and static edge-side draft model to remain compatible with a large family of evolving cloud-side target models. By decoupling edge deployment from cloud-side model updates, FlexSpec eliminates the need for edge-side retraining or repeated model downloads, substantially reducing communication and maintenance costs. Furthermore, to accommodate time-varying wireless conditions and heterogeneous device constraints, we develop a channel-aware adaptive speculation mechanism that dynamically adjusts the speculative draft length based on real-time channel state information and device energy budgets. Extensive experiments demonstrate that FlexSpec achieves superior performance compared to conventional SD approaches in terms of inference efficiency.", "AI": {"tldr": "FlexSpec\uff1a\u4e00\u79cd\u9762\u5411\u8fb9\u7f18-\u4e91\u534f\u540c\u63a8\u7406\u7684\u901a\u4fe1\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u4e3b\u5e72\u67b6\u6784\u548c\u4fe1\u9053\u611f\u77e5\u81ea\u9002\u5e94\u63a8\u6d4b\u673a\u5236\uff0c\u89e3\u51b3\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u4e2d\u6a21\u578b\u540c\u6b65\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u79fb\u52a8\u548c\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u8d44\u6e90\u6709\u9650\u3001\u65e0\u7ebf\u5e26\u5bbd\u7a00\u7f3a\u548c\u6a21\u578b\u9891\u7e41\u66f4\u65b0\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u8fb9\u7f18-\u4e91\u534f\u540c\u63a8\u7406\u6846\u67b6\u57fa\u4e8e\u63a8\u6d4b\u89e3\u7801\u6280\u672f\uff0c\u4f46\u5b58\u5728\u8fb9\u7f18\u4e0e\u4e91\u7aef\u6a21\u578b\u7d27\u5bc6\u8026\u5408\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u540c\u6b65\u5e26\u6765\u8fc7\u591a\u901a\u4fe1\u5f00\u9500\uff0c\u589e\u52a0\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u9650\u5236\u4e86\u63a8\u6d4b\u89e3\u7801\u5728\u8fb9\u7f18\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faFlexSpec\u6846\u67b6\uff0c\u6838\u5fc3\u8bbe\u8ba1\u5305\u62ec\uff1a1\uff09\u5171\u4eab\u4e3b\u5e72\u67b6\u6784\uff0c\u4f7f\u5355\u4e2a\u9759\u6001\u7684\u8fb9\u7f18\u4fa7\u8349\u7a3f\u6a21\u578b\u80fd\u4e0e\u4e00\u7cfb\u5217\u6f14\u5316\u7684\u4e91\u7aef\u76ee\u6807\u6a21\u578b\u4fdd\u6301\u517c\u5bb9\uff1b2\uff09\u4fe1\u9053\u611f\u77e5\u81ea\u9002\u5e94\u63a8\u6d4b\u673a\u5236\uff0c\u6839\u636e\u5b9e\u65f6\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u548c\u8bbe\u5907\u80fd\u91cf\u9884\u7b97\u52a8\u6001\u8c03\u6574\u63a8\u6d4b\u8349\u7a3f\u957f\u5ea6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFlexSpec\u5728\u63a8\u7406\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u548c\u7ef4\u62a4\u6210\u672c\uff0c\u6d88\u9664\u4e86\u8fb9\u7f18\u4fa7\u91cd\u65b0\u8bad\u7ec3\u6216\u91cd\u590d\u6a21\u578b\u4e0b\u8f7d\u7684\u9700\u6c42\u3002", "conclusion": "FlexSpec\u901a\u8fc7\u89e3\u8026\u8fb9\u7f18\u90e8\u7f72\u4e0e\u4e91\u7aef\u6a21\u578b\u66f4\u65b0\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18-\u4e91\u534f\u540c\u63a8\u7406\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u4e3a\u6f14\u5316\u8fb9\u7f18-\u4e91\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
