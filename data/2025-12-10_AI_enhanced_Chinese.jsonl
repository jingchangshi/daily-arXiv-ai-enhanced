{"id": "2512.08005", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.08005", "abs": "https://arxiv.org/abs/2512.08005", "authors": ["Stepan Vanecek", "Matthew Turner", "Manisha Gajbe", "Matthew Wolf", "Martin Schulz"], "title": "Modeling the Potential of Message-Free Communication via CXL.mem", "comment": "14 pages, including References, 10 figures, to be published in SCA/HPCAsia 2026: Supercomputing Asia and International Conference on High Performance Computing in Asia Pacific Region (SCA/HPCAsia 2026)", "summary": "Heterogeneous memory technologies are increasingly important instruments in addressing the memory wall in HPC systems. While most are deployed in single node setups, CXL.mem is a technology that implements memories that can be attached to multiple nodes simultaneously, enabling shared memory pooling. This opens new possibilities, particularly for efficient inter-node communication.\n  In this paper, we present a novel performance evaluation toolchain combined with an extended performance model for message-based communication, which can be used to predict potential performance benefits from using CXL.mem for data exchange. Our approach analyzes data access patterns of MPI applications: it analyzes on-node accesses to/from MPI buffers, as well as cross-node MPI traffic to gather a full understanding of the impact of memory performance. We combine this data in an extended performance model to predict which data transfers could benefit from direct CXL.mem implementations as compared to traditional MPI messages. Our model works on a per-MPI call granularity, allowing the identification and later optimizations of those MPI invocations in the code with the highest potential for speedup by using CXL.mem.\n  For our toolchain, we extend the memory trace sampling tool Mitos and use it to extract data access behavior. In the post-processing step, the raw data is automatically analyzed to provide performance models for each individual MPI call. We validate the models on two sample applications -- a 2D heat transfer miniapp and the HPCG benchmark -- and use them to demonstrate their support for targeted optimizations by integrating CXL.mem.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u7ed3\u5408CXL.mem\u6280\u672f\u7684MPI\u901a\u4fe1\u6027\u80fd\u8bc4\u4f30\u5de5\u5177\u94fe\u548c\u6269\u5c55\u6027\u80fd\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u548c\u4f18\u5316\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u7684\u8de8\u8282\u70b9\u6570\u636e\u4ea4\u6362\u6027\u80fd\u3002", "motivation": "CXL.mem\u6280\u672f\u5141\u8bb8\u591a\u8282\u70b9\u540c\u65f6\u8bbf\u95ee\u5171\u4eab\u5185\u5b58\u6c60\uff0c\u4e3a\u9ad8\u6548\u8de8\u8282\u70b9\u901a\u4fe1\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002\u9700\u8981\u5de5\u5177\u6765\u8bc4\u4f30\u548c\u9884\u6d4bMPI\u5e94\u7528\u5982\u4f55\u4eceCXL.mem\u4e2d\u83b7\u76ca\uff0c\u4ee5\u4f18\u5316\u6570\u636e\u4ea4\u6362\u6027\u80fd\u3002", "method": "\u6269\u5c55\u5185\u5b58\u8ddf\u8e2a\u91c7\u6837\u5de5\u5177Mitos\uff0c\u5206\u6790MPI\u5e94\u7528\u7684\u6570\u636e\u8bbf\u95ee\u6a21\u5f0f\uff08\u5305\u62ec\u8282\u70b9\u5185MPI\u7f13\u51b2\u533a\u8bbf\u95ee\u548c\u8de8\u8282\u70b9MPI\u6d41\u91cf\uff09\uff0c\u6784\u5efa\u57fa\u4e8e\u6bcf\u4e2aMPI\u8c03\u7528\u7684\u6269\u5c55\u6027\u80fd\u6a21\u578b\uff0c\u9884\u6d4b\u54ea\u4e9b\u6570\u636e\u4f20\u8f93\u53ef\u901a\u8fc7CXL.mem\u76f4\u63a5\u5b9e\u73b0\u83b7\u5f97\u6027\u80fd\u63d0\u5347\u3002", "result": "\u5de5\u5177\u94fe\u6210\u529f\u63d0\u53d6\u6570\u636e\u8bbf\u95ee\u884c\u4e3a\u5e76\u81ea\u52a8\u5206\u6790\u751f\u6210\u6bcf\u4e2aMPI\u8c03\u7528\u7684\u6027\u80fd\u6a21\u578b\u3002\u57282D\u70ed\u4f20\u5bfc\u8ff7\u4f60\u5e94\u7528\u548cHPCG\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u652f\u6301\u9488\u5bf9\u6027\u4f18\u5316\u7684\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u5de5\u5177\u94fe\u548c\u6027\u80fd\u6a21\u578b\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u548c\u9884\u6d4bCXL.mem\u5bf9MPI\u901a\u4fe1\u7684\u6027\u80fd\u5f71\u54cd\uff0c\u652f\u6301\u8bc6\u522b\u548c\u4f18\u5316\u5177\u6709\u6700\u5927\u52a0\u901f\u6f5c\u529b\u7684MPI\u8c03\u7528\uff0c\u4e3a\u5f02\u6784\u5185\u5b58\u6280\u672f\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2512.08067", "categories": ["cs.DC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.08067", "abs": "https://arxiv.org/abs/2512.08067", "authors": ["Qingyang Hu", "Yucheng Huang", "Manshi Yang"], "title": "CapsuleFS A Multi-credential DataCapsule Filesystem", "comment": null, "summary": "CapsuleFS (CFS) is the first filesystem to integrate multi-credential functionality within a POSIX-compliant framework, utilizing DataCapsule as the storage provider. This innovative system is established based on the Global Data Plane in the area of edge computing. Our comprehensive design and implementation of CFS successfully fulfill the objective of providing a multi-credential Common Access API. The architecture of CFS is methodically segmented into three integral components: Firstly, the DataCapsule server, tasked with the storage, dissemination, and replication of DataCapsules on the edge. Secondly, the middleware, a crucial element running in a Trusted Execution Environment responsible for the enforcement and management of write permissions and requests. Finally, the client component, which manifests as a POSIX-compliant filesystem, is adaptable and operational across many architectures. Experimental evaluations of CFS reveal that, while its read and write performances are comparatively modest, it upholds a high degree of functional correctness. This attribute distinctly positions CFS as a viable candidate for application in real-world software development scenarios. The paper also delineates potential future enhancements, aimed at augmenting the practicality of CFS in the landscape of software development.", "AI": {"tldr": "CapsuleFS (CFS) \u662f\u9996\u4e2a\u5728POSIX\u517c\u5bb9\u6846\u67b6\u5185\u96c6\u6210\u591a\u51ed\u8bc1\u529f\u80fd\u7684\u6587\u4ef6\u7cfb\u7edf\uff0c\u57fa\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u5168\u5c40\u6570\u636e\u5e73\u9762\u6784\u5efa\uff0c\u4f7f\u7528DataCapsule\u4f5c\u4e3a\u5b58\u50a8\u63d0\u4f9b\u8005\u3002", "motivation": "\u5728\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u652f\u6301\u591a\u51ed\u8bc1\u8bbf\u95ee\u63a7\u5236\u7684\u6587\u4ef6\u7cfb\u7edf\uff0c\u4ee5\u63d0\u4f9b\u5b89\u5168\u7684\u6570\u636e\u5171\u4eab\u548c\u7ba1\u7406\u529f\u80fd\uff0c\u540c\u65f6\u4fdd\u6301POSIX\u517c\u5bb9\u6027\u3002", "method": "CFS\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a1) DataCapsule\u670d\u52a1\u5668\u8d1f\u8d23\u8fb9\u7f18\u5b58\u50a8\u548c\u590d\u5236\uff1b2) \u5728\u53ef\u4fe1\u6267\u884c\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u4e2d\u95f4\u4ef6\u7ba1\u7406\u5199\u6743\u9650\uff1b3) POSIX\u517c\u5bb9\u7684\u5ba2\u6237\u7aef\u6587\u4ef6\u7cfb\u7edf\u9002\u914d\u591a\u79cd\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793aCFS\u8bfb\u5199\u6027\u80fd\u76f8\u5bf9\u4e00\u822c\uff0c\u4f46\u529f\u80fd\u6b63\u786e\u6027\u9ad8\uff0c\u9002\u5408\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u573a\u666f\u5e94\u7528\u3002", "conclusion": "CFS\u662f\u9996\u4e2a\u5b9e\u73b0\u591a\u51ed\u8bc1\u529f\u80fd\u7684POSIX\u517c\u5bb9\u6587\u4ef6\u7cfb\u7edf\uff0c\u867d\u7136\u6027\u80fd\u6709\u5f85\u63d0\u5347\uff0c\u4f46\u529f\u80fd\u6b63\u786e\u6027\u4f7f\u5176\u5177\u5907\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.08242", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.08242", "abs": "https://arxiv.org/abs/2512.08242", "authors": ["Marco Kurzynski", "Shaizeen Aga", "Di Wu"], "title": "Chopper: A Multi-Level GPU Characterization Tool & Derived Insights Into LLM Training Inefficiency", "comment": null, "summary": "Training large language models (LLMs) efficiently requires a deep understanding of how modern GPU systems behave under real-world distributed training workloads. While prior work has focused primarily on kernel-level performance or single-GPU microbenchmarks, the complex interaction between communication, computation, memory behavior, and power management in multi-GPU LLM training remains poorly characterized. In this work, we introduce Chopper, a profiling and analysis framework that collects, aligns, and visualizes GPU kernel traces and hardware performance counters across multiple granularities (i.e., from individual kernels to operations, layers, phases, iterations, and GPUs). Using Chopper, we perform a comprehensive end-to-end characterization of Llama 3 8B training under fully sharded data parallelism (FSDP) on an eight-GPU AMD InstinctTM MI300X node. Our analysis reveals several previously underexplored bottlenecks and behaviors, such as memory determinism enabling higher, more stable GPU and memory frequencies. We identify several sources of inefficiencies, with frequency overhead (DVFS effects) being the single largest contributor to the gap between theoretical and observed performance, exceeding the impact of MFMA utilization loss, communication/computation overlap, and kernel launch overheads. Overall, Chopper provides the first holistic, multi-granularity characterization of LLM training on AMD InstinctTM MI300X GPUs, yielding actionable insights for optimizing training frameworks, improving power-management strategies, and guiding future GPU architecture and system design.", "AI": {"tldr": "Chopper\u662f\u4e00\u4e2a\u7528\u4e8e\u5206\u6790\u548c\u53ef\u89c6\u5316\u591aGPU LLM\u8bad\u7ec3\u6027\u80fd\u7684\u6846\u67b6\uff0c\u9996\u6b21\u5728AMD MI300X GPU\u4e0a\u5bf9Llama 3 8B\u8bad\u7ec3\u8fdb\u884c\u4e86\u5168\u9762\u7aef\u5230\u7aef\u5206\u6790\uff0c\u53d1\u73b0\u9891\u7387\u5f00\u9500\u662f\u6027\u80fd\u5dee\u8ddd\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "motivation": "\u5f53\u524d\u5bf9LLM\u8bad\u7ec3\u6027\u80fd\u7684\u7406\u89e3\u4e3b\u8981\u5c40\u9650\u4e8e\u5185\u6838\u7ea7\u6027\u80fd\u6216\u5355GPU\u5fae\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7f3a\u4e4f\u5bf9\u591aGPU\u8bad\u7ec3\u4e2d\u901a\u4fe1\u3001\u8ba1\u7b97\u3001\u5185\u5b58\u884c\u4e3a\u548c\u7535\u6e90\u7ba1\u7406\u4e4b\u95f4\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u5f00\u53d1\u4e86Chopper\u6846\u67b6\uff0c\u6536\u96c6\u3001\u5bf9\u9f50\u548c\u53ef\u89c6\u5316GPU\u5185\u6838\u8ddf\u8e2a\u548c\u786c\u4ef6\u6027\u80fd\u8ba1\u6570\u5668\uff0c\u652f\u6301\u4ece\u5185\u6838\u5230\u64cd\u4f5c\u3001\u5c42\u3001\u9636\u6bb5\u3001\u8fed\u4ee3\u548cGPU\u7684\u591a\u7c92\u5ea6\u5206\u6790\u3002\u57288-GPU AMD MI300X\u8282\u70b9\u4e0a\u5bf9Llama 3 8B\u7684FSDP\u8bad\u7ec3\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002", "result": "\u53d1\u73b0\u5185\u5b58\u786e\u5b9a\u6027\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u3001\u66f4\u7a33\u5b9a\u7684GPU\u548c\u5185\u5b58\u9891\u7387\uff1b\u9891\u7387\u5f00\u9500\uff08DVFS\u6548\u5e94\uff09\u662f\u7406\u8bba\u6027\u80fd\u548c\u5b9e\u9645\u6027\u80fd\u5dee\u8ddd\u7684\u6700\u5927\u8d21\u732e\u8005\uff0c\u8d85\u8fc7\u4e86MFMA\u5229\u7528\u7387\u635f\u5931\u3001\u901a\u4fe1/\u8ba1\u7b97\u91cd\u53e0\u548c\u5185\u6838\u542f\u52a8\u5f00\u9500\u7684\u5f71\u54cd\u3002", "conclusion": "Chopper\u9996\u6b21\u5728AMD MI300X GPU\u4e0a\u63d0\u4f9b\u4e86\u591a\u7c92\u5ea6\u7684LLM\u8bad\u7ec3\u5168\u9762\u5206\u6790\uff0c\u4e3a\u4f18\u5316\u8bad\u7ec3\u6846\u67b6\u3001\u6539\u8fdb\u7535\u6e90\u7ba1\u7406\u7b56\u7565\u4ee5\u53ca\u6307\u5bfc\u672a\u6765GPU\u67b6\u6784\u548c\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2512.08288", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.08288", "abs": "https://arxiv.org/abs/2512.08288", "authors": ["Chinmaya Kumar Dehury", "Lauri Lov\u00e9n", "Praveen Kumar Donta", "Ilir Murturi", "Schahram Dustdar"], "title": "Synergizing Monetization, Orchestration, and Semantics in Computing Continuum", "comment": "Currently submitted to IEEE Computers", "summary": "Industry demands are growing for hyper-distributed applications that span from the cloud to the edge in domains such as smart manufacturing, transportation, and agriculture. Yet today's solutions struggle to meet these demands due to inherent limitations in scalability, interoperability, and trust. In this article, we introduce HERMES (Heterogeneous Computing Continuum with Resource Monetization, Orchestration, and Semantic) - a novel framework designed to transform connectivity and data utilization across the computing continuum. HERMES establishes an open, seamless, and secure environment where resources, from cloud servers to tiny edge devices, can be orchestrated intelligently, data and services can be monetized in a distributed marketplace, and knowledge is shared through semantic interoperability. By bridging these key facets, HERMES lays a foundation for a new generation of distributed applications that are more efficient, trustworthy, and autonomous.", "AI": {"tldr": "HERMES\u662f\u4e00\u4e2a\u7528\u4e8e\u5f02\u6784\u8ba1\u7b97\u8fde\u7eed\u4f53\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8d44\u6e90\u8d27\u5e01\u5316\u3001\u7f16\u6392\u548c\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\uff0c\u89e3\u51b3\u4e91\u5230\u8fb9\u7f18\u5e94\u7528\u7684\u6269\u5c55\u6027\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u4fe1\u4efb\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u5bf9\u4ece\u4e91\u5230\u8fb9\u7f18\u7684\u8d85\u5206\u5e03\u5f0f\u5e94\u7528\u9700\u6c42\u589e\u957f\uff0c\u4f46\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u53ef\u6269\u5c55\u6027\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u4fe1\u4efb\u65b9\u9762\u5b58\u5728\u56fa\u6709\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u5f15\u5165HERMES\u6846\u67b6\uff0c\u5efa\u7acb\u5f00\u653e\u3001\u65e0\u7f1d\u3001\u5b89\u5168\u7684\u73af\u5883\uff0c\u5b9e\u73b0\u4ece\u4e91\u670d\u52a1\u5668\u5230\u8fb9\u7f18\u8bbe\u5907\u7684\u667a\u80fd\u8d44\u6e90\u7f16\u6392\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u5e02\u573a\u8fdb\u884c\u6570\u636e\u548c\u670d\u52a1\u7684\u8d27\u5e01\u5316\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\u5171\u4eab\u77e5\u8bc6\u3002", "result": "HERMES\u901a\u8fc7\u6574\u5408\u8d44\u6e90\u7f16\u6392\u3001\u8d27\u5e01\u5316\u548c\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\uff0c\u4e3a\u65b0\u4e00\u4ee3\u5206\u5e03\u5f0f\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f7f\u5176\u66f4\u9ad8\u6548\u3001\u53ef\u4fe1\u548c\u81ea\u4e3b\u3002", "conclusion": "HERMES\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u8ba1\u7b97\u8fde\u7eed\u4f53\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u667a\u80fd\u5236\u9020\u3001\u4ea4\u901a\u548c\u519c\u4e1a\u7b49\u9886\u57df\u7684\u8d85\u5206\u5e03\u5f0f\u5e94\u7528\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.08089", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.08089", "abs": "https://arxiv.org/abs/2512.08089", "authors": ["Jebacyril Arockiaraj", "Dhruv Parikh", "Viktor Prasanna"], "title": "NysX: An Accurate and Energy-Efficient FPGA Accelerator for Hyperdimensional Graph Classification at the Edge", "comment": null, "summary": "Real-time, energy-efficient inference on edge devices is essential for graph classification across a range of applications. Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that encodes input features into low-precision, high-dimensional vectors with simple element-wise operations, making it well-suited for resource-constrained edge platforms. Recent work enhances HDC accuracy for graph classification via Nystr\u00f6m kernel approximations. Edge acceleration of such methods faces several challenges: (i) redundancy among (landmark) samples selected via uniform sampling, (ii) storing the Nystr\u00f6m projection matrix under limited on-chip memory, (iii) expensive, contention-prone codebook lookups, and (iv) load imbalance due to irregular sparsity in SpMV. To address these challenges, we propose NysX, the first end-to-end FPGA accelerator for Nystr\u00f6m-based HDC graph classification at the edge. NysX integrates four key optimizations: (i) a hybrid landmark selection strategy combining uniform sampling with determinantal point processes (DPPs) to reduce redundancy while improving accuracy; (ii) a streaming architecture for Nystr\u00f6m projection matrix maximizing external memory bandwidth utilization; (iii) a minimal-perfect-hash lookup engine enabling $O(1)$ key-to-index mapping with low on-chip memory overhead; and (iv) sparsity-aware SpMV engines with static load balancing. Together, these innovations enable real-time, energy-efficient inference on resource-constrained platforms. Implemented on an AMD Zynq UltraScale+ (ZCU104) FPGA, NysX achieves $6.85\\times$ ($4.32\\times$) speedup and $169\\times$ ($314\\times$) energy efficiency gains over optimized CPU (GPU) baselines, while improving classification accuracy by $3.4\\%$ on average across TUDataset benchmarks, a widely used standard for graph classification.", "AI": {"tldr": "NysX\uff1a\u9996\u4e2a\u9762\u5411\u8fb9\u7f18\u8bbe\u5907\u7684\u7aef\u5230\u7aefFPGA\u52a0\u901f\u5668\uff0c\u7528\u4e8e\u57fa\u4e8eNystr\u00f6m\u6838\u8fd1\u4f3c\u7684\u8d85\u7ef4\u8ba1\u7b97\u56fe\u5206\u7c7b\uff0c\u901a\u8fc7\u6df7\u5408\u5730\u6807\u9009\u62e9\u3001\u6d41\u5f0f\u67b6\u6784\u3001\u5b8c\u7f8e\u54c8\u5e0c\u67e5\u627e\u548c\u7a00\u758f\u611f\u77e5SpMV\u5f15\u64ce\u7b49\u4f18\u5316\uff0c\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u3001\u9ad8\u6548\u80fd\u56fe\u5206\u7c7b\u5e94\u7528\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u57fa\u4e8eNystr\u00f6m\u6838\u8fd1\u4f3c\u7684\u8d85\u7ef4\u8ba1\u7b97\uff08HDC\uff09\u65b9\u6cd5\u867d\u7136\u63d0\u5347\u4e86\u56fe\u5206\u7c7b\u7cbe\u5ea6\uff0c\u4f46\u5728\u8fb9\u7f18\u52a0\u901f\u65f6\u9762\u4e34\u591a\u4e2a\u6311\u6218\uff1a\u5747\u5300\u91c7\u6837\u7684\u5730\u6807\u6837\u672c\u5197\u4f59\u3001\u7247\u4e0a\u5185\u5b58\u6709\u9650\u5bfc\u81f4Nystr\u00f6m\u6295\u5f71\u77e9\u9635\u5b58\u50a8\u56f0\u96be\u3001\u6602\u8d35\u7684\u7801\u672c\u67e5\u627e\u64cd\u4f5c\uff0c\u4ee5\u53ca\u4e0d\u89c4\u5219\u7a00\u758f\u6027\u5bfc\u81f4\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u3002", "method": "\u63d0\u51faNysX\u52a0\u901f\u5668\uff0c\u5305\u542b\u56db\u9879\u5173\u952e\u4f18\u5316\uff1a1\uff09\u6df7\u5408\u5730\u6807\u9009\u62e9\u7b56\u7565\uff0c\u7ed3\u5408\u5747\u5300\u91c7\u6837\u548c\u884c\u5217\u5f0f\u70b9\u8fc7\u7a0b\uff08DPPs\uff09\u51cf\u5c11\u5197\u4f59\uff1b2\uff09\u6d41\u5f0f\u67b6\u6784\u5904\u7406Nystr\u00f6m\u6295\u5f71\u77e9\u9635\uff0c\u6700\u5927\u5316\u5916\u90e8\u5185\u5b58\u5e26\u5bbd\u5229\u7528\u7387\uff1b3\uff09\u6700\u5c0f\u5b8c\u7f8e\u54c8\u5e0c\u67e5\u627e\u5f15\u64ce\uff0c\u5b9e\u73b0O(1)\u952e\u503c\u6620\u5c04\u4e14\u7247\u4e0a\u5185\u5b58\u5f00\u9500\u4f4e\uff1b4\uff09\u7a00\u758f\u611f\u77e5SpMV\u5f15\u64ce\uff0c\u91c7\u7528\u9759\u6001\u8d1f\u8f7d\u5e73\u8861\u5904\u7406\u4e0d\u89c4\u5219\u7a00\u758f\u6027\u3002", "result": "\u5728AMD Zynq UltraScale+ (ZCU104) FPGA\u4e0a\u5b9e\u73b0\uff0c\u76f8\u6bd4\u4f18\u5316CPU\u57fa\u51c6\u83b7\u5f976.85\u500d\u52a0\u901f\u548c169\u500d\u80fd\u6548\u63d0\u5347\uff0c\u76f8\u6bd4GPU\u57fa\u51c6\u83b7\u5f974.32\u500d\u52a0\u901f\u548c314\u500d\u80fd\u6548\u63d0\u5347\u3002\u5728TUDataset\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u5206\u7c7b\u7cbe\u5ea6\u63d0\u53473.4%\u3002", "conclusion": "NysX\u901a\u8fc7\u56db\u9879\u521b\u65b0\u4f18\u5316\u6210\u529f\u89e3\u51b3\u4e86Nystr\u00f6m-based HDC\u56fe\u5206\u7c7b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u52a0\u901f\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u9ad8\u6548\u80fd\u7684\u63a8\u7406\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u5e73\u53f0\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.08321", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.08321", "abs": "https://arxiv.org/abs/2512.08321", "authors": ["Yuki Uchino", "Qianxiang Ma", "Toshiyuki Imamura", "Katsuhisa Ozaki", "Patrick Lars Gutsche"], "title": "Emulation of Complex Matrix Multiplication based on the Chinese Remainder Theorem", "comment": "11 pages, 13 figures", "summary": "Modern computing architectures feature low-precision matrix multiplication units that achieve substantially higher throughput than their high-precision counterparts. Motivated by this architectural trend, the emulation of high-precision matrix multiplication using low-precision hardware has attracted significant interest in the high-performance computing community. Ozaki, Uchino, and Imamura introduced the Ozaki-II scheme as a general framework for emulating matrix multiplication. Building on this framework, Uchino, Ozaki, and Imamura developed high-performance and power-efficient techniques for emulating single- and double-precision real matrix multiplication on INT8 matrix engines. Extending this line of research, the present study proposes high-performance emulation methods for single- and double-precision complex matrix multiplication on INT8 matrix engines, based on the Ozaki-II scheme. On an NVIDIA B200 GPU, the proposed methods achieve 4.0x--5.6x and 4.4x--6.5x speedups over the native single- and double-precision complex matrix multiplication routines from cuBLAS, respectively, for sufficiently large problem sizes. When lower accuracy than that of the standard routine is acceptable, the proposed methods can operate at even higher speed. Conversely, with only a modest increase in computation time, they can also deliver higher accuracy than the standard routines. These properties suggest that the proposed approach has the potential to serve as a default algorithm across a wide range of applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eOzaki-II\u65b9\u6848\u7684\u9ad8\u6027\u80fd\u590d\u6570\u77e9\u9635\u4e58\u6cd5\u4eff\u771f\u65b9\u6cd5\uff0c\u5728INT8\u77e9\u9635\u5f15\u64ce\u4e0a\u5b9e\u73b0\u5355\u53cc\u7cbe\u5ea6\u590d\u6570\u77e9\u9635\u8fd0\u7b97\uff0c\u76f8\u6bd4cuBLAS\u539f\u751f\u5b9e\u73b0\u83b7\u5f974-6.5\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u4ee3\u8ba1\u7b97\u67b6\u6784\u4e2d\u4f4e\u7cbe\u5ea6\u77e9\u9635\u4e58\u6cd5\u5355\u5143\u6bd4\u9ad8\u7cbe\u5ea6\u5355\u5143\u5177\u6709\u66f4\u9ad8\u7684\u541e\u5410\u91cf\uff0c\u56e0\u6b64\u5229\u7528\u4f4e\u7cbe\u5ea6\u786c\u4ef6\u4eff\u771f\u9ad8\u7cbe\u5ea6\u77e9\u9635\u4e58\u6cd5\u6210\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u9886\u57df\u7684\u91cd\u8981\u7814\u7a76\u65b9\u5411\u3002", "method": "\u57fa\u4e8eOzaki-II\u65b9\u6848\u6846\u67b6\uff0c\u5f00\u53d1\u5728INT8\u77e9\u9635\u5f15\u64ce\u4e0a\u4eff\u771f\u5355\u53cc\u7cbe\u5ea6\u590d\u6570\u77e9\u9635\u4e58\u6cd5\u7684\u9ad8\u6027\u80fd\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u5148\u524d\u9488\u5bf9\u5b9e\u6570\u77e9\u9635\u4e58\u6cd5\u7684\u7814\u7a76\u3002", "result": "\u5728NVIDIA B200 GPU\u4e0a\uff0c\u76f8\u6bd4cuBLAS\u539f\u751f\u5355\u53cc\u7cbe\u5ea6\u590d\u6570\u77e9\u9635\u4e58\u6cd5\uff0c\u5206\u522b\u83b7\u5f974.0-5.6\u500d\u548c4.4-6.5\u500d\u52a0\u901f\u3002\u65b9\u6cd5\u53ef\u6839\u636e\u9700\u6c42\u5728\u7cbe\u5ea6\u548c\u901f\u5ea6\u4e4b\u95f4\u7075\u6d3b\u6743\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5177\u6709\u4f5c\u4e3a\u9ed8\u8ba4\u7b97\u6cd5\u5728\u5e7f\u6cdb\u5e94\u7528\u4e2d\u4f7f\u7528\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u7075\u6d3b\u7684\u6743\u8861\u9009\u62e9\u3002"}}
{"id": "2512.08365", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.08365", "abs": "https://arxiv.org/abs/2512.08365", "authors": ["Yi Pan", "Wenbo Qian", "Dedong Xie", "Ruiyan Hu", "Yigong Hu", "Baris Kasikci"], "title": "Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging", "comment": "12 pages, 10 fi", "summary": "The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them.\n  We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMagneton\u7684\u5dee\u5206\u80fd\u91cf\u8c03\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u6bd4\u8f83\u76f8\u4f3cML\u7cfb\u7edf\u7684\u80fd\u91cf\u6d88\u8017\u6765\u68c0\u6d4b\u548c\u8bca\u65ad\u8f6f\u4ef6\u80fd\u91cf\u6d6a\u8d39\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u90e8\u7f72\u80fd\u8017\u5de8\u5927\uff0c\u73b0\u6709\u4f18\u5316\u4e3b\u8981\u5173\u6ce8\u786c\u4ef6\u80fd\u6548\uff0c\u4f46\u8f6f\u4ef6\u8bbe\u8ba1\u4e0d\u826f\u5bfc\u81f4\u7684\u80fd\u91cf\u6d6a\u8d39\u88ab\u5ffd\u89c6\u3002ML\u6846\u67b6\u548c\u5e94\u7528\u4e2d\u666e\u904d\u5b58\u5728\u5197\u4f59\u6216\u8bbe\u8ba1\u4e0d\u826f\u7684\u64cd\u4f5c\uff0c\u6d88\u8017\u66f4\u591a\u80fd\u91cf\u5374\u4e0d\u63d0\u5347\u6027\u80fd\uff0c\u800c\u5f00\u53d1\u8005\u7f3a\u4e4f\u68c0\u6d4b\u548c\u8bca\u65ad\u8fd9\u4e9b\u95ee\u9898\u7684\u5de5\u5177\u548c\u53ef\u89c1\u6027\u3002", "method": "\u63d0\u51fa\u5dee\u5206\u80fd\u91cf\u8c03\u8bd5\u65b9\u6cd5\uff0c\u57fa\u4e8e\u76f8\u4f3cML\u7cfb\u7edf\u5b9e\u73b0\u76f8\u540c\u529f\u80fd\u4f46\u80fd\u91cf\u6d88\u8017\u5dee\u5f02\u5de8\u5927\u7684\u89c2\u5bdf\u3002\u8bbe\u8ba1\u5e76\u5b9e\u73b0Magneton\u80fd\u91cf\u5206\u6790\u5668\uff0c\u5728\u7b97\u5b50\u7ea7\u522b\u6bd4\u8f83\u76f8\u4f3cML\u7cfb\u7edf\u7684\u80fd\u91cf\u6d88\u8017\uff0c\u81ea\u52a8\u5b9a\u4f4d\u5bfc\u81f4\u8fc7\u5ea6\u80fd\u91cf\u4f7f\u7528\u7684\u4ee3\u7801\u533a\u57df\u548c\u914d\u7f6e\u9009\u62e9\u3002", "result": "\u5e94\u7528\u4e8e9\u4e2a\u6d41\u884c\u7684ML\u7cfb\u7edf\uff08\u6db5\u76d6LLM\u63a8\u7406\u3001\u901a\u7528ML\u6846\u67b6\u548c\u56fe\u50cf\u751f\u6210\uff09\uff0cMagneton\u68c0\u6d4b\u5e76\u8bca\u65ad\u4e8616\u4e2a\u5df2\u77e5\u7684\u8f6f\u4ef6\u80fd\u91cf\u4f4e\u6548\u6848\u4f8b\uff0c\u5e76\u8fdb\u4e00\u6b65\u53d1\u73b0\u4e868\u4e2a\u5148\u524d\u672a\u77e5\u7684\u6848\u4f8b\uff0c\u5176\u4e2d7\u4e2a\u5df2\u5f97\u5230\u5f00\u53d1\u8005\u786e\u8ba4\u3002", "conclusion": "\u5dee\u5206\u80fd\u91cf\u8c03\u8bd5\u662f\u6709\u6548\u7684\u8f6f\u4ef6\u80fd\u91cf\u6d6a\u8d39\u68c0\u6d4b\u65b9\u6cd5\uff0cMagneton\u5de5\u5177\u80fd\u591f\u6210\u529f\u8bc6\u522bML\u7cfb\u7edf\u4e2d\u7684\u80fd\u91cf\u4f4e\u6548\u95ee\u9898\uff0c\u5305\u62ec\u5df2\u77e5\u548c\u672a\u77e5\u7684\u6848\u4f8b\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u68c0\u6d4b\u548c\u8bca\u65ad\u8f6f\u4ef6\u80fd\u91cf\u6d6a\u8d39\u7684\u80fd\u529b\u3002"}}
{"id": "2512.08563", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.08563", "abs": "https://arxiv.org/abs/2512.08563", "authors": ["Taras Skazhenik", "Nikolai Korobenikov", "Andrei Churbanov", "Anton Malakhov", "Vitaly Aksenov"], "title": "Basic Lock Algorithms in Lightweight Thread Environments", "comment": null, "summary": "Traditionally, multithreaded data structures have been designed for access by the threads of Operating Systems (OS). However, implementations for access by programmable alternatives known as lightweight threads (also referred to as asynchronous calls or coroutines) have not been thoroughly studied. The main advantage of lightweight threads is their significantly lower overhead during launch and context switching. However, this comes at a cost: to achieve proper parallelism, context switches must be manually invoked in the code; without these switches, new lightweight threads will never be executed.\n  In this paper, we focus on the simplest multithreaded data structure: a mutex (also known as a lock). We demonstrate that original implementations for OS threads cannot be used effectively in this new context due to the potential for deadlocks. Furthermore, correctness is not the only concern. In certain languages, such as C++, there are various lightweight thread libraries, each with different implementations and interfaces, which necessitate distinct lock implementations.\n  In this work, we present a modification of TTAS and MCS locks for the use from lightweight threads and demonstrate that the two context switch mechanisms of lightweight threads, yielding and sleeping, are crucial. However, the performance of TTAS and MCS may differ significantly depending on the settings. If one wants to have a lock that works well for any library, we suggest using the cohort lock, which strikes a balance between MCS and TTAS by utilizing several MCS queues with a common TTAS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u8f7b\u91cf\u7ea7\u7ebf\u7a0b\uff08\u534f\u7a0b\uff09\u73af\u5883\u4e0b\u7684\u4e92\u65a5\u9501\u5b9e\u73b0\uff0c\u53d1\u73b0\u4f20\u7edfOS\u7ebf\u7a0b\u9501\u5728\u8f7b\u91cf\u7ea7\u7ebf\u7a0b\u4e2d\u4f1a\u5bfc\u81f4\u6b7b\u9501\uff0c\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u4e0d\u540c\u8f7b\u91cf\u7ea7\u7ebf\u7a0b\u5e93\u7684TTAS\u548cMCS\u9501\u6539\u8fdb\u65b9\u6848\uff0c\u5e76\u63a8\u8350\u4f7f\u7528cohort\u9501\u4f5c\u4e3a\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u591a\u7ebf\u7a0b\u6570\u636e\u7ed3\u6784\u662f\u4e3a\u64cd\u4f5c\u7cfb\u7edf\u7ebf\u7a0b\u8bbe\u8ba1\u7684\uff0c\u800c\u8f7b\u91cf\u7ea7\u7ebf\u7a0b\uff08\u534f\u7a0b/\u5f02\u6b65\u8c03\u7528\uff09\u7684\u9501\u5b9e\u73b0\u7814\u7a76\u4e0d\u8db3\u3002\u8f7b\u91cf\u7ea7\u7ebf\u7a0b\u867d\u7136\u542f\u52a8\u548c\u4e0a\u4e0b\u6587\u5207\u6362\u5f00\u9500\u4f4e\uff0c\u4f46\u9700\u8981\u624b\u52a8\u8c03\u7528\u4e0a\u4e0b\u6587\u5207\u6362\u624d\u80fd\u5b9e\u73b0\u5e76\u884c\uff0c\u4f20\u7edfOS\u7ebf\u7a0b\u9501\u5728\u8fd9\u79cd\u65b0\u73af\u5883\u4e0b\u65e0\u6cd5\u6709\u6548\u5de5\u4f5c\u4e14\u53ef\u80fd\u5bfc\u81f4\u6b7b\u9501\u3002", "method": "\u9488\u5bf9\u8f7b\u91cf\u7ea7\u7ebf\u7a0b\u73af\u5883\uff0c\u4fee\u6539\u4e86TTAS\u548cMCS\u9501\u7684\u5b9e\u73b0\uff0c\u7279\u522b\u5173\u6ce8\u8f7b\u91cf\u7ea7\u7ebf\u7a0b\u7684\u4e24\u79cd\u4e0a\u4e0b\u6587\u5207\u6362\u673a\u5236\uff1ayielding\uff08\u8ba9\u51fa\uff09\u548csleeping\uff08\u4f11\u7720\uff09\u3002\u5206\u6790\u4e86\u4e0d\u540c\u8f7b\u91cf\u7ea7\u7ebf\u7a0b\u5e93\u7684\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86cohort\u9501\u4f5c\u4e3a\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u9501\u7ed3\u5408\u4e86MCS\u961f\u5217\u548cTTAS\u9501\u7684\u4f18\u70b9\u3002", "result": "\u7814\u7a76\u8868\u660eTTAS\u548cMCS\u9501\u5728\u8f7b\u91cf\u7ea7\u7ebf\u7a0b\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u53d6\u51b3\u4e8e\u5177\u4f53\u8bbe\u7f6e\u3002\u8f7b\u91cf\u7ea7\u7ebf\u7a0b\u7684yielding\u548csleeping\u673a\u5236\u5bf9\u9501\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002cohort\u9501\u901a\u8fc7\u4f7f\u7528\u591a\u4e2aMCS\u961f\u5217\u914d\u5408\u516c\u5171TTAS\u9501\uff0c\u5728\u5404\u79cd\u8f7b\u91cf\u7ea7\u7ebf\u7a0b\u5e93\u4e2d\u90fd\u80fd\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u7ebf\u7a0b\u73af\u5883\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u7684\u4e92\u65a5\u9501\u5b9e\u73b0\uff0c\u4f20\u7edfOS\u7ebf\u7a0b\u9501\u4e0d\u9002\u7528\u3002cohort\u9501\u4f5c\u4e3a\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86MCS\u548cTTAS\u7684\u4f18\u70b9\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u8f7b\u91cf\u7ea7\u7ebf\u7a0b\u5e93\u4e2d\u63d0\u4f9b\u826f\u597d\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u8f7b\u91cf\u7ea7\u7ebf\u7a0b\u73af\u5883\u4e0b\u7684\u9501\u5b9e\u73b0\u95ee\u9898\u3002"}}
{"id": "2512.08698", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.08698", "abs": "https://arxiv.org/abs/2512.08698", "authors": ["Ilya Kokorin", "Evgeny Chernatskiy", "Vitaly Aksenov"], "title": "Model-based Testing of Practical Distributed Systems in Actor Model", "comment": "16 pages", "summary": "Designing and implementing distributed systems correctly can be quite challenging. Although these systems are often accompanied by formal specifications that are verified using model-checking techniques, a gap still exists between the implementation and its formal specification: there is no guarantee that the implementation is free of bugs.\n  To bridge this gap, we can use model-based testing. Specifically, if the model of the system can be interpreted as a finite-state automaton, we can generate an exhaustive test suite for the implementation that covers all possible states and transitions.\n  In this paper, we discuss how to efficiently generate such a test suite for distributed systems written in the actor model. Importantly, our approach does not require any modifications to the code or interfering with the distributed system execution environment. As an example, we verified an implementation of a replication algorithm based on Viewstamped Replication, which is used in a real-world system.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6709\u9650\u72b6\u6001\u81ea\u52a8\u673a\u7684\u6a21\u578b\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u751f\u6210\u5168\u8986\u76d6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u65e0\u9700\u4fee\u6539\u4ee3\u7801\u6216\u5e72\u6270\u6267\u884c\u73af\u5883\uff0c\u4ee5Viewstamped Replication\u590d\u5236\u7b97\u6cd5\u4e3a\u4f8b\u9a8c\u8bc1", "motivation": "\u5206\u5e03\u5f0f\u7cfb\u7edf\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u5b58\u5728\u6311\u6218\uff0c\u5373\u4f7f\u6709\u5f62\u5f0f\u5316\u89c4\u8303\u548c\u6a21\u578b\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e0e\u89c4\u8303\u4e4b\u95f4\u4ecd\u6709\u5dee\u8ddd\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u5b9e\u73b0\u65e0bug", "method": "\u4f7f\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5c06\u7cfb\u7edf\u6a21\u578b\u89e3\u91ca\u4e3a\u6709\u9650\u72b6\u6001\u81ea\u52a8\u673a\uff0c\u751f\u6210\u8986\u76d6\u6240\u6709\u53ef\u80fd\u72b6\u6001\u548c\u8f6c\u6362\u7684\u7a77\u4e3e\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7279\u522b\u9488\u5bf9actor\u6a21\u578b\u7f16\u5199\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u65e0\u9700\u4fee\u6539\u4ee3\u7801\u6216\u5e72\u6270\u6267\u884c\u73af\u5883", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86\u57fa\u4e8eViewstamped Replication\u7684\u590d\u5236\u7b97\u6cd5\u5b9e\u73b0\uff0c\u8be5\u7b97\u6cd5\u7528\u4e8e\u5b9e\u9645\u7cfb\u7edf", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u5f25\u5408\u5206\u5e03\u5f0f\u7cfb\u7edf\u5b9e\u73b0\u4e0e\u5f62\u5f0f\u5316\u89c4\u8303\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u6a21\u578b\u6d4b\u8bd5\u4fdd\u8bc1\u5b9e\u73b0\u6b63\u786e\u6027\uff0c\u4e14\u5bf9\u7cfb\u7edf\u65e0\u4fb5\u5165\u6027"}}
{"id": "2512.08725", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.08725", "abs": "https://arxiv.org/abs/2512.08725", "authors": ["Giulio Attenni", "Youssef Moawad", "Novella Bartolini", "Lauritz Thamsen"], "title": "Spatio-Temporal Shifting to Reduce Carbon, Water, and Land-Use Footprints of Cloud Workloads", "comment": "This is a pre-print of our paper currently under review", "summary": "In this paper, we investigate the potential of spatial and temporal cloud workload shifting to reduce carbon, water, and land-use footprints. Specifically, we perform a simulation study using real-world data from multiple cloud providers (AWS and Azure) and workload traces for different applications (big data analytics and FaaS). Our simulation results indicate that spatial shifting can substantially lower carbon, water, and land use footprints, with observed reductions ranging from 20% to 85%, depending on the scenario and optimization criteria. Temporal shifting also decreases the footprint, though to a lesser extent. When applied together, the two strategies yield the greatest overall reduction, driven mainly by spatial shifting with temporal adjustments providing an additional, incremental benefit. Sensitivity analysis demonstrates that such shifting is robust to prediction errors in grid mix data and to variations across different seasons.", "AI": {"tldr": "\u901a\u8fc7\u65f6\u7a7a\u8f6c\u79fb\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u53ef\u663e\u8457\u964d\u4f4e\u78b3\u3001\u6c34\u548c\u571f\u5730\u4f7f\u7528\u8db3\u8ff9\uff0c\u7a7a\u95f4\u8f6c\u79fb\u6548\u679c\u6700\u660e\u663e\uff0820-85%\uff09\uff0c\u65f6\u95f4\u8f6c\u79fb\u6548\u679c\u8f83\u5c0f\uff0c\u4e24\u8005\u7ed3\u5408\u6548\u679c\u6700\u4f73", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u7684\u65f6\u7a7a\u8f6c\u79fb\u6765\u51cf\u5c11\u4e91\u8ba1\u7b97\u7684\u73af\u5883\u8db3\u8ff9\uff08\u78b3\u3001\u6c34\u3001\u571f\u5730\u4f7f\u7528\uff09\uff0c\u63a2\u7d22\u53ef\u6301\u7eed\u4e91\u8ba1\u7b97\u7684\u53ef\u80fd\u6027", "method": "\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u8fdb\u884c\u6a21\u62df\u7814\u7a76\uff0c\u5305\u62ecAWS\u548cAzure\u4e91\u63d0\u4f9b\u5546\u6570\u636e\uff0c\u4ee5\u53ca\u5927\u6570\u636e\u5206\u6790\u548cFaaS\u5e94\u7528\u7684\u5de5\u4f5c\u8d1f\u8f7d\u8ffd\u8e2a\uff0c\u5206\u6790\u7a7a\u95f4\u8f6c\u79fb\u548c\u65f6\u95f4\u8f6c\u79fb\u7b56\u7565", "result": "\u7a7a\u95f4\u8f6c\u79fb\u80fd\u663e\u8457\u964d\u4f4e\u73af\u5883\u8db3\u8ff9\uff0820-85%\uff09\uff0c\u65f6\u95f4\u8f6c\u79fb\u6548\u679c\u8f83\u5c0f\u4f46\u4ecd\u6709\u5e2e\u52a9\uff0c\u4e24\u8005\u7ed3\u5408\u6548\u679c\u6700\u4f73\uff1b\u654f\u611f\u6027\u5206\u6790\u663e\u793a\u7b56\u7565\u5bf9\u7535\u7f51\u6570\u636e\u9884\u6d4b\u8bef\u5dee\u548c\u5b63\u8282\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027", "conclusion": "\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u7684\u65f6\u7a7a\u8f6c\u79fb\u662f\u51cf\u5c11\u73af\u5883\u8db3\u8ff9\u7684\u6709\u6548\u7b56\u7565\uff0c\u7a7a\u95f4\u8f6c\u79fb\u4e3a\u4e3b\u8981\u9a71\u52a8\u529b\uff0c\u65f6\u95f4\u8f6c\u79fb\u63d0\u4f9b\u989d\u5916\u589e\u91cf\u6548\u76ca\uff0c\u8be5\u7b56\u7565\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u5177\u6709\u9c81\u68d2\u6027"}}
