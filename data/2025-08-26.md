<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.AR](#cs.AR) [Total: 10]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [SafeTree: Expressive Tree Policies for Microservices](https://arxiv.org/abs/2508.16746)
*Karuna Grewal,P. Brighten Godfrey,Justin Hsu*

Main category: cs.PL

TL;DR: 基于可见堆栈自动机的微服务树策略动态执行机制，通过服务网格实现低延迟监控


<details>
  <summary>Details</summary>
Motivation: 现有微服务部署工具仅支持单跳策略，忽略了服务调用树结构，导致策略过于免科。需要更细粒度的通信控制能力。

Method: 设计表达式策略语言来指定服务树结构，使用可见堆栈自动机实现动态执行机制，在Istio服务网格上构建分布式监控器

Result: 监控器能够执行丰富的安全属性，仅添加毫秒级别的延迟开销

Conclusion: 该方法无侵入性，无需修改服务实现或访问代码，通过服务网格实现了高效的微服务树策略执行

Abstract: A microservice-based application is composed of multiple self-contained
components called microservices, and controlling inter-service communication is
important for enforcing safety properties. Presently, inter-service
communication is configured using microservice deployment tools. However, such
tools only support a limited class of single-hop policies, which can be overly
permissive because they ignore the rich service tree structure of microservice
calls. Policies that can express the service tree structure can offer
development and security teams more fine-grained control over communication
patterns.
  To this end, we design an expressive policy language to specify service tree
structures, and we develop a visibly pushdown automata-based dynamic
enforcement mechanism to enforce service tree policies. Our technique is
non-invasive: it does not require any changes to service implementations, and
does not require access to microservice code. To realize our method, we build a
runtime monitor on top of a service mesh, an emerging network infrastructure
layer that can control inter-service communication during deployment. In
particular, we employ the programmable network traffic filtering capabilities
of Istio, a popular service mesh implementation, to implement an online and
distributed monitor. Our experiments show that our monitor can enforce rich
safety properties while adding minimal latency overhead on the order of
milliseconds.

</details>


### [2] [Syntactic Completions with Material Obligations](https://arxiv.org/abs/2508.16848)
*David Moon,Andrew Blinn,Thomas J. Porter,Cyrus Omar*

Main category: cs.PL

TL;DR: 本文提出了tylr，一种新型解析器和编辑器生成器，通过插入义务（obligations）来补全任意格式错误的代码，解决了现有语法错误恢复技术过于粗糙或产生过多补全选项的问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码编辑器在语法错误存在时经常失效，传统的语法错误恢复技术要么删除大量代码（panic模式），要么产生过多可能的补全选项，需要更精细的错误恢复方法。

Method: 基于瓦片解析理论，扩展了运算符优先级解析：1）用语法行走替代传统token优先级比较来生成义务；2）基于语法拉链的模制系统通过义务最小化准则来消歧。还开发了可视化显示义务的编辑器。

Result: 开发了tylr系统，能够处理任意格式错误的代码，通过插入义务来补全缺失的操作数、运算符、混合关键字和类型转换。系统作为文本编辑器和结构编辑器之间的新型混合编辑器。

Conclusion: tylr提供了一种新颖的错误纠正方法，通过人类主体研究评估了其实用性和可用性，发现了积极方面和未来工作的新方向。

Abstract: Code editors provide essential services that help developers understand,
navigate, and modify programs. However, these services often fail in the
presence of syntax errors. Existing syntax error recovery techniques, like
panic mode and multi-option repairs, are either too coarse, e.g. in deleting
large swathes of code, or lead to a proliferation of possible completions. This
paper introduces $\texttt{tylr}$, a parser and editor generator that completes
arbitrarily malformed code by inserting obligations, which generalize holes to
cover missing operands, operators, mixfix keywords, and sort transitions.
$\texttt{tylr}$ is backed by a novel theory of tile-based parsing, which
extends operator-precedence parsing in two ways. First, traditional token
precedence comparisons are replaced by a notion of grammar walks, which form
the basis for generating obligations. Second, a distinct "molding" system based
on grammar zippers expand grammar expressivity by allowing the system to
disambiguate between possible parses and completions based on an obligation
minimization criterion. In addition to serving as a novel approach to error
correction, $\texttt{tylr}$'s design enables the development of an editor that
visually materializes obligations to the human user, serving as a novel hybrid
between a text editor and a structure editor. We introduce $\texttt{tylr}$ by
example, then formalize its key ideas. Finally, we conduct a human subjects
study to evaluate the extent to which an editor like $\texttt{tylr}$ that
materializes syntactic obligations might be usable and useful, finding both
points of positivity and interesting new avenues for future work.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Performance measurements of modern Fortran MPI applications with Score-P](https://arxiv.org/abs/2508.16592)
*Gregor Corbin*

Main category: cs.DC

TL;DR: Score-P工具实现了对MPI Fortran 2008绑定的完整支持，通过Fortran包装器覆盖MPI 4.1标准全部特性，解决了十年来工具支持缺失的问题


<details>
  <summary>Details</summary>
Motivation: MPI 3.0标准在2012年引入了Fortran 2008语言绑定，提供了类型安全和标准兼容的MPI调用方式，但近十年来工具支持仍然缺乏，迫使用户使用不太安全和方便的接口

Method: 通过在Fortran中实现MPI包装器，使用代码生成器产生约5万行MPI包装代码，依赖Python pympistandard模块提供对MPI标准数据的程序化访问

Result: 实现了对MPI 4.1标准的完整支持，包括传递属性、信息对象和回调的MPI过程，已成功应用于两个流体动力学模拟代码（Neko和EPIC）的性能测量

Conclusion: 这项工作填补了MPI F08绑定工具支持的空白，为现代Fortran应用程序提供了完整的性能测量基础设施支持

Abstract: Version 3.0 of the Message-Passing Interface (MPI) standard, released in
2012, introduced a new set of language bindings for Fortran 2008. By making use
of modern language features and the enhanced interoperability with C, there was
finally a type safe and standard conforming method to call MPI from Fortran.
This highly recommended use mpi_f08 language binding has since then been widely
adopted among developers of modern Fortran applications. However, tool support
for the F08 bindings is still lacking almost a decade later, forcing users to
recede to the less safe and convenient interfaces. Full support for the F08
bindings was added to the performance measurement infrastructure Score-P by
implementing MPI wrappers in Fortran. Wrappers cover the latest MPI standard
version 4.1 in its entirety, matching the features of the C wrappers. By
implementing the wrappers in modern Fortran, we can provide full support for
MPI procedures passing attributes, info objects, or callbacks. The
implementation is regularly tested under the MPICH test suite. The new F08
wrappers were already used by two fluid dynamics simulation codes -- Neko, a
spectral finite-element code derived from Nek5000, and EPIC (Elliptical
Parcel-In-Cell) -- to successfully generate performance measurements. In this
work, we additionally present our design considerations and sketch out the
implementation, discussing the challenges we faced in the process. The key
component of the implementation is a code generator that produces approximately
50k lines of MPI wrapper code to be used by Score-P, relying on the Python
pympistandard module to provide programmatic access to the extracted data from
the MPI standard.

</details>


### [4] [GPU Acceleration for Faster Evolutionary Spatial Cyclic Game Systems](https://arxiv.org/abs/2508.16639)
*Louie Sinadjan*

Main category: cs.DC

TL;DR: 开发了基于GPU加速的进化空间循环游戏模拟框架，使用Metal和CUDA实现，相比单线程版本获得最高28倍加速，使更大规模系统模拟成为可能


<details>
  <summary>Details</summary>
Motivation: 传统的单线程进化空间循环游戏模拟计算成本高且扩展性差，需要高性能实现来支持生态和进化动力学研究

Method: 使用Apple Metal和Nvidia CUDA开发GPU加速框架，以验证过的单线程C++版本作为基准对比

Result: GPU加速带来显著性能提升，CUDA实现最高28倍加速，支持最大3200x3200系统规模，Metal存在扩展性限制

Conclusion: 项目提供了可配置的模拟平台，推进了该研究领域的计算工具集，研究成果已被欧洲建模与仿真研讨会接受发表

Abstract: This dissertation presents the design, implementation and evaluation of
GPU-accelerated simulation frameworks for Evolutionary Spatial Cyclic Games
(ESCGs), a class of agent-based models used to study ecological and
evolutionary dynamics. Traditional single-threaded ESCG simulations are
computationally expensive and scale poorly. To address this, high-performance
implementations were developed using Apple's Metal and Nvidia's CUDA, with a
validated single-threaded C++ version serving as a baseline comparison point.
  Benchmarking results show that GPU acceleration delivers significant
speedups, with the CUDA maxStep implementation achieving up to a 28x
improvement. Larger system sizes, up to 3200x3200, became tractable, while
Metal faced scalability limits. The GPU frameworks also enabled replication and
critical extension of recent ESCG studies, revealing sensitivities to system
size and runtime not fully explored in prior work.
  Overall, this project provides a configurable ESCG simulation platform that
advances the computational toolkit for this field of research. This
dissertation forms the basis for a paper accepted for publication and
presentation at the European Modelling and Simulation Symposium.

</details>


### [5] [Equinox: Holistic Fair Scheduling in Serving Large Language Models](https://arxiv.org/abs/2508.16646)
*Zhixiang Wei,James Yen,Jingyi Chen,Ziyang Zhang,Zhibai Huang,Chen Chen,Xingzi Yu,Yicheng Gu,Chenggang Wu,Yun Wang,Mingyuan Xia,Jie Wu,Hao Wang,Zhengwei Qi*

Main category: cs.DC

TL;DR: 提出Equinox系统，通过双计数器框架和MoPE预测专家，实现公平感知的LLM服务调度，在保持高GPU利用率的同时提升吞吐量和降低延迟


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM服务中用户服务质量与资源效率之间的调度矛盾，需要同时考虑用户公平性和资源公平性

Method: 使用用户公平计数器（加权token和延迟）和资源公平计数器（吞吐量和GPU利用率），结合MoPE框架预测关键指标，计算统一的全栈公平分数进行主动调度

Result: 在ShareGPT和LMSYS生产轨迹上，Equinox实现1.3倍吞吐量提升，60%首token延迟降低，13%公平性提升，94% GPU利用率

Conclusion: 双计数器框架和MoPE预测方法有效解决了LLM服务调度中的公平性问题，在异构平台上实现了有界差异的公平服务

Abstract: We address the limitations of current LLM serving with a dual-counter
framework separating user and operator perspectives. The User Fairness Counter
measures quality of service via weighted tokens and latency; the Resource
Fairness Counter measures operational efficiency through throughput and GPU
utilization. Since these metrics are only available post-execution, creating a
scheduling paradox, we introduce a deterministic Mixture of Prediction Experts
(MoPE) framework to predict user-perceived latency, output tokens, throughput,
and GPU utilization. These predictions enable calculation of a unified Holistic
Fairness score that balances both counters through tunable parameters for
proactive fairness-aware scheduling. We implement this in Equinox, an
open-source system with other optimizations like adaptive batching, and
stall-free scheduling. Evaluations on production traces (ShareGPT, LMSYS) and
synthetic workloads demonstrate Equinox achieves up to $1.3\times$ higher
throughput, 60\% lower time-to-first-token latency, and 13\% higher fairness
versus VTC while maintaining 94\% GPU utilization, proving fairness under
bounded discrepancy across heterogeneous platforms.

</details>


### [6] [Neuromorphic Simulation of Drosophila Melanogaster Brain Connectome on Loihi 2](https://arxiv.org/abs/2508.16792)
*Felix Wang,Bradley H. Theilman,Fred Rothganger,William Severa,Craig M. Vineyard,James B. Aimone*

Main category: cs.DC

TL;DR: 首次在神经模式硬件上实现了果蝉全脑连接组模拟，包含14万神经元和5000万突触，在Intel Loihi 2平台上达到了超高速度模拟


<details>
  <summary>Details</summary>
Motivation: 生物神经网络具有稀疏、循环、不规则的连接特征，传统计算方法无法高效处理，需要神经模式硬件来实现生物实际模型

Method: 使用Intel Loihi 2神经模式平台，对FlyWire联盟提供的果蝉全脑连接组进行映射和优化，解决了扇入扇出内存限制等硬件约束

Result: 成功在12块Loihi 2芯片上实现全部连接组，模拟速度比传统硬件快数个数级倍，且活动越稀疏性能优势越明显

Conclusion: 现代神经模式平台已能够实现和加速生物实际模型，为神经受灵智能和计算神经科学提供了关键技术支撑

Abstract: We demonstrate the first-ever nontrivial, biologically realistic connectome
simulated on neuromorphic computing hardware. Specifically, we implement the
whole-brain connectome of the adult Drosophila melanogaster (fruit fly) from
the FlyWire Consortium containing 140K neurons and 50M synapses on the Intel
Loihi 2 neuromorphic platform. This task is particularly challenging due to the
characteristic connectivity structure of biological networks. Unlike artificial
neural networks and most abstracted neural models, real biological circuits
exhibit sparse, recurrent, and irregular connectivity that is poorly suited to
conventional computing methods intended for dense linear algebra. Though
neuromorphic hardware is architecturally better suited to discrete event-based
biological communication, mapping the connectivity structure to frontier
systems still faces challenges from low-level hardware constraints, such as
fan-in and fan-out memory limitations. We describe solutions to these
challenges that allow for the full FlyWire connectome to fit onto 12 Loihi 2
chips. We statistically validate our implementation by comparing network
behavior across multiple reference simulations. Significantly, we achieve a
neuromorphic implementation that is orders of magnitude faster than numerical
simulations on conventional hardware, and we also find that performance
advantages increase with sparser activity. These results affirm that today's
scalable neuromorphic platforms are capable of implementing and accelerating
biologically realistic models -- a key enabling technology for advancing
neuro-inspired AI and computational neuroscience.

</details>


### [7] [PICO: Performance Insights for Collective Operations](https://arxiv.org/abs/2508.16809)
*Saverio Pasqualoni,Lorenzo Piarulli,Daniele De Sensi*

Main category: cs.DC

TL;DR: PICO是一个轻量级、可扩展的框架，用于简化和改进集体操作的性能评估与基准测试


<details>
  <summary>Details</summary>
Motivation: 现有的集体操作性能评估框架缺乏详细的性能分析信息，且无法保证可重现性和可扩展性，无法满足HPC应用和大规模AI训练的需求

Method: 开发了PICO框架，这是一个轻量级、可扩展的框架，专门用于集体操作的基准测试，提供详细的性能分析信息

Result: 提出了PICO框架，能够提供更详细的性能分析，确保测试的可重现性，并具有良好的扩展性

Conclusion: PICO框架解决了现有集体操作性能评估工具的不足，为HPC和AI领域的集体操作提供了更系统、可重现的性能测试解决方案

Abstract: Collective operations are cornerstones of both HPC application and
large-scale AI training and inference. Yet, comprehensive, systematic and
reproducible performance evaluation and benchmarking of said operations is not
straightforward. Existing frameworks do not provide sufficiently detailed
profiling information, nor they ensure reproducibility and extensibility. In
this paper, we present PICO (Performance Insights for Collective Operations), a
novel lightweight, extensible framework built with the aim of simplifying
collective operations benchmarking.

</details>


### [8] [Memory-Efficient Federated Fine-Tuning of Large Language Models via Layer Pruning](https://arxiv.org/abs/2508.17209)
*Yebo Wu,Jingguang Li,Chunlin Tian,Zhijiang Guo,Li Li*

Main category: cs.DC

TL;DR: FedPruner是一种创新的联邦微调范式，通过智能层剪枝解决联邦学习中设备内存限制问题，显著降低内存使用同时提高模型精度


<details>
  <summary>Details</summary>
Motivation: 联邦微调虽然能保护隐私，但高内存成本限制了资源受限设备的参与，需要一种方法在保持性能的同时降低内存需求

Method: 采用宏-微协同剪枝框架：宏观功能驱动的层编排机制分组层，微观重要性感知的层选择策略在组内剪枝；还提出细粒度变体独立剪枝多头注意力和前馈网络组件

Result: 实验结果表明FedPruner显著优于现有方法，平均模型精度提升1.98%，峰值内存使用减少75%

Conclusion: FedPruner通过智能层剪枝有效解决了联邦微调中的内存限制问题，在保持高性能的同时大幅降低内存需求，为资源受限设备参与联邦学习提供了可行方案

Abstract: Federated fine-tuning enables privacy-preserving Large Language Model (LLM)
adaptation, but its high memory cost limits participation from
resource-constrained devices. We propose FedPruner, an innovative federated
fine-tuning paradigm that tackles this via intelligent layer pruning. FedPruner
flexibly prunes the global model, creating personalized submodels based on
device memory constraints. It employs a macro-micro synergistic pruning
framework: a macro-level functionality-driven layer orchestration mechanism
groups layers, while a micro-level importance-aware layer selection strategy
prunes within groups to build device-specific submodels. We further introduce a
fine-grained variant that independently prunes Multi-Head Attention and
Feed-Forward Network components to precisely preserve critical architectural
elements. Extensive experimental results demonstrate that FedPruner
significantly outperforms state-of-the-art approaches, achieving up to a 1.98\%
improvement in average model accuracy while reducing peak memory usage by 75\%.

</details>


### [9] [TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained Elastic Long-Context LLM Serving](https://arxiv.org/abs/2508.17219)
*Bingyang Wu,Zili Zhang,Yinmin Zhong,Guanzhe Huang,Yibo Zhu,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: TokenLake是一个统一的分段级前缀缓存池系统，通过声明式缓存接口和负载均衡算法，解决了现有前缀缓存系统中的负载不平衡、数据冗余和内存碎片问题，显著提升了吞吐量和命中率。


<details>
  <summary>Details</summary>
Motivation: 现有集群级前缀缓存系统与请求调度紧密耦合，导致跨实例的缓存系统出现负载不平衡、数据冗余和内存碎片问题，需要内存池化技术来解耦调度和缓存管理。

Method: 提出TokenLake系统，使用声明式缓存接口暴露查询张量、前缀缓存和缓存感知操作，采用分段级缓存管理和基于热点感知的负载均衡算法，实现缓存负载平衡、去重和碎片整理。

Result: 在真实工作负载评估中，相比最先进的缓存感知路由和缓存中心化PD解耦方案，TokenLake可将吞吐量提升最高2.6倍和2.0倍，命中率提升2.0倍和2.1倍。

Conclusion: TokenLake通过统一的分段级前缀缓存池和高效的缓存管理机制，成功解决了现有前缀缓存系统的问题，为多轮交互和共享前缀请求提供了高效的加速方案。

Abstract: Prefix caching is crucial to accelerate multi-turn interactions and requests
with shared prefixes. At the cluster level, existing prefix caching systems are
tightly coupled with request scheduling to optimize cache efficiency and
computation performance together, leading to load imbalance, data redundancy,
and memory fragmentation of caching systems across instances. To address these
issues, memory pooling is promising to shield the scheduler from the underlying
cache management so that it can focus on the computation optimization. However,
because existing prefix caching systems only transfer increasingly longer
prefix caches between instances, they cannot achieve low-latency memory
pooling.
  To address these problems, we propose a unified segment-level prefix cache
pool, TokenLake. It uses a declarative cache interface to expose requests'
query tensors, prefix caches, and cache-aware operations to TokenLake for
efficient pooling. Powered by this abstraction, TokenLake can manage prefix
cache at the segment level with a heavy-hitter-aware load balancing algorithm
to achieve better cache load balance, deduplication, and defragmentation.
TokenLake also transparently minimizes the communication volume of query
tensors and new caches. Based on TokenLake, the scheduler can schedule requests
elastically by using existing techniques without considering prefix cache
management. Evaluations on real-world workloads show that TokenLake can improve
throughput by up to 2.6$\times$ and 2.0$\times$ and boost hit rate by
2.0$\times$ and 2.1$\times$, compared to state-of-the-art cache-aware routing
and cache-centric PD-disaggregation solutions, respectively.

</details>


### [10] [Bine Trees: Enhancing Collective Operations by Optimizing Communication Locality](https://arxiv.org/abs/2508.17311)
*Daniele De Sensi,Saverio Pasqualoni,Lorenzo Piarulli,Tommaso Bonato,Seydou Ba,Matteo Turisini,Jens Domke,Torsten Hoefler*

Main category: cs.DC

TL;DR: Bine树是一种改进通信局部性的集合通信算法家族，在大型HPC系统上可减少全局链路流量达33%，实现最高5倍的性能加速


<details>
  <summary>Details</summary>
Motivation: 在过载网络的大型HPC系统中，通信局部性对集合操作性能至关重要，特别是当节点组内部全连接但全局连接稀疏时

Method: 提出了Bine（二项式负二进制）树算法家族，保持二项式树和蝶形算法的通用性，同时优化通信局部性

Result: 在四种大规模超级计算机（Dragonfly、Dragonfly+、过载胖树和环面拓扑）上实现了8种基于Bine的集合操作，全局链路流量显著减少，最高获得5倍加速比

Conclusion: Bine树算法在不同向量大小和节点数量下都能一致减少全局链路流量，为大规模HPC系统的集合通信提供了有效的性能优化方案

Abstract: Communication locality plays a key role in the performance of collective
operations on large HPC systems, especially on oversubscribed networks where
groups of nodes are fully connected internally but sparsely linked through
global connections. We present Bine (binomial negabinary) trees, a family of
collective algorithms that improve communication locality. Bine trees maintain
the generality of binomial trees and butterflies while cutting global-link
traffic by up to 33%. We implement eight Bine-based collectives and evaluate
them on four large-scale supercomputers with Dragonfly, Dragonfly+,
oversubscribed fat-tree, and torus topologies, achieving up to 5x speedups and
consistent reductions in global-link traffic across different vector sizes and
node counts.

</details>


### [11] [Easy Acceleration with Distributed Arrays](https://arxiv.org/abs/2508.17493)
*Jeremy Kepner,Chansup Byun,LaToya Anderson,William Arcand,David Bestor,William Bergeron,Alex Bonn,Daniel Burrill,Vijay Gadepally,Ryan Haney,Michael Houle,Matthew Hubbell,Hayden Jananthan,Michael Jones,Piotr Luszczek,Lauren Milechin,Guillermo Morales,Julie Mullen,Andrew Prout,Albert Reuther,Antonio Rosa,Charles Yee,Peter Michaleas*

Main category: cs.DC

TL;DR: 本文通过STREAM内存带宽基准测试展示了分布式数组在各种硬件上的可扩展性能，包括CPU核心、CPU节点和GPU节点，实现了跨多个节点的线性水平扩展，并在MIT SuperCloud数百个节点上达到了超过1 PB/s的持续带宽。


<details>
  <summary>Details</summary>
Motivation: 探索分布式数组作为高效抽象，在保持编程生产力的同时实现垂直（节点内）、水平（跨节点）和时间（跨硬件代际）的可扩展性能，通过数据局部性实现高内存带宽效率。

Method: 使用STREAM内存带宽基准测试对分布式数组在各种硬件上进行性能评估，包括不同年代的CPU核心、CPU节点和GPU节点，测试其可扩展性和内存带宽表现。

Result: 展示了优异的可扩展性能：20年内CPU核心带宽提升10倍，CPU节点带宽提升100倍，5年内GPU节点带宽提升5倍；在数百个MIT SuperCloud节点上实现了超过1 PB/s的持续带宽，水平扩展呈线性增长。

Conclusion: 分布式数组是一种有效的编程抽象，能够实现高度可扩展的性能，特别是在内存带宽方面表现出色，为高性能计算提供了既保持生产力又获得卓越性能的解决方案。

Abstract: High level programming languages and GPU accelerators are powerful enablers
for a wide range of applications. Achieving scalable vertical (within a compute
node), horizontal (across compute nodes), and temporal (over different
generations of hardware) performance while retaining productivity requires
effective abstractions. Distributed arrays are one such abstraction that
enables high level programming to achieve highly scalable performance.
Distributed arrays achieve this performance by deriving parallelism from data
locality, which naturally leads to high memory bandwidth efficiency. This paper
explores distributed array performance using the STREAM memory bandwidth
benchmark on a variety of hardware. Scalable performance is demonstrated within
and across CPU cores, CPU nodes, and GPU nodes. Horizontal scaling across
multiple nodes was linear. The hardware used spans decades and allows a direct
comparison of hardware improvements for memory bandwidth over this time range;
showing a 10x increase in CPU core bandwidth over 20 years, 100x increase in
CPU node bandwidth over 20 years, and 5x increase in GPU node bandwidth over 5
years. Running on hundreds of MIT SuperCloud nodes simultaneously achieved a
sustained bandwidth $>$1 PB/s.

</details>


### [12] [Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD NPUs](https://arxiv.org/abs/2508.17593)
*Aadesh Deshmukh,Venkata Yaswanth Raparti,Samuel Hsu*

Main category: cs.DC

TL;DR: Zen-Attention框架通过系统优化注意力层的DRAM带宽利用，在AMD XDNA NPU上实现注意力块延迟提升4倍，端到端网络延迟提升32%


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型部署在能耗和DRAM带宽受限的设备上，如何高效映射动态注意力层到NPU成为挑战，需要优化tiling、缓存分配和数据移动

Method: 开发Zen-Attention框架，系统探索层折叠、tiling、互连数据移动和张量布局的复杂设计空间，寻找最优解决方案

Result: 在代表性Transformer模型中，注意力块延迟提升4倍，端到端网络延迟提升32%，支持变长输入维度的padding和masking处理

Conclusion: 该框架显著提升了NPU上注意力层的映射效率，为能效受限设备上的Transformer部署提供了有效解决方案

Abstract: Transformer-based deep learning models are increasingly deployed on energy,
and DRAM bandwidth constrained devices such as laptops and gaming consoles,
which presents significant challenges in meeting the latency requirements of
the models. The industry is turning to neural processing units (NPUs) for
superior performance-per-watt (perf/watt); however, efficiently mapping dynamic
attention layers to the NPUs remains a challenging task. For optimizing
perf/watt, AMD XDNA NPUs employ software managed caches and share system memory
with host. This requires substantial engineering effort to unlock efficient
tiling, buffer allocation, and data movement to extract the maximum efficiency
from the device. This paper introduces Zen-Attention, a framework that
optimizes DRAM bandwidth utilization in the attention layer of models by
systematically exploring the complex design space of layer folding, tiling, and
data-movement on the interconnect, and the tensor layouts to come up with an
optimal solution. Our evaluation includes comparative analysis of end-to-end
model latency and specific attention latency in each model. We demonstrate how
the framework enhances mapping capabilities by varying input dimensions, which
require padding and masking in the attention block. For representative
transformer models, the Zen-Attention Framework achieves up to 4x improvement
in the latency of the attention block and up to 32% improvement in end-to-end
network latency compared to the baseline Unfolded- approaches.

</details>


### [13] [ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters at Scale](https://arxiv.org/abs/2508.17624)
*Ge Shi,Hanieh Sadri,Qian Wang,Yu Zhang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: ExpertWeave是一个专门为ESFT适配器设计的服务系统，能够在单个MoE基础模型上同时服务多个专家专用微调模型，大幅减少内存占用并提高资源利用率。


<details>
  <summary>Details</summary>
Motivation: ESFT通过选择性微调MoE模型中最活跃的专家来提升任务特定性能，但现有服务系统无法有效支持这种专家导向的范式，导致资源消耗巨大。

Method: 系统采用虚拟内存辅助的专家权重管理器来共同定位基础模型和适配器专家，避免内存碎片化开销，并使用融合内核进行批量重路由，实现运行时轻量级令牌重定向。

Result: 评估显示，ExpertWeave可以在单个加速器上同时服务16B MoE模型的多个适配器，提供高达94倍的KV缓存容量和18%的吞吐量提升，同时保持模型准确性。

Conclusion: ExpertWeave以非侵入式修改和最小延迟开销的方式无缝集成到现有MoE推理管道中，即使扩展到20个适配器也能保持低开销，仅比单独服务基础模型增加4-11%的延迟。

Abstract: Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large
language models to enhance their task-specific performance by selectively
tuning the top-activated experts for the task. Serving these fine-tuned models
at scale is challenging: deploying merged models in isolation is prohibitively
resource-hungry, while existing multi-adapter serving systems with LoRA-style
additive updates are incompatible with ESFT's expert-oriented paradigm. We
present ExpertWeave, a system that serves multiple ESFT adapters concurrently
over a single shared MoE base model, drastically reducing the memory footprint
and improving resource utilization. To seamlessly integrate into existing
inference pipelines for MoE models with non-intrusive modifications and minimal
latency overhead, ExpertWeave introduces a virtual-memory-assisted expert
weight manager that co-locates base-model and adapter experts without incurring
memory overhead from fragmentation, and a fused kernel for batched rerouting to
enable lightweight redirection of tokens to the appropriate experts at runtime.
Our evaluations show that ExpertWeave can simultaneously serve multiple
adapters of a 16B MoE model on a single accelerator where the baseline runs out
of memory, or provides up to 94x more KV cache capacity and achieves up to 18%
higher throughput while using comparable resources, all without compromising
model accuracy. ExpertWeave maintains low overhead even when scaling to 20
adapters, with a 4-11% latency increase compared with serving the base model
alone. Source code will be released soon.

</details>


### [14] [Scalable Engine and the Performance of Different LLM Models in a SLURM based HPC architecture](https://arxiv.org/abs/2508.17814)
*Anderson de Lima Luiz,Shubham Vijay Kurlekar,Munir Georges*

Main category: cs.DC

TL;DR: 基于SLURM的高性能计算架构，用于部署异构大语言模型到可扩展推理引擎，通过动态资源调度和容器化微服务实现高效的多节点集群管理。


<details>
  <summary>Details</summary>
Motivation: 为了解决在大规模HPC基础设施上高效部署和运行不同规模的LLM模型，实现资源的高效利用和低延迟推理。

Method: 采用SLURM资源管理系统，结合动态资源调度和容器化微服务技术，管理CPU、GPU和内存分配，提供REST API端点和高级工作流。

Result: 小模型（1B-3B参数）可处理128个并发请求且延迟低于50ms，大模型（8B-70B参数）在2个并发用户时达到饱和，延迟超过2秒，容器和调度开销极小。

Conclusion: 该架构能够在大规模HPC基础设施上实现高效、响应迅速且容错性强的LLM推理，为实际应用场景提供了灵活可靠的解决方案。

Abstract: This work elaborates on a High performance computing (HPC) architecture based
on Simple Linux Utility for Resource Management (SLURM) [1] for deploying
heterogeneous Large Language Models (LLMs) into a scalable inference engine.
Dynamic resource scheduling and seamless integration of containerized
microservices have been leveraged herein to manage CPU, GPU, and memory
allocations efficiently in multi-node clusters. Extensive experiments, using
Llama 3.2 (1B and 3B parameters) [2] and Llama 3.1 (8B and 70B) [3], probe
throughput, latency, and concurrency and show that small models can handle up
to 128 concurrent requests at sub-50 ms latency, while for larger models,
saturation happens with as few as two concurrent users, with a latency of more
than 2 seconds. This architecture includes Representational State Transfer
Application Programming Interfaces (REST APIs) [4] endpoints for single and
bulk inferences, as well as advanced workflows such as multi-step "tribunal"
refinement. Experimental results confirm minimal overhead from container and
scheduling activities and show that the approach scales reliably both for batch
and interactive settings. We further illustrate real-world scenarios, including
the deployment of chatbots with retrievalaugmented generation, which helps to
demonstrate the flexibility and robustness of the architecture. The obtained
results pave ways for significantly more efficient, responsive, and
fault-tolerant LLM inference on large-scale HPC infrastructures.

</details>


### [15] [Wait-free Replicated Data Types and Fair Reconciliation](https://arxiv.org/abs/2508.18193)
*Petr Kuznetsov,Maxence Perion,Sara Tucci-Piergiovanni*

Main category: cs.DC

TL;DR: 本文提出了一个基于DAG的复制数据框架，解决最终一致性系统中的操作撤销和客户端饥饿问题，确保稳定收敛和公平进展


<details>
  <summary>Details</summary>
Motivation: 解决最终一致性复制系统中操作频繁撤销和客户端可能饥饿的两个关键挑战

Method: 设计基于DAG的通用复制框架，副本通过协调函数合并本地视图，维护操作序列的稳定前缀

Result: 实现了无等待最终一致复制状态机，保证所有副本共享单调增长的稳定操作前缀，且无客户端饥饿

Conclusion: 该框架成功解决了最终一致性系统中的稳定性和公平性问题，为分布式系统提供了实用的解决方案

Abstract: Replication is a standard way to maintain availability of shared data in
fault-prone distributed systems. To make sure that the data replicas are
up-to-date, they need to synchronize, which typically means engaging the
replicas in waiting for coherent responses from each other. The amount of
waiting depends on the consistency and availability guarantees we impose on the
system. The folklore CAP theory states that strong consistency (the set of
replicas create an illusion of one correct server) and strong availability (the
replicas' states are reachable despite network partitions) cannot be
implemented in the same system. A popular way to deal with this impossibility
is to relax consistency to be only eventual: the replicas eventually converge
to the same state. In return, the replicas can be wait-free, i.e., the clients
can get the data from the closest replica without waiting for other ones.
  Wait-free data replication faces two important challenges. First, the
operations issued by the clients may be constantly revoked, i.e., their effects
can be repeatedly recomputed due to asynchrony and concurrency. Second, even if
some operations eventually stabilize in their effects, a particular client may
still experience starvation if, from some point onward, each of its operations
is later revoked. In this paper, we address these challenges through a general
DAG-based framework for replicated data types, where replicas exchange their
local views and merge them using a reconciliation function. Within this
framework, we design reconciliation functions that implement a wait-free
eventually consistent replicated state machine ensuring both stable convergence
and fair progress. Specifically, every replica maintains a growing sequence of
client operations, and we guarantee that: (1) all replicas share a common,
monotonically growing stable prefix of operations, and (2) no client starves.

</details>


### [16] [Practical GPU Choices for Earth Observation: ResNet-50 Training Throughput on Integrated, Laptop, and Cloud Accelerators](https://arxiv.org/abs/2508.18206)
*Ritvik Chaturvedi*

Main category: cs.DC

TL;DR: 基于ResNet的Sentinel-2影像土地利用分类系统，在三种GPU上实现2倍训练加速并保持高精度


<details>
  <summary>Details</summary>
Motivation: 解决地理空间分析中深度学习模型在消费级和云GPU上的可扩展部署问题

Method: 采用ResNet架构，构建自动化数据获取、预处理、分块、训练和可视化的容器化流水线

Result: 在NVIDIA RTX 3060和Tesla T4上相比Apple M3 Pro实现2倍训练加速，在EuroSAT数据集上保持高分类准确率

Conclusion: 证明了在消费级和免费云GPU上部署深度学习LULC模型进行可扩展地理空间分析的可行性

Abstract: This project implements a ResNet-based pipeline for land use and land cover
(LULC) classification on Sentinel-2 imagery, benchmarked across three
heterogeneous GPUs. The workflow automates data acquisition, geospatial
preprocessing, tiling, model training, and visualization, and is fully
containerized for reproducibility. Performance evaluation reveals up to a 2x
training speed-up on an NVIDIA RTX 3060 and a Tesla T4 compared to the Apple M3
Pro baseline, while maintaining high classification accuracy on the EuroSAT
dataset. These results demonstrate the feasibility of deploying deep learning
LULC models on consumer and free cloud GPUs for scalable geospatial analytics.

</details>


### [17] [Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel](https://arxiv.org/abs/2508.18224)
*Ran Yan,Youhe Jiang,Binhang Yuan*

Main category: cs.DC

TL;DR: Flash Sparse Attention (FSA) 提出了一种新的稀疏注意力核设计，解决了现有NSA方法在小型GQA组上的效率限制，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的Native Sparse Attention (NSA) 在大型GQA组上表现良好，但现代LLM通常使用较小的GQA组，这限制了NSA的应用。需要一种能在各种GQA组大小下都高效的稀疏注意力方法。

Method: 提出了Flash Sparse Attention (FSA)，采用替代的核设计，使NSA计算能够在现代GPU上对各种小型GQA组的流行LLM实现高效运行。

Result: FSA相比原始NSA实现：核级延迟降低最高3.5倍、平均1.6倍；端到端训练速度提升最高1.25倍、平均1.09倍；端到端预填充速度提升最高1.36倍、平均1.11倍。

Conclusion: FSA成功扩展了稀疏注意力的适用性，使其能够在各种GQA配置的现代LLM中实现高效计算，同时保持与全注意力相当的准确性。

Abstract: Recent progress in sparse attention mechanisms has demonstrated strong
potential for reducing the computational cost of long-context training and
inference in large language models (LLMs). Native Sparse Attention (NSA), a
state-of-the-art approach, introduces natively trainable, hardware-aligned
sparse attention that delivers substantial system-level performance gains while
maintaining accuracy comparable to full attention. However, the kernel
implementation of NSA relies on a query-grouping strategy that is efficient
only with large Grouped Query Attention (GQA) sizes, whereas modern LLMs
typically adopt much smaller GQA groups, which limits the applicability of this
sparse algorithmic advance. In this work, we propose Flash Sparse Attention
(FSA), which includes an alternative kernel design that enables efficient NSA
computation across a wide range of popular LLMs with varied smaller GQA group
sizes on modern GPUs. Compared to vanilla NSA kernel implementation, our
empirical evaluation demonstrates that FSA achieves (i) up to 3.5$\times$ and
on average 1.6$\times$ kernel-level latency reduction, (ii) up to 1.25$\times$
and 1.09$\times$ on average end-to-end training speedup on state-of-the-art
LLMs, and (iii) up to 1.36$\times$ and 1.11$\times$ on average end-to-end
prefill speedup on state-of-the-art LLMs. The source code is open-sourced and
publicly available at
https://github.com/Relaxed-System-Lab/Flash-Sparse-Attention.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [TMA-Adaptive FP8 Grouped GEMM: Eliminating Padding Requirements in Low-Precision Training and Inference on Hopper](https://arxiv.org/abs/2508.16584)
*Zhongling Su,Rong Fu,Weihan Cao,Jianfei Gao,Minxi Jin,Zhilin Pei,Hui Wang*

Main category: cs.AR

TL;DR: 通过动态TMA描述符池和双阶段加载存储操作，消除FP8分组GEMM中的填充开销，实现了计算速度提升和内存节省


<details>
  <summary>Details</summary>
Motivation: 现有FP8分组GEMM实现需要将每个分组填充到固定对齐（如128），导致内存和计算开销

Method: 使用TMA描述符池动态适应变化组维度，通过双阶段加载存储操作和对齐悠慢管理来满足内存对齐要求

Result: 实验显示获得1.7%到20.4%的速度提升，内存减少23.8%，同时保持完全数值等价性

Conclusion: 该方法有效消除了分组GEMM中的填充开销，在提升性能的同时节省内存资源

Abstract: Current FP8 grouped GEMM implementations require padding each group to a
fixed alignment (e.g., 128), incurring memory and computational overhead. We
propose \textit{TMA-Adaptive FP8 Grouped GEMM}, which eliminates padding by
dynamically adapting to variable group dimensions via (1) a TMA descriptor pool
with $\log_2(block_M)$ preconfigured descriptors to handle all residual row
cases through dynamic runtime selection and dual-phase load-store operations,
achieving comprehensive coverage with minimal overhead, and (2)
TMA-alignment-aware management to satisfy 16-byte global memory alignment and
128-byte shared memory alignment. Experiments demonstrate 1.7\% to 20.4\% speed
up with up to 23.8\% memory reduction compared to padding operation plus
state-of-the-art FP8 grouped GEMM, while maintaining full numerical equivalence
for valid data. The source code is publicly available at an anonymous
repository: https://github.com/sukoncon/TMA-Adaptive-FP8-Grouped-GEMM.

</details>


### [19] [GPT-OSS-20B: A Comprehensive Deployment-Centric Analysis of OpenAI's Open-Weight Mixture of Experts Model](https://arxiv.org/abs/2508.16700)
*Deepak Kumar,Divakar Yadav,Yash Patel*

Main category: cs.AR

TL;DR: 在H100 GPU上对GPT-OSS-20B混合专家模型进行性能评测，在解码吞吐量、能源效率和显存使用方面都超过了密集基线模型Qwen3-32B和Yi-34B


<details>
  <summary>Details</summary>
Motivation: 评估混合专家（MoE）模型在实际部署中的性能优势，特别是在解码速度、能源效率和显存使用方面的表现

Method: 在单台H100 GPU上进行一致性测试，测量首个token延迟时间（TTFT）、全解码吞吐量（TPOT）、端到端延迟百分位数、带历史关键值的峰值显存以及能源消耗

Result: GPT-OSS-20B在2048个token上下文和64个token解码时，比Qwen3-32B提供了31.8%更高的解码吞吐量、25.8%更低的每1000个token能消和31.7%更少的峰值显存使用，首个token延迟因MoE路由过期略高

Conclusion: 混合专家模型在实际部署中具有显著优势，虽然总参数量较大，但激活参数仅占17.3%，却能够在性能和效率方面超过更大的密集模型，显示了MoE技术在生产环境中的应用价值

Abstract: We present a single-GPU (H100, bf16) evaluation of GPT-OSS-20B
(Mixture-of-Experts; 20.9B total, approx. 3.61B active) against dense baselines
Qwen3-32B and Yi-34B across multiple dimensions. We measure true
time-to-first-token (TTFT), full-decode throughput (TPOT), end-to-end latency
percentiles, peak VRAM with past key values (PKV) held, and energy via a
consistent nvidia-smi-based sampler. At a 2048-token context with 64-token
decode, GPT-OSS-20B delivers higher decode throughput and tokens per Joule than
dense baselines Qwen3-32B and Yi-34B, while substantially reducing peak VRAM
and energy per 1000 generated tokens; its TTFT is higher due to MoE routing
overhead. With only 17.3% of parameters active (3.61B of 20.9B), GPT-OSS-20B
provides about 31.8% higher decode throughput and 25.8% lower energy per 1000
generated tokens than Qwen3-32B at 2048/64, while using 31.7% less peak VRAM.
Normalized by active parameters, GPT-OSS-20B shows markedly stronger
per-active-parameter efficiency (APE), underscoring MoE's deployment
advantages. We do not evaluate accuracy; this is a deployment-focused study. We
release code and consolidated results to enable replication and extension.

</details>


### [20] [zkPHIRE: A Programmable Accelerator for ZKPs over HIgh-degRee, Expressive Gates](https://arxiv.org/abs/2508.16738)
*Alhad Daftardar,Jianqiao Mo,Joey Ah-kiow,Benedikt Bünz,Siddharth Garg,Brandon Reagen*

Main category: cs.AR

TL;DR: 本文提出了一种名为zkPHIRE的新型可编程加速器，通过SumCheck协议高效处理任意自定义门，实现了比CPU快1486倍、比现有技术快11.87倍的性能提升，并能扩展到2^30规模约束问题。


<details>
  <summary>Details</summary>
Motivation: 零知识证明(ZKPs)虽然在各领域有重要应用潜力，但由于计算开销过高而部署受限，特别是在处理复杂高次门时的SumCheck协议挑战。

Method: 设计了一种新型可编程加速器，专门优化SumCheck协议的处理，支持任意自定义门，并将其集成到完整的zkPHIRE加速器系统中。

Result: 实现了比CPU快1000倍以上的几何平均加速比，完整系统达到1486倍加速，在同等面积下比最先进技术快11.87倍，首次支持2^30规模约束问题。

Conclusion: zkPHIRE加速器成功解决了现代ZKP系统中复杂高次门处理的性能瓶颈，为ZKP的大规模实际部署提供了可行的硬件解决方案。

Abstract: Zero-Knowledge Proofs (ZKPs) have emerged as powerful tools for secure and
privacy-preserving computation. ZKPs enable one party to convince another of a
statement's validity without revealing anything else. This capability has
profound implications in many domains, including: machine learning, blockchain,
image authentication, and electronic voting. Despite their potential, ZKPs have
seen limited deployment because of their exceptionally high computational
overhead, which manifests primarily during proof generation. To mitigate these
overheads, a (growing) body of researchers has proposed hardware accelerators
and GPU implementations for kernels and complete protocols. Prior art spans a
wide variety of ZKP schemes that vary significantly in computational overhead,
proof size, verifier cost, protocol setup, and trust. The latest, and widely
used ZKP protocols are intentionally designed to balance these trade-offs. A
particular challenge in modern ZKP systems is supporting complex, high-degree
gates using the SumCheck protocol. We address this challenge with a novel
programmable accelerator that efficiently handles arbitrary custom gates via
SumCheck. Our accelerator achieves upwards of $1000\times$ geomean speedup over
CPU-based SumChecks across a range of gate types. We integrate this unit into a
full-system accelerator, zkPHIRE, which achieves $1486\times$ geomean speedup
over CPU and $11.87\times$ speedup over the state-of-the-art at iso-area.
zkPHIRE is the first accelerator to scale to problem sizes of $2^{30}$ nominal
constraints while maintaining small proof sizes and programmability.

</details>


### [21] [X-HEEP: An Open-Source, Configurable and Extendible RISC-V Platform for TinyAI Applications](https://arxiv.org/abs/2508.16959)
*Simone Machetti,Pasquale Davide Schiavone,Giovanni Ansaloni,Miguel Peón-Quirós,David Atienza*

Main category: cs.AR

TL;DR: X-HEEP是一个开源的、可配置的RISC-V超低功耗边缘计算平台，专为TinyAI应用设计，具有可扩展加速器接口和多种开发流程支持。


<details>
  <summary>Details</summary>
Motivation: 为超低功耗边缘AI应用提供一个灵活、可配置的开源硬件平台，解决现有平台在加速器集成和配置灵活性方面的不足。

Method: 开发了eXtendible Accelerator InterFace (XAIF)接口，支持各种加速器的无缝集成，提供核心、内存、总线和外设的广泛内部配置，支持FPGA原型设计、ASIC实现和混合SystemC-RTL建模。

Result: 在TSMC 65nm CMOS技术下实现，工作频率300MHz，面积仅0.15mm²，漏电功耗29uW。与近内存加速器集成后，相比纯CPU执行实现了7.3倍性能提升和3.6倍能效改善。

Conclusion: X-HEEP作为一个高度可配置的低功耗平台，成功展示了其在边缘AI应用中的有效性，为异构系统设计提供了优秀的宿主平台解决方案。

Abstract: In this work, we present X-HEEP, an open-source, configurable, and extendible
RISC-V platform for ultra-low-power edge applications (TinyAI). X-HEEP features
the eXtendible Accelerator InterFace (XAIF), which enables seamless integration
of accelerators with varying requirements along with an extensive internal
configuration of cores, memory, bus, and peripherals. Moreover, it supports
various development flows, including FPGA prototyping, ASIC implementation, and
mixed SystemC-RTL modeling, enabling efficient exploration and optimization.
Implemented in TSMC's 65 nm CMOS technology (300 MHz, 0.8 V), X-HEEP achieves a
minimal footprint of only 0.15 mm2 and consumes just 29 uW of leakage power. As
a demonstrator of the configurability and low overhead of X-HEEP as a host
platform, we present a study integrating it with near-memory accelerators
targeting early-exit dynamic network applications, achieving up to 7.3 x
performance speedup and 3.6 x energy improvement on the resulting heterogeneous
system compared to CPU-only execution.

</details>


### [22] [Invited Paper: FEMU: An Open-Source and Configurable Emulation Framework for Prototyping TinyAI Heterogeneous Systems](https://arxiv.org/abs/2508.16981)
*Simone Machetti,Deniz Kasap,Juan Sapriza,Rubén Rodríguez Álvarez,Hossein Taji,José Miranda,Miguel Peón-Quirós,David Atienza*

Main category: cs.AR

TL;DR: FEMU是一个开源的FPGA仿真框架，用于快速原型设计和评估TinyAI异构系统，结合了可重构硬件区域和软件控制区域。


<details>
  <summary>Details</summary>
Motivation: 为了提供快速原型设计和评估TinyAI异构系统的解决方案，需要一种能够结合硬件实现和软件环境的仿真框架。

Method: 利用SoC FPGA的能力，在可重构硬件区域实现待开发的异构系统，在控制软件区域运行标准操作系统进行监督和通信。通过X-HEEP-FEMU平台实例化该框架，集成X-HEEP主机、Linux Python环境和基于硅实现的能量模型。

Result: 开发了X-HEEP-FEMU平台，部署在Xilinx Zynq-7020 SoC上，成功集成了硬件组件和软件环境。

Conclusion: FEMU框架为TinyAI异构系统的快速原型设计和评估提供了有效的解决方案，通过硬件软件协同仿真实现了高效开发。

Abstract: In this paper, we present the new FPGA EMUlation (FEMU), an open-source and
configurable emulation framework for prototyping and evaluating TinyAI
heterogeneous systems (HS). FEMU leverages the capability of system-on-chip
(SoC)-based FPGAs to combine the under-development HS implemented in a
reconfigurable hardware region (RH) for quick prototyping with a software
environment running under a standard operating system in a control software
region (CS) for supervision and communication. To evaluate our approach, we
built the X-HEEP FPGA EMUlation (X-HEEP-FEMU) platform by instantiating the
proposed framework with real-world hardware and software components.
X-HEEP-FEMU is deployed on the Xilinx Zynq-7020 SoC and integrates the
eXtendible Heterogeneous Energy Efficient Platform (X-HEEP) host in the RH, a
Linux-based Python environment on the ARM Cortex-A9 CS, and energy models
derived from a TSMC 65 nm CMOS silicon implementation of X-HEEP, called
HEEPocrates.

</details>


### [23] [Optimizing Neural Networks with Learnable Non-Linear Activation Functions via Lookup-Based FPGA Acceleration](https://arxiv.org/abs/2508.17069)
*Mengyuan Yin,Benjamin Chen Ming Choong,Chuping Qu,Rick Siow Mong Goh,Weng-Fai Wong,Tao Luo*

Main category: cs.AR

TL;DR: 通过FPGA可重构查找表架构，解决学习激活函数在边缘AI中的计算复杂性问题，实现了超10^4倍的能消效率提升


<details>
  <summary>Details</summary>
Motivation: 学习激活函数（如KANs）虽然准确性和可解释性更好，但计算复杂度高，在边缘AI部署中面临着严重的能消和延迟挑战

Method: 采用可重构查找表架构，结合细粒度量化和适配性查找表，减少能耗密集的算术运算，保持激活函数保真度

Result: 在KANs上评测显示，该FPGA设计实现了更高的计算速度和超10^4倍的能消效率，同时保持了准确性和最小的占地费用

Conclusion: 该方法为能源关键的边缘AI提供了实用的解决方案，突破了以往适配性激活网络因计算强度和功耗限制而无法在边缘部署的问题

Abstract: Learned activation functions in models like Kolmogorov-Arnold Networks (KANs)
outperform fixed-activation architectures in terms of accuracy and
interpretability; however, their computational complexity poses critical
challenges for energy-constrained edge AI deployments. Conventional CPUs/GPUs
incur prohibitive latency and power costs when evaluating higher order
activations, limiting deployability under ultra-tight energy budgets. We
address this via a reconfigurable lookup architecture with edge FPGAs. By
coupling fine-grained quantization with adaptive lookup tables, our design
minimizes energy-intensive arithmetic operations while preserving activation
fidelity. FPGA reconfigurability enables dynamic hardware specialization for
learned functions, a key advantage for edge systems that require
post-deployment adaptability. Evaluations using KANs - where unique activation
functions play a critical role - demonstrate that our FPGA-based design
achieves superior computational speed and over $10^4$ times higher energy
efficiency compared to edge CPUs and GPUs, while maintaining matching accuracy
and minimal footprint overhead. This breakthrough positions our approach as a
practical enabler for energy-critical edge AI, where computational intensity
and power constraints traditionally preclude the use of adaptive activation
networks.

</details>


### [24] [A 28nm 1.80Mb/mm2 Digital/Analog Hybrid SRAM-CIM Macro Using 2D-Weighted Capacitor Array for Complex Number Mac Operations](https://arxiv.org/abs/2508.17562)
*Shota Konno,Che-Kai Liu,Sigang Ryu,Samuel Spetalnick,Arijit Raychowdhury*

Main category: cs.AR

TL;DR: 28nm 6T-SRAM数字模拟混合存内计算宏，支持复数MAC运算，采用2D加权电容阵列实现高精度低面积开销


<details>
  <summary>Details</summary>
Motivation: 传统存内计算方案在精度和面积开销方面存在局限，需要一种既能保持高精度又能降低硬件复杂度的混合计算架构

Method: 采用数字模拟混合配置，数字CIM仅应用于高位，模拟CIM应用于低位，引入2D加权电容阵列，无需输入DAC，通过单次转换输出复数实部和虚部

Result: 实现1.80 Mb/mm²的内存密度和0.435%的RMS误差，显著提升了计算精度并降低了面积开销

Conclusion: 该混合存内计算架构成功解决了传统方案的精度与面积权衡问题，为复数运算提供了高效硬件实现方案

Abstract: A 28nm dense 6T-SRAM Digital(D)/Analog(A) Hybrid compute-in-memory (CIM)
macro supporting complex num-ber MAC operation is presented. By introducing a
2D-weighted Capacitor Array, a hybrid configuration is adopted where digital
CIM is applied only to the upper bits and ana-log CIM is applied to the rest,
without the need for input DACs resulting in improved accuracy and lower area
overhead. The CIM prototype macro achieves 1.80 Mb/mm2 memory density and
0.435% RMS error. Complex CIM unit outputs real and imaginary part with a
single conversion to reduce latency.

</details>


### [25] [In-Memory Computing Enabled Deep MIMO Detection to Support Ultra-Low-Latency Communications](https://arxiv.org/abs/2508.17820)
*Tingyu Ding,Qunsong Zeng,Kaibin Huang*

Main category: cs.AR

TL;DR: 基于内存计算技术的深度IM-MIMO检测器，通过渠道依赖/独立模块分解和专用训练方法，实现了纳秒级MVM运算，满足6G极端延迟需求


<details>
  <summary>Details</summary>
Motivation: 6G网络对MIMO系统提出了极端延迟(0.1毫秒)和可靠性要求，传统深度展开检测器无法满足硬件实现要求，需要软硬件协同设计

Method: 提出deep IM-MIMO检测器架构：1)将计算模块分解为渠道依赖和渠道独立部分，减少存储器重编程延迟；2)采用利用存储器值统计知识的专用训练方法，提升对编程噪声的鲁棗性

Result: 对IM-MIMO检测器进行了全面性能分析，评估了检测准确性、处理延迟和硬件复杂度，并定量分析了渠道噪声、存储器编程噪声和神经网络规模对检测错误的影响

Conclusion: 该方法通过内存计算技术实现了纳秒级矩阵向量乘法运算，有效满足6G网络的极端延迟要求，为高速无线通信提供了重要的硬件加速解决方案

Abstract: The development of sixth-generation (6G) mobile networks imposes
unprecedented latency and reliability demands on multiple-input multiple-output
(MIMO) communication systems, a key enabler of high-speed radio access.
Recently, deep unfolding-based detectors, which map iterative algorithms onto
neural network architectures, have emerged as a promising approach, combining
the strengths of model-driven and data-driven methods to achieve high detection
accuracy with relatively low complexity. However, algorithmic innovation alone
is insufficient; software-hardware co-design is essential to meet the extreme
latency requirements of 6G (i.e., 0.1 milliseconds). This motivates us to
propose leveraging in-memory computing, which is an analog computing technology
that integrates memory and computation within memristor circuits, to perform
the intensive matrix-vector multiplication (MVM) operations inherent in deep
MIMO detection at the nanosecond scale. Specifically, we introduce a novel
architecture, called the deep in-memory MIMO (IM-MIMO) detector, characterized
by two key features. First, each of its cascaded computational blocks is
decomposed into channel-dependent and channel-independent neural network
modules. Such a design minimizes the latency of memristor reprogramming in
response to channel variations, which significantly exceeds computation time.
Second, we develop a customized detector-training method that exploits prior
knowledge of memristor-value statistics to enhance robustness against
programming noise. Furthermore, we conduct a comprehensive analysis of the
IM-MIMO detector's performance, evaluating detection accuracy, processing
latency, and hardware complexity. Our study quantifies detection error as a
function of various factors, including channel noise, memristor programming
noise, and neural network size.

</details>


### [26] [LLMulator: Generalizable Cost Modeling for Dataflow Accelerators with Input-Adaptive Control Flow](https://arxiv.org/abs/2508.17826)
*Kaiyan Chang,Wenlong Zhu,Shengwen Liang,Huawei Li,Ying Wang*

Main category: cs.AR

TL;DR: LLMulator是一个基于大语言模型的性能预测框架，通过将性能值视为分类标记序列，结合强化学习动态校准和渐进式数据增强，实现了跨架构、应用和输入相关控制流的鲁棒性能预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在跨架构、跨应用和输入相关控制流的情况下进行通用性能预测，需要一种更鲁棒和硬件感知的预测方法。

Method: 利用预训练大语言模型的程序语义知识，将性能值作为分类标记序列处理；引入基于强化学习的动态校准方法处理输入相关控制流；开发渐进式数据增强策略生成多样化数据集。

Result: 动态校准方法将周期预测误差降低9.7%，经过几次迭代后误差收敛至11.2%；数据增强策略显著提升了跨架构和配置的预测准确性。

Conclusion: LLMulator框架通过结合大语言模型的语义知识和创新的校准方法，为数据流加速器提供了通用且准确的性能预测解决方案。

Abstract: Accurate and fast performance prediction for dataflow-based accelerators is
vital for efficient hardware design and design space exploration, yet existing
methods struggle to generalize across architectures, applications, and
input-dependent control flows. We present LLMulator, a progressive numeric
modeling framework leveraging the program semantic knowledge of pre-trained
large language models (LLMs) for robust, hardware- and application-aware
prediction. Our numeric model treats performance values as categorical token
sequences, enabling range-agnostic estimates and confidence-aware predictions
for unseen applications. To handle input-dependent control flows, we introduce
a reinforcement learning-based dynamic calibration method, reducing cycle
prediction error by 9.7% over static models and converging to 11.2% error after
a few iterations. For cross-hardware generalization, we develop a progressive
data augmentation strategy that generates diverse datasets covering multi-level
dataflow structures, memory parameters, and loop mapping primitives,
significantly boosting prediction accuracy across architectures and
configurations.

</details>


### [27] [Anatomy of the gem5 Simulator: AtomicSimpleCPU, TimingSimpleCPU, O3CPU, and Their Interaction with the Ruby Memory System](https://arxiv.org/abs/2508.18043)
*Johan Söderström,Yuan Yao*

Main category: cs.AR

TL;DR: 这篇报告分析了gem5模拟器中三种主要CPU模型的性能瓶颈，发现内存子系统在顺序CPU中占用大部分执行时间，而乱序CPU则将更多时间用于指令处理和流水线阶段。


<details>
  <summary>Details</summary>
Motivation: gem5作为一款流行的计算机系统模拟器，存在模拟时间长、学习曲线渐等问题。研究者希望通过分析不同CPU模型的执行流程和性能特征，为优化gem5性能提供基础。

Method: 使用基于Linux perf_event接口的轻量级分析器，配置选项来目标定位特定函数并详细检查它们的交互。通过在广泛的性能测试集上对每种CPU进行分析，识别软件瓶颈。

Result: 结果显示Ruby内存子系统在顺序CPU（AS和TS）中占据了大部分执行时间，主要在指令取回阶段。相比之下，乱序CPU（O3）在Ruby中花费的时间比例较小，大部分时间用于构建指令实例和各种CPU流水线阶段。

Conclusion: 这种对每种CPU执行流程的解剖视图对教育目的很有价值，清晰地展示了模拟组件之间的交互。这些见解为优化gem5性能奠定了基础，特别是对AS、TS和O3 CPU。该分析框架还可应用于其他gem5组件的分析或新模型的开发评估。

Abstract: gem5 is a popular modular-based computer system simulator, widely used in
computer architecture research and known for its long simulation time and steep
learning curve. This report examines its three major CPU models: the
AtomicSimpleCPU (AS CPU), the TimingSimpleCPU (TS CPU), the Out-of-order (O3)
CPU, and their interactions with the memory subsystem. We provide a detailed
anatomical overview of each CPU's function call-chains and present how gem5
partitions its execution time for each simulated hardware layer.
  We perform our analysis using a lightweight profiler built on Linux's
perf_event interface, with user-configurable options to target specific
functions and examine their interactions in detail. By profiling each CPU
across a wide selection of benchmarks, we identify their software bottlenecks.
Our results show that the Ruby memory subsystem consistently accounts for the
largest share of execution time in the sequential AS and TS CPUs, primarily
during the instruction fetch stage. In contrast, the O3 CPU spends a relatively
smaller fraction of time in Ruby, with most of its time devoted to constructing
instruction instances and the various pipeline stages of the CPU.
  We believe that the anatomical view of each CPU's execution flow is valuable
for educational purposes, as it clearly illustrates the interactions among
simulated components. These insights form a foundation for optimizing gem5's
performance, particularly for the AS, TS, and O3 CPUs. Moreover, our framework
can be readily applied to analyze other gem5 components or to develop and
evaluate new models.

</details>
