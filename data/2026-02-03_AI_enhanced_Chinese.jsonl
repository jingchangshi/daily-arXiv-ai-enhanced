{"id": "2602.00330", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.00330", "abs": "https://arxiv.org/abs/2602.00330", "authors": ["Sheldon X. -D. Tan", "Haotian Lu"], "title": "Accelerating Physics-Based Electromigration Analysis via Rational Krylov Subspaces", "comment": null, "summary": "Electromigration (EM) induced stress evolution is a major reliability challenge in nanometer-scale VLSI interconnects. Accurate EM analysis requires solving stress-governing partial differential equations over large interconnect trees, which is computationally expensive using conventional finite-difference methods. This work proposes two fast EM stress analysis techniques based on rational Krylov subspace reduction. Unlike traditional Krylov methods that expand around zero frequency, rational Krylov methods enable expansion at selected time constants, aligning directly with metrics such as nucleation and steady-state times and producing compact reduced models with minimal accuracy loss. Two complementary frameworks are developed: a frequency-domain extended rational Krylov method, ExtRaKrylovEM, and a time-domain rational Krylov exponential integration method, EiRaKrylovEM. We show that the accuracy of both methods depends strongly on the choice of expansion point, or shift time, and demonstrate that effective shift times are typically close to times of interest such as nucleation or post-void steady state. Based on this observation, a coordinate descent optimization strategy is introduced to automatically determine optimal reduction orders and shift times for both nucleation and post-void phases. Experimental results on synthesized structures and industry-scale power grids show that the proposed methods achieve orders-of-magnitude improvements in efficiency and accuracy over finite-difference solutions. Using only 4 to 6 Krylov orders, the methods achieve sub-0.1 percent error in nucleation time and resistance change predictions while delivering 20 to 500 times speedup. In contrast, standard extended Krylov methods require more than 50 orders and still incur 10 to 20 percent nucleation time error, limiting their practicality for EM-aware optimization and stochastic EM analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u6709\u7406Krylov\u5b50\u7a7a\u95f4\u7f29\u51cf\u7684\u5feb\u901f\u7535\u8fc1\u79fb\u5e94\u529b\u5206\u6790\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u6709\u9650\u5dee\u5206\u6cd5\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u7684\u6548\u7387\u63d0\u5347\u548c\u7cbe\u5ea6\u6539\u5584\u3002", "motivation": "\u7535\u8fc1\u79fb\u5f15\u8d77\u7684\u5e94\u529b\u6f14\u5316\u662f\u7eb3\u7c73\u7ea7VLSI\u4e92\u8fde\u7684\u4e3b\u8981\u53ef\u9760\u6027\u6311\u6218\u3002\u4f20\u7edf\u6709\u9650\u5dee\u5206\u6cd5\u6c42\u89e3\u5e94\u529b\u63a7\u5236\u504f\u5fae\u5206\u65b9\u7a0b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684EM\u5e94\u529b\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u6709\u7406Krylov\u5b50\u7a7a\u95f4\u7f29\u51cf\u65b9\u6cd5\uff1a\u9891\u57df\u6269\u5c55\u6709\u7406Krylov\u65b9\u6cd5(ExtRaKrylovEM)\u548c\u65f6\u57df\u6709\u7406Krylov\u6307\u6570\u79ef\u5206\u65b9\u6cd5(EiRaKrylovEM)\u3002\u901a\u8fc7\u5750\u6807\u4e0b\u964d\u4f18\u5316\u7b56\u7565\u81ea\u52a8\u786e\u5b9a\u6700\u4f18\u7f29\u51cf\u9636\u6570\u548c\u504f\u79fb\u65f6\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u97004-6\u4e2aKrylov\u9636\u6570\u5373\u53ef\u5b9e\u73b0\u4e9a0.1%\u7684\u6210\u6838\u65f6\u95f4\u548c\u7535\u963b\u53d8\u5316\u9884\u6d4b\u8bef\u5dee\uff0c\u540c\u65f6\u83b7\u5f9720-500\u500d\u7684\u52a0\u901f\u3002\u800c\u6807\u51c6\u6269\u5c55Krylov\u65b9\u6cd5\u9700\u898150\u591a\u4e2a\u9636\u6570\u4e14\u4ecd\u670910-20%\u7684\u6210\u6838\u65f6\u95f4\u8bef\u5dee\u3002", "conclusion": "\u63d0\u51fa\u7684\u6709\u7406Krylov\u65b9\u6cd5\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3aEM\u611f\u77e5\u4f18\u5316\u548c\u968f\u673aEM\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u4e92\u8fde\u6811\u548c\u5de5\u4e1a\u7ea7\u7535\u6e90\u7f51\u683c\u5206\u6790\u3002"}}
{"id": "2602.00342", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.00342", "abs": "https://arxiv.org/abs/2602.00342", "authors": ["Rafi Zahedi", "Amirhossein Ahmadian", "Chen Zhang", "Shashank Narayana Gowda", "Kourosh SedghiSigarchi", "Rajit Gadh"], "title": "Optimal Engagement of Residential Battery Storage to Alleviate Grid Upgrades Caused by EVs and Solar Systems", "comment": "Published at Advances in Science, Technology and Engineering Systems Journal (ASTESJ)- https://www.astesj.com/v09/i02/p01/", "summary": "The integration of distributed energy resources has ushered in a host of complex challenges, significantly impacting power quality in distribution networks. This work studies these challenges, exploring issues such as voltage fluctuations and escalating power losses caused by the integration of solar systems and electric vehicle (EV) chargers. We present a robust methodology focused on mitigating voltage deviations and power losses, emphasizing the allocation of a Permitted Percentage (PP) of battery-based solar systems within residential areas endowed with storage capabilities. A key facet of this research lies in its adaptability to the changing landscape of electric transportation. With the rapid increase of electric trucks on the horizon, our proposed model gains relevance. By tactically deploying PP to oversee the charging and discharging of batteries within residential solar systems, utilities are poised not only to assist with grid resilience but also to cater to the upcoming demands spurred by the advent of new EVs, notably trucks. To validate the efficacy of our proposed model, rigorous simulations were conducted using the IEEE 33-bus distribution network as a designed testbed. Leveraging advanced Particle Swarm Optimization techniques, we have deciphered the optimal charging and discharging commands issued by utilities to energy storage systems. The outcomes of these simulations help us understand the transformative potential of various PP allocations, shedding light on the balance between non-battery-based and battery-based solar residences. This research underscores the need for carefully crafted approaches in navigating the complexities of modern grid dynamics amid the anticipated increase in electric vehicles.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7535\u6c60\u50a8\u80fd\u592a\u9633\u80fd\u7cfb\u7edf\u7684\u5141\u8bb8\u767e\u5206\u6bd4\u5206\u914d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5145\u653e\u7535\u7b56\u7565\u6765\u7f13\u89e3\u5206\u5e03\u5f0f\u80fd\u6e90\u63a5\u5165\u5e26\u6765\u7684\u7535\u538b\u6ce2\u52a8\u548c\u529f\u7387\u635f\u8017\u95ee\u9898\uff0c\u7279\u522b\u9488\u5bf9\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u9700\u6c42\u589e\u957f", "motivation": "\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\uff08\u7279\u522b\u662f\u592a\u9633\u80fd\u7cfb\u7edf\u548c\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u5668\uff09\u7684\u96c6\u6210\u7ed9\u914d\u7535\u7f51\u5e26\u6765\u4e86\u7535\u538b\u6ce2\u52a8\u548c\u529f\u7387\u635f\u8017\u7b49\u7535\u80fd\u8d28\u91cf\u95ee\u9898\uff0c\u968f\u7740\u7535\u52a8\u5361\u8f66\u7b49\u65b0\u578b\u7535\u52a8\u6c7d\u8f66\u7684\u9884\u671f\u589e\u957f\uff0c\u8fd9\u4e9b\u95ee\u9898\u5c06\u66f4\u52a0\u4e25\u5cfb", "method": "\u91c7\u7528\u5141\u8bb8\u767e\u5206\u6bd4\u5206\u914d\u7b56\u7565\uff0c\u5728\u5177\u6709\u50a8\u80fd\u80fd\u529b\u7684\u4f4f\u5b85\u592a\u9633\u80fd\u7cfb\u7edf\u4e2d\u90e8\u7f72\u7535\u6c60\u50a8\u80fd\u7cfb\u7edf\uff0c\u5229\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\u786e\u5b9a\u6700\u4f18\u5145\u653e\u7535\u6307\u4ee4\uff0c\u5728IEEE 33\u8282\u70b9\u914d\u7535\u7f51\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u4eff\u771f\u9a8c\u8bc1", "result": "\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u4e0d\u540c\u5141\u8bb8\u767e\u5206\u6bd4\u5206\u914d\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u7535\u6c60\u50a8\u80fd\u4e0e\u975e\u7535\u6c60\u50a8\u80fd\u592a\u9633\u80fd\u4f4f\u5b85\u4e4b\u95f4\u7684\u5e73\u8861\u5173\u7cfb\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f13\u89e3\u7535\u538b\u504f\u5dee\u548c\u529f\u7387\u635f\u8017", "conclusion": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u7684\u9884\u671f\u589e\u957f\uff0c\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u73b0\u4ee3\u7535\u7f51\u52a8\u6001\u7684\u590d\u6742\u6027\uff0c\u5141\u8bb8\u767e\u5206\u6bd4\u5206\u914d\u7b56\u7565\u4e3a\u7535\u7f51\u97e7\u6027\u548c\u6ee1\u8db3\u672a\u6765\u7535\u52a8\u6c7d\u8f66\u9700\u6c42\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.00803", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.00803", "abs": "https://arxiv.org/abs/2602.00803", "authors": ["Seungkwan Kang", "Seungjun Lee", "Donghyun Gouk", "Miryeong Kwon", "Hyunkyu Choi", "Junhyeok Jang", "Sangwon Lee", "Huiwon Choi", "Jie Zhang", "Wonil Choi", "Mahmut Taylan Kandemir", "Myoungsoo Jung"], "title": "AutoGNN: End-to-End Hardware-Driven Graph Preprocessing for Enhanced GNN Performance", "comment": null, "summary": "Graph neural network (GNN) inference faces significant bottlenecks in preprocessing, which often dominate overall inference latency. We introduce AutoGNN, an FPGA-based accelerator designed to address these challenges by leveraging FPGA's reconfigurability and specialized components. AutoGNN adapts to diverse graph inputs, efficiently performing computationally intensive tasks such as graph conversion and sampling. By utilizing components like adder trees, AutoGNN executes reduction operations in constant time, overcoming the limitations of serialization and synchronization on GPUs.\n  AutoGNN integrates unified processing elements (UPEs) and single-cycle reducers (SCRs) to streamline GNN preprocessing. UPEs enable scalable parallel processing for edge sorting and unique vertex selection, while SCRs efficiently handle sequential tasks such as pointer array construction and subgraph reindexing. A user-level software framework dynamically profiles graph inputs, determines optimal configurations, and reprograms AutoGNN to handle varying workloads. Implemented on a 7$n$m enterprise FPGA, AutoGNN achieves up to 9.0$\\times$ and 2.1$\\times$ speedup compared to conventional and GPU-accelerated preprocessing systems, respectively, enabling high-performance GNN preprocessing across diverse datasets.", "AI": {"tldr": "AutoGNN\u662f\u4e00\u4e2a\u57fa\u4e8eFPGA\u7684GNN\u63a8\u7406\u52a0\u901f\u5668\uff0c\u4e13\u6ce8\u4e8e\u89e3\u51b3\u9884\u5904\u7406\u74f6\u9888\uff0c\u901a\u8fc7\u53ef\u91cd\u6784\u67b6\u6784\u548c\u4e13\u7528\u7ec4\u4ef6\u5b9e\u73b0\u9ad8\u6548\u56fe\u8f6c\u6362\u548c\u91c7\u6837\uff0c\u76f8\u6bd4\u4f20\u7edf\u548cGPU\u52a0\u901f\u7cfb\u7edf\u5206\u522b\u83b7\u5f979.0\u500d\u548c2.1\u500d\u7684\u52a0\u901f\u3002", "motivation": "GNN\u63a8\u7406\u4e2d\u7684\u9884\u5904\u7406\u9636\u6bb5\u5b58\u5728\u663e\u8457\u74f6\u9888\uff0c\u5f80\u5f80\u4e3b\u5bfc\u6574\u4f53\u63a8\u7406\u5ef6\u8fdf\u3002\u73b0\u6709GPU\u89e3\u51b3\u65b9\u6848\u5728\u5904\u7406\u56fe\u6570\u636e\u65f6\u9762\u4e34\u4e32\u884c\u5316\u548c\u540c\u6b65\u9650\u5236\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u65b9\u6848\u3002", "method": "\u91c7\u7528FPGA\u53ef\u91cd\u6784\u67b6\u6784\uff0c\u8bbe\u8ba1\u7edf\u4e00\u5904\u7406\u5355\u5143(UPEs)\u548c\u5355\u5468\u671f\u5f52\u7ea6\u5668(SCRs)\u3002UPEs\u7528\u4e8e\u8fb9\u7f18\u6392\u5e8f\u548c\u9876\u70b9\u9009\u62e9\u7684\u5e76\u884c\u5904\u7406\uff0cSCRs\u5904\u7406\u6307\u9488\u6570\u7ec4\u6784\u5efa\u548c\u5b50\u56fe\u91cd\u7d22\u5f15\u7b49\u987a\u5e8f\u4efb\u52a1\u3002\u901a\u8fc7\u7528\u6237\u7ea7\u8f6f\u4ef6\u6846\u67b6\u52a8\u6001\u5206\u6790\u56fe\u8f93\u5165\u7279\u5f81\uff0c\u786e\u5b9a\u6700\u4f18\u914d\u7f6e\u5e76\u91cd\u7f16\u7a0bFPGA\u3002", "result": "\u57287nm\u4f01\u4e1a\u7ea7FPGA\u4e0a\u5b9e\u73b0\uff0c\u76f8\u6bd4\u4f20\u7edf\u9884\u5904\u7406\u7cfb\u7edf\u83b7\u5f979.0\u500d\u52a0\u901f\uff0c\u76f8\u6bd4GPU\u52a0\u901f\u7cfb\u7edf\u83b7\u5f972.1\u500d\u52a0\u901f\u3002\u80fd\u591f\u9ad8\u6548\u5904\u7406\u591a\u6837\u5316\u7684\u56fe\u6570\u636e\u96c6\uff0c\u663e\u8457\u964d\u4f4eGNN\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "AutoGNN\u901a\u8fc7FPGA\u53ef\u91cd\u6784\u6027\u548c\u4e13\u7528\u786c\u4ef6\u7ec4\u4ef6\u6709\u6548\u89e3\u51b3\u4e86GNN\u9884\u5904\u7406\u74f6\u9888\uff0c\u4e3a\u9ad8\u6027\u80fdGNN\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u5904\u7406\u591a\u6837\u5316\u7684\u56fe\u8f93\u5165\u6570\u636e\u3002"}}
{"id": "2602.00838", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00838", "abs": "https://arxiv.org/abs/2602.00838", "authors": ["Prabhu Vellaisamy", "Harideep Nair", "Di Wu", "Shawn Blanton", "John Paul Shen"], "title": "Exploration of Unary Arithmetic-Based Matrix Multiply Units for Low Precision DL Accelerators", "comment": null, "summary": "General matrix multiplication (GEMM) is a fundamental operation in deep learning (DL). With DL moving increasingly toward low precision, recent works have proposed novel unary GEMM designs as an alternative to conventional binary GEMM hardware. A rigorous evaluation of recent unary and binary GEMM designs is needed to assess the potential of unary hardware for future DL compute. This paper focuses on unary GEMM designs for integer-based DL inference and performs a detailed evaluation of three latest unary design proposals, namely, uGEMM, tuGEMM and tubGEMM, by comparing them to a conventional binary GEMM. Rigorous post-synthesis evaluations beyond prior works are performed across varying bit-widths and matrix sizes to assess the designs' tradeoffs and determine optimal sweetspots. Further, we perform weight sparsity analysis across eight pretrained convolutional neural networks (CNNs) and the LLaMA2 large language model (LLM). In this work, we demonstrate how unary GEMM can be effectively used for energy-efficient compute in future edge AI accelerators.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u4e09\u79cd\u6700\u65b0\u7684\u5355\u7cbe\u5ea6GEMM\u8bbe\u8ba1\uff08uGEMM\u3001tuGEMM\u3001tubGEMM\uff09\u4e0e\u4f20\u7edf\u4e8c\u8fdb\u5236GEMM\u8fdb\u884c\u4e86\u8be6\u7ec6\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5355\u7cbe\u5ea6GEMM\u5728\u672a\u6765\u8fb9\u7f18AI\u52a0\u901f\u5668\u4e2d\u5b9e\u73b0\u9ad8\u6548\u80fd\u8ba1\u7b97\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u5411\u4f4e\u7cbe\u5ea6\u53d1\u5c55\uff0c\u9700\u8981\u8bc4\u4f30\u65b0\u578b\u5355\u7cbe\u5ea6GEMM\u8bbe\u8ba1\u4f5c\u4e3a\u4f20\u7edf\u4e8c\u8fdb\u5236GEMM\u786c\u4ef6\u66ff\u4ee3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765DL\u8ba1\u7b97\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u5bf9\u4e09\u79cd\u6700\u65b0\u5355\u7cbe\u5ea6GEMM\u8bbe\u8ba1\uff08uGEMM\u3001tuGEMM\u3001tubGEMM\uff09\u4e0e\u4f20\u7edf\u4e8c\u8fdb\u5236GEMM\u8fdb\u884c\u8be6\u7ec6\u8bc4\u4f30\uff0c\u5305\u62ec\u4e0d\u540c\u4f4d\u5bbd\u548c\u77e9\u9635\u5c3a\u5bf8\u7684\u5408\u6210\u540e\u5206\u6790\uff0c\u4ee5\u53ca\u57288\u4e2a\u9884\u8bad\u7ec3CNN\u548cLLaMA2 LLM\u4e0a\u7684\u6743\u91cd\u7a00\u758f\u6027\u5206\u6790\u3002", "result": "\u901a\u8fc7\u4e25\u683c\u7684\u8bc4\u4f30\u786e\u5b9a\u4e86\u5404\u79cd\u8bbe\u8ba1\u7684\u6743\u8861\u70b9\u548c\u6700\u4f18\u5de5\u4f5c\u70b9\uff0c\u5c55\u793a\u4e86\u5355\u7cbe\u5ea6GEMM\u5728\u80fd\u91cf\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u8fb9\u7f18AI\u52a0\u901f\u5668\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u5355\u7cbe\u5ea6GEMM\u53ef\u4ee5\u6709\u6548\u7528\u4e8e\u672a\u6765\u8fb9\u7f18AI\u52a0\u901f\u5668\u7684\u9ad8\u6548\u80fd\u8ba1\u7b97\uff0c\u4e3a\u4f4e\u7cbe\u5ea6\u6df1\u5ea6\u5b66\u4e60\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2602.01720", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.01720", "abs": "https://arxiv.org/abs/2602.01720", "authors": ["Peisen Yao", "Zinan Gu", "Qingkai Shi"], "title": "Phoenix: A Modular and Versatile Framework for C/C++ Pointer Analysis", "comment": null, "summary": "We present Phoenix, a modular pointer analysis framework for C/C++ that unifies multiple state-of-the-art alias analysis algorithms behind a single, stable interface. Phoenix addresses the fragmentation of today's C/C++ pointer analysis ecosystem by cleanly separating IR construction, constraint generation, solver backends, and client-facing queries, making analyses easy to compare, swap, and compose while exposing explicit precision-performance trade-offs. We evaluate Phoenix against SVF under two representative configurations: a flow- and context-insensitive setting and a more precise flow- and context-sensitive setting, on 28 GNU coreutils programs. Phoenix delivers robust speedups in the baseline configuration (up to 2.88x) and remains competitive, and often faster, even in the stronger precision regime (up to 2.91x), without a systematic runtime penalty. In production, Phoenix serves as the analysis substrate for static analysis and fuzzing tools that have uncovered hundreds of new bugs and enabled deployments reporting more than 1000 bugs found in an industrial toolchain.", "AI": {"tldr": "Phoenix\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684C/C++\u6307\u9488\u5206\u6790\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u591a\u79cd\u5148\u8fdb\u7684\u522b\u540d\u5206\u6790\u7b97\u6cd5\uff0c\u901a\u8fc7\u6e05\u6670\u7684\u67b6\u6784\u5206\u79bb\u63d0\u9ad8\u4e86\u5206\u6790\u7684\u53ef\u6bd4\u6027\u548c\u53ef\u7ec4\u5408\u6027\uff0c\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edfSVF\u3002", "motivation": "\u5f53\u524dC/C++\u6307\u9488\u5206\u6790\u751f\u6001\u7cfb\u7edf\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u4e0d\u540c\u5206\u6790\u7b97\u6cd5\u96be\u4ee5\u6bd4\u8f83\u3001\u4ea4\u6362\u548c\u7ec4\u5408\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5c06IR\u6784\u9020\u3001\u7ea6\u675f\u751f\u6210\u3001\u6c42\u89e3\u5668\u540e\u7aef\u548c\u5ba2\u6237\u7aef\u67e5\u8be2\u6e05\u6670\u5206\u79bb\uff0c\u652f\u6301\u591a\u79cd\u5148\u8fdb\u7684\u522b\u540d\u5206\u6790\u7b97\u6cd5\u5728\u540c\u4e00\u63a5\u53e3\u4e0b\u5de5\u4f5c\u3002", "result": "\u572828\u4e2aGNU coreutils\u7a0b\u5e8f\u4e0a\u8bc4\u4f30\uff0cPhoenix\u5728\u6d41\u4e0d\u654f\u611f\u548c\u4e0a\u4e0b\u6587\u4e0d\u654f\u611f\u914d\u7f6e\u4e0b\u6bd4SVF\u5feb\u8fbe2.88\u500d\uff0c\u5728\u66f4\u7cbe\u786e\u7684\u6d41\u654f\u611f\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u914d\u7f6e\u4e0b\u4e5f\u4fdd\u6301\u7ade\u4e89\u529b\uff08\u5feb\u8fbe2.91\u500d\uff09\uff0c\u4e14\u6ca1\u6709\u7cfb\u7edf\u6027\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "conclusion": "Phoenix\u6210\u529f\u89e3\u51b3\u4e86\u6307\u9488\u5206\u6790\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u7a33\u5b9a\u3001\u9ad8\u6548\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5df2\u5728\u5de5\u4e1a\u7ea7\u9759\u6001\u5206\u6790\u548c\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\u4e2d\u5e94\u7528\uff0c\u53d1\u73b0\u4e86\u6570\u767e\u4e2a\u65b0bug\u3002"}}
{"id": "2602.00014", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00014", "abs": "https://arxiv.org/abs/2602.00014", "authors": ["Pierrick Pochelu", "Hyacinthe Cartiaux", "Julien Schleich"], "title": "What Artificial Intelligence can do for High-Performance Computing systems?", "comment": null, "summary": "High-performance computing (HPC) centers consume substantial power, incurring environmental and operational costs. This review assesses how artificial intelligence (AI), including machine learning (ML) and optimization, improves the efficiency of operational HPC systems. Approximately 1,800 publications from 2019 to 2025 were manually screened using predefined inclusion/exclusion criteria; 74 \"AI for HPC\" papers were retained and grouped into six application areas: performance estimation, performance optimization, scheduling, surrogate modeling, fault detection, and language-model-based automation.\n  Scheduling is the most active area, spanning research-oriented reinforcement-learning schedulers to production-friendly hybrids that combine ML with heuristics. Supervised performance estimation is foundational for both scheduling and optimization. Graph neural networks and time-series models strengthen anomaly detection by capturing spatio-temporal dependencies in production telemetry. Domain-specialized language models for HPC can outperform general-purpose LLMs on targeted coding and automation tasks. Together, these findings highlight integration opportunities such as LLM-based operating-system concepts and underscore the need for advances in MLOps, standardization of AI components, and benchmarking methodology.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bc4\u4f30\u4e86AI\uff08\u5305\u62ec\u673a\u5668\u5b66\u4e60\u548c\u4f18\u5316\uff09\u5982\u4f55\u63d0\u9ad8HPC\u7cfb\u7edf\u8fd0\u884c\u6548\u7387\uff0c\u5206\u6790\u4e862019-2025\u5e74\u95f474\u7bc7\u76f8\u5173\u8bba\u6587\uff0c\u8bc6\u522b\u51fa\u516d\u4e2a\u5e94\u7528\u9886\u57df\uff0c\u5176\u4e2d\u8c03\u5ea6\u662f\u6700\u6d3b\u8dc3\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u5fc3\u6d88\u8017\u5927\u91cf\u7535\u529b\uff0c\u5e26\u6765\u73af\u5883\u548c\u8fd0\u8425\u6210\u672c\u95ee\u9898\u3002AI\u6280\u672f\u6709\u671b\u63d0\u9ad8HPC\u7cfb\u7edf\u7684\u8fd0\u884c\u6548\u7387\uff0c\u51cf\u5c11\u80fd\u8017\u548c\u6210\u672c\u3002", "method": "\u624b\u52a8\u7b5b\u9009\u4e862019-2025\u5e74\u95f4\u7ea61800\u7bc7\u51fa\u7248\u7269\uff0c\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u7eb3\u5165/\u6392\u9664\u6807\u51c6\uff0c\u6700\u7ec8\u4fdd\u7559\u4e8674\u7bc7\"AI for HPC\"\u8bba\u6587\uff0c\u5e76\u5c06\u5176\u5206\u4e3a\u516d\u4e2a\u5e94\u7528\u9886\u57df\u8fdb\u884c\u5206\u6790\u3002", "result": "\u8bc6\u522b\u51fa\u516d\u4e2a\u4e3b\u8981\u5e94\u7528\u9886\u57df\uff1a\u6027\u80fd\u4f30\u8ba1\u3001\u6027\u80fd\u4f18\u5316\u3001\u8c03\u5ea6\u3001\u4ee3\u7406\u5efa\u6a21\u3001\u6545\u969c\u68c0\u6d4b\u548c\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u3002\u8c03\u5ea6\u662f\u6700\u6d3b\u8dc3\u7684\u7814\u7a76\u9886\u57df\uff0c\u4ece\u7814\u7a76\u5bfc\u5411\u7684\u5f3a\u5316\u5b66\u4e60\u8c03\u5ea6\u5668\u5230\u751f\u4ea7\u53cb\u597d\u7684\u6df7\u5408\u65b9\u6cd5\u3002\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u589e\u5f3a\u4e86\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\uff0c\u4e13\u95e8\u9488\u5bf9HPC\u7684\u9886\u57df\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u901a\u7528LLM\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86AI\u5728HPC\u7cfb\u7edf\u4f18\u5316\u4e2d\u7684\u6574\u5408\u673a\u4f1a\uff0c\u5982\u57fa\u4e8eLLM\u7684\u64cd\u4f5c\u7cfb\u7edf\u6982\u5ff5\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u9700\u8981\u5728MLOps\u3001AI\u7ec4\u4ef6\u6807\u51c6\u5316\u548c\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\u3002"}}
{"id": "2602.00909", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.00909", "abs": "https://arxiv.org/abs/2602.00909", "authors": ["Rafael Billig Tonetto", "Marcello Traiola", "Fernando Fernandes dos Santos", "Angeliki Kritikakou"], "title": "ENFOR-SA: End-to-end Cross-layer Transient Fault Injector for Efficient and Accurate DNN Reliability Assessment on Systolic Arrays", "comment": null, "summary": "Recent advances in deep learning have produced highly accurate but increasingly large and complex DNNs, making traditional fault-injection techniques impractical. Accurate fault analysis requires RTL-accurate hardware models. However, this significantly slows evaluation compared with software-only approaches, particularly when combined with expensive HDL instrumentation. In this work, we show that such high-overhead methods are unnecessary for systolic array (SA) architectures and propose ENFOR-SA, an end-to-end framework for DNN transient fault analysis on SAs. Our two-step approach employs cross-layer simulation and uses RTL SA components only during fault injection, with the rest executed at the software level. Experiments on CNNs and Vision Transformers demonstrate that ENFOR-SA achieves RTL-accurate fault injection with only 6% average slowdown compared to software-based injection, while delivering at least two orders of magnitude speedup (average $569\\times$) over full-SoC RTL simulation and a $2.03\\times$ improvement over a state-of-the-art cross-layer RTL injection tool. ENFOR-SA code is publicly available at https://github.com/rafaabt/ENFOR-SA.", "AI": {"tldr": "ENFOR-SA\uff1a\u9488\u5bf9\u8109\u52a8\u9635\u5217\u67b6\u6784\u7684\u7aef\u5230\u7aefDNN\u77ac\u6001\u6545\u969c\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u5c42\u4eff\u771f\u5b9e\u73b0RTL\u7ea7\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u8f6f\u4ef6\u6ce8\u5165\u4ec5\u61626%\uff0c\u6bd4\u5168SoC RTL\u4eff\u771f\u5feb569\u500d", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53d8\u5f97\u8d8a\u6765\u8d8a\u5927\u4e14\u590d\u6742\uff0c\u4f20\u7edf\u6545\u969c\u6ce8\u5165\u6280\u672f\u4e0d\u5b9e\u7528\u3002RTL\u7ea7\u786c\u4ef6\u6a21\u578b\u867d\u7136\u51c6\u786e\u4f46\u901f\u5ea6\u6162\uff0c\u7279\u522b\u662f\u7ed3\u5408\u6602\u8d35\u7684HDL\u4eea\u5668\u65f6\u3002\u5bf9\u4e8e\u8109\u52a8\u9635\u5217\u67b6\u6784\uff0c\u8fd9\u79cd\u9ad8\u5f00\u9500\u65b9\u6cd5\u662f\u4e0d\u5fc5\u8981\u7684\u3002", "method": "\u63d0\u51faENFOR-SA\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u8de8\u5c42\u4eff\u771f\uff0c\u4ec5\u5728\u6545\u969c\u6ce8\u5165\u65f6\u4f7f\u7528RTL\u8109\u52a8\u9635\u5217\u7ec4\u4ef6\uff0c\u5176\u4f59\u90e8\u5206\u5728\u8f6f\u4ef6\u5c42\u9762\u6267\u884c\u3002\u7ed3\u5408RTL\u7ea7\u7cbe\u5ea6\u548c\u8f6f\u4ef6\u7ea7\u901f\u5ea6\u3002", "result": "\u5728CNN\u548cVision Transformer\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cENFOR-SA\u5b9e\u73b0RTL\u7ea7\u7cbe\u5ea6\u6545\u969c\u6ce8\u5165\uff0c\u76f8\u6bd4\u8f6f\u4ef6\u6ce8\u5165\u4ec5\u61626%\uff0c\u6bd4\u5168SoC RTL\u4eff\u771f\u5feb569\u500d\uff08\u5e73\u5747\uff09\uff0c\u6bd4\u6700\u5148\u8fdb\u7684\u8de8\u5c42RTL\u6ce8\u5165\u5de5\u5177\u5feb2.03\u500d\u3002", "conclusion": "ENFOR-SA\u4e3a\u8109\u52a8\u9635\u5217\u67b6\u6784\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684DNN\u6545\u969c\u5206\u6790\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u9ad8\u5f00\u9500\u65b9\u6cd5\u5bf9\u4e8eSA\u67b6\u6784\u662f\u4e0d\u5fc5\u8981\u7684\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2602.00272", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.00272", "abs": "https://arxiv.org/abs/2602.00272", "authors": ["Wan Fokkink", "Georgios Karlos", "Andy Tatman"], "title": "A Fault-Tolerant Version of Safra's Termination Detection Algorithm", "comment": null, "summary": "Safra's distributed termination detection algorithm employs a logical token ring structure within a distributed network; only passive nodes forward the token, and a counter in the token keeps track of the number of sent minus the number of received messages. We adapt this classic algorithm to make it fault-tolerant. The counter is split into counters per node, to discard counts from crashed nodes. If a node crashes, the token ring is restored locally and a backup token is sent. Nodes inform each other of detected crashes via the token. Our algorithm imposes no additional message overhead, tolerates any number of crashes as well as simultaneous crashes, and copes with crashes in a decentralized fashion. Correctness proofs are provided of both the original Safra's algorithm and its fault-tolerant variant, as well as a model checking analysis.", "AI": {"tldr": "\u5c06Safra\u5206\u5e03\u5f0f\u7ec8\u6b62\u68c0\u6d4b\u7b97\u6cd5\u6539\u9020\u4e3a\u5bb9\u9519\u7248\u672c\uff0c\u901a\u8fc7\u8282\u70b9\u7ea7\u8ba1\u6570\u5668\u3001\u672c\u5730\u73af\u6062\u590d\u548c\u5907\u4efd\u4ee4\u724c\u673a\u5236\uff0c\u5b9e\u73b0\u65e0\u989d\u5916\u6d88\u606f\u5f00\u9500\u3001\u5bb9\u5fcd\u4efb\u610f\u6570\u91cf\u540c\u65f6\u5d29\u6e83\u7684\u5206\u5e03\u5f0f\u5bb9\u9519", "motivation": "\u7ecf\u5178Safra\u5206\u5e03\u5f0f\u7ec8\u6b62\u68c0\u6d4b\u7b97\u6cd5\u7f3a\u4e4f\u5bb9\u9519\u80fd\u529b\uff0c\u5f53\u8282\u70b9\u5d29\u6e83\u65f6\u65e0\u6cd5\u6b63\u5e38\u5de5\u4f5c\u3002\u9700\u8981\u8bbe\u8ba1\u4e00\u4e2a\u80fd\u591f\u5bb9\u5fcd\u8282\u70b9\u5d29\u6e83\u7684\u5bb9\u9519\u7248\u672c\uff0c\u4ee5\u589e\u5f3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "method": "1. \u5c06\u4ee4\u724c\u4e2d\u7684\u5168\u5c40\u8ba1\u6570\u5668\u62c6\u5206\u4e3a\u6bcf\u4e2a\u8282\u70b9\u7684\u72ec\u7acb\u8ba1\u6570\u5668\uff0c\u4e22\u5f03\u5d29\u6e83\u8282\u70b9\u7684\u8ba1\u6570\uff1b2. \u8282\u70b9\u5d29\u6e83\u65f6\u5728\u672c\u5730\u6062\u590d\u4ee4\u724c\u73af\u7ed3\u6784\uff1b3. \u53d1\u9001\u5907\u4efd\u4ee4\u724c\uff1b4. \u901a\u8fc7\u4ee4\u724c\u4f20\u9012\u8282\u70b9\u5d29\u6e83\u4fe1\u606f\uff1b5. \u4fdd\u6301\u65e0\u989d\u5916\u6d88\u606f\u5f00\u9500\u7684\u5206\u5e03\u5f0f\u8bbe\u8ba1\u3002", "result": "\u7b97\u6cd5\u80fd\u591f\u5bb9\u5fcd\u4efb\u610f\u6570\u91cf\u7684\u8282\u70b9\u5d29\u6e83\uff0c\u5305\u62ec\u540c\u65f6\u53d1\u751f\u7684\u5d29\u6e83\uff0c\u4ee5\u5b8c\u5168\u5206\u5e03\u5f0f\u65b9\u5f0f\u5904\u7406\u6545\u969c\uff0c\u4e14\u4e0d\u589e\u52a0\u989d\u5916\u7684\u6d88\u606f\u5f00\u9500\u3002\u63d0\u4f9b\u4e86\u539f\u59cb\u7b97\u6cd5\u548c\u5bb9\u9519\u53d8\u4f53\u7684\u6b63\u786e\u6027\u8bc1\u660e\uff0c\u4ee5\u53ca\u6a21\u578b\u68c0\u67e5\u5206\u6790\u3002", "conclusion": "\u6210\u529f\u5c06Safra\u7ec8\u6b62\u68c0\u6d4b\u7b97\u6cd5\u6539\u9020\u4e3a\u5bb9\u9519\u7248\u672c\uff0c\u4fdd\u6301\u4e86\u539f\u7b97\u6cd5\u7684\u6548\u7387\u4f18\u52bf\uff0c\u540c\u65f6\u663e\u8457\u589e\u5f3a\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u7ec8\u6b62\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01546", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.01546", "abs": "https://arxiv.org/abs/2602.01546", "authors": ["Shanmuga Venkatachalam", "Prabhu Vellaisamy", "Harideep Nair", "Wei-Che Huang", "Youngseok Na", "Yuyang Kang", "Quinn Jacobson", "John Paul Shen"], "title": "NeuroAI Temporal Neural Networks (NeuTNNs): Microarchitecture and Design Framework for Specialized Neuromorphic Processing Units", "comment": null, "summary": "Leading experts from both communities have suggested the need to (re)connect research in neuroscience and artificial intelligence (AI) to accelerate the development of next-generation AI innovations. They term this convergence as NeuroAI. Previous research has established temporal neural networks (TNNs) as a promising neuromorphic approach toward biological intelligence and efficiency. We fully embrace NeuroAI and propose a new category of TNNs we call NeuroAI TNNs (NeuTNNs) with greater capability and hardware efficiency by adopting neuroscience findings, including a neuron model with active dendrites and a hierarchy of distal and proximal segments. This work introduces a PyTorch-to-layout tool suite (NeuTNNGen) to design application-specific NeuTNNs. Compared to previous TNN designs, NeuTNNs achieve superior performance and efficiency. We demonstrate NeuTNNGen's capabilities using three example applications: 1) UCR time series benchmarks, 2) MNIST design exploration, and 3) Place Cells design for neocortical reference frames. We also explore using synaptic pruning to further reduce synapse counts and hardware costs by 30-50% while maintaining model precision across diverse sensory modalities. NeuTNNGen can facilitate the design of application-specific energy-efficient NeuTNNs for the next generation of NeuroAI computing systems.", "AI": {"tldr": "\u63d0\u51faNeuroAI TNNs (NeuTNNs) - \u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u53d1\u73b0\u7684\u65b0\u578b\u65f6\u5e8f\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u53caPyTorch-to-layout\u5de5\u5177\u5957\u4ef6NeuTNNGen\uff0c\u7528\u4e8e\u8bbe\u8ba1\u7279\u5b9a\u5e94\u7528\u7684\u80fd\u6548NeuTNNs\u3002", "motivation": "\u795e\u7ecf\u79d1\u5b66\u4e0e\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u9700\u8981\u91cd\u65b0\u8fde\u63a5\u4ee5\u52a0\u901f\u4e0b\u4e00\u4ee3AI\u521b\u65b0\uff08NeuroAI\uff09\u3002\u73b0\u6709\u65f6\u5e8f\u795e\u7ecf\u7f51\u7edc\uff08TNNs\uff09\u4f5c\u4e3a\u795e\u7ecf\u5f62\u6001\u65b9\u6cd5\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u53d1\u73b0\u6765\u63d0\u5347\u80fd\u529b\u548c\u786c\u4ef6\u6548\u7387\u3002", "method": "\u63d0\u51faNeuroAI TNNs (NeuTNNs)\uff0c\u91c7\u7528\u795e\u7ecf\u79d1\u5b66\u53d1\u73b0\uff1a\u5305\u542b\u4e3b\u52a8\u6811\u7a81\u7684\u795e\u7ecf\u5143\u6a21\u578b\u3001\u8fdc\u7aef\u548c\u8fd1\u7aef\u6bb5\u5c42\u6b21\u7ed3\u6784\u3002\u5f00\u53d1NeuTNNGen\u5de5\u5177\u5957\u4ef6\uff08PyTorch-to-layout\uff09\uff0c\u7528\u4e8e\u8bbe\u8ba1\u7279\u5b9a\u5e94\u7528\u7684NeuTNNs\u3002\u4f7f\u7528\u7a81\u89e6\u4fee\u526a\u8fdb\u4e00\u6b65\u51cf\u5c11\u7a81\u89e6\u6570\u91cf\u548c\u786c\u4ef6\u6210\u672c\u3002", "result": "\u76f8\u6bd4\u5148\u524dTNN\u8bbe\u8ba1\uff0cNeuTNNs\u5b9e\u73b0\u66f4\u4f18\u6027\u80fd\u548c\u6548\u7387\u3002\u901a\u8fc7\u4e09\u4e2a\u5e94\u7528\u5c55\u793a\uff1a1) UCR\u65f6\u95f4\u5e8f\u5217\u57fa\u51c6\u6d4b\u8bd5\uff0c2) MNIST\u8bbe\u8ba1\u63a2\u7d22\uff0c3) \u7528\u4e8e\u65b0\u76ae\u5c42\u53c2\u8003\u7cfb\u7684Place Cells\u8bbe\u8ba1\u3002\u7a81\u89e6\u4fee\u526a\u51cf\u5c1130-50%\u7a81\u89e6\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "NeuTNNGen\u5de5\u5177\u5957\u4ef6\u80fd\u591f\u4fc3\u8fdb\u8bbe\u8ba1\u7279\u5b9a\u5e94\u7528\u7684\u80fd\u6548NeuTNNs\uff0c\u4e3a\u4e0b\u4e00\u4ee3NeuroAI\u8ba1\u7b97\u7cfb\u7edf\u63d0\u4f9b\u652f\u6301\uff0c\u63a8\u52a8\u795e\u7ecf\u79d1\u5b66\u4e0eAI\u7684\u878d\u5408\u3002"}}
{"id": "2602.00277", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.00277", "abs": "https://arxiv.org/abs/2602.00277", "authors": ["Omkar Salpekar", "Rohan Varma", "Kenny Yu", "Vladimir Ivanov", "Yang Wang", "Ahmed Sharif", "Min Si", "Shawn Xu", "Feng Tian", "Shengbao Zheng", "Tristan Rice", "Ankush Garg", "Shangfu Peng", "Shreyas Siravara", "Wenyin Fu", "Rodrigo de Castro", "Adithya Gangidi", "Andrey Obraztsov", "Sharan Narang", "Sergey Edunov", "Maxim Naumov", "Chunqiang Tang", "Mathew Oldham"], "title": "Training LLMs with Fault Tolerant HSDP on 100,000 GPUs", "comment": null, "summary": "Large-scale training systems typically use synchronous training, requiring all GPUs to be healthy simultaneously. In our experience training on O(100K) GPUs, synchronous training results in a low efficiency due to frequent failures and long recovery time.\n  To address this problem, we propose a novel training paradigm, Fault Tolerant Hybrid-Shared Data Parallelism (FT-HSDP). FT-HSDP uses data parallel replicas as units of fault tolerance. When failures occur, only a single data-parallel replica containing the failed GPU or server is taken offline and restarted, while the other replicas continue training. To realize this idea at scale, FT-HSDP incorporates several techniques: 1) We introduce a Fault Tolerant All Reduce (FTAR) protocol for gradient exchange across data parallel replicas. FTAR relies on the CPU to drive the complex control logic for tasks like adding or removing participants dynamically, and relies on GPU to perform data transfer for best performance. 2) We introduce a non-blocking catch-up protocol, allowing a recovering replica to join training with minimal stall.\n  Compared with fully synchronous training at O(100K) GPUs, FT-HSDP can reduce the stall time due to failure recovery from 10 minutes to 3 minutes, increasing effective training time from 44\\% to 80\\%. We further demonstrate that FT-HSDP's asynchronous recovery does not bring any meaning degradation to the accuracy of the result model.", "AI": {"tldr": "FT-HSDP\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5bb9\u9519\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u5e76\u884c\u526f\u672c\u4f5c\u4e3a\u5bb9\u9519\u5355\u5143\uff0c\u5728GPU\u6545\u969c\u65f6\u4ec5\u91cd\u542f\u6545\u969c\u526f\u672c\u800c\u5176\u4ed6\u526f\u672c\u7ee7\u7eed\u8bad\u7ec3\uff0c\u5927\u5e45\u51cf\u5c11\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u7684\u505c\u673a\u65f6\u95f4\u3002", "motivation": "\u5927\u89c4\u6a21\u540c\u6b65\u8bad\u7ec3\uff08O(100K) GPU\uff09\u9762\u4e34\u9891\u7e41\u6545\u969c\u548c\u957f\u6062\u590d\u65f6\u95f4\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\uff08\u4ec544%\u6709\u6548\u8bad\u7ec3\u65f6\u95f4\uff09\u3002\u9700\u8981\u4e00\u79cd\u5bb9\u9519\u673a\u5236\u6765\u51cf\u5c11\u6545\u969c\u6062\u590d\u7684\u505c\u673a\u65f6\u95f4\u3002", "method": "1) \u63d0\u51faFT-HSDP\u8303\u5f0f\uff0c\u4ee5\u6570\u636e\u5e76\u884c\u526f\u672c\u4e3a\u5bb9\u9519\u5355\u5143\uff1b2) \u8bbe\u8ba1\u5bb9\u9519All Reduce\u534f\u8bae\uff08FTAR\uff09\uff0cCPU\u5904\u7406\u590d\u6742\u63a7\u5236\u903b\u8f91\uff0cGPU\u8d1f\u8d23\u6570\u636e\u4f20\u8f93\uff1b3) \u5f15\u5165\u975e\u963b\u585e\u8ffd\u8d76\u534f\u8bae\uff0c\u5141\u8bb8\u6062\u590d\u526f\u672c\u4ee5\u6700\u5c0f\u5ef6\u8fdf\u52a0\u5165\u8bad\u7ec3\u3002", "result": "\u5728O(100K) GPU\u89c4\u6a21\u4e0b\uff0cFT-HSDP\u5c06\u6545\u969c\u6062\u590d\u505c\u673a\u65f6\u95f4\u4ece10\u5206\u949f\u51cf\u5c11\u52303\u5206\u949f\uff0c\u6709\u6548\u8bad\u7ec3\u65f6\u95f4\u4ece44%\u63d0\u5347\u523080%\u3002\u5f02\u6b65\u6062\u590d\u4e0d\u4f1a\u5bf9\u6700\u7ec8\u6a21\u578b\u7684\u51c6\u786e\u6027\u4ea7\u751f\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "FT-HSDP\u901a\u8fc7\u521b\u65b0\u7684\u5bb9\u9519\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u540c\u6b65\u8bad\u7ec3\u4e2d\u7684\u6545\u969c\u6062\u590d\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u8d85\u5927\u89c4\u6a21AI\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01827", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.01827", "abs": "https://arxiv.org/abs/2602.01827", "authors": ["Tommaso Spagnolo", "Cristina Silvano", "Riccardo Massa", "Filippo Grillotti", "Thomas Boesch", "Giuseppe Desoli"], "title": "In-Pipeline Integration of Digital In-Memory-Computing into RISC-V Vector Architecture to Accelerate Deep Learning", "comment": null, "summary": "Expanding Deep Learning applications toward edge computing demands architectures capable of delivering high computational performance and efficiency while adhering to tight power and memory constraints. Digital In-Memory Computing (DIMC) addresses this need by moving part of the computation directly within memory arrays, significantly reducing data movement and improving energy efficiency. This paper introduces a novel architecture that extends the Vector RISC-V Instruction Set Architecture (ISA) to integrate a tightly coupled DIMC unit directly into the execution stage of the pipeline, to accelerate Deep Learning inference at the edge. Specifically, the proposed approach adds four custom instructions dedicated to data loading, computation, and write-back, enabling flexible and optimal control of the inference execution on the target architecture. Experimental results demonstrate high utilization of the DIMC tile in Vector RISC-V and sustained throughput across the ResNet-50 model, achieving a peak performance of 137 GOP/s. The proposed architecture achieves a speedup of 217x over the baseline core and 50x area-normalized speedup even when operating near the hardware resource limits. The experimental results confirm the high potential of the proposed architecture as a scalable and efficient solution to accelerate Deep Learning inference on the edge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6570\u5b57\u5185\u5b58\u8ba1\u7b97\u5355\u5143\u7d27\u5bc6\u96c6\u6210\u5230\u5411\u91cfRISC-V ISA\u4e2d\u7684\u65b0\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u6307\u4ee4\u52a0\u901f\u8fb9\u7f18\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86217\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u9700\u8981\u9ad8\u6027\u80fd\u3001\u9ad8\u6548\u7387\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u4f46\u9762\u4e34\u4e25\u683c\u7684\u529f\u8017\u548c\u5185\u5b58\u9650\u5236\u3002\u6570\u5b57\u5185\u5b58\u8ba1\u7b97\u901a\u8fc7\u5c06\u8ba1\u7b97\u79fb\u81f3\u5185\u5b58\u9635\u5217\u6765\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u5e76\u63d0\u9ad8\u80fd\u6548\u3002", "method": "\u6269\u5c55\u5411\u91cfRISC-V ISA\uff0c\u5c06\u7d27\u5bc6\u8026\u5408\u7684\u6570\u5b57\u5185\u5b58\u8ba1\u7b97\u5355\u5143\u96c6\u6210\u5230\u6d41\u6c34\u7ebf\u6267\u884c\u9636\u6bb5\uff0c\u6dfb\u52a0\u56db\u4e2a\u81ea\u5b9a\u4e49\u6307\u4ee4\u7528\u4e8e\u6570\u636e\u52a0\u8f7d\u3001\u8ba1\u7b97\u548c\u5199\u56de\uff0c\u5b9e\u73b0\u5bf9\u63a8\u7406\u6267\u884c\u7684\u7075\u6d3b\u4f18\u5316\u63a7\u5236\u3002", "result": "\u5728ResNet-50\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86137 GOP/s\u7684\u5cf0\u503c\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6838\u5fc3\u83b7\u5f97217\u500d\u52a0\u901f\uff0c\u5373\u4f7f\u5728\u786c\u4ef6\u8d44\u6e90\u53d7\u9650\u60c5\u51b5\u4e0b\u4e5f\u5b9e\u73b0\u4e8650\u500d\u9762\u79ef\u5f52\u4e00\u5316\u52a0\u901f\u3002", "conclusion": "\u8be5\u67b6\u6784\u5c55\u793a\u4e86\u4f5c\u4e3a\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u52a0\u901f\u8fb9\u7f18\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u6570\u5b57\u5185\u5b58\u8ba1\u7b97\u5355\u5143\u5728\u5411\u91cfRISC-V\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u5229\u7528\u7387\u3002"}}
{"id": "2602.00343", "categories": ["cs.DC", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.00343", "abs": "https://arxiv.org/abs/2602.00343", "authors": ["Austin Tapp", "Holger R. Roth", "Ziyue Xu", "Abhijeet Parida", "Hareem Nisar", "Marius George Linguraru"], "title": "Standardized Methods and Recommendations for Green Federated Learning", "comment": "4 sections, 9 pages, 5 figures, 26 references, submission to acm e-energy,", "summary": "Federated learning (FL) enables collaborative model training over privacy-sensitive, distributed data, but its environmental impact is difficult to compare across studies due to inconsistent measurement boundaries and heterogeneous reporting. We present a practical carbon-accounting methodology for FL CO2e tracking using NVIDIA NVFlare and CodeCarbon for explicit, phase-aware tasks (initialization, per-round training, evaluation, and idle/coordination). To capture non-compute effects, we additionally estimate communication emissions from transmitted model-update sizes under a network-configurable energy model. We validate the proposed approach on two representative workloads: CIFAR-10 image classification and retinal optic disk segmentation. In CIFAR-10, controlled client-efficiency scenarios show that system-level slowdowns and coordination effects can contribute meaningfully to carbon footprint under an otherwise fixed FL protocol, increasing total CO2e by 8.34x (medium) and 21.73x (low) relative to the high-efficiency baseline. In retinal segmentation, swapping GPU tiers (H100 vs.\\ V100) yields a consistent 1.7x runtime gap (290 vs. 503 minutes) while producing non-uniform changes in total energy and CO2e across sites, underscoring the need for per-site and per-round reporting. Overall, our results support a standardized carbon accounting method that acts as a prerequisite for reproducible 'green' FL evaluation. Our code is available at https://github.com/Pediatric-Accelerated-Intelligence-Lab/carbon_footprint.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u8054\u90a6\u5b66\u4e60\u78b3\u6392\u653e\u6838\u7b97\u65b9\u6cd5\uff0c\u4f7f\u7528NVFlare\u548cCodeCarbon\u8fdb\u884c\u9636\u6bb5\u611f\u77e5\u7684CO2e\u8ddf\u8e2a\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u6548\u7387\u5dee\u5f02\u5bf9\u78b3\u8db3\u8ff9\u7684\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u867d\u7136\u80fd\u4fdd\u62a4\u9690\u79c1\u654f\u611f\u6570\u636e\uff0c\u4f46\u5176\u73af\u5883\u5f71\u54cd\u96be\u4ee5\u5728\u4e0d\u540c\u7814\u7a76\u95f4\u6bd4\u8f83\uff0c\u56e0\u4e3a\u6d4b\u91cf\u8fb9\u754c\u4e0d\u4e00\u81f4\u4e14\u62a5\u544a\u5f02\u8d28\u6027\u9ad8\u3002\u9700\u8981\u6807\u51c6\u5316\u7684\u78b3\u6838\u7b97\u65b9\u6cd5\u6765\u652f\u6301\u53ef\u91cd\u590d\u7684\"\u7eff\u8272\"\u8054\u90a6\u5b66\u4e60\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eNVIDIA NVFlare\u548cCodeCarbon\u7684\u78b3\u6838\u7b97\u65b9\u6cd5\uff0c\u8fdb\u884c\u660e\u786e\u7684\u9636\u6bb5\u611f\u77e5\u4efb\u52a1\u8ddf\u8e2a\uff08\u521d\u59cb\u5316\u3001\u6bcf\u8f6e\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u7a7a\u95f2/\u534f\u8c03\uff09\uff0c\u5e76\u989d\u5916\u4f30\u8ba1\u6a21\u578b\u66f4\u65b0\u4f20\u8f93\u7684\u901a\u4fe1\u6392\u653e\u3002", "result": "\u5728CIFAR-10\u5b9e\u9a8c\u4e2d\uff0c\u7cfb\u7edf\u7ea7\u51cf\u901f\u548c\u534f\u8c03\u6548\u5e94\u663e\u8457\u589e\u52a0\u78b3\u8db3\u8ff9\uff08\u4e2d\u6548\u7387\u589e\u52a08.34\u500d\uff0c\u4f4e\u6548\u7387\u589e\u52a021.73\u500d\uff09\u3002\u5728\u89c6\u7f51\u819c\u5206\u5272\u4e2d\uff0cGPU\u5c42\u7ea7\u4ea4\u6362\uff08H100 vs V100\uff09\u4ea7\u751f1.7\u500d\u8fd0\u884c\u65f6\u95f4\u5dee\u8ddd\uff0c\u4f46\u4e0d\u540c\u7ad9\u70b9\u7684\u603b\u80fd\u8017\u548cCO2e\u53d8\u5316\u4e0d\u5747\u5300\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u6807\u51c6\u5316\u7684\u78b3\u6838\u7b97\u65b9\u6cd5\uff0c\u8fd9\u662f\u53ef\u91cd\u590d\"\u7eff\u8272\"\u8054\u90a6\u5b66\u4e60\u8bc4\u4f30\u7684\u524d\u63d0\u3002\u9700\u8981\u6309\u7ad9\u70b9\u548c\u6309\u8f6e\u6b21\u62a5\u544a\uff0c\u4ee5\u51c6\u786e\u8bc4\u4f30\u73af\u5883\u5f71\u54cd\u3002"}}
{"id": "2602.02005", "categories": ["cs.AR", "cs.LG", "eess.SY", "hep-ex", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.02005", "abs": "https://arxiv.org/abs/2602.02005", "authors": ["Duc Hoang"], "title": "Position: The Need for Ultrafast Training", "comment": "Position paper at the 2nd Workshop on Domain-Specialized FPGAs (WDSFPGA 2026)", "summary": "Domain-specialized FPGAs have delivered unprecedented performance for low-latency inference across scientific and industrial workloads, yet nearly all existing accelerators assume static models trained offline, relegating learning and adaptation to slower CPUs or GPUs. This separation fundamentally limits systems that must operate in non-stationary, high-frequency environments, where model updates must occur at the timescale of the underlying physics. In this paper, I argue for a shift from inference-only accelerators to ultrafast on-chip learning, in which both inference and training execute directly within the FPGA fabric under deterministic, sub-microsecond latency constraints. Bringing learning into the same real-time datapath as inference would enable closed-loop systems that adapt as fast as the physical processes they control, with applications spanning quantum error correction, cryogenic qubit calibration, plasma and fusion control, accelerator tuning, and autonomous scientific experiments. Enabling such regimes requires rethinking algorithms, architectures, and toolflows jointly, but promises to transform FPGAs from static inference engines into real-time learning machines.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u4ece\u4ec5\u63a8\u7406\u7684FPGA\u52a0\u901f\u5668\u8f6c\u5411\u8d85\u5feb\u901f\u7247\u4e0a\u5b66\u4e60\uff0c\u4f7f\u63a8\u7406\u548c\u8bad\u7ec3\u90fd\u5728FPGA\u4e0a\u4ee5\u4e9a\u5fae\u79d2\u5ef6\u8fdf\u6267\u884c\uff0c\u5b9e\u73b0\u4e0e\u7269\u7406\u8fc7\u7a0b\u540c\u6b65\u7684\u5b9e\u65f6\u81ea\u9002\u5e94\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u9886\u57df\u4e13\u7528FPGA\u4e3b\u8981\u4e13\u6ce8\u4e8e\u9759\u6001\u6a21\u578b\u7684\u4f4e\u5ef6\u8fdf\u63a8\u7406\uff0c\u5c06\u5b66\u4e60\u548c\u9002\u5e94\u4efb\u52a1\u4ea4\u7ed9\u8f83\u6162\u7684CPU/GPU\u5904\u7406\u3002\u8fd9\u79cd\u5206\u79bb\u9650\u5236\u4e86\u5728\u975e\u5e73\u7a33\u3001\u9ad8\u9891\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u9700\u8981\u5728\u7269\u7406\u8fc7\u7a0b\u7684\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8fdb\u884c\u6a21\u578b\u66f4\u65b0\u3002", "method": "\u63d0\u51fa\u91cd\u65b0\u601d\u8003\u7b97\u6cd5\u3001\u67b6\u6784\u548c\u5de5\u5177\u6d41\u7a0b\u7684\u8054\u5408\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5c06\u5b66\u4e60\u529f\u80fd\u76f4\u63a5\u96c6\u6210\u5230FPGA\u5b9e\u65f6\u6570\u636e\u8def\u5f84\u4e2d\uff0c\u5b9e\u73b0\u63a8\u7406\u548c\u8bad\u7ec3\u5728FPGA\u7ed3\u6784\u5185\u7684\u786e\u5b9a\u6027\u4e9a\u5fae\u79d2\u5ef6\u8fdf\u6267\u884c\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u6846\u67b6\uff0c\u5c06FPGA\u4ece\u9759\u6001\u63a8\u7406\u5f15\u64ce\u8f6c\u53d8\u4e3a\u5b9e\u65f6\u5b66\u4e60\u673a\u5668\uff0c\u80fd\u591f\u652f\u6301\u91cf\u5b50\u7ea0\u9519\u3001\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u6821\u51c6\u3001\u7b49\u79bb\u5b50\u4f53\u548c\u805a\u53d8\u63a7\u5236\u3001\u52a0\u901f\u5668\u8c03\u8c10\u4ee5\u53ca\u81ea\u4e3b\u79d1\u5b66\u5b9e\u9a8c\u7b49\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5b66\u4e60\u529f\u80fd\u5f15\u5165\u4e0e\u63a8\u7406\u76f8\u540c\u7684\u5b9e\u65f6\u6570\u636e\u8def\u5f84\uff0c\u53ef\u4ee5\u6784\u5efa\u4e0e\u6240\u63a7\u5236\u7269\u7406\u8fc7\u7a0b\u540c\u6b65\u9002\u5e94\u7684\u95ed\u73af\u7cfb\u7edf\uff0c\u8fd9\u9700\u8981\u7b97\u6cd5\u3001\u67b6\u6784\u548c\u5de5\u5177\u6d41\u7a0b\u7684\u534f\u540c\u91cd\u65b0\u8bbe\u8ba1\uff0c\u4f46\u6709\u671b\u5f7b\u5e95\u6539\u53d8FPGA\u5728\u5b9e\u65f6\u81ea\u9002\u5e94\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2602.00509", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.00509", "abs": "https://arxiv.org/abs/2602.00509", "authors": ["Qianchao Zhu", "Xucheng Ye", "Yuliang Liu", "Haodong Ouyang", "Chengru Song"], "title": "PROBE: Co-Balancing Computation and Communication in MoE Inference via Real-Time Predictive Prefetching", "comment": null, "summary": "Mixture-of-Experts models have become a dominant architecture for scaling Large Language Models by activating only a sparse subset of experts per token. However, latency-critical MoE inference faces a fundamental tension: while expert parallelism improves memory efficiency, it also amplifies execution stragglers. In real-world serving, continuous batching and diverse concurrent requests induce rapid semantic shifts, causing expert hotspots to migrate abruptly across GPUs and triggering the 'double penalty' of coupled computational skew and network congestion.\n  We propose PROBE, an inference system that co-balances computation and communication in real time. PROBE introduces Continuous Lookahead Pipelining, which proactively predicts, plans, and prefetches for upcoming layers while keeping all control overheads off the critical path. PROBE consists of: (1) a Gate-Initialized Lookahead Predictor that distills the target router to forecast next-layer expert activation with high fidelity; (2) a Hardware-Aware Balance Planning solver that jointly optimizes dynamic expert replication and token assignment under strict hiding-window constraints; and (3) a Phase-Locked Co-Scheduling policy that uses split-phase transmission to hide bandwidth-intensive expert transfers behind computation without contending with All-to-All collectives. Experiments show that PROBE reduces prefill latency by up to 1.32X and improves decoding throughput by up to 1.26X over state-of-the-art baselines, especially under extreme workload volatility.", "AI": {"tldr": "PROBE\uff1a\u4e00\u79cd\u5b9e\u65f6\u534f\u540c\u5e73\u8861\u8ba1\u7b97\u4e0e\u901a\u4fe1\u7684MoE\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fde\u7eed\u524d\u77bb\u6d41\u6c34\u7ebf\u9884\u6d4b\u3001\u89c4\u5212\u548c\u9884\u53d6\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u5347\u541e\u5410\u91cf", "motivation": "MoE\u6a21\u578b\u5728\u63a8\u7406\u65f6\u9762\u4e34\u4e13\u5bb6\u5e76\u884c\u5e26\u6765\u7684\u5185\u5b58\u6548\u7387\u63d0\u5347\u4e0e\u6267\u884c\u62d6\u5c3e\u6548\u5e94\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe\u3002\u73b0\u5b9e\u670d\u52a1\u4e2d\u7684\u8fde\u7eed\u6279\u5904\u7406\u548c\u591a\u6837\u5316\u5e76\u53d1\u8bf7\u6c42\u5bfc\u81f4\u4e13\u5bb6\u70ed\u70b9\u5728GPU\u95f4\u5feb\u901f\u8fc1\u79fb\uff0c\u5f15\u53d1\u8ba1\u7b97\u503e\u659c\u548c\u7f51\u7edc\u62e5\u585e\u7684\"\u53cc\u91cd\u60e9\u7f5a\"", "method": "PROBE\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u95e8\u63a7\u521d\u59cb\u5316\u524d\u77bb\u9884\u6d4b\u5668\uff0c\u84b8\u998f\u76ee\u6807\u8def\u7531\u5668\u4ee5\u9ad8\u4fdd\u771f\u5ea6\u9884\u6d4b\u4e0b\u4e00\u5c42\u4e13\u5bb6\u6fc0\u6d3b\uff1b2) \u786c\u4ef6\u611f\u77e5\u5e73\u8861\u89c4\u5212\u6c42\u89e3\u5668\uff0c\u5728\u4e25\u683c\u9690\u85cf\u7a97\u53e3\u7ea6\u675f\u4e0b\u8054\u5408\u4f18\u5316\u52a8\u6001\u4e13\u5bb6\u590d\u5236\u548c\u4ee4\u724c\u5206\u914d\uff1b3) \u76f8\u4f4d\u9501\u5b9a\u534f\u540c\u8c03\u5ea6\u7b56\u7565\uff0c\u4f7f\u7528\u5206\u9636\u6bb5\u4f20\u8f93\u5c06\u5e26\u5bbd\u5bc6\u96c6\u578b\u4e13\u5bb6\u4f20\u8f93\u9690\u85cf\u5728\u8ba1\u7b97\u540e\u9762\uff0c\u907f\u514d\u4e0eAll-to-All\u96c6\u4f53\u64cd\u4f5c\u7ade\u4e89", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPROBE\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9884\u586b\u5145\u5ef6\u8fdf\u964d\u4f4e\u8fbe1.32\u500d\uff0c\u89e3\u7801\u541e\u5410\u91cf\u63d0\u5347\u8fbe1.26\u500d\uff0c\u5728\u6781\u7aef\u5de5\u4f5c\u8d1f\u8f7d\u6ce2\u52a8\u4e0b\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa", "conclusion": "PROBE\u901a\u8fc7\u5b9e\u65f6\u534f\u540c\u5e73\u8861\u8ba1\u7b97\u4e0e\u901a\u4fe1\uff0c\u6709\u6548\u89e3\u51b3\u4e86MoE\u63a8\u7406\u4e2d\u7684\u6267\u884c\u62d6\u5c3e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u573a\u666f\u4e0b"}}
{"id": "2602.02056", "categories": ["cs.AR", "cs.LG", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.02056", "abs": "https://arxiv.org/abs/2602.02056", "authors": ["Duc Hoang", "Aarush Gupta", "Philip Harris"], "title": "Ultrafast On-chip Online Learning via Spline Locality in Kolmogorov-Arnold Networks", "comment": null, "summary": "Ultrafast online learning is essential for high-frequency systems, such as controls for quantum computing and nuclear fusion, where adaptation must occur on sub-microsecond timescales. Meeting these requirements demands low-latency, fixed-precision computation under strict memory constraints, a regime in which conventional Multi-Layer Perceptrons (MLPs) are both inefficient and numerically unstable. We identify key properties of Kolmogorov-Arnold Networks (KANs) that align with these constraints. Specifically, we show that: (i) KAN updates exploiting B-spline locality are sparse, enabling superior on-chip resource scaling, and (ii) KANs are inherently robust to fixed-point quantization. By implementing fixed-point online training on Field-Programmable Gate Arrays (FPGAs), a representative platform for on-chip computation, we demonstrate that KAN-based online learners are significantly more efficient and expressive than MLPs across a range of low-latency and resource-constrained tasks. To our knowledge, this work is the first to demonstrate model-free online learning at sub-microsecond latencies.", "AI": {"tldr": "KANs\u5728FPGA\u4e0a\u5b9e\u73b0\u4e9a\u5fae\u79d2\u7ea7\u5728\u7ebf\u5b66\u4e60\uff0c\u6bd4MLPs\u66f4\u9ad8\u6548\u4e14\u5bf9\u5b9a\u70b9\u91cf\u5316\u9c81\u68d2", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u3001\u6838\u805a\u53d8\u7b49\u9ad8\u9891\u7cfb\u7edf\u9700\u8981\u4e9a\u5fae\u79d2\u7ea7\u7684\u8d85\u5feb\u901f\u5728\u7ebf\u5b66\u4e60\uff0c\u4f20\u7edfMLPs\u5728\u4f4e\u5ef6\u8fdf\u3001\u56fa\u5b9a\u7cbe\u5ea6\u8ba1\u7b97\u548c\u4e25\u683c\u5185\u5b58\u7ea6\u675f\u4e0b\u6548\u7387\u4f4e\u4e0b\u4e14\u6570\u503c\u4e0d\u7a33\u5b9a", "method": "\u5229\u7528KANs\u7684B\u6837\u6761\u5c40\u90e8\u6027\u5b9e\u73b0\u7a00\u758f\u66f4\u65b0\uff0c\u5728FPGA\u4e0a\u5b9e\u73b0\u5b9a\u70b9\u5728\u7ebf\u8bad\u7ec3\uff0c\u5229\u7528KANs\u5bf9\u5b9a\u70b9\u91cf\u5316\u7684\u56fa\u6709\u9c81\u68d2\u6027", "result": "KAN-based\u5728\u7ebf\u5b66\u4e60\u5668\u5728\u4f4e\u5ef6\u8fdf\u548c\u8d44\u6e90\u53d7\u9650\u4efb\u52a1\u4e2d\u6bd4MLPs\u66f4\u9ad8\u6548\u548c\u8868\u8fbe\u80fd\u529b\u5f3a\uff0c\u9996\u6b21\u5b9e\u73b0\u4e9a\u5fae\u79d2\u7ea7\u6a21\u578b\u65e0\u5173\u5728\u7ebf\u5b66\u4e60", "conclusion": "KANs\u7279\u522b\u9002\u5408\u8d85\u5feb\u901f\u5728\u7ebf\u5b66\u4e60\u573a\u666f\uff0c\u5176\u7a00\u758f\u66f4\u65b0\u7279\u6027\u548c\u5b9a\u70b9\u91cf\u5316\u9c81\u68d2\u6027\u4f7f\u5176\u5728FPGA\u7b49\u7247\u4e0a\u8ba1\u7b97\u5e73\u53f0\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf"}}
{"id": "2602.00748", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.00748", "abs": "https://arxiv.org/abs/2602.00748", "authors": ["Fangxin Liu", "Qinghua Zhang", "Hanjing Shen", "Zhibo Liang", "Li Jiang", "Haibing Guan", "Chong Bao", "Xuefeng Jin"], "title": "HyperOffload: Graph-Driven Hierarchical Memory Management for Large Language Models on SuperNode Architectures", "comment": "Technical Report", "summary": "The rapid evolution of Large Language Models (LLMs) towards long-context reasoning and sparse architectures has pushed memory requirements far beyond the capacity of individual device HBM. While emerging supernode architectures offer terabyte-scale shared memory pools via high-bandwidth interconnects, existing software stacks fail to exploit this hardware effectively. Current runtime-based offloading and swapping techniques operate with a local view, leading to reactive scheduling and exposed communication latency that stall the computation pipeline.\n  In this paper, we propose the SuperNode Memory Management Framework (\\textbf{HyperOffload}). It employs a compiler-assisted approach that leverages graph-driven memory management to treat remote memory access as explicit operations in the computation graph, specifically designed for hierarchical SuperNode architectures. Unlike reactive runtime systems, SuperNode represents data movement using cache operators within the compiler's Intermediate Representation (IR). This design enables a global, compile-time analysis of tensor lifetimes and execution dependencies. Leveraging this visibility, we develop a global execution-order refinement algorithm that statically schedules data transfers to hide remote memory latency behind compute-intensive regions. We implement SuperNode within the production deep learning framework MindSpore, adding a remote memory backend and specialized compiler passes. Evaluation on representative LLM workloads shows that SuperNode reduces peak device memory usage by up to 26\\% for inference while maintaining end-to-end performance. Our work demonstrates that integrating memory-augmented hardware into the compiler's optimization framework is essential for scaling next-generation AI workloads.", "AI": {"tldr": "HyperOffload\u662f\u4e00\u4e2a\u9488\u5bf9\u8d85\u7ea7\u8282\u70b9\u67b6\u6784\u7684\u7f16\u8bd1\u5668\u8f85\u52a9\u5185\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u9a71\u52a8\u7684\u5185\u5b58\u7ba1\u7406\u548c\u9759\u6001\u8c03\u5ea6\u6570\u636e\u8f6c\u79fb\u6765\u9690\u85cf\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u5ef6\u8fdf\uff0c\u663e\u8457\u51cf\u5c11\u8bbe\u5907\u5185\u5b58\u4f7f\u7528\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5411\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u7a00\u758f\u67b6\u6784\u53d1\u5c55\uff0c\u5185\u5b58\u9700\u6c42\u5df2\u8fdc\u8d85\u5355\u4e2a\u8bbe\u5907HBM\u5bb9\u91cf\u3002\u867d\u7136\u65b0\u5174\u8d85\u7ea7\u8282\u70b9\u67b6\u6784\u901a\u8fc7\u9ad8\u901f\u4e92\u8fde\u63d0\u4f9bTB\u7ea7\u5171\u4eab\u5185\u5b58\u6c60\uff0c\u4f46\u73b0\u6709\u8f6f\u4ef6\u6808\u65e0\u6cd5\u6709\u6548\u5229\u7528\u8fd9\u79cd\u786c\u4ef6\u3002\u5f53\u524d\u57fa\u4e8e\u8fd0\u884c\u65f6\u7684\u5378\u8f7d\u548c\u4ea4\u6362\u6280\u672f\u5177\u6709\u5c40\u90e8\u89c6\u89d2\uff0c\u5bfc\u81f4\u53cd\u5e94\u5f0f\u8c03\u5ea6\u548c\u66b4\u9732\u7684\u901a\u4fe1\u5ef6\u8fdf\uff0c\u4ece\u800c\u963b\u585e\u8ba1\u7b97\u6d41\u6c34\u7ebf\u3002", "method": "\u63d0\u51faSuperNode\u5185\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u91c7\u7528\u7f16\u8bd1\u5668\u8f85\u52a9\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u9a71\u52a8\u5185\u5b58\u7ba1\u7406\u5c06\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u89c6\u4e3a\u8ba1\u7b97\u56fe\u4e2d\u7684\u663e\u5f0f\u64cd\u4f5c\u3002\u5728\u7f16\u8bd1\u5668\u4e2d\u95f4\u8868\u793a\u4e2d\u4f7f\u7528\u7f13\u5b58\u64cd\u4f5c\u7b26\u8868\u793a\u6570\u636e\u79fb\u52a8\uff0c\u5b9e\u73b0\u5f20\u91cf\u751f\u547d\u5468\u671f\u548c\u6267\u884c\u4f9d\u8d56\u7684\u5168\u5c40\u7f16\u8bd1\u65f6\u5206\u6790\u3002\u57fa\u4e8e\u6b64\u53ef\u89c1\u6027\uff0c\u5f00\u53d1\u5168\u5c40\u6267\u884c\u987a\u5e8f\u4f18\u5316\u7b97\u6cd5\uff0c\u9759\u6001\u8c03\u5ea6\u6570\u636e\u8f6c\u79fb\u4ee5\u5728\u8ba1\u7b97\u5bc6\u96c6\u578b\u533a\u57df\u540e\u9690\u85cf\u8fdc\u7a0b\u5185\u5b58\u5ef6\u8fdf\u3002\u5728MindSpore\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u5b9e\u73b0\uff0c\u6dfb\u52a0\u8fdc\u7a0b\u5185\u5b58\u540e\u7aef\u548c\u4e13\u95e8\u7f16\u8bd1\u5668\u901a\u9053\u3002", "result": "\u5728\u4ee3\u8868\u6027LLM\u5de5\u4f5c\u8d1f\u8f7d\u8bc4\u4f30\u4e2d\uff0cSuperNode\u5728\u63a8\u7406\u65f6\u5c06\u5cf0\u503c\u8bbe\u5907\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u9ad8\u8fbe26%\uff0c\u540c\u65f6\u4fdd\u6301\u7aef\u5230\u7aef\u6027\u80fd\u3002", "conclusion": "\u5c06\u5185\u5b58\u589e\u5f3a\u786c\u4ef6\u96c6\u6210\u5230\u7f16\u8bd1\u5668\u4f18\u5316\u6846\u67b6\u5bf9\u4e8e\u6269\u5c55\u4e0b\u4e00\u4ee3AI\u5de5\u4f5c\u8d1f\u8f7d\u81f3\u5173\u91cd\u8981\u3002HyperOffload\u5c55\u793a\u4e86\u7f16\u8bd1\u5668\u8f85\u52a9\u65b9\u6cd5\u5728\u8d85\u7ea7\u8282\u70b9\u67b6\u6784\u4e2d\u6709\u6548\u7ba1\u7406\u5185\u5b58\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.02119", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.02119", "abs": "https://arxiv.org/abs/2602.02119", "authors": ["Elio Vinciguerra", "Enrico Russo", "Giuseppe Ascia", "Maurizio Palesi"], "title": "CHAOS: Controlled Hardware fAult injectOr System for gem5", "comment": null, "summary": "Fault injectors are essential tools for evaluating the reliability and resilience of computing systems. They enable the simulation of hardware and software faults to analyze system behavior under error conditions and assess its ability to operate correctly despite disruptions. Such analysis is critical for identifying vulnerabilities and improving system robustness. CHAOS is a modular, open-source, and fully configurable fault injection framework designed for the gem5 simulator. It facilitates precise and systematic fault injection across multiple architectural levels, supporting comprehensive evaluations of fault tolerance mechanisms and resilience strategies. Its high configurability and seamless integration with gem5 allow researchers to explore a wide range of fault models and complex scenarios, making CHAOS a valuable tool for advancing research in dependable and high-performance computing systems.", "AI": {"tldr": "CHAOS\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u5f00\u6e90\u3001\u53ef\u914d\u7f6e\u7684\u6545\u969c\u6ce8\u5165\u6846\u67b6\uff0c\u4e13\u4e3agem5\u6a21\u62df\u5668\u8bbe\u8ba1\uff0c\u7528\u4e8e\u8bc4\u4f30\u8ba1\u7b97\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u5f39\u6027\u3002", "motivation": "\u6545\u969c\u6ce8\u5165\u5668\u5bf9\u4e8e\u8bc4\u4f30\u8ba1\u7b97\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u5f39\u6027\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u6a21\u62df\u786c\u4ef6\u548c\u8f6f\u4ef6\u6545\u969c\u6765\u5206\u6790\u7cfb\u7edf\u5728\u9519\u8bef\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u5e72\u6270\u4e0b\u6b63\u786e\u8fd0\u884c\u7684\u80fd\u529b\u3002\u8fd9\u79cd\u5206\u6790\u5bf9\u4e8e\u8bc6\u522b\u6f0f\u6d1e\u548c\u63d0\u9ad8\u7cfb\u7edf\u9c81\u68d2\u6027\u975e\u5e38\u5173\u952e\u3002", "method": "CHAOS\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u5f00\u6e90\u3001\u5b8c\u5168\u53ef\u914d\u7f6e\u7684\u6545\u969c\u6ce8\u5165\u6846\u67b6\uff0c\u4e13\u95e8\u4e3agem5\u6a21\u62df\u5668\u8bbe\u8ba1\u3002\u5b83\u652f\u6301\u8de8\u591a\u4e2a\u67b6\u6784\u7ea7\u522b\u7684\u7cbe\u786e\u548c\u7cfb\u7edf\u5316\u6545\u969c\u6ce8\u5165\uff0c\u4fc3\u8fdb\u5bf9\u5bb9\u9519\u673a\u5236\u548c\u5f39\u6027\u7b56\u7565\u7684\u5168\u9762\u8bc4\u4f30\u3002", "result": "CHAOS\u7684\u9ad8\u53ef\u914d\u7f6e\u6027\u548c\u4e0egem5\u7684\u65e0\u7f1d\u96c6\u6210\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u63a2\u7d22\u5e7f\u6cdb\u7684\u6545\u969c\u6a21\u578b\u548c\u590d\u6742\u573a\u666f\uff0c\u4f7f\u5176\u6210\u4e3a\u63a8\u8fdb\u53ef\u9760\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7814\u7a76\u7684\u5b9d\u8d35\u5de5\u5177\u3002", "conclusion": "CHAOS\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u5f00\u6e90\u3001\u53ef\u914d\u7f6e\u7684\u6545\u969c\u6ce8\u5165\u6846\u67b6\uff0c\u4e3agem5\u6a21\u62df\u5668\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6545\u969c\u6ce8\u5165\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u53ef\u9760\u548c\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7684\u7814\u7a76\u3002"}}
{"id": "2602.00892", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.00892", "abs": "https://arxiv.org/abs/2602.00892", "authors": ["Jebacyril Arockiaraj", "Sasindu Wijeratne", "Sugeet Sunder", "Md Abdullah-Al Kaiser", "Akhilesh Jaiswal", "Ajey P. Jacob", "Viktor Prasanna"], "title": "System-Level Performance Modeling of Photonic In-Memory Computing", "comment": null, "summary": "Photonic in-memory computing is a high-speed, low-energy alternative to traditional transistor-based digital computing that utilizes high photonic operating frequencies and bandwidths. In this work, we develop a comprehensive system-level performance model for photonic in-memory computing, capturing the effects of key latency sources such as external memory access and opto-electronic conversion. We perform algorithm-to-hardware mapping across a range of workloads, including the Sod shock tube problem, Matricized Tensor Times Khatri-Rao Product (MTTKRP), and the Vlasov-Maxwell equation, to evaluate how the latencies impact real-world high-performance computing workloads. Our performance model shows that, while accounting for system overheads, a compact 1x256 bit single-wavelength photonic SRAM array, fabricated using the standard silicon photonics process by GlobalFoundries, sustains up to 1.5 TOPS, 0.9 TOPS, and 1.3 TOPS on the Sod shock tube problem, MTTKRP, and the Vlasov-Maxwell equation with an average energy efficiency of 2.5 TOPS/W.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u5149\u5b50\u5185\u5b58\u8ba1\u7b97\u7684\u7cfb\u7edf\u7ea7\u6027\u80fd\u6a21\u578b\uff0c\u5206\u6790\u4e86\u5ef6\u8fdf\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5f71\u54cd\uff0c\u5c55\u793a\u4e861\u00d7256\u4f4d\u5355\u6ce2\u957f\u5149\u5b50SRAM\u9635\u5217\u5728\u591a\u4e2a\u79d1\u5b66\u8ba1\u7b97\u95ee\u9898\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5149\u5b50\u5185\u5b58\u8ba1\u7b97\u5177\u6709\u9ad8\u901f\u3001\u4f4e\u80fd\u8017\u7684\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u7cfb\u7edf\u7ea7\u6027\u80fd\u6a21\u578b\u6765\u8bc4\u4f30\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5ef6\u8fdf\u5f71\u54cd\uff0c\u4ee5\u63a8\u52a8\u5176\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u5168\u9762\u7684\u7cfb\u7edf\u7ea7\u6027\u80fd\u6a21\u578b\uff0c\u8003\u8651\u5916\u90e8\u5185\u5b58\u8bbf\u95ee\u548c\u5149\u7535\u8f6c\u6362\u7b49\u5173\u952e\u5ef6\u8fdf\u6e90\uff0c\u5e76\u5728Sod\u6fc0\u6ce2\u7ba1\u95ee\u9898\u3001MTTKRP\u548cVlasov-Maxwell\u65b9\u7a0b\u7b49\u591a\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u8fdb\u884c\u7b97\u6cd5\u5230\u786c\u4ef6\u7684\u6620\u5c04\u8bc4\u4f30\u3002", "result": "\u91c7\u7528GlobalFoundries\u6807\u51c6\u7845\u5149\u5b50\u5de5\u827a\u5236\u9020\u7684\u7d27\u51d1\u578b1\u00d7256\u4f4d\u5355\u6ce2\u957f\u5149\u5b50SRAM\u9635\u5217\uff0c\u5728Sod\u6fc0\u6ce2\u7ba1\u95ee\u9898\u3001MTTKRP\u548cVlasov-Maxwell\u65b9\u7a0b\u4e0a\u5206\u522b\u5b9e\u73b0\u4e861.5 TOPS\u30010.9 TOPS\u548c1.3 TOPS\u7684\u6027\u80fd\uff0c\u5e73\u5747\u80fd\u6548\u8fbe\u52302.5 TOPS/W\u3002", "conclusion": "\u5149\u5b50\u5185\u5b58\u8ba1\u7b97\u5728\u8003\u8651\u7cfb\u7edf\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u9ad8\u80fd\u6548\uff0c\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.01024", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.01024", "abs": "https://arxiv.org/abs/2602.01024", "authors": ["Zhiwen Pang", "Kang Wei", "Long Shi", "Zhe Wang", "Jun Li", "Feng Shu"], "title": "Low-latency Federated LLM Fine-tuning Over Wireless Networks", "comment": null, "summary": "Recently, federated large language models (LLMs) have drawn significant attention thanks to coupled capabilities of LLMs and federated learning (FL) that address privacy concerns in collaborative fine-tuning. However, due to large-scale parameters of LLMs, existing federated LLM fine-tuning frameworks incur significant challenges in resource-constrained clients characterized by heterogeneous computing capabilities and random wireless channels. To address this issue, we propose a joint client-specific pruning and bandwidth allocation (JCPBA) framework for federated LLMs to improve the fine-tuning efficiency over the wireless networks. Specifically, we formulate a fine-tuning latency minimization problem by jointly optimizing pruning rates and bandwidth allocations. Furthermore, we solve this optimization problem using a block coordinate descent method. Extensive experiments on the datasets of Yahoo Answers and GSM8K demonstrate that the proposed framework significantly reduces wall-clock fine-tuning time compared with state-of-the-art baselines and gains equal or lower test loss at the cost of lower computation and communication overhead.", "AI": {"tldr": "\u63d0\u51faJCPBA\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u526a\u679d\u7387\u548c\u5e26\u5bbd\u5206\u914d\u6765\u964d\u4f4e\u8054\u90a6\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5fae\u8c03\u5ef6\u8fdf", "motivation": "\u8054\u90a6\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u4e86LLM\u548c\u8054\u90a6\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u80fd\u89e3\u51b3\u534f\u4f5c\u5fae\u8c03\u4e2d\u7684\u9690\u79c1\u95ee\u9898\u3002\u4f46\u7531\u4e8eLLM\u53c2\u6570\u91cf\u5de8\u5927\uff0c\u73b0\u6709\u8054\u90a6LLM\u5fae\u8c03\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5ba2\u6237\u7aef\uff08\u5177\u6709\u5f02\u6784\u8ba1\u7b97\u80fd\u529b\u548c\u968f\u673a\u65e0\u7ebf\u4fe1\u9053\uff09\u4e0a\u9762\u4e34\u663e\u8457\u6311\u6218", "method": "\u63d0\u51fa\u8054\u5408\u5ba2\u6237\u7aef\u7279\u5b9a\u526a\u679d\u548c\u5e26\u5bbd\u5206\u914d\uff08JCPBA\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5757\u5750\u6807\u4e0b\u964d\u6cd5\u8054\u5408\u4f18\u5316\u526a\u679d\u7387\u548c\u5e26\u5bbd\u5206\u914d\uff0c\u4ee5\u6700\u5c0f\u5316\u5fae\u8c03\u5ef6\u8fdf", "result": "\u5728Yahoo Answers\u548cGSM8K\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u663e\u8457\u51cf\u5c11\u4e86\u5fae\u8c03\u65f6\u95f4\uff0c\u4ee5\u66f4\u4f4e\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u83b7\u5f97\u4e86\u76f8\u540c\u6216\u66f4\u4f4e\u7684\u6d4b\u8bd5\u635f\u5931", "conclusion": "JCPBA\u6846\u67b6\u80fd\u6709\u6548\u63d0\u9ad8\u8054\u90a6\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5fae\u8c03\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898"}}
{"id": "2602.01404", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.01404", "abs": "https://arxiv.org/abs/2602.01404", "authors": ["Zhouzi Li", "Cindy Zhu", "Arpan Mukhopadhyay", "Mor Harchol-Balter", "Benjamin Berg"], "title": "BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation", "comment": null, "summary": "The past decade has seen a dramatic increase in demand for GPUs to train Machine Learning (ML) models. Because it is prohibitively expensive for most organizations to build and maintain a large GPU cluster, organizations instead choose to rent GPUs from cloud providers. The customer is responsible for devising a policy for (i) deciding how many GPUs to rent at every moment in time to process a stream of ML training jobs and (ii) allocating the rented GPUs among the currently active jobs in the system. Because ML training jobs can be parallelized across different numbers of GPUs, the customer generally has many options for how many GPUs to use for each job. Allocating more GPUs to a single training job will cause the job to complete more quickly. However, the customer pays for each GPU-hour they use, and a training job receives a diminishing marginal benefit from running on additional GPUs. Hence, allocating too many GPUs to a single training job can dramatically increase the overall cost that the customer pays to the cloud provider. This gives rise to a cost-performance tradeoff that customers must balance when running training jobs in the cloud.\n  To balance the cost-performance tradeoff, we develop BOA Constrictor, a new scheduler for ML training jobs which uses a Budget-Optimal Allocation (BOA) policy to squeeze the highest level of performance out of a cloud-deployed GPU cluster given a fixed budget constraint. We explicitly formulate the problem as a budget-constrained scheduling problem and derive the BOA policy which minimizes the average job completion time (JCT) of a stream of arriving jobs subject to the user's budget. For a given budget level, we demonstrate that BOA Constrictor can reduce average JCT by 1.6 times in small-scale implementation experiments and by 2 times in detailed, large-scale simulations compared to state-of-the-art heuristic based schedulers.", "AI": {"tldr": "BOA Constrictor\uff1a\u4e00\u79cd\u57fa\u4e8e\u9884\u7b97\u6700\u4f18\u5206\u914d\u7b56\u7565\u7684ML\u8bad\u7ec3\u4f5c\u4e1a\u8c03\u5ea6\u5668\uff0c\u5728\u56fa\u5b9a\u9884\u7b97\u7ea6\u675f\u4e0b\u6700\u5927\u5316GPU\u96c6\u7fa4\u6027\u80fd", "motivation": "\u4e91\u4e0aML\u8bad\u7ec3\u9762\u4e34\u6210\u672c-\u6027\u80fd\u6743\u8861\uff1a\u79df\u7528\u66f4\u591aGPU\u53ef\u52a0\u901f\u4f5c\u4e1a\u5b8c\u6210\u4f46\u6210\u672c\u589e\u52a0\uff0c\u800cGPU\u5e76\u884c\u5316\u5b58\u5728\u8fb9\u9645\u6548\u76ca\u9012\u51cf\u3002\u7ec4\u7ec7\u9700\u8981\u667a\u80fd\u8c03\u5ea6\u7b56\u7565\u6765\u5e73\u8861\u8fd9\u4e00\u77db\u76fe\u3002", "method": "\u63d0\u51faBOA Constrictor\u8c03\u5ea6\u5668\uff0c\u91c7\u7528\u9884\u7b97\u6700\u4f18\u5206\u914d(BOA)\u7b56\u7565\u3002\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u9884\u7b97\u7ea6\u675f\u8c03\u5ea6\u95ee\u9898\uff0c\u63a8\u5bfc\u51fa\u5728\u7528\u6237\u9884\u7b97\u7ea6\u675f\u4e0b\u6700\u5c0f\u5316\u5e73\u5747\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u7684BOA\u7b56\u7565\u3002", "result": "\u5728\u7ed9\u5b9a\u9884\u7b97\u6c34\u5e73\u4e0b\uff0cBOA Constrictor\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u542f\u53d1\u5f0f\u8c03\u5ea6\u5668\uff1a\u5c0f\u89c4\u6a21\u5b9e\u9a8c\u5e73\u5747JCT\u964d\u4f4e1.6\u500d\uff0c\u5927\u89c4\u6a21\u4eff\u771f\u5e73\u5747JCT\u964d\u4f4e2\u500d\u3002", "conclusion": "BOA Constrictor\u80fd\u6709\u6548\u5e73\u8861\u4e91\u4e0aML\u8bad\u7ec3\u7684\u6210\u672c-\u6027\u80fd\u6743\u8861\uff0c\u5728\u56fa\u5b9a\u9884\u7b97\u4e0b\u663e\u8457\u63d0\u5347GPU\u96c6\u7fa4\u6027\u80fd\uff0c\u4e3a\u7ec4\u7ec7\u63d0\u4f9b\u7ecf\u6d4e\u9ad8\u6548\u7684ML\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.01411", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.01411", "abs": "https://arxiv.org/abs/2602.01411", "authors": ["Zhouzi Li", "Mor Harchol-Balter", "Benjamin Berg"], "title": "Mean field optimal Core Allocation across Malleable jobs", "comment": null, "summary": "Modern data centers and cloud computing clusters are increasingly running workloads composed of malleable jobs. A malleable job can be parallelized across any number of cores, yet the job typically exhibits diminishing marginal returns for each additional core on which it runs. This can be seen in the concavity of a job's speedup function, which describes the job's processing speed as a function of the number of cores on which it runs.\n  Given the prevalence of malleable jobs, several theoretical works have posed the problem of how to allocate a fixed number of cores across a stream of arriving malleable jobs so as to minimize the mean response time across jobs. We refer to this as the Core Allocation to Malleable jobs (CAM) problem. We solve the CAM problem under a highly general setting, allowing for multiple job classes, each with an arbitrary concave speedup function and holding costs (weight). Furthermore, we allow for generally distributed inter-arrival times and job sizes.\n  We analyze the CAM problem in the mean field asymptotic regime and derive two distinct mean field optimal policies, FW-CAM and WHAM. FW-CAM is interesting because it demonstrates a new intuition: in the mean field regime, job sizes are not relevant in finding an optimal policy. WHAM (Whittle Allocation for Malleable jobs) is interesting because it is asymptotically optimal and also serves as a good heuristic even outside of the asymptotic regime. Notably, none of the policies previously proposed in the literature are mean field optimal when jobs may follow different speedup functions.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u9488\u5bf9\u53ef\u5851\u4f5c\u4e1a\u7684\u6838\u5fc3\u5206\u914d\u7b56\u7565\uff1aFW-CAM\u548cWHAM\uff0c\u5728\u5e73\u5747\u573a\u6e10\u8fd1\u6761\u4ef6\u4e0b\u8fbe\u5230\u6700\u4f18\uff0c\u89e3\u51b3\u4e86\u591a\u7c7b\u4f5c\u4e1a\u3001\u4efb\u610f\u51f9\u52a0\u901f\u51fd\u6570\u548c\u4e00\u822c\u5206\u5e03\u4e0b\u7684\u6838\u5fc3\u5206\u914d\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u548c\u4e91\u8ba1\u7b97\u96c6\u7fa4\u4e2d\u53ef\u5851\u4f5c\u4e1a\u65e5\u76ca\u666e\u904d\uff0c\u8fd9\u7c7b\u4f5c\u4e1a\u53ef\u5728\u4efb\u610f\u6570\u91cf\u6838\u5fc3\u4e0a\u5e76\u884c\u5316\u4f46\u5b58\u5728\u8fb9\u9645\u6536\u76ca\u9012\u51cf\u3002\u73b0\u6709\u7406\u8bba\u5de5\u4f5c\u5173\u6ce8\u5982\u4f55\u5728\u56fa\u5b9a\u6838\u5fc3\u6570\u4e0b\u5206\u914d\u6838\u5fc3\u4ee5\u6700\u5c0f\u5316\u4f5c\u4e1a\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u591a\u7c7b\u4f5c\u4e1a\u3001\u4e0d\u540c\u51f9\u52a0\u901f\u51fd\u6570\u548c\u4e00\u822c\u5206\u5e03\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5728\u5e73\u5747\u573a\u6e10\u8fd1\u6761\u4ef6\u4e0b\u5206\u6790CAM\u95ee\u9898\uff0c\u63a8\u5bfc\u51fa\u4e24\u79cd\u5e73\u5747\u573a\u6700\u4f18\u7b56\u7565\uff1a1) FW-CAM\u7b56\u7565\uff0c\u57fa\u4e8e\u65b0\u76f4\u89c9\uff08\u5e73\u5747\u573a\u4e0b\u4f5c\u4e1a\u5927\u5c0f\u4e0d\u5f71\u54cd\u6700\u4f18\u7b56\u7565\uff09\uff1b2) WHAM\u7b56\u7565\uff0c\u57fa\u4e8eWhittle\u7d22\u5f15\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u6e10\u8fd1\u6700\u4f18\uff0c\u5728\u975e\u6e10\u8fd1\u6761\u4ef6\u4e0b\u4e5f\u662f\u826f\u597d\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "result": "FW-CAM\u5c55\u793a\u4e86\u5e73\u5747\u573a\u4e0b\u4f5c\u4e1a\u5927\u5c0f\u4e0d\u76f8\u5173\u7684\u53cd\u76f4\u89c9\u7ed3\u679c\uff1bWHAM\u88ab\u8bc1\u660e\u662f\u6e10\u8fd1\u6700\u4f18\u7684\uff0c\u4e14\u5728\u975e\u6e10\u8fd1\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\u3002\u73b0\u6709\u6587\u732e\u4e2d\u7684\u7b56\u7565\u5728\u4f5c\u4e1a\u9075\u5faa\u4e0d\u540c\u52a0\u901f\u51fd\u6570\u65f6\u5747\u975e\u5e73\u5747\u573a\u6700\u4f18\u3002", "conclusion": "\u63d0\u51fa\u7684FW-CAM\u548cWHAM\u7b56\u7565\u89e3\u51b3\u4e86\u901a\u7528CAM\u95ee\u9898\uff0cWHAM\u5c24\u5176\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u65e2\u4fdd\u8bc1\u6e10\u8fd1\u6700\u4f18\u6027\uff0c\u53c8\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u4f5c\u4e3a\u6709\u6548\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u5728\u591a\u7c7b\u53ef\u5851\u4f5c\u4e1a\u6838\u5fc3\u5206\u914d\u4f18\u5316\u65b9\u9762\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.01798", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.01798", "abs": "https://arxiv.org/abs/2602.01798", "authors": ["Leonardo Pelonero", "Fabio Vitello", "Eva Sciacca", "Mauro Imbrosciano", "Salvatore Scavo", "Ugo Becciani"], "title": "Developing a Portable Solution for Post-Event Analysis Pipelines", "comment": "Preprint of IWSG 2025 Conference Proceeding", "summary": "In recent years, the monitoring and study of natural hazards have gained significant attention, particularly due to climate change, which exacerbates incidents like floods, droughts, storm surges, and landslides. Together with the constant risk of earthquakes, these climate-induced events highlight the critical necessity for enhanced risk assessment and mitigation strategies in susceptible areas such as Italy.\n  In this work, we present a Science Gateway framework for the development of portable and fully automated post-event analysis pipelines integrating Photogrammetry techniques, Data Visualization and Artificial Intelligence technologies, applied on aerial images, to assess extreme natural events and evaluate their impact on risk-exposed assets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u79d1\u5b66\u7f51\u5173\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u4fbf\u643a\u5f0f\u3001\u5168\u81ea\u52a8\u7684\u707e\u540e\u5206\u6790\u6d41\u7a0b\uff0c\u6574\u5408\u6444\u5f71\u6d4b\u91cf\u3001\u6570\u636e\u53ef\u89c6\u5316\u548cAI\u6280\u672f\uff0c\u901a\u8fc7\u822a\u62cd\u56fe\u50cf\u8bc4\u4f30\u6781\u7aef\u81ea\u7136\u707e\u5bb3\u53ca\u5176\u5bf9\u98ce\u9669\u8d44\u4ea7\u7684\u5f71\u54cd\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u6d2a\u6c34\u3001\u5e72\u65f1\u3001\u98ce\u66b4\u6f6e\u548c\u6ed1\u5761\u7b49\u81ea\u7136\u707e\u5bb3\uff0c\u52a0\u4e0a\u6301\u7eed\u7684\u5730\u9707\u98ce\u9669\uff0c\u51f8\u663e\u4e86\u5728\u610f\u5927\u5229\u7b49\u6613\u53d7\u707e\u5730\u533a\u52a0\u5f3a\u98ce\u9669\u8bc4\u4f30\u548c\u7f13\u89e3\u7b56\u7565\u7684\u8feb\u5207\u9700\u8981\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u79d1\u5b66\u7f51\u5173\u6846\u67b6\uff0c\u6574\u5408\u6444\u5f71\u6d4b\u91cf\u6280\u672f\u3001\u6570\u636e\u53ef\u89c6\u5316\u548c\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u5e94\u7528\u4e8e\u822a\u62cd\u56fe\u50cf\uff0c\u6784\u5efa\u4fbf\u643a\u5f0f\u3001\u5168\u81ea\u52a8\u7684\u707e\u540e\u5206\u6790\u6d41\u7a0b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u6027\u7684\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u5904\u7406\u707e\u540e\u8bc4\u4f30\uff0c\u7ed3\u5408\u591a\u79cd\u6280\u672f\u624b\u6bb5\u5bf9\u81ea\u7136\u707e\u5bb3\u5f71\u54cd\u8fdb\u884c\u91cf\u5316\u5206\u6790\u3002", "conclusion": "\u8be5\u79d1\u5b66\u7f51\u5173\u6846\u67b6\u4e3a\u6781\u7aef\u81ea\u7136\u707e\u5bb3\u7684\u707e\u540e\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u98ce\u9669\u66b4\u9732\u8d44\u4ea7\u7684\u5f71\u54cd\u8bc4\u4f30\u548c\u7f13\u89e3\u7b56\u7565\u5236\u5b9a\u3002"}}
{"id": "2602.01872", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.01872", "abs": "https://arxiv.org/abs/2602.01872", "authors": ["Chongyang Xu", "Christoph Siebenbrunner", "Laurent Bindschaedler"], "title": "Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training", "comment": null, "summary": "Cross-partition edges dominate the cost of distributed GNN training: fetching remote features and activations per iteration overwhelms the network as graphs deepen and partition counts grow. Grappa is a distributed GNN training framework that enforces gradient-only communication: during each iteration, partitions train in isolation and exchange only gradients for the global update. To recover accuracy lost to isolation, Grappa (i) periodically repartitions to expose new neighborhoods and (ii) applies a lightweight coverage-corrected gradient aggregation inspired by importance sampling. We prove the corrected estimator is asymptotically unbiased under standard support and boundedness assumptions, and we derive a batch-level variant for compatibility with common deep-learning packages that minimizes mean-squared deviation from the ideal node-level correction. We also introduce a shrinkage version that improves stability in practice. Empirical results on real and synthetic graphs show that Grappa trains GNNs 4 times faster on average (up to 13 times) than state-of-the-art systems, achieves better accuracy especially for deeper models, and sustains training at the trillion-edge scale on commodity hardware. Grappa is model-agnostic, supports full-graph and mini-batch training, and does not rely on high-bandwidth interconnects or caching.", "AI": {"tldr": "Grappa\u662f\u4e00\u4e2a\u5206\u5e03\u5f0fGNN\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4ec5\u4ea4\u6362\u68af\u5ea6\u800c\u975e\u7279\u5f81/\u6fc0\u6d3b\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u4f7f\u7528\u5468\u671f\u6027\u91cd\u5206\u533a\u548c\u91cd\u8981\u6027\u91c7\u6837\u6821\u6b63\u6765\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u5728\u771f\u5b9e\u548c\u5408\u6210\u56fe\u4e0a\u5b9e\u73b04\u500d\u52a0\u901f\uff08\u6700\u9ad813\u500d\uff09\u3002", "motivation": "\u5206\u5e03\u5f0fGNN\u8bad\u7ec3\u4e2d\uff0c\u8de8\u5206\u533a\u8fb9\u7684\u8fdc\u7a0b\u7279\u5f81\u548c\u6fc0\u6d3b\u83b7\u53d6\u662f\u4e3b\u8981\u5f00\u9500\uff0c\u968f\u7740\u56fe\u6df1\u5ea6\u589e\u52a0\u548c\u5206\u533a\u6570\u91cf\u589e\u957f\uff0c\u7f51\u7edc\u901a\u4fe1\u6210\u4e3a\u74f6\u9888\u3002", "method": "1) \u5f3a\u5236\u4ec5\u68af\u5ea6\u901a\u4fe1\uff1a\u6bcf\u4e2a\u8fed\u4ee3\u4e2d\u5206\u533a\u72ec\u7acb\u8bad\u7ec3\uff0c\u4ec5\u4ea4\u6362\u68af\u5ea6\u8fdb\u884c\u5168\u5c40\u66f4\u65b0\uff1b2) \u5468\u671f\u6027\u91cd\u5206\u533a\u4ee5\u66b4\u9732\u65b0\u90bb\u57df\uff1b3) \u57fa\u4e8e\u91cd\u8981\u6027\u91c7\u6837\u7684\u8f7b\u91cf\u7ea7\u8986\u76d6\u6821\u6b63\u68af\u5ea6\u805a\u5408\uff1b4) \u63a8\u5bfc\u6279\u5904\u7406\u7ea7\u53d8\u4f53\u4ee5\u517c\u5bb9\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff1b5) \u5f15\u5165\u6536\u7f29\u7248\u672c\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u56fe\u4e0a\uff0cGrappa\u5e73\u5747\u8bad\u7ec3\u901f\u5ea6\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u5feb4\u500d\uff08\u6700\u9ad813\u500d\uff09\uff0c\u5bf9\u66f4\u6df1\u6a21\u578b\u83b7\u5f97\u66f4\u597d\u51c6\u786e\u6027\uff0c\u5728\u5546\u54c1\u786c\u4ef6\u4e0a\u652f\u6301\u4e07\u4ebf\u8fb9\u89c4\u6a21\u8bad\u7ec3\u3002\u6846\u67b6\u6a21\u578b\u65e0\u5173\uff0c\u652f\u6301\u5168\u56fe\u548cmini-batch\u8bad\u7ec3\uff0c\u4e0d\u4f9d\u8d56\u9ad8\u5e26\u5bbd\u4e92\u8fde\u6216\u7f13\u5b58\u3002", "conclusion": "Grappa\u901a\u8fc7\u68af\u5ea6\u4e13\u7528\u901a\u4fe1\u3001\u5468\u671f\u6027\u91cd\u5206\u533a\u548c\u91cd\u8981\u6027\u91c7\u6837\u6821\u6b63\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0fGNN\u8bad\u7ec3\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u53ef\u6269\u5c55\u5230\u5927\u89c4\u6a21\u56fe\u6570\u636e\u3002"}}
{"id": "2602.02204", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.02204", "abs": "https://arxiv.org/abs/2602.02204", "authors": ["Peiqi Yin", "Jiangyun Zhu", "Han Gao", "Chenguang Zheng", "Yongxiang Huang", "Taichang Zhou", "Ruirui Yang", "Weizhi Liu", "Weiqing Chen", "Canlin Guo", "Didan Deng", "Zifeng Mo", "Cong Wang", "James Cheng", "Roger Wang", "Hongsheng Liu"], "title": "vLLM-Omni: Fully Disaggregated Serving for Any-to-Any Multimodal Models", "comment": "12 pages, 8 figures", "summary": "Any-to-any multimodal models that jointly handle text, images, video, and audio represent a significant advance in multimodal AI. However, their complex architectures (typically combining multiple autoregressive LLMs, diffusion transformers, and other specialized components) pose substantial challenges for efficient model serving. Existing serving systems are mainly tailored to a single paradigm, such as autoregressive LLMs for text generation or diffusion transformers for visual generation. They lack support for any-to-any pipelines that involve multiple interconnected model components. As a result, developers must manually handle cross-stage interactions, leading to huge performance degradation. We present vLLM-Omni, a fully disaggregated serving system for any-to-any models. vLLM-Omni features a novel stage abstraction that enables users to decompose complex any-to-any architectures into interconnected stages represented as a graph, and a disaggregated stage execution backend that optimizes resource utilization and throughput across stages. Each stage is independently served by an LLM or diffusion engine with per-stage request batching, flexible GPU allocation, and unified inter-stage connectors for data routing. Experimental results demonstrate that vLLM-Omni reduces job completion time (JCT) by up to 91.4% compared to baseline methods. The code is public available at https://github.com/vllm-project/vllm-omni.", "AI": {"tldr": "vLLM-Omni\u662f\u4e00\u4e2a\u7528\u4e8e\u4efb\u610f\u5230\u4efb\u610f\u591a\u6a21\u6001\u6a21\u578b\u7684\u5168\u89e3\u8026\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u9636\u6bb5\u62bd\u8c61\u548c\u56fe\u8868\u793a\u4f18\u5316\u590d\u6742\u67b6\u6784\u7684\u670d\u52a1\u6027\u80fd\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c11\u9ad8\u8fbe91.4%\u7684\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u7684\u670d\u52a1\u7cfb\u7edf\u4e3b\u8981\u9488\u5bf9\u5355\u4e00\u8303\u5f0f\uff08\u5982\u6587\u672c\u751f\u6210\u7684\u81ea\u52a8\u56de\u5f52LLM\u6216\u89c6\u89c9\u751f\u6210\u7684\u6269\u6563\u53d8\u6362\u5668\uff09\uff0c\u7f3a\u4e4f\u5bf9\u6d89\u53ca\u591a\u4e2a\u4e92\u8fde\u6a21\u578b\u7ec4\u4ef6\u7684\u4efb\u610f\u5230\u4efb\u610f\u7ba1\u9053\u7684\u652f\u6301\uff0c\u5bfc\u81f4\u5f00\u53d1\u4eba\u5458\u5fc5\u987b\u624b\u52a8\u5904\u7406\u8de8\u9636\u6bb5\u4ea4\u4e92\uff0c\u9020\u6210\u5de8\u5927\u7684\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51favLLM-Omni\u7cfb\u7edf\uff0c\u5305\u542b\u65b0\u9896\u7684\u9636\u6bb5\u62bd\u8c61\uff0c\u5141\u8bb8\u7528\u6237\u5c06\u590d\u6742\u7684\u4efb\u610f\u5230\u4efb\u610f\u67b6\u6784\u5206\u89e3\u4e3a\u56fe\u8868\u793a\u7684\u4e92\u8fde\u9636\u6bb5\uff0c\u4ee5\u53ca\u89e3\u8026\u7684\u9636\u6bb5\u6267\u884c\u540e\u7aef\uff0c\u4f18\u5316\u8de8\u9636\u6bb5\u7684\u8d44\u6e90\u5229\u7528\u7387\u548c\u541e\u5410\u91cf\u3002\u6bcf\u4e2a\u9636\u6bb5\u7531LLM\u6216\u6269\u6563\u5f15\u64ce\u72ec\u7acb\u670d\u52a1\uff0c\u5177\u6709\u6bcf\u9636\u6bb5\u8bf7\u6c42\u6279\u5904\u7406\u3001\u7075\u6d3b\u7684GPU\u5206\u914d\u548c\u7edf\u4e00\u7684\u9636\u6bb5\u95f4\u8fde\u63a5\u5668\u8fdb\u884c\u6570\u636e\u8def\u7531\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cvLLM-Omni\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5c06\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff08JCT\uff09\u51cf\u5c11\u4e86\u9ad8\u8fbe91.4%\u3002\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u516c\u5f00\u3002", "conclusion": "vLLM-Omni\u4e3a\u4efb\u610f\u5230\u4efb\u610f\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u89e3\u8026\u67b6\u6784\u548c\u4f18\u5316\u8d44\u6e90\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u591a\u6a21\u6001AI\u6a21\u578b\u7684\u670d\u52a1\u6027\u80fd\u3002"}}
{"id": "2602.02234", "categories": ["cs.DC", "physics.chem-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.02234", "abs": "https://arxiv.org/abs/2602.02234", "authors": ["Andong Hu", "Luca Pennati", "Stefano Markidis", "Ivy Peng"], "title": "Enabling AI Deep Potentials for Ab Initio-quality Molecular Dynamics Simulations in GROMACS", "comment": null, "summary": "State-of-the-art AI deep potentials provide ab initio-quality results, but at a fraction of the computational cost of first-principles quantum mechanical calculations, such as density functional theory. In this work, we bring AI deep potentials into GROMACS, a production-level Molecular Dynamics (MD) code, by integrating with DeePMD-kit that provides domain-specific deep learning (DL) models of interatomic potential energy and force fields. In particular, we enable AI deep potentials inference across multiple DP model families and DL backends by coupling GROMACS Neural Network Potentials with the C++/CUDA backend in DeePMD-kit. We evaluate two recent large-atom-model architectures, DPA2 that is based on the attention mechanism and DPA3 that is based on GNN, in GROMACS using four ab initio-quality protein-in-water benchmarks (1YRF, 1UBQ, 3LZM, 2PTC) on NVIDIA A100 and GH200 GPUs. Our results show that DPA2 delivers up to 4.23x and 3.18x higher throughput than DPA3 on A100 and GH200 GPUs, respectively. We also provide a characterization study to further contrast DPA2 and DPA3 in throughput, memory usage, and kernel-level execution on GPUs. Our findings identify kernel-launch overhead and domain-decomposed inference as the main optimization priorities for AI deep potentials in production MD simulations.", "AI": {"tldr": "\u5c06AI\u6df1\u5ea6\u52bf\u80fd\u96c6\u6210\u5230GROMACS\u5206\u5b50\u52a8\u529b\u5b66\u8f6f\u4ef6\u4e2d\uff0c\u901a\u8fc7DeePMD-kit\u652f\u6301\u591a\u79cd\u6df1\u5ea6\u52bf\u80fd\u6a21\u578b\uff0c\u8bc4\u4f30\u4e86DPA2\u548cDPA3\u4e24\u79cd\u67b6\u6784\u5728\u86cb\u767d\u8d28\u6c34\u6eb6\u6db2\u6a21\u62df\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "AI\u6df1\u5ea6\u52bf\u80fd\u80fd\u4ee5\u8ba1\u7b97\u6210\u672c\u8fdc\u4f4e\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u7684\u65b9\u5f0f\u63d0\u4f9bab initio\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u4f46\u9700\u8981\u5c06\u5176\u96c6\u6210\u5230\u751f\u4ea7\u7ea7\u5206\u5b50\u52a8\u529b\u5b66\u8f6f\u4ef6\u4e2d\u624d\u80fd\u5145\u5206\u53d1\u6325\u5176\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5c06GROMACS\u4e0eDeePMD-kit\u7684C++/CUDA\u540e\u7aef\u8026\u5408\uff0c\u5b9e\u73b0\u591a\u79cd\u6df1\u5ea6\u52bf\u80fd\u6a21\u578b\u7684\u63a8\u7406\uff1b\u4f7f\u7528\u56db\u79cd\u86cb\u767d\u8d28\u6c34\u6eb6\u6db2\u57fa\u51c6\uff081YRF\u30011UBQ\u30013LZM\u30012PTC\uff09\u5728NVIDIA A100\u548cGH200 GPU\u4e0a\u8bc4\u4f30DPA2\uff08\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\uff09\u548cDPA3\uff08\u57fa\u4e8eGNN\uff09\u4e24\u79cd\u67b6\u6784\u3002", "result": "DPA2\u5728A100\u548cGH200 GPU\u4e0a\u5206\u522b\u6bd4DPA3\u63d0\u4f9b\u9ad8\u8fbe4.23\u500d\u548c3.18\u500d\u7684\u541e\u5410\u91cf\uff1b\u6027\u80fd\u5206\u6790\u663e\u793a\u5185\u6838\u542f\u52a8\u5f00\u9500\u548c\u57df\u5206\u89e3\u63a8\u7406\u662f\u4e3b\u8981\u4f18\u5316\u65b9\u5411\u3002", "conclusion": "\u6210\u529f\u5c06AI\u6df1\u5ea6\u52bf\u80fd\u96c6\u6210\u5230GROMACS\u4e2d\uff0cDPA2\u67b6\u6784\u5728\u6027\u80fd\u4e0a\u4f18\u4e8eDPA3\uff0c\u4e3a\u751f\u4ea7\u7ea7\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u7684\u6df1\u5ea6\u52bf\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u660e\u786e\u65b9\u5411\u3002"}}
{"id": "2602.02335", "categories": ["cs.DC", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.02335", "abs": "https://arxiv.org/abs/2602.02335", "authors": ["Weiming Sheng", "Jinlang Wang", "Manuel Barros", "Aldrin Montana", "Jacopo Tagliabue", "Luca Bigon"], "title": "Building a Correct-by-Design Lakehouse. Data Contracts, Versioning, and Transactional Pipelines for Humans and Agents", "comment": "Pre-print (PaPoC 2026)", "summary": "Lakehouses are the default cloud platform for analytics and AI, but they become unsafe when untrusted actors concurrently operate on production data: upstream-downstream mismatches surface only at runtime, and multi-table pipelines can leak partial effects. Inspired by software engineering, we design Bauplan, a code-first lakehouse that aims to make (most) illegal states unrepresentable using familiar abstractions. Bauplan acts along three axes: typed table contracts to make pipeline boundaries checkable, Git-like data versioning for review and reproducibility, and transactional runs that guarantee pipeline-level atomicity. We report early results from a lightweight formal transaction model and discuss future work motivated by counterexamples.", "AI": {"tldr": "Bauplan\u662f\u4e00\u4e2a\u4ee3\u7801\u4f18\u5148\u7684\u6e56\u4ed3\uff0c\u901a\u8fc7\u7c7b\u578b\u5316\u8868\u5951\u7ea6\u3001Git\u5f0f\u6570\u636e\u7248\u672c\u63a7\u5236\u548c\u4e8b\u52a1\u6027\u8fd0\u884c\u6765\u786e\u4fdd\u6570\u636e\u5b89\u5168\uff0c\u9632\u6b62\u4e0a\u6e38-\u4e0b\u6e38\u4e0d\u5339\u914d\u548c\u90e8\u5206\u6548\u679c\u6cc4\u6f0f\u3002", "motivation": "\u5f53\u524d\u6e56\u4ed3\u5728\u4e0d\u53d7\u4fe1\u4efb\u7684\u53c2\u4e0e\u8005\u5e76\u53d1\u64cd\u4f5c\u751f\u4ea7\u6570\u636e\u65f6\u5b58\u5728\u5b89\u5168\u95ee\u9898\uff1a\u4e0a\u6e38-\u4e0b\u6e38\u4e0d\u5339\u914d\u53ea\u5728\u8fd0\u884c\u65f6\u51fa\u73b0\uff0c\u591a\u8868\u7ba1\u9053\u53ef\u80fd\u6cc4\u6f0f\u90e8\u5206\u6548\u679c\uff0c\u9700\u8981\u66f4\u5b89\u5168\u7684\u67b6\u6784\u3002", "method": "\u8bbe\u8ba1Bauplan\u4ee3\u7801\u4f18\u5148\u6e56\u4ed3\uff0c\u91c7\u7528\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1) \u7c7b\u578b\u5316\u8868\u5951\u7ea6\u4f7f\u7ba1\u9053\u8fb9\u754c\u53ef\u68c0\u67e5\uff1b2) Git\u5f0f\u6570\u636e\u7248\u672c\u63a7\u5236\u7528\u4e8e\u5ba1\u67e5\u548c\u53ef\u91cd\u590d\u6027\uff1b3) \u4e8b\u52a1\u6027\u8fd0\u884c\u4fdd\u8bc1\u7ba1\u9053\u7ea7\u539f\u5b50\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u5f62\u5f0f\u5316\u4e8b\u52a1\u6a21\u578b\u5e76\u62a5\u544a\u4e86\u65e9\u671f\u7ed3\u679c\uff0c\u8ba8\u8bba\u4e86\u7531\u53cd\u4f8b\u6fc0\u53d1\u7684\u672a\u6765\u5de5\u4f5c\u65b9\u5411\u3002", "conclusion": "Bauplan\u901a\u8fc7\u8f6f\u4ef6\u5de5\u7a0b\u542f\u53d1\u7684\u8bbe\u8ba1\uff0c\u4f7f\u5927\u591a\u6570\u975e\u6cd5\u72b6\u6001\u65e0\u6cd5\u8868\u793a\uff0c\u4e3a\u5b89\u5168\u7684\u6570\u636e\u5206\u6790\u5e73\u53f0\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.02340", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.02340", "abs": "https://arxiv.org/abs/2602.02340", "authors": ["Gustav Schmid"], "title": "LCLs Beyond Bounded Degrees", "comment": null, "summary": "The study of Locally Checkable Labelings (LCLs) has led to a remarkably precise characterization of the distributed time complexities that can occur on bounded-degree trees. A central feature of this complexity landscape is the existence of strong gap results, which rule out large ranges of intermediate complexities. While it was initially hoped that these gaps might extend to more general graph classes, this has turned out not to be the case. In this work, we investigate a different direction: we remain in the class of trees, but allow arbitrarily large degrees.\n  We focus on the polynomial regime ($\u0398(n^{1/k} \\mid k \\in \\mathbb{N})$) and show that whether polynomial gap results persist in the unbounded-degree setting crucially depends on how LCLs are generalized beyond bounded degrees. We first demonstrate that if one allows LCLs to be defined using infinitely many local configurations, then the polynomial gaps disappear entirely: for every real exponent $0 < r \\leq 1$, there exists a locally checkable problem on trees with deterministic LOCAL complexity $\u0398(n^r)$.\n  Rather than stopping at this negative result, we identify a natural class of problems for which polynomial gap results can still be recovered. We introduce Locally Finite Labelings (LFLs), which formalize the intuition that ''every node must fall into one of finitely many local cases'', even in the presence of unbounded degrees.\n  Our main result shows that this restriction is sufficient to restore the polynomial gaps: for any LFL $\u03a0$ on trees with unbounded degrees, the deterministic LOCAL complexity of $\u03a0$ is either\n  - $\u0398(n^{1/k})$ for some integer $k \\geq 1$, or\n  - $O(\\log n)$.\n  Moreover, which case applies, and the corresponding value of $k$, can be determined solely from the description of $\u03a0$.", "AI": {"tldr": "\u5728\u65e0\u754c\u5ea6\u6811\u4e0a\u7684LCL\u95ee\u9898\u4e2d\uff0c\u591a\u9879\u5f0f\u95f4\u9699\u7684\u5b58\u5728\u6027\u53d6\u51b3\u4e8e\u95ee\u9898\u5b9a\u4e49\u65b9\u5f0f\uff1a\u82e5\u5141\u8bb8\u65e0\u9650\u591a\u5c40\u90e8\u914d\u7f6e\uff0c\u5219\u95f4\u9699\u6d88\u5931\uff1b\u82e5\u9650\u5236\u4e3a\u5c40\u90e8\u6709\u9650\u6807\u8bb0(LFLs)\uff0c\u5219\u80fd\u6062\u590d\u591a\u9879\u5f0f\u95f4\u9699\u3002", "motivation": "\u7814\u7a76\u5728\u65e0\u754c\u5ea6\u6811\u4e0a\uff0c\u5c40\u90e8\u53ef\u68c0\u67e5\u6807\u8bb0(LCLs)\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u662f\u5426\u4ecd\u4fdd\u6301\u591a\u9879\u5f0f\u95f4\u9699\u7279\u6027\uff0c\u63a2\u7d22\u4e0d\u540c\u95ee\u9898\u5b9a\u4e49\u65b9\u5f0f\u5bf9\u590d\u6742\u5ea6\u5206\u5e03\u7684\u5f71\u54cd\u3002", "method": "1. \u5c55\u793a\u82e5\u5141\u8bb8LCLs\u4f7f\u7528\u65e0\u9650\u591a\u5c40\u90e8\u914d\u7f6e\uff0c\u5219\u591a\u9879\u5f0f\u95f4\u9699\u6d88\u5931\uff1b2. \u5f15\u5165\u5c40\u90e8\u6709\u9650\u6807\u8bb0(LFLs)\u6982\u5ff5\uff0c\u9650\u5236\u8282\u70b9\u53ea\u80fd\u5c5e\u4e8e\u6709\u9650\u591a\u4e2a\u5c40\u90e8\u60c5\u51b5\uff1b3. \u8bc1\u660eLFLs\u80fd\u6062\u590d\u591a\u9879\u5f0f\u95f4\u9699\u7279\u6027\u3002", "result": "\u5bf9\u4e8e\u65e0\u754c\u5ea6\u6811\u4e0a\u7684LFLs\u95ee\u9898\uff0c\u786e\u5b9a\u6027LOCAL\u590d\u6742\u5ea6\u8981\u4e48\u662f\u0398(n^{1/k})\uff08k\u4e3a\u6574\u6570\uff09\uff0c\u8981\u4e48\u662fO(log n)\uff0c\u4e14\u53ef\u901a\u8fc7\u95ee\u9898\u63cf\u8ff0\u786e\u5b9a\u5177\u4f53\u5c5e\u4e8e\u54ea\u79cd\u60c5\u51b5\u3002", "conclusion": "\u5728\u65e0\u754c\u5ea6\u6811\u4e2d\uff0c\u591a\u9879\u5f0f\u95f4\u9699\u7684\u5b58\u5728\u6027\u53d6\u51b3\u4e8e\u95ee\u9898\u5b9a\u4e49\u7684\u9650\u5236\u7a0b\u5ea6\u3002\u901a\u8fc7\u5f15\u5165LFLs\u8fd9\u4e00\u81ea\u7136\u9650\u5236\uff0c\u53ef\u4ee5\u6062\u590d\u6709\u754c\u5ea6\u6811\u4e2d\u7684\u591a\u9879\u5f0f\u95f4\u9699\u7279\u6027\uff0c\u4e3a\u5206\u5e03\u5f0f\u8ba1\u7b97\u590d\u6742\u5ea6\u7406\u8bba\u63d0\u4f9b\u65b0\u89c1\u89e3\u3002"}}
{"id": "2602.02355", "categories": ["cs.DC", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.02355", "abs": "https://arxiv.org/abs/2602.02355", "authors": ["Amirreza Kazemi", "Seyed Mohammad Azimi-Abarghouyi", "Gabor Fodor", "Carlo Fischione"], "title": "Hierarchical Federated Learning with SignSGD: A Highly Communication-Efficient Approach", "comment": null, "summary": "Hierarchical federated learning (HFL) has emerged as a key architecture for large-scale wireless and Internet of Things systems, where devices communicate with nearby edge servers before reaching the cloud. In these environments, uplink bandwidth and latency impose strict communication limits, thereby making aggressive gradient compression essential. One-bit methods such as sign-based stochastic gradient descent (SignSGD) offer an attractive solution in flat federated settings, but existing theory and algorithms do not naturally extend to hierarchical settings. In particular, the interaction between majority-vote aggregation at the edge layer and model aggregation at the cloud layer, and its impact on end-to-end performance, remains unknown. To bridge this gap, we propose a highly communication-efficient sign-based HFL framework and develop its corresponding formulation for nonconvex learning, where devices send only signed stochastic gradients, edge servers combine them through majority-vote, and the cloud periodically averages the obtained edge models, while utilizing downlink quantization to broadcast the global model. We introduce the resulting scalable HFL algorithm, HierSignSGD, and provide the convergence analysis for SignSGD in a hierarchical setting. Our core technical contribution is a characterization of how biased sign compression, two-level aggregation intervals, and inter-cluster heterogeneity collectively affect convergence. Numerical experiments under homogeneous and heterogeneous data splits show that HierSignSGD, despite employing extreme compression, achieves accuracy comparable to or better than full-precision stochastic gradient descent while reducing communication cost in the process, and remains robust under aggressive downlink sparsification.", "AI": {"tldr": "\u63d0\u51faHierSignSGD\u7b97\u6cd5\uff0c\u4e00\u79cd\u7528\u4e8e\u5206\u5c42\u8054\u90a6\u5b66\u4e60\u7684\u9ad8\u6548\u901a\u4fe1\u7b26\u53f7\u68af\u5ea6\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u7f18\u5c42\u591a\u6570\u6295\u7968\u548c\u4e91\u5c42\u6a21\u578b\u805a\u5408\uff0c\u5728\u6781\u7aef\u538b\u7f29\u4e0b\u5b9e\u73b0\u4e0e\u5168\u7cbe\u5ea6SGD\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "motivation": "\u5206\u5c42\u8054\u90a6\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u65e0\u7ebf\u548c\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\u9762\u4e34\u4e25\u683c\u7684\u901a\u4fe1\u9650\u5236\uff0c\u73b0\u6709\u7684\u4e00\u6bd4\u7279\u65b9\u6cd5\uff08\u5982SignSGD\uff09\u5728\u6241\u5e73\u8054\u90a6\u8bbe\u7f6e\u4e2d\u6709\u6548\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u5206\u5c42\u67b6\u6784\u7684\u7406\u8bba\u548c\u7b97\u6cd5\u6269\u5c55\uff0c\u7279\u522b\u662f\u8fb9\u7f18\u5c42\u591a\u6570\u6295\u7968\u4e0e\u4e91\u5c42\u6a21\u578b\u805a\u5408\u7684\u4ea4\u4e92\u5f71\u54cd\u672a\u77e5\u3002", "method": "\u63d0\u51faHierSignSGD\u7b97\u6cd5\uff1a\u8bbe\u5907\u4ec5\u53d1\u9001\u7b26\u53f7\u5316\u968f\u673a\u68af\u5ea6\uff0c\u8fb9\u7f18\u670d\u52a1\u5668\u901a\u8fc7\u591a\u6570\u6295\u7968\u805a\u5408\uff0c\u4e91\u5c42\u5b9a\u671f\u5e73\u5747\u8fb9\u7f18\u6a21\u578b\uff0c\u540c\u65f6\u4f7f\u7528\u4e0b\u884c\u91cf\u5316\u5e7f\u64ad\u5168\u5c40\u6a21\u578b\u3002\u6838\u5fc3\u8d21\u732e\u662f\u5206\u6790\u6709\u504f\u7b26\u53f7\u538b\u7f29\u3001\u4e24\u7ea7\u805a\u5408\u95f4\u9694\u548c\u96c6\u7fa4\u95f4\u5f02\u8d28\u6027\u5bf9\u6536\u655b\u7684\u5f71\u54cd\u3002", "result": "\u5728\u5747\u5300\u548c\u975e\u5747\u5300\u6570\u636e\u5206\u5272\u4e0b\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cHierSignSGD\u5c3d\u7ba1\u91c7\u7528\u6781\u7aef\u538b\u7f29\uff0c\u4ecd\u80fd\u8fbe\u5230\u4e0e\u5168\u7cbe\u5ea6\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u76f8\u5f53\u6216\u66f4\u597d\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u5e76\u5728\u6fc0\u8fdb\u7684\u4e0b\u884c\u7a00\u758f\u5316\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "HierSignSGD\u4e3a\u5206\u5c42\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u901a\u4fe1\u7684\u7b26\u53f7\u68af\u5ea6\u538b\u7f29\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u7684\u6709\u6548\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u5c42\u67b6\u6784\u4e2d\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.02438", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.02438", "abs": "https://arxiv.org/abs/2602.02438", "authors": ["Lican Huang"], "title": "sVIRGO: A Scalable Virtual Tree Hierarchical Framework for Distributed Systems", "comment": "10 pages", "summary": "We propose sVIRGO, a scalable virtual tree hierarchical framework for large-scale distributed systems. sVIRGO constructs virtual hierarchical trees directly on physical nodes, allowing each node to assume multiple hierarchical roles without overlay networks. The hierarchy preserves locality and is organized into configurable layers within regions. Coordination across thousands of regions is achieved via virtual upper-layer roles dynamically mapped onto nodes up to the top layer.\n  Each region maintains multiple active coordinators that monitor local health and perform dynamic re-selection if failures occur. Temporary drops below the minimum threshold do not compromise coordination, ensuring near-zero recovery latency, bounded communication overhead, and exponentially reduced failure probability while maintaining safety, liveness, and robustness under mobile, interference-prone, or adversarial conditions.\n  Communication is decoupled from the hierarchy and may use multi-frequency wireless links. Two message hop strategies are supported: (i) with long-distance infrastructure-assisted channels, coordinators exploit the virtual tree to minimize hops; (ii) without such channels, messages propagate via adjacent regions.\n  sVIRGO also supports Layer-Scoped Command Execution. Commands and coordination actions are executed within the scope of each hierarchical layer, enabling efficient local and regional decision-making while limiting unnecessary global propagation.", "AI": {"tldr": "sVIRGO\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u865a\u62df\u6811\u5c42\u6b21\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u901a\u8fc7\u6784\u5efa\u865a\u62df\u5c42\u6b21\u6811\u76f4\u63a5\u5728\u7269\u7406\u8282\u70b9\u4e0a\u5b9e\u73b0\u591a\u89d2\u8272\u5206\u5c42\uff0c\u652f\u6301\u8de8\u6570\u5343\u4e2a\u533a\u57df\u7684\u534f\u8c03\uff0c\u5177\u6709\u8fd1\u96f6\u6062\u590d\u5ef6\u8fdf\u3001\u6709\u754c\u901a\u4fe1\u5f00\u9500\u548c\u6307\u6570\u7ea7\u964d\u4f4e\u7684\u6545\u969c\u6982\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u4f20\u7edf\u8986\u76d6\u7f51\u7edc\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3001\u5bb9\u9519\u6027\u548c\u534f\u8c03\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u79fb\u52a8\u3001\u5e72\u6270\u6216\u5bf9\u6297\u6027\u6761\u4ef6\u4e0b\u9700\u8981\u4fdd\u6301\u5b89\u5168\u6027\u548c\u6d3b\u8dc3\u6027\u7684\u573a\u666f\u3002", "method": "1. \u5728\u7269\u7406\u8282\u70b9\u4e0a\u76f4\u63a5\u6784\u5efa\u865a\u62df\u5c42\u6b21\u6811\uff0c\u8282\u70b9\u53ef\u627f\u62c5\u591a\u4e2a\u5c42\u6b21\u89d2\u8272\uff1b2. \u5c06\u5c42\u6b21\u7ec4\u7ec7\u4e3a\u533a\u57df\u5185\u7684\u53ef\u914d\u7f6e\u5c42\uff1b3. \u901a\u8fc7\u52a8\u6001\u6620\u5c04\u5230\u8282\u70b9\u7684\u865a\u62df\u4e0a\u5c42\u89d2\u8272\u5b9e\u73b0\u8de8\u533a\u57df\u534f\u8c03\uff1b4. \u6bcf\u4e2a\u533a\u57df\u7ef4\u62a4\u591a\u4e2a\u6d3b\u52a8\u534f\u8c03\u5668\u8fdb\u884c\u5065\u5eb7\u76d1\u63a7\u548c\u52a8\u6001\u91cd\u9009\uff1b5. \u652f\u6301\u4e24\u79cd\u6d88\u606f\u8df3\u8f6c\u7b56\u7565\uff1a\u57fa\u7840\u8bbe\u65bd\u8f85\u52a9\u901a\u9053\u6700\u5c0f\u5316\u8df3\u6570\uff0c\u6216\u65e0\u901a\u9053\u65f6\u901a\u8fc7\u76f8\u90bb\u533a\u57df\u4f20\u64ad\uff1b6. \u652f\u6301\u5c42\u8303\u56f4\u547d\u4ee4\u6267\u884c\u3002", "result": "\u5b9e\u73b0\u4e86\u8fd1\u96f6\u6062\u590d\u5ef6\u8fdf\u3001\u6709\u754c\u901a\u4fe1\u5f00\u9500\u3001\u6307\u6570\u7ea7\u964d\u4f4e\u7684\u6545\u969c\u6982\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u3001\u6d3b\u8dc3\u6027\u548c\u9c81\u68d2\u6027\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u5728\u79fb\u52a8\u3001\u5e72\u6270\u6216\u5bf9\u6297\u6027\u6761\u4ef6\u4e0b\u7684\u9ad8\u6548\u534f\u8c03\u3002", "conclusion": "sVIRGO\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u5bb9\u9519\u4e14\u9ad8\u6548\u7684\u5c42\u6b21\u534f\u8c03\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u865a\u62df\u6811\u7ed3\u6784\u3001\u591a\u534f\u8c03\u5668\u673a\u5236\u548c\u5c42\u8303\u56f4\u6267\u884c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
