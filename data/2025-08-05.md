<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.AR](#cs.AR) [Total: 7]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Efficient compilation and execution of synchronous programs via type-state programming](https://arxiv.org/abs/2508.01199)
*Avinash Malik*

Main category: cs.PL

TL;DR: 提出了一种线性时间编译技术，用于同步程序的自动机编译，生成更小更快的可执行文件。


<details>
  <summary>Details</summary>
Motivation: 同步程序在安全关键嵌入式软件中广泛应用，但多FSM同步组合会导致状态空间爆炸问题，编译效率和执行性能成为挑战。

Method: 引入基于图的改写规则和线性时间算法，生成FSM，并通过C++模板元编程编码为类型状态程序。

Result: 实验表明，编译时间和二进制大小与现有技术相当，执行时间平均快31-60%。

Conclusion: 该方法显著提升了同步程序的编译效率和执行性能。

Abstract: Synchronous programs are used extensively in implementation of safety
critical embedded software. Imperative synchronous programming languages model
multiple Finite State Machines (FSMs) executing in lockstep at logical clock
ticks. The synchronous view of time along with the FSM based design enables
easier formal verification. The synchronous composition of multiple FSMs,
during compilation, results in the well known state space explosion problem.
Hence, efficiently compiling imperative synchronous programs into small and
fast executables is challenging. This paper introduces a novel linear time
compilation technique for automata based compilation of synchronous programs.
Graph based rewrite rules for kernel programming constructs are introduced. A
linear time algorithm applies these rules to produce a FSM. The FSM is then
encoded into a type-state program using template meta-programming in C++.
Experimental results show that the compilation time and generated binary size
is comparable, while the execution times are on average 31-60% faster than
current state-of-the-art compilers.

</details>


### [2] [Proceedings 14th International Workshop on Trends in Functional Programming in Education](https://arxiv.org/abs/2508.02305)
*Rose Bohrer*

Main category: cs.PL

TL;DR: TFPIE是一个聚集研究人员、教师和专业人士的研讨会，专注于教育中函数式编程的应用。


<details>
  <summary>Details</summary>
Motivation: 促进函数式编程在教育中的应用，分享新颖和经过课堂验证的想法。

Method: 通过为期一天的开放讨论研讨会，采用会后出版物的评审流程。

Result: 为参与者提供一个交流平台，推动函数式编程在教育中的发展。

Conclusion: TFPIE旨在通过开放讨论和分享，促进教育中函数式编程的创新与实践。

Abstract: The goal of TFPIE is to gather researchers, teachers and professionals that
use, or are interested in the use of, functional programming in education.
TFPIE aims to be a venue where novel ideas, classroom-tested ideas and
work-in-progress on the use of functional programming in education are
discussed. The one-day workshop will foster a spirit of open discussion by
having a review process for publication after the workshop.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Deterministic Fault-Tolerant Local Load Balancing and its Applications against Adaptive Adversaries](https://arxiv.org/abs/2508.01373)
*Dariusz R. Kowalski,Jan Olkowski*

Main category: cs.DC

TL;DR: 本文提出了一种新型的确定性容错本地负载均衡（LLB）算法，适用于节点易故障的分布式网络，并展示了其在共识算法中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究在节点易故障的分布式网络中实现高效的本地负载均衡，并探索其在共识问题中的应用。

Method: 设计了一种确定性容错本地负载均衡算法，结合随机选择的虚拟通信图和负载均衡技术。

Result: 算法在故障情况下仍能快速收敛到平均值，并在共识问题中改进了通信复杂度。

Conclusion: 提出的算法在负载均衡和共识问题中均表现出色，是首个在自适应遗漏故障下同时接近最优的解决方案。

Abstract: Load balancing is among the basic primitives in distributed computing. In
this paper, we consider this problem when executed locally on a network with
nodes prone to failures. We show that there exist lightweight network
topologies that are immune to message delivery failures incurred by (at most) a
constant fraction of all nodes. More precisely, we design a novel deterministic
fault-tolerant local load balancing (LLB) algorithm, which, similarly to their
classical counterparts working in fault-free networks, has a relatively simple
structure and guarantees exponentially fast convergence to the average value
despite crash and omission failures.
  As the second part of our contribution, we show three applications of the
newly developed fault-tolerant local load balancing protocol. We give a
randomized consensus algorithm, working against $t < n / 3$ crash failures,
that improves over the best-known consensus solution by Hajiaghayi et al. with
respect to communication complexity, yet with an arguable simpler technique of
combining a randomly and locally selected virtual communication graph with a
deterministic fault-tolerant local load balancing on this graph.
  We also give a new solution for consensus for networks with omission
failures. Our solution works against $t < \frac{n}{C\log{n} (\log\log n)^2}$
omissions, for some constant $C$, is nearly optimal in terms of time
complexity, but most notably -- it has communication complexity $O((t^2 +
n)\text{ polylog } {n})$, matching, within a polylogarithmic factor, the lower
bound by Abraham et. al. with respect to both terms depending on $t$ and $n$.
Ours is the first algorithm in the literature that is simultaneously nearly
optimal, in terms of $n,t$, with respect to both complexity measures, against
the adaptive omission-causing adversary.

</details>


### [4] [An Analysis of HPC and Edge Architectures in the Cloud](https://arxiv.org/abs/2508.01494)
*Steven Santillan,Cristina L. Abad*

Main category: cs.DC

TL;DR: 分析396个真实AWS云架构，聚焦HPC和边缘组件，探讨其设计、服务组合、存储系统、复杂性和机器学习服务使用。


<details>
  <summary>Details</summary>
Motivation: 了解当前行业中HPC和边缘架构的设计趋势，为实践和研究提供参考。

Method: 基于AWS数据集，识别并分析包含HPC或边缘组件的架构，评估服务组合、存储系统、复杂性和机器学习服务。

Result: 揭示了HPC和边缘架构的常见设计模式、服务组合及行业实践，提供了行业趋势的见解。

Conclusion: 研究结果为构建稳健、可扩展的HPC和边缘云解决方案提供了参考，并指导未来研究方向。

Abstract: We analyze a recently published dataset of 396 real-world cloud architectures
deployed on AWS, from companies belonging to a wide range of industries. From
this dataset, we identify those architectures that contain HPC or edge
components and characterize their designs. Specifically, we investigate the
prevalence and interplay of AWS services within these architectures, examine
the types of storage systems employed, assess architectural complexity and the
use of machine learning services, discuss the implications of our findings and
how representative these results are of HPC and edge architectures in the
cloud. This characterization provides valuable insights into current industry
practices and trends in building robust and scalable HPC and edge solutions in
the cloud continuum, and can be valuable for those seeking to better understand
how these architectures are being built and to guide new research.

</details>


### [5] [Faster Distributed $Δ$-Coloring via a Reduction to MIS](https://arxiv.org/abs/2508.01762)
*Yann Bourreau,Sebastian Brandt,Alexandre Nolin*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent improvements on the deterministic complexities of fundamental graph
problems in the LOCAL model of distributed computing have yielded
state-of-the-art upper bounds of $\tilde{O}(\log^{5/3} n)$ rounds for maximal
independent set (MIS) and $(\Delta + 1)$-coloring [Ghaffari, Grunau, FOCS'24]
and $\tilde{O}(\log^{19/9} n)$ rounds for the more restrictive
$\Delta$-coloring problem [Ghaffari, Kuhn, FOCS'21; Ghaffari, Grunau, FOCS'24;
Bourreau, Brandt, Nolin, STOC'25]. In our work, we show that $\Delta$-coloring
can be solved deterministically in $\tilde{O}(\log^{5/3} n)$ rounds as well,
matching the currently best bound for $(\Delta + 1)$-coloring.
  We achieve our result by developing a reduction from $\Delta$-coloring to MIS
that guarantees that the (asymptotic) complexity of $\Delta$-coloring is at
most the complexity of MIS, unless MIS can be solved in sublogarithmic time, in
which case, due to the $\Omega(\log n)$-round $\Delta$-coloring lower bound
from [BFHKLRSU, STOC'16], our reduction implies a tight complexity of
$\Theta(\log n)$ for $\Delta$-coloring. In particular, any improvement on the
complexity of the MIS problem will yield the same improvement for the
complexity of $\Delta$-coloring (up to the true complexity of
$\Delta$-coloring).
  Our reduction yields improvements for $\Delta$-coloring in the randomized
LOCAL model and when complexities are parameterized by both $n$ and $\Delta$.
We obtain a randomized complexity bound of $\tilde{O}(\log^{5/3} \log n)$
rounds (improving over the state of the art of $\tilde{O}(\log^{8/3} \log n)$
rounds) on general graphs and tight complexities of $\Theta(\log n)$ and
$\Theta(\log \log n)$ for the deterministic, resp.\ randomized, complexity on
bounded-degree graphs. In the special case of graphs of constant clique number
(which for instance include bipartite graphs), we also give a reduction to the
$(\Delta+1)$-coloring problem.

</details>


### [6] [Efficient Byzantine Consensus MechanismBased on Reputation in IoT Blockchain](https://arxiv.org/abs/2508.01856)
*Xu Yuan,Fang Luo,Muhammad Zeeshan Haider,Zhikui Chen,Yucheng Li*

Main category: cs.DC

TL;DR: 本文提出了一种高效的拜占庭声誉共识机制（EBRC），以解决区块链在物联网中部署时的存储、计算和共识算法问题。


<details>
  <summary>Details</summary>
Motivation: 区块链在物联网中面临存储、计算能力和共识算法的挑战，现有算法存在节点可靠性低、吞吐量小和扩展性问题。

Method: 提出EBRC机制，重新设计节点可靠性和鲁棒性评估方法，优化活跃节点管理。

Result: 实验表明，EBRC具有更低的共识延迟、更高的吞吐量、更强的安全性和更低的验证成本。

Conclusion: EBRC为解决物联网+区块链+互联网法院建设问题提供了新思路。

Abstract: Blockchain technology has advanced rapidly in recent years and is now widely
used in a variety of fields. Blockchain appears to be one of the best solutions
for managing massive heterogeneous devices while achieving advanced data
security and data reputation, particularly in the field of large-scale IoT
(Internet of Things) networks. Despite the numerous advantages, there are still
challenges while deploying IoT applications on blockchain systems due to the
limited storage, power, and computing capability of IoT devices, and some of
these problems are caused by the consensus algorithm, which plays a significant
role in blockchain systems by ensuring overall system reliability and
robustness. Nonetheless, most existing consensus algorithms are prone to poor
node reliability, low transaction per second (TPS) rates, and scalability
issues. Aiming at some critical problems in the existing consensus algorithms,
this paper proposes the Efficient Byzantine Reputation-based Consensus (EBRC)
mechanism to resolve the issues raised above. In comparison to traditional
algorithms, we reinvented ways to evaluate node reliability and robustness and
manage active nodes. Our experiments show that the EBRC algorithm has lower
consensus delay, higher throughput, improved security, and lower verification
costs. It offers new reference ideas for solving the Internet of
Things+blockchain+Internet court construction problem.

</details>


### [7] [Machine Learning-Driven Performance Analysis of Compressed Communication in Aerial-RIS Networks for Future 6G Networks](https://arxiv.org/abs/2508.01911)
*Muhammad Farhan Khan,Muhammad Ahmed Mohsin,Zeeshan Alam,Muhammad Saad,Muhammad Waqar*

Main category: cs.DC

TL;DR: 论文提出了一种结合无人机辅助RIS、NOMA和CoMP的系统模型，以提高6G网络中的数据速率，并通过机器学习压缩反馈信息，优化性能。


<details>
  <summary>Details</summary>
Motivation: 解决密集城市环境中带宽耗尽和容量限制的问题，提升下一代多小区网络的数据速率。

Method: 整合无人机辅助RIS、NOMA和CoMP技术，优化部署和资源分配，并使用机器学习压缩反馈信息。

Result: 显著提高了频谱效率、降低了中断概率，并优化了带宽利用率。

Conclusion: 提出的架构在提升网络性能方面具有巨大潜力。

Abstract: In the future 6G and wireless networks, particularly in dense urban
environments, bandwidth exhaustion and limited capacity pose significant
challenges to enhancing data rates. We introduce a novel system model designed
to improve the data rate of users in next-generation multi-cell networks by
integrating Unmanned Aerial Vehicle (UAV)-Assisted Reconfigurable Intelligent
Surfaces (RIS), Non-Orthogonal Multiple Access (NOMA), and Coordinated
Multipoint Transmission (CoMP). Optimally deploying Aerial RIS for higher data
rates, employing NOMA to improve spectral efficiency, and utilizing CoMP to
mitigate inter-cell interference (ICI), we significantly enhance the overall
system capacity and sum rate. Furthermore, we address the challenge of feedback
overhead associated with Quantized Phase Shifts (QPS) from the receiver to RIS.
The feedback channel is band-limited and cannot support a large overhead of QPS
for uplink communication. To ensure seamless transmission, we propose a Machine
Learning Autoencoder technique for a compressed communication of QPS from the
receiver to RIS, while maintaining high accuracy. Additionally, we investigate
the impact of the number of Aerial RIS elements and power allocation ratio for
NOMA on the individual data rate of users. Our simulation results demonstrate
substantial improvements in spectral efficiency, outage probability, and
bandwidth utilization, highlighting the potential of the proposed architecture
to enhance network performance.

</details>


### [8] [Prefill-Decode Aggregation or Disaggregation? Unifying Both for Goodput-Optimized LLM Serving](https://arxiv.org/abs/2508.01989)
*Chao Wang,Pengfei Zuo,Zhangyu Chen,Yunkai Liang,Zhou Yu,Ming-Chang Yang*

Main category: cs.DC

TL;DR: TaiChi是一个统一的LLM服务系统，结合了预填充-解码（PD）的聚合与解聚，通过动态调整资源分配和调度机制，优化不同服务级别目标（SLOs）下的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决预填充-解码（PD）聚合与解聚在LLM服务中各自优势但无法在所有SLOs下最优的问题。

Method: 提出TaiChi系统，采用统一架构和可配置滑块动态调整资源分配，结合延迟转移和调度机制优化性能。

Result: 在平衡的TTFT和TPOT SLOs下，TaiChi比现有系统吞吐量提升高达77%。

Conclusion: TaiChi通过灵活的资源分配和调度机制，为LLM服务提供了更优的解决方案。

Abstract: An ongoing debate considers whether prefill-decode (PD) aggregation or
disaggregation is superior for serving large language models (LLMs). This has
driven optimizations for both approaches, each showing distinct advantages.
This paper compares PD aggregation and disaggregation, showing that each excels
under different service-level objectives (SLOs): aggregation is optimal for
tight time-to-first-token (TTFT) and relaxed time-per-output-token (TPOT),
while disaggregation excels for strict TPOT and relaxed TTFT. However, under
balanced TTFT and TPOT SLOs, neither approach delivers optimal goodput.
  This paper proposes TaiChi, an LLM serving system that unifies PD
disaggregation and aggregation for optimal goodput under any combination of
TTFT and TPOT SLOs. TaiChi uses a unified disaggregation-aggregation
architecture with differentiated-capability GPU instances: prefill-heavy (fast
prefill, high-interference decode) and decode-heavy (low-interference decode,
slow prefill). Three configurable sliders control the ratio between these
instances and their chunk sizes. TaiChi adapts to various SLO regimes by
adjusting sliders. When TTFT constraints are tight, TaiChi resembles a PD
aggregation configuration; when TPOT dominates, it adapts toward PD
disaggregation. Crucially, under balanced SLOs, TaiChi enables a hybrid mode
for superior goodput. The key innovation behind this hybrid mode is latency
shifting: selectively reallocating GPU resources from requests that meet SLOs
to those at risk of violation, maximizing the number of SLO-satisfied requests.
This fine-grained latency shifting is orchestrated by two scheduling
mechanisms: flowing decode scheduling to control TPOTs and length-aware prefill
scheduling to manage TTFTs, which jointly optimize request assignment. Our
experiments show TaiChi improves goodput by up to 77% over state-of-the-art
systems under balanced TTFT and TPOT SLOs.

</details>


### [9] [DySTop](https://arxiv.org/abs/2508.01996)
*Yizhou Shi,Qianpiao Ma,Yan Xu,Junlong Zhou,Ming Hu,Yunming Liao,Hongli Xu*

Main category: cs.DC

TL;DR: DySTop是一种创新的异步去中心化联邦学习机制，通过动态过时控制和拓扑结构优化，显著提高了训练效率和通信资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有的去中心化联邦学习方法依赖同步通信，在异构和动态边缘环境下效率低下，且异步方法存在模型过时和通信开销大的问题。

Method: DySTop结合动态过时控制和拓扑结构优化，每轮激活多个工人并选择邻居进行模型聚合和本地训练。

Result: DySTop在保持模型精度的同时，将完成时间减少51.8%，通信资源消耗降低57.1%。

Conclusion: DySTop通过动态优化机制有效解决了异步去中心化联邦学习中的效率和通信问题。

Abstract: Federated Learning (FL) has emerged as a potential distributed learning
paradigm that enables model training on edge devices (i.e., workers) while
preserving data privacy. However, its reliance on a centralized server leads to
limited scalability. Decentralized federated learning (DFL) eliminates the
dependency on a centralized server by enabling peer-to-peer model exchange.
Existing DFL mechanisms mainly employ synchronous communication, which may
result in training inefficiencies under heterogeneous and dynamic edge
environments. Although a few recent asynchronous DFL (ADFL) mechanisms have
been proposed to address these issues, they typically yield stale model
aggregation and frequent model transmission, leading to degraded training
performance on non-IID data and high communication overhead. To overcome these
issues, we present DySTop, an innovative mechanism that jointly optimizes
dynamic staleness control and topology construction in ADFL. In each round,
multiple workers are activated, and a subset of their neighbors is selected to
transmit models for aggregation, followed by local training. We provide a
rigorous convergence analysis for DySTop, theoretically revealing the
quantitative relationships between the convergence bound and key factors such
as maximum staleness, activating frequency, and data distribution among
workers. From the insights of the analysis, we propose a worker activation
algorithm (WAA) for staleness control and a phase-aware topology construction
algorithm (PTCA) to reduce communication overhead and handle data non-IID.
Extensive evaluations through both large-scale simulations and real-world
testbed experiments demonstrate that our DySTop reduces completion time by
51.8% and the communication resource consumption by 57.1% compared to
state-of-the-art solutions, while maintaining the same model accuracy.

</details>


### [10] [Self-assessment approach for resource management protocols in heterogeneous computational systems](https://arxiv.org/abs/2508.02202)
*Rui Eduardo Lopes,Duarte Raposo,Pedro V. Teixeira,Susana Sargento*

Main category: cs.DC

TL;DR: 提出了一种基于启发式的资源评估方法，支持动态权重和资源类型扩展，适用于异构计算系统。


<details>
  <summary>Details</summary>
Motivation: 异构计算系统中资源管理问题日益重要，现有解决方案缺乏灵活性和扩展性。

Method: 采用启发式算法，动态加权需求，计算节点容量，并支持资源类型扩展。

Result: 验证表明该方法在资源评估中表现直接，支持可扩展性和灵活性。

Conclusion: 该算法为分布式和集中式资源分配协议提供了有效的资源评估解决方案。

Abstract: With an ever growing number of heterogeneous applicational services running
on equally heterogeneous computational systems, the problem of resource
management becomes more essential. Although current solutions consider some
network and time requirements, they mostly handle a pre-defined list of
resource types by design and, consequently, fail to provide an extensible
solution to assess any other set of requirements or to switch strategies on its
resource estimation. This work proposes an heuristics-based estimation solution
to support any computational system as a self-assessment, including
considerations on dynamically weighting the requirements, how to compute each
node's capacity towards an admission request, and also offers the possibility
to extend the list of resource types considered for assessment, which is an
uncommon view in related works. This algorithm can be used by distributed and
centralized resource allocation protocols to decide the best node(s) for a
service intended for deployment. This approach was validated across its
components and the results show that its performance is straightforward in
resource estimation while allowing scalability and extensibility.

</details>


### [11] [FedAPTA: Federated Multi-task Learning in Computing Power Networks with Adaptive Layer-wise Pruning and Task-aware Aggregation](https://arxiv.org/abs/2508.02230)
*Yachao Yuan,Zhen Yu,Jin Wang,Zhipeng Cheng,Jianhua Hu*

Main category: cs.DC

TL;DR: FedAPTA是一个针对计算能力网络（CPNs）中联邦学习（FL）多任务部署问题的框架，通过模型剪枝和异构模型聚合策略显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决FL在CPNs中多任务部署时计算资源浪费的问题，现有方法主要关注单任务的计算和通信开销，忽略了多任务下异构设备的资源浪费。

Method: 设计FedAPTA框架，包括层间模型剪枝技术以减少本地模型大小，以及异构模型恢复策略和任务感知的模型聚合方法。

Result: 实验表明，FedAPTA在性能上比现有FL方法最高提升4.23%。

Conclusion: FedAPTA有效解决了FL在CPNs中多任务部署的资源浪费问题，性能显著优于现有方法。

Abstract: Federated Learning (FL) has shown considerable promise in Computing Power
Networks (CPNs) for privacy protection, efficient data utilization, and dynamic
collaboration. Although it offers practical benefits, applying FL in CPNs
continues to encounter a major obstacle, i.e., multi-task deployment. However,
existing work mainly focuses on mitigating FL's computation and communication
overhead of a single task while overlooking the computing resource wastage
issue of heterogeneous devices across multiple tasks in FL under CPNs. To
tackle this, we design FedAPTA, a federated multi-task learning framework in
CPNs. FedAPTA alleviates computing resource wastage through the developed
layer-wise model pruning technique, which reduces local model size while
considering both data and device heterogeneity. To aggregate structurally
heterogeneous local models of different tasks, we introduce a heterogeneous
model recovery strategy and a task-aware model aggregation method that enables
the aggregation through infilling local model architecture with the shared
global model and clustering local models according to their specific tasks. We
deploy FedAPTA on a realistic FL platform and benchmark it against nine SOTA FL
methods. The experimental outcomes demonstrate that the proposed FedAPTA
considerably outperforms the state-of-the-art FL methods by up to 4.23%. Our
code is available at https://github.com/Zhenzovo/FedCPN.

</details>


### [12] [PUSHtap: PIM-based In-Memory HTAP with Unified Data Storage Format](https://arxiv.org/abs/2508.02309)
*Yilong Zhao,Mingyu Gao,Huanchen Zhang,Fangxin Liu,Gongye Chen,He Xian,Haibing Guan,Li Jiang*

Main category: cs.DC

TL;DR: PUSHtap结合CPU和PIM单元的双维访问，解决了HTAP中的数据格式矛盾，实现了性能隔离、数据新鲜度和工作负载优化。


<details>
  <summary>Details</summary>
Motivation: HTAP系统中OLTP和OLAP工作负载的数据格式矛盾阻碍了性能隔离、数据新鲜度和工作负载优化的同时实现。

Method: 提出统一数据存储格式，优化CPU和PIM单元的带宽利用，扩展PIM架构支持OLAP操作和并发访问。

Result: PUSHtap在OLAP/OLTP吞吐量上比多实例PIM设计提高了3.4倍/4.4倍。

Conclusion: PUSHtap通过结合CPU和PIM的双维访问，有效解决了HTAP中的数据格式矛盾，显著提升了性能。

Abstract: Hybrid transaction/analytical processing (HTAP) is an emerging database
paradigm that supports both online transaction processing (OLTP) and online
analytical processing (OLAP) workloads. Computing-intensive OLTP operations,
involving row-wise data manipulation, are suitable for row-store format. In
contrast, memory-intensive OLAP operations, which are column-centric, benefit
from column-store format. This \emph{data-format dilemma} prevents HTAP systems
from concurrently achieving three design goals: performance isolation, data
freshness, and workload-specific optimization. Another background technology is
Processing-in-Memory (PIM), which integrates computing units (PIM units) inside
DRAM memory devices to accelerate memory-intensive workloads, including OLAP.
  Our key insight is to combine the interleaved CPU access and localized PIM
unit access to provide two-dimensional access to address the data format
contradictions inherent in HTAP. First, we propose a unified data storage
format with novel data alignment and placement techniques to optimize the
effective bandwidth of CPUs and PIM units and exploit the PIM's parallelism.
Second, we implement the multi-version concurrency control (MVCC) essential for
single-instance HTAP. Third, we extend the commercial PIM architecture to
support the OLAP operations and concurrent access from PIM and CPU. Experiments
show that PUSHtap can achieve 3.4\texttimes{}/4.4\texttimes{} OLAP/OLTP
throughput improvement compared to multi-instance PIM-based design.

</details>


### [13] [TeraNoC: A Multi-Channel 32-bit Fine-Grained, Hybrid Mesh-Crossbar NoC for Efficient Scale-up of 1000+ Core Shared-L1-Memory Clusters](https://arxiv.org/abs/2508.02446)
*Yichao Zhang,Zexin Fu,Tim Fischer,Yinrong Li,Marco Bertuletti,Luca Benini*

Main category: cs.DC

TL;DR: TeraNoC是一种混合网状-交叉开关片上互连，结合了低延迟和高可扩展性，解决了多核共享内存集群的互连瓶颈。


<details>
  <summary>Details</summary>
Motivation: 解决片上互连设计中带宽扩展与低延迟、高面积效率之间的矛盾，避免因互连瓶颈导致的多核集群规模受限。

Method: 采用32位字宽多通道2D网状和交叉开关混合拓扑，设计路由器重映射器以平衡流量负载。

Result: 在12nm工艺下实现1024核集群，IPC达0.85，功耗占比低（7.6%-22.7%），面积效率提升98.7%。

Conclusion: TeraNoC在低延迟、高可扩展性和低面积开销方面表现优异，适用于大规模共享内存集群设计。

Abstract: A key challenge in on-chip interconnect design is to scale up bandwidth while
maintaining low latency and high area efficiency. 2D-meshes scale with low
wiring area and congestion overhead; however, their end-to-end latency
increases with the number of hops, making them unsuitable for latency-sensitive
core-to-L1-memory access. On the other hand, crossbars offer low latency, but
their routing complexity grows quadratically with the number of I/Os, requiring
large physical routing resources and limiting area-efficient scalability. This
two-sided interconnect bottleneck hinders the scale-up of many-core,
low-latency, tightly coupled shared-memory clusters, pushing designers toward
instantiating many smaller and loosely coupled clusters, at the cost of
hardware and software overheads. We present TeraNoC, an open-source, hybrid
mesh-crossbar on-chip interconnect that offers both scalability and low
latency, while maintaining very low routing overhead. The topology, built on
32bit word-width multi-channel 2D-meshes and crossbars, enables the
area-efficient scale-up of shared-memory clusters. A router remapper is
designed to balance traffic load across interconnect channels. Using TeraNoC,
we build a cluster with 1024 single-stage, single-issue cores that share a
4096-banked L1 memory, implemented in 12nm technology. The low interconnect
stalls enable high compute utilization of up to 0.85 IPC in compute-intensive,
data-parallel key GenAI kernels. TeraNoC only consumes 7.6\% of the total
cluster power in kernels dominated by crossbar accesses, and 22.7\% in kernels
with high 2D-mesh traffic. Compared to a hierarchical crossbar-only cluster,
TeraNoC reduces die area by 37.8\% and improves area efficiency (GFLOP/s/mm2)
by up to 98.7\%, while occupying only 10.9\% of the logic area.

</details>


### [14] [xDeepServe: Model-as-a-Service on Huawei CloudMatrix384](https://arxiv.org/abs/2508.02520)
*Ao Xiao,Bangzheng He,Baoquan Zhang,Baoxing Huai,Bingji Wang,Bo Wang,Bo Xu,Boyi Hou,Chan Yang,Changhong Liu,Cheng Cui,Chenyu Zhu,Cong Feng,Daohui Wang,Dayun Lin,Duo Zhao,Fengshao Zou,Fu Wang,Gangqiang Zhang,Gengyuan Dan,Guanjie Chen,Guodong Guan,Guodong Yang,Haifeng Li,Haipei Zhu,Hao Feng,Hao Huang,Hao Xu,Hengrui Ma,Hengtao Fan,Hui Liu,Jia Li,Jiang Liu,Jiang Xu,Jie Meng,Jinhan Xin,Junhao Hu,Juwei Chen,Lan Yu,Lanxin Miao,Liang Liu,Linan Jing,Lu Zhou,Meina Han,Mingkun Deng,Mingyu Deng,Naitian Deng,Nizhong Lin,Peihan Zhao,Peng Pan,Pengfei Shen,Ping Li,Qi Zhang,Qin Zhang,Qingrong Xia,Qingyi Zhang,Qunchao Fu,Ren Guo,Ruimin Gao,Shaochun Li,Sheng Long,Shentian Li,Shining Wan,Shuai Shen,Shuangfu Zeng,Shuming Jing,Siqi Yang,Song Zhang,Tao Xu,Tianlin Du,Ting Chen,Wanxu Wu,Wei Jiang,Weinan Tong,Weiwei Chen,Wen Peng,Wenli Zhou,Wenquan Yang,Wenxin Liang,Xiang Liu,Xiaoli Zhou,Xin Jin,Xinyu Duan,Xu Li,Xu Zhang,Xusheng Chen,Yalong Shan,Yang Gan,Yao Lu,Yi Deng,Yi Zheng,Yingfei Zheng,Yiyun Zheng,Yizhou Shan,Yong Gao,Yongqiang Yang,Yuanjin Gong,Yue Yu,Yuetao Chen,Yukun Zhu,Yulong He,Yusu Zhao,Yuyan Wu,Zenan Zhang,Zhaojin Zhuo,Zhaoyang Ji,Zhefeng Wang,Zheng Wang,Zhenhua Yang,Zhenli Sheng,Zhibin Yu,Zhigang Ji,Zhihao Ren,Zhipeng Bian,Zhixia Liu,Zhiyu Dong,Zhonghua Li,Zhou Yu,Zhuoming Shen,Zhuwei Peng,Zi Ye,Zihao Xiang,Zimin Fu,Zixuan Zhang*

Main category: cs.DC

TL;DR: 论文介绍了xDeepServe，华为云的LLM服务系统，专为SuperPod规模的基础设施设计，解决了大规模MoE模型运行中的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLM通过MoE扩展和AI硬件（如SuperPod）的升级，运行大规模MoE模型带来了新的挑战，需要新的执行模型和高效调度。

Method: 提出了Transformerless架构，将Transformer模型分解为模块化单元，并设计了XCCL通信库和FlowServe服务引擎。

Result: 实现了计算和内存的独立扩展，同时保持性能，支持跨数百个NPU的可扩展推理。

Conclusion: xDeepServe为SuperPod规模的基础设施提供了高效的LLM服务解决方案。

Abstract: The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in
large-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in
recent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is
scaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s
high-speed interconnects. Running large MoE models on SuperPod-scale hardware
brings new challenges. It requires new execution models, scalable scheduling,
efficient expert load balancing, and elimination of single points of failure.
This paper presents xDeepServe, Huawei Cloud's LLM serving system designed for
SuperPod-scale infrastructure. At its core is Transformerless, a disaggregated
architecture that decomposes transformer models into modular units--attention,
feedforward, and MoE--executed independently on NPUs connected via high-speed
fabric. We implement this design in two forms: disaggregated prefill-decode and
disaggregated MoE-attention. This fully disaggregated setup enables independent
scaling of compute and memory without sacrificing performance. To support this
architecture, we propose XCCL, a communication library that leverages
CloudMatrix384's global shared memory to implement efficient point-to-point and
all-to-all primitives. We also extend our serving engine FlowServe with
system-level techniques, enabling scalable inference across hundreds of NPUs.

</details>


### [15] [Blockchain Epidemic Consensus for Large-Scale Networks](https://arxiv.org/abs/2508.02552)
*Siamak Abdi,Giuseppe Di Fatta,Atta Badii,Giancarlo Fortino*

Main category: cs.DC

TL;DR: BECP是一种新型的去中心化区块链共识协议，基于流行病传播原理，解决了现有协议在可扩展性、资源消耗和容错性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有区块链共识协议在可扩展性、资源消耗和容错性方面存在缺陷，需要一种更高效的解决方案。

Method: BECP采用流行病传播原理，无固定角色（如验证者或领导者），实现概率收敛、高效消息传播和延迟容忍。

Result: 实验表明，BECP在吞吐量、共识延迟和消息传递效率上优于经典协议（如PAXOS、RAFT、PBFT）和其他流行病协议（如Avalanche、Snowman）。

Conclusion: BECP是一种高效、可扩展的下一代区块链共识协议。

Abstract: Blockchain is a distributed ledger technology that has applications in many
domains such as cryptocurrency, smart contracts, supply chain management, and
many others. Distributed consensus is a fundamental component of blockchain
systems that enables secure, precise, and tamper-proof verification of data
without relying on central authorities. Existing consensus protocols,
nevertheless, suffer from drawbacks, some of which are related to scalability,
resource consumption, and fault tolerance. We introduce Blockchain Epidemic
Consensus Protocol (BECP), a novel fully decentralised consensus protocol for
blockchain networks at a large scale. BECP follows epidemic communication
principles, without fixed roles like validators or leaders, and achieves
probabilistic convergence, efficient message dissemination, and tolerance to
message delays. We provide an extensive experimental comparison of BECP against
classic protocols like PAXOS, RAFT, and PBFT, and newer epidemic-based
protocols like Avalanche and Snowman. The findings indicate that BECP provides
desirable gains in throughput, consensus latency, and substantial
message-passing efficiency compared to existing epidemic-based approaches,
validating its usability as an effective and scalable approach for
next-generation blockchain systems.

</details>


### [16] [Fully Decentralised Consensus for Extreme-scale Blockchain](https://arxiv.org/abs/2508.02595)
*Siamak Abdi,Giuseppe Di Fatta,Atta Badii,Giancarlo Fortino*

Main category: cs.DC

TL;DR: 论文提出了一种基于流行病协议的全去中心化区块链共识协议BECP，适用于超大规模系统，在吞吐量、扩展性和共识延迟方面优于传统协议。


<details>
  <summary>Details</summary>
Motivation: 传统共识算法存在节点故障、高资源消耗等问题，需要一种更高效、去中心化的解决方案。

Method: 利用流行病协议的优势，提出BECP协议，不依赖固定验证者集，具有概率收敛性和网络资源高效利用。

Result: BECP在吞吐量、共识延迟和消息数量上优于PAXOS、RAFT、PBFT和Avalanche协议。

Conclusion: BECP证明了基于流行病协议的全去中心化共识在区块链技术中的高效性和有效性。

Abstract: Blockchain is a decentralised, immutable ledger technology that has been
widely adopted in many sectors for various applications such as
cryptocurrencies, smart contracts and supply chain management. Distributed
consensus is a fundamental component of blockchain, which is required to ensure
trust, security, and integrity of the data stored and the transactions
processed in the blockchain. Various consensus algorithms have been developed,
each affected from certain issues such as node failures, high resource
consumption, collusion, etc. This work introduces a fully decentralised
consensus protocol, Blockchain Epidemic Consensus Protocol (BECP), suitable for
very large and extreme-scale blockchain systems. The proposed approach
leverages the benefits of epidemic protocols, such as no reliance on a fixed
set of validators or leaders, probabilistic guarantees of convergence,
efficient use of network resources, and tolerance to node and network failures.
A comparative experimental analysis has been carried out with traditional
protocols including PAXOS, RAFT, and Practical Byzantine Fault Tolerance
(PBFT), as well as a relatively more recent protocol such as Avalanche, which
is specifically designed for very large-scale systems. The results illustrate
how BECP outperforms them in terms of throughput, scalability and consensus
latency. BECP achieves an average of 1.196 times higher throughput in terms of
consensus on items and 4.775 times better average consensus latency.
Furthermore, BECP significantly reduces the number of messages compared to
Avalanche. These results demonstrate the effectiveness and efficiency of fully
decentralised consensus for blockchain technology based on epidemic protocols.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [A Dynamic Allocation Scheme for Adaptive Shared-Memory Mapping on Kilo-core RV Clusters for Attention-Based Model Deployment](https://arxiv.org/abs/2508.01180)
*Bowen Wang,Marco Bertuletti,Yichao Zhang,Victor J. B. Jung,Luca Benini*

Main category: cs.AR

TL;DR: 论文提出了一种动态分配方案（DAS），通过运行时可编程地址重映射硬件单元和统一内存分配器，优化大规模并行机器学习工作负载的数据局部性，显著提升处理单元（PE）的利用率。


<details>
  <summary>Details</summary>
Motivation: 注意力模型需要灵活硬件以应对不同算术强度和内存访问模式，而传统共享L1内存的大型集群在扩展时因PE间互联吞吐下降导致PE利用率不足。

Method: 提出DAS方案，结合地址重映射硬件和统一内存分配器，减少PE对多bank L1内存的访问竞争。在1024-PE RISC-V集群上验证。

Result: 在ViT-L/16模型中，每层编码器执行时间为5.67 ms，比基线提升1.94倍速度，PE利用率为0.81。DAS面积开销<0.1%。

Conclusion: DAS能有效提升大规模机器学习工作负载的性能和PE利用率，硬件开销极小。

Abstract: Attention-based models demand flexible hardware to manage diverse kernels
with varying arithmetic intensities and memory access patterns. Large clusters
with shared L1 memory, a common architectural pattern, struggle to fully
utilize their processing elements (PEs) when scaled up due to reduced
throughput in the hierarchical PE-to-L1 intra-cluster interconnect. This paper
presents Dynamic Allocation Scheme (DAS), a runtime programmable address
remapping hardware unit coupled with a unified memory allocator, designed to
minimize data access contention of PEs onto the multi-banked L1. We evaluated
DAS on an aggressively scaled-up 1024-PE RISC-V cluster with Non-Uniform Memory
Access (NUMA) PE-to-L1 interconnect to demonstrate its potential for improving
data locality in large parallel machine learning workloads. For a Vision
Transformer (ViT)-L/16 model, each encoder layer executes in 5.67 ms, achieving
a 1.94x speedup over the fixed word-level interleaved baseline with 0.81 PE
utilization. Implemented in 12nm FinFET technology, DAS incurs <0.1 % area
overhead.

</details>


### [18] [Silent Data Corruption by 10x Test Escapes Threatens Reliable Computing](https://arxiv.org/abs/2508.01786)
*Subhasish Mitra,Subho Banerjee,Martin Dixon,Rama Govindaraju,Peter Hochschild,Eric X. Liu,Bharath Parthasarathy,Parthasarathy Ranganathan*

Main category: cs.AR

TL;DR: 论文提出了一种三管齐下的方法来解决计算芯片制造测试中的缺陷逃逸问题，包括快速诊断、现场检测和新测试实验。


<details>
  <summary>Details</summary>
Motivation: 现有制造测试中缺陷芯片逃逸数量远超工业目标，导致静默数据损坏（SDC），威胁计算可靠性。

Method: 采用快速诊断、现场检测和新测试实验三种方法，以改进缺陷芯片检测。

Result: 未明确具体实验结果，但提出了未来研究方向。

Conclusion: 通过多角度方法改进测试技术，有望减少缺陷芯片逃逸，提升计算可靠性。

Abstract: Too many defective compute chips are escaping existing manufacturing tests --
at least an order of magnitude more than industrial targets across all compute
chip types in data centers. Silent data corruptions (SDCs) caused by test
escapes, when left unaddressed, pose a major threat to reliable computing. We
present a three-pronged approach to future directions in overcoming test
escapes: (a) Quick diagnosis of defective chips directly from system-level
incorrect behaviors. Such diagnosis is critical for gaining insights into why
so many defective chips escape existing manufacturing testing. (b) In-field
detection of defective chips. (c) New test experiments to understand the
effectiveness of new techniques for detecting defective chips. These
experiments must overcome the drawbacks and pitfalls of previous industrial
test experiments and case studies.

</details>


### [19] [MARVEL: An End-to-End Framework for Generating Model-Class Aware Custom RISC-V Extensions for Lightweight AI](https://arxiv.org/abs/2508.01800)
*Ajay Kumar M,Cian O'Mahoney,Pedro Kreutz Werle,Shreejith Shanker,Dimitrios S. Nikolopoulos,Bo Ji,Hans Vandierendonck,Deepu John*

Main category: cs.AR

TL;DR: MARVEL是一个自动化框架，为特定DNN模型类生成定制的RISC-V ISA扩展，适用于资源受限的IoT设备，无需操作系统支持。


<details>
  <summary>Details</summary>
Motivation: 现有工具如AMD的FINN无法解决无操作系统的IoT设备面临的极端资源限制问题。

Method: MARVEL通过分析Python中的DNN表示，生成定制的RISC-V核心及编译器工具，结合Apache TVM、Synopsys ASIP Designer和Xilinx Vivado实现高效部署。

Result: 在AMD Zynq UltraScale+平台上，MARVEL实现了推理速度提升2倍，能耗降低2倍，面积开销为28.23%。

Conclusion: MARVEL为资源受限环境提供了一种无需操作系统的高效DNN部署解决方案。

Abstract: Deploying deep neural networks (DNNs) on resource-constrained IoT devices
remains a challenging problem, often requiring hardware modifications tailored
to individual AI models. Existing accelerator-generation tools, such as AMD's
FINN, do not adequately address extreme resource limitations faced by IoT
endpoints operating in bare-metal environments without an operating system
(OS). To overcome these constraints, we propose MARVEL-an automated, end-to-end
framework that generates custom RISC-V ISA extensions tailored to specific DNN
model classes, with a primary focus on convolutional neural networks (CNNs).
The proposed method profiles high-level DNN representations in Python and
generates an ISA-extended RISC-V core with associated compiler tools for
efficient deployment. The flow leverages (1) Apache TVM for translating
high-level Python-based DNN models into optimized C code, (2) Synopsys ASIP
Designer for identifying compute-intensive kernels, modeling, and generating a
custom RISC-V and (3) Xilinx Vivado for FPGA implementation. Beyond a model
class specific RISC-V, our approach produces an optimized bare-metal C
implementation, eliminating the need for an OS or extensive software
dependencies. Unlike conventional deployment pipelines relying on
TensorFlow/PyTorch runtimes, our solution enables seamless execution in highly
resource-constrained environments. We evaluated the flow on popular DNN models
such as LeNet-5*, MobileNetV1, ResNet50, VGG16, MobileNetV2 and DenseNet121
using the Synopsys trv32p3 RISC-V core as a baseline. Results show a 2x speedup
in inference and upto 2x reduction in energy per inference at a 28.23% area
overhead when implemented on an AMD Zynq UltraScale+ ZCU104 FPGA platform.

</details>


### [20] [Revelator: Rapid Data Fetching via OS-Driven Hash-based Speculative Address Translation](https://arxiv.org/abs/2508.02007)
*Konstantinos Kanellopoulos,Konstantinos Sgouras,Andreas Kosmas Kakolyris,Vlad-Petru Nitu,Berkin Kerim Konar,Rahul Bera,Onur Mutlu*

Main category: cs.AR

TL;DR: Revelator是一种硬件-操作系统协作方案，通过预测性地址翻译提升性能，减少延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 现代计算系统中地址翻译是性能瓶颈，传统方法依赖大页或连续映射，效果有限且硬件改动成本高。

Method: 采用分层哈希分配策略创建可预测的虚拟地址到物理地址映射，结合轻量级推测引擎生成候选物理地址。

Result: 在原生和虚拟化环境中平均提速27%（20%），超越现有技术5%，能耗降低9%。

Conclusion: Revelator通过最小化硬件改动和操作系统协作，实现了高效且低成本的地址翻译优化。

Abstract: Address translation is a major performance bottleneck in modern computing
systems. Speculative address translation can hide this latency by predicting
the physical address (PA) of requested data early in the pipeline. However,
predicting the PA from the virtual address (VA) is difficult due to the
unpredictability of VA-to-PA mappings in conventional OSes. Prior works try to
overcome this but face two key issues: (i) reliance on large pages or VA-to-PA
contiguity, which is not guaranteed, and (ii) costly hardware changes to store
speculation metadata with limited effectiveness.
  We introduce Revelator, a hardware-OS cooperative scheme enabling highly
accurate speculative address translation with minimal modifications. Revelator
employs a tiered hash-based allocation strategy in the OS to create predictable
VA-to-PA mappings, falling back to conventional allocation when needed. On a
TLB miss, a lightweight speculation engine, guided by this policy, generates
candidate PAs for both program data and last-level page table entries (PTEs).
Thus, Revelator (i) speculatively fetches requested data before translation
resolves, reducing access latency, and (ii) fetches the fourth-level PTE before
the third-level PTE is accessed, accelerating page table walks.
  We prototype Revelator's OS support in Linux and evaluate it in simulation
across 11 diverse, data-intensive benchmarks in native and virtualized
environments. Revelator achieves average speedups of 27% (20%) in native
(virtualized) settings, surpasses a state-of-the-art speculative mechanism by
5%, and reduces energy use by 9% compared to baseline. Our RTL prototype shows
minimal area and power overheads on a modern CPU.

</details>


### [21] [GSIM: Accelerating RTL Simulation for Large-Scale Designs](https://arxiv.org/abs/2508.02236)
*Lu Chen,Dingyi Zhao,Zihao Yu,Ninghui Sun,Yungang Bao*

Main category: cs.AR

TL;DR: 本文探讨了RTL仿真的计算开销来源，提出了多层次的优化技术，并开发了新型仿真器GSIM，显著提升了仿真速度。


<details>
  <summary>Details</summary>
Motivation: RTL仿真在硬件设计中广泛应用，但复杂设计的仿真速度慢成为瓶颈。

Method: 分析仿真开销的四个因素，提出超节点、节点和比特级别的优化技术。

Result: GSIM仿真器成功应用于XiangShan处理器，速度提升显著（最高19.94倍）。

Conclusion: 通过多层次优化，GSIM显著提升了RTL仿真效率。

Abstract: Register Transfer Level (RTL) simulation is widely used in design space
exploration, verification, debugging, and preliminary performance evaluation
for hardware design. Among various RTL simulation approaches, software
simulation is the most commonly used due to its flexibility, low cost, and ease
of debugging. However, the slow simulation of complex designs has become the
bottleneck in design flow. In this work, we explore the sources of computation
overhead of RTL simulation and conclude them into four factors. To optimize
these factors, we propose several techniques at the supernode level, node
level, and bit level. Finally, we implement these techniques in a novel RTL
simulator GSIM. GSIM succeeds in simulating XiangShan, the state-of-the-art
open-source RISC-V processor. Besides, compared to Verilator, GSIM can achieve
speedup of 7.34x for booting Linux on XiangShan, and 19.94x for running
CoreMark on Rocket.

</details>


### [22] [ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant Neural Rendering](https://arxiv.org/abs/2508.02304)
*Fangxin Liu,Haomin Li,Bowen Zhu,Zongwu Wang,Zhuoran Song,Habing Guan,Li Jiang*

Main category: cs.AR

TL;DR: ASDR是一种基于CIM的加速器，通过算法-架构协同设计优化神经渲染，显著提升速度和能效。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染模型在实时性和能效方面表现不足，CIM技术有望解决这些问题。

Method: 提出动态采样和MLP优化算法，并设计高效的ReRAM CIM架构。

Result: 实验显示ASDR在渲染任务中比现有加速器和GPU快9.55倍和69.75倍，仅损失0.1 PSNR。

Conclusion: ASDR通过协同设计显著提升神经渲染的效率和实用性。

Abstract: Neural Radiance Fields (NeRF) offer significant promise for generating
photorealistic images and videos. However, existing mainstream neural rendering
models often fall short in meeting the demands for immediacy and power
efficiency in practical applications. Specifically, these models frequently
exhibit irregular access patterns and substantial computational overhead,
leading to undesirable inference latency and high power consumption.
Computing-in-memory (CIM), an emerging computational paradigm, has the
potential to address these access bottlenecks and reduce the power consumption
associated with model execution.
  To bridge the gap between model performance and real-world scene
requirements, we propose an algorithm-architecture co-design approach,
abbreviated as ASDR, a CIM-based accelerator supporting efficient neural
rendering. At the algorithmic level, we propose two rendering optimization
schemes: (1) Dynamic sampling by online sensing of the rendering difficulty of
different pixels, thus reducing access memory and computational overhead. (2)
Reducing MLP overhead by decoupling and approximating the volume rendering of
color and density. At the architecture level, we design an efficient
ReRAM-based CIM architecture with efficient data mapping and reuse
microarchitecture. Experiments demonstrate that our design can achieve up to
$9.55\times$ and $69.75\times$ speedup over state-of-the-art NeRF accelerators
and Xavier NX GPU in graphics rendering tasks with only $0.1$ PSNR loss.

</details>


### [23] [ReGate: Enabling Power Gating in Neural Processing Units](https://arxiv.org/abs/2508.02536)
*Yuqi Xue,Jian Huang*

Main category: cs.AR

TL;DR: ReGate是一种通过硬件/软件协同设计实现NPU芯片细粒度电源门控的技术，显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 现代NPU芯片缺乏电源管理支持，导致30%-72%的能耗来自静态功耗，亟需解决方案。

Method: ReGate针对NPU不同组件（如脉动阵列、互连控制器等）设计定制化电源门控方案，结合硬件和软件管理。

Result: ReGate平均降低能耗15.5%（最高32.8%），对性能影响可忽略，硬件开销低于3.3%。

Conclusion: ReGate为NPU芯片提供高效电源管理方案，显著提升能效，适用于可持续数据中心。

Abstract: The energy efficiency of neural processing units (NPU) is playing a critical
role in developing sustainable data centers. Our study with different
generations of NPU chips reveals that 30%-72% of their energy consumption is
contributed by static power dissipation, due to the lack of power management
support in modern NPU chips. In this paper, we present ReGate, which enables
fine-grained power-gating of each hardware component in NPU chips with
hardware/software co-design. Unlike conventional power-gating techniques for
generic processors, enabling power-gating in NPUs faces unique challenges due
to the fundamental difference in hardware architecture and program execution
model. To address these challenges, we carefully investigate the power-gating
opportunities in each component of NPU chips and decide the best-fit power
management scheme (i.e., hardware- vs. software-managed power gating).
Specifically, for systolic arrays (SAs) that have deterministic execution
patterns, ReGate enables cycle-level power gating at the granularity of
processing elements (PEs) following the inherent dataflow execution in SAs. For
inter-chip interconnect (ICI) and HBM controllers that have long idle
intervals, ReGate employs a lightweight hardware-based idle-detection
mechanism. For vector units and SRAM whose idle periods vary significantly
depending on workload patterns, ReGate extends the NPU ISA and allows software
like compilers to manage the power gating. With implementation on a
production-level NPU simulator, we show that ReGate can reduce the energy
consumption of NPU chips by up to 32.8% (15.5% on average), with negligible
impact on AI workload performance. The hardware implementation of power-gating
logic introduces less than 3.3% overhead in NPU chips.

</details>
