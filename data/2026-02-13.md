<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Compiler-Guided Inference-Time Adaptation: Improving GPT-5 Programming Performance in Idris](https://arxiv.org/abs/2602.11481)
*Minda Li,Bhaskar Krishnamachari*

Main category: cs.PL

TL;DR: GPT-5在低资源函数式编程语言Idris中表现不佳，但通过基于编译器错误的迭代提示，性能大幅提升至近乎完美


<details>
  <summary>Details</summary>
Motivation: 研究GPT-5在低资源或不常用编程语言中的能力，特别是函数式编程语言Idris，探索如何通过反馈驱动的方法提升其性能

Method: 使用Exercism平台的Idris练习，评估多种精炼策略：基于平台反馈的迭代提示、添加文档和错误分类指南、使用本地编译错误和失败测试用例的迭代提示

Result: 零样本提示仅解决22/56个问题，远低于Python（45/50）和Erlang（35/47）。使用本地编译错误的方法效果最好，将性能提升至54/56个问题

Conclusion: 虽然大语言模型在低资源环境中初始表现不佳，但结构化的编译器级别反馈可以显著解锁其能力，特别是在不熟悉的编程语言中

Abstract: GPT-5, a state of the art large language model from OpenAI, demonstrates strong performance in widely used programming languages such as Python, C++, and Java; however, its ability to operate in low resource or less commonly used languages remains underexplored. This work investigates whether GPT-5 can effectively acquire proficiency in an unfamiliar functional programming language, Idris, through iterative, feedback driven prompting. We first establish a baseline showing that with zero shot prompting the model solves only 22 out of 56 Idris exercises using the platform Exercism, substantially underperforming relative to higher resource languages (45 out of 50 in Python and 35 out of 47 in Erlang). We then evaluate several refinement strategies, including iterative prompting based on platform feedback, augmenting prompts with documentation and error classification guides, and iterative prompting using local compilation errors and failed test cases. Among these approaches, incorporating local compilation errors yields the most substantial improvements. Using this structured, error guided refinement loop, GPT-5 performance increased to an impressive 54 solved problems out of 56. These results suggest that while large language models may initially struggle in low resource settings, structured compiler level feedback can play a critical role in unlocking their capabilities.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [LAER-MoE: Load-Adaptive Expert Re-layout for Efficient Mixture-of-Experts Training](https://arxiv.org/abs/2602.11686)
*Xinyi Liu,Yujie Wang,Fangcheng Fu,Xuefeng Xiao,Huixia Li,Jiashi Li,Bin Cui*

Main category: cs.DC

TL;DR: LAER-MoE提出了一种高效的MoE训练框架，通过完全分片专家并行（FSEP）和负载均衡规划器解决专家并行训练中的负载不均衡问题，实现最高1.69倍加速。


<details>
  <summary>Details</summary>
Motivation: 专家并行训练中，动态路由导致专家间负载严重不均衡，少数过载专家成为训练瓶颈，需要解决这一性能问题。

Method: 提出完全分片专家并行（FSEP）范式，将每个专家参数完全分片到所有设备，通过All-to-All通信在训练期间按专家粒度恢复部分专家参数，实现参数灵活重布局；采用细粒度通信调度最小化开销；开发负载均衡规划器制定专家重布局策略和令牌路由方案。

Result: 在A100集群上的实验表明，相比当前最先进的训练系统，LAER-MoE实现了最高1.69倍的加速。

Conclusion: LAER-MoE通过创新的完全分片专家并行范式和负载均衡规划，有效解决了MoE训练中的负载不均衡问题，显著提升了训练效率。

Abstract: Expert parallelism is vital for effectively training Mixture-of-Experts (MoE) models, enabling different devices to host distinct experts, with each device processing different input data. However, during expert parallel training, dynamic routing results in significant load imbalance among experts: a handful of overloaded experts hinder overall iteration, emerging as a training bottleneck.
  In this paper, we introduce LAER-MoE, an efficient MoE training framework. The core of LAER-MoE is a novel parallel paradigm, Fully Sharded Expert Parallel (FSEP), which fully partitions each expert parameter by the number of devices and restores partial experts at expert granularity through All-to-All communication during training. This allows for flexible re-layout of expert parameters during training to enhance load balancing. In particular, we perform fine-grained scheduling of communication operations to minimize communication overhead. Additionally, we develop a load balancing planner to formulate re-layout strategies of experts and routing schemes for tokens during training. We perform experiments on an A100 cluster, and the results indicate that our system achieves up to 1.69x acceleration compared to the current state-of-the-art training systems. Source code available at https://github.com/PKU-DAIR/Hetu-Galvatron/tree/laer-moe.

</details>


### [3] [Designing Scalable Rate Limiting Systems: Algorithms, Architecture, and Distributed Solutions](https://arxiv.org/abs/2602.11741)
*Bo Guan*

Main category: cs.DC

TL;DR: 本文提出了一种基于Redis Sorted Set的分布式限流系统架构，通过Rolling Window算法在准确性、可用性和可扩展性之间取得平衡，采用三层架构和Lua脚本原子操作确保并发安全。


<details>
  <summary>Details</summary>
Motivation: 设计一个同时具备准确性、可用性和可扩展性的分布式限流系统面临根本性挑战，主要源于算法精度、可用性、一致性和分区容忍性之间的权衡。需要在生产环境中构建一个实用的分布式限流解决方案。

Method: 采用Redis内存缓存数据库及其Sorted Set数据结构（O(log(N))时间复杂度），实现Rolling Window限流算法。使用服务器端Lua脚本将清理、计数和插入操作捆绑为原子操作，消除竞争条件。提出三层架构管理限流规则存储和更新，通过规则参数哈希加载脚本实现规则动态变更。在Redis Cluster上部署，通过数据分片和复制提供可用性和可扩展性。

Result: 量化了Rolling Window算法与Token Bucket和Fixed Window算法在准确性和内存成本之间的权衡。通过接受CAP定理中的AP（可用性和分区容忍性）作为实用工程权衡，实现了高可用和可扩展的分布式限流系统。

Conclusion: 基于Redis Sorted Set的分布式限流架构能够在生产环境中有效平衡准确性、可用性和可扩展性需求。通过Rolling Window算法、Lua脚本原子操作和Redis Cluster部署，实现了高性能、低延迟且可扩展的限流解决方案，为分布式系统提供了实用的限流实现方案。

Abstract: Designing a rate limiter that is simultaneously accurate, available, and scalable presents a fundamental challenge in distributed systems, primarily due to the trade-offs between algorithmic precision, availability, consistency, and partition tolerance. This article presents a concrete architecture for a distributed rate limiting system in a production-grade environment. Our design chooses the in-memory cache database, the Redis, along with its Sorted Set data structure, which provides $O(log (N))$ time complexity operation for the key-value pair dataset with efficiency and low latency, and maintains precision. The core contribution is quantifying the accuracy and memory cost trade-off of the chosen Rolling Window as the implemented rate limiting algorithm against the Token Bucket and Fixed Window algorithms. In addition, we explain how server-side Lua scripting is critical to bundling cleanup, counting, and insertion into a single atomic operation, thereby eliminating race conditions in concurrent environments. In the system architecture, we propose a three-layer architecture that manages the storage and updating of the limit rules. Through script load by hashing the rule parameters, rules can be changed without modifying the cached scripts. Furthermore, we analyze the deployment of this architecture on a Redis Cluster, which provides the availability and scalability by data sharding and replication. We explain the acceptance of AP (Availability and Partition Tolerance) from the CAP theorem as the pragmatic engineering trade-off for this use case.

</details>


### [4] [Contention Resolution, With and Without a Global Clock](https://arxiv.org/abs/2602.12070)
*Zixi Cai,Kuowen Chen,Shengquan Du,Tsvi Kopelowitz,Seth Pettie,Ben Plosk*

Main category: cs.DC

TL;DR: 论文研究了具有全局时钟的竞争解决协议，设计了新的协议获得更低的延迟，发现了期望延迟与高概率延迟之间的复杂度差距，并证明两者无法同时最优。


<details>
  <summary>Details</summary>
Motivation: 竞争解决问题传统上假设没有全局时钟，但全局时钟在技术上可行且算法上有趣，能丰富问题并引入新技术。作者希望探索全局时钟下的竞争解决协议性能。

Method: 设计了新的竞争解决协议，分析随机化协议在期望延迟和高概率延迟下的性能差异，通过理论证明建立复杂度界限。

Result: 1) 新协议延迟为O(n(log log n)^{1+o(1)})；2) 发现期望延迟与高概率延迟存在log n因子差距；3) 证明无法同时优化两种延迟指标。

Conclusion: 全局时钟显著改变竞争解决问题的复杂度格局，揭示了期望延迟与高概率延迟之间的基本权衡，表明无法设计同时最优的协议。

Abstract: In the Contention Resolution problem $n$ parties each wish to have exclusive use of a shared resource for one unit of time. The problem has been studied since the early 1970s, under a variety of assumptions on feedback given to the parties, how the parties wake up, knowledge of $n$, and so on. The most consistent assumption is that parties do not have access to a global clock, only their local time since wake-up. This is surprising because the assumption of a global clock is both technologically realistic and algorithmically interesting. It enriches the problem, and opens the door to entirely new techniques. Our primary results are: [1] We design a new Contention Resolution protocol that guarantees latency $$O\left(\left(n\log\log n\log^{(3)} n\log^{(4)} n\cdots \log^{(\log^* n)} n\right)\cdot 2^{\log^* n}\right) \le n(\log\log n)^{1+o(1)}$$ in expectation and with high probability. This already establishes at least a roughly $\log n$ complexity gap between randomized protocols in GlobalClock and LocalClock. [2] Prior analyses of randomized ContentionResolution protocols in LocalClock guaranteed a certain latency with high probability, i.e., with probability $1-1/\text{poly}(n)$. We observe that it is just as natural to measure expected latency, and prove a $\log n$-factor complexity gap between the two objectives for memoryless protocols. The In-Expectation complexity is $Θ(n \log n/\log\log n)$ whereas the With-High-Probability latency is $Θ(n\log^2 n/\log\log n)$. Three of these four upper and lower bounds are new. [3] Given the complexity separation above, one would naturally want a ContentionResolution protocol that is optimal under both the In-Expectation and With-High-Probability metrics. This is impossible! It is even impossible to achieve In-Expectation latency $o(n\log^2 n/(\log\log n)^2)$ and With-High-Probability latency $n\log^{O(1)} n$ simultaneously.

</details>


### [5] [OServe: Accelerating LLM Serving via Spatial-Temporal Workload Orchestration](https://arxiv.org/abs/2602.12151)
*Youhe Jiang,Fangcheng Fu,Taiyi Wang,Guoliang He,Eiko Yoneki*

Main category: cs.DC

TL;DR: OServe是一个针对大语言模型服务的系统，通过异构灵活的模型部署来解决工作负载的空间和时间异质性问题，相比现有系统性能提升最高2倍


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统通常假设工作负载在空间上均匀且时间上稳定，采用同质、静态的模型部署。但实际工作负载存在显著的空间和时间异质性：空间上请求具有不同的计算和内存需求，时间上工作负载组成随时间变化。这种假设与现实的错配导致性能不佳。

Method: 1. 引入新颖的工作负载感知调度算法，根据实时工作负载特征优化异构模型部署；2. 提出高效的工作负载自适应切换方法，根据预测的工作负载变化迁移模型部署。

Result: 在真实世界追踪数据上的实验表明，OServe相比最先进的服务系统，性能提升最高可达2倍（平均1.5倍）。

Conclusion: OServe通过异构灵活的模型部署有效解决了LLM服务中的空间和时间异质性问题，显著提升了服务性能。

Abstract: Serving Large Language Models (LLMs) can benefit immensely from parallelizing both the model and input requests across multiple devices, but incoming workloads exhibit substantial spatial and temporal heterogeneity. Spatially, workloads comprise heterogeneous requests with varying compute and memory demands. Temporally, workload composition varies over time. Nevertheless, existing systems typically assume spatially uniform and temporally stable workloads, employing a homogeneous, static model deployment. This mismatch between the assumption and real-world spatial-temporal heterogeneity results in suboptimal performance. We present OServe, an LLM serving system with heterogeneous and flexible model deployment that addresses both spatial and temporal heterogeneity. First, OServe introduces a novel workload-aware scheduling algorithm that optimizes heterogeneous model deployments according to real-time workload characteristics. Second, OServe proposes an efficient workload-adaptive switching method that migrates model deployments in response to predicted workload changes. Experiments on real-world traces show that OServe improves performance by up to 2$\times$ (average: 1.5$\times$) compared to state-of-the-art serving systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [Device-Circuit Co-Design of Variation-Resilient Read and Write Drivers for Antiferromagnetic Tunnel Junction (AFMTJ) Memories](https://arxiv.org/abs/2602.11614)
*Yousuf Choudhary,Tosiron Adegbija*

Main category: cs.AR

TL;DR: 开发针对反铁磁隧道结的器件-电路协同设计读写接口，解决其超快动态和低隧穿磁阻带来的可靠性问题


<details>
  <summary>Details</summary>
Motivation: 反铁磁隧道结具有皮秒级开关速度和高度集成密度，适用于内存计算，但其超快动态特性和低隧穿磁阻使得现有MRAM接口不可靠

Method: 基于校准的SPICE AFMTJ模型，提出非对称脉冲驱动器实现确定性皮秒开关，以及具有动态触发点调谐的自定时感测放大器用于低TMR检测

Result: SPICE和蒙特卡洛评估表明，所提电路在保持AFMTJ延迟和能耗优势的同时，在现实PVT和3D集成寄生条件下实现了稳健的读写良率，优于标准MRAM前端

Conclusion: 通过器件-电路协同设计方法，成功开发了针对AFMTJ特性的优化读写接口，解决了其可靠性问题，为AFMTJ在内存计算中的应用提供了可行方案

Abstract: Antiferromagnetic Tunnel Junctions (AFMTJs) offer picosecond switching and high integration density for in-memory computing, but their ultrafast dynamics and low tunnel magnetoresistance (TMR) make state-of-the-art MRAM interfaces unreliable. This work develops a device-circuit co-designed read/write interface optimized for AFMTJ behavior. Using a calibrated SPICE AFMTJ model as a baseline, we identify the limitations of conventional drivers and propose an asymmetric pulse driver (PD) for deterministic picosecond switching and a self-timed sense amplifier (STSA) with dynamic trip-point tuning for low-TMR sensing. Our experiments using SPICE and Monte Carlo evaluations demonstrate that the proposed circuits preserve AFMTJ latency and energy benefits while achieving robust read/write yield under realistic PVT and 3D integration parasitics, outperforming standard MRAM front-ends under the same conditions.

</details>


### [7] [PAM: Processing Across Memory Hierarchy for Efficient KV-centric LLM Serving System](https://arxiv.org/abs/2602.11521)
*Lian Liu,Shixin Zhao,Yutian Zhou,Yintao He,Mengdi Wang,Yinhe Han,Ying Wang*

Main category: cs.AR

TL;DR: PAM是一个针对大语言模型服务的KV中心化系统，通过协调异构PIM内存设备的层次架构，平衡高内存带宽与可扩展容量，解决KV相关操作的内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用和上下文长度增加，KV相关操作（注意力计算和KV缓存存储）已成为关键瓶颈，需要大量内存带宽和容量。现有LLM服务系统针对计算密集型负载优化，无法有效处理这些内存密集型操作，即使使用PIM技术，当前单级内存设计也无法同时满足带宽和容量需求。

Method: 1. 利用KV访问模式中的上下文局部性，智能地将KV令牌分布在内存层次结构中；2. 提出PAMattention算法，支持跨异构PIM设备的细粒度并行注意力计算；3. 包含设备内KV映射、设备间KV迁移接口和设备间在线KV调度算法，动态平衡计算负载。

Result: PAM通过同时满足带宽和容量需求，显著提高了LLM服务系统的效率和可扩展性，为大规模AI时代提供了经济高效的高性能解决方案。

Conclusion: PAM是一个KV中心化的LLM服务系统，通过协调异构PIM内存设备的层次架构，平衡高内存带宽与可扩展容量，解决了现有系统在处理KV相关内存密集型操作时的瓶颈问题，为大规模AI应用提供了更高效的服务方案。

Abstract: The widespread adoption of Large Language Models (LLMs) has exponentially increased the demand for efficient serving systems. With growing requests and context lengths, key-value (KV)-related operations, including attention computation and KV cache storage, have emerged as critical bottlenecks. They require massive memory bandwidth and capacity. Unfortunately, existing LLM serving systems, optimized for compute-bound workloads, fail to handle these memory-intensive operations effectively. Even with Processing-In-Memory (PIM) technology, current single-level memory designs cannot simultaneously satisfy the bandwidth and capacity requirements.
  To address these challenges, we propose Processing Across Memory (PAM), a KV-centric LLM serving system that coordinates heterogeneous PIM-enabled memory devices within a hierarchical architecture. PAM introduces a novel computing paradigm to balance high memory bandwidth with scalable capacity. First, PAM exploits the inherent context locality in KV access patterns to intelligently distribute KV tokens across the memory hierarchy. Second, to further exploit context locality, it introduces the PAMattention algorithm, enabling fine-grained parallel attention computation across heterogeneous PIM devices. Finally, PAM incorporates an intra-device KV mapping, inter-device KV migration interface, and an inter-device online KV scheduling algorithm to dynamically balance computational workloads. By addressing both bandwidth and capacity demands simultaneously, PAM significantly enhances the efficiency and scalability of LLM serving systems, paving the way for cost-effective, high-performance solutions in the era of large-scale AI.

</details>
