<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Dynamic Stability of LLM-Generated Code](https://arxiv.org/abs/2511.07463)
*Prateek Rajput,Abdoul Aziz Bonkoungou,Yewei Song,Abdoul Kader Kabore,Iyiola E. Olatunji,Jacques Klein,Tegewende Bissyande*

Main category: cs.PL

TL;DR: 提出了一个评估代码生成动态稳定性的框架，通过SCTD和DCTD两个指标来衡量算法结构多样性和运行时行为方差，发现LLMs在功能正确代码中存在显著的算法方差，揭示了正确性与行为一致性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代码生成评估只关注功能正确性，忽略了算法复杂度差异。功能相同的解决方案可能有完全不同的性能成本，这暴露了现有评估方法的局限性。

Method: 提出基于操作码分布的两个指标：静态规范轨迹差异(SCTD)捕获算法结构多样性，动态规范轨迹差异(DCTD)量化运行时行为方差。它们的比值行为表达因子(BEF)作为诊断信号。

Result: 在BigOBench和CodeContests上的实验表明，最先进的LLMs即使在功能正确的输出中也表现出显著的算法方差。提高采样温度会改善pass@1率但降低稳定性。

Conclusion: 需要在代码生成中引入稳定性感知目标，并创建包含渐近测试用例的新基准，以实现更稳健、实用的LLM评估。

Abstract: Current evaluations of LLMs for code generation emphasize functional correctness, overlooking the fact that functionally correct solutions can differ significantly in algorithmic complexity. For instance, an $(O(n^2))$ versus $(O(n \log n))$ sorting algorithm may yield similar output but incur vastly different performance costs in production. This discrepancy reveals a critical limitation in current evaluation methods: they fail to capture the behavioral and performance diversity among correct solutions. To address this, we introduce a principled framework for evaluating the dynamic stability of generated code. We propose two metrics derived from opcode distributions: Static Canonical Trace Divergence (SCTD), which captures algorithmic structure diversity across generated solutions, and Dynamic Canonical Trace Divergence (DCTD), which quantifies runtime behavioral variance. Their ratio, the Behavioral Expression Factor (BEF), serves as a diagnostic signal: it indicates critical runtime instability when BEF $\ll$ 1 and functional redundancy when BEF $\gg$ 1. Empirical results on BigOBench and CodeContests show that state-of-the-art LLMs exhibit significant algorithmic variance even among functionally correct outputs. Notably, increasing sampling temperature improves pass@1 rates but degrades stability, revealing an unrecognized trade-off: searching for correct solutions in diverse output spaces introduces a "penalty of instability" between correctness and behavioral consistency. Our findings call for stability-aware objectives in code generation and new benchmarks with asymptotic test cases for robust, real-world LLM evaluation.

</details>


### [2] [Streaming Tensor Program: A streaming abstraction for dynamic parallelism](https://arxiv.org/abs/2511.07776)
*Gina Sohn,Genghan Zhang,Konstantin Hossfeld,Jungwoo Kim,Nathan Sobotka,Nathan Zhang,Olivia Hsu,Kunle Olukotun*

Main category: cs.PL

TL;DR: STeP是一个新的流式抽象，用于在空间数据流加速器上高效运行动态张量工作负载，通过引入灵活的路由操作符、显式内存层次和符号形状语义来支持动态行为。


<details>
  <summary>Details</summary>
Motivation: 当前的空间数据流加速器编程抽象表达能力有限，无法有效处理动态形状张量和数据相关控制流等动态行为，导致性能受限。

Method: 提出STeP流式抽象，包含灵活的路由操作符、显式内存层次和符号形状语义，支持动态分块、动态并行化和配置时分复用等优化。

Result: 在代表性LLM层上使用真实世界轨迹进行模拟，动态分块将片上内存需求减少2.18倍，动态并行化将延迟提高1.5倍，配置时分复用将计算利用率提高2.57倍。

Conclusion: STeP能够有效支持动态张量工作负载，在保持数据流效率的同时适应动态行为，显著提升了空间数据流加速器的性能。

Abstract: Dynamic behaviors are becoming prevalent in many tensor applications. In machine learning, for example, the input tensors are dynamically shaped or ragged, and data-dependent control flow is widely used in many models. However, the limited expressiveness of prior programming abstractions for spatial dataflow accelerators forces the dynamic behaviors to be implemented statically or lacks the visibility for performance-critical decisions. To address these challenges, we present the Streaming Tensor Program (STeP), a new streaming abstraction that enables dynamic tensor workloads to run efficiently on spatial dataflow accelerators. STeP introduces flexible routing operators, an explicit memory hierarchy, and symbolic shape semantics that expose dynamic data rates and tensor dimensions. These capabilities unlock new optimizations-dynamic tiling, dynamic parallelization, and configuration time-multiplexing-that adapt to dynamic behaviors while preserving dataflow efficiency. Using a cycle-approximate simulator on representative LLM layers with real-world traces, dynamic tiling reduces on-chip memory requirement by 2.18x, dynamic parallelization improves latency by 1.5x, and configuration time-multiplexing improves compute utilization by 2.57x over implementations available in prior abstractions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Towards Affordable, Adaptive and Automatic GNN Training on CPU-GPU Heterogeneous Platforms](https://arxiv.org/abs/2511.07421)
*Tong Qiao,Ao Zhou,Yingjie Qi,Yiou Wang,Han Wan,Jianlei Yang,Chunming Hu*

Main category: cs.DC

TL;DR: A3GNN是一个在异构CPU-GPU平台上实现经济实惠、自适应和自动GNN训练的框架，通过局部感知采样和细粒度并行调度提高资源利用率，使用强化学习优化吞吐量、内存占用和准确性的权衡。


<details>
  <summary>Details</summary>
Motivation: GNN训练通常依赖昂贵的高性能计算平台，限制了在许多任务中的可访问性。分析表明，通过在资源受限设备上充分利用可用资源，可以获得显著的效率提升。

Method: 提出A3GNN框架，采用局部感知采样和细粒度并行调度来提高资源使用效率，并利用强化学习探索设计空间，实现吞吐量、内存占用和准确性的帕累托最优权衡。

Result: 实验表明，A3GNN能够弥合性能差距，使7个Nvidia 2080Ti GPU在吞吐量上比2个A100 GPU高出1.8倍，且准确率损失最小。

Conclusion: A3GNN框架成功实现了在资源受限设备上高效训练GNN的目标，为更广泛的应用场景提供了可行的解决方案。

Abstract: Graph Neural Networks (GNNs) have been widely adopted due to their strong performance. However, GNN training often relies on expensive, high-performance computing platforms, limiting accessibility for many tasks. Profiling of representative GNN workloads indicates that substantial efficiency gains are possible on resource-constrained devices by fully exploiting available resources. This paper introduces A3GNN, a framework for affordable, adaptive, and automatic GNN training on heterogeneous CPU-GPU platforms. It improves resource usage through locality-aware sampling and fine-grained parallelism scheduling. Moreover, it leverages reinforcement learning to explore the design space and achieve pareto-optimal trade-offs among throughput, memory footprint, and accuracy. Experiments show that A3GNN can bridge the performance gap, allowing seven Nvidia 2080Ti GPUs to outperform two A100 GPUs by up to 1.8X in throughput with minimal accuracy loss.

</details>


### [4] [From Attention to Disaggregation: Tracing the Evolution of LLM Inference](https://arxiv.org/abs/2511.07422)
*Madabattula Rajesh Kumar,Srinivasa Rao Aravilli,Mustafa Saify,Shashank Srivastava*

Main category: cs.DC

TL;DR: 论文探讨了从传统单体GPU集群向分解式推理架构的转变，通过将计算密集的前填充阶段与内存密集的解码阶段解耦为独立可扩展组件，来解决大规模语言模型推理中的多目标优化问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型参数规模达到万亿级别，实时推理已成为主要瓶颈，部署这些模型面临着内存带宽、计算吞吐量和延迟要求的分布式系统挑战。

Method: 采用分解式推理架构，应用分布式系统原则如服务分解、资源解耦和工作负载分区，将预填充阶段和解码阶段分离为独立可扩展组件。

Result: 这种架构缓解了资源争用，使得关键指标如首令牌时间和令牌间延迟能够独立优化。

Conclusion: 分解式推理范式能够克服传统单体GPU集群的限制，为大规模语言模型推理提供更有效的解决方案。

Abstract: The evolution of Large Language Models from the Transformer architecture to models with trillions of parameters has shifted the primary bottleneck from model training to real time inference. Deploying these massive models is a complex distributed systems challenge constrained by memory bandwidth, computational throughput, and latency requirements. LLM inference fundamentally requires solving a multi objective optimization problem to minimize latency, maximize throughput, and reduce cost. This paper explores the necessary architectural shift towards disaggregated inference, which applies distributed systems principles such as service decomposition, resource disaggregation, and workload partitioning to overcome the limitations of traditional monolithic GPU clusters. By decoupling the compute intensive prefill phase from the memory intensive decode phase into independently scalable components, this paradigm mitigates resource contention and enables independent optimization of key metrics like Time to First Token and Inter Token Latency.

</details>


### [5] [Synera: Synergistic LLM Serving across Device and Cloud at Scale](https://arxiv.org/abs/2511.07423)
*Genglin Wang,Liekang Zeng,Bufang Yang,Kaiwei Liu,Guoliang Xing,Chumin Sun,Li Zhou,Jie Sun,Zhenyu Yan*

Main category: cs.DC

TL;DR: Synera是一个设备-云协同的LLM服务系统，通过SLM-LLM协同机制解决移动端LLM部署中的性能挑战，在保持低延迟的同时显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 移动端部署LLM面临生成质量下降和延迟延长的问题，现有云卸载方案受通信瓶颈限制，而设备端SLM方案因资源限制牺牲了生成质量。

Method: 提出设备-云协同LLM服务系统Synera，采用通信高效的选卸载、无停顿并行推理和可扩展云批处理等定制化设计。

Result: 在真实测试环境中，Synera相比竞争基线实现1.20-5.47倍的生成质量提升，延迟性能相当；相比现有云服务，在各种基准测试中降低8.2-16.5%的云服务成本。

Conclusion: Synera通过设备-云协同机制有效解决了移动端LLM部署的性能挑战，在保持低延迟的同时显著提升生成质量并降低云服务成本。

Abstract: Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.

</details>


### [6] [Enhancing reliability in AI inference services: An empirical study on real production incidents](https://arxiv.org/abs/2511.07424)
*Bhala Ranganathan,Mickey Zhang,Kai Wu*

Main category: cs.DC

TL;DR: 本文提出了首个基于实践的大语言模型推理事故分析框架，通过156个高严重性事故验证，识别出主要故障模式（60%为推理引擎故障）和缓解策略（74%可自动检测）。


<details>
  <summary>Details</summary>
Motivation: 超大规模LLM推理对云系统提出极高要求，即使短暂故障也会造成重大用户和业务影响，需要系统化分析来理解和降低这些风险。

Method: 基于一年运维经验开发分类法和方法论，验证了156个高严重性事故，并对2025年4-6月进行定量研究以确保时效性，达到高标注一致性（Cohen's K ~0.89）。

Result: 识别出主导故障模式（60%推理引擎故障，其中40%为超时），发现缓解杠杆（74%自动检测，28%需要热修复），许多事故通过流量路由、节点重新平衡或容量增加策略缓解。

Conclusion: 系统化、基于经验的分析可以推动更可靠和成本效益的LLM服务规模化，分类法指导了针对性策略如连接活性、GPU容量感知路由等，减少了事故影响并加速恢复。

Abstract: Hyperscale large language model (LLM) inference places extraordinary demands on cloud systems, where even brief failures can translate into significant user and business impact. To better understand and mitigate these risks, we present one of the first provider-internal, practice-based analysis of LLM inference incidents. We developed a taxonomy and methodology grounded in a year of operational experience, validating it on 156 high-severity incidents, and conducted a focused quantitative study of Apr-Jun 2025 to ensure recency and relevance. Our approach achieves high labeling consistency (Cohen's K ~0.89), identifies dominant failure modes (in our dataset ~60% inference engine failures, within that category ~40% timeouts), and surfaces mitigation levers (~74% auto-detected; ~28% required hotfix). Beyond hotfixes, many incidents were mitigated via traffic routing, node rebalancing, or capacity increase policies, indicating further automation opportunities. We also show how the taxonomy guided targeted strategies such as connection liveness, GPU capacity-aware routing, and per-endpoint isolation and reduced incident impact and accelerated recovery. Finally, we contribute a practitioner-oriented adoption checklist that enables others to replicate our taxonomy, analysis, and automation opportunities in their own systems. This study demonstrates how systematic, empirically grounded analysis of inference operations can drive more reliable and cost-efficient LLM serving at scale.

</details>


### [7] [An Evaluation of LLMs Inference on Popular Single-board Computers](https://arxiv.org/abs/2511.07425)
*Tung,Nguyen,Tuyen Nguyen*

Main category: cs.DC

TL;DR: 本研究在树莓派4/5和香橙派5 Pro三款单板计算机上，对25个量化开源大语言模型进行了性能基准测试，评估了生成吞吐量、内存使用和功耗表现。


<details>
  <summary>Details</summary>
Motivation: 随着设备端大语言模型推理需求增长，需要在边缘硬件上部署轻量级、经济高效的AI解决方案。单板计算机为本地化、保护隐私的推理提供了有前景的平台，但在LLM工作负载方面的研究仍不足。

Method: 使用Ollama和Llamafile两种推理运行时，在三种SBC上测试25个量化开源LLM，评估不同CPU配置下的生成吞吐量、内存使用和功耗，使用多种提示类型模拟实际工作负载。

Result: SBC可可靠支持最多15亿参数的模型，Llamafile相比Ollama实现高达4倍的吞吐量提升和30-40%的功耗降低。识别了架构特定的瓶颈，突出了运行时的权衡取舍。

Conclusion: 本研究首次对SBC上的LLM推理进行了广泛评估，弥合了高性能语言模型与可负担边缘计算之间的差距，为实际部署提供了实用建议。

Abstract: The growing demand for on-device large language model (LLM) inference is driving interest in deploying lightweight, cost-effective AI solutions on edge hardware. Single-board computers (SBCs) such as the Raspberry Pi and Orange Pi offer a promising platform for localized, privacy-preserving inference-but remain underexplored in the context of LLM workloads. In this work, we benchmark the performance of 25 quantized open-source LLMs across three SBCs-Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro-using two inference runtimes: Ollama and Llamafile. We evaluate generation throughput, memory usage, and power consumption under varying CPU configurations, using multiple prompt types to simulate realistic workloads. Our results show that SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving up to 4x higher throughput and 30-40% lower power usage than Ollama. We identify architecture-specific bottlenecks, highlight runtime-level trade-offs, and provide practical deployment recommendations. This study offers the first broad evaluation of LLM inference on SBCs, bridging the gap between high-performance language models and affordable edge computing.

</details>


### [8] [Network and Systems Performance Characterization of MCP-Enabled LLM Agents](https://arxiv.org/abs/2511.07426)
*Zihao Ding,Mufeng Zhu,Yao Liu*

Main category: cs.DC

TL;DR: 本文对MCP增强的LLM交互进行了测量分析，揭示了能力、性能和成本之间的权衡，并提出了并行工具调用和任务中止机制等优化建议。


<details>
  <summary>Details</summary>
Motivation: MCP虽然增强了LLM与外部工具的交互能力，但包含系统提示、工具定义和上下文历史等大量信息会显著增加token使用量，导致成本上升和计算负载增加。

Method: 采用基于测量的分析方法，研究不同LLM模型和MCP配置对token效率、成本、任务完成时间和成功率等关键性能指标的影响。

Result: 研究发现MCP配置会显著影响性能指标，揭示了能力增强与成本增加之间的权衡关系。

Conclusion: 通过启用并行工具调用和实施稳健的任务中止机制等优化措施，可以开发更高效、稳健且成本效益更高的MCP工作流程。

Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.

</details>


### [9] [DynaKV: Enabling Accurate and Efficient Long-Sequence LLM Decoding on Smartphones](https://arxiv.org/abs/2511.07427)
*Tuowei Wang,Minxing Huang,Fengzu Li,Ligeng Chen,Jinrui Zhang,Ju Ren*

Main category: cs.DC

TL;DR: DynaKV是一种针对智能手机上长序列解码的自适应键值缓存管理方法，通过迁移无关的集群适应、连续性中心的闪存管理和内存高效的缓存设计，解决了KVCache在有限DRAM容量下的效率和准确性问题。


<details>
  <summary>Details</summary>
Motivation: 随着对类人推理、多轮对话和长文本响应的需求增长，大语言模型需要支持高效的长序列解码。但由于智能手机DRAM容量有限，长序列解码受到键值缓存内存占用的限制，现有检索方法在解码过程中面临缓存分布偏移导致的准确性和效率问题。

Method: DynaKV集成了三种关键技术：1) 迁移无关的集群适应，在检索过程中自适应分割集群；2) 连续性中心的闪存管理，将相关条目和集群共置并使用双头布局；3) 内存高效的缓存设计，在DRAM和闪存间虚拟化缓存空间并扩展替换策略。

Result: 评估显示DynaKV相比最先进解决方案提高了检索准确性并减少了端到端延迟，平均准确率提升1.38倍，速度提升1.47倍。

Conclusion: DynaKV有效解决了智能手机上长序列解码的KVCache管理问题，其洞察可扩展到其他长上下文工作负载和多层内存层次结构，具有广泛适用性。

Abstract: As the demand for human-like reasoning, multi-turn dialogues, and long-form responses grows, large language models (LLMs) are increasingly expected to support efficient and effective long-sequence decoding. However, due to limited DRAM capacity, long-seuqence LLM decoding on smartphones is constrained by the key-value cache (KVCache), whose memory footprint increases linearly with sequence length. Retrieval-based methods mitigate DRAM pressure by offloading KVCache to flash and retrieving query-relevant entries through cluster-based indexing. Unfortunately, as decoding progresses, KVCache distribution shifts render static or local cluster updates progressively misaligned, excluding essential entries or fetching redundant ones. These issues are further exacerbated by smartphone-specific limitations in bandwidth, IOPS, and memory capacity.
  We propose DynaKV, the first adaptive KVCache management approach that jointly addresses accuracy and efficiency for long-sequence decoding on smartphones. DynaKV integrates three key techniques: (1) Migration-Free Cluster Adaptation, which adaptively splits clusters during retrieval without incurring additional transfers; (2) Continuity-Centric Flash Management, which co-locates correlated entries and clusters and employs a dual-head layout for efficient updates; and (3) Memory-Efficient Cache Design, which virtualizes cache space across DRAM and flash and extends replacement policies to align with cluster-level access patterns. Evaluations demonstrate that DynaKV improves retrieval accuracy and reduces end-to-end latency compared to state-of-the-art solutions, achieving average gains of $1.38\times$ in accuracy and $1.47\times$ speedups. Furthermore, the insights of DynaKV naturally extend to other long-context workloads and multi-tier memory hierarchies, underscoring its broader applicability.

</details>


### [10] [HyProv: Hybrid Provenance Management for Scientific Workflows](https://arxiv.org/abs/2511.07574)
*Vasilis Bountris,Lauritz Thamsen,Ulf Leser*

Main category: cs.DC

TL;DR: HyProv是一个混合溯源管理系统，结合集中式和联邦式范式，为工作流溯源轨迹提供可扩展的在线工作流感知查询。


<details>
  <summary>Details</summary>
Motivation: 现有溯源系统难以在可扩展性、实时处理、在线溯源分析和跨组件集成之间取得平衡，且大多数解决方案缺乏工作流感知能力。

Method: 使用集中式组件管理小型稳定的工作流规范特定溯源数据，并通过联邦查询不同可扩展监控和溯源数据库来补充大规模执行日志，支持低延迟访问当前执行数据。

Result: 实验表明HyProv可扩展到大型工作流，以亚秒级延迟回答溯源查询，且对集群仅增加适度的CPU和内存开销。

Conclusion: HyProv通过混合架构成功解决了工作流溯源管理中的可扩展性和实时性挑战，为复杂工作流系统提供了有效的溯源支持。

Abstract: Provenance plays a crucial role in scientific workflow execution, for instance by providing data for failure analysis, real-time monitoring, or statistics on resource utilization for right-sizing allocations. The workflows themselves, however, become increasingly complex in terms of involved components. Furthermore, they are executed on distributed cluster infrastructures, which makes the real-time collection, integration, and analysis of provenance data challenging. Existing provenance systems struggle to balance scalability, real-time processing, online provenance analytics, and integration across different components and compute resources. Moreover, most provenance solutions are not workflow-aware; by focusing on arbitrary workloads, they miss opportunities for workflow systems where optimization and analysis can exploit the availability of a workflow specification that dictates, to some degree, task execution orders and provides abstractions for physical tasks at a logical level.
  In this paper, we present HyProv, a hybrid provenance management system that combines centralized and federated paradigms to offer scalable, online, and workflow-aware queries over workflow provenance traces. HyProv uses a centralized component for efficient management of the small and stable workflow-specification-specific provenance, and complements this with federated querying over different scalable monitoring and provenance databases for the large-scale execution logs. This enables low-latency access to current execution data. Furthermore, the design supports complex provenance queries, which we exemplify for the workflow system Airflow in combination with the resource manager Kubernetes. Our experiments indicate that HyProv scales to large workflows, answers provenance queries with sub-second latencies, and adds only modest CPU and memory overhead to the cluster.

</details>


### [11] [Intelligence per Watt: Measuring Intelligence Efficiency of Local AI](https://arxiv.org/abs/2511.07885)
*Jon Saad-Falcon,Avanika Narayan,Hakki Orhun Akengin,J. Wes Griffin,Herumb Shandilya,Adrian Gamarra Lafuente,Medhya Goel,Rebecca Joseph,Shlok Natarajan,Etash Kumar Guha,Shang Zhu,Ben Athiwaratkun,John Hennessy,Azalia Mirhoseini,Christopher Ré*

Main category: cs.DC

TL;DR: 本地小语言模型（≤20B参数）在单轮对话和推理任务上能达到88.7%的准确率，通过智能每瓦特（IPW）指标评估，2023-2025年间IPW提升了5.3倍，本地查询覆盖率从23.2%增至71.3%，证明本地推理能有效分流集中式云基础设施的需求。


<details>
  <summary>Details</summary>
Motivation: 集中式云基础设施处理LLM查询面临扩展瓶颈，而小型LM性能提升和本地加速器的发展使得重新思考这一范式成为可能，研究本地推理是否能有效分流云基础设施需求。

Method: 提出智能每瓦特（IPW）作为评估指标，对20+个最先进本地LM、8个加速器和100万真实单轮对话及推理查询进行大规模实证研究，测量准确率、能耗、延迟和功率。

Result: 本地LM能准确回答88.7%的单轮对话和推理查询；2023-2025年IPW提升5.3倍，本地查询覆盖率从23.2%增至71.3%；本地加速器IPW比云加速器至少低1.4倍。

Conclusion: 本地推理能有效分流集中式基础设施需求，IPW是跟踪这一转变的关键指标，并发布了IPW分析工具用于系统化基准测试。

Abstract: Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.

</details>


### [12] [Generic Algorithm for Universal TDM Communication Over Inter Satellite Links](https://arxiv.org/abs/2511.08034)
*Miroslav Popovic,Marko Popovic,Pavle Vasiljevic,Ilija Basicevic*

Main category: cs.DC

TL;DR: 提出了一个通用的TDM通信算法，克服了现有联邦学习框架中节点只能与单个对等节点通信的限制，支持节点与任意数量对等节点通信。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习框架中的TDM通信算法只允许网络节点之间进行成对通信，这限制了实际应用场景，特别是卫星间链路通信的需求。

Method: 开发了新的通用TDM通信算法，包括理论基础、系统设计和系统验证三个部分，支持节点与多个对等节点同时通信。

Result: 新算法能够支持真实的TDM通信场景，特别是卫星间链路通信，克服了原有算法的局限性。

Conclusion: 提出的通用TDM通信算法扩展了联邦学习框架的通信能力，使其能够适应更复杂的现实世界通信需求。

Abstract: The original Python Testbed for Federated Learning Algorithms is a light FL framework, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the TDM communication (i.e., peer data exchange) in the current time slot. The limitation of the latter is that it allows communication only between pairs of network nodes. This paper presents the new generic algorithm for the universal TDM communication that overcomes this limitation, such that a node can communicate with an arbitrary number of peers (assuming the peers also want to communicate with it). The paper covers: (i) the algorithm's theoretical foundation, (ii) the system design, and (iii) the system validation. The main advantage of the new algorithm is that it supports real-world TDM communications over inter satellite links.

</details>


### [13] [UniFormer: Unified and Efficient Transformer for Reasoning Across General and Custom Computing](https://arxiv.org/abs/2511.08135)
*Zhuoheng Ran,Chong Wu,Renjie Xu,Maolin Che,Hong Yan*

Main category: cs.DC

TL;DR: UniFormer是一个统一高效的Transformer架构，旨在同时优化通用计算平台（如GPU）和定制计算平台（如FPGA）的性能，解决模型在不同计算平台间迁移时的效率损失问题。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer模型在通用计算和定制计算平台间迁移时，由于并行计算范式的根本差异，通常需要在复杂性、效率或准确性方面做出妥协，且跨平台优化原则尚未充分探索。

Method: 通过实现更高的并行性和计算-存储融合，UniFormer架构在保持Transformer核心优势的同时，优化了在不同计算平台上的部署效率。

Result: UniFormer在GPU上实现了最先进的准确性和延迟性能，同时在FPGA上表现出强大的适应性。

Conclusion: 这是首个同时考虑通用计算和定制计算架构的高效Transformer工作，为跨平台模型部署提供了统一解决方案。

Abstract: The success of neural networks such as convolutional neural networks (CNNs) has been largely attributed to their effective and widespread deployment on customised computing platforms, including field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). In the current era, Transformer-based architectures underpin the majority of state-of-the-art (SOTA) larger models that are also increasingly deployed on customised computing hardware for low-power and real-time applications. However, the fundamentally different parallel computation paradigms between general-purpose and customised computing often lead to compromises in model transfer and deployability, which typically come at the cost of complexity, efficiency or accuracy. Moreover, many cross-platform optimisation principles have also remained underexplored in existing studies. This paper introduces UniFormer, a unified and efficient Transformer architecture for both general-purpose and customised computing platforms. By enabling higher parallelism and compute-storage fusion, UniFormer achieves state-of-the-art (SOTA) accuracy and latency on GPUs while exhibiting strong adaptability on FPGAs. To the best of our knowledge, this paper is the first efficient Transformer work that jointly considers both general-purpose and customised computing architectures.

</details>


### [14] [ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum](https://arxiv.org/abs/2511.08147)
*Andrija Stanisic,Stefan Nastic*

Main category: cs.DC

TL;DR: ProbSelect是一种用于GPU加速设备上联邦学习客户端选择的新方法，通过分析建模和概率预测，无需历史数据或持续监控，在3D连续体中提高SLO合规性并减少计算浪费。


<details>
  <summary>Details</summary>
Motivation: 在边缘、云和空间设备统一的3D连续体中，传统客户端选择方法依赖持续监控和历史数据收集，这在动态环境中不实用。现有解决方案主要考虑基于CPU的计算，无法捕捉GPU加速训练的复杂特性。

Method: 使用分析建模和概率预测进行客户端选择，无需历史数据或持续监控。在用户定义的SLO内建模客户端选择过程。

Result: 在不同GPU架构和工作负载上的广泛评估显示，ProbSelect相比基线方法平均提高13.77%的SLO合规性，同时实现72.5%的计算浪费减少。

Conclusion: ProbSelect为3D连续体中的GPU加速联邦学习提供了一种有效的客户端选择方法，显著提高了性能并减少了资源浪费。

Abstract: Integration of edge, cloud and space devices into a unified 3D continuum imposes significant challenges for client selection in federated learning systems. Traditional approaches rely on continuous monitoring and historical data collection, which becomes impractical in dynamic environments where satellites and mobile devices frequently change operational conditions. Furthermore, existing solutions primarily consider CPU-based computation, failing to capture complex characteristics of GPU-accelerated training that is prevalent across the 3D continuum. This paper introduces ProbSelect, a novel approach utilizing analytical modeling and probabilistic forecasting for client selection on GPU-accelerated devices, without requiring historical data or continuous monitoring. We model client selection within user-defined SLOs. Extensive evaluation across diverse GPU architectures and workloads demonstrates that ProbSelect improves SLO compliance by 13.77% on average while achieving 72.5% computational waste reduction compared to baseline approaches.

</details>


### [15] [\uline{LO}w-c\uline{O}st yet High-\uline{P}erformant \uline{S}parse Matrix-Matrix Multiplication on Arm SME Architectures](https://arxiv.org/abs/2511.08158)
*Kelun Lei,Hailong Yang,Kaige Zhang,Kejie Ma,Yiqing Wang,Xin You,Yufan Xu,Enrique S. Quintana-Orti,Zhongzhi Luan,Yi Liu,Depei Qian*

Main category: cs.DC

TL;DR: LOOPS是一个混合执行框架，通过结合行式CSR和向量式BCSR布局，协同利用NEON向量指令和SME矩阵扩展资源，在多精度SpMM计算中实现显著性能提升和能效优势。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵-稠密矩阵乘法(SpMM)在科学计算和图学习中至关重要，但Armv9架构的SME扩展和传统SIMD资源在非结构化稀疏工作负载中的有效利用仍具挑战。

Method: 提出LOOPS混合执行框架，采用CSR-part和BCSR-part混合布局，通过自适应两级并行化方案和轻量级性能模型支持FP64/FP32/FP16多精度SpMM。

Result: 在Apple M4Pro CPU上，LOOPS相比TACO基准实现FP32 9.93倍/FP64 14.4倍加速，相比Armadillo实现FP32 71.3倍/FP64 54.8倍加速；与NVIDIA A100 GPU上的cuSPARSE和Magicube相比，LOOPS获得19.8-33.5倍加速，且能效显著更优。

Conclusion: LOOPS框架成功解决了Arm架构下稀疏矩阵计算的性能瓶颈，在CPU上实现了超越GPU的性能和能效表现。

Abstract: Sparse matrix-dense matrix multiplication (SpMM) is a critical kernel in both scientific computing and emerging graph learning workloads. The recent Armv9 architecture introduces Scalable Matrix Extension (SME), enabling tile-based matrix operations with high throughput. However, effectively exploiting both SME and traditional SIMD resources for unstructured sparse workloads remains an open challenge. To address this, we propose LOOPS, a hybrid execution framework that combines row-wise CSR-part with vector-wise BCSR-part layout, enabling cooperative utilization of vector instructions (NEON) and Scalable Matrix Extension (SME) resources. LOOPS supports multi-precision SpMM across FP64, FP32, and FP16 via an adaptive two-level parallelization scheme guided by a lightweight performance model. Experimental results on the entire SuiteSparse on an Apple's M4Pro CPU show that LOOPS achieves average speedups of 9.93$\times$ (FP32)/14.4$\times$ (FP64) against the CPU baseline TACO and 71.3$\times$ (FP32)/54.8$\times$ (FP64) with respect to Armadillo. A comparison of LOOPS running on the same CPU with two GPU methods (cuSPARSE, Magicube) executed on an NVIDIA A100 GPU show average speedups for LOOPS between 19.8$\times$ and 33.5$\times$, depending on the precision. Notably, LOOPS delivers significantly better energy efficiency than the GPU codes on the A100 GPU.

</details>


### [16] [Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin](https://arxiv.org/abs/2511.08222)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 该论文研究了图上的群机器人聚集问题，在顶点和边传递图（如无限网格和超立方体）上设计了两种时间最优的聚集算法，并给出了基本的不可能性结果。


<details>
  <summary>Details</summary>
Motivation: 研究在具有多重性且无法检测多重性的限制条件下，群机器人在顶点和边传递图上的聚集问题，探索在敌对环境下的可行解决方案。

Method: 使用轮询调度器，针对无限网格和超立方体这两种特定拓扑结构，设计了两种时间最优的聚集算法，充分利用了底层拓扑的特性。

Result: 提出了两种针对特定拓扑的时间最优聚集算法，并证明了在某些条件下聚集是不可能的。

Conclusion: 由于算法严重依赖特定拓扑的特性，作者推测不存在适用于所有可解情况的通用聚集算法。

Abstract: In the field of swarm robotics, one of the most studied problem is Gathering. It asks for a distributed algorithm that brings the robots to a common location, not known in advance. We consider the case of robots constrained to move along the edges of a graph under the well-known OBLOT model. Gathering is then accomplished once all the robots occupy a same vertex. Differently from classical settings, we assume: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities; iii) robots move along the edges of vertex- and edge-transitive graphs, i.e. graphs where all the vertices (and the edges, resp.) belong to a same class of equivalence. To balance somehow such a `hostile' setting, as a scheduler for the activation of the robots, we consider the round-robin, where robots are cyclically activated one at a time.
  We provide some basic impossibility results and we design two different algorithms approaching the Gathering for robots moving on two specific topologies belonging to edge- and vertex-transitive graphs: infinite grids and hypercubes. The two algorithms are both time-optimal and heavily exploit the properties of the underlying topologies. Because of this, we conjecture that no general algorithm can exist for all the solvable cases.

</details>


### [17] [Priority Matters: Optimising Kubernetes Clusters Usage with Constraint-Based Pod Packing](https://arxiv.org/abs/2511.08373)
*Henrik Daniel Christensen,Saverio Giallorenzo,Jacopo Mauro*

Main category: cs.DC

TL;DR: 使用约束编程优化Kubernetes调度器，在默认调度器失败时作为备用机制，在1秒调度窗口内能在44%场景中放置更多高优先级pod，10秒窗口内提升至73%场景


<details>
  <summary>Details</summary>
Motivation: Kubernetes默认调度器使用轻量级启发式算法，可能导致次优的pod放置和资源碎片化，阻止了本可部署的pod分配

Method: 提出使用约束编程找到满足所有优先级和资源请求的最优pod分配方案，作为默认调度器的插件，在pod无法分配时作为备用机制运行，使用OR-Tools约束求解器

Result: 在中小型集群实验中，1秒调度窗口内能在44%的可实现分配场景中比默认调度器放置更多高优先级pod，在19%场景中证明默认调度器已最优；10秒窗口内改进比例提升至73%，仍能在19%场景中证明默认调度器已最优

Conclusion: 约束编程方法能有效优化Kubernetes调度，在合理时间内显著提高pod分配效率，同时能够验证默认调度器的分配质量

Abstract: Distributed applications employ Kubernetes for scalable, fault-tolerant deployments over computer clusters, where application components run in groups of containers called pods. The scheduler, at the heart of Kubernetes' architecture, determines the placement of pods given their priority and resource requirements on cluster nodes. To quickly allocate pods, the scheduler uses lightweight heuristics that can lead to suboptimal placements and resource fragmentation, preventing allocations of otherwise deployable pods on the available nodes.
  We propose the usage of constraint programming to find the optimal allocation of pods satisfying all their priorities and resource requests. Implementation-wise, our solution comes as a plug-in to the default scheduler that operates as a fallback mechanism when some pods cannot be allocated. Using the OR-Tools constraint solver, our experiments on small-to-mid-sized clusters indicate that, within a 1-second scheduling window, our approach places more higher-priority pods than the default scheduler (possibly demonstrating allocation optimality) in over 44\% of realisable allocation scenarios where the default scheduler fails, while certifying that the default scheduler's placement is already optimal in over 19\% of scenarios. With a 10-second window, our approach improves placements in over 73\% and still certifies that the default scheduler's placement is already optimal in over 19\% of scenarios.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [FractalCloud: A Fractal-Inspired Architecture for Efficient Large-Scale Point Cloud Processing](https://arxiv.org/abs/2511.07665)
*Yuzhe Fu,Changchun Zhou,Hancheng Ye,Bowen Duan,Qiyu Huang,Chiyue Wei,Cong Guo,Hai "Helen'' Li,Yiran Chen*

Main category: cs.AR

TL;DR: FractalCloud是一种受分形启发的硬件架构，通过形状感知的分区方法和块并行点操作，高效处理大规模3D点云，相比现有加速器实现21.7倍加速和27倍能耗降低。


<details>
  <summary>Details</summary>
Motivation: 随着点云神经网络处理大规模点云（数十万点），全对全计算和全局内存访问带来O(n²)计算复杂度和内存流量，现有加速器因低效分区和非并行架构而扩展性差。

Method: 提出FractalCloud架构：1）协同设计的Fractal方法进行形状感知和硬件友好的分区；2）块并行点操作分解并并行化所有点操作；专用硬件设计支持片上分形和灵活并行。

Result: 在28nm工艺下实现1.5mm²核心面积，相比最先进加速器实现21.7倍加速和27倍能耗降低，同时保持网络精度。

Conclusion: FractalCloud展示了在大规模PNN推理中的可扩展性和高效性，为大规模3D点云处理提供了有效的硬件解决方案。

Abstract: Three-dimensional (3D) point clouds are increasingly used in applications such as autonomous driving, robotics, and virtual reality (VR). Point-based neural networks (PNNs) have demonstrated strong performance in point cloud analysis, originally targeting small-scale inputs. However, as PNNs evolve to process large-scale point clouds with hundreds of thousands of points, all-to-all computation and global memory access in point cloud processing introduce substantial overhead, causing $O(n^2)$ computational complexity and memory traffic where n is the number of points}. Existing accelerators, primarily optimized for small-scale workloads, overlook this challenge and scale poorly due to inefficient partitioning and non-parallel architectures. To address these issues, we propose FractalCloud, a fractal-inspired hardware architecture for efficient large-scale 3D point cloud processing. FractalCloud introduces two key optimizations: (1) a co-designed Fractal method for shape-aware and hardware-friendly partitioning, and (2) block-parallel point operations that decompose and parallelize all point operations. A dedicated hardware design with on-chip fractal and flexible parallelism further enables fully parallel processing within limited memory resources. Implemented in 28 nm technology as a chip layout with a core area of 1.5 $mm^2$, FractalCloud achieves 21.7x speedup and 27x energy reduction over state-of-the-art accelerators while maintaining network accuracy, demonstrating its scalability and efficiency for PNN inference.

</details>


### [19] [PIMfused: Near-Bank DRAM-PIM with Fused-layer Dataflow for CNN Data Transfer Optimization](https://arxiv.org/abs/2511.07985)
*Simei Yang,Xinyu Shi,Lu Zhao,Yunyu Ling,Quanjun Wang,Francky Catthoor*

Main category: cs.AR

TL;DR: PIMfused是一种硬件软件协同设计，通过融合层数据流在近内存处理架构中优化CNN执行，减少跨bank数据传输，提升性能效率。


<details>
  <summary>Details</summary>
Motivation: 传统逐层数据流在DRAM-PIM架构中导致跨bank数据传输，成为CNN加速的性能瓶颈，需要打破层间bank依赖关系。

Method: 采用融合层数据流，在近bank DRAM-PIM中实现端到端CNN执行，提高数据重用并消除跨bank数据依赖。

Result: 在4-bank PIMcores配置下，相比GDDR6-AiM基线，内存周期减少至30.6%，能耗降至83.4%，面积降至76.5%。

Conclusion: PIMfused通过融合层数据流有效优化了DRAM-PIM架构中的CNN执行，显著提升了性能、功耗和面积效率。

Abstract: Near-bank Processing-in-Memory (PIM) architectures integrate processing cores (PIMcores) close to DRAM banks to mitigate the high cost of off-chip memory accesses. When accelerating convolutional neural network (CNN) on DRAM-PIM, performance is often constrained by cross-bank (or cross-PIMcore) data transfers, which are induced by the conventional layer-by-layer dataflow that enforces inter-bank (or inter-PIMcore) dependencies across successive CNN layers. To address this challenge, we propose PIMfused, a hardware-software co-design that enables fused-layer dataflow for end-to-end CNN execution in near-bank DRAM-PIM. By adopting fused-layer dataflow, PIMfused improves data reuse and, more importantly, breaks inter-bank data dependencies, thereby optimizing cross-bank data transfers without sacrificing bank-level parallelism. We study the impact of buffer sizes and PIMcore parallelism (1-bank vs. 4-bank) on PIMfused using end-to-end ResNet18. We present three key takeaways and show that with 4-bank PIMcores, PIMfused achieves overall PPA gains over a GDDR6-AiM-like baseline, cutting memory cycles to 30.6%, energy to 83.4%, and area to 76.5%.

</details>


### [20] [Re$^{\text{2}}$MaP: Macro Placement by Recursively Prototyping and Packing Tree-based Relocating](https://arxiv.org/abs/2511.08054)
*Yunqi Shi,Xi Lin,Zhiang Wang,Siyuan Xu,Shixiong Kai,Yao Lai,Chengrui Gao,Ke Xue,Mingxuan Yuan,Chao Qian,Zhi-Hua Zhou*

Main category: cs.AR

TL;DR: Re$^{\text{2}}$MaP是一种通过递归原型设计和基于树的重新定位来生成专家级宏布局的方法，在WNS和TNS方面相比现有技术有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有宏布局方法在优化时序指标（如WNS和TNS）方面存在不足，需要一种能够综合考虑线长、数据流和设计约束的改进方法。

Method: 采用多级宏分组和PPA感知单元聚类，使用DREAMPlace构建混合尺寸布局原型，引入ABPlace在椭圆上优化宏位置，并通过基于树的重新定位程序联合调整宏组位置。

Result: 相比Hier-RTLMP，WNS提升最高22.22%（平均10.26%），TNS提升最高97.91%（平均33.97%），在WNS、TNS、功耗、DRC违规和运行时间方面均优于ReMaP。

Conclusion: Re$^{\text{2}}$MaP通过递归原型设计和基于树的重新定位，有效提升了宏布局质量，在多个关键指标上优于现有方法。

Abstract: This work introduces the Re$^{\text{2}}$MaP method, which generates expert-quality macro placements through recursively prototyping and packing tree-based relocating. We first perform multi-level macro grouping and PPA-aware cell clustering to produce a unified connection matrix that captures both wirelength and dataflow among macros and clusters. Next, we use DREAMPlace to build a mixed-size placement prototype and obtain reference positions for each macro and cluster. Based on this prototype, we introduce ABPlace, an angle-based analytical method that optimizes macro positions on an ellipse to distribute macros uniformly near chip periphery, while optimizing wirelength and dataflow. A packing tree-based relocating procedure is then designed to jointly adjust the locations of macro groups and the macros within each group, by optimizing an expertise-inspired cost function that captures various design constraints through evolutionary search. Re$^{\text{2}}$MaP repeats the above process: Only a subset of macro groups are positioned in each iteration, and the remaining macros are deferred to the next iteration to improve the prototype's accuracy. Using a well-established backend flow with sufficient timing optimizations, Re$^{\text{2}}$MaP achieves up to 22.22% (average 10.26%) improvement in worst negative slack (WNS) and up to 97.91% (average 33.97%) improvement in total negative slack (TNS) compared to the state-of-the-art academic placer Hier-RTLMP. It also ranks higher on WNS, TNS, power, design rule check (DRC) violations, and runtime than the conference version ReMaP, across seven tested cases. Our code is available at https://github.com/lamda-bbo/Re2MaP.

</details>


### [21] [BDD2Seq: Enabling Scalable Reversible-Circuit Synthesis via Graph-to-Sequence Learning](https://arxiv.org/abs/2511.08315)
*Mingkai Miao,Jianheng Tang,Guangyu Hu,Hongce Zhang*

Main category: cs.AR

TL;DR: BDD2Seq使用图神经网络和指针网络预测BDD变量排序，在可逆电路合成中实现更低的量子成本和更快的合成速度


<details>
  <summary>Details</summary>
Motivation: 现有启发式方法在处理复杂电路时变量排序效果不佳，影响BDD节点数和量子成本等关键指标

Method: 提出图到序列框架，结合图神经网络编码器、指针网络解码器和多样化束搜索来预测高质量变量排序

Result: 在三个公开基准测试中，BDD2Seq比现代启发式算法实现约1.4倍更低的量子成本和3.7倍更快的合成速度

Conclusion: 这是首个使用基于图的生成模型和多样性解码来解决BDD可逆电路合成中变量排序问题的工作

Abstract: Binary Decision Diagrams (BDDs) are instrumental in many electronic design automation (EDA) tasks thanks to their compact representation of Boolean functions. In BDD-based reversible-circuit synthesis, which is critical for quantum computing, the chosen variable ordering governs the number of BDD nodes and thus the key metrics of resource consumption, such as Quantum Cost. Because finding an optimal variable ordering for BDDs is an NP-complete problem, existing heuristics often degrade as circuit complexity grows. We introduce BDD2Seq, a graph-to-sequence framework that couples a Graph Neural Network encoder with a Pointer-Network decoder and Diverse Beam Search to predict high-quality orderings. By treating the circuit netlist as a graph, BDD2Seq learns structural dependencies that conventional heuristics overlooked, yielding smaller BDDs and faster synthesis. Extensive experiments on three public benchmarks show that BDD2Seq achieves around 1.4 times lower Quantum Cost and 3.7 times faster synthesis than modern heuristic algorithms. To the best of our knowledge, this is the first work to tackle the variable-ordering problem in BDD-based reversible-circuit synthesis with a graph-based generative model and diversity-promoting decoding.

</details>


### [22] [DRACO: Co-design for DSP-Efficient Rigid Body Dynamics Accelerator](https://arxiv.org/abs/2511.08395)
*Xingyu Liu,Jiawei Liang,Yipu Zhang,Linfeng Du,Chaofang Ma,Hui Yu,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: 提出基于FPGA的硬件高效RBD加速器，包含精度感知量化框架、除法延迟优化和模块间DSP复用方法，在多种机器人类型上实现8倍吞吐量提升和7.4倍延迟降低。


<details>
  <summary>Details</summary>
Motivation: 为高自由度机器人系统开发硬件高效的RBD加速器，解决传统方法在DSP资源需求和性能方面的限制。

Method: 1. 精度感知量化框架减少DSP需求同时保持运动精度；2. 质量矩阵逆算法中的除法延迟优化；3. 模块间DSP复用方法提高DSP利用率。

Result: 相比最先进的RBD加速器，在各种机器人类型上实现最高8倍吞吐量提升和7.4倍延迟降低。

Conclusion: 该工作展示了针对高自由度机器人系统的有效且可扩展的硬件加速解决方案。

Abstract: We propose a hardware-efficient RBD accelerator based on FPGA, introducing three key innovations. First, we propose a precision-aware quantization framework that reduces DSP demand while preserving motion accuracy. This is also the first study to systematically evaluate quantization impact on robot control and motion for hardware acceleration. Second, we leverage a division deferring optimization in mass matrix inversion algorithm, which decouples reciprocal operations from the longest latency path to improve the performance. Finally, we present an inter-module DSP reuse methodology to improve DSP utilization and save DSP usage. Experiment results show that our work achieves up to 8x throughput improvement and 7.4x latency reduction over state-of-the-art RBD accelerators across various robot types, demonstrating its effectiveness and scalability for high-DOF robotic systems.

</details>


### [23] [CO2-Meter: A Comprehensive Carbon Footprint Estimator for LLMs on Edge Devices](https://arxiv.org/abs/2511.08575)
*Zhenxiao Fu,Chen Fan,Lei Jiang*

Main category: cs.AR

TL;DR: CO2-Meter是一个用于估计LLM边缘推理中运营和隐含碳排放的统一框架，解决了现有估算器忽略外围能耗、不同推理阶段行为和SoC设计复杂性的问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在边缘设备上的部署面临巨大碳挑战，现有估算方法不完整，忽略了外围能耗、预填充/解码阶段的不同行为以及SoC设计复杂性。

Method: 提出CO2-Meter框架，包括：(1)基于方程的外围能耗模型和数据集；(2)基于GNN的预测器，包含特定阶段的LLM能耗数据；(3)用于SoC瓶颈分析的单元级隐含碳模型；(4)验证显示优于现有方法的准确性。

Result: 验证表明CO2-Meter在准确性上优于现有方法，案例研究显示其能有效识别碳热点并指导边缘平台上的可持续LLM设计。

Conclusion: CO2-Meter为LLM边缘推理提供了一个准确的碳排放估算框架，有助于识别碳热点和指导可持续设计实践。

Abstract: LLMs have transformed NLP, yet deploying them on edge devices poses great carbon challenges. Prior estimators remain incomplete, neglecting peripheral energy use, distinct prefill/decode behaviors, and SoC design complexity. This paper presents CO2-Meter, a unified framework for estimating operational and embodied carbon in LLM edge inference. Contributions include: (1) equation-based peripheral energy models and datasets; (2) a GNN-based predictor with phase-specific LLM energy data; (3) a unit-level embodied carbon model for SoC bottleneck analysis; and (4) validation showing superior accuracy over prior methods. Case studies show CO2-Meter's effectiveness in identifying carbon hotspots and guiding sustainable LLM design on edge platforms. Source code: https://github.com/fuzhenxiao/CO2-Meter

</details>
