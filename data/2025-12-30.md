<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 8]
- [cs.DC](#cs.DC) [Total: 29]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Symbolic Specification and Reasoning for Quantum Data and Operations](https://arxiv.org/abs/2512.22383)
*Mingsheng Ying*

Main category: cs.PL

TL;DR: 提出符号算子逻辑(SOL)框架，用于量子数据和操作的符号化规范与推理，通过将经典一阶逻辑嵌入到形式算子语言中，为量子计算的自动化验证提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 量子信息与计算研究中，符号方法已被广泛用于量子态和操作的人工规范与推理，但对量子数据和操作的符号化规范与推理缺乏形式化理论，这限制了量子计算中自动化验证技术的实际应用。

Method: 提出符号算子逻辑(SOL)框架，将经典一阶逻辑语言嵌入到用于规范量子数据和操作的形式算子语言中，包括递归定义。这种嵌入允许在底层经典数据理论（如布尔代数或群论）下推理量子性质。

Result: 建立了SOL框架，为量子数据和操作的符号化规范与推理提供了形式化基础，使得能够利用现有为经典计算开发的自动化验证工具来验证量子算法和程序。

Conclusion: SOL框架为量子计算和信息的正式验证及自动化定理证明（在Lean、Coq等证明助手中）提供了概念基础，解决了量子计算自动化验证中缺乏形式化理论的问题。

Abstract: In quantum information and computation research, symbolic methods have been widely used for human specification and reasoning about quantum states and operations. At the same time, they are essential for ensuring the scalability and efficiency of automated reasoning and verification tools for quantum algorithms and programs. However, a formal theory for symbolic specification and reasoning about quantum data and operations is still lacking, which significantly limits the practical applicability of automated verification techniques in quantum computing.
  In this paper, we present a general logical framework, called Symbolic Operator Logic $\mathbf{SOL}$, which enables symbolic specification and reasoning about quantum data and operations. Within this framework, a classical first-order logical language is embedded into a language of formal operators used to specify quantum data and operations, including their recursive definitions. This embedding allows reasoning about their properties modulo a chosen theory of the underlying classical data (e.g., Boolean algebra or group theory), thereby leveraging existing automated verification tools developed for classical computing. It should be emphasised that this embedding of classical first-order logic into $\mathbf{SOL}$ is precisely what makes the symbolic method possible.
  We envision that this framework can provide a conceptual foundation for the formal verification and automated theorem proving of quantum computation and information in proof assistants such as Lean, Coq, and related systems.

</details>


### [2] [Eliminate Branches by Melding IR Instructions](https://arxiv.org/abs/2512.22390)
*Yuze Li,Srinivasan Ramachandra Sharma,Charitha Saumya,Ali R. Butt,Kirshanthan Sundararajah*

Main category: cs.PL

TL;DR: MERIT是一种编译器转换技术，通过对齐和融合不同分支路径中的相似操作来消除分支，在LLVM中实现，平均加速10.9%，峰值提升32倍。


<details>
  <summary>Details</summary>
Motivation: 现代处理器中分支预测失败会导致严重的性能损失。虽然存在硬件预测器和配置文件引导技术，但具有不规则模式的数据相关分支仍然具有挑战性。传统的if-conversion通过软件谓词消除分支，但在x86等架构上存在限制，经常在包含内存指令的路径上失败，或者通过完全推测大型分支体产生过多的指令开销。

Method: MERIT（Melding IR Instructions）是一种编译器转换，通过在对齐的IR指令级别上融合不同路径中的相似操作来消除分支。它观察到不同路径通常执行结构相似但操作数不同的操作，采用序列对齐来发现融合机会，并使用安全的操作数级保护来确保语义正确性，无需硬件谓词。

Result: 在来自四个基准测试套件的102个程序上评估，MERIT实现了10.9%的几何平均加速，相比硬件分支预测器峰值改进达到32倍，展示了在减少静态指令开销方面的有效性。

Conclusion: MERIT通过IR级别的指令对齐和融合，有效消除了数据相关分支，避免了传统if-conversion的限制，显著提升了程序性能，特别是在具有不规则模式的分支场景中。

Abstract: Branch mispredictions cause catastrophic performance penalties in modern processors, leading to performance loss. While hardware predictors and profile-guided techniques exist, data-dependent branches with irregular patterns remain challenging. Traditional if-conversion eliminates branches via software predication but faces limitations on architectures like x86. It often fails on paths containing memory instructions or incurs excessive instruction overhead by fully speculating large branch bodies.
  This paper presents Melding IR Instructions (MERIT), a compiler transformation that eliminates branches by aligning and melding similar operations from divergent paths at the IR instruction level. By observing that divergent paths often perform structurally similar operations with different operands, MERIT adapts sequence alignment to discover merging opportunities and employs safe operand-level guarding to ensure semantic correctness without hardware predication. Implemented as an LLVM pass and evaluated on 102 programs from four benchmark suites, MERIT achieves a geometric mean speedup of 10.9% with peak improvements of 32x compared to hardware branch predictor, demonstrating the effectiveness with reduced static instruction overhead.

</details>


### [3] [A Bounded Game Semantics Checker for Precise Smart Contract Analysis](https://arxiv.org/abs/2512.22417)
*Vasileios Koutavas,Yu-Yang Lin,Nikos Tzevelekos*

Main category: cs.PL

TL;DR: 基于博弈语义的智能合约漏洞检测方法，通过YulToolkit工具实现有界完备性、无假阳性，可扩展到真实合约。


<details>
  <summary>Details</summary>
Motivation: 智能合约漏洞检测需要高精度（无假阳性）和可扩展性，特别是针对重入攻击等难以精确检测的漏洞。

Method: 基于博弈语义建模合约与环境交互，将外部合约推理转化为迹枚举，实现YulToolkit工具，支持Solidity编写的检测代码。

Result: 在DAO、PredyPool、Lendf.Me等真实漏洞案例中成功检测到已知漏洞，修复后无违规报告，验证了方法的有效性。

Conclusion: 有界博弈语义探索是智能合约分析工具箱的有效补充，特别适用于重入攻击等难以精确检测的漏洞类型。

Abstract: We present a new approach to finding smart contract vulnerabilities that is precise (no false positives up to our EVM-Yul interpreter), bounded-complete, and, when instrumented with domain knowledge, scales to real-world contracts. Our method is based on game semantics, modelling computation as an interaction between a contract and its environment, reducing reasoning about unknown or malicious external contracts to trace enumeration. We implement this in a tool we refer to as YulToolkit, a bounded game-semantics checker for Yul, the intermediate language of Solidity. By exploring only feasible interactions, YulToolkit avoids over-approximation, and by relying on the theory of game semantics it achieves bounded completeness. To make exploration tractable, YulToolkit supports instrumentation written in Solidity and propagated to Yul, comparable in effort to creating a test harness. Unlike tests, however, our technique explores all admissible traces within the chosen parameters and bounds. We evaluate YulToolkit on three real-world incidents: The DAO, PredyPool, and Lendf.Me, as well as benchmark contracts. In all cases, YulToolkit detects the known vulnerabilities (producing a violation-triggering trace), and after applying fixes, reports no further violations within bounds. These results show that bounded game semantics exploration is an effective and precise addition to the smart contract analysis toolbox, particularly for vulnerabilities such as reentrancy that are hard to detect precisely in real code.

</details>


### [4] [Compiling Gradual Types with Evidence](https://arxiv.org/abs/2512.22684)
*José Luis Romero,Cristóbal Isla,Matías Toro,Éric Tanter*

Main category: cs.PL

TL;DR: 本文提出并实现了一个基于证据的编译器GrEv，用于支持结构类型语言的渐进类型系统，性能可与基于强制转换的编译器竞争甚至更优。


<details>
  <summary>Details</summary>
Motivation: 渐进类型在结构类型语言中的高效实现具有挑战性。现有的基于强制转换的实现（如Grift）和基于证据的语义（AGT方法）之间存在差距，需要探索基于证据的语义是否能够实现高效的编译器实现。

Method: 设计、实现和评估基于证据的编译器GrEv，弥合形式语义与编译器实现之间的差距，提出新颖的单调语义，并在Grift基准测试套件上进行实证评估。

Result: GrEv编译器在性能上可与基于强制转换的编译器竞争甚至更快，在静态到动态的配置谱系中表现出更好的稳定性。

Conclusion: 基于证据的编译器是实现渐进类型的高效可行途径，为探索AGT方法中形式化推导的多种高级渐进类型系统的高效实现打开了大门。

Abstract: Efficiently supporting sound gradual typing in a language with structural types is challenging. To date, the Grift compiler is the only close-to-the-metal implementation of gradual typing in this setting, exploiting coercions for runtime checks, and further extended with monotonic references for efficient access to statically-typed data structures. On the language design and semantics side, the Abstracting Gradual Typing (AGT) methodology has proven fruitful to elucidate existing designs and to innovate by deriving gradualizations of a wide variety of typing disciplines and language features. Grounded in abstract interpretation, the Curry-Howard inspired runtime semantics of AGT is based on the notion of evidence for consistent judgments that evolve during reduction, monitoring the plausibility of well-typedness. While expressive and versatile, it is unclear whether such evidence-based semantics are a viable route to realize an efficient implementation of gradual typing.
  In this work, we explore this question by designing, implementing, and evaluating an evidence-based compiler, called GrEv. We explain how to bridge the gap between the formal semantics and the GrEv compiler implementation, and identify novel monotonic semantics. We empirically evaluate the performance of GrEv on the Grift benchmark suite. The results show that an evidence-based compiler can be competitive with, and even faster than, a coercion-based compiler, exhibiting more stability across configurations on the static-to-dynamic spectrum. In addition to enriching the space of gradual typing compilers, this work opens a direct door to exploring efficient implementations of the many advanced gradual typing disciplines formally derived with AGT in the literature.

</details>


### [5] [Fancy Some Chips for Your TeaStore? Modeling the Control of an Adaptable Discrete System](https://arxiv.org/abs/2512.23496)
*Anna Gallone,Simon Bliudze,Sophie Cerf,Olga Kouchnarenko*

Main category: cs.PL

TL;DR: Chips是一种建模语言，用于设计由各种交织组件组成的复杂系统，结合控制理论和通用编程语言概念，帮助开发者构建健壮的组件化模型。


<details>
  <summary>Details</summary>
Motivation: 开发者在设计新Web应用时需要处理软件、硬件、网络、在线微服务等多种资源的约束，这些实体形成复杂的通信依赖系统。为确保系统健壮性和服务质量，需要一种能简化这类复杂系统建模的方法。

Method: 提出Chips语言，采用功能块形式描述应用，融合控制理论和通用编程语言概念，支持系统化设计、建模和分析复杂系统项目，并以Adaptable TeaStore应用变体作为运行示例。

Result: Chips语言能够促进由各种交织组件组成的模型设计，通过功能块描述应用，生成健壮的组件化模型，为复杂系统项目提供系统化的设计、建模和分析方法。

Conclusion: Chips语言为复杂系统建模提供了有效工具，通过结合控制理论和编程语言概念，帮助开发者应对多资源约束，构建健壮的组件化应用模型。

Abstract: When designing new web applications, developers must cope with different kinds of constraints relative to the resources they rely on: software, hardware, network, online micro-services, or any combination of the mentioned entities. Together, these entities form a complex system of communicating interdependent processes, physical or logical. It is very desirable that such system ensures its robustness to provide a good quality of service. In this paper we introduce Chips, a language that aims at facilitating the design of models made of various entwined components. It allows the description of applications in the form of functional blocks. Chips mixes notions  from control theory and general purpose programming languages to generate robust component-based models. This paper presents how to use Chips to systematically design, model and analyse a complex system project, using a variation of the Adaptable TeaStore application as running example.

</details>


### [6] [Adaptable TeaStore: A Choreographic Approach](https://arxiv.org/abs/2512.23497)
*Giuseppe De Palma,Saverio Giallorenzo,Ivan Lanese,Gianluigi Zavattaro*

Main category: cs.PL

TL;DR: 使用AIOCJ编排语言实现Adaptable TeaStore，展示动态自适应微服务架构的构建方法，确保通信正确性


<details>
  <summary>Details</summary>
Motivation: Adaptable TeaStore作为可适应微服务架构的参考模型，需要一种能够确保通信正确性并支持运行时动态适应的实现方法

Method: 基于AIOCJ编排语言实现Adaptable TeaStore，利用编排编程确保通信正确性，支持运行时动态配置切换

Result: 成功实现Adaptable TeaStore，展示了AIOCJ在构建可适应微服务架构中的优势，同时识别了当前限制

Conclusion: AIOCJ为可适应微服务架构提供有前景的解决方案，但需要进一步改进以更好地对齐实际云架构需求

Abstract: The Adaptable TeaStore has recently been proposed as a reference model for adaptable microservice architectures. It includes different configurations, as well as scenarios requiring to transition between them. We describe an implementation of the Adaptable TeaStore based on AIOCJ, a choreographic language that allows one to program multiparty systems that can adapt at runtime to different conditions. Following the choreographic tradition, AIOCJ ensures by-construction correctness of communications (e.g., no deadlocks) before, during, and after adaptation. Adaptation is dynamic, and the adaptation scenarios need to be fully specified only at runtime. Using AIOCJ to model the Adaptable TeaStore, we showcase the strengths of the approach and its current limitations, providing suggestions for future directions for refining the paradigm (and the AIOCJ language, in particular), to better align it with real-world Cloud architectures.

</details>


### [7] [Beyond Per-Thread Lock Sets: Multi-Thread Critical Sections and Dynamic Deadlock Prediction](https://arxiv.org/abs/2512.23552)
*Martin Sulzmann*

Main category: cs.PL

TL;DR: 本文提出了一种改进的锁集构造方法，通过跨线程的临界区定义来减少死锁预测中的假阳性和假阴性。


<details>
  <summary>Details</summary>
Motivation: 传统的锁集构造只考虑同一线程内获取的锁，忽略了其他线程中的锁获取事件，这导致死锁预测中产生假阳性和假阴性。现有临界区概念的限制是根本原因。

Method: 提出基于追踪的临界区表征方法，允许临界区跨越多线程。通过偏序关系进行可靠近似，获得改进的锁集构造算法，可高效计算并集成到现有死锁预测工具中。

Result: 改进的锁集构造能够消除DIRK死锁预测器的假阳性，并减少SPDOffline死锁预测器的假阴性。扩展后的SPDOffline保持无假阳性的可靠性，同时提高了完整性（减少假阴性），且性能不受影响。

Conclusion: 跨线程的临界区概念是自然且正确的，改进的锁集构造方法能够显著提升死锁预测的准确性，同时保持计算效率和工具可靠性。

Abstract: Lock sets are commonly used for dynamic analysis of deadlocks. The standard per-thread lock set construction only considers locks acquired in the same thread, but is unaware of locks acquired in another thread. This leads to false positives and false negatives. The underlying issue is that the commonly used notion of a critical section on which the lock set construction relies ignores events from other threads. We give a trace-based characterization of critical sections that drops this restriction. Critical sections are no longer restricted to a single thread and can cover multiple threads. Such forms of critical sections exist, are natural, and correct the standard formulation.
  We show how to soundly approximate the trace-based characterization via partial order relations. Thus, we obtain an improved lock set construction that can still be efficiently computed and allows us to remove false positives reported by the DIRK deadlock predictor and remove false negatives by extending the SPDOffline deadlock predictor. We integrate various lock set constructions with increased precision in an extension of SPDOffline. Our extensions remain sound (no false positives) but are more complete (fewer false negatives) w.r.t. SPDOffline. For an extensive standard benchmark suite we can also show that the performance is not affected.

</details>


### [8] [Automating the Analysis of Parsing Algorithms (and other Dynamic Programs)](https://arxiv.org/abs/2512.23665)
*Tim Vieira,Ryan Cotterell,Jason Eisner*

Main category: cs.PL

TL;DR: 开发了一个帮助程序员分析NLP算法复杂度的系统，能够推断类型、死代码、冗余代码以及参数化的运行时和空间复杂度边界


<details>
  <summary>Details</summary>
Motivation: NLP研究中需要高效操作复杂的正式结构，算法设计者需要为算法提供保证（如运行时间或空间复杂度的上界），并确定算法导出量的必要属性以合成高效数据结构和验证类型错误

Method: 开发了一个系统来帮助程序员进行这些类型的分析

Result: 将系统应用于多个NLP算法，成功推断出类型、死代码、冗余代码以及参数化的运行时和空间复杂度边界

Conclusion: 该系统能够有效辅助NLP算法的复杂度分析和代码优化

Abstract: Much algorithmic research in NLP aims to efficiently manipulate rich formal structures. An algorithm designer typically seeks to provide guarantees about their proposed algorithm -- for example, that its running time or space complexity is upper-bounded as a certain function of its input size. They may also wish to determine the necessary properties of the quantities derived by the algorithm to synthesize efficient data structures and verify type errors. In this paper, we develop a system for helping programmers to perform these types of analyses. We apply our system to a number of NLP algorithms and find that it successfully infers types, dead and redundant code, and parametric runtime and space complexity bounds.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [9] [GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems](https://arxiv.org/abs/2512.22125)
*Jithin VG,Ditto PS*

Main category: cs.DC

TL;DR: GPU-Virt-Bench是一个全面的GPU虚拟化系统基准测试框架，用于评估软件虚拟化方案与硬件MIG的性能差异，包含56个性能指标和10个类别。


<details>
  <summary>Details</summary>
Motivation: GPU加速工作负载（特别是AI和LLM推理）的激增导致对云和容器环境中高效GPU资源共享的需求急剧增加。虽然NVIDIA的MIG技术提供硬件级隔离，但仅适用于高端数据中心GPU。软件虚拟化方案缺乏标准化评估方法。

Method: 开发了GPU-Virt-Bench基准测试框架，包含56个性能指标，组织成10个类别：开销、隔离质量、LLM特定性能、内存带宽、缓存行为、PCIe吞吐量、多GPU通信、调度效率、内存碎片和错误恢复。框架可系统比较软件虚拟化方案与理想MIG行为。

Result: 通过评估HAMi-core、BUD-FCSP和模拟MIG基线，展示了框架的实用性，揭示了生产部署决策所需的关键性能特征。

Conclusion: GPU-Virt-Bench为在多租户环境中部署GPU资源的从业者提供了可操作的见解，能够系统评估GPU虚拟化系统的性能，填补了软件虚拟化方案缺乏标准化评估方法的空白。

Abstract: The proliferation of GPU-accelerated workloads, particularly in artificial intelligence and large language model (LLM) inference, has created unprecedented demand for efficient GPU resource sharing in cloud and container environments. While NVIDIA's Multi-Instance GPU (MIG) technology provides hardware-level isolation, its availability is limited to high-end datacenter GPUs. Software-based virtualization solutions such as HAMi-core and BUD-FCSP offer alternatives for broader GPU families but lack standardized evaluation methodologies. We present GPU-Virt-Bench, a comprehensive benchmarking framework that evaluates GPU virtualization systems across 56 performance metrics organized into 10 categories. Our framework measures overhead, isolation quality, LLM-specific performance, memory bandwidth, cache behavior, PCIe throughput, multi-GPU communication, scheduling efficiency, memory fragmentation, and error recovery. GPU-Virt-Bench enables systematic comparison between software virtualization approaches and ideal MIG behavior, providing actionable insights for practitioners deploying GPU resources in multi-tenant environments. We demonstrate the framework's utility through evaluation of HAMi-core, BUD-FCSP, and simulated MIG baselines, revealing performance characteristics critical for production deployment decisions.

</details>


### [10] [SoDA: An Efficient Interaction Paradigm for the Agentic Web](https://arxiv.org/abs/2512.22135)
*Zicai Cui,Zhouyuan Jian,Weiwen Liu,Weinan Zhang*

Main category: cs.DC

TL;DR: 论文提出SoDA（主权数字化身）作为代理网络时代的新交互范式，通过存储、计算、交互正交解耦设计，解决数据锁定和认知过载问题，实现从"杀时间"到"省时间"的根本转变。


<details>
  <summary>Details</summary>
Motivation: 随着互联网从移动应用主导的注意力经济向代理网络的意图互联演进，现有交互模式无法解决日益严重的数据锁定和认知过载问题。需要建立面向未来的用户主权交互范式。

Method: 1. 提出主权数字化身（SoDA）架构，采用存储、计算、交互的正交解耦设计；2. 设计基于A2A协议的意图-权限握手机制，使用敏感系数和严格参数双因子自适应路由实现主动风险治理。

Result: 1. 跨平台服务迁移和复杂任务执行中减少约27-35%的token消耗；2. 多模态复杂任务编排中，相比标准RAG架构减少72%用户认知负载，相比手动工作流减少88%；3. 显著提升信息信噪比（SNR）。

Conclusion: SoDA是构建高效、低摩擦、去中心化代理网络的关键交互基础设施，通过将数据作为持久资产、模型作为临时工具，从根本上打破平台对用户记忆的垄断。

Abstract: As the internet evolves from the mobile App-dominated Attention Economy to the Intent-Interconnection of the Agentic Web era, existing interaction modes fail to address the escalating challenges of data lock-in and cognitive overload. Addressing this, we defines a future-oriented user sovereignty interaction paradigm, aiming to realize a fundamental shift from killing time to saving time. Specifically, we argue that decoupling memory from application logic eliminates the structural basis of data lock-in, while shifting from explicit manual instruction to implicit intent alignment resolves cognitive overload by offloading execution complexity. This paradigm is implemented via the Sovereign Digital Avatar (SoDA), which employs an orthogonal decoupling design of storage, computation, and interaction. This establishes the architectural principle of data as a persistent asset, model as a transient tool, fundamentally breaking the platform monopoly on user memory. To support the operation of this new paradigm in zero-trust environments, we design an Intent-Permission Handshake Mechanism based on A2A protocols, utilizing dual-factor (Sensitivity Coefficient and Strictness Parameter) adaptive routing to achieve active risk governance. Empirical evaluation with a high-fidelity simulation environment indicates that this paradigm reduces token consumption by approximately 27-35\% during cross-platform service migration and complex task execution. Furthermore, in the orchestration of multi-modal complex tasks, it reduces user cognitive load by 72\% compared to standard Retrieval-Augmented Generation (RAG) architectures, by 88\% relative to manual workflows, while significantly boosting the Information Signal-to-Noise Ratio (SNR). These results demonstrate that the SoDA is the essential interaction infrastructure for building an efficient, low-friction, and decentralized Agentic Web.

</details>


### [11] [SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware](https://arxiv.org/abs/2512.22136)
*Mahadev Sunil Kumar,Arnab Raha,Debayan Das,Gopakumar G,Amitava Mukherjee*

Main category: cs.DC

TL;DR: 提出一种针对分布式深度神经网络的高效部署方法，通过结构化剪枝和多目标优化，在满足硬件限制的同时保持任务性能，特别应用于MVCNN架构，实现1.2-5.0倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在计算机视觉中至关重要，但其在资源受限的边缘设备上部署面临参数多、计算需求大的挑战，需要平衡硬件限制和任务性能。

Method: 集成结构化模型剪枝和多目标优化，量化多视图CNN中各个视图对分类准确率的贡献，并相应分配剪枝预算，根据异构设备约束定制网络容量。

Result: 实验结果表明，生成的模型在满足用户指定的准确率和内存占用限制的同时，在不同硬件平台上实现了1.2-5.0倍的推理延迟降低。

Conclusion: 性能感知、视图自适应的压缩方法为在分布式边缘环境中部署复杂视觉模型提供了可行路径。

Abstract: Deep distributed networks (DNNs) have become central to modern computer vision, yet their deployment on resource-constrained edge devices remains hindered by substantial parameter counts and computational demands. Here, we present an approach to the efficient deployment of distributed DNNs that jointly respects hardware limitations and preserves task performance. Our method integrates a structured model pruning with a multi-objective optimization to tailor network capacity to heterogeneous device constraints. We demonstrate this framework using Multi-View Convolutional Neural Network (MVCNN), a state-of-the-art architecture for 3D object recognition, by quantifying the contribution of individual views to classification accuracy and allocating pruning budgets, respectively. Experimental results show that the resulting models satisfy user-specified bounds on accuracy and memory footprint while reducing inference latency by factors ranging from 1.2x to 5.0x across diverse hardware platforms. These findings suggest that performance-aware, view-adaptive compression provides a viable pathway for deploying complex vision models in distributed edge environments.

</details>


### [12] [HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA](https://arxiv.org/abs/2512.22139)
*Amur Saqib Pal,Muhammad Mohsin Ghaffar,Faisal Shafait,Christian Weis,Norbert Wehn*

Main category: cs.DC

TL;DR: HLS4PC：用于3D点云处理的FPGA加速框架，通过硬件感知压缩技术优化PointMLP模型，实现比GPU/CPU更高的吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有基于点的3D点云模型计算和内存密集，在GPU上执行效率低，难以满足安全关键应用的实时性需求，需要更高效的硬件加速方案

Method: 提出HLS4PC参数化HLS框架，利用FPGA并行化和算法优化实现定点映射和神经网络函数；通过硬件感知压缩技术（FPS替换为URS、参数量化、层融合、输入点剪枝）创建PointMLP-Lite轻量模型

Result: PointMLP-Lite复杂度降低4倍，在ModelNet40上仅损失2%精度；FPGA加速实现比先前工作高3.56倍吞吐量，比GPU和CPU分别高2.3倍和22倍

Conclusion: HLS4PC框架成功解决了3D点云处理的计算瓶颈，通过FPGA加速和模型压缩实现了高效的实时处理，为安全关键应用提供了可行解决方案

Abstract: Point-based 3D point cloud models employ computation and memory intensive mapping functions alongside NN layers for classification/segmentation, and are executed on server-grade GPUs. The sparse, and unstructured nature of 3D point cloud data leads to high memory and computational demand, hindering real-time performance in safety critical applications due to GPU under-utilization. To address this challenge, we present HLS4PC, a parameterizable HLS framework for FPGA acceleration. Our approach leverages FPGA parallelization and algorithmic optimizations to enable efficient fixed-point implementations of both mapping and NN functions. We explore several hardware-aware compression techniques on a state-of-the-art PointMLP-Elite model, including replacing FPS with URS, parameter quantization, layer fusion, and input-points pruning, yielding PointMLP-Lite, a 4x less complex variant with only 2% accuracy drop on ModelNet40. Secondly, we demonstrate that the FPGA acceleration of the PointMLP-Lite results in 3.56x higher throughput than previous works. Furthermore, our implementation achieves 2.3x and 22x higher throughput compared to the GPU and CPU implementations, respectively.

</details>


### [13] [HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration](https://arxiv.org/abs/2512.22137)
*Jiangwen Dong,Jiayu Li,Wanyu Lin*

Main category: cs.DC

TL;DR: HybridFlow是一个资源自适应的边缘-云协作推理框架，通过细粒度任务分解和并行执行来降低LLM推理延迟和token消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在边缘设备上部署面临高推理延迟和大量token消耗的问题，现有边缘-云协作方法采用粗粒度任务分配策略，无法充分利用细粒度推理并行性，导致冗余计算和资源利用效率低下。

Method: HybridFlow采用两阶段方法：1) 任务分解与并行执行，动态将复杂查询分解为相互依赖的子任务；2) 资源感知子任务路由，通过学习型路由器根据预测效用增益和实时预算状态自适应地将子任务分配给边缘或云模型。

Result: 在GPQA、MMLU-Pro、AIME和LiveBench-Reasoning等基准测试上的综合评估表明，HybridFlow能有效减少端到端推理时间和总体token使用量，同时保持有竞争力的准确性。

Conclusion: HybridFlow通过细粒度的边缘-云协作推理，实现了更高效的资源利用，为资源受限边缘设备上的LLM实时部署提供了有效解决方案。

Abstract: Large language models (LLMs) exhibit impressive reasoning and problem-solving abilities, yet their substantial inference latency and token consumption pose major challenges for real-time deployment on resource-limited edge devices. Recent efforts toward edge-cloud collaboration have attempted to mitigate this issue, but most existing methods adopt coarse-grained task allocation strategies-assigning entire queries either to the edge or the cloud. Such rigid partitioning fails to exploit fine-grained reasoning parallelism and often leads to redundant computation and inefficient resource utilization. To this end, we propose HybridFlow, a resource-adaptive inference framework that enables fast and token-efficient collaborative reasoning between edge and cloud LLMs. HybridFlow operates in two stages: (1) task decomposition and parallel execution, which dynamically splits a complex query into interdependent subtasks that can execute as soon as their dependencies are resolved; and (2) resource-aware subtask routing, where a learned router adaptively assigns each subtask to the edge or cloud model according to predicted utility gains and real-time budget states. Comprehensive evaluations on GPQA, MMLU-Pro, AIME, and LiveBench-Reasoning demonstrate that HybridFlow effectively reduces end-to-end inference time and overall token usage while maintaining competitive accuracy.

</details>


### [14] [TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures](https://arxiv.org/abs/2512.22168)
*Wei Li,Zhenyu Bai,Heru Wang,Pranav Dangi,Zhiqiang Zhang,Cheng Tan,Huiying Lan,Weng-Fai Wong,Tulika Mitra*

Main category: cs.DC

TL;DR: TL是一个端到端框架，用于将基于tile的程序（如Triton内核）编译到空间数据流架构上，解决了在分布式核心间分配tile实例和利用片上网络的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 空间数据流加速器通过显式的编译器管理数据移动和减少对全局共享内存的依赖，具有高性能和能效潜力，但其有限的可编程性阻碍了广泛采用。现有编译器框架主要关注单个tile内的代码优化，而缺乏在分布式核心间有效分配tile实例的能力。

Method: TL提出了一种硬件表示方法，捕捉互连拓扑、内存层次结构和计算能力，支持特定架构优化和多样化空间数据流目标。基于MLIR生态系统构建，定义了不同前端的通用入口点和不同后端的终点，专注于在空间分布式核心间分配tile实例并利用片上网络和分布式内存。

Result: TL框架能够编译基于tile的程序到空间数据流架构，通过增加数据重用和减少通信来提高性能，解决了空间数据流加速器的可编程性瓶颈问题。

Conclusion: TL提供了一个有效的端到端编译框架，解决了空间数据流加速器的可编程性挑战，通过硬件感知的优化和分布式tile映射，使这类架构更易于编程和部署。

Abstract: Spatial dataflow accelerators are a promising direction for next-generation computer systems because they can reduce the memory bottlenecks of traditional von Neumann machines such as CPUs and GPUs. They do so by organizing computation around explicit, compiler-managed data movement over the on-chip network, allowing operands to be directly forwarded between processing elements and reducing reliance on high-latency, bandwidth-limited global shared memory. Such localized communications can provide higher throughput and efficiency compared to repeated off-chip memory accesses. However, their end-to-end performance depends strongly on how workloads are mapped to the hardware. Naive mappings can perform very poorly, and most users rely on hand-tuned vendor libraries. In practice, although existing spatial-dataflow accelerators have strong potential for high performance, energy- and cost-efficiency, their limited programmability remains a major barrier to their wider adoption. This paper presents TL, an end-to-end framework that compiles tile-based programs (such as Triton kernels) onto spatial dataflow architectures. Unlike most existing compiler frameworks that focus on optimizing code generation within a single tile, TL addresses the central challenge of distributing tile instances across spatially distributed cores and exploiting the on-chip network and distributed memories to increase data reuse and reduce communications. TL proposes a hardware representation that captures interconnect topology, memory hierarchy, and compute capabilities, enabling both specialized architecture-specific optimizations and support for diverse spatial dataflow targets. TL is built on the MLIR ecosystem and defines a generic entry point for different front-ends and an end point for different back-ends.

</details>


### [15] [On Harnessing Idle Compute at the Edge for Foundation Model Training](https://arxiv.org/abs/2512.22142)
*Leyang Xue,Meghana Madhyastha,Myungjin Lee,Amos Storkey,Randal Burns,Mahesh K. Marina*

Main category: cs.DC

TL;DR: Cleave是一个用于在边缘设备上分布式训练基础模型的新范式，通过选择性混合张量并行和参数服务器框架，解决了现有边缘训练方法的性能、可扩展性、内存限制和通信开销问题。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型训练高度中心化，仅限于大型云数据中心，成本高昂。利用边缘设备空闲计算资源进行分布式训练是一个有前景的民主化替代方案，但现有边缘训练方法存在性能不足、可扩展性有限、内存超限、通信开销大等问题。

Method: 1. 提出选择性混合张量并行方法，精细划分训练操作；2. 采用参数服务器中心的训练框架，应对设备内存限制并避免通信瓶颈；3. 使用成本优化模型指导设备选择和训练工作负载分配，处理设备异构性和动态变化。

Result: 1. 匹配基于云的GPU训练性能，可扩展到更大模型和数千台设备；2. 支持比基线边缘训练方法多8倍的设备；3. 每批次训练时间比最先进的边缘训练方法快10倍；4. 高效处理设备故障，恢复速度比先前方法快至少100倍。

Conclusion: Cleave通过创新的张量并行划分和参数服务器框架，成功实现了在边缘设备上高效训练大型基础模型，解决了现有方法的局限性，为去中心化模型训练提供了可行的解决方案。

Abstract: The ecosystem behind foundation model development today is highly centralized and limited to large-scale cloud data center operators: training foundation models is costly, needing immense compute resources. Decentralized foundation model training across edge devices, leveraging their spare compute, promises a democratized alternative. However, existing edge-training approaches fall short: they struggle to match cloud-based training performance, exhibit limited scalability with model size, exceed device memory capacity, and have prohibitive communication overhead. They also fail to satisfactorily handle device heterogeneity and dynamism.
  We introduce a new paradigm, Cleave, which finely partitions training operations through a novel selective hybrid tensor parallelism method. Together with a parameter server centric training framework, Cleave copes with device memory limits and avoids communication bottlenecks, thereby enabling efficient training of large models on par with the cloud. Further, with a cost optimization model to guide device selection and training workload distribution, Cleave effectively accounts for device heterogeneity and churn.
  Our evaluations show that Cleave matches cloud-based GPU training by scaling efficiently to larger models and thousands of devices, supporting up to 8x more devices than baseline edge-training approaches. It outperforms state-of-the-art edge training methods by up to a factor of 10 in per-batch training time and efficiently handles device failures, achieving at least 100x faster recovery than prior methods.

</details>


### [16] [Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs](https://arxiv.org/abs/2512.22219)
*Xinhao Cheng,Zhihao Zhang,Yu Zhou,Jianan Ji,Jinchen Jiang,Zepeng Zhao,Ziruo Xiao,Zihao Ye,Yingyi Huang,Ruihang Lai,Hongyi Jin,Bohan Hou,Mengdi Wu,Yixin Dong,Anthony Yip,Zihao Ye,Songting Wang,Wenqin Yang,Xupeng Miao,Tianqi Chen,Zhihao Jia*

Main category: cs.DC

TL;DR: MPK是首个将多GPU模型推理自动转换为单个高性能megakernel的编译器和运行时系统，通过SM级图表示实现跨算子软件流水线等优化，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理系统通常采用算子级内核调度，存在内核启动开销、SM利用率不足等问题，无法充分利用GPU硬件潜力，需要更细粒度的优化方法。

Method: 提出SM级图表示捕获数据依赖，编译器将张量程序转换为SM级任务图并生成优化CUDA实现，运行时在单个megakernel内通过去中心化调度执行任务。

Result: MPK显著优于现有算子级LLM服务系统，端到端推理延迟降低高达1.7倍，将LLM推理性能推向硬件极限。

Conclusion: MPK实现了端到端内核融合，在保持编程模型灵活性的同时最小化开发者工作量，为多GPU模型推理提供了高效的编译和运行时解决方案。

Abstract: We introduce Mirage Persistent Kernel (MPK), the first compiler and runtime system that automatically transforms multi-GPU model inference into a single high-performance megakernel. MPK introduces an SM-level graph representation that captures data dependencies at the granularity of individual streaming multiprocessors (SMs), enabling cross-operator software pipelining, fine-grained kernel overlap, and other previously infeasible GPU optimizations. The MPK compiler lowers tensor programs into highly optimized SM-level task graphs and generates optimized CUDA implementations for all tasks, while the MPK in-kernel parallel runtime executes these tasks within a single mega-kernel using decentralized scheduling across SMs. Together, these components provide end-to-end kernel fusion with minimal developer effort, while preserving the flexibility of existing programming models. Our evaluation shows that MPK significantly outperforms existing kernel-per-operator LLM serving systems by reducing end-to-end inference latency by up to 1.7x, pushing LLM inference performance close to hardware limits. MPK is publicly available at https://github.com/mirage-project/mirage.

</details>


### [17] [GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs](https://arxiv.org/abs/2512.22147)
*Ruifan Chu,Anbang Wang,Xiuxiu Bai,Shuai Liu,Xiaoshe Dong*

Main category: cs.DC

TL;DR: 提出一个端到端LLM框架，通过构建最小可执行程序(MEP)来优化GPU热点内核，无需完整应用构建，实现跨平台性能提升


<details>
  <summary>Details</summary>
Motivation: 高性能计算中GPU热点内核是主要瓶颈，专家手动调优成本高且难以移植。现有LLM方法假设内核可以廉价编译执行，但在大型应用中完整构建和运行成本高昂

Method: 从独立提取的热点内核自动构建最小可执行程序(MEP)，进行多轮迭代优化评估，集成自动错误修复和性能模式继承来修复故障、保持正确性、重用有效的分块/内存/同步策略

Result: 在NVIDIA GPU和华为DCU平台上评估，平均加速比：PolyBench在NVIDIA上5.05x，在DCU上7.77x，AMD APP SDK上1.77x，三个热点内核上1.25x，超越直接LLM优化

Conclusion: 该方法无需完整源代码依赖，提供跨平台可移植性，实现了实用、低成本的GPU内核优化，为大规模高性能计算应用提供了有效的自动化优化方案

Abstract: In high-performance computing, hotspot GPU kernels are primary bottlenecks, and expert manual tuning is costly and hard to port. Large language model methods often assume kernels can be compiled and executed cheaply, which fails in large applications where full builds and runs are expensive. We present an end-to-end LLM framework with performance feedback that optimizes kernels without building the full application. From independently extracted hotspot kernels, it automatically completes code into a Minimal Executable Program (MEP), then performs multi-round iterative optimization and evaluation outside the full application. The framework integrates Automatic Error Repair and Performance Pattern Inheritance to fix faults, preserve correctness, reuse effective tiling/memory/synchronization strategies, and reduce search cost. Optimized variants are reintegrated into the original application for validation. We evaluate on NVIDIA GPUs and the Haiguang Deep Computing Unit (DCU) platform (AMD-licensed architecture) using PolyBench, the AMD APP SDK, and hotspot kernels from large-scale supercomputing applications. The method achieves average speedups of 5.05x (PolyBench on NVIDIA), 7.77x (PolyBench on DCU), 1.77x (AMD APP SDK), and 1.25x on three hotspot kernels, surpassing direct LLM optimization. The approach requires no full-source dependencies, offers cross-platform portability, and enables practical, low-cost GPU kernel optimization.

</details>


### [18] [Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments](https://arxiv.org/abs/2512.22149)
*Guilin Zhang,Wulan Guo,Ziqi Tan*

Main category: cs.DC

TL;DR: 提出自适应GPU资源分配框架，在服务器无GPU平台上为多智能体系统实现85%延迟降低，同时保持与静态分配相当的吞吐量


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在服务器无GPU平台部署面临资源分配挑战：异构智能体工作负载、不同计算需求、需要成本效益扩展

Method: 自适应GPU资源分配框架，基于工作负载特征、智能体优先级和最小资源需求动态分配GPU资源，使用O(N)复杂度算法实时适应

Result: 相比轮询调度减少85%延迟，吞吐量与静态分配相当；在延迟、成本和GPU利用率指标上优于静态平均和轮询策略

Conclusion: 该框架为在服务器无GPU基础设施上部署成本效益的多智能体AI系统提供了实用解决方案

Abstract: Multi-agent systems powered by large language models have emerged as a promising paradigm for solving complex reasoning tasks through collaborative intelligence. However, efficiently deploying these systems on serverless GPU platforms presents significant resource allocation challenges due to heterogeneous agent workloads, varying computational demands, and the need for cost-effective scaling. This paper presents an adaptive GPU resource allocation framework that achieves 85\% latency reduction compared to round-robin scheduling while maintaining comparable throughput to static allocation, using an $O(N)$ complexity algorithm for real-time adaptation. Our approach dynamically allocates GPU resources based on workload characteristics, agent priorities, and minimum resource requirements, enabling efficient utilization while maintaining quality of service. The framework addresses three key challenges: (1) heterogeneous computational demands across lightweight coordinators and heavyweight specialists, (2) dynamic workload fluctuations requiring millisecond-scale reallocation, and (3) capacity constraints in serverless environments. Through comprehensive simulations modeling realistic multi-agent workflows with four heterogeneous agents, we demonstrate that adaptive allocation outperforms static equal and round-robin strategies across latency, cost, and GPU utilization metrics. The framework provides a practical solution for deploying cost-efficient multi-agent AI systems on serverless GPU infrastructure.

</details>


### [19] [AiiDAlab: on the route to accelerate science](https://arxiv.org/abs/2512.22173)
*Aliaksandr V. Yakutovich,Jusong Yu,Daniel Hollas,Edan Bainglass,Corsin Battaglia,Miki Bonacci,Lucas Fernandez Vilanova,Stephan Henne,Anders Kaestner,Michel Kenzelmann,Graham Kimbell,Jakob Lass,Fabio Lopes,Daniel G. Mazzone,Andres Ortega-Guerrero,Xing Wang,Nicola Marzari,Carlo A. Pignedoli,Giovanni Pizzi*

Main category: cs.DC

TL;DR: AiiDAlab平台通过网页界面简化复杂计算工作流，支持多学科研究，自动追踪模拟溯源确保可重复性，并集成电子实验笔记本促进FAIR原则。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力增长，需要自动化工作流来管理大规模模拟，但执行这些工作流需要技术专业知识来设置输入、解释输出和处理远程并行计算。AiiDAlab旨在通过直观的网页界面使复杂计算工作流更易访问。

Method: 开发AiiDAlab平台，提供基于网页浏览器的用户界面，底层使用AiiDA引擎自动追踪完整的模拟溯源。平台简化用户入门流程，优化计算资源访问，提供处理大数据集的机制，并与电子实验笔记本集成。

Result: AiiDAlab已从计算材料科学扩展到多学科应用，包括量子化学、大气建模、电池研究和大规模设施实验数据分析，同时用于教育场景。平台让科学家专注于研究而非计算细节，确保可重复性并支持生成可重复的开放研究数据。

Conclusion: AiiDAlab已成为加速多学科科学发现的强大平台，通过简化复杂计算工作流、自动追踪溯源和集成电子实验笔记本，支持研究人员遵循FAIR原则并生成可重复的开放研究数据。

Abstract: With the availability of ever-increasing computational capabilities, robust and automated research workflows are essential to enable and facilitate the execution and orchestration of large numbers of interdependent simulations in supercomputer facilities. However, the execution of these workflows still typically requires technical expertise in setting up calculation inputs, interpreting outputs, and handling the complexity of parallel code execution on remote machines. To address these challenges, the AiiDAlab platform was developed, making complex computational workflows accessible through an intuitive user interface that runs in a web browser. Here, we discuss how AiiDAlab has matured over the past few years, shifting its focus from computational materials science to become a powerful platform that accelerates scientific discovery across multiple disciplines. Thanks to its design, AiiDAlab allows scientists to focus on their research rather than on computational details and challenges, while keeping automatically track of the full simulation provenance via the underlying AiiDA engine and thus ensuring reproducibility. In particular, we discuss its adoption into quantum chemistry, atmospheric modeling, battery research, and even experimental data analysis at large-scale facilities, while also being actively used in educational settings. Driven by user feedback, significant effort has been made to simplify user onboarding, streamline access to computational resources, and provide robust mechanisms to work with large datasets. Furthermore, AiiDAlab is being integrated with electronic laboratory notebooks (ELNs), reinforcing adherence to the FAIR principles and supporting researchers in data-centric scientific disciplines in easily generating reproducible Open Research Data (ORD).

</details>


### [20] [BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs](https://arxiv.org/abs/2512.22174)
*Muhammad Zeeshan Karamat,Sadman Saif,Christiana Chamon Garcia*

Main category: cs.DC

TL;DR: BitFlipScope是一个软件框架，用于定位Transformer架构中的比特翻转故障区域，支持有/无参考模型两种场景，并能实现轻量级性能恢复。


<details>
  <summary>Details</summary>
Motivation: LLMs在安全关键环境中部署时，容易受到硬件退化、宇宙辐射或Rowhammer攻击等引起的比特翻转故障影响。这些故障会静默地破坏内部参数，导致不可预测或危险的行为。定位这些损坏对于诊断问题、应用针对性修复措施至关重要。

Method: BitFlipScope采用两种方法：1）有干净参考模型时，通过输出、隐藏状态和内部激活的差异分析来检测异常行为；2）无参考模型时，使用残差路径扰动和损失敏感性分析直接从损坏模型中推断故障影响区域。

Result: 该框架不仅能有效诊断故障，还支持无需微调的轻量级性能恢复，为恢复损坏模型提供了实用路径。

Conclusion: BitFlipScope是实现可信赖、容错LLM部署在硬件易错和对抗环境中的重要一步。

Abstract: Large Language Models (LLMs) deployed in practical and safety-critical settings are increasingly susceptible to bit-flip faults caused by hardware degradation, cosmic radiation, or deliberate fault-injection attacks such as Rowhammer. These faults silently corrupt internal parameters and can lead to unpredictable or dangerous model behavior. Localizing these corruptions is essential: without identifying the affected region, it is impossible to diagnose the source of degradation, apply targeted corrective measures, or restore model functionality without resorting to costly fine-tuning or full retraining. This work introduces BitFlipScope, a scalable, software-based framework for identifying fault-affected regions within transformer architectures under two deployment scenarios. When a clean reference model is available, BitFlipScope performs differential analysis of outputs, hidden states, and internal activations for detecting anomalous behavior indicative of corruption to pinpoint or localize faults. When no reference model exists, it uses residual-path perturbation and loss-sensitivity profiling to infer the fault-impacted region directly from the corrupted model. In both settings, the framework not only enables effective fault diagnosis but also supports lightweight performance recovery without fine-tuning, offering a practical path to restoring corrupted models. Together, these capabilities make BitFlipScope an important step toward trustworthy, fault-resilient LLM deployment in hardware-prone and adversarial environments.

</details>


### [21] [iOS as Acceleration](https://arxiv.org/abs/2512.22180)
*Alexander K. Chen*

Main category: cs.DC

TL;DR: 利用iOS手机作为分布式计算节点，通过流水线并行技术增强本地机器学习计算能力，无需额外成本


<details>
  <summary>Details</summary>
Motivation: 大规模机器学习需要强大计算资源，但本地环境受限。虽然云计算可解决此问题，但涉及隐私数据、物理环境不可用或成本过高时仍需本地计算。移动手机作为普遍但未充分利用的资源，具有改善本地计算环境的潜力。

Method: 提出概念验证系统，利用iOS设备通过分布式流水线并行技术，克服内存限制、热节流和OS沙盒等限制，加速模型训练、批量推理和智能工具使用。

Result: 在较弱计算环境中实现了显著性能提升，展示了iOS设备作为分布式计算节点的可行性，为机器学习提供了新的计算资源。

Conclusion: 移动设备具有为机器学习做出更大贡献的潜力，本文提出的方法为利用普遍存在的移动设备资源提供了新思路，讨论了实际用例、限制和未来研究方向。

Abstract: Practical utilization of large-scale machine learning requires a powerful compute setup, a necessity which poses a significant barrier to engagement with such artificial intelligence in more restricted system environments. While cloud computing offers a solution to weaker local environments, certain situations like training involving private or sensitive data, physical environments not available through the cloud, or higher anticipated usage costs, necessitate computing locally. We explore the potential to improve weaker local compute systems at zero additional cost by taking advantage of ubiquitous yet underutilized resources: mobile phones. Specifically, recent iOS phones are equipped with surprisingly powerful processors, but they also face limitations like memory constraints, thermal throttling, and OS sandboxing. We present a proof-of-concept system demonstrating a novel approach to harness an iOS device via distributed pipeline parallelism, achieving significant benefits in a lesser compute environment by accelerating modest model training, batch inference, and agentic LRM tool-usage. We discuss practical use-cases, limitations, and directions for future work. The findings of this paper highlight the potential for the improving commonplace mobile devices to provide greater contributions to machine learning.

</details>


### [22] [MatKV: Trading Compute for Flash Storage in LLM Inference](https://arxiv.org/abs/2512.22195)
*Kun-Woo Shin,Jay H. Park,Moonwook Oh,Yohan Jo,Jaeyoung Do,Sang-Won Lee*

Main category: cs.DC

TL;DR: MatKV通过预计算RAG文档的KV向量并存储在闪存中，在推理时直接复用，显著降低推理时间和能耗


<details>
  <summary>Details</summary>
Motivation: LLM推理成本已超过训练成本，RAG在处理长文本时prefill阶段计算KV向量能耗高、耗时长，需要优化

Method: 预计算RAG对象的KV向量，将结果物化存储在快速、低功耗的闪存中，推理时直接加载复用而非重新计算

Result: 相比GPU全量计算，MatKV将RAG推理时间和能耗降低一半，问答任务准确率影响不大，并支持GPU解码与KV加载并行、低端GPU解码等优化

Conclusion: MatKV使大规模生成式AI应用更经济、高效、可访问，适用于更广泛的任务和硬件环境

Abstract: We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing the key-value vectors of input text is energy-intensive and time-consuming even with high-end GPUs. Thus, it is crucial to make the prefill phase in RAG inference efficient. To address this issue, we propose MatKV, a scheme that precomputes the key-value vectors (KVs) of RAG objects (e.g., documents), materializes them in inexpensive but fast and power-efficient flash storage, and reuses them at inference time instead of recomputing the KVs using costly and power-inefficient GPU. Experimental results using Hugging Face's Transformers library across state-of-the-art GPUs and flash memory SSDs confirm that, compared to full KV computation on GPUs, MatKV reduces both inference time and power consumption by half for RAG workloads, without severely impacting accuracy in the question-answering task. Furthermore, we demonstrate that MatKV enables additional optimizations in two ways. First, a GPU can decode text while simultaneously loading the materialized KVs for the next instance, reducing load latency. Second, since decoding speed is less sensitive to GPU performance than KV computation, low-end GPUs can be leveraged for decoding without significantly compromising speed once the materialized KVs are loaded into GPU memory. These findings underscore MatKV's potential to make large-scale generative AI applications more cost-effective, power-efficient, and accessible across a wider range of tasks and hardware environments.

</details>


### [23] [SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM](https://arxiv.org/abs/2512.22215)
*Simone Bnà,Giuseppe Giaquinto,Ettore Fadiga,Tommaso Zanelli,Francesco Bottau*

Main category: cs.DC

TL;DR: SPUMA实现了OPENFOAM在NVIDIA和AMD GPU上的完整移植，采用可移植编程模型和内存池管理，在LUMI和Leonardo集群上展示了良好的强扩展性（65%效率）、弱扩展性（75-85%效率）以及高达82%的能耗降低。


<details>
  <summary>Details</summary>
Motivation: 尽管GPU在HPC中广泛应用，但在开源CFD软件（如OPENFOAM）中的可编程性仍然面临挑战。现代加速器为CFD计算提供了重要机遇，但需要解决跨平台GPU编程的难题。

Method: 基于可移植编程模型，实现OPENFOAM在NVIDIA和AMD GPU上的完整移植。采用内存池管理器，利用现代GPU的统一内存特性，优化内存使用和性能。

Result: 在LUMI（AMD MI250X）和Leonardo（NVIDIA A100）集群上测试：强扩展性达到65%效率（每GPU约800万网格）；弱扩展性在20个GPU上达到75-85%效率；使用NVIDIA AmgX时效率不低于90%；一个A100 GPU相当于200-300个Intel Sapphire Rapids核心；能耗降低高达82%。

Conclusion: SPUMA成功实现了OPENFOAM在异构GPU集群上的高性能计算，展示了良好的可移植性、扩展性和能效优势，为开源CFD软件在GPU加速计算中的应用提供了有效解决方案。

Abstract: High Performance Computing (HPC) on hybrid clusters represents a significant opportunity for Computational Fluid Dynamics (CFD), especially when modern accelerators are utilized effectively. However, despite the widespread adoption of GPUs, programmability remains a challenge, particularly in open-source contexts. In this paper, we present SPUMA, a full GPU porting of OPENFOAM targeting NVIDIA and AMD GPUs. The implementation strategy is based on a portable programming model and the adoption of a memory pool manager that leverages the unified memory feature of modern GPUs. This approach is discussed alongside several numerical tests conducted on two pre-exascale clusters in Europe, LUMI and Leonardo, which host AMD MI250X and NVIDIA A100 GPUs, respectively. In the performance analysis section, we present results related to memory usage profiling and kernel wall-time, the impact of the memory pool, and energy consumption obtained by simulating the well-known DrivAer industrial test case. GPU utilization strongly affects strong scalability results, reaching 65% efficiency on both LUMI and Leonardo when approaching a load of 8 million cells per GPU. Weak scalability results, obtained on 20 GPUs with the OpenFOAM native multigrid solver, range from 75% on Leonardo to 85% on LUMI. Notably, efficiency is no lower than 90% when switching to the NVIDIA AmgX linear algebra solver. Our tests also reveal that one A100 GPU on Leonardo is equivalent 200-300 Intel Sapphire Rapids cores, provided the GPUs are sufficiently oversubscribed (more than 10 million of cells per GPU). Finally, energy consumption is reduced by up to 82% compared to analogous simulations executed on CPUs.

</details>


### [24] [Scalable Cloud-Native Architectures for Intelligent PMU Data Processing](https://arxiv.org/abs/2512.22231)
*Nachiappan Chockalingam,Akshay Deshpande,Lokesh Butra,Ram Sekhar Bodala,Nitin Saksena,Adithya Parthasarathy,Balakrishna Pothineni,Akash Kumar Agarwal*

Main category: cs.DC

TL;DR: 提出基于云原生架构的智能PMU数据处理方案，结合边缘计算与AI技术，解决大规模PMU部署中的延迟、可扩展性和可靠性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着PMU部署规模扩大，传统集中式处理架构难以应对高频同步数据的实时处理需求，特别是在动态运行条件下的现代电网中，存在延迟、可扩展性和可靠性等挑战。

Method: 采用云原生架构，整合分布式流处理、容器化微服务和弹性资源编排，结合机器学习模型进行时间序列分析，实现低延迟数据摄取、实时异常检测和高级分析。

Result: 通过分析模型评估系统延迟、吞吐量和可靠性，证明该架构可实现亚秒级响应时间，并能扩展到大规模PMU部署，同时内置安全隐私机制支持关键基础设施部署。

Conclusion: 提出的云原生架构为下一代智能电网分析提供了强大灵活的基础，能够有效应对大规模PMU数据处理挑战，提升电网可观测性和预测能力。

Abstract: Phasor Measurement Units (PMUs) generate high-frequency, time-synchronized data essential for real-time power grid monitoring, yet the growing scale of PMU deployments creates significant challenges in latency, scalability, and reliability. Conventional centralized processing architectures are increasingly unable to handle the volume and velocity of PMU data, particularly in modern grids with dynamic operating conditions. This paper presents a scalable cloud-native architecture for intelligent PMU data processing that integrates artificial intelligence with edge and cloud computing. The proposed framework employs distributed stream processing, containerized microservices, and elastic resource orchestration to enable low-latency ingestion, real-time anomaly detection, and advanced analytics. Machine learning models for time-series analysis are incorporated to enhance grid observability and predictive capabilities. Analytical models are developed to evaluate system latency, throughput, and reliability, showing that the architecture can achieve sub-second response times while scaling to large PMU deployments. Security and privacy mechanisms are embedded to support deployment in critical infrastructure environments. The proposed approach provides a robust and flexible foundation for next-generation smart grid analytics.

</details>


### [25] [Efficient Multi-Model Orchestration for Self-Hosted Large Language Models](https://arxiv.org/abs/2512.22402)
*Bhanu Prakash Vangala,Tanu Malik*

Main category: cs.DC

TL;DR: Pick and Spin是一个基于Kubernetes的LLM编排框架，通过统一部署、自适应缩容和混合路由策略，实现自托管大语言模型的经济高效管理，相比静态部署提升21.6%成功率、降低30%延迟和33%GPU成本。


<details>
  <summary>Details</summary>
Motivation: 组织自托管大语言模型面临GPU利用率低、工作负载路由困难和可靠性挑战，需要一种经济高效的解决方案来平衡隐私、成本控制和定制化需求。

Method: 基于Kubernetes构建，包含统一Helm部署系统、自适应缩容自动化，以及使用关键词启发式和轻量级DistilBERT分类器的混合路由模块，平衡成本、延迟和准确性。

Result: 在4个模型（Llama-3 90B、Gemma-3 27B、Qwen-3 235B、DeepSeek-R1 685B）上评估，涵盖8个公共基准数据集、5种推理策略和2种路由变体，总计31,019个提示和163,720次推理运行。相比静态部署，成功率高21.6%，延迟低30%，每查询GPU成本低33%。

Conclusion: Pick and Spin框架使自托管LLM编排具有可扩展性和经济性，通过智能路由和资源优化显著提升部署效率，为组织自托管大模型提供了实用解决方案。

Abstract: Self-hosting large language models (LLMs) is increasingly appealing for organizations seeking privacy, cost control, and customization. Yet deploying and maintaining in-house models poses challenges in GPU utilization, workload routing, and reliability. We introduce Pick and Spin, a practical framework that makes self-hosted LLM orchestration scalable and economical. Built on Kubernetes, it integrates a unified Helm-based deployment system, adaptive scale-to-zero automation, and a hybrid routing module that balances cost, latency, and accuracy using both keyword heuristics and a lightweight DistilBERT classifier. We evaluate four models, Llama-3 (90B), Gemma-3 (27B), Qwen-3 (235B), and DeepSeek-R1 (685B) across eight public benchmark datasets, with five inference strategies, and two routing variants encompassing 31,019 prompts and 163,720 inference runs. Pick and Spin achieves up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query compared with static deployments of the same models.

</details>


### [26] [Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving](https://arxiv.org/abs/2512.22420)
*Rui Li,Zhaoning Zhang,Libo Zhang,Huaimin Wang,Xiang Fu,Zhiquan Lai*

Main category: cs.DC

TL;DR: Nightjar是一种基于学习的自适应推测解码算法，能根据请求负载动态调整推测长度，在实时服务中显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码方法使用固定的推测长度，无法适应动态请求速率，在高负载计算受限环境中因验证开销导致性能下降，这在现实服务场景中造成显著性能瓶颈。

Method: 提出Nightjar算法，这是一种基于学习的自适应推测推理方法，能根据请求负载动态选择最优推测长度，甚至在没有收益时完全禁用推测解码。

Result: 实验表明，Nightjar相比标准推测解码实现了高达14.8%的吞吐量提升和20.2%的延迟降低，在实时服务中展现出强大的效率优势。

Conclusion: Nightjar通过自适应调整推测长度解决了传统推测解码在高负载环境中的性能瓶颈问题，为LLM推理服务提供了更高效的解决方案。

Abstract: Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.

</details>


### [27] [Role-Based Fault Tolerance System for LLM RL Post-Training](https://arxiv.org/abs/2512.22492)
*Zhenqian Chen,Baoquan Zhong,Xiang Li,Qing Dai,Xinkui Zhao,Miao Ye,Ren Cheng,Lufei Zhang,Jianwei Yin*

Main category: cs.DC

TL;DR: RobustRL：首个针对LLM RL后训练的容错系统，通过角色隔离、非中断恢复和动态重连，在GPU故障下保持高效训练


<details>
  <summary>Details</summary>
Motivation: RL后训练混合了训练和推理工作负载，现有容错框架只针对单一场景，无法充分利用异步执行的优化潜力，需要专门的容错解决方案

Method: 1) 角色感知监控区分真实故障与角色特定行为；2) 非中断恢复：训练器通过rollout热备快速恢复，rollout进行隔离机器替换；3) 动态重连：用UCX点对点通信替代静态集合通信

Result: 在256-GPU集群上，Qwen3-8B-Math任务，10%故障注入频率下，RobustRL的ETTR超过80%（ByteRobust为60%），端到端训练时间快8.4%-17.4%

Conclusion: RobustRL通过角色隔离、快速恢复和动态重连机制，显著提升了RL后训练在GPU故障下的容错能力和训练效率

Abstract: RL post-training for LLMs has been widely scaled to enhance reasoning and tool-using capabilities. However, RL post-training interleaves training and inference workloads, exposing the system to faults from both sides. Existing fault tolerance frameworks for LLMs target either training or inference, leaving the optimization potential in the asynchronous execution unexplored for RL. Our key insight is role-based fault isolation so the failure in one machine does not affect the others. We treat trainer, rollout, and other management roles in RL training as distinct distributed sub-tasks. Instead of restarting the entire RL task in ByteRobust, we recover only the failed role and reconnect it to living ones, thereby eliminating the full-restart overhead including rollout replay and initialization delay.
  We present RobustRL, the first comprehensive robust system to handle GPU machine errors for RL post-training Effective Training Time Ratio improvement. (1) \textit{Detect}. We implement role-aware monitoring to distinguish actual failures from role-specific behaviors to avoid the false positive and delayed detection. (2) \textit{Restart}. For trainers, we implement a non-disruptive recovery where rollouts persist state and continue trajectory generation, while the trainer is rapidly restored via rollout warm standbys. For rollout, we perform isolated machine replacement without interrupting the RL task. (3) \textit{Reconnect}. We replace static collective communication with dynamic, UCX-based (Unified Communication X) point-to-point communication, enabling immediate weight synchronization between recovered roles. In an RL training task on a 256-GPU cluster with Qwen3-8B-Math workload under 10\% failure injection frequency, RobustRL can achieve an ETTR of over 80\% compared with the 60\% in ByteRobust and achieves 8.4\%-17.4\% faster in end-to-end training time.

</details>


### [28] [Object Abstraction To Streamline Edge-Cloud-Native Application Development](https://arxiv.org/abs/2512.22534)
*Pawissanutt Lertpongrujikorn*

Main category: cs.DC

TL;DR: 该论文提出Object-as-a-Service (OaaS)范式，通过统一资源、状态和工作流管理解决云原生开发中的碎片化问题，显著降低基础设施复杂度并提升开发效率。


<details>
  <summary>Details</summary>
Motivation: 尽管云计算已经改变了应用开发方式，但无服务器计算在函数运行时、状态管理和编排方面的碎片化问题仍然存在，导致基础设施复杂度高，影响开发者的生产力和创新。

Method: 基于三项实证研究：从业者访谈（21人）、开发者体验研究（39人）和客户发现（101次访谈），提出OaaS范式，开发Oparaca原型，并扩展至边缘计算（EdgeWeaver）。

Result: OaaS原型展示可忽略的开销和最优可扩展性；OaaS-IoT实现任务完成速度提升31%，代码行数减少44.5%；建立了针对技术型中小企业和初创企业的商业化路径。

Conclusion: OaaS通过统一碎片化抽象和自动化性能优化，为云原生平台奠定基础，隐藏基础设施复杂度，使开发者能够专注于创新，并提供了基于实证需求验证的技术研究方法论。

Abstract: Cloud computing has fundamentally transformed application development, yet a gap remains between the serverless promise of simplified deployment and its practical realization due to fragmentation across function runtimes, state management, and orchestration. This dissertation addresses this gap through empirical validation and technical innovation, establishing the Object-as-a-Service (OaaS) paradigm as a unified approach to cloud-native development. Grounded in evidence from three studies - practitioner interviews (21 participants), a human study on developer experience (39 participants), and NSF I-Corps customer discovery (101 interviews across 86 organizations) - this work demonstrates that infrastructure complexity taxes productivity, with practitioners prioritizing automation and maintainability over cost optimization. The dissertation makes five major contributions: (1) the OaaS paradigm unifies resource, state, and workflow management via the Oparaca prototype, demonstrating negligible overhead and state-of-the-art scalability; (2) SLA-driven OaaS enables declarative management of non-functional requirements like availability, consistency, and latency; (3) OaaS-IoT with EdgeWeaver extends the paradigm to the edge-cloud continuum, achieving 31% faster task completion and a 44.5% reduction in lines of code compared to traditional FaaS; (4) commercialization validation establishes a pathway targeting technology SMEs and startups; and (5) an empirical methodology for grounding technical research in validated practitioner needs. By consolidating fragmented abstractions and automating performance optimization, OaaS establishes a foundation for cloud-native platforms that hide infrastructure complexity and empower developers to focus on innovation.

</details>


### [29] [RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure](https://arxiv.org/abs/2512.22560)
*Wei Gao,Yuheng Zhao,Tianyuan Wu,Shaopan Xiong,Weixun Wang,Dakai An,Lunxi Cao,Dilxat Muhtar,Zichen Liu,Haizhou Zhao,Ju Huang,Siran Yang,Yongbin Li,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: RollArc是一个分布式系统，用于在解耦基础设施上最大化多任务智能体强化学习的训练吞吐量，通过硬件亲和性工作负载映射、细粒度异步和状态感知计算实现1.35-2.05倍的训练时间减少。


<details>
  <summary>Details</summary>
Motivation: 智能体强化学习工作负载具有高度异构性，结合了计算密集型预填充阶段、带宽受限的解码和有状态的CPU密集型环境模拟。虽然解耦基础设施可以利用专业化硬件，但简单的解耦会因阶段间复杂依赖关系引入显著的同步开销和资源利用率不足问题。

Method: RollArc基于三个核心原则构建：1) 硬件亲和性工作负载映射，将计算受限和带宽受限任务路由到最适合的GPU设备；2) 细粒度异步，在轨迹级别管理执行以减少资源气泡；3) 状态感知计算，将无状态组件（如奖励模型）卸载到无服务器基础设施以实现弹性扩展。

Result: RollArc有效提高了训练吞吐量，相比整体式和同步基线实现了1.35-2.05倍的端到端训练时间减少。在阿里巴巴集群上使用3000多块GPU训练数百亿参数的MoE模型，进一步证明了RollArc的可扩展性和鲁棒性。

Conclusion: RollArc通过解耦基础设施上的智能工作负载管理和异步执行，成功解决了智能体强化学习训练中的异构工作负载挑战，显著提升了训练效率和可扩展性。

Abstract: Agentic Reinforcement Learning (RL) enables Large Language Models (LLMs) to perform autonomous decision-making and long-term planning. Unlike standard LLM post-training, agentic RL workloads are highly heterogeneous, combining compute-intensive prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations. We argue that efficient agentic RL training requires disaggregated infrastructure to leverage specialized, best-fit hardware. However, naive disaggregation introduces substantial synchronization overhead and resource underutilization due to the complex dependencies between stages.
  We present RollArc, a distributed system designed to maximize throughput for multi-task agentic RL on disaggregated infrastructure. RollArc is built on three core principles: (1) hardware-affinity workload mapping, which routes compute-bound and bandwidth-bound tasks to bestfit GPU devices, (2) fine-grained asynchrony, which manages execution at the trajectory level to mitigate resource bubbles, and (3) statefulness-aware computation, which offloads stateless components (e.g., reward models) to serverless infrastructure for elastic scaling. Our results demonstrate that RollArc effectively improves training throughput and achieves 1.35-2.05\(\times\) end-to-end training time reduction compared to monolithic and synchronous baselines. We also evaluate RollArc by training a hundreds-of-billions-parameter MoE model for Qoder product on an Alibaba cluster with more than 3,000 GPUs, further demonstrating RollArc scalability and robustness. The code is available at https://github.com/alibaba/ROLL.

</details>


### [30] [Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference](https://arxiv.org/abs/2512.22695)
*Mona Moghadampanah,Adib Rezaei Shahmirzadi,Farhana Amin,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: 本文首次对多模态大语言模型（MLLMs）推理的能耗进行详细分析，发现多模态输入导致17%-94%的额外能耗，并提出阶段级动态电压频率缩放（DVFS）作为有效优化方案。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在文本模型基础上整合其他模态，带来了新的能耗权衡问题。现有研究主要关注纯文本模型，对多模态推理的能耗特性缺乏深入理解，特别是模态膨胀导致的效率低下问题尚未被充分探索。

Method: 将MLLM推理流程分解为视觉编码、预填充和解码三个阶段，在NVIDIA A100 GPU上评估四种代表性MLLM模型。通过量化多模态推理相比纯文本基线的额外能耗，分析GPU功耗轨迹，并探索阶段级动态电压频率缩放（DVFS）优化技术。

Result: 多模态推理能耗比纯文本基线高出17%-94%，能耗瓶颈因模型架构而异：计算密集型视觉编码器或大量视觉标记序列在预填充阶段产生主要影响。GPU在多模态执行中存在显著利用率不足，输入复杂度导致不同模型的能耗扩展行为差异明显。阶段级DVFS能有效节省能耗，仅带来适度的性能影响。

Conclusion: 本研究首次提供了MLLM推理的详细阶段级能耗分析，揭示了多模态输入带来的显著能耗开销。研究结果为设计更节能的多模态LLM服务系统提供了实用见解和具体指导，特别是阶段级DVFS优化策略的有效性。

Abstract: Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly understood, as most prior work focuses on text-only models. In this paper, we examine modality inflation, a key source of inefficiency in which multimodal inputs increase inference workloads through extra encoding stages and expanded token sequences. We provide the first detailed, stage-level analysis of energy consumption in MLLM inference by breaking the pipeline into vision encoding, prefill, and decoding stages. Using four representative MLLMs evaluated on NVIDIA A100 GPU, we quantify the additional energy required for multimodal inference compared to text-only baselines, observing overheads ranging from 17% to 94% across models for identical inputs. Our results show that energy bottlenecks differ widely across model architectures, stemming either from compute-heavy vision encoders or from the downstream impact of large visual token sequences during prefill. By examining GPU power traces, we further uncover substantial GPU underutilization during multimodal execution and show that input complexity leads to markedly different energy scaling behaviors across models. Finally, we demonstrate that stage-wise dynamic voltage and frequency scaling (DVFS) is an effective optimization, allowing energy savings with only modest performance impact. Together, these findings offer practical insights and concrete guidance for designing more energy-efficient multimodal LLM serving systems.

</details>


### [31] [OptiNIC: A Resilient and Tail-Optimal RDMA NIC for Distributed ML Workloads](https://arxiv.org/abs/2512.22743)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: OptiNIC是一种针对分布式机器学习优化的RDMA传输协议，通过放弃传统可靠性和顺序保证，采用尽力而为的乱序传输模型，显著降低尾延迟并提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着分布式机器学习扩展到数千个GPU，RDMA传输中的尾延迟成为主要瓶颈。传统RDMA协议（如RoCE、IRN等）强制实施严格的可靠性和顺序交付，依赖重传和包排序，这些机制在ML场景中引入了不必要的复杂性和延迟，即使罕见的包延迟也会阻塞整个模型流水线。

Method: OptiNIC重新审视传统可靠性保证，基于ML对部分或缺失数据的容忍性，从NIC中消除重传和顺序交付，实现尽力而为的乱序传输模型。引入自适应超时机制来触发前向进度，同时保留标准拥塞控制机制，将丢失恢复转移到ML流水线本身（如通过Hadamard变换和擦除编码）。

Result: 评估显示OptiNIC在训练和推理中分别将时间到精度（TTA）提升2倍、吞吐量提升1.6倍，将第99百分位延迟降低3.5倍，BRAM使用减少2.7倍，NIC容错性几乎翻倍，在Hyperstack和CloudLab两个公共云平台上均表现出色。

Conclusion: OptiNIC为分布式ML工作负载提供了一个具有弹性、尾部优化的RDMA传输协议，通过利用ML应用对数据完整性的容忍特性，显著提升了大规模分布式ML系统的性能和可靠性。

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs connected by high-speed interconnects, tail latency in collective communication has become a major bottleneck. Existing RDMA transports, such as RoCE, IRN, SRNIC, and Falcon, enforce strict reliability and in-order delivery, relying on retransmissions and packet sequencing to ensure correctness. While these approaches work well for general-purpose workloads, they introduce complexity and latency that scale poorly in ML, where even rare packet delays can stall entire model pipelines.
  We present OptiNIC, a domain-specific RDMA transport that revisits traditional reliability guarantees based on ML's tolerance for partial or missing data. OptiNIC eliminates retransmissions and in-order delivery from the NIC, enabling a best-effort, out-of-order transport model for RDMA. Unlike traditional RDMA, which signals completion only after complete data delivery, OptiNIC introduces adaptive timeouts to trigger forward progress when data may be lost or delayed. OptiNIC retains standard congestion control mechanisms (e.g., DCQCN, EQDS, or Swift) while shifting loss recovery to the ML pipeline itself (e.g., via the Hadamard Transform and Erasure Coding).
  Our evaluation shows that OptiNIC improves time-to-accuracy (TTA) by 2x and increases throughput by 1.6x for training and inference, respectively, across two public clouds (i.e., Hyperstack and CloudLab). OptiNIC also lowers 99th-percentile latency by 3.5x, cuts BRAM usage by 2.7x, and nearly doubles NIC resilience to faults-delivering a resilient, tail-optimized RDMA transport purpose-built for distributed ML workloads.

</details>


### [32] [Argus: Token Aware Distributed LLM Inference Optimization](https://arxiv.org/abs/2512.22925)
*Panlong Wu,Yifei Zhong,Danyang Chen,Ting Wang,Fangxin Wang*

Main category: cs.DC

TL;DR: Argus：首个面向异构边缘云系统的令牌感知分布式LLM推理框架，通过长度预测和Lyapunov优化实现高效任务卸载


<details>
  <summary>Details</summary>
Motivation: 现有解决方案忽略了边缘云环境中LLM推理的动态性、随机性和异构性，特别是可变输出令牌长度和设备多样性的影响，导致推理时间不稳定

Method: 1) 长度感知语义模块：使用微调语言模型预测输出令牌长度；2) Lyapunov引导的卸载优化模块：考虑LLM预填充和解码成本的长时期QoE优化；3) 带阻尼和拥塞控制的迭代卸载算法解决整数非线性规划问题

Result: 理论和实证评估表明，Argus在高度动态、异构的环境中实现了稳健的性能和卓越的效率

Conclusion: Argus是首个令牌感知的分布式边缘云LLM推理框架，通过精确的长度预测和优化的任务卸载策略，有效解决了异构环境中的推理时间变异性问题

Abstract: Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochastic, and heterogeneous nature of such environments, often ignoring the impact of variable output token lengths and device diversity. In this work, we present Argus, the first token-aware distributed edge-cloud LLM inference framework that conducts efficient task offloading. Argus features a Length-Aware Semantics (LAS) module, which predicts output token lengths for incoming prompts using a fine-tuned language model with token-length-sensitive feature modulation, enabling precise estimation. Building on this, our Lyapunov-guided Offloading Optimization (LOO) module formulates long-term Quality-of-Experience optimization that explicitly considers both LLM prefilling and decoding costs. We introduce a novel Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to effectively solve the resulting integer nonlinear programming problem under time-varying constraints. Extensive theoretical and empirical evaluations demonstrate that Argus achieves robust performance and superior efficiency in highly dynamic, heterogeneous settings.

</details>


### [33] [Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware](https://arxiv.org/abs/2512.23029)
*Alex Khalil,Guillaume Heilles,Maria Parraga,Simon Heilles*

Main category: cs.DC

TL;DR: 本地部署量化30B参数MoE模型在消费级硬件上可实现接近云服务的性能，为中小企业提供低成本、高隐私的LLM解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要依赖云端专有系统，存在数据隐私、运营主权和成本高昂等问题，特别是对中小企业而言。需要探索本地部署高性能LLM的可行性。

Method: 在配备新一代NVIDIA GPU的消费级服务器上部署量化30B参数MoE模型（基于Qwen3）。从模型内在能力和服务器负载性能两个维度评估：模型性能与学术和行业标准对比；服务器性能通过延迟、每秒token数、首token时间等指标分析并发用户下的可扩展性。

Result: 精心配置的本地部署方案（使用新兴消费硬件和量化开源模型）能够实现与云服务相当的性能，为中小企业提供可行的LLM部署路径。

Conclusion: 本地部署量化MoE模型在消费级硬件上是可行的，为中小企业提供了无需高昂成本或隐私妥协的强大LLM部署方案，解决了云服务的成本、复杂性和隐私问题。

Abstract: The proliferation of Large Language Models (LLMs) has been accompanied by a reliance on cloud-based, proprietary systems, raising significant concerns regarding data privacy, operational sovereignty, and escalating costs. This paper investigates the feasibility of deploying a high-performance, private LLM inference server at a cost accessible to Small and Medium Businesses (SMBs). We present a comprehensive benchmarking analysis of a locally hosted, quantized 30-billion parameter Mixture-of-Experts (MoE) model based on Qwen3, running on a consumer-grade server equipped with a next-generation NVIDIA GPU. Unlike cloud-based offerings, which are expensive and complex to integrate, our approach provides an affordable and private solution for SMBs. We evaluate two dimensions: the model's intrinsic capabilities and the server's performance under load. Model performance is benchmarked against academic and industry standards to quantify reasoning and knowledge relative to cloud services. Concurrently, we measure server efficiency through latency, tokens per second, and time to first token, analyzing scalability under increasing concurrent users. Our findings demonstrate that a carefully configured on-premises setup with emerging consumer hardware and a quantized open-source model can achieve performance comparable to cloud-based services, offering SMBs a viable pathway to deploy powerful LLMs without prohibitive costs or privacy compromises.

</details>


### [34] [Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates](https://arxiv.org/abs/2512.23434)
*Yongjie Guan*

Main category: cs.DC

TL;DR: LRH是一种新的分布式哈希方案，通过将HRW选择限制在缓存局部窗口内，在保持令牌环结构的同时实现了更好的负载均衡和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的一致性哈希方案存在负载不均衡问题：基于环的方案需要大量虚拟节点才能降低峰值负载比，而多探针方法虽然改善了均衡性但导致内存访问分散、性能下降。

Method: 提出局部会合哈希(LRH)，保留令牌环结构但将最高随机权重选择限制在C个相邻物理节点的缓存局部窗口内。通过一次二分查找定位键，使用预计算的下一个不同偏移量枚举C个候选节点，选择HRW胜出者。

Result: 在N=5000节点、V=256虚拟节点、K=5000万键、C=8的基准测试中，LRH将最大/平均负载比从1.2785降至1.0947，达到60.05 Mkeys/s的吞吐量，比8探针多探针一致性哈希快6.8倍，同时接近其负载均衡水平。

Conclusion: LRH在保持令牌环结构优势的同时，通过缓存局部性设计显著提升了负载均衡和性能，解决了传统方案在均衡性和效率之间的权衡问题。

Abstract: Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic.

</details>


### [35] [Bitcoin-IPC: Scaling Bitcoin with a Network of Proof-of-Stake Subnets](https://arxiv.org/abs/2512.23439)
*Marko Vukolić,Orestis Alpos,Jakov Mitrovski,Themis Papameletiou,Nikola Ristić,Dionysis Zindros*

Main category: cs.DC

TL;DR: Bitcoin-IPC是一个软件栈和协议，通过创建以L1 BTC为质押的PoS Layer-2链（子网）来扩展比特币，使其成为通用交换媒介，无需修改比特币L1即可大幅提升交易吞吐量。


<details>
  <summary>Details</summary>
Motivation: 让比特币成为通用交换媒介（MoE），解决比特币L1交易吞吐量低（仅7tps）、成本高的问题，同时保持无需许可和去中心化的特性。

Method: 1. 设计受SWIFT消息系统启发的协议，嵌入比特币的SegWit机制；2. 创建完全可编程的PoS Layer-2链（子网），质押以L1 BTC计价；3. 利用比特币L1进行关键信息通信、结算和安全保障；4. 通过L1路由实现跨子网的无缝价值转移。

Result: 1. 每笔交易的虚拟字节成本降低高达23倍；2. 交易吞吐量从7tps提升到超过160tps；3. 无需修改比特币L1协议；4. 实现跨Layer-2子网的无缝价值转移。

Conclusion: Bitcoin-IPC通过创新的Layer-2架构和SWIFT启发的消息传递机制，显著提升了比特币的交易效率和可扩展性，为实现比特币作为通用交换媒介的目标提供了可行的技术路径，同时保持了比特币L1的安全性和无需许可的特性。

Abstract: We introduce Bitcoin-IPC, a software stack and protocol that scales Bitcoin towards helping it become the universal Medium of Exchange (MoE) by enabling the permissionless creation of fully programmable Proof-of-Stake (PoS) Layer-2 chains, called subnets, whose stake is denominated in L1 BTC. Bitcoin-IPC subnets rely on Bitcoin L1 for the communication of critical information, settlement, and security.
  Our design, inspired by SWIFT messaging and embedded within Bitcoin's SegWit mechanism, enables seamless value transfer across L2 subnets, routed through Bitcoin L1. Uniquely, this mechanism reduces the virtual-byte cost per transaction (vB per tx) by up to 23x, compared to transacting natively on Bitcoin L1, effectively increasing monetary transaction throughput from 7 tps to over 160 tps, without requiring any modifications to Bitcoin L1.

</details>


### [36] [Optimal Configuration of API Resources in Cloud Native Computing](https://arxiv.org/abs/2512.23494)
*Eddy Truyen,Wouter Joosen*

Main category: cs.DC

TL;DR: 将离线性能优化框架应用于微服务应用的发布阶段，优化CPU和内存资源配置参数，通过TeaStore应用评估不同优化算法，分析筛选与贝叶斯优化的权衡


<details>
  <summary>Details</summary>
Motivation: 当前研究主要集中在DevOps运维阶段的智能调度和自动扩缩容，但发布阶段的资源配置优化仍未被充分探索。容器水平自动扩缩容（如基于CPU使用率）可能导致内存分配不当，需要在部署前对两种资源进行精细调优

Method: 将现有离线性能优化框架应用于微服务应用的发布阶段，使用TeaStore微服务应用进行评估，统计比较不同优化算法，分析因子筛选（减少搜索空间）与贝叶斯优化的效果

Result: 当目标是找到最优资源配置且采样预算有限时，前置因子筛选有助于减少搜索空间；当需要统计比较不同算法时，筛选也使收集所有数据点变得可行；但当目标是找到接近最优配置时，无筛选的贝叶斯优化效果更好

Conclusion: 微服务应用发布阶段的资源配置优化是重要但被忽视的问题，不同的优化目标需要采用不同的策略：预算有限时使用筛选，需要算法比较时使用筛选，追求近优解时使用无筛选的贝叶斯优化

Abstract: This paper presents how an existing framework for offline performance optimization can be applied to microservice applications during the Release phase of the DevOps life cycle. Optimization of resource allocation configuration parameters for CPU and memory during the Release phase remains a largely unexplored problem as most research has focused on intelligent scheduling and autoscaling of microservices during the Ops stage of the DevOps cycle. Yet horizontal auto-scaling of containers, based on CPU usage for instance, may still leave these containers with an inappropriately allocated amount of memory, if no upfront fine-tuning of both resources is applied before the Deployment phase. We evaluate the performance optimization framework using the TeaStore microservice application and statistically compare different optimization algorithms, supporting informed decisions about their trade-offs between sampling cost and distance to the optimal resource configuration. This shows that upfront factor screening, for reducing the search space, is helpful when the goal is to find the optimal resource configuration with an affordable sampling budget. When the goal is to statistically compare different algorithms, screening must also be applied to make data collection of all data points in the search space feasible.  If the goal is to find a near-optimal configuration, however, it is better to run bayesian optimization without screening.

</details>


### [37] [Decoupling Adaptive Control in TeaStore](https://arxiv.org/abs/2512.23495)
*Eddy Truyen*

Main category: cs.DC

TL;DR: 该论文探讨了如何通过不同技术方法（软件架构、云原生Operator模式、传统编程技术）实现TeaStore微服务案例的自适应控制循环，分析各种方法在细粒度表达与系统级控制之间的权衡，并提出多层级架构方案。


<details>
  <summary>Details</summary>
Motivation: TeaStore规范为微服务自适应提供了案例研究，但实现需要考虑自适应系统的关键属性：系统范围一致性（跨副本协调适应）、规划（满足条件前持续执行适应）、模块化（适应逻辑的清晰集成）。需要探索如何将自适应控制逻辑与应用程序解耦。

Method: 论文分析了三种技术方法：1）软件架构方法；2）云原生Operator模式；3）传统编程语言技术。通过比较这些方法在实现自适应控制逻辑与TeaStore应用解耦时的权衡，特别是细粒度表达与系统级控制之间的平衡。

Result: 分析表明这些方法并非互斥，可以结合形成多层级架构。软件架构方法提供系统级协调，Operator模式支持声明式管理，传统技术提供细粒度控制。适应策略的复用在不同层级效果不同，需要根据具体需求选择合适组合。

Conclusion: 实现自适应微服务应采用多层级架构，结合不同方法的优势：架构方法确保系统一致性，Operator模式提供云原生管理，传统技术实现细粒度适应。这种组合能平衡表达性与控制力，有效支持自适应策略的复用。

Abstract: The Adaptable TeaStore specification provides a microservice-based case study for implementing self-adaptation through a control loop.  We argue that implementations of this specification should be informed by key properties of self-adaptation: system-wide consistency (coordinated adaptations across replicas), planning (executing an adaptation until appropriate conditions are met),  and modularity (clean integration of adaptation logic).  In this implementation discussion paper, we examine how software architectural methods, the cloud-native Operator pattern, and legacy programming language techniques can decouple self-adaptive control logic from the TeaStore application. We analyze the trade-offs that these different approaches make between fine-grained expressive adaptation and system-wide control, and highlight when reuse of adaptation strategies is most effective. Our analysis suggests that these approaches are not mutually exclusive but can be combined into a multi-tiered architecture for self-adaptive microservices.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [38] [An Energy-Efficient RFET-Based Stochastic Computing Neural Network Accelerator](https://arxiv.org/abs/2512.22131)
*Sheng Lu,Qianhou Qu,Sungyong Jung,Qilian Liang,Chenyun Pan*

Main category: cs.AR

TL;DR: 提出基于可重构场效应晶体管(RFET)的随机计算神经网络架构，显著降低硬件资源消耗


<details>
  <summary>Details</summary>
Motivation: 传统随机计算神经网络(SCNN)由于随机数生成器(SNG)和累加并行计数器(APC)等组件导致资源占用高，限制了性能

Method: 利用RFET器件级可重构特性设计高效紧凑的SNG、APC等核心模块，开发专用SCNN加速器架构进行系统级仿真

Result: 与相同技术节点的FinFET设计相比，RFET-based SCNN加速器在面积、延迟和能耗方面实现显著降低

Conclusion: RFET技术为SCNN提供了高效的硬件实现方案，大幅提升了随机计算神经网络的性能效率

Abstract: Stochastic computing (SC) offers significant reductions in hardware complexity for traditional convolutional neural networks (CNNs), but stochastic computing neural networks (SCNNs) still suffer from high resource usage due to components such as stochastic number generators (SNGs) and accumulative parallel counters (APCs), which limit performance. This paper introduces a novel SCNN architecture based on reconfigurable field-effect transistors (RFETs), whose device-level reconfigurability enables the design of highly efficient and compact SNGs, APCs, and other core modules. A dedicated SCNN accelerator architecture is also developed for system-level simulation. Using publicly available open-source standard cell libraries, experimental results show that the proposed RFET-based SCNN accelerator achieves substantial reductions in area, latency, and energy consumption compared to a FinFET-based design at the same technology node.

</details>


### [39] [AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience](https://arxiv.org/abs/2512.22435)
*Zining Wang,Jian Gao,Weimin Fu,Xiaolong Guo,Xuan Zhang*

Main category: cs.AR

TL;DR: AnalogSAGE：一个开源的自进化多智能体框架，通过分层记忆和仿真反馈实现模拟电路自动化设计，相比现有方法显著提升通过率和效率。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计高度依赖人类经验和直觉，现有基于LLM的方法通常依赖提示驱动的网表生成或预定义拓扑模板，难以满足复杂规格要求。

Method: 提出AnalogSAGE框架，协调三阶段智能体探索，通过四个分层记忆层实现迭代优化，利用仿真反馈进行验证，支持开源SKY130 PDK和ngspice仿真。

Result: 在十个不同难度的运算放大器设计问题基准测试中，相比现有框架实现10倍总体通过率、48倍Pass@1提升，参数搜索空间减少4倍。

Conclusion: 分层记忆和基于仿真的推理显著提升了模拟设计自动化的可靠性和自主性，开源代码支持可复现性和通用性。

Abstract: Analog circuit design remains a knowledge- and experience-intensive process that relies heavily on human intuition for topology generation and device parameter tuning. Existing LLM-based approaches typically depend on prompt-driven netlist generation or predefined topology templates, limiting their ability to satisfy complex specification requirements. We propose AnalogSAGE, an open-source self-evolving multi-agent framework that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. To support reproducibility and generality, we release the source code. Our benchmark spans ten specification-driven operational amplifier design problems of varying difficulty, enabling quantitative and cross-task comparison under identical conditions. Evaluated under the open-source SKY130 PDK with ngspice, AnalogSAGE achieves a 10$\times$ overall pass rate, a 48$\times$ Pass@1, and a 4$\times$ reduction in parameter search space compared with existing frameworks, demonstrating that stratified memory and grounded reasoning substantially enhance the reliability and autonomy of analog design automation in practice.

</details>


### [40] [TYTAN: Taylor-series based Non-Linear Activation Engine for Deep Learning Accelerators](https://arxiv.org/abs/2512.23062)
*Soham Pramanik,Vimal William,Arnab Raha,Debayan Das,Amitava Mukherjee,Janet L. Paluh*

Main category: cs.AR

TL;DR: 本文提出TYTAN：基于泰勒级数的非线性激活引擎，通过可重构硬件设计和动态近似算法优化边缘AI推理中的非线性激活函数，实现约2倍性能提升、56%功耗降低和35倍面积缩减。


<details>
  <summary>Details</summary>
Motivation: 随着AI架构快速发展和AI系统普及，边缘设备需要领域特定架构来提升AI推理的加速和能效。当前AI算法部署面临计算成本和能耗等资源限制，特别是GEMM和激活函数等高功耗操作需要优化。

Method: 提出TYTAN系统，包含可重构硬件设计和专门算法，通过动态估计每个激活函数所需的近似值，实现最小化与基准准确度的偏差。采用泰勒级数基础的非线性激活引擎和广义非线性近似引擎(G-NAE)方法。

Result: 在Silvaco FreePDK45工艺节点上的系统级仿真显示，TYTAN能在>950 MHz时钟频率下运行，相比基准开源NVDLA实现，性能提升约2倍，功耗降低约56%，面积减少约35倍。

Conclusion: TYTAN系统能有效支持边缘AI推理的加速和能效提升，通过优化非线性激活函数，在保持准确度的同时显著改善性能、功耗和面积效率。

Abstract: The rapid advancement in AI architectures and the proliferation of AI-enabled systems have intensified the need for domain-specific architectures that enhance both the acceleration and energy efficiency of AI inference, particularly at the edge. This need arises from the significant resource constraints-such as computational cost and energy consumption-associated with deploying AI algorithms, which involve intensive mathematical operations across multiple layers. High-power-consuming operations, including General Matrix Multiplications (GEMMs) and activation functions, can be optimized to address these challenges. Optimization strategies for AI at the edge include algorithmic approaches like quantization and pruning, as well as hardware methodologies such as domain-specific accelerators. This paper proposes TYTAN: TaYlor-series based non-linear acTivAtion eNgine, which explores the development of a Generalized Non-linear Approximation Engine (G-NAE). TYTAN targets the acceleration of non-linear activation functions while minimizing power consumption. The TYTAN integrates a re-configurable hardware design with a specialized algorithm that dynamically estimates the necessary approximation for each activation function, aimed at achieving minimal deviation from baseline accuracy. The proposed system is validated through performance evaluations with state-of-the-art AI architectures, including Convolutional Neural Networks (CNNs) and Transformers. Results from system-level simulations using Silvaco's FreePDK45 process node demonstrate TYTAN's capability to operate at a clock frequency >950 MHz, showcasing its effectiveness in supporting accelerated, energy-efficient AI inference at the edge, which is ~2 times performance improvement, with ~56% power reduction and ~35 times lower area compared to the baseline open-source NVIDIA Deep Learning Accelerator (NVDLA) implementation.

</details>
