<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [When Lifetimes Liberate: A Type System for Arenas with Higher-Order Reachability Tracking](https://arxiv.org/abs/2509.04253)
*Siyuan He,Songlin Jia,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 这篇论文提出了一种统一的资源管理方法，结合了区域系统、所有权类型和可达性类型的优点，支持任意共享和静态生命周期保证


<details>
  <summary>Details</summary>
Motivation: 解决高阶函数式语言中静态资源管理的挑战，统一区域系统的词法生命周期控制和表达力，Rust的非词法生命周期和安全保证，以及可达性类型的共享推理能力

Method: 在可达性类型基础上提出两个新扩展：A<:采用新题两维存储模型支持区域内资源的粗粒度可达性跟踪；{A}<:实现词法生命周期控制和静态保证

Result: 建立了第一个支持生命周期控制的可达性形式系统，避免了流效应性推理的复杂性，保持了表达力和简洁性

Conclusion: 该方法成功结合了不同资源管理方法的优点，为高阶函数式语言提供了灵活且安全的资源管理方案

Abstract: Static resource management in higher-order functional languages remains
elusive due to tensions between control, expressiveness, and flexibility.
Region-based systems [Grossman et al. 2002; Tofte et al. 2001] offer control
over lifetimes and expressive in-region sharing, but restrict resources to
lexical scopes. Rust, an instance of ownership types [Clarke et al. 2013],
offers non-lexical lifetimes and robust safety guarantees, yet its global
invariants make common sharing patterns hard to express. Reachability types
[Wei et al. 2024] enable reasoning about sharing and separation, but lack
practical tools for controlling resource lifetimes.
  In this work, we try to unify their strengths. Our solution enables grouping
resources as arenas for arbitrary sharing and static guarantees of lexically
scoped lifetimes. Crucially, arenas and lexical lifetimes are not the only
choice: users may also manage resources individually, with non-lexical
lifetimes. Regardless of mode, resources share the same type, preserving the
higher-order parametric nature of the language.
  Obtaining static safety guarantee in a higher-order language with flexible
sharing is nontrivial. To this end, we propose two new extensions atop
reachability types [Wei et al. 2024]. First, A<: features a novel
two-dimensional store model to enable coarse-grained reachability tracking for
arbitrarily shared resources within arenas. Building on this, {A}<: establishes
lexical lifetime control with static guarantees. As the first reachability
formalism presented for lifetime control, {A}<: avoids the complication of
flow-sensitive reasoning and retains expressive power and simplicity. Both
calculi are formalized and proven type safe in Rocq.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Combining Performance and Productivity: Accelerating the Network Sensing Graph Challenge with GPUs and Commodity Data Science Software](https://arxiv.org/abs/2509.03653)
*Siddharth Samsi,Dan Campbell,Emanuel Scoullos,Oded Green*

Main category: cs.DC

TL;DR: 这篇论文提出了一种基于数据科学语言的GraphBLAS替代方案，通过NVIDIA RAPIDS生态系统的开源商业软件实现了HPEC图谜题，在GPU上获得了147-2185倍的性能加速。


<details>
  <summary>Details</summary>
Motivation: 传统HPC测试如LINPACK无法测试复杂工作负荷，HPEC图谜题需要更多软件组件和整体工作流程，需要更高效的解决方案。

Method: 使用数据科学语言重新解释GraphBLAS形式，通过NVIDIA RAPIDS生态系统中的开源商业软件（RAPIDS cuDF和cupy）实现图谜题，无需专门的HPC代码。

Result: 在NVIDIA GPU上获得显著性能加速：A100 GPU 147-509倍，H100 GPU 243-1269倍，H200 GPU 332-2185倍，相比CPU上的Pandas实现。

Conclusion: 通过数据科学语言和开源商业软件可以高效实现复杂的HPC图谜题，提供了不需专门HPC代码的高性能解决方案。

Abstract: The HPEC Graph Challenge is a collection of benchmarks representing complex
workloads that test the hardware and software components of HPC systems, which
traditional benchmarks, such as LINPACK, do not. The first benchmark, Subgraph
Isomorphism, focused on several compute-bound and memory-bound kernels. The
most recent of the challenges, the Anonymized Network Sensing Graph Challenge,
represents a shift in direction, as it represents a longer end-to-end workload
that requires many more software components, including, but not limited to,
data I/O, data structures for representing graph data, and a wide range of
functions for data preparation and network analysis. A notable feature of this
new graph challenge is the use of GraphBLAS to represent the computational
aspects of the problem statement. In this paper, we show an alternative
interpretation of the GraphBLAS formulations using the language of data
science. With this formulation, we show that the new graph challenge can be
implemented using off-the-shelf ETL tools available in open-source, enterprise
software such as NVIDIA's RAPIDS ecosystem. Using off-the-shelf software,
RAPIDS cuDF and cupy, we enable significant software acceleration without
requiring any specific HPC code and show speedups, over the same code running
with Pandas on the CPU, of 147x-509x on an NVIDIA A100 GPU, 243x-1269X for an
NVIDIA H100 GPU, and 332X-2185X for an NVIDIA H200 GPU.

</details>


### [3] [Distributed Download from an External Data Source in Asynchronous Faulty Settings](https://arxiv.org/abs/2509.03755)
*John Augustine,Soumyottam Chatterjee,Valerie King,Manish Kumar,Shachar Meir,David Peleg*

Main category: cs.DC

TL;DR: 本文首次研究异步通信网络中的分布式数据检索问题，针对下载问题在拜占庭和崩溃故障模型下提出查询最优的确定性解决方案，并建立了随机协议的下界和上界。


<details>
  <summary>Details</summary>
Motivation: 扩展分布式数据检索研究到异步通信网络，解决现有工作主要关注同步网络的问题，探索在异步环境下如何最小化查询复杂度同时最大化容错能力。

Method: 在异步通信模型中设计确定性协议处理崩溃故障，分析随机协议在拜占庭故障下的性能，建立查询复杂度的理论下界和上界。

Result: 对于崩溃故障，提出了能容忍任意固定比例β<1故障的查询最优确定性方案；对于拜占庭故障，证明了随机协议在β≥1/2时的Ω(n)下界，并在β<1/2时给出了接近最优的随机协议。

Conclusion: 这是首个解决异步网络中下载问题的工作，为异步分布式数据检索提供了理论基础和实用解决方案，特别是在不同故障模型下的查询复杂度优化。

Abstract: The distributedData Retrieval (DR) model consists of $k$ peers connected by a
complete peer-to-peer communication network, and a trusted external data source
that stores an array $\textbf{X}$ of $n$ bits ($n \gg k$). Up to $\beta k$ of
the peers might fail in any execution (for $\beta \in [0, 1)$). Peers can
obtain the information either by inexpensive messages passed among themselves
or through expensive queries to the source array $\textbf{X}$. In the DR model,
we focus on designing protocols that minimize the number of queries performed
by any nonfaulty peer (a measure referred to as query complexity) while
maximizing the resilience parameter $\beta$.
  The Download problem requires each nonfaulty peer to correctly learn the
entire array $\textbf{X}$. Earlier work on this problem focused on synchronous
communication networks and established several deterministic and randomized
upper and lower bounds. Our work is the first to extend the study of
distributed data retrieval to asynchronous communication networks. We address
the Download problem under both the Byzantine and crash failure models. We
present query-optimal deterministic solutions in an asynchronous model that can
tolerate any fixed fraction $\beta<1$ of crash faults. In the Byzantine failure
model, it is known that deterministic protocols incur a query complexity of
$\Omega(n)$ per peer, even under synchrony. We extend this lower bound to
randomized protocols in the asynchronous model for $\beta \geq 1/2$, and
further show that for $\beta < 1/2$, a randomized protocol exists with
near-optimal query complexity. To the best of our knowledge, this is the first
work to address the Download problem in asynchronous communication networks.

</details>


### [4] [Gathering of asynchronous robots on circle with limited visibility using finite communication](https://arxiv.org/abs/2509.04004)
*Avisek Sharma,Satakshi Ghosh,Buddhadeb Sau*

Main category: cs.DC

TL;DR: 本文提出了一种在π-可见性模型下解决机器人聚集问题的算法，使用有限通信能力(ℱCOM)的机器人在完全异步调度器下实现非刚性移动的聚集。


<details>
  <summary>Details</summary>
Motivation: 解决有限可见性环境下自主匿名机器人的聚集问题。之前的研究表明，即使从可见性中移除一个点（π可见性），聚集也变得困难，且现有算法需要特殊调度器或有限内存。本文旨在在完全异步调度器下使用有限通信能力解决这一问题。

Method: 提出基于有限通信能力(ℱCOM)的算法，在π-可见性模型下实现机器人聚集。机器人具有非刚性移动特性，工作在完全异步调度器环境中。

Result: 成功解决了π-可见性模型下的机器人聚集问题，相比之前需要特殊异步调度器或有限内存的方案，本算法在完全异步调度器下使用有限通信能力即可实现聚集。

Conclusion: 该算法证明了在π-可见性限制下，通过有限通信能力可以在完全异步环境中实现机器人聚集，为非刚性移动机器人的聚集问题提供了新的解决方案。

Abstract: This work addresses the gathering problem for a set of autonomous, anonymous,
and homogeneous robots with limited visibility operating in a continuous
circle. The robots are initially placed at distinct positions, forming a
rotationally asymmetric configuration. The robots agree on the clockwise
direction. In the $\theta$-visibility model, a robot can only see those robots
on the circle that are at an angular distance $<\theta$ from it. Di Luna
\textit{et. al.} [DISC'20] have shown that, in $\pi/2$ visibility, gathering is
impossible. In addition, they provided an algorithm for robots with $\pi$
visibility, operating under a semi-synchronous scheduler. In the $\pi$
visibility model, only one point, the point at the angular distance $\pi$ is
removed from the visibility. Ghosh \textit{et. al.} [SSS'23] provided a
gathering algorithm for $\pi$ visibility model with robot having finite memory
($\mathcal{FSTA}$), operating under a special asynchronous scheduler.
  If the robots can see all points on the circle, then the gathering can be
done by electing a leader in the weakest robot model under a fully asynchronous
scheduler. However, previous works have shown that even the removal of one
point from the visibility makes gathering difficult. In both works, the robots
had rigid movement. In this work, we propose an algorithm that solves the
gathering problem under the $\pi$-visibility model for robots that have finite
communication ability ($\mathcal{FCOM}$). In this work the robot movement is
non-rigid and the robots work under a fully asynchronous scheduler.

</details>


### [5] [Counterfactual simulations for large scale systems with burnout variables](https://arxiv.org/abs/2509.04038)
*Benjamin Heymann*

Main category: cs.DC

TL;DR: 提出基于不确定性松弛的新算法，用于高效并行计算具有burnout变量的系统中的反事实估计


<details>
  <summary>Details</summary>
Motivation: 大规模系统中存在burnout变量（激活后不可逆失活），传统顺序处理反事实场景计算成本高，特别是在在线广告等应用中

Method: 引入不确定性松弛算法，通过并行计算替代顺序处理，提高反事实估计的可扩展性

Result: 显著提升了具有burnout变量系统的反事实估计计算效率

Conclusion: 不确定性松弛算法为处理复杂系统中的反事实分析提供了有效的可扩展解决方案

Abstract: We consider large-scale systems influenced by burnout variables - state
variables that start active, shape dynamics, and irreversibly deactivate once
certain conditions are met. Simulating what-if scenarios in such systems is
computationally demanding, as alternative trajectories often require sequential
processing, which does not scale very well. This challenge arises in settings
like online advertising, because of campaigns budgets, complicating
counterfactual analysis despite rich data availability. We introduce a new type
of algorithms based on what we refer to as uncertainty relaxation, that enables
efficient parallel computation, significantly improving scalability for
counterfactual estimation in systems with burnout variables.

</details>


### [6] [LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for High-Performance Distributed Training Systems](https://arxiv.org/abs/2509.04084)
*Chenxuan Yao,Yuchong Hu,Feifan Liu,Zhengyu Liu,Dan Feng*

Main category: cs.DC

TL;DR: LowDiff是一个高效的频繁检查点框架，通过重用压缩梯度作为差异检查点来降低成本，并采用批量梯度写入优化和动态调参策略，实现了每迭代检查点频率且运行时开销小于3.1%


<details>
  <summary>Details</summary>
Motivation: 分布式大模型训练经常失败需要检查点恢复，现有频繁检查点方法成本高昂，而差异检查点技术目前仅限于推荐系统，需要扩展到通用分布式训练系统

Method: 提出LowDiff框架：1)重用压缩梯度作为差异检查点 2)批量梯度写入优化 3)动态调整检查点频率和批量大小 4)分层梯度重用和快照方法 5)CPU异步持久化策略

Result: 在各种工作负载上的实验表明，LowDiff可以实现每迭代检查点频率，运行时开销小于3.1%

Conclusion: LowDiff通过创新性地重用压缩梯度和优化策略，成功实现了高效的频繁检查点，显著降低了分布式训练的成本和性能开销

Abstract: Distributed training of large deep-learning models often leads to failures,
so checkpointing is commonly employed for recovery. State-of-the-art studies
focus on frequent checkpointing for fast recovery from failures. However, it
generates numerous checkpoints, incurring substantial costs and thus degrading
training performance. Recently, differential checkpointing has been proposed to
reduce costs, but it is limited to recommendation systems, so its application
to general distributed training systems remains unexplored.
  This paper proposes LowDiff, an efficient frequent checkpointing framework
that \textit{reuses} compressed gradients, serving as differential checkpoints
to reduce cost. Furthermore, LowDiff incorporates a batched gradient write
optimization to persist these differentials to storage efficiently. It also
dynamically tunes both the checkpoint frequency and the batching size to
maximize performance. We further enhance LowDiff with a layer-wise gradient
reusing and snapshotting approach and a CPU-based asynchronous persistence
strategy, enabling frequent checkpointing without gradient compression.
Experiments on various workloads show that LowDiff can achieve checkpointing
frequency up to per iteration with less than 3.1\% runtime overhead.

</details>


### [7] [Trustworthy Second-hand Marketplace for Built Environment](https://arxiv.org/abs/2509.04085)
*Stanly Wilson,Kwabena Adu-Duodu,Yinhao Li,Ringo Sham,Yingli Wang,Ellis Solaiman,Charith Perera,Rajiv Ranjan,Omer Rana*

Main category: cs.DC

TL;DR: 基于区块链和IPFS的数字化市场平台，通过自动化和可追溯性提高建筑材料重用效率和信任度


<details>
  <summary>Details</summary>
Motivation: 建筑行业面临材料浪费和可持续性挑战，需要集成自动化、可追溯性和去中心化决策的创新解决方案

Method: 开发了一个基于区块链和InterPlanetary File System (IPFS)的数字化市场平台框架，确保材料交换的透明度和可追溯性

Result: 平台框架演示了市场的运营过程，显示其实际应用和效果，能够促进可重用材料的高效、可信任交换

Conclusion: 该研究代表了向更可持续建筑实践进行的重要一步，通过区块链技术提高了材料交换中的信任和责任制

Abstract: The construction industry faces significant challenges regarding material
waste and sustainable practices, necessitating innovative solutions that
integrate automation, traceability, and decentralised decision-making to enable
efficient material reuse. This paper presents a blockchain-enabled digital
marketplace for sustainable construction material reuse, ensuring transparency
and traceability using InterPlanetary File System (IPFS). The proposed
framework enhances trust and accountability in material exchange, addressing
key challenges in industrial automation and circular supply chains. A framework
has been developed to demonstrate the operational processes of the marketplace,
illustrating its practical application and effectiveness. Our contributions
show how the marketplace can facilitate the efficient and trustworthy exchange
of reusable materials, representing a substantial step towards more sustainable
construction practices.

</details>


### [8] [On the impact of unlimited computational power in OBLOT: consequences for synchronous robots on graphs](https://arxiv.org/abs/2509.04383)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 本文证明在同步机器人在有限图上移动时，无限计算能力对OBLOT模型有显著影响，并提出了一种保证最少移动和轮数的通用解决算法


<details>
  <summary>Details</summary>
Motivation: OBLOT模型中的机器人能力有限，传统研究主要关注移动次数和轮数成本，而忽视了计算能力的影响。本文旨在研究无限计算能力对算法性能的影响

Method: 利用同步机器人在有限图上的无限计算能力，设计了一种通用的确定性解决算法

Result: 提出的算法能够应用于广泛的问题类别，同时保证最少移动次数和轮数，证明了无限计算能力在OBLOT模型中的重要性

Conclusion: 无限计算能力在OBLOT模型中具有显著影响，通过合理利用可以设计出最优的分布式算法，为理论群体机器人学提供了新的设计思路

Abstract: The OBLOT model has been extensively studied in theoretical swarm robotics.
It assumes weak capabilities for the involved mobile robots, such as they are
anonymous, disoriented, no memory of past events (oblivious), and silent. Their
only means of (implicit) communication is transferred to their positioning,
i.e., stigmergic information. These limited capabilities make the design of
distributed algorithms a challenging task. Over the last two decades, numerous
research papers have addressed the question of which tasks can be accomplished
within this model. Nevertheless, as it usually happens in distributed
computing, also in OBLOT the computational power available to the robots is
neglected as the main cost measures for the designed algorithms refer to the
number of movements or the number of rounds required. In this paper, we prove
that for synchronous robots moving on finite graphs, the unlimited
computational power (other than finite time) has a significant impact. In fact,
by exploiting it, we provide a definitive resolution algorithm that applies to
a wide class of problems while guaranteeing the minimum number of moves and
rounds.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [Hardware-Aware Data and Instruction Mapping for AI Tasks: Balancing Parallelism, I/O and Memory Tradeoffs](https://arxiv.org/abs/2509.03846)
*Md Rownak Hossain Chowdhury,Mostafizur Rahman*

Main category: cs.AR

TL;DR: 通过预先规划计算和通信的流式执行框架，利用消息传递架构实现高效深度学习推理，减少I/O和主机干预，达到高利用率和低内存消耗


<details>
  <summary>Details</summary>
Motivation: 解决深度学习推理中常见的I/O瓶颈、内存希缺和主机干预问题，通过预测神经网络行为来优化计算和通信

Method: 构建统一的指令和数据流，采用程序化消息基础计算架构，实现细粒度消息传递，运用静态重用、数组内多播和分段约简等技术

Result: 在VGG-19上实现92%利用率，97%消息内部生成，89%时间用于芯片内传输，计算吞吐量超1 TFLOP/s，重用和局部聚合减少100MB/层

Conclusion: 流式计算方式高效可行，通过紧密协调数据和指令流可以实现自主执行，显著减少外部依赖

Abstract: We introduce a mapping framework for deep learning inference that takes
advantage of predictable neural network behavior to plan both computation and
communication ahead of time. The framework generates a unified stream of
instructions and data, enabling the hardware to execute operations and route
information on its own, without frequent involvement from the host and with
minimal off-chip memory use. This naturally reduces reliance on I/O, off-chip
memory, and host control. By leveraging fine-grained message passing on a
programmable, message-based compute architecture, the framework keeps data
movement local and coordinates computation across the array using techniques
such as stationary-weight reuse, in-array multicasting, and staged reductions.
Applied to VGG-19, the framework sustains high utilization (88 to 92 percent),
with over 97 percent of messages generated internally and nearly 89 percent of
time consumed on-chip transfers. Computation throughput scales beyond 1 TFLOP/s
on larger arrays, while traffic reductions from reuse and local aggregation
reach up to 100 MB per layer. Overall, the results highlight the effectiveness
of streaming-based computation and show how our mapper enables this execution
style by tightly coordinating data and instruction flow across the hardware.

</details>


### [10] [Real Time FPGA Based CNNs for Detection, Classification, and Tracking in Autonomous Systems: State of the Art Designs and Optimizations](https://arxiv.org/abs/2509.04153)
*Safa Mohammed Sali,Mahmoud Meribout,Ashiyana Abdul Majeed*

Main category: cs.AR

TL;DR: 综述论文，系统回顾了FPGA上CNN目标检测、分类和跟踪的最新进展，重点讨论了硬件加速技术、优化策略和软硬件协同设计方法。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶、机器人和监控等实时计算机视觉应用需求的增长，FPGA因其可重构性、低功耗和确定性延迟等优势，成为GPU和ASIC的有力替代方案。

Method: 通过批判性审查最先进的FPGA实现，涵盖算法创新、硬件加速技术、剪枝、量化和稀疏感知等优化策略，以及现代FPGA平台和开发工具的比较分析。

Result: 全面梳理了FPGA部署CNN的技术现状，包括混合架构、数据流优化和流水线处理等关键技术，为实时推理提供了有效解决方案。

Conclusion: 该综述为研究人员和工程师提供了开发下一代高效能、低功耗视觉系统的关键见解，特别适用于边缘和嵌入式应用的FPGA部署。

Abstract: This paper presents a comprehensive review of recent advances in deploying
convolutional neural networks (CNNs) for object detection, classification, and
tracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand
for real-time computer vision applications in domains such as autonomous
vehicles, robotics, and surveillance, FPGAs have emerged as a powerful
alternative to GPUs and ASICs due to their reconfigurability, low power
consumption, and deterministic latency. We critically examine state-of-the-art
FPGA implementations of CNN-based vision tasks, covering algorithmic
innovations, hardware acceleration techniques, and the integration of
optimization strategies like pruning, quantization, and sparsity-aware methods
to maximize performance within hardware constraints. This survey also explores
the landscape of modern FPGA platforms, including classical LUT-DSP based
architectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration
Platforms (ACAPs), comparing their capabilities in handling deep learning
workloads. Furthermore, we review available software development tools such as
Vitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the
design and deployment of AI models on FPGAs. The paper uniquely discusses
hybrid architecture that combine GPUs and FPGAs for collaborative acceleration
of AI inference, addressing challenges related to energy efficiency and
throughput. Additionally, we highlight hardware-software co-design practices,
dataflow optimizations, and pipelined processing techniques essential for
real-time inference on resource-constrained devices. Through this survey,
researchers and engineers are equipped with insights to develop
next-generation, power-efficient, and high-performance vision systems optimized
for FPGA deployment in edge and embedded applications.

</details>


### [11] [Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs and Optimizations](https://arxiv.org/abs/2509.04162)
*Safa Mohammed Sali,Mahmoud Meribout,Ashiyana Abdul Majeed*

Main category: cs.AR

TL;DR: 本文全面综述了基于FPGA的Transformer和视觉语言模型推理的设计权衡、优化策略和实现挑战，包括设备选择、内存约束、数据流编排、量化策略等关键技术问题，并讨论了硬件算法协同设计的新趋势。


<details>
  <summary>Details</summary>
Motivation: Transformer和视觉语言模型在计算机视觉和多模态AI中表现出色，但其高计算复杂度、大内存占用和不规则数据访问模式在延迟和功耗受限环境中部署面临重大挑战。FPGA因其可重构性、细粒度并行性和能效优势成为理想的硬件平台。

Method: 通过综合分析设计权衡、优化策略和实现挑战，研究设备类别选择、内存子系统约束、数据流编排、量化策略、稀疏性利用、工具链选择等关键技术，以及VLMs特有的模态特定问题如异构计算平衡和交叉注意力内存管理。

Result: 提出了针对FPGA部署的全面技术框架，涵盖了从硬件选择到算法优化的各个层面，为高效部署多模态AI模型提供了系统性的解决方案。

Conclusion: FPGA为Transformer和VLMs提供了有前景的部署平台，通过硬件算法协同设计、创新注意力机制、压缩技术和模块化覆盖层等新兴趋势，未来将朝着可扩展、可移植和可重构的解决方案发展，以适配不断演进的模型架构并保持高利用率和可预测性能。

Abstract: Transformers and vision-language models (VLMs) have emerged as dominant
architectures in computer vision and multimodal AI, offering state-of-the-art
performance in tasks such as image classification, object detection, visual
question answering, and caption generation. However, their high computational
complexity, large memory footprints, and irregular data access patterns present
significant challenges for deployment in latency- and power-constrained
environments. Field-programmable gate arrays (FPGAs) provide an attractive
hardware platform for such workloads due to their reconfigurability,
fine-grained parallelism, and potential for energy-efficient acceleration. This
paper presents a comprehensive review of design trade-offs, optimization
strategies, and implementation challenges for FPGA-based inference of
transformers and VLMs. We examine critical factors such as device-class
selection, memory subsystem constraints, dataflow orchestration, quantization
strategies, sparsity exploitation, and toolchain choices, alongside
modality-specific issues unique to VLMs, including heterogeneous compute
balancing and cross-attention memory management. Additionally, we discuss
emerging trends in hardware-algorithm co-design, highlighting innovations in
attention mechanisms, compression, and modular overlays to improve efficiency
and adaptability. Practical issues such as runtime flexibility, verification
overhead, and the absence of standardized FPGA multimodal benchmarks are also
considered. Finally, we outline future directions toward scalable, portable,
and reconfigurable FPGA solutions that adapt to evolving model architectures
while sustaining high utilization and predictable performance. This synthesis
offers both a technical foundation and a forward-looking perspective to help
bridge the gap between advanced multimodal AI models and efficient FPGA
deployment.

</details>


### [12] [Real-time Object Detection and Associated Hardware Accelerators Targeting Autonomous Vehicles: A Review](https://arxiv.org/abs/2509.04173)
*Safa Sali,Anis Meribout,Ashiyana Majeed,Mahmoud Meribout,Juan Pablo,Varun Tiwari,Asma Baobaid*

Main category: cs.AR

TL;DR: 这篇评论性论文系统调研了自主驾驶汽车实时目标检测算法及硬件加速器，旨在找到检测准确性、处理速度和计算资源之间的平衡点，并突出了商业系统与学术研究之间的差距问题。


<details>
  <summary>Details</summary>
Motivation: 自主驾驶汽车对实时目标检测有极高要求，需要在检测准确性、处理速度和计算资源之间找到最佳平衡。目前商业系统与学术研究存在明显不对称信息，很多研究成果无法反映到实际产品中。

Method: 通过系统性调研当前最先进的实时目标检测算法（以CNN为主）及硬件加速器（主要为GPU和ASIC），分析其并行特性、部署能力和处理速度。

Result: 现有算法在硬件加速器上可达到每秒百张帧的处理速度，但对于自主驾驶汽车多摄像头的全面检测需求，仍需更多硬件和算法优化。研究发现商业系统与学术研究存在明显隔间。

Conclusion: 这篇评论填补了学术研究与商业自主驾驶汽车技术之间的知识差距，为未来全自主驾驶汽车的设计提供了实用参考。该论文是首次系统性地将商业应用情况纳入到学术评论中。

Abstract: The efficiency of object detectors depends on factors like detection
accuracy, processing time, and computational resources. Processing time is
crucial for real-time applications, particularly for autonomous vehicles (AVs),
where instantaneous responses are vital for safety. This review paper provides
a concise yet comprehensive survey of real-time object detection (OD)
algorithms for autonomous cars delving into their hardware accelerators (HAs).
Non-neural network-based algorithms, which use statistical image processing,
have been entirely substituted by AI algorithms, such as different models of
convolutional neural networks (CNNs). Their intrinsically parallel features led
them to be deployable into edge-based HAs of various types, where GPUs and, to
a lesser extent, ASIC (application-specific integrated circuit) remain the most
widely used. Throughputs of hundreds of frames/s (fps) could be reached;
however, handling object detection for all the cameras available in a typical
AV requires further hardware and algorithmic improvements. The intensive
competition between AV providers has limited the disclosure of algorithms,
firmware, and even hardware platform details. This remains a hurdle for
researchers, as commercial systems provide valuable insights while academics
undergo lengthy training and testing on restricted datasets and road scenarios.
Consequently, many AV research papers may not be reflected in end products,
being developed under limited conditions. This paper surveys state-of-the-art
OD algorithms and aims to bridge the gap with technologies in commercial AVs.
To our knowledge, this aspect has not been addressed in earlier surveys. Hence,
the paper serves as a tangible reference for researchers designing future
generations of vehicles, expected to be fully autonomous for comfort and
safety.

</details>
