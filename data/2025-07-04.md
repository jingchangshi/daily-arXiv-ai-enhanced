<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs](https://arxiv.org/abs/2507.02226)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: 论文提出DecoRTL，一种运行时解码策略，解决了LLM在RTL代码生成中的结构性和语义性问题，显著提升了生成代码的正确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统LLM解码策略在RTL代码生成中常产生无效或重复代码，无法区分语法关键区域和设计关键区域的需求。

Method: DecoRTL结合自一致性采样和语法感知温度调整，生成多候选并重排序，同时根据语法角色调整采样温度。

Result: 在VerilogEval基准测试中，DecoRTL显著提升了语法有效性、功能正确性和输出多样性，且性能开销可忽略。

Conclusion: DecoRTL无需额外微调，有效解决了LLM在RTL生成中的解码问题，具有实际应用价值。

Abstract: As one of their many applications, large language models (LLMs) have recently
shown promise in automating register transfer level (RTL) code generation.
However, conventional LLM decoding strategies, originally designed for natural
language, often fail to meet the structural and semantic demands of RTL,
leading to hallucinated, repetitive, or invalid code outputs. In this paper, we
first investigate the root causes of these decoding failures through an
empirical analysis of token-level entropy during RTL generation. Our findings
reveal that LLMs exhibit low confidence in regions of structural ambiguity or
semantic complexity, showing that standard decoding strategies fail to
differentiate between regions requiring determinism (syntax-critical regions)
and those that benefit from creative exploratory variability (design-critical
regions). Then, to overcome this, we introduce DecoRTL, a novel run-time
decoding strategy, that is both syntax-aware and contrastive for RTL code
generation. DecoRTL integrates two complementary components: (i)
self-consistency sampling, which generates multiple candidates and re-ranks
them based on token-level agreement to promote correctness while maintaining
diversity; and (ii) syntax-aware temperature adaptation, which classifies
tokens by their syntactical and functional roles and adjusts the sampling
temperature accordingly, enforcing low temperature for syntax-critical tokens
and higher temperature for exploratory ones. Our approach operates entirely at
inference time without requiring any additional model fine-tuning. Through
evaluations on multiple open-source LLMs using the VerilogEval benchmark, we
demonstrate significant improvements in syntactic validity, functional
correctness, and output diversity, while the execution overhead (performance
overhead) is imperceptible.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan](https://arxiv.org/abs/2507.02124)
*Fumikazu Konishi*

Main category: cs.DC

TL;DR: SAKURAONE是一个高性能计算集群，采用开放网络技术和GPU服务器，专为大型语言模型训练等高级工作负载优化，全球TOP500排名第49。


<details>
  <summary>Details</summary>
Motivation: 展示开放和供应商中立技术在大规模HPC基础设施中的可行性，并优化高性能计算资源。

Method: 采用800 GbE和SONiC操作系统构建开放网络堆栈，配备NVIDIA H100 GPU和全闪存存储子系统。

Result: 在HPL、HPCG和HPL-MxP基准测试中表现优异，分别达到33.95 PFLOP/s、396.295 TFLOP/s和339.86 PFLOP/s。

Conclusion: SAKURAONE证明了开放网络技术在高性能计算中的竞争力，适用于大规模并行工作负载。

Abstract: SAKURAONE is a managed high performance computing (HPC) cluster developed and
operated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU
PHY'' configuration of bare-metal GPU servers and is designed as a cluster
computing resource optimized for advanced workloads, including large language
model (LLM) training.
  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked
\textbf{49th} in the world based on its High Performance Linpack (HPL) score,
demonstrating its global competitiveness. In particular, it is the \textbf{only
system within the top 100} that employs a fully open networking stack based on
\textbf{800~GbE (Gigabit Ethernet)} and the \textbf{SONiC (Software for Open
Networking in the Cloud)} operating system, highlighting the viability of open
and vendor-neutral technologies in large-scale HPC infrastructure.
  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL
benchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate
Gradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets
low-precision workloads representative of AI applications, SAKURAONE delivered
an impressive 339.86~PFLOP/s using FP8 precision.
  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100
GPUs. It is supported by an all-flash Lustre storage subsystem with a total
physical capacity of 2~petabytes, providing high-throughput and low-latency
data access. Internode communication is enabled by a full-bisection bandwidth
interconnect based on a Rail-Optimized topology, where the Leaf and Spine
layers are interconnected via 800~GbE links. This topology, in combination with
RoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless
data transfers and mitigates communication bottlenecks in large-scale parallel
workloads.

</details>


### [3] [Signalling Health for Improved Kubernetes Microservice Availability](https://arxiv.org/abs/2507.02158)
*Jacob Roberts,Blair Archibald,Phil Trinder*

Main category: cs.DC

TL;DR: 论文比较了基于轮询（PCM）和基于信号（SCM）的容器监控方法，发现SCM在故障检测速度和准确性上优于PCM。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决PCM方法在容器健康监控中存在的调优困难、服务可用性降低和故障检测慢的问题。

Method: 设计并实现了基于信号的容器监控（SCM）方法，并通过数学模型和实验（使用SockShop基准）与PCM进行比较。

Result: SCM无需调优，故障检测速度比PCM快86%，且资源开销有限；PCM存在误检，导致服务可用性降低4%。

Conclusion: 建议容器编排器提供SCM功能，以实现更快、更准确的故障检测。

Abstract: Microservices are often deployed and managed by a container orchestrator that
can detect and fix failures to maintain the service availability critical in
many applications. In Poll-based Container Monitoring (PCM), the orchestrator
periodically checks container health. While a common approach, PCM requires
careful tuning, may degrade service availability, and can be slow to detect
container health changes. An alternative is Signal-based Container Monitoring
(SCM), where the container signals the orchestrator when its status changes. We
present the design, implementation, and evaluation of an SCM approach for
Kubernetes and empirically show that it has benefits over PCM, as predicted by
a new mathematical model. We compare the service availability of SCM and PCM
over six experiments using the SockShop benchmark. SCM does not require that
polling intervals are tuned, and yet detects container failure 86\% faster than
PCM and container readiness in a comparable time with limited resource
overheads. We find PCM can erroneously detect failures, and this reduces
service availability by 4\%. We propose that orchestrators offer SCM features
for faster failure detection than PCM without erroneous detections or careful
tuning.

</details>


### [4] [Domain-Adversarial Transfer Learning for Fault Root Cause Identification in Cloud Computing Systems](https://arxiv.org/abs/2507.02233)
*Bruce Fang,Danyi Gao*

Main category: cs.DC

TL;DR: 提出了一种基于迁移学习的智能算法，用于解决云计算环境中故障根因识别的挑战，通过共享特征提取和领域对抗机制提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 云计算环境中系统结构复杂、服务耦合紧密且故障信息有限，导致故障根因识别困难。

Method: 采用迁移学习，引入共享特征提取模块和领域对抗机制，结合伪标签选择策略增强模型在目标域的泛化能力。

Result: 实验表明，该方法在准确性、F1-Score和AUC等关键指标上优于主流方法，尤其在类别不平衡和异构节点环境下表现稳健。

Conclusion: 所提机制在复杂云计算系统中具有有效性和实用价值，验证了其在极端条件下的高性能。

Abstract: This paper addresses the challenge of fault root cause identification in
cloud computing environments. The difficulty arises from complex system
structures, dense service coupling, and limited fault information. To solve
this problem, an intelligent identification algorithm based on transfer
learning is proposed. The method introduces a shared feature extraction module
and a domain adversarial mechanism to enable effective knowledge transfer from
the source domain to the target domain. This improves the model's
discriminative ability and generalization performance in the target domain. The
model incorporates a pseudo-label selection strategy. When labeled samples are
lacking in the target domain, high-confidence predictions are used in training.
This enhances the model's ability to recognize minority classes. To evaluate
the stability and adaptability of the method in real-world scenarios,
experiments are designed under three conditions: label scarcity, class
imbalance, and heterogeneous node environments. Experimental results show that
the proposed method outperforms existing mainstream approaches in several key
metrics, including accuracy, F1-Score, and AUC. The model demonstrates stronger
discriminative power and robustness. Notably, under extreme class imbalance and
significant structural differences in the target domain, the model still
maintains high performance. This validates the effectiveness and practical
value of the proposed mechanisms in complex cloud computing systems.

</details>


### [5] [Flotilla: A scalable, modular and resilient federated learning framework for heterogeneous resources](https://arxiv.org/abs/2507.02295)
*Roopkatha Banerjee,Prince Modi,Jinal Vyas,Chunduru Sri Abhijit,Tejus Chandrashekar,Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.DC

TL;DR: Flotilla是一个轻量级、可扩展的联邦学习框架，支持同步和异步策略，具有容错能力，并在边缘设备上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习框架多关注模拟学习而非实际部署，且缺乏对异步聚合和容错的支持。

Method: Flotilla采用模块化设计，支持无状态客户端和分离会话状态的服务器设计，并支持检查点机制。

Result: Flotilla在200+客户端上展示了容错能力，并在边缘设备上资源使用优于现有框架。

Conclusion: Flotilla是构建和比较联邦学习策略的理想平台，适合系统研究和优化。

Abstract: With the recent improvements in mobile and edge computing and rising concerns
of data privacy, Federated Learning(FL) has rapidly gained popularity as a
privacy-preserving, distributed machine learning methodology. Several FL
frameworks have been built for testing novel FL strategies. However, most focus
on validating the learning aspects of FL through pseudo-distributed simulation
but not for deploying on real edge hardware in a distributed manner to
meaningfully evaluate the federated aspects from a systems perspective. Current
frameworks are also inherently not designed to support asynchronous
aggregation, which is gaining popularity, and have limited resilience to client
and server failures. We introduce Flotilla, a scalable and lightweight FL
framework. It adopts a ``user-first'' modular design to help rapidly compose
various synchronous and asynchronous FL strategies while being agnostic to the
DNN architecture. It uses stateless clients and a server design that separates
out the session state, which are periodically or incrementally checkpointed. We
demonstrate the modularity of Flotilla by evaluating five different FL
strategies for training five DNN models. We also evaluate the client and
server-side fault tolerance on 200+ clients, and showcase its ability to
rapidly failover within seconds. Finally, we show that Flotilla's resource
usage on Raspberry Pis and Nvidia Jetson edge accelerators are comparable to or
better than three state-of-the-art FL frameworks, Flower, OpenFL and FedML. It
also scales significantly better compared to Flower for 1000+ clients. This
positions Flotilla as a competitive candidate to build novel FL strategies on,
compare them uniformly, rapidly deploy them, and perform systems research and
optimizations.

</details>


### [6] [Alps, a versatile research infrastructure](https://arxiv.org/abs/2507.02404)
*Maxime Martinasso,Mark Klein,Thomas C. Schulthess*

Main category: cs.DC

TL;DR: CSCS开发了Alps，一种新型高性能计算基础设施，通过独立端点和高速网络设计，满足多样化的科学需求。


<details>
  <summary>Details</summary>
Motivation: 传统高性能计算架构缺乏灵活性和可组合性，无法满足日益多样化的科学需求。

Method: Alps采用异构硬件（CPU和GPU）和高性能Slingshot网络，结合模块化存储系统和软件定义的vCluster技术。

Result: Alps支持多种科学领域，如数值天气预报和AI研究，提供定制化平台。

Conclusion: Alps通过创新的架构和技术，成功解决了传统HPC的局限性，为科学计算提供了更灵活和可扩展的解决方案。

Abstract: The Swiss National Supercomputing Centre (CSCS) has a long-standing tradition
of delivering top-tier high-performance computing systems, exemplified by the
Piz Daint supercomputer. However, the increasing diversity of scientific needs
has exposed limitations in traditional vertically integrated HPC architectures,
which often lack flexibility and composability. To address these challenges,
CSCS developed Alps, a next-generation HPC infrastructure designed with a
transformative principle: resources operate as independent endpoints within a
high-speed network. This architecture enables the creation of independent
tenant-specific and platform-specific services, tailored to diverse scientific
requirements.
  Alps incorporates heterogeneous hardware, including CPUs and GPUs,
interconnected by a high-performance Slingshot network, and offers a modular
storage system. A key innovation is the versatile software-defined cluster
(vCluster) technology, which bridges cloud and HPC paradigms. By abstracting
infrastructure, service management, and user environments into distinct layers,
vClusters allow for customized platforms that support diverse workloads.
Current platforms on Alps serve various scientific domains, including numerical
weather prediction, and AI research.

</details>


### [7] [FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference](https://arxiv.org/abs/2507.02620)
*Xing Liu,Lizhuo Luo,Ming Tang,Chao Huang*

Main category: cs.DC

TL;DR: FlowSpec是一个基于流水线并行和树状推测解码的框架，旨在提高边缘网络中大型语言模型（LLM）的分布式推理效率。


<details>
  <summary>Details</summary>
Motivation: 边缘网络中稀疏的推理请求导致现有流水线方法利用率低，影响了推理延迟。

Method: FlowSpec采用三种机制：基于分数的逐步验证、高效的草稿管理和动态草稿扩展策略。

Result: 实验表明，FlowSpec在多种模型和配置下显著提升推理速度，加速比为1.36×-1.77×。

Conclusion: FlowSpec通过优化流水线利用率和推测效率，有效解决了边缘网络中LLM推理的挑战。

Abstract: Distributed inference serves as a promising approach to enabling the
inference of large language models (LLMs) at the network edge. It distributes
the inference process to multiple devices to ensure that the LLMs can fit into
the device memory. Recent pipeline-based approaches have the potential to
parallelize communication and computation, which helps reduce inference
latency. However, the benefit diminishes when the inference request at the
network edge is sparse, where pipeline is typically at low utilization. To
enable efficient distributed LLM inference at the edge, we propose
\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding
framework. FlowSpec incorporates three key mechanisms to improve decoding
efficiency: 1) score-based step-wise verification prioritizes more important
draft tokens to bring earlier accpeted tokens; 2) efficient draft management to
prune invalid tokens while maintaining correct causal relationship during
verification; 3) dynamic draft expansion strategies to supply high-quality
speculative inputs. These techniques work in concert to enhance both pipeline
utilization and speculative efficiency. We evaluate FlowSpec on a real-world
testbed with other baselines. Experimental results demonstrate that our
proposed framework significantly improves inference speed across diverse models
and configurations, achieving speedup ratios 1.36$\times$-1.77$\times$ compared
to baselines. Our code is publicly available at
\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\#}

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [Advanced Printed Sensors for Environmental Applications: A Path Towards Sustainable Monitoring Solutions](https://arxiv.org/abs/2507.02067)
*Nikolaos Papanikolaou,Doha Touhafi,Jurgen Vandendriessche,Danial Karimi,Sohail Fatimi,Gianluca Cornetta,Abdellah Touhafi*

Main category: cs.AR

TL;DR: 印刷传感器是一种创新的传感器技术，通过印刷技术制造灵活、低成本且高度可定制的传感设备，广泛应用于环境监测等领域。


<details>
  <summary>Details</summary>
Motivation: 传统传感器成本高且灵活性不足，印刷传感器技术旨在提供更经济、灵活且高效的解决方案。

Method: 采用创新的印刷技术制造传感器，实现高灵敏度和准确性。

Result: 传感器能够高精度检测污染物、温度变化、湿度等环境参数。

Conclusion: 印刷传感器在环境监测和保护中具有广阔的应用前景。

Abstract: Printed sensors represent a transformative advancement in sensor technology,
utilizing innovative printing techniques to create flexible, cost-effective,
and highly customizable sensing devices. Their versatility allows integration
into numerous applications across diverse fields such as monitoring a wide
range of environmental factors e.g. air and water quality, soil conditions, and
atmospheric changes among others. These sensors demonstrate high sensitivity
and accuracy in detecting pollutants, temperature variations, humidity levels,
and other critical parameters essential for environmental assessment and
protection.

</details>


### [9] [Hardware-Accelerated Algorithm for Complex Function Roots Density Graph Plotting](https://arxiv.org/abs/2507.02164)
*Ruibai Tang,Chengbin Quan*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA硬件加速的算法，用于绘制复函数根的密度图，通过多项式近似和单位移QR迭代求解根，性能优于CPU但略逊于GPU。


<details>
  <summary>Details</summary>
Motivation: 复函数根的求解和可视化在理论和应用领域都很重要，但计算量大，需要高效方法。

Method: 利用多项式近似复函数，通过单位移QR迭代求解根；利用Hessenberg结构的伴随矩阵和Givens旋转优化QR分解，设计流水线FPGA架构。

Result: 实现了比CPU高65倍的能效，性能略低于现代GPU。

Conclusion: 该硬件加速算法为复函数根的高效求解和可视化提供了可行方案。

Abstract: Solving and visualizing the potential roots of complex functions is essential
in both theoretical and applied domains, yet often computationally intensive.
We present a hardware-accelerated algorithm for complex function roots density
graph plotting by approximating functions with polynomials and solving their
roots using single-shift QR iteration. By leveraging the Hessenberg structure
of companion matrices and optimizing QR decomposition with Givens rotations, we
design a pipelined FPGA architecture capable of processing a large amount of
polynomials with high throughput. Our implementation achieves up to 65x higher
energy efficiency than CPU-based approaches, and while it trails modern GPUs in
performance due to differences in fabrication technique.

</details>


### [10] [System-performance and cost modeling of Large Language Model training and inference](https://arxiv.org/abs/2507.02456)
*Wenzhe Guo,Joyjit Kundu,Uras Tos,Weijiang Kong,Giuliano Sisto,Timon Evenblij,Manu Perumkunnil*

Main category: cs.AR

TL;DR: 本文提出了一种针对大语言模型（LLM）训练和推理的性能-成本建模方法，结合了先进的计算技术、内存优化和通信技术，以解决分布式系统中的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模和复杂度的指数增长，计算能力、内存带宽、网络性能和成本效率的提升未能跟上，导致分布式系统中的扩展性挑战。

Method: 提出了一种性能-成本建模方法，整合了闪存注意力技术和专家混合模型，同时考虑了不同网络拓扑和通信算法的影响。

Result: 该方法能够分析不同系统架构配置的性能-成本权衡，为未来计算系统设计和软硬件协同开发提供指导。

Conclusion: 该建模方法为解决LLM扩展性问题提供了有价值的工具，并促进了硬件-软件的协同优化。

Abstract: Large language models (LLMs), based on transformer architectures, have
revolutionized numerous domains within artificial intelligence, science, and
engineering due to their exceptional scalability and adaptability. However, the
exponential growth in LLM size and complexity has outpaced advancements in
compute capacity, memory bandwidth, network performance, and cost efficiency,
posing significant challenges to their scalability on distributed systems. To
address these limitations, alternative model architectures, optimization
strategies, communication-aware network topologies, and novel system design
approaches have been proposed in literature. This paper introduces a
performance-cost modeling methodology for LLM training and inference that
integrates state-of-the-art compute techniques with memory optimizations, and
latest communication techniques. Building on an analytical performance model,
our approach incorporates recent innovations such as the flash attention
technique and mixture of experts models to address the memory bandwidth and
compute bottlenecks. It also considers the impact of different network
topologies and topology-specific communication algorithms with 5D parallellism.
The framework also integrates a chiplet cost model. The proposed modeling
methodology provides valuable insights to guide future compute system design
and facilitates hardware-software co-development, in particular due to its
ability to analyze performance-cost trade-offs for various system architectural
configurations.

</details>


### [11] [AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models](https://arxiv.org/abs/2507.02598)
*Chenhao Xue,Kezhi Li,Jiaxing Zhang,Yi Ren,Zhengyuan Shi,Chen Zhang,Yibo Lin,Lining Zhang,Qiang Xu,Guangyu Sun*

Main category: cs.AR

TL;DR: AC-Refiner是一种基于条件扩散模型的算术电路优化框架，通过将电路合成视为条件图像生成任务，显著提升设计质量。


<details>
  <summary>Details</summary>
Motivation: 算术电路（如加法器和乘法器）对数字系统的性能、能效和面积有直接影响，但优化复杂且设计空间大。现有深度学习方法难以高效探索高潜力设计变体。

Method: 提出AC-Refiner框架，利用条件扩散模型，将电路合成转化为条件图像生成任务，并通过目标质量结果（QoRs）指导去噪扩散过程。

Result: 实验表明，AC-Refiner生成的电路设计具有更优的Pareto最优性，优于现有基线方法，并在实际应用中验证了性能提升。

Conclusion: AC-Refiner通过条件扩散模型有效优化算术电路设计，显著提升性能，为电路优化提供了新思路。

Abstract: Arithmetic circuits, such as adders and multipliers, are fundamental
components of digital systems, directly impacting the performance, power
efficiency, and area footprint. However, optimizing these circuits remains
challenging due to the vast design space and complex physical constraints.
While recent deep learning-based approaches have shown promise, they struggle
to consistently explore high-potential design variants, limiting their
optimization efficiency. To address this challenge, we propose AC-Refiner, a
novel arithmetic circuit optimization framework leveraging conditional
diffusion models. Our key insight is to reframe arithmetic circuit synthesis as
a conditional image generation task. By carefully conditioning the denoising
diffusion process on target quality-of-results (QoRs), AC-Refiner consistently
produces high-quality circuit designs. Furthermore, the explored designs are
used to fine-tune the diffusion model, which focuses the exploration near the
Pareto frontier. Experimental results demonstrate that AC-Refiner generates
designs with superior Pareto optimality, outperforming state-of-the-art
baselines. The performance gain is further validated by integrating AC-Refiner
into practical applications.

</details>


### [12] [Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference Infrastructure](https://arxiv.org/abs/2507.02654)
*Rui Xie,Asad Ul Haq,Yunhua Fang,Linsen Ma,Sanchari Sen,Swagath Venkataramani,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: 论文提出了一种系统级方法，通过将HBM的故障管理完全转移到内存控制器，并结合特定领域的ECC框架，显著降低了HBM的成本，同时保持了高性能和模型准确性。


<details>
  <summary>Details</summary>
Motivation: HBM的高成本限制了其在AI基础设施中的可扩展部署，尤其是由于严格的片上可靠性要求。

Method: 采用系统级方法，消除片上ECC，引入结合大码字RS纠错、轻量级CRC检测、差分奇偶校验更新和可调保护的ECC框架。

Result: 即使在HBM比特错误率高达$10^{-3}$的情况下，系统仍能保持78%的吞吐量和97%的模型准确性。

Conclusion: 通过将可靠性作为可调系统参数，该设计为低成本、高性能HBM在AI基础设施中的部署提供了新途径。

Abstract: High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy
efficiency for AI workloads, but its high cost per bit, driven in part by
stringent on-die reliability requirements, poses a growing barrier to scalable
deployment. This work explores a system-level approach to cost reduction by
eliminating on-die ECC and shifting all fault management to the memory
controller. We introduce a domain-specific ECC framework combining
large-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC
detection, differential parity updates to mitigate write amplification, and
tunable protection based on data importance. Our evaluation using LLM inference
workloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the
system retains over 78\% of throughput and 97\% of model accuracy compared with
systems equipped with ideal error-free HBM. By treating reliability as a
tunable system parameter rather than a fixed hardware constraint, our design
opens a new path toward low-cost, high-performance HBM deployment in AI
infrastructure.

</details>
