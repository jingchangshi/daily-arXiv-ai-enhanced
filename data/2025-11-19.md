<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 16]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Inside VOLT: Designing an Open-Source GPU Compiler](https://arxiv.org/abs/2511.13751)
*Shinnung Jeong,Chihyo Ahn,Huanzhi Pu,Jisheng Zhao,Hyesoon Kim,Blaise Tine*

Main category: cs.DC

TL;DR: VOLT是一个轻量级编译器工具链，旨在解决开源GPU架构中SIMT功能执行和性能优化的技术挑战，通过分层设计支持多抽象级别的代码生成和优化。


<details>
  <summary>Details</summary>
Motivation: 开源GPU架构虽然定义了SIMT功能，但执行现有GPU程序和优化性能需要复杂的编译器框架，这在开源硬件开发中常被低估。

Method: 采用分层设计，在中端集中SIMT相关分析和优化，支持多种前端语言和开源GPU硬件，确保可扩展性。

Result: VOLT能够支持SIMT代码生成和优化，并通过ISA扩展和主机运行时API的案例研究展示了其扩展能力。

Conclusion: VOLT为开源GPU架构提供了一个可扩展的编译器框架，能够适应不断发展的GPU架构需求。

Abstract: Recent efforts in open-source GPU research are opening new avenues in a domain that has long been tightly coupled with a few commercial vendors. Emerging open GPU architectures define SIMT functionality through their own ISAs, but executing existing GPU programs and optimizing performance on these ISAs relies on a compiler framework that is technically complex and often undercounted in open hardware development costs.
  To address this challenge, the Vortex-Optimized Lightweight Toolchain (VOLT) has been proposed. This paper presents its design principles, overall structure, and the key compiler transformations required to support SIMT execution on Vortex. VOLT enables SIMT code generation and optimization across multiple levels of abstraction through a hierarchical design that accommodates diverse front-end languages and open GPU hardware. To ensure extensibility as GPU architectures evolve, VOLT centralizes fundamental SIMT-related analyses and optimizations in the middle-end, allowing them to be reused across front-ends and easily adapted to emerging open-GPU variants. Through two case studies on ISA extensions and host-runtime API, this paper also demonstrates how VOLT can support extensions

</details>


### [2] [Boosting performance: Gradient Clock Synchronisation with two-way measured links](https://arxiv.org/abs/2511.13727)
*Sophie Wenning*

Main category: cs.DC

TL;DR: 该论文将GCS算法的形式模型扩展到实现近似的假设下，通过将单向测量范式替换为双向测量范式，移除了许多限制，同时保持了GCS的核心行为。


<details>
  <summary>Details</summary>
Motivation: 扩展GCS算法的形式模型以在实现近似的假设下运行，通过改变测量范式来移除先前工作中为证明性能而施加的限制，使模型更贴近实际部署。

Method: 将单向测量范式替换为双向测量范式，取消统一链路长度的要求，提供频率源的形式模型，对算法估计误差的不同组成部分进行细粒度区分。

Result: 显著降低了不确定性对算法估计误差的贡献，从每个链路延迟的数量级降低到每个链路延迟的10%到0.1%，并给出了GCS局部和全局偏差的匹配上界。

Conclusion: 通过采用双向测量范式，在保持GCS核心行为的同时，创建了更现实的模型，显著提高了算法性能，使GCS在实际部署中更加灵活可行。

Abstract: This master thesis extends the formal model of the GCS algorithm as presented by (Fan and Lynch 2004, 325), (Lenzen, Locher and Wattenhofer 2008, 510) and (Függer et al. 2023) to operate under implementation-near assumptions by replacing the one-way measurement paradigm assumed in prior work by the two-way measurement paradigm. With this change of paradigm, we remove many restrictions previously enforced to allow provable performance. Most notability, while maintaining the core behaviour of GCS, we: 1. Lift the requirement for unitary link lengths and thereby create a realistic model for flexible deployment of implementations of GCS in practice. 2. Provide a formal model of frequency sources assumed in prior work. 3. Perform a fine grained distinction between the different components of the algorithm's estimation error and globally reduce its impact by multiple orders of magnitude. 4. Significantly reduce the contribution of the uncertainty to the algorithm's estimation error to be in the range of 10\% to 0,1\% of the delay per link instead of being in the oder of the delay per link as in prior work and show matching upper bounds on the local and global skew of GCS.

</details>


### [3] [Gaia: Hybrid Hardware Acceleration for Serverless AI in the 3D Compute Continuum](https://arxiv.org/abs/2511.13728)
*Maximilian Reisecker,Cynthia Marcelino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Gaia是一个GPU即服务模型，通过动态执行模式识别和运行时评估，为异构环境中的无服务器AI工作负载提供SLO感知、成本高效的硬件加速。


<details>
  <summary>Details</summary>
Motivation: 当前平台在管理硬件加速方面存在困难，静态用户-设备分配无法在变化负载或位置下保证SLO合规性，一次性动态选择往往导致次优或成本低效的配置。

Method: Gaia结合轻量级执行模式识别器（在部署时检查函数代码并发出四种执行模式之一）和动态函数运行时（持续重新评估用户定义的SLO以在CPU和GPU后端之间进行升级或降级）。

Result: 评估显示Gaia能够无缝选择最适合工作负载的硬件加速，将端到端延迟降低高达95%。

Conclusion: Gaia能够在异构环境中为无服务器AI实现SLO感知、成本高效的加速。

Abstract: Serverless computing offers elastic scaling and pay-per-use execution, making it well-suited for AI workloads. As these workloads run in heterogeneous environments such as the Edge-Cloud-Space 3D Continuum, they often require intensive parallel computation, which GPUs can perform far more efficiently than CPUs. However, current platforms struggle to manage hardware acceleration effectively, as static user-device assignments fail to ensure SLO compliance under varying loads or placements, and one-time dynamic selections often lead to suboptimal or cost-inefficient configurations. To address these issues, we present Gaia, a GPU-as-a-service model and architecture that makes hardware acceleration a platform concern. Gaia combines (i) a lightweight Execution Mode Identifier that inspects function code at deploy time to emit one of four execution modes, and a Dynamic Function Runtime that continuously reevaluates user-defined SLOs to promote or demote between CPU- and GPU backends. Our evaluation shows that it seamlessly selects the best hardware acceleration for the workload, reducing end-to-end latency by up to 95%. These results indicate that Gaia enables SLO-aware, cost-efficient acceleration for serverless AI across heterogeneous environments.

</details>


### [4] [TT-Edge: A Hardware-Software Co-Design for Energy-Efficient Tensor-Train Decomposition on Edge AI](https://arxiv.org/abs/2511.13738)
*Hyunseok Kwak,Kyeongwon Lee,Kyeongpil Min,Chaebin Jung,Woojoo Lee*

Main category: cs.DC

TL;DR: TT-Edge是一个软硬件协同设计框架，通过在边缘AI处理器上专门设计TTD引擎来加速张量训练分解，实现1.7倍加速和40.2%能耗降低。


<details>
  <summary>Details</summary>
Motivation: 分布式学习在资源受限边缘设备上的需求增长，需要高效的设备端模型压缩。TTD虽然提供高压缩率和低精度损失，但重复的SVD和矩阵乘法在低功耗处理器上会产生显著的延迟和能耗成本。

Method: 将SVD分解为双对角化和对角化两个阶段，将计算密集型任务卸载到专门的TTD引擎。该引擎与现有GEMM加速器紧密集成，减少频繁的矩阵向量传输。采用轻量级设计，重用GEMM资源并使用共享浮点单元。

Result: 在RISC-V边缘AI处理器上实现，压缩ResNet-32模型时相比仅使用GEMM的基线获得1.7倍加速，总能耗降低40.2%，总功耗仅增加4%，硬件开销最小。

Conclusion: TT-Edge有效解决了边缘环境中基于TTD压缩的延迟和能耗瓶颈问题，在FPGA原型和45nm后合成功耗分析中验证了其有效性。

Abstract: The growing demands of distributed learning on resource constrained edge devices underscore the importance of efficient on device model compression. Tensor Train Decomposition (TTD) offers high compression ratios with minimal accuracy loss, yet repeated singular value decompositions (SVDs) and matrix multiplications can impose significant latency and energy costs on low power processors. In this work, we present TT-Edge, a hardware software co designed framework aimed at overcoming these challenges. By splitting SVD into two phases--bidiagonalization and diagonalization--TT-Edge offloads the most compute intensive tasks to a specialized TTD Engine. This engine integrates tightly with an existing GEMM accelerator, thereby curtailing the frequent matrix vector transfers that often undermine system performance and energy efficiency. Implemented on a RISC-V-based edge AI processor, TT-Edge achieves a 1.7x speedup compared to a GEMM only baseline when compressing a ResNet 32 model via TTD, while reducing overall energy usage by 40.2 percent. These gains come with only a 4 percent increase in total power and minimal hardware overhead, enabled by a lightweight design that reuses GEMM resources and employs a shared floating point unit. Our experimental results on both FPGA prototypes and post-synthesis power analysis at 45 nm demonstrate that TT-Edge effectively addresses the latency and energy bottlenecks of TTD based compression in edge environments.

</details>


### [5] [What happens when nanochat meets DiLoCo?](https://arxiv.org/abs/2511.13761)
*Alexander Acker,Soeren Becker,Sasho Nedelkoski,Dominik Scheinert,Odej Kao,Philipp Wiesner*

Main category: cs.DC

TL;DR: 该研究在通信受限的分布式环境中比较了DiLoCo算法与标准数据并行(DDP)训练方法，发现DiLoCo在预训练中表现稳定但下游任务性能较差，揭示了异步更新导致的不可逆表示漂移问题。


<details>
  <summary>Details</summary>
Motivation: 研究在通信受限的分布式环境中LLM训练引入的模型权衡，这些权衡在现有研究中尚未充分探索。

Method: 使用nanochat项目作为基线，实现DiLoCo算法作为轻量级包装器，执行多个本地步骤后与外部优化器同步，大幅减少通信量，并与标准DDP设置进行比较。

Result: DiLoCo在预训练中实现稳定收敛和竞争性损失，但在中期训练和SFT后产生较差的MMLU、GSM8K和HumanEval分数。使用DiLoCo预训练权重后即使改用DDP也无法恢复性能。

Conclusion: 异步更新导致不可逆的表示漂移，损害了下游任务的对齐能力，揭示了分布式训练中通信效率与模型质量之间的权衡。

Abstract: Although LLM training is typically centralized with high-bandwidth interconnects and large compute budgets, emerging methods target communication-constrained training in distributed environments. The model trade-offs introduced by this shift remain underexplored, and our goal is to study them.
  We use the open-source nanochat project, a compact 8K-line full-stack ChatGPT-like implementation containing tokenization, pretraining, fine-tuning, and serving, as a controlled baseline. We implement the DiLoCo algorithm as a lightweight wrapper over nanochat's training loop, performing multiple local steps per worker before synchronization with an outer optimizer, effectively reducing communication by orders of magnitude. This inner-outer training is compared against a standard data-parallel (DDP) setup. Because nanochat is small and inspectable, it enables controlled pipeline adaptations and allows direct comparison with the conventional centralized baseline.
  DiLoCo achieves stable convergence and competitive loss in pretraining but yields worse MMLU, GSM8K, and HumanEval scores after mid-training and SFT. We discover that using DiLoCo-pretrained weights and running mid- and post-training with DDP fails to recover performance, revealing irreversible representation drift from asynchronous updates that impairs downstream alignment. We provide this implementation as an official fork of nanochat on GitHub.

</details>


### [6] [Guaranteed DGEMM Accuracy While Using Reduced Precision Tensor Cores Through Extensions of the Ozaki Scheme](https://arxiv.org/abs/2511.13778)
*Angelika Schwarz,Anton Anders,Cole Brower,Harun Bayraktar,John Gunnels,Kate Clark,RuQing G. Xu,Samuel Rodriguez,Sebastien Cayrols,Paweł Tabaszewski,Victor Podlozhnyuk*

Main category: cs.DC

TL;DR: ADP框架利用低精度GPU单元通过Ozaki分解模拟FP64精度，通过硬件无关的ESC估计器确保精度，在保持FP64保真度的同时获得显著加速。


<details>
  <summary>Details</summary>
Motivation: 现代GPU硬件转向低精度格式(FP16/FP8/FP4)，传统FP64流水线吞吐量较低，需要利用低精度单元高效模拟双精度计算。

Method: 提出自动动态精度(ADP)框架，核心是ESC估计器确定分解参数，集成异常处理、运行时启发式和原生FP64回退，并改进Ozaki分解使用无符号整数切片方案。

Result: 在55位尾数设置下，相比原生FP64 GEMM在NVIDIA Blackwell GB200和RTX Pro 6000 Blackwell上分别获得2.3倍和13.2倍加速，运行时间开销小于10%。

Conclusion: 低精度加速器可以作为高保真、高性能科学计算工作负载的实用生产就绪基础。

Abstract: The rapid growth of artificial intelligence (AI) has made low-precision formats such as FP16, FP8, and, most recently, block-scaled FP4 the primary focus of modern GPUs, where Tensor Cores now deliver orders-of-magnitude higher throughput than traditional FP64 pipelines. This hardware shift has sparked a new line of algorithm research: using low-precision units to emulate double-precision accuracy through schemes such as Ozaki decompositions. We advance this direction with Automatic Dynamic Precision (ADP), a fully GPU-resident framework that makes emulated FP64 matrix multiplication both efficient and reliable. At its core is the Exponent Span Capacity (ESC), a hardware-agnostic estimator that conservatively determines the decomposition parameter (also known as slices) required to achieve FP64-level accuracy. Built on ESC, ADP integrates exception handling, run time heuristics, and seamless fallback to native FP64, ensuring correctness without host-device synchronization or user intervention. Additionally, we further improve Ozaki-style decompositions with an unsigned integer slicing scheme, which increases representational efficiency and reduces computational waste. Validated against recently proposed BLAS grading tests, ADP consistently preserves FP64 fidelity on challenging inputs while incurring less than 10% run time overhead. In a 55-bit mantissa setting, our approach achieves up to 2.3x and 13.2x speedups over native FP64 GEMM on NVIDIA Blackwell GB200 and the RTX Pro 6000 Blackwell Server Edition, respectively. Our results demonstrate that low-precision accelerators can serve as a practical, production-ready foundation for high-fidelity and high-performance scientific computing workloads.

</details>


### [7] [Semantic Multiplexing](https://arxiv.org/abs/2511.13779)
*Mohammad Abdi,Francesca Meneghello,Francesco Restuccia*

Main category: cs.DC

TL;DR: 提出了语义多路复用新概念，将多任务压缩表示合并为单一语义表示，可在不增加天线或带宽的情况下同时处理更多任务，显著降低延迟、能耗和通信负载。


<details>
  <summary>Details</summary>
Motivation: 现有通信系统仅支持比特级并行传输，限制了可并发处理的任务数量，需要解决这一瓶颈。

Method: 将流多路复用从比特级转移到任务级，通过将多个任务相关的压缩表示合并为单一语义表示来实现语义多路复用。

Result: 实验表明语义多路复用可在保持足够任务准确性的同时联合处理多个任务，图像分类准确率在4×4信道上从2个任务增加到8个任务时仅下降不到4%，相比基线延迟降低8倍、能耗降低25倍、通信负载降低54倍。

Conclusion: 语义多路复用通过扩展语义层的有效自由度，在不违反香农容量规则的前提下实现了更多任务的并发处理，显著提升了系统性能。

Abstract: Mobile devices increasingly require the parallel execution of several computing tasks offloaded at the wireless edge. Existing communication systems only support parallel transmissions at the bit level, which fundamentally limits the number of tasks that can be concurrently processed. To address this bottleneck, this paper introduces the new concept of Semantic Multiplexing. Our approach shifts stream multiplexing from bits to tasks by merging multiple task-related compressed representations into a single semantic representation. As such, Semantic Multiplexing can multiplex more tasks than the number of physical channels without adding antennas or widening bandwidth by extending the effective degrees of freedom at the semantic layer, without contradicting Shannon capacity rules. We have prototyped Semantic Multiplexing on an experimental testbed with Jetson Orin Nano and millimeter-wave software-defined radios and tested its performance on image classification and sentiment analysis while comparing to several existing baselines in semantic communications. Our experiments demonstrate that Semantic Multiplexing allows jointly processing multiple tasks at the semantic level while maintaining sufficient task accuracy. For example, image classification accuracy drops by less than 4% when increasing from 2 to 8 the number of tasks multiplexed over a 4$\times$4 channel. Semantic Multiplexing reduces latency, energy consumption, and communication load respectively by up to 8$\times$, 25$\times$, and 54$\times$ compared to the baselines while keeping comparable performance. We pledge to publicly share the complete software codebase and the collected datasets for reproducibility.

</details>


### [8] [Do MPI Derived Datatypes Actually Help? A Single-Node Cross-Implementation Study on Shared-Memory Communication](https://arxiv.org/abs/2511.13804)
*Temitayo Adefemi*

Main category: cs.DC

TL;DR: MPI派生数据类型(DDTs)的性能在不同实现中存在显著差异，没有单一策略在所有程序、通信语义和MPI库中表现最优，建议根据具体MPI实现和通信模式进行性能分析。


<details>
  <summary>Details</summary>
Motivation: MPI派生数据类型承诺简化非连续数据的通信，但实际性能表现存在争议且通常只针对单一MPI实现进行评估，需要进行跨实现评估。

Method: 使用三个2D应用(CFD求解器、生命游戏、图像重建)，每个应用编写两种版本：手动打包版本和DDT版本，在四种MPI实现上测试非阻塞点对点、邻居集合和持久操作等通信语义。

Result: 结果混合：DDTs在某些情况下最快(如图像重建在Intel MPI和MPICH上)，但在其他情况下最慢(如同一个代码在Open MPI和MVAPICH2上)。CFD求解器中手动版本普遍优于DDTs，而生命游戏中性能排名因MPI库而异。

Conclusion: DDTs的性能可移植性无法保证，建议在目标MPI实现和通信模式下同时分析DDT和手动打包设计的性能。

Abstract: MPI's derived datatypes (DDTs) promise easier, copy-free communication of non-contiguous data, yet their practical performance remains debated and is often reported only for a single MPI stack. We present a cross-implementation assessment using three 2D applications: a Jacobi CFD solver, Conway's Game of Life, and a lattice-based image reconstruction. Each application is written in two ways: (i) a BASIC version with manual packing and unpacking of non-contiguous regions and (ii) a DDT version using MPI_Type_vector and MPI_Type_create_subarray with correct true extent via MPI_Type_create_resized. For API parity, we benchmark identical communication semantics: non-blocking point-to-point (Irecv/Isend + Waitall), neighborhood collectives (MPI_Neighbor_alltoallw), and MPI-4 persistent operations (*_init). We run strong and weak scaling on 1-4 ranks, validate bitwise-identical halos, and evaluate four widely used MPI implementations: MPICH, Open MPI, Intel MPI, and MVAPICH2 on a single ARCHER2 node. Results are mixed. DDTs can be fastest, for example for the image reconstruction code on Intel MPI and MPICH, but can also be among the slowest on other stacks, such as Open MPI and MVAPICH2 for the same code. For the CFD solver, BASIC variants generally outperform DDTs across semantics, whereas for Game of Life the ranking flips depending on the MPI library. We also observe stack-specific anomalies, for example MPICH slowdowns with DDT neighborhood and persistent modes. Overall, no strategy dominates across programs, semantics, and MPI stacks; performance portability for DDTs is not guaranteed. We therefore recommend profiling both DDT-based and manual-packing designs under the intended MPI implementation and communication mode. Our study is limited to a single node and does not analyze memory overhead; multi-node and GPU-aware paths are left for future work.

</details>


### [9] [ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels](https://arxiv.org/abs/2511.13940)
*Stuart H. Sul,Simran Arora,Benjamin F. Spector,Christopher Ré*

Main category: cs.DC

TL;DR: ParallelKittens (PK) 是一个简化的 CUDA 框架，通过八个核心原语和统一编程模板，简化了重叠多 GPU 内核的开发，在 Hopper 和 Blackwell 架构上显著提升了各种并行工作负载的性能。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模扩大和硬件计算吞吐量提升超过互连带宽改进，GPU 间通信已成为现代 AI 工作负载的主要瓶颈。现有系统通过计算-通信重叠来缓解，但在异构工作负载和新加速器上往往无法达到理论峰值性能。

Method: PK 扩展了 ThunderKittens 框架，通过八个核心原语和统一编程模板体现多 GPU 内核设计原则，基于对数据传输机制、资源调度和设计开销等影响多 GPU 性能因素的综合分析。

Result: 在 Hopper 和 Blackwell 架构上验证，仅用不到 50 行设备代码，PK 实现了数据并行工作负载 2.33 倍加速、序列并行工作负载 4.08 倍加速、专家并行工作负载 1.22 倍加速。

Conclusion: PK 框架证明了一小组简单、可重用的原则可以系统指导最优多 GPU 内核设计，显著简化了重叠多 GPU 内核的开发过程。

Abstract: Inter-GPU communication has become a major bottleneck for modern AI workloads as models scale and improvements in hardware compute throughput outpace improvements in interconnect bandwidth. Existing systems mitigate this through compute-communication overlap but often fail to meet theoretical peak performance across heterogeneous workloads and new accelerators. Instead of operator-specific techniques, we ask whether a small set of simple, reusable principles can systematically guide the design of optimal multi-GPU kernels. We present ParallelKittens (PK), a minimal CUDA framework that drastically simplifies the development of overlapped multi-GPU kernels. PK extends the ThunderKittens framework and embodies the principles of multi-GPU kernel design through eight core primitives and a unified programming template, derived from a comprehensive analysis of the factors that govern multi-GPU performance$\unicode{x2014}$data-transfer mechanisms, resource scheduling, and design overheads. We validate PK on both Hopper and Blackwell architectures. With fewer than 50 lines of device code, PK achieves up to $2.33 \times$ speedup for data- and tensor-parallel workloads, $4.08 \times$ for sequence-parallel workloads, and $1.22 \times$ for expert-parallel workloads.

</details>


### [10] [FailSafe: High-performance Resilient Serving](https://arxiv.org/abs/2511.14116)
*Ziyi Xu,Zhiqiang Xie,Swapnil Gandhi,Christos Kozyrakis*

Main category: cs.DC

TL;DR: FailSafe是一个容错的张量并行服务系统，通过循环KV缓存放置、混合注意力和细粒度负载感知路由等技术，在GPU故障时维持高性能LLM推理。


<details>
  <summary>Details</summary>
Motivation: 传统张量并行(TP)在GPU故障时会导致执行中断、昂贵的KV缓存重计算以及长期的计算和内存不平衡问题。

Method: 采用循环KV缓存放置实现均匀内存利用，混合注意力消除滞后，细粒度负载感知路由动态平衡请求，以及主动KV缓存备份和按需权重恢复。

Result: 在8xH100 DGX系统上，相比标准容错方法，FailSafe实现了高达2倍的吞吐量和两个数量级更低的恢复延迟，即使最多三个GPU故障也能维持高性能。

Conclusion: FailSafe展示了在动态不可靠硬件条件下实现鲁棒高效LLM服务的能力。

Abstract: Tensor parallelism (TP) enables large language models (LLMs) to scale inference efficiently across multiple GPUs, but its tight coupling makes systems fragile: a single GPU failure can halt execution, trigger costly KVCache recomputation, and introduce long-term compute and memory imbalance. We present FailSafe, a fault-tolerant TP serving system that sustains high performance under irregular GPU availability. FailSafe introduces three techniques to balance computation and memory across GPUs: (1) Cyclic KVCache Placement for uniform memory utilization, (2) Hybrid Attention combining tensor- and data-parallel attention to eliminate stragglers, and (3) Fine-Grained Load-Aware Routing to dynamically balance requests. It further employs proactive KVCache backup and on-demand weight recovery to avoid expensive recomputation and redundant data transfers. We implement these techniques in a lightweight serving engine compatible with existing LLM infrastructures. Evaluated on an 8xH100 DGX system with real-world fault traces and representative workloads, FailSafe achieves up to 2x higher throughput and two orders of magnitude lower recovery latency compared to standard fault handling approaches. Even with up to three GPU failures, FailSafe sustains high throughput and balanced utilization, demonstrating robust and efficient LLM serving under dynamic and unreliable hardware conditions.

</details>


### [11] [10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training](https://arxiv.org/abs/2511.14124)
*Sabiha Afroz,Redwan Ibne Seraj Khan,Hadeel Albahar,Jingoo Han,Ali R. Butt*

Main category: cs.DC

TL;DR: 10Cache是一个资源感知的张量缓存和迁移系统，通过智能协调GPU、CPU和NVMe之间的内存使用，加速大型语言模型训练，解决GPU内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 云端训练大型语言模型面临GPU内存容量有限和成本高昂的瓶颈，现有GPU内存卸载方法存在高张量迁移延迟和设备内存利用不足的问题，导致训练时间增加和云成本上升。

Method: 10Cache通过分析张量执行顺序构建预取策略，基于张量大小分布在固定内存中分配内存缓冲区，并重用内存缓冲区以减少分配开销，实现跨GPU、CPU和NVMe层的内存智能协调。

Result: 在多样化LLM工作负载中，10Cache相比最先进的卸载方法，训练时间最高加速2倍，GPU缓存命中率提升最高86.6倍，CPU和GPU内存利用率分别提高最高2.15倍和1.33倍。

Conclusion: 10Cache是优化云端LLM训练吞吐量和资源效率的实用且可扩展的解决方案，能够显著提升内存效率并减少对高端GPU的依赖。

Abstract: Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.
  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.

</details>


### [12] [Hyperion: Hierarchical Scheduling for Parallel LLM Acceleration in Multi-tier Networks](https://arxiv.org/abs/2511.14450)
*Mulei Ma,Minrui Xu,Zihan Chen,Yang Yang,Tony Q. S. Quek*

Main category: cs.DC

TL;DR: Hyperion是一个分层两阶段框架，联合优化LLM在多层网络中的分区和调度，通过离线分区和在线调度显著降低端到端延迟。


<details>
  <summary>Details</summary>
Motivation: LLM在边缘、雾和云层的部署受限于GPU内存、异构计算和可变带宽，需要同时考虑模型分区和请求调度来最小化延迟。

Method: 第一阶段使用BSDP算法进行离线分区，平衡各阶段时间；第二阶段使用ARTS算法进行在线调度，根据实时队列长度和有效容量分配请求。

Result: 在多层推理任务中，Hyperion相比GPipe和HEFT基线分别降低延迟52.1%和31.2%，在长序列生成中保持44.5%的延迟优势。

Conclusion: Hyperion通过联合优化分区和调度，在多层网络中实现了显著的延迟降低和更好的GPU利用率，无需模型重训练且引入的运行时开销可忽略。

Abstract: Large Language Models (LLMs) are increasingly executed across edge, fog, and cloud tiers where limited GPU memory, heterogeneous compute, and variable inter-tier bandwidth jointly constrain deployment and motivate model partitioning and request scheduling. In this setting, achieving low end-to-end latency is governed not only by where a model is deployed (inter-tier model partitioning) but also by how incoming requests are scheduled (intra-tier task scheduling) across heterogeneous nodes. These two problems are tightly coupled, as a suboptimal scheduler can negate the benefits of a good partition, and vice versa. In this paper, we propose Hyperion, a hierarchical two-stage framework that jointly optimizes partitioning and scheduling to minimize end-to-end latency for pipelined LLM inference in multi-tier networks, balancing compute and memory across tiers while introducing negligible runtime overhead and requiring no model retraining. Motivated by the observation that partition choices evolve on slower timescales than request arrivals, Stage 1 performs offline, inter-tier partitioning via a Binary Search with Dynamic Programming (BSDP) procedure to produce balanced stage times under tier capacity and memory constraints; to adapt to time-varying load, Stage 2 performs online, intra-tier scheduling with a lightweight Adaptive Real-time Task Scheduling (ARTS) algorithm that maps each request to the best available node using real-time estimates of queue length and effective capacity. Experimental results on multi-tier inference tasks demonstrate that Hyperion significantly reduces end-to-end latency by up to 52.1\% and 31.2\%, with the Phi-3-medium model, compared to the GPipe and HEFT baselines, respectively. Furthermore, Hyperion shows superior scalability in long-sequence generation, maintaining a 44.5\% lower latency than GPipe and achieving higher GPU utilization.

</details>


### [13] [Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning](https://arxiv.org/abs/2511.14456)
*Fabian Stricker,David Bermbach,Christian Zirpins*

Main category: cs.DC

TL;DR: 本文研究了跨机构联邦学习中参与者失败对模型质量的影响，重点分析了失败时机、数据分布和评估方法等因素，发现在高度偏斜数据下评估结果过于乐观，且失败时机显著影响模型质量。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在跨机构场景中需要可靠系统，但参与者可能因各种原因失败。目前针对跨设备联邦学习的研究较多，但对跨机构联邦学习中参与者失败影响的研究相对较少。

Method: 通过广泛研究分析跨机构联邦学习中参与者失败对模型质量的影响，重点关注失败时机、数据分布和评估方法等关键因素。

Result: 研究表明在高度偏斜数据下评估结果过于乐观，掩盖了真实影响；同时参与者失败的时机显著影响最终训练模型的质量。

Conclusion: 研究结果为构建鲁棒联邦学习系统的研究人员和软件架构师提供了重要见解，强调了在跨机构联邦学习中考虑参与者失败影响的重要性。

Abstract: Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.
  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.

</details>


### [14] [Hapax Locks : Value-Based Mutual Exclusion](https://arxiv.org/abs/2511.14608)
*Dave Dice,Alex Kogan*

Main category: cs.DC

TL;DR: Hapax Locks是一种新颖的锁定算法，具有简单性、恒定时间到达和解锁路径、FIFO准入顺序、空间效率高且在竞争情况下产生较少一致性流量的特点。


<details>
  <summary>Details</summary>
Motivation: 开发一种性能与最先进锁算法相当，但对运行时环境约束更少、更容易集成到现有系统中的锁定算法。

Method: 提出Hapax Locks算法，该算法在线程间不转移指针所有权，简化了实现和集成过程。

Result: Hapax Locks在延迟和可扩展性方面与最先进的锁算法性能相当，同时减少了环境依赖和集成难度。

Conclusion: Hapax Locks是一种高效、易于集成的锁定算法，特别适合在现有系统或API下进行集成或改造。

Abstract: We present Hapax Locks, a novel locking algorithm that is simple, enjoys constant-time arrival and unlock paths, provides FIFO admission order, and which is also space efficient and generates relatively little coherence traffic under contention in the common case. Hapax Locks offer performance (both latency and scalability) that is comparable with the best state of the art locks, while at the same time Hapax Locks impose fewer constraints and dependencies on the ambient runtime environment, making them particularly easy to integrate or retrofit into existing systems or under existing application programming interfaces Of particular note, no pointers shift or escape ownership between threads in our algorithm.

</details>


### [15] [Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning](https://arxiv.org/abs/2511.14617)
*Ruoyu Qin,Weiran He,Weixiao Huang,Yangkun Zhang,Yikai Zhao,Bo Pang,Xinran Xu,Yingdi Shan,Yongwei Wu,Mingxing Zhang*

Main category: cs.DC

TL;DR: Seer是一个针对大语言模型强化学习的在线上下文学习系统，通过利用相同提示请求间的输出长度和生成模式相似性，解决了现有同步RL系统在rollout阶段的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有同步强化学习系统在rollout阶段面临严重的性能瓶颈，包括显著的长尾延迟和由于工作负载不平衡导致的资源利用率低下问题。

Method: Seer引入了三项关键技术：用于动态负载均衡的分割rollout、上下文感知调度和自适应分组推测解码，这些机制共同减少了长尾延迟并提高了rollout阶段的资源效率。

Result: 在生产级RL工作负载上的评估表明，与最先进的同步RL系统相比，Seer将端到端rollout吞吐量提高了74%到97%，并将长尾延迟降低了75%到93%。

Conclusion: Seer系统显著加速了RL训练迭代，通过利用请求间的相似性有效解决了rollout阶段的性能瓶颈问题。

Abstract: Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.

</details>


### [16] [Multi-GPU Quantum Circuit Simulation and the Impact of Network Performance](https://arxiv.org/abs/2511.14664)
*W. Michael Brown,Anurag Ramesh,Thomas Lubinski,Thien Nguyen,David E. Bernal Neira*

Main category: cs.DC

TL;DR: 该论文介绍了在QED-C基准测试中引入MPI以支持HPC系统基准测试，评估了多GPU互连技术对量子计算模拟性能的影响，发现互连性能提升比GPU架构改进对多GPU模拟有更大影响。


<details>
  <summary>Details</summary>
Motivation: 量子计算模拟在资源需求上非常苛刻，但对于算法开发、验证和硬件设计至关重要。多GPU模拟需要处理系统间通信瓶颈，因此需要评估互连技术对性能的影响。

Method: 在QED-C基准测试中引入MPI，使用各种互连路径进行基准测试，包括最新的NVIDIA Grace Blackwell NVL72架构，比较不同GPU架构和互连技术的性能。

Result: GPU架构改进带来了4.5倍以上的加速，但互连性能提升对多GPU模拟的影响更大，实现了超过16倍的解决方案时间性能改进。

Conclusion: 互连技术的进步对多GPU量子计算模拟性能有显著影响，比GPU架构改进贡献更大，NVIDIA Grace Blackwell等新技术有效扩展了跨节点的高带宽GPU专用互连。

Abstract: As is intrinsic to the fundamental goal of quantum computing, classical simulation of quantum algorithms is notoriously demanding in resource requirements. Nonetheless, simulation is critical to the success of the field and a requirement for algorithm development and validation, as well as hardware design. GPU-acceleration has become standard practice for simulation, and due to the exponential scaling inherent in classical methods, multi-GPU simulation can be required to achieve representative system sizes. In this case, inter-GPU communications can bottleneck performance. In this work, we present the introduction of MPI into the QED-C Application-Oriented Benchmarks to facilitate benchmarking on HPC systems. We review the advances in interconnect technology and the APIs for multi-GPU communication. We benchmark using a variety of interconnect paths, including the recent NVIDIA Grace Blackwell NVL72 architecture that represents the first product to expand high-bandwidth GPU-specialized interconnects across multiple nodes. We show that while improvements to GPU architecture have led to speedups of over 4.5X across the last few generations of GPUs, advances in interconnect performance have had a larger impact with over 16X performance improvements in time to solution for multi-GPU simulations.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [NL-DPE: An Analog In-memory Non-Linear Dot Product Engine for Efficient CNN and LLM Inference](https://arxiv.org/abs/2511.13950)
*Lei Zhao,Luca Buonanno,Archit Gajjar,John Moon,Aishwarya Natarajan,Sergey Serebryakov,Ron M. Roth,Xia Sheng,Youtao Zhang,Paolo Faraboschi,Jim Ignowski,Giacomo Pedretti*

Main category: cs.AR

TL;DR: NL-DPE是一种非线性点积引擎，通过结合RRAM交叉阵列和模拟内容可寻址存储器，在模拟域执行任意非线性函数和数据相关矩阵乘法，完全消除ADC需求，显著提升能效和速度。


<details>
  <summary>Details</summary>
Motivation: 传统RRAM内存计算加速器存在三个主要限制：仅支持静态点积运算、需要大功耗ADC电路、设备非理想性引入误差，这些限制了现代大语言模型的可扩展和准确加速。

Method: NL-DPE通过RRAM交叉阵列与模拟内容可寻址存储器(ACAM)结合，将非线性函数和数据相关乘法转换为决策树在模拟域执行，并使用软件噪声感知微调(NAF)解决设备噪声问题。

Result: 实验显示NL-DPE相比GPU基线实现28倍能效和249倍加速，相比现有IMC加速器实现22倍能效和245倍加速，同时保持高精度。

Conclusion: NL-DPE成功克服了传统RRAM加速器的关键限制，为现代大语言模型提供了可扩展、高能效的加速解决方案。

Abstract: Resistive Random Access Memory (RRAM) based in-memory computing (IMC) accelerators offer significant performance and energy advantages for deep neural networks (DNNs), but face three major limitations: (1) they support only \textit{static} dot-product operations and cannot accelerate arbitrary non-linear functions or data-dependent multiplications essential to modern LLMs; (2) they demand large, power-hungry analog-to-digital converter (ADC) circuits; and (3) mapping model weights to device conductance introduces errors from cell nonidealities. These challenges hinder scalable and accurate IMC acceleration as models grow.
  We propose NL-DPE, a Non-Linear Dot Product Engine that overcomes these barriers. NL-DPE augments crosspoint arrays with RRAM-based Analog Content Addressable Memory (ACAM) to execute arbitrary non-linear functions and data-dependent matrix multiplications in the analog domain by transforming them into decision trees, fully eliminating ADCs. To address device noise, NL-DPE uses software-based Noise Aware Fine-tuning (NAF), requiring no in-device calibration. Experiments show that NL-DPE delivers 28X energy efficiency and 249X speedup over a GPU baseline, and 22X energy efficiency and 245X speedup over existing IMC accelerators, while maintaining high accuracy.

</details>


### [18] [A Bit Level Weight Reordering Strategy Based on Column Similarity to Explore Weight Sparsity in RRAM-based NN Accelerator](https://arxiv.org/abs/2511.14202)
*Weiping Yang,Shilin Zhou,Hui Xu,Yujiao Nie,Qimin Zhou,Zhiwei Li,Changlin Chen*

Main category: cs.AR

TL;DR: 提出了一种位级权重重排序策略，能够在RRAM加速器中同时实现计算内存和权重稀疏性，解决了稀疏神经网络在CIM加速器中结构模式被破坏的问题。


<details>
  <summary>Details</summary>
Motivation: 计算内存(CIM)和权重稀疏性都是减少神经网络推理中数据移动的有效技术，但很难在同一加速器中同时使用，因为稀疏神经网络会破坏CIM所需的结构化计算模式。

Method: 采用位级权重重排序策略，将稀疏神经网络权重矩阵紧凑映射到RRAM加速器上。该方法将位稀疏性视为位相似性的特例，保留具有相同位值的列对中仅一列，然后将压缩后的权重矩阵映射到操作单元中。

Result: 在典型神经网络上的仿真结果显示，平均性能提升61.24%，在不同稀疏度下实现1.51x-2.52x的能耗节省，且与最先进设计相比仅有轻微开销。

Conclusion: 该位级重排序方法有效解决了CIM与权重稀疏性难以同时使用的问题，显著提升了RRAM加速器的性能和能效。

Abstract: Compute-in-Memory (CIM) and weight sparsity are two effective techniques to reduce data movement during Neural Network (NN) inference. However, they can hardly be employed in the same accelerator simultaneously because CIM requires structural compute patterns which are disrupted in sparse NNs. In this paper, we partially solve this issue by proposing a bit level weight reordering strategy which can realize compact mapping of sparse NN weight matrices onto Resistive Random Access Memory (RRAM) based NN Accelerators (RRAM-Acc). In specific, when weights are mapped to RRAM crossbars in a binary complement manner, we can observe that, which can also be mathematically proven, bit-level sparsity and similarity commonly exist in the crossbars. The bit reordering method treats bit sparsity as a special case of bit similarity, reserve only one column in a pair of columns that have identical bit values, and then map the compressed weight matrices into Operation Units (OU). The performance of our design is evaluated with typical NNs. Simulation results show a 61.24% average performance improvement and 1.51x-2.52x energy savings under different sparsity ratios, with only slight overhead compared to the state-of-the-art design.

</details>
