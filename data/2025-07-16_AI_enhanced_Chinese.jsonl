{"id": "2507.10573", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.10573", "abs": "https://arxiv.org/abs/2507.10573", "authors": ["Tianyu Ren", "Yajuan Du", "Jinhua Cui", "Yina Lv", "Qiao Li", "Chun Jason Xue"], "title": "Device-Level Optimization Techniques for Solid-State Drives: A Survey", "comment": null, "summary": "Solid-state drives (SSDs) have revolutionized data storage with their high\nperformance, energy efficiency, and reliability. However, as storage demands\ngrow, SSDs face critical challenges in scalability, endurance, latency, and\nsecurity. This survey provides a comprehensive analysis of SSD architecture,\nkey challenges, and device-level optimization techniques. We first examine the\nfundamental components of SSDs, including NAND flash memory structures, SSD\ncontroller functionalities (e.g., address mapping, garbage collection, wear\nleveling), and host interface protocols (SATA, SAS, NVMe). Next, we discuss\nmajor challenges such as reliability degradation, endurance limitations,\nlatency variations, and security threats (e.g., secure deletion, ransomware\ndefense). We then explore advanced optimization techniques, including error\ncorrection mechanisms, flash translation layer (FTL) enhancements, and emerging\narchitectures like zoned namespace (ZNS) SSDs and flexible data placement\n(FDP). Finally, we highlight open research challenges, such as QLC/PLC NAND\nscalability, performance-reliability trade-offs, and SSD optimizations for\nAI/LLM workloads. This survey aims to guide future research in developing\nnext-generation SSDs that balance performance, longevity, and security in\nevolving storage ecosystems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u56fa\u6001\u786c\u76d8\uff08SSD\uff09\u7684\u67b6\u6784\u3001\u5173\u952e\u6311\u6218\u53ca\u4f18\u5316\u6280\u672f\uff0c\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5b58\u50a8\u9700\u6c42\u7684\u589e\u957f\uff0cSSD\u5728\u53ef\u6269\u5c55\u6027\u3001\u8010\u4e45\u6027\u3001\u5ef6\u8fdf\u548c\u5b89\u5168\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u6df1\u5165\u5206\u6790\u548c\u4f18\u5316\u3002", "method": "\u901a\u8fc7\u5206\u6790SSD\u7684\u57fa\u672c\u7ec4\u4ef6\uff08\u5982NAND\u95ea\u5b58\u7ed3\u6784\u3001\u63a7\u5236\u5668\u529f\u80fd\u3001\u4e3b\u673a\u63a5\u53e3\u534f\u8bae\uff09\u548c\u4e3b\u8981\u6311\u6218\uff08\u5982\u53ef\u9760\u6027\u3001\u8010\u4e45\u6027\u3001\u5ef6\u8fdf\u3001\u5b89\u5168\u6027\uff09\uff0c\u63a2\u8ba8\u4e86\u4f18\u5316\u6280\u672f\uff08\u5982\u7ea0\u9519\u673a\u5236\u3001FTL\u589e\u5f3a\u3001\u65b0\u5174\u67b6\u6784\uff09\u3002", "result": "\u603b\u7ed3\u4e86SSD\u5f53\u524d\u7684\u6280\u672f\u73b0\u72b6\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff08\u5982QLC/PLC NAND\u6269\u5c55\u6027\u3001AI/LLM\u5de5\u4f5c\u8d1f\u8f7d\u4f18\u5316\uff09\u3002", "conclusion": "\u672c\u6587\u4e3a\u4e0b\u4e00\u4ee3SSD\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u65e8\u5728\u5e73\u8861\u6027\u80fd\u3001\u5bff\u547d\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2507.10799", "categories": ["cs.PL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.10799", "abs": "https://arxiv.org/abs/2507.10799", "authors": ["Tyler Hou", "Michael Arntzenius", "Max Willsey"], "title": "Stream programs are monoid homomorphisms with state", "comment": null, "summary": "We define a broad class of deterministic stream functions and show they can\nbe implemented as homomorphisms into a \"state\" monoid. The homomorphism laws\nare simpler than the conditions of previous semantic frameworks for stream\nprogram optimization, yet retain support for rich equational reasoning over\nexpressive dataflow programs, including sequential composition, parallel\ncomposition, and feedback. We demonstrate this using examples of partitioned\ndatabase joins, stratified negation, and a simplified model of TCP.", "AI": {"tldr": "\u8bba\u6587\u5b9a\u4e49\u4e86\u4e00\u7c7b\u786e\u5b9a\u6027\u6d41\u51fd\u6570\uff0c\u5e76\u8bc1\u660e\u5176\u53ef\u4f5c\u4e3a\u72b6\u6001\u5e7a\u534a\u7fa4\u540c\u6001\u5b9e\u73b0\u3002\u540c\u6001\u5b9a\u5f8b\u6bd4\u5148\u524d\u7684\u6d41\u7a0b\u5e8f\u4f18\u5316\u8bed\u4e49\u6846\u67b6\u66f4\u7b80\u5355\uff0c\u540c\u65f6\u652f\u6301\u4e30\u5bcc\u7684\u7b49\u5f0f\u63a8\u7406\u3002", "motivation": "\u65e8\u5728\u7b80\u5316\u6d41\u7a0b\u5e8f\u4f18\u5316\u7684\u8bed\u4e49\u6846\u67b6\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u590d\u6742\u6570\u636e\u6d41\u7a0b\u5e8f\uff08\u5982\u987a\u5e8f\u7ec4\u5408\u3001\u5e76\u884c\u7ec4\u5408\u548c\u53cd\u9988\uff09\u7684\u7b49\u5f0f\u63a8\u7406\u652f\u6301\u3002", "method": "\u901a\u8fc7\u5c06\u786e\u5b9a\u6027\u6d41\u51fd\u6570\u5b9e\u73b0\u4e3a\u72b6\u6001\u5e7a\u534a\u7fa4\u7684\u540c\u6001\uff0c\u7b80\u5316\u540c\u6001\u5b9a\u5f8b\u3002", "result": "\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u5206\u533a\u6570\u636e\u5e93\u8fde\u63a5\u3001\u5206\u5c42\u5426\u5b9a\u548c\u7b80\u5316TCP\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86\u6d41\u7a0b\u5e8f\u4f18\u5316\u7684\u8bed\u4e49\u6846\u67b6\uff0c\u540c\u65f6\u652f\u6301\u4e30\u5bcc\u7684\u7b49\u5f0f\u63a8\u7406\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6570\u636e\u6d41\u7a0b\u5e8f\u3002"}}
{"id": "2507.10639", "categories": ["cs.AR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.10639", "abs": "https://arxiv.org/abs/2507.10639", "authors": ["Simon Nau", "Jan Krummenauer", "Andr\u00e9 Zimmermann"], "title": "SPICEAssistant: LLM using SPICE Simulation Tools for Schematic Design of Switched-Mode Power Supplies", "comment": "11 pages, 10 figures", "summary": "State-of-the-art large language models (LLMs) show high performance across a\nwide range of tasks in many domains of science. In the field of electronic\ndesign automation (EDA), it is yet to be determined to what extent they are\ncapable to understand, adapt, and dimension electronic circuits. This paper\nfocuses on the application of LLMs to switched-mode power supply (SMPS) design\non printed circuit boards (PCBs). Particular challenges for LLMs in this\ncontext include their limited ability to interpret results from key simulation\ntools like SPICE and the multi-step design process. To address these\nchallenges, we suggest SPICEAssistant, a framework that provides a broad\nselection of tools to an LLM. The tools serve as an interface to SPICE,\nallowing the LLM to interact flexibly with the simulator to estimate the impact\nof its modifications to the circuit. To evaluate the performance of\nSPICEAssistant, we defined a benchmark consisting of 256 questions testing the\nability to adapt circuit netlists to fulfil different SMPS design tasks. The\nbenchmarking results show that simulation feedback effectively improves SMPS\ndesign capabilities of LLMs. An increasing number of simulation iterations\nleads to enhanced performance. The SPICEAssistant framework significantly\noutperforms the standalone LLM GPT-4o on the benchmark by approximately 38%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSPICEAssistant\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u5177\u63a5\u53e3\u8ba9LLM\u4e0eSPICE\u6a21\u62df\u5668\u4ea4\u4e92\uff0c\u63d0\u5347\u5176\u5728SMPS\u8bbe\u8ba1\u4e2d\u7684\u6027\u80fd\uff0c\u6bd4GPT-4o\u8868\u73b0\u9ad838%\u3002", "motivation": "\u63a2\u7d22LLM\u5728\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\uff08EDA\uff09\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662fSMPS\u8bbe\u8ba1\u4e2d\u7684\u6311\u6218\uff0c\u5982\u6a21\u62df\u5de5\u5177\u7ed3\u679c\u89e3\u6790\u548c\u591a\u6b65\u9aa4\u8bbe\u8ba1\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faSPICEAssistant\u6846\u67b6\uff0c\u4e3aLLM\u63d0\u4f9b\u5de5\u5177\u63a5\u53e3\uff0c\u4f7f\u5176\u80fd\u4e0eSPICE\u6a21\u62df\u5668\u7075\u6d3b\u4ea4\u4e92\uff0c\u4f18\u5316\u7535\u8def\u8bbe\u8ba1\u3002", "result": "\u5728256\u4e2a\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPICEAssistant\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\uff0c\u6a21\u62df\u53cd\u9988\u548c\u591a\u8f6e\u8fed\u4ee3\u589e\u5f3a\u8bbe\u8ba1\u80fd\u529b\u3002", "conclusion": "SPICEAssistant\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728SMPS\u8bbe\u8ba1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002"}}
{"id": "2507.11282", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.11282", "abs": "https://arxiv.org/abs/2507.11282", "authors": ["Ren\u00e9 Rydhof Hansen", "Andreas Stenb\u00e6k Larsen", "Aslan Askarov"], "title": "The downgrading semantics of memory safety", "comment": "56 pages, 27 figures", "summary": "Memory safety is traditionally characterized in terms of bad things that\ncannot happen, an approach that is often criticized as unprincipled. Prior work\nsuggest a connection between memory safety and noninterference, but no\nsatisfactory semantic notion of memory safety is currently known.\n  This work proposes a notion of gradual allocator independence that accurately\ncaptures many allocator-specific aspects of memory safety. We consider a\nlow-level language with access to an allocator that provides malloc and free\nprimitives in a flat memory model. Pointers are just integers, and as such it\nis trivial to write memory-unsafe programs. The basic intuition of gradual\nallocator independence is that of noninterference, namely that allocators must\nnot influence program execution. This intuition is refined in two important\nways to account for the allocators running out-of-memory and for programs to\nhave pointer-to-integer casts. The key insight of the definition is to treat\nthese extensions as forms of downgrading and give them satisfactory technical\ntreatment using the state-of-the-art information flow machinery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u6e10\u8fdb\u5206\u914d\u5668\u72ec\u7acb\u6027\u201d\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u51c6\u786e\u6355\u6349\u5185\u5b58\u5b89\u5168\u7684\u5206\u914d\u5668\u7279\u5b9a\u65b9\u9762\u3002", "motivation": "\u4f20\u7edf\u7684\u5185\u5b58\u5b89\u5168\u5b9a\u4e49\u901a\u5e38\u57fa\u4e8e\u8d1f\u9762\u4e8b\u4ef6\uff0c\u88ab\u8ba4\u4e3a\u7f3a\u4e4f\u539f\u5219\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u975e\u5e72\u6270\u7406\u8bba\u63d0\u4f9b\u66f4\u8bed\u4e49\u5316\u7684\u5185\u5b58\u5b89\u5168\u5b9a\u4e49\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4f4e\u7ea7\u8bed\u8a00\u548c\u5e73\u9762\u5185\u5b58\u6a21\u578b\uff0c\u901a\u8fc7malloc\u548cfree\u539f\u8bed\u64cd\u4f5c\u6307\u9488\uff08\u89c6\u4e3a\u6574\u6570\uff09\uff0c\u63d0\u51fa\u6e10\u8fdb\u5206\u914d\u5668\u72ec\u7acb\u6027\u7684\u6982\u5ff5\uff0c\u5e76\u5229\u7528\u4fe1\u606f\u6d41\u6280\u672f\u5904\u7406\u5185\u5b58\u4e0d\u8db3\u548c\u6307\u9488\u5230\u6574\u6570\u7684\u8f6c\u6362\u3002", "result": "\u6e10\u8fdb\u5206\u914d\u5668\u72ec\u7acb\u6027\u6210\u529f\u6355\u6349\u4e86\u5185\u5b58\u5b89\u5168\u7684\u5206\u914d\u5668\u7279\u5b9a\u65b9\u9762\uff0c\u5e76\u901a\u8fc7\u4fe1\u606f\u6d41\u6280\u672f\u89e3\u51b3\u4e86\u5185\u5b58\u4e0d\u8db3\u548c\u6307\u9488\u8f6c\u6362\u7684\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6e10\u8fdb\u5206\u914d\u5668\u72ec\u7acb\u6027\u4e3a\u5185\u5b58\u5b89\u5168\u63d0\u4f9b\u4e86\u66f4\u8bed\u4e49\u5316\u7684\u5b9a\u4e49\uff0c\u5e76\u5c55\u793a\u4e86\u4fe1\u606f\u6d41\u6280\u672f\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2507.10748", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.10748", "abs": "https://arxiv.org/abs/2507.10748", "authors": ["Jason Ho", "James A. Boyle", "Linshen Liu", "Andreas Gerstlauer"], "title": "LASANA: Large-scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration", "comment": null, "summary": "Neuromorphic systems using in-memory or event-driven computing are motivated\nby the need for more energy-efficient processing of artificial intelligence\nworkloads. Emerging neuromorphic architectures aim to combine traditional\ndigital designs with the computational efficiency of analog computing and novel\ndevice technologies. A crucial problem in the rapid exploration and co-design\nof such architectures is the lack of tools for fast and accurate modeling and\nsimulation. Typical mixed-signal design tools integrate a digital simulator\nwith an analog solver like SPICE, which is prohibitively slow for large\nsystems. By contrast, behavioral modeling of analog components is faster, but\nexisting approaches are fixed to specific architectures with limited energy and\nperformance modeling. In this paper, we propose LASANA, a novel approach that\nleverages machine learning to derive data-driven surrogate models of analog\nsub-blocks in a digital backend architecture. LASANA uses SPICE-level\nsimulations of a circuit to train ML models that predict circuit energy,\nperformance, and behavior at analog/digital interfaces. Such models can provide\nenergy and performance annotation on top of existing behavioral models or\nfunction as replacements to analog simulation. We apply LASANA to an analog\ncrossbar array and a spiking neuron circuit. Running MNIST and spiking MNIST,\nLASANA surrogates demonstrate up to three orders of magnitude speedup over\nSPICE, with energy, latency, and behavioral error less than 7%, 8%, and 2%,\nrespectively.", "AI": {"tldr": "LASANA\u662f\u4e00\u79cd\u5229\u7528\u673a\u5668\u5b66\u4e60\u5feb\u901f\u5efa\u6a21\u548c\u6a21\u62df\u795e\u7ecf\u5f62\u6001\u7cfb\u7edf\u4e2d\u6a21\u62df\u5b50\u5757\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u795e\u7ecf\u5f62\u6001\u7cfb\u7edf\u9700\u8981\u66f4\u9ad8\u6548\u7684AI\u5de5\u4f5c\u8d1f\u8f7d\u5904\u7406\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u5728\u5feb\u901f\u63a2\u7d22\u548c\u534f\u540c\u8bbe\u8ba1\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7SPICE\u7ea7\u6a21\u62df\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u9884\u6d4b\u7535\u8def\u80fd\u91cf\u3001\u6027\u80fd\u548c\u6a21\u62df/\u6570\u5b57\u63a5\u53e3\u884c\u4e3a\u3002", "result": "\u5728MNIST\u548c\u8109\u51b2MNIST\u4e0a\uff0cLASANA\u6bd4SPICE\u5feb\u4e09\u4e2a\u6570\u91cf\u7ea7\uff0c\u80fd\u91cf\u3001\u5ef6\u8fdf\u548c\u884c\u4e3a\u8bef\u5dee\u5206\u522b\u4f4e\u4e8e7%\u30018%\u548c2%\u3002", "conclusion": "LASANA\u4e3a\u795e\u7ecf\u5f62\u6001\u67b6\u6784\u7684\u5feb\u901f\u63a2\u7d22\u548c\u534f\u540c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2507.10849", "categories": ["cs.AR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10849", "abs": "https://arxiv.org/abs/2507.10849", "authors": ["Xinxin Wang", "Lixian Yan", "Shuhan Liu", "Luke Upton", "Zhuoqi Cai", "Yiming Tan", "Shengman Li", "Koustav Jana", "Peijing Li", "Jesse Cirimelli-Low", "Thierry Tambe", "Matthew Guthaus", "H. -S. Philip Wong"], "title": "OpenGCRAM: An Open-Source Gain Cell Compiler Enabling Design-Space Exploration for AI Workloads", "comment": null, "summary": "Gain Cell memory (GCRAM) offers higher density and lower power than SRAM,\nmaking it a promising candidate for on-chip memory in domain-specific\naccelerators. To support workloads with varying traffic and lifetime metrics,\nGCRAM also offers high bandwidth, ultra low leakage power and a wide range of\nretention times, which can be adjusted through transistor design (like\nthreshold voltage and channel material) and on-the-fly by changing the\noperating voltage. However, designing and optimizing GCRAM sub-systems can be\ntime-consuming. In this paper, we present OpenGCRAM, an open-source GCRAM\ncompiler capable of generating GCRAM bank circuit designs and DRC- and\nLVS-clean layouts for commercially available foundry CMOS, while also providing\narea, delay, and power simulations based on user-specified configurations\n(e.g., word size and number of words). OpenGCRAM enables fast, accurate,\ncustomizable, and optimized GCRAM block generation, reduces design time, ensure\nprocess compliance, and delivers performance-tailored memory blocks that meet\ndiverse application requirements.", "AI": {"tldr": "OpenGCRAM\u662f\u4e00\u4e2a\u5f00\u6e90GCRAM\u7f16\u8bd1\u5668\uff0c\u7528\u4e8e\u5feb\u901f\u751f\u6210\u4f18\u5316\u7684GCRAM\u7535\u8def\u8bbe\u8ba1\u548c\u5e03\u5c40\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u5e94\u7528\u9700\u6c42\u3002", "motivation": "GCRAM\u56e0\u5176\u9ad8\u5bc6\u5ea6\u3001\u4f4e\u529f\u8017\u548c\u53ef\u8c03\u4fdd\u7559\u65f6\u95f4\u6210\u4e3a\u52a0\u901f\u5668\u7247\u4e0a\u5185\u5b58\u7684\u7406\u60f3\u9009\u62e9\uff0c\u4f46\u8bbe\u8ba1\u548c\u4f18\u5316\u8fc7\u7a0b\u8017\u65f6\u3002", "method": "\u901a\u8fc7\u7528\u6237\u6307\u5b9a\u7684\u914d\u7f6e\uff08\u5982\u5b57\u5927\u5c0f\u548c\u5b57\u6570\uff09\uff0c\u751f\u6210\u7535\u8def\u8bbe\u8ba1\u3001DRC/LVS\u6e05\u6d01\u5e03\u5c40\uff0c\u5e76\u63d0\u4f9b\u9762\u79ef\u3001\u5ef6\u8fdf\u548c\u529f\u8017\u6a21\u62df\u3002", "result": "OpenGCRAM\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u51c6\u786e\u3001\u53ef\u5b9a\u5236\u548c\u4f18\u5316\u7684GCRAM\u5757\u751f\u6210\uff0c\u51cf\u5c11\u4e86\u8bbe\u8ba1\u65f6\u95f4\u5e76\u786e\u4fdd\u5de5\u827a\u5408\u89c4\u3002", "conclusion": "OpenGCRAM\u4e3a\u591a\u6837\u5316\u5e94\u7528\u9700\u6c42\u63d0\u4f9b\u4e86\u6027\u80fd\u5b9a\u5236\u7684\u5185\u5b58\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86GCRAM\u7684\u8bbe\u8ba1\u6548\u7387\u3002"}}
{"id": "2507.10757", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.10757", "abs": "https://arxiv.org/abs/2507.10757", "authors": ["Ryan Zarick", "Isaac Zhang", "Daniel Wong", "Thomas Kim", "Bryan Pellegrino", "Mignon Li", "Kelvin Wong"], "title": "FAFO: Over 1 million TPS on a single node running EVM while still Merkleizing every block", "comment": null, "summary": "Current blockchain execution throughput is limited by data contention,\nreducing execution layer parallelism. Fast Ahead-of-Formation Optimization\n(FAFO) is the first blockchain transaction scheduler to address this problem by\nreordering transactions before block formation for maximum concurrency. FAFO\nuses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts\nand schedule parallel transaction execution at high throughput and low\noverhead.\n  We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1\nmillion native ETH transfers per second and over half a million ERC20 transfers\nper second on a single node (Table 1), with 91% lower cost compared to\nstate-of-the-art sharded execution. Unlike many other existing high throughput\nblockchain execution clients, FAFO uses QMDB to Merkleize world state after\nevery block, enabling light clients and stateless validation for ZK-based\nvApps. FAFO scales with minimal synchronization overhead, scaling linearly with\nadditional CPU resources until it fully exploits the maximum parallelism of the\nunderlying transaction flow. FAFO proves that the high throughput necessary to\nsupport future decentralized applications can be achieved with a streamlined\nexecution layer and innovations in blockchain transaction scheduler design.\nFAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.", "AI": {"tldr": "FAFO\u662f\u4e00\u79cd\u533a\u5757\u94fe\u4ea4\u6613\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u91cd\u65b0\u6392\u5e8f\u4ea4\u6613\u4ee5\u63d0\u9ad8\u5e76\u884c\u6027\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7ade\u4e89\u5bfc\u81f4\u7684\u541e\u5410\u91cf\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u533a\u5757\u94fe\u6267\u884c\u541e\u5410\u91cf\u53d7\u9650\u4e8e\u6570\u636e\u7ade\u4e89\uff0c\u964d\u4f4e\u4e86\u6267\u884c\u5c42\u7684\u5e76\u884c\u6027\u3002", "method": "FAFO\u4f7f\u7528CPU\u4f18\u5316\u7684Bloom\u8fc7\u6ee4\u5668\u68c0\u6d4b\u51b2\u7a81\uff0c\u5e76\u9ad8\u6548\u8c03\u5ea6\u5e76\u884c\u4ea4\u6613\u6267\u884c\u3002", "result": "\u5728\u5355\u8282\u70b9\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d2110\u4e07\u6b21\u539f\u751fETH\u8f6c\u8d26\u548c50\u4e07\u6b21ERC20\u8f6c\u8d26\uff0c\u6210\u672c\u964d\u4f4e91%\u3002", "conclusion": "FAFO\u8bc1\u660e\u901a\u8fc7\u4f18\u5316\u6267\u884c\u5c42\u548c\u4ea4\u6613\u8c03\u5ea6\u5668\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u5b9e\u73b0\u652f\u6301\u672a\u6765\u53bb\u4e2d\u5fc3\u5316\u5e94\u7528\u7684\u9ad8\u541e\u5410\u91cf\u3002"}}
{"id": "2507.10912", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.10912", "abs": "https://arxiv.org/abs/2507.10912", "authors": ["Cunxi Yu"], "title": "Mapping Fusion: Improving FPGA Technology Mapping with ASIC Mapper", "comment": "7 pages. to appear at MLCAD 2025", "summary": "LUT (Look-Up Table) mapping is a critical step in FPGA logic synthesis, where\na logic network is transformed into a form that can be directly implemented\nusing the FPGA's LUTs. An FPGA LUT is a flexible digital memory structure that\ncan implement any logic function of a limited number of inputs, typically 4 to\n6 inputs, depending on the FPGA architecture. The goal of LUT mapping is to map\nthe Boolean network into LUTs, where each LUT can implement any function with a\nfixed number of inputs. In parallel to FPGA technology mapping, ASIC technology\nmapping maps the Boolean network to user-defined standard cells, which has\ntraditionally been developed separately from LUT mapping algorithms. However,\nin this work, our motivating examples demonstrate that ASIC technology mappers\ncan potentially improve the performance of LUT mappers, such that standard cell\nmapping and LUT mapping work in an incremental manner.\n  Therefore, we propose the FuseMap framework, which explores this opportunity\nto improve LUT mapping in the FPGA design flow by utilizing reinforcement\nlearning to make design-specific choices during cell selection. The\neffectiveness of FuseMap is evaluated on a wide range of benchmarks, different\ntechnology libraries, and technology mappers. The experimental results\ndemonstrate that FuseMap achieves higher mapping accuracy while reducing delay\nand area across diverse circuit designs collected from ISCAS 85/89, ITC/ISCAS\n99, VTR 8.0, and EPFL benchmarks.", "AI": {"tldr": "FuseMap\u6846\u67b6\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6539\u8fdbFPGA\u7684LUT\u6620\u5c04\uff0c\u7ed3\u5408ASIC\u6280\u672f\u6620\u5c04\uff0c\u63d0\u5347\u6027\u80fd\u548c\u9762\u79ef\u6548\u7387\u3002", "motivation": "ASIC\u6280\u672f\u6620\u5c04\u53ef\u80fd\u63d0\u5347LUT\u6620\u5c04\u6027\u80fd\uff0c\u4e24\u8005\u53ef\u534f\u540c\u5de5\u4f5c\u3002", "method": "\u63d0\u51faFuseMap\u6846\u67b6\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bbe\u8ba1\u7279\u5b9a\u9009\u62e9\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFuseMap\u63d0\u9ad8\u4e86\u6620\u5c04\u7cbe\u5ea6\uff0c\u51cf\u5c11\u4e86\u5ef6\u8fdf\u548c\u9762\u79ef\u3002", "conclusion": "FuseMap\u4e3aFPGA\u8bbe\u8ba1\u6d41\u7a0b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684LUT\u6620\u5c04\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10789", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.10789", "abs": "https://arxiv.org/abs/2507.10789", "authors": ["Aaron Jarmusch", "Nathan Graddon", "Sunita Chandrasekaran"], "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks", "comment": null, "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.", "AI": {"tldr": "\u672c\u6587\u5bf9NVIDIA Blackwell\u67b6\u6784\u8fdb\u884c\u4e86\u5fae\u67b6\u6784\u5206\u6790\uff0c\u901a\u8fc7\u5fae\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5176\u5173\u952e\u5b50\u7cfb\u7edf\uff0c\u5e76\u4e0eHopper\u67b6\u6784\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u79d1\u5b66\u7814\u7a76\u7684\u5feb\u901f\u53d1\u5c55\u9700\u8981\u66f4\u5f3a\u7684\u8ba1\u7b97\u80fd\u529b\uff0cGPU\u662f\u89e3\u51b3\u65b9\u6848\u4e4b\u4e00\u3002\u672c\u6587\u65e8\u5728\u6df1\u5165\u5206\u6790Blackwell\u67b6\u6784\u7684\u6027\u80fd\u7279\u5f81\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4f18\u5316\u4f9d\u636e\u3002", "method": "\u901a\u8fc7\u5fae\u57fa\u51c6\u6d4b\u8bd5\u7814\u7a76Blackwell\u67b6\u6784\u7684\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u3001\u7f13\u5b58\u884c\u4e3a\u548c\u8c03\u5ea6\u7ec6\u8282\uff0c\u5e76\u4e0eHopper\u67b6\u6784\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u63ed\u793a\u4e86Blackwell\u67b6\u6784\u7684\u5173\u952e\u8bbe\u8ba1\u7ec6\u8282\uff0c\u5305\u62ec\u7b2c5\u4ee3\u5f20\u91cf\u6838\u5fc3\u652f\u6301FP4\u548cFP6\u7cbe\u5ea6\uff0c\u5e76\u5c55\u793a\u4e86\u4e0eHopper\u67b6\u6784\u7684\u6027\u80fd\u5bf9\u6bd4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u8005\u4f18\u5316Blackwell\u5e73\u53f0\u4e0a\u7684\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u4e3aGPU\u67b6\u6784\u7814\u7a76\u8d21\u732e\u4e86\u65b0\u6570\u636e\u3002"}}
{"id": "2507.10971", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.10971", "abs": "https://arxiv.org/abs/2507.10971", "authors": ["Kshitij Raj", "Atri Chatterjee", "Patanjali SLPSK", "Swarup Bhunia", "Sandip Ray"], "title": "Security Enclave Architecture for Heterogeneous Security Primitives for Supply-Chain Attacks", "comment": null, "summary": "Designing secure architectures for system-on-chip (SoC) platforms is a highly\nintricate and time-intensive task, often requiring months of development and\nmeticulous verification. Even minor architectural oversights can lead to\ncritical vulnerabilities that undermine the security of the entire chip. In\nresponse to this challenge, we introduce CITADEL, a modular security framework\naimed at streamlining the creation of robust security architectures for SoCs.\nCITADEL offers a configurable, plug-and-play subsystem composed of custom\nintellectual property (IP) blocks, enabling the construction of diverse\nsecurity mechanisms tailored to specific threats. As a concrete demonstration,\nwe instantiate CITADEL to defend against supply-chain threats, illustrating how\nthe framework adapts to one of the most pressing concerns in hardware security.\nThis paper explores the range of obstacles encountered when building a unified\nsecurity architecture capable of addressing multiple attack vectors and\npresents CITADEL's strategies for overcoming them. Through several real-world\ncase studies, we showcase the practical implementation of CITADEL and present a\nthorough evaluation of its impact on silicon area and power consumption across\nvarious ASIC technologies. Results indicate that CITADEL introduces only\nminimal resource overhead, making it a practical solution for enhancing SoC\nsecurity.", "AI": {"tldr": "CITADEL\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u5b89\u5168\u6846\u67b6\uff0c\u65e8\u5728\u7b80\u5316SoC\u5b89\u5168\u67b6\u6784\u7684\u8bbe\u8ba1\uff0c\u901a\u8fc7\u53ef\u914d\u7f6e\u7684IP\u5757\u6784\u5efa\u5b9a\u5236\u5b89\u5168\u673a\u5236\uff0c\u8d44\u6e90\u5f00\u9500\u6781\u5c0f\u3002", "motivation": "SoC\u5b89\u5168\u67b6\u6784\u8bbe\u8ba1\u590d\u6742\u4e14\u6613\u51fa\u9519\uff0c\u9700\u8981\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faCITADEL\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u5757\u5316\u3001\u53ef\u914d\u7f6e\u7684IP\u5757\uff0c\u9002\u5e94\u591a\u79cd\u5a01\u80c1\u3002", "result": "CITADEL\u8d44\u6e90\u5f00\u9500\u5c0f\uff0c\u9002\u7528\u4e8e\u4e0d\u540cASIC\u6280\u672f\u3002", "conclusion": "CITADEL\u662f\u589e\u5f3aSoC\u5b89\u5168\u6027\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11067", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11067", "abs": "https://arxiv.org/abs/2507.11067", "authors": ["Yinuo Wang", "Tianqi Mao", "Lin Gan", "Wubing Wan", "Zeyu Song", "Jiayu Fu", "Lanke He", "Wenqiang Wang", "Zekun Yin", "Wei Xue", "Guangwen Yang"], "title": "MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix Unit", "comment": "Yinuo Wang and Tianqi Mao contributed equally to this work", "summary": "Matrix-accelerated stencil computation is a hot research topic, yet its\napplication to three-dimensional (3D) high-order stencils and HPC remains\nunderexplored. With the emergence of matrix units on multicore CPUs, we analyze\nmatrix-based acceleration strategies and tailor an optimal approach for 3D\nhigh-order stencils. We introduce algorithmic optimizations based on SIMD and\nmatrix units to address strided memory accesses, alignment conflicts, and\nredundant accesses. We propose memory optimizations to boost on-package memory\nefficiency, and a novel multi-thread parallelism paradigm to overcome\ndata-sharing challenges caused by the absence of shared data caches. MMStencil\nsustains consistently high hardware utilization across diverse stencil shapes\nand dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA\neffects and MPI limitations in hybrid parallelism. Combining all the\ninnovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100\nGPGPU by up to 2.1x. Moreover, the performance improvements translate directly\nto real-world HPC applications and enable RTM applications to yield 1.8x\nspeedup versus a highly optimized industrial Nvidia A100 GPGPU version.", "AI": {"tldr": "MMStencil\u901a\u8fc7\u77e9\u9635\u52a0\u901f\u7b56\u7565\u3001SIMD\u548c\u77e9\u9635\u5355\u5143\u4f18\u5316\u3001\u5185\u5b58\u4f18\u5316\u53ca\u591a\u7ebf\u7a0b\u5e76\u884c\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u9636\u6a21\u677f\u8ba1\u7b97\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u5e93\u548c\u5de5\u4e1a\u7ea7GPU\u5b9e\u73b0\u3002", "motivation": "\u7814\u7a763D\u9ad8\u9636\u6a21\u677f\u8ba1\u7b97\u5728HPC\u4e2d\u7684\u5e94\u7528\u4e0d\u8db3\uff0c\u5229\u7528\u591a\u6838CPU\u4e0a\u7684\u77e9\u9635\u5355\u5143\u63a2\u7d22\u52a0\u901f\u7b56\u7565\u3002", "method": "\u7ed3\u5408SIMD\u548c\u77e9\u9635\u5355\u5143\u4f18\u5316\u7b97\u6cd5\uff0c\u89e3\u51b3\u5185\u5b58\u8bbf\u95ee\u95ee\u9898\uff1b\u63d0\u51fa\u5185\u5b58\u4f18\u5316\u548c\u591a\u7ebf\u7a0b\u5e76\u884c\u8303\u5f0f\uff1b\u91c7\u7528DMA-based NUMA\u901a\u4fe1\u3002", "result": "MMStencil\u5728Nvidia A100 GPU\u4e0a\u6027\u80fd\u63d0\u53472.1\u500d\uff0c\u5b9e\u9645HPC\u5e94\u7528\u4e2dRTM\u901f\u5ea6\u63d0\u53471.8\u500d\u3002", "conclusion": "MMStencil\u901a\u8fc7\u7efc\u5408\u4f18\u5316\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u76843D\u9ad8\u9636\u6a21\u677f\u8ba1\u7b97\uff0c\u9002\u7528\u4e8e\u5b9e\u9645HPC\u573a\u666f\u3002"}}
{"id": "2507.11331", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11331", "abs": "https://arxiv.org/abs/2507.11331", "authors": ["Jiawei Lin", "Guokai Chen", "Yuanlong Li", "Thomas Bourgeat"], "title": "SystolicAttention: Fusing FlashAttention within a Single Systolic Array", "comment": null, "summary": "Transformer models rely heavily on scaled dot-product attention (SDPA),\ntypically implemented using the FlashAttention algorithm. However, current\nsystolic-array-based accelerators face significant challenges when executing\nFlashAttention. Systolic arrays can only achieve high utilization for\nconsecutive and large matrix multiplications. In contrast, FlashAttention\nrequires frequently interleaved matrix multiplications and softmax operations.\n  The frequent data swaps between the systolic array and external vector units\nresult in low systolic array utilization. This is further exacerbated by the\nfact that softmax involves numerous non-matrix operations, which are not\nwell-suited for systolic arrays. Moreover, the concurrent execution of matrix\nmultiplication on systolic arrays and softmax on vector units leads to register\nfile and SRAM port contention, further degrading performance.\n  To overcome these limitations, we propose FSA, an enhanced systolic array\narchitecture that enables the entire FlashAttention algorithm to run entirely\nwithin a single systolic array, eliminating the need for external vector units.\nAt the core of FSA is SystolicAttention, a novel scheduling algorithm that maps\nFlashAttention operations onto systolic arrays with fine-grained, element-wise\noverlap. This significantly improves array utilization while preserving the\noriginal floating-point operation order to maintain numerical stability.\n  We implement FSA in synthesizable RTL and evaluate its performance against\nstate-of-the-art commercial accelerators. Our results show that FSA achieves\n1.77x and 4.83x higher attention FLOPs/s utilization compared to AWS\nNeuronCore-v2 and Google TPUv5e, respectively, with only about 10% area\noverhead.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u8109\u52a8\u9635\u5217\u67b6\u6784FSA\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u8109\u52a8\u9635\u5217\u5728\u6267\u884cFlashAttention\u65f6\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u901a\u8fc7SystolicAttention\u8c03\u5ea6\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9635\u5217\u5229\u7528\u7387\u3002", "motivation": "\u5f53\u524d\u8109\u52a8\u9635\u5217\u5728\u6267\u884cFlashAttention\u65f6\u56e0\u9891\u7e41\u7684\u6570\u636e\u4ea4\u6362\u548c\u975e\u77e9\u9635\u64cd\u4f5c\u5bfc\u81f4\u5229\u7528\u7387\u4f4e\u4e0b\uff0cFSA\u65e8\u5728\u6d88\u9664\u5bf9\u5916\u90e8\u5411\u91cf\u5355\u5143\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faFSA\u67b6\u6784\u548cSystolicAttention\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5c06FlashAttention\u64cd\u4f5c\u6620\u5c04\u5230\u8109\u52a8\u9635\u5217\u4e0a\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u5143\u7d20\u7ea7\u91cd\u53e0\u6267\u884c\u3002", "result": "FSA\u5728\u6ce8\u610f\u529bFLOPs/s\u5229\u7528\u7387\u4e0a\u6bd4AWS NeuronCore-v2\u548cGoogle TPUv5e\u5206\u522b\u9ad8\u51fa1.77\u500d\u548c4.83\u500d\uff0c\u9762\u79ef\u5f00\u9500\u4ec5\u7ea610%\u3002", "conclusion": "FSA\u901a\u8fc7\u4f18\u5316\u8109\u52a8\u9635\u5217\u67b6\u6784\u548c\u8c03\u5ea6\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86FlashAttention\u7684\u6267\u884c\u6548\u7387\uff0c\u4e3aTransformer\u6a21\u578b\u7684\u52a0\u901f\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2507.11094", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11094", "abs": "https://arxiv.org/abs/2507.11094", "authors": ["Nibedita Behera", "Ashwina Kumar", "Atharva Chougule", "Mohammed Shan P S", "Rushabh Nirdosh Lalwani", "Rupesh Nasre"], "title": "Generating Dynamic Graph Algorithms for Multiple Backends for a Graph DSL", "comment": null, "summary": "With the rapid growth of unstructured and semistructured data, parallelizing\ngraph algorithms has become essential for efficiency. However, due to the\ninherent irregularity in computation, memory access patterns, and\ncommunication, graph algorithms are notoriously difficult to parallelize. To\naddress this challenge, several libraries, frameworks, and domain-specific\nlanguages (DSLs) have been proposed to ease the parallel programming burden for\ndomain experts. Existing frameworks partially or fully abstract away\nparallelism intricacies, provide intuitive scheduling mnemonics, and employ\nprogram analysis to identify data races and generate synchronization code.\nDespite these advances, most frameworks are limited in their abstractions and\nruntime optimizations, especially when dealing with static graphs. In contrast,\nmany real-world graphs are inherently dynamic, with evolving structures over\ntime through insertions, deletions, and modifications of vertices, edges, and\nattributes. Generating efficient and correctly synchronized code for such\ndynamic graph algorithms remains a significant challenge.\n  In this work, we introduce an abstraction scheme and runtime optimizations\nfor the efficient processing of morph algorithms. Specifically, given an\ninitial graph G and a set of updates $\\Delta$G involving edge insertions and\ndeletions, we express the dynamic processing logic through a DSL and\nautomatically generate parallel code targeting multicore, distributed, and\nmany-core environments. We demonstrate the effectiveness of our approach by\napplying the DSL-generated code to ten large graphs with diverse\ncharacteristics and three widely used algorithms: Shortest Paths, PageRank, and\nTriangle Counting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u62bd\u8c61\u65b9\u6848\u548c\u8fd0\u884c\u65f6\u4f18\u5316\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u52a8\u6001\u56fe\u7b97\u6cd5\uff0c\u901a\u8fc7DSL\u81ea\u52a8\u751f\u6210\u5e76\u884c\u4ee3\u7801\uff0c\u5e76\u5728\u591a\u6838\u3001\u5206\u5e03\u5f0f\u548c\u591a\u6838\u73af\u5883\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u975e\u7ed3\u6784\u5316\u548c\u534a\u7ed3\u6784\u5316\u6570\u636e\u7684\u5feb\u901f\u589e\u957f\uff0c\u5e76\u884c\u5316\u56fe\u7b97\u6cd5\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u52a8\u6001\u56fe\u7684\u9ad8\u6548\u548c\u6b63\u786e\u540c\u6b65\u4ee3\u7801\u751f\u6210\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5f15\u5165\u62bd\u8c61\u65b9\u6848\u548c\u8fd0\u884c\u65f6\u4f18\u5316\uff0c\u901a\u8fc7DSL\u8868\u8fbe\u52a8\u6001\u5904\u7406\u903b\u8f91\uff0c\u5e76\u81ea\u52a8\u751f\u6210\u9488\u5bf9\u591a\u6838\u3001\u5206\u5e03\u5f0f\u548c\u591a\u6838\u73af\u5883\u7684\u5e76\u884c\u4ee3\u7801\u3002", "result": "\u5728\u5341\u79cd\u5927\u578b\u56fe\u548c\u4e09\u79cd\u5e38\u7528\u7b97\u6cd5\uff08\u6700\u77ed\u8def\u5f84\u3001PageRank\u548c\u4e09\u89d2\u5f62\u8ba1\u6570\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u52a8\u6001\u56fe\u7b97\u6cd5\u7684\u9ad8\u6548\u5e76\u884c\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11506", "categories": ["cs.AR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11506", "abs": "https://arxiv.org/abs/2507.11506", "authors": ["Yiqi Liu", "Yuqi Xue", "Noelle Crawford", "Jilong Xue", "Jian Huang"], "title": "Elk: Exploring the Efficiency of Inter-core Connected AI Chips with Deep Learning Compiler Techniques", "comment": "This paper is accepted at the 58th IEEE/ACM International Symposium\n  on Microarchitecture (MICRO'25)", "summary": "To meet the increasing demand of deep learning (DL) models, AI chips are\nemploying both off-chip memory (e.g., HBM) and high-bandwidth low-latency\ninterconnect for direct inter-core data exchange. However, it is not easy to\nexplore the efficiency of these inter-core connected AI (ICCA) chips, due to a\nfundamental tussle among compute (per-core execution), communication\n(inter-core data exchange), and I/O (off-chip data access).\n  In this paper, we develop Elk, a DL compiler framework to maximize the\nefficiency of ICCA chips by jointly trading off all the three performance\nfactors discussed above. Elk structures these performance factors into\nconfigurable parameters and forms a global trade-off space in the DL compiler.\nTo systematically explore this space and maximize overall efficiency, Elk\nemploys a new inductive operator scheduling policy and a cost-aware on-chip\nmemory allocation algorithm. It generates globally optimized execution plans\nthat best overlap off-chip data loading and on-chip execution. To examine the\nefficiency of Elk, we build a full-fledged emulator based on a real ICCA chip\nIPU-POD4, and an ICCA chip simulator for sensitivity analysis with different\ninterconnect network topologies. Elk achieves 94% of the ideal roofline\nperformance of ICCA chips on average, showing the benefits of supporting large\nDL models on ICCA chips. We also show Elk's capability of enabling architecture\ndesign space exploration for new ICCA chip development.", "AI": {"tldr": "Elk\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\u5668\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8ba1\u7b97\u3001\u901a\u4fe1\u548cI/O\u6027\u80fd\uff0c\u6700\u5927\u5316ICCA\u82af\u7247\u7684\u6548\u7387\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u6c42\u7684\u589e\u957f\uff0cICCA\u82af\u7247\u9762\u4e34\u8ba1\u7b97\u3001\u901a\u4fe1\u548cI/O\u4e4b\u95f4\u7684\u6027\u80fd\u6743\u8861\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "Elk\u901a\u8fc7\u53ef\u914d\u7f6e\u53c2\u6570\u6784\u5efa\u5168\u5c40\u6743\u8861\u7a7a\u95f4\uff0c\u91c7\u7528\u65b0\u7684\u5f52\u7eb3\u8fd0\u7b97\u7b26\u8c03\u5ea6\u7b56\u7565\u548c\u6210\u672c\u611f\u77e5\u7684\u7247\u4e0a\u5185\u5b58\u5206\u914d\u7b97\u6cd5\uff0c\u751f\u6210\u5168\u5c40\u4f18\u5316\u7684\u6267\u884c\u8ba1\u5212\u3002", "result": "Elk\u5728ICCA\u82af\u7247\u4e0a\u5b9e\u73b0\u4e86\u5e73\u574794%\u7684\u7406\u60f3\u6027\u80fd\uff0c\u652f\u6301\u5927\u578bDL\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u67b6\u6784\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u7684\u80fd\u529b\u3002", "conclusion": "Elk\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6027\u80fd\u56e0\u7d20\uff0c\u663e\u8457\u63d0\u5347\u4e86ICCA\u82af\u7247\u7684\u6548\u7387\uff0c\u5e76\u4e3a\u65b0\u82af\u7247\u5f00\u53d1\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u5de5\u5177\u3002"}}
{"id": "2507.11165", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11165", "abs": "https://arxiv.org/abs/2507.11165", "authors": ["Shixun Wu", "Jinwen Pan", "Jinyang Liu", "Jiannan Tian", "Ziwei Qiu", "Jiajun Huang", "Kai Zhao", "Xin Liang", "Sheng Di", "Zizhong Chen", "Franck Cappello"], "title": "Boosting Scientific Error-Bounded Lossy Compression through Optimized Synergistic Lossy-Lossless Orchestration", "comment": "accepted by SC '25", "summary": "As high-performance computing architectures evolve, more scientific computing\nworkflows are being deployed on advanced computing platforms such as GPUs.\nThese workflows can produce raw data at extremely high throughputs, requiring\nurgent high-ratio and low-latency error-bounded data compression solutions. In\nthis paper, we propose cuSZ-Hi, an optimized high-ratio GPU-based scientific\nerror-bounded lossy compressor with a flexible, domain-irrelevant, and fully\nopen-source framework design. Our novel contributions are: 1) We maximally\noptimize the parallelized interpolation-based data prediction scheme on GPUs,\nenabling the full functionalities of interpolation-based scientific data\nprediction that are adaptive to diverse data characteristics; 2) We thoroughly\nexplore and investigate lossless data encoding techniques, then craft and\nincorporate the best-fit lossless encoding pipelines for maximizing the\ncompression ratio of cuSZ-Hi; 3) We systematically evaluate cuSZ-Hi on\nbenchmarking datasets together with representative baselines. Compared to\nexisting state-of-the-art scientific lossy compressors, with comparative or\nbetter throughput than existing high-ratio scientific error-bounded lossy\ncompressors on GPUs, cuSZ-Hi can achieve up to 249% compression ratio\nimprovement under the same error bound, and up to 215% compression ratio\nimprovement under the same decompression data PSNR.", "AI": {"tldr": "cuSZ-Hi\u662f\u4e00\u79cd\u4f18\u5316\u7684GPU\u79d1\u5b66\u6570\u636e\u538b\u7f29\u5668\uff0c\u652f\u6301\u9ad8\u538b\u7f29\u6bd4\u548c\u4f4e\u5ef6\u8fdf\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\u53d1\u5c55\u4e0b\uff0c\u79d1\u5b66\u8ba1\u7b97\u5de5\u4f5c\u6d41\u9700\u8981\u9ad8\u6548\u7684\u6570\u636e\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f18\u5316\u5e76\u884c\u63d2\u503c\u6570\u636e\u9884\u6d4b\u65b9\u6848\uff0c\u63a2\u7d22\u65e0\u635f\u7f16\u7801\u6280\u672f\uff0c\u7cfb\u7edf\u8bc4\u4f30\u6027\u80fd\u3002", "result": "cuSZ-Hi\u5728\u76f8\u540c\u8bef\u5dee\u4e0b\u538b\u7f29\u6bd4\u63d0\u5347249%\uff0c\u76f8\u540cPSNR\u4e0b\u63d0\u5347215%\u3002", "conclusion": "cuSZ-Hi\u5728\u538b\u7f29\u6bd4\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6570\u636e\u7279\u5f81\u3002"}}
{"id": "2507.11289", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.11289", "abs": "https://arxiv.org/abs/2507.11289", "authors": ["Martin Rose", "Simon Homes", "Lukas Ramsperger", "Jose Gracia", "Christoph Niethammer", "Jadran Vrabec"], "title": "Cyclic Data Streaming on GPUs for Short Range Stencils Applied to Molecular Dynamics", "comment": "Accepted for publication at HeteroPar 2025 co-located with Euro-Par\n  2025", "summary": "In the quest for highest performance in scientific computing, we present a\nnovel framework that relies on high-bandwidth communication between GPUs in a\ncompute cluster. The framework offers linear scaling of performance for\nexplicit algorithms that is only limited by the size of the dataset and the\nnumber of GPUs. Slices of the dataset propagate in a ring of processes (GPUs)\nfrom one GPU, where they are processed, to the next, which results in a\nparallel-in-time parallelization. The user of the framework has to write GPU\nkernels that implement the algorithm and provide slices of the dataset.\nKnowledge about the underlying parallelization strategy is not required because\nthe communication between processes is carried out by the framework. As a case\nstudy, molecular dynamics simulation based on the Lennard-Jones potential is\nimplemented to measure the performance for a homogeneous fluid. Single node\nperformance and strong scaling behavior of this framework is compared to\nLAMMPS, which is outperformed in the strong scaling case.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u96c6\u7fa4\u7684\u9ad8\u5e26\u5bbd\u901a\u4fe1\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6027\u80fd\u79d1\u5b66\u8ba1\u7b97\uff0c\u5b9e\u73b0\u4e86\u663e\u5f0f\u7b97\u6cd5\u7684\u7ebf\u6027\u6027\u80fd\u6269\u5c55\u3002", "motivation": "\u8ffd\u6c42\u79d1\u5b66\u8ba1\u7b97\u7684\u6700\u9ad8\u6027\u80fd\uff0c\u89e3\u51b3GPU\u96c6\u7fa4\u4e2d\u9ad8\u6548\u901a\u4fe1\u548c\u5e76\u884c\u5316\u7684\u95ee\u9898\u3002", "method": "\u6846\u67b6\u901a\u8fc7\u73af\u5f62\u901a\u4fe1\u5728GPU\u95f4\u4f20\u9012\u6570\u636e\u5207\u7247\uff0c\u5b9e\u73b0\u65f6\u95f4\u5e76\u884c\u5316\uff0c\u7528\u6237\u53ea\u9700\u7f16\u5199GPU\u5185\u6838\u800c\u65e0\u9700\u4e86\u89e3\u5e95\u5c42\u5e76\u884c\u7b56\u7565\u3002", "result": "\u5728\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\uff0c\u8be5\u6846\u67b6\u5728\u5f3a\u6269\u5c55\u6027\u80fd\u4e0a\u4f18\u4e8eLAMMPS\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u6027\u80fd\u79d1\u5b66\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11386", "categories": ["cs.DC", "65M50, 65N50"], "pdf": "https://arxiv.org/pdf/2507.11386", "abs": "https://arxiv.org/abs/2507.11386", "authors": ["Carsten Burstedde", "Mikhail Kirilin", "Robert Kl\u00f6fkorn"], "title": "A new Dune grid for scalable dynamic adaptivity based on the p4est software library", "comment": "27 pages, 8 figures, 2 algorithms", "summary": "In this work we extend the Dune solver library with another grid interface to\nthe open-source p4est software. While Dune already supports about a dozen\ndifferent mesh implementations through its mesh interface Dune-Grid, we\nundertake this new coupling effort in order to inherit p4est's practically\nunlimited MPI scalability as well as its relatively thin data structures, and\nits native support for multi-block (forest) mesh topologies in both 2D and 3D.\n  The presented implementation is compared to an existing implementation based\non Dune-ALUGrid for a variety of challenging test examples in a parallel\nenvironment. The numerical experiments show that the implementation presented\nhere is outperforming Dune-ALUGrid in terms of scalability. In addition, an\nalternative balancing strategy is presented to ensure 2:1 balancing across\nelement faces showing improved performance compared to the existing p4est\nbalance strategy in the numerical examples considered in this work.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86Dune\u6c42\u89e3\u5668\u5e93\uff0c\u65b0\u589e\u4e86\u5bf9\u5f00\u6e90p4est\u8f6f\u4ef6\u7684\u7f51\u683c\u63a5\u53e3\uff0c\u4ee5\u7ee7\u627f\u5176\u5353\u8d8a\u7684MPI\u53ef\u6269\u5c55\u6027\u548c\u591a\u5757\u7f51\u683c\u652f\u6301\u3002", "motivation": "\u5229\u7528p4est\u7684\u65e0\u9650MPI\u53ef\u6269\u5c55\u6027\u3001\u8f7b\u91cf\u6570\u636e\u7ed3\u6784\u548c\u539f\u751f\u591a\u5757\u7f51\u683c\u652f\u6301\uff0c\u5f25\u8865Dune\u73b0\u6709\u7f51\u683c\u63a5\u53e3\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7Dune-Grid\u63a5\u53e3\u5b9e\u73b0\u4e0ep4est\u7684\u8026\u5408\uff0c\u5e76\u4e0e\u57fa\u4e8eDune-ALUGrid\u7684\u73b0\u6709\u5b9e\u73b0\u8fdb\u884c\u5e76\u884c\u73af\u5883\u4e0b\u7684\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "\u65b0\u5b9e\u73b0\u7684\u53ef\u6269\u5c55\u6027\u4f18\u4e8eDune-ALUGrid\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u5e73\u8861\u7b56\u7565\uff0c\u6027\u80fd\u4f18\u4e8ep4est\u539f\u6709\u7b56\u7565\u3002", "conclusion": "\u65b0\u63a5\u53e3\u663e\u8457\u63d0\u5347\u4e86Dune\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7f51\u683c\u62d3\u6251\u7684\u5e76\u884c\u8ba1\u7b97\u3002"}}
{"id": "2507.11417", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11417", "abs": "https://arxiv.org/abs/2507.11417", "authors": ["Miray \u00d6zcan", "Philipp Wiesner", "Philipp Wei\u00df", "Odej Kao"], "title": "Quantifying the Energy Consumption and Carbon Emissions of LLM Inference via Simulations", "comment": "Presented at the Workshop on Performance and Energy Efficiency in\n  Concurrent and Distributed Systems (PECS) at Euro-PAR'25", "summary": "The environmental impact of Large Language Models (LLMs) is rising\nsignificantly, with inference now accounting for more than half of their total\nlifecycle carbon emissions. However, existing simulation frameworks, which are\nincreasingly used to determine efficient LLM deployments, lack any concept of\npower and, therefore, cannot accurately estimate inference-related emissions.\nWe present a simulation framework to assess the energy and carbon implications\nof LLM inference under varying deployment setups. First, we extend a\nhigh-fidelity LLM inference simulator with a GPU power model that estimates\npower consumption based on utilization metrics, enabling analysis across\nconfigurations like batch size, sequence length, and model parallelism. Second,\nwe integrate simulation outputs into an energy system co-simulation environment\nto quantify carbon emissions under specific grid conditions and explore the\npotential of carbon-aware scheduling. Through scenario-based analysis, our\nframework reveals how inference parameters affect energy demand and carbon\nfootprint, demonstrates a renewable offset potential of up to 69.2% in an\nillustrative deployment case, and provides a foundation for future carbon-aware\ninference infrastructure design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u63a8\u7406\u5728\u4e0d\u540c\u90e8\u7f72\u8bbe\u7f6e\u4e0b\u7684\u80fd\u6e90\u548c\u78b3\u6392\u653e\u5f71\u54cd\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6846\u67b6\u7684\u7a7a\u767d\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u73af\u5883\u5f71\u54cd\u65e5\u76ca\u663e\u8457\uff0c\u5c24\u5176\u662f\u63a8\u7406\u9636\u6bb5\u7684\u78b3\u6392\u653e\u5360\u5176\u603b\u751f\u547d\u5468\u671f\u7684\u4e00\u534a\u4ee5\u4e0a\uff0c\u4f46\u73b0\u6709\u6a21\u62df\u6846\u67b6\u7f3a\u4e4f\u5bf9\u80fd\u6e90\u7684\u8003\u91cf\u3002", "method": "\u6269\u5c55\u4e86\u9ad8\u4fdd\u771fLLM\u63a8\u7406\u6a21\u62df\u5668\uff0c\u52a0\u5165GPU\u529f\u8017\u6a21\u578b\uff0c\u5e76\u96c6\u6210\u5230\u80fd\u6e90\u7cfb\u7edf\u534f\u540c\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u4ee5\u91cf\u5316\u78b3\u6392\u653e\u5e76\u63a2\u7d22\u78b3\u611f\u77e5\u8c03\u5ea6\u7684\u6f5c\u529b\u3002", "result": "\u6846\u67b6\u63ed\u793a\u4e86\u63a8\u7406\u53c2\u6570\u5bf9\u80fd\u6e90\u9700\u6c42\u548c\u78b3\u6392\u653e\u7684\u5f71\u54cd\uff0c\u5c55\u793a\u4e86\u4e00\u4e2a\u6848\u4f8b\u4e2d69.2%\u7684\u53ef\u518d\u751f\u80fd\u6e90\u62b5\u6d88\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u672a\u6765\u78b3\u611f\u77e5\u63a8\u7406\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.11430", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11430", "abs": "https://arxiv.org/abs/2507.11430", "authors": ["Arnab Mukherjee", "Raju Halder", "Joydeep Chandra"], "title": "FLsim: A Modular and Library-Agnostic Simulation Framework for Federated Learning", "comment": null, "summary": "Federated Learning (FL) has undergone significant development since its\ninception in 2016, advancing from basic algorithms to complex methodologies\ntailored to address diverse challenges and use cases. However, research and\nbenchmarking of novel FL techniques against a plethora of established\nstate-of-the-art solutions remain challenging. To streamline this process, we\nintroduce FLsim, a comprehensive FL simulation framework designed to meet the\ndiverse requirements of FL workflows in the literature. FLsim is characterized\nby its modularity, scalability, resource efficiency, and controlled\nreproducibility of experimental outcomes. Its easy to use interface allows\nusers to specify customized FL requirements through job configuration, which\nsupports: (a) customized data distributions, ranging from non-independent and\nidentically distributed (non-iid) data to independent and identically\ndistributed (iid) data, (b) selection of local learning algorithms according to\nuser preferences, with complete agnosticism to ML libraries, (c) choice of\nnetwork topology illustrating communication patterns among nodes, (d)\ndefinition of model aggregation and consensus algorithms, and (e) pluggable\nblockchain support for enhanced robustness. Through a series of experimental\nevaluations, we demonstrate the effectiveness and versatility of FLsim in\nsimulating a diverse range of state-of-the-art FL experiments. We envisage that\nFLsim would mark a significant advancement in FL simulation frameworks,\noffering unprecedented flexibility and functionality for researchers and\npractitioners alike.", "AI": {"tldr": "FLsim\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u6a21\u62df\u6846\u67b6\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u5b9e\u9a8c\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u4e2d\u65b0\u65b9\u6cd5\u4e0e\u73b0\u6709\u6280\u672f\u5bf9\u6bd4\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1FLsim\u6846\u67b6\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u6570\u636e\u5206\u5e03\u3001\u5b66\u4e60\u7b97\u6cd5\u3001\u7f51\u7edc\u62d3\u6251\u3001\u6a21\u578b\u805a\u5408\u53ca\u533a\u5757\u94fe\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eFLsim\u80fd\u6709\u6548\u6a21\u62df\u591a\u79cd\u5148\u8fdb\u8054\u90a6\u5b66\u4e60\u5b9e\u9a8c\u3002", "conclusion": "FLsim\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u7075\u6d3b\u6027\u548c\u529f\u80fd\u6027\u3002"}}
{"id": "2507.11437", "categories": ["cs.DC", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.11437", "abs": "https://arxiv.org/abs/2507.11437", "authors": ["Sagar Bharadwaj", "Srinivasan Seshan", "Anthony Rowe"], "title": "Uniting the World by Dividing it: Federated Maps to Enable Spatial Applications", "comment": null, "summary": "The emergence of the Spatial Web -- the Web where content is tied to\nreal-world locations has the potential to improve and enable many applications\nsuch as augmented reality, navigation, robotics, and more. The Spatial Web is\nmissing a key ingredient that is impeding its growth -- a spatial naming system\nto resolve real-world locations to names. Today's spatial naming systems are\ndigital maps such as Google and Apple maps. These maps and the location-based\nservices provided on top of these maps are primarily controlled by a few large\ncorporations and mostly cover outdoor public spaces. Emerging classes of\napplications, such as persistent world-scale augmented reality, require\ndetailed maps of both outdoor and indoor spaces. Existing centralized mapping\ninfrastructures are proving insufficient for such applications because of the\nscale of cartography efforts required and the privacy of indoor map data.\n  In this paper, we present a case for a federated spatial naming system, or in\nother words, a federated mapping infrastructure. This enables disparate parties\nto manage and serve their own maps of physical regions and unlocks scalability\nof map management, isolation and privacy of maps. Map-related services such as\naddress-to-location mapping, location-based search, and routing needs\nre-architecting to work on federated maps. We discuss some essential services\nand practicalities of enabling these services.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u7a7a\u95f4\u547d\u540d\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u96c6\u4e2d\u5f0f\u5730\u56fe\u57fa\u7840\u8bbe\u65bd\u5728\u8986\u76d6\u8303\u56f4\u548c\u9690\u79c1\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u652f\u6301\u5ba4\u5185\u5916\u7a7a\u95f4\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7a7a\u95f4\u547d\u540d\u7cfb\u7edf\uff08\u5982Google\u548cApple\u5730\u56fe\uff09\u7531\u5c11\u6570\u5927\u516c\u53f8\u63a7\u5236\uff0c\u4e3b\u8981\u8986\u76d6\u5ba4\u5916\u516c\u5171\u7a7a\u95f4\uff0c\u65e0\u6cd5\u6ee1\u8db3\u65b0\u5174\u5e94\u7528\uff08\u5982\u589e\u5f3a\u73b0\u5b9e\uff09\u5bf9\u5ba4\u5185\u5916\u8be6\u7ec6\u5730\u56fe\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u8054\u90a6\u7a7a\u95f4\u547d\u540d\u7cfb\u7edf\uff0c\u5141\u8bb8\u591a\u65b9\u7ba1\u7406\u548c\u63d0\u4f9b\u81ea\u5df1\u7684\u5730\u56fe\uff0c\u5b9e\u73b0\u5730\u56fe\u7ba1\u7406\u7684\u53ef\u6269\u5c55\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "result": "\u8ba8\u8bba\u4e86\u652f\u6301\u8054\u90a6\u5730\u56fe\u7684\u57fa\u672c\u670d\u52a1\u548c\u5b9e\u9645\u5b9e\u73b0\u65b9\u6cd5\u3002", "conclusion": "\u8054\u90a6\u7a7a\u95f4\u547d\u540d\u7cfb\u7edf\u80fd\u591f\u89e3\u51b3\u96c6\u4e2d\u5f0f\u5730\u56fe\u7684\u5c40\u9650\u6027\uff0c\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u7a7a\u95f4\u5e94\u7528\u3002"}}
{"id": "2507.11512", "categories": ["cs.DC", "cs.NA", "cs.PF", "math.NA", "65Y10", "G.4; C.4"], "pdf": "https://arxiv.org/pdf/2507.11512", "abs": "https://arxiv.org/abs/2507.11512", "authors": ["Aditya Kashi", "Nicholson Koukpaizan", "Hao Lu", "Michael Matheson", "Sarp Oral", "Feiyi Wang"], "title": "Scaling the memory wall using mixed-precision -- HPG-MxP on an exascale machine", "comment": "Accepted for presentation at SC25, St. Louis, MO, USA", "summary": "Mixed-precision algorithms have been proposed as a way for scientific\ncomputing to benefit from some of the gains seen for artificial intelligence\n(AI) on recent high performance computing (HPC) platforms. A few applications\ndominated by dense matrix operations have seen substantial speedups by\nutilizing low precision formats such as FP16. However, a majority of scientific\nsimulation applications are memory bandwidth limited. Beyond preliminary\nstudies, the practical gain from using mixed-precision algorithms on a given\nHPC system is largely unclear.\n  The High Performance GMRES Mixed Precision (HPG-MxP) benchmark has been\nproposed to measure the useful performance of a HPC system on sparse\nmatrix-based mixed-precision applications. In this work, we present a highly\noptimized implementation of the HPG-MxP benchmark for an exascale system and\ndescribe our algorithm enhancements. We show for the first time a speedup of\n1.6x using a combination of double- and single-precision on modern GPU-based\nsupercomputers.", "AI": {"tldr": "\u6df7\u5408\u7cbe\u5ea6\u7b97\u6cd5\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u5e94\u7528\uff0cHPG-MxP\u57fa\u51c6\u6d4b\u8bd5\u9996\u6b21\u5728GPU\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u5b9e\u73b01.6\u500d\u52a0\u901f\u3002", "motivation": "\u63a2\u7d22\u6df7\u5408\u7cbe\u5ea6\u7b97\u6cd5\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u5b9e\u9645\u589e\u76ca\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u7a00\u758f\u77e9\u9635\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u5e76\u4f18\u5316HPG-MxP\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u5408\u53cc\u7cbe\u5ea6\u548c\u5355\u7cbe\u5ea6\u8ba1\u7b97\u3002", "result": "\u5728\u73b0\u4ee3GPU\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u5b9e\u73b01.6\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u6df7\u5408\u7cbe\u5ea6\u7b97\u6cd5\u5728\u7a00\u758f\u77e9\u9635\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5b9e\u9645\u589e\u76ca\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
