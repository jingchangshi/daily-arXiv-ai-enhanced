{"id": "2509.20514", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.20514", "abs": "https://arxiv.org/abs/2509.20514", "authors": ["Ian McDougall", "Harish Batchu", "Michael Davies", "Karthikeyan Sankaralingam"], "title": "Pedagogically Motivated and Composable Open-Source RISC-V Processors for Computer Science Education", "comment": "8 pages, 2 figures", "summary": "While most instruction set architectures (ISAs) are only available to use\nthrough the purchase of a restrictive commercial license, the RISC-V ISA\npresents a free and open-source alternative. Due to this availability, many\nfree and open-source implementations have been developed and can be accessed on\nplatforms such as GitHub. If an open source, easy-to-use, and robust RISC-V\nimplementation could be obtained, it could be easily adapted for pedagogical\nand amateur use. In this work we accomplish three goals in relation to this\noutlook. First, we propose a set of criteria for evaluating the components of a\nRISC-V implementation's ecosystem from a pedagogical perspective. Second, we\nanalyze a number of existing open-source RISC-V implementations to determine\nhow many of the criteria they fulfill. We then develop a comprehensive solution\nthat meets all of these criterion and is released open-source for other\ninstructors to use. The framework is developed in a composable way that it's\ndifferent components can be disaggregated per individual course needs. Finally,\nwe also report on a limited study of student feedback.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30RISC-V\u5b9e\u73b0\u751f\u6001\u7cfb\u7edf\u7684\u6807\u51c6\uff0c\u5206\u6790\u4e86\u73b0\u6709\u5f00\u6e90\u5b9e\u73b0\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u6ee1\u8db3\u6240\u6709\u6807\u51c6\u7684\u53ef\u7ec4\u5408\u6559\u5b66\u6846\u67b6\u3002", "motivation": "RISC-V ISA\u4f5c\u4e3a\u514d\u8d39\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\uff0c\u9700\u8981\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u4e14\u7a33\u5065\u7684\u5b9e\u73b0\u7528\u4e8e\u6559\u5b66\u548c\u4e1a\u4f59\u7528\u9014\u3002", "method": "\u9996\u5148\u5236\u5b9aRISC-V\u5b9e\u73b0\u751f\u6001\u7cfb\u7edf\u7684\u6559\u5b66\u8bc4\u4f30\u6807\u51c6\uff0c\u7136\u540e\u5206\u6790\u73b0\u6709\u5f00\u6e90\u5b9e\u73b0\uff0c\u6700\u540e\u5f00\u53d1\u6ee1\u8db3\u6240\u6709\u6807\u51c6\u7684\u53ef\u7ec4\u5408\u6846\u67b6\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u7ec4\u4ef6\u53ef\u4ee5\u6839\u636e\u8bfe\u7a0b\u9700\u6c42\u8fdb\u884c\u5206\u89e3\uff0c\u5e76\u6536\u96c6\u4e86\u5b66\u751f\u53cd\u9988\u3002", "conclusion": "\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u6ee1\u8db3\u6559\u5b66\u9700\u6c42\u7684RISC-V\u5b9e\u73b0\u6846\u67b6\uff0c\u4e3a\u5176\u4ed6\u6559\u80b2\u5de5\u4f5c\u8005\u63d0\u4f9b\u4e86\u53ef\u7528\u7684\u5f00\u6e90\u5de5\u5177\u3002"}}
{"id": "2509.20543", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.20543", "abs": "https://arxiv.org/abs/2509.20543", "authors": ["Daniel Ruelas-Petrisko", "Farzam Gilani", "Anoop Mysore Nataraja", "Zoe Taylor", "Michael Taylor"], "title": "ZynqParrot: A Scale-Down Approach to Cycle-Accurate, FPGA-Accelerated Co-Emulation", "comment": null, "summary": "As processors increase in complexity, costs grow even more rapidly, both for\nfunctional verification and performance validation. Most often, silicon\ncharacterizations comprise simple performance counters, which are aggregated\nand separated to tell a story. Based on these inferences, performance engineers\nemploy microarchitectural simulation to inspect deeply into the core.\nUnfortunately, dramatically longer runtimes make simulation infeasible for long\nworkloads.\n  We propose a Scale-Down approach to modelling and validation. Rather than\nup-sizing a prototyping platform to fit large and complex system designs, we\nshow that it can be more accurate, faster, and more economical to decompose a\nsystem into manageable sub-components that can be prototyped independently. By\ncarefully designing the prototyping interface, it is possible to adhere to\nstrict non-interference of the Device Under Test (DUT). This allows architects\nto have the best of both worlds: the speed of FPGA acceleration while\neliminating the inaccuracies of Scale-Out and the inherent costs of Scale-Up.\n  In this work, we present ZynqParrot: a Scale-Down FPGA-based modelling\nplatform, capable of executing non-interfering, cycle-accurate co-emulations of\narbitrary RTL designs. ZynqParrot is capable of verifying functionality and\nperformance with arbitrary granularity. We also provide case studies using\nZynqParrot to analyze the full-stack performance of an open-source RISC-V\nprocessor.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdScale-Down\u7684FPGA\u5efa\u6a21\u5e73\u53f0ZynqParrot\uff0c\u901a\u8fc7\u5c06\u590d\u6742\u7cfb\u7edf\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u5b50\u7ec4\u4ef6\u8fdb\u884c\u72ec\u7acb\u539f\u578b\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u51c6\u786e\u4e14\u7ecf\u6d4e\u7684\u5904\u7406\u5668\u9a8c\u8bc1\u3002", "motivation": "\u968f\u7740\u5904\u7406\u5668\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u4f20\u7edf\u9a8c\u8bc1\u65b9\u6cd5\uff08\u6027\u80fd\u8ba1\u6570\u5668\u548c\u5fae\u67b6\u6784\u6a21\u62df\uff09\u9762\u4e34\u8fd0\u884c\u65f6\u95f4\u8fc7\u957f\u548c\u6210\u672c\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528Scale-Down\u65b9\u6cd5\uff0c\u5c06\u7cfb\u7edf\u5206\u89e3\u4e3a\u5b50\u7ec4\u4ef6\u8fdb\u884c\u72ec\u7acb\u539f\u578b\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u539f\u578b\u63a5\u53e3\u786e\u4fdd\u88ab\u6d4b\u8bbe\u5907\u7684\u4e25\u683c\u975e\u5e72\u6270\u6027\uff0c\u5b9e\u73b0\u5468\u671f\u7cbe\u786e\u7684\u534f\u540c\u4eff\u771f\u3002", "result": "\u5f00\u53d1\u4e86ZynqParrot\u5e73\u53f0\uff0c\u80fd\u591f\u6267\u884c\u975e\u5e72\u6270\u7684\u5468\u671f\u7cbe\u786e\u534f\u540c\u4eff\u771f\uff0c\u9a8c\u8bc1\u529f\u80fd\u548c\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90RISC-V\u5904\u7406\u5668\u7684\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "Scale-Down\u65b9\u6cd5\u7ed3\u5408FPGA\u52a0\u901f\uff0c\u5728\u4fdd\u6301\u901f\u5ea6\u7684\u540c\u65f6\u6d88\u9664\u4e86Scale-Out\u7684\u4e0d\u51c6\u786e\u6027\u548cScale-Up\u7684\u9ad8\u6210\u672c\uff0c\u4e3a\u67b6\u6784\u5e08\u63d0\u4f9b\u4e86\u6700\u4f73\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.20603", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20603", "abs": "https://arxiv.org/abs/2509.20603", "authors": ["Angel M. Beltre", "Jeff Ogden", "Kevin Pedretti"], "title": "Experience Deploying Containerized GenAI Services at an HPC Center", "comment": "10 pages, 12 figures", "summary": "Generative Artificial Intelligence (GenAI) applications are built from\nspecialized components -- inference servers, object storage, vector and graph\ndatabases, and user interfaces -- interconnected via web-based APIs. While\nthese components are often containerized and deployed in cloud environments,\nsuch capabilities are still emerging at High-Performance Computing (HPC)\ncenters. In this paper, we share our experience deploying GenAI workloads\nwithin an established HPC center, discussing the integration of HPC and cloud\ncomputing environments. We describe our converged computing architecture that\nintegrates HPC and Kubernetes platforms running containerized GenAI workloads,\nhelping with reproducibility. A case study illustrates the deployment of the\nLlama Large Language Model (LLM) using a containerized inference server (vLLM)\nacross both Kubernetes and HPC platforms using multiple container runtimes. Our\nexperience highlights practical considerations and opportunities for the HPC\ncontainer community, guiding future research and tool development.", "AI": {"tldr": "\u672c\u6587\u5206\u4eab\u4e86\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u5fc3\u90e8\u7f72\u751f\u6210\u5f0fAI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7ecf\u9a8c\uff0c\u8ba8\u8bba\u4e86HPC\u4e0e\u4e91\u8ba1\u7b97\u73af\u5883\u7684\u96c6\u6210\uff0c\u63d0\u51fa\u4e86\u878d\u5408\u8ba1\u7b97\u67b6\u6784\uff0c\u5e76\u901a\u8fc7Llama\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u8de8Kubernetes\u548cHPC\u5e73\u53f0\u7684\u5bb9\u5668\u5316\u90e8\u7f72\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5e94\u7528\u901a\u5e38\u57fa\u4e8e\u5bb9\u5668\u5316\u7ec4\u4ef6\u6784\u5efa\uff0c\u4f46\u5728HPC\u4e2d\u5fc3\u7684\u90e8\u7f72\u80fd\u529b\u4ecd\u5728\u53d1\u5c55\u4e2d\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5728\u4f20\u7edfHPC\u73af\u5883\u4e2d\u6709\u6548\u90e8\u7f72\u548c\u7ba1\u7406\u5bb9\u5668\u5316\u7684GenAI\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u63d0\u51fa\u878d\u5408\u8ba1\u7b97\u67b6\u6784\uff0c\u96c6\u6210HPC\u548cKubernetes\u5e73\u53f0\u6765\u8fd0\u884c\u5bb9\u5668\u5316\u7684GenAI\u5de5\u4f5c\u8d1f\u8f7d\u3002\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff0c\u4f7f\u7528vLLM\u5bb9\u5668\u5316\u63a8\u7406\u670d\u52a1\u5668\u5728Kubernetes\u548cHPC\u5e73\u53f0\u4e0a\u90e8\u7f72Llama\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u6d4b\u8bd5\u591a\u79cd\u5bb9\u5668\u8fd0\u884c\u65f6\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u8de8\u5e73\u53f0\u7684GenAI\u5de5\u4f5c\u8d1f\u8f7d\u90e8\u7f72\uff0c\u9a8c\u8bc1\u4e86\u878d\u5408\u67b6\u6784\u7684\u53ef\u884c\u6027\u3002\u7ecf\u9a8c\u8868\u660e\u5bb9\u5668\u5316\u6280\u672f\u6709\u52a9\u4e8e\u63d0\u9ad8\u5de5\u4f5c\u8d1f\u8f7d\u7684\u53ef\u91cd\u590d\u6027\u3002", "conclusion": "\u672c\u6587\u4e3aHPC\u5bb9\u5668\u793e\u533a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u90e8\u7f72\u8003\u8651\u56e0\u7d20\u548c\u53d1\u5c55\u673a\u4f1a\uff0c\u6307\u5bfc\u672a\u6765\u7684\u7814\u7a76\u548c\u5de5\u5177\u5f00\u53d1\uff0c\u8bc1\u660e\u4e86\u5728HPC\u73af\u5883\u4e2d\u6709\u6548\u90e8\u7f72\u5bb9\u5668\u5316GenAI\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.21137", "categories": ["cs.DC", "cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.21137", "abs": "https://arxiv.org/abs/2509.21137", "authors": ["Huynh Q. N. Vo", "Md Tawsif Rahman Chowdhury", "Paritosh Ramanan", "Gozde Tutuncuoglu", "Junchi Yang", "Feng Qiu", "Murat Yildirim"], "title": "From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient Method for Solving Large-Scale Linear Optimization Problem", "comment": "Main Article (12 Pages, 3 Figures), Appendix (4 Pages)", "summary": "The exponential growth of computational workloads is surpassing the\ncapabilities of conventional architectures, which are constrained by\nfundamental limits. In-memory computing (IMC) with RRAM provides a promising\nalternative by providing analog computations with significant gains in latency\nand energy use. However, existing algorithms developed for conventional\narchitectures do not translate to IMC, particularly for constrained\noptimization problems where frequent matrix reprogramming remains\ncost-prohibitive for IMC applications. Here we present a distributed in-memory\nprimal-dual hybrid gradient (PDHG) method, specifically co-designed for arrays\nof RRAM devices. Our approach minimizes costly write cycles, incorporates\nrobustness against device non-idealities, and leverages a symmetric\nblock-matrix formulation to unify operations across distributed crossbars. We\nintegrate a physics-based simulation framework called MELISO+ to evaluate\nperformance under realistic device conditions. Benchmarking against\nGPU-accelerated solvers on large-scale linear programs demonstrates that our\nRRAM-based solver achieves comparable accuracy with up to three orders of\nmagnitude reductions in energy consumption and latency. These results\ndemonstrate the first PDHG-based LP solver implemented on RRAMs, showcasing the\ntransformative potential of algorithm-hardware co-design for solving\nlarge-scale optimization through distributed in-memory computing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3aRRAM\u9635\u5217\u8bbe\u8ba1\u7684\u5206\u5e03\u5f0f\u5185\u5b58\u539f\u59cb-\u5bf9\u5076\u6df7\u5408\u68af\u5ea6\uff08PDHG\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u76f8\u6bd4GPU\u52a0\u901f\u6c42\u89e3\u5668\u5b9e\u73b0\u4e86\u4e09\u4e2a\u6570\u91cf\u7ea7\u7684\u80fd\u8017\u548c\u5ef6\u8fdf\u964d\u4f4e\u3002", "motivation": "\u4f20\u7edf\u67b6\u6784\u53d7\u9650\u4e8e\u57fa\u672c\u7269\u7406\u9650\u5236\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8ba1\u7b97\u5de5\u4f5c\u91cf\u7684\u6307\u6570\u7ea7\u589e\u957f\u9700\u6c42\u3002\u5185\u5b58\u8ba1\u7b97\uff08IMC\uff09\u4e0eRRAM\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u7b97\u6cd5\u4e0d\u9002\u7528\u4e8eIMC\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9891\u7e41\u77e9\u9635\u91cd\u7f16\u7a0b\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u4e2d\u3002", "method": "\u5f00\u53d1\u4e86\u5206\u5e03\u5f0f\u5185\u5b58PDHG\u65b9\u6cd5\uff0c\u6700\u5c0f\u5316\u6602\u8d35\u7684\u5199\u5165\u5468\u671f\uff0c\u5305\u542b\u5bf9\u8bbe\u5907\u975e\u7406\u60f3\u6027\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5229\u7528\u5bf9\u79f0\u5757\u77e9\u9635\u516c\u5f0f\u7edf\u4e00\u5206\u5e03\u5f0f\u4ea4\u53c9\u5f00\u5173\u4e0a\u7684\u64cd\u4f5c\u3002\u4f7f\u7528MELISO+\u7269\u7406\u6a21\u62df\u6846\u67b6\u8bc4\u4f30\u5b9e\u9645\u8bbe\u5907\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u4e0eGPU\u52a0\u901f\u6c42\u89e3\u5668\u5728\u5927\u89c4\u6a21\u7ebf\u6027\u7a0b\u5e8f\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u57fa\u4e8eRRAM\u7684\u6c42\u89e3\u5668\u5b9e\u73b0\u4e86\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u80fd\u8017\u548c\u5ef6\u8fdf\u964d\u4f4e\u4e86\u4e09\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728RRAM\u4e0a\u5b9e\u73b0\u7684PDHG\u57faLP\u6c42\u89e3\u5668\uff0c\u5c55\u793a\u4e86\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u901a\u8fc7\u5206\u5e03\u5f0f\u5185\u5b58\u8ba1\u7b97\u89e3\u51b3\u5927\u89c4\u6a21\u4f18\u5316\u95ee\u9898\u7684\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2509.20426", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.20426", "abs": "https://arxiv.org/abs/2509.20426", "authors": ["Mahmoud Samir Fayed"], "title": "Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications", "comment": "PhD thesis", "summary": "Most visual programming languages (VPLs) are domain-specific, with few\ngeneral-purpose VPLs like Programming Without Coding Technology (PWCT). These\ngeneral-purpose VPLs are developed using textual programming languages and\nimproving them requires textual programming. In this thesis, we designed and\ndeveloped PWCT2, a dual-language (Arabic/English), general-purpose,\nself-hosting visual programming language. Before doing so, we specifically\ndesigned a textual programming language called Ring for its development. Ring\nis a dynamically typed language with a lightweight implementation, offering\nsyntax customization features. It permits the creation of domain-specific\nlanguages through new features that extend object-oriented programming,\nallowing for specialized languages resembling Cascading Style Sheets (CSS) or\nSupernova language. The Ring Compiler and Virtual Machine are designed using\nthe PWCT visual programming language where the visual implementation is\ncomposed of 18,945 components that generate 24,743 lines of C code, which\nincreases the abstraction level and hides unnecessary details. Using PWCT to\ndevelop Ring allowed us to realize several issues in PWCT, which led to the\ndevelopment of the PWCT2 visual programming language using the Ring textual\nprogramming language. PWCT2 provides approximately 36 times faster code\ngeneration and requires 20 times less storage for visual source files. It also\nallows for the conversion of Ring code into visual code, enabling the creation\nof a self-hosting VPL that can be developed using itself. PWCT2 consists of\napproximately 92,000 lines of Ring code and comes with 394 visual components.\nPWCT2 is distributed to many users through the Steam platform and has received\npositive feedback, On Steam, 1772 users have launched the software, and the\ntotal recorded usage time exceeds 17,000 hours, encouraging further research\nand development.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86PWCT2\uff0c\u8fd9\u662f\u4e00\u4e2a\u53cc\u8bed\u8a00\uff08\u963f\u62c9\u4f2f\u8bed/\u82f1\u8bed\uff09\u3001\u901a\u7528\u3001\u81ea\u6258\u7ba1\u7684\u53ef\u89c6\u5316\u7f16\u7a0b\u8bed\u8a00\uff0c\u901a\u8fc7Ring\u6587\u672c\u7f16\u7a0b\u8bed\u8a00\u5b9e\u73b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u901f\u5ea6\u548c\u5b58\u50a8\u6548\u7387\u3002", "motivation": "\u5927\u591a\u6570\u53ef\u89c6\u5316\u7f16\u7a0b\u8bed\u8a00\u662f\u9886\u57df\u7279\u5b9a\u7684\uff0c\u901a\u7528VPL\u5982PWCT\u9700\u8981\u6587\u672c\u7f16\u7a0b\u6765\u6539\u8fdb\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u81ea\u6258\u7ba1\u7684\u901a\u7528VPL\uff0c\u80fd\u591f\u81ea\u6211\u6539\u8fdb\u3002", "method": "\u9996\u5148\u8bbe\u8ba1Ring\u6587\u672c\u7f16\u7a0b\u8bed\u8a00\uff0c\u7136\u540e\u7528PWCT\u5f00\u53d1Ring\u7f16\u8bd1\u5668\uff0c\u6700\u540e\u7528Ring\u5f00\u53d1PWCT2\u3002PWCT2\u5305\u542b92,000\u884cRing\u4ee3\u7801\u548c394\u4e2a\u53ef\u89c6\u5316\u7ec4\u4ef6\u3002", "result": "PWCT2\u4ee3\u7801\u751f\u6210\u901f\u5ea6\u63d0\u534736\u500d\uff0c\u5b58\u50a8\u9700\u6c42\u51cf\u5c1120\u500d\u3002\u5728Steam\u5e73\u53f0\u67091772\u7528\u6237\u4f7f\u7528\uff0c\u603b\u4f7f\u7528\u65f6\u95f4\u8d85\u8fc717,000\u5c0f\u65f6\u3002", "conclusion": "PWCT2\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u6258\u7ba1VPL\uff0c\u8bc1\u660e\u4e86\u53ef\u89c6\u5316\u7f16\u7a0b\u8bed\u8a00\u53ef\u4ee5\u81ea\u6211\u6539\u8fdb\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.20563", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20563", "abs": "https://arxiv.org/abs/2509.20563", "authors": ["Skyler Ruiter", "Jiannan Tian", "Fengguang Song"], "title": "FZModules: A Heterogeneous Computing Framework for Customizable Scientific Data Compression Pipelines", "comment": null, "summary": "Modern scientific simulations and instruments generate data volumes that\noverwhelm memory and storage, throttling scalability. Lossy compression\nmitigates this by trading controlled error for reduced footprint and throughput\ngains, yet optimal pipelines are highly data and objective specific, demanding\ncompression expertise. GPU compressors supply raw throughput but often\nhard-code fused kernels that hinder rapid experimentation, and underperform in\nrate-distortion. We present FZModules, a heterogeneous framework for assembling\nerror-bounded custom compression pipelines from high-performance modules\nthrough a concise extensible interface. We further utilize an asynchronous\ntask-backed execution library that infers data dependencies, manages memory\nmovement, and exposes branch and stage level concurrency for powerful\nasynchronous compression pipelines. Evaluating three pipelines built with\nFZModules on four representative scientific datasets, we show they can compare\nend-to-end speedup of fused-kernel GPU compressors while achieving similar\nrate-distortion to higher fidelity CPU or hybrid compressors, enabling rapid,\ndomain-tailored design.", "AI": {"tldr": "FZModules\u662f\u4e00\u4e2a\u5f02\u6784\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u9ad8\u6027\u80fd\u6a21\u5757\u7ec4\u88c5\u8bef\u5dee\u6709\u754c\u7684\u81ea\u5b9a\u4e49\u538b\u7f29\u6d41\u6c34\u7ebf\uff0c\u652f\u6301\u5feb\u901f\u5b9e\u9a8c\u548c\u5f02\u6b65\u6267\u884c\uff0c\u5728\u4fdd\u6301\u9ad8\u538b\u7f29\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e0eGPU\u538b\u7f29\u5668\u76f8\u5f53\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u6a21\u62df\u548c\u4eea\u5668\u751f\u6210\u7684\u6570\u636e\u91cf\u8fc7\u5927\uff0c\u8d85\u8fc7\u5185\u5b58\u548c\u5b58\u50a8\u5bb9\u91cf\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u867d\u7136\u6709\u635f\u538b\u7f29\u901a\u8fc7\u63a7\u5236\u8bef\u5dee\u6765\u51cf\u5c11\u5b58\u50a8\u5360\u7528\u548c\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u4f46\u6700\u4f18\u6d41\u6c34\u7ebf\u9ad8\u5ea6\u4f9d\u8d56\u6570\u636e\u548c\u76ee\u6807\uff0c\u9700\u8981\u538b\u7f29\u4e13\u4e1a\u77e5\u8bc6\u3002\u73b0\u6709\u7684GPU\u538b\u7f29\u5668\u867d\u7136\u63d0\u4f9b\u9ad8\u541e\u5410\u91cf\uff0c\u4f46\u901a\u5e38\u91c7\u7528\u56fa\u5b9a\u5185\u6838\uff0c\u963b\u788d\u5feb\u901f\u5b9e\u9a8c\uff0c\u4e14\u5728\u7387\u5931\u771f\u6027\u80fd\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faFZModules\u5f02\u6784\u6846\u67b6\uff0c\u901a\u8fc7\u7b80\u6d01\u53ef\u6269\u5c55\u7684\u63a5\u53e3\u4ece\u9ad8\u6027\u80fd\u6a21\u5757\u7ec4\u88c5\u8bef\u5dee\u6709\u754c\u7684\u81ea\u5b9a\u4e49\u538b\u7f29\u6d41\u6c34\u7ebf\u3002\u5229\u7528\u5f02\u6b65\u4efb\u52a1\u652f\u6301\u7684\u6267\u884c\u5e93\uff0c\u63a8\u65ad\u6570\u636e\u4f9d\u8d56\u5173\u7cfb\uff0c\u7ba1\u7406\u5185\u5b58\u79fb\u52a8\uff0c\u5e76\u66b4\u9732\u5206\u652f\u548c\u9636\u6bb5\u7ea7\u5e76\u53d1\uff0c\u5b9e\u73b0\u5f3a\u5927\u7684\u5f02\u6b65\u538b\u7f29\u6d41\u6c34\u7ebf\u3002", "result": "\u5728\u56db\u4e2a\u4ee3\u8868\u6027\u79d1\u5b66\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e09\u4e2a\u4f7f\u7528FZModules\u6784\u5efa\u7684\u6d41\u6c34\u7ebf\uff0c\u7ed3\u679c\u663e\u793a\u5b83\u4eec\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u878d\u5408\u5185\u6838GPU\u538b\u7f29\u5668\u76f8\u5f53\u7684\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u540c\u65f6\u8fbe\u5230\u4e0e\u66f4\u9ad8\u4fdd\u771f\u5ea6\u7684CPU\u6216\u6df7\u5408\u538b\u7f29\u5668\u76f8\u4f3c\u7684\u7387\u5931\u771f\u6027\u80fd\u3002", "conclusion": "FZModules\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u3001\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u5b9a\u5236\u7684\u538b\u7f29\u6d41\u6c34\u7ebf\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u9ad8\u538b\u7f29\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u4f9b\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.20534", "categories": ["cs.PL", "cs.SC", "68W30", "I.1.1; G.4"], "pdf": "https://arxiv.org/pdf/2509.20534", "abs": "https://arxiv.org/abs/2509.20534", "authors": ["Bowen Zhu", "Aayush Sabharwal", "Songchen Tan", "Yingbo Ma", "Alan Edelman", "Christopher Rackauckas"], "title": "Efficient Symbolic Computation vis Hash Consing", "comment": null, "summary": "Symbolic computation systems suffer from memory inefficiencies due to\nredundant storage of structurally identical subexpressions, commonly known as\nexpression swell, which degrades performance in both classical computer algebra\nand emerging AI-driven mathematical reasoning tools. In this paper, we present\nthe first integration of hash consing into JuliaSymbolics, a high-performance\nsymbolic toolkit in Julia, by employing a global weak-reference hash table that\ncanonicalizes expressions and eliminates duplication. This approach reduces\nmemory consumption and accelerates key operations such as differentiation,\nsimplification, and code generation, while seamlessly integrating with Julia's\nmetaprogramming and just-in-time compilation infrastructure. Benchmark\nevaluations across different computational domains reveal substantial\nimprovements: symbolic computations are accelerated by up to 3.2 times, memory\nusage is reduced by up to 2 times, code generation is up to 5 times faster,\nfunction compilation up to 10 times faster, and numerical evaluation up to 100\ntimes faster for larger models. While certain workloads with fewer duplicate\nunknown-variable expressions show more modest gains or even slight overhead in\ninitial computation stages, downstream processing consistently benefits\nsignificantly. These findings underscore the importance of hash consing in\nscaling symbolic computation and pave the way for future work integrating hash\nconsing with e-graphs for enhanced equivalence-aware expression sharing in\nAI-driven pipelines.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u54c8\u5e0c\u4e00\u81f4\u6027\u6280\u672f\u96c6\u6210\u5230JuliaSymbolics\u4e2d\uff0c\u901a\u8fc7\u5168\u5c40\u5f31\u5f15\u7528\u54c8\u5e0c\u8868\u89c4\u8303\u5316\u8868\u8fbe\u5f0f\u5e76\u6d88\u9664\u91cd\u590d\u5b58\u50a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b26\u53f7\u8ba1\u7b97\u7684\u6027\u80fd\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u7b26\u53f7\u8ba1\u7b97\u7cfb\u7edf\u5b58\u5728\u5185\u5b58\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u7531\u4e8e\u7ed3\u6784\u76f8\u540c\u5b50\u8868\u8fbe\u5f0f\u7684\u5197\u4f59\u5b58\u50a8\uff08\u8868\u8fbe\u5f0f\u81a8\u80c0\uff09\uff0c\u8fd9\u5f71\u54cd\u4e86\u7ecf\u5178\u8ba1\u7b97\u673a\u4ee3\u6570\u548c\u65b0\u5174AI\u9a71\u52a8\u6570\u5b66\u63a8\u7406\u5de5\u5177\u7684\u6027\u80fd\u3002", "method": "\u5728JuliaSymbolics\u4e2d\u96c6\u6210\u54c8\u5e0c\u4e00\u81f4\u6027\u6280\u672f\uff0c\u4f7f\u7528\u5168\u5c40\u5f31\u5f15\u7528\u54c8\u5e0c\u8868\u6765\u89c4\u8303\u5316\u8868\u8fbe\u5f0f\u5e76\u6d88\u9664\u91cd\u590d\u5b58\u50a8\uff0c\u540c\u65f6\u4e0eJulia\u7684\u5143\u7f16\u7a0b\u548c\u5373\u65f6\u7f16\u8bd1\u57fa\u7840\u8bbe\u65bd\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u663e\u8457\u6539\u8fdb\uff1a\u7b26\u53f7\u8ba1\u7b97\u52a0\u901f\u8fbe3.2\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u8fbe2\u500d\uff0c\u4ee3\u7801\u751f\u6210\u5feb\u8fbe5\u500d\uff0c\u51fd\u6570\u7f16\u8bd1\u5feb\u8fbe10\u500d\uff0c\u5927\u578b\u6a21\u578b\u7684\u6570\u503c\u8bc4\u4f30\u5feb\u8fbe100\u500d\u3002", "conclusion": "\u54c8\u5e0c\u4e00\u81f4\u6027\u5bf9\u4e8e\u6269\u5c55\u7b26\u53f7\u8ba1\u7b97\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u672a\u6765\u5c06\u54c8\u5e0c\u4e00\u81f4\u6027\u4e0ee-graphs\u96c6\u6210\u4ee5\u589e\u5f3aAI\u9a71\u52a8\u7ba1\u9053\u4e2d\u7684\u7b49\u4ef7\u611f\u77e5\u8868\u8fbe\u5f0f\u5171\u4eab\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.21039", "categories": ["cs.DC", "cs.CE", "cs.ET", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.21039", "abs": "https://arxiv.org/abs/2509.21039", "authors": ["William F. Godoy", "Tatiana Melnichenko", "Pedro Valero-Lara", "Wael Elwasif", "Philip Fackler", "Rafael Ferreira Da Silva", "Keita Teranishi", "Jeffrey S. Vetter"], "title": "Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem", "comment": "Accepted at the IEEE/ACM SC25 Conference WACCPD Workshop. The\n  International Conference for High Performance Computing, Networking, Storage,\n  and Analysis, St. Louis, MO, Nov 16-21, 2025. 15 pages, 7 figures. WFG and TM\n  contributed equally", "summary": "We explore the performance and portability of the novel Mojo language for\nscientific computing workloads on GPUs. As the first language based on the\nLLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure,\nMojo aims to close performance and productivity gaps by combining Python's\ninteroperability and CUDA-like syntax for compile-time portable GPU\nprogramming. We target four scientific workloads: a seven-point stencil\n(memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and\nHartree-Fock (compute-bound with atomic operations); and compare their\nperformance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We\nshow that Mojo's performance is competitive with CUDA and HIP for memory-bound\nkernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math\ncompute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve\nand programming requirements are still fairly low-level, Mojo can close\nsignificant gaps in the fragmented Python ecosystem in the convergence of\nscientific computing and AI.", "AI": {"tldr": "Mojo\u8bed\u8a00\u5728GPU\u79d1\u5b66\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u6027\u80fd\u4e0e\u53ef\u79fb\u690d\u6027\u8bc4\u4f30\uff0c\u663e\u793a\u5176\u5728\u5185\u5b58\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u4e0eCUDA/HIP\u7ade\u4e89\uff0c\u4f46\u5728\u539f\u5b50\u64cd\u4f5c\u548c\u5feb\u901f\u6570\u5b66\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u5b58\u5728\u5dee\u8ddd", "motivation": "\u63a2\u7d22\u57fa\u4e8eMLIR\u7684\u65b0\u578bMojo\u8bed\u8a00\u5728GPU\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u8868\u73b0\uff0c\u65e8\u5728\u89e3\u51b3Python\u751f\u6001\u7cfb\u7edf\u5728\u79d1\u5b66\u8ba1\u7b97\u4e0eAI\u878d\u5408\u4e2d\u7684\u6027\u80fd\u548c\u751f\u4ea7\u6548\u7387\u5dee\u8ddd", "method": "\u9488\u5bf9\u56db\u79cd\u79d1\u5b66\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\uff08\u4e03\u70b9\u6a21\u677f\u3001BabelStream\u3001miniBUDE\u3001Hartree-Fock\uff09\u5728NVIDIA H100\u548cAMD MI300A GPU\u4e0a\u6bd4\u8f83Mojo\u4e0eCUDA/HIP\u7684\u6027\u80fd", "result": "Mojo\u5728\u5185\u5b58\u5bc6\u96c6\u578b\u5185\u6838\u4e0a\u6027\u80fd\u4e0eCUDA/HIP\u76f8\u5f53\uff0c\u4f46\u5728AMD GPU\u4e0a\u7684\u539f\u5b50\u64cd\u4f5c\u4ee5\u53ca\u4e24\u5e73\u53f0\u4e0a\u7684\u5feb\u901f\u6570\u5b66\u8ba1\u7b97\u5bc6\u96c6\u578b\u5185\u6838\u5b58\u5728\u6027\u80fd\u5dee\u8ddd", "conclusion": "\u5c3d\u7ba1\u5b66\u4e60\u66f2\u7ebf\u548c\u7f16\u7a0b\u8981\u6c42\u4ecd\u8f83\u5e95\u5c42\uff0cMojo\u80fd\u5728\u79d1\u5b66\u8ba1\u7b97\u4e0eAI\u878d\u5408\u7684\u788e\u7247\u5316Python\u751f\u6001\u7cfb\u7edf\u4e2d\u586b\u8865\u91cd\u8981\u7a7a\u767d"}}
{"id": "2509.20776", "categories": ["cs.DC", "cs.MS", "G.4"], "pdf": "https://arxiv.org/pdf/2509.20776", "abs": "https://arxiv.org/abs/2509.20776", "authors": ["Elaheh Hassani", "Md Taufique Hussain", "Ariful Azad"], "title": "Distributed-memory Algorithms for Sparse Matrix Permutation, Extraction, and Assignment", "comment": "32 pages", "summary": "We present scalable distributed-memory algorithms for sparse matrix\npermutation, extraction, and assignment. Our methods follow an\nIdentify-Exchange-Build (IEB) strategy where each process identifies the local\nnonzeros to be sent, exchanges the required data, and then builds its local\nsubmatrix from the received elements. This approach reduces communication\ncompared to SpGEMM-based methods in distributed memory. By employing\nsynchronization-free multithreaded algorithms, we further accelerate local\ncomputations, achieving substantially better performance than existing\nlibraries such as CombBLAS and PETSc. We design efficient software for these\noperations and evaluate their performance on two university clusters and the\nPerlmutter supercomputer. Our experiments span a variety of application\nscenarios, including matrix permutation for load balancing, matrix reordering,\nsubgraph extraction, and streaming graph applications. In all cases, we compare\nour algorithms against CombBLAS, the most comprehensive distributed library for\nthese operations, and, in some scenarios, against PETSc. Overall, this work\nprovides a comprehensive study of algorithms, software implementations,\nexperimental evaluations, and applications for sparse matrix permutation,\nextraction, and assignment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u7684\u5206\u5e03\u5f0f\u5185\u5b58\u7b97\u6cd5\u7528\u4e8e\u7a00\u758f\u77e9\u9635\u7684\u7f6e\u6362\u3001\u63d0\u53d6\u548c\u8d4b\u503c\u64cd\u4f5c\uff0c\u91c7\u7528Identify-Exchange-Build\u7b56\u7565\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u901a\u8fc7\u65e0\u540c\u6b65\u591a\u7ebf\u7a0b\u7b97\u6cd5\u63d0\u5347\u672c\u5730\u8ba1\u7b97\u6027\u80fd", "motivation": "\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u5e93\u5982CombBLAS\u548cPETSc\u5728\u7a00\u758f\u77e9\u9635\u64cd\u4f5c\u4e0a\u5b58\u5728\u901a\u4fe1\u5f00\u9500\u5927\u548c\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u652f\u6301\u5927\u89c4\u6a21\u7a00\u758f\u77e9\u9635\u5904\u7406", "method": "\u91c7\u7528Identify-Exchange-Build\u4e09\u9636\u6bb5\u7b56\u7565\uff1a\u8bc6\u522b\u8981\u53d1\u9001\u7684\u672c\u5730\u975e\u96f6\u5143\u7d20\u3001\u4ea4\u6362\u6240\u9700\u6570\u636e\u3001\u4ece\u63a5\u6536\u5143\u7d20\u6784\u5efa\u672c\u5730\u5b50\u77e9\u9635\uff0c\u7ed3\u5408\u65e0\u540c\u6b65\u591a\u7ebf\u7a0b\u7b97\u6cd5\u52a0\u901f\u672c\u5730\u8ba1\u7b97", "result": "\u5728\u591a\u4e2a\u96c6\u7fa4\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u76f8\u6bd4CombBLAS\u548cPETSc\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u9002\u7528\u4e8e\u8d1f\u8f7d\u5747\u8861\u3001\u77e9\u9635\u91cd\u6392\u5e8f\u3001\u5b50\u56fe\u63d0\u53d6\u548c\u6d41\u56fe\u5e94\u7528\u7b49\u591a\u79cd\u573a\u666f", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7a00\u758f\u77e9\u9635\u7f6e\u6362\u3001\u63d0\u53d6\u548c\u8d4b\u503c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u7b97\u6cd5\u3001\u8f6f\u4ef6\u5b9e\u73b0\u548c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u4e3a\u5927\u89c4\u6a21\u7a00\u758f\u77e9\u9635\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.20819", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20819", "abs": "https://arxiv.org/abs/2509.20819", "authors": ["Andre Merzky", "Mikhail Titov", "Matteo Turilli", "Shantenu Jha"], "title": "Integrating and Characterizing HPC Task Runtime Systems for hybrid AI-HPC workloads", "comment": "12 pages, 1 table, 8 figures", "summary": "Scientific workflows increasingly involve both HPC and machine-learning\ntasks, combining MPI-based simulations, training, and inference in a single\nexecution. Launchers such as Slurm's srun constrain concurrency and throughput,\nmaking them unsuitable for dynamic and heterogeneous workloads. We present a\nperformance study of RADICAL-Pilot (RP) integrated with Flux and Dragon, two\ncomplementary runtime systems that enable hierarchical resource management and\nhigh-throughput function execution. Using synthetic and production-scale\nworkloads on Frontier, we characterize the task execution properties of RP\nacross runtime configurations. RP+Flux sustains up to 930 tasks/s, and\nRP+Flux+Dragon exceeds 1,500 tasks/s with over 99.6% utilization. In contrast,\nsrun peaks at 152 tasks/s and degrades with scale, with utilization below 50%.\nFor IMPECCABLE.v2 drug discovery campaign, RP+Flux reduces makespan by 30-60%\nrelative to srun/Slurm and increases throughput more than four times on up to\n1,024. These results demonstrate hybrid runtime integration in RP as a scalable\napproach for hybrid AI-HPC workloads.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86RADICAL-Pilot\u4e0eFlux\u548cDragon\u8fd0\u884c\u65f6\u7cfb\u7edf\u96c6\u6210\uff0c\u7528\u4e8e\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u6df7\u5408\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u4f18\u5316\uff0c\u76f8\u6bd4\u4f20\u7edfSlurm\u7684srun\u5728\u4efb\u52a1\u6267\u884c\u901f\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u79d1\u5b66\u5de5\u4f5c\u6d41\u8d8a\u6765\u8d8a\u591a\u5730\u6d89\u53caHPC\u548c\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u7ed3\u5408\uff0c\u4f46\u4f20\u7edf\u7684\u542f\u52a8\u5668\u5982Slurm\u7684srun\u5728\u5e76\u53d1\u6027\u548c\u541e\u5410\u91cf\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u4e0d\u9002\u5408\u52a8\u6001\u548c\u5f02\u6784\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u5c06RADICAL-Pilot\u4e0eFlux\u548cDragon\u4e24\u4e2a\u4e92\u8865\u7684\u8fd0\u884c\u65f6\u7cfb\u7edf\u96c6\u6210\uff0c\u5b9e\u73b0\u5206\u5c42\u8d44\u6e90\u7ba1\u7406\u548c\u9ad8\u541e\u5410\u91cf\u51fd\u6570\u6267\u884c\uff0c\u4f7f\u7528\u5408\u6210\u548c\u751f\u4ea7\u89c4\u6a21\u7684\u5de5\u4f5c\u8d1f\u8f7d\u5728Frontier\u7cfb\u7edf\u4e0a\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002", "result": "RP+Flux\u53ef\u6301\u7eed\u8fbe\u5230930\u4efb\u52a1/\u79d2\uff0cRP+Flux+Dragon\u8d85\u8fc71,500\u4efb\u52a1/\u79d2\uff0c\u5229\u7528\u7387\u8d85\u8fc799.6%\uff1b\u800csrun\u5cf0\u503c\u4ec5\u4e3a152\u4efb\u52a1/\u79d2\uff0c\u5229\u7528\u7387\u4f4e\u4e8e50%\u3002\u5728IMPEccABLE.v2\u836f\u7269\u53d1\u73b0\u5e94\u7528\u4e2d\uff0cRP+Flux\u76f8\u6bd4srun/Slurm\u5c06makespan\u51cf\u5c11\u4e8630-60%\uff0c\u541e\u5410\u91cf\u63d0\u9ad8\u4e86\u56db\u500d\u4ee5\u4e0a\u3002", "conclusion": "RADICAL-Pilot\u4e0e\u8fd0\u884c\u65f6\u7cfb\u7edf\u7684\u6df7\u5408\u96c6\u6210\u4e3a\u6df7\u5408AI-HPC\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.21009", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21009", "abs": "https://arxiv.org/abs/2509.21009", "authors": ["Wei Gao", "Yuheng Zhao", "Dakai An", "Tianyuan Wu", "Lunxi Cao", "Shaopan Xiong", "Ju Huang", "Weixun Wang", "Siran Yang", "Wenbo Su", "Jiamang Wang", "Lin Qu", "Bo Zheng", "Wei Wang"], "title": "RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training", "comment": "16pages,14 figures", "summary": "Reinforcement Learning (RL) is a pivotal post-training technique for\nenhancing the reasoning capabilities of Large Language Models (LLMs). However,\nsynchronous RL post-training often suffers from significant GPU\nunderutilization, referred to as bubbles, caused by imbalanced response lengths\nwithin rollout steps. Many RL systems attempt to alleviate this problem by\nrelaxing synchronization, but this can compromise training accuracy. In this\npaper, we introduce tail batching, a novel rollout scheduling strategy for\nsynchronous RL that systematically consolidates prompts leading to long-tail\nresponses into a small subset of rollout steps (long rounds), while ensuring\nthat the majority of steps (short rounds) involve only balanced, short\nrollouts. By excluding long responses from short rounds and rescheduling them\ninto a few designated long rounds, tail batching effectively reduces GPU idle\ntime during rollouts and significantly accelerates RL training without\nsacrificing accuracy. We present RollPacker, a system that fully harnesses the\nbenefits of tail batching through holistic optimizations across all three RL\nstages: elastic parallelism adaptation for rollout, dynamic resource allocation\nand scheduling for reward, and stream-based training. Empirical results show\nthat RollPacker achieves a 2.03x-2.56x end-to-end training time reduction\ncompared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5\nfamily of LLMs on up to 128 H800 GPUs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3atail batching\u7684\u65b0\u578brollout\u8c03\u5ea6\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u957f\u5c3e\u54cd\u5e94\u6574\u5408\u5230\u5c11\u91cf\u4e13\u95e8\u7684long rounds\u4e2d\uff0c\u6709\u6548\u51cf\u5c11GPU\u7a7a\u95f2\u65f6\u95f4\uff0c\u663e\u8457\u52a0\u901fRL\u8bad\u7ec3\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002", "motivation": "\u540c\u6b65RL\u540e\u8bad\u7ec3\u5b58\u5728GPU\u5229\u7528\u7387\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u7531\u4e8erollout\u6b65\u9aa4\u4e2d\u54cd\u5e94\u957f\u5ea6\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684\u6c14\u6ce1\u73b0\u8c61\u3002\u73b0\u6709\u7cfb\u7edf\u901a\u8fc7\u653e\u677e\u540c\u6b65\u6027\u6765\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u4f1a\u635f\u5bb3\u8bad\u7ec3\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165tail batching\u7b56\u7565\uff0c\u7cfb\u7edf\u5730\u5c06\u5bfc\u81f4\u957f\u5c3e\u54cd\u5e94\u7684\u63d0\u793a\u6574\u5408\u5230\u5c11\u91cfrollout\u6b65\u9aa4\uff08long rounds\uff09\u4e2d\uff0c\u786e\u4fdd\u5927\u591a\u6570\u6b65\u9aa4\uff08short rounds\uff09\u4ec5\u6d89\u53ca\u5e73\u8861\u7684\u77edrollout\u3002\u540c\u65f6\u63d0\u51faRollPacker\u7cfb\u7edf\uff0c\u5728\u4e09\u4e2aRL\u9636\u6bb5\u8fdb\u884c\u5168\u9762\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRollPacker\u5728128\u4e2aH800 GPU\u4e0a\u5bf9Qwen2.5\u7cfb\u5217LLMs\u5b9e\u73b0\u4e862.03x-2.56x\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\uff0c\u76f8\u6bd4veRL\u548cRLHFuse\u5206\u522b\u8fbe\u52302.24x\u7684\u52a0\u901f\u3002", "conclusion": "tail batching\u7b56\u7565\u548cRollPacker\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u89e3\u51b3\u540c\u6b65RL\u8bad\u7ec3\u4e2d\u7684GPU\u5229\u7528\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2509.21037", "categories": ["cs.DC", "cs.MS", "D.1.3; G.1.3; G.4"], "pdf": "https://arxiv.org/pdf/2509.21037", "abs": "https://arxiv.org/abs/2509.21037", "authors": ["Jakub Homola", "Ond\u0159ej Meca", "Lubom\u00edr \u0158\u00edha", "Tom\u00e1\u0161 Brzobohat\u00fd"], "title": "Utilizing Sparsity in the GPU-accelerated Assembly of Schur Complement Matrices in Domain Decomposition Methods", "comment": "12 pages (originally 10 pages without references), 10 figures,\n  submitted to SC25 conference", "summary": "Schur complement matrices emerge in many domain decomposition methods that\ncan solve complex engineering problems using supercomputers. Today, as most of\nthe high-performance clusters' performance lies in GPUs, these methods should\nalso be accelerated.\n  Typically, the offloaded components are the explicitly assembled dense Schur\ncomplement matrices used later in the iterative solver for multiplication with\na vector. As the explicit assembly is expensive, it represents a significant\noverhead associated with this approach to acceleration. It has already been\nshown that the overhead can be minimized by assembling the Schur complements\ndirectly on the GPU.\n  This paper shows that the GPU assembly can be further improved by wisely\nutilizing the sparsity of the input matrices. In the context of FETI methods,\nwe achieved a speedup of 5.1 in the GPU section of the code and 3.3 for the\nwhole assembly, making the acceleration beneficial from as few as 10\niterations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5229\u7528\u8f93\u5165\u77e9\u9635\u7a00\u758f\u6027\u6765\u6539\u8fdbGPU\u4e0aSchur\u8865\u77e9\u9635\u7ec4\u88c5\u7684\u65b9\u6cd5\uff0c\u5728FETI\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e865.1\u500d\u7684GPU\u90e8\u5206\u52a0\u901f\u548c3.3\u500d\u7684\u6574\u4f53\u7ec4\u88c5\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\u4e3b\u8981\u4f9d\u8d56GPU\uff0c\u9700\u8981\u52a0\u901f\u57df\u5206\u89e3\u65b9\u6cd5\u4e2d\u7684Schur\u8865\u77e9\u9635\u7ec4\u88c5\u8fc7\u7a0b\u3002\u663e\u5f0f\u7ec4\u88c5Schur\u8865\u77e9\u9635\u6210\u672c\u9ad8\u6602\uff0c\u662fGPU\u52a0\u901f\u7684\u4e3b\u8981\u5f00\u9500\u3002", "method": "\u901a\u8fc7\u660e\u667a\u5229\u7528\u8f93\u5165\u77e9\u9635\u7684\u7a00\u758f\u6027\u6765\u4f18\u5316GPU\u4e0a\u7684Schur\u8865\u77e9\u9635\u7ec4\u88c5\u8fc7\u7a0b\uff0c\u5728FETI\u65b9\u6cd5\u6846\u67b6\u4e0b\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684GPU\u8ba1\u7b97\u3002", "result": "\u5728GPU\u4ee3\u7801\u90e8\u5206\u5b9e\u73b0\u4e865.1\u500d\u7684\u52a0\u901f\uff0c\u6574\u4f53\u7ec4\u88c5\u8fc7\u7a0b\u5b9e\u73b0\u4e863.3\u500d\u7684\u52a0\u901f\uff0c\u4f7f\u5f97\u4ece\u4ec510\u6b21\u8fed\u4ee3\u5f00\u59cb\u5c31\u80fd\u83b7\u5f97\u52a0\u901f\u6536\u76ca\u3002", "conclusion": "\u5229\u7528\u8f93\u5165\u77e9\u9635\u7a00\u758f\u6027\u53ef\u4ee5\u663e\u8457\u6539\u8fdbGPU\u4e0a\u7684Schur\u8865\u77e9\u9635\u7ec4\u88c5\u6027\u80fd\uff0c\u4f7fGPU\u52a0\u901f\u5728\u66f4\u5c11\u7684\u8fed\u4ee3\u6b21\u6570\u4e0b\u5c31\u53d8\u5f97\u6709\u76ca\u3002"}}
{"id": "2509.21275", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21275", "abs": "https://arxiv.org/abs/2509.21275", "authors": ["Shiju Wang", "Yujie Wang", "Ao Sun", "Fangcheng Fu", "Zijian Zhu", "Bin Cui", "Xu Han", "Kaisheng Ma"], "title": "Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training", "comment": null, "summary": "Long context training is crucial for LLM's context extension. Existing\nschemes, such as sequence parallelism, incur substantial communication\noverhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness\nhinges on partitioning granularity. Batch-level PP dividing input samples\nexhibits high memory consumption in long-context scenario, whereas token-level\nPP splitting sequences into slices alleviates memory overhead but may incur\nhardware under-utilization. This trade-off motivates adaptively selecting PP\ngranularity to match resource and workload characteristics. Moreover, sequence\nlength distribution of the real-world dataset exhibits skewness, posing a\nchallenge on PP's workload balance and efficient scheduling. Current static PP\nscheduling methods overlook the variance of sequence length, leading to\nsuboptimal performance. In this paper, we propose Elastic Pipeline Parallelism\n(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource\nand workload heterogeneity. We build InfiniPipe, a distributed training system\nthat unleashes the potential of EPP via (1) a resource-aware and\nworkload-balanced sequence processor that splits long sequences and packs short\nones; and (2) a co-optimization methodology that jointly optimizes pipeline\nschedule and gradient checkpointing via a mechanism named stage-aware\nchunk-level adaptive checkpointing. Comprehensive experiments demonstrate that\nInfiniPipe achieves a 1.69x speedup over state-of-the-art systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5f39\u6027\u7ba1\u9053\u5e76\u884c\uff08EPP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u8c03token\u7ea7\u548cbatch\u7ea7\u7ba1\u9053\u5e76\u884c\u6765\u9002\u5e94\u8d44\u6e90\u548c\u8d1f\u8f7d\u5f02\u8d28\u6027\uff0c\u5e76\u6784\u5efa\u4e86InfiniPipe\u7cfb\u7edf\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u7cfb\u7edf\u5b9e\u73b0\u4e861.69\u500d\u52a0\u901f\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u5bf9LLM\u4e0a\u4e0b\u6587\u6269\u5c55\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6848\u5982\u5e8f\u5217\u5e76\u884c\u4f1a\u4ea7\u751f\u5927\u91cf\u901a\u4fe1\u5f00\u9500\u3002\u7ba1\u9053\u5e76\u884c\uff08PP\uff09\u80fd\u964d\u4f4e\u6b64\u6210\u672c\uff0c\u4f46\u5176\u6548\u679c\u53d6\u51b3\u4e8e\u5206\u533a\u7c92\u5ea6\u3002batch\u7ea7PP\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u5185\u5b58\u6d88\u8017\u9ad8\uff0ctoken\u7ea7PP\u80fd\u7f13\u89e3\u5185\u5b58\u5f00\u9500\u4f46\u53ef\u80fd\u5bfc\u81f4\u786c\u4ef6\u5229\u7528\u7387\u4e0d\u8db3\u3002\u6b64\u5916\uff0c\u771f\u5b9e\u6570\u636e\u96c6\u5e8f\u5217\u957f\u5ea6\u5206\u5e03\u7684\u504f\u659c\u6027\u5bf9PP\u7684\u8d1f\u8f7d\u5e73\u8861\u548c\u9ad8\u6548\u8c03\u5ea6\u6784\u6210\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5f39\u6027\u7ba1\u9053\u5e76\u884c\uff08EPP\uff09\uff0c\u6784\u5efaInfiniPipe\u5206\u5e03\u5f0f\u8bad\u7ec3\u7cfb\u7edf\uff0c\u5305\u542b\uff1a\uff081\uff09\u8d44\u6e90\u611f\u77e5\u548c\u8d1f\u8f7d\u5e73\u8861\u7684\u5e8f\u5217\u5904\u7406\u5668\uff0c\u7528\u4e8e\u5206\u5272\u957f\u5e8f\u5217\u548c\u6253\u5305\u77ed\u5e8f\u5217\uff1b\uff082\uff09\u534f\u540c\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9636\u6bb5\u611f\u77e5\u7684\u5757\u7ea7\u81ea\u9002\u5e94\u68c0\u67e5\u70b9\u673a\u5236\u8054\u5408\u4f18\u5316\u7ba1\u9053\u8c03\u5ea6\u548c\u68af\u5ea6\u68c0\u67e5\u70b9\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cInfiniPipe\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u7cfb\u7edf\u5b9e\u73b0\u4e861.69\u500d\u7684\u52a0\u901f\u3002", "conclusion": "EPP\u65b9\u6cd5\u80fd\u6709\u6548\u9002\u5e94\u8d44\u6e90\u548c\u8d1f\u8f7d\u5f02\u8d28\u6027\uff0cInfiniPipe\u7cfb\u7edf\u5728\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u4f18\u52bf\u3002"}}
