<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 7]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution](https://arxiv.org/abs/2509.05504)
*Karl Aaron Rudkowski,Sallar Ahmadi-Pour,Rolf Drechsler*

Main category: cs.PL

TL;DR: 该论文提出了CrosSym和SEFOS两种对立方法来实现虚拟原型中外设的符号执行验证，解决了现有SystemC核心修改或跨层次场景忽略的问题


<details>
  <summary>Details</summary>
Motivation: 虚拟原型在硬件开发中重要，但现有符号执行验证方法需要修改SystemC核心或忽略跨层次验证机会，特别是在外设子系统验证方面存在限制

Method: CrosSym修改SystemC核心，SEFOS修改现代符号执行引擎，两种方法都支持多抽象级别的跨层次符号执行

Result: 在多种抽象级别的各种外设上进行了涉广法评估，实现了(1)不同验证场景(2)识别300+变异体。SEFOS保持原生SystemC核心和外设，CrosSym运行时间和内存使用略优

Conclusion: 与仅支持交易级模建(TLM)的现有技术相比，该方法保持相似性能的同时支持跨层次符号执行，为虚拟原型验证提供了更灵活的解决方案

Abstract: Virtual Prototypes (VPs) are important tools in modern hardware development.
At high abstractions, they are often implemented in SystemC and offer early
analysis of increasingly complex designs. These complex designs often combine
one or more processors, interconnects, and peripherals to perform tasks in
hardware or interact with the environment. Verifying these subsystems is a
well-suited task for VPs, as they allow reasoning across different abstraction
levels. While modern verification techniques like symbolic execution can be
seamlessly integrated into VP-based workflows, they require modifications in
the SystemC kernel. Hence, existing approaches therefore modify and replace the
SystemC kernel, or ignore the opportunity of cross-level scenarios completely,
and would not allow focusing on special challenges of particular subsystems
like peripherals. We propose CrosSym and SEFOS, two opposing approaches for a
versatile symbolic execution of peripherals. CrosSym modifies the SystemC
kernel, while SEFOS instead modifies a modern symbolic execution engine. Our
extensive evaluation applies our tools to various peripherals on different
levels of abstractions. Both tools extensive sets of features are demonstrated
for (1) different verification scenarios, and (2) identifying 300+ mutants. In
comparison with each other, SEFOS convinces with the unmodified SystemC kernel
and peripheral, while CrosSym offers slightly better runtime and memory usage.
In comparison to the state-of-the-art, that is limited to Transaction Level
Modelling (TLM), our tools offered comparable runtime, while enabling
cross-level verification with symbolic execution.

</details>


### [2] [Fixed Parameter Tractable Linearizability Monitoring for Stack, Queue and Anagram Agnostic Data Types](https://arxiv.org/abs/2509.05586)
*Lee Zheng Han,Umang Mathur*

Main category: cs.PL

TL;DR: 提出了针对栈、队列和字谜无关数据类型(AADT)的固定参数可追踪算法，通过最大并发度参数化，利用前沿图和分区状态来限制搜索空间。


<details>
  <summary>Details</summary>
Motivation: 并发数据结构线性化验证是NP难问题，即使在简单类型下也是如此。需要开发在有限并发条件下的高效验证方法。

Method: 使用前沿图和分区状态来约束搜索空间。对于AADT，利用线性化等价性实现对数线性时间监控；对于栈，采用基于语法的方法并简化为矩阵乘法；对于队列，使用分割序列转换系统支持高效动态规划。

Result: 为栈、队列和AADT开发了固定参数可追踪算法，在有限并发条件下实现了高效的线性化验证。

Conclusion: 这些结果统一了在有界并发条件下对顺序敏感和字谜无关数据类型的可追踪性保证，为并发数据结构验证提供了新的理论框架。

Abstract: Verifying linearizability of concurrent data structures is NP-hard, even for
simple types. We present fixed-parameter tractable algorithms for monitoring
stacks, queues, and anagram-agnostic data types (AADTs), parameterized by the
maximum concurrency. Our approach leverages frontier graphs and partition
states to bound the search space. For AADTs, equivalence of linearizations
enables monitoring in log-linear time. For stacks, we introduce a grammar-based
method with a sub-cubic reduction to matrix multiplication, and for queues, a
split-sequence transition system supporting efficient dynamic programming.
These results unify tractability guarantees for both order-sensitive and
anagram-agnostic data types under bounded concurrency.

</details>


### [3] [Pacing Types: Safe Monitoring of Asynchronous Streams](https://arxiv.org/abs/2509.06724)
*Florian Kohn,Arthur Correnson,Jan Baumeister,Bernd Finkbeiner*

Main category: cs.PL

TL;DR: 这篇论文提出了一种新的类型系统（pacing types）来确保异步数据流监控器的运行时安全性，并在RTLola监控框架中实现。


<details>
  <summary>Details</summary>
Motivation: 异步数据流监控器是现代系统安全保障的关键组件，但异步数据同步策略可能导致细微的运行时错误，需要一种方法来确保监控器的可靠性。

Method: 设计并实现了pacing types类型系统，形式化了RTLola核心模块的类型系统，使用新的逻辑关系进行声音性证明。

Result: 形式化了pacing types的核心概念，完成了类型系统的声音性证明，确保了异步流监控器在运行时的良好行为。

Conclusion: pacing types类型系统有效地解决了异步数据流监控器设计中的同步问题，提供了一种可靠的方法来确保安全关键监控组件的运行时安全性。

Abstract: Stream-based monitoring is a real-time safety assurance mechanism for complex
cyber-physical systems such as unmanned aerial vehicles. In this context, a
monitor aggregates streams of input data from sensors and other sources to give
real-time statistics and assessments of the system's health. Since monitors are
safety-critical components, it is crucial to ensure that they are free of
potential runtime errors. One of the central challenges in designing reliable
stream-based monitors is to deal with the asynchronous nature of data streams:
in concrete applications, the different sensors being monitored produce values
at different speeds, and it is the monitor's responsibility to correctly react
to the asynchronous arrival of different streams of values. To ease this
process, modern frameworks for stream-based monitoring such as RTLola feature
an expressive specification language that allows to finely specify data
synchronization policies. While this feature dramatically simplifies the design
of monitors, it can also lead to subtle runtime errors. To mitigate this issue,
this paper presents pacing types, a novel type system implemented in RTLola to
ensure that monitors for asynchronous streams are well-behaved at runtime. We
formalize the essence of pacing types for a core fragment of RTLola, and
present a soundness proof of the pacing type system using a new logical
relation.

</details>


### [4] [Termination Analysis of Linear-Constraint Programs](https://arxiv.org/abs/2509.06752)
*Amir M. Ben-Amram,Samir Genaim,Joël Ouaknine,James Worrell*

Main category: cs.PL

TL;DR: 这篇论文统计了具有数值变量和线性约束过渡的程序的终止性分析技术，重点分析了可判定性结果、排序函数和过渡不变量等方法，而不涵盖实际编程语言或非线性模型。


<details>
  <summary>Details</summary>
Motivation: 程序终止性分析存在不可判定性问题，需要系统性探索缓解这种本质困难的方法。

Method: 系统性探讨了基础可判定性结果、排序函数以及分离良基过渡不变量等技术，同时也分析了非终止性证据。

Result: 不同方法在表达能力与计算复杂性之间存在特定的权衡关系，为程序终止性分析提供了系统化的解决方案。

Conclusion: 该统计性研究为数值变量程序的终止性分析提供了全面的技术概览，虽然不涵盖实际编程语言或更复杂的模型，但在理论基础方面具有重要价值。

Abstract: This Survey provides an overview of techniques in termination analysis for
programs with numerical variables and transitions defined by linear
constraints. This subarea of program analysis is challenging due to the
existence of undecidable problems, and this Survey systematically explores
approaches that mitigate this inherent difficulty. These include foundational
decidability results, the use of ranking functions, and disjunctive
well-founded transition invariants. The Survey also discusses non-termination
witnesses, used to prove that a program will not halt. We examine the
algorithmic and complexity aspects of these methods, showing how different
approaches offer a trade-off between expressive power and computational
complexity. The Survey does not discuss how termination analysis is performed
on real-world programming languages, nor does it consider more expressive
abstract models that include non-linear arithmetic, probabilistic choice, or
term rewriting systems.

</details>


### [5] [Dato: A Task-Based Programming Model for Dataflow Accelerators](https://arxiv.org/abs/2509.06794)
*Shihan Fang,Hongzheng Chen,Niansong Zhang,Jiajie Li,Han Meng,Adrian Liu,Zhiru Zhang*

Main category: cs.PL

TL;DR: Dato是一个用于数据流加速器的Python嵌入式任务编程模型，通过将数据通信和分片作为一等类型构造，简化了高性能代码开发，在NPU和FPGA上均实现了接近理论峰值的高性能。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习工作负载的计算需求超过了当前内存系统的承载能力，许多内核因数据移动而非计算而停滞。现有编程模型难以有效利用数据流加速器的片上流处理能力，低层接口开发成本高，高层语言又抽象过度限制了优化。

Method: 提出Dato编程模型，将数据通信和分片作为一等类型构造。开发者通过显式流类型连接任务图，使用布局类型指定分片输入。任务首先虚拟映射到加速器空间结构，编译器再生成符合硬件约束的物理映射。

Result: 在AMD Ryzen AI NPU上，Dato对GEMM实现了84%的硬件利用率，注意力核相比最先进商业框架加速2.81倍。在FPGA上生成定制脉动阵列时，性能超越主流框架，达到理论峰值性能的98%。

Conclusion: Dato编程模型显著降低了编写优化代码的负担，同时实现了高性能，为数据流加速器提供了一种有效的编程解决方案。

Abstract: Recent deep learning workloads increasingly push computational demand beyond
what current memory systems can sustain, with many kernels stalling on data
movement rather than computation. While modern dataflow accelerators
incorporate on-chip streaming to mitigate off-chip bandwidth limitations,
existing programming models struggle to harness these capabilities effectively.
Low-level interfaces provide fine-grained control but impose significant
development overhead, whereas high-level tile-based languages abstract away
communication details, restricting optimization and forcing compilers to
reconstruct the intended dataflow. We present Dato, a Python-embedded,
task-based programming model for dataflow accelerators that elevates data
communication and sharding to first-class type constructs. Developers write
programs as a graph of tasks connected via explicit stream types, with sharded
inputs specified using layout types. These tasks are first mapped virtually
onto the accelerator's spatial fabric, and the compiler then generates a
physical mapping that respects hardware constraints. Experimental results on
both AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves
high performance while significantly reducing the burden of writing optimized
code. On the NPU, Dato attains up to 84% hardware utilization for GEMM and
delivers a 2.81x speedup on attention kernels compared to a state-of-the-art
commercial framework. On the FPGA, Dato surpasses leading frameworks in
performance when generating custom systolic arrays, achieving 98% of the
theoretical peak performance.

</details>


### [6] [MIO: Multiverse Debugging in the Face of Input/Output -- Extended Version with Additional Appendices](https://arxiv.org/abs/2509.06845)
*Tom Lauwaerts,Maarten Steevens,Christophe Scholliers*

Main category: cs.PL

TL;DR: 这篇论文提出了一种新的多宇宙调试方法MIO，解决微控制器上非确定性程序调试中可能访问到不可达程序状态的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多宇宙调试器在调试包含输入/输出操作的程序时，可能揭示无法访问的程序状态，这会很大程序影响调试效果和导致误判。

Method: 设计了一种新的多宇宙调试方法，支持广泛的输入/输出操作，并且从语义上证明了调试器的正确性，确保只探索正常执行中可到达的程序状态。

Result: 开发了原型系统MIO，基于WARDuino WebAssembly虚拟机，并通过一个Lego Mindstorms颜色调节器案例在STM32微控制器上验证了方法的可行性和效率。

Conclusion: MIO方法能够有效地解决微控制器上非确定性程序调试中的访问不可达状态问题，提供了一种可靠的多宇宙调试方案。

Abstract: Debugging non-deterministic programs on microcontrollers is notoriously
challenging, especially when bugs manifest in unpredictable, input-dependent
execution paths. A recent approach, called multiverse debugging, makes it
easier to debug non-deterministic programs by allowing programmers to explore
all potential execution paths. Current multiverse debuggers enable both forward
and backward traversal of program paths, and some facilitate jumping to any
previously visited states, potentially branching into alternative execution
paths within the state space.
  Unfortunately, debugging programs that involve input/output operations using
existing multiverse debuggers can reveal inaccessible program states, i.e.
states which are not encountered during regular execution. This can
significantly hinder the debugging process, as the programmer may spend
substantial time exploring and examining inaccessible program states, or worse,
may mistakenly assume a bug is present in the code, when in fact, the issue is
caused by the debugger.
  This paper presents a novel approach to multiverse debugging, which can
accommodate a broad spectrum of input/output operations. We provide the
semantics of our approach and prove the correctness of our debugger, ensuring
that despite having support for a wide range of input/output operations the
debugger will only explore those program states which can be reached during
regular execution.
  We have developed a prototype, called MIO, leveraging the WARDuino
WebAssembly virtual machine to demonstrate the feasibility and efficiency of
our techniques. As a demonstration of the approach we highlight a color dial
built with a Lego Mindstorms motor, and color sensor, providing a tangible
example of how our approach enables multiverse debugging for programs running
on an STM32 microcontroller.

</details>


### [7] [Mechanized Metatheory of Forward Reasoning for End-to-End Linearizability Proofs](https://arxiv.org/abs/2509.06872)
*Zachary Kent,Ugur Y. Yavuz,Siddhartha Jayanti,Stephanie Balzer,Guy Blelloch*

Main category: cs.PL

TL;DR: 本文形式化并机械化了Jayanti等人的前向推理线性化证明技术，在Rocq中验证其安全性和完备性，并通过并发寄存器案例实现了端到端的验证证明。


<details>
  <summary>Details</summary>
Motivation: 虽然Jayanti等人提出了第一个安全完备的前向推理线性化证明技术，但其重要的形式化理论结果仍未得到机械化验证，导致无法生成端到端的验证证明。

Method: 在Rocq中形式化前向推理技术，机械化其安全性和完备性证明，并使用该方法为简单并发寄存器生成端到端的线性化证明。

Result: 成功实现了前向推理技术的形式化机械化，减少了可信计算基础的规模，并生成了端到端的验证证明。

Conclusion: 该工作填补了线性化证明技术形式化的重要空白，为生成验证证明提供了可靠的基础，对并发数据结构正确性验证具有重要意义。

Abstract: In the past decade, many techniques have been developed to prove
linearizability, the gold standard of correctness for concurrent data
structures. Intuitively, linearizability requires that every operation on a
concurrent data structure appears to take place instantaneously, even when
interleaved with other operations. Most recently, Jayanti et al. presented the
first sound and complete "forward reasoning" technique for proving
linearizability that relates the behavior of a concurrent data structure to a
reference atomic data structure as time moves forward. This technique can be
used to produce machine-checked proofs of linearizability in TLA+. However,
while Jayanti et al.'s approach is shown to be sound and complete, a
mechanization of this important metatheoretic result is still outstanding. As a
result, it is not possible to produce verified end-to-end proofs of
linearizability. To reduce the size of this trusted computing base, we
formalize this forward reasoning technique and mechanize proofs of its
soundness and completeness in Rocq. As a case study, we use the approach to
produce a verified end-to-end proof of linearizability for a simple concurrent
register.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across Multiple Formats](https://arxiv.org/abs/2509.05303)
*Sam Davidson,Li Sun,Bhavana Bhasker,Laurent Callot,Anoop Deoras*

Main category: cs.DC

TL;DR: 这篇论文提出了多格式IaC生成性能测评基准Multi-IaC-Bench，用于评估大语言模型在AWS CloudFormation、Terraform咋CDK格式上的IaC生成能力，发现现代LLM语法正确率高但语义对齐仍面挑战。


<details>
  <summary>Details</summary>
Motivation: 不同云服务提供商使用不同的IaC格式，缺乏标准化格式增加了云部署复杂性，而现有LLM在IaC自动化方面的进展受限于缺乏跨多格式的综合性测评基准。

Method: 构建Multi-IaC-Bench数据集，包含AWS CloudFormation、Terraform咋CDK格式的三元组（初始模板、自然语言修改请求、更新后模板），通过合成数据生成流水线进行严格验证。评估多个先进LLM模型的IaC生成和翻译能力。

Result: 现代LLM在多种IaC格式上能够达到高语法正确率（>95%），但在语义对齐和处理复杂基础设施模式方面仍面临重大挑战。分析展示提示工程和重试机制对IaC生成成功的重要性。

Conclusion: 论文提供了一个标准化的跨多IaC格式测评基准，为AI辅助基础设施管理领域的进一步研究奠定基础，并建立了关键性能评估指标。

Abstract: Infrastructure as Code (IaC) is fundamental to modern cloud computing,
enabling teams to define and manage infrastructure through machine-readable
configuration files. However, different cloud service providers utilize diverse
IaC formats. The lack of a standardized format requires cloud architects to be
proficient in multiple IaC languages, adding complexity to cloud deployment.
While Large Language Models (LLMs) show promise in automating IaC creation and
maintenance, progress has been limited by the lack of comprehensive benchmarks
across multiple IaC formats. We present Multi-IaC-Bench, a novel benchmark
dataset for evaluating LLM-based IaC generation and mutation across AWS
CloudFormation, Terraform, and Cloud Development Kit (CDK) formats. The dataset
consists of triplets containing initial IaC templates, natural language
modification requests, and corresponding updated templates, created through a
synthetic data generation pipeline with rigorous validation. We evaluate
several state-of-the-art LLMs on Multi-IaC-Bench, demonstrating that while
modern LLMs can achieve high success rates (>95%) in generating syntactically
valid IaC across formats, significant challenges remain in semantic alignment
and handling complex infrastructure patterns. Our ablation studies highlight
the importance of prompt engineering and retry mechanisms in successful IaC
generation. We release Multi-IaC-Bench to facilitate further research in
AI-assisted infrastructure management and establish standardized evaluation
metrics for this crucial domain.

</details>


### [9] [A Simple and Robust Protocol for Distributed Counting](https://arxiv.org/abs/2509.05870)
*Edith Cohen,Moshe Shechner,Uri Stemmer*

Main category: cs.DC

TL;DR: 本文重新审视分布式计数问题，发现Huang等人(2012)的随机化协议在非遗忘设置下不鲁棒，并提出了一种新的简单鲁棒协议，达到了最优通信复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决分布式计数问题中现有协议在非遗忘设置下的鲁棒性问题，Huang等人的协议虽然通信复杂度低但仅限于遗忘设置，而Xiong等人的协议虽然鲁棒但复杂度较高且非最优。

Method: 首先构造显式自适应攻击证明Huang协议的非鲁棒性，然后设计新的简单鲁棒协议，采用随机化技术实现最优通信复杂度。

Result: 证明了Huang协议在自适应攻击下会失去准确性，提出的新协议达到了O(√k/ε log N)的最优通信复杂度，比Xiong等人的协议更简单。

Conclusion: 新协议是首个在自适应设置下匹配最优遗忘复杂度的分布式计数协议，具有简单性和最优性的双重优势。

Abstract: We revisit the distributed counting problem, where a server must continuously
approximate the total number of events occurring across $k$ sites while
minimizing communication. The communication complexity of this problem is known
to be $\Theta(\frac{k}{\epsilon}\log N)$ for deterministic protocols. Huang,
Yi, and Zhang (2012) showed that randomization can reduce this to
$\Theta(\frac{\sqrt{k}}{\epsilon}\log N)$, but their analysis is restricted to
the {\em oblivious setting}, where the stream of events is independent of the
protocol's outputs.
  Xiong, Zhu, and Huang (2023) presented a robust protocol for distributed
counting that removes the oblivious assumption. However, their communication
complexity is suboptimal by a $polylog(k)$ factor and their protocol is
substantially more complex than the oblivious protocol of Huang et al. (2012).
This left open a natural question: could it be that the simple protocol of
Huang et al. (2012) is already robust?
  We resolve this question with two main contributions. First, we show that the
protocol of Huang et al. (2012) is itself not robust by constructing an
explicit adaptive attack that forces it to lose its accuracy. Second, we
present a new, surprisingly simple, robust protocol for distributed counting
that achieves the optimal communication complexity of
$O(\frac{\sqrt{k}}{\epsilon} \log N)$. Our protocol is simpler than that of
Xiong et al. (2023), perhaps even simpler than that of Huang et al. (2012), and
is the first to match the optimal oblivious complexity in the adaptive setting.

</details>


### [10] [DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers](https://arxiv.org/abs/2509.06046)
*Philip Adams,Menghao Li,Shi Zhang,Li Tan,Qi Chen,Mingqin Li,Zengzhong Li,Knut Risvik,Harsha Vardhan Simhadri*

Main category: cs.DC

TL;DR: DISTRIBUTEDANN是一个分布式向量搜索服务，能够在超过1000台机器上搜索500亿向量的图索引，提供26ms中位查询延迟和10万QPS，比现有方案效率高6倍


<details>
  <summary>Details</summary>
Motivation: 现有的分区和路由策略在扩展向量搜索系统时效率不足，需要更高效的分布式向量搜索解决方案来支持大规模应用如Bing搜索引擎

Method: 使用两个成熟组件构建：分布式键值存储和内存ANN索引，替代传统的横向扩展架构

Result: 实现了在1000+机器上搜索500亿向量图索引，26ms中位查询延迟，10万+ QPS，效率比现有方案高6倍

Conclusion: DISTRIBUTEDANN成功替代了Bing搜索引擎的传统架构，证明了其在超大规模向量搜索场景中的高效性和实用性

Abstract: We present DISTRIBUTEDANN, a distributed vector search service that makes it
possible to search over a single 50 billion vector graph index spread across
over a thousand machines that offers 26ms median query latency and processes
over 100,000 queries per second. This is 6x more efficient than existing
partitioning and routing strategies that route the vector query to a subset of
partitions in a scale out vector search system. DISTRIBUTEDANN is built using
two well-understood components: a distributed key-value store and an in-memory
ANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for
serving the Bing search engine, and we share our experience from making this
transition.

</details>


### [11] [Gathering in Non-Vertex-Transitive Graphs Under Round Robin](https://arxiv.org/abs/2509.06064)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 本文研究了在非顶点传递图上机器人聚集问题的分布式算法，考虑了存在多重性且无法检测多重性的敌对情况，使用轮询调度器，提供了完整的算法解决方案和复杂度分析。


<details>
  <summary>Details</summary>
Motivation: 研究在敌对条件下（存在多重性且无法检测）的非顶点传递图上机器人聚集问题，填补了传统设置下的研究空白。

Method: 采用OBLOT模型，机器人沿图边移动，使用轮询调度器逐个激活机器人，设计了适用于非顶点传递图的聚集算法。

Result: 提出了一个完整的算法解决方案，证明了其正确性，并分析了时间复杂度，实现了在任何初始配置下的机器人聚集。

Conclusion: 成功解决了非顶点传递图上在敌对条件下的机器人聚集问题，为这类图的聚集算法提供了完整的理论框架和实用解决方案。

Abstract: The Gathering problem for a swarm of robots asks for a distributed algorithm
that brings such entities to a common place, not known in advance. We consider
the well-known OBLOT model with robots constrained to move along the edges of a
graph, hence gathering in one vertex, eventually. Despite the classical setting
under which the problem has been usually approached, we consider the `hostile'
case where: i) the initial configuration may contain multiplicities, i.e. more
than one robot may occupy the same vertex; ii) robots cannot detect
multiplicities. As a scheduler for robot activation, we consider the
"favorable" round-robin case, where robots are activated one at a time.
  Our objective is to achieve a complete characterization of the problem in the
broad context of non-vertex-transitive graphs, i.e., graphs where the vertices
are partitioned into at least two different classes of equivalence. We provide
a resolution algorithm for any configuration of robots moving on such graphs,
along with its correctness. Furthermore, we analyze its time complexity.

</details>


### [12] [20 Years in Life of a Smart Building: A retrospective](https://arxiv.org/abs/2509.06229)
*Karolina Skrivankova,Mark Handley,Stephen Hailes*

Main category: cs.DC

TL;DR: KaOS是一个基于现成IoT硬件的分布式控制平台，通过容器化和资源管理实现灵活、安全、容错的智能建筑自动化系统


<details>
  <summary>Details</summary>
Motivation: 解决智能建筑自动化系统面临的硬件故障、供应商过时、安全威胁等挑战，这些挑战限制了大规模智能自动化部署的可行性

Method: 利用容器化和托管资源访问来支持控制应用和分布式系统操作，使用经济实惠的现成IoT硬件构建分布式控制平台

Result: 初步评估证实了该方法的实际可行性，突显了其在长时间范围内可持续维护和逐步演进建筑控制功能的潜力

Conclusion: KaOS平台能够在不牺牲成本效益的前提下实现灵活性、安全性和容错性，为构建稳健且可演进的智能建筑自动化系统提供了可行解决方案

Abstract: Operating an intelligent smart building automation system in 2025 is met with
many challenges: hardware failures, vendor obsolescence, evolving security
threats and more. None of these have been comprehensibly addressed by the
industrial building nor home automation industries, limiting feasibility of
operating large, truly smart automation deployments. This paper introduces
KaOS, a distributed control platform for constructing robust and evolvable
smart building automation systems using affordable, off-the-shelf IoT hardware.
Supporting control applications and distributed system operations by leveraging
containerisation and managed resource access, KaOS seeks to achieve
flexibility, security, and fault tolerance without sacrificing
cost-effectiveness. Initial evaluation confirms the practical feasibility of
our approach, highlighting its potential to sustainably maintain and
incrementally evolve building control functionalities over extended timeframes.

</details>


### [13] [FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving](https://arxiv.org/abs/2509.06261)
*Kyungmin Bin,Seungbeom Choi,Jimyoung Son,Jieun Choi,Daseul Bae,Daehyeon Baek,Kihyo Moon,Minsung Jang,Hyojung Lee*

Main category: cs.DC

TL;DR: FineServe是一个用于混合精度大语言模型推理服务的框架，通过KV Slab内存管理和两级调度系统，解决了量化模型的内存碎片和调度效率问题，显著提升了服务质量和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 量化大语言模型虽然能提高吞吐量和减少内存使用，但存在KV缓存块大小较小导致的内存碎片问题，以及量化与非量化模型资源使用模式不同带来的调度挑战。

Method: 提出FineServe框架，包含：(1) KV Slab - 基于模型量化特性的自适应内存管理技术，动态分配KV缓存；(2) 两级调度框架 - 全局调度器根据请求率、延迟SLO和内存约束进行模型放置，本地调度器根据实时请求波动自适应调整批大小。

Result: 实验结果显示，FineServe相比最先进的GPU共享系统，实现了高达2.2倍的SLO达成率和1.8倍的token生成吞吐量提升。

Conclusion: FineServe通过精确感知的内存管理和自适应调度，有效解决了混合精度LLM推理服务中的内存碎片和调度效率问题，显著提升了服务性能。

Abstract: Recent advances in Post-Training Quantization (PTQ) techniques have
significantly increased demand for serving quantized large language models
(LLMs), enabling higher throughput and substantially reduced memory usage with
minimal accuracy loss. Quantized models address memory constraints in LLMs and
enhance GPU resource utilization through efficient GPU sharing. However,
quantized models have smaller KV block sizes than non-quantized models, causing
limited memory efficiency due to memory fragmentation. Also, distinct resource
usage patterns between quantized and non-quantized models require efficient
scheduling to maximize throughput. To address these challenges, we propose
FineServe, an inference serving framework for mixed-precision LLMs. FineServe's
key contributions include: (1) KV Slab, a precision-aware adaptive memory
management technique dynamically allocating KV cache based on model
quantization characteristics, significantly reducing GPU memory fragmentation,
and (2) a two-level scheduling framework comprising a global scheduler that
places models to GPUs based on request rates, latency SLOs, and memory
constraints and efficiency, and a local scheduler that adaptively adjusts batch
sizes according to real-time request fluctuations. Experimental results
demonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x
higher token generation throughput compared to the state-of-the-art GPU sharing
systems.

</details>


### [14] [MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS](https://arxiv.org/abs/2509.06362)
*Mo Xuan,Zhang yue,Wu Weigang*

Main category: cs.DC

TL;DR: MaaSO是一个针对MaaS平台的智能编排系统，通过异构实例配置和SLO感知请求分发，显著提升SLO满足率和降低延迟


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理系统使用相同配置的实例，无法利用不同并行策略和批处理大小带来的性能差异来满足多样化的SLO需求

Method: 包含三个模块：性能分析器（分析不同配置下的实例性能）、配置优化器（优化异构实例配置）、分发器（SLO感知请求分发和防止级联超时）

Result: 相比现有方法，SLO满足率提升15-30%，响应延迟降低40-60%，编排开销显著降低

Conclusion: MaaSO通过利用LLM实例的异构配置能力，有效解决了MaaS平台中多样化SLO需求的编排问题

Abstract: Model-as-a-Service (MaaS) platforms face diverse Service Level Objective
(SLO) requirements stemming from various large language model (LLM)
applications, manifested in contextual complexity, first-token latency, and
between-token latency. On the other hand, an LLM instance, when configured with
different parallelism strategies and inference batch sizes, exhibits distinct
performance characteristics and can thus be used to serve different SLO
requirements. However, current LLM inference systems typically deploy instances
of the same model with identical configurations, lacking mechanisms to leverage
such heterogeneity. To fill this research gap, we propose MaaSO, the first MaaS
Orchestrator, which comprises three modules: (1) a profiler characterizing
instance performance under diverse parallelism strategies and inference batch
sizes; (2) a placer optimizing heterogeneous instance configurations; (3) a
distributor enabling SLO-aware request distribution and preventing cascaded
timeouts in continuous batching. Experiments show that MaaSO improves the SLO
satisfaction ratio by 15 to 30% and reduces response latency by 40 to 60%
compared to existing approaches, and significantly lowers overall orchestration
overhead.

</details>


### [15] [IM-PIR: In-Memory Private Information Retrieval](https://arxiv.org/abs/2509.06514)
*Mpoki Mwaisela,Peterson Yuhala,Pascal Felber,Valerio Schiavoni*

Main category: cs.DC

TL;DR: 本文提出了首个基于内存计算(PIM)的多服务器私有信息检索(PIR)架构IM-PIR，通过利用PIM的高并行性和内存带宽优势，相比传统CPU方案实现了3.7倍以上的查询吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 当前PIR实现由于需要扫描大量数据库(GB级别)而受限于内存带宽瓶颈，传统处理器中心架构无法满足计算需求。PIM作为一种新兴计算范式，能够有效解决内存带宽瓶颈并提供大规模并行处理能力

Method: 基于UPMEM PIM商业化架构，设计了IM-PIR方法，将多服务器PIR的算法操作与PIM架构的核心优势(高并行性和内存带宽)进行对齐和优化实现

Result: 评估显示基于PIM的多服务器PIR实现相比标准CPU-based PIR方法，查询吞吐量提升了3.7倍以上

Conclusion: PIM架构能够显著提升PIR性能，证明了将数据密集型密码原语与新兴内存计算范式结合的有效性

Abstract: Private information retrieval (PIR) is a cryptographic primitive that allows
a client to securely query one or multiple servers without revealing their
specific interests. In spite of their strong security guarantees, current PIR
constructions are computationally costly. Specifically, most PIR
implementations are memory-bound due to the need to scan extensive databases
(in the order of GB), making them inherently constrained by the limited memory
bandwidth in traditional processor-centric computing
architectures.Processing-in-memory (PIM) is an emerging computing paradigm that
augments memory with compute capabilities, addressing the memory bandwidth
bottleneck while simultaneously providing extensive parallelism.Recent research
has demonstrated PIM's potential to significantly improve performance across a
range of data-intensive workloads, including graph processing, genome analysis,
and machine learning.
  In this work, we propose the first PIM-based architecture for multi-server
PIR. We discuss the algorithmic foundations of the latter and show how its
operations align with the core strengths of PIM architectures: extensive
parallelism and high memory bandwidth. Based on this observation, we design and
implement IM-PIR, a PIM-based multi-server PIR approach on top of UPMEM PIM,
the first openly commercialized PIM architecture. Our evaluation demonstrates
that a PIM-based multi-server PIR implementation significantly improves query
throughput by more than 3.7x when compared to a standard CPU-based PIR
approach.

</details>


### [16] [Mangrove: Fast and Parallelizable State Replication for Blockchains](https://arxiv.org/abs/2509.06616)
*Anton Paramonov,Yann Vonlanthen,Quentin Kniep,Jakub Sliwinski,Roger Wattenhofer*

Main category: cs.DC

TL;DR: Mangrove是一种新颖的区块链扩展方法，通过为每个智能合约使用独立的共识实例来实现并行智能合约支持，无需全局排序。


<details>
  <summary>Details</summary>
Motivation: 解决传统单体区块链中单一共识机制导致交易严格全局排序的性能瓶颈问题，实现更高的并行处理能力。

Method: 采用Parallel Optimistic Agreement机制确保并行运行时不提交冲突交易，对简单交易使用轻量级Byzantine Reliable Broadcast原语降低延迟。

Result: 在乐观条件下（无恶意行为且网络同步），协议可在创建和执行交易之间实现2个通信步骤的延迟。

Conclusion: Mangrove为构建支持并行智能合约的高性能区块链提供了一种有效的扩展解决方案，特别在理想网络条件下表现出优异的性能。

Abstract: Mangrove is a novel scaling approach to building blockchains with parallel
smart contract support. Unlike in monolithic blockchains, where a single
consensus mechanism determines a strict total order over all transactions,
Mangrove uses separate consensus instances per smart contract, without a global
order. To allow multiple instances to run in parallel while ensuring that no
conflicting transactions are committed, we propose a mechanism called Parallel
Optimistic Agreement. Additionally, for simple transactions, we leverage a
lightweight Byzantine Reliable Broadcast primitive to reduce latency. Mangrove
is optimized for performance under optimistic conditions, where there is no
misbehavior and the network is synchronous. Under these conditions, our
protocol can achieve a latency of 2 communication steps between creating and
executing a transaction.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device](https://arxiv.org/abs/2509.05451)
*Niansong Zhang,Wenbo Zhu,Courtney Golden,Dan Ilan,Hongzheng Chen,Christopher Batten,Zhiru Zhang*

Main category: cs.AR

TL;DR: 计算在SRAM中的商业设备GSI APU在实际工作负荷下的性能和能源效率分析，通过优化技术在RAG应用中实现了比CPU基准更快4.8-6.6倍的加速效果，且比GPU更节能54.4-117.9倍


<details>
  <summary>Details</summary>
Motivation: 过往对计算在SRAM架构的评估主要依赖仿真器或小规模原型，导致对其实际潜力理解有限。本研究通过实际商业设备评测来验证计算在SRAM在复杂实际应用中的可行性和优势

Method: 对商业计算在SRAM设备GSI APU进行综合性能和能源实际测量，与CPU、GPU等常规架构进行对比。提出了分析框架来建模性能交易，并提出三种优化技术：通信感知的缩减映射、聚合DMA和广播友好的数据布局

Result: 在10GB-200GB大规模数据集上的RAG应用中，优化后的计算在SRAM系统对比优化CPU基准实现了4.8-6.6倍检索加速，结构化延迟提升1.1-1.8倍。在保持与NVIDIA A6000 GPU相同性能的情况下，能源效率提升了54.4-117.9倍

Conclusion: 这些发现验证了计算在SRAM在复杂实际应用中的可行性，为该技术的进一步发展提供了指导。计算在SRAM架构在数据密集型应用中显示出显著的性能和能源效率优势

Abstract: Compute-in-SRAM architectures offer a promising approach to achieving higher
performance and energy efficiency across a range of data-intensive
applications. However, prior evaluations have largely relied on simulators or
small prototypes, limiting the understanding of their real-world potential. In
this work, we present a comprehensive performance and energy characterization
of a commercial compute-in-SRAM device, the GSI APU, under realistic workloads.
We compare the GSI APU against established architectures, including CPUs and
GPUs, to quantify its energy efficiency and performance potential. We introduce
an analytical framework for general-purpose compute-in-SRAM devices that
reveals fundamental optimization principles by modeling performance trade-offs,
thereby guiding program optimizations.
  Exploiting the fine-grained parallelism of tightly integrated memory-compute
architectures requires careful data management. We address this by proposing
three optimizations: communication-aware reduction mapping, coalesced DMA, and
broadcast-friendly data layouts. When applied to retrieval-augmented generation
(RAG) over large corpora (10GB--200GB), these optimizations enable our
compute-in-SRAM system to accelerate retrieval by 4.8$\times$--6.6$\times$ over
an optimized CPU baseline, improving end-to-end RAG latency by
1.1$\times$--1.8$\times$. The shared off-chip memory bandwidth is modeled using
a simulated HBM, while all other components are measured on the real
compute-in-SRAM device. Critically, this system matches the performance of an
NVIDIA A6000 GPU for RAG while being significantly more energy-efficient
(54.4$\times$-117.9$\times$ reduction). These findings validate the viability
of compute-in-SRAM for complex, real-world applications and provide guidance
for advancing the technology.

</details>


### [18] [High Utilization Energy-Aware Real-Time Inference Deep Convolutional Neural Network Accelerator](https://arxiv.org/abs/2509.05688)
*Kuan-Ting Lin,Ching-Te Chiu,Jheng-Yi Chang,Shi-Zong Huang,Yu-Ting Li*

Main category: cs.AR

TL;DR: 这篇论文提出了一种高利用率的深度卷积神经网络加速器，通过数据重用策略和芯片内计算优化，大幅减少数据访问和提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度卷积神经网络在边缘设备上进行推理时计算复杂度高、数据访问量大、延迟高的问题，以满足实际应用的实时性需求。

Method: 1、使用1x1卷积核作为计算单元基础。2、设计数据重用策略和环形流数据流。3、引入芯片内池化模块直接完成计算。

Result: 实现了533倍的数据访问量减少，与VGG16和MobileNet模型实现实时执行，继比VWA设计速度提升7.52倍，能消率提升1.92倍。

Conclusion: 该加速器设计通过数据重用策略和芯片内计算优化，有效降低了数据传输开销，实现了高硬件利用率和高能效的实时推理能力。

Abstract: Deep convolution Neural Network (DCNN) has been widely used in computer
vision tasks. However, for edge devices even inference has too large
computational complexity and data access amount. The inference latency of
state-of-the-art models are impractical for real-world applications. In this
paper, we propose a high utilization energy-aware real-time inference deep
convolutional neural network accelerator, which improves the performance of the
current accelerators. First, we use the 1x1 size convolution kernel as the
smallest unit of the computing unit. Then we design suitable computing unit
based on the requirements of each model. Secondly, we use Reuse Feature SRAM to
store the output of the current layer in the chip and use the value as the
input of the next layer. Moreover, we import Output Reuse Strategy and Ring
Stream Dataflow to reduce the amount of data exchange between chips and DRAM.
Finally, we present On-fly Pooling Module to let the calculation of the Pooling
layer directly complete in the chip. With the aid of the proposed method, the
implemented acceleration chip has an extremely high hardware utilization rate.
We reduce a generous amount of data transfer on the specific module, ECNN.
Compared to the methods without reuse strategy, we can reduce 533 times of data
access amount. At the same time, we have enough computing power to perform
real-time execution of the existing image classification model, VGG16 and
MobileNet. Compared with the design in VWA, we can speed up 7.52 times and have
1.92x energy efficiency

</details>


### [19] [Hardware Acceleration of Kolmogorov-Arnold Network (KAN) in Large-Scale Systems](https://arxiv.org/abs/2509.05937)
*Wei-Hsing Huang,Jianwei Jia,Yuyao Kong,Faaiq Waqar,Tai-Hao Wen,Meng-Fan Chang,Shimeng Yu*

Main category: cs.AR

TL;DR: 本文提出了一种KAN网络的算法-硬件协同设计方法，通过量化、稀疏映射和模拟存内计算等技术，在保持精度的同时显著降低了硬件开销。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold Networks (KAN) 虽然参数效率高，但其B样条函数组件在硬件加速方面存在复杂性，需要专门的电路实现，这促使研究者开发更高效的硬件加速方案。

Method: 采用算法-硬件协同设计：算法层面包括对齐对称性和PowerGap量化、稀疏映射策略；电路层面包括N:1时间调制动态电压输入生成器和模拟存内计算(ACIM)电路。

Result: 在22nm RRAM-ACIM原型芯片上验证，大规模任务参数增加500Kx-807Kx时，面积开销仅增加28Kx-41Kx，功耗增加51x-94x，精度损失仅0.11%-0.23%。

Conclusion: 所提出的架构具有良好的扩展潜力，能够在保持高精度的同时显著降低硬件资源消耗，为KAN网络的实际部署提供了可行的硬件解决方案。

Abstract: Recent developments have introduced Kolmogorov-Arnold Networks (KAN), an
innovative architectural paradigm capable of replicating conventional deep
neural network (DNN) capabilities while utilizing significantly reduced
parameter counts through the employment of parameterized B-spline functions
with trainable coefficients. Nevertheless, the B-spline functional components
inherent to KAN architectures introduce distinct hardware acceleration
complexities. While B-spline function evaluation can be accomplished through
look-up table (LUT) implementations that directly encode functional mappings,
thus minimizing computational overhead, such approaches continue to demand
considerable circuit infrastructure, including LUTs, multiplexers, decoders,
and related components. This work presents an algorithm-hardware co-design
approach for KAN acceleration. At the algorithmic level, techniques include
Alignment-Symmetry and PowerGap KAN hardware aware quantization, KAN sparsity
aware mapping strategy, and circuit-level techniques include N:1 Time
Modulation Dynamic Voltage input generator with analog-compute-in-memory (ACIM)
circuits. This work conducts evaluations on large-scale KAN networks to
validate the proposed methodologies. Non-ideality factors, including partial
sum deviations from process variations, have been evaluated with statistics
measured from the TSMC 22nm RRAM-ACIM prototype chips. Utilizing optimally
determined KAN hyperparameters in conjunction with circuit optimizations
fabricated at the 22nm technology node, despite the parameter count for
large-scale tasks in this work increasing by 500Kx to 807Kx compared to
tiny-scale tasks in previous work, the area overhead increases by only 28Kx to
41Kx, with power consumption rising by merely 51x to 94x, while accuracy
degradation remains minimal at 0.11% to 0.23%, demonstrating the scaling
potential of our proposed architecture.

</details>


### [20] [SCREME: A Scalable Framework for Resilient Memory Design](https://arxiv.org/abs/2509.06101)
*Fan Li,Mimi Xie,Yanan Guo,Huize Li,Xin Xin*

Main category: cs.AR

TL;DR: SCREME是一个可扩展的内存框架，利用低成本、低性能的芯片来满足日益增长的内存可靠性需求，通过重新配置ECC芯片的带宽和利用未充分利用的I/O资源来实现。


<details>
  <summary>Details</summary>
Motivation: 内存技术的快速发展带来了性能提升，但也加剧了可靠性挑战。传统ECC方案假设为奇偶校验数据分配额外内存空间成本高昂且不可扩展，需要新的解决方案。

Method: 提出SCREME框架：1）识别ECC芯片不需要与常规数据芯片相同的性能水平；2）将原本为高性能ECC芯片配置的带宽用于容纳多个低成本芯片；3）利用服务器内存芯片中未充分利用的I/O资源实现灵活的DIMM内部连接。

Result: 该方法能够有效降低ECC芯片成本（低性能通常意味着低成本），并通过带宽重新配置提供更多的奇偶校验存储空间，从而提升内存系统的可靠性。

Conclusion: SCREME框架打破了传统ECC设计的刻板印象，利用技术演进过程中自然产生的低成本芯片来满足可靠性需求，提供了一个可扩展的内存保护解决方案。

Abstract: The continuing advancement of memory technology has not only fueled a surge
in performance, but also substantially exacerbate reliability challenges.
Traditional solutions have primarily focused on improving the efficiency of
protection schemes, i.e., Error Correction Codes (ECC), under the assumption
that allocating additional memory space for parity data is always expensive and
therefore not a scalable solution.
  We break the stereotype by proposing an orthogonal approach that provides
additional, cost-effective memory space for resilient memory design. In
particular, we recognize that ECC chips (used for parity storage) do not
necessarily require the same performance level as regular data chips. This
offers two-fold benefits: First, the bandwidth originally provisioned for a
regular-performance ECC chip can instead be used to accommodate multiple
low-performance chips. Second, the cost of ECC chips can be effectively
reduced, as lower performance often correlates with lower expense. In addition,
we observe that server-class memory chips are often provisioned with ample, yet
underutilized I/O resources. This further offers the opportunity to repurpose
these resources to enable flexible on-DIMM interconnections. Based on the above
two insights, we finally propose SCREME, a scalable memory framework leverages
cost-effective, albeit slower, chips -- naturally produced during rapid
technology evolution -- to meet the growing reliability demands driven by this
evolution.

</details>


### [21] [Hardware Acceleration in Portable MRIs: State of the Art and Future Prospects](https://arxiv.org/abs/2509.06365)
*Omar Al Habsi,Safa Mohammed Sali,Anis Meribout,Mahmoud Meribout,Saif Almazrouei,Mohamed Seghier*

Main category: cs.AR

TL;DR: 本文综述了可移动MRI系统的硬件加速技术，分析了GPU、FPGA、ASIC等在加速图像重建和降低功耗方面的优势，并提出了低场MRI联盟和标准化测试框架的建议。


<details>
  <summary>Details</summary>
Motivation: 可移动MRI系统在远程和资源受限环境中具有重要价值，但图像重建和机器学习算法的计算复杂性构成了重大挑战，而现有研究对硬件加速的关注很少。

Method: 通过综述性评估方法，评估了GPU、FPGA、ASIC等硬件加速技术在pMRI图像重建速度和功耗方面的性能，分析了AI驱动重建、开政低场pMRI数据集和边缘硬件解决方案的潜力。

Result: 硬件加速技术能够显著提高pMRI的图像质量、降低功耗和增强空间移动性，为下一代pMRI技术提供了关键支持。

Conclusion: 本文建议成立低场MRI联盟和建立标准化的证据梯度测试框架，以促进可复现的AI技术在可移动MRI中的应用，提供统一的数据集、基准测试和监管准备测试平台。

Abstract: There is a growing interest in portable MRI (pMRI) systems for point-of-care
imaging, particularly in remote or resource-constrained environments. However,
the computational complexity of pMRI, especially in image reconstruction and
machine learning (ML) algorithms for enhanced imaging, presents significant
challenges. Such challenges can be potentially addressed by harnessing hardware
application solutions, though there is little focus in the current pMRI
literature on hardware acceleration. This paper bridges that gap by reviewing
recent developments in pMRI, emphasizing the role and impact of hardware
acceleration to speed up image acquisition and reconstruction. Key technologies
such as Graphics Processing Units (GPUs), Field-Programmable Gate Arrays
(FPGAs), and Application-Specific Integrated Circuits (ASICs) offer excellent
performance in terms of reconstruction speed and power consumption. This review
also highlights the promise of AI-powered reconstruction, open low-field pMRI
datasets, and innovative edge-based hardware solutions for the future of pMRI
technology. Overall, hardware acceleration can enhance image quality, reduce
power consumption, and increase portability for next-generation pMRI
technology. To accelerate reproducible AI for portable MRI, we propose forming
a Low-Field MRI Consortium and an evidence ladder (analytic/phantom validation,
retrospective multi-center testing, prospective reader and non-inferiority
trials) to provide standardized datasets, benchmarks, and regulator-ready
testbeds.

</details>


### [22] [VCO-CARE: VCO-based Calibration-free Analog Readout for Electrodermal activity sensing](https://arxiv.org/abs/2509.06698)
*Leidy Mabel Alvero-Gonzalez,Matias Miguez,Eric Gutierrez,Juan Sapriza,Susana Patón,David Atienza,José Miranda*

Main category: cs.AR

TL;DR: VCO-CARE是一种基于压控振荡器的模拟读出系统，专为连续EDA传感设计，具有高灵敏度(40pS)、低功耗(2.3μW)和低噪声(0.8μVrms)的特点。


<details>
  <summary>Details</summary>
Motivation: 解决可穿戴设备中EDA连续监测对高灵敏度、低功耗和最小校准要求的模拟前端系统的迫切需求。

Method: 采用电压控制振荡器(VCO)为基础的模拟读出架构，专为连续EDA传感定制设计。

Result: 系统在0-20μS范围内达到40pS的平均灵敏度，固定电阻相对误差小于0.0025%，功耗仅2.3μW，噪声贡献仅0.8μVrms。

Conclusion: 该研究推动了具有无缝适应性、最小功耗和出色噪声耐受性的可穿戴传感器的发展。

Abstract: Continuous monitoring of electrodermal activity (EDA) through wearable
devices has attracted much attention in recent times. However, the persistent
challenge demands analog front-end (AFE) systems with high sensitivity, low
power consumption, and minimal calibration requirements to ensure practical
usability in wearable technologies. In response to this challenge, this
research introduces VCO-CARE, a Voltage-Controlled Oscillator-based Analog
Readout tailored for continuous EDA sensing. The results show that our system
achieves an exceptional average sensitivity of up to 40 pS within a 0-20 uS
range and a negligible relative error of less than 0.0025% for
fixed-resistance. Furthermore, the proposed system consumes only an average of
2.3 uW based on post-layout validations and introduces a low noise
contribution, measuring only 0.8 uVrms across the 0-1.5 Hz EDA signal band.
This research aims to drive the evolution of wearable sensors characterized by
seamless adaptability to diverse users, minimal power consumption, and
outstanding noise resilience.

</details>
