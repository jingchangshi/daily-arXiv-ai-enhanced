{"id": "2601.11553", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.11553", "abs": "https://arxiv.org/abs/2601.11553", "authors": ["Kaiwei Liu", "Liekang Zeng", "Lilin Xu", "Bufang Yang", "Zhenyu Yan"], "title": "PerCache: Predictive Hierarchical Cache for RAG Applications on Mobile Devices", "comment": null, "summary": "Retrieval-augmented generation (RAG) has been extensively used as a de facto paradigm in various large language model (LLM)-driven applications on mobile devices, such as mobile assistants leveraging personal emails or meeting records. However, due to the lengthy prompts and the resource constraints, mobile RAG systems exhibit significantly high response latency. On this issue, one promising approach is to reuse intermediate computational results across different queries to eliminate redundant computation. But most existing approaches, such as KV cache reuse and semantic cache reuse, are designed for cloud settings and perform poorly, overlooking the distinctive characteristics of mobile RAG.\n  We propose PerCache, a novel hierarchical cache solution designed for reducing end-to-end latency of personalized RAG applications on mobile platforms. PerCache adopts a hierarchical architecture that progressively matches similar queries and QKV cache to maximize the reuse of intermediate results at different computing stages. To improve cache hit rate, PerCache applies a predictive method to populate cache with queries that are likely to be raised in the future. In addition, PerCache can adapt its configurations to dynamic system loads, aiming at maximizing the caching utility with minimal resource consumption. We implement PerCache on top of an existing mobile LLM inference engine with commodity mobile phones. Extensive evaluations show that PerCache can surpass the best-performing baseline by 34.4% latency reduction across various applications and maintain optimal latency performance under dynamic resource changes.", "AI": {"tldr": "PerCache\u662f\u4e00\u4e2a\u9488\u5bf9\u79fb\u52a8RAG\u5e94\u7528\u7684\u5c42\u6b21\u5316\u7f13\u5b58\u7cfb\u7edf\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u5339\u914d\u76f8\u4f3c\u67e5\u8be2\u548cQKV\u7f13\u5b58\u6765\u91cd\u7528\u4e2d\u95f4\u8ba1\u7b97\u7ed3\u679c\uff0c\u7ed3\u5408\u9884\u6d4b\u6027\u7f13\u5b58\u586b\u5145\u548c\u81ea\u9002\u5e94\u914d\u7f6e\uff0c\u663e\u8457\u964d\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u5e94\u7528\uff08\u5982\u79fb\u52a8\u52a9\u624b\u5904\u7406\u4e2a\u4eba\u90ae\u4ef6\u6216\u4f1a\u8bae\u8bb0\u5f55\uff09\u9762\u4e34\u9ad8\u5ef6\u8fdf\u95ee\u9898\u3002\u73b0\u6709\u7f13\u5b58\u65b9\u6848\uff08\u5982KV\u7f13\u5b58\u91cd\u7528\u548c\u8bed\u4e49\u7f13\u5b58\u91cd\u7528\uff09\u4e3b\u8981\u9488\u5bf9\u4e91\u7aef\u8bbe\u8ba1\uff0c\u5ffd\u89c6\u4e86\u79fb\u52a8RAG\u7684\u72ec\u7279\u7279\u6027\uff0c\u5728\u79fb\u52a8\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "PerCache\u91c7\u7528\u5c42\u6b21\u5316\u67b6\u6784\uff0c\u6e10\u8fdb\u5f0f\u5339\u914d\u76f8\u4f3c\u67e5\u8be2\u548cQKV\u7f13\u5b58\uff0c\u6700\u5927\u5316\u4e0d\u540c\u8ba1\u7b97\u9636\u6bb5\u7684\u4e2d\u95f4\u7ed3\u679c\u91cd\u7528\u3002\u4f7f\u7528\u9884\u6d4b\u65b9\u6cd5\u586b\u5145\u672a\u6765\u53ef\u80fd\u51fa\u73b0\u7684\u67e5\u8be2\u4ee5\u63d0\u9ad8\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u5e76\u80fd\u6839\u636e\u52a8\u6001\u7cfb\u7edf\u8d1f\u8f7d\u81ea\u9002\u5e94\u8c03\u6574\u914d\u7f6e\uff0c\u4ee5\u6700\u5c0f\u8d44\u6e90\u6d88\u8017\u6700\u5927\u5316\u7f13\u5b58\u6548\u7528\u3002", "result": "\u5728\u73b0\u6709\u79fb\u52a8LLM\u63a8\u7406\u5f15\u64ce\u4e0a\u5b9e\u73b0PerCache\uff0c\u5e76\u5728\u5546\u7528\u624b\u673a\u4e0a\u8bc4\u4f30\u3002\u5b9e\u9a8c\u8868\u660e\uff0cPerCache\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u80fd\u51cf\u5c1134.4%\u7684\u5ef6\u8fdf\uff0c\u5e76\u5728\u52a8\u6001\u8d44\u6e90\u53d8\u5316\u4e0b\u4fdd\u6301\u6700\u4f18\u5ef6\u8fdf\u6027\u80fd\u3002", "conclusion": "PerCache\u662f\u9488\u5bf9\u79fb\u52a8RAG\u5e94\u7528\u7684\u6709\u6548\u7f13\u5b58\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u7f13\u5b58\u67b6\u6784\u3001\u9884\u6d4b\u6027\u586b\u5145\u548c\u81ea\u9002\u5e94\u914d\u7f6e\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u9002\u5e94\u79fb\u52a8\u73af\u5883\u7684\u8d44\u6e90\u7ea6\u675f\u548c\u52a8\u6001\u7279\u6027\u3002"}}
{"id": "2601.11577", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.11577", "abs": "https://arxiv.org/abs/2601.11577", "authors": ["Yuankai Fan", "Qizhen Weng", "Xuelong Li"], "title": "Computation-Bandwidth-Memory Trade-offs: A Unified Paradigm for AI Infrastructure", "comment": null, "summary": "Large-scale artificial intelligence models are transforming industries and redefining human machine collaboration. However, continued scaling exposes critical limitations in hardware, including constraints on computation, bandwidth, and memory. These dimensions are tightly interconnected, so improvements in one often create bottlenecks in others, making isolated optimizations less effective. Balancing them to maximize system efficiency remains a central challenge in scalable AI design. To address this challenge, we introduce {Computation-Bandwidth-Memory Trade-offs}, termed the {AI Trinity}, a unified paradigm that positions {computation}, {bandwidth}, and {memory} as coequal pillars for next-generation AI infrastructure. AI Trinity enables dynamic allocation of resources across these pillars, alleviating single-resource bottlenecks and adapting to diverse scenarios to optimize system performance. Within this framework, AI Trinity identifies three fundamental trade-offs: (1) {More Computation$\\rightarrow$Less Bandwidth}, wherein computational resources are exploited to reduce data transmission under limited bandwidth conditions, (2) {More Bandwidth$\\rightarrow$Less Memory}, which exploits abundant communication capacity to populate or refresh memory when local storage resources are constrained, and (3) {More Memory$\\rightarrow$Less Computation}, whereby storage capacity are utilized to mitigate redundant computation when computational costs are prohibitive. We illustrate the effectiveness of AI Trinity through representative system designs spanning edge-cloud communication, large-scale distributed training, and model inference. The innovations embodied in AI Trinity advance a new paradigm for scalable AI infrastructure, providing both a conceptual foundation and practical guidance for a broad range of application scenarios.", "AI": {"tldr": "AI Trinity\u63d0\u51fa\u8ba1\u7b97-\u5e26\u5bbd-\u5185\u5b58\u4e09\u8981\u7d20\u6743\u8861\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u5206\u914d\u4f18\u5316\u53ef\u6269\u5c55AI\u7cfb\u7edf\u6027\u80fd", "motivation": "\u5927\u89c4\u6a21AI\u6a21\u578b\u9762\u4e34\u786c\u4ef6\u9650\u5236\uff0c\u8ba1\u7b97\u3001\u5e26\u5bbd\u548c\u5185\u5b58\u76f8\u4e92\u5236\u7ea6\uff0c\u5b64\u7acb\u4f18\u5316\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u7edf\u4e00\u6846\u67b6\u6765\u5e73\u8861\u8fd9\u4e9b\u8d44\u6e90", "method": "\u63d0\u51faAI Trinity\u6846\u67b6\uff0c\u5c06\u8ba1\u7b97\u3001\u5e26\u5bbd\u3001\u5185\u5b58\u89c6\u4e3a\u540c\u7b49\u91cd\u8981\u7684\u652f\u67f1\uff0c\u8bc6\u522b\u4e09\u79cd\u57fa\u672c\u6743\u8861\u5173\u7cfb\uff1a\u66f4\u591a\u8ba1\u7b97\u2192\u66f4\u5c11\u5e26\u5bbd\u3001\u66f4\u591a\u5e26\u5bbd\u2192\u66f4\u5c11\u5185\u5b58\u3001\u66f4\u591a\u5185\u5b58\u2192\u66f4\u5c11\u8ba1\u7b97", "result": "\u901a\u8fc7\u8fb9\u7f18-\u4e91\u901a\u4fe1\u3001\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\u548c\u6a21\u578b\u63a8\u7406\u7b49\u4ee3\u8868\u6027\u7cfb\u7edf\u8bbe\u8ba1\u9a8c\u8bc1\u4e86AI Trinity\u7684\u6709\u6548\u6027", "conclusion": "AI Trinity\u4e3a\u53ef\u6269\u5c55AI\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u65e2\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\uff0c\u53c8\u4e3a\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc"}}
{"id": "2601.11584", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.11584", "abs": "https://arxiv.org/abs/2601.11584", "authors": ["Jody Almaida Putra"], "title": "Cost-Aware Logging: Measuring the Financial Impact of Excessive Log Retention in Small-Scale Cloud Deployments", "comment": "8 pages, 3 tables. Simulation-based study on cost-aware log retention", "summary": "Log data plays a critical role in observability, debugging, and performance monitoring in modern cloud-native systems. In small and early-stage cloud deployments, however, log retention policies are frequently configured far beyond operational requirements, often defaulting to 90 days or more, without explicit consideration of their financial and performance implications. As a result, excessive log retention becomes a hidden and recurring cost.\n  This study examines the financial and operational impact of log retention window selection from a cost-aware perspective. Using synthetic log datasets designed to reflect real-world variability in log volume and access patterns, we evaluate retention windows of 7, 14, 30, and 90 days. The analysis focuses on three metrics: storage cost, operationally useful log ratio, and cost per useful log. Operational usefulness is defined as log data accessed during simulated debugging and incident analysis tasks.\n  The results show that reducing log retention from 90 days to 14 days can lower log storage costs by up to 78 percent while preserving more than 97 percent of operationally useful logs. Longer retention windows provide diminishing operational returns while disproportionately increasing storage cost and query overhead. These findings suggest that modest configuration changes can yield significant cost savings without compromising system reliability.\n  Rather than proposing new logging mechanisms, this work offers a lightweight and accessible framework to help small engineering teams reason about log retention policies through a cost-effectiveness lens. The study aims to encourage more deliberate observability configurations, particularly in resource-constrained cloud environments.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c06\u65e5\u5fd7\u4fdd\u7559\u671f\u4ece90\u5929\u7f29\u77ed\u81f314\u5929\u53ef\u964d\u4f4e78%\u5b58\u50a8\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u755997%\u4ee5\u4e0a\u64cd\u4f5c\u6709\u7528\u65e5\u5fd7\uff0c\u4e3a\u5c0f\u578b\u4e91\u56e2\u961f\u63d0\u4f9b\u6210\u672c\u6548\u76ca\u5206\u6790\u6846\u67b6\u3002", "motivation": "\u65e9\u671f\u4e91\u90e8\u7f72\u4e2d\uff0c\u65e5\u5fd7\u4fdd\u7559\u7b56\u7565\u5e38\u9ed8\u8ba4\u8bbe\u7f6e\u4e3a90\u5929\u6216\u66f4\u957f\uff0c\u672a\u8003\u8651\u8d22\u52a1\u548c\u6027\u80fd\u5f71\u54cd\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u4fdd\u7559\u6210\u4e3a\u9690\u85cf\u7684\u91cd\u590d\u6210\u672c\u3002\u7814\u7a76\u65e8\u5728\u4ece\u6210\u672c\u610f\u8bc6\u89d2\u5ea6\u5206\u6790\u65e5\u5fd7\u4fdd\u7559\u7a97\u53e3\u9009\u62e9\u7684\u8d22\u52a1\u548c\u8fd0\u8425\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u65e5\u5fd7\u91cf\u548c\u8bbf\u95ee\u6a21\u5f0f\u53d8\u5316\u7684\u5408\u6210\u65e5\u5fd7\u6570\u636e\u96c6\uff0c\u8bc4\u4f307\u300114\u300130\u548c90\u5929\u4fdd\u7559\u7a97\u53e3\uff0c\u805a\u7126\u4e09\u4e2a\u6307\u6807\uff1a\u5b58\u50a8\u6210\u672c\u3001\u64cd\u4f5c\u6709\u7528\u65e5\u5fd7\u6bd4\u4f8b\u3001\u6bcf\u6709\u7528\u65e5\u5fd7\u6210\u672c\u3002\u64cd\u4f5c\u6709\u7528\u6027\u5b9a\u4e49\u4e3a\u6a21\u62df\u8c03\u8bd5\u548c\u4e8b\u4ef6\u5206\u6790\u4efb\u52a1\u671f\u95f4\u8bbf\u95ee\u7684\u65e5\u5fd7\u6570\u636e\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5c06\u65e5\u5fd7\u4fdd\u7559\u671f\u4ece90\u5929\u51cf\u5c11\u523014\u5929\u53ef\u5c06\u65e5\u5fd7\u5b58\u50a8\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe78%\uff0c\u540c\u65f6\u4fdd\u7559\u8d85\u8fc797%\u7684\u64cd\u4f5c\u6709\u7528\u65e5\u5fd7\u3002\u66f4\u957f\u4fdd\u7559\u7a97\u53e3\u63d0\u4f9b\u9012\u51cf\u7684\u8fd0\u8425\u56de\u62a5\uff0c\u540c\u65f6\u4e0d\u6210\u6bd4\u4f8b\u5730\u589e\u52a0\u5b58\u50a8\u6210\u672c\u548c\u67e5\u8be2\u5f00\u9500\u3002", "conclusion": "\u7814\u7a76\u672a\u63d0\u51fa\u65b0\u65e5\u5fd7\u673a\u5236\uff0c\u800c\u662f\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u53ef\u8bbf\u95ee\u6846\u67b6\uff0c\u5e2e\u52a9\u5c0f\u578b\u5de5\u7a0b\u56e2\u961f\u901a\u8fc7\u6210\u672c\u6548\u76ca\u89c6\u89d2\u601d\u8003\u65e5\u5fd7\u4fdd\u7559\u7b56\u7565\u3002\u65e8\u5728\u9f13\u52b1\u5728\u8d44\u6e90\u53d7\u9650\u4e91\u73af\u5883\u4e2d\u8fdb\u884c\u66f4\u5ba1\u614e\u7684\u53ef\u89c2\u6d4b\u6027\u914d\u7f6e\u3002"}}
{"id": "2601.11589", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11589", "abs": "https://arxiv.org/abs/2601.11589", "authors": ["Jianshu She", "Zonghang Li", "Hongchao Du", "Shangyu Wu", "Wenhao Zheng", "Eric Xing", "Zhengzhong Liu", "Huaxiu Yao", "Jason Xue", "Qirong Ho"], "title": "PLA-Serve: A Prefill-Length-Aware LLM Serving System", "comment": "12 pages", "summary": "PLA-Serve identifies and disaggregates requests with different prompt lengths in LLM serving to reduce TTFT latency. While recent systems have decoupled the prefill and decode stages to improve throughput, they still rely on unified scheduling policies that fail to adapt to heterogeneous workload characteristics. We observe that prompt-length variations lead to distinct performance bottlenecks, motivating an adaptive scheduling strategy. PLA-Serve disaggregates multi-turn long-prefill requests from short-prefill ones and introduces a length-aware smart batching mechanism for short-prefill workloads. It adopts a dual-queue design that supports temporal disaggregation on a single prefill instance or spatial disaggregation across multiple instances. For short-prefill batches, a batch waiting window and CUDA Graph-based clustering mitigate interference from heterogeneous computation, reducing batching delay and lowering average latency. In real multi-turn workloads, PLA-Serve reduces prefill latency by over 30% compared to vanilla SGLang under prefill**--**decode disaggregation, and further decreases SLO violations by 28% in multi-instance deployments with vanilla data-parallel configuration. Compared to the SGLang router with load balancing, it further lowers SLO violations by 12% in multi-GPU settings. Under high concurrency and mixed-request scenarios, PLA-Serve improves request throughput by 35% serving Qwen2.5-32B model for prefill instance, demonstrating its effectiveness in optimizing heterogeneous LLM serving workloads.", "AI": {"tldr": "PLA-Serve\u901a\u8fc7\u6839\u636e\u63d0\u793a\u957f\u5ea6\u5bf9LLM\u670d\u52a1\u8bf7\u6c42\u8fdb\u884c\u89e3\u8026\u548c\u667a\u80fd\u8c03\u5ea6\uff0c\u4f18\u5316\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u867d\u7136\u89e3\u8026\u4e86\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u4f46\u4ecd\u91c7\u7528\u7edf\u4e00\u8c03\u5ea6\u7b56\u7565\uff0c\u65e0\u6cd5\u9002\u5e94\u63d0\u793a\u957f\u5ea6\u53d8\u5316\u5e26\u6765\u7684\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u74f6\u9888\u3002", "method": "1. \u6839\u636e\u63d0\u793a\u957f\u5ea6\u5c06\u591a\u8f6e\u957f\u9884\u586b\u5145\u8bf7\u6c42\u4e0e\u77ed\u9884\u586b\u5145\u8bf7\u6c42\u89e3\u8026\uff1b2. \u4e3a\u77ed\u9884\u586b\u5145\u5de5\u4f5c\u8d1f\u8f7d\u5f15\u5165\u957f\u5ea6\u611f\u77e5\u667a\u80fd\u6279\u5904\u7406\u673a\u5236\uff1b3. \u91c7\u7528\u53cc\u961f\u5217\u8bbe\u8ba1\uff0c\u652f\u6301\u5355\u5b9e\u4f8b\u65f6\u95f4\u89e3\u8026\u6216\u591a\u5b9e\u4f8b\u7a7a\u95f4\u89e3\u8026\uff1b4. \u4f7f\u7528\u6279\u5904\u7406\u7b49\u5f85\u7a97\u53e3\u548cCUDA Graph\u805a\u7c7b\u51cf\u5c11\u5f02\u6784\u8ba1\u7b97\u5e72\u6270\u3002", "result": "\u76f8\u6bd4vanilla SGLang\uff0c\u9884\u586b\u5145\u5ef6\u8fdf\u964d\u4f4e30%\u4ee5\u4e0a\uff1b\u591a\u5b9e\u4f8b\u90e8\u7f72\u4e2dSLO\u8fdd\u89c4\u51cf\u5c1128%\uff1b\u76f8\u6bd4SGLang\u8def\u7531\u8d1f\u8f7d\u5747\u8861\uff0c\u591aGPU\u8bbe\u7f6e\u4e0bSLO\u8fdd\u89c4\u8fdb\u4e00\u6b65\u964d\u4f4e12%\uff1b\u5728\u9ad8\u5e76\u53d1\u6df7\u5408\u8bf7\u6c42\u573a\u666f\u4e0b\uff0cQwen2.5-32B\u6a21\u578b\u9884\u586b\u5145\u5b9e\u4f8b\u541e\u5410\u91cf\u63d0\u9ad835%\u3002", "conclusion": "PLA-Serve\u901a\u8fc7\u957f\u5ea6\u611f\u77e5\u7684\u8bf7\u6c42\u89e3\u8026\u548c\u81ea\u9002\u5e94\u8c03\u5ea6\u7b56\u7565\uff0c\u6709\u6548\u4f18\u5316\u4e86\u5f02\u6784LLM\u670d\u52a1\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2601.11770", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.11770", "abs": "https://arxiv.org/abs/2601.11770", "authors": ["Voktho Das", "Kimia Azar", "Hadi Kamali"], "title": "NuRedact: Non-Uniform eFPGA Architecture for Low-Overhead and Secure IP Redaction", "comment": "Accepted at Design, Automation, and Test 2026", "summary": "While logic locking has been extensively studied as a countermeasure against integrated circuit (IC) supply chain threats, recent research has shifted toward reconfigurable-based redaction techniques, e.g., LUT- and eFPGA-based schemes. While these approaches raise the bar against attacks, they incur substantial overhead, much of which arises not from genuine functional reconfigurability need, but from artificial complexity intended solely to frustrate reverse engineering (RE). As a result, fabrics are often underutilized, and security is achieved at disproportionate cost. This paper introduces NuRedact, the first full-custom eFPGA redaction framework that embraces architectural non-uniformity to balance security and efficiency. Built as an extension of the widely adopted OpenFPGA infrastructure, NuRedact introduces a three-stage methodology: (i) custom fabric generation with pin-mapping irregularity, (ii) VPR-level modifications to enable non-uniform placement guided by an automated Python-based optimizer, and (iii) redaction-aware reconfiguration and mapping of target IP modules. Experimental results show up to 9x area reduction compared to conventional uniform fabrics, achieving competitive efficiency with LUT-based and even transistor-level redaction techniques while retaining strong resilience. From a security perspective, NuRedact fabrics are evaluated against state-of-the-art attack models, including SAT-based, cyclic, and sequential variants, and show enhanced resilience while maintaining practical design overheads.", "AI": {"tldr": "NuRedact\u662f\u4e00\u4e2a\u975e\u5747\u5300eFPGA\u91cd\u5220\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u5236\u5316\u67b6\u6784\u5e73\u8861\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u76f8\u6bd4\u4f20\u7edf\u5747\u5300\u7ed3\u6784\u5b9e\u73b0\u9ad8\u8fbe9\u500d\u9762\u79ef\u7f29\u51cf\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u97e7\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53ef\u91cd\u6784\u7684\u91cd\u5220\u6280\u672f\uff08\u5982LUT\u548ceFPGA\u65b9\u6848\uff09\u867d\u7136\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\uff0c\u4f46\u5e26\u6765\u4e86\u5de8\u5927\u5f00\u9500\uff0c\u8fd9\u4e9b\u5f00\u9500\u4e3b\u8981\u6765\u81ea\u4e3a\u963b\u788d\u9006\u5411\u5de5\u7a0b\u800c\u5f15\u5165\u7684\u4eba\u5de5\u590d\u6742\u6027\uff0c\u800c\u975e\u771f\u6b63\u7684\u529f\u80fd\u53ef\u91cd\u6784\u9700\u6c42\uff0c\u5bfc\u81f4\u7ed3\u6784\u5229\u7528\u7387\u4f4e\u4e14\u5b89\u5168\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u57fa\u4e8eOpenFPGA\u57fa\u7840\u8bbe\u65bd\u6784\u5efa\u7684\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u751f\u6210\u5177\u6709\u5f15\u811a\u6620\u5c04\u4e0d\u89c4\u5219\u6027\u7684\u5b9a\u5236\u7ed3\u6784\uff1b2\uff09VPR\u7ea7\u4fee\u6539\u5b9e\u73b0\u57fa\u4e8ePython\u4f18\u5316\u5668\u7684\u975e\u5747\u5300\u5e03\u5c40\uff1b3\uff09\u76ee\u6807IP\u6a21\u5757\u7684\u91cd\u5220\u611f\u77e5\u91cd\u914d\u7f6e\u548c\u6620\u5c04\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u5747\u5300\u7ed3\u6784\u5b9e\u73b0\u9ad8\u8fbe9\u500d\u9762\u79ef\u7f29\u51cf\uff0c\u5728\u4fdd\u6301\u5f3a\u97e7\u6027\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e0eLUT\u57fa\u751a\u81f3\u6676\u4f53\u7ba1\u7ea7\u91cd\u5220\u6280\u672f\u7ade\u4e89\u6027\u7684\u6548\u7387\u3002\u5728SAT\u57fa\u3001\u5faa\u73af\u548c\u5e8f\u5217\u53d8\u4f53\u7b49\u5148\u8fdb\u653b\u51fb\u6a21\u578b\u4e0b\u8868\u73b0\u51fa\u589e\u5f3a\u7684\u97e7\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u9645\u8bbe\u8ba1\u5f00\u9500\u3002", "conclusion": "NuRedact\u9996\u6b21\u5c55\u793a\u4e86\u901a\u8fc7\u67b6\u6784\u975e\u5747\u5300\u6027\u5e73\u8861\u5b89\u5168\u6027\u548c\u6548\u7387\u7684\u5b8c\u6574\u5b9a\u5236eFPGA\u91cd\u5220\u6846\u67b6\uff0c\u4e3a\u96c6\u6210\u7535\u8def\u4f9b\u5e94\u94fe\u5b89\u5168\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u91cd\u5220\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11590", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11590", "abs": "https://arxiv.org/abs/2601.11590", "authors": ["Fan Bai", "Pai Peng", "Zhengzhi Tang", "Zhe Wang", "Gong Chen", "Xiang Lu", "Yinuo Li", "Huan Lin", "Weizhe Lin", "Yaoyuan Wang", "Xiaosong Li"], "title": "EPD-Serve: A Flexible Multimodal EPD Disaggregation Inference Serving System On Ascend", "comment": null, "summary": "With the widespread adoption of large multimodal models, efficient inference across text, image, audio, and video modalities has become critical. However, existing multimodal inference systems typically employ monolithic architectures that tightly couple the Encode, Prefill, and Decode stages on homogeneous hardware, neglecting the heterogeneous computational characteristics of each stage. This design leads to inefficient resource utilization and limited system throughput. To address these issues, we propose EPD-Serve, a stage-level disaggregated inference serving system for multimodal models. EPD-Serve decouples the inference pipeline into independent Encode, Prefill, and Decode stages, enabling logical isolation and flexible co-located deployment through dynamic orchestration. Leveraging the Ascend interconnect topology, EPD-Serve introduces asynchronous feature prefetching between Encode and Prefill stages and a hierarchical grouped KV cache transmission mechanism between Prefill and Decode stages to improve cross-node communication efficiency. In addition, EPD-Serve incorporates multi-route scheduling, instance-level load balancing, and multi-stage hardware co-location with spatial multiplexing to better support diverse multimodal workloads. Comprehensive experiments on multimodal understanding models demonstrate that, under high-concurrency scenarios, EPD-Serve improves end-to-end throughput by 57.37-69.48% compared to PD-disaggregated deployment, while satisfying strict SLO constraints, including TTFT below 2000 ms and TPOT below 50 ms. These results highlight the effectiveness of stage-level disaggregation for optimizing multimodal large model inference systems.", "AI": {"tldr": "EPD-Serve\uff1a\u4e00\u79cd\u9762\u5411\u591a\u6a21\u6001\u5927\u6a21\u578b\u63a8\u7406\u7684\u9636\u6bb5\u7ea7\u89e3\u8026\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u6d41\u6c34\u7ebf\u89e3\u8026\u4e3a\u72ec\u7acb\u7684\u7f16\u7801\u3001\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u5e76\u5f15\u5165\u5f02\u6b65\u7279\u5f81\u9884\u53d6\u548c\u5206\u5c42KV\u7f13\u5b58\u4f20\u8f93\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u63a8\u7406\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u5355\u4f53\u67b6\u6784\uff0c\u5c06\u7f16\u7801\u3001\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u7d27\u5bc6\u8026\u5408\u5728\u540c\u6784\u786c\u4ef6\u4e0a\uff0c\u5ffd\u89c6\u4e86\u5404\u9636\u6bb5\u7684\u5f02\u6784\u8ba1\u7b97\u7279\u6027\uff0c\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u548c\u7cfb\u7edf\u541e\u5410\u91cf\u53d7\u9650\u3002", "method": "\u63d0\u51faEPD-Serve\u7cfb\u7edf\uff1a1\uff09\u5c06\u63a8\u7406\u6d41\u6c34\u7ebf\u89e3\u8026\u4e3a\u72ec\u7acb\u7684\u7f16\u7801\u3001\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff1b2\uff09\u5229\u7528\u6607\u817e\u4e92\u8054\u62d3\u6251\uff0c\u5f15\u5165\u7f16\u7801\u4e0e\u9884\u586b\u5145\u9636\u6bb5\u95f4\u7684\u5f02\u6b65\u7279\u5f81\u9884\u53d6\u673a\u5236\uff0c\u4ee5\u53ca\u9884\u586b\u5145\u4e0e\u89e3\u7801\u9636\u6bb5\u95f4\u7684\u5206\u5c42\u5206\u7ec4KV\u7f13\u5b58\u4f20\u8f93\u673a\u5236\uff1b3\uff09\u7ed3\u5408\u591a\u8def\u7531\u8c03\u5ea6\u3001\u5b9e\u4f8b\u7ea7\u8d1f\u8f7d\u5747\u8861\u548c\u591a\u9636\u6bb5\u786c\u4ef6\u5171\u7f6e\u7b49\u4f18\u5316\u6280\u672f\u3002", "result": "\u5728\u9ad8\u5e76\u53d1\u573a\u666f\u4e0b\uff0c\u76f8\u6bd4PD\u89e3\u8026\u90e8\u7f72\uff0cEPD-Serve\u5c06\u7aef\u5230\u7aef\u541e\u5410\u91cf\u63d0\u5347\u4e8657.37-69.48%\uff0c\u540c\u65f6\u6ee1\u8db3\u4e25\u683c\u7684SLO\u7ea6\u675f\uff1aTTFT\u4f4e\u4e8e2000ms\uff0cTPOT\u4f4e\u4e8e50ms\u3002", "conclusion": "\u9636\u6bb5\u7ea7\u89e3\u8026\u67b6\u6784\u80fd\u6709\u6548\u4f18\u5316\u591a\u6a21\u6001\u5927\u6a21\u578b\u63a8\u7406\u7cfb\u7edf\uff0cEPD-Serve\u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u3001\u901a\u4fe1\u4f18\u5316\u548c\u8d44\u6e90\u7ba1\u7406\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2601.12089", "categories": ["cs.AR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.12089", "abs": "https://arxiv.org/abs/2601.12089", "authors": ["Erwan Tanguy-Legac", "Tommaso Belvedere", "Gianluca Corsini", "Marco Tognon", "Marcello Traiola"], "title": "Domain-specific Hardware Acceleration for Model Predictive Path Integral Control", "comment": "7 pages, 11 figures", "summary": "Accurately controlling a robotic system in real time is a challenging problem. To address this, the robotics community has adopted various algorithms, such as Model Predictive Control (MPC) and Model Predictive Path Integral (MPPI) control. The first is difficult to implement on non-linear systems such as unmanned aerial vehicles, whilst the second requires a heavy computational load. GPUs have been successfully used to accelerate MPPI implementations; however, their power consumption is often excessive for autonomous or unmanned targets, especially when battery-powered. On the other hand, custom designs, often implemented on FPGAs, have been proposed to accelerate robotic algorithms while consuming considerably less energy than their GPU (or CPU) implementation. However, no MPPI custom accelerator has been proposed so far. In this work, we present a hardware accelerator for MPPI control and simulate its execution. Results show that the MPPI custom accelerator allows more accurate trajectories than GPU-based MPPI implementations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8eMPPI\u63a7\u5236\u7684\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u76f8\u6bd4GPU\u5b9e\u73b0\u80fd\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u8f68\u8ff9\u63a7\u5236", "motivation": "\u673a\u5668\u4eba\u5b9e\u65f6\u63a7\u5236\u9762\u4e34\u6311\u6218\uff0cMPC\u96be\u4ee5\u5e94\u7528\u4e8e\u975e\u7ebf\u6027\u7cfb\u7edf\uff0cMPPI\u8ba1\u7b97\u8d1f\u8f7d\u91cd\uff0cGPU\u5b9e\u73b0\u529f\u8017\u8fc7\u9ad8\uff0c\u800cFPGA\u5b9a\u5236\u8bbe\u8ba1\u80fd\u964d\u4f4e\u80fd\u8017\uff0c\u4f46\u76ee\u524d\u5c1a\u65e0MPPI\u4e13\u7528\u52a0\u901f\u5668", "method": "\u8bbe\u8ba1\u5e76\u6a21\u62df\u6267\u884cMPPI\u63a7\u5236\u7684\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u91c7\u7528\u5b9a\u5236\u5316\u67b6\u6784", "result": "MPPI\u5b9a\u5236\u52a0\u901f\u5668\u76f8\u6bd4\u57fa\u4e8eGPU\u7684MPPI\u5b9e\u73b0\u80fd\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u8f68\u8ff9\u63a7\u5236", "conclusion": "\u63d0\u51fa\u7684MPPI\u786c\u4ef6\u52a0\u901f\u5668\u5728\u4fdd\u6301\u4f4e\u80fd\u8017\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u8f68\u8ff9\u63a7\u5236\u7684\u7cbe\u5ea6\uff0c\u4e3a\u673a\u5668\u4eba\u5b9e\u65f6\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12385", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.12385", "abs": "https://arxiv.org/abs/2601.12385", "authors": ["Feifei Li", "Xiao Chen", "Xiaoyu Sun", "Xi Xiao", "Shaohua Wang", "Yong Ding", "Sheng Wen", "Qing Li"], "title": "Context-Free Grammar Inference for Complex Programming Languages in Black Box Settings", "comment": null, "summary": "Grammar inference for complex programming languages remains a significant challenge, as existing approaches fail to scale to real world datasets within practical time constraints. In our experiments, none of the state-of-the-art tools, including Arvada, Treevada and Kedavra were able to infer grammars for complex languages such as C, C++, and Java within 48 hours. Arvada and Treevada perform grammar inference directly on full-length input examples, which proves inefficient for large files commonly found in such languages. While Kedavra introduces data decomposition to create shorter examples for grammar inference, its lexical analysis still relies on the original inputs. Additionally, its strict no-overgeneralization constraint limits the construction of complex grammars.\n  To overcome these limitations, we propose Crucio, which builds a decomposition forest to extract short examples for lexical and grammar inference via a distributional matrix. Experimental results show that Crucio is the only method capable of successfully inferring grammars for complex programming languages (where the number of nonterminals is up to 23x greater than in prior benchmarks) within reasonable time limits. On the prior simple benchmark, Crucio achieves an average recall improvement of 1.37x and 1.19x over Treevada and Kedavra, respectively, and improves F1 scores by 1.21x and 1.13x.", "AI": {"tldr": "Crucio \u662f\u4e00\u79cd\u65b0\u7684\u8bed\u6cd5\u63a8\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5206\u89e3\u68ee\u6797\u63d0\u53d6\u77ed\u793a\u4f8b\u8fdb\u884c\u8bcd\u6cd5\u548c\u8bed\u6cd5\u63a8\u65ad\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u590d\u6742\u7f16\u7a0b\u8bed\u8a00\uff0c\u572848\u5c0f\u65f6\u5185\u6210\u529f\u63a8\u65adC\u3001C++\u3001Java\u7b49\u8bed\u8a00\u7684\u8bed\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bed\u6cd5\u63a8\u65ad\u65b9\u6cd5\uff08\u5982Arvada\u3001Treevada\u3001Kedavra\uff09\u65e0\u6cd5\u572848\u5c0f\u65f6\u5185\u63a8\u65ad\u590d\u6742\u7f16\u7a0b\u8bed\u8a00\uff08C\u3001C++\u3001Java\uff09\u7684\u8bed\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u8981\u4e48\u76f4\u63a5\u5728\u5b8c\u6574\u8f93\u5165\u4e0a\u64cd\u4f5c\u6548\u7387\u4f4e\u4e0b\uff0c\u8981\u4e48\u5206\u89e3\u7b56\u7565\u4e0d\u5f7b\u5e95\uff0c\u4e14\u4e25\u683c\u7ea6\u675f\u9650\u5236\u4e86\u590d\u6742\u8bed\u6cd5\u7684\u6784\u5efa\u3002", "method": "\u63d0\u51faCrucio\u65b9\u6cd5\uff1a\u6784\u5efa\u5206\u89e3\u68ee\u6797\u63d0\u53d6\u77ed\u793a\u4f8b\uff0c\u901a\u8fc7\u5206\u5e03\u77e9\u9635\u8fdb\u884c\u8bcd\u6cd5\u548c\u8bed\u6cd5\u63a8\u65ad\u3002\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u5927\u578b\u6587\u4ef6\uff0c\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "result": "Crucio\u662f\u552f\u4e00\u80fd\u5728\u5408\u7406\u65f6\u95f4\u5185\u6210\u529f\u63a8\u65ad\u590d\u6742\u7f16\u7a0b\u8bed\u8a00\u8bed\u6cd5\u7684\u65b9\u6cd5\uff08\u975e\u7ec8\u7ed3\u7b26\u6570\u91cf\u6bd4\u5148\u524d\u57fa\u51c6\u591a23\u500d\uff09\u3002\u5728\u7b80\u5355\u57fa\u51c6\u4e0a\uff0c\u76f8\u6bd4Treevada\u548cKedavra\uff0c\u5e73\u5747\u53ec\u56de\u7387\u5206\u522b\u63d0\u9ad81.37\u500d\u548c1.19\u500d\uff0cF1\u5206\u6570\u5206\u522b\u63d0\u9ad81.21\u500d\u548c1.13\u500d\u3002", "conclusion": "Crucio\u901a\u8fc7\u521b\u65b0\u7684\u5206\u89e3\u68ee\u6797\u548c\u5206\u5e03\u77e9\u9635\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u7f16\u7a0b\u8bed\u8a00\u8bed\u6cd5\u63a8\u65ad\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11595", "categories": ["cs.DC", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11595", "abs": "https://arxiv.org/abs/2601.11595", "authors": ["Meenakshi Amulya Jayanti", "X. Y. Han"], "title": "Enhancing Model Context Protocol (MCP) with Context-Aware Server Collaboration", "comment": null, "summary": "The Model Context Protocol (MCP) has emerged as a widely used framework for enabling LLM-based agents to communicate with external tools and services. The most common implementation of MCP, proposed by Anthropic, heavily relies on a Large Language Model (LLM) to decompose tasks and issue instructions to servers, which act as stateless executors. In particular, the agents, models, and servers are stateless and do not have access to a global context. However, in tasks involving LLM-driven coordination, it is natural that a Shared Context Store (SCS) could improve the efficiency and coherence of multi-agent workflows by reducing redundancy and enabling knowledge transfer between servers. Thus, in this work, we design and assess the performance of a Context-Aware MCP (CA-MCP) that offloads execution logic to specialized MCP servers that read from and write to a shared context memory, allowing them to coordinate more autonomously in real time. In this design, context management serves as the central mechanism that maintains continuity across task executions by tracking intermediate states and shared variables, thereby enabling persistent collaboration among agents without repeated prompting. We present experiments showing that the CA-MCP can outperform the traditional MCP by reducing the number of LLM calls required for complex tasks and decreasing the frequency of response failures when task conditions are not satisfied, thereby improving overall efficiency and responsiveness. In particular, we conducted experiments on the TravelPlanner and REALM-Bench benchmark datasets and observed statistically significant results indicating the potential advantages of incorporating a shared context store via CA-MCP in LLM-driven multi-agent systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u7684MCP\uff08CA-MCP\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u5171\u4eab\u4e0a\u4e0b\u6587\u5b58\u50a8\u6765\u6539\u8fdb\u4f20\u7edf\u7684MCP\u6846\u67b6\uff0c\u4f7f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u534f\u8c03\u5de5\u4f5c\u3002", "motivation": "\u4f20\u7edfMCP\u6846\u67b6\u4e2d\u7684\u667a\u80fd\u4f53\u3001\u6a21\u578b\u548c\u670d\u52a1\u5668\u90fd\u662f\u65e0\u72b6\u6001\u7684\uff0c\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\u8bbf\u95ee\u80fd\u529b\u3002\u5728\u6d89\u53caLLM\u9a71\u52a8\u7684\u534f\u8c03\u4efb\u52a1\u4e2d\uff0c\u5171\u4eab\u4e0a\u4e0b\u6587\u5b58\u50a8\uff08SCS\uff09\u53ef\u4ee5\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u548c\u5b9e\u73b0\u670d\u52a1\u5668\u95f4\u77e5\u8bc6\u8f6c\u79fb\u6765\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5MCP\uff08CA-MCP\uff09\uff0c\u5c06\u6267\u884c\u903b\u8f91\u5378\u8f7d\u5230\u4e13\u95e8\u7684MCP\u670d\u52a1\u5668\uff0c\u8fd9\u4e9b\u670d\u52a1\u5668\u53ef\u4ee5\u8bfb\u53d6\u548c\u5199\u5165\u5171\u4eab\u4e0a\u4e0b\u6587\u5185\u5b58\u3002\u4e0a\u4e0b\u6587\u7ba1\u7406\u4f5c\u4e3a\u6838\u5fc3\u673a\u5236\uff0c\u901a\u8fc7\u8ddf\u8e2a\u4e2d\u95f4\u72b6\u6001\u548c\u5171\u4eab\u53d8\u91cf\u6765\u7ef4\u6301\u4efb\u52a1\u6267\u884c\u7684\u8fde\u7eed\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCA-MCP\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u51cf\u5c11\u4e86LLM\u8c03\u7528\u6b21\u6570\uff0c\u964d\u4f4e\u4e86\u4efb\u52a1\u6761\u4ef6\u4e0d\u6ee1\u8db3\u65f6\u7684\u54cd\u5e94\u5931\u8d25\u9891\u7387\u3002\u5728TravelPlanner\u548cREALM-Bench\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u7edf\u8ba1\u663e\u8457\u7684\u7ed3\u679c\u3002", "conclusion": "CA-MCP\u901a\u8fc7\u5f15\u5165\u5171\u4eab\u4e0a\u4e0b\u6587\u5b58\u50a8\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u9a71\u52a8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\u548c\u54cd\u5e94\u6027\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u534f\u8c03\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728\u4f18\u52bf\u3002"}}
{"id": "2601.12156", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.12156", "abs": "https://arxiv.org/abs/2601.12156", "authors": ["Debabrata Das", "Yogeeth G. K.", "Arnav Gupta"], "title": "Biological Intuition on Digital Hardware: An RTL Implementation of Poisson-Encoded SNNs for Static Image Classification", "comment": "5 pages, 8 figures, 2 tables. Code available at: https://github.com/Yogeeth/neuromorphic-lif-rtl", "summary": "The deployment of Artificial Intelligence on edge devices (TinyML) is often constrained by the high power consumption and latency associated with traditional Artificial Neural Networks (ANNs) and their reliance on intensive Matrix-Multiply (MAC) operations. Neuromorphic computing offers a compelling alternative by mimicking biological efficiency through event-driven processing. This paper presents the design and implementation of a cycle-accurate, hardware-oriented Spiking Neural Network (SNN) core implemented in SystemVerilog. Unlike conventional accelerators, this design utilizes a Leaky Integrate-and-Fire (LIF) neuron model powered by fixed-point arithmetic and bit-wise primitives (shifts and additions) to eliminate the need for complex floating-point hardware. The architecture features an on-chip Poisson encoder for stochastic spike generation and a novel active pruning mechanism that dynamically disables neurons post-classification to minimize dynamic power consumption. We demonstrate the hardware's efficacy through a fully connected layer implementation targeting digit classification. Simulation results indicate that the design achieves rapid convergence (89% accuracy) within limited timesteps while maintaining a significantly reduced computational footprint compared to traditional dense architectures. This work serves as a foundational building block for scalable, energy-efficient neuromorphic hardware on FPGA and ASIC platforms.", "AI": {"tldr": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eSystemVerilog\u7684\u7cbe\u786e\u5468\u671fSNN\u6838\u5fc3\uff0c\u91c7\u7528LIF\u795e\u7ecf\u5143\u6a21\u578b\u548c\u5b9a\u70b9\u8fd0\u7b97\uff0c\u901a\u8fc7\u52a8\u6001\u526a\u679d\u964d\u4f4e\u529f\u8017\uff0c\u5728\u6570\u5b57\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u523089%\u51c6\u786e\u7387", "motivation": "\u8fb9\u7f18AI\u90e8\u7f72\u53d7\u9650\u4e8e\u4f20\u7edfANN\u7684\u9ad8\u529f\u8017\u548c\u5ef6\u8fdf\uff0c\u800c\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u5904\u7406\u6a21\u62df\u751f\u7269\u6548\u7387\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u5438\u5f15\u529b\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7cbe\u786e\u5468\u671f\u7684\u786c\u4ef6\u5bfc\u5411SNN\u6838\u5fc3\uff0c\u4f7f\u7528LIF\u795e\u7ecf\u5143\u6a21\u578b\u914d\u5408\u5b9a\u70b9\u8fd0\u7b97\u548c\u4f4d\u7ea7\u539f\u8bed\uff0c\u5305\u542b\u7247\u4e0a\u6cca\u677e\u7f16\u7801\u5668\u548c\u52a8\u6001\u526a\u679d\u673a\u5236", "result": "\u5728\u6570\u5b57\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u8be5\u8bbe\u8ba1\u5728\u6709\u9650\u65f6\u95f4\u6b65\u5185\u5feb\u901f\u6536\u655b\u8fbe\u523089%\u51c6\u786e\u7387\uff0c\u540c\u65f6\u76f8\u6bd4\u4f20\u7edf\u5bc6\u96c6\u67b6\u6784\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aFPGA\u548cASIC\u5e73\u53f0\u4e0a\u53ef\u6269\u5c55\u3001\u9ad8\u80fd\u6548\u7684\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u63d0\u4f9b\u4e86\u57fa\u7840\u6784\u5efa\u6a21\u5757"}}
{"id": "2601.12741", "categories": ["cs.PL", "cs.LO", "math.CO"], "pdf": "https://arxiv.org/pdf/2601.12741", "abs": "https://arxiv.org/abs/2601.12741", "authors": ["Gyeongwon Jeong", "Seonghun Park", "Hongseok Yang"], "title": "An Introduction to Razborov's Flag Algebra as a Proof System for Extremal Graph Theory", "comment": null, "summary": "Razborov's flag algebra forms a powerful framework for deriving asymptotic inequalities between induced subgraph densities, underpinning many advances in extremal graph theory. This survey introduces flag algebra to computer scientists working in logic, programming languages, automated verification, and formal methods. We take a logical perspective on flag algebra and present it in terms of syntax, semantics, and proof strategies, in a style closer to formal logic. One popular proof strategy derives valid inequalities by first proving inequalities in a labelled variant of flag algebra and then transferring them to the original unlabelled setting using the so-called downward operator. We explain this strategy in detail and highlight that its transfer mechanism relies on the notion of what we call an adjoint pair, reminiscent of Galois connections and categorical adjunctions, which appear frequently in work on automated verification and programming languages. Along the way, we work through representative examples, including Mantel's theorem and Goodman's bound on Ramsey multiplicity, to illustrate how mathematical arguments can be carried out symbolically in the flag algebra framework.", "AI": {"tldr": "\u672c\u6587\u662f\u4e00\u7bc7\u9762\u5411\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u7684\u65d7\u4ee3\u6570\u5165\u95e8\u7efc\u8ff0\uff0c\u4ece\u903b\u8f91\u89c6\u89d2\u4ecb\u7ecd\u65d7\u4ee3\u6570\u7684\u8bed\u6cd5\u3001\u8bed\u4e49\u548c\u8bc1\u660e\u7b56\u7565\uff0c\u7279\u522b\u89e3\u91ca\u4e86\u901a\u8fc7\u6807\u8bb0\u53d8\u4f53\u63a8\u5bfc\u4e0d\u7b49\u5f0f\u518d\u901a\u8fc7\u4f34\u968f\u5bf9\u8f6c\u79fb\u5230\u65e0\u6807\u8bb0\u8bbe\u7f6e\u7684\u8bc1\u660e\u65b9\u6cd5\u3002", "motivation": "\u5c06Razborov\u7684\u65d7\u4ee3\u6570\u6846\u67b6\u4ecb\u7ecd\u7ed9\u4ece\u4e8b\u903b\u8f91\u3001\u7f16\u7a0b\u8bed\u8a00\u3001\u81ea\u52a8\u9a8c\u8bc1\u548c\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\uff0c\u5e2e\u52a9\u4ed6\u4eec\u7406\u89e3\u8fd9\u4e00\u5728\u6781\u503c\u56fe\u8bba\u4e2d\u53d6\u5f97\u91cd\u8981\u8fdb\u5c55\u7684\u5f3a\u5927\u5de5\u5177\u3002", "method": "\u91c7\u7528\u903b\u8f91\u89c6\u89d2\uff0c\u5c06\u65d7\u4ee3\u6570\u8868\u8ff0\u4e3a\u8bed\u6cd5\u3001\u8bed\u4e49\u548c\u8bc1\u660e\u7b56\u7565\u7684\u5f62\u5f0f\u7cfb\u7edf\u3002\u8be6\u7ec6\u89e3\u91ca\u4e86\u4e00\u79cd\u6d41\u884c\u7684\u8bc1\u660e\u7b56\u7565\uff1a\u5148\u5728\u6807\u8bb0\u53d8\u4f53\u4e2d\u8bc1\u660e\u4e0d\u7b49\u5f0f\uff0c\u7136\u540e\u901a\u8fc7\u4f34\u968f\u5bf9\uff08\u7c7b\u4f3cGalois\u8fde\u63a5\u548c\u8303\u7574\u4f34\u968f\uff09\u4f7f\u7528\u5411\u4e0b\u7b97\u5b50\u8f6c\u79fb\u5230\u539f\u59cb\u65e0\u6807\u8bb0\u8bbe\u7f6e\u3002", "result": "\u901a\u8fc7Mantel\u5b9a\u7406\u548cGoodman\u7684Ramsey\u591a\u91cd\u6027\u754c\u9650\u7b49\u4ee3\u8868\u6027\u4f8b\u5b50\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5728\u65d7\u4ee3\u6570\u6846\u67b6\u4e2d\u7b26\u53f7\u5316\u5730\u6267\u884c\u6570\u5b66\u8bba\u8bc1\uff0c\u4f7f\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u80fd\u591f\u7406\u89e3\u5e76\u5e94\u7528\u8fd9\u4e00\u5de5\u5177\u3002", "conclusion": "\u65d7\u4ee3\u6570\u4f5c\u4e3a\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u4e0d\u4ec5\u9002\u7528\u4e8e\u6781\u503c\u56fe\u8bba\uff0c\u5176\u903b\u8f91\u7ed3\u6784\u548c\u4f34\u968f\u5bf9\u7684\u6982\u5ff5\u4e5f\u4e0e\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\u3001\u81ea\u52a8\u9a8c\u8bc1\u7b49\u9886\u57df\u6709\u5bc6\u5207\u8054\u7cfb\uff0c\u4e3a\u8de8\u5b66\u79d1\u7814\u7a76\u63d0\u4f9b\u4e86\u6865\u6881\u3002"}}
{"id": "2601.11608", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11608", "abs": "https://arxiv.org/abs/2601.11608", "authors": ["Ganesh Bikshandi"], "title": "Hardware-Aware Reformulation of Convolutions for Efficient Execution on Specialized AI Hardware: A Case Study on NVIDIA Tensor Cores", "comment": null, "summary": "Convolutional Neural Networks (CNNs) are central to modern AI, but their performance is often limited by hardware constraints. NVIDIA Tensor Cores, for instance, require input channels to be multiples of 8 and sometimes 512 for efficient execution. {\\em oneDNN} framework for CPU imposes such a requirement for the blocked format. Traditional approaches address such alignment issue using zero-padding, which can be inefficient. In this work, we present a first-step, hardware-aware reformulation of CNN computations using rewrite rules, restructuring the underlying math to satisfy hardware alignment entirely {\\bf post-training} without modifying network weights. While our current implementation focuses on a single transformation for Tensor Cores, this approach is generalizable, laying the foundation to explore additional transformations for CPU and accelerators. This study represents an initial step toward {\\em semantic tuning}, a systematic, hardware-aware optimization strategy for efficient deployment of CNN models on specialized AI hardware.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u7684CNN\u8ba1\u7b97\u91cd\u5199\u65b9\u6cd5\uff0c\u901a\u8fc7\u540e\u8bad\u7ec3\u6570\u5b66\u91cd\u6784\u6ee1\u8db3\u786c\u4ef6\u5bf9\u9f50\u8981\u6c42\uff0c\u65e0\u9700\u4fee\u6539\u7f51\u7edc\u6743\u91cd", "motivation": "\u73b0\u4ee3AI\u786c\u4ef6\uff08\u5982NVIDIA Tensor Cores\u548coneDNN\u6846\u67b6\uff09\u5bf9\u8f93\u5165\u901a\u9053\u6709\u5bf9\u9f50\u8981\u6c42\uff08\u59828\u6216512\u7684\u500d\u6570\uff09\uff0c\u4f20\u7edf\u96f6\u586b\u5145\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u4f18\u7684\u786c\u4ef6\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u91cd\u5199\u89c4\u5219\u5bf9CNN\u8ba1\u7b97\u8fdb\u884c\u786c\u4ef6\u611f\u77e5\u7684\u91cd\u65b0\u8868\u8ff0\uff0c\u901a\u8fc7\u6570\u5b66\u91cd\u6784\u6ee1\u8db3\u786c\u4ef6\u5bf9\u9f50\u8981\u6c42\uff0c\u5b8c\u5168\u5728\u540e\u8bad\u7ec3\u9636\u6bb5\u5b8c\u6210\uff0c\u4e0d\u4fee\u6539\u7f51\u7edc\u6743\u91cd", "result": "\u5f53\u524d\u5b9e\u73b0\u4e13\u6ce8\u4e8eTensor Cores\u7684\u5355\u4e00\u53d8\u6362\uff0c\u4f46\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u5230CPU\u548c\u5176\u4ed6\u52a0\u901f\u5668\u7684\u66f4\u591a\u53d8\u6362\uff0c\u4e3a\u8bed\u4e49\u8c03\u4f18\u5960\u5b9a\u57fa\u7840", "conclusion": "\u8fd9\u662f\u8fc8\u5411\u8bed\u4e49\u8c03\u4f18\u7684\u521d\u6b65\u63a2\u7d22\uff0c\u4e3aCNN\u6a21\u578b\u5728\u4e13\u7528AI\u786c\u4ef6\u4e0a\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u7b56\u7565"}}
{"id": "2601.12298", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.12298", "abs": "https://arxiv.org/abs/2601.12298", "authors": ["Ye Lin", "Chao Fang", "Xiaoyong Song", "Qi Wu", "Anying Jiang", "Yichuan Bai", "Li Du"], "title": "CD-PIM: A High-Bandwidth and Compute-Efficient LPDDR5-Based PIM for Low-Batch LLM Acceleration on Edge-Device", "comment": "To appear in 2026 Design, Automation and Test in Europe Conference (DATE 2026)", "summary": "Edge deployment of low-batch large language models (LLMs) faces critical memory bandwidth bottlenecks when executing memory-intensive general matrix-vector multiplications (GEMV) operations. While digital processing-in-memory (PIM) architectures promise to accelerate GEMV operations, existing PIM-equipped edge devices still suffer from three key limitations: limited bandwidth improvement, component under-utilization in mixed workloads, and low compute capacity of computing units (CUs). In this paper, we propose CD-PIM to address these challenges through three key innovations. First, we introduce a high-bandwidth compute-efficient mode (HBCEM) that enhances bandwidth by dividing each bank into four pseudo-banks through segmented global bitlines. Second, we propose a low-batch interleaving mode (LBIM) to improve component utilization by overlapping GEMV operations with GEMM operations. Third, we design a compute-efficient CU that performs enhanced GEMV operations in a pipelined manner by serially feeding weight data into the computing core. Forth, we adopt a column-wise mapping for the key-cache matrix and row-wise mapping for the value-cache matrix, which fully utilizes CU resources. Our evaluation shows that compared to a GPU-only baseline and state-of-the-art PIM designs, our CD-PIM achieves 11.42x and 4.25x speedup on average within a single batch in HBCEM mode, respectively. Moreover, for low-batch sizes, the CD-PIM achieves an average speedup of 1.12x in LBIM compared to HBCEM.", "AI": {"tldr": "CD-PIM\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8fb9\u7f18\u4f4e\u6279\u6b21LLM\u90e8\u7f72\u7684\u5185\u5b58\u8ba1\u7b97\u67b6\u6784\uff0c\u901a\u8fc7\u9ad8\u5e26\u5bbd\u8ba1\u7b97\u6548\u7387\u6a21\u5f0f\u3001\u4f4e\u6279\u6b21\u4ea4\u9519\u6a21\u5f0f\u548c\u8ba1\u7b97\u6548\u7387\u5355\u5143\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709PIM\u67b6\u6784\u7684\u5e26\u5bbd\u9650\u5236\u3001\u7ec4\u4ef6\u5229\u7528\u7387\u4f4e\u548c\u8ba1\u7b97\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u8fb9\u7f18\u90e8\u7f72\u4f4e\u6279\u6b21\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\uff0c\u73b0\u6709\u6570\u5b57\u5185\u5b58\u8ba1\u7b97\u67b6\u6784\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff1a\u5e26\u5bbd\u6539\u8fdb\u6709\u9650\u3001\u6df7\u5408\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7ec4\u4ef6\u5229\u7528\u7387\u4f4e\u3001\u8ba1\u7b97\u5355\u5143\u8ba1\u7b97\u80fd\u529b\u4e0d\u8db3\u3002", "method": "1. \u9ad8\u5e26\u5bbd\u8ba1\u7b97\u6548\u7387\u6a21\u5f0f\uff1a\u901a\u8fc7\u5206\u6bb5\u5168\u5c40\u4f4d\u7ebf\u5c06\u6bcf\u4e2a\u5b58\u50a8\u4f53\u5206\u4e3a\u56db\u4e2a\u4f2a\u5b58\u50a8\u4f53\uff1b2. \u4f4e\u6279\u6b21\u4ea4\u9519\u6a21\u5f0f\uff1a\u91cd\u53e0GEMV\u548cGEMM\u64cd\u4f5c\uff1b3. \u8ba1\u7b97\u6548\u7387\u5355\u5143\uff1a\u4ee5\u6d41\u6c34\u7ebf\u65b9\u5f0f\u4e32\u884c\u8f93\u5165\u6743\u91cd\u6570\u636e\uff1b4. \u5217\u5411\u6620\u5c04\u952e\u7f13\u5b58\u77e9\u9635\u548c\u884c\u5411\u6620\u5c04\u503c\u7f13\u5b58\u77e9\u9635\u3002", "result": "\u5728\u5355\u6279\u6b21HBCEM\u6a21\u5f0f\u4e0b\uff0c\u76f8\u6bd4GPU\u57fa\u7ebf\u548c\u6700\u5148\u8fdbPIM\u8bbe\u8ba1\uff0c\u5206\u522b\u5b9e\u73b011.42\u500d\u548c4.25\u500d\u5e73\u5747\u52a0\u901f\uff1b\u5728\u4f4e\u6279\u6b21\u4e0b\uff0cLBIM\u76f8\u6bd4HBCEM\u5b9e\u73b01.12\u500d\u5e73\u5747\u52a0\u901f\u3002", "conclusion": "CD-PIM\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18\u4f4e\u6279\u6b21LLM\u90e8\u7f72\u4e2d\u7684\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2601.12813", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.12813", "abs": "https://arxiv.org/abs/2601.12813", "authors": ["Keyin Wang", "Xiaomu Shi", "Jiaxiang Liu", "Zhilin Wu", "Taolve Chen", "Fu Song", "David N. Jansen"], "title": "A Formally Verified Procedure for Width Inference in FIRRTL", "comment": "Arxiv version for the European Symposium on Programming (ESOP 2026)(to appear) This work was supported by the Strategic Priority Research Program of the Chinese Academy of Sciences, Grant No.~XDA0320101, and partially supported by NSFC-RGC Collaborative Research Grant No.~62561160151. D.N. Jansen is supported by Beijing Natural Science Foundation Project No.~IS25071", "summary": "FIRRTL is an intermediate representation language for Register Transfer Level (RTL) hardware designs. In FIRRTL programs, the bit widths of many components are not specified explicitly and must be inferred during compilation. In mainstream FIRRTL compilers, such as the official compiler firtool, width inference is conducted by a compilation pass referred to as InferWidths, which may fail even for simple FIRRTL programs. In this paper, we thoroughly investigate the width inference problem for FIRRTL programs. We show that, if the constraints obtained from a FIRRTL program are satisfiable, there exists a unique least solution. Based on this result, we propose a complete procedure for solving the width inference problem. We implement it in the interactive theorem prover Rocq and prove its functional correctness. From the Rocq implementation, we extract an OCaml implementation, which is the first formally verified implementation of the InferWidths pass. Extensive experiments demonstrate that our approach can solve more instances than the official InferWidths pass in firtool, normally with high efficiency.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9FIRRTL\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u95f4\u8868\u793a\u8bed\u8a00\u7684\u4f4d\u5bbd\u63a8\u65ad\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u6bd4\u5b98\u65b9\u7f16\u8bd1\u5668\u80fd\u5904\u7406\u66f4\u591a\u5b9e\u4f8b\u4e14\u6548\u7387\u9ad8\u3002", "motivation": "FIRRTL\u7f16\u8bd1\u5668\u4e2d\u7684InferWidths\u5bbd\u5ea6\u63a8\u65ad\u8fc7\u7a0b\u5b58\u5728\u7f3a\u9677\uff0c\u5373\u4f7f\u5bf9\u4e8e\u7b80\u5355\u7684FIRRTL\u7a0b\u5e8f\u4e5f\u53ef\u80fd\u5931\u8d25\uff0c\u9700\u8981\u66f4\u53ef\u9760\u3001\u5b8c\u6574\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u7ea6\u675f\u53ef\u6ee1\u8db3\u6027\u5b58\u5728\u552f\u4e00\u6700\u5c0f\u89e3\u7684\u7406\u8bba\u7ed3\u679c\uff0c\u63d0\u51fa\u5b8c\u6574\u7684\u5bbd\u5ea6\u63a8\u65ad\u8fc7\u7a0b\uff0c\u5728Rocq\u4ea4\u4e92\u5f0f\u5b9a\u7406\u8bc1\u660e\u5668\u4e2d\u5b9e\u73b0\u5e76\u8bc1\u660e\u529f\u80fd\u6b63\u786e\u6027\uff0c\u7136\u540e\u63d0\u53d6\u4e3aOCaml\u5b9e\u73b0\u3002", "result": "\u5b9e\u73b0\u4e86\u9996\u4e2a\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684InferWidths\u8fc7\u7a0b\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6bd4\u5b98\u65b9firtool\u7f16\u8bd1\u5668\u80fd\u89e3\u51b3\u66f4\u591a\u5b9e\u4f8b\uff0c\u901a\u5e38\u5177\u6709\u9ad8\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u4e3aFIRRTL\u5bbd\u5ea6\u63a8\u65ad\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u5b9e\u73b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bbd\u5ea6\u63a8\u65ad\u7684\u53ef\u9760\u6027\u548c\u8986\u76d6\u7387\u3002"}}
{"id": "2601.11624", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.11624", "abs": "https://arxiv.org/abs/2601.11624", "authors": ["Liming Wang", "Feng Li", "Linlin Cui"], "title": "Radio Labeling of Strong Prismatic Network With Star", "comment": null, "summary": "The rapid development of wireless communication has made efficient spectrum assignment a crucial factor in enhancing network performance. As a combinatorial optimization model for channel assignment, the radio labeling is recognized as an NP-hard problem. Therefore, converting the spectrum assignment problem into the radio labeling of graphs and studying the radio labeling of specific graph classes is of great significance. For $G$, a radio labeling $\\varphi: V(G) \\to \\{0, 1, 2, \\ldots\\}$ is required to satisfy $|\\varphi(u) - \\varphi(v)| \\geq \\text{diam}(G) + 1 -d_G(u, v)$, where ${diam(G)}$ and $d_G(u, v)$ are diameter and distance between $u$ and $v$. For a radio labeling $\\varphi$, its $\\text{span}$ is defined as the largest integer assigned by $\\varphi$ to the vertices of $G$; the radio labeling specifically denotes the labeling with the minimal span among possible radio labeling. The strong product is a crucial tool for constructing regular networks, and studying its radio labeling is necessary for the design of optimal channel assignment in wireless networks. Within this manuscript, we discuss the radio labeling of strong prismatic network with star, present the relevant theorems and examples, and propose a parallel algorithm to improve computational efficiency in large-scale network scenarios.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u661f\u5f62\u5f3a\u68f1\u67f1\u7f51\u7edc\u7684\u65e0\u7ebf\u9891\u8c31\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u8bba\u4e2d\u7684\u65e0\u7ebf\u7535\u6807\u53f7\u6a21\u578b\uff0c\u63d0\u51fa\u76f8\u5173\u5b9a\u7406\u3001\u793a\u4f8b\u548c\u5e76\u884c\u7b97\u6cd5\u4ee5\u63d0\u9ad8\u5927\u89c4\u6a21\u7f51\u7edc\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5f97\u9ad8\u6548\u9891\u8c31\u5206\u914d\u6210\u4e3a\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002\u65e0\u7ebf\u7535\u6807\u53f7\u4f5c\u4e3a\u4fe1\u9053\u5206\u914d\u7684\u7ec4\u5408\u4f18\u5316\u6a21\u578b\uff0c\u662f\u4e00\u4e2aNP\u96be\u95ee\u9898\u3002\u7814\u7a76\u7279\u5b9a\u56fe\u7c7b\u7684\u65e0\u7ebf\u7535\u6807\u53f7\u5bf9\u4e8e\u65e0\u7ebf\u7f51\u7edc\u7684\u6700\u4f18\u4fe1\u9053\u5206\u914d\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u5c06\u9891\u8c31\u5206\u914d\u95ee\u9898\u8f6c\u5316\u4e3a\u56fe\u7684\u65e0\u7ebf\u7535\u6807\u53f7\u95ee\u9898\uff0c\u7814\u7a76\u661f\u5f62\u5f3a\u68f1\u67f1\u7f51\u7edc\u7684\u65e0\u7ebf\u7535\u6807\u53f7\u3002\u63d0\u51fa\u76f8\u5173\u5b9a\u7406\u548c\u793a\u4f8b\uff0c\u5e76\u8bbe\u8ba1\u5e76\u884c\u7b97\u6cd5\u4ee5\u63d0\u9ad8\u5927\u89c4\u6a21\u7f51\u7edc\u573a\u666f\u4e0b\u7684\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u8ba8\u8bba\u4e86\u661f\u5f62\u5f3a\u68f1\u67f1\u7f51\u7edc\u7684\u65e0\u7ebf\u7535\u6807\u53f7\uff0c\u63d0\u51fa\u4e86\u76f8\u5173\u5b9a\u7406\u548c\u793a\u4f8b\uff0c\u5e76\u5f00\u53d1\u4e86\u5e76\u884c\u7b97\u6cd5\u6765\u63d0\u5347\u5927\u89c4\u6a21\u7f51\u7edc\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u5f3a\u79ef\u662f\u6784\u5efa\u6b63\u5219\u7f51\u7edc\u7684\u91cd\u8981\u5de5\u5177\uff0c\u7814\u7a76\u5176\u65e0\u7ebf\u7535\u6807\u53f7\u5bf9\u4e8e\u65e0\u7ebf\u7f51\u7edc\u7684\u6700\u4f18\u4fe1\u9053\u5206\u914d\u8bbe\u8ba1\u662f\u5fc5\u8981\u7684\u3002\u672c\u6587\u7684\u7814\u7a76\u4e3a\u5927\u89c4\u6a21\u7f51\u7edc\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u9891\u8c31\u5206\u914d\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12686", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.12686", "abs": "https://arxiv.org/abs/2601.12686", "authors": ["Rafi Zahedi", "Amin Zamani", "Rahul Anilkumar"], "title": "Best Practices for Large Load Interconnections: A North American Perspective on Data Centers", "comment": "Presented at CIGRE United States, and published by CIGRE", "summary": "Large loads are expanding rapidly across North America, led by data centers, cryptocurrency mining, hydrogen production facilities, and heavy-duty charging stations. Each class presents distinct electrical characteristics, but data centers are drawing particular attention as AI deployment drives unprecedented capacity growth. Their scale, duty cycles, and converter-dominated interfaces introduce new challenges for transmission interconnections, especially regarding disturbance behavior, steady-state performance, and operational visibility. This paper reviews best practices for large-load interconnections across North America, synthesizing utility and system operator guidelines into a coherent set of technical requirements. The approach combines handbook and manual analysis with cross-utility comparisons and an outlook on European directions. The review highlights requirements on power quality, telemetry, commissioning tests, and protection coordination, while noting gaps in ride-through specifications, load-variation management, and post-disturbance recovery targets. Building on these findings, the paper proposes practical guidance for developers and utilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u5317\u7f8e\u5927\u578b\u8d1f\u8f7d\uff08\u6570\u636e\u4e2d\u5fc3\u3001\u52a0\u5bc6\u8d27\u5e01\u6316\u77ff\u3001\u5236\u6c22\u8bbe\u65bd\u3001\u91cd\u578b\u5145\u7535\u7ad9\uff09\u5e76\u7f51\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u5206\u6790\u4e86\u6280\u672f\u6311\u6218\u5e76\u63d0\u51fa\u4e86\u5b9e\u7528\u6307\u5bfc\u5efa\u8bae\u3002", "motivation": "\u5317\u7f8e\u5927\u578b\u8d1f\u8f7d\u5feb\u901f\u589e\u957f\uff0c\u7279\u522b\u662fAI\u9a71\u52a8\u4e0b\u7684\u6570\u636e\u4e2d\u5fc3\u6269\u5f20\uff0c\u5e26\u6765\u4e86\u5e76\u7f51\u6311\u6218\u3002\u8fd9\u4e9b\u8d1f\u8f7d\u7684\u89c4\u6a21\u3001\u5de5\u4f5c\u5468\u671f\u548c\u53d8\u6d41\u5668\u4e3b\u5bfc\u7684\u63a5\u53e3\u5bf9\u8f93\u7535\u4e92\u8054\u63d0\u51fa\u4e86\u65b0\u7684\u6270\u52a8\u884c\u4e3a\u3001\u7a33\u6001\u6027\u80fd\u548c\u8fd0\u884c\u53ef\u89c1\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5317\u7f8e\u516c\u7528\u4e8b\u4e1a\u516c\u53f8\u548c\u7cfb\u7edf\u8fd0\u8425\u5546\u7684\u6307\u5357\uff0c\u7ed3\u5408\u624b\u518c\u5206\u6790\u3001\u8de8\u516c\u7528\u4e8b\u4e1a\u6bd4\u8f83\u548c\u6b27\u6d32\u53d1\u5c55\u65b9\u5411\u5c55\u671b\uff0c\u7efc\u5408\u5f62\u6210\u4e00\u5957\u8fde\u8d2f\u7684\u6280\u672f\u8981\u6c42\u6846\u67b6\u3002", "result": "\u8bc6\u522b\u4e86\u5728\u7535\u80fd\u8d28\u91cf\u3001\u9065\u6d4b\u3001\u8c03\u8bd5\u6d4b\u8bd5\u548c\u4fdd\u62a4\u534f\u8c03\u65b9\u9762\u7684\u6280\u672f\u8981\u6c42\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5728\u7a7f\u8d8a\u89c4\u8303\u3001\u8d1f\u8f7d\u53d8\u5316\u7ba1\u7406\u548c\u6270\u52a8\u540e\u6062\u590d\u76ee\u6807\u65b9\u9762\u7684\u5dee\u8ddd\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\uff0c\u4e3a\u5f00\u53d1\u5546\u548c\u516c\u7528\u4e8b\u4e1a\u516c\u53f8\u63d0\u51fa\u4e86\u5b9e\u7528\u7684\u6307\u5bfc\u5efa\u8bae\uff0c\u4ee5\u5e94\u5bf9\u5927\u578b\u8d1f\u8f7d\u5e76\u7f51\u7684\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2601.12943", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.12943", "abs": "https://arxiv.org/abs/2601.12943", "authors": ["Han Xu", "Di Wang"], "title": "Dependently-Typed AARA: A Non-Affine Approach for Resource Analysis of Higher-Order Programs", "comment": null, "summary": "Static resource analysis determines the resource consumption (e.g., time complexity) of a program without executing it. Among the numerous existing approaches for resource analysis, affine type systems have been one dominant approach. However, these affine type systems fall short of deriving precise resource behavior of higher-order programs, particularly in cases that involve partial applications.\n  This article presents \u03bb_\\ms{amor}^\\ms{na}}, a non-affine AARA-style dependent type system for resource reasoning about higher-order functional programs. The key observation is that the main issue in previous approaches comes from (i) the close coupling of types and resources, and (ii) the conflict between affine and higher-order typing mechanisms. To derive precise resource behavior of higher-order functions, \u03bb_\\ms{amor}^\\ms{na}} decouples resources from types and follows a non-affine typing mechanism. The non-affine type system of \u03bb_\\ms{amor}^\\ms{na}} achieves this by using dependent types, which allows expressing type-level potential functions separate from ordinary types. This article formalizes \u03bb_\\ms{amor}^\\ms{na}}'s syntax and semantics, and proves its soundness, which guarantees the correctness of resource bounds. Several challenging classic and higher-order examples are presented to demonstrate the expressiveness and compositionality of \u03bb_\\ms{amor}^\\ms{na}}'s reasoning capability.", "AI": {"tldr": "\u03bb_amor^na \u662f\u4e00\u4e2a\u975e\u4eff\u5c04\u7684AARA\u98ce\u683c\u4f9d\u8d56\u7c7b\u578b\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u9636\u51fd\u6570\u7a0b\u5e8f\u7684\u8d44\u6e90\u5206\u6790\uff0c\u901a\u8fc7\u89e3\u8026\u7c7b\u578b\u548c\u8d44\u6e90\u3001\u4f7f\u7528\u4f9d\u8d56\u7c7b\u578b\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u8d44\u6e90\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u4eff\u5c04\u7c7b\u578b\u7cfb\u7edf\u5728\u5904\u7406\u9ad8\u9636\u7a0b\u5e8f\uff08\u7279\u522b\u662f\u90e8\u5206\u5e94\u7528\uff09\u65f6\u65e0\u6cd5\u63a8\u5bfc\u7cbe\u786e\u7684\u8d44\u6e90\u884c\u4e3a\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u7c7b\u578b\u4e0e\u8d44\u6e90\u7684\u7d27\u5bc6\u8026\u5408\u4ee5\u53ca\u4eff\u5c04\u4e0e\u9ad8\u9636\u7c7b\u578b\u673a\u5236\u7684\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u03bb_amor^na\u7cfb\u7edf\uff0c\u91c7\u7528\u975e\u4eff\u5c04\u7c7b\u578b\u673a\u5236\uff0c\u901a\u8fc7\u4f9d\u8d56\u7c7b\u578b\u5c06\u7c7b\u578b\u7ea7\u52bf\u51fd\u6570\u4e0e\u666e\u901a\u7c7b\u578b\u5206\u79bb\uff0c\u89e3\u8026\u8d44\u6e90\u4e0e\u7c7b\u578b\uff0c\u652f\u6301\u9ad8\u9636\u51fd\u6570\u7684\u7cbe\u786e\u8d44\u6e90\u63a8\u7406\u3002", "result": "\u5f62\u5f0f\u5316\u4e86\u03bb_amor^na\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\uff0c\u8bc1\u660e\u4e86\u5176\u53ef\u9760\u6027\uff08\u4fdd\u8bc1\u8d44\u6e90\u754c\u9650\u7684\u6b63\u786e\u6027\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u4e2a\u7ecf\u5178\u548c\u9ad8\u9636\u793a\u4f8b\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u8868\u8fbe\u80fd\u529b\u548c\u7ec4\u5408\u6027\u3002", "conclusion": "\u03bb_amor^na\u901a\u8fc7\u975e\u4eff\u5c04\u4f9d\u8d56\u7c7b\u578b\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u9ad8\u9636\u51fd\u6570\u8d44\u6e90\u5206\u6790\u7684\u7cbe\u5ea6\u95ee\u9898\uff0c\u4e3a\u9ad8\u9636\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u548c\u7ec4\u5408\u6027\u7684\u8d44\u6e90\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2601.11646", "categories": ["cs.DC", "cs.FL"], "pdf": "https://arxiv.org/pdf/2601.11646", "abs": "https://arxiv.org/abs/2601.11646", "authors": ["Chao Wang", "Ruijia Li", "Yang Zhou", "Peng Wu", "Yi Lv", "Jianwei Liao", "Jim Woodcock", "Zhiming Liu"], "title": "A Forward Simulation-Based Hierarchy of Linearizable Concurrent Objects", "comment": null, "summary": "In this paper, we systematically investigate the connection between linearizable objects and forward simulation. We prove that the sets of linearizable objects satisfying wait-freedom (resp., lock-freedom or obstruction-freedom) form a bounded join-semilattice under the forward simulation relation, and that the sets of linearizable objects without liveness constraints form a bounded lattice under the same relation. As part of our lattice result, we propose an equivalent characterization of linearizability by reducing checking linearizability w.r.t. sequential specification $Spec$ into checking forward simulation by an object $\\mathcal{U}_{Spec}$. To demonstrate the forward simulation relation between linearizable objects, we prove that the objects that are strongly linearizable w.r.t. the same sequential specification and are wait-free (resp., lock-free, obstruction-free) simulate each other, and we prove that the time-stamped queue simulates the Herlihy-Wing queue. We also prove that the Herlihy-Wing queue is simulated by $\\mathcal{U}_{Spec}$, and thus, our equivalent characterization of linearizability can be used in the verification of linearizability.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u7ebf\u6027\u5316\u5bf9\u8c61\u4e0e\u5411\u524d\u6a21\u62df\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8bc1\u660e\u4e86\u5728\u4e0d\u540c\u6d3b\u6027\u7ea6\u675f\u4e0b\u7ebf\u6027\u5316\u5bf9\u8c61\u96c6\u5408\u5728\u5411\u524d\u6a21\u62df\u5173\u7cfb\u4e0b\u5f62\u6210\u6709\u754c\u534a\u683c\u6216\u683c\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u4e86\u7ebf\u6027\u5316\u7684\u7b49\u4ef7\u7279\u5f81\u5316\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u7ebf\u6027\u5316\u5bf9\u8c61\u4e0e\u5411\u524d\u6a21\u62df\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u4e3a\u5e76\u53d1\u5bf9\u8c61\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002\u5f53\u524d\u7f3a\u4e4f\u5bf9\u7ebf\u6027\u5316\u5bf9\u8c61\u5728\u5411\u524d\u6a21\u62df\u5173\u7cfb\u4e0b\u4ee3\u6570\u7ed3\u6784\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u4ee5\u53ca\u5982\u4f55\u5229\u7528\u8fd9\u79cd\u5173\u7cfb\u7b80\u5316\u7ebf\u6027\u5316\u9a8c\u8bc1\u3002", "method": "\u91c7\u7528\u5f62\u5f0f\u5316\u65b9\u6cd5\u5206\u6790\u7ebf\u6027\u5316\u5bf9\u8c61\u4e0e\u5411\u524d\u6a21\u62df\u7684\u5173\u7cfb\uff1a1) \u8bc1\u660e\u5728\u4e0d\u540c\u6d3b\u6027\u7ea6\u675f\u4e0b\u7ebf\u6027\u5316\u5bf9\u8c61\u96c6\u5408\u5f62\u6210\u6709\u754c\u534a\u683c\u6216\u683c\u7ed3\u6784\uff1b2) \u63d0\u51fa\u901a\u8fc7\u5bf9\u8c61\ud835\udcb0_\ud835\udc46\ud835\udc5d\ud835\udc52\ud835\udc50\u5c06\u7ebf\u6027\u5316\u9a8c\u8bc1\u8f6c\u5316\u4e3a\u5411\u524d\u6a21\u62df\u68c0\u67e5\u7684\u7b49\u4ef7\u7279\u5f81\u5316\u65b9\u6cd5\uff1b3) \u901a\u8fc7\u5177\u4f53\u6848\u4f8b\u8bc1\u660e\u5bf9\u8c61\u95f4\u7684\u6a21\u62df\u5173\u7cfb\u3002", "result": "1) \u8bc1\u660e\u4e86\u7b49\u5f85\u81ea\u7531\u3001\u9501\u81ea\u7531\u548c\u969c\u788d\u81ea\u7531\u7ebf\u6027\u5316\u5bf9\u8c61\u96c6\u5408\u5728\u5411\u524d\u6a21\u62df\u5173\u7cfb\u4e0b\u5f62\u6210\u6709\u754c\u5e76\u534a\u683c\uff1b2) \u65e0\u6d3b\u6027\u7ea6\u675f\u7684\u7ebf\u6027\u5316\u5bf9\u8c61\u96c6\u5408\u5f62\u6210\u6709\u754c\u683c\uff1b3) \u63d0\u51fa\u4e86\u7ebf\u6027\u5316\u7684\u7b49\u4ef7\u7279\u5f81\u5316\u65b9\u6cd5\uff1b4) \u8bc1\u660e\u4e86\u5f3a\u7ebf\u6027\u5316\u5bf9\u8c61\u95f4\u7684\u76f8\u4e92\u6a21\u62df\u5173\u7cfb\uff0c\u4ee5\u53ca\u65f6\u95f4\u6233\u961f\u5217\u6a21\u62dfHerlihy-Wing\u961f\u5217\uff1b5) \u8bc1\u660e\u4e86Herlihy-Wing\u961f\u5217\u88ab\ud835\udcb0_\ud835\udc46\ud835\udc5d\ud835\udc52\ud835\udc50\u6a21\u62df\u3002", "conclusion": "\u7ebf\u6027\u5316\u5bf9\u8c61\u5728\u5411\u524d\u6a21\u62df\u5173\u7cfb\u4e0b\u5177\u6709\u4e30\u5bcc\u7684\u4ee3\u6570\u7ed3\u6784\uff0c\u63d0\u51fa\u7684\u7b49\u4ef7\u7279\u5f81\u5316\u65b9\u6cd5\u4e3a\u7ebf\u6027\u5316\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002\u8fd9\u4e9b\u7406\u8bba\u7ed3\u679c\u4e3a\u5e76\u53d1\u5bf9\u8c61\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.13628", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.13628", "abs": "https://arxiv.org/abs/2601.13628", "authors": ["Yue Jiet Chong", "Yimin Wang", "Zhen Wu", "Xuanyao Fong"], "title": "PRIMAL: Processing-In-Memory Based Low-Rank Adaptation for LLM Inference Accelerator", "comment": "Accepted to 2026 IEEE International Symposium on Circuits and Systems (ISCAS'26)", "summary": "This paper presents PRIMAL, a processing-in-memory (PIM) based large language model (LLM) inference accelerator with low-rank adaptation (LoRA). PRIMAL integrates heterogeneous PIM processing elements (PEs), interconnected by 2D-mesh inter-PE computational network (IPCN). A novel SRAM reprogramming and power gating (SRPG) scheme enables pipelined LoRA updates and sub-linear power scaling by overlapping reconfiguration with computation and gating idle resources. PRIMAL employs optimized spatial mapping and dataflow orchestration to minimize communication overhead, and achieves $1.5\\times$ throughput and $25\\times$ energy efficiency over NVIDIA H100 with LoRA rank 8 (Q,V) on Llama-13B.", "AI": {"tldr": "PRIMAL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b58\u5185\u8ba1\u7b97\uff08PIM\uff09\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u52a0\u901f\u5668\uff0c\u96c6\u6210\u4e86\u4f4e\u79e9\u81ea\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\uff0c\u901a\u8fc7\u5f02\u6784PIM\u5904\u7406\u5355\u5143\u30012D\u7f51\u683c\u4e92\u8fde\u7f51\u7edc\u548c\u521b\u65b0\u7684SRAM\u91cd\u7f16\u7a0b\u4e0e\u7535\u6e90\u95e8\u63a7\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u80fd\u63a8\u7406\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\uff0c\u4f20\u7edfGPU\u67b6\u6784\u5728\u63a8\u7406\u6548\u7387\u548c\u80fd\u8017\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u5b58\u5185\u8ba1\u7b97\uff08PIM\uff09\u6280\u672f\u53ef\u4ee5\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u5f00\u9500\uff0c\u800c\u4f4e\u79e9\u81ea\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\u80fd\u591f\u9ad8\u6548\u5730\u5fae\u8c03\u6a21\u578b\u3002PRIMAL\u65e8\u5728\u7ed3\u5408PIM\u548cLoRA\u7684\u4f18\u52bf\uff0c\u6784\u5efa\u4e00\u4e2a\u9ad8\u6548\u80fd\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u52a0\u901f\u5668\u3002", "method": "PRIMAL\u91c7\u7528\u5f02\u6784PIM\u5904\u7406\u5355\u5143\uff08PEs\uff09\uff0c\u901a\u8fc72D\u7f51\u683c\u4e92\u8fde\u7f51\u7edc\uff08IPCN\uff09\u8fde\u63a5\u3002\u521b\u65b0\u6027\u5730\u63d0\u51fa\u4e86SRAM\u91cd\u7f16\u7a0b\u4e0e\u7535\u6e90\u95e8\u63a7\uff08SRPG\uff09\u65b9\u6848\uff0c\u652f\u6301\u6d41\u6c34\u7ebf\u5316\u7684LoRA\u66f4\u65b0\u548c\u4e9a\u7ebf\u6027\u529f\u8017\u6269\u5c55\u3002\u901a\u8fc7\u4f18\u5316\u7684\u7a7a\u95f4\u6620\u5c04\u548c\u6570\u636e\u6d41\u7f16\u6392\u6765\u6700\u5c0f\u5316\u901a\u4fe1\u5f00\u9500\u3002", "result": "\u5728Llama-13B\u6a21\u578b\u4e0a\uff0c\u4f7f\u7528LoRA\u79e9\u4e3a8\uff08Q,V\uff09\u65f6\uff0cPRIMAL\u76f8\u6bd4NVIDIA H100\u5b9e\u73b0\u4e861.5\u500d\u7684\u541e\u5410\u91cf\u548c25\u500d\u7684\u80fd\u6548\u63d0\u5347\u3002", "conclusion": "PRIMAL\u6210\u529f\u5c55\u793a\u4e86PIM\u67b6\u6784\u4e0eLoRA\u6280\u672f\u7ed3\u5408\u5728\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u52a0\u901f\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u786c\u4ef6\u8bbe\u8ba1\u548c\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u548c\u80fd\u6548\u3002"}}
{"id": "2601.13224", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.13224", "abs": "https://arxiv.org/abs/2601.13224", "authors": ["Michael Hanus", "Steven Libby"], "title": "Functional Logic Program Transformations", "comment": "Presented at Conference on Declarative Programming (DECLARE 2025)", "summary": "Many tools used to process programs, like compilers, analyzers, or verifiers, perform transformations on their intermediate program representation, like abstract syntax trees. Implementing such program transformations is a non-trivial task, since it is necessary to iterate over the complete syntax tree and apply various transformations at nodes in a tree. In this paper we show how the features of functional logic programming are useful to implement program transformations in a compact and comprehensible manner. For this purpose, we propose to write program transformations as partially defined and non-deterministic operations. Since the implementation of non-determinism usually causes some overhead compared to deterministically defined operations, we compare our approach to a deterministic transformation method. We evaluate these alternatives for the functional logic language Curry and its intermediate representation FlatCurry which is used in various analysis and verification tools and compilers.", "AI": {"tldr": "\u4f7f\u7528\u51fd\u6570\u903b\u8f91\u7f16\u7a0b\u5b9e\u73b0\u7a0b\u5e8f\u8f6c\u6362\uff0c\u901a\u8fc7\u90e8\u5206\u5b9a\u4e49\u548c\u975e\u786e\u5b9a\u6027\u64cd\u4f5c\u7b80\u5316\u8bed\u6cd5\u6811\u53d8\u6362", "motivation": "\u7f16\u8bd1\u5668\u3001\u5206\u6790\u5668\u548c\u9a8c\u8bc1\u5668\u7b49\u5de5\u5177\u9700\u8981\u5728\u4e2d\u95f4\u7a0b\u5e8f\u8868\u793a\uff08\u5982\u62bd\u8c61\u8bed\u6cd5\u6811\uff09\u4e0a\u6267\u884c\u53d8\u6362\uff0c\u4f46\u5b9e\u73b0\u8fd9\u4e9b\u53d8\u6362\u5f88\u590d\u6742\uff0c\u9700\u8981\u904d\u5386\u5b8c\u6574\u8bed\u6cd5\u6811\u5e76\u5728\u8282\u70b9\u5e94\u7528\u5404\u79cd\u53d8\u6362", "method": "\u63d0\u51fa\u5c06\u7a0b\u5e8f\u53d8\u6362\u7f16\u5199\u4e3a\u90e8\u5206\u5b9a\u4e49\u548c\u975e\u786e\u5b9a\u6027\u64cd\u4f5c\uff0c\u5229\u7528\u51fd\u6570\u903b\u8f91\u7f16\u7a0b\u7684\u7279\u6027\uff0c\u5728Curry\u8bed\u8a00\u53ca\u5176\u4e2d\u95f4\u8868\u793aFlatCurry\u4e0a\u5b9e\u73b0", "result": "\u4e0e\u786e\u5b9a\u6027\u53d8\u6362\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u975e\u786e\u5b9a\u6027\u5b9e\u73b0\u7684\u5f00\u9500", "conclusion": "\u51fd\u6570\u903b\u8f91\u7f16\u7a0b\u7684\u7279\u6027\u6709\u52a9\u4e8e\u4ee5\u7d27\u51d1\u4e14\u6613\u4e8e\u7406\u89e3\u7684\u65b9\u5f0f\u5b9e\u73b0\u7a0b\u5e8f\u53d8\u6362"}}
{"id": "2601.11652", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11652", "abs": "https://arxiv.org/abs/2601.11652", "authors": ["Xiangchen Li", "Jiakun Fan", "Qingyuan Wang", "Dimitrios Spatharakis", "Saeid Ghafouri", "Hans Vandierendonck", "Deepu John", "Bo Ji", "Ali R. Butt", "Dimitrios S. Nikolopoulos"], "title": "WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching", "comment": "28 Pages, 11 Figures, 12 Tables", "summary": "As Large Language Models (LLMs) become increasingly accessible to end users, an ever-growing number of inference requests are initiated from edge devices and computed on centralized GPU clusters. However, the resulting exponential growth in computation workload is placing significant strain on data centers, while edge devices remain largely underutilized, leading to imbalanced workloads and resource inefficiency across the network. Integrating edge devices into the LLM inference process via speculative decoding helps balance the workload between the edge and the cloud, while maintaining lossless prediction accuracy. In this paper, we identify and formalize two critical bottlenecks that limit the efficiency and scalability of distributed speculative LLM serving: Wasted Drafting Time and Verification Interference. To address these challenges, we propose WISP, an efficient and SLO-aware distributed LLM inference system that consists of an intelligent speculation controller, a verification time estimator, and a verification batch scheduler. These components collaboratively enhance drafting efficiency and optimize verification request scheduling on the server. Extensive numerical results show that WISP improves system capacity by up to 2.1x and 4.1x, and increases system goodput by up to 1.94x and 3.7x, compared to centralized serving and SLED, respectively.", "AI": {"tldr": "WISP\uff1a\u4e00\u79cd\u9ad8\u6548\u7684\u5206\u5e03\u5f0fLLM\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u667a\u80fd\u63a8\u6d4b\u89e3\u7801\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u4e0e\u4e91\u7aef\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u63d0\u5347\u7cfb\u7edf\u5bb9\u91cf\u548c\u541e\u5410\u91cf", "motivation": "\u968f\u7740LLM\u8d8a\u6765\u8d8a\u666e\u53ca\uff0c\u5927\u91cf\u63a8\u7406\u8bf7\u6c42\u4ece\u8fb9\u7f18\u8bbe\u5907\u53d1\u8d77\u4f46\u5728\u4e2d\u5fc3GPU\u96c6\u7fa4\u8ba1\u7b97\uff0c\u5bfc\u81f4\u6570\u636e\u4e2d\u5fc3\u8d1f\u8f7d\u8fc7\u91cd\u800c\u8fb9\u7f18\u8bbe\u5907\u5229\u7528\u7387\u4e0d\u8db3\uff0c\u9020\u6210\u7f51\u7edc\u8d44\u6e90\u6548\u7387\u4f4e\u4e0b\u548c\u8d1f\u8f7d\u4e0d\u5747\u8861", "method": "\u63d0\u51faWISP\u7cfb\u7edf\uff0c\u5305\u542b\u667a\u80fd\u63a8\u6d4b\u63a7\u5236\u5668\u3001\u9a8c\u8bc1\u65f6\u95f4\u4f30\u8ba1\u5668\u548c\u9a8c\u8bc1\u6279\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u89e3\u51b3\"\u6d6a\u8d39\u7684\u8349\u7a3f\u65f6\u95f4\"\u548c\"\u9a8c\u8bc1\u5e72\u6270\"\u4e24\u4e2a\u5173\u952e\u74f6\u9888\uff0c\u4f18\u5316\u8fb9\u7f18\u8bbe\u5907\u8349\u7a3f\u751f\u6210\u6548\u7387\u548c\u4e91\u7aef\u9a8c\u8bc1\u8c03\u5ea6", "result": "\u76f8\u6bd4\u96c6\u4e2d\u5f0f\u670d\u52a1\u548cSLED\uff0cWISP\u5206\u522b\u5c06\u7cfb\u7edf\u5bb9\u91cf\u63d0\u53472.1\u500d\u548c4.1\u500d\uff0c\u7cfb\u7edf\u6709\u6548\u541e\u5410\u91cf\u63d0\u53471.94\u500d\u548c3.7\u500d", "conclusion": "WISP\u901a\u8fc7\u5206\u5e03\u5f0f\u63a8\u6d4b\u89e3\u7801\u6709\u6548\u5e73\u8861\u8fb9\u7f18\u4e0e\u4e91\u7aef\u8d1f\u8f7d\uff0c\u5728\u4fdd\u6301\u65e0\u635f\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027"}}
{"id": "2601.13804", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.13804", "abs": "https://arxiv.org/abs/2601.13804", "authors": ["Ioannis Constantinou", "Arthur Perais", "Yiannakis Sazeides"], "title": "The Non-Predictability of Mispredicted Branches using Timing Information", "comment": null, "summary": "Branch misprediction latency is one of the most important contributors to performance degradation and wasted energy consumption in a modern core. State-of-the-art predictors generally perform very well but occasionally suffer from high Misprediction Per Kilo Instruction due to hard-to-predict branches. In this work, we investigate if predicting branches using microarchitectural information, in addition to traditional branch history, can improve prediction accuracy. Our approach considers branch timing information (resolution cycle) both for older branches in the Reorder Buffer (ROB) and recently committed, and for younger branches relative to the branch we re-predict. We propose Speculative Branch Resolution (SBR) in which, N cycles after a branch allocates in the ROB, various timing information is collected and used to re-predict. Using the gem5 simulator we implement and perform a limit-study of SBR using a TAGE-Like predictor. Our experiments show that the post-alloc timing information we used was not able to yield performance gains over an unbounded TAGE-SC. However, we find two hard to predict branches where timing information did provide an advantage and thoroughly analysed one of them to understand why. This finding suggests that predictors may benefit from specific microarchitectural information to increase accuracy on specific hard to predict branches and that overriding predictions in the backend may yet yield performance benefits, but that further research is needed to determine such information vectors.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u5229\u7528\u5fae\u67b6\u6784\u65f6\u5e8f\u4fe1\u606f\u6539\u8fdb\u5206\u652f\u9884\u6d4b\uff0c\u63d0\u51faSBR\u65b9\u6cd5\uff0c\u4f46\u5b9e\u9a8c\u663e\u793a\u5bf9\u6574\u4f53\u6027\u80fd\u63d0\u5347\u6709\u9650\uff0c\u4ec5\u5bf9\u7279\u5b9a\u96be\u9884\u6d4b\u5206\u652f\u6709\u6548\u3002", "motivation": "\u5206\u652f\u9884\u6d4b\u9519\u8bef\u662f\u73b0\u4ee3\u5904\u7406\u5668\u6027\u80fd\u4e0b\u964d\u548c\u80fd\u8017\u6d6a\u8d39\u7684\u4e3b\u8981\u539f\u56e0\u3002\u73b0\u6709\u9884\u6d4b\u5668\u867d\u7136\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u67d0\u4e9b\u96be\u9884\u6d4b\u5206\u652f\u4ecd\u5b58\u5728\u9ad8\u8bef\u9884\u6d4b\u7387\u3002\u4f5c\u8005\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u7ed3\u5408\u5fae\u67b6\u6784\u65f6\u5e8f\u4fe1\u606f\u6765\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faSpeculative Branch Resolution (SBR)\u65b9\u6cd5\uff1a\u5728\u5206\u652f\u6307\u4ee4\u8fdb\u5165\u91cd\u6392\u5e8f\u7f13\u51b2\u533a(ROB)\u540eN\u4e2a\u5468\u671f\uff0c\u6536\u96c6\u5404\u79cd\u65f6\u5e8f\u4fe1\u606f\uff08\u5305\u62ecROB\u4e2d\u8f83\u8001\u5206\u652f\u548c\u5df2\u63d0\u4ea4\u5206\u652f\u7684\u89e3\u51b3\u5468\u671f\uff0c\u4ee5\u53ca\u76f8\u5bf9\u4e8e\u5f53\u524d\u9884\u6d4b\u5206\u652f\u7684\u8f83\u5e74\u8f7b\u5206\u652f\u4fe1\u606f\uff09\u8fdb\u884c\u91cd\u65b0\u9884\u6d4b\u3002\u4f7f\u7528gem5\u6a21\u62df\u5668\u5b9e\u73b0\uff0c\u5e76\u57fa\u4e8eTAGE-Like\u9884\u6d4b\u5668\u8fdb\u884c\u6781\u9650\u7814\u7a76\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u7684\u5206\u914d\u540e\u65f6\u5e8f\u4fe1\u606f\u672a\u80fd\u8d85\u8d8a\u65e0\u9650\u5236\u7684TAGE-SC\u9884\u6d4b\u5668\u7684\u6027\u80fd\u3002\u4f46\u53d1\u73b0\u4e24\u4e2a\u96be\u9884\u6d4b\u5206\u652f\u786e\u5b9e\u4ece\u65f6\u5e8f\u4fe1\u606f\u4e2d\u83b7\u76ca\uff0c\u5e76\u6df1\u5165\u5206\u6790\u4e86\u5176\u4e2d\u4e00\u4e2a\u6848\u4f8b\u4ee5\u7406\u89e3\u539f\u56e0\u3002", "conclusion": "\u7279\u5b9a\u5fae\u67b6\u6784\u4fe1\u606f\u53ef\u80fd\u6709\u52a9\u4e8e\u63d0\u5347\u7279\u5b9a\u96be\u9884\u6d4b\u5206\u652f\u7684\u51c6\u786e\u6027\uff0c\u540e\u7aef\u8986\u76d6\u9884\u6d4b\u53ef\u80fd\u5e26\u6765\u6027\u80fd\u6536\u76ca\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u786e\u5b9a\u6709\u6548\u7684\u4fe1\u606f\u5411\u91cf\u3002"}}
{"id": "2601.13341", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.13341", "abs": "https://arxiv.org/abs/2601.13341", "authors": ["Namratha Gangamreddypalli", "Constantin Enea", "Shaz Qadeer"], "title": "Reduction for Structured Concurrent Programs", "comment": null, "summary": "Commutativity reasoning based on Lipton's movers is a powerful technique for verification of concurrent programs. The idea is to define a program transformation that preserves a subset of the initial set of interleavings, which is sound modulo reorderings of commutative actions. Scaling commutativity reasoning to routinely-used features in software systems, such as procedures and parallel composition, remains a significant challenge.\n  In this work, we introduce a novel reduction technique for structured concurrent programs that unifies two key advances. First, we present a reduction strategy that soundly replaces parallel composition with sequential composition. Second, we generalize Lipton's reduction to support atomic sections containing (potentially recursive) procedure calls. Crucially, these two foundational strategies can be composed arbitrarily, greatly expanding the scope and flexibility of reduction-based reasoning. We implemented this technique in Civl and demonstrated its effectiveness on a number of challenging case studies, including a snapshot object, a fault-tolerant and linearizable register, the FLASH cache coherence protocol, and a non-trivial variant of Two-Phase Commit.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u5316\u5e76\u53d1\u7a0b\u5e8f\u89c4\u7ea6\u6280\u672f\uff0c\u7edf\u4e00\u4e86\u4e24\u79cd\u5173\u952e\u8fdb\u5c55\uff1a\u5c06\u5e76\u884c\u7ec4\u5408\u66ff\u6362\u4e3a\u987a\u5e8f\u7ec4\u5408\uff0c\u4ee5\u53ca\u6269\u5c55Lipton\u89c4\u7ea6\u4ee5\u652f\u6301\u5305\u542b\u8fc7\u7a0b\u8c03\u7528\u7684\u539f\u5b50\u6bb5\u3002", "motivation": "\u57fa\u4e8eLipton\u79fb\u52a8\u5b50\u7684\u4ea4\u6362\u6027\u63a8\u7406\u662f\u9a8c\u8bc1\u5e76\u53d1\u7a0b\u5e8f\u7684\u5f3a\u5927\u6280\u672f\uff0c\u4f46\u5c06\u5176\u6269\u5c55\u5230\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u5e38\u7528\u7684\u7279\u6027\uff08\u5982\u8fc7\u7a0b\u548c\u5e76\u884c\u7ec4\u5408\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u89c4\u7ea6\u6280\u672f\uff1a1\uff09\u63d0\u51fa\u5c06\u5e76\u884c\u7ec4\u5408\u66ff\u6362\u4e3a\u987a\u5e8f\u7ec4\u5408\u7684\u89c4\u7ea6\u7b56\u7565\uff1b2\uff09\u5c06Lipton\u89c4\u7ea6\u63a8\u5e7f\u5230\u652f\u6301\u5305\u542b\uff08\u53ef\u80fd\u9012\u5f52\uff09\u8fc7\u7a0b\u8c03\u7528\u7684\u539f\u5b50\u6bb5\uff1b3\uff09\u8fd9\u4e24\u79cd\u57fa\u7840\u7b56\u7565\u53ef\u4ee5\u4efb\u610f\u7ec4\u5408\u3002", "result": "\u5728Civl\u4e2d\u5b9e\u73b0\u4e86\u8be5\u6280\u672f\uff0c\u5e76\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u5305\u62ec\u5feb\u7167\u5bf9\u8c61\u3001\u5bb9\u9519\u7ebf\u6027\u5316\u5bc4\u5b58\u5668\u3001FLASH\u7f13\u5b58\u4e00\u81f4\u6027\u534f\u8bae\u548cTwo-Phase Commit\u7684\u975e\u5e73\u51e1\u53d8\u4f53\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u7edf\u4e00\u4e86\u4e24\u79cd\u5173\u952e\u7684\u89c4\u7ea6\u7b56\u7565\uff0c\u6781\u5927\u5730\u6269\u5c55\u4e86\u57fa\u4e8e\u89c4\u7ea6\u7684\u63a8\u7406\u7684\u8303\u56f4\u548c\u7075\u6d3b\u6027\uff0c\u4e3a\u7ed3\u6784\u5316\u5e76\u53d1\u7a0b\u5e8f\u7684\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2601.11676", "categories": ["cs.DC", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.11676", "abs": "https://arxiv.org/abs/2601.11676", "authors": ["Peirong Zheng", "Wenchao Xu", "Haozhao Wang", "Jinyu Chen", "Xuemin Shen"], "title": "HALO: Semantic-Aware Distributed LLM Inference in Lossy Edge Network", "comment": "Accepted by IEEE International Conference on Computer Communications (INFOCOM) 2026", "summary": "The deployment of large language models' (LLMs) inference at the edge can facilitate prompt service responsiveness while protecting user privacy. However, it is critically challenged by the resource constraints of a single edge node. Distributed inference has emerged to aggregate and leverage computational resources across multiple devices. Yet, existing methods typically require strict synchronization, which is often infeasible due to the unreliable network conditions. In this paper, we propose HALO, a novel framework that can boost the distributed LLM inference in lossy edge network. The core idea is to enable a relaxed yet effective synchronization by strategically allocating less critical neuron groups to unstable devices, thus avoiding the excessive waiting time incurred by delayed packets. HALO introduces three key mechanisms: (1) a semantic-aware predictor to assess the significance of neuron groups prior to activation. (2) a parallel execution scheme of neuron group loading during the model inference. (3) a load-balancing scheduler that efficiently orchestrates multiple devices with heterogeneous resources. Experimental results from a Raspberry Pi cluster demonstrate that HALO achieves a 3.41x end-to-end speedup for LLaMA-series LLMs under unreliable network conditions. It maintains performance comparable to optimal conditions and significantly outperforms the state-of-the-art in various scenarios.", "AI": {"tldr": "HALO\u662f\u4e00\u4e2a\u7528\u4e8e\u63d0\u5347\u8fb9\u7f18\u7f51\u7edc\u4e2d\u5206\u5e03\u5f0fLLM\u63a8\u7406\u6027\u80fd\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u677e\u5f1b\u540c\u6b65\u7b56\u7565\u548c\u667a\u80fd\u795e\u7ecf\u5143\u7ec4\u5206\u914d\u6765\u5e94\u5bf9\u4e0d\u53ef\u9760\u7f51\u7edc\u6761\u4ef6\u3002", "motivation": "\u8fb9\u7f18\u90e8\u7f72LLM\u63a8\u7406\u53ef\u4ee5\u63d0\u9ad8\u670d\u52a1\u54cd\u5e94\u901f\u5ea6\u5e76\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u4f46\u5355\u4e2a\u8fb9\u7f18\u8282\u70b9\u7684\u8d44\u6e90\u9650\u5236\u662f\u4e3b\u8981\u6311\u6218\u3002\u5206\u5e03\u5f0f\u63a8\u7406\u53ef\u4ee5\u805a\u5408\u591a\u4e2a\u8bbe\u5907\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4e25\u683c\u540c\u6b65\uff0c\u8fd9\u5728\u4e0d\u53ef\u9760\u7684\u7f51\u7edc\u6761\u4ef6\u4e0b\u5f80\u5f80\u4e0d\u53ef\u884c\u3002", "method": "HALO\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u673a\u5236\uff1a1) \u8bed\u4e49\u611f\u77e5\u9884\u6d4b\u5668\uff0c\u5728\u6fc0\u6d3b\u524d\u8bc4\u4f30\u795e\u7ecf\u5143\u7ec4\u7684\u91cd\u8981\u6027\uff1b2) \u6a21\u578b\u63a8\u7406\u671f\u95f4\u7684\u795e\u7ecf\u5143\u7ec4\u52a0\u8f7d\u5e76\u884c\u6267\u884c\u65b9\u6848\uff1b3) \u8d1f\u8f7d\u5747\u8861\u8c03\u5ea6\u5668\uff0c\u6709\u6548\u534f\u8c03\u5f02\u6784\u8d44\u6e90\u7684\u591a\u4e2a\u8bbe\u5907\u3002\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u5c06\u4e0d\u592a\u5173\u952e\u7684\u795e\u7ecf\u5143\u7ec4\u5206\u914d\u7ed9\u4e0d\u7a33\u5b9a\u7684\u8bbe\u5907\uff0c\u5b9e\u73b0\u677e\u5f1b\u4f46\u6709\u6548\u7684\u540c\u6b65\uff0c\u907f\u514d\u5ef6\u8fdf\u6570\u636e\u5305\u5bfc\u81f4\u7684\u8fc7\u5ea6\u7b49\u5f85\u65f6\u95f4\u3002", "result": "\u5728\u6811\u8393\u6d3e\u96c6\u7fa4\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHALO\u5728\u4e0d\u53ef\u9760\u7f51\u7edc\u6761\u4ef6\u4e0b\u4e3aLLaMA\u7cfb\u5217LLM\u5b9e\u73b0\u4e863.41\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002\u5b83\u5728\u5404\u79cd\u573a\u666f\u4e0b\u4fdd\u6301\u4e0e\u6700\u4f18\u6761\u4ef6\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "HALO\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8fb9\u7f18\u7f51\u7edc\u4e2d\u5206\u5e03\u5f0fLLM\u63a8\u7406\u7684\u540c\u6b65\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u667a\u80fd\u7684\u795e\u7ecf\u5143\u7ec4\u5206\u914d\u548c\u677e\u5f1b\u540c\u6b65\u7b56\u7565\uff0c\u5728\u4e0d\u53ef\u9760\u7f51\u7edc\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u8fb9\u7f18AI\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13815", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.13815", "abs": "https://arxiv.org/abs/2601.13815", "authors": ["Lukas Krupp", "Matthew Venn", "Norbert Wehn"], "title": "From RTL to Prompt Coding: Empowering the Next Generation of Chip Designers through LLMs", "comment": "Accepted for presentation at the 2026 IEEE International Symposium on Circuits and Systems (ISCAS 2026). Proceedings to be included in IEEE Xplore", "summary": "This paper presents an LLM-based learning platform for chip design education, aiming to make chip design accessible to beginners without overwhelming them with technical complexity. It represents the first educational platform that assists learners holistically across both frontend and backend design. The proposed approach integrates an LLM-based chat agent into a browser-based workflow built upon the Tiny Tapeout ecosystem. The workflow guides users from an initial design idea through RTL code generation to a tapeout-ready chip. To evaluate the concept, a case study was conducted with 18 high-school students. Within a 90-minute session they developed eight functional VGA chip designs in a 130 nm technology. Despite having no prior experience in chip design, all groups successfully implemented tapeout-ready projects. The results demonstrate the feasibility and educational impact of LLM-assisted chip design, highlighting its potential to attract and inspire early learners and significantly broaden the target audience for the field.", "AI": {"tldr": "\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u82af\u7247\u8bbe\u8ba1\u6559\u80b2\u5e73\u53f0\uff0c\u96c6\u6210\u804a\u5929\u4ee3\u7406\u5230\u6d4f\u89c8\u5668\u5de5\u4f5c\u6d41\uff0c\u8ba9\u65e0\u7ecf\u9a8c\u7684\u9ad8\u4e2d\u751f\u572890\u5206\u949f\u5185\u5b8c\u6210\u53ef\u6d41\u7247\u7684VGA\u82af\u7247\u8bbe\u8ba1", "motivation": "\u964d\u4f4e\u82af\u7247\u8bbe\u8ba1\u5165\u95e8\u95e8\u69db\uff0c\u8ba9\u521d\u5b66\u8005\u65e0\u9700\u9762\u5bf9\u590d\u6742\u6280\u672f\u7ec6\u8282\uff0c\u540c\u65f6\u63d0\u4f9b\u8986\u76d6\u524d\u7aef\u548c\u540e\u7aef\u8bbe\u8ba1\u7684\u5168\u9762\u6559\u80b2\u652f\u6301", "method": "\u5c06LLM\u804a\u5929\u4ee3\u7406\u96c6\u6210\u5230\u57fa\u4e8eTiny Tapeout\u751f\u6001\u7cfb\u7edf\u7684\u6d4f\u89c8\u5668\u5de5\u4f5c\u6d41\u4e2d\uff0c\u5f15\u5bfc\u7528\u6237\u4ece\u8bbe\u8ba1\u60f3\u6cd5\u5230RTL\u4ee3\u7801\u751f\u6210\u518d\u5230\u53ef\u6d41\u7247\u82af\u7247", "result": "18\u540d\u9ad8\u4e2d\u751f\u5728\u6ca1\u6709\u82af\u7247\u8bbe\u8ba1\u7ecf\u9a8c\u7684\u60c5\u51b5\u4e0b\uff0c90\u5206\u949f\u5185\u5f00\u53d1\u51fa8\u4e2a\u529f\u80fd\u6b63\u5e38\u7684130nm\u5de5\u827aVGA\u82af\u7247\u8bbe\u8ba1\uff0c\u6240\u6709\u7ec4\u90fd\u6210\u529f\u5b9e\u73b0\u4e86\u53ef\u6d41\u7247\u9879\u76ee", "conclusion": "LLM\u8f85\u52a9\u7684\u82af\u7247\u8bbe\u8ba1\u5177\u6709\u53ef\u884c\u6027\u548c\u6559\u80b2\u5f71\u54cd\u529b\uff0c\u80fd\u591f\u5438\u5f15\u548c\u6fc0\u52b1\u65e9\u671f\u5b66\u4e60\u8005\uff0c\u663e\u8457\u6269\u5927\u8be5\u9886\u57df\u7684\u53d7\u4f17\u7fa4\u4f53"}}
{"id": "2601.13727", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.13727", "abs": "https://arxiv.org/abs/2601.13727", "authors": ["Bart Jacobs"], "title": "Foundational VeriFast: Pragmatic Certification of Verification Tool Results through Hinted Mirroring", "comment": "8 pages, 2 figures", "summary": "VeriFast is a leading tool for the modular formal verification of correctness properties of single-threaded and multi-threaded C and Rust programs. It verifies a program by symbolically executing each function in isolation, exploiting user-annotated preconditions, postconditions, and loop invariants written in a form of separation logic, and using a separation logic-based symbolic representation of memory. However, the tool itself, written in roughly 30K lines of OCaml code, has not been formally verified. Therefore, bugs in the tool could cause it to falsely report the correctness of the input program. We here report on an early result extending VeriFast to emit, upon successful verification of a Rust program, a Rocq proof script that proves correctness of the program with respect to a Rocq-encoded axiomatic semantics of Rust. This significantly enhances VeriFast's applicability in safety-critical domains. We apply hinted mirroring: we record key information from VeriFast's symbolic execution run, and use it to direct a replay of the run in Rocq.", "AI": {"tldr": "VeriFast\u5de5\u5177\u901a\u8fc7\u6269\u5c55\u652f\u6301\u751f\u6210Rocq\u8bc1\u660e\u811a\u672c\uff0c\u4e3aRust\u7a0b\u5e8f\u9a8c\u8bc1\u63d0\u4f9b\u5f62\u5f0f\u5316\u8bc1\u660e\uff0c\u589e\u5f3a\u5176\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u9002\u7528\u6027\u3002", "motivation": "VeriFast\u4f5c\u4e3a\u9886\u5148\u7684\u5f62\u5f0f\u9a8c\u8bc1\u5de5\u5177\uff0c\u5176\u672c\u8eab\u7ea63\u4e07\u884cOCaml\u4ee3\u7801\u672a\u7ecf\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u53ef\u80fd\u5b58\u5728\u5bfc\u81f4\u9519\u8bef\u9a8c\u8bc1\u7ed3\u679c\u7684bug\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\"\u63d0\u793a\u955c\u50cf\"\u65b9\u6cd5\uff1a\u8bb0\u5f55VeriFast\u7b26\u53f7\u6267\u884c\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u4fe1\u606f\uff0c\u5728Rocq\u4e2d\u91cd\u653e\u6267\u884c\u8fc7\u7a0b\uff0c\u751f\u6210Rocq\u8bc1\u660e\u811a\u672c\u6765\u8bc1\u660e\u7a0b\u5e8f\u76f8\u5bf9\u4e8eRocq\u7f16\u7801\u7684Rust\u516c\u7406\u8bed\u4e49\u7684\u6b63\u786e\u6027\u3002", "result": "\u6210\u529f\u6269\u5c55VeriFast\uff0c\u4f7f\u5176\u5728\u9a8c\u8bc1Rust\u7a0b\u5e8f\u6210\u529f\u540e\u80fd\u591f\u751f\u6210Rocq\u8bc1\u660e\u811a\u672c\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5de5\u5177\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06VeriFast\u4e0eRocq\u8bc1\u660e\u52a9\u624b\u96c6\u6210\uff0c\u4e3aRust\u7a0b\u5e8f\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u8bc1\u660e\uff0c\u89e3\u51b3\u4e86\u5de5\u5177\u672c\u8eab\u672a\u7ecf\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9a8c\u8bc1\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2601.11822", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11822", "abs": "https://arxiv.org/abs/2601.11822", "authors": ["Amna Masood", "Pratishtha Gaur", "Nuwan Jayasena"], "title": "RAPID-Serve: Resource-efficient and Accelerated P/D Intra-GPU Disaggregation", "comment": null, "summary": "Two widely adopted techniques for LLM inference serving systems today are hybrid batching and disaggregated serving. A hybrid batch combines prefill and decode tokens of different requests in the same batch to improve resource utilization and throughput at the cost of increased latency per token. In contrast, disaggregated serving decouples compute-bound prefill and bandwidth-bound decode phases to optimize for service level objectives (SLOs) at the cost of resource under-utilization and KV-cache transfer overheads. To address the limitations of these techniques, we propose RAPID-Serve: a technique to concurrently execute prefill and decode on the same GPU(s) to meet latency SLOs while maintaining high throughput and efficient resource utilization. Furthermore, we propose Adaptive Resource Management for runtime compute resource allocation, optionally leveraging CU masking (a fine-grained Compute Unit partitioning feature on AMD Instinct\\textsuperscript{TM} GPUs). RAPID-Serve provides up to 4.1x (average 1.7x) unconstrained throughput improvement and 32x and higher (average 4.9x) throughput improvement under SLO constraints, showing it as an effective strategy compared to the state-of-the-art approaches, particularly in resource-constrained environments.", "AI": {"tldr": "RAPID-Serve\uff1a\u4e00\u79cd\u5728\u76f8\u540cGPU\u4e0a\u5e76\u53d1\u6267\u884c\u9884\u586b\u5145\u548c\u89e3\u7801\u7684\u6280\u672f\uff0c\u5728\u6ee1\u8db3\u5ef6\u8fdfSLO\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u541e\u5410\u91cf\u548c\u8d44\u6e90\u5229\u7528\u7387\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u670d\u52a1\u7cfb\u7edf\u7684\u4e24\u79cd\u4e3b\u6d41\u6280\u672f\u5404\u6709\u7f3a\u9677\uff1a\u6df7\u5408\u6279\u5904\u7406\u867d\u7136\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u7387\u548c\u541e\u5410\u91cf\uff0c\u4f46\u589e\u52a0\u6bcftoken\u5ef6\u8fdf\uff1b\u89e3\u8026\u670d\u52a1\u867d\u7136\u4f18\u5316SLO\uff0c\u4f46\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u548cKV\u7f13\u5b58\u4f20\u8f93\u5f00\u9500\u3002\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u6ee1\u8db3\u5ef6\u8fdfSLO\u3001\u9ad8\u541e\u5410\u91cf\u548c\u9ad8\u6548\u8d44\u6e90\u5229\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faRAPID-Serve\u6280\u672f\uff0c\u5728\u76f8\u540cGPU\u4e0a\u5e76\u53d1\u6267\u884c\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff1b\u91c7\u7528\u81ea\u9002\u5e94\u8d44\u6e90\u7ba1\u7406\u8fdb\u884c\u8fd0\u884c\u65f6\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\uff1b\u53ef\u9009\u5229\u7528AMD Instinct\u2122 GPU\u4e0a\u7684CU\u63a9\u7801\uff08\u7ec6\u7c92\u5ea6\u8ba1\u7b97\u5355\u5143\u5206\u533a\u529f\u80fd\uff09\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u8d44\u6e90\u63a7\u5236\u3002", "result": "RAPID-Serve\u63d0\u4f9b\u9ad8\u8fbe4.1\u500d\uff08\u5e73\u57471.7\u500d\uff09\u7684\u65e0\u7ea6\u675f\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5728SLO\u7ea6\u675f\u4e0b\u63d0\u4f9b32\u500d\u53ca\u4ee5\u4e0a\uff08\u5e73\u57474.9\u500d\uff09\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "RAPID-Serve\u662f\u4e00\u79cd\u6709\u6548\u7684LLM\u63a8\u7406\u670d\u52a1\u7b56\u7565\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u5728\u6ee1\u8db3\u5ef6\u8fdfSLO\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u548c\u9ad8\u6548\u8d44\u6e90\u5229\u7528\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2601.14087", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14087", "abs": "https://arxiv.org/abs/2601.14087", "authors": ["Ruichi Han", "Yizhi Chen", "Tong Lei", "Jordi Altayo Gonzalez", "Ahmed Hemani"], "title": "'1'-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators", "comment": "Accepted for oral presentation at the 2026 VLSI Symposium on Technology, Systems and Applications (VLSI TSA) on April 13-17, 2026, at the Ambassador Hotel, Hsinchu, Taiwan", "summary": "Interconnect power consumption remains a bottleneck in Deep Neural Network (DNN) accelerators. While ordering data based on '1'-bit counts can mitigate this via reduced switching activity, practical hardware sorting implementations remain underexplored. This work proposes the hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNN). By leveraging approximate computing to group population counts into coarse-grained buckets, our design achieves hardware area reductions while preserving the link power benefits of data reordering. Our approximate sorting unit achieves up to 35.4% area reduction while maintaining 19.50\\% BT reduction compared to 20.42% of precise implementation.", "AI": {"tldr": "\u63d0\u51fa\u786c\u4ef6\u5b9e\u73b0\u7684\u514d\u6bd4\u8f83\u6392\u5e8f\u5355\u5143\uff0c\u901a\u8fc7\u8fd1\u4f3c\u8ba1\u7b97\u5c06\u4eba\u53e3\u8ba1\u6570\u5206\u7ec4\u5230\u7c97\u7c92\u5ea6\u6876\u4e2d\uff0c\u5728\u4fdd\u6301\u6570\u636e\u91cd\u6392\u5e8f\u7684\u94fe\u8def\u529f\u8017\u4f18\u52bf\u7684\u540c\u65f6\u51cf\u5c11\u786c\u4ef6\u9762\u79ef", "motivation": "\u4e92\u8fde\u529f\u8017\u662f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u867d\u7136\u57fa\u4e8e'1'\u4f4d\u8ba1\u6570\u7684\u6570\u636e\u6392\u5e8f\u53ef\u4ee5\u51cf\u5c11\u5f00\u5173\u6d3b\u52a8\u4ece\u800c\u964d\u4f4e\u529f\u8017\uff0c\u4f46\u5b9e\u7528\u7684\u786c\u4ef6\u6392\u5e8f\u5b9e\u73b0\u4ecd\u7136\u7f3a\u4e4f\u63a2\u7d22", "method": "\u8bbe\u8ba1\u9488\u5bf9\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u7684\u514d\u6bd4\u8f83\u6392\u5e8f\u5355\u5143\uff0c\u5229\u7528\u8fd1\u4f3c\u8ba1\u7b97\u5c06\u4eba\u53e3\u8ba1\u6570\u5206\u7ec4\u5230\u7c97\u7c92\u5ea6\u6876\u4e2d\uff0c\u5b9e\u73b0\u786c\u4ef6\u9762\u79ef\u51cf\u5c11\u540c\u65f6\u4fdd\u6301\u6570\u636e\u91cd\u6392\u5e8f\u7684\u94fe\u8def\u529f\u8017\u4f18\u52bf", "result": "\u8fd1\u4f3c\u6392\u5e8f\u5355\u5143\u5b9e\u73b0\u9ad8\u8fbe35.4%\u7684\u9762\u79ef\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u630119.50%\u7684BT\u51cf\u5c11\uff0c\u4e0e\u7cbe\u786e\u5b9e\u73b0\u768420.42%\u76f8\u6bd4\u6027\u80fd\u76f8\u8fd1", "conclusion": "\u63d0\u51fa\u7684\u8fd1\u4f3c\u6392\u5e8f\u5355\u5143\u5728\u4fdd\u6301\u94fe\u8def\u529f\u8017\u4f18\u52bf\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u786c\u4ef6\u9762\u79ef\uff0c\u4e3aDNN\u52a0\u901f\u5668\u4e2d\u7684\u4e92\u8fde\u529f\u8017\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.13991", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.13991", "abs": "https://arxiv.org/abs/2601.13991", "authors": ["Darion Haase", "Kevin Batz", "Adrian Gallus", "Benjamin Lucien Kaminski", "Joost-Pieter Katoen", "Lutz Klinkenberg", "Tobias Winkler"], "title": "Generating Functions Meet Occupation Measures: Invariant Synthesis for Probabilistic Loops (Extended Version)", "comment": "Full version of ESOP2026 paper 'Generating Functions Meet Occupation Measures: Invariant Synthesis for Probabilistic Loops'", "summary": "A fundamental computational task in probabilistic programming is to infer a program's output (posterior) distribution from a given initial (prior) distribution. This problem is challenging, especially for expressive languages that feature loops or unbounded recursion. While most of the existing literature focuses on statistical approximation, in this paper we address the problem of mathematically exact inference.\n  To achieve this for programs with loops, we rely on a relatively underexplored type of probabilistic loop invariant, which is linked to a loop's so-called occupation measure. The occupation measure associates program states with their expected number of visits, given the initial distribution. Based on this, we derive the notion of an occupation invariant. Such invariants are essentially dual to probabilistic martingales, the predominant technique for formal probabilistic loop analysis in the literature. A key feature of occupation invariants is that they can take the initial distribution into account and often yield a proof of positive almost sure termination as a by-product.\n  Finally, we present an automatic, template-based invariant synthesis approach for occupation invariants by encoding them as generating functions. The approach is implemented and evaluated on a set of benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5360\u7528\u4e0d\u53d8\u91cf\u7684\u7cbe\u786e\u6982\u7387\u63a8\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5e26\u5faa\u73af\u7684\u7a0b\u5e8f\uff0c\u901a\u8fc7\u81ea\u52a8\u6a21\u677f\u5408\u6210\u5b9e\u73b0", "motivation": "\u6982\u7387\u7f16\u7a0b\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u8ba1\u7b97\u4efb\u52a1\u662f\u4ece\u7ed9\u5b9a\u7684\u5148\u9a8c\u5206\u5e03\u63a8\u65ad\u7a0b\u5e8f\u7684\u540e\u9a8c\u5206\u5e03\u3002\u8fd9\u4e2a\u95ee\u9898\u5bf9\u4e8e\u5177\u6709\u5faa\u73af\u6216\u65e0\u754c\u9012\u5f52\u7684\u8868\u8fbe\u6027\u8bed\u8a00\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u6587\u732e\u5927\u591a\u5173\u6ce8\u7edf\u8ba1\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u800c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u6570\u5b66\u4e0a\u7cbe\u786e\u63a8\u7406\u7684\u95ee\u9898\u3002", "method": "\u9488\u5bf9\u5e26\u5faa\u73af\u7684\u7a0b\u5e8f\uff0c\u5f15\u5165\u4e86\u5360\u7528\u6d4b\u5ea6\u76f8\u5173\u7684\u6982\u7387\u5faa\u73af\u4e0d\u53d8\u91cf\u6982\u5ff5\uff08\u5360\u7528\u4e0d\u53d8\u91cf\uff09\u3002\u8fd9\u79cd\u4e0d\u53d8\u91cf\u4e0e\u7a0b\u5e8f\u72b6\u6001\u7684\u671f\u671b\u8bbf\u95ee\u6b21\u6570\u76f8\u5173\uff0c\u80fd\u591f\u8003\u8651\u521d\u59cb\u5206\u5e03\uff0c\u5e76\u80fd\u63a8\u5bfc\u51fa\u6b63\u51e0\u4e4e\u5fc5\u7136\u7ec8\u6b62\u6027\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u677f\u7684\u81ea\u52a8\u4e0d\u53d8\u91cf\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u51fd\u6570\u8fdb\u884c\u7f16\u7801\u3002", "result": "\u8be5\u65b9\u6cd5\u5df2\u5b9e\u73b0\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u5360\u7528\u4e0d\u53d8\u91cf\u4e3a\u6982\u7387\u5faa\u73af\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0e\u73b0\u6709\u9785\u65b9\u6cd5\u4e92\u8865\u7684\u65b0\u6280\u672f\uff0c\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u63a8\u7406\u5e76\u81ea\u52a8\u5408\u6210\u4e0d\u53d8\u91cf\u8bc1\u660e\u3002"}}
{"id": "2601.11935", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11935", "abs": "https://arxiv.org/abs/2601.11935", "authors": ["Milan Parikh", "Aniket Abhishek Soni", "Sneja Mitinbhai Shah", "Ayush Raj Jha"], "title": "Big Data Workload Profiling for Energy-Aware Cloud Resource Management", "comment": "10 pages, 3 figures. Accepted and presented at the 2026 International Conference on Data Analytics for Sustainability and Engineering Technology (DASET 2026), Track: Big Data and Machine Learning Applications", "summary": "Cloud data centers face increasing pressure to reduce operational energy consumption as big data workloads continue to grow in scale and complexity. This paper presents a workload aware and energy efficient scheduling framework that profiles CPU utilization, memory demand, and storage IO behavior to guide virtual machine placement decisions. By combining historical execution logs with real time telemetry, the proposed system predicts the energy and performance impact of candidate placements and enables adaptive consolidation while preserving service level agreement compliance. The framework is evaluated using representative Hadoop MapReduce, Spark MLlib, and ETL workloads deployed on a multi node cloud testbed. Experimental results demonstrate consistent energy savings of 15 to 20 percent compared to a baseline scheduler, with negligible performance degradation. These findings highlight workload profiling as a practical and scalable strategy for improving the sustainability of cloud based big data processing environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7684\u8282\u80fd\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790CPU\u3001\u5185\u5b58\u548c\u5b58\u50a8IO\u884c\u4e3a\u6765\u4f18\u5316\u865a\u62df\u673a\u653e\u7f6e\u51b3\u7b56\uff0c\u5728\u4fdd\u6301SLA\u7684\u540c\u65f6\u5b9e\u73b015-20%\u7684\u80fd\u8017\u8282\u7701\u3002", "motivation": "\u968f\u7740\u5927\u6570\u636e\u5de5\u4f5c\u8d1f\u8f7d\u89c4\u6a21\u548c\u590d\u6742\u6027\u7684\u589e\u957f\uff0c\u4e91\u6570\u636e\u4e2d\u5fc3\u9762\u4e34\u964d\u4f4e\u8fd0\u8425\u80fd\u8017\u7684\u538b\u529b\u3002\u4f20\u7edf\u8c03\u5ea6\u5668\u7f3a\u4e4f\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5bfc\u81f4\u80fd\u6e90\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7684\u8282\u80fd\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790CPU\u5229\u7528\u7387\u3001\u5185\u5b58\u9700\u6c42\u548c\u5b58\u50a8IO\u884c\u4e3a\u6765\u6307\u5bfc\u865a\u62df\u673a\u653e\u7f6e\u51b3\u7b56\u3002\u7ed3\u5408\u5386\u53f2\u6267\u884c\u65e5\u5fd7\u548c\u5b9e\u65f6\u9065\u6d4b\u6570\u636e\uff0c\u9884\u6d4b\u5019\u9009\u653e\u7f6e\u65b9\u6848\u7684\u80fd\u8017\u548c\u6027\u80fd\u5f71\u54cd\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u6574\u5408\u540c\u65f6\u4fdd\u6301SLA\u5408\u89c4\u6027\u3002", "result": "\u5728\u5305\u542bHadoop MapReduce\u3001Spark MLlib\u548cETL\u5de5\u4f5c\u8d1f\u8f7d\u7684\u591a\u8282\u70b9\u4e91\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u76f8\u6bd4\u57fa\u7ebf\u8c03\u5ea6\u5668\u5b9e\u73b0\u4e8615-20%\u7684\u6301\u7eed\u80fd\u8017\u8282\u7701\uff0c\u6027\u80fd\u4e0b\u964d\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u5de5\u4f5c\u8d1f\u8f7d\u5206\u6790\u662f\u63d0\u9ad8\u57fa\u4e8e\u4e91\u7684\u5927\u6570\u636e\u5904\u7406\u73af\u5883\u53ef\u6301\u7eed\u6027\u7684\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7b56\u7565\uff0c\u80fd\u591f\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u964d\u4f4e\u80fd\u8017\u3002"}}
{"id": "2601.14140", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14140", "abs": "https://arxiv.org/abs/2601.14140", "authors": ["Tong Xie", "Yijiahao Qi", "Jinqi Wen", "Zishen Wan", "Yanchi Dong", "Zihao Wang", "Shaofei Cai", "Yitao Liang", "Tianyu Jia", "Yuan Wang", "Runsheng Wang", "Meng Li"], "title": "CREATE: Cross-Layer Resilience Characterization and Optimization for Efficient yet Reliable Embodied AI Systems", "comment": "18 pages, 21 figures. Accepted by ASPLOS 2026", "summary": "Embodied Artificial Intelligence (AI) has recently attracted significant attention as it bridges AI with the physical world. Modern embodied AI systems often combine a Large Language Model (LLM)-based planner for high-level task planning and a reinforcement learning (RL)-based controller for low-level action generation, enabling embodied agents to tackle complex tasks in real-world environments. However, deploying embodied agents remains challenging due to their high computation requirements, especially for battery-powered local devices. Although techniques like lowering operating voltage can improve energy efficiency, they can introduce bit errors and result in task failures. In this work, we propose CREATE, a general design principle that leverages heterogeneous resilience at different layers for synergistic energy-reliability co-optimization. For the first time, we conduct a comprehensive error injection study on modern embodied AI systems and observe an inherent but heterogeneous fault tolerance. Building upon these insights, we develop an anomaly detection and clearance mechanism at the circuit level to eliminate outlier errors. At the model level, we propose a weight-rotation-enhanced planning algorithm to improve the fault tolerance of the LLM-based planner. Furthermore, we introduce an application-level technique, autonomy-adaptive voltage scaling, to dynamically adjust the operating voltage of the controllers. The voltage scaling circuit is co-designed to enable online voltage adjustment. Extensive experiments demonstrate that without compromising task quality, CREATE achieves 40.6% computational energy savings on average over nominal-voltage baselines and 35.0% over prior-art techniques. This further leads to 29.5% to 37.3% chip-level energy savings and approximately a 15% to 30% improvement in battery life.", "AI": {"tldr": "CREATE\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u5f39\u6027\u8bbe\u8ba1\u539f\u5219\uff0c\u901a\u8fc7\u7535\u8def\u5c42\u3001\u6a21\u578b\u5c42\u548c\u5e94\u7528\u5c42\u7684\u534f\u540c\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5177\u8eabAI\u7cfb\u7edf\u7684\u80fd\u8017", "motivation": "\u90e8\u7f72\u5177\u8eabAI\u7cfb\u7edf\u9762\u4e34\u9ad8\u8ba1\u7b97\u9700\u6c42\u548c\u80fd\u8017\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7535\u6c60\u4f9b\u7535\u8bbe\u5907\u3002\u867d\u7136\u964d\u4f4e\u5de5\u4f5c\u7535\u538b\u53ef\u4ee5\u63d0\u9ad8\u80fd\u6548\uff0c\u4f46\u4f1a\u5f15\u5165\u6bd4\u7279\u9519\u8bef\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\uff0c\u9700\u8981\u5728\u80fd\u6548\u548c\u53ef\u9760\u6027\u4e4b\u95f4\u8fdb\u884c\u6743\u8861", "method": "1) \u7535\u8def\u5c42\uff1a\u5f02\u5e38\u68c0\u6d4b\u548c\u6e05\u9664\u673a\u5236\u6d88\u9664\u5f02\u5e38\u9519\u8bef\uff1b2) \u6a21\u578b\u5c42\uff1a\u6743\u91cd\u65cb\u8f6c\u589e\u5f3a\u89c4\u5212\u7b97\u6cd5\u63d0\u9ad8LLM\u89c4\u5212\u5668\u7684\u5bb9\u9519\u80fd\u529b\uff1b3) \u5e94\u7528\u5c42\uff1a\u81ea\u4e3b\u9002\u5e94\u6027\u7535\u538b\u7f29\u653e\u52a8\u6001\u8c03\u6574\u63a7\u5236\u5668\u5de5\u4f5c\u7535\u538b\uff1b4) \u534f\u540c\u8bbe\u8ba1\u7535\u538b\u7f29\u653e\u7535\u8def\u652f\u6301\u5728\u7ebf\u7535\u538b\u8c03\u6574", "result": "CREATE\u5728\u4e0d\u5f71\u54cd\u4efb\u52a1\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u6807\u79f0\u7535\u538b\u57fa\u7ebf\u5e73\u5747\u8282\u770140.6%\u8ba1\u7b97\u80fd\u8017\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u8282\u770135.0%\u3002\u82af\u7247\u7ea7\u80fd\u8017\u8282\u770129.5%-37.3%\uff0c\u7535\u6c60\u5bff\u547d\u63d0\u5347\u7ea615%-30%", "conclusion": "CREATE\u901a\u8fc7\u5229\u7528\u4e0d\u540c\u5c42\u7684\u5f02\u6784\u5f39\u6027\u8fdb\u884c\u534f\u540c\u80fd\u91cf-\u53ef\u9760\u6027\u4f18\u5316\uff0c\u4e3a\u5177\u8eabAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u80fd\u6548\u63d0\u5347\u65b9\u6848\uff0c\u663e\u8457\u5ef6\u957f\u4e86\u7535\u6c60\u4f9b\u7535\u8bbe\u5907\u7684\u8fd0\u884c\u65f6\u95f4"}}
{"id": "2601.14059", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.14059", "abs": "https://arxiv.org/abs/2601.14059", "authors": ["Andrea Gilot", "Axel Bergstr\u00f6m", "Eva Darulova"], "title": "Verifying Floating-Point Programs in Stainless", "comment": null, "summary": "We extend the Stainless deductive verifier with floating-point support, providing the first automated verification support for floating-point numbers for a subset of Scala that includes polymorphism, recursion and higher-order functions. We follow the recent approach in the KeY verifier to axiomatise reasoning about mathematical functions, but go further by supporting all functions from Scala's math API, and by verifying the correctness of the axioms against the actual implementation in Stainless itself. We validate Stainless' floating-point support on a new set of benchmarks sampled from real-world code from GitHub, showing that it can verify specifications about, e.g., ranges of output or absence of special values for most supported functions, or produce counter-examples when the specifications do not hold.", "AI": {"tldr": "Stainless\u9a8c\u8bc1\u5668\u6269\u5c55\u652f\u6301\u6d6e\u70b9\u6570\uff0c\u6210\u4e3a\u9996\u4e2a\u652f\u6301Scala\u5b50\u96c6\uff08\u542b\u591a\u6001\u3001\u9012\u5f52\u3001\u9ad8\u9636\u51fd\u6570\uff09\u7684\u81ea\u52a8\u5316\u6d6e\u70b9\u9a8c\u8bc1\u5de5\u5177", "motivation": "\u73b0\u6709\u9a8c\u8bc1\u5de5\u5177\u7f3a\u4e4f\u5bf9\u6d6e\u70b9\u6570\u7684\u81ea\u52a8\u5316\u9a8c\u8bc1\u652f\u6301\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5305\u542b\u591a\u6001\u3001\u9012\u5f52\u548c\u9ad8\u9636\u51fd\u6570\u7684Scala\u8bed\u8a00\u5b50\u96c6", "method": "\u91c7\u7528KeY\u9a8c\u8bc1\u5668\u7684\u6570\u5b66\u51fd\u6570\u516c\u7406\u5316\u65b9\u6cd5\uff0c\u6269\u5c55\u652f\u6301Scala math API\u7684\u6240\u6709\u51fd\u6570\uff0c\u5e76\u5728Stainless\u4e2d\u9a8c\u8bc1\u516c\u7406\u7684\u6b63\u786e\u6027", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86\u4eceGitHub\u771f\u5b9e\u4ee3\u7801\u91c7\u6837\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u80fd\u591f\u9a8c\u8bc1\u8f93\u51fa\u8303\u56f4\u3001\u7279\u6b8a\u503c\u4e0d\u5b58\u5728\u7b49\u89c4\u8303\uff0c\u6216\u5728\u89c4\u8303\u4e0d\u6210\u7acb\u65f6\u751f\u6210\u53cd\u4f8b", "conclusion": "Stainless\u6210\u4e3a\u9996\u4e2a\u652f\u6301\u6d6e\u70b9\u6570\u81ea\u52a8\u5316\u9a8c\u8bc1\u7684Scala\u9a8c\u8bc1\u5de5\u5177\uff0c\u4e3a\u5b9e\u9645\u4ee3\u7801\u4e2d\u7684\u6d6e\u70b9\u8fd0\u7b97\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12209", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12209", "abs": "https://arxiv.org/abs/2601.12209", "authors": ["Sana Taghipour Anvari", "Julian Samaroo", "Matin Raayai Ardakani", "David Kaeli"], "title": "DaggerFFT: A Distributed FFT Framework Using Task Scheduling in Julia", "comment": null, "summary": "The Fast Fourier Transform (FFT) is a fundamental numerical technique with widespread application in a range of scientific problems. As scientific simulations attempt to exploit exascale systems, there has been a growing demand for distributed FFT algorithms that can effectively utilize modern heterogeneous high-performance computing (HPC) systems. Conventional FFT algorithms commonly encounter performance bottlenecks, especially when run on heterogeneous platforms. Most distributed FFT approaches rely on static task distribution and require synchronization barriers, limiting scalability and impacting overall resource utilization. In this paper we present DaggerFFT, a distributed FFT framework, developed in Julia, that treats highly parallel FFT computations as a dynamically scheduled task graph. Each FFT stage operates on a separately defined distributed array. FFT operations are expressed as DTasks operating on pencil or slab partitioned DArrays. Each FFT stage owns its own DArray, and the runtime assigns DTasks across devices using Dagger's dynamic scheduler that uses work stealing. We demonstrate how DaggerFFT's dynamic scheduler can outperform state-of-the-art distributed FFT libraries on both CPU and GPU backends, achieving up to a 2.6x speedup on CPU clusters and up to a 1.35x speedup on GPU clusters. We have integrated DaggerFFT into Oceananigans.jl, a geophysical fluid dynamics framework, demonstrating that high-level, task-based runtimes can deliver both superior performance and modularity in large-scale, real-world simulations.", "AI": {"tldr": "DaggerFFT\uff1a\u57fa\u4e8eJulia\u7684\u52a8\u6001\u4efb\u52a1\u8c03\u5ea6\u5206\u5e03\u5f0fFFT\u6846\u67b6\uff0c\u5728CPU\u548cGPU\u96c6\u7fa4\u4e0a\u5206\u522b\u5b9e\u73b02.6\u500d\u548c1.35\u500d\u52a0\u901f", "motivation": "\u968f\u7740\u79d1\u5b66\u6a21\u62df\u5411\u767e\u4ebf\u4ebf\u6b21\u8ba1\u7b97\u7cfb\u7edf\u53d1\u5c55\uff0c\u9700\u8981\u80fd\u6709\u6548\u5229\u7528\u73b0\u4ee3\u5f02\u6784\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7684\u5206\u5e03\u5f0fFFT\u7b97\u6cd5\u3002\u4f20\u7edfFFT\u7b97\u6cd5\u5728\u5f02\u6784\u5e73\u53f0\u4e0a\u5e38\u9047\u5230\u6027\u80fd\u74f6\u9888\uff0c\u73b0\u6709\u5206\u5e03\u5f0fFFT\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u4efb\u52a1\u5206\u914d\u548c\u540c\u6b65\u5c4f\u969c\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "method": "\u63d0\u51faDaggerFFT\u6846\u67b6\uff0c\u5c06\u9ad8\u5ea6\u5e76\u884c\u7684FFT\u8ba1\u7b97\u89c6\u4e3a\u52a8\u6001\u8c03\u5ea6\u7684\u4efb\u52a1\u56fe\u3002\u6bcf\u4e2aFFT\u9636\u6bb5\u5728\u5355\u72ec\u5b9a\u4e49\u7684\u5206\u5e03\u5f0f\u6570\u7ec4\u4e0a\u64cd\u4f5c\uff0cFFT\u64cd\u4f5c\u8868\u793a\u4e3a\u5728\u94c5\u7b14\u6216\u5e73\u677f\u5206\u533aDArray\u4e0a\u8fd0\u884c\u7684DTasks\u3002\u6bcf\u4e2aFFT\u9636\u6bb5\u62e5\u6709\u81ea\u5df1\u7684DArray\uff0c\u8fd0\u884c\u65f6\u4f7f\u7528Dagger\u7684\u52a8\u6001\u8c03\u5ea6\u5668\uff08\u652f\u6301\u5de5\u4f5c\u7a83\u53d6\uff09\u5728\u8bbe\u5907\u95f4\u5206\u914d\u4efb\u52a1\u3002", "result": "DaggerFFT\u7684\u52a8\u6001\u8c03\u5ea6\u5668\u5728CPU\u548cGPU\u540e\u7aef\u4e0a\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5206\u5e03\u5f0fFFT\u5e93\uff0c\u5728CPU\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u9ad8\u8fbe2.6\u500d\u52a0\u901f\uff0c\u5728GPU\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u9ad8\u8fbe1.35\u500d\u52a0\u901f\u3002\u5df2\u6210\u529f\u96c6\u6210\u5230\u5730\u7403\u7269\u7406\u6d41\u4f53\u52a8\u529b\u5b66\u6846\u67b6Oceananigans.jl\u4e2d\u3002", "conclusion": "\u57fa\u4e8e\u4efb\u52a1\u7684\u8fd0\u884c\u65f6\u7cfb\u7edf\uff08\u5982DaggerFFT\uff09\u80fd\u591f\u5728\u5927\u89c4\u6a21\u5b9e\u9645\u6a21\u62df\u4e2d\u540c\u65f6\u63d0\u4f9b\u5353\u8d8a\u6027\u80fd\u548c\u6a21\u5757\u5316\uff0c\u8bc1\u660e\u4e86\u9ad8\u7ea7\u4efb\u52a1\u8c03\u5ea6\u65b9\u6cd5\u5728\u5f02\u6784\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.14148", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14148", "abs": "https://arxiv.org/abs/2601.14148", "authors": ["Meng Li", "Tong Xie", "Zuodong Zhang", "Runsheng Wang"], "title": "The Quest for Reliable AI Accelerators: Cross-Layer Evaluation and Design Optimization", "comment": "4 pages, 9 figures. Invited paper at ASICON 2025", "summary": "As the CMOS technology pushes to the nanoscale, aging effects and process variations have become increasingly pronounced, posing significant reliability challenges for AI accelerators. Traditional guardband-based design approaches, which rely on pessimistic timing margin, sacrifice significant performance and computational efficiency, rendering them inadequate for high-performance AI computing demands. Current reliability-aware AI accelerator design faces two core challenges: (1) the lack of systematic cross-layer analysis tools to capture coupling reliability effects across device, circuit, architecture, and application layers; and (2) the fundamental trade-off between conventional reliability optimization and computational efficiency. To address these challenges, this paper systematically presents a series of reliability-aware accelerator designs, encompassing (1) aging and variation-aware dynamic timing analyzer, (2) accelerator dataflow optimization using critical input pattern reduction, and (3) resilience characterization and novel architecture design for large language models (LLMs). By tightly integrating cross-layer reliability modeling and AI workload characteristics, these co-optimization approaches effectively achieve reliable and efficient AI acceleration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8de8\u5c42\u53ef\u9760\u6027\u611f\u77e5AI\u52a0\u901f\u5668\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u89e3\u51b3\u7eb3\u7c73CMOS\u5de5\u827a\u4e0b\u8001\u5316\u6548\u5e94\u548c\u5de5\u827a\u53d8\u5f02\u5e26\u6765\u7684\u53ef\u9760\u6027\u6311\u6218\uff0c\u901a\u8fc7\u52a8\u6001\u65f6\u5e8f\u5206\u6790\u3001\u6570\u636e\u6d41\u4f18\u5316\u548cLLM\u67b6\u6784\u8bbe\u8ba1\u5b9e\u73b0\u53ef\u9760\u9ad8\u6548AI\u52a0\u901f\u3002", "motivation": "\u968f\u7740CMOS\u6280\u672f\u8fdb\u5165\u7eb3\u7c73\u5c3a\u5ea6\uff0c\u8001\u5316\u6548\u5e94\u548c\u5de5\u827a\u53d8\u5f02\u65e5\u76ca\u663e\u8457\uff0c\u5bf9AI\u52a0\u901f\u5668\u7684\u53ef\u9760\u6027\u6784\u6210\u91cd\u5927\u6311\u6218\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u4fdd\u62a4\u5e26\u7684\u8bbe\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u60b2\u89c2\u65f6\u5e8f\u88d5\u5ea6\uff0c\u727a\u7272\u4e86\u5927\u91cf\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u6027\u80fdAI\u8ba1\u7b97\u9700\u6c42\u3002\u5f53\u524d\u53ef\u9760\u6027\u611f\u77e5AI\u52a0\u901f\u5668\u8bbe\u8ba1\u9762\u4e34\u4e24\u5927\u6838\u5fc3\u6311\u6218\uff1a\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u8de8\u5c42\u5206\u6790\u5de5\u5177\u6765\u6355\u6349\u5668\u4ef6\u3001\u7535\u8def\u3001\u67b6\u6784\u548c\u5e94\u7528\u5c42\u4e4b\u95f4\u7684\u8026\u5408\u53ef\u9760\u6027\u6548\u5e94\uff1b\u4ee5\u53ca\u4f20\u7edf\u53ef\u9760\u6027\u4f18\u5316\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6839\u672c\u6027\u6743\u8861\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u6027\u5730\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u53ef\u9760\u6027\u611f\u77e5\u52a0\u901f\u5668\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a(1) \u8001\u5316\u548c\u53d8\u5f02\u611f\u77e5\u7684\u52a8\u6001\u65f6\u5e8f\u5206\u6790\u5668\uff1b(2) \u4f7f\u7528\u5173\u952e\u8f93\u5165\u6a21\u5f0f\u51cf\u5c11\u7684\u52a0\u901f\u5668\u6570\u636e\u6d41\u4f18\u5316\uff1b(3) \u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u5f39\u6027\u7279\u5f81\u5206\u6790\u548c\u65b0\u578b\u67b6\u6784\u8bbe\u8ba1\u3002\u901a\u8fc7\u7d27\u5bc6\u96c6\u6210\u8de8\u5c42\u53ef\u9760\u6027\u5efa\u6a21\u548cAI\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\uff0c\u8fd9\u4e9b\u534f\u540c\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e86\u53ef\u9760\u9ad8\u6548\u7684AI\u52a0\u901f\u3002", "result": "\u901a\u8fc7\u63d0\u51fa\u7684\u8de8\u5c42\u534f\u540c\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7eb3\u7c73CMOS\u5de5\u827a\u4e0b\u7684\u53ef\u9760\u6027\u6311\u6218\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347AI\u52a0\u901f\u5668\u7684\u53ef\u9760\u6027\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u4fdd\u62a4\u5e26\u8bbe\u8ba1\u65b9\u6cd5\u7684\u6027\u80fd\u635f\u5931\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u53ef\u9760\u6027\u611f\u77e5AI\u52a0\u901f\u5668\u8bbe\u8ba1\u65b9\u6cd5\u901a\u8fc7\u8de8\u5c42\u534f\u540c\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7eb3\u7c73CMOS\u5de5\u827a\u4e0b\u7684\u53ef\u9760\u6027\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u53ef\u9760\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\uff0c\u4e3a\u9ad8\u6027\u80fdAI\u8ba1\u7b97\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14114", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.14114", "abs": "https://arxiv.org/abs/2601.14114", "authors": ["Liam Chung", "Tobias Kapp\u00e9"], "title": "Partial Reductions for Kleene Algebra with Linear Hypotheses", "comment": null, "summary": "Kleene algebra (KA) is an important tool for reasoning about general program equivalences, with a decidable and complete equational theory. However, KA cannot always prove equivalences between specific programs. For this purpose, one adds hypotheses to KA that encode program-specific knowledge. Traditionally, a map on regular expressions called a reduction then lets us lift decidability and completeness to these more expressive systems. Explicitly constructing such a reduction requires significant labour. Moreover, due to regularity constraints, a reduction may not exist for all combinations of expression and hypothesis.\n  We describe an automaton-based construction to mechanically derive reductions for a wide class of hypotheses. These reductions can be partial, in which case they yield partial completeness: completeness for expressions in their domain. This allows us to automatically establish the provability of more equivalences than what is covered in existing work.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u673a\u7684\u6784\u9020\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u63a8\u5bfcKleene\u4ee3\u6570\u4e2d\u5e7f\u6cdb\u5047\u8bbe\u7c7b\u7684\u7ea6\u7b80\uff0c\u652f\u6301\u90e8\u5206\u7ea6\u7b80\u4ee5\u5b9e\u73b0\u90e8\u5206\u5b8c\u5907\u6027\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u5de5\u4f5c\u7684\u53ef\u8bc1\u660e\u7b49\u4ef7\u8303\u56f4\u3002", "motivation": "Kleene\u4ee3\u6570\uff08KA\uff09\u867d\u7136\u5177\u6709\u53ef\u5224\u5b9a\u4e14\u5b8c\u5907\u7684\u7b49\u5f0f\u7406\u8bba\uff0c\u4f46\u65e0\u6cd5\u8bc1\u660e\u7279\u5b9a\u7a0b\u5e8f\u95f4\u7684\u6240\u6709\u7b49\u4ef7\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u624b\u52a8\u6784\u9020\u7ea6\u7b80\u6620\u5c04\uff0c\u5de5\u4f5c\u91cf\u5927\u4e14\u53d7\u6b63\u5219\u6027\u7ea6\u675f\uff0c\u67d0\u4e9b\u8868\u8fbe\u5f0f\u548c\u5047\u8bbe\u7ec4\u5408\u53ef\u80fd\u4e0d\u5b58\u5728\u7ea6\u7b80\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u52a8\u673a\u7684\u6784\u9020\u65b9\u6cd5\uff0c\u80fd\u591f\u673a\u68b0\u5730\u63a8\u5bfc\u51fa\u5e7f\u6cdb\u5047\u8bbe\u7c7b\u7684\u7ea6\u7b80\u3002\u8fd9\u4e9b\u7ea6\u7b80\u53ef\u4ee5\u662f\u90e8\u5206\u7684\uff0c\u5f53\u7ea6\u7b80\u4e3a\u90e8\u5206\u65f6\uff0c\u5b83\u4eec\u5728\u5176\u5b9a\u4e49\u57df\u5185\u63d0\u4f9b\u90e8\u5206\u5b8c\u5907\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u5efa\u7acb\u6bd4\u73b0\u6709\u5de5\u4f5c\u8986\u76d6\u8303\u56f4\u66f4\u5e7f\u7684\u7b49\u4ef7\u6027\u7684\u53ef\u8bc1\u660e\u6027\uff0c\u901a\u8fc7\u90e8\u5206\u7ea6\u7b80\u5b9e\u73b0\u4e86\u90e8\u5206\u5b8c\u5907\u6027\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u6b63\u5219\u6027\u7ea6\u675f\u9650\u5236\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u673a\u6784\u9020\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u63a8\u5bfcKleene\u4ee3\u6570\u4e2d\u5e7f\u6cdb\u5047\u8bbe\u7c7b\u7684\u7ea6\u7b80\uff0c\u652f\u6301\u90e8\u5206\u7ea6\u7b80\u4ee5\u5b9e\u73b0\u90e8\u5206\u5b8c\u5907\u6027\uff0c\u663e\u8457\u6269\u5c55\u4e86\u53ef\u81ea\u52a8\u8bc1\u660e\u7684\u7a0b\u5e8f\u7b49\u4ef7\u6027\u8303\u56f4\uff0c\u51cf\u5c11\u4e86\u624b\u52a8\u6784\u9020\u7684\u5de5\u4f5c\u91cf\u3002"}}
{"id": "2601.12241", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12241", "abs": "https://arxiv.org/abs/2601.12241", "authors": ["Yiwei Jiang", "Sangeeta Chowdhary", "Nathaniel Morris", "Rutwik Jain", "Srilatha Manne", "Sam Bayliss"], "title": "Power Aware Dynamic Reallocation For Inference", "comment": null, "summary": "Disaggregation has emerged as a powerful strategy for optimizing large language model (LLM) inference by separating compute-intensive prefill and memory-bound decode phases across specialized GPUs. This separation improves utilization and throughput under fixed hardware capacity. However, as model and cluster scales grow, power, rather than compute, has become the dominant limiter of overall performance and cost efficiency. In this paper, we propose RAPID, a power-aware disaggregated inference framework that jointly manages GPU roles and power budgets to sustain goodput within strict power caps. RAPID utilizes static and dynamic power reallocation in addition to GPU reallocation to improve performance under fixed power bounds. RAPID improves overall performance and application consistency beyond what is achievable in current disaggregation solutions, resulting in up to a 2x improvement in SLO attainment at peak load when compared to a static assignment without an increase in complexity or cost.", "AI": {"tldr": "RAPID\u662f\u4e00\u4e2a\u9762\u5411\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u529f\u7387\u611f\u77e5\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u7ba1\u7406GPU\u89d2\u8272\u548c\u529f\u7387\u9884\u7b97\uff0c\u5728\u4e25\u683c\u529f\u7387\u9650\u5236\u4e0b\u63d0\u5347\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u548c\u96c6\u7fa4\u89c4\u6a21\u6269\u5927\uff0c\u529f\u7387\u800c\u975e\u8ba1\u7b97\u80fd\u529b\u5df2\u6210\u4e3a\u9650\u5236\u6574\u4f53\u6027\u80fd\u548c\u6210\u672c\u6548\u7387\u7684\u4e3b\u8981\u56e0\u7d20\u3002\u73b0\u6709\u89e3\u8026\u65b9\u6848\u867d\u80fd\u63d0\u5347\u786c\u4ef6\u5229\u7528\u7387\uff0c\u4f46\u672a\u5145\u5206\u8003\u8651\u529f\u7387\u7ea6\u675f\u4e0b\u7684\u4f18\u5316\u3002", "method": "RAPID\u91c7\u7528\u9759\u6001\u548c\u52a8\u6001\u529f\u7387\u91cd\u5206\u914d\u4ee5\u53caGPU\u91cd\u5206\u914d\u7b56\u7565\uff0c\u5728\u56fa\u5b9a\u529f\u7387\u9650\u5236\u4e0b\u8054\u5408\u4f18\u5316GPU\u89d2\u8272\u5206\u914d\u548c\u529f\u7387\u9884\u7b97\u7ba1\u7406\u3002", "result": "\u76f8\u6bd4\u9759\u6001\u5206\u914d\u65b9\u6848\uff0cRAPID\u5728\u5cf0\u503c\u8d1f\u8f7d\u4e0b\u5c06SLO\u8fbe\u6210\u7387\u63d0\u5347\u9ad8\u8fbe2\u500d\uff0c\u4e14\u4e0d\u589e\u52a0\u590d\u6742\u5ea6\u6216\u6210\u672c\u3002", "conclusion": "\u529f\u7387\u611f\u77e5\u7684\u89e3\u8026\u63a8\u7406\u6846\u67b6RAPID\u80fd\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5728\u529f\u7387\u7ea6\u675f\u4e0b\u7684\u6027\u80fd\u548c\u4e00\u81f4\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u90e8\u7f72\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13040", "categories": ["cs.DC", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.13040", "abs": "https://arxiv.org/abs/2601.13040", "authors": ["Harry Fitchett", "Charles Fox"], "title": "CPU-less parallel execution of lambda calculus in digital logic", "comment": null, "summary": "While transistor density is still increasing, clock speeds are not, motivating the search for new parallel architectures. One approach is to completely abandon the concept of CPU -- and thus serial imperative programming -- and instead to specify and execute tasks in parallel, compiling from programming languages to data flow digital logic. It is well-known that pure functional languages are inherently parallel, due to the Church-Rosser theorem, and CPU-based parallel compilers exist for many functional languages. However, these still rely on conventional CPUs and their von Neumann bottlenecks. An alternative is to compile functional languages directly into digital logic to maximize available parallelism. It is difficult to work with complete modern functional languages due to their many features, so we demonstrate a proof-of-concept system using lambda calculus as the source language and compiling to digital logic. We show how functional hardware can be tailored to a simplistic functional language, forming the ground for a new model of CPU-less functional computation. At the algorithmic level, we use a tree-based representation, with data localized within nodes and communicated data passed between them. This is implemented by physical digital logic blocks corresponding to nodes, and buses enabling message passing. Node types and behaviors correspond to lambda grammar forms, and beta-reductions are performed in parallel allowing branches independent from one another to perform transformations simultaneously. As evidence for this approach, we present an implementation, along with simulation results, showcasing successful execution of lambda expressions. This suggests that the approach could be scaled to larger functional languages. Successful execution of a test suite of lambda expressions suggests that the approach could be scaled to larger functional languages.", "AI": {"tldr": "\u5c06\u7eaf\u51fd\u6570\u5f0f\u8bed\u8a00\uff08\u03bb\u6f14\u7b97\uff09\u76f4\u63a5\u7f16\u8bd1\u4e3a\u6570\u5b57\u903b\u8f91\u7535\u8def\uff0c\u7ed5\u8fc7\u4f20\u7edfCPU\u548c\u51af\u00b7\u8bfa\u4f9d\u66fc\u74f6\u9888\uff0c\u5b9e\u73b0\u5e76\u884c\u8ba1\u7b97\u7684\u65b0\u67b6\u6784\u3002", "motivation": "\u6676\u4f53\u7ba1\u5bc6\u5ea6\u6301\u7eed\u589e\u52a0\u4f46\u65f6\u949f\u901f\u5ea6\u505c\u6ede\uff0c\u9700\u8981\u5bfb\u627e\u65b0\u7684\u5e76\u884c\u67b6\u6784\u3002\u4f20\u7edfCPU\u5e76\u884c\u7f16\u8bd1\u4ecd\u53d7\u51af\u00b7\u8bfa\u4f9d\u66fc\u74f6\u9888\u9650\u5236\uff0c\u800c\u7eaf\u51fd\u6570\u5f0f\u8bed\u8a00\u5929\u751f\u5177\u6709\u5e76\u884c\u6027\uff0c\u76f4\u63a5\u7f16\u8bd1\u5230\u786c\u4ef6\u53ef\u6700\u5927\u5316\u5229\u7528\u5e76\u884c\u6027\u3002", "method": "\u4f7f\u7528\u03bb\u6f14\u7b97\u4f5c\u4e3a\u6e90\u8bed\u8a00\uff0c\u91c7\u7528\u6811\u5f62\u8868\u793a\u6cd5\uff0c\u8282\u70b9\u5bf9\u5e94\u03bb\u8bed\u6cd5\u5f62\u5f0f\uff0c\u6570\u636e\u5c40\u90e8\u5316\u5b58\u50a8\uff0c\u901a\u8fc7\u603b\u7ebf\u4f20\u9012\u6d88\u606f\u3002\u8282\u70b9\u7c7b\u578b\u548c\u884c\u4e3a\u5bf9\u5e94\u03bb\u8bed\u6cd5\u5f62\u5f0f\uff0c\u03b2\u5f52\u7ea6\u5e76\u884c\u6267\u884c\uff0c\u72ec\u7acb\u5206\u652f\u53ef\u540c\u65f6\u8fdb\u884c\u53d8\u6362\u3002", "result": "\u5b9e\u73b0\u4e86\u6982\u5ff5\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u901a\u8fc7\u4eff\u771f\u5c55\u793a\u4e86\u03bb\u8868\u8fbe\u5f0f\u7684\u6210\u529f\u6267\u884c\u3002\u6d4b\u8bd5\u5957\u4ef6\u7684\u6210\u529f\u6267\u884c\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u66f4\u5927\u7684\u51fd\u6570\u5f0f\u8bed\u8a00\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0CPU\u7684\u51fd\u6570\u5f0f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u6a21\u578b\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u5c06\u51fd\u6570\u5f0f\u8bed\u8a00\u76f4\u63a5\u7f16\u8bd1\u5230\u6570\u5b57\u903b\u8f91\u7684\u53ef\u884c\u6027\uff0c\u6709\u671b\u6269\u5c55\u5230\u66f4\u5b8c\u6574\u7684\u73b0\u4ee3\u51fd\u6570\u5f0f\u8bed\u8a00\u3002"}}
{"id": "2601.12266", "categories": ["cs.DC", "cs.NI", "cs.PF", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.12266", "abs": "https://arxiv.org/abs/2601.12266", "authors": ["Neelkamal Bhuyan", "Randeep Bhatia", "Murali Kodialam", "TV Lakshman"], "title": "Opportunistic Scheduling for Optimal Spot Instance Savings in the Cloud", "comment": "Accepted for publication in the 45th IEEE International Conference on Computer Communications (INFOCOM 2026). Copyright 2026 IEEE", "summary": "We study the problem of scheduling delay-sensitive jobs over spot and on-demand cloud instances to minimize average cost while meeting an average delay constraint. Jobs arrive as a general stochastic process, and incur different costs based on the instance type. This work provides the first analytical treatment of this problem using tools from queuing theory, stochastic processes, and optimization. We derive cost expressions for general policies, prove queue length one is optimal for low target delays, and characterize the optimal wait-time distribution. For high target delays, we identify a knapsack structure and design a scheduling policy that exploits it. An adaptive algorithm is proposed to fully utilize the allowed delay, and empirical results confirm its near-optimality.", "AI": {"tldr": "\u7814\u7a76\u5728\u6ee1\u8db3\u5e73\u5747\u5ef6\u8fdf\u7ea6\u675f\u4e0b\uff0c\u4f7f\u7528\u7ade\u4ef7\u578b\u548c\u6309\u9700\u578b\u4e91\u5b9e\u4f8b\u8c03\u5ea6\u5ef6\u8fdf\u654f\u611f\u4f5c\u4e1a\u4ee5\u6700\u5c0f\u5316\u5e73\u5747\u6210\u672c\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6392\u961f\u8bba\u548c\u4f18\u5316\u7684\u5206\u6790\u65b9\u6cd5\u53ca\u81ea\u9002\u5e94\u7b97\u6cd5\u3002", "motivation": "\u4e91\u73af\u5883\u4e2d\u5ef6\u8fdf\u654f\u611f\u4f5c\u4e1a\u7684\u8c03\u5ea6\u9700\u8981\u5728\u6210\u672c\uff08\u7ade\u4ef7\u5b9e\u4f8b\u4fbf\u5b9c\u4f46\u4e0d\u7a33\u5b9a\uff09\u548c\u5ef6\u8fdf\uff08\u6309\u9700\u5b9e\u4f8b\u53ef\u9760\u4f46\u6602\u8d35\uff09\u4e4b\u95f4\u6743\u8861\uff0c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u6b64\u95ee\u9898\u7684\u7cfb\u7edf\u6027\u7406\u8bba\u5206\u6790\u3002", "method": "\u4f7f\u7528\u6392\u961f\u8bba\u3001\u968f\u673a\u8fc7\u7a0b\u548c\u4f18\u5316\u5de5\u5177\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u63a8\u5bfc\u4e00\u822c\u7b56\u7565\u7684\u6210\u672c\u8868\u8fbe\u5f0f\uff0c\u8bc1\u660e\u4f4e\u76ee\u6807\u5ef6\u8fdf\u65f6\u961f\u5217\u957f\u5ea6\u4e3a1\u6700\u4f18\uff0c\u9ad8\u76ee\u6807\u5ef6\u8fdf\u65f6\u5229\u7528\u80cc\u5305\u7ed3\u6784\u8bbe\u8ba1\u8c03\u5ea6\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u81ea\u9002\u5e94\u7b97\u6cd5\u5145\u5206\u5229\u7528\u5141\u8bb8\u7684\u5ef6\u8fdf\u3002", "result": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u6700\u4f18\u7b49\u5f85\u65f6\u95f4\u5206\u5e03\u7279\u5f81\uff0c\u8bbe\u8ba1\u7684\u8c03\u5ea6\u7b56\u7565\u5728\u9ad8\u5ef6\u8fdf\u573a\u666f\u4e0b\u6709\u6548\uff0c\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u7b97\u6cd5\u5728\u5b9e\u8bc1\u4e2d\u8868\u73b0\u51fa\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e91\u73af\u5883\u4e2d\u5ef6\u8fdf\u654f\u611f\u4f5c\u4e1a\u8c03\u5ea6\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u7b97\u6cd5\u80fd\u6709\u6548\u5e73\u8861\u6210\u672c\u548c\u5ef6\u8fdf\u7ea6\u675f\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.14159", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2601.14159", "abs": "https://arxiv.org/abs/2601.14159", "authors": ["Panagiotis-Eleftherios Eleftherakis", "George Anagnostopoulos", "Anastassis Kapetanakis", "Mohammad Umair", "Jean-Yves Vet", "Konstantinos Iliakis", "Jonathan Vincent", "Jing Gong", "Akshay Patil", "Clara Garc\u00eda-S\u00e1nchez", "Gerardo Zampino", "Ricardo Vinuesa", "Sotirios Xydis"], "title": "Multi-Partner Project: Multi-GPU Performance Portability Analysis for CFD Simulations at Scale", "comment": "DATE 26 conference Multi-Partner Project Paper", "summary": "As heterogeneous supercomputing architectures leveraging GPUs become increasingly central to high-performance computing (HPC), it is crucial for computational fluid dynamics (CFD) simulations, a de-facto HPC workload, to efficiently utilize such hardware. One of the key challenges of HPC codes is performance portability, i.e. the ability to maintain near-optimal performance across different accelerators. In the context of the \\textbf{REFMAP} project, which targets scalable, GPU-enabled multi-fidelity CFD for urban airflow prediction, this paper analyzes the performance portability of SOD2D, a state-of-the-art Spectral Elements simulation framework across AMD and NVIDIA GPU architectures. We first discuss the physical and numerical models underlying SOD2D, highlighting its computational hotspots. Then, we examine its performance and scalability in a multi-level manner, i.e. defining and characterizing an extensive full-stack design space spanning across application, software and hardware infrastructure related parameters. Single-GPU performance characterization across server-grade NVIDIA and AMD GPU architectures and vendor-specific compiler stacks, show the potential as well as the diverse effect of memory access optimizations, i.e. 0.69$\\times$ - 3.91$\\times$ deviations in acceleration speedup. Performance variability of SOD2D at scale is further examined on the LUMI multi-GPU cluster, where profiling reveals similar throughput variations, highlighting the limits of performance projections and the need for multi-level, informed tuning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86SOD2D CFD\u6846\u67b6\u5728AMD\u548cNVIDIA GPU\u67b6\u6784\u4e0a\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6027\uff0c\u53d1\u73b0\u5185\u5b58\u8bbf\u95ee\u4f18\u5316\u5e26\u67650.69-3.91\u500d\u52a0\u901f\u5dee\u5f02\uff0c\u591aGPU\u6269\u5c55\u5b58\u5728\u6027\u80fd\u6ce2\u52a8\uff0c\u5f3a\u8c03\u9700\u8981\u591a\u5c42\u6b21\u8c03\u4f18\u3002", "motivation": "\u968f\u7740GPU\u5f02\u6784\u8d85\u7b97\u67b6\u6784\u6210\u4e3aHPC\u6838\u5fc3\uff0cCFD\u6a21\u62df\u9700\u8981\u9ad8\u6548\u5229\u7528\u6b64\u7c7b\u786c\u4ef6\u3002\u6027\u80fd\u53ef\u79fb\u690d\u6027\uff08\u5728\u4e0d\u540c\u52a0\u901f\u5668\u4e0a\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff09\u662fHPC\u4ee3\u7801\u7684\u5173\u952e\u6311\u6218\u3002REFMAP\u9879\u76ee\u9488\u5bf9\u53ef\u6269\u5c55\u7684GPU\u591a\u4fdd\u771f\u5ea6CFD\u57ce\u5e02\u6c14\u6d41\u9884\u6d4b\uff0c\u9700\u8981\u5206\u6790SOD2D\u6846\u67b6\u5728AMD\u548cNVIDIA GPU\u4e0a\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6027\u3002", "method": "\u9996\u5148\u8ba8\u8bbaSOD2D\u7684\u7269\u7406\u548c\u6570\u503c\u6a21\u578b\uff0c\u8bc6\u522b\u8ba1\u7b97\u70ed\u70b9\u3002\u7136\u540e\u4ee5\u591a\u5c42\u6b21\u65b9\u5f0f\u5206\u6790\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff1a\u5b9a\u4e49\u8de8\u8d8a\u5e94\u7528\u3001\u8f6f\u4ef6\u548c\u786c\u4ef6\u57fa\u7840\u8bbe\u65bd\u53c2\u6570\u7684\u5b8c\u6574\u8bbe\u8ba1\u7a7a\u95f4\u3002\u5728\u670d\u52a1\u5668\u7ea7NVIDIA\u548cAMD GPU\u67b6\u6784\u53ca\u4f9b\u5e94\u5546\u7279\u5b9a\u7f16\u8bd1\u5668\u6808\u4e0a\u8fdb\u884c\u5355GPU\u6027\u80fd\u8868\u5f81\uff0c\u8bc4\u4f30\u5185\u5b58\u8bbf\u95ee\u4f18\u5316\u6548\u679c\u3002\u5728LUMI\u591aGPU\u96c6\u7fa4\u4e0a\u8fdb\u884c\u6269\u5c55\u6027\u80fd\u5206\u6790\uff0c\u901a\u8fc7\u6027\u80fd\u5256\u6790\u8bc6\u522b\u541e\u5410\u91cf\u53d8\u5316\u3002", "result": "\u5355GPU\u6027\u80fd\u8868\u5f81\u663e\u793a\u5185\u5b58\u8bbf\u95ee\u4f18\u5316\u5e26\u6765\u663e\u8457\u5dee\u5f02\uff1a0.69-3.91\u500d\u7684\u52a0\u901f\u53d8\u5316\u8303\u56f4\u3002\u591aGPU\u6269\u5c55\u5206\u6790\u63ed\u793a\u7c7b\u4f3c\u7684\u541e\u5410\u91cf\u53d8\u5316\uff0c\u8868\u660e\u6027\u80fd\u9884\u6d4b\u5b58\u5728\u5c40\u9650\u6027\u3002\u4e0d\u540cGPU\u67b6\u6784\u548c\u7f16\u8bd1\u5668\u6808\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u4f18\u5316\u6548\u679c\u3002", "conclusion": "SOD2D\u5728AMD\u548cNVIDIA GPU\u4e0a\u8868\u73b0\u51fa\u6027\u80fd\u53ef\u79fb\u690d\u6027\u6311\u6218\uff0c\u5185\u5b58\u8bbf\u95ee\u4f18\u5316\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\u4e14\u591a\u6837\u5316\u3002\u591aGPU\u6269\u5c55\u5b58\u5728\u6027\u80fd\u6ce2\u52a8\uff0c\u5f3a\u8c03\u9700\u8981\u57fa\u4e8e\u591a\u5c42\u6b21\u4fe1\u606f\u7684\u8c03\u4f18\uff0c\u800c\u4e0d\u80fd\u4ec5\u4f9d\u8d56\u6027\u80fd\u9884\u6d4b\u3002\u8fd9\u5bf9\u5f02\u6784HPC\u67b6\u6784\u4e0a\u7684CFD\u5e94\u7528\u5f00\u53d1\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2601.12347", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12347", "abs": "https://arxiv.org/abs/2601.12347", "authors": ["Pranjal Naman", "Parv Agarwal", "Hrishikesh Haritas", "Yogesh Simmhan"], "title": "RIPPLE++: An Incremental Framework for Efficient GNN Inference on Evolving Graphs", "comment": "Extended full-length version of paper that appeared at ICDCS 2025: \"RIPPLE: Scalable Incremental GNN Inferencing on Large Streaming Graphs\", Pranjal Naman and Yogesh Simmhan, in International Conference on Distributed Computing Systems (ICDCS), 2025. DOI: https://doi.org/10.1109/icdcs63083.2025.00088", "summary": "Real-world graphs are dynamic, with frequent updates to their structure and features due to evolving vertex and edge properties. These continual changes pose significant challenges for efficient inference in graph neural networks (GNNs). Existing vertex-wise and layer-wise inference approaches are ill-suited for dynamic graphs, as they incur redundant computations, large neighborhood traversals, and high communication costs, especially in distributed settings. Additionally, while sampling-based approaches can be adopted to approximate final layer embeddings, these are often not preferred in critical applications due to their non-determinism. These limitations hinder low-latency inference required in real-time applications. To address this, we propose RIPPLE++, a framework for streaming GNN inference that efficiently and accurately updates embeddings in response to changes in the graph structure or features. RIPPLE++ introduces a generalized incremental programming model that captures the semantics of GNN aggregation functions and incrementally propagates updates to affected neighborhoods. RIPPLE++ accommodates all common graph updates, including vertex/edge addition/deletions and vertex feature updates. RIPPLE++ supports both single-machine and distributed deployments. On a single machine, it achieves up to $56$K updates/sec on sparse graphs like Arxiv ($169$K vertices, $1.2$M edges), and about $7.6$K updates/sec on denser graphs like Products ($2.5$M vertices, $123.7$M edges), with latencies of $0.06$--$960$ms, and outperforming state-of-the-art baselines by $2.2$--$24\\times$ on throughput. In distributed settings, RIPPLE++ offers up to $\\approx25\\times$ higher throughput and $20\\times$ lower communication costs compared to recomputing baselines.", "AI": {"tldr": "RIPPLE++\uff1a\u7528\u4e8e\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u7684\u6d41\u5f0f\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u91cf\u66f4\u65b0\u907f\u514d\u5197\u4f59\u8ba1\u7b97\uff0c\u5728\u5355\u673a\u548c\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u56fe\u662f\u52a8\u6001\u53d8\u5316\u7684\uff0c\u9891\u7e41\u7684\u7ed3\u6784\u548c\u7279\u5f81\u66f4\u65b0\u7ed9\u56fe\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u5e26\u6765\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u56fe\u573a\u666f\u4e0b\u5b58\u5728\u5197\u4f59\u8ba1\u7b97\u3001\u5927\u90bb\u57df\u904d\u5386\u548c\u9ad8\u901a\u4fe1\u6210\u672c\u7b49\u95ee\u9898\uff0c\u4e14\u91c7\u6837\u65b9\u6cd5\u56e0\u975e\u786e\u5b9a\u6027\u4e0d\u9002\u5408\u5173\u952e\u5e94\u7528\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u4f4e\u5ef6\u8fdf\u9700\u6c42\u3002", "method": "\u63d0\u51faRIPPLE++\u6846\u67b6\uff0c\u5f15\u5165\u901a\u7528\u589e\u91cf\u7f16\u7a0b\u6a21\u578b\uff0c\u6355\u83b7GNN\u805a\u5408\u51fd\u6570\u8bed\u4e49\uff0c\u589e\u91cf\u4f20\u64ad\u66f4\u65b0\u5230\u53d7\u5f71\u54cd\u90bb\u57df\u3002\u652f\u6301\u6240\u6709\u5e38\u89c1\u56fe\u66f4\u65b0\uff08\u9876\u70b9/\u8fb9\u589e\u5220\u3001\u7279\u5f81\u66f4\u65b0\uff09\uff0c\u9002\u7528\u4e8e\u5355\u673a\u548c\u5206\u5e03\u5f0f\u90e8\u7f72\u3002", "result": "\u5355\u673a\u73af\u5883\u4e0b\uff1a\u7a00\u758f\u56feArxiv\uff08169K\u9876\u70b9\uff0c1.2M\u8fb9\uff09\u8fbe\u523056K\u66f4\u65b0/\u79d2\uff0c\u7a20\u5bc6\u56feProducts\uff082.5M\u9876\u70b9\uff0c123.7M\u8fb9\uff09\u8fbe\u52307.6K\u66f4\u65b0/\u79d2\uff0c\u5ef6\u8fdf0.06-960ms\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53472.2-24\u500d\u541e\u5410\u91cf\u3002\u5206\u5e03\u5f0f\u73af\u5883\u4e0b\uff1a\u6bd4\u91cd\u65b0\u8ba1\u7b97\u57fa\u7ebf\u63d0\u5347\u7ea625\u500d\u541e\u5410\u91cf\uff0c\u964d\u4f4e20\u500d\u901a\u4fe1\u6210\u672c\u3002", "conclusion": "RIPPLE++\u4e3a\u52a8\u6001\u56feGNN\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u6d41\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u589e\u91cf\u66f4\u65b0\u673a\u5236\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2601.12434", "categories": ["cs.DC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.12434", "abs": "https://arxiv.org/abs/2601.12434", "authors": ["Shengwei You", "Aditya Joshi", "Andrey Kuehlkamp", "Jarek Nabrzyski"], "title": "ASAS-BridgeAMM: Trust-Minimized Cross-Chain Bridge AMM with Failure Containment", "comment": null, "summary": "Cross-chain bridges constitute the single largest vector of systemic risk in Decentralized Finance (DeFi), accounting for over \\$2.8 billion in losses since 2021. The fundamental vulnerability lies in the binary nature of existing bridge security models: a bridge is either fully operational or catastrophically compromised, with no intermediate state to contain partial failures. We present ASAS-BridgeAMM, a bridge-coupled automated market maker that introduces Contained Degradation: a formally specified operational state where the system gracefully degrades functionality in response to adversarial signals. By treating cross-chain message latency as a quantifiable execution risk, the protocol dynamically adjusts collateral haircuts, slippage bounds, and withdrawal limits. Across 18 months of historical replay on Ethereum and two auxiliary chains, ASAS-BridgeAMM reduces worst-case bridge-induced insolvency by 73% relative to baseline mint-and-burn architectures, while preserving 104.5% of transaction volume during stress periods. In rigorous adversarial simulations involving delayed finality, oracle manipulation, and liquidity griefing, the protocol maintains solvency with probability $>0.9999$ and bounds per-epoch bad debt to $<0.2%$ of total collateral. We provide a reference implementation in Solidity and formally prove safety (bounded debt), liveness (settlement completion), and manipulation resistance under a Byzantine relayer model.", "AI": {"tldr": "ASAS-BridgeAMM \u63d0\u51fa\u4e86\u4e00\u79cd\u6865\u63a5\u8026\u5408\u7684\u81ea\u52a8\u505a\u5e02\u5546\uff0c\u901a\u8fc7\"\u53d7\u63a7\u964d\u7ea7\"\u673a\u5236\u5728\u68c0\u6d4b\u5230\u653b\u51fb\u4fe1\u53f7\u65f6\u4f18\u96c5\u964d\u4f4e\u529f\u80fd\uff0c\u800c\u975e\u5b8c\u5168\u5d29\u6e83\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8de8\u94fe\u6865\u7684\u7cfb\u7edf\u6027\u98ce\u9669\u3002", "motivation": "\u8de8\u94fe\u6865\u662fDeFi\u4e2d\u7cfb\u7edf\u6027\u98ce\u9669\u7684\u6700\u5927\u6765\u6e90\uff0c\u81ea2021\u5e74\u4ee5\u6765\u5df2\u9020\u6210\u8d85\u8fc728\u4ebf\u7f8e\u5143\u635f\u5931\u3002\u73b0\u6709\u6865\u63a5\u5b89\u5168\u6a21\u578b\u7684\u6839\u672c\u7f3a\u9677\u5728\u4e8e\u5176\u4e8c\u5143\u6027\uff1a\u8981\u4e48\u5b8c\u5168\u8fd0\u884c\uff0c\u8981\u4e48\u707e\u96be\u6027\u5d29\u6e83\uff0c\u7f3a\u4e4f\u5904\u7406\u90e8\u5206\u6545\u969c\u7684\u4e2d\u95f4\u72b6\u6001\u3002", "method": "\u63d0\u51faASAS-BridgeAMM\u6865\u63a5\u8026\u5408\u81ea\u52a8\u505a\u5e02\u5546\uff0c\u5f15\u5165\"\u53d7\u63a7\u964d\u7ea7\"\u673a\u5236\uff0c\u5c06\u8de8\u94fe\u6d88\u606f\u5ef6\u8fdf\u91cf\u5316\u4e3a\u6267\u884c\u98ce\u9669\uff0c\u52a8\u6001\u8c03\u6574\u62b5\u62bc\u54c1\u6298\u6263\u3001\u6ed1\u70b9\u8fb9\u754c\u548c\u63d0\u6b3e\u9650\u5236\u3002\u5728\u62dc\u5360\u5ead\u4e2d\u7ee7\u6a21\u578b\u4e0b\u5f62\u5f0f\u5316\u8bc1\u660e\u5b89\u5168\u6027\u3001\u6d3b\u6027\u53ca\u6297\u64cd\u7eb5\u6027\u3002", "result": "\u5728\u4ee5\u592a\u574a\u548c\u4e24\u6761\u8f85\u52a9\u94fe\u4e0a18\u4e2a\u6708\u5386\u53f2\u56de\u653e\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u51c6\u94f8\u5e01-\u9500\u6bc1\u67b6\u6784\uff0c\u6700\u574f\u60c5\u51b5\u4e0b\u6865\u63a5\u5bfc\u81f4\u7684\u7834\u4ea7\u51cf\u5c1173%\uff0c\u538b\u529b\u671f\u95f4\u4ea4\u6613\u91cf\u4fdd\u6301104.5%\u3002\u5728\u5ef6\u8fdf\u6700\u7ec8\u6027\u3001\u9884\u8a00\u673a\u64cd\u7eb5\u548c\u6d41\u52a8\u6027\u653b\u51fb\u7684\u5bf9\u6297\u6a21\u62df\u4e2d\uff0c\u4fdd\u6301\u507f\u4ed8\u80fd\u529b\u7684\u6982\u7387>0.9999\uff0c\u6bcf\u65f6\u671f\u574f\u8d26\u9650\u5236\u5728\u603b\u62b5\u62bc\u54c1\u7684<0.2%\u3002", "conclusion": "ASAS-BridgeAMM\u901a\u8fc7\u53d7\u63a7\u964d\u7ea7\u673a\u5236\u89e3\u51b3\u4e86\u8de8\u94fe\u6865\u7684\u4e8c\u5143\u5b89\u5168\u6a21\u578b\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7cfb\u7edf\u6027\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u53ef\u7528\u6027\uff0c\u4e3a\u8de8\u94fe\u6865\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002"}}
{"id": "2601.12524", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12524", "abs": "https://arxiv.org/abs/2601.12524", "authors": ["Zechuan Gong", "Hui Zhang", "Yuquan Yang", "Wenyu Lu"], "title": "SGCP: A Self-Organized Game-Theoretic Framework For Collaborative Perception", "comment": null, "summary": "Collaborative perception holds great promise for improving safety in autonomous driving, particularly in dense traffic where vehicles can share sensory information to overcome individual blind spots and extend awareness. However, deploying such collaboration at scale remains difficult when communication bandwidth is limited and no roadside infrastructure is available. To overcome these limitations, we introduce a fully decentralized framework that enables vehicles to self organize into cooperative groups using only vehicle to vehicle communication. The approach decomposes the problem into two sequential game theoretic stages. In the first stage, vehicles form stable clusters by evaluating mutual sensing complementarity and motion coherence, and each cluster elects a coordinator. In the second stage, the coordinator guides its members to selectively transmit point cloud segments from perceptually salient regions through a non cooperative potential game, enabling efficient local fusion. Global scene understanding is then achieved by exchanging compact detection messages across clusters rather than raw sensor data. We design distributed algorithms for both stages that guarantee monotonic improvement of the system wide potential function. Comprehensive experiments on the CARLA-OpenCDA-NS3 co-simulation platform show that our method reduces communication overhead while delivering higher perception accuracy and wider effective coverage compared to existing baselines.", "AI": {"tldr": "\u63d0\u51fa\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u534f\u4f5c\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u535a\u5f08\u8bba\u65b9\u6cd5\u8ba9\u8f66\u8f86\u81ea\u7ec4\u7ec7\u6210\u534f\u4f5c\u96c6\u7fa4\uff0c\u5728\u6709\u9650\u901a\u4fe1\u5e26\u5bbd\u4e0b\u5b9e\u73b0\u9ad8\u6548\u611f\u77e5\u5171\u4eab\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u534f\u4f5c\u611f\u77e5\u80fd\u63d0\u5347\u5b89\u5168\u6027\uff0c\u4f46\u5728\u5bc6\u96c6\u4ea4\u901a\u4e2d\u90e8\u7f72\u9762\u4e34\u901a\u4fe1\u5e26\u5bbd\u9650\u5236\u548c\u7f3a\u4e4f\u8def\u8fb9\u57fa\u7840\u8bbe\u65bd\u7684\u6311\u6218\uff0c\u9700\u8981\u53bb\u4e2d\u5fc3\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4e24\u9636\u6bb5\u535a\u5f08\u8bba\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u8f66\u8f86\u57fa\u4e8e\u611f\u77e5\u4e92\u8865\u6027\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\u5f62\u6210\u7a33\u5b9a\u96c6\u7fa4\u5e76\u9009\u4e3e\u534f\u8c03\u8005\uff1b\u7b2c\u4e8c\u9636\u6bb5\u534f\u8c03\u8005\u5f15\u5bfc\u6210\u5458\u901a\u8fc7\u975e\u5408\u4f5c\u52bf\u535a\u5f08\u9009\u62e9\u6027\u4f20\u8f93\u70b9\u4e91\u7247\u6bb5\uff0c\u5b9e\u73b0\u5c40\u90e8\u878d\u5408\uff0c\u96c6\u7fa4\u95f4\u4ea4\u6362\u7d27\u51d1\u68c0\u6d4b\u6d88\u606f\u800c\u975e\u539f\u59cb\u6570\u636e\u3002", "result": "\u5728CARLA-OpenCDA-NS3\u534f\u540c\u4eff\u771f\u5e73\u53f0\u4e0a\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u611f\u77e5\u7cbe\u5ea6\u548c\u66f4\u5e7f\u7684\u6709\u6548\u8986\u76d6\u8303\u56f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\u901a\u8fc7\u8f66\u8f86\u81ea\u7ec4\u7ec7\u548c\u9009\u62e9\u6027\u6570\u636e\u4f20\u8f93\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6709\u9650\u901a\u4fe1\u5e26\u5bbd\u4e0b\u7684\u534f\u4f5c\u611f\u77e5\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u81ea\u52a8\u9a7e\u9a76\u534f\u4f5c\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.12713", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12713", "abs": "https://arxiv.org/abs/2601.12713", "authors": ["Luke Marzen", "Junhyung Shim", "Ali Jannesari"], "title": "Dynamic Detection of Inefficient Data Mapping Patterns in Heterogeneous OpenMP Applications", "comment": "Accepted to The 31st ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming (PPoPP '26)", "summary": "With the growing prevalence of heterogeneous computing, CPUs are increasingly being paired with accelerators to achieve new levels of performance and energy efficiency. However, data movement between devices remains a significant bottleneck, complicating application development. Existing performance tools require considerable programmer intervention to diagnose and locate data transfer inefficiencies. To address this, we propose dynamic analysis techniques to detect and profile inefficient data transfer and allocation patterns in heterogeneous applications. We implemented these techniques into OMPDataPerf, which provides detailed traces of problematic data mappings, source code attribution, and assessments of optimization potential in heterogeneous OpenMP applications. OMPDataPerf uses the OpenMP Tools Interface (OMPT) and incurs only a 5 % geometric-mean runtime overhead.", "AI": {"tldr": "OMPDataPerf\uff1a\u57fa\u4e8e\u52a8\u6001\u5206\u6790\u68c0\u6d4b\u5f02\u6784OpenMP\u5e94\u7528\u4e2d\u6570\u636e\u79fb\u52a8\u4f4e\u6548\u95ee\u9898\u7684\u5de5\u5177\uff0c\u4ec5\u5f15\u51655%\u8fd0\u884c\u65f6\u5f00\u9500", "motivation": "\u968f\u7740CPU\u4e0e\u52a0\u901f\u5668\u5f02\u6784\u8ba1\u7b97\u7684\u666e\u53ca\uff0c\u8bbe\u5907\u95f4\u6570\u636e\u79fb\u52a8\u6210\u4e3a\u6027\u80fd\u74f6\u9888\uff0c\u73b0\u6709\u6027\u80fd\u5de5\u5177\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5e72\u9884\u6765\u8bca\u65ad\u6570\u636e\u4f20\u8f93\u4f4e\u6548\u95ee\u9898", "method": "\u63d0\u51fa\u52a8\u6001\u5206\u6790\u6280\u672f\u68c0\u6d4b\u5f02\u6784\u5e94\u7528\u4e2d\u7684\u6570\u636e\u4f20\u8f93\u548c\u5206\u914d\u6a21\u5f0f\u4f4e\u6548\u95ee\u9898\uff0c\u5b9e\u73b0\u4e3aOMPDataPerf\u5de5\u5177\uff0c\u5229\u7528OpenMP Tools Interface (OMPT)\u63d0\u4f9b\u8be6\u7ec6\u7684\u95ee\u9898\u6570\u636e\u6620\u5c04\u8ffd\u8e2a\u3001\u6e90\u4ee3\u7801\u5f52\u56e0\u548c\u4f18\u5316\u6f5c\u529b\u8bc4\u4f30", "result": "OMPDataPerf\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5f02\u6784OpenMP\u5e94\u7528\u4e2d\u7684\u6570\u636e\u79fb\u52a8\u4f4e\u6548\u6a21\u5f0f\uff0c\u4ec5\u5f15\u51655%\u7684\u51e0\u4f55\u5e73\u5747\u8fd0\u884c\u65f6\u5f00\u9500\uff0c\u4e3a\u7a0b\u5e8f\u5458\u63d0\u4f9b\u8be6\u7ec6\u7684\u8bca\u65ad\u4fe1\u606f", "conclusion": "OMPDataPerf\u901a\u8fc7\u52a8\u6001\u5206\u6790\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u8ba1\u7b97\u4e2d\u6570\u636e\u79fb\u52a8\u74f6\u9888\u7684\u8bca\u65ad\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7a0b\u5e8f\u5458\u5e72\u9884\u9700\u6c42\uff0c\u4e3a\u4f18\u5316\u5f02\u6784\u5e94\u7528\u6027\u80fd\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177"}}
{"id": "2601.12749", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12749", "abs": "https://arxiv.org/abs/2601.12749", "authors": ["Hui Zhang", "Yuquan Yang", "Zechuan Gong", "Xiaohua Xu", "Dan Keun Sung"], "title": "Efficient Local-to-Global Collaborative Perception via Joint Communication and Computation Optimization", "comment": null, "summary": "Autonomous driving relies on accurate perception to ensure safe driving. Collaborative perception improves accuracy by mitigating the sensing limitations of individual vehicles, such as limited perception range and occlusion-induced blind spots. However, collaborative perception often suffers from high communication overhead due to redundant data transmission, as well as increasing computation latency caused by excessive load with growing connected and autonomous vehicles (CAVs) participation. To address these challenges, we propose a novel local-to-global collaborative perception framework (LGCP) to achieve collaboration in a communication- and computation-efficient manner. The road of interest is partitioned into non-overlapping areas, each of which is assigned a dedicated CAV group to perform localized perception. A designated leader in each group collects and fuses perception data from its members, and uploads the perception result to the roadside unit (RSU), establishing a link between local perception and global awareness. The RSU aggregates perception results from all groups and broadcasts a global view to all CAVs. LGCP employs a centralized scheduling strategy via the RSU, which assigns CAV groups to each area, schedules their transmissions, aggregates area-level local perception results, and propagates the global view to all CAVs. Experimental results demonstrate that the proposed LGCP framework achieves an average 44 times reduction in the amount of data transmission, while maintaining or even improving the overall collaborative performance.", "AI": {"tldr": "\u63d0\u51faLGCP\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u57df\u5212\u5206\u548c\u5206\u7ec4\u534f\u4f5c\uff0c\u5728\u4fdd\u6301\u611f\u77e5\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u901a\u4fe1\u5f00\u9500", "motivation": "\u534f\u540c\u611f\u77e5\u5b58\u5728\u901a\u4fe1\u5f00\u9500\u9ad8\u548c\u8ba1\u7b97\u5ef6\u8fdf\u5927\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u534f\u4f5c\u6846\u67b6", "method": "\u5c06\u9053\u8def\u5212\u5206\u4e3a\u975e\u91cd\u53e0\u533a\u57df\uff0c\u6bcf\u4e2a\u533a\u57df\u5206\u914d\u4e13\u7528CAV\u7ec4\u8fdb\u884c\u672c\u5730\u611f\u77e5\uff0c\u901a\u8fc7RSU\u96c6\u4e2d\u8c03\u5ea6\u548c\u805a\u5408\u5168\u5c40\u89c6\u56fe", "result": "\u6570\u636e\u4f20\u8f93\u91cf\u5e73\u5747\u51cf\u5c1144\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u534f\u540c\u611f\u77e5\u6027\u80fd", "conclusion": "LGCP\u6846\u67b6\u5728\u901a\u4fe1\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u534f\u540c\u611f\u77e5\u65b9\u6cd5"}}
{"id": "2601.12784", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12784", "abs": "https://arxiv.org/abs/2601.12784", "authors": ["Haoyang Li", "Sheng Lin", "Fangcheng Fu", "Yuming Zhou", "Xiaodong Ji", "Yanfeng Zhao", "Lefeng Wang", "Jie Jiang", "Bin Cui"], "title": "Unleashing Efficient Asynchronous RL Post-Training via Staleness-Constrained Rollout Coordination", "comment": null, "summary": "Reinforcement learning (RL) post-training has become pivotal for enhancing the capabilities of modern large models. A recent trend is to develop RL systems with a fully disaggregated architecture, which decouples the three RL phases (rollout, reward, and training) onto separate resources and executes them asynchronously. However, two critical data-level concerns arise: (1) asynchronous execution leads to data staleness in trajectories (the data generated by rollout) as the model parameters used in rollout may not be up to date, which impairs RL convergence; and (2) the length variation of trajectories introduces severe data skewness, leading to workload imbalance and degraded system performance.\n  Existing systems fail to address these two concerns in a unified manner. Techniques that tightly control data staleness often constrain effective data skewness mitigation, while aggressive data skewness mitigation tends to exacerbate data staleness. As a result, systems are forced to trade off convergence for performance, or vice versa. To address this, we propose StaleFlow, an RL post-training system that jointly tackles data staleness and skewness. First, to control staleness, StaleFlow introduces a global consistency protocol that tracks the full lifecycle of each trajectory and constrains staleness. Second, to mitigate skewness, StaleFlow re-designs the RL system architecture by constructing data servers for trajectories and parameters to achieve flexible rollout coordination. Subsequently, we develop a suite of staleness-aware, throughput-oriented strategies to enhance system performance. Evaluations show that StaleFlow achieves up to 1.42-2.68$\\times$ (1.17-2.01$\\times$ on average) higher throughput than state-of-the-art systems, without compromising convergence.", "AI": {"tldr": "StaleFlow\uff1a\u4e00\u4e2a\u89e3\u51b3RL\u540e\u8bad\u7ec3\u4e2d\u6570\u636e\u9648\u65e7\u6027\u548c\u504f\u659c\u6027\u95ee\u9898\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5168\u5c40\u4e00\u81f4\u6027\u534f\u8bae\u548c\u91cd\u6784\u67b6\u6784\uff0c\u5728\u4fdd\u8bc1\u6536\u655b\u7684\u540c\u65f6\u63d0\u5347\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u4ee3\u5927\u89c4\u6a21\u6a21\u578b\u7684RL\u540e\u8bad\u7ec3\u4e2d\uff0c\u5b8c\u5168\u89e3\u8026\u67b6\u6784\u5bfc\u81f4\u4e24\u4e2a\u5173\u952e\u6570\u636e\u95ee\u9898\uff1a\u5f02\u6b65\u6267\u884c\u5f15\u8d77\u7684\u6570\u636e\u9648\u65e7\u6027\uff08\u8f68\u8ff9\u8fc7\u65f6\uff09\u548c\u8f68\u8ff9\u957f\u5ea6\u53d8\u5316\u5bfc\u81f4\u7684\u6570\u636e\u504f\u659c\u6027\uff0c\u73b0\u6709\u7cfb\u7edf\u65e0\u6cd5\u7edf\u4e00\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u5728\u6536\u655b\u548c\u6027\u80fd\u4e4b\u95f4\u505a\u51fa\u6743\u8861\u3002", "method": "1. \u5f15\u5165\u5168\u5c40\u4e00\u81f4\u6027\u534f\u8bae\u8ddf\u8e2a\u6bcf\u4e2a\u8f68\u8ff9\u7684\u5b8c\u6574\u751f\u547d\u5468\u671f\u4ee5\u63a7\u5236\u9648\u65e7\u6027\uff1b2. \u91cd\u6784RL\u7cfb\u7edf\u67b6\u6784\uff0c\u6784\u5efa\u8f68\u8ff9\u548c\u53c2\u6570\u7684\u6570\u636e\u670d\u52a1\u5668\u5b9e\u73b0\u7075\u6d3b\u7684rollout\u534f\u8c03\uff1b3. \u5f00\u53d1\u4e00\u5957\u9762\u5411\u541e\u5410\u91cf\u7684\u9648\u65e7\u6027\u611f\u77e5\u7b56\u7565\u6765\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "result": "StaleFlow\u76f8\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u5b9e\u73b0\u4e861.42-2.68\u500d\uff08\u5e73\u57471.17-2.01\u500d\uff09\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u6536\u655b\u6027\u80fd\u3002", "conclusion": "StaleFlow\u901a\u8fc7\u8054\u5408\u89e3\u51b3\u6570\u636e\u9648\u65e7\u6027\u548c\u504f\u659c\u6027\u95ee\u9898\uff0c\u5728RL\u540e\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u4e86\u6536\u655b\u548c\u6027\u80fd\u7684\u53cc\u91cd\u4f18\u5316\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12830", "categories": ["cs.DC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.12830", "abs": "https://arxiv.org/abs/2601.12830", "authors": ["Om Mishra", "Jayesh Patil", "Sathwik Narkedimilli", "G Srikantha Sharma", "Ananda S", "Manjunath K Vanahalli"], "title": "From Design to Deorbit: A Solar-Electric Autonomous Module for Multi-Debris Remediation", "comment": "6 pages, 13 Figures, 2 tables", "summary": "The escalating accumulation of orbital debris threatens the sustainability of space operations, necessitating active removal solutions that overcome the limitations of current fuel-dependent methods. To address this, this study introduces a novel remediation architecture that integrates a mechanical clamping system for secure capture with a high-efficiency, solar-powered NASA Evolutionary Xenon Thruster (NEXT) and autonomous navigation protocols. High-fidelity simulations validate the architecture's capabilities, demonstrating a successful retrograde deorbit from 800 km to 100 km, <10m position Root Mean Square Errors (RMSE) via radar-based Extended Kalman Filter (EKF) navigation, and a 93\\% data delivery efficiency within 1 second using Delay/Disruption Tolerant Network (DTN) protocols. This approach significantly advances orbital management by establishing a benchmark for renewable solar propulsion that minimizes reliance on conventional fuels and extends mission longevity for multi-target removal.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u8f68\u9053\u788e\u7247\u4e3b\u52a8\u6e05\u9664\u67b6\u6784\uff0c\u7ed3\u5408\u673a\u68b0\u5939\u6301\u7cfb\u7edf\u3001\u592a\u9633\u80fd\u63a8\u8fdb\u5668\u548c\u81ea\u4e3b\u5bfc\u822a\u534f\u8bae\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6301\u7eed\u7684\u788e\u7247\u79fb\u9664", "motivation": "\u8f68\u9053\u788e\u7247\u65e5\u76ca\u7d2f\u79ef\u5a01\u80c1\u592a\u7a7a\u64cd\u4f5c\u53ef\u6301\u7eed\u6027\uff0c\u73b0\u6709\u71c3\u6599\u4f9d\u8d56\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u4e3b\u52a8\u6e05\u9664\u89e3\u51b3\u65b9\u6848", "method": "\u6574\u5408\u673a\u68b0\u5939\u6301\u7cfb\u7edf\u7528\u4e8e\u5b89\u5168\u6355\u83b7\u3001\u9ad8\u6548\u592a\u9633\u80fdNASA\u8fdb\u5316\u6c19\u63a8\u8fdb\u5668(NEXT)\u548c\u81ea\u4e3b\u5bfc\u822a\u534f\u8bae\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u4eff\u771f\u9a8c\u8bc1\u67b6\u6784\u80fd\u529b", "result": "\u6210\u529f\u5b9e\u73b0\u4ece800\u516c\u91cc\u5230100\u516c\u91cc\u7684\u9006\u884c\u79bb\u8f68\uff0c\u96f7\u8fbe\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5bfc\u822a\u4f4d\u7f6eRMSE\u5c0f\u4e8e10\u7c73\uff0c\u5ef6\u8fdf/\u4e2d\u65ad\u5bb9\u5fcd\u7f51\u7edc\u534f\u8bae\u6570\u636e\u4ea4\u4ed8\u6548\u7387\u8fbe93%", "conclusion": "\u8be5\u67b6\u6784\u663e\u8457\u63a8\u8fdb\u8f68\u9053\u7ba1\u7406\uff0c\u4e3a\u53ef\u518d\u751f\u592a\u9633\u80fd\u63a8\u8fdb\u5efa\u7acb\u57fa\u51c6\uff0c\u51cf\u5c11\u5bf9\u4f20\u7edf\u71c3\u6599\u4f9d\u8d56\uff0c\u5ef6\u957f\u591a\u76ee\u6807\u6e05\u9664\u4efb\u52a1\u5bff\u547d"}}
{"id": "2601.12853", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12853", "abs": "https://arxiv.org/abs/2601.12853", "authors": ["Shudi Weng", "Xiang Zhang", "Yizhou Zhao", "Giuseppe Caire", "Ming Xiao", "Mikael Skoglund"], "title": "On Resilient and Efficient Linear Secure Aggregation in Hierarchical Federated Learning", "comment": null, "summary": "In this paper, we study the fundamental limits of hierarchical secure aggregation under unreliable communication. We consider a hierarchical network where each client connects to multiple relays, and both client-to-relay and relay-to-server links are intermittent. Under this setting, we characterize the minimum communication and randomness costs required to achieve robust secure aggregation. We then propose an optimal protocol that attains these minimum costs, and establish its optimality through a matching converse proof. In addition, we introduce an improved problem formulation that bridges the gap between existing information-theoretic secure aggregation protocols and practical real-world federated learning problems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u53ef\u9760\u901a\u4fe1\u4e0b\u5206\u5c42\u5b89\u5168\u805a\u5408\u7684\u57fa\u672c\u6781\u9650\uff0c\u63d0\u51fa\u4e86\u6700\u4f18\u534f\u8bae\u5e76\u5efa\u7acb\u4e86\u5339\u914d\u7684\u9006\u8bc1\u660e", "motivation": "\u73b0\u6709\u4fe1\u606f\u8bba\u5b89\u5168\u805a\u5408\u534f\u8bae\u4e0e\u5b9e\u9645\u8054\u90a6\u5b66\u4e60\u95ee\u9898\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u9700\u8981\u7814\u7a76\u4e0d\u53ef\u9760\u901a\u4fe1\u73af\u5883\u4e0b\u7684\u5206\u5c42\u5b89\u5168\u805a\u5408\u57fa\u672c\u6781\u9650", "method": "\u8003\u8651\u5ba2\u6237\u7aef\u8fde\u63a5\u591a\u4e2a\u4e2d\u7ee7\u7684\u5206\u5c42\u7f51\u7edc\uff0c\u5ba2\u6237\u7aef-\u4e2d\u7ee7\u548c\u4e2d\u7ee7-\u670d\u52a1\u5668\u94fe\u8def\u90fd\u53ef\u80fd\u4e2d\u65ad\uff0c\u63d0\u51fa\u6700\u4f18\u534f\u8bae\u5e76\u5efa\u7acb\u5339\u914d\u7684\u9006\u8bc1\u660e", "result": "\u523b\u753b\u4e86\u5b9e\u73b0\u9c81\u68d2\u5b89\u5168\u805a\u5408\u6240\u9700\u7684\u6700\u5c0f\u901a\u4fe1\u548c\u968f\u673a\u6027\u6210\u672c\uff0c\u63d0\u51fa\u4e86\u8fbe\u5230\u8fd9\u4e9b\u6700\u5c0f\u6210\u672c\u7684\u6700\u4f18\u534f\u8bae", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u4fe1\u606f\u8bba\u5b89\u5168\u805a\u5408\u534f\u8bae\u4e0e\u5b9e\u9645\u8054\u90a6\u5b66\u4e60\u95ee\u9898\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u4e0d\u53ef\u9760\u901a\u4fe1\u73af\u5883\u4e0b\u7684\u5206\u5c42\u5b89\u5168\u805a\u5408\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6700\u4f18\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.12967", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12967", "abs": "https://arxiv.org/abs/2601.12967", "authors": ["Anish Biswas", "Kanishk Goel", "Jayashree Mohan", "Alind Khare", "Anjaly Parayil", "Ramachandran Ramjee", "Chetan Bansal"], "title": "Sutradhara: An Intelligent Orchestrator-Engine Co-design for Tool-based Agentic Inference", "comment": null, "summary": "Agentic applications are LLMs that iteratively invoke external tools to accomplish complex tasks. Such tool-based agents are rapidly becoming the dominant paradigm for deploying language models in production. Unlike traditional single-turn inference, agentic workloads chain together multiple LLM calls and tool executions before producing a final response, creating a new performance bottleneck that manifests as increased latency in First Token Rendered (FTR) of the final answer. Through analysis of synthetic requests at production scale, we reveal three critical challenges: tool calls account for 30-80% of FTR latency, KV cache hit rates collapse despite substantial context reuse across iterations, and sequential orchestration wastes potential intra-request parallelism by sequentially executing LLM calls and tools. These bottlenecks stem from a design gap in which orchestrators and LLM engines operate as decoupled black boxes, preventing cross-layer optimizations. We present SUTRADHARA, a co-designed agentic inference system that integrates orchestration with LLM serving through a thin API enabling three optimizations: overlap tool execution with subsequent LLM prefill using tool-aware prompt splitting, streaming tool execution to dispatch tools incrementally during decode rather than waiting for complete output, and orchestrator-aware cache management that uses semantic hints to improve hit rates and reduce thrashing. Implemented on vLLM, SUTRADHARA reduces median FTR latency by 15% and end-to-end latency by 10% across workloads on A100 GPUs, demonstrating that co-design can systematically tame latency in agentic systems.", "AI": {"tldr": "SUTRADHARA\u662f\u4e00\u4e2a\u534f\u540c\u8bbe\u8ba1\u7684\u667a\u80fd\u4f53\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u7f16\u6392\u548cLLM\u670d\u52a1\u6765\u4f18\u5316\u5de5\u5177\u8c03\u7528\u5ef6\u8fdf\uff0c\u51cf\u5c11\u6700\u7ec8\u7b54\u6848\u7684\u9996\u4ee4\u724c\u6e32\u67d3\u65f6\u95f4\u3002", "motivation": "\u57fa\u4e8e\u5de5\u5177\u7684\u667a\u80fd\u4f53\u5e94\u7528\u5df2\u6210\u4e3aLLM\u90e8\u7f72\u7684\u4e3b\u6d41\u8303\u5f0f\uff0c\u4f46\u591a\u8f6eLLM\u8c03\u7528\u548c\u5de5\u5177\u6267\u884c\u5bfc\u81f4\u9996\u4ee4\u724c\u6e32\u67d3\u5ef6\u8fdf\u663e\u8457\u589e\u52a0\u3002\u7814\u7a76\u53d1\u73b0\u5de5\u5177\u8c03\u7528\u536030-80%\u7684\u5ef6\u8fdf\uff0cKV\u7f13\u5b58\u547d\u4e2d\u7387\u5d29\u6e83\uff0c\u987a\u5e8f\u7f16\u6392\u6d6a\u8d39\u4e86\u6f5c\u5728\u7684\u5e76\u884c\u6027\u3002", "method": "\u63d0\u51faSUTRADHARA\u7cfb\u7edf\uff0c\u901a\u8fc7\u8584API\u6574\u5408\u7f16\u6392\u548cLLM\u670d\u52a1\uff0c\u5b9e\u73b0\u4e09\u79cd\u4f18\u5316\uff1a1) \u5de5\u5177\u611f\u77e5\u63d0\u793a\u5206\u5272\uff0c\u91cd\u53e0\u5de5\u5177\u6267\u884c\u4e0e\u540e\u7eedLLM\u9884\u586b\u5145\uff1b2) \u6d41\u5f0f\u5de5\u5177\u6267\u884c\uff0c\u5728\u89e3\u7801\u671f\u95f4\u589e\u91cf\u8c03\u5ea6\u5de5\u5177\uff1b3) \u7f16\u6392\u5668\u611f\u77e5\u7f13\u5b58\u7ba1\u7406\uff0c\u4f7f\u7528\u8bed\u4e49\u63d0\u793a\u63d0\u9ad8\u547d\u4e2d\u7387\u3002", "result": "\u5728vLLM\u4e0a\u5b9e\u73b0SUTRADHARA\uff0c\u5728A100 GPU\u4e0a\uff0c\u4e2d\u4f4d\u6570\u9996\u4ee4\u724c\u6e32\u67d3\u5ef6\u8fdf\u964d\u4f4e15%\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e10%\uff0c\u8bc1\u660e\u534f\u540c\u8bbe\u8ba1\u80fd\u7cfb\u7edf\u6027\u5730\u964d\u4f4e\u667a\u80fd\u4f53\u7cfb\u7edf\u5ef6\u8fdf\u3002", "conclusion": "\u7f16\u6392\u5668\u548cLLM\u5f15\u64ce\u7684\u89e3\u8026\u9ed1\u76d2\u8bbe\u8ba1\u5bfc\u81f4\u6027\u80fd\u74f6\u9888\uff0c\u534f\u540c\u8bbe\u8ba1\u901a\u8fc7\u8de8\u5c42\u4f18\u5316\u80fd\u6709\u6548\u89e3\u51b3\u667a\u80fd\u4f53\u63a8\u7406\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0cSUTRADHARA\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.12989", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12989", "abs": "https://arxiv.org/abs/2601.12989", "authors": ["Yitian Wang", "Yebo Feng", "Yingjiu Li", "Jiahua Xu"], "title": "Enshrined Proposer Builder Separation in the presence of Maximal Extractable Value", "comment": null, "summary": "In blockchain systems operating under the Proof-of-Stake (PoS) consensus mechanism, fairness in transaction processing is essential to preserving decentralization and maintaining user trust. However, with the emergence of Maximal Extractable Value (MEV), concerns about economic centralization and content manipulation have intensified. To address these vulnerabilities, the Ethereum community has introduced Proposer Builder Separation (PBS), which separates block construction from block proposal. Later, enshrined Proposer Builder Separation (ePBS) was also proposed in EIP-7732, which embeds PBS directly into the Ethereum consensus layer.\n  Our work identifies key limitations of ePBS by developing a formal framework that combines mathematical analysis and agent-based simulations to evaluate its auction-based block-building mechanism, with particular emphasis on MEV dynamics. Our results reveal that, although ePBS redistributes responsibilities between builders and proposers, it significantly amplifies profit and content centralization: the Gini coefficient for profits rises from 0.1749 under standard PoS without ePBS to 0.8358 under ePBS. This sharp increase indicates that a small number of efficient builders capture most value via MEV-driven auctions. Moreover, 95.4% of the block value is rewarded to proposers in ePBS, revealing a strong economic bias despite their limited role in block assembly. These findings highlight that ePBS exacerbates incentives for builders to adopt aggressive MEV strategies, suggesting the need for future research into mechanism designs that better balance decentralization, fairness, and MEV mitigation.", "AI": {"tldr": "ePBS\u867d\u7136\u5206\u79bb\u4e86\u533a\u5757\u6784\u5efa\u4e0e\u63d0\u8bae\uff0c\u4f46\u901a\u8fc7MEV\u9a71\u52a8\u7684\u62cd\u5356\u673a\u5236\u52a0\u5267\u4e86\u5229\u6da6\u548c\u5185\u5bb9\u96c6\u4e2d\u5316\uff0c\u4f7f\u5c11\u6570\u9ad8\u6548\u6784\u5efa\u8005\u83b7\u53d6\u5927\u90e8\u5206\u4ef7\u503c\uff0c\u540c\u65f695.4%\u7684\u533a\u5757\u4ef7\u503c\u6d41\u5411\u63d0\u8bae\u8005\uff0c\u5b58\u5728\u7ecf\u6d4e\u504f\u89c1\u3002", "motivation": "PoS\u5171\u8bc6\u4e2dMEV\u5f15\u53d1\u7ecf\u6d4e\u96c6\u4e2d\u5316\u548c\u5185\u5bb9\u64cd\u7eb5\u62c5\u5fe7\uff0cEthereum\u793e\u533a\u63d0\u51faePBS\uff08EIP-7732\uff09\u5c06PBS\u5d4c\u5165\u5171\u8bc6\u5c42\u4ee5\u5206\u79bb\u533a\u5757\u6784\u5efa\u4e0e\u63d0\u8bae\uff0c\u4f46\u9700\u8981\u8bc4\u4f30\u5176\u5b9e\u9645\u6548\u679c\u3002", "method": "\u5efa\u7acb\u7ed3\u5408\u6570\u5b66\u5206\u6790\u548c\u57fa\u4e8e\u4ee3\u7406\u6a21\u62df\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u8bc4\u4f30ePBS\u7684\u62cd\u5356\u5f0f\u533a\u5757\u6784\u5efa\u673a\u5236\uff0c\u7279\u522b\u5173\u6ce8MEV\u52a8\u6001\u3002", "result": "ePBS\u663e\u8457\u52a0\u5267\u5229\u6da6\u96c6\u4e2d\u5316\uff1a\u57fa\u5c3c\u7cfb\u6570\u4ece\u6807\u51c6PoS\u76840.1749\u4e0a\u5347\u5230ePBS\u4e0b\u76840.8358\uff1b95.4%\u7684\u533a\u5757\u4ef7\u503c\u5956\u52b1\u7ed9\u63d0\u8bae\u8005\uff0c\u5c3d\u7ba1\u4ed6\u4eec\u5728\u533a\u5757\u7ec4\u88c5\u4e2d\u4f5c\u7528\u6709\u9650\uff1b\u5c11\u6570\u9ad8\u6548\u6784\u5efa\u8005\u901a\u8fc7MEV\u9a71\u52a8\u7684\u62cd\u5356\u6355\u83b7\u5927\u90e8\u5206\u4ef7\u503c\u3002", "conclusion": "ePBS\u867d\u7136\u91cd\u65b0\u5206\u914d\u4e86\u6784\u5efa\u8005\u548c\u63d0\u8bae\u8005\u7684\u8d23\u4efb\uff0c\u4f46\u52a0\u5267\u4e86\u6784\u5efa\u8005\u91c7\u7528\u6fc0\u8fdbMEV\u7b56\u7565\u7684\u6fc0\u52b1\uff0c\u9700\u8981\u672a\u6765\u7814\u7a76\u66f4\u597d\u7684\u673a\u5236\u8bbe\u8ba1\u6765\u5e73\u8861\u53bb\u4e2d\u5fc3\u5316\u3001\u516c\u5e73\u6027\u548cMEV\u7f13\u89e3\u3002"}}
{"id": "2601.13047", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.13047", "abs": "https://arxiv.org/abs/2601.13047", "authors": ["Ashish Saxena", "Kaushik Mondal"], "title": "Exploration on Highly Dynamic Graphs", "comment": null, "summary": "We study the exploration problem by mobile agents in two prominent models of dynamic graphs: $1$-Interval Connectivity and Connectivity Time. The $1$-Interval Connectivity model was introduced by Kuhn et al.~[STOC 2010], and the Connectivity Time model was proposed by Michail et al.~[JPDC 2014]. Recently, Saxena et al.~[TCS 2025] investigated the exploration problem under both models. In this work, we first strengthen the existing impossibility results for the $1$-Interval Connectivity model. We then show that, in Connectivity Time dynamic graphs, exploration is impossible with $\\frac{(n-1)(n-2)}{2}$ mobile agents, even when the agents have full knowledge of all system parameters, global communication, full visibility, and infinite memory. This significantly improves the previously known bound of $n$. Moreover, we prove that to solve exploration with $\\frac{(n-1)(n-2)}{2}+1$ agents, $1$-hop visibility is necessary. Finally, we present an exploration algorithm that uses $\\frac{(n-1)(n-2)}{2}+1$ agents, assuming global communication, $1$-hop visibility, and $O(\\log n)$ memory per agent.", "AI": {"tldr": "\u5728\u52a8\u6001\u56fe\u7684\u63a2\u7d22\u95ee\u9898\u4e2d\uff0c\u672c\u6587\u6539\u8fdb\u4e861-Interval Connectivity\u6a21\u578b\u7684\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\uff0c\u5e76\u8bc1\u660e\u5728Connectivity Time\u6a21\u578b\u4e2d\uff0c\u5373\u4f7f\u6709(n-1)(n-2)/2\u4e2a\u79fb\u52a8\u4ee3\u7406\u4e5f\u65e0\u6cd5\u5b8c\u6210\u63a2\u7d22\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u5148\u524dn\u4e2a\u4ee3\u7406\u7684\u754c\u9650\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u4f7f\u7528(n-1)(n-2)/2+1\u4e2a\u4ee3\u7406\u7684\u63a2\u7d22\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u79fb\u52a8\u4ee3\u7406\u5728\u52a8\u6001\u56fe\u4e2d\u7684\u63a2\u7d22\u95ee\u9898\uff0c\u7279\u522b\u662f\u57281-Interval Connectivity\u548cConnectivity Time\u4e24\u79cd\u91cd\u8981\u6a21\u578b\u4e2d\u3002\u5148\u524d\u7814\u7a76\u5df2\u53d6\u5f97\u4e00\u4e9b\u7ed3\u679c\uff0c\u4f46\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7406\u6570\u91cf\u754c\u9650\u548c\u7b97\u6cd5\u8bbe\u8ba1\u65b9\u9762\u3002", "method": "\u9996\u5148\u5f3a\u53161-Interval Connectivity\u6a21\u578b\u7684\u73b0\u6709\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\u3002\u7136\u540e\u8bc1\u660e\u5728Connectivity Time\u6a21\u578b\u4e2d\uff0c\u5373\u4f7f\u6709(n-1)(n-2)/2\u4e2a\u4ee3\u7406\u4e5f\u65e0\u6cd5\u5b8c\u6210\u63a2\u7d22\u3002\u63a5\u7740\u8bc1\u660e\u8981\u4f7f\u7528(n-1)(n-2)/2+1\u4e2a\u4ee3\u7406\u89e3\u51b3\u63a2\u7d22\u95ee\u9898\uff0c\u9700\u89811\u8df3\u53ef\u89c1\u6027\u3002\u6700\u540e\u63d0\u51fa\u4e00\u4e2a\u4f7f\u7528\u8be5\u6570\u91cf\u4ee3\u7406\u7684\u63a2\u7d22\u7b97\u6cd5\u3002", "result": "1. \u6539\u8fdb\u4e861-Interval Connectivity\u6a21\u578b\u7684\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\uff1b2. \u8bc1\u660e\u5728Connectivity Time\u6a21\u578b\u4e2d\uff0c\u5373\u4f7f\u6709(n-1)(n-2)/2\u4e2a\u4ee3\u7406\u4e5f\u65e0\u6cd5\u5b8c\u6210\u63a2\u7d22\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u5148\u524dn\u4e2a\u4ee3\u7406\u7684\u754c\u9650\uff1b3. \u8bc1\u660e\u8981\u4f7f\u7528(n-1)(n-2)/2+1\u4e2a\u4ee3\u7406\uff0c\u9700\u89811\u8df3\u53ef\u89c1\u6027\uff1b4. \u63d0\u51fa\u4e86\u4f7f\u7528\u8be5\u6570\u91cf\u4ee3\u7406\u7684\u63a2\u7d22\u7b97\u6cd5\uff0c\u5047\u8bbe\u5168\u5c40\u901a\u4fe1\u30011\u8df3\u53ef\u89c1\u6027\u548cO(log n)\u5185\u5b58\u3002", "conclusion": "\u672c\u6587\u5728\u52a8\u6001\u56fe\u63a2\u7d22\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u4ee3\u7406\u6570\u91cf\u7684\u4e0d\u53ef\u80fd\u6027\u754c\u9650\uff0c\u5e76\u63d0\u51fa\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u52a8\u6001\u7f51\u7edc\u4e2d\u7684\u79fb\u52a8\u4ee3\u7406\u63a2\u7d22\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c1\u89e3\u3002"}}
{"id": "2601.13146", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.13146", "abs": "https://arxiv.org/abs/2601.13146", "authors": ["Nicolas Nicolaou", "Kishori M. Konwar", "Moritz Grundei", "Aleksandr Bezobchuk", "Muriel M\u00e9dard", "Sriram Vishwanath"], "title": "OPTIMUM-DERAM: Highly Consistent, Scalable, and Secure Multi-Object Memory using RLNC", "comment": null, "summary": "This paper introduces OPTIMUM-DERAM, a highly consistent, scalable, secure, and decentralized shared memory solution. Traditional distributed shared memory implementations offer multi-object support by multi-threading a single object memory instance over the same set of data hosts. While theoretically sound, the amount of resources required made such solutions prohibitively expensive in practical systems. OPTIMUM-DERAM proposes a decentralized, reconfigurable, atomic read/write shared memory (DeRAM) that: (i) achieves improved performance and storage scalability by leveraging Random Linear Network Codes (RLNC); (ii) scales in the number of supported atomic objects by introducing a new object placement and discovery approach based on a consistent hashing ring; (iii) scales in the number of participants by allowing dynamic joins and departures leveraging a blockchain oracle to serve as a registry service; and (iv) is secure against malicious behavior by tolerating Byzantine failures.\n  Experimental results over a globally distributed set of nodes, help us realize the performance and scalability gains of OPTIMUM-DERAM over previous distributed shared memory solutions (i.e., the ABD algorithm [3])", "AI": {"tldr": "OPTIMUM-DERAM\u662f\u4e00\u4e2a\u9ad8\u5ea6\u4e00\u81f4\u3001\u53ef\u6269\u5c55\u3001\u5b89\u5168\u4e14\u53bb\u4e2d\u5fc3\u5316\u7684\u5171\u4eab\u5185\u5b58\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u968f\u673a\u7ebf\u6027\u7f51\u7edc\u7f16\u7801\u548c\u4e00\u81f4\u6027\u54c8\u5e0c\u73af\u7b49\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5206\u5e03\u5f0f\u5171\u4eab\u5185\u5b58\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5206\u5e03\u5f0f\u5171\u4eab\u5185\u5b58\u5b9e\u73b0\u901a\u8fc7\u5728\u540c\u4e00\u7ec4\u6570\u636e\u4e3b\u673a\u4e0a\u591a\u7ebf\u7a0b\u5316\u5355\u4e2a\u5bf9\u8c61\u5185\u5b58\u5b9e\u4f8b\u6765\u652f\u6301\u591a\u5bf9\u8c61\uff0c\u7406\u8bba\u4e0a\u53ef\u884c\u4f46\u8d44\u6e90\u9700\u6c42\u5de8\u5927\uff0c\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u53bb\u4e2d\u5fc3\u5316\u3001\u53ef\u91cd\u6784\u7684\u539f\u5b50\u8bfb\u5199\u5171\u4eab\u5185\u5b58(DeRAM)\uff1a1)\u5229\u7528\u968f\u673a\u7ebf\u6027\u7f51\u7edc\u7f16\u7801(Random Linear Network Codes)\u63d0\u9ad8\u6027\u80fd\u548c\u5b58\u50a8\u53ef\u6269\u5c55\u6027\uff1b2)\u57fa\u4e8e\u4e00\u81f4\u6027\u54c8\u5e0c\u73af\u5b9e\u73b0\u5bf9\u8c61\u653e\u7f6e\u548c\u53d1\u73b0\uff0c\u652f\u6301\u66f4\u591a\u539f\u5b50\u5bf9\u8c61\uff1b3)\u901a\u8fc7\u533a\u5757\u94fe\u9884\u8a00\u673a\u4f5c\u4e3a\u6ce8\u518c\u670d\u52a1\uff0c\u652f\u6301\u52a8\u6001\u8282\u70b9\u52a0\u5165/\u79bb\u5f00\uff1b4)\u5bb9\u5fcd\u62dc\u5360\u5ead\u6545\u969c\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u3002", "result": "\u5728\u5168\u7403\u5206\u5e03\u5f0f\u8282\u70b9\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOPTIMUM-DERAM\u76f8\u6bd4\u4e4b\u524d\u7684\u5206\u5e03\u5f0f\u5171\u4eab\u5185\u5b58\u89e3\u51b3\u65b9\u6848\uff08\u5982ABD\u7b97\u6cd5\uff09\u5728\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "OPTIMUM-DERAM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u3001\u5b89\u5168\u7684\u53bb\u4e2d\u5fc3\u5316\u5171\u4eab\u5185\u5b58\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u8d44\u6e90\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2601.13351", "categories": ["cs.DC", "cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.13351", "abs": "https://arxiv.org/abs/2601.13351", "authors": ["Rute C. Sofia", "Josh Salomon", "Ray Carrol", "Luis Garc\u00e9s-Erice", "Peter Urbanetz", "J\u00fcrgen Gesswein", "Rizkallah Touma", "Alejandro Espinosa", "Luis M. Contreras", "Vasileios Theodorou", "George Papathanail", "Georgios Koukis", "Vassilis Tsaoussidis", "Alberto del Rio", "David Jimenez", "Efterpi Paraskevoulakou", "Panagiotis Karamolegkos", "John Soldatos", "Borja Dorado Nogales", "Alejandro Tjaarda"], "title": "Towards Scalable Federated Container Orchestration: The CODECO Approach", "comment": null, "summary": "This paper presents CODECO, a federated orchestration framework for Kubernetes that addresses the limitations of cloud-centric deployment. CODECO adopts a data-compute-network co-orchestration approach to support heterogeneous infrastructures, mobility, and multi-provider operation.\n  CODECO extends Kubernetes with semantic application models, partition-based federation, and AI-assisted decision support, enabling context-aware placement and adaptive management of applications and their micro-services across federated environments. A hybrid governance model combines centralized policy enforcement with decentralized execution and learning to preserve global coherence while supporting far Edge autonomy. The paper describes the architecture and core components of CODECO, outlines representative orchestration workflows, and introduces a software-based experimentation framework for reproducible evaluation in federated Edge-Cloud infrastructure environments.", "AI": {"tldr": "CODECO\u662f\u4e00\u4e2a\u9762\u5411Kubernetes\u7684\u8054\u90a6\u7f16\u6392\u6846\u67b6\uff0c\u91c7\u7528\u6570\u636e-\u8ba1\u7b97-\u7f51\u7edc\u534f\u540c\u7f16\u6392\u65b9\u6cd5\uff0c\u652f\u6301\u5f02\u6784\u57fa\u7840\u8bbe\u65bd\u3001\u79fb\u52a8\u6027\u548c\u591a\u4e91\u63d0\u4f9b\u5546\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u4e91\u4e2d\u5fc3\u5316\u90e8\u7f72\u7684\u5c40\u9650\u6027\uff0c\u652f\u6301\u5728\u8054\u90a6\u8fb9\u7f18\u4e91\u73af\u5883\u4e2d\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5e94\u7528\u7f16\u6392\u548c\u7ba1\u7406\u3002", "method": "\u6269\u5c55Kubernetes\uff0c\u5f15\u5165\u8bed\u4e49\u5e94\u7528\u6a21\u578b\u3001\u57fa\u4e8e\u5206\u533a\u7684\u8054\u90a6\u673a\u5236\u548cAI\u8f85\u52a9\u51b3\u7b56\u652f\u6301\uff1b\u91c7\u7528\u6df7\u5408\u6cbb\u7406\u6a21\u578b\uff0c\u7ed3\u5408\u96c6\u4e2d\u7b56\u7565\u6267\u884c\u4e0e\u5206\u6563\u6267\u884c\u5b66\u4e60\u3002", "result": "\u5f00\u53d1\u4e86CODECO\u67b6\u6784\u548c\u6838\u5fc3\u7ec4\u4ef6\uff0c\u8bbe\u8ba1\u4e86\u4ee3\u8868\u6027\u7f16\u6392\u5de5\u4f5c\u6d41\uff0c\u5e76\u5efa\u7acb\u4e86\u57fa\u4e8e\u8f6f\u4ef6\u7684\u53ef\u91cd\u590d\u5b9e\u9a8c\u6846\u67b6\u3002", "conclusion": "CODECO\u901a\u8fc7\u6570\u636e-\u8ba1\u7b97-\u7f51\u7edc\u534f\u540c\u7f16\u6392\u548c\u6df7\u5408\u6cbb\u7406\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5728\u8054\u90a6\u8fb9\u7f18\u4e91\u73af\u5883\u4e2d\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u7684\u540c\u65f6\u652f\u6301\u8fb9\u7f18\u81ea\u4e3b\u6027\u7684\u5e94\u7528\u7f16\u6392\u3002"}}
{"id": "2601.13424", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.13424", "abs": "https://arxiv.org/abs/2601.13424", "authors": ["Alexander Martinez Mendez", "Antonio J. Rubio-Montero", "Carlos J. Barrios H.", "Hern\u00e1n Asorey", "Rafael Mayo-Garc\u00eda", "Luis A. N\u00fa\u00f1ez"], "title": "Driving Computational Efficiency in Large-Scale Platforms using HPC Technologies", "comment": "Accepted and presented at CARLA 2025. To appear in Springer LNCS proceedings", "summary": "The Latin American Giant Observatory (LAGO) project utilizes extensive High-Performance Computing (HPC) resources for complex astroparticle physics simulations, making resource efficiency critical for scientific productivity and sustainability. This article presents a detailed analysis focused on quantifying and improving HPC resource utilization efficiency specifically within the LAGO computational environment. The core objective is to understand how LAGO's distinct computational workloads-characterized by a prevalent coarse-grained, task-parallel execution model-consume resources in practice. To achieve this, we analyze historical job accounting data from the EGI FedCloud platform, identifying primary workload categories (Monte Carlo simulations, data processing, user analysis/testing) and evaluating their performance using key efficiency metrics (CPU utilization, walltime utilization, and I/O patterns). Our analysis reveals significant patterns, including high CPU efficiency within individual simulation tasks contrasted with the distorting impact of short test jobs on aggregate metrics. This work pinpoints specific inefficiencies and provides data-driven insights into LAGO's HPC usage. The findings directly inform recommendations for optimizing resource requests, refining workflow management strategies, and guiding future efforts to enhance computational throughput, ultimately maximizing the scientific return from LAGO's HPC investments.", "AI": {"tldr": "LAGO\u9879\u76ee\u5206\u6790\u5176HPC\u8d44\u6e90\u5229\u7528\u6548\u7387\uff0c\u53d1\u73b0\u8499\u7279\u5361\u6d1b\u6a21\u62df\u4efb\u52a1CPU\u6548\u7387\u9ad8\uff0c\u4f46\u77ed\u6d4b\u8bd5\u4f5c\u4e1a\u4f1a\u626d\u66f2\u6574\u4f53\u6307\u6807\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u8d44\u6e90\u8bf7\u6c42\u548c\u5de5\u4f5c\u6d41\u7ba1\u7406\u7684\u5efa\u8bae\u3002", "motivation": "LAGO\u9879\u76ee\u4f7f\u7528\u5927\u91cfHPC\u8d44\u6e90\u8fdb\u884c\u590d\u6742\u7684\u5b87\u5b99\u7ebf\u7269\u7406\u6a21\u62df\uff0c\u8d44\u6e90\u6548\u7387\u5bf9\u79d1\u5b66\u4ea7\u51fa\u548c\u53ef\u6301\u7eed\u6027\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u91cf\u5316\u5e76\u6539\u8fdbHPC\u8d44\u6e90\u5229\u7528\u6548\u7387\uff0c\u7279\u522b\u662f\u9488\u5bf9LAGO\u7279\u6709\u7684\u7c97\u7c92\u5ea6\u3001\u4efb\u52a1\u5e76\u884c\u8ba1\u7b97\u8d1f\u8f7d\u3002", "method": "\u5206\u6790EGI FedCloud\u5e73\u53f0\u7684\u5386\u53f2\u4f5c\u4e1a\u8bb0\u8d26\u6570\u636e\uff0c\u8bc6\u522b\u4e3b\u8981\u5de5\u4f5c\u8d1f\u8f7d\u7c7b\u522b\uff08\u8499\u7279\u5361\u6d1b\u6a21\u62df\u3001\u6570\u636e\u5904\u7406\u3001\u7528\u6237\u5206\u6790/\u6d4b\u8bd5\uff09\uff0c\u4f7f\u7528CPU\u5229\u7528\u7387\u3001walltime\u5229\u7528\u7387\u548cI/O\u6a21\u5f0f\u7b49\u5173\u952e\u6548\u7387\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5206\u6790\u663e\u793a\u663e\u8457\u6a21\u5f0f\uff1a\u5355\u4e2a\u6a21\u62df\u4efb\u52a1\u5185CPU\u6548\u7387\u9ad8\uff0c\u4f46\u77ed\u6d4b\u8bd5\u4f5c\u4e1a\u5bf9\u805a\u5408\u6307\u6807\u6709\u626d\u66f2\u5f71\u54cd\u3002\u8bc6\u522b\u4e86\u5177\u4f53\u4f4e\u6548\u95ee\u9898\uff0c\u4e3aLAGO\u7684HPC\u4f7f\u7528\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u76f4\u63a5\u4e3a\u4f18\u5316\u8d44\u6e90\u8bf7\u6c42\u3001\u6539\u8fdb\u5de5\u4f5c\u6d41\u7ba1\u7406\u7b56\u7565\u63d0\u4f9b\u5efa\u8bae\uff0c\u6307\u5bfc\u672a\u6765\u63d0\u5347\u8ba1\u7b97\u541e\u5410\u91cf\u7684\u52aa\u529b\uff0c\u6700\u7ec8\u6700\u5927\u5316LAGO HPC\u6295\u8d44\u7684\u79d1\u5b66\u56de\u62a5\u3002"}}
{"id": "2601.13496", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.13496", "abs": "https://arxiv.org/abs/2601.13496", "authors": ["Anna Karanika", "Kai-Siang Wang", "Han-Ting Liang", "Shalni Sundram", "Indranil Gupta"], "title": "RASC: Enhancing Observability & Programmability in Smart Spaces", "comment": "16 pages, 19 figures. This paper is a preprint version of our upcoming paper of the same name in the USENIX Symposium on Networked Systems Design and Implementation (NSDI), 2026", "summary": "While RPCs form the bedrock of systems stacks, we posit that IoT device collections in smart spaces like homes, warehouses, and office buildings--which are all \"user-facing\"--require a more expressive abstraction. Orthogonal to prior work, which improved the reliability of IoT communication, our work focuses on improving the observability and programmability of IoT actions. We present the RASC (Request-Acknowledge-Start-Complete) abstraction, which provides acknowledgments at critical points after an IoT device action is initiated. RASC is a better fit for IoT actions, which naturally vary in length spatially (across devices) and temporally (across time, for a given device). RASC also enables the design of several new features: predicting action completion times accurately, detecting failures of actions faster, allowing fine-grained dependencies in programming, and scheduling. RASC is intended to be implemented atop today's available RPC mechanisms, rather than as a replacement. We integrated RASC into a popular and open-source IoT framework called Home Assistant. Our trace-driven evaluation finds that RASC meets latency SLOs, especially for long actions that last O(mins), which are common in smart spaces. Our scheduling policies for home automations (e.g., routines) outperform state-of-the-art counterparts by 10%-55%.", "AI": {"tldr": "\u63d0\u51faRASC\u62bd\u8c61\uff0c\u4e3a\u7269\u8054\u7f51\u8bbe\u5907\u52a8\u4f5c\u63d0\u4f9b\u8bf7\u6c42-\u786e\u8ba4-\u5f00\u59cb-\u5b8c\u6210\u56db\u4e2a\u5173\u952e\u70b9\u7684\u786e\u8ba4\u673a\u5236\uff0c\u63d0\u5347\u7269\u8054\u7f51\u7cfb\u7edf\u7684\u53ef\u89c2\u6d4b\u6027\u548c\u53ef\u7f16\u7a0b\u6027\u3002", "motivation": "\u73b0\u6709RPC\u673a\u5236\u867d\u7136\u6784\u6210\u7cfb\u7edf\u6808\u7684\u57fa\u7840\uff0c\u4f46\u5bf9\u4e8e\u667a\u80fd\u5bb6\u5c45\u3001\u4ed3\u5e93\u3001\u529e\u516c\u697c\u7b49\"\u9762\u5411\u7528\u6237\"\u7684\u7269\u8054\u7f51\u8bbe\u5907\u96c6\u5408\uff0c\u9700\u8981\u66f4\u5bcc\u8868\u8fbe\u529b\u7684\u62bd\u8c61\u3002\u4ee5\u5f80\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u7269\u8054\u7f51\u901a\u4fe1\u7684\u53ef\u9760\u6027\uff0c\u800c\u672c\u7814\u7a76\u805a\u7126\u4e8e\u63d0\u5347\u7269\u8054\u7f51\u52a8\u4f5c\u7684\u53ef\u89c2\u6d4b\u6027\u548c\u53ef\u7f16\u7a0b\u6027\u3002", "method": "\u63d0\u51faRASC\uff08\u8bf7\u6c42-\u786e\u8ba4-\u5f00\u59cb-\u5b8c\u6210\uff09\u62bd\u8c61\uff0c\u5728\u7269\u8054\u7f51\u8bbe\u5907\u52a8\u4f5c\u542f\u52a8\u540e\u7684\u5173\u952e\u8282\u70b9\u63d0\u4f9b\u786e\u8ba4\u673a\u5236\u3002RASC\u8bbe\u8ba1\u4e3a\u5728\u73b0\u6709RPC\u673a\u5236\u4e4b\u4e0a\u5b9e\u73b0\uff0c\u800c\u975e\u66ff\u4ee3\u65b9\u6848\u3002\u5c06\u5176\u96c6\u6210\u5230\u6d41\u884c\u7684\u5f00\u6e90\u7269\u8054\u7f51\u6846\u67b6Home Assistant\u4e2d\u3002", "result": "RASC\u80fd\u591f\u6ee1\u8db3\u5ef6\u8fdfSLO\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u667a\u80fd\u7a7a\u95f4\u4e2d\u5e38\u89c1\u7684\u6301\u7eed\u6570\u5206\u949f\u7684\u957f\u52a8\u4f5c\u3002\u57fa\u4e8eRASC\u7684\u5bb6\u5ead\u81ea\u52a8\u5316\u8c03\u5ea6\u7b56\u7565\u6bd4\u6700\u5148\u8fdb\u7684\u5bf9\u5e94\u65b9\u6848\u6027\u80fd\u63d0\u534710%-55%\u3002", "conclusion": "RASC\u62bd\u8c61\u66f4\u9002\u5408\u7269\u8054\u7f51\u52a8\u4f5c\u7684\u81ea\u7136\u7279\u6027\uff08\u8bbe\u5907\u95f4\u7a7a\u95f4\u53d8\u5316\u548c\u65f6\u95f4\u4e0a\u65f6\u95f4\u53d8\u5316\uff09\uff0c\u652f\u6301\u51c6\u786e\u9884\u6d4b\u52a8\u4f5c\u5b8c\u6210\u65f6\u95f4\u3001\u66f4\u5feb\u68c0\u6d4b\u52a8\u4f5c\u5931\u8d25\u3001\u7ec6\u7c92\u5ea6\u7f16\u7a0b\u4f9d\u8d56\u548c\u8c03\u5ea6\u7b49\u65b0\u529f\u80fd\u3002"}}
{"id": "2601.13579", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.13579", "abs": "https://arxiv.org/abs/2601.13579", "authors": ["Hanlin Zhou", "Huah Yong Chan", "Shun Yao Zhang", "Meie Lin", "Jingfei Ni"], "title": "A Kubernetes custom scheduler based on reinforcement learning for compute-intensive pods", "comment": null, "summary": "With the rise of cloud computing and lightweight containers, Docker has emerged as a leading technology for rapid service deployment, with Kubernetes responsible for pod orchestration. However, for compute-intensive workloads-particularly web services executing containerized machine-learning training-the default Kubernetes scheduler does not always achieve optimal placement. To address this, we propose two custom, reinforcement-learning-based schedulers, SDQN and SDQN-n, both built on the Deep Q-Network (DQN) framework. In compute-intensive scenarios, these models outperform the default Kubernetes scheduler as well as Transformer-and LSTM-based alternatives, reducing average CPU utilization per cluster node by 10%, and by over 20% when using SDQN-n. Moreover, our results show that SDQN-n approach of consolidating pods onto fewer nodes further amplifies resource savings and helps advance greener, more energy-efficient data centers.Therefore, pod scheduling must employ different strategies tailored to each scenario in order to achieve better performance.Since the reinforcement-learning components of the SDQN and SDQN-n architectures proposed in this paper can be easily tuned by adjusting their parameters, they can accommodate the requirements of various future scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6df1\u5ea6Q\u7f51\u7edc\u7684Kubernetes\u81ea\u5b9a\u4e49\u8c03\u5ea6\u5668SDQN\u548cSDQN-n\uff0c\u5728\u8ba1\u7b97\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u6bd4\u9ed8\u8ba4\u8c03\u5ea6\u5668\u6027\u80fd\u66f4\u4f18\uff0c\u80fd\u964d\u4f4e\u8282\u70b9CPU\u5229\u7528\u738710-20%\uff0c\u5b9e\u73b0\u66f4\u8282\u80fd\u7684\u6570\u636e\u4e2d\u5fc3\u3002", "motivation": "\u968f\u7740\u4e91\u8ba1\u7b97\u548c\u8f7b\u91cf\u7ea7\u5bb9\u5668\u6280\u672f\u7684\u53d1\u5c55\uff0cDocker\u6210\u4e3a\u5feb\u901f\u670d\u52a1\u90e8\u7f72\u7684\u4e3b\u6d41\u6280\u672f\uff0cKubernetes\u8d1f\u8d23pod\u7f16\u6392\u3002\u4f46\u5bf9\u4e8e\u8ba1\u7b97\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\uff08\u7279\u522b\u662f\u6267\u884c\u5bb9\u5668\u5316\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u7684Web\u670d\u52a1\uff09\uff0c\u9ed8\u8ba4\u7684Kubernetes\u8c03\u5ea6\u5668\u65e0\u6cd5\u5b9e\u73b0\u6700\u4f18\u7684\u8d44\u6e90\u5206\u914d\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u6846\u67b6\u7684\u81ea\u5b9a\u4e49\u5f3a\u5316\u5b66\u4e60\u8c03\u5ea6\u5668\uff1aSDQN\u548cSDQN-n\u3002SDQN-n\u91c7\u7528\u4e86\u5c06pod\u6574\u5408\u5230\u66f4\u5c11\u8282\u70b9\u4e0a\u7684\u7b56\u7565\uff0c\u8fdb\u4e00\u6b65\u653e\u5927\u8d44\u6e90\u8282\u7701\u6548\u679c\u3002", "result": "\u5728\u8ba1\u7b97\u5bc6\u96c6\u578b\u573a\u666f\u4e0b\uff0c\u8fd9\u4e24\u79cd\u6a21\u578b\u7684\u8868\u73b0\u4f18\u4e8e\u9ed8\u8ba4Kubernetes\u8c03\u5ea6\u5668\u4ee5\u53ca\u57fa\u4e8eTransformer\u548cLSTM\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5c06\u96c6\u7fa4\u8282\u70b9\u7684\u5e73\u5747CPU\u5229\u7528\u7387\u964d\u4f4e\u4e8610%\uff0c\u4f7f\u7528SDQN-n\u65f6\u964d\u4f4e\u8d85\u8fc720%\u3002SDQN-n\u901a\u8fc7\u5c06pod\u6574\u5408\u5230\u66f4\u5c11\u8282\u70b9\u4e0a\u8fdb\u4e00\u6b65\u653e\u5927\u4e86\u8d44\u6e90\u8282\u7701\u3002", "conclusion": "pod\u8c03\u5ea6\u5fc5\u987b\u6839\u636e\u4e0d\u540c\u573a\u666f\u91c7\u7528\u5b9a\u5236\u5316\u7b56\u7565\u4ee5\u83b7\u5f97\u66f4\u597d\u6027\u80fd\u3002\u672c\u6587\u63d0\u51fa\u7684SDQN\u548cSDQN-n\u67b6\u6784\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u7ec4\u4ef6\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u53c2\u6570\u8f7b\u677e\u8c03\u4f18\uff0c\u80fd\u591f\u9002\u5e94\u5404\u79cd\u672a\u6765\u573a\u666f\u7684\u9700\u6c42\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u66f4\u7eff\u8272\u3001\u66f4\u8282\u80fd\u7684\u6570\u636e\u4e2d\u5fc3\u53d1\u5c55\u3002"}}
{"id": "2601.13817", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13817", "abs": "https://arxiv.org/abs/2601.13817", "authors": ["Haitao Zhao", "Xiaoyu Tang", "Bo Xu", "Jinlong Sun", "Linghao Zhang"], "title": "Device Association and Resource Allocation for Hierarchical Split Federated Learning in Space-Air-Ground Integrated Network", "comment": null, "summary": "6G facilitates deployment of Federated Learning (FL) in the Space-Air-Ground Integrated Network (SAGIN), yet FL confronts challenges such as resource constrained and unbalanced data distribution. To address these issues, this paper proposes a Hierarchical Split Federated Learning (HSFL) framework and derives its upper bound of loss function. To minimize the weighted sum of training loss and latency, we formulate a joint optimization problem that integrates device association, model split layer selection, and resource allocation. We decompose the original problem into several subproblems, where an iterative optimization algorithm for device association and resource allocation based on brute-force split point search is proposed. Simulation results demonstrate that the proposed algorithm can effectively balance training efficiency and model accuracy for FL in SAGIN.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u5206\u5272\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8bbe\u5907\u5173\u8054\u3001\u6a21\u578b\u5206\u5272\u5c42\u9009\u62e9\u548c\u8d44\u6e90\u5206\u914d\uff0c\u5728SAGIN\u4e2d\u5e73\u8861\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u7cbe\u5ea6", "motivation": "6G\u652f\u6301\u5728\u7a7a\u5929\u5730\u4e00\u4f53\u5316\u7f51\u7edc\u4e2d\u90e8\u7f72\u8054\u90a6\u5b66\u4e60\uff0c\u4f46\u9762\u4e34\u8d44\u6e90\u53d7\u9650\u548c\u6570\u636e\u5206\u5e03\u4e0d\u5e73\u8861\u7684\u6311\u6218\uff0c\u9700\u8981\u8bbe\u8ba1\u9ad8\u6548\u7684\u5b66\u4e60\u6846\u67b6", "method": "\u63d0\u51fa\u5206\u5c42\u5206\u5272\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u63a8\u5bfc\u635f\u5931\u51fd\u6570\u4e0a\u754c\uff0c\u5c06\u8054\u5408\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u66b4\u529b\u641c\u7d22\u5206\u5272\u70b9\u7684\u8fed\u4ee3\u4f18\u5316\u7b97\u6cd5", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u80fd\u6709\u6548\u5e73\u8861SAGIN\u4e2d\u8054\u90a6\u5b66\u4e60\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u7cbe\u5ea6", "conclusion": "HSFL\u6846\u67b6\u548c\u4f18\u5316\u7b97\u6cd5\u4e3a\u89e3\u51b3SAGIN\u4e2d\u8054\u90a6\u5b66\u4e60\u7684\u8d44\u6e90\u7ea6\u675f\u548c\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.13994", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13994", "abs": "https://arxiv.org/abs/2601.13994", "authors": ["Mingyuan Chi"], "title": "torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch", "comment": null, "summary": "Industrial scientific computing predominantly uses sparse matrices to represent unstructured data -- finite element meshes, graphs, point clouds. We present \\torchsla{}, an open-source PyTorch library that enables GPU-accelerated, scalable, and differentiable sparse linear algebra. The library addresses three fundamental challenges: (1) GPU acceleration for sparse linear solves, nonlinear solves (Newton, Picard, Anderson), and eigenvalue computation; (2) Multi-GPU scaling via domain decomposition with halo exchange, reaching \\textbf{400 million DOF linear solve on 3 GPUs}; and (3) Adjoint-based differentiation} achieving $\\mathcal{O}(1)$ computational graph nodes (for autograd) and $\\mathcal{O}(\\text{nnz})$ memory -- independent of solver iterations. \\torchsla{} supports multiple backends (SciPy, cuDSS, PyTorch-native) and seamlessly integrates with PyTorch autograd for end-to-end differentiable simulations. Code is available at https://github.com/walkerchi/torch-sla.", "AI": {"tldr": "TorchSLA\u662f\u4e00\u4e2a\u5f00\u6e90\u7684PyTorch\u5e93\uff0c\u63d0\u4f9bGPU\u52a0\u901f\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u5fae\u5206\u7684\u7a00\u758f\u7ebf\u6027\u4ee3\u6570\u8ba1\u7b97\uff0c\u652f\u6301\u5927\u89c4\u6a21\u79d1\u5b66\u8ba1\u7b97\u3002", "motivation": "\u5de5\u4e1a\u79d1\u5b66\u8ba1\u7b97\u4e3b\u8981\u4f7f\u7528\u7a00\u758f\u77e9\u9635\u8868\u793a\u975e\u7ed3\u6784\u5316\u6570\u636e\uff08\u6709\u9650\u5143\u7f51\u683c\u3001\u56fe\u3001\u70b9\u4e91\uff09\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u7f3a\u4e4fGPU\u52a0\u901f\u3001\u591aGPU\u6269\u5c55\u548c\u9ad8\u6548\u53ef\u5fae\u5206\u8ba1\u7b97\u7684\u80fd\u529b\u3002", "method": "\u5f00\u53d1TorchSLA\u5e93\uff0c\u5b9e\u73b0\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1) GPU\u52a0\u901f\u7684\u7a00\u758f\u7ebf\u6027\u6c42\u89e3\u3001\u975e\u7ebf\u6027\u6c42\u89e3\u548c\u7279\u5f81\u503c\u8ba1\u7b97\uff1b2) \u901a\u8fc7\u57df\u5206\u89e3\u548chalo\u4ea4\u6362\u5b9e\u73b0\u591aGPU\u6269\u5c55\uff1b3) \u57fa\u4e8e\u4f34\u968f\u65b9\u6cd5\u7684\u5fae\u5206\uff0c\u5b9e\u73b0O(1)\u8ba1\u7b97\u56fe\u8282\u70b9\u548cO(nnz)\u5185\u5b58\u6d88\u8017\u3002", "result": "\u57283\u4e2aGPU\u4e0a\u5b9e\u73b0\u4e864\u4ebf\u81ea\u7531\u5ea6\u7ebf\u6027\u6c42\u89e3\uff0c\u652f\u6301\u591a\u79cd\u540e\u7aef\uff08SciPy\u3001cuDSS\u3001PyTorch-native\uff09\uff0c\u5e76\u4e0ePyTorch autograd\u65e0\u7f1d\u96c6\u6210\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u6a21\u62df\u3002", "conclusion": "TorchSLA\u4e3a\u5927\u89c4\u6a21\u79d1\u5b66\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u5fae\u5206\u7684\u7a00\u758f\u7ebf\u6027\u4ee3\u6570\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u7684\u7a7a\u767d\u3002"}}
