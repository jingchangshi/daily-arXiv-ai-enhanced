{"id": "2511.13751", "categories": ["cs.DC", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.13751", "abs": "https://arxiv.org/abs/2511.13751", "authors": ["Shinnung Jeong", "Chihyo Ahn", "Huanzhi Pu", "Jisheng Zhao", "Hyesoon Kim", "Blaise Tine"], "title": "Inside VOLT: Designing an Open-Source GPU Compiler", "comment": "11 pages, 10 figures, two tables, two algorithms", "summary": "Recent efforts in open-source GPU research are opening new avenues in a domain that has long been tightly coupled with a few commercial vendors. Emerging open GPU architectures define SIMT functionality through their own ISAs, but executing existing GPU programs and optimizing performance on these ISAs relies on a compiler framework that is technically complex and often undercounted in open hardware development costs.\n  To address this challenge, the Vortex-Optimized Lightweight Toolchain (VOLT) has been proposed. This paper presents its design principles, overall structure, and the key compiler transformations required to support SIMT execution on Vortex. VOLT enables SIMT code generation and optimization across multiple levels of abstraction through a hierarchical design that accommodates diverse front-end languages and open GPU hardware. To ensure extensibility as GPU architectures evolve, VOLT centralizes fundamental SIMT-related analyses and optimizations in the middle-end, allowing them to be reused across front-ends and easily adapted to emerging open-GPU variants. Through two case studies on ISA extensions and host-runtime API, this paper also demonstrates how VOLT can support extensions", "AI": {"tldr": "VOLT\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7f16\u8bd1\u5668\u5de5\u5177\u94fe\uff0c\u65e8\u5728\u89e3\u51b3\u5f00\u6e90GPU\u67b6\u6784\u4e2dSIMT\u529f\u80fd\u6267\u884c\u548c\u6027\u80fd\u4f18\u5316\u7684\u6280\u672f\u6311\u6218\uff0c\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u652f\u6301\u591a\u62bd\u8c61\u7ea7\u522b\u7684\u4ee3\u7801\u751f\u6210\u548c\u4f18\u5316\u3002", "motivation": "\u5f00\u6e90GPU\u67b6\u6784\u867d\u7136\u5b9a\u4e49\u4e86SIMT\u529f\u80fd\uff0c\u4f46\u6267\u884c\u73b0\u6709GPU\u7a0b\u5e8f\u548c\u4f18\u5316\u6027\u80fd\u9700\u8981\u590d\u6742\u7684\u7f16\u8bd1\u5668\u6846\u67b6\uff0c\u8fd9\u5728\u5f00\u6e90\u786c\u4ef6\u5f00\u53d1\u4e2d\u5e38\u88ab\u4f4e\u4f30\u3002", "method": "\u91c7\u7528\u5206\u5c42\u8bbe\u8ba1\uff0c\u5728\u4e2d\u7aef\u96c6\u4e2dSIMT\u76f8\u5173\u5206\u6790\u548c\u4f18\u5316\uff0c\u652f\u6301\u591a\u79cd\u524d\u7aef\u8bed\u8a00\u548c\u5f00\u6e90GPU\u786c\u4ef6\uff0c\u786e\u4fdd\u53ef\u6269\u5c55\u6027\u3002", "result": "VOLT\u80fd\u591f\u652f\u6301SIMT\u4ee3\u7801\u751f\u6210\u548c\u4f18\u5316\uff0c\u5e76\u901a\u8fc7ISA\u6269\u5c55\u548c\u4e3b\u673a\u8fd0\u884c\u65f6API\u7684\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u6269\u5c55\u80fd\u529b\u3002", "conclusion": "VOLT\u4e3a\u5f00\u6e90GPU\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7f16\u8bd1\u5668\u6846\u67b6\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u53d1\u5c55\u7684GPU\u67b6\u6784\u9700\u6c42\u3002"}}
{"id": "2511.13950", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.13950", "abs": "https://arxiv.org/abs/2511.13950", "authors": ["Lei Zhao", "Luca Buonanno", "Archit Gajjar", "John Moon", "Aishwarya Natarajan", "Sergey Serebryakov", "Ron M. Roth", "Xia Sheng", "Youtao Zhang", "Paolo Faraboschi", "Jim Ignowski", "Giacomo Pedretti"], "title": "NL-DPE: An Analog In-memory Non-Linear Dot Product Engine for Efficient CNN and LLM Inference", "comment": null, "summary": "Resistive Random Access Memory (RRAM) based in-memory computing (IMC) accelerators offer significant performance and energy advantages for deep neural networks (DNNs), but face three major limitations: (1) they support only \\textit{static} dot-product operations and cannot accelerate arbitrary non-linear functions or data-dependent multiplications essential to modern LLMs; (2) they demand large, power-hungry analog-to-digital converter (ADC) circuits; and (3) mapping model weights to device conductance introduces errors from cell nonidealities. These challenges hinder scalable and accurate IMC acceleration as models grow.\n  We propose NL-DPE, a Non-Linear Dot Product Engine that overcomes these barriers. NL-DPE augments crosspoint arrays with RRAM-based Analog Content Addressable Memory (ACAM) to execute arbitrary non-linear functions and data-dependent matrix multiplications in the analog domain by transforming them into decision trees, fully eliminating ADCs. To address device noise, NL-DPE uses software-based Noise Aware Fine-tuning (NAF), requiring no in-device calibration. Experiments show that NL-DPE delivers 28X energy efficiency and 249X speedup over a GPU baseline, and 22X energy efficiency and 245X speedup over existing IMC accelerators, while maintaining high accuracy.", "AI": {"tldr": "NL-DPE\u662f\u4e00\u79cd\u975e\u7ebf\u6027\u70b9\u79ef\u5f15\u64ce\uff0c\u901a\u8fc7\u7ed3\u5408RRAM\u4ea4\u53c9\u9635\u5217\u548c\u6a21\u62df\u5185\u5bb9\u53ef\u5bfb\u5740\u5b58\u50a8\u5668\uff0c\u5728\u6a21\u62df\u57df\u6267\u884c\u4efb\u610f\u975e\u7ebf\u6027\u51fd\u6570\u548c\u6570\u636e\u76f8\u5173\u77e9\u9635\u4e58\u6cd5\uff0c\u5b8c\u5168\u6d88\u9664ADC\u9700\u6c42\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\u548c\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edfRRAM\u5185\u5b58\u8ba1\u7b97\u52a0\u901f\u5668\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u4ec5\u652f\u6301\u9759\u6001\u70b9\u79ef\u8fd0\u7b97\u3001\u9700\u8981\u5927\u529f\u8017ADC\u7535\u8def\u3001\u8bbe\u5907\u975e\u7406\u60f3\u6027\u5f15\u5165\u8bef\u5dee\uff0c\u8fd9\u4e9b\u9650\u5236\u4e86\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u6269\u5c55\u548c\u51c6\u786e\u52a0\u901f\u3002", "method": "NL-DPE\u901a\u8fc7RRAM\u4ea4\u53c9\u9635\u5217\u4e0e\u6a21\u62df\u5185\u5bb9\u53ef\u5bfb\u5740\u5b58\u50a8\u5668(ACAM)\u7ed3\u5408\uff0c\u5c06\u975e\u7ebf\u6027\u51fd\u6570\u548c\u6570\u636e\u76f8\u5173\u4e58\u6cd5\u8f6c\u6362\u4e3a\u51b3\u7b56\u6811\u5728\u6a21\u62df\u57df\u6267\u884c\uff0c\u5e76\u4f7f\u7528\u8f6f\u4ef6\u566a\u58f0\u611f\u77e5\u5fae\u8c03(NAF)\u89e3\u51b3\u8bbe\u5907\u566a\u58f0\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u663e\u793aNL-DPE\u76f8\u6bd4GPU\u57fa\u7ebf\u5b9e\u73b028\u500d\u80fd\u6548\u548c249\u500d\u52a0\u901f\uff0c\u76f8\u6bd4\u73b0\u6709IMC\u52a0\u901f\u5668\u5b9e\u73b022\u500d\u80fd\u6548\u548c245\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "NL-DPE\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edfRRAM\u52a0\u901f\u5668\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u80fd\u6548\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13727", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13727", "abs": "https://arxiv.org/abs/2511.13727", "authors": ["Sophie Wenning"], "title": "Boosting performance: Gradient Clock Synchronisation with two-way measured links", "comment": "Master's thesis", "summary": "This master thesis extends the formal model of the GCS algorithm as presented by (Fan and Lynch 2004, 325), (Lenzen, Locher and Wattenhofer 2008, 510) and (F\u00fcgger et al. 2023) to operate under implementation-near assumptions by replacing the one-way measurement paradigm assumed in prior work by the two-way measurement paradigm. With this change of paradigm, we remove many restrictions previously enforced to allow provable performance. Most notability, while maintaining the core behaviour of GCS, we: 1. Lift the requirement for unitary link lengths and thereby create a realistic model for flexible deployment of implementations of GCS in practice. 2. Provide a formal model of frequency sources assumed in prior work. 3. Perform a fine grained distinction between the different components of the algorithm's estimation error and globally reduce its impact by multiple orders of magnitude. 4. Significantly reduce the contribution of the uncertainty to the algorithm's estimation error to be in the range of 10\\% to 0,1\\% of the delay per link instead of being in the oder of the delay per link as in prior work and show matching upper bounds on the local and global skew of GCS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06GCS\u7b97\u6cd5\u7684\u5f62\u5f0f\u6a21\u578b\u6269\u5c55\u5230\u5b9e\u73b0\u8fd1\u4f3c\u7684\u5047\u8bbe\u4e0b\uff0c\u901a\u8fc7\u5c06\u5355\u5411\u6d4b\u91cf\u8303\u5f0f\u66ff\u6362\u4e3a\u53cc\u5411\u6d4b\u91cf\u8303\u5f0f\uff0c\u79fb\u9664\u4e86\u8bb8\u591a\u9650\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u4e86GCS\u7684\u6838\u5fc3\u884c\u4e3a\u3002", "motivation": "\u6269\u5c55GCS\u7b97\u6cd5\u7684\u5f62\u5f0f\u6a21\u578b\u4ee5\u5728\u5b9e\u73b0\u8fd1\u4f3c\u7684\u5047\u8bbe\u4e0b\u8fd0\u884c\uff0c\u901a\u8fc7\u6539\u53d8\u6d4b\u91cf\u8303\u5f0f\u6765\u79fb\u9664\u5148\u524d\u5de5\u4f5c\u4e2d\u4e3a\u8bc1\u660e\u6027\u80fd\u800c\u65bd\u52a0\u7684\u9650\u5236\uff0c\u4f7f\u6a21\u578b\u66f4\u8d34\u8fd1\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u5c06\u5355\u5411\u6d4b\u91cf\u8303\u5f0f\u66ff\u6362\u4e3a\u53cc\u5411\u6d4b\u91cf\u8303\u5f0f\uff0c\u53d6\u6d88\u7edf\u4e00\u94fe\u8def\u957f\u5ea6\u7684\u8981\u6c42\uff0c\u63d0\u4f9b\u9891\u7387\u6e90\u7684\u5f62\u5f0f\u6a21\u578b\uff0c\u5bf9\u7b97\u6cd5\u4f30\u8ba1\u8bef\u5dee\u7684\u4e0d\u540c\u7ec4\u6210\u90e8\u5206\u8fdb\u884c\u7ec6\u7c92\u5ea6\u533a\u5206\u3002", "result": "\u663e\u8457\u964d\u4f4e\u4e86\u4e0d\u786e\u5b9a\u6027\u5bf9\u7b97\u6cd5\u4f30\u8ba1\u8bef\u5dee\u7684\u8d21\u732e\uff0c\u4ece\u6bcf\u4e2a\u94fe\u8def\u5ef6\u8fdf\u7684\u6570\u91cf\u7ea7\u964d\u4f4e\u5230\u6bcf\u4e2a\u94fe\u8def\u5ef6\u8fdf\u768410%\u52300.1%\uff0c\u5e76\u7ed9\u51fa\u4e86GCS\u5c40\u90e8\u548c\u5168\u5c40\u504f\u5dee\u7684\u5339\u914d\u4e0a\u754c\u3002", "conclusion": "\u901a\u8fc7\u91c7\u7528\u53cc\u5411\u6d4b\u91cf\u8303\u5f0f\uff0c\u5728\u4fdd\u6301GCS\u6838\u5fc3\u884c\u4e3a\u7684\u540c\u65f6\uff0c\u521b\u5efa\u4e86\u66f4\u73b0\u5b9e\u7684\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7b97\u6cd5\u6027\u80fd\uff0c\u4f7fGCS\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u66f4\u52a0\u7075\u6d3b\u53ef\u884c\u3002"}}
{"id": "2511.14202", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.14202", "abs": "https://arxiv.org/abs/2511.14202", "authors": ["Weiping Yang", "Shilin Zhou", "Hui Xu", "Yujiao Nie", "Qimin Zhou", "Zhiwei Li", "Changlin Chen"], "title": "A Bit Level Weight Reordering Strategy Based on Column Similarity to Explore Weight Sparsity in RRAM-based NN Accelerator", "comment": "accepted by ICPADS 2025 (International Conference on Parallel and Distributed Systems)", "summary": "Compute-in-Memory (CIM) and weight sparsity are two effective techniques to reduce data movement during Neural Network (NN) inference. However, they can hardly be employed in the same accelerator simultaneously because CIM requires structural compute patterns which are disrupted in sparse NNs. In this paper, we partially solve this issue by proposing a bit level weight reordering strategy which can realize compact mapping of sparse NN weight matrices onto Resistive Random Access Memory (RRAM) based NN Accelerators (RRAM-Acc). In specific, when weights are mapped to RRAM crossbars in a binary complement manner, we can observe that, which can also be mathematically proven, bit-level sparsity and similarity commonly exist in the crossbars. The bit reordering method treats bit sparsity as a special case of bit similarity, reserve only one column in a pair of columns that have identical bit values, and then map the compressed weight matrices into Operation Units (OU). The performance of our design is evaluated with typical NNs. Simulation results show a 61.24% average performance improvement and 1.51x-2.52x energy savings under different sparsity ratios, with only slight overhead compared to the state-of-the-art design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4d\u7ea7\u6743\u91cd\u91cd\u6392\u5e8f\u7b56\u7565\uff0c\u80fd\u591f\u5728RRAM\u52a0\u901f\u5668\u4e2d\u540c\u65f6\u5b9e\u73b0\u8ba1\u7b97\u5185\u5b58\u548c\u6743\u91cd\u7a00\u758f\u6027\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u795e\u7ecf\u7f51\u7edc\u5728CIM\u52a0\u901f\u5668\u4e2d\u7ed3\u6784\u6a21\u5f0f\u88ab\u7834\u574f\u7684\u95ee\u9898\u3002", "motivation": "\u8ba1\u7b97\u5185\u5b58(CIM)\u548c\u6743\u91cd\u7a00\u758f\u6027\u90fd\u662f\u51cf\u5c11\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u4e2d\u6570\u636e\u79fb\u52a8\u7684\u6709\u6548\u6280\u672f\uff0c\u4f46\u5f88\u96be\u5728\u540c\u4e00\u52a0\u901f\u5668\u4e2d\u540c\u65f6\u4f7f\u7528\uff0c\u56e0\u4e3a\u7a00\u758f\u795e\u7ecf\u7f51\u7edc\u4f1a\u7834\u574fCIM\u6240\u9700\u7684\u7ed3\u6784\u5316\u8ba1\u7b97\u6a21\u5f0f\u3002", "method": "\u91c7\u7528\u4f4d\u7ea7\u6743\u91cd\u91cd\u6392\u5e8f\u7b56\u7565\uff0c\u5c06\u7a00\u758f\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u77e9\u9635\u7d27\u51d1\u6620\u5c04\u5230RRAM\u52a0\u901f\u5668\u4e0a\u3002\u8be5\u65b9\u6cd5\u5c06\u4f4d\u7a00\u758f\u6027\u89c6\u4e3a\u4f4d\u76f8\u4f3c\u6027\u7684\u7279\u4f8b\uff0c\u4fdd\u7559\u5177\u6709\u76f8\u540c\u4f4d\u503c\u7684\u5217\u5bf9\u4e2d\u4ec5\u4e00\u5217\uff0c\u7136\u540e\u5c06\u538b\u7f29\u540e\u7684\u6743\u91cd\u77e9\u9635\u6620\u5c04\u5230\u64cd\u4f5c\u5355\u5143\u4e2d\u3002", "result": "\u5728\u5178\u578b\u795e\u7ecf\u7f51\u7edc\u4e0a\u7684\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u534761.24%\uff0c\u5728\u4e0d\u540c\u7a00\u758f\u5ea6\u4e0b\u5b9e\u73b01.51x-2.52x\u7684\u80fd\u8017\u8282\u7701\uff0c\u4e14\u4e0e\u6700\u5148\u8fdb\u8bbe\u8ba1\u76f8\u6bd4\u4ec5\u6709\u8f7b\u5fae\u5f00\u9500\u3002", "conclusion": "\u8be5\u4f4d\u7ea7\u91cd\u6392\u5e8f\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86CIM\u4e0e\u6743\u91cd\u7a00\u758f\u6027\u96be\u4ee5\u540c\u65f6\u4f7f\u7528\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86RRAM\u52a0\u901f\u5668\u7684\u6027\u80fd\u548c\u80fd\u6548\u3002"}}
{"id": "2511.13728", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13728", "abs": "https://arxiv.org/abs/2511.13728", "authors": ["Maximilian Reisecker", "Cynthia Marcelino", "Thomas Pusztai", "Stefan Nastic"], "title": "Gaia: Hybrid Hardware Acceleration for Serverless AI in the 3D Compute Continuum", "comment": "In IEEE ACM 12th International Conference on Big Data Computing, Applications and Technologies (BDCAT 25), 2025, Nantes, France", "summary": "Serverless computing offers elastic scaling and pay-per-use execution, making it well-suited for AI workloads. As these workloads run in heterogeneous environments such as the Edge-Cloud-Space 3D Continuum, they often require intensive parallel computation, which GPUs can perform far more efficiently than CPUs. However, current platforms struggle to manage hardware acceleration effectively, as static user-device assignments fail to ensure SLO compliance under varying loads or placements, and one-time dynamic selections often lead to suboptimal or cost-inefficient configurations. To address these issues, we present Gaia, a GPU-as-a-service model and architecture that makes hardware acceleration a platform concern. Gaia combines (i) a lightweight Execution Mode Identifier that inspects function code at deploy time to emit one of four execution modes, and a Dynamic Function Runtime that continuously reevaluates user-defined SLOs to promote or demote between CPU- and GPU backends. Our evaluation shows that it seamlessly selects the best hardware acceleration for the workload, reducing end-to-end latency by up to 95%. These results indicate that Gaia enables SLO-aware, cost-efficient acceleration for serverless AI across heterogeneous environments.", "AI": {"tldr": "Gaia\u662f\u4e00\u4e2aGPU\u5373\u670d\u52a1\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u6267\u884c\u6a21\u5f0f\u8bc6\u522b\u548c\u8fd0\u884c\u65f6\u8bc4\u4f30\uff0c\u4e3a\u5f02\u6784\u73af\u5883\u4e2d\u7684\u65e0\u670d\u52a1\u5668AI\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9bSLO\u611f\u77e5\u3001\u6210\u672c\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u3002", "motivation": "\u5f53\u524d\u5e73\u53f0\u5728\u7ba1\u7406\u786c\u4ef6\u52a0\u901f\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9759\u6001\u7528\u6237-\u8bbe\u5907\u5206\u914d\u65e0\u6cd5\u5728\u53d8\u5316\u8d1f\u8f7d\u6216\u4f4d\u7f6e\u4e0b\u4fdd\u8bc1SLO\u5408\u89c4\u6027\uff0c\u4e00\u6b21\u6027\u52a8\u6001\u9009\u62e9\u5f80\u5f80\u5bfc\u81f4\u6b21\u4f18\u6216\u6210\u672c\u4f4e\u6548\u7684\u914d\u7f6e\u3002", "method": "Gaia\u7ed3\u5408\u8f7b\u91cf\u7ea7\u6267\u884c\u6a21\u5f0f\u8bc6\u522b\u5668\uff08\u5728\u90e8\u7f72\u65f6\u68c0\u67e5\u51fd\u6570\u4ee3\u7801\u5e76\u53d1\u51fa\u56db\u79cd\u6267\u884c\u6a21\u5f0f\u4e4b\u4e00\uff09\u548c\u52a8\u6001\u51fd\u6570\u8fd0\u884c\u65f6\uff08\u6301\u7eed\u91cd\u65b0\u8bc4\u4f30\u7528\u6237\u5b9a\u4e49\u7684SLO\u4ee5\u5728CPU\u548cGPU\u540e\u7aef\u4e4b\u95f4\u8fdb\u884c\u5347\u7ea7\u6216\u964d\u7ea7\uff09\u3002", "result": "\u8bc4\u4f30\u663e\u793aGaia\u80fd\u591f\u65e0\u7f1d\u9009\u62e9\u6700\u9002\u5408\u5de5\u4f5c\u8d1f\u8f7d\u7684\u786c\u4ef6\u52a0\u901f\uff0c\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe95%\u3002", "conclusion": "Gaia\u80fd\u591f\u5728\u5f02\u6784\u73af\u5883\u4e2d\u4e3a\u65e0\u670d\u52a1\u5668AI\u5b9e\u73b0SLO\u611f\u77e5\u3001\u6210\u672c\u9ad8\u6548\u7684\u52a0\u901f\u3002"}}
{"id": "2511.13738", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.13738", "abs": "https://arxiv.org/abs/2511.13738", "authors": ["Hyunseok Kwak", "Kyeongwon Lee", "Kyeongpil Min", "Chaebin Jung", "Woojoo Lee"], "title": "TT-Edge: A Hardware-Software Co-Design for Energy-Efficient Tensor-Train Decomposition on Edge AI", "comment": "8 pages, 6 figures, 4 Tables, DATE 2026 accepted paper", "summary": "The growing demands of distributed learning on resource constrained edge devices underscore the importance of efficient on device model compression. Tensor Train Decomposition (TTD) offers high compression ratios with minimal accuracy loss, yet repeated singular value decompositions (SVDs) and matrix multiplications can impose significant latency and energy costs on low power processors. In this work, we present TT-Edge, a hardware software co designed framework aimed at overcoming these challenges. By splitting SVD into two phases--bidiagonalization and diagonalization--TT-Edge offloads the most compute intensive tasks to a specialized TTD Engine. This engine integrates tightly with an existing GEMM accelerator, thereby curtailing the frequent matrix vector transfers that often undermine system performance and energy efficiency. Implemented on a RISC-V-based edge AI processor, TT-Edge achieves a 1.7x speedup compared to a GEMM only baseline when compressing a ResNet 32 model via TTD, while reducing overall energy usage by 40.2 percent. These gains come with only a 4 percent increase in total power and minimal hardware overhead, enabled by a lightweight design that reuses GEMM resources and employs a shared floating point unit. Our experimental results on both FPGA prototypes and post-synthesis power analysis at 45 nm demonstrate that TT-Edge effectively addresses the latency and energy bottlenecks of TTD based compression in edge environments.", "AI": {"tldr": "TT-Edge\u662f\u4e00\u4e2a\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u8fb9\u7f18AI\u5904\u7406\u5668\u4e0a\u4e13\u95e8\u8bbe\u8ba1TTD\u5f15\u64ce\u6765\u52a0\u901f\u5f20\u91cf\u8bad\u7ec3\u5206\u89e3\uff0c\u5b9e\u73b01.7\u500d\u52a0\u901f\u548c40.2%\u80fd\u8017\u964d\u4f4e\u3002", "motivation": "\u5206\u5e03\u5f0f\u5b66\u4e60\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9700\u6c42\u589e\u957f\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8bbe\u5907\u7aef\u6a21\u578b\u538b\u7f29\u3002TTD\u867d\u7136\u63d0\u4f9b\u9ad8\u538b\u7f29\u7387\u548c\u4f4e\u7cbe\u5ea6\u635f\u5931\uff0c\u4f46\u91cd\u590d\u7684SVD\u548c\u77e9\u9635\u4e58\u6cd5\u5728\u4f4e\u529f\u8017\u5904\u7406\u5668\u4e0a\u4f1a\u4ea7\u751f\u663e\u8457\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u6210\u672c\u3002", "method": "\u5c06SVD\u5206\u89e3\u4e3a\u53cc\u5bf9\u89d2\u5316\u548c\u5bf9\u89d2\u5316\u4e24\u4e2a\u9636\u6bb5\uff0c\u5c06\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u5378\u8f7d\u5230\u4e13\u95e8\u7684TTD\u5f15\u64ce\u3002\u8be5\u5f15\u64ce\u4e0e\u73b0\u6709GEMM\u52a0\u901f\u5668\u7d27\u5bc6\u96c6\u6210\uff0c\u51cf\u5c11\u9891\u7e41\u7684\u77e9\u9635\u5411\u91cf\u4f20\u8f93\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\uff0c\u91cd\u7528GEMM\u8d44\u6e90\u5e76\u4f7f\u7528\u5171\u4eab\u6d6e\u70b9\u5355\u5143\u3002", "result": "\u5728RISC-V\u8fb9\u7f18AI\u5904\u7406\u5668\u4e0a\u5b9e\u73b0\uff0c\u538b\u7f29ResNet-32\u6a21\u578b\u65f6\u76f8\u6bd4\u4ec5\u4f7f\u7528GEMM\u7684\u57fa\u7ebf\u83b7\u5f971.7\u500d\u52a0\u901f\uff0c\u603b\u80fd\u8017\u964d\u4f4e40.2%\uff0c\u603b\u529f\u8017\u4ec5\u589e\u52a04%\uff0c\u786c\u4ef6\u5f00\u9500\u6700\u5c0f\u3002", "conclusion": "TT-Edge\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18\u73af\u5883\u4e2d\u57fa\u4e8eTTD\u538b\u7f29\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u74f6\u9888\u95ee\u9898\uff0c\u5728FPGA\u539f\u578b\u548c45nm\u540e\u5408\u6210\u529f\u8017\u5206\u6790\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2511.13761", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13761", "abs": "https://arxiv.org/abs/2511.13761", "authors": ["Alexander Acker", "Soeren Becker", "Sasho Nedelkoski", "Dominik Scheinert", "Odej Kao", "Philipp Wiesner"], "title": "What happens when nanochat meets DiLoCo?", "comment": "8pages, 3 figures, technical report", "summary": "Although LLM training is typically centralized with high-bandwidth interconnects and large compute budgets, emerging methods target communication-constrained training in distributed environments. The model trade-offs introduced by this shift remain underexplored, and our goal is to study them.\n  We use the open-source nanochat project, a compact 8K-line full-stack ChatGPT-like implementation containing tokenization, pretraining, fine-tuning, and serving, as a controlled baseline. We implement the DiLoCo algorithm as a lightweight wrapper over nanochat's training loop, performing multiple local steps per worker before synchronization with an outer optimizer, effectively reducing communication by orders of magnitude. This inner-outer training is compared against a standard data-parallel (DDP) setup. Because nanochat is small and inspectable, it enables controlled pipeline adaptations and allows direct comparison with the conventional centralized baseline.\n  DiLoCo achieves stable convergence and competitive loss in pretraining but yields worse MMLU, GSM8K, and HumanEval scores after mid-training and SFT. We discover that using DiLoCo-pretrained weights and running mid- and post-training with DDP fails to recover performance, revealing irreversible representation drift from asynchronous updates that impairs downstream alignment. We provide this implementation as an official fork of nanochat on GitHub.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5728\u901a\u4fe1\u53d7\u9650\u7684\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u6bd4\u8f83\u4e86DiLoCo\u7b97\u6cd5\u4e0e\u6807\u51c6\u6570\u636e\u5e76\u884c(DDP)\u8bad\u7ec3\u65b9\u6cd5\uff0c\u53d1\u73b0DiLoCo\u5728\u9884\u8bad\u7ec3\u4e2d\u8868\u73b0\u7a33\u5b9a\u4f46\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u8f83\u5dee\uff0c\u63ed\u793a\u4e86\u5f02\u6b65\u66f4\u65b0\u5bfc\u81f4\u7684\u4e0d\u53ef\u9006\u8868\u793a\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5728\u901a\u4fe1\u53d7\u9650\u7684\u5206\u5e03\u5f0f\u73af\u5883\u4e2dLLM\u8bad\u7ec3\u5f15\u5165\u7684\u6a21\u578b\u6743\u8861\uff0c\u8fd9\u4e9b\u6743\u8861\u5728\u73b0\u6709\u7814\u7a76\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528nanochat\u9879\u76ee\u4f5c\u4e3a\u57fa\u7ebf\uff0c\u5b9e\u73b0DiLoCo\u7b97\u6cd5\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u5305\u88c5\u5668\uff0c\u6267\u884c\u591a\u4e2a\u672c\u5730\u6b65\u9aa4\u540e\u4e0e\u5916\u90e8\u4f18\u5316\u5668\u540c\u6b65\uff0c\u5927\u5e45\u51cf\u5c11\u901a\u4fe1\u91cf\uff0c\u5e76\u4e0e\u6807\u51c6DDP\u8bbe\u7f6e\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "DiLoCo\u5728\u9884\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u6536\u655b\u548c\u7ade\u4e89\u6027\u635f\u5931\uff0c\u4f46\u5728\u4e2d\u671f\u8bad\u7ec3\u548cSFT\u540e\u4ea7\u751f\u8f83\u5dee\u7684MMLU\u3001GSM8K\u548cHumanEval\u5206\u6570\u3002\u4f7f\u7528DiLoCo\u9884\u8bad\u7ec3\u6743\u91cd\u540e\u5373\u4f7f\u6539\u7528DDP\u4e5f\u65e0\u6cd5\u6062\u590d\u6027\u80fd\u3002", "conclusion": "\u5f02\u6b65\u66f4\u65b0\u5bfc\u81f4\u4e0d\u53ef\u9006\u7684\u8868\u793a\u6f02\u79fb\uff0c\u635f\u5bb3\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u901a\u4fe1\u6548\u7387\u4e0e\u6a21\u578b\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2511.13778", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13778", "abs": "https://arxiv.org/abs/2511.13778", "authors": ["Angelika Schwarz", "Anton Anders", "Cole Brower", "Harun Bayraktar", "John Gunnels", "Kate Clark", "RuQing G. Xu", "Samuel Rodriguez", "Sebastien Cayrols", "Pawe\u0142 Tabaszewski", "Victor Podlozhnyuk"], "title": "Guaranteed DGEMM Accuracy While Using Reduced Precision Tensor Cores Through Extensions of the Ozaki Scheme", "comment": null, "summary": "The rapid growth of artificial intelligence (AI) has made low-precision formats such as FP16, FP8, and, most recently, block-scaled FP4 the primary focus of modern GPUs, where Tensor Cores now deliver orders-of-magnitude higher throughput than traditional FP64 pipelines. This hardware shift has sparked a new line of algorithm research: using low-precision units to emulate double-precision accuracy through schemes such as Ozaki decompositions. We advance this direction with Automatic Dynamic Precision (ADP), a fully GPU-resident framework that makes emulated FP64 matrix multiplication both efficient and reliable. At its core is the Exponent Span Capacity (ESC), a hardware-agnostic estimator that conservatively determines the decomposition parameter (also known as slices) required to achieve FP64-level accuracy. Built on ESC, ADP integrates exception handling, run time heuristics, and seamless fallback to native FP64, ensuring correctness without host-device synchronization or user intervention. Additionally, we further improve Ozaki-style decompositions with an unsigned integer slicing scheme, which increases representational efficiency and reduces computational waste. Validated against recently proposed BLAS grading tests, ADP consistently preserves FP64 fidelity on challenging inputs while incurring less than 10% run time overhead. In a 55-bit mantissa setting, our approach achieves up to 2.3x and 13.2x speedups over native FP64 GEMM on NVIDIA Blackwell GB200 and the RTX Pro 6000 Blackwell Server Edition, respectively. Our results demonstrate that low-precision accelerators can serve as a practical, production-ready foundation for high-fidelity and high-performance scientific computing workloads.", "AI": {"tldr": "ADP\u6846\u67b6\u5229\u7528\u4f4e\u7cbe\u5ea6GPU\u5355\u5143\u901a\u8fc7Ozaki\u5206\u89e3\u6a21\u62dfFP64\u7cbe\u5ea6\uff0c\u901a\u8fc7\u786c\u4ef6\u65e0\u5173\u7684ESC\u4f30\u8ba1\u5668\u786e\u4fdd\u7cbe\u5ea6\uff0c\u5728\u4fdd\u6301FP64\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u83b7\u5f97\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u73b0\u4ee3GPU\u786c\u4ef6\u8f6c\u5411\u4f4e\u7cbe\u5ea6\u683c\u5f0f(FP16/FP8/FP4)\uff0c\u4f20\u7edfFP64\u6d41\u6c34\u7ebf\u541e\u5410\u91cf\u8f83\u4f4e\uff0c\u9700\u8981\u5229\u7528\u4f4e\u7cbe\u5ea6\u5355\u5143\u9ad8\u6548\u6a21\u62df\u53cc\u7cbe\u5ea6\u8ba1\u7b97\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u52a8\u6001\u7cbe\u5ea6(ADP)\u6846\u67b6\uff0c\u6838\u5fc3\u662fESC\u4f30\u8ba1\u5668\u786e\u5b9a\u5206\u89e3\u53c2\u6570\uff0c\u96c6\u6210\u5f02\u5e38\u5904\u7406\u3001\u8fd0\u884c\u65f6\u542f\u53d1\u5f0f\u548c\u539f\u751fFP64\u56de\u9000\uff0c\u5e76\u6539\u8fdbOzaki\u5206\u89e3\u4f7f\u7528\u65e0\u7b26\u53f7\u6574\u6570\u5207\u7247\u65b9\u6848\u3002", "result": "\u572855\u4f4d\u5c3e\u6570\u8bbe\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u539f\u751fFP64 GEMM\u5728NVIDIA Blackwell GB200\u548cRTX Pro 6000 Blackwell\u4e0a\u5206\u522b\u83b7\u5f972.3\u500d\u548c13.2\u500d\u52a0\u901f\uff0c\u8fd0\u884c\u65f6\u95f4\u5f00\u9500\u5c0f\u4e8e10%\u3002", "conclusion": "\u4f4e\u7cbe\u5ea6\u52a0\u901f\u5668\u53ef\u4ee5\u4f5c\u4e3a\u9ad8\u4fdd\u771f\u3001\u9ad8\u6027\u80fd\u79d1\u5b66\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5b9e\u7528\u751f\u4ea7\u5c31\u7eea\u57fa\u7840\u3002"}}
{"id": "2511.13779", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.NI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.13779", "abs": "https://arxiv.org/abs/2511.13779", "authors": ["Mohammad Abdi", "Francesca Meneghello", "Francesco Restuccia"], "title": "Semantic Multiplexing", "comment": null, "summary": "Mobile devices increasingly require the parallel execution of several computing tasks offloaded at the wireless edge. Existing communication systems only support parallel transmissions at the bit level, which fundamentally limits the number of tasks that can be concurrently processed. To address this bottleneck, this paper introduces the new concept of Semantic Multiplexing. Our approach shifts stream multiplexing from bits to tasks by merging multiple task-related compressed representations into a single semantic representation. As such, Semantic Multiplexing can multiplex more tasks than the number of physical channels without adding antennas or widening bandwidth by extending the effective degrees of freedom at the semantic layer, without contradicting Shannon capacity rules. We have prototyped Semantic Multiplexing on an experimental testbed with Jetson Orin Nano and millimeter-wave software-defined radios and tested its performance on image classification and sentiment analysis while comparing to several existing baselines in semantic communications. Our experiments demonstrate that Semantic Multiplexing allows jointly processing multiple tasks at the semantic level while maintaining sufficient task accuracy. For example, image classification accuracy drops by less than 4% when increasing from 2 to 8 the number of tasks multiplexed over a 4$\\times$4 channel. Semantic Multiplexing reduces latency, energy consumption, and communication load respectively by up to 8$\\times$, 25$\\times$, and 54$\\times$ compared to the baselines while keeping comparable performance. We pledge to publicly share the complete software codebase and the collected datasets for reproducibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8bed\u4e49\u591a\u8def\u590d\u7528\u65b0\u6982\u5ff5\uff0c\u5c06\u591a\u4efb\u52a1\u538b\u7f29\u8868\u793a\u5408\u5e76\u4e3a\u5355\u4e00\u8bed\u4e49\u8868\u793a\uff0c\u53ef\u5728\u4e0d\u589e\u52a0\u5929\u7ebf\u6216\u5e26\u5bbd\u7684\u60c5\u51b5\u4e0b\u540c\u65f6\u5904\u7406\u66f4\u591a\u4efb\u52a1\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u3001\u80fd\u8017\u548c\u901a\u4fe1\u8d1f\u8f7d\u3002", "motivation": "\u73b0\u6709\u901a\u4fe1\u7cfb\u7edf\u4ec5\u652f\u6301\u6bd4\u7279\u7ea7\u5e76\u884c\u4f20\u8f93\uff0c\u9650\u5236\u4e86\u53ef\u5e76\u53d1\u5904\u7406\u7684\u4efb\u52a1\u6570\u91cf\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\u3002", "method": "\u5c06\u6d41\u591a\u8def\u590d\u7528\u4ece\u6bd4\u7279\u7ea7\u8f6c\u79fb\u5230\u4efb\u52a1\u7ea7\uff0c\u901a\u8fc7\u5c06\u591a\u4e2a\u4efb\u52a1\u76f8\u5173\u7684\u538b\u7f29\u8868\u793a\u5408\u5e76\u4e3a\u5355\u4e00\u8bed\u4e49\u8868\u793a\u6765\u5b9e\u73b0\u8bed\u4e49\u591a\u8def\u590d\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8bed\u4e49\u591a\u8def\u590d\u7528\u53ef\u5728\u4fdd\u6301\u8db3\u591f\u4efb\u52a1\u51c6\u786e\u6027\u7684\u540c\u65f6\u8054\u5408\u5904\u7406\u591a\u4e2a\u4efb\u52a1\uff0c\u56fe\u50cf\u5206\u7c7b\u51c6\u786e\u7387\u57284\u00d74\u4fe1\u9053\u4e0a\u4ece2\u4e2a\u4efb\u52a1\u589e\u52a0\u52308\u4e2a\u4efb\u52a1\u65f6\u4ec5\u4e0b\u964d\u4e0d\u52304%\uff0c\u76f8\u6bd4\u57fa\u7ebf\u5ef6\u8fdf\u964d\u4f4e8\u500d\u3001\u80fd\u8017\u964d\u4f4e25\u500d\u3001\u901a\u4fe1\u8d1f\u8f7d\u964d\u4f4e54\u500d\u3002", "conclusion": "\u8bed\u4e49\u591a\u8def\u590d\u7528\u901a\u8fc7\u6269\u5c55\u8bed\u4e49\u5c42\u7684\u6709\u6548\u81ea\u7531\u5ea6\uff0c\u5728\u4e0d\u8fdd\u53cd\u9999\u519c\u5bb9\u91cf\u89c4\u5219\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u66f4\u591a\u4efb\u52a1\u7684\u5e76\u53d1\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2511.13804", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13804", "abs": "https://arxiv.org/abs/2511.13804", "authors": ["Temitayo Adefemi"], "title": "Do MPI Derived Datatypes Actually Help? A Single-Node Cross-Implementation Study on Shared-Memory Communication", "comment": "9 pages, 6 figures", "summary": "MPI's derived datatypes (DDTs) promise easier, copy-free communication of non-contiguous data, yet their practical performance remains debated and is often reported only for a single MPI stack. We present a cross-implementation assessment using three 2D applications: a Jacobi CFD solver, Conway's Game of Life, and a lattice-based image reconstruction. Each application is written in two ways: (i) a BASIC version with manual packing and unpacking of non-contiguous regions and (ii) a DDT version using MPI_Type_vector and MPI_Type_create_subarray with correct true extent via MPI_Type_create_resized. For API parity, we benchmark identical communication semantics: non-blocking point-to-point (Irecv/Isend + Waitall), neighborhood collectives (MPI_Neighbor_alltoallw), and MPI-4 persistent operations (*_init). We run strong and weak scaling on 1-4 ranks, validate bitwise-identical halos, and evaluate four widely used MPI implementations: MPICH, Open MPI, Intel MPI, and MVAPICH2 on a single ARCHER2 node. Results are mixed. DDTs can be fastest, for example for the image reconstruction code on Intel MPI and MPICH, but can also be among the slowest on other stacks, such as Open MPI and MVAPICH2 for the same code. For the CFD solver, BASIC variants generally outperform DDTs across semantics, whereas for Game of Life the ranking flips depending on the MPI library. We also observe stack-specific anomalies, for example MPICH slowdowns with DDT neighborhood and persistent modes. Overall, no strategy dominates across programs, semantics, and MPI stacks; performance portability for DDTs is not guaranteed. We therefore recommend profiling both DDT-based and manual-packing designs under the intended MPI implementation and communication mode. Our study is limited to a single node and does not analyze memory overhead; multi-node and GPU-aware paths are left for future work.", "AI": {"tldr": "MPI\u6d3e\u751f\u6570\u636e\u7c7b\u578b(DDTs)\u7684\u6027\u80fd\u5728\u4e0d\u540c\u5b9e\u73b0\u4e2d\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u6ca1\u6709\u5355\u4e00\u7b56\u7565\u5728\u6240\u6709\u7a0b\u5e8f\u3001\u901a\u4fe1\u8bed\u4e49\u548cMPI\u5e93\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5efa\u8bae\u6839\u636e\u5177\u4f53MPI\u5b9e\u73b0\u548c\u901a\u4fe1\u6a21\u5f0f\u8fdb\u884c\u6027\u80fd\u5206\u6790\u3002", "motivation": "MPI\u6d3e\u751f\u6570\u636e\u7c7b\u578b\u627f\u8bfa\u7b80\u5316\u975e\u8fde\u7eed\u6570\u636e\u7684\u901a\u4fe1\uff0c\u4f46\u5b9e\u9645\u6027\u80fd\u8868\u73b0\u5b58\u5728\u4e89\u8bae\u4e14\u901a\u5e38\u53ea\u9488\u5bf9\u5355\u4e00MPI\u5b9e\u73b0\u8fdb\u884c\u8bc4\u4f30\uff0c\u9700\u8981\u8fdb\u884c\u8de8\u5b9e\u73b0\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a2D\u5e94\u7528(CFD\u6c42\u89e3\u5668\u3001\u751f\u547d\u6e38\u620f\u3001\u56fe\u50cf\u91cd\u5efa)\uff0c\u6bcf\u4e2a\u5e94\u7528\u7f16\u5199\u4e24\u79cd\u7248\u672c\uff1a\u624b\u52a8\u6253\u5305\u7248\u672c\u548cDDT\u7248\u672c\uff0c\u5728\u56db\u79cdMPI\u5b9e\u73b0\u4e0a\u6d4b\u8bd5\u975e\u963b\u585e\u70b9\u5bf9\u70b9\u3001\u90bb\u5c45\u96c6\u5408\u548c\u6301\u4e45\u64cd\u4f5c\u7b49\u901a\u4fe1\u8bed\u4e49\u3002", "result": "\u7ed3\u679c\u6df7\u5408\uff1aDDTs\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6700\u5feb(\u5982\u56fe\u50cf\u91cd\u5efa\u5728Intel MPI\u548cMPICH\u4e0a)\uff0c\u4f46\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u6700\u6162(\u5982\u540c\u4e00\u4e2a\u4ee3\u7801\u5728Open MPI\u548cMVAPICH2\u4e0a)\u3002CFD\u6c42\u89e3\u5668\u4e2d\u624b\u52a8\u7248\u672c\u666e\u904d\u4f18\u4e8eDDTs\uff0c\u800c\u751f\u547d\u6e38\u620f\u4e2d\u6027\u80fd\u6392\u540d\u56e0MPI\u5e93\u800c\u5f02\u3002", "conclusion": "DDTs\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6027\u65e0\u6cd5\u4fdd\u8bc1\uff0c\u5efa\u8bae\u5728\u76ee\u6807MPI\u5b9e\u73b0\u548c\u901a\u4fe1\u6a21\u5f0f\u4e0b\u540c\u65f6\u5206\u6790DDT\u548c\u624b\u52a8\u6253\u5305\u8bbe\u8ba1\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13940", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13940", "abs": "https://arxiv.org/abs/2511.13940", "authors": ["Stuart H. Sul", "Simran Arora", "Benjamin F. Spector", "Christopher R\u00e9"], "title": "ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels", "comment": null, "summary": "Inter-GPU communication has become a major bottleneck for modern AI workloads as models scale and improvements in hardware compute throughput outpace improvements in interconnect bandwidth. Existing systems mitigate this through compute-communication overlap but often fail to meet theoretical peak performance across heterogeneous workloads and new accelerators. Instead of operator-specific techniques, we ask whether a small set of simple, reusable principles can systematically guide the design of optimal multi-GPU kernels. We present ParallelKittens (PK), a minimal CUDA framework that drastically simplifies the development of overlapped multi-GPU kernels. PK extends the ThunderKittens framework and embodies the principles of multi-GPU kernel design through eight core primitives and a unified programming template, derived from a comprehensive analysis of the factors that govern multi-GPU performance$\\unicode{x2014}$data-transfer mechanisms, resource scheduling, and design overheads. We validate PK on both Hopper and Blackwell architectures. With fewer than 50 lines of device code, PK achieves up to $2.33 \\times$ speedup for data- and tensor-parallel workloads, $4.08 \\times$ for sequence-parallel workloads, and $1.22 \\times$ for expert-parallel workloads.", "AI": {"tldr": "ParallelKittens (PK) \u662f\u4e00\u4e2a\u7b80\u5316\u7684 CUDA \u6846\u67b6\uff0c\u901a\u8fc7\u516b\u4e2a\u6838\u5fc3\u539f\u8bed\u548c\u7edf\u4e00\u7f16\u7a0b\u6a21\u677f\uff0c\u7b80\u5316\u4e86\u91cd\u53e0\u591a GPU \u5185\u6838\u7684\u5f00\u53d1\uff0c\u5728 Hopper \u548c Blackwell \u67b6\u6784\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5404\u79cd\u5e76\u884c\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u89c4\u6a21\u6269\u5927\u548c\u786c\u4ef6\u8ba1\u7b97\u541e\u5410\u91cf\u63d0\u5347\u8d85\u8fc7\u4e92\u8fde\u5e26\u5bbd\u6539\u8fdb\uff0cGPU \u95f4\u901a\u4fe1\u5df2\u6210\u4e3a\u73b0\u4ee3 AI \u5de5\u4f5c\u8d1f\u8f7d\u7684\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709\u7cfb\u7edf\u901a\u8fc7\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\u6765\u7f13\u89e3\uff0c\u4f46\u5728\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u548c\u65b0\u52a0\u901f\u5668\u4e0a\u5f80\u5f80\u65e0\u6cd5\u8fbe\u5230\u7406\u8bba\u5cf0\u503c\u6027\u80fd\u3002", "method": "PK \u6269\u5c55\u4e86 ThunderKittens \u6846\u67b6\uff0c\u901a\u8fc7\u516b\u4e2a\u6838\u5fc3\u539f\u8bed\u548c\u7edf\u4e00\u7f16\u7a0b\u6a21\u677f\u4f53\u73b0\u591a GPU \u5185\u6838\u8bbe\u8ba1\u539f\u5219\uff0c\u57fa\u4e8e\u5bf9\u6570\u636e\u4f20\u8f93\u673a\u5236\u3001\u8d44\u6e90\u8c03\u5ea6\u548c\u8bbe\u8ba1\u5f00\u9500\u7b49\u5f71\u54cd\u591a GPU \u6027\u80fd\u56e0\u7d20\u7684\u7efc\u5408\u5206\u6790\u3002", "result": "\u5728 Hopper \u548c Blackwell \u67b6\u6784\u4e0a\u9a8c\u8bc1\uff0c\u4ec5\u7528\u4e0d\u5230 50 \u884c\u8bbe\u5907\u4ee3\u7801\uff0cPK \u5b9e\u73b0\u4e86\u6570\u636e\u5e76\u884c\u5de5\u4f5c\u8d1f\u8f7d 2.33 \u500d\u52a0\u901f\u3001\u5e8f\u5217\u5e76\u884c\u5de5\u4f5c\u8d1f\u8f7d 4.08 \u500d\u52a0\u901f\u3001\u4e13\u5bb6\u5e76\u884c\u5de5\u4f5c\u8d1f\u8f7d 1.22 \u500d\u52a0\u901f\u3002", "conclusion": "PK \u6846\u67b6\u8bc1\u660e\u4e86\u4e00\u5c0f\u7ec4\u7b80\u5355\u3001\u53ef\u91cd\u7528\u7684\u539f\u5219\u53ef\u4ee5\u7cfb\u7edf\u6307\u5bfc\u6700\u4f18\u591a GPU \u5185\u6838\u8bbe\u8ba1\uff0c\u663e\u8457\u7b80\u5316\u4e86\u91cd\u53e0\u591a GPU \u5185\u6838\u7684\u5f00\u53d1\u8fc7\u7a0b\u3002"}}
{"id": "2511.14116", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.14116", "abs": "https://arxiv.org/abs/2511.14116", "authors": ["Ziyi Xu", "Zhiqiang Xie", "Swapnil Gandhi", "Christos Kozyrakis"], "title": "FailSafe: High-performance Resilient Serving", "comment": null, "summary": "Tensor parallelism (TP) enables large language models (LLMs) to scale inference efficiently across multiple GPUs, but its tight coupling makes systems fragile: a single GPU failure can halt execution, trigger costly KVCache recomputation, and introduce long-term compute and memory imbalance. We present FailSafe, a fault-tolerant TP serving system that sustains high performance under irregular GPU availability. FailSafe introduces three techniques to balance computation and memory across GPUs: (1) Cyclic KVCache Placement for uniform memory utilization, (2) Hybrid Attention combining tensor- and data-parallel attention to eliminate stragglers, and (3) Fine-Grained Load-Aware Routing to dynamically balance requests. It further employs proactive KVCache backup and on-demand weight recovery to avoid expensive recomputation and redundant data transfers. We implement these techniques in a lightweight serving engine compatible with existing LLM infrastructures. Evaluated on an 8xH100 DGX system with real-world fault traces and representative workloads, FailSafe achieves up to 2x higher throughput and two orders of magnitude lower recovery latency compared to standard fault handling approaches. Even with up to three GPU failures, FailSafe sustains high throughput and balanced utilization, demonstrating robust and efficient LLM serving under dynamic and unreliable hardware conditions.", "AI": {"tldr": "FailSafe\u662f\u4e00\u4e2a\u5bb9\u9519\u7684\u5f20\u91cf\u5e76\u884c\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5faa\u73afKV\u7f13\u5b58\u653e\u7f6e\u3001\u6df7\u5408\u6ce8\u610f\u529b\u548c\u7ec6\u7c92\u5ea6\u8d1f\u8f7d\u611f\u77e5\u8def\u7531\u7b49\u6280\u672f\uff0c\u5728GPU\u6545\u969c\u65f6\u7ef4\u6301\u9ad8\u6027\u80fdLLM\u63a8\u7406\u3002", "motivation": "\u4f20\u7edf\u5f20\u91cf\u5e76\u884c(TP)\u5728GPU\u6545\u969c\u65f6\u4f1a\u5bfc\u81f4\u6267\u884c\u4e2d\u65ad\u3001\u6602\u8d35\u7684KV\u7f13\u5b58\u91cd\u8ba1\u7b97\u4ee5\u53ca\u957f\u671f\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5faa\u73afKV\u7f13\u5b58\u653e\u7f6e\u5b9e\u73b0\u5747\u5300\u5185\u5b58\u5229\u7528\uff0c\u6df7\u5408\u6ce8\u610f\u529b\u6d88\u9664\u6ede\u540e\uff0c\u7ec6\u7c92\u5ea6\u8d1f\u8f7d\u611f\u77e5\u8def\u7531\u52a8\u6001\u5e73\u8861\u8bf7\u6c42\uff0c\u4ee5\u53ca\u4e3b\u52a8KV\u7f13\u5b58\u5907\u4efd\u548c\u6309\u9700\u6743\u91cd\u6062\u590d\u3002", "result": "\u57288xH100 DGX\u7cfb\u7edf\u4e0a\uff0c\u76f8\u6bd4\u6807\u51c6\u5bb9\u9519\u65b9\u6cd5\uff0cFailSafe\u5b9e\u73b0\u4e86\u9ad8\u8fbe2\u500d\u7684\u541e\u5410\u91cf\u548c\u4e24\u4e2a\u6570\u91cf\u7ea7\u66f4\u4f4e\u7684\u6062\u590d\u5ef6\u8fdf\uff0c\u5373\u4f7f\u6700\u591a\u4e09\u4e2aGPU\u6545\u969c\u4e5f\u80fd\u7ef4\u6301\u9ad8\u6027\u80fd\u3002", "conclusion": "FailSafe\u5c55\u793a\u4e86\u5728\u52a8\u6001\u4e0d\u53ef\u9760\u786c\u4ef6\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u9ad8\u6548LLM\u670d\u52a1\u7684\u80fd\u529b\u3002"}}
{"id": "2511.14124", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14124", "abs": "https://arxiv.org/abs/2511.14124", "authors": ["Sabiha Afroz", "Redwan Ibne Seraj Khan", "Hadeel Albahar", "Jingoo Han", "Ali R. Butt"], "title": "10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training", "comment": "This paper accepted for presentation to the 16th ACM Symposium on Cloud Computing (SOCC'25)", "summary": "Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.\n  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.", "AI": {"tldr": "10Cache\u662f\u4e00\u4e2a\u8d44\u6e90\u611f\u77e5\u7684\u5f20\u91cf\u7f13\u5b58\u548c\u8fc1\u79fb\u7cfb\u7edf\uff0c\u901a\u8fc7\u667a\u80fd\u534f\u8c03GPU\u3001CPU\u548cNVMe\u4e4b\u95f4\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\uff0c\u89e3\u51b3GPU\u5185\u5b58\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u4e91\u7aef\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34GPU\u5185\u5b58\u5bb9\u91cf\u6709\u9650\u548c\u6210\u672c\u9ad8\u6602\u7684\u74f6\u9888\uff0c\u73b0\u6709GPU\u5185\u5b58\u5378\u8f7d\u65b9\u6cd5\u5b58\u5728\u9ad8\u5f20\u91cf\u8fc1\u79fb\u5ef6\u8fdf\u548c\u8bbe\u5907\u5185\u5b58\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8bad\u7ec3\u65f6\u95f4\u589e\u52a0\u548c\u4e91\u6210\u672c\u4e0a\u5347\u3002", "method": "10Cache\u901a\u8fc7\u5206\u6790\u5f20\u91cf\u6267\u884c\u987a\u5e8f\u6784\u5efa\u9884\u53d6\u7b56\u7565\uff0c\u57fa\u4e8e\u5f20\u91cf\u5927\u5c0f\u5206\u5e03\u5728\u56fa\u5b9a\u5185\u5b58\u4e2d\u5206\u914d\u5185\u5b58\u7f13\u51b2\u533a\uff0c\u5e76\u91cd\u7528\u5185\u5b58\u7f13\u51b2\u533a\u4ee5\u51cf\u5c11\u5206\u914d\u5f00\u9500\uff0c\u5b9e\u73b0\u8de8GPU\u3001CPU\u548cNVMe\u5c42\u7684\u5185\u5b58\u667a\u80fd\u534f\u8c03\u3002", "result": "\u5728\u591a\u6837\u5316LLM\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\uff0c10Cache\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5378\u8f7d\u65b9\u6cd5\uff0c\u8bad\u7ec3\u65f6\u95f4\u6700\u9ad8\u52a0\u901f2\u500d\uff0cGPU\u7f13\u5b58\u547d\u4e2d\u7387\u63d0\u5347\u6700\u9ad886.6\u500d\uff0cCPU\u548cGPU\u5185\u5b58\u5229\u7528\u7387\u5206\u522b\u63d0\u9ad8\u6700\u9ad82.15\u500d\u548c1.33\u500d\u3002", "conclusion": "10Cache\u662f\u4f18\u5316\u4e91\u7aefLLM\u8bad\u7ec3\u541e\u5410\u91cf\u548c\u8d44\u6e90\u6548\u7387\u7684\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5185\u5b58\u6548\u7387\u5e76\u51cf\u5c11\u5bf9\u9ad8\u7aefGPU\u7684\u4f9d\u8d56\u3002"}}
{"id": "2511.14450", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.14450", "abs": "https://arxiv.org/abs/2511.14450", "authors": ["Mulei Ma", "Minrui Xu", "Zihan Chen", "Yang Yang", "Tony Q. S. Quek"], "title": "Hyperion: Hierarchical Scheduling for Parallel LLM Acceleration in Multi-tier Networks", "comment": null, "summary": "Large Language Models (LLMs) are increasingly executed across edge, fog, and cloud tiers where limited GPU memory, heterogeneous compute, and variable inter-tier bandwidth jointly constrain deployment and motivate model partitioning and request scheduling. In this setting, achieving low end-to-end latency is governed not only by where a model is deployed (inter-tier model partitioning) but also by how incoming requests are scheduled (intra-tier task scheduling) across heterogeneous nodes. These two problems are tightly coupled, as a suboptimal scheduler can negate the benefits of a good partition, and vice versa. In this paper, we propose Hyperion, a hierarchical two-stage framework that jointly optimizes partitioning and scheduling to minimize end-to-end latency for pipelined LLM inference in multi-tier networks, balancing compute and memory across tiers while introducing negligible runtime overhead and requiring no model retraining. Motivated by the observation that partition choices evolve on slower timescales than request arrivals, Stage 1 performs offline, inter-tier partitioning via a Binary Search with Dynamic Programming (BSDP) procedure to produce balanced stage times under tier capacity and memory constraints; to adapt to time-varying load, Stage 2 performs online, intra-tier scheduling with a lightweight Adaptive Real-time Task Scheduling (ARTS) algorithm that maps each request to the best available node using real-time estimates of queue length and effective capacity. Experimental results on multi-tier inference tasks demonstrate that Hyperion significantly reduces end-to-end latency by up to 52.1\\% and 31.2\\%, with the Phi-3-medium model, compared to the GPipe and HEFT baselines, respectively. Furthermore, Hyperion shows superior scalability in long-sequence generation, maintaining a 44.5\\% lower latency than GPipe and achieving higher GPU utilization.", "AI": {"tldr": "Hyperion\u662f\u4e00\u4e2a\u5206\u5c42\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316LLM\u5728\u591a\u5c42\u7f51\u7edc\u4e2d\u7684\u5206\u533a\u548c\u8c03\u5ea6\uff0c\u901a\u8fc7\u79bb\u7ebf\u5206\u533a\u548c\u5728\u7ebf\u8c03\u5ea6\u663e\u8457\u964d\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "LLM\u5728\u8fb9\u7f18\u3001\u96fe\u548c\u4e91\u5c42\u7684\u90e8\u7f72\u53d7\u9650\u4e8eGPU\u5185\u5b58\u3001\u5f02\u6784\u8ba1\u7b97\u548c\u53ef\u53d8\u5e26\u5bbd\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u6a21\u578b\u5206\u533a\u548c\u8bf7\u6c42\u8c03\u5ea6\u6765\u6700\u5c0f\u5316\u5ef6\u8fdf\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528BSDP\u7b97\u6cd5\u8fdb\u884c\u79bb\u7ebf\u5206\u533a\uff0c\u5e73\u8861\u5404\u9636\u6bb5\u65f6\u95f4\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528ARTS\u7b97\u6cd5\u8fdb\u884c\u5728\u7ebf\u8c03\u5ea6\uff0c\u6839\u636e\u5b9e\u65f6\u961f\u5217\u957f\u5ea6\u548c\u6709\u6548\u5bb9\u91cf\u5206\u914d\u8bf7\u6c42\u3002", "result": "\u5728\u591a\u5c42\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cHyperion\u76f8\u6bd4GPipe\u548cHEFT\u57fa\u7ebf\u5206\u522b\u964d\u4f4e\u5ef6\u8fdf52.1%\u548c31.2%\uff0c\u5728\u957f\u5e8f\u5217\u751f\u6210\u4e2d\u4fdd\u630144.5%\u7684\u5ef6\u8fdf\u4f18\u52bf\u3002", "conclusion": "Hyperion\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5206\u533a\u548c\u8c03\u5ea6\uff0c\u5728\u591a\u5c42\u7f51\u7edc\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5ef6\u8fdf\u964d\u4f4e\u548c\u66f4\u597d\u7684GPU\u5229\u7528\u7387\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u8bad\u7ec3\u4e14\u5f15\u5165\u7684\u8fd0\u884c\u65f6\u5f00\u9500\u53ef\u5ffd\u7565\u3002"}}
{"id": "2511.14456", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14456", "abs": "https://arxiv.org/abs/2511.14456", "authors": ["Fabian Stricker", "David Bermbach", "Christian Zirpins"], "title": "Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning", "comment": "Accepted for publication in 3rd IEEE International Conference on Federated Learning Applications and Technologies (FLTA2025)", "summary": "Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.\n  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8de8\u673a\u6784\u8054\u90a6\u5b66\u4e60\u4e2d\u53c2\u4e0e\u8005\u5931\u8d25\u5bf9\u6a21\u578b\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5931\u8d25\u65f6\u673a\u3001\u6570\u636e\u5206\u5e03\u548c\u8bc4\u4f30\u65b9\u6cd5\u7b49\u56e0\u7d20\uff0c\u53d1\u73b0\u5728\u9ad8\u5ea6\u504f\u659c\u6570\u636e\u4e0b\u8bc4\u4f30\u7ed3\u679c\u8fc7\u4e8e\u4e50\u89c2\uff0c\u4e14\u5931\u8d25\u65f6\u673a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u8d28\u91cf\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u8de8\u673a\u6784\u573a\u666f\u4e2d\u9700\u8981\u53ef\u9760\u7cfb\u7edf\uff0c\u4f46\u53c2\u4e0e\u8005\u53ef\u80fd\u56e0\u5404\u79cd\u539f\u56e0\u5931\u8d25\u3002\u76ee\u524d\u9488\u5bf9\u8de8\u8bbe\u5907\u8054\u90a6\u5b66\u4e60\u7684\u7814\u7a76\u8f83\u591a\uff0c\u4f46\u5bf9\u8de8\u673a\u6784\u8054\u90a6\u5b66\u4e60\u4e2d\u53c2\u4e0e\u8005\u5931\u8d25\u5f71\u54cd\u7684\u7814\u7a76\u76f8\u5bf9\u8f83\u5c11\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u7814\u7a76\u5206\u6790\u8de8\u673a\u6784\u8054\u90a6\u5b66\u4e60\u4e2d\u53c2\u4e0e\u8005\u5931\u8d25\u5bf9\u6a21\u578b\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u5931\u8d25\u65f6\u673a\u3001\u6570\u636e\u5206\u5e03\u548c\u8bc4\u4f30\u65b9\u6cd5\u7b49\u5173\u952e\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u8868\u660e\u5728\u9ad8\u5ea6\u504f\u659c\u6570\u636e\u4e0b\u8bc4\u4f30\u7ed3\u679c\u8fc7\u4e8e\u4e50\u89c2\uff0c\u63a9\u76d6\u4e86\u771f\u5b9e\u5f71\u54cd\uff1b\u540c\u65f6\u53c2\u4e0e\u8005\u5931\u8d25\u7684\u65f6\u673a\u663e\u8457\u5f71\u54cd\u6700\u7ec8\u8bad\u7ec3\u6a21\u578b\u7684\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u9c81\u68d2\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u7814\u7a76\u4eba\u5458\u548c\u8f6f\u4ef6\u67b6\u6784\u5e08\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u5728\u8de8\u673a\u6784\u8054\u90a6\u5b66\u4e60\u4e2d\u8003\u8651\u53c2\u4e0e\u8005\u5931\u8d25\u5f71\u54cd\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.14608", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.14608", "abs": "https://arxiv.org/abs/2511.14608", "authors": ["Dave Dice", "Alex Kogan"], "title": "Hapax Locks : Value-Based Mutual Exclusion", "comment": null, "summary": "We present Hapax Locks, a novel locking algorithm that is simple, enjoys constant-time arrival and unlock paths, provides FIFO admission order, and which is also space efficient and generates relatively little coherence traffic under contention in the common case. Hapax Locks offer performance (both latency and scalability) that is comparable with the best state of the art locks, while at the same time Hapax Locks impose fewer constraints and dependencies on the ambient runtime environment, making them particularly easy to integrate or retrofit into existing systems or under existing application programming interfaces Of particular note, no pointers shift or escape ownership between threads in our algorithm.", "AI": {"tldr": "Hapax Locks\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9501\u5b9a\u7b97\u6cd5\uff0c\u5177\u6709\u7b80\u5355\u6027\u3001\u6052\u5b9a\u65f6\u95f4\u5230\u8fbe\u548c\u89e3\u9501\u8def\u5f84\u3001FIFO\u51c6\u5165\u987a\u5e8f\u3001\u7a7a\u95f4\u6548\u7387\u9ad8\u4e14\u5728\u7ade\u4e89\u60c5\u51b5\u4e0b\u4ea7\u751f\u8f83\u5c11\u4e00\u81f4\u6027\u6d41\u91cf\u7684\u7279\u70b9\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u9501\u7b97\u6cd5\u76f8\u5f53\uff0c\u4f46\u5bf9\u8fd0\u884c\u65f6\u73af\u5883\u7ea6\u675f\u66f4\u5c11\u3001\u66f4\u5bb9\u6613\u96c6\u6210\u5230\u73b0\u6709\u7cfb\u7edf\u4e2d\u7684\u9501\u5b9a\u7b97\u6cd5\u3002", "method": "\u63d0\u51faHapax Locks\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u7ebf\u7a0b\u95f4\u4e0d\u8f6c\u79fb\u6307\u9488\u6240\u6709\u6743\uff0c\u7b80\u5316\u4e86\u5b9e\u73b0\u548c\u96c6\u6210\u8fc7\u7a0b\u3002", "result": "Hapax Locks\u5728\u5ef6\u8fdf\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u4e0e\u6700\u5148\u8fdb\u7684\u9501\u7b97\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u73af\u5883\u4f9d\u8d56\u548c\u96c6\u6210\u96be\u5ea6\u3002", "conclusion": "Hapax Locks\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u6613\u4e8e\u96c6\u6210\u7684\u9501\u5b9a\u7b97\u6cd5\uff0c\u7279\u522b\u9002\u5408\u5728\u73b0\u6709\u7cfb\u7edf\u6216API\u4e0b\u8fdb\u884c\u96c6\u6210\u6216\u6539\u9020\u3002"}}
{"id": "2511.14617", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14617", "abs": "https://arxiv.org/abs/2511.14617", "authors": ["Ruoyu Qin", "Weiran He", "Weixiao Huang", "Yangkun Zhang", "Yikai Zhao", "Bo Pang", "Xinran Xu", "Yingdi Shan", "Yongwei Wu", "Mingxing Zhang"], "title": "Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning", "comment": "16 pages, 12 figures, 6 tables", "summary": "Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.", "AI": {"tldr": "Seer\u662f\u4e00\u4e2a\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7684\u5728\u7ebf\u4e0a\u4e0b\u6587\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u5229\u7528\u76f8\u540c\u63d0\u793a\u8bf7\u6c42\u95f4\u7684\u8f93\u51fa\u957f\u5ea6\u548c\u751f\u6210\u6a21\u5f0f\u76f8\u4f3c\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u540c\u6b65RL\u7cfb\u7edf\u5728rollout\u9636\u6bb5\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u540c\u6b65\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u5728rollout\u9636\u6bb5\u9762\u4e34\u4e25\u91cd\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5305\u62ec\u663e\u8457\u7684\u957f\u5c3e\u5ef6\u8fdf\u548c\u7531\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "Seer\u5f15\u5165\u4e86\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a\u7528\u4e8e\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\u7684\u5206\u5272rollout\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u8c03\u5ea6\u548c\u81ea\u9002\u5e94\u5206\u7ec4\u63a8\u6d4b\u89e3\u7801\uff0c\u8fd9\u4e9b\u673a\u5236\u5171\u540c\u51cf\u5c11\u4e86\u957f\u5c3e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86rollout\u9636\u6bb5\u7684\u8d44\u6e90\u6548\u7387\u3002", "result": "\u5728\u751f\u4ea7\u7ea7RL\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u540c\u6b65RL\u7cfb\u7edf\u76f8\u6bd4\uff0cSeer\u5c06\u7aef\u5230\u7aefrollout\u541e\u5410\u91cf\u63d0\u9ad8\u4e8674%\u523097%\uff0c\u5e76\u5c06\u957f\u5c3e\u5ef6\u8fdf\u964d\u4f4e\u4e8675%\u523093%\u3002", "conclusion": "Seer\u7cfb\u7edf\u663e\u8457\u52a0\u901f\u4e86RL\u8bad\u7ec3\u8fed\u4ee3\uff0c\u901a\u8fc7\u5229\u7528\u8bf7\u6c42\u95f4\u7684\u76f8\u4f3c\u6027\u6709\u6548\u89e3\u51b3\u4e86rollout\u9636\u6bb5\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2511.14664", "categories": ["cs.DC", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.14664", "abs": "https://arxiv.org/abs/2511.14664", "authors": ["W. Michael Brown", "Anurag Ramesh", "Thomas Lubinski", "Thien Nguyen", "David E. Bernal Neira"], "title": "Multi-GPU Quantum Circuit Simulation and the Impact of Network Performance", "comment": "13 Pages, 5 Figures, Preprint submitted for publication", "summary": "As is intrinsic to the fundamental goal of quantum computing, classical simulation of quantum algorithms is notoriously demanding in resource requirements. Nonetheless, simulation is critical to the success of the field and a requirement for algorithm development and validation, as well as hardware design. GPU-acceleration has become standard practice for simulation, and due to the exponential scaling inherent in classical methods, multi-GPU simulation can be required to achieve representative system sizes. In this case, inter-GPU communications can bottleneck performance. In this work, we present the introduction of MPI into the QED-C Application-Oriented Benchmarks to facilitate benchmarking on HPC systems. We review the advances in interconnect technology and the APIs for multi-GPU communication. We benchmark using a variety of interconnect paths, including the recent NVIDIA Grace Blackwell NVL72 architecture that represents the first product to expand high-bandwidth GPU-specialized interconnects across multiple nodes. We show that while improvements to GPU architecture have led to speedups of over 4.5X across the last few generations of GPUs, advances in interconnect performance have had a larger impact with over 16X performance improvements in time to solution for multi-GPU simulations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u5728QED-C\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f15\u5165MPI\u4ee5\u652f\u6301HPC\u7cfb\u7edf\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u591aGPU\u4e92\u8fde\u6280\u672f\u5bf9\u91cf\u5b50\u8ba1\u7b97\u6a21\u62df\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e92\u8fde\u6027\u80fd\u63d0\u5347\u6bd4GPU\u67b6\u6784\u6539\u8fdb\u5bf9\u591aGPU\u6a21\u62df\u6709\u66f4\u5927\u5f71\u54cd\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u6a21\u62df\u5728\u8d44\u6e90\u9700\u6c42\u4e0a\u975e\u5e38\u82db\u523b\uff0c\u4f46\u5bf9\u4e8e\u7b97\u6cd5\u5f00\u53d1\u3001\u9a8c\u8bc1\u548c\u786c\u4ef6\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002\u591aGPU\u6a21\u62df\u9700\u8981\u5904\u7406\u7cfb\u7edf\u95f4\u901a\u4fe1\u74f6\u9888\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u4e92\u8fde\u6280\u672f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5728QED-C\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f15\u5165MPI\uff0c\u4f7f\u7528\u5404\u79cd\u4e92\u8fde\u8def\u5f84\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u6700\u65b0\u7684NVIDIA Grace Blackwell NVL72\u67b6\u6784\uff0c\u6bd4\u8f83\u4e0d\u540cGPU\u67b6\u6784\u548c\u4e92\u8fde\u6280\u672f\u7684\u6027\u80fd\u3002", "result": "GPU\u67b6\u6784\u6539\u8fdb\u5e26\u6765\u4e864.5\u500d\u4ee5\u4e0a\u7684\u52a0\u901f\uff0c\u4f46\u4e92\u8fde\u6027\u80fd\u63d0\u5347\u5bf9\u591aGPU\u6a21\u62df\u7684\u5f71\u54cd\u66f4\u5927\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc716\u500d\u7684\u89e3\u51b3\u65b9\u6848\u65f6\u95f4\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u4e92\u8fde\u6280\u672f\u7684\u8fdb\u6b65\u5bf9\u591aGPU\u91cf\u5b50\u8ba1\u7b97\u6a21\u62df\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6bd4GPU\u67b6\u6784\u6539\u8fdb\u8d21\u732e\u66f4\u5927\uff0cNVIDIA Grace Blackwell\u7b49\u65b0\u6280\u672f\u6709\u6548\u6269\u5c55\u4e86\u8de8\u8282\u70b9\u7684\u9ad8\u5e26\u5bbdGPU\u4e13\u7528\u4e92\u8fde\u3002"}}
