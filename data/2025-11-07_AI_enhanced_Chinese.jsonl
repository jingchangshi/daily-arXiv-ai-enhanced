{"id": "2511.03866", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.03866", "abs": "https://arxiv.org/abs/2511.03866", "authors": ["Arijit Bhattacharjee", "Ali TehraniJamsaz", "Le Chen", "Niranjan Hasabnis", "Mihai Capota", "Nesreen Ahmed", "Ali Jannesari"], "title": "OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms", "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly\naccelerated progress in code translation, enabling more accurate and efficient\ntransformation across programming languages. While originally developed for\nnatural language processing, LLMs have shown strong capabilities in modeling\nprogramming language syntax and semantics, outperforming traditional rule-based\nsystems in both accuracy and flexibility. These models have streamlined\ncross-language conversion, reduced development overhead, and accelerated legacy\ncode migration. In this paper, we introduce OMPILOT, a novel domain-specific\nencoder-decoder transformer tailored for translating C++ code into OpenMP,\nenabling effective shared-memory parallelization. OMPILOT leverages custom\npre-training objectives that incorporate the semantics of parallel constructs\nand combines both unsupervised and supervised learning strategies to improve\ncode translation robustness. Unlike previous work that focused primarily on\nloop-level transformations, OMPILOT operates at the function level to capture a\nwider semantic context. To evaluate our approach, we propose OMPBLEU, a novel\ncomposite metric specifically crafted to assess the correctness and quality of\nOpenMP parallel constructs, addressing limitations in conventional translation\nmetrics.", "AI": {"tldr": "OMPILOT\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u5c06C++\u4ee3\u7801\u7ffb\u8bd1\u6210OpenMP\u7684\u9886\u57df\u7279\u5b9a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u8f6c\u6362\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u65e0\u76d1\u7763\u548c\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u63d0\u9ad8\u4ee3\u7801\u7ffb\u8bd1\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u51fd\u6570\u7ea7\u522b\u8fdb\u884c\u64cd\u4f5c\u4ee5\u6355\u83b7\u66f4\u5e7f\u6cdb\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7ffb\u8bd1\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5faa\u73af\u7ea7\u8f6c\u6362\uff0c\u7f3a\u4e4f\u5bf9\u66f4\u5e7f\u6cdb\u8bed\u4e49\u4e0a\u4e0b\u6587\u7684\u6355\u83b7\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86OMPILOT\u6a21\u578b\uff0c\u91c7\u7528\u81ea\u5b9a\u4e49\u9884\u8bad\u7ec3\u76ee\u6807\u7ed3\u5408\u5e76\u884c\u6784\u9020\u8bed\u4e49\uff0c\u4f7f\u7528\u65e0\u76d1\u7763\u548c\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u5728\u51fd\u6570\u7ea7\u522b\u8fdb\u884c\u4ee3\u7801\u7ffb\u8bd1\u3002", "result": "\u63d0\u51fa\u4e86OMPBLEU\u8bc4\u4f30\u6307\u6807\u6765\u8bc4\u4f30OpenMP\u5e76\u884c\u6784\u9020\u7684\u6b63\u786e\u6027\u548c\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ffb\u8bd1\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "conclusion": "OMPILOT\u5728C++\u5230OpenMP\u7684\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u5171\u4eab\u5185\u5b58\u5e76\u884c\u5316\uff0c\u4e3a\u9057\u7559\u4ee3\u7801\u8fc1\u79fb\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.03941", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.03941", "abs": "https://arxiv.org/abs/2511.03941", "authors": ["Fabio Diniz Rossi"], "title": "Stochastic Modeling for Energy-Efficient Edge Infrastructure", "comment": "8 pages, 4 figures, 3 tables", "summary": "Edge Computing enables low-latency processing for real-time applications but\nintroduces challenges in power management due to the distributed nature of edge\ndevices and their limited energy resources. This paper proposes a stochastic\nmodeling approach using Markov Chains to analyze power state transitions in\nEdge Computing. By deriving steady-state probabilities and evaluating energy\nconsumption, we demonstrate the benefits of AI-driven predictive power scaling\nover conventional reactive methods. Monte Carlo simulations validate the model,\nshowing strong alignment between theoretical and empirical results. Sensitivity\nanalysis highlights how varying transition probabilities affect power\nefficiency, confirming that predictive scaling minimizes unnecessary\ntransitions and improves overall system responsiveness. Our findings suggest\nthat AI-based power management strategies significantly enhance energy\nefficiency by anticipating workload demands and optimizing state transitions.\nExperimental results indicate that AI-based power management optimizes workload\ndistribution across heterogeneous edge nodes, reducing energy consumption\ndisparities between devices, improving overall efficiency, and enhancing\nadaptive power coordination in multi-node environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u968f\u673a\u5efa\u6a21\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u529f\u7387\u72b6\u6001\u8f6c\u6362\uff0c\u901a\u8fc7AI\u9a71\u52a8\u7684\u9884\u6d4b\u6027\u529f\u7387\u7f29\u653e\u663e\u8457\u63d0\u9ad8\u4e86\u80fd\u6e90\u6548\u7387\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u867d\u7136\u652f\u6301\u4f4e\u5ef6\u8fdf\u5904\u7406\uff0c\u4f46\u7531\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u5206\u5e03\u5f0f\u7279\u6027\u548c\u6709\u9650\u7684\u80fd\u6e90\u8d44\u6e90\uff0c\u5728\u529f\u7387\u7ba1\u7406\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u94fe\u8fdb\u884c\u968f\u673a\u5efa\u6a21\uff0c\u63a8\u5bfc\u7a33\u6001\u6982\u7387\u5e76\u8bc4\u4f30\u80fd\u8017\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAI\u9a71\u52a8\u7684\u9884\u6d4b\u6027\u529f\u7387\u7f29\u653e\u76f8\u6bd4\u4f20\u7edf\u53cd\u5e94\u5f0f\u65b9\u6cd5\uff0c\u80fd\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u72b6\u6001\u8f6c\u6362\uff0c\u63d0\u9ad8\u7cfb\u7edf\u54cd\u5e94\u6027\uff0c\u4f18\u5316\u5f02\u6784\u8fb9\u7f18\u8282\u70b9\u95f4\u7684\u8d1f\u8f7d\u5206\u914d\uff0c\u964d\u4f4e\u80fd\u8017\u5dee\u5f02\u3002", "conclusion": "\u57fa\u4e8eAI\u7684\u529f\u7387\u7ba1\u7406\u7b56\u7565\u901a\u8fc7\u9884\u6d4b\u5de5\u4f5c\u8d1f\u8f7d\u9700\u6c42\u5e76\u4f18\u5316\u72b6\u6001\u8f6c\u6362\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\u7684\u80fd\u6e90\u6548\u7387\u3002"}}
{"id": "2511.04268", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04268", "abs": "https://arxiv.org/abs/2511.04268", "authors": ["Iker Mart\u00edn-\u00c1lvarez", "Jos\u00e9 I. Aliaga", "Maribel Castillo", "Sergio Iserte"], "title": "Parallel Spawning Strategies for Dynamic-Aware MPI Applications", "comment": "10 pages, 1 Table, 6 Figures, 8 Equations, 2 Listings", "summary": "Dynamic resource management is an increasingly important capability of High\nPerformance Computing systems, as it enables jobs to adjust their resource\nallocation at runtime. This capability has been shown to reduce workload\nmakespan, substantially decrease job waiting times and improve overall system\nutilization. In this context, malleability refers to the ability of\napplications to adapt to new resource allocations during execution. Although\nbeneficial, malleability incurs significant reconfiguration costs, making the\nreduction of these costs an important research topic.\n  Some existing methods for MPI applications respawn the entire application,\nwhich is an expensive solution that avoids the reuse of original processes.\nOther MPI methods reuse them, but fail to fully release unneeded processes when\nshrinking, since some ranks within the same communicator remain active across\nnodes, preventing the application from returning those nodes to the system.\nThis work overcomes both limitations by proposing a novel parallel spawning\nstrategy, in which all processes cooperate in spawning before redistribution,\nthereby reducing execution time. Additionally, it removes shrinkage\nlimitations, allowing better adaptation of parallel systems to workload and\nreducing their makespan. As a result, it preserves competitive expansion times\nwith at most a $1.25\\times$ overhead, while enabling fast shrink operations\nthat reduce their cost by at least $20\\times$. This strategy has been validated\non both homogeneous and heterogeneous systems and can also be applied in\nshared-resource environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e76\u884c\u751f\u6210\u7b56\u7565\uff0c\u7528\u4e8eMPI\u5e94\u7528\u7a0b\u5e8f\u7684\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\uff0c\u901a\u8fc7\u8fdb\u7a0b\u534f\u4f5c\u751f\u6210\u548c\u91cd\u65b0\u5206\u914d\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6536\u7f29\u64cd\u4f5c\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u7684\u6269\u5c55\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709MPI\u5e94\u7528\u7a0b\u5e8f\u7684\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u8981\u4e48\u91cd\u65b0\u751f\u6210\u6574\u4e2a\u5e94\u7528\u7a0b\u5e8f\uff08\u6210\u672c\u9ad8\uff09\uff0c\u8981\u4e48\u5728\u6536\u7f29\u65f6\u65e0\u6cd5\u5b8c\u5168\u91ca\u653e\u4e0d\u9700\u8981\u7684\u8fdb\u7a0b\u3002\u8fd9\u9650\u5236\u4e86\u7cfb\u7edf\u8d44\u6e90\u5229\u7528\u6548\u7387\u548c\u4f5c\u4e1a\u7b49\u5f85\u65f6\u95f4\u7684\u6539\u5584\u3002", "method": "\u91c7\u7528\u5e76\u884c\u751f\u6210\u7b56\u7565\uff0c\u6240\u6709\u8fdb\u7a0b\u5728\u91cd\u65b0\u5206\u914d\u524d\u534f\u4f5c\u8fdb\u884c\u751f\u6210\uff0c\u4ece\u800c\u51cf\u5c11\u6267\u884c\u65f6\u95f4\u3002\u540c\u65f6\u6d88\u9664\u4e86\u6536\u7f29\u9650\u5236\uff0c\u4f7f\u5e76\u884c\u7cfb\u7edf\u80fd\u66f4\u597d\u5730\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u6269\u5c55\u65f6\u95f4\uff08\u6700\u591a1.25\u500d\u5f00\u9500\uff09\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u7684\u6536\u7f29\u64cd\u4f5c\uff0c\u5c06\u6536\u7f29\u6210\u672c\u964d\u4f4e\u4e86\u81f3\u5c1120\u500d\u3002\u5728\u5f02\u6784\u548c\u540c\u6784\u7cfb\u7edf\u4e0a\u5747\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5e76\u884c\u751f\u6210\u7b56\u7565\u6709\u6548\u514b\u670d\u4e86\u73b0\u6709MPI\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u91cd\u65b0\u914d\u7f6e\u6210\u672c\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u8d44\u6e90\u5229\u7528\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5171\u4eab\u8d44\u6e90\u73af\u5883\u3002"}}
{"id": "2511.04477", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04477", "abs": "https://arxiv.org/abs/2511.04477", "authors": ["Rongxiang Wang", "Kangyuan Shu", "Felix Xiaozhu Lin"], "title": "Enabling Dynamic Sparsity in Quantized LLM Inference", "comment": null, "summary": "Deploying large language models (LLMs) on end-user devices is gaining\nimportance due to benefits in responsiveness, privacy, and operational cost.\nYet the limited memory and compute capability of mobile and desktop GPUs make\nefficient execution difficult. Recent observations suggest that the internal\nactivations of LLMs are often dynamically sparse, meaning that for each input,\nonly part of the network contributes significantly to the output. Such sparsity\ncould reduce computation, but it interacts poorly with group-wise quantization,\nwhich remains the dominant approach for fitting LLMs onto resource-constrained\nhardware. To reconcile these two properties, this study proposes a set of\ntechniques that realize dynamic sparse inference under low-bit quantization.\nThe method features: (1) a zigzag-patterned quantization layout that organizes\nweights in a way consistent with activation sparsity and improves GPU memory\nlocality; (2) a specialized GEMV kernel designed for this layout to fully\nutilize parallel compute units; and (3) a compact runtime mechanism that\ngathers sparse indices with minimal overhead. Across several model scales and\nhardware configurations, the approach achieves up to 1.55x faster decoding\nthroughput while maintaining accuracy comparable to dense quantized inference,\nshowing that structured sparsity and quantization can effectively coexist on\ncommodity GPUs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u4f4e\u6bd4\u7279\u91cf\u5316\u4e0b\u5b9e\u73b0\u52a8\u6001\u7a00\u758f\u63a8\u7406\u7684\u6280\u672f\uff0c\u5305\u62eczigzag\u91cf\u5316\u5e03\u5c40\u3001\u4e13\u7528GEMV\u5185\u6838\u548c\u7d27\u51d1\u8fd0\u884c\u65f6\u673a\u5236\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u89e3\u7801\u541e\u5410\u91cf1.55\u500d", "motivation": "\u5728\u7ec8\u7aef\u8bbe\u5907\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u5185\u5b58\u548c\u8ba1\u7b97\u80fd\u529b\u9650\u5236\uff0c\u800cLLM\u5185\u90e8\u6fc0\u6d3b\u7684\u52a8\u6001\u7a00\u758f\u6027\u4e0e\u4e3b\u6d41\u7684\u7ec4\u91cf\u5316\u65b9\u6cd5\u5b58\u5728\u51b2\u7a81\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe", "method": "\u91c7\u7528zigzag\u6a21\u5f0f\u7684\u91cf\u5316\u5e03\u5c40\u4ee5\u5339\u914d\u6fc0\u6d3b\u7a00\u758f\u6027\u5e76\u6539\u5584GPU\u5185\u5b58\u5c40\u90e8\u6027\uff1b\u8bbe\u8ba1\u4e13\u7528GEMV\u5185\u6838\u5145\u5206\u5229\u7528\u5e76\u884c\u8ba1\u7b97\u5355\u5143\uff1b\u5f00\u53d1\u7d27\u51d1\u8fd0\u884c\u65f6\u673a\u5236\u4ee5\u6700\u5c0f\u5f00\u9500\u6536\u96c6\u7a00\u758f\u7d22\u5f15", "result": "\u5728\u591a\u79cd\u6a21\u578b\u89c4\u6a21\u548c\u786c\u4ef6\u914d\u7f6e\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad81.55\u500d\u7684\u89e3\u7801\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5bc6\u96c6\u91cf\u5316\u63a8\u7406\u76f8\u5f53\u7684\u7cbe\u5ea6", "conclusion": "\u7ed3\u6784\u5316\u7a00\u758f\u6027\u548c\u91cf\u5316\u53ef\u4ee5\u5728\u5546\u7528GPU\u4e0a\u6709\u6548\u5171\u5b58\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2511.03944", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.03944", "abs": "https://arxiv.org/abs/2511.03944", "authors": ["Tong Zhang", "Vikram Sharma Mailthody", "Fei Sun", "Linsen Ma", "Chris J. Newburn", "Teresa Zhang", "Yang Liu", "Jiangpeng Li", "Hao Zhong", "Wen-Mei Hwu"], "title": "From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies", "comment": "13 pages, 10 figures", "summary": "In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a\nsimple, storage-memory-economics-based heuristic for deciding when data should\nlive in DRAM rather than on storage. Subsequent revisits to the rule largely\nretained that economics-only view, leaving host costs, feasibility limits, and\nworkload behavior out of scope. This paper revisits the rule from first\nprinciples, integrating host costs, DRAM bandwidth/capacity, and\nphysics-grounded models of SSD performance and cost, and then embedding these\nelements in a constraint- and workload-aware framework that yields actionable\nprovisioning guidance. We show that, for modern AI platforms, especially\nGPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained\nrandom access, the DRAM-to-flash caching threshold collapses from minutes to a\nfew seconds. This shift reframes NAND flash memory as an active data tier and\nexposes a broad research space across the hardware-software stack. We further\nintroduce MQSim-Next, a calibrated SSD simulator that supports validation and\nsensitivity analysis and facilitates future architectural and system research.\nFinally, we present two concrete case studies that showcase the software system\ndesign space opened by such memory hierarchy paradigm shift. Overall, we turn a\nclassical heuristic into an actionable, feasibility-aware analysis and\nprovisioning framework and set the stage for further research on AI-era memory\nhierarchy.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u7ecf\u5178\u76845\u5206\u949f\u89c4\u5219\uff0c\u5c06\u5176\u4ece\u5355\u7eaf\u7684\u7ecf\u6d4e\u5b66\u542f\u53d1\u5f0f\u65b9\u6cd5\u53d1\u5c55\u4e3a\u5305\u542b\u4e3b\u673a\u6210\u672c\u3001DRAM\u5e26\u5bbd/\u5bb9\u91cf\u9650\u5236\u3001SSD\u6027\u80fd\u6a21\u578b\u548c\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7efc\u5408\u6027\u5206\u6790\u6846\u67b6\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u73b0\u4ee3AI\u5e73\u53f0\u4e2d\uff0cDRAM\u5230\u95ea\u5b58\u7684\u7f13\u5b58\u9608\u503c\u4ece\u5206\u949f\u7ea7\u964d\u81f3\u79d2\u7ea7\uff0c\u8fd9\u91cd\u65b0\u5b9a\u4e49\u4e86NAND\u95ea\u5b58\u4f5c\u4e3a\u6d3b\u8dc3\u6570\u636e\u5c42\u7684\u89d2\u8272\u3002", "motivation": "\u4f20\u7edf\u76845\u5206\u949f\u89c4\u5219\u4ec5\u57fa\u4e8e\u5b58\u50a8-\u5185\u5b58\u7ecf\u6d4e\u5b66\uff0c\u5ffd\u7565\u4e86\u4e3b\u673a\u6210\u672c\u3001\u53ef\u884c\u6027\u9650\u5236\u548c\u5de5\u4f5c\u8d1f\u8f7d\u884c\u4e3a\u3002\u968f\u7740\u73b0\u4ee3AI\u5e73\u53f0\uff08\u7279\u522b\u662fGPU\u4e3b\u673a\u4e0e\u9ad8\u6027\u80fdSSD\u914d\u5bf9\uff09\u7684\u53d1\u5c55\uff0c\u9700\u8981\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e00\u89c4\u5219\u3002", "method": "\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u51fa\u53d1\uff0c\u6574\u5408\u4e3b\u673a\u6210\u672c\u3001DRAM\u5e26\u5bbd/\u5bb9\u91cf\u9650\u5236\u3001\u57fa\u4e8e\u7269\u7406\u7684SSD\u6027\u80fd\u548c\u6210\u672c\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u5230\u7ea6\u675f\u548c\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7684\u6846\u67b6\u4e2d\u3002\u5f00\u53d1\u4e86MQSim-Next SSD\u6a21\u62df\u5668\u8fdb\u884c\u9a8c\u8bc1\u548c\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u5bf9\u4e8e\u73b0\u4ee3AI\u5e73\u53f0\uff0c\u7279\u522b\u662fGPU\u4e3b\u673a\u4e0e\u8d85\u9ad8\u6027\u80fdSSD\u914d\u5bf9\u65f6\uff0cDRAM\u5230\u95ea\u5b58\u7684\u7f13\u5b58\u9608\u503c\u4ece\u5206\u949f\u7ea7\u5d29\u6e83\u5230\u79d2\u7ea7\u3002\u8fd9\u91cd\u65b0\u5b9a\u4e49\u4e86NAND\u95ea\u5b58\u4f5c\u4e3a\u6d3b\u8dc3\u6570\u636e\u5c42\u7684\u89d2\u8272\uff0c\u5e76\u63ed\u793a\u4e86\u786c\u4ef6-\u8f6f\u4ef6\u6808\u7684\u5e7f\u6cdb\u7814\u7a76\u7a7a\u95f4\u3002", "conclusion": "\u5c06\u7ecf\u5178\u542f\u53d1\u5f0f\u65b9\u6cd5\u8f6c\u53d8\u4e3a\u53ef\u64cd\u4f5c\u7684\u3001\u53ef\u884c\u6027\u611f\u77e5\u7684\u5206\u6790\u548c\u914d\u7f6e\u6846\u67b6\uff0c\u4e3aAI\u65f6\u4ee3\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.03946", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.03946", "abs": "https://arxiv.org/abs/2511.03946", "authors": ["Marcelo P. Fiore", "Ohad Kammar", "Georg Moser", "Sam Staton"], "title": "Modular abstract syntax trees (MAST): substitution tensors with second-class sorts", "comment": null, "summary": "We adapt Fiore, Plotkin, and Turi's treatment of abstract syntax with\nbinding, substitution, and holes to account for languages with second-class\nsorts. These situations include programming calculi such as the Call-by-Value\nlambda-calculus (CBV) and Levy's Call-by-Push-Value (CBPV). Prohibiting\nsecond-class sorts from appearing in variable contexts changes the\ncharacterisation of the abstract syntax from monoids in monoidal categories to\nactions in actegories. We reproduce much of the development through\nbicategorical arguments. We apply the resulting theory by proving substitution\nlemmata for varieties of CBV.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86Fiore\u7b49\u4eba\u7684\u62bd\u8c61\u8bed\u6cd5\u7406\u8bba\uff0c\u4ee5\u5904\u7406\u5177\u6709\u4e8c\u9636\u7c7b\u578b\u7684\u8bed\u8a00\uff08\u5982CBV\u548cCBPV\uff09\uff0c\u901a\u8fc7\u4f7f\u7528actegories\u800c\u975emonoidal categories\u6765\u91cd\u65b0\u63cf\u8ff0\u62bd\u8c61\u8bed\u6cd5\uff0c\u5e76\u5e94\u7528\u8be5\u7406\u8bba\u8bc1\u660eCBV\u53d8\u4f53\u7684\u66ff\u6362\u5f15\u7406\u3002", "motivation": "\u73b0\u6709\u62bd\u8c61\u8bed\u6cd5\u7406\u8bba\u65e0\u6cd5\u5f88\u597d\u5730\u5904\u7406\u5177\u6709\u4e8c\u9636\u7c7b\u578b\u7684\u7f16\u7a0b\u6f14\u7b97\uff08\u5982CBV\u548cCBPV\uff09\uff0c\u9700\u8981\u6269\u5c55\u7406\u8bba\u6846\u67b6\u4ee5\u652f\u6301\u8fd9\u4e9b\u8bed\u8a00\u3002", "method": "\u5c06Fiore\u7b49\u4eba\u7684\u62bd\u8c61\u8bed\u6cd5\u7406\u8bba\u4ecemonoidal categories\u6269\u5c55\u5230actegories\uff0c\u4f7f\u7528\u53cc\u8303\u7574\u8bba\u8bc1\uff0c\u5e76\u7981\u6b62\u4e8c\u9636\u7c7b\u578b\u51fa\u73b0\u5728\u53d8\u91cf\u4e0a\u4e0b\u6587\u4e2d\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u9002\u7528\u4e8e\u4e8c\u9636\u7c7b\u578b\u8bed\u8a00\u7684\u62bd\u8c61\u8bed\u6cd5\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u5e94\u7528\u8be5\u6846\u67b6\u8bc1\u660e\u4e86CBV\u53d8\u4f53\u7684\u66ff\u6362\u5f15\u7406\u3002", "conclusion": "\u901a\u8fc7\u5c06\u62bd\u8c61\u8bed\u6cd5\u7684\u7279\u5f81\u4ecemonoidal categories\u8f6c\u5411actegories\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5904\u7406\u5177\u6709\u4e8c\u9636\u7c7b\u578b\u7684\u7f16\u7a0b\u8bed\u8a00\uff0c\u4e3a\u8fd9\u7c7b\u8bed\u8a00\u7684\u8bed\u4e49\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.04523", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04523", "abs": "https://arxiv.org/abs/2511.04523", "authors": ["Silvia Bonomi", "Giovanni Farina", "Roy Friedman", "Eviatar B. Procaccia", "Sebastien Tixeuil"], "title": "A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems", "comment": null, "summary": "Modern distributed systems face growing security threats, as attackers\ncontinuously enhance their skills and vulnerabilities span across the entire\nsystem stack, from hardware to the application layer. In the system design\nphase, fault tolerance techniques can be employed to safeguard systems. From a\ntheoretical perspective, an attacker attempting to compromise a system can be\nabstracted by considering the presence of Byzantine processes in the system.\nAlthough this approach enhances the resilience of the distributed system, it\nintroduces certain limitations regarding the accuracy of the model in\nreflecting real-world scenarios. In this paper, we consider a self-protecting\ndistributed system based on the \\emph{Monitoring-Analyse-Plan-Execute over a\nshared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic\nMobile Byzantine Failure (MBF) that can be plugged into the Analysis component.\nOur new model captures the dynamics of evolving attacks and can be used to\ndrive the self-protection and reconfiguration strategy. We analyze\nmathematically the time that it takes until the number of Byzantine nodes\ncrosses given thresholds, or for the system to self-recover back into a safe\nstate, depending on the rates of Byzantine infection spreading \\emph{vs.} the\nrate of self-recovery. We also provide simulation results that illustrate the\nbehavior of the system under such assumptions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMAPE-K\u67b6\u6784\u7684\u81ea\u4fdd\u62a4\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u5f15\u5165\u6982\u7387\u6027\u79fb\u52a8\u62dc\u5360\u5ead\u6545\u969c\u6a21\u578b\u6765\u6a21\u62df\u52a8\u6001\u653b\u51fb\u6f14\u5316\uff0c\u5e76\u5206\u6790\u4e86\u62dc\u5360\u5ead\u8282\u70b9\u6570\u91cf\u8d85\u8fc7\u9608\u503c\u7684\u65f6\u95f4\u4ee5\u53ca\u7cfb\u7edf\u81ea\u6062\u590d\u7684\u6982\u7387\u3002", "motivation": "\u73b0\u4ee3\u5206\u5e03\u5f0f\u7cfb\u7edf\u9762\u4e34\u65e5\u76ca\u589e\u957f\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u653b\u51fb\u8005\u6280\u80fd\u4e0d\u65ad\u63d0\u5347\uff0c\u6f0f\u6d1e\u904d\u5e03\u6574\u4e2a\u7cfb\u7edf\u6808\u3002\u73b0\u6709\u62dc\u5360\u5ead\u6545\u969c\u6a21\u578b\u5728\u53cd\u6620\u73b0\u5b9e\u573a\u666f\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u5728MAPE-K\u67b6\u6784\u7684\u5206\u6790\u7ec4\u4ef6\u4e2d\u5f15\u5165\u65b0\u7684\u6982\u7387\u6027\u79fb\u52a8\u62dc\u5360\u5ead\u6545\u969c\u6a21\u578b\uff0c\u901a\u8fc7\u6570\u5b66\u5206\u6790\u8ba1\u7b97\u62dc\u5360\u5ead\u8282\u70b9\u8d85\u8fc7\u9608\u503c\u7684\u65f6\u95f4\u548c\u7cfb\u7edf\u81ea\u6062\u590d\u6982\u7387\uff0c\u5e76\u8fdb\u884c\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u5efa\u7acb\u4e86\u62dc\u5360\u5ead\u611f\u67d3\u4f20\u64ad\u901f\u7387\u4e0e\u81ea\u6062\u590d\u901f\u7387\u4e4b\u95f4\u7684\u6570\u5b66\u5173\u7cfb\u6a21\u578b\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u7cfb\u7edf\u72b6\u6001\u53d8\u5316\uff0c\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6982\u7387\u6027\u79fb\u52a8\u62dc\u5360\u5ead\u6545\u969c\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u52a8\u6001\u653b\u51fb\u6f14\u5316\uff0c\u4e3a\u81ea\u4fdd\u62a4\u548c\u91cd\u914d\u7f6e\u7b56\u7565\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc\uff0c\u589e\u5f3a\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u5f39\u6027\u3002"}}
{"id": "2511.04036", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.04036", "abs": "https://arxiv.org/abs/2511.04036", "authors": ["Yue Jiet Chong", "Yimin Wang", "Zhen Wu", "Xuanyao Fong"], "title": "PICNIC: Silicon Photonic Interconnected Chiplets with Computational Network and In-memory Computing for LLM Inference Acceleration", "comment": null, "summary": "This paper presents a 3D-stacked chiplets based large language model (LLM)\ninference accelerator, consisting of non-volatile in-memory-computing\nprocessing elements (PEs) and Inter-PE Computational Network (IPCN),\ninterconnected via silicon photonic to effectively address the communication\nbottlenecks. A LLM mapping scheme was developed to optimize hardware scheduling\nand workload mapping. Simulation results show it achieves $3.95\\times$ speedup\nand $30\\times$ efficiency improvement over the Nvidia A100 before chiplet\nclustering and power gating scheme (CCPG). Additionally, the system achieves\nfurther scalability and efficiency improvement with the implementation of CCPG\nto accommodate larger models, attaining $57\\times$ efficiency improvement over\nNvidia H100 at similar throughput.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u5806\u53e0\u82af\u7c92\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u52a0\u901f\u5668\uff0c\u91c7\u7528\u975e\u6613\u5931\u6027\u5185\u5b58\u8ba1\u7b97\u5904\u7406\u5355\u5143\u548c\u7845\u5149\u4e92\u8fde\u6280\u672f\u89e3\u51b3\u901a\u4fe1\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u4f7f\u75283D\u5806\u53e0\u82af\u7c92\u67b6\u6784\uff0c\u96c6\u6210\u975e\u6613\u5931\u6027\u5185\u5b58\u8ba1\u7b97\u5904\u7406\u5355\u5143\u548c\u7845\u5149\u5b50\u4e92\u8fde\u7684IPCN\u7f51\u7edc\uff0c\u5f00\u53d1\u4e86\u4e13\u95e8\u7684LLM\u6620\u5c04\u65b9\u6848\u6765\u4f18\u5316\u786c\u4ef6\u8c03\u5ea6\u548c\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u3002", "result": "\u76f8\u6bd4Nvidia A100\u5b9e\u73b0\u4e863.95\u500d\u52a0\u901f\u548c30\u500d\u6548\u7387\u63d0\u5347\uff1b\u5728\u5b9e\u65bd\u82af\u7c92\u805a\u7c7b\u548c\u529f\u7387\u95e8\u63a7\u65b9\u6848\u540e\uff0c\u76f8\u6bd4Nvidia H100\u5b9e\u73b0\u4e8657\u500d\u6548\u7387\u63d0\u5347\u3002", "conclusion": "\u8be53D\u5806\u53e0\u82af\u7c92\u67b6\u6784\u80fd\u6709\u6548\u89e3\u51b3LLM\u63a8\u7406\u7684\u901a\u4fe1\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u80fd\u6548\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.04631", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.04631", "abs": "https://arxiv.org/abs/2511.04631", "authors": ["Petr Kuznetsov", "Nathan Josia Schrodt"], "title": "Resolving Conflicts with Grace: Dynamically Concurrent Universality", "comment": null, "summary": "Synchronization is the major obstacle to scalability in distributed\ncomputing. Concurrent operations on the shared data engage in synchronization\nwhen they encounter a \\emph{conflict}, i.e., their effects depend on the order\nin which they are applied. Ideally, one would like to detect conflicts in a\n\\emph{dynamic} manner, i.e., adjusting to the current system state. Indeed, it\nis very common that two concurrent operations conflict only in some rarely\noccurring states. In this paper, we define the notion of \\emph{dynamic\nconcurrency}: an operation employs strong synchronization primitives only if it\n\\emph{has} to arbitrate with concurrent operations, given the current system\nstate. We then present a dynamically concurrent universal construction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u52a8\u6001\u5e76\u53d1\u6982\u5ff5\uff0c\u4ec5\u5728\u9700\u8981\u6839\u636e\u5f53\u524d\u7cfb\u7edf\u72b6\u6001\u4e0e\u5e76\u53d1\u64cd\u4f5c\u8fdb\u884c\u4ef2\u88c1\u65f6\u624d\u4f7f\u7528\u5f3a\u540c\u6b65\u539f\u8bed\uff0c\u5e76\u63d0\u51fa\u4e86\u52a8\u6001\u5e76\u53d1\u901a\u7528\u6784\u9020\u3002", "motivation": "\u540c\u6b65\u662f\u5206\u5e03\u5f0f\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u7684\u4e3b\u8981\u969c\u788d\u3002\u5e76\u53d1\u64cd\u4f5c\u5728\u9047\u5230\u51b2\u7a81\u65f6\u8fdb\u884c\u540c\u6b65\uff0c\u800c\u51b2\u7a81\u901a\u5e38\u53ea\u5728\u67d0\u4e9b\u7f55\u89c1\u72b6\u6001\u4e0b\u53d1\u751f\u3002\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u5e0c\u671b\u80fd\u591f\u52a8\u6001\u68c0\u6d4b\u51b2\u7a81\uff0c\u5373\u6839\u636e\u5f53\u524d\u7cfb\u7edf\u72b6\u6001\u8fdb\u884c\u8c03\u6574\u3002", "method": "\u5b9a\u4e49\u4e86\u52a8\u6001\u5e76\u53d1\u6982\u5ff5\uff0c\u5e76\u63d0\u51fa\u4e86\u52a8\u6001\u5e76\u53d1\u901a\u7528\u6784\u9020\uff0c\u4f7f\u64cd\u4f5c\u4ec5\u5728\u9700\u8981\u4ef2\u88c1\u65f6\u624d\u4f7f\u7528\u5f3a\u540c\u6b65\u539f\u8bed\u3002", "result": "\u63d0\u51fa\u4e86\u52a8\u6001\u5e76\u53d1\u901a\u7528\u6784\u9020\uff0c\u80fd\u591f\u6839\u636e\u5f53\u524d\u7cfb\u7edf\u72b6\u6001\u52a8\u6001\u8c03\u6574\u540c\u6b65\u9700\u6c42\u3002", "conclusion": "\u52a8\u6001\u5e76\u53d1\u65b9\u6cd5\u80fd\u591f\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u540c\u6b65\u5f00\u9500\uff0c\u63d0\u9ad8\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.04104", "categories": ["cs.AR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.04104", "abs": "https://arxiv.org/abs/2511.04104", "authors": ["Chao Guo", "Jiahe Xu", "Moshe Zukerman"], "title": "Disaggregated Architectures and the Redesign of Data Center Ecosystems: Scheduling, Pooling, and Infrastructure Trade-offs", "comment": null, "summary": "Hardware disaggregation seeks to transform Data Center (DC) resources from\ntraditional server fleets into unified resource pools. Despite existing\nchallenges that may hinder its full realization, significant progress has been\nmade in both industry and academia. In this article, we provide an overview of\nthe motivations and recent advancements in hardware disaggregation. We further\ndiscuss the research challenges and opportunities associated with disaggregated\narchitectures, focusing on aspects that have received limited attention. We\nargue that hardware disaggregation has the potential to reshape the entire DC\necosystem, impacting application design, resource scheduling, hardware\nconfiguration, cooling, and power system optimization. Additionally, we present\na numerical study to illustrate several key aspects of these challenges.", "AI": {"tldr": "\u786c\u4ef6\u89e3\u8026\u5c06\u6570\u636e\u4e2d\u5fc3\u8d44\u6e90\u4ece\u4f20\u7edf\u670d\u52a1\u5668\u96c6\u7fa4\u8f6c\u53d8\u4e3a\u7edf\u4e00\u8d44\u6e90\u6c60\uff0c\u672c\u6587\u6982\u8ff0\u4e86\u5176\u52a8\u673a\u3001\u6700\u65b0\u8fdb\u5c55\u3001\u7814\u7a76\u6311\u6218\u548c\u673a\u9047\uff0c\u5e76\u6307\u51fa\u8fd9\u5c06\u91cd\u5851\u6574\u4e2a\u6570\u636e\u4e2d\u5fc3\u751f\u6001\u7cfb\u7edf\u3002", "motivation": "\u786c\u4ef6\u89e3\u8026\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u670d\u52a1\u5668\u67b6\u6784\u7684\u8d44\u6e90\u5229\u7528\u6548\u7387\u4f4e\u3001\u6269\u5c55\u6027\u53d7\u9650\u7b49\u95ee\u9898\uff0c\u901a\u8fc7\u8d44\u6e90\u6c60\u5316\u5b9e\u73b0\u66f4\u7075\u6d3b\u9ad8\u6548\u7684\u8d44\u6e90\u5206\u914d\u3002", "method": "\u91c7\u7528\u7efc\u8ff0\u5206\u6790\u65b9\u6cd5\uff0c\u7ed3\u5408\u6570\u503c\u7814\u7a76\u6765\u9610\u8ff0\u786c\u4ef6\u89e3\u8026\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u3002", "result": "\u786c\u4ef6\u89e3\u8026\u5728\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b8c\u5168\u5b9e\u73b0\u4ecd\u9762\u4e34\u6280\u672f\u6311\u6218\u3002", "conclusion": "\u786c\u4ef6\u89e3\u8026\u6709\u6f5c\u529b\u91cd\u5851\u6570\u636e\u4e2d\u5fc3\u751f\u6001\u7cfb\u7edf\uff0c\u5f71\u54cd\u5e94\u7528\u8bbe\u8ba1\u3001\u8d44\u6e90\u8c03\u5ea6\u3001\u786c\u4ef6\u914d\u7f6e\u3001\u51b7\u5374\u548c\u7535\u529b\u7cfb\u7edf\u4f18\u5316\u7b49\u591a\u4e2a\u65b9\u9762\u3002"}}
{"id": "2511.04321", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04321", "abs": "https://arxiv.org/abs/2511.04321", "authors": ["Yuanpeng Zhang", "Xing Hu", "Xi Chen", "Zhihang Yuan", "Cong Li", "Jingchen Zhu", "Zhao Wang", "Chenguang Zhang", "Xin Si", "Wei Gao", "Qiang Wu", "Runsheng Wang", "Guangyu Sun"], "title": "AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM", "comment": "18 pages, 22 figures, accepted by ISCA 2025", "summary": "SRAM Processing-in-Memory (PIM) has emerged as the most promising\nimplementation for high-performance PIM, delivering superior computing density,\nenergy efficiency, and computational precision. However, the pursuit of higher\nperformance necessitates more complex circuit designs and increased operating\nfrequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly\ndegrade chip performance and even threaten reliability. Conventional\ncircuit-level IR-drop mitigation methods, such as back-end optimizations, are\nresource-intensive and often compromise power, performance, and area (PPA). To\naddress these challenges, we propose AIM, comprehensive software and hardware\nco-design for architecture-level IR-drop mitigation in high-performance PIM.\nInitially, leveraging the bit-serial and in-situ dataflow processing properties\nof PIM, we introduce Rtog and HR, which establish a direct correlation between\nPIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS,\nenabling extensive exploration of architecture-level IR-drop mitigation while\nmaintaining computational accuracy through software optimization. Subsequently,\nwe develop IR-Booster, a dynamic adjustment mechanism that integrates\nsoftware-level HR information with hardware-based IR-drop monitoring to adapt\nthe V-f pairs of the PIM macro, achieving enhanced energy efficiency and\nperformance. Finally, we propose the HR-aware task mapping method, bridging\nsoftware and hardware designs to achieve optimal improvement. Post-layout\nsimulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up\nto 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement\nand 1.152x speedup.", "AI": {"tldr": "AIM\u662f\u4e00\u79cd\u9488\u5bf9\u9ad8\u6027\u80fdSRAM\u5b58\u5185\u8ba1\u7b97(PIM)\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u67b6\u6784\u7ea7IR-drop\u7f13\u89e3\u6280\u672f\uff0c\u57287nm 256-TOPS PIM\u82af\u7247\u4e0a\u5b9e\u73b0\u4e8669.2%\u7684IR-drop\u7f13\u89e3\u30012.29\u500d\u80fd\u6548\u63d0\u5347\u548c1.152\u500d\u52a0\u901f\u3002", "motivation": "SRAM\u5b58\u5185\u8ba1\u7b97\u867d\u7136\u5177\u6709\u9ad8\u6027\u80fd\u4f18\u52bf\uff0c\u4f46\u8ffd\u6c42\u66f4\u9ad8\u6027\u80fd\u4f1a\u5bfc\u81f4\u66f4\u590d\u6742\u7684\u7535\u8def\u8bbe\u8ba1\u548c\u66f4\u9ad8\u7684\u5de5\u4f5c\u9891\u7387\uff0c\u4ece\u800c\u52a0\u5267IR-drop\u95ee\u9898\uff0c\u5f71\u54cd\u82af\u7247\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002\u4f20\u7edf\u7535\u8def\u7ea7IR-drop\u7f13\u89e3\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u4e14\u4f1a\u635f\u5bb3PPA(\u529f\u8017\u3001\u6027\u80fd\u3001\u9762\u79ef)\u3002", "method": "\u5229\u7528PIM\u7684\u4f4d\u4e32\u884c\u548c\u539f\u4f4d\u6570\u636e\u6d41\u5904\u7406\u7279\u6027\uff0c\u63d0\u51faRtog\u548cHR\u5efa\u7acbPIM\u5de5\u4f5c\u8d1f\u8f7d\u4e0eIR-drop\u7684\u76f4\u63a5\u5173\u8054\uff1b\u5f00\u53d1LHR\u548cWDS\u8fdb\u884c\u67b6\u6784\u7ea7IR-drop\u7f13\u89e3\u63a2\u7d22\uff1b\u8bbe\u8ba1IR-Booster\u52a8\u6001\u8c03\u6574\u673a\u5236\uff0c\u96c6\u6210\u8f6f\u4ef6\u7ea7HR\u4fe1\u606f\u548c\u786c\u4ef6IR-drop\u76d1\u6d4b\u6765\u8c03\u6574PIM\u5b8f\u7684\u7535\u538b-\u9891\u7387\u5bf9\uff1b\u63d0\u51faHR\u611f\u77e5\u7684\u4efb\u52a1\u6620\u5c04\u65b9\u6cd5\u3002", "result": "\u57287nm 256-TOPS PIM\u82af\u7247\u4e0a\u7684\u540e\u5e03\u5c40\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cAIM\u5b9e\u73b0\u4e86\u9ad8\u8fbe69.2%\u7684IR-drop\u7f13\u89e3\uff0c\u5e26\u67652.29\u500d\u7684\u80fd\u6548\u63d0\u5347\u548c1.152\u500d\u7684\u52a0\u901f\u3002", "conclusion": "AIM\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6210\u529f\u89e3\u51b3\u4e86\u9ad8\u6027\u80fdPIM\u4e2d\u7684IR-drop\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u548c\u6027\u80fd\u3002"}}
{"id": "2511.04677", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.04677", "abs": "https://arxiv.org/abs/2511.04677", "authors": ["Joaquin Tarraga-Moreno", "Daniel Barley", "Francisco J. Andujar Munoz", "Jesus Escudero-Sahuquillo", "Holger Froning", "Pedro Javier Garcia", "Francisco J. Quiles", "Jose Duato"], "title": "Scalable and Efficient Intra- and Inter-node Interconnection Networks for Post-Exascale Supercomputers and Data centers", "comment": null, "summary": "The rapid growth of data-intensive applications such as generative AI,\nscientific simulations, and large-scale analytics is driving modern\nsupercomputers and data centers toward increasingly heterogeneous and tightly\nintegrated architectures. These systems combine powerful CPUs and accelerators\nwith emerging high-bandwidth memory and storage technologies to reduce data\nmovement and improve computational efficiency. However, as the number of\naccelerators per node increases, communication bottlenecks emerge both within\nand between nodes, particularly when network resources are shared among\nheterogeneous components.", "AI": {"tldr": "\u73b0\u4ee3\u8d85\u7ea7\u8ba1\u7b97\u673a\u548c\u6570\u636e\u4e2d\u5fc3\u6b63\u671d\u7740\u5f02\u6784\u548c\u7d27\u5bc6\u96c6\u6210\u7684\u67b6\u6784\u53d1\u5c55\uff0c\u4ee5\u652f\u6301\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\uff0c\u4f46\u52a0\u901f\u5668\u6570\u91cf\u7684\u589e\u52a0\u5bfc\u81f4\u4e86\u901a\u4fe1\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u5f0fAI\u3001\u79d1\u5b66\u6a21\u62df\u548c\u5927\u89c4\u6a21\u5206\u6790\u7b49\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\u7684\u5feb\u901f\u589e\u957f\uff0c\u63a8\u52a8\u4e86\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\u7684\u9700\u6c42\uff0c\u9700\u8981\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u5e76\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u7ed3\u5408\u5f3a\u5927\u7684CPU\u548c\u52a0\u901f\u5668\uff0c\u91c7\u7528\u65b0\u5174\u7684\u9ad8\u5e26\u5bbd\u5185\u5b58\u548c\u5b58\u50a8\u6280\u672f\uff0c\u6784\u5efa\u5f02\u6784\u548c\u7d27\u5bc6\u96c6\u6210\u7684\u7cfb\u7edf\u67b6\u6784\u3002", "result": "\u968f\u7740\u6bcf\u4e2a\u8282\u70b9\u4e2d\u52a0\u901f\u5668\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5728\u8282\u70b9\u5185\u90e8\u548c\u8282\u70b9\u4e4b\u95f4\u51fa\u73b0\u4e86\u901a\u4fe1\u74f6\u9888\uff0c\u7279\u522b\u662f\u5728\u7f51\u7edc\u8d44\u6e90\u88ab\u5f02\u6784\u7ec4\u4ef6\u5171\u4eab\u65f6\u3002", "conclusion": "\u867d\u7136\u5f02\u6784\u96c6\u6210\u67b6\u6784\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4f46\u901a\u4fe1\u74f6\u9888\u6210\u4e3a\u9650\u5236\u7cfb\u7edf\u6027\u80fd\u7684\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u7f51\u7edc\u8d44\u6e90\u5206\u914d\u548c\u901a\u4fe1\u673a\u5236\u3002"}}
