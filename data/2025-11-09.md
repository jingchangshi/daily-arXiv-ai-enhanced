<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Modular abstract syntax trees (MAST): substitution tensors with second-class sorts](https://arxiv.org/abs/2511.03946)
*Marcelo P. Fiore,Ohad Kammar,Georg Moser,Sam Staton*

Main category: cs.PL

TL;DR: 本文扩展了Fiore等人的抽象语法理论，以处理具有第二类排序的语言，如CBV和CBPV演算，通过使用actegories代替monoidal categories来建模语法结构。


<details>
  <summary>Details</summary>
Motivation: 现有抽象语法理论无法很好地处理具有第二类排序的编程演算，如CBV和CBPV，这些语言中某些排序不能出现在变量上下文中。

Method: 使用actegories代替monoidal categories来建模语法结构，通过双范畴论证重现理论发展，并应用于证明CBV变体的替换引理。

Result: 成功扩展了抽象语法理论以支持第二类排序，建立了相应的数学模型，并验证了其在CBV演算中的应用。

Conclusion: 提出的actegories方法能够有效处理具有第二类排序的语言，为这类编程演算的形式化分析提供了理论基础。

Abstract: We adapt Fiore, Plotkin, and Turi's treatment of abstract syntax with
binding, substitution, and holes to account for languages with second-class
sorts. These situations include programming calculi such as the Call-by-Value
lambda-calculus (CBV) and Levy's Call-by-Push-Value (CBPV). Prohibiting
second-class sorts from appearing in variable contexts changes the
characterisation of the abstract syntax from monoids in monoidal categories to
actions in actegories. We reproduce much of the development through
bicategorical arguments. We apply the resulting theory by proving substitution
lemmata for varieties of CBV.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms](https://arxiv.org/abs/2511.03866)
*Arijit Bhattacharjee,Ali TehraniJamsaz,Le Chen,Niranjan Hasabnis,Mihai Capota,Nesreen Ahmed,Ali Jannesari*

Main category: cs.DC

TL;DR: OMPILOT是一个专门用于将C++代码翻译为OpenMP的领域特定编码器-解码器变换器，通过函数级翻译和定制预训练目标实现共享内存并行化。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型在代码翻译中的优势，解决传统基于规则系统在准确性和灵活性上的不足，专注于C++到OpenMP的有效并行化转换。

Method: 采用定制编码器-解码器变换器架构，结合无监督和监督学习策略，在函数级别进行翻译，并引入包含并行结构语义的预训练目标。

Result: 开发了OMPBLEU评估指标来专门评估OpenMP并行结构的正确性和质量，解决了传统翻译指标的局限性。

Conclusion: OMPILOT通过领域特定的方法和综合学习策略，在代码翻译和并行化方面表现出色，为遗留代码迁移和跨语言转换提供了有效解决方案。

Abstract: Recent advances in large language models (LLMs) have significantly
accelerated progress in code translation, enabling more accurate and efficient
transformation across programming languages. While originally developed for
natural language processing, LLMs have shown strong capabilities in modeling
programming language syntax and semantics, outperforming traditional rule-based
systems in both accuracy and flexibility. These models have streamlined
cross-language conversion, reduced development overhead, and accelerated legacy
code migration. In this paper, we introduce OMPILOT, a novel domain-specific
encoder-decoder transformer tailored for translating C++ code into OpenMP,
enabling effective shared-memory parallelization. OMPILOT leverages custom
pre-training objectives that incorporate the semantics of parallel constructs
and combines both unsupervised and supervised learning strategies to improve
code translation robustness. Unlike previous work that focused primarily on
loop-level transformations, OMPILOT operates at the function level to capture a
wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel
composite metric specifically crafted to assess the correctness and quality of
OpenMP parallel constructs, addressing limitations in conventional translation
metrics.

</details>


### [3] [Stochastic Modeling for Energy-Efficient Edge Infrastructure](https://arxiv.org/abs/2511.03941)
*Fabio Diniz Rossi*

Main category: cs.DC

TL;DR: 提出了一种基于马尔可夫链的随机建模方法来分析边缘计算中的电源状态转换，通过AI驱动的预测性功率调节相比传统反应式方法能显著提高能效。


<details>
  <summary>Details</summary>
Motivation: 边缘计算虽然支持低延迟处理，但由于边缘设备的分布式特性和有限的能源资源，在电源管理方面面临挑战。

Method: 使用马尔可夫链建立随机模型分析功率状态转换，通过蒙特卡洛模拟验证模型，并进行敏感性分析评估不同转换概率对功率效率的影响。

Result: 实验结果表明，AI驱动的功率管理通过预测工作负载需求优化状态转换，减少了不必要的转换，提高了系统响应性和整体能效。

Conclusion: 基于AI的功率管理策略能显著提升边缘计算的能源效率，特别是在多节点环境中优化工作负载分配和自适应功率协调。

Abstract: Edge Computing enables low-latency processing for real-time applications but
introduces challenges in power management due to the distributed nature of edge
devices and their limited energy resources. This paper proposes a stochastic
modeling approach using Markov Chains to analyze power state transitions in
Edge Computing. By deriving steady-state probabilities and evaluating energy
consumption, we demonstrate the benefits of AI-driven predictive power scaling
over conventional reactive methods. Monte Carlo simulations validate the model,
showing strong alignment between theoretical and empirical results. Sensitivity
analysis highlights how varying transition probabilities affect power
efficiency, confirming that predictive scaling minimizes unnecessary
transitions and improves overall system responsiveness. Our findings suggest
that AI-based power management strategies significantly enhance energy
efficiency by anticipating workload demands and optimizing state transitions.
Experimental results indicate that AI-based power management optimizes workload
distribution across heterogeneous edge nodes, reducing energy consumption
disparities between devices, improving overall efficiency, and enhancing
adaptive power coordination in multi-node environments.

</details>


### [4] [Parallel Spawning Strategies for Dynamic-Aware MPI Applications](https://arxiv.org/abs/2511.04268)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo,Sergio Iserte*

Main category: cs.DC

TL;DR: 提出了一种新颖的并行生成策略，用于MPI应用程序的动态资源管理，通过进程协作生成和重新分配来减少执行时间，同时解决了收缩限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有MPI应用程序的动态资源管理方法存在局限性：要么重新生成整个应用程序成本高昂，要么在收缩时无法完全释放不需要的进程，导致系统资源无法有效回收。

Method: 采用并行生成策略，所有进程在重新分配前协作生成，减少执行时间；同时消除了收缩限制，允许并行系统更好地适应工作负载。

Result: 在保持竞争性扩展时间的同时（最多1.25倍开销），实现了快速收缩操作，成本降低至少20倍；在异构和同构系统上均得到验证。

Conclusion: 该策略有效解决了MPI应用程序动态资源管理中的关键问题，显著降低了重新配置成本，提高了系统资源利用效率。

Abstract: Dynamic resource management is an increasingly important capability of High
Performance Computing systems, as it enables jobs to adjust their resource
allocation at runtime. This capability has been shown to reduce workload
makespan, substantially decrease job waiting times and improve overall system
utilization. In this context, malleability refers to the ability of
applications to adapt to new resource allocations during execution. Although
beneficial, malleability incurs significant reconfiguration costs, making the
reduction of these costs an important research topic.
  Some existing methods for MPI applications respawn the entire application,
which is an expensive solution that avoids the reuse of original processes.
Other MPI methods reuse them, but fail to fully release unneeded processes when
shrinking, since some ranks within the same communicator remain active across
nodes, preventing the application from returning those nodes to the system.
This work overcomes both limitations by proposing a novel parallel spawning
strategy, in which all processes cooperate in spawning before redistribution,
thereby reducing execution time. Additionally, it removes shrinkage
limitations, allowing better adaptation of parallel systems to workload and
reducing their makespan. As a result, it preserves competitive expansion times
with at most a $1.25\times$ overhead, while enabling fast shrink operations
that reduce their cost by at least $20\times$. This strategy has been validated
on both homogeneous and heterogeneous systems and can also be applied in
shared-resource environments.

</details>


### [5] [Enabling Dynamic Sparsity in Quantized LLM Inference](https://arxiv.org/abs/2511.04477)
*Rongxiang Wang,Kangyuan Shu,Felix Xiaozhu Lin*

Main category: cs.DC

TL;DR: 提出了一种在低比特量化下实现动态稀疏推理的技术，包括zigzag量化布局、专用GEMV内核和紧凑运行时机制，在保持精度的同时提升解码吞吐量


<details>
  <summary>Details</summary>
Motivation: 在终端设备部署大语言模型面临内存和计算能力限制，而动态稀疏性与分组量化存在冲突，需要解决这一矛盾

Method: 采用zigzag模式量化布局组织权重以匹配激活稀疏性，设计专用GEMV内核充分利用并行计算单元，开发紧凑运行时机制收集稀疏索引

Result: 在多种模型规模和硬件配置下，解码吞吐量提升达1.55倍，精度与密集量化推理相当

Conclusion: 结构化稀疏性和量化可以在商用GPU上有效共存

Abstract: Deploying large language models (LLMs) on end-user devices is gaining
importance due to benefits in responsiveness, privacy, and operational cost.
Yet the limited memory and compute capability of mobile and desktop GPUs make
efficient execution difficult. Recent observations suggest that the internal
activations of LLMs are often dynamically sparse, meaning that for each input,
only part of the network contributes significantly to the output. Such sparsity
could reduce computation, but it interacts poorly with group-wise quantization,
which remains the dominant approach for fitting LLMs onto resource-constrained
hardware. To reconcile these two properties, this study proposes a set of
techniques that realize dynamic sparse inference under low-bit quantization.
The method features: (1) a zigzag-patterned quantization layout that organizes
weights in a way consistent with activation sparsity and improves GPU memory
locality; (2) a specialized GEMV kernel designed for this layout to fully
utilize parallel compute units; and (3) a compact runtime mechanism that
gathers sparse indices with minimal overhead. Across several model scales and
hardware configurations, the approach achieves up to 1.55x faster decoding
throughput while maintaining accuracy comparable to dense quantized inference,
showing that structured sparsity and quantization can effectively coexist on
commodity GPUs.

</details>


### [6] [A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems](https://arxiv.org/abs/2511.04523)
*Silvia Bonomi,Giovanni Farina,Roy Friedman,Eviatar B. Procaccia,Sebastien Tixeuil*

Main category: cs.DC

TL;DR: 提出了一种基于MAPE-K架构的自保护分布式系统，引入概率性移动拜占庭故障模型来捕获动态演变的攻击行为，并分析了系统在拜占庭节点数量超过阈值或自我恢复到安全状态所需的时间。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统面临日益增长的安全威胁，攻击者技能不断提升，漏洞遍布从硬件到应用层的整个系统栈。现有的拜占庭故障模型虽然增强了系统韧性，但在反映真实场景准确性方面存在局限。

Method: 基于MAPE-K架构构建自保护分布式系统，在分析组件中引入新的概率性移动拜占庭故障模型，该模型能够捕获攻击的动态演变特性，并驱动系统的自我保护与重配置策略。

Result: 通过数学分析得出拜占庭节点数量超过给定阈值或系统自我恢复到安全状态所需的时间，这取决于拜占庭感染传播速率与自我恢复速率的对比关系。同时提供了模拟结果来展示系统在此类假设下的行为。

Conclusion: 提出的概率性移动拜占庭故障模型能够更好地反映现实攻击场景的动态特性，为自保护分布式系统的安全分析与策略制定提供了有效工具。

Abstract: Modern distributed systems face growing security threats, as attackers
continuously enhance their skills and vulnerabilities span across the entire
system stack, from hardware to the application layer. In the system design
phase, fault tolerance techniques can be employed to safeguard systems. From a
theoretical perspective, an attacker attempting to compromise a system can be
abstracted by considering the presence of Byzantine processes in the system.
Although this approach enhances the resilience of the distributed system, it
introduces certain limitations regarding the accuracy of the model in
reflecting real-world scenarios. In this paper, we consider a self-protecting
distributed system based on the \emph{Monitoring-Analyse-Plan-Execute over a
shared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic
Mobile Byzantine Failure (MBF) that can be plugged into the Analysis component.
Our new model captures the dynamics of evolving attacks and can be used to
drive the self-protection and reconfiguration strategy. We analyze
mathematically the time that it takes until the number of Byzantine nodes
crosses given thresholds, or for the system to self-recover back into a safe
state, depending on the rates of Byzantine infection spreading \emph{vs.} the
rate of self-recovery. We also provide simulation results that illustrate the
behavior of the system under such assumptions.

</details>


### [7] [Resolving Conflicts with Grace: Dynamically Concurrent Universality](https://arxiv.org/abs/2511.04631)
*Petr Kuznetsov,Nathan Josia Schrodt*

Main category: cs.DC

TL;DR: 提出动态并发概念，仅在必要时使用强同步原语，并给出了动态并发的通用构造方法


<details>
  <summary>Details</summary>
Motivation: 同步是分布式计算可扩展性的主要障碍。并发操作在遇到冲突时需要同步，而冲突通常只在某些罕见状态下发生。理想情况下，应该根据当前系统状态动态检测冲突。

Method: 定义了动态并发概念：操作仅在必须与并发操作仲裁时才使用强同步原语。提出了动态并发的通用构造方法。

Result: 提出了动态并发的新概念和相应的通用构造方法

Conclusion: 动态并发方法可以根据系统状态动态调整同步需求，减少不必要的同步开销，提高分布式系统的可扩展性

Abstract: Synchronization is the major obstacle to scalability in distributed
computing. Concurrent operations on the shared data engage in synchronization
when they encounter a \emph{conflict}, i.e., their effects depend on the order
in which they are applied. Ideally, one would like to detect conflicts in a
\emph{dynamic} manner, i.e., adjusting to the current system state. Indeed, it
is very common that two concurrent operations conflict only in some rarely
occurring states. In this paper, we define the notion of \emph{dynamic
concurrency}: an operation employs strong synchronization primitives only if it
\emph{has} to arbitrate with concurrent operations, given the current system
state. We then present a dynamically concurrent universal construction.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies](https://arxiv.org/abs/2511.03944)
*Tong Zhang,Vikram Sharma Mailthody,Fei Sun,Linsen Ma,Chris J. Newburn,Teresa Zhang,Yang Liu,Jiangpeng Li,Hao Zhong,Wen-Mei Hwu*

Main category: cs.AR

TL;DR: 本文重新审视了经典的五分钟规则，通过整合主机成本、DRAM带宽/容量限制以及基于物理的SSD性能模型，提出了一个约束和负载感知的框架，发现在现代AI平台中DRAM到闪存的缓存阈值从分钟级降至秒级。


<details>
  <summary>Details</summary>
Motivation: 传统的五分钟规则仅基于存储-内存经济学，忽略了主机成本、可行性限制和工作负载行为。随着现代AI平台（特别是GPU主机与高性能SSD配对）的发展，需要重新评估这一经典启发式方法。

Method: 从第一性原理出发，整合主机成本、DRAM带宽/容量限制、基于物理的SSD性能和成本模型，并嵌入到约束和负载感知的框架中。开发了MQSim-Next SSD模拟器进行验证和敏感性分析。

Result: 对于现代AI平台，特别是GPU主机与超高性能SSD配对时，DRAM到闪存的缓存阈值从分钟级降至几秒级，这重新定义了NAND闪存作为活跃数据层的作用。

Conclusion: 将经典启发式方法转变为可操作的、可行性感知的分析和配置框架，为AI时代内存层次结构的进一步研究奠定了基础。

Abstract: In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a
simple, storage-memory-economics-based heuristic for deciding when data should
live in DRAM rather than on storage. Subsequent revisits to the rule largely
retained that economics-only view, leaving host costs, feasibility limits, and
workload behavior out of scope. This paper revisits the rule from first
principles, integrating host costs, DRAM bandwidth/capacity, and
physics-grounded models of SSD performance and cost, and then embedding these
elements in a constraint- and workload-aware framework that yields actionable
provisioning guidance. We show that, for modern AI platforms, especially
GPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained
random access, the DRAM-to-flash caching threshold collapses from minutes to a
few seconds. This shift reframes NAND flash memory as an active data tier and
exposes a broad research space across the hardware-software stack. We further
introduce MQSim-Next, a calibrated SSD simulator that supports validation and
sensitivity analysis and facilitates future architectural and system research.
Finally, we present two concrete case studies that showcase the software system
design space opened by such memory hierarchy paradigm shift. Overall, we turn a
classical heuristic into an actionable, feasibility-aware analysis and
provisioning framework and set the stage for further research on AI-era memory
hierarchy.

</details>


### [9] [PICNIC: Silicon Photonic Interconnected Chiplets with Computational Network and In-memory Computing for LLM Inference Acceleration](https://arxiv.org/abs/2511.04036)
*Yue Jiet Chong,Yimin Wang,Zhen Wu,Xuanyao Fong*

Main category: cs.AR

TL;DR: 提出了一种基于3D堆叠芯粒的大语言模型推理加速器，采用非易失性内存计算处理单元和硅光子互连技术解决通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型推理中的通信瓶颈问题，提高计算效率和可扩展性。

Method: 使用3D堆叠芯粒架构，集成非易失性内存计算处理单元和硅光子互连的IPCN网络，开发了专门的LLM映射方案来优化硬件调度和工作负载映射。

Result: 相比Nvidia A100实现了3.95倍加速和30倍效率提升；在采用芯粒聚类和功率门控方案后，相比Nvidia H100实现了57倍效率提升，同时保持相似吞吐量。

Conclusion: 该3D堆叠芯粒架构能有效解决LLM推理的通信瓶颈，显著提升性能和能效，并具有良好的可扩展性以适应更大模型。

Abstract: This paper presents a 3D-stacked chiplets based large language model (LLM)
inference accelerator, consisting of non-volatile in-memory-computing
processing elements (PEs) and Inter-PE Computational Network (IPCN),
interconnected via silicon photonic to effectively address the communication
bottlenecks. A LLM mapping scheme was developed to optimize hardware scheduling
and workload mapping. Simulation results show it achieves $3.95\times$ speedup
and $30\times$ efficiency improvement over the Nvidia A100 before chiplet
clustering and power gating scheme (CCPG). Additionally, the system achieves
further scalability and efficiency improvement with the implementation of CCPG
to accommodate larger models, attaining $57\times$ efficiency improvement over
Nvidia H100 at similar throughput.

</details>


### [10] [Disaggregated Architectures and the Redesign of Data Center Ecosystems: Scheduling, Pooling, and Infrastructure Trade-offs](https://arxiv.org/abs/2511.04104)
*Chao Guo,Jiahe Xu,Moshe Zukerman*

Main category: cs.AR

TL;DR: 本文概述了硬件解耦在数据中心中的动机、进展、挑战和机遇，认为其有潜力重塑整个数据中心生态系统。


<details>
  <summary>Details</summary>
Motivation: 将传统服务器资源转变为统一资源池，提升数据中心资源利用率和灵活性。

Method: 提供硬件解耦的综述，包括动机、最新进展，并通过数值研究展示关键挑战。

Result: 硬件解耦在工业界和学术界都取得了显著进展，但仍有挑战需要解决。

Conclusion: 硬件解耦有潜力重塑数据中心生态系统，影响应用设计、资源调度、硬件配置、冷却和电力系统优化等方面。

Abstract: Hardware disaggregation seeks to transform Data Center (DC) resources from
traditional server fleets into unified resource pools. Despite existing
challenges that may hinder its full realization, significant progress has been
made in both industry and academia. In this article, we provide an overview of
the motivations and recent advancements in hardware disaggregation. We further
discuss the research challenges and opportunities associated with disaggregated
architectures, focusing on aspects that have received limited attention. We
argue that hardware disaggregation has the potential to reshape the entire DC
ecosystem, impacting application design, resource scheduling, hardware
configuration, cooling, and power system optimization. Additionally, we present
a numerical study to illustrate several key aspects of these challenges.

</details>


### [11] [AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM](https://arxiv.org/abs/2511.04321)
*Yuanpeng Zhang,Xing Hu,Xi Chen,Zhihang Yuan,Cong Li,Jingchen Zhu,Zhao Wang,Chenguang Zhang,Xin Si,Wei Gao,Qiang Wu,Runsheng Wang,Guangyu Sun*

Main category: cs.AR

TL;DR: AIM是一个针对高性能SRAM存内计算(PIM)的软硬件协同设计架构，通过架构级IR-drop缓解技术，在7nm 256-TOPS PIM芯片上实现了69.2%的IR-drop缓解、2.29倍能效提升和1.152倍加速。


<details>
  <summary>Details</summary>
Motivation: SRAM存内计算虽然具有高性能优势，但追求更高性能会导致更复杂的电路设计和更高的工作频率，从而加剧IR-drop问题，影响芯片性能和可靠性。传统的电路级IR-drop缓解方法资源密集且会牺牲PPA(功耗、性能、面积)。

Method: 提出AIM软硬件协同设计框架：1) 利用PIM的位串行和原位数据流特性，建立PIM工作负载与IR-drop的直接关联(Rtog和HR)；2) 通过LHR和WDS进行架构级IR-drop缓解探索；3) 开发IR-Booster动态调整机制，结合软件级HR信息和硬件IR-drop监控来调整PIM宏的电压-频率对；4) 提出HR感知的任务映射方法。

Result: 在7nm 256-TOPS PIM芯片的后端仿真中，AIM实现了高达69.2%的IR-drop缓解，带来2.29倍的能效提升和1.152倍的加速。

Conclusion: AIM通过软硬件协同设计成功解决了高性能PIM中的IR-drop问题，在保持计算精度的同时显著提升了能效和性能，为未来高性能存内计算系统提供了有效的架构级解决方案。

Abstract: SRAM Processing-in-Memory (PIM) has emerged as the most promising
implementation for high-performance PIM, delivering superior computing density,
energy efficiency, and computational precision. However, the pursuit of higher
performance necessitates more complex circuit designs and increased operating
frequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly
degrade chip performance and even threaten reliability. Conventional
circuit-level IR-drop mitigation methods, such as back-end optimizations, are
resource-intensive and often compromise power, performance, and area (PPA). To
address these challenges, we propose AIM, comprehensive software and hardware
co-design for architecture-level IR-drop mitigation in high-performance PIM.
Initially, leveraging the bit-serial and in-situ dataflow processing properties
of PIM, we introduce Rtog and HR, which establish a direct correlation between
PIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS,
enabling extensive exploration of architecture-level IR-drop mitigation while
maintaining computational accuracy through software optimization. Subsequently,
we develop IR-Booster, a dynamic adjustment mechanism that integrates
software-level HR information with hardware-based IR-drop monitoring to adapt
the V-f pairs of the PIM macro, achieving enhanced energy efficiency and
performance. Finally, we propose the HR-aware task mapping method, bridging
software and hardware designs to achieve optimal improvement. Post-layout
simulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up
to 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement
and 1.152x speedup.

</details>


### [12] [Scalable and Efficient Intra- and Inter-node Interconnection Networks for Post-Exascale Supercomputers and Data centers](https://arxiv.org/abs/2511.04677)
*Joaquin Tarraga-Moreno,Daniel Barley,Francisco J. Andujar Munoz,Jesus Escudero-Sahuquillo,Holger Froning,Pedro Javier Garcia,Francisco J. Quiles,Jose Duato*

Main category: cs.AR

TL;DR: 现代超级计算机和数据中心正朝着异构和紧密集成的架构发展，以支持数据密集型应用，但多加速器节点间的通信瓶颈日益突出。


<details>
  <summary>Details</summary>
Motivation: 生成式AI、科学模拟和大规模分析等数据密集型应用的快速增长，推动了对高性能计算系统的需求，需要减少数据移动并提高计算效率。

Method: 通过结合强大的CPU、加速器以及新兴的高带宽内存和存储技术，构建异构和紧密集成的系统架构。

Result: 随着每个节点中加速器数量的增加，在节点内部和节点之间出现了通信瓶颈，特别是在网络资源被异构组件共享时。

Conclusion: 异构系统架构在提升计算效率的同时，面临着通信瓶颈的挑战，特别是在多加速器环境下网络资源共享的问题。

Abstract: The rapid growth of data-intensive applications such as generative AI,
scientific simulations, and large-scale analytics is driving modern
supercomputers and data centers toward increasingly heterogeneous and tightly
integrated architectures. These systems combine powerful CPUs and accelerators
with emerging high-bandwidth memory and storage technologies to reduce data
movement and improve computational efficiency. However, as the number of
accelerators per node increases, communication bottlenecks emerge both within
and between nodes, particularly when network resources are shared among
heterogeneous components.

</details>
