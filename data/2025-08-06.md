<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Compositional Quantum Control Flow with Efficient Compilation in Qunity](https://arxiv.org/abs/2508.02857)
*Mikhail Mints,Finn Voichick,Leonidas Lampropoulos,Robert Rand*

Main category: cs.PL

TL;DR: 本文提出了一种高效的量子控制流构造编译方法，基于Qunity语言，通过优化技术显著减少了量子比特和门的数量。


<details>
  <summary>Details</summary>
Motivation: 现有量子编程语言缺乏高效的高层抽象实现，尤其是量子控制流构造，Qunity虽有提出但缺乏实现且编译效率低。

Method: 在Qunity基础上引入更多抽象构造，开发完整的编译器实现，并优化编译流程的多个阶段。

Result: 实现了高效的Qunity编译器，显著减少了量子比特和门的数量，生成OpenQASM 3代码。

Conclusion: 通过优化技术，成功提升了量子控制流构造的编译效率，为高层量子编程提供了实用工具。

Abstract: Most existing quantum programming languages are based on the quantum circuit
model of computation, as higher-level abstractions are particularly challenging
to implement - especially ones relating to quantum control flow. The Qunity
language, proposed by Voichick et al., offered such an abstraction in the form
of a quantum control construct, with great care taken to ensure that the
resulting language is still realizable. However, Qunity lacked a working
implementation, and the originally proposed compilation procedure was very
inefficient, with even simple quantum algorithms compiling to unreasonably
large circuits.
  In this work, we focus on the efficient compilation of high-level quantum
control flow constructs, using Qunity as our starting point. We introduce a
wider range of abstractions on top of Qunity's core language that offer
compelling trade-offs compared to its existing control construct. We create a
complete implementation of a Qunity compiler, which converts high-level Qunity
code into the quantum assembly language OpenQASM 3. We develop optimization
techniques for multiple stages of the Qunity compilation procedure, including
both low-level circuit optimizations as well as methods that consider the
high-level structure of a Qunity program, greatly reducing the number of qubits
and gates used by the compiler.

</details>


### [2] [SAGE-HLS: Syntax-Aware AST-Guided LLM for High-Level Synthesis Code Generation](https://arxiv.org/abs/2508.03558)
*M Zafir Sadik Khan,Nowfel Mashnoor,Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: 论文提出SAGE-HLS，一种针对HLS代码生成的微调LLM，解决了HLS领域数据稀缺问题，并通过Verilog-to-C/C++转换、AST引导的微调策略和半自动化评估框架，实现了高成功率的代码生成。


<details>
  <summary>Details</summary>
Motivation: 由于HLS领域公开代码数据集稀缺，现有LLM在HLS代码生成中的应用受限，亟需一种专门针对HLS的微调模型。

Method: 1. 通过Verilog-to-C/C++转换创建16.7K HLS代码数据集；2. 基于AST的指令提示微调策略；3. 使用VerilogEval半自动化评估框架。

Result: SAGE-HLS在QwenCoder (2.5) 7B模型上微调后，代码可合成率接近100%，功能正确率达75%。

Conclusion: SAGE-HLS为解决HLS代码生成问题提供了有效方案，显著提升了生成代码的质量和功能性。

Abstract: In today's rapidly evolving field of electronic design automation (EDA), the
complexity of hardware designs is increasing, necessitating more sophisticated
automation solutions. High-level synthesis (HLS), as a pivotal solution,
automates hardware designs from high-level abstractions (e.g., C/C++). However,
it faces significant challenges, particularly in design space exploration and
optimization. While large language models (LLMs) have shown notable
capabilities in code generation, their application to HLS has been limited due
to the scarcity of (publicly) available HLS code datasets. Hence, research in
this domain has primarily focused on techniques such as prompt engineering and
retrieval-augmented generation (RAG). To overcome this limitation, this paper
introduces SAGE-HLS, the first-of-its-kind fine-tuned LLM specifically for HLS
code generation. Our method includes three key advancements: (i) We implement
Verilog-to-C/C++ porting, converting verified and synthesizable Verilog codes
into corresponding C, creating a dataset of 16.7K HLS codes; (ii) We implement
a fine-tuning strategy, which is based on instruction prompting to code
generation guided by abstract syntax tree (AST); (iii) We develop a
semi-automated evaluation framework using VerilogEval to assess the
functionality of the generated HLS code. Our experiments show that SAGE-HLS,
fined-tuned on the QwenCoder (2.5) 7B model, achieves a near 100% success rate
in code synthesizability and a 75% success rate in functional correctness.

</details>


### [3] [Teaching Introductory Functional Programming Using Haskelite](https://arxiv.org/abs/2508.03640)
*Pedro Vasconcelos*

Main category: cs.PL

TL;DR: 论文探讨了在教授函数式编程时使用逐步追踪解释器的经验，展示了其在帮助学生理解递归、代数数据类型和高阶函数方面的效果。


<details>
  <summary>Details</summary>
Motivation: 学生在学习函数式编程时对替换概念的应用存在困难，尤其是在递归定义、代数数据类型和高阶函数等新场景中。逐步解释器被证明能帮助初学者澄清误解并提升理解。

Method: 在波尔图大学的函数式编程入门课程中，使用了一个针对Haskell子集的逐步追踪解释器，并收集了学生的反馈。

Result: 学生反馈表明逐步解释器有助于理解复杂概念，课程经验提供了改进方向。

Conclusion: 逐步追踪解释器在教学中具有潜力，未来可进一步优化和扩展其应用。

Abstract: Learning functional programming requires learning a substitution-based
computational model. While substitution should be a familiar concept from
high-school algebra, students often have difficulty applying it to new
settings, such as recursive definitions, algebraic data types and higher-order
functions. Step-by-step interpreters have been shown to help beginners by
clarifying misconceptions and improving understanding.
  This paper reports on the experience of using a step-by-step tracing
interpreter for a subset of Haskell while teaching an introductory functional
programming course at the University of Porto. We describe the use of the
interpreter, present some feedback obtained from students, reflect on the
lessons learned and point directions for further work.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Low-Communication Resilient Distributed Estimation Algorithm Based on Memory Mechanism](https://arxiv.org/abs/2508.02705)
*Wei Li,Limei Hu,Feng Chen,Ye Yao*

Main category: cs.DC

TL;DR: 提出了一种低通信的弹性分布式估计算法，通过信誉节点选择和W-SVDD模型增强抗攻击能力，减少通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决多任务对抗网络中因受攻击节点或链路导致的参数估计不准确问题。

Method: 采用信誉节点选择策略和W-SVDD模型训练数据，引入事件触发机制减少无效更新。

Result: 仿真结果显示算法在减少通信成本的同时表现优于其他算法。

Conclusion: 该算法通过信誉选择和W-SVDD模型有效提升了分布式估计的弹性和效率。

Abstract: In multi-task adversarial networks, the accurate estimation of unknown
parameters in a distributed algorithm is hindered by attacked nodes or links.
To tackle this challenge, this brief proposes a low-communication resilient
distributed estimation algorithm. First, a node selection strategy based on
reputation is introduced that allows nodes to communicate with more reliable
subset of neighbors. Subsequently, to discern trustworthy intermediate
estimates, the Weighted Support Vector Data Description (W-SVDD) model is
employed to train the memory data. This trained model contributes to reinforce
the resilience of the distributed estimation process against the impact of
attacked nodes or links. Additionally, an event-triggered mechanism is
introduced to minimize ineffective updates to the W-SVDD model, and a suitable
threshold is derived based on assumptions. The convergence of the algorithm is
analyzed. Finally, simulation results demonstrate that the proposed algorithm
achieves superior performance with less communication cost compared to other
algorithms.

</details>


### [5] [A DataOps Toolbox Enabling Continuous Semantic Integration of Devices for Edge-Cloud AI Applications](https://arxiv.org/abs/2508.02708)
*Mario Scrocca,Marco Grassi,Alessio Carenini,Jean-Paul Calbimonte,Darko Anicic,Irene Celino*

Main category: cs.DC

TL;DR: 论文提出了一种基于语义Web技术和低代码机制的DataOps工具箱，用于解决复杂环境中AI应用的多设备协作与数据互操作性问题。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中（如工业车间、道路基础设施和医疗治疗）实现AI应用时，多设备协作与异构数据互操作性是一个关键挑战。

Method: 设计并实现了一个DataOps工具箱，采用语义Web技术和低代码机制，支持持续语义集成，以处理多种设备、数据格式、语义和通信接口。

Result: 工具箱在三个不同领域的用例中成功应用，实现了静态节点信息和运行时数据交换的互操作性。

Conclusion: 通过试点活动验证了工具箱的有效性，并总结了相关经验教训。

Abstract: The implementation of AI-based applications in complex environments often
requires the collaboration of several devices spanning from edge to cloud.
Identifying the required devices and configuring them to collaborate is a
challenge relevant to different scenarios, like industrial shopfloors, road
infrastructures, and healthcare therapies. We discuss the design and
implementation of a DataOps toolbox leveraging Semantic Web technologies and a
low-code mechanism to address heterogeneous data interoperability requirements
in the development of such applications. The toolbox supports a continuous
semantic integration approach to tackle various types of devices, data formats,
and semantics, as well as different communication interfaces. The paper
presents the application of the toolbox to three use cases from different
domains, the DataOps pipelines implemented, and how they guarantee
interoperability of static nodes' information and runtime data exchanges.
Finally, we discuss the results from the piloting activities in the use cases
and the lessons learned.

</details>


### [6] [PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows](https://arxiv.org/abs/2508.02866)
*Renan Souza,Amal Gueroudji,Stephen DeWitt,Daniel Rosendo,Tirthankar Ghosal,Robert Ross,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 论文提出PROV-AGENT模型，扩展W3C PROV以支持AI代理工作流中的细粒度溯源，提升透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: AI代理在复杂工作流中可能产生错误或幻觉，其决策可能传播错误，因此需要细粒度溯源以追踪代理决策及其影响。

Method: 扩展W3C PROV并利用MCP协议，提出PROV-AGENT模型，实时捕获代理交互的溯源数据。

Result: 开发了开源系统支持实时溯源，并在多环境中验证了关键溯源查询和代理可靠性分析。

Conclusion: PROV-AGENT模型有效解决了AI代理工作流中的溯源问题，提升了透明度和可靠性。

Abstract: Foundation models, such as Large Language Models (LLMs), are increasingly
used as core components of AI agents in complex, large-scale workflows across
federated and heterogeneous environments. In agentic workflows, autonomous
agents plan tasks, interact with humans and peers, and shape scientific
outcomes. This makes transparency, traceability, reproducibility, and
reliability essential. However, AI-based agents can hallucinate or reason
incorrectly, and their decisions may propagate errors through the workflow,
especially when one agent's output feeds into another's input. Therefore,
fine-grained provenance is essential to link agent decisions, their end-to-end
context, and downstream impacts. While provenance techniques have long
supported reproducibility and workflow data understanding, they fail to capture
and relate agent-centric metadata (prompts, responses, and decisions) with the
rest of the workflow. In this paper, we introduce PROV-AGENT, a provenance
model that extends W3C PROV and leverages the Model Context Protocol (MCP) to
integrate agent interactions into end-to-end workflow provenance. Our
contributions include: (1) a provenance model tailored for agentic workflows,
(2) a near real-time, open-source system for capturing agentic provenance, and
(3) a cross-facility evaluation spanning edge, cloud, and HPC environments,
demonstrating support for critical provenance queries and agent reliability
analysis.

</details>


### [7] [Optimal Simultaneous Byzantine Agreement, Common Knowledge and Limited Information Exchange](https://arxiv.org/abs/2508.03418)
*Ron van der Meyden*

Main category: cs.DC

TL;DR: 论文重新审视了基于认知逻辑的分布式算法分析，提出了更实用的信息交换方式，解决了同时拜占庭协议问题中的一些模糊点，并展示了在某些条件下其解决方案的最优性。


<details>
  <summary>Details</summary>
Motivation: 传统基于‘全信息协议’的分布式算法分析在空间和计算时间上效率低下，论文旨在探索更实用的信息交换方式。

Method: 通过认知逻辑分析同时拜占庭协议问题，明确‘非故障’与‘尚未故障’的区别，并在特定条件下实现基于知识的程序。

Result: 在特定失败模型和信息交换协议下，该方案相对于同类信息交换的解决方案是最优的，但并非在所有情况下都成立。

Conclusion: 论文为同时拜占庭协议问题提供了更高效的解决方案，但需注意其适用条件。

Abstract: In order to develop solutions that perform actions as early as possible,
analysis of distributed algorithms using epistemic logic has generally
concentrated on ``full information protocols'', which may be inefficient with
respect to space and computation time. The paper reconsiders the epistemic
analysis of the problem of Simultaneous Byzantine Agreement with respect to
weaker, but more practical, exchanges of information. The paper first clarifies
some issues concerning both the specification of this problem and the knowledge
based program characterizing its solution, concerning the distinction between
the notions of ``nonfaulty'' and ``not yet failed'', on which there are
variances in the literature. It is then shown that, when implemented relative
to a given failure model and an information exchange protocol satisfying
certain conditions, this knowledge based program yields a protocol that is
optimal relative to solutions using the same information exchange. Conditions
are also identified under which this implementation is also an optimum, but an
example is provided that shows this does not hold in general.

</details>


### [8] [Understanding the Landscape of Ampere GPU Memory Errors](https://arxiv.org/abs/2508.03513)
*Zhu Zhu,Yu Sun,Dhatri Parakal,Bo Fang,Steven Farrell,Gregory H. Bauer,Brett Bode,Ian T. Foster,Michael E. Papka,William Gropp,Zhao Zhang,Lishan Yang*

Main category: cs.DC

TL;DR: 本文通过大规模跨超级计算机研究，分析了NVIDIA A100 GPU的内存可靠性，比较了Delta、Polaris和Perlmutter三台超级计算机的错误率与MTBE，并探讨了其对高性能计算系统设计的启示。


<details>
  <summary>Details</summary>
Motivation: GPU已成为加速高性能计算应用的主要工具，了解其内存错误行为对实现高效可靠的HPC系统至关重要。

Method: 研究覆盖了三台配备NVIDIA A100 GPU的超级计算机（Delta、Polaris和Perlmutter），分析了67.77百万GPU设备小时的数据，比较了错误率和MTBE。

Result: 研究揭示了这三台系统的共享和独特错误特征，并提供了对超级计算机可靠运行、检查点间隔选择以及与上一代GPU可靠性比较的见解。

Conclusion: 该研究为容错HPC系统设计和操作提供了宝贵见解，有助于更高效地执行HPC应用。

Abstract: Graphics Processing Units (GPUs) have become a de facto solution for
accelerating high-performance computing (HPC) applications. Understanding their
memory error behavior is an essential step toward achieving efficient and
reliable HPC systems. In this work, we present a large-scale
cross-supercomputer study to characterize GPU memory reliability, covering
three supercomputers - Delta, Polaris, and Perlmutter - all equipped with
NVIDIA A100 GPUs. We examine error logs spanning 67.77 million GPU device-hours
across 10,693 GPUs. We compare error rates and mean-time-between-errors (MTBE)
and highlight both shared and distinct error characteristics among these three
systems. Based on these observations and analyses, we discuss the implications
and lessons learned, focusing on the reliable operation of supercomputers, the
choice of checkpointing interval, and the comparison of reliability
characteristics with those of previous-generation GPUs. Our characterization
study provides valuable insights into fault-tolerant HPC system design and
operation, enabling more efficient execution of HPC applications.

</details>


### [9] [In-Memory Non-Binary LDPC Decoding](https://arxiv.org/abs/2508.03567)
*Oscar Ferraz,Vitor Silva,Gabriel Falcao*

Main category: cs.DC

TL;DR: 本文提出了一种基于近内存处理（PiM）的非二进制LDPC解码器，首次在UPMEM系统中实现，性能优于低功耗GPU并行解决方案。


<details>
  <summary>Details</summary>
Motivation: LDPC码在通信和存储应用中具有重要作用，但计算复杂且受限于数据移动瓶颈。PiM范式通过将计算单元设计在数据存储位置附近，缓解了这一瓶颈。

Method: 设计了一种新型高效的近内存非二进制LDPC解码器，利用UPMEM系统中的低复杂度处理单元进行位级和简单算术操作。

Result: PiM非二进制LDPC解码器实现了76 Mbit/s的解码吞吐量，性能优于边缘GPU实现。

Conclusion: 该研究展示了PiM在非二进制LDPC解码中的潜力，为未来高效解码器设计提供了新思路。

Abstract: Low-density parity-check (LDPC) codes are an important feature of several
communication and storage applications, offering a flexible and effective
method for error correction. These codes are computationally complex and
require the exploitation of parallel processing to meet real-time constraints.
As advancements in arithmetic and logic unit technology allowed for higher
performance of computing systems, memory technology has not kept the same pace
of development, creating a data movement bottleneck and affecting parallel
processing systems more dramatically. To alleviate the severity of this
bottleneck, several solutions have been proposed, namely the processing
in-memory (PiM) paradigm that involves the design of compute units to where (or
near) the data is stored, utilizing thousands of low-complexity processing
units to perform out bit-wise and simple arithmetic operations. This paper
presents a novel efficient solution for near-memory non-binary LDPC decoders in
the UPMEM system, for the best of our knowledge the first real hardware
PiM-based non-binary LDPC decoder that is benchmarked against low-power GPU
parallel solutions highly optimized for throughput performance. PiM-based
non-binary LDPC decoders can achieve 76 Mbit/s of decoding throughput, which is
even competitive when compared against implementations running in edge GPUs.

</details>


### [10] [Block: Balancing Load in LLM Serving with Context, Knowledge and Predictive Scheduling](https://arxiv.org/abs/2508.03611)
*Wei Da,Evangelia Kalyvianaki*

Main category: cs.DC

TL;DR: Block是一个分布式调度框架，通过利用请求的上下文信息优化大型语言模型服务框架中的负载均衡和自动配置。


<details>
  <summary>Details</summary>
Motivation: 现有模型服务系统依赖单一且启发式的任务调度器，无法满足低开销、可靠性和可扩展性的需求。

Method: Block采用完全分布式、无状态和预测性调度系统，利用LLM推理的确定性特征（如主机配置、响应长度和硬件性能）进行调度决策。

Result: 在12个GPU集群上的评估显示，Block显著优于启发式调度器，服务容量提升16.7%，P99尾部延迟降低49.5%。

Conclusion: Block在多样化模型、负载和配置下均表现稳定，代码和数据已开源。

Abstract: This paper presents Block, a distributed scheduling framework designed to
optimize load balancing and auto-provisioning across instances in large
language model serving frameworks by leveraging contextual information from
incoming requests. Unlike popular model serving systems that rely on monolithic
and heuristic task schedulers, Block operates as a fully distributed,
stateless, and predictive scheduling system to achieve low overhead,
reliability, and scalability. It leverages the deterministic and predictable
characteristics of LLM inferences, such as host configurations, response
lengths, and hardware performance, to make scheduling decisions based on
accurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block
significantly outperforms heuristic schedulers, boosting serving capacity by up
to 16.7\% and reducing P99 tail latency by up to 49.5\%. These performance
gains remain consistent across diverse models, workloads and configurations.
Code and data are open-sourced.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [Mamba-X: An End-to-End Vision Mamba Accelerator for Edge Computing Devices](https://arxiv.org/abs/2508.02977)
*Dongho Yoon,Gungyu Lee,Jaewon Chang,Yunjae Lee,Dongjae Lee,Minsoo Rhu*

Main category: cs.AR

TL;DR: Vision Mamba利用状态空间模型（SSM）降低计算和内存需求，但GPU效率受限。Mamba-X通过并行扫描阵列和量化技术优化部署。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在视觉任务中计算和内存需求高的问题，同时优化Vision Mamba在边缘设备上的部署效率。

Method: 提出Mamba-X加速器，采用并行扫描阵列和混合量化技术。

Result: 降低了延迟和内存消耗，同时保持准确性。

Conclusion: Mamba-X为Vision Mamba在边缘设备上的高效部署提供了可行方案。

Abstract: Transformers have proven effective in language modeling but are limited by
high computational and memory demands that grow quadratically with input
sequence length. State space models (SSMs) offer a promising alternative by
reducing attention complexity from $O(L^2)$ to $O(L)$ while also lowering
overall memory consumption. Vision Mamba adapts the SSM approach for computer
vision tasks, achieving lower latency and memory consumption than traditional
transformer models. However, deploying Vision Mamba on edge devices is
challenging due to its sequential scan operations, which hinder GPU efficiency.
We propose Mamba-X, an end-to-end Vision Mamba accelerator that includes a
systolic scan array to maximize parallelism and minimize memory traffic, along
with a hybrid, hardware-friendly quantization technique to reduce memory usage
and improve hardware efficiency without sacrificing accuracy.

</details>


### [12] [Towards Memory Specialization: A Case for Long-Term and Short-Term RAM](https://arxiv.org/abs/2508.02992)
*Peijing Li,Muhammad Shahir Abdurraman,Rachel Cleaveland,Sergey Legtchenko,Philip Levis,Ioan Stefanovici,Thierry Tambe,David Tennenhouse,Caroline Trippel*

Main category: cs.AR

TL;DR: 论文提出从传统内存层次结构转向利用应用特定访问模式的专用内存架构，引入LtRAM和StRAM两类新内存，并探讨其实现技术和研究挑战。


<details>
  <summary>Details</summary>
Motivation: 内存成本停止下降，成为系统成本主导，需通过专用内存架构优化应用性能。

Method: 提出LtRAM和StRAM两类新内存，探讨其设备技术和系统集成方案。

Result: 识别了实现高效可扩展计算系统的关键研究挑战。

Conclusion: 专用内存架构是未来计算系统高效扩展的必要方向。

Abstract: Both SRAM and DRAM have stopped scaling: there is no technical roadmap to
reduce their cost (per byte/GB). As a result, memory now dominates system cost.
This paper argues for a paradigm shift from today's simple memory hierarchy
toward specialized memory architectures that exploit application-specific
access patterns. Rather than relying solely on traditional off-chip DRAM and
on-chip SRAM, we envisage memory systems equipped with additional types of
memory whose performance trade-offs benefit workloads through non-hierarchical
optimization. We propose two new memory classes deserving explicit OS support:
long-term RAM (LtRAM) optimized for read-intensive data with long lifetimes,
and short-term RAM (StRAM) designed for transient, frequently-accessed data
with short lifetimes. We explore underlying device technologies that could
implement these classes, including their evolution and their potential
integration into current system designs given emerging workload requirements.
We identify critical research challenges to realize what we believe is a
necessary evolution toward more efficient and scalable computing systems
capable of meeting future demands.

</details>
