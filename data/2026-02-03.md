<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 18]
- [cs.AR](#cs.AR) [Total: 10]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Phoenix: A Modular and Versatile Framework for C/C++ Pointer Analysis](https://arxiv.org/abs/2602.01720)
*Peisen Yao,Zinan Gu,Qingkai Shi*

Main category: cs.PL

TL;DR: Phoenix是一个模块化的C/C++指针分析框架，统一了多种先进的别名分析算法，通过清晰的架构分离提高了分析的可比性和可组合性，在性能上优于现有系统SVF。


<details>
  <summary>Details</summary>
Motivation: 当前C/C++指针分析生态系统存在碎片化问题，不同分析算法难以比较、交换和组合，需要一个统一的框架来解决这些问题。

Method: 采用模块化架构，将IR构造、约束生成、求解器后端和客户端查询清晰分离，支持多种先进的别名分析算法在同一接口下工作。

Result: 在28个GNU coreutils程序上评估，Phoenix在流不敏感和上下文不敏感配置下比SVF快达2.88倍，在更精确的流敏感和上下文敏感配置下也保持竞争力（快达2.91倍），且没有系统性运行时开销。

Conclusion: Phoenix成功解决了指针分析碎片化问题，提供了稳定、高效的统一框架，已在工业级静态分析和模糊测试工具中应用，发现了数百个新bug。

Abstract: We present Phoenix, a modular pointer analysis framework for C/C++ that unifies multiple state-of-the-art alias analysis algorithms behind a single, stable interface. Phoenix addresses the fragmentation of today's C/C++ pointer analysis ecosystem by cleanly separating IR construction, constraint generation, solver backends, and client-facing queries, making analyses easy to compare, swap, and compose while exposing explicit precision-performance trade-offs. We evaluate Phoenix against SVF under two representative configurations: a flow- and context-insensitive setting and a more precise flow- and context-sensitive setting, on 28 GNU coreutils programs. Phoenix delivers robust speedups in the baseline configuration (up to 2.88x) and remains competitive, and often faster, even in the stronger precision regime (up to 2.91x), without a systematic runtime penalty. In production, Phoenix serves as the analysis substrate for static analysis and fuzzing tools that have uncovered hundreds of new bugs and enabled deployments reporting more than 1000 bugs found in an industrial toolchain.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [What Artificial Intelligence can do for High-Performance Computing systems?](https://arxiv.org/abs/2602.00014)
*Pierrick Pochelu,Hyacinthe Cartiaux,Julien Schleich*

Main category: cs.DC

TL;DR: 这篇综述评估了AI（包括机器学习和优化）如何提高HPC系统运行效率，分析了2019-2025年间74篇相关论文，识别出六个应用领域，其中调度是最活跃的研究方向。


<details>
  <summary>Details</summary>
Motivation: 高性能计算中心消耗大量电力，带来环境和运营成本问题。AI技术有望提高HPC系统的运行效率，减少能耗和成本。

Method: 手动筛选了2019-2025年间约1800篇出版物，使用预定义的纳入/排除标准，最终保留了74篇"AI for HPC"论文，并将其分为六个应用领域进行分析。

Result: 识别出六个主要应用领域：性能估计、性能优化、调度、代理建模、故障检测和基于语言模型的自动化。调度是最活跃的研究领域，从研究导向的强化学习调度器到生产友好的混合方法。图神经网络和时间序列模型增强了异常检测能力，专门针对HPC的领域语言模型在特定任务上优于通用LLM。

Conclusion: 研究强调了AI在HPC系统优化中的整合机会，如基于LLM的操作系统概念，同时指出了需要在MLOps、AI组件标准化和基准测试方法方面取得进展。

Abstract: High-performance computing (HPC) centers consume substantial power, incurring environmental and operational costs. This review assesses how artificial intelligence (AI), including machine learning (ML) and optimization, improves the efficiency of operational HPC systems. Approximately 1,800 publications from 2019 to 2025 were manually screened using predefined inclusion/exclusion criteria; 74 "AI for HPC" papers were retained and grouped into six application areas: performance estimation, performance optimization, scheduling, surrogate modeling, fault detection, and language-model-based automation.
  Scheduling is the most active area, spanning research-oriented reinforcement-learning schedulers to production-friendly hybrids that combine ML with heuristics. Supervised performance estimation is foundational for both scheduling and optimization. Graph neural networks and time-series models strengthen anomaly detection by capturing spatio-temporal dependencies in production telemetry. Domain-specialized language models for HPC can outperform general-purpose LLMs on targeted coding and automation tasks. Together, these findings highlight integration opportunities such as LLM-based operating-system concepts and underscore the need for advances in MLOps, standardization of AI components, and benchmarking methodology.

</details>


### [3] [A Fault-Tolerant Version of Safra's Termination Detection Algorithm](https://arxiv.org/abs/2602.00272)
*Wan Fokkink,Georgios Karlos,Andy Tatman*

Main category: cs.DC

TL;DR: 将Safra分布式终止检测算法改造为容错版本，通过节点级计数器、本地环恢复和备份令牌机制，实现无额外消息开销、容忍任意数量同时崩溃的分布式容错


<details>
  <summary>Details</summary>
Motivation: 经典Safra分布式终止检测算法缺乏容错能力，当节点崩溃时无法正常工作。需要设计一个能够容忍节点崩溃的容错版本，以增强分布式系统的可靠性。

Method: 1. 将令牌中的全局计数器拆分为每个节点的独立计数器，丢弃崩溃节点的计数；2. 节点崩溃时在本地恢复令牌环结构；3. 发送备份令牌；4. 通过令牌传递节点崩溃信息；5. 保持无额外消息开销的分布式设计。

Result: 算法能够容忍任意数量的节点崩溃，包括同时发生的崩溃，以完全分布式方式处理故障，且不增加额外的消息开销。提供了原始算法和容错变体的正确性证明，以及模型检查分析。

Conclusion: 成功将Safra终止检测算法改造为容错版本，保持了原算法的效率优势，同时显著增强了分布式系统的鲁棒性，为实际部署提供了可靠的终止检测解决方案。

Abstract: Safra's distributed termination detection algorithm employs a logical token ring structure within a distributed network; only passive nodes forward the token, and a counter in the token keeps track of the number of sent minus the number of received messages. We adapt this classic algorithm to make it fault-tolerant. The counter is split into counters per node, to discard counts from crashed nodes. If a node crashes, the token ring is restored locally and a backup token is sent. Nodes inform each other of detected crashes via the token. Our algorithm imposes no additional message overhead, tolerates any number of crashes as well as simultaneous crashes, and copes with crashes in a decentralized fashion. Correctness proofs are provided of both the original Safra's algorithm and its fault-tolerant variant, as well as a model checking analysis.

</details>


### [4] [Training LLMs with Fault Tolerant HSDP on 100,000 GPUs](https://arxiv.org/abs/2602.00277)
*Omkar Salpekar,Rohan Varma,Kenny Yu,Vladimir Ivanov,Yang Wang,Ahmed Sharif,Min Si,Shawn Xu,Feng Tian,Shengbao Zheng,Tristan Rice,Ankush Garg,Shangfu Peng,Shreyas Siravara,Wenyin Fu,Rodrigo de Castro,Adithya Gangidi,Andrey Obraztsov,Sharan Narang,Sergey Edunov,Maxim Naumov,Chunqiang Tang,Mathew Oldham*

Main category: cs.DC

TL;DR: FT-HSDP是一种新型的容错训练范式，通过将数据并行副本作为容错单元，在GPU故障时仅重启故障副本而其他副本继续训练，大幅减少大规模训练中的停机时间。


<details>
  <summary>Details</summary>
Motivation: 大规模同步训练（O(100K) GPU）面临频繁故障和长恢复时间的问题，导致训练效率低下（仅44%有效训练时间）。需要一种容错机制来减少故障恢复的停机时间。

Method: 1) 提出FT-HSDP范式，以数据并行副本为容错单元；2) 设计容错All Reduce协议（FTAR），CPU处理复杂控制逻辑，GPU负责数据传输；3) 引入非阻塞追赶协议，允许恢复副本以最小延迟加入训练。

Result: 在O(100K) GPU规模下，FT-HSDP将故障恢复停机时间从10分钟减少到3分钟，有效训练时间从44%提升到80%。异步恢复不会对最终模型的准确性产生显著影响。

Conclusion: FT-HSDP通过创新的容错机制显著提高了大规模训练的效率，解决了同步训练中的故障恢复瓶颈问题，为超大规模AI训练提供了可行的解决方案。

Abstract: Large-scale training systems typically use synchronous training, requiring all GPUs to be healthy simultaneously. In our experience training on O(100K) GPUs, synchronous training results in a low efficiency due to frequent failures and long recovery time.
  To address this problem, we propose a novel training paradigm, Fault Tolerant Hybrid-Shared Data Parallelism (FT-HSDP). FT-HSDP uses data parallel replicas as units of fault tolerance. When failures occur, only a single data-parallel replica containing the failed GPU or server is taken offline and restarted, while the other replicas continue training. To realize this idea at scale, FT-HSDP incorporates several techniques: 1) We introduce a Fault Tolerant All Reduce (FTAR) protocol for gradient exchange across data parallel replicas. FTAR relies on the CPU to drive the complex control logic for tasks like adding or removing participants dynamically, and relies on GPU to perform data transfer for best performance. 2) We introduce a non-blocking catch-up protocol, allowing a recovering replica to join training with minimal stall.
  Compared with fully synchronous training at O(100K) GPUs, FT-HSDP can reduce the stall time due to failure recovery from 10 minutes to 3 minutes, increasing effective training time from 44\% to 80\%. We further demonstrate that FT-HSDP's asynchronous recovery does not bring any meaning degradation to the accuracy of the result model.

</details>


### [5] [Standardized Methods and Recommendations for Green Federated Learning](https://arxiv.org/abs/2602.00343)
*Austin Tapp,Holger R. Roth,Ziyue Xu,Abhijeet Parida,Hareem Nisar,Marius George Linguraru*

Main category: cs.DC

TL;DR: 本文提出了一种实用的联邦学习碳排放核算方法，使用NVFlare和CodeCarbon进行阶段感知的CO2e跟踪，并验证了系统效率差异对碳足迹的显著影响。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能保护隐私敏感数据，但其环境影响难以在不同研究间比较，因为测量边界不一致且报告异质性高。需要标准化的碳核算方法来支持可重复的"绿色"联邦学习评估。

Method: 提出基于NVIDIA NVFlare和CodeCarbon的碳核算方法，进行明确的阶段感知任务跟踪（初始化、每轮训练、评估、空闲/协调），并额外估计模型更新传输的通信排放。

Result: 在CIFAR-10实验中，系统级减速和协调效应显著增加碳足迹（中效率增加8.34倍，低效率增加21.73倍）。在视网膜分割中，GPU层级交换（H100 vs V100）产生1.7倍运行时间差距，但不同站点的总能耗和CO2e变化不均匀。

Conclusion: 研究结果支持标准化的碳核算方法，这是可重复"绿色"联邦学习评估的前提。需要按站点和按轮次报告，以准确评估环境影响。

Abstract: Federated learning (FL) enables collaborative model training over privacy-sensitive, distributed data, but its environmental impact is difficult to compare across studies due to inconsistent measurement boundaries and heterogeneous reporting. We present a practical carbon-accounting methodology for FL CO2e tracking using NVIDIA NVFlare and CodeCarbon for explicit, phase-aware tasks (initialization, per-round training, evaluation, and idle/coordination). To capture non-compute effects, we additionally estimate communication emissions from transmitted model-update sizes under a network-configurable energy model. We validate the proposed approach on two representative workloads: CIFAR-10 image classification and retinal optic disk segmentation. In CIFAR-10, controlled client-efficiency scenarios show that system-level slowdowns and coordination effects can contribute meaningfully to carbon footprint under an otherwise fixed FL protocol, increasing total CO2e by 8.34x (medium) and 21.73x (low) relative to the high-efficiency baseline. In retinal segmentation, swapping GPU tiers (H100 vs.\ V100) yields a consistent 1.7x runtime gap (290 vs. 503 minutes) while producing non-uniform changes in total energy and CO2e across sites, underscoring the need for per-site and per-round reporting. Overall, our results support a standardized carbon accounting method that acts as a prerequisite for reproducible 'green' FL evaluation. Our code is available at https://github.com/Pediatric-Accelerated-Intelligence-Lab/carbon_footprint.

</details>


### [6] [PROBE: Co-Balancing Computation and Communication in MoE Inference via Real-Time Predictive Prefetching](https://arxiv.org/abs/2602.00509)
*Qianchao Zhu,Xucheng Ye,Yuliang Liu,Haodong Ouyang,Chengru Song*

Main category: cs.DC

TL;DR: PROBE：一种实时协同平衡计算与通信的MoE推理系统，通过连续前瞻流水线预测、规划和预取，显著降低延迟并提升吞吐量


<details>
  <summary>Details</summary>
Motivation: MoE模型在推理时面临专家并行带来的内存效率提升与执行拖尾效应之间的根本矛盾。现实服务中的连续批处理和多样化并发请求导致专家热点在GPU间快速迁移，引发计算倾斜和网络拥塞的"双重惩罚"

Method: PROBE系统包含三个核心组件：1) 门控初始化前瞻预测器，蒸馏目标路由器以高保真度预测下一层专家激活；2) 硬件感知平衡规划求解器，在严格隐藏窗口约束下联合优化动态专家复制和令牌分配；3) 相位锁定协同调度策略，使用分阶段传输将带宽密集型专家传输隐藏在计算后面，避免与All-to-All集体操作竞争

Result: 实验表明，PROBE相比最先进的基线方法，预填充延迟降低达1.32倍，解码吞吐量提升达1.26倍，在极端工作负载波动下表现尤为突出

Conclusion: PROBE通过实时协同平衡计算与通信，有效解决了MoE推理中的执行拖尾问题，显著提升了推理性能，特别是在动态工作负载场景下

Abstract: Mixture-of-Experts models have become a dominant architecture for scaling Large Language Models by activating only a sparse subset of experts per token. However, latency-critical MoE inference faces a fundamental tension: while expert parallelism improves memory efficiency, it also amplifies execution stragglers. In real-world serving, continuous batching and diverse concurrent requests induce rapid semantic shifts, causing expert hotspots to migrate abruptly across GPUs and triggering the 'double penalty' of coupled computational skew and network congestion.
  We propose PROBE, an inference system that co-balances computation and communication in real time. PROBE introduces Continuous Lookahead Pipelining, which proactively predicts, plans, and prefetches for upcoming layers while keeping all control overheads off the critical path. PROBE consists of: (1) a Gate-Initialized Lookahead Predictor that distills the target router to forecast next-layer expert activation with high fidelity; (2) a Hardware-Aware Balance Planning solver that jointly optimizes dynamic expert replication and token assignment under strict hiding-window constraints; and (3) a Phase-Locked Co-Scheduling policy that uses split-phase transmission to hide bandwidth-intensive expert transfers behind computation without contending with All-to-All collectives. Experiments show that PROBE reduces prefill latency by up to 1.32X and improves decoding throughput by up to 1.26X over state-of-the-art baselines, especially under extreme workload volatility.

</details>


### [7] [HyperOffload: Graph-Driven Hierarchical Memory Management for Large Language Models on SuperNode Architectures](https://arxiv.org/abs/2602.00748)
*Fangxin Liu,Qinghua Zhang,Hanjing Shen,Zhibo Liang,Li Jiang,Haibing Guan,Chong Bao,Xuefeng Jin*

Main category: cs.DC

TL;DR: HyperOffload是一个针对超级节点架构的编译器辅助内存管理框架，通过图驱动的内存管理和静态调度数据转移来隐藏远程内存访问延迟，显著减少设备内存使用同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型向长上下文推理和稀疏架构发展，内存需求已远超单个设备HBM容量。虽然新兴超级节点架构通过高速互连提供TB级共享内存池，但现有软件栈无法有效利用这种硬件。当前基于运行时的卸载和交换技术具有局部视角，导致反应式调度和暴露的通信延迟，从而阻塞计算流水线。

Method: 提出SuperNode内存管理框架，采用编译器辅助方法，利用图驱动内存管理将远程内存访问视为计算图中的显式操作。在编译器中间表示中使用缓存操作符表示数据移动，实现张量生命周期和执行依赖的全局编译时分析。基于此可见性，开发全局执行顺序优化算法，静态调度数据转移以在计算密集型区域后隐藏远程内存延迟。在MindSpore深度学习框架中实现，添加远程内存后端和专门编译器通道。

Result: 在代表性LLM工作负载评估中，SuperNode在推理时将峰值设备内存使用减少高达26%，同时保持端到端性能。

Conclusion: 将内存增强硬件集成到编译器优化框架对于扩展下一代AI工作负载至关重要。HyperOffload展示了编译器辅助方法在超级节点架构中有效管理内存的潜力。

Abstract: The rapid evolution of Large Language Models (LLMs) towards long-context reasoning and sparse architectures has pushed memory requirements far beyond the capacity of individual device HBM. While emerging supernode architectures offer terabyte-scale shared memory pools via high-bandwidth interconnects, existing software stacks fail to exploit this hardware effectively. Current runtime-based offloading and swapping techniques operate with a local view, leading to reactive scheduling and exposed communication latency that stall the computation pipeline.
  In this paper, we propose the SuperNode Memory Management Framework (\textbf{HyperOffload}). It employs a compiler-assisted approach that leverages graph-driven memory management to treat remote memory access as explicit operations in the computation graph, specifically designed for hierarchical SuperNode architectures. Unlike reactive runtime systems, SuperNode represents data movement using cache operators within the compiler's Intermediate Representation (IR). This design enables a global, compile-time analysis of tensor lifetimes and execution dependencies. Leveraging this visibility, we develop a global execution-order refinement algorithm that statically schedules data transfers to hide remote memory latency behind compute-intensive regions. We implement SuperNode within the production deep learning framework MindSpore, adding a remote memory backend and specialized compiler passes. Evaluation on representative LLM workloads shows that SuperNode reduces peak device memory usage by up to 26\% for inference while maintaining end-to-end performance. Our work demonstrates that integrating memory-augmented hardware into the compiler's optimization framework is essential for scaling next-generation AI workloads.

</details>


### [8] [System-Level Performance Modeling of Photonic In-Memory Computing](https://arxiv.org/abs/2602.00892)
*Jebacyril Arockiaraj,Sasindu Wijeratne,Sugeet Sunder,Md Abdullah-Al Kaiser,Akhilesh Jaiswal,Ajey P. Jacob,Viktor Prasanna*

Main category: cs.DC

TL;DR: 该论文开发了光子内存计算的系统级性能模型，分析了延迟对高性能计算工作负载的影响，展示了1×256位单波长光子SRAM阵列在多个科学计算问题上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 光子内存计算具有高速、低能耗的优势，但需要系统级性能模型来评估实际应用中的延迟影响，以推动其在高性能计算领域的应用。

Method: 开发了全面的系统级性能模型，考虑外部内存访问和光电转换等关键延迟源，并在Sod激波管问题、MTTKRP和Vlasov-Maxwell方程等多种工作负载上进行算法到硬件的映射评估。

Result: 采用GlobalFoundries标准硅光子工艺制造的紧凑型1×256位单波长光子SRAM阵列，在Sod激波管问题、MTTKRP和Vlasov-Maxwell方程上分别实现了1.5 TOPS、0.9 TOPS和1.3 TOPS的性能，平均能效达到2.5 TOPS/W。

Conclusion: 光子内存计算在考虑系统开销的情况下，仍能实现高性能和高能效，为高性能计算应用提供了有前景的替代方案。

Abstract: Photonic in-memory computing is a high-speed, low-energy alternative to traditional transistor-based digital computing that utilizes high photonic operating frequencies and bandwidths. In this work, we develop a comprehensive system-level performance model for photonic in-memory computing, capturing the effects of key latency sources such as external memory access and opto-electronic conversion. We perform algorithm-to-hardware mapping across a range of workloads, including the Sod shock tube problem, Matricized Tensor Times Khatri-Rao Product (MTTKRP), and the Vlasov-Maxwell equation, to evaluate how the latencies impact real-world high-performance computing workloads. Our performance model shows that, while accounting for system overheads, a compact 1x256 bit single-wavelength photonic SRAM array, fabricated using the standard silicon photonics process by GlobalFoundries, sustains up to 1.5 TOPS, 0.9 TOPS, and 1.3 TOPS on the Sod shock tube problem, MTTKRP, and the Vlasov-Maxwell equation with an average energy efficiency of 2.5 TOPS/W.

</details>


### [9] [Low-latency Federated LLM Fine-tuning Over Wireless Networks](https://arxiv.org/abs/2602.01024)
*Zhiwen Pang,Kang Wei,Long Shi,Zhe Wang,Jun Li,Feng Shu*

Main category: cs.DC

TL;DR: 提出JCPBA框架，通过联合优化剪枝率和带宽分配来降低联邦大语言模型在无线网络中的微调延迟


<details>
  <summary>Details</summary>
Motivation: 联邦大语言模型结合了LLM和联邦学习的优势，能解决协作微调中的隐私问题。但由于LLM参数量巨大，现有联邦LLM微调框架在资源受限的客户端（具有异构计算能力和随机无线信道）上面临显著挑战

Method: 提出联合客户端特定剪枝和带宽分配（JCPBA）框架，通过块坐标下降法联合优化剪枝率和带宽分配，以最小化微调延迟

Result: 在Yahoo Answers和GSM8K数据集上的实验表明，该框架相比最先进基线显著减少了微调时间，以更低的计算和通信开销获得了相同或更低的测试损失

Conclusion: JCPBA框架能有效提高联邦大语言模型在无线网络中的微调效率，解决了资源受限环境下的性能瓶颈问题

Abstract: Recently, federated large language models (LLMs) have drawn significant attention thanks to coupled capabilities of LLMs and federated learning (FL) that address privacy concerns in collaborative fine-tuning. However, due to large-scale parameters of LLMs, existing federated LLM fine-tuning frameworks incur significant challenges in resource-constrained clients characterized by heterogeneous computing capabilities and random wireless channels. To address this issue, we propose a joint client-specific pruning and bandwidth allocation (JCPBA) framework for federated LLMs to improve the fine-tuning efficiency over the wireless networks. Specifically, we formulate a fine-tuning latency minimization problem by jointly optimizing pruning rates and bandwidth allocations. Furthermore, we solve this optimization problem using a block coordinate descent method. Extensive experiments on the datasets of Yahoo Answers and GSM8K demonstrate that the proposed framework significantly reduces wall-clock fine-tuning time compared with state-of-the-art baselines and gains equal or lower test loss at the cost of lower computation and communication overhead.

</details>


### [10] [BOA Constrictor: Squeezing Performance out of GPUs in the Cloud via Budget-Optimal Allocation](https://arxiv.org/abs/2602.01404)
*Zhouzi Li,Cindy Zhu,Arpan Mukhopadhyay,Mor Harchol-Balter,Benjamin Berg*

Main category: cs.DC

TL;DR: BOA Constrictor：一种基于预算最优分配策略的ML训练作业调度器，在固定预算约束下最大化GPU集群性能


<details>
  <summary>Details</summary>
Motivation: 云上ML训练面临成本-性能权衡：租用更多GPU可加速作业完成但成本增加，而GPU并行化存在边际效益递减。组织需要智能调度策略来平衡这一矛盾。

Method: 提出BOA Constrictor调度器，采用预算最优分配(BOA)策略。将问题形式化为预算约束调度问题，推导出在用户预算约束下最小化平均作业完成时间的BOA策略。

Result: 在给定预算水平下，BOA Constrictor相比最先进的启发式调度器：小规模实验平均JCT降低1.6倍，大规模仿真平均JCT降低2倍。

Conclusion: BOA Constrictor能有效平衡云上ML训练的成本-性能权衡，在固定预算下显著提升GPU集群性能，为组织提供经济高效的ML训练解决方案。

Abstract: The past decade has seen a dramatic increase in demand for GPUs to train Machine Learning (ML) models. Because it is prohibitively expensive for most organizations to build and maintain a large GPU cluster, organizations instead choose to rent GPUs from cloud providers. The customer is responsible for devising a policy for (i) deciding how many GPUs to rent at every moment in time to process a stream of ML training jobs and (ii) allocating the rented GPUs among the currently active jobs in the system. Because ML training jobs can be parallelized across different numbers of GPUs, the customer generally has many options for how many GPUs to use for each job. Allocating more GPUs to a single training job will cause the job to complete more quickly. However, the customer pays for each GPU-hour they use, and a training job receives a diminishing marginal benefit from running on additional GPUs. Hence, allocating too many GPUs to a single training job can dramatically increase the overall cost that the customer pays to the cloud provider. This gives rise to a cost-performance tradeoff that customers must balance when running training jobs in the cloud.
  To balance the cost-performance tradeoff, we develop BOA Constrictor, a new scheduler for ML training jobs which uses a Budget-Optimal Allocation (BOA) policy to squeeze the highest level of performance out of a cloud-deployed GPU cluster given a fixed budget constraint. We explicitly formulate the problem as a budget-constrained scheduling problem and derive the BOA policy which minimizes the average job completion time (JCT) of a stream of arriving jobs subject to the user's budget. For a given budget level, we demonstrate that BOA Constrictor can reduce average JCT by 1.6 times in small-scale implementation experiments and by 2 times in detailed, large-scale simulations compared to state-of-the-art heuristic based schedulers.

</details>


### [11] [Mean field optimal Core Allocation across Malleable jobs](https://arxiv.org/abs/2602.01411)
*Zhouzi Li,Mor Harchol-Balter,Benjamin Berg*

Main category: cs.DC

TL;DR: 提出两种针对可塑作业的核心分配策略：FW-CAM和WHAM，在平均场渐近条件下达到最优，解决了多类作业、任意凹加速函数和一般分布下的核心分配问题。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心和云计算集群中可塑作业日益普遍，这类作业可在任意数量核心上并行化但存在边际收益递减。现有理论工作关注如何在固定核心数下分配核心以最小化作业平均响应时间，但缺乏对多类作业、不同凹加速函数和一般分布的通用解决方案。

Method: 在平均场渐近条件下分析CAM问题，推导出两种平均场最优策略：1) FW-CAM策略，基于新直觉（平均场下作业大小不影响最优策略）；2) WHAM策略，基于Whittle索引方法，不仅渐近最优，在非渐近条件下也是良好启发式算法。

Result: FW-CAM展示了平均场下作业大小不相关的反直觉结果；WHAM被证明是渐近最优的，且在非渐近条件下表现良好。现有文献中的策略在作业遵循不同加速函数时均非平均场最优。

Conclusion: 提出的FW-CAM和WHAM策略解决了通用CAM问题，WHAM尤其具有实用价值，既保证渐近最优性，又在实际场景中作为有效启发式算法，填补了现有文献在多类可塑作业核心分配优化方面的空白。

Abstract: Modern data centers and cloud computing clusters are increasingly running workloads composed of malleable jobs. A malleable job can be parallelized across any number of cores, yet the job typically exhibits diminishing marginal returns for each additional core on which it runs. This can be seen in the concavity of a job's speedup function, which describes the job's processing speed as a function of the number of cores on which it runs.
  Given the prevalence of malleable jobs, several theoretical works have posed the problem of how to allocate a fixed number of cores across a stream of arriving malleable jobs so as to minimize the mean response time across jobs. We refer to this as the Core Allocation to Malleable jobs (CAM) problem. We solve the CAM problem under a highly general setting, allowing for multiple job classes, each with an arbitrary concave speedup function and holding costs (weight). Furthermore, we allow for generally distributed inter-arrival times and job sizes.
  We analyze the CAM problem in the mean field asymptotic regime and derive two distinct mean field optimal policies, FW-CAM and WHAM. FW-CAM is interesting because it demonstrates a new intuition: in the mean field regime, job sizes are not relevant in finding an optimal policy. WHAM (Whittle Allocation for Malleable jobs) is interesting because it is asymptotically optimal and also serves as a good heuristic even outside of the asymptotic regime. Notably, none of the policies previously proposed in the literature are mean field optimal when jobs may follow different speedup functions.

</details>


### [12] [Developing a Portable Solution for Post-Event Analysis Pipelines](https://arxiv.org/abs/2602.01798)
*Leonardo Pelonero,Fabio Vitello,Eva Sciacca,Mauro Imbrosciano,Salvatore Scavo,Ugo Becciani*

Main category: cs.DC

TL;DR: 提出一个科学网关框架，用于开发便携式、全自动的灾后分析流程，整合摄影测量、数据可视化和AI技术，通过航拍图像评估极端自然灾害及其对风险资产的影响。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了洪水、干旱、风暴潮和滑坡等自然灾害，加上持续的地震风险，凸显了在意大利等易受灾地区加强风险评估和缓解策略的迫切需要。

Method: 开发一个科学网关框架，整合摄影测量技术、数据可视化和人工智能技术，应用于航拍图像，构建便携式、全自动的灾后分析流程。

Result: 提出了一个综合性的分析框架，能够自动化处理灾后评估，结合多种技术手段对自然灾害影响进行量化分析。

Conclusion: 该科学网关框架为极端自然灾害的灾后评估提供了高效、自动化的解决方案，有助于改善风险暴露资产的影响评估和缓解策略制定。

Abstract: In recent years, the monitoring and study of natural hazards have gained significant attention, particularly due to climate change, which exacerbates incidents like floods, droughts, storm surges, and landslides. Together with the constant risk of earthquakes, these climate-induced events highlight the critical necessity for enhanced risk assessment and mitigation strategies in susceptible areas such as Italy.
  In this work, we present a Science Gateway framework for the development of portable and fully automated post-event analysis pipelines integrating Photogrammetry techniques, Data Visualization and Artificial Intelligence technologies, applied on aerial images, to assess extreme natural events and evaluate their impact on risk-exposed assets.

</details>


### [13] [Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training](https://arxiv.org/abs/2602.01872)
*Chongyang Xu,Christoph Siebenbrunner,Laurent Bindschaedler*

Main category: cs.DC

TL;DR: Grappa是一个分布式GNN训练框架，通过仅交换梯度而非特征/激活来减少通信开销，使用周期性重分区和重要性采样校正来保持准确性，在真实和合成图上实现4倍加速（最高13倍）。


<details>
  <summary>Details</summary>
Motivation: 分布式GNN训练中，跨分区边的远程特征和激活获取是主要开销，随着图深度增加和分区数量增长，网络通信成为瓶颈。

Method: 1) 强制仅梯度通信：每个迭代中分区独立训练，仅交换梯度进行全局更新；2) 周期性重分区以暴露新邻域；3) 基于重要性采样的轻量级覆盖校正梯度聚合；4) 推导批处理级变体以兼容深度学习框架；5) 引入收缩版本提高稳定性。

Result: 在真实和合成图上，Grappa平均训练速度比最先进系统快4倍（最高13倍），对更深模型获得更好准确性，在商品硬件上支持万亿边规模训练。框架模型无关，支持全图和mini-batch训练，不依赖高带宽互连或缓存。

Conclusion: Grappa通过梯度专用通信、周期性重分区和重要性采样校正，有效解决了分布式GNN训练的通信瓶颈问题，在保持准确性的同时显著提升训练效率，可扩展到大规模图数据。

Abstract: Cross-partition edges dominate the cost of distributed GNN training: fetching remote features and activations per iteration overwhelms the network as graphs deepen and partition counts grow. Grappa is a distributed GNN training framework that enforces gradient-only communication: during each iteration, partitions train in isolation and exchange only gradients for the global update. To recover accuracy lost to isolation, Grappa (i) periodically repartitions to expose new neighborhoods and (ii) applies a lightweight coverage-corrected gradient aggregation inspired by importance sampling. We prove the corrected estimator is asymptotically unbiased under standard support and boundedness assumptions, and we derive a batch-level variant for compatibility with common deep-learning packages that minimizes mean-squared deviation from the ideal node-level correction. We also introduce a shrinkage version that improves stability in practice. Empirical results on real and synthetic graphs show that Grappa trains GNNs 4 times faster on average (up to 13 times) than state-of-the-art systems, achieves better accuracy especially for deeper models, and sustains training at the trillion-edge scale on commodity hardware. Grappa is model-agnostic, supports full-graph and mini-batch training, and does not rely on high-bandwidth interconnects or caching.

</details>


### [14] [vLLM-Omni: Fully Disaggregated Serving for Any-to-Any Multimodal Models](https://arxiv.org/abs/2602.02204)
*Peiqi Yin,Jiangyun Zhu,Han Gao,Chenguang Zheng,Yongxiang Huang,Taichang Zhou,Ruirui Yang,Weizhi Liu,Weiqing Chen,Canlin Guo,Didan Deng,Zifeng Mo,Cong Wang,James Cheng,Roger Wang,Hongsheng Liu*

Main category: cs.DC

TL;DR: vLLM-Omni是一个用于任意到任意多模态模型的全解耦服务系统，通过阶段抽象和图表示优化复杂架构的服务性能，相比基线方法减少高达91.4%的任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 现有的服务系统主要针对单一范式（如文本生成的自动回归LLM或视觉生成的扩散变换器），缺乏对涉及多个互连模型组件的任意到任意管道的支持，导致开发人员必须手动处理跨阶段交互，造成巨大的性能下降。

Method: 提出vLLM-Omni系统，包含新颖的阶段抽象，允许用户将复杂的任意到任意架构分解为图表示的互连阶段，以及解耦的阶段执行后端，优化跨阶段的资源利用率和吞吐量。每个阶段由LLM或扩散引擎独立服务，具有每阶段请求批处理、灵活的GPU分配和统一的阶段间连接器进行数据路由。

Result: 实验结果表明，vLLM-Omni相比基线方法将任务完成时间（JCT）减少了高达91.4%。代码已在GitHub上公开。

Conclusion: vLLM-Omni为任意到任意多模态模型提供了一个高效的服务系统，通过解耦架构和优化资源管理，显著提升了复杂多模态AI模型的服务性能。

Abstract: Any-to-any multimodal models that jointly handle text, images, video, and audio represent a significant advance in multimodal AI. However, their complex architectures (typically combining multiple autoregressive LLMs, diffusion transformers, and other specialized components) pose substantial challenges for efficient model serving. Existing serving systems are mainly tailored to a single paradigm, such as autoregressive LLMs for text generation or diffusion transformers for visual generation. They lack support for any-to-any pipelines that involve multiple interconnected model components. As a result, developers must manually handle cross-stage interactions, leading to huge performance degradation. We present vLLM-Omni, a fully disaggregated serving system for any-to-any models. vLLM-Omni features a novel stage abstraction that enables users to decompose complex any-to-any architectures into interconnected stages represented as a graph, and a disaggregated stage execution backend that optimizes resource utilization and throughput across stages. Each stage is independently served by an LLM or diffusion engine with per-stage request batching, flexible GPU allocation, and unified inter-stage connectors for data routing. Experimental results demonstrate that vLLM-Omni reduces job completion time (JCT) by up to 91.4% compared to baseline methods. The code is public available at https://github.com/vllm-project/vllm-omni.

</details>


### [15] [Enabling AI Deep Potentials for Ab Initio-quality Molecular Dynamics Simulations in GROMACS](https://arxiv.org/abs/2602.02234)
*Andong Hu,Luca Pennati,Stefano Markidis,Ivy Peng*

Main category: cs.DC

TL;DR: 将AI深度势能集成到GROMACS分子动力学软件中，通过DeePMD-kit支持多种深度势能模型，评估了DPA2和DPA3两种架构在蛋白质水溶液模拟中的性能表现。


<details>
  <summary>Details</summary>
Motivation: AI深度势能能以计算成本远低于第一性原理计算的方式提供ab initio质量的结果，但需要将其集成到生产级分子动力学软件中才能充分发挥其潜力。

Method: 通过将GROMACS与DeePMD-kit的C++/CUDA后端耦合，实现多种深度势能模型的推理；使用四种蛋白质水溶液基准（1YRF、1UBQ、3LZM、2PTC）在NVIDIA A100和GH200 GPU上评估DPA2（基于注意力机制）和DPA3（基于GNN）两种架构。

Result: DPA2在A100和GH200 GPU上分别比DPA3提供高达4.23倍和3.18倍的吞吐量；性能分析显示内核启动开销和域分解推理是主要优化方向。

Conclusion: 成功将AI深度势能集成到GROMACS中，DPA2架构在性能上优于DPA3，为生产级分子动力学模拟中的深度势能优化提供了明确方向。

Abstract: State-of-the-art AI deep potentials provide ab initio-quality results, but at a fraction of the computational cost of first-principles quantum mechanical calculations, such as density functional theory. In this work, we bring AI deep potentials into GROMACS, a production-level Molecular Dynamics (MD) code, by integrating with DeePMD-kit that provides domain-specific deep learning (DL) models of interatomic potential energy and force fields. In particular, we enable AI deep potentials inference across multiple DP model families and DL backends by coupling GROMACS Neural Network Potentials with the C++/CUDA backend in DeePMD-kit. We evaluate two recent large-atom-model architectures, DPA2 that is based on the attention mechanism and DPA3 that is based on GNN, in GROMACS using four ab initio-quality protein-in-water benchmarks (1YRF, 1UBQ, 3LZM, 2PTC) on NVIDIA A100 and GH200 GPUs. Our results show that DPA2 delivers up to 4.23x and 3.18x higher throughput than DPA3 on A100 and GH200 GPUs, respectively. We also provide a characterization study to further contrast DPA2 and DPA3 in throughput, memory usage, and kernel-level execution on GPUs. Our findings identify kernel-launch overhead and domain-decomposed inference as the main optimization priorities for AI deep potentials in production MD simulations.

</details>


### [16] [Building a Correct-by-Design Lakehouse. Data Contracts, Versioning, and Transactional Pipelines for Humans and Agents](https://arxiv.org/abs/2602.02335)
*Weiming Sheng,Jinlang Wang,Manuel Barros,Aldrin Montana,Jacopo Tagliabue,Luca Bigon*

Main category: cs.DC

TL;DR: Bauplan是一个代码优先的湖仓，通过类型化表契约、Git式数据版本控制和事务性运行来确保数据安全，防止上游-下游不匹配和部分效果泄漏。


<details>
  <summary>Details</summary>
Motivation: 当前湖仓在不受信任的参与者并发操作生产数据时存在安全问题：上游-下游不匹配只在运行时出现，多表管道可能泄漏部分效果，需要更安全的架构。

Method: 设计Bauplan代码优先湖仓，采用三个核心机制：1) 类型化表契约使管道边界可检查；2) Git式数据版本控制用于审查和可重复性；3) 事务性运行保证管道级原子性。

Result: 提出了轻量级形式化事务模型并报告了早期结果，讨论了由反例激发的未来工作方向。

Conclusion: Bauplan通过软件工程启发的设计，使大多数非法状态无法表示，为安全的数据分析平台提供了新方向。

Abstract: Lakehouses are the default cloud platform for analytics and AI, but they become unsafe when untrusted actors concurrently operate on production data: upstream-downstream mismatches surface only at runtime, and multi-table pipelines can leak partial effects. Inspired by software engineering, we design Bauplan, a code-first lakehouse that aims to make (most) illegal states unrepresentable using familiar abstractions. Bauplan acts along three axes: typed table contracts to make pipeline boundaries checkable, Git-like data versioning for review and reproducibility, and transactional runs that guarantee pipeline-level atomicity. We report early results from a lightweight formal transaction model and discuss future work motivated by counterexamples.

</details>


### [17] [LCLs Beyond Bounded Degrees](https://arxiv.org/abs/2602.02340)
*Gustav Schmid*

Main category: cs.DC

TL;DR: 在无界度树上的LCL问题中，多项式间隙的存在性取决于问题定义方式：若允许无限多局部配置，则间隙消失；若限制为局部有限标记(LFLs)，则能恢复多项式间隙。


<details>
  <summary>Details</summary>
Motivation: 研究在无界度树上，局部可检查标记(LCLs)的时间复杂度是否仍保持多项式间隙特性，探索不同问题定义方式对复杂度分布的影响。

Method: 1. 展示若允许LCLs使用无限多局部配置，则多项式间隙消失；2. 引入局部有限标记(LFLs)概念，限制节点只能属于有限多个局部情况；3. 证明LFLs能恢复多项式间隙特性。

Result: 对于无界度树上的LFLs问题，确定性LOCAL复杂度要么是Θ(n^{1/k})（k为整数），要么是O(log n)，且可通过问题描述确定具体属于哪种情况。

Conclusion: 在无界度树中，多项式间隙的存在性取决于问题定义的限制程度。通过引入LFLs这一自然限制，可以恢复有界度树中的多项式间隙特性，为分布式计算复杂度理论提供新见解。

Abstract: The study of Locally Checkable Labelings (LCLs) has led to a remarkably precise characterization of the distributed time complexities that can occur on bounded-degree trees. A central feature of this complexity landscape is the existence of strong gap results, which rule out large ranges of intermediate complexities. While it was initially hoped that these gaps might extend to more general graph classes, this has turned out not to be the case. In this work, we investigate a different direction: we remain in the class of trees, but allow arbitrarily large degrees.
  We focus on the polynomial regime ($Θ(n^{1/k} \mid k \in \mathbb{N})$) and show that whether polynomial gap results persist in the unbounded-degree setting crucially depends on how LCLs are generalized beyond bounded degrees. We first demonstrate that if one allows LCLs to be defined using infinitely many local configurations, then the polynomial gaps disappear entirely: for every real exponent $0 < r \leq 1$, there exists a locally checkable problem on trees with deterministic LOCAL complexity $Θ(n^r)$.
  Rather than stopping at this negative result, we identify a natural class of problems for which polynomial gap results can still be recovered. We introduce Locally Finite Labelings (LFLs), which formalize the intuition that ''every node must fall into one of finitely many local cases'', even in the presence of unbounded degrees.
  Our main result shows that this restriction is sufficient to restore the polynomial gaps: for any LFL $Π$ on trees with unbounded degrees, the deterministic LOCAL complexity of $Π$ is either
  - $Θ(n^{1/k})$ for some integer $k \geq 1$, or
  - $O(\log n)$.
  Moreover, which case applies, and the corresponding value of $k$, can be determined solely from the description of $Π$.

</details>


### [18] [Hierarchical Federated Learning with SignSGD: A Highly Communication-Efficient Approach](https://arxiv.org/abs/2602.02355)
*Amirreza Kazemi,Seyed Mohammad Azimi-Abarghouyi,Gabor Fodor,Carlo Fischione*

Main category: cs.DC

TL;DR: 提出HierSignSGD算法，一种用于分层联邦学习的高效通信符号梯度压缩框架，通过边缘层多数投票和云层模型聚合，在极端压缩下实现与全精度SGD相当的精度。


<details>
  <summary>Details</summary>
Motivation: 分层联邦学习在大规模无线和物联网系统中面临严格的通信限制，现有的一比特方法（如SignSGD）在扁平联邦设置中有效，但缺乏针对分层架构的理论和算法扩展，特别是边缘层多数投票与云层模型聚合的交互影响未知。

Method: 提出HierSignSGD算法：设备仅发送符号化随机梯度，边缘服务器通过多数投票聚合，云层定期平均边缘模型，同时使用下行量化广播全局模型。核心贡献是分析有偏符号压缩、两级聚合间隔和集群间异质性对收敛的影响。

Result: 在均匀和非均匀数据分割下的数值实验表明，HierSignSGD尽管采用极端压缩，仍能达到与全精度随机梯度下降相当或更好的精度，同时显著降低通信成本，并在激进的下行稀疏化下保持鲁棒性。

Conclusion: HierSignSGD为分层联邦学习提供了一种高效通信的符号梯度压缩解决方案，通过理论分析和实验验证了其在保持精度的同时大幅降低通信开销的有效性，填补了现有方法在分层架构中的空白。

Abstract: Hierarchical federated learning (HFL) has emerged as a key architecture for large-scale wireless and Internet of Things systems, where devices communicate with nearby edge servers before reaching the cloud. In these environments, uplink bandwidth and latency impose strict communication limits, thereby making aggressive gradient compression essential. One-bit methods such as sign-based stochastic gradient descent (SignSGD) offer an attractive solution in flat federated settings, but existing theory and algorithms do not naturally extend to hierarchical settings. In particular, the interaction between majority-vote aggregation at the edge layer and model aggregation at the cloud layer, and its impact on end-to-end performance, remains unknown. To bridge this gap, we propose a highly communication-efficient sign-based HFL framework and develop its corresponding formulation for nonconvex learning, where devices send only signed stochastic gradients, edge servers combine them through majority-vote, and the cloud periodically averages the obtained edge models, while utilizing downlink quantization to broadcast the global model. We introduce the resulting scalable HFL algorithm, HierSignSGD, and provide the convergence analysis for SignSGD in a hierarchical setting. Our core technical contribution is a characterization of how biased sign compression, two-level aggregation intervals, and inter-cluster heterogeneity collectively affect convergence. Numerical experiments under homogeneous and heterogeneous data splits show that HierSignSGD, despite employing extreme compression, achieves accuracy comparable to or better than full-precision stochastic gradient descent while reducing communication cost in the process, and remains robust under aggressive downlink sparsification.

</details>


### [19] [sVIRGO: A Scalable Virtual Tree Hierarchical Framework for Distributed Systems](https://arxiv.org/abs/2602.02438)
*Lican Huang*

Main category: cs.DC

TL;DR: sVIRGO是一个可扩展的虚拟树层次框架，用于大规模分布式系统，通过构建虚拟层次树直接在物理节点上实现多角色分层，支持跨数千个区域的协调，具有近零恢复延迟、有界通信开销和指数级降低的故障概率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模分布式系统中传统覆盖网络方法的可扩展性、容错性和协调效率问题，特别是在移动、干扰或对抗性条件下需要保持安全性和活跃性的场景。

Method: 1. 在物理节点上直接构建虚拟层次树，节点可承担多个层次角色；2. 将层次组织为区域内的可配置层；3. 通过动态映射到节点的虚拟上层角色实现跨区域协调；4. 每个区域维护多个活动协调器进行健康监控和动态重选；5. 支持两种消息跳转策略：基础设施辅助通道最小化跳数，或无通道时通过相邻区域传播；6. 支持层范围命令执行。

Result: 实现了近零恢复延迟、有界通信开销、指数级降低的故障概率，同时保持安全性、活跃性和鲁棒性，支持大规模分布式系统在移动、干扰或对抗性条件下的高效协调。

Conclusion: sVIRGO框架为大规模分布式系统提供了一种可扩展、容错且高效的层次协调解决方案，通过虚拟树结构、多协调器机制和层范围执行策略，显著提升了系统在复杂环境下的性能和可靠性。

Abstract: We propose sVIRGO, a scalable virtual tree hierarchical framework for large-scale distributed systems. sVIRGO constructs virtual hierarchical trees directly on physical nodes, allowing each node to assume multiple hierarchical roles without overlay networks. The hierarchy preserves locality and is organized into configurable layers within regions. Coordination across thousands of regions is achieved via virtual upper-layer roles dynamically mapped onto nodes up to the top layer.
  Each region maintains multiple active coordinators that monitor local health and perform dynamic re-selection if failures occur. Temporary drops below the minimum threshold do not compromise coordination, ensuring near-zero recovery latency, bounded communication overhead, and exponentially reduced failure probability while maintaining safety, liveness, and robustness under mobile, interference-prone, or adversarial conditions.
  Communication is decoupled from the hierarchy and may use multi-frequency wireless links. Two message hop strategies are supported: (i) with long-distance infrastructure-assisted channels, coordinators exploit the virtual tree to minimize hops; (ii) without such channels, messages propagate via adjacent regions.
  sVIRGO also supports Layer-Scoped Command Execution. Commands and coordination actions are executed within the scope of each hierarchical layer, enabling efficient local and regional decision-making while limiting unnecessary global propagation.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [20] [Accelerating Physics-Based Electromigration Analysis via Rational Krylov Subspaces](https://arxiv.org/abs/2602.00330)
*Sheldon X. -D. Tan,Haotian Lu*

Main category: cs.AR

TL;DR: 该论文提出了两种基于有理Krylov子空间缩减的快速电迁移应力分析方法，相比传统有限差分法实现了数量级的效率提升和精度改善。


<details>
  <summary>Details</summary>
Motivation: 电迁移引起的应力演化是纳米级VLSI互连的主要可靠性挑战。传统有限差分法求解应力控制偏微分方程计算成本高，需要更高效的EM应力分析方法。

Method: 提出了两种有理Krylov子空间缩减方法：频域扩展有理Krylov方法(ExtRaKrylovEM)和时域有理Krylov指数积分方法(EiRaKrylovEM)。通过坐标下降优化策略自动确定最优缩减阶数和偏移时间。

Result: 实验结果表明，仅需4-6个Krylov阶数即可实现亚0.1%的成核时间和电阻变化预测误差，同时获得20-500倍的加速。而标准扩展Krylov方法需要50多个阶数且仍有10-20%的成核时间误差。

Conclusion: 提出的有理Krylov方法在效率和精度上显著优于传统方法，为EM感知优化和随机EM分析提供了实用解决方案，特别适用于大规模互连树和工业级电源网格分析。

Abstract: Electromigration (EM) induced stress evolution is a major reliability challenge in nanometer-scale VLSI interconnects. Accurate EM analysis requires solving stress-governing partial differential equations over large interconnect trees, which is computationally expensive using conventional finite-difference methods. This work proposes two fast EM stress analysis techniques based on rational Krylov subspace reduction. Unlike traditional Krylov methods that expand around zero frequency, rational Krylov methods enable expansion at selected time constants, aligning directly with metrics such as nucleation and steady-state times and producing compact reduced models with minimal accuracy loss. Two complementary frameworks are developed: a frequency-domain extended rational Krylov method, ExtRaKrylovEM, and a time-domain rational Krylov exponential integration method, EiRaKrylovEM. We show that the accuracy of both methods depends strongly on the choice of expansion point, or shift time, and demonstrate that effective shift times are typically close to times of interest such as nucleation or post-void steady state. Based on this observation, a coordinate descent optimization strategy is introduced to automatically determine optimal reduction orders and shift times for both nucleation and post-void phases. Experimental results on synthesized structures and industry-scale power grids show that the proposed methods achieve orders-of-magnitude improvements in efficiency and accuracy over finite-difference solutions. Using only 4 to 6 Krylov orders, the methods achieve sub-0.1 percent error in nucleation time and resistance change predictions while delivering 20 to 500 times speedup. In contrast, standard extended Krylov methods require more than 50 orders and still incur 10 to 20 percent nucleation time error, limiting their practicality for EM-aware optimization and stochastic EM analysis.

</details>


### [21] [Optimal Engagement of Residential Battery Storage to Alleviate Grid Upgrades Caused by EVs and Solar Systems](https://arxiv.org/abs/2602.00342)
*Rafi Zahedi,Amirhossein Ahmadian,Chen Zhang,Shashank Narayana Gowda,Kourosh SedghiSigarchi,Rajit Gadh*

Main category: cs.AR

TL;DR: 提出一种基于电池储能太阳能系统的允许百分比分配方法，通过优化充放电策略来缓解分布式能源接入带来的电压波动和功率损耗问题，特别针对电动汽车充电需求增长


<details>
  <summary>Details</summary>
Motivation: 分布式能源资源（特别是太阳能系统和电动汽车充电器）的集成给配电网带来了电压波动和功率损耗等电能质量问题，随着电动卡车等新型电动汽车的预期增长，这些问题将更加严峻

Method: 采用允许百分比分配策略，在具有储能能力的住宅太阳能系统中部署电池储能系统，利用粒子群优化算法确定最优充放电指令，在IEEE 33节点配电网测试平台上进行仿真验证

Result: 通过仿真验证了不同允许百分比分配方案的有效性，揭示了电池储能与非电池储能太阳能住宅之间的平衡关系，证明了所提方法能够有效缓解电压偏差和功率损耗

Conclusion: 随着电动汽车的预期增长，需要精心设计的方法来应对现代电网动态的复杂性，允许百分比分配策略为电网韧性和满足未来电动汽车需求提供了有效解决方案

Abstract: The integration of distributed energy resources has ushered in a host of complex challenges, significantly impacting power quality in distribution networks. This work studies these challenges, exploring issues such as voltage fluctuations and escalating power losses caused by the integration of solar systems and electric vehicle (EV) chargers. We present a robust methodology focused on mitigating voltage deviations and power losses, emphasizing the allocation of a Permitted Percentage (PP) of battery-based solar systems within residential areas endowed with storage capabilities. A key facet of this research lies in its adaptability to the changing landscape of electric transportation. With the rapid increase of electric trucks on the horizon, our proposed model gains relevance. By tactically deploying PP to oversee the charging and discharging of batteries within residential solar systems, utilities are poised not only to assist with grid resilience but also to cater to the upcoming demands spurred by the advent of new EVs, notably trucks. To validate the efficacy of our proposed model, rigorous simulations were conducted using the IEEE 33-bus distribution network as a designed testbed. Leveraging advanced Particle Swarm Optimization techniques, we have deciphered the optimal charging and discharging commands issued by utilities to energy storage systems. The outcomes of these simulations help us understand the transformative potential of various PP allocations, shedding light on the balance between non-battery-based and battery-based solar residences. This research underscores the need for carefully crafted approaches in navigating the complexities of modern grid dynamics amid the anticipated increase in electric vehicles.

</details>


### [22] [AutoGNN: End-to-End Hardware-Driven Graph Preprocessing for Enhanced GNN Performance](https://arxiv.org/abs/2602.00803)
*Seungkwan Kang,Seungjun Lee,Donghyun Gouk,Miryeong Kwon,Hyunkyu Choi,Junhyeok Jang,Sangwon Lee,Huiwon Choi,Jie Zhang,Wonil Choi,Mahmut Taylan Kandemir,Myoungsoo Jung*

Main category: cs.AR

TL;DR: AutoGNN是一个基于FPGA的GNN推理加速器，专注于解决预处理瓶颈，通过可重构架构和专用组件实现高效图转换和采样，相比传统和GPU加速系统分别获得9.0倍和2.1倍的加速。


<details>
  <summary>Details</summary>
Motivation: GNN推理中的预处理阶段存在显著瓶颈，往往主导整体推理延迟。现有GPU解决方案在处理图数据时面临串行化和同步限制，需要更高效的硬件加速方案。

Method: 采用FPGA可重构架构，设计统一处理单元(UPEs)和单周期归约器(SCRs)。UPEs用于边缘排序和顶点选择的并行处理，SCRs处理指针数组构建和子图重索引等顺序任务。通过用户级软件框架动态分析图输入特征，确定最优配置并重编程FPGA。

Result: 在7nm企业级FPGA上实现，相比传统预处理系统获得9.0倍加速，相比GPU加速系统获得2.1倍加速。能够高效处理多样化的图数据集，显著降低GNN推理延迟。

Conclusion: AutoGNN通过FPGA可重构性和专用硬件组件有效解决了GNN预处理瓶颈，为高性能GNN推理提供了高效的硬件加速方案，特别适合处理多样化的图输入数据。

Abstract: Graph neural network (GNN) inference faces significant bottlenecks in preprocessing, which often dominate overall inference latency. We introduce AutoGNN, an FPGA-based accelerator designed to address these challenges by leveraging FPGA's reconfigurability and specialized components. AutoGNN adapts to diverse graph inputs, efficiently performing computationally intensive tasks such as graph conversion and sampling. By utilizing components like adder trees, AutoGNN executes reduction operations in constant time, overcoming the limitations of serialization and synchronization on GPUs.
  AutoGNN integrates unified processing elements (UPEs) and single-cycle reducers (SCRs) to streamline GNN preprocessing. UPEs enable scalable parallel processing for edge sorting and unique vertex selection, while SCRs efficiently handle sequential tasks such as pointer array construction and subgraph reindexing. A user-level software framework dynamically profiles graph inputs, determines optimal configurations, and reprograms AutoGNN to handle varying workloads. Implemented on a 7$n$m enterprise FPGA, AutoGNN achieves up to 9.0$\times$ and 2.1$\times$ speedup compared to conventional and GPU-accelerated preprocessing systems, respectively, enabling high-performance GNN preprocessing across diverse datasets.

</details>


### [23] [Exploration of Unary Arithmetic-Based Matrix Multiply Units for Low Precision DL Accelerators](https://arxiv.org/abs/2602.00838)
*Prabhu Vellaisamy,Harideep Nair,Di Wu,Shawn Blanton,John Paul Shen*

Main category: cs.AR

TL;DR: 该论文对三种最新的单精度GEMM设计（uGEMM、tuGEMM、tubGEMM）与传统二进制GEMM进行了详细评估，展示了单精度GEMM在未来边缘AI加速器中实现高效能计算的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习向低精度发展，需要评估新型单精度GEMM设计作为传统二进制GEMM硬件替代方案的潜力，为未来DL计算提供参考。

Method: 对三种最新单精度GEMM设计（uGEMM、tuGEMM、tubGEMM）与传统二进制GEMM进行详细评估，包括不同位宽和矩阵尺寸的合成后分析，以及在8个预训练CNN和LLaMA2 LLM上的权重稀疏性分析。

Result: 通过严格的评估确定了各种设计的权衡点和最优工作点，展示了单精度GEMM在能量效率方面的优势，特别是在边缘AI加速器中的应用潜力。

Conclusion: 单精度GEMM可以有效用于未来边缘AI加速器的高效能计算，为低精度深度学习硬件设计提供了重要参考。

Abstract: General matrix multiplication (GEMM) is a fundamental operation in deep learning (DL). With DL moving increasingly toward low precision, recent works have proposed novel unary GEMM designs as an alternative to conventional binary GEMM hardware. A rigorous evaluation of recent unary and binary GEMM designs is needed to assess the potential of unary hardware for future DL compute. This paper focuses on unary GEMM designs for integer-based DL inference and performs a detailed evaluation of three latest unary design proposals, namely, uGEMM, tuGEMM and tubGEMM, by comparing them to a conventional binary GEMM. Rigorous post-synthesis evaluations beyond prior works are performed across varying bit-widths and matrix sizes to assess the designs' tradeoffs and determine optimal sweetspots. Further, we perform weight sparsity analysis across eight pretrained convolutional neural networks (CNNs) and the LLaMA2 large language model (LLM). In this work, we demonstrate how unary GEMM can be effectively used for energy-efficient compute in future edge AI accelerators.

</details>


### [24] [ENFOR-SA: End-to-end Cross-layer Transient Fault Injector for Efficient and Accurate DNN Reliability Assessment on Systolic Arrays](https://arxiv.org/abs/2602.00909)
*Rafael Billig Tonetto,Marcello Traiola,Fernando Fernandes dos Santos,Angeliki Kritikakou*

Main category: cs.AR

TL;DR: ENFOR-SA：针对脉动阵列架构的端到端DNN瞬态故障分析框架，通过跨层仿真实现RTL级精度，相比软件注入仅慢6%，比全SoC RTL仿真快569倍


<details>
  <summary>Details</summary>
Motivation: 深度学习模型变得越来越大且复杂，传统故障注入技术不实用。RTL级硬件模型虽然准确但速度慢，特别是结合昂贵的HDL仪器时。对于脉动阵列架构，这种高开销方法是不必要的。

Method: 提出ENFOR-SA框架，采用两步法：跨层仿真，仅在故障注入时使用RTL脉动阵列组件，其余部分在软件层面执行。结合RTL级精度和软件级速度。

Result: 在CNN和Vision Transformer上的实验表明，ENFOR-SA实现RTL级精度故障注入，相比软件注入仅慢6%，比全SoC RTL仿真快569倍（平均），比最先进的跨层RTL注入工具快2.03倍。

Conclusion: ENFOR-SA为脉动阵列架构提供了高效准确的DNN故障分析解决方案，证明了高开销方法对于SA架构是不必要的，代码已开源。

Abstract: Recent advances in deep learning have produced highly accurate but increasingly large and complex DNNs, making traditional fault-injection techniques impractical. Accurate fault analysis requires RTL-accurate hardware models. However, this significantly slows evaluation compared with software-only approaches, particularly when combined with expensive HDL instrumentation. In this work, we show that such high-overhead methods are unnecessary for systolic array (SA) architectures and propose ENFOR-SA, an end-to-end framework for DNN transient fault analysis on SAs. Our two-step approach employs cross-layer simulation and uses RTL SA components only during fault injection, with the rest executed at the software level. Experiments on CNNs and Vision Transformers demonstrate that ENFOR-SA achieves RTL-accurate fault injection with only 6% average slowdown compared to software-based injection, while delivering at least two orders of magnitude speedup (average $569\times$) over full-SoC RTL simulation and a $2.03\times$ improvement over a state-of-the-art cross-layer RTL injection tool. ENFOR-SA code is publicly available at https://github.com/rafaabt/ENFOR-SA.

</details>


### [25] [NeuroAI Temporal Neural Networks (NeuTNNs): Microarchitecture and Design Framework for Specialized Neuromorphic Processing Units](https://arxiv.org/abs/2602.01546)
*Shanmuga Venkatachalam,Prabhu Vellaisamy,Harideep Nair,Wei-Che Huang,Youngseok Na,Yuyang Kang,Quinn Jacobson,John Paul Shen*

Main category: cs.AR

TL;DR: 提出NeuroAI TNNs (NeuTNNs) - 一种结合神经科学发现的新型时序神经网络，以及PyTorch-to-layout工具套件NeuTNNGen，用于设计特定应用的能效NeuTNNs。


<details>
  <summary>Details</summary>
Motivation: 神经科学与人工智能研究需要重新连接以加速下一代AI创新（NeuroAI）。现有时序神经网络（TNNs）作为神经形态方法有潜力，但需要结合神经科学发现来提升能力和硬件效率。

Method: 提出NeuroAI TNNs (NeuTNNs)，采用神经科学发现：包含主动树突的神经元模型、远端和近端段层次结构。开发NeuTNNGen工具套件（PyTorch-to-layout），用于设计特定应用的NeuTNNs。使用突触修剪进一步减少突触数量和硬件成本。

Result: 相比先前TNN设计，NeuTNNs实现更优性能和效率。通过三个应用展示：1) UCR时间序列基准测试，2) MNIST设计探索，3) 用于新皮层参考系的Place Cells设计。突触修剪减少30-50%突触数量，同时保持模型精度。

Conclusion: NeuTNNGen工具套件能够促进设计特定应用的能效NeuTNNs，为下一代NeuroAI计算系统提供支持，推动神经科学与AI的融合。

Abstract: Leading experts from both communities have suggested the need to (re)connect research in neuroscience and artificial intelligence (AI) to accelerate the development of next-generation AI innovations. They term this convergence as NeuroAI. Previous research has established temporal neural networks (TNNs) as a promising neuromorphic approach toward biological intelligence and efficiency. We fully embrace NeuroAI and propose a new category of TNNs we call NeuroAI TNNs (NeuTNNs) with greater capability and hardware efficiency by adopting neuroscience findings, including a neuron model with active dendrites and a hierarchy of distal and proximal segments. This work introduces a PyTorch-to-layout tool suite (NeuTNNGen) to design application-specific NeuTNNs. Compared to previous TNN designs, NeuTNNs achieve superior performance and efficiency. We demonstrate NeuTNNGen's capabilities using three example applications: 1) UCR time series benchmarks, 2) MNIST design exploration, and 3) Place Cells design for neocortical reference frames. We also explore using synaptic pruning to further reduce synapse counts and hardware costs by 30-50% while maintaining model precision across diverse sensory modalities. NeuTNNGen can facilitate the design of application-specific energy-efficient NeuTNNs for the next generation of NeuroAI computing systems.

</details>


### [26] [In-Pipeline Integration of Digital In-Memory-Computing into RISC-V Vector Architecture to Accelerate Deep Learning](https://arxiv.org/abs/2602.01827)
*Tommaso Spagnolo,Cristina Silvano,Riccardo Massa,Filippo Grillotti,Thomas Boesch,Giuseppe Desoli*

Main category: cs.AR

TL;DR: 提出了一种将数字内存计算单元紧密集成到向量RISC-V ISA中的新型架构，通过自定义指令加速边缘深度学习推理，实现了217倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 边缘计算需要高性能、高效率的深度学习架构，但面临严格的功耗和内存限制。数字内存计算通过将计算移至内存阵列来减少数据移动并提高能效。

Method: 扩展向量RISC-V ISA，将紧密耦合的数字内存计算单元集成到流水线执行阶段，添加四个自定义指令用于数据加载、计算和写回，实现对推理执行的灵活优化控制。

Result: 在ResNet-50模型上实现了137 GOP/s的峰值性能，相比基线核心获得217倍加速，即使在硬件资源受限情况下也实现了50倍面积归一化加速。

Conclusion: 该架构展示了作为可扩展、高效解决方案加速边缘深度学习推理的巨大潜力，数字内存计算单元在向量RISC-V中实现了高利用率。

Abstract: Expanding Deep Learning applications toward edge computing demands architectures capable of delivering high computational performance and efficiency while adhering to tight power and memory constraints. Digital In-Memory Computing (DIMC) addresses this need by moving part of the computation directly within memory arrays, significantly reducing data movement and improving energy efficiency. This paper introduces a novel architecture that extends the Vector RISC-V Instruction Set Architecture (ISA) to integrate a tightly coupled DIMC unit directly into the execution stage of the pipeline, to accelerate Deep Learning inference at the edge. Specifically, the proposed approach adds four custom instructions dedicated to data loading, computation, and write-back, enabling flexible and optimal control of the inference execution on the target architecture. Experimental results demonstrate high utilization of the DIMC tile in Vector RISC-V and sustained throughput across the ResNet-50 model, achieving a peak performance of 137 GOP/s. The proposed architecture achieves a speedup of 217x over the baseline core and 50x area-normalized speedup even when operating near the hardware resource limits. The experimental results confirm the high potential of the proposed architecture as a scalable and efficient solution to accelerate Deep Learning inference on the edge.

</details>


### [27] [Position: The Need for Ultrafast Training](https://arxiv.org/abs/2602.02005)
*Duc Hoang*

Main category: cs.AR

TL;DR: 论文主张从仅推理的FPGA加速器转向超快速片上学习，使推理和训练都在FPGA上以亚微秒延迟执行，实现与物理过程同步的实时自适应系统。


<details>
  <summary>Details</summary>
Motivation: 当前领域专用FPGA主要专注于静态模型的低延迟推理，将学习和适应任务交给较慢的CPU/GPU处理。这种分离限制了在非平稳、高频环境中运行的系统，这些系统需要在物理过程的时间尺度上进行模型更新。

Method: 提出重新思考算法、架构和工具流程的联合设计方法，将学习功能直接集成到FPGA实时数据路径中，实现推理和训练在FPGA结构内的确定性亚微秒延迟执行。

Result: 论文提出了一个概念框架，将FPGA从静态推理引擎转变为实时学习机器，能够支持量子纠错、超导量子比特校准、等离子体和聚变控制、加速器调谐以及自主科学实验等应用。

Conclusion: 通过将学习功能引入与推理相同的实时数据路径，可以构建与所控制物理过程同步适应的闭环系统，这需要算法、架构和工具流程的协同重新设计，但有望彻底改变FPGA在实时自适应系统中的应用。

Abstract: Domain-specialized FPGAs have delivered unprecedented performance for low-latency inference across scientific and industrial workloads, yet nearly all existing accelerators assume static models trained offline, relegating learning and adaptation to slower CPUs or GPUs. This separation fundamentally limits systems that must operate in non-stationary, high-frequency environments, where model updates must occur at the timescale of the underlying physics. In this paper, I argue for a shift from inference-only accelerators to ultrafast on-chip learning, in which both inference and training execute directly within the FPGA fabric under deterministic, sub-microsecond latency constraints. Bringing learning into the same real-time datapath as inference would enable closed-loop systems that adapt as fast as the physical processes they control, with applications spanning quantum error correction, cryogenic qubit calibration, plasma and fusion control, accelerator tuning, and autonomous scientific experiments. Enabling such regimes requires rethinking algorithms, architectures, and toolflows jointly, but promises to transform FPGAs from static inference engines into real-time learning machines.

</details>


### [28] [Ultrafast On-chip Online Learning via Spline Locality in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02056)
*Duc Hoang,Aarush Gupta,Philip Harris*

Main category: cs.AR

TL;DR: KANs在FPGA上实现亚微秒级在线学习，比MLPs更高效且对定点量化鲁棒


<details>
  <summary>Details</summary>
Motivation: 量子计算、核聚变等高频系统需要亚微秒级的超快速在线学习，传统MLPs在低延迟、固定精度计算和严格内存约束下效率低下且数值不稳定

Method: 利用KANs的B样条局部性实现稀疏更新，在FPGA上实现定点在线训练，利用KANs对定点量化的固有鲁棒性

Result: KAN-based在线学习器在低延迟和资源受限任务中比MLPs更高效和表达能力强，首次实现亚微秒级模型无关在线学习

Conclusion: KANs特别适合超快速在线学习场景，其稀疏更新特性和定点量化鲁棒性使其在FPGA等片上计算平台上具有显著优势

Abstract: Ultrafast online learning is essential for high-frequency systems, such as controls for quantum computing and nuclear fusion, where adaptation must occur on sub-microsecond timescales. Meeting these requirements demands low-latency, fixed-precision computation under strict memory constraints, a regime in which conventional Multi-Layer Perceptrons (MLPs) are both inefficient and numerically unstable. We identify key properties of Kolmogorov-Arnold Networks (KANs) that align with these constraints. Specifically, we show that: (i) KAN updates exploiting B-spline locality are sparse, enabling superior on-chip resource scaling, and (ii) KANs are inherently robust to fixed-point quantization. By implementing fixed-point online training on Field-Programmable Gate Arrays (FPGAs), a representative platform for on-chip computation, we demonstrate that KAN-based online learners are significantly more efficient and expressive than MLPs across a range of low-latency and resource-constrained tasks. To our knowledge, this work is the first to demonstrate model-free online learning at sub-microsecond latencies.

</details>


### [29] [CHAOS: Controlled Hardware fAult injectOr System for gem5](https://arxiv.org/abs/2602.02119)
*Elio Vinciguerra,Enrico Russo,Giuseppe Ascia,Maurizio Palesi*

Main category: cs.AR

TL;DR: CHAOS是一个模块化、开源、可配置的故障注入框架，专为gem5模拟器设计，用于评估计算系统的可靠性和弹性。


<details>
  <summary>Details</summary>
Motivation: 故障注入器对于评估计算系统的可靠性和弹性至关重要，能够模拟硬件和软件故障来分析系统在错误条件下的行为，并评估其在干扰下正确运行的能力。这种分析对于识别漏洞和提高系统鲁棒性非常关键。

Method: CHAOS是一个模块化、开源、完全可配置的故障注入框架，专门为gem5模拟器设计。它支持跨多个架构级别的精确和系统化故障注入，促进对容错机制和弹性策略的全面评估。

Result: CHAOS的高可配置性和与gem5的无缝集成使研究人员能够探索广泛的故障模型和复杂场景，使其成为推进可靠和高性能计算系统研究的宝贵工具。

Conclusion: CHAOS作为一个模块化、开源、可配置的故障注入框架，为gem5模拟器提供了强大的故障注入能力，有助于推进可靠和高性能计算系统的研究。

Abstract: Fault injectors are essential tools for evaluating the reliability and resilience of computing systems. They enable the simulation of hardware and software faults to analyze system behavior under error conditions and assess its ability to operate correctly despite disruptions. Such analysis is critical for identifying vulnerabilities and improving system robustness. CHAOS is a modular, open-source, and fully configurable fault injection framework designed for the gem5 simulator. It facilitates precise and systematic fault injection across multiple architectural levels, supporting comprehensive evaluations of fault tolerance mechanisms and resilience strategies. Its high configurability and seamless integration with gem5 allow researchers to explore a wide range of fault models and complex scenarios, making CHAOS a valuable tool for advancing research in dependable and high-performance computing systems.

</details>
