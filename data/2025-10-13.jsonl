{"id": "2510.08726", "categories": ["cs.PL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08726", "abs": "https://arxiv.org/abs/2510.08726", "authors": ["Yifan Zhao", "Egan Johnson", "Prasanth Chatarasi", "Vikram Adve", "Sasa Misailovic"], "title": "Neptune: Advanced ML Operator Fusion for Locality and Parallelism on GPUs", "comment": null, "summary": "Operator fusion has become a key optimization for deep learning, which\ncombines multiple deep learning operators to improve data reuse and reduce\nglobal memory transfers. However, existing tensor compilers struggle to fuse\ncomplex reduction computations involving loop-carried dependencies, such as\nattention mechanisms.\n  The paper introduces Neptune, a tensor compiler for advanced operator fusion\nfor sequences of reduction operators. Neptune presents a new approach for\nadvanced operator fusion, which intentionally breaks some existing dependencies\nand compensates by constructing algebraic correction expressions that allow the\nkernel to produce the correct result.\n  On ten attention-based benchmarks, Neptune, starting from simple attention\ncode and a high-level scheduling template, outperforms existing compilers like\nTriton, TVM, and FlexAttention, including Triton-based implementations of\nFlashAttention. Across four different GPU architectures from NVIDIA and AMD,\nNeptune-generated kernels have average speedup of $1.35\\times$ over the next\nbest alternative, demonstrating its effectiveness for deep learning workloads."}
{"id": "2510.08889", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.08889", "abs": "https://arxiv.org/abs/2510.08889", "authors": ["Songlin Jia", "Craig Liu", "Siyuan He", "Haotian Deng", "Yuyan Bao", "Tiark Rompf"], "title": "Typestate via Revocable Capabilities", "comment": null, "summary": "Managing stateful resources safely and expressively is a longstanding\nchallenge in programming languages, especially in the presence of aliasing.\nWhile scope-based constructs such as Java's synchronized blocks offer ease of\nreasoning, they restrict expressiveness and parallelism. Conversely,\nimperative, flow-sensitive management enables fine-grained control but demands\nsophisticated typestate analyses and often burdens programmers with explicit\nstate tracking.\n  In this work, we present a novel approach that unifies the strengths of both\nparadigms by extending flow-insensitive capability mechanisms into\nflow-sensitive typestate tracking. Our system decouples capability lifetimes\nfrom lexical scopes, allowing functions to provide, revoke, and return\ncapabilities in a flow-sensitive manner, based on the existing mechanisms\nexplored for the safety and ergonomics of scoped capability programming.\n  We implement our approach as an extension to the Scala 3 compiler, leveraging\npath-dependent types and implicit resolution to enable concise, statically\nsafe, and expressive typestate programming. Our prototype generically supports\na wide range of stateful patterns, including file operations, advanced locking\nprotocols, DOM construction, and session types. This work demonstrates that\nexpressive and safe typestate management can be achieved with minimal\nextensions to existing capability-based languages, paving the way for more\nrobust and ergonomic stateful programming."}
{"id": "2510.08939", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.08939", "abs": "https://arxiv.org/abs/2510.08939", "authors": ["Haotian Deng", "Siyuan He", "Songlin Jia", "Yuyan Bao", "Tiark Rompf"], "title": "Free to Move: Reachability Types with Flow-Sensitive Effects for Safe Deallocation and Ownership Transfer", "comment": null, "summary": "We present a flow-sensitive effect system for reachability types that\nsupports explicit memory management, including Rust-style move semantics, in\nhigher-order impure functional languages. Our system refines the existing\nreachability qualifier with polymorphic \\emph{use} and \\emph{kill} effects that\nrecord how references are read, written, transferred, and deallocated. The\neffect discipline tracks operations performed on each resource using\nqualifiers, enabling the type system to express ownership transfer, contextual\nfreshness, and destructive updates without regions or linearity. We formalize\nthe calculus, its typing and effect rules, and a compositional operational\nsemantics that validates use-after-free safety. All metatheoretic results,\nincluding preservation, progress, and effect soundness, are mechanized. The\nsystem models idioms such as reference deallocation, move semantics, reference\nswapping, while exposing precise safety guarantee. Together, these\ncontributions integrate reachability-based reasoning with explicit resource\ncontrol, advancing the state of the art in safe manual memory management for\nhigher-order functional languages."}
{"id": "2510.08969", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08969", "abs": "https://arxiv.org/abs/2510.08969", "authors": ["Bjarne Stroustrup"], "title": "Concept-Based Generic Programming in C++", "comment": null, "summary": "We present programming techniques to illustrate the facilities and principles\nof C++ generic programming using concepts. Concepts are C++'s way to express\nconstraints on generic code. As an initial example, we provide a simple type\nsystem that eliminates narrowing conversions and provides range checking\nwithout unnecessary notational or run-time overheads. Concepts are used\nthroughout to provide user-defined extensions to the type system. The aim is to\nshow their utility and the fundamental ideas behind them, rather than to\nprovide a detailed or complete explanation of C++'s language support for\ngeneric programming or the extensive support provided by the standard library.\nGeneric programming is an integral part of C++, rather than an isolated\nsub-language. In particular, key facilities support general programming as well\nas generic programming (e.g., uniform notation for types, lambdas, variadic\ntemplates, and C++26 static reflection). Finally, we give design rationales and\norigins for key parts of the concept design, including use patterns, the\nrelationship to Object-Oriented Programming, value arguments, notation, concept\ntype-matching, and definition checking."}
{"id": "2510.08873", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.08873", "abs": "https://arxiv.org/abs/2510.08873", "authors": ["Haoran Jin", "Jirong Yang", "Yunpeng Liu", "Barry Lyu", "Kangqi Zhang", "Nathaniel Bleier"], "title": "Mozart: A Chiplet Ecosystem-Accelerator Codesign Framework for Composable Bespoke Application Specific Integrated Circuits", "comment": null, "summary": "Modern AI acceleration faces a fundamental challenge: conventional\nassumptions about memory requirements, batching effectiveness, and\nlatency-throughput tradeoffs are systemwide generalizations that ignore the\nheterogeneous computational patterns of individual neural network operators.\nHowever, going towards network-level customization and operator-level\nheterogeneity incur substantial Non-Recurring Engineering (NRE) costs. While\nchiplet-based approaches have been proposed to amortize NRE costs, reuse\nopportunities remain limited without carefully identifying which chiplets are\ntruly necessary. This paper introduces Mozart, a chiplet ecosystem and\naccelerator codesign framework that systematically constructs low cost bespoke\napplication-specific integrated circuits (BASICs). BASICs leverage\noperator-level disaggregation to explore chiplet and memory heterogeneity,\ntensor fusion, and tensor parallelism, with place-and-route validation ensuring\nphysical implementability. The framework also enables constraint-aware\nsystem-level optimization across deployment contexts ranging from datacenter\ninference serving to edge computing in autonomous vehicles. The evaluation\nconfirms that with just 8 strategically selected chiplets, Mozart-generated\ncomposite BASICs achieve 43.5%, 25.4%, 67.7%, and 78.8% reductions in energy,\nenergy-cost product, energy-delay product (EDP), and energy-delay-cost product\ncompared to traditional homogeneous accelerators. For datacenter LLM serving,\nMozart achieves 15-19% energy reduction and 35-39% energy-cost improvement. In\nspeculative decoding, Mozart delivers throughput improvements of 24.6-58.6%\nwhile reducing energy consumption by 38.6-45.6%. For autonomous vehicle\nperception, Mozart reduces energy-cost by 25.54% and energy by 10.53% under\nreal-time constraints."}
{"id": "2510.08842", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.08842", "abs": "https://arxiv.org/abs/2510.08842", "authors": ["Molang Wu", "Zhao Zhang"], "title": "Maple: A Multi-agent System for Portable Deep Learning across Clusters", "comment": null, "summary": "Training deep learning (DL) models across Graphics Processing Unit (GPU)\nclusters is technically challenging. One aspect is that users have to compose\ncommand lines to adapt to the heterogeneous launchers, schedulers, affinity\noptions, DL framework arguments, and environment variables. Composing correct\ncommand lines is error-prone and can easily frustrate users, impeding research\nor wasting resources. In this work, we present Maple, a multi-agent system that\ngenerates correct DL command lines with users' natural language input. Maple\nconsists of four agents with the functionalities of information extraction,\ntemplate retrieval, command line verification, and error correction. We\nevaluate Maple on nine GPU clusters across national computing centers in the\nU.S., five representative deep learning model families, and four commonly used\nparallel DL training paradigms. Our experiments also cover schedulers of SLURM\nand PBS and heterogeneous architectures, such as NVIDIA A100/H200 GPUs and\nIntel Max series GPUs. Maple achieves 92.0% accuracy in generating command\nlines across the 567 test cases. Leverage multiple language models with an\naggregated size of 10B parameters, Maple delivers comparable performance to the\nstate-of-the-art models of GPT-5, Claude, and Gemini. Together, these results\nhighlight Maple's practical value in enabling portable and scalable distributed\nDL across heterogeneous HPC environments."}
{"id": "2510.09591", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.09591", "abs": "https://arxiv.org/abs/2510.09591", "authors": ["Saad Ahmed Bazaz", "Mirza Omer Beg"], "title": "A Multilingual Python Programming Language", "comment": "For project homepage, see https://universalpython.github.io/", "summary": "All widely used and useful programming languages have a common problem. They\nrestrict entry on the basis of knowledge of the English language. The lack of\nknowledge of English poses a major hurdle to many newcomers who do not have the\nresources, in terms of time and money, to learn the English language. Studies\nshow that people learn better in their own language. Therefore, we propose a\nlanguage transpiler built on top of the Python programming language, called\nUniversalPython, which allows one to write Python in their own human language.\nWe demonstrate the ability to create an \"Urdu Python\" with this transpiler. In\nthe future, we aim to scale the language to encapsulate more human languages to\nincrease the availability of programming. The source code for this transpiler\nis open-source, and available at\nhttps://github.com/universalpython/universalpython"}
{"id": "2510.08940", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.08940", "abs": "https://arxiv.org/abs/2510.08940", "authors": ["Abel Beyene", "Zhongpan Wu", "Yunus Dawji", "Karim Hammad", "Ebrahim Ghafar-Zadeh", "Sebastian Magierowski"], "title": "A High-Efficiency SoC for Next-Generation Mobile DNA Sequencing", "comment": null, "summary": "Hand-sized Deoxyribonucleic acid (DNA) sequencing machines are of growing\nimportance in several life sciences fields as their small footprints enable a\nbroader range of use cases than their larger, stationary counterparts. However,\nas currently designed, they lack sufficient embedded computing to process the\nlarge volume of measurements generated by their internal sensory system. As a\nconsequence, they rely on external devices for additional processing\ncapability. This dependence on external processing places a significant\ncommunication burden on the sequencer's embedded electronics. Moreover, it also\nprevents a truly mobile solution for sequencing in real-time. Anticipating\nnext-generation machines that include suitably advanced processing, we present\na System-on-Chip (SoC) fabricated in 22-nm complementary metal-oxide\nsemiconductor (CMOS). Our design, based on a general-purpose reduced\ninstruction set computing (RISC-V) core, also includes accelerators for DNA\ndetection that allow our system to demonstrate a 13X performance improvement\nover commercial embedded multicore processors combined with a near 3000X boost\nin energy efficiency."}
{"id": "2510.08874", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08874", "abs": "https://arxiv.org/abs/2510.08874", "authors": ["Benjamin Brock", "Renato Golin"], "title": "Slicing Is All You Need: Towards A Universal One-Sided Algorithm for Distributed Matrix Multiplication", "comment": null, "summary": "Many important applications across science, data analytics, and AI workloads\ndepend on distributed matrix multiplication. Prior work has developed a large\narray of algorithms suitable for different problem sizes and partitionings\nincluding 1D, 2D, 1.5D, and 2.5D algorithms. A limitation of current work is\nthat existing algorithms are limited to a subset of partitionings. Multiple\nalgorithm implementations are required to support the full space of possible\npartitionings. If no algorithm implementation is available for a particular set\nof partitionings, one or more operands must be redistributed, increasing\ncommunication costs. This paper presents a universal one-sided algorithm for\ndistributed matrix multiplication that supports all combinations of\npartitionings and replication factors. Our algorithm uses slicing (index\narithmetic) to compute the sets of overlapping tiles that must be multiplied\ntogether. This list of local matrix multiplies can then either be executed\ndirectly, or reordered and lowered to an optimized IR to maximize overlap. We\nimplement our algorithm using a high-level C++-based PGAS programming framework\nthat performs direct GPU-to-GPU communication using intra-node interconnects.\nWe evaluate performance for a wide variety of partitionings and replication\nfactors, finding that our work is competitive with PyTorch DTensor, a highly\noptimized distributed tensor library targeting AI models."}
{"id": "2510.09010", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.09010", "abs": "https://arxiv.org/abs/2510.09010", "authors": ["Yipu Zhang", "Chaofang Ma", "Jinming Ge", "Lin Jiang", "Jiang Xu", "Wei Zhang"], "title": "HERO: Hardware-Efficient RL-based Optimization Framework for NeRF Quantization", "comment": "Accepted by ASPDAC 2026", "summary": "Neural Radiance Field (NeRF) has emerged as a promising 3D reconstruction\nmethod, delivering high-quality results for AR/VR applications. While\nquantization methods and hardware accelerators have been proposed to enhance\nNeRF's computational efficiency, existing approaches face crucial limitations.\nCurrent quantization methods operate without considering hardware architecture,\nresulting in sub-optimal solutions within the vast design space encompassing\naccuracy, latency, and model size. Additionally, existing NeRF accelerators\nheavily rely on human experts to explore this design space, making the\noptimization process time-consuming, inefficient, and unlikely to discover\noptimal solutions. To address these challenges, we introduce HERO, a\nreinforcement learning framework performing hardware-aware quantization for\nNeRF. Our framework integrates a NeRF accelerator simulator to generate\nreal-time hardware feedback, enabling fully automated adaptation to hardware\nconstraints. Experimental results demonstrate that HERO achieves 1.31-1.33\n$\\times$ better latency, 1.29-1.33 $\\times$ improved cost efficiency, and a\nmore compact model size compared to CAQ, a previous state-of-the-art NeRF\nquantization framework. These results validate our framework's capability to\neffectively navigate the complex design space between hardware and algorithm\nrequirements, discovering superior quantization policies for NeRF\nimplementation. Code is available at https://github.com/ypzhng/HERO."}
{"id": "2510.09163", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.09163", "abs": "https://arxiv.org/abs/2510.09163", "authors": ["Alessandro Ottaviano", "Andrino Meli", "Paul Scheffler", "Giovanni Bambini", "Robert Balas", "Davide Rossi", "Andrea Bartolini", "Luca Benini"], "title": "Co-designing a Programmable RISC-V Accelerator for MPC-based Energy and Thermal Management of Many-Core HPC Processors", "comment": "18 pages, 16 figures, 1 table", "summary": "Managing energy and thermal profiles is critical for many-core HPC processors\nwith hundreds of application-class processing elements (PEs). Advanced model\npredictive control (MPC) delivers state-of-the-art performance but requires\nsolving an online optimization problem over a thousand times per second (1 kHz\ncontrol bandwidth), with computational and memory demands scaling with PE\ncount. Traditional MPC approaches execute the controller on the PEs, but\noperating system overheads create jitter and limit control bandwidth. Running\nMPC on dedicated on-chip controllers enables fast, deterministic control but\nraises concerns about area and power overhead. In this work, we tackle these\nchallenges by proposing a hardware-software codesign of a lightweight MPC\ncontroller, based on an operator-splitting quadratic programming solver and an\nembedded multi-core RISC-V controller. Key innovations include pruning weak\nthermal couplings to reduce model memory and ahead-of-time scheduling for\nefficient parallel execution of sparse triangular systems arising from the\noptimization problem. The proposed controller achieves sub-millisecond latency\nwhen controlling 144 PEs at 500 MHz, delivering 33x lower latency and 7.9x\nhigher energy efficiency than a single-core baseline. Operating within a\ncompact less than 1 MiB memory footprint, it consumes as little as 325 mW while\noccupying less than 1.5% of a typical HPC processor's die area."}
{"id": "2510.09339", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.09339", "abs": "https://arxiv.org/abs/2510.09339", "authors": ["Sebastian Magierowski", "Zhongpan Wu", "Abel Beyene", "Karim Hammad"], "title": "Sequencing on Silicon: AI SoC Design for Mobile Genomics at the Edge", "comment": null, "summary": "Miniature DNA sequencing hardware has begun to succeed in mobile contexts,\ndriving demand for efficient machine learning at the edge. This domain\nleverages deep learning techniques familiar from speech and time-series\nanalysis for both low-level signal processing and high-level genomic\ninterpretation. Unlike audio, however, nanopore sequencing presents raw data\nrates over 100X higher, requiring more aggressive compute and memory handling.\nIn this paper, we present a CMOS system-on-chip (SoC) designed for mobile\ngenetic analysis. Our approach combines a multi-core RISC-V processor with\ntightly coupled accelerators for deep learning and bioinformatics. A\nhardware/software co-design strategy enables energy-efficient operation across\na heterogeneous compute fabric, targeting real-time, on-device genome analysis.\nThis work exemplifies the integration of deep learning, edge computing, and\ndomain-specific hardware to advance next-generation mobile genomics."}
