{"id": "2512.04527", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04527", "abs": "https://arxiv.org/abs/2512.04527", "authors": ["Xingyu Liu", "Jiawei Liang", "Linfeng Du", "Yipu Zhang", "Chaofang Ma", "Hanwei Fan", "Jiang Xu", "Wei Zhang"], "title": "FLEX: Leveraging FPGA-CPU Synergy for Mixed-Cell-Height Legalization Acceleration", "comment": null, "summary": "In this work, we present FLEX, an FPGA-CPU accelerator for mixed-cell-height legalization tasks. We address challenges from the following perspectives. First, we optimize the task assignment strategy and perform an efficient task partition between FPGA and CPU to exploit their complementary strengths. Second, a multi-granularity pipelining technique is employed to accelerate the most time-consuming step, finding optimal placement position (FOP), in legalization. At last, we particularly target the computationally intensive cell shifting process in FOP, optimizing the design to align it seamlessly with the multi-granularity pipelining framework for further speedup. Experimental results show that FLEX achieves up to 18.3x and 5.4x speedups compared to state-of-the-art CPU-GPU and multi-threaded CPU legalizers with better scalability, while improving legalization quality by 4% and 1%.", "AI": {"tldr": "FLEX\uff1a\u4e00\u79cd\u7528\u4e8e\u6df7\u5408\u5355\u5143\u9ad8\u5ea6\u5408\u6cd5\u5316\u4efb\u52a1\u7684FPGA-CPU\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u4f18\u5316\u4efb\u52a1\u5206\u914d\u7b56\u7565\u3001\u591a\u7c92\u5ea6\u6d41\u6c34\u7ebf\u6280\u672f\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u5355\u5143\u79fb\u4f4d\u4f18\u5316\uff0c\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u548c\u66f4\u597d\u7684\u5408\u6cd5\u5316\u8d28\u91cf\u3002", "motivation": "\u6df7\u5408\u5355\u5143\u9ad8\u5ea6\u5408\u6cd5\u5316\u5728\u7269\u7406\u8bbe\u8ba1\u4e2d\u8ba1\u7b97\u5bc6\u96c6\u4e14\u8017\u65f6\uff0c\u73b0\u6709CPU-GPU\u548c\u591a\u7ebf\u7a0bCPU\u89e3\u51b3\u65b9\u6848\u5728\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002\u9700\u8981\u5229\u7528FPGA\u548cCPU\u7684\u4e92\u8865\u4f18\u52bf\u6765\u52a0\u901f\u8fd9\u4e00\u5173\u952e\u8bbe\u8ba1\u6b65\u9aa4\u3002", "method": "1. \u4f18\u5316\u4efb\u52a1\u5206\u914d\u7b56\u7565\uff0c\u5728FPGA\u548cCPU\u4e4b\u95f4\u8fdb\u884c\u9ad8\u6548\u4efb\u52a1\u5212\u5206\uff1b2. \u91c7\u7528\u591a\u7c92\u5ea6\u6d41\u6c34\u7ebf\u6280\u672f\u52a0\u901f\u6700\u8017\u65f6\u7684\u6b65\u9aa4\u2014\u2014\u5bfb\u627e\u6700\u4f18\u653e\u7f6e\u4f4d\u7f6e(FOP)\uff1b3. \u9488\u5bf9FOP\u4e2d\u8ba1\u7b97\u5bc6\u96c6\u7684\u5355\u5143\u79fb\u4f4d\u8fc7\u7a0b\u8fdb\u884c\u4e13\u95e8\u4f18\u5316\uff0c\u4f7f\u5176\u4e0e\u591a\u7c92\u5ea6\u6d41\u6c34\u7ebf\u6846\u67b6\u65e0\u7f1d\u5bf9\u9f50\u3002", "result": "FLEX\u76f8\u6bd4\u6700\u5148\u8fdb\u7684CPU-GPU\u548c\u591a\u7ebf\u7a0bCPU\u5408\u6cd5\u5316\u5668\u5206\u522b\u5b9e\u73b0\u4e8618.3\u500d\u548c5.4\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002\u5728\u5408\u6cd5\u5316\u8d28\u91cf\u65b9\u9762\u5206\u522b\u63d0\u5347\u4e864%\u548c1%\u3002", "conclusion": "FLEX\u901a\u8fc7\u521b\u65b0\u7684FPGA-CPU\u534f\u540c\u52a0\u901f\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df7\u5408\u5355\u5143\u9ad8\u5ea6\u5408\u6cd5\u5316\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u7269\u7406\u8bbe\u8ba1\u4e2d\u7684\u8fd9\u4e00\u5173\u952e\u6b65\u9aa4\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.04867", "categories": ["cs.AR", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.04867", "abs": "https://arxiv.org/abs/2512.04867", "authors": ["Bychkov Oleksii", "Senysh Taras"], "title": "Functional Stability of Software-Hardware Neural Network Implementation The NeuroComp Project", "comment": "14 pages", "summary": "This paper presents an innovative approach to ensuring functional stability of neural networks through hardware redundancy at the individual neuron level. Unlike the classical Dropout method, which is used during training for regularization purposes, the proposed system ensures resilience to hardware failures during network operation. Each neuron is implemented on a separate microcomputer (ESP32), allowing the system to continue functioning even when individual computational nodes fail.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u786c\u4ef6\u5197\u4f59\u786e\u4fdd\u795e\u7ecf\u7f51\u7edc\u529f\u80fd\u7a33\u5b9a\u6027\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u72ec\u7acb\u5fae\u8ba1\u7b97\u673a\u5b9e\u73b0\u6bcf\u4e2a\u795e\u7ecf\u5143\uff0c\u786e\u4fdd\u5728\u786c\u4ef6\u6545\u969c\u65f6\u7f51\u7edc\u4ecd\u80fd\u8fd0\u884c", "motivation": "\u4f20\u7edfDropout\u65b9\u6cd5\u4e3b\u8981\u7528\u4e8e\u8bad\u7ec3\u9636\u6bb5\u7684\u6b63\u5219\u5316\uff0c\u65e0\u6cd5\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5728\u8fd0\u884c\u65f6\u7684\u786c\u4ef6\u6545\u969c\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u786c\u4ef6\u5c42\u9762\u786e\u4fdd\u795e\u7ecf\u7f51\u7edc\u529f\u80fd\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5f0f\u786c\u4ef6\u7cfb\u7edf\u4e2d\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u5143\u7ea7\u786c\u4ef6\u5197\u4f59\u67b6\u6784\uff0c\u6bcf\u4e2a\u795e\u7ecf\u5143\u90fd\u5728\u72ec\u7acb\u7684\u5fae\u8ba1\u7b97\u673a\uff08ESP32\uff09\u4e0a\u5b9e\u73b0\uff0c\u5f62\u6210\u5206\u5e03\u5f0f\u8ba1\u7b97\u7cfb\u7edf\u3002\u5f53\u67d0\u4e2a\u8ba1\u7b97\u8282\u70b9\u6545\u969c\u65f6\uff0c\u7cfb\u7edf\u80fd\u591f\u7ee7\u7eed\u8fd0\u884c\uff0c\u786e\u4fdd\u795e\u7ecf\u7f51\u7edc\u7684\u529f\u80fd\u7a33\u5b9a\u6027\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5728\u5355\u4e2a\u8ba1\u7b97\u8282\u70b9\u6545\u969c\u65f6\u4fdd\u6301\u6b63\u5e38\u8fd0\u884c\uff0c\u5b9e\u73b0\u4e86\u786c\u4ef6\u5c42\u9762\u7684\u5bb9\u9519\u80fd\u529b\u3002\u76f8\u6bd4\u4f20\u7edfDropout\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e13\u95e8\u9488\u5bf9\u8fd0\u884c\u65f6\u7684\u786c\u4ef6\u6545\u969c\u63d0\u4f9b\u4fdd\u62a4\u3002", "conclusion": "\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u786c\u4ef6\u5197\u4f59\u7684\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u80fd\u591f\u6709\u6548\u786e\u4fdd\u795e\u7ecf\u7f51\u7edc\u5728\u786c\u4ef6\u6545\u969c\u65f6\u7684\u529f\u80fd\u7a33\u5b9a\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.04910", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04910", "abs": "https://arxiv.org/abs/2512.04910", "authors": ["Fang Li"], "title": "Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming", "comment": "Accepted by the 43rd IEEE International Conference on Computer Design (ICCD 2025)", "summary": "This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.", "AI": {"tldr": "\u4f7f\u7528ASP\u5b9e\u73b0\u81ea\u52a8\u6761\u677f\u7535\u8def\u5e03\u5c40\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6c42\u89e3\u65b9\u6cd5\u751f\u6210\u7d27\u51d1\u3001\u53ef\u5236\u9020\u7684\u5e03\u5c40", "motivation": "\u89e3\u51b3\u6761\u677f\u7535\u8def\u5e03\u5c40\u81ea\u52a8\u5316\u95ee\u9898\uff0c\u4e3a\u7535\u5b50\u539f\u578b\u8bbe\u8ba1\u548c\u6559\u80b2\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\uff0c\u540c\u65f6\u5c55\u793a\u58f0\u660e\u5f0f\u7f16\u7a0b\u5728\u590d\u6742\u8bbe\u8ba1\u81ea\u52a8\u5316\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u6f5c\u529b", "method": "\u4f7f\u7528\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u5c06\u5e03\u5c40\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u7efc\u5408\u548c\u591a\u76ee\u6807\u4f18\u5316\u4efb\u52a1\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6c42\u89e3\u65b9\u6cd5\uff1a\u5148\u786e\u4fdd\u53ef\u884c\u6027\uff0c\u518d\u4f18\u5316\u5e03\u5c40\u8d28\u91cf", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u4e3a\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u7535\u8def\u751f\u6210\u7d27\u51d1\u3001\u53ef\u5236\u9020\u7684\u5e03\u5c40", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u6761\u677f\u7535\u8def\u5e03\u5c40\u81ea\u52a8\u5316\u7684\u91cd\u5927\u8fdb\u5c55\uff0c\u5c55\u793a\u4e86\u58f0\u660e\u5f0f\u7f16\u7a0b\u5728\u89e3\u51b3\u590d\u6742\u8bbe\u8ba1\u81ea\u52a8\u5316\u95ee\u9898\u4e2d\u7684\u5f3a\u5927\u80fd\u529b"}}
{"id": "2512.04088", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04088", "abs": "https://arxiv.org/abs/2512.04088", "authors": ["Kolichala Rajashekar", "Nafiseh Sharghivand", "Radu Prodan", "Reza Farahani"], "title": "Toward Sustainability-Aware LLM Inference on Edge Clusters", "comment": "4 pages, 5 figures, 3 tables, conference paper", "summary": "Large language models (LLMs) require substantial computational resources, leading to significant carbon emissions and operational costs. Although training is energy-intensive, the long-term environmental burden arises from inference, amplified by the massive global query volume. Cloud-based inference offers scalability but suffers from latency and bandwidth constraints due to centralized processing and continuous data transfer. Edge clusters instead can mitigate these limitations by enabling localized execution, yet they face trade-offs between performance, energy efficiency, and device constraints. This short paper presents a sustainability-aware LLM inference for edge clusters comprising NVIDIA Jetson Orin NX (8GB) and Nvidia Ada 2000 (16GB) devices. It aims to balance inference latency and carbon footprint through carbon- and latency-aware routing strategies, guided by empirical benchmarking of energy consumption and execution time across diverse prompts and batch (i.e., group of prompts) configurations. We compared baseline greedy strategies to carbon-aware and latency-aware strategies in prompt routing to specific hardware based on benchmarking information. Experimental evaluation shows that a batch size of four prompts achieves a trade-off between throughput, energy efficiency, while larger batches risk GPU memory saturation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8fb9\u7f18\u96c6\u7fa4\u7684\u53ef\u6301\u7eed\u6027\u611f\u77e5LLM\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u78b3\u611f\u77e5\u548c\u5ef6\u8fdf\u611f\u77e5\u7684\u8def\u7531\u7b56\u7565\uff0c\u5728NVIDIA Jetson Orin NX\u548cAda 2000\u8bbe\u5907\u4e0a\u5e73\u8861\u63a8\u7406\u5ef6\u8fdf\u4e0e\u78b3\u8db3\u8ff9\u3002", "motivation": "LLM\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u5bfc\u81f4\u663e\u8457\u7684\u78b3\u6392\u653e\u548c\u8fd0\u8425\u6210\u672c\u3002\u867d\u7136\u8bad\u7ec3\u80fd\u8017\u9ad8\uff0c\u4f46\u957f\u671f\u73af\u5883\u8d1f\u62c5\u6765\u81ea\u63a8\u7406\u8fc7\u7a0b\uff0c\u4e14\u56e0\u5168\u7403\u67e5\u8be2\u91cf\u5de8\u5927\u800c\u52a0\u5267\u3002\u4e91\u7aef\u63a8\u7406\u5b58\u5728\u5ef6\u8fdf\u548c\u5e26\u5bbd\u9650\u5236\uff0c\u8fb9\u7f18\u96c6\u7fa4\u53ef\u5b9e\u73b0\u672c\u5730\u5316\u6267\u884c\uff0c\u4f46\u9762\u4e34\u6027\u80fd\u3001\u80fd\u6548\u548c\u8bbe\u5907\u7ea6\u675f\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u63d0\u51fa\u53ef\u6301\u7eed\u6027\u611f\u77e5\u7684LLM\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u5305\u542bNVIDIA Jetson Orin NX (8GB)\u548cNvidia Ada 2000 (16GB)\u8bbe\u5907\u7684\u8fb9\u7f18\u96c6\u7fa4\u3002\u901a\u8fc7\u78b3\u611f\u77e5\u548c\u5ef6\u8fdf\u611f\u77e5\u7684\u8def\u7531\u7b56\u7565\uff0c\u57fa\u4e8e\u5bf9\u4e0d\u540c\u63d0\u793a\u548c\u6279\u91cf\u914d\u7f6e\u7684\u80fd\u8017\u548c\u6267\u884c\u65f6\u95f4\u7684\u7ecf\u9a8c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e73\u8861\u63a8\u7406\u5ef6\u8fdf\u548c\u78b3\u8db3\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u6279\u91cf\u5927\u5c0f\u4e3a4\u4e2a\u63d0\u793a\u65f6\u80fd\u5728\u541e\u5410\u91cf\u548c\u80fd\u6548\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u800c\u66f4\u5927\u7684\u6279\u91cf\u53ef\u80fd\u5bfc\u81f4GPU\u5185\u5b58\u9971\u548c\u3002\u6bd4\u8f83\u4e86\u57fa\u7ebf\u8d2a\u5a6a\u7b56\u7565\u4e0e\u78b3\u611f\u77e5\u548c\u5ef6\u8fdf\u611f\u77e5\u7b56\u7565\u5728\u63d0\u793a\u8def\u7531\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u78b3\u611f\u77e5\u548c\u5ef6\u8fdf\u611f\u77e5\u7684\u8def\u7531\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u8fb9\u7f18\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u53ef\u6301\u7eed\u7684LLM\u63a8\u7406\uff0c\u5e73\u8861\u5ef6\u8fdf\u548c\u78b3\u8db3\u8ff9\uff0c\u6279\u91cf\u5927\u5c0f4\u662f\u6700\u4f73\u914d\u7f6e\uff0c\u4e3a\u8fb9\u7f18\u73af\u5883\u4e2d\u7684\u9ad8\u6548LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.04755", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.04755", "abs": "https://arxiv.org/abs/2512.04755", "authors": ["Stian Lybech", "Daniele Gorla", "Luca Aceto"], "title": "Typing Fallback Functions: A Semantic Approach to Type Safe Smart Contracts", "comment": null, "summary": "This paper develops semantic typing in a smart-contract setting to ensure type safety of code that uses statically untypable language constructs, such as the fallback function. The idea is that the creator of a contract on the blockchain equips code containing such constructs with a formal proof of its type safety, given in terms of the semantics of types. Then, a user of the contract only needs to check the validity of the provided `proof certificate' of type safety. This is a form of proof-carrying code, which naturally fits with the immutable nature of the blockchain environment.\n  As a concrete application of our approach, we focus on ensuring information flow control and non-interference for the language TINYSOL, a distilled version of the Solidity language, through security types. We provide the semantics of types in terms of a typed operational semantics of TINYSOL, and a way for expressing the proofs of safety as coinductively-defined typing interpretations and for representing them compactly via up-to techniques, similar to those used for bisimilarity. We also show how our machinery can be used to type the typical pointer-to-implementation pattern based on the fallback function. However, our main contribution is not the safety theorem per se (and so security properties different from non-interference can be considered as well), but rather the presentation of the theoretical developments necessary to make this approach work in a blockchain/smart-contract setting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u667a\u80fd\u5408\u7ea6\u73af\u5883\u4e2d\u4f7f\u7528\u8bed\u4e49\u7c7b\u578b\u6765\u786e\u4fdd\u7c7b\u578b\u5b89\u5168\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u50cffallback\u51fd\u6570\u8fd9\u6837\u7684\u9759\u6001\u4e0d\u53ef\u7c7b\u578b\u5316\u8bed\u8a00\u6784\u9020\u3002\u901a\u8fc7\u8ba9\u5408\u7ea6\u521b\u5efa\u8005\u63d0\u4f9b\u7c7b\u578b\u5b89\u5168\u7684\u5f62\u5f0f\u5316\u8bc1\u660e\u8bc1\u4e66\uff0c\u7528\u6237\u53ea\u9700\u9a8c\u8bc1\u8bc1\u4e66\u6709\u6548\u6027\uff0c\u8fd9\u7c7b\u4f3c\u4e8e\u533a\u5757\u94fe\u73af\u5883\u4e2d\u7684\u8bc1\u660e\u643a\u5e26\u4ee3\u7801\u3002", "motivation": "\u667a\u80fd\u5408\u7ea6\u4e2d\u67d0\u4e9b\u8bed\u8a00\u6784\u9020\uff08\u5982fallback\u51fd\u6570\uff09\u65e0\u6cd5\u8fdb\u884c\u9759\u6001\u7c7b\u578b\u68c0\u67e5\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7c7b\u578b\u5b89\u5168\u95ee\u9898\u3002\u533a\u5757\u94fe\u7684\u4e0d\u53ef\u53d8\u6027\u4f7f\u5f97\u4e00\u65e6\u90e8\u7f72\u9519\u8bef\u5408\u7ea6\u96be\u4ee5\u4fee\u590d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u786e\u4fdd\u8fd9\u7c7b\u4ee3\u7801\u7684\u7c7b\u578b\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u8bed\u4e49\u7c7b\u578b\u65b9\u6cd5\uff0c\u4e3aTINYSOL\u8bed\u8a00\uff08Solidity\u7684\u7b80\u5316\u7248\u672c\uff09\u63d0\u4f9b\u7c7b\u578b\u64cd\u4f5c\u8bed\u4e49\u3002\u901a\u8fc7\u5171\u5f52\u7eb3\u5b9a\u4e49\u7684\u7c7b\u578b\u89e3\u91ca\u6765\u8868\u8fbe\u5b89\u5168\u8bc1\u660e\uff0c\u5e76\u4f7f\u7528\u7c7b\u4f3c\u4e8e\u53cc\u76f8\u4f3c\u6027\u7684up-to\u6280\u672f\u6765\u7d27\u51d1\u8868\u793a\u8bc1\u660e\u8bc1\u4e66\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u533a\u5757\u94fe/\u667a\u80fd\u5408\u7ea6\u73af\u5883\u7684\u8bed\u4e49\u7c7b\u578b\u6846\u67b6\uff0c\u80fd\u591f\u786e\u4fdd\u4fe1\u606f\u6d41\u63a7\u5236\u548c\u975e\u5e72\u6270\u6027\u5b89\u5168\u5c5e\u6027\u3002\u8be5\u65b9\u6cd5\u8fd8\u53ef\u7528\u4e8e\u7c7b\u578b\u5316\u57fa\u4e8efallback\u51fd\u6570\u7684\u5178\u578b\u6307\u9488\u5230\u5b9e\u73b0\u6a21\u5f0f\u3002", "conclusion": "\u8bba\u6587\u7684\u4e3b\u8981\u8d21\u732e\u4e0d\u662f\u5b89\u5168\u5b9a\u7406\u672c\u8eab\uff0c\u800c\u662f\u5c55\u793a\u4e86\u5728\u533a\u5757\u94fe/\u667a\u80fd\u5408\u7ea6\u73af\u5883\u4e2d\u5b9e\u73b0\u8fd9\u79cd\u65b9\u6cd5\u6240\u9700\u7684\u7406\u8bba\u53d1\u5c55\uff0c\u4e3a\u5904\u7406\u9759\u6001\u4e0d\u53ef\u7c7b\u578b\u5316\u6784\u9020\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.04089", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04089", "abs": "https://arxiv.org/abs/2512.04089", "authors": ["Mario Colosi", "Reza Farahani", "Lauri Loven", "Radu Prodan", "Massimo Villari"], "title": "Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud", "comment": "7 pages, 8 Figures, 2 Tables, conference paper", "summary": "WebAssembly (Wasm) is a binary instruction format that enables portable, sandboxed, and near-native execution across heterogeneous platforms, making it well-suited for serverless workflow execution on browsers, edge nodes, and cloud servers. However, its performance and stability depend heavily on factors such as startup overhead, runtime execution model (e.g., Ahead-of-Time (AOT) and Just-in-Time (JIT) compilation), and resource variability across deployment contexts. This paper evaluates a Wasm-based serverless workflow executed consistently from the browser to edge and cloud instances. The setup uses wasm32-wasi modules: in the browser, execution occurs within a web worker, while on Edge and Cloud, an HTTP shim streams frames to the Wasm runtime. We measure cold- and warm-start latency, per-step delays, workflow makespan, throughput, and CPU/memory utilization to capture the end-to-end behavior across environments. Results show that AOT compilation and instance warming substantially reduce startup latency. For workflows with small payloads, the browser achieves competitive performance owing to fully in-memory data exchanges. In contrast, as payloads grow, the workflow transitions into a compute- and memory-intensive phase where AOT execution on edge and cloud nodes distinctly surpasses browser performance.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86WebAssembly\u5728\u6d4f\u89c8\u5668\u3001\u8fb9\u7f18\u8282\u70b9\u548c\u4e91\u670d\u52a1\u5668\u4e0a\u6267\u884c\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u6d41\u7684\u6027\u80fd\uff0c\u53d1\u73b0AOT\u7f16\u8bd1\u548c\u5b9e\u4f8b\u9884\u70ed\u80fd\u663e\u8457\u964d\u4f4e\u542f\u52a8\u5ef6\u8fdf\uff0c\u5c0f\u8d1f\u8f7d\u65f6\u6d4f\u89c8\u5668\u6027\u80fd\u6709\u7ade\u4e89\u529b\uff0c\u5927\u8d1f\u8f7d\u65f6\u8fb9\u7f18\u548c\u4e91\u8282\u70b9\u7684AOT\u6267\u884c\u6027\u80fd\u66f4\u4f18\u3002", "motivation": "WebAssembly\uff08Wasm\uff09\u4f5c\u4e3a\u4e00\u79cd\u4e8c\u8fdb\u5236\u6307\u4ee4\u683c\u5f0f\uff0c\u5177\u6709\u8de8\u5e73\u53f0\u3001\u6c99\u7bb1\u5316\u548c\u63a5\u8fd1\u539f\u751f\u6267\u884c\u7684\u7279\u6027\uff0c\u9002\u5408\u5728\u6d4f\u89c8\u5668\u3001\u8fb9\u7f18\u8282\u70b9\u548c\u4e91\u670d\u52a1\u5668\u4e0a\u6267\u884c\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u6d41\u3002\u4f46\u5176\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u53d7\u542f\u52a8\u5f00\u9500\u3001\u8fd0\u884c\u65f6\u6267\u884c\u6a21\u578b\uff08AOT/JIT\u7f16\u8bd1\uff09\u4ee5\u53ca\u4e0d\u540c\u90e8\u7f72\u73af\u5883\u8d44\u6e90\u5dee\u5f02\u7b49\u56e0\u7d20\u5f71\u54cd\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528wasm32-wasi\u6a21\u5757\u6784\u5efaWasm\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u6d41\uff0c\u5728\u6d4f\u89c8\u5668\u4e2d\u901a\u8fc7web worker\u6267\u884c\uff0c\u5728\u8fb9\u7f18\u548c\u4e91\u73af\u5883\u4e2d\u901a\u8fc7HTTP shim\u5c06\u5e27\u6d41\u5f0f\u4f20\u8f93\u5230Wasm\u8fd0\u884c\u65f6\u3002\u6d4b\u91cf\u51b7\u542f\u52a8/\u70ed\u542f\u52a8\u5ef6\u8fdf\u3001\u6bcf\u6b65\u5ef6\u8fdf\u3001\u5de5\u4f5c\u6d41\u603b\u65f6\u95f4\u3001\u541e\u5410\u91cf\u4ee5\u53caCPU/\u5185\u5b58\u5229\u7528\u7387\uff0c\u4ee5\u6355\u83b7\u8de8\u73af\u5883\u7684\u7aef\u5230\u7aef\u884c\u4e3a\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a1\uff09AOT\u7f16\u8bd1\u548c\u5b9e\u4f8b\u9884\u70ed\u80fd\u663e\u8457\u964d\u4f4e\u542f\u52a8\u5ef6\u8fdf\uff1b2\uff09\u5bf9\u4e8e\u5c0f\u8d1f\u8f7d\u5de5\u4f5c\u6d41\uff0c\u6d4f\u89c8\u5668\u7531\u4e8e\u5b8c\u5168\u5185\u5b58\u6570\u636e\u4ea4\u6362\u800c\u5177\u6709\u7ade\u4e89\u529b\u6027\u80fd\uff1b3\uff09\u968f\u7740\u8d1f\u8f7d\u589e\u5927\uff0c\u5de5\u4f5c\u6d41\u8fdb\u5165\u8ba1\u7b97\u548c\u5185\u5b58\u5bc6\u96c6\u578b\u9636\u6bb5\uff0c\u8fb9\u7f18\u548c\u4e91\u8282\u70b9\u7684AOT\u6267\u884c\u6027\u80fd\u660e\u663e\u8d85\u8d8a\u6d4f\u89c8\u5668\u3002", "conclusion": "Wasm\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u6d41\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002AOT\u7f16\u8bd1\u548c\u9884\u70ed\u7b56\u7565\u5bf9\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73af\u5883\u9009\u62e9\u5e94\u6839\u636e\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\uff1a\u5c0f\u8d1f\u8f7d\u9002\u5408\u6d4f\u89c8\u5668\u6267\u884c\uff0c\u5927\u8d1f\u8f7d\u5219\u66f4\u9002\u5408\u8fb9\u7f18\u548c\u4e91\u8282\u70b9\u7684AOT\u6267\u884c\u3002"}}
{"id": "2512.04876", "categories": ["cs.PL", "cs.DS"], "pdf": "https://arxiv.org/pdf/2512.04876", "abs": "https://arxiv.org/abs/2512.04876", "authors": ["Ioannis Karras"], "title": "Optimizations and extensions for fair join pattern matching", "comment": "This is a Master's thesis for the Master's in Computer Science and Engineering at DTU (Technical University of Denmark)", "summary": "Join patterns are an underexplored approach for the programming of concurrent and distributed systems. When applied to the actor model, join patterns offer the novel capability of matching combinations of messages in the mailbox of an actor. Previous work by Philipp Haller et al. in the paper \"Fair Join Pattern Matching for Actors\" (ECOOP 2024) explored join patterns with conditional guards in an actor-based setting with a specification of fair and deterministic matching semantics. Nevertheless, the question of time efficiency in fair join pattern matching has remained underexplored. The stateful tree-based matching algorithm of Haller et al. performs worse than an implementation that adapts the Rete algorithm to the regular version of a join pattern matching benchmark, while outperforming on a variant with heavy conditional guards, which take longer to evaluate. Nevertheless, conforming Rete to the problem of join pattern matching requires heavy manual adaptation.\n  In this thesis, we enhance and optimize the stateful tree-based matching algorithm of Haller et al. to achieve up to tenfold performance improvements on certain benchmarks, approaching the performance of Rete on regular benchmarks while maintaining the advantages of versatility and performance with heavy guards. We also enhance the benchmark suite, adding new features and enhancing its extensibility and user-friendliness. We extend the join pattern implementation with a less ambiguous syntax as well as dynamic pattern switching. Finally, we present a new complex model use case for join patterns, showing their applicability in a microservice web architecture.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4f18\u5316\u4e86Haller\u7b49\u4eba\u7684\u72b6\u6001\u6811\u5339\u914d\u7b97\u6cd5\uff0c\u5728\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u9ad8\u8fbe10\u500d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u63a5\u8fd1Rete\u7b97\u6cd5\u5728\u5e38\u89c4\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u590d\u6742\u6761\u4ef6\u5b88\u536b\u7684\u4f18\u52bf\u3002", "motivation": "\u867d\u7136Haller\u7b49\u4eba\u63d0\u51fa\u4e86\u516c\u5e73\u8fde\u63a5\u6a21\u5f0f\u5339\u914d\u7684\u8bed\u4e49\u89c4\u8303\uff0c\u4f46\u5176\u72b6\u6001\u6811\u5339\u914d\u7b97\u6cd5\u5728\u65f6\u95f4\u6548\u7387\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u5e38\u89c4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0d\u5982Rete\u7b97\u6cd5\u3002\u73b0\u6709Rete\u7b97\u6cd5\u9700\u8981\u5927\u91cf\u624b\u52a8\u9002\u914d\u624d\u80fd\u5e94\u7528\u4e8e\u8fde\u63a5\u6a21\u5f0f\u5339\u914d\u95ee\u9898\u3002", "method": "\u589e\u5f3a\u548c\u4f18\u5316Haller\u7b49\u4eba\u7684\u72b6\u6001\u6811\u5339\u914d\u7b97\u6cd5\uff0c\u6539\u8fdb\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff08\u589e\u52a0\u65b0\u529f\u80fd\u3001\u589e\u5f3a\u53ef\u6269\u5c55\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\uff09\uff0c\u6269\u5c55\u8fde\u63a5\u6a21\u5f0f\u5b9e\u73b0\uff08\u63d0\u4f9b\u66f4\u5c11\u6b67\u4e49\u7684\u8bed\u6cd5\u548c\u52a8\u6001\u6a21\u5f0f\u5207\u6362\uff09\u3002", "result": "\u5728\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u9ad8\u8fbe10\u500d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u63a5\u8fd1Rete\u7b97\u6cd5\u5728\u5e38\u89c4\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u590d\u6742\u6761\u4ef6\u5b88\u536b\u7684\u6027\u80fd\u4f18\u52bf\u3002\u63d0\u4f9b\u4e86\u65b0\u7684\u590d\u6742\u6a21\u578b\u7528\u4f8b\uff0c\u5c55\u793a\u4e86\u8fde\u63a5\u6a21\u5f0f\u5728\u5fae\u670d\u52a1Web\u67b6\u6784\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u72b6\u6001\u6811\u5339\u914d\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fde\u63a5\u6a21\u5f0f\u5339\u914d\u7684\u65f6\u95f4\u6548\u7387\uff0c\u4f7f\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u5177\u7ade\u4e89\u529b\u3002\u6269\u5c55\u7684\u5b9e\u73b0\u548c\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e3a\u8fde\u63a5\u6a21\u5f0f\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5de5\u5177\u652f\u6301\u3002"}}
{"id": "2512.04093", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04093", "abs": "https://arxiv.org/abs/2512.04093", "authors": ["Ali Akbar Vali", "Sadoon Azizi", "Mohammad Shojafar", "Rajkumar Buyya"], "title": "Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions", "comment": "46 pages", "summary": "The exponential growth of Internet of Things (IoT) devices has intensified the demand for efficient and responsive services. To address this demand, fog and edge computing have emerged as distributed paradigms that bring computational resources closer to end users, reducing latency, bandwidth limitations, and energy consumption. However, these paradigms present challenges in resource management due to resource constraints, computational heterogeneity, dynamic workloads, and diverse Quality of Service (QoS) requirements. This paper presents a comprehensive survey of state-of-the-art resource management strategies in microservices-based fog and edge computing, focusing on energy-efficient solutions. We systematically review and classify more than 136 studies (2020-2024) into five key subdomains: service placement, resource provisioning, task scheduling and offloading, resource allocation, and instance selection. Our categorization is based on optimization techniques, targeted objectives, and the strengths and limitations of each approach. In addition, we examine existing surveys and identify unresolved challenges and gaps in the literature. By highlighting the lack of synergy among fundamental resource management components, we outline promising research directions leveraging AI-driven optimization, quantum computing, and serverless computing. This survey serves as a comprehensive reference for researchers and practitioners by providing a unified and energy-aware perspective on resource management in microservices-based fog and edge computing, paving the way for more integrated, efficient, and sustainable future solutions.", "AI": {"tldr": "\u5bf92020-2024\u5e74\u95f4136+\u7bc7\u7814\u7a76\u7684\u7efc\u8ff0\uff0c\u7cfb\u7edf\u5206\u7c7b\u4e86\u5fae\u670d\u52a1\u96fe/\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u8d44\u6e90\u7ba1\u7406\u7b56\u7565\uff0c\u91cd\u70b9\u5173\u6ce8\u8282\u80fd\u65b9\u6848\uff0c\u8bc6\u522b\u7814\u7a76\u7a7a\u767d\u5e76\u63d0\u51faAI\u3001\u91cf\u5b50\u8ba1\u7b97\u7b49\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u6fc0\u589e\u9700\u8981\u9ad8\u6548\u54cd\u5e94\u670d\u52a1\uff0c\u96fe/\u8fb9\u7f18\u8ba1\u7b97\u867d\u80fd\u964d\u4f4e\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u4f46\u9762\u4e34\u8d44\u6e90\u53d7\u9650\u3001\u5f02\u6784\u6027\u3001\u52a8\u6001\u8d1f\u8f7d\u548c\u591a\u6837\u5316QoS\u7b49\u8d44\u6e90\u7ba1\u7406\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u7efc\u8ff02020-2024\u5e74\u95f4136+\u9879\u7814\u7a76\uff0c\u6309\u4e94\u4e2a\u5173\u952e\u5b50\u9886\u57df\u5206\u7c7b\uff1a\u670d\u52a1\u653e\u7f6e\u3001\u8d44\u6e90\u4f9b\u5e94\u3001\u4efb\u52a1\u8c03\u5ea6\u4e0e\u5378\u8f7d\u3001\u8d44\u6e90\u5206\u914d\u3001\u5b9e\u4f8b\u9009\u62e9\uff0c\u57fa\u4e8e\u4f18\u5316\u6280\u672f\u3001\u76ee\u6807\u53ca\u4f18\u7f3a\u70b9\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5efa\u7acb\u4e86\u5fae\u670d\u52a1\u96fe/\u8fb9\u7f18\u8ba1\u7b97\u8d44\u6e90\u7ba1\u7406\u7684\u7edf\u4e00\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u57fa\u672c\u8d44\u6e90\u7ba1\u7406\u7ec4\u4ef6\u7f3a\u4e4f\u534f\u540c\u7684\u95ee\u9898\uff0c\u5e76\u53d1\u73b0\u6587\u732e\u4e2d\u7684\u672a\u89e3\u51b3\u6311\u6218\u548c\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u8d44\u6e90\u7ba1\u7406\u7684\u7edf\u4e00\u8282\u80fd\u89c6\u89d2\uff0c\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ecAI\u9a71\u52a8\u4f18\u5316\u3001\u91cf\u5b50\u8ba1\u7b97\u548cServerless\u8ba1\u7b97\uff0c\u4e3a\u66f4\u96c6\u6210\u3001\u9ad8\u6548\u3001\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2512.04096", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04096", "abs": "https://arxiv.org/abs/2512.04096", "authors": ["Sushant Kumar Gupta", "Anil Raghunath Iyer", "Chang Yu", "Neel Bagora", "Olivier Pomerleau", "Vivek Kumar", "Prunthaban Kanthakumar"], "title": "Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale", "comment": null, "summary": "Low-latency message delivery is crucial for real-time systems. Data originating from a producer must be delivered to consumers, potentially distributed in clusters across metropolitan and continental boundaries. With the growing scale of computing, there can be several thousand consumers of the data. Such systems require a robust messaging system capable of transmitting messages containing data across clusters and efficiently delivering them to consumers. The system must offer guarantees like ordering and at-least-once delivery while avoiding overload on consumers, allowing them to consume messages at their own pace.\n  This paper presents the design of Fast ACS (an abbreviation for Ads Copy Service), a file-based ordered message delivery system that leverages a combination of two-sided (inter-cluster) and one-sided (intra-cluster) communication primitives - namely, Remote Procedure Call and Remote Memory Access, respectively - to deliver messages. The system has been successfully deployed to dozens of production clusters and scales to accommodate several thousand consumers within each cluster, which amounts to Tbps-scale intra-cluster consumer traffic at peak. Notably, Fast ACS delivers messages to consumers across the globe within a few seconds or even sub-seconds (p99) based on the message volume and consumer scale, at a low resource cost.", "AI": {"tldr": "Fast ACS\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u4ef6\u7684\u987a\u5e8f\u6d88\u606f\u4f20\u9012\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408RPC\uff08\u8fdc\u7a0b\u8fc7\u7a0b\u8c03\u7528\uff09\u548cRMA\uff08\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\uff09\u4e24\u79cd\u901a\u4fe1\u539f\u8bed\uff0c\u5b9e\u73b0\u8de8\u96c6\u7fa4\u7684\u4f4e\u5ef6\u8fdf\u6d88\u606f\u4f20\u9012\uff0c\u652f\u6301\u6570\u5343\u6d88\u8d39\u8005\uff0c\u8fbe\u5230Tbps\u7ea7\u522b\u7684\u6d41\u91cf\u3002", "motivation": "\u5b9e\u65f6\u7cfb\u7edf\u9700\u8981\u4f4e\u5ef6\u8fdf\u6d88\u606f\u4f20\u9012\uff0c\u6570\u636e\u9700\u8981\u4ece\u751f\u4ea7\u8005\u4f20\u9012\u5230\u53ef\u80fd\u5206\u5e03\u5728\u57ce\u5e02\u548c\u5927\u9646\u8fb9\u754c\u7684\u96c6\u7fa4\u4e2d\u7684\u6d88\u8d39\u8005\u3002\u968f\u7740\u8ba1\u7b97\u89c4\u6a21\u7684\u6269\u5927\uff0c\u53ef\u80fd\u6709\u6570\u5343\u4e2a\u6d88\u8d39\u8005\uff0c\u9700\u8981\u5f3a\u5927\u7684\u6d88\u606f\u7cfb\u7edf\u6765\u4fdd\u8bc1\u987a\u5e8f\u6027\u548c\u81f3\u5c11\u4e00\u6b21\u4f20\u9012\uff0c\u540c\u65f6\u907f\u514d\u6d88\u8d39\u8005\u8fc7\u8f7d\u3002", "method": "\u8bbe\u8ba1Fast ACS\uff08Ads Copy Service\uff09\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6587\u4ef6\u7684\u987a\u5e8f\u6d88\u606f\u4f20\u9012\u7cfb\u7edf\uff0c\u5229\u7528\u4e24\u79cd\u901a\u4fe1\u539f\u8bed\u7684\u7ec4\u5408\uff1a\u53cc\u9762\uff08\u96c6\u7fa4\u95f4\uff09\u901a\u4fe1\u4f7f\u7528\u8fdc\u7a0b\u8fc7\u7a0b\u8c03\u7528\uff08RPC\uff09\uff0c\u5355\u9762\uff08\u96c6\u7fa4\u5185\uff09\u901a\u4fe1\u4f7f\u7528\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\uff08RMA\uff09\u3002", "result": "\u7cfb\u7edf\u5df2\u6210\u529f\u90e8\u7f72\u5230\u6570\u5341\u4e2a\u751f\u4ea7\u96c6\u7fa4\uff0c\u6bcf\u4e2a\u96c6\u7fa4\u53ef\u6269\u5c55\u5230\u6570\u5343\u6d88\u8d39\u8005\uff0c\u5cf0\u503c\u65f6\u8fbe\u5230Tbps\u7ea7\u522b\u7684\u96c6\u7fa4\u5185\u6d88\u8d39\u8005\u6d41\u91cf\u3002\u6839\u636e\u6d88\u606f\u91cf\u548c\u6d88\u8d39\u8005\u89c4\u6a21\uff0c\u80fd\u591f\u5728\u51e0\u79d2\u751a\u81f3\u4e9a\u79d2\u7ea7\u522b\uff08p99\uff09\u5c06\u6d88\u606f\u4f20\u9012\u5230\u5168\u7403\u6d88\u8d39\u8005\uff0c\u8d44\u6e90\u6210\u672c\u4f4e\u3002", "conclusion": "Fast ACS\u901a\u8fc7\u521b\u65b0\u7684\u6587\u4ef6\u57fa\u7840\u67b6\u6784\u548c\u6df7\u5408\u901a\u4fe1\u539f\u8bed\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u3001\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u7684\u6d88\u606f\u4f20\u9012\u7cfb\u7edf\uff0c\u6ee1\u8db3\u5b9e\u65f6\u7cfb\u7edf\u7684\u4e25\u683c\u8981\u6c42\u3002"}}
{"id": "2512.04226", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04226", "abs": "https://arxiv.org/abs/2512.04226", "authors": ["Ryan Swann", "Muhammad Osama", "Xiaohu Guo", "Bryant Nelson", "Lixun Zhang", "Alex Brown", "Yen Ong", "Ali Yazdani", "Sean Siddens", "Ganesh Dasika", "Alex Underwood"], "title": "tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection", "comment": null, "summary": "We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.", "AI": {"tldr": "tritonBLAS\u662f\u4e00\u4e2a\u57fa\u4e8e\u67b6\u6784\u53c2\u6570\uff08\u5982\u7f13\u5b58\u5c42\u6b21\u7ed3\u6784\uff09\u7684\u786e\u5b9a\u6027\u5206\u6790\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u6027\u80fdGPU GEMM\u5185\u6838\uff0c\u65e0\u9700\u8fd0\u884c\u65f6\u81ea\u52a8\u8c03\u4f18\u5373\u53ef\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfGPU GEMM\u5185\u6838\u901a\u5e38\u4f9d\u8d56\u8017\u65f6\u7684\u8fd0\u884c\u65f6\u81ea\u52a8\u8c03\u4f18\u6765\u83b7\u5f97\u9ad8\u6027\u80fd\uff0c\u8fd9\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u4e0d\u5b9e\u7528\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u57fa\u4e8e\u67b6\u6784\u53c2\u6570\u76f4\u63a5\u751f\u6210\u9ad8\u6027\u80fd\u5185\u6838\u7684\u786e\u5b9a\u6027\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86tritonBLAS\u5206\u6790\u6a21\u578b\uff0c\u660e\u786e\u5efa\u6a21\u67b6\u6784\u62d3\u6251\u3001\u77e9\u9635\u5f62\u72b6\u548c\u7b97\u6cd5\u5206\u5757\u884c\u4e3a\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u57fa\u4e8e\u6b64\u6a21\u578b\uff0c\u5728Triton\u6846\u67b6\u5185\u5b9e\u73b0\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7GEMM\u6846\u67b6\u3002", "result": "\u5728\u73b0\u4ee3GPU\u4e0a\u8bc4\u4f30\u591a\u79cdGEMM\u95ee\u9898\u89c4\u6a21\uff0ctritonBLAS\u5b9e\u73b0\u4e86\u8d85\u8fc795%\u7684\u81ea\u52a8\u8c03\u4f18\u89e3\u51b3\u65b9\u6848\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u81ea\u52a8\u8c03\u4f18\u65f6\u95f4\u964d\u4e3a\u96f6\u3002", "conclusion": "tritonBLAS\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u5373\u63d2\u5373\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u7528\u4e8e\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u81ea\u52a8\u8c03\u4f18\u7684\u5f00\u9500\u3002"}}
{"id": "2512.04291", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04291", "abs": "https://arxiv.org/abs/2512.04291", "authors": ["Huda Ibeid", "Anthony-Trung Nguyen", "Aditya Nishtala", "Premanand Sakarda", "Larry Kaplan", "Nilakantan Mahadevan", "Michael Woodacre", "Victor Anisimov", "Kalyan Kumaran", "JaeHyuk Kwack", "Vitali Morozov", "Servesh Muralidharan", "Scott Parker"], "title": "Scaling MPI Applications on Aurora", "comment": null, "summary": "The Aurora supercomputer, which was deployed at Argonne National Laboratory in 2024, is currently one of three Exascale machines in the world on the Top500 list. The Aurora system is composed of over ten thousand nodes each of which contains six Intel Data Center Max Series GPUs, Intel's first data center-focused discrete GPU, and two Intel Xeon Max Series CPUs, Intel's first Xeon processor to contain HBM memory. To achieve Exascale performance the system utilizes the HPE Slingshot high-performance fabric interconnect to connect the nodes. Aurora is currently the largest deployment of the Slingshot fabric to date with nearly 85,000 Cassini NICs and 5,600 Rosetta switches connected in a dragonfly topology. The combination of the Intel powered nodes and the Slingshot network enabled Aurora to become the second fastest system on the Top500 list in June of 2024 and the fastest system on the HPL MxP benchmark. The system is one of the most powerful systems in the world dedicated to AI and HPC simulations for open science. This paper presents details of the Aurora system design with a particular focus on the network fabric and the approach taken to validating it. The performance of the systems is demonstrated through the presentation of the results of MPI benchmarks as well as performance benchmarks including HPL, HPL-MxP, Graph500, and HPCG run on a large fraction of the system. Additionally results are presented for a diverse set of applications including HACC, AMR-Wind, LAMMPS, and FMM demonstrating that Aurora provides the throughput, latency, and bandwidth across system needed to allow applications to perform and scale to large node counts and providing new levels of capability and enabling breakthrough science.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Aurora\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5176\u7f51\u7edc\u67b6\u6784\u548c\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u8be5\u7cfb\u7edf\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548c\u79d1\u5b66\u5e94\u7528\u4e2d\u7684\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "Aurora\u662f2024\u5e74\u90e8\u7f72\u5728\u963f\u8d21\u56fd\u5bb6\u5b9e\u9a8c\u5ba4\u7684\u767e\u4ebf\u4ebf\u6b21\u8d85\u7ea7\u8ba1\u7b97\u673a\uff0c\u76ee\u524d\u662f\u5168\u7403Top500\u699c\u5355\u4e0a\u4e09\u53f0\u767e\u4ebf\u4ebf\u6b21\u673a\u5668\u4e4b\u4e00\u3002\u672c\u6587\u65e8\u5728\u8be6\u7ec6\u4ecb\u7ecdAurora\u7684\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u7279\u522b\u662f\u5176\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u9a8c\u8bc1\u5176\u80fd\u591f\u652f\u6301\u5927\u89c4\u6a21AI\u548cHPC\u6a21\u62df\uff0c\u63a8\u52a8\u5f00\u653e\u79d1\u5b66\u53d1\u5c55\u3002", "method": "\u8bba\u6587\u91cd\u70b9\u4ecb\u7ecd\u4e86Aurora\u7684\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\uff0c\u91c7\u7528HPE Slingshot\u9ad8\u6027\u80fd\u4e92\u8fde\u7f51\u7edc\uff0c\u5305\u542b\u8fd185,000\u4e2aCassini\u7f51\u5361\u548c5,600\u4e2aRosetta\u4ea4\u6362\u673a\uff0c\u91c7\u7528\u873b\u8713\u62d3\u6251\u7ed3\u6784\u3002\u901a\u8fc7MPI\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53caHPL\u3001HPL-MxP\u3001Graph500\u3001HPCG\u7b49\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u6765\u9a8c\u8bc1\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u5728HACC\u3001AMR-Wind\u3001LAMMPS\u3001FMM\u7b49\u591a\u79cd\u79d1\u5b66\u5e94\u7528\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "Aurora\u57282024\u5e746\u6708\u7684Top500\u699c\u5355\u4e2d\u6392\u540d\u7b2c\u4e8c\uff0c\u5728HPL MxP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u7b2c\u4e00\u3002\u7cfb\u7edf\u5c55\u793a\u4e86\u4f18\u5f02\u7684\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u5e26\u5bbd\u6027\u80fd\uff0c\u80fd\u591f\u652f\u6301\u5e94\u7528\u7a0b\u5e8f\u6269\u5c55\u5230\u5927\u89c4\u6a21\u8282\u70b9\u6570\u91cf\uff0c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u80fd\u529b\u6c34\u5e73\u3002", "conclusion": "Aurora\u8d85\u7ea7\u8ba1\u7b97\u673a\u901a\u8fc7\u521b\u65b0\u7684\u786c\u4ef6\u67b6\u6784\u548c\u5148\u8fdb\u7684\u7f51\u7edc\u8bbe\u8ba1\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u767e\u4ebf\u4ebf\u6b21\u8ba1\u7b97\u80fd\u529b\uff0c\u6210\u4e3a\u5168\u7403\u6700\u5f3a\u5927\u7684AI\u548cHPC\u7cfb\u7edf\u4e4b\u4e00\uff0c\u80fd\u591f\u652f\u6301\u7a81\u7834\u6027\u7684\u79d1\u5b66\u7814\u7a76\u3002"}}
{"id": "2512.04320", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2512.04320", "abs": "https://arxiv.org/abs/2512.04320", "authors": ["Yineng Yan", "William Ruys", "Hochan Lee", "Ian Henriksen", "Arthur Peters", "Sean Stephens", "Bozhi You", "Henrique Fingler", "Martin Burtscher", "Milos Gligoric", "Keshav Pingali", "Mattan Erez", "George Biros", "Christopher J. Rossbach"], "title": "VLCs: Managing Parallelism with Virtualized Libraries", "comment": "Research Paper accepted to the ACM Symposium on Cloud Computing (SoCC'25)", "summary": "As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible.\n  We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process.\n  In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.", "AI": {"tldr": "VLCs\uff08\u865a\u62df\u5e93\u4e0a\u4e0b\u6587\uff09\u901a\u8fc7\u521b\u5efa\u8fdb\u7a0b\u5b50\u5355\u5143\u6765\u5c01\u88c5\u5e93\u548c\u8d44\u6e90\u5206\u914d\uff0c\u65e0\u9700\u4fee\u6539\u5e93\u4ee3\u7801\u5373\u53ef\u63a7\u5236\u5e93\u7684\u8d44\u6e90\u4f7f\u7528\uff0c\u89e3\u51b3\u5e76\u884c\u5e93\u7ec4\u5408\u65f6\u7684\u8d44\u6e90\u4e89\u7528\u95ee\u9898\uff0c\u5b9e\u73b0\u6700\u9ad82.85\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u4ee3\u5e76\u884c\u673a\u5668\u65e5\u76ca\u590d\u6742\uff0c\u7a0b\u5e8f\u5458\u4f9d\u8d56\u8f6f\u4ef6\u5e93\u7ec4\u5408\u6765\u5229\u7528\u5e76\u884c\u6027\u3002\u4f46\u8bb8\u591a\u5e93\u8bbe\u8ba1\u65f6\u672a\u8003\u8651\u7ec4\u5408\u4f7f\u7528\uff0c\u5047\u8bbe\u72ec\u5360\u6240\u6709\u8d44\u6e90\uff0c\u5bfc\u81f4\u5e76\u53d1\u4f7f\u7528\u65f6\u4ea7\u751f\u8d44\u6e90\u4e89\u7528\u548c\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6848\u9700\u8981\u4fee\u6539\u5e93\u6216\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u5e38\u4e0d\u53ef\u884c\u3002", "method": "\u63d0\u51fa\u865a\u62df\u5e93\u4e0a\u4e0b\u6587\uff08VLCs\uff09\u4f5c\u4e3a\u8fdb\u7a0b\u5b50\u5355\u5143\uff0c\u5c01\u88c5\u5e93\u96c6\u5408\u53ca\u5176\u8d44\u6e90\u5206\u914d\u3002VLCs\u5728\u4e0d\u4fee\u6539\u5e93\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u63a7\u5236\u5e93\u7684\u8d44\u6e90\u4f7f\u7528\uff0c\u5141\u8bb8\u7528\u6237\u5728\u5e93\u4e4b\u95f4\u5212\u5206\u8d44\u6e90\u9632\u6b62\u4e89\u7528\uff0c\u6216\u52a0\u8f7d\u540c\u4e00\u5e93\u7684\u591a\u4e2a\u526f\u672c\u4ee5\u652f\u6301\u5e76\u884c\u6267\u884c\u7ebf\u7a0b\u4e0d\u5b89\u5168\u7684\u4ee3\u7801\u3002", "result": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86C++\u548cPython\u7684VLCs\u539f\u578b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4f7f\u7528OpenMP\u3001OpenBLAS\u548cLibTorch\u7b49\u5e93\u7684\u5e94\u7528\u4e2d\uff0cVLCs\u80fd\u5b9e\u73b0\u6700\u9ad82.85\u500d\u7684\u52a0\u901f\u3002", "conclusion": "VLCs\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u5e93\u4ee3\u7801\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5e76\u884c\u5e93\u7ec4\u5408\u65f6\u7684\u8d44\u6e90\u4e89\u7528\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e76\u884c\u5e94\u7528\u7684\u6027\u80fd\u3002"}}
{"id": "2512.04355", "categories": ["cs.DC", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.04355", "abs": "https://arxiv.org/abs/2512.04355", "authors": ["Gregory Bolet", "Giorgis Georgakoudis", "Konstantinos Parasyris", "Harshitha Menon", "Niranjan Hasabnis", "Kirk W. Cameron", "Gal Oren"], "title": "Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity", "comment": "13 pages, 6 figures, MLSys 2026 Submission", "summary": "Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to \"count without running\" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench", "AI": {"tldr": "gpuFLOPBench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u9884\u6d4bGPU\u6d6e\u70b9\u8fd0\u7b97\u6b21\u6570\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b577\u4e2aCUDA\u5185\u6838\uff0c\u65e8\u5728\u6d4b\u8bd5\u6a21\u578b\u5728\u4e0d\u8fd0\u884c\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u524d\u77bb\u6027\u6027\u80fd\u63a8\u7406\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u5f88\u5c11\u6d4b\u8bd5\u5176\u5728GPU\u6027\u80fd\u9884\u6d4b\u65b9\u9762\u7684\u524d\u77bb\u6027\u63a8\u7406\u80fd\u529b\u3002GPU\u8f6f\u4ef6\u5f00\u53d1\u9700\u8981\u5f00\u53d1\u8005\u80fd\u591f\u9884\u6d4b\u6027\u80fd\u74f6\u9888\uff0c\u4f46\u73b0\u6709\u4ee3\u7801\u52a9\u624b\u65e0\u6cd5\u5185\u5316\u786c\u4ef6\u7279\u5b9a\u7684\u5fae\u7801\u6548\u5e94\u3002", "method": "\u521b\u5efagpuFLOPBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4eceHeCBench\u63d0\u53d6\u7684577\u4e2aCUDA\u5185\u6838\uff0c\u6bcf\u4e2a\u5185\u6838\u90fd\u6807\u6ce8\u4e86\u771f\u5b9e\u6027\u80fd\u5206\u6790\u548c8\u4e2a\u6267\u884c\u5c5e\u6027\uff0c\u533a\u5206\u53ef\u7b80\u5355\u5206\u6790\u7684\u4ee3\u7801\u548cFLOPs\u4f9d\u8d56\u4e8e\u7f16\u8bd1\u5668\u6216\u8fd0\u884c\u65f6\u884c\u4e3a\u7684\u590d\u6742\u5185\u6838\u3002", "result": "\u6700\u65b0LLM\u5728\u7b80\u5355\u5185\u6838\u4e0a\u80fd\u5b9e\u73b0\u5b8c\u7f8e\u5206\u7c7b\uff0c\u4f46\u5728\u6d89\u53ca\u9664\u6cd5\u3001\u5185\u7f6e\u6570\u5b66\u51fd\u6570\u6216\u516c\u5171\u5b50\u8868\u8fbe\u5f0f\u7b49\u9690\u542bFLOPs\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u4f1a\u51fa\u73b0\u591a\u4e2a\u6570\u91cf\u7ea7\u7684\u9519\u8bef\uff0c\u663e\u793a\u51fa\u5bf9\u786c\u4ef6\u7279\u5b9a\u5fae\u7801\u6548\u5e94\u7406\u89e3\u4e0d\u8db3\u7684\u6838\u5fc3\u9650\u5236\u3002", "conclusion": "gpuFLOPBench\u4e3a\u5f00\u53d1\u80fd\u591f\u50cf\u7ecf\u9a8c\u4e30\u5bcc\u7684GPU\u5f00\u53d1\u8005\u4e00\u6837\u4e25\u683c\u63a8\u7406\u6027\u80fd\u7684LLM\u5de5\u5177\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e13\u6ce8\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u4ee3\u7801\u52a9\u624b\u5728\u786c\u4ef6\u7279\u5b9a\u6027\u80fd\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.04389", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04389", "abs": "https://arxiv.org/abs/2512.04389", "authors": ["Zhen Hu", "Dongliang Xiong", "Kai Huang", "Changjun Wu", "Xiaowen Jiang"], "title": "A Structure-Aware Irregular Blocking Method for Sparse LU Factorization", "comment": null, "summary": "In sparse LU factorization, nonzero elements after symbolic factorization tend to distribute in diagonal and right-bottom region of sparse matrices. However, regular 2D blocking on this non-uniform distribution structure may lead to workload imbalance across blocks. Besides, existing matrix features fail to guide us effectively in blocking. In this paper, we propose a structure-aware irregular blocking method for numerical factorization. A novel diagonal block-based feature is introduced to effectively characterize the local nonzero distribution of sparse matrices. Based on this, we further propose an irregular blocking method that adjusts block sizes according to the local distribution of nonzeros. The strategy utilizes fine-grained blocks in dense regions and coarse-grained blocks in sparse regions, adequately balancing the nonzeros of blocks both within the same level and across levels in the dependency tree. Experiments demonstrate that, on a single NVIDIA A100 GPU, our proposed irregular blocking method achieves average speedups of 1.50x and 3.32x over PanguLU and the latest SuperLU_DIST, respectively. In addition, it achieves speedups of 1.40x and 3.84x over PanguLU and SuperLU_DIST on 4 NVIDIA A100 GPUs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5bf9\u89d2\u7ebf\u5757\u7279\u5f81\u7684\u7ed3\u6784\u611f\u77e5\u4e0d\u89c4\u5219\u5206\u5757\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a00\u758fLU\u5206\u89e3\uff0c\u901a\u8fc7\u8c03\u6574\u5757\u5927\u5c0f\u9002\u5e94\u5c40\u90e8\u975e\u96f6\u5143\u5206\u5e03\uff0c\u5728GPU\u4e0a\u5b9e\u73b0\u663e\u8457\u52a0\u901f", "motivation": "\u7a00\u758fLU\u5206\u89e3\u4e2d\uff0c\u7b26\u53f7\u5206\u89e3\u540e\u7684\u975e\u96f6\u5143\u503e\u5411\u4e8e\u5206\u5e03\u5728\u77e9\u9635\u5bf9\u89d2\u7ebf\u548c\u53f3\u4e0b\u533a\u57df\uff0c\u8fd9\u79cd\u975e\u5747\u5300\u5206\u5e03\u5bfc\u81f4\u5e38\u89c42D\u5206\u5757\u65b9\u6cd5\u4ea7\u751f\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u73b0\u6709\u77e9\u9635\u7279\u5f81\u65e0\u6cd5\u6709\u6548\u6307\u5bfc\u5206\u5757", "method": "\u63d0\u51fa\u5bf9\u89d2\u7ebf\u5757\u7279\u5f81\u6765\u6709\u6548\u8868\u5f81\u7a00\u758f\u77e9\u9635\u7684\u5c40\u90e8\u975e\u96f6\u5143\u5206\u5e03\uff0c\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e0d\u89c4\u5219\u5206\u5757\u65b9\u6cd5\uff0c\u6839\u636e\u5c40\u90e8\u975e\u96f6\u5143\u5206\u5e03\u8c03\u6574\u5757\u5927\u5c0f\uff1a\u5728\u5bc6\u96c6\u533a\u57df\u4f7f\u7528\u7ec6\u7c92\u5ea6\u5757\uff0c\u5728\u7a00\u758f\u533a\u57df\u4f7f\u7528\u7c97\u7c92\u5ea6\u5757\uff0c\u5e73\u8861\u4f9d\u8d56\u6811\u4e2d\u540c\u4e00\u5c42\u7ea7\u548c\u8de8\u5c42\u7ea7\u7684\u5757\u975e\u96f6\u5143\u6570\u91cf", "result": "\u5728\u5355\u5757NVIDIA A100 GPU\u4e0a\uff0c\u76f8\u6bd4PanguLU\u548c\u6700\u65b0SuperLU_DIST\u5206\u522b\u83b7\u5f97\u5e73\u57471.50\u500d\u548c3.32\u500d\u52a0\u901f\uff1b\u57284\u5757A100 GPU\u4e0a\uff0c\u5206\u522b\u83b7\u5f971.40\u500d\u548c3.84\u500d\u52a0\u901f", "conclusion": "\u63d0\u51fa\u7684\u7ed3\u6784\u611f\u77e5\u4e0d\u89c4\u5219\u5206\u5757\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u7a00\u758f\u77e9\u9635\u7684\u975e\u5747\u5300\u5206\u5e03\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u5757\u5927\u5c0f\u5e73\u8861\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5728GPU\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347"}}
{"id": "2512.04449", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04449", "abs": "https://arxiv.org/abs/2512.04449", "authors": ["Suyeon Lee", "Kangkyu Park", "Kwangsik Shin", "Ada Gavrilovska"], "title": "Offloading to CXL-based Computational Memory", "comment": null, "summary": "CXL-based Computational Memory (CCM) enables near-memory processing within expanded remote memory, presenting opportunities to address data movement costs associated with disaggregated memory systems and to accelerate overall performance. However, existing operation offloading mechanisms are not capable of leveraging the trade-offs of different models based on different CXL protocols. This work first examines these tradeoffs and demonstrates their impact on end-to-end performance and system efficiency for workloads with diverse data and processing requirements. We propose a novel 'Asynchronous Back-Streaming' protocol by carefully layering data and control transfer operations on top of the underlying CXL protocols. We design KAI, a system that realizes the asynchronous back-streaming model that supports asynchronous data movement and lightweight pipelining in host-CCM interactions. Overall, KAI reduces end-to-end runtime by up to 50.4%, and CCM and host idle times by average 22.11x and 3.85x, respectively.", "AI": {"tldr": "KAI\u7cfb\u7edf\u901a\u8fc7\u5f02\u6b65\u56de\u4f20\u534f\u8bae\u4f18\u5316CXL\u8ba1\u7b97\u5185\u5b58\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u7aef\u5230\u7aef\u8fd0\u884c\u65f6\u95f4\u8fbe50.4%", "motivation": "\u73b0\u6709CXL\u8ba1\u7b97\u5185\u5b58\u7684\u64cd\u4f5c\u5378\u8f7d\u673a\u5236\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e0d\u540cCXL\u534f\u8bae\u6a21\u578b\u7684\u6743\u8861\u4f18\u52bf\uff0c\u5bfc\u81f4\u6570\u636e\u79fb\u52a8\u6210\u672c\u9ad8\u548c\u7cfb\u7edf\u6548\u7387\u4f4e\u4e0b", "method": "\u63d0\u51fa\"\u5f02\u6b65\u56de\u4f20\"\u534f\u8bae\uff0c\u5728\u5e95\u5c42CXL\u534f\u8bae\u4e0a\u5206\u5c42\u5904\u7406\u6570\u636e\u548c\u63a7\u5236\u4f20\u8f93\u64cd\u4f5c\uff1b\u8bbe\u8ba1KAI\u7cfb\u7edf\u5b9e\u73b0\u5f02\u6b65\u6570\u636e\u79fb\u52a8\u548c\u8f7b\u91cf\u7ea7\u6d41\u6c34\u7ebf\u7684\u4e3b\u673a-CCM\u4ea4\u4e92", "result": "KAI\u5c06\u7aef\u5230\u7aef\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe50.4%\uff0cCCM\u548c\u4e3b\u673a\u7a7a\u95f2\u65f6\u95f4\u5206\u522b\u5e73\u5747\u51cf\u5c1122.11\u500d\u548c3.85\u500d", "conclusion": "\u5f02\u6b65\u56de\u4f20\u534f\u8bae\u548cKAI\u7cfb\u7edf\u80fd\u6709\u6548\u5229\u7528CXL\u534f\u8bae\u6743\u8861\uff0c\u663e\u8457\u63d0\u5347CXL\u8ba1\u7b97\u5185\u5b58\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6548\u7387"}}
{"id": "2512.04984", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04984", "abs": "https://arxiv.org/abs/2512.04984", "authors": ["O. Tansel Baydas", "Ozgur B. Akan"], "title": "Federated Learning for Terahertz Wireless Communication", "comment": "10 pages, 4 figures", "summary": "The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.", "AI": {"tldr": "THz\u901a\u4fe1\u4e0e\u8054\u90a6\u5b66\u4e60\u7684\u7ed3\u5408\u9762\u4e34\u5bbd\u5e26\u635f\u4f24\u7684\u7406\u8bba\u6311\u6218\uff0c\u672c\u6587\u63ed\u793a\u4e86\u9891\u8c31\u7a7a\u6d1e\u5bfc\u81f4\u6536\u655b\u8bef\u5dee\u7684\"\u591a\u6837\u6027\u9677\u9631\"\uff0c\u5e76\u63d0\u51faSNR\u52a0\u6743\u805a\u5408\u7b56\u7565\u6765\u6062\u590d\u6536\u655b\u6027\u3002", "motivation": "THz\u901a\u4fe1\u4e0e\u8054\u90a6\u5b66\u4e60\u7ed3\u5408\u53ef\u5b9e\u73b0\u8d85\u5feb\u901f\u5206\u5e03\u5f0f\u5b66\u4e60\uff0c\u4f46\u73b0\u5b9e\u5bbd\u5e26\u635f\u4f24\uff08\u5982\u6ce2\u675f\u503e\u659c\u3001\u5206\u5b50\u5438\u6536\u7b49\uff09\u5bf9\u4f18\u5316\u52a8\u6001\u7684\u5f71\u54cd\u7f3a\u4e4f\u7406\u8bba\u5206\u6790\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u591a\u8f7d\u6ce2\u968f\u673a\u6846\u67b6\uff0c\u5c06\u672c\u5730\u68af\u5ea6\u66f4\u65b0\u4e0e\u9891\u7387\u9009\u62e9\u6027THz\u6548\u5e94\uff08\u6ce2\u675f\u503e\u659c\u3001\u5206\u5b50\u5438\u6536\u3001\u6296\u52a8\uff09\u663e\u5f0f\u8026\u5408\uff0c\u5206\u6790\u6536\u655b\u8bef\u5dee\u5e95\u9650\u4e0e\u5b50\u8f7d\u6ce2SNR\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\"\u591a\u6837\u6027\u9677\u9631\"\uff1a\u6807\u51c6\u65e0\u504f\u805a\u5408\u4e0b\u6536\u655b\u8bef\u5dee\u5e95\u9650\u7531\u5b50\u8f7d\u6ce2SNR\u7684\u8c03\u548c\u5747\u503c\u9a71\u52a8\uff0c\u5355\u4e2a\u9891\u8c31\u7a7a\u6d1e\u53ef\u4f7f\u6574\u4e2a\u5e26\u5bbd\u5931\u6548\uff1b\u8bc6\u522b\u5e26\u5bbd\u6781\u9650\uff0c\u8d85\u8fc7\u4e34\u754c\u70b9\u4f1a\u56e0\u70ed\u566a\u58f0\u548c\u589e\u76ca\u5d29\u6e83\u800c\u964d\u4f4e\u6536\u655b\u6027\uff1b\u8bc1\u660eSNR\u52a0\u6743\u805a\u5408\u7b56\u7565\u53ef\u6291\u5236\u65b9\u5dee\u5947\u70b9\uff0c\u5728\u9ad8\u503e\u659c\u533a\u57df\u6062\u590d\u6536\u655b\u3002", "conclusion": "THz-FL\u7cfb\u7edf\u6027\u80fd\u53d7\u7269\u7406\u5c42\u53c2\u6570\u663e\u8457\u5f71\u54cd\uff0c\u6807\u51c6\u5e73\u5747\u805a\u5408\u5728\u4e25\u91cd\u6ce2\u675f\u503e\u659c\u4e0b\u5931\u6548\uff0c\u9700\u8981SNR\u52a0\u6743\u805a\u5408\u6765\u514b\u670d\u9891\u8c31\u7a7a\u6d1e\u95ee\u9898\uff0c\u786e\u4fdd\u53ef\u9760\u6a21\u578b\u66f4\u65b0\u3002"}}
