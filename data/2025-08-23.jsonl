{"id": "2508.15105", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.15105", "abs": "https://arxiv.org/abs/2508.15105", "authors": ["Yunzhao Yang", "Runhui Wang", "Xuanqing Liu", "Adit Krishnan", "Yefan Tao", "Yuqian Deng", "Kuangyou Yao", "Peiyuan Sun", "Henrik Johnson", "Aditi sinha", "Davor Golac", "Gerald Friedland", "Usman Shakeel", "Daryl Cooke", "Joe Sullivan", "Chris Kong"], "title": "Declarative Data Pipeline for Large Scale ML Services", "comment": null, "summary": "Modern distributed data processing systems face significant challenges in\nbalancing system performance with code maintainability and developer\nproductivity, particularly when integrating machine learning capabilities at\nscale. In large collaborative environments, these challenges are amplified by\nhigh communication overhead between teams and the complexity of coordinating\ndevelopment across multiple groups. This paper presents a novel \"Declarative\nData Pipeline\" architecture that addresses these challenges while processing\nbillions of records with high accuracy and efficiency. Our architecture\nintroduces a modular framework that seamlessly integrates machine learning\ncapabilities within Apache Spark by combining logical computation units that we\nrefer as Pipes, departing from traditional microservice-based approaches. By\nestablishing clear component boundaries and standardized interfaces, we achieve\nboth modularity and system optimization without sacrificing maintainability.\nThe enterprise case study demonstrate substantial improvements in multiple\ndimensions: development efficiency improved by 50%,\ncollaboration/troubleshooting efforts compressed from weeks to days,\nperformance improved by 500x in scalability and by 10x in throughput. The\nacademic experiment also proves at least 5.7x faster in throughput with 99% CPU\nutilization than non-framework implementations. This paper details the\narchitectural decisions, implementation strategies, and performance\noptimizations that enable these improvements, providing insights for building\nscalable, maintainable data processing systems that effectively balance system\nperformance with development velocity."}
{"id": "2508.15351", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.15351", "abs": "https://arxiv.org/abs/2508.15351", "authors": ["Cynthia Marcelino", "Leonard Guelmino", "Thomas Pusztai", "Stefan Nastic"], "title": "Databelt: A Continuous Data Path for Serverless Workflows in the 3D Compute Continuum", "comment": null, "summary": "Typically, serverless functions rely on remote storage services for managing\nstate, which can result in increased latency and network communication\noverhead. In a dynamic environment such as the 3D (Edge-Cloud-Space) Compute\nContinuum, serverless functions face additional challenges due to frequent\nchanges in network topology. As satellites move in and out of the range of\nground stations, functions must make multiple hops to access cloud services,\nleading to high-latency state access and unnecessary data transfers. In this\npaper, we present Databelt, a state management framework for serverless\nworkflows designed for the dynamic environment of the 3D Compute Continuum.\nDatabelt introduces an SLO-aware state propagation mechanism that enables the\nfunction state to move continuously in orbit. Databelt proactively offloads\nfunction states to the most suitable node, such that when functions execute,\nthe data is already present on the execution node or nearby, thus minimizing\nstate access latency and reducing the number of network hops. Additionally,\nDatabelt introduces a function state fusion mechanism that abstracts state\nmanagement for functions sharing the same serverless runtime. When functions\nare fused, Databelt seamlessly retrieves their state as a group, reducing\nredundant network and storage operations and improving overall workflow\nefficiency. Our experimental results show that Databelt reduces workflow\nexecution time by up to 66% and increases throughput by 50% compared to the\nbaselines. Furthermore, our results show that Databelt function state fusion\nreduces storage operations latency by up to 20%, by reducing repetitive storage\nrequests for functions within the same runtime, ensuring efficient execution of\nserverless workflows in highly dynamic network environments such as the 3D\nContinuum."}
{"id": "2508.15484", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.15484", "abs": "https://arxiv.org/abs/2508.15484", "authors": ["Caterina Feletti", "Paola Flocchini", "Debasish Pattanayak", "Giuseppe Prencipe", "Nicola Santoro"], "title": "Universal Dancing by Luminous Robots under Sequential Schedulers", "comment": null, "summary": "The Dancing problem requires a swarm of $n$ autonomous mobile robots to form\na sequence of patterns, aka perform a choreography. Existing work has proven\nthat some crucial restrictions on choreographies and initial configurations\n(e.g., on repetitions of patterns, periodicity, symmetries,\ncontractions/expansions) must hold so that the Dancing problem can be solved\nunder certain robot models. Here, we prove that these necessary constraints can\nbe dropped by considering the LUMI model (i.e., where robots are endowed with a\nlight whose color can be chosen from a constant-size palette) under the quite\nunexplored sequential scheduler. We formalize the class of Universal Dancing\nproblems which require a swarm of $n$ robots starting from any initial\nconfiguration to perform a (periodic or finite) sequence of arbitrary patterns,\nonly provided that each pattern consists of $n$ vertices (including\nmultiplicities). However, we prove that, to be solvable under LUMI, the length\nof the feasible choreographies is bounded by the compositions of $n$ into the\nnumber of colors available to the robots. We provide an algorithm solving the\nUniversal Dancing problem by exploiting the peculiar capability of sequential\nrobots to implement a distributed counter mechanism. Even assuming non-rigid\nmovements, our algorithm ensures spatial homogeneity of the performed\nchoreography."}
{"id": "2508.15562", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.15562", "abs": "https://arxiv.org/abs/2508.15562", "authors": ["Pierre Fraigniaud", "Minh Hang Nguyen", "Ami Paz", "Ulrich Schmid", "Hugo Rincon Galeana"], "title": "Lower Bounds for $k$-Set Agreement in Fault-Prone Networks", "comment": "To be presented in DISC 2025", "summary": "We develop a new lower bound for k-set agreement in synchronous\nmessage-passing systems connected by an arbitrary directed communication\nnetwork, where up to t processes may crash. Our result thus generalizes the\nt/k+1 lower bound for complete networks in the t-resilient model by Chaudhuri,\nHerlihy, Lynch, and Tuttle [JACM'00]. Moreover, it generalizes two lower bounds\nfor oblivious algorithms in synchronous systems connected by an arbitrary\nundirected communication network known to the processes, namely, the domination\nnumber-based lower bound by Castaneda, Fraigniaud, Paz, Rajsbaum, Roy, and\nTravers [TCS'21] for failure-free processes, and the radius-based lower bound\nin the t-resilient model by Fraigniaud, Nguyen, and Paz [STACS'24].\n  Our topological proof non-trivially generalizes and extends the\nconnectivity-based approach for the complete network, as presented in the book\nby Herlihy, Kozlov, and Rajsbaum (2013). It is based on a sequence of shellable\ncarrier maps that, starting from a shellable input complex, determine the\nevolution of the protocol complex: During the first t/k rounds, carrier maps\nthat crash exactly k processes per round are used, ensuring high connectivity\nof their images. A Sperner's lemma style argument is used to prove that k-set\nagreement is still impossible by that round. From round t/k+1 up to our lower\nbound, we employ a novel carrier map that maintains high connectivity. Our\nproof also provides a strikingly simple lower bound for k-set agreement in\nsynchronous systems with an arbitrary communication network with initial\ncrashes. We express the resulting additional agreement overhead via an\nappropriately defined radius of the communication graphs. Finally, we prove\nthat the usual input pseudosphere complex for k-set agreement can be replaced\nby an exponentially smaller input complex based on Kuhn triangulations, which\nwe prove to be also shellable."}
{"id": "2508.14899", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14899", "abs": "https://arxiv.org/abs/2508.14899", "authors": ["Adeel Ahmad", "Ahmad Tameem Kamal", "Nouman Amir", "Bilal Zafar", "Saad Bin Nasir"], "title": "Accelerating GenAI Workloads by Enabling RISC-V Microkernel Support in IREE", "comment": null, "summary": "This project enables RISC-V microkernel support in IREE, an MLIR-based\nmachine learning compiler and runtime. The approach begins by enabling the\nlowering of MLIR linalg dialect contraction ops to linalg.mmt4d op for the\nRISC-V64 target within the IREE pass pipeline, followed by the development of\noptimized microkernels for RISC-V. The performance gains are compared with\nupstream IREE and Llama.cpp for the Llama-3.2-1B-Instruct model."}
{"id": "2508.15109", "categories": ["cs.PL", "D.3.0; F.3.1"], "pdf": "https://arxiv.org/pdf/2508.15109", "abs": "https://arxiv.org/abs/2508.15109", "authors": ["Ziteng Wang", "Ruijie Fang", "Linus Zheng", "Dixin Tang", "Isil Dillig"], "title": "Homomorphism Calculus for User-Defined Aggregations", "comment": null, "summary": "Data processing frameworks like Apache Spark and Flink provide built-in\nsupport for user-defined aggregation functions (UDAFs), enabling the\nintegration of domain-specific logic. However, for these frameworks to support\n\\emph{efficient} UDAF execution, the function needs to satisfy a\n\\emph{homomorphism property}, which ensures that partial results from\nindependent computations can be merged correctly. Motivated by this problem,\nthis paper introduces a novel \\emph{homomorphism calculus} that can both verify\nand refute whether a UDAF is a dataframe homomorphism. If so, our calculus also\nenables the construction of a corresponding merge operator which can be used\nfor incremental computation and parallel execution. We have implemented an\nalgorithm based on our proposed calculus and evaluate it on real-world UDAFs,\ndemonstrating that our approach significantly outperforms two leading\nsynthesizers."}
{"id": "2508.15601", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.15601", "abs": "https://arxiv.org/abs/2508.15601", "authors": ["Li Zhang", "Youhe Jiang", "Guoliang He", "Xin Chen", "Han Lv", "Qian Yao", "Fangcheng Fu", "Kai Chen"], "title": "Efficient Mixed-Precision Large Language Model Inference with TurboMind", "comment": null, "summary": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy."}
{"id": "2508.14907", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.14907", "abs": "https://arxiv.org/abs/2508.14907", "authors": ["Lukas Krupp", "Ian O'Connor", "Luca Benini", "Christoph Studer", "Joachim Rodrigues", "Norbert Wehn"], "title": "Improving Chip Design Enablement for Universities in Europe -- A Position Paper", "comment": "Published in the proceedings of the 2025 Design, Automation & Test in\n  Europe Conference (DATE)", "summary": "The semiconductor industry is pivotal to Europe's economy, especially within\nthe industrial and automotive sectors. However, Europe faces a significant\nshortfall in chip design capabilities, marked by a severe skilled labor\nshortage and lagging contributions in the design value chain segment. This\npaper explores the role of European universities and academic initiatives in\nenhancing chip design education and research to address these deficits. We\nprovide a comprehensive overview of current European chip design initiatives,\nanalyze major challenges in recruitment, productivity, technology access, and\ndesign enablement, and identify strategic opportunities to strengthen chip\ndesign capabilities within academic institutions. Our analysis leads to a\nseries of recommendations that highlight the need for coordinated efforts and\nstrategic investments to overcome these challenges."}
{"id": "2508.15137", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15137", "abs": "https://arxiv.org/abs/2508.15137", "authors": ["Ruijie Fang", "Zachary Kincaid", "Thomas Reps"], "title": "Software Model Checking via Summary-Guided Search (Extended Version)", "comment": "Preliminary manuscript of extended version of paper that will appear\n  in OOPSLA 2025. 36 pages", "summary": "In this work, we describe a new software model-checking algorithm called GPS.\nGPS treats the task of model checking a program as a directed search of the\nprogram states, guided by a compositional, summary-based static analysis. The\nsummaries produced by static analysis are used both to prune away infeasible\npaths and to drive test generation to reach new, unexplored program states. GPS\ncan find both proofs of safety and counter-examples to safety (i.e., inputs\nthat trigger bugs), and features a novel two-layered search strategy that\nrenders it particularly efficient at finding bugs in programs featuring long,\ninput-dependent error paths. To make GPS refutationally complete (in the sense\nthat it will find an error if one exists, if it is allotted enough time), we\nintroduce an instrumentation technique and show that it helps GPS achieve\nrefutation-completeness without sacrificing overall performance. We benchmarked\nGPS on a suite of benchmarks including both programs from the Software\nVerification Competition (SV-COMP) and from prior literature, and found that\nour implementation of GPS outperforms state-of-the-art software model checkers\n(including the top performers in SV-COMP ReachSafety-Loops category), both in\nterms of the number of benchmarks solved and in terms of running time."}
{"id": "2508.15647", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.15647", "abs": "https://arxiv.org/abs/2508.15647", "authors": ["Haoran Zhang", "Zihao Zhang", "Shuai Mu", "Sebastian Angel", "Vincent Liu"], "title": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless Computing", "comment": "Extended version from PVLDB Volume 17, Issue 13, 2024. This version\n  includes full proofs and formal verification in Dafny and fixes some small\n  bugs", "summary": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals"}
{"id": "2508.14917", "categories": ["cs.AR", "cs.CV", "cs.DC", "eess.IV", "eess.SP", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2508.14917", "abs": "https://arxiv.org/abs/2508.14917", "authors": ["Weichien Liao"], "title": "Scalable FPGA Framework for Real-Time Denoising in High-Throughput Imaging: A DRAM-Optimized Pipeline using High-Level Synthesis", "comment": "FPGA-based denoising pipeline for PRISM-scale imaging. Real-time\n  frame subtraction and averaging via burst-mode AXI4 and DRAM buffering.\n  Benchmarked against CPU/GPU workflows; scalable across multi-bank FPGA setups", "summary": "High-throughput imaging workflows, such as Parallel Rapid Imaging with\nSpectroscopic Mapping (PRISM), generate data at rates that exceed conventional\nreal-time processing capabilities. We present a scalable FPGA-based\npreprocessing pipeline for real-time denoising, implemented via High-Level\nSynthesis (HLS) and optimized for DRAM-backed buffering. Our architecture\nperforms frame subtraction and averaging directly on streamed image data,\nminimizing latency through burst-mode AXI4 interfaces. The resulting kernel\noperates below the inter-frame interval, enabling inline denoising and reducing\ndataset size for downstream CPU/GPU analysis. Validated under PRISM-scale\nacquisition, this modular FPGA framework offers a practical solution for\nlatency-sensitive imaging workflows in spectroscopy and microscopy."}
{"id": "2508.15157", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15157", "abs": "https://arxiv.org/abs/2508.15157", "authors": ["David M Kahn", "Jan Hoffmann", "Runming Li"], "title": "Big-Stop Semantics: A Simple Way to Get the Benefits of Small-Step Semantics in a Big-Step Judgment", "comment": "26 pages, 27 figures", "summary": "As evident in the programming language literature, many practitioners favor\nspecifying dynamic program behavior using big-step over small-step semantics.\nUnlike small-step semantics, which must dwell on every intermediate program\nstate, big-step semantics conveniently jump directly to the ever-important\nresult of the computation. Big-step semantics also typically involve fewer\ninference rules than their small-step counterparts. However, in exchange for\nergonomics, big-step semantics give up power: Small-step semantics describes\nprogram behaviors that are outside the grasp of big-step semantics, notably\ndivergence. This work presents a little-known extension of big-step semantics\nwith inductive definitions that captures diverging computations without\nintroducing error states. This big-stop semantics is illustrated for typed,\nuntyped, and effectful variants of PCF, as well as a while-loop-based\nimperative language. Big-stop semantics extends the standard big-step inference\nrules with a few additional rules to define an evaluation judgment that is\nequivalent to the reflexive-transitive closure of small-step transitions. This\nsimple extension contrasts with other solutions in the literature which\nsacrifice ergonomics by introducing many additional inference rules, global\nstate, and/or less-commonly-understood reasoning principles like coinduction."}
{"id": "2508.14917", "categories": ["cs.AR", "cs.CV", "cs.DC", "eess.IV", "eess.SP", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2508.14917", "abs": "https://arxiv.org/abs/2508.14917", "authors": ["Weichien Liao"], "title": "Scalable FPGA Framework for Real-Time Denoising in High-Throughput Imaging: A DRAM-Optimized Pipeline using High-Level Synthesis", "comment": "FPGA-based denoising pipeline for PRISM-scale imaging. Real-time\n  frame subtraction and averaging via burst-mode AXI4 and DRAM buffering.\n  Benchmarked against CPU/GPU workflows; scalable across multi-bank FPGA setups", "summary": "High-throughput imaging workflows, such as Parallel Rapid Imaging with\nSpectroscopic Mapping (PRISM), generate data at rates that exceed conventional\nreal-time processing capabilities. We present a scalable FPGA-based\npreprocessing pipeline for real-time denoising, implemented via High-Level\nSynthesis (HLS) and optimized for DRAM-backed buffering. Our architecture\nperforms frame subtraction and averaging directly on streamed image data,\nminimizing latency through burst-mode AXI4 interfaces. The resulting kernel\noperates below the inter-frame interval, enabling inline denoising and reducing\ndataset size for downstream CPU/GPU analysis. Validated under PRISM-scale\nacquisition, this modular FPGA framework offers a practical solution for\nlatency-sensitive imaging workflows in spectroscopy and microscopy."}
{"id": "2508.15685", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15685", "abs": "https://arxiv.org/abs/2508.15685", "authors": ["Kang Eun Jeon", "Sangheum Yeon", "Jinhee Kim", "Hyeonsu Bang", "Johnny Rhe", "Jong Hwan Ko"], "title": "Row-Column Hybrid Grouping for Fault-Resilient Multi-Bit Weight Representation on IMC Arrays", "comment": "Accepted to appear at ICCAD'25 (Munich, Germany)", "summary": "This paper addresses two critical challenges in analog In-Memory Computing\n(IMC) systems that limit their scalability and deployability: the computational\nunreliability caused by stuck-at faults (SAFs) and the high compilation\noverhead of existing fault-mitigation algorithms, namely Fault-Free (FF). To\novercome these limitations, we first propose a novel multi-bit weight\nrepresentation technique, termed row-column hybrid grouping, which generalizes\nconventional column grouping by introducing redundancy across both rows and\ncolumns. This structural redundancy enhances fault tolerance and can be\neffectively combined with existing fault-mitigation solutions. Second, we\ndesign a compiler pipeline that reformulates the fault-aware weight\ndecomposition problem as an Integer Linear Programming (ILP) task, enabling\nfast and scalable compilation through off-the-shelf solvers. Further\nacceleration is achieved through theoretical insights that identify fault\npatterns amenable to trivial solutions, significantly reducing computation.\nExperimental results on convolutional networks and small language models\ndemonstrate the effectiveness of our approach, achieving up to 8%p improvement\nin accuracy, 150x faster compilation, and 2x energy efficiency gain compared to\nexisting baselines."}
{"id": "2508.15166", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15166", "abs": "https://arxiv.org/abs/2508.15166", "authors": ["Jingbo Wang", "Shashin Halalingaiah", "Weiyi Chen", "Chao Wang", "Isil Dillig"], "title": "Probabilistic Inference for Datalog with Correlated Inputs", "comment": "Accepted for publication at OOPSLA 2025 (R2)", "summary": "Probabilistic extensions of logic programming languages, such as ProbLog,\nintegrate logical reasoning with probabilistic inference to evaluate\nprobabilities of output relations; however, prior work does not account for\npotential statistical correlations among input facts. This paper introduces\nPraline, a new extension to Datalog designed for precise probabilistic\ninference in the presence of (partially known) input correlations. We formulate\nthe inference task as a constrained optimization problem, where the solution\nyields sound and precise probability bounds for output facts. However, due to\nthe complexity of the resulting optimization problem, this approach alone often\ndoes not scale to large programs. To address scalability, we propose a more\nefficient $\\delta$-exact inference algorithm that leverages constraint solving,\nstatic analysis, and iterative refinement. Our empirical evaluation on\nchallenging real-world benchmarks, including side-channel analysis,\ndemonstrates that our method not only scales effectively but also delivers\ntight probability bounds."}
{"id": "2508.15264", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15264", "abs": "https://arxiv.org/abs/2508.15264", "authors": ["Patrick Redmond", "Jonathan Castello", "José Manuel Calderón Trilla", "Lindsey Kuper"], "title": "Exploring the Theory and Practice of Concurrency in the Entity-Component-System Pattern", "comment": "This is an extended version (with appendices) of the OOPSLA 2025\n  paper", "summary": "The Entity-Component-System (ECS) software design pattern, long used in game\ndevelopment, encourages a clean separation of identity (entities), data\nproperties (components), and computational behaviors (systems). Programs\nwritten using the ECS pattern are naturally concurrent, and the pattern offers\nmodularity, flexibility, and performance benefits that have led to a\nproliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known\nand not well understood outside of a few domains. Existing explanations of the\nECS pattern tend to be mired in the concrete details of particular ECS\nframeworks, or they explain the pattern in terms of imperfect metaphors or in\nterms of what it is not. We seek a rigorous understanding of the ECS pattern\nvia the design of a formal model, Core ECS, that abstracts away the details of\nspecific implementations to reveal the essence of software using the ECS\npattern. We identify a class of Core ECS programs that behave deterministically\nregardless of scheduling, enabling use of the ECS pattern as a\ndeterministic-by-construction concurrent programming model. With Core ECS as a\npoint of comparison, we then survey several real-world ECS frameworks and find\nthat they all leave opportunities for deterministic concurrency unexploited.\nOur findings point out a space for new ECS implementation techniques that\nbetter leverage such opportunities."}
{"id": "2508.15333", "categories": ["cs.PL", "F.3.3"], "pdf": "https://arxiv.org/pdf/2508.15333", "abs": "https://arxiv.org/abs/2508.15333", "authors": ["Francesco Dagnino", "Paola Giannini", "Violet Ka I Pun", "Ulises Torrella"], "title": "Fair Termination for Resource-Aware Active Objects", "comment": "18 pages, 12 pages of appendix, 12 figures, APLAS 2025", "summary": "Active object systems are a model of distributed computation that has been\nadopted for modelling distributed systems and business process workflows. This\nfield of modelling is, in essence, concurrent and resource-aware, motivating\nthe development of resource-aware formalisations on the active object model.\nThe contributions of this work are the development of a core calculus for\nresource-aware active objects together with a type system ensuring that\nwell-typed programs are fairly terminating, i.e., they can always eventually\nterminate. To achieve this, we combine techniques from graded semantics and\ntype systems, which are quite well understood for sequential programs, with\nthose for fair termination, which have been developed for synchronous~sessions."}
{"id": "2508.15576", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15576", "abs": "https://arxiv.org/abs/2508.15576", "authors": ["Andreas Lööw", "Seung Hoon Park", "Daniele Nantes-Sobrinho", "Sacha-Élie Ayoun", "Opale Sjöstedt", "Philippa Gardner"], "title": "Compositional Symbolic Execution for the Next 700 Memory Models (Extended Version)", "comment": null, "summary": "Multiple successful compositional symbolic execution (CSE) tools and\nplatforms exploit separation logic (SL) for compositional verification and/or\nincorrectness separation logic (ISL) for compositional bug-finding, including\nVeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian\nplatform, the only CSE platform that is parametric on the memory model, meaning\nthat it can be instantiated to different memory models, suggests that the\nability to use custom memory models allows for more flexibility in supporting\nanalysis of a wide range of programming languages, for implementing custom\nautomation, and for improving performance. However, the literature lacks a\nsatisfactory formal foundation for memory-model-parametric CSE platforms.\n  In this paper, inspired by Gillian, we provide a new formal foundation for\nmemory-model-parametric CSE platforms. Our foundation advances the state of the\nart in four ways. First, we mechanise our foundation (in the interactive\ntheorem prover Rocq). Second, we validate our foundation by instantiating it to\na broad range of memory models, including models for C and CHERI. Third,\nwhereas previous memory-model-parametric work has only covered SL analyses, we\ncover both SL and ISL analyses. Fourth, our foundation is based on standard\ndefinitions of SL and ISL (including definitions of function specification\nvalidity, to ensure sound interoperation with other tools and platforms also\nbased on standard definitions)."}
{"id": "2508.15750", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.15750", "abs": "https://arxiv.org/abs/2508.15750", "authors": ["Celeste Barnaby", "Qiaochu Chen", "Ramya Ramalingam", "Osbert Bastani", "Isil Dillig"], "title": "Active Learning for Neurosymbolic Program Synthesis", "comment": null, "summary": "The goal of active learning for program synthesis is to synthesize the\ndesired program by asking targeted questions that minimize user interaction.\nWhile prior work has explored active learning in the purely symbolic setting,\nsuch techniques are inadequate for the increasingly popular paradigm of\nneurosymbolic program synthesis, where the synthesized program incorporates\nneural components. When applied to the neurosymbolic setting, such techniques\ncan -- and, in practice, do -- return an unintended program due to\nmispredictions of neural components. This paper proposes a new active learning\ntechnique that can handle the unique challenges posed by neural network\nmispredictions. Our approach is based upon a new evaluation strategy called\nconstrained conformal evaluation (CCE), which accounts for neural\nmispredictions while taking into account user-provided feedback. Our proposed\nmethod iteratively makes CCE more precise until all remaining programs are\nguaranteed to be observationally equivalent. We have implemented this method in\na tool called SmartLabel and experimentally evaluated it on three neurosymbolic\ndomains. Our results demonstrate that SmartLabel identifies the ground truth\nprogram for 98% of the benchmarks, requiring under 5 rounds of user interaction\non average. In contrast, prior techniques for active learning are only able to\nconverge to the ground truth program for at most 65% of the benchmarks."}
