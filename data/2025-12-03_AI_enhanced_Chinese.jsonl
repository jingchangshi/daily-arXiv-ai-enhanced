{"id": "2512.02278", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.02278", "abs": "https://arxiv.org/abs/2512.02278", "authors": ["Yi Liu", "Chen Qian"], "title": "Fantasy: Efficient Large-scale Vector Search on GPU Clusters with GPUDirect Async", "comment": null, "summary": "Vector similarity search has become a critical component in AI-driven applications such as large language models (LLMs). To achieve high recall and low latency, GPUs are utilized to exploit massive parallelism for faster query processing. However, as the number of vectors continues to grow, the graph size quickly exceeds the memory capacity of a single GPU, making it infeasible to store and process the entire index on a single GPU. Recent work uses CPU-GPU architectures to keep vectors in CPU memory or SSDs, but the loading step stalls GPU computation. We present Fantasy, an efficient system that pipelines vector search and data transfer in a GPU cluster with GPUDirect Async. Fantasy overlaps computation and network communication to significantly improve search throughput for large graphs and deliver large query batch sizes.", "AI": {"tldr": "Fantasy\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a21\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u7684GPU\u96c6\u7fa4\u7cfb\u7edf\uff0c\u901a\u8fc7GPUDirect Async\u6280\u672f\u5b9e\u73b0\u8ba1\u7b97\u4e0e\u7f51\u7edc\u901a\u4fe1\u7684\u6d41\u6c34\u7ebf\u5316\uff0c\u89e3\u51b3\u5355GPU\u5185\u5b58\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u5347\u641c\u7d22\u541e\u5410\u91cf\u3002", "motivation": "\u968f\u7740AI\u5e94\u7528\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\uff09\u4e2d\u5411\u91cf\u6570\u91cf\u6301\u7eed\u589e\u957f\uff0c\u56fe\u7d22\u5f15\u5927\u5c0f\u8fc5\u901f\u8d85\u8fc7\u5355GPU\u5185\u5b58\u5bb9\u91cf\uff0c\u65e0\u6cd5\u5728\u5355GPU\u4e0a\u5b58\u50a8\u548c\u5904\u7406\u6574\u4e2a\u7d22\u5f15\u3002\u73b0\u6709CPU-GPU\u67b6\u6784\u4e2d\u6570\u636e\u52a0\u8f7d\u6b65\u9aa4\u4f1a\u963b\u585eGPU\u8ba1\u7b97\uff0c\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u63d0\u51faFantasy\u7cfb\u7edf\uff0c\u5728GPU\u96c6\u7fa4\u4e2d\u5229\u7528GPUDirect Async\u6280\u672f\u5b9e\u73b0\u5411\u91cf\u641c\u7d22\u548c\u6570\u636e\u4f20\u8f93\u7684\u6d41\u6c34\u7ebf\u5316\uff0c\u91cd\u53e0\u8ba1\u7b97\u548c\u7f51\u7edc\u901a\u4fe1\uff0c\u652f\u6301\u5927\u89c4\u6a21\u56fe\u7d22\u5f15\u548c\u5927\u91cf\u67e5\u8be2\u6279\u5904\u7406\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u56fe\u7d22\u5f15\u7684\u641c\u7d22\u541e\u5410\u91cf\uff0c\u80fd\u591f\u5904\u7406\u66f4\u5927\u7684\u67e5\u8be2\u6279\u5927\u5c0f\uff0c\u89e3\u51b3\u4e86\u5355GPU\u5185\u5b58\u9650\u5236\u95ee\u9898\u3002", "conclusion": "Fantasy\u901a\u8fc7GPU\u96c6\u7fa4\u4e2d\u7684\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\u673a\u5236\uff0c\u4e3a\u5927\u89c4\u6a21\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u5355GPU\u5185\u5b58\u9650\u5236\u548c\u73b0\u6709CPU-GPU\u67b6\u6784\u7684\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2512.02300", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.02300", "abs": "https://arxiv.org/abs/2512.02300", "authors": ["Haoyu Zheng", "Shouwei Gao", "Jie Ren", "Wenqian Dong"], "title": "DOLMA: A Data Object Level Memory Disaggregation Framework for HPC Applications", "comment": null, "summary": "Memory disaggregation is promising to scale memory capacity and improves utilization in HPC systems. However, the performance overhead of accessing remote memory poses a significant challenge, particularly for compute-intensive HPC applications where execution times are highly sensitive to data locality. In this work, we present DOLMA, a Data Object Level M emory dis Aggregation framework designed for HPC applications. DOLMA intelligently identifies and offloads data objects to remote memory, while providing quantitative analysis to decide a suitable local memory size. Furthermore, DOLMA leverages the predictable memory access patterns typical in HPC applications and enables remote memory prefetch via a dual-buffer design. By carefully balancing local and remote memory usage and maintaining multi-thread concurrency, DOLMA provides a flexible and efficient solution for leveraging disaggregated memory in HPC domains while minimally compromising application performance. Evaluating with eight HPC workloads and computational kernels, DOLMA limits performance degradation to less than 16% while reducing local memory usage by up to 63%, on average.", "AI": {"tldr": "DOLMA\u662f\u4e00\u4e2a\u9762\u5411HPC\u5e94\u7528\u7684\u6570\u636e\u5bf9\u8c61\u7ea7\u5185\u5b58\u89e3\u805a\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u8bc6\u522b\u548c\u5378\u8f7d\u6570\u636e\u5bf9\u8c61\u5230\u8fdc\u7a0b\u5185\u5b58\uff0c\u5728\u5e73\u5747\u51cf\u5c1163%\u672c\u5730\u5185\u5b58\u4f7f\u7528\u7684\u540c\u65f6\uff0c\u5c06\u6027\u80fd\u4e0b\u964d\u63a7\u5236\u572816%\u4ee5\u5185\u3002", "motivation": "\u5185\u5b58\u89e3\u805a\u6280\u672f\u6709\u671b\u6269\u5c55HPC\u7cfb\u7edf\u7684\u5185\u5b58\u5bb9\u91cf\u5e76\u63d0\u9ad8\u5229\u7528\u7387\uff0c\u4f46\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u7684\u6027\u80fd\u5f00\u9500\u5bf9\u8ba1\u7b97\u5bc6\u96c6\u578bHPC\u5e94\u7528\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u8fd9\u7c7b\u5e94\u7528\u7684\u6267\u884c\u65f6\u95f4\u5bf9\u6570\u636e\u5c40\u90e8\u6027\u9ad8\u5ea6\u654f\u611f\u3002", "method": "DOLMA\u6846\u67b6\u667a\u80fd\u8bc6\u522b\u5e76\u5c06\u6570\u636e\u5bf9\u8c61\u5378\u8f7d\u5230\u8fdc\u7a0b\u5185\u5b58\uff0c\u63d0\u4f9b\u5b9a\u91cf\u5206\u6790\u4ee5\u786e\u5b9a\u5408\u9002\u7684\u672c\u5730\u5185\u5b58\u5927\u5c0f\u3002\u5229\u7528HPC\u5e94\u7528\u5178\u578b\u53ef\u9884\u6d4b\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\uff0c\u901a\u8fc7\u53cc\u7f13\u51b2\u533a\u8bbe\u8ba1\u5b9e\u73b0\u8fdc\u7a0b\u5185\u5b58\u9884\u53d6\uff0c\u5e76\u4ed4\u7ec6\u5e73\u8861\u672c\u5730\u548c\u8fdc\u7a0b\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u7ebf\u7a0b\u5e76\u53d1\u6027\u3002", "result": "\u5728\u516b\u4e2aHPC\u5de5\u4f5c\u8d1f\u8f7d\u548c\u8ba1\u7b97\u5185\u6838\u7684\u8bc4\u4f30\u4e2d\uff0cDOLMA\u5c06\u6027\u80fd\u4e0b\u964d\u9650\u5236\u572816%\u4ee5\u5185\uff0c\u540c\u65f6\u5c06\u672c\u5730\u5185\u5b58\u4f7f\u7528\u91cf\u5e73\u5747\u51cf\u5c11\u4e8663%\u3002", "conclusion": "DOLMA\u4e3aHPC\u9886\u57df\u5229\u7528\u89e3\u805a\u5185\u5b58\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6700\u5c0f\u5316\u5f71\u54cd\u5e94\u7528\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u672c\u5730\u5185\u5b58\u4f7f\u7528\u3002"}}
{"id": "2512.02546", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.02546", "abs": "https://arxiv.org/abs/2512.02546", "authors": ["Jan Meizner", "Maciej Malawski"], "title": "Solutions for Distributed Memory Access Mechanism on HPC Clusters", "comment": null, "summary": "Paper presents and evaluates various mechanisms for remote access to memory in distributed systems based on two distinct HPC clusters. We are comparing solutions based on the shared storage and MPI (over Infiniband and Slingshot) to the local memory access. This paper also mentions medical use-cases that would mostly benefit from the described solution. We have found out that results for remote access esp. backed by MPI are similar to local memory access.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u57fa\u4e8e\u4e24\u4e2aHPC\u96c6\u7fa4\u7684\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u673a\u5236\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u5171\u4eab\u5b58\u50a8\u548cMPI\uff08\u901a\u8fc7Infiniband\u548cSlingshot\uff09\u7684\u89e3\u51b3\u65b9\u6848\u4e0e\u672c\u5730\u5185\u5b58\u8bbf\u95ee\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u57fa\u4e8eMPI\u7684\u8fdc\u7a0b\u8bbf\u95ee\u7ed3\u679c\u4e0e\u672c\u5730\u5185\u5b58\u8bbf\u95ee\u76f8\u4f3c\u3002", "motivation": "\u7814\u7a76\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u673a\u5236\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728HPC\u96c6\u7fa4\u73af\u5883\u4e0b\uff0c\u4e3a\u533b\u7597\u7b49\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u9ad8\u6548\u7684\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u4e24\u4e2a\u4e0d\u540c\u7684HPC\u96c6\u7fa4\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u5185\u5b58\u8bbf\u95ee\u673a\u5236\uff1a1) \u57fa\u4e8e\u5171\u4eab\u5b58\u50a8\u7684\u8fdc\u7a0b\u8bbf\u95ee\uff0c2) \u57fa\u4e8eMPI\uff08\u901a\u8fc7Infiniband\u548cSlingshot\u7f51\u7edc\uff09\u7684\u8fdc\u7a0b\u8bbf\u95ee\uff0c3) \u672c\u5730\u5185\u5b58\u8bbf\u95ee\u4f5c\u4e3a\u57fa\u51c6\u5bf9\u6bd4\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7279\u522b\u662f\u57fa\u4e8eMPI\u7684\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u6027\u80fd\u4e0e\u672c\u5730\u5185\u5b58\u8bbf\u95ee\u76f8\u4f3c\uff0c\u8868\u660e\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u53ef\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u672c\u5730\u6027\u80fd\u7684\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u3002", "conclusion": "\u57fa\u4e8eMPI\u7684\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u673a\u5236\u5728HPC\u96c6\u7fa4\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u63a5\u8fd1\u672c\u5730\u8bbf\u95ee\uff0c\u7279\u522b\u9002\u7528\u4e8e\u533b\u7597\u7b49\u9700\u8981\u9ad8\u6548\u8fdc\u7a0b\u6570\u636e\u8bbf\u95ee\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.02646", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.02646", "abs": "https://arxiv.org/abs/2512.02646", "authors": ["Alex Barcel\u00f3", "Sebasti\u00e1n A. Cajas Ordo\u00f1ez", "Jaydeep Samanta", "Andr\u00e9s L. Su\u00e1rez-Cetrulo", "Romila Ghosh", "Ricardo Sim\u00f3n Carbajo", "Anna Queralt"], "title": "Offloading Artificial Intelligence Workloads across the Computing Continuum by means of Active Storage Systems", "comment": "17 pages, 7 tables, 12 figures", "summary": "The increasing demand for artificial intelligence (AI) workloads across diverse computing environments has driven the need for more efficient data management strategies. Traditional cloud-based architectures struggle to handle the sheer volume and velocity of AI-driven data, leading to inefficiencies in storage, computation, and data movement. This paper explores the integration of active storage systems within the computing continuum to optimize AI workload distribution.\n  By embedding computation directly into storage architectures, active storage is able to reduce data transfer overhead, enhancing performance and improving resource utilization. Other existing frameworks and architectures offer mechanisms to distribute certain AI processes across distributed environments; however, they lack the flexibility and adaptability that the continuum requires, both regarding the heterogeneity of devices and the rapid-changing algorithms and models being used by domain experts and researchers.\n  This article proposes a software architecture aimed at seamlessly distributing AI workloads across the computing continuum, and presents its implementation using mainstream Python libraries and dataClay, an active storage platform. The evaluation shows the benefits and trade-offs regarding memory consumption, storage requirements, training times, and execution efficiency across different devices. Experimental results demonstrate that the process of offloading workloads through active storage significantly improves memory efficiency and training speeds while maintaining accuracy. Our findings highlight the potential of active storage to revolutionize AI workload management, making distributed AI deployments more scalable and resource-efficient with a very low entry barrier for domain experts and application developers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b58\u50a8\u7684\u8ba1\u7b97\u8fde\u7eed\u4f53\u8f6f\u4ef6\u67b6\u6784\uff0c\u7528\u4e8e\u4f18\u5316AI\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\uff0c\u901a\u8fc7\u5c06\u8ba1\u7b97\u5d4c\u5165\u5b58\u50a8\u67b6\u6784\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u5f00\u9500\uff0c\u63d0\u9ad8\u5185\u5b58\u6548\u7387\u548c\u8bad\u7ec3\u901f\u5ea6\u3002", "motivation": "\u968f\u7740AI\u5de5\u4f5c\u8d1f\u8f7d\u5728\u591a\u6837\u5316\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u9700\u6c42\u589e\u957f\uff0c\u4f20\u7edf\u4e91\u67b6\u6784\u5728\u5904\u7406AI\u9a71\u52a8\u6570\u636e\u65f6\u9762\u4e34\u5b58\u50a8\u3001\u8ba1\u7b97\u548c\u6570\u636e\u79fb\u52a8\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002\u73b0\u6709\u6846\u67b6\u7f3a\u4e4f\u9002\u5e94\u8ba1\u7b97\u8fde\u7eed\u4f53\u6240\u9700\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\uff0c\u65e0\u6cd5\u5e94\u5bf9\u8bbe\u5907\u5f02\u6784\u6027\u548c\u5feb\u901f\u53d8\u5316\u7684\u7b97\u6cd5\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8f6f\u4ef6\u67b6\u6784\uff0c\u5229\u7528\u4e3b\u6d41Python\u5e93\u548cactive storage\u5e73\u53f0dataClay\uff0c\u5c06AI\u5de5\u4f5c\u8d1f\u8f7d\u65e0\u7f1d\u5206\u5e03\u5230\u8ba1\u7b97\u8fde\u7eed\u4f53\u4e2d\u3002\u901a\u8fc7\u5c06\u8ba1\u7b97\u5d4c\u5165\u5b58\u50a8\u67b6\u6784\uff08\u4e3b\u52a8\u5b58\u50a8\uff09\uff0c\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u5f00\u9500\uff0c\u4f18\u5316\u8d44\u6e90\u5229\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u901a\u8fc7\u4e3b\u52a8\u5b58\u50a8\u5378\u8f7d\u5de5\u4f5c\u8d1f\u8f7d\u663e\u8457\u63d0\u9ad8\u4e86\u5185\u5b58\u6548\u7387\uff08\u51cf\u5c11\u5185\u5b58\u6d88\u8017\uff09\u548c\u8bad\u7ec3\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002\u5728\u4e0d\u540c\u8bbe\u5907\u4e0a\u5c55\u793a\u4e86\u5185\u5b58\u6d88\u8017\u3001\u5b58\u50a8\u9700\u6c42\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u6267\u884c\u6548\u7387\u65b9\u9762\u7684\u4f18\u52bf\u4e0e\u6743\u8861\u3002", "conclusion": "\u4e3b\u52a8\u5b58\u50a8\u6709\u6f5c\u529b\u5f7b\u5e95\u6539\u53d8AI\u5de5\u4f5c\u8d1f\u8f7d\u7ba1\u7406\uff0c\u4f7f\u5206\u5e03\u5f0fAI\u90e8\u7f72\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u8d44\u6e90\u6548\u7387\uff0c\u540c\u65f6\u4e3a\u9886\u57df\u4e13\u5bb6\u548c\u5e94\u7528\u5f00\u53d1\u8005\u63d0\u4f9b\u6781\u4f4e\u7684\u5b66\u4e60\u95e8\u69db\u3002\u8be5\u65b9\u6cd5\u4e3a\u5904\u7406AI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5f02\u6784\u5206\u5e03\u5f0f\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.02371", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.02371", "abs": "https://arxiv.org/abs/2512.02371", "authors": ["Yihong Zhang", "Derek Gerstmann", "Andrew Adams", "Maaz Bin Safeer Ahmad"], "title": "Pushing Tensor Accelerators Beyond MatMul in a User-Schedulable Language", "comment": "CGO 2026", "summary": "Tensor accelerators now represent a growing share of compute resources in modern CPUs and GPUs. However, they are hard to program, leading developers to use vendor-provided kernel libraries that support tensor accelerators. As a result, the usage of tensor accelerators is limited to the provided interface, mainly designed for traditional ML and scientific computing workloads.\n  In this paper, we show that tensor accelerators can improve the performance of applications beyond simple variants of MatMul. For example, many image processing pipelines are linear transformations over matrices in disguise and can therefore utilize such specialized hardware. This is nonetheless hindered by the difficulties in programming tensor accelerators. We tackle this problem with compiler-based techniques. We use the Halide user-schedulable language and express operations as Halide algorithms succinctly. To this end, we implement a flexible tensor instruction selector based on equality saturation. The tensor instruction selector supports both CPU- and GPU-attached tensor accelerators and works with existing scheduling operations (e.g., producer-consumer fusion). Together, this enables developers to write diverse accelerator-leveraging applications in a few dozen lines.\n  Using our system, we demonstrate the potential of tensor accelerators beyond their traditional domains. We implement several image processing pipelines (e.g., filtering, resampling, and denoising) in our system and evaluate them against non-accelerator-leveraging baselines. We show that these pipelines can achieve significant speedups. For example, a downsampling routine is sped up by $6.1\\times$ by utilizing Tensor Cores on an Nvidia RTX 4070 GPU.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7f16\u8bd1\u5668\u7684\u6280\u672f\uff0c\u4f7f\u7528Halide\u8bed\u8a00\u548c\u7b49\u5f0f\u9971\u548c\u7684\u6307\u4ee4\u9009\u62e9\u5668\uff0c\u8ba9\u5f20\u91cf\u52a0\u901f\u5668\u80fd\u5e94\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u7b49\u4f20\u7edfML\u4e4b\u5916\u7684\u5e94\u7528\uff0c\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f20\u91cf\u52a0\u901f\u5668\u5728\u73b0\u4ee3CPU/GPU\u4e2d\u5360\u6bd4\u589e\u52a0\uff0c\u4f46\u96be\u4ee5\u7f16\u7a0b\uff0c\u53ea\u80fd\u901a\u8fc7\u5382\u5546\u63d0\u4f9b\u7684\u6838\u5e93\u4f7f\u7528\uff0c\u9650\u5236\u4e86\u5176\u5728\u4f20\u7edfML\u548c\u79d1\u5b66\u8ba1\u7b97\u4e4b\u5916\u7684\u5e94\u7528\u3002\u56fe\u50cf\u5904\u7406\u7b49\u5e94\u7528\u672c\u8d28\u4e0a\u662f\u77e9\u9635\u7ebf\u6027\u53d8\u6362\uff0c\u7406\u8bba\u4e0a\u53ef\u4ee5\u5229\u7528\u5f20\u91cf\u52a0\u901f\u5668\uff0c\u4f46\u7f16\u7a0b\u56f0\u96be\u963b\u788d\u4e86\u8fd9\u79cd\u5e94\u7528\u3002", "method": "\u4f7f\u7528Halide\u7528\u6237\u53ef\u8c03\u5ea6\u8bed\u8a00\u7b80\u6d01\u8868\u8fbe\u7b97\u6cd5\uff0c\u5b9e\u73b0\u57fa\u4e8e\u7b49\u5f0f\u9971\u548c\u7684\u7075\u6d3b\u5f20\u91cf\u6307\u4ee4\u9009\u62e9\u5668\uff0c\u652f\u6301CPU\u548cGPU\u9644\u52a0\u7684\u5f20\u91cf\u52a0\u901f\u5668\uff0c\u5e76\u4e0e\u73b0\u6709\u8c03\u5ea6\u64cd\u4f5c\uff08\u5982\u751f\u4ea7\u8005-\u6d88\u8d39\u8005\u878d\u5408\uff09\u534f\u540c\u5de5\u4f5c\u3002", "result": "\u5b9e\u73b0\u4e86\u591a\u4e2a\u56fe\u50cf\u5904\u7406\u6d41\u6c34\u7ebf\uff08\u6ee4\u6ce2\u3001\u91cd\u91c7\u6837\u3001\u53bb\u566a\u7b49\uff09\uff0c\u76f8\u6bd4\u975e\u52a0\u901f\u5668\u57fa\u7ebf\u83b7\u5f97\u663e\u8457\u52a0\u901f\u3002\u4f8b\u5982\uff0c\u5728Nvidia RTX 4070 GPU\u4e0a\uff0c\u4e0b\u91c7\u6837\u4f8b\u7a0b\u901a\u8fc7\u4f7f\u7528Tensor Cores\u5b9e\u73b0\u4e866.1\u500d\u52a0\u901f\u3002", "conclusion": "\u901a\u8fc7\u7f16\u8bd1\u5668\u6280\u672f\uff0c\u5f20\u91cf\u52a0\u901f\u5668\u53ef\u4ee5\u6269\u5c55\u5230\u4f20\u7edf\u9886\u57df\u4e4b\u5916\u7684\u5e94\u7528\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u7528\u5c11\u91cf\u4ee3\u7801\u7f16\u5199\u591a\u6837\u5316\u7684\u52a0\u901f\u5668\u5229\u7528\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5f20\u91cf\u52a0\u901f\u5668\u5728\u66f4\u5e7f\u6cdb\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.02189", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.02189", "abs": "https://arxiv.org/abs/2512.02189", "authors": ["Aaron Jarmusch", "Sunita Chandrasekaran"], "title": "Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis", "comment": null, "summary": "As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.\n  Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5f00\u6e90\u5fae\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7cfb\u7edf\u8bc4\u4f30NVIDIA Blackwell (B200) GPU\u67b6\u6784\u521b\u65b0\uff0c\u5305\u62ec\u7b2c5\u4ee3\u5f20\u91cf\u6838\u5fc3\u3001\u5f20\u91cf\u5185\u5b58\u3001\u89e3\u538b\u7f29\u5f15\u64ce\u7b49\u7279\u6027\uff0c\u5e76\u4e0eH200\u5bf9\u6bd4\uff0c\u5c55\u793aB200\u5728\u6df7\u5408\u7cbe\u5ea6\u541e\u5410\u91cf\u63d0\u53471.56\u500d\u3001\u80fd\u6548\u63d0\u534742%\u3001\u5185\u5b58\u8bbf\u95ee\u5ef6\u8fdf\u964d\u4f4e58%\u7b49\u4f18\u52bf\u3002", "motivation": "\u968f\u7740GPU\u67b6\u6784\u5feb\u901f\u53d1\u5c55\u4ee5\u6ee1\u8db3\u767e\u4ebf\u4ebf\u6b21\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u9700\u6c42\uff0c\u67b6\u6784\u521b\u65b0\u5bf9\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u5f71\u54cd\u7f3a\u4e4f\u7cfb\u7edf\u7406\u89e3\u3002NVIDIA Blackwell (B200)\u5f15\u5165\u591a\u9879\u91cd\u8981\u67b6\u6784\u521b\u65b0\uff0c\u4f46\u91cf\u5316\u8fd9\u4e9b\u6539\u8fdb\u7684\u7cfb\u7edf\u65b9\u6cd5\u6ede\u540e\u4e8e\u786c\u4ef6\u5f00\u53d1\u5468\u671f\u3002", "method": "\u5f00\u53d1\u5f00\u6e90\u5fae\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7cfb\u7edf\u8bc4\u4f30Blackwell GPU\u5e76\u4e0eH200\u5bf9\u6bd4\uff0c\u5206\u6790\u5185\u5b58\u5b50\u7cfb\u7edf\u3001\u5f20\u91cf\u6838\u5fc3\u6d41\u6c34\u7ebf\u548c\u6d6e\u70b9\u7cbe\u5ea6\uff08FP32\u3001FP16\u3001FP8\u3001FP6\u3001FP4\uff09\u3002\u8bc4\u4f30\u5bc6\u96c6/\u7a00\u758fGEMM\u3001Transformer\u63a8\u7406\u548c\u8bad\u7ec3\u5de5\u4f5c\u8d1f\u8f7d\u3002", "result": "B200\u7684\u5f20\u91cf\u6838\u5fc3\u589e\u5f3a\u5b9e\u73b0\u6bd4H200\u9ad81.56\u500d\u7684\u6df7\u5408\u7cbe\u5ea6\u541e\u5410\u91cf\u548c42%\u7684\u80fd\u6548\u63d0\u5347\u3002\u5185\u5b58\u5206\u6790\u663e\u793a\u7f13\u5b58\u672a\u547d\u4e2d\u65f6\u5185\u5b58\u8bbf\u95ee\u5ef6\u8fdf\u964d\u4f4e58%\uff0c\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4e86\u6700\u4f18\u7b97\u6cd5\u8bbe\u8ba1\u7b56\u7565\u3002", "conclusion": "\u5f00\u6e90\u5fae\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e3a\u5e94\u7528\u5f00\u53d1\u8005\u63d0\u4f9b\u5b9e\u7528\u89c1\u89e3\uff0c\u5e2e\u52a9\u5145\u5206\u5229\u7528\u73b0\u4ee3GPU\u67b6\u6784\u7279\u6027\uff0c\u652f\u6301\u660e\u667a\u7684\u67b6\u6784\u51b3\u7b56\uff0c\u5e76\u6307\u5bfc\u672a\u6765GPU\u8bbe\u8ba1\u65b9\u5411\u3002"}}
{"id": "2512.02683", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.02683", "abs": "https://arxiv.org/abs/2512.02683", "authors": ["Luiz A. Rodrigues", "Elias P. Duarte", "Luciana Arantes"], "title": "Distributed and Autonomic Minimum Spanning Trees", "comment": "This preprint is an English translation and slightly extended version of the paper published in Portuguese at the 32nd Brazilian Symposium on Computer Networks and Distributed Systems (2014), reference [1]", "summary": "The most common strategy for enabling a process in a distributed system to broadcast a message is one-to-all communication. However, this approach is not scalable, as it places a heavy load on the sender. This work presents an autonomic algorithm that enables the $n$ processes in a distributed system to build and maintain a spanning tree connecting themselves. In this context, processes are the vertices of the spanning tree. By definition, a spanning tree connects all processes without forming cycles. The proposed algorithm ensures that every vertex in the spanning tree has both an in-degree and the tree depth of at most $log_2 n$. When all processes are correct, the degree of each process is exactly $log_2 n$. A spanning tree is dynamically created from any source process and is transparently reconstructed as processes fail or recover. Up to $n-1$ processes can fail, and the correct processes remain connected through a scalable, functioning spanning tree. To build and maintain the tree, processes use the VCube virtual topology, which also serves as a failure detector. Two broadcast algorithms based on the autonomic spanning tree algorithm are presented: one for best-effort broadcast and one for reliable broadcast. Simulation results are provided, including comparisons with other alternatives.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u7ec4\u7ec7\u7b97\u6cd5\uff0c\u4f7f\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684n\u4e2a\u8fdb\u7a0b\u6784\u5efa\u548c\u7ef4\u62a4\u8fde\u63a5\u81ea\u8eab\u7684\u751f\u6210\u6811\uff0c\u6bcf\u4e2a\u8282\u70b9\u7684\u5165\u5ea6\u548c\u6811\u6df1\u5ea6\u4e0d\u8d85\u8fc7log\u2082n\uff0c\u652f\u6301\u52a8\u6001\u521b\u5efa\u3001\u6545\u969c\u6062\u590d\uff0c\u5e76\u57fa\u4e8e\u6b64\u5b9e\u73b0\u4e24\u79cd\u5e7f\u64ad\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u4e00\u5bf9\u591a\u5e7f\u64ad\u7b56\u7565\u4e0d\u53ef\u6269\u5c55\uff0c\u4f1a\u7ed9\u53d1\u9001\u8005\u5e26\u6765\u6c89\u91cd\u8d1f\u8f7d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u6784\u5efa\u548c\u7ef4\u62a4\u751f\u6210\u6811\u7684\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u53ef\u6269\u5c55\u7684\u5e7f\u64ad\u901a\u4fe1\u3002", "method": "\u4f7f\u7528VCube\u865a\u62df\u62d3\u6251\u4f5c\u4e3a\u6545\u969c\u68c0\u6d4b\u5668\uff0c\u63d0\u51fa\u81ea\u7ec4\u7ec7\u7b97\u6cd5\u52a8\u6001\u6784\u5efa\u548c\u7ef4\u62a4\u751f\u6210\u6811\u3002\u7b97\u6cd5\u786e\u4fdd\u6bcf\u4e2a\u9876\u70b9\u7684\u5165\u5ea6\u548c\u6811\u6df1\u5ea6\u4e0d\u8d85\u8fc7log\u2082n\uff0c\u652f\u6301\u4ece\u4efb\u610f\u6e90\u8fdb\u7a0b\u521b\u5efa\uff0c\u5e76\u80fd\u900f\u660e\u5730\u91cd\u5efa\u4ee5\u5e94\u5bf9\u8fdb\u7a0b\u6545\u969c\u548c\u6062\u590d\u3002", "result": "\u7b97\u6cd5\u80fd\u5bb9\u5fcd\u6700\u591an-1\u4e2a\u8fdb\u7a0b\u6545\u969c\uff0c\u6b63\u786e\u8fdb\u7a0b\u4ecd\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u529f\u80fd\u6027\u751f\u6210\u6811\u4fdd\u6301\u8fde\u63a5\u3002\u5f53\u6240\u6709\u8fdb\u7a0b\u6b63\u5e38\u65f6\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\u7684\u5ea6\u6070\u597d\u4e3alog\u2082n\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u4e24\u79cd\u5e7f\u64ad\u7b97\u6cd5\uff1a\u5c3d\u529b\u800c\u4e3a\u5e7f\u64ad\u548c\u53ef\u9760\u5e7f\u64ad\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u4e0e\u5176\u4ed6\u65b9\u6848\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u8be5\u81ea\u7ec4\u7ec7\u751f\u6210\u6811\u7b97\u6cd5\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5e7f\u64ad\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u52a8\u6001\u9002\u5e94\u7cfb\u7edf\u53d8\u5316\uff0c\u652f\u6301\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u7684\u9ad8\u6548\u901a\u4fe1\u3002"}}
{"id": "2512.02738", "categories": ["cs.PL", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02738", "abs": "https://arxiv.org/abs/2512.02738", "authors": ["Joel Nyholm", "Wojciech Mostowski", "Christoph Reichenbach"], "title": "Probabilistic energy profiler for statically typed JVM-based programming languages", "comment": null, "summary": "Energy consumption is a growing concern in several fields, from mobile devices to large data centers. Developers need detailed data on the energy consumption of their software to mitigate consumption issues. Previous approaches have a broader focus, such as on specific functions or programs, rather than source code statements. They primarily focus on estimating the CPU's energy consumption using point estimates, thereby disregarding other hardware effects and limiting their use for statistical reasoning and explainability. We developed a novel methodology to address the limitations of measuring only the CPU's consumption and using point estimates, focusing on predicting the energy usage of statically typed JVM-based programming languages, such as Java and Scala. We measure the energy consumption of Bytecode patterns, the translation from the programming language's source code statement to their Java Bytecode representation. With the energy measurements, we construct a statistical model using Bayesian statistics, which allows us to predict the energy consumption through statistical distributions and analyze individual factors. The model includes three factors we obtain statically from the code: data size, data type, operation, and one factor about the hardware platform the code executes on: device. To validate our methodology, we implemented it for Java and evaluated its energy predictions on unseen programs. We observe that all four factors are influential, notably that two devices of the same model may differ in energy consumption and that the operations and data types cause consumption differences. The experiments also show that the energy prediction of programs closely follows the program's real energy consumption, validating our approach. Our work presents a methodology for constructing an energy model that future work, such as verification tools, can use for their energy estimates.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8d1d\u53f6\u65af\u7edf\u8ba1\u7684JVM\u5b57\u8282\u7801\u80fd\u8017\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5b57\u8282\u7801\u6a21\u5f0f\u3001\u6570\u636e\u7c7b\u578b\u3001\u64cd\u4f5c\u548c\u8bbe\u5907\u56e0\u7d20\uff0c\u6784\u5efa\u7edf\u8ba1\u5206\u5e03\u6a21\u578b\u800c\u975e\u70b9\u4f30\u8ba1\uff0c\u63d0\u9ad8\u80fd\u8017\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u80fd\u8017\u5206\u6790\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8CPU\u80fd\u8017\u7684\u70b9\u4f30\u8ba1\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u786c\u4ef6\u5f71\u54cd\uff0c\u4e14\u7f3a\u4e4f\u7edf\u8ba1\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u66f4\u5168\u9762\u9884\u6d4b\u8f6f\u4ef6\u80fd\u8017\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9759\u6001\u7c7b\u578bJVM\u8bed\u8a00\uff08\u5982Java\u548cScala\uff09\u3002", "method": "\u6d4b\u91cf\u5b57\u8282\u7801\u6a21\u5f0f\u7684\u80fd\u8017\uff0c\u6784\u5efa\u57fa\u4e8e\u8d1d\u53f6\u65af\u7edf\u8ba1\u7684\u7edf\u8ba1\u6a21\u578b\u3002\u6a21\u578b\u5305\u542b\u56db\u4e2a\u56e0\u7d20\uff1a\u6570\u636e\u5927\u5c0f\u3001\u6570\u636e\u7c7b\u578b\u3001\u64cd\u4f5c\uff08\u4ece\u4ee3\u7801\u9759\u6001\u83b7\u53d6\uff09\u4ee5\u53ca\u6267\u884c\u786c\u4ef6\u5e73\u53f0\uff08\u8bbe\u5907\uff09\u3002\u901a\u8fc7Java\u5b9e\u73b0\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u6240\u6709\u56db\u4e2a\u56e0\u7d20\u90fd\u5bf9\u80fd\u8017\u6709\u663e\u8457\u5f71\u54cd\uff0c\u7279\u522b\u662f\u540c\u578b\u53f7\u8bbe\u5907\u95f4\u5b58\u5728\u80fd\u8017\u5dee\u5f02\uff0c\u64cd\u4f5c\u548c\u6570\u636e\u7c7b\u578b\u4e5f\u4f1a\u5bfc\u81f4\u80fd\u8017\u5dee\u5f02\u3002\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u5bf9\u672a\u89c1\u7a0b\u5e8f\u7684\u80fd\u8017\u9884\u6d4b\u4e0e\u5b9e\u9645\u80fd\u8017\u9ad8\u5ea6\u543b\u5408\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u80fd\u8017\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\u5b66\uff0c\u8be5\u65b9\u6cd5\u53ef\u4e3a\u672a\u6765\u9a8c\u8bc1\u5de5\u5177\u7b49\u63d0\u4f9b\u80fd\u8017\u4f30\u8ba1\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8CPU\u70b9\u4f30\u8ba1\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u7edf\u8ba1\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2512.02346", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.02346", "abs": "https://arxiv.org/abs/2512.02346", "authors": ["Hongyang Shang", "An Guo", "Shuai Dong", "Junyi Yang", "Ye Ke", "Arindam Basu"], "title": "Near-Memory Architecture for Threshold-Ordinal Surface-Based Corner Detection of Event Cameras", "comment": null, "summary": "Event-based Cameras (EBCs) are widely utilized in surveillance and autonomous driving applications due to their high speed and low power consumption. Corners are essential low-level features in event-driven computer vision, and novel algorithms utilizing event-based representations, such as Threshold-Ordinal Surface (TOS), have been developed for corner detection. However, the implementation of these algorithms on resource-constrained edge devices is hindered by significant latency, undermining the advantages of EBCs. To address this challenge, a near-memory architecture for efficient TOS updates (NM-TOS) is proposed. This architecture employs a read-write decoupled 8T SRAM cell and optimizes patch update speed through pipelining. Hardware-software co-optimized peripheral circuits and dynamic voltage and frequency scaling (DVFS) enable power and latency reductions. Compared to traditional digital implementations, our architecture reduces latency/energy by 24.7x/1.2x at Vdd = 1.2 V or 1.93x/6.6x at Vdd = 0.6 V based on 65nm CMOS process. Monte Carlo simulations confirm robust circuit operation, demonstrating zero bit error rate at operating voltages above 0.62 V, with only 0.2% at 0.61 V and 2.5% at 0.6 V. Corner detection evaluation using precision-recall area under curve (AUC) metrics reveals minor AUC reductions of 0.027 and 0.015 at 0.6 V for two popular EBC datasets.", "AI": {"tldr": "\u63d0\u51faNM-TOS\u8fd1\u5b58\u67b6\u6784\uff0c\u901a\u8fc78T SRAM\u5355\u5143\u3001\u6d41\u6c34\u7ebf\u4f18\u5316\u548cDVFS\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4eTOS\u89d2\u70b9\u68c0\u6d4b\u7b97\u6cd5\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u9002\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u76d1\u63a7\u548c\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46TOS\u7b49\u89d2\u70b9\u68c0\u6d4b\u7b97\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b58\u5728\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u9650\u5236\u4e86\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u901f\u4f4e\u529f\u8017\u4f18\u52bf\u3002", "method": "\u63d0\u51faNM-TOS\u8fd1\u5b58\u67b6\u6784\uff0c\u91c7\u7528\u8bfb\u5199\u89e3\u8026\u76848T SRAM\u5355\u5143\uff0c\u901a\u8fc7\u6d41\u6c34\u7ebf\u4f18\u5316\u8865\u4e01\u66f4\u65b0\u901f\u5ea6\uff0c\u7ed3\u5408\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u4f18\u5316\u7684\u5916\u56f4\u7535\u8def\u548c\u52a8\u6001\u7535\u538b\u9891\u7387\u7f29\u653e(DVFS)\u6280\u672f\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u6570\u5b57\u5b9e\u73b0\uff0c\u57281.2V\u7535\u538b\u4e0b\u5ef6\u8fdf/\u80fd\u8017\u964d\u4f4e24.7\u500d/1.2\u500d\uff0c\u57280.6V\u7535\u538b\u4e0b\u964d\u4f4e1.93\u500d/6.6\u500d\u3002\u8499\u7279\u5361\u6d1b\u4eff\u771f\u663e\u793a\u57280.62V\u4ee5\u4e0a\u96f6\u6bd4\u7279\u9519\u8bef\u7387\uff0c\u89d2\u70b9\u68c0\u6d4b\u7cbe\u5ea6\u635f\u5931\u5f88\u5c0f\u3002", "conclusion": "NM-TOS\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u4e8b\u4ef6\u76f8\u673a\u89d2\u70b9\u68c0\u6d4b\u7b97\u6cd5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2512.02818", "categories": ["cs.DC", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.02818", "abs": "https://arxiv.org/abs/2512.02818", "authors": ["Sean R. Wilkinson", "Patrick Widener", "Sarp Oral", "Rafael Ferreira da Silva"], "title": "Designing FAIR Workflows at OLCF: Building Scalable and Reusable Ecosystems for HPC Science", "comment": null, "summary": "High Performance Computing (HPC) centers provide advanced infrastructure that enables scientific research at extreme scale. These centers operate with hardware configurations, software environments, and security requirements that differ substantially from most users' local systems. As a result, users often develop customized digital artifacts that are tightly coupled to a given HPC center. This practice can lead to significant duplication of effort as multiple users independently create similar solutions to common problems. The FAIR Principles offer a framework to address these challenges. Initially designed to improve data stewardship, the FAIR approach has since been extended to encompass software, workflows, models, and infrastructure. By encouraging the use of rich metadata and community standards, FAIR practices aim to make digital artifacts easier to share and reuse, both within and across scientific domains. Many FAIR initiatives have emerged within individual research communities, often aligned by discipline (e.g. bioinformatics, earth sciences). These communities have made progress in adopting FAIR practices, but their domain-specific nature can lead to silos that limit broader collaboration. Thus, we propose that HPC centers play a more active role in fostering FAIR ecosystems that support research across multiple disciplines. This requires designing infrastructure that enables researchers to discover, share, and reuse computational components more effectively. Here, we build on the architecture of the European Open Science Cloud (EOSC) EOSC-Life FAIR Workflows Collaboratory to propose a model tailored to the needs of HPC. Rather than focusing on entire workflows, we emphasize the importance of making individual workflow components FAIR. This component-based approach better supports the diverse and evolving needs of HPC users while maximizing the long-term value of their work.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faHPC\u4e2d\u5fc3\u5e94\u66f4\u79ef\u6781\u5730\u5728\u8de8\u5b66\u79d1\u7814\u7a76\u4e2d\u57f9\u80b2FAIR\u751f\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u57fa\u4e8e\u7ec4\u4ef6\u7684FAIR\u65b9\u6cd5\u800c\u975e\u5b8c\u6574\u5de5\u4f5c\u6d41\uff0c\u63d0\u5347\u8ba1\u7b97\u7ec4\u4ef6\u7684\u53ef\u53d1\u73b0\u6027\u3001\u5171\u4eab\u548c\u91cd\u7528\u3002", "motivation": "HPC\u4e2d\u5fc3\u7684\u57fa\u7840\u8bbe\u65bd\u3001\u8f6f\u4ef6\u73af\u5883\u548c\u5b89\u5168\u8981\u6c42\u4e0e\u7528\u6237\u672c\u5730\u7cfb\u7edf\u5dee\u5f02\u5927\uff0c\u5bfc\u81f4\u7528\u6237\u5f00\u53d1\u4e0e\u7279\u5b9aHPC\u4e2d\u5fc3\u7d27\u5bc6\u8026\u5408\u7684\u6570\u5b57\u5236\u54c1\uff0c\u9020\u6210\u91cd\u590d\u52b3\u52a8\u3002\u867d\u7136FAIR\u539f\u5219\u5df2\u5728\u5404\u7814\u7a76\u793e\u533a\u63a8\u5e7f\uff0c\u4f46\u9886\u57df\u7279\u5b9a\u6027\u5f62\u6210\u4e86\u4fe1\u606f\u5b64\u5c9b\uff0c\u9650\u5236\u4e86\u8de8\u5b66\u79d1\u5408\u4f5c\u3002", "method": "\u57fa\u4e8e\u6b27\u6d32\u5f00\u653e\u79d1\u5b66\u4e91EOSC-Life FAIR\u5de5\u4f5c\u6d41\u534f\u4f5c\u5e73\u53f0\u7684\u67b6\u6784\uff0c\u63d0\u51fa\u9488\u5bf9HPC\u9700\u6c42\u5b9a\u5236\u7684\u6a21\u578b\u3002\u5f3a\u8c03\u4f7f\u5355\u4e2a\u5de5\u4f5c\u6d41\u7ec4\u4ef6FAIR\u5316\uff0c\u800c\u975e\u6574\u4e2a\u5de5\u4f5c\u6d41\uff0c\u91c7\u7528\u57fa\u4e8e\u7ec4\u4ef6\u7684\u65b9\u6cd5\u6765\u652f\u6301HPC\u7528\u6237\u7684\u591a\u6837\u5316\u9700\u6c42\u3002", "result": "\u63d0\u51faHPC\u4e2d\u5fc3\u5e94\u8bbe\u8ba1\u652f\u6301\u7814\u7a76\u4eba\u5458\u66f4\u6709\u6548\u53d1\u73b0\u3001\u5171\u4eab\u548c\u91cd\u7528\u8ba1\u7b97\u7ec4\u4ef6\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u7ec4\u4ef6\u7ea7FAIR\u5316\u6700\u5927\u5316HPC\u7528\u6237\u5de5\u4f5c\u7684\u957f\u671f\u4ef7\u503c\u3002", "conclusion": "HPC\u4e2d\u5fc3\u5e94\u53d1\u6325\u66f4\u79ef\u6781\u4f5c\u7528\uff0c\u57f9\u80b2\u8de8\u5b66\u79d1\u7684FAIR\u751f\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ec4\u4ef6\u5316FAIR\u65b9\u6cd5\u89e3\u51b3HPC\u73af\u5883\u4e2d\u7684\u91cd\u590d\u52b3\u52a8\u548c\u8de8\u5b66\u79d1\u534f\u4f5c\u95ee\u9898\uff0c\u63d0\u5347\u79d1\u5b66\u7814\u7a76\u7684\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2512.02966", "categories": ["cs.PL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.02966", "abs": "https://arxiv.org/abs/2512.02966", "authors": ["Isha Chaudhary", "Vedaant Jain", "Avaljot Singh", "Kavya Sachdeva", "Sayan Ranu", "Gagandeep Singh"], "title": "Lumos: Let there be Language Model System Certification", "comment": null, "summary": "We introduce the first principled framework, Lumos, for specifying and formally certifying Language Model System (LMS) behaviors. Lumos is an imperative probabilistic programming DSL over graphs, with constructs to generate independent and identically distributed prompts for LMS. It offers a structured view of prompt distributions via graphs, forming random prompts from sampled subgraphs. Lumos supports certifying LMS for arbitrary prompt distributions via integration with statistical certifiers. We provide hybrid (operational and denotational) semantics for Lumos, providing a rigorous way to interpret the specifications. Using only a small set of composable constructs, Lumos can encode existing LMS specifications, including complex relational and temporal specifications. It also facilitates specifying new properties - we present the first safety specifications for vision-language models (VLMs) in autonomous driving scenarios developed with Lumos. Using these, we show that the state-of-the-art VLM Qwen-VL exhibits critical safety failures, producing incorrect and unsafe responses with at least 90% probability in right-turn scenarios under rainy driving conditions, revealing substantial safety risks. Lumos's modular structure allows easy modification of the specifications, enabling LMS certification to stay abreast with the rapidly evolving threat landscape. We further demonstrate that specification programs written in Lumos enable finding specific failure cases exhibited by state-of-the-art LMS. Lumos is the first systematic and extensible language-based framework for specifying and certifying LMS behaviors, paving the way for a wider adoption of LMS certification.", "AI": {"tldr": "Lumos\u662f\u4e00\u4e2a\u7528\u4e8e\u89c4\u8303\u548c\u5f62\u5f0f\u5316\u8ba4\u8bc1\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u884c\u4e3a\u7684\u6982\u7387\u7f16\u7a0b\u6846\u67b6\uff0c\u652f\u6301\u901a\u8fc7\u56fe\u7ed3\u6784\u8868\u793a\u63d0\u793a\u5206\u5e03\uff0c\u5e76\u80fd\u53d1\u73b0\u6700\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u4e25\u91cd\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u5316\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\u6765\u89c4\u8303\u548c\u5f62\u5f0f\u5316\u8ba4\u8bc1\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u7684\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u5feb\u901f\u6f14\u53d8\u7684\u5a01\u80c1\u73af\u5883\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7075\u6d3b\u4fee\u6539\u89c4\u8303\u5e76\u4fdd\u6301\u8ba4\u8bc1\u6709\u6548\u6027\u7684\u65b9\u6cd5\u3002", "method": "Lumos\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u547d\u4ee4\u5f0f\u6982\u7387\u7f16\u7a0b\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u8868\u793a\u63d0\u793a\u5206\u5e03\uff0c\u4ece\u91c7\u6837\u5b50\u56fe\u751f\u6210\u968f\u673a\u63d0\u793a\uff0c\u63d0\u4f9b\u6df7\u5408\u8bed\u4e49\u89e3\u91ca\u89c4\u8303\uff0c\u5e76\u4e0e\u7edf\u8ba1\u8ba4\u8bc1\u5668\u96c6\u6210\u6765\u8ba4\u8bc1\u4efb\u610f\u63d0\u793a\u5206\u5e03\u4e0b\u7684\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u3002", "result": "Lumos\u80fd\u591f\u7f16\u7801\u73b0\u6709\u7684\u8bed\u8a00\u6a21\u578b\u89c4\u8303\uff0c\u5305\u62ec\u590d\u6742\u7684\u5173\u7cfb\u548c\u65f6\u95f4\u89c4\u8303\uff0c\u5e76\u9996\u6b21\u4e3a\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5236\u5b9a\u4e86\u5b89\u5168\u89c4\u8303\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578bQwen-VL\u5728\u96e8\u5929\u53f3\u8f6c\u573a\u666f\u4e2d\u81f3\u5c11\u670990%\u7684\u6982\u7387\u4ea7\u751f\u9519\u8bef\u548c\u4e0d\u5b89\u5168\u7684\u54cd\u5e94\uff0c\u66b4\u9732\u51fa\u91cd\u5927\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "Lumos\u662f\u7b2c\u4e00\u4e2a\u7cfb\u7edf\u5316\u3001\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u8bed\u8a00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89c4\u8303\u548c\u8ba4\u8bc1\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u884c\u4e3a\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u91c7\u7528\u8bed\u8a00\u6a21\u578b\u8ba4\u8bc1\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5176\u6a21\u5757\u5316\u7ed3\u6784\u4f7f\u89c4\u8303\u6613\u4e8e\u4fee\u6539\uff0c\u80fd\u591f\u8ddf\u4e0a\u5feb\u901f\u6f14\u53d8\u7684\u5a01\u80c1\u73af\u5883\u3002"}}
{"id": "2512.02859", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.02859", "abs": "https://arxiv.org/abs/2512.02859", "authors": ["Cristian Tirelli", "Rodrigo Otoni", "Laura Pozzi"], "title": "Monomorphism-based CGRA Mapping via Space and Time Decoupling", "comment": null, "summary": "Coarse-Grain Reconfigurable Arrays (CGRAs) provide flexibility and energy efficiency in accelerating compute-intensive loops. Existing compilation techniques often struggle with scalability, unable to map code onto large CGRAs. To address this, we propose a novel approach to the mapping problem where the time and space dimensions are decoupled and explored separately. We leverage an SMT formulation to traverse the time dimension first, and then perform a monomorphism-based search to find a valid spatial solution. Experimental results show that our approach achieves the same mapping quality of state-of-the-art techniques while significantly reducing compilation time, with this reduction being particularly tangible when compiling for large CGRAs. We achieve approximately $10^5\\times$ average compilation speedup for the benchmarks evaluated on a $20\\times 20$ CGRA.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684CGRA\u6620\u5c04\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u8026\u65f6\u95f4\u548c\u7a7a\u95f4\u7ef4\u5ea6\u5e76\u5206\u522b\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347\u7f16\u8bd1\u901f\u5ea6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u578bCGRA", "motivation": "\u73b0\u6709CGRA\u7f16\u8bd1\u6280\u672f\u5728\u5904\u7406\u5927\u89c4\u6a21\u9635\u5217\u65f6\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u96be\u4ee5\u5c06\u4ee3\u7801\u9ad8\u6548\u6620\u5c04\u5230\u5927\u578bCGRA\u4e0a", "method": "\u91c7\u7528\u65f6\u95f4\u548c\u7a7a\u95f4\u7ef4\u5ea6\u89e3\u8026\u7684\u65b9\u6cd5\uff1a\u9996\u5148\u4f7f\u7528SMT\u516c\u5f0f\u904d\u5386\u65f6\u95f4\u7ef4\u5ea6\uff0c\u7136\u540e\u57fa\u4e8e\u5355\u6001\u641c\u7d22\u5bfb\u627e\u6709\u6548\u7684\u7a7a\u95f4\u89e3\u51b3\u65b9\u6848", "result": "\u5728\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u6280\u672f\u76f8\u540c\u6620\u5c04\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u7f16\u8bd1\u65f6\u95f4\uff0c\u7279\u522b\u662f\u5728\u5927\u578bCGRA\u4e0a\u6548\u679c\u66f4\u660e\u663e\u3002\u572820\u00d720 CGRA\u4e0a\u5b9e\u73b0\u4e86\u7ea610^5\u500d\u7684\u7f16\u8bd1\u901f\u5ea6\u63d0\u5347", "conclusion": "\u63d0\u51fa\u7684\u89e3\u8026\u65f6\u95f4\u548c\u7a7a\u95f4\u7ef4\u5ea6\u7684\u6620\u5c04\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86CGRA\u7f16\u8bd1\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e3a\u5927\u578bCGRA\u7684\u9ad8\u6548\u7f16\u8bd1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2512.02875", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.02875", "abs": "https://arxiv.org/abs/2512.02875", "authors": ["Cristian Tirelli", "Lorenzo Ferretti", "Laura Pozzi"], "title": "SAT-MapIt: A SAT-based Modulo Scheduling Mapper for Coarse Grain Reconfigurable Architectures", "comment": null, "summary": "Coarse-Grain Reconfigurable Arrays (CGRAs) are emerging low-power architectures aimed at accelerating compute-intensive application loops. The acceleration that a CGRA can ultimately provide, however, heavily depends on the quality of the mapping, i.e. on how effectively the loop is compiled onto the given platform. State of the Art compilation techniques achieve mapping through modulo scheduling, a strategy which attempts to minimize the II (Iteration Interval) needed to execute a loop, and they do so usually through well known graph algorithms, such as Max-Clique Enumeration.\n  We address the mapping problem through a SAT formulation, instead, and thus explore the solution space more effectively than current SoA tools. To formulate the SAT problem, we introduce an ad-hoc schedule called the \\textit{kernel mobility schedule} (KMS), which we use in conjunction with the data-flow graph and the architectural information of the CGRA in order to create a set of boolean statements that describe all constraints to be obeyed by the mapping for a given II. We then let the SAT solver efficiently navigate this complex space. As in other SoA techniques, the process is iterative: if a valid mapping does not exist for the given II, the II is increased and a new KMS and set of constraints is generated and solved.\n  Our experimental results show that SAT-MapIt obtains better results compared to SoA alternatives in $47.72\\%$ of the benchmarks explored: sometimes finding a lower II, and others even finding a valid mapping when none could previously be found.", "AI": {"tldr": "SAT-MapIt\uff1a\u4f7f\u7528SAT\u6c42\u89e3\u5668\u89e3\u51b3CGRA\u6620\u5c04\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u80fd\u66f4\u6709\u6548\u5730\u63a2\u7d22\u89e3\u7a7a\u95f4\uff0c\u572847.72%\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u66f4\u597d\u7ed3\u679c\u3002", "motivation": "CGRA\u7684\u52a0\u901f\u6548\u679c\u4e25\u91cd\u4f9d\u8d56\u4e8e\u6620\u5c04\u8d28\u91cf\uff0c\u73b0\u6709\u6280\u672f\u4f7f\u7528\u6a21\u8c03\u5ea6\u548c\u56fe\u7b97\u6cd5\uff08\u5982\u6700\u5927\u56e2\u679a\u4e3e\uff09\u8fdb\u884c\u7f16\u8bd1\uff0c\u4f46\u89e3\u7a7a\u95f4\u63a2\u7d22\u4e0d\u591f\u6709\u6548\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eSAT\u516c\u5f0f\u5316\u7684\u6620\u5c04\u65b9\u6cd5\uff1a1\uff09\u5f15\u5165\u6838\u79fb\u52a8\u8c03\u5ea6\uff08KMS\uff09\uff1b2\uff09\u7ed3\u5408\u6570\u636e\u6d41\u56fe\u548cCGRA\u67b6\u6784\u4fe1\u606f\u751f\u6210\u5e03\u5c14\u7ea6\u675f\uff1b3\uff09\u4f7f\u7528SAT\u6c42\u89e3\u5668\u9ad8\u6548\u5bfc\u822a\u89e3\u7a7a\u95f4\uff1b4\uff09\u91c7\u7528\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u5982\u679c\u5f53\u524dII\u65e0\u6709\u6548\u6620\u5c04\u5219\u589e\u52a0II\u91cd\u65b0\u6c42\u89e3\u3002", "result": "SAT-MapIt\u572847.72%\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff1a\u6709\u65f6\u80fd\u627e\u5230\u66f4\u4f4e\u7684II\uff0c\u6709\u65f6\u80fd\u5728\u73b0\u6709\u6280\u672f\u627e\u4e0d\u5230\u6709\u6548\u6620\u5c04\u7684\u60c5\u51b5\u4e0b\u627e\u5230\u6709\u6548\u6620\u5c04\u3002", "conclusion": "\u57fa\u4e8eSAT\u7684\u6620\u5c04\u65b9\u6cd5\u6bd4\u73b0\u6709\u6a21\u8c03\u5ea6\u6280\u672f\u80fd\u66f4\u6709\u6548\u5730\u63a2\u7d22\u89e3\u7a7a\u95f4\uff0c\u4e3aCGRA\u7f16\u8bd1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.02884", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.02884", "abs": "https://arxiv.org/abs/2512.02884", "authors": ["Cristian Tirelli", "Laura Pozzi"], "title": "Mapping code on Coarse Grained Reconfigurable Arrays using a SAT solver", "comment": null, "summary": "Emerging low-powered architectures like Coarse-Grain Reconfigurable Arrays (CGRAs) are becoming more common. Often included as co-processors, they are used to accelerate compute-intensive workloads like loops. The speedup obtained is defined by the hardware design of the accelerator and by the quality of the compilation. State of the art (SoA) compilation techniques leverage modulo scheduling to minimize the Iteration Interval (II), exploit the architecture parallelism and, consequentially, reduce the execution time of the accelerated workload. In our work, we focus on improving the compilation process by finding the lowest II for any given topology, through a satisfiability (SAT) formulation of the mapping problem. We introduce a novel schedule, called Kernel Mobility Schedule, to encode all the possible mappings for a given Data Flow Graph (DFG) and for a given II. The schedule is used together with the CGRA architectural information to generate all the constraints necessary to find a valid mapping. Experimental results demonstrate that our method not only reduces compilation time on average but also achieves higher quality mappings compared to existing SoA techniques.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eSAT\u7684CGRA\u6620\u5c04\u65b9\u6cd5\uff0c\u4f7f\u7528Kernel Mobility Schedule\u7f16\u7801\u6240\u6709\u53ef\u80fd\u7684\u6620\u5c04\uff0c\u663e\u8457\u51cf\u5c11\u7f16\u8bd1\u65f6\u95f4\u5e76\u63d0\u9ad8\u6620\u5c04\u8d28\u91cf", "motivation": "CGRA\u4f5c\u4e3a\u534f\u5904\u7406\u5668\u52a0\u901f\u8ba1\u7b97\u5bc6\u96c6\u578b\u5faa\u73af\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4f46\u73b0\u6709\u7f16\u8bd1\u6280\u672f\u96be\u4ee5\u627e\u5230\u6700\u4f18\u6620\u5c04\uff0c\u9700\u8981\u6539\u8fdb\u7f16\u8bd1\u8fc7\u7a0b\u4ee5\u627e\u5230\u7ed9\u5b9a\u62d3\u6251\u7ed3\u6784\u4e0b\u7684\u6700\u4f4e\u8fed\u4ee3\u95f4\u9694", "method": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u6ee1\u8db3\u6027(SAT)\u7684\u6620\u5c04\u95ee\u9898\u516c\u5f0f\u5316\u65b9\u6cd5\uff0c\u5f15\u5165Kernel Mobility Schedule\u6765\u7f16\u7801\u7ed9\u5b9a\u6570\u636e\u6d41\u56fe\u548c\u8fed\u4ee3\u95f4\u9694\u7684\u6240\u6709\u53ef\u80fd\u6620\u5c04\uff0c\u7ed3\u5408CGRA\u67b6\u6784\u4fe1\u606f\u751f\u6210\u7ea6\u675f\u6761\u4ef6", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5e73\u5747\u51cf\u5c11\u4e86\u7f16\u8bd1\u65f6\u95f4\uff0c\u800c\u4e14\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u83b7\u5f97\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u6620\u5c04", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eSAT\u7684CGRA\u6620\u5c04\u65b9\u6cd5\u901a\u8fc7Kernel Mobility Schedule\u6709\u6548\u89e3\u51b3\u4e86\u6620\u5c04\u95ee\u9898\uff0c\u5728\u7f16\u8bd1\u65f6\u95f4\u548c\u6620\u5c04\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f"}}
