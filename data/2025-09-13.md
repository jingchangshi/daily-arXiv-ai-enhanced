<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs](https://arxiv.org/abs/2509.09019)
*Mohit Tekriwal,John Sarracino*

Main category: cs.PL

TL;DR: 利用Rocq定理证明器中的Verified LLVM框架，证明LLVM IR层浮点数FMA优化的正确性，并提出扩展方案


<details>
  <summary>Details</summary>
Motivation: 科学计算程序需要高性能优化，但必须确保浮点数优化的正确性，特别是fast math优化

Method: 基于Rocq定理证明器中的Verified LLVM框架，证明FMA优化在基本块a*b+c算术表达式中的正确性

Result: 完成了FMA优化正确性的预期证明工作

Conclusion: 该初步工作为验证更多浮点数优化的正确性奠定了基础，并提出了扩展更多程序特征和优化的方向

Abstract: Scientific computing programs often undergo aggressive compiler optimization
to achieve high performance and efficient resource utilization. While
performance is critical, we also need to ensure that these optimizations are
correct. In this paper, we focus on a specific class of optimizations,
floating-point optimizations, notably due to fast math, at the LLVM IR level.
We present a preliminary work, which leverages the Verified LLVM framework in
the Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)
optimization for a basic block implementing the arithmetic expression $a * b +
c$ . We then propose ways to extend this preliminary results by adding more
program features and fast math floating-point optimizations.

</details>


### [2] [Dependent-Type-Preserving Memory Allocation](https://arxiv.org/abs/2509.09059)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: 依赖类型语言在编译时类型规范被擦除，外部程序链接可能违反原始程序规范。本文开发支持依赖内存分配的中间语言和类型保持的编译器传递来解决此问题。


<details>
  <summary>Details</summary>
Motivation: 依赖类型语言如Coq、Agda等允许编写详细程序规范，但这些规范在编译时被擦除，外部链接程序可能违反原始程序的内存安全等规范，即使使用验证编译器编译。

Method: 开发支持依赖内存分配的类型化中间语言，以及依赖类型保持的内存分配编译器传递，通过在链接时进行类型检查来防止与类型不正确的程序链接。

Result: 本文是进行中的工作，提出了类型保持编译的方法论框架，具体实现结果尚未报告。

Conclusion: 类型保持编译可以解决依赖类型语言编译后规范被擦除的问题，通过在链接时进行类型检查确保外部程序不会违反原始程序的类型规范。

Abstract: Dependently typed programming languages such as Coq, Agda, Idris, and F*,
allow programmers to write detailed specifications of their programs and prove
their programs meet these specifications. However, these specifications can be
violated during compilation since they are erased after type checking. External
programs linked with the compiled program can violate the specifications of the
original program and change the behavior of the compiled program -- even when
compiled with a verified compiler. For example, since Coq does not allow
explicitly allocating memory, a programmer might link their Coq program with a
C program that can allocate memory. Even if the Coq program is compiled with a
verified compiler, the external C program can still violate the memory-safe
specification of the Coq program by providing an uninitialized pointer to
memory. This error could be ruled out by type checking in a language expressive
enough to indicate whether memory is initialized versus uninitialized. Linking
with a program with an uninitialized pointer could be considered ill-typed, and
our linking process could prevent linking with ill-typed programs. To
facilitate type checking during linking, we can use type-preserving
compilation, which preserves the types through the compilation process. In this
ongoing work, we develop a typed intermediate language that supports dependent
memory allocation, as well as a dependent-type-preserving compiler pass for
memory allocation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems](https://arxiv.org/abs/2509.08969)
*Nima Karimian Kakolaki*

Main category: cs.DC

TL;DR: 本文比较了分布式系统中不同标识符方案（UUIDv4、UUIDv7、ULIDs）的性能，发现ULIDs在网络开销、生成速度和碰撞风险方面均显著优于其他方案


<details>
  <summary>Details</summary>
Motivation: 分布式系统需要健壮、可扩展的标识符方案来确保数据唯一性和跨节点高效索引，因此需要系统分析不同标识符方案的性能表现

Method: 结合碰撞概率的数学计算和在模拟分布式环境中的实证实验，测量生成速度和网络传输开销

Result: ULIDs相比UUIDv4和UUIDv7减少网络开销83.7%，提高生成速度97.32%，碰撞风险降低98.42%，即使在高生成率下也能保持可忽略的碰撞概率

Conclusion: ULIDs是高性能分布式系统的最佳选择，提供高效、时间有序且字典可排序的标识符，适合可扩展应用

Abstract: Distributed systems require robust, scalable identifier schemes to ensure
data uniqueness and efficient indexing across multiple nodes. This paper
presents a comprehensive analysis of the evolution of distributed identifiers,
comparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We
combine mathematical calculation of collision probabilities with empirical
experiments measuring generation speed and network transmission overhead in a
simulated distributed environment. Results demonstrate that ULIDs significantly
outperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing
generation speed by 97.32%. statistical analysis further shows ULIDs offer a
98.42% lower collision risk compared to UUIDv7, while maintaining negligible
collision probabilities even at high generation rates. These findings highlight
ULIDs as an optimal choice for high-performance distributed systems, providing
efficient, time-ordered, and lexicographically sortable identifiers suitable
for scalable applications. All source code, datasets, and analysis scripts
utilized in this research are publicly available in our dedicated repository at
https://github.com/nimakarimiank/uids-comparison. This repository contains
comprehensive documentation of the experimental setup, including configuration
files for the distributed environment, producer and consumer implementations,
and message broker integration. Additionally, it provides the data scripts and
datasets. Researchers and practitioners are encouraged to explore the
repository for full reproducibility of the experiments and to facilitate
further investigation or extension of the presented work.

</details>


### [4] [Optimizing the Variant Calling Pipeline Execution on Human Genomes Using GPU-Enabled Machines](https://arxiv.org/abs/2509.09058)
*Ajay Kumar,Praveen Rao,Peter Sanders*

Main category: cs.DC

TL;DR: 这篇论文提出了一种基于机器学习的优化方法，通过预测变异检测流水线各阶段执行时间和生成最优执行计划，在GPU机器上实现了人类基因组工作负载的高效处理，获得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于变异检测计算密集型质，基因组数据通常在云环境中处理。但在GPU机器上高效执行变异检测流水线仍面临挑战，需要优化工作负载执行以最小化总执行时间。

Method: 提出了两种关键技术：1）使用机器学习根据基因组序列特征预测变异检测流水线各阶段执行时间；2）受灵活作业调度问题启发，使用预测时间生成最优执行计划，通过精心同步在不同机器上执行。

Result: 在公开基因组序列工作负载上评估，方法能够有效预测执行时间（使用序列大小、读质量、重复读百分比、平均读长度等特征）。与贪心方法相比获得2倍加速，与动态方法相比获得1.6倍加速。

Conclusion: 该研究提出的基于机器学习的优化方法能够在GPU机器上高效执行变异检测流水线，通过准确的执行时间预测和最优调度，实现了显著的性能提升。

Abstract: Variant calling is the first step in analyzing a human genome and aims to
detect variants in an individual's genome compared to a reference genome. Due
to the computationally-intensive nature of variant calling, genomic data are
increasingly processed in cloud environments as large amounts of compute and
storage resources can be acquired with the pay-as-you-go pricing model. In this
paper, we address the problem of efficiently executing a variant calling
pipeline for a workload of human genomes on graphics processing unit
(GPU)-enabled machines. We propose a novel machine learning (ML)-based approach
for optimizing the workload execution to minimize the total execution time. Our
approach encompasses two key techniques: The first technique employs ML to
predict the execution times of different stages in a variant calling pipeline
based on the characteristics of a genome sequence. Using the predicted times,
the second technique generates optimal execution plans for the machines by
drawing inspiration from the flexible job shop scheduling problem. The plans
are executed via careful synchronization across different machines. We
evaluated our approach on a workload of publicly available genome sequences
using a testbed with different types of GPU hardware. We observed that our
approach was effective in predicting the execution times of variant calling
pipeline stages using ML on features such as sequence size, read quality,
percentage of duplicate reads, and average read length. In addition, our
approach achieved 2X speedup (on an average) over a greedy approach that also
used ML for predicting the execution times on the tested workload of sequences.
Finally, our approach achieved 1.6X speedup (on an average) over a dynamic
approach that executed the workload based on availability of resources without
using any ML-based time predictions.

</details>


### [5] [Coherence-Aware Task Graph Modeling for Realistic Application](https://arxiv.org/abs/2509.09094)
*Guochu Xiong,Xiangzhong Luo,Weichen Liu*

Main category: cs.DC

TL;DR: CoTAM是一个针对多核系统中缓存一致性感知的任务图建模框架，通过解耦一致性影响、量化其权重并推断任务间依赖关系，为动态工作负载生成统一的任务图，弥补了静态方法与实际运行时行为之间的差距。


<details>
  <summary>Details</summary>
Motivation: 随着多核系统规模扩大，缓存一致性成为系统性能的关键因素。现有任务图建模方法要么依赖预定义图（不适用于动态应用），要么忽略一致性交互，导致设计假设与实际运行时行为存在差距。

Method: CoTAM框架通过三个步骤：1) 解耦缓存一致性影响与整体执行；2) 通过学习权重方案量化一致性影响；3) 推断任务间依赖关系以生成一致性感知的任务图。

Result: 大量实验表明，CoTAM优于隐式方法，能够有效弥合动态工作负载行为与现有设计之间的差距，证明了将缓存一致性纳入任务图建模对于准确和可推广的系统级分析的重要性。

Conclusion: CoTAM提供了一个统一的、一致性感知的任务图建模框架，能够更准确地反映实际运行时行为，为多核系统的性能分析和优化提供了更有效的工具。

Abstract: As multicore systems continue to scale, cache coherence has emerged as a
critical determinant of system performance, with coherence behavior and task
execution closely intertwined, reshaping inter-task dependencies. Task graph
modeling provides a structured way to capture such dependencies and serves as
the foundation for many system-level design strategies. However, these
strategies typically rely on predefined task graphs, while many real-world
applications lack explicit graphs and exhibit dynamic, data-dependent behavior,
limiting the effectiveness of static approaches. To address this, several task
graph modeling methods for realistic workloads have been developed. Yet, they
either rely on implicit techniques that use application-specific features
without producing explicit graphs, or they generate graphs tailored to fixed
scheduling models, which limits generality. More importantly, they often
overlook coherence interactions, creating a gap between design assumptions and
actual runtime behavior. To overcome these limitations, we propose CoTAM, a
Coherence-Aware Task Graph Modeling framework for realistic workloads that
constructs a unified task graph reflecting runtime behavior. CoTAM analyzes the
impact of coherence by decoupling its effects from overall execution,
quantifies its influence through a learned weighting scheme, and infers
inter-task dependencies for coherence-aware graph generation. Extensive
experiments show that CoTAM outperforms implicit methods, bridging the gap
between dynamic workload behavior and existing designs while demonstrating the
importance of incorporating cache coherence into task graph modeling for
accurate and generalizable system-level analysis.

</details>


### [6] [WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge](https://arxiv.org/abs/2509.09400)
*Valerio Besozzi,Enrico Fiasco,Marco Danelutto,Patrizio Dazzi*

Main category: cs.DC

TL;DR: 这篇论文比较了WebAssembly和unikernel基的MicroVM在无服务器边缘计算中的性能，发现WebAssembly在轻量函数冷启动时间更低，而Firecracker在复杂工作负载和I/O密集任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 边缘无服务器计算需要轻量执行环境以减少冷启动延迟，特别是在紧急边缘计算(UEC)场景下。需要比较不同技术方案的性能特点。

Method: 研究提出了Limes，一个基于Wasmtime的WebAssembly运行时，并将其与基于Firecracker的SPARE环境进行性能对比分析。测试了不同类型的无服务器工作负载。

Result: 结果显示WebAssembly在轻量函数上拥有更低的冷启动时间，但在复杂工作负载下性能受限。Firecracker提供更高但稳定的冷启动时间，并在执行性能上表现更好，特别是处理I/O密集型任务时。

Conclusion: 两种技术各有优势：WebAssembly适用于对冷启动时间敏感的轻量函数，而Firecracker更适合需要高性能执行和I/O操作的复杂工作负载。

Abstract: Serverless computing at the edge requires lightweight execution environments
to minimize cold start latency, especially in Urgent Edge Computing (UEC). This
paper compares WebAssembly and unikernel-based MicroVMs for serverless
workloads. We present Limes, a WebAssembly runtime built on Wasmtime, and
evaluate it against the Firecracker-based environment used in SPARE. Results
show that WebAssembly offers lower cold start times for lightweight functions
but suffers with complex workloads, while Firecracker provides higher, but
stable, cold starts and better execution performance, particularly for
I/O-heavy tasks.

</details>


### [7] [Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing](https://arxiv.org/abs/2509.09435)
*Houming Qiu,Kun Zhu,Dusit Niyato,Nguyen Cong Luong,Changyan Yi,Chen Dai*

Main category: cs.DC

TL;DR: 提出基于重心有理插值的近似编码分布式计算方案，解决现有CDC方案恢复阈值固定和数值不稳定问题，支持任意返回结果解码，在等待时间和近似精度方面优于现有方案


<details>
  <summary>Details</summary>
Motivation: 现有编码分布式计算(CDC)方案存在两个关键限制：需要固定恢复阈值才能成功解码，以及编码/解码函数存在极点导致解码不准确和数值不稳定

Method: 基于重心有理插值(BRI)的近似CDC方案，设计无极点的编码/解码函数，支持任意数量返回结果解码，并集成BRI梯度编码算法

Result: 实验结果显示该方案在等待时间和近似精度方面优于现有CDC方案，支持有限域和实数域计算，确保数值稳定性

Conclusion: 提出的BRI-based CDC方案有效解决了现有CDC方案的局限性，提供了更灵活的解码能力、更好的数值稳定性和更高的近似精度

Abstract: Collaborative mobile edge computing (MEC) has emerged as a promising paradigm
to enable low-capability edge nodes to cooperatively execute
computation-intensive tasks. However, straggling edge nodes (stragglers)
significantly degrade the performance of MEC systems by prolonging computation
latency. While coded distributed computing (CDC) as an effective technique is
widely adopted to mitigate straggler effects, existing CDC schemes exhibit two
critical limitations: (i) They cannot successfully decode the final result
unless the number of received results reaches a fixed recovery threshold, which
seriously restricts their flexibility; (ii) They suffer from inherent poles in
their encoding/decoding functions, leading to decoding inaccuracies and
numerical instability in the computational results. To address these
limitations, this paper proposes an approximated CDC scheme based on
barycentric rational interpolation. The proposed CDC scheme offers several
outstanding advantages. Firstly, it can decode the final result leveraging any
returned results from workers. Secondly, it supports computations over both
finite and real fields while ensuring numerical stability. Thirdly, its
encoding/decoding functions are free of poles, which not only enhances
approximation accuracy but also achieves flexible accuracy tuning. Fourthly, it
integrates a novel BRI-based gradient coding algorithm accelerating the
training process while providing robustness against stragglers. Finally,
experimental results reveal that the proposed scheme is superior to existing
CDC schemes in both waiting time and approximate accuracy.

</details>


### [8] [Weaker Assumptions for Asymmetric Trust](https://arxiv.org/abs/2509.09493)
*Ignacio Amores-Sesar,Christian Cachin,Juan Villacis*

Main category: cs.DC

TL;DR: 在不对称信任分布式系统中，本文提出了新的问题定义方法和算法，在比现有方案更弱的假设下实现了可靠广播和共识。


<details>
  <summary>Details</summary>
Motivation: 解决不对称信任模型中只满足经典一致性和可用性时无法解决基础问题的挑战，避免现有方案过于严格的假设影响不对称信任的优势。

Method: 提出了新的不对称问题定义方法，并基于此设计了可靠广播和共识算法，这些算法需要比之前更弱的假设条件。

Result: 新算法在更弱的假设下实现了不对称信任模型中的基础问题解决，保持了不对称信任的优势。

Conclusion: 该方法具有普遍性，可扩展到不对称信任系统中的其他核心问题，为分布式系统提供了更灵活的信任模型解决方案。

Abstract: In distributed systems with asymmetric trust, each participant is free to
make its own trust assumptions about others, captured by an asymmetric quorum
system. This contrasts with ordinary, symmetric quorum systems and threshold
models, where trust assumptions are uniformly shared among participants.
Fundamental problems like reliable broadcast and consensus are unsolvable in
the asymmetric model if quorum systems satisfy only the classical properties of
consistency and availability. Existing approaches overcome this by introducing
stronger assumptions. We show that some of these assumptions are overly
restrictive, so much so that they effectively eliminate the benefits of
asymmetric trust. To address this, we propose a new approach to characterize
asymmetric problems and, building upon it, present algorithms for reliable
broadcast and consensus that require weaker assumptions than previous
solutions. Our methods are general and can be extended to other core problems
in systems with asymmetric trust.

</details>


### [9] [TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes](https://arxiv.org/abs/2509.09525)
*Jialiang Huang,Teng Ma,Zheng Liu,Sixing Lin,Kang Chen,Jinlei Jiang,Xia Liao,Yingdi Shan,Yongwei Wu,Ning Zhang,Mengting Lu,Tao Ma,Haifeng Gong,Mingxing Zhang*

Main category: cs.DC

TL;DR: TrEnv是一个专为LLM代理优化的无服务器平台，通过可重用沙箱和内存模板等技术，显著降低了启动延迟和内存使用，相比现有系统性能提升显著


<details>
  <summary>Details</summary>
Motivation: 现有无服务器计算基础设施对LLM代理等新兴工作负载存在瓶颈，运行成本可达LLM API调用成本的70%，需要更高效的高密度无服务器平台

Method: 设计支持容器和VM环境的协同设计平台，采用可重用沙箱、内存模板、浏览器共享和页面缓存绕过机制来优化执行环境

Result: 容器环境下P99延迟降低7倍，内存使用减少48%；VM环境下P99延迟降低58%，内存节省61%，相比E2B等先进系统表现更优

Conclusion: TrEnv通过针对LLM代理独特需求的设计优化，有效解决了无服务器平台在支持新兴工作负载时的性能瓶颈问题

Abstract: Serverless computing provides dynamic scalability, but its infrastructure
overhead becomes a bottleneck for emerging workloads such as LLM agents, which
exhibit unpredictable invocation patterns and variable resource demands. Our
analysis shows that for these agents, the cost of running on serverless
platforms can reach up to 70% of the cost of LLM API calls. This finding
motivates the need for a more efficient, high-density serverless platform. We
present TrEnv, a co-designed serverless platform that supports both container-
and VM-based environments, optimized for the unique demands of LLM agents.
TrEnv reduces startup latency and memory usage through repurposable sandboxes
and memory templates, which enable fast reuse and restoration of execution
environments. To further reduce overhead in VM-based agent workloads, TrEnv
leverages browser sharing and a page cache bypassing mechanism. Evaluations
show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in
container-based settings, and achieves up to 58% lower P99 latency and 61%
memory savings for VM-based agents compared to state-of-the-art systems like
E2B.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [Implementation of a 8-bit Wallace Tree Multiplier](https://arxiv.org/abs/2509.09178)
*Ayan Biswas,Jimmy Jin*

Main category: cs.AR

TL;DR: 8位华莱士树乘法器的设计与实现，包括电路原理图、版图设计以及乘积累加单元的尝试


<details>
  <summary>Details</summary>
Motivation: 设计并行数字乘法器架构以最小化电路深度的最坏情况时间复杂度，实现O(log(n))的时间复杂度

Method: 使用Cadence Virtuoso在gpdk45工艺上设计8位华莱士树乘法器的原理图和版图，通过全加器和半加器电路减少每级部分积

Result: 成功完成了8位华莱士树乘法器的设计，并尝试了16位组合乘积累加单元的实现

Conclusion: 华莱士树乘法器架构能够有效降低乘法运算的时间复杂度，为高性能数字电路设计提供了可行的解决方案

Abstract: Wallace tree multipliers are a parallel digital multiplier architecture
designed to minimize the worst-case time complexity of the circuit depth
relative to the input size [1]. In particular, it seeks to perform long
multiplication in the binary sense, reducing as many partial products per stage
as possible through full and half adders circuits, achieving O(log(n)) where n
= bit length of input. This paper provides an overview of the design, progress
and methodology in the final project of ECE 55900, consisting of the schematic
and layout of a Wallace tree 8-bit input multiplier on the gpdk45 technology in
Cadence Virtuoso, as well as any design attempts prior to the final product.
This also includes our endeavors in designing the final MAC (Multiply
Accumulate) unit with undefined targets, which we chose to implement as a 16
bit combinational multiply-add.

</details>


### [11] [Combating the Memory Walls: Optimization Pathways for Long-Context Agentic LLM Inference](https://arxiv.org/abs/2509.09505)
*Haoran Wu,Can Xiao,Jiayi Nie,Xuan Guo,Binglei Lou,Jeffrey T. H. Wong,Zhiwen Mo,Cheng Zhang,Przemyslaw Forys,Wayne Luk,Hongxiang Fan,Jianyi Cheng,Timothy M. Jones,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: PLENA是一个硬件-软件协同设计的系统，针对长上下文LLM推理中的内存带宽和容量瓶颈问题，通过非对称量化方案、扁平化脉动阵列架构和完整软件栈优化，实现了比现有加速器高达8.5倍的利用率提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的AI代理在处理长上下文（如完整网页DOM、复杂工具调用轨迹）时面临严重的内存带宽和容量限制，导致计算单元利用率低下，需要专门的硬件优化方案。

Method: PLENA采用三种核心优化路径：1）支持非对称量化方案的高效硬件实现；2）具有FlashAttention原生支持的扁平化脉动阵列架构；3）完整的软件栈包括自定义ISA、编译器、周期模拟器和自动化设计空间探索流程。

Result: 模拟结果显示，PLENA相比现有加速器实现高达8.5倍的利用率提升，在相同乘法器数量和内存配置下，吞吐量比A100 GPU高2.24倍，比TPU v6e高3.85倍。

Conclusion: PLENA系统有效解决了长上下文LLM推理中的内存瓶颈问题，显著提升了硬件利用率和推理性能，该系统将开源发布。

Abstract: LLMs now form the backbone of AI agents for a diverse array of applications,
including tool use, command-line agents, and web or computer use agents. These
agentic LLM inference tasks are fundamentally different from chatbot-focused
inference -- they often have much larger context lengths to capture complex,
prolonged inputs, such as entire webpage DOMs or complicated tool call
trajectories. This, in turn, generates significant off-chip memory traffic for
the underlying hardware at the inference stage and causes the workload to be
constrained by two memory walls, namely the bandwidth and capacity memory
walls, preventing the on-chip compute units from achieving high utilization.
  In this paper, we introduce PLENA, a hardware-software co-designed system
that applies three core optimization pathways to tackle these challenges. PLENA
includes an efficient hardware implementation of compute and memory units
supporting an asymmetric quantization scheme. PLENA also features a novel
flattened systolic array architecture that has native support for
FlashAttention to tackle these memory walls in the scenario of inference
serving for long-context LLMs. Additionally, PLENA is developed with a complete
stack, including a custom ISA, a compiler, a cycle-emulated simulator, and an
automated design space exploration flow. The simulated results show that PLENA
achieves up to 8.5x higher utilization than existing accelerators, and delivers
2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the
TPU v6e, under the same multiplier count and memory settings. The full PLENA
system will also be open-sourced.

</details>
