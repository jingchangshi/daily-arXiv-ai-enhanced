<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.AR](#cs.AR) [Total: 31]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A Word Sampler for Well-Typed Functions](https://arxiv.org/abs/2512.01036)
*Breandan Considine*

Main category: cs.PL

TL;DR: 提出一种针对简单类型一阶函数式语言的精确采样器，能够从有限自动机接受的语言中均匀无放回地采样类型正确的函数


<details>
  <summary>Details</summary>
Motivation: 需要一种能够从类型正确的函数集合中均匀采样的方法，以支持程序分析、测试生成和语言研究等应用

Method: 通过固定参数可归约性将语法导向的类型系统归约到上下文无关文法，同时保持类型安全性和完备性

Result: 实现了从有限自动机接受的语言中均匀无放回地采样类型正确函数的能力，保持了形式语言的健壮元理论

Conclusion: 该方法成功地将类型系统与形式语言理论结合，为程序采样和分析提供了理论基础和实用工具

Abstract: We describe an exact sampler for a simply-typed, first-order functional programming language. Given an acyclic finite automaton, $α_{\varnothing}$, it samples a random function uniformly without replacement from well-typed functions in $\mathcal{L}(α_{\varnothing})$. This is achieved via a fixed-parameter tractable reduction from a syntax-directed type system to a context-free grammar, preserving type soundness and completeness w.r.t. $\mathcal{L}(α_{\varnothing})$, while retaining the robust metatheory of formal languages.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [A Parallel and Distributed Rust Library for Core Decomposition on Large Graphs](https://arxiv.org/abs/2512.00233)
*Davide Rucci,Sebastian Parfeniuc,Matteo Mordacchini,Emanuele Carlini,Alfredo Cuzzocrea,Patrizio Dazzi*

Main category: cs.DC

TL;DR: 将分布式k-core算法移植到共享内存系统，用Rust实现三个优化版本，在16线程上获得11倍加速，比Python实现快两个数量级


<details>
  <summary>Details</summary>
Motivation: 现代网络规模庞大，需要并行化的k-core分解算法来处理大规模图数据，现有高效算法主要是串行的，需要多核并行方法

Method: 将Montresor等人的分布式k-core算法适配到共享内存系统，用Rust实现三个版本：SequentialK（基线）、ParallelK（多线程消息传递）、FastK（进一步减少同步开销）

Result: 在道路网络、web图、社交网络等真实数据集上，FastK始终优于SequentialK和ParallelK，以及NetworkX的Python实现，在16线程上获得11倍加速，执行时间比Python实现快两个数量级

Conclusion: Rust实现的并行k-core分解算法能有效利用多核系统，显著提升大规模图分析性能，证明了共享内存并行化的有效性

Abstract: In this paper, we investigate the parallelization of $k$-core decomposition, a method used in graph analysis to identify cohesive substructures and assess node centrality. Although efficient sequential algorithms exist for this task, the scale of modern networks requires faster, multicore-ready approaches. To this end, we adapt a distributed $k$-core algorithm originally proposed by Montresor et al. to shared-memory systems and implement it in Rust, leveraging the language's strengths in concurrency and memory safety. We developed three progressively optimized versions: SequentialK as a baseline, ParallelK introducing multi-threaded message passing, and FastK further reducing synchronization overhead. Extensive experiments on real-world datasets, including road networks, web graphs, and social networks, show that FastK consistently outperforms both SequentialK and ParallelK, as well as a reference Python implementation available in the NetworkX library. Results indicate up to an 11x speedup on 16 threads and execution times up to two orders of magnitude faster than the Python implementation.

</details>


### [3] [Heimdall++: Optimizing GPU Utilization and Pipeline Parallelism for Efficient Single-Pulse Detection](https://arxiv.org/abs/2512.00398)
*Bingzheng Xia,Zujie Ren,Kuang Ma,Xiaoqian Li,Wenda Li,Shuibing He*

Main category: cs.DC

TL;DR: Heimdall++ 是 Heimdall GPU 加速单脉冲搜索工具的优化版本，通过细粒度GPU并行化、增强内存管理和多线程框架，解决了GPU利用率不足的问题，在单文件处理中实现2.66倍加速，多文件批处理中实现2.05倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着现代射电望远镜时空分辨率提高和数据量指数增长，实时单脉冲检测成为时域射电天文学的关键需求。Heimdall作为GPU加速单脉冲搜索工具，虽然比CPU方法有性能优势，但其顺序执行模型和中间处理阶段的资源争用限制了GPU利用率，导致吞吐量不足和计算延迟增加。

Method: 提出Heimdall++，采用细粒度GPU并行化、增强内存管理和多线程框架，解耦CPU绑定和GPU绑定的处理阶段，缓解GPU停滞问题，提高端到端效率。

Result: 在配备NVIDIA RTX 3080 Ti GPU的系统上评估，使用单个大规模观测文件和多个文件进行测试。Heimdall++在单文件处理中实现最高2.66倍加速，在多文件批处理中实现2.05倍加速，同时保持与原始Heimdall搜索结果的完全一致性。

Conclusion: Heimdall++通过优化GPU并行化和处理流水线，显著提升了单脉冲搜索的性能和效率，能够更好地满足现代射电天文大数据处理的实时性要求。

Abstract: With the increasing time and frequency resolution of modern radio telescopes and the exponential growth in observational data volumes, real-time single-pulse detection has become a critical requirement for time-domain radio astronomy. Heimdall, as a representative GPU-accelerated single-pulse search tool, offers substantial performance advantages over CPU-based approaches. However, its sequential execution model and resource contention in intermediate processing stages limit GPU utilization, leading to suboptimal throughput and increased computational latency. To address these limitations, we present Heimdall++, an optimized successor to Heimdall that incorporates fine-grained GPU parallelization, enhanced memory management, and a multi-threaded framework to decouple CPU-bound and GPU-bound processing stages. This design mitigates the GPU stall problem and improves end-to-end efficiency. We evaluated Heimdall++ on a system equipped with NVIDIA RTX 3080 Ti GPUs using both a single large-scale observational file and multiple files. Experimental results demonstrate that Heimdall++ achieves up to 2.66x speedup in single-file processing and 2.05x speedup in multi-file batch processing, while maintaining full consistency with the original Heimdall's search results.

</details>


### [4] [IslandRun: Privacy-Aware Multi-Objective Orchestration for Distributed AI Inference](https://arxiv.org/abs/2512.00595)
*Bala Siva Sai Akhil Malepati*

Main category: cs.DC

TL;DR: IslandRun是一个多目标编排系统，将计算资源视为跨越个人设备、私有边缘服务器和公共云的"岛屿"，通过智能路由和可逆匿名化实现隐私感知的分布式推理编排。


<details>
  <summary>Details</summary>
Motivation: 现代AI推理面临不可调和的矛盾：没有单一计算资源能同时最大化性能、保护隐私、最小化成本和保持信任。现有编排框架只优化单一维度，无法应对现实世界的异构性。

Method: 1. 将计算资源视为自主"岛屿"；2. 基于代理的路由机制；3. 分层岛屿组与差异化信任；4. 类型化占位符消毒以跨信任边界保持上下文语义；5. 数据局部性优先的路由策略。

Result: IslandRun建立了一个新的范式，用于在异构个人计算生态系统上进行隐私感知、去中心化的推理编排，能够同时优化多个目标。

Conclusion: 通过将计算资源视为岛屿、利用数据局部性、实施可逆匿名化，IslandRun解决了现代AI推理中多目标优化的根本矛盾，为隐私感知的分布式推理编排提供了新范式。

Abstract: Modern AI inference faces an irreducible tension: no single computational resource simultaneously maximizes performance, preserves privacy, minimizes cost, and maintains trust. Existing orchestration frameworks optimize single dimensions (Kubernetes prioritizes latency, federated learning preserves privacy, edge computing reduces network distance), creating solutions that struggle under real-world heterogeneity. We present IslandRun, a multi-objective orchestration system that treats computational resources as autonomous "islands" spanning personal devices, private edge servers, and public cloud. Our key insights: (1) request-level heterogeneity demands policy-constrained multi-objective optimization, (2) data locality enables routing compute to data rather than data to compute, and (3) typed placeholder sanitization preserves context semantics across trust boundaries. IslandRun introduces agent-based routing, tiered island groups with differential trust, and reversible anonymization. This establishes a new paradigm for privacy-aware, decentralized inference orchestration across heterogeneous personal computing ecosystems.

</details>


### [5] [Steady and Energy-Efficient Multi-Hop Clustering for Flying Ad-Hoc Networks (FANETs)](https://arxiv.org/abs/2512.00623)
*Basilis Mamalis,Marios Perlitis*

Main category: cs.DC

TL;DR: 提出一种用于无人机自组织网络(FANETs)的多跳分簇算法，通过移动感知分簇、能量中心簇头选择和地面站辅助的簇维护机制，提高集群稳定性和通信效率。


<details>
  <summary>Details</summary>
Motivation: 无人机自组织网络面临高移动性、有限能量资源和动态拓扑等独特挑战，需要设计稳定的能量高效分簇算法来提升网络性能。

Method: 提出多跳分簇算法，包括：1）构建稳定的多跳集群，选择具有高稳定性和高能量的簇头，并考虑其邻域稳定性；2）采用地面站辅助的簇维护管理机制。

Result: 基于扩展仿真的实验结果表明，该方法在集群稳定性、通信开销和安全弹性方面显著优于现有方案。

Conclusion: 提出的多跳分簇算法能够有效解决FANETs中的挑战，通过移动感知和能量优化的簇头选择以及地面站辅助维护，实现了更稳定、高效的网络通信。

Abstract: Flying Ad-hoc Networks (FANETs), formed by Unmanned Aerial Vehicles (UAVs), represent an emerging and promising communication paradigm. These networks face unique challenges due to UAVs high mobility, limited energy resources, and dynamic topology. In this work, we propose a novel multi-hop clustering algorithm aimed at creating stable, energy-efficient clusters in FANET environments. The proposed solution enhances cluster longevity and communication efficiency through mobility-aware clustering, energy-centric cluster head (CH) selection, and a ground station(GS)-assisted cluster maintenance management mechanism. First, steady multi-hop clusters are constructed, having CHs with not only high stability and high energy but also with steady and high-energy neighboring areas, and then a proper GS-assisted cluster maintenance mechanism is applied. Experimental results, based on extended simulations, demonstrate that our approach outperforms existing schemes significantly, in terms of cluster stability, communication overhead, and security resilience.

</details>


### [6] [FlexiWalker: Extensible GPU Framework for Efficient Dynamic Random Walks with Runtime Adaptation](https://arxiv.org/abs/2512.00705)
*Seongyeon Park,Jaeyong Song,Changmin Shin,Sukjin Kim,Junguk Hong,Jinho Lee*

Main category: cs.DC

TL;DR: FlexiWalker：首个GPU框架，为动态随机游走提供高效、通用的支持，通过优化采样内核、轻量级成本模型和编译时组件，显著超越现有CPU/GPU基线。


<details>
  <summary>Details</summary>
Motivation: 动态随机游走因运行时依赖的转移概率破坏了现有CPU/GPU静态随机游走的预计算优化策略，导致现有框架性能不佳且无法适应工作负载多样性，需要手动调优内核。

Method: 1) 设计拒绝采样和蓄水池采样的高性能内核，消除全局归约、冗余内存访问和随机数生成；2) 采用轻量级一阶成本模型在运行时为每个节点选择最佳采样策略；3) 引入编译时组件自动将用户提供的游走逻辑特化为优化构建块。

Result: 在真实世界图的各种动态随机游走工作负载上，FlexiWalker分别比最佳已发布的CPU和GPU基线几何平均提升73.44倍和5.91倍，同时能成功执行先前系统无法支持的工作负载。

Conclusion: FlexiWalker是首个为动态随机游走提供高效、通用GPU支持的框架，通过创新的采样内核、运行时成本模型和编译时优化，显著提升了性能并增强了可用性。

Abstract: Dynamic random walks are fundamental to various graph analysis applications, offering advantages by adapting to evolving graph properties. Their runtime-dependent transition probabilities break down the pre-computation strategy that underpins most existing CPU and GPU static random walk optimizations. This leaves practitioners suffering from suboptimal frameworks and having to write hand-tuned kernels that do not adapt to workload diversity. To handle this issue, we present FlexiWalker, the first GPU framework that delivers efficient, workload-generic support for dynamic random walks. Our design-space study shows that rejection sampling and reservoir sampling are more suitable than other sampling techniques under massive parallelism. Thus, we devise (i) new high-performance kernels for them that eliminate global reductions, redundant memory accesses, and random-number generation. Given the necessity of choosing the best-fitting sampling strategy at runtime, we adopt (ii) a lightweight first-order cost model that selects the faster kernel per node at runtime. To enhance usability, we introduce (iii) a compile-time component that automatically specializes user-supplied walk logic into optimized building blocks. On various dynamic random walk workloads with real-world graphs, FlexiWalker outperforms the best published CPU/GPU baselines by geometric means of 73.44x and 5.91x, respectively, while successfully executing workloads that prior systems cannot support. We open-source FlexiWalker in https://github.com/AIS-SNU/FlexiWalker.

</details>


### [7] [SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving](https://arxiv.org/abs/2512.00719)
*Bohan Zhao,Zane Cao,Yongchao He*

Main category: cs.DC

TL;DR: SIMPLE是一个解决大语言模型采样瓶颈的系统，通过CPU端序列并行采样和推测性热词采样技术，将采样从GPU卸载到CPU，显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型采用张量并行和流水线并行扩展，数据平面（注意力/GEMM和KV缓存）已得到优化，但采样（将logits转换为tokens的决策平面）成为新瓶颈。采样既不能随TP扩展，也不能在PP阶段间平衡，导致其在迭代时间中的占比随GPU加速而增长，并限制流水线频率。

Method: SIMPLE包含三个关键技术：1）序列并行采样：沿批次维度分片工作，消除词汇轴集体通信；2）CPU端算法：采用列式惩罚和截断优先过滤实现单次线性时间内核；3）推测性热词采样：在小热词集上采样并通过拒绝校正保证正确性，使用简单尺寸模型选择最大化吞吐量的热词大小。

Result: SIMPLE将端到端吞吐量提升高达96%，P95延迟降低20-65%。系统无需用户端代码更改，可与现有数据平面优化组合，解锁随未来GPU代际增长的扩展效益。

Conclusion: SIMPLE通过将采样解耦为CPU端服务，解决了大语言模型扩展中的采样瓶颈问题，使采样回归到次要的隐藏角色，为未来GPU代际的扩展提供了可组合的解决方案。

Abstract: As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations.

</details>


### [8] [Elastic Mixture of Rank-Wise Experts for Knowledge Reuse in Federated Fine-Tuning](https://arxiv.org/abs/2512.00902)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.DC

TL;DR: SmartFed是一个资源高效的联邦微调框架，通过重用现有LoRA模块的知识，避免从头训练，使用MoRE分解和EEQA分配机制，显著提升模型性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 联邦微调虽然能保护数据隐私，但其高计算和通信成本阻碍了在资源受限设备上的部署，需要更高效的解决方案。

Method: 提出SmartFed框架：1）重用现有LoRA模块知识；2）引入MoRE（Mixture of Rank-Wise Experts）将LoRA模块分解为细粒度的秩级专家；3）提出EEQA（Elastic Expert Quota Allocation）根据参数矩阵对性能的贡献自适应分配专家容量。

Result: 在多个基准测试上的广泛评估表明，SmartFed在模型性能和训练效率方面显著优于现有方法。

Conclusion: SmartFed通过智能重用知识和优化资源分配，为资源受限环境下的联邦微调提供了高效解决方案。

Abstract: Federated fine-tuning offers a promising solution for adapting Large Language Models (LLMs) to downstream tasks while safeguarding data privacy. However, its high computational and communication demands hinder its deployment on resource-constrained devices. In this paper, we propose SmartFed, a resource-efficient federated fine-tuning framework. SmartFed intelligently reuses knowledge embedded in existing LoRA modules, eliminating the need for expensive training from scratch when adapting LLMs to new tasks. To effectively exploit this knowledge and ensure scalability, we introduce the Mixture of Rank-Wise Experts (MoRE). MoRE decomposes LoRA modules into fine-grained rank-level experts. These experts are selectively activated and combined based on input semantics and resource budgets. Moreover, to optimize resource utilization, we present the Elastic Expert Quota Allocation (EEQA). EEQA adaptively allocates expert capacity across parameter matrices based on their contribution to model performance, focusing computing resources on the critical experts. Extensive evaluations across multiple benchmarks demonstrate that SmartFed significantly outperforms existing methods in model performance and training efficiency.

</details>


### [9] [Joint Partitioning and Placement of Foundation Models for Real-Time Edge AI](https://arxiv.org/abs/2512.01039)
*Aladin Djuhera,Fernando Koch,Alecio Binotto*

Main category: cs.DC

TL;DR: 提出一个动态可重构的框架，用于在异构边缘环境中优化大规模基础模型的推理部署，通过运行时调整模型层的空间放置和内部分割来适应资源波动。


<details>
  <summary>Details</summary>
Motivation: 在异构边缘环境中部署大规模基础模型时，静态划分模型层的方法假设计算和网络资源具有时间稳定性，这与现实部署中的资源波动性不匹配，需要更灵活的编排方法。

Method: 引入一个框架，将基础模型的空间放置和内部分割提升为运行时解析的构造，将编排问题形式化为受延迟、利用率和隐私梯度约束的层分配优化，集成模型感知能力分析、动态图重新分区和重新分配。

Result: 提出了架构和算法组件，并在6G多接入边缘计算中展示了代表性用例，实现了对基础设施波动的响应式推理组合。

Conclusion: 该框架为异构边缘环境中的大规模基础模型推理提供了动态可重构的编排解决方案，能够适应资源波动并优化性能指标。

Abstract: Inference over large-scale foundation models within heterogeneous edge environments necessitates a fundamentally reconfigurable orchestration substrate. Static partitioning of model layers presumes temporal stability across compute and network resources, which is misaligned with the volatility of real-world deployments. We introduce a framework in which both the spatial placement and internal segmentation of foundation models are elevated to runtime-resolved constructs. The orchestration problem is formalized as a constrained optimization over layer-wise assignments, subject to evolving latency, utilization, and privacy gradients. The framework implements reactive inference composition responsive to infrastructural fluctuations by integrating model-aware capacity profiling with dynamic graph re-partitioning and reallocation. We introduce architectural and algorithmic components, along with a representative use case in 6G multi-access edge computing.

</details>


### [10] [Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity](https://arxiv.org/abs/2512.01357)
*Wenbin Zhu,Zhaoyan Shen,Zili Shao,Hongjun Dai,Feng Chen*

Main category: cs.DC

TL;DR: Tangram通过GPU内存复用加速Serverless LLM加载，减少冷启动延迟，实现最高6.2倍加载加速和23-55%的TTFT降低


<details>
  <summary>Details</summary>
Motivation: Serverless LLM部署中的冷启动延迟（特别是模型加载阶段）已成为关键性能瓶颈，随着模型规模线性增长，严重限制了大规模LLM服务的实际部署

Method: 提出Tangram系统，包含三个关键技术：统一GPU内存池实现跨模型张量级参数共享、按需KV缓存分配进行动态内存管理、GPU亲和性感知调度最大化资源利用率

Result: 实验显示Tangram实现最高6.2倍的加载加速，冷启动期间Time-To-First-Token (TTFT) 降低23-55%，优于现有最优方法

Conclusion: Tangram通过高效的GPU内存复用技术，有效解决了Serverless LLM平台中内存使用效率低下和冷启动问题的关键挑战

Abstract: Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods.

</details>


### [11] [Delta Sum Learning: an approach for fast and global convergence in Gossip Learning](https://arxiv.org/abs/2512.01549)
*Tom Goethals,Merlijn Sebrechts,Stijn De Schrijver,Filip De Turck,Bruno Volckaert*

Main category: cs.DC

TL;DR: 本文提出Delta Sum Learning方法改进Gossip Learning中的聚合操作，并在基于Open Application Model的去中心化编排框架中实现，显著提升了大规模节点下的全局收敛性和准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习和Gossip Learning中常用的平均聚合方法对模型准确性和全局收敛性不理想，且边缘计算环境中缺乏使用声明式方法（如Kubernetes清单）部署学习工作负载的选项。

Method: 提出Delta Sum Learning方法改进Gossip Learning的基本聚合操作，并在基于Open Application Model的去中心化编排框架中实现，支持动态节点发现和意图驱动的多工作负载应用部署。

Result: 在10节点拓扑中，Delta Sum性能与替代集成方法相当；在扩展到50节点时，全局准确性下降减少58%；相比替代方法在线性损失下，Delta Sum在有限连接性下表现出对数级准确性损失。

Conclusion: Delta Sum Learning在去中心化边缘学习中表现出强大的全局收敛性，随着拓扑规模增大，准确性损失呈对数增长而非线性增长，显著优于现有方法。

Abstract: Federated Learning is a popular approach for distributed learning due to its security and computational benefits. With the advent of powerful devices in the network edge, Gossip Learning further decentralizes Federated Learning by removing centralized integration and relying fully on peer to peer updates. However, the averaging methods generally used in both Federated and Gossip Learning are not ideal for model accuracy and global convergence. Additionally, there are few options to deploy Learning workloads in the edge as part of a larger application using a declarative approach such as Kubernetes manifests. This paper proposes Delta Sum Learning as a method to improve the basic aggregation operation in Gossip Learning, and implements it in a decentralized orchestration framework based on Open Application Model, which allows for dynamic node discovery and intent-driven deployment of multi-workload applications. Evaluation results show that Delta Sum performance is on par with alternative integration methods for 10 node topologies, but results in a 58% lower global accuracy drop when scaling to 50 nodes. Overall, it shows strong global convergence and a logarithmic loss of accuracy with increasing topology size compared to a linear loss for alternatives under limited connectivity.

</details>


### [12] [StarDist: A Code Generator for Distributed Graph Algorithms](https://arxiv.org/abs/2512.01646)
*Barenya Kumar Nandy,Rupesh Nasre*

Main category: cs.DC

TL;DR: StarPlat是一个分布式图计算框架，通过分析-转换框架优化通信模式，利用聚合通信和缓存技术提升性能，在SSSP算法上优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的关系数据通常以图结构表示，但随着图规模增大，不规则访问模式、NUMA架构和物理内存限制使得传统顺序/共享内存框架难以扩展，需要分布式内存解决方案。

Method: 1. 开发分析-转换框架，识别节点与邻居迭代中的通用语义模式；2. 重新排序邻域访问模式以聚合通信；3. 在归约结构中利用机会性缓存避免通信；4. 基于Open MPI的被动RMA构建优化的批量归约底层。

Result: 优化后的StarPlat分布式后端在多个大数据图的单源最短路径(SSSP)算法上，性能优于d-Galois 2.05倍，优于DRONE 1.44倍。

Conclusion: StarPlat的分析-转换框架通过通信聚合和缓存优化，有效解决了分布式图算法的扩展性问题，显著提升了大规模图计算的性能。

Abstract: Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs.

</details>


### [13] [Trace-based, time-resolved analysis of MPI application performance using standard metrics](https://arxiv.org/abs/2512.01764)
*Kingshuk Haldar*

Main category: cs.DC

TL;DR: 提出基于时间窗口的MPI性能指标计算方法，通过离散化执行轨迹来揭示瞬态性能瓶颈，相比传统全局聚合指标能更好地发现局部性能问题。


<details>
  <summary>Details</summary>
Motivation: MPI应用详细轨迹分析对性能工程至关重要，但轨迹规模增长和复杂通信行为使得全面可视化检查变得不切实际。现有工具的时间聚合指标可能掩盖瞬态性能瓶颈。

Method: 将执行轨迹离散化为固定或自适应时间窗口，处理后验Paraver轨迹，重建关键执行路径并处理常见事件异常（如时钟不一致和未匹配MPI事件），为每个窗口稳健计算性能指标。

Result: 在合成基准测试和实际应用（LaMEM和ls1-MarDyn）上的评估表明，时间分辨指标能揭示被全局聚合掩盖的局部性能瓶颈，提供轻量级且可扩展的替代方案。

Conclusion: 该方法通过时间窗口分析MPI性能指标，有效暴露瞬态性能瓶颈，即使在轨迹可视化不切实际的情况下，也能提供有价值的性能洞察。

Abstract: Detailed trace analysis of MPI applications is essential for performance engineering, but growing trace sizes and complex communication behaviour often render comprehensive visual inspection impractical. This work presents a trace-based calculation of time-resolved values of standard MPI performance metrics, load balance, serialisation, and transfer efficiency, by discretising execution traces into fixed or adaptive time segments. The implementation processes Paraver traces postmortem, reconstructing critical execution paths and handling common event anomalies, such as clock inconsistencies and unmatched MPI events, to robustly calculate metrics for each segment. The calculated per-window metric values expose transient performance bottlenecks that the timeaggregated metrics from existing tools may conceal. Evaluations on a synthetic benchmark and real-world applications (LaMEM and ls1-MarDyn) demonstrate how time-resolved metrics reveal localised performance bottlenecks obscured by global aggregates, offering a lightweight and scalable alternative even when trace visualisation is impractical.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [VeriPy - A New Python-Based Approach for SDR Pipelined/Unrolled Hardware Accelerator Generation](https://arxiv.org/abs/2512.00006)
*Yuqin Zhao,Linghui Ye,Haihang Xia,Luke Seed,Tiantai Deng*

Main category: cs.AR

TL;DR: VeriPy：一个面向SDR工程师的Python HLS工具，无需硬件知识即可生成Verilog硬件加速器设计，性能优于Vivado HLS，接近手工编码实现。


<details>
  <summary>Details</summary>
Motivation: SDR应用需要硬件加速器提升性能，但通信工程师缺乏硬件专业知识。现有HLS工具门槛高，需要详细的硬件知识和HDL技能，因此需要专门为SDR工程师设计的高层综合工具。

Method: 开发基于Python的HLS工具VeriPy，支持生成两种主流硬件加速器架构（展开设计和流水线设计），提供自动测试平台生成、可扩展硬件库、性能和资源估计，并在算法和硬件层面进行优化。

Result: VeriPy生成的硬件设计比pragma优化的Vivado HLS设计快70%（频率更高），资源消耗略高但合理，性能与手工编码实现相当。代码复杂度低，无需pragma或底层硬件知识。

Conclusion: VeriPy成功降低了SDR工程师的硬件开发门槛，无需硬件专业知识即可生成高性能硬件加速器设计，为通信系统开发提供了高效的工具支持。

Abstract: Software-defined radio (SDR) plays an important role in the communication field by providing a flexible and customized communication system for different purposes according to the needs. To enhance the performance of SDR applications, hardware accelerators have been widely deployed in recent years. In facing this obstacle, a necessity arises for a high-level synthesis (HLS) tool specifically designed for communication engineers without detailed hardware knowledge. To lower the barrier between SDR engineers and hardware development, this work proposed a Python-based HLS tool, VeriPy, which can generate both mainstream architecture for hardware accelerators in Verilog specifically for SDR designs including unrolled design and pipelined design, requiring no detailed digital hardware knowledge or Hardware Description Languages (HDL). Furthermore, VeriPy supports automatic testbench generation with random input stimulus, an extensible hardware library, performance and resource estimation, and offers strong optimisation potential at both the algorithmic and digital hardware levels. The generated hardware design by VeriPy can achieve up to 70% faster operating frequency compared to pragma-optimised Vivado HLS designs with a reasonably higher resource con-sumption while delivering comparable performance and resource consumption to hand-coded implementations. Regarding code complexity, VeriPy requires no pragmas, completely eliminating the need for low-level hardware knowledge. For straightforward algorithms, the input code length remains comparable to that of Vivado HLS.

</details>


### [15] [Partial Cross-Compilation and Mixed Execution for Accelerating Dynamic Binary Translation](https://arxiv.org/abs/2512.00487)
*Yuhao Gu,Zhongchun Zheng,Nong Xiao,Yutong Lu,Xianwei Zhang*

Main category: cs.AR

TL;DR: 提出混合执行系统，结合编译与仿真，通过选择性函数卸载机制减少动态二进制翻译开销，实现最高13倍加速


<details>
  <summary>Details</summary>
Motivation: 随着指令集架构多样化，跨ISA程序执行变得普遍。动态二进制翻译性能差，交叉编译受限于"全有或全无"模式，完全交叉编译常不可行，导致程序面临高仿真开销

Method: 提出混合执行系统，结合编译与仿真，采用选择性函数卸载机制建立跨环境调用通道，将符合条件的函数卸载到主机进行本地执行以减少DBT开销。基于LLVM和QEMU构建，自动适用于应用程序和库

Result: 评估显示，相比现有DBT系统，该系统实现了最高13倍的加速，具有强大的实用价值

Conclusion: 提出的混合执行系统通过选择性函数卸载机制有效解决了跨ISA执行中的性能问题，在保持兼容性的同时显著提升了执行效率

Abstract: With the growing diversity of instruction set architectures (ISAs), cross-ISA program execution has become common. Dynamic binary translation (DBT) is the main solution but suffers from poor performance. Cross-compilation avoids emulation costs but is constrained by an "all-or-nothing" model-programs are either fully cross-compiled or entirely emulated. Complete cross-compilation is often unfeasible due to ISA-specific code or missing dependencies, leaving programs with high emulation overhead.
  We propose a hybrid execution system that combines compilation and emulation, featuring a selective function offloading mechanism. This mechanism establishes cross-environment calling channels, offloading eligible functions to the host for native execution to reduce DBT overhead. Key optimizations address offloading costs, enabling efficient hybrid operation. Built on LLVM and QEMU, the system works automatically for both applications and libraries. Evaluations show it achieves up to 13x speedups over existing DBT, with strong practical value.

</details>


### [16] [Architect in the Loop Agentic Hardware Design and Verification](https://arxiv.org/abs/2512.00016)
*Mubarek Mohammed*

Main category: cs.AR

TL;DR: 提出基于智能代理的自动化处理器设计与验证框架，利用大语言模型分层生成HDL代码和测试，结合工程师指导完成处理器设计验证流程。


<details>
  <summary>Details</summary>
Motivation: 硬件设计复杂度日益增加，需要改进设计与验证方法。现有AI自动化方案多针对小型组件，缺乏完整的处理器设计自动化方案。

Method: 采用智能代理框架，基于分层模块化设计原则，将处理器分解为子组件，使用大语言模型（如Gemini-pro和GPT-5-mini）生成HDL代码和cocotb测试，结合工程师指导进行调试和综合。

Result: 成功设计了两种处理器：类似LEGv8的处理器在DE-10 Lite FPGA上验证、综合并编程；类似RISC-V的32位处理器完成设计和验证。每个处理器约使用百万推理token，成本效益高。

Conclusion: 该方法实现了可扩展的自动化硬件设计验证，无需专用硬件，成本效益高，未来可扩展到片上系统设计。

Abstract: The ever increasing complexity of the hardware design process demands improved hardware design and verification methodologies. With the advent of generative AI various attempts have been made to automate parts of the design and verification process. Large language models (LLMs) as well as specialized models generate hdl and testbenches for small components, having a few leaf level components. However, there are only a few attempts to automate the entire processor design process. Hardware design demands hierarchical and modular design processes. We utilized this best practice systematically and effectively. We propose agentic automated processor design and verification with engineers in the loop. The agent with optional specification tries to break down the design into sub-components, generate HDL and cocotb tests, and verifies the components involving engineer guidance, especially during debugging and synthesis. We designed various digital systems using this approach. However, we selected two simple processors for demonstration purposes in this work. The first one is a LEGv8 like a simple processor verified, synthesized and programmed for the DE-10 Lite FPGA. The second one is a RISC-V like 32-bit processor designed and verified in similar manner and synthesized. However, it is not programmed into the DE-10 Lite. This process is accomplished usually using around a million inference tokens per processor, using a combination of reasoning (e.g gemini-pro) and non-reasoning models (eg. gpt-5-mini) based on the complexity of the task. This indicates that hardware design and verification experimentation can be done cost effectively without using any specialized hardware. The approach is scalable, we even attempted system-on-chip, which we want to experiment in our future work.

</details>


### [17] [Hardware-Aware DNN Compression for Homogeneous Edge Devices](https://arxiv.org/abs/2512.00017)
*Kunlong Zhang,Guiying Li,Ning Lu,Peng Yang,Ke Tang*

Main category: cs.AR

TL;DR: HDAP提出了一种针对同构边缘设备的硬件感知DNN压缩框架，通过设备聚类和代理评估解决大规模设备性能差异问题，实现最优平均性能。


<details>
  <summary>Details</summary>
Motivation: 同构边缘设备在部署后因用户配置、环境条件、制造差异、电池退化等因素导致性能差异，现有DNN压缩方法未考虑此场景，无法保证在所有设备上的压缩效果。

Method: HDAP将设备聚类为几个设备簇，大幅减少需要评估的设备数量，并使用基于代理的评估替代实时硬件评估，实现硬件感知的DNN压缩。

Result: 在多种设备类型和任务类型上的实验表明，HDAP相比最先进方法能实现更低的平均延迟和竞争性准确率，显著加速（如ResNet50在1.0G FLOPs下达到2.86倍加速）。

Conclusion: HDAP为同构边缘设备提供了可扩展、高性能的DNN部署解决方案，有效解决了大规模同构设备性能差异下的模型压缩问题。

Abstract: Deploying deep neural networks (DNNs) across homogeneous edge devices (the devices with the same SKU labeled by the manufacturer) often assumes identical performance among them. However, once a device model is widely deployed, the performance of each device becomes different after a period of running. This is caused by the differences in user configurations, environmental conditions, manufacturing variances, battery degradation, etc. Existing DNN compression methods have not taken this scenario into consideration and can not guarantee good compression results in all homogeneous edge devices. To address this, we propose Homogeneous-Device Aware Pruning (HDAP), a hardware-aware DNN compression framework explicitly designed for homogeneous edge devices, aiming to achieve optimal average performance of the compressed model across all devices. To deal with the difficulty of time-consuming hardware-aware evaluations for thousands or millions of homogeneous edge devices, HDAP partitions all the devices into several device clusters, which can dramatically reduce the number of devices to evaluate and use the surrogate-based evaluation instead of hardware evaluation in real-time. Extensive experiments on multiple device types (Jetson Xavier NX and Jetson Nano) and task types (image classification with ResNet50, MobileNetV1, ResNet56, VGG16; object detection with YOLOv8n) demonstrate that HDAP consistently achieves lower average latency and competitive accuracy compared to state-of-the-art methods, with significant speedups (e.g., 2.86$\times$ on ResNet50 at 1.0G FLOPs). HDAP offers an effective solution for scalable, high-performance DNN deployment methods for homogeneous edge devices.

</details>


### [18] [Large Language Model for Verilog Code Generation: Literature Review and the Road Ahead](https://arxiv.org/abs/2512.00020)
*Guang Yang,Wei Zheng,Xiang Chen,Dong Liang,Peng Hu,Yukui Yang,Shaohang Peng,Zhenghan Li,Jiahui Feng,Xiao Wei,Kexin Sun,Deyuan Ma,Haotian Cheng,Yiheng Shen,Xing Hu,Terry Yue Zhuo,David Lo*

Main category: cs.AR

TL;DR: 这篇论文是关于大语言模型在Verilog代码生成领域的系统性文献综述，涵盖了102篇相关研究，分析了现有方法的有效性、局限性，并为未来研究提供了路线图。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成成为软件工程和人工智能交叉领域的重要研究方向，Verilog作为硬件描述语言在数字电路设计中具有基础性作用。尽管已有大量研究探索LLM在Verilog生成中的应用，但缺乏对这些发展的系统性综述，本文旨在填补这一空白。

Method: 通过系统性文献综述方法，收集了来自SE、AI和EDA领域的70篇会议/期刊论文和32篇高质量预印本论文（总计102篇），通过回答四个关键研究问题来组织分析框架。

Result: 综述识别了用于Verilog生成的LLM模型，检查了评估中使用的数据集和指标，对Verilog生成技术进行了分类，并分析了Verilog生成的LLM对齐方法，同时指出了现有研究的局限性。

Conclusion: 论文为LLM辅助硬件设计领域提供了全面的研究现状分析，识别了现有方法的不足，并提出了未来研究的路线图，有助于推动自动化硬件设计的发展。

Abstract: Code generation has emerged as a critical research area at the intersection of Software Engineering (SE) and Artificial Intelligence (AI), attracting significant attention from both academia and industry. Within this broader landscape, Verilog, as a representative hardware description language (HDL), plays a fundamental role in digital circuit design and verification, making its automated generation particularly significant for Electronic Design Automation (EDA). Consequently, recent research has increasingly focused on applying Large Language Models (LLMs) to Verilog code generation, particularly at the Register Transfer Level (RTL), exploring how these AI-driven techniques can be effectively integrated into hardware design workflows. Despite substantial research efforts have explored LLM applications in this domain, a comprehensive survey synthesizing these developments remains absent from the literature. This review fill addresses this gap by providing a systematic literature review of LLM-based methods for Verilog code generation, examining their effectiveness, limitations, and potential for advancing automated hardware design. The review encompasses research work from conferences and journals in the fields of SE, AI, and EDA, encompassing 70 papers published on venues, along with 32 high-quality preprint papers, bringing the total to 102 papers. By answering four key research questions, we aim to (1) identify the LLMs used for Verilog generation, (2) examine the datasets and metrics employed in evaluation, (3) categorize the techniques proposed for Verilog generation, and (4) analyze LLM alignment approaches for Verilog generation. Based on our findings, we have identified a series of limitations of existing studies. Finally, we have outlined a roadmap highlighting potential opportunities for future research endeavors in LLM-assisted hardware design.

</details>


### [19] [ML-PCM : Machine Learning Technique for Write Optimization in Phase Change Memory (PCM)](https://arxiv.org/abs/2512.00026)
*Mahek Desai,Rowena Quinn,Marjan Asadinia*

Main category: cs.AR

TL;DR: 使用神经网络模型预测相变存储器(PCM)的写入延迟、能耗和耐久性，以优化性能并解决PCM寿命短和能耗高的问题


<details>
  <summary>Details</summary>
Motivation: 随着DRAM等晶体管内存技术接近可扩展性极限，需要探索替代存储方案。PCM因其可扩展性、快速访问速度和零泄漏功率而备受关注，但其有限的寿命（由于写入操作中的材料退化）和高能耗阻碍了广泛应用。

Method: 提出使用神经网络模型来预测关键参数（写入延迟、能耗和耐久性），通过监测实时操作条件和设备特性来优化PCM性能并确定最佳写入设置。

Result: 神经网络预测取得了显著改进：耐久性的平均绝对百分比误差(MAPE)为0.0073%，总写入延迟为0.23%，总写入能量为4.92%。

Conclusion: 通过神经网络预测PCM关键参数可以显著提高其性能和效率，使PCM成为频繁写入操作应用中更实用和高效的数据存储选择。

Abstract: As transistor-based memory technologies like dynamic random access memory (DRAM) approach their scalability limits, the need to explore alternative storage solutions becomes increasingly urgent. Phase-change memory (PCM) has gained attention as a promising option due to its scalability, fast access speeds, and zero leakage power compared to conventional memory systems. However, despite these advantages, PCM faces several challenges that impede its broader adoption, particularly its limited lifespan due to material degradation during write operations, as well as the high energy demands of these processes. For PCM to become a viable storage alternative, enhancing its endurance and reducing the energy required for write operations are essential. This paper proposes the use of a neural network (NN) model to predict critical parameters such as write latency, energy consumption, and endurance by monitoring real-time operating conditions and device characteristics. These predictions are key to improving PCM performance and identifying optimal write settings, making PCM a more practical and efficient option for data storage in applications with frequent write operations. Our approach leads to significant improvements, with NN predictions achieving a Mean Absolute Percentage Error (MAPE) of 0.0073% for endurance, 0.23% for total write latency, and 4.92% for total write energy.

</details>


### [20] [Analysis of Single Event Induced Bit Faults in a Deep Neural Network Accelerator Pipeline](https://arxiv.org/abs/2512.00028)
*Naïn Jonckers,Toon Vinck,Peter Karsmakers,Jeffrey Prinzie*

Main category: cs.AR

TL;DR: 该论文分析了辐射粒子引起的故障对基于脉动阵列的DNN加速器计算流水线的影响，通过RTL级故障注入模拟评估硬件模块敏感性，并提出了高效的低开销故障缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI和DNN应用的快速增长，专用DNN加速器在辐射恶劣环境（如太空、核设施）中面临辐射粒子引发故障的挑战，需要评估其可靠性并设计有效的容错方案。

Method: 采用RTL级故障注入模拟方法，分析脉动阵列DNN加速器计算流水线对单比特故障的敏感性，使用MNIST和CIFAR-10数据集评估三种不同DNN工作负载。

Result: 提出了故障传播概率P(f_non-crit)和误分类概率P(f_crit)两个敏感性指标，量化了不同寄存器组对故障的敏感性，为高效硬件加固提供了依据。

Conclusion: 基于敏感性分析结果，设计了一种在面积和功耗开销方面都高效的故障缓解策略，能够有效硬化SA-DNN加速器以应对辐射环境。

Abstract: In recent years, the increased interest and the growth in application domains of Artificial Intelligence (AI), and more specifically Deep Neural Networks (DNNs), has led to an extensive usage of domain specific DNN accelerator processors to improve the computational efficiency of DNN inference. However, like any digital circuit, these processors are prone to faults induced by radiation particles such as heavy ions, protons, etc., making their use in harsh radiation environments a challenge. This work presents an in-depth analysis of the impact of such faults on the computational pipeline of a Systolic Array based Deep Neural Network accelerator (SA-DNN accelerator) by means of a Register Transfer Level (RTL) Fault Injection (FI) simulation in order to improve the observability of each hardware block. From this analysis, we present the sensitivity to single bit faults of register groups in the pipeline for three different DNN workloads utilising two datasets, namely MNIST and CIFAR-10. These sensitivity figures are presented in terms of Fault Propagation Probability ($P(f_{non-crit})$) and False Classification Probability ($P(f_{crit})$) which respectively show the probability that an injected fault causes a non-critical error (numerical offset) or a critical error (classification fault). From these results, we devise a fault mitigation strategy to harden the SA-DNN accelerator in an efficient way, both in terms of area and power overhead.

</details>


### [21] [Hardware-Aware Neural Network Compilation with Learned Optimization: A RISC-V Accelerator Approach](https://arxiv.org/abs/2512.00031)
*Ravindra Ganti,Steve Xu*

Main category: cs.AR

TL;DR: XgenSilicon ML编译器是一个端到端自动化框架，可将ML模型编译为针对定制ASIC加速器优化的RISC-V汇编代码，通过五项创新技术显著提升PPA指标。


<details>
  <summary>Details</summary>
Motivation: 传统ML模型部署在通用硬件上存在性能、功耗和面积效率低下的问题，需要自动化框架将高级ML模型直接编译为针对定制ASIC优化的代码，同时统一软件和硬件成本模型。

Method: 1) 多算法自动调优框架结合贝叶斯优化等五种搜索策略和学习成本模型；2) 集成量化框架支持FP32到二进制的极端精度，包含KL散度校准和动量QAT梯度更新；3) 硬件感知验证确保100% ISA合规和内存约束满足；4) 动态形状支持与多配置专业化；5) 高级缓存感知成本建模与多级缓存层次分析。

Result: 编译器生成的ASIC相比基线实现：性能提升2.5-4.5倍，功耗降低3-6倍，面积减少40-60%。支持100多个ONNX算子，实现高级RISC-V向量优化，生成可直接用于ASIC综合的硬件验证汇编代码。

Conclusion: XgenSilicon ML编译器通过完全自动化的端到端编译流程，成功将高级ML模型转换为针对定制ASIC优化的RISC-V代码，显著提升了PPA指标，为ML硬件加速提供了高效解决方案。

Abstract: We present XgenSilicon ML Compiler, a fully automated end-to-end compilation framework that transforms high-level machine learning models into optimized RISC-V assembly code for custom ASIC accelerators. By unifying the system's cost model across software and hardware, the compiler achieves significant improvements in Power, Performance, and Area (PPA) metrics compared to standard off-the-shelf components and hand-designed chips through five key innovations: (1) a multi-algorithm auto-tuning framework with five search strategies (Bayesian Optimization, Genetic Algorithm, Simulated Annealing, Random Search, Grid Search) combined with a learned cost model, (2) an integrated quantization framework supporting extreme precisions from FP32 to Binary with full KL divergence calibration (2048-bin histogram optimization) and momentum-based QAT gradient updates, (3) hardware-aware validation ensuring 100 percent ISA compliance and memory constraint satisfaction, (4) dynamic shape support with multi-configuration specialization, and (5) advanced cache-aware cost modeling with multi-level cache hierarchy analysis. Our evaluation demonstrates that ASICs produced by this compiler achieve 2.5-4.5x better performance, 3-6x lower power consumption, and 40-60 percent area reduction compared to baseline implementations. The compiler supports more than 100 ONNX operators across 12 categories, implements advanced RISC-V Vector optimizations, and generates hardware-validated assembly code suitable for direct ASIC synthesis. All compilation steps are fully automated, requiring zero manual intervention from model input to ASIC-ready output.

</details>


### [22] [Decoupled Control Flow and Data Access in RISC-V GPGPUs](https://arxiv.org/abs/2512.00032)
*Giuseppe M. Sarda,Nimish Shah,Abubakr Nada,Debjyoti Bhattacharjee,Marian Verhelst*

Main category: cs.AR

TL;DR: 本文提出针对开源GPGPU平台Vortex的架构优化，通过解耦控制流管理和数据访问，显著提升性能，使其成为GPGPU研究的理想平台。


<details>
  <summary>Details</summary>
Motivation: Vortex是基于RISC-V的开源GPGPU平台，为GPGPU研究提供了替代商业GPU的选项。然而，作为新兴硬件平台，Vortex缺乏商业GPU的复杂架构特性，特别是在控制流管理和内存编排方面性能不足，这限制了其在内存密集型应用（如机器学习基础算法）中的性能竞争力。

Method: 提出两种简单的微架构改进：1) 硬件控制流管理器，加速常规循环执行中的分支和谓词操作；2) 解耦内存流通道，通过有用计算进一步隐藏内存延迟。

Result: 评估结果显示：不同内核的执行速度提升8倍，动态指令数减少10倍，整体性能从0.35 GFLOP/s/mm²提升到1.63 GFLOP/s/mm²。

Conclusion: 通过这些增强，Vortex能够成为下一代机器学习GPGPU研究的理想实验平台，为开源GPGPU研究提供高性能基础。

Abstract: Vortex, a newly proposed open-source GPGPU platform based on the RISC-V ISA, offers a valid alternative for GPGPU research over the broadly-used modeling platforms based on commercial GPUs. Similarly to the push originating from the RISC-V movement for CPUs, Vortex can enable a myriad of fresh research directions for GPUs. However, as a young hardware platform, it currently lacks the performance competitiveness of commercial GPUs, which is crucial for widespread adoption. State-of-the-art GPUs, in fact, rely on complex architectural features, still unavailable in Vortex, to hide the micro-code overheads linked to control flow (CF) management and memory orchestration for data access. In particular, these components account for the majority of the dynamic instruction count in regular, memory-intensive kernels, such as linear algebra routines, which form the basis of many applications, including Machine Learning. To address these challenges with simple yet powerful micro-architecture modifications, this paper introduces decoupled CF and data access through 1.) a hardware CF manager to accelerate branching and predication in regular loop execution and 2.) decoupled memory streaming lanes to further hide memory latency with useful computation. The evaluation results for different kernels show 8$\times$ faster execution, 10$\times$ reduction in dynamic instruction count, and overall performance improvement from 0.35 to 1.63 $\mathrm{GFLOP/s/mm^2}$. Thanks to these enhancements, Vortex can become an ideal playground to enable GPGPU research for the next generation of Machine Learning.

</details>


### [23] [WebAssembly on Resource-Constrained IoT Devices: Performance, Efficiency, and Portability](https://arxiv.org/abs/2512.00035)
*Mislav Has,Tao Xiong,Fehmi Ben Abdesslem,Mario Kušek*

Main category: cs.AR

TL;DR: 该论文评估了WebAssembly在嵌入式IoT系统中的可行性，通过对比WASM运行时（WAMR和wasm3）与原生C执行在三种微控制器上的性能、内存占用和能耗表现，发现虽然原生执行在速度和能效上更优，但WASM在跨平台兼容性和沙箱执行方面提供了可接受的权衡。


<details>
  <summary>Details</summary>
Motivation: 物联网设备和软件的异构性日益增加，给资源受限设备上的软件可移植性、可维护性和部署带来了重大挑战。WebAssembly作为一种最初为Web设计的便携、安全、高效的运行时环境，有望解决这些问题，但需要验证其在嵌入式IoT系统中的实际可行性。

Method: 在三种代表性微控制器（Raspberry Pi Pico、ESP32 C6和nRF5340）上评估WASM的性能、内存占用和能耗。使用两个轻量级WASM运行时（WAMR和wasm3）与原生C执行进行对比分析。

Result: 结果显示，原生执行在速度和能效方面仍然优于WASM，但WASM在跨平台兼容性和沙箱执行方面提供了可接受的权衡。WASM的内存占用和能耗虽然高于原生执行，但在可接受范围内。

Conclusion: 当可移植性和安全性优先于严格的性能约束时，WASM是嵌入式IoT应用的可行选择。进一步的运行时优化可以扩展其在该领域的实用性，使其在物联网环境中更具竞争力。

Abstract: The increasing heterogeneity of hardware and software in the Internet of Things (IoT) poses a major challenge for the portability, maintainability and deployment of software on devices with limited resources. WebAssembly (WASM), originally designed for the web, is increasingly recognized as a portable, secure and efficient runtime environment that can overcome these challenges. This paper explores the feasibility of using WASM in embedded IoT systems by evaluating its performance, memory footprint and energy consumption on three representative microcontrollers: the Raspberry Pi Pico, the ESP32 C6 and the nRF5340. Two lightweight WASM runtimes, WAMR and wasm3, are compared with the native C execution. The results show that while the native execution remains superior in terms of speed and energy efficiency, WASM offers acceptable trade-offs in return for cross-platform compatibility and sandbox execution. The results highlight that WASM is a viable option for embedded IoT applications when portability and security outweigh strict performance constraints, and that further runtime optimization could extend its practicality in this area.

</details>


### [24] [LLaMCAT: Optimizing Large Language Model Inference with Cache Arbitration and Throttling](https://arxiv.org/abs/2512.00083)
*Zhongchun Zhou,Chengtao Lai,Wei Zhang*

Main category: cs.AR

TL;DR: LLaMCAT：针对LLM推理中KV Cache访问优化的缓存仲裁与线程节流技术，在LLC架构上实现显著加速


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在推理阶段需要大量内存，对现有内存系统设计构成挑战，特别是在最后一级缓存（LLC）架构上。KV Cache访问中的缓存停顿和带宽需求是主要瓶颈，现有方案未针对LLM解码特有的MSHR争用进行优化。

Method: 提出LLaMCAT方法：1）结合MSHR感知和负载均衡感知的缓存仲裁；2）线程节流技术；3）混合仿真框架（分析模型+周期级模拟器+内存轨迹），平衡架构细节与效率。

Result: 在主要受缺失处理吞吐量限制时，平均加速1.26倍；在缓存大小也受限时，相比未优化版本加速1.58倍，相比最佳基线（dyncta）提升1.26倍。首次针对LLM解码特有的MSHR争用进行优化。

Conclusion: LLaMCAT填补了先前工作中针对LLM解码特定MSHR争用的空白，为未来硬件平台上的LLM推理加速提供了实用解决方案。

Abstract: Large Language Models (LLMs) have achieved unprecedented success across various applications, but their substantial memory requirements pose significant challenges to current memory system designs, especially during inference. Our work targets last-level cache (LLC) based architectures, including GPUs (e.g., NVIDIA GPUs) and AI accelerators. We introduce LLaMCAT, a novel approach to optimize the LLC for LLM inference. LLaMCAT combines Miss Status Holding Register (MSHR)- and load balance-aware cache arbitration with thread throttling to address stringent bandwidth demands and minimize cache stalls in KV Cache access. We also propose a hybrid simulation framework integrating analytical models with cycle-level simulators via memory traces, balancing architecture detail and efficiency.
  Experiments demonstrate that LLaMCAT achieves an average speedup of 1.26x when the system is mainly bottlenecked by miss handling throughput, while baselines mostly show negative improvements since they are not optimized for this scenario. When the cache size is also limited, our policy achieves a speedup of 1.58x over the unoptimized version, and a 1.26x improvement over the best baseline (dyncta). Overall, LLaMCAT is the first to target LLM decoding-specific MSHR contention, a gap in previous work. It presents a practical solution for accelerating LLM inference on future hardware platforms.

</details>


### [25] [Critical Path Aware Timing-Driven Global Placement for Large-Scale Heterogeneous FPGAs](https://arxiv.org/abs/2512.00038)
*He Jiang,Yi Guo,Shikai Guo,Huijiang Liu,Xiaochen Li,Ning Wang,Zhixiong Di*

Main category: cs.AR

TL;DR: TD-Placer：面向FPGA的时序驱动全局布局框架，通过图表示和轻量级算法优化关键路径延迟，相比现有方法提升WNS 10%，降低CPD 5%


<details>
  <summary>Details</summary>
Motivation: 随着FPGA设计规模扩大和异构资源增加，密集互连带来的电阻电容效应使时序收敛日益困难。现有方法在多因素非线性约束、负载和串扰耦合效应下难以构建准确的时序模型。

Method: 提出TD-Placer时序驱动全局布局框架：1) 使用图表示捕获全局网络交互；2) 采用非线性模型整合多种时序相关特征进行精确延迟预测；3) 采用二次布局目标最小化线长并加入轻量级算法构建的时序项；4) 使用细粒度加权方案处理网络级时序竞争。

Result: 在7个真实开源FPGA项目（LUT数量60K-400K）上测试，相比现有最优方法：平均WNS提升10%，CPD降低5%。在5个AMD Vivado版本（2020.2-2024.2）上，平均CPD相当（*1.01）。

Conclusion: TD-Placer通过图表示和轻量级时序优化算法，有效解决了FPGA时序驱动布局中的挑战，在保持线长优化的同时显著改善了时序性能，代码和数据集已开源。

Abstract: Timing optimization during global placement is critical for achieving optimal circuit performance and remains a key challenge in modern Field Programmable Gate Array (FPGA) design. As FPGA designs scale and heterogeneous resources increase, dense interconnects introduce significant resistive and capacitive effects, making timing closure increasingly difficult. Existing methods face challenges in constructing accurate timing models due to multi-factor nonlinear constraints as well as load and crosstalk coupling effects arising in multi-pin driving scenarios. To address these challenges, we propose TD-Placer, a critical path aware, timing-driven global placement framework. It leverages graph-based representations to capture global net interactions and employs a nonlinear model to integrate diverse timing-related features for precise delay prediction, thereby improving the overall placement quality for FPGAs. TD-Placer adopts a quadratic placement objective that minimizes wirelength while incorporating a timing term constructed by a lightweight algorithm, enabling efficient and high-quality timing optimization. Regarding net-level timing contention, it also employs a finer-grained weighting scheme to facilitate smooth reduction of the Critical Path Delay (CPD). Extensive experiments were carried out on seven real-world open-source FPGA projects with LUT counts ranging from 60K to 400K. The results demonstrate that TD-Placer achieves an average 10% improvement in Worst Negative Slack (WNS) and a 5% reduction in CPD compared to the state-of-the-art method, with an average CPD comparable (*1.01) to the commercial AMD Vivado across five versions (2020.2-2024.2). Its code and dataset are publicly available.

</details>


### [26] [SetupKit: Efficient Multi-Corner Setup/Hold Time Characterization Using Bias-Enhanced Interpolation and Active Learning](https://arxiv.org/abs/2512.00044)
*Junzhuo Zhou,Ziwen Wang,Haoxuan Xia,Yuxin Yan,Chengyu Zhu,Ting-Jung Lin,Wei Xing,Lei He*

Main category: cs.AR

TL;DR: SetupKit：基于统计智能、电路分析和主动学习的框架，用于加速芯片时序库特征化中的建立/保持时间表征，相比标准方法减少2.4倍CPU时间


<details>
  <summary>Details</summary>
Motivation: 现代芯片时序收敛需要准确的建立/保持时间表征，但依赖数百万次SPICE仿真在不同PVT角下造成主要瓶颈，通常耗时数周甚至数月。现有方法收敛慢且探索效率低，尤其在多角设置下。

Method: 提出SetupKit框架，集成三个关键创新：1) BEIRA - 基于统计误差建模的偏置增强插值搜索，加速收敛并克服停滞问题；2) 通过电路分析进行初始搜索区间估计；3) 使用高斯过程的主动学习策略，智能学习PVT-时序相关性，主动引导昂贵仿真到最具信息量的角，减少多角表征中的冗余。

Result: 在工业22nm标准单元库的16个PVT角上评估，SetupKit相比标准实践实现了2.4倍的整体CPU时间减少（从单核720天降至290天），显著缩短了表征时间。

Conclusion: SetupKit为库特征化提供了基于学习的原理性方法，解决了关键的EDA挑战，为更智能的仿真管理铺平了道路。

Abstract: Accurate setup/hold time characterization is crucial for modern chip timing closure, but its reliance on potentially millions of SPICE simulations across diverse process-voltagetemperature (PVT) corners creates a major bottleneck, often lasting weeks or months. Existing methods suffer from slow search convergence and inefficient exploration, especially in the multi-corner setting. We introduce SetupKit, a novel framework designed to break this bottleneck using statistical intelligence, circuit analysis and active learning (AL). SetupKit integrates three key innovations: BEIRA, a bias-enhanced interpolation search derived from statistical error modeling to accelerate convergence by overcoming stagnation issues, initial search interval estimation by circuit analysis and AL strategy using Gaussian Process. This AL component intelligently learns PVT-timing correlations, actively guiding the expensive simulations to the most informative corners, thus minimizing redundancy in multicorner characterization. Evaluated on industrial 22nm standard cells across 16 PVT corners, SetupKit demonstrates a significant 2.4x overall CPU time reduction (from 720 to 290 days on a single core) compared to standard practices, drastically cutting characterization time. SetupKit offers a principled, learningbased approach to library characterization, addressing a critical EDA challenge and paving the way for more intelligent simulation management.

</details>


### [27] [Assessing Large Language Models in Generating RTL Design Specifications](https://arxiv.org/abs/2512.00045)
*Hung-Ming Huang,Yu-Hsin Yang,Fu-Chieh Chang,Yun-Chia Hsu,Yin-Yu Lin,Ming-Fang Tsai,Chun-Chih Yang,Pei-Yuan Wu*

Main category: cs.AR

TL;DR: 该研究探索如何利用LLM从RTL代码自动生成规格说明，提出了评估生成规格说明质量的指标，并比较了开源和商业LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 随着IC设计复杂度增加，手动解读RTL代码并编写规格说明变得耗时且易错。虽然已有研究使用LLM从规格说明生成RTL代码，但反向的自动化规格说明生成仍缺乏可靠评估方法。

Method: 研究探索不同提示策略对RTL到规格说明生成质量的影响，并引入评估生成规格说明忠实度的指标。同时，对开源和商业LLM进行基准测试。

Result: 建立了评估生成规格说明质量的指标框架，为IC设计中更自动化和高效的规格说明工作流程奠定了基础。

Conclusion: 该研究填补了RTL代码自动化规格说明生成领域的评估方法空白，为IC设计流程的自动化改进提供了重要基础。

Abstract: As IC design grows more complex, automating comprehension and documentation of RTL code has become increasingly important. Engineers currently should manually interpret existing RTL code and write specifications, a slow and error-prone process. Although LLMs have been studied for generating RTL from specifications, automated specification generation remains underexplored, largely due to the lack of reliable evaluation methods. To address this gap, we investigate how prompting strategies affect RTL-to-specification quality and introduce metrics for faithfully evaluating generated specs. We also benchmark open-source and commercial LLMs, providing a foundation for more automated and efficient specification workflows in IC design.

</details>


### [28] [A Configurable Mixed-Precision Fused Dot Product Unit for GPGPU Tensor Computation](https://arxiv.org/abs/2512.00053)
*Nikhil Rout,Blaise Tine*

Main category: cs.AR

TL;DR: 提出了一种可扩展的混合精度点积单元，将浮点和整数算术流水线集成在单一融合架构中，作为开源RISC-V Vortex GPGPU张量核心单元的扩展，支持多种低精度格式和高精度累加。


<details>
  <summary>Details</summary>
Motivation: 现有开源RTL实现的内积运算使用离散算术单元，导致吞吐量不理想和资源利用率低下，需要更高效的混合精度矩阵乘法加速器来提升深度学习工作负载性能。

Method: 设计了一个可扩展的混合精度点积单元，将浮点和整数算术流水线融合在单一架构中，支持FP16/BF16/FP8/BF8/INT8/UINT4等低精度格式的乘法运算和FP32/INT32高精度累加，并采用可扩展框架支持未来自定义表示。

Result: 在AMD Xilinx Alveo U55C FPGA上实现4周期操作延迟和306.6 MHz时钟频率，在4线程每warp配置下达到9.812 GFLOPS的理想流水线吞吐量。

Conclusion: 提出的融合架构混合精度点积单元有效解决了现有离散算术单元的问题，提供了高效、可扩展的混合精度计算解决方案，显著提升了GPGPU上深度学习工作负载的加速性能。

Abstract: Efficient mixed-precision MMA operations are critical for accelerating Deep Learning workloads on GPGPUs. However, existing open-source RTL implementations of inner dot products rely on discrete arithmetic units, leading to suboptimal throughput and poor resource utilization. To address these challenges, we propose a scalable mixed-precision dot product unit that integrates floating-point and integer arithmetic pipelines within a singular fused architecture, implemented as part of the open-source RISC-V based Vortex GPGPU's Tensor Core Unit extension. Our design supports low-precision multiplication in (FP16/BF16/FP8/BF8/INT8/UINT4) formats and higher-precision accumulation in (FP32/INT32), with an extensible framework for adding and evaluating other custom representations in the future. Experimental results demonstrate 4-cycle operation latency at 306.6 MHz clock frequency on the AMD Xilinx Alveo U55C FPGA, delivering an ideal filled pipeline throughput of 9.812 GFLOPS in a 4-thread per warp configuration.

</details>


### [29] [KAN-SAs: Efficient Acceleration of Kolmogorov-Arnold Networks on Systolic Arrays](https://arxiv.org/abs/2512.00055)
*Sohaib Errabii,Olivier Sentieys,Marcello Traiola*

Main category: cs.AR

TL;DR: KAN-SAs：一种基于脉动阵列的新型加速器，通过非递归B样条实现和利用KAN稀疏性，将脉动阵列利用率提升至100%，时钟周期减少50%，高效加速Kolmogorov-Arnold网络推理。


<details>
  <summary>Details</summary>
Motivation: KANs因其参数效率和可解释性优于传统DNN而备受关注，但其核心的B样条激活函数难以加速。脉动阵列作为高效的DNN加速器，其在KAN加速中的适用性和效率尚未评估，传统脉动阵列在加速KAN时利用率仅30%。

Method: 提出KAN-SAs加速器，包含非递归B样条实现，利用KAN的固有稀疏性，增强传统脉动阵列架构，使其既能高效加速KAN推理，也能处理传统DNN。

Result: 在28nm FD-SOI工艺上的硬件综合结果显示，KAN-SAs相比同等面积的传统脉动阵列，实现高达100%的阵列利用率，时钟周期减少达50%。在不同KAN应用上的评估证实了其推理效率的提升。

Conclusion: KAN-SAs通过创新的非递归B样条实现和稀疏性利用，显著提升了脉动阵列在KAN推理中的效率，为KAN硬件加速提供了有效解决方案。

Abstract: Kolmogorov-Arnold Networks (KANs) have garnered significant attention for their promise of improved parameter efficiency and explainability compared to traditional Deep Neural Networks (DNNs). KANs' key innovation lies in the use of learnable non-linear activation functions, which are parametrized as splines. Splines are expressed as a linear combination of basis functions (B-splines). B-splines prove particularly challenging to accelerate due to their recursive definition. Systolic Array (SA)based architectures have shown great promise as DNN accelerators thanks to their energy efficiency and low latency. However, their suitability and efficiency in accelerating KANs have never been assessed. Thus, in this work, we explore the use of SA architecture to accelerate the KAN inference. We show that, while SAs can be used to accelerate part of the KAN inference, their utilization can be reduced to 30%. Hence, we propose KAN-SAs, a novel SA-based accelerator that leverages intrinsic properties of B-splines to enable efficient KAN inference. By including a nonrecursive B-spline implementation and leveraging the intrinsic KAN sparsity, KAN-SAs enhances conventional SAs, enabling efficient KAN inference, in addition to conventional DNNs. KAN-SAs achieves up to 100% SA utilization and up to 50% clock cycles reduction compared to conventional SAs of equivalent area, as shown by hardware synthesis results on a 28nm FD-SOI technology. We also evaluate different configurations of the accelerator on various KAN applications, confirming the improved efficiency of KAN inference provided by KAN-SAs.

</details>


### [30] [SafeCiM: Investigating Resilience of Hybrid Floating-Point Compute-in-Memory Deep Learning Accelerators](https://arxiv.org/abs/2512.00059)
*Swastik Bhattacharya,Sanjay Das,Anand Menon,Shamik Kundu,Arnab Raha,Kanad Basu*

Main category: cs.AR

TL;DR: 本文系统分析了浮点计算内存（FP-CiM）架构中的硬件故障影响，并提出了一种名为SafeCiM的容错设计，显著降低了故障对神经网络推理精度的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型等深度神经网络参数规模不断增长，传统加速器面临数据传输瓶颈，计算内存（CiM）架构通过在内存中集成计算来减少数据移动。浮点计算（FP）CiM因其宽动态范围和精度在生成式AI应用中具有优势，但其硬件故障脆弱性尚未充分研究，在关键任务环境中构成重大可靠性问题。

Method: 通过在关键计算阶段（包括数字乘法器、CiM存储单元和数字加法器树）引入比特翻转故障，系统分析硬件故障对FP-CiM的影响。使用AlexNet等卷积神经网络以及LLaMA-3.2-1B和Qwen-0.3B-Base等先进大语言模型进行实验，评估每个阶段故障对推理精度的影响。基于这些洞察，提出名为SafeCiM的故障弹性设计，相比带有预对齐阶段的朴素FP-CiM架构，能更好地缓解故障影响。

Result: 实验发现单个加法器故障可将LLM精度降至0%。提出的SafeCiM设计在4096个MAC单元配置下，相比基线FP-CiM架构，将单个加法器故障导致的精度下降减少了高达49倍。

Conclusion: FP-CiM架构对硬件故障高度敏感，特别是在加法器阶段。SafeCiM设计通过有效的故障缓解机制，显著提高了计算内存架构的可靠性，为关键任务应用中的FP-CiM部署提供了可行的解决方案。

Abstract: Deep Neural Networks (DNNs) continue to grow in complexity with Large Language Models (LLMs) incorporating vast numbers of parameters. Handling these parameters efficiently in traditional accelerators is limited by data-transmission bottlenecks, motivating Compute-in-Memory (CiM) architectures that integrate computation within or near memory to reduce data movement. Recent work has explored CiM designs using Floating-Point (FP) and Integer (INT) operations. FP computations typically deliver higher output quality due to their wider dynamic range and precision, benefiting precision-sensitive Generative AI applications. These include models such as LLMs, thus driving advancements in FP-CiM accelerators. However, the vulnerability of FP-CiM to hardware faults remains underexplored, posing a major reliability concern in mission-critical settings. To address this gap, we systematically analyze hardware fault effects in FP-CiM by introducing bit-flip faults at key computational stages, including digital multipliers, CiM memory cells, and digital adder trees. Experiments with Convolutional Neural Networks (CNNs) such as AlexNet and state-of-the-art LLMs including LLaMA-3.2-1B and Qwen-0.3B-Base reveal how faults at each stage affect inference accuracy. Notably, a single adder fault can reduce LLM accuracy to 0%. Based on these insights, we propose a fault-resilient design, SafeCiM, that mitigates fault impact far better than a naive FP-CiM with a pre-alignment stage. For example, with 4096 MAC units, SafeCiM reduces accuracy degradation by up to 49x for a single adder fault compared to the baseline FP-CiM architecture.

</details>


### [31] [A CNN-Based Technique to Assist Layout-to-Generator Conversion for Analog Circuits](https://arxiv.org/abs/2512.00070)
*Sungyu Jeong,Minsu Kim,Byungsub Kim*

Main category: cs.AR

TL;DR: 使用CNN模型自动检测模拟电路子单元，识别哪些可由现有生成器脚本生成，显著减少人工检查时间


<details>
  <summary>Details</summary>
Motivation: 将参考布局转换为程序化布局生成器时，需要大量人工检查哪些子单元可由现有生成器创建，过程耗时且容易出错

Method: 提出基于卷积神经网络（CNN）的模型，自动检测子单元是否可由库中可用生成器脚本生成，并在生成器软件的正确层次位置建议使用

Result: CNN模型在高速有线接收器电路（含4,885个子单元实例，145种不同设计）上，将子单元分类为51个可生成类和1个不可生成类，达到99.3%的精确度，检查时间从88分钟减少到18秒

Conclusion: CNN模型能有效自动化子单元生成器识别过程，显著提高效率，并能正确分类与训练数据差异较大的陌生子单元

Abstract: We propose a technique to assist in converting a reference layout of an analog circuit into the procedural layout generator by efficiently reusing available generators for sub-cell creation. The proposed convolutional neural network (CNN) model automatically detects sub-cells that can be generated by available generator scripts in the library, and suggests using them in the hierarchically correct places of the generator software. In experiments, the CNN model examined sub-cells of a high-speed wireline receiver that has a total of 4,885 sub-cell instances including different 145 sub-cell designs. The CNN model classified the sub-cell instances into 51 generatable and one not-generatable classes. One not-generatable class indicates that no available generator can generate the classified sub-cell. The CNN model achieved 99.3% precision in examining the 145 different sub-cell designs. The CNN model greatly reduced the examination time to 18 seconds from 88 minutes required in manual examination. Also, the proposed CNN model could correctly classify unfamiliar sub-cells that are very different from the training dataset.

</details>


### [32] [InF-ATPG: Intelligent FFR-Driven ATPG with Advanced Circuit Representation Guided Reinforcement Learning](https://arxiv.org/abs/2512.00079)
*Bin Sun,Rengang Zhang,Zhiteng Chao,Zizhen Liu,Jianan Mu,Jing Ye,Huawei Li*

Main category: cs.AR

TL;DR: InF-ATPG是一种基于强化学习和图神经网络的智能测试模式生成框架，通过电路分区和特征增强，显著减少了回溯次数并提高了故障覆盖率。


<details>
  <summary>Details</summary>
Motivation: 随着半导体技术进步，传统ATPG方法执行时间长，难以达到预期故障覆盖率，影响芯片上市时间。现有的机器学习方法（如强化学习和图神经网络）存在奖励延迟和电路表示不足的问题。

Method: 提出InF-ATPG框架：1）将电路划分为扇出自由区域（FFRs）；2）在新型QGNN架构中融入ATPG特定特征；3）使用增强的电路表示来指导强化学习。

Result: 与传统方法相比平均减少55.06%的回溯次数，与机器学习方法相比减少38.31%的回溯次数，同时提高了故障覆盖率。

Conclusion: InF-ATPG通过先进的电路表示和强化学习指导，有效解决了ATPG中的效率问题，为集成电路测试提供了更高效的解决方案。

Abstract: Automatic test pattern generation (ATPG) is a crucial process in integrated circuit (IC) design and testing, responsible for efficiently generating test patterns. As semiconductor technology progresses, traditional ATPG struggles with long execution times to achieve the expected fault coverage, which impacts the time-to-market of chips. Recent machine learning techniques, like reinforcement learning (RL) and graph neural networks (GNNs), show promise but face issues such as reward delay in RL models and inadequate circuit representation in GNN-based methods. In this paper, we propose InF-ATPG, an intelligent FFR-driven ATPG framework that overcomes these challenges by using advanced circuit representation to guide RL. By partitioning circuits into fanout-free regions (FFRs) and incorporating ATPG-specific features into a novel QGNN architecture, InF-ATPG enhances test pattern generation efficiency. Experimental results show InF-ATPG reduces backtracks by 55.06\% on average compared to traditional methods and 38.31\% compared to the machine learning approach, while also improving fault coverage.

</details>


### [33] [Modeling and Simulation Frameworks for Processing-in-Memory Architectures](https://arxiv.org/abs/2512.00096)
*Mahdi Aghaei,Saba Ebrahimi,Mohammad Saleh Arafati,Elham Cheshmikhani,Dara Rahmati,Saeid Gorgin,Jungrae Kim*

Main category: cs.AR

TL;DR: 本章对存内计算（PIM）仿真方法学和工具进行了全面综述，对仿真器按抽象层次、设计目标和评估指标进行分类，并讨论了PIM研究中常用的基准测试套件和仿真方法学的开放挑战。


<details>
  <summary>Details</summary>
Motivation: 存内计算（PIM）作为解决冯·诺依曼架构内存墙问题的有前景计算范式，需要通过仿真在制造前评估、比较和优化PIM设计。然而现有的PIM仿真器在保真度、可扩展性、支持技术等方面差异很大，研究人员需要理解这些权衡以选择合适的仿真工具。

Method: 对PIM仿真方法学和工具进行全面综述，按抽象层次、设计目标和评估指标对仿真器进行分类，分析代表性示例。同时调查PIM研究中常用的基准测试套件，并讨论仿真方法学中的开放挑战。

Result: 提供了PIM仿真领域的系统性分类框架，帮助研究人员根据具体需求选择合适的仿真工具。识别了当前仿真方法学在可靠性、可扩展性和效率方面的不足，为未来更可靠的PIM建模指明了方向。

Conclusion: 本章为PIM仿真研究提供了全面的参考指南，通过分类分析帮助研究人员理解不同仿真工具的权衡，并指出了仿真方法学需要进一步发展的方向，以支持更可靠、可扩展和高效的PIM建模。

Abstract: Processing-in-Memory (PIM) has emerged as a promising computing paradigm to address the memory wall and the fundamental bottleneck of the von Neumann architecture by reducing costly data movement between memory and processing units. As with any engineering challenge, identifying the most effective solutions requires thorough exploration of diverse architectural proposals, device technologies, and application domains. In this context, simulation plays a critical role in enabling researchers to evaluate, compare, and refine PIM designs prior to fabrication. Over the past decade, a variety of PIM simulators have been introduced, spanning low-level device models, architectural frameworks, and application-oriented environments. These tools differ significantly in fidelity, scalability, supported memory/compute technologies, and benchmark compatibility. Understanding these trade-offs is essential for researchers to select appropriate simulators that accurately map and validate their research efforts. This chapter provides a comprehensive overview of PIM simulation methodologies and tools. We categorize simulators according to abstraction levels, design objectives, and evaluation metrics, highlighting representative examples. To improve accessibility, some content may appear in multiple contexts to guide readers with different backgrounds. We also survey benchmark suites commonly employed in PIM studies and discuss open challenges in simulation methodology, paving the way for more reliable, scalable, and efficient PIM modeling.

</details>


### [34] [An Analytical and Empirical Investigation of Tag Partitioning for Energy-Efficient Reliable Cache](https://arxiv.org/abs/2512.00112)
*Elham Cheshmikhani,Hamed Farbeh*

Main category: cs.AR

TL;DR: 本文提出了一种分析模型来确定标签分区技术中的最优分割点k，以最大化减少标签读取次数并提高缓存能效和可靠性。


<details>
  <summary>Details</summary>
Motivation: 关联缓存内存对处理器性能和能耗有重要影响，但易受故障影响。标签分区技术通过分两阶段比较标签来降低能耗和提高可靠性，但现有方法选择分割点k时缺乏理论依据，通常凭直觉、随机或经验选择，导致效果不佳且无法泛化到不同缓存配置。

Method: 通过分析推导出确定最优标签分割点k的公式，该公式基于缓存配置参数，具有凸性和可微性，能够精确量化任何k值和配置下的标签分区效率。

Result: 实验验证表明，分析模型与实验结果高度一致，能够在广泛的缓存设计中准确预测标签分区效率和最优k值。模型显示选择过大或过小的k值都会显著降低标签分区效果。

Conclusion: 提出的分析模型使设计者和研究人员能够即时计算最优标签分割点并准确估计标签读取减少量，为标签分区技术提供了理论依据和实用工具。

Abstract: Associative cache memory significantly influences processor performance and energy consumption. Because it occupies over half of the chip area, cache memory is highly susceptible to transient and permanent faults, posing reliability challenges. As the only hardware-managed memory module, the cache tag array is the most active and critical component, dominating both energy usage and error rate. Tag partitioning is a widely used technique to reduce tag-access energy and enhance reliability. It divides tag comparison into two phases: first comparing the k lower bits, and then activating only the matching tag entries to compare the remaining higher bits. The key design parameter is the selection of the tag-splitting point k, which determines how many reads are eliminated. However, prior studies have chosen k intuitively, randomly, or empirically, without justification. Even experimentally determined values are ad-hoc and do not generalize across cache configurations due to high sensitivity to architectural parameters.
  In this paper, we analytically show that choosing k too large or too small substantially reduces the effectiveness of tag partitioning. We then derive a formulation that determines the optimal splitting point based on cache configuration parameters. The formulation is convex, differentiable, and capable of precisely quantifying tag-partitioning efficiency for any k and configuration. To validate our model, we experimentally evaluate tag-partitioning efficiency and optimal k across a broad set of cache designs and demonstrate close agreement between analytical and experimental results. The proposed formulation enables designers and researchers to instantly compute the optimal tag-splitting point and accurately estimate tag-read reduction.

</details>


### [35] [From RISC-V Cores to Neuromorphic Arrays: A Tutorial on Building Scalable Digital Neuromorphic Processors](https://arxiv.org/abs/2512.00113)
*Amirreza Yousefzadeh*

Main category: cs.AR

TL;DR: 本文是关于数字神经形态处理器架构设计的教程，以SENECA平台为例，从基础RISC-V核心阵列逐步演进到专用神经处理单元，重点讨论架构权衡和能效优化。


<details>
  <summary>Details</summary>
Motivation: 数字神经形态处理器在低功耗、常开边缘AI应用中具有前景，但需要系统化的架构设计指导。本文旨在为希望设计自己数字神经形态处理器的学生和实践者提供连贯的架构视角。

Method: 以SENECA平台为案例，从灵活的RISC-V处理核心阵列开始，逐步演进架构：从事件驱动的全连接网络实现，到添加专用神经处理单元和循环控制器，同时讨论软件映射技术如脉冲分组、事件驱动深度优先卷积和硬注意力处理。

Result: 本文不呈现新的实验结果，而是综合先前SENECA出版物中的发现，提供系统的架构设计原则、性能瓶颈分析和能效优化策略。

Conclusion: 数字神经形态处理器设计需要平衡灵活性和专用加速，通过渐进式架构演进和软件映射技术优化，能够有效支持边缘AI应用的低功耗需求。

Abstract: Digital neuromorphic processors are emerging as a promising computing substrate for low-power, always-on EdgeAI applications. In this tutorial paper, we outline the main architectural design principles behind fully digital neuromorphic processors and illustrate them using the SENECA platform as a running example. Starting from a flexible array of tiny RISC-V processing cores connected by a simple Network-on-Chip (NoC), we show how to progressively evolve the architecture: from a baseline event-driven implementation of fully connected networks, to versions with dedicated Neural Processing Elements (NPEs) and a loop controller that offloads fine-grained control from the general-purpose cores. Along the way, we discuss software and mapping techniques such as spike grouping, event-driven depth-first convolution for convolutional networks, and hard-attention style processing for high-resolution event-based vision. The focus is on architectural trade-offs, performance and energy bottlenecks, and on leveraging flexibility to incrementally add domain-specific acceleration. This paper assumes familiarity with basic neuromorphic concepts (spikes, event-driven computation, sparse activation) and deep neural network workloads. It does not present new experimental results; instead, it synthesizes and contextualizes findings previously reported in our SENECA publications to provide a coherent, step-by-step architectural perspective for students and practitioners who wish to design their own digital neuromorphic processors.

</details>


### [36] [Ternary-Input Binary-Weight CNN Accelerator Design for Miniature Object Classification System with Query-Driven Spatial DVS](https://arxiv.org/abs/2512.00138)
*Yuyang Li,Swasthik Muloor,Jack Laudati,Nickolas Dematteis,Yidam Park,Hana Kim,Nathan Chang,Inhee Lee*

Main category: cs.AR

TL;DR: 本文提出了一种面向微型成像系统的CNN硬件加速器，采用空间动态视觉传感器和三元输入/二元权重神经网络，显著降低了计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 微型成像系统受限于内存和功耗约束，传统机器学习虽然能减少数据尺寸但能耗过高，需要专门优化的硬件加速器来平衡性能与功耗。

Method: 使用空间动态视觉传感器（可重构为时间DVS），采用三元DVS输出和三元输入、二元权重的神经网络设计，在28nm CMOS工艺中实现硬件加速器。

Result: 数据尺寸减少81%，MAC操作减少27%，推理时间440ms，功耗仅1.6mW，相比先前微型系统CNN加速器的FoM提升7.3倍。

Conclusion: 该硬件加速器设计有效解决了微型成像系统的功耗和内存限制问题，为空间受限应用提供了高效的物体分类解决方案。

Abstract: Miniature imaging systems are essential for space-constrained applications but are limited by memory and power constraints. While machine learning can reduce data size by extracting key features, its high energy demands often exceed the capacity of small batteries. This paper presents a CNN hardware accelerator optimized for object classification in miniature imaging systems. It processes data from a spatial Dynamic Vision Sensor (DVS), reconfigurable to a temporal DVS via pixel sharing, minimizing sensor area. By using ternary DVS outputs and a ternary-input, binary-weight neural network, the design reduces computation and memory needs. Fabricated in 28 nm CMOS, the accelerator cuts data size by 81% and MAC operations by 27%. It achieves 440 ms inference time at just 1.6 mW power consumption, improving the Figure-of-Merit (FoM) by 7.3x over prior CNN accelerators for miniature systems.

</details>


### [37] [Variable Point: A Number Format for Area- and Energy-Efficient Multiplication of High-Dynamic-Range Numbers](https://arxiv.org/abs/2512.00186)
*Seyed Hadi Mirfarshbafan,Nicolas Filliol,Oscar Castañeda,Christoph Studer*

Main category: cs.AR

TL;DR: 提出一种新型变量点（VP）数字格式，在保持硬件复杂度不显著增加的同时，提供比定点数更大的动态范围，适用于高动态范围信号的VLSI设计。


<details>
  <summary>Details</summary>
Motivation: 定点数表示在硬件效率受限的VLSI设计中常用，但动态范围有限；浮点数动态范围大但硬件复杂度高。需要一种既能提供较大动态范围又不显著增加硬件复杂度的数字格式。

Method: 提出变量点（VP）数字格式，结合了定点数和浮点数的优点。通过多天线无线通信系统中的空间均衡矩阵向量乘法引擎来验证VP格式的有效性。

Result: 后布局VLSI实现结果显示，基于VP的设计相比完全优化的定点设计，实现了20%的面积节省和10%的功耗节省，且没有明显的性能下降。

Conclusion: VP数字格式为高动态范围信号的VLSI设计提供了一种高效的解决方案，在保持硬件复杂度可控的同时显著扩展了动态范围，实现了更好的面积和功耗效率。

Abstract: Fixed-point number representation is commonly employed in digital VLSI designs that have stringent hardware efficiency constraints. However, fixed-point numbers cover a relatively small dynamic range for a given bitwidth. In contrast, floating-point numbers offer a larger dynamic range at the cost of increased hardware complexity. In this paper, we propose a novel number format called variable-point (VP). VP numbers cover a larger dynamic range than fixed-point numbers with similar bitwidth, without notably increasing hardware complexity -- this allows for a more efficient representation of signals with high dynamic range. To demonstrate the efficacy of the proposed VP number format, we consider a matrix-vector multiplication engine for spatial equalization in multi-antenna wireless communication systems involving high-dynamic-range signals. Through post-layout VLSI implementation results, we demonstrate that the proposed VP-based design achieves 20% and 10% area and power savings, respectively, compared to a fully optimized fixed-point design, without incurring any noticeable performance degradation.

</details>


### [38] [Efficient Kernel Mapping and Comprehensive System Evaluation of LLM Acceleration on a CGLA](https://arxiv.org/abs/2512.00335)
*Takuto Ando,Yu Eto,Ayumu Takeuchi,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 本文首次全面评估了非AI专用粗粒度线性阵列加速器在Qwen大语言模型上的性能，相比GPU在能效方面有显著优势，但数据传输是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在GPU上运行需要大量计算资源，能耗很高。需要寻找在能效和可编程性之间取得平衡的替代方案，粗粒度可重构阵列(CGRA)是一个有前景的选择。

Method: 使用非AI专用的粗粒度线性阵列(CGLA)加速器，采用通用、任务无关的设计，但具有灵活的指令集支持领域特定适配。在FPGA原型上通过llama.cpp框架评估性能，并投影到28nm ASIC实现。

Result: 相比NVIDIA RTX 4090和Jetson AGX Orin，该加速器在能效方面表现优异：功率延迟积分别提升44.4倍和13.6倍，能量延迟积相比高性能GPU提升11.5倍。但系统级分析显示主机-加速器数据传输是主要性能瓶颈。

Conclusion: 粗粒度可重构阵列是功率受限环境下大语言模型推理的合适平台，无需局限于特定算法。系统级数据传输瓶颈的发现为下一代LLM加速器设计提供了重要指导。

Abstract: Large Language Models (LLMs) demand substantial computational resources, resulting in high energy consumption on GPUs. To address this challenge, we focus on Coarse-Grained Reconfigurable Arrays (CGRAs) as an effective alternative that provides a trade-off between energy efficiency and programmability. This paper presents the first comprehensive, end-to-end evaluation of a non-AI-specialized Coarse-Grained Linear Array (CGLA) accelerator for the state-of-the-art Qwen LLM family. The architecture has a general-purpose, task-agnostic design, yet its flexible instruction set allows for domain-specific adaptations. This flexibility enables the architecture to achieve high efficiency for sustainable LLM inference. We assess the performance of our architecture on an FPGA prototype using the widely adopted llama.cpp framework. We then project its potential as a 28nm ASIC and compare it against a high-performance GPU (NVIDIA RTX 4090) and an edge AI device (NVIDIA Jetson AGX Orin). While GPUs exhibit lower latency, our non-AI-specific accelerator achieves higher energy efficiency, improving the Power-Delay Product (PDP) by up to 44.4x and 13.6x compared with the RTX 4090 and Jetson, respectively. Similarly, it reduces the Energy-Delay Product (EDP) by up to 11.5x compared to the high-performance GPU, demonstrating a favorable performance-energy trade-off. Critically, our system-level analysis identifies host-accelerator data transfer as the primary performance bottleneck, a factor often overlooked in kernel-level studies. These findings provide design guidance for next-generation LLM accelerators. This work validates CGRAs as a suitable platform for LLM inference in power-constrained environments, without being confined to specific algorithms.

</details>


### [39] [A Novel 8T SRAM-Based In-Memory Computing Architecture for MAC-Derived Logical Functions](https://arxiv.org/abs/2512.00441)
*Amogh K M,Sunita M S*

Main category: cs.AR

TL;DR: 提出基于8T SRAM阵列的内存计算架构，支持多比特并行MAC运算和标准内存处理，通过新颖的模拟-数字解码方案实现逻辑运算功能。


<details>
  <summary>Details</summary>
Motivation: 传统6T SRAM设计存在可靠性限制，需要开发更可靠的内存计算架构来支持高效的多比特并行MAC运算和逻辑运算。

Method: 采用8x8 8T SRAM阵列，通过电荷共享在专用读位线上实现MAC运算，解耦读写路径提高可靠性，使用模拟-数字解码方案将MAC电压输出转换为数字计数以实现逻辑功能。

Result: 在90nm CMOS工艺、1.8V电源电压下，实现142.85MHz频率、0.7ns延迟、56.56fJ/bit每MAC操作的性能，吞吐量达15.8M操作/秒。

Conclusion: 8T SRAM内存计算架构成功克服了6T SRAM的可靠性限制，实现了高效的多比特MAC运算和逻辑运算，为内存计算系统提供了可行的解决方案。

Abstract: This paper presents an in-memory computing (IMC) architecture developed on an 8x8 array of 8T SRAM cells. This architecture enables both multi-bit parallel Multiply-Accumulate (MAC) operations and standard memory processing through charge-sharing on dedicated read bit-lines. By leveraging the maturity of SRAM technology, this work introduces an 8T SRAM-based IMC architecture that decouples read and write paths, thereby overcoming the reliability limitations of prior 6T SRAM designs. A novel analog-to-digital decoding scheme converts the MAC voltage output into digital counts, which are subsequently interpreted to realize fundamental logic functions including AND/NAND, NOR/OR, XOR/XNOR, and 1-bit addition within the same array. Simulated in a 90 nm CMOS process at 1.8 V supply voltage, the proposed design achieves 8-bit MAC and logical operations at a frequency of 142.85 MHz, with a latency of 0.7 ns and energy consumption of 56.56 fJ/bit per MAC operation and throughput of 15.8 M operations/s.

</details>


### [40] [A WASM-Subset Stack Architecture for Low-cost FPGAs using Open-Source EDA Flows](https://arxiv.org/abs/2512.00974)
*Aradhya Chakrabarti*

Main category: cs.AR

TL;DR: 本文设计了一种针对资源受限FPGA优化的32位双栈微处理器架构，采用WASM子集指令集提高代码密度，使用开源工具链实现，在Gowin GW1NR-9 FPGA上达到27MHz稳定频率。


<details>
  <summary>Details</summary>
Motivation: 传统软核处理器在资源受限FPGA上存在代码密度低、依赖专有工具链的问题。本文旨在设计一种优化的微处理器架构，提高代码密度，同时利用开源工具链提供透明性和可移植性。

Method: 采用32位双栈架构（数据栈和返回栈），基于WebAssembly子集指令集，通过XIP机制直接从SPI Flash执行代码以减少BRAM使用。使用开源综合流程，分析栈深度参数化权衡，采用8项分布式RAM实现，并通过改进FSM设计解决时序冒险。

Result: 在Gowin GW1NR-9 FPGA上实现，逻辑资源利用率约80%，达到27MHz稳定运行频率（受Flash延迟限制），成功执行单/多位中缀计算器等简单应用。

Conclusion: 该设计在资源受限FPGA上实现了高代码密度和开源工具链支持，通过双栈架构和XIP机制有效平衡了资源利用和性能，为低成本嵌入式系统提供了可行的软核处理器解决方案。

Abstract: Soft-core processors on resource-constrained FPGAs often suffer from low code density and reliance on proprietary toolchains. This paper details the design, implementation, and evaluation of a 32-bit dual-stack microprocessor architecture optimized for low-cost, resource-constrained Field-Programmable Gate Arrays (FPGAs). Implemented on the Gowin GW1NR-9 (Tang Nano 9K), the processor utilizes an instruction set architecture (ISA) inspired from a subset of the WebAssembly (WASM) specification to achieve high code density. Unlike traditional soft-cores that often rely on proprietary vendor toolchains and opaque IP blocks, this design is synthesized and routed utilizing an open-source flow, providing transparency and portability. The architecture features a dual-stack model (Data and Return), executing directly from SPI Flash via an Execute-in-Place (XIP) mechanism to conserve scarce Block RAM on the intended target device. An analysis of the trade-offs involved in stack depth parametrization is presented, demonstrating that an 8-entry distributed RAM implementation provides a balance between logic resource utilization ($\sim 80\%$) and routing congestion. Furthermore, timing hazards in single-cycle stack operations are identified and resolved through a refined Finite State Machine (FSM) design. The system achieves a stable operating frequency of 27 MHz, limited by Flash latency, and successfully executes simple applications including a single and multi-digit infix calculator.

</details>


### [41] [Leveraging Recurrent Patterns in Graph Accelerators](https://arxiv.org/abs/2512.01193)
*Masoud Rahimi,Sébastien Le Beux*

Main category: cs.AR

TL;DR: 提出一种基于ReRAM的图加速器优化方法，通过识别频繁子图模式并静态分配给图引擎，减少内存写入操作，提升性能、能效和电路寿命。


<details>
  <summary>Details</summary>
Motivation: 现有ReRAM图加速器设计存在大量图分区导致的内存访问开销问题，这会增加执行时间、能耗并降低电路寿命。需要一种减少内存写入操作的方法来优化性能。

Method: 通过识别频繁出现的子图模式，将这些模式静态分配给图引擎（称为静态分配），使大多数子图处理无需重新配置交叉开关阵列，从而最小化内存写入操作。

Result: 实验结果显示，相比最先进的图加速器，该方法实现了最高2.38倍的加速和7.23倍的能耗节省，同时将电路寿命延长了2倍。

Conclusion: 提出的基于频繁子图模式识别和静态分配的方法有效减少了ReRAM图加速器的内存写入操作，显著提升了性能、能效和电路寿命。

Abstract: Graph accelerators have emerged as a promising solution for processing large-scale sparse graphs, leveraging the in-situ compu-tation of ReRAM-based crossbars to maximize computational efficiency. However, existing designs suffer from memristor access overhead due to the large number of graph partitions. This leads to increased execution time, higher energy consumption, and re-duced circuit lifetime. This paper proposes a graph processing method that minimizes memristor write operations by identifying frequent subgraph patterns and assigning them to graph engines, referred to as static, allowing most subgraphs to be processed without a need for crossbar reconfiguration. Experimental results show speed up to 2.38x speedup and 7.23x energy savings com-pared to state-of-the-art accelerators. Furthermore, our method extends the circuit lifetime by 2x compared to state-of-the-art ReRAM graph accelerators.

</details>


### [42] [hls4ml: A Flexible, Open-Source Platform for Deep Learning Acceleration on Reconfigurable Hardware](https://arxiv.org/abs/2512.01463)
*Jan-Frederik Schulte,Benjamin Ramhorst,Chang Sun,Jovan Mitrevski,Nicolò Ghielmetti,Enrico Lupi,Dimitrios Danopoulos,Vladimir Loncar,Javier Duarte,David Burnette,Lauri Laatu,Stylianos Tzelepis,Konstantinos Axiotis,Quentin Berthet,Haoyan Wang,Paul White,Suleyman Demirsoy,Marco Colombo,Thea Aarrestad,Sioni Summers,Maurizio Pierini,Giuseppe Di Guglielmo,Jennifer Ngadiuba,Javier Campos,Ben Hawks,Abhijith Gandrakota,Farah Fahim,Nhan Tran,George Constantinides,Zhiqiang Que,Wayne Luk,Alexander Tapper,Duc Hoang,Noah Paladino,Philip Harris,Bo-Cheng Lai,Manuel Valentin,Ryan Forelli,Seda Ogrenci,Lino Gerlach,Rian Flynn,Mia Liu,Daniel Diaz,Elham Khoda,Melissa Quinnan,Russell Solares,Santosh Parajuli,Mark Neubauer,Christian Herwig,Ho Fung Tsoi,Dylan Rankin,Shih-Chieh Hsu,Scott Hauck*

Main category: cs.AR

TL;DR: hls4ml是一个开源平台，可将深度学习模型转换为HLS代码，用于FPGA/ASIC部署，支持多种框架和编译器，适用于低延迟、低功耗的ML推理加速。


<details>
  <summary>Details</summary>
Motivation: 在商业和科学应用中，机器学习推理需要低延迟、低资源使用和低功耗，特别是在边缘计算和实时处理场景中。传统软件实现难以满足这些要求，需要硬件加速解决方案。

Method: 开发了一个模块化、灵活的开源平台hls4ml，能够将现代深度学习框架（如TensorFlow、PyTorch等）的模型转换为高层次综合（HLS）代码。该平台支持多种HLS编译器（Vitis HLS、Intel oneAPI、Catapult HLS），并集成了软硬件协同设计生态系统。

Result: hls4ml平台已成功应用于广泛的商业和科学应用，实现了ML推理的硬件加速。生成的HLS代码在低延迟、资源使用和功耗方面表现出色，满足了关键应用需求。

Conclusion: hls4ml提供了一个有效的解决方案，将深度学习模型部署到FPGA和ASIC硬件上，解决了ML推理在实时、低功耗应用中的挑战。其开源特性和广泛支持使其成为软硬件协同设计的重要工具。

Abstract: We present hls4ml, a free and open-source platform that translates machine learning (ML) models from modern deep learning frameworks into high-level synthesis (HLS) code that can be integrated into full designs for field-programmable gate arrays (FPGAs) or application-specific integrated circuits (ASICs). With its flexible and modular design, hls4ml supports a large number of deep learning frameworks and can target HLS compilers from several vendors, including Vitis HLS, Intel oneAPI and Catapult HLS. Together with a wider eco-system for software-hardware co-design, hls4ml has enabled the acceleration of ML inference in a wide range of commercial and scientific applications where low latency, resource usage, and power consumption are critical. In this paper, we describe the structure and functionality of the hls4ml platform. The overarching design considerations for the generated HLS code are discussed, together with selected performance results.

</details>


### [43] [RoMe: Row Granularity Access Memory System for Large Language Models](https://arxiv.org/abs/2512.01541)
*Hwayong Nam,Seungmin Baek,Jumin Kim,Michael Jaemin Kim,Jung Ho Ahn*

Main category: cs.AR

TL;DR: RoMe提出了一种针对大语言模型优化的新型HBM内存架构，通过行粒度访问替代传统的缓存行粒度，简化内存控制器调度，释放引脚资源以增加额外通道，实现12.5%带宽提升


<details>
  <summary>Details</summary>
Motivation: 现代HBM内存系统虽然演进多代但仍保持缓存行粒度访问，这需要复杂的bank组和伪通道结构，增加了时序参数和控制开销。大语言模型工作负载需要连续数据块传输（KB到MB级别），但在传统HBM中被分割成数百个32B缓存行事务，导致内存控制器调度过于复杂且效率低下。

Method: RoMe采用行粒度访问DRAM，从内存接口中移除列、bank组和伪通道结构。这种设计简化了内存调度，减少每个通道所需的引脚数量。释放的引脚被聚合形成额外通道，以最小的额外引脚成本增加整体带宽。

Result: RoMe能够显著简化LLM工作负载的内存调度逻辑，在最小硬件开销下实现12.5%的带宽提升，为下一代HBM内存系统提供了替代方案。

Conclusion: RoMe展示了针对特定工作负载（如LLM）优化内存架构的价值，通过简化接口和调度逻辑，在最小硬件成本下实现带宽提升，为未来内存系统设计提供了新思路。

Abstract: Modern HBM-based memory systems have evolved over generations while retaining cache line granularity accesses. Preserving this fine granularity necessitated the introduction of bank groups and pseudo channels. These structures expand timing parameters and control overhead, significantly increasing memory controller scheduling complexity. Large language models (LLMs) now dominate deep learning workloads, streaming contiguous data blocks ranging from several kilobytes to megabytes per operation. In a conventional HBM-based memory system, these transfers are fragmented into hundreds of 32B cache line transactions. This forces the memory controller to employ unnecessarily intricate scheduling, leading to growing inefficiency.
  To address this problem, we propose RoMe. RoMe accesses DRAM at row granularity and removes columns, bank groups, and pseudo channels from the memory interface. This design simplifies memory scheduling, thereby requiring fewer pins per channel. The freed pins are aggregated to form additional channels, increasing overall bandwidth by 12.5% with minimal extra pins. RoMe demonstrates how memory scheduling logic can be significantly simplified for representative LLM workloads, and presents an alternative approach for next-generation HBM-based memory systems achieving increased bandwidth with minimal hardware overhead.

</details>


### [44] [A Systematic Characterization of LLM Inference on GPUs](https://arxiv.org/abs/2512.01644)
*Haonan Wang,Xuxin Xiao,Mingyu Yan,Zhuoyuan Zhu,Dengke Han,Duo Wang,Wenming Li,Xiaochun Ye,Cunchen Hu,Hongyang Chen,Guangyu Sun*

Main category: cs.AR

TL;DR: 本文系统分析了LLM推理性能，建立了四维分析框架，从观察到预测，为LLM推理提供了实证基础和优化指导。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型推理性能的理解较为零散，缺乏系统性分析。本文旨在通过系统化研究，建立全面的分析框架，填补这一空白。

Method: 通过全面实验，建立了四维分析框架：1) 两阶段异质性观察；2) 微架构根因分析；3) 系统扩展原则；4) 新兴范式边界。研究从观察到预测系统推进。

Result: 研究不仅为现有研究提供了可靠的实证基础，还发现了新的现象，并为LLM推理提供了实用的优化指导。

Conclusion: 本文的系统性分析框架为理解LLM推理性能提供了全面视角，从观察到预测的方法论有助于推动LLM推理优化和新范式探索。

Abstract: This work presents a systematic characterization of Large Language Model (LLM) inference to address fragmented understanding. Through comprehensive experiments, we establish a four-dimensional analytical framework: (1) Two-Phase Heterogeneity Observation; (2) Microarchitectural Root Cause Analysis; (3) System Scaling Principles; and (4) Emerging Paradigm Boundaries. Our investigation progresses systematically from observation to foresight: identifying performance phenomena, revealing hardware causes, validating system behavior, and exploring new paradigms. This study not only consolidates a reliable empirical foundation for existing research but also provides new discoveries and practical optimization guidance for LLM inference.

</details>
