{"id": "2601.21090", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21090", "abs": "https://arxiv.org/abs/2601.21090", "authors": ["Mohammad Walid Charrwi", "Zaid Hussain"], "title": "Deep Reinforcement Learning for Fault-Adaptive Routing in Eisenstein-Jacobi Interconnection Topologies", "comment": null, "summary": "The increasing density of many-core architectures necessitates interconnection networks that are both high-performance and fault-resilient. Eisenstein-Jacobi (EJ) networks, with their symmetric 6-regular topology, offer superior topological properties but challenge traditional routing heuristics under fault conditions. This paper evaluates three routing paradigms in faulty EJ environments: deterministic Greedy Adaptive Routing, theoretically optimal Dijkstra's algorithm, and a reinforcement learning (RL)-based approach. Using a multi-objective reward function to penalize fault proximity and reward path efficiency, the RL agent learns to navigate around clustered failures that typically induce dead-ends in greedy geometric routing. Dijkstra's algorithm establishes the theoretical performance ceiling by computing globally optimal paths with complete topology knowledge, revealing the true connectivity limits of faulty networks. Quantitative analysis at nine faulty nodes shows greedy routing catastrophically degrades to 10% effective reachability and packet delivery, while Dijkstra proves 52-54% represents the topological optimum. The RL agent achieves 94% effective reachability and 91% packet delivery, making it suitable for distributed deployment. Furthermore, throughput evaluations demonstrate that RL sustains over 90% normalized throughput across all loads, actually outperforming Dijkstra under congestion through implicit load balancing strategies. These results establish RL-based adaptive policies as a practical solution that bridges the gap between greedy's efficiency and Dijkstra's optimality, providing robust, self-healing communication in fault-prone interconnection networks without requiring the global topology knowledge or computational overhead of optimal algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u4e09\u79cd\u8def\u7531\u7b97\u6cd5\u5728\u6545\u969cEJ\u7f51\u7edc\u4e2d\u7684\u8868\u73b0\uff1a\u8d2a\u5a6a\u8def\u7531\u5728\u6545\u969c\u4e0b\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0cDijkstra\u7b97\u6cd5\u63d0\u4f9b\u7406\u8bba\u6700\u4f18\u4f46\u9700\u8981\u5168\u5c40\u62d3\u6251\u77e5\u8bc6\uff0c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u4e14\u9002\u5408\u5206\u5e03\u5f0f\u90e8\u7f72\u3002", "motivation": "\u968f\u7740\u591a\u6838\u67b6\u6784\u5bc6\u5ea6\u589e\u52a0\uff0c\u9700\u8981\u9ad8\u6027\u80fd\u4e14\u5bb9\u9519\u7684\u4e92\u8fde\u7f51\u7edc\u3002EJ\u7f51\u7edc\u5177\u6709\u4f18\u8d8a\u7684\u62d3\u6251\u7279\u6027\uff0c\u4f46\u5728\u6545\u969c\u6761\u4ef6\u4e0b\u5bf9\u4f20\u7edf\u8def\u7531\u542f\u53d1\u5f0f\u7b97\u6cd5\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u8def\u7531\u8303\u5f0f\u5728\u6545\u969c\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u8bc4\u4f30\u4e09\u79cd\u8def\u7531\u8303\u5f0f\uff1a\u786e\u5b9a\u6027\u8d2a\u5a6a\u81ea\u9002\u5e94\u8def\u7531\u3001\u7406\u8bba\u6700\u4f18\u7684Dijkstra\u7b97\u6cd5\u3001\u4ee5\u53ca\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u3002\u4f7f\u7528\u591a\u76ee\u6807\u5956\u52b1\u51fd\u6570\u60e9\u7f5a\u6545\u969c\u63a5\u8fd1\u5ea6\u5e76\u5956\u52b1\u8def\u5f84\u6548\u7387\uff0cRL\u667a\u80fd\u4f53\u5b66\u4e60\u5728\u6545\u969c\u96c6\u7fa4\u5468\u56f4\u5bfc\u822a\u3002", "result": "\u57289\u4e2a\u6545\u969c\u8282\u70b9\u4e0b\uff0c\u8d2a\u5a6a\u8def\u7531\u6709\u6548\u53ef\u8fbe\u6027\u548c\u6570\u636e\u5305\u6295\u9012\u7387\u964d\u81f310%\uff0cDijkstra\u7b97\u6cd5\u8bc1\u660e52-54%\u662f\u62d3\u6251\u6700\u4f18\u503c\uff0c\u800cRL\u667a\u80fd\u4f53\u5b9e\u73b0\u4e8694%\u6709\u6548\u53ef\u8fbe\u6027\u548c91%\u6570\u636e\u5305\u6295\u9012\u7387\u3002\u541e\u5410\u91cf\u8bc4\u4f30\u663e\u793aRL\u5728\u6240\u6709\u8d1f\u8f7d\u4e0b\u7ef4\u6301\u8d85\u8fc790%\u7684\u5f52\u4e00\u5316\u541e\u5410\u91cf\u3002", "conclusion": "\u57fa\u4e8eRL\u7684\u81ea\u9002\u5e94\u7b56\u7565\u662f\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u8d2a\u5a6a\u8def\u7531\u6548\u7387\u548cDijkstra\u7b97\u6cd5\u6700\u4f18\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5728\u6545\u969c\u6613\u53d1\u7684\u4e92\u8fde\u7f51\u7edc\u4e2d\u63d0\u4f9b\u9c81\u68d2\u3001\u81ea\u6108\u7684\u901a\u4fe1\uff0c\u65e0\u9700\u5168\u5c40\u62d3\u6251\u77e5\u8bc6\u6216\u6700\u4f18\u7b97\u6cd5\u7684\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2601.21146", "categories": ["cs.DC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.21146", "abs": "https://arxiv.org/abs/2601.21146", "authors": ["Francesco Paladino", "Shulu Li", "Edward A. Lee"], "title": "Maxwait: A Generalized Mechanism for Distributed Time-Sensitive Systems", "comment": null, "summary": "Distributed time-sensitive systems must balance timing requirements (availability) and consistency in the presence of communication delays and synchronization uncertainty. This paper presents maxwait, a simple coordination mechanism with surprising generality that makes these tradeoffs explicit and configurable. We demonstrate that this mechanism subsumes classical distributed system methods such as PTIDES, Chandy-and-Misra with or without null messages, Jefferson's Time-Warp, and Lamport's time-based fault detection, while enabling real-time behavior in distributed cyber-physical applications. The mechanism can also realize many commonly used distributed system patterns, including logical execution time (LET), publish and subscribe, actors, conflict-free replicated data types (CRDTs), and remote procedure calls with futures. More importantly, it adds to these mechanisms better control over timing, bounded time fault detection, and the option of making them more deterministic, all within a single semantic framework. Implemented as an extension of the Lingua Franca coordination language, maxwait enforces logical-time consistency when communication latencies are bounded and provides structured fault handling when bounds are violated.", "AI": {"tldr": "maxwait\u662f\u4e00\u79cd\u7b80\u5355\u4f46\u901a\u7528\u7684\u534f\u8c03\u673a\u5236\uff0c\u53ef\u5728\u5206\u5e03\u5f0f\u65f6\u95f4\u654f\u611f\u7cfb\u7edf\u4e2d\u663e\u5f0f\u914d\u7f6e\u65f6\u5e8f\u8981\u6c42\u4e0e\u4e00\u81f4\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u6db5\u76d6\u591a\u79cd\u7ecf\u5178\u5206\u5e03\u5f0f\u7cfb\u7edf\u65b9\u6cd5\u5e76\u652f\u6301\u5b9e\u65f6\u884c\u4e3a\u3002", "motivation": "\u5206\u5e03\u5f0f\u65f6\u95f4\u654f\u611f\u7cfb\u7edf\u9700\u8981\u5728\u901a\u4fe1\u5ef6\u8fdf\u548c\u540c\u6b65\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u5e73\u8861\u65f6\u5e8f\u8981\u6c42\uff08\u53ef\u7528\u6027\uff09\u4e0e\u4e00\u81f4\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u63d0\u51famaxwait\u534f\u8c03\u673a\u5236\uff0c\u4f5c\u4e3aLingua Franca\u534f\u8c03\u8bed\u8a00\u7684\u6269\u5c55\u5b9e\u73b0\u3002\u8be5\u673a\u5236\u5728\u901a\u4fe1\u5ef6\u8fdf\u6709\u754c\u65f6\u5f3a\u5236\u6267\u884c\u903b\u8f91\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5728\u8fb9\u754c\u88ab\u8fdd\u53cd\u65f6\u63d0\u4f9b\u7ed3\u6784\u5316\u6545\u969c\u5904\u7406\u3002", "result": "maxwait\u673a\u5236\u80fd\u591f\u6db5\u76d6PTIDES\u3001Chandy-and-Misra\uff08\u542b/\u4e0d\u542b\u7a7a\u6d88\u606f\uff09\u3001Jefferson's Time-Warp\u3001Lamport\u65f6\u95f4\u6545\u969c\u68c0\u6d4b\u7b49\u7ecf\u5178\u65b9\u6cd5\uff0c\u540c\u65f6\u652f\u6301LET\u3001\u53d1\u5e03\u8ba2\u9605\u3001actor\u3001CRDT\u3001RPC with futures\u7b49\u5e38\u89c1\u5206\u5e03\u5f0f\u6a21\u5f0f\uff0c\u5e76\u63d0\u4f9b\u66f4\u597d\u7684\u65f6\u5e8f\u63a7\u5236\u3001\u6709\u754c\u65f6\u95f4\u6545\u969c\u68c0\u6d4b\u548c\u786e\u5b9a\u6027\u589e\u5f3a\u3002", "conclusion": "maxwait\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u8bed\u4e49\u6846\u67b6\uff0c\u4f7f\u5206\u5e03\u5f0f\u65f6\u95f4\u654f\u611f\u7cfb\u7edf\u80fd\u591f\u663e\u5f0f\u914d\u7f6e\u65f6\u5e8f\u4e0e\u4e00\u81f4\u6027\u7684\u6743\u8861\uff0c\u5728\u5355\u4e00\u6846\u67b6\u5185\u5b9e\u73b0\u591a\u79cd\u7ecf\u5178\u65b9\u6cd5\u548c\u6a21\u5f0f\uff0c\u540c\u65f6\u589e\u5f3a\u65f6\u5e8f\u63a7\u5236\u548c\u786e\u5b9a\u6027\u3002"}}
{"id": "2601.21198", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.21198", "abs": "https://arxiv.org/abs/2601.21198", "authors": ["Yuchen Yang", "Yaru Zhao", "Pu Yang", "Shaowei Wang", "Zhi-Hua Zhou"], "title": "ZipMoE: Efficient On-Device MoE Serving via Lossless Compression and Cache-Affinity Scheduling", "comment": null, "summary": "While Mixture-of-Experts (MoE) architectures substantially bolster the expressive power of large-language models, their prohibitive memory footprint severely impedes the practical deployment on resource-constrained edge devices, especially when model behavior must be preserved without relying on lossy quantization. In this paper, we present ZipMoE, an efficient and semantically lossless on-device MoE serving system. ZipMoE exploits the synergy between the hardware properties of edge devices and the statistical redundancy inherent to MoE parameters via a caching-scheduling co-design with provable performance guarantee. Fundamentally, our design shifts the paradigm of on-device MoE inference from an I/O-bound bottleneck to a compute-centric workflow that enables efficient parallelization. We implement a prototype of ZipMoE and conduct extensive experiments on representative edge computing platforms using popular open-source MoE models and real-world workloads. Our evaluation reveals that ZipMoE achieves up to $72.77\\%$ inference latency reduction and up to $6.76\\times$ higher throughput than the state-of-the-art systems.", "AI": {"tldr": "ZipMoE\uff1a\u4e00\u79cd\u9ad8\u6548\u4e14\u8bed\u4e49\u65e0\u635f\u7684\u7aef\u4fa7MoE\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u7f13\u5b58\u8c03\u5ea6\u534f\u540c\u8bbe\u8ba1\uff0c\u5c06\u7aef\u4fa7MoE\u63a8\u7406\u4eceI/O\u74f6\u9888\u8f6c\u53d8\u4e3a\u8ba1\u7b97\u4e2d\u5fc3\u5de5\u4f5c\u6d41\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "MoE\u67b6\u6784\u867d\u7136\u589e\u5f3a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4f46\u5176\u5de8\u5927\u7684\u5185\u5b58\u5360\u7528\u4e25\u91cd\u963b\u788d\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u4fdd\u6301\u6a21\u578b\u884c\u4e3a\u800c\u4e0d\u4f9d\u8d56\u6709\u635f\u91cf\u5316\u7684\u60c5\u51b5\u4e0b\u3002", "method": "ZipMoE\u5229\u7528\u8fb9\u7f18\u8bbe\u5907\u786c\u4ef6\u7279\u6027\u4e0eMoE\u53c2\u6570\u56fa\u6709\u7edf\u8ba1\u5197\u4f59\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\uff0c\u901a\u8fc7\u5177\u6709\u53ef\u8bc1\u660e\u6027\u80fd\u4fdd\u8bc1\u7684\u7f13\u5b58\u8c03\u5ea6\u534f\u540c\u8bbe\u8ba1\uff0c\u5c06\u7aef\u4fa7MoE\u63a8\u7406\u8303\u5f0f\u4eceI/O\u74f6\u9888\u8f6c\u53d8\u4e3a\u8ba1\u7b97\u4e2d\u5fc3\u5de5\u4f5c\u6d41\uff0c\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\u5316\u3002", "result": "\u5728\u4ee3\u8868\u6027\u8fb9\u7f18\u8ba1\u7b97\u5e73\u53f0\u4e0a\u4f7f\u7528\u5f00\u6e90MoE\u6a21\u578b\u548c\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u5b9e\u9a8c\uff0cZipMoE\u76f8\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9ad8\u8fbe72.77%\u7684\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u548c\u9ad8\u8fbe6.76\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "ZipMoE\u901a\u8fc7\u521b\u65b0\u7684\u7f13\u5b58\u8c03\u5ea6\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86MoE\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u65f6\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u8bed\u4e49\u65e0\u635f\u7684\u7aef\u4fa7\u63a8\u7406\u670d\u52a1\u3002"}}
{"id": "2601.21286", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.21286", "abs": "https://arxiv.org/abs/2601.21286", "authors": ["Adithya Bhat", "Harshal Bhadreshkumar Shah", "Mohsen Minaei"], "title": "Ira: Efficient Transaction Replay for Distributed Systems", "comment": null, "summary": "In primary-backup replication, consensus latency is bounded by the time for backup nodes to replay (re-execute) transactions proposed by the primary. In this work, we present Ira, a framework to accelerate backup replay by transmitting compact \\emph{hints} alongside transaction batches. Our key insight is that the primary, having already executed transactions, possesses knowledge of future access patterns which is exactly the information needed for optimal replay.\n  We use Ethereum for our case study and present a concrete protocol, Ira-L, within our framework to improve cache management of Ethereum block execution. The primaries implementing Ira-L provide hints that consist of the working set of keys used in an Ethereum block and one byte of metadata per key indicating the table to read from, and backups use these hints for efficient block replay.\n  We evaluated Ira-L against the state-of-the-art Ethereum client reth over two weeks of Ethereum mainnet activity ($100,800$ blocks containing over $24$ million transactions). Our hints are compact, adding a median of $47$ KB compressed per block ($\\sim5\\%$ of block payload). We observe that the sequential hint generation and block execution imposes a $28.6\\%$ wall-time overhead on the primary, though the direct cost from hints is $10.9\\%$ of execution time; all of which can be pipelined and parallelized in production deployments. On the backup side, we observe that Ira-L achieves a median per-block speedup of $25\\times$ over baseline reth. With $16$ prefetch threads, aggregate replay time drops from $6.5$ hours to $16$ minutes ($23.6\\times$ wall-time speedup).", "AI": {"tldr": "Ira\u6846\u67b6\u901a\u8fc7\u4f20\u8f93\u7d27\u51d1\u7684\u63d0\u793a\u4fe1\u606f\u52a0\u901f\u4e3b\u5907\u590d\u5236\u4e2d\u7684\u5907\u4efd\u91cd\u653e\uff0c\u5728\u4ee5\u592a\u574a\u6848\u4f8b\u4e2d\u5b9e\u73b025\u500d\u52a0\u901f", "motivation": "\u5728\u4e3b\u5907\u590d\u5236\u4e2d\uff0c\u5171\u8bc6\u5ef6\u8fdf\u53d7\u9650\u4e8e\u5907\u4efd\u8282\u70b9\u91cd\u653e\u4e3b\u8282\u70b9\u63d0\u8bae\u4ea4\u6613\u7684\u65f6\u95f4\u3002\u4e3b\u8282\u70b9\u5df2\u7ecf\u6267\u884c\u8fc7\u4ea4\u6613\uff0c\u62e5\u6709\u672a\u6765\u8bbf\u95ee\u6a21\u5f0f\u7684\u77e5\u8bc6\uff0c\u8fd9\u6b63\u662f\u4f18\u5316\u91cd\u653e\u6240\u9700\u7684\u4fe1\u606f\u3002", "method": "\u63d0\u51faIra\u6846\u67b6\uff0c\u901a\u8fc7\u4f20\u8f93\u7d27\u51d1\u7684\u63d0\u793a\u4fe1\u606f\u52a0\u901f\u5907\u4efd\u91cd\u653e\u3002\u5177\u4f53\u5b9e\u73b0Ira-L\u534f\u8bae\u7528\u4e8e\u4ee5\u592a\u574a\uff1a\u4e3b\u8282\u70b9\u63d0\u4f9b\u5305\u542b\u4ee5\u592a\u574a\u533a\u5757\u4e2d\u4f7f\u7528\u7684\u952e\u5de5\u4f5c\u96c6\u548c\u6bcf\u4e2a\u952e\u4e00\u5b57\u8282\u5143\u6570\u636e\u7684\u63d0\u793a\uff0c\u5907\u4efd\u8282\u70b9\u5229\u7528\u8fd9\u4e9b\u63d0\u793a\u8fdb\u884c\u9ad8\u6548\u7684\u533a\u5757\u91cd\u653e\u3002", "result": "\u63d0\u793a\u4fe1\u606f\u7d27\u51d1\uff0c\u6bcf\u4e2a\u533a\u5757\u4e2d\u4f4d\u6570\u589e\u52a047KB\u538b\u7f29\u6570\u636e\uff08\u7ea65%\u7684\u533a\u5757\u8d1f\u8f7d\uff09\u3002\u4e3b\u8282\u70b9\u5f00\u9500\u4e3a28.6%\u7684\u5899\u949f\u65f6\u95f4\uff08\u63d0\u793a\u76f4\u63a5\u6210\u672c\u4e3a10.9%\u6267\u884c\u65f6\u95f4\uff09\u3002\u5907\u4efd\u7aef\u5b9e\u73b0\u4e2d\u4f4d\u6570\u6bcf\u533a\u575725\u500d\u52a0\u901f\uff0c16\u4e2a\u9884\u53d6\u7ebf\u7a0b\u4e0b\u603b\u91cd\u653e\u65f6\u95f4\u4ece6.5\u5c0f\u65f6\u964d\u81f316\u5206\u949f\uff0823.6\u500d\u5899\u949f\u52a0\u901f\uff09\u3002", "conclusion": "Ira\u6846\u67b6\u901a\u8fc7\u5229\u7528\u4e3b\u8282\u70b9\u7684\u6267\u884c\u77e5\u8bc6\u751f\u6210\u7d27\u51d1\u63d0\u793a\uff0c\u663e\u8457\u52a0\u901f\u5907\u4efd\u91cd\u653e\uff0c\u5728\u4ee5\u592a\u574a\u5e94\u7528\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u6240\u6709\u5f00\u9500\u90fd\u53ef\u4ee5\u5728\u751f\u4ea7\u90e8\u7f72\u4e2d\u6d41\u6c34\u7ebf\u5316\u548c\u5e76\u884c\u5316\u3002"}}
{"id": "2601.21842", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.21842", "abs": "https://arxiv.org/abs/2601.21842", "authors": ["Jan-Willem Roorda"], "title": "Optimal Software Pipelining using an SMT-Solver", "comment": null, "summary": "Software Pipelining is a classic and important loop-optimization for VLIW processors. It improves instruction-level parallelism by overlapping multiple iterations of a loop and executing them in parallel. Typically, it is implemented using heuristics. In this paper, we present an optimal software pipeliner based on a Satisfiability Modulo Theories (SMT) Solver. We show that our approach significantly outperforms heuristic algorithms and hand-optimization. Furthermore, we show how the solver can be used to give feedback to programmers and processor designers on why a software pipelined schedule of a certain initiation interval is not feasible.", "AI": {"tldr": "\u57fa\u4e8eSMT\u6c42\u89e3\u5668\u7684\u6700\u4f18\u8f6f\u4ef6\u6d41\u6c34\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u8d85\u8d8a\u542f\u53d1\u5f0f\u7b97\u6cd5\u548c\u624b\u5de5\u4f18\u5316", "motivation": "\u8f6f\u4ef6\u6d41\u6c34\u7ebf\u662fVLIW\u5904\u7406\u5668\u7684\u91cd\u8981\u5faa\u73af\u4f18\u5316\u6280\u672f\uff0c\u4f20\u7edf\u57fa\u4e8e\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u627e\u5230\u6700\u4f18\u89e3\u3002\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u8bc1\u6700\u4f18\u6027\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4e3a\u7a0b\u5e8f\u5458\u548c\u5904\u7406\u5668\u8bbe\u8ba1\u8005\u63d0\u4f9b\u53cd\u9988\u3002", "method": "\u4f7f\u7528\u53ef\u6ee1\u8db3\u6027\u6a21\u7406\u8bba\uff08SMT\uff09\u6c42\u89e3\u5668\u6784\u5efa\u6700\u4f18\u8f6f\u4ef6\u6d41\u6c34\u7ebf\u5668\uff0c\u5c06\u8f6f\u4ef6\u6d41\u6c34\u7ebf\u8c03\u5ea6\u95ee\u9898\u8f6c\u5316\u4e3aSMT\u7ea6\u675f\u6c42\u89e3\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u542f\u53d1\u5f0f\u7b97\u6cd5\u548c\u624b\u5de5\u4f18\u5316\uff0c\u5e76\u80fd\u63d0\u4f9b\u5173\u4e8e\u7279\u5b9a\u542f\u52a8\u95f4\u9694\u4e0d\u53ef\u884c\u7684\u53cd\u9988\u4fe1\u606f\u3002", "conclusion": "\u57fa\u4e8eSMT\u6c42\u89e3\u5668\u7684\u6700\u4f18\u8f6f\u4ef6\u6d41\u6c34\u7ebf\u65b9\u6cd5\u662f\u6709\u6548\u7684\uff0c\u4e0d\u4ec5\u80fd\u627e\u5230\u6700\u4f18\u8c03\u5ea6\u65b9\u6848\uff0c\u8fd8\u80fd\u4e3a\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u53cd\u9988\u3002"}}
{"id": "2601.21222", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.21222", "abs": "https://arxiv.org/abs/2601.21222", "authors": ["Tenglong Li", "Jindong Li", "Guobin Shen", "Dongcheng Zhao", "Qian Zhang", "Yi Zeng"], "title": "FireFly-P: FPGA-Accelerated Spiking Neural Network Plasticity for Robust Adaptive Control", "comment": "5 pages, 4 figures. Accepted for lecture presentation at the 2026 IEEE International Symposium on Circuits and Systems (ISCAS 2026)", "summary": "Spiking Neural Networks (SNNs) offer a biologically plausible learning mechanism through synaptic plasticity, enabling unsupervised adaptation without the computational overhead of backpropagation. To harness this capability for robotics, this paper presents FireFly-P, an FPGA-based hardware accelerator that implements a novel plasticity algorithm for real-time adaptive control. By leveraging on-chip plasticity, our architecture enhances the network's generalization, ensuring robust performance in dynamic and unstructured environments. The hardware design achieves an end-to-end latency of just 8~$\u03bc$s for both inference and plasticity updates, enabling rapid adaptation to unseen scenarios. Implemented on a tiny Cmod A7-35T FPGA, FireFly-P consumes only 0.713~W and $\\sim$10K~LUTs, making it ideal for power- and resource-constrained embedded robotic platforms. This work demonstrates that hardware-accelerated SNN plasticity is a viable path toward enabling adaptive, low-latency, and energy-efficient control systems.", "AI": {"tldr": "FireFly-P\uff1a\u57fa\u4e8eFPGA\u7684SNN\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u5b9e\u73b0\u5b9e\u65f6\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u4ec5\u97008\u03bcs\u5ef6\u8fdf\u548c0.713W\u529f\u8017", "motivation": "\u5229\u7528SNN\u7684\u751f\u7269\u53ef\u5851\u6027\u673a\u5236\u5b9e\u73b0\u65e0\u76d1\u7763\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u907f\u514d\u53cd\u5411\u4f20\u64ad\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u5b9e\u65f6\u9002\u5e94\u52a8\u6001\u975e\u7ed3\u6784\u5316\u73af\u5883\u7684\u80fd\u529b", "method": "\u8bbe\u8ba1FPGA\u786c\u4ef6\u52a0\u901f\u5668FireFly-P\uff0c\u5b9e\u73b0\u65b0\u578b\u53ef\u5851\u6027\u7b97\u6cd5\uff0c\u5728Cmod A7-35T FPGA\u4e0a\u5b9e\u73b0\u7247\u4e0a\u53ef\u5851\u6027\u66f4\u65b0\uff0c\u4f18\u5316\u7f51\u7edc\u6cdb\u5316\u80fd\u529b", "result": "\u7aef\u5230\u7aef\u5ef6\u8fdf\u4ec58\u03bcs\uff08\u63a8\u7406\u548c\u53ef\u5851\u6027\u66f4\u65b0\uff09\uff0c\u529f\u80170.713W\uff0c\u5360\u7528\u7ea610K LUTs\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b9e\u73b0\u5feb\u901f\u9002\u5e94", "conclusion": "\u786c\u4ef6\u52a0\u901f\u7684SNN\u53ef\u5851\u6027\u662f\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u80fd\u6548\u63a7\u5236\u7cfb\u7edf\u7684\u53ef\u884c\u8def\u5f84\uff0c\u7279\u522b\u9002\u5408\u5d4c\u5165\u5f0f\u673a\u5668\u4eba\u5e94\u7528"}}
{"id": "2601.21758", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.21758", "abs": "https://arxiv.org/abs/2601.21758", "authors": ["Bronislav Sidik", "Chaya Levi", "Joseph Kampeas"], "title": "EWSJF: An Adaptive Scheduler with Hybrid Partitioning for Mixed-Workload LLM Inference", "comment": null, "summary": "Serving Large Language Models (LLMs) under mixed workloads--short, latency-sensitive interactive queries alongside long, throughput-oriented batch requests--poses a fundamental scheduling challenge. Standard First-Come, First-Served (FCFS) policies suffer from severe head-of-line blocking, leading to high tail latency and underutilized hardware. We introduce EWSJF (Effective Workload-based Shortest Job First), an adaptive request-level scheduler that learns workload structure in real time to jointly improve fairness and throughput. EWSJF operates upstream of execution-level schedulers and integrates four components: (1) Refine-and-Prune, an unsupervised partitioning algorithm that discovers performance-homogeneous request groups; (2) Dynamic Queue Routing for assigning requests to these groups; (3) Density-Weighted Scoring, a context-aware prioritization function balancing urgency and fairness; and (4) Bayesian Meta-Optimization, which continuously tunes scoring and partitioning parameters based on live performance feedback. Implemented in vLLM, EWSJF improves end-to-end throughput by over 30% and reduces average Time-To-First-Token for short requests by up to 4x compared to FCFS. These results demonstrate that adaptive, learning-based request scheduling is a critical missing layer for efficient and responsive LLM serving. Implementation available at https://anonymous.4open.science/r/vllm_0110-32D8.", "AI": {"tldr": "EWSJF\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u8bf7\u6c42\u7ea7\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7ed3\u6784\u5b9e\u65f6\u4f18\u5316LLM\u670d\u52a1\u4e2d\u7684\u516c\u5e73\u6027\u548c\u541e\u5410\u91cf\uff0c\u76f8\u6bd4FCFS\u63d0\u5347\u541e\u5410\u91cf30%\u4ee5\u4e0a\uff0c\u77ed\u8bf7\u6c42\u5e73\u5747TTFT\u964d\u4f4e4\u500d\u3002", "motivation": "LLM\u670d\u52a1\u4e2d\u6df7\u5408\u5de5\u4f5c\u8d1f\u8f7d\uff08\u77ed\u5ef6\u8fdf\u654f\u611f\u67e5\u8be2+\u957f\u541e\u5410\u91cf\u5bfc\u5411\u6279\u5904\u7406\uff09\u7684\u8c03\u5ea6\u6311\u6218\uff1a\u6807\u51c6FCFS\u7b56\u7565\u5b58\u5728\u4e25\u91cd\u7684\u961f\u5934\u963b\u585e\u95ee\u9898\uff0c\u5bfc\u81f4\u9ad8\u5c3e\u5ef6\u8fdf\u548c\u786c\u4ef6\u5229\u7528\u7387\u4f4e\u4e0b\u3002", "method": "EWSJF\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\uff1a1) Refine-and-Prune\u65e0\u76d1\u7763\u5206\u533a\u7b97\u6cd5\u53d1\u73b0\u6027\u80fd\u540c\u8d28\u8bf7\u6c42\u7ec4\uff1b2) Dynamic Queue Routing\u5c06\u8bf7\u6c42\u5206\u914d\u5230\u8fd9\u4e9b\u7ec4\uff1b3) Density-Weighted Scoring\u4e0a\u4e0b\u6587\u611f\u77e5\u4f18\u5148\u7ea7\u51fd\u6570\u5e73\u8861\u7d27\u6025\u6027\u548c\u516c\u5e73\u6027\uff1b4) Bayesian Meta-Optimization\u57fa\u4e8e\u5b9e\u65f6\u6027\u80fd\u53cd\u9988\u6301\u7eed\u8c03\u4f18\u8bc4\u5206\u548c\u5206\u533a\u53c2\u6570\u3002", "result": "\u5728vLLM\u4e2d\u5b9e\u73b0\uff0c\u76f8\u6bd4FCFS\uff1a\u7aef\u5230\u7aef\u541e\u5410\u91cf\u63d0\u5347\u8d85\u8fc730%\uff0c\u77ed\u8bf7\u6c42\u5e73\u5747TTFT\u964d\u4f4e\u9ad8\u8fbe4\u500d\u3002", "conclusion": "\u81ea\u9002\u5e94\u3001\u57fa\u4e8e\u5b66\u4e60\u7684\u8bf7\u6c42\u8c03\u5ea6\u662f\u9ad8\u6548\u54cd\u5e94LLM\u670d\u52a1\u7684\u5173\u952e\u7f3a\u5931\u5c42\uff0cEWSJF\u901a\u8fc7\u5b9e\u65f6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7ed3\u6784\u663e\u8457\u6539\u5584\u4e86\u516c\u5e73\u6027\u548c\u541e\u5410\u91cf\u3002"}}
{"id": "2601.21584", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.21584", "abs": "https://arxiv.org/abs/2601.21584", "authors": ["Pin-Han Ho", "Limei Peng", "Yiming Miao", "Xu Fan", "Kairan Liang", "Haoran Mei", "Wei Duan"], "title": "Frequency as Aperture: Enabling Embeddable Near-Field Sensing for 6G Wireless Radios", "comment": null, "summary": "Integrated sensing and communication (ISAC) is expected to be natively supported by future 6G wireless radios, yet most mmWave sensing solutions still rely on dedicated radar hardware incompatible with cost and power constrained wireless nodes. This article introduces Frequency-as-Aperture (FaA), a wireless-first sensing paradigm that repurposes inherent frequency agility into a virtual sensing aperture, enabling near-field perception with minimal RF front end complexity. Using a single RF chain and a frequency-scanning leaky-wave antenna, FaA achieves two dimensional spatial sensing by reusing the local oscillator (LO) frequency sweep already employed for wideband communication. From a wireless-system perspective, this shifts spatial sampling from the antenna domain to the frequency domain, embedding radar-grade spatial fingerprints directly into the communication RF chain. A case study shows that FaA provides fine angular and range discrimination with low power consumption and unit cost, demonstrating significantly higher architectural efficiency than conventional multi-channel MIMO based sensing under identical physical and spectral constraints. These results indicate that near-field sensing can be seamlessly integrated into frequency-agile wireless radios, enabling hardware-efficient, embeddable, and privacy-preserving ISAC nodes for smart homes, wearables, and industrial edge deployments.", "AI": {"tldr": "FaA\u5229\u7528\u9891\u7387\u654f\u6377\u6027\u5b9e\u73b0\u865a\u62df\u4f20\u611f\u5b54\u5f84\uff0c\u5c06\u7a7a\u95f4\u91c7\u6837\u4ece\u5929\u7ebf\u57df\u8f6c\u79fb\u5230\u9891\u57df\uff0c\u7528\u5355\u5c04\u9891\u94fe\u5b9e\u73b0\u8fd1\u573a\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u67b6\u6784\u6548\u7387", "motivation": "\u73b0\u6709\u6beb\u7c73\u6ce2\u4f20\u611f\u65b9\u6848\u4f9d\u8d56\u4e13\u7528\u96f7\u8fbe\u786c\u4ef6\uff0c\u4e0e\u6210\u672c\u529f\u8017\u53d7\u9650\u7684\u65e0\u7ebf\u8282\u70b9\u4e0d\u517c\u5bb9\uff0c\u9700\u8981\u65e0\u7ebf\u4f18\u5148\u7684\u4f20\u611f\u8303\u5f0f", "method": "\u4f7f\u7528\u5355\u5c04\u9891\u94fe\u548c\u9891\u7387\u626b\u63cf\u6f0f\u6ce2\u5929\u7ebf\uff0c\u5c06\u672c\u5730\u632f\u8361\u5668\u9891\u7387\u626b\u63cf\u91cd\u7528\u4e8e\u4f20\u611f\uff0c\u901a\u8fc7\u9891\u7387\u57df\u865a\u62df\u5b54\u5f84\u5b9e\u73b0\u4e8c\u7ef4\u7a7a\u95f4\u611f\u77e5", "result": "FaA\u5728\u76f8\u540c\u7269\u7406\u548c\u9891\u8c31\u7ea6\u675f\u4e0b\uff0c\u6bd4\u4f20\u7edf\u591a\u901a\u9053MIMO\u4f20\u611f\u5177\u6709\u66f4\u9ad8\u7684\u67b6\u6784\u6548\u7387\uff0c\u63d0\u4f9b\u7cbe\u7ec6\u7684\u89d2\u5ea6\u548c\u8ddd\u79bb\u5206\u8fa8\u80fd\u529b", "conclusion": "\u8fd1\u573a\u611f\u77e5\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u9891\u7387\u654f\u6377\u65e0\u7ebf\u5c04\u9891\u4e2d\uff0c\u5b9e\u73b0\u786c\u4ef6\u9ad8\u6548\u3001\u53ef\u5d4c\u5165\u3001\u9690\u79c1\u4fdd\u62a4\u7684ISAC\u8282\u70b9"}}
{"id": "2601.21855", "categories": ["cs.DC", "cs.DB", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.21855", "abs": "https://arxiv.org/abs/2601.21855", "authors": ["Chuan-Chi Lai"], "title": "Self-Adaptive Probabilistic Skyline Query Processing in Distributed Edge Computing via Deep Reinforcement Learning", "comment": "12 pages, 4 figures, manuscript submitted to IEEE Transactions on Emerging Topics in Computing", "summary": "In the era of the Internet of Everything (IoE), the exponential growth of sensor-generated data at the network edge renders efficient Probabilistic Skyline Query (PSKY) processing a critical challenge. Traditional distributed PSKY methodologies predominantly rely on pre-defined static thresholds to filter local candidates. However, these rigid approaches are fundamentally ill-suited for the highly volatile and heterogeneous nature of edge computing environments, often leading to either severe communication bottlenecks or excessive local computational latency. To resolve this resource conflict, this paper presents SA-PSKY, a novel Self-Adaptive framework designed for distributed edge-cloud collaborative systems. We formalize the dynamic threshold adjustment problem as a continuous Markov Decision Process (MDP) and leverage a Deep Deterministic Policy Gradient (DDPG) agent to autonomously optimize filtering intensities in real-time. By intelligently analyzing multi-dimensional system states, including data arrival rates, uncertainty distributions, and instantaneous resource availability, our framework effectively minimizes a joint objective function of computation and communication costs. Comprehensive experimental evaluations demonstrate that SA-PSKY consistently outperforms state-of-the-art static and heuristic baselines. Specifically, it achieves a reduction of up to 60\\% in communication overhead and 40\\% in total response time, while ensuring robust scalability across diverse data distributions.", "AI": {"tldr": "SA-PSKY\uff1a\u4e00\u79cd\u7528\u4e8e\u8fb9\u7f18-\u4e91\u534f\u4f5c\u7cfb\u7edf\u7684\u81ea\u9002\u5e94\u6982\u7387\u5929\u9645\u7ebf\u67e5\u8be2\u6846\u67b6\uff0c\u4f7f\u7528DDPG\u667a\u80fd\u4f53\u52a8\u6001\u8c03\u6574\u8fc7\u6ee4\u9608\u503c\uff0c\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u548c\u54cd\u5e94\u65f6\u95f4\u3002", "motivation": "\u5728\u4e07\u7269\u4e92\u8054\u65f6\u4ee3\uff0c\u8fb9\u7f18\u4f20\u611f\u5668\u6570\u636e\u7206\u70b8\u5f0f\u589e\u957f\u4f7f\u5f97\u6982\u7387\u5929\u9645\u7ebf\u67e5\u8be2\u5904\u7406\u9762\u4e34\u6311\u6218\u3002\u4f20\u7edf\u5206\u5e03\u5f0f\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u9608\u503c\uff0c\u65e0\u6cd5\u9002\u5e94\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u7684\u9ad8\u5ea6\u52a8\u6001\u548c\u5f02\u6784\u7279\u6027\uff0c\u5bfc\u81f4\u901a\u4fe1\u74f6\u9888\u6216\u8ba1\u7b97\u5ef6\u8fdf\u3002", "method": "\u63d0\u51faSA-PSKY\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u5c06\u52a8\u6001\u9608\u503c\u8c03\u6574\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u8fde\u7eed\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5229\u7528\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u667a\u80fd\u4f53\u5b9e\u65f6\u4f18\u5316\u8fc7\u6ee4\u5f3a\u5ea6\u3002\u667a\u80fd\u5206\u6790\u591a\u7ef4\u7cfb\u7edf\u72b6\u6001\uff0c\u5305\u62ec\u6570\u636e\u5230\u8fbe\u7387\u3001\u4e0d\u786e\u5b9a\u6027\u5206\u5e03\u548c\u77ac\u65f6\u8d44\u6e90\u53ef\u7528\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cSA-PSKY\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u9759\u6001\u548c\u542f\u53d1\u5f0f\u57fa\u7ebf\u65b9\u6cd5\uff0c\u901a\u4fe1\u5f00\u9500\u964d\u4f4e\u9ad8\u8fbe60%\uff0c\u603b\u54cd\u5e94\u65f6\u95f4\u51cf\u5c1140%\uff0c\u5e76\u5728\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0b\u4fdd\u6301\u9c81\u68d2\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "SA-PSKY\u901a\u8fc7\u81ea\u9002\u5e94\u7684\u52a8\u6001\u9608\u503c\u8c03\u6574\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u6982\u7387\u5929\u9645\u7ebf\u67e5\u8be2\u7684\u8d44\u6e90\u51b2\u7a81\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\u7684\u6700\u4f18\u5316\u5e73\u8861\u3002"}}
{"id": "2601.21935", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.21935", "abs": "https://arxiv.org/abs/2601.21935", "authors": ["Tom Yates", "Yuzhou Cheng", "Ignacio Alzugaray", "Danyal Akarca", "Pedro A. M. Mediano", "Andrew J. Davison"], "title": "Belief Propagation Converges to Gaussian Distributions in Sparsely-Connected Factor Graphs", "comment": "Preprint. Under review. 25 pages (including Appendix). 8 Figures", "summary": "Belief Propagation (BP) is a powerful algorithm for distributed inference in probabilistic graphical models, however it quickly becomes infeasible for practical compute and memory budgets. Many efficient, non-parametric forms of BP have been developed, but the most popular is Gaussian Belief Propagation (GBP), a variant that assumes all distributions are locally Gaussian. GBP is widely used due to its efficiency and empirically strong performance in applications like computer vision or sensor networks - even when modelling non-Gaussian problems. In this paper, we seek to provide a theoretical guarantee for when Gaussian approximations are valid in highly non-Gaussian, sparsely-connected factor graphs performing BP (common in spatial AI). We leverage the Central Limit Theorem (CLT) to prove mathematically that variables' beliefs under BP converge to a Gaussian distribution in complex, loopy factor graphs obeying our 4 key assumptions. We then confirm experimentally that variable beliefs become increasingly Gaussian after just a few BP iterations in a stereo depth estimation task.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u4e86\u5728\u6ee1\u8db3\u56db\u4e2a\u5173\u952e\u5047\u8bbe\u7684\u7a00\u758f\u8fde\u63a5\u56e0\u5b50\u56fe\u4e2d\uff0c\u5373\u4f7f\u662f\u975e\u9ad8\u65af\u95ee\u9898\uff0cBP\u7b97\u6cd5\u7684\u53d8\u91cf\u4fe1\u5ff5\u4e5f\u4f1a\u6536\u655b\u5230\u9ad8\u65af\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7406\u8bba\u3002", "motivation": "\u867d\u7136\u9ad8\u65af\u4fe1\u5ff5\u4f20\u64ad(GBP)\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u4f20\u611f\u5668\u7f51\u7edc\u7b49\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5373\u4f7f\u5904\u7406\u975e\u9ad8\u65af\u95ee\u9898\u65f6\u4e5f\u5e38\u88ab\u4f7f\u7528\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\u8bf4\u660e\u4f55\u65f6\u9ad8\u65af\u8fd1\u4f3c\u5728\u9ad8\u5ea6\u975e\u9ad8\u65af\u3001\u7a00\u758f\u8fde\u63a5\u7684\u56e0\u5b50\u56fe\u4e2d\u662f\u6709\u6548\u7684\u3002", "method": "\u5229\u7528\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406(CLT)\u4ece\u6570\u5b66\u4e0a\u8bc1\u660e\uff0c\u5728\u6ee1\u8db3\u56db\u4e2a\u5173\u952e\u5047\u8bbe\u7684\u590d\u6742\u73af\u72b6\u56e0\u5b50\u56fe\u4e2d\uff0cBP\u7b97\u6cd5\u7684\u53d8\u91cf\u4fe1\u5ff5\u4f1a\u6536\u655b\u5230\u9ad8\u65af\u5206\u5e03\u3002\u5e76\u901a\u8fc7\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8868\u660e\u5728\u6ee1\u8db3\u5047\u8bbe\u7684\u6761\u4ef6\u4e0b\uff0c\u53d8\u91cf\u4fe1\u5ff5\u4f1a\u6536\u655b\u5230\u9ad8\u65af\u5206\u5e03\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\uff0c\u4ec5\u7ecf\u8fc7\u51e0\u6b21BP\u8fed\u4ee3\u540e\uff0c\u53d8\u91cf\u4fe1\u5ff5\u5c31\u53d8\u5f97\u8d8a\u6765\u8d8a\u9ad8\u65af\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aGBP\u5728\u9ad8\u5ea6\u975e\u9ad8\u65af\u3001\u7a00\u758f\u8fde\u63a5\u7684\u56e0\u5b50\u56fe\u4e2d\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48GBP\u5728\u5b9e\u8df5\u4e2d\u5373\u4f7f\u5904\u7406\u975e\u9ad8\u65af\u95ee\u9898\u4e5f\u80fd\u8868\u73b0\u826f\u597d\u3002"}}
