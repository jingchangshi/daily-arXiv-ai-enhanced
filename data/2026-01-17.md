<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [From Dynamic to Lexical: A Comparative Exploration of Scoping Rules in SAS and R](https://arxiv.org/abs/2601.09808)
*Chen Ling,Yachen Wang*

Main category: cs.PL

TL;DR: 本文对比了SAS的动态作用域和R的词法作用域机制，分析了它们在变量访问、解析方式上的差异，并提供了调试和优化策略。


<details>
  <summary>Details</summary>
Motivation: 变量作用域对代码效率和结构至关重要，但SAS和R采用不同的作用域机制（动态vs词法），理解这些差异对程序员优化变量管理、提高代码精确性和可靠性非常重要。

Method: 通过对比分析SAS的动态作用域（使用符号表在运行时解析变量）和R的词法作用域（使用环境基于函数定义结构解析变量），提供具体示例展示不同作用域策略对代码行为的影响，并介绍检查SAS符号表和R环境的方法。

Result: 论文阐明了两种作用域机制的核心差异：SAS在运行时动态搜索活动宏层解析变量，而R基于函数定义时的词法环境解析变量。提供了实用的调试工具和变量作用域控制策略。

Conclusion: 理解SAS和R的作用域差异对程序员优化变量管理至关重要，掌握相关调试和优化策略能显著提高编程实践中的代码精确性和可靠性。

Abstract: Variable scoping dictates how and where variables are accessible within programming languages, playing a crucial role in code efficiency and organization. This paper examines the distinct scoping rules in SAS and R, focusing on SAS's dynamic scoping and R's lexical scoping. In SAS, dynamic scoping utilizes symbol tables, resolving variables at runtime by dynamically searching through active macro layers. R, in contrast, employs lexical scoping, using environments to resolve variables based on the structure in which functions are defined. Illustrative examples highlight the differences between these scoping strategies, showcasing their impact on code behavior. Additionally, the paper outlines methods for inspecting variables in SAS's symbol tables and R's environments, offering practical insights for debugging and optimization. Strategies for controlling variable scope in both languages are discussed, enhancing code precision and reliability. This exploration equips programmers with critical understanding to optimize variable management, improving their programming practices in SAS and R.

</details>


### [2] [Lazy Evaluation: A Comparative Analysis of SAS MACROs and R Functions](https://arxiv.org/abs/2601.09839)
*Chen Ling,Yachen Wang*

Main category: cs.PL

TL;DR: 该论文比较了SAS MACROs和R函数中的惰性求值机制，分析了两种语言在实现惰性求值上的不同策略及其对编程效率的影响。


<details>
  <summary>Details</summary>
Motivation: 随着制药行业从SAS向R的转型日益普遍，理解两种语言中惰性求值技术的差异对于优化代码效率至关重要。虽然R中的惰性求值已被广泛研究，但SAS中的相关应用尚未得到充分探索。

Method: 通过比较分析SAS MACROs和R函数的惰性求值机制：R使用Promise数据结构实现按需调用策略，而SAS通过符号表实现按名调用策略。论文通过示例说明这些差异如何影响编程结果。

Result: 研究发现R的Promise机制在内存使用上更高效（直到需要时才占用内存），而SAS的符号表方法会预先存储参数。这些不同的惰性求值策略显著影响R函数和SAS MACROs的执行结果和效率。

Conclusion: 理解SAS和R中惰性求值机制的差异有助于程序员优化代码，提高编程效率。随着行业从SAS向R的转型，掌握这些技术能够提升在两种语言中的编程能力和性能。

Abstract: Lazy evaluation is a powerful technique that can optimize code execution by deferring evaluations until their results are required, thus enhancing efficiency. In most modern programming languages, like R, lazy evaluation is commonly applied to function arguments. However, the application of lazy evaluation in SAS has not been extensively explored. This paper focuses on the mechanisms of lazy evaluation in SAS MACROs and R functions, offering a comparative analysis of the underlying principles that drive these processes.
  R's lazy evaluation is driven by a data structure called Promise, which postpones evaluation and does not occupy memory until the value is needed, utilizing a call-by-need strategy. SAS, on the other hand, achieves lazy evaluation through its symbol tables, employing memory to store parameters, and operates on a call-by-name basis. These discrepancies in lazy evaluation strategies can notably impact the results of R functions and SAS MACROs. By examining these distinct approaches, the paper illuminates the impact of lazy evaluation on programming efficiency, supported by illustrative examples. As the shift from SAS to R becomes increasingly prevalent in the pharmaceutical industry, understanding these techniques enables programmers to optimize their code for greater efficacy. This exploration serves as a guide to enhance programming capabilities and performance in both languages.

</details>


### [3] [Outrunning Big KATs: Efficient Decision Procedures for Variants of GKAT](https://arxiv.org/abs/2601.09986)
*Cheng Zhang,Qiancheng Fu,Hang Ji,Ines Santacruz Del Valle,Alexandra Silva,Marco Gaboardi*

Main category: cs.PL

TL;DR: 提出基于SAT求解器的符号化技术，实现GKAT自动机迹等价的高效判定算法，并在CF-GKAT系统中验证，相比现有方法获得数量级性能提升


<details>
  <summary>Details</summary>
Motivation: GKAT自动机的迹等价判定在程序验证和编译器优化中很重要，但现有方法效率有限，需要更高效的决策过程来支持实际应用

Method: 采用基于SAT求解器的在线符号化技术，设计了符号导数方法用于CF-GKAT系统，并在Rust中实现算法

Result: 在随机生成基准测试和实际控制流转换案例中，相比KAT和CF-GKAT的现有实现获得数量级性能提升，并在Ghidra反编译器中发现了实际bug

Conclusion: 提出的符号化决策过程显著提高了GKAT迹等价判定的效率，证明了在实际编译器验证中的可行性，并发现了工业级工具中的真实缺陷

Abstract: This paper presents several efficient decision procedures for trace equivalence of GKAT automata, which make use of on-the-fly symbolic techniques via SAT solvers. To demonstrate applicability of our algorithms, we designed symbolic derivatives for CF-GKAT, a practical system based on GKAT designed to validate control-flow transformations. We implemented the algorithms in Rust and evaluated them on both randomly generated benchmarks and real-world control-flow transformations. Indeed, we observed order-of-magnitude performance improvements against existing implementations for both KAT and CF-GKAT. Notably, our experiments also revealed a bug in Ghidra, an industry-standard decompiler, highlighting the practical viability of these systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Federated Unlearning in Edge Networks: A Survey of Fundamentals, Challenges, Practical Applications and Future Directions](https://arxiv.org/abs/2601.09978)
*Jer Shyuan Ng,Wathsara Daluwatta,Shehan Edirimannage,Charitha Elvitigala,Asitha Kottahachchi Kankanamge Don,Ibrahim Khalil,Heng Zhang,Dusit Niyato*

Main category: cs.DC

TL;DR: 本文是一篇关于联邦遗忘（Federated Unlearning, FUL）的综述性论文，系统介绍了这一新兴研究领域，旨在解决联邦学习中数据删除的挑战，以符合隐私法规要求。


<details>
  <summary>Details</summary>
Motivation: 随着连接设备和隐私敏感应用的普及，联邦学习（FL）成为重要的分布式训练范式，但它本身不支持数据删除请求。而像"被遗忘权"（RTBF）这样的法规要求能够删除特定数据的影响，这促使了联邦遗忘（FUL）这一新研究领域的出现。

Method: 本文采用综述研究方法：首先介绍FUL的基础知识，然后回顾现有的FUL框架，这些框架主要解决三个实施挑战：通信成本、资源分配以及安全和隐私。接着讨论FUL在现代分布式计算机网络中的应用，最后指出开放挑战和未来研究方向。

Result: 本文系统梳理了联邦遗忘的研究现状，总结了现有框架如何应对通信成本、资源分配和安全隐私三大挑战，并展示了FUL在分布式网络中的应用潜力，为构建可信、合规、以用户为中心的联邦系统提供了基础参考。

Conclusion: 联邦遗忘是联邦学习的重要扩展，能够满足隐私法规的数据删除要求。本文通过整合现有知识和映射开放问题，旨在为研究人员和从业者提供基础参考，推动构建更可信、合规、以用户为中心的联邦系统。

Abstract: The proliferation of connected devices and privacy-sensitive applications has accelerated the adoption of Federated Learning (FL), a decentralized paradigm that enables collaborative model training without sharing raw data. While FL addresses data locality and privacy concerns, it does not inherently support data deletion requests that are increasingly mandated by regulations such as the Right to be Forgotten (RTBF). In centralized learning, this challenge has been studied under the concept of Machine Unlearning (MU), that focuses on efficiently removing the influence of specific data samples or clients from trained models. Extending this notion to federated settings has given rise to Federated Unlearning (FUL), a new research area concerned with eliminating the contributions of individual clients or data subsets from the global FL model in a distributed and heterogeneous environment. In this survey, we first introduce the fundamentals of FUL. Then, we review the FUL frameworks that are proposed to address the three main implementation challenges, i.e., communication cost, resource allocation as well as security and privacy. Furthermore, we discuss applications of FUL in the modern distributed computer networks. We also highlight the open challenges and future research opportunities. By consolidating existing knowledge and mapping open problems, this survey aims to serve as a foundational reference for researchers and practitioners seeking to advance FL to build trustworthy, regulation-compliant and user-centric federated systems.

</details>


### [5] [Distributed Linearly Separable Computation with Arbitrary Heterogeneous Data Assignment](https://arxiv.org/abs/2601.10177)
*Ziting Zhang,Kai Wan,Minquan Cheng,Shuo Shao,Giuseppe Caire*

Main category: cs.DC

TL;DR: 研究异构分布式线性可分计算问题，针对任意异构数据分配，提出通用计算方案和逆界，刻画任务函数可计算维度与通信成本的基本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注同构设置（每个工作者持有相同数量的数据集），而实际分布式系统中数据分配往往是异构且预先给定的。需要研究在任意异构数据分配下，线性可分任务函数的可计算维度与通信成本之间的基本权衡关系。

Method: 针对整数通信成本约束，提出通用计算方案和通用逆界，通过刻画数据分配结构来分析问题。然后将方案和逆界扩展到分数通信成本情况。

Result: 在任意异构数据分配下，建立了任务函数可计算维度与通信成本的基本权衡关系。在某些参数范围内，提出的计算方案和逆界完全匹配，表明达到了最优性能。

Conclusion: 该研究为异构分布式线性可分计算问题提供了理论框架，揭示了数据分配结构对计算-通信权衡的影响，提出的通用方案和逆界为实际系统设计提供了理论指导。

Abstract: Distributed linearly separable computation is a fundamental problem in large-scale distributed systems, requiring the computation of linearly separable functions over different datasets across distributed workers. This paper studies a heterogeneous distributed linearly separable computation problem, including one master and N distributed workers. The linearly separable task function involves Kc linear combinations of K messages, where each message is a function of one dataset. Distinguished from the existing homogeneous settings that assume each worker holds the same number of datasets, where the data assignment is carefully designed and controlled by the data center (e.g., the cyclic assignment), we consider a more general setting with arbitrary heterogeneous data assignment across workers, where `arbitrary' means that the data assignment is given in advance and `heterogeneous' means that the workers may hold different numbers of datasets. Our objective is to characterize the fundamental tradeoff between the computable dimension of the task function and the communication cost under arbitrary heterogeneous data assignment. Under the constraint of integer communication costs, for arbitrary heterogeneous data assignment, we propose a universal computing scheme and a universal converse bound by characterizing the structure of data assignment, where they coincide under some parameter regimes. We then extend the proposed computing scheme and converse bound to the case of fractional communication costs.

</details>


### [6] [SCRamble: Adaptive Decentralized Overlay Construction for Blockchain Networks](https://arxiv.org/abs/2601.10277)
*Evangelos Kolyvas,Alexandros Antonov,Spyros Voulgaris*

Main category: cs.DC

TL;DR: SCRamble协议通过创新的链路选择策略，结合区块到达时间评分和网络延迟启发式方法，显著减少区块链网络中的区块传播时间，从而提高交易吞吐量和系统安全性。


<details>
  <summary>Details</summary>
Motivation: 区块链发展15年来，交易吞吐量仍是关键挑战，通常受限于每秒有限交易数。网络延迟是限制该指标的根本因素，加速区块传播不仅能提高交易速率，还能通过减少分叉概率增强系统安全性。

Method: 提出SCRamble去中心化协议，采用创新的链路选择策略，集成两种启发式方法：1) 评估相邻节点区块到达时间的评分机制；2) 考虑网络延迟的第二启发式方法。

Result: SCRamble协议能显著减少区块链网络中的区块传播时间。

Conclusion: SCRamble通过创新的链路选择策略有效加速区块传播，从而提高区块链交易吞吐量和系统安全性。

Abstract: Despite being under development for over 15 years, transaction throughput remains one of the key challenges confronting blockchains, which typically has a cap of a limited number of transactions per second. A fundamental factor limiting this metric is the network latency associated with the block propagation throughout of the underlying peer-to-peer network, typically formed through random connections. Accelerating the dissemination of blocks not only improves transaction rates, but also enhances system security by reducing the probability of forks. This paper introduces SCRamble: a decentralized protocol that significantly reduces block dissemination time in blockchain networks. SCRamble's effectiveness is attributed to its innovative link selection strategy, which integrates two heuristics: a scoring mechanism that assesses block arrival times from neighboring peers, and a second heuristic that takes network latency into account.

</details>


### [7] [Mitigating GIL Bottlenecks in Edge AI Systems](https://arxiv.org/abs/2601.10582)
*Mridankan Mandal,Smit Sanjay Shende*

Main category: cs.DC

TL;DR: 针对边缘设备上Python AI代理的运行时优化问题，提出基于阻塞比(beta)度量的轻量级分析工具和自适应运行时系统，解决线程池扩展导致的"饱和悬崖"问题，在资源受限的边缘设备上实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署基于Python的AI代理面临运行时优化挑战：需要高线程数来掩盖I/O延迟，但Python的全局解释器锁(GIL)会序列化执行。简单的线程池扩展会导致"饱和悬崖"现象，在过度配置线程数时造成显著的吞吐量下降。

Method: 提出一个轻量级分析工具和自适应运行时系统，使用阻塞比(beta)度量来区分真正的I/O等待和GIL争用。开发基于库的解决方案，无需手动调优即可实现接近最优性能。与多进程（受内存开销限制）和asyncio（受CPU密集型阶段阻塞）方法进行比较。

Result: 该解决方案实现了96.5%的最优性能，在七个边缘AI工作负载配置（包括使用ONNX Runtime MobileNetV2的真实ML推理）中平均效率达到93.9%。与Python 3.13t（无GIL）的比较显示，虽然GIL消除能在多核设备上实现约4倍吞吐量，但在单核设备上饱和悬崖问题仍然存在，验证了beta度量在GIL和无GIL环境中的有效性。

Conclusion: 提出的基于阻塞比度量的自适应运行时系统为边缘AI系统提供了实用的优化方案，能够有效解决Python在资源受限边缘设备上的运行时性能问题，同时适用于GIL和无GIL环境。

Abstract: Deploying Python based AI agents on resource-constrained edge devices presents a runtime optimization challenge: high thread counts are needed to mask I/O latency, yet Python's Global Interpreter Lock (GIL) serializes execution. We demonstrate that naive thread-pool scaling causes a "saturation cliff": >= 20% throughput degradation at overprovisioned thread counts (N >= 512) on edge-representative configurations. We present a lightweight profiling tool and adaptive runtime system using a Blocking Ratio metric (beta) that distinguishes genuine I/O wait from GIL contention. Our library-based solution achieves 96.5% of optimal performance without manual tuning, outperforming multiprocessing (limited by ~8x memory overhead on devices with 512 MB-2 GB RAM) and asyncio (blocked by CPU-bound phases). Evaluation across seven edge AI workload profiles, including real ML inference with ONNX Runtime MobileNetV2, demonstrates 93.9% average efficiency. Comparative experiments with Python 3.13t (free threading) show that while GIL elimination enables ~4x throughput on multi-core edge devices, the saturation cliff persists on single-core devices, validating our beta metric for both GIL and no-GIL environments. This provides practical optimization for edge AI systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [Enhancing LUT-based Deep Neural Networks Inference through Architecture and Connectivity Optimization](https://arxiv.org/abs/2601.09773)
*Binglei Lou,Ruilin Wu,Philip Leong*

Main category: cs.AR

TL;DR: SparseLUT框架通过架构增强和训练算法优化，解决了LUT-based DNNs中LUT尺寸指数增长和稀疏连接效率低的问题，在保持精度的同时显著降低了硬件资源消耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 在FPGA等资源受限的边缘设备上部署DNN需要平衡延迟、功耗、硬件资源和精度。现有基于查找表(LUT)的DNN（如LogicNets、PolyLUT、NeuraLUT）面临两个关键挑战：LUT尺寸的指数级增长和随机稀疏连接的低效率。

Method: 提出SparseLUT框架，包含两个正交优化：1）架构增强：通过加法器聚合多个PolyLUT子神经元，显著减少LUT消耗；2）非贪婪训练算法：通过选择性剪枝不重要的输入并策略性地重新生长更有效的连接来优化神经元连接。

Result: 架构优化将LUT消耗减少2.0x-13.9x，推理延迟降低1.2x-1.6x，同时保持可比精度。训练优化在基准测试中带来一致的精度提升，在MNIST上达到2.13%增益，在Jet Substructure Classification上达到0.94%增益，且不增加额外面积和延迟开销。

Conclusion: SparseLUT通过架构和训练协同优化，有效解决了LUT-based DNNs的资源效率和连接效率问题，为资源受限边缘设备上的高效DNN部署提供了全面解决方案。

Abstract: Deploying deep neural networks (DNNs) on resource-constrained edge devices such as FPGAs requires a careful balance among latency, power, and hardware resource usage, while maintaining high accuracy. Existing Lookup Table (LUT)-based DNNs -- such as LogicNets, PolyLUT, and NeuraLUT -- face two critical challenges: the exponential growth of LUT size and inefficient random sparse connectivity. This paper presents SparseLUT, a comprehensive framework that addresses these challenges through two orthogonal optimizations. First, we propose an architectural enhancement that aggregates multiple PolyLUT sub-neurons via an adder, significantly reducing LUT consumption by 2.0x-13.9x and lowering inference latency by 1.2x-1.6x, all while maintaining comparable accuracy. Building upon this foundation, we further introduce a non-greedy training algorithm that optimizes neuron connectivity by selectively pruning less significant inputs and strategically regrowing more effective ones. This training optimization, which incurs no additional area and latency overhead, delivers consistent accuracy improvements across benchmarks -- achieving up to a 2.13% gain on MNIST and 0.94% on Jet Substructure Classification compared to existing LUT-DNN approaches.

</details>


### [9] [Architectural Classification of XR Workloads: Cross-Layer Archetypes and Implications](https://arxiv.org/abs/2601.10463)
*Xinyu Shi,Simei Yang,Francky Catthoor*

Main category: cs.AR

TL;DR: 该论文提出了一种跨层方法对XR工作负载进行架构分类，识别出容量受限和开销敏感等工作负载原型，为下一代XR SoC设计提供指导，强调需要从通用资源扩展转向阶段感知调度和弹性资源分配。


<details>
  <summary>Details</summary>
Motivation: 扩展现实(XR)平台需要在严格的功耗和面积约束下提供确定性超低延迟性能，但XR工作负载多样性快速增加，具有异构算子类型和复杂数据流结构，这对以CNN为中心的传统加速器架构构成挑战，且缺乏对完整XR管道的系统性架构理解。

Method: 采用跨层方法整合基于模型的高层设计空间探索(DSE)与商用GPU和CPU硬件的经验分析，分析12个不同XR内核的代表性工作负载集，将其复杂架构特征提炼为少量跨层工作负载原型。

Result: 识别出容量受限和开销敏感等工作负载原型，提取关键架构洞察，为下一代XR SoC提供可操作的设计指南，发现XR架构设计需要从通用资源扩展转向阶段感知调度和弹性资源分配。

Conclusion: XR架构设计必须从通用资源扩展转向阶段感知调度和弹性资源分配，以实现未来XR系统更高的能效和性能，跨层工作负载原型方法为下一代XR SoC设计提供了系统性指导。

Abstract: Edge and mobile platforms for augmented and virtual reality, collectively referred to as extended reality (XR) must deliver deterministic ultra-low-latency performance under stringent power and area constraints. However, the diversity of XR workloads is rapidly increasing, characterized by heterogeneous operator types and complex dataflow structures. This trend poses significant challenges to conventional accelerator architectures centered around convolutional neural networks (CNNs), resulting in diminishing returns for traditional compute-centric optimization strategies. Despite the importance of this problem, a systematic architectural understanding of the full XR pipeline remains lacking. In this paper, we present an architectural classification of XR workloads using a cross-layer methodology that integrates model-based high-level design space exploration (DSE) with empirical profiling on commercial GPU and CPU hardware. By analyzing a representative set of workloads spanning 12 distinct XR kernels, we distill their complex architectural characteristics into a small set of cross-layer workload archetypes (e.g., capacity-limited and overhead-sensitive). Building on these archetypes, we further extract key architectural insights and provide actionable design guidelines for next-generation XR SoCs. Our study highlights that XR architecture design must shift from generic resource scaling toward phase-aware scheduling and elastic resource allocation in order to achieve greater energy efficiency and high performance in future XR systems.

</details>
