{"id": "2512.11762", "categories": ["cs.PL", "math.CT"], "pdf": "https://arxiv.org/pdf/2512.11762", "abs": "https://arxiv.org/abs/2512.11762", "authors": ["Jack Liell-Cock", "Zev Shirazi", "Sam Staton"], "title": "The Relative Monadic Metalanguage", "comment": "41 pages. Published in Proceedings of the ACM on Programming Languages (POPL 2026)", "summary": "Relative monads provide a controlled view of computation. We generalise the monadic metalanguage to a relative setting and give a complete semantics with strong relative monads. Adopting this perspective, we generalise two existing program calculi from the literature. We provide a linear-non-linear language for graded monads, LNL-RMM, along with a semantic proof that it is a conservative extension of the graded monadic metalanguage. Additionally, we provide a complete semantics for the arrow calculus, showing it is a restricted relative monadic metalanguage. This motivates the introduction of ARMM, a computational lambda calculus-style language for arrows that conservatively extends the arrow calculus.", "AI": {"tldr": "\u8bba\u6587\u5c06\u76f8\u5bf9\u5355\u5b50\u7406\u8bba\u6269\u5c55\u5230\u7f16\u7a0b\u8bed\u8a00\u8bbe\u8ba1\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7a0b\u5e8f\u6f14\u7b97\uff1a\u7528\u4e8e\u5206\u7ea7\u5355\u5b50\u7684LNL-RMM\u8bed\u8a00\u548c\u7528\u4e8e\u7bad\u5934\u7684ARMM\u8bed\u8a00\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u76f8\u5bf9\u4e8e\u73b0\u6709\u8bed\u8a00\u7684\u4fdd\u5b88\u6269\u5c55\u6027\u3002", "motivation": "\u76f8\u5bf9\u5355\u5b50\u4e3a\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53d7\u63a7\u7684\u89c6\u56fe\uff0c\u4f46\u73b0\u6709\u7684\u5355\u5b50\u5143\u8bed\u8a00\u9700\u8981\u6269\u5c55\u5230\u76f8\u5bf9\u8bbe\u7f6e\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u76f8\u5bf9\u5355\u5b50\u7684\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6\u548c\u6269\u5c55\u4e24\u79cd\u73b0\u6709\u7684\u7a0b\u5e8f\u6f14\u7b97\uff1a\u5206\u7ea7\u5355\u5b50\u5143\u8bed\u8a00\u548c\u7bad\u5934\u6f14\u7b97\u3002", "method": "1. \u5c06\u5355\u5b50\u5143\u8bed\u8a00\u63a8\u5e7f\u5230\u76f8\u5bf9\u8bbe\u7f6e\uff0c\u5e76\u63d0\u4f9b\u5f3a\u76f8\u5bf9\u5355\u5b50\u7684\u5b8c\u6574\u8bed\u4e49\n2. \u63d0\u51faLNL-RMM\u8bed\u8a00\uff08\u7ebf\u6027-\u975e\u7ebf\u6027\u8bed\u8a00\u7528\u4e8e\u5206\u7ea7\u5355\u5b50\uff09\uff0c\u5e76\u8bc1\u660e\u5b83\u662f\u5206\u7ea7\u5355\u5b50\u5143\u8bed\u8a00\u7684\u4fdd\u5b88\u6269\u5c55\n3. \u4e3a\u7bad\u5934\u6f14\u7b97\u63d0\u4f9b\u5b8c\u6574\u8bed\u4e49\uff0c\u5c55\u793a\u5b83\u662f\u53d7\u9650\u7684\u76f8\u5bf9\u5355\u5b50\u5143\u8bed\u8a00\n4. \u5f15\u5165ARMM\u8bed\u8a00\uff08\u7bad\u5934\u8ba1\u7b97lambda\u6f14\u7b97\u98ce\u683c\u8bed\u8a00\uff09\uff0c\u8bc1\u660e\u5b83\u4fdd\u5b88\u6269\u5c55\u4e86\u7bad\u5934\u6f14\u7b97", "result": "1. \u6210\u529f\u5c06\u5355\u5b50\u5143\u8bed\u8a00\u63a8\u5e7f\u5230\u76f8\u5bf9\u8bbe\u7f6e\uff0c\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u8bed\u4e49\u7406\u8bba\n2. LNL-RMM\u8bed\u8a00\u88ab\u8bc1\u660e\u662f\u5206\u7ea7\u5355\u5b50\u5143\u8bed\u8a00\u7684\u4fdd\u5b88\u6269\u5c55\n3. \u7bad\u5934\u6f14\u7b97\u88ab\u8bc1\u660e\u662f\u53d7\u9650\u7684\u76f8\u5bf9\u5355\u5b50\u5143\u8bed\u8a00\n4. ARMM\u8bed\u8a00\u88ab\u63d0\u51fa\u5e76\u8bc1\u660e\u4fdd\u5b88\u6269\u5c55\u4e86\u7bad\u5934\u6f14\u7b97", "conclusion": "\u76f8\u5bf9\u5355\u5b50\u7406\u8bba\u4e3a\u7a0b\u5e8f\u8bed\u8a00\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u7136\u5730\u6269\u5c55\u73b0\u6709\u7684\u7a0b\u5e8f\u6f14\u7b97\u3002\u8bba\u6587\u63d0\u51fa\u7684LNL-RMM\u548cARMM\u8bed\u8a00\u5c55\u793a\u4e86\u76f8\u5bf9\u5355\u5b50\u89c6\u89d2\u5728\u7f16\u7a0b\u8bed\u8a00\u7406\u8bba\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3a\u672a\u6765\u7684\u8bed\u8a00\u8bbe\u8ba1\u548c\u8bed\u4e49\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002"}}
{"id": "2512.10977", "categories": ["cs.DC", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.10977", "abs": "https://arxiv.org/abs/2512.10977", "authors": ["Alec M. Hammond", "Aram Markosyan", "Aman Dontula", "Simon Mahns", "Zacharias Fisches", "Dmitrii Pedchenko", "Keyur Muzumdar", "Natacha Supper", "Mark Saroufim", "Joe Isaacson", "Laura Wang", "Warren Hunt", "Kaustubh Gondkar", "Roman Levenstein", "Gabriel Synnaeve", "Richard Li", "Jacob Kahn", "Ajit Mathews"], "title": "Agentic Operator Generation for ML ASICs", "comment": null, "summary": "We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.", "AI": {"tldr": "TritorX\u662f\u4e00\u4e2aAI\u7cfb\u7edf\uff0c\u80fd\u591f\u5927\u89c4\u6a21\u751f\u6210\u529f\u80fd\u6b63\u786e\u7684Triton PyTorch ATen\u5185\u6838\uff0c\u4e13\u6ce8\u4e8e\u65b0\u5174\u52a0\u901f\u5668\u5e73\u53f0\uff0c\u5f3a\u8c03\u8986\u76d6\u7387\u548c\u6b63\u786e\u6027\u800c\u975e\u6027\u80fd\u3002", "motivation": "\u4e3a\u65b0\u5174\u52a0\u901f\u5668\u5e73\u53f0\u5feb\u901f\u751f\u6210\u5b8c\u6574\u7684PyTorch ATen\u540e\u7aef\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u53ea\u5173\u6ce8\u5c11\u6570\u9ad8\u6027\u80fd\u5185\u6838\u800c\u7f3a\u4e4f\u5168\u9762\u8986\u76d6\u7684\u95ee\u9898\u3002", "method": "\u96c6\u6210\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u3001\u81ea\u5b9a\u4e49\u4ee3\u7801\u68c0\u67e5\u5668\u3001JIT\u7f16\u8bd1\u548c\u57fa\u4e8ePyTorch OpInfo\u7684\u6d4b\u8bd5\u6846\u67b6\uff0c\u652f\u6301\u771f\u5b9eMTIA\u82af\u7247\u548c\u786c\u4ef6\u4eff\u771f\u73af\u5883\u3002", "result": "\u6210\u529f\u4e3a481\u4e2a\u72ec\u7279\u7684ATen\u7b97\u5b50\u751f\u6210\u4e86\u901a\u8fc7\u6240\u6709PyTorch OpInfo\u6d4b\u8bd5\u7684\u5185\u6838\u548c\u5305\u88c5\u5668\uff08\u603b\u8ba1\u8d85\u8fc720,000\u4e2a\u6d4b\u8bd5\uff09\u3002", "conclusion": "TritorX\u80fd\u591f\u5728\u4e00\u591c\u4e4b\u95f4\u4e3a\u65b0\u52a0\u901f\u5668\u5e73\u53f0\u751f\u6210\u5b8c\u6574\u7684PyTorch ATen\u540e\u7aef\uff0c\u5b9e\u73b0\u4e86\u8986\u76d6\u7387\u548c\u6b63\u786e\u6027\u7684\u7a81\u7834\u3002"}}
{"id": "2512.11200", "categories": ["cs.DC", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.11200", "abs": "https://arxiv.org/abs/2512.11200", "authors": ["Adilet Metinov", "Gulida M. Kudakeeva", "Gulnara D. Kabaeva"], "title": "Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration", "comment": "9 pages , 2 tables", "summary": "Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u79cdGPU\u539f\u751f\u7f16\u8bd1\u65b9\u6cd5\u6d88\u9664CPU-GPU\u6570\u636e\u4f20\u8f93\u74f6\u9888\uff0c\u7406\u8bba\u5206\u6790\u663e\u793a\u53ef\u5b9e\u73b010-100\u500d\u4ee3\u7801\u8fed\u4ee3\u52a0\u901f", "motivation": "\u5f53\u524dAI\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u5728\u7f16\u8bd1\u3001\u6267\u884c\u548c\u6d4b\u8bd5\u9636\u6bb5\u5b58\u5728\u663e\u8457\u7684CPU-GPU\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\u74f6\u9888\uff0c\u9650\u5236\u4e86\u4ee3\u7801\u8fed\u4ee3\u6548\u7387", "method": "\u63d0\u51fa\u4e09\u79cd\u4e92\u8865\u7684GPU\u539f\u751f\u7f16\u8bd1\u65b9\u6cd5\uff1a1) \u5e76\u884c\u4f20\u7edf\u7f16\u8bd1\u9002\u914dGPU\u6267\u884c\uff1b2) \u795e\u7ecf\u7f16\u8bd1\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u7ffb\u8bd1\u4e0e\u6982\u7387\u9a8c\u8bc1\uff1b3) \u7ed3\u5408\u4e24\u8005\u7684\u6df7\u5408\u67b6\u6784", "result": "\u7406\u8bba\u5206\u6790\u663e\u793a\uff1a\u4f20\u7edfGPU\u7f16\u8bd1\u901a\u8fc7\u6d88\u9664\u4f20\u8f93\u53ef\u83b7\u5f972-5\u500d\u6539\u8fdb\uff0c\u795e\u7ecf\u7f16\u8bd1\u901a\u8fc7\u5927\u89c4\u6a21\u5e76\u884c\u5b9e\u73b010-100\u500d\u52a0\u901f\uff0c\u6df7\u5408\u65b9\u6cd5\u63d0\u4f9b\u5177\u6709\u4fdd\u8bc1\u6b63\u786e\u6027\u7684\u5b9e\u7528\u90e8\u7f72\u8def\u5f84", "conclusion": "GPU\u539f\u751f\u7f16\u8bd1\u80fd\u663e\u8457\u52a0\u901f\u4ee3\u7801\u8fed\u4ee3\u5468\u671f\uff0c\u6982\u7387\u9a8c\u8bc1\u6846\u67b6\u5141\u8bb8\u5728\u7f16\u8bd1\u51c6\u786e\u6027\u548c\u5e76\u884c\u63a2\u7d22\u4e4b\u95f4\u6743\u8861\uff0c\u5bf9\u81ea\u6539\u8fdbAI\u7cfb\u7edf\u548c\u672a\u6765\u6a21\u62df\u8ba1\u7b97\u57fa\u677f\u6709\u91cd\u8981\u5f71\u54cd"}}
{"id": "2512.11550", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.11550", "abs": "https://arxiv.org/abs/2512.11550", "authors": ["Yifan Zhang", "Zhiheng Chen", "Ye Qiao", "Sitao Huang"], "title": "PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration", "comment": null, "summary": "Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost.", "AI": {"tldr": "PD-Swap\u662f\u4e00\u79cd\u57fa\u4e8eFPGA\u7684LLM\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u90e8\u5206\u91cd\u914d\u7f6e\u6280\u672f\u5c06\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u5206\u79bb\uff0c\u9488\u5bf9\u4e0d\u540c\u9636\u6bb5\u4f18\u5316\u786c\u4ef6\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8fb9\u7f18FPGA\u4e0a\u7684\u91cf\u5316LLM\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u9762\u4e34\u6027\u80fd\u74f6\u9888\uff1a\u9884\u586b\u5145\u9636\u6bb5\u8ba1\u7b97\u5bc6\u96c6\uff0c\u89e3\u7801\u9636\u6bb5\u5185\u5b58\u5e26\u5bbd\u53d7\u9650\u3002\u9759\u6001\u52a0\u901f\u5668\u9700\u8981\u4e3a\u4e24\u4e2a\u9636\u6bb5\u63d0\u4f9b\u7edf\u4e00\u8d44\u6e90\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u548c\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faPD-Swap\u67b6\u6784\uff0c\u4f7f\u7528\u52a8\u6001\u90e8\u5206\u91cd\u914d\u7f6e\u6280\u672f\uff0c\u5728FPGA\u4e0a\u5b9e\u73b0\u9884\u586b\u5145-\u89e3\u7801\u5206\u79bb\u3002\u6838\u5fc3\u7684\u67e5\u8868\u5f0f\u4e09\u5143\u77e9\u9635\u4e58\u6cd5\u548c\u6743\u91cd\u7f13\u51b2\u5f15\u64ce\u4fdd\u6301\u9759\u6001\uff0c\u6ce8\u610f\u529b\u5b50\u7cfb\u7edf\u4f5c\u4e3a\u53ef\u91cd\u914d\u7f6e\u5206\u533a\uff0c\u5305\u542b\u4e24\u4e2a\u4e13\u95e8\u5316\u67b6\u6784\uff1a\u8ba1\u7b97\u5bc6\u96c6\u7684\u9884\u586b\u5145\u5f15\u64ce\u548c\u5e26\u5bbd\u4f18\u5316\u7684\u89e3\u7801\u5f15\u64ce\u3002", "result": "PD-Swap\u5728\u8fb9\u7f18FPGA\u4e0a\u5b9e\u73b0\u9ad8\u8fbe27 tokens/s\u7684\u89e3\u7801\u541e\u5410\u91cf\uff0c\u6bd4\u73b0\u6709\u6700\u4f18\u5de5\u4f5c\u63d0\u53471.3-2.1\u500d\uff08\u4e0a\u4e0b\u6587\u8d8a\u957f\u63d0\u5347\u8d8a\u5927\uff09\uff0c\u4e14\u4e0d\u589e\u52a0\u989d\u5916\u9762\u79ef\u6210\u672c\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u90e8\u5206\u91cd\u914d\u7f6e\u5b9e\u73b0\u9884\u586b\u5145-\u89e3\u7801\u5206\u79bb\u7684\u67b6\u6784\u80fd\u6709\u6548\u89e3\u51b3LLM\u63a8\u7406\u4e2d\u7684\u9636\u6bb5\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u4e0a\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u6027\u80fd\uff0c\u4e3a\u4f4e\u529f\u8017AI\u90e8\u7f72\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2512.10974", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10974", "abs": "https://arxiv.org/abs/2512.10974", "authors": ["Sohan Kumar Pande", "Sanjaya Kumar Panda", "Preeti Ranjan Sahu"], "title": "An Efficient Approach for Energy Conservation in Cloud Computing Environment", "comment": null, "summary": "Recent trends of technology have explored a numerous applications of cloud services, which require a significant amount of energy. In the present scenario, most of the energy sources are limited and have a greenhouse effect on the environment. Therefore, it is the need of the hour that the energy consumed by the cloud service providers must be reduced and it is a great challenge to the research community to develop energy-efficient algorithms. To design the same, some researchers tried to maximize the average resource utilization, whereas some researchers tried to minimize the makespan. However, they have not considered different types of resources that are present in the physical machines. In this paper, we propose a task scheduling algorithm, which tries to improve utilization of resources (like CPU, disk, I/O) explicitly, which in turn increases the utilization of active resources. For this, the proposed algorithm uses a fitness value, which is a function of CPU, disk and I/O utilization, and processing time of the task. To demonstrate the performance of the proposed algorithm, extensive simulations are performed on both proposed algorithm and existing algorithm MaxUtil using synthetic datasets. From the simulation results, it can be observed that the proposed algorithm is a better energy-efficient algorithm and consumes less energy than the MaxUtil algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eCPU\u3001\u78c1\u76d8\u548cI/O\u8d44\u6e90\u5229\u7528\u7387\u7684\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\uff0c\u901a\u8fc7\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u7387\u6765\u964d\u4f4e\u4e91\u670d\u52a1\u80fd\u8017", "motivation": "\u4e91\u670d\u52a1\u80fd\u8017\u5de8\u5927\uff0c\u4f20\u7edf\u80fd\u6e90\u6709\u9650\u4e14\u5bf9\u73af\u5883\u6709\u6e29\u5ba4\u6548\u5e94\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5e73\u5747\u8d44\u6e90\u5229\u7528\u7387\u6216\u6700\u5c0f\u5316\u5b8c\u6210\u65f6\u95f4\uff0c\u4f46\u672a\u5145\u5206\u8003\u8651\u7269\u7406\u673a\u4e2d\u4e0d\u540c\u7c7b\u578b\u8d44\u6e90\uff08CPU\u3001\u78c1\u76d8\u3001I/O\uff09\u7684\u5dee\u5f02", "method": "\u63d0\u51fa\u57fa\u4e8e\u9002\u5e94\u5ea6\u503c\u7684\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\uff0c\u9002\u5e94\u5ea6\u503c\u662fCPU\u3001\u78c1\u76d8\u3001I/O\u5229\u7528\u7387\u548c\u4efb\u52a1\u5904\u7406\u65f6\u95f4\u7684\u51fd\u6570\uff0c\u901a\u8fc7\u4f18\u5316\u8d44\u6e90\u5229\u7528\u7387\u6765\u63d0\u9ad8\u6d3b\u8dc3\u8d44\u6e90\u5229\u7528\u7387", "result": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u4eff\u771f\u5b9e\u9a8c\uff0c\u4e0e\u73b0\u6709MaxUtil\u7b97\u6cd5\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\u63d0\u51fa\u7684\u7b97\u6cd5\u66f4\u8282\u80fd\uff0c\u80fd\u8017\u66f4\u4f4e", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u901a\u8fc7\u663e\u5f0f\u4f18\u5316\u591a\u79cd\u8d44\u6e90\u5229\u7528\u7387\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u80fd\u6548\u8868\u73b0\uff0c\u4e3a\u4e91\u670d\u52a1\u8282\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.10979", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10979", "abs": "https://arxiv.org/abs/2512.10979", "authors": ["Sima Attar-Khorasani", "Lincoln Sherpa", "Matthias Lieber", "Siavash Ghiasvand"], "title": "Seamless Transitions: A Comprehensive Review of Live Migration Technologies", "comment": "35 pages, 0 figures", "summary": "Live migration, a technology enabling seamless transition of operational computational entities between various hosts while preserving continuous functionality and client connectivity, has been the subject of extensive research. However, existing reviews often overlook critical technical aspects and practical challenges integral to the usage of live migration techniques in real-world scenarios. This work bridges this gap by integrating the aspects explored in existing reviews together with a comprehensive analysis of live migration technologies across multiple dimensions, with focus on migration techniques, migration units, and infrastructure characteristics. Despite efforts to make live migration widely accessible, its reliance on multiple system factors can create challenges. In certain cases, the complexities and resource demands outweigh the benefits, making its implementation hard to justify. The focus of this work is mainly on container based and virtual machine-based migration technologies, examining the current state of the art and the disparity in adoption between these two approaches. Furthermore, this work explores the impact of migration objectives and operational constraints on the usability and efficacy of existing technologies. By outlining current technical challenges and providing guidelines for future research and development directions, this work serves a dual purpose: first, to equip enthusiasts with a valuable resource on live migration, and second, to contribute to the advancement of live migration technologies and their practical implementation across diverse computing environments.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5b9e\u65f6\u8fc1\u79fb\u6280\u672f\u8fdb\u884c\u5168\u9762\u7efc\u8ff0\uff0c\u91cd\u70b9\u5206\u6790\u5bb9\u5668\u548c\u865a\u62df\u673a\u8fc1\u79fb\u7684\u6280\u672f\u73b0\u72b6\u3001\u5b9e\u9645\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411", "motivation": "\u73b0\u6709\u7efc\u8ff0\u5f80\u5f80\u5ffd\u89c6\u5b9e\u65f6\u8fc1\u79fb\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6280\u672f\u7ec6\u8282\u548c\u5b9e\u8df5\u6311\u6218\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u5168\u9762\u7684\u6280\u672f\u5206\u6790", "method": "\u6574\u5408\u73b0\u6709\u7efc\u8ff0\u5185\u5bb9\uff0c\u4ece\u8fc1\u79fb\u6280\u672f\u3001\u8fc1\u79fb\u5355\u5143\u548c\u57fa\u7840\u8bbe\u65bd\u7279\u6027\u7b49\u591a\u4e2a\u7ef4\u5ea6\u7efc\u5408\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8\u5bb9\u5668\u548c\u865a\u62df\u673a\u4e24\u79cd\u8fc1\u79fb\u65b9\u5f0f", "result": "\u53d1\u73b0\u5b9e\u65f6\u8fc1\u79fb\u6280\u672f\u867d\u7136\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5176\u5bf9\u591a\u79cd\u7cfb\u7edf\u56e0\u7d20\u7684\u4f9d\u8d56\u5e26\u6765\u4e86\u5b9e\u9645\u6311\u6218\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u590d\u6742\u6027\u548c\u8d44\u6e90\u9700\u6c42\u53ef\u80fd\u8d85\u8fc7\u5176\u6536\u76ca", "conclusion": "\u672c\u6587\u901a\u8fc7\u6982\u8ff0\u5f53\u524d\u6280\u672f\u6311\u6218\u548c\u63d0\u4f9b\u672a\u6765\u7814\u7a76\u65b9\u5411\u6307\u5357\uff0c\u65e2\u4e3a\u7231\u597d\u8005\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u53c8\u6709\u52a9\u4e8e\u63a8\u52a8\u5b9e\u65f6\u8fc1\u79fb\u6280\u672f\u5728\u4e0d\u540c\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u5e94\u7528"}}
{"id": "2512.10980", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10980", "abs": "https://arxiv.org/abs/2512.10980", "authors": ["Akhmadillo Mamirov"], "title": "Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling", "comment": null, "summary": "GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e09\u79cd\u52a8\u6001\u8c03\u5ea6\u5668(HPS\u3001PBS\u3001SBS)\u89e3\u51b3GPU\u96c6\u7fa4\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u572864-GPU\u96c6\u7fa4\u6a21\u62df\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u5229\u7528\u7387\u81f378.2%\uff0c\u541e\u5410\u91cf\u8fbe25.8\u4f5c\u4e1a/\u5c0f\u65f6\uff0c\u51cf\u5c11\u9965\u997f\u4f5c\u4e1a\u6570\u91cf\u3002", "motivation": "GPU\u96c6\u7fa4\u5728AI\u8bad\u7ec3\u548c\u90e8\u7f72\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u7684\u5e73\u5747\u5229\u7528\u7387\u4ec5\u7ea650%\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8d44\u6e90\u788e\u7247\u5316\u3001\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u548c\u9759\u6001\u8c03\u5ea6\u7b56\u7565\u7684\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u4e13\u7528\u52a8\u6001\u8c03\u5ea6\u5668\uff1a\u6df7\u5408\u4f18\u5148\u7ea7\u8c03\u5ea6(HPS)\u3001\u9884\u6d4b\u6027\u56de\u586b\u8c03\u5ea6(PBS)\u548c\u667a\u80fd\u6279\u5904\u7406\u8c03\u5ea6(SBS)\uff0c\u5e76\u5728\u5305\u542b1000\u4e2aAI\u4f5c\u4e1a\u768464-GPU\u30018\u8282\u70b9\u96c6\u7fa4\u4e0a\u8fdb\u884c\u53d7\u63a7\u6a21\u62df\u8bc4\u4f30\u3002", "result": "\u9759\u6001\u57fa\u7ebf\u8c03\u5ea6(FIFO\u3001SJF\u7b49)GPU\u5229\u7528\u7387\u4e3a45-67%\uff0c\u541e\u5410\u91cf12.5-18.3\u4f5c\u4e1a/\u5c0f\u65f6\uff0c\u6700\u591a156\u4e2a\u4f5c\u4e1a\u7b49\u5f85\u8d85\u8fc730\u5206\u949f\u3002\u52a8\u6001\u8c03\u5ea6\u5668\u663e\u8457\u4f18\u4e8e\u9759\u6001\u7b56\u7565\uff1aHPS\u8fbe\u5230\u6700\u9ad8\u5229\u7528\u738778.2%\u548c\u6700\u9ad8\u541e\u5410\u91cf25.8\u4f5c\u4e1a/\u5c0f\u65f6\uff0c\u9965\u997f\u4f5c\u4e1a\u51cf\u5c11\u81f312\u4e2a\uff1bPBS\u8fbe\u523076.1%\u5229\u7528\u7387\uff1bSBS\u8fbe\u523074.6%\u5229\u7528\u7387\u3002", "conclusion": "\u52a8\u6001\u591a\u76ee\u6807\u8c03\u5ea6\u5668\u5728\u541e\u5410\u91cf\u3001\u4f5c\u4e1a\u7b49\u5f85\u65f6\u95f4\u3001\u516c\u5e73\u6027\u65b9\u5dee\u548c\u9965\u997f\u7b49\u65b9\u9762\u5747\u4f18\u4e8e\u5355\u76ee\u6807\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8868\u660e\u6709\u9488\u5bf9\u6027\u548c\u900f\u660e\u7684\u8c03\u5ea6\u7b56\u7565\u80fd\u663e\u8457\u63d0\u9ad8\u5f02\u6784AI\u96c6\u7fa4\u7684GPU\u6548\u7387\uff0c\u4e3a\u672a\u6765\u751f\u4ea7\u8c03\u5ea6\u6846\u67b6\u63d0\u4f9b\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2512.10987", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10987", "abs": "https://arxiv.org/abs/2512.10987", "authors": ["Sumit Chongder"], "title": "Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems", "comment": "13 pages, 14 figures, 2 tables", "summary": "In recent years, the landscape of federated learning has witnessed significant advancements, particularly in decentralized methodologies. This research paper presents a comprehensive comparison of Centralized Hierarchical Federated Learning (HFL) with Decentralized Aggregated Federated Learning (AFL) and Decentralized Continual Federated Learning (CFL) architectures. While HFL, in its centralized approach, faces challenges such as communication bottlenecks and privacy concerns due to centralized data aggregation, AFL and CFL provide promising alternatives by distributing computation and aggregation processes across devices. Through evaluation of Fashion MNIST and MNIST datasets, this study demonstrates the advantages of decentralized methodologies, showcasing how AFL and CFL outperform HFL in precision, recall, F1 score, and balanced accuracy. The analysis highlights the importance of decentralized aggregation mechanisms in AFL and CFL, which effectively enables collaborative model training across distributed devices. This comparative study contributes valuable insights into the evolving landscape of federated learning, guiding researchers and practitioners towards decentralized methodologies for enhanced performance in collaborative model training scenarios.", "AI": {"tldr": "\u6bd4\u8f83\u96c6\u4e2d\u5f0f\u5206\u5c42\u8054\u90a6\u5b66\u4e60\uff08HFL\uff09\u4e0e\u53bb\u4e2d\u5fc3\u5316\u805a\u5408\u8054\u90a6\u5b66\u4e60\uff08AFL\uff09\u548c\u53bb\u4e2d\u5fc3\u5316\u6301\u7eed\u8054\u90a6\u5b66\u4e60\uff08CFL\uff09\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u96c6\u4e2d\u5f0f\u5206\u5c42\u8054\u90a6\u5b66\u4e60\u5b58\u5728\u901a\u4fe1\u74f6\u9888\u548c\u9690\u79c1\u95ee\u9898\uff0c\u9700\u8981\u63a2\u7d22\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\uff08AFL\u548cCFL\uff09\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u6539\u5584\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "method": "\u4f7f\u7528Fashion MNIST\u548cMNIST\u6570\u636e\u96c6\uff0c\u5bf9HFL\u3001AFL\u548cCFL\u4e09\u79cd\u67b6\u6784\u8fdb\u884c\u7efc\u5408\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548c\u5e73\u8861\u51c6\u786e\u7387\u7b49\u6307\u6807\u4e0a\u7684\u8868\u73b0\u3002", "result": "AFL\u548cCFL\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548c\u5e73\u8861\u51c6\u786e\u7387\u7b49\u591a\u4e2a\u6307\u6807\u4e0a\u5747\u4f18\u4e8eHFL\uff0c\u8bc1\u660e\u4e86\u53bb\u4e2d\u5fc3\u5316\u805a\u5408\u673a\u5236\u5728\u5206\u5e03\u5f0f\u8bbe\u5907\u534f\u540c\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff08AFL\u548cCFL\uff09\u76f8\u6bd4\u96c6\u4e2d\u5f0fHFL\u5177\u6709\u6027\u80fd\u4f18\u52bf\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u9009\u62e9\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5206\u5e03\u5f0f\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2512.10990", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10990", "abs": "https://arxiv.org/abs/2512.10990", "authors": ["Jianli Jin", "Ziyang Lin", "Qianli Dong", "Yi Chen", "Jayanth Srinivasa", "Myungjin Lee", "Zhaowei Tan", "Fan Lai"], "title": "Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI", "comment": null, "summary": "With the proliferation of edge AI applications, satisfying user quality of experience (QoE) requirements, such as model inference latency, has become a first class objective, as these models operate in resource constrained settings and directly interact with users. Yet, modern AI models routinely exceed the resource capacity of individual devices, necessitating distributed execution across heterogeneous devices over variable and contention prone networks. Existing planners for hybrid (e.g., data and pipeline) parallelism largely optimize for throughput or device utilization, overlooking QoE, leading to severe resource inefficiency (e.g., unnecessary energy drain) or QoE violations under runtime dynamics.\n  We present Dora, a framework for QoE aware hybrid parallelism in distributed edge AI training and inference. Dora jointly optimizes heterogeneous computation, contention prone networks, and multi dimensional QoE objectives via three key mechanisms: (i) a heterogeneity aware model partitioner that determines and assigns model partitions across devices, forming a compact set of QoE compliant plans; (ii) a contention aware network scheduler that further refines these candidate plans by maximizing compute communication overlap; and (iii) a runtime adapter that adaptively composes multiple plans to maximize global efficiency while respecting overall QoEs. Across representative edge deployments, including smart homes, traffic analytics, and small edge clusters, Dora achieves 1.1--6.3 times faster execution and, alternatively, reduces energy consumption by 21--82 percent, all while maintaining QoE under runtime dynamics.", "AI": {"tldr": "Dora\u662f\u4e00\u4e2a\u9762\u5411\u8fb9\u7f18AI\u8bad\u7ec3\u548c\u63a8\u7406\u7684QoE\u611f\u77e5\u6df7\u5408\u5e76\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6784\u611f\u77e5\u6a21\u578b\u5206\u533a\u3001\u7ade\u4e89\u611f\u77e5\u7f51\u7edc\u8c03\u5ea6\u548c\u8fd0\u884c\u65f6\u9002\u914d\u5668\uff0c\u5728\u6ee1\u8db3QoE\u8981\u6c42\u7684\u540c\u65f6\u63d0\u5347\u6267\u884c\u6548\u7387\u5e76\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u968f\u7740\u8fb9\u7f18AI\u5e94\u7528\u7684\u666e\u53ca\uff0c\u6ee1\u8db3\u7528\u6237QoE\u8981\u6c42\uff08\u5982\u6a21\u578b\u63a8\u7406\u5ef6\u8fdf\uff09\u6210\u4e3a\u9996\u8981\u76ee\u6807\u3002\u7136\u800c\uff0c\u73b0\u4ee3AI\u6a21\u578b\u901a\u5e38\u8d85\u51fa\u5355\u4e2a\u8bbe\u5907\u7684\u8d44\u6e90\u5bb9\u91cf\uff0c\u9700\u8981\u5728\u5f02\u6784\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u6267\u884c\uff0c\u800c\u73b0\u6709\u89c4\u5212\u5668\u4e3b\u8981\u4f18\u5316\u541e\u5410\u91cf\u6216\u8bbe\u5907\u5229\u7528\u7387\uff0c\u5ffd\u7565\u4e86QoE\uff0c\u5bfc\u81f4\u8d44\u6e90\u6548\u7387\u4f4e\u4e0b\u6216QoE\u8fdd\u89c4\u3002", "method": "Dora\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u673a\u5236\u5b9e\u73b0QoE\u611f\u77e5\u7684\u6df7\u5408\u5e76\u884c\uff1a1\uff09\u5f02\u6784\u611f\u77e5\u6a21\u578b\u5206\u533a\u5668\uff0c\u786e\u5b9a\u5e76\u5206\u914d\u6a21\u578b\u5206\u533a\uff0c\u5f62\u6210\u4e00\u7ec4\u7d27\u51d1\u7684QoE\u5408\u89c4\u8ba1\u5212\uff1b2\uff09\u7ade\u4e89\u611f\u77e5\u7f51\u7edc\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u6700\u5927\u5316\u8ba1\u7b97\u901a\u4fe1\u91cd\u53e0\u6765\u4f18\u5316\u5019\u9009\u8ba1\u5212\uff1b3\uff09\u8fd0\u884c\u65f6\u9002\u914d\u5668\uff0c\u81ea\u9002\u5e94\u7ec4\u5408\u591a\u4e2a\u8ba1\u5212\u4ee5\u6700\u5927\u5316\u5168\u5c40\u6548\u7387\u540c\u65f6\u5c0a\u91cd\u6574\u4f53QoE\u3002", "result": "\u5728\u4ee3\u8868\u6027\u8fb9\u7f18\u90e8\u7f72\u573a\u666f\uff08\u667a\u80fd\u5bb6\u5c45\u3001\u4ea4\u901a\u5206\u6790\u3001\u5c0f\u578b\u8fb9\u7f18\u96c6\u7fa4\uff09\u4e2d\uff0cDora\u5b9e\u73b0\u4e861.1-6.3\u500d\u7684\u6267\u884c\u52a0\u901f\uff0c\u540c\u65f6\u5c06\u80fd\u8017\u964d\u4f4e21-82%\uff0c\u5e76\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u53d8\u5316\u4e0b\u4fdd\u6301QoE\u3002", "conclusion": "Dora\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8fb9\u7f18AI\u5206\u5e03\u5f0f\u6267\u884c\u4e2d\u7684QoE\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5f02\u6784\u8ba1\u7b97\u3001\u7ade\u4e89\u7f51\u7edc\u548c\u591a\u7ef4QoE\u76ee\u6807\uff0c\u5728\u4fdd\u8bc1\u7528\u6237\u4f53\u9a8c\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2512.11306", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.11306", "abs": "https://arxiv.org/abs/2512.11306", "authors": ["Tianyuan Wu", "Lunxi Cao", "Yining Wei", "Wei Gao", "Yuheng Zhao", "Dakai An", "Shaopan Xiong", "Zhiqiang Lv", "Ju Huang", "Siran Yang", "Yinghao Yu", "Jiamang Wang", "Lin Qu", "Wei Wang"], "title": "RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training", "comment": "17 pages, 15 figures", "summary": "Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable \"warm-star\" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.", "AI": {"tldr": "RollMux\u662f\u4e00\u4e2a\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2drollout-training\u89e3\u8026\u67b6\u6784\u7684\u96c6\u7fa4\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u96c6\u7fa4\u7f16\u6392\u6d88\u9664\u540c\u6b65\u4f9d\u8d56\u5e26\u6765\u7684\u7a7a\u95f2\u65f6\u95f4\uff0c\u5c06\u6210\u672c\u6548\u7387\u63d0\u53471.84\u500d\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\uff0crollout\u548ctraining\u89e3\u8026\u5230\u4e13\u7528\u96c6\u7fa4\u80fd\u6700\u5927\u5316\u786c\u4ef6\u6548\u7387\uff0c\u4f46on-policy\u7b97\u6cd5\u7684\u4e25\u683c\u540c\u6b65\u8981\u6c42\u5bfc\u81f4\u4e25\u91cd\u7684\u4f9d\u8d56\u6c14\u6ce1\uff0c\u4f7f\u5f97\u4e00\u4e2a\u96c6\u7fa4\u8fd0\u884c\u65f6\u53e6\u4e00\u4e2a\u96c6\u7fa4\u5fc5\u987b\u7a7a\u95f2\u7b49\u5f85\u3002", "method": "\u63d0\u51faRollMux\u6846\u67b6\uff0c\u57fa\u4e8e\"\u4e00\u4e2a\u4f5c\u4e1a\u7684\u7ed3\u6784\u6027\u7a7a\u95f2\u53ef\u4ee5\u88ab\u53e6\u4e00\u4e2a\u4f5c\u4e1a\u7684\u6d3b\u52a8\u9636\u6bb5\u5229\u7528\"\u7684\u6d1e\u5bdf\u3002\u5f15\u5165co-execution group\u62bd\u8c61\u5c06\u96c6\u7fa4\u5212\u5206\u4e3a\u9694\u79bb\u7684\u5c40\u90e8\u6027\u57df\uff0c\u91c7\u7528\u4e24\u5c42\u8c03\u5ea6\u67b6\u6784\uff1a\u7ec4\u95f4\u8c03\u5ea6\u5668\u4f7f\u7528\u4fdd\u5b88\u968f\u673a\u89c4\u5212\u4f18\u5316\u4f5c\u4e1a\u653e\u7f6e\uff0c\u7ec4\u5185\u8c03\u5ea6\u5668\u7f16\u6392\u53ef\u8bc1\u660e\u6700\u4f18\u7684\u8f6e\u8be2\u8c03\u5ea6\u3002", "result": "\u5728\u5305\u542b328\u4e2aH20\u548c328\u4e2aH800 GPU\u7684\u751f\u4ea7\u7ea7\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0cRollMux\u76f8\u6bd4\u6807\u51c6\u89e3\u8026\u67b6\u6784\u5c06\u6210\u672c\u6548\u7387\u63d0\u53471.84\u500d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5171\u7f6e\u57fa\u7ebf\u63d0\u53471.38\u500d\uff0c\u540c\u65f6\u5b9e\u73b0100%\u7684\u670d\u52a1\u6c34\u5e73\u76ee\u6807\u8fbe\u6210\u7387\u3002", "conclusion": "RollMux\u901a\u8fc7\u8de8\u96c6\u7fa4\u7f16\u6392\u6709\u6548\u56de\u6536\u4e86rollout-training\u89e3\u8026\u67b6\u6784\u4e2d\u7684\u4f9d\u8d56\u6c14\u6ce1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u786c\u4ef6\u5229\u7528\u7387\u548c\u6210\u672c\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u670d\u52a1\u8d28\u91cf\u3002"}}
{"id": "2512.11512", "categories": ["cs.DC", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.11512", "abs": "https://arxiv.org/abs/2512.11512", "authors": ["Patrick D. Manya", "Eugene M. Mbuyi", "Gothy T. Ngoie", "Jordan F. Masakuna"], "title": "Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging", "comment": "9 pages, 7 figures and 3 tables. Published in a local annal at the University of Kinshasa, although the annal is not indexed", "summary": "Identifying central nodes using closeness centrality is a critical task in analyzing large-scale complex networks, yet its decentralized computation remains challenging due to high communication overhead. Existing distributed approximation techniques, such as pruning, often fail to fully mitigate the cost of exchanging numerous data packets in large network settings. In this paper, we introduce a novel enhancement to the distributed pruning method specifically designed to overcome this communication bottleneck. Our core contribution is a technique that leverages multi-packet messaging, allowing nodes to batch and transmit larger, consolidated data blocks. This approach significantly reduces the number of exchanged messages and minimizes data loss without compromising the accuracy of the centrality estimates. We demonstrate that our multi-packet approach substantially outperforms the original pruning technique in both message efficiency (fewer overall messages) and computation time, preserving the core approximation properties of the baseline method. While we observe a manageable trade-off in increased per-node memory usage and local overhead, our findings show that this is outweighed by the gains in communication efficiency, particularly for very large networks and complex packet structures. Our work offers a more scalable and efficient solution for decentralized closeness centrality computation, promising a significant step forward for large-scale network analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u5305\u6d88\u606f\u7684\u5206\u5e03\u5f0f\u4fee\u526a\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e2d\u5fc3\u6027\u8ba1\u7b97\u4e2d\u7684\u901a\u4fe1\u5f00\u9500", "motivation": "\u5927\u89c4\u6a21\u590d\u6742\u7f51\u7edc\u4e2d\u57fa\u4e8e\u63a5\u8fd1\u4e2d\u5fc3\u6027\u7684\u4e2d\u5fc3\u8282\u70b9\u8bc6\u522b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5206\u5e03\u5f0f\u8fd1\u4f3c\u6280\u672f\uff08\u5982\u4fee\u526a\uff09\u901a\u4fe1\u5f00\u9500\u9ad8\uff0c\u96be\u4ee5\u5e94\u5bf9\u5927\u578b\u7f51\u7edc\u8bbe\u7f6e", "method": "\u63d0\u51fa\u589e\u5f3a\u7684\u5206\u5e03\u5f0f\u4fee\u526a\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u5305\u6d88\u606f\u6280\u672f\uff0c\u5141\u8bb8\u8282\u70b9\u6279\u91cf\u4f20\u8f93\u66f4\u5927\u7684\u6570\u636e\u5757\uff0c\u51cf\u5c11\u6d88\u606f\u4ea4\u6362\u6570\u91cf\u5e76\u6700\u5c0f\u5316\u6570\u636e\u4e22\u5931", "result": "\u591a\u5305\u65b9\u6cd5\u5728\u6d88\u606f\u6548\u7387\uff08\u66f4\u5c11\u603b\u6d88\u606f\u6570\uff09\u548c\u8ba1\u7b97\u65f6\u95f4\u4e0a\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u4fee\u526a\u6280\u672f\uff0c\u540c\u65f6\u4fdd\u6301\u57fa\u7ebf\u65b9\u6cd5\u7684\u8fd1\u4f3c\u7279\u6027", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u8282\u70b9\u5185\u5b58\u4f7f\u7528\u548c\u672c\u5730\u5f00\u9500\u7684\u53ef\u7ba1\u7406\u6743\u8861\uff0c\u4f46\u901a\u4fe1\u6548\u7387\u7684\u63d0\u5347\u4f18\u52bf\u660e\u663e\uff0c\u4e3a\u5927\u89c4\u6a21\u7f51\u7edc\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.11532", "categories": ["cs.DC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11532", "abs": "https://arxiv.org/abs/2512.11532", "authors": ["Chong Tang", "Hao Dai", "Jagmohan Chauhan"], "title": "Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems", "comment": null, "summary": "The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.", "AI": {"tldr": "Parallax\u662f\u4e00\u4e2a\u79fb\u52a8\u7aefDNN\u63a8\u7406\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u56fe\u5206\u533a\u3001\u5206\u652f\u611f\u77e5\u5185\u5b58\u7ba1\u7406\u548c\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u8fbe46%\u7684\u5ef6\u8fdf\u964d\u4f4e\u548c30%\u7684\u80fd\u8017\u8282\u7701\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u65f6DNN\u5e94\u7528\u9700\u6c42\u589e\u957f\uff0c\u4f46\u73b0\u6709\u6846\u67b6\u5728\u5904\u7406\u52a8\u6001\u63a7\u5236\u6d41\u64cd\u4f5c\u7b26\u548c\u4e0d\u652f\u6301\u7684\u6838\u51fd\u6570\u65f6\uff0c\u56de\u9000\u5230CPU\u6267\u884c\u7684\u65b9\u5f0f\u6548\u7387\u4f4e\u4e0b\uff0c\u5bfc\u81f4CPU\u6838\u5fc3\u95f2\u7f6e\u3001\u9ad8\u5ef6\u8fdf\u548c\u5185\u5b58\u5cf0\u503c\u95ee\u9898\u3002", "method": "1. \u5bf9\u8ba1\u7b97DAG\u8fdb\u884c\u5206\u533a\u4ee5\u66b4\u9732\u5e76\u884c\u6027\uff1b2. \u91c7\u7528\u5206\u652f\u611f\u77e5\u5185\u5b58\u7ba1\u7406\uff0c\u4f7f\u7528\u4e13\u7528\u5185\u5b58\u6c60\u548c\u7f13\u51b2\u533a\u91cd\u7528\u51cf\u5c11\u8fd0\u884c\u65f6\u5185\u5b58\u5360\u7528\uff1b3. \u81ea\u9002\u5e94\u8c03\u5ea6\u5668\u6839\u636e\u8bbe\u5907\u5185\u5b58\u7ea6\u675f\u6267\u884c\u5206\u652f\uff1b4. \u7ec6\u7c92\u5ea6\u5b50\u56fe\u63a7\u5236\u5b9e\u73b0\u52a8\u6001\u6a21\u578b\u7684\u5f02\u6784\u63a8\u7406\u3002", "result": "\u5728\u4e09\u79cd\u4e0d\u540c\u79fb\u52a8\u8bbe\u5907\u4e0a\u8bc4\u4f30\u4e94\u4e2a\u4ee3\u8868\u6027DNN\u6a21\u578b\uff0cParallax\u5b9e\u73b0\u4e86\u9ad8\u8fbe46%\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u5e73\u5747\u5185\u5b58\u5f00\u9500\u63a7\u5236\u572826.5%\uff0c\u80fd\u8017\u8282\u7701\u9ad8\u8fbe30%\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u6846\u67b6\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "Parallax\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u5e76\u884c\u5316\u3001\u5185\u5b58\u7ba1\u7406\u548c\u8c03\u5ea6\u7b56\u7565\uff0c\u5728\u4e0d\u9700\u6a21\u578b\u91cd\u6784\u6216\u81ea\u5b9a\u4e49\u64cd\u4f5c\u7b26\u5b9e\u73b0\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u7aefDNN\u63a8\u7406\u6027\u80fd\uff0c\u6ee1\u8db3\u4e86\u5b9e\u65f6\u79fb\u52a8\u63a8\u7406\u7684\u54cd\u5e94\u6027\u9700\u6c42\u3002"}}
{"id": "2512.11634", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.11634", "abs": "https://arxiv.org/abs/2512.11634", "authors": ["Elia Palme", "Juan Pablo Dorsch", "Ali Khosravi", "Giovanni Pizzi", "Francesco Pagnamenta", "Andrea Ceriani", "Eirini Koutsaniti", "Rafael Sarmiento", "Ivano Bonesana", "Alejandro Dabin"], "title": "FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access", "comment": "6 pages, 2 figures. Presented at Cray User Group 2025 conference at New York, USA (May 4-8, 2025)", "summary": "Introducing FirecREST v2, the next generation of our open-source RESTful API for programmatic access to HPC resources. FirecREST v2 delivers a 100x performance improvement over its predecessor. This paper explores the lessons learned from redesigning FirecREST from the ground up, with a focus on integrating enhanced security and high throughput as core requirements.\n  We provide a detailed account of our systematic performance testing methodology, highlighting common bottlenecks in proxy-based APIs with intensive I/O operations. Key design and architectural changes that enabled these performance gains are presented. Finally, we demonstrate the impact of these improvements, supported by independent peer validation, and discuss opportunities for further improvements.", "AI": {"tldr": "FirecREST v2 \u662f\u4e0b\u4e00\u4ee3\u7528\u4e8eHPC\u8d44\u6e90\u7a0b\u5e8f\u5316\u8bbf\u95ee\u7684RESTful API\uff0c\u76f8\u6bd4\u524d\u4ee3\u6027\u80fd\u63d0\u5347100\u500d\uff0c\u91cd\u70b9\u6539\u8fdb\u4e86\u5b89\u5168\u6027\u548c\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u91cd\u65b0\u8bbe\u8ba1FirecEST\u4ee5\u89e3\u51b3\u4ee3\u7406\u5f0fAPI\u5728\u5bc6\u96c6I/O\u64cd\u4f5c\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u540c\u65f6\u5c06\u589e\u5f3a\u5b89\u5168\u6027\u548c\u9ad8\u541e\u5410\u91cf\u4f5c\u4e3a\u6838\u5fc3\u9700\u6c42\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u5316\u7684\u6027\u80fd\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u8bc6\u522b\u5e38\u89c1\u74f6\u9888\uff0c\u5e76\u8fdb\u884c\u5168\u9762\u7684\u67b6\u6784\u91cd\u65b0\u8bbe\u8ba1\uff0c\u5305\u62ec\u5173\u952e\u7684\u8bbe\u8ba1\u548c\u67b6\u6784\u53d8\u66f4\u3002", "result": "\u5b9e\u73b0\u4e86100\u500d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u901a\u8fc7\u72ec\u7acb\u540c\u884c\u9a8c\u8bc1\u8bc1\u660e\u4e86\u6539\u8fdb\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f00\u6e90\u7279\u6027\u3002", "conclusion": "FirecREST v2\u7684\u6210\u529f\u91cd\u8bbe\u8ba1\u5c55\u793a\u4e86\u89e3\u51b3HPC API\u6027\u80fd\u74f6\u9888\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u8fdb\u4e00\u6b65\u6539\u8fdb\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002"}}
{"id": "2512.11643", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.11643", "abs": "https://arxiv.org/abs/2512.11643", "authors": ["Manideep Reddy Chinthareddy"], "title": "Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity", "comment": "12 pages, 3 tables, 1 figure", "summary": "Snowflake-style distributed ID generators are the industry standard for producing k-ordered, unique identifiers at scale. However, the traditional requirement for manually assigned or centrally coordinated worker IDs introduces significant friction in modern container-orchestrated environments (e.g., Kubernetes), where workloads are ephemeral and autoscaled. In such systems, maintaining stable worker identities requires complex stateful sets or external coordination services (e.g., ZooKeeper), negating the operational benefits of stateless microservices.\n  This paper presents a cloud-agnostic, container-native ID generation protocol that eliminates the dependency on explicit worker IDs. By deriving node uniqueness deterministically from ephemeral network properties - specifically the container's private IPv4 address - the proposed method removes the need for centralized coordination. We introduce a modified bit-allocation scheme (1-41-16-6) that accommodates 16 bits of network-derived entropy while preserving strict monotonicity. We validate the approach across AWS, GCP, and Azure environments. Evaluation results demonstrate that while the design has a theoretical single-node ceiling of approximately 64,000 TPS, in practical microservice deployments the network I/O dominates latency, resulting in end-to-end performance (approximately 31,000 TPS on a 3-node cluster) comparable to classic stateful generators while offering effectively unbounded horizontal scalability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u663e\u5f0fworker ID\u7684\u5206\u5e03\u5f0fID\u751f\u6210\u534f\u8bae\uff0c\u5229\u7528\u5bb9\u5668\u79c1\u6709IPv4\u5730\u5740\u4f5c\u4e3a\u8282\u70b9\u552f\u4e00\u6027\u6765\u6e90\uff0c\u6d88\u9664\u4e2d\u5fc3\u5316\u534f\u8c03\u9700\u6c42\uff0c\u5b9e\u73b0\u65e0\u72b6\u6001\u5fae\u670d\u52a1\u7684\u6c34\u5e73\u6269\u5c55\u3002", "motivation": "\u4f20\u7edfSnowflake\u98ce\u683c\u5206\u5e03\u5f0fID\u751f\u6210\u5668\u9700\u8981\u624b\u52a8\u5206\u914d\u6216\u4e2d\u5fc3\u5316\u534f\u8c03worker ID\uff0c\u8fd9\u5728\u5bb9\u5668\u7f16\u6392\u73af\u5883\uff08\u5982Kubernetes\uff09\u4e2d\u5e26\u6765\u663e\u8457\u6469\u64e6\u3002\u77ed\u6682\u3001\u81ea\u52a8\u6269\u5c55\u7684\u5de5\u4f5c\u8d1f\u8f7d\u9700\u8981\u590d\u6742\u7684\u6709\u72b6\u6001\u96c6\u6216\u5916\u90e8\u534f\u8c03\u670d\u52a1\uff0c\u8fdd\u80cc\u4e86\u65e0\u72b6\u6001\u5fae\u670d\u52a1\u7684\u64cd\u4f5c\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u4e91\u65e0\u5173\u3001\u5bb9\u5668\u539f\u751f\u7684ID\u751f\u6210\u534f\u8bae\uff0c\u4ece\u77ed\u6682\u7f51\u7edc\u5c5e\u6027\uff08\u5bb9\u5668\u79c1\u6709IPv4\u5730\u5740\uff09\u786e\u5b9a\u6027\u63a8\u5bfc\u8282\u70b9\u552f\u4e00\u6027\u3002\u5f15\u5165\u4fee\u6539\u7684\u4f4d\u5206\u914d\u65b9\u6848\uff081-41-16-6\uff09\uff0c\u5bb9\u7eb316\u4f4d\u7f51\u7edc\u6d3e\u751f\u71b5\uff0c\u540c\u65f6\u4fdd\u6301\u4e25\u683c\u5355\u8c03\u6027\u3002", "result": "\u5728AWS\u3001GCP\u548cAzure\u73af\u5883\u4e2d\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u3002\u7406\u8bba\u5355\u8282\u70b9\u4e0a\u9650\u7ea664,000 TPS\uff0c\u5b9e\u9645\u5fae\u670d\u52a1\u90e8\u7f72\u4e2d\u7f51\u7edcI/O\u4e3b\u5bfc\u5ef6\u8fdf\uff0c3\u8282\u70b9\u96c6\u7fa4\u7aef\u5230\u7aef\u6027\u80fd\u7ea631,000 TPS\uff0c\u4e0e\u7ecf\u5178\u6709\u72b6\u6001\u751f\u6210\u5668\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u6709\u6548\u65e0\u9650\u6c34\u5e73\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86\u5206\u5e03\u5f0fID\u751f\u6210\u5bf9\u663e\u5f0fworker ID\u7684\u4f9d\u8d56\uff0c\u901a\u8fc7\u5229\u7528\u5bb9\u5668\u7f51\u7edc\u5c5e\u6027\u5b9e\u73b0\u65e0\u534f\u8c03\u3001\u65e0\u72b6\u6001\u64cd\u4f5c\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u5bb9\u5668\u73af\u5883\u9002\u5e94\u6027\u3002"}}
{"id": "2512.11727", "categories": ["cs.DC", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.11727", "abs": "https://arxiv.org/abs/2512.11727", "authors": ["Yuze He", "Ferdi Kossmann", "Srinivasan Seshan", "Peter Steenkiste"], "title": "ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning", "comment": null, "summary": "Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.", "AI": {"tldr": "ECCO\u662f\u4e00\u4e2a\u89c6\u9891\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u7ecf\u5386\u76f8\u4f3c\u6570\u636e\u6f02\u79fb\u7684\u6444\u50cf\u5934\u5e76\u4e3a\u5176\u8bad\u7ec3\u5171\u4eab\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8fde\u7eed\u5b66\u4e60\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u4e3a\u6bcf\u4e2a\u6444\u50cf\u5934\u5355\u72ec\u91cd\u65b0\u8bad\u7ec3\u4e13\u7528\u6a21\u578b\u7684\u65b9\u6cd5\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u8fc7\u9ad8\uff0c\u65e0\u6cd5\u6269\u5c55\u3002\u6570\u636e\u6f02\u79fb\u901a\u5e38\u5728\u76f8\u90bb\u6444\u50cf\u5934\u95f4\u5b58\u5728\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u8fd9\u4e3a\u5171\u4eab\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "ECCO\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u8f7b\u91cf\u7ea7\u5206\u7ec4\u7b97\u6cd5\u52a8\u6001\u5f62\u6210\u548c\u66f4\u65b0\u6444\u50cf\u5934\u7ec4\uff1b2) GPU\u5206\u914d\u5668\u52a8\u6001\u5206\u914dGPU\u8d44\u6e90\u4ee5\u63d0\u9ad8\u91cd\u8bad\u7ec3\u51c6\u786e\u6027\u5e76\u786e\u4fdd\u516c\u5e73\u6027\uff1b3) \u6bcf\u4e2a\u6444\u50cf\u5934\u7684\u4f20\u8f93\u63a7\u5236\u5668\u914d\u7f6e\u5e27\u91c7\u6837\u5e76\u57fa\u4e8e\u5206\u914d\u7684GPU\u8d44\u6e90\u534f\u8c03\u5e26\u5bbd\u5171\u4eab\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cECCO\u5728\u76f8\u540c\u8ba1\u7b97\u548c\u901a\u4fe1\u8d44\u6e90\u4e0b\u5c06\u91cd\u8bad\u7ec3\u51c6\u786e\u6027\u63d0\u9ad8\u4e866.7%-18.1%\uff0c\u6216\u5728\u76f8\u540c\u51c6\u786e\u6027\u4e0b\u652f\u63013.3\u500d\u66f4\u591a\u7684\u5e76\u53d1\u6444\u50cf\u5934\u3002", "conclusion": "ECCO\u901a\u8fc7\u5229\u7528\u6444\u50cf\u5934\u95f4\u6570\u636e\u6f02\u79fb\u7684\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u5b9e\u73b0\u4e86\u8d44\u6e90\u9ad8\u6548\u7684\u8fde\u7eed\u5b66\u4e60\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u89c6\u9891\u5206\u6790\u7cfb\u7edf\u7684\u6269\u5c55\u6210\u672c\u3002"}}
{"id": "2512.11775", "categories": ["cs.DC", "cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.11775", "abs": "https://arxiv.org/abs/2512.11775", "authors": ["Ayush Nainwal", "Atharva Kamble", "Nitin Awathare"], "title": "Hypergraph based Multi-Party Payment Channel", "comment": null, "summary": "Public blockchains inherently offer low throughput and high latency, motivating off-chain scalability solutions such as Payment Channel Networks (PCNs). However, existing PCNs suffer from liquidity fragmentation-funds locked in one channel cannot be reused elsewhere-and channel depletion, both of which limit routing efficiency and reduce transaction success rates. Multi-party channel (MPC) constructions mitigate these issues, but they typically rely on leaders or coordinators, creating single points of failure and providing only limited flexibility for inter-channel payments.\n  We introduce Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces bilateral channels with collectively funded hyperedges. These hyperedges enable fully concurrent, leaderless intra- and inter-hyperedge payments through verifiable, proposer-ordered DAG updates, offering significantly greater flexibility and concurrency than prior designs.\n  Our implementation on a 150-node network demonstrates a transaction success rate of approximately 94% without HTLC expiry or routing failures, highlighting the robustness of H-MPCs.", "AI": {"tldr": "H-MPCs\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u591a\u65b9\u652f\u4ed8\u901a\u9053\uff0c\u901a\u8fc7\u96c6\u4f53\u8d44\u91d1\u8d85\u8fb9\u66ff\u4ee3\u4f20\u7edf\u53cc\u8fb9\u901a\u9053\uff0c\u5b9e\u73b0\u65e0\u9886\u5bfc\u8005\u7684\u9ad8\u5e76\u53d1\u94fe\u4e0b\u652f\u4ed8\uff0c\u89e3\u51b3\u6d41\u52a8\u6027\u788e\u7247\u5316\u548c\u901a\u9053\u8017\u5c3d\u95ee\u9898\u3002", "motivation": "\u516c\u5171\u533a\u5757\u94fe\u541e\u5410\u91cf\u4f4e\u3001\u5ef6\u8fdf\u9ad8\uff0c\u73b0\u6709\u652f\u4ed8\u901a\u9053\u7f51\u7edc\u5b58\u5728\u6d41\u52a8\u6027\u788e\u7247\u5316\uff08\u8d44\u91d1\u9501\u5b9a\u5728\u5355\u4e00\u901a\u9053\u65e0\u6cd5\u590d\u7528\uff09\u548c\u901a\u9053\u8017\u5c3d\u95ee\u9898\uff0c\u9650\u5236\u4e86\u8def\u7531\u6548\u7387\u548c\u4ea4\u6613\u6210\u529f\u7387\u3002\u73b0\u6709\u7684\u591a\u65b9\u901a\u9053\u65b9\u6848\u4f9d\u8d56\u9886\u5bfc\u8005\u6216\u534f\u8c03\u8005\uff0c\u5b58\u5728\u5355\u70b9\u6545\u969c\u4e14\u8de8\u901a\u9053\u652f\u4ed8\u7075\u6d3b\u6027\u6709\u9650\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8d85\u56fe\u7684\u591a\u65b9\u652f\u4ed8\u901a\u9053\uff08H-MPCs\uff09\uff0c\u7528\u96c6\u4f53\u8d44\u91d1\u8d85\u8fb9\u66ff\u4ee3\u4f20\u7edf\u53cc\u8fb9\u901a\u9053\u3002\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u63d0\u8bae\u8005\u6709\u5e8fDAG\u66f4\u65b0\uff0c\u5b9e\u73b0\u5b8c\u5168\u5e76\u53d1\u7684\u3001\u65e0\u9886\u5bfc\u8005\u7684\u8d85\u8fb9\u5185\u548c\u8d85\u8fb9\u95f4\u652f\u4ed8\u3002", "result": "\u5728150\u4e2a\u8282\u70b9\u7684\u7f51\u7edc\u4e0a\u5b9e\u73b0\uff0c\u4ea4\u6613\u6210\u529f\u7387\u7ea694%\uff0c\u6ca1\u6709HTLC\u8fc7\u671f\u6216\u8def\u7531\u5931\u8d25\uff0c\u8bc1\u660e\u4e86H-MPCs\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "H-MPCs\u63d0\u4f9b\u4e86\u6bd4\u73b0\u6709\u8bbe\u8ba1\u663e\u8457\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u5e76\u53d1\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u652f\u4ed8\u901a\u9053\u7f51\u7edc\u7684\u6d41\u52a8\u6027\u788e\u7247\u5316\u548c\u901a\u9053\u8017\u5c3d\u95ee\u9898\u3002"}}
