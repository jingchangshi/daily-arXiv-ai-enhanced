<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Invertible Syntax without the Tuples (Functional Pearl)](https://arxiv.org/abs/2508.09856)
*Mathieu Boespflug,Arnaud Spiwack*

Main category: cs.PL

TL;DR: 本文重新探讨了Danvy的延续传递风格（CPS）在解析和打印结构化数据中的应用，提出了三种解决方案，展示了CPS在表达能力上的优势。


<details>
  <summary>Details</summary>
Motivation: 重新评估Danvy的CPS方法在现代组合库（如应用式、单子或箭头）中的适用性，尤其是在处理可逆语法描述时的潜力。

Method: 提出了三种基于CPS的解决方案，用于解析和打印结构化数据，避免了依赖类型和单子聚合的复杂性。

Result: 展示了CPS在处理列表和树等归纳结构时的表达能力，验证了其在现代组合库中的有效性。

Conclusion: CPS方法在解析和打印结构化数据中仍具有重要价值，为现代组合库提供了一种简洁且强大的替代方案。

Abstract: In the seminal paper Functional unparsing, Olivier Danvy used continuation
passing to reanalyse printf-like format strings as combinators. In the
intervening decades, the conversation shifted towards a concurrent line of work
-- applicative, monadic or arrow-based combinator libraries -- in an effort to
find combinators for invertible syntax descriptions that simultaneously
determine a parser as well as a printer, and with more expressive power, able
to handle inductive structures such as lists and trees. Along the way,
continuation passing got lost. This paper argues that Danvy's insight remains
as relevant to the general setting as it was to the restricted setting of his
original paper. Like him, we present three solutions that exploit
continuation-passing style as an alternative to both dependent types and
monoidal aggregation via nested pairs, in our case to parse and print
structured data with increasing expressive power.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Verify Distributed Deep Learning Model Implementation Refinement with Iterative Relation Inference](https://arxiv.org/abs/2508.09505)
*Zhanghan Wang,Ding Ding,Hang Zhu,Haibin Lin,Aurojit Panda*

Main category: cs.DC

TL;DR: 提出了一种静态检测分布式机器学习模型与顺序模型输出差异的方法，通过检查模型细化来识别潜在错误。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习模型在实现过程中可能引入错误，导致输出与顺序模型不一致，需要一种静态分析方法来识别这些问题。

Method: 使用GraphGuard工具，通过迭代重写技术验证模型细化，即能否从分布式模型输出重建顺序模型输出。

Result: 方法适用于大型模型（如GPT和Llama-3），并能提供可操作的错误定位信息。

Conclusion: 该方法能有效检测分布式模型中的潜在错误，并帮助开发者定位问题。

Abstract: Distributed machine learning training and inference is common today because
today's large models require more memory and compute than can be provided by a
single GPU. Distributed models are generally produced by programmers who take a
sequential model specification and apply several distribution strategies to
distribute state and computation across GPUs. Unfortunately, bugs can be
introduced in the process, and a distributed model implementation's outputs
might differ from the sequential model's outputs. In this paper, we describe an
approach to statically identify such bugs by checking model refinement, that
is, can the sequential model's outputs be reconstructed from the distributed
model's outputs? Our approach, implemented in GraphGuard, uses iterative
rewriting to prove model refinement. Our approach can scale to today's large
models and deployments: we evaluate it using GPT and Llama-3. Further, it
provides actionable output that aids in bug localization.

</details>


### [3] [HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap](https://arxiv.org/abs/2508.09591)
*Wenxiang Lin,Xinglin Pan,Lin Zhang,Shaohuai Shi,Xuan Wang,Xiaowen Chu*

Main category: cs.DC

TL;DR: HierMoE通过拓扑感知技术（令牌去重和专家交换）加速MoE模型训练，提升通信和负载均衡。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型中因动态选择专家导致的通信和负载不均衡问题，提升分布式系统的可扩展性。

Method: 提出两种拓扑感知技术：令牌去重减少通信流量，专家交换平衡GPU负载，并建立理论模型优化策略。

Result: 在32-GPU集群上实验，HierMoE通信速度提升1.55×至3.32×，训练速度提升1.18×至1.27×。

Conclusion: HierMoE显著提升MoE模型的训练效率和可扩展性，优于现有系统。

Abstract: The sparsely activated mixture-of-experts (MoE) transformer has become a
common architecture for large language models (LLMs) due to its sparsity, which
requires fewer computational demands while easily scaling the model size. In
MoE models, each MoE layer requires to dynamically choose tokens to activate
particular experts for computation while the activated experts may not be
located in the same device or GPU as the token. However, this leads to
substantial communication and load imbalances across all GPUs, which obstructs
the scalability of distributed systems within a GPU cluster. To this end, we
introduce HierMoE to accelerate the training of MoE models by two
topology-aware techniques: 1) token deduplication to reduce the communication
traffic, and 2) expert swap to balance the workloads among all GPUs. To enable
the above two proposed approaches to be more general, we build theoretical
models aimed at achieving the best token duplication and expert swap strategy
under different model configurations and hardware environments. We implement
our prototype HierMoE system atop Megatron-LM and conduct experiments on a
32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results
show that our HierMoE achieves $1.55\times$ to $3.32\times$ faster
communication and delivers $1.18\times$ to $1.27\times$ faster end-to-end
training compared to state-of-the-art MoE training systems, Tutel-2DH,
SmartMoE, and Megatron-LM.

</details>


### [4] [Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for Kubernetes](https://arxiv.org/abs/2508.09663)
*Philipp A. Friese,Ahmed Eleliemy,Utz-Uwe Haus,Martin Schulz*

Main category: cs.DC

TL;DR: 本文提出了一种针对HPE Slingshot网络的扩展设计，以支持多租户的HPC-云计算融合部署，通过Kubernetes实现安全且低开销的容器级访问。


<details>
  <summary>Details</summary>
Motivation: HPC-云计算融合需要满足高性能和隔离性需求，但现有Slingshot网络栈仅支持单租户模式，无法适应多租户环境。

Method: 设计并实现了基于Kubernetes的Slingshot网络栈扩展，提供容器级的多租户安全访问。

Result: 实现了对Slingshot RDMA网络功能的安全、多租户访问，且开销极低。

Conclusion: 该扩展成功解决了HPC-云计算融合中网络隔离与性能的平衡问题。

Abstract: Converged HPC-Cloud computing is an emerging computing paradigm that aims to
support increasingly complex and multi-tenant scientific workflows. These
systems require reconciliation of the isolation requirements of native cloud
workloads and the performance demands of HPC applications. In this context,
networking hardware is a critical boundary component: it is the conduit for
high-throughput, low-latency communication and enables isolation across
tenants. HPE Slingshot is a high-speed network interconnect that provides up to
200 Gbps of throughput per port and targets high-performance computing (HPC)
systems. The Slingshot host software, including hardware drivers and network
middleware libraries, is designed to meet HPC deployments, which predominantly
use single-tenant access modes. Hence, the Slingshot stack is not suited for
secure use in multi-tenant deployments, such as converged HPC-Cloud
deployments. In this paper, we design and implement an extension to the
Slingshot stack targeting converged deployments on the basis of Kubernetes. Our
integration provides secure, container-granular, and multi-tenant access to
Slingshot RDMA networking capabilities at minimal overhead.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [5] [Re-thinking Memory-Bound Limitations in CGRAs](https://arxiv.org/abs/2508.09570)
*Xiangfeng Liu,Zhe Jiang,Anzhen Zhu,Xiaomeng Han,Mingsong Lyu,Qingxu Deng,Nan Guan*

Main category: cs.AR

TL;DR: 论文提出了一种针对粗粒度可重构阵列（CGRA）的改进内存子系统，通过运行前执行机制和缓存重配置技术，有效处理不规则内存访问，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有CGRA研究通常假设内核可以从Scratchpad Memory（SPM）访问所有数据，但复杂工作负载（如图分析、不规则数据库操作）的不规则内存访问模式导致CGRA利用率极低，甚至低于1.5%。

Method: 通过重新设计内存子系统并优化内存模型，结合运行前执行机制和缓存重配置技术，解决不规则内存访问问题。

Result: 改进后的系统性能与原始SPM-only系统相当，仅需1.27%的存储空间；运行前执行机制平均提速3.04倍（最高6.91倍），缓存重配置技术额外提升6.02%。

Conclusion: 提出的方法显著提升了CGRA在不规则内存访问模式下的性能，解决了现有研究的局限性。

Abstract: Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators
commonly employed to boost performance in workloads with iterative structures.
Existing research typically focuses on compiler or architecture optimizations
aimed at improving CGRA performance, energy efficiency, flexibility, and area
utilization, under the idealistic assumption that kernels can access all data
from Scratchpad Memory (SPM). However, certain complex workloads-particularly
in fields like graph analytics, irregular database operations, and specialized
forms of high-performance computing (e.g., unstructured mesh
simulations)-exhibit irregular memory access patterns that hinder CGRA
utilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To
address this challenge, we conduct a thorough analysis of the underlying causes
of performance degradation, then propose a redesigned memory subsystem and
refine the memory model. With both microarchitectural and theoretical
optimization, our solution can effectively manage irregular memory accesses
through CGRA-specific runahead execution mechanism and cache reconfiguration
techniques. Our results demonstrate that we can achieve performance comparable
to the original SPM-only system while requiring only 1.27% of the storage size.
The runahead execution mechanism achieves an average 3.04x speedup (up to
6.91x), with cache reconfiguration technique providing an additional 6.02%
improvement, significantly enhancing CGRA performance for irregular memory
access patterns.

</details>
