{"id": "2510.06296", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06296", "abs": "https://arxiv.org/abs/2510.06296", "authors": ["Lingfei Zeng", "Fengdi Che", "Xuhan Huang", "Fei Ye", "Xu Xu", "Binhang Yuan", "Jie Fu"], "title": "VeriEquivBench: An Equivalence Score for Ground-Truth-Free Evaluation of Formally Verifiable Code", "comment": null, "summary": "Formal verification is the next frontier for ensuring the correctness of code\ngenerated by Large Language Models (LLMs). While methods that co-generate code\nand formal specifications in formal languages, like Dafny, can, in principle,\nprove alignment with user intent, progress is bottlenecked by specification\nquality evaluation. Current benchmarks rely on matching against ground-truth\nspecifications, a manual and expertise-intensive process that has limited\nexisting datasets to a few hundred simple problems and also suffers from a\nreliability issue. To address this, we introduce VeriEquivBench, a new\nbenchmark with $2,389$ complex algorithmic problems that probe the limitations\nof current models in both code generation and formal reasoning. Our evaluation\nframework replaces ground-truth matching with a formally grounded metric, the\nequivalence score, and rigorously verifies the quality of generated\nspecifications and code. Our results show that generating formally verifiable\ncode remains a profound challenge for state-of-the-art LLMs. This underscores\nboth the difficulty of the task and the need for benchmarks like VeriEquivBench\nto drive progress toward scalable and reliable coding agents."}
{"id": "2510.06387", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.06387", "abs": "https://arxiv.org/abs/2510.06387", "authors": ["Raaghav Ravishankar", "Sandeep Kulkarni", "Sathya Peri", "Gokarna Sharma"], "title": "DiLi: A Lock-Free Asynchronously Distributable Linked List", "comment": null, "summary": "Modern databases use dynamic search structures that store a huge amount of\ndata, and often serve them using multi-threaded algorithms to support the\never-increasing throughput needs. When this throughput need exceeds the\ncapacity of the machine hosting the structure, one either needs to replace the\nunderlying hardware (an option that is typically not viable and introduces a\nlong down time) or make the data structure distributed. Static partitioning of\nthe data structure for distribution is not desirable, as it is prone to uneven\nload distribution over time, and having to change the partitioning scheme later\nwill require downtime.\n  Since a distributed data structure, inherently, relies on communication\nsupport from the network stack and operating systems, we introduce the notion\nof conditional lock-freedom that extends the notion of lock-free computation\nwith reasonable assumptions about communication between processes. We present\nDiLi, a conditional lock-free, linearizable, and distributable linked list that\ncan be asynchronously and dynamically (1) partitioned into multiple sublists\nand (2) load balanced by distributing sublists across multiple machines. DiLi\ncontains primitives for these that also maintain the lock-free property of the\nunderlying search structure that supports find, remove, and insert of a key as\nthe client operations.\n  Searching for an item in DiLi is by a novel traversal that involves a binary\nsearch on the partitioning scheme, and then a linear traversal on a limitable\nnumber of linked nodes. As a result, we are able to empirically show that DiLi\nperforms as well as the state-of-the-art lock-free concurrent search structures\nthat are based off of a linked list when executed on a single-machine. We also\nshow that the throughput of DiLi scales linearly with the number of machines\nthat host it."}
{"id": "2510.06513", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.06513", "abs": "https://arxiv.org/abs/2510.06513", "authors": ["Debendra Das Sharma", "Swadesh Choudhary", "Peter Onufryk", "Rob Pelt"], "title": "On-Package Memory with Universal Chiplet Interconnect Express (UCIe): A Low Power, High Bandwidth, Low Latency and Low Cost Approach", "comment": "10 pages", "summary": "Emerging computing applications such as Artificial Intelligence (AI) are\nfacing a memory wall with existing on-package memory solutions that are unable\nto meet the power-efficient bandwidth demands. We propose to enhance UCIe with\nmemory semantics to deliver power-efficient bandwidth and cost-effective\non-package memory solutions applicable across the entire computing continuum.\nWe propose approaches by reusing existing LPDDR6 and HBM memory through a logic\ndie that connects to the SoC using UCIe. We also propose an approach where the\nDRAM die natively supports UCIe instead of the LPDDR6 bus interface. Our\napproaches result in significantly higher bandwidth density (up to 10x), lower\nlatency (up to 3x), lower power (up to 3x), and lower cost compared to existing\nHBM4 and LPDDR on-package memory solutions."}
{"id": "2510.06396", "categories": ["cs.DC", "cs.AI", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06396", "abs": "https://arxiv.org/abs/2510.06396", "authors": ["Aymen Alsaadi", "Jonathan Ash", "Mikhail Titov", "Matteo Turilli", "Andre Merzky", "Shantenu Jha", "Sagar Khare"], "title": "Adaptive Protein Design Protocols and Middleware", "comment": "N/A", "summary": "Computational protein design is experiencing a transformation driven by\nAI/ML. However, the range of potential protein sequences and structures is\nastronomically vast, even for moderately sized proteins. Hence, achieving\nconvergence between generated and predicted structures demands substantial\ncomputational resources for sampling. The Integrated Machine-learning for\nProtein Structures at Scale (IMPRESS) offers methods and advanced computing\nsystems for coupling AI to high-performance computing tasks, enabling the\nability to evaluate the effectiveness of protein designs as they are developed,\nas well as the models and simulations used to generate data and train models.\nThis paper introduces IMPRESS and demonstrates the development and\nimplementation of an adaptive protein design protocol and its supporting\ncomputing infrastructure. This leads to increased consistency in the quality of\nprotein design and enhanced throughput of protein design due to dynamic\nresource allocation and asynchronous workload execution."}
{"id": "2510.06644", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.06644", "abs": "https://arxiv.org/abs/2510.06644", "authors": ["Leshu Li", "Jiayin Qin", "Jie Peng", "Zishen Wan", "Huaizhi Qu", "Ye Han", "Pingqing Zheng", "Hongsen Zhang", "Yu", "Cao", "Tianlong Chen", "Yang", "Zhao"], "title": "RTGS: Real-Time 3D Gaussian Splatting SLAM via Multi-Level Redundancy Reduction", "comment": "Accepted by MICRO2025", "summary": "3D Gaussian Splatting (3DGS) based Simultaneous Localization and Mapping\n(SLAM) systems can largely benefit from 3DGS's state-of-the-art rendering\nefficiency and accuracy, but have not yet been adopted in resource-constrained\nedge devices due to insufficient speed. Addressing this, we identify notable\nredundancies across the SLAM pipeline for acceleration. While conceptually\nstraightforward, practical approaches are required to minimize the overhead\nassociated with identifying and eliminating these redundancies. In response, we\npropose RTGS, an algorithm-hardware co-design framework that comprehensively\nreduces the redundancies for real-time 3DGS-SLAM on edge. To minimize the\noverhead, RTGS fully leverages the characteristics of the 3DGS-SLAM pipeline.\nOn the algorithm side, we introduce (1) an adaptive Gaussian pruning step to\nremove the redundant Gaussians by reusing gradients computed during\nbackpropagation; and (2) a dynamic downsampling technique that directly reuses\nthe keyframe identification and alpha computing steps to eliminate redundant\npixels. On the hardware side, we propose (1) a subtile-level streaming strategy\nand a pixel-level pairwise scheduling strategy that mitigates workload\nimbalance via a Workload Scheduling Unit (WSU) guided by previous iteration\ninformation; (2) a Rendering and Backpropagation (R&B) Buffer that accelerates\nthe rendering backpropagation by reusing intermediate data computed during\nrendering; and (3) a Gradient Merging Unit (GMU) to reduce intensive memory\naccesses caused by atomic operations while enabling pipelined aggregation.\nIntegrated into an edge GPU, RTGS achieves real-time performance (>= 30 FPS) on\nfour datasets and three algorithms, with up to 82.5x energy efficiency over the\nbaseline and negligible quality loss. Code is available at\nhttps://github.com/UMN-ZhaoLab/RTGS."}
{"id": "2510.06404", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.06404", "abs": "https://arxiv.org/abs/2510.06404", "authors": ["Raaghav Ravishankar", "Sandeep Kulkarni", "Nitin H Vaidya"], "title": "MuFASA -- Asynchronous Checkpoint for Weakly Consistent Fully Replicated Databases", "comment": null, "summary": "We focus on the problem of checkpointing in fully replicated weakly\nconsistent distributed databases, which we refer to as Distributed Transaction\nConsistent Snapshot (DTCS). A typical example of such a system is a main-memory\ndatabase that provides strong eventual consistency. This problem is important\nand challenging for several reasons: (1) eventual consistency often creates\nanomalies that the users do not anticipate. Hence, frequent checkpoints to\nascertain desired invariants is highly beneficial in their use, and (2)\ntraditional checkpoints lead to significant overhead and/or inconsistencies. By\nshowing that the traditional checkpoint leads to inconsistencies or excessive\noverhead, we define the notion of size-minimal checkpointing for fully\nreplicated databases. We present an algorithm for checkpointing with minimal\ncheckpointing overhead (only O(n) new messages and addition of a single counter\nfor existing messages). It also provides a significant benefit over existing\ncheckpointing algorithms for distributed systems and main-memory databases.\n  A key benefit of DTCS is that it summarizes the computation by a sequence of\nsnapshots that are strongly consistent even though the underlying computation\nis weakly consistent. In essence, when anomalies arise in an eventually\nconsistent system, DTCS enables one to concentrate solely on the snapshots\nsurrounding the time point of the anomaly."}
{"id": "2510.06767", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.06767", "abs": "https://arxiv.org/abs/2510.06767", "authors": ["Bindu G Gowda", "Yogesh Goyal", "Yash Gupta", "Madhav Rao"], "title": "Hardware-Efficient CNNs: Interleaved Approximate FP32 Multipliers for Kernel Computation", "comment": null, "summary": "Single-precision floating point (FP32) data format, defined by the IEEE 754\nstandard, is widely employed in scientific computing, signal processing, and\ndeep learning training, where precision is critical. However, FP32\nmultiplication is computationally expensive and requires complex hardware,\nespecially for precisely handling mantissa multiplication. In practical\napplications like neural network inference, perfect accuracy is not always\nnecessary, minor multiplication errors often have little impact on final\naccuracy. This enables trading precision for gains in area, power, and speed.\nThis work focuses on CNN inference using approximate FP32 multipliers, where\nthe mantissa multiplication is approximated by employing error-variant\napproximate compressors, that significantly reduce hardware cost. Furthermore,\nthis work optimizes CNN performance by employing differently approximated FP32\nmultipliers and studying their impact when interleaved within the kernels\nacross the convolutional layers. The placement and ordering of these\napproximate multipliers within each kernel are carefully optimized using the\nNon-dominated Sorting Genetic Algorithm-II, balancing the trade-off between\naccuracy and hardware efficiency."}
{"id": "2510.06675", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.06675", "abs": "https://arxiv.org/abs/2510.06675", "authors": ["Xu Bai", "Muhammed Tawfiqul Islam", "Rajkumar Buyya", "Adel N. Toosi"], "title": "REACH: Reinforcement Learning for Adaptive Microservice Rescheduling in the Cloud-Edge Continuum", "comment": "10 pages, 10 figures", "summary": "Cloud computing, despite its advantages in scalability, may not always fully\nsatisfy the low-latency demands of emerging latency-sensitive pervasive\napplications. The cloud-edge continuum addresses this by integrating the\nresponsiveness of edge resources with cloud scalability. Microservice\nArchitecture (MSA) characterized by modular, loosely coupled services, aligns\neffectively with this continuum. However, the heterogeneous and dynamic\ncomputing resource poses significant challenges to the optimal placement of\nmicroservices. We propose REACH, a novel rescheduling algorithm that\ndynamically adapts microservice placement in real time using reinforcement\nlearning to react to fluctuating resource availability, and performance\nvariations across distributed infrastructures. Extensive experiments on a\nreal-world testbed demonstrate that REACH reduces average end-to-end latency by\n7.9%, 10%, and 8% across three benchmark MSA applications, while effectively\nmitigating latency fluctuations and spikes."}
{"id": "2510.07304", "categories": ["cs.AR", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07304", "abs": "https://arxiv.org/abs/2510.07304", "authors": ["Donghwan Kim", "Xin Gu", "Jinho Baek", "Timothy Lo", "Younghoon Min", "Kwangsik Shin", "Jongryool Kim", "Jongse Park", "Kiwan Maeng"], "title": "Cocoon: A System Architecture for Differentially Private Training with Correlated Noises", "comment": null, "summary": "Machine learning (ML) models memorize and leak training data, causing serious\nprivacy issues to data owners. Training algorithms with differential privacy\n(DP), such as DP-SGD, have been gaining attention as a solution. However,\nDP-SGD adds a noise at each training iteration, which degrades the accuracy of\nthe trained model. To improve accuracy, a new family of approaches adds\ncarefully designed correlated noises, so that noises cancel out each other\nacross iterations. We performed an extensive characterization study of these\nnew mechanisms, for the first time to the best of our knowledge, and show they\nincur non-negligible overheads when the model is large or uses large embedding\ntables. Motivated by the analysis, we propose Cocoon, a hardware-software\nco-designed framework for efficient training with correlated noises. Cocoon\naccelerates models with embedding tables through pre-computing and storing\ncorrelated noises in a coalesced format (Cocoon-Emb), and supports large models\nthrough a custom near-memory processing device (Cocoon-NMP). On a real system\nwith an FPGA-based NMP device prototype, Cocoon improves the performance by\n2.33-10.82x(Cocoon-Emb) and 1.55-3.06x (Cocoon-NMP)."}
{"id": "2510.06882", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.06882", "abs": "https://arxiv.org/abs/2510.06882", "authors": ["Boris Sedlak", "Philipp Raith", "Andrea Morichetta", "VÃ­ctor Casamayor Pujol", "Schahram Dustdar"], "title": "Multi-Dimensional Autoscaling of Stream Processing Services on Edge Devices", "comment": null, "summary": "Edge devices have limited resources, which inevitably leads to situations\nwhere stream processing services cannot satisfy their needs. While existing\nautoscaling mechanisms focus entirely on resource scaling, Edge devices require\nalternative ways to sustain the Service Level Objectives (SLOs) of competing\nservices. To address these issues, we introduce a Multi-dimensional Autoscaling\nPlatform (MUDAP) that supports fine-grained vertical scaling across both\nservice- and resource-level dimensions. MUDAP supports service-specific scaling\ntailored to available parameters, e.g., scale data quality or model size for a\nparticular service. To optimize the execution across services, we present a\nscaling agent based on Regression Analysis of Structural Knowledge (RASK). The\nRASK agent efficiently explores the solution space and learns a continuous\nregression model of the processing environment for inferring optimal scaling\nactions. We compared our approach with two autoscalers, the Kubernetes VPA and\na reinforcement learning agent, for scaling up to 9 services on a single Edge\ndevice. Our results showed that RASK can infer an accurate regression model in\nmerely 20 iterations (i.e., observe 200s of processing). By increasingly adding\nelasticity dimensions, RASK sustained the highest request load with 28% less\nSLO violations, compared to baselines."}
{"id": "2510.06998", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.06998", "abs": "https://arxiv.org/abs/2510.06998", "authors": ["Martin Wilhelm", "Franz Freitag", "Max Tzschoppe", "Thilo Pionteck"], "title": "Evaluating Rapid Makespan Predictions for Heterogeneous Systems with Programmable Logic", "comment": "To be published on NorCAS 2025", "summary": "Heterogeneous computing systems, which combine general-purpose processors\nwith specialized accelerators, are increasingly important for optimizing the\nperformance of modern applications. A central challenge is to decide which\nparts of an application should be executed on which accelerator or, more\ngenerally, how to map the tasks of an application to available devices.\nPredicting the impact of a change in a task mapping on the overall makespan is\nnon-trivial. While there are very capable simulators, these generally require a\nfull implementation of the tasks in question, which is particularly\ntime-intensive for programmable logic. A promising alternative is to use a\npurely analytical function, which allows for very fast predictions, but\nabstracts significantly from reality. Bridging the gap between theory and\npractice poses a significant challenge to algorithm developers. This paper aims\nto aid in the development of rapid makespan prediction algorithms by providing\na highly flexible evaluation framework for heterogeneous systems consisting of\nCPUs, GPUs and FPGAs, which is capable of collecting real-world makespan\nresults based on abstract task graph descriptions. We analyze to what extent\nactual makespans can be predicted by existing analytical approaches.\nFurthermore, we present common challenges that arise from high-level\ncharacteristics such as data transfer overhead and device congestion in\nheterogeneous systems."}
{"id": "2510.06902", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.06902", "abs": "https://arxiv.org/abs/2510.06902", "authors": ["Ayesha Afzal", "Anna Kahler", "Georg Hager", "Gerhard Wellein"], "title": "GROMACS Unplugged: How Power Capping and Frequency Shapes Performance on GPUs", "comment": "12 pages", "summary": "Molecular dynamics simulations are essential tools in computational\nbiophysics, but their performance depend heavily on hardware choices and\nconfiguration. In this work, we presents a comprehensive performance analysis\nof four NVIDIA GPU accelerators -- A40, A100, L4, and L40 -- using six\nrepresentative GROMACS biomolecular workloads alongside two synthetic\nbenchmarks: Pi Solver (compute bound) and STREAM Triad (memory bound). We\ninvestigate how performance scales with GPU graphics clock frequency and how\nworkloads respond to power capping. The two synthetic benchmarks define the\nextremes of frequency scaling: Pi Solver shows ideal compute scalability, while\nSTREAM Triad reveals memory bandwidth limits -- framing GROMACS's performance\nin context. Our results reveal distinct frequency scaling behaviors: Smaller\nGROMACS systems exhibit strong frequency sensitivity, while larger systems\nsaturate quickly, becoming increasingly memory bound. Under power capping,\nperformance remains stable until architecture- and workload-specific thresholds\nare reached, with high-end GPUs like the A100 maintaining near-maximum\nperformance even under reduced power budgets. Our findings provide practical\nguidance for selecting GPU hardware and optimizing GROMACS performance for\nlarge-scale MD workflows under power constraints."}
{"id": "2510.06998", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.06998", "abs": "https://arxiv.org/abs/2510.06998", "authors": ["Martin Wilhelm", "Franz Freitag", "Max Tzschoppe", "Thilo Pionteck"], "title": "Evaluating Rapid Makespan Predictions for Heterogeneous Systems with Programmable Logic", "comment": "To be published on NorCAS 2025", "summary": "Heterogeneous computing systems, which combine general-purpose processors\nwith specialized accelerators, are increasingly important for optimizing the\nperformance of modern applications. A central challenge is to decide which\nparts of an application should be executed on which accelerator or, more\ngenerally, how to map the tasks of an application to available devices.\nPredicting the impact of a change in a task mapping on the overall makespan is\nnon-trivial. While there are very capable simulators, these generally require a\nfull implementation of the tasks in question, which is particularly\ntime-intensive for programmable logic. A promising alternative is to use a\npurely analytical function, which allows for very fast predictions, but\nabstracts significantly from reality. Bridging the gap between theory and\npractice poses a significant challenge to algorithm developers. This paper aims\nto aid in the development of rapid makespan prediction algorithms by providing\na highly flexible evaluation framework for heterogeneous systems consisting of\nCPUs, GPUs and FPGAs, which is capable of collecting real-world makespan\nresults based on abstract task graph descriptions. We analyze to what extent\nactual makespans can be predicted by existing analytical approaches.\nFurthermore, we present common challenges that arise from high-level\ncharacteristics such as data transfer overhead and device congestion in\nheterogeneous systems."}
{"id": "2510.06513", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.06513", "abs": "https://arxiv.org/abs/2510.06513", "authors": ["Debendra Das Sharma", "Swadesh Choudhary", "Peter Onufryk", "Rob Pelt"], "title": "On-Package Memory with Universal Chiplet Interconnect Express (UCIe): A Low Power, High Bandwidth, Low Latency and Low Cost Approach", "comment": "10 pages", "summary": "Emerging computing applications such as Artificial Intelligence (AI) are\nfacing a memory wall with existing on-package memory solutions that are unable\nto meet the power-efficient bandwidth demands. We propose to enhance UCIe with\nmemory semantics to deliver power-efficient bandwidth and cost-effective\non-package memory solutions applicable across the entire computing continuum.\nWe propose approaches by reusing existing LPDDR6 and HBM memory through a logic\ndie that connects to the SoC using UCIe. We also propose an approach where the\nDRAM die natively supports UCIe instead of the LPDDR6 bus interface. Our\napproaches result in significantly higher bandwidth density (up to 10x), lower\nlatency (up to 3x), lower power (up to 3x), and lower cost compared to existing\nHBM4 and LPDDR on-package memory solutions."}
