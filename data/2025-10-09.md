<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [VeriEquivBench: An Equivalence Score for Ground-Truth-Free Evaluation of Formally Verifiable Code](https://arxiv.org/abs/2510.06296)
*Lingfei Zeng,Fengdi Che,Xuhan Huang,Fei Ye,Xu Xu,Binhang Yuan,Jie Fu*

Main category: cs.PL

TL;DR: 介绍了VeriEquivBench基准测试，包含2389个复杂算法问题，用于评估LLM在代码生成和形式推理方面的能力，使用形式化等价评分代替人工标注。


<details>
  <summary>Details</summary>
Motivation: 解决当前基于人工标注的形式化验证基准测试规模小、可靠性差的问题，推动LLM生成可验证代码的发展。

Method: 创建包含2389个复杂算法问题的基准测试，采用形式化等价评分作为评估指标，严格验证生成的规范和代码质量。

Result: 实验表明，当前最先进的LLM在生成可形式化验证的代码方面仍面临巨大挑战。

Conclusion: VeriEquivBench基准测试对于推动可扩展和可靠的编码代理发展至关重要，突显了该任务的难度。

Abstract: Formal verification is the next frontier for ensuring the correctness of code
generated by Large Language Models (LLMs). While methods that co-generate code
and formal specifications in formal languages, like Dafny, can, in principle,
prove alignment with user intent, progress is bottlenecked by specification
quality evaluation. Current benchmarks rely on matching against ground-truth
specifications, a manual and expertise-intensive process that has limited
existing datasets to a few hundred simple problems and also suffers from a
reliability issue. To address this, we introduce VeriEquivBench, a new
benchmark with $2,389$ complex algorithmic problems that probe the limitations
of current models in both code generation and formal reasoning. Our evaluation
framework replaces ground-truth matching with a formally grounded metric, the
equivalence score, and rigorously verifies the quality of generated
specifications and code. Our results show that generating formally verifiable
code remains a profound challenge for state-of-the-art LLMs. This underscores
both the difficulty of the task and the need for benchmarks like VeriEquivBench
to drive progress toward scalable and reliable coding agents.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [DiLi: A Lock-Free Asynchronously Distributable Linked List](https://arxiv.org/abs/2510.06387)
*Raaghav Ravishankar,Sandeep Kulkarni,Sathya Peri,Gokarna Sharma*

Main category: cs.DC

TL;DR: DiLi是一个条件无锁、可线性化、可分布式部署的链表数据结构，支持动态分区和负载均衡，在多机环境下能够线性扩展吞吐量


<details>
  <summary>Details</summary>
Motivation: 现代数据库需要处理海量数据和高吞吐量需求，当单机容量不足时，要么替换硬件（不可行且停机时间长），要么采用分布式数据结构。静态分区容易导致负载不均，动态调整分区方案又需要停机时间

Method: 提出条件无锁概念，扩展无锁计算以支持进程间通信。DiLi支持异步动态分区和负载均衡，通过分区方案上的二分搜索和有限数量的链表节点线性遍历来实现搜索操作

Result: 实验表明DiLi在单机环境下性能与最先进的无锁并发搜索结构相当，在多机环境下吞吐量随机器数量线性扩展

Conclusion: DiLi提供了一种有效的分布式数据结构解决方案，能够在保持无锁特性的同时实现动态分区和负载均衡，满足现代数据库的高吞吐量需求

Abstract: Modern databases use dynamic search structures that store a huge amount of
data, and often serve them using multi-threaded algorithms to support the
ever-increasing throughput needs. When this throughput need exceeds the
capacity of the machine hosting the structure, one either needs to replace the
underlying hardware (an option that is typically not viable and introduces a
long down time) or make the data structure distributed. Static partitioning of
the data structure for distribution is not desirable, as it is prone to uneven
load distribution over time, and having to change the partitioning scheme later
will require downtime.
  Since a distributed data structure, inherently, relies on communication
support from the network stack and operating systems, we introduce the notion
of conditional lock-freedom that extends the notion of lock-free computation
with reasonable assumptions about communication between processes. We present
DiLi, a conditional lock-free, linearizable, and distributable linked list that
can be asynchronously and dynamically (1) partitioned into multiple sublists
and (2) load balanced by distributing sublists across multiple machines. DiLi
contains primitives for these that also maintain the lock-free property of the
underlying search structure that supports find, remove, and insert of a key as
the client operations.
  Searching for an item in DiLi is by a novel traversal that involves a binary
search on the partitioning scheme, and then a linear traversal on a limitable
number of linked nodes. As a result, we are able to empirically show that DiLi
performs as well as the state-of-the-art lock-free concurrent search structures
that are based off of a linked list when executed on a single-machine. We also
show that the throughput of DiLi scales linearly with the number of machines
that host it.

</details>


### [3] [Adaptive Protein Design Protocols and Middleware](https://arxiv.org/abs/2510.06396)
*Aymen Alsaadi,Jonathan Ash,Mikhail Titov,Matteo Turilli,Andre Merzky,Shantenu Jha,Sagar Khare*

Main category: cs.DC

TL;DR: IMPRESS是一个将AI与高性能计算结合的蛋白质设计系统，通过自适应协议和动态资源分配提高设计质量和效率


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列和结构空间极其庞大，传统计算方法需要大量计算资源进行采样，难以实现生成结构与预测结构之间的收敛

Method: 开发IMPRESS系统，结合AI与高性能计算，采用自适应蛋白质设计协议、动态资源分配和异步工作负载执行

Result: 提高了蛋白质设计质量的一致性，增强了蛋白质设计的吞吐量

Conclusion: IMPRESS系统通过AI与高性能计算的集成，有效解决了蛋白质设计中的计算挑战，提升了设计效率和可靠性

Abstract: Computational protein design is experiencing a transformation driven by
AI/ML. However, the range of potential protein sequences and structures is
astronomically vast, even for moderately sized proteins. Hence, achieving
convergence between generated and predicted structures demands substantial
computational resources for sampling. The Integrated Machine-learning for
Protein Structures at Scale (IMPRESS) offers methods and advanced computing
systems for coupling AI to high-performance computing tasks, enabling the
ability to evaluate the effectiveness of protein designs as they are developed,
as well as the models and simulations used to generate data and train models.
This paper introduces IMPRESS and demonstrates the development and
implementation of an adaptive protein design protocol and its supporting
computing infrastructure. This leads to increased consistency in the quality of
protein design and enhanced throughput of protein design due to dynamic
resource allocation and asynchronous workload execution.

</details>


### [4] [MuFASA -- Asynchronous Checkpoint for Weakly Consistent Fully Replicated Databases](https://arxiv.org/abs/2510.06404)
*Raaghav Ravishankar,Sandeep Kulkarni,Nitin H Vaidya*

Main category: cs.DC

TL;DR: 提出了一种用于完全复制弱一致性分布式数据库的最小化检查点算法DTCS，通过O(n)消息开销和单个计数器实现强一致性快照，解决了传统检查点的不一致性和高开销问题。


<details>
  <summary>Details</summary>
Motivation: 弱一致性分布式数据库中的最终一致性会导致用户未预期的异常，传统检查点方法存在显著开销或不一致问题，需要一种高效且一致的检查点机制来确保期望的不变性。

Method: 定义了完全复制数据库的大小最小化检查点概念，提出DTCS算法，仅需O(n)新消息和现有消息添加单个计数器，生成强一致性快照序列。

Result: DTCS算法实现了最小化检查点开销，相比现有分布式系统和内存数据库检查点算法有显著优势，能够在弱一致性计算中提供强一致性快照。

Conclusion: DTCS通过最小化开销的检查点机制，在最终一致性系统中提供强一致性快照，使异常分析能够集中在异常时间点周围的快照上，有效解决了弱一致性数据库的检查点挑战。

Abstract: We focus on the problem of checkpointing in fully replicated weakly
consistent distributed databases, which we refer to as Distributed Transaction
Consistent Snapshot (DTCS). A typical example of such a system is a main-memory
database that provides strong eventual consistency. This problem is important
and challenging for several reasons: (1) eventual consistency often creates
anomalies that the users do not anticipate. Hence, frequent checkpoints to
ascertain desired invariants is highly beneficial in their use, and (2)
traditional checkpoints lead to significant overhead and/or inconsistencies. By
showing that the traditional checkpoint leads to inconsistencies or excessive
overhead, we define the notion of size-minimal checkpointing for fully
replicated databases. We present an algorithm for checkpointing with minimal
checkpointing overhead (only O(n) new messages and addition of a single counter
for existing messages). It also provides a significant benefit over existing
checkpointing algorithms for distributed systems and main-memory databases.
  A key benefit of DTCS is that it summarizes the computation by a sequence of
snapshots that are strongly consistent even though the underlying computation
is weakly consistent. In essence, when anomalies arise in an eventually
consistent system, DTCS enables one to concentrate solely on the snapshots
surrounding the time point of the anomaly.

</details>


### [5] [REACH: Reinforcement Learning for Adaptive Microservice Rescheduling in the Cloud-Edge Continuum](https://arxiv.org/abs/2510.06675)
*Xu Bai,Muhammed Tawfiqul Islam,Rajkumar Buyya,Adel N. Toosi*

Main category: cs.DC

TL;DR: REACH是一种基于强化学习的微服务重调度算法，用于在云边连续体中动态优化微服务部署，以应对资源波动和性能变化，显著降低端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 云计算虽然具有可扩展性优势，但无法完全满足新兴延迟敏感应用的实时需求。云边连续体结合了边缘资源的响应性和云的可扩展性，但异构动态的计算资源给微服务优化部署带来了挑战。

Method: 提出REACH算法，使用强化学习实时动态调整微服务部署位置，以响应分布式基础设施中波动的资源可用性和性能变化。

Result: 在真实测试平台上的广泛实验表明，REACH在三个基准MSA应用中分别将平均端到端延迟降低了7.9%、10%和8%，同时有效缓解了延迟波动和峰值。

Conclusion: REACH算法通过强化学习驱动的动态重调度，在云边连续体中实现了微服务的优化部署，显著提升了延迟敏感应用的性能表现。

Abstract: Cloud computing, despite its advantages in scalability, may not always fully
satisfy the low-latency demands of emerging latency-sensitive pervasive
applications. The cloud-edge continuum addresses this by integrating the
responsiveness of edge resources with cloud scalability. Microservice
Architecture (MSA) characterized by modular, loosely coupled services, aligns
effectively with this continuum. However, the heterogeneous and dynamic
computing resource poses significant challenges to the optimal placement of
microservices. We propose REACH, a novel rescheduling algorithm that
dynamically adapts microservice placement in real time using reinforcement
learning to react to fluctuating resource availability, and performance
variations across distributed infrastructures. Extensive experiments on a
real-world testbed demonstrate that REACH reduces average end-to-end latency by
7.9%, 10%, and 8% across three benchmark MSA applications, while effectively
mitigating latency fluctuations and spikes.

</details>


### [6] [Multi-Dimensional Autoscaling of Stream Processing Services on Edge Devices](https://arxiv.org/abs/2510.06882)
*Boris Sedlak,Philipp Raith,Andrea Morichetta,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.DC

TL;DR: 提出了MUDAP多维度自动伸缩平台，支持服务级和资源级的细粒度垂直伸缩，通过RASK智能代理学习环境回归模型来优化伸缩决策，在边缘设备上比现有方法减少28%的SLO违规。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，现有自动伸缩机制仅关注资源伸缩，无法满足竞争服务的SLO要求，需要支持多维度伸缩来维持服务质量。

Method: 开发MUDAP平台支持服务级和资源级垂直伸缩，使用基于结构知识回归分析的RASK代理探索解空间并学习连续回归模型，推断最优伸缩动作。

Result: RASK仅需20次迭代（约200秒处理时间）即可学习准确回归模型，相比Kubernetes VPA和强化学习代理，在9个服务场景下减少28%的SLO违规，支持更高请求负载。

Conclusion: 多维度自动伸缩方法能有效提升边缘设备上流处理服务的性能，RASK代理通过高效学习环境模型实现了更好的SLO保障。

Abstract: Edge devices have limited resources, which inevitably leads to situations
where stream processing services cannot satisfy their needs. While existing
autoscaling mechanisms focus entirely on resource scaling, Edge devices require
alternative ways to sustain the Service Level Objectives (SLOs) of competing
services. To address these issues, we introduce a Multi-dimensional Autoscaling
Platform (MUDAP) that supports fine-grained vertical scaling across both
service- and resource-level dimensions. MUDAP supports service-specific scaling
tailored to available parameters, e.g., scale data quality or model size for a
particular service. To optimize the execution across services, we present a
scaling agent based on Regression Analysis of Structural Knowledge (RASK). The
RASK agent efficiently explores the solution space and learns a continuous
regression model of the processing environment for inferring optimal scaling
actions. We compared our approach with two autoscalers, the Kubernetes VPA and
a reinforcement learning agent, for scaling up to 9 services on a single Edge
device. Our results showed that RASK can infer an accurate regression model in
merely 20 iterations (i.e., observe 200s of processing). By increasingly adding
elasticity dimensions, RASK sustained the highest request load with 28% less
SLO violations, compared to baselines.

</details>


### [7] [Evaluating Rapid Makespan Predictions for Heterogeneous Systems with Programmable Logic](https://arxiv.org/abs/2510.06998)
*Martin Wilhelm,Franz Freitag,Max Tzschoppe,Thilo Pionteck*

Main category: cs.DC

TL;DR: 本文提出了一个用于异构计算系统的灵活评估框架，能够基于抽象任务图描述收集真实世界的makespan结果，分析现有分析方法预测实际makespan的能力，并识别异构系统中的常见挑战。


<details>
  <summary>Details</summary>
Motivation: 异构计算系统结合通用处理器和专用加速器对现代应用性能优化日益重要，但预测任务映射变化对整体makespan的影响具有挑战性。现有模拟器需要完整任务实现，而分析方法虽然快速但过于抽象。

Method: 开发了一个高度灵活的评估框架，支持CPU、GPU和FPGA异构系统，能够基于抽象任务图描述收集真实makespan结果，并分析现有分析方法预测能力。

Result: 分析了现有分析方法预测实际makespan的程度，识别了数据传输开销和设备拥塞等异构系统高层特性带来的常见挑战。

Conclusion: 该框架有助于开发快速makespan预测算法，弥合理论与实践之间的差距，为异构系统算法开发提供支持。

Abstract: Heterogeneous computing systems, which combine general-purpose processors
with specialized accelerators, are increasingly important for optimizing the
performance of modern applications. A central challenge is to decide which
parts of an application should be executed on which accelerator or, more
generally, how to map the tasks of an application to available devices.
Predicting the impact of a change in a task mapping on the overall makespan is
non-trivial. While there are very capable simulators, these generally require a
full implementation of the tasks in question, which is particularly
time-intensive for programmable logic. A promising alternative is to use a
purely analytical function, which allows for very fast predictions, but
abstracts significantly from reality. Bridging the gap between theory and
practice poses a significant challenge to algorithm developers. This paper aims
to aid in the development of rapid makespan prediction algorithms by providing
a highly flexible evaluation framework for heterogeneous systems consisting of
CPUs, GPUs and FPGAs, which is capable of collecting real-world makespan
results based on abstract task graph descriptions. We analyze to what extent
actual makespans can be predicted by existing analytical approaches.
Furthermore, we present common challenges that arise from high-level
characteristics such as data transfer overhead and device congestion in
heterogeneous systems.

</details>


### [8] [GROMACS Unplugged: How Power Capping and Frequency Shapes Performance on GPUs](https://arxiv.org/abs/2510.06902)
*Ayesha Afzal,Anna Kahler,Georg Hager,Gerhard Wellein*

Main category: cs.DC

TL;DR: 对四种NVIDIA GPU加速器（A40、A100、L4、L40）在GROMACS分子动力学模拟中的性能分析，包括频率缩放和功耗限制的影响研究。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟的性能高度依赖于硬件选择和配置，需要为大规模MD工作流程在功耗限制下提供GPU硬件选择和GROMACS性能优化的实用指导。

Method: 使用六个代表性GROMACS生物分子工作负载和两个合成基准测试（计算受限的Pi Solver和内存受限的STREAM Triad），研究GPU图形时钟频率缩放和功耗限制对性能的影响。

Result: 较小的GROMACS系统表现出强烈的频率敏感性，而较大的系统快速饱和，变得越来越受内存限制。在功耗限制下，性能保持稳定直到达到架构和工作负载特定的阈值，高端GPU如A100即使在降低的功耗预算下也能保持接近最大性能。

Conclusion: 研究结果为在功耗限制下选择GPU硬件和优化GROMACS性能提供了实用指导，揭示了不同规模系统在频率缩放和功耗管理方面的不同行为特征。

Abstract: Molecular dynamics simulations are essential tools in computational
biophysics, but their performance depend heavily on hardware choices and
configuration. In this work, we presents a comprehensive performance analysis
of four NVIDIA GPU accelerators -- A40, A100, L4, and L40 -- using six
representative GROMACS biomolecular workloads alongside two synthetic
benchmarks: Pi Solver (compute bound) and STREAM Triad (memory bound). We
investigate how performance scales with GPU graphics clock frequency and how
workloads respond to power capping. The two synthetic benchmarks define the
extremes of frequency scaling: Pi Solver shows ideal compute scalability, while
STREAM Triad reveals memory bandwidth limits -- framing GROMACS's performance
in context. Our results reveal distinct frequency scaling behaviors: Smaller
GROMACS systems exhibit strong frequency sensitivity, while larger systems
saturate quickly, becoming increasingly memory bound. Under power capping,
performance remains stable until architecture- and workload-specific thresholds
are reached, with high-end GPUs like the A100 maintaining near-maximum
performance even under reduced power budgets. Our findings provide practical
guidance for selecting GPU hardware and optimizing GROMACS performance for
large-scale MD workflows under power constraints.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [On-Package Memory with Universal Chiplet Interconnect Express (UCIe): A Low Power, High Bandwidth, Low Latency and Low Cost Approach](https://arxiv.org/abs/2510.06513)
*Debendra Das Sharma,Swadesh Choudhary,Peter Onufryk,Rob Pelt*

Main category: cs.AR

TL;DR: 提出通过增强UCIe接口支持内存语义，为AI等计算应用提供高带宽密度、低延迟、低功耗和低成本的封装内存解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有封装内存解决方案无法满足AI等新兴计算应用对高能效带宽的需求，面临内存墙问题。

Method: 通过重用LPDDR6和HBM内存，使用逻辑芯片通过UCIe连接到SoC；或让DRAM芯片原生支持UCIe接口替代LPDDR6总线接口。

Result: 相比现有HBM4和LPDDR封装内存方案，带宽密度提升高达10倍，延迟降低高达3倍，功耗降低高达3倍，成本更低。

Conclusion: 增强UCIe支持内存语义能够为整个计算连续体提供高能效带宽和成本效益的封装内存解决方案。

Abstract: Emerging computing applications such as Artificial Intelligence (AI) are
facing a memory wall with existing on-package memory solutions that are unable
to meet the power-efficient bandwidth demands. We propose to enhance UCIe with
memory semantics to deliver power-efficient bandwidth and cost-effective
on-package memory solutions applicable across the entire computing continuum.
We propose approaches by reusing existing LPDDR6 and HBM memory through a logic
die that connects to the SoC using UCIe. We also propose an approach where the
DRAM die natively supports UCIe instead of the LPDDR6 bus interface. Our
approaches result in significantly higher bandwidth density (up to 10x), lower
latency (up to 3x), lower power (up to 3x), and lower cost compared to existing
HBM4 and LPDDR on-package memory solutions.

</details>


### [10] [RTGS: Real-Time 3D Gaussian Splatting SLAM via Multi-Level Redundancy Reduction](https://arxiv.org/abs/2510.06644)
*Leshu Li,Jiayin Qin,Jie Peng,Zishen Wan,Huaizhi Qu,Ye Han,Pingqing Zheng,Hongsen Zhang,Yu,Cao,Tianlong Chen,Yang,Zhao*

Main category: cs.AR

TL;DR: RTGS是一个算法-硬件协同设计框架，通过减少3D高斯溅射SLAM流水线中的冗余，在边缘设备上实现实时3DGS-SLAM性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS-SLAM系统虽然具有最先进的渲染效率和精度，但由于速度不足尚未在资源受限的边缘设备上采用，需要解决冗余问题以实现实时性能。

Method: 算法层面：引入自适应高斯剪枝和动态下采样技术；硬件层面：提出子瓦片级流式策略、渲染与反向传播缓冲区、梯度合并单元等优化方案。

Result: 在边缘GPU上实现实时性能（≥30 FPS），在四个数据集和三种算法上验证，相比基线实现高达82.5倍的能效提升，且质量损失可忽略。

Conclusion: RTGS通过算法-硬件协同设计成功解决了3DGS-SLAM在边缘设备上的实时性能瓶颈，为资源受限环境下的3D重建应用提供了可行解决方案。

Abstract: 3D Gaussian Splatting (3DGS) based Simultaneous Localization and Mapping
(SLAM) systems can largely benefit from 3DGS's state-of-the-art rendering
efficiency and accuracy, but have not yet been adopted in resource-constrained
edge devices due to insufficient speed. Addressing this, we identify notable
redundancies across the SLAM pipeline for acceleration. While conceptually
straightforward, practical approaches are required to minimize the overhead
associated with identifying and eliminating these redundancies. In response, we
propose RTGS, an algorithm-hardware co-design framework that comprehensively
reduces the redundancies for real-time 3DGS-SLAM on edge. To minimize the
overhead, RTGS fully leverages the characteristics of the 3DGS-SLAM pipeline.
On the algorithm side, we introduce (1) an adaptive Gaussian pruning step to
remove the redundant Gaussians by reusing gradients computed during
backpropagation; and (2) a dynamic downsampling technique that directly reuses
the keyframe identification and alpha computing steps to eliminate redundant
pixels. On the hardware side, we propose (1) a subtile-level streaming strategy
and a pixel-level pairwise scheduling strategy that mitigates workload
imbalance via a Workload Scheduling Unit (WSU) guided by previous iteration
information; (2) a Rendering and Backpropagation (R&B) Buffer that accelerates
the rendering backpropagation by reusing intermediate data computed during
rendering; and (3) a Gradient Merging Unit (GMU) to reduce intensive memory
accesses caused by atomic operations while enabling pipelined aggregation.
Integrated into an edge GPU, RTGS achieves real-time performance (>= 30 FPS) on
four datasets and three algorithms, with up to 82.5x energy efficiency over the
baseline and negligible quality loss. Code is available at
https://github.com/UMN-ZhaoLab/RTGS.

</details>


### [11] [Hardware-Efficient CNNs: Interleaved Approximate FP32 Multipliers for Kernel Computation](https://arxiv.org/abs/2510.06767)
*Bindu G Gowda,Yogesh Goyal,Yash Gupta,Madhav Rao*

Main category: cs.AR

TL;DR: 该论文提出了一种使用近似FP32乘法器的CNN推理方法，通过采用误差可变的近似压缩器来近似尾数乘法，显著降低硬件成本，并使用NSGA-II算法优化近似乘法器的放置和排序，以平衡精度和硬件效率。


<details>
  <summary>Details</summary>
Motivation: FP32乘法在计算上昂贵且需要复杂硬件，但在神经网络推理等实际应用中，完美精度并非总是必要，微小的乘法误差通常对最终精度影响不大，这为在面积、功耗和速度方面换取增益提供了可能。

Method: 使用误差可变的近似压缩器来近似FP32乘法器的尾数乘法，显著降低硬件成本；采用非支配排序遗传算法II（NSGA-II）优化CNN中不同近似FP32乘法器的放置和排序，平衡精度和硬件效率的权衡。

Result: 通过近似FP32乘法器和优化策略，在CNN推理中实现了硬件成本的显著降低，同时保持了可接受的精度水平。

Conclusion: 在CNN推理中采用近似FP32乘法器并结合优化算法，可以有效平衡精度和硬件效率，为实际应用提供了一种可行的解决方案。

Abstract: Single-precision floating point (FP32) data format, defined by the IEEE 754
standard, is widely employed in scientific computing, signal processing, and
deep learning training, where precision is critical. However, FP32
multiplication is computationally expensive and requires complex hardware,
especially for precisely handling mantissa multiplication. In practical
applications like neural network inference, perfect accuracy is not always
necessary, minor multiplication errors often have little impact on final
accuracy. This enables trading precision for gains in area, power, and speed.
This work focuses on CNN inference using approximate FP32 multipliers, where
the mantissa multiplication is approximated by employing error-variant
approximate compressors, that significantly reduce hardware cost. Furthermore,
this work optimizes CNN performance by employing differently approximated FP32
multipliers and studying their impact when interleaved within the kernels
across the convolutional layers. The placement and ordering of these
approximate multipliers within each kernel are carefully optimized using the
Non-dominated Sorting Genetic Algorithm-II, balancing the trade-off between
accuracy and hardware efficiency.

</details>


### [12] [Cocoon: A System Architecture for Differentially Private Training with Correlated Noises](https://arxiv.org/abs/2510.07304)
*Donghwan Kim,Xin Gu,Jinho Baek,Timothy Lo,Younghoon Min,Kwangsik Shin,Jongryool Kim,Jongse Park,Kiwan Maeng*

Main category: cs.AR

TL;DR: Cocoon是一个软硬件协同设计的框架，通过预计算和存储相关噪声以及定制近内存处理设备，显著提升了使用相关噪声的差分隐私训练效率。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型会记忆和泄露训练数据，带来严重的隐私问题。虽然DP-SGD等差分隐私训练算法是解决方案，但它们在每个训练迭代中添加噪声会降低模型准确性。新的相关噪声方法虽然能提高准确性，但在模型较大或使用大嵌入表时会产生显著开销。

Method: 提出Cocoon框架：1) 通过预计算并以合并格式存储相关噪声来加速带有嵌入表的模型(Cocoon-Emb)；2) 通过定制近内存处理设备支持大型模型(Cocoon-NMP)。

Result: 在基于FPGA的NMP设备原型上，Cocoon-Emb将性能提升了2.33-10.82倍，Cocoon-NMP提升了1.55-3.06倍。

Conclusion: Cocoon通过软硬件协同设计有效解决了相关噪声方法在大型模型和大嵌入表场景下的性能开销问题，显著提升了差分隐私训练的效率。

Abstract: Machine learning (ML) models memorize and leak training data, causing serious
privacy issues to data owners. Training algorithms with differential privacy
(DP), such as DP-SGD, have been gaining attention as a solution. However,
DP-SGD adds a noise at each training iteration, which degrades the accuracy of
the trained model. To improve accuracy, a new family of approaches adds
carefully designed correlated noises, so that noises cancel out each other
across iterations. We performed an extensive characterization study of these
new mechanisms, for the first time to the best of our knowledge, and show they
incur non-negligible overheads when the model is large or uses large embedding
tables. Motivated by the analysis, we propose Cocoon, a hardware-software
co-designed framework for efficient training with correlated noises. Cocoon
accelerates models with embedding tables through pre-computing and storing
correlated noises in a coalesced format (Cocoon-Emb), and supports large models
through a custom near-memory processing device (Cocoon-NMP). On a real system
with an FPGA-based NMP device prototype, Cocoon improves the performance by
2.33-10.82x(Cocoon-Emb) and 1.55-3.06x (Cocoon-NMP).

</details>
