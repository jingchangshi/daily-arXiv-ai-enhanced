<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.DC](#cs.DC) [Total: 21]
- [cs.AR](#cs.AR) [Total: 11]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [ChopChop: a Programmable Framework for Semantically Constraining the Output of Language Models](https://arxiv.org/abs/2509.00360)
*Shaan Nagy,Timothy Zhou,Nadia Polikarpova,Loris D'Antoni*

Main category: cs.PL

TL;DR: ChopChop是一个可编程的语义约束解码框架，通过连接token级生成和抽象程序结构推理，确保语言模型生成的代码满足丰富的语义属性，如类型安全和程序等价性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型生成的代码无法保证正确性，经常违反类型安全、程序不变量或语义等价性。现有的约束解码方法仅限于浅层语法约束或依赖脆弱的语义编码。

Method: 使用基于共归纳的形式化方法，将token级生成与抽象程序结构推理连接起来，将约束执行简化为正则共数据上的可实现性问题。

Result: ChopChop将语义约束解码从小众技术转变为系统化、原则性的语言模型扩展，提高了跨模型和任务的成功率，同时保持实用的解码延迟。

Conclusion: 该框架展示了形式化方法可以无缝集成到LM驱动的代码生成中，为生成可证明满足丰富语义属性的代码提供了系统化解决方案。

Abstract: Language models (LMs) can generate code, but cannot guarantee its
correctness--producing outputs that often violate type safety, program
invariants, or semantic equivalence. Constrained decoding offers a solution by
restricting generation to programs that satisfy desired properties. Yet,
existing methods are limited to shallow syntactic constraints or rely on
brittle, ad hoc encodings of semantics over token sequences.
  We present ChopChop, the first programmable framework for semantic
constrained decoding, enabling LMs to generate code that provably satisfies
rich semantic properties. ChopChop connects token-level generation with
reasoning over abstract program structures using a coinduction-based formalism
and reduces constraint enforcement to a realizability problem over regular
codata. We demonstrate ChopChop's generality through generation constrained by
type safety and program equivalence, showing how formal methods can be
seamlessly integrated into LM-driven code generation. ChopChop transforms
semantic constrained decoding from a niche technique into a systematic,
principled extension of LMs--improving success rates across models and tasks
while maintaining practical decoding latency.

</details>


### [2] [A Hoare Logic for Symmetry Properties](https://arxiv.org/abs/2509.00587)
*Vaibhav Mehta,Justin Hsu*

Main category: cs.PL

TL;DR: 提出了一种用于验证程序对称性属性的形式化方法和工具，通过群动作语法和Hoare风格逻辑来验证对称性属性


<details>
  <summary>Details</summary>
Motivation: 现有的形式化方法对对称性属性的支持不足，而许多程序正确性属性都可以用对称性来表达

Method: 设计了群动作语法来指定对称性属性，开发了Hoare风格的逻辑来验证命令式程序的对称性属性，并实现了原型工具SymVerif

Result: 在手工制作的基准测试上验证了对称性属性，并发现了McLachlan和Quispel(2002)描述的动态系统模型中的一个错误

Conclusion: 该方法能够有效验证广泛的对称性属性，为程序正确性验证提供了新的形式化工具

Abstract: Many natural program correctness properties can be stated in terms of
  symmetries, but existing formal methods have little support for reasoning
  about such properties. We consider how to formally verify a broad class of
  symmetry properties expressed in terms of group actions. To specify these
  properties, we design a syntax for group actions, supporting standard
  constructions and a natural notion of entailment. Then, we develop a
  Hoare-style logic for verifying symmetry properties of imperative programs,
  where group actions take the place of the typical pre- and post-condition
  assertions. Finally, we develop a prototype tool $\mathsf{SymVerif}$, and use
  it to verify symmetry properties on a series of handcrafted benchmarks. Our
  tool uncovered an error in a model of a dynamical system described by
\citet{McLachlan_Quispel_2002}.

</details>


### [3] [Formalizing Linear Motion G-code for Invariant Checking and Differential Testing of Fabrication Tools](https://arxiv.org/abs/2509.00699)
*Yumeng He,Chandrakana Nandi,Sreepathi Pai*

Main category: cs.PL

TL;DR: 这篇论文提出了一种新的G代码分析算法，通过将G代码转换为立方体集合和近似点云表示，支持CAD模型错误定位、切片器比较和网格修复工具效果评估。


<details>
  <summary>Details</summary>
Motivation: 传统编译器有很多程序不变量检查技术，但3D打印制造流水线缺乏类似方法。需要一种能够处理几何对象和机器代码的方法来检测切片过程中的错误和问题。

Method: 提出了一种新的G代码提取算法，将G代码程序转换为立方体集合，然后定义近似点云表示以高效操作这些立方体。实现了原型工具GlitchFinder。

Result: 在58个实际CAD模型上评估，结果显示GlitchFinder能有效识别小特征导致的切片问题，显示不同切片器（Cura和PrusaSlicer）的差异，以及识别网格修复工具（MeshLab和Meshmixer）在修复过程中引入的新错误。

Conclusion: 该算法为3D打印制造流水线提供了类似传统编译器的错误检测能力，开启了CAD模型质量控制、切片器比较和网格修复效果评估等新的应用可能性。

Abstract: The computational fabrication pipeline for 3D printing is much like a
compiler - users design models in Computer Aided Design (CAD) tools that are
lowered to polygon meshes to be ultimately compiled to machine code by 3D
slicers. For traditional compilers and programming languages, techniques for
checking program invariants are well-established. Similarly, methods like
differential testing are often used to uncover bugs in compilers themselves,
which makes them more reliable. The fabrication pipeline would benefit from
similar techniques but traditional approaches do not directly apply to the
representations used in this domain. Unlike traditional programs, 3D models
exist both as geometric objects as well as machine code that ultimately runs on
the hardware. The machine code, like in traditional compiling, is affected by
many factors like the model, the slicer being used, and numerous
user-configurable parameters that control the slicing process. In this work, we
propose a new algorithm for lifting G-code (a common language used in
fabrication pipelines) by denoting a G-code program to a set of cuboids, and
then defining an approximate point cloud representation for efficiently
operating on these cuboids. Our algorithm opens up new opportunities: we show
three use cases that demonstrate how it enables error localization in CAD
models through invariant checking, quantitative comparisons between slicers,
and evaluating the efficacy of mesh repair tools. We present a prototype
implementation of our algorithm in a tool, GlitchFinder, and evaluate it on 58
real-world CAD models. Our results show that GlitchFinder is particularly
effective in identifying slicing issues due to small features, can highlight
differences in how popular slicers (Cura and PrusaSlicer) slice the same model,
and can identify cases where mesh repair tools (MeshLab and Meshmixer)
introduce new errors during repair.

</details>


### [4] [Decision Procedure for A Theory of String Sequences](https://arxiv.org/abs/2509.00948)
*Denghang Hu,Taolue Chen,Philipp Rümmer,Fu Song,Zhilin Wu*

Main category: cs.PL

TL;DR: 该论文提出了字符串序列理论，通过将字符串序列编码为字符串并利用OSTRICH框架实现约束求解，解决了SMT求解器对字符串操作支持不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的SMT求解器仅支持通用序列理论，但缺乏对字符串操作（如正则表达式匹配、字符串分割和连接）的直接支持，而这些操作在字符串处理程序中经常使用。

Method: 提出字符串序列理论，将每个字符串序列编码为字符串，并将字符串序列操作映射为对应的字符串操作。通过预像计算和OSTRICH框架实现约束求解，限制在直线片段内保证可判定性。

Result: 实现了工具ostrichseq，在真实JavaScript程序生成的基准约束、手工制作的模板和单元测试上进行了实验，验证了方法的有效性。

Conclusion: 字符串序列理论虽然一般情况下不可判定，但通过限制在直线片段可以恢复可判定性，该方法能够有效处理字符串序列操作，为SMT求解器提供了更好的字符串操作支持。

Abstract: The theory of sequences, supported by many SMT solvers, can model program
data types including bounded arrays and lists. Sequences are parameterized by
the element data type and provide operations such as accessing elements,
concatenation, forming sub-sequences and updating elements. Strings and
sequences are intimately related; many operations, e.g., matching a string
according to a regular expression, splitting strings, or joining strings in a
sequence, are frequently used in string-manipulating programs. Nevertheless,
these operations are typically not directly supported by existing SMT solvers,
which instead only consider the generic theory of sequences. In this paper, we
propose a theory of string sequences and study its satisfiability. We show
that, while it is undecidable in general, the decidability can be recovered by
restricting to the straight-line fragment. This is shown by encoding each
string sequence as a string, and each string sequence operation as a
corresponding string operation. We provide pre-image computation for the
resulting string operations with respect to automata, effectively casting it
into the generic OSTRICH string constraint solving framework. We implement the
new decision procedure as a tool $\ostrichseq$, and carry out experiments on
benchmark constraints generated from real-world JavaScript programs,
hand-crafted templates and unit tests. The experiments confirm the efficacy of
our approach.

</details>


### [5] [Type-Based Incorrectness Reasoning](https://arxiv.org/abs/2509.01511)
*Zhe Zhou,Benjamin Delaware,Suresh Jagannathan*

Main category: cs.PL

TL;DR: 本文探讨了覆盖类型与不正确性逻辑之间的深层联系，提出将不正确性推理系统化集成到精化类型系统中的机制，为函数式程序员和程序分析工具提供新机会


<details>
  <summary>Details</summary>
Motivation: 覆盖类型能够支持must-style的欠近似推理，这在基于属性的测试框架中特别有用，可以验证测试生成器的完整性和安全性。研究发现覆盖类型提供的推理方式与最近提出的不正确性逻辑框架存在惊人联系

Method: 通过深入探索覆盖类型与不正确性逻辑之间的连接，识别出将不正确性推理更系统化地集成到表达性精化类型系统中的机制

Result: 建立了覆盖类型推理与不正确性逻辑之间的理论联系，提出了系统化集成方法

Conclusion: 这种集成为函数式程序员、程序验证器、程序分析器及相关工具提供了新的机会和可能性，推动了欠近似推理在程序分析中的应用

Abstract: A coverage type generalizes refinement types found in many functional
languages with support for must-style underapproximate reasoning.
Property-based testing frameworks are one particularly useful domain where such
capabilities are useful as they allow us to verify the completeness, as well as
safety, of test generators. There is a surprising connection between the kind
of underapproximate reasoning coverage types offer and the style of reasoning
enabled by recently proposed Incorrectness Logic frameworks. In our
presentation, we propose to explore this connection more deeply, identifying
mechanisms that more systematically integrate incorrectness reasoning within an
expressive refinement type system and the opportunities that such integration
offers to functional programmers, program verifiers, and program analyzers and
related tools.

</details>


### [6] [From Traces to Program Incorrectness: A Type-Theoretic Approach](https://arxiv.org/abs/2509.02428)
*Yongwei Yuan,Zhe Zhou,Julia Belyakova,Benjamin Delaware,Suresh Jagannathan*

Main category: cs.PL

TL;DR: 基于符号正则表达式的类型推断框架，用于分析函数程序中与效果库API交互时的错误行为


<details>
  <summary>Details</summary>
Motivation: 需要一种系统方法来进行不正确性推理，特别是在函数程序与不透明库API交互时识别可能的错误行为

Method: 使用符号正则表达式(SREs)表示跟踪序列，通过符号有限自动机(SFAs)进行组合推理，开发了一种新的类型推断算法

Result: 算法成功时，推断出的类型能够证明ADT实现可能展现指定的不正确行为子集

Conclusion: 这是首个系统性的不正确性轨迹规范分析方法，开启了轨迹导向的组合分析新方向

Abstract: We present a type-theoretic framework for reasoning about incorrectness in
functional programs that interact with effectful, opaque library APIs. Our
approach centers on traces -- temporally-ordered sequences of library API
invocations -- which naturally characterize both the preconditions of
individual APIs and their composite behavior. We represent these traces using
symbolic regular expressions (SREs), enabling formal specification of incorrect
abstract data type (ADT) behaviors across function boundaries. The core
contribution is a novel type inference algorithm that operates modulo specified
incorrectness properties and leverages the symbolic finite automata (SFAs)
representations of regexes for compositional reasoning of traces. When the
algorithm succeeds, the inferred types witness that an ADT implementation can
exhibit some subset of the specified incorrect behaviors. This represents the
first systematic approach to underapproximate reasoning against trace-based
incorrectness specifications, enabling a new form of trace-guided compositional
analysis.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for KV Cache](https://arxiv.org/abs/2509.00579)
*Bo Jiang,Taolue Yang,Youyuan Liu,Chengming Zhang,Xubin He,Sian Jin*

Main category: cs.DC

TL;DR: KVComp是一个针对长文本生成的KV缓存管理框架，通过新颖的有损压缩技术显著减少内存使用，同时保持模型精度和高计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度和批量大小的增加，Transformer大语言模型中的KV缓存内存需求可达数GB，成为长上下文推理的主要挑战。

Method: 采用专门为KV缓存数据特性设计的有损压缩技术，结合压缩算法和系统架构的协同设计，保持与KV缓存增长特性的兼容性。

Result: 平均实现47%的内存减少率，最高可达83%，且几乎没有模型精度损失。执行吞吐量极高，在某些情况下甚至加速矩阵向量乘法操作。

Conclusion: KVComp是一个通用高效的KV缓存管理框架，在长文本生成任务中显著优于现有方法，为长上下文推理提供了有效的内存优化解决方案。

Abstract: Transformer-based large language models (LLMs) demonstrate impressive
potential in various practical applications. However, long context inference
poses a significant challenge due to the enormous memory requirements of the
key-value (KV) cache, which can scale to multiple gigabytes as sequence length
and batch size increase. In this paper, we present KVComp, a generic and
efficient KV cache management framework optimized for long-text generation that
synergistically works with both latency-critical and throughput-critical
inference systems. KVComp employs novel lossy compression techniques
specifically designed for KV cache data characteristics, featuring careful
co-design of compression algorithms and system architecture. Our approach
maintains compatibility with the growing nature of KV cache while preserving
high computational efficiency. Experimental results show that KVComp achieves
on average 47\% and up to 83\% higher memory reduction rate compared to
existing methods with little/no model accuracy degradation. Furthermore, KVComp
achieves extremely high execution throughput, effectively reducing
decompression overhead and, in some cases, even accelerating the matrix-vector
multiplication operation and outperform cuBLAS-based attention kernels with
less data movement.

</details>


### [8] [HADIS: Hybrid Adaptive Diffusion Model Serving for Efficient Text-to-Image Generation](https://arxiv.org/abs/2509.00642)
*Qizheng Yang,Tung-I Chen,Siyu Zhao,Ramesh K. Sitaraman,Hui Guan*

Main category: cs.DC

TL;DR: HADIS是一个混合自适应扩散模型服务系统，通过智能查询路由和资源配置优化，在保证质量的同时显著降低计算成本和延迟违规率


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型虽然视觉质量出色但计算成本高昂，现有服务系统采用固定级联配置且所有查询都经过轻量级阶段，对复杂查询造成资源浪费

Method: 使用基于规则的提示路由器将明显困难的查询直接路由到重量级模型；通过离线分析生成帕累托最优级联配置表；运行时根据延迟和工作负载约束选择最佳级联配置和GPU分配

Result: 在真实世界跟踪评估中，HADIS将响应质量提升高达35%，同时将延迟违规率降低2.7-45倍

Conclusion: HADIS系统通过自适应查询路由和资源配置优化，有效解决了扩散模型服务的高计算成本和延迟问题，显著提升了服务效率和质量

Abstract: Text-to-image diffusion models have achieved remarkable visual quality but
incur high computational costs, making real-time, scalable deployment
challenging. Existing query-aware serving systems mitigate the cost by
cascading lightweight and heavyweight models, but most rely on a fixed cascade
configuration and route all prompts through an initial lightweight stage,
wasting resources on complex queries. We present HADIS, a hybrid adaptive
diffusion model serving system that jointly optimizes cascade model selection,
query routing, and resource allocation. HADIS employs a rule-based prompt
router to send clearly hard queries directly to heavyweight models, bypassing
the overhead of the lightweight stage. To reduce the complexity of resource
management, HADIS uses an offline profiling phase to produce a Pareto-optimal
cascade configuration table. At runtime, HADIS selects the best cascade
configuration and GPU allocation given latency and workload constraints.
Empirical evaluations on real-world traces demonstrate that HADIS improves
response quality by up to 35% while reducing latency violation rates by
2.7-45$\times$ compared to state-of-the-art model serving systems.

</details>


### [9] [Accelerating Latency-Critical Applications with AI-Powered Semi-Automatic Fine-Grained Parallelization on SMT Processors](https://arxiv.org/abs/2509.00883)
*Denis Los,Igor Petushkov*

Main category: cs.DC

TL;DR: 本文提出Aira AI并行化顾问系统，利用SMT技术实现延迟关键应用的细粒度并行化，在工业级基准测试中获得了17%的几何平均性能提升


<details>
  <summary>Details</summary>
Motivation: 延迟关键应用在超标量处理器中由于缓存缺失和预测错误导致功能单元利用率低，但传统SMT技术很少用于此类重线程应用

Method: 扩展Cursor IDE中的AI编码代理，通过模型上下文协议连接额外工具，实现LLM引导的热点检测、动态依赖收集和SMT感知性能模拟，结合Relic并行框架

Result: 使用Aira和Relic框架对延迟关键基准测试进行并行化，获得了17%的几何平均性能增益

Conclusion: Aira系统成功证明了AI驱动的并行化方法可以有效提升延迟关键应用的性能，为SMT技术在重线程应用中的使用提供了新思路

Abstract: Latency-critical applications tend to show low utilization of functional
units due to frequent cache misses and mispredictions during speculative
execution in high-performance superscalar processors. However, due to
significant impact on single-thread performance, Simultaneous Multithreading
(SMT) technology is rarely used with heavy threads of latency-critical
applications. In this paper, we explore utilization of SMT technology to
support fine-grained parallelization of latency-critical applications.
Following the advancements in the development of Large Language Models (LLMs),
we introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we
extend AI Coding Agent in Cursor IDE with additional tools connected through
Model Context Protocol, enabling end-to-end AI Agent for parallelization.
Additional connected tools enable LLM-guided hotspot detection, collection of
dynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance
simulation to estimate performance gains. We apply Aira with Relic parallel
framework for fine-grained task parallelism on SMT cores to parallelize
latency-critical benchmarks representing real-world applications used in
industry. We show 17% geomean performance gain from parallelization of
latency-critical benchmarks using Aira with Relic framework.

</details>


### [10] [Safe Memory Reclamation Techniques](https://arxiv.org/abs/2509.02457)
*Ajay Singh*

Main category: cs.DC

TL;DR: 这篇论文研究了非垃圾回收编程语言中乐观和无锁并发数据结构的安全内存回收技术，提出了跨软件硬件层次的解决方案来应对高性能、易用性、应用范围和内存占用等挑战。


<details>
  <summary>Details</summary>
Motivation: 在非垃圾回收编程语言中，安全内存回收对于乐观和无锁并发数据结构的内存安全至关重要。但设计理想的安全内存回收算法面临着多重挑战，包括实现高速度和可扩展性、编程人员易用性、应用于广泛数据结构类型、管理因延迟释放内存而导致的大内存占用以及避免数据结构操作的不对称开销。

Method: 研究了多种设计安全内存回收算法的方法，通过融合来自硬件-软件栈不同层次的思想和工具。这些解决方案跨越了传统界限，利用了不同层次暴露的特性。

Result: 提出了一种跨层次的方法来解决安全内存回收的多重挑战，包括性能、可扩展性、易用性和内存管理等方面。

Conclusion: 通过融合硬件-软件栈不同层次的技术特性，可以设计出更有效的安全内存回收算法，以满足并发数据结构在非垃圾回收语言中的内存安全需求。

Abstract: Safe memory reclamation is crucial to memory safety for optimistic and
lock-free concurrent data structures in non garbage collected programming
languages. However, several challenges arise in designing an ideal safe memory
reclamation algorithm, including achieving high speed and scalability, easy of
use for programmers, applicability to wide class of data structures, managing
the large memory footprint caused by delayed freeing of memory for safety and
performance, and avoiding asymmetric overhead on data structure operations.
Several approaches to designing safe memory reclamation algorithms are studied
by blending ideas and tools from across the hardware-software stack. These
solutions cross traditional boundaries and exploit features exposed at
different layers.

</details>


### [11] [Parallelizing Drug Discovery: HPC Pipelines for Alzheimer's Molecular Docking and Simulation](https://arxiv.org/abs/2509.00937)
*Paul Ruiz Alliata,Diana Rubaga,Daniel Kumlin,Alberto Puliga*

Main category: cs.DC

TL;DR: 本文探索了HPC驱动的高性能计算在阿尔茨海默病药物发现中的应用，通过并行化工作流程和分子对接原型，在虚拟筛选、分子对接和分子动力学模拟方面实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 利用高性能计算重塑计算药物发现，通过大规模、时间高效的分子模拟来加速神经退行性疾病药物研发，特别是针对阿尔茨海默病的治疗药物发现。

Method: 采用GROMACS工具配合混合MPI-OpenMP策略实现并行化工作流程，包括能量最小化、平衡和生产阶段；使用Python的multiprocessing库开发了基于进程并行的分子对接原型。

Result: 在能量最小化、平衡和生产阶段展示了良好的扩展性能；分子对接从顺序执行转向进程并行后获得了显著的运行时增益；通过脯氨酰胺衍生物和黄芩素案例研究验证了工作流程的生物相关性。

Conclusion: 尽管在数据管理、计算成本和扩展效率方面仍存在限制，但研究结果强调了HPC在加速神经退行性疾病药物发现方面的巨大潜力。

Abstract: High-performance computing (HPC) is reshaping computational drug discovery by
enabling large-scale, time-efficient molecular simulations. In this work, we
explore HPC-driven pipelines for Alzheimer's disease drug discovery, focusing
on virtual screening, molecular docking, and molecular dynamics simulations. We
implemented a parallelised workflow using GROMACS with hybrid MPI-OpenMP
strategies, benchmarking scaling performance across energy minimisation,
equilibration, and production stages. Additionally, we developed a docking
prototype that demonstrates significant runtime gains when moving from
sequential execution to process-based parallelism using Python's
multiprocessing library. Case studies on prolinamide derivatives and baicalein
highlight the biological relevance of these workflows in targeting amyloid-beta
and tau proteins. While limitations remain in data management, computational
costs, and scaling efficiency, our results underline the potential of HPC to
accelerate neurodegenerative drug discovery.

</details>


### [12] [DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World Serving](https://arxiv.org/abs/2509.01083)
*Mingyu Yang,Jae-Young Choi,Kihyo Moon,Minsung Jang,Eunjoo Joen*

Main category: cs.DC

TL;DR: 提出DSDE框架，通过KLD方差预测信号和自适应推测长度上限，实现动态推测解码，在批量服务环境中提升大语言模型推理效率


<details>
  <summary>Details</summary>
Motivation: 传统推测解码依赖固定推测长度，在批量服务环境中面对多样化请求时效率不佳，需要动态适应机制

Method: DSDE框架包含两个核心组件：基于KLD方差的分析生成区域稳定性的预测信号，以及解决单序列解码中滞后问题的自适应推测长度上限机制

Result: 实验显示基于KLD的稳定性信号在动态适应方面具有潜力，算法在端到端延迟方面与领先基线竞争，并在多样化工作负载下表现出优越的鲁棒性

Conclusion: 后验信号是构建更鲁棒智能LLM推理系统的重要组件，为动态推测长度适应的未来研究指明了有前景的方向

Abstract: Speculative decoding accelerates large language model inference, but its
reliance on a fixed speculation length is suboptimal in large-batch serving
environments with diverse requests. This paper explores a new direction for
dynamic adaptation by investigating a novel class of post-hoc, diagnostic
signals. We propose Dynamic Speculative Decoding Engine (DSDE), a training-free
framework built on two primary components: (1) a predictive signal based on the
variance of the Kullback-Leibler (KLD) divergence, which diagnoses the
generation's regional stability, and (2) an adaptive speculation length cap to
mitigate the straggler problem in per-sequence decoding. Experiments
demonstrate the potential of using KLD-based stability signals for dynamic
adaptation. An algorithm guided by these signals achieves end-to-end latency
competitive with leading baselines and exhibits superior robustness across
diverse workloads. This robustness is particularly valuable in challenging
low-acceptance-rate regimes, where the proposed signal maintains its diagnostic
utility. Collectively, these findings validate post-hoc signals as a valuable
component for building more robust and intelligent LLM inference systems, and
highlight a promising direction for future research on dynamic speculation
length adaptation.

</details>


### [13] [Ocior: Ultra-Fast Asynchronous Leaderless Consensus with Two-Round Finality, Linear Overhead, and Adaptive Security](https://arxiv.org/abs/2509.01118)
*Jinyuan Chen*

Main category: cs.DC

TL;DR: Ocior是一个实用的异步拜占庭容错共识协议，在弹性、通信、计算和轮次复杂度方面达到最优性能。它是一个无领导者的协议，通过并行共识实例处理交易，支持瞬时阈值签名聚合。


<details>
  <summary>Details</summary>
Motivation: 传统BFT共识协议依赖指定领导者来提议交易，存在性能瓶颈。需要开发一个无领导者的共识协议，在异步环境中实现最优性能，同时保证稳定的活跃性。

Method: 提出Ocior协议，使用并行共识实例处理交易，引入新型非交互式阈值签名方案OciorBLSts，支持瞬时签名聚合和自适应安全性。

Result: 达到最优弹性（n≥3t+1）、最优通信复杂度（O(n)）、最优或接近最优计算复杂度（O(n)或O(n log² n)）、最优轮次复杂度（2轮异步延迟）。

Conclusion: Ocior协议在异步拜占庭容错共识中实现了全面的性能优化，通过无领导者设计和创新的阈值签名技术，为分布式系统提供了高效的共识解决方案。

Abstract: In this work, we propose Ocior, a practical asynchronous Byzantine
fault-tolerant (BFT) consensus protocol that achieves the optimal performance
in resilience, communication, computation, and round complexity. Unlike
traditional BFT consensus protocols, Ocior processes incoming transactions
individually and concurrently using parallel instances of consensus. While
leader-based consensus protocols rely on a designated leader to propose
transactions, Ocior is a leaderless consensus protocol that guarantees stable
liveness. Ocior achieves: 1) Optimal resilience: Ocior tolerates up to $t$
faulty nodes controlled by an adaptive adversary, for $n\geq 3t+1$. 2) Optimal
communication complexity: The total expected communication per transaction is
$O(n)$. 3) Optimal (or near-optimal) computation complexity: The total
computation per transaction is $O(n)$ in the best case, or $O(n \log^2 n)$ in
the worst case. 4) Optimal round complexity: A legitimate two-party transaction
can be finalized with a good-case latency of two asynchronous rounds, for any
$n\geq 3t+1$. The good case in terms of latency refers to the scenario where
the transaction is proposed by any (not necessarily designated) honest node. A
two-party transaction involves the transfer of digital assets from one user (or
group of users) to one or more recipients. To support efficient consensus, we
introduce a novel non-interactive threshold signature (TS) scheme called
OciorBLSts. It offers fast signature aggregation, and is adaptively secure.
OciorBLSts achieves a signature aggregation computation cost of only $O(n)$ for
the best case. Moreover, OciorBLSts supports the property of Instantaneous TS
Aggregation. This enables real-time aggregation of partial signatures as they
arrive, reducing waiting time and improving responsiveness.

</details>


### [14] [Detecting Rug Pulls in Decentralized Exchanges: Machine Learning Evidence from the TON Blockchain](https://arxiv.org/abs/2509.01168)
*Dmitry Yaremus,Jianghai Li,Alisa Kalacheva,Igor Vodolazov,Yury Yanovich*

Main category: cs.DC

TL;DR: 提出了一个机器学习框架，用于在TON区块链上去中心化交易所中早期检测rug pull骗局，通过两种不同的rug pull定义方法进行比较分析，能够在交易前5分钟内有效识别欺诈行为。


<details>
  <summary>Details</summary>
Motivation: TON区块链的独特架构（异步执行和Telegram带来的海量web2用户）为欺诈分析提供了新颖且关键的环境，需要保护投资者并增强TON DeFi生态系统的安全基础设施。

Method: 在TON最大的两个DEX平台Ston.Fi和DeDust上收集数据，融合两个平台的数据训练模型。采用两种rug pull定义方法：TVL-based（灾难性流动性提取）和idle-based（交易活动突然停止），使用梯度提升模型进行早期检测。

Result: 梯度提升模型能在交易前5分钟内有效识别rug pull，TVL-based方法达到最佳AUC（0.891），idle-based方法在召回率方面表现优异。研究发现虽然特征集在交易所间一致，但底层分布差异显著。

Conclusion: 该工作为投资者提供了关键的早期预警机制，增强了快速增长的TON DeFi生态系统的安全基础设施，强调了需要健壮的、平台感知的模型来应对不同交易所的数据分布差异。

Abstract: This paper presents a machine learning framework for the early detection of
rug pull scams on decentralized exchanges (DEXs) within The Open Network (TON)
blockchain. TON's unique architecture, characterized by asynchronous execution
and a massive web2 user base from Telegram, presents a novel and critical
environment for fraud analysis. We conduct a comprehensive study on the two
largest TON DEXs, Ston.Fi and DeDust, fusing data from both platforms to train
our models. A key contribution is the implementation and comparative analysis
of two distinct rug pull definitions--TVL-based (a catastrophic liquidity
withdrawal) and idle-based (a sudden cessation of all trading activity)--within
a single, unified study. We demonstrate that Gradient Boosting models can
effectively identify rug pulls within the first five minutes of trading, with
the TVL-based method achieving superior AUC (up to 0.891) while the idle-based
method excels at recall. Our analysis reveals that while feature sets are
consistent across exchanges, their underlying distributions differ
significantly, challenging straightforward data fusion and highlighting the
need for robust, platform-aware models. This work provides a crucial
early-warning mechanism for investors and enhances the security infrastructure
of the rapidly growing TON DeFi ecosystem.

</details>


### [15] [LobRA: Multi-tenant Fine-tuning over Heterogeneous Data](https://arxiv.org/abs/2509.01193)
*Sheng Lin,Fangcheng Fu,Haoyang Li,Hao Ge,Xuanyu Wang,Jiawen Niu,Yaofeng Tu,Bin Cui*

Main category: cs.DC

TL;DR: LobRA框架通过异构资源配置和负载均衡调度，解决了多任务LoRA联合微调中的序列长度变化和偏斜问题，显著提升训练效率


<details>
  <summary>Details</summary>
Motivation: 随着Transformer预训练模型的普及，微调需求激增，需要降低处理成本。LoRA技术允许多任务联合训练，但存在序列长度变化和偏斜导致的效率问题

Method: 1) 部署异构资源配置的FT副本匹配不同工作负载；2) 基于序列长度偏斜进行训练数据调度实现负载均衡

Result: 实验验证LobRA显著减少联合微调所需的GPU时间，降低45.03%-60.67%

Conclusion: LobRA框架有效解决了多任务LoRA联合训练中的异构性问题，大幅提升了训练效率

Abstract: With the breakthrough of Transformer-based pre-trained models, the demand for
fine-tuning (FT) to adapt the base pre-trained models to downstream
applications continues to grow, so it is essential for service providers to
reduce the cost of processing FT requests. Low-rank adaption (LoRA) is a widely
used FT technique that only trains small-scale adapters and keeps the base
model unaltered, conveying the possibility of processing multiple FT tasks by
jointly training different LoRA adapters with a shared base model.
  Nevertheless, through in-depth analysis, we reveal the efficiency of joint FT
is dampened by two heterogeneity issues in the training data -- the sequence
length variation and skewness. To tackle these issues, we develop LobRA, a
brand new framework that supports processing multiple FT tasks by jointly
training LoRA adapters. Two innovative designs are introduced. Firstly, LobRA
deploys the FT replicas (i.e., model replicas for FT) with heterogeneous
resource usages and parallel configurations, matching the diverse workloads
caused by the sequence length variation. Secondly, for each training step,
LobRA takes account of the sequence length skewness and dispatches the training
data among the heterogeneous FT replicas to achieve workload balance. We
conduct experiments to assess the performance of LobRA, validating that it
significantly reduces the GPU seconds required for joint FT by 45.03%-60.67%.

</details>


### [16] [LiquidGEMM: Hardware-Efficient W4A8 GEMM Kernel for High-Performance LLM Serving](https://arxiv.org/abs/2509.01229)
*Huanqi Hu,Bowen Xiao,Shixuan Sun,Jianian Yin,Zhexi Zhang,Xiang Luo,Chengquan Jiang,Weiqi Xu,Xiaoying Jia,Xin Liu,Minyi Guo*

Main category: cs.DC

TL;DR: LiquidGEMM是一个硬件高效的W4A8 GEMM内核，通过LiquidQuant量化方法和隐式细粒度流水线技术，实现了比现有方案最高2.9倍的加速和4.94倍的系统级端到端加速。


<details>
  <summary>Details</summary>
Motivation: 现有的W4A8 GEMM内核在实践中表现不佳，因为CUDA核心上的低效反量化无法跟上Tensor Core的高吞吐量，需要更高效的量化方案来加速LLM推理。

Method: 提出了LiquidGEMM内核，包含两个关键技术：1) LiquidQuant硬件高效量化方法，每4个元素仅需两条算术指令实现快速、防溢出的反量化；2) 隐式细粒度流水线，在warp组间完全重叠权重加载、反量化和矩阵乘法运算，无需软件同步或冗余内存访问。

Result: 实验结果显示：相比最先进的W4A8内核达到2.90倍加速，端到端系统级加速达4.94倍；相比NVIDIA TensorRT-LLM中的各种量化GEMM内核，性能提升1.12-1.63倍，系统级加速最高1.63倍。

Conclusion: LiquidGEMM通过硬件优化的量化方法和高效的流水线设计，显著提升了W4A8量化在LLM推理中的性能，为高效LLM服务提供了有效的解决方案。

Abstract: Quantization is a critical technique for accelerating LLM inference by
reducing memory footprint and improving computational efficiency. Among various
schemes, 4-bit weight and 8-bit activation quantization (W4A8) offers a strong
balance between accuracy and performance. However, existing W4A8 GEMM kernels
fall short in practice due to inefficient dequantization on CUDA Cores, which
cannot keep pace with the high throughput of Tensor Cores. In this paper, we
present LiquidGEMM, a hardware-efficient W4A8 GEMM kernel for efficient LLM
serving. LiquidGEMM designs two key techniques: LiquidQuant, a
hardware-efficient quantization method that enables fast, overflow-safe
dequantization using just two arithmetic instructions per four elements; and an
implicit fine-grained pipeline that fully overlaps weight loading,
dequantization, and MMA across warp groups without software synchronization or
redundant memory traffic. Experimental results show that LiquidGEMM achieves up
to 2.90x speedup over state-of-the-art W4A8 kernels and up to 4.94x end-to-end
system-level speedup. Compared to various quantized GEMM kernels in NVIDIA
TensorRT-LLM, LiquidGEMM delivers 1.12-1.63x performance gains, and achieves up
to 1.63x system-level speedup.

</details>


### [17] [HiCR, an Abstract Model for Distributed Heterogeneous Programming](https://arxiv.org/abs/2509.01425)
*Sergio Miguel Martin,Luca Terracciano,Kiril Dichev,Noah Baumann,Jiashu Lin,Albert-Jan Yzelman*

Main category: cs.DC

TL;DR: HiCR是一个用于表示分布式异构应用和运行时系统语义的模型，提供硬件拓扑发现、内核执行、内存管理、通信和实例管理等抽象操作，位于分布式异构系统和运行时系统之间的抽象层。


<details>
  <summary>Details</summary>
Motivation: 为了解决分布式异构系统在硬件多样性和编程范式变化时需要进行重大重构的问题，提供一个能够适应当前和未来系统、支持各种并行编程范式的统一抽象模型。

Method: 采用基于插件的方法实现模型组件和操作，处理设备特定的实现细节，定义最小化的抽象操作集而不规定具体实现决策。

Result: 开发了HiCR模型并实现了基于该模型的应用，这些应用能够在多种平台上同等运行，证明了模型的有效性和跨平台兼容性。

Conclusion: HiCR作为运行时支持层抽象，成功提供了硬件无关的编程接口，使分布式异构应用能够在不进行重大重构的情况下适应不同的硬件平台和编程范式。

Abstract: We present HiCR, a model to represent the semantics of distributed
heterogeneous applications and runtime systems. The model describes a minimal
set of abstract operations to enable hardware topology discovery, kernel
execution, memory management, communication, and instance management, without
prescribing any implementation decisions. The goal of the model is to enable
execution in current and future systems without the need for significant
refactoring, while also being able to serve any governing parallel programming
paradigm. In terms of software abstraction, HiCR is naturally located between
distributed heterogeneous systems and runtime systems. We coin the phrase
\emph{Runtime Support Layer} for this level of abstraction. We explain how the
model's components and operations are realized by a plugin-based approach that
takes care of device-specific implementation details, and present examples of
HiCR-based applications that operate equally on a diversity of platforms.

</details>


### [18] [STZ: A High Quality and High Speed Streaming Lossy Compression Framework for Scientific Data](https://arxiv.org/abs/2509.01626)
*Daoce Wang,Pascal Grosset,Jesus Pulido,Jiannan Tian,Tushar M. Athawale,Jinda Jia,Baixi Sun,Boyuan Zhang,Sian Jin,Kai Zhao,James Ahrens,Fengguang Song*

Main category: cs.DC

TL;DR: 这篇论文提出了一种新的流式压缩框架，同时支持渐进解压缩和随机访问解压缩，在保持高压缩质量和速度的前提下解决了传统方法的限制。


<details>
  <summary>Details</summary>
Motivation: 渐进解压缩和随机访问解压缩对科学数据的按需访问和灵活分析至关重要，但传统方法会严重降低压缩质量和速度。

Method: 设计了第一个同时支持两种流式特性的压缩框架，采用层次分区策略和层次预测机制来减少分区带来的影响，保持高压缩质量。

Result: 压缩质量可与最先进非流式压缩器SZ3相比，压缩和解压缩速度最高可达SZ3的6.7倍。

Conclusion: 该框架成功解决了渐进解压缩和随机访问解压缩的性能问题，在保持高压缩质量的同时实现了显著的速度提升。

Abstract: Error-bounded lossy compression is one of the most efficient solutions to
reduce the volume of scientific data. For lossy compression, progressive
decompression and random-access decompression are critical features that enable
on-demand data access and flexible analysis workflows. However, these features
can severely degrade compression quality and speed. To address these
limitations, we propose a novel streaming compression framework that supports
both progressive decompression and random-access decompression while
maintaining high compression quality and speed. Our contributions are
three-fold: (1) we design the first compression framework that simultaneously
enables both progressive decompression and random-access decompression; (2) we
introduce a hierarchical partitioning strategy to enable both streaming
features, along with a hierarchical prediction mechanism that mitigates the
impact of partitioning and achieves high compression quality -- even comparable
to state-of-the-art (SOTA) non-streaming compressor SZ3; and (3) our framework
delivers high compression and decompression speed, up to 6.7$\times$ faster
than SZ3.

</details>


### [19] [Optimal Parallel Scheduling under Concave Speedup Functions](https://arxiv.org/abs/2509.01811)
*Chengzhang Li,Peizhong Ju,Atilla Eryilmaz,Ness Shroff*

Main category: cs.DC

TL;DR: 本文解决了并行作业调度中的开放性问题，提出了基于一般凹加速函数的优化调度算法，包括CDR规则、GWF方法和SmartFill算法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代云计算系统中，如何高效调度并行计算资源是一个基础性问题。现有方法heSRPT只能处理特定指数形式的加速函数，而现实工作负载更符合一般凹加速函数，这个问题一直未得到解决。

Method: 首先发现了最优并行调度的基本规则——恒定导数比(CDR)规则，然后提出了广义注水(GWF)方法来计算满足CDR规则的最优分配，最后设计了SmartFill算法来选择性确定哪些作业应该获得资源以及分配多少。

Result: 对于一类正则加速函数，SmartFill能够给出闭式最优解；对于非正则函数，也能以低复杂度高效计算最优解。数值评估显示，SmartFill在各种凹加速函数下都显著优于heSRPT。

Conclusion: 本文解决了并行作业调度中的一般凹加速函数问题，提出的CDR规则、GWF方法和SmartFill算法为实际云计算系统提供了有效的优化调度解决方案。

Abstract: Efficient scheduling of parallel computation resources across multiple jobs
is a fundamental problem in modern cloud/edge computing systems for many
AI-based applications. Allocating more resources to a job accelerates its
completion, but with diminishing returns. Prior work (heSRPT) solved this
problem only for some specific speedup functions with an exponential form,
providing a closed-form solution. However, the general case with arbitrary
concave speedup functions -- which more accurately capture real-world workloads
-- has remained open.
  In this paper, we solve this open problem by developing optimal scheduling
algorithms for parallel jobs under general concave speedup functions. We first
discover a fundamental and broadly-applicable rule for optimal parallel
scheduling, namely the Consistent Derivative Ratio (CDR) Rule, which states
that the ratio of the derivatives of the speedup functions across active jobs
remains constant over time. To efficiently compute the optimal allocations that
satisfy the CDR Rule, we propose the General Water-Filling (GWF) method, a more
general version of classical water-filling in wireless communications.
Combining these insights, we design the SmartFill Algorithm to solve the
general scheduling problem. Unlike heSRPT, which always allocates resources to
all active jobs, SmartFill selectively determines which jobs should receive
resources and how much they should be allocated. For a broad class of so-called
\emph{regular} speedup functions, SmartFill yields closed-form optimal
solutions, while for non-regular functions it efficiently computes the optimum
with low complexity. Numerical evaluations show that SmartFill can
substantially outperform heSRPT across a wide range of concave speedup
functions.

</details>


### [20] [A Continuous Energy Ising Machine Leveraging Difference-of-Convex Programming](https://arxiv.org/abs/2509.01928)
*Debraj Banerjee,Santanu Mahapatra,Kunal Narayan Chaudhury*

Main category: cs.DC

TL;DR: 通过将二进制旋铃放松为连续变量并引入吸引子函数，设计了具有收敛保证的高效Ising求解器，在各种GPU平台上都超过现有求解器。


<details>
  <summary>Details</summary>
Motivation: 现有保温求解器虽然可扩展，但缺乏收敛保证且对冷却调度敏感，需要一种更稳健的方法来解决Ising问题。

Method: 将二进制旋铃放松为连续变量，引入吸引子函数将解引导向二进制配置，形成可表示为凸函数差的哈密顿量，设计每迭代仅需一次矩阵-向量乘法的高效算法。

Result: 在从边缘设备到高性能计算集群的各种GPU平台上实现，在从小规模(10^3旋铃)到超大规模(10^8旋铃)的问题上均一致超过现有求解器。

Conclusion: 该方法提供了一种具有收敛保证、高效且可扩展的Ising问题解决方案，充分利用了GPU平台的计算能力。

Abstract: Many combinatorial optimization problems can be reformulated as the task of
finding the ground state of a physical system, such as the Ising model. Most
existing Ising solvers are inspired by simulated annealing. Although annealing
techniques offer scalability, they lack convergence guarantees and are
sensitive to the cooling schedule. We propose to solve the Ising problem by
relaxing the binary spins to continuous variables and introducing a potential
function (attractor) that steers the solution toward binary spin
configurations. The resulting Hamiltonian can be expressed as a difference of
convex functions, enabling the design of efficient iterative algorithms that
require a single matrix-vector multiplication per iteration and are backed by
convergence guarantees. We implement our Ising solver across a range of GPU
platforms: from edge devices to high-performance computing clusters and
demonstrate that it consistently outperforms existing solvers across problem
sizes ranging from small ($10^3$ spins) to ultra-large ($10^8$ spins).

</details>


### [21] [Fault-Tolerant Decentralized Distributed Asynchronous Federated Learning with Adaptive Termination Detection](https://arxiv.org/abs/2509.02186)
*Phani Sahasra Akkinepally,Manaswini Piduguralla,Sushant Joshi,Sathya Peri,Sandeep Kulkarni*

Main category: cs.DC

TL;DR: 本文提出了一种异步分布式联邦学习方法，通过客户端自主终止技术解决了同步方式的性能瓶颈和故障容锐问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习依赖中央服务器，存在单点故障和性能瓶颈，同步方式又容易受慢速客户端影响。需要一种更灵活、可扩展且故障容锐的异步分布式方案。

Method: 分两个阶段开发：第一阶段构建异步FL框架，允许客户端独立学习和更新；第二阶段增加故障容锐机制，处理客户端故障和消息丢失。核心创新是Client-Confident Convergence和Client-Responsive Termination技术，让客户端自主决定终止时机。

Result: 该方法在异质性环境中实现了更好的可扩展性和响应能力，能够在不可预测条件下保持稳健性能。客户端自主终止技术确保了所有活跃客户端都能有效完成训练。

Conclusion: 这种异步分布式FL方案成功解决了传统方案的性能和故障容锐问题，通过客户端自主终止技术实现了高效、可靠的收敛，适用于异质性网络环境。

Abstract: Federated Learning (FL) facilitates collaborative model training across
distributed clients while ensuring data privacy. Traditionally, FL relies on a
centralized server to coordinate learning, which creates bottlenecks and a
single point of failure. Decentralized FL architectures eliminate the need for
a central server and can operate in either synchronous or asynchronous modes.
Synchronous FL requires all clients to compute updates and wait for one another
before aggregation, guaranteeing consistency but often suffering from delays
due to slower participants. Asynchronous FL addresses this by allowing clients
to update independently, offering better scalability and responsiveness in
heterogeneous environments.
  Our research develops an asynchronous decentralized FL approach in two
progressive phases. (a) In Phase 1, we develop an asynchronous FL framework
that enables clients to learn and update independently, removing the need for
strict synchronization. (b) In Phase 2, we extend this framework with fault
tolerance mechanisms to handle client failures and message drops, ensuring
robust performance even under unpredictable conditions. As a central
contribution, we propose Client-Confident Convergence and Client-Responsive
Termination novel techniques that provide each client with the ability to
autonomously determine appropriate termination points. These methods ensure
that all active clients conclude meaningfully and efficiently, maintaining
reliable convergence despite the challenges of asynchronous communication and
faults.

</details>


### [22] [Near-Optimal Stability for Distributed Transaction Processing in Blockchain Sharding](https://arxiv.org/abs/2509.02421)
*Ramesh Adhikari,Costas Busch,Dariusz R. Kowalski*

Main category: cs.DC

TL;DR: 区块链分片系统中，提出了两种调度器（单领导者和分布式多领导者）来保证系统稳定性，显著提高了事务处理速率的上界，接近最优解。


<details>
  <summary>Details</summary>
Motivation: 解决区块链分片中系统稳定性的关键挑战，即在最坏情况事务模式（包括DoS攻击）下确保事务队列大小和延迟有界。现有调度器的处理速率上界较低，需要改进。

Method: 设计了两种调度器：1）单领导者调度器，通过集中式控制保证稳定性；2）分布式多领导者调度器，利用分片图直径和日志因子实现分布式协调。

Result: 单领导者调度器支持注入速率ρ ≤ max{1/(16k), 1/(16⌈√s⌉)}；分布式调度器支持ρ ≤ 1/(16c₁logDlogs) · max{1/k, 1/⌈√s⌉}，比之前最佳结果有显著提升。

Conclusion: 所提调度器在事务处理速率上界方面接近最优，分布式方案尤其在大规模系统中表现出色，为区块链分片系统提供了有效的稳定性保障方案。

Abstract: In blockchain sharding, $n$ processing nodes are divided into $s$ shards, and
each shard processes transactions in parallel. A key challenge in such a system
is to ensure system stability for any ``tractable'' pattern of generated
transactions; this is modeled by an adversary generating transactions with a
certain rate of at most $\rho$ and burstiness $b$. This model captures
worst-case scenarios and even some attacks on transactions' processing, e.g.,
DoS. A stable system ensures bounded transaction queue sizes and bounded
transaction latency. It is known that the absolute upper bound on the maximum
injection rate for which any scheduler could guarantee bounded queues and
latency of transactions is $\max\left\{ \frac{2}{k+1}, \frac{2}{
\left\lfloor\sqrt{2s}\right\rfloor}\right\}$, where $k$ is the maximum number
of shards that each transaction accesses. Here, we first provide a single
leader scheduler that guarantees stability under injection rate $\rho \leq
\max\left\{ \frac{1}{16k}, \frac{1}{16\lceil \sqrt{s} \rceil}\right\}$.
Moreover, we also give a distributed scheduler with multiple leaders that
guarantees stability under injection rate $\rho \leq \frac{1}{16c_1 \log D \log
s}\max\left\{ \frac{1}{k}, \frac{1}{\lceil \sqrt{s} \rceil} \right\}$, where
$c_1$ is some positive constant and $D$ is the diameter of shard graph $G_s$.
This bound is within a poly-log factor from the optimal injection rate, and
significantly improves the best previous known result for the distributed
setting by Adhikari et al., SPAA 2024.

</details>


### [23] [Efficient Pyramidal Analysis of Gigapixel Images on a Decentralized Modest Computer Cluster](https://arxiv.org/abs/2509.02440)
*Marie Reinbigler,Rishi Sharma,Rafael Pires,Elisabeth Brunet,Anne-Marie Kermarrec,Catalin Fetita*

Main category: cs.DC

TL;DR: PyramidAI是一种分析千兆像素图像的技术，通过渐进式分辨率分析和自适应区域选择，在保持精度的同时将计算量减少2.65倍，使用12个普通计算节点可将分析时间从1小时缩短到几分钟。


<details>
  <summary>Details</summary>
Motivation: 千兆像素图像分析计算需求巨大，需要开发能够降低计算成本的技术，使普通计算机也能进行高效的大规模图像分析。

Method: 采用渐进式分析方法，从低分辨率开始逐步聚焦感兴趣区域进行高分辨率详细检查，研究两种自适应分辨率选择策略，并在Camelyon16生物医学图像数据集上验证。还评估了并行化潜力，通过模拟器选择最佳数据分布和负载均衡算法。

Result: PyramidAI将处理数据量减少高达2.65倍，同时保持识别相关区域的准确性。使用12个普通计算节点时，分析时间从超过1小时缩短到几分钟。

Conclusion: PyramidAI为千兆像素图像分析提供了实用的高效解决方案，通过计算优化和并行化实现了在普通计算机上的可行部署，促进了该技术的民主化应用。

Abstract: Analyzing gigapixel images is recognized as computationally demanding. In
this paper, we introduce PyramidAI, a technique for analyzing gigapixel images
with reduced computational cost. The proposed approach adopts a gradual
analysis of the image, beginning with lower resolutions and progressively
concentrating on regions of interest for detailed examination at higher
resolutions. We investigated two strategies for tuning the accuracy-computation
performance trade-off when implementing the adaptive resolution selection,
validated against the Camelyon16 dataset of biomedical images. Our results
demonstrate that PyramidAI substantially decreases the amount of processed data
required for analysis by up to 2.65x, while preserving the accuracy in
identifying relevant sections on a single computer. To ensure democratization
of gigapixel image analysis, we evaluated the potential to use mainstream
computers to perform the computation by exploiting the parallelism potential of
the approach. Using a simulator, we estimated the best data distribution and
load balancing algorithm according to the number of workers. The selected
algorithms were implemented and highlighted the same conclusions in a
real-world setting. Analysis time is reduced from more than an hour to a few
minutes using 12 modest workers, offering a practical solution for efficient
large-scale image analysis.

</details>


### [24] [An Efficient and Adaptive Watermark Detection System with Tile-based Error Correction](https://arxiv.org/abs/2509.02447)
*Xinrui Zhong,Xinze Feng,Jingwei Zuo,Fanjiang Ye,Yi Mu,Junfeng Guo,Heng Huang,Myungjin Lee,Yuke Wang*

Main category: cs.DC

TL;DR: QRMark是一个高效的自适应端到端图像水印检测方法，结合QR码纠错机制和分块技术，在保持检测准确性和鲁棒性的同时显著提升检测效率


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注检测准确性和鲁棒性，但忽视了在大规模图像集合中水印检测的效率挑战

Method: 结合QR码启发的纠错机制和定制化分块技术，采用Reed-Solomon纠错机制减轻分块带来的精度下降，实现资源感知的流分配策略和基于分块的工作负载交错策略

Result: 端到端评估显示QRMark相比顺序基线平均实现2.43倍的推理加速

Conclusion: QRMark通过算法和系统层面的优化，有效解决了大规模图像水印检测的效率问题，在保持准确性的同时显著提升检测速度

Abstract: Efficient and reliable detection of generated images is critical for the
responsible deployment of generative models. Existing approaches primarily
focus on improving detection accuracy and robustness under various image
transformations and adversarial manipulations, yet they largely overlook the
efficiency challenges of watermark detection across large-scale image
collections. To address this gap, we propose QRMark, an efficient and adaptive
end-to-end method for detecting embedded image watermarks. The core idea of
QRMark is to combine QR Code inspired error correction with tailored tiling
techniques to improve detection efficiency while preserving accuracy and
robustness. At the algorithmic level, QRMark employs a Reed-Solomon error
correction mechanism to mitigate the accuracy degradation introduced by tiling.
At the system level, QRMark implements a resource-aware stream allocation
policy that adaptively assigns more streams to GPU-intensive stages of the
detection pipeline. It further employs a tile-based workload interleaving
strategy to overlap data-loading overhead with computation and schedules
kernels across stages to maximize efficiency. End-to-end evaluations show that
QRMark achieves an average 2.43x inference speedup over the sequential
baseline.

</details>


### [25] [KubeIntellect: A Modular LLM-Orchestrated Agent Framework for End-to-End Kubernetes Management](https://arxiv.org/abs/2509.02449)
*Mohsen Seyedkazemi Ardebili,Andrea Bartolini*

Main category: cs.DC

TL;DR: KubeIntellect是一个基于大语言模型的智能Kubernetes控制系统，支持自然语言交互，能够处理完整的Kubernetes API操作，包括读写、删除、执行等高级功能。


<details>
  <summary>Details</summary>
Motivation: Kubernetes管理复杂且碎片化，管理员需要处理大量API、管理异构工作负载，并使用精确命令和YAML配置，这需要专业知识和上下文理解。

Method: 系统采用模块化代理架构，按功能域划分（如日志、指标、RBAC），由监督器协调用户查询、维护工作流内存、调用可重用工具或通过安全的代码生成代理合成新工具。

Result: 评估结果显示93%的工具合成成功率和100%的可靠性（基于200个自然语言查询），系统能够在多样化工作负载下高效运行。

Conclusion: 这项工作引入了新一代可解释、可扩展的LLM驱动系统，用于管理复杂基础设施，提供了自动化的演示环境和本地测试支持。

Abstract: Kubernetes has become the foundation of modern cloud-native infrastructure,
yet its management remains complex and fragmented. Administrators must navigate
a vast API surface, manage heterogeneous workloads, and coordinate tasks across
disconnected tools - often requiring precise commands, YAML configuration, and
contextual expertise.
  This paper presents KubeIntellect, a Large Language Model (LLM)-powered
system for intelligent, end-to-end Kubernetes control. Unlike existing tools
that focus on observability or static automation, KubeIntellect supports
natural language interaction across the full spectrum of Kubernetes API
operations, including read, write, delete, exec, access control, lifecycle, and
advanced verbs. The system uses modular agents aligned with functional domains
(e.g., logs, metrics, RBAC), orchestrated by a supervisor that interprets user
queries, maintains workflow memory, invokes reusable tools, or synthesizes new
ones via a secure Code Generator Agent.
  KubeIntellect integrates memory checkpoints, human-in-the-loop clarification,
and dynamic task sequencing into a structured orchestration framework.
Evaluation results show a 93% tool synthesis success rate and 100% reliability
across 200 natural language queries, demonstrating the system's ability to
operate efficiently under diverse workloads. An automated demo environment is
provided on Azure, with additional support for local testing via kind. This
work introduces a new class of interpretable, extensible, and LLM-driven
systems for managing complex infrastructure.

</details>


### [26] [MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to Break the GPU Memory Wall](https://arxiv.org/abs/2509.02480)
*Avinash Maurya,M. Mustafa Rafique,Franck Cappello,Bogdan Nicolae*

Main category: cs.DC

TL;DR: MLP-Offload是一种针对资源受限环境下LLM训练优化的多级多路径卸载引擎，通过缓解I/O瓶颈，在280B参数模型上实现了比现有技术快2.5倍的迭代速度。


<details>
  <summary>Details</summary>
Motivation: 由于LLM模型规模的增长速度远超GPU内存容量，需要将模型参数卸载到主机内存或磁盘。现有卸载技术虽然采用了先进的异步多级读写策略，但在训练关键路径中仍产生显著的I/O开销，导致迭代速度变慢。

Method: 提出了MLP-Offload多级多路径卸载引擎，通过关键观察发现：更新阶段的I/O开销主导迭代时间；第三级远程存储带宽未被充分利用；并发卸载导致的争用放大了I/O瓶颈。基于这些洞察，以缓存高效和并发控制的方式跨多级卸载优化器状态。

Result: 在高达280B参数的模型评估中，MLP-Offload相比最先进的LLM训练运行时实现了2.5倍的迭代加速。

Conclusion: MLP-Offload通过有效缓解I/O瓶颈，显著提升了大规模LLM在资源受限环境下的训练效率，为解决GPU内存限制与模型规模增长之间的矛盾提供了有效解决方案。

Abstract: Training LLMs larger than the aggregated memory of multiple GPUs is
increasingly necessary due to the faster growth of LLM sizes compared to GPU
memory. To this end, multi-tier host memory or disk offloading techniques are
proposed by state of art. Despite advanced asynchronous multi-tier read/write
strategies, such offloading strategies result in significant I/O overheads in
the critical path of training, resulting in slower iterations. To this end, we
propose MLP-Offload, a novel multi-level, multi-path offloading engine
specifically designed for optimizing LLM training on resource-constrained
setups by mitigating I/O bottlenecks. We make several key observations that
drive the design of MLP-Offload, such as I/O overheads during the update
dominate the iteration time; I/O bandwidth of the third-level remote storage
tier remains unutilized; and, contention due to concurrent offloading amplifies
I/O bottlenecks. Driven by these insights, we design and implement MLP-Offload
to offload the optimizer states across multiple tiers in a cache-efficient and
concurrency-controlled fashion to mitigate I/O bottlenecks during the backward
and update phases. Evaluations on models up to 280B parameters shows that
MLP-Offload achieves 2.5$\times$ faster iterations compared to the
state-of-the-art LLM training runtimes.

</details>


### [27] [Energy-Efficient Split Learning for Resource-Constrained Environments: A Smart Farming Solution](https://arxiv.org/abs/2509.02549)
*Keiwan Soltani,Vishesh Kumar Tanwar,Ashish Gupta,Sajal K. Das*

Main category: cs.DC

TL;DR: eEnergy-Split是一个基于分割学习的高效能框架，通过将模型分布在边缘设备和中央服务器之间，在保护数据隐私的同时，相比联邦学习减少高达86%的设备能耗，并提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 解决智能农业系统面临的资源有限、数据隐私需求和农村地区连接性差等挑战，需要一种既能保护隐私又能高效利用能源的协作学习方案。

Method: 采用分割学习(SL)框架，在边缘设备和中央服务器之间分配模型训练；提出最优边缘部署算法和基于TSP精确解的无人机轨迹规划策略，以最小化飞行成本并最大化通信轮次。

Result: 在农业害虫数据集上，相比联邦学习，能耗降低86%，分类准确率提升6.2%(ResNet-18)；整体准确率提升高达17%，无人机能耗显著降低。

Conclusion: 分割学习与能源感知设计相结合，可为资源受限的智能农业环境提供可扩展、保护隐私的解决方案，但其能效优势具有模型依赖性。

Abstract: Smart farming systems encounter significant challenges, including limited
resources, the need for data privacy, and poor connectivity in rural areas. To
address these issues, we present eEnergy-Split, an energy-efficient framework
that utilizes split learning (SL) to enable collaborative model training
without direct data sharing or heavy computation on edge devices. By
distributing the model between edge devices and a central server, eEnergy-Split
reduces on-device energy usage by up to 86 percent compared to federated
learning (FL) while safeguarding data privacy. Moreover, SL improves
classification accuracy by up to 6.2 percent over FL on ResNet-18 and by more
modest amounts on GoogleNet and MobileNetV2. We propose an optimal edge
deployment algorithm and a UAV trajectory planning strategy that solves the
Traveling Salesman Problem (TSP) exactly to minimize flight cost and extend and
maximize communication rounds. Comprehensive evaluations on agricultural pest
datasets reveal that eEnergy-Split lowers UAV energy consumption compared to
baseline methods and boosts overall accuracy by up to 17 percent. Notably, the
energy efficiency of SL is shown to be model-dependent-yielding substantial
savings in lightweight models like MobileNet, while communication and memory
overheads may reduce efficiency gains in deeper networks. These results
highlight the potential of combining SL with energy-aware design to deliver a
scalable, privacy-preserving solution for resource-constrained smart farming
environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [28] [AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame Covisibility Detection](https://arxiv.org/abs/2509.00433)
*Houshu He,Naifeng Jing,Li Jiang,Xiaoyao Liang,Zhuoran Song*

Main category: cs.AR

TL;DR: AGS是一个算法-硬件协同设计框架，通过利用SLAM系统中相邻帧的高相似性来加速3D高斯溅射SLAM系统，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS-SLAM系统由于每帧需要多次训练迭代和大量高斯元素，导致吞吐量不足，需要更高效的解决方案。

Method: 软件层面：提出粗到细的位姿跟踪方法和跨帧共享高斯贡献信息；硬件层面：设计帧共视检测引擎、位姿跟踪引擎和建图引擎，并配备工作负载调度器。

Result: AGS相比移动和高性能GPU以及最先进的3DGS加速器GSCore，分别实现了17.12倍、6.71倍和5.41倍的加速比。

Conclusion: AGS框架通过算法-硬件协同设计，有效利用了SLAM系统中帧间相似性，显著提升了3DGS-SLAM系统的效率。

Abstract: Simultaneous Localization and Mapping (SLAM) is a critical task that enables
autonomous vehicles to construct maps and localize themselves in unknown
environments. Recent breakthroughs combine SLAM with 3D Gaussian Splatting
(3DGS) to achieve exceptional reconstruction fidelity. However, existing
3DGS-SLAM systems provide insufficient throughput due to the need for multiple
training iterations per frame and the vast number of Gaussians.
  In this paper, we propose AGS, an algorithm-hardware co-design framework to
boost the efficiency of 3DGS-SLAM based on the intuition that SLAM systems
process frames in a streaming manner, where adjacent frames exhibit high
similarity that can be utilized for acceleration. On the software level: 1) We
propose a coarse-then-fine-grained pose tracking method with respect to the
robot's movement. 2) We avoid redundant computations of Gaussians by sharing
their contribution information across frames. On the hardware level, we propose
a frame covisibility detection engine to extract intermediate data from the
video CODEC. We also implement a pose tracking engine and a mapping engine with
workload schedulers to efficiently deploy the AGS algorithm. Our evaluation
shows that AGS achieves up to $17.12\times$, $6.71\times$, and $5.41\times$
speedups against the mobile and high-end GPUs, and a state-of-the-art 3DGS
accelerator, GSCore.

</details>


### [29] [Bit Transition Reduction by Data Transmission Ordering in NoC-based DNN Accelerator](https://arxiv.org/abs/2509.00500)
*Yizhi Chen,Jingwei Li,Wenyao Zhu,Zhonghai Lu*

Main category: cs.AR

TL;DR: 提出基于'1'比特计数的排序方法来减少神经网络加速器中片上网络的比特翻转，从而降低链路功耗。通过数学证明和实验验证，该方法在浮点32和定点8数据上分别实现了最高32.01%和40.85%的比特翻转减少。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络(DNN)的普及，基于片上网络(NoC)的DNN加速器越来越受欢迎。为了节省NoC中的链路功耗，许多研究者专注于减少比特翻转(BT)。

Method: 提出'1'比特计数排序方法，包括关联排序和分离排序两种方式来处理权重和输入数据。通过数学证明方法的有效性，并在有无NoC的情况下进行实验评估。

Result: 无NoC时：浮点32数据最高减少20.38% BT，定点8数据最高减少55.71% BT。有NoC时：浮点32数据最高减少32.01% BT，定点8数据最高减少40.85% BT。在不同DNN模型、NoC配置、权重类型和数据精度下均表现良好。

Conclusion: 提出的基于'1'比特计数的排序方法能有效减少DNN加速器中NoC的比特翻转，显著降低链路功耗，适用于各种配置和数据类型。

Abstract: As Deep Neural Networks (DNN) are becoming essential, Network-on-Chip
(NoC)-based DNN accelerators gained increasing popularity. To save link power
in NoC, many researchers focus on reducing the Bit Transition (BT). We propose
'1'-bit count-based ordering method to reduce BT for DNN workloads. We provide
a mathematical proof of the efficacy of proposed ordering. We evaluate our
method through experiments without NoC and with NoC. Without NoC, our proposed
ordering method achieves up to 20.38% BT reduction for floating-point-32 data
and 55.71% for fixed-point-8 data, respectively. We propose two data ordering
methods, affiliated-ordering and separated-ordering to process weight and input
jointly or individually and apply them to run full DNNs in NoC-based DNN
accelerator. We evaluate our approaches under various configurations, including
different DNN models such as LeNet and DarkNet, various NoC sizes with
different numbers of memory controllers, random weights and trained weights,
and different data precision. Our approach efficiently reduces the link power
by achieving up to 32.01% BT reduction for floating-point-32 data and 40.85% BT
reduction for fixed-point-8 data.

</details>


### [30] [Real-Time Piano Note Frequency Detection Using FPGA and FFT Core](https://arxiv.org/abs/2509.00589)
*Shafayet M. Anik,D. G. Perera*

Main category: cs.AR

TL;DR: 使用FPGA实现钢琴音频信号的实时FFT频率分析，相比传统软件DSP方法具有更低延迟和更高计算效率


<details>
  <summary>Details</summary>
Motivation: 传统软件DSP方法在实时音乐频率分析中存在延迟高、计算资源消耗大的问题，而FPGA的并行处理能力可以提供更快速和确定性的分析性能

Method: 采用基于FPGA的实时快速傅里叶变换(FFT)系统来分析数字钢琴的模拟音频信号

Result: FPGA平台能够实现比软件DSP方法更快速、更确定的实时频率分析

Conclusion: FPGA硬件平台在实时音乐信号处理领域具有显著优势，特别适合钢琴等乐器的高性能频率分析应用

Abstract: Real-time frequency analysis of musical instruments, such as the piano, is an
essential feature in areas like electronic tuners, music visualizers, and live
sound monitoring. Traditional methods often rely on software-based digital
signal processing (DSP), which may introduce latency and require significant
computational power. In contrast, hardware platforms such as FPGAs (Field
Programmable Gate Arrays) offer the ability to perform such analyses with
greater speed and determinism due to their parallel processing capabilities.
The primary objective of this project was to analyze analog audio signals from
a digital piano using an FPGA-based real-time Fast Fourier Transform (FFT)
system.

</details>


### [31] [COMET: A Framework for Modeling Compound Operation Dataflows with Explicit Collectives](https://arxiv.org/abs/2509.00599)
*Shubham Negi,Manik Singhal,Aayush Ankit,Sudeep Bhoja,Kaushik Roy*

Main category: cs.AR

TL;DR: COMET是一个针对机器学习加速器的数据流建模和优化框架，专门处理复合操作和集体通信，相比未融合基准实现了1.42-3.46倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习加速器面临新兴DNN模型（如大语言模型、状态空间模型）中复合操作带来的数据流优化挑战，以及分布式计算集群中集体通信成本缺乏明确建模的问题。

Method: 提出COMET框架，引入新颖表示法显式建模空间集群间的集体通信，包含考虑GEMM和非GEMM操作级依赖关系的延迟和能耗成本模型。

Result: 优化的数据流在GEMM-Softmax上实现1.42倍加速，GEMM-LayerNorm上3.46倍加速，自注意力机制上1.82倍加速。

Conclusion: COMET通过集体通信感知建模扩展了映射空间探索能力，显著提升了现代工作负载的性能和能效。

Abstract: Modern machine learning accelerators are designed to efficiently execute deep
neural networks (DNNs) by optimizing data movement, memory hierarchy, and
compute throughput. However, emerging DNN models such as large language models,
state space models increasingly rely on compound operations-structured
compositions of multiple basic operations-which introduce new challenges for
dataflow optimization and minimizing off-chip memory traffic. Moreover, as
model size continues to grow, deployment across spatially distributed compute
clusters becomes essential, requiring frequent and complex collective
communication. Existing dataflow optimization frameworks and performance models
either focus on single operations or lack explicit modeling of collective
communication cost, limiting their applicability to modern workloads.
  To address these limitations, we propose, a framework for modeling and
optimizing dataflow for compound operations on machine learning accelerators.
COMET introduces a novel representation that explicitly models collective
communication across spatial clusters, along with latency and energy cost
models that account for both GEMM and non-GEMM operation level dependencies
within compound operations. We demonstrate COMET's capabilities to analyze and
optimize dataflows for compound operations such as GEMM--Softmax,
GEMM--LayerNorm, and self-attention, across both edge and cloud accelerator
configurations. Our collective-aware modeling enables exploration of a broader
mapping space, leading to improved performance and energy efficiency.
Specifically, our optimized dataflows achieve up to 1.42$\times$ speedup for
GEMM-Softmax, 3.46$\times$ for GEMM-LayerNorm and 1.82$\times$ for
self-attention compared to unfused baselines.

</details>


### [32] [On the Thermal Vulnerability of 3D-Stacked High-Bandwidth Memory Architectures](https://arxiv.org/abs/2509.00633)
*Mehdi Elahi,Mohamed R. Elshamy,Abdel-Hameed A. Badawy,Ahmad Patooghy*

Main category: cs.AR

TL;DR: HBM架构存在热安全漏洞，攻击者可通过相邻内存库注入热脉冲，制造汇聚热浪来延迟受害者应用访问数据，这种攻击模仿合法工作负载难以检测


<details>
  <summary>Details</summary>
Motivation: 3D堆叠高带宽内存(HBM)架构虽然解决了内存墙性能问题，但由于制造过程中的垂直邻接特性，容易受到热漏洞攻击，攻击者可能利用这种邻接性设计热性能降级攻击

Method: 攻击者通过垂直和/或水平相邻的内存库注入短暂而强烈的热脉冲，制造汇聚热波来最大化影响，延迟受害者应用访问其数据/指令。攻击应用不访问越界内存位置，可绕过设计时安全测试和操作系统内存管理策略

Result: 这种攻击能够有效延迟受害者应用的内存访问，由于攻击行为模仿合法工作负载，检测具有挑战性

Conclusion: HBM架构的热邻接特性使其面临新的安全威胁，需要开发新的检测和防御机制来应对这种模仿合法工作负载的热攻击

Abstract: 3D-stacked High Bandwidth Memory (HBM) architectures provide high-performance
memory interactions to address the well-known performance challenge, namely the
memory wall. However, these architectures are susceptible to thermal
vulnerabilities due to the inherent vertical adjacency that occurs during the
manufacturing process of HBM architectures. We anticipate that adversaries may
exploit the intense vertical and lateral adjacency to design and develop
thermal performance degradation attacks on the memory banks that host
data/instructions from victim applications. In such attacks, the adversary
manages to inject short and intense heat pulses from vertically and/or
laterally adjacent memory banks, creating a convergent thermal wave that
maximizes impact and delays the victim application from accessing its
data/instructions. As the attacking application does not access any
out-of-range memory locations, it can bypass both design-time security tests
and the operating system's memory management policies. In other words, since
the attack mimics legitimate workloads, it will be challenging to detect.

</details>


### [33] [Low Power Approximate Multiplier Architecture for Deep Neural Networks](https://arxiv.org/abs/2509.00764)
*Pragun Jaswal,L. Hemanth Krishna,B. Srinivasu*

Main category: cs.AR

TL;DR: 提出一种用于深度神经网络的低功耗近似乘法器架构，通过设计仅产生单一组合错误的4:2压缩器，在保持低错误率的同时显著减少精确压缩器的使用。


<details>
  <summary>Details</summary>
Motivation: 为深度神经网络应用开发低功耗硬件解决方案，在保持计算精度的同时实现显著的能耗节省。

Method: 设计仅引入单一组合错误的4:2压缩器，并将其集成到8x8无符号乘法器中，减少精确压缩器的使用，并在自定义卷积层中应用该乘法器。

Result: 硬件评估显示能耗节省达30.24%，图像去噪任务中PSNR和SSIM指标优于其他近似设计，手写数字识别保持高分类准确率。

Conclusion: 该架构在能效和计算精度之间实现了良好平衡，适用于低功耗AI硬件实现。

Abstract: This paper proposes an low power approximate multiplier architecture for deep
neural network (DNN) applications. A 4:2 compressor, introducing only a single
combination error, is designed and integrated into an 8x8 unsigned multiplier.
This integration significantly reduces the usage of exact compressors while
preserving low error rates. The proposed multiplier is employed within a custom
convolution layer and evaluated on neural network tasks, including image
recognition and denoising. Hardware evaluation demonstrates that the proposed
design achieves up to 30.24% energy savings compared to the best among existing
multipliers. In image denoising, the custom approximate convolution layer
achieves improved Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity
Index Measure (SSIM) compared to other approximate designs. Additionally, when
applied to handwritten digit recognition, the model maintains high
classification accuracy. These results demonstrate that the proposed
architecture offers a favorable balance between energy efficiency and
computational precision, making it suitable for low-power AI hardware
implementations.

</details>


### [34] [Energy Efficient Exact and Approximate Systolic Array Architecture for Matrix Multiplication](https://arxiv.org/abs/2509.00778)
*Pragun Jaswal,L. Hemanth Krishna,B. Srinivasu*

Main category: cs.AR

TL;DR: 提出一种包含精确和近似处理单元的脉动阵列架构，在DCT计算和边缘检测应用中实现显著节能同时保持良好输出质量


<details>
  <summary>Details</summary>
Motivation: 深度神经网络需要高效的矩阵乘法引擎，传统设计能耗较高，需要开发更节能的架构来满足复杂计算需求

Method: 使用能量高效的PPC和NPPC单元设计8位精确和近似处理单元，并集成到8x8脉动阵列中

Result: 相比现有设计节能22%和32%，DCT计算PSNR达38.21dB，边缘检测PSNR达30.45dB

Conclusion: 所提设计在保持竞争力的输出质量同时实现显著能效提升，非常适合容错图像和视觉处理应用

Abstract: Deep Neural Networks (DNNs) require highly efficient matrix multiplication
engines for complex computations. This paper presents a systolic array
architecture incorporating novel exact and approximate processing elements
(PEs), designed using energy-efficient positive partial product and negative
partial product cells, termed as PPC and NPPC, respectively. The proposed 8-bit
exact and approximate PE designs are employed in a 8x8 systolic array, which
achieves a energy savings of 22% and 32%, respectively, compared to the
existing design. To demonstrate their effectiveness, the proposed PEs are
integrated into a systolic array (SA) for Discrete Cosine Transform (DCT)
computation, achieving high output quality with a PSNR of 38.21,dB.
Furthermore, in an edge detection application using convolution, the
approximate PE achieves a PSNR of 30.45,dB. These results highlight the
potential of the proposed design to deliver significant energy efficiency while
maintaining competitive output quality, making it well-suited for
error-resilient image and vision processing applications.

</details>


### [35] [GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing Redundant Sorting while Preserving Rasterization Efficiency](https://arxiv.org/abs/2509.00911)
*Joongho Jo,Jongsun Park*

Main category: cs.AR

TL;DR: GS-TG是一种基于瓦片分组的3D高斯泼溅加速器，通过减少冗余排序操作并保持光栅化效率，实现1.54倍的平均加速效果


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术虽然比NeRF更快，但仍无法满足实时应用的高帧率需求，需要在减少排序冗余和保持光栅化效率之间找到平衡

Method: 提出瓦片分组方法：在排序阶段将小瓦片分组形成大瓦片来共享排序操作，在光栅化阶段使用位掩码标识相关小瓦片，实现排序用大瓦片、光栅化用小瓦片的策略

Result: 实验结果表明GS-TG相比最先进的3D-GS加速器平均加速1.54倍，且无需重新训练或微调

Conclusion: GS-TG成功解决了3D-GS渲染中的关键权衡问题，是一种无损的加速方法，可与现有优化技术无缝集成

Abstract: 3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to
neural radiance fields (NeRF) as it offers high speed as well as high image
quality in novel view synthesis. Despite these advancements, 3D-GS still
struggles to meet the frames per second (FPS) demands of real-time
applications. In this paper, we introduce GS-TG, a tile-grouping-based
accelerator that enhances 3D-GS rendering speed by reducing redundant sorting
operations and preserving rasterization efficiency. GS-TG addresses a critical
trade-off issue in 3D-GS rendering: increasing the tile size effectively
reduces redundant sorting operations, but it concurrently increases unnecessary
rasterization computations. So, during sorting of the proposed approach, GS-TG
groups small tiles (for making large tiles) to share sorting operations across
tiles within each group, significantly reducing redundant computations. During
rasterization, a bitmask assigned to each Gaussian identifies relevant small
tiles, to enable efficient sharing of sorting results. Consequently, GS-TG
enables sorting to be performed as if a large tile size is used by grouping
tiles during the sorting stage, while allowing rasterization to proceed with
the original small tiles by using bitmasks in the rasterization stage. GS-TG is
a lossless method requiring no retraining or fine-tuning and it can be
seamlessly integrated with previous 3D-GS optimization techniques. Experimental
results show that GS-TG achieves an average speed-up of 1.54 times over
state-of-the-art 3D-GS accelerators.

</details>


### [36] [GeneTEK: Low-power, high-performance and scalable genome sequence matching in FPGAs](https://arxiv.org/abs/2509.01020)
*Elena Espinosa,Rubén Rodríguez Álvarez,José Miranda,Rafael Larrosa,Miguel Peón-Quirós,Oscar Plata,David Atienza*

Main category: cs.AR

TL;DR: 这篇论文提出了一种可扩展的FPGA加速器模板GeneTEK，通过高级综合和工作者架构实现Myers算法，在基因组序列比对正对中实现了更高的速度和能消耗效率。


<details>
  <summary>Details</summary>
Motivation: 下一代序列技术产生的巨量基因组数据导致序列比对成为耐时和能耗的关键步骤，需要快速加速解决方案。

Method: 设计了一种可扩展的FPGA加速器模板，使用高级综合技术实现Myers算法，采用工作者基础架构来提高扩展性。

Result: GeneTEK在Xilinx Zynq UltraScale+ FPGA上实现，与领先的CPU和GPU实现相比，速度提高至少19.4%，能耗降低达62倍，并能处理比之前FPGA方案大72%的比对矩阵。

Conclusion: 这些结果确认了FPGA作为能效高的平台，在可扩展的基因组工作负荷处理方面具有巨大潜力。

Abstract: The advent of next-generation sequencing (NGS) has revolutionized genomic
research by enabling high-throughput data generation through parallel
sequencing of a diverse range of organisms at significantly reduced costs. This
breakthrough has unleashed a "Cambrian explosion" in genomic data volume and
diversity. This volume of workloads places genomics among the top four big data
challenges anticipated for this decade. In this context, pairwise sequence
alignment represents a very time- and energy-consuming step in common
bioinformatics pipelines. Speeding up this step requires the implementation of
heuristic approaches, optimized algorithms, and/or hardware acceleration.
  Whereas state-of-the-art CPU and GPU implementations have demonstrated
significant performance gains, recent field programmable gate array (FPGA)
implementations have shown improved energy efficiency. However, the latter
often suffer from limited scalability due to constraints on hardware resources
when aligning longer sequences. In this work, we present a scalable and
flexible FPGA-based accelerator template that implements Myers's algorithm
using high-level synthesis and a worker-based architecture. GeneTEK, an
instance of this accelerator template in a Xilinx Zynq UltraScale+ FPGA,
outperforms state-of-the-art CPU and GPU implementations in both speed and
energy efficiency, while overcoming scalability limitations of current FPGA
approaches. Specifically, GeneTEK achieves at least a 19.4% increase in
execution speed and up to 62x reduction in energy consumption compared to
leading CPU and GPU solutions, while fitting comparison matrices up to 72%
larger compared to previous FPGA solutions. These results reaffirm the
potential of FPGAs as an energy-efficient platform for scalable genomic
workloads.

</details>


### [37] [LinkBo: An Adaptive Single-Wire, Low-Latency, and Fault-Tolerant Communications Interface for Variable-Distance Chip-to-Chip Systems](https://arxiv.org/abs/2509.01339)
*Bochen Ye,Gustavo Naspolini,Kimmo Salo,Manil Dev Gomony*

Main category: cs.AR

TL;DR: LinkBo是一种创新的单线通信协议，相比现有的1-wire和UNI/O协议，在延迟、吞吐量和鲁棒性方面都有显著提升，支持最长15米线缆和最高7.5 Mbps的数据速率。


<details>
  <summary>Details</summary>
Motivation: 现有单线通信协议存在延迟高、吞吐量受限和鲁棒性不足的问题，而嵌入式系统需要更高效的单线通信解决方案来减少引脚数量。

Method: 提出了LinkBo协议及其硬件架构，采用硬件中断机制和错误检测功能，在FPGA平台上进行性能评估。

Result: LinkBo协议高优先级消息传输延迟仅50.4μs，比现有商业方案快20倍和6.3倍，支持300 kbps@15米和7.5 Mbps@11厘米的通信性能。

Conclusion: LinkBo协议为可变距离芯片间通信提供了低延迟、高吞吐量和高鲁棒性的单线通信解决方案，性能显著优于现有技术。

Abstract: Cost-effective embedded systems necessitate utilizing the single-wire
communication protocol for inter-chip communication, thanks to its reduced pin
count in comparison to the multi-wire I2C or SPI protocols. However, current
single-wire protocols suffer from increased latency, restricted throughput, and
lack of robustness. This paper presents LinkBo, an innovative single-wire
protocol that offers reduced latency, enhanced throughput, and greater
robustness with hardware-interrupt for variable-distance inter-chip
communication. The LinkBo protocol-level guarantees that high-priority messages
are delivered with an error detection feature in just 50.4 $\mu$s, surpassing
current commercial options, 1-wire and UNI/O by at least 20X and 6.3X,
respectively. In addition, we present the hardware architecture for this new
protocol and its performance evaluation on a hardware platform consisting of
two FPGAs. Our findings demonstrate that the protocol reliably supports wire
lengths up to 15 meters with a data rate of 300 kbps, while reaching a maximum
data rate of 7.5 Mbps over an 11 cm wire, providing reliable performance for
varying inter-chip communication distances.

</details>


### [38] [Guidance and Control Neural Network Acceleration using Memristors](https://arxiv.org/abs/2509.02369)
*Zacharia A. Rudge,Dario Izzo,Moritz Fieback,Anteneh Gebregiorgis,Said Hamdioui,Dominik Dold*

Main category: cs.AR

TL;DR: 该论文研究使用相变存储器(PCM)和电阻式随机存取存储器(RRAM)忆阻器在太空应用中实现星载内存计算AI加速，特别针对制导控制神经网络(G&CNET)的加速性能进行仿真评估。


<details>
  <summary>Details</summary>
Motivation: 小型卫星和立方体卫星的有限能源预算以及现代芯片的辐射问题限制了AI在星载应用中的发展，需要研究能够满足这些要求同时满足应用计算和性能需求的神经网络加速器。

Method: 使用PCM和RRAM忆阻器对制导控制神经网络进行加速仿真，考虑器件的非理想特性如噪声和电导漂移，并在各种场景下评估性能。

Result: 忆阻器加速器能够学习专家动作，但噪声对准确性的影响仍存在挑战；在性能退化后重新训练能够将性能恢复到标称水平。

Conclusion: 这项研究为未来太空应用中基于忆阻器的AI加速器研究奠定了基础，突出了其潜力以及需要进一步研究的必要性。

Abstract: In recent years, the space community has been exploring the possibilities of
Artificial Intelligence (AI), specifically Artificial Neural Networks (ANNs),
for a variety of on board applications. However, this development is limited by
the restricted energy budget of smallsats and cubesats as well as radiation
concerns plaguing modern chips. This necessitates research into neural network
accelerators capable of meeting these requirements whilst satisfying the
compute and performance needs of the application. This paper explores the use
of Phase-Change Memory (PCM) and Resistive Random-Access Memory (RRAM)
memristors for on-board in-memory computing AI acceleration in space
applications. A guidance and control neural network (G\&CNET) accelerated using
memristors is simulated in a variety of scenarios and with both device types to
evaluate the performance of memristor-based accelerators, considering device
non-idealities such as noise and conductance drift. We show that the memristive
accelerator is able to learn the expert actions, though challenges remain with
the impact of noise on accuracy. We also show that re-training after
degradation is able to restore performance to nominal levels. This study
provides a foundation for future research into memristor-based AI accelerators
for space, highlighting their potential and the need for further investigation.

</details>
