{"id": "2509.16246", "categories": ["cs.PL", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.16246", "abs": "https://arxiv.org/abs/2509.16246", "authors": ["Juxin Niu", "Yuxin Du", "Dan Niu", "Xi Wang", "Zhe Jiang", "Nan Guan"], "title": "VerilogMonkey: Exploring Parallel Scaling for Automated Verilog Code Generation with LLMs", "comment": null, "summary": "We present VerilogMonkey, an empirical study of parallel scaling for the\nunder-explored task of automated Verilog generation. Parallel scaling improves\nLLM performance by sampling many outputs in parallel. Across multiple\nbenchmarks and mainstream LLMs, we find that scaling to hundreds of samples is\ncost-effective in both time and money and, even without any additional\nenhancements such as post-training or agentic methods, surpasses prior results\non LLM-based Verilog generation. We further dissect why parallel scaling\ndelivers these gains and show how output randomness in LLMs affects its\neffectiveness."}
{"id": "2509.16248", "categories": ["cs.PL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.16248", "abs": "https://arxiv.org/abs/2509.16248", "authors": ["Savini Kashmira", "Jayanaka Dantanarayana", "Thamirawaran Sathiyalogeswaran", "Yichao Yuan", "Nishil Talati", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "title": "GraphMend: Code Transformations for Fixing Graph Breaks in PyTorch 2", "comment": null, "summary": "This paper presents GraphMend, a high-level compiler that eliminates FX graph\nbreaks in PyTorch 2 programs. Although PyTorch 2 introduced TorchDynamo and\nTorchInductor to enable just-in-time graph compilation, unresolved dynamic\ncontrol flow and unsupported Python constructs often fragment models into\nmultiple FX graphs. These fragments force frequent fallbacks to eager mode,\nincur costly CPU-to-GPU synchronizations, and reduce optimization\nopportunities. GraphMend addresses this limitation by analyzing and\ntransforming source code before execution. Built on the Jac compilation\nframework, GraphMend introduces two code transformations that remove graph\nbreaks due to dynamic control flow and Python I/O functions. This design allows\nPyTorch's compilation pipeline to capture larger, uninterrupted FX graphs\nwithout requiring manual refactoring by developers. Evaluation across eight\nHugging Face models shows that GraphMend removes all fixable graph breaks due\nto dynamic control flow and Python I/O functions, driving the break count to 0\nin 6 models and reducing it from 5 to 2 in another model. On NVIDIA RTX 3090\nand A40 GPUs, GraphMend achieves up to 75% latency reductions and up to 8%\nhigher end-to-end throughput. These results demonstrate that high-level code\ntransformation is an effective complement to PyTorch's dynamic JIT compilation\npipeline, substantially improving both usability and performance."}
{"id": "2509.17795", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.17795", "abs": "https://arxiv.org/abs/2509.17795", "authors": ["Parosh Aziz Abdulla", "Samuel Grahn", "Bengt Jonsson", "Shankaranarayanan Krishna", "Om Swostik Mishra"], "title": "Efficient Linearizability Monitoring", "comment": null, "summary": "This paper revisits the fundamental problem of monitoring the linearizability\nof concurrent stacks, queues, sets, and multisets. Given a history of a library\nimplementing one of these abstract data types, the monitoring problem is to\nanswer whether the given history is linearizable. For stacks, queues, and\n(multi)sets, we present monitoring algorithms with complexities\n$\\mathcal{O}(n^2)$, $\\mathcal{O}(n\\; log\\, n)$, and $\\mathcal{O}{(n)}$,\nrespectively, where $n$ is the number of operations in the input history. For\nstacks and queues, our results hold under the standard assumption of {\\it\ndata-independence}, i.e., the behavior of the library is not sensitive to the\nactual values stored in the data structure. Past works to solve the same\nproblems have cubic time complexity and (more seriously) have correctness\nissues: they either (i) lack correctness proofs or (ii) the suggested\ncorrectness proofs are erroneous (we present counter-examples), or (iii) have\nincorrect algorithms. Our improved complexity results rely on substantially\ndifferent algorithms for which we provide detailed proofs of correctness. We\nhave implemented our stack and queue algorithms in LiMo (Linearizability\nMonitor). We evaluate LiMo and compare it with the state-of-the-art tool Violin\n-- whose correctness proofs we have found errors in -- which checks for\nlinearizability violations. Our experimental evaluation confirms that LiMo\noutperforms Violin regarding both efficiency and scalability."}
{"id": "2509.16407", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2509.16407", "abs": "https://arxiv.org/abs/2509.16407", "authors": ["Hunter McCoy", "Prashant Pandey"], "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables", "comment": null, "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."}
{"id": "2509.16495", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.16495", "abs": "https://arxiv.org/abs/2509.16495", "authors": ["Mert Hidayetoglu", "Aurick Qiao", "Michael Wyatt", "Jeff Rasley", "Yuxiong He", "Samyam Rajbhandari"], "title": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for Dynamic Workloads", "comment": null, "summary": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads."}
{"id": "2509.17072", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.17072", "abs": "https://arxiv.org/abs/2509.17072", "authors": ["Junyi Wu", "Chao Fang", "Zhongfeng Wang"], "title": "SnipSnap: A Joint Compression Format and Dataflow Co-Optimization Framework for Efficient Sparse LLM Accelerator Design", "comment": "To appear in the 31st Asia and South Pacific Design Automation\n  Conference (ASP-DAC 2026)", "summary": "The growing scale of large language models (LLMs) has intensified demands on\ncomputation and memory, making efficient inference a key challenge. While\nsparsity can reduce these costs, existing design space exploration (DSE)\nframeworks often overlook compression formats, a key factor for leveraging\nsparsity on accelerators. This paper proposes SnipSnap, a joint compression\nformat and dataflow co-optimization framework for efficient sparse LLM\naccelerator design. SnipSnap introduces: (1) a hierarchical compression format\nencoding to expand the design space; (2) an adaptive compression engine for\nselecting formats under diverse sparsity; and (3) a progressive co-search\nworkflow that jointly optimizes dataflow and compression formats. SnipSnap\nachieves 18.24\\% average memory energy savings via format optimization, along\nwith 2248.3$\\times$ and 21.0$\\times$ speedups over Sparseloop and DiMO-Sparse\nframeworks, respectively."}
{"id": "2509.16504", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.16504", "abs": "https://arxiv.org/abs/2509.16504", "authors": ["Dev Gurung", "Shiva Raj Pokhrel"], "title": "sat-QFL: Secure Quantum Federated Learning for Low Orbit Satellites", "comment": null, "summary": "Low Earth orbit (LEO) constellations violate core assumptions of standard\n(quantum) federated learning (FL): client-server connectivity is intermittent,\nparticipation is time varying, and latency budgets are strict. We present\nsat-QFL, a hierarchical, access aware quantum federated learning (QFL)\nframework that partitions satellites into primary (ground connected) and\nsecondary as inter-satellite links (ISL-only) roles, and schedules sequential,\nsimultaneous, or asynchronous edge training aligned with visibility windows.\nFor quantum-resilient confidentiality and integrity, sat-QFL integrates quantum\nkey distribution (QKD) based key establishment with authenticated encryption\nfor model exchange; we also assess teleportation as a feasibility primitive for\nquantum state transfer. Using derived constellation traces and QFL workloads\n(Qiskit), we show that sat-QFL sustains robust aggregation under varying\nparticipation and reduces communication bottlenecks with modest security\noverhead. Our implementation and results are available at\nhttps://github.com/s222416822/satQFL."}
{"id": "2509.17721", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.17721", "abs": "https://arxiv.org/abs/2509.17721", "authors": ["Pierre Boucher", "Victor Fr√©chard", "Diego Ramirez-Cardona", "Claudiane Ouellet-Plamondon"], "title": "Overcoming challenges in bamboo connections: A review of mechanical properties and structural considerations", "comment": null, "summary": "Over the past decades, bamboo has increasingly gained attention as a\nsustainable construction material, through its rapid growth, naturally\noptimized shape, high mechanical properties, and significant environmental\nbenefits. However, despite these advantages, the use of bamboo in its natural\nform for structural applications remains limited, partly due to insufficient\nknowledge of connection behavior, which is crucial for ensuring the long-term\nreliability and performance of bamboo structures. This article provides a\ncomprehensive review of the key factors to consider in the design of structural\nbamboo connections and discusses the existing connection classification methods\nused as guidelines by designers. By synthesizing findings from the literature,\nour research aims to identify the key parameters interacting with the\nconnection design process, focusing on the anatomical, geometric, and\nmechanical properties of bamboo, the mechanical requirements of the structure\ndesign, and the building methods. A critical analysis of Janssen's\nclassification of bamboo connections, based on force transfer modes and later\nrefined by Widyowijatnoko, is presented. Finally, we discuss the identified\nresearch gaps and emphasize the need for integrated design approaches supported\nby guidelines to support the broader adoption of bamboo in construction."}
{"id": "2509.16505", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16505", "abs": "https://arxiv.org/abs/2509.16505", "authors": ["Dev Gurung", "Shiva Raj Pokhrel"], "title": "orb-QFL: Orbital Quantum Federated Learning", "comment": null, "summary": "Recent breakthroughs in quantum computing present transformative\nopportunities for advancing Federated Learning (FL), particularly in\nnon-terrestrial environments characterized by stringent communication and\ncoordination constraints. In this study, we propose orbital QFL, termed\norb-QFL, a novel quantum-assisted Federated Learning framework tailored for Low\nEarth Orbit (LEO) satellite constellations. Distinct from conventional FL\nparadigms, termed orb-QFL operates without centralized servers or global\naggregation mechanisms (e.g., FedAvg), instead leveraging quantum entanglement\nand local quantum processing to facilitate decentralized, inter-satellite\ncollaboration. This design inherently addresses the challenges of orbital\ndynamics, such as intermittent connectivity, high propagation delays, and\ncoverage variability. The framework enables continuous model refinement through\ndirect quantum-based synchronization between neighboring satellites, thereby\nenhancing resilience and preserving data locality. To validate our approach, we\nintegrate the Qiskit quantum machine learning toolkit with Poliastro-based\norbital simulations and conduct experiments using Statlog dataset."}
{"id": "2509.17731", "categories": ["cs.AR", "cs.NE", "B.7.1; I.2.0"], "pdf": "https://arxiv.org/pdf/2509.17731", "abs": "https://arxiv.org/abs/2509.17731", "authors": ["Amr Nabil", "T. Nandha Kumar", "Haider Abbas F. Almurib"], "title": "Minimal Neuron Circuits: Bursters", "comment": "11 pages, 15 figures, 1 table", "summary": "This work introduces a novel methodology for designing biologically plausible\nbursting neuron circuits using a minimal number of components. We hypothesize\nthat to design circuits capable of bursting, the neuron circuit design must\nmimic a neuron model that inherently exhibits bursting dynamics. Consequently,\nclassical models such as the Hodgkin-Huxley, $I_{Na,p}+I_{K}$, and\nFitzHugh-Nagumo models are not suitable choices. Instead, we propose a\nmethodology for designing neuron circuits that emulate the qualitative\ncharacteristics of the $I_{Na,p}+I_{K}+I_{K(M)}$ model, a well-established\nminimal bursting neuron model. Based on this methodology, we present two novel\nMOSFET-based circuits that exhibit bursting. Using the method of dissection of\nneural bursting, we demonstrate that the nullcline and bifurcation diagrams of\nthe fast subsystem in our circuits are qualitatively equivalent to those of the\n$I_{Na,p}+I_{K}+I_{K(M)}$ model. Furthermore, we examine the effect of the type\nof bifurcation at burst initiation and termination on the bursting\ncharacteristics, showing that our circuits can exhibit diverse bursting\nbehaviours. Importantly, the main contribution of this work lies not in the\nspecific circuit implementation, but in the methodology proposed for\nconstructing bursting neuron circuits."}
{"id": "2509.16513", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.16513", "abs": "https://arxiv.org/abs/2509.16513", "authors": ["Wesley Brewer", "Matthias Maiterth", "Damien Fay"], "title": "Trace Replay Simulation of MIT SuperCloud for Studying Optimal Sustainability Policies", "comment": "2 pages, 2 figures", "summary": "The rapid growth of AI supercomputing is creating unprecedented power\ndemands, with next-generation GPU datacenters requiring hundreds of megawatts\nand producing fast, large swings in consumption. To address the resulting\nchallenges for utilities and system operators, we extend ExaDigiT, an\nopen-source digital twin framework for modeling power, cooling, and scheduling\nof supercomputers. Originally developed for replaying traces from\nleadership-class HPC systems, ExaDigiT now incorporates heterogeneity,\nmulti-tenancy, and cloud-scale workloads. In this work, we focus on trace\nreplay and rescheduling of jobs on the MIT SuperCloud TX-GAIA system to enable\nreinforcement learning (RL)-based experimentation with sustainability policies.\nThe RAPS module provides a simulation environment with detailed power and\nperformance statistics, supporting the study of scheduling strategies,\nincentive structures, and hardware/software prototyping. Preliminary RL\nexperiments using Proximal Policy Optimization demonstrate the feasibility of\nlearning energy-aware scheduling decisions, highlighting ExaDigiT's potential\nas a platform for exploring optimal policies to improve throughput, efficiency,\nand sustainability."}
{"id": "2509.16246", "categories": ["cs.PL", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.16246", "abs": "https://arxiv.org/abs/2509.16246", "authors": ["Juxin Niu", "Yuxin Du", "Dan Niu", "Xi Wang", "Zhe Jiang", "Nan Guan"], "title": "VerilogMonkey: Exploring Parallel Scaling for Automated Verilog Code Generation with LLMs", "comment": null, "summary": "We present VerilogMonkey, an empirical study of parallel scaling for the\nunder-explored task of automated Verilog generation. Parallel scaling improves\nLLM performance by sampling many outputs in parallel. Across multiple\nbenchmarks and mainstream LLMs, we find that scaling to hundreds of samples is\ncost-effective in both time and money and, even without any additional\nenhancements such as post-training or agentic methods, surpasses prior results\non LLM-based Verilog generation. We further dissect why parallel scaling\ndelivers these gains and show how output randomness in LLMs affects its\neffectiveness."}
{"id": "2509.16857", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.16857", "abs": "https://arxiv.org/abs/2509.16857", "authors": ["Xingyu Xiang", "Raj Joshi", "Yuhan Liu", "Jiayi Yao", "Chenxingyu Zhao", "Junchen Jiang", "Yang Zhou", "Eddie Kohler", "Minlan Yu"], "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix Caching", "comment": null, "summary": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput."}
{"id": "2509.16995", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.16995", "abs": "https://arxiv.org/abs/2509.16995", "authors": ["Zheming Yang", "Qi Guo", "Yunqing Hu", "Chang Zhao", "Chang Zhang", "Jian Zhao", "Wen Ji"], "title": "MoA-Off: Adaptive Heterogeneous Modality-Aware Offloading with Edge-Cloud Collaboration for Efficient Multimodal LLM Inference", "comment": "5 pages, 4 figures", "summary": "Multimodal large language models (MLLMs) enable powerful cross-modal\ninference but impose significant computational and latency burdens, posing\nsevere challenges for deployment in resource-constrained environments. In this\npaper, we propose MoA-Off, an adaptive heterogeneous modality-aware offloading\nframework with edge-cloud collaboration for efficient MLLM inference. MoA-Off\nintroduces a lightweight heterogeneous modality-aware module that estimates the\ncomplexity of heterogeneous inputs through multi-dimensional feature analysis.\nThen, an adaptive edge-cloud collaborative offloading strategy is proposed that\ndynamically schedules workloads between edge and cloud based on modality-aware\ncomplexity scores and real-time system states. The experimental results\ndemonstrate that MoA-Off can achieve over 30% reduction in latency and 30%-65%\ndecrease in resource overhead while maintaining competitive accuracy compared\nto traditional approaches."}
{"id": "2509.17351", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17351", "abs": "https://arxiv.org/abs/2509.17351", "authors": ["Slava Kitaeff", "Luc Betbeder-Matibet", "Jake Carroll", "Stephen Giugni", "David Abramson", "John Zaitseff", "Sarah Walters", "David Powell", "Chris Bording", "Trung Nguyen", "Angus Macoustra", "Fabien Voisin", "Bowen Chen", "Jarrod Hurley"], "title": "Institutional Research Computing Capabilities in Australia: 2024", "comment": "9 pages in IEEE Proceedings format, International Conference on\n  eScience 2025, Accepted", "summary": "Institutional research computing infrastructure plays a vital role in\nAustralia's research ecosystem, complementing and extending national\nfacilities. This paper analyses research computing capabilities across\nAustralian universities and organisations, showing how institutional systems\nsupport research excellence through local compute resources, specialised\nhardware, and cluster solutions. Our study finds that nearly 112,258 CPU cores\nand 2,241 GPUs serve over 6,000 researchers as essential bridges between\ndesktops and national facilities, enabling workflows from development to\nlarge-scale computations. The estimated replacement value of this\ninfrastructure is $144M AUD. Drawing on detailed data from multiple\ninstitutions, we identify key patterns in deployment, utilisation, and\nstrategic alignment with research priorities. Institutional resources provide\ncritical support for data-intensive projects, facilitate training and\nhigher-degree student research, enable prototyping and development, and ensure\ndata sovereignty compliance when required. The analysis shows how these\nfacilities leverage national investments while addressing institution-specific\nneeds that national systems cannot meet. We present evidence that strategic\ninvestment in institutional capabilities yields significant returns through\ngreater research productivity, enhanced graduate training, and improved\noutcomes. The study offers insights for organisations planning computing\nstrategies and highlights the importance of maintaining robust institutional\nresources alongside national facilities."}
{"id": "2509.17357", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17357", "abs": "https://arxiv.org/abs/2509.17357", "authors": ["Yunzhao Liu", "Qiang Xu", "Y. Charlie Hu"], "title": "Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via Partially Disaggregated Prefill", "comment": null, "summary": "Efficient LLM inference is critical for real-world applications, especially\nwithin heterogeneous GPU clusters commonly found in organizations and\non-premise datacenters as GPU architecture rapidly evolves. Current\ndisaggregated prefill strategies, which separate the prefill and decode stages\nof LLM inference across different GPUs, often suffer from suboptimal\nperformance due to imbalances between GPU capabilities and workload demands. On\nthe other hand, extending conventional data parallelism and pipeline\nparallelism to heterogeneous setups incurs high inference latencies. To address\nthese challenges, we introduce Cronus, a novel LLM inference system designed to\ndynamically balance workloads across heterogeneous GPUs using partially\ndisaggregated prefill. Cronus partitions each prefill stage and executes its\ninitial portion on the low-end GPU, while overlapping the remaining prefill and\ndecode stages of earlier requests on the high-end GPU. Extensive evaluations\nacross various high-end and low-end GPU combinations demonstrate that Cronus\nsignificantly improves the throughput over disaggregated prefill. It also\nreduces TTFT P99 and TBT P99 significantly over DP and PP while maintaining\nsimilar or better throughput."}
{"id": "2509.17360", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17360", "abs": "https://arxiv.org/abs/2509.17360", "authors": ["Chaoyi Ruan", "Chao Bi", "Kaiwen Zheng", "Ziji Shi", "Xinyi Wan", "Jialin Li"], "title": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access", "comment": null, "summary": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads."}
{"id": "2509.17388", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17388", "abs": "https://arxiv.org/abs/2509.17388", "authors": ["Manel Lurbe", "Miguel Avargues", "Salvador Petit", "Maria E. Gomez", "Rui Yang", "Guanhao Wang", "Julio Sahuquillo"], "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory", "comment": null, "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."}
{"id": "2509.17496", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17496", "abs": "https://arxiv.org/abs/2509.17496", "authors": ["Kaiji Yang", "Jingjing Zhang", "Junyao Zheng", "Qiwen Liu", "Weigang Wu", "Jieying Zhou"], "title": "pBeeGees: A Prudent Approach to Certificate-Decoupled BFT Consensus", "comment": "Accepted by the 25th International Conference on Algorithms and\n  Architectures for Parallel Processing (ICA3PP 2025)", "summary": "Pipelined Byzantine Fault Tolerant (BFT) consensus is fundamental to\npermissioned blockchains. However, many existing protocols are limited by the\nrequirement for view-consecutive quorum certificates (QCs). This constraint\nimpairs performance and creates liveness vulnerabilities under adverse network\nconditions. Achieving \"certificate decoupling\"-committing blocks without this\nrequirement-is therefore a key research goal. While the recent BeeGees\nalgorithm achieves this, our work reveals that it suffers from security and\nliveness issues. To address this problem, this paper makes two primary\ncontributions. First, we formally define these flaws as the Invalid Block\nProblem and the Hollow Chain Problem. Second, we propose pBeeGees, a new\nalgorithm that addresses these issues while preserving certificate decoupling\nwith no additional computational overhead. To achieve this, pBeeGees integrates\ntraceback and pre-commit validation to solve the Invalid Block Problem.Further,\nto mitigate the Hollow Chain Problem, we introduce a prudent validation\nmechanism, which prevents unverified branches from growing excessively. To\nsummarize, pBeeGees is the first protocol to simultaneously achieve safety,\nliveness, and certificate decoupling in a pipelined BFT framework. Experiments\nconfirm that our design significantly reduces block commit latency compared to\nclassic algorithms, particularly under frequent stopping faults."}
{"id": "2509.17532", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17532", "abs": "https://arxiv.org/abs/2509.17532", "authors": ["Guanxiong Sun", "Majid Mirmehdi", "Zahraa Abdallah", "Raul Santos-Rodriguez", "Ian Craddock", "Telmo de Menezes e Silva Filho"], "title": "TACTFL: Temporal Contrastive Training for Multi-modal Federated Learning with Similarity-guided Model Aggregation", "comment": null, "summary": "Real-world federated learning faces two key challenges: limited access to\nlabelled data and the presence of heterogeneous multi-modal inputs. This paper\nproposes TACTFL, a unified framework for semi-supervised multi-modal federated\nlearning. TACTFL introduces a modality-agnostic temporal contrastive training\nscheme that conducts representation learning from unlabelled client data by\nleveraging temporal alignment across modalities. However, as clients perform\nself-supervised training on heterogeneous data, local models may diverge\nsemantically. To mitigate this, TACTFL incorporates a similarity-guided model\naggregation strategy that dynamically weights client models based on their\nrepresentational consistency, promoting global alignment. Extensive experiments\nacross diverse benchmarks and modalities, including video, audio, and wearable\nsensors, demonstrate that TACTFL achieves state-of-the-art performance. For\ninstance, on the UCF101 dataset with only 10% labelled data, TACTFL attains\n68.48% top-1 accuracy, significantly outperforming the FedOpt baseline of\n35.35%. Code will be released upon publication."}
{"id": "2509.17542", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17542", "abs": "https://arxiv.org/abs/2509.17542", "authors": ["Xing Chen", "Rong Shi", "Lu Zhao", "Lingbin Wang", "Xiao Jin", "Yueqiang Chen", "Hongfeng Sun"], "title": "Disaggregated Prefill and Decoding Inference System for Large Language Model Serving on Multi-Vendor GPUs", "comment": null, "summary": "LLM-based applications have been widely used in various industries, but with\nthe increasing of models size, an efficient large language model (LLM)\ninference system is an urgent problem to be solved for service providers. Since\nthe inference system is divided into two stage with different characteristics:\nPrefill and Decode, the two stage will interfere with each other during the\ninference process. Toward this end, a P-D disaggregated inference framework is\nproposed by some researchers. Current research is done on homogeneous GPUs, and\nlacks deployment solutions based on business scenarios. Compared with\nhomogeneous GPUs, using heterogeneous GPUs to construct inference systems can\nbetter improve resource utilization and reduce costs. Even if GPUs from\ndifferent vendors are used to build inference systems, on the basis of reducing\ncosts, the resource utilization rate can be improved and the dependence on a\nsingle vendor can be reduced. Therefore, a P-D disaggreagetd inference system\nbased on heterogeneous GPUs is designed, and the heterogeneous compatible\ntransmission module in the system is designed to address heterogeneous GPU data\ncompatibility issues. Then, a joint optimization algorithm of parallel strategy\nand instance number allocation is proposed to obtain the deployment solutions.\nFinally, the experimental results show that the P-D disaggregated inference\nsystem can well solve the hybrid inference problem of heterogeneous GPUs from\ndifferent vendors, and the joint optimization algorithm can obtain the optimal\ndeployment solution."}
{"id": "2509.17771", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17771", "abs": "https://arxiv.org/abs/2509.17771", "authors": ["Christian Cachin", "Jinfeng Dou", "Christian Scheideler", "Philipp Schneider"], "title": "A Lightweight Approach for State Machine Replication", "comment": null, "summary": "We present a lightweight solution for state machine replication with\ncommitment certificates. Specifically, we adapt a simple median rule from the\nstabilizing consensus problem [Doerr11] to operate in a client-server setting\nwhere arbitrary servers may be blocked adaptively based on past system\ninformation. We further extend our protocol by compressing information about\ncommitted commands, thus keeping the protocol lightweight, while still enabling\nclients to easily prove that their commands have indeed been committed on the\nshared state. Our approach guarantees liveness as long as at most a constant\nfraction of servers are blocked, ensures safety under any number of blocked\nservers, and supports fast recovery from massive blocking attacks. In addition\nto offering near-optimal performance in several respects, our method is fully\ndecentralized, unlike other near-optimal solutions that rely on leaders. In\nparticular, our solution is robust against adversaries that target key servers\n(which captures insider-based denial-of-service attacks), whereas leader-based\napproaches fail under such a blocking model."}
{"id": "2509.17863", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17863", "abs": "https://arxiv.org/abs/2509.17863", "authors": ["Ziming Liu", "Boyu Tian", "Guoteng Wang", "Zhen Jiang", "Peng Sun", "Zhenhua Han", "Tian Tang", "Xiaohe Hu", "Yanmin Jia", "Yan Zhang", "He Liu", "Mingjun Zhang", "Yiqi Zhang", "Qiaoling Chen", "Shenggan Cheng", "Mingyu Gao", "Yang You", "Siyuan Feng"], "title": "Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale MoE Serving", "comment": null, "summary": "Mixture-of-Experts (MoE) models challenge serving infrastructures with\ndynamic, sparse expert utilization, causing instability on conventional systems\ndesigned for dense architectures. We propose EaaS, a novel serving system to\nenable efficient, scalable, and robust MoE deployment. Our system disaggregates\nMoE modules into independent, stateless services. This design enables\nfine-grained resource scaling and provides inherent fault tolerance by\ndecoupling compute units. The architecture is powered by a high-performance,\nCPU-free peer-to-peer communication library that ensures minimal overhead and\nhigh throughput. Experiments confirm EaaS's scalability and efficiency,\nachieving performance comparable to monolithic systems while providing robust\nfault tolerance and strong scalability. EaaS incurs less than a 2% throughput\nreduction under simulated hardware failures that would otherwise halt\nmonolithic architectures. It further saves up to 37.5% of computing resources\nthrough dynamic fine-grained adaptation to serving traffic, demonstrating\nstrong resilience for large-scale MoE deployment in production."}
{"id": "2509.17914", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.17914", "abs": "https://arxiv.org/abs/2509.17914", "authors": ["Marcin Copik", "Eiman Alnuaimi", "Alok Kamatar", "Valerie Hayot-Sasson", "Alberto Madonna", "Todd Gamblin", "Kyle Chard", "Ian Foster", "Torsten Hoefler"], "title": "XaaS Containers: Performance-Portable Representation With Source and IR Containers", "comment": "Accepted at the International Conference for High Performance\n  Computing, Networking, Storage and Analysis (SC'25)", "summary": "High-performance computing (HPC) systems and cloud data centers are\nconverging, and containers are becoming the default method of portable software\ndeployment. Yet, while containers simplify software management, they face\nsignificant performance challenges in HPC environments as they must sacrifice\nhardware-specific optimizations to achieve portability. Although HPC containers\ncan use runtime hooks to access optimized MPI libraries and GPU devices, they\nare limited by application binary interface (ABI) compatibility and cannot\novercome the effects of early-stage compilation decisions. Acceleration as a\nService (XaaS) proposes a vision of performance-portable containers, where a\ncontainerized application should achieve peak performance across all HPC\nsystems. We present a practical realization of this vision through Source and\nIntermediate Representation (IR) containers, where we delay\nperformance-critical decisions until the target system specification is known.\nWe analyze specialization mechanisms in HPC software and propose a new\nLLM-assisted method for automatic discovery of specializations. By examining\nthe compilation pipeline, we develop a methodology to build containers\noptimized for target architectures at deployment time. Our prototype\ndemonstrates that new XaaS containers combine the convenience of\ncontainerization with the performance benefits of system-specialized builds."}
