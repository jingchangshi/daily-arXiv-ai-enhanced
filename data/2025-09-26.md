<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications](https://arxiv.org/abs/2509.20426)
*Mahmoud Samir Fayed*

Main category: cs.PL

TL;DR: 该论文介绍了PWCT2的开发，这是一个双语言（阿拉伯语/英语）、通用、自托管的可视化编程语言。通过先设计Ring文本编程语言，然后用PWCT开发Ring编译器，最终用Ring开发出PWCT2，实现了可视化编程语言的自托管开发。


<details>
  <summary>Details</summary>
Motivation: 大多数可视化编程语言都是领域特定的，通用VPL如PWCT需要文本编程来改进。作者希望创建一个能够自我开发的可视化编程语言，提高抽象级别并隐藏不必要的细节。

Method: 1. 先设计Ring文本编程语言（动态类型、轻量级实现、支持语法定制）
2. 使用PWCT可视化语言开发Ring编译器和虚拟机（18,945个组件生成24,743行C代码）
3. 基于Ring开发PWCT2可视化编程语言（92,000行Ring代码，394个可视化组件）

Result: PWCT2实现了36倍更快的代码生成速度和20倍更小的可视化源文件存储需求。支持将Ring代码转换为可视化代码，实现了自托管VPL。通过Steam平台分发给1,772名用户，总使用时间超过17,000小时。

Conclusion: PWCT2成功证明了可视化编程语言可以实现自托管开发，通过双语言支持和改进的性能指标，为可视化编程语言的进一步发展提供了可行路径。

Abstract: Most visual programming languages (VPLs) are domain-specific, with few
general-purpose VPLs like Programming Without Coding Technology (PWCT). These
general-purpose VPLs are developed using textual programming languages and
improving them requires textual programming. In this thesis, we designed and
developed PWCT2, a dual-language (Arabic/English), general-purpose,
self-hosting visual programming language. Before doing so, we specifically
designed a textual programming language called Ring for its development. Ring
is a dynamically typed language with a lightweight implementation, offering
syntax customization features. It permits the creation of domain-specific
languages through new features that extend object-oriented programming,
allowing for specialized languages resembling Cascading Style Sheets (CSS) or
Supernova language. The Ring Compiler and Virtual Machine are designed using
the PWCT visual programming language where the visual implementation is
composed of 18,945 components that generate 24,743 lines of C code, which
increases the abstraction level and hides unnecessary details. Using PWCT to
develop Ring allowed us to realize several issues in PWCT, which led to the
development of the PWCT2 visual programming language using the Ring textual
programming language. PWCT2 provides approximately 36 times faster code
generation and requires 20 times less storage for visual source files. It also
allows for the conversion of Ring code into visual code, enabling the creation
of a self-hosting VPL that can be developed using itself. PWCT2 consists of
approximately 92,000 lines of Ring code and comes with 394 visual components.
PWCT2 is distributed to many users through the Steam platform and has received
positive feedback, On Steam, 1772 users have launched the software, and the
total recorded usage time exceeds 17,000 hours, encouraging further research
and development.

</details>


### [2] [Efficient Symbolic Computation vis Hash Consing](https://arxiv.org/abs/2509.20534)
*Bowen Zhu,Aayush Sabharwal,Songchen Tan,Yingbo Ma,Alan Edelman,Christopher Rackauckas*

Main category: cs.PL

TL;DR: 本文将哈希一致性技术集成到JuliaSymbolics中，通过全局弱引用哈希表消除重复子表达式，显著提升符号计算性能和内存效率。


<details>
  <summary>Details</summary>
Motivation: 符号计算系统存在内存效率低下的问题，结构相同的子表达式冗余存储导致表达式膨胀，影响经典计算机代数和新兴AI驱动数学推理工具的性能。

Method: 在JuliaSymbolics中集成哈希一致性技术，使用全局弱引用哈希表对表达式进行规范化处理，消除重复存储。

Result: 基准测试显示显著改进：符号计算加速达3.2倍，内存使用减少达2倍，代码生成快5倍，函数编译快10倍，数值评估快100倍。

Conclusion: 哈希一致性对于扩展符号计算至关重要，为未来将哈希一致性与e-graphs集成以实现AI驱动管道中增强的等价感知表达式共享铺平了道路。

Abstract: Symbolic computation systems suffer from memory inefficiencies due to
redundant storage of structurally identical subexpressions, commonly known as
expression swell, which degrades performance in both classical computer algebra
and emerging AI-driven mathematical reasoning tools. In this paper, we present
the first integration of hash consing into JuliaSymbolics, a high-performance
symbolic toolkit in Julia, by employing a global weak-reference hash table that
canonicalizes expressions and eliminates duplication. This approach reduces
memory consumption and accelerates key operations such as differentiation,
simplification, and code generation, while seamlessly integrating with Julia's
metaprogramming and just-in-time compilation infrastructure. Benchmark
evaluations across different computational domains reveal substantial
improvements: symbolic computations are accelerated by up to 3.2 times, memory
usage is reduced by up to 2 times, code generation is up to 5 times faster,
function compilation up to 10 times faster, and numerical evaluation up to 100
times faster for larger models. While certain workloads with fewer duplicate
unknown-variable expressions show more modest gains or even slight overhead in
initial computation stages, downstream processing consistently benefits
significantly. These findings underscore the importance of hash consing in
scaling symbolic computation and pave the way for future work integrating hash
consing with e-graphs for enhanced equivalence-aware expression sharing in
AI-driven pipelines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem](https://arxiv.org/abs/2509.21039)
*William F. Godoy,Tatiana Melnichenko,Pedro Valero-Lara,Wael Elwasif,Philip Fackler,Rafael Ferreira Da Silva,Keita Teranishi,Jeffrey S. Vetter*

Main category: cs.DC

TL;DR: 评估Mojo语言在GPU科学计算工作负载中的性能和可移植性，比较其在NVIDIA H100和AMD MI300A GPU上与传统CUDA和HIP的性能表现


<details>
  <summary>Details</summary>
Motivation: Mojo作为首个基于LLVM MLIR编译器基础设施的语言，旨在通过结合Python的互操作性和类CUDA语法来弥合性能和生产力差距，为科学计算提供编译时便携的GPU编程方案

Method: 针对四种科学计算工作负载进行测试：七点模板（内存受限）、BabelStream（内存受限）、miniBUDE（计算受限）和Hartree-Fock（带原子操作的计算受限），在NVIDIA H100和AMD MI300A GPU上与供应商基准进行比较

Result: Mojo在内存受限内核上的性能与CUDA和HIP相当，但在AMD GPU上的原子操作以及AMD和NVIDIA GPU上的快速数学计算受限内核存在性能差距

Conclusion: 虽然学习曲线和编程要求仍然相对底层，但Mojo可以在科学计算与AI融合的碎片化Python生态系统中弥合重要差距

Abstract: We explore the performance and portability of the novel Mojo language for
scientific computing workloads on GPUs. As the first language based on the
LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure,
Mojo aims to close performance and productivity gaps by combining Python's
interoperability and CUDA-like syntax for compile-time portable GPU
programming. We target four scientific workloads: a seven-point stencil
(memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and
Hartree-Fock (compute-bound with atomic operations); and compare their
performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We
show that Mojo's performance is competitive with CUDA and HIP for memory-bound
kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math
compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve
and programming requirements are still fairly low-level, Mojo can close
significant gaps in the fragmented Python ecosystem in the convergence of
scientific computing and AI.

</details>


### [4] [FZModules: A Heterogeneous Computing Framework for Customizable Scientific Data Compression Pipelines](https://arxiv.org/abs/2509.20563)
*Skyler Ruiter,Jiannan Tian,Fengguang Song*

Main category: cs.DC

TL;DR: FZModules是一个异构框架，用于通过高性能模块组装误差有界的自定义压缩流水线，支持快速实验和异步执行，在保持高压缩率的同时实现与融合内核GPU压缩器相当的端到端加速。


<details>
  <summary>Details</summary>
Motivation: 现代科学模拟和仪器生成的数据量超过了内存和存储容量，限制了可扩展性。虽然有损压缩通过控制误差来减少存储占用和提高吞吐量，但最优流水线高度依赖于数据和目标，需要压缩专业知识。现有的GPU压缩器虽然提供高吞吐量，但通常采用硬编码的融合内核，阻碍了快速实验，且在率失真性能上表现不佳。

Method: 提出FZModules异构框架，通过简洁可扩展的接口从高性能模块组装误差有界的自定义压缩流水线。利用异步任务支持的执行库推断数据依赖关系、管理内存移动，并暴露分支和阶段级并发，实现强大的异步压缩流水线。

Result: 在四个代表性科学数据集上评估三个使用FZModules构建的流水线，结果显示它们可以实现与融合内核GPU压缩器相当的端到端加速，同时达到与更高保真度的CPU或混合压缩器相似的率失真性能。

Conclusion: FZModules框架支持快速、针对特定领域的压缩流水线设计，能够在保持高压缩质量的同时实现高性能压缩。

Abstract: Modern scientific simulations and instruments generate data volumes that
overwhelm memory and storage, throttling scalability. Lossy compression
mitigates this by trading controlled error for reduced footprint and throughput
gains, yet optimal pipelines are highly data and objective specific, demanding
compression expertise. GPU compressors supply raw throughput but often
hard-code fused kernels that hinder rapid experimentation, and underperform in
rate-distortion. We present FZModules, a heterogeneous framework for assembling
error-bounded custom compression pipelines from high-performance modules
through a concise extensible interface. We further utilize an asynchronous
task-backed execution library that infers data dependencies, manages memory
movement, and exposes branch and stage level concurrency for powerful
asynchronous compression pipelines. Evaluating three pipelines built with
FZModules on four representative scientific datasets, we show they can compare
end-to-end speedup of fused-kernel GPU compressors while achieving similar
rate-distortion to higher fidelity CPU or hybrid compressors, enabling rapid,
domain-tailored design.

</details>


### [5] [Experience Deploying Containerized GenAI Services at an HPC Center](https://arxiv.org/abs/2509.20603)
*Angel M. Beltre,Jeff Ogden,Kevin Pedretti*

Main category: cs.DC

TL;DR: 本文分享了在高性能计算中心部署生成式AI工作负载的经验，讨论了HPC与云计算环境的集成，提出了融合计算架构，并通过Llama大语言模型的案例研究展示了跨平台部署实践。


<details>
  <summary>Details</summary>
Motivation: 生成式AI应用通常部署在云环境中，但在高性能计算中心的部署能力仍在发展中。本文旨在探索如何将HPC与云计算环境有效集成，以支持容器化的GenAI工作负载。

Method: 提出融合计算架构，集成HPC和Kubernetes平台运行容器化的GenAI工作负载；通过案例研究部署Llama大语言模型，使用容器化推理服务器在Kubernetes和HPC平台上运行，并测试多种容器运行时。

Result: 成功实现了跨平台的GenAI工作负载部署，验证了融合架构的可行性，并识别了HPC容器社区面临的实际挑战和机遇。

Conclusion: 本文为HPC容器社区提供了实用的部署经验和指导，有助于推动未来相关研究和工具开发，促进HPC环境更好地支持生成式AI应用。

Abstract: Generative Artificial Intelligence (GenAI) applications are built from
specialized components -- inference servers, object storage, vector and graph
databases, and user interfaces -- interconnected via web-based APIs. While
these components are often containerized and deployed in cloud environments,
such capabilities are still emerging at High-Performance Computing (HPC)
centers. In this paper, we share our experience deploying GenAI workloads
within an established HPC center, discussing the integration of HPC and cloud
computing environments. We describe our converged computing architecture that
integrates HPC and Kubernetes platforms running containerized GenAI workloads,
helping with reproducibility. A case study illustrates the deployment of the
Llama Large Language Model (LLM) using a containerized inference server (vLLM)
across both Kubernetes and HPC platforms using multiple container runtimes. Our
experience highlights practical considerations and opportunities for the HPC
container community, guiding future research and tool development.

</details>


### [6] [Distributed-memory Algorithms for Sparse Matrix Permutation, Extraction, and Assignment](https://arxiv.org/abs/2509.20776)
*Elaheh Hassani,Md Taufique Hussain,Ariful Azad*

Main category: cs.DC

TL;DR: 本文提出了可扩展的分布式内存算法，用于稀疏矩阵的置换、提取和赋值操作，采用Identify-Exchange-Build策略减少通信开销，并通过多线程加速本地计算。


<details>
  <summary>Details</summary>
Motivation: 现有分布式内存库如CombBLAS和PETSc在稀疏矩阵操作上存在通信开销大和性能不足的问题，需要更高效的算法来提升大规模稀疏矩阵处理的性能。

Method: 采用Identify-Exchange-Build（IEB）策略：每个进程识别需要发送的本地非零元素，交换所需数据，然后从接收的元素构建本地子矩阵。使用无同步多线程算法加速本地计算。

Result: 在多个集群和超级计算机上的实验表明，该方法在矩阵置换、子图提取和流图应用等场景中，性能显著优于CombBLAS和PETSc。

Conclusion: 该工作为稀疏矩阵置换、提取和赋值操作提供了全面的算法研究、软件实现和实验评估，证明了IEB策略的有效性和性能优势。

Abstract: We present scalable distributed-memory algorithms for sparse matrix
permutation, extraction, and assignment. Our methods follow an
Identify-Exchange-Build (IEB) strategy where each process identifies the local
nonzeros to be sent, exchanges the required data, and then builds its local
submatrix from the received elements. This approach reduces communication
compared to SpGEMM-based methods in distributed memory. By employing
synchronization-free multithreaded algorithms, we further accelerate local
computations, achieving substantially better performance than existing
libraries such as CombBLAS and PETSc. We design efficient software for these
operations and evaluate their performance on two university clusters and the
Perlmutter supercomputer. Our experiments span a variety of application
scenarios, including matrix permutation for load balancing, matrix reordering,
subgraph extraction, and streaming graph applications. In all cases, we compare
our algorithms against CombBLAS, the most comprehensive distributed library for
these operations, and, in some scenarios, against PETSc. Overall, this work
provides a comprehensive study of algorithms, software implementations,
experimental evaluations, and applications for sparse matrix permutation,
extraction, and assignment.

</details>


### [7] [From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient Method for Solving Large-Scale Linear Optimization Problem](https://arxiv.org/abs/2509.21137)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Gozde Tutuncuoglu,Junchi Yang,Feng Qiu,Murat Yildirim*

Main category: cs.DC

TL;DR: 本文提出了一种针对RRAM阵列的分布式内存原始-对偶混合梯度方法，通过算法-硬件协同设计解决大规模优化问题，相比GPU加速求解器实现了三个数量级的能耗和延迟降低。


<details>
  <summary>Details</summary>
Motivation: 传统架构受限于基本物理限制，无法满足计算工作负载的指数增长需求。内存计算虽然提供了低延迟和低能耗的模拟计算，但现有算法无法直接应用于内存计算，特别是在需要频繁矩阵重编程的约束优化问题中。

Method: 开发了分布式内存原始-对偶混合梯度方法，专门为RRAM设备阵列协同设计。该方法最小化昂贵的写周期，包含对设备非理想性的鲁棒性，并利用对称块矩阵公式在分布式交叉阵列上统一操作。使用MELISO+物理模拟框架评估实际设备条件下的性能。

Result: 在大规模线性程序上对比GPU加速求解器，RRAM求解器达到相当精度，同时能耗和延迟降低高达三个数量级。

Conclusion: 这是首个在RRAM上实现的PDHG线性规划求解器，展示了通过分布式内存计算的算法-硬件协同设计解决大规模优化问题的变革性潜力。

Abstract: The exponential growth of computational workloads is surpassing the
capabilities of conventional architectures, which are constrained by
fundamental limits. In-memory computing (IMC) with RRAM provides a promising
alternative by providing analog computations with significant gains in latency
and energy use. However, existing algorithms developed for conventional
architectures do not translate to IMC, particularly for constrained
optimization problems where frequent matrix reprogramming remains
cost-prohibitive for IMC applications. Here we present a distributed in-memory
primal-dual hybrid gradient (PDHG) method, specifically co-designed for arrays
of RRAM devices. Our approach minimizes costly write cycles, incorporates
robustness against device non-idealities, and leverages a symmetric
block-matrix formulation to unify operations across distributed crossbars. We
integrate a physics-based simulation framework called MELISO+ to evaluate
performance under realistic device conditions. Benchmarking against
GPU-accelerated solvers on large-scale linear programs demonstrates that our
RRAM-based solver achieves comparable accuracy with up to three orders of
magnitude reductions in energy consumption and latency. These results
demonstrate the first PDHG-based LP solver implemented on RRAMs, showcasing the
transformative potential of algorithm-hardware co-design for solving
large-scale optimization through distributed in-memory computing.

</details>


### [8] [Integrating and Characterizing HPC Task Runtime Systems for hybrid AI-HPC workloads](https://arxiv.org/abs/2509.20819)
*Andre Merzky,Mikhail Titov,Matteo Turilli,Shantenu Jha*

Main category: cs.DC

TL;DR: 本文研究了RADICAL-Pilot（RP）与Flux和Dragon两种运行时系统的集成性能，展示了在混合AI-HPC工作负载中的高吞吐量优势，相比传统srun/Slurm有显著提升。


<details>
  <summary>Details</summary>
Motivation: 科学工作流日益涉及HPC和机器学习任务的结合，但传统启动器如srun在并发性和吞吐量方面存在限制，不适合动态异构工作负载。

Method: 将RADICAL-Pilot与Flux和Dragon运行时系统集成，实现分层资源管理和高吞吐量函数执行，在Frontier系统上使用合成和生产级工作负载进行性能测试。

Result: RP+Flux可持续达到930任务/秒，RP+Flux+Dragon超过1,500任务/秒，利用率超过99.6%；而srun峰值仅为152任务/秒，利用率低于50%。在IMPECCABLE.v2药物发现应用中，RP+Flux比srun/Slurm减少30-60%的完成时间，吞吐量提高4倍以上。

Conclusion: RP与运行时系统的混合集成为混合AI-HPC工作负载提供了可扩展的解决方案。

Abstract: Scientific workflows increasingly involve both HPC and machine-learning
tasks, combining MPI-based simulations, training, and inference in a single
execution. Launchers such as Slurm's srun constrain concurrency and throughput,
making them unsuitable for dynamic and heterogeneous workloads. We present a
performance study of RADICAL-Pilot (RP) integrated with Flux and Dragon, two
complementary runtime systems that enable hierarchical resource management and
high-throughput function execution. Using synthetic and production-scale
workloads on Frontier, we characterize the task execution properties of RP
across runtime configurations. RP+Flux sustains up to 930 tasks/s, and
RP+Flux+Dragon exceeds 1,500 tasks/s with over 99.6% utilization. In contrast,
srun peaks at 152 tasks/s and degrades with scale, with utilization below 50%.
For IMPECCABLE.v2 drug discovery campaign, RP+Flux reduces makespan by 30-60%
relative to srun/Slurm and increases throughput more than four times on up to
1,024. These results demonstrate hybrid runtime integration in RP as a scalable
approach for hybrid AI-HPC workloads.

</details>


### [9] [RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training](https://arxiv.org/abs/2509.21009)
*Wei Gao,Yuheng Zhao,Dakai An,Tianyuan Wu,Lunxi Cao,Shaopan Xiong,Ju Huang,Weixun Wang,Siran Yang,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: 提出了tail batching策略和RollPacker系统，通过将长尾响应集中在少数rollout步骤中，显著减少GPU空闲时间，实现2.03x-2.56x的端到端训练加速。


<details>
  <summary>Details</summary>
Motivation: 同步强化学习后训练存在GPU利用率低的问题，主要由于rollout步骤中响应长度不平衡导致的bubbles现象，现有方法通过放松同步性会牺牲训练精度。

Method: 引入tail batching策略，将导致长尾响应的prompts集中到少数long rounds中，大部分short rounds只处理平衡的短rollouts；开发RollPacker系统，在三个RL阶段进行整体优化。

Result: 在128个H800 GPU上对Qwen2.5系列LLMs进行测试，相比veRL实现2.03x-2.56x端到端训练时间减少，相比RLHFuse最高达到2.24x加速。

Conclusion: tail batching策略和RollPacker系统能够在不牺牲精度的情况下显著加速RL训练，有效解决了同步RL中的GPU利用率问题。

Abstract: Reinforcement Learning (RL) is a pivotal post-training technique for
enhancing the reasoning capabilities of Large Language Models (LLMs). However,
synchronous RL post-training often suffers from significant GPU
underutilization, referred to as bubbles, caused by imbalanced response lengths
within rollout steps. Many RL systems attempt to alleviate this problem by
relaxing synchronization, but this can compromise training accuracy. In this
paper, we introduce tail batching, a novel rollout scheduling strategy for
synchronous RL that systematically consolidates prompts leading to long-tail
responses into a small subset of rollout steps (long rounds), while ensuring
that the majority of steps (short rounds) involve only balanced, short
rollouts. By excluding long responses from short rounds and rescheduling them
into a few designated long rounds, tail batching effectively reduces GPU idle
time during rollouts and significantly accelerates RL training without
sacrificing accuracy. We present RollPacker, a system that fully harnesses the
benefits of tail batching through holistic optimizations across all three RL
stages: elastic parallelism adaptation for rollout, dynamic resource allocation
and scheduling for reward, and stream-based training. Empirical results show
that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction
compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5
family of LLMs on up to 128 H800 GPUs.

</details>


### [10] [Utilizing Sparsity in the GPU-accelerated Assembly of Schur Complement Matrices in Domain Decomposition Methods](https://arxiv.org/abs/2509.21037)
*Jakub Homola,Ondřej Meca,Lubomír Říha,Tomáš Brzobohatý*

Main category: cs.DC

TL;DR: 本文展示了通过利用输入矩阵的稀疏性来改进GPU上Schur补矩阵的组装，在FETI方法中实现了5.1倍的GPU部分加速和3.3倍的整体组装加速。


<details>
  <summary>Details</summary>
Motivation: 随着高性能集群性能主要依赖于GPU，需要加速域分解方法中的Schur补矩阵计算。显式组装密集Schur补矩阵成本高昂，是GPU加速的主要开销。

Method: 通过智能利用输入矩阵的稀疏性来优化GPU上的Schur补矩阵组装过程，特别针对FETI方法进行优化。

Result: 在FETI方法中实现了GPU部分代码5.1倍的加速，整体组装过程3.3倍的加速，使得从仅10次迭代开始就能获得加速效益。

Conclusion: 利用输入矩阵稀疏性可以显著改进GPU上的Schur补矩阵组装性能，使得GPU加速在较少迭代次数下就具有实际应用价值。

Abstract: Schur complement matrices emerge in many domain decomposition methods that
can solve complex engineering problems using supercomputers. Today, as most of
the high-performance clusters' performance lies in GPUs, these methods should
also be accelerated.
  Typically, the offloaded components are the explicitly assembled dense Schur
complement matrices used later in the iterative solver for multiplication with
a vector. As the explicit assembly is expensive, it represents a significant
overhead associated with this approach to acceleration. It has already been
shown that the overhead can be minimized by assembling the Schur complements
directly on the GPU.
  This paper shows that the GPU assembly can be further improved by wisely
utilizing the sparsity of the input matrices. In the context of FETI methods,
we achieved a speedup of 5.1 in the GPU section of the code and 3.3 for the
whole assembly, making the acceleration beneficial from as few as 10
iterations.

</details>


### [11] [Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training](https://arxiv.org/abs/2509.21275)
*Shiju Wang,Yujie Wang,Ao Sun,Fangcheng Fu,Zijian Zhu,Bin Cui,Xu Han,Kaisheng Ma*

Main category: cs.DC

TL;DR: 提出了弹性流水线并行（EPP）方法，通过协调token级和batch级流水线并行来适应资源和负载异质性，并构建了InfiniPipe系统实现1.69倍的加速。


<details>
  <summary>Details</summary>
Motivation: 长上下文训练对LLM上下文扩展至关重要。现有方案如序列并行通信开销大，流水线并行（PP）效果取决于分区粒度。batch级PP在长上下文场景内存消耗高，token级PP内存开销小但可能导致硬件利用率不足。真实数据集序列长度分布存在偏斜，对PP负载平衡和调度提出挑战。

Method: 提出弹性流水线并行（EPP），构建InfiniPipe系统，包含：（1）资源感知和负载平衡的序列处理器，分割长序列并打包短序列；（2）通过阶段感知块级自适应检查点机制联合优化流水线调度和梯度检查点的协同优化方法。

Result: 综合实验表明，InfiniPipe相比最先进系统实现了1.69倍的加速。

Conclusion: EPP方法能够有效适应资源和负载异质性，InfiniPipe系统通过创新的序列处理和协同优化机制，显著提升了长上下文训练的效率。

Abstract: Long context training is crucial for LLM's context extension. Existing
schemes, such as sequence parallelism, incur substantial communication
overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness
hinges on partitioning granularity. Batch-level PP dividing input samples
exhibits high memory consumption in long-context scenario, whereas token-level
PP splitting sequences into slices alleviates memory overhead but may incur
hardware under-utilization. This trade-off motivates adaptively selecting PP
granularity to match resource and workload characteristics. Moreover, sequence
length distribution of the real-world dataset exhibits skewness, posing a
challenge on PP's workload balance and efficient scheduling. Current static PP
scheduling methods overlook the variance of sequence length, leading to
suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism
(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource
and workload heterogeneity. We build InfiniPipe, a distributed training system
that unleashes the potential of EPP via (1) a resource-aware and
workload-balanced sequence processor that splits long sequences and packs short
ones; and (2) a co-optimization methodology that jointly optimizes pipeline
schedule and gradient checkpointing via a mechanism named stage-aware
chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that
InfiniPipe achieves a 1.69x speedup over state-of-the-art systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [Pedagogically Motivated and Composable Open-Source RISC-V Processors for Computer Science Education](https://arxiv.org/abs/2509.20514)
*Ian McDougall,Harish Batchu,Michael Davies,Karthikeyan Sankaralingam*

Main category: cs.AR

TL;DR: 本文提出了一套评估RISC-V实现生态系统的标准，分析了现有开源实现，并开发了一个满足所有标准的可组合教学框架。


<details>
  <summary>Details</summary>
Motivation: RISC-V ISA作为免费开源替代方案，需要易于使用且稳健的实现来适应教学和业余使用需求。

Method: 首先提出教学视角的评估标准，然后分析现有开源RISC-V实现，最后开发满足所有标准的可组合框架。

Result: 开发了一个全面的开源解决方案，其组件可根据课程需求进行分解，并收集了有限的学⽣反馈。

Conclusion: 成功创建了一个满足教学需求的RISC-V实现框架，为其他教师提供了可用的开源工具。

Abstract: While most instruction set architectures (ISAs) are only available to use
through the purchase of a restrictive commercial license, the RISC-V ISA
presents a free and open-source alternative. Due to this availability, many
free and open-source implementations have been developed and can be accessed on
platforms such as GitHub. If an open source, easy-to-use, and robust RISC-V
implementation could be obtained, it could be easily adapted for pedagogical
and amateur use. In this work we accomplish three goals in relation to this
outlook. First, we propose a set of criteria for evaluating the components of a
RISC-V implementation's ecosystem from a pedagogical perspective. Second, we
analyze a number of existing open-source RISC-V implementations to determine
how many of the criteria they fulfill. We then develop a comprehensive solution
that meets all of these criterion and is released open-source for other
instructors to use. The framework is developed in a composable way that it's
different components can be disaggregated per individual course needs. Finally,
we also report on a limited study of student feedback.

</details>


### [13] [ZynqParrot: A Scale-Down Approach to Cycle-Accurate, FPGA-Accelerated Co-Emulation](https://arxiv.org/abs/2509.20543)
*Daniel Ruelas-Petrisko,Farzam Gilani,Anoop Mysore Nataraja,Zoe Taylor,Michael Taylor*

Main category: cs.AR

TL;DR: 提出了一种Scale-Down建模验证方法，通过将复杂系统分解为可管理的子组件进行独立原型设计，实现快速准确的FPGA加速验证。


<details>
  <summary>Details</summary>
Motivation: 随着处理器复杂性增加，传统验证方法成本急剧上升，性能计数器推断和微架构模拟存在运行时间过长的问题，无法满足长工作负载的验证需求。

Method: 开发了ZynqParrot平台，采用Scale-Down方法将系统分解为子组件，通过精心设计的原型接口确保被测设备的严格非干扰性，实现周期精确的协同仿真。

Result: ZynqParrot能够以任意粒度验证功能和性能，通过案例研究成功分析了开源RISC-V处理器的全栈性能。

Conclusion: Scale-Down方法结合FPGA加速，既保持了Scale-Out的速度优势，又避免了其不准确性，同时降低了Scale-Up的固有成本，为架构师提供了最佳解决方案。

Abstract: As processors increase in complexity, costs grow even more rapidly, both for
functional verification and performance validation. Most often, silicon
characterizations comprise simple performance counters, which are aggregated
and separated to tell a story. Based on these inferences, performance engineers
employ microarchitectural simulation to inspect deeply into the core.
Unfortunately, dramatically longer runtimes make simulation infeasible for long
workloads.
  We propose a Scale-Down approach to modelling and validation. Rather than
up-sizing a prototyping platform to fit large and complex system designs, we
show that it can be more accurate, faster, and more economical to decompose a
system into manageable sub-components that can be prototyped independently. By
carefully designing the prototyping interface, it is possible to adhere to
strict non-interference of the Device Under Test (DUT). This allows architects
to have the best of both worlds: the speed of FPGA acceleration while
eliminating the inaccuracies of Scale-Out and the inherent costs of Scale-Up.
  In this work, we present ZynqParrot: a Scale-Down FPGA-based modelling
platform, capable of executing non-interfering, cycle-accurate co-emulations of
arbitrary RTL designs. ZynqParrot is capable of verifying functionality and
performance with arbitrary granularity. We also provide case studies using
ZynqParrot to analyze the full-stack performance of an open-source RISC-V
processor.

</details>
