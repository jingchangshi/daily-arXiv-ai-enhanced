<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 18]
- [cs.AR](#cs.AR) [Total: 7]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Oriented Metrics for Bottom-Up Enumerative Synthesis](https://arxiv.org/abs/2511.02491)
*Roland Meyer,Jakob Tepe*

Main category: cs.PL

TL;DR: 本文提出了一种称为定向度量的新结构来减少语法引导综合中的搜索空间大小，并在字符串和位向量域中开发了多种定向度量。通过四种技术（修剪、分解、抽象和枚举优化）显著提升了综合性能。


<details>
  <summary>Details</summary>
Motivation: 语法引导综合面临搜索空间过大的挑战，大多数搜索空间不是简单的平面程序集合，而是具有特定结构。作者观察到这些空间可以赋予定向度量结构，用于测量程序间的距离，特别适用于字符串和位向量等非对称操作领域。

Method: 开发了字符串和位向量域的新定向度量；提出了四种搜索空间缩减技术：围绕真实值的球体修剪、定向度量诱导的等价分解、度量抽象与精化、基于抽象信息学习的枚举顺序优化；实现了一个通用的综合算法求解器。

Result: 在字符串和位向量域的实验表明，新方法相比现有最优技术的性能提升超过一个数量级。

Conclusion: 通过理解定向度量的理论基础，可以显著提高语法引导综合技术的适用性和效率，新提出的定向度量框架为搜索空间缩减提供了系统化的解决方案。

Abstract: In syntax-guided synthesis, one of the challenges is to reduce the enormous
size of the search space. We observe that most search spaces are not just flat
sets of programs, but can be endowed with a structure that we call an oriented
metric. Oriented metrics measure the distance between programs, like ordinary
metrics do, but are designed for settings in which operations have an
orientation. Our focus is on the string and the bitvector domains, where
operations like concatenation and bitwise conjunction transform an input into
an output in a way that is not symmetric. We develop several new oriented
metrics for these domains. Oriented metrics are designed for search space
reduction, and we present four techniques: (i) pruning the search space to a
ball around the ground truth, (ii) factorizing the search space by an
equivalence that is induced by the oriented metric, (iii) abstracting the
oriented metric (and hence the equivalence) and refining it, and (iv) improving
the enumeration order by learning from abstract information. We acknowledge
that these techniques are inspired by developments in the literature. By
understanding their roots in oriented metrics, we can substantially increase
their applicability and efficiency. We have integrated these techniques into a
new synthesis algorithm and implemented the algorithm in a new solver. Notably,
our solver is generic in the oriented metric over which it computes. We
conducted experiments in the string and the bitvector domains, and consistently
improve the performance over the state-of-the-art by more than an order of
magnitude.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks](https://arxiv.org/abs/2511.01860)
*Leszek Sliwko*

Main category: cs.DC

TL;DR: 本文对已部署和使用的负载调度器解决方案进行分析，提出了基于架构和设计的层次化分类法，特别关注影响吞吐量和可扩展性的关键设计因素以及架构改进。


<details>
  <summary>Details</summary>
Motivation: 现有分类法存在不足，需要重点关注影响负载调度器吞吐量和可扩展性的关键设计因素，以及架构的渐进式改进。

Method: 分析已部署和使用的负载调度器解决方案，建立基于架构和设计的层次化分类法，特别关注Google Borg等先进系统。

Result: 提出了一个系统化的分类框架，识别了影响调度器性能的关键设计因素，并展示了架构改进的演进路径。

Conclusion: 该分类法为理解负载调度器的设计选择提供了系统化框架，特别强调了Google Borg作为先进系统的设计理念，对后续系统开发具有指导意义。

Abstract: This review analyzes deployed and actively used workload schedulers'
solutions and presents a taxonomy in which those systems are divided into
several hierarchical groups based on their architecture and design. While other
taxonomies do exist, this review has focused on the key design factors that
affect the throughput and scalability of a given solution, as well as the
incremental improvements which bettered such an architecture. This review gives
special attention to Google's Borg, which is one of the most advanced and
published systems of this kind.

</details>


### [3] [Conceptual Design Report for FAIR Computing](https://arxiv.org/abs/2511.01861)
*Johan Messchendorp,Mohammad Al-Turany,Volker Friese,Thorsten Kollegger,Bastian Loeher,Jochen Markert,Andrew Mistry,Thomas Neff,Adrian Oeftiger,Michael Papenbrock,Stephane Pietri,Shahab Sanjari,Tobias Stockmanns*

Main category: cs.DC

TL;DR: 本概念设计报告介绍了德国达姆施塔特FAIR研究设施的计算基础设施计划，涵盖从2028年"首次科学（增强）"阶段到模块化启动版本期间的各项规划。


<details>
  <summary>Details</summary>
Motivation: 为FAIR各研究组提供满足其计算需求的基础设施，应对未来的数据挑战，构建一个能够服务多样化研究线路的联邦式、集中编排的基础设施。

Method: 制定计算和存储基础设施政策，设计FAIR计算模型，包括开放数据、软件和服务政策及架构，采用联邦式集中编排的基础设施架构。

Result: 提出了一个具有足够可扩展性和灵活性的计算基础设施计划，能够应对FAIR未来将面临的数据挑战。

Conclusion: 该计划旨在创建一个联邦式、集中编排的基础设施，服务于FAIR多样化的研究线路，具备应对未来数据挑战所需的可扩展性和灵活性。

Abstract: This Conceptual Design Report (CDR) presents the plans of the computing
infrastructure for research at FAIR, Darmstadt, Germany. It presents the
computing requirements of the various research groups, the policies for the
computing and storage infrastructure, the foreseen FAIR computing model
including the open data, software and services policies and architecture for
the periods starting in 2028 with the "first science (plus)" phase to the
modularized start version of FAIR. The overall ambition is to create a
federated and centrally-orchestrated infrastructure serving the large diversity
of the research lines present with sufficient scalability and flexibility to
cope with future data challenges that will be present at FAIR.

</details>


### [4] [Possible Futures for Cloud Cost Models](https://arxiv.org/abs/2511.01862)
*Vanessa Sochat,Daniel Milroy*

Main category: cs.DC

TL;DR: 云计算已成为AI/ML创新的主导力量，但当前的资源成本模型不适合科学计算需求，可能导致科学工作负载在不适合的环境中运行。


<details>
  <summary>Details</summary>
Motivation: 分析云计算从科学计算驱动转向AI/ML主导的演变，探讨当前资源模型对科学计算的不适应性及其未来影响。

Method: 通过历史回顾和现状分析，讨论云计算成本模型的演变过程及其对科学计算的影响。

Result: 发现AI/ML需求主导的云计算资源模型与科学计算需求不匹配，可能导致科学工作负载在非预期环境中运行。

Conclusion: 需要重新思考云计算成本模型，以确保科学发现和研究能够持续获得适当的计算资源支持。

Abstract: Cloud is now the leading software and computing hardware innovator, and is
changing the landscape of compute to one that is optimized for artificial
intelligence and machine learning (AI/ML). Computing innovation was initially
driven to meet the needs of scientific computing. As industry and consumer
usage of computing proliferated, there was a shift to satisfy a multipolar
customer base. Demand for AI/ML now dominates modern computing and innovation
has centralized on cloud. As a result, cost and resource models designed to
serve AI/ML use cases are not currently well suited for science. If resource
contention resulting from a unipole consumer makes access to contended
resources harder for scientific users, a likely future is running scientific
workloads where they were not intended. In this article, we discuss the past,
current, and possible futures of cloud cost models for the continued support of
discovery and science.

</details>


### [5] [SPHERE: Spherical partitioning for large-scale routing optimization](https://arxiv.org/abs/2511.01863)
*Robert Fabian Lindermann,Paul-Niklas Ken Kandora,Simon Caspar Zeller,Adrian Asmund Fessler,Steffen Rebennack*

Main category: cs.DC

TL;DR: SPHERE是一种源-目标感知的启发式算法，通过识别s-t重叠区域来分割最短路径问题，从而提高大规模加权无向图中的路由效率。


<details>
  <summary>Details</summary>
Motivation: 在大规模加权无向图中，扩展搜索边界会增加精确求解器的时间和内存成本，需要更高效的路径规划方法。

Method: SPHERE算法识别s-t重叠区域（靠近源点和目标点的顶点），选择锚点将问题分割为两个子问题，并在需要时递归处理，无需边界修复即可拼接路径。

Result: 在大型网络上，SPHERE比基于Louvain的路由和基于METIS的流水线方法具有更快的运行时间和更小的最优性差距，即使对于超过百万节点和边的图也优于Dijkstra算法。

Conclusion: SPHERE是一种独立于下游求解器的有效启发式方法，能够在大规模图中实现高效的最短路径计算，并支持子问题间的并行处理。

Abstract: We study shortest-path routing in large weighted, undirected graphs, where
expanding search frontiers raise time and memory costs for exact solvers. We
propose \emph{SPHERE}, a source-target-aware heuristic that identifies an
$s$-$t$ overlap: vertices that are close to both $s$ and $t$ in hop count.
Selecting an anchor $a$ in this overlap partitions the task into two
subproblems with unchanged problem-topology, $s\to a$ and $a\to t$; if either
remains large, the procedure recurses on its induced subgraph. Because the cut
lies inside the overlap, concatenating the resulting subpaths yields a valid
$s\to t$ route without boundary repair. SPHERE is independent of the downstream
solver (e.g., Dijkstra) and exposes parallelism across subproblems. On large
networks, it achieves faster runtimes and smaller optimality gaps than
Louvain-based routing and a METIS-based pipeline, even on graphs with more than
a million nodes and edges, while also outperforming Dijkstra in runtime.

</details>


### [6] [EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs](https://arxiv.org/abs/2511.01866)
*Benjamin Kubwimana,Qijing Huang*

Main category: cs.DC

TL;DR: EdgeReasoning 是一项关于在边缘 GPU 上部署推理型大语言模型的综合研究，系统量化了不同 LLM 架构和模型大小的延迟-准确率权衡，为边缘部署提供优化指导。


<details>
  <summary>Details</summary>
Motivation: 边缘智能范式对新兴自主系统越来越重要，但将大型语言模型部署到边缘 GPU 上面临严格延迟约束和有限计算资源的挑战，缺乏关于设计因素组合的指导。

Method: 系统评估不同 LLM 架构和模型大小的延迟-准确率权衡；评估基于提示和模型调优的技术以减少推理令牌长度；分析不同并行度的测试时缩放方法。

Result: 通过分析绘制了可实现准确率-延迟配置的帕累托前沿，为推理型 LLM 的边缘部署提供系统指导。

Conclusion: EdgeReasoning 为在严格延迟预算下优化边缘部署推理型 LLM 提供了全面的量化分析和配置指导。

Abstract: Edge intelligence paradigm is increasingly demanded by the emerging
autonomous systems, such as robotics. Beyond ensuring privacy-preserving
operation and resilience in connectivity-limited environments, edge deployment
offers significant energy and cost advantages over cloud-based solutions.
However, deploying large language models (LLMs) for reasoning tasks on edge
GPUs faces critical challenges from strict latency constraints and limited
computational resources. To navigate these constraints, developers must balance
multiple design factors - choosing reasoning versus non-reasoning
architectures, selecting appropriate model sizes, allocating token budgets, and
applying test-time scaling strategies - to meet target latency and optimize
accuracy. Yet guidance on optimal combinations of these variables remains
scarce. In this work, we present EdgeReasoning, a comprehensive study
characterizing the deployment of reasoning LLMs on edge GPUs. We systematically
quantify latency-accuracy tradeoffs across various LLM architectures and model
sizes. We systematically evaluate prompt-based and model-tuning-based
techniques for reducing reasoning token length while maintaining performance
quality. We further profile test-time scaling methods with varying degrees of
parallelism to maximize accuracy under strict latency budgets. Through these
analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency
configurations, offering systematic guidance for optimal edge deployment of
reasoning LLMs.

</details>


### [7] [Learned Cost Model for Placement on Reconfigurable Dataflow Hardware](https://arxiv.org/abs/2511.01872)
*Etash Guha,Tianxiao Jiang,Andrew Deng,Jian Zhang,Muthu Annamalai*

Main category: cs.DC

TL;DR: 提出了一种学习方法来预测数据流图在可重构系统上的吞吐量，比传统分析方法准确31%-52%，并能生成5.6%更快的编译图。


<details>
  <summary>Details</summary>
Motivation: 将ML模型的数据流图映射到可重构系统很困难，因为不同映射具有不同的吞吐量并消耗不同资源约束。完全测量吞吐量成本高昂，而传统手工分析模型依赖代理特征或直觉，会引入误差。

Method: 采用学习方法来预测映射的吞吐量，该方法在去除性能注释后仍能保持准确性。

Result: 该方法在各种图上比传统方法准确预测吞吐量31%-52%，使用该方法能生成5.6%更快的编译图。

Conclusion: 学习方法是预测数据流图在可重构系统上吞吐量的有效方法，显著优于传统手工分析模型，并能提升编译性能。

Abstract: Mapping a dataflow-graph of an ML model onto a reconfigurable system is
difficult, as different mappings have different throughputs and consume
resource constraints differently. To solve this, a model to evaluate the
throughput of mappings is necessary as measuring throughput completely is
expensive. Many use a hand-designed analytical model, relying on proxy features
or intuition, introducing error. We provide a Learned Approach that predicts
throughput 31%-52% more accurately over a variety of graphs. In addition, our
approach shows no accuracy degradation after removing performance annotations.
We show that using this approach results in 5.6% faster compiled graphs.

</details>


### [8] [Structural Analysis of Multi-Core Processor and Reliability Evaluation Model](https://arxiv.org/abs/2511.01871)
*S. Tsiramua,H. Meladze,T. Davitashvili,J. M. Sanchez,F. Criado-Aldeanueva*

Main category: cs.DC

TL;DR: 本文开发了多核处理器结构分析和效率指标评估模型，包括可靠性、容错性、生存性和灵活性评估，使用逻辑概率方法分析多功能核心的性能状态。


<details>
  <summary>Details</summary>
Motivation: 研究多核处理器结构分析和效率指标评估，旨在提高多核处理器的可靠性、容错性和性能。

Method: 使用逻辑概率方法开发了可靠性、容错性评估模型，最短路径、灵活性和性能条件模型，以及考虑所有性能状态的多核处理器可靠性、容错性和寿命估计模型。

Result: 提供了双核和四核处理器的结构分析结果，并展示了多核处理器效率指标的增长趋势。

Conclusion: 通过结构分析和效率指标评估，多核处理器的可靠性、容错性和性能得到有效提升，展示了多核处理器的发展潜力。

Abstract: In the present paper, the models of structural analysis and evaluation of
efficiency indicators (reliability, fault tolerance, viability, and
flexibility) of a multi core processor with variable structure, equipped with
multi functional cores, are considered. Using logical probabilistic methods,
the following has been developed: models for evaluating the reliability and
fault tolerance of processor cores as multi functional elements; logical
probabilistic models of the shortest paths, flexibility, and performance
conditions for successful operation of multi core processors based on multi
functional cores; and models for estimating the reliability, fault tolerance,
and lifetime of multi core processors considering all possible states of
performance. The results of the structural analysis of two core and four core
processors and the trends of increasing the efficiency indicators of multi core
processors are presented.

</details>


### [9] [HGraphScale: Hierarchical Graph Learning for Autoscaling Microservice Applications in Container-based Cloud Computing](https://arxiv.org/abs/2511.01881)
*Zhengxin Fang,Hui Ma,Gang Chen,Rajkumar Buyya*

Main category: cs.DC

TL;DR: HGraphScale是一种基于分层图神经网络的微服务自动扩缩容方法，能够有效处理微服务依赖关系和容器云部署方案，在用户请求负载快速变化时做出有效的扩缩容决策。


<details>
  <summary>Details</summary>
Motivation: 微服务架构在容器云中的部署带来了细粒度的弹性资源分配能力，但微服务间的复杂依赖关系和容器云的部署方案给资源扩缩容带来了额外挑战。

Method: 提出HGraphScale方法，通过新设计的分层图神经网络捕获微服务依赖关系和部署方案，为快速变化的用户请求负载做出有效的扩缩容决策。

Result: 基于真实用户请求轨迹的广泛实验表明，HGraphScale在特定VM租赁预算下，最多能减少80.16%的平均响应时间，优于现有最先进的自动扩缩容方法。

Conclusion: HGraphScale通过分层图神经网络有效解决了微服务在容器云中的自动扩缩容问题，显著提升了系统性能。

Abstract: Microservice architecture has become a dominant paradigm in application
development due to its advantages of being lightweight, flexible, and
resilient. Deploying microservice applications in the container-based cloud
enables fine-grained elastic resource allocation. Autoscaling is an effective
approach to dynamically adjust the resource provisioned to containers. However,
the intricate microservice dependencies and the deployment scheme of the
container-based cloud bring extra challenges of resource scaling. This article
proposes a novel autoscaling approach named HGraphScale. In particular,
HGraphScale captures microservice dependencies and the deployment scheme by a
newly designed hierarchical graph neural network, and makes effective scaling
actions for rapidly changing user requests workloads. Extensive experiments
based on real-world traces of user requests are conducted to evaluate the
effectiveness of HGraphScale. The experiment results show that the HGraphScale
outperforms existing state-of-the-art autoscaling approaches by reducing at
most 80.16\% of the average response time under a certain VM rental budget of
application providers.

</details>


### [10] [Roadrunner: Accelerating Data Delivery to WebAssembly-Based Serverless Functions](https://arxiv.org/abs/2511.01888)
*Cynthia Marcelino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Roadrunner是一个sidecar shim，通过零拷贝和免序列化数据传输技术，显著提升WebAssembly无服务器函数的通信性能。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算中函数间数据传输需要序列化/反序列化，导致多副本、上下文切换和内存分配开销，增加延迟和资源消耗。

Method: 使用sidecar shim架构，通过映射函数内存和专用虚拟数据管道，绕过序列化过程，实现近零拷贝数据传输。

Result: 实验显示通信延迟降低44%-89%，97%的数据传输消除了序列化开销，吞吐量提升69倍。

Conclusion: Roadrunner通过近零拷贝技术有效解决了无服务器函数间数据传输的性能瓶颈，实现了接近原生性能的通信效率。

Abstract: Serverless computing provides infrastructure management and elastic
auto-scaling, therefore reducing operational overhead. By design serverless
functions are stateless, which means they typically leverage external remote
services to store and exchange data. Transferring data over a network typically
involves serialization and deserialization. These operations usually require
multiple data copies and transitions between user and kernel space, resulting
in overhead from context switching and memory allocation, contributing
significantly to increased latency and resource consumption. To address these
issues, we present Roadrunner, a sidecar shim that enables near-zero copy and
serialization-free data transfer between WebAssembly-based serverless
functions. Roadrunner reduces the multiple copies between user space and kernel
space by mapping the function memory and moving the data along a dedicated
virtual data hose, bypassing the costly processes of serialization and
deserialization. This approach reduces data movement overhead and context
switching, achieving near-native latency performance for WebAssembly-based
serverless functions. Our experimental results demonstrate that Roadrunner
significantly improves the inter-function communication latency from 44% up to
89%, reducing the serialization overhead in 97% of data transfer, and
increasing throughput by 69 times compared to state-of-the-art
WebAssembly-based serverless functions.

</details>


### [11] [mLR: Scalable Laminography Reconstruction based on Memoization](https://arxiv.org/abs/2511.01893)
*Bin Ma,Viktor Nikitin,Xi Wang,Tekin Bicer,Dong Li*

Main category: cs.DC

TL;DR: mLR通过记忆化技术优化ADMM-FFT算法，用缓存替代重复的FFT计算，显著提升计算性能并降低内存消耗，支持更大规模问题求解。


<details>
  <summary>Details</summary>
Motivation: ADMM-FFT算法在层析成像重建中精度高但计算时间长、内存消耗大，限制了其在大规模问题中的应用。

Method: 利用迭代中FFT操作相似的观察，引入记忆化技术缓存计算结果；采用变量卸载技术节省CPU内存；支持跨GPU和跨节点扩展。

Result: 成功在2Kx2Kx2K规模问题上扩展ADMM-FFT，这是该算法在有限内存下处理的最大规模问题；相比原始ADMM-FFT平均性能提升52.8%，最高达65.4%。

Conclusion: mLRA方法有效解决了ADMM-FFT的计算瓶颈，使其能够处理更大规模的层析成像重建问题，同时显著提升计算效率。

Abstract: ADMM-FFT is an iterative method with high reconstruction accuracy for
laminography but suffers from excessive computation time and large memory
consumption. We introduce mLR, which employs memoization to replace the
time-consuming Fast Fourier Transform (FFT) operations based on an unique
observation that similar FFT operations appear in iterations of ADMM-FFT. We
introduce a series of techniques to make the application of memoization to
ADMM-FFT performance-beneficial and scalable. We also introduce variable
offloading to save CPU memory and scale ADMM-FFT across GPUs within and across
nodes. Using mLR, we are able to scale ADMM-FFT on an input problem of
2Kx2Kx2K, which is the largest input problem laminography reconstruction has
ever worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8%
performance improvement on average (up to 65.4%), compared to the original
ADMM-FFT.

</details>


### [12] [GPoS: Geospatially-aware Proof of Stake](https://arxiv.org/abs/2511.02034)
*Shashank Motepalli,Naman Garg,Gengrui Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: 该论文提出了一种地理空间感知的权益证明机制（GPoS），通过将地理空间多样性与基于权益的投票权相结合，显著提高了PoS区块链的地理空间去中心化程度。


<details>
  <summary>Details</summary>
Motivation: 当前主要PoS区块链存在地理空间集中问题，少数地区主导共识投票权，影响了区块链的监管弹性、鲁棒性和公平性。

Method: 提出GPoS机制，将地理空间多样性与权益证明相结合，在HotStuff和CometBFT等BFT协议中实现地理空间感知的共识。

Result: 实验评估显示，GPoS使地理空间去中心化程度平均提高了45%（基于特征向量中心性的基尼系数），同时对共识性能影响极小。

Conclusion: GPoS能够有效改善PoS区块链的地理空间去中心化，且在实践中仅带来最小的性能开销。

Abstract: Geospatial decentralization is essential for blockchains, ensuring regulatory
resilience, robustness, and fairness. We empirically analyze five major Proof
of Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui,
revealing that a few geographic regions dominate consensus voting power,
resulting in limited geospatial decentralization. To address this, we propose
Geospatially aware Proof of Stake (GPoS), which integrates geospatial diversity
with stake-based voting power. Experimental evaluation demonstrates an average
45% improvement in geospatial decentralization, as measured by the Gini
coefficient of Eigenvector centrality, while incurring minimal performance
overhead in BFT protocols, including HotStuff and CometBFT. These results
demonstrate that GPoS can improve geospatial decentralization {while, in our
experiments, incurring minimal overhead} to consensus performance.

</details>


### [13] [Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs](https://arxiv.org/abs/2511.02168)
*Octavian Alexandru Trifan,Karthik Sangaiah,Muhammad Awad,Muhammad Osama,Sumanth Gudaparthi,Alexandru Nicolau,Alexander Veidenbaum,Ganesh Dasika*

Main category: cs.DC

TL;DR: 提出超越传统BSP模型的分布式GPU执行方法，通过细粒度编程模式消除"三大税"，在LLM分布式工作负载中实现10-20%的端到端延迟加速。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，传统BSP模型在多GPU分布式执行中引入显著性能瓶颈，需要更高效的执行范式。

Method: 利用Iris for Triton等库提供的核内通信原语，设计细粒度编程模式，建立直接的瓦片级生产者-消费者流水线，用细粒度数据流同步替代全局屏障。

Result: 在关键内核（从All-Gather + GEMM操作到复杂Flash Decode算法）上，相比BSP方法实现了10-20%的端到端延迟加速。

Conclusion: 建立了一个更可编程和高效的分布式LLM工作负载执行范式，系统性地消除了BSP模型的性能瓶颈。

Abstract: As large language models (LLMs) continue to scale, their workloads
increasingly rely on distributed execution across multiple GPUs. However, the
conventional bulk synchronous parallel~(BSP) model used in such settings
introduces significant performance inefficiencies. To characterize these
bottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel
Data Locality, and Kernel Launch Overhead) as an analytical framework. We
propose moving beyond the rigid BSP model to address key inefficiencies in
distributed GPU execution. By exploiting libraries like Iris for Triton, we
gain access to in-kernel communication primitives that enable the design of
novel fine-grained programming patterns, offering greater flexibility and
performance than traditional BSP-based approaches. These patterns
systematically eliminate the three taxes by creating direct, tile-level
producer-consumer pipelines and replacing global barriers with fine-grained
dataflow synchronization. Applying this methodology to critical kernels, from
the foundational All-Gather + general matrix multiplication operation to the
complex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end
latency over BSP-based approaches, establishing a more programmable and
efficient paradigm for distributed LLM workloads.

</details>


### [14] [From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models](https://arxiv.org/abs/2511.02248)
*Xingqi Cui,Chieh-Jan Mike Liang,Jiarong Xing,Haoran Qiu*

Main category: cs.DC

TL;DR: 提出了一种基于算子级别的自动扩缩容框架，通过细粒度资源管理优化大模型推理服务，相比传统模型级方法可减少40%GPU和35%能耗


<details>
  <summary>Details</summary>
Motivation: 现有大模型服务方案采用静态资源配置或模型级自动扩缩容，将模型视为整体进行粗粒度管理，导致性能下降或资源利用率低下，无法适应动态推理流量

Method: 通过分析生成式模型内部算子图结构，发现算子在计算和内存占用上具有异质性，提出算子级自动扩缩容框架，基于个体算子特征进行扩缩容、批处理和资源分配优化

Result: 在生产规模测试中，在保持SLO的前提下减少40%GPU和35%能耗，或在固定资源下实现1.6倍吞吐量提升和5%能耗降低

Conclusion: 算子比模型更适合作为大生成式工作负载的扩缩容基本单元，细粒度算子级管理能显著提升资源利用效率

Abstract: Serving large generative models such as LLMs and multi- modal transformers
requires balancing user-facing SLOs (e.g., time-to-first-token,
time-between-tokens) with provider goals of efficiency and cost reduction.
Existing solutions rely on static provisioning or model-level autoscaling, both
of which treat the model as a monolith. This coarse-grained resource management
leads to degraded performance or significant resource underutilization due to
poor adaptability to dynamic inference traffic that is common online.
  The root cause of this inefficiency lies in the internal structure of
generative models: they are executed as graphs of interconnected operators.
Through detailed characterization and systematic analysis, we find that
operators are heterogeneous in their compute and memory footprints and exhibit
diverse sensitivity to workload and resource factors such as batch size,
sequence length, and traffic rate. This heterogeneity suggests that the
operator, rather than the entire model, is the right granularity for scaling
decisions.
  We propose an operator-level autoscaling framework, which allocates resources
at finer (operator)-granularity, optimizing the scaling, batching, and
placement based on individual operator profiles. Evaluated on production-scale
traces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less
energy, or under fixed resources achieves 1.6x higher throughput with 5% less
energy. These results show that the operator, rather than the model, is
fundamentally a more effective unit for scaling large generative workloads.

</details>


### [15] [Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators](https://arxiv.org/abs/2511.02257)
*Oguz Selvitopi,Emin Ozturk,Jie Chen,Ponnuswamy Sadayappan,Robert G. Edwards,Aydın Buluç*

Main category: cs.DC

TL;DR: 提出了两种新的调度算法来优化LQCD模拟中的张量收缩计算，通过增加时间局部性和张量重用，显著降低了峰值内存使用和数据传输量，提升了计算性能。


<details>
  <summary>Details</summary>
Motivation: LQCD模拟中的关联函数计算涉及大量二进制批量张量收缩，每个张量可能占用数百MB内存。在GPU加速器上执行这些收缩时，需要优化调度以提高张量重用并减少数据流量。

Method: 开发了两种快速调度算法，通过重新排序收缩操作来增加时间局部性，利用输入/中间张量重用。算法利用了应用特定特征，如收缩的二元性和收缩树中的局部性，以最小化峰值内存为目标。

Result: 调度器实现了高达2.1倍的峰值内存改进，减少了高达4.2倍的驱逐次数和高达1.8倍的数据流量，使关联函数计算时间加快了高达1.9倍。

Conclusion: 所提出的调度算法成功集成到LQCD分析软件Redstar中，显著改善了求解时间，证明了在优化张量收缩计算调度方面的有效性。

Abstract: Computation of correlation functions is a key operation in Lattice quantum
chromodynamics (LQCD) simulations to extract nuclear physics observables. These
functions involve many binary batch tensor contractions, each tensor possibly
occupying hundreds of MBs of memory. Performing these contractions on GPU
accelerators poses the challenge of scheduling them as to optimize tensor reuse
and reduce data traffic. In this work we propose two fast novel scheduling
algorithms that reorder contractions to increase temporal locality via
input/intermediate tensor reuse. Our schedulers take advantage of
application-specific features, such as contractions being binary and locality
within contraction trees, to optimize the objective of minimizing peak memory.
We integrate them into the LQCD analysis software suite Redstar and improve
time-to-solution. Our schedulers attain upto 2.1x improvement in peak memory,
which is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data
traffic, resulting in upto 1.9x faster correlation function computation time.

</details>


### [16] [3D Point Cloud Object Detection on Edge Devices for Split Computing](https://arxiv.org/abs/2511.02293)
*Taisuke Noguchi,Takuya Azumi*

Main category: cs.DC

TL;DR: 该研究利用分割计算技术来优化自动驾驶中的3D物体检测，通过在点云体素化后或网络内部进行分割，显著降低了边缘设备的推理时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术中的3D物体检测模型复杂，导致边缘设备处理时间长、功耗高，需要解决这些性能瓶颈问题。

Method: 采用分割计算（Split Computing）分布式机器学习推理方法，在点云体素化后或深度神经网络内部进行分割，减轻边缘设备计算负担。

Result: 体素化后分割：推理时间减少70.8%，边缘设备执行时间减少90.0%；网络内部分割：推理时间最多减少57.1%，边缘设备执行时间最多减少69.5%。

Conclusion: 分割计算能有效降低自动驾驶3D物体检测的边缘设备计算负担，显著提升推理效率并降低能耗，同时通过仅传输中间数据增强数据安全性。

Abstract: The field of autonomous driving technology is rapidly advancing, with deep
learning being a key component. Particularly in the field of sensing, 3D point
cloud data collected by LiDAR is utilized to run deep neural network models for
3D object detection. However, these state-of-the-art models are complex,
leading to longer processing times and increased power consumption on edge
devices. The objective of this study is to address these issues by leveraging
Split Computing, a distributed machine learning inference method. Split
Computing aims to lessen the computational burden on edge devices, thereby
reducing processing time and power consumption. Furthermore, it minimizes the
risk of data breaches by only transmitting intermediate data from the deep
neural network model. Experimental results show that splitting after
voxelization reduces the inference time by 70.8% and the edge device execution
time by 90.0%. When splitting within the network, the inference time is reduced
by up to 57.1%, and the edge device execution time is reduced by up to 69.5%.

</details>


### [17] [Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks](https://arxiv.org/abs/2511.02647)
*Xiumei Deng,Zehui Xiong,Binbin Chen,Dong In Kim,Merouane Debbah,H. Vincent Poor*

Main category: cs.DC

TL;DR: FedAttn是一个新的分布式LLM推理框架，通过将联邦学习范式集成到自注意力机制中，在保护隐私的同时实现通信和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在边缘协作部署中的隐私漏洞、通信开销和计算瓶颈等挑战。

Method: 提出联邦注意力机制，让参与者在本地执行自注意力计算，定期交换和聚合KV矩阵，协作生成LLM响应而不暴露私有提示。

Result: 理论分析揭示了错误传播动态和响应质量与通信/计算效率之间的权衡关系，实验验证了理论分析并展示了通过稀疏注意力和自适应KV聚合的优化机会。

Conclusion: FedAttn具有在实际边缘部署中提供可扩展性和效率的潜力，为协作LLM推理提供了原则性基础。

Abstract: Large language models (LLMs) are proliferating rapidly at the edge,
delivering intelligent capabilities across diverse application scenarios.
However, their practical deployment in collaborative scenarios confronts
fundamental challenges: privacy vulnerabilities, communication overhead, and
computational bottlenecks. To address these, we propose Federated Attention
(FedAttn), which integrates the federated paradigm into the self-attention
mechanism, creating a new distributed LLM inference framework that
simultaneously achieves privacy protection, communication efficiency, and
computational efficiency. FedAttn enables participants to perform local
self-attention over their own token representations while periodically
exchanging and aggregating Key-Value (KV) matrices across multiple Transformer
blocks, collaboratively generating LLM responses without exposing private
prompts. Further, we identify a structural duality between contextual
representation refinement in FedAttn and parameter optimization in FL across
private data, local computation, and global aggregation. This key insight
provides a principled foundation for systematically porting federated
optimization techniques to collaborative LLM inference. Building on this
framework, we theoretically analyze how local self-attention computation within
participants and heterogeneous token relevance among participants shape error
propagation dynamics across Transformer blocks. Moreover, we characterize the
fundamental trade-off between response quality and communication/computation
efficiency, which is governed by the synchronization interval and the number of
participants. Experimental results validate our theoretical analysis, and
reveal significant optimization opportunities through sparse attention and
adaptive KV aggregation, highlighting FedAttn's potential to deliver
scalability and efficiency in real-world edge deployments.

</details>


### [18] [Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks](https://arxiv.org/abs/2511.02655)
*Johansell Villalobos,Josef Ruzicka,Silvio Rizzi*

Main category: cs.DC

TL;DR: 本文比较了四种性能可移植性框架（Kokkos、OpenMP、RAJA、OCCA）在科学计算应用中的性能表现，发现不同框架在N体模拟和结构化网格模拟中表现出显著性能差异。


<details>
  <summary>Details</summary>
Motivation: 随着异构计算架构的兴起，科学计算需要能够在不同硬件平台上高效执行的供应商无关性能可移植框架。

Method: 在Polaris超级计算机上使用四个NVIDIA A100 GPU，对N体模拟和结构化网格模拟进行分布式内存方法测试，比较四种框架的时间到解决方案性能。

Result: OCCA在小规模验证问题上执行更快（可能由于JIT编译），但缺乏优化的归约算法；OpenMP在结构化网格模拟中表现不佳，主要由于节点间数据同步和通信效率低。

Conclusion: 需要进一步优化各框架的归约算法、数据通信和内存管理，并进行可扩展性研究和统计分析来全面评估框架性能。

Abstract: Scientific computing in the exascale era demands increased computational
power to solve complex problems across various domains. With the rise of
heterogeneous computing architectures the need for vendor-agnostic, performance
portability frameworks has been highlighted. Libraries like Kokkos have become
essential for enabling high-performance computing applications to execute
efficiently across different hardware platforms with minimal code changes. In
this direction, this paper presents preliminary time-to-solution results for
two representative scientific computing applications: an N-body simulation and
a structured grid simulation. Both applications used a distributed memory
approach and hardware acceleration through four performance portability
frameworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single
node of the Polaris supercomputer using four NVIDIA A100 GPUs revealed
significant performance variability among frameworks. OCCA demonstrated faster
execution times for small-scale validation problems, likely due to JIT
compilation, however its lack of optimized reduction algorithms may limit
scalability for larger simulations while using its out of the box API. OpenMP
performed poorly in the structured grid simulation most likely due to
inefficiencies in inter-node data synchronization and communication. These
findings highlight the need for further optimization to maximize each
framework's capabilities. Future work will focus on enhancing reduction
algorithms, data communication, memory management, as wells as performing
scalability studies, and a comprehensive statistical analysis to evaluate and
compare framework performance.

</details>


### [19] [Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)](https://arxiv.org/abs/2511.02743)
*Fedor Ryabinin,Alexey Gotsman,Pierre Sutra*

Main category: cs.DC

TL;DR: EPaxos*是一个更简单且正确的Egalitarian Paxos变体，通过简化的故障恢复算法解决了原协议的复杂性和错误问题，并扩展到最优的进程数量配置。


<details>
  <summary>Details</summary>
Motivation: 经典的Paxos等状态机复制协议依赖单一领导者，存在单点故障和延迟问题。Egalitarian Paxos引入了无领导者方法但协议复杂、规范模糊且存在严重错误。

Method: 提出了EPaxos*协议，关键贡献是简化的故障恢复算法，并扩展到覆盖最优的故障阈值范围：n ≥ max{2e+f-1, 2f+1}。

Result: EPaxos*保持了Egalitarian Paxos的优势：在最多f个进程故障时保持非零吞吐量，在不超过e个进程故障且命令可交换时实现2消息延迟的快速执行。

Conclusion: EPaxos*提供了一个更简单、正确且最优的Egalitarian Paxos变体，解决了原协议的复杂性和正确性问题。

Abstract: Classical state-machine replication protocols, such as Paxos, rely on a
distinguished leader process to order commands. Unfortunately, this approach
makes the leader a single point of failure and increases the latency for
clients that are not co-located with it. As a response to these drawbacks,
Egalitarian Paxos introduced an alternative, leaderless approach, that allows
replicas to order commands collaboratively. Not relying on a single leader
allows the protocol to maintain non-zero throughput with up to $f$ crashes of
any processes out of a total of $n = 2f+1$. The protocol furthermore allows any
process to execute a command $c$ fast, in $2$ message delays, provided no more
than $e = \lceil\frac{f+1}{2}\rceil$ other processes fail, and all concurrently
submitted commands commute with $c$; the latter condition is often satisfied in
practical systems.
  Egalitarian Paxos has served as a foundation for many other replication
protocols. But unfortunately, the protocol is very complex, ambiguously
specified and suffers from nontrivial bugs. In this paper, we present EPaxos*
-- a simpler and correct variant of Egalitarian Paxos. Our key technical
contribution is a simpler failure-recovery algorithm, which we have rigorously
proved correct. Our protocol also generalizes Egalitarian Paxos to cover the
whole spectrum of failure thresholds $f$ and $e$ such that $n \ge \max\{2e+f-1,
2f+1\}$ -- the number of processes that we show to be optimal.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [20] [Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects](https://arxiv.org/abs/2511.02132)
*Mansi Choudhary,Karthik Sangaiah,Sonali Singh,Muhammad Osama,Lisa Wu Wills,Ganesh Dasika*

Main category: cs.AR

TL;DR: 论文提出了一种针对多芯片GPU架构的NUMA感知调度策略，通过空间感知的注意力头映射来优化多注意力头工作负载的性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI GPU向多芯片架构发展，非统一内存访问(NUMA)成为大规模注意力工作负载的性能瓶颈，传统GPU调度策略无法有效处理内存访问延迟和带宽的不均匀性。

Method: 提出了Swizzled Head-first Mapping策略，将注意力头与GPU NUMA域对齐，利用芯片内缓存重用，优化多注意力头工作负载的空间调度。

Result: 在AMD MI300X架构上，相比最先进的注意力算法，性能提升高达50%，L2缓存命中率维持在80-97%的高水平。

Conclusion: NUMA感知调度对于下一代分解式GPU实现全效率至关重要，为可扩展的AI训练和推理提供了前进路径。

Abstract: The rise of disaggregated AI GPUs has exposed a critical bottleneck in
large-scale attention workloads: non-uniform memory access (NUMA). As
multi-chiplet designs become the norm for scaling compute capabilities, memory
latency and bandwidth vary sharply across compute regions, undermining the
performance of traditional GPU kernel scheduling strategies that assume uniform
memory access. We identify how these NUMA effects distort locality in
multi-head attention (MHA) and present Swizzled Head-first Mapping, a
spatially-aware scheduling strategy that aligns attention heads with GPU NUMA
domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our
method achieves up to 50% higher performance over state-of-the-art attention
algorithms using conventional scheduling techniques and sustains consistently
high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware
scheduling is now fundamental to achieving full efficiency on next-generation
disaggregated GPUs, offering a path forward for scalable AI training and
inference.

</details>


### [21] [BoolSkeleton: Boolean Network Skeletonization via Homogeneous Pattern Reduction](https://arxiv.org/abs/2511.02196)
*Liwei Ni,Jiaxi Zhang,Shenggen Zheng,Junfeng Liu,Xingyu Meng,Biwei Xie,Xingquan Li,Huawei Li*

Main category: cs.AR

TL;DR: BoolSkeleton是一种布尔网络骨架化方法，通过预处理和归约两个步骤，将布尔网络转换为布尔依赖图并进行模式归约，提高设计一致性评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 布尔等价性使得功能相同的布尔网络具有不同的图结构，这为逻辑优化提供了探索空间，但也给布尔网络间的一致性任务带来了挑战。

Method: 包含预处理和归约两个步骤：预处理将布尔网络转换为布尔依赖图，节点被赋予功能相关状态；归约步骤定义同质和异质模式，保留异质模式以维持关键功能依赖，归约同质模式，参数K控制模式扇入大小以调节图归约粒度。

Result: 在布尔网络的四个分析/下游任务中验证了有效性：压缩分析、分类、关键路径分析和时序预测，在时序预测任务中相比原始布尔网络平均准确率提高55%以上。

Conclusion: 实验证明了BoolSkeleton在增强逻辑综合中设计一致性方面的潜力。

Abstract: Boolean equivalence allows Boolean networks with identical functionality to
exhibit diverse graph structures. This gives more room for exploration in logic
optimization, while also posing a challenge for tasks involving consistency
between Boolean networks. To tackle this challenge, we introduce BoolSkeleton,
a novel Boolean network skeletonization method that improves the consistency
and reliability of design-specific evaluations. BoolSkeleton comprises two key
steps: preprocessing and reduction. In preprocessing, the Boolean network is
transformed into a defined Boolean dependency graph, where nodes are assigned
the functionality-related status. Next, the homogeneous and heterogeneous
patterns are defined for the node-level pattern reduction step. Heterogeneous
patterns are preserved to maintain critical functionality-related dependencies,
while homogeneous patterns can be reduced. Parameter K of the pattern further
constrains the fanin size of these patterns, enabling fine-tuned control over
the granularity of graph reduction. To validate BoolSkeleton's effectiveness,
we conducted four analysis/downstream tasks around the Boolean network:
compression analysis, classification, critical path analysis, and timing
prediction, demonstrating its robustness across diverse scenarios. Furthermore,
it improves above 55% in the average accuracy compared to the original Boolean
network for the timing prediction task. These experiments underscore the
potential of BoolSkeleton to enhance design consistency in logic synthesis.

</details>


### [22] [VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning](https://arxiv.org/abs/2511.02285)
*Zhuorui Zhao,Bing Li,Grace Li Zhang,Ulf Schlichtmann*

Main category: cs.AR

TL;DR: VFocus是一个三阶段框架，通过聚焦LLM推理于代码生成的关键决策点来增强Verilog生成。它包括预排名阶段的密度引导过滤、排名阶段的基于一致性的聚类，以及后排名阶段的细化，显著提高了Verilog代码的功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖自一致性或模拟反馈来选择最佳候选，但未能将LLM推理聚焦在设计中信息最丰富的部分，导致功能正确性难以保证。

Method: 三阶段框架：1) 预排名阶段：生成多个代码候选，应用密度引导过滤保留在"推理甜点"范围内的候选；2) 排名阶段：自动生成测试平台模拟代码，基于自一致性聚类识别最一致输出；3) 后排名细化阶段：对排名靠前的候选进行不一致性挖掘，使用推理增强的LLM提示进行候选细化。

Result: 在VerilogEval-Human基准测试中，VFocus显著提高了多个推理LLM的pass@1正确性，证明了其在复杂硬件设计任务中增强Verilog生成的有效性。

Conclusion: VFocus通过聚焦LLM推理于关键决策点，有效提升了Verilog代码生成的功能正确性，为复杂硬件设计任务提供了更可靠的解决方案。

Abstract: Large Language Models (LLMs) have shown impressive potential in generating
Verilog codes, but ensuring functional correctness remains a challenge.
Existing approaches often rely on self-consistency or simulation feedback to
select the best candidate, but they miss opportunities to focus LLM reasoning
on the most informative parts of the design. We propose VFocus, a three-stage
framework that enhances Verilog generation by sharpening the focus of LLM
reasoning onto critical decision points in the code generation process. In the
\textbf{pre-ranking stage}, VFocus generates multiple code candidates through
LLM prompting, retries for syntactically valid outputs, and introduces a
\textit{Density-guided Filtering} to retain candidates that fall within the
"reasoning sweet spot" for functional correctness. In the \textbf{ranking
stage}, we simulate each code candidate using an automatically generated
testbench and apply self-consistency-based clustering to identify the most
consistent outputs. Finally, in the \textbf{post-ranking refinement stage},
VFocus performs inconsistency mining on top-ranked candidates and invokes
reasoning-augmented LLM prompts for candidate refinement. Experiments on the
VerilogEval-Human benchmark show that VFocus significantly improves the pass@1
correctness across multiple reasoning LLMs, demonstrating its effectiveness in
enhancing Verilog generation for complex hardware design tasks.

</details>


### [23] [Energy-Efficient Hardware Acceleration of Whisper ASR on a CGLA](https://arxiv.org/abs/2511.02269)
*Takuto Ando,Yu Eto,Ayumu Takeuchi,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 在IMAX CGLA加速器上实现和评估Whisper核心计算内核，相比CPU和GPU展现出更优的能效表现。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在自动语音识别(ASR)任务中的兴起带来了严重的能耗挑战，ASIC虽然高效但缺乏可编程性来适应不断演进的算法。

Method: 采用硬件/软件协同设计方法，在FPGA原型上实现Whisper核心计算内核，并基于28nm ASIC进行性能预测。

Result: 投影的ASIC在Q8_0模型上比NVIDIA Jetson AGX Orin能效高1.90倍，比NVIDIA RTX 4090高9.83倍。

Conclusion: CGLA是功率受限边缘设备上实现可持续ASR的有前景平台。

Abstract: The rise of generative AI for tasks like Automatic Speech Recognition (ASR)
has created a critical energy consumption challenge. While ASICs offer high
efficiency, they lack the programmability to adapt to evolving algorithms. To
address this trade-off, we implement and evaluate Whisper's core computational
kernel on the IMAX, a general-purpose Coarse-Grained Linear Arrays (CGLAs)
accelerator. To our knowledge, this is the first work to execute a Whisper
kernel on a CGRA and compare its performance against CPUs and GPUs. Using
hardware/software co-design, we evaluate our system via an FPGA prototype and
project performance for a 28 nm ASIC. Our results demonstrate superior energy
efficiency. The projected ASIC is 1.90x more energy-efficient than the NVIDIA
Jetson AGX Orin and 9.83x more than an NVIDIA RTX 4090 for the Q8_0 model. This
work positions CGLA as a promising platform for sustainable ASR on
power-constrained edge devices.

</details>


### [24] [Facial Expression Recognition System Using DNN Accelerator with Multi-threading on FPGA](https://arxiv.org/abs/2511.02408)
*Takuto Ando,Yusuke Inoue*

Main category: cs.AR

TL;DR: 在SoC FPGA上实现了一个基于DPU的多线程面部表情识别系统，使用DenseBox进行人脸检测和CNN进行表情识别，相比之前的工作提高了准确率和系统效率。


<details>
  <summary>Details</summary>
Motivation: 之前的工作使用Haar Cascade检测器在CPU上运行人脸检测，准确率较低，且需要专用电路加速器。本文旨在使用DPU统一处理两个DNN推理，提高准确率和资源利用效率。

Method: 在SoC FPGA上使用DPU运行DenseBox人脸检测和CNN表情识别两个DNN推理，开发多线程技术提高吞吐量和DPU利用率。

Result: 实现了25 FPS的整体系统吞吐量，功耗效率提高了2.4倍。

Conclusion: 使用DPU统一处理人脸检测和表情识别任务，能够在保持小电路规模的同时高效利用FPGA资源，并通过多线程技术显著提升系统性能。

Abstract: In this paper, we implement a stand-alone facial expression recognition
system on an SoC FPGA with multi-threading using a Deep learning Processor Unit
(DPU). The system consists of two steps: one for face detection step and one
for facial expression recognition. In the previous work, the Haar Cascade
detector was run on a CPU in the face detection step due to FPGA resource
limitations, but this detector is less accurate for profile and variable
illumination condition images. Moreover, the previous work used a dedicated
circuit accelerator, so running a second DNN inference for face detection on
the FPGA would require the addition of a new accelerator. As an alternative to
this approach, we run the two inferences by DNN on a DPU, which is a
general-purpose CNN accelerator of the systolic array type. Our method for face
detection using DenseBox and facial expression recognition using CNN on the
same DPU enables the efficient use of FPGA resources while maintaining a small
circuit size. We also developed a multi-threading technique that improves the
overall throughput while increasing the DPU utilization efficiency. With this
approach, we achieved an overall system throughput of 25 FPS and a throughput
per power consumption of 2.4 times.

</details>


### [25] [Digit-Recurrence Posit Division](https://arxiv.org/abs/2511.02494)
*Raul Murillo,Julio Villalba-Moreno,Alberto A. Del Barrio,Guillermo Botella*

Main category: cs.AR

TL;DR: 本文提出了基于数字递归算法的posit除法单元，首次在posit系统中实现基数-4数字递归技术，显著降低了能耗和迭代次数。


<details>
  <summary>Details</summary>
Motivation: Posit算术作为IEEE 754浮点表示的替代方案，具有更高的精度和动态范围，但其除法操作由于硬件复杂性而面临挑战。

Method: 采用数字递归算法，结合冗余算术、在线商转换和操作数缩放等硬件优化技术来简化除法过程。

Result: 综合评估显示，相比现有方法实现了超过80%的能耗降低，仅有小面积开销，并大幅减少了迭代次数。

Conclusion: 该算法有潜力显著提升基于posit的算术单元的效率。

Abstract: Posit arithmetic has emerged as a promising alternative to IEEE 754
floating-point representation, offering enhanced accuracy and dynamic range.
However, division operations in posit systems remain challenging due to their
inherent hardware complexity. In this work, we present posit division units
based on the digit-recurrence algorithm, marking the first implementation of
radix-4 digit-recurrence techniques within this context. Our approach
incorporates hardware-centric optimizations including redundant arithmetic,
on-the-fly quotient conversion, and operand scaling to streamline the division
process while mitigating latency, area, and power overheads. Comprehensive
synthesis evaluations across multiple posit configurations demonstrate
significant performance improvements, including more than 80% energy reduction
with small area overhead compared to existing methods, and a substantial
decrease in the number of iterations. These results underscore the potential of
our adapted algorithm to enhance the efficiency of posit-based arithmetic
units.

</details>


### [26] [Implementation and Evaluation of Stable Diffusion on a General-Purpose CGLA Accelerator](https://arxiv.org/abs/2511.02530)
*Takuto Ando,Yu Eto,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 在IMAX3 CGRA加速器上首次实现并评估stable-diffusion.cpp图像生成核心计算任务，评估其在FPGA原型上的性能并预测ASIC实现的潜力。


<details>
  <summary>Details</summary>
Motivation: 评估通用CGRA加速器IMAX3在处理要求较高的图像生成工作负载时的能力，为未来IMAX架构设计和AI专用CGLA加速器开发提供指导。

Method: 在IMAX3 CGRA加速器上实现stable-diffusion.cpp的主要计算内核，在FPGA原型上进行性能评估，并预测ASIC实现的性能潜力。

Result: 尽管采用通用架构，IMAX3在性能（特别是预测的ASIC形式）和能效方面表现良好。

Conclusion: 这项工作为实现能效高、设备端、多模态AI平台奠定了基础，并为未来IMAX架构设计和AI专用CGLA加速器开发提供了具体指导。

Abstract: This paper presents the first implementation and in-depth evaluation of the
primary computational kernels from the stable-diffusion.cpp image generation
framework on IMAX3, a general-purpose Coarse-Grained Reconfigurable Array
(CGRA) accelerator. We designed IMAX3 as a versatile computational platform,
and this work assesses its capabilities by executing a demanding image
generation workload. We evaluate its performance on a current
Field-Programmable Gate Array (FPGA) prototype to establish a baseline and
project its potential for a future Application-Specific Integrated Circuit
(ASIC) implementation. Our results demonstrate that, despite its
general-purpose architecture, IMAX3 achieves promising performance and power
efficiency, particularly in its projected ASIC form. This work provides
concrete guidelines for future IMAX architectural designs and establishes a
foundation for developing next-generation, AI-specialized Coarse-Grained Linear
Array (CGLA) accelerators by refining this versatile platform. Ultimately, this
achievement contributes to the realization of energy-efficient, on-device,
multi-modal AI platforms.

</details>
