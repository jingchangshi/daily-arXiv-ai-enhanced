<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Designing Walrus: Relational Programming with Rich Types, On-Demand Laziness, and Structured Traces](https://arxiv.org/abs/2510.02579)
*Santiago Cuéllar,Naomi Spargo,Jonathan Daugherty,David Darais*

Main category: cs.PL

TL;DR: Walrus是一个嵌入在Haskell中的函数式关系编程语言，扩展了miniKanren模型，增加了类型多态统一、按需惰性求值和实用开发功能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决实际开发中的可用性问题，为双向编译器开发等应用提供更好的工具支持。

Method: 通过Haskell泛型减少样板代码，提供结构化调试跟踪，支持产品类型，并扩展miniKanren模型。

Result: 开发了具有类型多态统一和按需惰性求值功能的Walrus语言，提升了实际开发体验。

Conclusion: Walrus通过关键设计决策解决了实际开发中常见的可用性挑战，为关系编程提供了更实用的工具。

Abstract: We present Walrus, a functional relational programming language embedded in
Haskell that extends the miniKanren model with type-polymorphic unification,
on-demand laziness, and a range of usability features aimed at practical
development. These include use of Haskell Generics for boilerplate reduction,
structured debugging traces, and ergonomic support for product types. We
describe the design and implementation of Walrus through the lens of our
experience developing bidirectional compilers, and reflect on key design
decisions and recurring usability challenges encountered in practice.

</details>


### [2] [Beyond Cons: Purely Relational Data Structures](https://arxiv.org/abs/2510.03170)
*Rafaello Sanna,William E. Byrd,Nada Amin*

Main category: cs.PL

TL;DR: Kanren是miniKanren的扩展，增加了集合和关联列表的约束推理功能，包括一等集合对象、集合论约束和关联列表约束，提高了抽象数据操作程序的表达能力和操作性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决在miniKanren中描述集合和关联列表时需要依赖结构编码和急切搜索的问题，提供更声明式和惰性的方式来描述集合。

Method: 扩展miniKanren系统，引入一等集合对象、完整的集合论约束（成员、并集、不相交等）以及支持遮蔽和范围查找的关联列表约束。

Result: 提高了程序的表达能力和操作性能，特别是在操作抽象数据（如解释器）时，支持基于内容的集合相等性和有限失败。

Conclusion: Kanren通过引入集合和关联列表约束，为miniKanren提供了更强大的抽象数据操作能力，使程序描述更加声明式和高效。

Abstract: We present {Kanren} (read: set-Kanren), an extension to miniKanren with
constraints for reasoning about sets and association lists. {Kanren} includes
first-class set objects, a functionally complete family of set-theoretic
constraints (including membership, union, and disjointedness), and new
constraints for reasoning about association lists with shadowing and scoped
lookup. These additions allow programmers to describe collections declaratively
and lazily, without relying on structural encodings and eager search over
representation spaces. The result is improved expressiveness and operational
behavior in programs that manipulate abstract data -- particularly interpreters
-- by supporting set equality based on contents, enabling finite failure. We
describe the design and implementation of {Kanren} in a constraint-enabled
miniKanren system and illustrate its use in representative examples.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts Models](https://arxiv.org/abs/2510.02613)
*Gursimran Singh,Timothy Yu,Haley Li,Cheng Chen,Hanieh Sadri,Qintao Zhang,Yu Zhang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: ElasticMoE是一个用于MoE LLM的弹性扩展框架，通过解耦推理执行和内存操作，实现细粒度、低延迟、零停机的扩展，显著提升了在动态云环境中部署大规模MoE LLM的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE模型扩展策略存在局限性：水平扩展粒度粗、延迟长、成本高；垂直扩展需要实例重启导致停机。这些方法不适合云部署中常见的突发、短时流量模式。

Method: ElasticMoE通过解耦推理执行与内存操作实现并发扩展，包含HBM管理模块重用权重和KV缓存，使用零拷贝重映射和点对点传输，以及基于虚拟内存的专家重分配机制。

Result: 在Ascend NPU上的评估显示，ElasticMoE相比基线实现了高达9倍的扩展延迟降低、2倍的吞吐量提升，并显著改善了SLO达成率。

Conclusion: ElasticMoE通过实现细粒度、并发的扩展且干扰最小，推进了在动态云环境中部署大规模MoE LLM的实用性。

Abstract: Mixture-of-Experts (MoE) models promise efficient scaling of large language
models (LLMs) by activating only a small subset of experts per token, but their
parallelized inference pipelines make elastic serving challenging. Existing
strategies fall short: horizontal scaling provisions entire replicas of the
current configuration, often tens to hundreds of accelerators, leading to
coarse granularity, long provisioning delays, and costly overprovisioning.
Vertical scaling offers finer adjustments but typically requires instance
restarts, incurring downtime. These limitations make current approaches
ill-suited for the bursty, short-lived traffic patterns common in cloud
deployments.
  We present ElasticMoE, an elastic scaling framework for MoE LLMs that
achieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE
decouples inference execution from memory operations, enabling scaling steps to
proceed concurrently with serving. An HBM Management Module (HMM) reuses
weights and KV caches via zero-copy remapping, while high-bandwidth
peer-to-peer transfers bring newly added accelerators online without
interrupting service. A virtual memory based expert redistribution mechanism
migrates MoE experts without costly buffer reallocations, reducing peak memory
usage during expert parallelism reconfiguration.
  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that
ElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput
during scaling, and significantly improves SLO attainment compared to
baselines. By enabling fine-grained, concurrent scaling with minimal
disruption, ElasticMoE advances the practicality of deploying massive MoE LLMs
in dynamic cloud environments.

</details>


### [4] [GRNND: A GPU-Parallel Relative NN-Descent Algorithm for Efficient Approximate Nearest Neighbor Graph Construction](https://arxiv.org/abs/2510.02774)
*Xiang Li,Qiong Chang,Yun Li,Jun Miyazaki*

Main category: cs.DC

TL;DR: GRNND是首个GPU并行化的RNN-Descent算法，通过无序邻居传播策略、warp级协作操作和双缓冲邻居池等技术，显著提升了大规模高维数据下近似最近邻图的构建效率。


<details>
  <summary>Details</summary>
Motivation: 随着数据量和维度的增加，RNN-Descent算法在图构建阶段的复杂度急剧上升，变得非常耗时，甚至阻碍后续查询处理。

Method: 提出了GRNND算法，采用无序邻居传播策略避免同步更新陷阱，利用warp级协作操作和固定容量的双缓冲邻居池实现高效内存访问和高度并行化邻居更新。

Result: GRNND在性能上显著优于现有CPU和GPU方法，相比GPU方法加速2.4-51.7倍，相比CPU方法加速17.8-49.8倍。

Conclusion: GRNND是首个充分利用GPU架构的RNN-Descent并行算法，能够有效解决大规模高维数据下近似最近邻图构建的效率瓶颈问题。

Abstract: Relative Nearest Neighbor Descent (RNN-Descent) is a state-of-the-art
algorithm for constructing sparse approximate nearest neighbor (ANN) graphs by
combining the iterative refinement of NN-Descent with the edge-pruning rules of
the Relative Neighborhood Graph (RNG). It has demonstrated strong effectiveness
in large-scale search tasks such as information retrieval and related tasks.
However, as the amount and dimensionality of data increase, the complexity of
graph construction in RNN-Descent rises sharply, making this stage increasingly
time-consuming and even prohibitive for subsequent query processing. In this
paper, we propose GRNND, the first GPU-parallel algorithm of RNN-Descent
designed to fully exploit GPU architecture. GRNND introduces a disordered
neighbor propagation strategy to mitigate synchronized update traps, enhancing
structural diversity, and avoiding premature convergence during parallel
execution. It also leverages warp-level cooperative operations and a
double-buffered neighbor pool with fixed capacity for efficient memory access,
eliminate contention, and enable highly parallelized neighbor updates.
Extensive experiments demonstrate that GRNND consistently outperforms existing
CPU- and GPU-based methods. GRNND achieves 2.4 to 51.7x speedup over existing
GPU methods, and 17.8 to 49.8x speedup over CPU methods.

</details>


### [5] [TridentServe: A Stage-level Serving System for Diffusion Pipelines](https://arxiv.org/abs/2510.02838)
*Yifei Xia,Fangcheng Fu,Hao Yuan,Hanke Zhang,Xupeng Miao,Yijun Liu,Suhan Ling,Jie Jiang,Bin Cui*

Main category: cs.DC

TL;DR: 提出了TridentServe系统，采用动态阶段级服务范式来优化扩散管道的资源分配，显著提升服务质量和降低延迟


<details>
  <summary>Details</summary>
Motivation: 当前扩散管道服务系统采用静态、手动、管道级的资源分配方式，无法适应不同阶段和请求的资源需求差异，导致效率低下

Method: 开发TridentServe系统，自动动态生成管道部署的放置计划和请求处理的路由计划，协同优化模型和请求的资源分配

Result: 在各种工作负载下，TridentServe相比现有工作将平均/P95延迟降低高达2.5倍和3.6倍/4.1倍，持续改善SLO达成率

Conclusion: 动态阶段级服务范式能有效解决扩散管道服务中的资源分配效率问题，TridentServe系统证明了该方法的优越性

Abstract: Diffusion pipelines, renowned for their powerful visual generation
capabilities, have seen widespread adoption in generative vision tasks (e.g.,
text-to-image/video). These pipelines typically follow an
encode--diffuse--decode three-stage architecture. Current serving systems
deploy diffusion pipelines within a static, manual, and pipeline-level
paradigm, allocating the same resources to every request and stage. However,
through an in-depth analysis, we find that such a paradigm is inefficient due
to the discrepancy in resource needs across the three stages of each request,
as well as across different requests. Following the analysis, we propose the
dynamic stage-level serving paradigm and develop TridentServe, a brand new
diffusion serving system. TridentServe automatically, dynamically derives the
placement plan (i.e., how each stage resides) for pipeline deployment and the
dispatch plan (i.e., how the requests are routed) for request processing,
co-optimizing the resource allocation for both model and requests. Extensive
experiments show that TridentServe consistently improves SLO attainment and
reduces average/P95 latencies by up to 2.5x and 3.6x/4.1x over existing works
across a variety of workloads.

</details>


### [6] [On the energy efficiency of sparse matrix computations on multi-GPU clusters](https://arxiv.org/abs/2510.02878)
*Massimo Bernaschi,Alessandro Celestini,Pasqua D'Ambra,Giorgio Richelli*

Main category: cs.DC

TL;DR: 该论文研究了稀疏矩阵并行计算库的能效表现，在已证明性能优势的基础上进一步提供了能耗分析，证实优化GPU计算和减少数据移动能同时降低求解时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 满足现代高性能计算平台日益增长的可持续性需求，在已有性能优势的基础上提供能耗分析，验证能效优化策略。

Method: 开发了精确的运行时能耗测量方法和工具，对库的核心组件进行能耗分析，优化GPU计算并最小化跨内存和计算节点的数据移动。

Result: 优化GPU计算和减少数据移动能同时降低求解时间和能耗，在标准基准测试中相比同类软件框架具有显著优势。

Conclusion: 该稀疏矩阵并行计算库不仅性能优越，在能效方面也表现出色，为大规模科学应用提供了高效且可持续的解决方案。

Abstract: We investigate the energy efficiency of a library designed for parallel
computations with sparse matrices. The library leverages high-performance,
energy-efficient Graphics Processing Unit (GPU) accelerators to enable
large-scale scientific applications. Our primary development objective was to
maximize parallel performance and scalability in solving sparse linear systems
whose dimensions far exceed the memory capacity of a single node. To this end,
we devised methods that expose a high degree of parallelism while optimizing
algorithmic implementations for efficient multi-GPU usage. Previous work has
already demonstrated the library's performance efficiency on large-scale
systems comprising thousands of NVIDIA GPUs, achieving improvements over
state-of-the-art solutions. In this paper, we extend those results by providing
energy profiles that address the growing sustainability requirements of modern
HPC platforms. We present our methodology and tools for accurate runtime energy
measurements of the library's core components and discuss the findings. Our
results confirm that optimizing GPU computations and minimizing data movement
across memory and computing nodes reduces both time-to-solution and energy
consumption. Moreover, we show that the library delivers substantial advantages
over comparable software frameworks on standard benchmarks.

</details>


### [7] [Energy Efficiency in Cloud-Based Big Data Processing for Earth Observation: Gap Analysis and Future Directions](https://arxiv.org/abs/2510.02882)
*Adhitya Bhawiyuga,Serkan Girgin,Rolf A. de By,Raul Zurita-Milla*

Main category: cs.DC

TL;DR: 本文分析了云处理地球观测大数据的能效问题，识别了当前平台在能效方面的关键差距，并提出了改进的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着地球观测数据量快速增长和云计算的广泛应用，能效问题日益突出，特别是在关注计算密集型基础模型和碳足迹的背景下，云处理地球观测大数据的能效实践存在明显不足。

Method: 首先分析当前地球观测大数据处理现状和云处理需求，然后研究其他大数据领域成功的能效策略，识别地球观测大数据处理平台的关键能效差距。

Result: 识别出四个关键能效差距：能效监测机制不足、数据管理缺乏能效意识、资源分配未充分考虑能效、任务调度缺乏能效标准。

Conclusion: 提出了开发能效感知的性能监测框架、优化基础设施编排技术、采用能效任务调度方法等研究方向，旨在提高地球观测大数据处理的能效意识，降低能耗和环境影​响。

Abstract: Earth observation (EO) data volumes are rapidly increasing. While cloud
computing are now used for processing large EO datasets, the energy efficiency
aspects of such a processing have received much less attention. This issue is
notable given the increasing awareness of energy costs and carbon footprint in
big data processing, particularly with increased attention on compute-intensive
foundation models. In this paper we identify gaps in energy efficiency
practices within cloud-based EO big data (EOBD) processing and propose several
research directions for improvement. We first examine the current EOBD
landscape, focus on the requirements that necessitate cloud-based processing
and analyze existing cloud-based EOBD solutions. We then investigate energy
efficiency strategies that have been successfully employed in well-studied big
data domains. Through this analysis, we identify several critical gaps in
existing EOBD processing platforms, which primarily focus on data accessibility
and computational feasibility, instead of energy efficiency. These gaps include
insufficient energy monitoring mechanisms, lack of energy awareness in data
management, inadequate implementation of energy-aware resource allocation and
lack of energy efficiency criteria on task scheduling. Based on these findings,
we propose the development of energy-aware performance monitoring and
benchmarking frameworks, the use of optimization techniques for infrastructure
orchestration, and of energy-efficient task scheduling approaches for
distributed cloud-based EOBD processing frameworks. These proposed approaches
aim to foster more energy awareness in EOBD processing , potentially reducing
power consumption and environmental impact while maintaining or minimally
impacting processing performance.

</details>


### [8] [PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics](https://arxiv.org/abs/2510.02894)
*Jakub Lisowski,Piotr Tyrakowski,Szymon Zyguła,Krzysztof Kaczmarski*

Main category: cs.DC

TL;DR: PyRadiomics-cuda是一个GPU加速的PyRadiomics库扩展，通过将关键几何计算卸载到GPU硬件，显著减少医学图像三维形状特征提取的处理时间。


<details>
  <summary>Details</summary>
Motivation: 解决从医学图像中提取三维形状特征时的计算挑战，特别是处理大型体积数据集时的性能瓶颈。

Method: 使用Python和C/CUDA实现，将关键几何计算卸载到GPU硬件，同时保持与原始PyRadiomics API的完全兼容性。

Result: 在各种计算环境（计算集群、预算设备和家庭设备）中显著减少了处理时间，支持高效、可扩展的放射组学分析。

Conclusion: PyRadiomics-cuda提供了透明的GPU加速，支持高吞吐量AI流程中的快速特征提取，且无需修改现有代码即可集成到现有工作流中。

Abstract: PyRadiomics-cuda is a GPU-accelerated extension of the PyRadiomics library,
designed to address the computational challenges of extracting
three-dimensional shape features from medical images. By offloading key
geometric computations to GPU hardware it dramatically reduces processing times
for large volumetric datasets. The system maintains full compatibility with the
original PyRadiomics API, enabling seamless integration into existing AI
workflows without code modifications. This transparent acceleration facilitates
efficient, scalable radiomics analysis, supporting rapid feature extraction
essential for high-throughput AI pipeline. Tests performed on a typical
computational cluster, budget and home devices prove usefulness in all
scenarios. PyRadiomics-cuda is implemented in Python and C/CUDA and is freely
available under the BSD license at https://github.com/mis-wut/pyradiomics-CUDA
Additionally PyRadiomics-cuda test suite is available at
https://github.com/mis-wut/pyradiomics-cuda-data-gen. It provides detailed
handbook and sample scripts suited for different kinds of workflows plus
detailed installation instructions. The dataset used for testing is available
at Kaggle
https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19

</details>


### [9] [iDDS: Intelligent Distributed Dispatch and Scheduling for Workflow Orchestration](https://arxiv.org/abs/2510.02930)
*Wen Guan,Tadashi Maeno,Aleksandr Alekseev,Fernando Harald Barreiro Megino,Kaushik De,Edward Karavakis,Alexei Klimentov,Tatiana Korchuganova,FaHui Lin,Paul Nilsson,Torre Wenaus,Zhaoyu Yang,Xin Zhao*

Main category: cs.DC

TL;DR: iDDS是一个面向大规模分布式科学计算的多功能工作流编排系统，通过集成数据感知执行、条件逻辑和可编程工作流，实现复杂动态处理管道的自动化。


<details>
  <summary>Details</summary>
Motivation: 传统工作负载和数据管理无法满足大规模科学计算中复杂动态工作流的需求，需要开发能够统一工作负载调度、数据移动和自适应决策的系统。

Method: 采用模块化消息驱动架构，集成PanDA和Rucio等系统，支持模板驱动工作流和Python编排的Function-as-a-Task模型。

Result: 成功应用于ATLAS实验磁带资源优化、Rubin天文台DAG工作流编排、机器学习超参数优化、物理分析主动学习和AI辅助探测器设计等多个场景。

Conclusion: iDDS通过统一工作负载调度、数据移动和自适应决策，降低了操作开销，实现了跨异构基础设施的可重现高吞吐量工作流。

Abstract: The intelligent Distributed Dispatch and Scheduling (iDDS) service is a
versatile workflow orchestration system designed for large-scale, distributed
scientific computing. iDDS extends traditional workload and data management by
integrating data-aware execution, conditional logic, and programmable
workflows, enabling automation of complex and dynamic processing pipelines.
Originally developed for the ATLAS experiment at the Large Hadron Collider,
iDDS has evolved into an experiment-agnostic platform that supports both
template-driven workflows and a Function-as-a-Task model for Python-based
orchestration.
  This paper presents the architecture and core components of iDDS,
highlighting its scalability, modular message-driven design, and integration
with systems such as PanDA and Rucio. We demonstrate its versatility through
real-world use cases: fine-grained tape resource optimization for ATLAS,
orchestration of large Directed Acyclic Graph (DAG) workflows for the Rubin
Observatory, distributed hyperparameter optimization for machine learning
applications, active learning for physics analyses, and AI-assisted detector
design at the Electron-Ion Collider.
  By unifying workload scheduling, data movement, and adaptive decision-making,
iDDS reduces operational overhead and enables reproducible, high-throughput
workflows across heterogeneous infrastructures. We conclude with current
challenges and future directions, including interactive, cloud-native, and
serverless workflow support.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [HALO: Memory-Centric Heterogeneous Accelerator with 2.5D Integration for Low-Batch LLM Inference](https://arxiv.org/abs/2510.02675)
*Shubham Negi,Kaushik Roy*

Main category: cs.AR

TL;DR: HALO是一种异构内存中心加速器，针对LLM推理中的prefill和decode阶段的不同计算特性进行优化，通过结合HBM CiD和片上CiM技术，在低批量推理场景下实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: LLM推理包含计算密集的prefill阶段和内存密集的decode阶段，现有方案主要针对高批量推理或短上下文场景，而交互式应用所需的低批量长上下文推理方案缺乏。

Method: 采用异构内存架构，集成HBM CiD和片上CiM，通过2.5D封装。提出阶段感知映射策略：prefill阶段的计算密集型操作映射到CiM，decode阶段的内存密集型操作映射到CiD。

Result: 在LLaMA-2 7B和Qwen3 8B模型上评估，相比AttAcc获得最高18倍几何平均加速，相比全CiD方案CENT获得2.5倍加速。

Conclusion: 异构内存架构能有效解决LLM推理中prefill和decode阶段的差异化需求，在低批量长上下文场景下提供显著性能优势。

Abstract: The rapid adoption of Large Language Models (LLMs) has driven a growing
demand for efficient inference, particularly in latency-sensitive applications
such as chatbots and personalized assistants. Unlike traditional deep neural
networks, LLM inference proceeds in two distinct phases: the prefill phase,
which processes the full input sequence in parallel, and the decode phase,
which generates tokens sequentially. These phases exhibit highly diverse
compute and memory requirements, which makes accelerator design particularly
challenging. Prior works have primarily been optimized for high-batch inference
or evaluated only short input context lengths, leaving the low-batch and long
context regime, which is critical for interactive applications, largely
underexplored.
  We propose HALO, a heterogeneous memory centric accelerator designed for
these unique challenges of prefill and decode phases in low-batch LLM
inference. HALO integrates HBM based Compute-in-DRAM (CiD) with an on-chip
analog Compute-in-Memory (CiM), co-packaged using 2.5D integration. To further
improve the hardware utilization, we introduce a phase-aware mapping strategy
that adapts to the distinct demands of the prefill and decode phases. Compute
bound operations in the prefill phase are mapped to CiM to exploit its high
throughput matrix multiplication capability, while memory-bound operations in
the decode phase are executed on CiD to benefit from reduced data movement
within DRAM. Additionally, we present an analysis of the performance tradeoffs
of LLMs under two architectural extremes: a fully CiD and a fully on-chip
analog CiM design to highlight the need for a heterogeneous design. We evaluate
HALO on LLaMA-2 7B and Qwen3 8B models. Our experimental results show that LLMs
mapped to HALO achieve up to 18x geometric mean speedup over AttAcc, an
attention-optimized mapping and 2.5x over CENT, a fully CiD based mapping.

</details>


### [11] [A Hardware Accelerator for the Goemans-Williamson Algorithm](https://arxiv.org/abs/2510.02863)
*D. A. Herrera-Martí,E. Guthmuller,J. Fereyre*

Main category: cs.AR

TL;DR: 该论文研究了在凸优化中使用扩展浮点精度来加速大规模Max-Cut问题的求解，特别是在共轭梯度等间接矩阵求逆方法中。


<details>
  <summary>Details</summary>
Motivation: Max-Cut问题已成为量子与经典优化器局部搜索启发式算法的基准测试。与仅提供平均情况性能保证的局部搜索相比，Goemans-Williamson的凸半定松弛方法提供最坏情况保证，适合构建基准和性能关键应用。

Method: 将扩展浮点精度整合到凸优化的代数子程序中，特别是在内点法中使用的共轭梯度等间接矩阵求逆方法。分析了在原生支持扩展精度的硬件架构上的预期加速效果。

Result: 当使用共轭梯度等间接矩阵求逆方法时，增加内部工作精度可减少求解时间，且加速因子随系统规模增大而增加。

Conclusion: 扩展浮点精度在大规模凸优化问题中能显著加速求解过程，特别是在使用间接矩阵求逆方法处理超大规模问题时效果更明显。

Abstract: The combinatorial problem Max-Cut has become a benchmark in the evaluation of
local search heuristics for both quantum and classical optimisers. In contrast
to local search, which only provides average-case performance guarantees, the
convex semidefinite relaxation of Max-Cut by Goemans and Williamson, provides
worst-case guarantees and is therefore suited to both the construction of
benchmarks and in applications to performance-critic scenarios.
  We show how extended floating point precision can be incorporated in
algebraic subroutines in convex optimisation, namely in indirect matrix
inversion methods like Conjugate Gradient, which are used in Interior Point
Methods in the case of very large problem sizes. Also, an estimate is provided
of the expected acceleration of the time to solution for a hardware
architecture that runs natively on extended precision. Specifically, when using
indirect matrix inversion methods like Conjugate Gradient, which have lower
complexity than direct methods and are therefore used in very large problems,
we see that increasing the internal working precision reduces the time to
solution by a factor that increases with the system size.

</details>


### [12] [A Resource-Driven Approach for Implementing CNNs on FPGAs Using Adaptive IPs](https://arxiv.org/abs/2510.02990)
*Philippe Magalhães,Virginie Fresse,Benoît Suffran,Olivier Alata*

Main category: cs.AR

TL;DR: 提出了一个资源高效的卷积IP库，能够自动适应FPGA可用资源，采用VHDL开发，参数化设计，使用定点算术优化性能。


<details>
  <summary>Details</summary>
Motivation: 实时低延迟AI应用需求增长，FPGA在CNN实现中具有可重构性、能效和性能优势，适合边缘设备和嵌入式系统。

Method: 开发了四个针对特定资源约束的IP核，采用VHDL参数化设计，使用定点算术，灵活配置DSP使用、逻辑消耗和精度。

Result: 在Zynq UltraScale+ FPGA上的实验结果显示性能与资源使用之间的权衡，与现有FPGA加速技术相比具有更好的通用性和架构独立性。

Conclusion: 该方法展示了FPGA上CNN实现的灵活性和效率，未来将扩展库以包含池化和激活函数，提升适用性和集成能力。

Abstract: The increasing demand for real-time, low-latency artificial intelligence
applications has propelled the use of Field-Programmable Gate Arrays (FPGAs)
for Convolutional Neural Network (CNN) implementations. FPGAs offer
reconfigurability, energy efficiency, and performance advantages over GPUs,
making them suitable for edge devices and embedded systems. This work presents
a novel library of resource-efficient convolution IPs designed to automatically
adapt to the available FPGA resources. Developed in VHDL, these IPs are
parameterizable and utilize fixed-point arithmetic for optimal performance.
Four IPs are introduced, each tailored to specific resource constraints,
offering flexibility in DSP usage, logic consumption, and precision.
Experimental results on a Zynq UltraScale+ FPGA highlight the trade-offs
between performance and resource usage. The comparison with recent FPGA-based
CNN acceleration techniques emphasizes the versatility and independence of this
approach from specific FPGA architectures or technological advancements. Future
work will expand the library to include pooling and activation functions,
enabling broader applicability and integration into CNN frameworks.

</details>
