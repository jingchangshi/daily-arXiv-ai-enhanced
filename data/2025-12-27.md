<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale](https://arxiv.org/abs/2512.20795)
*Aymen Alsaadi,Mason Hooten,Mariya Goliyad,Andre Merzky,Andrew Shao,Mikhail Titov,Tianle Wang,Yian Chen,Maria Kalantzi,Kent Lee,Andrew Park,Indira Pimpalkhare,Nick Radcliffe,Colin Wahl,Pete Mendygral,Matteo Turilli,Shantenu Jha*

Main category: cs.DC

TL;DR: RHAPSODY是一个多运行时中间件，通过统一的任务、服务、资源和执行策略抽象，支持在单一作业分配中并发执行异构AI-HPC工作负载，解决了现有系统只能处理部分需求的限制。


<details>
  <summary>Details</summary>
Motivation: 混合AI-HPC工作流结合了大规模模拟、训练、高吞吐量推理和紧密耦合的智能体驱动控制，对运行时系统提出了异构且经常冲突的需求。现有系统通常只能满足部分需求，限制了支持新兴AI-HPC应用的能力。

Method: RHAPSODY采用多运行时中间件架构，不替代现有运行时系统，而是通过统一的任务、服务、资源和执行策略抽象来组合和协调它们，使模拟代码、推理服务和智能体工作流能够在领导级HPC平台的单一作业分配中共存。

Result: 在多个HPC系统上使用Dragon和vLLM进行评估，结果显示：RHAPSODY引入的运行时开销最小，能够持续支持规模不断增加的异构性，为高吞吐量推理工作负载实现接近线性的扩展，并在智能体工作流中实现AI和HPC任务之间的数据和控制器高效耦合。

Conclusion: RHAPSODY通过协调现有运行时系统，成功解决了混合AI-HPC工作流的异构需求问题，为新兴AI-HPC应用在领导级HPC平台上的规模化部署提供了有效解决方案。

Abstract: Hybrid AI-HPC workflows combine large-scale simulation, training, high-throughput inference, and tightly coupled, agent-driven control within a single execution campaign. These workflows impose heterogeneous and often conflicting requirements on runtime systems, spanning MPI executables, persistent AI services, fine-grained tasks, and low-latency AI-HPC coupling. Existing systems typically address only subsets of these requirements, limiting their ability to support emerging AI-HPC applications at scale. We present RHAPSODY, a multi-runtime middleware that enables concurrent execution of heterogeneous AI-HPC workloads through uniform abstractions for tasks, services, resources, and execution policies. Rather than replacing existing runtimes, RHAPSODY composes and coordinates them, allowing simulation codes, inference services, and agentic workflows to coexist within a single job allocation on leadership-class HPC platforms. We evaluate RHAPSODY with Dragon and vLLM on multiple HPC systems using representative heterogeneous, inference-at-scale, and tightly coupled AI-HPC workflows. Our results show that RHAPSODY introduces minimal runtime overhead, sustains increasing heterogeneity at scale, achieves near-linear scaling for high-throughput inference workloads, and data- and control-efficient coupling between AI and HPC tasks in agentic workflows.

</details>


### [2] [Stochastic well-structured transition systems](https://arxiv.org/abs/2512.20939)
*James Aspnes*

Main category: cs.DC

TL;DR: 论文定义了随机良构转移系统，扩展了传统良构转移系统以包含概率调度规则，涵盖种群协议、化学反应网络等模型。证明了这些系统中的相位时钟实现要么停止，要么在多项式期望步数内过快"滴答"，且任何终止计算在期望多项式时间内完成或失败。这精确刻画了增强系统的计算能力。


<details>
  <summary>Details</summary>
Motivation: 动机是扩展传统的良构转移系统以包含概率调度规则，从而统一建模种群协议、化学反应网络、八卦模型等随机分布式系统。同时研究这些系统在增强（如添加全序或等价关系）后的计算能力。

Method: 方法是通过定义新的随机良构转移系统类，该系统包含概率调度规则。然后分析这些系统中相位时钟的实现特性，证明其要么停止，要么在多项式期望步数内过快"滴答"。进一步分析终止计算的时间特性，最终刻画增强系统的计算能力。

Result: 主要结果是：1) 随机良构转移系统中的相位时钟实现要么停止，要么在多项式期望步数内过快"滴答"；2) 任何终止计算在期望多项式时间内完成或失败；3) 增强系统（添加全序或等价关系）的计算能力精确对应BPP类语言，而未增强系统仅能计算BPL中的对称语言。

Conclusion: 结论是随机良构转移系统为种群协议、化学反应网络等随机分布式系统提供了统一框架。增强系统（添加全序或等价关系）具有BPP的计算能力，而未增强系统仅具有BPL中对称语言的计算能力。这为理解这些系统的计算极限提供了精确的理论刻画。

Abstract: Extending well-structured transition systems to incorporate a probabilistic scheduling rule, we define a new class of stochastic well-structured transition systems that includes population protocols, chemical reaction networks, and many common gossip models; as well as augmentations of these systems by an oracle that exposes a total order on agents as in population protocols in the comparison model or an equivalence relation as in population protocols with unordered data.
  We show that any implementation of a phase clock in these systems either stops or ticks too fast after polynomially many expected steps, and that any terminating computation in these systems finishes or fails in expected polynomial time. This latter property allows an exact characterization of the computational power of many stochastic well-structured transition systems augmented with a total order or equivalence relation on agents, showing that these compute exactly the languages in BPP, while the corresponding unaugmented systems compute just the symmetric languages in BPL.

</details>


### [3] [Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications](https://arxiv.org/abs/2512.20953)
*Yuxiao Wang,Yuedong Xu,Qingyang Duan,Yuxuan Liu,Lei Jiao,Yinghao Yu,Jun Wu*

Main category: cs.DC

TL;DR: AutoHet系统自动为异构GPU环境寻找最优3D并行训练方案，支持非对称并行结构，通过优化设备分组和负载均衡来最小化训练时间，并实现高效的弹性恢复机制。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速增长和新GPU产品的不断发布，异构GPU环境下的分布式训练需求显著增加。传统系统在异构环境中面临对称张量并行、非对称流水线并行梯度同步、内存利用与计算效率权衡等挑战。

Method: 提出AutoHet系统，支持非对称3D并行结构，实现细粒度工作负载分配。建立理论模型，将设备分组和负载均衡构建为优化问题以最小化每次迭代训练时间。设计弹性恢复策略，优先从本地节点恢复训练状态，仅从云存储下载缺失检查点。

Result: 在三个大规模模型和三种不同GPU类型组合上的评估显示，AutoHet优于现有DNN训练系统：相比Megatron-LM和Whale，训练吞吐量提升最高达1.79倍；相比spot instance基线，恢复速度提升4.38倍。

Conclusion: AutoHet系统有效解决了异构GPU环境下分布式训练的挑战，通过自动优化并行方案和高效的恢复机制，显著提升了训练效率和弹性能力。

Abstract: The rapid growth of large language models (LLMs) and the continuous release of new GPU products have significantly increased the demand for distributed training across heterogeneous GPU environments. In this paper, we present a comprehensive analysis of the challenges involved in implementing 3D parallelism in such environments, addressing critical issues such as the need for symmetric tensor parallelism, efficient gradient synchronization in asymmetric pipeline parallelism, and the trade-offs between memory utilization and computational efficiency. Building upon these insights, we introduce AutoHet, a novel system that automatically identifies the optimal parallelism plan for distributed training on heterogeneous GPUs. AutoHet supports asymmetric 3D parallelism structures and facilitates fine-grained workload distribution. We propose a theoretical model that frames the device grouping and load balancing as an optimization problem to minimize per-iteration training time, thus effectively balancing computing power and memory usage across GPUs with diverse capabilities. To enable elastic training upon spot instance preemption, AutoHet presents an efficient recovery strategy that prioritizes to retrieve training states from local nodes, and only downloads the missing checkpoints from the cloud storage. Our extensive evaluation, conducted on three large-scale models and utilizing combinations of three different GPU types, demonstrates that AutoHet outperforms existing DNN training systems, achieving up to a 1.79$\times$ speedup in training throughput compared with Megatron-LM and Whale, and a 4.38$\times$ speedup of recovery speed compared to a spot instance baseline.

</details>


### [4] [Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions](https://arxiv.org/abs/2512.20967)
*Linggao Kong,Yuedong Xu,Lei Jiao,Chuan Xu*

Main category: cs.DC

TL;DR: 该论文提出了一种基于预测的在线调度框架，通过混合使用spot和on-demand实例来降低大模型微调成本，同时保证截止时间要求。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模增大，微调成本急剧上升。GPU spot实例虽然成本较低，但其价格和可用性波动大，使得满足截止时间要求的调度变得困难。

Method: 1) 证明spot市场价格和可用性的可预测性；2) 建立整数规划模型捕捉混合实例使用；3) 提出基于预测的在线分配算法（使用commitment level）；4) 当预测不准确时，提出无预测的补充算法；5) 开发策略选择算法从参数化策略池中学习最优策略。

Result: 1) 预测算法在预测误差减小时性能界限更紧；2) 策略选择算法具有O(√T)的遗憾界；3) 实验表明该框架能自适应选择最优策略，比基线方法提升效用达54.8%。

Conclusion: 该研究展示了spot市场价格的可预测性，并提出了一个自适应在线调度框架，能够在动态的spot市场环境和预测质量变化下有效降低大模型微调成本。

Abstract: As foundation models grow in size, fine-tuning them becomes increasingly expensive. While GPU spot instances offer a low-cost alternative to on-demand resources, their volatile prices and availability make deadline-aware scheduling particularly challenging. We tackle this difficulty by using a mix of spot and on-demand instances. Distinctively, we show the predictability of prices and availability in a spot instance market, the power of prediction in enabling cost-efficient scheduling and its sensitivity to estimation errors. An integer programming problem is formulated to capture the use of mixed instances under both the price and availability dynamics. We propose an online allocation algorithm with prediction based on the committed horizon control approach that leverages a \emph{commitment level} to enforce the partial sequence of decisions. When this prediction becomes inaccurate, we further present a complementary online algorithm without predictions. An online policy selection algorithm is developed that learns the best policy from a pool constructed by varying the parameters of both algorithms. We prove that the prediction-based algorithm achieves tighter performance bounds as prediction error decreases, while the policy selection algorithm possesses a regret bound of $\mathcal{O}(\sqrt{T})$. Experimental results demonstrate that our online framework can adaptively select the best policy under varying spot market dynamics and prediction quality, consistently outperforming baselines and improving utility by up to 54.8\%.

</details>


### [5] [Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality](https://arxiv.org/abs/2512.20968)
*Sirui Chen,Jingji Chen,Siqi Zhu,Ziheng Jiang,Yanghua Peng,Xuehai Qian*

Main category: cs.DC

TL;DR: Mesh-Attention是一种新的分布式注意力算法，通过二维分块计算降低通信计算比，相比Ring-Attention在256个GPU上平均提速2.9倍，通信量减少79%


<details>
  <summary>Details</summary>
Motivation: 现有分布式注意力方法（如Ring-Attention）存在通信开销大的可扩展性问题，限制了LLM上下文窗口的扩展

Method: 提出Mesh-Attention算法：1）使用基于矩阵的新模型重新设计分布式注意力空间；2）为每个GPU分配二维计算块而非一维行列；3）通过贪婪算法高效搜索调度空间；4）允许通过不同分块形状调整通信计算比

Result: 在256个GPU上实现最高3.4倍（平均2.9倍）加速，通信量减少最高85.4%（平均79%），理论分析显示通信复杂度显著降低，具有良好的可扩展性

Conclusion: Mesh-Attention通过优化通信计算比，显著提升了分布式注意力的效率和可扩展性，为大规模LLM部署提供了有效的解决方案

Abstract: Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-Attention, by rethinking the design space of distributed attention with a new matrix-based model. Our method assigns a two-dimensional tile -- rather than one-dimensional row or column -- of computation blocks to each GPU to achieve higher efficiency through lower communication-computation (CommCom) ratio. The general approach covers Ring-Attention as a special case, and allows the tuning of CommCom ratio with different tile shapes. Importantly, we propose a greedy algorithm that can efficiently search the scheduling space within the tile with restrictions that ensure efficient communication among GPUs. The theoretical analysis shows that Mesh-Attention leads to a much lower communication complexity and exhibits good scalability comparing to other current algorithms.
  Our extensive experiment results show that Mesh-Attention can achieve up to 3.4x speedup (2.9x on average) and reduce the communication volume by up to 85.4% (79.0% on average) on 256 GPUs. Our scalability results further demonstrate that Mesh-Attention sustains superior performance as the system scales, substantially reducing overhead in large-scale deployments. The results convincingly confirm the advantage of Mesh-Attention.

</details>


### [6] [ESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting](https://arxiv.org/abs/2512.21009)
*S. M. Shovan,Arindam Khanda,Sanjukta Bhowmick,Sajal K. Das*

Main category: cs.DC

TL;DR: ESCHER：一种面向GPU的并行数据结构，用于高效管理大规模动态超图，并设计了超图三元组计数更新框架，显著提升了动态超图分析性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的复杂网络通常包含超越成对关系的高阶交互，这些交互可以用超图建模。分析超图属性（如三元组计数）对于揭示传统图无法捕捉的复杂群体交互模式至关重要。然而，大规模动态超图分析面临计算挑战，缺乏专门的软件包和数据结构，使得这一领域研究不足。

Method: 提出了ESCHER（Efficient and Scalable Hypergraph Evolution Representation），一种GPU中心的并行数据结构，用于高效管理大规模超图动态变化。同时设计了超图三元组计数更新框架，最小化冗余计算，充分利用ESCHER的动态操作能力。

Result: 在真实世界和合成数据集上的实验表明，该方法在超边基、关联顶点基和时间三元组等多种超图三元组计数任务中，性能显著优于现有最优方法，分别实现了最高104.5倍、473.7倍和112.5倍的加速。

Conclusion: ESCHER及其配套的三元组计数更新框架为大规模动态超图分析提供了高效解决方案，填补了该领域缺乏专门数据结构和软件工具的空白，显著提升了超图动态分析的性能。

Abstract: Higher-order interactions beyond pairwise relationships in large complex networks are often modeled as hypergraphs. Analyzing hypergraph properties such as triad counts is essential, as hypergraphs can reveal intricate group interaction patterns that conventional graphs fail to capture. In real-world scenarios, these networks are often large and dynamic, introducing significant computational challenges. Due to the absence of specialized software packages and data structures, the analysis of large dynamic hypergraphs remains largely unexplored. Motivated by this gap, we propose ESCHER, a GPU-centric parallel data structure for Efficient and Scalable Hypergraph Evolution Representation, designed to manage large scale hypergraph dynamics efficiently. We also design a hypergraph triad-count update framework that minimizes redundant computation while fully leveraging the capabilities of ESCHER for dynamic operations. We validate the efficacy of our approach across multiple categories of hypergraph triad counting, including hyperedge-based, incident-vertex-based, and temporal triads. Empirical results on both large real-world and synthetic datasets demonstrate that our proposed method outperforms existing state-of-the-art methods, achieving speedups of up to 104.5x, 473.7x, and 112.5x for hyperedge-based, incident-vertex-based, and temporal triad types, respectively.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [NotSoTiny: A Large, Living Benchmark for RTL Code Generation](https://arxiv.org/abs/2512.20823)
*Razine Moundir Ghorab,Emanuele Parisi,Cristian Gutierrez,Miquel Alberti-Binimelis,Miquel Moreto,Dario Garcia-Gasulla,Gokcen Kestor*

Main category: cs.AR

TL;DR: NotSoTiny是一个评估LLM生成RTL代码能力的基准测试，基于Tiny Tapeout社区的真实硬件设计，解决了现有基准测试规模小、设计简单、验证不严格和数据污染等问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在RTL代码生成方面显示出潜力，但现有评估基准存在局限性：规模有限、偏向简单设计、验证不严格、容易受到数据污染。需要更真实、更具挑战性的基准来推动该领域发展。

Method: 从Tiny Tapeout社区的数百个真实硬件设计中构建基准，采用自动化流水线去除重复设计、验证正确性，并定期纳入新设计以匹配Tiny Tapeout发布计划，从而缓解数据污染问题。

Result: 评估结果显示NotSoTiny任务比现有基准更具挑战性，证明了其在克服LLM应用于硬件设计当前局限性方面的有效性，能够指导该有前景技术的改进。

Conclusion: NotSoTiny基准通过基于真实硬件设计、结构丰富且上下文感知的RTL生成评估，为LLM在硬件设计领域的应用提供了更可靠、更具挑战性的评估框架，有助于推动该技术的发展。

Abstract: LLMs have shown early promise in generating RTL code, yet evaluating their capabilities in realistic setups remains a challenge. So far, RTL benchmarks have been limited in scale, skewed toward trivial designs, offering minimal verification rigor, and remaining vulnerable to data contamination. To overcome these limitations and to push the field forward, this paper introduces NotSoTiny, a benchmark that assesses LLM on the generation of structurally rich and context-aware RTL. Built from hundreds of actual hardware designs produced by the Tiny Tapeout community, our automated pipeline removes duplicates, verifies correctness and periodically incorporates new designs to mitigate contamination, matching Tiny Tapeout release schedule. Evaluation results show that NotSoTiny tasks are more challenging than prior benchmarks, emphasizing its effectiveness in overcoming current limitations of LLMs applied to hardware design, and in guiding the improvement of such promising technology.

</details>


### [8] [ElfCore: A 28nm Neural Processor Enabling Dynamic Structured Sparse Training and Online Self-Supervised Learning with Activity-Dependent Weight Update](https://arxiv.org/abs/2512.21153)
*Zhe Su,Giacomo Indiveri*

Main category: cs.AR

TL;DR: ElfCore是一款28nm数字脉冲神经网络处理器，专为事件驱动传感信号处理设计，集成了本地在线自监督学习引擎、动态结构化稀疏训练引擎和活动依赖稀疏权重更新机制，在功耗、内存和网络容量效率方面显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 开发一款高效的脉冲神经网络处理器，解决事件驱动传感信号处理中的关键挑战：无需标记输入的多层时序学习、高精度稀疏训练以及基于网络动态的智能权重更新。

Method: 设计了ElfCore处理器，集成了三个核心技术：1) 本地在线自监督学习引擎，支持多层时序学习；2) 动态结构化稀疏训练引擎，实现高精度稀疏到稀疏学习；3) 活动依赖稀疏权重更新机制，仅基于输入活动和网络动态选择性更新权重。

Result: 在姿态识别、语音和生物医学信号处理等任务上，ElfCore相比最先进解决方案：功耗降低高达16倍，片上内存需求减少3.8倍，网络容量效率提升5.9倍。

Conclusion: ElfCore通过创新的架构设计，在事件驱动传感信号处理领域实现了显著的性能提升，为低功耗、高效率的脉冲神经网络处理提供了有效的硬件解决方案。

Abstract: In this paper, we present ElfCore, a 28nm digital spiking neural network processor tailored for event-driven sensory signal processing. ElfCore is the first to efficiently integrate: (1) a local online self-supervised learning engine that enables multi-layer temporal learning without labeled inputs; (2) a dynamic structured sparse training engine that supports high-accuracy sparse-to-sparse learning; and (3) an activity-dependent sparse weight update mechanism that selectively updates weights based solely on input activity and network dynamics. Demonstrated on tasks including gesture recognition, speech, and biomedical signal processing, ElfCore outperforms state-of-the-art solutions with up to 16X lower power consumption, 3.8X reduced on-chip memory requirements, and 5.9X greater network capacity efficiency.

</details>
