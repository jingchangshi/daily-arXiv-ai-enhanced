{"id": "2511.16368", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.16368", "abs": "https://arxiv.org/abs/2511.16368", "authors": ["Yingjie Qi", "Jianlei Yang", "Rubing Yang", "Cenlin Duan", "Xiaolin He", "Ziyan He", "Weitao Pan", "Weisheng Zhao"], "title": "CIMinus: Empowering Sparse DNN Workloads Modeling and Exploration on SRAM-based CIM Architectures", "comment": "14 pages, 12 figures, accepted by IEEE Transactions on Computers", "summary": "Compute-in-memory (CIM) has emerged as a pivotal direction for accelerating workloads in the field of machine learning, such as Deep Neural Networks (DNNs). However, the effective exploitation of sparsity in CIM systems presents numerous challenges, due to the inherent limitations in their rigid array structures. Designing sparse DNN dataflows and developing efficient mapping strategies also become more complex when accounting for diverse sparsity patterns and the flexibility of a multi-macro CIM structure. Despite these complexities, there is still an absence of a unified systematic view and modeling approach for diverse sparse DNN workloads in CIM systems. In this paper, we propose CIMinus, a framework dedicated to cost modeling for sparse DNN workloads on CIM architectures. It provides an in-depth energy consumption analysis at the level of individual components and an assessment of the overall workload latency. We validate CIMinus against contemporary CIM architectures and demonstrate its applicability in two use-cases. These cases provide valuable insights into both the impact of sparsity patterns and the effectiveness of mapping strategies, bridging the gap between theoretical design and practical implementation."}
{"id": "2511.16374", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16374", "abs": "https://arxiv.org/abs/2511.16374", "authors": ["Abdelrahman Helaly", "Nourhan Sakr", "Kareem Madkour", "Ilhami Torunoglu"], "title": "Unsupervised Graph Neural Network Framework for Balanced Multipatterning in Advanced Electronic Design Automation Layouts", "comment": "manuscript under review", "summary": "Multipatterning is an essential decomposition strategy in electronic design automation (EDA) that overcomes lithographic limitations when printing dense circuit layouts. Although heuristic-based backtracking and SAT solvers can address these challenges, they often struggle to simultaneously handle both complex constraints and secondary objectives. In this study, we present a hybrid workflow that casts multipatterning as a variant of a constrained graph coloring problem with the primary objective of minimizing feature violations and a secondary objective of balancing the number of features on each mask. Our pipeline integrates two main components: (1) A GNN-based agent, trained in an unsupervised manner to generate initial color predictions, which are refined by (2) refinement strategies (a GNN-based heuristic and simulated annealing) that together enhance solution quality and balance. Experimental evaluation in both proprietary data sets and publicly available open source layouts demonstrate complete conflict-free decomposition and consistent color balancing. The proposed framework provides a reproducible, data-efficient and deployable baseline for scalable layout decomposition in EDA workflows."}
{"id": "2511.15950", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.15950", "abs": "https://arxiv.org/abs/2511.15950", "authors": ["Michael V. DeBole", "Rathinakumar Appuswamy", "Neil McGlohon", "Brian Taba", "Steven K. Esser", "Filipp Akopyan", "John V. Arthur", "Arnon Amir", "Alexander Andreopoulos", "Peter J. Carlson", "Andrew S. Cassidy", "Pallab Datta", "Myron D. Flickner", "Rajamohan Gandhasri", "Guillaume J. Garreau", "Megumi Ito", "Jennifer L. Klamo", "Jeffrey A. Kusnitz", "Nathaniel J. McClatchey", "Jeffrey L. McKinstry", "Tapan K. Nayak", "Carlos Ortega Otero", "Hartmut Penner", "William P. Risk", "Jun Sawada", "Jay Sivagnaname", "Daniel F. Smith", "Rafael Sousa", "Ignacio Terrizzano", "Takanori Ueda", "Trent Gray-Donald", "David Cox", "Dharmendra S. Modha"], "title": "A Scalable NorthPole System with End-to-End Vertical Integration for Low-Latency and Energy-Efficient LLM Inference", "comment": null, "summary": "A vertically integrated, end-to-end, research prototype system combines 288 NorthPole neural inference accelerator cards, offline training algorithms, a high-performance runtime stack, and a containerized inference pipeline to deliver a scalable and efficient cloud inference service. The system delivers 115 peta-ops at 4-bit integer precision and 3.7 PB/s of memory bandwidth across 18 2U servers, while consuming only 30 kW of power and weighing 730 kg in a 0.67 m^2 42U rack footprint. The system can run 3 simultaneous instances of the 8-billion-parameter open-source IBM Granite-3.3-8b-instruct model at 2,048 context length with 28 simultaneous users and a per-user inter-token latency of 2.8 ms. The system is scalable, modular, and reconfigurable, supporting various model sizes and context lengths, and is ideal for deploying agentic workflows for enterprise AI applications in existing data center (cloud, on-prem) environments. For example, the system can support 18 instances of a 3-billion-parameter model or a single instance of a 70-billion-parameter model."}
{"id": "2511.16041", "categories": ["cs.DC", "cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.16041", "abs": "https://arxiv.org/abs/2511.16041", "authors": ["Chengyue Wang", "Wesley Pang", "Xinrui Wu", "Gregory Jun", "Luis Romero", "Endri Taka", "Diana Marculescu", "Tony Nowatzki", "Pranathi Vasireddy", "Joseph Melber", "Deming Chen", "Jason Cong"], "title": "Can Asymmetric Tile Buffering Be Beneficial?", "comment": null, "summary": "General matrix multiplication (GEMM) is the computational backbone of modern AI workloads, and its efficiency is critically dependent on effective tiling strategies. Conventional approaches employ symmetric tile buffering, where the buffered tile size of the input $A$ along the dimension $M$ matches the output tile size of $C$.\n  In this paper, we introduce asymmetric tile buffering (ATB), a simple but powerful technique that decouples the buffered tile dimensions of the input and output operands. We show, for the first time, that ATB is both practical and highly beneficial. To explain this effect, we develop a performance model that incorporates both the benefits of ATB (higher arithmetic intensity) and its overheads (higher kernel switching costs), providing insight into how to select effective ATB tiling factors. As a case study, we apply ATB to AMD's latest XDNA2 AI Engine (AIE), achieving up to a 4.54x speedup, from 4.8 to 24.6 TFLOPS on mixed-precision BFP16--BF16 GEMM, establishing a new performance record for XDNA2 AIE."}
{"id": "2511.15819", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.15819", "abs": "https://arxiv.org/abs/2511.15819", "authors": ["Bohdan Liesnikov", "David Binder", "Tim Süberkrüb"], "title": "Filling the Gaps of Polarity: Implementing Dependent Data and Codata Types with Implicit Arguments", "comment": null, "summary": "The expression problem describes a fundamental tradeoff between two types of extensibility: extending a type with new operations, such as by pattern matching on an algebraic data type in functional programming, and extending a type with new constructors, such as by adding a new object implementing an interface in object-oriented programming. Most dependently typed languages have good support for the former style through inductive types, but support for the latter style through coinductive types is usually much poorer. Polarity is a language that treats both kinds of types symmetrically and allows the developer to switch between type representations.However, it currently lacks several features expected of a state-of-the-art dependently typed language, such as implicit arguments. The central aim of this paper is to provide an algorithmic type system and inference algorithm for implicit arguments that respect the core symmetry of the language. Our work provides two key contributions: a complete algorithmic description of the type system backing Polarity, and a comprehensive description of a unification algorithm that covers arbitrary inductive and coinductive types. We give rules for reduction semantics, conversion checking, and a unification algorithm for pattern-matching, which are essential for a usable implementation. A work-in-progress implementation of the algorithms in this paper is available at https://polarity-lang.github.io/. We expect that the comprehensive account of the unification algorithm and our design decisions can serve as a blueprint for other dependently typed languages that support inductive and coinductive types symmetrically."}
{"id": "2511.15950", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.15950", "abs": "https://arxiv.org/abs/2511.15950", "authors": ["Michael V. DeBole", "Rathinakumar Appuswamy", "Neil McGlohon", "Brian Taba", "Steven K. Esser", "Filipp Akopyan", "John V. Arthur", "Arnon Amir", "Alexander Andreopoulos", "Peter J. Carlson", "Andrew S. Cassidy", "Pallab Datta", "Myron D. Flickner", "Rajamohan Gandhasri", "Guillaume J. Garreau", "Megumi Ito", "Jennifer L. Klamo", "Jeffrey A. Kusnitz", "Nathaniel J. McClatchey", "Jeffrey L. McKinstry", "Tapan K. Nayak", "Carlos Ortega Otero", "Hartmut Penner", "William P. Risk", "Jun Sawada", "Jay Sivagnaname", "Daniel F. Smith", "Rafael Sousa", "Ignacio Terrizzano", "Takanori Ueda", "Trent Gray-Donald", "David Cox", "Dharmendra S. Modha"], "title": "A Scalable NorthPole System with End-to-End Vertical Integration for Low-Latency and Energy-Efficient LLM Inference", "comment": null, "summary": "A vertically integrated, end-to-end, research prototype system combines 288 NorthPole neural inference accelerator cards, offline training algorithms, a high-performance runtime stack, and a containerized inference pipeline to deliver a scalable and efficient cloud inference service. The system delivers 115 peta-ops at 4-bit integer precision and 3.7 PB/s of memory bandwidth across 18 2U servers, while consuming only 30 kW of power and weighing 730 kg in a 0.67 m^2 42U rack footprint. The system can run 3 simultaneous instances of the 8-billion-parameter open-source IBM Granite-3.3-8b-instruct model at 2,048 context length with 28 simultaneous users and a per-user inter-token latency of 2.8 ms. The system is scalable, modular, and reconfigurable, supporting various model sizes and context lengths, and is ideal for deploying agentic workflows for enterprise AI applications in existing data center (cloud, on-prem) environments. For example, the system can support 18 instances of a 3-billion-parameter model or a single instance of a 70-billion-parameter model."}
{"id": "2511.16177", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.16177", "abs": "https://arxiv.org/abs/2511.16177", "authors": ["Thomas Collignon", "Kouds Halitim", "Raphaël Bleuse", "Sophie Cerf", "Bogdan Robu", "Éric Rutten", "Lionel Seinturier", "Alexandre van Kempen"], "title": "Mitigating Shared Storage Congestion Using Control Theory", "comment": null, "summary": "Efficient data access in High-Performance Computing (HPC) systems is essential to the performance of intensive computing tasks. Traditional optimizations of the I/O stack aim to improve peak performance but are often workload specific and require deep expertise, making them difficult to generalize or re-use. In shared HPC environments, resource congestion can lead to unpredictable performance, causing slowdowns and timeouts. To address these challenges, we propose a self-adaptive approach based on Control Theory to dynamically regulate client-side I/O rates. Our approach leverages a small set of runtime system load metrics to reduce congestion and enhance performance stability. We implement a controller in a multi-node cluster and evaluate it on a real testbed under a representative workload. Experimental results demonstrate that our method effectively mitigates I/O congestion, reducing total runtime by up to 20% and lowering tail latency, while maintaining stable performance."}
{"id": "2511.15820", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.15820", "abs": "https://arxiv.org/abs/2511.15820", "authors": ["Ashton Wiersdorf", "Ben Greenman"], "title": "Chorex: Restartable, Language-Integrated Choreographies", "comment": null, "summary": "We built Chorex, a language that brings choreographic programming to Elixir as a path toward robust distributed applications. Chorex is unique among choreographic languages because it tolerates failure among actors: when an actor crashes, Chorex spawns a new process, restores state using a checkpoint, and updates the network configuration for all actors. Chorex also proves that full-featured choreographies can be implemented via metaprogramming, and that doing so achieves tight integration with the host language. For example, mismatches between choreography requirements and an actor implementation are reported statically and in terms of source code rather than macro-expanded code. This paper illustrates Chorex on several examples, ranging from a higher-order bookseller to a secure remote password protocol, details its implementation, and measures the overhead of checkpointing. We conjecture that Chorex's projection strategy, which outputs sets of stateless functions, is a viable approach for other languages to support restartable actors."}
{"id": "2511.15957", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.15957", "abs": "https://arxiv.org/abs/2511.15957", "authors": ["Nasit S Sony", "Xianzhong Ding"], "title": "Optimizing Communication in Byzantine Agreement Protocols with Slim-HBBFT", "comment": "4", "summary": "Byzantine agreement protocols in asynchronous networks have received renewed interest because they do not rely on network behavior to achieve termination. Conventional asynchronous Byzantine agreement protocols require every party to broadcast its requests (e.g., transactions), and at the end of the protocol, parties agree on one party's request. If parties agree on one party's requests while exchanging every party's request, the protocol becomes expensive. These protocols are used to design an atomic broadcast (ABC) protocol where parties agree on $\\langle n-f \\rangle$ parties' requests (assuming $n=3f+1$, where $n$ is the total number of parties, and $f$ is the number of Byzantine parties). Although the parties agree on a subset of requests in the ABC protocol, if the requests do not vary (are duplicated), investing in a costly protocol is not justified. We propose Slim-HBBFT, an atomic broadcast protocol that considers requests from a fraction of $n$ parties and improves communication complexity by a factor of $O(n)$. At the core of our design is a prioritized provable-broadcast (P-PB) protocol that generates proof of broadcast only for selected parties. We use the P-PB protocol to design the Slim-HBBFT atomic broadcast protocol. Additionally, we conduct a comprehensive security analysis to demonstrate that Slim-HBBFT satisfies the properties of the Asynchronous Common Subset protocol, ensuring robust security and reliability."}
{"id": "2511.15821", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.15821", "abs": "https://arxiv.org/abs/2511.15821", "authors": ["Fumika Mochizuki", "Tetsuro Yamazaki", "Shigeru Chiba"], "title": "BlueScript: A Disaggregated Virtual Machine for Microcontrollers", "comment": null, "summary": "Virtual machines (VMs) are highly beneficial for microcontroller development. \nIn particular, interactive programming environments greatly facilitate iterative development processes, \nand higher execution speeds expand the range of applications that can be developed. \nHowever, due to their limited memory size, microcontroller VMs provide a limited set of features. \nWidely used VMs for microcontrollers often lack interactive responsiveness and/or high execution speed. \nWhile researchers have investigated offloading certain VM components to other machines,the types of components that can be offloaded are still restricted. \nIn this paper, we propose a disaggregated VM that offloads as many components as possible to a host machine. \nThis makes it possible to exploit the abundant memory of the host machine and its powerful processing capability to provide rich features through the VM. \nAs an instance of a disaggregated VM, we design and implement a BlueScript VM. \nThe BlueScript VM is a virtual machine for microcontrollers that provides an interactive development environment. \nWe offload most of the components of the BlueScript VM to a host machine. \nTo reduce communication overhead between the host machine and the microcontroller,  \nwe employed a data structure called a shadow machine on the host machine, \nwhich mirrors the execution state of the microcontroller. \nThrough our experiments, we confirmed that offloading components does not seriously compromise their expected benefits.  \nWe assess that an offloaded incremental compiler results in faster execution speed than MicroPython and Espruino,  \nwhile keeping interactivity comparable with MicroPython.  \nIn addition, our experiments observe that the offloaded dynamic compiler improves VM performance. \nThrough this investigation, we demonstrate the feasibility of providing rich features even on VMs for memory-limited microcontrollers."}
{"id": "2511.15977", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2511.15977", "abs": "https://arxiv.org/abs/2511.15977", "authors": ["Daniel Mas Montserrat", "Ray Verma", "Míriam Barrabés", "Francisco M. de la Vega", "Carlos D. Bustamante", "Alexander G. Ioannidis"], "title": "Efficient Chromosome Parallelization for Precision Medicine Genomic Workflows", "comment": "Accepted at AAAI 2026", "summary": "Large-scale genomic workflows used in precision medicine can process datasets spanning tens to hundreds of gigabytes per sample, leading to high memory spikes, intensive disk I/O, and task failures due to out-of-memory errors. Simple static resource allocation methods struggle to handle the variability in per-chromosome RAM demands, resulting in poor resource utilization and long runtimes. In this work, we propose multiple mechanisms for adaptive, RAM-efficient parallelization of chromosome-level bioinformatics workflows. First, we develop a symbolic regression model that estimates per-chromosome memory consumption for a given task and introduces an interpolating bias to conservatively minimize over-allocation. Second, we present a dynamic scheduler that adaptively predicts RAM usage with a polynomial regression model, treating task packing as a Knapsack problem to optimally batch jobs based on predicted memory requirements. Additionally, we present a static scheduler that optimizes chromosome processing order to minimize peak memory while preserving throughput. Our proposed methods, evaluated on simulations and real-world genomic pipelines, provide new mechanisms to reduce memory overruns and balance load across threads. We thereby achieve faster end-to-end execution, showcasing the potential to optimize large-scale genomic workflows."}
{"id": "2511.16080", "categories": ["cs.PL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16080", "abs": "https://arxiv.org/abs/2511.16080", "authors": ["Sungbin Moon", "Jiho Park", "Suyoung Hwang", "Donghyun Koh", "Seunghyun Moon", "Minhyeong Lee"], "title": "Operon: Incremental Construction of Ragged Data via Named Dimensions", "comment": null, "summary": "Modern data processing workflows frequently encounter ragged data: collections with variable-length elements that arise naturally in domains like natural language processing, scientific measurements, and autonomous AI agents. Existing workflow engines lack native support for tracking the shapes and dependencies inherent to ragged data, forcing users to manage complex indexing and dependency bookkeeping manually. We present Operon, a Rust-based workflow engine that addresses these challenges through a novel formalism of named dimensions with explicit dependency relations. Operon provides a domain-specific language where users declare pipelines with dimension annotations that are statically verified for correctness, while the runtime system dynamically schedules tasks as data shapes are incrementally discovered during execution. We formalize the mathematical foundation for reasoning about partial shapes and prove that Operon's incremental construction algorithm guarantees deterministic and confluent execution in parallel settings. The system's explicit modeling of partially-known states enables robust persistence and recovery mechanisms, while its per-task multi-queue architecture achieves efficient parallelism across heterogeneous task types. Empirical evaluation demonstrates that Operon outperforms an existing workflow engine with 14.94x baseline overhead reduction while maintaining near-linear end-to-end output rates as workloads scale, making it particularly suitable for large-scale data generation pipelines in machine learning applications."}
{"id": "2511.16041", "categories": ["cs.DC", "cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.16041", "abs": "https://arxiv.org/abs/2511.16041", "authors": ["Chengyue Wang", "Wesley Pang", "Xinrui Wu", "Gregory Jun", "Luis Romero", "Endri Taka", "Diana Marculescu", "Tony Nowatzki", "Pranathi Vasireddy", "Joseph Melber", "Deming Chen", "Jason Cong"], "title": "Can Asymmetric Tile Buffering Be Beneficial?", "comment": null, "summary": "General matrix multiplication (GEMM) is the computational backbone of modern AI workloads, and its efficiency is critically dependent on effective tiling strategies. Conventional approaches employ symmetric tile buffering, where the buffered tile size of the input $A$ along the dimension $M$ matches the output tile size of $C$.\n  In this paper, we introduce asymmetric tile buffering (ATB), a simple but powerful technique that decouples the buffered tile dimensions of the input and output operands. We show, for the first time, that ATB is both practical and highly beneficial. To explain this effect, we develop a performance model that incorporates both the benefits of ATB (higher arithmetic intensity) and its overheads (higher kernel switching costs), providing insight into how to select effective ATB tiling factors. As a case study, we apply ATB to AMD's latest XDNA2 AI Engine (AIE), achieving up to a 4.54x speedup, from 4.8 to 24.6 TFLOPS on mixed-precision BFP16--BF16 GEMM, establishing a new performance record for XDNA2 AIE."}
{"id": "2511.16177", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.16177", "abs": "https://arxiv.org/abs/2511.16177", "authors": ["Thomas Collignon", "Kouds Halitim", "Raphaël Bleuse", "Sophie Cerf", "Bogdan Robu", "Éric Rutten", "Lionel Seinturier", "Alexandre van Kempen"], "title": "Mitigating Shared Storage Congestion Using Control Theory", "comment": null, "summary": "Efficient data access in High-Performance Computing (HPC) systems is essential to the performance of intensive computing tasks. Traditional optimizations of the I/O stack aim to improve peak performance but are often workload specific and require deep expertise, making them difficult to generalize or re-use. In shared HPC environments, resource congestion can lead to unpredictable performance, causing slowdowns and timeouts. To address these challenges, we propose a self-adaptive approach based on Control Theory to dynamically regulate client-side I/O rates. Our approach leverages a small set of runtime system load metrics to reduce congestion and enhance performance stability. We implement a controller in a multi-node cluster and evaluate it on a real testbed under a representative workload. Experimental results demonstrate that our method effectively mitigates I/O congestion, reducing total runtime by up to 20% and lowering tail latency, while maintaining stable performance."}
{"id": "2511.16193", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16193", "abs": "https://arxiv.org/abs/2511.16193", "authors": ["Rongxin Cheng", "Kai Zhou", "Xingda Wei", "Siyuan Liu", "Mingcong Han", "Mingjing Ai", "Yeju Zhou", "Baoquan Zhong", "Wencong Xiao", "Xin Liu", "Rong Chen", "Haibo Chen"], "title": "Fast LLM Post-training via Decoupled and Best-of-N Speculation", "comment": null, "summary": "Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. SpecActor achieves fast rollout with speculative decoding that deploys a fast path (e.g., a smaller model) to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges in speculative rollout by (1) a \\emph{dynamic decoupled speculation} execution method that maximizes the GPU computational efficiency to realize speedup for large-batch execution -- a configuration common in training but unfriendly to speculative execution and (2) a \\emph{dynamic Best-of-N speculation} method that selects and combines different drafting methods according to the rollout progress. It substantially improves the speculation accuracy even when the best drafting method is unknown a priori, meanwhile without requiring adding extra computation resources. {\\sys} is {1.3--1.7}\\,$\\times$ faster than common post-training baselines, and is {1.3--1.5}\\,$\\times$ faster compared to naively adopting speculative decoding for rollout."}
{"id": "2511.16450", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.16450", "abs": "https://arxiv.org/abs/2511.16450", "authors": ["Ziyue Xu", "Zhihong Zhang", "Holger R. Roth", "Chester Chen", "Yan Cheng", "Andrew Feng"], "title": "Optimizing Federated Learning in the Era of LLMs: Message Quantization and Streaming", "comment": "FLLM 2025", "summary": "Federated Learning (FL) offers a promising solution for training machine learning models across distributed data sources while preserving data privacy. However, FL faces critical challenges related to communication overhead and local resource constraints, especially in the era of Large Language Models (LLMs) with billions of parameters. The sheer size of these models exacerbates both memory and communication constraints, making efficient transmission and processing essential for practical deployment. NVIDIA FLARE, an open-source SDK for federated learning, addresses these challenges by introducing advanced communication capabilities. Building upon existing solutions for large object streaming, we enhance FL workflows for LLMs through two key techniques: message quantization and container/file streaming. Quantization reduces message size, while streaming enables efficient memory management, improving scalability and integration with existing workflows. These advancements significantly enhance the robustness and efficiency of FL with LLMs, ensuring better performance in real-world federated learning scenarios."}
{"id": "2511.16533", "categories": ["cs.DC", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.16533", "abs": "https://arxiv.org/abs/2511.16533", "authors": ["Nithin Salevemula", "Shreyas Pai"], "title": "Distributed MIS Algorithms for Rational Agents using Games", "comment": null, "summary": "We study the problem of computing a Maximal Independent Set (MIS) in distributed networks where each node is a rational agent whose payoff depends on whether it joins the MIS. Classical distributed algorithms assume that nodes follow the prescribed protocol, but this assumption fails when nodes are strategic and may deviate if doing so increases their expected utility.\n  Standard MIS algorithms rely on honest randomness or unique identifiers to break symmetry. In rational settings, however, agents may manipulate randomness, and relying solely on identifiers can create unfairness, giving some nodes zero probability of joining the MIS and thus no incentive to participate. To address these issues, we propose two algorithms based on a utility model in which agents seek locally correct solutions while also having preferences over which solution is chosen. Randomness in our algorithms is generated through pairwise interactions between neighboring nodes, viewed as simple games in which no single node can unilaterally affect the outcome. This allows symmetry breaking while remaining compatible with rational behavior.\n  For both algorithms, we prove that at every stage of the execution, given any history, no agent can increase its expected utility through a unilateral deviation, assuming others follow the algorithm. This gives a stronger guarantee than Trembling-Hand Perfect Equilibrium. When all nodes follow the protocol, every node has a positive probability of joining the MIS, and the final output is a correct MIS. Under mild additional assumptions, both algorithms terminate in $O(\\log n)$ rounds with high probability."}
