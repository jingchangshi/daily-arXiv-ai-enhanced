<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Enforcing Temporal Constraints for LLM Agents](https://arxiv.org/abs/2512.23738)
*Adharsh Kamath,Sishen Zhang,Calvin Xu,Shubham Ugare,Gagandeep Singh,Sasa Misailovic*

Main category: cs.PL

TL;DR: Agent-C是一个为LLM智能体提供运行时安全保障的框架，确保智能体遵守形式化的时序安全属性，防止违反顺序约束的安全策略。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体被部署在安全关键应用中，但现有的护栏系统无法防止违反时序安全策略的行为，例如在用户认证前访问敏感数据或向未授权支付方式退款。现有方法依赖不精确的自然语言指令或事后监控，无法提供形式化保证。

Method: Agent-C引入领域特定语言表达时序属性，将规范转换为一阶逻辑，使用SMT求解在token生成期间检测不合规的智能体动作。当LLM试图生成不合规的工具调用时，利用约束生成技术确保每个动作都符合规范，并为不合规动作生成合规替代方案。

Result: 在零售客服和机票预订系统等真实应用中评估显示，Agent-C实现了完美的安全性（100%合规，0%伤害），同时相比最先进的护栏和无限制智能体提高了任务效用。在Claude Sonnet 4.5上将合规率从77.4%提升到100%，GPT-5从83.7%提升到100%，同时分别将效用从71.8%提升到75.2%和66.1%提升到70.6%。

Conclusion: Agent-C为可靠的智能体推理建立了新的最先进前沿，通过形式化方法和运行时约束生成，在确保100%时序安全合规的同时提高任务效用，解决了现有护栏系统无法保证时序安全策略的关键问题。

Abstract: LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning.

</details>


### [2] [Towards representation agnostic probabilistic programming](https://arxiv.org/abs/2512.23740)
*Ole Fenske,Maximilian Popko,Sebastian Bader,Thomas Kirste*

Main category: cs.PL

TL;DR: 提出一种因子抽象作为通用接口，支持混合不同表示形式的概率编程，解决现有工具耦合模型表示与推理算法的问题。


<details>
  <summary>Details</summary>
Motivation: 当前概率编程语言和工具将模型表示与特定推理算法紧密耦合，阻碍了对新表示形式或混合离散-连续模型的实验。需要一种更灵活的框架来支持复杂混合模型的推理。

Method: 引入因子抽象概念，定义五种基本操作作为通用接口，无论底层表示形式如何都能操作因子。支持在统一框架内自由混合不同表示形式（如离散表、高斯分布、基于采样的方法）。

Result: 实现了表示无关的概率编程，能够在单一统一框架中混合不同表示形式，支持当前工具无法充分表达的复杂混合模型的实用推理。

Conclusion: 因子抽象为概率编程提供了通用接口，打破了模型表示与推理算法的紧耦合，使复杂混合模型的推理成为可能，为概率编程语言设计提供了新方向。

Abstract: Current probabilistic programming languages and tools tightly couple model representations with specific inference algorithms, preventing experimentation with novel representations or mixed discrete-continuous models. We introduce a factor abstraction with five fundamental operations that serve as a universal interface for manipulating factors regardless of their underlying representation. This enables representation-agnostic probabilistic programming where users can freely mix different representations (e.g. discrete tables, Gaussians distributions, sample-based approaches) within a single unified framework, allowing practical inference in complex hybrid models that current toolkits cannot adequately express.

</details>


### [3] [VGC: A High-Performance Zone-Based Garbage Collector Architecture for Python with Partitioning and Parallel Execution](https://arxiv.org/abs/2512.23768)
*Abdulla M*

Main category: cs.PL

TL;DR: VGC提出了一种新颖的双层垃圾回收框架，通过主动层和被动层的分离，结合编译时和运行时优化，为从嵌入式设备到高性能并行架构的多样化系统提供高效内存管理。


<details>
  <summary>Details</summary>
Motivation: 传统垃圾回收器在不同系统（从资源受限的嵌入式设备到高性能并行架构）中难以同时实现高效、低开销的内存管理，需要一种更灵活、可扩展的解决方案。

Method: 采用双层架构：主动VGC使用并发标记清除策略动态管理运行时对象；被动VGC在编译时通过预测性内存映射优化静态对象分配，将对象对齐到缓存边界以减少碎片。

Result: 相比分代回收器，在并行工作负载中暂停时间减少30%；内存碎片最小化，总内存使用减少25%；提供可预测的内存访问模式，提升现代并行应用的可扩展性。

Conclusion: VGC通过整合编译时和运行时优化，为内存密集型系统提供了一个稳健且适应性强的内存管理解决方案，适用于从低级到高级编程环境的广泛场景。

Abstract: The Virtual Garbage Collector (VGC) introduces a novel memory management framework designed to optimize performance across diverse systems, ranging from resource constrained embedded devices to high performance parallel architectures. Unlike conventional garbage collectors, VGC employs a dual layer architecture consisting of Active VGC and Passive VGC to enable efficient, low overhead memory management. Active VGC dynamically manages runtime objects using a concurrent mark and sweep strategy tailored for parallel workloads, reducing pause times by up to 30 percent compared to generational collectors in multithreaded benchmarks. Passive VGC operates at compile time and optimizes static object allocation through predictive memory mapping, minimizing fragmentation by aligning objects to cache boundaries. This separation of responsibilities ensures predictable memory access patterns, reduces total memory usage by up to 25 percent, and improves scalability for modern parallel applications. By integrating compile time and runtime optimizations, VGC provides a robust and adaptable solution for memory intensive systems across both low level and high level programming environments.

</details>


### [4] [State Space Estimation for DPOR-based Model Checkers](https://arxiv.org/abs/2512.23996)
*A. R. Balasubramanian,Mohammad Hossein Khoshechin Jorshari,Rupak Majumdar,Umang Mathur,Minjian Zhang*

Main category: cs.PL

TL;DR: 提出首个多项式时间无偏估计器，用于计算并发程序的Mazurkiewicz迹等价类数量，解决模型检查中资源分配的关键问题。


<details>
  <summary>Details</summary>
Motivation: 并发程序的Mazurkiewicz迹等价类数量估计对基于枚举的模型检查至关重要：它能预测模型检查运行时间，并评估搜索空间覆盖进度。然而，精确计数是#P-难问题，且无法在多项式时间内近似。

Method: 1) 将无状态最优DPOR算法的探索视为有界深度和宽度的树，其叶子节点是最大Mazurkiewicz迹；2) 应用Knuth经典估计器在该树上获得无偏估计；3) 使用随机枚举技术控制方差，通过维护每层部分路径的小种群并耦合其演化。

Result: 在JMC模型检查器中实现并评估，在共享内存基准测试中，即使状态空间有10^5-10^6个等价类，仅需数百次试验即可获得稳定估计（通常在20%误差范围内）。同时展示了如何通过加权所有探索图（不仅是完整迹）来估计模型检查成本。

Conclusion: 提出了首个可证明的多项式时间无偏估计器，用于计数Mazurkiewicz迹等价类，解决了模型检查资源分配中的重要问题，为并发程序验证提供了实用的估计工具。

Abstract: We study the estimation problem for concurrent programs: given a bounded program $P$, estimate the number of Mazurkiewicz trace-equivalence classes induced by its interleavings. This quantity informs two practical questions for enumeration-based model checking: how long a model checking run is likely to take, and what fraction of the search space has been covered so far. We first show the counting problem is #P-hard even for restricted programs and, unless $P=NP$, inapproximable within any subexponential factor, ruling out efficient exact or randomized approximation algorithms. We give a Monte Carlo approach to obtain a poly-time unbiased estimator: we convert a stateless optimal DPOR algorithm into an unbiased estimator by viewing its exploration as a bounded-depth, bounded-width tree whose leaves are the maximal Mazurkiewicz traces. A classical estimator by Knuth, when run on this tree, yields an unbiased estimate. To control the variance, we apply stochastic enumeration by maintaining a small population of partial paths per depth whose evolution is coupled. We have implemented our estimator in the JMC model checker and evaluated it on shared-memory benchmarks. With modest budgets, our estimator yields stable estimates, typically within a 20% band, within a few hundred trials, even when the state space has $10^5$--$10^6$ classes. We also show how the same machinery estimates model-checking cost by weighting all explored graphs, not only complete traces. Our algorithms provide the first provable poly-time unbiased estimators for counting traces, a problem of considerable importance when allocating model checking resources.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Governing Cloud Data Pipelines with Agentic AI](https://arxiv.org/abs/2512.23737)
*Aswathnarayan Muthukrishnan Kirubakaran,Adithya Parthasarathy,Nitin Saksena,Ram Sekhar Bodala,Akshay Deshpande,Suhas Malempati,Shiva Carimireddy,Abhirup Mazumder*

Main category: cs.DC

TL;DR: 论文提出Agentic Cloud Data Engineering平台，通过集成有界AI代理到云数据管道的治理和控制平面，实现自适应资源重配置、模式协调和自动故障恢复，显著提升管道恢复速度、降低运营成本并减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 当前云数据管道在动态工作负载、演进模式、成本约束和严格治理要求下运行，但大多数生产管道依赖静态配置和被动操作实践，导致恢复时间长、资源利用效率低、人工开销高。

Method: 提出Agentic Cloud Data Engineering平台，采用策略感知控制架构，集成有界AI代理到治理和控制平面。专门代理分析管道遥测和元数据，基于声明性成本和合规策略进行推理，提出约束性操作行动（如自适应资源重配置、模式协调、自动故障恢复），所有代理行动都经过治理策略验证以确保可预测和可审计行为。

Result: 实验使用代表性批处理和流分析工作负载，结果显示：与静态编排相比，平台将平均管道恢复时间减少高达45%，运营成本降低约25%，手动干预事件减少超过70%，同时保持数据新鲜度和策略合规性。

Conclusion: 策略有界代理控制为在企业环境中治理云数据管道提供了一种有效且实用的方法，能够显著提升自动化水平和运营效率。

Abstract: Cloud data pipelines increasingly operate under dynamic workloads, evolving schemas, cost constraints, and strict governance requirements. Despite advances in cloud-native orchestration frameworks, most production pipelines rely on static configurations and reactive operational practices, resulting in prolonged recovery times, inefficient resource utilization, and high manual overhead. This paper presents Agentic Cloud Data Engineering, a policy-aware control architecture that integrates bounded AI agents into the governance and control plane of cloud data pipelines. In Agentic Cloud Data Engineering platform, specialized agents analyze pipeline telemetry and metadata, reason over declarative cost and compliance policies, and propose constrained operational actions such as adaptive resource reconfiguration, schema reconciliation, and automated failure recovery. All agent actions are validated against governance policies to ensure predictable and auditable behavior. We evaluate Agentic Cloud Data Engineering platform using representative batch and streaming analytics workloads constructed from public enterprise-style datasets. Experimental results show that Agentic Cloud Data Engineering platform reduces mean pipeline recovery time by up to 45%, lowers operational cost by approximately 25%, and decreases manual intervention events by over 70% compared to static orchestration, while maintaining data freshness and policy compliance. These results demonstrate that policy-bounded agentic control provides an effective and practical approach for governing cloud data pipelines in enterprise environments.

</details>


### [6] [Squeezing Edge Performance: A Sensitivity-Aware Container Management for Heterogeneous Tasks](https://arxiv.org/abs/2512.23952)
*Yongmin Zhang,Pengyu Huang,Mingyi Dong,Jing Yao*

Main category: cs.DC

TL;DR: 提出基于测量的容器资源管理框架CRMS，通过非线性拟合模型和队列延迟建模，在单边缘服务器上联合优化延迟和能耗，实现多项式时间复杂度的资源分配。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中任务异构性和资源有限性对高效编排带来挑战，需要针对单边缘服务器上多异构应用的资源管理方案。

Method: 1) 通过大量性能分析实验建立CPU/内存分配与处理延迟的非线性拟合模型；2) 基于该模型和队列延迟公式构建MINLP问题；3) 将NP-hard问题分解为可处理的凸子问题；4) 设计两阶段容器资源管理方案CRMS，结合凸优化和贪心细化。

Result: CRMS相比启发式和基于搜索的基线方法，延迟降低超过14%，能效提升，支持在全局资源约束下的准动态执行，具有多项式时间复杂度。

Conclusion: 提出的测量驱动容器资源管理框架为具有动态工作负载特征的异构边缘环境提供了实用且可扩展的解决方案。

Abstract: Edge computing enables latency-critical applications to process data close to end devices, yet task heterogeneity and limited resources pose significant challenges to efficient orchestration. This paper presents a measurement-driven, container-based resource management framework for intra-node optimization on a single edge server hosting multiple heterogeneous applications. Extensive profiling experiments are conducted to derive a nonlinear fitting model that characterizes the relationship among CPU/memory allocations and processing latency across diverse workloads, enabling reliable estimation of performance under varying configurations and providing quantitative support for subsequent optimization. Using this model and a queueing-based delay formulation, we formulate a mixed-integer nonlinear programming (MINLP) problem to jointly minimize system latency and power consumption, which is shown to be NP-hard. The problem is decomposed into tractable convex subproblems and solved through a two-stage container-based resource management scheme (CRMS) combining convex optimization and greedy refinement. The proposed scheme achieves polynomial-time complexity and supports quasi-dynamic execution under global resource constraints. Simulation results demonstrate that CRMS reduces latency by over 14\% and improves energy efficiency compared with heuristic and search-based baselines, offering a practical and scalable solution for heterogeneous edge environments with dynamic workload characteristics.

</details>


### [7] [Data Heterogeneity-Aware Client Selection for Federated Learning in Wireless Networks](https://arxiv.org/abs/2512.24286)
*Yanbing Yang,Huiling Zhu,Wenchi Cheng,Jingqing Wang,Changrun Chen,Jiangzhou Wang*

Main category: cs.DC

TL;DR: 提出联合客户端选择和资源分配方案，解决联邦学习中数据异构性导致的泛化误差问题，降低延迟和能耗


<details>
  <summary>Details</summary>
Motivation: 联邦学习在无线网络中受限于通信计算资源和客户端数据异构性，导致重复训练、能耗增加和延迟延长

Method: 理论分析数据异构性对泛化误差的影响，建立优化问题联合最小化学习延迟和能耗，提出联合客户端选择和资源分配方案，采用凸优化和松弛技术

Result: 相比不考虑数据异构性的基准方法，提出的CSRA方案获得更高的测试精度、更低的学习延迟和能耗

Conclusion: 通过联合客户端选择和资源分配，有效解决了联邦学习中数据异构性带来的效率问题，提升了系统性能

Abstract: Federated Learning (FL) enables mobile edge devices, functioning as clients, to collaboratively train a decentralized model while ensuring local data privacy. However, the efficiency of FL in wireless networks is limited not only by constraints on communication and computational resources but also by significant data heterogeneity among clients, particularly in large-scale networks. This paper first presents a theoretical analysis of the impact of client data heterogeneity on global model generalization error, which can result in repeated training cycles, increased energy consumption, and prolonged latency. Based on the theoretical insights, an optimization problem is formulated to jointly minimize learning latency and energy consumption while constraining generalization error. A joint client selection and resource allocation (CSRA) approach is then proposed, employing a series of convex optimization and relaxation techniques. Extensive simulation results demonstrate that the proposed CSRA scheme yields higher test accuracy, reduced learning latency, and lower energy consumption compared to baseline methods that do not account for data heterogeneity.

</details>


### [8] [PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression](https://arxiv.org/abs/2512.24449)
*Bo Jiang,Taolue Yang,Youyuan Liu,Xubin He,Sheng Di,Sian Jin*

Main category: cs.DC

TL;DR: PackKV是一个针对长上下文生成的KV缓存管理框架，通过创新的有损压缩技术显著减少内存占用，同时保持高计算效率


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型的长上下文推理面临KV缓存内存占用巨大的挑战，随着序列长度和批处理大小的增加，KV缓存可达数GB，限制了实际应用

Method: PackKV设计了专门针对KV缓存数据特性的有损压缩技术，包括压缩算法与系统架构的协同设计，支持KV缓存的动态增长特性，同时保持高计算效率

Result: 在相同精度损失下，相比现有量化方法，PackKV平均实现K缓存153.2%、V缓存179.6%的内存减少率；在A100和RTX Pro 6000 GPU上，相比cuBLAS矩阵向量乘法内核，平均吞吐量提升K为75.7%、V为171.7%

Conclusion: PackKV是一个高效通用的KV缓存管理框架，通过创新的压缩技术显著减少内存占用并提高执行吞吐量，为长上下文生成提供了有效的解决方案

Abstract: Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \textbf{153.2}\% higher memory reduction rate for the K cache and \textbf{179.6}\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \textbf{75.7}\% for K and \textbf{171.7}\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV

</details>


### [9] [Understanding LLM Checkpoint/Restore I/O Strategies and Patterns](https://arxiv.org/abs/2512.24511)
*Mikaila J. Gossman,Avinash Maurya,Bogdan Nicolae,Jon C. Calhoun*

Main category: cs.DC

TL;DR: 本文研究了在LLM大规模训练中，使用liburing进行异步I/O优化检查点性能的方法，通过微基准测试发现文件系统感知的聚合和合并策略能显著提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM和基础模型规模扩大，检查点/恢复成为训练和推理的关键模式。在3D并行（张量、流水线、数据）下，检查点涉及大量进程管理不同形状和大小的张量，需要频繁持久化到稳定存储，这带来了大数据I/O问题（数据量大、种类多、速度快）。存储栈各层性能差异巨大，即使使用异步刷新/预取也会在并发下产生瓶颈。

Method: 开发微基准测试来量化使用liburing时的权衡，评估聚合、对齐和I/O合并在缓冲I/O和直接I/O下的交互效果。研究文件系统感知的聚合策略。

Result: 未合并的小缓冲区操作使吞吐量相比合成工作负载减半，而文件系统感知的聚合恢复了带宽并减少了元数据开销。相比最先进的LLM检查点引擎，该方法实现了比DataStates-LLM高3.9倍的写入吞吐量，比TorchSnapshot高7.6倍。

Conclusion: 研究结果表明，需要与现代文件系统和I/O后端对齐的聚合和合并策略，才能有效优化LLM检查点性能。liburing等内核加速I/O库在LLM检查点场景中的效果值得进一步探索。

Abstract: As LLMs and foundation models scale, checkpoint/restore has become a critical pattern for training and inference. With 3D parallelism (tensor, pipeline, data), checkpointing involves many processes, each managing numerous tensors of varying shapes and sizes, that must be persisted frequently to stable storage (e.g., parallel file systems). This turns checkpoint/restore into a big-data I/O problem characterized by volume, variety, and velocity. The workflow must traverse the full storage stack -- from GPU memory through host memory and local storage to external repositories -- whose tiers differ by orders of magnitude in performance, creating bottlenecks under concurrency even with asynchronous flush/prefetch. Kernel-accelerated I/O libraries such as \texttt{liburing} may mitigate these issues versus POSIX, but their effectiveness for LLM checkpointing remains underexplored. We develop microbenchmarks to quantify trade-offs when using \texttt{liburing}, evaluating how aggregation, alignment, and I/O coalescing interact under buffered and direct I/O. We find that uncoalesced small-buffer operations halve throughput relative to synthetic workloads, while file system-aware aggregation restores bandwidth and reduces metadata overhead. Compared to state-of-the-art LLM checkpointing engines, our approach achieves up to $3.9\times$ higher write throughput than DataStates-LLM and $7.6\times$ higher than TorchSnapshot. These results highlight the need for aggregation and coalescing strategies that align with modern file systems and I/O backends.

</details>


### [10] [Distributed Bilevel Optimization with Dual Pruning for Resource-limited Clients](https://arxiv.org/abs/2512.24667)
*Mingyi Li,Xiao Zhang,Ruisheng Zheng,Hongjian Shi,Yuan Yuan,Xiuzhen Cheng,Dongxiao Yu*

Main category: cs.DC

TL;DR: 提出首个资源自适应的分布式双层优化框架，包含二阶自由超梯度估计器，允许客户端根据可用资源优化子模型，实现渐近最优收敛率


<details>
  <summary>Details</summary>
Motivation: 随着大规模模型的发展，传统分布式双层优化算法无法直接应用于低资源客户端，主要原因是优化内外层函数的计算量过大

Method: 提出资源自适应分布式双层优化框架，包含二阶自由超梯度估计器，允许客户端根据资源优化子模型，分析全局平均超梯度的上界和局部部分训练的内参数误差界

Result: 理论证明RABO和RAFBO都能达到渐近最优收敛率O(1/√(C_x*Q))，由外参数最小覆盖率C_x*主导；在两个不同任务上的实验证明了方法的有效性和计算效率

Conclusion: 提出的资源自适应分布式双层优化框架解决了低资源客户端应用大规模模型的问题，实现了计算效率和理论保证的平衡

Abstract: With the development of large-scale models, traditional distributed bilevel optimization algorithms cannot be applied directly in low-resource clients. The key reason lies in the excessive computation involved in optimizing both the lower- and upper-level functions. Thus, we present the first resource-adaptive distributed bilevel optimization framework with a second-order free hypergradient estimator, which allows each client to optimize the submodels adapted to the available resources. Due to the coupled influence of partial outer parameters x and inner parameters y, it's challenging to theoretically analyze the upper bound regarding the globally averaged hypergradient for full model parameters. The error bound of inner parameter also needs to be reformulated since the local partial training. The provable theorems show that both RABO and RAFBO can achieve an asymptotically optimal convergence rate of $O(1/\sqrt{C_x^{\ast}Q})$, which is dominated by the minimum coverage of the outer parameter $C_x^{\ast}$. Extensive experiments on two different tasks demonstrate the effectiveness and computation efficiency of our proposed methods.

</details>


### [11] [AI-Driven Cloud Resource Optimization for Multi-Cluster Environments](https://arxiv.org/abs/2512.24914)
*Vinoth Punniyamoorthy,Akash Kumar Agarwal,Bikesh Kumar,Abhirup Mazumder,Kabilan Kannan,Sumit Saha*

Main category: cs.DC

TL;DR: 提出AI驱动的多集群云系统自适应资源优化框架，通过预测学习和策略感知决策实现跨集群主动协调管理，相比传统反应式方法提高了资源效率、稳定性和性能一致性。


<details>
  <summary>Details</summary>
Motivation: 现代云原生系统依赖多集群部署支持可扩展性、弹性和地理分布，但现有资源管理方法主要基于反应式和集群中心化，无法在动态工作负载下优化系统范围行为，导致资源利用率低、适应延迟和运维开销增加。

Method: 提出AI驱动的自适应资源优化框架，集成预测学习、策略感知决策和持续反馈，通过分析跨集群遥测数据和历史执行模式，动态调整资源分配以平衡性能、成本和可靠性目标。

Result: 原型实现显示相比传统反应式方法，该框架提高了资源效率、在工作负载波动期间更快稳定、减少了性能变异性，验证了智能自适应基础设施管理的有效性。

Conclusion: 智能自适应的基础设施管理是实现可扩展和弹性云平台的关键推动因素，AI驱动的资源优化框架能够显著改善多集群云系统的资源管理和系统性能。

Abstract: Modern cloud-native systems increasingly rely on multi-cluster deployments to support scalability, resilience, and geographic distribution. However, existing resource management approaches remain largely reactive and cluster-centric, limiting their ability to optimize system-wide behavior under dynamic workloads. These limitations result in inefficient resource utilization, delayed adaptation, and increased operational overhead across distributed environments. This paper presents an AI-driven framework for adaptive resource optimization in multi-cluster cloud systems. The proposed approach integrates predictive learning, policy-aware decision-making, and continuous feedback to enable proactive and coordinated resource management across clusters. By analyzing cross-cluster telemetry and historical execution patterns, the framework dynamically adjusts resource allocation to balance performance, cost, and reliability objectives. A prototype implementation demonstrates improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional reactive approaches. The results highlight the effectiveness of intelligent, self-adaptive infrastructure management as a key enabler for scalable and resilient cloud platforms.

</details>


### [12] [Reliable and Resilient Collective Communication Library for LLM Training and Serving](https://arxiv.org/abs/2512.25059)
*Wei Wang,Nengneng Yu,Sixian Xiong,Zaoxing Liu*

Main category: cs.DC

TL;DR: R²CCL是一个容错通信库，利用多NIC硬件实现无损、低开销的故障转移，显著减少GPU训练和推理中的网络故障恢复开销。


<details>
  <summary>Details</summary>
Motivation: 现代ML训练和推理扩展到成千上万个GPU，网络故障会导致10-15%的GPU时间浪费在缓慢的恢复上。常见的网络错误和链路波动会触发超时，导致整个作业终止，迫使训练时昂贵的检查点回滚和推理时请求重新处理。

Method: R²CCL通过利用多NIC硬件，执行快速连接迁移、带宽感知负载重新分配和弹性集体算法，在故障下保持进度。它提供了无损、低开销的故障转移。

Result: R²CCL对NIC故障具有高度鲁棒性，产生少于1%的训练开销和少于3%的推理开销。在8-GPU H100 InfiniBand服务器和模拟数百个GPU的大规模ML模拟器上评估，R²CCL分别比AdapCC和DejaVu基线高出12.18倍和47倍。

Conclusion: R²CCL是一个有效的容错通信解决方案，能够显著减少大规模ML训练和推理中的网络故障恢复开销，提高系统效率和可靠性。

Abstract: Modern ML training and inference now span tens to tens of thousands of GPUs, where network faults can waste 10--15\% of GPU hours due to slow recovery. Common network errors and link fluctuations trigger timeouts that often terminate entire jobs, forcing expensive checkpoint rollback during training and request reprocessing during inference. We present R$^2$CCL, a fault-tolerant communication library that provides lossless, low-overhead failover by exploiting multi-NIC hardware. R$^2$CCL performs rapid connection migration, bandwidth-aware load redistribution, and resilient collective algorithms to maintain progress under failures. We evaluate R$^2$CCL on two 8-GPU H100 InfiniBand servers and via large-scale ML simulators modeling hundreds of GPUs with diverse failure patterns. Experiments show that R$^2$CCL is highly robust to NIC failures, incurring less than 1\% training and less than 3\% inference overheads. R$^2$CCL outperforms baselines AdapCC and DejaVu by 12.18$\times$ and 47$\times$, respectively.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [HERO-Sign: Hierarchical Tuning and Efficient Compiler-Time GPU Optimizations for SPHINCS+ Signature Generation](https://arxiv.org/abs/2512.23969)
*Yaoyun Zhou,Qian Wang*

Main category: cs.AR

TL;DR: HERO Sign：基于GPU加速的SPHINCS+签名方案，通过层次化调优和编译时优化，显著提升后量子安全签名生成速度


<details>
  <summary>Details</summary>
Motivation: SPHINCS+作为无状态哈希签名方案提供强后量子安全性，但其签名生成因密集哈希计算而缓慢。现有GPU优化未能充分利用Merkle树结构并行性，也缺乏跨计算内核的细粒度编译器级定制。

Method: 采用层次化调优和高效编译时优化：1) 重新审视SPHINCS+组件（FORS、MSS、WOTS+）的数据独立性并行机会；2) 为FORS引入Tree Fusion策略，通过自动Tree Tuning搜索算法适配不同GPU架构；3) 采用自适应编译策略，根据FORS Sign、TREE Sign、WOTS+ Sign等内核选择PTX或原生代码路径；4) 批量签名生成时使用基于任务图的构造优化内核级重叠。

Result: 在RTX 4090上，相比现有GPU实现，HERO Sign在SPHINCS+ 128f、192f、256f参数集下分别获得1.28-3.13、1.28-2.92、1.24-2.60倍的吞吐量提升。在A100、H100、GTX 2080上观察到类似增益，同时内核启动延迟降低两个数量级。

Conclusion: HERO Sign通过系统性的GPU优化策略，显著加速了SPHINCS+签名生成，为后量子安全签名提供了高效的GPU实现方案，特别适用于需要高性能批量签名的应用场景。

Abstract: SPHINCS+ is a stateless hash-based signature scheme that provides strong post quantum security, but its signature generation is slow due to intensive hash computations. GPUs offer massive parallelism that can potentially accelerate SPHINCS+ signatures. However, existing GPU-based optimizations either fail to fully exploit the inherent parallelism of SPHINCS+'s Merkle tree structure or lack fine-grained, compiler-level customization across its diverse computational kernels. This paper proposes HERO Sign, a GPU-accelerated SPHINCS+ implementation that adopts hierarchical tuning and efficient compiler time optimizations. HERO Sign reexamines the parallelization opportunities enabled by data independence across SPHINCS+ components, including FORS, MSS, and WOTS+. It introduces a Tree Fusion strategy for FORS, which contains a large number of independent branches. The fusion strategy is guided by an automated Tree Tuning search algorithm that adapts fusion schemes to different GPU architectures. To further improve performance, HERO Sign employs an adaptive compilation strategy that accounts for the varying effectiveness of compiler optimizations across SPHINCS+ kernels such as FORS Sign, TREE Sign, and WOTS+ Sign. During compilation, the strategy automatically selects between PTX and native code paths to maximize efficiency. For batched signature generation, HERO Sign optimizes kernel-level overlapping using a task graph-based construction to reduce multi-stream idle time and kernel launch overhead. Experimental results show that, compared to state of the art GPU implementations, HERO Sign achieves throughput improvements of 1.28-3.13, 1.28-2.92, and 1.24-2.60 under the SPHINCS+ 128f, 192f, and 256f parameter sets on RTX 4090. Similar gains are observed on A100, H100, and GTX 2080, along with a two orders of magnitude reduction in kernel launch latency.

</details>
