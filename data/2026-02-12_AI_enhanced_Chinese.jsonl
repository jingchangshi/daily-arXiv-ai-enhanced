{"id": "2602.10246", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.10246", "abs": "https://arxiv.org/abs/2602.10246", "authors": ["Mayur Akewar", "Sandeep Madireddy", "Dongsheng Luo", "Janki Bhimani"], "title": "KORAL: Knowledge Graph Guided LLM Reasoning for SSD Operational Analysis", "comment": null, "summary": "Solid State Drives (SSDs) are critical to datacenters, consumer platforms, and mission-critical systems. Yet diagnosing their performance and reliability is difficult because data are fragmented and time-disjoint, and existing methods demand large datasets and expert input while offering only limited insights. Degradation arises not only from shifting workloads and evolving architectures but also from environmental factors such as temperature, humidity, and vibration. We present KORAL, a knowledge driven reasoning framework that integrates Large Language Models (LLMs) with a structured Knowledge Graph (KG) to generate insights into SSD operations. Unlike traditional approaches that require extensive expert input and large datasets, KORAL generates a Data KG from fragmented telemetry and integrates a Literature KG that already organizes knowledge from literature, reports, and traces. This turns unstructured sources into a queryable graph and telemetry into structured knowledge, and both the Graphs guide the LLM to deliver evidence-based, explainable analysis aligned with the domain vocabulary and constraints. Evaluation using real production traces shows that the KORAL delivers expert-level diagnosis and recommendations, supported by grounded explanations that improve reasoning transparency, guide operator decisions, reduce manual effort, and provide actionable insights to improve service quality. To our knowledge, this is the first end-to-end system that combines LLMs and KGs for full-spectrum SSD reasoning including Descriptive, Predictive, Prescriptive, and What-if analysis. We release the generated SSD-specific KG to advance reproducible research in knowledge-based storage system analysis. GitHub Repository: https://github.com/Damrl-lab/KORAL", "AI": {"tldr": "KORAL\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u77e5\u8bc6\u56fe\u8c31\u7684\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8eSSD\u6027\u80fd\u4e0e\u53ef\u9760\u6027\u8bca\u65ad\uff0c\u65e0\u9700\u5927\u91cf\u6570\u636e\u96c6\u548c\u4e13\u5bb6\u8f93\u5165\u5373\u53ef\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5206\u6790\u3002", "motivation": "SSD\u6027\u80fd\u8bca\u65ad\u56f0\u96be\uff0c\u6570\u636e\u5206\u6563\u4e14\u65f6\u95f4\u4e0d\u8fde\u7eed\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\u548c\u4e13\u5bb6\u8f93\u5165\uff0c\u4f46\u53ea\u80fd\u63d0\u4f9b\u6709\u9650\u6d1e\u5bdf\u3002\u6027\u80fd\u4e0b\u964d\u4e0d\u4ec5\u6765\u81ea\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\u548c\u67b6\u6784\u6f14\u8fdb\uff0c\u8fd8\u53d7\u6e29\u5ea6\u3001\u6e7f\u5ea6\u3001\u632f\u52a8\u7b49\u73af\u5883\u56e0\u7d20\u5f71\u54cd\u3002", "method": "KORAL\u6846\u67b6\u6574\u5408LLM\u4e0e\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\uff1a\u4ece\u788e\u7247\u5316\u9065\u6d4b\u6570\u636e\u751f\u6210\u6570\u636e\u77e5\u8bc6\u56fe\u8c31\uff0c\u96c6\u6210\u6587\u732e\u77e5\u8bc6\u56fe\u8c31\uff08\u5305\u542b\u6587\u732e\u3001\u62a5\u544a\u3001\u8ffd\u8e2a\u6570\u636e\uff09\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u6e90\u8f6c\u4e3a\u53ef\u67e5\u8be2\u56fe\uff0c\u5f15\u5bfcLLM\u63d0\u4f9b\u57fa\u4e8e\u8bc1\u636e\u3001\u53ef\u89e3\u91ca\u7684\u5206\u6790\u3002", "result": "\u4f7f\u7528\u771f\u5b9e\u751f\u4ea7\u8ffd\u8e2a\u6570\u636e\u8bc4\u4f30\u663e\u793a\uff0cKORAL\u63d0\u4f9b\u4e13\u5bb6\u7ea7\u8bca\u65ad\u548c\u5efa\u8bae\uff0c\u652f\u6301\u57fa\u4e8e\u8bc1\u636e\u7684\u89e3\u91ca\uff0c\u63d0\u9ad8\u63a8\u7406\u900f\u660e\u5ea6\uff0c\u6307\u5bfc\u64cd\u4f5c\u51b3\u7b56\uff0c\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6539\u8fdb\u670d\u52a1\u8d28\u91cf\u7684\u89c1\u89e3\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7ed3\u5408LLM\u548cKG\u8fdb\u884c\u5168\u9891\u8c31SSD\u63a8\u7406\uff08\u63cf\u8ff0\u6027\u3001\u9884\u6d4b\u6027\u3001\u89c4\u8303\u6027\u3001\u5047\u8bbe\u5206\u6790\uff09\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u53d1\u5e03\u7684SSD\u7279\u5b9a\u77e5\u8bc6\u56fe\u8c31\u5c06\u63a8\u52a8\u57fa\u4e8e\u77e5\u8bc6\u7684\u5b58\u50a8\u7cfb\u7edf\u5206\u6790\u7684\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2602.10262", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.10262", "abs": "https://arxiv.org/abs/2602.10262", "authors": ["Aaron Jarmusch", "Connor Vitz", "Sunita Chandrasekaran"], "title": "Execution-Centric Characterization of FP8 Matrix Cores, Asynchronous Execution, and Structured Sparsity on AMD MI300A", "comment": null, "summary": "The AMD MI300A APU integrates CDNA3 GPUs with high-bandwidth memory and advanced accelerator features: FP8 matrix cores, asynchronous compute engines (ACE), and 2:4 structured sparsity. These capabilities are increasingly relied upon by modern HPC and HPC-AI workloads, yet their execution characteristics and system-level implications remain insufficiently understood. In this paper, we present an execution-centric characterization of FP8 matrix execution, ACE concurrency, and structured sparsity on MI300A using targeted microbenchmarks. We quantify occupancy thresholds, fairness, throughput trade-offs under concurrent execution, and context-dependent sparsity benefits. We evaluate representative case studies - transformer-style, concurrent, and mixed-precision kernels - to show how these effects translate into application-level performance and predictability. Our results provide practical guidance for occupancy-aware scheduling, concurrency decisions, and sparsity enablement on MI300A-class unified nodes.", "AI": {"tldr": "\u672c\u6587\u5bf9AMD MI300A APU\u7684FP8\u77e9\u9635\u6267\u884c\u3001\u5f02\u6b65\u8ba1\u7b97\u5f15\u64ce\u5e76\u53d1\u548c\u7ed3\u6784\u5316\u7a00\u758f\u6027\u8fdb\u884c\u4e86\u6267\u884c\u4e2d\u5fc3\u5316\u8868\u5f81\uff0c\u901a\u8fc7\u5fae\u57fa\u51c6\u6d4b\u8bd5\u91cf\u5316\u4e86\u5360\u7528\u9608\u503c\u3001\u516c\u5e73\u6027\u3001\u5e76\u53d1\u541e\u5410\u91cf\u6743\u8861\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7a00\u758f\u6027\u6536\u76ca\uff0c\u4e3aMI300A\u7c7b\u7edf\u4e00\u8282\u70b9\u7684\u8c03\u5ea6\u51b3\u7b56\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "AMD MI300A APU\u96c6\u6210\u4e86CDNA3 GPU\u3001\u9ad8\u5e26\u5bbd\u5185\u5b58\u548c\u5148\u8fdb\u52a0\u901f\u5668\u7279\u6027\uff08FP8\u77e9\u9635\u6838\u5fc3\u3001\u5f02\u6b65\u8ba1\u7b97\u5f15\u64ce\u30012:4\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff09\uff0c\u8fd9\u4e9b\u7279\u6027\u88ab\u73b0\u4ee3HPC\u548cHPC-AI\u5de5\u4f5c\u8d1f\u8f7d\u65e5\u76ca\u4f9d\u8d56\uff0c\u4f46\u5176\u6267\u884c\u7279\u6027\u548c\u7cfb\u7edf\u7ea7\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u9488\u5bf9\u6027\u7684\u5fae\u57fa\u51c6\u6d4b\u8bd5\u5bf9MI300A\u8fdb\u884c\u6267\u884c\u4e2d\u5fc3\u5316\u8868\u5f81\uff0c\u91cf\u5316FP8\u77e9\u9635\u6267\u884c\u3001ACE\u5e76\u53d1\u548c\u7ed3\u6784\u5316\u7a00\u758f\u6027\u7684\u6027\u80fd\u7279\u6027\uff0c\u5305\u62ec\u5360\u7528\u9608\u503c\u3001\u516c\u5e73\u6027\u3001\u5e76\u53d1\u6267\u884c\u4e0b\u7684\u541e\u5410\u91cf\u6743\u8861\u4ee5\u53ca\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7a00\u758f\u6027\u6536\u76ca\u3002", "result": "\u901a\u8fc7\u4ee3\u8868\u6027\u6848\u4f8b\u7814\u7a76\uff08transformer\u98ce\u683c\u3001\u5e76\u53d1\u548c\u6df7\u5408\u7cbe\u5ea6\u5185\u6838\uff09\u5c55\u793a\u4e86\u8fd9\u4e9b\u6548\u5e94\u5982\u4f55\u8f6c\u5316\u4e3a\u5e94\u7528\u7ea7\u6027\u80fd\u548c\u53ef\u9884\u6d4b\u6027\uff0c\u4e3a\u5360\u7528\u611f\u77e5\u8c03\u5ea6\u3001\u5e76\u53d1\u51b3\u7b56\u548c\u7a00\u758f\u6027\u542f\u7528\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3aMI300A\u7c7b\u7edf\u4e00\u8282\u70b9\u4e0a\u7684\u5360\u7528\u611f\u77e5\u8c03\u5ea6\u3001\u5e76\u53d1\u51b3\u7b56\u548c\u7a00\u758f\u6027\u542f\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u73b0\u4ee3HPC\u548cHPC-AI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u548c\u53ef\u9884\u6d4b\u6027\u3002"}}
{"id": "2602.10378", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10378", "abs": "https://arxiv.org/abs/2602.10378", "authors": ["Elliot L. Epstein", "Rajat Vadiraj Dwaraknath", "John Winnicki"], "title": "Flash-SD-KDE: Accelerating SD-KDE with Tensor Cores", "comment": "11 pages", "summary": "Score-debiased kernel density estimation (SD-KDE) achieves improved asymptotic convergence rates over classical KDE, but its use of an empirical score has made it significantly slower in practice. We show that by re-ordering the SD-KDE computation to expose matrix-multiplication structure, Tensor Cores can be used to accelerate the GPU implementation. On a 32k-sample 16-dimensional problem, our approach runs up to $47\\times$ faster than a strong SD-KDE GPU baseline and $3{,}300\\times$ faster than scikit-learn's KDE. On a larger 1M-sample 16-dimensional task evaluated on 131k queries, Flash-SD-KDE completes in $2.3$ s on a single GPU, making score-debiased density estimation practical at previously infeasible scales.", "AI": {"tldr": "Flash-SD-KDE\u901a\u8fc7\u91cd\u65b0\u7ec4\u7ec7\u8ba1\u7b97\u4ee5\u5229\u7528Tensor Core\uff0c\u5c06\u5f97\u5206\u53bb\u504f\u6838\u5bc6\u5ea6\u4f30\u8ba1\u7684\u901f\u5ea6\u63d0\u5347\u4e8647\u500d\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u767e\u4e07\u7ea7\u6837\u672c\u7684\u9ad8\u7ef4\u6570\u636e\u3002", "motivation": "SD-KDE\u867d\u7136\u6bd4\u4f20\u7edfKDE\u6709\u66f4\u597d\u7684\u6e10\u8fd1\u6536\u655b\u7387\uff0c\u4f46\u7531\u4e8e\u4f7f\u7528\u7ecf\u9a8c\u5f97\u5206\u5bfc\u81f4\u5b9e\u9645\u8ba1\u7b97\u901f\u5ea6\u5f88\u6162\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u9ad8\u7ef4\u6570\u636e\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u91cd\u65b0\u7ec4\u7ec7SD-KDE\u7684\u8ba1\u7b97\u6d41\u7a0b\uff0c\u66b4\u9732\u77e9\u9635\u4e58\u6cd5\u7ed3\u6784\uff0c\u4ece\u800c\u5229\u7528GPU\u7684Tensor Core\u8fdb\u884c\u52a0\u901f\u3002", "result": "\u572832k\u6837\u672c16\u7ef4\u95ee\u9898\u4e0a\uff0c\u6bd4GPU\u57fa\u7ebf\u5feb47\u500d\uff0c\u6bd4scikit-learn\u5feb3300\u500d\uff1b\u57281M\u6837\u672c16\u7ef4\u4efb\u52a1\u4e0a\uff0c131k\u4e2a\u67e5\u8be2\u4ec5\u97002.3\u79d2\u3002", "conclusion": "Flash-SD-KDE\u4f7f\u5f97\u5206\u53bb\u504f\u5bc6\u5ea6\u4f30\u8ba1\u5728\u4ee5\u524d\u4e0d\u53ef\u884c\u7684\u89c4\u6a21\u4e0a\u53d8\u5f97\u5b9e\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2602.10486", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.10486", "abs": "https://arxiv.org/abs/2602.10486", "authors": ["Vijay K. Garg", "Rohan Garg"], "title": "Computing Least Fixed Points with Overwrite Semantics in Parallel and Distributed Systems", "comment": null, "summary": "We present methods to compute least fixed points of multiple monotone inflationary functions in parallel and distributed settings. While the classic Knaster-Tarski theorem addresses a single function with sequential iteration, modern computing systems require parallel execution with overwrite semantics, non-atomic updates, and stale reads. We prove three convergence theorems under progressively relaxed synchronization: (1) Interleaving semantics with fair scheduling, (2) Parallel execution with update-only-on-change semantics (processes write only on those coordinates whose values change), and (3) Distributed execution with bounded staleness (updates propagate within $T$ rounds) and $i$-locality (each process modifies only its own component).\n  Our approach differs from prior work in fundamental ways: Cousot-Cousot's chaotic iteration uses join-based merges that preserve information. Instead, we use coordinate-wise overwriting. Bertsekas's asynchronous methods assume contractions. We use coordinate-wise overwriting with structural constraints (locality, bounded staleness) instead. Applications include parallel and distributed algorithms for the transitive closure, stable marriage, shortest paths, and fair division with subsidy problems. Our results provide the first exact least-fixed-point convergence guarantees for overwrite-based parallel updates without join operations or contraction assumptions.", "AI": {"tldr": "\u63d0\u51fa\u5728\u5e76\u884c\u548c\u5206\u5e03\u5f0f\u8bbe\u7f6e\u4e2d\u8ba1\u7b97\u591a\u4e2a\u5355\u8c03\u81a8\u80c0\u51fd\u6570\u6700\u5c0f\u4e0d\u52a8\u70b9\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u4e09\u79cd\u6e10\u8fdb\u653e\u677e\u540c\u6b65\u6761\u4ef6\u4e0b\u7684\u6536\u655b\u5b9a\u7406\uff0c\u4e3a\u57fa\u4e8e\u8986\u76d6\u7684\u5e76\u884c\u66f4\u65b0\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cbe\u786e\u7684\u6700\u5c0f\u4e0d\u52a8\u70b9\u6536\u655b\u4fdd\u8bc1\u3002", "motivation": "\u7ecf\u5178Knaster-Tarski\u5b9a\u7406\u5904\u7406\u5355\u51fd\u6570\u987a\u5e8f\u8fed\u4ee3\uff0c\u800c\u73b0\u4ee3\u8ba1\u7b97\u7cfb\u7edf\u9700\u8981\u5e76\u884c\u6267\u884c\uff0c\u5177\u6709\u8986\u76d6\u8bed\u4e49\u3001\u975e\u539f\u5b50\u66f4\u65b0\u548c\u8fc7\u65f6\u8bfb\u53d6\u7b49\u7279\u70b9\u3002\u9700\u8981\u4e3a\u5e76\u884c\u548c\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u7684\u4e0d\u52a8\u70b9\u8ba1\u7b97\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u4f7f\u7528\u5750\u6807\u8986\u76d6\u65b9\u6cd5\uff08\u800c\u975e\u57fa\u4e8e\u8fde\u63a5\u7684\u5408\u5e76\uff09\uff0c\u5728\u4e09\u79cd\u6e10\u8fdb\u653e\u677e\u7684\u540c\u6b65\u6761\u4ef6\u4e0b\uff1a1\uff09\u4ea4\u9519\u8bed\u4e49\u4e0e\u516c\u5e73\u8c03\u5ea6\uff1b2\uff09\u4ec5\u5f53\u503c\u53d8\u5316\u65f6\u66f4\u65b0\u7684\u5e76\u884c\u6267\u884c\uff1b3\uff09\u5177\u6709\u6709\u9650\u8fc7\u65f6\u6027\u548ci-\u5c40\u90e8\u6027\u7684\u5206\u5e03\u5f0f\u6267\u884c\u3002", "result": "\u8bc1\u660e\u4e86\u4e09\u79cd\u6536\u655b\u5b9a\u7406\uff0c\u4e3a\u57fa\u4e8e\u8986\u76d6\u7684\u5e76\u884c\u66f4\u65b0\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cbe\u786e\u7684\u6700\u5c0f\u4e0d\u52a8\u70b9\u6536\u655b\u4fdd\u8bc1\uff0c\u65e0\u9700\u8fde\u63a5\u64cd\u4f5c\u6216\u6536\u7f29\u5047\u8bbe\u3002\u5e94\u7528\u4e8e\u4f20\u9012\u95ed\u5305\u3001\u7a33\u5b9a\u5a5a\u59fb\u3001\u6700\u77ed\u8def\u5f84\u548c\u5e26\u8865\u8d34\u7684\u516c\u5e73\u5206\u914d\u7b49\u95ee\u9898\u7684\u5e76\u884c\u5206\u5e03\u5f0f\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5e76\u884c\u548c\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u7684\u4e0d\u52a8\u70b9\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u5750\u6807\u8986\u76d6\u65b9\u6cd5\u548c\u6e10\u8fdb\u653e\u677e\u7684\u540c\u6b65\u6761\u4ef6\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8fde\u63a5\u64cd\u4f5c\u6216\u6536\u7f29\u5047\u8bbe\u7684\u7cbe\u786e\u6536\u655b\u4fdd\u8bc1\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u4e0d\u52a8\u70b9\u7406\u8bba\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2602.10218", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.10218", "abs": "https://arxiv.org/abs/2602.10218", "authors": ["Chenhui Deng", "Zhongzhi Yu", "Guan-Ting Liu", "Nathaniel Pinckney", "Haoxing Ren"], "title": "ACE-RTL: When Agentic Context Evolution Meets RTL-Specialized LLMs", "comment": null, "summary": "Recent advances in large language models (LLMs) have sparked growing interest in applying them to hardware design automation, particularly for accurate RTL code generation. Prior efforts follow two largely independent paths: (i) training domain-adapted RTL models to internalize hardware semantics, (ii) developing agentic systems that leverage frontier generic LLMs guided by simulation feedback. However, these two paths exhibit complementary strengths and weaknesses. In this work, we present ACE-RTL that unifies both directions through Agentic Context Evolution (ACE). ACE-RTL integrates an RTL-specialized LLM, trained on a large-scale dataset of 1.7 million RTL samples, with a frontier reasoning LLM through three synergistic components: the generator, reflector, and coordinator. These components iteratively refine RTL code toward functional correctness. We further introduce a parallel scaling strategy that significantly reduces the number of iterations required to reach correct solutions. On the Comprehensive Verilog Design Problems (CVDP) benchmark, ACE-RTL achieves up to a 44.87% pass rate improvement over 14 competitive baselines while requiring only four iterations on average.", "AI": {"tldr": "ACE-RTL\u7ed3\u5408\u4e86\u9886\u57df\u4e13\u7528RTL\u6a21\u578b\u548c\u524d\u6cbf\u901a\u7528LLM\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u751f\u6210\u5668\u3001\u53cd\u5c04\u5668\u548c\u534f\u8c03\u5668\u4e09\u4e2a\u7ec4\u4ef6\u8fed\u4ee3\u4f18\u5316RTL\u4ee3\u7801\uff0c\u5728CVDP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u901a\u8fc7\u7387\u3002", "motivation": "\u5f53\u524d\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\uff0c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u6761\u72ec\u7acb\u8def\u5f84\uff1a\u8bad\u7ec3\u9886\u57df\u4e13\u7528RTL\u6a21\u578b\u548c\u5f00\u53d1\u57fa\u4e8e\u6a21\u62df\u53cd\u9988\u7684\u4ee3\u7406\u7cfb\u7edf\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u9700\u8981\u7edf\u4e00\u6846\u67b6\u6765\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faACE-RTL\u6846\u67b6\uff0c\u901a\u8fc7Agentic Context Evolution (ACE)\u6574\u5408RTL\u4e13\u7528LLM\uff08\u57fa\u4e8e170\u4e07RTL\u6837\u672c\u8bad\u7ec3\uff09\u548c\u524d\u6cbf\u63a8\u7406LLM\u3002\u5305\u542b\u4e09\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1a\u751f\u6210\u5668\u3001\u53cd\u5c04\u5668\u3001\u534f\u8c03\u5668\uff0c\u8fed\u4ee3\u4f18\u5316RTL\u4ee3\u7801\u3002\u8fd8\u5f15\u5165\u4e86\u5e76\u884c\u6269\u5c55\u7b56\u7565\u51cf\u5c11\u8fed\u4ee3\u6b21\u6570\u3002", "result": "\u5728Comprehensive Verilog Design Problems (CVDP)\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cACE-RTL\u76f8\u6bd414\u4e2a\u7ade\u4e89\u57fa\u7ebf\u5b9e\u73b0\u4e86\u9ad8\u8fbe44.87%\u7684\u901a\u8fc7\u7387\u63d0\u5347\uff0c\u5e73\u5747\u4ec5\u97004\u6b21\u8fed\u4ee3\u3002", "conclusion": "ACE-RTL\u6210\u529f\u7edf\u4e00\u4e86\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u7684\u4e24\u79cd\u4e3b\u6d41\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u540c\u7ec4\u4ef6\u548c\u5e76\u884c\u7b56\u7565\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3aLLM\u5728\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2602.10729", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.10729", "abs": "https://arxiv.org/abs/2602.10729", "authors": ["Youhe Jiang", "Fangcheng Fu", "Eiko Yoneki"], "title": "BOute: Cost-Efficient LLM Serving with Heterogeneous LLMs and GPUs via Multi-Objective Bayesian Optimization", "comment": "MLSys 2026", "summary": "The rapid growth of large language model (LLM) deployments has made cost-efficient serving systems essential. Recent efforts to enhance system cost-efficiency adopt two main perspectives: (i) An algorithmic perspective that exploits heterogeneous model capabilities to route simpler queries to lower-cost models and complex queries to higher-cost models (i.e., heterogeneous query routing); and (ii) a systems perspective that utilizes heterogeneous GPU resources as cost-effective alternatives to homogeneous high-end GPUs (i.e., heterogeneous model deployment). However, algorithm-system co-design for cost-efficient LLM serving necessitates sophisticated management: (i) Determining optimal query routing strategies under latency and quality requirements, (ii) configuring model deployment across heterogeneous GPUs with appropriate resource allocation and parallelism strategies, and (iii) co-optimizing routing and deployment decisions to maximize overall system performance. To address these challenges, we present BOute, a quality-aware scheduling system that jointly exploits heterogeneous model and GPU capabilities for cost-efficient LLM serving. BOute employs a multi-objective Bayesian optimization (MOBO) framework to co-optimize the routing strategy and model deployment, thereby maximizing the cost-efficiency of the serving system while guaranteeing response quality. Evaluation results demonstrate that BOute outperforms state-of-the-art LLM serving systems by up to 157% and 59% on average under identical cost budgets and quality requirements, or reducing serving costs by 15%-61% (38% on average) while maintaining the same performance targets, validating its effectiveness in achieving cost-efficient LLM serving.", "AI": {"tldr": "BOute\u662f\u4e00\u4e2a\u8d28\u91cf\u611f\u77e5\u8c03\u5ea6\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5f02\u6784\u6a21\u578b\u8def\u7531\u548cGPU\u90e8\u7f72\u7b56\u7565\uff0c\u5b9e\u73b0\u6210\u672c\u9ad8\u6548\u7684LLM\u670d\u52a1\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u5feb\u901f\u589e\u957f\uff0c\u9700\u8981\u6210\u672c\u9ad8\u6548\u7684\u670d\u52a1\u7cfb\u7edf\u3002\u73b0\u6709\u65b9\u6cd5\u5206\u522b\u4ece\u7b97\u6cd5\u89d2\u5ea6\uff08\u5f02\u6784\u67e5\u8be2\u8def\u7531\uff09\u548c\u7cfb\u7edf\u89d2\u5ea6\uff08\u5f02\u6784GPU\u90e8\u7f72\uff09\u4f18\u5316\uff0c\u4f46\u7f3a\u4e4f\u4e24\u8005\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u96be\u4ee5\u5b9e\u73b0\u6574\u4f53\u6700\u4f18\u3002", "method": "\u63d0\u51faBOute\u7cfb\u7edf\uff0c\u91c7\u7528\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u8def\u7531\u7b56\u7565\u548c\u6a21\u578b\u90e8\u7f72\u914d\u7f6e\uff0c\u5728\u4fdd\u8bc1\u54cd\u5e94\u8d28\u91cf\u7684\u540c\u65f6\u6700\u5927\u5316\u7cfb\u7edf\u6210\u672c\u6548\u7387\u3002", "result": "\u5728\u76f8\u540c\u6210\u672c\u9884\u7b97\u548c\u8d28\u91cf\u8981\u6c42\u4e0b\uff0cBOute\u6bd4\u73b0\u6709\u6700\u4f73LLM\u670d\u52a1\u7cfb\u7edf\u6027\u80fd\u63d0\u5347\u6700\u9ad8157%\uff0c\u5e73\u574759%\uff1b\u6216\u5728\u4fdd\u6301\u76f8\u540c\u6027\u80fd\u76ee\u6807\u4e0b\uff0c\u670d\u52a1\u6210\u672c\u964d\u4f4e15%-61%\uff08\u5e73\u574738%\uff09\u3002", "conclusion": "BOute\u901a\u8fc7\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u6210\u672c\u9ad8\u6548\u7684LLM\u670d\u52a1\uff0c\u9a8c\u8bc1\u4e86\u8054\u5408\u4f18\u5316\u5f02\u6784\u6a21\u578b\u80fd\u529b\u548cGPU\u8d44\u6e90\u7684\u4ef7\u503c\u3002"}}
{"id": "2602.10254", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.10254", "abs": "https://arxiv.org/abs/2602.10254", "authors": ["Hanyuan Gao", "Xiaoxuan Yang"], "title": "Area-Efficient In-Memory Computing for Mixture-of-Experts via Multiplexing and Caching", "comment": "Accepted by ISCAS 2026", "summary": "Mixture-of-Experts (MoE) layers activate a subset of model weights, dubbed experts, to improve model performance. MoE is particularly promising for deployment on process-in-memory (PIM) architectures, because PIM can naturally fit experts separately and provide great benefits for energy efficiency. However, PIM chips often suffer from large area overhead, especially in the peripheral circuits. In this paper, we propose an area-efficient in-memory computing architecture for MoE transformers. First, to reduce area, we propose a crossbar-level multiplexing strategy that exploits MoE sparsity: experts are deployed on crossbars and multiple crossbars share the same peripheral circuits. Second, we propose expert grouping and group-wise scheduling methods to alleviate the load imbalance and contention overhead caused by sharing. In addition, to address the problem that the expert choice router requires access to all hidden states during generation, we propose a gate-output (GO)cache to store necessary results and bypass expensive additional computation. Experiments show that our approaches improve the area efficiency of the MoE part by up to 2.2x compared to a SOTA architecture. During generation, the cache improves performance and energy efficiency by 4.2x and 10.1x, respectively, compared to the baseline when generating 8 tokens. The total performance density achieves 15.6 GOPS/W/mm2. The code is open source at https://github.com/superstarghy/MoEwithPIM.", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411MoE\u53d8\u538b\u5668\u7684\u9762\u79ef\u9ad8\u6548\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\uff0c\u901a\u8fc7\u4ea4\u53c9\u9635\u5217\u590d\u7528\u3001\u4e13\u5bb6\u5206\u7ec4\u8c03\u5ea6\u548c\u95e8\u8f93\u51fa\u7f13\u5b58\u6280\u672f\uff0c\u63d0\u5347\u9762\u79ef\u6548\u7387\u548c\u751f\u6210\u6027\u80fd\u3002", "motivation": "MoE\u5c42\u901a\u8fc7\u6fc0\u6d3b\u90e8\u5206\u4e13\u5bb6\u6743\u91cd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u9002\u5408\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\u90e8\u7f72\uff0c\u4f46\u5b58\u5185\u8ba1\u7b97\u82af\u7247\u5b58\u5728\u9762\u79ef\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5916\u56f4\u7535\u8def\u5360\u7528\u5927\u91cf\u9762\u79ef\u3002", "method": "1) \u4ea4\u53c9\u9635\u5217\u7ea7\u590d\u7528\u7b56\u7565\uff1a\u5229\u7528MoE\u7a00\u758f\u6027\uff0c\u591a\u4e2a\u4ea4\u53c9\u9635\u5217\u5171\u4eab\u5916\u56f4\u7535\u8def\uff1b2) \u4e13\u5bb6\u5206\u7ec4\u548c\u7ec4\u95f4\u8c03\u5ea6\uff1a\u7f13\u89e3\u5171\u4eab\u5bfc\u81f4\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u548c\u4e89\u7528\u5f00\u9500\uff1b3) \u95e8\u8f93\u51fa\u7f13\u5b58\uff1a\u5b58\u50a8\u5fc5\u8981\u7ed3\u679c\uff0c\u907f\u514d\u4e13\u5bb6\u9009\u62e9\u8def\u7531\u5668\u5728\u751f\u6210\u65f6\u8bbf\u95ee\u6240\u6709\u9690\u85cf\u72b6\u6001\u5e26\u6765\u7684\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "result": "MoE\u90e8\u5206\u9762\u79ef\u6548\u7387\u76f8\u6bd4SOTA\u67b6\u6784\u63d0\u53472.2\u500d\uff1b\u751f\u62108\u4e2atoken\u65f6\uff0c\u7f13\u5b58\u4f7f\u6027\u80fd\u548c\u80fd\u6548\u5206\u522b\u63d0\u53474.2\u500d\u548c10.1\u500d\uff1b\u603b\u6027\u80fd\u5bc6\u5ea6\u8fbe\u523015.6 GOPS/W/mm\u00b2\u3002", "conclusion": "\u63d0\u51fa\u7684\u9762\u79ef\u9ad8\u6548\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\u901a\u8fc7\u4ea4\u53c9\u9635\u5217\u590d\u7528\u3001\u4e13\u5bb6\u5206\u7ec4\u8c03\u5ea6\u548c\u95e8\u8f93\u51fa\u7f13\u5b58\uff0c\u6709\u6548\u89e3\u51b3\u4e86MoE\u5728\u5b58\u5185\u8ba1\u7b97\u90e8\u7f72\u4e2d\u7684\u9762\u79ef\u5f00\u9500\u548c\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9762\u79ef\u6548\u7387\u548c\u751f\u6210\u6548\u7387\u3002"}}
{"id": "2602.11000", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.11000", "abs": "https://arxiv.org/abs/2602.11000", "authors": ["Ali Tehrani", "Yahya Emara", "Essam Wissam", "Wojciech Paluch", "Waleed Atallah", "\u0141ukasz Dudziak", "Mohamed S. Abdelfattah"], "title": "Fine-Tuning GPT-5 for GPU Kernel Generation", "comment": null, "summary": "Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code generation, they face significant challenges in GPU code generation because of the scarcity of high-quality labeled training data, compiler biases when generating synthetic solutions, and limited generalization across hardware generations. This precludes supervised fine-tuning (SFT) as a scalable methodology for improving current LLMs. In contrast, reinforcement learning (RL) offers a data-efficient and adaptive alternative but requires access to relevant tools, careful selection of training problems, and a robust evaluation environment. We present Makora's environment and tools for reinforcement learning finetuning of frontier models and report our results from fine-tuning GPT-5 for Triton code generation. In the single-attempt setting, our fine-tuned model improves kernel correctness from 43.7% to 77.0% (+33.3 percentage points) and increases the fraction of problems outperforming TorchInductor from 14.8% to 21.8% (+7 percentage points) compared to baseline GPT-5, while exceeding prior state-of-the-art models on KernelBench. When integrated into a full coding agent, it is able to solve up to 97.4% of problems in an expanded KernelBench suite, outperforming the PyTorch TorchInductor compiler on 72.9% of problems with a geometric mean speedup of 2.12x. Our work demonstrates that targeted post-training with reinforcement learning can unlock LLM capabilities in highly specialized technical domains where traditional supervised learning is limited by data availability, opening new pathways for AI-assisted accelerator programming.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03GPT-5\u751f\u6210Triton GPU\u5185\u6838\u4ee3\u7801\uff0c\u663e\u8457\u63d0\u5347\u6b63\u786e\u7387\u548c\u6027\u80fd\uff0c\u8d85\u8d8a\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5", "motivation": "GPU\u5185\u6838\u5f00\u53d1\u590d\u6742\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u56e0\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3001\u7f16\u8bd1\u5668\u504f\u89c1\u548c\u786c\u4ef6\u6cdb\u5316\u95ee\u9898\u800c\u53d7\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u63d0\u5347LLM\u5728GPU\u4ee3\u7801\u751f\u6210\u9886\u57df\u7684\u80fd\u529b", "method": "\u5f00\u53d1Makora\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u548c\u5de5\u5177\uff0c\u5bf9GPT-5\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u4e13\u6ce8\u4e8eTriton\u4ee3\u7801\u751f\u6210\uff0c\u91c7\u7528\u6570\u636e\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u8bad\u7ec3\u65b9\u6cd5", "result": "\u5355\u6b21\u5c1d\u8bd5\u6b63\u786e\u7387\u4ece43.7%\u63d0\u5347\u81f377.0%\uff0c\u8d85\u8d8aTorchInductor\u7684\u95ee\u9898\u6bd4\u4f8b\u4ece14.8%\u589e\u81f321.8%\uff1b\u5b8c\u6574\u7f16\u7801\u4ee3\u7406\u53ef\u89e3\u51b397.4%\u7684\u95ee\u9898\uff0c\u572872.9%\u95ee\u9898\u4e0a\u8d85\u8d8aPyTorch\u7f16\u8bd1\u5668\uff0c\u51e0\u4f55\u5e73\u5747\u52a0\u901f2.12\u500d", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u80fd\u6709\u6548\u89e3\u9501LLM\u5728\u4e13\u4e1a\u9886\u57df\u7684\u80fd\u529b\uff0c\u4e3aAI\u8f85\u52a9\u52a0\u901f\u5668\u7f16\u7a0b\u5f00\u8f9f\u65b0\u9014\u5f84\uff0c\u7279\u522b\u662f\u5728\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u53d7\u6570\u636e\u9650\u5236\u7684\u9886\u57df"}}
{"id": "2602.10654", "categories": ["cs.AR", "cs.FL"], "pdf": "https://arxiv.org/pdf/2602.10654", "abs": "https://arxiv.org/abs/2602.10654", "authors": ["Derek Christ", "Thomas Zimmermann", "Philippe Barbie", "Dmitri Saberi", "Yao Yin", "Matthias Jung"], "title": "DRAMPyML: A Formal Description of DRAM Protocols with Timed Petri Nets", "comment": null, "summary": "The JEDEC committee defines various domain-specific DRAM standards. These standards feature increasingly complex and evolving protocol specifications, which are detailed in timing diagrams and command tables. Understanding these protocols is becoming progressively challenging as new features and complex device hierarchies are difficult to comprehend without an expressive model. While each JEDEC standard features a simplified state machine, this state machine fails to reflect the parallel operation of memory banks.\n  In this paper, we present an evolved modeling approach based on timed Petri nets and Python. This model provides a more accurate representation of DRAM protocols, making them easier to understand and directly executable, which enables the evaluation of interesting metrics and the verification of controller RTL models, DRAM logic and memory simulators.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4Petri\u7f51\u548cPython\u7684DRAM\u534f\u8bae\u5efa\u6a21\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3JEDEC\u6807\u51c6\u4e2d\u7b80\u5316\u72b6\u6001\u673a\u65e0\u6cd5\u53cd\u6620\u5185\u5b58bank\u5e76\u884c\u64cd\u4f5c\u7684\u95ee\u9898\u3002", "motivation": "JEDEC\u5b9a\u4e49\u7684DRAM\u6807\u51c6\u534f\u8bae\u65e5\u76ca\u590d\u6742\uff0c\u65f6\u5e8f\u56fe\u548c\u547d\u4ee4\u8868\u96be\u4ee5\u7406\u89e3\uff0c\u73b0\u6709\u7b80\u5316\u72b6\u6001\u673a\u65e0\u6cd5\u53cd\u6620\u5185\u5b58bank\u7684\u5e76\u884c\u64cd\u4f5c\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u8868\u8fbe\u6a21\u578b\u3002", "method": "\u91c7\u7528\u65f6\u95f4Petri\u7f51\u548cPython\u76f8\u7ed3\u5408\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u6784\u5efa\u53ef\u76f4\u63a5\u6267\u884c\u7684DRAM\u534f\u8bae\u6a21\u578b\uff0c\u80fd\u591f\u51c6\u786e\u53cd\u6620\u534f\u8bae\u884c\u4e3a\u3002", "result": "\u8be5\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684DRAM\u534f\u8bae\u8868\u793a\uff0c\u6613\u4e8e\u7406\u89e3\u4e14\u53ef\u76f4\u63a5\u6267\u884c\uff0c\u80fd\u591f\u8bc4\u4f30\u6027\u80fd\u6307\u6807\u5e76\u9a8c\u8bc1\u63a7\u5236\u5668RTL\u6a21\u578b\u3001DRAM\u903b\u8f91\u548c\u5185\u5b58\u6a21\u62df\u5668\u3002", "conclusion": "\u57fa\u4e8e\u65f6\u95f4Petri\u7f51\u548cPython\u7684\u5efa\u6a21\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3JEDEC DRAM\u6807\u51c6\u534f\u8bae\u590d\u6742\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7cbe\u786e\u3001\u53ef\u6267\u884c\u7684\u534f\u8bae\u8868\u793a\uff0c\u6709\u52a9\u4e8e\u534f\u8bae\u7406\u89e3\u548c\u7cfb\u7edf\u9a8c\u8bc1\u3002"}}
{"id": "2602.11125", "categories": ["cs.DC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.11125", "abs": "https://arxiv.org/abs/2602.11125", "authors": ["Animesh Maiti", "Abhinav Chakraborty", "Bibhuti Das", "Subhash Bhagat", "Krishnendu Mukhopadhyaya"], "title": "Min-Sum Uniform Coverage Problem by Autonomous Mobile Robots", "comment": null, "summary": "We study the \\textit{min-sum uniform coverage} problem for a swarm of $n$ mobile robots on a given finite line segment and on a circle having finite positive radius, where the circle is given as an input. The robots must coordinate their movements to reach a uniformly spaced configuration that minimizes the total distance traveled by all robots. The robots are autonomous, anonymous, identical, and homogeneous, and operate under the \\textit{Look-Compute-Move} (LCM) model with \\textit{non-rigid} motion controlled by a fair asynchronous scheduler. They are oblivious and silent, possessing neither persistent memory nor a means of explicit communication. In the \\textbf{line-segment setting}, the \\textit{min-sum uniform coverage} problem requires placing the robots at uniformly spaced points along the segment so as to minimize the total distance traveled by all robots. In the \\textbf{circle setting} for this problem, the robots have to arrange themselves uniformly around the given circle to form a regular $n$-gon. There is no fixed orientation or designated starting vertex, and the goal is to minimize the total distance traveled by all the robots. We present a deterministic distributed algorithm that achieves uniform coverage in the line-segment setting with minimum total movement cost. For the circle setting, we characterize all initial configurations for which the \\textit{min-sum uniform coverage} problem is deterministically unsolvable under the considered robot model. For all the other remaining configurations, we provide a deterministic distributed algorithm that achieves uniform coverage while minimizing the total distance traveled. These results characterize the deterministic solvability of min-sum coverage for oblivious robots and achieve optimal cost whenever solvable.", "AI": {"tldr": "\u7814\u7a76\u673a\u5668\u4eba\u5728\u7ebf\u6bb5\u548c\u5706\u4e0a\u7684\u6700\u5c0f\u603b\u79fb\u52a8\u8ddd\u79bb\u5747\u5300\u8986\u76d6\u95ee\u9898\uff0c\u63d0\u51fa\u786e\u5b9a\u6027\u5206\u5e03\u5f0f\u7b97\u6cd5\u5b9e\u73b0\u6700\u4f18\u6210\u672c\u8986\u76d6", "motivation": "\u7814\u7a76\u81ea\u4e3b\u3001\u533f\u540d\u3001\u65e0\u8bb0\u5fc6\u3001\u65e0\u901a\u4fe1\u7684\u673a\u5668\u4eba\u5728\u5f02\u6b65\u8c03\u5ea6\u4e0b\u5982\u4f55\u534f\u8c03\u8fd0\u52a8\uff0c\u4ee5\u6700\u5c0f\u603b\u79fb\u52a8\u8ddd\u79bb\u5b9e\u73b0\u5747\u5300\u8986\u76d6\u914d\u7f6e", "method": "\u63d0\u51fa\u786e\u5b9a\u6027\u5206\u5e03\u5f0f\u7b97\u6cd5\uff1a\u7ebf\u6bb5\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u6700\u5c0f\u603b\u79fb\u52a8\u8ddd\u79bb\u7684\u5747\u5300\u8986\u76d6\uff1b\u5706\u8bbe\u7f6e\u4e2d\u5206\u6790\u4e0d\u53ef\u89e3\u914d\u7f6e\u5e76\u63d0\u4f9b\u53ef\u89e3\u914d\u7f6e\u7684\u6700\u4f18\u7b97\u6cd5", "result": "\u7ebf\u6bb5\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u6700\u5c0f\u603b\u79fb\u52a8\u8ddd\u79bb\u7684\u5747\u5300\u8986\u76d6\uff1b\u5706\u8bbe\u7f6e\u4e2d\u5b8c\u6574\u523b\u753b\u4e86\u786e\u5b9a\u6027\u53ef\u89e3\u548c\u4e0d\u53ef\u89e3\u7684\u521d\u59cb\u914d\u7f6e\uff0c\u5e76\u4e3a\u53ef\u89e3\u914d\u7f6e\u63d0\u4f9b\u6700\u4f18\u7b97\u6cd5", "conclusion": "\u8be5\u7814\u7a76\u5b8c\u6574\u523b\u753b\u4e86\u65e0\u8bb0\u5fc6\u673a\u5668\u4eba\u5728\u6700\u5c0f\u603b\u79fb\u52a8\u8ddd\u79bb\u5747\u5300\u8986\u76d6\u95ee\u9898\u4e2d\u7684\u786e\u5b9a\u6027\u53ef\u89e3\u6027\uff0c\u5e76\u5728\u53ef\u89e3\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u6700\u4f18\u6210\u672c\u8986\u76d6"}}
{"id": "2602.10790", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.10790", "abs": "https://arxiv.org/abs/2602.10790", "authors": ["Paula Carolina Lozano Duarte", "Sule Ozev", "Mehdi Tahoori"], "title": "Fault Tolerant Design of IGZO-based Binary Search ADCs", "comment": "Accepted for publication at the 27th International Symposium on Quality Electronic Design (ISQED'26), April 8-10, 2026", "summary": "Thin-film technologies such as Indium Gallium Zinc Oxide (IGZO) enable Flexible Electronics (FE) for emerging applications in wearable sensing, personal health monitoring, and large-area systems. Analog-to-digital converters (ADCs) serve as critical sensor interfaces in these systems. Yet, their vulnerability to manufacturing defects remains poorly understood despite unipolar technologies' inherently high defect densities and process variations compared to mature CMOS technologies. We present a hierarchical fault injection framework to characterize defect sensitivity in Binary Search ADCs implemented in n-type only technologies. Our methodology combines transistor-level defect characterization with system-level fault propagation analysis, enabling efficient exploration of both single and multiple fault scenarios across the conversion hierarchy. The framework identifies critical fault-sensitive circuit components and enables selective redundancy strategies targeting only the most sensitive components. The resulting defect-tolerant designs improve fault coverage from 60% to 92% under single-fault injections and from 34% to 77.6% under multi-fault injection, while incurring only 4.2% area overhead and 6% power increase. While validated on IGZO-TFTs, the methodology applies to all emerging unipolar technologies.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u6545\u969c\u6ce8\u5165\u6846\u67b6\u5206\u6790IGZO\u7b49\u5355\u6781\u6027\u6280\u672f\u4e2d\u4e8c\u8fdb\u5236\u641c\u7d22ADC\u7684\u7f3a\u9677\u654f\u611f\u6027\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5197\u4f59\u7b56\u7565\u5c06\u6545\u969c\u8986\u76d6\u7387\u4ece60%\u63d0\u5347\u81f392%\uff0c\u9762\u79ef\u5f00\u9500\u4ec54.2%", "motivation": "\u67d4\u6027\u7535\u5b50\u6280\u672f\uff08\u5982IGZO\uff09\u5728\u53ef\u7a7f\u6234\u4f20\u611f\u548c\u5065\u5eb7\u76d1\u6d4b\u7b49\u65b0\u5174\u5e94\u7528\u4e2d\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5355\u6781\u6027\u6280\u672f\u76f8\u6bd4\u6210\u719fCMOS\u6280\u672f\u5177\u6709\u66f4\u9ad8\u7684\u7f3a\u9677\u5bc6\u5ea6\u548c\u5de5\u827a\u53d8\u5316\u3002ADC\u4f5c\u4e3a\u5173\u952e\u4f20\u611f\u5668\u63a5\u53e3\uff0c\u5176\u5236\u9020\u7f3a\u9677\u654f\u611f\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u6545\u969c\u6ce8\u5165\u6846\u67b6\uff0c\u7ed3\u5408\u6676\u4f53\u7ba1\u7ea7\u7f3a\u9677\u8868\u5f81\u548c\u7cfb\u7edf\u7ea7\u6545\u969c\u4f20\u64ad\u5206\u6790\uff0c\u9ad8\u6548\u63a2\u7d22\u8f6c\u6362\u5c42\u6b21\u7ed3\u6784\u4e2d\u7684\u5355\u6545\u969c\u548c\u591a\u6545\u969c\u573a\u666f\uff0c\u8bc6\u522b\u5173\u952e\u6545\u969c\u654f\u611f\u7535\u8def\u7ec4\u4ef6\u3002", "result": "\u901a\u8fc7\u9009\u62e9\u6027\u5197\u4f59\u7b56\u7565\uff0c\u5355\u6545\u969c\u6ce8\u5165\u4e0b\u7684\u6545\u969c\u8986\u76d6\u7387\u4ece60%\u63d0\u5347\u81f392%\uff0c\u591a\u6545\u969c\u6ce8\u5165\u4e0b\u4ece34%\u63d0\u5347\u81f377.6%\uff0c\u4ec5\u5e26\u67654.2%\u7684\u9762\u79ef\u5f00\u9500\u548c6%\u7684\u529f\u8017\u589e\u52a0\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u8bc6\u522b\u4e86\u4e8c\u8fdb\u5236\u641c\u7d22ADC\u4e2d\u7684\u5173\u952e\u6545\u969c\u654f\u611f\u7ec4\u4ef6\uff0c\u5e76\u901a\u8fc7\u9009\u62e9\u6027\u5197\u4f59\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7f3a\u9677\u5bb9\u9519\u8bbe\u8ba1\u3002\u867d\u7136\u57fa\u4e8eIGZO-TFT\u9a8c\u8bc1\uff0c\u4f46\u65b9\u6cd5\u9002\u7528\u4e8e\u6240\u6709\u65b0\u5174\u5355\u6781\u6027\u6280\u672f\u3002"}}
{"id": "2602.11016", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.11016", "abs": "https://arxiv.org/abs/2602.11016", "authors": ["Jinxin Yu", "Yudong Pan", "Mengdi Wang", "Huawei Li", "Yinhe Han", "Xiaowei Li", "Ying Wang"], "title": "From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design", "comment": "Accepted to DATE 2026", "summary": "Transformer-based models dominate modern AI workloads but exacerbate memory bottlenecks due to their quadratic attention complexity and ever-growing model sizes. Existing accelerators, such as Groq and Cerebras, mitigate off-chip traffic with large on-chip caches, while algorithmic innovations such as FlashAttention fuse operators to avoid materializing large attention matrices. However, as off-chip traffic decreases, our measurements show that on-chip SRAM accesses account for over 60% of energy in long-sequence workloads, making cache access the new bottleneck. We propose 3D-Flow, a hybrid-bonded, 3D-stacked spatial accelerator that enables register-to-register communication across vertically partitioned PE tiers. Unlike 2D multi-array architectures limited by NoC-based router-to-router transfers, 3D-Flow leverages sub-10 um vertical TSVs to sustain cycle-level operator pipelining with minimal overhead. On top of this architecture, we design 3D-FlashAttention, a fine-grained scheduling method that balances latency across tiers, forming a bubble-free vertical dataflow without on-chip SRAM roundtrips. Evaluations on Transformer workloads (OPT and QWEN models) show that our 3D spatial accelerator reduces 46-93% energy consumption and achieves 1.4x-7.6x speedups compared to state-of-the-art 2D and 3D designs.", "AI": {"tldr": "3D-Flow\uff1a\u4e00\u79cd\u6df7\u5408\u952e\u54083D\u5806\u53e0\u7a7a\u95f4\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u5782\u76f4\u5206\u533aPE\u5c42\u95f4\u7684\u5bc4\u5b58\u5668\u5230\u5bc4\u5b58\u5668\u901a\u4fe1\uff0c\u51cf\u5c11Transformer\u6a21\u578b\u4e2dSRAM\u8bbf\u95ee\u7684\u80fd\u8017\u74f6\u9888\uff0c\u76f8\u6bd4\u73b0\u67092D/3D\u8bbe\u8ba1\u5b9e\u73b046-93%\u80fd\u8017\u964d\u4f4e\u548c1.4-7.6\u500d\u52a0\u901f\u3002", "motivation": "Transformer\u6a21\u578b\u56e0\u4e8c\u6b21\u6ce8\u610f\u529b\u590d\u6742\u5ea6\u548c\u6a21\u578b\u89c4\u6a21\u589e\u957f\u52a0\u5267\u5185\u5b58\u74f6\u9888\u3002\u73b0\u6709\u52a0\u901f\u5668\uff08\u5982Groq\u3001Cerebras\uff09\u901a\u8fc7\u5927\u5bb9\u91cf\u7247\u4e0a\u7f13\u5b58\u51cf\u5c11\u7247\u5916\u6d41\u91cf\uff0cFlashAttention\u7b49\u7b97\u6cd5\u521b\u65b0\u907f\u514d\u751f\u6210\u5927\u6ce8\u610f\u529b\u77e9\u9635\u3002\u4f46\u968f\u7740\u7247\u5916\u6d41\u91cf\u51cf\u5c11\uff0c\u6d4b\u91cf\u663e\u793a\u957f\u5e8f\u5217\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7247\u4e0aSRAM\u8bbf\u95ee\u80fd\u8017\u5360\u6bd4\u8d8560%\uff0c\u6210\u4e3a\u65b0\u74f6\u9888\u3002", "method": "\u63d0\u51fa3D-Flow\uff1a\u6df7\u5408\u952e\u54083D\u5806\u53e0\u7a7a\u95f4\u52a0\u901f\u5668\uff0c\u652f\u6301\u5782\u76f4\u5206\u533aPE\u5c42\u95f4\u7684\u5bc4\u5b58\u5668\u5230\u5bc4\u5b58\u5668\u901a\u4fe1\u3002\u5229\u7528\u4e9a10\u5fae\u7c73\u5782\u76f4TSV\u7ef4\u6301\u5468\u671f\u7ea7\u7b97\u5b50\u6d41\u6c34\u7ebf\uff0c\u76f8\u6bd42D\u591a\u9635\u5217\u67b6\u6784\u53d7\u9650\u4e8eNoC\u8def\u7531\u5668\u95f4\u4f20\u8f93\uff0c\u5177\u6709\u66f4\u4f4e\u5f00\u9500\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u8bbe\u8ba13D-FlashAttention\uff1a\u7ec6\u7c92\u5ea6\u8c03\u5ea6\u65b9\u6cd5\uff0c\u5e73\u8861\u5404\u5c42\u5ef6\u8fdf\uff0c\u5f62\u6210\u65e0\u6c14\u6ce1\u5782\u76f4\u6570\u636e\u6d41\uff0c\u907f\u514d\u7247\u4e0aSRAM\u5f80\u8fd4\u8bbf\u95ee\u3002", "result": "\u5728Transformer\u5de5\u4f5c\u8d1f\u8f7d\uff08OPT\u548cQWEN\u6a21\u578b\uff09\u8bc4\u4f30\u663e\u793a\uff1a3D\u7a7a\u95f4\u52a0\u901f\u5668\u76f8\u6bd4\u6700\u5148\u8fdb\u76842D\u548c3D\u8bbe\u8ba1\uff0c\u80fd\u8017\u964d\u4f4e46-93%\uff0c\u901f\u5ea6\u63d0\u53471.4-7.6\u500d\u3002", "conclusion": "3D-Flow\u901a\u8fc73D\u5806\u53e0\u67b6\u6784\u548c\u5782\u76f4\u6570\u636e\u6d41\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86Transformer\u52a0\u901f\u4e2d\u7247\u4e0aSRAM\u8bbf\u95ee\u7684\u80fd\u8017\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u957f\u5e8f\u5217\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u65b9\u6848\u3002"}}
