<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [DeliverC: Teaching Pointers through GenAI-Powered Game-Based Learning](https://arxiv.org/abs/2509.14496)
*Wyatt Petula,Anushcka Joshi,Peggy Tu,Amrutha Somasundar,Suman Saha*

Main category: cs.PL

TL;DR: DeliverC是一款集成GPT-4-mini的GenAI增强游戏，为C语言指针学习提供实时个性化提示和挑战生成，在25名本科生试点研究中显示能提升自信心和反思能力，但AI生成反馈仍需改进。


<details>
  <summary>Details</summary>
Motivation: 传统游戏化编程教育缺乏对复杂主题（如C指针）的实时自适应支持，需要开发能够提供个性化学习体验的工具。

Method: 开发DeliverC游戏，集成GPT-4-mini提供实时个性化提示和动态生成指针相关挑战。通过25名本科生的试点研究，收集游戏数据和15项问卷调查（涵盖动机、自我效能、元认知和反馈质量等构念）。

Result: 大多数学生使用后感到更自信和善于反思，错误率随着脚手架式关卡推进而下降。但参与度随任务难度增加而降低，部分学生反馈AI生成的提示不够清晰。

Conclusion: DeliverC能够增强系统编程学习的参与度和理解，但AI生成反馈需要进一步优化。研究展示了GenAI与游戏化学习结合在传统挑战性编程领域支持个性化互动实践的潜力。

Abstract: While game-based learning is widely used in programming education, few tools
offer adaptive, real-time support for complex topics, such as C pointers. We
present DeliverC, a GenAI-enhanced game that integrates GPT-4-mini to provide
personalized hints and generate pointer-related challenges on the fly. In a
pilot study involving 25 undergraduate students, we investigated the impact of
the system on learning through gameplay data and a 15-item survey that covered
constructs such as motivation, self-efficacy, metacognition, and feedback
quality. Results show that most students felt more confident and reflective
after using the tool, and error rates decreased as students progressed through
scaffolded levels. However, participation decreased with task difficulty, and
some students reported receiving unclear or vague feedback. These findings
suggest that DeliverC can enhance engagement and understanding in systems
programming, although refinement in AI-generated feedback is still needed. Our
study highlights the potential of combining GenAI with game-based learning to
support personalized and interactive practice in traditionally challenging
programming domains.

</details>


### [2] [Refinement-Types Driven Development: A study](https://arxiv.org/abs/2509.15005)
*Facundo Domínguez,Arnaud Spiwack*

Main category: cs.PL

TL;DR: 本文主张将SMT求解器更广泛地应用于日常编程，通过精化类型和编译器静态检查的集成来增强普通类型检查器的能力


<details>
  <summary>Details</summary>
Motivation: 挑战SMT求解器仅用于形式化方法和验证的传统观念，探索其在日常编程任务中的应用价值

Method: 采用精化类型（以Liquid Haskell为例）和SMT求解器的无缝集成，通过编译器绑定器作用域处理的案例研究进行验证

Result: 展示了精化类型和SMT求解器能够简化普通编程任务，并开发了Liquid Haskell求解器的有限映射理论原型实现

Conclusion: 精化类型和SMT求解器的结合有望使普通编程变得更简单、更愉快，为日常编程工具的发展指明新方向

Abstract: This paper advocates for the broader application of SMT solvers in everyday
programming, challenging the conventional wisdom that these tools are solely
for formal methods and verification. We claim that SMT solvers, when seamlessly
integrated into a compiler's static checks, significantly enhance the
capabilities of ordinary type checkers in program composition. Specifically, we
argue that refinement types, as embodied by Liquid Haskell, enable the use of
SMT solvers in mundane programming tasks. Through a case study on handling
binder scopes in compilers, we envision a future where ordinary programming is
made simpler and more enjoyable with the aid of refinement types and SMT
solvers. As a secondary contribution, we present a prototype implementation of
a theory of finite maps for Liquid Haskell's solver, developed to support our
case study.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless and GPU-Based Training Architectures](https://arxiv.org/abs/2509.14920)
*Amine Barrak,Fabio Petrillo,Fehmi Jaafar*

Main category: cs.DC

TL;DR: 本文对SPIRT、ScatterReduce、AllReduce和MLLess等无服务器分布式机器学习架构进行了比较分析，发现SPIRT在训练时间效率、通信开销和容错能力方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习对可扩展且经济高效的训练解决方案需求日益增长，无服务器计算因其动态扩展性和资源高效执行而成为有前景的范式。

Method: 比较分析多种无服务器分布式ML架构，重点关注训练时间效率、成本效益、通信开销和容错能力等关键指标。SPIRT采用并行批处理和RedisAI支持的数据库内操作策略。

Result: SPIRT通过并行批处理和RedisAI操作显著减少了训练时间和通信开销。传统架构存在可扩展性挑战，对故障和对抗攻击的脆弱性各不相同。成本分析显示SPIRT尽管初始设置成本较高，但具有长期经济效益。

Conclusion: 研究不仅揭示了当前无服务器ML架构的优势和局限性，还为未来开发结合现有系统最有效特性的新模型奠定了基础。

Abstract: The field of distributed machine learning (ML) faces increasing demands for
scalable and cost-effective training solutions, particularly in the context of
large, complex models. Serverless computing has emerged as a promising paradigm
to address these challenges by offering dynamic scalability and
resource-efficient execution. Building upon our previous work, which introduced
the Serverless Peer Integrated for Robust Training (SPIRT) architecture, this
paper presents a comparative analysis of several serverless distributed ML
architectures. We examine SPIRT alongside established architectures like
ScatterReduce, AllReduce, and MLLess, focusing on key metrics such as training
time efficiency, cost-effectiveness, communication overhead, and fault
tolerance capabilities. Our findings reveal that SPIRT provides significant
improvements in reducing training times and communication overhead through
strategies such as parallel batch processing and in-database operations
facilitated by RedisAI. However, traditional architectures exhibit scalability
challenges and varying degrees of vulnerability to faults and adversarial
attacks. The cost analysis underscores the long-term economic benefits of SPIRT
despite its higher initial setup costs. This study not only highlights the
strengths and limitations of current serverless ML architectures but also sets
the stage for future research aimed at developing new models that combine the
most effective features of existing systems.

</details>


### [4] [Conditional Prior-based Non-stationary Channel Estimation Using Accelerated Diffusion Models](https://arxiv.org/abs/2509.15182)
*Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Umer,Asad Aali,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 该论文提出了一种基于条件先验扩散的无线信道估计方法，通过历史条件评分来去噪噪声信道快照，在移动丰富的城市微蜂窝环境中实现了优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 移动丰富的城市微蜂窝环境中的无线信道是非平稳的，移动性和散射体动态会随时间改变分布，导致传统和深度学习估计器性能下降，需要新的方法来处理这种时变特性。

Method: 使用带有跨时间注意力的时序编码器将短观测窗口压缩为上下文向量，通过特征调制引导去噪器；采用SNR匹配初始化选择扩散步长，使用几何间隔的缩短调度；引入时序自条件和训练时的平滑性惩罚来稳定演化。

Result: 在3GPP基准测试中，该方法在所有SNR下均表现出比LMMSE、GMM、LSTM和LDAMP基线更低的NMSE，展示了稳定的性能和强大的高SNR保真度。

Conclusion: 条件先验扩散方法能够有效处理非平稳无线信道的时变特性，通过历史信息引导的去噪过程实现了优越的信道估计性能，为移动环境中的通信系统提供了有效的解决方案。

Abstract: Wireless channels in motion-rich urban microcell (UMi) settings are
non-stationary; mobility and scatterer dynamics shift the distribution over
time, degrading classical and deep estimators. This work proposes conditional
prior diffusion for channel estimation, which learns a history-conditioned
score to denoise noisy channel snapshots. A temporal encoder with cross-time
attention compresses a short observation window into a context vector, which
captures the channel's instantaneous coherence and steers the denoiser via
feature-wise modulation. In inference, an SNR-matched initialization selects
the diffusion step whose marginal aligns with the measured input SNR, and the
process follows a shortened, geometrically spaced schedule, preserving the
signal-to-noise trajectory with far fewer iterations. Temporal
self-conditioning with the previous channel estimate and a training-only
smoothness penalty further stabilizes evolution without biasing the test-time
estimator. Evaluations on a 3GPP benchmark show lower NMSE across all SNRs than
LMMSE, GMM, LSTM, and LDAMP baselines, demonstrating stable performance and
strong high SNR fidelity.

</details>


### [5] [Channel Prediction under Network Distribution Shift Using Continual Learning-based Loss Regularization](https://arxiv.org/abs/2509.15192)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ibtsaam Qadir,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 本文提出一种基于损失正则化的持续学习框架，解决异构网络环境下频道预测的灾难性遗忘问题，在资源受限的无线基础设施中实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现代无线网络中，移动用户穿越异构网络配置时，传统预测器在分布偏移下性能显著降级（NMSE上升37.5%），需要解决频道预测中的灾难性遗忘问题。

Method: 提出基于损失正则化的持续学习框架，通过在标准训练目标中增加惩罚项来选择性保留对之前配置关键的网络参数，同时适应新环境。研究了弹性权重固化（EWC）和突触智能（SI）两种正则化策略。

Result: 在3GPP场景和多种架构下，SI将高SNR的NMSE底器降低了1.8dB（约32-34%），EWC达到1.4dB（约17-28%）。SI保持了O(M)的内存复杂度，而标准EWC需要O(MK)的复杂度。

Conclusion: 该持续学习框架有效解决了异构网络环境下频道预测的灾难性遗忘问题，特别是SI策略在性能和资源效率方面都显示出优势，适合于资源受限的无线基础设施。

Abstract: Modern wireless networks face critical challenges when mobile users traverse
heterogeneous network configurations with varying antenna layouts, carrier
frequencies, and scattering statistics. Traditional predictors degrade under
distribution shift, with NMSE rising by 37.5\% during cross-configuration
handovers. This work addresses catastrophic forgetting in channel prediction by
proposing a continual learning framework based on loss regularization. The
approach augments standard training objectives with penalty terms that
selectively preserve network parameters essential for previous configurations
while enabling adaptation to new environments. Two prominent regularization
strategies are investigated: Elastic Weight Consolidation (EWC) and Synaptic
Intelligence (SI). Across 3GPP scenarios and multiple architectures, SI lowers
the high-SNR NMSE floor by up to 1.8 dB ($\approx$32--34\%), while EWC achieves
up to 1.4 dB ($\approx$17--28\%). Notably, standard EWC incurs
$\mathcal{O}(MK)$ complexity (storing $M$ Fisher diagonal entries and
corresponding parameter snapshots across $K$ tasks) unless consolidated,
whereas SI maintains $\mathcal{O}(M)$ memory complexity (storing $M$ model
parameters), independent of task sequence length, making it suitable for
resource-constrained wireless infrastructure

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations](https://arxiv.org/abs/2509.14388)
*Lennart Bamberg,Filippo Minnella,Roberto Bosio,Fabrizio Ottati,Yuebin Wang,Jongmin Lee,Luciano Lavagno,Adam Fuks*

Main category: cs.AR

TL;DR: 本文介绍了eIQ Neutron高效NPU架构及其编译器优化，在相同TOPS和内存资源下比领先嵌入式NPU快1.8倍，即使面对计算和内存资源翻倍的NPU也能实现3.3倍性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统NPU的峰值TOPS指标不能真实反映实际性能且通常与更高的硅成本相关，需要在保持灵活性的同时最大化计算利用率。

Method: 采用灵活的数据驱动架构设计，配合编译器使用约束编程方法根据工作负载特性优化计算和数据移动。

Result: 在标准AI基准测试中，相比领先的嵌入式NPU和编译器堆栈，在相同TOPS和内存资源下平均加速1.8倍（峰值4倍）；即使面对计算和内存资源翻倍的NPU，也能实现最高3.3倍的性能提升。

Conclusion: eIQ Neutron NPU通过架构和编译器的协同设计，在保持灵活性的同时显著提升了计算效率，为边缘AI推理提供了高效的解决方案。

Abstract: Neural Processing Units (NPUs) are key to enabling efficient AI inference in
resource-constrained edge environments. While peak tera operations per second
(TOPS) is often used to gauge performance, it poorly reflects real-world
performance and typically rather correlates with higher silicon cost. To
address this, architects must focus on maximizing compute utilization, without
sacrificing flexibility. This paper presents the eIQ Neutron efficient-NPU,
integrated into a commercial flagship MPU, alongside co-designed compiler
algorithms. The architecture employs a flexible, data-driven design, while the
compiler uses a constrained programming approach to optimize compute and data
movement based on workload characteristics. Compared to the leading embedded
NPU and compiler stack, our solution achieves an average speedup of 1.8x (4x
peak) at equal TOPS and memory resources across standard AI-benchmarks. Even
against NPUs with double the compute and memory resources, Neutron delivers up
to 3.3x higher performance.

</details>


### [7] [Shift-Left Techniques in Electronic Design Automation: A Survey](https://arxiv.org/abs/2509.14551)
*Xinyue Wu,Zixuan Li,Fan Hu,Ting Lin,Xiaotian Zhao,Runxi Wang,Xinfei Guo*

Main category: cs.AR

TL;DR: 本文对EDA领域中的Shift-Left方法进行了全面调查，分析了现有和新兴的研究范式，重点关注了数字孪生、设计步骤融合以及AI技术在电子设计自动化中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着芯片设计复杂度的增加，传统的串行设计流程效率低下。Shift-Left方法通过将下游设计考虑提前到早期阶段，建立更强的相关性并优化设计，但面临准确复制下游行为和确定采用时机等挑战。

Method: 采用文献综述的方法，对EDA和更广泛设计生态系统中的Shift-Left研究进行了系统性调查，整理了相关论文并提供了独特的领域视角。

Result: 研究发现AI技术和开源设计流程的兴起显著增强了预测和建模能力，使数据驱动方法在EDA社区中日益重要，从而增强了当前工具中的Shift-Left功能。

Conclusion: Shift-Left方法为EDA供应商、物理设计师和逻辑设计师带来了新的机遇，随着智能EDA工具和技术的发展，该方法将继续演进并推动芯片设计流程的创新。

Abstract: The chip design process involves numerous steps, beginning with defining
product requirements and progressing through architectural planning,
system-level design, and the physical layout of individual circuit blocks. As
the enablers of large-scale chip development, Electronic Design Automation
(EDA) tools play a vital role in helping designers achieve high-quality
results. The Shift-Left methodology introduces a pathway toward creating
digital twins and fusing multiple design steps, thereby transitioning
traditionally sequential, physically-aware processes into virtual design
environments. This shift allows designers to establish stronger correlations
earlier and optimize designs more effectively. However, challenges remain,
especially in accurately replicating downstream behaviors and determining the
right scope and timing for adoption. These challenges, in turn, have revealed
new opportunities for EDA vendors, physical designers, and logic designers
alike. As the industry advances toward intelligent EDA tools and techniques, it
is timely to reflect on Shift-Left progress made and the challenges that
remain. The rise of AI techniques and the momentum of open-source design flows
have significantly strengthened prediction and modeling capabilities, making
data-driven methods increasingly relevant to the EDA community. This, in turn,
enhances the ''Shift-Left'' features embedded in current tools. In this paper,
we present a comprehensive survey of existing and emerging paradigms in
Shift-Left research within EDA and the broader design ecosystem. Our goal is to
provide a unique perspective on the state of the field and its future
directions. Relevant papers mentioned are organized in
https://github.com/iCAS-SJTU/Shift-Left-EDA-Papers.

</details>


### [8] [DeepAssert: An LLM-Aided Verification Framework with Fine-Grained Assertion Generation for Modules with Extracted Module Specifications](https://arxiv.org/abs/2509.14668)
*Yonghao Wang,Jiaxin Zhou,Hongqin Lyu,Zhiteng Chao,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: DeepAssert是一个基于大语言模型的验证框架，能够分析模块间的调用关系并提取独立规范，自动生成细粒度的深度断言，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有断言生成方法只能针对顶层设计生成断言，无法生成针对模块内部功能的深度断言，以及依赖难以获取的黄金RTL模型的问题。

Method: 提出LLM辅助验证框架DeepAssert，分析模块调用关系，提取各模块的I/O端口信息和独立规范，指导LLM自动生成细粒度深度断言。

Result: DeepAssert在生成高质量模块深度断言方面显著优于AssertLLM和Spec2Assertion等方法，并能提升这些方法的整体断言质量。

Conclusion: DeepAssert提供了更全面有效的验证过程，能够生成针对模块内部功能的深度断言，解决了现有方法的局限性。

Abstract: Assertion-Based Verification (ABV) is a crucial method for ensuring that
logic designs conform to their architectural specifications. However, existing
assertion generation methods primarily rely on information either from the
design specification, or register-transfer level (RTL) code. The former methods
are typically limited to generating assertions for the top-level design. As the
top-level design is composed of different modules without module-level
specifications, they are unable to generate deep assertions that target the
internal functionality of modules. The latter methods often rely on a golden
RTL model, which is difficult to obtain. To address the above limitations, this
paper presents a novel large language model (LLM)-aided verification framework
named DeepAssert. DeepAssert is capable of analyzing the invocation
relationships between modules and extracting independent specifications for
each module with its I/O port information. These extracted specifications are
subsequently used to guide LLMs to automatically generate fine-grained deep
assertions for these modules. Our evaluation demonstrates that DeepAssert
significantly outperforms existing methods such as AssertLLM and Spec2Assertion
in generating high-quality deep assertions for modules. Furthermore, when
integrated with these methods, DeepAssert can enhance the overall quality of
the assertions generated. This allows for a more comprehensive and effective
verification process.

</details>


### [9] [LEAP: LLM Inference on Scalable PIM-NoC Architecture with Balanced Dataflow and Fine-Grained Parallelism](https://arxiv.org/abs/2509.14781)
*Yimin Wang,Yue Jiet Chong,Xuanyao Fong*

Main category: cs.AR

TL;DR: 这篇论文提出了LEAP加速器，通过内存计算融合(PIM)和计算网络芯片(NoC)的协同设计，优化大语言模型的推理效果，实现了显著的吞吐量和能效提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理存在内存、计算和数据交互挑战，需要一种能够协同计算/内存/通信的非冬尼曼加速器来提高性能和能效。

Method: 提出LEAP加速器，结合PIM和NoC技术，根据数据动态性分配矩阵乘法计算任务，通过偏向设计空间搜索优化模型分割和映射，采用细粒度并行和分块技术实现高吞吐量数据流。

Result: 在Llama 1B/8B/13B模型上评估，与A100 GPU相比，吞吐量提升约第2.55倍，能量效率提升约第71.94倍。

Conclusion: LEAP加速器通过计算/内存/通信协同设计，有效解决了LLM推理的性能瓶颈，为大语言模型推理提供了高效的硬件加速方案。

Abstract: Large language model (LLM) inference has been a prevalent demand in daily
life and industries. The large tensor sizes and computing complexities in LLMs
have brought challenges to memory, computing, and databus. This paper proposes
a computation/memory/communication co-designed non-von Neumann accelerator by
aggregating processing-in-memory (PIM) and computational network-on-chip (NoC),
termed LEAP. The matrix multiplications in LLMs are assigned to PIM or NoC
based on the data dynamicity to maximize data locality. Model partition and
mapping are optimized by heuristic design space exploration. Dedicated
fine-grained parallelism and tiling techniques enable high-throughput dataflow
across the distributed resources in PIM and NoC. The architecture is evaluated
on Llama 1B/8B/13B models and shows $\sim$2.55$\times$ throughput (tokens/sec)
improvement and $\sim$71.94$\times$ energy efficiency (tokens/Joule) boost
compared to the A100 GPU.

</details>


### [10] [NEURAL: An Elastic Neuromorphic Architecture with Hybrid Data-Event Execution and On-the-fly Attention Dataflow](https://arxiv.org/abs/2509.15036)
*Yuehai Chen,Farhad Merchant*

Main category: cs.AR

TL;DR: NEURAL是一种基于混合数据-事件执行范式的神经形态架构，通过解耦稀疏感知处理和神经元计算，使用弹性FIFO，实现了高效的脉冲神经网络加速，在资源利用和能效方面显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有SNN硬件实现受限于脉冲稀疏性和多时间步执行，导致延迟增加和能效降低，需要新的架构设计来解决这些问题。

Method: 提出NEURAL架构，采用混合数据-事件执行范式，解耦稀疏感知处理与神经元计算，使用弹性FIFO；集成W2TTFS机制替代平均池化；引入基于知识蒸馏的训练框架构建单时间步SNN模型。

Result: 在算法层面，VGG-11模型在CIFAR-10和CIFAR-100上准确率分别提升3.20%和5.13%；在架构层面，相比现有SNN加速器，资源利用率降低50%，能效提升1.97倍。

Conclusion: NEURAL架构通过创新的混合执行范式和训练方法，有效解决了SNN硬件实现的延迟和能效问题，为高效神经形态计算提供了可行方案。

Abstract: Spiking neural networks (SNNs) have emerged as a promising alternative to
artificial neural networks (ANNs), offering improved energy efficiency by
leveraging sparse and event-driven computation. However, existing hardware
implementations of SNNs still suffer from the inherent spike sparsity and
multi-timestep execution, which significantly increase latency and reduce
energy efficiency. This study presents NEURAL, a novel neuromorphic
architecture based on a hybrid data-event execution paradigm by decoupling
sparsity-aware processing from neuron computation and using elastic
first-in-first-out (FIFO). NEURAL supports on-the-fly execution of spiking
QKFormer by embedding its operations within the baseline computing flow without
requiring dedicated hardware units. It also integrates a novel
window-to-time-to-first-spike (W2TTFS) mechanism to replace average pooling and
enable full-spike execution. Furthermore, we introduce a knowledge distillation
(KD)-based training framework to construct single-timestep SNN models with
competitive accuracy. NEURAL is implemented on a Xilinx Virtex-7 FPGA and
evaluated using ResNet-11, QKFResNet-11, and VGG-11. Experimental results
demonstrate that, at the algorithm level, the VGG-11 model trained with KD
improves accuracy by 3.20% on CIFAR-10 and 5.13% on CIFAR-100. At the
architecture level, compared to existing SNN accelerators, NEURAL achieves a
50% reduction in resource utilization and a 1.97x improvement in energy
efficiency.

</details>


### [11] [Voyager: An End-to-End Framework for Design-Space Exploration and Generation of DNN Accelerators](https://arxiv.org/abs/2509.15205)
*Kartik Prabhu,Jeffrey Yu,Xinyuan Allen Pan,Zhouhua Xie,Abigail Aleshire,Zihan Chen,Ammar Ali Ratnani,Priyanka Raina*

Main category: cs.AR

TL;DR: Voyager是一个基于HLS的DNN加速器设计框架，提供高度可配置的参数化设计、支持多种数据类型和量化方案，并集成了端到端的软件编译器，能够自动生成高性能的硬件加速器设计。


<details>
  <summary>Details</summary>
Motivation: 现有的DNN加速器设计方法存在参数化有限、无法生成高性能量产就绪设计、数据类型支持不足、缺乏集成编译器等问题，需要一种更自动化、可扩展的解决方案。

Method: 基于高层次综合(HLS)的设计空间探索框架，支持技术节点、时钟频率、处理单元数量、片上缓存大小等参数配置，提供PyTorch编译器进行端到端网络映射。

Result: 在视觉和语言模型上实现高达99.8%的利用率，相比之前生成器延迟降低61%、面积减少56%，性能与手工优化加速器相当但自动化程度更高。

Conclusion: Voyager框架能够高效自动化地生成高性能DNN加速器设计，解决了现有方法的局限性，为硬件加速器设计提供了可扩展的解决方案。

Abstract: While deep neural networks (DNNs) have achieved state-of-the-art performance
in fields from computer vision to natural language processing, efficiently
running these computationally demanding models requires hardware accelerators.
However, designing these accelerators is a time-consuming, labor-intensive
process that does not scale well. While prior efforts have sought to automate
DNN accelerator generation, they offer limited parameterization, cannot produce
high-performance, tapeout-ready designs, provide limited support for datatypes
and quantization schemes, and lack an integrated, end-to-end software compiler.
This work proposes Voyager, a high-level synthesis (HLS)-based framework for
design space exploration (DSE) and generation of DNN accelerators. Voyager
overcomes the limitations of prior work by offering extensive configurability
across technology nodes, clock frequencies, and scales, with customizable
parameters such as number of processing elements, on-chip buffer sizes, and
external memory bandwidth. Voyager supports a wider variety of datatypes and
quantization schemes versus prior work, including both built-in floating-point,
posit and integer formats, as well as user-defined formats with both per-tensor
scaling and microscaling quantization. Voyager's PyTorch-based compiler
efficiently maps networks end-to-end on the generated hardware, with support
for quantization, fusion, and tiling. We evaluate Voyager on state-of-the-art
vision and language models. Voyager enables fast DSE with full-dataset accuracy
evaluation for datatypes and quantization schemes. Generated designs achieve a
high utilization across models and scales, up to 99.8%, and outperform prior
generators with up to 61% lower latency and 56% lower area. Compared to
hand-optimized accelerators, Voyager achieves comparable performance, while
offering much greater automation in design and workload mapping.

</details>
