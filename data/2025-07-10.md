<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Pyrosome: Verified Compilation for Modular Metatheory](https://arxiv.org/abs/2507.06360)
*Dustin Jamner,Gabriel Kammer,Ritam Nag,Adam Chlipala*

Main category: cs.PL

TL;DR: Pyrosome是一个模块化语言元理论的通用框架，支持可扩展语义和编译，通过Coq实现。其核心创新是支持语言、目标语言和编译器的新规则添加，同时保持等价性。


<details>
  <summary>Details</summary>
Motivation: 传统语义推理技术通常与特定语言和编译器结构绑定，缺乏灵活性和可扩展性。Pyrosome旨在解决这一问题，提供一种可扩展的验证编译器框架。

Method: Pyrosome通过依赖排序的等式理论定义编程语言语义，编译器正确性证明简化为类型检查和等式推理。支持垂直组合和功能扩展。

Result: 案例研究展示了从System F到CPS翻译和闭包转换的多阶段编译器，支持逐步扩展功能并重用原有定理。

Conclusion: Pyrosome为语言设计和编译器验证提供了灵活且可扩展的框架，适用于多种语言特性和编译场景。

Abstract: We present Pyrosome, a generic framework for modular language metatheory that
embodies a novel approach to extensible semantics and compilation, implemented
in Coq. Common techniques for semantic reasoning are often tied to the specific
structures of the languages and compilers that they support. In Pyrosome,
verified compilers are fully extensible, meaning that to extend a language
(even with a new kind of effect) simply requires defining and verifying the
compilation of the new feature, reusing the old correctness theorem for all
other cases. The novel enabling idea is an inductive formulation of equivalence
preservation that supports the addition of new rules to the source language,
target language, and compiler.
  Pyrosome defines a formal, deeply embedded notion of programming languages
with semantics given by dependently sorted equational theories, so all
compiler-correctness proofs boil down to type-checking and equational
reasoning. We support vertical composition of any compilers expressed in our
framework in addition to feature extension. As a case study, we present a
multipass compiler from System F with simple references, through CPS
translation and closure conversion. Specifically, we demonstrate how we can
build such a compiler incrementally by starting with a compiler for simply
typed lambda-calculus and adding natural numbers, the unit type, recursive
functions, and a global heap, then extending judgments with a type environment
and adding type abstraction, all while reusing the original theorems. We also
present a linear version of the simply typed CPS pass and compile a small
imperative language to the simply typed target to show how Pyrosome handles
substructural typing and imperative features.

</details>


### [2] [Fast Collection Operations from Indexed Stream Fusion](https://arxiv.org/abs/2507.06456)
*Scott Kovach,Praneeth Kolichala,Kyle A. Miller,David Broman,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: 提出了一种高效遍历和组合关联集合数据结构的系统，无需专用编译器或分阶段编译即可实现高效性和可组合性。


<details>
  <summary>Details</summary>
Motivation: 解决传统迭代器库在效率和可组合性上的局限性，同时避免依赖专用编译器。

Method: 基于索引流的表示方法，实现无中间分配的复杂连接操作。

Result: 在Lean、Morphic和Rust中实现该库，并在Lean中提供了功能正确性的机械化证明。

Conclusion: 该系统在保持高效和可组合的同时，无需依赖专用编译器基础设施。

Abstract: We present a system of efficient methods for traversing and combining
associative collection data structures. A distinguishing feature of the system
is that, like traditional sequential iterator libraries, it does not require
specialized compiler infrastructure or staged compilation for efficiency and
composability. By using a representation based on indexed streams, the library
can express complex joins over input collections while using no intermediate
allocations. We implement the library for the Lean, Morphic, and Rust
programming languages and provide a mechanized proof of functional correctness
in Lean.

</details>


### [3] [Finding Compiler Bugs through Cross-Language Code Generator and Differential Testing](https://arxiv.org/abs/2507.06584)
*Qiong Feng,Xiaotian Ma,Ziyuan Feng,Marat Akhin,Wei Song,Peng Liang*

Main category: cs.PL

TL;DR: 论文提出了CrossLangFuzzer框架，用于检测跨语言编译中的编译器错误，成功发现多个编译器中的错误，并分析了其根本原因。


<details>
  <summary>Details</summary>
Motivation: 跨语言编译的正确性研究不足，现有工作多集中于单语言编译验证，因此需要填补这一研究空白。

Method: 提出CrossLangFuzzer框架，使用通用中间表示（IR）生成跨语言测试程序，并应用三种变异技术增强多样性。

Result: 发现了多个编译器中的24个错误，其中TypeChanger变异技术效果最佳。

Conclusion: 研究首次专注于跨语言编译错误的检测与诊断，为多语言环境下的编译器正确性提供了改进方向。

Abstract: Compilers play a central role in translating high-level code into executable
programs, making their correctness essential for ensuring code safety and
reliability. While extensive research has focused on verifying the correctness
of compilers for single-language compilation, the correctness of cross-language
compilation - which involves the interaction between two languages and their
respective compilers - remains largely unexplored. To fill this research gap,
we propose CrossLangFuzzer, a novel framework that introduces a universal
intermediate representation (IR) for JVM-based languages and automatically
generates cross-language test programs with diverse type parameters and complex
inheritance structures. After generating the initial IR, CrossLangFuzzer
applies three mutation techniques - LangShuffler, FunctionRemoval, and
TypeChanger - to enhance program diversity. By evaluating both the original and
mutated programs across multiple compiler versions, CrossLangFuzzer
successfully uncovered 10 confirmed bugs in the Kotlin compiler, 4 confirmed
bugs in the Groovy compiler, 7 confirmed bugs in the Scala 3 compiler, 2
confirmed bugs in the Scala 2 compiler, and 1 confirmed bug in the Java
compiler. Among all mutators, TypeChanger is the most effective, detecting 11
of the 24 compiler bugs. Furthermore, we analyze the symptoms and root causes
of cross-compilation bugs, examining the respective responsibilities of
language compilers when incorrect behavior occurs during cross-language
compilation. To the best of our knowledge, this is the firstwork specifically
focused on identifying and diagnosing compiler bugs in cross-language
compilation scenarios. Our research helps to understand these challenges and
contributes to improving compiler correctness in multi-language environments.

</details>


### [4] [Sound Interval-Based Synthesis for Probabilistic Programs](https://arxiv.org/abs/2507.06939)
*Guilherme Espada,Alcides Fonseca*

Main category: cs.PL

TL;DR: 提出一种类型系统和类型导向的合成算法，自动选择概率程序模型，解决传统方法依赖统计专家的问题。


<details>
  <summary>Details</summary>
Motivation: 概率编程需统计专家手动选择模型，限制了其广泛应用。

Method: 设计类型系统静态拒绝无效程序，结合类型导向合成算法和启发式搜索处理大搜索空间。

Result: 方法优于随机搜索和DaPPer，特别在复杂程序上表现更佳。

Conclusion: 该技术显著提升合成效率，支持如遗传编程等复杂应用。

Abstract: Probabilistic programming has become a standard practice to model stochastic
events and learn about the behavior of nature in different scientific contexts,
ranging from Genetics and Ecology to Linguistics and Psychology. However,
domain practitioners (such as biologists) also need to be experts in statistics
in order to select which probabilistic model is suitable for a given particular
problem, relying then on probabilistic inference engines such as Stan, Pyro or
Edward to fine-tune the parameters of that particular model. Probabilistic
Programming would be more useful if the model selection is made automatic,
without requiring statistics expertise from the end user. Automatically
selecting the model is challenging because of the large search space of
probabilistic programs needed to be explored, because the fact that most of
that search space contains invalid programs, and because invalid programs may
only be detected in some executions, due to its probabilistic nature. We
propose a type system to statically reject invalid probabilistic programs, a
type-directed synthesis algorithm that guarantees that generated programs are
type-safe by construction, and an heuristic search procedure to handle the vast
search space. We collect a number of probabilistic programs from the
literature, and use them to compare our method with both a type-agnostic random
search, and a data-guided method from the literature (DaPPer). Our results show
that our technique both outperforms random search and DaPPer, specially on more
complex programs. This drastic performance difference in synthesis allows for
fast sampling of programs and enables techniques that previously suffered from
the complexity of synthesis, such as Genetic Programming, to be applied.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Designing Parallel Algorithms for Community Detection using Arachne](https://arxiv.org/abs/2507.06471)
*Fuhuan Li,Zhihui Du,David A. Bader*

Main category: cs.DC

TL;DR: 本文介绍了基于Arachne框架的并行社区检测算法（Label Propagation和Louvain），显著提升了性能，优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 随着图数据的广泛应用，需要高效且可扩展的社区检测算法。

Method: 在Arachne框架中并行实现Label Propagation和Louvain算法。

Result: 实验显示，Arachne方法比NetworkX快710倍，比igraph快75倍，比NetworKit快12倍。

Conclusion: Arachne框架的并行实现显著提升了社区检测算法的性能，且开源可用。

Abstract: The rise of graph data in various fields calls for efficient and scalable
community detection algorithms. In this paper, we present parallel
implementations of two widely used algorithms: Label Propagation and Louvain,
specifically designed to leverage the capabilities of Arachne which is a
Python-accessible, open-source framework for large-scale graph analysis. Our
implementations achieve substantial speedups over existing Python-based tools
like NetworkX and igraph, which lack efficient parallelization, and are
competitive with parallel frameworks such as NetworKit. Experimental results
show that Arachne-based methods outperform these baselines, achieving speedups
of up to 710x over NetworkX, 75x over igraph, and 12x over NetworKit.
Additionally, we analyze the scalability of our implementation under varying
thread counts, demonstrating how different phases contribute to overall
performance gains of the parallel Louvain algorithm. Arachne, including our
community detection implementation, is open-source and available at
https://github.com/Bears-R-Us/arkouda-njit .

</details>


### [6] [Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient GPU Sharing](https://arxiv.org/abs/2507.06608)
*Xiaoxiang Shi,Colin Cai,Junjia Du,Zhanda Zhu,Xingda Wei,Zhihao Jia*

Main category: cs.DC

TL;DR: 论文提出了一种在同一GPU内动态分配资源的方法，以解决预填充和解码阶段的资源冲突问题，显著提高了吞吐量和降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的预填充-解码（PD）解耦方法通常需要多个GPU，硬件需求高。研究旨在探索是否能在单个GPU内实现解耦，以提高资源利用率。

Method: 通过分析GPU资源的边际效益，动态分配同一GPU的资源给预填充和解码阶段，避免干扰。

Result: Nexus系统在多种模型和工作负载下，实现了高达2.2倍的吞吐量提升、20倍的TTFT降低和2.5倍的TBT降低。

Conclusion: 在同一GPU内动态分配资源是可行的，能够显著提升性能并减少硬件需求。

Abstract: Current prefill-decode (PD) disaggregation is typically deployed at the level
of entire serving engines, assigning separate GPUs to handle prefill and decode
phases. While effective at reducing latency, this approach demands more
hardware. To improve GPU utilization, Chunked Prefill mixes prefill and decode
requests within the same batch, but introduces phase interference between
prefill and decode.
  While existing PD disaggregation solutions separate the phases across GPUs,
we ask: can the same decoupling be achieved within a single serving engine? The
key challenge lies in managing the conflicting resource requirements of prefill
and decode when they share the same hardware. In this paper, we first show that
chunked prefill requests cause interference with decode requests due to their
distinct requirements for GPU resources. Second, we find that GPU resources
exhibit diminishing returns. Beyond a saturation point, increasing GPU
allocation yields negligible latency improvements. This insight enables us to
split a single GPU's resources and dynamically allocate them to prefill and
decode on the fly, effectively disaggregating the two phases within the same
GPU.
  Across a range of models and workloads, our system Nexus achieves up to 2.2x
higher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM. It also
outperforms SGLang with up to 2x higher throughput, 2x lower TTFT, and 1.7x
lower TBT, and achieves 1.4x higher throughput than vLLM-disaggregation using
only half the number of GPUs.

</details>


### [7] [Towards Efficient and Scalable Distributed Vector Search with RDMA](https://arxiv.org/abs/2507.06653)
*Xiangyu Zhi,Meng Chen,Xiao Yan,Baotong Lu,Hui Li,Qianxi Zhang,Qi Chen,James Cheng*

Main category: cs.DC

TL;DR: CoTra是一个分布式向量搜索系统，通过算法与系统协同设计解决计算与通信效率的冲突，显著提升查询吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大规模向量搜索受限于单机内存和带宽，需分布式执行以扩展性能。

Method: 采用聚类数据分区、异步执行和任务推送等技术，结合系统优化（如任务调度、通信批处理）。

Result: 在16台机器上，CoTra的查询吞吐量提升至单机的9.8-13.4倍，优于基线2.12-3.58倍。

Conclusion: CoTra通过协同设计与优化，有效解决了分布式向量搜索的扩展性问题。

Abstract: Similarity-based vector search facilitates many important applications such
as search and recommendation but is limited by the memory capacity and
bandwidth of a single machine due to large datasets and intensive data read. In
this paper, we present CoTra, a system that scales up vector search for
distributed execution. We observe a tension between computation and
communication efficiency, which is the main challenge for good scalability,
i.e., handling the local vectors on each machine independently blows up
computation as the pruning power of vector index is not fully utilized, while
running a global index over all machines introduces rich data dependencies and
thus extensive communication. To resolve such tension, we leverage the fact
that vector search is approximate in nature and robust to asynchronous
execution. In particular, we run collaborative vector search over the machines
with algorithm-system co-designs including clustering-based data partitioning
to reduce communication, asynchronous execution to avoid communication stall,
and task push to reduce network traffic. To make collaborative search
efficient, we introduce a suite of system optimizations including task
scheduling, communication batching, and storage format. We evaluate CoTra on
real datasets and compare with four baselines. The results show that when using
16 machines, the query throughput of CoTra scales to 9.8-13.4x over a single
machine and is 2.12-3.58x of the best-performing baseline at 0.95 recall@10.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [SLDB: An End-To-End Heterogeneous System-on-Chip Benchmark Suite for LLM-Aided Design](https://arxiv.org/abs/2507.06376)
*Elisavet Lydia Alvanaki,Kevin Lee,Luca P. Carloni*

Main category: cs.AR

TL;DR: 论文介绍了System-Level Design Benchmark（SLDB），一个用于评估大语言模型在系统级集成和配置任务中的数据集，填补了现有基准在系统级设计上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和基准主要关注组件级信息和低复杂度设计，缺乏对系统级设计的支持，因此需要SLDB来填补这一空白。

Method: SLDB包含10个基线SoC设计的基准套件，通过合成库将其组件组合成多种不同的基于瓦片的SoC，并提供完整的SoC配置、加速器集成代码等。

Result: SLDB提供了全面的系统级设计评估工具，支持加速器集成和系统配置，并与ESP平台兼容。

Conclusion: SLDB为评估LLM在系统级设计中的能力提供了高质量基准，推动了AI在硬件设计中的应用。

Abstract: Over the last few years, Large Language Models (LLMs) have emerged as a
valuable tool for Electronic Design Automation (EDA). State-of-the-art research
in LLM-aided design has demonstrated the ability of LLMs to generate
syntactically correct RTL code, showcasing encouraging prospects for
integrating AI into the hardware design process. A key enabler of these
advancements is the availability of high-quality benchmarks to evaluate new
approaches. However, existing datasets and benchmarks fall short of
system-level design, as they focus primarily on component-level information and
low-complexity designs. To address this gap, we introduce the System-Level
Design Benchmark (SLDB), a dataset tailored for evaluating LLMs in system-level
integration and configuration tasks. SLDB includes a curated benchmark suite of
10 baseline SoC designs, whose components can be combined into an exponential
number of distinct tile-based SoCs through a synthetic library. The dataset
provides full SoC configurations, accelerator integration code, communication
parameters, and accelerator-aware system configurations, along with
testing-application code, compatible with the ESP platform[1].

</details>


### [9] [Towards LLM-based Root Cause Analysis of Hardware Design Failures](https://arxiv.org/abs/2507.06512)
*Siyu Qiu,Muzhi Wang,Raheel Afsharmazayejani,Mohammad Moradi Shahmiri,Benjamin Tan,Hammond Pearce*

Main category: cs.AR

TL;DR: LLMs在数字硬件设计中用于解释合成和仿真阶段的设计问题和错误根源，展示了高准确率。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在硬件设计过程中的应用潜力，特别是在错误分析和安全性评估方面。

Method: 使用OpenAI的o3-mini推理模型及其他先进模型，结合检索增强生成技术，测试34种错误场景。

Result: o3-mini模型在pass@5评分下达到100%准确率，其他模型和配置通常超过80%，检索增强生成下超过90%。

Conclusion: LLMs在硬件设计错误分析中表现优异，为广泛应用奠定了基础。

Abstract: With advances in large language models (LLMs), new opportunities have emerged
to develop tools that support the digital hardware design process. In this
work, we explore how LLMs can assist with explaining the root cause of design
issues and bugs that are revealed during synthesis and simulation, a necessary
milestone on the pathway towards widespread use of LLMs in the hardware design
process and for hardware security analysis. We find promising results: for our
corpus of 34 different buggy scenarios, OpenAI's o3-mini reasoning model
reached a correct determination 100% of the time under pass@5 scoring, with
other state of the art models and configurations usually achieving more than
80% performance and more than 90% when assisted with retrieval-augmented
generation.

</details>


### [10] [Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics](https://arxiv.org/abs/2507.07044)
*Mehrdad Morsali,Chengwei Zhou,Deniz Najafi,Sreetama Sarkar,Pietro Mercati,Navid Khoshavi,Peter Beerel,Mahdi Nikdast,Gourav Datta,Shaahin Angizi*

Main category: cs.AR

TL;DR: OptoViT是一种基于硅光子学的近传感器ViT加速器，通过混合电子-光子架构和区域感知优化，实现高效能视觉处理。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在计算和内存需求上较高，限制了其在严格能源和带宽限制场景下的部署。

Method: 采用混合电子-光子架构，光学核心处理矩阵乘法，电子部分处理非线性函数；引入轻量级MGNet减少冗余计算；通过量化感知训练和矩阵分解优化ViT。

Result: 实验显示OptoViT达到100.4 KFPS/W，节能84%，准确率损失小于1.6%。

Conclusion: OptoViT为边缘设备提供了可扩展且高效的ViT部署方案。

Abstract: Vision Transformers (ViTs) have emerged as a powerful architecture for
computer vision tasks due to their ability to model long-range dependencies and
global contextual relationships. However, their substantial compute and memory
demands hinder efficient deployment in scenarios with strict energy and
bandwidth limitations. In this work, we propose OptoViT, the first near-sensor,
region-aware ViT accelerator leveraging silicon photonics (SiPh) for real-time
and energy-efficient vision processing. Opto-ViT features a hybrid
electronic-photonic architecture, where the optical core handles
compute-intensive matrix multiplications using Vertical-Cavity Surface-Emitting
Lasers (VCSELs) and Microring Resonators (MRs), while nonlinear functions and
normalization are executed electronically. To reduce redundant computation and
patch processing, we introduce a lightweight Mask Generation Network (MGNet)
that identifies regions of interest in the current frame and prunes irrelevant
patches before ViT encoding. We further co-optimize the ViT backbone using
quantization-aware training and matrix decomposition tailored for photonic
constraints. Experiments across device fabrication, circuit and architecture
co-design, to classification, detection, and video tasks demonstrate that
OptoViT achieves 100.4 KFPS/W with up to 84% energy savings with less than 1.6%
accuracy loss, while enabling scalable and efficient ViT deployment at the
edge.

</details>
