<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Converting IEC 61131-3 LD into SFC Using Large Language Model: Dataset and Testing](https://arxiv.org/abs/2509.12593)
*Yimin Zhang,Mario de Sousa*

Main category: cs.PL

TL;DR: 使用精调的大语言模型实现桶式图向顺序功能图的自动转换，突破了传统方法的领域知识缺乏和状态爆炸问题


<details>
  <summary>Details</summary>
Motivation: 解决PLC编程中桶式图转换为顺序功能图的挑战，利用AI发展提供新方案，充实领域数据集缺口

Method: 构建符合IEC 61131-3标准的SFC-LD文本表示对数据集，对LLM进行精调训练

Result: 精调模型在某些数据集上达到91%准确率，最低也有79%，显示良好效果

Conclusion: LLM通过适当训练和表示能够有效支持LD-SFC转换，该方法具有可行性和强大潜力

Abstract: In the domain of Programmable Logic Controller (PLC) programming, converting
a Ladder Diagram (LD) into a Sequential Function Chart (SFC) is an inherently
challenging problem, primarily due to the lack of domain-specific knowledge and
the issue of state explosion in existing algorithms. However, the rapid
development of Artificial Intelligence (AI) - especially Large Language Model
(LLM) - offers a promising new approach.
  Despite this potential, data-driven approaches in this field have been
hindered by a lack of suitable datasets. To address this gap, we constructed
several datasets consisting of paired textual representations of SFC and LD
programs that conform to the IEC 61131-3 standard.
  Based on these datasets, we explored the feasibility of automating the LD-SFC
conversion using LLM. Our preliminary experiments show that a fine-tuned LLM
model achieves up to 91% accuracy on certain dataset, with the lowest observed
accuracy being 79%, suggesting that with proper training and representation,
LLMs can effectively support LD-SFC conversion. These early results highlight
the viability and future potential of this approach.

</details>


### [2] [Efficient Compilation of Algorithms into Compact Linear Programs](https://arxiv.org/abs/2509.13006)
*Shermin Khosravi,David Bremner*

Main category: cs.PL

TL;DR: 通过分层线性流水线技术，将程序结构分解为同步区域，大幅减小线性规划问题的规模，实现了最多25倍的规模缩减和解器性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管现有LP编译器能将算法转换为多项式规模的LP，但生成的LP规模过大，对解器构成挑战，需要方法生成更紧凑的LP规模。

Method: 提出分层线性流水线技术，将嵌套程序结构分解为同步区域，通过编译时参数定义执行迁移，局部化约束条件和变量，大幅缩减LP规模规模。

Result: 在最大完工时间问题和带权最小生成树问题上，实现了最多25倍的LP规模缩减，并在商业和非商业LP解器上获得了显著的性能提升。

Conclusion: 该方法能够系统地生成紧凑的线性规划规模，为处理具有指数扩展复杂度的问题提供了有效解决方案，在实践中显示出重要的应用价值。

Abstract: Linear Programming (LP) is widely applied in industry and is a key component
of various other mathematical problem-solving techniques. Recent work
introduced an LP compiler translating polynomial-time, polynomial-space
algorithms into polynomial-size LPs using intuitive high-level programming
languages, offering a promising alternative to manually specifying each set of
constraints through Algebraic Modeling Languages (AMLs). However, the resulting
LPs, while polynomial in size, are often extremely large, posing challenges for
existing LP solvers. In this paper, we propose a novel approach for generating
substantially smaller LPs from algorithms. Our goal is to establish
minimum-size compact LP formulations for problems in P having natural
formulations with exponential extension complexities. Our broader vision is to
enable the systematic generation of Compact Integer Programming (CIP)
formulations for problems with exponential-size IPs having polynomial-time
separation oracles. To this end, we introduce a hierarchical linear pipelining
technique that decomposes nested program structures into synchronized regions
with well-defined execution transitions -- functions of compile-time
parameters. This decomposition allows us to localize LP constraints and
variables within each region, significantly reducing LP size without the loss
of generality, ensuring the resulting LP remains valid for all inputs of size
$n$. We demonstrate the effectiveness of our method on two benchmark problems
-- the makespan problem, which has exponential extension complexity, and the
weighted minimum spanning tree problem -- both of which have exponential-size
natural LPs. Our results show up to a $25$-fold reduction in LP size and
substantial improvements in solver performance across both commercial and
non-commercial LP solvers.

</details>


### [3] [Pleasant Imperative Program Proofs with GallinaC](https://arxiv.org/abs/2509.13019)
*Frédéric Fort,David Nowak,Vlad Rusu*

Main category: cs.PL

TL;DR: GallinaC是一个在Rocq证明助手的Gallina语言中浅层嵌入的图灵完备命令式语言，旨在为底层系统软件提供机器验证的正确性证明


<details>
  <summary>Details</summary>
Motivation: 当前命令式编程语言语义过于宽松，使得正确性证明变得繁琐且容易出错。需要一种既支持命令式编程范式又具有良好语义的证明导向语言

Method: 在Gallina函数式语言中浅层嵌入图灵完备的命令式语言，包含通用的无界while循环，使命令式程序证明可以使用与纯函数式程序相同的策略

Result: 原型实现已证明GallinaC的可行性，成功验证了未知大小链表反转程序的正确性。目前正专注于GallinaC中间表示与CompCert后端入口语言Cminor的前向模拟

Conclusion: GallinaC为底层系统软件的正确性证明提供了一种有前景的方法，通过在证明助手中开发语言工具来实现机器验证的高置信度正确性保证

Abstract: Even with the increase of popularity of functional programming, imperative
programming remains a key programming paradigm, especially for programs
operating at lower levels of abstraction. When such software offers key
components of a Trusted Computing Base (TCB), e.g. an operating system kernel,
it becomes desirable to provide mathematical correctness proofs.
  However, current real-world imperative programming languages possess
"expressive", i.e. overly permissive, semantics. Thus, producing correctness
proofs of such programs becomes tedious and error-prone, requiring to take care
of numerous "administrative" details. Ideally, a proof-oriented imperative
language should feature well-behaved semantics while allowing imperative
idioms.
  To obtain a high-degree of confidence in the correctness of such a language,
its tools should be developed inside a proof-assistant such that program proofs
are machine checked.
  We present GallinaC, a shallow embedding of a Turing-complete imperative
language directly inside the functional programming language of the Rocq proof
assistant, Gallina. In particular, it features a truly generic and unbounded
while loop. Having a functional core means proofs about GallinaC programs may
use the same tactics as proofs about pure functional ones.
  Work on GallinaC is still under progress, but we present first promising
results. A prototype implementation has shown the viability of GallinaC with
the correctness proof of a list reversal procedure for linked-lists of unknown
size. We currently focus on the forward simulation between the GallinaC
intermediate representation (IR) and Cminor, the entry language of the CompCert
back-end.

</details>


### [4] [Navigating the Python Type Jungle](https://arxiv.org/abs/2509.13022)
*Andrei Nacu,Dorel Lucanu*

Main category: cs.PL

TL;DR: 这篇论文提出了Python类型系统的正式化基础，用类型理论概念简洁地描述Python的类型系统，以解决当前规范分散的问题


<details>
  <summary>Details</summary>
Motivation: Python的类型系统实践性地发展成为功能强大但理论分散的系统，规范散落，需要正式化来解决这个问题

Method: 使用类型理论概念建立正式基础，展示Python类型系统可以被雅地描述

Result: 提供了一个正式的基础框架，能够简洁地描述Python的类型系统

Conclusion: 这个工作是向类型推断工具未来发展的关键第一步，为Python类型系统的理论化和工具支持奠定了基础

Abstract: Python's typing system has evolved pragmatically into a powerful but
theoretically fragmented system, with scattered specifications. This paper
proposes a formalization to address this fragmentation. The central
contribution is a formal foundation that uses concepts from type theory to
demonstrate that Python's type system can be elegantly described. This work
aims to serve as a crucial first step toward the future development of type
inference tools.

</details>


### [5] [Try-Mopsa: Relational Static Analysis in Your Pocket](https://arxiv.org/abs/2509.13128)
*Raphaël Monat*

Main category: cs.PL

TL;DR: Try-Mopsa是一个基于JavaScript的Mopsa静态分析平台简化版，可在浏览器中运行，无需安装，便于教学和入门使用


<details>
  <summary>Details</summary>
Motivation: 静态分析器依赖复杂、安装困难，阻碍了采用和学习。需要一种无需安装、易于访问的解决方案来降低使用门槛

Method: 将Mopsa静态分析平台的核心组件编译为JavaScript，开发纯客户端Web应用，支持桌面和移动设备，保留关系数值域等核心功能

Result: 成功开发了Try-Mopsa，提供响应式界面，完全在浏览器中运行，具备Mopsa所有核心组件功能

Conclusion: Try-Mopsa为静态分析的学习和入门提供了便捷平台，通过Web技术降低了使用门槛，特别适合教学目的

Abstract: Static analyzers are complex pieces of software with large dependencies. They
can be difficult to install, which hinders adoption and creates barriers for
students learning static analysis. This work introduces Try-Mopsa: a
scaled-down version of the Mopsa static analysis platform, compiled into
JavaScript to run purely as a client-side application in web browsers.
Try-Mopsa provides a responsive interface that works on both desktop and mobile
devices. Try-Mopsa features all the core components of Mopsa. In particular, it
supports relational numerical domains. We present the interface, changes and
adaptations required to have a pure JavaScript version of Mopsa. We envision
Try-Mopsa as a convenient platform for onboarding or teaching purposes.

</details>


### [6] [Rebound: Efficient, Expressive, and Well-Scoped Binding](https://arxiv.org/abs/2509.13261)
*Noé De Santo,Stephanie Weirich*

Main category: cs.PL

TL;DR: Rebound是一个Haskell库，通过静态跟踪作用域和一级环境来自动化处理绑定结构的替换、alpha等价等操作，确保de Bruijn索引的正确性，并提供性能优化。


<details>
  <summary>Details</summary>
Motivation: 处理绑定结构时，手动维护de Bruijn索引的微妙不变性容易出错，需要自动化工具来确保正确性和提供性能优化。

Method: 使用一级环境映射变量到表达式，静态跟踪作用域，自动化定义替换、alpha等价等操作，并提供环境优化替换应用。

Result: 成功实现了包含依赖类型编程语言类型检查器在内的多种语言特性，性能测试显示比竞争库更快。

Conclusion: Rebound库通过作用域跟踪和环境优化，有效解决了绑定结构处理的正确性和性能问题，适用于复杂的语言实现场景。

Abstract: We introduce the Rebound library that supports well-scoped term
representations in Haskell and automates the definition of substitution,
alpha-equivalence, and other operations that work with binding structures. The
key idea of our design is the use of first-class environments that map
variables to expressions in some new scope. By statically tracking scopes,
users of this library gain confidence that they have correctly maintained the
subtle invariants that stem from using de Bruijn indices. Behind the scenes,
Rebound uses environments to optimize the application of substitutions, while
providing explicit access to these data structures when desired. We demonstrate
that this library is expressive by using it to implement a wide range of
language features with sophisticated uses of binding and several different
operations that use this abstract syntax. Our examples include pi-forall, a
tutorial implementation of a type checker for a dependently-typed programming
language. Finally, we benchmark Rebound to understand its performance
characteristics and find that it produces faster code than competing libraries.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [IsoSched: Preemptive Tile Cascaded Scheduling of Multi-DNN via Subgraph Isomorphism](https://arxiv.org/abs/2509.12208)
*Boran Zhao,Zihang Yuan,Yanbin Hu,Haiming Zhai,Haoruo Zhang,Wenzhe Zhao,Tian Xia,Pengju Ren*

Main category: cs.DC

TL;DR: IsoSched是首个支持抢占式多DNN调度的TSS架构框架，通过ILP规划、子图同构、负载均衡和加速匹配算法，在延迟、吞吐量和能效方面优于现有LTS-PRM方法。


<details>
  <summary>Details</summary>
Motivation: 现有TSS工作缺乏对抢占的支持，而先前的抢占方案依赖LTS并继承其开销，需要为具有复杂拓扑的并发DNN应用开发高效的抢占式TSS框架。

Method: 1) 将复杂拓扑图调度建模为整数线性规划和子图同构问题；2) 使用层连接和分割(LCS)进行瓦片流水线负载均衡；3) 采用基于Ullmann的算法和MCTS加速子图匹配，使用CSR压缩矩阵编码减少内存使用。

Result: IsoSched在延迟约束吞吐量(LBT)、加速比和能效方面优于LTS-PRM方法(PREMA、Planaria等)，并在不同任务复杂度下比TSS-NPRM(HASP)实现更高的关键任务满足率。

Conclusion: IsoSched成功解决了在TSS架构上实现抢占式多DNN调制的挑战，为具有严格延迟要求的并发DNN应用提供了高效的调度解决方案。

Abstract: Deploying deep neural network (DNN) accelerators with Layer Temporal
Scheduling (LTS) often incurs significant overheads (e.g., energy and latency),
as intermediate activations must be cached in DRAM. To alleviate this, Tile
Spatial Scheduling (TSS) reduces such costs by fragmenting inter-layer data
into smaller tiles communicated via on-chip links.However, many emerging
applications require concurrent execution of multiple DNNs with complex
topologies, where critical tasks must preempt others to meet stringent latency
requirements (e.g., in autonomous driving, obstacle detection must complete
within tens of milliseconds). Existing TSS works lack support for preemption,
while prior preemption schemes rely on LTS and thus inherit its overheads. This
highlights the need for preemptive and efficient TSS-based frameworks. Yet,
realizing such systems is challenging due to the complexity of enabling
preemption in graphs with large-scale topologies (e.g., modern large language
models may contain tens of thousands of edges). To tackle this, we present
IsoSched, the first framework enabling preemptive multi-DNN scheduling on TSS
architecture. IsoSched first formulates scheduling of complex-topology graphs
as an integer-linear program (ILP) and subgraph isomorphism problem; second, it
applies Layer Concatenate and Split (LCS) for load balancing in tile pipelines;
third, it employs an Ullmann-based algorithm enhanced by Monte Carlo Tree
Search (MCTS) to accelerate subgraph matching, and uses compact matrix encoding
(i.e., Compressed Sparse Row, CSR) to reduce memory usage. IsoSched outperforms
LTS-PRM approaches (i.e., PREMA, Planaria, CD-MSA, MoCA) in Latency-Bound
Throughput (LBT), speedup, and energy efficiency, and achieves higher critical
task satisfaction than TSS-NPRM (i.e., HASP) across varying task complexities.

</details>


### [8] [A Proposal for High-Level Architectural Model Capable of Expressing Various Data Collaboration Platform and Data Space Concepts](https://arxiv.org/abs/2509.12210)
*Masaru Dobashi,Kohei Toshimitsu,Hirotsugu Seike,Miki Kanno,Genki Horie,Noboru Koshizuka*

Main category: cs.DC

TL;DR: 数据空间高层次架构模型(DS-HLAM)，通过有限状态自动机理进行严格定义，支持区域性数据协作平台的互操作性和数字主权要求。


<details>
  <summary>Details</summary>
Motivation: 解决不同地区数据协作平台在互操作性和数字主权保护方面的挑战，需要一个统一的架构模型来表达和规范多样化的数据协作平台。

Method: 提出DS-HLAM框架，采用数学严格的定义方法，通过有限状态自动机理来形式化成功条件和要求。

Result: 该框架能够支持不同地区实施的数据协作平台实现互操作性，同时保持数字主权的要求。

Conclusion: DS-HLAM为区域性数据协作平台提供了一个统一的架构模型，通过数学严格的形式化方法确保了系统的可靠性和互操作性。

Abstract: This paper proposes "Data Space High-Level Architecture Model" (DS-HLAM) for
expressing diverse data collaboration platforms across regional
implementations. The framework introduces mathematically rigorous definitions
with success conditions formalized through finite state automata theory,
enabling interoperability while preserving digital sovereignty requirements.

</details>


### [9] [TinyServe: Query-Aware Cache Selection for Efficient LLM Serving](https://arxiv.org/abs/2509.12211)
*Dong Liu,Yanxuan Yu*

Main category: cs.DC

TL;DR: TinyServe是一个轻量级LLM服务系统，通过结构化KV稀疏化、查询感知页面选择和硬件高效注意力内核，实现3.4倍加速和2倍内存节省，精度损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理时KV缓存访问的高内存和延迟开销是主要挑战，需要高效的部署解决方案。

Method: 提出查询感知页面选择机制，利用边界框元数据估计查询与KV缓存块的注意力相关性，实现选择性KV加载；开发融合CUDA内核，集成页面评分、稀疏内存访问和掩码注意力。

Result: 实验显示达到3.4倍加速和超过2倍内存节省，精度损失可忽略；缓存重用、页面命中率和多GPU扩展分析证实其实用性。

Conclusion: TinyServe是资源受限硬件上LLM训练和推理研究的高效系统级设计解决方案。

Abstract: Serving large language models (LLMs) efficiently remains challenging due to
the high memory and latency overhead of key-value (KV) cache access during
autoregressive decoding. We present \textbf{TinyServe}, a lightweight and
extensible serving system for deploying tiny LLMs (e.g., TinyLLaMA, GPT2-345M)
with support for structured KV sparsity, plugin-based token selection, and
hardware-efficient attention kernels. Unlike prior simulation frameworks,
TinyServe executes real-time decoding with configurable sparsity strategies and
fine-grained instrumentation.
  To reduce decoding cost, we introduce a \textit{query-aware page selection}
mechanism that leverages bounding-box metadata to estimate attention relevance
between the query and KV cache blocks. This enables selective KV loading with
minimal overhead and no model modifications. Our fused CUDA kernel integrates
page scoring, sparse memory access, and masked attention in a single pass.
  Experiments show that TinyServe achieves up to \textbf{3.4x} speedup and over
\textbf{2x} memory savings with negligible accuracy drop. Additional analysis
of cache reuse, page hit rate, and multi-GPU scaling confirms its practicality
as an efficient system-level design for LLM training and inference research on
resource-constrained hardware.

</details>


### [10] [Research on fault diagnosis and root cause analysis based on full stack observability](https://arxiv.org/abs/2509.12231)
*Jian Hou*

Main category: cs.DC

TL;DR: 论文综述了故障根因分析的两大主流方法，提出了融合动态因果发现和多模态融合的KylinRCA框架，通过时间因果发现和跨模态图学习实现全局根因定位，并提供可审计的证据链。


<details>
  <summary>Details</summary>
Motivation: 随着云计算和大规模数据中心的发展，系统规模和复杂性增加导致故障频发且具有级联传播特性，需要基于可观测性数据实现高效、准确且可解释的根因分析。

Method: 提出KylinRCA框架，整合动态因果发现和多模态融合思想：通过时间因果发现描绘传播链，利用跨模态图学习实现全局根因定位和类型识别，结合基于掩码的解释方法输出可审计证据链。

Result: 设计了多维实验方案，明确了评估指标，讨论了工程挑战，为全栈可观测性下的故障诊断提供了有效解决方案。

Conclusion: 该框架融合了两种主流方法的优势，能够实现更全面和可解释的故障根因分析，为AIOps领域的故障诊断提供了新的解决方案。

Abstract: With the rapid development of cloud computing and ultra-large-scale data
centers, the scale and complexity of systems have increased significantly,
leading to frequent faults that often show cascading propagation. How to
achieve efficient, accurate, and interpretable Root Cause Analysis (RCA) based
on observability data (metrics, logs, traces) has become a core issue in AIOps.
This paper reviews two mainstream research threads in top conferences and
journals over the past five years: FaultInsight[1] focusing on dynamic causal
discovery and HolisticRCA[2] focusing on multi-modal/cross-level fusion, and
analyzes the advantages and disadvantages of existing methods. A KylinRCA
framework integrating the ideas of both is proposed, which depicts the
propagation chain through temporal causal discovery, realizes global root cause
localization and type identification through cross-modal graph learning, and
outputs auditable evidence chains combined with mask-based explanation methods.
A multi-dimensional experimental scheme is designed, evaluation indicators are
clarified, and engineering challenges are discussed, providing an effective
solution for fault diagnosis under full-stack observability.

</details>


### [11] [Towards High-Performance and Portable Molecular Docking on CPUs through Vectorization](https://arxiv.org/abs/2509.12232)
*Gianmarco Accordi,Jens Domke,Theresa Pollinger,Davide Gadioli,Gianluca Palermo*

Main category: cs.DC

TL;DR: 本文评估了现代CPU架构上的自动向量化和显式向量化性能，使用分子对接应用作为案例研究，发现自动向量化通过代码转换可以达到接近显式向量化的性能，x86 CPU性能更高但ARM在能耗和成本效益方面更具竞争力


<details>
  <summary>Details</summary>
Motivation: HPC领域新CPU架构的向量化能力提升带来了性能可移植性挑战，需要评估代码、编译器和硬件之间的复杂交互以实现峰值性能

Method: 选择分子对接应用作为案例研究，评估编译器自动向量化和显式向量化在现代长向量CPU上的性能表现，分析代码转换策略

Result: 自动向量化通过适当的代码转换可以达到与显式向量化相似的性能；x86 CPU由于更宽的向量化单元通常获得更高执行性能；ARM架构在能耗和成本效益方面表现更具竞争力

Conclusion: 研究为科学应用在HPC领域的未来发展提供了技术挑战、架构趋势和优化策略的见解，强调了性能可移植性的重要性

Abstract: Recent trends in the HPC field have introduced new CPU architectures with
improved vectorization capabilities that require optimization to achieve peak
performance and thus pose challenges for performance portability. The
deployment of high-performing scientific applications for CPUs requires
adapting the codebase and optimizing for performance. Evaluating these
applications provides insights into the complex interactions between code,
compilers, and hardware. We evaluate compiler auto-vectorization and explicit
vectorization to achieve performance portability across modern CPUs with long
vectors. We select a molecular docking application as a case study, as it
represents computational patterns commonly found across HPC workloads. We
report insights into the technical challenges, architectural trends, and
optimization strategies relevant to the future development of scientific
applications for HPC. Our results show which code transformations enable
portable auto-vectorization, reaching performance similar to explicit
vectorization. Experimental data confirms that x86 CPUs typically achieve
higher execution performance than ARM CPUs, primarily due to their wider
vectorization units. However, ARM architectures demonstrate competitive energy
consumption and cost-effectiveness.

</details>


### [12] [SynergAI: Edge-to-Cloud Synergy for Architecture-Driven High-Performance Orchestration for AI Inference](https://arxiv.org/abs/2509.12252)
*Foteini Stathopoulou,Aggelos Ferikoglou,Manolis Katsaragakis,Dimosthenis Masouros,Sotirios Xydis,Dimitrios Soudris*

Main category: cs.DC

TL;DR: SynergAI是一个用于异构边缘到云基础设施的性能和架构感知推理服务框架，通过智能调度减少QoS违规2.4倍


<details>
  <summary>Details</summary>
Motivation: AI推理服务需求快速增长，传统云部署面临网络拥塞、高能耗和隐私问题，边缘计算虽有低延迟优势但计算资源有限

Method: 基于现代推理引擎性能特征分析，结合离线和在线决策策略，在Kubernetes生态中实现智能轻量级架构感知调度

Result: 在Kubernetes环境中实现，相比现有最优解决方案平均减少2.4倍的QoS违规

Conclusion: 架构驱动的推理服务能够在新兴硬件平台上实现优化和架构感知的部署

Abstract: The rapid evolution of Artificial Intelligence (AI) and Machine Learning (ML)
has significantly heightened computational demands, particularly for
inference-serving workloads. While traditional cloud-based deployments offer
scalability, they face challenges such as network congestion, high energy
consumption, and privacy concerns. In contrast, edge computing provides
low-latency and sustainable alternatives but is constrained by limited
computational resources. In this work, we introduce SynergAI, a novel framework
designed for performance- and architecture-aware inference serving across
heterogeneous edge-to-cloud infrastructures. Built upon a comprehensive
performance characterization of modern inference engines, SynergAI integrates a
combination of offline and online decision-making policies to deliver
intelligent, lightweight, and architecture-aware scheduling. By dynamically
allocating workloads across diverse hardware architectures, it effectively
minimizes Quality of Service (QoS) violations. We implement SynergAI within a
Kubernetes-based ecosystem and evaluate its efficiency. Our results demonstrate
that architecture-driven inference serving enables optimized and
architecture-aware deployments on emerging hardware platforms, achieving an
average reduction of 2.4x in QoS violations compared to a State-of-the-Art
(SotA) solution.

</details>


### [13] [The Entropy of Parallel Systems](https://arxiv.org/abs/2509.12256)
*Temitayo Adefemi*

Main category: cs.DC

TL;DR: 本文提出了一种基于熵理论的方法来量化并行集群中组件不兼容性导致的噪声和混乱，发现系统熵与计算性能存在显著负相关关系。


<details>
  <summary>Details</summary>
Motivation: 受香农信息论启发，将熵概念应用于计算机系统，旨在量化并行集群中组件不兼容性导致的系统混乱程度。

Method: 基于图论和对数运算建立数学模型，量化并行集群中每个系统的熵，并应用于Top500榜单前10名超级计算机。

Result: 发现系统熵与计算性能存在显著负相关：LINPACK基准测试显示强负相关（r=-0.7832, p=0.0077），MLPerf混合精度基准（r=-0.6234）和HPCC综合评分（r=-0.5890）也显示中等负相关。

Conclusion: 熵框架能够有效评估超级计算机性能，低熵系统具有更高的计算效率，该方法可扩展到传统密集线性代数之外的工作负载。

Abstract: Ever since Claude Shannon used entropy for his "Mathematical Theory of
Communication", entropy has become a buzzword in research circles with
scientists applying entropy to describe any phenomena that are reminiscent of
disorder. In this paper, we used entropy to describe the incompatibility
between components in the computer, which can cause noise and disorder within
the parallel cluster. We develop a mathematical theory, primarily based on
graph theory and logarithms, to quantify the entropy of a parallel cluster by
accounting for the entropy of each system within the cluster. We proceed using
this model to calculate the entropy of the Top 10 supercomputers in the Top500
list. Our entropy framework reveals a statistically significant negative
correlation between system entropy and computational performance across the
world's fastest supercomputers. Most notably, the LINPACK benchmark
demonstrates a strong negative correlation (r = -0.7832, p = 0.0077) with our
entropy measure, indicating that systems with lower entropy consistently
achieve higher computational efficiency, this Relationship is further supported
by moderate correlations with MLPerf mixed-precision benchmarks (r = -0.6234)
and HPCC composite scores (r = -0.5890), suggesting the framework's
applicability extends beyond traditional dense linear algebra workloads.

</details>


### [14] [An End to End Edge to Cloud Data and Analytics Strategy](https://arxiv.org/abs/2509.12296)
*Vijay Kumar Butte,Sujata Butte*

Main category: cs.DC

TL;DR: 这篇论文提出了一种从边缘到云端的完整安全数据分析策略，包括设备层、边缘层和云层的参考架构


<details>
  <summary>Details</summary>
Motivation: 因为IoT设备的指数增长和企业快速采用云技术，需要开发安全高效的策略来利用边缘和云资产的能力

Method: 提供了一种端到端的安全数据分析策略，包括设备层、边缘层和云层的参考架构设计

Result: 论文提供了可以在实际生产环境中实施的完整架构方案

Conclusion: 这种端到端的安全数据分析策略和参考架构能够有效支持企业利用边缘和云计算的能力，满足实时数据处理的需求

Abstract: There is an exponential growth of connected Internet of Things (IoT) devices.
These have given rise to applications that rely on real time data to make
critical decisions quickly. Enterprises today are adopting cloud at a rapid
pace. There is a critical need to develop secure and efficient strategy and
architectures to best leverage capabilities of cloud and edge assets. This
paper provides an end to end secure edge to cloud data and analytics strategy.
To enable real life implementation, the paper provides reference architectures
for device layer, edge layer and cloud layer.

</details>


### [15] [Exploring Distributed Vector Databases Performance on HPC Platforms: A Study with Qdrant](https://arxiv.org/abs/2509.12384)
*Seth Ockerman,Amal Gueroudji,Song Young Oh,Robert Underwood,Nicholas Chia,Kyle Chard,Robert Ross,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 这篇论文通过在Polaris超算机上进行实验研究，首次实证性分析了分布式向量数据库在HPC系统中的性能特征，以指导未来的优化研究。


<details>
  <summary>Details</summary>
Motivation: 向量数据库在现代AI工作流中发挥着关键作用，但对其在高性能计算(HPC)系统中的表现特征了解很少。本文旨在填补这一研究空白，为大规模科学应用提供性能指南。

Method: 在Argonne领导计算设施的Polaris超算机上，使用Qdrant向量数据库，构建了来自BV-BRC的实际生物文本工作负载，并使用Qwen3-Embedding-4B从peS2o语料库生成嵌入向量。评估了插入、索引构建和查询延迟性能，最多使用32个工作节点。

Result: 论文获得了关于分布式向量数据库在HPC平台上的实践经验和性能数据，为评估插入、索引构建和查询操作的延迟性能提供了实证基础。

Conclusion: 这项研究是首次在HPC平台上对向量数据库性能进行系统性实证分析的重要尝试，为未来的研究和优化工作奠定了基础，将有助于推动大规模科学计算中向量数据库的高效使用。

Abstract: Vector databases have rapidly grown in popularity, enabling efficient
similarity search over data such as text, images, and video. They now play a
central role in modern AI workflows, aiding large language models by grounding
model outputs in external literature through retrieval-augmented generation.
Despite their importance, little is known about the performance characteristics
of vector databases in high-performance computing (HPC) systems that drive
large-scale science. This work presents an empirical study of distributed
vector database performance on the Polaris supercomputer in the Argonne
Leadership Computing Facility. We construct a realistic biological-text
workload from BV-BRC and generate embeddings from the peS2o corpus using
Qwen3-Embedding-4B. We select Qdrant to evaluate insertion, index construction,
and query latency with up to 32 workers. Informed by practical lessons from our
experience, this work takes a first step toward characterizing vector database
performance on HPC platforms to guide future research and optimization.

</details>


### [16] [AI Factories: It's time to rethink the Cloud-HPC divide](https://arxiv.org/abs/2509.12849)
*Pedro Garcia Lopez,Daniel Barcelona Pons,Marcin Copik,Torsten Hoefler,Eduardo Quiñones,Maciej Malawski,Peter Pietzutch,Alberto Marti,Thomas Ohlson Timoudas,Aleksander Slominski*

Main category: cs.DC

TL;DR: 这篇论文推荐在超级计算机中采用双栈方案，结合HPC和云原生技术，以满足国家AI工厂的性能和易用性需求


<details>
  <summary>Details</summary>
Motivation: 欧洲正在投资数以亿计的欧元建设AI工厂，但现有HPC超级计算机虽然性能强大，却缺乏易用性和云原生技术支持，不适合作为公众AI服务平台

Method: 提出双栈方案，将HPC高性能计算与云原生技术（如Kubernetes、对象存储）相结合，研究无服务器HPC和高性能云计算的挑战

Result: 通过这种汇聚方式，可以让HPC和云计算两种范式相互增强，既保持高性能和硬件加速优势，又提供易用的服务化前端

Conclusion: 双栈方案为国家AI工厂的建设提供了有效解决方案，能够渗透HPC和云计算之间的差距，支持更广泛的AI应用场景

Abstract: The strategic importance of artificial intelligence is driving a global push
toward Sovereign AI initiatives. Nationwide governments are increasingly
developing dedicated infrastructures, called AI Factories (AIF), to achieve
technological autonomy and secure the resources necessary to sustain robust
local digital ecosystems.
  In Europe, the EuroHPC Joint Undertaking is investing hundreds of millions of
euros into several AI Factories, built atop existing high-performance computing
(HPC) supercomputers. However, while HPC systems excel in raw performance, they
are not inherently designed for usability, accessibility, or serving as
public-facing platforms for AI services such as inference or agentic
applications. In contrast, AI practitioners are accustomed to cloud-native
technologies like Kubernetes and object storage, tools that are often difficult
to integrate within traditional HPC environments.
  This article advocates for a dual-stack approach within supercomputers:
integrating both HPC and cloud-native technologies. Our goal is to bridge the
divide between HPC and cloud computing by combining high performance and
hardware acceleration with ease of use and service-oriented front-ends. This
convergence allows each paradigm to amplify the other. To this end, we will
study the cloud challenges of HPC (Serverless HPC) and the HPC challenges of
cloud technologies (High-performance Cloud).

</details>


### [17] [Analysis and Optimization of Wireless Multimodal Federated Learning on Modal Heterogeneity](https://arxiv.org/abs/2509.12930)
*Xuefeng Han,Wen Chen,Jun Li,Ming Ding,Qingqing Wu,Kang Wei,Xiumei Deng,Yumeng Shao,Qiong Wu*

Main category: cs.DC

TL;DR: 这篇论文提出了一种聚合客户调度和带宽分配的算法(JCSBA)，用于优化无线多模态联邦学习的性能，解决模态异质性和通信约束问题。


<details>
  <summary>Details</summary>
Motivation: 多模态联邦学习在无线场景中面临模态异质性、固定延迟需求和有限通信带宽的挑战，传统的单模态联邦学习方法不适用。

Method: 基于决策级融合架构，添加单模态损失函数，并提出JCSBA算法来聚合客户调度和带宽分配，最小化性能上界。

Result: 在多模态数据集上，JCSBA算法相比传统算法将多模态准确率和单模态准确率分别提高4.06%和2.73%。

Conclusion: 该算法有效解决了无线多模态联邦学习中的模态异质性和通信挑战，显著提升了学习性能。

Abstract: Multimodal federated learning (MFL) is a distributed framework for training
multimodal models without uploading local multimodal data of clients, thereby
effectively protecting client privacy. However, multimodal data is commonly
heterogeneous across diverse clients, where each client possesses only a subset
of all modalities, renders conventional analysis results and optimization
methods in unimodal federated learning inapplicable. In addition, fixed latency
demand and limited communication bandwidth pose significant challenges for
deploying MFL in wireless scenarios. To optimize the wireless MFL performance
on modal heterogeneity, this paper proposes a joint client scheduling and
bandwidth allocation (JCSBA) algorithm based on a decision-level fusion
architecture with adding a unimodal loss function. Specifically, with the
decision results, the unimodal loss functions are added to both the training
objective and local update loss functions to accelerate multimodal convergence
and improve unimodal performance. To characterize MFL performance, we derive a
closed-form upper bound related to client and modality scheduling and minimize
the derived bound under the latency, energy, and bandwidth constraints through
JCSBA. Experimental results on multimodal datasets demonstrate that the JCSBA
algorithm improves the multimodal accuracy and the unimodal accuracy by 4.06%
and 2.73%, respectively, compared to conventional algorithms.

</details>


### [18] [Asymmetric Grid Quorum Systems for Heterogeneous Processes](https://arxiv.org/abs/2509.12942)
*Michael Senn,Christian Cachin*

Main category: cs.DC

TL;DR: 不对称网格法定数系统解决分布式系统中过程的异质故障假设问题，允许每个过程独立选择兼容的主观失故假设，无需协调。


<details>
  <summary>Details</summary>
Motivation: 解决分布式系统中过程在没有共享失故假设的情况下如何协作达成兼容失故假设的困境，突破Catch-22困境。

Method: 提出不对称网格法定数系统，基于过程的定性属性异质性，让每个过程独立选择最符合其主观视角的法定数系统，这些选择在设计上自动兼容。

Result: 设计出了能够在无需协调的情况下支持异质信任假设的法定数系统类型，解决了过程间失故假设不兼容的问题。

Conclusion: 不对称网格法定数系统为云计算平台和区块链等领域提供了一种新的解决方案，能够在异质信任环境中实现可靠的分布式协作。

Abstract: Quorum systems are a common way to formalize failure assumptions in
distributed systems. Traditionally, these assumptions are shared by all
involved processes. More recently, systems have emerged which allow processes
some freedom in choosing their own, subjective or asymmetric, failure
assumptions. For such a system to work, individual processes' assumptions must
be compatible. However, this leads to a Catch-22-style scenario: How can
processes collaborate to agree on compatible failure assumptions when they have
no compatible failure assumptions to start with?
  We introduce asymmetric grid quorum systems that allow a group of processes
to specify heterogeneous trust assumptions independently of each other and
without coordination. They are based on qualitative attributes describing how
the processes differ. Each process may select a quorum system from this class
that aligns best with its subjective view. The available choices are designed
to be compatible by definition, thereby breaking the cycling dependency.
Asymmetric grid quorum systems have many applications that range from cloud
platforms to blockchain networks.

</details>


### [19] [Space-Time Trade-off in Bounded Iterated Memory](https://arxiv.org/abs/2509.13157)
*Guillermo Toyos-Marfurt,Petr Kuznetsov*

Main category: cs.DC

TL;DR: 本文研究了有界共享内存模型中模拟无界全信息协议的轮次复杂度，建立了内存位容量与所需轮次之间的精确关系，并提出了渐近最优的有界全信息算法。


<details>
  <summary>Details</summary>
Motivation: 异步可计算定理(ACT)假设无界容量的共享内存变量，但实际系统中内存容量是有限的。虽然已有研究表明有界变量可以通过额外轮次实现相同的计算能力，但内存位容量与所需轮次之间的确切关系尚不清楚。

Method: 通过分析协议复合体（表示可达状态的组合结构），推导必要条件并设计针对可用位数b的有界全信息算法。研究基于迭代共享内存模型，从常规读写寄存器到原子和即时快照模型。

Result: 对于n>2个进程，实现全信息协议所需的轮次复杂度满足Ω((n!)^{r-1} · 2^{n-b})。所提出的有界全信息算法在迭代收集模型中渐近最优，在基于快照的模型中与最优解相差线性因子n。

Conclusion: 本文建立了有界共享内存模型中模拟无界全信息协议的轮次复杂度的精确下界，并设计了接近最优的算法，为实际分布式系统的设计提供了理论基础。

Abstract: The celebrated asynchronous computability theorem (ACT) characterizes tasks
solvable in the read-write shared-memory model using the unbounded
full-information protocol, where in every round of computation, each process
shares its complete knowledge of the system with the other processes.
Therefore, ACT assumes shared-memory variables of unbounded capacity. It has
been recently shown that boundedvariables can achieve the same computational
power at the expense of extra rounds. However, the exact relationship between
the bit capacity of the shared memory and the number of rounds required in
order to implement one round of the full-information protocol remained unknown.
  In this paper, we focus on the asymptotic round complexity of bounded
iterated shared-memory algorithms that simulate, up to isomorphism, the
unbounded full-information protocol. We relate the round complexity to the
number of processes $n$, the number of iterations of the full information
protocol $r$, and the bit size per shared-memory entry $b$. By analyzing the
corresponding protocol complex, a combinatorial structure representing
reachable states, we derive necessary conditions and present a bounded
full-information algorithm tailored to the bits available $b$ per shared memory
entry. We show that for $n>2$, the round complexity required to implement the
full-information protocol satisfies $\Omega((n!)^{r-1} \cdot 2^{n-b})$. Our
results apply to a range of iterated shared-memory models, from regular
read-write registers to atomic and immediate snapshots. Moreover, our bounded
full-information algorithm is asymptotically optimal for the iterated collect
model and within a linear factor $n$ of optimal for the snapshot-based models.

</details>


### [20] [Scaling Up Throughput-oriented LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management](https://arxiv.org/abs/2509.13201)
*Thanh Son Phung,Douglas Thain*

Main category: cs.DC

TL;DR: 通过流量导向的动态资源分配和普遍上下文管理，利用机会资源来大幅缩短LLM应用的执行时间，避免长时间排队和贵重硬件购买


<details>
  <summary>Details</summary>
Motivation: 解决LLM应用对计算资源的巨大需求与集群资源供给不足之间的矛盾，避免长时间排队或过高硬件成本

Method: 普遍上下文管理技术，利用LLM应用中的共享计算上下文，通过机制和策略实现在机会资源上的无缝上下文重用

Result: 在机会资源上运行的LLM应用执行时间减少98.1%

Conclusion: 普遍上下文管理为非延迟敏感的LLM应用提供了高效的资源利用方案，能够显著提升性能并降低成本

Abstract: The widespread growth in LLM developments increasingly demands more
computational power from clusters than what they can supply. Traditional LLM
applications inherently require huge static resource allocations, which force
users to either wait in a long job queue and accept progress delay, or buy
expensive hardware to fulfill their needs and exacerbate the demand-supply
problem. However, not all LLM applications are latency-sensitive and can
instead be executed in a throughput-oriented way. This throughput orientation
allows a dynamic allocation that opportunistically pools available resources
over time, avoiding both the long queue and expensive GPU purchases.
Effectively utilizing opportunistic resources brings numerous challenges
nevertheless. Our solution, pervasive context management, exploits the common
computational context in LLM applications and provides mechanisms and policies
that allow seamless context reuse on opportunistic resources. Our evaluation
shows an LLM application with pervasive context management on opportunistic
resources reduces its execution time by 98.1%.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [21] [A Scalable Architecture for Efficient Multi-bit Fully Homomorphic Encryption](https://arxiv.org/abs/2509.12676)
*Jiaao Ma,Ceyu Xu,Lisa Wu Wills*

Main category: cs.AR

TL;DR: Taurus硬件加速器通过支持10位密文的新型FFT单元和内存优化策略，显著提升多比特TFHE计算效率，在CPU、GPU和现有TFHE加速器上分别实现2600倍、1200倍和7倍加速，并首次实现GPT-2等大语言模型的隐私保护推理。


<details>
  <summary>Details</summary>
Motivation: 解决全同态加密中多比特TFHE方案数值表示范围有限的问题，提升隐私保护计算在云环境中的实用性和可扩展性。

Method: 设计Taurus硬件加速器，采用新型FFT单元支持10位密文，通过密钥重用策略优化内存带宽，并开发具有操作去重功能的编译器提升内存利用率。

Result: 相比CPU实现2600倍加速，相比GPU实现1200倍加速，比现有最佳TFHE加速器快7倍，首次实现GPT-2等大语言模型的隐私保护推理。

Conclusion: Taurus显著提升了多比特TFHE的计算效率，使隐私保护计算在云环境中更加实用和可扩展，为大规模应用如大语言模型推理提供了可行的解决方案。

Abstract: In the era of cloud computing, privacy-preserving computation offloading is
crucial for safeguarding sensitive data. Fully Homomorphic Encryption (FHE)
enables secure processing of encrypted data, but the inherent computational
complexity of FHE operations introduces significant computational overhead on
the server side. FHE schemes often face a tradeoff between efficiency and
versatility. While the CKKS scheme is highly efficient for polynomial
operations, it lacks the flexibility of the binary TFHE (Torus-FHE) scheme,
which offers greater versatility but at the cost of efficiency. The recent
multi-bit TFHE extension offers greater flexibility and performance by
supporting native non-polynomial operations and efficient integer processing.
However, current implementations of multi-bit TFHE are constrained by its
narrower numeric representation, which prevents its adoption in applications
requiring wider numeric representations.
  To address this challenge, we introduce Taurus, a hardware accelerator
designed to enhance the efficiency of multi-bit TFHE computations. Taurus
supports ciphertexts up to 10 bits by leveraging novel FFT units and optimizing
memory bandwidth through key reuse strategies. We also propose a compiler with
operation deduplication to improve memory utilization. Our experiment results
demonstrate that Taurus achieves up to 2600x speedup over a CPU, 1200x speedup
over a GPU, and up to 7x faster compared to the previous state-of-the-art TFHE
accelerator. Moreover, Taurus is the first accelerator to demonstrate
privacy-preserving inference with large language models such as GPT-2. These
advancements enable more practical and scalable applications of
privacy-preserving computation in cloud environments.

</details>


### [22] [HPIM: Heterogeneous Processing-In-Memory-based Accelerator for Large Language Models Inference](https://arxiv.org/abs/2509.12993)
*Cenlin Duan,Jianlei Yang,Rubing Yang,Yikun Wang,Yiou Wang,Lingkun Long,Yingjie Qi,Xiaolin He,Ao Zhou,Xueyan Wang,Weisheng Zhao*

Main category: cs.AR

TL;DR: HPIM是一种内存中心异构内存处理加速器，专为LLM推理设计，通过SRAM-PIM和HBM-PIM子系统协同工作，实现22.8倍性能提升


<details>
  <summary>Details</summary>
Motivation: 传统GPU等计算中心加速器在处理LLM推理时存在资源利用率低和内存带宽瓶颈问题，特别是在自回归解码阶段

Method: 采用软硬件协同设计方法，结合专用编译器框架和异构硬件架构，智能分区工作负载：注意力操作映射到SRAM-PIM，GEMV计算分配到HBM-PIM

Result: 相比NVIDIA A100 GPU实现峰值22.8倍加速，优于现有PIM加速器

Conclusion: HPIM是加速大规模LLM推理的高实用性和可扩展性解决方案

Abstract: The deployment of large language models (LLMs) presents significant
challenges due to their enormous memory footprints, low arithmetic intensity,
and stringent latency requirements, particularly during the autoregressive
decoding stage. Traditional compute-centric accelerators, such as GPUs, suffer
from severe resource underutilization and memory bandwidth bottlenecks in these
memory-bound workloads. To overcome these fundamental limitations, we propose
HPIM, the first memory-centric heterogeneous Processing-In-Memory (PIM)
accelerator that integrates SRAM-PIM and HBM-PIM subsystems designed
specifically for LLM inference. HPIM employs a software-hardware co-design
approach that combines a specialized compiler framework with a heterogeneous
hardware architecture. It intelligently partitions workloads based on their
characteristics: latency-critical attention operations are mapped to the
SRAM-PIM subsystem to exploit its ultra-low latency and high computational
flexibility, while weight-intensive GEMV computations are assigned to the
HBM-PIM subsystem to leverage its high internal bandwidth and large storage
capacity. Furthermore, HPIM introduces a tightly coupled pipeline strategy
across SRAM-PIM and HBM-PIM subsystems to maximize intra-token parallelism,
thereby significantly mitigating serial dependency of the autoregressive
decoding stage. Comprehensive evaluations using a cycle-accurate simulator
demonstrate that HPIM significantly outperforms state-of-the-art accelerators,
achieving a peak speedup of up to 22.8x compared to the NVIDIA A100 GPU.
Moreover, HPIM exhibits superior performance over contemporary PIM-based
accelerators, highlighting its potential as a highly practical and scalable
solution for accelerating large-scale LLM inference.

</details>


### [23] [Orthrus: Dual-Loop Automated Framework for System-Technology Co-Optimization](https://arxiv.org/abs/2509.13029)
*Yi Ren,Baokang Peng,Chenhao Xue,Kairong Guo,Yukun Wang,Guoyao Cheng,Yibo Lin,Lining Zhang,Guangyu Sun*

Main category: cs.AR

TL;DR: Orthrus是一个双循环自动化框架，通过系统级和工艺级的协同优化，在7nm工艺上实现了12.5%的延迟降低和61.4%的功耗节省


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律收益递减，系统技术协同优化(STCO)成为维持VLSI行业扩展趋势的有前景方法。现有研究缺乏对高效STCO方法的充分讨论，特别是在解决设计层次间的信息鸿沟和导航庞大的跨层设计空间方面

Method: 提出Orthrus双循环自动化框架：系统级通过新颖机制优先优化关键标准单元，使用贝叶斯优化探索帕累托前沿的法线方向指导工艺级优化；工艺级利用系统感知洞察优化标准单元库，采用神经网络辅助的增强差分进化算法高效优化工艺参数

Result: 在7nm工艺上的实验结果表明，与基线方法相比，Orthrus在等功耗下实现12.5%的延迟降低，在等延迟下实现61.4%的功耗节省，建立了新的STCO帕累托前沿

Conclusion: Orthrus框架成功解决了系统技术协同优化中的信息鸿沟和设计空间导航挑战，通过系统级和工艺级的协同优化实现了显著的性能提升

Abstract: With the diminishing return from Moore's Law, system-technology
co-optimization (STCO) has emerged as a promising approach to sustain the
scaling trends in the VLSI industry. By bridging the gap between system
requirements and technology innovations, STCO enables customized optimizations
for application-driven system architectures. However, existing research lacks
sufficient discussion on efficient STCO methodologies, particularly in
addressing the information gap across design hierarchies and navigating the
expansive cross-layer design space. To address these challenges, this paper
presents Orthrus, a dual-loop automated framework that synergizes system-level
and technology-level optimizations. At the system level, Orthrus employs a
novel mechanism to prioritize the optimization of critical standard cells using
system-level statistics. It also guides technology-level optimization via the
normal directions of the Pareto frontier efficiently explored by Bayesian
optimization. At the technology level, Orthrus leverages system-aware insights
to optimize standard cell libraries. It employs a neural network-assisted
enhanced differential evolution algorithm to efficiently optimize technology
parameters. Experimental results on 7nm technology demonstrate that Orthrus
achieves 12.5% delay reduction at iso-power and 61.4% power savings at
iso-delay over the baseline approaches, establishing new Pareto frontiers in
STCO.

</details>
