<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 40]
- [cs.AR](#cs.AR) [Total: 14]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Modular GPU Programming with Typed Perspectives](https://arxiv.org/abs/2511.11939)
*Manya Bansal,Daniel Sainati,Joseph W. Cutler,Saman Amarasinghe,Jonathan Ragan-Kelley*

Main category: cs.PL

TL;DR: Prism是一个新的GPU编程语言，通过类型化视角在类型层面体现线程控制粒度，解决了GPU编程中个体线程控制与集体操作之间的张力问题，实现了模块化编程而不牺牲性能。


<details>
  <summary>Details</summary>
Motivation: 现代GPU编程需要在控制单个线程行为的同时跟踪多个线程协同执行集体操作（如Tensor Core指令），这种张力使得模块化编程容易出错。封装集体操作的函数虽然按线程调用，但必须由线程组协作执行。

Method: 引入Prism语言，核心思想是类型化视角（typed perspectives），在类型层面体现程序员控制线程行为的粒度。设计了Prism语言，实现了编译器，并在核心演算Bundl中建立了理论基础。

Result: 在Prism中实现了最先进的GPU内核，发现Prism为程序员提供了必要的安全保障，能够自信地编写模块化代码而不牺牲性能。

Conclusion: Prism语言通过类型化视角恢复了GPU编程的模块性，同时为程序员提供了对集体操作的低级控制，实现了高性能和编程安全性的平衡。

Abstract: To achieve peak performance on modern GPUs, one must balance two frames of mind: issuing instructions to individual threads to control their behavior, while simultaneously tracking the convergence of many threads acting in concert to perform collective operations like Tensor Core instructions. The tension between these two mindsets makes modular programming error prone. Functions that encapsulate collective operations, despite being called per-thread, must be executed cooperatively by groups of threads.
  In this work, we introduce Prism, a new GPU language that restores modularity while still giving programmers the low-level control over collective operations necessary for high performance. Our core idea is typed perspectives, which materialize, at the type level, the granularity at which the programmer is controlling the behavior of threads. We describe the design of Prism, implement a compiler for it, and lay its theoretical foundations in a core calculus called Bundl. We implement state-of-the-art GPU kernels in Prism and find that it offers programmers the safety guarantees needed to confidently write modular code without sacrificing performance.

</details>


### [2] [The Search for Constrained Random Generators](https://arxiv.org/abs/2511.12253)
*Harrison Goldstein,Hila Peleg,Cassia Torczon,Daniel Sainati,Leonidas Lampropoulos,Benjamin C. Pierce*

Main category: cs.PL

TL;DR: 提出基于演绎程序综合的约束随机生成方法，使用生成器的指称语义和综合规则自动合成正确生成器，处理递归谓词时通过catamorphisms和anamorphisms匹配实现。


<details>
  <summary>Details</summary>
Motivation: 解决基于属性测试中的约束随机生成问题，即给定程序值的谓词，随机采样满足该谓词的所有值，这对PBT中具有前置条件的可执行规范至关重要。

Method: 基于生成器指称语义的合成规则集，将递归谓词重写为catamorphisms并与适当的anamorphisms匹配，使用标准证明搜索策略实现综合算法。

Result: 实现了名为Palamedes的可扩展Lean定理证明器库，综合算法建立在标准证明搜索策略上，减少了实现负担并能受益于Lean证明自动化的进步。

Conclusion: 该方法为约束随机生成问题提供了新颖的解决方案，理论上比其他递归函数综合方法更简单，同时保持高度表达能力。

Abstract: Among the biggest challenges in property-based testing (PBT) is the constrained random generation problem: given a predicate on program values, randomly sample from the set of all values satisfying that predicate, and only those values. Efficient solutions to this problem are critical, since the executable specifications used by PBT often have preconditions that input values must satisfy in order to be valid test cases, and satisfying values are often sparsely distributed.
  We propose a novel approach to this problem using ideas from deductive program synthesis. We present a set of synthesis rules, based on a denotational semantics of generators, that give rise to an automatic procedure for synthesizing correct generators. Our system handles recursive predicates by rewriting them as catamorphisms and then matching with appropriate anamorphisms; this is theoretically simpler than other approaches to synthesis for recursive functions, yet still extremely expressive.
  Our implementation, Palamedes, is an extensible library for the Lean theorem prover. The synthesis algorithm itself is built on standard proof-search tactics, reducing implementation burden and allowing the algorithm to benefit from further advances in Lean proof automation.

</details>


### [3] [Equivalence Checking of ML GPU Kernels](https://arxiv.org/abs/2511.12638)
*Kshitij Dubey,Benjamin Driscoll,Anjiang Wei,Neeraj Kayal,Rahul Sharma,Alex Aiken*

Main category: cs.PL

TL;DR: 提出了第一个GPU内核等价检查器VOLTA，用于形式化验证手工优化、LLM生成和编译器优化的机器学习内核的正确性。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习和大型语言模型的发展，GPU内核执行成本巨大，成为优化重点。现有LLM生成内核的方法缺乏形式化正确性保证。

Method: 开发了VOLTA等价检查器，证明其对于一类定义良好的GPU内核（包括机器学习程序）是完备的，能够验证卷积、矩阵乘法、注意力机制等计算。

Result: 实现了首个GPU内核等价检查器，能够验证各种机器学习计算，为内核优化提供形式化正确性保证。

Conclusion: VOLTA为GPU内核优化提供了可靠的形式化验证工具，填补了LLM生成内核缺乏正确性保证的空白。

Abstract: With the rapid progress of deep learning and large language models (LLMs), companies now spend enormous sums executing GPU kernels. These kernels have, therefore, become prime targets for aggressive optimization. Recent efforts increasingly leverage LLMs to generate GPU kernels, but make no formal guarantees about the generated kernels. We present the first equivalence checker for GPU kernels and use it to formally verify the correctness of machine learning (ML) kernels optimized by hand, by LLMs, and by compilers. We show that our equivalence checker is sound and, for a well-defined class of GPU kernels which includes the programs of interest, complete. Our implementation, VOLTA, can verify ML computations such as convolutions, matrix multiplications, and various attention mechanisms.

</details>


### [4] [Cost-Driven Synthesis of Sound Abstract Interpreters](https://arxiv.org/abs/2511.13663)
*Qiuhan Gu,Avaljot Singh,Gagandeep Singh*

Main category: cs.PL

TL;DR: 利用LLMs合成神经网络验证中跨多个抽象域的全局可靠抽象解释器，通过约束优化和数学化成本函数解决抽象解释中的可靠性保证问题。


<details>
  <summary>Details</summary>
Motivation: 构建提供全局可靠性保证的抽象解释器仍然是抽象解释中的主要障碍，研究现代LLMs能否通过合成可靠的抽象解释器来减轻这一负担。

Method: 将合成建模为约束优化问题，引入基于数学的成本函数衡量不可靠性，开发统一框架结合LLM生成、语法语义验证和成本引导反馈机制。

Result: 实证结果表明该框架不仅能匹配手工构建转换器的质量，更重要的是发现了复杂非线性算子的可靠高精度转换器，这些在现有文献中缺失。

Conclusion: LLMs能够有效合成可靠的抽象解释器，为神经网络验证提供新的自动化解决方案，填补了复杂非线性算子抽象解释的空白。

Abstract: Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [ACE-GNN: Adaptive GNN Co-Inference with System-Aware Scheduling in Dynamic Edge Environments](https://arxiv.org/abs/2511.11586)
*Ao Zhou,Jianlei Yang,Tong Qiao,Yingjie Qi,Xinming Wei,Cenlin Duan,Weisheng Zhao,Chunming Hu*

Main category: cs.DC

TL;DR: ACE-GNN是一个自适应图神经网络协同推理框架，针对动态边缘环境设计，通过系统级抽象和两种新颖的预测方法实现性能感知，支持运行时方案优化，并在PP和DP之间自适应调度，显著提升系统性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于离线模型分割和流水线并行的静态部署方法性能受环境动态性（如网络波动和多设备访问）影响，这些问题尚未得到解决。

Method: 通过系统级抽象和两种新颖的预测方法实现性能感知；引入数据并行机制在运行时优化空间中实现PP和DP的自适应调度；实施高效的批量推理策略和专用通信中间件。

Result: 在各种应用和边缘设置下的广泛实验表明，ACE-GNN相比GCoDE实现了高达12.7倍的加速和82.3%的能耗节省，相比Fograph实现了11.7倍更好的能效。

Conclusion: ACE-GNN是首个针对动态边缘环境的自适应GNN协同推理框架，能够显著提升系统性能和稳定性，有效应对环境动态性挑战。

Abstract: The device-edge co-inference paradigm effectively bridges the gap between the high resource demands of Graph Neural Networks (GNNs) and limited device resources, making it a promising solution for advancing edge GNN applications. Existing research enhances GNN co-inference by leveraging offline model splitting and pipeline parallelism (PP), which enables more efficient computation and resource utilization during inference. However, the performance of these static deployment methods is significantly affected by environmental dynamics such as network fluctuations and multi-device access, which remain unaddressed. We present ACE-GNN, the first Adaptive GNN Co-inference framework tailored for dynamic Edge environments, to boost system performance and stability. ACE-GNN achieves performance awareness for complex multi-device access edge systems via system-level abstraction and two novel prediction methods, enabling rapid runtime scheme optimization. Moreover, we introduce a data parallelism (DP) mechanism in the runtime optimization space, enabling adaptive scheduling between PP and DP to leverage their distinct advantages and maintain stable system performance. Also, an efficient batch inference strategy and specialized communication middleware are implemented to further improve performance. Extensive experiments across diverse applications and edge settings demonstrate that ACE-GNN achieves a speedup of up to 12.7x and an energy savings of 82.3% compared to GCoDE, as well as 11.7 better energy efficiency than Fograph.

</details>


### [6] [Distributed Q-learning-based Shortest-Path Tree Construction in IoT Sensor Networks](https://arxiv.org/abs/2511.11598)
*Van-Vi Vo,Tien-Dung Nguyen,Duc-Tai Le,Hyunseung Choo*

Main category: cs.DC

TL;DR: 提出基于分布式Q学习的最短路径树构建框架，用于物联网传感器网络的高效路由，相比传统集中式算法减少通信开销并提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统集中式路由算法（如Dijkstra）在动态分布式物联网环境中计算密集且不适用，需要一种能够适应拓扑变化、减少通信开销的分布式路由方案。

Method: 采用分布式Q学习框架，传感器节点基于本地信息独立学习最优下一跳决策，状态定义基于节点位置和路由历史，奖励函数激励向汇聚节点前进并惩罚低效路径。

Result: 在100-500节点网络模拟中，路由准确率接近最优（300节点以上网络超过99%），小规模网络仅有1-2跳的轻微偏差，对性能影响可忽略。

Conclusion: Q学习为资源受限的物联网网络提供了自主、鲁棒的路由解决方案，相比传统协议具有更好的可扩展性和能效。

Abstract: Efficient routing in IoT sensor networks is critical for minimizing energy consumption and latency. Traditional centralized algorithms, such as Dijkstra's, are computationally intensive and ill-suited for dynamic, distributed IoT environments. We propose a novel distributed Q-learning framework for constructing shortest-path trees (SPTs), enabling sensor nodes to independently learn optimal next-hop decisions using only local information. States are defined based on node positions and routing history, with a reward function that incentivizes progression toward the sink while penalizing inefficient paths. Trained on diverse network topologies, the framework generalizes effectively to unseen networks. Simulations across 100 to 500 nodes demonstrate near-optimal routing accuracy (over 99% for networks with more than 300 nodes), with minor deviations (1-2 extra hops) in smaller networks having negligible impact on performance. Compared to centralized and flooding-based methods, our approach reduces communication overhead, adapts to topology changes, and enhances scalability and energy efficiency. This work underscores the potential of Q-learning for autonomous, robust routing in resource-constrained IoT networks, offering a scalable alternative to traditional protocols.

</details>


### [7] [Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators](https://arxiv.org/abs/2511.11601)
*Elliott Wen,Sean Ma,Ewan Tempero,Jens Dietrich,Daniel Luo,Jiaxing Shen,Kaiqi Zhao,Bruce Sham,Yousong Song,Jiayi Hua,Jia Hong*

Main category: cs.DC

TL;DR: 本文首次实证研究了异构AI加速器上机器学习模型的输出差异，发现新兴厂商（Mac、华为）相比NVIDIA支持更少的算子，输出差异率更高，且更容易在模型编译过程中失败。


<details>
  <summary>Details</summary>
Motivation: 随着AMD、Intel、Mac和华为等新兴厂商提供成本更低的AI加速器替代方案，需要验证这些平台在兼容性和性能方面的声称，并评估异构硬件生态系统中机器学习行为的一致性挑战。

Method: 使用自动化流水线，从4000个真实模型合成超过10万个变体模型，并在5种企业级加速器上执行测试，分析算子支持、输出差异、编译失败等问题。

Result: Mac和华为的新平台比NVIDIA少支持至少17%的算子，输出差异率超过5%，编译加速失败率更高，编译模型与标准执行模式输出存在明显差异，发现PyTorch中7个实现缺陷和40个平台特定问题。

Conclusion: 在日益多样化的硬件生态系统中实现一致的机器学习行为面临重大挑战，新兴AI平台在算子支持、输出一致性和编译稳定性方面与主导厂商存在显著差距。

Abstract: While NVIDIA remains the dominant provider of AI accelerators within cloud data center, emerging vendors such as AMD, Intel, Mac, and Huawei offer cost-effective alternatives with claims of compatibility and performance. This paper presents the first empirical study investigating divergence in machine learning model across heterogeneous AI accelerators. Utilizing an automated pipeline, we synthesize over 100,000 variant models derived from 4,000 real-world models and execute them across five different enterprise-grade accelerators. Our findings suggest that newer AI platforms from Mac and Huawei support at least 17\% fewer operators than NVIDIA. These platforms also exhibit a higher rate of output discrepancies (exceeding 5\%), which stem from differences in operator implementations, handling of exceptional numerical values, and instruction scheduling. They are also more susceptible to failures during model compilation-based acceleration, and in some cases, the compiled models produce outputs that differ noticeably from those generated using the standard execution mode. In addition, we identify 7 implementation flaws in PyTorch and 40 platform-specific issues across vendors. These results underscore the challenges of achieving consistent machine learning behavior in an increasingly diverse hardware ecosystem.

</details>


### [8] [Machine learning-based cloud resource allocation algorithms: a comprehensive comparative review](https://arxiv.org/abs/2511.11603)
*Deep Bodra,Sushil Khairnar*

Main category: cs.DC

TL;DR: 对10种AI/ML算法在云资源分配中的比较分析，发现混合架构方法在性能、成本和能效方面显著优于传统方法，其中边缘计算环境部署准备度最高。


<details>
  <summary>Details</summary>
Motivation: 传统启发式方法无法满足现代云基础设施的多目标优化需求，需要更先进的AI/ML方法来解决复杂动态工作负载下的资源分配挑战。

Method: 系统评估了10种算法，分为四类：深度强化学习方法、神经网络架构、增强的传统机器学习方法和多智能体系统，进行对比分析。

Result: 分析显示在多个指标上都有显著改进，包括makespan减少、成本优化和能效提升，混合架构方法持续优于单一方法。

Conclusion: 混合AI/ML架构在复杂动态计算环境中表现出色，为学术研究和行业实践提供了下一代云资源分配策略的关键见解。

Abstract: Cloud resource allocation has emerged as a major challenge in modern computing environments, with organizations struggling to manage complex, dynamic workloads while optimizing performance and cost efficiency. Traditional heuristic approaches prove inadequate for handling the multi-objective optimization demands of existing cloud infrastructures. This paper presents a comparative analysis of state-of-the-art artificial intelligence and machine learning algorithms for resource allocation. We systematically evaluate 10 algorithms across four categories: Deep Reinforcement Learning approaches, Neural Network architectures, Traditional Machine Learning enhanced methods, and Multi-Agent systems. Analysis of published results demonstrates significant performance improvements across multiple metrics including makespan reduction, cost optimization, and energy efficiency gains compared to traditional methods. The findings reveal that hybrid architectures combining multiple artificial intelligence and machine learning techniques consistently outperform single-method approaches, with edge computing environments showing the highest deployment readiness. Our analysis provides critical insights for both academic researchers and industry practitioners seeking to implement next-generation cloud resource allocation strategies in increasingly complex and dynamic computing environments.

</details>


### [9] [PACE Solver Description: twin_width_fmi](https://arxiv.org/abs/2511.11605)
*David Balaban,Adrian Miclăuş*

Main category: cs.DC

TL;DR: 本文介绍了三种求解最小支配集问题的启发式算法：贪心算法greedy-ln、模拟退火局部搜索算法，以及最终提交的hedom5算法。hedom5结合了迭代贪心构造、反向剪枝和局部改进步骤，在PACE 2025竞赛中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 最小支配集问题是经典的NP难问题，需要高效的启发式算法来求解大规模图实例。PACE 2025竞赛为此问题设立了启发式赛道，旨在开发实用的高性能算法。

Method: hedom5采用迭代贪心策略：1) 图预处理和简化；2) 基于优先队列的懒惰增益贪心构造；3) 反向插入顺序的激进剪枝；4) 预算限制的1-交换局部改进；5) 最终安全补丁确保完全支配。

Result: 在PACE 2025竞赛中，hedom5算法表现最佳，相比传统的贪心算法和模拟退火方法有显著改进。

Conclusion: 迭代贪心框架结合构造、剪枝和局部改进步骤是解决最小支配集问题的有效方法，hedom5算法在竞赛中验证了这种组合策略的优越性。

Abstract: In this paper we present \texttt{twin\_width\_fmi}'s solver for the heuristic track of PACE's 2025 competition on Minimum Dominating Set.
  As a baseline, we implement \texttt{greedy-ln}, a standard greedy dominating-set heuristic that repeatedly selects the vertex that newly dominates the largest number of currently undominated vertices. We then use this greedy solution as the starting point for a simulated annealing local search: we attempt vertex removals and exchanges and accept worsening moves with decaying probability, in order to escape local minima while preserving domination.
  Our best-performing component, which we ultimately submitted, is \texttt{hedom5}. The design of \texttt{hedom5} is inspired by recent iterative-greedy style domination heuristics~\cite{IterativeGreedy22} that alternate between constructive steps, pruning, and focused repair rather than relying on a single pass. In \texttt{hedom5}, the input graph is first stored in a compact CSR structure and simplified using fast reductions such as forcing neighbors of leaves and handling isolates. We then run a lazy gain-based greedy stage using a priority queue: each candidate vertex is scored by how many currently undominated vertices its closed neighborhood would newly dominate, and scores are only recomputed when necessary. After this constructive phase, we perform an aggressive backward pruning pass that iterates over the chosen dominators in reverse insertion order and deletes any vertex whose closed neighborhood is still fully dominated by the remaining set. Finally, we run a budgeted 1-swap local improvement step that attempts to replace a dominator by an alternative vertex that covers all of its uniquely covered vertices, thereby reducing the size of the dominating set. A brief safety patch at the end guarantees full domination.

</details>


### [10] [Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression](https://arxiv.org/abs/2511.11608)
*Mingyu Sung,Suhwan Im,Daeho Bang,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.DC

TL;DR: SLICER是一个无需重新训练、架构无关的框架，通过压缩中间特征来减少分割计算中的通信和服务器负载，在保持任务质量的同时显著降低上行流量和服务器GPU时间。


<details>
  <summary>Details</summary>
Motivation: 现代DNN依赖边缘-云模型分割，但现有方案使用固定浅层分割点，未能充分利用边缘计算能力，且将延迟和能耗集中在服务器端。在自回归LLM推理中，每个token的前向传递会重复生成庞大的中间特征，加剧了这一问题。

Method: SLICER结合三种技术：(1)非对称top-K过滤稀疏化低幅值激活；(2)幅值分割将剩余非零值分组为等基数块；(3)自适应位量化在失真预算下选择每块位宽。

Result: 在标准视觉和LLM工作负载上，SLICER将上行流量减少高达10倍，服务器GPU时间减少高达4.4倍，同时任务质量保持在基线0-3个百分点内。在多设备设置和AR LLM中，通过将有意义计算转移到边缘并降低每token比特数和服务器时间，稳定了每步流量。

Conclusion: SLICER无需重新训练或架构更改即可附加到现成模型上，为可扩展、低延迟分布式推理提供了即插即用路径。

Abstract: Modern DNNs often rely on edge-cloud model partitioning (MP), but widely used schemes fix shallow, static split points that underutilize edge compute and concentrate latency and energy on the server. The problem is exacerbated in autoregressive (AR) LLM inference, where per-token forward passes repeatedly generate bulky intermediate features (IFs). We introduce SLICER, a retraining-free, architecture-agnostic framework that compresses IFs to reduce both communication and server load in split computing. SLICER combines (i) asymmetric top-K filtering (ATKF) to sparsify low-magnitude activations, (ii) magnitude-splitting (MS) to group the remaining non-zeros into equal-cardinality blocks, and (iii) adaptive bit quantization (ABQ) that selects per-block bitwidths under a distortion budget. Across standard vision and LLM workloads (e.g., ImageNet/COCO; HellaSwag, PIQA, ARC-E/C, GSM8K, HumanEval), SLICER reduces uplink volume by up to 10x and server GPU time by up to 4.4x, while keeping task quality within ~0-3 pp of baseline. In multi-device settings and AR LLMs, SLICER scales by shifting meaningful compute to the edge and lowering bits-per-token and server time per token, stabilizing per-step traffic. The codec attaches to off-the-shelf models without retraining or architectural changes, offering a plug-and-play path to scalable, low-latency distributed inference. Code is provided in the supplementary material.

</details>


### [11] [Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems](https://arxiv.org/abs/2511.11612)
*Aasish Kumar Sharma,Julian Kunkel*

Main category: cs.DC

TL;DR: 评估21个公开LLM在HPC工作负载调度问题上的表现，发现3个模型能精确复现最优解，12个接近最优，6个表现较差。LLM在组合优化中展现出解释性优势，但作为自主求解器仍有局限。


<details>
  <summary>Details</summary>
Motivation: 理解LLM在结构化约束优化问题上的推理能力，特别是从自然语言描述中解决组合优化问题的表现。

Method: 让21个LLM处理相同的HPC工作负载调度问题描述，要求分配任务到节点、计算总完成时间并解释推理过程，以手动推导的9小时20秒最优解为基准。

Result: 3个模型精确复现最优解，12个在2分钟内接近最优，6个存在算术或依赖错误。所有模型都能生成可行映射，但仅约一半严格遵循约束。19个模型生成部分可执行验证代码，18个提供连贯推理步骤。

Conclusion: 领先LLM能从自然语言重建最优调度，但多数在精确计时、数据传输计算和依赖执行方面仍有困难。LLM更适合作为可解释的优化决策辅助工具而非自主求解器。

Abstract: Large language models (LLMs) are increasingly explored for their reasoning capabilities, yet their ability to perform structured, constraint-based optimization from natural language remains insufficiently understood. This study evaluates twenty-one publicly available LLMs on a representative heterogeneous high-performance computing (HPC) workload mapping and scheduling problem. Each model received the same textual description of system nodes, task requirements, and scheduling constraints, and was required to assign tasks to nodes, compute the total makespan, and explain its reasoning. A manually derived analytical optimum of nine hours and twenty seconds served as the ground truth reference. Three models exactly reproduced the analytical optimum while satisfying all constraints, twelve achieved near-optimal results within two minutes of the reference, and six produced suboptimal schedules with arithmetic or dependency errors. All models generated feasible task-to-node mappings, though only about half maintained strict constraint adherence. Nineteen models produced partially executable verification code, and eighteen provided coherent step-by-step reasoning, demonstrating strong interpretability even when logical errors occurred. Overall, the results define the current capability boundary of LLM reasoning in combinatorial optimization: leading models can reconstruct optimal schedules directly from natural language, but most still struggle with precise timing, data transfer arithmetic, and dependency enforcement. These findings highlight the potential of LLMs as explainable co-pilots for optimization and decision-support tasks rather than autonomous solvers.

</details>


### [12] [Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI](https://arxiv.org/abs/2511.11614)
*Arturo Urías Jiménez*

Main category: cs.DC

TL;DR: FPGAs作为可重构硬件平台，通过直接映射AI算法到设备逻辑，为需要低延迟、能效和硬件定制化的AI工作负载提供了比GPU更灵活和高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: GPU在AI加速中存在固定架构的限制，无法满足低延迟、高能效和细粒度硬件控制的需求，而FPGA的可重构特性能够解决这些问题。

Method: 利用FPGA的可重构能力，将AI算法直接映射到设备逻辑中，实现卷积、注意力机制和后处理的并行流水线，支持确定性时序和低功耗运行。

Result: FPGA能够实现模型特定的物理结构适配，作为SoC集成嵌入式处理器，在传感器附近运行推理，减少延迟和带宽需求，提高隐私保护，并释放GPU资源。

Conclusion: FPGA通过部分重配置和从AI框架的编译流程，缩短了从原型到部署的路径，实现了硬件-算法协同设计，是AI加速的重要战略选择。

Abstract: AI acceleration has been dominated by GPUs, but the growing need for lower latency, energy efficiency, and fine-grained hardware control exposes the limits of fixed architectures. In this context, Field-Programmable Gate Arrays (FPGAs) emerge as a reconfigurable platform that allows mapping AI algorithms directly into device logic. Their ability to implement parallel pipelines for convolutions, attention mechanisms, and post-processing with deterministic timing and reduced power consumption makes them a strategic option for workloads that demand predictable performance and deep customization.
  Unlike CPUs and GPUs, whose architecture is immutable, an FPGA can be reconfigured in the field to adapt its physical structure to a specific model, integrate as a SoC with embedded processors, and run inference near the sensor without sending raw data to the cloud. This reduces latency and required bandwidth, improves privacy, and frees GPUs from specialized tasks in data centers. Partial reconfiguration and compilation flows from AI frameworks are shortening the path from prototype to deployment, enabling hardware--algorithm co-design.

</details>


### [13] [AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism](https://arxiv.org/abs/2511.11617)
*Wendong Xu,Chujie Chen,He Xiao,Kuan Li,Jing Xiong,Chen Zhang,Wenyong Zhou,Chaofan Tao,Yang Bai,Bei Yu,Ngai Wong*

Main category: cs.DC

TL;DR: AnchorTP是一个用于LLM推理服务的弹性张量并行框架，通过状态保持和最小化数据迁移实现快速故障恢复，显著减少停机时间。


<details>
  <summary>Details</summary>
Motivation: 多GPU张量并行(TP)使LLM推理服务容易受到单GPU故障影响，需要高可用性和低延迟的快速恢复方案。

Method: 采用弹性张量并行(ETP)支持任意GPU数量的不等宽分区，与MoE兼容；通过解耦的守护进程保存模型参数和KV缓存；使用基于连续最小迁移(CMM)算法的带宽感知规划器和流水线执行调度器。

Result: 在典型故障场景下，AnchorTP将首次成功时间(TFS)减少高达11倍，峰值时间(TTP)减少高达59%。

Conclusion: AnchorTP能够在不改变服务接口的情况下，通过最小化数据移动快速恢复服务，显著提升LLM推理服务的可靠性。

Abstract: Large Language Model (LLM) inference services demand exceptionally high availability and low latency, yet multi-GPU Tensor Parallelism (TP) makes them vulnerable to single-GPU failures. We present AnchorTP, a state-preserving elastic TP framework for fast recovery. It (i) enables Elastic Tensor Parallelism (ETP) with unequal-width partitioning over any number of GPUs and compatibility with Mixture-of-Experts (MoE), and (ii) preserves model parameters and KV caches in GPU memory via a daemon decoupled from the inference process. To minimize downtime, we propose a bandwidth-aware planner based on a Continuous Minimal Migration (CMM) algorithm that minimizes reload bytes under a byte-cost dominance assumption, and an execution scheduler that pipelines P2P transfers with reloads. These components jointly restore service quickly with minimal data movement and without changing service interfaces. In typical failure scenarios, AnchorTP reduces Time to First Success (TFS) by up to 11x and Time to Peak (TTP) by up to 59% versus restart-and-reload.

</details>


### [14] [DIAP: A Decentralized Agent Identity Protocol with Zero-Knowledge Proofs and a Hybrid P2P Stack](https://arxiv.org/abs/2511.11619)
*Yuanjie Liu,Wenpeng Xing,Ye Zhou,Gaowei Chang,Changting Lin,Meng Han*

Main category: cs.DC

TL;DR: 提出了DIAP协议，这是一个完全去中心化、可验证且保护隐私的自主代理通信框架，通过IPFS/IPNS绑定代理身份，使用零知识证明实现无状态所有权验证。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化系统存在中心化中介或缺乏去中心化身份解析机制的问题，限制了持久性和跨网络互操作性。

Method: 结合Noir零知识证明、DID-Key、IPFS和混合P2P堆栈（Libp2p GossipSub和Iroh），采用零依赖ZKP部署模型和预编译Noir电路。

Result: 实现了即时、可验证且保护隐私的身份证明，消除了外部ZKP工具链的需求。

Conclusion: 为下一代自主代理生态系统和代理间经济建立了实用、高性能的基础。

Abstract: The absence of a fully decentralized, verifiable, and privacy-preserving communication protocol for autonomous agents remains a core challenge in decentralized computing. Existing systems often rely on centralized intermediaries, which reintroduce trust bottlenecks, or lack decentralized identity-resolution mechanisms, limiting persistence and cross-network interoperability.
  We propose the Decentralized Interstellar Agent Protocol (DIAP), a novel framework for agent identity and communication that enables persistent, verifiable, and trustless interoperability in fully decentralized environments. DIAP binds an agent's identity to an immutable IPFS or IPNS content identifier and uses zero-knowledge proofs (ZKP) to dynamically and statelessly prove ownership, removing the need for record updates.
  We present a Rust SDK that integrates Noir (for zero-knowledge proofs), DID-Key, IPFS, and a hybrid peer-to-peer stack combining Libp2p GossipSub for discovery and Iroh for high-performance, QUIC based data exchange. DIAP introduces a zero-dependency ZKP deployment model through a universal proof manager and compile-time build script that embeds a precompiled Noir circuit, eliminating the need for external ZKP toolchains. This enables instant, verifiable, and privacy-preserving identity proofs.
  This work establishes a practical, high-performance foundation for next-generation autonomous agent ecosystems and agent-to-agent (A to A) economies.

</details>


### [15] [AIvailable: A Software-Defined Architecture for LLM-as-a-Service on Heterogeneous and Legacy GPUs](https://arxiv.org/abs/2511.11621)
*Pedro Antunes,Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luís Frazão,Nuno Costa,António Pereira*

Main category: cs.DC

TL;DR: AIvailable是一个低成本、高可用的LLM即服务平台，能够在异构GPU节点上运行大型语言模型，专注于充分利用每个节点的VRAM资源。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理框架通常假设同质化、资源丰富的硬件环境，这在学术或资源受限环境中不现实。需要为异构和遗留GPU节点提供可扩展的LLM推理解决方案。

Method: 采用软件定义方法，在NVIDIA和AMD等异构GPU节点上运行LLM，完全GPU加速推理，无CPU回退。架构包括客户端接口、服务前端、SDAI控制器和服务后端四个主要组件。

Result: 实现了动态、VRAM感知的模型分配和重新分配，确保资源高效利用和对故障或工作负载波动的弹性。

Conclusion: AIvailable通过重新利用遗留GPU，支持多样化的开源LLM，有助于在学术实验室、私营公司等资源受限组织中普及生成式AI。

Abstract: The rise of Large Language Models (LLM) has increased the need for scalable, high-performance inference systems, yet most existing frameworks assume homogeneous, resource-rich hardware, often unrealistic in academic, or resource-constrained settings. We introduce AIvailable, a low-cost, highly available LLM-as-a-Service (LLMaaS) platform, that uses a software-defined approach for running LLMs across heterogeneous and legacy GPU nodes, including NVIDIA and AMD devices, with a focus on fully utilizing each node's VRAM. AIvailable operates as a fully GPU-accelerated inference without CPU fallbacks, featuring a unified client interface that allows seamless interaction with all deployed LLMs through a single logical unit. The architecture comprises four main components: the Client Interface for user access, the Service Frontend for secure request routing and load balancing, the SDAI Controller for orchestration, deployment, and monitoring, and the Service Backend of heterogeneous GPU nodes executing workloads. By abstracting GPU-specific details and providing dynamic, VRAM-aware allocation and reallocation of models, AIvailable ensures efficient use of resources and resilience against failures or workload fluctuations. Targeting academic labs, private companies, and other constrained organizations, it supports diverse open LLMs helping democratize generative AI through the repurposing of legacy GPUs.

</details>


### [16] [Exploring Parallelism in FPGA-Based Accelerators for Machine Learning Applications](https://arxiv.org/abs/2511.11640)
*Sed Centeno,Christopher Sprague,Arnab A Purkayastha,Ray Simar,Neeraj Magotra*

Main category: cs.DC

TL;DR: 在MNIST数据集上实现基于OpenMP的推测性反向传播，通过重叠前向和后向传播步骤，在保持准确率接近基线的情况下获得最高24%的训练加速。


<details>
  <summary>Details</summary>
Motivation: 推测性反向传播是一种通过重叠前向和后向传播来加速神经网络训练的有前景技术，当误差梯度在特定阈值内时使用推测性权重更新可以减少训练时间而不显著影响准确率。

Method: 使用OpenMP作为并行编程平台实现推测性反向传播，利用OpenMP的多线程能力同时执行前向和推测性反向传播步骤，计划在FPGA上进行硬件加速演示。

Result: 在CPU实验中，当阈值为0.25时，推测性反向传播在训练时间上获得最高24%的加速，准确率保持在基线3-4%范围内；在单步执行时间比较中，获得最高35%的加速。

Conclusion: 推测性反向传播通过重叠前向和后向传播步骤，在保持模型准确率的同时显著提高了训练速度，证明了该方法在神经网络训练加速方面的有效性。

Abstract: Speculative backpropagation has emerged as a promising technique to accelerate the training of neural networks by overlapping the forward and backward passes. Leveraging speculative weight updates when error gradients fall within a specific threshold reduces training time without substantially compromising accuracy. In this work, we implement speculative backpropagation on the MNIST dataset using OpenMP as the parallel programming platform. OpenMP's multi-threading capabilities enable simultaneous execution of forward and speculative backpropagation steps, significantly improving training speed. The application is planned for synthesis on a state-of-the-art FPGA to demonstrate its potential for hardware acceleration. Our CPU-based experimental results demonstrate that speculative backpropagation achieves a maximum speedup of 24% in execution time when using a threshold of 0.25, and accuracy remaining within 3-4% of the baseline across various epochs. Additionally, when comparing individual step execution time, speculative backpropagation yields a maximum speedup of 35% over the baseline, demonstrating the effectiveness of overlapping forward and backward passes.

</details>


### [17] [Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges](https://arxiv.org/abs/2511.11624)
*Md Romyull Islam,Bobin Deng,Nobel Dhar,Tu N. Nguyen,Selena He,Yong Shi,Kun Suo*

Main category: cs.DC

TL;DR: 评估五种小型语言模型在边缘设备上的能效表现，发现Jetson Orin Nano GPU配置能效最高，Llama 3.2在准确性和能效间平衡最佳，TinyLlama适合低功耗环境，而Phi-3 Mini能耗最高。


<details>
  <summary>Details</summary>
Motivation: 边缘设备部署小型语言模型具有延迟低、不依赖网络的优势，但受限于计算资源和能源预算，需要评估其能效表现。

Method: 在Raspberry Pi 5、Jetson Nano和Jetson Orin Nano（CPU和GPU配置）上评估五种代表性SLM的功率效率。

Result: Jetson Orin Nano GPU加速实现最高能效比，显著优于CPU配置；Llama 3.2准确性和能效平衡最佳；TinyLlama适合低功耗但准确性较低；Phi-3 Mini能耗最高。

Conclusion: GPU加速、内存带宽和模型架构是优化推理能效的关键因素，为AI、智能系统和移动平台在能源受限环境中权衡准确性、推理延迟和能效提供实践指导。

Abstract: Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments.

</details>


### [18] [Mixture-of-Schedulers: An Adaptive Scheduling Agent as a Learned Router for Expert Policies](https://arxiv.org/abs/2511.11628)
*Xinbo Wang,Shian Jia,Ziyang Huang,Jing Cao,Mingli Song*

Main category: cs.DC

TL;DR: ASA是一个动态调度框架，通过机器学习模型识别工作负载模式，从专家调度器组合中实时选择最优策略，显著提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统操作系统调度器采用单一静态策略，无法适应现代系统的异构硬件和多样化工作负载，导致公平性、吞吐量和延迟方面的显著折衷。

Method: 提出自适应调度代理(ASA)：1)离线训练硬件无关的机器学习模型识别抽象工作负载模式；2)运行时使用时间加权概率投票算法识别工作负载，通过预配置映射表切换到最优调度器。

Result: 在基于用户体验指标的新基准测试中，ASA在86.4%的测试场景中优于默认Linux调度器(EEVDF)，在78.6%的场景中调度器选择接近最优。

Conclusion: ASA验证了通过动态选择专家调度器而非设计单一调度器的可行性，为实现更智能、自适应和响应迅速的操作系统调度器提供了实用路径。

Abstract: Modern operating system schedulers employ a single, static policy, which struggles to deliver optimal performance across the diverse and dynamic workloads of contemporary systems. This "one-policy-fits-all" approach leads to significant compromises in fairness, throughput, and latency, particularly with the rise of heterogeneous hardware and varied application architectures.
  This paper proposes a new paradigm: dynamically selecting the optimal policy from a portfolio of specialized schedulers rather than designing a single, monolithic one. We present the Adaptive Scheduling Agent (ASA), a lightweight framework that intelligently matches workloads to the most suitable "expert" scheduling policy at runtime. ASA's core is a novel, low-overhead offline/online approach. First, an offline process trains a universal, hardware-agnostic machine learning model to recognize abstract workload patterns from system behaviors. Second, at runtime, ASA continually processes the model's predictions using a time-weighted probability voting algorithm to identify the workload, then makes a scheduling decision by consulting a pre-configured, machine-specific mapping table to switch to the optimal scheduler via Linux's sched_ext framework. This decoupled architecture allows ASA to adapt to new hardware platforms rapidly without expensive retraining of the core recognition model.
  Our evaluation, based on a novel benchmark focused on user-experience metrics, demonstrates that ASA consistently outperforms the default Linux scheduler (EEVDF), achieving superior results in 86.4% of test scenarios. Furthermore, ASA's selections are near-optimal, ranking among the top three schedulers in 78.6% of all scenarios. This validates our approach as a practical path toward more intelligent, adaptive, and responsive operating system schedulers.

</details>


### [19] [HeteroSTA: A CPU-GPU Heterogeneous Static Timing Analysis Engine with Holistic Industrial Design Support](https://arxiv.org/abs/2511.11660)
*Zizheng Guo,Haichuan Liu,Xizhe Shi,Shenglu Hua,Zuodong Zhang,Chunyuan Zhao,Runsheng Wang,Yibo Lin*

Main category: cs.DC

TL;DR: HeteroSTA是首个CPU-GPU异构时序分析引擎，支持多种延迟计算模型、行业格式和端到端GPU加速，提供零开销的异构API。


<details>
  <summary>Details</summary>
Motivation: 开发一个支持多种延迟计算模型、行业标准格式和GPU加速的时序分析工具，以满足学术界和工业界对高效时序分析的需求。

Method: 采用CPU-GPU异构架构，提供多种延迟计算模型选择，支持.sdc约束格式，实现图基和路径基时序查询的端到端GPU加速，并提供零开销的异构API。

Result: 作为独立工具、时序驱动DREAMPlace 4.0集成和时序驱动全局布线集成等用例都展示了显著的运行速度提升和可比的质量。

Conclusion: HeteroSTA成功实现了首个CPU-GPU异构时序分析引擎，在保持质量的同时显著提升了运行效率，适用于广泛的学术和工业应用。

Abstract: We introduce in this paper, HeteroSTA, the first CPU-GPU heterogeneous timing analysis engine that efficiently supports: (1) a set of delay calculation models providing versatile accuracy-speed choices without relying on an external golden tool, (2) robust support for industry formats, including especially the .sdc constraints containing all common timing exceptions, clock domains, and case analysis modes, and (3) end-to-end GPU-acceleration for both graph-based and path-based timing queries, all exposed as a zero-overhead flattened heterogeneous application programming interface (API). HeteroSTA is publicly available with both a standalone binary executable and an embeddable shared library targeting ubiquitous academic and industry applications. Example use cases as a standalone tool, a timing-driven DREAMPlace 4.0 integration, and a timing-driven global routing integration have all demonstrated remarkable runtime speed-up and comparable quality.

</details>


### [20] [Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression for Split Computing of Deep Neural Networks](https://arxiv.org/abs/2511.11664)
*Mingyu Sung,Suhwan Im,Vikas Palakonda,Jae-Mo Kang*

Main category: cs.DC

TL;DR: 提出了一种轻量级压缩框架，利用rANS编码结合非对称整数量化和稀疏张量表示，显著减少边缘计算中的传输开销，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 解决边缘计算中深度神经网络推理时中间特征传输的通信瓶颈问题，在带宽受限环境中部署复杂AI系统而不牺牲模型性能。

Method: 结合非对称整数量化和稀疏表示技术，使用rANS编码，无需复杂概率建模或网络修改，通过优化张量重塑维度最大化压缩效率，并实现GPU加速的亚毫秒级编码/解码延迟。

Result: 在多种神经网络架构（ResNet、VGG16、MobileNetV2等）和数据集（CIFAR100、ImageNet）上保持接近基线精度，在NLP任务（Llama2 7B/13B）的多个基准测试中也验证了有效性。

Conclusion: 该框架成功解决了带宽受限环境中部署复杂AI系统的根本瓶颈，在保持模型性能的同时显著减少传输开销，具有广泛的适用性。

Abstract: Split computing distributes deep neural network inference between resource-constrained edge devices and cloud servers but faces significant communication bottlenecks when transmitting intermediate features. To this end, in this paper, we propose a novel lightweight compression framework that leverages Range Asymmetric Numeral Systems (rANS) encoding with asymmetric integer quantization and sparse tensor representation to reduce transmission overhead dramatically. Specifically, our approach combines asymmetric integer quantization with a sparse representation technique, eliminating the need for complex probability modeling or network modifications. The key contributions include: (1) a distribution-agnostic compression pipeline that exploits inherent tensor sparsity to achieve bandwidth reduction with minimal computational overhead; (2) an approximate theoretical model that optimizes tensor reshaping dimensions to maximize compression efficiency; and (3) a GPU-accelerated implementation with sub-millisecond encoding/decoding latency. Extensive evaluations across diverse neural architectures (ResNet, VGG16, MobileNetV2, SwinT, DenseNet121, EfficientNetB0) demonstrate that the proposed framework consistently maintains near-baseline accuracy across CIFAR100 and ImageNet benchmarks. Moreover, we validated the framework's effectiveness on advanced natural language processing tasks by employing Llama2 7B and 13B on standard benchmarks such as MMLU, HellaSwag, ARC, PIQA, Winogrande, BoolQ, and OpenBookQA, demonstrating its broad applicability beyond computer vision. Furthermore, this method addresses a fundamental bottleneck in deploying sophisticated artificial intelligence systems in bandwidth-constrained environments without compromising model performance.

</details>


### [21] [OSGym: Super-Scalable Distributed Data Engine for Generalizable Computer Agents](https://arxiv.org/abs/2511.11672)
*Zengyi Qin,Jinyuan Chen,Yunze Man,Shengcao Cao,Ziqi Pang,Zhuoyuan Wang,Xin Sun,Gen Lin,Han Fang,Ling Zhu,Zixin Xie,Zibu Wei,Tianshu Ran,Haoran Geng,Xander Wu,Zachary Bright,Qizhen Sun,Rui Wang,Yuyang Cai,Song Wang,Jiace Zhao,Han Cao,Yeyang Zhou,Tianrui Liu,Ray Pan,Chongye Yang,Xiang Ren,Bo Zhang,Yutong Ban,Jitendra Malik,Brian Anthony,Pieter Abbeel*

Main category: cs.DC

TL;DR: OSGym是一个超可扩展的分布式数据引擎，用于在多样化计算机相关任务中训练智能代理，能够以学术可负担的成本扩展到上千个操作系统副本。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限条件下运行多个操作系统副本的高资源需求问题，为智能代理提供动态运行时环境，推动计算机代理研究的可扩展性和通用性发展。

Method: 采用分布式架构并行化上千个操作系统实例，支持广泛的OS平台任务（工具使用、浏览器交互、软件工程、办公应用等），并提供灵活的训练算法支持。

Result: 在资源受限条件下每分钟生成1420个多轮轨迹，每个OS副本每日成本仅0.2-0.3美元，训练出的模型性能优于最先进基线方法。

Conclusion: OSGym通过其可扩展性、通用性和经济可行性，为未来代理研究提供了强大的基础设施，展示了在推动计算机代理可扩展性和通用性方面的潜力。

Abstract: We introduce OSGym, a super-scalable distributed data engine for training agents across diverse computer-related tasks. OSGym efficiently scales to over a thousand operating system (OS) replicas at an academia-affordable cost, serving as dynamic runtime environments for intelligent agents. It offers three key advantages. (1) Scalability: Despite the intensive resource requirements of running multiple OS replicas, OSGym parallelizes over a thousand instances while maintaining operational efficiency under constrained resources, generating up to 1420 multi-turn trajectories per minute. (2) Generality and Customizability: OSGym supports a broad spectrum of tasks that run on OS platforms, including tool use, browser interactions, software engineering, and office applications, with flexible support for diverse model training algorithms. (3) Economic Viability: OSGym operates at only 0.2-0.3 USD per day per OS replica using accessible on-demand compute providers. It is fully open-source and freely available for both research and commercial use. Experiments show that OSGym enables comprehensive data collection, supervised fine-tuning, and reinforcement learning pipelines for computer agents. Models trained with OSGym outperform state-of-the-art baselines, demonstrating its potential to advance scalability and universality in future agent research.

</details>


### [22] [A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems](https://arxiv.org/abs/2511.11678)
*Yuze Liu,Yunhan Wang,Tiehua Zhang,Zhishu Shen,Cheng Peng,Libing Wu,Feng Xia,Jiong Jin*

Main category: cs.DC

TL;DR: Co-PLMs是一个新颖的协同调优框架，用于大型和小型语言模型的协作训练，通过结构无关的相互学习实现异构语言模型间的知识交换，使用蒸馏代理模型作为桥梁，在保护设备特定领域洞察的同时提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 智能应用激增使得带宽受限的云服务器难以实时处理大量LLM工作负载而不损害用户数据隐私，需要构建云边协同系统，但跨域部署SLM和架构异构性给模型性能提升带来挑战。

Method: 提出Co-PLMs框架，采用结构无关的相互学习方法，使用蒸馏代理模型作为异构服务器LLM和边缘设备SLM之间协作训练的桥梁，同时保留每个设备的领域特定洞察。

Result: 实验结果显示Co-PLMs优于最先进方法，在Rouge-L和EM指标上分别平均提升5.38%和4.88%。

Conclusion: Co-PLMs框架有效解决了云边协同系统中异构语言模型协作训练的挑战，通过知识交换显著提升了推理性能。

Abstract: The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM.

</details>


### [23] [ECCENTRIC: Edge-Cloud Collaboration Framework for Distributed Inference Using Knowledge Adaptation](https://arxiv.org/abs/2511.11719)
*Mohammad Mahdi Kamani,Zhongwei Cheng,Lin Chen*

Main category: cs.DC

TL;DR: 提出了Eccentric框架，通过边缘模型向云端模型的知识适配，在边缘-云推理系统中实现计算、通信和性能之间的最佳权衡。


<details>
  <summary>Details</summary>
Motivation: 边缘AI应用快速增长，但边缘设备计算资源有限，必须依赖云端系统。随着边缘设备数量增加，云端推理系统的计算和通信成本急剧上升，需要在计算、通信和性能之间找到平衡。

Method: 基于边缘模型向云端模型的知识适配，学习具有不同权衡水平的模型，减少推理过程中的计算和通信成本。

Result: 在分类和物体检测任务上的实证研究证实了该框架的有效性。

Conclusion: Eccentric框架是一种适用于边缘-云推理系统的新型压缩方法，能够同时降低计算和通信成本，同时实现最佳性能。

Abstract: The massive growth in the utilization of edge AI has made the applications of machine learning models ubiquitous in different domains. Despite the computation and communication efficiency of these systems, due to limited computation resources on edge devices, relying on more computationally rich systems on the cloud side is inevitable in most cases. Cloud inference systems can achieve the best performance while the computation and communication cost is dramatically increasing by the expansion of a number of edge devices relying on these systems. Hence, there is a trade-off between the computation, communication, and performance of these systems. In this paper, we propose a novel framework, dubbed as Eccentric that learns models with different levels of trade-offs between these conflicting objectives. This framework, based on an adaptation of knowledge from the edge model to the cloud one, reduces the computation and communication costs of the system during inference while achieving the best performance possible. The Eccentric framework can be considered as a new form of compression method suited for edge-cloud inference systems to reduce both computation and communication costs. Empirical studies on classification and object detection tasks corroborate the efficacy of this framework.

</details>


### [24] [A Meta-Heuristic Load Balancer for Cloud Computing Systems](https://arxiv.org/abs/2511.11721)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: 提出一种云系统服务分配策略，避免节点过载并保持系统稳定，同时最小化成本。


<details>
  <summary>Details</summary>
Motivation: 在云系统中有效分配服务，防止节点过载，维持系统稳定性，并降低运营成本。

Method: 建立云资源利用的抽象模型，考虑多种资源类型和服务迁移成本；开发原型元启发式负载均衡器；提出新颖的遗传算法，用其他元启发式算法的输出作为种群种子。

Result: 展示了原型负载均衡器，并呈现和讨论了实验结果。

Conclusion: 所提出的策略能够有效分配云服务，避免节点过载，维持系统稳定性，并以最小成本实现资源优化。

Abstract: This paper presents a strategy to allocate services on a Cloud system without overloading nodes and maintaining the system stability with minimum cost. We specify an abstract model of cloud resources utilization, including multiple types of resources as well as considerations for the service migration costs. A prototype meta-heuristic load balancer is demonstrated and experimental results are presented and discussed. We also propose a novel genetic algorithm, where population is seeded with the outputs of other meta-heuristic algorithms.

</details>


### [25] [Harli: Harvest Underutilized Resources in LLM Serving with Finetuning Tasks](https://arxiv.org/abs/2511.11729)
*Ao Xu,Han Zhao,Weihao Cui,Quan Chen,Yukang Chen,Shulai Zhang,Shuang Chen,Jiemin Jiang,Zhibin Yu,Minyi Guo*

Main category: cs.DC

TL;DR: Harli是一个LLM服务系统，通过将参数高效微调(PEFT)任务与LLM解码实例共同部署来提高GPU利用率，解决了解码实例GPU利用率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统中，解码实例由于内存限制特性和动态工作负载中批处理不足，导致GPU利用率低，计算资源未充分利用。

Method: 使用三个组件：统一内存分配器实现运行时内存复用，两阶段延迟预测器建模解码延迟，以及QoS保证的吞吐量最大化调度器。

Result: 实验结果显示，Harli相比最先进的服务系统，平均将微调吞吐量提高了46.2%（最高达92.0%），同时保持推理解码的严格QoS保证。

Conclusion: Harli通过安全地共同部署PEFT任务和LLM解码实例，有效提高了GPU利用率，实现了更高的微调吞吐量，同时确保推理服务质量。

Abstract: Large language models (LLMs) are increasingly deployed under the Model-as-a-Service (MaaS) paradigm. To meet stringent quality-of-service (QoS) requirements, existing LLM serving systems disaggregate the prefill and decode phases of inference. However, decode instances often experience low GPU utilization due to their memory-bound nature and insufficient batching in dynamic workloads, leaving compute resources underutilized.
  We introduce Harli, a serving system that improves GPU utilization by co-locating parameter-efficient finetuning (PEFT) tasks with LLM decode instances. PEFT tasks are compute-bound and memory-efficient, making them ideal candidates for safe co-location. Specifically, Harli addresses key challenges--limited memory and unpredictable interference--using three components: a unified memory allocator for runtime memory reuse, a two-stage latency predictor for decode latency modeling, and a QoS-guaranteed throughput-maximizing scheduler for throughput maximization. Experimental results show that Harli improves the finetune throughput by 46.2% on average (up to 92.0%) over state-of-the-art serving systems, while maintaining strict QoS guarantees for inference decode.

</details>


### [26] [Speculative Decoding in Decentralized LLM Inference: Turning Communication Latency into Computation Throughput](https://arxiv.org/abs/2511.11733)
*Jingwei Song,Wanyi Chen,Xinyuan Song,Max,Chris Tong,Gufeng Chen,Tianyi Zhao,Eric Yang,Bill Shi,Lynn Ai*

Main category: cs.DC

TL;DR: 提出了去中心化推测解码（DSD）框架，通过并行验证多个候选token将网络延迟转化为有用计算，在保持准确性的同时实现2.56-2.59倍加速。


<details>
  <summary>Details</summary>
Motivation: 推测解码在集中式系统中有效，但在网络延迟主导的去中心化环境中表现未被充分研究。

Method: DSD框架通过分布式节点并行验证多个候选token，并引入基于token语义重要性的自适应推测验证策略。

Result: 理论减少(N-1)t1(k-1)/k的跨节点通信成本，实际在HumanEval和GSM8K上分别实现2.56x和2.59x加速，超越Eagle3基线。

Conclusion: DSD将网络停滞转化为吞吐量，无需模型重训练或架构变更即可实现更快的分布式LLM推理。

Abstract: Speculative decoding accelerates large language model (LLM) inference by using a lightweight draft model to propose tokens that are later verified by a stronger target model. While effective in centralized systems, its behavior in decentralized settings, where network latency often dominates compute, remains under-characterized. We present Decentralized Speculative Decoding (DSD), a plug-and-play framework for decentralized inference that turns communication delay into useful computation by verifying multiple candidate tokens in parallel across distributed nodes. We further introduce an adaptive speculative verification strategy that adjusts acceptance thresholds by token-level semantic importance, delivering an additional 15% to 20% end-to-end speedup without retraining. In theory, DSD reduces cross-node communication cost by approximately (N-1)t1(k-1)/k, where t1 is per-link latency and k is the average number of tokens accepted per round. In practice, DSD achieves up to 2.56x speedup on HumanEval and 2.59x on GSM8K, surpassing the Eagle3 baseline while preserving accuracy. These results show that adapting speculative decoding for decentralized execution provides a system-level optimization that converts network stalls into throughput, enabling faster distributed LLM inference with no model retraining or architectural changes.

</details>


### [27] [Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows](https://arxiv.org/abs/2511.11739)
*Christina Schenk,Miguel Hernández-del-Valle,Luis Calero-Lumbreras,Marcus Noack,Maciej Haranczyk*

Main category: cs.DC

TL;DR: 提出了一种噪声感知决策算法，通过量化设备特定噪声配置文件来管理变异性，使用分布分析和聚类选择单设备或多设备贝叶斯优化策略，提高性能、可重复性和效率。


<details>
  <summary>Details</summary>
Motivation: 实验噪声的设备间变异性严重影响可重复性，特别是在自动化高通量系统中，这种变异性在大型系统（如建筑3D打印）中可能引发结构或经济故障。

Method: 使用分布分析和成对散度度量与聚类，选择单设备或鲁棒多设备贝叶斯优化策略，明确利用设备间差异。

Result: 在三个名义相同的3D打印机上的实验案例显示减少了冗余、降低了资源使用并提高了可靠性。

Conclusion: 该框架为可扩展自动化实验平台建立了精确和资源感知优化的范式。

Abstract: Device-to-device variability in experimental noise critically impacts reproducibility, especially in automated, high-throughput systems like additive manufacturing farms. While manageable in small labs, such variability can escalate into serious risks at larger scales, such as architectural 3D printing, where noise may cause structural or economic failures. This contribution presents a noise-aware decision-making algorithm that quantifies and models device-specific noise profiles to manage variability adaptively. It uses distributional analysis and pairwise divergence metrics with clustering to choose between single-device and robust multi-device Bayesian optimization strategies. Unlike conventional methods that assume homogeneous devices or generic robustness, this framework explicitly leverages inter-device differences to enhance performance, reproducibility, and efficiency. An experimental case study involving three nominally identical 3D printers (same brand, model, and close serial numbers) demonstrates reduced redundancy, lower resource usage, and improved reliability. Overall, this framework establishes a paradigm for precision- and resource-aware optimization in scalable, automated experimental platforms.

</details>


### [28] [How Machine Learning-Data Driven Replication Strategies Enhance Fault Tolerance in Large-Scale Distributed Systems](https://arxiv.org/abs/2511.11749)
*Almond Kiruthu Murimi*

Main category: cs.DC

TL;DR: 该研究探讨了如何通过机器学习驱动的数据复制策略提升大规模分布式系统的容错能力，相比传统静态配置方法，机器学习方法能更好地适应动态工作负载和意外故障。


<details>
  <summary>Details</summary>
Motivation: 传统复制方法依赖静态配置，难以适应动态工作负载和意外故障，导致资源利用率低下和停机时间延长，需要更智能的自适应解决方案。

Method: 集成机器学习技术（特别是预测分析和强化学习），提出自适应复制机制，能够预测系统故障并实时优化数据放置。通过文献综述、定性分析和与传统方法的比较评估进行研究。

Result: 识别出现有复制策略的关键局限性，凸显了机器学习在创建更具弹性、自优化系统方面的变革潜力。

Conclusion: 研究强调了在现实环境中实施ML驱动解决方案的前景和挑战，为云基和企业系统的未来研究和实际部署提供了建议。

Abstract: This research paper investigates how machine learning-driven data replication strategies can enhance fault tolerance in large-scale distributed systems. Traditional replication methods, which rely on static configurations, often struggle to adapt to dynamic workloads and unexpected failures, leading to inefficient resource utilization and prolonged downtime. By integrating machine learning techniques-specifically predictive analytics and reinforcement learning. The study proposes adaptive replication mechanisms capable of forecasting system failures and optimizing data placement in real time. Through an extensive literature review, qualitative analysis, and comparative evaluations with traditional approaches, the paper identifies key limitations in existing replication strategies and highlights the transformative potential of machine learning in creating more resilient, self-optimizing systems. The findings underscore both the promise and the challenges of implementing ML-driven solutions in real-world environments, offering recommendations for future research and practical deployment in cloud-based and enterprise systems.

</details>


### [29] [TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing](https://arxiv.org/abs/2511.11843)
*Yiwei Zhao,Qiushi Lin,Hongbo Kang,Guy E. Blelloch,Laxman Dhulipala,Charles McGuffey,Phillip B. Gibbons*

Main category: cs.DC

TL;DR: 提出了TD-Orch任务数据编排框架，通过分布式推拉技术实现任务与数据的协同定位，解决了分布式应用中数据热点问题，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 分布式应用中任务和数据分布在不同机器上，需要高效地将任务与目标数据协同定位并执行，特别是在数据请求高度倾斜（数据热点）的情况下。

Method: 采用分布式推拉技术，支持任务和数据的双向流动，实现可扩展的负载均衡，最小化通信开销。

Result: TD-Orch比现有分布式调度基线提速最高2.7倍；基于TD-Orch的TDO-GP图处理系统比最佳开源分布式图系统平均提速4.1倍。

Conclusion: TD-Orch框架能有效解决分布式应用中的任务数据编排问题，特别是在数据热点场景下表现优异，为构建高效分布式系统提供了有力支撑。

Abstract: In this paper, we highlight a task-data orchestration abstraction that supports a range of distributed applications, including graph processing and key-value stores. Given a batch of tasks each requesting one or more data items, where both tasks and data are distributed across multiple machines, each task must get co-located with its target data (by moving tasks and/or data) and executed. We present TD-Orch, an efficient and scalable orchestration framework featuring a simple application developer interface. TD-Orch employs a distributed push-pull technique, leveraging the bidirectional f low of both tasks and data to achieve scalable load balance across machines even under highly skewed data request (data hot spots), with minimal communication overhead. Experimental results show that TD-Orch achieves up to 2.7x speedup over existing distributed scheduling baselines. Building on TD-Orch, we present TDO-GP, a distributed graph processing system for general graph problems, demonstrating the effectiveness of the underlying framework. We design three families of implementation techniques to fully leverage the execution flow provided by TD-Orch. Experimental results show that TDO-GP achieves an average speedup of 4.1x over the best prior open-source distributed graph systems for general graph processing.

</details>


### [30] [Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs](https://arxiv.org/abs/2511.11885)
*Kausar Patherya,Ashutosh Dhekne,Francisco Romero*

Main category: cs.DC

TL;DR: Flash-Fusion是一个端到端的边缘云系统，通过边缘统计摘要和云端查询规划来解决IoT数据分析中的两大挑战：数据量过大和分析过程缓慢。


<details>
  <summary>Details</summary>
Motivation: 智能城市和IoT部署产生了大量传感器数据，但直接使用LLM分析面临数据量过大、成本高昂和延迟问题，需要一种能自动解析查询、选择相关数据并优化表示的系统。

Method: 采用边缘统计摘要实现73.5%的数据压缩，云端查询规划通过聚类行为数据并组装上下文丰富的提示来优化LLM调用。

Result: 在大学巴士车队部署中，相比直接使用原始数据的基线方法，Flash-Fusion实现了95%的延迟降低和98%的token使用量和成本减少，同时保持高质量响应。

Conclusion: Flash-Fusion使安全官员、城市规划师、车队经理和数据科学家等不同角色能够高效地迭代分析IoT数据，无需手动编写查询或预处理。

Abstract: Smart cities and pervasive IoT deployments have generated interest in IoT data analysis across transportation and urban planning. At the same time, Large Language Models offer a new interface for exploring IoT data - particularly through natural language. Users today face two key challenges when working with IoT data using LLMs: (1) data collection infrastructure is expensive, producing terabytes of low-level sensor readings that are too granular for direct use, and (2) data analysis is slow, requiring iterative effort and technical expertise. Directly feeding all IoT telemetry to LLMs is impractical due to finite context windows, prohibitive token costs at scale, and non-interactive latencies. What is missing is a system that first parses a user's query to identify the analytical task, then selects the relevant data slices, and finally chooses the right representation before invoking an LLM.
  We present Flash-Fusion, an end-to-end edge-cloud system that reduces the IoT data collection and analysis burden on users. Two principles guide its design: (1) edge-based statistical summarization (achieving 73.5% data reduction) to address data volume, and (2) cloud-based query planning that clusters behavioral data and assembles context-rich prompts to address data interpretation. We deploy Flash-Fusion on a university bus fleet and evaluate it against a baseline that feeds raw data to a state-of-the-art LLM. Flash-Fusion achieves a 95% latency reduction and 98% decrease in token usage and cost while maintaining high-quality responses. It enables personas across disciplines - safety officers, urban planners, fleet managers, and data scientists - to efficiently iterate over IoT data without the burden of manual query authoring or preprocessing.

</details>


### [31] [KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference](https://arxiv.org/abs/2511.11907)
*Huawei Zhang,Chunwei Xia,Zheng Wang*

Main category: cs.DC

TL;DR: KVSwap是一个软件框架，通过将KV缓存卸载到非易失性二级存储（磁盘）来突破长上下文推理的内存容量限制，在严格内存预算下提供更高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 移动和嵌入式AI应用需要处理多个长上下文输入，但本地运行语言模型时，KV缓存随上下文长度和批大小线性增长，导致内存容量瓶颈。

Method: 利用只有少量动态变化的KV条目对生成至关重要的观察，将完整缓存存储在磁盘上，使用紧凑的内存元数据预测要预加载的条目，重叠计算与硬件感知的磁盘访问，并协调读取模式以匹配存储设备特性。

Result: 在代表性语言模型和存储类型上的评估显示，与现有KV缓存卸载方案相比，KVSwap在严格内存预算下提供更高吞吐量，同时保持生成质量。

Conclusion: KVSwap通过智能的KV缓存磁盘卸载策略，有效解决了长上下文推理的内存容量限制问题，为移动和嵌入式AI应用提供了可行的解决方案。

Abstract: Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size.
  We present KVSwap, a software framework to break this memory wall by offloading the KV cache to non-volatile secondary storage (disk). KVSwap leverages the observation that only a small, dynamically changing subset of KV entries is critical for generation. It stores the full cache on disk, uses a compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining the generation quality when compared with existing KV cache offloading schemes.

</details>


### [32] [High-Performance N-Queens Solver on GPU: Iterative DFS with Zero Bank Conflicts](https://arxiv.org/abs/2511.12009)
*Guangchao Yao,Yali Li*

Main category: cs.DC

TL;DR: 提出了一种创新的GPU并行计算方法，在8块RTX 5090 GPU上仅用28.4天验证了27皇后问题，相比现有方法实现了10-26倍的加速。


<details>
  <summary>Details</summary>
Motivation: N皇后问题计数具有极高的计算复杂度，目前学术界仅严格验证到N<=26。验证27皇后问题需要约17个月，时间和计算资源成本过高。

Method: 在NVIDIA GPU平台上提出并行计算方法：迭代深度优先搜索算法、将栈结构完全映射到GPU共享内存、精心设计内存访问模式避免bank冲突、采用多种优化技术实现最佳性能。

Result: 成功验证27皇后问题仅需28.4天，确认了PreuBer团队结果的正确性；将28皇后问题的预计求解时间缩短至约11个月。

Conclusion: 该方法相比现有GPU方法在相同硬件配置下实现10倍加速，使用RTX 5090 GPU时实现26倍加速，为这个长期停滞的问题带来了新的解决思路。

Abstract: The counting of solutions to the N-Queens problem is a classic NP-complete problem with extremely high computational complexity. As of now, the academic community has rigorously verified the number of solutions only up to N <= 26. In 2016, the research team led by PreuBer solved the 27-Queens problem using FPGA hardware, which took approximately one year, though the result remains unverified independently. Recent studies on GPU parallel computing suggest that verifying the 27-Queens solution would still require about 17 months, indicating excessively high time and computational resource costs. To address this challenge, we propose an innovative parallel computing method on NVIDIA GPU platform, with the following core contributions: (1) An iterative depth-first search (DFS) algorithm for solving the N-Queens problem; (2) Complete mapping of the required stack structure to GPU shared memory; (3) Effective avoidance of bank conflicts through meticulously designed memory access patterns; (4) Various optimization techniques are employed to achieve optimal performance. Under the proposed optimization framework, we successfully verified the 27-Queens problem in just 28.4 days using eight RTX 5090 GPUs, thereby confirming the correctness of PreuBer's computational results. Moreover, we have reduced the projected solving time for the next open case-the 28-Queens problem-to approximately 11 months, making its resolution computationally feasible. Compared to the state-of-the-art GPU methods, our method achieves over 10x speedup on identical hardware configurations (8 A100), while delivering over 26x acceleration when utilizing 8 RTX 5090 GPUs, and brings fresh perspectives to this long-stagnant problem.

</details>


### [33] [A Quick and Exact Method for Distributed Quantile Computation](https://arxiv.org/abs/2511.12025)
*Ivan Cao,Jaromir J. Saloni,David A. G. Harrison*

Main category: cs.DC

TL;DR: GK Select是Spark中一种精确计算分位数的算法，避免了全数据洗牌，在常数次操作内完成，比Spark全排序快约10.5倍。


<details>
  <summary>Details</summary>
Motivation: Spark中分位数计算通常依赖近似的GK Sketch方法，当需要精确分位数时，默认选项是昂贵的全局排序，这在大规模数据分析中效率低下。

Method: 利用GK Sketch识别接近目标的枢轴，在每个分区中线性时间内提取误差范围内的所有值，然后对候选集进行树形归约。

Result: 分析表明GK Select在执行器端时间复杂度和GK Sketch相当，同时返回精确分位数。在30核AWS EMR集群上，处理10^9个值、120个分区时，达到草图级延迟，比Spark全排序快约10.5倍。

Conclusion: GK Select提供了一种高效计算精确分位数的方法，结合了GK Sketch的速度优势和精确算法的准确性，避免了昂贵的全局排序。

Abstract: Quantile computation is a core primitive in large-scale data analytics. In Spark, practitioners typically rely on the Greenwald-Khanna (GK) Sketch, an approximate method. When exact quantiles are required, the default option is an expensive global sort. We present GK Select, an exact Spark algorithm that avoids full-data shuffles and completes in a constant number of actions. GK Select leverages GK Sketch to identify a near-target pivot, extracts all values within the error bound around this pivot in each partition in linear time, and then tree-reduces the resulting candidate sets. We show analytically that GK Select matches the executor-side time complexity of GK Sketch while returning the exact quantile. Empirically, GK Select achieves sketch-level latency and outperforms Spark's full sort by approximately 10.5x on 10^9 values across 120 partitions on a 30-core AWS EMR cluster.

</details>


### [34] [Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding](https://arxiv.org/abs/2511.12031)
*Arun Ramachandran,Ramaswamy Govindarajan,Murali Annavaram,Prakash Raghavendra,Hossein Entezari Zarch,Lei Gao,Chaoyi Jiang*

Main category: cs.DC

TL;DR: 提出BMC方法优化LLM推理中的KV缓存管理，通过定期分配冗余行实现原地更新，避免复制开销，并能用于推测解码，在CPU和GPU上均实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 随着GPU成本飙升，需要在CPU上高效运行LLM推理。传统KV缓存更新方法存在分配和复制开销，而预分配大矩阵又导致冗余计算。

Method: 提出BMC方法：每r次迭代分配带r个冗余行的KV张量，支持原地更新；将冗余行和计算重新用于推测解码；通过分析模型选择最佳设计点。

Result: BMC在HuggingFace基准上实现最高3.2倍吞吐量加速；与推测解码结合时额外提供1.39倍加速；优于vLLM和DeepSpeed，分别达到1.36倍和2.29倍加速。

Conclusion: BMC在CPU和GPU上均能有效优化LLM推理性能，通过平衡内存和计算开销实现显著加速。

Abstract: With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs.

</details>


### [35] [Combining Serverless and High-Performance Computing Paradigms to support ML Data-Intensive Applications](https://arxiv.org/abs/2511.12185)
*Mills Staylor,Arup Kumar Sarker,Gregor von Laszewski,Geoffrey Fox,Yue Cheng,Judy Fox*

Main category: cs.DC

TL;DR: Cylon是一个高性能分布式数据框架，通过设计无服务器通信器解决AWS Lambda等无服务器函数在处理大数据时的通信性能问题，性能比传统服务器(EC2)和HPC低1%以下。


<details>
  <summary>Details</summary>
Motivation: 传统数据工程和AI工作负载需要大量硬件投资和维护，云无服务器函数提供水平扩展和精确计费，但在处理大数据时依赖外部存储导致性能显著下降。

Method: 借鉴FMI库设计无服务器通信器，通过NAT穿透TCP打洞技术实现直接通信，解决无服务器函数的通信和性能问题。

Result: 实验显示AWS Lambda的性能在强扩展实验中比服务器型AWS(EC2)和HPC低1%以下。

Conclusion: Cylon框架通过创新的通信设计显著提升了无服务器函数的数据处理性能，证明了在无服务器环境中实现高性能分布式数据处理的可行性。

Abstract: Data is found everywhere, from health and human infrastructure to the surge of sensors and the proliferation of internet-connected devices. To meet this challenge, the data engineering field has expanded significantly in recent years in both research and industry. Traditionally, data engineering, Machine Learning, and AI workloads have been run on large clusters within data center environments, requiring substantial investment in hardware and maintenance. With the rise of the public cloud, it is now possible to run large applications across nodes without owning or maintaining hardware. Serverless functions such as AWS Lambda provide horizontal scaling and precise billing without the hassle of managing traditional cloud infrastructure. However, when processing large datasets, users often rely on external storage options that are significantly slower than direct communication typical of HPC clusters. We introduce Cylon, a high-performance distributed data frame solution that has shown promising results for data processing using Python. We describe how we took inspiration from the FMI library and designed a serverless communicator to tackle communication and performance issues associated with serverless functions. With our design, we demonstrate that the performance of AWS Lambda falls below one percent of strong scaling experiments compared to serverful AWS (EC2) and HPCs based on implementing direct communication via NAT Traversal TCP Hole Punching.

</details>


### [36] [Distributed Seasonal Temporal Pattern Mining](https://arxiv.org/abs/2511.12216)
*Van Ho-Long,Nguyen Ho,Anh-Vu Dinh-Duc,Ha Manh Tran,Ky Trung Nguyen,Tran Dung Pham,Quoc Viet Hung Nguyen*

Main category: cs.DC

TL;DR: 提出了第一个分布式季节性时间模式挖掘框架DSTPM，通过分布式层次查找哈希结构高效处理大规模时间序列数据，显著优于顺序基线方法。


<details>
  <summary>Details</summary>
Motivation: 物联网传感器产生的大量时间序列数据中蕴含着有价值的周期性模式，但传统挖掘方法无法有效捕捉季节性特征，且顺序处理方式难以扩展到大规模数据集。

Method: 采用分布式框架，利用分布式层次查找哈希结构进行高效计算，解决了传统方法缺乏反单调性导致的搜索空间爆炸问题。

Result: 广泛的实验评估表明，DSTPM在运行时间和内存使用方面显著优于顺序基线方法，并能有效扩展到非常大的数据集。

Conclusion: DSTPM是第一个分布式季节性时间模式挖掘框架，成功解决了传统方法在大规模时间序列数据挖掘中的可扩展性问题。

Abstract: The explosive growth of IoT-enabled sensors is producing enormous amounts of time series data across many domains, offering valuable opportunities to extract insights through temporal pattern mining. Among these patterns, an important class exhibits periodic occurrences, referred to as \textit{seasonal temporal patterns} (STPs). However, mining STPs poses challenges, as traditional measures such as support and confidence cannot capture seasonality, and the lack of the anti-monotonicity property results in an exponentially large search space. Existing STP mining methods operate sequentially and therefore do not scale to large datasets. In this paper, we propose the Distributed Seasonal Temporal Pattern Mining (DSTPM), the first distributed framework for mining seasonal temporal patterns from time series. DSTPM leverages efficient data structures, specifically distributed hierarchical lookup hash structures, to enable efficient computation. Extensive experimental evaluations demonstrate that DSTPM significantly outperforms sequential baselines in runtime and memory usage, while scaling effectively to very large datasets.

</details>


### [37] [Design of A Low-Latency and Parallelizable SVD Dataflow Architecture on FPGA](https://arxiv.org/abs/2511.12461)
*Fangqiang Du,Sixuan Chong,Zixuan Huang,Rui Qin,Fengnan Mi,Caibao Hu,Jiangang Chen*

Main category: cs.DC

TL;DR: 提出DSB Jacobi算法，显著减少片上BRAM使用并提高计算速度，为大规模数据流的实时SVD计算提供实用解决方案


<details>
  <summary>Details</summary>
Motivation: 随着矩阵维度快速增长，SVD计算成本显著增加，现有硬件架构存在可扩展性有限、片上内存资源消耗高的问题，不适合嵌入式系统中大规模数据流矩阵的实时处理

Method: 提出基于数据流的SVD处理算法(DSB Jacobi)，优化片上内存使用和计算效率

Result: 与先前工作相比，片上RAM消耗减少41.5%，计算效率提高23倍

Conclusion: DSB Jacobi算法为大规模数据流的实时SVD计算提供了高效实用的解决方案

Abstract: Singular value decomposition (SVD) is widely used for dimensionality reduction and noise suppression, and it plays a pivotal role in numerous scientific and engineering applications. As the dimensions of the matrix grow rapidly, the computational cost increases significantly, posing a serious challenge to the efficiency of data analysis and signal processing systems,especially in time-sensitive scenarios with large-scale datasets. Although various dedicated hardware architectures have been proposed to accelerate the computation of intensive SVD, many of these designs suffer from limited scalability and high consumption of on-chip memory resources. Moreover, they typically overlook the computational and data transfer challenges associated with SVD, enabling them unsuitable for real-time processing of large-scale data stream matrices in embedded systems. In this express, we propose a Data Stream-Based SVD processing algorithm (DSB Jacobi), which significantly reduces on-chip BRAM usage while improving computational speed, offering a practical solution for real-time SVD computation of large-scale data streams. Compared with previous works, our experimental results indicate that the proposed method reduces on-chip RAM consumption by 41.5 percent and improves computational efficiency by 23 times.

</details>


### [38] [A Decentralized Root Cause Localization Approach for Edge Computing Environments](https://arxiv.org/abs/2511.12486)
*Duneesha Fernando,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 提出了一种去中心化的根因定位方法，在边缘设备层面使用个性化PageRank算法进行本地化异常诊断，通过服务集群化和轻量级跨集群协调，在保持高准确率的同时显著降低定位时间。


<details>
  <summary>Details</summary>
Motivation: 现有的根因定位方法是为云环境设计的，依赖集中式分析，在边缘环境中会增加延迟和通信开销。边缘计算环境中的微服务IoT应用容易发生性能异常，且异常会在依赖服务间传播，需要高效的本地化诊断方案。

Method: 1. 将微服务分组为通信和共置感知的集群，限制异常传播范围；2. 在每个集群内使用PPR算法本地执行根因定位；3. 对于跨集群传播的异常，引入集群间点对点近似处理；4. 提出针对异构边缘环境的异常评分机制。

Result: 在公开边缘数据集MicroCERCL上的评估显示，该去中心化方法达到与集中式方法相当或更高的定位准确率，同时将定位时间减少高达34%。

Conclusion: 去中心化的基于图的根因定位方法为资源受限的边缘环境提供了实用高效的异常诊断解决方案。

Abstract: Edge computing environments host increasingly complex microservice-based IoT applications, which are prone to performance anomalies that can propagate across dependent services. Identifying the true source of such anomalies, known as Root Cause Localization (RCL), is essential for timely mitigation. However, existing RCL approaches are designed for cloud environments and rely on centralized analysis, which increases latency and communication overhead when applied at the edge. This paper proposes a decentralized RCL approach that executes localization directly at the edge device level using the Personalized PageRank (PPR) algorithm. The proposed method first groups microservices into communication- and colocation-aware clusters, thereby confining most anomaly propagation within cluster boundaries. Within each cluster, PPR is executed locally to identify the root cause, significantly reducing localization time. For the rare cases where anomalies propagate across clusters, we introduce an inter-cluster peer-to-peer approximation process, enabling lightweight coordination among clusters with minimal communication overhead. To enhance the accuracy of localization in heterogeneous edge environments, we also propose a novel anomaly scoring mechanism tailored to the diverse anomaly triggers that arise across microservice, device, and network layers. Evaluation results on the publicly available edge dataset, MicroCERCL, demonstrate that the proposed decentralized approach achieves comparable or higher localization accuracy than its centralized counterpart while reducing localization time by up to 34%. These findings highlight that decentralized graph-based RCL can provide a practical and efficient solution for anomaly diagnosis in resource-constrained edge environments.

</details>


### [39] [Iris: First-Class Multi-GPU Programming Experience in Triton](https://arxiv.org/abs/2511.12500)
*Muhammad Awad,Muhammad Osama,Brandon Potter*

Main category: cs.DC

TL;DR: Iris是一个完全用Python和Triton实现的多GPU通信库，通过tile-based对称内存抽象消除性能与可编程性之间的权衡，支持计算-通信重叠模式，在保持高性能的同时大幅简化多GPU编程。


<details>
  <summary>Details</summary>
Motivation: 传统多GPU编程需要在性能和可编程性之间进行复杂权衡：高性能实现依赖低层HIP/CUDA通信库需要大量工程努力，而简单抽象往往牺牲性能。

Method: Iris提供tile-based对称内存抽象，与Triton编程模型自然对齐，支持开发者编写单源内核无缝交织计算和通信，实现从bulk-synchronous到细粒度workgroup专业化的重叠模式。

Result: 评估显示Iris在微基准测试中实现接近最优的带宽利用率，在GEMM+All-Scatter工作负载上比PyTorch和RCCL快达1.79倍。

Conclusion: 高层次实现可以匹配或超过经过大量优化的库，同时显著简化多GPU编程。

Abstract: Multi-GPU programming traditionally requires developers to navigate complex trade-offs between performance and programmability. High-performance implementations typically rely on low-level HIP/CUDA communication libraries that demand substantial engineering effort for even basic overlap patterns, while simpler abstractions often sacrifice performance. We present Iris, a multi-GPU communication library implemented entirely in Python and Triton that eliminates this trade-off. Iris provides tile-based symmetric memory abstractions that naturally align with Triton's programming model, enabling developers to write single-source kernels that seamlessly interleave computation and communication. We demonstrate a taxonomy of compute-communication overlap patterns--from bulk-synchronous to fine-grained workgroup specialization--that can be implemented with minimal code changes in Iris, often requiring just a few additional lines within the same Triton kernel. Our evaluation shows that Iris achieves near-optimal bandwidth utilization in microbenchmarks and delivers up to 1.79x speedup over PyTorch and RCCL for GEMM+All-Scatter workloads, demonstrating that high-level implementations can match or exceed heavily-optimized libraries while dramatically simplifying multi-GPU programming.

</details>


### [40] [Artifact for A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines](https://arxiv.org/abs/2511.12667)
*Sepideh Masoudi,Mark Edward Michael Daly,Jannis Kiesel*

Main category: cs.DC

TL;DR: 开发了一个基于Kubernetes的工具，支持非侵入式、延迟应用设计模式，无需修改服务代码，同时收集能耗指标以支持能耗感知决策。


<details>
  <summary>Details</summary>
Motivation: 随着数据网格架构的发展，组织越来越多地构建消费者特定的数据共享管道，但传统云设计模式会降低服务在不同管道中的可重用性。

Method: 基于Kubernetes的工具，自动化模式注入并收集能耗指标，支持非侵入式应用设计模式。

Result: 该工具能够在保持转换服务在各种管道结构中可重用性的同时，支持能耗感知决策。

Conclusion: 提出的工具解决了传统云设计模式降低服务可重用性的问题，同时支持能耗优化决策。

Abstract: As data mesh architectures grow, organizations increasingly build consumer-specific data-sharing pipelines from modular, cloud-based transformation services. While reusable transformation services can improve cost and energy efficiency, applying traditional cloud design patterns can reduce reusability of services in different pipelines. We present a Kubernetes-based tool that enables non-intrusive, deferred application of design patterns without modifying services code. The tool automates pattern injection and collects energy metrics, supporting energy-aware decisions while preserving reusability of transformation services in various pipeline structures.

</details>


### [41] [The Time to Consensus in a Blockchain: Insights into Bitcoin's "6 Blocks Rule''](https://arxiv.org/abs/2511.12687)
*Partha S. Dey,Aditya S. Gopalan,Vijay G. Subramanian*

Main category: cs.DC

TL;DR: 分析Nakamoto区块链中达成共识的时间，研究诚实和恶意两种竞争增长过程，使用排队论技术计算诚实过程永久超过恶意过程的时间。


<details>
  <summary>Details</summary>
Motivation: 理解Nakamoto区块链中达成共识的时间特性，特别是在存在随机延迟和恶意行为者的情况下，评估系统的安全性和稳定性。

Method: 使用排队论技术分析诚实和恶意两种竞争增长过程，考虑诚实过程的随机延迟特性，在简化的比特币模型中计算共识时间的拉普拉斯变换。

Result: 计算出了共识时间的拉普拉斯变换，并通过模拟验证了结果的正确性。

Conclusion: 成功量化了Nakamoto区块链中达成共识的时间特性，为评估区块链安全性提供了理论依据。

Abstract: We investigate the time to consensus in Nakamoto blockchains. Specifically, we consider two competing growth processes, labeled \emph{honest} and \emph{adversarial}, and determine the time after which the honest process permananetly exceeds the adversarial process. This is done via queueing techniques. The predominant difficulty is that the honest growth process is subject to \emph{random delays}. In a stylized Bitcoin model, we compute the Laplace transform for the time to consensus and verify it via simulation.

</details>


### [42] [Learning Process Energy Profiles from Node-Level Power Data](https://arxiv.org/abs/2511.13155)
*Jonathan Bader,Julius Irion,Jannis Kappel,Joel Witzke,Niklas Fomin,Diellza Sherifi,Odej Kao*

Main category: cs.DC

TL;DR: 提出了一种基于eBPF和perf收集的细粒度进程级资源指标，结合节点级能耗测量，通过回归模型预测进程级能耗的方法。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗因高性能计算、云计算和AI需求激增而急剧上升，需要进程级能耗洞察来提升能效。现有方法如Intel RAPL受限于特定硬件且只能提供粗粒度的域级测量。

Method: 利用eBPF和perf收集细粒度进程级资源指标，与来自PDU的节点级能耗测量同步，通过回归模型统计学习进程级资源使用与节点级能耗的关系。

Result: 实现了更细粒度的进程级能耗预测。

Conclusion: 该方法能够提供比现有机制更精细的进程级能耗分析，有助于提升数据中心能效。

Abstract: The growing demand for data center capacity, driven by the growth of high-performance computing, cloud computing, and especially artificial intelligence, has led to a sharp increase in data center energy consumption. To improve energy efficiency, gaining process-level insights into energy consumption is essential. While node-level energy consumption data can be directly measured with hardware such as power meters, existing mechanisms for estimating per-process energy usage, such as Intel RAPL, are limited to specific hardware and provide only coarse-grained, domain-level measurements. Our proposed approach models per-process energy profiles by leveraging fine-grained process-level resource metrics collected via eBPF and perf, which are synchronized with node-level energy measurements obtained from an attached power distribution unit. By statistically learning the relationship between process-level resource usage and node-level energy consumption through a regression-based model, our approach enables more fine-grained per-process energy predictions.

</details>


### [43] [Pico-Cloud: Cloud Infrastructure for Tiny Edge Devices](https://arxiv.org/abs/2511.13253)
*Mordechai Guri*

Main category: cs.DC

TL;DR: Pico-Cloud是基于Raspberry Pi Zero等超小型硬件平台的微边缘云架构，提供容器虚拟化、服务发现和轻量级编排，实现低延迟、低功耗的本地操作。


<details>
  <summary>Details</summary>
Motivation: 解决对集中式数据中心的依赖问题，为网络边缘的轻量级分布式工作负载提供成本效益高、去中心化和可持续的平台。

Method: 在超小型硬件平台上构建微边缘云架构，实现容器化虚拟化、服务发现和轻量级编排，支持本地操作。

Result: Pico-Cloud被证明是适用于农村连接、教育集群和边缘AI推理等用例的成本效益高、去中心化和可持续平台。

Conclusion: Pico-Cloud为网络边缘的轻量级分布式工作负载提供了有效的解决方案，解决了计算、网络、存储和电源管理方面的设计挑战。

Abstract: This paper introduces the Pico-Cloud, a micro-edge cloud architecture built on ultra-minimal hardware platforms such as the Raspberry Pi Zero and comparable single-board computers. The Pico-Cloud delivers container-based virtualization, service discovery, and lightweight orchestration directly at the device layer, enabling local operation with low latency and low power consumption without reliance on centralized data centers. We present its architectural model, outline representative use cases including rural connectivity, educational clusters, and edge AI inference, and analyze design challenges in computation, networking, storage, and power management. The results highlight Pico-Clouds as a cost-effective, decentralized, and sustainable platform for lightweight distributed workloads at the network edge.

</details>


### [44] [Distributed Hierarchical Machine Learning for Joint Resource Allocation and Slice Selection in In-Network Edge Systems](https://arxiv.org/abs/2511.13313)
*Sulaiman Muhammad Rashid,Ibrahim Aliyu,Jaehyung Park,Jinsul Kim*

Main category: cs.DC

TL;DR: 提出了一种基于DeepSets的分布式层次模型(DeepSets-S)，用于解决元界边缘计算中的无线和计算资源联合管理问题，通过切片网络架构结合COIN和MEC技术，显著降低了执行时间并保持接近最优的系统成本。


<details>
  <summary>Details</summary>
Motivation: 元界需要低延迟和实时体验，但传统优化方法在动态边缘条件和高用户负载下效果不佳，需要新的资源管理方案来满足严格的延迟和资源需求。

Method: 将联合资源管理问题分解为三个子问题(SP1-SP3)，设计了基于DeepSets的分布式层次模型，包含共享编码器和任务特定解码器，采用松弛感知归一化机制确保对可变大小设备集的置换等变性。

Result: DeepSets-S在SP1/SP2上达到95.26%和95.67%的容错精度，SP3的多类卸载精度为0.7486，相比精确求解器减少86.1%执行时间，系统成本仅比最优解高6.1%。

Conclusion: 所提出的DeepSets-S模型能够高效处理元界边缘计算中的资源分配问题，在保持接近最优性能的同时显著降低计算复杂度，为动态边缘环境提供了可行的解决方案。

Abstract: The Metaverse promises immersive, real-time experiences; however, meeting its stringent latency and resource demands remains a major challenge. Conventional optimization techniques struggle to respond effectively under dynamic edge conditions and high user loads. In this study, we explore a slice-enabled in-network edge architecture that combines computing-in-the-network (COIN) with multi-access edge computing (MEC). In addition, we formulate the joint problem of wireless and computing resource management with optimal slice selection as a mixed-integer nonlinear program (MINLP). Because solving this model online is computationally intensive, we decompose it into three sub-problems (SP1) intra-slice allocation, (SP2) inter-slice allocation, and (SP3) offloading decision and train a distributed hierarchical DeepSets-based model (DeepSets-S) on optimal solutions obtained offline. In the proposed model, we design a slack-aware normalization mechanism for a shared encoder and task-specific decoders, ensuring permutation equivariance over variable-size wireless device (WD) sets. The learned system produces near-optimal allocations with low inference time and maintains permutation equivariance over variable-size device sets. Our experimental results show that DeepSets-S attains high tolerance-based accuracies on SP1/SP2 (Acc1 = 95.26% and 95.67%) and improves multiclass offloading accuracy on SP3 (Acc = 0.7486; binary local/offload Acc = 0.8824). Compared to exact solvers, the proposed approach reduces the execution time by 86.1%, while closely tracking the optimal system cost (within 6.1% in representative regimes). Compared with baseline models, DeepSets-S consistently achieves higher cost ratios and better utilization across COIN/MEC resources.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [45] [Uncertainty-Guided Live Measurement Sequencing for Fast SAR ADC Linearity Testing](https://arxiv.org/abs/2511.11895)
*Thorben Schey,Khaled Karoonlatifi,Michael Weyrich,Andrey Morozov*

Main category: cs.AR

TL;DR: 提出了一种基于扩展卡尔曼滤波的闭环测试方法，用于高效测试高分辨率SAR ADC的线性度，显著减少测试时间和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有测试方法依赖密集数据采集和离线后处理，导致测试时间长、复杂度高，需要更高效的在线测试方案。

Method: 使用扩展卡尔曼滤波器实时迭代优化行为模型，动态选择测量点以最大化信息增益，直接估计电容失配参数。

Result: 实验结果显示总测试时间和计算开销大幅减少，适合生产环境集成。

Conclusion: 该方法通过自适应闭环测试策略，有效解决了传统ADC线性度测试的低效问题，具有实际应用价值。

Abstract: This paper introduces a novel closed-loop testing methodology for efficient linearity testing of high-resolution Successive Approximation Register (SAR) Analog-to-Digital Converters (ADCs). Existing test strategies, including histogram-based approaches, sine wave testing, and model-driven reconstruction, often rely on dense data acquisition followed by offline post-processing, which increases overall test time and complexity. To overcome these limitations, we propose an adaptive approach that utilizes an iterative behavioral model refined by an Extended Kalman Filter (EKF) in real time, enabling direct estimation of capacitor mismatch parameters that determine INL behavior. Our algorithm dynamically selects measurement points based on current model uncertainty, maximizing information gain with respect to parameter confidence and narrowing sampling intervals as estimation progresses. By providing immediate feedback and adaptive targeting, the proposed method eliminates the need for large-scale data collection and post-measurement analysis. Experimental results demonstrate substantial reductions in total test time and computational overhead, highlighting the method's suitability for integration in production environments.

</details>


### [46] [Advanced Strategies for Uncertainty-Guided Live Measurement Sequencing in Fast, Robust SAR ADC Linearity Testing](https://arxiv.org/abs/2511.11917)
*Thorben Schey,Khaled Karoonlatifi,Michael Weyrich,Andrey Morozov*

Main category: cs.AR

TL;DR: 增强版UGLMS方法通过秩-1 EKF更新、协方差膨胀策略、载波多项式扩展和基于轨迹的终止机制，显著提升了SAR ADC线性度测试速度，16位ADC仅需36ms即可重建INL/DNL。


<details>
  <summary>Details</summary>
Motivation: 传统SAR ADC测试需要全范围扫描和离线后处理，UGLMS方法旨在实现实时、自适应的测试策略，消除这些耗时步骤。

Method: 使用秩-1 EKF更新替代矩阵求逆，引入协方差膨胀加速收敛，扩展静态失配模型以捕获系统非线性，采用基于轨迹的测试终止机制。

Result: 16位ADC测试时间仅36ms，18位ADC测试时间低于70ms，结合协方差膨胀和秩-1 EKF更新，16位ADC测试速度提升8倍。

Conclusion: 增强版UGLMS实现了生产就绪的实时SAR ADC线性度测试，大幅提升了测试效率。

Abstract: This paper builds on our Uncertainty-Guided Live Measurement Sequencing (UGLMS) method. UGLMS is a closed-loop test strategy that adaptively selects SAR ADC code edges based on model uncertainty and refines a behavioral mismatch model in real time via an Extended Kalman Filter (EKF), eliminating full-range sweeps and offline post-processing. We introduce an enhanced UGLMS that delivers significantly faster test runtimes while maintaining estimation accuracy. First, a rank-1 EKF update replaces costly matrix inversions with efficient vector operations, and a measurement-aligned covariance-inflation strategy accelerates convergence under unexpected innovations. Second, we extend the static mismatch model with a low-order carrier polynomial to capture systematic nonlinearities beyond pure capacitor mismatch. Third, a trace-based termination adapts test length to convergence, preventing premature stops and redundant iterations. Simulations show the enhanced UGLMS reconstructs full Integral- and Differential-Non-Linearity (INL/DNL) in just 36 ms for 16-bit and under 70 ms for 18-bit ADCs (120 ms with the polynomial extension). Combining the faster convergence from covariance inflation with reduced per-iteration runtime from the rank-1 EKF update, the method reaches equal accuracy 8x faster for 16-bit ADCs. These improvements enable real-time, production-ready SAR ADC linearity testing.

</details>


### [47] [TIMERIPPLE: Accelerating vDiTs by Understanding the Spatio-Temporal Correlations in Latent Space](https://arxiv.org/abs/2511.12035)
*Wenxuan Miao,Yulin Sun,Aiyue Chen,Jing Lin,Yiwu Yao,Yiming Gan,Jieru Zhao,Jingwen Leng,Mingyi Guo,Yu Feng*

Main category: cs.AR

TL;DR: 提出了一种基于时空相关性的轻量级自适应重用策略，通过重用空间或时间相关token的注意力分数来加速视频扩散变换器中的自注意力计算。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型主要基于视频扩散变换器(vDiT)，但由于自注意力机制导致推理延迟显著。先前研究主要关注减少自注意力中的冗余计算，但忽略了视频流中固有的时空相关性。

Method: 利用潜在空间中的时空相关性，提出轻量级自适应重用策略，通过重用空间或时间相关token的部分注意力分数来近似注意力计算。

Result: 在4个vDiT模型上实现了显著更高的计算节省(85%)，同时保持了几乎相同的视频质量(VBench上损失<0.06%)。

Conclusion: 该方法通过利用视频数据中的时空相关性，有效加速了vDiT中的自注意力计算，在保持视频质量的同时大幅减少了计算开销。

Abstract: The recent surge in video generation has shown the growing demand for high-quality video synthesis using large vision models. Existing video generation models are predominantly based on the video diffusion transformer (vDiT), however, they suffer from substantial inference delay due to self-attention. While prior studies have focused on reducing redundant computations in self-attention, they often overlook the inherent spatio-temporal correlations in video streams and directly leverage sparsity patterns from large language models to reduce attention computations.
  In this work, we take a principled approach to accelerate self-attention in vDiTs by leveraging the spatio-temporal correlations in the latent space. We show that the attention patterns within vDiT are primarily due to the dominant spatial and temporal correlations at the token channel level. Based on this insight, we propose a lightweight and adaptive reuse strategy that approximates attention computations by reusing partial attention scores of spatially or temporally correlated tokens along individual channels. We demonstrate that our method achieves significantly higher computational savings (85\%) compared to state-of-the-art techniques over 4 vDiTs, while preserving almost identical video quality ($<$0.06\% loss on VBench).

</details>


### [48] [A digital SRAM-based compute-in-memory macro for weight-stationary dynamic matrix multiplication in Transformer attention score computation](https://arxiv.org/abs/2511.12152)
*Jianyi Yu,Yuxuan Wang,Xiang Fu,Fei Qiao,Ying Wang,Rui Yuan,Liyuan Liu,Cong Shi*

Main category: cs.AR

TL;DR: 提出了一种用于Transformer注意力计算的数字存内计算宏，通过重新设计注意力计算流程和使用位串行移位加法，在65nm工艺下实现了34.1 TOPS/W的能效和120.77 GOPS/mm²的面积效率。


<details>
  <summary>Details</summary>
Motivation: 存内计算技术被广泛应用于能效优化的AI处理器，但传统的权重固定存内计算范式不适合Transformer注意力中的动态矩阵乘法，需要解决数据移动瓶颈问题。

Method: 1) 基于组合QK权重矩阵重新设计注意力分数计算流程；2) 将二项矩阵乘法分解为4组位串行移位和加法；3) 采用零值位跳过、数据驱动字线激活、读写分离6T单元和位交替14T/28T加法器等优化技术。

Result: 在65nm工艺下实现0.35mm²面积，峰值性能42.27 GOPS，功耗1.24mW，能效34.1 TOPS/W，面积效率120.77 GOPS/mm²。相比CPU和GPU分别提升25倍和13倍能效，相比其他Transformer存内计算设计至少提升7倍能效和2倍面积效率。

Conclusion: 该存内计算宏设计在能效和面积效率方面显著优于传统处理器和其他存内计算方案，展示了在边缘智能应用中的巨大潜力。

Abstract: Compute-in-memory (CIM) techniques are widely employed in energy-efficient artificial intelligent (AI) processors. They alleviate power and latency bottlenecks caused by extensive data movements between compute and storage units. This work proposes a digital CIM macro to compute Transformer attention. To mitigate dynamic matrix multiplication that is unsuitable for the common weight-stationary CIM paradigm, we reformulate the attention score computation process based on a combined QK-weight matrix, so that inputs can be directly fed to CIM cells to obtain the score results. Moreover, the involved binomial matrix multiplication operation is decomposed into 4 groups of bit-serial shifting and additions, without costly physical multipliers in the CIM. We maximize the energy efficiency of the CIM circuit through zero-value bit-skipping, data-driven word line activation, read-write separate 6T cells and bit-alternating 14T/28T adders. The proposed CIM macro was implemented using a 65-nm process. It occupied only 0.35 mm2 area, and delivered a 42.27 GOPS peak performance with 1.24 mW power consumption at a 1.0 V power supply and a 100 MHz clock frequency, resulting in 34.1 TOPS/W energy efficiency and 120.77 GOPS/mm2 area efficiency. When compared to the CPU and GPU, our CIM macro is 25x and 13x more energy efficient on practical tasks, respectively. Compared with other Transformer-CIMs, our design exhibits at least 7x energy efficiency and at least 2x area efficiency improvements when scaled to the same technology node, showcasing its potential for edge-side intelligent applications.

</details>


### [49] [Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing](https://arxiv.org/abs/2511.12286)
*Khyati Kiyawat,Zhenxing Fan,Yasas Seneviratne,Morteza Baradaran,Akhil Shekar,Zihan Xia,Mingu Kang,Kevin Skadron*

Main category: cs.AR

TL;DR: 提出Sangam，一种基于CXL连接的PIM-chiplet内存模块，通过将逻辑和内存解耦到不同工艺节点的chiplet中，解决了现有内存计算方案的容量和性能限制，显著加速LLM推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型变得数据密集且内存受限，现有内存计算方案面临内存容量减少和处理能力有限的问题。

Method: 使用chiplet技术将逻辑和内存分离到不同工艺节点的芯片中，通过中介层连接，逻辑chiplet支持高带宽访问DRAM chiplet，集成脉动阵列和SRAM缓冲区来加速内存受限的GEMM操作。

Result: 相比H100 GPU，在LLaMA 2-7B、Mistral-7B和LLaMA 3-70B上，端到端查询延迟分别提升3.93、4.22、2.82倍，解码吞吐量分别提升10.3、9.5、6.36倍，能耗节省一个数量级。

Conclusion: Sangam架构有效解决了内存计算的关键限制，显著提升了LLM推理性能，可作为GPU的替代或协同执行方案。

Abstract: Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively.

</details>


### [50] [Pushing the Memory Bandwidth Wall with CXL-enabled Idle I/O Bandwidth Harvesting](https://arxiv.org/abs/2511.12349)
*Divya Kiran Kadiyala,Alexandros Daglis*

Main category: cs.AR

TL;DR: SURGE是一种软件支持的架构技术，通过利用空闲的I/O带宽资源来提升内存带宽可用性，解决服务器CPU核心增加导致的内存带宽不足问题。


<details>
  <summary>Details</summary>
Motivation: 随着服务器CPU核心数量的持续增加，内存系统面临压力，受限于有限的片外引脚和数据传输速率可扩展性，高端处理器通常每个核心的内存带宽较低，影响内存密集型工作负载的性能。

Method: SURGE利用CXL等通用互连技术，在相同的处理器接口上动态复用内存和I/O流量，通过软件支持实现I/O和内存带宽的可互换性。

Result: SURGE增强的架构可以在带宽受限的服务器上加速内存密集型工作负载，性能提升高达1.3倍。

Conclusion: SURGE通过提高CPU有限引脚的利用率，解决了内存带宽不足的问题，为带宽受限的服务器提供了有效的性能提升方案。

Abstract: The continual increase of cores on server-grade CPUs raises demands on memory systems, which are constrained by limited off-chip pin and data transfer rate scalability. As a result, high-end processors typically feature lower memory bandwidth per core, at the detriment of memory-intensive workloads. We propose alleviating this challenge by improving the utility of the CPU's limited pins. In a typical CPU design process, the available pins are apportioned between memory and I/O traffic, each accounting for about half of the total off-chip bandwidth availability. Consequently, unless both memory and I/O are simultaneously highly utilized, such fragmentation leads to underutilization of the valuable off-chip bandwidth resources. An ideal architecture would offer I/O and memory bandwidth fungibility, allowing use of the aggregate off-chip bandwidth in the form required by each workload.
  In this work, we introduce SURGE, a software-supported architectural technique that boosts memory bandwidth availability by salvaging idle I/O bandwidth resources. SURGE leverages the capability of versatile interconnect technologies like CXL to dynamically multiplex memory and I/O traffic over the same processor interface. We demonstrate that SURGE-enhanced architectures can accelerate memory-intensive workloads on bandwidth-constrained servers by up to 1.3x.

</details>


### [51] [FERMI-ML: A Flexible and Resource-Efficient Memory-In-Situ SRAM Macro for TinyML acceleration](https://arxiv.org/abs/2511.12544)
*Mukul Lokhande,Akash Sankhe,S. V. Jaya Chand,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: FERMI-ML是一种用于TinyML加速的灵活资源高效内存原位SRAM宏，集成了9T XNOR位单元和22T压缩器树累加器，支持可变精度MAC和CAM操作，在65nm工艺下实现1.93 TOPS吞吐量和364 TOPS/W能效。


<details>
  <summary>Details</summary>
Motivation: AIoT设备对低功耗和面积高效的TinyML推理需求日益增长，需要最小化数据移动同时保持高计算效率的内存架构。

Method: 提出9T XNOR基RX9T位单元，将5T存储单元与4T XNOR计算单元集成，支持可变精度MAC和CAM操作；使用22T压缩器树累加器进行对数1-64位MAC计算。

Result: 4KB宏在65nm工艺下以350MHz频率和0.9V电压运行，实现1.93 TOPS吞吐量和364 TOPS/W能效，在InceptionV4和ResNet-18上保持97.5%以上的结果质量。

Conclusion: FERMI-ML展示了一种紧凑、可重构且能量感知的数字内存原位宏，能够支持混合精度TinyML工作负载。

Abstract: The growing demand for low-power and area-efficient TinyML inference on AIoT devices necessitates memory architectures that minimise data movement while sustaining high computational efficiency. This paper presents FERMI-ML, a Flexible and Resource-Efficient Memory-In-Situ (MIS) SRAM macro designed for TinyML acceleration. The proposed 9T XNOR-based RX9T bit-cell integrates a 5T storage cell with a 4T XNOR compute unit, enabling variable-precision MAC and CAM operations within the same array. A 22-transistor (C22T) compressor-tree-based accumulator facilitates logarithmic 1-64-bit MAC computation with reduced delay and power compared to conventional adder trees. The 4 KB macro achieves dual functionality for in-situ computation and CAM-based lookup operations, supporting Posit-4 or FP-4 precision. Post-layout results at 65 nm show operation at 350 MHz with 0.9 V, delivering a throughput of 1.93 TOPS and an energy efficiency of 364 TOPS/W, while maintaining a Quality-of-Result (QoR) above 97.5% with InceptionV4 and ResNet-18. FERMI-ML thus demonstrates a compact, reconfigurable, and energy-aware digital Memory-In-Situ macro capable of supporting mixed-precision TinyML workloads.

</details>


### [52] [SynapticCore-X: A Modular Neural Processing Architecture for Low-Cost FPGA Acceleration](https://arxiv.org/abs/2511.12616)
*Arya Parameshwara*

Main category: cs.AR

TL;DR: SynapticCore-X是一个模块化、资源高效的神经处理架构，专为低成本FPGA平台优化，集成了RISC-V控制核心和可配置神经计算单元，提供完全开源的SystemVerilog微架构。


<details>
  <summary>Details</summary>
Motivation: 现有FPGA加速器依赖重量级IP块，SynapticCore-X旨在提供开源、可配置的神经微架构，降低学术和开源硬件研究的入门门槛。

Method: 集成RV32IMC RISC-V控制核心与可配置神经计算单元，支持融合矩阵、激活和数据移动操作，提供可调并行性、暂存器内存深度和DMA突发行为。

Result: 在Zynq-7020上实现100MHz时序收敛，仅消耗6.1% LUTs、32.5% DSPs和21.4% BRAMs。PYNQ-Z2硬件验证确认了寄存器级执行正确性、确定性控制路径行为和周期精确性能。

Conclusion: SynapticCore-X证明类似NPU的高效能加速可以在商用教育FPGA上原型设计，为神经微架构研究降低了入门门槛。

Abstract: This paper presents SynapticCore-X, a modular and resource-efficient neural processing architecture optimized for deployment on low-cost FPGA platforms. The design integrates a lightweight RV32IMC RISC-V control core with a configurable neural compute tile that supports fused matrix, activation, and data-movement operations. Unlike existing FPGA accelerators that rely on heavyweight IP blocks, SynapticCore-X provides a fully open-source SystemVerilog microarchitecture with tunable parallelism, scratchpad memory depth, and DMA burst behavior, enabling rapid exploration of hardware-software co-design trade-offs. We document an automated, reproducible Vivado build pipeline that achieves timing closure at 100 MHz on the Zynq-7020 while consuming only 6.1% LUTs, 32.5% DSPs, and 21.4% BRAMs. Hardware validation on PYNQ-Z2 confirms correct register-level execution, deterministic control-path behavior, and cycle-accurate performance for matrix and convolution kernels. SynapticCore-X demonstrates that energy-efficient NPU-like acceleration can be prototyped on commodity educational FPGAs, lowering the entry barrier for academic and open-hardware research in neural microarchitectures.

</details>


### [53] [Dissecting and Re-architecting 3D NAND Flash PIM Arrays for Efficient Single-Batch Token Generation in LLMs](https://arxiv.org/abs/2511.12860)
*Yongjoo Jang,Sangwoo Hwang,Hojin Lee,Sangwoo Jung,Donghun Lee,Wonbo Shim,Jaeha Kung*

Main category: cs.AR

TL;DR: 提出使用3D NAND闪存处理内存设备来卸载单批次token生成，以解决大语言模型的内存和计算需求问题，通过优化的PIM架构实现显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型参数数量激增导致内存和计算需求大幅增加，传统硬件DRAM容量有限且GPU成本高昂，需要寻找替代解决方案。

Method: 重新设计3D NAND闪存PIM阵列，采用H-tree网络优化延迟和单元密度，结合适当的PIM阵列大小，开发操作分块和映射方法用于LLM层。

Result: 相比四个RTX4090使用vLLM实现了2.4倍加速，与四个A100性能相当且仅有4.9%延迟开销，可在4.98mm²芯片面积内集成而不产生额外面积开销。

Conclusion: 3D NAND闪存PIM架构是解决大语言模型内存容量瓶颈的有效方案，能够在有限面积内提供高性能的推理服务。

Abstract: The advancement of large language models has led to models with billions of parameters, significantly increasing memory and compute demands. Serving such models on conventional hardware is challenging due to limited DRAM capacity and high GPU costs. Thus, in this work, we propose offloading the single-batch token generation to a 3D NAND flash processing-in-memory (PIM) device, leveraging its high storage density to overcome the DRAM capacity wall. We explore 3D NAND flash configurations and present a re-architected PIM array with an H-tree network for optimal latency and cell density. Along with the well-chosen PIM array size, we develop operation tiling and mapping methods for LLM layers, achieving a 2.4x speedup over four RTX4090 with vLLM and comparable performance to four A100 with only 4.9% latency overhead. Our detailed area analysis reveals that the proposed 3D NAND flash PIM architecture can be integrated within a 4.98mm2 die area under the memory array, without extra area overhead.

</details>


### [54] [Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration](https://arxiv.org/abs/2511.12930)
*Changhun Oh,Seongryong Oh,Jinwoo Hwang,Yoonsung Kim,Hardik Sharma,Jongse Park*

Main category: cs.AR

TL;DR: Neo是一个针对3D高斯泼溅渲染的硬件加速器，通过重用和更新排序算法利用帧间高斯排序的时间冗余，显著降低内存带宽需求，在资源受限设备上实现高质量实时3D渲染。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上实现3D高斯泼溅的实时渲染对于AR/VR体验至关重要，但现有解决方案难以达到高帧率，特别是高分辨率渲染时。分析发现渲染流水线中的排序阶段是主要瓶颈。

Method: 提出重用和更新排序算法，利用连续帧间高斯排序的时间冗余，设计针对该算法的硬件加速器，通过跟踪和更新高斯深度排序而非从头重新排序来减少冗余计算和内存带宽压力。

Result: Neo相比最先进的边缘GPU和ASIC解决方案，分别实现了10.0倍和5.6倍的吞吐量提升，同时将DRAM流量减少了94.5%和81.3%。

Conclusion: Neo的改进使得在设备上实现高质量、低延迟的3D渲染更加实用可行。

Abstract: 3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical.

</details>


### [55] [Think with Self-Decoupling and Self-Verification: Automated RTL Design with Backtrack-ToT](https://arxiv.org/abs/2511.13139)
*Zhiteng Chao,Yonghao Wang,Xinyu Zhang,Jiaxin Zhou,Tenghui Hua,Husheng Han,Tianmeng Yang,Jianan Mu,Bei Yu,Rui Zhang,Jing Ye,Huawei Li*

Main category: cs.AR

TL;DR: VeriBToT是一种专门用于自动化Verilog生成的LLM推理范式，通过集成自上而下和面向验证的设计方法，实现中间步骤的自解耦和自验证，构建带有形式化算子的回溯思维树。


<details>
  <summary>Details</summary>
Motivation: 传统CoT方法在自动化IC设计工作流中效果不佳，需要人工干预，主要问题在于控制CoT推理方向和步骤粒度与专家RTL设计知识不匹配。

Method: 集成Top-down和design-for-verification方法，实现自解耦和自验证中间步骤，构建带有形式化算子的回溯思维树。

Result: 相比传统CoT范式，该方法在增强Verilog生成的同时，通过灵活的模块化、层次化和可重用性优化了token成本。

Conclusion: VeriBToT为自动化Verilog生成提供了一种有效的专门化LLM推理范式，解决了传统方法在IC设计中的局限性。

Abstract: Large language models (LLMs) hold promise for automating integrated circuit (IC) engineering using register transfer level (RTL) hardware description languages (HDLs) like Verilog. However, challenges remain in ensuring the quality of Verilog generation. Complex designs often fail in a single generation due to the lack of targeted decoupling strategies, and evaluating the correctness of decoupled sub-tasks remains difficult. While the chain-of-thought (CoT) method is commonly used to improve LLM reasoning, it has been largely ineffective in automating IC design workflows, requiring manual intervention. The key issue is controlling CoT reasoning direction and step granularity, which do not align with expert RTL design knowledge. This paper introduces VeriBToT, a specialized LLM reasoning paradigm for automated Verilog generation. By integrating Top-down and design-for-verification (DFV) approaches, VeriBToT achieves self-decoupling and self-verification of intermediate steps, constructing a Backtrack Tree of Thought with formal operators. Compared to traditional CoT paradigms, our approach enhances Verilog generation while optimizing token costs through flexible modularity, hierarchy, and reusability.

</details>


### [56] [Coliseum project: Correlating climate change data with the behavior of heritage materials](https://arxiv.org/abs/2511.13343)
*A Cormier,David Roqui,Fabrice Surma,Martin Labouré,Jean-Marc Vallet,Odile Guillon,N Grozavu,Ann Bourgès*

Main category: cs.AR

TL;DR: COLISEUM项目开发了一种利用人工智能模型预测文化遗产材料行为的方法论，通过在法国三个不同气候和材料的遗址收集多模态数据，建立风化模型来预测气候变化对遗产材料的影响。


<details>
  <summary>Details</summary>
Motivation: 气候变化正在影响遗产材料，增加的气候变化减少了古迹的寿命。由于风化取决于多种因素，很难将其进展与气候变化联系起来。为了预测风化，需要同时收集气候数据并监测劣化进展。

Method: 在法国三个文化遗址（斯特拉斯堡大教堂、比布拉克特考古遗址、维尔弗朗什滨海圣皮埃尔教堂）建立气候监测方法，使用微气候传感器连续记录参数变化，通过化学分析、制图测量和科学成像定期监测劣化状态，利用计算的风化指数构建风化模型。

Result: 项目建立了完整的仪器监测方法、初始诊断，并以斯特拉斯堡大教堂为例展示了初步结果，能够持续收集多模态数据用于风化预测模型。

Conclusion: 通过COLISEUM项目的方法论，可以收集多模态数据并建立人工智能模型，利用IPCC的不同气候变化情景数据来预测未来遗产材料的行为，为文化遗产保护提供科学依据。

Abstract: Heritage materials are already affected by climate change, and increasing climatic variations reduces the lifespan of monuments. As weathering depends on many factors, it is also difficult to link its progression to climatic changes. To predict weathering, it is essential to gather climatic data while simultaneously monitoring the progression of deterioration. The multimodal nature of collected data (images, text{\ldots}) makes correlations difficult, particularly on different time scales. To address this issue, the COLISEUM project proposes a methodology for collecting data in three French sites to predict heritage material behaviour using artificial intelligence computer models. Over time, prediction models will allow the prediction of future material behaviours using known data from different climate change scenarios by the IPCC (Intergovernmental Panel on Climate Change). Thus, a climate monitoring methodology has been set up in three cultural sites in France: Notre-Dame cathedral in Strasbourg ( 67), Bibracte archaeological site (71), and the Saint-Pierre chapel in Villefranche-sur-Mer (06). Each site has a different climate and specific materials. In situ, microclimatic sensors continuously record variations parameters over time. The state of alteration is monitored at regular intervals by means of chemical analyses, cartographic measurements and scientific imaging campaigns. To implement weathering models, data is gathered in alteration matrix by mean of a calculated weathering index. This article presents the instrumentation methodology, the initial diagnostic and the first results with the example of Strasbourg Cathedral site.

</details>


### [57] [T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization](https://arxiv.org/abs/2511.13676)
*Hyunwoo Oh,KyungIn Nam,Rajat Bhattacharjya,Hanning Chen,Tamoghno Das,Sanggeon Yun,Suyeon Jang,Andrew Ding,Nikil Dutt,Mohsen Imani*

Main category: cs.AR

TL;DR: T-SAR是一个在CPU上实现可扩展三元LLM推理的框架，通过重新利用SIMD寄存器文件进行动态寄存器内查找表生成，显著提升了边缘平台的推理效率。


<details>
  <summary>Details</summary>
Motivation: LLM的快速发展超过了边缘平台的计算和内存能力，现有CPU解决方案依赖内存查找表限制了可扩展性，而FPGA或GPU加速器在边缘场景中不实用。

Method: 重新利用SIMD寄存器文件进行动态寄存器内查找表生成，只需最小的硬件修改，消除内存瓶颈并最大化数据级并行性。

Result: 在GEMM延迟和GEMV吞吐量上分别实现5.6-24.5倍和1.1-86.2倍的提升，SIMD单元仅增加3.2%功耗和1.4%面积开销，能效比NVIDIA Jetson AGX Orin高2.5-4.9倍。

Conclusion: T-SAR为边缘平台上高效的LLM推理提供了一种实用方法，通过最小硬件修改实现了显著的性能提升。

Abstract: Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms.

</details>


### [58] [QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention](https://arxiv.org/abs/2511.13679)
*Hyunwoo Oh,Hanning Chen,Sanggeon Yun,Yang Ni,Wenjun Huang,Tamoghno Das,Suyeon Jang,Mohsen Imani*

Main category: cs.AR

TL;DR: QUILL是一个针对可变形注意力机制的硬件加速器，通过距离排序查询和调度感知预取，将不规则内存访问转换为缓存友好的单次计算，显著提升吞吐量和能效。


<details>
  <summary>Details</summary>
Motivation: 可变形变换器在检测任务中表现优异，但由于不规则内存访问和低算术强度，难以在硬件上高效实现。

Method: 采用距离排序查询(DOOQ)按空间邻近度排序查询，结合调度感知预取循环，融合MSDeformAttn引擎在单次计算中完成插值、Softmax、聚合和投影操作。

Result: 相比RTX 4090，QUILL实现7.29倍吞吐量和47.3倍能效提升；相比现有加速器，吞吐量提升3.26-9.82倍，能效提升2.01-6.07倍。

Conclusion: QUILL通过将稀疏性转换为局部性，再将局部性转换为利用率，实现了端到端的持续加速。

Abstract: Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.

</details>
