<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Linear effects, exceptions, and resource safety: a Curry-Howard correspondence for destructors](https://arxiv.org/abs/2510.23517)
*Sidney Congard,Guillaume Munch-Maccagnoni,Rémi Douence*

Main category: cs.PL

TL;DR: 该论文研究了在线性设置中结合线性性、效应和异常的问题，通过为分配单子提供某种强度来实现。提出了两个线性效应演算，第一个是线性call-by-push-value语言，第二个是仿射有序call-by-push-value语言，包含异常和析构器。


<details>
  <summary>Details</summary>
Motivation: 研究如何在线性编程语言模型中同时处理线性性、效应和异常，特别是为分配单子T(-⊕E)提供强度，以建模和研究资源安全性属性。

Method: 1. 引入分配单子来建模资源安全性；2. 构建第一个线性call-by-push-value演算，包含new和delete分配效应；3. 通过为类型添加默认析构动作来集成异常，将析构器视为切片范畴中的对象δ:A→TI；4. 构建第二个仿射有序call-by-push-value演算，包含异常和析构器。

Result: 1. 第一个演算通过线性（甚至有序）类型规则实现了资源安全性；2. 第二个演算支持异常处理，其中弱化规则会产生副作用，需要"移动"操作来允许随机顺序释放资源（而非后进先出）。

Conclusion: 成功在线性设置中结合了线性性、效应和异常，通过分配单子和析构器机制实现了资源安全性，并提供了类似C++/Rust的移动操作来管理资源释放顺序。

Abstract: We analyse the problem of combining linearity, effects, and exceptions, in
abstract models of programming languages, as the issue of providing some kind
of strength for a monad $T(- \oplus E)$ in a linear setting. We consider in
particular for $T$ the allocation monad, which we introduce to model and study
resource-safety properties. We apply these results to a series of two linear
effectful calculi for which we establish their resource-safety properties.
  The first calculus is a linear call-by-push-value language with two
allocation effects $\mathit{new}$ and $\mathit{delete}$. The resource-safety
properties follow from the linear (and even ordered) character of the typing
rules.
  We then explain how to integrate exceptions on top of linearity and effects
by adjoining default destruction actions to types, as inspired by C++/Rust
destructors. We see destructors as objects $\delta : A\rightarrow TI$ in the
slice category over $TI$. This construction gives rise to a second calculus, an
affine ordered call-by-push-value language with exceptions and destructors, in
which the weakening rule performs a side-effect. As in C++/Rust, a ``move''
operation is necessary to allow random-order release of resources, as opposed
to last-in-first-out order. Moving resources is modelled as an exchange rule
that performs a side-effect.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [When Agents are Powerful: Black Hole Search in Time-Varying Graphs](https://arxiv.org/abs/2510.22309)
*Tanvir Kaur,Ashish Saxena*

Main category: cs.DC

TL;DR: 本文改进了黑洞搜索问题中代理的能力，通过引入全局通信和1跳可见性，显著减少了解决动态图中黑洞搜索问题所需的代理数量。


<details>
  <summary>Details</summary>
Motivation: 先前的工作在任意动态图中解决黑洞搜索问题时，使用面对面的通信方式需要大量代理。这种限制促使研究者增强代理能力以提高效率。

Method: 为代理提供两种增强能力：全局通信（允许代理在任何位置交互）和1跳可见性（允许代理观察其直接邻居）。

Result: 这些增强能力使得在动态图中解决黑洞搜索问题更加高效，显著减少了所需的代理数量。

Conclusion: 通过增强代理的通信和感知能力，可以在动态图中以更少的代理数量有效解决黑洞搜索问题。

Abstract: A black hole is a harmful node in a graph that destroys any resource entering
it, making its identification a critical task. In the \emph{Black Hole Search
(BHS)} problem, a team of agents operates on a graph $G$ with the objective
that at least one agent must survive and correctly identify an edge incident to
the black hole. Prior work has addressed BHS in arbitrary dynamic graphs under
the restrictive \emph{face-to-face} communication, where agents can exchange
information only when co-located. This constraint significantly increases the
number of agents required to solve the problem. In this work, we strengthen the
capabilities of agents in two ways: (i) granting them \emph{global
communication}, enabling interaction regardless of location, and (ii) equipping
them with \emph{1-hop visibility}, allowing each agent to observe its immediate
neighborhood. These enhancements lead to more efficient solutions for the BHS
problem in dynamic graphs.

</details>


### [3] [Separation of Unconscious Robots with Obstructed Visibility](https://arxiv.org/abs/2510.22434)
*Prajyot Pyati,Navjot Kaur,Saswata Jana,Adri Bhattacharya,Partha Sarathi Mandal*

Main category: cs.DC

TL;DR: 本文研究了一种无意识移动机器人模型，在遮挡可见性条件下解决分离问题，将机器人分离成同心半圆。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设机器人是透明的，能直接看到所有其他机器人的位置和颜色。本文考虑更现实的遮挡可见性模型，即机器人可能遮挡其他机器人之间的视线。

Method: 提出了一种无碰撞算法，在半同步调度器下运行。机器人基于颜色分离，形成同心半圆形，且只需要就一个坐标轴达成一致，无需知道机器人总数n。

Result: 算法在O(n)个周期内解决分离问题，其中n是机器人数量。

Conclusion: 在遮挡可见性条件下，无意识机器人能够有效解决分离问题，形成同心半圆几何形状。

Abstract: We study a recently introduced \textit{unconscious} mobile robot model, where
each robot is associated with a \textit{color}, which is visible to other
robots but not to itself. The robots are autonomous, anonymous, oblivious and
silent, operating in the Euclidean plane under the conventional
\textit{Look-Compute-Move} cycle. A primary task in this model is the
\textit{separation problem}, where unconscious robots sharing the same color
must separate from others, forming recognizable geometric shapes such as
circles, points, or lines. All prior works model the robots as
\textit{transparent}, enabling each to know the positions and colors of all
other robots. In contrast, we model the robots as \textit{opaque}, where a
robot can obstruct the visibility of two other robots, if it lies on the line
segment between them. Under this obstructed visibility, we consider a variant
of the separation problem in which robots, starting from any arbitrary initial
configuration, are required to separate into concentric semicircles. We present
a collision-free algorithm that solves the separation problem under a
semi-synchronous scheduler in $O(n)$ epochs, where $n$ is the number of robots.
The robots agree on one coordinate axis but have no knowledge of $n$.

</details>


### [4] [Rethinking Inference Placement for Deep Learning across Edge and Cloud Platforms: A Multi-Objective Optimization Perspective and Future Directions](https://arxiv.org/abs/2510.22909)
*Zongshun Zhang,Ibrahim Matta*

Main category: cs.DC

TL;DR: 这篇论文综述了边缘智能应用中深度学习模型卸载和优化的最新方法，重点研究了在推理延迟、数据隐私和资源成本之间的多目标平衡。


<details>
  <summary>Details</summary>
Motivation: 随着物联网和移动设备的快速发展，边缘智能应用如VR/AR和基于语言模型的聊天机器人日益普及，但受限的边缘设备难以支持日益庞大复杂的深度学习模型。

Method: 通过模型压缩、模型蒸馏、传输压缩和模型架构适配（包括内部分类器）等技术，优化和卸载深度学习模型在用户设备、边缘服务器和云之间的分区。

Result: 研究分析了现有模型卸载方法和模型适配技术对推理延迟、数据隐私和资源成本的多目标优化影响。

Conclusion: 该调查为边缘智能应用中深度学习模型卸载和优化提供了全面的技术综述，强调了在延迟、隐私和成本之间的权衡策略。

Abstract: Edge intelligent applications like VR/AR and language model based chatbots
have become widespread with the rapid expansion of IoT and mobile devices.
However, constrained edge devices often cannot serve the increasingly large and
complex deep learning (DL) models. To mitigate these challenges, researchers
have proposed optimizing and offloading partitions of DL models among user
devices, edge servers, and the cloud. In this setting, users can take advantage
of different services to support their intelligent applications. For example,
edge resources offer low response latency. In contrast, cloud platforms provide
low monetary cost computation resources for computation-intensive workloads.
However, communication between DL model partitions can introduce transmission
bottlenecks and pose risks of data leakage. Recent research aims to balance
accuracy, computation delay, transmission delay, and privacy concerns. They
address these issues with model compression, model distillation, transmission
compression, and model architecture adaptations, including internal
classifiers. This survey contextualizes the state-of-the-art model offloading
methods and model adaptation techniques by studying their implication to a
multi-objective optimization comprising inference latency, data privacy, and
resource monetary cost.

</details>


### [5] [Bayes-Split-Edge: Bayesian Optimization for Constrained Collaborative Inference in Wireless Edge Systems](https://arxiv.org/abs/2510.23503)
*Fatemeh Zahra Safaeipour,Jacob Chakareski,Morteza Hashemi*

Main category: cs.DC

TL;DR: 提出Bayes-Split-Edge框架，使用贝叶斯优化联合优化传输功率和神经网络分割点，实现无线边缘网络中协作推理的约束感知优化。


<details>
  <summary>Details</summary>
Motivation: 移动边缘设备（如AR/VR头显）需要在有限的算力和能源资源下完成及时推理任务，因此需要优化在能量和延迟约束下的推理性能。

Method: 提出Bayes-Split-Edge解决方案，利用贝叶斯优化进行无线边缘网络的协作分割推理，采用混合采集函数平衡推理任务效用、样本效率和约束违规惩罚。

Result: 在VGG19和Resnet101模型上的实验表明，相比标准贝叶斯优化减少2.4倍评估成本，实现近线性收敛，优于CMA-ES、DIRECT、穷举搜索和PPO等基线方法。

Conclusion: 该框架为边缘计算系统中的无线分割推理提供了样本高效的解决方案，最多仅需20次函数评估即可实现约束感知优化。

Abstract: Mobile edge devices (e.g., AR/VR headsets) typically need to complete timely
inference tasks while operating with limited on-board computing and energy
resources. In this paper, we investigate the problem of collaborative inference
in wireless edge networks, where energy-constrained edge devices aim to
complete inference tasks within given deadlines. These tasks are carried out
using neural networks, and the edge device seeks to optimize inference
performance under energy and delay constraints. The inference process can be
split between the edge device and an edge server, thereby achieving
collaborative inference over wireless networks. We formulate an inference
utility optimization problem subject to energy and delay constraints, and
propose a novel solution called Bayes-Split-Edge, which leverages Bayesian
optimization for collaborative split inference over wireless edge networks. Our
solution jointly optimizes the transmission power and the neural network split
point. The Bayes-Split-Edge framework incorporates a novel hybrid acquisition
function that balances inference task utility, sample efficiency, and
constraint violation penalties. We evaluate our approach using the VGG19 model
on the ImageNet-Mini dataset, and Resnet101 on Tiny-ImageNet, and real-world
mMobile wireless channel datasets. Numerical results demonstrate that
Bayes-Split-Edge achieves up to 2.4x reduction in evaluation cost compared to
standard Bayesian optimization and achieves near-linear convergence. It also
outperforms several baselines, including CMA-ES, DIRECT, exhaustive search, and
Proximal Policy Optimization (PPO), while matching exhaustive search
performance under tight constraints. These results confirm that the proposed
framework provides a sample-efficient solution requiring maximum 20 function
evaluations and constraint-aware optimization for wireless split inference in
edge computing systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [Simopt-Power: Leveraging Simulation Metadata for Low-Power Design Synthesis](https://arxiv.org/abs/2510.21745)
*Eashan Wadhwa,Shanker Shreejith*

Main category: cs.AR

TL;DR: Simopt-power是一个基于仿真分析的FPGA功耗优化框架，通过识别高切换路径并选择性重构来降低动态功耗，平均减少约9%的切换功耗，仅增加约9%的LUT资源。


<details>
  <summary>Details</summary>
Motivation: 现代FPGA中过度的信号切换是动态功耗的主要来源，传统低功耗技术需要侵入性设计变更或随着器件密度增加而收益递减。

Method: 利用仿真分析识别高切换路径，通过Shannon分解原理插入重复真值表逻辑并重新定位关键网络，在不改变功能行为的情况下减少不必要的切换。

Result: 在开源RTLLM基准测试中，平均切换功耗减少约9%，算术设计仅增加约9%的LUT等效资源。

Conclusion: 将仿真洞察与针对性优化相结合可以有效降低动态功耗，为在FPGA-CAD流程中使用仿真元数据提供了实用路径。

Abstract: Excessive switching activity is a primary contributor to dynamic power
dissipation in modern FPGAs, where fine-grained configurability amplifies
signal toggling and associated capacitance. Conventional low-power techniques
-- gating, clock-domain partitioning, and placement-aware netlist rewrites -
either require intrusive design changes or offer diminishing returns as device
densities grow. In this work, we present Simopt-power, a simulator-driven
optimisation framework that leverages simulation analysis to identify and
selectively reconfigure high-toggle paths. By feeding activity profiles back
into a lightweight transformation pass, Simopt-power judiciously inserts
duplicate truth table logic using Shannon Decomposition principle and relocates
critical nets, thereby attenuating unnecessary transitions without perturbing
functional behaviour. We evaluated this framework on open-source RTLLM
benchmark, with Simopt-power achieves an average switching-induced power
reduction of ~9\% while incurring only ~9\% additional LUT-equivalent resources
for arithmetic designs. These results demonstrate that coupling simulation
insights with targeted optimisations can yield a reduced dynamic power,
offering a practical path toward using simulation metadata in the FPGA-CAD
flow.

</details>


### [7] [QuArch: A Benchmark for Evaluating LLM Reasoning in Computer Architecture](https://arxiv.org/abs/2510.22087)
*Shvetank Prakash,Andrew Cheng,Arya Tschand,Mark Mazumder,Varun Gohil,Jeffrey Ma,Jason Yik,Zishen Wan,Jessica Quaye,Elisavet Lydia Alvanaki,Avinash Kumar,Chandrashis Mazumdar,Tuhin Khare,Alexander Ingare,Ikechukwu Uchendu,Radhika Ghosal,Abhishek Tyagi,Chenyu Wang,Andrea Mattia Garavagno,Sarah Gu,Alice Guo,Grace Hur,Luca Carloni,Tushar Krishna,Ankita Nayak,Amir Yazdanbakhsh,Vijay Janapa Reddi*

Main category: cs.AR

TL;DR: QuArch是首个专门评估大语言模型在计算机架构领域知识和推理能力的基准测试，包含2,671个专家验证的问题-答案对，覆盖处理器设计、内存系统等核心主题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评估缺乏对计算机架构这一连接软件抽象和硬件实现的关键领域的专门测试，需要建立专门基准来促进该领域LLM能力的发展。

Method: 开发包含2,671个专家验证QA对的综合基准测试，涵盖计算机架构的多个方面，包括处理器设计、内存系统和互连网络等。

Result: 前沿模型在计算机架构领域具有专业知识，但在需要高阶思维的问题上表现不佳，准确率从34%到72%不等，在分析、设计和实现类问题上存在明显差距。

Conclusion: QuArch通过全面评估基础技能，为构建和衡量能够加速计算系统创新的LLM能力提供了基础，代表了社区在设定LLM架构推理评估标准方面的努力。

Abstract: The field of computer architecture, which bridges high-level software
abstractions and low-level hardware implementations, remains absent from
current large language model (LLM) evaluations. To this end, we present QuArch
(pronounced 'quark'), the first benchmark designed to facilitate the
development and evaluation of LLM knowledge and reasoning capabilities
specifically in computer architecture. QuArch provides a comprehensive
collection of 2,671 expert-validated question-answer (QA) pairs covering
various aspects of computer architecture, including processor design, memory
systems, and interconnection networks. Our evaluation reveals that while
frontier models possess domain-specific knowledge, they struggle with skills
that require higher-order thinking in computer architecture. Frontier model
accuracies vary widely (from 34% to 72%) on these advanced questions,
highlighting persistent gaps in architectural reasoning across analysis,
design, and implementation QAs. By holistically assessing fundamental skills,
QuArch provides a foundation for building and measuring LLM capabilities that
can accelerate innovation in computing systems. With over 140 contributors from
40 institutions, this benchmark represents a community effort to set the
standard for architectural reasoning in LLM evaluation.

</details>


### [8] [RAMAN: Resource-efficient ApproxiMate Posit Processing for Algorithm-Hardware Co-desigN](https://arxiv.org/abs/2510.22627)
*Mohd Faisal Khan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: RAMAN提出了一种基于近似posit(8,2)的MAC架构，通过硬件近似和算法-硬件协同设计，在边缘AI应用中实现资源效率和计算效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI应用在资源受限环境下的计算效率问题，特别是在带宽限制下提升硬件效率。

Method: 使用REAP近似MAC引擎，结合可扩展的向量执行单元(VEU)，并采用近似感知训练的算法-硬件协同设计框架。

Result: 在FPGA和ASIC平台上验证，相比基线PDPU设计节省46% LUT、35.66%面积和31.28%功耗，手写数字识别准确率达98.45%。

Conclusion: RAMAN在硬件效率和学习性能之间取得了良好的权衡，适用于下一代边缘智能应用。

Abstract: Edge-AI applications still face considerable challenges in enhancing
computational efficiency in resource-constrained environments. This work
presents RAMAN, a resource-efficient and approximate posit(8,2)-based
Multiply-Accumulate (MAC) architecture designed to improve hardware efficiency
within bandwidth limitations. The proposed REAP (Resource-Efficient Approximate
Posit) MAC engine, which is at the core of RAMAN, uses approximation in the
posit multiplier to achieve significant area and power reductions with an
impact on accuracy. To support diverse AI workloads, this MAC unit is
incorporated in a scalable Vector Execution Unit (VEU), which permits hardware
reuse and parallelism among deep neural network layers. Furthermore, we propose
an algorithm-hardware co-design framework incorporating approximation-aware
training to evaluate the impact of hardware-level approximation on
application-level performance. Empirical validation on FPGA and ASIC platforms
shows that the proposed REAP MAC achieves up to 46% in LUT savings and 35.66%
area, 31.28% power reduction, respectively, over the baseline Posit Dot-Product
Unit (PDPU) design, while maintaining high accuracy (98.45%) for handwritten
digit recognition. RAMAN demonstrates a promising trade-off between hardware
efficiency and learning performance, making it suitable for next-generation
edge intelligence.

</details>


### [9] [Approximate Signed Multiplier with Sign-Focused Compressor for Edge Detection Applications](https://arxiv.org/abs/2510.22674)
*L. Hemanth Krishna,Srinivasu Bodapati,Sreehari Veeramachaneni,BhaskaraRao Jammu,Noor Mahammad Sk*

Main category: cs.AR

TL;DR: 提出了一种用于边缘检测的近似有符号乘法器架构，采用符号聚焦压缩器设计，在保持功能性的同时显著降低功耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 针对机器学习和信号处理中的边缘检测应用，需要高效处理有符号乘法运算，特别是处理常数"1"和负部分积的常见情况。

Method: 使用两种符号聚焦压缩器（A+B+C+1和A+B+C+D+1），结合精确和近似压缩器设计，截断部分积矩阵的N-1列并采用误差补偿机制。

Result: 8位近似乘法器的功耗延迟积降低29.21%，功耗降低14.39%，在卷积层中成功实现边缘检测功能。

Conclusion: 该符号聚焦压缩器架构在保持应用功能的同时显著提升了能效，适用于边缘检测等实际应用场景。

Abstract: This paper presents an approximate signed multiplier architecture that
incorporates a sign-focused compressor, specifically designed for edge
detection applications in machine learning and signal processing. The
multiplier incorporates two types of sign-focused compressors: A + B + C + 1
and A + B + C + D + 1. Both exact and approximate compressor designs are
utilized, with a focus on efficiently handling constant value "1" and negative
partial products, which frequently appear in the partial product matrices of
signed multipliers. To further enhance efficiency, the lower N - 1 columns of
the partial product matrix are truncated, followed by an error compensation
mechanism. Experimental results show that the proposed 8-bit approximate
multiplier achieves a 29.21% reduction in power delay product (PDP) and a
14.39% reduction in power compared to the best of existing multipliers. The
proposed multiplier is integrated into a custom convolution layer and performs
edge detection, demonstrating its practical utility in real-world applications.

</details>
