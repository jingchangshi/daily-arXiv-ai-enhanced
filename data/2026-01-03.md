<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Enforcing Temporal Constraints for LLM Agents](https://arxiv.org/abs/2512.23738)
*Adharsh Kamath,Sishen Zhang,Calvin Xu,Shubham Ugare,Gagandeep Singh,Sasa Misailovic*

Main category: cs.PL

TL;DR: Agent-C：一个为LLM智能体提供运行时安全保障的框架，确保遵守时序安全策略，实现100%合规和零危害


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在安全关键应用中部署，但现有防护系统无法防止违反时序安全策略的问题，如用户认证前访问敏感数据等需要序列推理的场景。现有方法依赖不精确的自然语言指令或事后监控，无法提供形式化保证。

Method: 提出Agent-C框架：1）引入领域特定语言表达时序属性；2）将规范转换为一阶逻辑；3）使用SMT求解在token生成期间检测不合规动作；4）当LLM尝试生成不合规工具调用时，利用约束生成技术确保所有动作合规，并为不合规动作生成合规替代方案。

Result: 在零售客服和机票预订系统等真实应用中评估，Agent-C实现完美安全性（100%合规，0%危害），同时相比最先进防护系统和无限制智能体提高了任务效用。在Claude Sonnet 4.5上合规率从77.4%提升至100%，GPT-5上从83.7%提升至100%，同时效用分别从71.8%提升至75.2%和66.1%提升至70.6%。

Conclusion: Agent-C为LLM智能体提供了运行时保证，确保遵守形式化时序安全属性，在实现完美安全性的同时提高任务效用，代表了可靠智能体推理的新前沿。

Abstract: LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning.

</details>


### [2] [Towards representation agnostic probabilistic programming](https://arxiv.org/abs/2512.23740)
*Ole Fenske,Maximilian Popko,Sebastian Bader,Thomas Kirste*

Main category: cs.PL

TL;DR: 提出因子抽象作为概率编程的通用接口，支持混合不同表示形式，解决现有工具耦合模型表示与推理算法的问题


<details>
  <summary>Details</summary>
Motivation: 当前概率编程语言和工具将模型表示与特定推理算法紧密耦合，阻碍了对新表示形式或混合离散-连续模型的实验探索

Method: 引入包含五个基本操作的因子抽象，作为操作因子的通用接口，无论其底层表示形式如何

Result: 实现了表示无关的概率编程，用户可以在统一框架内自由混合不同表示形式，支持当前工具无法充分表达的复杂混合模型

Conclusion: 因子抽象为概率编程提供了灵活的通用接口，突破了现有工具在表示形式耦合方面的限制，支持更复杂的混合模型推理

Abstract: Current probabilistic programming languages and tools tightly couple model representations with specific inference algorithms, preventing experimentation with novel representations or mixed discrete-continuous models. We introduce a factor abstraction with five fundamental operations that serve as a universal interface for manipulating factors regardless of their underlying representation. This enables representation-agnostic probabilistic programming where users can freely mix different representations (e.g. discrete tables, Gaussians distributions, sample-based approaches) within a single unified framework, allowing practical inference in complex hybrid models that current toolkits cannot adequately express.

</details>


### [3] [VGC: A High-Performance Zone-Based Garbage Collector Architecture for Python with Partitioning and Parallel Execution](https://arxiv.org/abs/2512.23768)
*Abdulla M*

Main category: cs.PL

TL;DR: VGC提出了一种新颖的双层垃圾回收框架，通过主动层（运行时并发标记清除）和被动层（编译时静态对象优化）相结合，在嵌入式设备到高性能并行架构中实现高效低开销内存管理。


<details>
  <summary>Details</summary>
Motivation: 传统垃圾回收器难以同时满足资源受限的嵌入式设备和高性能并行架构的需求，需要在降低开销、减少暂停时间、优化内存使用等方面实现更好的平衡。

Method: 采用双层架构：Active VGC使用并发标记清除策略管理运行时对象，Passive VGC在编译时通过预测性内存映射优化静态对象分配，将对象对齐到缓存边界以减少碎片化。

Result: 在多线程基准测试中，暂停时间比代际回收器减少30%，内存使用总量减少25%，内存访问模式更可预测，现代并行应用的扩展性得到改善。

Conclusion: VGC通过整合编译时和运行时优化，为从低级到高级编程环境的内存密集型系统提供了强大且适应性强的内存管理解决方案。

Abstract: The Virtual Garbage Collector (VGC) introduces a novel memory management framework designed to optimize performance across diverse systems, ranging from resource constrained embedded devices to high performance parallel architectures. Unlike conventional garbage collectors, VGC employs a dual layer architecture consisting of Active VGC and Passive VGC to enable efficient, low overhead memory management. Active VGC dynamically manages runtime objects using a concurrent mark and sweep strategy tailored for parallel workloads, reducing pause times by up to 30 percent compared to generational collectors in multithreaded benchmarks. Passive VGC operates at compile time and optimizes static object allocation through predictive memory mapping, minimizing fragmentation by aligning objects to cache boundaries. This separation of responsibilities ensures predictable memory access patterns, reduces total memory usage by up to 25 percent, and improves scalability for modern parallel applications. By integrating compile time and runtime optimizations, VGC provides a robust and adaptable solution for memory intensive systems across both low level and high level programming environments.

</details>


### [4] [State Space Estimation for DPOR-based Model Checkers](https://arxiv.org/abs/2512.23996)
*A. R. Balasubramanian,Mohammad Hossein Khoshechin Jorshari,Rupak Majumdar,Umang Mathur,Minjian Zhang*

Main category: cs.PL

TL;DR: 提出首个多项式时间无偏估计器，用于计算并发程序的Mazurkiewicz迹等价类数量，解决基于枚举的模型检查中资源分配问题。


<details>
  <summary>Details</summary>
Motivation: 在并发程序的模型检查中，需要估计迹等价类的数量来预测模型检查的运行时间和评估搜索空间的覆盖程度，但精确计数是#P-难的，且难以近似。

Method: 将无状态最优DPOR算法转换为无偏估计器，将其探索视为有界深度和宽度的树，应用Knuth经典估计器，并通过随机枚举和耦合部分路径演化来控制方差。

Result: 在JMC模型检查器中实现，在共享内存基准测试中，即使状态空间有10^5-10^6个类，通过几百次试验即可获得稳定估计（通常在20%误差范围内）。

Conclusion: 提供了首个可证明的多项式时间无偏估计器，用于计数迹等价类，解决了模型检查资源分配中的重要问题，并能扩展到估计模型检查成本。

Abstract: We study the estimation problem for concurrent programs: given a bounded program $P$, estimate the number of Mazurkiewicz trace-equivalence classes induced by its interleavings. This quantity informs two practical questions for enumeration-based model checking: how long a model checking run is likely to take, and what fraction of the search space has been covered so far. We first show the counting problem is #P-hard even for restricted programs and, unless $P=NP$, inapproximable within any subexponential factor, ruling out efficient exact or randomized approximation algorithms. We give a Monte Carlo approach to obtain a poly-time unbiased estimator: we convert a stateless optimal DPOR algorithm into an unbiased estimator by viewing its exploration as a bounded-depth, bounded-width tree whose leaves are the maximal Mazurkiewicz traces. A classical estimator by Knuth, when run on this tree, yields an unbiased estimate. To control the variance, we apply stochastic enumeration by maintaining a small population of partial paths per depth whose evolution is coupled. We have implemented our estimator in the JMC model checker and evaluated it on shared-memory benchmarks. With modest budgets, our estimator yields stable estimates, typically within a 20% band, within a few hundred trials, even when the state space has $10^5$--$10^6$ classes. We also show how the same machinery estimates model-checking cost by weighting all explored graphs, not only complete traces. Our algorithms provide the first provable poly-time unbiased estimators for counting traces, a problem of considerable importance when allocating model checking resources.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Governing Cloud Data Pipelines with Agentic AI](https://arxiv.org/abs/2512.23737)
*Aswathnarayan Muthukrishnan Kirubakaran,Adithya Parthasarathy,Nitin Saksena,Ram Sekhar Bodala,Akshay Deshpande,Suhas Malempati,Shiva Carimireddy,Abhirup Mazumder*

Main category: cs.DC

TL;DR: 论文提出Agentic Cloud Data Engineering平台，通过集成有界AI代理到云数据管道的治理和控制平面，实现自适应资源重配置、模式协调和自动化故障恢复，相比静态编排减少45%恢复时间、25%运营成本、70%人工干预。


<details>
  <summary>Details</summary>
Motivation: 当前云数据管道面临动态工作负载、模式演进、成本约束和严格治理要求，但大多数生产管道依赖静态配置和反应式操作实践，导致恢复时间长、资源利用率低、人工开销高。

Method: 提出Agentic Cloud Data Engineering架构，集成有界AI代理到云数据管道的治理和控制平面。专用代理分析管道遥测和元数据，基于声明性成本和合规策略进行推理，提出约束性操作行动（如自适应资源重配置、模式协调、自动化故障恢复），所有代理行动都经过治理策略验证以确保可预测和可审计行为。

Result: 使用代表性批处理和流分析工作负载评估，结果显示：相比静态编排，平均管道恢复时间减少45%，运营成本降低约25%，人工干预事件减少70%以上，同时保持数据新鲜度和策略合规性。

Conclusion: 策略有界的代理控制为企业在云环境中治理数据管道提供了有效实用的方法，能够显著改善运营效率、降低成本并减少人工干预。

Abstract: Cloud data pipelines increasingly operate under dynamic workloads, evolving schemas, cost constraints, and strict governance requirements. Despite advances in cloud-native orchestration frameworks, most production pipelines rely on static configurations and reactive operational practices, resulting in prolonged recovery times, inefficient resource utilization, and high manual overhead. This paper presents Agentic Cloud Data Engineering, a policy-aware control architecture that integrates bounded AI agents into the governance and control plane of cloud data pipelines. In Agentic Cloud Data Engineering platform, specialized agents analyze pipeline telemetry and metadata, reason over declarative cost and compliance policies, and propose constrained operational actions such as adaptive resource reconfiguration, schema reconciliation, and automated failure recovery. All agent actions are validated against governance policies to ensure predictable and auditable behavior. We evaluate Agentic Cloud Data Engineering platform using representative batch and streaming analytics workloads constructed from public enterprise-style datasets. Experimental results show that Agentic Cloud Data Engineering platform reduces mean pipeline recovery time by up to 45%, lowers operational cost by approximately 25%, and decreases manual intervention events by over 70% compared to static orchestration, while maintaining data freshness and policy compliance. These results demonstrate that policy-bounded agentic control provides an effective and practical approach for governing cloud data pipelines in enterprise environments.

</details>


### [6] [Squeezing Edge Performance: A Sensitivity-Aware Container Management for Heterogeneous Tasks](https://arxiv.org/abs/2512.23952)
*Yongmin Zhang,Pengyu Huang,Mingyi Dong,Jing Yao*

Main category: cs.DC

TL;DR: 提出基于测量的容器资源管理框架CRMS，通过非线性拟合模型和队列延迟建模，在单边缘服务器上联合优化延迟和能耗，相比基线方法降低延迟14%以上


<details>
  <summary>Details</summary>
Motivation: 边缘计算中任务异构性和资源有限性对高效编排提出挑战，需要针对单边缘服务器上多异构应用的资源管理方案

Method: 1) 通过大量剖析实验建立CPU/内存分配与处理延迟的非线性拟合模型；2) 基于队列延迟公式建立MINLP问题；3) 分解为可处理的凸子问题，采用两阶段容器资源管理方案(CRMS)结合凸优化和贪婪细化

Result: CRMS相比启发式和基于搜索的基线方法，降低延迟超过14%，提高能效，具有多项式时间复杂度和准动态执行能力

Conclusion: 提出的测量驱动容器资源管理框架为动态工作负载特征的异构边缘环境提供了实用、可扩展的解决方案

Abstract: Edge computing enables latency-critical applications to process data close to end devices, yet task heterogeneity and limited resources pose significant challenges to efficient orchestration. This paper presents a measurement-driven, container-based resource management framework for intra-node optimization on a single edge server hosting multiple heterogeneous applications. Extensive profiling experiments are conducted to derive a nonlinear fitting model that characterizes the relationship among CPU/memory allocations and processing latency across diverse workloads, enabling reliable estimation of performance under varying configurations and providing quantitative support for subsequent optimization. Using this model and a queueing-based delay formulation, we formulate a mixed-integer nonlinear programming (MINLP) problem to jointly minimize system latency and power consumption, which is shown to be NP-hard. The problem is decomposed into tractable convex subproblems and solved through a two-stage container-based resource management scheme (CRMS) combining convex optimization and greedy refinement. The proposed scheme achieves polynomial-time complexity and supports quasi-dynamic execution under global resource constraints. Simulation results demonstrate that CRMS reduces latency by over 14\% and improves energy efficiency compared with heuristic and search-based baselines, offering a practical and scalable solution for heterogeneous edge environments with dynamic workload characteristics.

</details>


### [7] [Data Heterogeneity-Aware Client Selection for Federated Learning in Wireless Networks](https://arxiv.org/abs/2512.24286)
*Yanbing Yang,Huiling Zhu,Wenchi Cheng,Jingqing Wang,Changrun Chen,Jiangzhou Wang*

Main category: cs.DC

TL;DR: 提出联合客户端选择与资源分配方案，解决联邦学习中数据异构性导致的泛化误差问题，降低延迟和能耗


<details>
  <summary>Details</summary>
Motivation: 联邦学习在无线网络中的效率受到通信计算资源限制和数据异构性的双重制约，数据异构性会导致重复训练、能耗增加和延迟延长

Method: 首先理论分析客户端数据异构性对全局模型泛化误差的影响，然后构建联合优化问题，提出基于凸优化和松弛技术的客户端选择与资源分配方案

Result: 仿真结果表明，相比不考虑数据异构性的基线方法，所提方案能获得更高的测试精度、更低的训练延迟和更少的能耗

Conclusion: 通过联合优化客户端选择和资源分配，可以有效应对联邦学习中的数据异构性问题，提升系统整体性能

Abstract: Federated Learning (FL) enables mobile edge devices, functioning as clients, to collaboratively train a decentralized model while ensuring local data privacy. However, the efficiency of FL in wireless networks is limited not only by constraints on communication and computational resources but also by significant data heterogeneity among clients, particularly in large-scale networks. This paper first presents a theoretical analysis of the impact of client data heterogeneity on global model generalization error, which can result in repeated training cycles, increased energy consumption, and prolonged latency. Based on the theoretical insights, an optimization problem is formulated to jointly minimize learning latency and energy consumption while constraining generalization error. A joint client selection and resource allocation (CSRA) approach is then proposed, employing a series of convex optimization and relaxation techniques. Extensive simulation results demonstrate that the proposed CSRA scheme yields higher test accuracy, reduced learning latency, and lower energy consumption compared to baseline methods that do not account for data heterogeneity.

</details>


### [8] [PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression](https://arxiv.org/abs/2512.24449)
*Bo Jiang,Taolue Yang,Youyuan Liu,Xubin He,Sheng Di,Sian Jin*

Main category: cs.DC

TL;DR: PackKV是一个针对长上下文生成的KV缓存管理框架，通过创新的有损压缩技术显著减少内存占用并提升执行吞吐量


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型在长上下文推理时面临KV缓存内存占用过大的挑战，随着序列长度和批次大小增加，KV缓存可达数GB，限制了实际应用

Method: PackKV引入专门针对KV缓存数据特性的有损压缩技术，精心设计了压缩算法与系统架构的协同设计，支持KV缓存的动态增长特性，同时保持高计算效率

Result: 在相同精度损失下，相比现有量化方法，PackKV平均实现K缓存153.2%、V缓存179.6%的内存减少率；在A100和RTX Pro 6000 GPU上，相比cuBLAS矩阵向量乘法内核，平均吞吐量提升K缓存75.7%、V缓存171.7%

Conclusion: PackKV是一个高效通用的KV缓存管理框架，通过创新的压缩技术有效解决了长上下文生成中的内存瓶颈问题，同时显著提升了执行性能

Abstract: Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \textbf{153.2}\% higher memory reduction rate for the K cache and \textbf{179.6}\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \textbf{75.7}\% for K and \textbf{171.7}\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV

</details>


### [9] [Understanding LLM Checkpoint/Restore I/O Strategies and Patterns](https://arxiv.org/abs/2512.24511)
*Mikaila J. Gossman,Avinash Maurya,Bogdan Nicolae,Jon C. Calhoun*

Main category: cs.DC

TL;DR: 论文研究了LLM大规模训练中检查点/恢复的I/O性能问题，通过liburing等内核加速I/O库优化，实现了比现有方案最高7.6倍的吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 随着LLM和基础模型规模扩大，检查点/恢复成为训练和推理的关键模式。在3D并行（张量、流水线、数据）环境下，检查点涉及大量进程管理各种形状大小的张量，需要频繁持久化到稳定存储，这变成了一个具有数据量大、种类多、速度要求高的I/O问题。现有方案在并发下存在性能瓶颈，而liburing等内核加速I/O库的有效性尚未充分探索。

Method: 开发微基准测试来量化使用liburing时的权衡，评估聚合、对齐和I/O合并在不同I/O模式下的交互效果。研究文件系统感知的聚合策略，分析缓冲I/O和直接I/O的性能差异。

Result: 未合并的小缓冲区操作使吞吐量相比合成工作负载减半，而文件系统感知的聚合恢复了带宽并减少了元数据开销。相比最先进的LLM检查点引擎，该方法实现了比DataStates-LLM高3.9倍的写入吞吐量，比TorchSnapshot高7.6倍。

Conclusion: 研究结果表明，需要与现代文件系统和I/O后端对齐的聚合和合并策略。内核加速I/O库如liburing在优化LLM检查点性能方面具有显著潜力，文件系统感知的聚合是提升吞吐量的关键。

Abstract: As LLMs and foundation models scale, checkpoint/restore has become a critical pattern for training and inference. With 3D parallelism (tensor, pipeline, data), checkpointing involves many processes, each managing numerous tensors of varying shapes and sizes, that must be persisted frequently to stable storage (e.g., parallel file systems). This turns checkpoint/restore into a big-data I/O problem characterized by volume, variety, and velocity. The workflow must traverse the full storage stack -- from GPU memory through host memory and local storage to external repositories -- whose tiers differ by orders of magnitude in performance, creating bottlenecks under concurrency even with asynchronous flush/prefetch. Kernel-accelerated I/O libraries such as \texttt{liburing} may mitigate these issues versus POSIX, but their effectiveness for LLM checkpointing remains underexplored. We develop microbenchmarks to quantify trade-offs when using \texttt{liburing}, evaluating how aggregation, alignment, and I/O coalescing interact under buffered and direct I/O. We find that uncoalesced small-buffer operations halve throughput relative to synthetic workloads, while file system-aware aggregation restores bandwidth and reduces metadata overhead. Compared to state-of-the-art LLM checkpointing engines, our approach achieves up to $3.9\times$ higher write throughput than DataStates-LLM and $7.6\times$ higher than TorchSnapshot. These results highlight the need for aggregation and coalescing strategies that align with modern file systems and I/O backends.

</details>


### [10] [Distributed Bilevel Optimization with Dual Pruning for Resource-limited Clients](https://arxiv.org/abs/2512.24667)
*Mingyi Li,Xiao Zhang,Ruisheng Zheng,Hongjian Shi,Yuan Yuan,Xiuzhen Cheng,Dongxiao Yu*

Main category: cs.DC

TL;DR: 提出首个资源自适应的分布式双层优化框架，通过二阶自由超梯度估计器，让客户端根据可用资源优化子模型，达到渐近最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 随着大规模模型的发展，传统分布式双层优化算法无法直接应用于低资源客户端，主要原因是优化上下层函数涉及过多计算。

Method: 提出资源自适应分布式双层优化框架（RABO和RAFBO），采用二阶自由超梯度估计器，允许客户端根据可用资源优化子模型。

Result: 理论证明RABO和RAFBO都能达到渐近最优收敛率O(1/√(C_x*Q))，其中C_x*是外参数的最小覆盖范围。在两个不同任务上的实验证明了方法的有效性和计算效率。

Conclusion: 提出的资源自适应分布式双层优化框架解决了传统算法在低资源环境下的适用性问题，通过资源自适应设计和二阶自由超梯度估计实现了高效优化。

Abstract: With the development of large-scale models, traditional distributed bilevel optimization algorithms cannot be applied directly in low-resource clients. The key reason lies in the excessive computation involved in optimizing both the lower- and upper-level functions. Thus, we present the first resource-adaptive distributed bilevel optimization framework with a second-order free hypergradient estimator, which allows each client to optimize the submodels adapted to the available resources. Due to the coupled influence of partial outer parameters x and inner parameters y, it's challenging to theoretically analyze the upper bound regarding the globally averaged hypergradient for full model parameters. The error bound of inner parameter also needs to be reformulated since the local partial training. The provable theorems show that both RABO and RAFBO can achieve an asymptotically optimal convergence rate of $O(1/\sqrt{C_x^{\ast}Q})$, which is dominated by the minimum coverage of the outer parameter $C_x^{\ast}$. Extensive experiments on two different tasks demonstrate the effectiveness and computation efficiency of our proposed methods.

</details>


### [11] [AI-Driven Cloud Resource Optimization for Multi-Cluster Environments](https://arxiv.org/abs/2512.24914)
*Vinoth Punniyamoorthy,Akash Kumar Agarwal,Bikesh Kumar,Abhirup Mazumder,Kabilan Kannan,Sumit Saha*

Main category: cs.DC

TL;DR: AI驱动的多集群云系统自适应资源优化框架，通过预测学习和策略感知决策实现主动协调的资源管理


<details>
  <summary>Details</summary>
Motivation: 现代云原生系统依赖多集群部署实现可扩展性、弹性和地理分布，但现有资源管理方法主要是反应式和集群中心的，无法在动态负载下优化系统范围行为，导致资源利用率低、适应延迟和运维开销增加

Method: 提出AI驱动的自适应资源优化框架，集成预测学习、策略感知决策和持续反馈，通过分析跨集群遥测数据和历史执行模式，动态调整资源分配以平衡性能、成本和可靠性目标

Result: 原型实现显示相比传统反应式方法，提高了资源效率、在负载波动期间更快稳定、减少了性能变异性

Conclusion: 智能自适应的基础设施管理是构建可扩展和弹性云平台的关键使能技术

Abstract: Modern cloud-native systems increasingly rely on multi-cluster deployments to support scalability, resilience, and geographic distribution. However, existing resource management approaches remain largely reactive and cluster-centric, limiting their ability to optimize system-wide behavior under dynamic workloads. These limitations result in inefficient resource utilization, delayed adaptation, and increased operational overhead across distributed environments. This paper presents an AI-driven framework for adaptive resource optimization in multi-cluster cloud systems. The proposed approach integrates predictive learning, policy-aware decision-making, and continuous feedback to enable proactive and coordinated resource management across clusters. By analyzing cross-cluster telemetry and historical execution patterns, the framework dynamically adjusts resource allocation to balance performance, cost, and reliability objectives. A prototype implementation demonstrates improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional reactive approaches. The results highlight the effectiveness of intelligent, self-adaptive infrastructure management as a key enabler for scalable and resilient cloud platforms.

</details>


### [12] [Reliable and Resilient Collective Communication Library for LLM Training and Serving](https://arxiv.org/abs/2512.25059)
*Wei Wang,Nengneng Yu,Sixian Xiong,Zaoxing Liu*

Main category: cs.DC

TL;DR: R²CCL是一个容错的通信库，利用多NIC硬件实现无损、低开销的故障转移，显著减少GPU因网络故障造成的浪费。


<details>
  <summary>Details</summary>
Motivation: 现代ML训练和推理扩展到成千上万个GPU，网络故障会导致10-15%的GPU时间浪费。常见的网络错误和链路波动会触发超时，导致整个作业终止，在训练中需要昂贵的检查点回滚，在推理中需要请求重处理。

Method: R²CCL通过利用多NIC硬件实现容错通信，包括快速连接迁移、带宽感知负载重分配和弹性集体算法，以在故障下保持进度。

Result: 在8-GPU H100 InfiniBand服务器和大规模ML模拟器上的实验表明，R²CCL对NIC故障具有高度鲁棒性，训练开销小于1%，推理开销小于3%。R²CCL分别优于基线AdapCC和DejaVu 12.18倍和47倍。

Conclusion: R²CCL提供了一个有效的解决方案，通过硬件冗余和智能故障转移机制，显著减少大规模ML系统中网络故障造成的GPU时间浪费。

Abstract: Modern ML training and inference now span tens to tens of thousands of GPUs, where network faults can waste 10--15\% of GPU hours due to slow recovery. Common network errors and link fluctuations trigger timeouts that often terminate entire jobs, forcing expensive checkpoint rollback during training and request reprocessing during inference. We present R$^2$CCL, a fault-tolerant communication library that provides lossless, low-overhead failover by exploiting multi-NIC hardware. R$^2$CCL performs rapid connection migration, bandwidth-aware load redistribution, and resilient collective algorithms to maintain progress under failures. We evaluate R$^2$CCL on two 8-GPU H100 InfiniBand servers and via large-scale ML simulators modeling hundreds of GPUs with diverse failure patterns. Experiments show that R$^2$CCL is highly robust to NIC failures, incurring less than 1\% training and less than 3\% inference overheads. R$^2$CCL outperforms baselines AdapCC and DejaVu by 12.18$\times$ and 47$\times$, respectively.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [HERO-Sign: Hierarchical Tuning and Efficient Compiler-Time GPU Optimizations for SPHINCS+ Signature Generation](https://arxiv.org/abs/2512.23969)
*Yaoyun Zhou,Qian Wang*

Main category: cs.AR

TL;DR: HERO Sign：基于GPU加速的SPHINCS+签名方案，通过层次化调优和编译器优化实现显著性能提升


<details>
  <summary>Details</summary>
Motivation: SPHINCS+作为后量子安全的无状态哈希签名方案，签名生成速度慢，现有GPU优化未能充分利用其Merkle树结构的并行性，缺乏细粒度的编译器级定制

Method: 采用层次化调优和高效编译时优化，包括：1) Tree Fusion策略处理FORS的大量独立分支，2) 自动Tree Tuning搜索算法适配不同GPU架构，3) 自适应编译策略根据SPHINCS+内核选择PTX或本地代码路径，4) 基于任务图的批处理优化减少多流空闲时间和内核启动开销

Result: 在RTX 4090上，相比现有GPU实现，SPHINCS+ 128f/192f/256f参数集的吞吐量分别提升1.28-3.13、1.28-2.92、1.24-2.60倍；在A100、H100、GTX 2080上也有类似提升，内核启动延迟降低两个数量级

Conclusion: HERO Sign通过系统化的GPU优化策略，显著提升了SPHINCS+签名性能，为后量子密码学的高效实现提供了有效解决方案

Abstract: SPHINCS+ is a stateless hash-based signature scheme that provides strong post quantum security, but its signature generation is slow due to intensive hash computations. GPUs offer massive parallelism that can potentially accelerate SPHINCS+ signatures. However, existing GPU-based optimizations either fail to fully exploit the inherent parallelism of SPHINCS+'s Merkle tree structure or lack fine-grained, compiler-level customization across its diverse computational kernels. This paper proposes HERO Sign, a GPU-accelerated SPHINCS+ implementation that adopts hierarchical tuning and efficient compiler time optimizations. HERO Sign reexamines the parallelization opportunities enabled by data independence across SPHINCS+ components, including FORS, MSS, and WOTS+. It introduces a Tree Fusion strategy for FORS, which contains a large number of independent branches. The fusion strategy is guided by an automated Tree Tuning search algorithm that adapts fusion schemes to different GPU architectures. To further improve performance, HERO Sign employs an adaptive compilation strategy that accounts for the varying effectiveness of compiler optimizations across SPHINCS+ kernels such as FORS Sign, TREE Sign, and WOTS+ Sign. During compilation, the strategy automatically selects between PTX and native code paths to maximize efficiency. For batched signature generation, HERO Sign optimizes kernel-level overlapping using a task graph-based construction to reduce multi-stream idle time and kernel launch overhead. Experimental results show that, compared to state of the art GPU implementations, HERO Sign achieves throughput improvements of 1.28-3.13, 1.28-2.92, and 1.24-2.60 under the SPHINCS+ 128f, 192f, and 256f parameter sets on RTX 4090. Similar gains are observed on A100, H100, and GTX 2080, along with a two orders of magnitude reduction in kernel launch latency.

</details>
