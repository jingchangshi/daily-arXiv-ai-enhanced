<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Macro-embedding Compiler Intermediate Languages in Racket](https://arxiv.org/abs/2509.19607)
*William J. Bowman*

Main category: cs.PL

TL;DR: 本文介绍了一种在Racket中嵌入编译器中间语言家族的宏嵌入设计和实现，用于编译器课程的测试框架，实现从Scheme类语言到x86-64的完整编译链。


<details>
  <summary>Details</summary>
Motivation: 旨在展示面向语言的技术和抽象，用于实现(1)大型语言家族和(2)高低级语言之间的互操作性，强调代码重用和互操作性。

Method: 通过局部宏展开到单一宿主语言中，实现语言特征的模块化和组合性，而不是实现由封闭特征集预定义的语言。

Result: 实现了安全函数抽象与不安全汇编特征及其在中间阶段的交互，支持从宿主语言和中间语言之间的重用，简化了中间语言语义的开发。

Conclusion: 该方法促进了中间语言中单个语言特征的扩展或重定义，并暴露了嵌入语言的多个接口，展示了语言导向编程在编译器实现中的优势。

Abstract: We present the design and implementation of a macro-embedding of a family of
compiler intermediate languages, from a Scheme-like language to x86-64, into
Racket. This embedding is used as part of a testing framework for a compilers
course to derive interpreters for all the intermediate languages. The embedding
implements features including safe, functional abstractions as well as unsafe
assembly features, and the interactions between the two at various intermediate
stages.
  This paper aims to demonstrate language-oriented techniques and abstractions
for implementing (1) a large family of languages and (2) interoperability
between low- and high-level languages. The primary strength of this approach is
the high degree of code reuse and interoperability compared to implementing
each interpreter separately. The design emphasizes modularity and
compositionality of an open set of language features by local macro expansion
into a single host language, rather than implementing a language pre-defined by
a closed set of features. This enables reuse from both the host language
(Racket) and between intermediate languages, and enables interoperability
between high- and low-level features, simplifying development of the
intermediate language semantics. It also facilitates extending or redefining
individual language features in intermediate languages, and exposing multiple
interfaces to the embedded languages.

</details>


### [2] [Compilation as Multi-Language Semantics](https://arxiv.org/abs/2509.19613)
*William J. Bowman*

Main category: cs.PL

TL;DR: 提出一种统一的多语言语义方法，将编译器建模为开放项的归约系统，而非语法翻译，从而同时定义编译器和互操作性语义。


<details>
  <summary>Details</summary>
Motivation: 现有多语言语义方法需要为每个编译通道定义两个变体：用于建模编译的开放项语法翻译，以及用于建模互操作性的多语言边界封闭项运行时翻译，存在重复工作。

Method: 将编译器完全建模为多语言语义中开放项的归约系统，通过跨语言redex的归一化实现AOT编译，通过多语言求值实现JIT编译。

Result: 该方法减少了重复定义，提供了语义洞察，多语言归约的汇合性隐含编译器正确性，主题归约隐含类型保持性。

Conclusion: 统一的多语言归约方法能够同时定义编译和互操作性，简化证明过程，为安全编译验证提供新视角。

Abstract: Modeling interoperability between programs in different languages is a key
problem when modeling verified and secure compilation, which has been
successfully addressed using multi-language semantics. Unfortunately, existing
models of compilation using multi-language semantics define two variants of
each compiler pass: a syntactic translation on open terms to model compilation,
and a run-time translation of closed terms at multi-language boundaries to
model interoperability.
  In this talk, I discuss work-in-progress approach to uniformly model a
compiler entirely as a reduction system on open term in a multi-language
semantics, rather than as a syntactic translation. This simultaneously defines
the compiler and the interoperability semantics, reducing duplication. It also
provides interesting semantic insights. Normalization of the cross-language
redexes performs ahead-of-time (AOT) compilation. Evaluation in the
multi-language models just-in-time (JIT) compilation. Confluence of
multi-language reduction implies compiler correctness, and part of the secure
compilation proof (full abstraction), enabling focus on the difficult part of
the proof. Subject reduction of the multi-language reduction implies
type-preservation of the compiler.

</details>


### [3] [The Syntax and Semantics of einsum](https://arxiv.org/abs/2509.20020)
*Maurice Wenig,Paul G. Rump,Mark Blacher,Joachim Giesen*

Main category: cs.PL

TL;DR: 本文为einsum符号提供了理论基础，定义了einsum语言并证明了张量表达式的重要等价规则


<details>
  <summary>Details</summary>
Motivation: einsum符号虽然在实际应用中很成功，但缺乏坚实的理论基础，且在不同框架中不统一，限制了形式化推理和系统优化的机会

Method: 讨论张量表达式的术语，提供einsum语言的形式化定义，基于此定义形式化并证明张量表达式的重要等价规则

Result: 建立了einsum符号的理论基础，证明了重要的等价规则，并展示了这些规则在实际应用中的相关性

Conclusion: 这项工作为einsum符号提供了形式化基础，有助于促进形式化推理和系统优化

Abstract: In 2011, einsum was introduced to NumPy as a practical and convenient
notation for tensor expressions in machine learning, quantum circuit
simulation, and other fields. It has since been implemented in additional
Python frameworks such as PyTorch and TensorFlow, as well as in other
programming languages such as Julia. Despite its practical success, the einsum
notation still lacks a solid theoretical basis, and is not unified across the
different frameworks, limiting opportunities for formal reasoning and
systematic optimization. In this work, we discuss the terminology of tensor
expressions and provide a formal definition of the einsum language. Based on
this definition, we formalize and prove important equivalence rules for tensor
expressions and highlight their relevance in practical applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Investigating Sharding Advancements, Methodologies, and Adoption Potential in Hedera](https://arxiv.org/abs/2509.19478)
*Ziwei Wang,Cong Wu,Paolo Tasca*

Main category: cs.DC

TL;DR: 本文研究了分片技术在Hedera区块链中的应用，提出了一种混合分片解决方案，通过将网络划分为本地和全局委员会来提高交易吞吐量、降低延迟并优化资源使用。


<details>
  <summary>Details</summary>
Motivation: 解决区块链网络面临的可扩展性挑战，提高交易吞吐量、降低延迟并优化资源使用，特别是在Hedera这种采用Gossip about Gossip协议和异步拜占庭容错（ABFT）的分布式账本技术中。

Method: 提出了一种混合分片解决方案，将网络划分为本地和全局委员会，实现高效的跨分片交易，并通过动态重配置确保强大的安全性。

Result: 分析显示存储和通信开销显著减少，可扩展性得到改善，容错能力增强，证明了将分片技术集成到Hedera架构中的可行性和优势。

Conclusion: 分片技术可以有效地集成到Hedera的架构中，显著提高其性能、可扩展性和安全性，为区块链网络的可扩展性挑战提供了可行的解决方案。

Abstract: Sharding has emerged as a critical solution to address the scalability
challenges faced by blockchain networks, enabling them to achieve higher
transaction throughput, reduced latency, and optimized resource usage. This
paper investigates the advancements, methodologies, and adoption potential of
sharding in the context of Hedera, a distributed ledger technology known for
its unique Gossip about Gossip protocol and asynchronous Byzantine Fault
Tolerance (ABFT). We explore various academic and industrial sharding
techniques, emphasizing their benefits and trade-offs. Building on these
insights, we propose a hybrid sharding solution for Hedera that partitions the
network into local and global committees, facilitating efficient cross-shard
transactions and ensuring robust security through dynamic reconfiguration. Our
analysis highlights significant reductions in storage and communication
overhead, improved scalability, and enhanced fault tolerance, demonstrating the
feasibility and advantages of integrating sharding into Hedera's architecture.

</details>


### [5] [To Stream or Not to Stream: Towards A Quantitative Model for Remote HPC Processing Decisions](https://arxiv.org/abs/2509.19532)
*Flavio Castro,Weijian Zheng,Joaquin Chung,Ian Foster,Rajkumar Kettimuthu*

Main category: cs.DC

TL;DR: 本文提出了一个定量框架和Streaming Speed Score来评估远程HPC资源是否能够比本地替代方案提供及时的数据处理，特别针对高数据生成率的科学仪器场景。


<details>
  <summary>Details</summary>
Motivation: 现代科学仪器生成数据的速度超过了本地计算能力，而基于文件的传输方式在时间敏感的分析和实验引导中变得不实用。实时流式处理框架承诺降低延迟和提高系统效率，但缺乏评估其可行性的原则性方法。

Method: 开发了一个定量框架，包含数据生成率、传输效率、远程处理能力和文件I/O开销等关键参数，计算总处理完成时间，并识别流式处理有利的操作区域。

Result: 测量显示，在高数据速率下，流式处理可以实现比基于文件的方法低97%的端到端完成时间，但最坏情况下的拥塞可能使传输时间增加一个数量级以上。

Conclusion: 流式处理在高数据速率场景下具有显著优势，但尾部延迟在流式处理可行性决策中至关重要，需要综合考虑各种操作条件。

Abstract: Modern scientific instruments generate data at rates that increasingly exceed
local compute capabilities and, when paired with the staging and I/O overheads
of file-based transfers, also render file-based use of remote HPC resources
impractical for time-sensitive analysis and experimental steering. Real-time
streaming frameworks promise to reduce latency and improve system efficiency,
but lack a principled way to assess their feasibility. In this work, we
introduce a quantitative framework and an accompanying Streaming Speed Score to
evaluate whether remote high-performance computing (HPC) resources can provide
timely data processing compared to local alternatives. Our model incorporates
key parameters including data generation rate, transfer efficiency, remote
processing power, and file input/output overhead to compute total processing
completion time and identify operational regimes where streaming is beneficial.
We motivate our methodology with use cases from facilities such as APS, FRIB,
LCLS-II, and the LHC, and validate our approach through an illustrative case
study based on LCLS-II data. Our measurements show that streaming can achieve
up to 97% lower end-to-end completion time than file-based methods under high
data rates, while worst-case congestion can increase transfer times by over an
order of magnitude, underscoring the importance of tail latency in streaming
feasibility decisions.

</details>


### [6] [A Survey of Recent Advancements in Secure Peer-to-Peer Networks](https://arxiv.org/abs/2509.19539)
*Raj Patel,Umesh Biswas,Surya Kodipaka,Will Carroll,Preston Peranich,Maxwell Young*

Main category: cs.DC

TL;DR: 本文对P2P网络安全的最新理论进展进行了更新综述，重点关注经典威胁（如Sybil攻击和路由攻击）的解决方案，并分析了机器学习、社交网络和动态系统等新兴趋势带来的新挑战和新方法。


<details>
  <summary>Details</summary>
Motivation: P2P网络是现代计算的基石，其安全性是一个活跃的研究领域。虽然已经提出了许多具有强大安全保证的防御措施，但最近的调查已有十多年历史，需要更新综述以反映最新进展。

Method: 通过系统性文献回顾和分析，评估针对经典威胁（Sybil攻击、路由攻击）的解决方案，并研究新兴趋势（机器学习、社交网络、动态系统）如何影响P2P网络安全。

Result: 识别了各种解决方案的优势和局限性，揭示了新兴技术带来的新挑战，同时也推动了新颖安全方法的发展。

Conclusion: 本文不仅总结了当前P2P网络安全的研究现状，还指出了未来研究的方向，为领域发展提供了重要指导。

Abstract: Peer-to-peer (P2P) networks are a cornerstone of modern computing, and their
security is an active area of research. Many defenses with strong security
guarantees have been proposed; however, the most-recent survey is over a decade
old. This paper delivers an updated review of recent theoretical advances that
address classic threats, such as the Sybil and routing attacks, while
highlighting how emerging trends -- such as machine learning, social networks,
and dynamic systems -- pose new challenges and drive novel solutions. We
evaluate the strengths and weaknesses of these solutions and suggest directions
for future research.

</details>


### [7] [Characterizing Adaptive Mesh Refinement on Heterogeneous Platforms with Parthenon-VIBE](https://arxiv.org/abs/2509.19701)
*Akash Poptani,Alireza Khadem,Scott Mahlke,Jonah Miller,Joshua Dolence,Reetuparna Das*

Main category: cs.DC

TL;DR: 分析Parthenon（块结构自适应网格细化基准测试）在CPU-GPU系统上的性能表现，发现小网格块和深AMR层级会降低GPU性能，并提出优化建议


<details>
  <summary>Details</summary>
Motivation: 随着异构超级计算机的发展，需要了解自适应网格细化（AMR）在CPU-GPU系统上的性能特征，为美国能源部即将推出的异构超级计算机提供部署指导

Method: 通过详细性能分析，识别GPU性能瓶颈，包括通信开销、串行开销、GPU利用率低、占用率不足和内存访问瓶颈，并分析等级可扩展性和内存约束

Result: 发现较小的网格块和较深的AMR层级会显著降低GPU性能，主要由于通信增加、串行开销和GPU利用率低下

Conclusion: 提出了优化建议以提高GPU吞吐量和减少内存占用，这些见解可为未来在异构超级计算机上的AMR部署提供指导

Abstract: Hero-class HPC simulations rely on Adaptive Mesh Refinement (AMR) to reduce
compute and memory demands while maintaining accuracy. This work analyzes the
performance of Parthenon, a block-structured AMR benchmark, on CPU-GPU systems.
We show that smaller mesh blocks and deeper AMR levels degrade GPU performance
due to increased communication, serial overheads, and inefficient GPU
utilization. Through detailed profiling, we identify inefficiencies, low
occupancy, and memory access bottlenecks. We further analyze rank scalability
and memory constraints, and propose optimizations to improve GPU throughput and
reduce memory footprint. Our insights can inform future AMR deployments on
Department of Energy's upcoming heterogeneous supercomputers.

</details>


### [8] [Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient LLM Inference](https://arxiv.org/abs/2509.19729)
*Haoyu Chen,Xue Li,Kun Qian,Yu Guan,Jin Zhao,Xin Wang*

Main category: cs.DC

TL;DR: Gyges提出跨实例并行转换方法，通过动态调整运行实例的并行策略来适应请求的动态变化，包括KV缓存转换、权重转换和转换感知调度器，显著提升LLM服务吞吐量。


<details>
  <summary>Details</summary>
Motivation: LLM服务中处理请求动态变化（特别是上下文长度变化）存在内在权衡：使用张量并行等策略可以协调多个GPU处理更大上下文，但会降低整体吞吐量。

Method: 设计(1)页面友好的头中心布局加速KV缓存转换；(2)专用权重填充加速模型权重转换；(3)转换感知调度器协同调度请求和并行转换。

Result: 使用真实世界trace评估显示，Gyges相比最先进解决方案将吞吐量提升1.75倍到6.57倍。

Conclusion: Gyges通过动态并行策略转换有效解决了LLM服务中上下文长度变化带来的吞吐量瓶颈问题。

Abstract: Efficiently processing the dynamics of requests, especially the context
length variance, is important in Large Language Model (LLM) serving scenarios.
However, there is an intrinsic trade-off: while leveraging parallelism
strategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to
accommodate larger context lengths, it inevitably results in degraded overall
throughput. In this paper, we propose Cross-Instance Parallelism Transformation
(Gyges), which adaptively adjusts the parallelism strategies of running
instances to align with the dynamics of incoming requests. We design (1) a
page-friendly, header-centric layout to accelerate KV cache transformations;
(2) dedicated weight padding to accelerate model weight transformations; and
(3) a transformation-aware scheduler to cooperatively schedule requests and
parallelism transformations, optimizing the overall performance. Evaluations
using real-world traces show that Gyges improves throughput by 1.75x-6.57x
compared to state-of-the-art solutions.

</details>


### [9] [BurstEngine: an Efficient Distributed Framework for Training Transformers on Extremely Long Sequences of over 1M Tokens](https://arxiv.org/abs/2509.19836)
*Ao Sun,Weilin Zhao,Xu Han,Cheng Yang,Zhiyuan Liu,Chuan Shi,Maosong sun*

Main category: cs.DC

TL;DR: BurstEngine是一个高效训练LLM长序列数据的框架，通过BurstAttention降低通信成本，结合序列级选择性检查点和负载均衡优化，在超过100万token的超长序列训练中实现1.2倍加速和更低内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Tensor Parallelism和Context Parallelism在序列长度超过100万token时模型FLOPs利用率低，特别是GPU数量增加时效率下降明显。

Method: 提出BurstAttention优化分布式注意力机制，利用拓扑感知环形通信和细粒度通信计算重叠；引入序列级选择性检查点、语言建模头与损失函数融合、注意力掩码负载均衡优化。

Result: 在训练超过100万token的超长序列时，相比最先进基线方法实现1.2倍加速，内存开销显著降低。

Conclusion: BurstEngine通过综合优化有效解决了长序列训练中的效率和内存问题，代码已在GitHub开源。

Abstract: Existing methods for training LLMs on long-sequence data, such as Tensor
Parallelism and Context Parallelism, exhibit low Model FLOPs Utilization as
sequence lengths and number of GPUs increase, especially when sequence lengths
exceed 1M tokens. To address these challenges, we propose BurstEngine, an
efficient framework designed to train LLMs on long-sequence data. BurstEngine
introduces BurstAttention, an optimized distributed attention with lower
communication cost than RingAttention. BurstAttention leverages topology-aware
ring communication to fully utilize network bandwidth and incorporates
fine-grained communication-computation overlap. Furthermore, BurstEngine
introduces sequence-level selective checkpointing and fuses the language
modeling head with the loss function to reduce memory cost. Additionally,
BurstEngine introduces workload balance optimization for various types of
attention masking. By integrating these optimizations, BurstEngine achieves a
$1.2\times$ speedup with much lower memory overhead than the state-of-the-art
baselines when training LLMs on extremely long sequences of over 1M tokens. We
have made our code publicly available on GitHub:
https://github.com/thunlp/BurstEngine.

</details>


### [10] [Characterizing the Performance of Accelerated Jetson Edge Devices for Training Deep Learning Models](https://arxiv.org/abs/2509.20160)
*Prashanthi S. K.,Sai Anuroop Kesanapalli,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文对边缘设备上的DNN训练进行了系统性研究，分析了不同Jetson设备在训练参数变化下的性能表现，并建立了预测模型来优化训练时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 随着联邦学习的发展和边缘设备计算能力的提升，边缘设备上的DNN训练需求日益增长，但现有研究对此关注不足。边缘设备与服务器在工作站资源特性上存在显著差异，需要专门的性能分析。

Method: 在三种Jetson设备（AGX Xavier、Xavier NX和Nano）上对三种不同的DNN模型-数据集组合进行训练实验，系统性地调整I/O流水线、存储介质、批大小和功耗模式等参数。

Result: 研究揭示了资源间的相互依赖关系和反直觉的发现，量化了已知经验。通过实验结果建立了简单的预测模型，能够以最少的额外分析预测不同功耗模式下的训练时间和能耗。

Conclusion: 该研究为优化边缘设备上的训练性能、在受限设备上权衡时间和能耗提供了指导，并有助于为特定DNN工作负载选择理想的边缘硬件，未来可扩展到联邦学习场景。

Abstract: Deep Neural Networks (DNNs) have had a significant impact on domains like
autonomous vehicles and smart cities through low-latency inferencing on edge
computing devices close to the data source. However, DNN training on the edge
is poorly explored. Techniques like federated learning and the growing capacity
of GPU-accelerated edge devices like NVIDIA Jetson motivate the need for a
holistic characterization of DNN training on the edge. Training DNNs is
resource-intensive and can stress an edge's GPU, CPU, memory and storage
capacities. Edge devices also have different resources compared to workstations
and servers, such as slower shared memory and diverse storage media. Here, we
perform a principled study of DNN training on individual devices of three
contemporary Jetson device types: AGX Xavier, Xavier NX and Nano for three
diverse DNN model--dataset combinations. We vary device and training parameters
such as I/O pipelining and parallelism, storage media, mini-batch sizes and
power modes, and examine their effect on CPU and GPU utilization, fetch stalls,
training time, energy usage, and variability. Our analysis exposes several
resource inter-dependencies and counter-intuitive insights, while also helping
quantify known wisdom. Our rigorous study can help tune the training
performance on the edge, trade-off time and energy usage on constrained
devices, and even select an ideal edge hardware for a DNN workload, and, in
future, extend to federated learning too. As an illustration, we use these
results to build a simple model to predict the training time and energy per
epoch for any given DNN across different power modes, with minimal additional
profiling.

</details>


### [11] [Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge Accelerators](https://arxiv.org/abs/2509.20189)
*Prashanthi S. K.,Kunal Kumar Sahoo,Amartya Ranjan Saikia,Pranav Gupta,Atharva Vinay Joshi,Priyanshu Pansari,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文开发了Jetson Orin AGX边缘加速器的时间屋顶线和能量屋顶线模型，结合DNN推理工作负载的分析模型，揭示了功率模式的独特性能特征，并实现了最高15%的能耗优化。


<details>
  <summary>Details</summary>
Motivation: 边缘加速器如Nvidia Jetson拥有2000+ CUDA核心和数千种功率模式，但缺乏对其功率-性能权衡原理的系统研究，需要从第一性原理分析DNN工作负载在边缘设备上的行为。

Method: 开发时间屋顶线和新型能量屋顶线模型，结合DNN推理工作负载的计算(FLOP)和内存访问(字节)分析模型，对Jetson Orin AGX的不同功率模式进行第一性原理分析。

Result: 揭示了边缘加速器功率性能行为的独特见解：默认MAXN功率模式并非最节能，时间效率在所有功率模式下都意味着能量效率。通过功率模式调优可实现最高15%的能耗降低，且推理时间退化最小。

Conclusion: 屋顶线模型为理解边缘加速器性能提供了理论基础，功率模式调优能显著优化DNN推理的延迟和能耗，该方法可扩展到DNN训练场景。

Abstract: Edge accelerators such as Nvidia Jetsons are becoming an integral part of the
computing continuum, and are often used for DNN inferencing and training.
Nvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power
envelope and offer $1000$s of power modes to customize CPU, GPU and memory
frequencies. Their widely varying power--performance trade-offs can be
exploited for energy and power-constrained deployments. While data-driven
methods to predict the power and latency of DNN workloads for edge devices
exist, there is a lack of principled study to understand why edge accelerators
and their power modes perform the way they do. We develop a time roofline and a
novel energy roofline model for the Jetson Orin AGX for diverse power modes,
and couple it with an analytical model of the compute (FLOP) and memory access
(bytes) for DNN inference workloads to analyze them from first principles.
These reveal unique, sometimes counter-intuitive, insights into the power and
performance behavior of DNN workloads on edge accelerators, e.g., the default
power mode MAXN is not the most energy efficient and time efficiency implies
energy efficiency for all power modes. We also extend our analytical roofline
models to DNN training. Finally, we apply these methods to tune the power mode
(and hence the roofline) of the edge device to optimize the latency and energy
for DNN inference, with up to $15\%$ lower energy and minimal degradation in
inference time.

</details>


### [12] [Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge Accelerators](https://arxiv.org/abs/2509.20205)
*Prashanthi S. K.,Saisamarth Taluri,Pranav Gupta,Amartya Ranjan Saikia,Kunal Kumar Sahoo,Atharva Vinay Joshi,Lakshya Karwa,Kedar Dhule,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 本文提出了一种智能时间切片方法GMD和ALS，用于在Jetson等边缘设备上优化并发DNN训练和推理的性能，在有限的分析成本下实现训练吞吐量最大化并满足延迟和功耗约束。


<details>
  <summary>Details</summary>
Motivation: 随着GPU加速边缘设备的普及和隐私问题的增加，需要在边缘设备上同时进行DNN训练和推理。但边缘加速器不支持原生GPU共享且具有数千种功耗模式，需要智能的时间共享策略来满足功耗-性能目标。

Method: 设计了Fulcrum调度器，提出GMD（高效多维梯度下降搜索）仅分析15种功耗模式，以及ALS（主动学习技术）识别可重用的帕累托最优功耗模式但分析50-150种模式。通过优化问题决定训练和推理小批量的交错方式、设备功耗模式和推理小批量大小。

Result: 在15个DNN工作负载的273,000+配置中评估，ALS和GMD优于简单和复杂的基线方法。解决方案在97%以上的运行中满足延迟和功耗预算，平均吞吐量接近最优解的7%以内。

Conclusion: 提出的智能时间切片方法能够在有限的性能分析成本下，有效优化边缘设备上并发DNN训练和推理的性能，满足实际应用的约束要求。

Abstract: The proliferation of GPU accelerated edge devices like Nvidia Jetsons and the
rise in privacy concerns are placing an emphasis on concurrent DNN training and
inferencing on edge devices. Inference and training have different computing
and QoS goals. But edge accelerators like Jetson do not support native GPU
sharing and expose 1000s of power modes. This requires careful time-sharing of
concurrent workloads to meet power--performance goals, while limiting costly
profiling. In this paper, we design an intelligent time-slicing approach for
concurrent DNN training and inferencing on Jetsons. We formulate an
optimization problem to interleave training and inferencing minibatches, and
decide the device power mode and inference minibatch size, while maximizing the
training throughput and staying within latency and power budgets, with modest
profiling costs. We propose GMD, an efficient multi-dimensional gradient
descent search which profiles just $15$ power modes; and ALS, an Active
Learning technique which identifies reusable Pareto-optimal power modes, but
profiles $50$--$150$ power modes. We evaluate these within our Fulcrum
scheduler for $273,000+$ configurations across $15$ DNN workloads. We also
evaluate our strategies on dynamic arrival inference and concurrent inferences.
ALS and GMD outperform simpler and more complex baselines with larger-scale
profiling. Their solutions satisfy the latency and power budget for $>97\%$ of
our runs, and on average are within $7\%$ of the optimal throughput.

</details>


### [13] [An Empirical Analysis of Secure Federated Learning for Autonomous Vehicle Applications](https://arxiv.org/abs/2509.20223)
*Md Jueal Mia,M. Hadi Amini*

Main category: cs.DC

TL;DR: 本文对联邦学习在自动驾驶车辆应用中的安全性进行实证分析，研究不同安全聚合技术和多方计算在面对各类网络攻击时的防护效果。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能为自动驾驶车辆提供分布式学习和数据隐私保护，但仍容易受到投毒攻击和推理攻击等网络威胁，需要更强大的安全措施来保障系统性能。

Method: 使用交通图像数据集（如LISA交通灯数据集），采用多种安全聚合技术和多方计算方法，在存在不同类型网络攻击的环境下进行实证分析。

Result: 研究发现攻击者可能误导自动驾驶学习模型，导致交通灯错误分类等严重后果。多方计算作为先进的安全机制，能为边缘自动驾驶车辆的本地模型更新提供标准隐私保护。

Conclusion: 该实证研究探索了各种安全联邦学习聚合技术和多方计算在训练和推理阶段保护自动驾驶应用免受网络威胁的韧性。

Abstract: Federated Learning lends itself as a promising paradigm in enabling
distributed learning for autonomous vehicles applications and ensuring data
privacy while enhancing and refining predictive model performance through
collaborative training on edge client vehicles. However, it remains vulnerable
to various categories of cyber-attacks, necessitating more robust security
measures to effectively mitigate potential threats. Poisoning attacks and
inference attacks are commonly initiated within the federated learning
environment to compromise secure system performance. Secure aggregation can
limit the disclosure of sensitive information from outsider and insider
attackers of the federated learning environment. In this study, our aim is to
conduct an empirical analysis on the transportation image dataset (e.g., LISA
traffic light) using various secure aggregation techniques and multiparty
computation in the presence of diverse categories of cyber-attacks. Multiparty
computation serves as a state-of-the-art security mechanism, offering standard
privacy for secure aggregation of edge autonomous vehicles local model updates
through various security protocols. The presence of adversaries can mislead the
autonomous vehicle learning model, leading to the misclassification of traffic
lights, and resulting in detrimental impacts. This empirical study explores the
resilience of various secure federated learning aggregation techniques and
multiparty computation in safeguarding autonomous vehicle applications against
various cyber threats during both training and inference times.

</details>


### [14] [xGFabric: Coupling Sensor Networks and HPC Facilities with Private 5G Wireless Networks for Real-Time Digital Agriculture](https://arxiv.org/abs/2509.20340)
*Liubov Kurafeeva,Alan Subedi,Ryan Hartung,Michael Fay,Avhishek Biswas,Shantenu Jha,Ozgur O. Kilic,Chandra Krintz,Andre Merzky,Douglas Thain,Mehmet C. Vuran,Rich Wolski*

Main category: cs.DC

TL;DR: xGFabric是一个通过私有5G网络将传感器网络与高性能计算设施耦合的端到端系统，用于数字农业中的实时模拟


<details>
  <summary>Details</summary>
Motivation: 数字农业中的柑橘保护屏研究需要将分布式传感器网络与集中式高性能计算设施耦合，以进行环境条件建模和实时干预指导

Method: 开发xGFabric原型系统，通过5G网络切片将远程传感器连接到HPC系统，实现实时数字农业模拟

Result: 成功构建了能够耦合传感器网络和HPC设施的原型系统

Conclusion: 私有5G网络为解决边缘传感器网络与HPC模拟的近实时耦合挑战提供了新的能力

Abstract: Advanced scientific applications require coupling distributed sensor networks
with centralized high-performance computing facilities. Citrus Under Protective
Screening (CUPS) exemplifies this need in digital agriculture, where citrus
research facilities are instrumented with numerous sensors monitoring
environmental conditions and detecting protective screening damage. CUPS
demands access to computational fluid dynamics codes for modeling environmental
conditions and guiding real-time interventions like water application or
robotic repairs. These computing domains have contrasting properties: sensor
networks provide low-performance, limited-capacity, unreliable data access,
while high-performance facilities offer enormous computing power through
high-latency batch processing. Private 5G networks present novel capabilities
addressing this challenge by providing low latency, high throughput, and
reliability necessary for near-real-time coupling of edge sensor networks with
HPC simulations. This work presents xGFabric, an end-to-end system coupling
sensor networks with HPC facilities through Private 5G networks. The prototype
connects remote sensors via 5G network slicing to HPC systems, enabling
real-time digital agriculture simulation.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [15] [Open-source Stand-Alone Versatile Tensor Accelerator](https://arxiv.org/abs/2509.19790)
*Anthony Faure-Gignoux,Kevin Delmas,Adrien Gauffriau,Claire Pagetti*

Main category: cs.AR

TL;DR: 开发了一个开源的独立Python编译器管道，用于Versatile Tensor Accelerator (VTA)，解决了VTA对TVM编译器的依赖和代码不符合认证要求的问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习应用需要大量计算资源，在航空等安全关键领域面临挑战。VTA是一个有前景的FPGA解决方案，但其采用受到对TVM编译器的依赖以及代码不符合认证要求的限制。

Method: 从头开发了一个开源的独立Python编译器管道，设计时考虑了认证要求、模块化和可扩展性。使用VTA模拟器编译和执行LeNet-5卷积神经网络来验证编译器效果。

Result: 初步结果表明该编译器具有扩展到更大CNN架构的强大潜力。所有贡献都已公开可用。

Conclusion: 提出的编译器管道成功解决了VTA的采用障碍，为安全关键领域的机器学习应用提供了可行的FPGA加速解决方案。

Abstract: Machine Learning (ML) applications demand significant computational
resources, posing challenges for safety-critical domains like aeronautics. The
Versatile Tensor Accelerator (VTA) is a promising FPGA-based solution, but its
adoption was hindered by its dependency on the TVM compiler and by other code
non-compliant with certification requirements. This paper presents an
open-source, standalone Python compiler pipeline for the VTA, developed from
scratch and designed with certification requirements, modularity, and
extensibility in mind. The compiler's effectiveness is demonstrated by
compiling and executing LeNet-5 Convolutional Neural Network (CNN) using the
VTA simulators, and preliminary results indicate a strong potential for scaling
its capabilities to larger CNN architectures. All contributions are publicly
available.

</details>


### [16] [SpecMamba: Accelerating Mamba Inference on FPGA with Speculative Decoding](https://arxiv.org/abs/2509.19873)
*Linfeng Zhong,Songqiang Xu,Huifeng Wen,Tong Xie,Qingyu Guo,Yuan Wang,Meng Li*

Main category: cs.AR

TL;DR: SpecMamba是首个基于FPGA的Mamba模型加速器，采用推测解码技术解决状态空间模型在边缘设备上长序列建模的内存瓶颈问题，通过系统、算法和硬件协同设计实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着边缘设备对长序列建模需求的增长，状态空间模型（如Mamba）因其计算效率和可扩展性而广泛应用。然而，其自回归生成过程仍受内存限制，推测解码技术面临隐藏状态回溯困难、树形并行验证不兼容和硬件工作负载不匹配三大挑战。

Method: 提出SpecMamba加速器，采用三层次协同设计：系统层面使用内存感知混合回溯策略协调模型；算法层面提出基于FIFO的树形验证与分块技术减少内存访问；硬件层面定制数据流，并行计算线性层、串行计算SSM层以实现最大重叠。

Result: 在AMD FPGA平台（VHK158和VCK190）上实现，相比GPU基线获得2.27倍加速，相比现有FPGA解决方案提升2.85倍，同时能效分别提高5.41倍和1.26倍。

Conclusion: SpecMamba成功解决了SSM模型推测解码的关键挑战，通过协同设计方法在FPGA平台上实现了显著的性能和能效提升，为边缘设备上的高效长序列建模提供了有效解决方案。

Abstract: The growing demand for efficient long-sequence modeling on edge devices has
propelled widespread adoption of State Space Models (SSMs) like Mamba, due to
their superior computational efficiency and scalability. As its autoregressive
generation process remains memory-bound, speculative decoding has been proposed
that incorporates draft model generation and target model verification.
However, directly applying speculative decoding to SSMs faces three key
challenges: (1) hidden state backtracking difficulties, (2) tree-based parallel
verification incompatibility, and (3) hardware workload mismatch. To address
these challenges, we propose SpecMamba, the first FPGA-based accelerator for
Mamba with speculative decoding, which features system, algorithm, and hardware
co-design. At the system level, we present a memory-aware hybrid backtracking
strategy to coordinate both models. At the algorithm level, we propose
first-in-first-out (FIFO)-based tree verification with tiling to minimize
memory access. At the hardware level, we customize a dataflow that computes
linear layers in parallel and SSM layers in series to enable maximal
overlapping. Implemented on AMD FPGA platforms (VHK158 and VCK190), SpecMamba
achieves a 2.27x speedup over GPU baselines and a 2.85x improvement compared to
prior FPGA solutions, while demonstrating 5.41x and 1.26x higher energy
efficiency, respectively.

</details>


### [17] [OpenGL GPU-Based Rowhammer Attack (Work in Progress)](https://arxiv.org/abs/2509.19959)
*Antoine Plin,Frédéric Fauberteau,Nga Nguyen*

Main category: cs.AR

TL;DR: 本文提出了一种基于GPU计算着色器的自适应多面Rowhammer攻击方法，通过统计分布优化行目标选择并规避现有缓解措施，在树莓派4上实现了比传统CPU攻击更高的比特翻转率。


<details>
  <summary>Details</summary>
Motivation: Rowhammer攻击已成为现代DRAM内存系统的重大威胁，但传统CPU攻击效率有限。本研究旨在利用GPU的并行处理能力提升攻击效率，探索异构系统中GPU的安全漏洞。

Method: 使用OpenGL计算着色器实现高效行锤击，通过初始化已知内存模式、迭代锤击受害行、监控诱导错误并动态调整参数来最大化成功率。利用统计分布优化目标行选择。

Result: 在树莓派4上的实验结果表明，基于GPU的方法相比传统CPU锤击实现了更高的比特翻转率，证实了其在破坏DRAM完整性方面的有效性。

Conclusion: 本研究加深了对GPU辅助故障注入攻击的理解，强调了未来内存架构中需要改进的缓解策略，突显了GPU在异构系统中对安全漏洞的易感性。

Abstract: Rowhammer attacks have emerged as a significant threat to modern DRAM-based
memory systems, leveraging frequent memory accesses to induce bit flips in
adjacent memory cells. This work-in-progress paper presents an adaptive,
many-sided Rowhammer attack utilizing GPU compute shaders to systematically
achieve high-frequency memory access patterns. Our approach employs statistical
distributions to optimize row targeting and avoid current mitigations. The
methodology involves initializing memory with known patterns, iteratively
hammering victim rows, monitoring for induced errors, and dynamically adjusting
parameters to maximize success rates. The proposed attack exploits the parallel
processing capabilities of GPUs to accelerate hammering operations, thereby
increasing the probability of successful bit flips within a constrained
timeframe. By leveraging OpenGL compute shaders, our implementation achieves
highly efficient row hammering with minimal software overhead. Experimental
results on a Raspberry Pi 4 demonstrate that the GPU-based approach attains a
high rate of bit flips compared to traditional CPU-based hammering, confirming
its effectiveness in compromising DRAM integrity. Our findings align with
existing research on microarchitectural attacks in heterogeneous systems that
highlight the susceptibility of GPUs to security vulnerabilities. This study
contributes to the understanding of GPU-assisted fault-injection attacks and
underscores the need for improved mitigation strategies in future memory
architectures.

</details>


### [18] [Automated Multi-Agent Workflows for RTL Design](https://arxiv.org/abs/2509.20182)
*Amulya Bhattaram,Janani Ramamoorthy,Ranit Gupta,Diana Marculescu,Dimitrios Stamoulis*

Main category: cs.AR

TL;DR: VeriMaAS是一个多智能体框架，通过集成形式验证反馈来自动化RTL代码生成的智能体工作流，在减少监督成本的同时提升合成性能。


<details>
  <summary>Details</summary>
Motivation: 针对程序合成等专业领域，由于HDL和专有EDA资源的稀缺性，现有方法需要任务特定的微调、高推理成本和手动编排智能体，存在挑战。

Method: 提出VeriMaAS多智能体框架，将HDL工具的形式验证反馈直接集成到工作流生成中，减少基于梯度的更新或冗长推理轨迹的成本。

Result: 在pass@k指标上比微调基线提升5-7%，仅需数百个训练样本，监督成本降低一个数量级。

Conclusion: VeriMaAS通过形式验证反馈驱动的智能体工作流自动化，有效解决了专业领域程序合成的资源稀缺和成本问题。

Abstract: The rise of agentic AI workflows unlocks novel opportunities for computer
systems design and optimization. However, for specialized domains such as
program synthesis, the relative scarcity of HDL and proprietary EDA resources
online compared to more common programming tasks introduces challenges, often
necessitating task-specific fine-tuning, high inference costs, and
manually-crafted agent orchestration. In this work, we present VeriMaAS, a
multi-agent framework designed to automatically compose agentic workflows for
RTL code generation. Our key insight is to integrate formal verification
feedback from HDL tools directly into workflow generation, reducing the cost of
gradient-based updates or prolonged reasoning traces. Our method improves
synthesis performance by 5-7% for pass@k over fine-tuned baselines, while
requiring only a few hundred training examples, representing an
order-of-magnitude reduction in supervision cost.

</details>
