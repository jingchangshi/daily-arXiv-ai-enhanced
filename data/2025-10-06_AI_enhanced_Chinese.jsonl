{"id": "2510.02613", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.02613", "abs": "https://arxiv.org/abs/2510.02613", "authors": ["Gursimran Singh", "Timothy Yu", "Haley Li", "Cheng Chen", "Hanieh Sadri", "Qintao Zhang", "Yu Zhang", "Ying Xiong", "Yong Zhang", "Zhenan Fan"], "title": "ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts Models", "comment": "19 pages, 15 figures, Under Submission", "summary": "Mixture-of-Experts (MoE) models promise efficient scaling of large language\nmodels (LLMs) by activating only a small subset of experts per token, but their\nparallelized inference pipelines make elastic serving challenging. Existing\nstrategies fall short: horizontal scaling provisions entire replicas of the\ncurrent configuration, often tens to hundreds of accelerators, leading to\ncoarse granularity, long provisioning delays, and costly overprovisioning.\nVertical scaling offers finer adjustments but typically requires instance\nrestarts, incurring downtime. These limitations make current approaches\nill-suited for the bursty, short-lived traffic patterns common in cloud\ndeployments.\n  We present ElasticMoE, an elastic scaling framework for MoE LLMs that\nachieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE\ndecouples inference execution from memory operations, enabling scaling steps to\nproceed concurrently with serving. An HBM Management Module (HMM) reuses\nweights and KV caches via zero-copy remapping, while high-bandwidth\npeer-to-peer transfers bring newly added accelerators online without\ninterrupting service. A virtual memory based expert redistribution mechanism\nmigrates MoE experts without costly buffer reallocations, reducing peak memory\nusage during expert parallelism reconfiguration.\n  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that\nElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput\nduring scaling, and significantly improves SLO attainment compared to\nbaselines. By enabling fine-grained, concurrent scaling with minimal\ndisruption, ElasticMoE advances the practicality of deploying massive MoE LLMs\nin dynamic cloud environments.", "AI": {"tldr": "ElasticMoE\u662f\u4e00\u4e2a\u7528\u4e8eMoE LLM\u7684\u5f39\u6027\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u6267\u884c\u548c\u5185\u5b58\u64cd\u4f5c\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u3001\u96f6\u505c\u673a\u7684\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u52a8\u6001\u4e91\u73af\u5883\u4e2d\u90e8\u7f72\u5927\u89c4\u6a21MoE LLM\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684MoE\u6a21\u578b\u6269\u5c55\u7b56\u7565\u5b58\u5728\u5c40\u9650\u6027\uff1a\u6c34\u5e73\u6269\u5c55\u7c92\u5ea6\u7c97\u3001\u5ef6\u8fdf\u957f\u3001\u6210\u672c\u9ad8\uff1b\u5782\u76f4\u6269\u5c55\u9700\u8981\u5b9e\u4f8b\u91cd\u542f\u5bfc\u81f4\u505c\u673a\u3002\u8fd9\u4e9b\u65b9\u6cd5\u4e0d\u9002\u5408\u4e91\u90e8\u7f72\u4e2d\u5e38\u89c1\u7684\u7a81\u53d1\u3001\u77ed\u65f6\u6d41\u91cf\u6a21\u5f0f\u3002", "method": "ElasticMoE\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u6267\u884c\u4e0e\u5185\u5b58\u64cd\u4f5c\u5b9e\u73b0\u5e76\u53d1\u6269\u5c55\uff0c\u5305\u542bHBM\u7ba1\u7406\u6a21\u5757\u91cd\u7528\u6743\u91cd\u548cKV\u7f13\u5b58\uff0c\u4f7f\u7528\u96f6\u62f7\u8d1d\u91cd\u6620\u5c04\u548c\u70b9\u5bf9\u70b9\u4f20\u8f93\uff0c\u4ee5\u53ca\u57fa\u4e8e\u865a\u62df\u5185\u5b58\u7684\u4e13\u5bb6\u91cd\u5206\u914d\u673a\u5236\u3002", "result": "\u5728Ascend NPU\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cElasticMoE\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u4e86\u9ad8\u8fbe9\u500d\u7684\u6269\u5c55\u5ef6\u8fdf\u964d\u4f4e\u30012\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5e76\u663e\u8457\u6539\u5584\u4e86SLO\u8fbe\u6210\u7387\u3002", "conclusion": "ElasticMoE\u901a\u8fc7\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u3001\u5e76\u53d1\u7684\u6269\u5c55\u4e14\u5e72\u6270\u6700\u5c0f\uff0c\u63a8\u8fdb\u4e86\u5728\u52a8\u6001\u4e91\u73af\u5883\u4e2d\u90e8\u7f72\u5927\u89c4\u6a21MoE LLM\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.02774", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.02774", "abs": "https://arxiv.org/abs/2510.02774", "authors": ["Xiang Li", "Qiong Chang", "Yun Li", "Jun Miyazaki"], "title": "GRNND: A GPU-Parallel Relative NN-Descent Algorithm for Efficient Approximate Nearest Neighbor Graph Construction", "comment": null, "summary": "Relative Nearest Neighbor Descent (RNN-Descent) is a state-of-the-art\nalgorithm for constructing sparse approximate nearest neighbor (ANN) graphs by\ncombining the iterative refinement of NN-Descent with the edge-pruning rules of\nthe Relative Neighborhood Graph (RNG). It has demonstrated strong effectiveness\nin large-scale search tasks such as information retrieval and related tasks.\nHowever, as the amount and dimensionality of data increase, the complexity of\ngraph construction in RNN-Descent rises sharply, making this stage increasingly\ntime-consuming and even prohibitive for subsequent query processing. In this\npaper, we propose GRNND, the first GPU-parallel algorithm of RNN-Descent\ndesigned to fully exploit GPU architecture. GRNND introduces a disordered\nneighbor propagation strategy to mitigate synchronized update traps, enhancing\nstructural diversity, and avoiding premature convergence during parallel\nexecution. It also leverages warp-level cooperative operations and a\ndouble-buffered neighbor pool with fixed capacity for efficient memory access,\neliminate contention, and enable highly parallelized neighbor updates.\nExtensive experiments demonstrate that GRNND consistently outperforms existing\nCPU- and GPU-based methods. GRNND achieves 2.4 to 51.7x speedup over existing\nGPU methods, and 17.8 to 49.8x speedup over CPU methods.", "AI": {"tldr": "GRNND\u662f\u9996\u4e2aGPU\u5e76\u884c\u5316\u7684RNN-Descent\u7b97\u6cd5\uff0c\u901a\u8fc7\u65e0\u5e8f\u90bb\u5c45\u4f20\u64ad\u7b56\u7565\u3001warp\u7ea7\u534f\u4f5c\u64cd\u4f5c\u548c\u53cc\u7f13\u51b2\u90bb\u5c45\u6c60\u7b49\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u9ad8\u7ef4\u6570\u636e\u4e0b\u8fd1\u4f3c\u6700\u8fd1\u90bb\u56fe\u7684\u6784\u5efa\u6548\u7387\u3002", "motivation": "\u968f\u7740\u6570\u636e\u91cf\u548c\u7ef4\u5ea6\u7684\u589e\u52a0\uff0cRNN-Descent\u7b97\u6cd5\u5728\u56fe\u6784\u5efa\u9636\u6bb5\u7684\u590d\u6742\u5ea6\u6025\u5267\u4e0a\u5347\uff0c\u53d8\u5f97\u975e\u5e38\u8017\u65f6\uff0c\u751a\u81f3\u963b\u788d\u540e\u7eed\u67e5\u8be2\u5904\u7406\u3002", "method": "\u63d0\u51fa\u4e86GRNND\u7b97\u6cd5\uff0c\u91c7\u7528\u65e0\u5e8f\u90bb\u5c45\u4f20\u64ad\u7b56\u7565\u907f\u514d\u540c\u6b65\u66f4\u65b0\u9677\u9631\uff0c\u5229\u7528warp\u7ea7\u534f\u4f5c\u64cd\u4f5c\u548c\u56fa\u5b9a\u5bb9\u91cf\u7684\u53cc\u7f13\u51b2\u90bb\u5c45\u6c60\u5b9e\u73b0\u9ad8\u6548\u5185\u5b58\u8bbf\u95ee\u548c\u9ad8\u5ea6\u5e76\u884c\u5316\u90bb\u5c45\u66f4\u65b0\u3002", "result": "GRNND\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709CPU\u548cGPU\u65b9\u6cd5\uff0c\u76f8\u6bd4GPU\u65b9\u6cd5\u52a0\u901f2.4-51.7\u500d\uff0c\u76f8\u6bd4CPU\u65b9\u6cd5\u52a0\u901f17.8-49.8\u500d\u3002", "conclusion": "GRNND\u662f\u9996\u4e2a\u5145\u5206\u5229\u7528GPU\u67b6\u6784\u7684RNN-Descent\u5e76\u884c\u7b97\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u9ad8\u7ef4\u6570\u636e\u4e0b\u8fd1\u4f3c\u6700\u8fd1\u90bb\u56fe\u6784\u5efa\u7684\u6548\u7387\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2510.02838", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.02838", "abs": "https://arxiv.org/abs/2510.02838", "authors": ["Yifei Xia", "Fangcheng Fu", "Hao Yuan", "Hanke Zhang", "Xupeng Miao", "Yijun Liu", "Suhan Ling", "Jie Jiang", "Bin Cui"], "title": "TridentServe: A Stage-level Serving System for Diffusion Pipelines", "comment": null, "summary": "Diffusion pipelines, renowned for their powerful visual generation\ncapabilities, have seen widespread adoption in generative vision tasks (e.g.,\ntext-to-image/video). These pipelines typically follow an\nencode--diffuse--decode three-stage architecture. Current serving systems\ndeploy diffusion pipelines within a static, manual, and pipeline-level\nparadigm, allocating the same resources to every request and stage. However,\nthrough an in-depth analysis, we find that such a paradigm is inefficient due\nto the discrepancy in resource needs across the three stages of each request,\nas well as across different requests. Following the analysis, we propose the\ndynamic stage-level serving paradigm and develop TridentServe, a brand new\ndiffusion serving system. TridentServe automatically, dynamically derives the\nplacement plan (i.e., how each stage resides) for pipeline deployment and the\ndispatch plan (i.e., how the requests are routed) for request processing,\nco-optimizing the resource allocation for both model and requests. Extensive\nexperiments show that TridentServe consistently improves SLO attainment and\nreduces average/P95 latencies by up to 2.5x and 3.6x/4.1x over existing works\nacross a variety of workloads.", "AI": {"tldr": "\u63d0\u51fa\u4e86TridentServe\u7cfb\u7edf\uff0c\u91c7\u7528\u52a8\u6001\u9636\u6bb5\u7ea7\u670d\u52a1\u8303\u5f0f\u6765\u4f18\u5316\u6269\u6563\u7ba1\u9053\u7684\u8d44\u6e90\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u670d\u52a1\u8d28\u91cf\u548c\u964d\u4f4e\u5ef6\u8fdf", "motivation": "\u5f53\u524d\u6269\u6563\u7ba1\u9053\u670d\u52a1\u7cfb\u7edf\u91c7\u7528\u9759\u6001\u3001\u624b\u52a8\u3001\u7ba1\u9053\u7ea7\u7684\u8d44\u6e90\u5206\u914d\u65b9\u5f0f\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u9636\u6bb5\u548c\u8bf7\u6c42\u7684\u8d44\u6e90\u9700\u6c42\u5dee\u5f02\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b", "method": "\u5f00\u53d1TridentServe\u7cfb\u7edf\uff0c\u81ea\u52a8\u52a8\u6001\u751f\u6210\u7ba1\u9053\u90e8\u7f72\u7684\u653e\u7f6e\u8ba1\u5212\u548c\u8bf7\u6c42\u5904\u7406\u7684\u8def\u7531\u8ba1\u5212\uff0c\u534f\u540c\u4f18\u5316\u6a21\u578b\u548c\u8bf7\u6c42\u7684\u8d44\u6e90\u5206\u914d", "result": "\u5728\u5404\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0cTridentServe\u76f8\u6bd4\u73b0\u6709\u5de5\u4f5c\u5c06\u5e73\u5747/P95\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe2.5\u500d\u548c3.6\u500d/4.1\u500d\uff0c\u6301\u7eed\u6539\u5584SLO\u8fbe\u6210\u7387", "conclusion": "\u52a8\u6001\u9636\u6bb5\u7ea7\u670d\u52a1\u8303\u5f0f\u80fd\u6709\u6548\u89e3\u51b3\u6269\u6563\u7ba1\u9053\u670d\u52a1\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u6548\u7387\u95ee\u9898\uff0cTridentServe\u7cfb\u7edf\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027"}}
{"id": "2510.02878", "categories": ["cs.DC", "cs.MS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.02878", "abs": "https://arxiv.org/abs/2510.02878", "authors": ["Massimo Bernaschi", "Alessandro Celestini", "Pasqua D'Ambra", "Giorgio Richelli"], "title": "On the energy efficiency of sparse matrix computations on multi-GPU clusters", "comment": null, "summary": "We investigate the energy efficiency of a library designed for parallel\ncomputations with sparse matrices. The library leverages high-performance,\nenergy-efficient Graphics Processing Unit (GPU) accelerators to enable\nlarge-scale scientific applications. Our primary development objective was to\nmaximize parallel performance and scalability in solving sparse linear systems\nwhose dimensions far exceed the memory capacity of a single node. To this end,\nwe devised methods that expose a high degree of parallelism while optimizing\nalgorithmic implementations for efficient multi-GPU usage. Previous work has\nalready demonstrated the library's performance efficiency on large-scale\nsystems comprising thousands of NVIDIA GPUs, achieving improvements over\nstate-of-the-art solutions. In this paper, we extend those results by providing\nenergy profiles that address the growing sustainability requirements of modern\nHPC platforms. We present our methodology and tools for accurate runtime energy\nmeasurements of the library's core components and discuss the findings. Our\nresults confirm that optimizing GPU computations and minimizing data movement\nacross memory and computing nodes reduces both time-to-solution and energy\nconsumption. Moreover, we show that the library delivers substantial advantages\nover comparable software frameworks on standard benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7a00\u758f\u77e9\u9635\u5e76\u884c\u8ba1\u7b97\u5e93\u7684\u80fd\u6548\u8868\u73b0\uff0c\u5728\u5df2\u8bc1\u660e\u6027\u80fd\u4f18\u52bf\u7684\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u63d0\u4f9b\u4e86\u80fd\u8017\u5206\u6790\uff0c\u8bc1\u5b9e\u4f18\u5316GPU\u8ba1\u7b97\u548c\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u80fd\u540c\u65f6\u964d\u4f4e\u6c42\u89e3\u65f6\u95f4\u548c\u80fd\u8017\u3002", "motivation": "\u6ee1\u8db3\u73b0\u4ee3\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e73\u53f0\u65e5\u76ca\u589e\u957f\u7684\u53ef\u6301\u7eed\u6027\u9700\u6c42\uff0c\u5728\u5df2\u6709\u6027\u80fd\u4f18\u52bf\u7684\u57fa\u7840\u4e0a\u63d0\u4f9b\u80fd\u8017\u5206\u6790\uff0c\u9a8c\u8bc1\u80fd\u6548\u4f18\u5316\u7b56\u7565\u3002", "method": "\u5f00\u53d1\u4e86\u7cbe\u786e\u7684\u8fd0\u884c\u65f6\u80fd\u8017\u6d4b\u91cf\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u5bf9\u5e93\u7684\u6838\u5fc3\u7ec4\u4ef6\u8fdb\u884c\u80fd\u8017\u5206\u6790\uff0c\u4f18\u5316GPU\u8ba1\u7b97\u5e76\u6700\u5c0f\u5316\u8de8\u5185\u5b58\u548c\u8ba1\u7b97\u8282\u70b9\u7684\u6570\u636e\u79fb\u52a8\u3002", "result": "\u4f18\u5316GPU\u8ba1\u7b97\u548c\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u80fd\u540c\u65f6\u964d\u4f4e\u6c42\u89e3\u65f6\u95f4\u548c\u80fd\u8017\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u540c\u7c7b\u8f6f\u4ef6\u6846\u67b6\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7a00\u758f\u77e9\u9635\u5e76\u884c\u8ba1\u7b97\u5e93\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u5728\u80fd\u6548\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5927\u89c4\u6a21\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.02579", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.02579", "abs": "https://arxiv.org/abs/2510.02579", "authors": ["Santiago Cu\u00e9llar", "Naomi Spargo", "Jonathan Daugherty", "David Darais"], "title": "Designing Walrus: Relational Programming with Rich Types, On-Demand Laziness, and Structured Traces", "comment": "20 pages, miniKanren 2025", "summary": "We present Walrus, a functional relational programming language embedded in\nHaskell that extends the miniKanren model with type-polymorphic unification,\non-demand laziness, and a range of usability features aimed at practical\ndevelopment. These include use of Haskell Generics for boilerplate reduction,\nstructured debugging traces, and ergonomic support for product types. We\ndescribe the design and implementation of Walrus through the lens of our\nexperience developing bidirectional compilers, and reflect on key design\ndecisions and recurring usability challenges encountered in practice.", "AI": {"tldr": "Walrus\u662f\u4e00\u4e2a\u5d4c\u5165\u5728Haskell\u4e2d\u7684\u51fd\u6570\u5f0f\u5173\u7cfb\u7f16\u7a0b\u8bed\u8a00\uff0c\u6269\u5c55\u4e86miniKanren\u6a21\u578b\uff0c\u589e\u52a0\u4e86\u7c7b\u578b\u591a\u6001\u7edf\u4e00\u3001\u6309\u9700\u60f0\u6027\u6c42\u503c\u548c\u5b9e\u7528\u5f00\u53d1\u529f\u80fd\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5b9e\u9645\u5f00\u53d1\u4e2d\u7684\u53ef\u7528\u6027\u95ee\u9898\uff0c\u4e3a\u53cc\u5411\u7f16\u8bd1\u5668\u5f00\u53d1\u7b49\u5e94\u7528\u63d0\u4f9b\u66f4\u597d\u7684\u5de5\u5177\u652f\u6301\u3002", "method": "\u901a\u8fc7Haskell\u6cdb\u578b\u51cf\u5c11\u6837\u677f\u4ee3\u7801\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u8c03\u8bd5\u8ddf\u8e2a\uff0c\u652f\u6301\u4ea7\u54c1\u7c7b\u578b\uff0c\u5e76\u6269\u5c55miniKanren\u6a21\u578b\u3002", "result": "\u5f00\u53d1\u4e86\u5177\u6709\u7c7b\u578b\u591a\u6001\u7edf\u4e00\u548c\u6309\u9700\u60f0\u6027\u6c42\u503c\u529f\u80fd\u7684Walrus\u8bed\u8a00\uff0c\u63d0\u5347\u4e86\u5b9e\u9645\u5f00\u53d1\u4f53\u9a8c\u3002", "conclusion": "Walrus\u901a\u8fc7\u5173\u952e\u8bbe\u8ba1\u51b3\u7b56\u89e3\u51b3\u4e86\u5b9e\u9645\u5f00\u53d1\u4e2d\u5e38\u89c1\u7684\u53ef\u7528\u6027\u6311\u6218\uff0c\u4e3a\u5173\u7cfb\u7f16\u7a0b\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2510.02675", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.02675", "abs": "https://arxiv.org/abs/2510.02675", "authors": ["Shubham Negi", "Kaushik Roy"], "title": "HALO: Memory-Centric Heterogeneous Accelerator with 2.5D Integration for Low-Batch LLM Inference", "comment": null, "summary": "The rapid adoption of Large Language Models (LLMs) has driven a growing\ndemand for efficient inference, particularly in latency-sensitive applications\nsuch as chatbots and personalized assistants. Unlike traditional deep neural\nnetworks, LLM inference proceeds in two distinct phases: the prefill phase,\nwhich processes the full input sequence in parallel, and the decode phase,\nwhich generates tokens sequentially. These phases exhibit highly diverse\ncompute and memory requirements, which makes accelerator design particularly\nchallenging. Prior works have primarily been optimized for high-batch inference\nor evaluated only short input context lengths, leaving the low-batch and long\ncontext regime, which is critical for interactive applications, largely\nunderexplored.\n  We propose HALO, a heterogeneous memory centric accelerator designed for\nthese unique challenges of prefill and decode phases in low-batch LLM\ninference. HALO integrates HBM based Compute-in-DRAM (CiD) with an on-chip\nanalog Compute-in-Memory (CiM), co-packaged using 2.5D integration. To further\nimprove the hardware utilization, we introduce a phase-aware mapping strategy\nthat adapts to the distinct demands of the prefill and decode phases. Compute\nbound operations in the prefill phase are mapped to CiM to exploit its high\nthroughput matrix multiplication capability, while memory-bound operations in\nthe decode phase are executed on CiD to benefit from reduced data movement\nwithin DRAM. Additionally, we present an analysis of the performance tradeoffs\nof LLMs under two architectural extremes: a fully CiD and a fully on-chip\nanalog CiM design to highlight the need for a heterogeneous design. We evaluate\nHALO on LLaMA-2 7B and Qwen3 8B models. Our experimental results show that LLMs\nmapped to HALO achieve up to 18x geometric mean speedup over AttAcc, an\nattention-optimized mapping and 2.5x over CENT, a fully CiD based mapping.", "AI": {"tldr": "HALO\u662f\u4e00\u79cd\u5f02\u6784\u5185\u5b58\u4e2d\u5fc3\u52a0\u901f\u5668\uff0c\u9488\u5bf9LLM\u63a8\u7406\u4e2d\u7684prefill\u548cdecode\u9636\u6bb5\u7684\u4e0d\u540c\u8ba1\u7b97\u7279\u6027\u8fdb\u884c\u4f18\u5316\uff0c\u901a\u8fc7\u7ed3\u5408HBM CiD\u548c\u7247\u4e0aCiM\u6280\u672f\uff0c\u5728\u4f4e\u6279\u91cf\u63a8\u7406\u573a\u666f\u4e0b\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "LLM\u63a8\u7406\u5305\u542b\u8ba1\u7b97\u5bc6\u96c6\u7684prefill\u9636\u6bb5\u548c\u5185\u5b58\u5bc6\u96c6\u7684decode\u9636\u6bb5\uff0c\u73b0\u6709\u65b9\u6848\u4e3b\u8981\u9488\u5bf9\u9ad8\u6279\u91cf\u63a8\u7406\u6216\u77ed\u4e0a\u4e0b\u6587\u573a\u666f\uff0c\u800c\u4ea4\u4e92\u5f0f\u5e94\u7528\u6240\u9700\u7684\u4f4e\u6279\u91cf\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u65b9\u6848\u7f3a\u4e4f\u3002", "method": "\u91c7\u7528\u5f02\u6784\u5185\u5b58\u67b6\u6784\uff0c\u96c6\u6210HBM CiD\u548c\u7247\u4e0aCiM\uff0c\u901a\u8fc72.5D\u5c01\u88c5\u3002\u63d0\u51fa\u9636\u6bb5\u611f\u77e5\u6620\u5c04\u7b56\u7565\uff1aprefill\u9636\u6bb5\u7684\u8ba1\u7b97\u5bc6\u96c6\u578b\u64cd\u4f5c\u6620\u5c04\u5230CiM\uff0cdecode\u9636\u6bb5\u7684\u5185\u5b58\u5bc6\u96c6\u578b\u64cd\u4f5c\u6620\u5c04\u5230CiD\u3002", "result": "\u5728LLaMA-2 7B\u548cQwen3 8B\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4AttAcc\u83b7\u5f97\u6700\u9ad818\u500d\u51e0\u4f55\u5e73\u5747\u52a0\u901f\uff0c\u76f8\u6bd4\u5168CiD\u65b9\u6848CENT\u83b7\u5f972.5\u500d\u52a0\u901f\u3002", "conclusion": "\u5f02\u6784\u5185\u5b58\u67b6\u6784\u80fd\u6709\u6548\u89e3\u51b3LLM\u63a8\u7406\u4e2dprefill\u548cdecode\u9636\u6bb5\u7684\u5dee\u5f02\u5316\u9700\u6c42\uff0c\u5728\u4f4e\u6279\u91cf\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u63d0\u4f9b\u663e\u8457\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2510.02882", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.02882", "abs": "https://arxiv.org/abs/2510.02882", "authors": ["Adhitya Bhawiyuga", "Serkan Girgin", "Rolf A. de By", "Raul Zurita-Milla"], "title": "Energy Efficiency in Cloud-Based Big Data Processing for Earth Observation: Gap Analysis and Future Directions", "comment": null, "summary": "Earth observation (EO) data volumes are rapidly increasing. While cloud\ncomputing are now used for processing large EO datasets, the energy efficiency\naspects of such a processing have received much less attention. This issue is\nnotable given the increasing awareness of energy costs and carbon footprint in\nbig data processing, particularly with increased attention on compute-intensive\nfoundation models. In this paper we identify gaps in energy efficiency\npractices within cloud-based EO big data (EOBD) processing and propose several\nresearch directions for improvement. We first examine the current EOBD\nlandscape, focus on the requirements that necessitate cloud-based processing\nand analyze existing cloud-based EOBD solutions. We then investigate energy\nefficiency strategies that have been successfully employed in well-studied big\ndata domains. Through this analysis, we identify several critical gaps in\nexisting EOBD processing platforms, which primarily focus on data accessibility\nand computational feasibility, instead of energy efficiency. These gaps include\ninsufficient energy monitoring mechanisms, lack of energy awareness in data\nmanagement, inadequate implementation of energy-aware resource allocation and\nlack of energy efficiency criteria on task scheduling. Based on these findings,\nwe propose the development of energy-aware performance monitoring and\nbenchmarking frameworks, the use of optimization techniques for infrastructure\norchestration, and of energy-efficient task scheduling approaches for\ndistributed cloud-based EOBD processing frameworks. These proposed approaches\naim to foster more energy awareness in EOBD processing , potentially reducing\npower consumption and environmental impact while maintaining or minimally\nimpacting processing performance.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u4e91\u5904\u7406\u5730\u7403\u89c2\u6d4b\u5927\u6570\u636e\u7684\u80fd\u6548\u95ee\u9898\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u5e73\u53f0\u5728\u80fd\u6548\u65b9\u9762\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5730\u7403\u89c2\u6d4b\u6570\u636e\u91cf\u5feb\u901f\u589e\u957f\u548c\u4e91\u8ba1\u7b97\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u80fd\u6548\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u7279\u522b\u662f\u5728\u5173\u6ce8\u8ba1\u7b97\u5bc6\u96c6\u578b\u57fa\u7840\u6a21\u578b\u548c\u78b3\u8db3\u8ff9\u7684\u80cc\u666f\u4e0b\uff0c\u4e91\u5904\u7406\u5730\u7403\u89c2\u6d4b\u5927\u6570\u636e\u7684\u80fd\u6548\u5b9e\u8df5\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "method": "\u9996\u5148\u5206\u6790\u5f53\u524d\u5730\u7403\u89c2\u6d4b\u5927\u6570\u636e\u5904\u7406\u73b0\u72b6\u548c\u4e91\u5904\u7406\u9700\u6c42\uff0c\u7136\u540e\u7814\u7a76\u5176\u4ed6\u5927\u6570\u636e\u9886\u57df\u6210\u529f\u7684\u80fd\u6548\u7b56\u7565\uff0c\u8bc6\u522b\u5730\u7403\u89c2\u6d4b\u5927\u6570\u636e\u5904\u7406\u5e73\u53f0\u7684\u5173\u952e\u80fd\u6548\u5dee\u8ddd\u3002", "result": "\u8bc6\u522b\u51fa\u56db\u4e2a\u5173\u952e\u80fd\u6548\u5dee\u8ddd\uff1a\u80fd\u6548\u76d1\u6d4b\u673a\u5236\u4e0d\u8db3\u3001\u6570\u636e\u7ba1\u7406\u7f3a\u4e4f\u80fd\u6548\u610f\u8bc6\u3001\u8d44\u6e90\u5206\u914d\u672a\u5145\u5206\u8003\u8651\u80fd\u6548\u3001\u4efb\u52a1\u8c03\u5ea6\u7f3a\u4e4f\u80fd\u6548\u6807\u51c6\u3002", "conclusion": "\u63d0\u51fa\u4e86\u5f00\u53d1\u80fd\u6548\u611f\u77e5\u7684\u6027\u80fd\u76d1\u6d4b\u6846\u67b6\u3001\u4f18\u5316\u57fa\u7840\u8bbe\u65bd\u7f16\u6392\u6280\u672f\u3001\u91c7\u7528\u80fd\u6548\u4efb\u52a1\u8c03\u5ea6\u65b9\u6cd5\u7b49\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u63d0\u9ad8\u5730\u7403\u89c2\u6d4b\u5927\u6570\u636e\u5904\u7406\u7684\u80fd\u6548\u610f\u8bc6\uff0c\u964d\u4f4e\u80fd\u8017\u548c\u73af\u5883\u5f71\u200b\u54cd\u3002"}}
{"id": "2510.03170", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.03170", "abs": "https://arxiv.org/abs/2510.03170", "authors": ["Rafaello Sanna", "William E. Byrd", "Nada Amin"], "title": "Beyond Cons: Purely Relational Data Structures", "comment": "17 pages, 6 figures, Source code available at\n  https://www.github.com/rvs314/faster-clpset-minikanren . To be published in\n  the 7th Workshop on miniKanren and Relational Programming (miniKanren'25)", "summary": "We present {Kanren} (read: set-Kanren), an extension to miniKanren with\nconstraints for reasoning about sets and association lists. {Kanren} includes\nfirst-class set objects, a functionally complete family of set-theoretic\nconstraints (including membership, union, and disjointedness), and new\nconstraints for reasoning about association lists with shadowing and scoped\nlookup. These additions allow programmers to describe collections declaratively\nand lazily, without relying on structural encodings and eager search over\nrepresentation spaces. The result is improved expressiveness and operational\nbehavior in programs that manipulate abstract data -- particularly interpreters\n-- by supporting set equality based on contents, enabling finite failure. We\ndescribe the design and implementation of {Kanren} in a constraint-enabled\nminiKanren system and illustrate its use in representative examples.", "AI": {"tldr": "Kanren\u662fminiKanren\u7684\u6269\u5c55\uff0c\u589e\u52a0\u4e86\u96c6\u5408\u548c\u5173\u8054\u5217\u8868\u7684\u7ea6\u675f\u63a8\u7406\u529f\u80fd\uff0c\u5305\u62ec\u4e00\u7b49\u96c6\u5408\u5bf9\u8c61\u3001\u96c6\u5408\u8bba\u7ea6\u675f\u548c\u5173\u8054\u5217\u8868\u7ea6\u675f\uff0c\u63d0\u9ad8\u4e86\u62bd\u8c61\u6570\u636e\u64cd\u4f5c\u7a0b\u5e8f\u7684\u8868\u8fbe\u80fd\u529b\u548c\u64cd\u4f5c\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728miniKanren\u4e2d\u63cf\u8ff0\u96c6\u5408\u548c\u5173\u8054\u5217\u8868\u65f6\u9700\u8981\u4f9d\u8d56\u7ed3\u6784\u7f16\u7801\u548c\u6025\u5207\u641c\u7d22\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u58f0\u660e\u5f0f\u548c\u60f0\u6027\u7684\u65b9\u5f0f\u6765\u63cf\u8ff0\u96c6\u5408\u3002", "method": "\u6269\u5c55miniKanren\u7cfb\u7edf\uff0c\u5f15\u5165\u4e00\u7b49\u96c6\u5408\u5bf9\u8c61\u3001\u5b8c\u6574\u7684\u96c6\u5408\u8bba\u7ea6\u675f\uff08\u6210\u5458\u3001\u5e76\u96c6\u3001\u4e0d\u76f8\u4ea4\u7b49\uff09\u4ee5\u53ca\u652f\u6301\u906e\u853d\u548c\u8303\u56f4\u67e5\u627e\u7684\u5173\u8054\u5217\u8868\u7ea6\u675f\u3002", "result": "\u63d0\u9ad8\u4e86\u7a0b\u5e8f\u7684\u8868\u8fbe\u80fd\u529b\u548c\u64cd\u4f5c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u64cd\u4f5c\u62bd\u8c61\u6570\u636e\uff08\u5982\u89e3\u91ca\u5668\uff09\u65f6\uff0c\u652f\u6301\u57fa\u4e8e\u5185\u5bb9\u7684\u96c6\u5408\u76f8\u7b49\u6027\u548c\u6709\u9650\u5931\u8d25\u3002", "conclusion": "Kanren\u901a\u8fc7\u5f15\u5165\u96c6\u5408\u548c\u5173\u8054\u5217\u8868\u7ea6\u675f\uff0c\u4e3aminiKanren\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u62bd\u8c61\u6570\u636e\u64cd\u4f5c\u80fd\u529b\uff0c\u4f7f\u7a0b\u5e8f\u63cf\u8ff0\u66f4\u52a0\u58f0\u660e\u5f0f\u548c\u9ad8\u6548\u3002"}}
{"id": "2510.02863", "categories": ["cs.AR", "cs.DS", "cs.NA", "math.NA", "quant-ph", "G.1.3; J.2; B.6.1"], "pdf": "https://arxiv.org/pdf/2510.02863", "abs": "https://arxiv.org/abs/2510.02863", "authors": ["D. A. Herrera-Mart\u00ed", "E. Guthmuller", "J. Fereyre"], "title": "A Hardware Accelerator for the Goemans-Williamson Algorithm", "comment": "Impact of Extended Precision Arithmetic in Interior Point Methods\n  using Conjugate Gradient. 10 pages. Hardware estimates", "summary": "The combinatorial problem Max-Cut has become a benchmark in the evaluation of\nlocal search heuristics for both quantum and classical optimisers. In contrast\nto local search, which only provides average-case performance guarantees, the\nconvex semidefinite relaxation of Max-Cut by Goemans and Williamson, provides\nworst-case guarantees and is therefore suited to both the construction of\nbenchmarks and in applications to performance-critic scenarios.\n  We show how extended floating point precision can be incorporated in\nalgebraic subroutines in convex optimisation, namely in indirect matrix\ninversion methods like Conjugate Gradient, which are used in Interior Point\nMethods in the case of very large problem sizes. Also, an estimate is provided\nof the expected acceleration of the time to solution for a hardware\narchitecture that runs natively on extended precision. Specifically, when using\nindirect matrix inversion methods like Conjugate Gradient, which have lower\ncomplexity than direct methods and are therefore used in very large problems,\nwe see that increasing the internal working precision reduces the time to\nsolution by a factor that increases with the system size.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u51f8\u4f18\u5316\u4e2d\u4f7f\u7528\u6269\u5c55\u6d6e\u70b9\u7cbe\u5ea6\u6765\u52a0\u901f\u5927\u89c4\u6a21Max-Cut\u95ee\u9898\u7684\u6c42\u89e3\uff0c\u7279\u522b\u662f\u5728\u5171\u8f6d\u68af\u5ea6\u7b49\u95f4\u63a5\u77e9\u9635\u6c42\u9006\u65b9\u6cd5\u4e2d\u3002", "motivation": "Max-Cut\u95ee\u9898\u5df2\u6210\u4e3a\u91cf\u5b50\u4e0e\u7ecf\u5178\u4f18\u5316\u5668\u5c40\u90e8\u641c\u7d22\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002\u4e0e\u4ec5\u63d0\u4f9b\u5e73\u5747\u60c5\u51b5\u6027\u80fd\u4fdd\u8bc1\u7684\u5c40\u90e8\u641c\u7d22\u76f8\u6bd4\uff0cGoemans-Williamson\u7684\u51f8\u534a\u5b9a\u677e\u5f1b\u65b9\u6cd5\u63d0\u4f9b\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\uff0c\u9002\u5408\u6784\u5efa\u57fa\u51c6\u548c\u6027\u80fd\u5173\u952e\u5e94\u7528\u3002", "method": "\u5c06\u6269\u5c55\u6d6e\u70b9\u7cbe\u5ea6\u6574\u5408\u5230\u51f8\u4f18\u5316\u7684\u4ee3\u6570\u5b50\u7a0b\u5e8f\u4e2d\uff0c\u7279\u522b\u662f\u5728\u5185\u70b9\u6cd5\u4e2d\u4f7f\u7528\u7684\u5171\u8f6d\u68af\u5ea6\u7b49\u95f4\u63a5\u77e9\u9635\u6c42\u9006\u65b9\u6cd5\u3002\u5206\u6790\u4e86\u5728\u539f\u751f\u652f\u6301\u6269\u5c55\u7cbe\u5ea6\u7684\u786c\u4ef6\u67b6\u6784\u4e0a\u7684\u9884\u671f\u52a0\u901f\u6548\u679c\u3002", "result": "\u5f53\u4f7f\u7528\u5171\u8f6d\u68af\u5ea6\u7b49\u95f4\u63a5\u77e9\u9635\u6c42\u9006\u65b9\u6cd5\u65f6\uff0c\u589e\u52a0\u5185\u90e8\u5de5\u4f5c\u7cbe\u5ea6\u53ef\u51cf\u5c11\u6c42\u89e3\u65f6\u95f4\uff0c\u4e14\u52a0\u901f\u56e0\u5b50\u968f\u7cfb\u7edf\u89c4\u6a21\u589e\u5927\u800c\u589e\u52a0\u3002", "conclusion": "\u6269\u5c55\u6d6e\u70b9\u7cbe\u5ea6\u5728\u5927\u89c4\u6a21\u51f8\u4f18\u5316\u95ee\u9898\u4e2d\u80fd\u663e\u8457\u52a0\u901f\u6c42\u89e3\u8fc7\u7a0b\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u95f4\u63a5\u77e9\u9635\u6c42\u9006\u65b9\u6cd5\u5904\u7406\u8d85\u5927\u89c4\u6a21\u95ee\u9898\u65f6\u6548\u679c\u66f4\u660e\u663e\u3002"}}
{"id": "2510.02894", "categories": ["cs.DC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.02894", "abs": "https://arxiv.org/abs/2510.02894", "authors": ["Jakub Lisowski", "Piotr Tyrakowski", "Szymon Zygu\u0142a", "Krzysztof Kaczmarski"], "title": "PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics", "comment": null, "summary": "PyRadiomics-cuda is a GPU-accelerated extension of the PyRadiomics library,\ndesigned to address the computational challenges of extracting\nthree-dimensional shape features from medical images. By offloading key\ngeometric computations to GPU hardware it dramatically reduces processing times\nfor large volumetric datasets. The system maintains full compatibility with the\noriginal PyRadiomics API, enabling seamless integration into existing AI\nworkflows without code modifications. This transparent acceleration facilitates\nefficient, scalable radiomics analysis, supporting rapid feature extraction\nessential for high-throughput AI pipeline. Tests performed on a typical\ncomputational cluster, budget and home devices prove usefulness in all\nscenarios. PyRadiomics-cuda is implemented in Python and C/CUDA and is freely\navailable under the BSD license at https://github.com/mis-wut/pyradiomics-CUDA\nAdditionally PyRadiomics-cuda test suite is available at\nhttps://github.com/mis-wut/pyradiomics-cuda-data-gen. It provides detailed\nhandbook and sample scripts suited for different kinds of workflows plus\ndetailed installation instructions. The dataset used for testing is available\nat Kaggle\nhttps://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19", "AI": {"tldr": "PyRadiomics-cuda\u662f\u4e00\u4e2aGPU\u52a0\u901f\u7684PyRadiomics\u5e93\u6269\u5c55\uff0c\u901a\u8fc7\u5c06\u5173\u952e\u51e0\u4f55\u8ba1\u7b97\u5378\u8f7d\u5230GPU\u786c\u4ef6\uff0c\u663e\u8457\u51cf\u5c11\u533b\u5b66\u56fe\u50cf\u4e09\u7ef4\u5f62\u72b6\u7279\u5f81\u63d0\u53d6\u7684\u5904\u7406\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u4ece\u533b\u5b66\u56fe\u50cf\u4e2d\u63d0\u53d6\u4e09\u7ef4\u5f62\u72b6\u7279\u5f81\u65f6\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u7279\u522b\u662f\u5904\u7406\u5927\u578b\u4f53\u79ef\u6570\u636e\u96c6\u65f6\u7684\u6027\u80fd\u74f6\u9888\u3002", "method": "\u4f7f\u7528Python\u548cC/CUDA\u5b9e\u73b0\uff0c\u5c06\u5173\u952e\u51e0\u4f55\u8ba1\u7b97\u5378\u8f7d\u5230GPU\u786c\u4ef6\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cbPyRadiomics API\u7684\u5b8c\u5168\u517c\u5bb9\u6027\u3002", "result": "\u5728\u5404\u79cd\u8ba1\u7b97\u73af\u5883\uff08\u8ba1\u7b97\u96c6\u7fa4\u3001\u9884\u7b97\u8bbe\u5907\u548c\u5bb6\u5ead\u8bbe\u5907\uff09\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u5904\u7406\u65f6\u95f4\uff0c\u652f\u6301\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u653e\u5c04\u7ec4\u5b66\u5206\u6790\u3002", "conclusion": "PyRadiomics-cuda\u63d0\u4f9b\u4e86\u900f\u660e\u7684GPU\u52a0\u901f\uff0c\u652f\u6301\u9ad8\u541e\u5410\u91cfAI\u6d41\u7a0b\u4e2d\u7684\u5feb\u901f\u7279\u5f81\u63d0\u53d6\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u73b0\u6709\u4ee3\u7801\u5373\u53ef\u96c6\u6210\u5230\u73b0\u6709\u5de5\u4f5c\u6d41\u4e2d\u3002"}}
{"id": "2510.02990", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.02990", "abs": "https://arxiv.org/abs/2510.02990", "authors": ["Philippe Magalh\u00e3es", "Virginie Fresse", "Beno\u00eet Suffran", "Olivier Alata"], "title": "A Resource-Driven Approach for Implementing CNNs on FPGAs Using Adaptive IPs", "comment": "HiPEAC Workshop on Reconfigurable Computing (WRC), Jan 2025,\n  Barcelona, Spain", "summary": "The increasing demand for real-time, low-latency artificial intelligence\napplications has propelled the use of Field-Programmable Gate Arrays (FPGAs)\nfor Convolutional Neural Network (CNN) implementations. FPGAs offer\nreconfigurability, energy efficiency, and performance advantages over GPUs,\nmaking them suitable for edge devices and embedded systems. This work presents\na novel library of resource-efficient convolution IPs designed to automatically\nadapt to the available FPGA resources. Developed in VHDL, these IPs are\nparameterizable and utilize fixed-point arithmetic for optimal performance.\nFour IPs are introduced, each tailored to specific resource constraints,\noffering flexibility in DSP usage, logic consumption, and precision.\nExperimental results on a Zynq UltraScale+ FPGA highlight the trade-offs\nbetween performance and resource usage. The comparison with recent FPGA-based\nCNN acceleration techniques emphasizes the versatility and independence of this\napproach from specific FPGA architectures or technological advancements. Future\nwork will expand the library to include pooling and activation functions,\nenabling broader applicability and integration into CNN frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8d44\u6e90\u9ad8\u6548\u7684\u5377\u79efIP\u5e93\uff0c\u80fd\u591f\u81ea\u52a8\u9002\u5e94FPGA\u53ef\u7528\u8d44\u6e90\uff0c\u91c7\u7528VHDL\u5f00\u53d1\uff0c\u53c2\u6570\u5316\u8bbe\u8ba1\uff0c\u4f7f\u7528\u5b9a\u70b9\u7b97\u672f\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u5b9e\u65f6\u4f4e\u5ef6\u8fdfAI\u5e94\u7528\u9700\u6c42\u589e\u957f\uff0cFPGA\u5728CNN\u5b9e\u73b0\u4e2d\u5177\u6709\u53ef\u91cd\u6784\u6027\u3001\u80fd\u6548\u548c\u6027\u80fd\u4f18\u52bf\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u548c\u5d4c\u5165\u5f0f\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u56db\u4e2a\u9488\u5bf9\u7279\u5b9a\u8d44\u6e90\u7ea6\u675f\u7684IP\u6838\uff0c\u91c7\u7528VHDL\u53c2\u6570\u5316\u8bbe\u8ba1\uff0c\u4f7f\u7528\u5b9a\u70b9\u7b97\u672f\uff0c\u7075\u6d3b\u914d\u7f6eDSP\u4f7f\u7528\u3001\u903b\u8f91\u6d88\u8017\u548c\u7cbe\u5ea6\u3002", "result": "\u5728Zynq UltraScale+ FPGA\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u4e0e\u8d44\u6e90\u4f7f\u7528\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e0e\u73b0\u6709FPGA\u52a0\u901f\u6280\u672f\u76f8\u6bd4\u5177\u6709\u66f4\u597d\u7684\u901a\u7528\u6027\u548c\u67b6\u6784\u72ec\u7acb\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86FPGA\u4e0aCNN\u5b9e\u73b0\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u672a\u6765\u5c06\u6269\u5c55\u5e93\u4ee5\u5305\u542b\u6c60\u5316\u548c\u6fc0\u6d3b\u51fd\u6570\uff0c\u63d0\u5347\u9002\u7528\u6027\u548c\u96c6\u6210\u80fd\u529b\u3002"}}
{"id": "2510.02930", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.02930", "abs": "https://arxiv.org/abs/2510.02930", "authors": ["Wen Guan", "Tadashi Maeno", "Aleksandr Alekseev", "Fernando Harald Barreiro Megino", "Kaushik De", "Edward Karavakis", "Alexei Klimentov", "Tatiana Korchuganova", "FaHui Lin", "Paul Nilsson", "Torre Wenaus", "Zhaoyu Yang", "Xin Zhao"], "title": "iDDS: Intelligent Distributed Dispatch and Scheduling for Workflow Orchestration", "comment": null, "summary": "The intelligent Distributed Dispatch and Scheduling (iDDS) service is a\nversatile workflow orchestration system designed for large-scale, distributed\nscientific computing. iDDS extends traditional workload and data management by\nintegrating data-aware execution, conditional logic, and programmable\nworkflows, enabling automation of complex and dynamic processing pipelines.\nOriginally developed for the ATLAS experiment at the Large Hadron Collider,\niDDS has evolved into an experiment-agnostic platform that supports both\ntemplate-driven workflows and a Function-as-a-Task model for Python-based\norchestration.\n  This paper presents the architecture and core components of iDDS,\nhighlighting its scalability, modular message-driven design, and integration\nwith systems such as PanDA and Rucio. We demonstrate its versatility through\nreal-world use cases: fine-grained tape resource optimization for ATLAS,\norchestration of large Directed Acyclic Graph (DAG) workflows for the Rubin\nObservatory, distributed hyperparameter optimization for machine learning\napplications, active learning for physics analyses, and AI-assisted detector\ndesign at the Electron-Ion Collider.\n  By unifying workload scheduling, data movement, and adaptive decision-making,\niDDS reduces operational overhead and enables reproducible, high-throughput\nworkflows across heterogeneous infrastructures. We conclude with current\nchallenges and future directions, including interactive, cloud-native, and\nserverless workflow support.", "AI": {"tldr": "iDDS\u662f\u4e00\u4e2a\u9762\u5411\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u79d1\u5b66\u8ba1\u7b97\u7684\u591a\u529f\u80fd\u5de5\u4f5c\u6d41\u7f16\u6392\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210\u6570\u636e\u611f\u77e5\u6267\u884c\u3001\u6761\u4ef6\u903b\u8f91\u548c\u53ef\u7f16\u7a0b\u5de5\u4f5c\u6d41\uff0c\u5b9e\u73b0\u590d\u6742\u52a8\u6001\u5904\u7406\u7ba1\u9053\u7684\u81ea\u52a8\u5316\u3002", "motivation": "\u4f20\u7edf\u5de5\u4f5c\u8d1f\u8f7d\u548c\u6570\u636e\u7ba1\u7406\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21\u79d1\u5b66\u8ba1\u7b97\u4e2d\u590d\u6742\u52a8\u6001\u5de5\u4f5c\u6d41\u7684\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7edf\u4e00\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u3001\u6570\u636e\u79fb\u52a8\u548c\u81ea\u9002\u5e94\u51b3\u7b56\u7684\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u6d88\u606f\u9a71\u52a8\u67b6\u6784\uff0c\u96c6\u6210PanDA\u548cRucio\u7b49\u7cfb\u7edf\uff0c\u652f\u6301\u6a21\u677f\u9a71\u52a8\u5de5\u4f5c\u6d41\u548cPython\u7f16\u6392\u7684Function-as-a-Task\u6a21\u578b\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8eATLAS\u5b9e\u9a8c\u78c1\u5e26\u8d44\u6e90\u4f18\u5316\u3001Rubin\u5929\u6587\u53f0DAG\u5de5\u4f5c\u6d41\u7f16\u6392\u3001\u673a\u5668\u5b66\u4e60\u8d85\u53c2\u6570\u4f18\u5316\u3001\u7269\u7406\u5206\u6790\u4e3b\u52a8\u5b66\u4e60\u548cAI\u8f85\u52a9\u63a2\u6d4b\u5668\u8bbe\u8ba1\u7b49\u591a\u4e2a\u573a\u666f\u3002", "conclusion": "iDDS\u901a\u8fc7\u7edf\u4e00\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u3001\u6570\u636e\u79fb\u52a8\u548c\u81ea\u9002\u5e94\u51b3\u7b56\uff0c\u964d\u4f4e\u4e86\u64cd\u4f5c\u5f00\u9500\uff0c\u5b9e\u73b0\u4e86\u8de8\u5f02\u6784\u57fa\u7840\u8bbe\u65bd\u7684\u53ef\u91cd\u73b0\u9ad8\u541e\u5410\u91cf\u5de5\u4f5c\u6d41\u3002"}}
