<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.AR](#cs.AR) [Total: 8]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Expanding Specification Capabilities of a Gradual Verifier with Pure Functions](https://arxiv.org/abs/2511.22075)
*Doruk Alp Mutlu*

Main category: cs.PL

TL;DR: 扩展Gradual C0验证器，支持纯函数以增强规范表达能力和简化观察者方法编码


<details>
  <summary>Details</summary>
Motivation: 尽管Gradual C0是目前唯一基于符号执行的实用渐进验证器，但其规范语言在复杂表达式方面仍有限制，需要扩展表达能力

Method: 在Gradual C0设计中引入纯函数支持，解决具有不精确规范的纯函数的公理化技术挑战

Result: 扩展了Gradual C0的规范能力，提高了观察者方法编码的便利性

Conclusion: 通过支持纯函数，显著增强了Gradual C0验证器的表达能力和实用性

Abstract: Gradual verification soundly combines static checking and dynamic checking to provide an incremental approach for software verification. With gradual verification, programs can be partially specified first, and then the full specification of a program can be achieved in incremental steps. The first and only practicable gradual verifier based on symbolic execution, Gradual C0, supports recursive heap data structures. Despite recent efforts to improve the expressivity of Gradual C0's specification language, Gradual C0's specification language is still limited in its capabilities for complex expressions. This work explores an extension to Gradual C0's design with a common construct supported by many static verification tools, pure functions, which both extend the specification capabilities of Gradual C0 and increase the ease of encoding observer methods in Gradual C0. Our approach addresses the technical challenges related to the axiomatisation of pure functions with imprecise specifications.

</details>


### [2] [On Circuit Description Languages, Indexed Monads, and Resource Analysis](https://arxiv.org/abs/2511.22419)
*Ken Sakayori,Andrea Colledan,Ugo Dal Lago*

Main category: cs.PL

TL;DR: 提出基于单子的指称语义模型，适用于Proto-Quipper系列演算，能分离计算值和电路副作用，支持控制电路大小的类型系统验证。


<details>
  <summary>Details</summary>
Motivation: 为Proto-Quipper演算（Quipper编程语言的理想化版本）建立形式化语义模型，以支持对量子电路生成进行类型控制和验证。

Method: 采用基于单子的指称语义方法，引入电路代数新概念，分离计算值和电路副作用，支持效果类型系统。

Result: 建立了Proto-Quipper演算的充分性指称模型，能验证控制电路大小的类型系统，即使在优化存在时也能保证电路的定量性质。

Conclusion: 单子方法为量子编程语言提供了有效的语义框架，通过电路代数和效果类型系统，能对生成的电路进行形式化验证和控制。

Abstract: In this paper, a monad-based denotational model is introduced and shown adequate for the Proto-Quipper family of calculi, themselves being idealized versions of the Quipper programming language. The use of a monadic approach allows us to separate the value to which a term reduces from the circuit that the term itself produces as a side effect. In turn, this enables the denotational interpretation and validation of rich type systems in which the size of the produced circuit can be controlled. Notably, the proposed semantic framework, through the novel concept of circuit algebra, suggests forms of effect typing guaranteeing quantitative properties about the resulting circuit, even in presence of optimizations.

</details>


### [3] [A Synthetic Reconstruction of Multiparty Session Types (with Appendix)](https://arxiv.org/abs/2511.22692)
*David Castro-Perez,Francisco Ferreira,Sung-Shik Jongmans*

Main category: cs.PL

TL;DR: 提出一种新的多会话类型方法，通过直接验证进程与全局协议规范（LTS）来同时实现表达性和组合性，避免了传统投影方法的限制。


<details>
  <summary>Details</summary>
Motivation: 现有多会话类型方法面临表达性与组合性的权衡：基于投影的经典方法具有组合性但表达性有限，而新方法通过非组合的全系统模型检查提高表达性但扩展性差。

Method: 提出合成方法，设计类型系统直接验证每个进程与全局协议规范（标记转移系统LTS），避免中间局部类型和投影，支持一般LTS规范而不仅是全局类型。

Result: 该方法支持先前组合技术无法处理的挑战性协议基准，可验证任何"行为良好"LTS的进程，支持标准全局类型语法无法表达的协议，已在Agda中形式化并开发VS Code原型。

Conclusion: 合成方法为多会话类型提供了同时实现表达性和组合性的新途径，概念更简单且支持更广泛的协议规范，具有实际应用潜力。

Abstract: Multiparty session types (MPST) provide a rigorous foundation for verifying the safety and liveness of concurrent systems. However, existing approaches often force a difficult trade-off: classical, projection-based techniques are compositional but limited in expressiveness, while more recent techniques achieve higher expressiveness by relying on non-compositional, whole-system model checking, which scales poorly.
  This paper introduces a new approach to MPST that delivers both expressiveness and compositionality, called the synthetic approach. Our key innovation is a type system that verifies each process directly against a global protocol specification, represented as a labelled transition system (LTS) in general, with global types as a special case. This approach uniquely avoids the need for intermediate local types and projection.
  We demonstrate that our approach, while conceptually simpler, supports a benchmark of challenging protocols that were previously beyond the reach of compositional techniques in the MPST literature. We generalise our type system, showing that it can validate processes against any specification that constitutes a "well-behaved" LTS, supporting protocols not expressible with the standard global type syntax. The entire framework, including all theorems and many examples, has been formalised and mechanised in Agda, and we have developed a prototype implementation as an extension to VS Code.

</details>


### [4] [All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs](https://arxiv.org/abs/2511.23283)
*Alexandre Moine,Sam Westrick,Joseph Tassarotti*

Main category: cs.PL

TL;DR: 本文提出了Musketeer分离逻辑来验证调度无关安全性，以及Angelic逻辑来简化内部确定性并行程序的验证。


<details>
  <summary>Details</summary>
Motivation: 内部确定性并行编程虽然简化了程序推理，但缺乏利用这一特性进行形式化验证的框架。现有方法无法充分利用内部确定性来简化程序验证。

Method: 1. 定义调度无关安全性：只需验证一个终止执行的安全性即可保证所有顺序的安全性；2. 提出Musketeer分离逻辑证明调度无关安全性；3. 提出Angelic逻辑动态选择和验证单一顺序；4. 证明MiniDet类型系统的正确性。

Result: 1. 建立了调度无关安全性的形式化定义；2. 开发了Musketeer和Angelic逻辑框架；3. 证明了MiniDet类型系统的正确性；4. 支持包括确定性并发哈希集在内的核心算法原语；5. 所有结果在Rocq中使用Iris分离逻辑框架验证。

Conclusion: 本文提供了首个利用内部确定性简化并行程序验证的框架，通过调度无关安全性和新的逻辑系统，显著降低了内部确定性程序的验证复杂度。

Abstract: Nondeterminism makes parallel programs challenging to write and reason about. To avoid these challenges, researchers have developed techniques for internally deterministic parallel programming, in which the steps of a parallel computation proceed in a deterministic way. Internal determinism is useful because it lets a programmer reason about a program as if it executed in a sequential order. However, no verification framework exists to exploit this property and simplify formal reasoning about internally deterministic programs.
  To capture the essence of why internally deterministic programs should be easier to reason about, this paper defines a property called schedule-independent safety. A program satisfies schedule-independent safety, if, to show that the program is safe across all orderings, it suffices to show that one terminating execution of the program is safe. We then present a separation logic called Musketeer for proving that a program satisfies schedule-independent safety. Once a parallel program has been shown to satisfy schedule-independent safety, we can verify it with a new logic called Angelic, which allows one to dynamically select and verify just one sequential ordering of the program.
  Using Musketeer, we prove the soundness of MiniDet, an affine type system for enforcing internal determinism. MiniDet supports several core algorithmic primitives for internally deterministic programming that have been identified in the research literature, including a deterministic version of a concurrent hash set. Because any syntactically well-typed MiniDet program satisfies schedule-independent safety, we can apply Angelic to verify such programs.
  All results in this paper have been verified in Rocq using the Iris separation logic framework.

</details>


### [5] [TypeDis: A Type System for Disentanglement](https://arxiv.org/abs/2511.23358)
*Alexandre Moine,Stephanie Balzer,Alex Xu,Sam Westrick*

Main category: cs.PL

TL;DR: TypeDis：基于时间戳类型系统自动验证并行程序解耦性，无需手动证明


<details>
  <summary>Details</summary>
Motivation: 解耦性是并行程序的重要运行时属性，能实现高效的无同步任务本地垃圾回收。但现有验证方法DisLog需要手动证明，对程序员要求高且工作量大，需要更自动化的验证方案。

Method: 提出TypeDis类型系统，受区域类型启发，每个类型都标注时间戳标识分配任务。支持同构递归类型、类型和时间戳多态性。关键创新是允许时间戳在类型检查时变化，包括在汇合点通过"子时间"子类型机制。

Result: TypeDis能自动验证任何良类型程序都是解耦的，减轻了程序员手动证明的负担。系统在Rocq证明助手中通过改进的DisLog2进行了机械化验证，并展示了多个示例。

Conclusion: TypeDis为并行程序解耦性提供了实用的自动验证方案，通过类型系统替代复杂的手动证明，使解耦性验证更易于应用。

Abstract: Disentanglement is a runtime property of parallel programs guaranteeing that parallel tasks remain oblivious to each other's allocations. As demonstrated in the MaPLe compiler and run-time system, disentanglement can be exploited for fast automatic memory management, especially task-local garbage collection with no synchronization between parallel tasks. However, as a low-level property, disentanglement can be difficult to reason about for programmers. The only means of statically verifying disentanglement so far has been DisLog, an Iris-fueled variant of separation logic, mechanized in the Rocq proof assistant. DisLog is a fully-featured program logic, allowing for proof of functional correctness as well as verification of disentanglement. Yet its employment requires significant expertise and per-program proof effort.
  This paper explores the route of automatic verification via a type system, ensuring that any well-typed program is disentangled and lifting the burden of carrying out manual proofs from the programmer. It contributes TypeDis, a type system inspired by region types, where each type is annotated with a timestamp, identifying the task that allocated it. TypeDis supports iso-recursive types as well as polymorphism over both types and timestamps. Crucially, timestamps are allowed to change during type-checking, at join points as well as via a form of subtyping, dubbed subtiming. The paper illustrates TypeDis and its features on a range of examples. The soundness of TypeDis and the examples are mechanized in the Rocq proof assistant, using an improved version of DisLog, dubbed DisLog2.

</details>


### [6] [RapunSL: Untangling Quantum Computing with Separation, Linear Combination and Mixing](https://arxiv.org/abs/2511.23472)
*Yusuke Matsushita,Kengo Hirata,Ryo Wakizaka,Emanuele D'Osualdo*

Main category: cs.PL

TL;DR: RapunSL是一种新型量子分离逻辑，通过引入线性组合和混合连接词，能够将叠加态推理简化为纯态推理（基态局部性），并将测量产生的混合态推理简化为纯态推理（结果局部性），显著提升了量子程序推理的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 量子分离逻辑(QSL)虽然能提高量子程序演绎推理的可扩展性，但现有方法在处理叠加态和测量产生的混合态时仍有局限性。需要开发新的逻辑系统来更有效地处理量子领域特有的局部性问题。

Method: 提出RapunSL量子分离逻辑，引入两个新连接词：线性组合（处理叠加态）和混合（处理测量产生的混合态），结合分离概念，实现基态局部性和结果局部性，将复杂量子态推理简化为纯态推理。

Result: RapunSL能够将叠加态推理简化为纯态推理（基态局部性），将测量产生的混合态推理简化为纯态推理（结果局部性），在多个具有挑战性的案例研究中显著提升了推理的可扩展性。

Conclusion: RapunSL通过引入线性组合和混合连接词，解决了量子领域特有的局部性问题，为量子程序推理提供了更强大和可扩展的逻辑框架，在复杂量子程序验证中表现出显著优势。

Abstract: Quantum Separation Logic (QSL) has been proposed as an effective tool to improve the scalability of deductive reasoning for quantum programs. In QSL, separation is interpreted as disentanglement, and the frame rule brings a notion of entanglement-local specification (one that only talks about the qubits entangled with those acted upon by the program). In this paper, we identify two notions of locality unique to the quantum domain, and we construct a novel quantum separation logic, RapunSL, which is able to soundly reduce reasoning about superposition states to reasoning about pure states (basis-locality), and reasoning about mixed states arising from measurement to reasoning about pure states (outcome-locality). To do so, we introduce two connectives, linear combination and mixing, which together with separation provide a dramatic improvement in the scalability of reasoning, as we demonstrate on a series of challenging case studies.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain](https://arxiv.org/abs/2511.21844)
*Murat Yaslioglu*

Main category: cs.DC

TL;DR: 提出一个结合高性能集群计算、智能算法与区块链的新框架，通过改进的PoW共识、动态信任评级和统计抽签系统，实现高效、环保、包容的分布式计算。


<details>
  <summary>Details</summary>
Motivation: 当前高性能计算和智能算法需要大量计算资源，导致高能耗，且排除了计算能力较弱的系统。需要一种更包容、可扩展且环保的智能算法开发和实施方法。

Method: 1) 提出新框架，将高性能集群计算与智能算法集成到区块链基础设施中；2) 改进的PoW共识机制，将计算工作与区块奖励直接挂钩；3) 动态信任评级系统，基于准确验证历史调整节点信誉；4) 统计抽签系统，为计算能力较弱的节点提供参与机会。

Result: 该框架实现了资源优化利用和广泛参与，创建了基于贡献质量的奖励系统，使不同计算能力的节点都能参与区块生成过程。

Conclusion: 提出的解决方案为可持续、包容且高效的智能算法实施提供了可行路径，通过区块链技术平衡了性能、能耗和参与度之间的矛盾。

Abstract: In an age where sustainability is of paramount importance, the significance of both high-performance computing and intelligent algorithms cannot be understated. Yet, these domains often demand hefty computational power, translating to substantial energy usage and potentially sidelining less robust computing systems. It's evident that we need an approach that is more encompassing, scalable, and eco-friendly for intelligent algorithm development and implementation. The strategy we present in this paper offers a compelling answer to these issues. We unveil a fresh framework that seamlessly melds high-performance cluster computing with intelligent algorithms, all within a blockchain infrastructure. This promotes both efficiency and a broad-based participation. At its core, our design integrates an evolved proof-of-work consensus process, which links computational efforts directly to rewards for producing blocks. This ensures both optimal resource use and participation from a wide spectrum of computational capacities. Additionally, our approach incorporates a dynamic 'trust rating' that evolves based on a track record of accurate block validations. This rating determines the likelihood of a node being chosen for block generation, creating a merit-based system that recognizes and rewards genuine and precise contributions. To level the playing field further, we suggest a statistical 'draw' system, allowing even less powerful nodes a chance to be part of the block creation process.

</details>


### [8] [Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models](https://arxiv.org/abs/2511.21859)
*Hagit Attiya,Armando Castañeda,Dhrubajyoti Ghosh,Thomas Nowak*

Main category: cs.DC

TL;DR: 本文重新审视了异步消息传递模型(AMP_f)和Heard-Of模型(HO_f)之间的关系，发现对于n>2f的情况，两种模型在无色任务的可解性上是等价的，但对于有色任务，仅当f=1时等价，f更大时存在分离。


<details>
  <summary>Details</summary>
Motivation: 研究两种基本分布式计算模型（异步消息传递模型和Heard-Of模型）之间的等价关系，明确界定基于轮次的抽象在何处能够捕获异步计算，在何处不能。

Method: 通过双向模拟方法，在AMP_f和HO_f之间建立等价关系证明，使用一个中间模型来捕捉HO_f中的"静默进程"概念。证明扩展到针对非自适应对手的随机化协议。

Result: 对于n>2f，两种模型在无色任务可解性上等价；对于有色任务，仅当f=1时等价（n>2），f更大时存在分离。这种分离源于HO_f中静默进程可能导致决策不兼容。

Conclusion: 结果精确界定了基于轮次的抽象能够捕获异步计算的范围，表明规范轮次的表达能力限制是结构性的而非概率性的，为分布式计算模型的理论理解提供了重要洞见。

Abstract: We revisit the relationship between two fundamental models of distributed computation: the asynchronous message-passing model with up to $f$ crash failures ($\operatorname{AMP}_f$) and the Heard-Of model with up to $f$ message omissions ($\operatorname{HO}_f$). We show that for $n > 2f$, the two models are equivalent with respect to the solvability of colorless tasks, and that for colored tasks the equivalence holds only when $f = 1$ (and $n > 2$). The separation for larger $f$ arises from the presence of silenced processes in $\operatorname{HO}_f$, which may lead to incompatible decisions. The proofs proceed through bidirectional simulations between $\operatorname{AMP}_f$ and $\operatorname{HO}_f$ via an intermediate model that captures this notion of silencing. The results extend to randomized protocols against a non-adaptive adversary, indicating that the expressive limits of canonical rounds are structural rather than probabilistic. Together, these results delineate precisely where round-based abstractions capture asynchronous computation, and where they do not.

</details>


### [9] [OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving](https://arxiv.org/abs/2511.21862)
*Siyu Wu,Zihan Tang,Yuting Zeng,Hui Chen,Guiguang Ding,Tongxuan Liu,Ke Zhang,Hailong Yang*

Main category: cs.DC

TL;DR: 提出一种基于延迟约束的解耦架构，将集群资源分为延迟严格和延迟宽松两个池，通过瓶颈调度器和快速抢占机制，在保证在线服务SLO的同时，将离线吞吐量提升至3倍。


<details>
  <summary>Details</summary>
Motivation: LLM在在线服务和离线工作负载中部署日益增多，将两者共置在共享实例上可以提高资源利用率，但在Prefill/Decode解耦系统中，请求组合的波动会导致严重的负载不平衡，现有动态调整技术无法应对在线服务的突发流量模式。

Method: 1) 延迟约束解耦架构：根据任务延迟要求将集群资源分为延迟严格和延迟宽松两个池；2) 基于瓶颈的调度器：使用Roofline性能模型指导瓶颈感知调度；3) 快速抢占机制：严格强制执行在线请求的SLO。

Result: 在真实世界trace上的实验表明，相比现有离线系统方法，该方法在保持在线请求SLO的同时，将离线吞吐量提升至3倍。

Conclusion: 提出的延迟约束解耦架构和调度机制有效解决了Prefill/Decode解耦系统中的负载不平衡问题，在保证在线服务质量的同时显著提升了离线工作负载的吞吐量。

Abstract: Large Language Models (LLMs) are increasingly deployed in both latency-sensitive online services and cost-sensitive offline workloads. Co-locating these workloads on shared serving instances can improve resource utilization, but directly applying this approach to Prefill/Decode (P/D) disaggregated systems introduces severe load imbalance, as fluctuating request mixes alter the intrinsic P/D ratio. Existing dynamic adjustment techniques cannot keep up with the bursty traffic patterns of online services.
  We propose a latency-constraint disaggregated architecture, which separates cluster resources into latency-strict and latency-relaxed pools based on task latency requirements. This design enables flexible placement of offline decode tasks, mitigating P/D imbalance while preserving online performance. To fully exploit this flexibility, we propose (1) a bottleneck-based scheduler guided by a Roofline-based performance model for performance bottleneck based scheduling, and (2) a fast preemption mechanism that strictly enforces Service Level Objectives (SLOs) for online requests.
  Experiments on real-world traces show that compared to existing offline system approaches, our method improves offline throughput by up to 3x, while maintaining online request SLOs.

</details>


### [10] [Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN](https://arxiv.org/abs/2511.21958)
*Yiyan Zhai,Bintang Dwi Marthen,Sarath Balivada,Vamsi Sudhakar Bojji,Eric Knauft,Jitender Rohilla,Jiaqi Zuo,Quanxing Liu,Maxime Austruy,Wenguang Wang,Juncheng Yang*

Main category: cs.DC

TL;DR: Clock2Q+是针对元数据缓存设计的缓存替换算法，通过在小FIFO队列中引入相关窗口来避免误判热块，在元数据跟踪中比S3-FIFO降低高达28.5%的缺失率。


<details>
  <summary>Details</summary>
Motivation: 元数据缓存存在固有的相关引用特性，即使对应的数据访问不包含相关引用。这些相关引用会降低缓存替换算法的效果，因为它们经常被错误地分类为热块。

Method: Clock2Q+使用三个队列（类似S3-FIFO），但在小FIFO队列中引入了相关窗口，该窗口内的块不设置引用位。这种简单增强使算法能更好地区分真正的热块和相关引用。

Result: 在元数据跟踪中，Clock2Q+比第二佳算法S3-FIFO降低高达28.5%的缺失率。该算法已在VMware的vSAN和VDFS存储产品中实现，具有低CPU开销、低内存开销、多CPU扩展性好、易于调优和实现等优点。

Conclusion: Clock2Q+是针对元数据缓存优化的高效缓存替换算法，通过处理相关引用问题显著提升了性能，同时具备大规模存储系统所需的关键特性，在数据跟踪上也优于现有先进算法。

Abstract: Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.

</details>


### [11] [ZipperChain: Transmuting Trusted Third-Party Services Into Trustless Atomic Broadcast](https://arxiv.org/abs/2511.21969)
*Matteo Bjornsson,Taylor Hardin,Taylor Heinecke,Marcin Furtak,David L. Millman,Mike P. Wittie*

Main category: cs.DC

TL;DR: ZipperChain是一种无需分布式共识的分布式账本技术，通过专用服务流水线在少数节点上构建区块，实现接近网络线路速度的交易吞吐量和500毫秒级的最终确认时间。


<details>
  <summary>Details</summary>
Motivation: 传统分布式共识机制因节点间网络通信存在性能限制，ZipperChain旨在解决这一问题，提供无需分布式共识的替代方案。

Method: 采用专用服务流水线架构，在少量节点上部署，利用快速数据中心网络，将信任从广泛使用的第三方服务转移到ZipperChain的正确性保证上。

Result: 交易吞吐量接近网络线路速度，区块最终确认时间约500毫秒，且无需原生代币激励验证者社区。

Conclusion: ZipperChain通过中心化区块创建架构，在保证交易数据不可篡改、一致性和可用性的同时，避免了分布式共识的性能瓶颈，实现了高性能的分布式账本解决方案。

Abstract: Distributed ledger technologies (DLTs) rely on distributed consensus mechanisms to reach agreement over the order of transactions and to provide immutability and availability of transaction data. Distributed consensus suffers from performance limitations of network communication between participating nodes. BLOCKY ZipperChain guarantees immutability, agreement, and availability of transaction data, but without relying on distributed consensus. Instead, its construction process transfers trust from widely-used, third-party services onto ZipperChains's correctness guarantees. ZipperChain blocks are built by a pipeline of specialized services deployed on a small number of nodes connected by a fast data center network. As a result, ZipperChain transaction throughput approaches network line speeds and block finality is on the order of 500 ms. Finally, ZipperChain infrastructure creates blocks centrally and so does not need a native token to incentivize a community of verifiers.

</details>


### [12] [An Empirical Study of Cross-Language Interoperability in Replicated Data Systems](https://arxiv.org/abs/2511.22010)
*Provakar Mondal,Eli Tilevich*

Main category: cs.DC

TL;DR: 该研究比较了多语言复制数据系统中集成复制数据库(RDL)的两种策略：外部函数接口(FFI)和通用数据格式(CDF)。研究发现CDF在软件质量、延迟、内存消耗和吞吐量方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统需要在多个执行站点复制数据，业务需求和资源约束常导致不同站点使用不同编程语言。现有的复制数据库(RDL)通常只支持单一语言或特定绑定，在多语言环境中集成RDL需要专门的代码，但这些代码的软件质量和性能特性缺乏深入研究。

Method: 通过实证研究比较两种RDL集成策略：外部函数接口(FFI)和通用数据格式(CDF)。测量并比较它们的软件指标和性能，评估它们对多语言复制数据系统的适用性。

Result: 研究结果显示，采用CDF进行跨语言交互在软件质量、延迟、内存消耗和吞吐量方面具有优势。研究还验证了：(1)创建基于CDF的RDL支持混合编译、解释和托管语言；(2)通过插件扩展性增强RDL，允许在单一语言中添加功能，同时保持多语言环境中的集成。

Conclusion: 随着现代分布式系统使用多种语言，本研究为设计多语言复制数据系统中的RDL提供了新的见解，表明CDF策略在软件质量和性能方面优于传统的FFI方法。

Abstract: BACKGROUND: Modern distributed systems replicate data across multiple execution sites. Business requirements and resource constraints often necessitate mixing different languages across replica sites. To facilitate the management of replicated data, modern software engineering practices integrate special-purpose replicated data libraries (RDLs) that provide read-write access to the data and ensure its synchronization. Irrespective of the implementation languages, an RDL typically uses a single language or offers bindings to a designated one. Hence, integrating existing RDLs in multilingual environments requires special-purpose code, whose software quality and performance characteristics are poorly understood.
  AIMS: We aim to bridge this knowledge gap to understand the software quality and performance characteristics of RDL integration in multilingual environments.
  METHOD: We conduct an empirical study of two key strategies for integrating RDLs in the context of multilingual replicated data systems: foreign-function interface (FFI) and a common data format (CDF); we measure and compare their respective software metrics and performance to understand their suitability for the task at hand.
  RESULTS: Our results reveal that adopting CDF for cross-language interaction offers software quality, latency, memory consumption, and throughput advantages. We further validate our findings by (1) creating a CDF-based RDL for mixing compiled, interpreted, and managed languages; and (2) enhancing our RDL with plug-in extensibility that enables adding functionality in a single language while maintaining integration within a multilingual environment.
  CONCLUSIONS: With modern distributed systems utilizing multiple languages, our findings provide novel insights for designing RDLs in multilingual replicated data systems.

</details>


### [13] [PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel](https://arxiv.org/abs/2511.22333)
*Jinjun Yi,Zhixin Zhao,Yitao Hu,Ke Yan,Weiwei Sun,Hao Wang,Laiping Zhao,Yuhao Zhang,Wenxin Li,Keqiu Li*

Main category: cs.DC

TL;DR: PAT提出了一种前缀感知的注意力核实现，通过打包-前向-合并范式减少重复内存访问，显著降低LLM解码注意力延迟


<details>
  <summary>Details</summary>
Motivation: LLM服务中解码注意力成为内存瓶颈，而实际工作负载存在大量跨请求的层次化共享前缀（如系统提示、工具模板、RAG）。现有注意力实现无法充分利用前缀共享，导致重复加载共享KV缓存，加剧内存带宽压力

Method: 采用打包-前向-合并范式：1) 按共享前缀打包查询以减少重复内存访问；2) 运行定制化多tile核实现高资源效率；3) 应用多流前向和KV分割减少资源气泡；4) 最终合并执行在线softmax

Result: 在真实和合成工作负载评估中，PAT平均减少注意力延迟67.4%，在相同配置下TPOT降低13.6-83.4%，优于现有注意力核

Conclusion: PAT通过前缀感知的注意力核实现，有效利用工作负载中的共享前缀，显著提升LLM解码效率，可作为vLLM的即插即用插件

Abstract: LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.
  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.

</details>


### [14] [Optimality of Simultaneous Consensus with Limited Information Exchange (Extended Abstract)](https://arxiv.org/abs/2511.22380)
*Kaya Alpturer,Ron van der Meyden,Sushmita Ruj,Godfrey Wong*

Main category: cs.DC

TL;DR: 本文研究在崩溃故障模型下，针对多种有限信息交换的同步共识问题，提出新信息交换机制，实现比最优协议晚一轮但计算成本更低，并为每种信息交换推导出最优协议。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识逻辑的最优容错共识协议多采用"全信息"交换方式，消息开销大。Alpturer等人提出了有限信息交换下的最优性概念，本文在此基础上研究崩溃故障模型下的同步共识问题，探索多种文献中的有限信息交换机制。

Method: 1) 分析文献中的有限信息交换机制：FloodSet协议、带故障计数的变体、关联代理值的变体；2) 提出新的信息交换机制，能在最优协议基础上最多晚一轮达成共识，但计算成本和空间需求更低；3) 通过实现知识基程序，为每种信息交换推导出最优协议。

Result: 为每种有限信息交换机制推导出了最优协议，特别是新提出的信息交换机制能在Dwork和Moses的最优协议基础上最多晚一轮达成共识，同时显著降低计算成本和空间需求。

Conclusion: 本文在崩溃故障模型下，针对多种有限信息交换机制解决了同步共识问题，提出的新信息交换机制在延迟和效率之间取得了良好平衡，为每种信息交换找到了最优协议实现。

Abstract: Work on the development of optimal fault-tolerant Agreement protocols using the logic of knowledge has concentrated on the "full information" approach to information exchange, which is costly with respect to message size. Alpturer, Halpern, and van der Meyden (PODC 2023) introduced the notion of optimality with respect to a limited information exchange, and studied the Eventual Agreement problem in the sending omissions failure model. The present paper studies the Simultaneous Agreement problem for the crash failures model, and a number of limited information exchanges from the literature. In particular, the paper considers information exchanges from a FloodSet protocol (Lynch, Distributed Algorithms 1996), a variant of this in which agents also count the number of failures (Castañeda et al, NETYS 2017), and a variant in which agents associate each agent with a value (Raynal, PRDC 2002). A new information exchange is also introduced that enables decisions to be made at worst one round later than the optimal protocol of Dwork and Moses (I&C 88), but with lower computation cost and space requirements. By determining implementations of a knowledge based program, protocols are derived that are optimal amongst protocols for each of these information exchanges.

</details>


### [15] [OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency](https://arxiv.org/abs/2511.22481)
*Jun Wang,Yunxiang Yao,Wenwei Kuang,Runze Mao,Zhenhao Sun,Zhuang Tao,Ziyang Zhang,Dengyu Li,Jiajun Chen,Zhili Wang,Kai Cui,Congzhi Cai,Longwen Lan,Ken Zhang*

Main category: cs.DC

TL;DR: OmniInfer是一个统一的系统级加速框架，通过专家放置、缓存压缩和调度优化来最大化LLM服务效率，在DeepSeek-R1上实现了616 QPM，TPOT减少36%，TTFT减少38%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在计算密集、延迟严格和吞吐瓶颈方面给大规模服务系统带来重大挑战，需要系统级优化来提升端到端服务效率。

Method: OmniInfer包含三个互补组件：OmniPlacement用于负载感知的专家混合调度，OmniAttn用于稀疏注意力加速，OmniProxy用于解耦感知的请求调度，基于vLLM构建，通过自适应资源解耦、高效稀疏利用和全局协调实现优化。

Result: 在10节点Ascend 910C集群上评估DeepSeek-R1，OmniInfer达到616 QPM，统一框架减少TPOT 36%，OmniProxy叠加进一步减少TTFT 38%。

Conclusion: OmniInfer通过系统级优化显著提升LLM服务效率，已开源供社区使用。

Abstract: Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\%, and the superimposition of OmniProxy further slashes TTFT by 38\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).

</details>


### [16] [DisCEdge: Distributed Context Management for Large Language Models at the Edge](https://arxiv.org/abs/2511.22599)
*Mohammadreza Malekabbasi,Minghe Wang,David Bermbach*

Main category: cs.DC

TL;DR: DisCEdge是一个分布式上下文管理系统，通过将用户上下文以token序列形式存储和复制在边缘节点之间，解决了LLM在边缘部署中的上下文管理挑战，显著提升了响应时间和降低了同步开销。


<details>
  <summary>Details</summary>
Motivation: 在边缘部署LLM服务有利于延迟敏感和隐私保护应用，但LLM的无状态特性使得跨地理分布式边缘节点管理用户上下文（如会话、偏好）具有挑战性。现有解决方案（如客户端上下文存储）通常引入网络延迟和带宽开销，削弱了边缘部署的优势。

Method: 提出DisCEdge系统，以token化形式而非原始文本存储和复制用户上下文。通过将上下文维护为token序列，避免冗余计算并实现高效数据复制。在真实边缘环境中使用商用硬件实现并评估开源原型。

Result: 相比基于原始文本的系统，DisCEdge将中位响应时间提升高达14.46%，中位节点间同步开销降低高达15%。相比客户端上下文管理，将客户端请求大小中位减少90%，同时保证数据一致性。

Conclusion: DisCEdge通过token化的分布式上下文管理，有效解决了边缘LLM部署中的上下文管理问题，在保持数据一致性的同时显著提升了性能和效率。

Abstract: Deploying Large Language Model (LLM) services at the edge benefits latency-sensitive and privacy-aware applications. However, the stateless nature of LLMs makes managing user context (e.g., sessions, preferences) across geo-distributed edge nodes challenging. Existing solutions, such as client-side context storage, often introduce network latency and bandwidth overhead, undermining the advantages of edge deployment.
  We propose DisCEdge, a distributed context management system that stores and replicates user context in tokenized form across edge nodes. By maintaining context as token sequences rather than raw text, our system avoids redundant computation and enables efficient data replication. We implement and evaluate an open-source prototype in a realistic edge environment with commodity hardware. We show DisCEdge improves median response times by up to 14.46% and lowers median inter-node synchronization overhead by up to 15% compared to a raw-text-based system. It also reduces client request sizes by a median of 90% compared to client-side context management, while guaranteeing data consistency.

</details>


### [17] [Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware](https://arxiv.org/abs/2511.22779)
*Shijie Yan,Douglas Dwyer,David R. Kaeli,Qianqian Fang*

Main category: cs.DC

TL;DR: RT-MMC：利用GPU光线追踪核心硬件加速的蒙特卡洛算法，显著提升网格蒙特卡洛模拟速度，简化工作流程


<details>
  <summary>Details</summary>
Motivation: 传统网格蒙特卡洛（MMC）方法虽然精度高，但频繁的射线-边界相交测试计算成本高，限制了性能。现代GPU的光线追踪核心（RT-cores）提供了硬件加速能力，但尚未被充分利用于生物光子学模拟。

Method: 基于NVIDIA OptiX平台开发RT-MMC算法，将图形光线追踪管道扩展到浑浊介质中的体积光线追踪。利用RT-cores的硬件加速能力，避免复杂的四面体网格生成，同时支持宽场光源而无需复杂网格重划分。

Result: RT-MMC与传统软件光线追踪MMC算法结果高度一致，在不同GPU架构上实现1.5倍到4.5倍的加速。性能提升显著增强了MMC在常规模拟中的实用性。

Conclusion: 从软件转向硬件光线追踪不仅大大简化了MMC模拟工作流程，还带来了显著的性能提升。随着光线追踪硬件的快速普及，这种加速效果预计会进一步增强。在定量MMC模拟中采用图形光线追踪管道能够利用新兴硬件资源，惠及广泛的生物光子学应用。

Abstract: Significance: Monte Carlo (MC) methods are the gold-standard for modeling light-tissue interactions due to their accuracy. Mesh-based MC (MMC) offers enhanced precision for complex tissue structures using tetrahedral mesh models. Despite significant speedups achieved on graphics processing units (GPUs), MMC performance remains hindered by the computational cost of frequent ray-boundary intersection tests.
  Aim: We propose a highly accelerated MMC algorithm, RT-MMC, that leverages the hardware-accelerated ray traversal and intersection capabilities of ray-tracing cores (RT-cores) on modern GPUs.
  Approach: Implemented using NVIDIA's OptiX platform, RT-MMC extends graphics ray-tracing pipelines towards volumetric ray-tracing in turbid media, eliminating the need for challenging tetrahedral mesh generation while delivering significant speed improvements through hardware acceleration. It also intrinsically supports wide-field sources without complex mesh retesselation.
  Results: RT-MMC demonstrates excellent agreement with traditional software-ray-tracing MMC algorithms while achieving 1.5x to 4.5x speedups across multiple GPU architectures. These performance gains significantly enhance the practicality of MMC for routine simulations.
  Conclusion: Migration from software- to hardware-based ray-tracing not only greatly simplifies MMC simulation workflows, but also results in significant speedups that are expected to increase further as ray-tracing hardware rapidly gains adoption. Adoption of graphics ray-tracing pipelines in quantitative MMC simulations enables leveraging of emerging hardware resources and benefits a wide range of biophotonics applications.

</details>


### [18] [Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems](https://arxiv.org/abs/2511.22880)
*Shashwat Jaiswal,Shrikara Arun,Anjaly Parayil,Ankur Mallick,Spyros Mastorakis,Alind Khare,Chloi Alverti,Renee St Amant,Chetan Bansal,Victor Rühle,Josep Torrellas*

Main category: cs.DC

TL;DR: LoRAServe是一个动态适配器放置和路由框架，专门解决LoRA服务中因适配器大小差异导致的性能倾斜问题，通过动态重平衡和远程访问优化，显著提升吞吐量并减少GPU需求。


<details>
  <summary>Details</summary>
Motivation: 当前LoRA服务系统在处理多租户环境中不同大小的适配器时，由于忽略适配器大小（秩）的差异，导致严重的性能倾斜和GPU资源利用不足，需要更多GPU来满足服务级别目标。

Method: 提出LoRAServe框架，采用工作负载感知的动态适配器放置和路由策略，通过动态重平衡适配器在GPU间的分布，并利用GPU Direct RDMA进行远程访问，以优化资源利用。

Result: 在真实生产环境测试中，相比现有最优系统，LoRAServe实现了最高2倍的吞吐量提升，最高9倍的首令牌时间降低，并在满足SLO约束下减少了50%的GPU使用。

Conclusion: LoRAServe通过动态处理适配器大小差异，有效解决了LoRA服务中的性能倾斜问题，显著提升了多租户环境下的资源利用效率和服务性能。

Abstract: Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\times$ higher throughput, up to 9$\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.

</details>


### [19] [Areon: Latency-Friendly and Resilient Multi-Proposer Consensus](https://arxiv.org/abs/2511.23025)
*Álvaro Castro-Castilla,Marcin Pawlowski,Hong-Sheng Zhou*

Main category: cs.DC

TL;DR: Areon是一个基于DAG的多提议者PoS共识协议，通过滑动窗口内的块引用形成最大反链作为并行投票，使用CCA-local fork choice解决冲突，实现了低延迟最终性和低重组频率。


<details>
  <summary>Details</summary>
Motivation: 传统链式PoS协议在部分同步网络下存在延迟和重组问题，需要设计更鲁棒、低延迟的共识机制来改善最终性性能。

Method: 提出基于DAG的多提议者协议，使用滑动窗口内的块引用形成最大反链作为并行投票，通过CCA-local、窗口过滤的分叉选择规则比较子DAG权重，结合Tip-Boundedness结构不变量确保有界宽度前沿。

Result: 证明了(k,ε)-最终性定理，通过离散事件模拟显示Areon-Base相比Ouroboros Praos在各种对抗股权和网络延迟下实现了有界延迟最终性，重组频率和深度更低。

Conclusion: Areon协议通过DAG结构和多提议者设计有效改善了PoS共识的延迟和鲁棒性，为低延迟最终性提供了可行方案，但更丰富的交易选择和冗余策略需要未来工作。

Abstract: We present Areon, a family of latency-friendly, stake-weighted, multi-proposer proof-of-stake consensus protocols. By allowing multiple proposers per slot and organizing blocks into a directed acyclic graph (DAG), Areon achieves robustness under partial synchrony. Blocks reference each other within a sliding window, forming maximal antichains that represent parallel ``votes'' on history. Conflicting subDAGs are resolved by a closest common ancestor (CCA)-local, window-filtered fork choice that compares the weight of each subDAG -- the number of recent short references -- and prefers the heavier one. Combined with a structural invariant we call Tip-Boundedness (TB), this yields a bounded-width frontier and allows honest work to aggregate quickly.
  We formalize an idealized protocol (Areon-Ideal) that abstracts away network delay and reference bounds, and a practical protocol (Areon-Base) that adds VRF-based eligibility, bounded short and long references, and application-level validity and conflict checks at the block level. On top of DAG analogues of the classical common-prefix, chain-growth, and chain-quality properties, we prove a backbone-style $(k,\varepsilon)$-finality theorem that calibrates confirmation depth as a function of the window length and target tail probability. We focus on consensus at the level of blocks; extending the framework to richer transaction selection, sampling, and redundancy policies is left to future work.
  Finally, we build a discrete-event simulator and compare Areon-Base against a chain-based baseline (Ouroboros Praos) under matched block-arrival rates. Across a wide range of adversarial stakes and network delays, Areon-Base achieves bounded-latency finality with consistently lower reorganization frequency and depth.

</details>


### [20] [Communication-Computation Pipeline Parallel Split Learning over Wireless Edge Networks](https://arxiv.org/abs/2511.23167)
*Chenyu Liu,Zhaoyang Zhang,Zirui Chen,Zhaohui Yang*

Main category: cs.DC

TL;DR: 提出C²P²SL方法，通过流水线并行化通信和计算过程，显著减少分割学习在无线网络中的训练时间


<details>
  <summary>Details</summary>
Motivation: 传统分割学习虽然保护了本地数据隐私，但其计算和通信过程是顺序执行的，导致系统效率有限。需要克服这一限制以提高训练效率。

Method: 将分布式训练中的流水线并行技术应用于无线网络中的分割学习，将UE和BS的通信与计算过程视为整体流水线，在不同微批次间实现流水线并行化。通过联合优化任务分割和资源分配问题，并设计基于交替优化的解决方案。

Result: 实验结果表明，C²P²SL在不同通信条件下能保持收敛精度的同时，显著减少系统训练时间超过38%。

Conclusion: C²P²SL通过流水线并行化有效解决了传统分割学习中的效率瓶颈问题，为无线网络中的分布式学习提供了高效解决方案。

Abstract: Split learning (SL) offloads main computing tasks from multiple resource-constrained user equippments (UEs) to the base station (BS), while preserving local data privacy. However, its computation and communication processes remain sequential, resulting in limited system efficiency. To overcome this limitation, this paper applies pipeline parallelism (PP) of distributed training to SL in wireless networks, proposing the so-called communication-computation pipeline parallel split learning (C$^2$P$^2$SL). By considering the communicating and computing processes of UEs and BS as an overall pipeline, C$^2$P$^2$SL achieves pipeline parallelization among different micro-batches which are split from each batch of data samples. The overlap of communication and computation in this way significantly reduces the total training time. Given that training efficiency is affected by position of cutting layer and heterogeneity of the UEs, we formulate a joint optimization problem of task split and resource allocation, and design a solution based on alternating optimization. Experimental results demonstrate that C$^2$P$^2$SL significantly reduces system training time by over 38\% while maintaining convergence accuracy under different communication conditions.

</details>


### [21] [Beyond 2-Edge-Connectivity: Algorithms and Impossibility for Content-Oblivious Leader Election](https://arxiv.org/abs/2511.23297)
*Yi-Jun Chang,Lyuting Chen,Haoran Zhou*

Main category: cs.DC

TL;DR: 在内容无关通信模型下，利用拓扑知识可实现某些图中的领导者选举，但拓扑对称性和拓扑知识精确度是关键限制因素。


<details>
  <summary>Details</summary>
Motivation: 内容无关通信模型是一种极弱的通信形式，节点只能发送异步、无内容的脉冲。先前研究表明，在单边上无法计算任何非常数函数，这似乎排除了许多自然图问题。本文探索在已知网络拓扑的情况下，是否能在更广泛的图中实现领导者选举。

Method: 1. 证明关于边对称的图无法进行随机终止领导者选举；2. 为非对称树设计基于拓扑知识的静默终止算法；3. 分析拓扑知识精确度的必要性，比较精确拓扑知识与拓扑族知识的差异。

Result: 1. 边对称图无法进行领导者选举；2. 非对称树可实现领导者选举，消息复杂度为O(n²)；3. 偶直径树仅需直径知识即可选举，消息复杂度O(nr)；4. 拓扑知识精确度至关重要：已知确切拓扑时可选举，仅知拓扑族时不可选举。

Conclusion: 在内容无关通信模型中，拓扑知识是实现领导者选举的关键因素。拓扑对称性构成根本障碍，而拓扑知识的精确度直接影响选举可能性。这为弱通信模型下的分布式计算提供了新的理论边界。

Abstract: The content-oblivious model, introduced by Censor-Hillel, Cohen, Gelles, and Sel (PODC 2022; Distributed Computing 2023), captures an extremely weak form of communication where nodes can only send asynchronous, content-less pulses. Censor-Hillel, Cohen, Gelles, and Sel showed that no non-constant function $f(x,y)$ can be computed correctly by two parties using content-oblivious communication over a single edge, where one party holds $x$ and the other holds $y$. This seemingly ruled out many natural graph problems on non-2-edge-connected graphs.
  In this work, we show that, with the knowledge of network topology $G$, leader election is possible in a wide range of graphs.
  Impossibility: Graphs symmetric about an edge admit no randomized terminating leader election algorithm, even when nodes have unique identifiers and full knowledge of $G$.
  Leader election algorithms: Trees that are not symmetric about any edge admit a quiescently terminating leader election algorithm with topology knowledge, even in anonymous networks, using $O(n^2)$ messages, where $n$ is the number of nodes. Moreover, even-diameter trees admit a terminating leader election given only the knowledge of the network diameter $D = 2r$, with message complexity $O(nr)$.
  Necessity of topology knowledge: In the family of graphs $\mathcal{G} = \{P_3, P_5\}$, both the 3-path $P_3$ and the 5-path $P_5$ admit a quiescently terminating leader election if nodes know the topology exactly. However, if nodes only know that the underlying topology belongs to $\mathcal{G}$, then terminating leader election is impossible.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [22] [Platinum: Path-Adaptable LUT-Based Accelerator Tailored for Low-Bit Weight Matrix Multiplication](https://arxiv.org/abs/2511.21910)
*Haoxuan Shan,Cong Guo,Chiyue Wei,Feng Cheng,Junyao Zhang,Hai,Li,Yiran Chen*

Main category: cs.AR

TL;DR: Platinum：一种基于查找表的轻量级ASIC加速器，用于整数权重混合精度矩阵乘法，通过离线生成构造路径减少开销，支持比特串行和三元权重自适应切换，在BitNet b1.58-3B上实现显著加速和能效提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速扩展需要更高效硬件，超低位量化提供了效率与性能的平衡，但现有基于查找表的方法存在构造开销大、依赖比特串行计算等问题，对三元权重网络不够优化。

Method: 提出Platinum加速器，采用离线生成的构造路径减少查找表构建开销，支持自适应路径切换实现通用比特串行和优化的三元权重执行，设计轻量级ASIC架构。

Result: 在BitNet b1.58-3B上，相比SpikingEyeriss、Prosperity和16线程T-MAC（CPU）分别实现73.6倍、4.09倍和2.15倍加速，能耗降低32.4倍、3.23倍和20.9倍，芯片面积仅0.96mm²。

Conclusion: Platinum展示了基于查找表的ASIC作为超低位神经网络在边缘平台上的高效、可扩展解决方案的潜力，为大规模语言模型部署提供了硬件优化路径。

Abstract: The rapid scaling of large language models demands more efficient hardware. Quantization offers a promising trade-off between efficiency and performance. With ultra-low-bit quantization, there are abundant opportunities for results reuse, and thus it can be boosted with lookup tables (LUTs) based acceleration. However, existing LUT-based methods suffer from computation and hardware overheads for LUT construction, and rely solely on bit-serial computation, which is suboptimal for ternary-weight networks. We propose Platinum, a lightweight ASIC accelerator for integer weight mixed-precision matrix multiplication (mpGEMM) using LUTs. Platinum reduces LUT construction overhead via offline-generated construction paths and supports both general bit-serial and optimized ternary-weight execution through adaptive path switching. On BitNet b1.58-3B, Platinum achieves up to 73.6x, 4.09x, and 2.15x speedups over SpikingEyeriss, Prosperity, and 16-thread T-MAC (CPU), respectively, along with energy reductions of 32.4x, 3.23x, and 20.9x, all within a 0.96mm2 chip area. This demonstrates the potential of LUT-based ASICs as efficient, scalable solutions for ultra-low-bit neural networks on edge platforms.

</details>


### [23] [CADC: Crossbar-Aware Dendritic Convolution for Efficient In-memory Computing](https://arxiv.org/abs/2511.22166)
*Shuai Dong,Junyi Yang,Ye Ke,Hongyang Shang,Arindam Basu*

Main category: cs.AR

TL;DR: 提出CADC方法，通过在交叉阵列计算中嵌入非线性树突函数（将负值归零）来大幅增加部分和的稀疏性，减少系统开销并提升能效


<details>
  <summary>Details</summary>
Motivation: CNN在交叉阵列内存计算架构中加速时，大卷积层需要跨多个交叉阵列分区，产生大量部分和，导致额外的缓冲、传输和累加开销，引入显著的系统级开销

Method: 提出交叉阵列感知的树突卷积（CADC），受神经科学中树突计算原理启发，在交叉阵列计算中直接嵌入非线性树突函数（将负值归零），从而大幅增加部分和的稀疏性

Result: CADC显著减少部分和：LeNet-5减少80%，ResNet-18减少54%，VGG-16减少66%，SNN减少88%。实现零压缩和零跳过，减少29.3%的缓冲传输开销和47.9%的累加开销，同时最小化ADC量化噪声积累，精度损失很小

Conclusion: CADC在SRAM-based IMC实现中达到2.15 TOPS和40.8 TOPS/W的能效，相比现有IMC加速器实现11-18倍加速和1.9-22.9倍的能效提升，为CNN加速提供了高效的解决方案

Abstract: Convolutional neural networks (CNNs) are computationally intensive and often accelerated using crossbar-based in-memory computing (IMC) architectures. However, large convolutional layers must be partitioned across multiple crossbars, generating numerous partial sums (psums) that require additional buffer, transfer, and accumulation, thus introducing significant system-level overhead. Inspired by dendritic computing principles from neuroscience, we propose crossbar-aware dendritic convolution (CADC), a novel approach that dramatically increases sparsity in psums by embedding a nonlinear dendritic function (zeroing negative values) directly within crossbar computations. Experimental results demonstrate that CADC significantly reduces psums, eliminating 80% in LeNet-5 on MNIST, 54% in ResNet-18 on CIFAR-10, 66% in VGG-16 on CIFAR-100, and up to 88% in spiking neural networks (SNN) on the DVS Gesture dataset. The induced sparsity from CADC provides two key benefits: (1) enabling zero-compression and zero-skipping, thus reducing buffer and transfer overhead by 29.3% and accumulation overhead by 47.9%; (2) minimizing ADC quantization noise accumulation, resulting in small accuracy degradation - only 0.01% for LeNet-5, 0.1% for ResNet-18, 0.5% for VGG-16, and 0.9% for SNN. Compared to vanilla convolution (vConv), CADC exhibits accuracy changes ranging from +0.11% to +0.19% for LeNet-5, -0.04% to -0.27% for ResNet-18, +0.99% to +1.60% for VGG-16, and -0.57% to +1.32% for SNN, across crossbar sizes from 64x64 to 256x256. Ultimately, a SRAM-based IMC implementation of CADC achieves 2.15 TOPS and 40.8 TOPS/W for ResNet-18 (4/2/4b), realizing an 11x-18x speedup and 1.9x-22.9x improvement in energy efficiency compared to existing IMC accelerators.

</details>


### [24] [Aquas: Enhancing Domain Specialization through Holistic Hardware-Software Co-Optimization based on MLIR](https://arxiv.org/abs/2511.22267)
*Yuyang Zou,Youwei Xiao,Yansong Xu,Chenyun Yin,Yuhao Luo,Yitian Sun,Ruifan Xu,Renze Chen,Yun Liang*

Main category: cs.AR

TL;DR: Aquas是一个基于MLIR的硬件-软件协同设计框架，通过集成突发DMA引擎和高级HLS优化来提升RISC-V ASIP性能，并结合基于e-graph的可重定向编译器方法，在点云处理和LLM推理等实际工作负载上实现最高9.27倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有开源RISC-V生态系统中的ASIP框架存在性能受限问题，主要原因是硬件合成能力有限和编译器支持僵化，这阻碍了RISC-V ASIP在各种应用中的专业化潜力。

Method: 提出Aquas框架：1）硬件方面：通过突发DMA引擎增强快速内存访问能力，并采用高级HLS优化；2）编译器方面：提出基于e-graph的可重定向方法，配备新型匹配引擎以实现高效指令匹配。整个框架基于MLIR构建。

Result: 在真实工作负载评估中，包括点云处理和LLM推理，Aquas框架实现了最高9.27倍的性能加速。

Conclusion: Aquas通过硬件-软件协同设计方法有效解决了现有RISC-V ASIP框架的性能限制问题，为应用特定处理器开发提供了更高效的解决方案。

Abstract: Application-Specific Instruction-Set Processors (ASIPs) built on the RISC-V architecture offer specialization opportunities for various applications. However, existing frameworks from the open-source RISC-V ecosystem suffer from limited performance due to restricted hardware synthesis and rigid compiler support. To address these challenges, we introduce Aquas, a holistic hardware-software co-design framework built upon MLIR. Aquas enhances ASIP synthesis with fast memory access capability via a burst DMA engine and advanced high-level synthesis (HLS) optimizations. On the compiler side, we propose an e-graph based retargetable approach with a novel matching engine for efficient instruction matching. Evaluation demonstrates up to 9.27x speedup on real-world workloads, including point cloud processing and LLM inference.

</details>


### [25] [FADiff: Fusion-Aware Differentiable Optimization for DNN Scheduling on Tensor Accelerators](https://arxiv.org/abs/2511.22348)
*Shuao Jia,Zichao Ling,Chen Bai,Kang Zhao,Jianwang Zhai*

Main category: cs.AR

TL;DR: FADiff是一个基于梯度的优化框架，用于自动寻找DNN在张量加速器上的最优层内映射和层间融合策略，以提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 在张量加速器上高效部署DNN（如LLM）对最大化计算效率至关重要，但由于层内映射和层间融合的复杂交互形成了巨大设计空间，实现这一目标具有挑战性。

Method: 首先构建统一且可微的分析成本模型，准确预测单层映射和各种层融合策略的能耗和延迟；然后将离散约束编码到损失函数中，采用基于梯度的方法高效探索设计空间，确定映射和融合的最优联合策略。

Result: 实验结果表明FADiff优于现有方法，在能耗和延迟方面实现了更好的优化效果。

Conclusion: FADiff框架能够有效解决DNN在张量加速器上的部署优化问题，通过梯度方法自动寻找最优的映射和融合策略。

Abstract: Efficient deployment of Deep Neural Networks (DNNs), such as Large Language Models (LLMs), on tensor accelerators is essential for maximizing computational efficiency in modern AI systems. However, achieving this is challenging due to the enormous and complex design space created by the interaction of intra-layer mapping and inter-layer fusion. In this work, we present FADiff, a gradient-based optimization framework capable of automatically identifying high-quality intra-layer mapping and inter-layer fusion strategies to accelerate inference for DNN workloads. We first construct a unified and differentiable analytical cost model, which accurately predicts the energy and latency of both single-layer mappings and various layer fusion strategies. Then, by encoding discrete constraints into the loss function, we employ a gradient-based approach to efficiently explore the vast design space, determining the optimal joint strategy for mapping and fusion. Experimental results demonstrate the superiority of FADiff, achieving better optimization in terms of energy and latency compared to existing methods.

</details>


### [26] [3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison](https://arxiv.org/abs/2511.22551)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asad*

Main category: cs.AR

TL;DR: 提出3RSeT方案，通过选择性标签比较减少STT-MRAM缓存中的读取干扰错误率，降低71.8%错误率，提升3.6倍MTTF，减少62.1%能耗，性能无损且面积开销小于0.4%。


<details>
  <summary>Details</summary>
Motivation: STT-MRAM作为SRAM替代品具有低功耗、高密度等优势，但读取干扰错误是严重可靠性挑战。现有工作未解决缓存集合中并行比较操作导致的所有标签同时访问问题，这会极大增加读取干扰率。

Method: 提出3RSeT方案：1）主动禁用无命中机会的标签；2）利用访问请求中标签的低有效位进行选择性比较；3）消除大部分标签读取操作。

Result: 使用gem5全系统周期精确模拟器评估：标签阵列读取干扰率降低71.8%，平均故障时间提升3.6倍，能耗降低62.1%，性能无损失，面积开销小于0.4%。

Conclusion: 3RSeT方案有效解决了STT-MRAM缓存中的读取干扰问题，通过低成本的选择性标签比较机制显著提升可靠性并降低能耗，具有实际应用价值。

Abstract: Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead.

</details>


### [27] [The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference](https://arxiv.org/abs/2511.22889)
*Fang Li*

Main category: cs.AR

TL;DR: ITA架构通过将LLM权重编码到ASIC物理电路中，消除内存层次结构，解决边缘设备上的"内存墙"问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在消费边缘设备上的部署受到"内存墙"的限制——每次生成token都需要从DRAM获取数十亿模型权重，带来巨大的带宽和能耗成本。当前架构将模型权重视为可变软件数据，为保持通用可编程性付出了巨大能耗代价。

Method: 提出不可变张量架构(ITA)，将模型权重视为物理电路拓扑而非数据。通过将参数直接编码到成熟节点ASIC(28nm/40nm)的金属互连和逻辑中，完全消除内存层次结构。采用"分脑"系统设计：主机CPU管理动态KV缓存操作，ITA ASIC作为无状态的ROM嵌入式数据流引擎。

Result: 该方法理论上能够消除模型权重访问的DRAM带宽和能耗成本，实现更高效的边缘LLM部署。

Conclusion: ITA代表了一种范式转变，将模型权重从软件数据重新定义为硬件电路拓扑，为边缘设备上的高效LLM推理提供了新的架构解决方案。

Abstract: The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the "Memory Wall" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a "Split-Brain" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.

</details>


### [28] [Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation](https://arxiv.org/abs/2511.23011)
*Yanjing Wang,Lizhou Wu,Sunfeng Gao,Yibo Tang,Junhui Luo,Zicong Wang,Yang Ou,Dezun Dong,Nong Xiao,Mingche Lai*

Main category: cs.AR

TL;DR: Cohet：首个基于CXL的缓存一致性异构计算框架，通过解耦计算与内存资源形成CPU和XPU池，共享统一内存池，显著提升异构系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于PCIe的异构计算系统存在细粒度主机-设备交互效率低下和编程模型复杂的问题。CXL作为新兴的缓存一致性互连标准具有变革潜力，但相关研究受限于平台稀缺、生态不成熟和应用前景不明朗。

Method: 提出Cohet框架，将计算和内存资源解耦形成CPU和XPU池，共享统一缓存一致性内存池。通过标准malloc/mmap接口暴露给计算线程，由操作系统智能管理异构资源。同时开发了全系统周期级模拟器SimCXL，支持所有CXL子协议和设备类型建模。

Result: CXL.cache相比DMA传输在缓存行粒度上延迟降低68%，带宽提升14.4倍。基于CXL的NIC相比PCIe-NIC在远程原子操作卸载上实现5.5-40.2倍加速，在RPC序列化/反序列化卸载上平均加速1.86倍。

Conclusion: Cohet是首个CXL驱动的缓存一致性异构计算框架，通过统一内存池和标准接口简化了异构编程，SimCXL模拟器为CXL研究提供了重要工具，展示了CXL在提升异构系统性能方面的巨大潜力。

Abstract: Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading.

</details>


### [29] [GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration](https://arxiv.org/abs/2511.23203)
*Jordi Fornt,Pau Fontova-Musté,Adrian Gras,Omar Lahyani,Martí Caro,Jaume Abella,Francesc Moll,Josep Altet*

Main category: cs.AR

TL;DR: 提出GAV技术，结合欠压和位串行计算，实现灵活近似计算，并设计GAVINA架构，在ResNet-18上实现20%能效提升且精度损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 电压过缩放（欠压）作为近似计算技术具有吸引力，但高错误率阻碍其广泛应用，且现有欠压加速器基于8位算术，无法与先进低精度架构竞争。

Method: 提出GAV技术，结合欠压和位串行计算，选择性地对最低有效位组合进行激进降压；实现GAVINA架构，支持任意混合精度和灵活欠压。

Result: GAVINA在最激进配置下能效达89 TOP/sW；GAV技术通过欠压实现20%能效提升，在ResNet-18上精度损失可忽略。

Conclusion: GAV技术成功克服了欠压技术的高错误率问题，实现了能效显著提升且精度损失可接受，为低功耗DNN加速提供了新方案。

Abstract: Voltage overscaling, or undervolting, is an enticing approximate technique in the context of energy-efficient Deep Neural Network (DNN) acceleration, given the quadratic relationship between power and voltage. Nevertheless, its very high error rate has thwarted its general adoption. Moreover, recent undervolting accelerators rely on 8-bit arithmetic and cannot compete with state-of-the-art low-precision (<8b) architectures. To overcome these issues, we propose a new technique called Guarded Aggressive underVolting (GAV), which combines the ideas of undervolting and bit-serial computation to create a flexible approximation method based on aggressively lowering the supply voltage on a select number of least significant bit combinations. Based on this idea, we implement GAVINA (GAV mIxed-precisioN Accelerator), a novel architecture that supports arbitrary mixed precision and flexible undervolting, with an energy efficiency of up to 89 TOP/sW in its most aggressive configuration. By developing an error model of GAVINA, we show that GAV can achieve an energy efficiency boost of 20% via undervolting, with negligible accuracy degradation on ResNet-18.

</details>
