{"id": "2510.15095", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15095", "abs": "https://arxiv.org/abs/2510.15095", "authors": ["Md Sabbir Hossain Polak", "David Troendle", "Byunghyun Jang"], "title": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table for GPUs", "comment": null, "summary": "Hash tables are essential building blocks in data-intensive applications, yet\nexisting GPU implementations often struggle with concurrent updates, high load\nfactors, and irregular memory access patterns. We present Hive hash table, a\nhigh-performance, warp-cooperative and dynamically resizable GPU hash table\nthat adapts to varying workloads without global rehashing.\n  Hive hash table makes three key contributions. First, a cache-aligned packed\nbucket layout stores key-value pairs as 64-bit words, enabling coalesced memory\naccess and atomic updates via single-CAS operations. Second, warp-synchronous\nconcurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and\nWarp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic\noperation per warp while ensuring lock-free progress. Third, a\nload-factor-aware dynamic resizing strategy expands or contracts capacity in\nwarp-parallel K-bucket batches using linear hashing, maintaining balanced\noccupancy. To handle insertions under heavy contention, Hive hash table employs\na four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and\noverflow-stash fallback. This design provides lock-free fast paths and bounded\nrecovery cost under contention determined by a fixed eviction depth, while\neliminating ABA hazards during concurrent updates.\n  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains\nload factors up to 95% while delivering 1.5-2x higher throughput than\nstate-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed\ninsert-delete-lookup workloads. On balanced workload, Hive hash table reaches\n3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability\nand efficiency for GPU-accelerated data processing.", "AI": {"tldr": "Hive\u54c8\u5e0c\u8868\u662f\u4e00\u79cd\u9ad8\u6027\u80fd\u3001\u652f\u6301\u52a8\u6001\u8c03\u6574\u5927\u5c0f\u7684GPU\u54c8\u5e0c\u8868\uff0c\u901a\u8fc7\u7f13\u5b58\u5bf9\u9f50\u7684\u6876\u5e03\u5c40\u3001warp\u540c\u6b65\u5e76\u53d1\u534f\u8bae\u548c\u8d1f\u8f7d\u611f\u77e5\u52a8\u6001\u8c03\u6574\u7b56\u7565\uff0c\u5728\u4fdd\u630195%\u9ad8\u8d1f\u8f7d\u56e0\u5b50\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u6bd4\u73b0\u6709GPU\u54c8\u5e0c\u8868\u9ad81.5-2\u500d\u7684\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709GPU\u54c8\u5e0c\u8868\u5728\u5e76\u53d1\u66f4\u65b0\u3001\u9ad8\u8d1f\u8f7d\u56e0\u5b50\u548c\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u9002\u5e94\u53d8\u5316\u5de5\u4f5c\u8d1f\u8f7d\u4e14\u65e0\u9700\u5168\u5c40\u91cd\u54c8\u5e0c\u7684\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u7f13\u5b58\u5bf9\u9f50\u7684\u6253\u5305\u6876\u5e03\u5c40\uff0c\u5c06\u952e\u503c\u5bf9\u5b58\u50a8\u4e3a64\u4f4d\u5b57\uff0c\u652f\u6301\u5408\u5e76\u5185\u5b58\u8bbf\u95ee\u548c\u5355CAS\u539f\u5b50\u64cd\u4f5c\uff1b2. Warp\u540c\u6b65\u5e76\u53d1\u534f\u8bae(WABC\u548cWCME)\uff0c\u5c06\u4e89\u7528\u51cf\u5c11\u5230\u6bcf\u4e2awarp\u4e00\u4e2a\u539f\u5b50\u64cd\u4f5c\uff1b3. \u8d1f\u8f7d\u56e0\u5b50\u611f\u77e5\u7684\u52a8\u6001\u8c03\u6574\u7b56\u7565\uff0c\u4f7f\u7528\u7ebf\u6027\u54c8\u5e0c\u4ee5warp\u5e76\u884cK\u6876\u6279\u6b21\u6269\u5c55\u6216\u6536\u7f29\u5bb9\u91cf\uff1b4. \u56db\u6b65\u63d2\u5165\u7b56\u7565\u5904\u7406\u9ad8\u4e89\u7528\u60c5\u51b5\u3002", "result": "\u5728NVIDIA RTX 4090\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHive\u54c8\u5e0c\u8868\u5728\u6df7\u5408\u63d2\u5165-\u5220\u9664-\u67e5\u627e\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u6bd4\u6700\u5148\u8fdb\u7684GPU\u54c8\u5e0c\u8868(Slab-Hash\u3001DyCuckoo\u3001WarpCore)\u541e\u5410\u91cf\u9ad81.5-2\u500d\uff0c\u8d1f\u8f7d\u56e0\u5b50\u53ef\u8fbe95%\u3002\u5728\u5e73\u8861\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u8fbe\u523035\u4ebf\u6b21\u66f4\u65b0/\u79d2\u548c\u8fd140\u4ebf\u6b21\u67e5\u627e/\u79d2\u3002", "conclusion": "Hive\u54c8\u5e0c\u8868\u901a\u8fc7\u521b\u65b0\u7684warp\u534f\u4f5c\u8bbe\u8ba1\u548c\u52a8\u6001\u8c03\u6574\u673a\u5236\uff0c\u4e3aGPU\u52a0\u901f\u6570\u636e\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u54c8\u5e0c\u8868\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e76\u53d1\u6027\u80fd\u548c\u8d1f\u8f7d\u80fd\u529b\u3002"}}
{"id": "2510.15122", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15122", "abs": "https://arxiv.org/abs/2510.15122", "authors": ["Fran\u00e7ois Ezard", "Can Umut Ileri", "J\u00e9r\u00e9mie Decouchant"], "title": "NEMO: Faster Parallel Execution for Highly Contended Blockchain Workloads (Full version)", "comment": "This is the full version of a paper that will appear at the 7th\n  Conference on Blockchain Research & Applications for Innovative Networks and\n  Services (BRAINS 2025)", "summary": "Following the design of more efficient blockchain consensus algorithms, the\nexecution layer has emerged as the new performance bottleneck of blockchains,\nespecially under high contention. Current parallel execution frameworks either\nrely on optimistic concurrency control (OCC) or on pessimistic concurrency\ncontrol (PCC), both of which see their performance decrease when workloads are\nhighly contended, albeit for different reasons. In this work, we present NEMO,\na new blockchain execution engine that combines OCC with the object data model\nto address this challenge. NEMO introduces four core innovations: (i) a greedy\ncommit rule for transactions using only owned objects; (ii) refined handling of\ndependencies to reduce re-executions; (iii) the use of incomplete but\nstatically derivable read/write hints to guide execution; and (iv) a\npriority-based scheduler that favors transactions that unblock others. Through\nsimulated execution experiments, we demonstrate that NEMO significantly reduces\nredundant computation and achieves higher throughput than representative\napproaches. For example, with 16 workers NEMO's throughput is up to 42% higher\nthan the one of Block-STM, the state-of-the-art OCC approach, and 61% higher\nthan the pessimistic concurrency control baseline used.", "AI": {"tldr": "NEMO\u662f\u4e00\u4e2a\u65b0\u7684\u533a\u5757\u94fe\u6267\u884c\u5f15\u64ce\uff0c\u7ed3\u5408\u4e50\u89c2\u5e76\u53d1\u63a7\u5236(OCC)\u548c\u5bf9\u8c61\u6570\u636e\u6a21\u578b\u6765\u89e3\u51b3\u9ad8\u7ade\u4e89\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u533a\u5757\u94fe\u5171\u8bc6\u7b97\u6cd5\u6548\u7387\u63d0\u5347\uff0c\u6267\u884c\u5c42\u6210\u4e3a\u65b0\u7684\u6027\u80fd\u74f6\u9888\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ade\u4e89\u573a\u666f\u4e0b\u3002\u73b0\u6709\u7684\u5e76\u884c\u6267\u884c\u6846\u67b6(\u4e50\u89c2\u6216\u60b2\u89c2\u5e76\u53d1\u63a7\u5236)\u5728\u9ad8\u7ade\u4e89\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u6027\u80fd\u90fd\u4f1a\u4e0b\u964d\u3002", "method": "NEMO\u5f15\u5165\u56db\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1) \u4f7f\u7528\u81ea\u6709\u5bf9\u8c61\u7684\u8d2a\u5a6a\u63d0\u4ea4\u89c4\u5219\uff1b2) \u7cbe\u70bc\u4f9d\u8d56\u5904\u7406\u4ee5\u51cf\u5c11\u91cd\u6267\u884c\uff1b3) \u4f7f\u7528\u9759\u6001\u53ef\u63a8\u5bfc\u7684\u8bfb/\u5199\u63d0\u793a\u6307\u5bfc\u6267\u884c\uff1b4) \u57fa\u4e8e\u4f18\u5148\u7ea7\u7684\u8c03\u5ea6\u5668\uff0c\u4f18\u5148\u5904\u7406\u80fd\u89e3\u9501\u5176\u4ed6\u4ea4\u6613\u7684\u4e8b\u52a1\u3002", "result": "\u6a21\u62df\u6267\u884c\u5b9e\u9a8c\u8868\u660e\uff0cNEMO\u663e\u8457\u51cf\u5c11\u4e86\u5197\u4f59\u8ba1\u7b97\uff0c\u572816\u4e2a\u5de5\u4f5c\u7ebf\u7a0b\u4e0b\uff0c\u541e\u5410\u91cf\u6bd4\u6700\u5148\u8fdb\u7684OCC\u65b9\u6cd5Block-STM\u9ad8\u51fa42%\uff0c\u6bd4\u60b2\u89c2\u5e76\u53d1\u63a7\u5236\u57fa\u7ebf\u9ad8\u51fa61%\u3002", "conclusion": "NEMO\u901a\u8fc7\u7ed3\u5408OCC\u548c\u5bf9\u8c61\u6570\u636e\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ade\u4e89\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u533a\u5757\u94fe\u6267\u884c\u6027\u80fd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u3002"}}
{"id": "2510.15147", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15147", "abs": "https://arxiv.org/abs/2510.15147", "authors": ["Aditya Bhosale", "Kavitha Chandrasekar", "Laxmikant Kale", "Sara Kokkila-Schumacher"], "title": "An Elastic Job Scheduler for HPC Applications on the Cloud", "comment": null, "summary": "The last few years have seen an increase in adoption of the cloud for running\nHPC applications. The pay-as-you-go cost model of these cloud resources has\nnecessitated the development of specialized programming models and schedulers\nfor HPC jobs for efficient utilization of cloud resources. A key aspect of\nefficient utilization is the ability to rescale applications on the fly to\nmaximize the utilization of cloud resources. Most commonly used parallel\nprogramming models like MPI have traditionally not supported autoscaling either\nin a cloud environment or on supercomputers. While more recent work has been\ndone to implement this functionality in MPI, it is still nascent and requires\nadditional programmer effort. Charm++ is a parallel programming model that\nnatively supports dynamic rescaling through its migratable objects paradigm. In\nthis paper, we present a Kubernetes operator to run Charm++ applications on a\nKubernetes cluster. We then present a priority-based elastic job scheduler that\ncan dynamically rescale jobs based on the state of a Kubernetes cluster to\nmaximize cluster utilization while minimizing response time for high-priority\njobs. We show that our elastic scheduler, with the ability to rescale HPC jobs\nwith minimal overhead, demonstrates significant performance improvements over\ntraditional static schedulers.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2aKubernetes\u64cd\u4f5c\u7b26\u6765\u8fd0\u884cCharm++\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u4f18\u5148\u7ea7\u7684\u5f39\u6027\u4f5c\u4e1a\u8c03\u5ea6\u5668\uff0c\u80fd\u591f\u5728Kubernetes\u96c6\u7fa4\u4e2d\u52a8\u6001\u8c03\u6574HPC\u4f5c\u4e1a\u89c4\u6a21\u4ee5\u6700\u5927\u5316\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u968f\u7740HPC\u5e94\u7528\u5728\u4e91\u7aef\u7684\u91c7\u7528\u589e\u52a0\uff0c\u9700\u8981\u4e13\u95e8\u7684\u7f16\u7a0b\u6a21\u578b\u548c\u8c03\u5ea6\u5668\u6765\u9ad8\u6548\u5229\u7528\u4e91\u8d44\u6e90\uff0c\u7279\u522b\u662f\u652f\u6301\u52a8\u6001\u4f38\u7f29\u7684\u80fd\u529b\u3002\u4f20\u7edfMPI\u7b49\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\u7f3a\u4e4f\u539f\u751f\u81ea\u52a8\u4f38\u7f29\u652f\u6301\uff0c\u800cCharm++\u901a\u8fc7\u5176\u53ef\u8fc1\u79fb\u5bf9\u8c61\u8303\u5f0f\u539f\u751f\u652f\u6301\u52a8\u6001\u91cd\u4f38\u7f29\u3002", "method": "\u5f00\u53d1\u4e86Kubernetes\u64cd\u4f5c\u7b26\u6765\u8fd0\u884cCharm++\u5e94\u7528\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u4f18\u5148\u7ea7\u7684\u5f39\u6027\u4f5c\u4e1a\u8c03\u5ea6\u5668\uff0c\u6839\u636e\u96c6\u7fa4\u72b6\u6001\u52a8\u6001\u8c03\u6574\u4f5c\u4e1a\u89c4\u6a21\u3002", "result": "\u5f39\u6027\u8c03\u5ea6\u5668\u80fd\u591f\u4ee5\u6700\u5c0f\u5f00\u9500\u91cd\u4f38\u7f29HPC\u4f5c\u4e1a\uff0c\u76f8\u6bd4\u4f20\u7edf\u9759\u6001\u8c03\u5ea6\u5668\u5c55\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728\u6700\u5927\u5316\u96c6\u7fa4\u5229\u7528\u7387\u7684\u540c\u65f6\u6700\u5c0f\u5316\u9ad8\u4f18\u5148\u7ea7\u4f5c\u4e1a\u7684\u54cd\u5e94\u65f6\u95f4\u3002", "conclusion": "Charm++\u4e0eKubernetes\u7684\u7ed3\u5408\u4ee5\u53ca\u5f39\u6027\u8c03\u5ea6\u5668\u4e3a\u4e91\u7aefHPC\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u8c03\u5ea6\u65b9\u6cd5\u3002"}}
{"id": "2510.15215", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15215", "abs": "https://arxiv.org/abs/2510.15215", "authors": ["Zhimin Qiu", "Feng Liu", "Yuxiao Wang", "Chenrui Hu", "Ziyu Cheng", "Di Wu"], "title": "Spatiotemporal Traffic Prediction in Distributed Backend Systems via Graph Neural Networks", "comment": null, "summary": "This paper addresses the problem of traffic prediction in distributed backend\nsystems and proposes a graph neural network based modeling approach to overcome\nthe limitations of traditional models in capturing complex dependencies and\ndynamic features. The system is abstracted as a graph with nodes and edges,\nwhere node features represent traffic and resource states, and adjacency\nrelations describe service interactions. A graph convolution mechanism enables\nmulti order propagation and aggregation of node features, while a gated\nrecurrent structure models historical sequences dynamically, thus integrating\nspatial structures with temporal evolution. A spatiotemporal joint modeling\nmodule further fuses graph representation with temporal dependency, and a\ndecoder generates future traffic predictions. The model is trained with mean\nsquared error to minimize deviations from actual values. Experiments based on\npublic distributed system logs construct combined inputs of node features,\ntopology, and sequences, and compare the proposed method with mainstream\nbaselines using MSE, RMSE, MAE, and MAPE. Results show that the proposed method\nachieves stable performance and low error across different prediction horizons\nand model depths, significantly improving the accuracy and robustness of\ntraffic forecasting in distributed backend systems and verifying the potential\nof graph neural networks in complex system modeling.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5206\u5e03\u5f0f\u540e\u7aef\u7cfb\u7edf\u6d41\u91cf\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u5377\u79ef\u548c\u95e8\u63a7\u5faa\u73af\u7ed3\u6784\u6574\u5408\u65f6\u7a7a\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u6355\u6349\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u590d\u6742\u7684\u4f9d\u8d56\u5173\u7cfb\u548c\u52a8\u6001\u7279\u5f81\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5efa\u6a21\u65b9\u6cd5\u6765\u63d0\u5347\u6d41\u91cf\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u5c06\u7cfb\u7edf\u62bd\u8c61\u4e3a\u56fe\u7ed3\u6784\uff0c\u4f7f\u7528\u56fe\u5377\u79ef\u673a\u5236\u8fdb\u884c\u8282\u70b9\u7279\u5f81\u4f20\u64ad\u805a\u5408\uff0c\u7ed3\u5408\u95e8\u63a7\u5faa\u73af\u7ed3\u6784\u5efa\u6a21\u5386\u53f2\u5e8f\u5217\uff0c\u901a\u8fc7\u65f6\u7a7a\u8054\u5408\u5efa\u6a21\u6a21\u5757\u878d\u5408\u56fe\u8868\u793a\u4e0e\u65f6\u5e8f\u4f9d\u8d56\uff0c\u89e3\u7801\u5668\u751f\u6210\u672a\u6765\u6d41\u91cf\u9884\u6d4b\u3002", "result": "\u5728\u516c\u5f00\u5206\u5e03\u5f0f\u7cfb\u7edf\u65e5\u5fd7\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u9884\u6d4b\u65f6\u57df\u548c\u6a21\u578b\u6df1\u5ea6\u4e0b\u5747\u8868\u73b0\u7a33\u5b9a\u4e14\u8bef\u5dee\u8f83\u4f4e\uff0c\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u590d\u6742\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5206\u5e03\u5f0f\u540e\u7aef\u7cfb\u7edf\u6d41\u91cf\u9884\u6d4b\u63d0\u4f9b\u4e86\u51c6\u786e\u53ef\u9760\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2510.15744", "categories": ["cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.15744", "abs": "https://arxiv.org/abs/2510.15744", "authors": ["Haocong Luo", "Ataberk Olgun", "Maria Makeenkova", "F. Nisa Bostanci", "Geraldo F. Oliveira", "A. Giray Yaglikci", "Onur Mutlu"], "title": "Cleaning up the Mess", "comment": null, "summary": "A MICRO 2024 best paper runner-up publication (the Mess paper) with all three\nartifact badges awarded (including \"Reproducible\") proposes a new benchmark to\nevaluate real and simulated memory system performance. In this paper, we\ndemonstrate that the Ramulator 2.0 simulation results reported in the Mess\npaper are incorrect and, at the time of the publication of the Mess paper,\nirreproducible. We find that the authors of Mess paper made multiple trivial\nhuman errors in both the configuration and usage of the simulators. We show\nthat by correctly configuring Ramulator 2.0, Ramulator 2.0's simulated memory\nsystem performance actually resembles real system characteristics well, and\nthus a key claimed contribution of the Mess paper is factually incorrect. We\nalso identify that the DAMOV simulation results in the Mess paper use wrong\nsimulation statistics that are unrelated to the simulated DRAM performance.\nMoreover, the Mess paper's artifact repository lacks the necessary sources to\nfully reproduce all the Mess paper's results.\n  Our work corrects the Mess paper's errors regarding Ramulator 2.0 and\nidentifies important issues in the Mess paper's memory simulator evaluation\nmethodology. We emphasize the importance of both carefully and rigorously\nvalidating simulation results and contacting simulator authors and developers,\nin true open source spirit, to ensure these simulators are used with correct\nconfigurations and as intended. We encourage the computer architecture\ncommunity to correct the Mess paper's errors. This is necessary to prevent the\npropagation of inaccurate and misleading results, and to maintain the\nreliability of the scientific record. Our investigation also opens up questions\nabout the integrity of the review and artifact evaluation processes. To aid\nfuture work, our source code and scripts are openly available at https:\n//github.com/CMU-SAFARI/ramulator2/tree/mess.", "AI": {"tldr": "\u672c\u6587\u6307\u51faMICRO 2024\u6700\u4f73\u8bba\u6587\u4e9a\u519b\uff08Mess\u8bba\u6587\uff09\u4e2d\u5173\u4e8eRamulator 2.0\u6a21\u62df\u5668\u7684\u7ed3\u679c\u5b58\u5728\u9519\u8bef\u4e14\u4e0d\u53ef\u590d\u73b0\uff0c\u901a\u8fc7\u6b63\u786e\u914d\u7f6e\u540e\u663e\u793a\u8be5\u6a21\u62df\u5668\u80fd\u51c6\u786e\u53cd\u6620\u771f\u5b9e\u7cfb\u7edf\u7279\u6027\uff0c\u4ece\u800c\u53cd\u9a73\u4e86Mess\u8bba\u6587\u7684\u5173\u952e\u8d21\u732e\u3002", "motivation": "\u7ea0\u6b63Mess\u8bba\u6587\u4e2d\u5173\u4e8eRamulator 2.0\u6a21\u62df\u5668\u7684\u9519\u8bef\u7ed3\u679c\uff0c\u9632\u6b62\u4e0d\u51c6\u786e\u548c\u8bef\u5bfc\u6027\u7ed3\u679c\u7684\u4f20\u64ad\uff0c\u7ef4\u62a4\u79d1\u5b66\u8bb0\u5f55\u7684\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u914d\u7f6e\u548c\u4f7f\u7528Ramulator 2.0\u6a21\u62df\u5668\uff0c\u5bf9\u6bd4Mess\u8bba\u6587\u4e2d\u7684\u9519\u8bef\u914d\u7f6e\uff0c\u9a8c\u8bc1\u6a21\u62df\u7ed3\u679c\u7684\u51c6\u786e\u6027\u3002", "result": "\u53d1\u73b0Mess\u8bba\u6587\u5728Ramulator 2.0\u7684\u914d\u7f6e\u548c\u4f7f\u7528\u4e0a\u5b58\u5728\u591a\u4e2a\u7b80\u5355\u4eba\u4e3a\u9519\u8bef\uff0c\u6b63\u786e\u914d\u7f6e\u540e\u6a21\u62df\u7ed3\u679c\u4e0e\u771f\u5b9e\u7cfb\u7edf\u7279\u6027\u76f8\u7b26\uff1b\u540c\u65f6\u53d1\u73b0DAMOV\u6a21\u62df\u7ed3\u679c\u4f7f\u7528\u4e86\u9519\u8bef\u7684\u7edf\u8ba1\u6307\u6807\u3002", "conclusion": "\u5f3a\u8c03\u4ed4\u7ec6\u9a8c\u8bc1\u6a21\u62df\u7ed3\u679c\u7684\u91cd\u8981\u6027\uff0c\u5efa\u8bae\u4e0e\u6a21\u62df\u5668\u5f00\u53d1\u8005\u6c9f\u901a\u786e\u4fdd\u6b63\u786e\u4f7f\u7528\uff0c\u547c\u5401\u793e\u533a\u7ea0\u6b63Mess\u8bba\u6587\u7684\u9519\u8bef\uff0c\u5e76\u8d28\u7591\u8bc4\u5ba1\u548c\u5236\u54c1\u8bc4\u4f30\u8fc7\u7a0b\u7684\u5b8c\u6574\u6027\u3002"}}
{"id": "2510.15178", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.15178", "abs": "https://arxiv.org/abs/2510.15178", "authors": ["Brysen Pfingsten", "Jason Hemann"], "title": "Visualizing miniKanren Search with a Fine-Grained Small-Step Semantics", "comment": "2025 miniKanren Workshop", "summary": "We present a deterministic small-step operational semantics for miniKanren\nthat explicitly represents the evolving search tree during execution. This\nsemantics models interleaving and goal scheduling at fine granularity, allowing\neach evaluation step-goal activation, suspension, resumption, and success -- to\nbe visualized precisely. Building on this model, we implement an interactive\nvisualizer that renders the search tree as it develops and lets users step\nthrough execution. The tool acts as a pedagogical notional machine for\nreasoning about miniKanren's fair search behavior, helping users understand\nsurprising answer orders and operational effects. Our semantics and tool are\nvalidated through property-based testing and illustrated with several examples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u786e\u5b9a\u6027\u5c0f\u6b65\u64cd\u4f5c\u8bed\u4e49\u6765\u663e\u5f0f\u8868\u793aminiKanren\u6267\u884c\u8fc7\u7a0b\u4e2d\u7684\u641c\u7d22\u6811\u6f14\u5316\uff0c\u5e76\u57fa\u4e8e\u6b64\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\u3002", "motivation": "\u4e3a\u4e86\u7cbe\u786e\u5efa\u6a21miniKanren\u7684\u4ea4\u9519\u641c\u7d22\u548c\u76ee\u6807\u8c03\u5ea6\u673a\u5236\uff0c\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u5176\u516c\u5e73\u641c\u7d22\u884c\u4e3a\u548c\u64cd\u4f5c\u6548\u679c\uff0c\u7279\u522b\u662f\u4ee4\u4eba\u60ca\u8bb6\u7684\u7b54\u6848\u987a\u5e8f\u3002", "method": "\u5f00\u53d1\u4e86\u786e\u5b9a\u6027\u5c0f\u6b65\u64cd\u4f5c\u8bed\u4e49\uff0c\u663e\u5f0f\u8868\u793a\u641c\u7d22\u6811\u7684\u6f14\u5316\u8fc7\u7a0b\uff0c\u5305\u62ec\u76ee\u6807\u6fc0\u6d3b\u3001\u6302\u8d77\u3001\u6062\u590d\u548c\u6210\u529f\u7b49\u6b65\u9aa4\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bed\u4e49\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5668\u3002", "result": "\u901a\u8fc7\u57fa\u4e8e\u5c5e\u6027\u7684\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8bed\u4e49\u548c\u5de5\u5177\u7684\u6b63\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u591a\u4e2a\u793a\u4f8b\u5c55\u793a\u4e86\u5de5\u5177\u7684\u6559\u5b66\u4ef7\u503c\u3002", "conclusion": "\u8be5\u8bed\u4e49\u548c\u53ef\u89c6\u5316\u5de5\u5177\u4e3aminiKanren\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6559\u5b66\u6982\u5ff5\u673a\u5668\uff0c\u5e2e\u52a9\u7528\u6237\u6df1\u5165\u7406\u89e3\u5176\u641c\u7d22\u884c\u4e3a\u548c\u64cd\u4f5c\u8bed\u4e49\u3002"}}
{"id": "2510.15330", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.15330", "abs": "https://arxiv.org/abs/2510.15330", "authors": ["Tella Rajashekhar Reddy", "Atharva Deshmukh", "Karan Tandon", "Rohan Gandhi", "Anjaly Parayil", "Debopam Bhattacherjee"], "title": "BeLLMan: Controlling LLM Congestion", "comment": "To be presented at FAISYS 2025", "summary": "Large language model (LLM) applications are blindfolded to the infrastructure\nunderneath and generate tokens autoregressively, indifferent to the system\nload, thus risking inferencing latency inflation and poor user experience. Our\nfirst-cut controller, named beLLMan, enables the LLM infrastructure to actively\nand progressively signal the first-party LLM application to adjust the output\nlength in response to changing system load. On a real testbed with H100 GPUs,\nbeLLMan helps keep inferencing latency under control (upto 8X lower end-to-end\nlatency) and reduces energy consumption by 25% (while serving 19% more\nrequests) during periods of congestion for a summarization workload.", "AI": {"tldr": "beLLMan\u63a7\u5236\u5668\u901a\u8fc7\u4e3b\u52a8\u8c03\u6574LLM\u5e94\u7528\u7684\u8f93\u51fa\u957f\u5ea6\u6765\u5e94\u5bf9\u7cfb\u7edf\u8d1f\u8f7d\u53d8\u5316\uff0c\u5728\u771f\u5b9e\u6d4b\u8bd5\u73af\u5883\u4e2d\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff08\u6700\u9ad88\u500d\uff09\u5e76\u51cf\u5c1125%\u7684\u80fd\u8017\uff0c\u540c\u65f6\u5904\u7406\u66f4\u591a\u8bf7\u6c42\u3002", "motivation": "\u5f53\u524dLLM\u5e94\u7528\u5bf9\u5e95\u5c42\u57fa\u7840\u8bbe\u65bd\u8d1f\u8f7d\u4e0d\u654f\u611f\uff0c\u5728\u7cfb\u7edf\u8d1f\u8f7d\u9ad8\u65f6\u4f1a\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u589e\u52a0\u548c\u7528\u6237\u4f53\u9a8c\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u52a8\u6001\u8c03\u6574\u8f93\u51fa\u957f\u5ea6\u4ee5\u5e94\u5bf9\u8d1f\u8f7d\u53d8\u5316\u3002", "method": "\u5f00\u53d1beLLMan\u63a7\u5236\u5668\uff0c\u4f7fLLM\u57fa\u7840\u8bbe\u65bd\u80fd\u591f\u4e3b\u52a8\u5411\u7b2c\u4e00\u65b9LLM\u5e94\u7528\u53d1\u9001\u4fe1\u53f7\uff0c\u6839\u636e\u7cfb\u7edf\u8d1f\u8f7d\u52a8\u6001\u8c03\u6574\u8f93\u51fa\u957f\u5ea6\u3002", "result": "\u5728\u914d\u5907H100 GPU\u7684\u771f\u5b9e\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0cbeLLMan\u5728\u62e5\u585e\u671f\u95f4\u5c06\u63a8\u7406\u5ef6\u8fdf\u63a7\u5236\u5728\u8f83\u4f4e\u6c34\u5e73\uff08\u7aef\u5230\u7aef\u5ef6\u8fdf\u6700\u9ad8\u964d\u4f4e8\u500d\uff09\uff0c\u80fd\u8017\u51cf\u5c1125%\uff0c\u540c\u65f6\u5904\u7406\u8bf7\u6c42\u91cf\u589e\u52a019%\u3002", "conclusion": "beLLMan\u901a\u8fc7\u4e3b\u52a8\u8c03\u6574LLM\u8f93\u51fa\u957f\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7cfb\u7edf\u8d1f\u8f7d\u5bfc\u81f4\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u670d\u52a1\u7684\u6027\u80fd\u548c\u80fd\u6548\u3002"}}
{"id": "2510.15747", "categories": ["cs.PL", "cs.CR", "cs.DC", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.15747", "abs": "https://arxiv.org/abs/2510.15747", "authors": ["Ehud Shapiro"], "title": "Grassroots Logic Programs: A Secure, Multiagent, Concurrent, Logic Programming Language", "comment": null, "summary": "Grassroots platforms are distributed applications run by\\linebreak\ncryptographically-identified people on their networked personal devices, where\nmultiple disjoint platform instances emerge independently and coalesce when\nthey interoperate. Their foundation is the grassroots social graph, upon which\ngrassroots social networks, grassroots cryptocurrencies, and grassroots\ndemocratic federations can be built.\n  Grassroots platforms have yet to be implemented, the key challenge being\nfaulty and malicious participants: without secure programming support, correct\nparticipants cannot reliably identify each other, establish secure\ncommunication, or verify each other's code integrity.\n  We present Grassroots Logic Programs (GLP), a secure, multiagent, concurrent,\nlogic programming language for implementing grassroots platforms. GLP extends\nlogic programs with paired single-reader/single-writer (SRSW) logic variables,\nproviding secure communication channels among cryptographically-identified\npeople through encrypted, signed and attested messages, which enable identity\nand code integrity verification. We present GLP progressively: logic programs,\nconcurrent GLP, multiagent GLP, augmenting it with cryptographic security, and\nproviding smartphone implementation-ready specifications. We prove safety\nproperties including that GLP computations are deductions, SRSW preservation,\nacyclicity, and monotonicity. We prove multiagent GLP is grassroots and that\nGLP streams achieve blockchain security properties. We present a grassroots\nsocial graph protocol establishing authenticated peer-to-peer connections and\ndemonstrate secure grassroots social networking applications.", "AI": {"tldr": "\u63d0\u51faGrassroots Logic Programs (GLP)\uff0c\u4e00\u79cd\u5b89\u5168\u7684\u3001\u591a\u667a\u80fd\u4f53\u7684\u5e76\u53d1\u903b\u8f91\u7f16\u7a0b\u8bed\u8a00\uff0c\u7528\u4e8e\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u7684\u57fa\u5c42\u5e73\u53f0\uff0c\u89e3\u51b3\u6076\u610f\u53c2\u4e0e\u8005\u95ee\u9898\u3002", "motivation": "\u57fa\u5c42\u5e73\u53f0\u9762\u4e34\u6076\u610f\u53c2\u4e0e\u8005\u7684\u6311\u6218\uff0c\u9700\u8981\u5b89\u5168\u7f16\u7a0b\u652f\u6301\u6765\u786e\u4fdd\u6b63\u786e\u53c2\u4e0e\u8005\u80fd\u591f\u53ef\u9760\u8bc6\u522b\u5f7c\u6b64\u3001\u5efa\u7acb\u5b89\u5168\u901a\u4fe1\u5e76\u9a8c\u8bc1\u4ee3\u7801\u5b8c\u6574\u6027\u3002", "method": "\u6269\u5c55\u903b\u8f91\u7a0b\u5e8f\uff0c\u6dfb\u52a0\u914d\u5bf9\u7684\u5355\u8bfb\u5355\u5199\u903b\u8f91\u53d8\u91cf\uff0c\u901a\u8fc7\u52a0\u5bc6\u3001\u7b7e\u540d\u548c\u8ba4\u8bc1\u6d88\u606f\u63d0\u4f9b\u5b89\u5168\u901a\u4fe1\u901a\u9053\uff0c\u652f\u6301\u8eab\u4efd\u548c\u4ee3\u7801\u5b8c\u6574\u6027\u9a8c\u8bc1\u3002", "result": "\u8bc1\u660e\u4e86GLP\u7684\u5b89\u5168\u5c5e\u6027\uff0c\u5305\u62ec\u8ba1\u7b97\u662f\u6f14\u7ece\u7684\u3001SRSW\u4fdd\u6301\u6027\u3001\u65e0\u73af\u6027\u548c\u5355\u8c03\u6027\uff0c\u591a\u667a\u80fd\u4f53GLP\u5177\u6709\u57fa\u5c42\u7279\u6027\uff0cGLP\u6d41\u5b9e\u73b0\u533a\u5757\u94fe\u5b89\u5168\u5c5e\u6027\u3002", "conclusion": "GLP\u4e3a\u57fa\u5c42\u5e73\u53f0\u63d0\u4f9b\u4e86\u5b89\u5168\u5b9e\u73b0\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5b89\u5168\u7684\u57fa\u5c42\u793e\u4ea4\u7f51\u7edc\u5e94\u7528\u3002"}}
{"id": "2510.15355", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15355", "abs": "https://arxiv.org/abs/2510.15355", "authors": ["Tim Kraus", "Axel Sauer", "Ingo Feldner"], "title": "Cloud-Enabled Virtual Prototypes", "comment": "8 pages, 5 figures, Published in DVCon Europe 2025", "summary": "The rapid evolution of embedded systems, along with the growing variety and\ncomplexity of AI algorithms, necessitates a powerful hardware/software\nco-design methodology based on virtual prototyping technologies. The market\noffers a diverse range of simulation solutions, each with its unique\ntechnological approach and therefore strengths and weaknesses. Additionally,\nwith the increasing availability of remote on-demand computing resources and\ntheir adaptation throughout the industry, the choice of the host infrastructure\nfor execution opens even more new possibilities for operational strategies.\nThis work explores the dichotomy between local and cloud-based simulation\nenvironments, focusing on the trade-offs between scalability and privacy. We\ndiscuss how the setup of the compute infrastructure impacts the performance of\nthe execution and security of data involved in the process. Furthermore, we\nhighlight the development workflow associated with embedded AI and the critical\nrole of efficient simulations in optimizing these algorithms. With the proposed\nsolution, we aim to sustainably improve trust in remote simulations and\nfacilitate the adoption of virtual prototyping practices.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u672c\u5730\u4e0e\u4e91\u7aef\u4eff\u771f\u73af\u5883\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u91cd\u70b9\u5173\u6ce8\u53ef\u6269\u5c55\u6027\u4e0e\u9690\u79c1\u4fdd\u62a4\uff0c\u65e8\u5728\u63d0\u9ad8\u8fdc\u7a0b\u4eff\u771f\u7684\u53ef\u4fe1\u5ea6\u5e76\u4fc3\u8fdb\u865a\u62df\u539f\u578b\u6280\u672f\u7684\u91c7\u7528\u3002", "motivation": "\u5d4c\u5165\u5f0f\u7cfb\u7edf\u548cAI\u7b97\u6cd5\u7684\u5feb\u901f\u53d1\u5c55\u9700\u8981\u5f3a\u5927\u7684\u786c\u4ef6/\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u800c\u5e02\u573a\u4e0a\u591a\u6837\u5316\u7684\u4eff\u771f\u89e3\u51b3\u65b9\u6848\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u52a0\u4e0a\u8fdc\u7a0b\u8ba1\u7b97\u8d44\u6e90\u7684\u666e\u53ca\uff0c\u4f7f\u5f97\u9009\u62e9\u5408\u9002\u7684\u4e3b\u673a\u57fa\u7840\u8bbe\u65bd\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5206\u6790\u672c\u5730\u4e0e\u4e91\u7aef\u4eff\u771f\u73af\u5883\u7684\u5bf9\u6bd4\uff0c\u7814\u7a76\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u8bbe\u7f6e\u5bf9\u6267\u884c\u6027\u80fd\u548c\u6570\u636e\u5b89\u5168\u7684\u5f71\u54cd\uff0c\u5e76\u5f3a\u8c03\u5d4c\u5165\u5f0fAI\u5f00\u53d1\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u9ad8\u6548\u4eff\u771f\u7684\u5173\u952e\u4f5c\u7528\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u53ef\u6301\u7eed\u5730\u63d0\u9ad8\u5bf9\u8fdc\u7a0b\u4eff\u771f\u7684\u4fe1\u4efb\u5ea6\uff0c\u5e76\u4fc3\u8fdb\u865a\u62df\u539f\u578b\u8bbe\u8ba1\u5b9e\u8df5\u7684\u91c7\u7528\u3002", "conclusion": "\u5728\u5d4c\u5165\u5f0fAI\u5f00\u53d1\u4e2d\uff0c\u9700\u8981\u5728\u672c\u5730\u548c\u4e91\u7aef\u4eff\u771f\u73af\u5883\u4e4b\u95f4\u505a\u51fa\u6743\u8861\uff0c\u5e73\u8861\u53ef\u6269\u5c55\u6027\u4e0e\u9690\u79c1\u4fdd\u62a4\uff0c\u901a\u8fc7\u4f18\u5316\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u8bbe\u7f6e\u6765\u63d0\u5347\u6027\u80fd\u548c\u6570\u636e\u5b89\u5168\u3002"}}
{"id": "2510.15473", "categories": ["cs.DC", "math.PR"], "pdf": "https://arxiv.org/pdf/2510.15473", "abs": "https://arxiv.org/abs/2510.15473", "authors": ["Petra Berenbrink", "Robert Els\u00e4sser", "Tom Friedetzky", "Hamed Hosseinpour", "Dominik Kaaser", "Peter Kling", "Thomas Sauerwald"], "title": "(Almost) Perfect Discrete Iterative Load Balancing", "comment": null, "summary": "We consider discrete, iterative load balancing via matchings on arbitrary\ngraphs. Initially each node holds a certain number of tokens, defining the load\nof the node, and the objective is to redistribute the tokens such that\neventually each node has approximately the same number of tokens. We present\nresults for a general class of simple local balancing schemes where the tokens\nare balanced via matchings. In each round the process averages the tokens of\nany two matched nodes. If the sum of their tokens is odd, the node to receive\nthe one excess token is selected at random. Our class covers three popular\nmodels: in the matching model a new matching is generated randomly in each\nround, in the balancing circuit model a fixed sequence of matchings is applied\nperiodically, and in the asynchronous model the load is balanced over a\nrandomly chosen edge.\n  We measure the quality of a load vector by its discrepancy, defined as the\ndifference between the maximum and minimum load across all nodes. As our main\nresult we show that with high probability our discrete balancing scheme reaches\na discrepancy of $3$ in a number of rounds which asymptotically matches the\nspectral bound for continuous load balancing with fractional load.\n  This result improves and tightens a long line of previous works, by not only\nachieving a small constant discrepancy (instead of a non-explicit, large\nconstant) but also holding for arbitrary instead of regular graphs. The result\nalso demonstrates that in the general model we consider, discrete load\nbalancing is no harder than continuous load balancing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u79bb\u6563\u8d1f\u8f7d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u5339\u914d\u673a\u5236\u5728\u4efb\u610f\u56fe\u4e0a\u8fdb\u884c\u8fed\u4ee3\u8d1f\u8f7d\u5747\u8861\uff0c\u8bc1\u660e\u4e86\u79bb\u6563\u8d1f\u8f7d\u5e73\u8861\u53ef\u4ee5\u8fbe\u5230\u4e0e\u8fde\u7eed\u8d1f\u8f7d\u5e73\u8861\u76f8\u540c\u7684\u6027\u80fd\uff0c\u6700\u7ec8\u5b9e\u73b0\u5e38\u6570\u4e3a3\u7684\u8d1f\u8f7d\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u79bb\u6563\u8d1f\u8f7d\u5e73\u8861\u95ee\u9898\u7684\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u5728\u4efb\u610f\u56fe\u4e0a\u901a\u8fc7\u5339\u914d\u673a\u5236\u8fdb\u884c\u8d1f\u8f7d\u5747\u8861\u7684\u6027\u80fd\uff0c\u63a2\u7d22\u79bb\u6563\u8d1f\u8f7d\u5e73\u8861\u662f\u5426\u4e0e\u8fde\u7eed\u8d1f\u8f7d\u5e73\u8861\u5177\u6709\u76f8\u540c\u7684\u590d\u6742\u5ea6\uff0c\u5e76\u6539\u8fdb\u4e4b\u524d\u5de5\u4f5c\u4e2d\u8f83\u5927\u7684\u5e38\u6570\u5dee\u5f02\u548c\u975e\u663e\u5f0f\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e00\u7c7b\u7b80\u5355\u7684\u5c40\u90e8\u5e73\u8861\u65b9\u6848\uff0c\u901a\u8fc7\u5339\u914d\u673a\u5236\u5728\u6bcf\u8f6e\u4e2d\u5e73\u8861\u8282\u70b9\u95f4\u7684\u4ee4\u724c\u6570\u91cf\u3002\u5f53\u4e24\u4e2a\u5339\u914d\u8282\u70b9\u7684\u4ee4\u724c\u603b\u548c\u4e3a\u5947\u6570\u65f6\uff0c\u968f\u673a\u9009\u62e9\u63a5\u6536\u591a\u4f59\u4ee4\u724c\u7684\u8282\u70b9\u3002\u8be5\u65b9\u6cd5\u6db5\u76d6\u4e09\u79cd\u6d41\u884c\u6a21\u578b\uff1a\u5339\u914d\u6a21\u578b\u3001\u5e73\u8861\u7535\u8def\u6a21\u578b\u548c\u5f02\u6b65\u6a21\u578b\u3002", "result": "\u4e3b\u8981\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u79bb\u6563\u5e73\u8861\u65b9\u6848\u4ee5\u9ad8\u6982\u7387\u5728\u6e10\u8fdb\u5339\u914d\u8fde\u7eed\u8d1f\u8f7d\u5e73\u8861\u8c31\u754c\u6240\u9700\u7684\u8f6e\u6570\u5185\u8fbe\u5230\u8d1f\u8f7d\u5dee\u5f02\u4e3a3\u7684\u7ed3\u679c\u3002\u8fd9\u4e00\u7ed3\u679c\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u5c0f\u7684\u5e38\u6570\u5dee\u5f02\uff0c\u800c\u4e14\u9002\u7528\u4e8e\u4efb\u610f\u56fe\u800c\u975e\u4ec5\u9650\u4e8e\u6b63\u5219\u56fe\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u4e00\u822c\u6a21\u578b\u4e2d\uff0c\u79bb\u6563\u8d1f\u8f7d\u5e73\u8861\u5e76\u4e0d\u6bd4\u8fde\u7eed\u8d1f\u8f7d\u5e73\u8861\u66f4\u96be\uff0c\u79bb\u6563\u65b9\u6848\u53ef\u4ee5\u8fbe\u5230\u4e0e\u8fde\u7eed\u65b9\u6848\u76f8\u540c\u7684\u6027\u80fd\uff0c\u4e3a\u8d1f\u8f7d\u5e73\u8861\u7406\u8bba\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2510.15485", "categories": ["cs.DC", "cs.DB", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.15485", "abs": "https://arxiv.org/abs/2510.15485", "authors": ["D\u0101vis Ka\u017eemaks", "Laurens Versluis", "Burcu Kulahcioglu Ozkan", "J\u00e9r\u00e9mie Decouchant"], "title": "Balancing Fairness and Performance in Multi-User Spark Workloads with Dynamic Scheduling (extended version)", "comment": "This paper is an extended version of a paper accepted at the ACM\n  Symposium on Cloud Computing (SoCC'25) that contains a proof of correctness", "summary": "Apache Spark is a widely adopted framework for large-scale data processing.\nHowever, in industrial analytics environments, Spark's built-in schedulers,\nsuch as FIFO and fair scheduling, struggle to maintain both user-level fairness\nand low mean response time, particularly in long-running shared applications.\nExisting solutions typically focus on job-level fairness which unintentionally\nfavors users who submit more jobs. Although Spark offers a built-in fair\nscheduler, it lacks adaptability to dynamic user workloads and may degrade\noverall job performance. We present the User Weighted Fair Queuing (UWFQ)\nscheduler, designed to minimize job response times while ensuring equitable\nresource distribution across users and their respective jobs. UWFQ simulates a\nvirtual fair queuing system and schedules jobs based on their estimated finish\ntimes under a bounded fairness model. To further address task skew and reduce\npriority inversions, which are common in Spark workloads, we introduce runtime\npartitioning, a method that dynamically refines task granularity based on\nexpected runtime. We implement UWFQ within the Spark framework and evaluate its\nperformance using multi-user synthetic workloads and Google cluster traces. We\nshow that UWFQ reduces the average response time of small jobs by up to 74%\ncompared to existing built-in Spark schedulers and to state-of-the-art fair\nscheduling algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86UWFQ\u8c03\u5ea6\u5668\uff0c\u5728Spark\u6846\u67b6\u4e2d\u5b9e\u73b0\u7528\u6237\u7ea7\u516c\u5e73\u8c03\u5ea6\uff0c\u901a\u8fc7\u865a\u62df\u516c\u5e73\u961f\u5217\u7cfb\u7edf\u548c\u8fd0\u884c\u65f6\u5206\u533a\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u5c0f\u4f5c\u4e1a\u54cd\u5e94\u65f6\u95f4\u5e76\u786e\u4fdd\u7528\u6237\u95f4\u8d44\u6e90\u516c\u5e73\u5206\u914d\u3002", "motivation": "Spark\u5185\u7f6e\u8c03\u5ea6\u5668\u5728\u5de5\u4e1a\u5206\u6790\u73af\u5883\u4e2d\u96be\u4ee5\u540c\u65f6\u7ef4\u6301\u7528\u6237\u7ea7\u516c\u5e73\u6027\u548c\u4f4e\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u504f\u5411\u63d0\u4ea4\u66f4\u591a\u4f5c\u4e1a\u7684\u7528\u6237\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u6001\u7528\u6237\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9002\u5e94\u6027\u3002", "method": "\u8bbe\u8ba1UWFQ\u8c03\u5ea6\u5668\uff0c\u6a21\u62df\u865a\u62df\u516c\u5e73\u961f\u5217\u7cfb\u7edf\uff0c\u57fa\u4e8e\u6709\u754c\u516c\u5e73\u6a21\u578b\u6309\u9884\u4f30\u5b8c\u6210\u65f6\u95f4\u8c03\u5ea6\u4f5c\u4e1a\uff1b\u5f15\u5165\u8fd0\u884c\u65f6\u5206\u533a\u6280\u672f\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u7c92\u5ea6\u4ee5\u89e3\u51b3\u4efb\u52a1\u504f\u659c\u548c\u4f18\u5148\u7ea7\u53cd\u8f6c\u95ee\u9898\u3002", "result": "\u4f7f\u7528\u591a\u7528\u6237\u5408\u6210\u5de5\u4f5c\u8d1f\u8f7d\u548cGoogle\u96c6\u7fa4\u8ddf\u8e2a\u8fdb\u884c\u8bc4\u4f30\uff0cUWFQ\u76f8\u6bd4\u73b0\u6709Spark\u8c03\u5ea6\u5668\u548c\u5148\u8fdb\u516c\u5e73\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5c06\u5c0f\u4f5c\u4e1a\u7684\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\u964d\u4f4e\u9ad8\u8fbe74%\u3002", "conclusion": "UWFQ\u8c03\u5ea6\u5668\u6709\u6548\u89e3\u51b3\u4e86Spark\u73af\u5883\u4e2d\u7528\u6237\u7ea7\u516c\u5e73\u6027\u548c\u6027\u80fd\u4f18\u5316\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u4f5c\u4e1a\u6027\u80fd\u5e76\u786e\u4fdd\u8d44\u6e90\u516c\u5e73\u5206\u914d\u3002"}}
{"id": "2510.15490", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15490", "abs": "https://arxiv.org/abs/2510.15490", "authors": ["Diogo Landau", "Gijs Blanken", "Jorge Barbosa", "Nishant Saurabh"], "title": "Retrofitting Service Dependency Discovery in Distributed Systems", "comment": null, "summary": "Modern distributed systems rely on complex networks of interconnected\nservices, creating direct or indirect dependencies that can propagate faults\nand cause cascading failures. To localize the root cause of performance\ndegradation in these environments, constructing a service dependency graph is\nhighly beneficial. However, building an accurate service dependency graph is\nimpaired by complex routing techniques, such as Network Address Translation\n(NAT), an essential mechanism for connecting services across networks. NAT\nobfuscates the actual hosts running the services, causing existing run-time\napproaches that passively observe network metadata to fail in accurately\ninferring service dependencies. To this end, this paper introduces XXXX, a\nnovel run-time system for constructing process-level service dependency graphs.\nIt operates without source code instrumentation and remains resilient under\ncomplex network routing mechanisms, including NAT. XXXX implements a\nnon-disruptive method of injecting metadata onto a TCP packet's header that\nmaintains protocol correctness across host boundaries. In other words, if no\nreceiving agent is present, the instrumentation leaves existing TCP connections\nunaffected, ensuring non-disruptive operation when it is partially deployed\nacross hosts. We evaluated XXXX extensively against three state-of-the-art\nsystems across nine scenarios, involving three network configurations\n(NAT-free, internal-NAT, external-NAT) and three microservice benchmarks. XXXX\nwas the only approach that performed consistently across networking\nconfigurations. With regards to correctness, it performed on par with, or\nbetter than, the state-of-the-art with precision and recall values of 100% in\nthe majority of the scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aXXXX\u7684\u65b0\u578b\u8fd0\u884c\u65f6\u7cfb\u7edf\uff0c\u7528\u4e8e\u6784\u5efa\u8fdb\u7a0b\u7ea7\u670d\u52a1\u4f9d\u8d56\u56fe\uff0c\u80fd\u591f\u5728\u590d\u6742\u7f51\u7edc\u8def\u7531\u673a\u5236\uff08\u5305\u62ecNAT\uff09\u4e0b\u51c6\u786e\u63a8\u65ad\u670d\u52a1\u4f9d\u8d56\u5173\u7cfb\uff0c\u65e0\u9700\u6e90\u4ee3\u7801\u63d2\u6869\u3002", "motivation": "\u73b0\u4ee3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u590d\u6742\u7684\u670d\u52a1\u4f9d\u8d56\u5173\u7cfb\u5bb9\u6613\u5bfc\u81f4\u7ea7\u8054\u6545\u969c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728NAT\u7b49\u590d\u6742\u8def\u7531\u6280\u672f\u4e0b\u65e0\u6cd5\u51c6\u786e\u6784\u5efa\u670d\u52a1\u4f9d\u8d56\u56fe\uff0c\u56e0\u4e3aNAT\u4f1a\u6a21\u7cca\u5b9e\u9645\u8fd0\u884c\u670d\u52a1\u7684\u4e3b\u673a\u4fe1\u606f\u3002", "method": "XXXX\u91c7\u7528\u975e\u7834\u574f\u6027\u65b9\u6cd5\uff0c\u5728TCP\u5305\u5934\u4e2d\u6ce8\u5165\u5143\u6570\u636e\uff0c\u4fdd\u6301\u8de8\u4e3b\u673a\u8fb9\u754c\u7684\u534f\u8bae\u6b63\u786e\u6027\u3002\u5982\u679c\u6ca1\u6709\u63a5\u6536\u4ee3\u7406\uff0c\u63d2\u6869\u4e0d\u4f1a\u5f71\u54cd\u73b0\u6709TCP\u8fde\u63a5\uff0c\u786e\u4fdd\u5728\u90e8\u5206\u90e8\u7f72\u65f6\u4e5f\u80fd\u65e0\u4e2d\u65ad\u8fd0\u884c\u3002", "result": "\u57289\u4e2a\u573a\u666f\u4e0b\u4e0e3\u4e2a\u6700\u5148\u8fdb\u7cfb\u7edf\u8fdb\u884c\u6bd4\u8f83\uff0c\u6d89\u53ca3\u79cd\u7f51\u7edc\u914d\u7f6e\uff08\u65e0NAT\u3001\u5185\u90e8NAT\u3001\u5916\u90e8NAT\uff09\u548c3\u4e2a\u5fae\u670d\u52a1\u57fa\u51c6\u6d4b\u8bd5\u3002XXXX\u662f\u552f\u4e00\u5728\u6240\u6709\u7f51\u7edc\u914d\u7f6e\u4e0b\u8868\u73b0\u4e00\u81f4\u7684\u65b9\u6cd5\uff0c\u5728\u5927\u591a\u6570\u573a\u666f\u4e0b\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u8fbe\u5230100%\u3002", "conclusion": "XXXX\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u89e3\u51b3NAT\u73af\u5883\u4e0b\u670d\u52a1\u4f9d\u8d56\u56fe\u6784\u5efa\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u51c6\u786e\u4e14\u975e\u7834\u574f\u6027\u7684\u4f9d\u8d56\u5173\u7cfb\u63a8\u65ad\uff0c\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u6545\u969c\u5b9a\u4f4d\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.15596", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15596", "abs": "https://arxiv.org/abs/2510.15596", "authors": ["Alicia Golden", "Michael Kuchnik", "Samuel Hsia", "Zachary DeVito", "Gu-Yeon Wei", "David Brooks", "Carole-Jean Wu"], "title": "PRISM: Probabilistic Runtime Insights and Scalable Performance Modeling for Large-Scale Distributed Training", "comment": null, "summary": "Large model training beyond tens of thousands of GPUs is an uncharted\nterritory. At such scales, disruptions to the training process are not a matter\nof if, but a matter of when -- a stochastic process degrading training\nproductivity. Dynamic runtime variation will become increasingly more frequent\nas training scales up and GPUs are operated in increasingly power-limited and\nthermally-stressed environments. At the 64k GPU scale, we already observed 9%\nGPU time variability for frontier foundation model training. To understand\npotential causes of variability, we analyze GPU microbenchmarks at scale across\na variety of platforms, showing up to 14% variation in GPU performance on GEMM\nworkloads depending on training hardware and deployed environment.\n  Motivated by our analysis and the large design space around performance\nvariability, we present PRISM -- a performance modeling framework that\nconsiders the stochastic nature of the large-scale distributed training. The\ncore of PRISM is the statistical method that provides a quantifiable measure\nfor probabilistic guarantees on training time. Using PRISM, we explore the\ndesign and optimization space of distributed training, from parallelization\nmethods to next-generation training systems. PRISM is validated with\nreal-system measurement, showing training time prediction accuracy with 20.8%\nKolmogorov-Smirnov distance. Using PRISM, we demonstrate that, depending on\ncomputation node placement, up to 1.26x performance improvement potential is\navailable if we factor in sensitivities of parallelization strategies to\nvariation. In addition, we use PRISM to identify kernels to optimize for\nreducing performance variability and predict probability of slow-down for\nlarge-scale jobs where variation is magnified. We find optimizing communication\nkernels, such as AllGather and ReduceScatter, contribute most to minimizing\nvariability in training step time.", "AI": {"tldr": "PRISM\u662f\u4e00\u4e2a\u6027\u80fd\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u6027\u80fd\u53d8\u5f02\u6027\uff0c\u901a\u8fc7\u7edf\u8ba1\u65b9\u6cd5\u63d0\u4f9b\u8bad\u7ec3\u65f6\u95f4\u7684\u6982\u7387\u4fdd\u8bc1\uff0c\u5e76\u4f18\u5316\u5e76\u884c\u5316\u7b56\u7565\u4ee5\u51cf\u5c11\u53d8\u5f02\u6027\u5f71\u54cd\u3002", "motivation": "\u968f\u7740GPU\u8bad\u7ec3\u89c4\u6a21\u6269\u5c55\u5230\u6570\u4e07\u4e2aGPU\uff0c\u6027\u80fd\u53d8\u5f02\u6027\u6210\u4e3a\u5fc5\u7136\u95ee\u9898\uff0c\u572864k GPU\u89c4\u6a21\u4e0b\u5df2\u89c2\u5bdf\u52309%\u7684GPU\u65f6\u95f4\u53d8\u5f02\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u65b9\u6cd5\u6765\u7406\u89e3\u548c\u4f18\u5316\u8fd9\u79cd\u968f\u673a\u6027\u80fd\u6ce2\u52a8\u3002", "method": "\u5f00\u53d1PRISM\u6027\u80fd\u5efa\u6a21\u6846\u67b6\uff0c\u4f7f\u7528\u7edf\u8ba1\u65b9\u6cd5\u91cf\u5316\u8bad\u7ec3\u65f6\u95f4\u7684\u6982\u7387\u4fdd\u8bc1\uff0c\u5206\u6790GPU\u5fae\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63a2\u7d22\u5e76\u884c\u5316\u65b9\u6cd5\u548c\u8bad\u7ec3\u7cfb\u7edf\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002", "result": "PRISM\u9a8c\u8bc1\u663e\u793a\u8bad\u7ec3\u65f6\u95f4\u9884\u6d4b\u51c6\u786e\u5ea6\u8fbe\u523020.8% KS\u8ddd\u79bb\uff0c\u901a\u8fc7\u4f18\u5316\u8282\u70b9\u5e03\u5c40\u53ef\u83b7\u5f971.26\u500d\u6027\u80fd\u63d0\u5347\u6f5c\u529b\uff0c\u53d1\u73b0AllGather\u548cReduceScatter\u901a\u4fe1\u5185\u6838\u4f18\u5316\u5bf9\u51cf\u5c11\u8bad\u7ec3\u6b65\u9aa4\u65f6\u95f4\u53d8\u5f02\u6027\u8d21\u732e\u6700\u5927\u3002", "conclusion": "\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\u5fc5\u987b\u8003\u8651\u6027\u80fd\u53d8\u5f02\u6027\uff0cPRISM\u6846\u67b6\u80fd\u6709\u6548\u9884\u6d4b\u548c\u4f18\u5316\u8bad\u7ec3\u6027\u80fd\uff0c\u901a\u4fe1\u5185\u6838\u4f18\u5316\u662f\u51cf\u5c11\u53d8\u5f02\u6027\u7684\u5173\u952e\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u8bad\u7ec3\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2510.15652", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.15652", "abs": "https://arxiv.org/abs/2510.15652", "authors": ["Ahmad Raeisi", "Mahdi Dolati", "Sina Darabi", "Sadegh Talebi", "Patrick Eugster", "Ahmad Khonsari"], "title": "GOGH: Correlation-Guided Orchestration of GPUs in Heterogeneous Clusters", "comment": "10 pages, 5 figures", "summary": "The growing demand for computational resources in machine learning has made\nefficient resource allocation a critical challenge, especially in heterogeneous\nhardware clusters where devices vary in capability, age, and energy efficiency.\nUpgrading to the latest hardware is often infeasible, making sustainable use of\nexisting, mixed-generation resources essential. In this paper, we propose a\nlearning-based architecture for managing machine learning workloads in\nheterogeneous clusters. The system operates online, allocating resources to\nincoming training or inference requests while minimizing energy consumption and\nmeeting performance requirements. It uses two neural networks: the first\nprovides initial estimates of how well a new model will utilize different\nhardware types and how it will affect co-located models. An optimizer then\nallocates resources based on these estimates. After deployment, the system\nmonitors real performance and uses this data to refine its predictions via a\nsecond neural network. This updated model improves estimates not only for the\ncurrent hardware but also for hardware not initially allocated and for\nco-location scenarios not yet observed. The result is an adaptive, iterative\napproach that learns over time to make more effective resource allocation\ndecisions in heterogeneous deep learning clusters.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5b66\u4e60\u7684\u5f02\u6784\u96c6\u7fa4\u8d44\u6e90\u5206\u914d\u67b6\u6784\uff0c\u901a\u8fc7\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\u5728\u7ebf\u4f18\u5316\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\uff0c\u964d\u4f4e\u80fd\u8017\u5e76\u6ee1\u8db3\u6027\u80fd\u8981\u6c42", "motivation": "\u673a\u5668\u5b66\u4e60\u8ba1\u7b97\u9700\u6c42\u589e\u957f\uff0c\u5f02\u6784\u786c\u4ef6\u96c6\u7fa4\u4e2d\u8bbe\u5907\u80fd\u529b\u3001\u5e74\u9650\u548c\u80fd\u6548\u5404\u5f02\uff0c\u5347\u7ea7\u6700\u65b0\u786c\u4ef6\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u53ef\u6301\u7eed\u5229\u7528\u73b0\u6709\u6df7\u5408\u4ee3\u9645\u8d44\u6e90", "method": "\u4f7f\u7528\u4e24\u4e2a\u795e\u7ecf\u7f51\u7edc\uff1a\u7b2c\u4e00\u4e2a\u63d0\u4f9b\u65b0\u6a21\u578b\u5728\u4e0d\u540c\u786c\u4ef6\u4e0a\u7684\u5229\u7528\u7387\u4f30\u8ba1\u548c\u5171\u7f6e\u5f71\u54cd\uff0c\u4f18\u5316\u5668\u636e\u6b64\u5206\u914d\u8d44\u6e90\uff1b\u90e8\u7f72\u540e\u76d1\u63a7\u5b9e\u9645\u6027\u80fd\uff0c\u7528\u7b2c\u4e8c\u4e2a\u795e\u7ecf\u7f51\u7edc\u6539\u8fdb\u9884\u6d4b", "result": "\u5f00\u53d1\u51fa\u81ea\u9002\u5e94\u8fed\u4ee3\u65b9\u6cd5\uff0c\u968f\u65f6\u95f4\u5b66\u4e60\u5728\u5f02\u6784\u6df1\u5ea6\u5b66\u4e60\u96c6\u7fa4\u4e2d\u505a\u51fa\u66f4\u6709\u6548\u7684\u8d44\u6e90\u5206\u914d\u51b3\u7b56", "conclusion": "\u8be5\u5b66\u4e60\u578b\u67b6\u6784\u80fd\u591f\u6709\u6548\u7ba1\u7406\u5f02\u6784\u96c6\u7fa4\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5728\u6ee1\u8db3\u6027\u80fd\u8981\u6c42\u7684\u540c\u65f6\u6700\u5c0f\u5316\u80fd\u8017"}}
{"id": "2510.15698", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.15698", "abs": "https://arxiv.org/abs/2510.15698", "authors": ["Sebastian Brandt", "Tim G\u00f6ttlicher"], "title": "A Post-Quantum Lower Bound for the Distributed Lov\u00e1sz Local Lemma", "comment": "46 pages, 3 figures", "summary": "In this work, we study the Lov\\'asz local lemma (LLL) problem in the area of\ndistributed quantum computing, which has been the focus of attention of recent\nadvances in quantum computing [STOC'24, STOC'25, STOC'25]. We prove a lower\nbound of $2^{\\Omega(\\log^* n)}$ for the complexity of the distributed LLL in\nthe quantum-LOCAL model. More specifically, we obtain our lower bound already\nfor a very well-studied special case of the LLL, called sinkless orientation,\nin a stronger model than quantum-LOCAL, called the randomized online-LOCAL\nmodel. As a consequence, we obtain the same lower bounds for sinkless\norientation and the distributed LLL also in a variety of other models studied\nacross different research communities.\n  Our work provides the first superconstant lower bound for sinkless\norientation and the distributed LLL in all of these models, addressing recently\nstated open questions. Moreover, to obtain our results, we develop an entirely\nnew lower bound technique that we believe has the potential to become the first\ngeneric technique for proving post-quantum lower bounds for many of the most\nimportant problems studied in the context of locality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5728\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u9886\u57df\u7814\u7a76\u4e86Lov\u00e1sz\u5c40\u90e8\u5f15\u7406\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u91cf\u5b50LOCAL\u6a21\u578b\u4e2d\u5206\u5e03\u5f0fLLL\u7684\u590d\u6742\u5ea6\u4e0b\u754c\u4e3a2^\u03a9(log* n)\uff0c\u8fd9\u662f\u8be5\u95ee\u9898\u5728\u591a\u4e2a\u6a21\u578b\u4e2d\u7684\u9996\u4e2a\u8d85\u5e38\u6570\u4e0b\u754c\u3002", "motivation": "\u7814\u7a76\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684Lov\u00e1sz\u5c40\u90e8\u5f15\u7406\u95ee\u9898\uff0c\u89e3\u51b3\u8fd1\u671f\u63d0\u51fa\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u4e3a\u5206\u5e03\u5f0f\u91cf\u5b50\u7b97\u6cd5\u63d0\u4f9b\u7406\u8bba\u4e0b\u754c\u5206\u6790\u3002", "method": "\u901a\u8fc7\u7814\u7a76LLL\u7684\u7279\u6b8a\u60c5\u51b5sinkless orientation\u95ee\u9898\uff0c\u5728\u6bd4\u91cf\u5b50LOCAL\u66f4\u5f3a\u7684\u968f\u673a\u5728\u7ebfLOCAL\u6a21\u578b\u4e2d\u5efa\u7acb\u4e0b\u754c\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u4e0b\u754c\u8bc1\u660e\u6280\u672f\u3002", "result": "\u8bc1\u660e\u4e86\u5206\u5e03\u5f0fLLL\u548csinkless orientation\u5728\u91cf\u5b50LOCAL\u6a21\u578b\u4e2d\u7684\u590d\u6742\u5ea6\u4e0b\u754c\u4e3a2^\u03a9(log* n)\uff0c\u8fd9\u662f\u8be5\u95ee\u9898\u5728\u591a\u4e2a\u6a21\u578b\u4e2d\u7684\u9996\u4e2a\u8d85\u5e38\u6570\u4e0b\u754c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u89e3\u51b3\u4e86\u5f00\u653e\u6027\u95ee\u9898\uff0c\u8fd8\u5f00\u53d1\u4e86\u53ef\u80fd\u6210\u4e3a\u540e\u91cf\u5b50\u4e0b\u754c\u8bc1\u660e\u7684\u901a\u7528\u6280\u672f\uff0c\u5bf9\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u7684\u7406\u8bba\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.15755", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2510.15755", "abs": "https://arxiv.org/abs/2510.15755", "authors": ["Atsushi Koshiba", "Charalampos Mainas", "Pramod Bhatotia"], "title": "Funky: Cloud-Native FPGA Virtualization and Orchestration", "comment": "17 pages, ACM Symposium on Cloud Computing (SoCC'25)", "summary": "The adoption of FPGAs in cloud-native environments is facing impediments due\nto FPGA limitations and CPU-oriented design of orchestrators, as they lack\nvirtualization, isolation, and preemption support for FPGAs. Consequently,\ncloud providers offer no orchestration services for FPGAs, leading to low\nscalability, flexibility, and resiliency.\n  This paper presents Funky, a full-stack FPGA-aware orchestration engine for\ncloud-native applications. Funky offers primary orchestration services for FPGA\nworkloads to achieve high performance, utilization, scalability, and fault\ntolerance, accomplished by three contributions: (1) FPGA virtualization for\nlightweight sandboxes, (2) FPGA state management enabling task preemption and\ncheckpointing, and (3) FPGA-aware orchestration components following the\nindustry-standard CRI/OCI specifications.\n  We implement and evaluate Funky using four x86 servers with Alveo U50 FPGA\ncards. Our evaluation highlights that Funky allows us to port 23 OpenCL\napplications from the Xilinx Vitis and Rosetta benchmark suites by modifying\n3.4% of the source code while keeping the OCI image sizes 28.7 times smaller\nthan AMD's FPGA-accessible Docker containers. In addition, Funky incurs only\n7.4% performance overheads compared to native execution, while providing\nvirtualization support with strong hypervisor-enforced isolation and\ncloud-native orchestration for a set of distributed FPGAs. Lastly, we evaluate\nFunky's orchestration services in a large-scale cluster using Google production\ntraces, showing its scalability, fault tolerance, and scheduling efficiency.", "AI": {"tldr": "Funky\u662f\u4e00\u4e2a\u9762\u5411\u4e91\u539f\u751f\u5e94\u7528\u7684FPGA\u611f\u77e5\u7f16\u6392\u5f15\u64ce\uff0c\u901a\u8fc7FPGA\u865a\u62df\u5316\u3001\u72b6\u6001\u7ba1\u7406\u548c\u7f16\u6392\u7ec4\u4ef6\u89e3\u51b3\u4e86FPGA\u5728\u4e91\u73af\u5883\u4e2d\u7684\u7f16\u6392\u96be\u9898\u3002", "motivation": "FPGA\u5728\u4e91\u539f\u751f\u73af\u5883\u4e2d\u7684\u91c7\u7528\u9762\u4e34\u969c\u788d\uff0c\u56e0\u4e3aFPGA\u7f3a\u4e4f\u865a\u62df\u5316\u3001\u9694\u79bb\u548c\u62a2\u5360\u652f\u6301\uff0c\u5bfc\u81f4\u4e91\u63d0\u4f9b\u5546\u65e0\u6cd5\u63d0\u4f9bFPGA\u7f16\u6392\u670d\u52a1\uff0c\u9020\u6210\u4f4e\u53ef\u6269\u5c55\u6027\u3001\u7075\u6d3b\u6027\u548c\u5f39\u6027\u3002", "method": "Funky\u901a\u8fc7\u4e09\u4e2a\u4e3b\u8981\u8d21\u732e\u5b9e\u73b0\uff1a(1) FPGA\u865a\u62df\u5316\u521b\u5efa\u8f7b\u91cf\u7ea7\u6c99\u7bb1\uff1b(2) FPGA\u72b6\u6001\u7ba1\u7406\u652f\u6301\u4efb\u52a1\u62a2\u5360\u548c\u68c0\u67e5\u70b9\uff1b(3) \u9075\u5faa\u884c\u4e1a\u6807\u51c6CRI/OCI\u89c4\u8303\u7684FPGA\u611f\u77e5\u7f16\u6392\u7ec4\u4ef6\u3002", "result": "\u57284\u53f0x86\u670d\u52a1\u5668\u4e0a\u8bc4\u4f30\u663e\u793a\uff1aFunky\u53ea\u9700\u4fee\u65393.4%\u6e90\u4ee3\u7801\u5373\u53ef\u79fb\u690d23\u4e2aOpenCL\u5e94\u7528\uff0cOCI\u955c\u50cf\u6bd4AMD\u7684FPGA Docker\u5bb9\u5668\u5c0f28.7\u500d\uff0c\u6027\u80fd\u5f00\u9500\u4ec57.4%\uff0c\u540c\u65f6\u63d0\u4f9b\u5f3a\u9694\u79bb\u548c\u5206\u5e03\u5f0fFPGA\u7f16\u6392\u3002", "conclusion": "Funky\u5c55\u793a\u4e86\u5728\u5927\u578b\u96c6\u7fa4\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3001\u5bb9\u9519\u6027\u548c\u8c03\u5ea6\u6548\u7387\uff0c\u4e3a\u4e91\u539f\u751f\u73af\u5883\u4e2d\u7684FPGA\u7f16\u6392\u63d0\u4f9b\u4e86\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\u3002"}}
