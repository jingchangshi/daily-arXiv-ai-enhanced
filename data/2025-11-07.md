<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Modular abstract syntax trees (MAST): substitution tensors with second-class sorts](https://arxiv.org/abs/2511.03946)
*Marcelo P. Fiore,Ohad Kammar,Georg Moser,Sam Staton*

Main category: cs.PL

TL;DR: 本文扩展了Fiore等人的抽象语法理论，以处理具有二阶类型的语言（如CBV和CBPV），通过使用actegories而非monoidal categories来重新描述抽象语法，并应用该理论证明CBV变体的替换引理。


<details>
  <summary>Details</summary>
Motivation: 现有抽象语法理论无法很好地处理具有二阶类型的编程演算（如CBV和CBPV），需要扩展理论框架以支持这些语言。

Method: 将Fiore等人的抽象语法理论从monoidal categories扩展到actegories，使用双范畴论证，并禁止二阶类型出现在变量上下文中。

Result: 成功建立了适用于二阶类型语言的抽象语法理论框架，并应用该框架证明了CBV变体的替换引理。

Conclusion: 通过将抽象语法的特征从monoidal categories转向actegories，可以有效地处理具有二阶类型的编程语言，为这类语言的语义分析提供了理论基础。

Abstract: We adapt Fiore, Plotkin, and Turi's treatment of abstract syntax with
binding, substitution, and holes to account for languages with second-class
sorts. These situations include programming calculi such as the Call-by-Value
lambda-calculus (CBV) and Levy's Call-by-Push-Value (CBPV). Prohibiting
second-class sorts from appearing in variable contexts changes the
characterisation of the abstract syntax from monoids in monoidal categories to
actions in actegories. We reproduce much of the development through
bicategorical arguments. We apply the resulting theory by proving substitution
lemmata for varieties of CBV.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms](https://arxiv.org/abs/2511.03866)
*Arijit Bhattacharjee,Ali TehraniJamsaz,Le Chen,Niranjan Hasabnis,Mihai Capota,Nesreen Ahmed,Ali Jannesari*

Main category: cs.DC

TL;DR: OMPILOT是一个专门用于将C++代码翻译成OpenMP的领域特定编码器-解码器转换器，通过结合无监督和监督学习策略提高代码翻译的鲁棒性，并在函数级别进行操作以捕获更广泛的语义上下文。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码翻译方面取得了显著进展，但现有方法主要关注循环级转换，缺乏对更广泛语义上下文的捕获能力。

Method: 开发了OMPILOT模型，采用自定义预训练目标结合并行构造语义，使用无监督和监督学习策略，并在函数级别进行代码翻译。

Result: 提出了OMPBLEU评估指标来评估OpenMP并行构造的正确性和质量，解决了传统翻译指标的局限性。

Conclusion: OMPILOT在C++到OpenMP的代码翻译中表现出色，能够有效实现共享内存并行化，为遗留代码迁移提供了高效解决方案。

Abstract: Recent advances in large language models (LLMs) have significantly
accelerated progress in code translation, enabling more accurate and efficient
transformation across programming languages. While originally developed for
natural language processing, LLMs have shown strong capabilities in modeling
programming language syntax and semantics, outperforming traditional rule-based
systems in both accuracy and flexibility. These models have streamlined
cross-language conversion, reduced development overhead, and accelerated legacy
code migration. In this paper, we introduce OMPILOT, a novel domain-specific
encoder-decoder transformer tailored for translating C++ code into OpenMP,
enabling effective shared-memory parallelization. OMPILOT leverages custom
pre-training objectives that incorporate the semantics of parallel constructs
and combines both unsupervised and supervised learning strategies to improve
code translation robustness. Unlike previous work that focused primarily on
loop-level transformations, OMPILOT operates at the function level to capture a
wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel
composite metric specifically crafted to assess the correctness and quality of
OpenMP parallel constructs, addressing limitations in conventional translation
metrics.

</details>


### [3] [Stochastic Modeling for Energy-Efficient Edge Infrastructure](https://arxiv.org/abs/2511.03941)
*Fabio Diniz Rossi*

Main category: cs.DC

TL;DR: 该论文提出了一种基于马尔可夫链的随机建模方法，用于分析边缘计算中的功率状态转换，通过AI驱动的预测性功率缩放显著提高了能源效率。


<details>
  <summary>Details</summary>
Motivation: 边缘计算虽然支持低延迟处理，但由于边缘设备的分布式特性和有限的能源资源，在功率管理方面面临挑战。

Method: 使用马尔可夫链进行随机建模，推导稳态概率并评估能耗，通过蒙特卡洛模拟验证模型，并进行敏感性分析。

Result: 实验结果表明，AI驱动的预测性功率缩放相比传统反应式方法，能减少不必要的状态转换，提高系统响应性，优化异构边缘节点间的负载分配，降低能耗差异。

Conclusion: 基于AI的功率管理策略通过预测工作负载需求并优化状态转换，显著提高了边缘计算系统的能源效率。

Abstract: Edge Computing enables low-latency processing for real-time applications but
introduces challenges in power management due to the distributed nature of edge
devices and their limited energy resources. This paper proposes a stochastic
modeling approach using Markov Chains to analyze power state transitions in
Edge Computing. By deriving steady-state probabilities and evaluating energy
consumption, we demonstrate the benefits of AI-driven predictive power scaling
over conventional reactive methods. Monte Carlo simulations validate the model,
showing strong alignment between theoretical and empirical results. Sensitivity
analysis highlights how varying transition probabilities affect power
efficiency, confirming that predictive scaling minimizes unnecessary
transitions and improves overall system responsiveness. Our findings suggest
that AI-based power management strategies significantly enhance energy
efficiency by anticipating workload demands and optimizing state transitions.
Experimental results indicate that AI-based power management optimizes workload
distribution across heterogeneous edge nodes, reducing energy consumption
disparities between devices, improving overall efficiency, and enhancing
adaptive power coordination in multi-node environments.

</details>


### [4] [Parallel Spawning Strategies for Dynamic-Aware MPI Applications](https://arxiv.org/abs/2511.04268)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo,Sergio Iserte*

Main category: cs.DC

TL;DR: 提出了一种新的并行生成策略，用于MPI应用程序的动态资源管理，通过进程协作生成和重新分配，显著降低了收缩操作成本，同时保持竞争性的扩展时间。


<details>
  <summary>Details</summary>
Motivation: 现有MPI应用程序的动态资源管理方法存在局限性：要么重新生成整个应用程序（成本高），要么在收缩时无法完全释放不需要的进程。这限制了系统资源利用效率和作业等待时间的改善。

Method: 采用并行生成策略，所有进程在重新分配前协作进行生成，从而减少执行时间。同时消除了收缩限制，使并行系统能更好地适应工作负载。

Result: 该方法在保持竞争性扩展时间（最多1.25倍开销）的同时，实现了快速的收缩操作，将收缩成本降低了至少20倍。在异构和同构系统上均得到验证。

Conclusion: 所提出的并行生成策略有效克服了现有MPI动态资源管理方法的局限性，显著降低了重新配置成本，提高了系统资源利用效率，适用于共享资源环境。

Abstract: Dynamic resource management is an increasingly important capability of High
Performance Computing systems, as it enables jobs to adjust their resource
allocation at runtime. This capability has been shown to reduce workload
makespan, substantially decrease job waiting times and improve overall system
utilization. In this context, malleability refers to the ability of
applications to adapt to new resource allocations during execution. Although
beneficial, malleability incurs significant reconfiguration costs, making the
reduction of these costs an important research topic.
  Some existing methods for MPI applications respawn the entire application,
which is an expensive solution that avoids the reuse of original processes.
Other MPI methods reuse them, but fail to fully release unneeded processes when
shrinking, since some ranks within the same communicator remain active across
nodes, preventing the application from returning those nodes to the system.
This work overcomes both limitations by proposing a novel parallel spawning
strategy, in which all processes cooperate in spawning before redistribution,
thereby reducing execution time. Additionally, it removes shrinkage
limitations, allowing better adaptation of parallel systems to workload and
reducing their makespan. As a result, it preserves competitive expansion times
with at most a $1.25\times$ overhead, while enabling fast shrink operations
that reduce their cost by at least $20\times$. This strategy has been validated
on both homogeneous and heterogeneous systems and can also be applied in
shared-resource environments.

</details>


### [5] [Enabling Dynamic Sparsity in Quantized LLM Inference](https://arxiv.org/abs/2511.04477)
*Rongxiang Wang,Kangyuan Shu,Felix Xiaozhu Lin*

Main category: cs.DC

TL;DR: 提出一种在低比特量化下实现动态稀疏推理的技术，包括zigzag量化布局、专用GEMV内核和紧凑运行时机制，在保持精度的同时提升解码吞吐量1.55倍


<details>
  <summary>Details</summary>
Motivation: 在终端设备部署大语言模型面临内存和计算能力限制，而LLM内部激活的动态稀疏性与主流的组量化方法存在冲突，需要解决这一矛盾

Method: 采用zigzag模式的量化布局以匹配激活稀疏性并改善GPU内存局部性；设计专用GEMV内核充分利用并行计算单元；开发紧凑运行时机制以最小开销收集稀疏索引

Result: 在多种模型规模和硬件配置下，该方法实现了最高1.55倍的解码吞吐量提升，同时保持与密集量化推理相当的精度

Conclusion: 结构化稀疏性和量化可以在商用GPU上有效共存，为资源受限设备上的高效LLM部署提供了可行方案

Abstract: Deploying large language models (LLMs) on end-user devices is gaining
importance due to benefits in responsiveness, privacy, and operational cost.
Yet the limited memory and compute capability of mobile and desktop GPUs make
efficient execution difficult. Recent observations suggest that the internal
activations of LLMs are often dynamically sparse, meaning that for each input,
only part of the network contributes significantly to the output. Such sparsity
could reduce computation, but it interacts poorly with group-wise quantization,
which remains the dominant approach for fitting LLMs onto resource-constrained
hardware. To reconcile these two properties, this study proposes a set of
techniques that realize dynamic sparse inference under low-bit quantization.
The method features: (1) a zigzag-patterned quantization layout that organizes
weights in a way consistent with activation sparsity and improves GPU memory
locality; (2) a specialized GEMV kernel designed for this layout to fully
utilize parallel compute units; and (3) a compact runtime mechanism that
gathers sparse indices with minimal overhead. Across several model scales and
hardware configurations, the approach achieves up to 1.55x faster decoding
throughput while maintaining accuracy comparable to dense quantized inference,
showing that structured sparsity and quantization can effectively coexist on
commodity GPUs.

</details>


### [6] [A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems](https://arxiv.org/abs/2511.04523)
*Silvia Bonomi,Giovanni Farina,Roy Friedman,Eviatar B. Procaccia,Sebastien Tixeuil*

Main category: cs.DC

TL;DR: 提出了一种基于MAPE-K架构的自保护分布式系统，引入概率性移动拜占庭故障模型来模拟动态攻击演化，并分析了拜占庭节点数量超过阈值的时间以及系统自恢复的概率。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统面临日益增长的安全威胁，攻击者技能不断提升，漏洞遍布整个系统栈。现有拜占庭故障模型在反映现实场景准确性方面存在局限。

Method: 在MAPE-K架构的分析组件中引入新的概率性移动拜占庭故障模型，通过数学分析计算拜占庭节点超过阈值的时间和系统自恢复概率，并进行仿真验证。

Result: 建立了拜占庭感染传播速率与自恢复速率之间的数学关系模型，能够准确预测系统状态变化，仿真结果验证了模型的有效性。

Conclusion: 提出的概率性移动拜占庭故障模型能够更好地捕捉动态攻击演化，为自保护和重配置策略提供理论指导，增强了分布式系统的安全性和弹性。

Abstract: Modern distributed systems face growing security threats, as attackers
continuously enhance their skills and vulnerabilities span across the entire
system stack, from hardware to the application layer. In the system design
phase, fault tolerance techniques can be employed to safeguard systems. From a
theoretical perspective, an attacker attempting to compromise a system can be
abstracted by considering the presence of Byzantine processes in the system.
Although this approach enhances the resilience of the distributed system, it
introduces certain limitations regarding the accuracy of the model in
reflecting real-world scenarios. In this paper, we consider a self-protecting
distributed system based on the \emph{Monitoring-Analyse-Plan-Execute over a
shared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic
Mobile Byzantine Failure (MBF) that can be plugged into the Analysis component.
Our new model captures the dynamics of evolving attacks and can be used to
drive the self-protection and reconfiguration strategy. We analyze
mathematically the time that it takes until the number of Byzantine nodes
crosses given thresholds, or for the system to self-recover back into a safe
state, depending on the rates of Byzantine infection spreading \emph{vs.} the
rate of self-recovery. We also provide simulation results that illustrate the
behavior of the system under such assumptions.

</details>


### [7] [Resolving Conflicts with Grace: Dynamically Concurrent Universality](https://arxiv.org/abs/2511.04631)
*Petr Kuznetsov,Nathan Josia Schrodt*

Main category: cs.DC

TL;DR: 提出了动态并发概念，仅在需要根据当前系统状态与并发操作进行仲裁时才使用强同步原语，并提出了动态并发通用构造。


<details>
  <summary>Details</summary>
Motivation: 同步是分布式计算可扩展性的主要障碍。并发操作在遇到冲突时进行同步，而冲突通常只在某些罕见状态下发生。理想情况下，希望能够动态检测冲突，即根据当前系统状态进行调整。

Method: 定义了动态并发概念，并提出了动态并发通用构造，使操作仅在需要仲裁时才使用强同步原语。

Result: 提出了动态并发通用构造，能够根据当前系统状态动态调整同步需求。

Conclusion: 动态并发方法能够减少不必要的同步开销，提高分布式系统的可扩展性。

Abstract: Synchronization is the major obstacle to scalability in distributed
computing. Concurrent operations on the shared data engage in synchronization
when they encounter a \emph{conflict}, i.e., their effects depend on the order
in which they are applied. Ideally, one would like to detect conflicts in a
\emph{dynamic} manner, i.e., adjusting to the current system state. Indeed, it
is very common that two concurrent operations conflict only in some rarely
occurring states. In this paper, we define the notion of \emph{dynamic
concurrency}: an operation employs strong synchronization primitives only if it
\emph{has} to arbitrate with concurrent operations, given the current system
state. We then present a dynamically concurrent universal construction.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies](https://arxiv.org/abs/2511.03944)
*Tong Zhang,Vikram Sharma Mailthody,Fei Sun,Linsen Ma,Chris J. Newburn,Teresa Zhang,Yang Liu,Jiangpeng Li,Hao Zhong,Wen-Mei Hwu*

Main category: cs.AR

TL;DR: 本文重新审视了经典的5分钟规则，将其从单纯的经济学启发式方法发展为包含主机成本、DRAM带宽/容量限制、SSD性能模型和实际工作负载的综合性分析框架。研究发现，在现代AI平台中，DRAM到闪存的缓存阈值从分钟级降至秒级，这重新定义了NAND闪存作为活跃数据层的角色。


<details>
  <summary>Details</summary>
Motivation: 传统的5分钟规则仅基于存储-内存经济学，忽略了主机成本、可行性限制和工作负载行为。随着现代AI平台（特别是GPU主机与高性能SSD配对）的发展，需要从第一性原理重新审视这一规则。

Method: 从第一性原理出发，整合主机成本、DRAM带宽/容量限制、基于物理的SSD性能和成本模型，并将其嵌入到约束和工作负载感知的框架中。开发了MQSim-Next SSD模拟器进行验证和敏感性分析。

Result: 对于现代AI平台，特别是GPU主机与超高性能SSD配对时，DRAM到闪存的缓存阈值从分钟级崩溃到秒级。这重新定义了NAND闪存作为活跃数据层的角色，并揭示了硬件-软件栈的广泛研究空间。

Conclusion: 将经典启发式方法转变为可操作的、可行性感知的分析和配置框架，为AI时代内存层次结构的进一步研究奠定了基础。

Abstract: In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a
simple, storage-memory-economics-based heuristic for deciding when data should
live in DRAM rather than on storage. Subsequent revisits to the rule largely
retained that economics-only view, leaving host costs, feasibility limits, and
workload behavior out of scope. This paper revisits the rule from first
principles, integrating host costs, DRAM bandwidth/capacity, and
physics-grounded models of SSD performance and cost, and then embedding these
elements in a constraint- and workload-aware framework that yields actionable
provisioning guidance. We show that, for modern AI platforms, especially
GPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained
random access, the DRAM-to-flash caching threshold collapses from minutes to a
few seconds. This shift reframes NAND flash memory as an active data tier and
exposes a broad research space across the hardware-software stack. We further
introduce MQSim-Next, a calibrated SSD simulator that supports validation and
sensitivity analysis and facilitates future architectural and system research.
Finally, we present two concrete case studies that showcase the software system
design space opened by such memory hierarchy paradigm shift. Overall, we turn a
classical heuristic into an actionable, feasibility-aware analysis and
provisioning framework and set the stage for further research on AI-era memory
hierarchy.

</details>


### [9] [PICNIC: Silicon Photonic Interconnected Chiplets with Computational Network and In-memory Computing for LLM Inference Acceleration](https://arxiv.org/abs/2511.04036)
*Yue Jiet Chong,Yimin Wang,Zhen Wu,Xuanyao Fong*

Main category: cs.AR

TL;DR: 提出了一种基于3D堆叠芯粒的大语言模型推理加速器，采用非易失性内存计算处理单元和硅光互连技术解决通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型推理中的通信瓶颈问题，提高计算效率和可扩展性。

Method: 使用3D堆叠芯粒架构，集成非易失性内存计算处理单元和硅光子互连的IPCN网络，开发了专门的LLM映射方案来优化硬件调度和工作负载映射。

Result: 相比Nvidia A100实现了3.95倍加速和30倍效率提升；在实施芯粒聚类和功率门控方案后，相比Nvidia H100实现了57倍效率提升。

Conclusion: 该3D堆叠芯粒架构能有效解决LLM推理的通信瓶颈，显著提升性能和能效，并具有良好的可扩展性。

Abstract: This paper presents a 3D-stacked chiplets based large language model (LLM)
inference accelerator, consisting of non-volatile in-memory-computing
processing elements (PEs) and Inter-PE Computational Network (IPCN),
interconnected via silicon photonic to effectively address the communication
bottlenecks. A LLM mapping scheme was developed to optimize hardware scheduling
and workload mapping. Simulation results show it achieves $3.95\times$ speedup
and $30\times$ efficiency improvement over the Nvidia A100 before chiplet
clustering and power gating scheme (CCPG). Additionally, the system achieves
further scalability and efficiency improvement with the implementation of CCPG
to accommodate larger models, attaining $57\times$ efficiency improvement over
Nvidia H100 at similar throughput.

</details>


### [10] [Disaggregated Architectures and the Redesign of Data Center Ecosystems: Scheduling, Pooling, and Infrastructure Trade-offs](https://arxiv.org/abs/2511.04104)
*Chao Guo,Jiahe Xu,Moshe Zukerman*

Main category: cs.AR

TL;DR: 硬件解耦将数据中心资源从传统服务器集群转变为统一资源池，本文概述了其动机、最新进展、研究挑战和机遇，并指出这将重塑整个数据中心生态系统。


<details>
  <summary>Details</summary>
Motivation: 硬件解耦旨在解决传统服务器架构的资源利用效率低、扩展性受限等问题，通过资源池化实现更灵活高效的资源分配。

Method: 采用综述分析方法，结合数值研究来阐述硬件解耦面临的关键挑战。

Result: 硬件解耦在工业界和学术界都取得了显著进展，但完全实现仍面临技术挑战。

Conclusion: 硬件解耦有潜力重塑数据中心生态系统，影响应用设计、资源调度、硬件配置、冷却和电力系统优化等多个方面。

Abstract: Hardware disaggregation seeks to transform Data Center (DC) resources from
traditional server fleets into unified resource pools. Despite existing
challenges that may hinder its full realization, significant progress has been
made in both industry and academia. In this article, we provide an overview of
the motivations and recent advancements in hardware disaggregation. We further
discuss the research challenges and opportunities associated with disaggregated
architectures, focusing on aspects that have received limited attention. We
argue that hardware disaggregation has the potential to reshape the entire DC
ecosystem, impacting application design, resource scheduling, hardware
configuration, cooling, and power system optimization. Additionally, we present
a numerical study to illustrate several key aspects of these challenges.

</details>


### [11] [AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM](https://arxiv.org/abs/2511.04321)
*Yuanpeng Zhang,Xing Hu,Xi Chen,Zhihang Yuan,Cong Li,Jingchen Zhu,Zhao Wang,Chenguang Zhang,Xin Si,Wei Gao,Qiang Wu,Runsheng Wang,Guangyu Sun*

Main category: cs.AR

TL;DR: AIM是一种针对高性能SRAM存内计算(PIM)的软硬件协同设计方法，通过架构级IR-drop缓解技术，在7nm 256-TOPS PIM芯片上实现了69.2%的IR-drop缓解、2.29倍能效提升和1.152倍加速。


<details>
  <summary>Details</summary>
Motivation: SRAM存内计算虽然具有高性能优势，但追求更高性能会导致更复杂的电路设计和更高的工作频率，从而加剧IR-drop问题，影响芯片性能和可靠性。传统电路级IR-drop缓解方法资源密集且会损害PPA(功耗、性能、面积)。

Method: 利用PIM的位串行和原位数据流处理特性，提出Rtog和HR建立PIM工作负载与IR-drop的直接关联；开发LHR和WDS进行架构级IR-drop缓解探索；设计IR-Booster动态调整机制，集成软件级HR信息和硬件IR-drop监测来调整PIM宏的电压-频率对；提出HR感知的任务映射方法。

Result: 在7nm 256-TOPS PIM芯片上的后布局仿真结果显示，AIM实现了高达69.2%的IR-drop缓解，带来2.29倍的能效提升和1.152倍的加速。

Conclusion: AIM通过软硬件协同设计成功解决了高性能PIM中的IR-drop问题，在保持计算精度的同时显著提升了能效和性能。

Abstract: SRAM Processing-in-Memory (PIM) has emerged as the most promising
implementation for high-performance PIM, delivering superior computing density,
energy efficiency, and computational precision. However, the pursuit of higher
performance necessitates more complex circuit designs and increased operating
frequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly
degrade chip performance and even threaten reliability. Conventional
circuit-level IR-drop mitigation methods, such as back-end optimizations, are
resource-intensive and often compromise power, performance, and area (PPA). To
address these challenges, we propose AIM, comprehensive software and hardware
co-design for architecture-level IR-drop mitigation in high-performance PIM.
Initially, leveraging the bit-serial and in-situ dataflow processing properties
of PIM, we introduce Rtog and HR, which establish a direct correlation between
PIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS,
enabling extensive exploration of architecture-level IR-drop mitigation while
maintaining computational accuracy through software optimization. Subsequently,
we develop IR-Booster, a dynamic adjustment mechanism that integrates
software-level HR information with hardware-based IR-drop monitoring to adapt
the V-f pairs of the PIM macro, achieving enhanced energy efficiency and
performance. Finally, we propose the HR-aware task mapping method, bridging
software and hardware designs to achieve optimal improvement. Post-layout
simulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up
to 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement
and 1.152x speedup.

</details>


### [12] [Scalable and Efficient Intra- and Inter-node Interconnection Networks for Post-Exascale Supercomputers and Data centers](https://arxiv.org/abs/2511.04677)
*Joaquin Tarraga-Moreno,Daniel Barley,Francisco J. Andujar Munoz,Jesus Escudero-Sahuquillo,Holger Froning,Pedro Javier Garcia,Francisco J. Quiles,Jose Duato*

Main category: cs.AR

TL;DR: 现代超级计算机和数据中心正朝着异构和紧密集成的架构发展，以支持数据密集型应用，但加速器数量的增加导致了通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI、科学模拟和大规模分析等数据密集型应用的快速增长，推动了对高性能计算架构的需求，需要减少数据移动并提高计算效率。

Method: 结合强大的CPU和加速器，采用新兴的高带宽内存和存储技术，构建异构和紧密集成的系统架构。

Result: 随着每个节点中加速器数量的增加，在节点内部和节点之间出现了通信瓶颈，特别是在网络资源被异构组件共享时。

Conclusion: 虽然异构集成架构提高了计算效率，但通信瓶颈成为限制系统性能的关键挑战，需要进一步优化网络资源分配和通信机制。

Abstract: The rapid growth of data-intensive applications such as generative AI,
scientific simulations, and large-scale analytics is driving modern
supercomputers and data centers toward increasingly heterogeneous and tightly
integrated architectures. These systems combine powerful CPUs and accelerators
with emerging high-bandwidth memory and storage technologies to reduce data
movement and improve computational efficiency. However, as the number of
accelerators per node increases, communication bottlenecks emerge both within
and between nodes, particularly when network resources are shared among
heterogeneous components.

</details>
