<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Simple Modal Types for Functional Reactive Programming](https://arxiv.org/abs/2512.09412)
*Patrick Bahr*

Main category: cs.PL

TL;DR: 本文提出了一种新的函数响应式编程语言，采用简化的模态类型系统，在保证因果性、生产性和无空间泄漏的同时，减少了对程序的限制，提高了表达能力和运行效率。


<details>
  <summary>Details</summary>
Motivation: 函数响应式编程（FRP）虽然提供了高层次的声明式编程范式，但在实际应用中需要确保程序的因果性、生产性和无空间泄漏。现有的模态类型系统虽然能保证这些性质，但往往对程序施加了过多限制，影响了语言的表达能力。

Method: 通过改变信号的语义，将信号建模为受"later"模态类型严格控制的可变引用。这种新的语义允许类型系统安全地接受更多程序，同时通过受控的可变性支持更高效的原位更新。

Result: 新语言在保持函数式编程风格的同时，实现了比以往模态FRP语言更少的限制、更强的表达能力，并能保证因果性、生产性和无空间泄漏的核心操作性质。

Conclusion: 通过重新设计信号的语义，可以构建一个既保持FRP核心保证又具有更好实用性的语言，在类型系统的严谨性和编程灵活性之间取得了更好的平衡。

Abstract: Functional reactive programming (FRP) is a declarative programming paradigm for implementing reactive programs at a high level of abstraction. It applies functional programming principles to construct and manipulate time-varying values, also known as signals. However, for this programming paradigm to work in practice, an FRP language must ensure that programs are causal, productive, and free from space leaks. Over the past fifteen years, several modal type systems to enforce these operational properties have been developed.
  We present a new FRP language with a significantly simplified modal type system that imposes fewer restrictions than previous modal FRP languages while still guaranteeing the central operational properties of causality, productivity, and absence of space leaks. The key enabling idea is to alter the semantics of signals so that the type system can safely allow more programs to type-check, which also makes the language more expressive. With this new semantics, signals are modelled as mutable references whose mutability is tightly controlled by the 'later' type modality. This disciplined form of mutability also enables more efficient in-place updates of signals, all while preserving a functional programming style.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens](https://arxiv.org/abs/2512.09277)
*Yanpeng Yu,Haiyue Ma,Krish Agarwal,Nicolai Oswald,Qijing Huang,Hugo Linsenmaier,Chunhui Mei,Ritchie Zhao,Ritika Borkar,Bita Darvish Rouhani,David Nellans,Ronny Krashinsky,Anurag Khandelwal*

Main category: cs.DC

TL;DR: METRO是一种针对内存受限场景的MoE模型专家并行路由算法，通过平衡GPU上的激活专家数量而非令牌数量来提升性能，相比现有方法可降低解码延迟11-22%，提升吞吐量最高达4.11倍。


<details>
  <summary>Details</summary>
Motivation: 现有专家并行方法通过平衡各GPU处理的令牌数量来解决负载不均衡问题，但在内存受限场景（特别是MoE服务的解码阶段）中，这种平衡反而会降低性能，因为会增加激活专家数量，加剧内存压力。

Method: 提出METRO（Minimum Expert Token ROuting）算法：1）平衡各GPU的激活专家数量而非令牌数量；2）联合优化算法效率并利用GPU并行处理能力实现接近最优的路由质量；3）采用新颖的allGather方案收集全局top-k信息，相比传统allToAll开销更小。

Result: 在真实系统（vLLM over 8 A100 GPUs）和专有模拟器（8-16 B200 GPUs）上的评估显示：1）解码延迟降低11-22%；2）Qwen3和DeepSeek-V3服务的总令牌吞吐量提升3-21%；3）在固定解码SLO下，解码吞吐量最高提升4.11倍。

Conclusion: 在内存受限的MoE服务场景中，平衡激活专家数量而非令牌数量是关键优化方向。METRO算法通过创新的路由策略和通信优化，显著提升了专家并行服务的性能和效率。

Abstract: Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.
  We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.

</details>


### [3] [A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge](https://arxiv.org/abs/2512.09309)
*Zihao Ding,Mufeng Zhu,Zhongze Tang,Sheng Wei,Yao Liu*

Main category: cs.DC

TL;DR: 提出分层分布式卸载框架，将视觉数据分割到多个云服务器，防止单服务器重建完整图像，保护隐私


<details>
  <summary>Details</summary>
Motivation: 移动和可穿戴设备计算能力有限，但云卸载会带来传输和服务器端的隐私泄露风险，需要保护视觉数据隐私

Method: 使用本地可信边缘设备作为协调器，将视觉数据分割成小块分发到多个独立云服务器，最终合并计算只在边缘设备进行

Result: 在SAM模型上验证，保持接近基准的分割性能，显著降低内容重建和用户数据暴露风险

Conclusion: 提供了一种可扩展的隐私保护解决方案，适用于边缘云连续体中的视觉任务

Abstract: Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.

</details>


### [4] [Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN](https://arxiv.org/abs/2512.09331)
*Nam Anh Dang,Ben Landrum,Ken Birman*

Main category: cs.DC

TL;DR: BatANN：首个开源分布式磁盘向量检索系统，通过单全局图实现对数搜索效率，在10台服务器上对10亿级数据集达到0.95召回率，吞吐量比基准提升2.5-6.49倍


<details>
  <summary>Details</summary>
Motivation: 随着数据集扩展到数十亿向量，磁盘向量检索成为实用方案，但需要预见到未来数据集可能超过单台服务器容量，因此需要分布式解决方案

Method: 提出BatANN分布式磁盘近似最近邻系统，核心创新是当访问存储在其他机器上的邻域时，将查询完整状态发送到目标机器继续执行，提高局部性，保持单全局图的对数搜索效率

Result: 在100M和1B点数据集上，使用10台服务器达到0.95召回率时，吞吐量分别比scatter-gather基准提升6.21-6.49倍和2.5-5.10倍，平均延迟低于6毫秒，且基于标准TCP实现

Conclusion: BatANN是首个开源分布式磁盘向量检索系统，能在单全局图上操作，实现近线性的吞吐量扩展，为大规模向量搜索提供了高效解决方案

Abstract: Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.

</details>


### [5] [WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving](https://arxiv.org/abs/2512.09472)
*Chiheng Lou,Sheng Qi,Rui Kang,Yong Zhang,Chen Sun,Pengcheng Wang,Bingyang Liu,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: WarmServe是一个多LLM服务系统，通过通用GPU工作器实现基于未来工作负载预测的预热，显著提升首令牌时间(TTFT)性能


<details>
  <summary>Details</summary>
Motivation: 现有多LLM服务系统在提升GPU利用率的同时牺牲了推理性能，特别是首令牌时间(TTFT)。问题的根源在于这些系统缺乏对未来工作负载特性的认知，而实际工作负载具有高度周期性和长期可预测性。

Method: 提出通用GPU工作器实现一对多GPU预热，基于此设计WarmServe系统：1)采用驱逐感知模型放置策略减轻集群级预热干扰；2)通过主动预热提前准备通用GPU工作器；3)使用零开销内存切换机制管理GPU内存。

Result: 在真实数据集评估中，WarmServe相比最先进的自动扩缩系统将TTFT提升高达50.8倍，同时相比GPU共享系统能够服务多达2.5倍的请求。

Conclusion: 通过利用工作负载的可预测性并设计通用GPU工作器，WarmServe成功解决了多LLM服务中GPU利用率与推理性能之间的权衡问题，实现了高效且高性能的服务。

Abstract: Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.
  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\times$ more requests compared to the GPU-sharing system.

</details>


### [6] [Scalable Construction of Spiking Neural Networks using up to thousands of GPUs](https://arxiv.org/abs/2512.09502)
*Bruno Golosio,Gianmarco Tiddia,José Villamar,Luca Pontisso,Luca Sergi,Francesco Simula,Pooja Babu,Elena Pastorelli,Abigail Morrison,Markus Diesmann,Alessandro Lonardo,Pier Stanislao Paolucci,Johanna Senk*

Main category: cs.DC

TL;DR: 提出一种用于多GPU集群和百亿亿次超级计算机的MPI网络构建方法，用于大规模脉冲神经网络模拟


<details>
  <summary>Details</summary>
Motivation: 大规模脉冲神经网络模拟在计算神经科学研究中很重要，但需要高效管理通信和内存，特别是在多GPU集群和百亿亿次超级计算机上

Method: 使用MPI的消息传递接口，每个进程构建本地连接性并准备数据结构，以在状态传播期间实现高效的集群间脉冲交换

Result: 展示了两种皮层模型使用点对点和集体通信的扩展性能

Conclusion: 该方法为大规模脉冲神经网络模拟提供了高效的网络构建和通信解决方案，适用于高性能计算环境

Abstract: Diverse scientific and engineering research areas deal with discrete, time-stamped changes in large systems of interacting delay differential equations. Simulating such complex systems at scale on high-performance computing clusters demands efficient management of communication and memory. Inspired by the human cerebral cortex -- a sparsely connected network of $\mathcal{O}(10^{10})$ neurons, each forming $\mathcal{O}(10^{3})$--$\mathcal{O}(10^{4})$ synapses and communicating via short electrical pulses called spikes -- we study the simulation of large-scale spiking neural networks for computational neuroscience research. This work presents a novel network construction method for multi-GPU clusters and upcoming exascale supercomputers using the Message Passing Interface (MPI), where each process builds its local connectivity and prepares the data structures for efficient spike exchange across the cluster during state propagation. We demonstrate scaling performance of two cortical models using point-to-point and collective communication, respectively.

</details>


### [7] [PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing](https://arxiv.org/abs/2512.09568)
*Zhi Zhao,Hang Xiao,Wei Rang*

Main category: cs.DC

TL;DR: PHWSOA算法结合鲸鱼优化和海鸥优化，通过帕累托优化同时减少任务完成时间、改善虚拟机负载均衡并降低成本，在云任务调度中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有云任务调度方案大多只优化单一或有限指标（如执行时间或资源利用率），缺乏全面的多目标优化方法，需要同时考虑任务完成时间、虚拟机负载均衡和经济成本等多个关键目标。

Method: 提出帕累托混合鲸鱼-海鸥优化算法(PHWSOA)，结合鲸鱼优化算法的全局探索能力和海鸥优化算法的局部开发能力，采用Halton序列初始化提高种群多样性，帕累托引导变异防止早熟收敛，并行处理加速收敛，并集成动态虚拟机负载重分配机制。

Result: 在CloudSim模拟器上使用NASA-iPSC和HPC2N真实工作负载进行实验，PHWSOA相比基线方法显著提升：任务完成时间减少72.1%，虚拟机负载均衡改善36.8%，成本节省23.5%，优于WOA、GA、PEWOA和GCWOA等算法。

Conclusion: PHWSOA通过有效的多目标优化策略，在云任务调度中实现了综合性能提升，展示了在实际云环境中实现高效资源管理的强大潜力。

Abstract: Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.

</details>


### [8] [SynthPix: A lightspeed PIV images generator](https://arxiv.org/abs/2512.09664)
*Antonio Terpin,Alan Bonomi,Francesco Banelli,Raffaello D'Andrea*

Main category: cs.DC

TL;DR: SynthPix是基于JAX实现的高性能并行PIV合成图像生成器，相比现有工具提升数个数量级的图像对生成速度


<details>
  <summary>Details</summary>
Motivation: 为数据饥渴的强化学习方法提供训练数据，并减少实时PIV反馈控制研究中快速流场估计算法的开发迭代时间

Method: 使用JAX框架实现，支持与现有工具相同的配置参数，但通过加速器并行化实现高性能图像生成

Result: 实现了数个数量级的吞吐量提升（每秒图像对生成速度），为流体动力学社区提供实用工具

Conclusion: SynthPix是一个高性能的PIV合成图像生成器，能够显著加速流场估计方法的开发和训练，对流体动力学研究具有重要价值

Abstract: We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.

</details>


### [9] [Straggler Tolerant and Resilient DL Training on Homogeneous GPUs](https://arxiv.org/abs/2512.09685)
*Zeyu Zhang,Haiying Shen*

Main category: cs.DC

TL;DR: STAR系统通过新的同步模式和资源管理策略，有效解决GPU深度学习训练中的straggler问题，相比现有方法显著降低训练时间。


<details>
  <summary>Details</summary>
Motivation: 尽管GPU深度学习训练很流行，但straggler问题的普遍性、原因和影响，以及现有缓解方法的有效性仍不清楚。研究发现straggler广泛存在，且现有方法（如同步到异步SGD切换）可能无法改善训练时间甚至产生更多straggler。

Method: 提出STAR系统，包含：1）新的同步模式，将worker分组进行参数更新；2）启发式和机器学习方法选择最优同步模式以最小化训练时间；3）资源重分配支持所选模式，同时最小化对共存作业的影响；4）主动预防straggler，避免CPU和带宽过载。

Result: 在AWS上的trace驱动评估显示，STAR在PS架构中降低48-84%的训练时间，在all-reduce架构中降低51-70%的训练时间，同时保持同步SGD的收敛精度。

Conclusion: STAR系统有效解决了GPU深度学习训练中的straggler问题，显著提高了训练效率，同时开源了代码。

Abstract: Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.

</details>


### [10] [Recoverable Lock-Free Locks](https://arxiv.org/abs/2512.09710)
*Hagit Attiya,Panagiota Fatourou,Eleftherios Kosmas,Yuanhao Wei*

Main category: cs.DC

TL;DR: 提出首个同时实现无锁和可恢复性的转换方法，将基于锁的实现转换为可恢复的无锁实现


<details>
  <summary>Details</summary>
Motivation: 现有系统要么关注无锁性，要么关注可恢复性，缺乏同时具备这两种特性的解决方案。需要一种方法能将现有的基于锁的实现转换为既无锁又可恢复的系统。

Method: 从基于锁的实现开始，为锁获取和锁释放操作提供可恢复的无锁替代方案。支持嵌套锁以增强通用性，确保可恢复性而不损害原始基于锁实现的正确性。

Result: 首次实现了同时具备无锁和可恢复性的转换方法，能够将锁基系统转换为可恢复的无锁系统，支持嵌套锁并保持原始实现的正确性。

Conclusion: 该转换方法填补了无锁性和可恢复性之间的空白，为构建既高效又可靠的无锁可恢复系统提供了实用工具。

Abstract: This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [RACAM: Enhancing DRAM with Reuse-Aware Computation and Automated Mapping for ML Inference](https://arxiv.org/abs/2512.09304)
*Siyuan Ma,Jiajun Hu,Jeeho Ryoo,Aman Arora,Lizy Kurian John*

Main category: cs.AR

TL;DR: RACAM是一种新型的DRAM内存内处理架构，通过专用局部性缓冲区、位串行处理单元、popcount归约单元和广播单元，解决了现有DRAM-PIM架构中数据重用不足、冗余数据传输和负载映射支持不够的问题，在LLM推理中相比GPU和现有最佳方案实现了显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有DRAM-PIM架构存在三个主要问题：缺乏数据重用、大量冗余数据传输、以及对负载映射支持不足。这些问题限制了DRAM-PIM在内存密集型工作负载（如大语言模型推理）中的效率提升潜力。

Method: 提出RACAM架构，包含：1) 专用局部性缓冲区实现数据重用；2) 位串行处理单元支持运行时可变数据精度；3) popcount归约单元和广播单元减少冗余数据传输；4) 负载映射机制充分利用DRAM架构的大规模并行性。

Result: 在端到端LLM推理评估中，RACAM相比GPU实现了9-102倍加速，相比现有最佳DRAM-PIM系统Proteus，在GPT3推理中实现了每平方毫米233倍的性能提升。

Conclusion: RACAM通过创新的架构设计和负载映射机制，有效解决了DRAM-PIM架构的关键瓶颈，为内存密集型工作负载（特别是大语言模型推理）提供了高效加速方案。

Abstract: In-DRAM Processing-In-Memory (DRAM-PIM) has emerged as a promising approach to accelerate memory-intensive workloads by mitigating data transfer overhead between DRAM and the host processor. Bit-serial DRAM-PIM architectures, further enhance efficiency by supporting runtime variable data precision, which is critical for emerging workloads, such as large language model (LLM) inference. However, existing works still have major limitations: lack of data reuse, significant amounts of redundant data transfer, and insufficient support for workload mapping. To address these issues, we propose RACAM, the first in-DRAM bit-serial architecture which uses dedicated locality buffers, bit-serial PEs, popcount reduction units and broadcast units to enable data reuse and alleviate redundant data transfers. Furthermore, a workload mapping mechanism is proposed to fully explore the massive parallelism of DRAM architecture and identify the best mapping scheme of a given workload. We evaluate RACAM against GPUs and the state-of-the-art, in-DRAM PIM system, Proteus, across end-to-end LLM inferences. RACAM achieves 9x to 102x speedup over GPUs and 233x higher performance per mm2 compared to Proteus in case of GPT3.

</details>


### [12] [ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators](https://arxiv.org/abs/2512.09427)
*Guoqiang Zou,Wanyu Wang,Hao Zheng,Longxiang Yin,Yinhe Han*

Main category: cs.AR

TL;DR: ODMA是一个面向随机访问受限内存（RACM）加速器的按需内存分配框架，通过轻量级长度预测器、动态桶分区和大桶保护机制，显著提升LLM服务的内存利用率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有内存管理方案在随机访问带宽受限的加速器（如基于LPDDR5的Cambricon MLU370）上效率低下：静态预分配浪费内存，而细粒度分页（如PagedAttention）因高随机访问成本而不适用。现有HBM中心化解决方案未能充分利用RACM加速器的特性。

Method: ODMA框架结合轻量级长度预测器和动态桶分区策略，通过大桶保护机制处理分布漂移和长尾请求。边界根据实时跟踪数据定期更新以最大化内存利用率。该方案专门针对RACM加速器的硬件特性设计。

Result: 在Alpaca和Google-NQ数据集上，ODMA将预测准确率从82.68%提升至93.36%。在Cambricon MLU370-X4上服务DeepSeek-R1-Distill-Qwen-7B模型，内存利用率从55.05%提升至72.45%，RPS和TPS分别比静态基线提高29%和27%。

Conclusion: 硬件感知的内存分配方案能够解锁RACM平台上高效的LLM服务，ODMA框架通过针对性的设计解决了现有方案在随机访问受限加速器上的局限性。

Abstract: Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.

</details>
