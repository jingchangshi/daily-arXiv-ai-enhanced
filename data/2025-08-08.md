<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Consistent Updates for Scalable Microservices](https://arxiv.org/abs/2508.04829)
*Devora Chait-Roth,Kedar S. Namjoshi,Thomas Wies*

Main category: cs.PL

TL;DR: 本文提出了首个保证混合模式更新一致性的算法，利用服务行为的语义属性（如交换性）避免不一致性。


<details>
  <summary>Details</summary>
Motivation: 在线服务通常采用可扩展的微服务架构，但动态修改功能时，新旧版本工作进程的混合操作可能导致不一致性。现有方法要么效率低下，要么无法避免不一致性。

Method: 通过分析服务行为的语义属性（如交换性），提出了一种框架和理论，用于推导新的算法并证明其正确性。

Result: 证明了语义感知是避免不一致性的必要条件，并提出了首个保证一致性的混合模式更新算法。

Conclusion: 语义感知的算法能够有效避免混合模式更新的不一致性，为在线服务的动态更新提供了理论基础和实用方法。

Abstract: Online services are commonly implemented with a scalable microservice
architecture, where isomorphic worker processes service client requests,
recording persistent state in a backend data store. To maintain service, any
modifications to the service functionality must be made on the fly -- i.e., as
the service continues to process client requests -- but doing so is
challenging. The central difficulty is that of avoiding potential
inconsistencies caused by ''mixed mode'' operation, where workers of current
and new versions are concurrently active and interact via the data store. Some
update methods avoid mixed mode altogether, but only at the cost of substantial
inefficiency -- by doubling resources (memory and compute), or by halving
throughput. The alternative is a so-called ''rolling'' update, which is
uncontrolled and runs the risk of serious service failures arising from
inconsistent mixed-mode behavior.
  In this paper, we present the first algorithms that guarantee consistency for
mixed mode updates. The algorithms rely on semantic properties of service
actions, such as commutativity. We show that semantic awareness is required, by
proving that any semantically oblivious, mixed-mode update method cannot avoid
inconsistencies. Ideally, it should appear to every client that a service
update takes effect atomically; this ensures that a client is not exposed to
inconsistent mixed-mode behavior. We introduce a framework that formalizes this
intuition and develop foundational theory for reasoning about the consistency
of mixed-mode updates, applying that theory to derive the new algorithms and
establish their correctness.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [OPTIMUMP2P: Fast and Reliable Gossiping in P2P Networks](https://arxiv.org/abs/2508.04833)
*Nicolas Nicolaou,Onyeka Obi,Aayush Rajasekaran,Alejandro Bergasov,Aleksandr Bezobchuk,Kishori M. Konwar,Michael Meier,Santiago Paiva,Har Preet Singh,Swarnabha Sinha*

Main category: cs.DC

TL;DR: OPTIMUMP2P是一种新型的gossip算法，利用RLNC技术提升P2P网络中信息传播的性能和可靠性，优于现有的Gossipsub协议。


<details>
  <summary>Details</summary>
Motivation: 提升libp2p的性能和可靠性，特别是在存在恶意行为者时确保信息可靠传输。

Method: 引入基于RLNC的OPTIMUMP2P算法，优化信息传播速度和可靠性。

Result: 在仿真和实际环境中验证了OPTIMUMP2P优于Gossipsub协议的性能提升。

Conclusion: OPTIMUMP2P通过RLNC技术显著提升了P2P网络中的信息传播效率和可靠性。

Abstract: Gossip algorithms are pivotal in the dissemination of information within
decentralized systems. Consequently, numerous gossip libraries have been
developed and widely utilized especially in blockchain protocols for the
propagation of blocks and transactions. A well-established library is libp2p,
which provides two gossip algorithms: floodsup and gossibsup. These algorithms
enable the delivery of published messages to a set of peers. In this work we
aim to enhance the performance and reliability of libp2p by introducing
OPTIMUMP2P, a novel gossip algorithm that leverages the capabilities of Random
Linear Network Coding (RLNC) to expedite the dissemination of information in a
peer-to-peer (P2P) network while ensuring reliable delivery, even in the
presence of malicious actors capable of corrupting the transmitted data.
Preliminary research from the Ethereum Foundation has demonstrated the use of
RLNC in the significant improvement in the block propagation time [14]. Here we
present extensive evaluation results both in simulation and real-world
environments that demonstrate the performance gains of OPTIMUMP2P over the
Gossipsub protocol.

</details>


### [3] [Linear Search for Capturing an Oblivious Mobile Target in the Sender/Receiver Model](https://arxiv.org/abs/2508.04870)
*Khaled Jawhar,Evangelos Kranakis*

Main category: cs.DC

TL;DR: 论文研究了两个具有不同通信能力的自主机器人如何通过线性搜索捕获一个移动目标，分析了不同通信模型对竞争比的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨不对称通信能力（Sender/Receiver模型）如何影响线性搜索的效率，特别是在目标移动速度、方向和起始距离已知的情况下。

Method: 设计了新的线性搜索算法，考虑了目标移动的两种模型（away和toward），并分析了不同环境信息（如起始距离、速度）对算法的影响。

Result: 通过算法设计和分析，得出了在不同场景下捕获目标所需时间的竞争比。

Conclusion: 研究表明，不对称通信能力对线性搜索的竞争比有显著影响，为相关领域提供了新的理解和算法支持。

Abstract: We consider linear search for capturing an oblivious moving target by two
autonomous robots with different communicating abilities. Both robots can
communicate Face-to-Face (F2F) when co-located but in addition one robot is a
Sender (can also send messages wirelessly) and the other also a Receiver (can
also receive messages wirelessly). This is known as Sender/Receiver (S/R, for
short) communication model. The robots can move with max speed $1$. The moving
target starts at distance $d$ from the origin and can move either with speed
$v<1$ away from the origin in the ``away'' model or with speed $v \geq 0$
toward the origin in the ``toward'' model. We assume that the direction of
motion of the target (i.e., whether it is the away or toward model) is known to
the robots in advance. To capture the target the two robots must be co-located
with it.
  We design new linear search algorithms and analyze the competitive ratio of
the time required to capture the target. The approach takes into account
various scenarios related to what the robots know about the search environment
(e.g., starting distance or speed of the mobile, away or toward model, or a
combination thereof). Our study contributes to understanding how asymmetric
communication affects the competitive ratio of linear search.

</details>


### [4] [Managing, Analyzing and Sharing Research Data with Gen3 Data Commons](https://arxiv.org/abs/2508.04944)
*Craig Barnes,Kyle Burton,Michael S. Fitzsimons,Hara Prasad Juvvala,Brienna Larrick,Christopher Meyer,Pauline Ribeyre,Ao Liu,Clint Malson,Noah Metoki-Shlubsky,Andrii Prokhorenkov,Jawad Qureshi,Radhika Reddy,L. Philip Schumm,Mingfei Shao,Trevar Simmons,Alexander VanTol,Peter Vassilatos,Aarti Venkat,Robert L. Grossman*

Main category: cs.DC

TL;DR: Gen3是一个开源数据平台，用于构建数据共享空间，支持管理、分析和共享研究数据。


<details>
  <summary>Details</summary>
Motivation: 为研究社区提供一个云数据平台，促进数据管理和共享。

Method: 通过定义数据模型自动生成数据门户和FAIR API。

Result: 已支持构建多个数据共享空间，管理超过28 PB数据和6400万FAIR数据对象。

Conclusion: Gen3基于标准化服务设计，支持与其他数据平台和生态系统的互操作性。

Abstract: Gen3 is an open-source data platform for building data commons. A data
commons is a cloud-based data platform for managing, analyzing, and sharing
data with a research community. Gen3 has been used to build over a dozen data
commons that in aggregate contain over 28 PB of data and 64 million FAIR data
objects. To set up a Gen3 data commons, you first define a data model. Gen3
then autogenerates 1) a data portal for searching and exploring data in the
commons; 2) a data portal for submitting data to the commons; and 3) FAIR APIs
for accessing the data programmatically. Gen3 is built over a small number of
standards-based software services, which are designed to support current and
future Gen3 components so that Gen3 can interoperate with other data platforms
and data ecosystems.

</details>


### [5] [Tesserae: Scalable Placement Policies for Deep Learning Workloads](https://arxiv.org/abs/2508.04953)
*Song Bian,Saurabh Agarwal,Md. Tareq Mahmood,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 本文提出了一种基于图匹配的GPU集群调度器Tesserae，通过优化任务放置策略，显著提升了资源利用率和调度效率。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型训练已成为数据中心的主要负载，现有调度器的任务放置策略要么性能不佳，要么扩展性差，亟需改进。

Method: 将任务放置约束建模为图匹配问题，设计新的放置策略以减少任务迁移开销并优化任务打包。

Result: 实验表明，Tesserae将平均作业完成时间（JCT）提升至1.62倍，总完成时间（Makespan）提升至1.15倍。

Conclusion: Tesserae通过图匹配方法实现了高效且可扩展的GPU集群调度。

Abstract: Training deep learning (DL) models has become a dominant workload in
data-centers and improving resource utilization is a key goal of DL cluster
schedulers. In order to do this, schedulers typically incorporate placement
policies that govern where jobs are placed on the cluster. Existing placement
policies are either designed as ad-hoc heuristics or incorporated as
constraints within a complex optimization problem and thus either suffer from
suboptimal performance or poor scalability. Our key insight is that many
placement constraints can be formulated as graph matching problems and based on
that we design novel placement policies for minimizing job migration overheads
and job packing. We integrate these policies into Tesserae and describe how our
design leads to a scalable and effective GPU cluster scheduler. Our
experimental results show that Tesserae improves average JCT by up to 1.62x and
the Makespan by up to 1.15x compared with the existing schedulers.

</details>


### [6] [Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations](https://arxiv.org/abs/2508.05020)
*Anjiang Wei,Hang Song,Mert Hidayetoglu,Elliott Slaughter,Sanjiva K. Lele,Alex Aiken*

Main category: cs.DC

TL;DR: 开发了一种基于自适应网格细化（AMR）的高阶求解器，解决了在Regent中实现AMR的挑战，包括动态数据结构、网格有效性强制和任务融合。实验显示任务融合带来18倍加速，GPU内核生成带来9.7倍加速。


<details>
  <summary>Details</summary>
Motivation: 为科学应用中的可压缩流提供高效的高阶求解器，同时通过AMR降低计算成本。

Method: 使用Regent编程语言开发AMR求解器，解决动态数据结构、网格有效性强制和任务融合等挑战。

Result: 任务融合实现18倍加速，GPU内核生成实现9.7倍加速。

Conclusion: 通过两个欧拉方程控制的可压缩流问题验证了方法的有效性。

Abstract: High-order solvers for compressible flows are vital in scientific
applications. Adaptive mesh refinement (AMR) is a key technique for reducing
computational cost by concentrating resolution in regions of interest. In this
work, we develop an AMR-based numerical solver using Regent, a high-level
programming language for the Legion programming model. We address several
challenges associated with implementing AMR in Regent. These include dynamic
data structures for patch refinement/coarsening, mesh validity enforcement, and
reducing task launch overhead via task fusion. Experimental results show that
task fusion achieves 18x speedup, while automated GPU kernel generation via
simple annotations yields 9.7x speedup for the targeted kernel. We demonstrate
our approach through simulations of two canonical compressible flow problems
governed by the Euler equations.

</details>


### [7] [Theseus: A Distributed and Scalable GPU-Accelerated Query Processing Platform Optimized for Efficient Data Movement](https://arxiv.org/abs/2508.05029)
*Felipe Aramburú,William Malpica,Kaouther Abrougui,Amin Aramoon,Romulo Auccapuclla,Claude Brisson,Matthijs Brobbel,Colby Farrell,Pradeep Garigipati,Joost Hoozemans,Supun Kamburugamuve,Akhil Nair,Alexander Ocsa,Johan Peltenburg,Rubén Quesada López,Deepak Sihag,Ahmet Uyar,Dhruv Vats,Michael Wendt,Jignesh M. Patel,Rodrigo Aramburú*

Main category: cs.DC

TL;DR: Theseus是一个分布式加速器原生查询引擎，旨在优化数据移动、内存利用和计算，显著提升大规模数据分析性能。


<details>
  <summary>Details</summary>
Motivation: 降低大规模数据分析的成本并提高吞吐量，利用GPU等加速器优化查询处理。

Method: 采用异步控制机制、固定大小页锁定主机内存分配等技术，平衡数据移动、内存和计算。

Result: 在TPC-H基准测试中，Theseus性能优于Databricks Photon达4倍，且能以2个DGX A100节点处理100TB规模数据。

Conclusion: Theseus展示了在分布式加速器环境中高效处理大规模数据分析的潜力。

Abstract: Online analytical processing of queries on datasets in the many-terabyte
range is only possible with costly distributed computing systems. To decrease
the cost and increase the throughput, systems can leverage accelerators such as
GPUs, which are now ubiquitous in the compute infrastructure. This introduces
many challenges, the majority of which are related to when, where, and how to
best move data around the system. We present Theseus -- a production-ready
enterprise-scale distributed accelerator-native query engine designed to
balance data movement, memory utilization, and computation in an
accelerator-based system context. Specialized asynchronous control mechanisms
are tightly coupled to the hardware resources for the purpose of network
communication, data pre-loading, data spilling across memories and storage, and
GPU compute tasks. The memory subsystem contains a mechanism for fixed-size
page-locked host memory allocations to increase throughput and reduce memory
fragmentation. For the TPC-H benchmarks at scale factors ranging from 1k to 30k
on cloud infrastructure, Theseus outperforms Databricks Photon by up to
$4\times$ at cost parity. Theseus is capable of processing all queries of the
TPC-H and TPC-DS benchmarks at scale factor 100k (100 TB scale) with as few as
2 DGX A100 640GB nodes.

</details>


### [8] [Simulating LLM training workloads for heterogeneous compute and network infrastructure](https://arxiv.org/abs/2508.05370)
*Sumit Kumar,Arjun Temura,Naman Sharma,Ramanjeet Singh,Meet Dadhania,Praveen Tammana,Satananda Burla,Abed Mohammad Kamaluddin,Rinku Shah*

Main category: cs.DC

TL;DR: 论文提出了一种异构感知的分布式LLM模拟器，用于预测训练时间并支持自定义设备配置，解决了现有模拟器假设同质基础设施的局限性。


<details>
  <summary>Details</summary>
Motivation: 大规模GPU集群在分布式模型训练中的需求增长，但现有LLM训练模拟器假设基础设施同质，无法应对实践中设备异构性带来的挑战。

Method: 设计了异构感知的分布式LLM模拟器，支持自定义设备组配置和非均匀工作负载分区，以更准确地预测训练时间。

Result: 初步模拟结果显示异构性对模型计算和通信时间有显著影响。

Conclusion: 异构感知模拟器填补了现有技术与实际需求之间的差距，为分布式训练优化提供了新工具。

Abstract: The growing demand for large-scale GPU clusters in distributed model training
presents a significant barrier to innovation, particularly in model
optimization, performance tuning, and system-level enhancements. To address
this challenge, LLM training simulators are employed to estimate training time
and guide design decisions. However, the state-of-the-art LLM training
simulators assume homogeneous compute and network infrastructure. In practice,
device heterogeneity is inevitable due to resource sharing in cloud
environments, frequent shifts in device generations, and inherent intra-chip
interconnect heterogeneity. To address the gap between state-of-the-art and
practical requirements, we propose the design of a heterogeneity-aware
distributed LLM simulator capable of predicting training time while enabling
abstractions to specify custom configurations for device groups and
device-to-parallelism mapping. We present the design requirements and
challenges in building a heterogeneity-aware distributed ML training simulator,
and design components such as non-uniform workload partitioning. Our initial
simulation results demonstrate the impact of heterogeneity on the model
computation and communication time.

</details>


### [9] [Adaptive Parallel Downloader for Large Genomic Datasets](https://arxiv.org/abs/2508.05511)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: FastBioDL是一种自适应并行文件下载工具，用于大型生物数据集，通过实时调整并发流数量，显著提升下载速度。


<details>
  <summary>Details</summary>
Motivation: 现有下载工具因静态并发设置无法适应动态网络条件，导致带宽利用低效和下载时间长。

Method: FastBioDL将下载过程建模为在线优化问题，使用效用函数和梯度下降动态调整并发流数量。

Result: 在公共基因组数据集上，FastBioDL比现有工具快4倍，高速网络下快2.1倍。

Conclusion: FastBioDL为大规模基因组数据获取提供了高效解决方案，无需专业商业软件。

Abstract: Modern next-generation sequencing (NGS) projects routinely generate terabytes
of data, which researchers commonly download from public repositories such as
SRA or ENA. Existing download tools often employ static concurrency settings,
leading to inefficient bandwidth utilization and prolonged download times due
to their inability to adapt to dynamic network conditions. We introduce
FastBioDL, a parallel file downloader designed for large biological datasets,
featuring an adaptive concurrency controller. FastBioDL frames the download
process as an online optimization problem, utilizing a utility function and
gradient descent to adjust the number of concurrent socket streams in real-time
dynamically. This approach maximizes download throughput while minimizing
resource overhead. Comprehensive evaluations on public genomic datasets
demonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art
tools. Moreover, in high-speed network experiments, its adaptive design was up
to $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP
or FTP downloads on the client side, FastBioDL provides a robust and efficient
solution for large-scale genomic data acquisition, democratizing
high-performance data retrieval for researchers without requiring specialized
commercial software or protocols.

</details>


### [10] [Modular Architecture for High-Performance and Low Overhead Data Transfers](https://arxiv.org/abs/2508.05546)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: AutoMDT是一种基于深度强化学习的模块化数据传输架构，通过优化并发级别显著提升数据传输性能。


<details>
  <summary>Details</summary>
Motivation: 高性能应用需要快速可靠地传输大规模数据，但传统工具因固定配置或单一优化方法导致资源利用不足和不稳定。

Method: 采用深度强化学习（PPO）代理优化并发级别，结合轻量级网络系统模拟器进行离线训练。

Result: 在测试中，AutoMDT比现有方案快8倍收敛，传输完成时间减少68%。

Conclusion: AutoMDT通过模块化设计和离线训练，高效适应动态条件，显著提升数据传输性能。

Abstract: High-performance applications necessitate rapid and dependable transfer of
massive datasets across geographically dispersed locations. Traditional file
transfer tools often suffer from resource underutilization and instability
because of fixed configurations or monolithic optimization methods. We propose
AutoMDT, a novel modular data transfer architecture that employs a deep
reinforcement learning based agent to simultaneously optimize concurrency
levels for read, network, and write operations. Our solution incorporates a
lightweight network-system simulator, enabling offline training of a Proximal
Policy Optimization (PPO) agent in approximately 45 minutes on average, thereby
overcoming the impracticality of lengthy online training in production
networks. AutoMDT's modular design decouples I/O and network tasks, allowing
the agent to capture complex buffer dynamics precisely and to adapt quickly to
changing system and network conditions. Evaluations on production-grade
testbeds show that AutoMDT achieves up to 8x faster convergence and a 68%
reduction in transfer completion times compared with state-of-the-art
solutions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [Understanding and Mitigating Errors of LLM-Generated RTL Code](https://arxiv.org/abs/2508.05266)
*Jiazheng Zhang,Cheng Liu,Huawei Li*

Main category: cs.AR

TL;DR: 论文通过分析LLM在RTL代码生成中的错误原因，提出针对性修正技术，显著提升了生成准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在RTL代码生成中潜力巨大，但成功率仍不理想，缺乏对具体失败原因的理解阻碍了改进。

Method: 通过错误分析和分类，提出基于上下文学习的修正技术，包括构建领域知识库、引入设计规则和迭代调试。

Result: 增强后的框架在VerilogEval基准测试中达到91.0%准确率，比基线方法提升32.7%。

Conclusion: 针对性修正技术有效解决了LLM在RTL代码生成中的主要错误，显著提升了性能。

Abstract: Despite the promising potential of large language model (LLM) based
register-transfer-level (RTL) code generation, the overall success rate remains
unsatisfactory. Errors arise from various factors, with limited understanding
of specific failure causes hindering improvement. To address this, we conduct a
comprehensive error analysis and manual categorization. Our findings reveal
that most errors stem not from LLM reasoning limitations, but from insufficient
RTL programming knowledge, poor understanding of circuit concepts, ambiguous
design descriptions, or misinterpretation of complex multimodal inputs.
Leveraging in-context learning, we propose targeted error correction
techniques. Specifically, we construct a domain-specific knowledge base and
employ retrieval-augmented generation (RAG) to supply necessary RTL knowledge.
To mitigate ambiguity errors, we introduce design description rules and
implement a rule-checking mechanism. For multimodal misinterpretation, we
integrate external tools to convert inputs into LLM-compatible meta-formats.
For remaining errors, we adopt an iterative debugging loop (simulation-error
localization-correction). Integrating these techniques into an LLM-based
framework significantly improves performance. We incorporate these error
correction techniques into a foundational LLM-based RTL code generation
framework, resulting in significantly improved performance. Experimental
results show that our enhanced framework achieves 91.0\% accuracy on the
VerilogEval benchmark, surpassing the baseline code generation approach by
32.7\%, demonstrating the effectiveness of our methods.

</details>


### [12] [relOBI: A Reliable Low-latency Interconnect for Tightly-Coupled On-chip Communication](https://arxiv.org/abs/2508.05354)
*Michael Rogenmoser,Angelo Garofalo,Luca Benini*

Main category: cs.AR

TL;DR: 提出了一种名为relOBI的扩展方案，结合三模冗余（TMR）和纠错码（ECC），显著提升了SoC互连的可靠性，将故障率从34.85%降至0%。


<details>
  <summary>Details</summary>
Motivation: 在辐射密集环境中，SoC互连的软错误可能导致整个系统失效，因此需要高可靠性解决方案。

Method: 扩展Open Bus Interface（OBI），结合TMR和ECC技术，对关键握手信号和其他信号分别进行保护。

Result: 测试显示，完全可靠的交叉开关设计将故障率降至0%，面积增加2.6倍，时序影响1.4倍，面积开销比文献中的细粒度三重化方案低1.8倍。

Conclusion: relOBI方案在保证高可靠性的同时，优化了面积开销，适用于辐射密集环境中的SoC设计。

Abstract: On-chip communication is a critical element of modern systems-on-chip (SoCs),
allowing processor cores to interact with memory and peripherals. Interconnects
require special care in radiation-heavy environments, as any soft error within
the SoC interconnect is likely to cause a functional failure of the whole SoC.
This work proposes relOBI, an extension to Open Bus Interface (OBI) combining
triple modular redundancy (TMR) for critical handshake signals with error
correction codes (ECC) protection on other signals for complete reliability.
Implementing and testing a fully reliable crossbar shows improved reliability
to injected faults from a vulnerability of 34.85 % to 0 % compared to a
reference design, with an area increase of 2.6x and 1.4x timing impact. The
area overhead is 1.8x lower than that reported in the literature for
fine-grained triplication and voting.

</details>
