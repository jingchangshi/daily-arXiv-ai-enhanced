<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Marionette: Data Structure Description and Management for Heterogeneous Computing](https://arxiv.org/abs/2511.04853)
*Nuno dos Santos Fernandes,Pedro Tomás,Nuno Roma,Frank Winklmeier,Patricia Conde-Muíño*

Main category: cs.DC

TL;DR: Marionette是一个C++17库，通过解耦数据布局与接口描述、支持多种内存管理策略和高效数据传输，帮助大型面向对象C++代码库适应硬件加速，特别是针对GPU等异构平台。


<details>
  <summary>Details</summary>
Motivation: 大型面向对象C++代码库在面向GPU等异构平台进行硬件加速时面临极大挑战，需要解决数据布局、内存管理和跨设备数据传输等问题。

Method: Marionette采用编译时抽象，解耦数据布局与接口描述，支持多种内存管理策略，提供高效的数据传输和转换机制，并允许接口通过任意函数进行扩展。

Result: 通过CUDA案例研究证明了Marionette的高效性和灵活性，能够在保持与现有代码兼容的同时，提供最小运行时开销。

Conclusion: Marionette为大型C++代码库的硬件加速提供了一种灵活、高效且可移植的解决方案，特别适用于异构计算平台。

Abstract: Adapting large, object-oriented C++ codebases for hardware acceleration might
be extremely challenging, particularly when targeting heterogeneous platforms
such as GPUs. Marionette is a C++17 library designed to address this by
enabling flexible, efficient, and portable data structure definitions. It
decouples data layout from the description of the interface, supports multiple
memory management strategies, and provides efficient data transfers and
conversions across devices, all of this with minimal runtime overhead due to
the compile-time nature of its abstractions. By allowing interfaces to be
augmented with arbitrary functions, Marionette maintains compatibility with
existing code and offers a streamlined interface that supports both
straightforward and advanced use cases. This paper outlines its design, usage,
and performance, including a CUDA-based case study demonstrating its efficiency
and flexibility.

</details>


### [2] [Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs](https://arxiv.org/abs/2511.05053)
*Wakuto Matsumi,Riaz-Ul-Haque Mian*

Main category: cs.DC

TL;DR: 本文提出了一种基于RISC-V GPU的混合HDC-CNN加速器设计，通过定制GPU指令优化超维计算操作，实现了高达56.2倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 神经网络的高能耗问题严重，而超维计算(HDC)虽然轻量但精度较低。现有混合加速器存在泛化性和可编程性不足的问题，RISC-V架构为定制GPU设计提供了新机会。

Method: 设计并实现了针对HDC操作优化的定制GPU指令，支持混合HDC-CNN工作负载的高效处理，使用了四种类型的定制HDC指令。

Result: 微基准测试显示性能提升最高达56.2倍，证明了RISC-V GPU在高性能能效计算方面的潜力。

Conclusion: 基于RISC-V的GPU平台能够有效支持定制计算模型如HDC，为能效高性能计算提供了可行解决方案。

Abstract: Machine learning based on neural networks has advanced rapidly, but the high
energy consumption required for training and inference remains a major
challenge. Hyperdimensional Computing (HDC) offers a lightweight,
brain-inspired alternative that enables high parallelism but often suffers from
lower accuracy on complex visual tasks. To overcome this, hybrid accelerators
combining HDC and Convolutional Neural Networks (CNNs) have been proposed,
though their adoption is limited by poor generalizability and programmability.
The rise of open-source RISC-V architectures has created new opportunities for
domain-specific GPU design. Unlike traditional proprietary GPUs, emerging
RISC-V-based GPUs provide flexible, programmable platforms suitable for custom
computation models such as HDC. In this study, we design and implement custom
GPU instructions optimized for HDC operations, enabling efficient processing
for hybrid HDC-CNN workloads. Experimental results using four types of custom
HDC instructions show a performance improvement of up to 56.2 times in
microbenchmark tests, demonstrating the potential of RISC-V GPUs for
energy-efficient, high-performance computing.

</details>


### [3] [GPU Under Pressure: Estimating Application's Stress via Telemetry and Performance Counters](https://arxiv.org/abs/2511.05067)
*Giuseppe Esposito,Juan-David Guerrero-Balaguera,Josie Esteban Rodriguez Condia,Matteo Sonza Reorda,Marco Barbiero,Rossella Fortuna*

Main category: cs.DC

TL;DR: 本研究结合在线遥测参数和硬件性能计数器来评估不同应用对GPU造成的压力，通过测量吞吐量、指令数量和停顿事件来预测并行工作负载的压力水平。


<details>
  <summary>Details</summary>
Motivation: 持续的工作负载会对GPU组件造成显著压力，引发可靠性问题，特别是老化效应导致的潜在故障会破坏中间计算并产生错误结果。

Method: 结合在线遥测参数和硬件性能计数器，重点关注测量吞吐量、已发出指令数量和停顿事件这三个性能指标。

Result: 实验结果表明，通过结合遥测数据和性能计数器可以估计并行工作负载对GPU造成的压力，这些计数器能够揭示目标工作负载在资源使用方面的效率。

Conclusion: 利用遥测数据和性能计数器可以有效评估GPU工作负载压力，为预测GPU可靠性（特别是老化效应）提供了重要方法。

Abstract: Graphics Processing Units (GPUs) are specialized accelerators in data centers
and high-performance computing (HPC) systems, enabling the fast execution of
compute-intensive applications, such as Convolutional Neural Networks (CNNs).
However, sustained workloads can impose significant stress on GPU components,
raising reliability concerns due to potential faults that corrupt the
intermediate application computations, leading to incorrect results. Estimating
the stress induced by an application is thus crucial to predict reliability
(with\,special\,emphasis\,on\,aging\,effects). In this work, we combine online
telemetry parameters and hardware performance counters to assess GPU stress
induced by different applications. The experimental results indicate the stress
induced by a parallel workload can be estimated by combining telemetry data and
Performance Counters that reveal the efficiency in the resource usage of the
target workload. For this purpose the selected performance counters focus on
measuring the i) throughput, ii) amount of issued instructions and iii) stall
events.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [4] [Efficient Deployment of CNN Models on Multiple In-Memory Computing Units](https://arxiv.org/abs/2511.04682)
*Eleni Bougioukou,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: 提出Load-Balance-Longest-Path (LBLP)算法，用于在内存计算(IMC)硬件上优化卷积神经网络(CNN)的任务分配，以提高处理速率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 内存计算(IMC)通过减少数据移动瓶颈和利用内存计算的固有并行性来加速深度学习，但在多处理单元IMC硬件上高效部署CNN需要先进的任务分配策略。

Method: 使用IMC仿真器(IMCE)和多处理单元(PUs)，开发LBLP算法动态分配CNN节点到可用PUs，最大化处理速率并最小化延迟。

Result: 实验结果表明，LBLP算法在多个CNN模型上相比其他调度策略具有更好的性能表现。

Conclusion: LBLP算法能有效优化IMC硬件上CNN模型的部署，实现高效资源利用和性能提升。

Abstract: In-Memory Computing (IMC) represents a paradigm shift in deep learning
acceleration by mitigating data movement bottlenecks and leveraging the
inherent parallelism of memory-based computations. The efficient deployment of
Convolutional Neural Networks (CNNs) on IMC-based hardware necessitates the use
of advanced task allocation strategies for achieving maximum computational
efficiency. In this work, we exploit an IMC Emulator (IMCE) with multiple
Processing Units (PUs) for investigating how the deployment of a CNN model in a
multi-processing system affects its performance, in terms of processing rate
and latency. For that purpose, we introduce the Load-Balance-Longest-Path
(LBLP) algorithm, that dynamically assigns all CNN nodes to the available IMCE
PUs, for maximizing the processing rate and minimizing latency due to efficient
resources utilization. We are benchmarking LBLP against other alternative
scheduling strategies for a number of CNN models and experimental results
demonstrate the effectiveness of the proposed algorithm.

</details>


### [5] [RAS: A Bit-Exact rANS Accelerator For High-Performance Neural Lossless Compression](https://arxiv.org/abs/2511.04684)
*Yuchao Qin,Anjunyi Fan,Bonan Yan*

Main category: cs.AR

TL;DR: RAS是一个硬件架构，将rANS算法集成到无损压缩流水线中，通过消除关键瓶颈实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 数据中心处理大量数据需要高效无损压缩，但基于概率模型的新方法计算速度慢，需要硬件加速。

Method: 集成rANS核心与概率生成器，使用BF16格式存储分布，采用两阶段rANS更新和字节级重归一化，预测引导解码路径，多通道组织。

Result: 在图像工作负载上，相比Python rANS基线实现121.2倍编码和70.9倍解码加速，解码器二分搜索步骤从7.00减少到3.15（约减少55%）。

Conclusion: RAS与神经概率模型结合时，比传统编解码器保持更高压缩比，优于CPU/GPU rANS实现，为快速神经无损压缩提供了实用方法。

Abstract: Data centers handle vast volumes of data that require efficient lossless
compression, yet emerging probabilistic models based methods are often
computationally slow. To address this, we introduce RAS, the Range Asymmetric
Numeral System Acceleration System, a hardware architecture that integrates the
rANS algorithm into a lossless compression pipeline and eliminates key
bottlenecks. RAS couples an rANS core with a probabilistic generator, storing
distributions in BF16 format and converting them once into a fixed-point domain
shared by a unified division/modulo datapath. A two-stage rANS update with
byte-level re-normalization reduces logic cost and memory traffic, while a
prediction-guided decoding path speculatively narrows the cumulative
distribution function (CDF) search window and safely falls back to maintain
bit-exactness. A multi-lane organization scales throughput and enables
fine-grained clock gating for efficient scheduling. On image workloads, our
RTL-simulated prototype achieves 121.2x encode and 70.9x decode speedups over a
Python rANS baseline, reducing average decoder binary-search steps from 7.00 to
3.15 (approximately 55% fewer). When paired with neural probability models, RAS
sustains higher compression ratios than classical codecs and outperforms
CPU/GPU rANS implementations, offering a practical approach to fast neural
lossless compression.

</details>


### [6] [Eliminating the Hidden Cost of Zone Management in ZNS SSDs](https://arxiv.org/abs/2511.04687)
*Teona Bagashvili,Tarikul Islam Papon,Subhadeep Sarkar,Manos Athanassoulis*

Main category: cs.AR

TL;DR: SilentZNS是一种新的ZNS SSD区域映射和管理方法，通过动态分配资源解决传统ZNS实现中的设备级写入放大、磨损增加和主机I/O干扰问题。


<details>
  <summary>Details</summary>
Motivation: 传统ZNS SSD实现存在设备级写入放大(DLWA)、增加磨损以及与主机I/O干扰的问题，主要原因是固定物理区域和全区域操作导致过多物理写入。

Method: 提出SilentZNS，采用灵活的区域分配方案，摆脱传统的逻辑到物理区域映射，允许任意块集合分配给区域，同时确保磨损均衡和最佳读取性能。

Result: SilentZNS消除了高达20倍的虚拟写入负担，减少86%的设备级写入放大（在10%区域占用率时），降低76.9%的整体磨损，工作负载执行速度提升3.7倍。

Conclusion: SilentZNS通过动态资源分配和避免不必要写入，有效解决了ZNS SSD的关键性能问题，显著提升了设备性能和寿命。

Abstract: Zoned Namespace (ZNS) SSDs offer a promising interface for stable throughput
and low-latency storage by eliminating device-side garbage collection. They
expose storage as append-only zones that give the host applications direct
control over data placement. However, current ZNS implementations suffer from
(a) device-level write amplification (DLWA), (b) increased wear, and (c)
interference with host I/O due to zone mapping and management. We identify two
primary design decisions as the main cause: (i) fixed physical zones and (ii)
full-zone operations that lead to excessive physical writes. We propose
SilentZNS, a new zone mapping and management approach that addresses the
aforementioned limitations by on-the-fly allocating available resources to
zones, while minimizing wear, maintaining parallelism, and avoiding unnecessary
writes at the device-level. SilentZNS is a flexible zone allocation scheme that
departs from the traditional logical-to-physical zone mapping and allows for
arbitrary collections of blocks to be assigned to a zone. We add the necessary
constraints to ensure wear-leveling and state-of-the-art read performance, and
use only the required blocks to avoid dummy writes during zone reset. We
implement SilentZNS using the state-of-the-art ConfZNS++ emulator and show that
it eliminates the undue burden of dummy writes by up to 20x, leading to lower
DLWA (86% less at 10% zone occupancy), less overall wear (up to 76.9%), and up
to 3.7x faster workload execution.

</details>


### [7] [SMART-WRITE: Adaptive Learning-based Write Energy Optimization for Phase Change Memory](https://arxiv.org/abs/2511.04713)
*Mahek Desai,Rowena Quinn,Marjan Asadinia*

Main category: cs.AR

TL;DR: SMART-WRITE是一种结合神经网络和强化学习的方法，通过动态优化相变存储器(PCM)的写入参数，显著降低写入能耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着DRAM等传统存储器接近可扩展性极限，相变存储器(PCM)因其可扩展性、快速访问时间和零泄漏功率而成为有前景的替代方案。但PCM存在寿命有限和写入能耗高的问题，需要改进其耐用性和降低写入能耗。

Method: 提出SMART-WRITE方法，集成神经网络(NN)和强化学习(RL)。NN模型监控实时运行条件和设备特性以确定最优写入参数，RL模型动态调整这些参数以进一步优化PCM能耗。

Result: 通过基于实时系统条件持续调整PCM写入参数，SMART-WRITE相比基线和先前模型，写入能耗降低高达63%，性能提升高达51%。

Conclusion: SMART-WRITE方法有效解决了PCM的写入能耗和性能问题，为PCM作为数据存储的实用选择提供了可行方案。

Abstract: As dynamic random access memory (DRAM) and other current transistor-based
memories approach their scalability limits, the search for alternative storage
methods becomes increasingly urgent. Phase-change memory (PCM) emerges as a
promising candidate due to its scalability, fast access time, and zero leakage
power compared to many existing memory technologies. However, PCM has
significant drawbacks that currently hinder its viability as a replacement. PCM
cells suffer from a limited lifespan because write operations degrade the
physical material, and these operations consume a considerable amount of
energy. For PCM to be a practical option for data storage-which involves
frequent write operations-its cell endurance must be enhanced, and write energy
must be reduced. In this paper, we propose SMART-WRITE, a method that
integrates neural networks (NN) and reinforcement learning (RL) to dynamically
optimize write energy and improve performance. The NN model monitors real-time
operating conditions and device characteristics to determine optimal write
parameters, while the RL model dynamically adjusts these parameters to further
optimize PCM's energy consumption. By continuously adjusting PCM write
parameters based on real-time system conditions, SMART-WRITE reduces write
energy consumption by up to 63% and improves performance by up to 51% compared
to the baseline and previous models.

</details>


### [8] [MDM: Manhattan Distance Mapping of DNN Weights for Parasitic-Resistance-Resilient Memristive Crossbars](https://arxiv.org/abs/2511.04798)
*Matheus Farias,Wanghley Martins,H. T. Kung*

Main category: cs.AR

TL;DR: MDM是一种后训练DNN权重映射技术，通过优化有源忆阻器位置来减少忆阻计算内存交叉阵列中的寄生电阻非理想性，提高模拟计算精度。


<details>
  <summary>Details</summary>
Motivation: 寄生电阻限制了交叉阵列效率，需要将DNN矩阵映射到小型交叉阵列瓦片中，这会降低CIM加速效果并增加数字同步、ADC转换、延迟和芯片面积开销。

Method: 利用比特级结构化稀疏性，从密度较高的低阶侧输入激活，并根据曼哈顿距离重新排列行，将有源单元重新定位到受寄生电阻影响较小的区域。

Result: 在ImageNet-1k上的DNN模型中，MDM将非理想性因子降低高达46%，在ResNets中模拟失真下的准确率平均提高3.6%。

Conclusion: MDM提供了一种轻量级、空间感知的方法来扩展CIM DNN加速器。

Abstract: Manhattan Distance Mapping (MDM) is a post-training deep neural network (DNN)
weight mapping technique for memristive bit-sliced compute-in-memory (CIM)
crossbars that reduces parasitic resistance (PR) nonidealities.
  PR limits crossbar efficiency by mapping DNN matrices into small crossbar
tiles, reducing CIM-based speedup. Each crossbar executes one tile, requiring
digital synchronization before the next layer. At this granularity, designers
either deploy many small crossbars in parallel or reuse a few sequentially-both
increasing analog-to-digital conversions, latency, I/O pressure, and chip area.
  MDM alleviates PR effects by optimizing active-memristor placement.
Exploiting bit-level structured sparsity, it feeds activations from the denser
low-order side and reorders rows according to the Manhattan distance,
relocating active cells toward regions less affected by PR and thus lowering
the nonideality factor (NF).
  Applied to DNN models on ImageNet-1k, MDM reduces NF by up to 46% and
improves accuracy under analog distortion by an average of 3.6% in ResNets.
Overall, it provides a lightweight, spatially informed method for scaling CIM
DNN accelerators.

</details>


### [9] [MultiVic: A Time-Predictable RISC-V Multi-Core Processor Optimized for Neural Network Inference](https://arxiv.org/abs/2511.05321)
*Maximilian Kirschner,Konstantin Dudzik,Ben Krusekamp,Jürgen Becker*

Main category: cs.AR

TL;DR: 提出了一种新的硬件架构，通过多核向量处理器和本地暂存内存来解决AI加速器中性能与可预测性之间的平衡问题，特别适用于自动驾驶等实时系统。


<details>
  <summary>Details</summary>
Motivation: 实时系统（如自动驾驶）越来越多地采用神经网络，需要高性能且具有可预测时序行为的硬件。现有实时硬件资源有限，而现代AI加速器由于内存干扰缺乏可预测性。

Method: 设计多核向量处理器架构，每个核心配备本地暂存内存，由中央管理核心按照静态确定的调度方案协调共享外部内存访问。

Result: 分析不同参数化设计变体，发现配置更多小型核心的架构由于增加的有效内存带宽和更高时钟频率而获得更好性能，同时执行时间波动非常低。

Conclusion: 该架构在保持高性能的同时实现了时间可预测性，成功弥合了实时系统中性能与可预测性之间的差距。

Abstract: Real-time systems, particularly those used in domains like automated driving,
are increasingly adopting neural networks. From this trend arises the need for
high-performance hardware exhibiting predictable timing behavior. While
state-of-the-art real-time hardware often suffers from limited memory and
compute resources, modern AI accelerators typically lack the crucial
predictability due to memory interference.
  We present a new hardware architecture to bridge this gap between performance
and predictability. The architecture features a multi-core vector processor
with predictable cores, each equipped with local scratchpad memories. A central
management core orchestrates access to shared external memory following a
statically determined schedule.
  To evaluate the proposed hardware architecture, we analyze different variants
of our parameterized design. We compare these variants to a baseline
architecture consisting of a single-core vector processor with large vector
registers. We find that configurations with a larger number of smaller cores
achieve better performance due to increased effective memory bandwidth and
higher clock frequencies. Crucially for real-time systems, execution time
fluctuation remains very low, demonstrating the platform's time predictability.

</details>
