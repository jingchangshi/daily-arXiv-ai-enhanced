{"id": "2510.19972", "categories": ["cs.DC", "cs.CC"], "pdf": "https://arxiv.org/pdf/2510.19972", "abs": "https://arxiv.org/abs/2510.19972", "authors": ["Alkida Balliu", "Filippo Casagrande", "Francesco d'Amore", "Dennis Olivetti"], "title": "New Hardness Results for the LOCAL Model via a Simple Self-Reduction", "comment": "21 pages, no figures", "summary": "Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL\nalgorithm that solves maximal matching requires $\\Omega(\\min\\{\\log \\Delta,\n\\log_\\Delta n\\})$ rounds, where $n$ is the number of nodes in the graph and\n$\\Delta$ is the maximum degree. This result is shown through a new technique,\ncalled round elimination via self-reduction. The lower bound proof is beautiful\nand presents very nice ideas. However, it spans more than 25 pages of technical\ndetails, and hence it is hard to digest and generalize to other problems.\nHistorically, the simplification of proofs and techniques has marked an\nimportant turning point in our understanding of the complexity of graph\nproblems. Our paper makes a step forward towards this direction, and provides\nthe following contributions.\n  1. We present a short and simplified version of the round elimination via\nself-reduction technique. The simplification of this technique enables us to\nobtain the following two hardness results.\n  2. We show that any randomized LOCAL algorithm that solves the maximal\n$b$-matching problem requires $\\Omega(\\min\\{\\log_{1+b}\\Delta, \\log_\\Delta n\\})$\nand $\\Omega(\\sqrt{\\log_{1+b} n})$ rounds. We recall that the $b$-matching\nproblem is a generalization of the matching problem where each vertex can have\nup to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain\na short proof for the maximal matching lower bound shown by Khoury and Schild.\n  3. Finally, we show that any randomized LOCAL algorithm that properly colors\nthe edges of a graph with $\\Delta + k$ colors requires $\\Omega(\\min\\{\\log\n\\Delta, \\log_\\Delta n\\})$ and $\\Omega(\\sqrt{\\log n})$ rounds, for any $k\\le\n\\Delta^{1-\\varepsilon}$ and any constant $\\varepsilon > 0$.", "AI": {"tldr": "\u672c\u6587\u7b80\u5316\u4e86Khoury\u548cSchild\u7684\u8f6e\u6d88\u9664\u81ea\u7ea6\u7b80\u6280\u672f\uff0c\u5e76\u5e94\u7528\u8be5\u6280\u672f\u8bc1\u660e\u4e86\u6700\u5927b\u5339\u914d\u548c\u8fb9\u7740\u8272\u7684\u968f\u673aLOCAL\u7b97\u6cd5\u4e0b\u754c\u3002", "motivation": "Khoury\u548cSchild\u63d0\u51fa\u7684\u6700\u5927\u5339\u914d\u4e0b\u754c\u8bc1\u660e\u957f\u8fbe25\u9875\u4e14\u6280\u672f\u590d\u6742\uff0c\u96be\u4ee5\u7406\u89e3\u548c\u63a8\u5e7f\u3002\u672c\u6587\u65e8\u5728\u7b80\u5316\u8be5\u6280\u672f\u5e76\u6269\u5c55\u5e94\u7528\u5230\u5176\u4ed6\u56fe\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7b80\u5316\u7684\u8f6e\u6d88\u9664\u81ea\u7ea6\u7b80\u6280\u672f\uff0c\u6784\u5efa\u4e86\u9002\u7528\u4e8e\u6700\u5927b\u5339\u914d\u548c\u8fb9\u7740\u8272\u95ee\u9898\u7684\u4e0b\u754c\u8bc1\u660e\u6846\u67b6\u3002", "result": "\u8bc1\u660e\u4e86\u6700\u5927b\u5339\u914d\u9700\u8981\u03a9(min{log\u2081\u208ab\u0394, log_\u0394 n})\u548c\u03a9(\u221alog\u2081\u208ab n)\u8f6e\uff0c\u8fb9\u7740\u8272\u9700\u8981\u03a9(min{log \u0394, log_\u0394 n})\u548c\u03a9(\u221alog n)\u8f6e\u7684\u4e0b\u754c\u3002", "conclusion": "\u7b80\u5316\u540e\u7684\u8f6e\u6d88\u9664\u81ea\u7ea6\u7b80\u6280\u672f\u4e0d\u4ec5\u91cd\u73b0\u4e86\u6700\u5927\u5339\u914d\u4e0b\u754c\uff0c\u8fd8\u6210\u529f\u63a8\u5e7f\u5230\u6700\u5927b\u5339\u914d\u548c\u8fb9\u7740\u8272\u95ee\u9898\uff0c\u4e3a\u7406\u89e3\u56fe\u95ee\u9898\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u66f4\u7b80\u6d01\u7684\u5de5\u5177\u3002"}}
{"id": "2510.20111", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20111", "abs": "https://arxiv.org/abs/2510.20111", "authors": ["Huawei Bai", "Yifan Huang", "Wenqi Shi", "Ansheng You", "Feifan Shao", "Tengfei Han", "Minghui Yu"], "title": "AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training", "comment": "14 pages, 5 figures, tech report", "summary": "The training efficiency and scalability of language models on massive\nclusters currently remain a critical bottleneck. Mainstream approaches like ND\nparallelism are often cumbersome and complex, while flexible alternatives such\nas the Zero Redundancy Optimizer (ZeRO) are frequently hampered by\ncommunication overhead. In this paper, we propose Asynchronous Hierarchical\nZero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to\nachieve superior performance while maintaining simplicity and memory\nefficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding\nthat can lead to inefficient communication, AsyncHZP adaptively reshards\nparameters, gradients, and optimizer states across different replica groups.\nThis strategy optimizes device memory utilization and significantly reduces\ncommunication overhead. In addition, we also design a multi-stream asynchronous\nscheduling method that executes parameter all-gather and gradient\nreduce-scatter operations in dedicated background threads, effectively\noverlapping communication with computation while incurring negligible memory\nfragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)\nmodels confirm that AsyncHZP maintains robust stability at scale. It\nconsistently outperforms classic ND parallelism, achieving state-of-the-art\nperformance without complex strategic tuning, thereby simplifying the path to\nefficient large-scale training.", "AI": {"tldr": "\u63d0\u51faAsyncHZP\uff0c\u4e00\u79cd\u5f02\u6b65\u5206\u5c42\u96f6\u5e76\u884c\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u53c2\u6570\u5206\u7247\u548c\u591a\u6d41\u5f02\u6b65\u8c03\u5ea6\uff0c\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u5927\u89c4\u6a21\u96c6\u7fa4\u4e0a\u7684\u8bad\u7ec3\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u5b58\u5728\u74f6\u9888\uff0c\u4e3b\u6d41ND\u5e76\u884c\u65b9\u6cd5\u590d\u6742\u7e41\u7410\uff0c\u800cZeRO\u7b49\u7075\u6d3b\u65b9\u6848\u53c8\u53d7\u901a\u4fe1\u5f00\u9500\u9650\u5236\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u53c2\u6570\u5206\u7247\u7b56\u7565\uff0c\u5728\u4e0d\u540c\u526f\u672c\u7ec4\u95f4\u91cd\u65b0\u5206\u7247\u53c2\u6570\u3001\u68af\u5ea6\u548c\u4f18\u5316\u5668\u72b6\u6001\uff1b\u8bbe\u8ba1\u591a\u6d41\u5f02\u6b65\u8c03\u5ea6\u65b9\u6cd5\uff0c\u5728\u540e\u53f0\u7ebf\u7a0b\u6267\u884c\u53c2\u6570\u6536\u96c6\u548c\u68af\u5ea6\u5206\u6563\u64cd\u4f5c\uff0c\u5b9e\u73b0\u901a\u4fe1\u4e0e\u8ba1\u7b97\u91cd\u53e0\u3002", "result": "\u5728\u5bc6\u96c6\u6a21\u578b\u548cMoE\u6a21\u578b\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cAsyncHZP\u5728\u5927\u89c4\u6a21\u4e0b\u4fdd\u6301\u7a33\u5b9a\uff0c\u59cb\u7ec8\u4f18\u4e8e\u7ecf\u5178ND\u5e76\u884c\u65b9\u6cd5\uff0c\u65e0\u9700\u590d\u6742\u7b56\u7565\u8c03\u4f18\u5373\u53ef\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "AsyncHZP\u7b80\u5316\u4e86\u9ad8\u6548\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u8def\u5f84\uff0c\u5728\u4fdd\u6301\u7b80\u5355\u6027\u548c\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.20128", "categories": ["cs.DC", "quant-ph", "D.2.6"], "pdf": "https://arxiv.org/pdf/2510.20128", "abs": "https://arxiv.org/abs/2510.20128", "authors": ["Xin Zhan", "K. Grace Johnson", "Aniello Esposito", "Barbara Chapman", "Marco Fiorentino", "Kirk M. Bresniker", "Raymond G. Beausoleil", "Masoud Mohseni"], "title": "A Full Stack Framework for High Performance Quantum-Classical Computing", "comment": "9 pages, 8 figures, presented at Cray User Group Meeting 2025, May\n  04-09, 2025, New York, NY", "summary": "To address the growing needs for scalable High Performance Computing (HPC)\nand Quantum Computing (QC) integration, we present our HPC-QC full stack\nframework and its hybrid workload development capability with modular\nhardware/device-agnostic software integration approach. The latest development\nin extensible interfaces for quantum programming, dispatching, and compilation\nwithin existing mature HPC programming environment are demonstrated. Our HPC-QC\nfull stack enables high-level, portable invocation of quantum kernels from\ncommercial quantum SDKs within HPC meta-program in compiled languages (C/C++\nand Fortran) as well as Python through a quantum programming interface library\nextension. An adaptive circuit knitting hypervisor is being developed to\npartition large quantum circuits into sub-circuits that fit on smaller noisy\nquantum devices and classical simulators. At the lower-level, we leverage Cray\nLLVM-based compilation framework to transform and consume LLVM IR and Quantum\nIR (QIR) from commercial quantum software frontends in a retargetable fashion\nto different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU\nand GPU workloads (including solving linear system of equations, quantum\noptimization, and simulating quantum phase transitions) have been demonstrated\non HPE EX supercomputers to illustrate functionality and execution viability\nfor all three components developed so far. This work provides the framework for\na unified quantum-classical programming environment built upon classical HPC\nsoftware stack (compilers, libraries, parallel runtime and process scheduling).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2aHPC-QC\u5168\u6808\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u786c\u4ef6/\u8bbe\u5907\u65e0\u5173\u7684\u8f6f\u4ef6\u96c6\u6210\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e0e\u91cf\u5b50\u8ba1\u7b97\u7684\u6df7\u5408\u5de5\u4f5c\u8d1f\u8f7d\u5f00\u53d1\u3002", "motivation": "\u89e3\u51b3\u53ef\u6269\u5c55\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e0e\u91cf\u5b50\u8ba1\u7b97\u96c6\u6210\u7684\u9700\u6c42\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u91cf\u5b50-\u7ecf\u5178\u7f16\u7a0b\u73af\u5883\u3002", "method": "\u91c7\u7528\u53ef\u6269\u5c55\u63a5\u53e3\u8fdb\u884c\u91cf\u5b50\u7f16\u7a0b\u3001\u8c03\u5ea6\u548c\u7f16\u8bd1\uff0c\u5728\u73b0\u6709\u6210\u719fHPC\u7f16\u7a0b\u73af\u5883\u4e2d\u5b9e\u73b0\u91cf\u5b50\u5185\u6838\u7684\u9ad8\u5c42\u53ef\u79fb\u690d\u8c03\u7528\uff0c\u5f00\u53d1\u81ea\u9002\u5e94\u7535\u8def\u7f16\u7ec7\u865a\u62df\u673a\u6765\u5206\u533a\u5927\u578b\u91cf\u5b50\u7535\u8def\u3002", "result": "\u5728HPE EX\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u6210\u529f\u6f14\u793a\u4e86\u591a\u4e2a\u6df7\u5408HPC-QC\u591a\u8282\u70b9\u591aCPU\u548cGPU\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5305\u62ec\u6c42\u89e3\u7ebf\u6027\u65b9\u7a0b\u7ec4\u3001\u91cf\u5b50\u4f18\u5316\u548c\u6a21\u62df\u91cf\u5b50\u76f8\u53d8\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u57fa\u4e8e\u7ecf\u5178HPC\u8f6f\u4ef6\u6808\u7684\u7edf\u4e00\u91cf\u5b50-\u7ecf\u5178\u7f16\u7a0b\u73af\u5883\u63d0\u4f9b\u4e86\u6846\u67b6\u3002"}}
{"id": "2510.20171", "categories": ["cs.DC", "cs.AI", "cs.NI", "C.2.4; I.2"], "pdf": "https://arxiv.org/pdf/2510.20171", "abs": "https://arxiv.org/abs/2510.20171", "authors": ["Min Si", "Pavan Balaji", "Yongzhou Chen", "Ching-Hsiang Chu", "Adi Gangidi", "Saif Hasan", "Subodh Iyengar", "Dan Johnson", "Bingzhe Liu", "Jingliang Ren", "Ashmitha Jeevaraj Shetty", "Greg Steinbrecher", "Xinfeng Xie", "Yulun Wang", "Bruce Wu", "Jingyi Yang", "Mingran Yang", "Minlan Yu", "Cen Zhao", "Wes Bland", "Denis Boyda", "Suman Gumudavelli", "Cristian Lumezanu", "Rui Miao", "Zhe Qu", "Venkat Ramesh", "Maxim Samoylov", "Jan Seidel", "Feng Tian", "Qiye Tan", "Shuqiang Zhang", "Yimeng Zhao", "Shengbao Zheng", "Art Zhu", "Hongyi Zeng"], "title": "Collective Communication for 100k+ GPUs", "comment": null, "summary": "The increasing scale of large language models (LLMs) necessitates highly\nefficient collective communication frameworks, particularly as training\nworkloads extend to hundreds of thousands of GPUs. Traditional communication\nmethods face significant throughput and latency limitations at this scale,\nhindering both the development and deployment of state-of-the-art models. This\npaper presents the NCCLX collective communication framework, developed at Meta,\nengineered to optimize performance across the full LLM lifecycle, from the\nsynchronous demands of large-scale training to the low-latency requirements of\ninference. The framework is designed to support complex workloads on clusters\nexceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency\ndata exchange. Empirical evaluation on the Llama4 model demonstrates\nsubstantial improvements in communication efficiency. This research contributes\na robust solution for enabling the next generation of LLMs to operate at\nunprecedented scales.", "AI": {"tldr": "NCCLX\u662fMeta\u5f00\u53d1\u7684\u96c6\u4f53\u901a\u4fe1\u6846\u67b6\uff0c\u65e8\u5728\u4f18\u5316\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u8d8510\u4e07GPU\u96c6\u7fa4\u4e0a\u7684\u901a\u4fe1\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u4f20\u7edf\u901a\u4fe1\u65b9\u6cd5\u5728\u6570\u4e07GPU\u89c4\u6a21\u4e0b\u9762\u4e34\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u9650\u5236\uff0c\u963b\u788d\u4e86\u6700\u5148\u8fdb\u6a21\u578b\u7684\u5f00\u53d1\u548c\u90e8\u7f72\u3002", "method": "\u5f00\u53d1NCCLX\u96c6\u4f53\u901a\u4fe1\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u8d8510\u4e07GPU\u96c6\u7fa4\u7684\u590d\u6742\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u4f18\u5316\uff0c\u652f\u6301\u4ece\u5927\u89c4\u6a21\u8bad\u7ec3\u5230\u4f4e\u5ef6\u8fdf\u63a8\u7406\u7684\u5b8c\u6574LLM\u751f\u547d\u5468\u671f\u3002", "result": "\u5728Llama4\u6a21\u578b\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\u901a\u4fe1\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u5b9e\u73b0\u4e86\u53ef\u9760\u3001\u9ad8\u541e\u5410\u91cf\u548c\u4f4e\u5ef6\u8fdf\u7684\u6570\u636e\u4ea4\u6362\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e0b\u4e00\u4ee3LLM\u5728\u7a7a\u524d\u89c4\u6a21\u4e0a\u8fd0\u884c\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.19850", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.19850", "abs": "https://arxiv.org/abs/2510.19850", "authors": ["Mostapha Kalami Heris"], "title": "Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs", "comment": null, "summary": "Large Language Models (LLMs) are central to reasoning, writing, and\ndecision-support workflows, yet users lack consistent control over how they\nreason and express outputs. Conventional prompt engineering relies on verbose\nnatural-language instructions, limiting reproducibility, modularity, and\ninterpretability. This paper introduces Prompt Decorators, a declarative,\ncomposable syntax that governs LLM behavior through compact control tokens such\nas +++Reasoning, +++Tone(style=formal), and +++Import(topic=\"Systems\nThinking\"). Each decorator modifies a behavioral dimension, such as reasoning\nstyle, structure, or tone, without changing task content. The framework\nformalizes twenty core decorators organized into two functional families\n(Cognitive & Generative and Expressive & Systemic), each further decomposed\ninto subcategories that govern reasoning, interaction, expression, and\nsession-control. It defines a unified syntax, scoping model, and deterministic\nprocessing pipeline enabling predictable and auditable behavior composition. By\ndecoupling task intent from execution behavior, Prompt Decorators create a\nreusable and interpretable interface for prompt design. Illustrative use cases\ndemonstrate improved reasoning transparency, reduced prompt complexity, and\nstandardized model behavior across domains. The paper concludes with\nimplications for interoperability, behavioral consistency, and the development\nof declarative interfaces for scalable AI systems.", "AI": {"tldr": "\u63d0\u51faPrompt Decorators\u6846\u67b6\uff0c\u901a\u8fc7\u58f0\u660e\u5f0f\u8bed\u6cd5\u548c\u7d27\u51d1\u63a7\u5236\u4ee4\u724c\u6765\u6807\u51c6\u5316\u63a7\u5236LLM\u7684\u884c\u4e3a\u7ef4\u5ea6\uff0c\u5b9e\u73b0\u53ef\u590d\u7528\u3001\u53ef\u89e3\u91ca\u7684\u63d0\u793a\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edf\u63d0\u793a\u5de5\u7a0b\u4f9d\u8d56\u5197\u957f\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u9650\u5236\u4e86\u53ef\u91cd\u590d\u6027\u3001\u6a21\u5757\u5316\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u7528\u6237\u7f3a\u4e4f\u5bf9LLM\u63a8\u7406\u548c\u8f93\u51fa\u8868\u8fbe\u7684\u4e00\u81f4\u63a7\u5236\u3002", "method": "\u5f15\u5165Prompt Decorators\u58f0\u660e\u5f0f\u8bed\u6cd5\uff0c\u4f7f\u7528\u7d27\u51d1\u63a7\u5236\u4ee4\u724c\uff08\u5982+++Reasoning\u3001+++Tone\uff09\u6765\u4fee\u6539\u884c\u4e3a\u7ef4\u5ea6\uff0c\u5b9a\u4e4920\u4e2a\u6838\u5fc3\u88c5\u9970\u5668\uff0c\u5206\u4e3a\u8ba4\u77e5\u4e0e\u751f\u6210\u3001\u8868\u8fbe\u4e0e\u7cfb\u7edf\u4e24\u5927\u529f\u80fd\u65cf\uff0c\u5efa\u7acb\u7edf\u4e00\u8bed\u6cd5\u3001\u4f5c\u7528\u57df\u6a21\u578b\u548c\u786e\u5b9a\u6027\u5904\u7406\u6d41\u7a0b\u3002", "result": "\u6f14\u793a\u7528\u4f8b\u663e\u793a\u63d0\u9ad8\u4e86\u63a8\u7406\u900f\u660e\u5ea6\u3001\u51cf\u5c11\u4e86\u63d0\u793a\u590d\u6742\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u8de8\u9886\u57df\u7684\u6807\u51c6\u5316\u6a21\u578b\u884c\u4e3a\u3002", "conclusion": "Prompt Decorators\u4e3a\u53ef\u6269\u5c55AI\u7cfb\u7edf\u5f00\u53d1\u58f0\u660e\u5f0f\u63a5\u53e3\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5bf9\u4e92\u64cd\u4f5c\u6027\u3001\u884c\u4e3a\u4e00\u81f4\u6027\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2510.20137", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.20137", "abs": "https://arxiv.org/abs/2510.20137", "authors": ["Hasnain A. Ziad", "Ashiq A. Sakib"], "title": "HALOC-AxA: An Area/-Energy-Efficient Approximate Adder for Image Processing Application", "comment": "5 Pages, 6 Figures, and 1 Table", "summary": "The design of approximate adders has been widely researched to advance\nenergy-efficient hardware for computation-intensive multimedia applications,\nsuch as image, audio, or video processing. The design of approximate adders has\nbeen widely researched to advance energy-efficient hardware for computation\nintensive multimedia applications, such as image/audio/video processing.\nSeveral static and dynamic approximate adders exist in the literature, each of\nwhich endeavors to balance the conflicting demands of high performance,\ncomputational accuracy, and energy efficiency. This work introduces a novel\napproximate adder that is more energy- and area-efficient than existing adders,\nwhile achieving improved or comparable accuracy, as demonstrated by simulation\nresults. The proposed adder's ability to digitally reconstruct high quality\nimages is further demonstrated by the deployment of the design for an image\nprocessing task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8fd1\u4f3c\u52a0\u6cd5\u5668\uff0c\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u8ba1\u7b97\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u6bd4\u73b0\u6709\u52a0\u6cd5\u5668\u5177\u6709\u66f4\u597d\u7684\u80fd\u91cf\u548c\u9762\u79ef\u6548\u7387\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u5904\u7406\u7b49\u8ba1\u7b97\u5bc6\u96c6\u578b\u591a\u5a92\u4f53\u5e94\u7528\u3002", "motivation": "\u4e3a\u8ba1\u7b97\u5bc6\u96c6\u578b\u591a\u5a92\u4f53\u5e94\u7528\uff08\u5982\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u5904\u7406\uff09\u8bbe\u8ba1\u80fd\u91cf\u9ad8\u6548\u7684\u786c\u4ef6\uff0c\u9700\u8981\u5e73\u8861\u9ad8\u6027\u80fd\u3001\u8ba1\u7b97\u7cbe\u5ea6\u548c\u80fd\u91cf\u6548\u7387\u4e4b\u95f4\u7684\u51b2\u7a81\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u8fd1\u4f3c\u52a0\u6cd5\u5668\uff0c\u901a\u8fc7\u6a21\u62df\u7ed3\u679c\u9a8c\u8bc1\u5176\u6027\u80fd\uff0c\u5e76\u5c06\u5176\u90e8\u7f72\u5230\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u4e2d\u5c55\u793a\u6570\u5b57\u91cd\u5efa\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u80fd\u529b\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u52a0\u6cd5\u5668\u6bd4\u73b0\u6709\u52a0\u6cd5\u5668\u66f4\u8282\u80fd\u4e14\u9762\u79ef\u6548\u7387\u66f4\u9ad8\uff0c\u540c\u65f6\u5b9e\u73b0\u6539\u8fdb\u6216\u76f8\u5f53\u7684\u8ba1\u7b97\u7cbe\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8fd1\u4f3c\u52a0\u6cd5\u5668\u5728\u80fd\u91cf\u6548\u7387\u3001\u9762\u79ef\u6548\u7387\u548c\u8ba1\u7b97\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u9002\u7528\u4e8e\u591a\u5a92\u4f53\u5904\u7406\u5e94\u7528\u3002"}}
{"id": "2510.20388", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20388", "abs": "https://arxiv.org/abs/2510.20388", "authors": ["V\u00edctor Ramp\u00e9rez", "Javier Soriano", "David Lizcano", "Juan A. Lara"], "title": "FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services", "comment": null, "summary": "Cloud computing has established itself as the support for the vast majority\nof emerging technologies, mainly due to the characteristic of elasticity it\noffers. Auto-scalers are the systems that enable this elasticity by acquiring\nand releasing resources on demand to ensure an agreed service level. In this\narticle we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for\ndistributed services that combines the advantages of proactive and reactive\napproaches according to the situation to decide the optimal scaling actions in\nevery moment. The main novelties introduced by FLAS are (i) a predictive model\nof the high-level metrics trend which allows to anticipate changes in the\nrelevant SLA parameters (e.g. performance metrics such as response time or\nthroughput) and (ii) a reactive contingency system based on the estimation of\nhigh-level metrics from resource use metrics, reducing the necessary\ninstrumentation (less invasive) and allowing it to be adapted agnostically to\ndifferent applications. We provide a FLAS implementation for the use case of a\ncontent-based publish-subscribe middleware (E-SilboPS) that is the cornerstone\nof an event-driven architecture. To the best of our knowledge, this is the\nfirst auto-scaling system for content-based publish-subscribe distributed\nsystems (although it is generic enough to fit any distributed service). Through\nan evaluation based on several test cases recreating not only the expected\ncontexts of use, but also the worst possible scenarios (following the\nBoundary-Value Analysis or BVA test methodology), we have validated our\napproach and demonstrated the effectiveness of our solution by ensuring\ncompliance with performance requirements over 99% of the time.", "AI": {"tldr": "FLAS\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u4e3b\u52a8\u548c\u88ab\u52a8\u65b9\u6cd5\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u6d4b\u9ad8\u7ef4\u6307\u6807\u8d8b\u52bf\u548c\u57fa\u4e8e\u8d44\u6e90\u4f7f\u7528\u6307\u6807\u7684\u88ab\u52a8\u5e94\u6025\u7cfb\u7edf\uff0c\u4e3a\u5206\u5e03\u5f0f\u670d\u52a1\u63d0\u4f9b\u4f18\u5316\u7684\u6269\u7f29\u5bb9\u51b3\u7b56\u3002", "motivation": "\u4e91\u8ba1\u7b97\u5f39\u6027\u7279\u6027\u9700\u8981\u81ea\u52a8\u6269\u7f29\u5bb9\u7cfb\u7edf\u6765\u786e\u4fdd\u670d\u52a1\u6c34\u5e73\u534f\u8bae\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u9884\u6d4bSLA\u53c2\u6570\u53d8\u5316\u5e76\u51cf\u5c11\u5fc5\u8981\u7684\u68c0\u6d4b\u5f00\u9500\u3002", "method": "FLAS\u7ed3\u5408\u4e3b\u52a8\u9884\u6d4b\u548c\u88ab\u52a8\u5e94\u6025\u65b9\u6cd5\uff1a\u4f7f\u7528\u9884\u6d4b\u6a21\u578b\u9884\u6d4b\u9ad8\u7ef4\u6307\u6807\u8d8b\u52bf\uff0c\u4ee5\u53ca\u57fa\u4e8e\u8d44\u6e90\u4f7f\u7528\u6307\u6807\u4f30\u8ba1\u9ad8\u7ef4\u6307\u6807\u7684\u88ab\u52a8\u7cfb\u7edf\uff0c\u51cf\u5c11\u68c0\u6d4b\u9700\u6c42\u3002", "result": "\u5728\u5185\u5bb9\u53d1\u5e03-\u8ba2\u9605\u4e2d\u95f4\u4ef6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cFLAS\u572899%\u4ee5\u4e0a\u7684\u65f6\u95f4\u5185\u786e\u4fdd\u6027\u80fd\u8981\u6c42\u5f97\u5230\u6ee1\u8db3\uff0c\u5305\u62ec\u6700\u574f\u573a\u666f\u6d4b\u8bd5\u3002", "conclusion": "FLAS\u662f\u9996\u4e2a\u9488\u5bf9\u57fa\u4e8e\u5185\u5bb9\u7684\u53d1\u5e03-\u8ba2\u9605\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u901a\u7528\u6027\u4e14\u80fd\u6709\u6548\u786e\u4fdd\u6027\u80fdSLA\u3002"}}
{"id": "2510.19853", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.19853", "abs": "https://arxiv.org/abs/2510.19853", "authors": ["Assaf Marron", "David Harel"], "title": "A Specification's Realm: Characterizing the Knowledge Required for Executing a Given Algorithm Specification", "comment": null, "summary": "An algorithm specification in natural language or pseudocode is expected to\nbe clear and explicit enough to enable mechanical execution. In this position\npaper we contribute an initial characterization of the knowledge that an\nexecuting agent, human or machine, should possess in order to be able to carry\nout the instructions of a given algorithm specification as a stand-alone\nentity, independent of any system implementation. We argue that, for that\nalgorithm specification, such prerequisite knowledge, whether unique or shared\nwith other specifications, can be summarized in a document of practical size.\nWe term this document the realm of the algorithm specification. The generation\nof such a realm is itself a systematic analytical process, significant parts of\nwhich can be automated with the help of large language models and the reuse of\nexisting documents. The algorithm-specification's realm would consist of\nspecification language syntax and semantics, domain knowledge restricted to the\nreferenced entities, inter-entity relationships, relevant underlying\ncause-and-effect rules, and detailed instructions and means for carrying out\ncertain operations. Such characterization of the realm can contribute to\nmethodological implementation of the algorithm specification in diverse systems\nand to its formalization for mechanical verification. The paper also touches\nupon the question of assessing execution faithfulness, which is distinct from\ncorrectness: in the absence of a reference interpretation of natural language\nor pseudocode specification with a given vocabulary, how can we determine if an\nobserved agent's execution indeed complies with the input specification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7b97\u6cd5\u89c4\u8303\u7684\"\u9886\u57df\"\u6982\u5ff5\uff0c\u5373\u6267\u884c\u7b97\u6cd5\u6240\u9700\u7684\u524d\u63d0\u77e5\u8bc6\u96c6\u5408\uff0c\u5305\u62ec\u8bed\u6cd5\u8bed\u4e49\u3001\u9886\u57df\u77e5\u8bc6\u3001\u5b9e\u4f53\u5173\u7cfb\u3001\u56e0\u679c\u89c4\u5219\u548c\u64cd\u4f5c\u6307\u4ee4\u7b49\uff0c\u8fd9\u4e9b\u77e5\u8bc6\u53ef\u88ab\u7cfb\u7edf\u5316\u6574\u7406\u6210\u5b9e\u7528\u6587\u6863\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u81ea\u7136\u8bed\u8a00\u6216\u4f2a\u4ee3\u7801\u7684\u7b97\u6cd5\u89c4\u8303\u80fd\u591f\u88ab\u673a\u68b0\u6267\u884c\uff0c\u9700\u8981\u660e\u786e\u6267\u884c\u4ee3\u7406\uff08\u4eba\u6216\u673a\u5668\uff09\u6240\u9700\u7684\u524d\u63d0\u77e5\u8bc6\uff0c\u8fd9\u4e9b\u77e5\u8bc6\u5e94\u8be5\u80fd\u591f\u72ec\u7acb\u4e8e\u5177\u4f53\u7cfb\u7edf\u5b9e\u73b0\u800c\u5b58\u5728\u3002", "method": "\u63d0\u51fa\u7b97\u6cd5\u89c4\u8303\"\u9886\u57df\"\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u5206\u6790\u8fc7\u7a0b\u751f\u6210\u5305\u542b\u8bed\u6cd5\u8bed\u4e49\u3001\u9886\u57df\u77e5\u8bc6\u3001\u5b9e\u4f53\u5173\u7cfb\u3001\u56e0\u679c\u89c4\u5219\u548c\u64cd\u4f5c\u6307\u4ee4\u7684\u6587\u6863\uff0c\u90e8\u5206\u8fc7\u7a0b\u53ef\u501f\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u548c\u73b0\u6709\u6587\u6863\u5b9e\u73b0\u81ea\u52a8\u5316\u3002", "result": "\u7b97\u6cd5\u89c4\u8303\u7684\u9886\u57df\u6587\u6863\u80fd\u591f\u4e3a\u7b97\u6cd5\u5728\u4e0d\u540c\u7cfb\u7edf\u4e2d\u7684\u65b9\u6cd5\u5316\u5b9e\u73b0\u548c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u63d0\u4f9b\u652f\u6301\uff0c\u540c\u65f6\u6709\u52a9\u4e8e\u8bc4\u4f30\u6267\u884c\u5fe0\u5b9e\u5ea6\uff08\u800c\u975e\u6b63\u786e\u6027\uff09\u3002", "conclusion": "\u7b97\u6cd5\u89c4\u8303\u7684\u9886\u57df\u6982\u5ff5\u4e3a\u7b97\u6cd5\u6267\u884c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u524d\u63d0\u77e5\u8bc6\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u8de8\u7cfb\u7edf\u7684\u6807\u51c6\u5316\u6267\u884c\u548c\u9a8c\u8bc1\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u6267\u884c\u5fe0\u5b9e\u5ea6\u8bc4\u4f30\u8fd9\u4e00\u91cd\u8981\u95ee\u9898\u3002"}}
{"id": "2510.20269", "categories": ["cs.AR", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.20269", "abs": "https://arxiv.org/abs/2510.20269", "authors": ["Ismail Emir Yuksel", "Ataberk Olgun", "F. Nisa Bostanci", "Oguzhan Canpolat", "Geraldo F. Oliveira", "Mohammad Sadrosadati", "Abdullah Giray Yaglikci", "Onur Mutlu"], "title": "In-DRAM True Random Number Generation Using Simultaneous Multiple-Row Activation: An Experimental Study of Real DRAM Chips", "comment": "Extended version of our publication at the 43rd IEEE International\n  Conference on Computer Design (ICCD-43), 2025", "summary": "In this work, we experimentally demonstrate that it is possible to generate\ntrue random numbers at high throughput and low latency in commercial\noff-the-shelf (COTS) DRAM chips by leveraging simultaneous multiple-row\nactivation (SiMRA) via an extensive characterization of 96 DDR4 DRAM chips. We\nrigorously analyze SiMRA's true random generation potential in terms of\nentropy, latency, and throughput for varying numbers of simultaneously\nactivated DRAM rows (i.e., 2, 4, 8, 16, and 32), data patterns, temperature\nlevels, and spatial variations. Among our 11 key experimental observations, we\nhighlight four key results. First, we evaluate the quality of our TRNG designs\nusing the commonly-used NIST statistical test suite for randomness and find\nthat all SiMRA-based TRNG designs successfully pass each test. Second, 2-, 8-,\n16-, and 32-row activation-based TRNG designs outperform the state-of-theart\nDRAM-based TRNG in throughput by up to 1.15x, 1.99x, 1.82x, and 1.39x,\nrespectively. Third, SiMRA's entropy tends to increase with the number of\nsimultaneously activated DRAM rows. Fourth, operational parameters and\nconditions (e.g., data pattern and temperature) significantly affect entropy.\nFor example, for most of the tested modules, the average entropy of 32-row\nactivation is 2.51x higher than that of 2-row activation. For example,\nincreasing the temperature from 50{\\deg}C to 90{\\deg}C decreases SiMRA's\nentropy by 1.53x for 32-row activation. To aid future research and development,\nwe open-source our infrastructure at https://github.com/CMU-SAFARI/SiMRA-TRNG.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u5546\u7528DRAM\u82af\u7247\u4e2d\u5229\u7528\u540c\u65f6\u591a\u884c\u6fc0\u6d3b(SiMRA)\u6280\u672f\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u771f\u968f\u673a\u6570\uff0c\u5728\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709DRAM-based TRNG\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u5728\u5546\u7528DRAM\u82af\u7247\u4e2d\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u771f\u968f\u673a\u6570\u751f\u6210\u5668(TRNG)\uff0c\u5229\u7528DRAM\u7684\u7269\u7406\u7279\u6027\u6765\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u968f\u673a\u6027\u3002", "method": "\u4f7f\u752896\u4e2aDDR4 DRAM\u82af\u7247\u8fdb\u884c\u5e7f\u6cdb\u8868\u5f81\uff0c\u901a\u8fc7\u540c\u65f6\u6fc0\u6d3b\u591a\u4e2aDRAM\u884c(2\u30014\u30018\u300116\u300132\u884c)\u6765\u751f\u6210\u968f\u673a\u6570\uff0c\u5206\u6790\u4e0d\u540c\u6570\u636e\u6a21\u5f0f\u3001\u6e29\u5ea6\u6c34\u5e73\u548c\u7a7a\u95f4\u53d8\u5316\u5bf9\u71b5\u7684\u5f71\u54cd\u3002", "result": "\u6240\u6709SiMRA-based TRNG\u8bbe\u8ba1\u90fd\u901a\u8fc7\u4e86NIST\u968f\u673a\u6027\u6d4b\u8bd5\uff1b\u4e0e\u73b0\u6709DRAM-based TRNG\u76f8\u6bd4\uff0c2\u30018\u300116\u300132\u884c\u6fc0\u6d3b\u7684\u541e\u5410\u91cf\u5206\u522b\u63d0\u9ad8\u4e861.15x\u30011.99x\u30011.82x\u548c1.39x\uff1b\u71b5\u968f\u6fc0\u6d3b\u884c\u6570\u589e\u52a0\u800c\u589e\u52a0\uff1b\u64cd\u4f5c\u53c2\u6570\u663e\u8457\u5f71\u54cd\u71b5\u3002", "conclusion": "SiMRA\u6280\u672f\u4e3a\u5728\u5546\u7528DRAM\u82af\u7247\u4e2d\u5b9e\u73b0\u9ad8\u6548\u771f\u968f\u673a\u6570\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5f00\u6e90\u4e86\u76f8\u5173\u57fa\u7840\u8bbe\u65bd\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2510.20495", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.20495", "abs": "https://arxiv.org/abs/2510.20495", "authors": ["Panagiotis Giannakopoulos", "Bart van Knippenberg", "Kishor Chandra Joshi", "Nicola Calabretta", "George Exarchakos"], "title": "Accurate Performance Predictors for Edge Computing Applications", "comment": null, "summary": "Accurate prediction of application performance is critical for enabling\neffective scheduling and resource management in resource-constrained dynamic\nedge environments. However, achieving predictable performance in such\nenvironments remains challenging due to the co-location of multiple\napplications and the node heterogeneity. To address this, we propose a\nmethodology that automatically builds and assesses various performance\npredictors. This approach prioritizes both accuracy and inference time to\nidentify the most efficient model. Our predictors achieve up to 90% accuracy\nwhile maintaining an inference time of less than 1% of the Round Trip Time.\nThese predictors are trained on the historical state of the most correlated\nmonitoring metrics to application performance and evaluated across multiple\nservers in dynamic co-location scenarios. As usecase we consider electron\nmicroscopy (EM) workflows, which have stringent real-time demands and diverse\nresource requirements. Our findings emphasize the need for a systematic\nmethodology that selects server-specific predictors by jointly optimizing\naccuracy and inference latency in dynamic co-location scenarios. Integrating\nsuch predictors into edge environments can improve resource utilization and\nresult in predictable performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u52a8\u6001\u8fb9\u7f18\u73af\u5883\u4e2d\u81ea\u52a8\u6784\u5efa\u548c\u8bc4\u4f30\u6027\u80fd\u9884\u6d4b\u5668\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u51c6\u786e\u6027\u548c\u63a8\u7406\u65f6\u95f4\u6765\u5b9e\u73b0\u53ef\u9884\u6d4b\u7684\u5e94\u7528\u6027\u80fd\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u52a8\u6001\u8fb9\u7f18\u73af\u5883\u4e2d\uff0c\u7531\u4e8e\u591a\u4e2a\u5e94\u7528\u5171\u5b58\u548c\u8282\u70b9\u5f02\u6784\u6027\uff0c\u5b9e\u73b0\u53ef\u9884\u6d4b\u7684\u6027\u80fd\u5177\u6709\u6311\u6218\u6027\uff0c\u8fd9\u5bf9\u6709\u6548\u7684\u8c03\u5ea6\u548c\u8d44\u6e90\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u81ea\u52a8\u6784\u5efa\u548c\u8bc4\u4f30\u5404\u79cd\u6027\u80fd\u9884\u6d4b\u5668\uff0c\u4f18\u5148\u8003\u8651\u51c6\u786e\u6027\u548c\u63a8\u7406\u65f6\u95f4\uff0c\u9009\u62e9\u6700\u6709\u6548\u6a21\u578b\u3002\u9884\u6d4b\u5668\u57fa\u4e8e\u4e0e\u5e94\u7528\u7a0b\u5e8f\u6027\u80fd\u6700\u76f8\u5173\u7684\u76d1\u63a7\u6307\u6807\u5386\u53f2\u72b6\u6001\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u52a8\u6001\u5171\u7f6e\u573a\u666f\u4e2d\u8bc4\u4f30\u3002", "result": "\u9884\u6d4b\u5668\u8fbe\u5230\u9ad8\u8fbe90%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u65f6\u95f4\u5c0f\u4e8e\u5f80\u8fd4\u65f6\u95f4\u76841%\u3002\u5728\u7535\u5b50\u663e\u5fae\u955c\u5de5\u4f5c\u6d41\u7684\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u9700\u8981\u7cfb\u7edf\u5316\u65b9\u6cd5\u5728\u52a8\u6001\u5171\u7f6e\u573a\u666f\u4e2d\u901a\u8fc7\u8054\u5408\u4f18\u5316\u51c6\u786e\u6027\u548c\u63a8\u7406\u5ef6\u8fdf\u6765\u9009\u62e9\u670d\u52a1\u5668\u7279\u5b9a\u7684\u9884\u6d4b\u5668\uff0c\u96c6\u6210\u6b64\u7c7b\u9884\u6d4b\u5668\u53ef\u4ee5\u6539\u5584\u8d44\u6e90\u5229\u7528\u7387\u548c\u5b9e\u73b0\u53ef\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2510.20018", "categories": ["cs.PL", "68N18 (Primary), 03B70 (Secondary)", "F.3.3; D.3.1"], "pdf": "https://arxiv.org/pdf/2510.20018", "abs": "https://arxiv.org/abs/2510.20018", "authors": ["Ryan Kavanagh", "Chuta Sano", "Brigitte Pientka"], "title": "Deconstructed Proto-Quipper: A Rational Reconstruction", "comment": "Submitted to the 35th European Symposium on Programming (ESOP 2026)", "summary": "The Proto-Quipper family of programming languages aims to provide a formal\nfoundation for the Quipper quantum programming language. Unfortunately,\nProto-Quipper languages have complex operational semantics: they are inherently\neffectful, and they rely on set-theoretic operations and fresh name generation\nto manipulate quantum circuits. This makes them difficult to reason about using\nstandard programming language techniques and, ultimately, to mechanize. We\nintroduce Proto-Quipper-A, a rational reconstruction of Proto-Quipper languages\nfor static circuit generation. It uses a linear $\\lambda$-calculus to describe\nquantum circuits with normal forms that closely correspond to box-and-wire\ncircuit diagrams. Adjoint-logical foundations integrate this circuit language\nwith a linear/non-linear functional language and let us reconstruct\nProto-Quipper's circuit programming abstractions using more primitive\nadjoint-logical operations. Proto-Quipper-A enjoys a simple call-by-value\nreduction semantics, and to illustrate its tractability as a foundation for\nProto-Quipper languages, we show that it is normalizing. We show how to use\nstandard logical relations to prove normalization of linear and substructural\nsystems, thereby avoiding the inherent complexity of existing linear logical\nrelations.", "AI": {"tldr": "\u63d0\u51fa\u4e86Proto-Quipper-A\uff0c\u8fd9\u662fProto-Quipper\u91cf\u5b50\u7f16\u7a0b\u8bed\u8a00\u5bb6\u65cf\u7684\u7406\u6027\u91cd\u6784\uff0c\u4f7f\u7528\u7ebf\u6027\u03bb\u6f14\u7b97\u548c\u4f34\u968f\u903b\u8f91\u57fa\u7840\u6765\u7b80\u5316\u91cf\u5b50\u7535\u8def\u7684\u9759\u6001\u751f\u6210\u548c\u63a8\u7406\u3002", "motivation": "Proto-Quipper\u8bed\u8a00\u5177\u6709\u590d\u6742\u7684\u64cd\u4f5c\u8bed\u4e49\uff0c\u4f9d\u8d56\u4e8e\u96c6\u5408\u8bba\u64cd\u4f5c\u548c\u65b0\u9c9c\u540d\u79f0\u751f\u6210\u6765\u64cd\u7eb5\u91cf\u5b50\u7535\u8def\uff0c\u8fd9\u4f7f\u5f97\u4f7f\u7528\u6807\u51c6\u7f16\u7a0b\u8bed\u8a00\u6280\u672f\u8fdb\u884c\u63a8\u7406\u548c\u673a\u68b0\u5316\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u03bb\u6f14\u7b97\u63cf\u8ff0\u91cf\u5b50\u7535\u8def\uff0c\u5176\u8303\u5f0f\u4e0e\u76d2\u7ebf\u7535\u8def\u56fe\u7d27\u5bc6\u5bf9\u5e94\u3002\u901a\u8fc7\u4f34\u968f\u903b\u8f91\u57fa\u7840\u5c06\u7535\u8def\u8bed\u8a00\u4e0e\u7ebf\u6027/\u975e\u7ebf\u6027\u51fd\u6570\u8bed\u8a00\u96c6\u6210\uff0c\u91cd\u6784Proto-Quipper\u7684\u7535\u8def\u7f16\u7a0b\u62bd\u8c61\u3002", "result": "Proto-Quipper-A\u5177\u6709\u7b80\u5355\u7684\u6309\u503c\u8c03\u7528\u5f52\u7ea6\u8bed\u4e49\uff0c\u5e76\u4e14\u88ab\u8bc1\u660e\u662f\u89c4\u8303\u5316\u7684\uff0c\u5c55\u793a\u4e86\u4f5c\u4e3aProto-Quipper\u8bed\u8a00\u57fa\u7840\u7684\u6613\u5904\u7406\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u73b0\u6709\u7ebf\u6027\u903b\u8f91\u5173\u7cfb\u7684\u56fa\u6709\u590d\u6742\u6027\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u6807\u51c6\u903b\u8f91\u5173\u7cfb\u8bc1\u660e\u7ebf\u6027\u548c\u5b50\u7ed3\u6784\u7cfb\u7edf\u7684\u89c4\u8303\u5316\u3002"}}
{"id": "2510.20400", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.20400", "abs": "https://arxiv.org/abs/2510.20400", "authors": ["Rub\u00e9n Langarita", "Jes\u00fas Alastruey-Bened\u00e9", "Pablo Ib\u00e1\u00f1ez-Mar\u00edn", "Santiago Marco-Sola", "Miquel Moret\u00f3", "Adri\u00e0 Armejach"], "title": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism on Dependency-Bound Kernels", "comment": "11 pages, 10 figures, 5 tables, 4 algorithms, accepted on PACT25", "summary": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline.", "AI": {"tldr": "Squire\u662f\u4e00\u79cd\u901a\u7528\u52a0\u901f\u5668\uff0c\u65e8\u5728\u6709\u6548\u5229\u7528\u4f9d\u8d56\u7ed1\u5b9a\u5185\u6838\u4e2d\u7684\u7ec6\u7c92\u5ea6\u5e76\u884c\u6027\uff0c\u901a\u8fc7\u4f4e\u529f\u8017\u6838\u5fc3\u76f4\u63a5\u901a\u4fe1\u548cL2\u7f13\u5b58\u8bbf\u95ee\uff0c\u5728\u52a8\u6001\u7f16\u7a0b\u5185\u6838\u4e2d\u5b9e\u73b0\u6700\u9ad87.64\u500d\u52a0\u901f\uff0c\u7aef\u5230\u7aef\u5e94\u7528\u52a0\u901f3.66\u500d\uff0c\u80fd\u8017\u964d\u4f4e56%\uff0c\u9762\u79ef\u5f00\u9500\u4ec510.5%\u3002", "motivation": "\u4f20\u7edf\u901a\u7528\u52a0\u901f\u5668\uff08\u5982SIMD\u3001GPGPU\uff09\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u4f9d\u8d56\u6a21\u5f0f\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u5b9a\u5236FPGA/ASIC\u8bbe\u8ba1\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u5904\u7406\u4f9d\u8d56\u7ed1\u5b9a\u5185\u6838\u7684\u901a\u7528\u52a0\u901f\u5668\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6bcf\u4e2aSquire\u52a0\u901f\u5668\u5305\u542b\u4e00\u7ec4\u4f4e\u529f\u8017\u987a\u5e8f\u6838\u5fc3\uff0c\u8fd9\u4e9b\u6838\u5fc3\u80fd\u5feb\u901f\u76f8\u4e92\u901a\u4fe1\u5e76\u76f4\u63a5\u8bbf\u95eeL2\u7f13\u5b58\u3002\u5728\u5178\u578b\u591a\u6838\u7cfb\u7edf\u4e2d\u4e3a\u6bcf\u4e2a\u6838\u5fc3\u96c6\u6210\u4e00\u4e2aSquire\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u6700\u5c0f\u8f6f\u4ef6\u4fee\u6539\u52a0\u901f\u5e76\u884c\u4efb\u52a1\u4e2d\u7684\u4f9d\u8d56\u7ed1\u5b9a\u5185\u6838\u3002", "result": "\u5728\u5b9e\u73b0\u590d\u6742\u4f9d\u8d56\u6a21\u5f0f\u7684\u4e94\u4e2a\u5185\u6838\u4e2d\uff0cSquire\u5728\u52a8\u6001\u7f16\u7a0b\u5185\u6838\u4e2d\u83b7\u5f97\u6700\u9ad87.64\u500d\u52a0\u901f\uff0c\u7aef\u5230\u7aef\u5e94\u7528\u52a0\u901f3.66\u500d\u3002\u80fd\u8017\u964d\u4f4e\u9ad8\u8fbe56%\uff0c\u4e0eNeoverse-N1\u57fa\u7ebf\u76f8\u6bd4\u9762\u79ef\u5f00\u9500\u4ec510.5%\u3002", "conclusion": "Squire\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5904\u7406\u4f9d\u8d56\u7ed1\u5b9a\u5185\u6838\u7684\u901a\u7528\u52a0\u901f\u5668\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u4f4e\u9762\u79ef\u5f00\u9500\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u80fd\u6548\uff0c\u4e3aHPC\u5e94\u7528\u4e2d\u7684\u8ba1\u7b97\u5bc6\u96c6\u578b\u5185\u6838\u63d0\u4f9b\u4e86\u5b9e\u7528\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2510.20506", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.20506", "abs": "https://arxiv.org/abs/2510.20506", "authors": ["Panagiotis Giannakopoulos", "Bart van Knippenberg", "Kishor Chandra Joshi", "Nicola Calabretta", "George Exarchakos"], "title": "Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing", "comment": null, "summary": "Distributed applications increasingly demand low end-to-end latency,\nespecially in edge and cloud environments where co-located workloads contend\nfor limited resources. Traditional load-balancing strategies are typically\nreactive and rely on outdated or coarse-grained metrics, often leading to\nsuboptimal routing decisions and increased tail latencies. This paper\ninvestigates the use of round-trip time (RTT) predictors to enhance request\nrouting by anticipating application latency. We develop lightweight and\naccurate RTT predictors that are trained on time-series monitoring data\ncollected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of\nhighly correlated monitoring metrics, our approach maintains low overhead while\nremaining adaptable to diverse co-location scenarios and heterogeneous\nhardware. The predictors achieve up to 95% accuracy while keeping the\nprediction delay within 10% of the application RTT. In addition, we identify\nthe minimum prediction accuracy threshold and key system-level factors required\nto ensure effective predictor deployment in resource-constrained clusters.\nSimulation-based evaluation demonstrates that performance-aware load balancing\ncan significantly reduce application RTT and minimize resource waste. These\nresults highlight the feasibility of integrating predictive load balancing into\nfuture production systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528RTT\u9884\u6d4b\u5668\u6765\u6539\u8fdb\u8d1f\u8f7d\u5747\u8861\uff0c\u901a\u8fc7\u9884\u6d4b\u5e94\u7528\u5ef6\u8fdf\u6765\u4f18\u5316\u8bf7\u6c42\u8def\u7531\u51b3\u7b56\uff0c\u5728Kubernetes\u7ba1\u7406\u7684GPU\u96c6\u7fa4\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe95%\u7684\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u5206\u5e03\u5f0f\u5e94\u7528\u5bf9\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f20\u7edf\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\u901a\u5e38\u57fa\u4e8e\u8fc7\u65f6\u6216\u7c97\u7c92\u5ea6\u7684\u6307\u6807\uff0c\u5bfc\u81f4\u6b21\u4f18\u7684\u8def\u7531\u51b3\u7b56\u548c\u5c3e\u90e8\u5ef6\u8fdf\u589e\u52a0\u3002", "method": "\u5f00\u53d1\u8f7b\u91cf\u7ea7\u4e14\u51c6\u786e\u7684RTT\u9884\u6d4b\u5668\uff0c\u57fa\u4e8e\u4eceKubernetes\u7ba1\u7406\u7684GPU\u96c6\u7fa4\u6536\u96c6\u7684\u65f6\u95f4\u5e8f\u5217\u76d1\u63a7\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5229\u7528\u9ad8\u5ea6\u76f8\u5173\u7684\u76d1\u63a7\u6307\u6807\u5b50\u96c6\u6765\u4fdd\u6301\u4f4e\u5f00\u9500\u3002", "result": "\u9884\u6d4b\u5668\u8fbe\u5230\u9ad8\u8fbe95%\u7684\u51c6\u786e\u7387\uff0c\u9884\u6d4b\u5ef6\u8fdf\u4fdd\u6301\u5728\u5e94\u7528RTT\u768410%\u4ee5\u5185\u3002\u4eff\u771f\u8bc4\u4f30\u663e\u793a\u6027\u80fd\u611f\u77e5\u8d1f\u8f7d\u5747\u8861\u80fd\u663e\u8457\u964d\u4f4e\u5e94\u7528RTT\u5e76\u6700\u5c0f\u5316\u8d44\u6e90\u6d6a\u8d39\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u663e\u4e86\u5c06\u9884\u6d4b\u6027\u8d1f\u8f7d\u5747\u8861\u96c6\u6210\u5230\u672a\u6765\u751f\u4ea7\u7cfb\u7edf\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u786e\u4fdd\u5728\u8d44\u6e90\u53d7\u9650\u96c6\u7fa4\u4e2d\u6709\u6548\u90e8\u7f72\u9884\u6d4b\u5668\u6240\u9700\u7684\u6700\u5c0f\u9884\u6d4b\u51c6\u786e\u7387\u9608\u503c\u548c\u5173\u952e\u7cfb\u7edf\u7ea7\u56e0\u7d20\u3002"}}
{"id": "2510.20532", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.20532", "abs": "https://arxiv.org/abs/2510.20532", "authors": ["Patrycja Balik", "Szymon J\u0119dras", "Piotr Polesiuk"], "title": "Deciding not to Decide: Sound and Complete Effect Inference in the Presence of Higher-Rank Polymorphism", "comment": null, "summary": "Type-and-effect systems help the programmer to organize data and\ncomputational effects in a program. While for traditional type systems\nexpressive variants with sophisticated inference algorithms have been developed\nand widely used in programming languages, type-and-effect systems did not yet\ngain widespread adoption. One reason for this is that type-and-effect systems\nare more complex and the existing inference algorithms make compromises between\nexpressiveness, intuitiveness, and decidability. In this work, we present an\neffect inference algorithm for a type-and-effect system with subtyping,\nexpressive higher-rank polymorphism, and intuitive set-like semantics of\neffects. In order to deal with scoping issues of higher-rank polymorphism, we\ndelay solving of effect constraints by transforming them into formulae of\npropositional logic. We prove soundness and completeness of our algorithm with\nrespect to a declarative type-and-effect system. All the presented results have\nbeen formalized in the Rocq proof assistant, and the algorithm has been\nsuccessfully implemented in a realistic programming language.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7c7b\u578b\u548c\u6548\u5e94\u7cfb\u7edf\u7684\u6548\u5e94\u63a8\u65ad\u7b97\u6cd5\uff0c\u8be5\u7cfb\u7edf\u5305\u542b\u5b50\u7c7b\u578b\u3001\u9ad8\u9636\u591a\u6001\u6027\u548c\u76f4\u89c2\u7684\u96c6\u5408\u5f0f\u6548\u5e94\u8bed\u4e49\u3002\u901a\u8fc7\u5c06\u6548\u5e94\u7ea6\u675f\u8f6c\u6362\u4e3a\u547d\u9898\u903b\u8f91\u516c\u5f0f\u6765\u5904\u7406\u9ad8\u9636\u591a\u6001\u6027\u7684\u4f5c\u7528\u57df\u95ee\u9898\u3002", "motivation": "\u7c7b\u578b\u548c\u6548\u5e94\u7cfb\u7edf\u5c1a\u672a\u83b7\u5f97\u5e7f\u6cdb\u5e94\u7528\uff0c\u56e0\u4e3a\u73b0\u6709\u63a8\u65ad\u7b97\u6cd5\u5728\u8868\u8fbe\u80fd\u529b\u3001\u76f4\u89c2\u6027\u548c\u53ef\u5224\u5b9a\u6027\u4e4b\u95f4\u505a\u51fa\u59a5\u534f\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u66f4\u590d\u6742\u4f46\u76f4\u89c2\u7684\u6548\u5e94\u63a8\u65ad\u7b97\u6cd5\u3002", "method": "\u5c06\u6548\u5e94\u7ea6\u675f\u8f6c\u6362\u4e3a\u547d\u9898\u903b\u8f91\u516c\u5f0f\uff0c\u5ef6\u8fdf\u6c42\u89e3\u4ee5\u5904\u7406\u9ad8\u9636\u591a\u6001\u6027\u7684\u4f5c\u7528\u57df\u95ee\u9898\u3002\u7b97\u6cd5\u57fa\u4e8e\u5177\u6709\u5b50\u7c7b\u578b\u548c\u9ad8\u9636\u591a\u6001\u6027\u7684\u7c7b\u578b\u548c\u6548\u5e94\u7cfb\u7edf\u3002", "result": "\u8bc1\u660e\u4e86\u7b97\u6cd5\u76f8\u5bf9\u4e8e\u58f0\u660e\u5f0f\u7c7b\u578b\u548c\u6548\u5e94\u7cfb\u7edf\u7684\u5065\u5168\u6027\u548c\u5b8c\u5907\u6027\u3002\u7ed3\u679c\u5df2\u5728Rocq\u8bc1\u660e\u52a9\u624b\u4e2d\u5f62\u5f0f\u5316\uff0c\u5e76\u5728\u5b9e\u9645\u7f16\u7a0b\u8bed\u8a00\u4e2d\u6210\u529f\u5b9e\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684\u6548\u5e94\u63a8\u65ad\u7b97\u6cd5\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u7c7b\u578b\u548c\u6548\u5e94\u7cfb\u7edf\uff0c\u540c\u65f6\u4fdd\u6301\u76f4\u89c2\u7684\u8bed\u4e49\uff0c\u4e3a\u7c7b\u578b\u548c\u6548\u5e94\u7cfb\u7edf\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.20547", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.20547", "abs": "https://arxiv.org/abs/2510.20547", "authors": ["Nikolaus Huber", "Susanne Graf", "Philipp R\u00fcmmer", "Wang Yi"], "title": "Compiling the Mimosa programming language to RTOS tasks", "comment": null, "summary": "This paper introduces a compilation scheme for programs written in the Mimosa\nprogramming language, which builds upon the MIMOS model of computation. Mimosa\ndescribes embedded systems software as a collection of time-triggered processes\nwhich communicate through FIFO queues. We formally describe an adaptation of\nthe Lustre compilation scheme to the semantics of Mimosa and show how the\ncoordination layer can be mapped to real-time operating system primitives.", "AI": {"tldr": "\u63d0\u51fa\u4e86Mimosa\u7f16\u7a0b\u8bed\u8a00\u7684\u7f16\u8bd1\u65b9\u6848\uff0c\u57fa\u4e8eMIMOS\u8ba1\u7b97\u6a21\u578b\uff0c\u5c06\u5d4c\u5165\u5f0f\u7cfb\u7edf\u8f6f\u4ef6\u63cf\u8ff0\u4e3a\u901a\u8fc7FIFO\u961f\u5217\u901a\u4fe1\u7684\u65f6\u95f4\u89e6\u53d1\u8fdb\u7a0b\u96c6\u5408", "motivation": "\u4e3aMimosa\u8bed\u8a00\u5f00\u53d1\u6b63\u5f0f\u7684\u7f16\u8bd1\u65b9\u6848\uff0c\u5c06\u534f\u8c03\u5c42\u6620\u5c04\u5230\u5b9e\u65f6\u64cd\u4f5c\u7cfb\u7edf\u539f\u8bed", "method": "\u57fa\u4e8eLustre\u7f16\u8bd1\u65b9\u6848\u7684\u9002\u914d\uff0c\u9488\u5bf9Mimosa\u8bed\u4e49\u8fdb\u884c\u5f62\u5f0f\u5316\u63cf\u8ff0", "result": "\u6210\u529f\u5f00\u53d1\u4e86Mimosa\u8bed\u8a00\u7684\u7f16\u8bd1\u65b9\u6848", "conclusion": "\u8be5\u7f16\u8bd1\u65b9\u6848\u80fd\u591f\u6709\u6548\u652f\u6301Mimosa\u8bed\u8a00\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5b9e\u73b0"}}
{"id": "2510.20688", "categories": ["cs.PL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.20688", "abs": "https://arxiv.org/abs/2510.20688", "authors": ["Oliver Braunsdorf", "Tim Lange", "Konrad Hohentanner", "Julian Horsch", "Johannes Kinder"], "title": "SafeFFI: Efficient Sanitization at the Boundary Between Safe and Unsafe Code in Rust and Mixed-Language Applications", "comment": null, "summary": "Unsafe Rust code is necessary for interoperability with C/C++ libraries and\nimplementing low-level data structures, but it can cause memory safety\nviolations in otherwise memory-safe Rust programs. Sanitizers can catch such\nmemory errors at runtime, but introduce many unnecessary checks even for memory\naccesses guaranteed safe by the Rust type system. We introduce SafeFFI, a\nsystem for optimizing memory safety instrumentation in Rust binaries such that\nchecks occur at the boundary between unsafe and safe code, handing over the\nenforcement of memory safety from the sanitizer to the Rust type system. Unlike\nprevious approaches, our design avoids expensive whole-program analysis and\nadds much less compile-time overhead (2.64x compared to over 8.83x). On a\ncollection of popular Rust crates and known vulnerable Rust code, SafeFFI\nachieves superior performance compared to state-of-the-art systems, reducing\nsanitizer checks by up to 98%, while maintaining correctness and flagging all\nspatial and temporal memory safety violations.", "AI": {"tldr": "SafeFFI\u662f\u4e00\u4e2a\u4f18\u5316Rust\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e2d\u5185\u5b58\u5b89\u5168\u68c0\u6d4b\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728unsafe\u548csafe\u4ee3\u7801\u8fb9\u754c\u5904\u8fdb\u884c\u68c0\u67e5\uff0c\u5c06\u5185\u5b58\u5b89\u5168\u6267\u884c\u4ecesanitizer\u8f6c\u79fb\u5230Rust\u7c7b\u578b\u7cfb\u7edf\uff0c\u663e\u8457\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u68c0\u67e5\u3002", "motivation": "Unsafe Rust\u4ee3\u7801\u5728\u4e0eC/C++\u5e93\u4e92\u64cd\u4f5c\u548c\u5b9e\u73b0\u5e95\u5c42\u6570\u636e\u7ed3\u6784\u65f6\u662f\u5fc5\u8981\u7684\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u5185\u5b58\u5b89\u5168\u8fdd\u89c4\u3002\u73b0\u6709\u7684sanitizer\u4f1a\u5f15\u5165\u8bb8\u591a\u4e0d\u5fc5\u8981\u7684\u68c0\u67e5\uff0c\u5373\u4f7f\u5bf9\u4e8eRust\u7c7b\u578b\u7cfb\u7edf\u4fdd\u8bc1\u5b89\u5168\u7684\u5185\u5b58\u8bbf\u95ee\u4e5f\u662f\u5982\u6b64\u3002", "method": "SafeFFI\u7cfb\u7edf\u4f18\u5316\u5185\u5b58\u5b89\u5168\u68c0\u6d4b\uff0c\u4f7f\u5f97\u68c0\u67e5\u53d1\u751f\u5728unsafe\u548csafe\u4ee3\u7801\u7684\u8fb9\u754c\u5904\uff0c\u5c06\u5185\u5b58\u5b89\u5168\u6267\u884c\u4ecesanitizer\u8f6c\u79fb\u5230Rust\u7c7b\u578b\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u6602\u8d35\u7684\u5168\u7a0b\u5e8f\u5206\u6790\uff0c\u7f16\u8bd1\u65f6\u5f00\u9500\u66f4\u5c0f\u3002", "result": "\u5728\u6d41\u884c\u7684Rust crate\u548c\u5df2\u77e5\u6613\u53d7\u653b\u51fb\u7684Rust\u4ee3\u7801\u4e0a\uff0cSafeFFI\u76f8\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u5b9e\u73b0\u4e86\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5c06sanitizer\u68c0\u67e5\u51cf\u5c11\u4e86\u9ad8\u8fbe98%\uff0c\u540c\u65f6\u4fdd\u6301\u6b63\u786e\u6027\u5e76\u6807\u8bb0\u6240\u6709\u7a7a\u95f4\u548c\u65f6\u95f4\u5185\u5b58\u5b89\u5168\u8fdd\u89c4\u3002", "conclusion": "SafeFFI\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u5b89\u5168\u68c0\u6d4b\uff0c\u5728unsafe\u548csafe\u4ee3\u7801\u8fb9\u754c\u5904\u8fdb\u884c\u68c0\u67e5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684sanitizer\u68c0\u67e5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5185\u5b58\u5b89\u5168\u4fdd\u8bc1\uff0c\u4e3aRust\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5185\u5b58\u5b89\u5168\u68c0\u6d4b\u65b9\u6848\u3002"}}
