<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 25]
- [cs.AR](#cs.AR) [Total: 7]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Efficient Cost Bounds with Linear Maps](https://arxiv.org/abs/2509.22982)
*David M Kahn,Jan Hoffmann,Thomas Reps,Jessie Grosen*

Main category: cs.PL

TL;DR: 提出了一种新的线性映射方法来表示函数的无成本类型，解决了现有AARA方法中无成本类型推断效率低下的问题，支持多项式和非多项式成本边界分析。


<details>
  <summary>Details</summary>
Motivation: 现有的自动摊销资源分析(AARA)方法在推断无成本类型时效率低下，存在递归依赖问题，且现有启发式方法仅适用于多项式成本边界，无法处理指数级等非多项式边界。

Method: 使用线性映射来表示函数的无成本类型，这种映射可以代表无限多个无成本类型。通过矩阵不等式进行代数推理，利用现成的线性规划工具求解成本边界问题。

Result: 原型实现实验表明，当适用时，线性映射推断比现有最先进算法效率呈指数级提升，能够高效检查和推断类型。

Conclusion: 线性映射方法为AARA提供了更高效的无成本类型表示和推断机制，支持更广泛的成本边界分析，包括非多项式边界。

Abstract: The Automatic Amortized Resource Analysis (AARA) derives program-execution
cost bounds using types. To do so, AARA often makes use of cost-free types,
which are critical for the composition of types and cost bounds. However,
inferring cost-free types using the current state-of-the-art algorithm is
expensive due to recursive dependence on additional cost-free types.
Furthermore, that algorithm uses a heuristic only applicable to polynomial cost
bounds, and not, e.g., exponential bounds. This paper presents a new approach
to these problems by representing the cost-free types of a function in a new
way: with a linear map, which can stand for infinitely many cost-free types.
Such maps enable an algebraic flavor of reasoning about cost bounds (including
non-polynomial bounds) via matrix inequalities. These inequalities can be
solved with off-the-shelf linear-programming tools for many programs, so that
types can always be efficiently checked and often be efficiently inferred. An
experimental evaluation with a prototype implementation shows that-when it is
applicable-the inference of linear maps is exponentially more efficient than
the state-of-the-art algorithm.

</details>


### [2] [Local Success Does Not Compose: Benchmarking Large Language Models for Compositional Formal Verification](https://arxiv.org/abs/2509.23061)
*Xu Xu,Xin Li,Xingwei Qu,Jie Fu,Binhang Yuan*

Main category: cs.PL

TL;DR: DafnyCOMP是一个评估LLM在Dafny组合式规范生成能力的基准，包含300个自动合成的多函数程序，发现LLM在组合任务上表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注单函数任务，缺乏评估LLM在具有数据依赖的多函数交互程序上的组合推理能力。

Method: 构建包含300个自动合成的多函数Dafny程序的基准，评估多个先进LLM在组合规范生成任务上的表现。

Result: LLM在单函数验证上表现良好，但在组合任务上性能急剧下降，存在跨函数推理的系统性失败。

Conclusion: DafnyCOMP为衡量LLM在可靠、可验证和组合代码生成方面的进展提供了诊断工具。

Abstract: We introduce DafnyCOMP, a benchmark for evaluating large language models
(LLMs) on compositional specification generation in Dafny. Unlike prior
benchmarks that focus on single-function tasks, DafnyCOMP targets programs
composed of multiple interacting functions with data dependencies, requiring
reasoning across component boundaries. The benchmark consists of 300
automatically synthesized multi-function programs. We evaluate several
state-of-the-art LLM families and find that, while they perform well on
single-function verification, their performance drops sharply on compositional
tasks. Analysis reveals systematic failures in cross-functional reasoning,
including fragile specifications, misalignment between implementations and
proofs, and unstable reasoning. DafnyCOMP thus provides a diagnostic tool for
measuring progress toward reliable, verifiable, and compositional code
generation with LLMs.

</details>


### [3] [Fine-Grained Reasoning About Container-Internal Pointers with Logical Pinning](https://arxiv.org/abs/2509.23229)
*Yawen Guan,Clément Pit-Claudel*

Main category: cs.PL

TL;DR: 提出逻辑固定(logical pinning)方法，允许在分离逻辑中有选择地跟踪容器内部指针，解决了传统分离逻辑隐藏内部指针导致的API规范和验证困难问题。


<details>
  <summary>Details</summary>
Motivation: 传统分离逻辑为了模块性隐藏容器内部指针，这使得难以规范那些临时暴露内部指针的容器API，也难以验证使用这些API的程序。

Method: 逻辑固定是一种轻量级的借用模型，扩展了魔法棒操作符，允许在逻辑层面有选择地跟踪容器内部指针，只需改变表示谓词和规范写法。

Result: 验证了小型但具有代表性的指针操作程序，推导出更精确的常见容器规范，证明该方法包含了一些知名证明模式，简化了复杂证明，支持传统规范无法处理的程序模式推理。

Conclusion: 逻辑固定方法实用且兼容大多数分离逻辑变体，在Rocq证明助手中使用CFML库实现了所有结果。

Abstract: Most separation logics hide container-internal pointers for modularity. This
makes it difficult to specify container APIs that temporarily expose those
pointers to the outside, and to verify programs that use these APIs. We present
logical pinning, a lightweight borrowing model for sequential programs that
allows users to selectively track container-internal pointers at the logical
level. Our model generalizes the magic-wand operator, making it easy to write
and prove precise specifications, including pointer-stability properties.
Because it only changes how representation predicates and specifications are
written, our approach is compatible with most separation logic variants. We
demonstrate the practicality of logical pinning by verifying small but
representative pointer-manipulating programs, and deriving more precise
versions of common container specifications. In doing so, we show that our
approach subsumes some well-known proof patterns, simplifies some complex
proofs, and enables reasoning about program patterns not supported by
traditional specifications. All of our results are mechanized in the Rocq proof
assistant, using the CFML library.

</details>


### [4] [From Affine to Polynomial: Synthesizing Loops with Branches via Algebraic Geometry](https://arxiv.org/abs/2509.25114)
*Erdenebayar Bayarmagnai,Fatemeh Mohammadi,Rémi Prébet*

Main category: cs.PL

TL;DR: 提出了一种从多项式不变量合成循环的新方法，允许循环具有多项式更新映射、不等式保护条件和任意形式的多项式不变量。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能从多项式不变量合成无保护条件的仿射循环，需要解决更一般的循环合成问题。

Method: 使用代数几何工具设计算法，将循环合成问题转化为求解有理数多元多项式系统，并用SMT求解器处理。

Result: 开发了能计算满足给定不变量的所有非确定性分支循环的算法，并对特定不变量类提出了更高效的算法。

Conclusion: 成功将循环合成问题简化为多项式系统求解，为软件正确性验证提供了更强大的工具。

Abstract: Ensuring software correctness remains a fundamental challenge in formal
program verification. One promising approach relies on finding polynomial
invariants for loops. Polynomial invariants are properties of a program loop
that hold before and after each iteration. Generating such invariants is a
crucial task in loop analysis, but it is undecidable in the general case.
Recently, an alternative approach to this problem has emerged, focusing on
synthesizing loops from invariants. However, existing methods only synthesize
affine loops without guard conditions from polynomial invariants. In this
paper, we address a more general problem, allowing loops to have polynomial
update maps with a given structure, inequations in the guard condition, and
polynomial invariants of arbitrary form.
  We use algebraic geometry tools to design and implement an algorithm that
computes a finite set of polynomial equations whose solutions correspond to all
nondeterministic branching loops satisfying the given invariants. Furthermore,
we introduce a new class of invariants for which we present a significantly
more efficient algorithm. In other words, we reduce the problem of synthesizing
loops to find solutions of multivariate polynomial systems with rational
entries. This final step is handled in our software using an SMT solver.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Analysis of the carbon footprint of HPC](https://arxiv.org/abs/2509.22679)
*Abdessalam Benhari,Yves Denneulin,Frédéric Desprez,Fanny Dufossé,Denis Trystram*

Main category: cs.DC

TL;DR: 本文研究了高性能计算系统的碳排放演变，通过分析Top500系统的能源结构，建立了预测未来5年HPC碳排放的模型。


<details>
  <summary>Details</summary>
Motivation: 高性能计算系统的计算能力持续增长，但能源成本和碳足迹也随之增加，需要研究其碳排放演变趋势。

Method: 考虑多个大规模系统的完整生命周期，结合Top500系统的能源结构分析，建立预测模型。

Result: 提出了一个预测未来5年HPC碳排放的模型，并分析了向2030年发展的轨迹。

Conclusion: 高性能计算系统的碳排放问题日益重要，需要综合考虑能源结构和生命周期来评估其环境影响。

Abstract: The demand in computing power has never stopped growing over the years.
Today, the performance of the most powerful systems exceeds the exascale.
Unfortunately, this growth also comes with ever-increasing energy costs,
leading to a high carbon footprint. This paper investigates the evolution of
high performance systems in terms of carbon emissions. A lot of studies focus
on Top500 (and Green500) as the tip of an iceberg to identify trends in the
domain in terms of computing performance. We propose here to go further in
considering the whole span life of several large scale systems and to link the
evolution with trajectory toward 2030. More precisely, we introduce the energy
mix in the analysis of Top500 systems and we derive a predictive model for
estimating the weight of HPC for the next 5 years.

</details>


### [6] [FLAME: A Serving System Optimized for Large-Scale Generative Recommendation with Efficiency](https://arxiv.org/abs/2509.22681)
*Xianwen Guo,Bin Huang,Xiaomeng Wu,Guanlin Wu,Fangjian Li,Shijia Wang,Qiang Xiao,Chuanjiang Luo,Yong Li*

Main category: cs.DC

TL;DR: FLAME是一个针对大规模生成式推荐模型的高效在线服务系统，通过CPU-GPU异构硬件、内存优化和动态流协调等技术，显著提升了系统吞吐量和延迟性能。


<details>
  <summary>Details</summary>
Motivation: 生成式推荐模型相比传统深度学习推荐模型具有更强的扩展能力，但计算负担增加了4个数量级，在保持毫秒级响应时间的同时处理数十亿请求对在线服务系统提出了极高要求。

Method: 采用CPU-GPU异构硬件解耦特征预处理和模型计算；使用Proximal Data Accelerator模块进行内存优化；基于NVIDIA TensorRT实现Fused Kernel Engine模块加速模型计算；设计Dynamic Stream Orchestrator模块协调并发请求。

Result: PDA模块实现1.9倍吞吐量增益和1.7倍延迟降低；FKE模块提供4.6x-6.1x加速比和4.7x-6.3x吞吐量增益；DSO模块在非均匀分布场景下实现1.3倍吞吐量提升和2.3倍加速。

Conclusion: FLAME系统有效支持大规模生成式推荐模型的在线部署，在系统性能方面取得了显著改进。

Abstract: Generative recommendation (GR) models possess greater scaling power compared
to traditional deep learning recommendation models (DLRMs), yet they also
impose a tremendous increase in computational burden. Measured in FLOPs, a
typical GR model's workload sits in $10^9 \sim 10^{11}$ range, roughly four
orders of magnitude higher than traditional DLRMs. Delivering accurate results
in a few tens of milliseconds while processing billions of such requests per
day puts extreme demands on the performance of the online serving system.
Therefore, for industry practitioners, the alluring gains of GR models are
tempered by the formidable challenge of online deployment at scale in
production services. In this work, we introduce a comprehensive solution of
online serving system tailored For Large-scale GenerAtive RecoMmendation with
Efficiency (FLAME). Specifically, we leveraging CPU-GPU heterogeneous hardware
to decouple feature pre-processing and model computation. We encapsulated
several memory optimization features as the Proximal Data Accelerator (PDA)
module to make full use of limited bandwidth and storage resources, which
achieves a 1.9x throughput gain and a 1.7x latency reduction. We implement the
Fused Kernel Engine (FKE) module based on the functionality and interface of
NVIDIA TensorRT to boost model computation, delivering a speedup ratio of
4.6x-6.1x, throughput gain ratio of 4.7x-6.3x one step further. In addition, we
design the Dynamic Stream Orchestrator (DSO) module to coordinate concurrent
requests, enhancing the system throughput performance with 1.3x improvement in
throughput and 2.3x speed-up under non-uniform distribution of upstream
candidates. Comprehensive evaluations demonstrate that our FLAME effectively
supports large-scale online deployment of GR models and achieves remarkable
improvements in system performance.

</details>


### [7] [ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs](https://arxiv.org/abs/2509.22684)
*Tarunesh Verma,Yichao Yuan,Nishil Talati,Todd Austin*

Main category: cs.DC

TL;DR: ZKProphet：对GPU上零知识证明性能的全面研究，发现NTT内核成为主要瓶颈，并提出通过参数调优和架构优化来提升性能的路线图


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对零知识证明在GPU架构上执行瓶颈和可扩展性的系统表征，特别是在MSM大幅加速后，NTT等内核成为新的性能瓶颈

Method: 通过系统性能分析，识别ZKPs在GPU上的执行瓶颈，分析NTT等内核的资源利用不足问题，并提出运行时参数调优、预计算输入和替代数据表示等优化方法

Result: 发现NTT内核占用GPU证明生成延迟的90%，现有实现未充分利用GPU计算资源，算术操作仅限于32位整数流水线且指令级并行性有限

Conclusion: 为ZKP社区提供了在GPU上扩展性能的路线图，通过参数调优和架构优化可以构建针对特定应用需求和硬件资源的GPU加速ZKP

Abstract: Zero-Knowledge Proofs (ZKP) are protocols which construct cryptographic
proofs to demonstrate knowledge of a secret input in a computation without
revealing any information about the secret. ZKPs enable novel applications in
private and verifiable computing such as anonymized cryptocurrencies and
blockchain scaling and have seen adoption in several real-world systems. Prior
work has accelerated ZKPs on GPUs by leveraging the inherent parallelism in
core computation kernels like Multi-Scalar Multiplication (MSM). However, we
find that a systematic characterization of execution bottlenecks in ZKPs, as
well as their scalability on modern GPU architectures, is missing in the
literature. This paper presents ZKProphet, a comprehensive performance study of
Zero-Knowledge Proofs on GPUs. Following massive speedups of MSM, we find that
ZKPs are bottlenecked by kernels like Number-Theoretic Transform (NTT), as they
account for up to 90% of the proof generation latency on GPUs when paired with
optimized MSM implementations. Available NTT implementations under-utilize GPU
compute resources and often do not employ architectural features like
asynchronous compute and memory operations. We observe that the arithmetic
operations underlying ZKPs execute exclusively on the GPU's 32-bit integer
pipeline and exhibit limited instruction-level parallelism due to data
dependencies. Their performance is thus limited by the available integer
compute units. While one way to scale the performance of ZKPs is adding more
compute units, we discuss how runtime parameter tuning for optimizations like
precomputed inputs and alternative data representations can extract additional
speedup. With this work, we provide the ZKP community a roadmap to scale
performance on GPUs and construct definitive GPU-accelerated ZKPs for their
application requirements and available hardware resources.

</details>


### [8] [Enhancing Cluster Scheduling in HPC: A Continuous Transfer Learning for Real-Time Optimization](https://arxiv.org/abs/2509.22701)
*Leszek Sliwko,Jolanta Mizera-Pietraszko*

Main category: cs.DC

TL;DR: 提出基于机器学习的集群任务调度优化方法，通过持续迁移学习动态适应节点亲和性约束，在Google集群数据上实现99%以上准确率，降低计算开销并改善调度延迟。


<details>
  <summary>Details</summary>
Motivation: 传统调度器（如Kubernetes）在实时适应性方面存在不足，无法有效处理节点亲和性约束，需要一种能够动态演化的调度优化方案。

Method: 采用持续迁移学习模型，在操作过程中动态演化，最小化重新训练需求，专注于节点亲和性约束的优化。

Result: 在Google集群数据上的评估显示，模型达到超过99%的准确率，显著降低了计算开销，改善了受限任务的调度延迟。

Conclusion: 该可扩展解决方案实现了实时优化，推动了机器学习在集群管理中的集成，为未来自适应调度策略奠定了基础。

Abstract: This study presents a machine learning-assisted approach to optimize task
scheduling in cluster systems, focusing on node-affinity constraints.
Traditional schedulers like Kubernetes struggle with real-time adaptability,
whereas the proposed continuous transfer learning model evolves dynamically
during operations, minimizing retraining needs. Evaluated on Google Cluster
Data, the model achieves over 99% accuracy, reducing computational overhead and
improving scheduling latency for constrained tasks. This scalable solution
enables real-time optimization, advancing machine learning integration in
cluster management and paving the way for future adaptive scheduling
strategies.

</details>


### [9] [Intelligent Load Balancing in Cloud Computer Systems](https://arxiv.org/abs/2509.22704)
*Leszek Sliwko*

Main category: cs.DC

TL;DR: 该研究提出了一种动态任务分配策略，以防止云节点过载并维持系统稳定性，同时最小化成本。研究包括调度器分类、云资源利用模型、虚拟机迁移网络流量估算、高保真云工作负载模拟器以及两种资源管理方法。


<details>
  <summary>Details</summary>
Motivation: 云计算系统连接大量服务器处理相关任务，虽然比单机更具成本效益，但系统规模庞大可能导致节点过载。研究旨在设计动态任务分配策略，在维持系统稳定性的同时最小化成本。

Method: 1) 提出三类调度器（OS级、集群和大数据）的分类法；2) 建立包含多种资源和任务迁移成本的云资源利用抽象模型；3) 通过虚拟机实时迁移实验估算网络流量；4) 基于Google计算单元一个月工作负载痕迹创建高保真云工作负载模拟器；5) 提出并测试集中式元启发式负载均衡器和分散式基于代理系统两种资源管理方法。

Result: 在威斯敏斯特大学HPC集群上进行了广泛实验，获得了有前景的结果，表明所提出的方法能够有效管理云资源并维持系统稳定性。

Conclusion: 该研究成功开发了动态任务分配策略，通过分类调度器、建立资源模型、估算迁移成本、创建模拟器以及测试两种管理方法，为云计算资源管理提供了有效的解决方案，能够在维持系统稳定性的同时最小化成本。

Abstract: Cloud computing is an established technology allowing users to share
resources on a large scale, never before seen in IT history. A cloud system
connects multiple individual servers in order to process related tasks in
several environments at the same time. Clouds are typically more cost-effective
than single computers of comparable computing performance. The sheer physical
size of the system itself means that thousands of machines may be involved. The
focus of this research was to design a strategy to dynamically allocate tasks
without overloading Cloud nodes which would result in system stability being
maintained at minimum cost. This research has added the following new
contributions to the state of knowledge: (i) a novel taxonomy and
categorisation of three classes of schedulers, namely OS-level, Cluster and Big
Data, which highlight their unique evolution and underline their different
objectives; (ii) an abstract model of cloud resources utilisation is specified,
including multiple types of resources and consideration of task migration
costs; (iii) a virtual machine live migration was experimented with in order to
create a formula which estimates the network traffic generated by this process;
(iv) a high-fidelity Cloud workload simulator, based on a month-long workload
traces from Google's computing cells, was created; (v) two possible approaches
to resource management were proposed and examined in the practical part of the
manuscript: the centralised metaheuristic load balancer and the decentralised
agent-based system. The project involved extensive experiments run on the
University of Westminster HPC cluster, and the promising results are presented
together with detailed discussions and a conclusion.

</details>


### [10] [Metadata-Guided Adaptable Frequency Scaling across Heterogeneous Applications and Devices](https://arxiv.org/abs/2509.22707)
*Jinqi Yan,Fang He,Qianlong Sang,Bifeng Tong,Peng Sun,Yili Gong,Chuang Hu,Dazhao Cheng*

Main category: cs.DC

TL;DR: MetaDVFS是一个基于元数据的动态电压频率缩放框架，通过多任务强化学习方法解决异构设备和应用的DVFS问题，显著提升性能功耗比和用户体验质量。


<details>
  <summary>Details</summary>
Motivation: 传统启发式DVFS调控器难以应对异构SoC设计和多样化应用负载的复杂性，而现有强化学习方法泛化能力差且部署成本高。

Method: 将DVFS建模为多任务强化学习问题，利用设备和应用元数据来发现和迁移共享知识，输出具有强泛化能力的DVFS模型集。

Result: 在5款Google Pixel设备和6个应用上的评估显示，MetaDVFS在性能功耗比上提升达17%，用户体验质量提升达26%，适配速度比现有方法快70.8%。

Conclusion: MetaDVFS为异构移动环境中的DVFS部署提供了有效且可扩展的解决方案，避免了负迁移效应。

Abstract: Dynamic Voltage and Frequency Scaling is essential for enhancing energy
efficiency in mobile platforms. However, traditional heuristic-based governors
are increasingly inadequate for managing the complexity of heterogeneous
System-on-Chip designs and diverse application workloads. Although
reinforcement learning approaches offer improved performance, their poor
generalization capability and reliance on extensive retraining for each
hardware and application combination leads to significant deployment costs. In
this work, we observe that device and application metadata inherently
encapsulate valuable knowledge for DVFS, presenting an opportunity to overcome
these limitations. We formulate DVFS for heterogeneous devices and applications
as a multi-task reinforcement learning problem. We introduce MetaDVFS, which is
a metadata-guided framework that systematically leverages metadata to discover
and transfer shared knowledge across DVFS tasks. MetaDVFS can output a set of
DVFS models with significant generalization capability for various applications
of heterogeneous devices. Evaluations on five Google Pixel devices running six
applications show that MetaDVFS achieves up to 17% improvement in
Performance-Power Ratio and up to 26% improvement in Quality of Experience.
Compared to state-of-the-art methods, MetaDVFS delivers 70.8% faster adaptation
and 5.8-27.6% higher performance over standalone device-application specific
training, while avoiding negative transfer effects. These results establish
MetaDVFS as an effective and scalable solution for DVFS deployment in
heterogeneous mobile environments.

</details>


### [11] [Efficient Fine-Grained GPU Performance Modeling for Distributed Deep Learning of LLM](https://arxiv.org/abs/2509.22832)
*Biyao Zhang,Mingkai Zheng,Debargha Ganguly,Xuecen Zhang,Vikash Singh,Vipin Chaudhary,Zhao Zhang*

Main category: cs.DC

TL;DR: 提出了一种预测大型语言模型训练时间的框架，通过分解LLM为计算原语，结合轻量级采样和硬件感知预测，在CPU上实现准确预测，避免昂贵的集群实验。


<details>
  <summary>Details</summary>
Motivation: 预测多GPU分布式训练数十亿参数LLM的端到端训练时间具有挑战性，现有学习方法成本高，分析方法难以处理现实网络和硬件复杂性。

Method: 将LLM分解为核心计算原语，采用算子级分解进行细粒度分析，基于轻量级采样的硬件感知预测模型，集成端到端预测系统支持复杂并行策略。

Result: 在两个大规模HPC系统上验证，Perlmutter(A100)平均预测误差4.98%，Vista(GH200)平均预测误差9.38%，支持20B参数模型在128 GPU上的预测。

Conclusion: 该框架在CPU上运行，能够快速迭代硬件配置和训练策略，无需昂贵的集群实验，为LLM训练时间预测提供了有效解决方案。

Abstract: Training Large Language Models(LLMs) is one of the most compute-intensive
tasks in high-performance computing. Predicting end-to-end training time for
multi-billion parameter models distributed across hundreds of GPUs remains
challenging due to complex interactions between transformer components,
parallelism strategies(data, model, pipeline, tensor), and multi-tier
communication. Learned models require costly sampling, while analytical models
often struggle with real-world network and hardware complexities. We address
this by decomposing LLMs into core computational primitives and modeling them
with: (1) operator-level decomposition for fine-grained analysis; (2)
lightweight sampling based hardware-aware prediction models for key operations;
(3) an end-to-end prediction system integrating these components across complex
parallelization strategies. Crucially, our methodology has been validated on
two large-scale HPC systems. Our framework achieves low average prediction
errors-4.98\% on Perlmutter(A100) and 9.38\% on Vista(GH200)-for models up to
20B parameters across 128 GPUs. Importantly, it runs entirely on CPUs, enabling
rapid iteration over hardware configurations and training strategies without
costly on-cluster experimentation.

</details>


### [12] [OptimES: Optimizing Federated Learning Using Remote Embeddings for Graph Neural Networks](https://arxiv.org/abs/2509.22922)
*Pranjal Naman,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 提出了一种优化的联邦图神经网络训练框架OptimES，通过远程邻域剪枝、嵌入推送与本地训练重叠、动态嵌入拉取等策略，在保持隐私的同时显著减少通信成本和训练时间。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图数据通常分散在不同数据所有者手中，由于隐私问题无法集中。现有联邦GNN方法虽然保护隐私，但通信成本高导致性能下降。

Method: 采用远程邻域剪枝、将嵌入推送到服务器与本地训练重叠、动态拉取嵌入等优化策略来减少网络成本和训练时间。

Result: 在四个图数据集上的评估显示，对于大型密集图（如Reddit和Products），训练速度比EmbC快约3.5倍，准确率比默认联邦GNN提高约16%；对于稀疏图（如Arxiv和Papers），达到目标准确率的速度比EmbC快约11倍。

Conclusion: OptimES框架通过优化策略有效解决了联邦GNN训练中的通信瓶颈问题，在保持隐私的同时显著提升了训练效率和模型性能。

Abstract: Graph Neural Networks (GNNs) have experienced rapid advancements in recent
years due to their ability to learn meaningful representations from graph data
structures. However, in most real-world settings, such as financial transaction
networks and healthcare networks, this data is localized to different data
owners and cannot be aggregated due to privacy concerns. Federated Learning
(FL) has emerged as a viable machine learning approach for training a shared
model that iteratively aggregates local models trained on decentralized data.
This addresses privacy concerns while leveraging parallelism. State-of-the-art
methods enhance the privacy-respecting convergence accuracy of federated GNN
training by sharing remote embeddings of boundary vertices through a server
(EmbC). However, they are limited by diminished performance due to large
communication costs. In this article, we propose OptimES, an optimized
federated GNN training framework that employs remote neighbourhood pruning,
overlapping the push of embeddings to the server with local training, and
dynamic pulling of embeddings to reduce network costs and training time. We
perform a rigorous evaluation of these strategies for four common graph
datasets with up to $111M$ vertices and $1.8B$ edges. We see that a modest drop
in per-round accuracy due to the preemptive push of embeddings is out-stripped
by the reduction in per-round training time for large and dense graphs like
Reddit and Products, converging up to $\approx 3.5\times$ faster than EmbC and
giving up to $\approx16\%$ better accuracy than the default federated GNN
learning. While accuracy improvements over default federated GNNs are modest
for sparser graphs like Arxiv and Papers, they achieve the target accuracy
about $\approx11\times$ faster than EmbC.

</details>


### [13] [Characterizing FaaS Workflows on Public Clouds: The Good, the Bad and the Ugly](https://arxiv.org/abs/2509.23013)
*Varad Kulkarni,Nikhil Reddy,Tuhin Khare,Abhinandan S. Prasad,Chitra Babu,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 对AWS和Azure的三种流行FaaS工作流平台进行大规模评估，通过13.2万次调用运行25个微基准和应用工作流，揭示了函数执行、工作流编排、冷启动扩展和成本方面的独特见解。


<details>
  <summary>Details</summary>
Motivation: FaaS工作流平台（如AWS Step Functions和Azure Durable Functions）的复杂函数交互和专有平台内部可见性有限，阻碍了对这些平台的深入理解。现有研究缺乏对FaaS工作流平台的系统性和严谨研究。

Method: 对AWS和Azure的三种流行FaaS工作流平台进行广泛评估，运行25个微基准和应用工作流，共进行132,000次调用，详细分析函数执行、工作流编排等各个方面。

Result: 研究既证实了一些传统观点，也发现了关于函数执行、工作流编排、函数间交互、冷启动扩展和货币成本的独特见解。

Conclusion: 研究结果帮助开发者更好地配置和编程这些平台，设定性能和可扩展性预期，并识别增强平台的研究空白。

Abstract: Function-as-a-service (FaaS) is a popular serverless computing paradigm for
developing event-driven functions that elastically scale on public clouds. FaaS
workflows, such as AWS Step Functions and Azure Durable Functions, are composed
from FaaS functions, like AWS Lambda and Azure Functions, to build practical
applications. But, the complex interactions between functions in the workflow
and the limited visibility into the internals of proprietary FaaS platforms are
major impediments to gaining a deeper understanding of FaaS workflow platforms.
While several works characterize FaaS platforms to derive such insights, there
is a lack of a principled and rigorous study for FaaS workflow platforms, which
have unique scaling, performance and costing behavior influenced by the
platform design, dataflow and workloads. In this article, we perform extensive
evaluations of three popular FaaS workflow platforms from AWS and Azure,
running 25 micro-benchmark and application workflows over 132k invocations. Our
detailed analysis confirms some conventional wisdom but also uncovers unique
insights on the function execution, workflow orchestration, inter-function
interactions, cold-start scaling and monetary costs. Our observations help
developers better configure and program these platforms, set performance and
scalability expectations, and identify research gaps on enhancing the
platforms.

</details>


### [14] [Memory Efficient and Staleness Free Pipeline Parallel DNN Training Framework with Improved Convergence Speed](https://arxiv.org/abs/2509.23241)
*Ankita Dutta,Nabendu Chaki,Rajat K. De*

Main category: cs.DC

TL;DR: 本文提出了两种基于流水线并行的DNN训练框架：V-TiMePReSt（完全无陈旧系统）和I-TiMePReSt（陈旧感知系统），旨在解决多GPU训练中的资源需求和收敛效率问题。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在多GPU训练中的高资源需求，开发更高效的并行训练技术，平衡GPU内存消耗与训练收敛速度。

Method: V-TiMePReSt采用完全无陈旧设计，在所有前向和后向传播阶段使用最新权重；I-TiMePReSt是陈旧感知系统，基于陈旧程度数学量化陈旧权重的重要性，计算中间权重进行后向传播。

Result: 实验表明V-TiMePReSt在权重陈旧程度和GPU内存效率方面优于现有模型；I-TiMePReSt在消除权重陈旧性同时保持GPU内存消耗与收敛速度之间的平衡方面表现更优。

Conclusion: 两种框架各有优势：V-TiMePReSt适合追求最新权重和内存效率的场景，I-TiMePReSt在平衡内存消耗和收敛速度方面表现更好，为DNN并行训练提供了新的解决方案。

Abstract: High resource requirement for Deep Neural Network (DNN) training across
multiple GPUs necessitates development of various parallelism techniques. In
this paper, we introduce two interconnected DNN training frameworks, namely,
V-TiMePReSt and I-TiMePReSt, based on pipeline parallelism, a variant of model
parallelism. V-TiMePReSt is a completely staleness-free system which enables
the DNNs to be trained on the latest updated weights in each stage of all
forward and backward passes. Developing staleness-aware systems at the expense
of weight stashing reduces GPU-memory consumption, however, increases the
number of epochs to converge. Thus, we introduce I-TiMePReSt, which is also a
staleness-aware system, but not at the expense of weight stashing. It does not
rely solely on the stale weights or the latest updated weights. I-TiMePReSt
computes an intermediate weight towards the latter and performs backward pass
on it. Additionally, we formulate the significance of the stale weights
mathematically depending on the degree of staleness. In contrast to
V-TiMePReSt, I-TiMePReSt works based on the assumption that stale weights have
a significant contribution in training, which can be quantified mathematically
based on the degree of staleness, although there are other contributory factors
which should not be ignored. Experimental results show that V-TiMePReSt is
advantageous over existing models in terms of $1)$ the extent of staleness of
the weight parameter values and $2)$ GPU memory efficiency, while I-TiMePReSt
is superior in terms of $1)$ removing staleness of the weight parameters
without removing weight stashing and $2)$ maintaining the trade-off between GPU
memory consumption and convergence speed (number of epochs).

</details>


### [15] [Scaling LLM Test-Time Compute with Mobile NPU on Smartphones](https://arxiv.org/abs/2509.23324)
*Zixu Hao,Jianyu Wei,Tuowei Wang,Minxing Huang,Huiqiang Jiang,Shiqi Jiang,Ting Cao,Ju Ren*

Main category: cs.DC

TL;DR: 在移动设备上部署大型语言模型时，通过利用NPU未充分利用的计算资源，应用并行测试时缩放技术提升小模型性能，实现了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: 移动设备部署LLM面临小模型性能不足和大模型资源消耗过大的挑战，同时发现移动NPU在LLM推理中存在计算资源未充分利用的问题。

Method: 提出硬件感知的瓦片量化方案，将组量化与NPU内存访问模式对齐；使用基于查找表的高效替代方案处理复杂操作；设计端到端推理系统在骁龙平台上支持测试时缩放。

Result: 实现了显著加速：混合精度GEMM最高加速19.0倍，Softmax加速2.2倍；小模型使用测试时缩放后能够达到或超过大模型的准确度。

Conclusion: 该方法在移动NPU上成功应用测试时缩放技术，为小模型性能提升开辟了新途径，建立了新的性能-成本帕累托前沿。

Abstract: Deploying Large Language Models (LLMs) on mobile devices faces the challenge
of insufficient performance in smaller models and excessive resource
consumption in larger ones. This paper highlights that mobile Neural Processing
Units (NPUs) have underutilized computational resources, particularly their
matrix multiplication units, during typical LLM inference. To leverage this
wasted compute capacity, we propose applying parallel test-time scaling
techniques on mobile NPUs to enhance the performance of smaller LLMs. However,
this approach confronts inherent NPU challenges, including inadequate hardware
support for fine-grained quantization and low efficiency in general-purpose
computations. To overcome these, we introduce two key techniques: a
hardware-aware tile quantization scheme that aligns group quantization with NPU
memory access patterns, and efficient LUT-based replacements for complex
operations such as Softmax and dequantization. We design and implement an
end-to-end inference system that leverages the NPU's compute capability to
support test-time scaling on Qualcomm Snapdragon platforms. Experiments show
our approach brings significant speedups: up to 19.0 for mixed-precision GEMM
and 2.2 for Softmax. More importantly, we demonstrate that smaller models using
test-time scaling can match or exceed the accuracy of larger models, achieving
a new performance-cost Pareto frontier.

</details>


### [16] [A Predictive and Synergistic Two-Layer Scheduling Framework for LLM Serving](https://arxiv.org/abs/2509.23384)
*Yue Zhang,Yuansheng Chen,Xuan Mo,Alex Xi,Jialun Li,WeiGang Wu*

Main category: cs.DC

TL;DR: SynergySched是一个跨层调度框架，通过预测性编排解决LLM推理服务中的两层架构效率问题，提高SLO达成率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统LLM推理服务的两层架构存在信息鸿沟：集群层路由器依赖滞后指标导致决策延迟，引擎层静态调度策略无法有效处理动态工作负载，造成SLO违规和资源浪费。

Method: 提出基于结构感知的在线性能模型，提供准确的逐步延迟和容量预测。引擎层LENS组件实现SLO感知的自适应调度，集群层PRISM组件使用预测信号进行状态驱动路由。

Result: 在长上下文和异构场景中，SLO达成率平均提高43%，吞吐量最高提升3倍。在FlowGPT生产环境中验证了其优势。

Conclusion: SynergySched通过跨层预测性编排，有效解决了LLM推理服务中的系统效率问题，显著提升了性能和资源利用率。

Abstract: LLM inference serving typically scales out with a two-tier architecture: a
cluster router distributes requests to multiple inference engines, each of
which then in turn performs its own internal scheduling. However, this commonly
used paradigm suffers from critical, systemic inefficiency caused by the
information gaps across two layers. At the cluster-layer, the router mainly
relies on lagging, coarse-grained metrics, such as average latency and queue
length to make decisions, resulting in "decision lag" that leads to suboptimal
request routing. At the engine-layer, static heuristic scheduling policies
cannot effectively handle the dynamic workloads, leading a poor balance between
latency and throughput. Besides, these gaps may cause SLO violations and
resource waste, especially in heterogeneous cloud environments.
  To bridge such gaps, we propose SynergySched, a cross-layer framework that
shifts LLM serving system from reactive load balancing to predictive
orchestration. The core of SynergySched lies in a structurally-informed online
performance model that provides accurate, forward-looking per-step latency and
capacity estimations. This model empowers two key components. At the
engine-layer, LENS performs SLO-aware, adaptive scheduling, dynamically
optimizing batching to meet SLOs under real-time loads. At the cluster-layer,
PRISM uses predictive signals to perform state-driven routing, maximizing
cluster-wide performance and SLO attainment. Performance evaluations show that
SynergySched improves SLO attainment by 43% on average and achieves up to 3x
throughput speedup in long-context and heterogeneous scenarios. Besides, we
also deploy SynergySched on FlowGPT's clusters to demonstrate its advantages in
production environment.

</details>


### [17] [Enhancing Communication Efficiency in FL with Adaptive Gradient Quantization and Communication Frequency Optimization](https://arxiv.org/abs/2509.23419)
*Asadullah Tariq,Tariq Qayyum,Mohamed Adel Serhani,Farag Sallabi,Ikbal Taleb,Ezedin S. Barka*

Main category: cs.DC

TL;DR: 提出了一种三阶段策略来优化联邦学习的通信效率：自适应特征消除、自适应梯度量化和通信频率优化，在保持准确性的同时显著降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临的主要瓶颈是设备与服务器之间频繁模型更新带来的高通信开销，这在资源受限的无线网络中限制了部署。

Method: 1. 自适应特征消除策略：丢弃不重要特征，保留高价值特征；2. 自适应梯度创新和误差敏感量化：动态调整量化级别进行梯度压缩；3. 通信频率优化：提高通信效率。

Result: 通过广泛实验评估，与基线技术相比，该模型在保持准确性的同时实现了高通信效率。

Conclusion: 所提出的三阶段策略有效解决了联邦学习的通信瓶颈问题，在保持模型性能的同时显著提升了通信效率。

Abstract: Federated Learning (FL) enables participant devices to collaboratively train
deep learning models without sharing their data with the server or other
devices, effectively addressing data privacy and computational concerns.
However, FL faces a major bottleneck due to high communication overhead from
frequent model updates between devices and the server, limiting deployment in
resource-constrained wireless networks. In this paper, we propose a three-fold
strategy. Firstly, an Adaptive Feature-Elimination Strategy to drop less
important features while retaining high-value ones; secondly, Adaptive Gradient
Innovation and Error Sensitivity-Based Quantization, which dynamically adjusts
the quantization level for innovative gradient compression; and thirdly,
Communication Frequency Optimization to enhance communication efficiency. We
evaluated our proposed model's performance through extensive experiments,
assessing accuracy, loss, and convergence compared to baseline techniques. The
results show that our model achieves high communication efficiency in the
framework while maintaining accuracy.

</details>


### [18] [Lyte Quorum: Off-Chain Ready Smart Contract Hosted with Choice](https://arxiv.org/abs/2509.23448)
*Hao Hao,Dahlia Malkhi,Maofan Yin,Lizan Zhou*

Main category: cs.DC

TL;DR: Lyquor是一个去中心化平台，通过服务中心模型重新构想区块链基础设施，节点选择性托管智能合约（称为Lyquids）同时保持全局可组合性。


<details>
  <summary>Details</summary>
Motivation: 解决现有区块链系统的关键限制，特别是状态访问瓶颈和可扩展性问题，同时保持与以太坊API的兼容性。

Method: 提出三个关键创新：Fate-Constrained Ordering（FCO）将共识与执行解耦；Direct Memory Architecture（DMA）消除状态访问瓶颈；Universal Procedure Call（UPC）实现跨分布式链下计算的容错可编程协调。

Result: 构建了一个支持传统智能合约模式和新型分布式应用的统一编程模型，其中链上和链下逻辑无缝共存。

Conclusion: Lyquor为真正可扩展的去中心化计算提供了一条路径，解决了现有系统的关键限制，同时保持兼容性。

Abstract: This paper introduces Lyquor, a decentralized platform that reimagines
blockchain infrastructure through a service-centric model where nodes
selectively host smart contracts (called Lyquids) while preserving global
composability. We present three key innovations: (1) Fate-Constrained Ordering
(FCO), which decouples consensus from execution to enable selective hosting
without sacrificing Layer-1 grade composability; (2) Direct Memory Architecture
(DMA), which eliminates state access bottlenecks by providing each contract
with persistent, byte-addressable virtual memory; and (3) Universal Procedure
Call (UPC), which enables fault-tolerant, programmable coordination across
distributed off-chain computation. Together, these components are powered by a
Rust-macroed unified programming model where on-chain and off-chain logic
coexist seamlessly, supporting both traditional smart contract patterns and
novel distributed applications. Lyquor addresses critical limitations in
existing systems while maintaining compatibility with Ethereum APIs, offering a
path toward truly scalable decentralized computation.

</details>


### [19] [Parallel Algorithms for the One Sided Crossing Minimization Problem](https://arxiv.org/abs/2509.23706)
*Bogdan-Ioan Popa,Adrian-Marius Dumitran,Livia Magureanu*

Main category: cs.DC

TL;DR: 本文研究了单边交叉最小化问题的并行化，实现了多种精确和固定参数可处理算法，在16核32线程机器上实现了接近19倍的加速，证明了并行化的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管多核系统兴起，但OSCM问题的精确和FPT算法的并行化研究仍很缺乏。并行变体在处理更大规模图时具有显著潜力，但需要仔细处理同步和内存管理问题。

Method: 探索了多种先前研究的OSCM精确和FPT算法，实现了它们的顺序和并行版本，并进行对比分析。

Result: 经验证明这些算法在并行化下可以实现接近线性的加速，最佳结果在16核32线程机器上实现了近19倍的加速。

Conclusion: 并行化OSCM算法是有效的，能够显著提升性能，但线性加速并不总能实现，需要进一步研究其原因。

Abstract: The One Sided Crossing Minimization (OSCM) problem is an optimization problem
in graph drawing that aims to minimize the number of edge crossings in
bipartite graph layouts. It has practical applications in areas such as network
visualization and VLSI (Very Large Scale Integration) design, where reducing
edge crossings improves the arrangement of circuit components and their
interconnections. Despite the rise of multi-core systems, the parallelization
of exact and fixed-parameter tractable (FPT) algorithms for OSCM remains
largely unexplored. Parallel variants offer significant potential for scaling
to larger graphs but require careful handling of synchronization and memory
management. In this paper, we explore various previously studied exact and FPT
algorithms for OSCM, implementing and analyzing them in both sequential and
parallel forms. Our main contribution lies in empirically proving that these
algorithms can achieve close to linear speedup under parallelization. In
particular, our best result achieves a speedup of nearly 19 on a 16-core,
32-thread machine. We further investigate and discuss the reasons why linear
speedup is not always attained.

</details>


### [20] [AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on Heterogeneous Models](https://arxiv.org/abs/2509.23722)
*Jihu Guo,Tenghui Ma,Wei Gao,Peng Sun,Jiaxing Li,Xun Chen,Yuyang Jin,Dahua Lin*

Main category: cs.DC

TL;DR: AdaPtis是一个自适应流水线并行训练系统，通过联合优化模型划分、模型放置和工作负载调度，显著提升大语言模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方法忽视了模型划分、模型放置和工作负载调度的协同优化，导致效率提升有限甚至性能下降。

Method: 开发了流水线性能模型来准确估计训练吞吐量，联合优化模型划分、模型放置和工作负载调度策略，并设计了统一的流水线执行器。

Result: 在各种LLM架构和规模下，AdaPtis相比Megatron-LM I-1F1B实现了平均1.42倍（最高2.14倍）的加速。

Conclusion: AdaPtis通过自适应流水线并行有效解决了模型架构异质性带来的流水线气泡问题，显著提升了训练效率。

Abstract: Pipeline parallelism is widely used to train large language models (LLMs).
However, increasing heterogeneity in model architectures exacerbates pipeline
bubbles, thereby reducing training efficiency. Existing approaches overlook the
co-optimization of model partition, model placement, and workload scheduling,
resulting in limited efficiency improvement or even performance degradation. To
respond, we propose AdaPtis, an LLM training system that supports adaptive
pipeline parallelism. First, we develop a pipeline performance model to
accurately estimate training throughput. Second, AdaPtis jointly optimizes
model partition, model placement, and workload scheduling policies guided by
this performance model. Third, we design a unified pipeline executor that
efficiently supports the execution of diverse pipeline strategies. Extensive
experiments show that AdaPtis achieves an average speedup of 1.42x (up to
2.14x) over Megatron-LM I-1F1B across various LLM architectures and scales.

</details>


### [21] [From Edge to HPC: Investigating Cross-Facility Data Streaming Architectures](https://arxiv.org/abs/2509.24030)
*Anjus George,Michael Brim,Christopher Zimmer,David Rogers,Sarp Oral,Zach Mayes*

Main category: cs.DC

TL;DR: 本文比较了三种跨设施数据流架构（DTS、PRS、MSS），通过模拟实验评估它们在AI-HPC通信模式下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 研究跨设施数据流架构的性能差异，为科学工作流提供最佳的数据流解决方案。

Method: 使用DS2HPC架构框架和SciStream工具包，在OLCF的ACE基础设施上实现三种架构，并通过三种合成工作负载进行模拟实验。

Result: DTS提供最短路径，具有更高吞吐量和更低延迟；MSS部署可行性更好但开销显著；PRS性能在大多数情况下与DTS相当。

Conclusion: 不同架构各有优劣，DTS适合性能敏感场景，MSS适合多用户可扩展场景，PRS在性能和可扩展性之间取得平衡。

Abstract: In this paper, we investigate three cross-facility data streaming
architectures, Direct Streaming (DTS), Proxied Streaming (PRS), and Managed
Service Streaming (MSS). We examine their architectural variations in data flow
paths and deployment feasibility, and detail their implementation using the
Data Streaming to HPC (DS2HPC) architectural framework and the SciStream
memory-to-memory streaming toolkit on the production-grade Advanced Computing
Ecosystem (ACE) infrastructure at Oak Ridge Leadership Computing Facility
(OLCF). We present a workflow-specific evaluation of these architectures using
three synthetic workloads derived from the streaming characteristics of
scientific workflows. Through simulated experiments, we measure streaming
throughput, round-trip time, and overhead under work sharing, work sharing with
feedback, and broadcast and gather messaging patterns commonly found in AI-HPC
communication motifs. Our study shows that DTS offers a minimal-hop path,
resulting in higher throughput and lower latency, whereas MSS provides greater
deployment feasibility and scalability across multiple users but incurs
significant overhead. PRS lies in between, offering a scalable architecture
whose performance matches DTS in most cases.

</details>


### [22] [TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents](https://arxiv.org/abs/2509.24063)
*Lukas Breitwieser,Ahmad Hesam,Abdullah Giray Yağlıkçı,Mohammad Sadrosadati,Fons Rademakers,Onur Mutlu*

Main category: cs.DC

TL;DR: TeraAgent是一个分布式代理仿真引擎，解决了现有平台BioDynaMo无法跨服务器扩展的问题，支持万亿级代理仿真。


<details>
  <summary>Details</summary>
Motivation: 现有代理仿真平台BioDynaMo由于基于共享内存实现，无法在多个服务器间扩展，限制了大规模复杂系统的仿真能力。

Method: 提出两种解决方案：1）定制化序列化机制，允许从接收缓冲区直接访问和修改代理；2）利用代理仿真的迭代特性，通过增量编码减少数据传输。

Result: TeraAgent实现了万亿级代理仿真（提升84倍），通过增加计算节点减少结果时间，提高了与第三方工具的互操作性，并为用户提供更多硬件灵活性。

Conclusion: TeraAgent成功解决了分布式代理仿真的关键性能瓶颈，实现了前所未有的仿真规模，为复杂系统研究提供了强大的计算平台。

Abstract: Agent-based simulation is an indispensable paradigm for studying complex
systems. These systems can comprise billions of agents, requiring the computing
resources of multiple servers to simulate. Unfortunately, the state-of-the-art
platform, BioDynaMo, does not scale out across servers due to its
shared-memory-based implementation.
  To overcome this key limitation, we introduce TeraAgent, a distributed
agent-based simulation engine. A critical challenge in distributed execution is
the exchange of agent information across servers, which we identify as a major
performance bottleneck. We propose two solutions: 1) a tailored serialization
mechanism that allows agents to be accessed and mutated directly from the
receive buffer, and 2) leveraging the iterative nature of agent-based
simulations to reduce data transfer with delta encoding.
  Built on our solutions, TeraAgent enables extreme-scale simulations with half
a trillion agents (an 84x improvement), reduces time-to-result with additional
compute nodes, improves interoperability with third-party tools, and provides
users with more hardware flexibility.

</details>


### [23] [RServe: Overlapping Encoding and Prefill for Efficient LMM Inference](https://arxiv.org/abs/2509.24381)
*Tianyu Guo,Tianming Xu,Xianjie Chen,Junru Chen,Nong Xiao,Xianwei Zhang*

Main category: cs.DC

TL;DR: REDServe是一个高效的多模态模型推理系统，通过细粒度调度方法在编码模块和语言模型解耦架构上实现请求内和请求间的并行优化，显著降低延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统多模态模型服务引擎存在编码模块与语言模型紧密耦合导致的资源干扰和数据依赖问题，现有解耦方法未能充分利用请求内和请求间的并行性。

Method: 采用编码模块与语言模型解耦架构，通过细粒度调度方法重叠多模态编码与语言模型前向计算，利用可调度令牌和令牌预算平衡微批次间的计算负载，结合分块预填充实现请求内外管道的协调执行。

Result: 在代表性多模态模型上的实验评估显示，REDServe实现高达66%的延迟降低和109%的吞吐量提升，显著优于现有服务方法。

Conclusion: REDServe通过高效的请求内外管道编排，在多模态模型推理服务中实现了显著的性能改进，证明了细粒度调度策略的有效性。

Abstract: Large multimodal models (LMMs) typically employ an encoding module to
transform multimodal data inputs into embeddings, which are then fed to
language models for further processing. However, efficiently serving LMMs
remains highly challenging due to the inherent complexity of their inference
pipelines. Traditional serving engines co-locate the encoding module and the
language model, leading to significant resource interference and tight data
dependency. Recent studies have alleviated this issue by disaggregating the
encoding module from the model, following a design style of prefill-decode
disaggregation. Nevertheless, these approaches fail to fully exploit
parallelism both within individual requests (intra-request) and across multiple
requests (inter-request).
  To overcome the limitation, we propose REDServe, an LMM inference system that
efficiently orchestrates intra- and inter-request pipelines. REDServe is
designed to reduce low latency and maximize parallelism at both intra- and
inter-request granularities. Built on the disaggregated architecture of the
encoding module and language model, REDServe adopts a fine-grained scheduling
method that overlaps multimodal encoding with the forward computation of the
language model within a single request. For inter-request pipeline, REDServe
leverages schedulable tokens and token budgets to balance computational loads
across micro-batches. Combined with chunked prefill, this enables a novel
scheduling strategy that coordinates the execution of intra- and inter-request
pipelines. Experimental evaluations on representative LMMs show that REDServe
achieves substantial latency reduction of up to 66% while improving throughput
by up to 109%, significantly outperforming existing serving approaches.

</details>


### [24] [SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in Long-Context LLM Serving](https://arxiv.org/abs/2509.24626)
*Qihui Zhou,Peiqi Yin,Pengfei Zuo,James Cheng*

Main category: cs.DC

TL;DR: SparseServe通过高效的HBM-DRAM分层管理解决了动态稀疏注意力算法中的KV缓存容量瓶颈问题，显著提升了长上下文LLM的服务性能。


<details>
  <summary>Details</summary>
Motivation: 动态稀疏注意力算法虽然减少了计算开销，但将性能瓶颈从HBM带宽转移到了HBM容量，未选中的KV缓存占用大量HBM空间，限制了并行批处理大小。

Method: 提出三个关键技术：碎片感知KV缓存传输（FlashH2D和FlashD2H）、工作集感知批处理大小控制、层分段预填充。

Result: 相比最先进的LLM服务系统，SparseServe实现了高达9.26倍的首令牌延迟降低和3.14倍的令牌生成吞吐量提升。

Conclusion: SparseServe通过高效的HBM-DRAM分层存储管理，成功释放了动态稀疏注意力算法的并行潜力，显著提升了长上下文LLM的服务效率。

Abstract: Serving long-context LLMs is costly because attention computation grows
linearly with context length. Dynamic sparse attention algorithms (DSAs)
mitigate this by attending only to the key-value (KV) cache of critical tokens.
However, with DSAs, the main performance bottleneck shifts from HBM bandwidth
to HBM capacity: KV caches for unselected tokens must remain in HBM for
low-latency decoding, constraining parallel batch size and stalling further
throughput gains. Offloading these underutilized KV caches to DRAM could free
HBM capacity, allowing larger parallel batch sizes. Yet, achieving such
hierarchical HBM-DRAM storage raises new challenges, including fragmented KV
cache access, HBM cache contention, and high HBM demands of hybrid batching,
that remain unresolved in prior work.
  This paper proposes SparseServe, an LLM serving system that unlocks the
parallel potential of DSAs through efficient hierarchical HBM-DRAM management.
SparseServe introduces three key innovations to address the challenges
mentioned above: (1) fragmentation-aware KV cache transfer, which accelerates
HBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted
saving (FlashD2H); (2) working-set-aware batch size control that adjusts batch
sizes based on real-time working set estimation to minimize HBM cache
thrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a
single layer, enabling efficient execution even for long prompts. Extensive
experimental results demonstrate that SparseServe achieves up to 9.26x lower
mean time-to-first-token (TTFT) latency and up to 3.14x higher token generation
throughput compared to state-of-the-art LLM serving systems.

</details>


### [25] [HAPT: Heterogeneity-Aware Automated Parallel Training on Heterogeneous Clusters](https://arxiv.org/abs/2509.24859)
*Antian Liang,Zhigang Zhao,Kai Zhang,Xuri Shi,Chuantao Li,Chunxiao Wang,Zhenying He,Yinan Jing,X. Sean Wang*

Main category: cs.DC

TL;DR: Hapt是一个专为异构集群设计的自动并行训练框架，通过细粒度规划器和异构感知调度器，在异构加速器和网络环境下实现高性能训练。


<details>
  <summary>Details</summary>
Motivation: 随着GPU架构的快速演进，训练基础设施的异构性不断增加。现有主要针对同构集群设计的框架在异构加速器和网络上部署时存在显著的资源利用不足问题。

Method: Hapt引入细粒度规划器搜索广泛的算子间并行策略，同时采用异构感知的1F1B调度器，根据网络特性自适应调整微批次的执行时间和顺序。

Result: 评估结果显示，Hapt在异构集群上的性能比最先进的训练框架高出1.3倍到1.6倍。

Conclusion: Hapt框架能够有效利用异构集群资源，在保持负载平衡的同时最大化计算-通信重叠，显著提升分布式模型训练性能。

Abstract: With the rapid evolution of GPU architectures, the heterogeneity of model
training infrastructures is steadily increasing. In such environments,
effectively utilizing all available heterogeneous accelerators becomes critical
for distributed model training. However, existing frameworks, which are
primarily designed for homogeneous clusters, often exhibit significant resource
underutilization when deployed on heterogeneous accelerators and networks. In
this paper, we present Hapt, an automated parallel training framework designed
specifically for heterogeneous clusters. Hapt introduces a fine-grained planner
that efficiently searches a wide space for the inter-operator parallel
strategy, enabling Hapt to alleviate communication overheads while maintaining
balanced loads across heterogeneous accelerators. In addition, Hapt implements
a heterogeneity-aware 1F1B scheduler that adaptively adjusts the execution
timing and ordering of microbatches based on network characteristics,
maximizing computation-communication overlap under cross-cluster interconnects
while incurring only minimal memory overhead. Our evaluation results show that
Hapt can deliver 1.3x-1.6x higher performance on heterogeneous clusters than
state-of-the-art training frameworks.

</details>


### [26] [Graph Theory Meets Federated Learning over Satellite Constellations: Spanning Aggregations, Network Formation, and Performance Optimization](https://arxiv.org/abs/2509.24932)
*Fardis Nadimi,Payam Abdisarabshali,Jacob Chakareski,Nicholas Mastronarde,Seyyedali Hosseinalipour*

Main category: cs.DC

TL;DR: Fed-Span是一个新颖的联邦/分布式学习框架，专为低地球轨道卫星星座设计。它利用图论原理解决卫星网络中的连接间歇性、计算能力异构性和数据集时变性等挑战，通过最小生成树/森林拓扑实现模型聚合和分发，并优化能耗、延迟和模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决低地球轨道卫星网络中分布式学习面临的独特挑战：卫星连接间歇性、卫星计算能力异构性、卫星数据集时变性，这些因素使得传统分布式学习方法在卫星网络中效率低下。

Method: 基于最小生成树(MST)和最小生成森林(MSF)拓扑构建分布式学习框架；通过连续约束表示(CCRs)形式化图论抽象；推导非凸损失函数的收敛边界；将联合优化问题转化为几何规划形式，使用逐次凸优化求解。

Result: 在真实数据集上的评估显示，Fed-Span相比现有方法具有更快的模型收敛速度、更高的能效和更低的延迟，证明了其在卫星网络中分布式学习的有效性。

Conclusion: Fed-Span为卫星网络中的高效分布式学习提供了一个新颖解决方案，通过图论抽象和系统优化，成功解决了动态卫星网络中的关键挑战。

Abstract: We introduce Fed-Span, a novel federated/distributed learning framework
designed for low Earth orbit satellite constellations. By leveraging
graph-theoretic principles, Fed-Span addresses critical challenges inherent to
distributed learning in dynamic satellite networks, including intermittent
satellite connectivity, heterogeneous computational capabilities of satellites,
and time-varying satellites' datasets. At its core, Fed-Span builds upon
minimum spanning tree (MST) and minimum spanning forest (MSF) topologies,
enabling spanning model aggregation and dispatching processes for distributed
learning. To formalize Fed-Span, we offer a fresh perspective on MST/MSF
topologies by formulating them through a set of continuous constraint
representations (CCRs), thereby devising graph-theoretical abstractions into an
optimizable framework for satellite networks. Using these CCRs, we obtain the
energy consumption and latency of operations in Fed-Span. Moreover, we derive
novel convergence bounds for non-convex machine learning loss functions,
accommodating the key system characteristics and degrees of freedom of
Fed-Span. Finally, we propose a comprehensive optimization problem that jointly
minimizes model prediction loss, energy consumption, and latency of Fed-Span.
We unveil that this problem is NP-hard and develop a systematic approach to
transform it into a geometric programming formulation, solved via successive
convex optimization with performance guarantees. Through evaluations on
real-world datasets, we demonstrate that Fed-Span outperforms existing methods,
with faster model convergence, greater energy efficiency, and reduced latency.
These results highlight Fed-Span as a novel solution for efficient distributed
learning in satellite networks.

</details>


### [27] [GRACE-MoE: Grouping and Replication with Locality-Aware Routing for Efficient Distributed MoE Inference](https://arxiv.org/abs/2509.25041)
*Yu Han,Lehan Pan,Jie Peng,Ziyang Tao,Wuyang Zhang,Yanyong Zhang*

Main category: cs.DC

TL;DR: GRACE-MoE是一个针对稀疏专家混合模型推理的协同优化框架，通过分组复制和局部感知路由来减少通信开销并缓解计算负载不平衡，实现最高3.79倍的加速。


<details>
  <summary>Details</summary>
Motivation: 稀疏专家混合模型在分布式部署时面临两个关键挑战：通信开销大和计算负载不均衡，其中通信开销是主要瓶颈。

Method: 采用两阶段方法：1) 基于专家亲和性的分组和动态复制来减少跨设备通信；2) 带负载预测的局部感知路由策略，优先使用本地副本。

Result: 在多节点多GPU环境下的实验表明，GRACE-MoE能有效减少端到端推理延迟，相比最先进系统实现最高3.79倍加速。

Conclusion: GRACE-MoE通过协同优化通信和计算负载，成功解决了SMoE推理中的关键瓶颈问题。

Abstract: Sparse Mixture of Experts (SMoE) performs conditional computation by
selectively activating a subset of experts, thereby enabling scalable parameter
growth in large language models (LLMs). However, the expanded parameter scale
exceeds the memory capacity of a single device, necessitating distributed
deployment for inference. This setup introduces two critical challenges: (1)
Communication Issue: Transferring features to devices with activated experts
leads to significant communication overhead. (2) Computational Load Issue:
Skewed expert activation overloads certain GPUs, resulting in load imbalance
across devices. Among these, communication overhead is identified as the main
bottleneck in SMoE inference. Nevertheless, reducing communication between
devices may exacerbate computational load imbalance, leading to device idleness
and resource waste. Therefore, we present GRACE-MoE, short for Grouping and
Replication with Locality-Aware Routing for SMoE inference. GRACE-MoE is a
co-optimization framework that jointly reduces communication overhead and
alleviates computational load imbalance. Specifically, the framework comprises
two key phases: (1) Grouping & Replication: This phase groups experts based on
their affinity to reduce cross-device communication. Additionally, dynamic
replication is applied to address load skew, improving computational load
balance across GPUs. (2) Routing: This phase employs a locality-aware routing
strategy with load prediction. It prioritizes local replicas to minimize
communication overhead and balances requests across remote replicas when
necessary. Experiments on diverse models and multi-node, multi-GPU environments
demonstrate that GRACE-MoE efficiently reduces end-to-end inference latency,
achieving up to 3.79x speedup over state-of-the-art systems. Code for GRACE-MoE
will be released upon acceptance.

</details>


### [28] [Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs](https://arxiv.org/abs/2509.25121)
*Anvitha Ramachandran,Dhruv Parikh,Viktor Prasanna*

Main category: cs.DC

TL;DR: 提出了一种用于动态图像图构建（DIGC）的FPGA加速器，解决了DIGC在Vision GNNs中占主导地位的计算瓶颈问题，相比CPU和GPU实现了显著加速。


<details>
  <summary>Details</summary>
Motivation: DIGC在Vision GNNs中占据了超过50%的端到端推理延迟，在高分辨率图像下甚至达到95%，成为主要计算瓶颈。现有方法主要通过算法优化，但往往牺牲了DIGC的灵活性、准确性或通用性。

Method: 设计了一个流式、深度流水线的FPGA加速器，采用片上缓冲器以小块均匀块处理输入特征。通过局部化计算最小化外部内存流量，使用局部归并排序和全局k路合并进行高效并行排序。

Result: 该设计在布局布线后实现了高时钟频率，静态配置的并行性最小化了关键路径延迟，相比优化的CPU和GPU DIGC基线实现了最高16.6倍和6.8倍的加速比。

Conclusion: 提出的模块化架构能够无缝扩展到不同图像分辨率、ViG层类型和模型大小，支持多种基于ViG的视觉骨干网络中的DIGC操作。

Abstract: Vision Graph Neural Networks (Vision GNNs, or ViGs) represent images as
unstructured graphs, achieving state of the art performance in computer vision
tasks such as image classification, object detection, and instance
segmentation. Dynamic Image Graph Construction (DIGC) builds image graphs by
connecting patches (nodes) based on feature similarity, and is dynamically
repeated in each ViG layer following GNN based patch (node) feature updates.
However, DIGC constitutes over 50% of end to end ViG inference latency, rising
to 95% at high image resolutions, making it the dominant computational
bottleneck. While hardware acceleration holds promise, prior works primarily
optimize graph construction algorithmically, often compromising DIGC
flexibility, accuracy, or generality. To address these limitations, we propose
a streaming, deeply pipelined FPGA accelerator for DIGC, featuring on chip
buffers that process input features in small, uniform blocks. Our design
minimizes external memory traffic via localized computation and performs
efficient parallel sorting with local merge sort and global k way merging
directly on streaming input blocks via heap insertion. This modular
architecture scales seamlessly across image resolutions, ViG layer types, and
model sizes and variants, and supports DIGC across diverse ViG based vision
backbones. The design achieves high clock frequencies post place and route due
to the statically configured parallelism minimizing critical path delay and
delivers up to 16.6x and 6.8x speedups over optimized CPU and GPU DIGC
baselines.

</details>


### [29] [Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units](https://arxiv.org/abs/2509.25155)
*Neelesh Gupta,Rakshith Jayanth,Dhruv Parikh,Viktor Prasanna*

Main category: cs.DC

TL;DR: 本文分析了在NPU上部署长上下文LLM的性能问题，比较了标准二次注意力与多种次二次替代方法的性能表现。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，在资源受限的边缘设备上进行长上下文推理需求增加，但标准注意力机制的二次复杂度与边缘加速器的内存和计算模式存在架构不匹配问题。

Method: 在现代NPU上对各种因果推理算子进行全面的性能分析，基准测试标准二次注意力与几种次二次替代方法（包括结构化状态空间和线性注意力模型）。

Result: 分析显示，次二次方法具有更好的可扩展性，但在NPU的专用执行单元上引入了独特的计算瓶颈。二次注意力在长上下文下严重受内存限制，缓存效率低下，流水线停顿超过95%；而次二次模型可能在可编程向量核心上受计算限制。

Conclusion: 这些发现为硬件感知模型和优化策略的协同设计提供了关键见解，以实现设备端的长上下文AI推理。

Abstract: The proliferation of large language models (LLMs) has driven demand for long
context inference on resource constrained edge devices. However, deploying
these models on Neural Processing Units (NPUs) presents significant challenges
due to the architectural mismatch: quadratic complexity of standard attention
mechanisms conflicts with memory and compute patterns of edge accelerators.
This paper presents a comprehensive performance analysis of various causal
inference operators on a modern NPU. We benchmark standard quadratic attention
against several sub-quadratic alternatives, including structured state-space
and linear attention models. Our analysis reveals that while sub-quadratic
methods offer superior scalability, they introduce distinct computational
bottlenecks on the NPU's specialized execution units. We identify that
quadratic attention becomes severely memory-bound, suffering from cache
inefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,
sub-quadratic models can become compute-bound on programmable vector cores.
These findings provide critical insights for the co-design of hardware-aware
models and optimization strategies to enable on-device AI inference with
long-contexts.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [30] [\textit{No One-Size-Fits-All}: A Workload-Driven Characterization of Bit-Parallel vs. Bit-Serial Data Layouts for Processing-using-Memory](https://arxiv.org/abs/2509.22980)
*Jingyao Zhang,Elaheh Sadredini*

Main category: cs.AR

TL;DR: 本文首次系统性地分析了PIM架构中的两种基本数据布局（位并行BP和位串行BS），发现没有单一布局适用于所有场景，最优选择取决于工作负载特性。


<details>
  <summary>Details</summary>
Motivation: PIM社区长期以来将BP和BS布局视为可互换的，这种"一种布局适用所有"的假设缺乏系统性的工作负载驱动指导，导致架构师无法为特定应用选择最优数据布局。

Method: 开发了等面积、周期精确的BP和BS PIM架构模型，使用多样化基准测试套件进行综合评估，包括MIMDRAM的细粒度微工作负载和PIMBench套件的大规模应用（如VGG网络）。

Result: BP在控制流密集型任务和不规则内存访问模式中表现优异，而BS在AI常见的大规模并行、低精度计算（如INT4/INT8）中具有显著优势。

Conclusion: 挑战了PIM数据布局的通用适用性观点，为设计下一代工作负载感知和潜在混合PIM系统提供了原则性基础，并提炼出一套可操作的设计指南。

Abstract: Processing-in-Memory (PIM) is a promising approach to overcoming the
memory-wall bottleneck. However, the PIM community has largely treated its two
fundamental data layouts, Bit-Parallel (BP) and Bit-Serial (BS), as if they
were interchangeable. This implicit "one-layout-fits-all" assumption, often
hard-coded into existing evaluation frameworks, creates a critical gap:
architects lack systematic, workload-driven guidelines for choosing the optimal
data layout for their target applications.
  To address this gap, this paper presents the first systematic,
workload-driven characterization of BP and BS PIM architectures. We develop
iso-area, cycle-accurate BP and BS PIM architectural models and conduct a
comprehensive evaluation using a diverse set of benchmarks. Our suite includes
both fine-grained microworkloads from MIMDRAM to isolate specific operational
characteristics, and large-scale applications from the PIMBench suite, such as
the VGG network, to represent realistic end-to-end workloads.
  Our results quantitatively demonstrate that no single layout is universally
superior; the optimal choice is strongly dependent on workload characteristics.
BP excels on control-flow-intensive tasks with irregular memory access
patterns, whereas BS shows substantial advantages in massively parallel,
low-precision (e.g., INT4/INT8) computations common in AI. Based on this
characterization, we distill a set of actionable design guidelines for
architects. This work challenges the prevailing one-size-fits-all view on PIM
data layouts and provides a principled foundation for designing
next-generation, workload-aware, and potentially hybrid PIM systems.

</details>


### [31] [Enhanced Hybrid Temporal Computing Using Deterministic Summations for Ultra-Low-Power Accelerators](https://arxiv.org/abs/2509.22999)
*Sachin Sachdeva,Jincong Lu,Wantong Li,Sheldon X. -D. Tan*

Main category: cs.AR

TL;DR: 提出了精度增强的混合时序计算框架E-HTC，通过两种新型加法器方案（EMBA和DTSA）改进了HTC架构的精度问题，在保持低功耗的同时显著提升了计算精度。


<details>
  <summary>Details</summary>
Motivation: 现有HTC架构利用脉冲率和时序数据编码降低开关活动和能耗，但因其基于MUX的缩放加法导致精度损失，需要开发更精确的加法方案。

Method: 提出EMBA精确多输入二进制累加器和DTSA确定性阈值缩放加法器，集成到支持单极和双极编码的MAC单元中，并在FIR滤波器和DCT/iDCT引擎上验证。

Result: 在4x4 MAC中，单极模式下E-HTC与CBSC的RMSE相当，比MUX-HTC精度提升94%，功耗和面积分别减少23%和7%；双极模式下RMSE为2.09%，比MUX-HTC提升83%。在FIR和DCT/iDCT实验中均获得显著PSNR提升和功耗面积节省。

Conclusion: E-HTC框架成功解决了HTC架构的精度问题，在保持超低功耗优势的同时实现了接近CBSC的精度水平，为超低功耗硬件加速器提供了有效的解决方案。

Abstract: This paper presents an accuracy-enhanced Hybrid Temporal Computing (E-HTC)
framework for ultra-low-power hardware accelerators with deterministic
additions. Inspired by the recently proposed HTC architecture, which leverages
pulse-rate and temporal data encoding to reduce switching activity and energy
consumption but loses accuracy due to its multiplexer (MUX)-based scaled
addition, we propose two bitstream addition schemes: (1) an Exact
Multiple-input Binary Accumulator (EMBA), which performs precise binary
accumulation, and (2) a Deterministic Threshold-based Scaled Adder (DTSA),
which employs threshold logic for scaled addition. These adders are integrated
into a multiplier accumulator (MAC) unit supporting both unipolar and bipolar
encodings. To validate the framework, we implement two accelerators: a Finite
Impulse Response (FIR) filter and an 8-point Discrete Cosine Transform
(DCT)/iDCT engine. Results on a 4x4 MAC show that, in unipolar mode, E-HTC
matches the RMSE of state-of-the-art Counter-Based Stochastic Computing (CBSC)
MAC, improves accuracy by 94% over MUX-based HTC, and reduces power and area by
23% and 7% compared to MUX-based HTC and 64% and 74% compared to CBSC. In
bipolar mode, E-HTC MAC achieves 2.09% RMSE -- an 83% improvement over
MUX-based HTC -- and approaches CBSC's 1.40% RMSE with area and power savings
of 28% and 43% vs. MUX-based HTC and about 76% vs. CBSC. In FIR experiments,
both E-HTC variants yield PSNR gains of 3--5 dB (30--45% RMSE reduction) while
saving 13% power and 3% area. For DCT/iDCT, E-HTC boosts PSNR by 10--13 dB
(70--75% RMSE reduction) while saving area and power over both MUX- and
CBSC-based designs.

</details>


### [32] [A Near-Cache Architectural Framework for Cryptographic Computing](https://arxiv.org/abs/2509.23179)
*Jingyao Zhang,Elaheh Sadredini*

Main category: cs.AR

TL;DR: 提出了一种名为Crypto-Near-Cache (CNC)的近缓存切片计算范式，通过将具有位线计算能力的SRAM阵列放置在缓存切片附近，来加速后量子密码算法，解决缓存带宽瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 后量子密码算法的公钥和签名长度比前量子密码长3-9倍，导致显著的性能和能效开销。分析发现缓存带宽是关键瓶颈，这促使采用片上近缓存计算范式来加速后量子密码算法。

Method: 设计CNC近缓存切片计算范式，在缓存切片附近放置具有位线计算能力的SRAM阵列，实现高内部带宽和短数据移动，支持虚拟地址，并提出了相应的ISA扩展。

Result: 通过CNC架构实现了高内部带宽和短数据移动，能够有效加速后量子密码算法和其他应用，解决了外部带宽限制带来的性能问题。

Conclusion: CNC提供了一种创新的近缓存计算解决方案，能够无缝集成到现有系统中，解决后量子密码算法的性能和能效问题，同时支持虚拟地址和定制化需求。

Abstract: Recent advancements in post-quantum cryptographic algorithms have led to
their standardization by the National Institute of Standards and Technology
(NIST) to safeguard information security in the post-quantum era. These
algorithms, however, employ public keys and signatures that are 3 to 9$\times$
longer than those used in pre-quantum cryptography, resulting in significant
performance and energy efficiency overheads. A critical bottleneck identified
in our analysis is the cache bandwidth. This limitation motivates the adoption
of on-chip in-/near-cache computing, a computing paradigm that offers
high-performance, exceptional energy efficiency, and flexibility to accelerate
post-quantum cryptographic algorithms. Our analysis of existing works reveals
challenges in integrating in-/near-cache computing into modern computer systems
and performance limitations due to external bandwidth limitation, highlighting
the need for innovative solutions that can seamlessly integrate into existing
systems without performance and energy efficiency issues. In this paper, we
introduce a near-cache-slice computing paradigm with support of customization
and virtual address, named Crypto-Near-Cache (CNC), designed to accelerate
post-quantum cryptographic algorithms and other applications. By placing SRAM
arrays with bitline computing capability near cache slices, high internal
bandwidth and short data movement are achieved with native support of virtual
addressing. An ISA extension to facilitate CNC is also proposed, with detailed
discussion on the implementation aspects of the core/cache datapath.

</details>


### [33] [AssertGen: Enhancement of LLM-aided Assertion Generation through Cross-Layer Signal Bridging](https://arxiv.org/abs/2509.23674)
*Hongqin Lyu,Yonghao Wang,Yunlin Du,Mingyu Shi,Zhiteng Chao,Wenxing Li,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: AssertGen是一个基于LLM的断言生成框架，通过提取验证目标、构建跨层信号链来生成SystemVerilog断言，在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM辅助的断言生成方法无法有效识别设计规范与RTL设计之间的关系，导致生成的断言不充分。

Method: 首先使用思维链推理从规范中提取验证目标，然后在目标与RTL代码之间桥接信号构建跨层信号链，最后基于LLM生成SVA断言。

Result: 实验结果表明AssertGen在形式属性验证通过率、影响锥、证明核心和变异测试覆盖率等关键指标上优于现有最先进方法。

Conclusion: AssertGen框架通过有效连接规范与RTL设计，显著提升了断言生成的质量和效果。

Abstract: Assertion-based verification (ABV) serves as a crucial technique for ensuring
that register-transfer level (RTL) designs adhere to their specifications.
While Large Language Model (LLM) aided assertion generation approaches have
recently achieved remarkable progress, existing methods are still unable to
effectively identify the relationship between design specifications and RTL
designs, which leads to the insufficiency of the generated assertions. To
address this issue, we propose AssertGen, an assertion generation framework
that automatically generates SystemVerilog assertions (SVA). AssertGen first
extracts verification objectives from specifications using a chain-of-thought
(CoT) reasoning strategy, then bridges corresponding signals between these
objectives and the RTL code to construct a cross-layer signal chain, and
finally generates SVAs based on the LLM. Experimental results demonstrate that
AssertGen outperforms the existing state-of-the-art methods across several key
metrics, such as pass rate of formal property verification (FPV), cone of
influence (COI), proof core and mutation testing coverage.

</details>


### [34] [ASIC-based Compression Accelerators for Storage Systems: Design, Placement, and Profiling Insights](https://arxiv.org/abs/2509.23693)
*Tao Lu,Jiapin Wang,Yelin Shan,Xiangping Zhang,Xiang Chen*

Main category: cs.AR

TL;DR: 本文评估了硬件压缩解压处理单元(CDPU)的不同部署位置(外设、片上、存储内)对性能和效率的影响，揭示了部署位置对吞吐量、延迟、功耗效率的重要影响，并提出了针对超大规模存储基础设施的跨层重新思考。


<details>
  <summary>Details</summary>
Motivation: 无损压缩在CPU上执行会给数据中心带来显著计算开销，硬件CDPU可以缓解这一开销，但最优算法选择、微架构设计和系统级部署位置仍不明确。

Method: 设计了一个基于ASIC的存储内CDPU，并与Intel QAT 8970和QAT 4xxx两个领先的ASIC加速器进行全面的端到端评估，涵盖三种主要CDPU部署机制。

Result: 结果显示：(i)吞吐量和延迟对CDPU部署位置和互连极度敏感；(ii)压缩效率与数据模式/布局强相关；(iii)微基准测试增益与实际应用加速存在部署驱动的差异；(iv)模块级和系统级功耗效率存在差异；(v)各种CDPU的可扩展性和多租户干扰问题。

Conclusion: 这些发现促使对超大规模存储基础设施的硬件压缩解压进行部署感知的跨层重新思考。

Abstract: Lossless compression imposes significant computational over head on
datacenters when performed on CPUs. Hardware compression and decompression
processing units (CDPUs) can alleviate this overhead, but optimal algorithm
selection, microarchitectural design, and system-level placement of CDPUs are
still not well understood. We present the design of an ASIC-based in-storage
CDPU and provide a comprehensive end-to-end evaluation against two leading ASIC
accelerators, Intel QAT 8970 and QAT 4xxx. The evaluation spans three dominant
CDPU placement regimes: peripheral, on-chip, and in-storage. Our results
reveal: (i) acute sensitivity of throughput and latency to CDPU placement and
interconnection, (ii) strong correlation between compression efficiency and
data patterns/layouts, (iii) placement-driven divergences between
microbenchmark gains and real-application speedups, (iv) discrepancies between
module and system-level power efficiency, and (v) scalability and multi-tenant
interference is sues of various CDPUs. These findings motivate a
placement-aware, cross-layer rethinking of hardware (de)compression for
hyperscale storage infrastructures.

</details>


### [35] [AssertFix: Empowering Automated Assertion Fix via Large Language Models](https://arxiv.org/abs/2509.23972)
*Hongqin Lyu,Yunlin Du,Yonghao Wang,Zhiteng Chao,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: 提出基于LLM的自动断言修复框架AssertFix，通过定位RTL代码、识别错误根源、分类错误类型并应用专用修复策略，自动修正错误断言，提高断言生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统断言生成方法将断言生成作为最终步骤，需要人工修复错误断言，限制了这些方法的应用。

Method: AssertFix框架：1) 准确定位与错误断言相关的RTL代码；2) 系统识别断言错误的根本原因；3) 分类错误类型；4) 应用专用修复策略自动修正错误。

Result: 在Opencore基准测试中，AssertFix在修复率和验证覆盖率方面都取得了显著提升。

Conclusion: AssertFix能够有效自动修复错误断言，提高断言生成的整体质量，解决了传统方法需要人工干预的局限性。

Abstract: Assertion-based verification (ABV) is critical in ensuring that
register-transfer level (RTL) designs conform to their functional
specifications. SystemVerilog Assertions (SVA) effectively specify design
properties, but writing and maintaining them manually is challenging and
error-prone. Although recent progress of assertion generation methods
leveraging large language models (LLMs) have shown great potential in improving
assertion quality, they typically treat assertion generation as a final step,
leaving the burden of fixing of the incorrect assertions to human effects,
which may significantly limits the application of these methods. To address the
above limitation, we propose an automatic assertion fix framework based on
LLMs, named AssertFix. AsserFix accurately locates the RTL code related to the
incorrect assertion, systematically identifies the root causes of the assertion
errors, classifies the error type and finally applies dedicated fix strategies
to automatically correct these errors, improving the overall quality of the
generated assertions. Experimental results show that AssertFix achieves
noticeable improvements in both fix rate and verification coverage across the
Opencore benchmarks.

</details>


### [36] [Fault Injection in On-Chip Interconnects: A Comparative Study of Wishbone, AXI-Lite, and AXI](https://arxiv.org/abs/2509.24929)
*Hongwei Zhao,Vianney Lapotre,Guy Gogniat*

Main category: cs.AR

TL;DR: 该研究通过仿真驱动的故障注入分析了三种主流总线协议(Wishbone、AXI Lite、AXI)的漏洞，揭示了总线协议在故障攻击下的行为模式和时空依赖性。


<details>
  <summary>Details</summary>
Motivation: 随着SoC架构复杂度增加，片上通信总线作为IP核间互连的潜在故障攻击向量，其脆弱性日益突出，需要系统性地评估总线协议的安全漏洞。

Method: 采用仿真驱动的故障注入方法，对三种主流总线协议(Wishbone、AXI Lite、AXI)进行系统性的故障注入实验。

Result: 研究发现了跨协议的一致行为模式，包括故障成功率、空间脆弱性分布和时序依赖性，揭示了故障与总线级事务交互的特征。

Conclusion: 该研究为攻击建模和弹性SoC设计提供了实用见解，强调了总线协议在故障注入攻击下的系统性漏洞特征。

Abstract: Fault injection attacks exploit physical disturbances to compromise the
functionality and security of integrated circuits. As System on Chip (SoC)
architectures grow in complexity, the vulnerability of on chip communication
fabrics has become increasingly prominent. Buses, serving as interconnects
among various IP cores, represent potential vectors for fault-based
exploitation. In this study, we perform simulation-driven fault injection
across three mainstream bus protocols Wishbone, AXI Lite, and AXI. We
systematically examine fault success rates, spatial vulnerability
distributions, and timing dependencies to characterize how faults interact with
bus-level transactions. The results uncover consistent behavioral patterns
across protocols, offering practical insights for both attack modeling and the
development of resilient SoC designs.

</details>
