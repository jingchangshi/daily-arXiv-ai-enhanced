<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 17]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [VerilogMonkey: Exploring Parallel Scaling for Automated Verilog Code Generation with LLMs](https://arxiv.org/abs/2509.16246)
*Juxin Niu,Yuxin Du,Dan Niu,Xi Wang,Zhe Jiang,Nan Guan*

Main category: cs.PL

TL;DR: VerilogMonkey是一项关于Verilog自动生成任务中并行扩展的实证研究，发现通过并行采样大量输出可以在不增加额外增强的情况下超越现有LLM-based Verilog生成结果


<details>
  <summary>Details</summary>
Motivation: 研究并行扩展在自动化Verilog生成这一未被充分探索任务中的效果，探索如何通过并行采样提升LLM性能

Method: 在多个基准测试和主流LLMs上进行实验，通过并行采样数百个输出，分析输出随机性对并行扩展效果的影响

Result: 并行扩展到数百个样本在时间和成本上都是有效的，即使没有后训练或代理方法等额外增强，也能超越现有LLM-based Verilog生成结果

Conclusion: 并行扩展是提升LLM在Verilog生成任务中性能的有效方法，输出随机性会影响其效果

Abstract: We present VerilogMonkey, an empirical study of parallel scaling for the
under-explored task of automated Verilog generation. Parallel scaling improves
LLM performance by sampling many outputs in parallel. Across multiple
benchmarks and mainstream LLMs, we find that scaling to hundreds of samples is
cost-effective in both time and money and, even without any additional
enhancements such as post-training or agentic methods, surpasses prior results
on LLM-based Verilog generation. We further dissect why parallel scaling
delivers these gains and show how output randomness in LLMs affects its
effectiveness.

</details>


### [2] [GraphMend: Code Transformations for Fixing Graph Breaks in PyTorch 2](https://arxiv.org/abs/2509.16248)
*Savini Kashmira,Jayanaka Dantanarayana,Thamirawaran Sathiyalogeswaran,Yichao Yuan,Nishil Talati,Krisztian Flautner,Lingjia Tang,Jason Mars*

Main category: cs.PL

TL;DR: GraphMend是一个高层编译器，通过代码转换消除PyTorch 2程序中的FX图断点，提高编译效率和性能


<details>
  <summary>Details</summary>
Motivation: PyTorch 2的TorchDynamo和TorchInductor虽然支持即时图编译，但动态控制流和不受支持的Python构造会导致模型被分割成多个FX图，造成频繁回退到eager模式、昂贵的CPU-GPU同步和优化机会减少

Method: 基于Jac编译框架，GraphMend在代码执行前进行分析和转换，引入两种代码转换技术来消除由动态控制流和Python I/O函数引起的图断点

Result: 在8个Hugging Face模型上的评估显示，GraphMend消除了所有可修复的图断点，6个模型中断点降为0，另一个模型从5个降至2个。在NVIDIA RTX 3090和A40 GPU上，延迟最多降低75%，端到端吞吐量最多提高8%

Conclusion: 高层代码转换是PyTorch动态JIT编译管道的有效补充，显著提高了可用性和性能

Abstract: This paper presents GraphMend, a high-level compiler that eliminates FX graph
breaks in PyTorch 2 programs. Although PyTorch 2 introduced TorchDynamo and
TorchInductor to enable just-in-time graph compilation, unresolved dynamic
control flow and unsupported Python constructs often fragment models into
multiple FX graphs. These fragments force frequent fallbacks to eager mode,
incur costly CPU-to-GPU synchronizations, and reduce optimization
opportunities. GraphMend addresses this limitation by analyzing and
transforming source code before execution. Built on the Jac compilation
framework, GraphMend introduces two code transformations that remove graph
breaks due to dynamic control flow and Python I/O functions. This design allows
PyTorch's compilation pipeline to capture larger, uninterrupted FX graphs
without requiring manual refactoring by developers. Evaluation across eight
Hugging Face models shows that GraphMend removes all fixable graph breaks due
to dynamic control flow and Python I/O functions, driving the break count to 0
in 6 models and reducing it from 5 to 2 in another model. On NVIDIA RTX 3090
and A40 GPUs, GraphMend achieves up to 75% latency reductions and up to 8%
higher end-to-end throughput. These results demonstrate that high-level code
transformation is an effective complement to PyTorch's dynamic JIT compilation
pipeline, substantially improving both usability and performance.

</details>


### [3] [Efficient Linearizability Monitoring](https://arxiv.org/abs/2509.17795)
*Parosh Aziz Abdulla,Samuel Grahn,Bengt Jonsson,Shankaranarayanan Krishna,Om Swostik Mishra*

Main category: cs.PL

TL;DR: 本文重新审视了监控并发栈、队列、集合和多集线性化性的基本问题，提出了更高效的监控算法，并纠正了现有工作中的正确性问题。


<details>
  <summary>Details</summary>
Motivation: 现有监控线性化性的方法存在时间复杂度高（立方级）和正确性问题（缺乏证明、证明错误或算法错误），需要提出更高效且正确性可验证的算法。

Method: 针对栈、队列和（多）集合分别设计了时间复杂度为O(n²)、O(n log n)和O(n)的监控算法，基于数据独立性假设，并提供了详细的正确性证明。

Result: 实现了栈和队列算法的工具LiMo，实验评估显示LiMo在效率和可扩展性方面优于现有工具Violin。

Conclusion: 本文提出的算法在保证正确性的同时显著提高了监控线性化性的效率，为并发数据结构的验证提供了更可靠的解决方案。

Abstract: This paper revisits the fundamental problem of monitoring the linearizability
of concurrent stacks, queues, sets, and multisets. Given a history of a library
implementing one of these abstract data types, the monitoring problem is to
answer whether the given history is linearizable. For stacks, queues, and
(multi)sets, we present monitoring algorithms with complexities
$\mathcal{O}(n^2)$, $\mathcal{O}(n\; log\, n)$, and $\mathcal{O}{(n)}$,
respectively, where $n$ is the number of operations in the input history. For
stacks and queues, our results hold under the standard assumption of {\it
data-independence}, i.e., the behavior of the library is not sensitive to the
actual values stored in the data structure. Past works to solve the same
problems have cubic time complexity and (more seriously) have correctness
issues: they either (i) lack correctness proofs or (ii) the suggested
correctness proofs are erroneous (we present counter-examples), or (iii) have
incorrect algorithms. Our improved complexity results rely on substantially
different algorithms for which we provide detailed proofs of correctness. We
have implemented our stack and queue algorithms in LiMo (Linearizability
Monitor). We evaluate LiMo and compare it with the state-of-the-art tool Violin
-- whose correctness proofs we have found errors in -- which checks for
linearizability violations. Our experimental evaluation confirms that LiMo
outperforms Violin regarding both efficiency and scalability.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables](https://arxiv.org/abs/2509.16407)
*Hunter McCoy,Prashant Pandey*

Main category: cs.DC

TL;DR: WarpSpeed是一个高性能并发GPU哈希表库，解决了现有GPU哈希表功能受限的问题，包括不完整的并发支持和缺少复合操作。


<details>
  <summary>Details</summary>
Motivation: 现有GPU哈希表功能有限，限制了其在大规模数据处理应用中的采用，特别是缺乏完整的并发支持和复合操作（如upserts）。

Method: 实现了8种最先进的NVIDIA GPU哈希表设计，提供丰富的API，并提出了优化技术（如基于指纹的元数据减少缓存行探测、专用GPU指令实现无锁查询）。

Result: 通过多样化基准测试评估正确性和可扩展性，并将这些哈希表集成到三个下游应用中展示了实际影响。

Conclusion: 研究为并发GPU哈希表设计提供了新见解，并为在现代GPU上开发高效、可扩展的数据结构提供了实用指导。

Abstract: GPU hash tables are increasingly used to accelerate data processing, but
their limited functionality restricts adoption in large-scale data processing
applications. Current limitations include incomplete concurrency support and
missing compound operations such as upserts.
  This paper presents WarpSpeed, a library of high-performance concurrent GPU
hash tables with a unified benchmarking framework for performance analysis.
WarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and
provides a rich API designed for modern GPU applications. Our evaluation uses
diverse benchmarks to assess both correctness and scalability, and we
demonstrate real-world impact by integrating these hash tables into three
downstream applications.
  We propose several optimization techniques to reduce concurrency overhead,
including fingerprint-based metadata to minimize cache line probes and
specialized Nvidia GPU instructions for lock-free queries. Our findings provide
new insights into concurrent GPU hash table design and offer practical guidance
for developing efficient, scalable data structures on modern GPUs.

</details>


### [5] [Shift Parallelism: Low-Latency, High-Throughput LLM Inference for Dynamic Workloads](https://arxiv.org/abs/2509.16495)
*Mert Hidayetoglu,Aurick Qiao,Michael Wyatt,Jeff Rasley,Yuxiong He,Samyam Rajbhandari*

Main category: cs.DC

TL;DR: Shift Parallelism是一种新的并行化方法，通过动态切换张量并行(TP)和序列并行(SP)来优化大语言模型推理，在低流量时最小化延迟，在高流量时不损失吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有张量并行(TP)方法在降低延迟方面表现良好但吞吐量受GPU通信限制，数据并行(DP)吞吐量高但延迟慢。两种方法无法结合使用，因为KV缓存在不同并行化方法间存在差异。

Method: 将序列并行(SP)适应到推理场景，并与TP结合形成Shift Parallelism。该方法动态在TP和SP之间切换，利用SP的KV缓存不变性特性。

Result: 在真实生产环境测试中，Shift Parallelism相比TP-only方案：交互式工作负载响应速度提升1.51倍，批量工作负载吞吐量提高50%。

Conclusion: Shift Parallelism在延迟与吞吐量的权衡上优于TP或DP，能够在动态工作负载中实现低延迟而不降低吞吐量。

Abstract: Efficient parallelism is necessary for achieving low-latency, high-throughput
inference with large language models (LLMs). Tensor parallelism (TP) is the
state-of-the-art method for reducing LLM response latency, however GPU
communications reduces combined token throughput. On the other hand, data
parallelism (DP) obtains a higher throughput yet is slow in response latency.
Best of both worlds does not exist, and it is not possible to combine TP and DP
because of the KV cache variance across the parallelisms.
  We notice Sequence Parallelism (SP - Ulysses in training) has similar
properties as DP but with KV cache invariance. We adapt SP to inference, and
combine it with TP to get the best of both worlds. Our solution: Shift
Parallelism.
  Shift Parallelism dynamically switches across TP and SP, and minimizes
latency in low traffic without losing throughput in high traffic. The efficient
GPU communications of Shift Parallelism yields up to i) 1.51x faster response
in interactive workloads and ii) 50% higher throughput in batch workloads,
compared to a TP-only solution.
  We evaluate Shift Parallelism with real-world production traces with dynamic
traffic patterns as well as synthetic benchmarking patterns across models,
context sizes, and arrival rates. All results affirm the same: Shift
Parallelism has a better the latency vs. throughput tradeoff than TP or DP, and
hence obtains low latency without degrading throughput in dynamic workloads.

</details>


### [6] [sat-QFL: Secure Quantum Federated Learning for Low Orbit Satellites](https://arxiv.org/abs/2509.16504)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.DC

TL;DR: sat-QFL是一个针对低地球轨道卫星星座的分层量子联邦学习框架，通过主次卫星角色划分和可见窗口对齐的训练调度，解决了传统FL在卫星环境中的连通性、参与度和延迟挑战，并集成了量子密钥分发确保安全性。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星星座违反了传统量子联邦学习的核心假设：客户端-服务器连接不稳定、参与度随时间变化、延迟预算严格。需要设计能够适应这些特殊约束的FL框架。

Method: 提出分层架构，将卫星分为主要（地面连接）和次要（仅卫星间链路）角色，采用顺序、同步或异步的边缘训练调度与可见窗口对齐。集成量子密钥分发进行密钥建立和认证加密，评估量子态传输的可行性。

Result: 使用星座轨迹和QFL工作负载测试表明，sat-QFL在不同参与度下保持稳健的聚合，减少了通信瓶颈，安全开销适中。

Conclusion: sat-QFL框架能够有效应对LEO卫星环境的特殊挑战，为量子联邦学习在太空应用提供了可行的解决方案。

Abstract: Low Earth orbit (LEO) constellations violate core assumptions of standard
(quantum) federated learning (FL): client-server connectivity is intermittent,
participation is time varying, and latency budgets are strict. We present
sat-QFL, a hierarchical, access aware quantum federated learning (QFL)
framework that partitions satellites into primary (ground connected) and
secondary as inter-satellite links (ISL-only) roles, and schedules sequential,
simultaneous, or asynchronous edge training aligned with visibility windows.
For quantum-resilient confidentiality and integrity, sat-QFL integrates quantum
key distribution (QKD) based key establishment with authenticated encryption
for model exchange; we also assess teleportation as a feasibility primitive for
quantum state transfer. Using derived constellation traces and QFL workloads
(Qiskit), we show that sat-QFL sustains robust aggregation under varying
participation and reduces communication bottlenecks with modest security
overhead. Our implementation and results are available at
https://github.com/s222416822/satQFL.

</details>


### [7] [orb-QFL: Orbital Quantum Federated Learning](https://arxiv.org/abs/2509.16505)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.DC

TL;DR: 提出orb-QFL框架，利用量子计算和量子纠缠实现低地球轨道卫星星座的去中心化联邦学习，无需中央服务器或全局聚合机制。


<details>
  <summary>Details</summary>
Motivation: 量子计算突破为非地面环境中的联邦学习提供了变革性机会，特别是在通信和协调受限的轨道环境中。

Method: 结合Qiskit量子机器学习工具包和Poliastro轨道模拟，利用量子纠缠和本地量子处理实现卫星间的直接量子同步。

Result: 通过Statlog数据集实验验证，该框架能够应对轨道动力学挑战，如间歇性连接和高传播延迟。

Conclusion: orb-QFL框架通过量子辅助的联邦学习方法，为LEO卫星星座提供了增强的弹性和数据本地性保护。

Abstract: Recent breakthroughs in quantum computing present transformative
opportunities for advancing Federated Learning (FL), particularly in
non-terrestrial environments characterized by stringent communication and
coordination constraints. In this study, we propose orbital QFL, termed
orb-QFL, a novel quantum-assisted Federated Learning framework tailored for Low
Earth Orbit (LEO) satellite constellations. Distinct from conventional FL
paradigms, termed orb-QFL operates without centralized servers or global
aggregation mechanisms (e.g., FedAvg), instead leveraging quantum entanglement
and local quantum processing to facilitate decentralized, inter-satellite
collaboration. This design inherently addresses the challenges of orbital
dynamics, such as intermittent connectivity, high propagation delays, and
coverage variability. The framework enables continuous model refinement through
direct quantum-based synchronization between neighboring satellites, thereby
enhancing resilience and preserving data locality. To validate our approach, we
integrate the Qiskit quantum machine learning toolkit with Poliastro-based
orbital simulations and conduct experiments using Statlog dataset.

</details>


### [8] [Trace Replay Simulation of MIT SuperCloud for Studying Optimal Sustainability Policies](https://arxiv.org/abs/2509.16513)
*Wesley Brewer,Matthias Maiterth,Damien Fay*

Main category: cs.DC

TL;DR: 本文扩展了ExaDigiT开源数字孪生框架，用于模拟AI超级计算中心的电力、冷却和调度，通过强化学习实验展示学习能源感知调度决策的可行性。


<details>
  <summary>Details</summary>
Motivation: AI超级计算的快速增长带来了前所未有的电力需求，下一代GPU数据中心需要数百兆瓦电力并产生快速、大幅度的消耗波动，这给公用事业和系统运营商带来了挑战。

Method: 扩展ExaDigiT框架，纳入异构性、多租户和云规模工作负载，重点关注MIT SuperCloud TX-GAIA系统的跟踪重放和作业重新调度，使用近端策略优化进行强化学习实验。

Result: 初步强化学习实验证明了学习能源感知调度决策的可行性，RAPS模块提供了详细的电力和性能统计模拟环境。

Conclusion: ExaDigiT有潜力作为探索优化策略的平台，以提高吞吐量、效率和可持续性。

Abstract: The rapid growth of AI supercomputing is creating unprecedented power
demands, with next-generation GPU datacenters requiring hundreds of megawatts
and producing fast, large swings in consumption. To address the resulting
challenges for utilities and system operators, we extend ExaDigiT, an
open-source digital twin framework for modeling power, cooling, and scheduling
of supercomputers. Originally developed for replaying traces from
leadership-class HPC systems, ExaDigiT now incorporates heterogeneity,
multi-tenancy, and cloud-scale workloads. In this work, we focus on trace
replay and rescheduling of jobs on the MIT SuperCloud TX-GAIA system to enable
reinforcement learning (RL)-based experimentation with sustainability policies.
The RAPS module provides a simulation environment with detailed power and
performance statistics, supporting the study of scheduling strategies,
incentive structures, and hardware/software prototyping. Preliminary RL
experiments using Proximal Policy Optimization demonstrate the feasibility of
learning energy-aware scheduling decisions, highlighting ExaDigiT's potential
as a platform for exploring optimal policies to improve throughput, efficiency,
and sustainability.

</details>


### [9] [ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix Caching](https://arxiv.org/abs/2509.16857)
*Xingyu Xiang,Raj Joshi,Yuhan Liu,Jiayi Yao,Chenxingyu Zhao,Junchen Jiang,Yang Zhou,Eddie Kohler,Minlan Yu*

Main category: cs.DC

TL;DR: ShadowServe是一个基于SmartNIC加速的分布式前缀缓存系统，通过将数据平面完全卸载到SmartNIC上来消除对主机GPU和CPU的干扰，在低带宽场景下显著提升LLM服务性能。


<details>
  <summary>Details</summary>
Motivation: 分布式前缀缓存虽然能加速长上下文LLM服务，但在网络带宽受限时KV缓存获取会成为瓶颈。压缩技术可以缓解带宽问题，但解压缩过程会干扰模型计算，影响整体性能。

Method: 设计ShadowServe系统，将控制平面保留在主机上，数据平面完全卸载到SmartNIC。采用分块流水线技术并行化数据平面操作，并使用最小拷贝内存管理方案减少SmartNIC内存压力。

Result: 相比现有最优解决方案，ShadowServe在低带宽场景（≤20 Gbps）下实现了2.2倍更低的加载时间每输出令牌（TPOT），首令牌时间（TTFT）降低1.38倍，吞吐量提升1.35倍。

Conclusion: ShadowServe通过SmartNIC加速和干扰消除设计，有效解决了分布式前缀缓存中的带宽瓶颈问题，显著提升了LLM服务的性能表现。

Abstract: Distributed prefix caching accelerates long-context LLM serving by reusing KV
cache entries for common context prefixes. However, KV cache fetches can become
a bottleneck when network bandwidth is limited. Compression mitigates the
bandwidth issue, but can degrade overall performance when decompression
interferes with model computation.
  We present ShadowServe, the first SmartNIC-accelerated, interference-free
prefix caching system for LLM serving. ShadowServe separates a control plane on
the host and a data plane fully offloaded to the SmartNIC, which eliminates
interference to both host GPU and CPU. To overcome the SmartNIC's limited
compute and memory resources, we design a chunked pipeline that parallelizes
data plane operations across the SmartNIC's compute resources, and a
minimal-copy memory management scheme that reduces memory pressure on the
SmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to
2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token
(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to
up to 1.35x higher throughput.

</details>


### [10] [MoA-Off: Adaptive Heterogeneous Modality-Aware Offloading with Edge-Cloud Collaboration for Efficient Multimodal LLM Inference](https://arxiv.org/abs/2509.16995)
*Zheming Yang,Qi Guo,Yunqing Hu,Chang Zhao,Chang Zhang,Jian Zhao,Wen Ji*

Main category: cs.DC

TL;DR: MoA-Off是一个自适应异构模态感知的边缘-云协同卸载框架，旨在解决多模态大语言模型在资源受限环境中的计算和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型虽然具有强大的跨模态推理能力，但在资源受限环境中部署时面临显著的计算和延迟负担挑战。

Method: 引入轻量级异构模态感知模块通过多维特征分析估计异构输入的复杂度，并提出自适应边缘-云协同卸载策略，基于模态感知复杂度分数和实时系统状态动态调度工作负载。

Result: 实验结果表明，MoA-Off相比传统方法能够实现超过30%的延迟降低和30%-65%的资源开销减少，同时保持竞争力的准确性。

Conclusion: MoA-Off框架通过模态感知和自适应卸载策略，有效解决了MLLM在边缘计算环境中的部署挑战，为资源受限场景提供了高效的推理解决方案。

Abstract: Multimodal large language models (MLLMs) enable powerful cross-modal
inference but impose significant computational and latency burdens, posing
severe challenges for deployment in resource-constrained environments. In this
paper, we propose MoA-Off, an adaptive heterogeneous modality-aware offloading
framework with edge-cloud collaboration for efficient MLLM inference. MoA-Off
introduces a lightweight heterogeneous modality-aware module that estimates the
complexity of heterogeneous inputs through multi-dimensional feature analysis.
Then, an adaptive edge-cloud collaborative offloading strategy is proposed that
dynamically schedules workloads between edge and cloud based on modality-aware
complexity scores and real-time system states. The experimental results
demonstrate that MoA-Off can achieve over 30% reduction in latency and 30%-65%
decrease in resource overhead while maintaining competitive accuracy compared
to traditional approaches.

</details>


### [11] [Institutional Research Computing Capabilities in Australia: 2024](https://arxiv.org/abs/2509.17351)
*Slava Kitaeff,Luc Betbeder-Matibet,Jake Carroll,Stephen Giugni,David Abramson,John Zaitseff,Sarah Walters,David Powell,Chris Bording,Trung Nguyen,Angus Macoustra,Fabien Voisin,Bowen Chen,Jarrod Hurley*

Main category: cs.DC

TL;DR: 澳大利亚机构研究计算基础设施在支持研究生态系统方面发挥关键作用，拥有超过11.2万CPU核心和2241个GPU，服务6000多名研究人员，总价值约1.44亿澳元。


<details>
  <summary>Details</summary>
Motivation: 分析澳大利亚大学和组织的计算能力，展示机构系统如何通过本地计算资源、专用硬件和集群解决方案支持研究卓越。

Method: 基于多个机构的详细数据，分析部署模式、利用率和与研究优先事项的战略对齐情况。

Result: 发现机构资源为数据密集型项目提供关键支持，促进培训和研究生研究，支持原型开发，并确保数据主权合规。这些设施利用国家投资同时满足机构特定需求。

Conclusion: 机构能力的战略投资通过提高研究生产力、加强研究生培训和改善成果带来显著回报。研究为组织规划计算策略提供见解，强调在保持国家设施的同时维护强大机构资源的重要性。

Abstract: Institutional research computing infrastructure plays a vital role in
Australia's research ecosystem, complementing and extending national
facilities. This paper analyses research computing capabilities across
Australian universities and organisations, showing how institutional systems
support research excellence through local compute resources, specialised
hardware, and cluster solutions. Our study finds that nearly 112,258 CPU cores
and 2,241 GPUs serve over 6,000 researchers as essential bridges between
desktops and national facilities, enabling workflows from development to
large-scale computations. The estimated replacement value of this
infrastructure is $144M AUD. Drawing on detailed data from multiple
institutions, we identify key patterns in deployment, utilisation, and
strategic alignment with research priorities. Institutional resources provide
critical support for data-intensive projects, facilitate training and
higher-degree student research, enable prototyping and development, and ensure
data sovereignty compliance when required. The analysis shows how these
facilities leverage national investments while addressing institution-specific
needs that national systems cannot meet. We present evidence that strategic
investment in institutional capabilities yields significant returns through
greater research productivity, enhanced graduate training, and improved
outcomes. The study offers insights for organisations planning computing
strategies and highlights the importance of maintaining robust institutional
resources alongside national facilities.

</details>


### [12] [Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via Partially Disaggregated Prefill](https://arxiv.org/abs/2509.17357)
*Yunzhao Liu,Qiang Xu,Y. Charlie Hu*

Main category: cs.DC

TL;DR: Cronus是一个针对异构GPU集群的LLM推理系统，通过部分解耦预填充阶段来动态平衡工作负载，在保持高吞吐量的同时显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前在异构GPU集群中，解耦预填充策略由于GPU能力与工作负载需求不匹配导致性能不佳，而传统数据并行和流水线并行方法在异构环境下推理延迟过高。

Method: Cronus采用部分解耦预填充方法，将每个预填充阶段分区，初始部分在低端GPU上执行，同时将剩余预填充和早期请求的解码阶段在高端GPU上重叠执行。

Result: 在各种高端和低端GPU组合的广泛评估中，Cronus相比解耦预填充显著提高了吞吐量，同时相比DP和PP显著降低了TTFT P99和TBT P99延迟。

Conclusion: Cronus在异构GPU集群中实现了工作负载的动态平衡，在保持或改善吞吐量的同时显著降低了关键延迟指标。

Abstract: Efficient LLM inference is critical for real-world applications, especially
within heterogeneous GPU clusters commonly found in organizations and
on-premise datacenters as GPU architecture rapidly evolves. Current
disaggregated prefill strategies, which separate the prefill and decode stages
of LLM inference across different GPUs, often suffer from suboptimal
performance due to imbalances between GPU capabilities and workload demands. On
the other hand, extending conventional data parallelism and pipeline
parallelism to heterogeneous setups incurs high inference latencies. To address
these challenges, we introduce Cronus, a novel LLM inference system designed to
dynamically balance workloads across heterogeneous GPUs using partially
disaggregated prefill. Cronus partitions each prefill stage and executes its
initial portion on the low-end GPU, while overlapping the remaining prefill and
decode stages of earlier requests on the high-end GPU. Extensive evaluations
across various high-end and low-end GPU combinations demonstrate that Cronus
significantly improves the throughput over disaggregated prefill. It also
reduces TTFT P99 and TBT P99 significantly over DP and PP while maintaining
similar or better throughput.

</details>


### [13] [Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access](https://arxiv.org/abs/2509.17360)
*Chaoyi Ruan,Chao Bi,Kaiwen Zheng,Ziji Shi,Xinyi Wan,Jialin Li*

Main category: cs.DC

TL;DR: Asteria是一个针对LLM代理的跨区域知识缓存架构，通过语义元素和语义检索索引实现高效的知识重用，显著提升吞吐量而不影响准确性。


<details>
  <summary>Details</summary>
Motivation: LLM代理在处理数据密集型任务时需要频繁与远程知识源交互，导致延迟和成本瓶颈。现有缓存方案仅支持精确匹配查询，无法有效支持语义知识重用。

Method: Asteria引入语义元素（SE）和语义检索索引（Sine）两个抽象。SE包含查询的语义嵌入表示和性能感知元数据；Sine提供两阶段检索：向量相似索引快速候选选择和轻量级LLM语义判断器精确验证。

Result: 在代表性搜索工作负载上，Asteria实现吞吐量提升3.6倍，缓存命中率超过85%，同时保持与非缓存基线几乎相同的准确性。在复杂编码任务中吞吐量提升20%。

Conclusion: Asteria通过语义感知缓存机制有效解决了LLM代理的知识访问瓶颈，在保持正确性的同时显著提升了性能，适用于多样化的代理工作负载。

Abstract: Large Language Model (LLM) agents tackle data-intensive tasks such as deep
research and code generation. However, their effectiveness depends on frequent
interactions with knowledge sources across remote clouds or regions. Such
interactions can create non-trivial latency and cost bottlenecks. Existing
caching solutions focus on exact-match queries, limiting their effectiveness
for semantic knowledge reuse.
  To address this challenge, we introduce Asteria, a novel cross-region
knowledge caching architecture for LLM agents. At its core are two
abstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A
semantic element captures the semantic embedding representation of an LLM query
together with performance-aware metadata such as latency, cost, and staticity.
Sine then provides two-stage retrieval: a vector similar index with semantic
embedding for fast candidate selection and a lightweight LLM-powered semantic
judger for precise validation. Atop these primitives, Asteria builds a new
cache interface that includes a new semantic-aware cache hit definition, a
cost-efficient eviction policy, and proactive prefetching. To reduce overhead,
Asteria co-locates the small LLM judger with the main LLM using adaptive
scheduling and resource sharing. Our evaluation demonstrates that Asteria
delivers substantial performance improvements without compromising correctness.
On representative search workloads, Asteria achieves up to a 3.6$\times$
increase in throughput by maintaining cache hit rates of over 85%, while
preserving accuracy virtually identical to non-cached baselines. Asteria also
improves throughput for complex coding tasks by 20%, showcasing its versatility
across diverse agentic workloads.

</details>


### [14] [Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory](https://arxiv.org/abs/2509.17388)
*Manel Lurbe,Miguel Avargues,Salvador Petit,Maria E. Gomez,Rui Yang,Guanhao Wang,Julio Sahuquillo*

Main category: cs.DC

TL;DR: 本文研究多级预取技术在混合内存系统中的优势，评估了HMC和HMC+L1两种预取方法对系统性能的影响。


<details>
  <summary>Details</summary>
Motivation: 大数据分析和机器学习等新兴应用需要大量主内存，而现有DRAM技术容量有限。混合内存控制器虽然提升了性能，但也加剧了内存延迟问题，因此需要多级预取技术来缓解延迟。

Method: 研究评估了两种关键预取方法：HMC（在片外混合内存控制器中集成预取器）和HMC+L1（在HMC基础上增加L1片上预取器），在乱序执行处理器上进行实验。

Result: HMC预取器的覆盖率和准确率分别超过60%和80%，而HMC+L1方法将片外预取器覆盖率提升至92%。整体性能从HMC的9%提升到HMC+L1的12%。

Conclusion: 片上缓存预取器对于最大化片外预取效益至关重要，两者结合能显著提升系统性能，表明多级预取是解决混合内存系统延迟问题的有效方案。

Abstract: Emerging applications, such as big data analytics and machine learning,
require increasingly large amounts of main memory, often exceeding the capacity
of current commodity processors built on DRAM technology. To address this,
recent research has focused on off-chip memory controllers that facilitate
access to diverse memory media, each with unique density and latency
characteristics. While these solutions improve memory system performance, they
also exacerbate the already significant memory latency. As a result,
multi-level prefetching techniques are essential to mitigate these extended
latencies.
  This paper investigates the advantages of prefetching across both sides of
the memory system: the off-chip memory and the on-chip cache hierarchy. Our
primary objective is to assess the impact of a multi-level prefetching engine
on overall system performance. Additionally, we analyze the individual
contribution of each prefetching level to system efficiency. To achieve this,
the study evaluates two key prefetching approaches: HMC (Hybrid Memory
Controller) and HMC+L1, both of which employ prefetching mechanisms commonly
used by processor vendors. The HMC approach integrates a prefetcher within the
off-chip hybrid memory controller, while the HMC+L1 approach combines this with
additional L1 on-chip prefetchers.
  Experimental results on an out-of-order execution processor show that on-chip
cache prefetchers are crucial for maximizing the benefits of off-chip
prefetching, which in turn further enhances performance. Specifically, the
off-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and
up to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher
coverage to as much as 92%. Consequently, overall performance increases from 9%
with the HMC approach to 12% when L1 prefetching is also employed.

</details>


### [15] [pBeeGees: A Prudent Approach to Certificate-Decoupled BFT Consensus](https://arxiv.org/abs/2509.17496)
*Kaiji Yang,Jingjing Zhang,Junyao Zheng,Qiwen Liu,Weigang Wu,Jieying Zhou*

Main category: cs.DC

TL;DR: 本文提出了pBeeGees算法，解决了BeeGees在流水线拜占庭容错共识中的安全和活性问题，首次在流水线BFT框架中同时实现安全性、活性和证书解耦。


<details>
  <summary>Details</summary>
Motivation: 现有流水线BFT共识协议受限于视图连续仲裁证书的要求，这影响了性能并在不利网络条件下产生活性漏洞。虽然BeeGees算法实现了证书解耦，但存在安全和活性问题。

Method: 提出pBeeGees算法，通过集成回溯和预提交验证解决无效区块问题，引入谨慎验证机制缓解空链问题，保持证书解耦且无额外计算开销。

Result: 实验证实pBeeGees显著降低了区块提交延迟，特别是在频繁停止故障下相比经典算法表现更优。

Conclusion: pBeeGees是首个在流水线BFT框架中同时实现安全性、活性和证书解耦的协议，解决了BeeGees的安全漏洞。

Abstract: Pipelined Byzantine Fault Tolerant (BFT) consensus is fundamental to
permissioned blockchains. However, many existing protocols are limited by the
requirement for view-consecutive quorum certificates (QCs). This constraint
impairs performance and creates liveness vulnerabilities under adverse network
conditions. Achieving "certificate decoupling"-committing blocks without this
requirement-is therefore a key research goal. While the recent BeeGees
algorithm achieves this, our work reveals that it suffers from security and
liveness issues. To address this problem, this paper makes two primary
contributions. First, we formally define these flaws as the Invalid Block
Problem and the Hollow Chain Problem. Second, we propose pBeeGees, a new
algorithm that addresses these issues while preserving certificate decoupling
with no additional computational overhead. To achieve this, pBeeGees integrates
traceback and pre-commit validation to solve the Invalid Block Problem.Further,
to mitigate the Hollow Chain Problem, we introduce a prudent validation
mechanism, which prevents unverified branches from growing excessively. To
summarize, pBeeGees is the first protocol to simultaneously achieve safety,
liveness, and certificate decoupling in a pipelined BFT framework. Experiments
confirm that our design significantly reduces block commit latency compared to
classic algorithms, particularly under frequent stopping faults.

</details>


### [16] [TACTFL: Temporal Contrastive Training for Multi-modal Federated Learning with Similarity-guided Model Aggregation](https://arxiv.org/abs/2509.17532)
*Guanxiong Sun,Majid Mirmehdi,Zahraa Abdallah,Raul Santos-Rodriguez,Ian Craddock,Telmo de Menezes e Silva Filho*

Main category: cs.DC

TL;DR: TACTFL是一个用于半监督多模态联邦学习的统一框架，通过模态无关的时间对比训练和相似性引导的模型聚合策略，解决了现实联邦学习中标记数据有限和多模态输入异质性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界联邦学习面临两个关键挑战：标记数据访问有限和存在异质多模态输入。需要开发能够同时处理半监督学习和多模态数据的联邦学习框架。

Method: TACTFL引入模态无关的时间对比训练方案，利用跨模态的时间对齐进行无标记数据的表示学习，并采用相似性引导的模型聚合策略，根据表示一致性动态加权客户端模型。

Result: 在包括视频、音频和可穿戴传感器等多种基准测试和模态上的广泛实验表明，TACTFL实现了最先进的性能。例如，在UCF101数据集上仅使用10%标记数据，TACTFL达到68.48%的top-1准确率，显著优于FedOpt基线的35.35%。

Conclusion: TACTFL为半监督多模态联邦学习提供了一个有效的解决方案，通过时间对比学习和智能模型聚合策略，在标记数据稀缺的情况下实现了优异的性能表现。

Abstract: Real-world federated learning faces two key challenges: limited access to
labelled data and the presence of heterogeneous multi-modal inputs. This paper
proposes TACTFL, a unified framework for semi-supervised multi-modal federated
learning. TACTFL introduces a modality-agnostic temporal contrastive training
scheme that conducts representation learning from unlabelled client data by
leveraging temporal alignment across modalities. However, as clients perform
self-supervised training on heterogeneous data, local models may diverge
semantically. To mitigate this, TACTFL incorporates a similarity-guided model
aggregation strategy that dynamically weights client models based on their
representational consistency, promoting global alignment. Extensive experiments
across diverse benchmarks and modalities, including video, audio, and wearable
sensors, demonstrate that TACTFL achieves state-of-the-art performance. For
instance, on the UCF101 dataset with only 10% labelled data, TACTFL attains
68.48% top-1 accuracy, significantly outperforming the FedOpt baseline of
35.35%. Code will be released upon publication.

</details>


### [17] [Disaggregated Prefill and Decoding Inference System for Large Language Model Serving on Multi-Vendor GPUs](https://arxiv.org/abs/2509.17542)
*Xing Chen,Rong Shi,Lu Zhao,Lingbin Wang,Xiao Jin,Yueqiang Chen,Hongfeng Sun*

Main category: cs.DC

TL;DR: 提出了一种基于异构GPU的P-D解耦推理系统，通过异构兼容传输模块解决不同厂商GPU的数据兼容问题，并使用联合优化算法获得最优部署方案。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模增大，需要高效的推理系统。当前研究主要在同类GPU上进行，缺乏基于业务场景的部署方案。使用异构GPU构建推理系统可以更好地提高资源利用率并降低成本。

Method: 设计基于异构GPU的P-D解耦推理系统，包含异构兼容传输模块解决数据兼容问题，并提出并行策略和实例数分配的联合优化算法。

Result: 实验结果表明，P-D解耦推理系统能很好地解决不同厂商异构GPU的混合推理问题，联合优化算法能获得最优部署方案。

Conclusion: 基于异构GPU的P-D解耦推理系统是解决大语言模型推理效率问题的有效方案，能提高资源利用率并降低成本。

Abstract: LLM-based applications have been widely used in various industries, but with
the increasing of models size, an efficient large language model (LLM)
inference system is an urgent problem to be solved for service providers. Since
the inference system is divided into two stage with different characteristics:
Prefill and Decode, the two stage will interfere with each other during the
inference process. Toward this end, a P-D disaggregated inference framework is
proposed by some researchers. Current research is done on homogeneous GPUs, and
lacks deployment solutions based on business scenarios. Compared with
homogeneous GPUs, using heterogeneous GPUs to construct inference systems can
better improve resource utilization and reduce costs. Even if GPUs from
different vendors are used to build inference systems, on the basis of reducing
costs, the resource utilization rate can be improved and the dependence on a
single vendor can be reduced. Therefore, a P-D disaggreagetd inference system
based on heterogeneous GPUs is designed, and the heterogeneous compatible
transmission module in the system is designed to address heterogeneous GPU data
compatibility issues. Then, a joint optimization algorithm of parallel strategy
and instance number allocation is proposed to obtain the deployment solutions.
Finally, the experimental results show that the P-D disaggregated inference
system can well solve the hybrid inference problem of heterogeneous GPUs from
different vendors, and the joint optimization algorithm can obtain the optimal
deployment solution.

</details>


### [18] [A Lightweight Approach for State Machine Replication](https://arxiv.org/abs/2509.17771)
*Christian Cachin,Jinfeng Dou,Christian Scheideler,Philipp Schneider*

Main category: cs.DC

TL;DR: 本文提出了一种轻量级的基于承诺证书的状态机复制解决方案，通过中位数规则和压缩提交命令信息来保证系统在服务器被阻塞情况下的安全性和活跃性。


<details>
  <summary>Details</summary>
Motivation: 现有的状态机复制解决方案在面对自适应服务器阻塞攻击时存在性能问题，特别是基于领导者的方法在关键服务器被攻击时容易失效。本文旨在设计一个完全去中心化、轻量级且能抵抗阻塞攻击的解决方案。

Method: 1. 将稳定共识问题中的简单中位数规则适配到客户端-服务器设置中；2. 通过压缩已提交命令的信息来保持协议轻量级；3. 支持从大规模阻塞攻击中快速恢复；4. 采用完全去中心化架构，不依赖领导者。

Result: 该方法在最多有恒定比例服务器被阻塞时保证活跃性，在任何数量服务器被阻塞时保证安全性，支持快速恢复，并在多个方面提供接近最优的性能。

Conclusion: 提出的解决方案在抵抗阻塞攻击方面优于基于领导者的方法，特别适用于面临内部拒绝服务攻击等安全威胁的环境，实现了安全性和性能的良好平衡。

Abstract: We present a lightweight solution for state machine replication with
commitment certificates. Specifically, we adapt a simple median rule from the
stabilizing consensus problem [Doerr11] to operate in a client-server setting
where arbitrary servers may be blocked adaptively based on past system
information. We further extend our protocol by compressing information about
committed commands, thus keeping the protocol lightweight, while still enabling
clients to easily prove that their commands have indeed been committed on the
shared state. Our approach guarantees liveness as long as at most a constant
fraction of servers are blocked, ensures safety under any number of blocked
servers, and supports fast recovery from massive blocking attacks. In addition
to offering near-optimal performance in several respects, our method is fully
decentralized, unlike other near-optimal solutions that rely on leaders. In
particular, our solution is robust against adversaries that target key servers
(which captures insider-based denial-of-service attacks), whereas leader-based
approaches fail under such a blocking model.

</details>


### [19] [Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale MoE Serving](https://arxiv.org/abs/2509.17863)
*Ziming Liu,Boyu Tian,Guoteng Wang,Zhen Jiang,Peng Sun,Zhenhua Han,Tian Tang,Xiaohe Hu,Yanmin Jia,Yan Zhang,He Liu,Mingjun Zhang,Yiqi Zhang,Qiaoling Chen,Shenggan Cheng,Mingyu Gao,Yang You,Siyuan Feng*

Main category: cs.DC

TL;DR: EaaS是一个新颖的MoE模型服务系统，通过解耦专家模块实现高效、可扩展和鲁棒的MoE部署


<details>
  <summary>Details</summary>
Motivation: 传统的密集架构系统无法有效处理MoE模型中动态稀疏的专家利用模式，导致服务不稳定

Method: 将MoE模块解耦为独立的无状态服务，采用高性能的CPU-free点对点通信库，实现细粒度资源扩展和故障容错

Result: EaaS在保持与单体系统相当性能的同时，提供强大的故障容错能力，在模拟硬件故障下仅产生不到2%的吞吐量下降，并通过动态细粒度适应节省37.5%的计算资源

Conclusion: EaaS系统证明了在大规模生产环境中部署MoE模型的强大韧性和效率优势

Abstract: Mixture-of-Experts (MoE) models challenge serving infrastructures with
dynamic, sparse expert utilization, causing instability on conventional systems
designed for dense architectures. We propose EaaS, a novel serving system to
enable efficient, scalable, and robust MoE deployment. Our system disaggregates
MoE modules into independent, stateless services. This design enables
fine-grained resource scaling and provides inherent fault tolerance by
decoupling compute units. The architecture is powered by a high-performance,
CPU-free peer-to-peer communication library that ensures minimal overhead and
high throughput. Experiments confirm EaaS's scalability and efficiency,
achieving performance comparable to monolithic systems while providing robust
fault tolerance and strong scalability. EaaS incurs less than a 2% throughput
reduction under simulated hardware failures that would otherwise halt
monolithic architectures. It further saves up to 37.5% of computing resources
through dynamic fine-grained adaptation to serving traffic, demonstrating
strong resilience for large-scale MoE deployment in production.

</details>


### [20] [XaaS Containers: Performance-Portable Representation With Source and IR Containers](https://arxiv.org/abs/2509.17914)
*Marcin Copik,Eiman Alnuaimi,Alok Kamatar,Valerie Hayot-Sasson,Alberto Madonna,Todd Gamblin,Kyle Chard,Ian Foster,Torsten Hoefler*

Main category: cs.DC

TL;DR: 本文提出了一种新的高性能计算容器方法——源和中间表示容器，通过延迟性能关键决策到部署时，实现性能可移植性，结合容器便利性和系统专用构建的性能优势。


<details>
  <summary>Details</summary>
Motivation: 高性能计算系统与云数据中心正在融合，容器成为默认的软件部署方法。但容器在HPC环境中面临性能挑战，因为为了可移植性必须牺牲硬件特定优化。现有HPC容器虽然可以使用运行时钩子访问优化的MPI库和GPU设备，但受限于ABI兼容性，无法克服早期编译决策的影响。

Method: 提出源和中间表示容器方法，延迟性能关键决策直到目标系统规格已知。分析HPC软件中的专业化机制，提出新的LLM辅助方法用于自动发现专业化。通过检查编译流水线，开发在部署时为目标架构构建优化容器的方法论。

Result: 原型演示表明，新的XaaS容器结合了容器化的便利性和系统专用构建的性能优势。

Conclusion: 通过源和中间表示容器实现了性能可移植的容器愿景，为HPC环境提供了既方便又高性能的容器解决方案。

Abstract: High-performance computing (HPC) systems and cloud data centers are
converging, and containers are becoming the default method of portable software
deployment. Yet, while containers simplify software management, they face
significant performance challenges in HPC environments as they must sacrifice
hardware-specific optimizations to achieve portability. Although HPC containers
can use runtime hooks to access optimized MPI libraries and GPU devices, they
are limited by application binary interface (ABI) compatibility and cannot
overcome the effects of early-stage compilation decisions. Acceleration as a
Service (XaaS) proposes a vision of performance-portable containers, where a
containerized application should achieve peak performance across all HPC
systems. We present a practical realization of this vision through Source and
Intermediate Representation (IR) containers, where we delay
performance-critical decisions until the target system specification is known.
We analyze specialization mechanisms in HPC software and propose a new
LLM-assisted method for automatic discovery of specializations. By examining
the compilation pipeline, we develop a methodology to build containers
optimized for target architectures at deployment time. Our prototype
demonstrates that new XaaS containers combine the convenience of
containerization with the performance benefits of system-specialized builds.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [21] [SnipSnap: A Joint Compression Format and Dataflow Co-Optimization Framework for Efficient Sparse LLM Accelerator Design](https://arxiv.org/abs/2509.17072)
*Junyi Wu,Chao Fang,Zhongfeng Wang*

Main category: cs.AR

TL;DR: SnipSnap是一个联合压缩格式和数据流协同优化的框架，用于高效稀疏LLM加速器设计，通过层次化压缩格式编码、自适应压缩引擎和渐进式协同搜索工作流，显著提升性能和能效。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模的不断增长，对计算和内存的需求日益加剧，使得高效推理成为关键挑战。虽然稀疏性可以降低这些成本，但现有的设计空间探索框架往往忽略了压缩格式这一在加速器上利用稀疏性的关键因素。

Method: SnipSnap引入了三个关键创新：(1)层次化压缩格式编码以扩展设计空间；(2)自适应压缩引擎用于在不同稀疏度下选择格式；(3)渐进式协同搜索工作流，联合优化数据流和压缩格式。

Result: SnipSnap通过格式优化实现了18.24%的平均内存能耗节省，同时在Sparseloop和DiMO-Sparse框架上分别实现了2248.3倍和21.0倍的加速比。

Conclusion: SnipSnap框架通过联合优化压缩格式和数据流，为稀疏LLM加速器设计提供了高效的解决方案，显著提升了性能和能效。

Abstract: The growing scale of large language models (LLMs) has intensified demands on
computation and memory, making efficient inference a key challenge. While
sparsity can reduce these costs, existing design space exploration (DSE)
frameworks often overlook compression formats, a key factor for leveraging
sparsity on accelerators. This paper proposes SnipSnap, a joint compression
format and dataflow co-optimization framework for efficient sparse LLM
accelerator design. SnipSnap introduces: (1) a hierarchical compression format
encoding to expand the design space; (2) an adaptive compression engine for
selecting formats under diverse sparsity; and (3) a progressive co-search
workflow that jointly optimizes dataflow and compression formats. SnipSnap
achieves 18.24\% average memory energy savings via format optimization, along
with 2248.3$\times$ and 21.0$\times$ speedups over Sparseloop and DiMO-Sparse
frameworks, respectively.

</details>


### [22] [Overcoming challenges in bamboo connections: A review of mechanical properties and structural considerations](https://arxiv.org/abs/2509.17721)
*Pierre Boucher,Victor Fréchard,Diego Ramirez-Cardona,Claudiane Ouellet-Plamondon*

Main category: cs.AR

TL;DR: 本文对竹结构连接设计进行了全面综述，分析了影响连接性能的关键参数，批判性评估了现有的连接分类方法，并指出了研究空白和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 竹子作为可持续建筑材料具有快速生长、天然优化形状、高机械性能等优势，但其在结构应用中的使用仍然有限，主要原因是连接行为知识不足，这影响了竹结构的长期可靠性和性能。

Method: 通过文献综述的方法，综合分析了竹结构连接设计的关键因素，包括竹子的解剖、几何和机械特性，结构设计的机械要求，以及建筑方法。特别对Janssen基于力传递模式的竹连接分类及其后续改进进行了批判性分析。

Result: 识别了连接设计过程中的关键相互作用参数，揭示了现有连接分类方法的局限性，并明确了当前研究中的空白领域。

Conclusion: 需要开发集成设计方法并制定支持性指南，以促进竹子在建筑中的更广泛应用，确保竹结构连接的可靠性和性能。

Abstract: Over the past decades, bamboo has increasingly gained attention as a
sustainable construction material, through its rapid growth, naturally
optimized shape, high mechanical properties, and significant environmental
benefits. However, despite these advantages, the use of bamboo in its natural
form for structural applications remains limited, partly due to insufficient
knowledge of connection behavior, which is crucial for ensuring the long-term
reliability and performance of bamboo structures. This article provides a
comprehensive review of the key factors to consider in the design of structural
bamboo connections and discusses the existing connection classification methods
used as guidelines by designers. By synthesizing findings from the literature,
our research aims to identify the key parameters interacting with the
connection design process, focusing on the anatomical, geometric, and
mechanical properties of bamboo, the mechanical requirements of the structure
design, and the building methods. A critical analysis of Janssen's
classification of bamboo connections, based on force transfer modes and later
refined by Widyowijatnoko, is presented. Finally, we discuss the identified
research gaps and emphasize the need for integrated design approaches supported
by guidelines to support the broader adoption of bamboo in construction.

</details>


### [23] [Minimal Neuron Circuits: Bursters](https://arxiv.org/abs/2509.17731)
*Amr Nabil,T. Nandha Kumar,Haider Abbas F. Almurib*

Main category: cs.AR

TL;DR: 提出了一种使用最少组件设计生物合理爆发神经元电路的新方法，基于模仿I_Na,p+I_K+I_K(M)模型而非传统模型


<details>
  <summary>Details</summary>
Motivation: 传统神经元模型如Hodgkin-Huxley等不适合设计爆发电路，需要找到能模拟爆发动力学的合适模型

Method: 提出设计方法模仿I_Na,p+I_K+I_K(M)模型的定性特征，开发了两个基于MOSFET的爆发电路，使用神经爆发分析方法验证

Result: 电路快子系统的零斜线和分岔图与目标模型定性等价，能够展现多样的爆发行为，分岔类型影响爆发特征

Conclusion: 主要贡献在于提出构建爆发神经元电路的方法论，而非具体电路实现

Abstract: This work introduces a novel methodology for designing biologically plausible
bursting neuron circuits using a minimal number of components. We hypothesize
that to design circuits capable of bursting, the neuron circuit design must
mimic a neuron model that inherently exhibits bursting dynamics. Consequently,
classical models such as the Hodgkin-Huxley, $I_{Na,p}+I_{K}$, and
FitzHugh-Nagumo models are not suitable choices. Instead, we propose a
methodology for designing neuron circuits that emulate the qualitative
characteristics of the $I_{Na,p}+I_{K}+I_{K(M)}$ model, a well-established
minimal bursting neuron model. Based on this methodology, we present two novel
MOSFET-based circuits that exhibit bursting. Using the method of dissection of
neural bursting, we demonstrate that the nullcline and bifurcation diagrams of
the fast subsystem in our circuits are qualitatively equivalent to those of the
$I_{Na,p}+I_{K}+I_{K(M)}$ model. Furthermore, we examine the effect of the type
of bifurcation at burst initiation and termination on the bursting
characteristics, showing that our circuits can exhibit diverse bursting
behaviours. Importantly, the main contribution of this work lies not in the
specific circuit implementation, but in the methodology proposed for
constructing bursting neuron circuits.

</details>
