<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Refactoring and Equivalence in Rust: Expanding the REM Toolchain with a Novel Approach to Automated Equivalence Proofs](https://arxiv.org/abs/2601.19207)
*Matthew Britton,Sasha Pak,Alex Potanin*

Main category: cs.PL

TL;DR: REM2.0是一个基于rust-analyzer的Rust提取函数重构工具链，提供低延迟重构、自动修复借用检查器问题，并可选择性地生成Coq等价证明来验证行为保留。


<details>
  <summary>Details</summary>
Motivation: Rust的所有权、借用和高级类型特性使得自动化提取函数重构具有挑战性。现有工具要么依赖缓慢的编译器分析，要么只支持受限的语言片段，或者仅提供"仍然编译"的有限保证。

Method: REM2.0基于rust-analyzer作为持久守护进程运行，提供VSCode前端。包含自动调整生命周期和签名的修复器，以及可选的验证管道（连接CHARON和AENEAS）为支持的Rust子集生成Coq等价证明。

Result: 在三个基准测试套件上评估：1) 原始REM工件上实现100%兼容性，延迟从~1000ms降至个位数毫秒；2) 在40个GitHub高星仓库的提取案例中，能处理async/await、const fn、非局部控制流、泛型等复杂特性；3) 验证基准上，CHARON/AENEAS管道为当前子集构建端到端等价证明。

Conclusion: 基于rust-analyzer的设计能够为真实Rust程序提供快速、功能丰富的提取函数重构，而可选验证则提供机器检查的行为保留保证。

Abstract: Refactoring tools are central to modern development, with extract-function refactorings used heavily in day-to-day work. For Rust, however, ownership, borrowing, and advanced type features make automated extract-function refactoring challenging. Existing tools either rely on slow compiler-based analysis, support only restricted language fragments, or provide little assurance beyond "it still compiles." This paper presents REM2.0, a new extract-function and verification toolchain for Rust. REM2.0 works atop rust-analyzer as a persistent daemon, providing low-latency refactorings with a VSCode front-end. It adds a repairer that automatically adjusts lifetimes and signatures when extraction exposes borrow-checker issues, and an optional verification pipeline connecting to CHARON and AENEAS to generate Coq equivalence proofs for a supported Rust subset. The architecture is evaluated on three benchmark suites. On the original REM artefact, REM2.0 achieves 100% compatibility while reducing latency from ~1000ms to single-digit milliseconds in the daemon. On 40 feature-focused extractions from 20 highly starred GitHub repositories, REM2.0 handles most examples involving async/await, const fn, non-local control flow, generics, and higher-ranked trait bounds. On twenty verification benchmarks, the CHARON/AENEAS pipeline constructs end-to-end equivalence proofs for cases within its current subset. Overall, results show that a rust-analyzer-based design can provide fast, feature-rich extract-function refactoring for real Rust programs, while opt-in verification delivers machine-checked behaviour preservation.

</details>


### [2] [For Generalised Algebraic Theories, Two Sorts Are Enough](https://arxiv.org/abs/2601.19426)
*Samy Avrillon,Ambrus Kaposi,Ambroise Lafont,Niyousha Najmaei,Johann Rosain*

Main category: cs.PL

TL;DR: 该论文提出了一种将任意广义代数理论（GAT）简化为仅有两个排序的GAT的方法，并建立了原始模型与简化模型之间的严格共反射对应关系。


<details>
  <summary>Details</summary>
Motivation: 广义代数理论（GATs）允许排序相互索引，例如范畴论和Martin-Löf类型理论。然而，复杂的排序结构和相互索引使得实现和形式化变得困难，特别是在Cubical Agda等类型论元理论中，某些构造不被允许。

Method: 采用语义方法，不依赖GAT的语法描述，而是基于Uemura在具有可指数化态射的有限完备范畴的2-范畴中对（有限）GAT范畴的双初始刻画。通过严格共反射将任意GAT简化为仅有两个排序的GAT。

Result: 证明了任何GAT都可以简化为仅有两个排序的GAT，并建立了原始模型与简化模型之间的严格共反射对应关系。简化后的GAT消除了排序等式、排序与操作的交错构造等复杂性。

Conclusion: 该简化方法为在Cubical Agda等类型论元理论中实现具有排序等式或交错构造的商归纳-归纳类型（QIITs）提供了途径，是互归纳类型归约为单一索引族方法的推广。

Abstract: Generalised algebraic theories (GATs) allow multiple sorts indexed over each other. For example, the theories of categories or Martin-L{ö}f type theories form GATs. Categories have two sorts, objects and morphisms, and the latter are double-indexed over the former. Martin-L{ö}f type theory has four sorts: contexts, substitutions, types and terms. For example, types are indexed over contexts, and terms are indexed over both contexts and types. In this paper we show that any GAT can be reduced to a GAT with only two sorts, and there is a section-retraction correspondence (formally, a strict coreflection) between models of the original and the reduced GAT. In particular, any model of the original GAT can be turned into a model of the reduced (two-sorted) GAT and back, and this roundtrip is the identity.
  The reduced GAT is simpler than the original GAT in the following aspects: it does not have sort equalities; it does not have interleaved sorts and operations; if the original GAT did not have interleaved sorts and operations, then the reduced GAT won't have operations interleaved between different sorts. In a type-theoretic metatheory, the initial algebra of a GAT is called a quotient inductive-inductive type (QIIT). Our reduction provides a way to implement QIITs with sort equalities or interleaved constructors which are not allowed by Cubical Agda. An instance of our reduction is the well-known method of reducing mutual inductive types to a single indexed family. Our approach is semantic in that it does not rely on a syntactic description of GATs, but instead, on Uemura's bi-initial characterisation of the category of (finite) GATs in the 2-category of finitely complete categories with a chosen exponentiable morphism.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Trustworthy Scheduling for Big Data Applications](https://arxiv.org/abs/2601.18983)
*Dimitrios Tomaras,Vana Kalogeraki,Dimitrios Gunopulos*

Main category: cs.DC

TL;DR: X-Sched是一个中间件，使用可解释性技术为容器化环境中的任务执行提供可操作的资源配置指导，帮助满足SLOs。


<details>
  <summary>Details</summary>
Motivation: 现有调度器虽然优化性能指标，但决策过程不透明，无法为开发者提供明确指导来满足服务级别目标。

Method: 集成反事实解释与随机森林等机器学习模型，高效识别最优资源配置，提供可操作的调度决策解释。

Result: 实验验证显示该方法高效、有益且实用，能确保任务按性能目标执行，并为用户提供清晰的决策依据。

Conclusion: X-Sched填补了调度器透明性不足的空白，通过可解释性技术提供可操作的资源配置指导，提升容器化环境任务执行的可行性。

Abstract: Recent advances in modern containerized execution environments have resulted in substantial benefits in terms of elasticity and more efficient utilization of computing resources. Although existing schedulers strive to optimize performance metrics like task execution times and resource utilization, they provide limited transparency into their decision-making processes or the specific actions developers must take to meet Service Level Objectives (SLOs). In this work, we propose X-Sched, a middleware that uses explainability techniques to generate actionable guidance on resource configurations that makes task execution in containerized environments feasible, under resource and time constraints. X-Sched addresses this gap by integrating counterfactual explanations with advanced machine learning models, such as Random Forests, to efficiently identify optimal configurations. This approach not only ensures that tasks are executed in line with performance goals but also gives users clear, actionable insights into the rationale behind scheduling decisions. Our experimental results validated with data from real-world execution environments, illustrate the efficiency, benefits and practicality of our approach.

</details>


### [4] [Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers](https://arxiv.org/abs/2601.19092)
*Bohan Hou,Hongyi Jin,Guanjie Wang,Jinqi Chen,Yaxing Cai,Lijie Yang,Zihao Ye,Yaoyao Ding,Ruihang Lai,Tianqi Chen*

Main category: cs.DC

TL;DR: Axe Layout是一个硬件感知的抽象层，通过命名轴将逻辑张量坐标映射到多轴物理空间，统一了跨设备分布和设备内布局的平铺、分片、复制和偏移操作。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习工作负载需要在设备网格、内存层次结构和异构加速器之间协调数据和计算的位置，这需要统一的抽象来处理跨设备分布和设备内布局的复杂性。

Method: 提出Axe Layout抽象，通过命名轴映射逻辑张量到物理空间；基于Axe设计多粒度、分布感知的DSL和编译器，在单个内核中组合线程本地控制和集体操作符。

Result: 实验表明，这种统一方法能够在最新的GPU设备、多设备环境和加速器后端上实现接近手工调优内核的性能。

Conclusion: Axe Layout提供了一个统一的硬件感知抽象，能够有效协调深度学习工作负载在复杂硬件环境中的数据和计算分布，实现接近手工优化的性能。

Abstract: Scaling modern deep learning workloads demands coordinated placement of data and compute across device meshes, memory hierarchies, and heterogeneous accelerators. We present Axe Layout, a hardware-aware abstraction that maps logical tensor coordinates to a multi-axis physical space via named axes. Axe unifies tiling, sharding, replication, and offsets across inter-device distribution and on-device layouts, enabling collective primitives to be expressed consistently from device meshes to threads. Building on Axe, we design a multi-granularity, distribution-aware DSL and compiler that composes thread-local control with collective operators in a single kernel. Experiments show that our unified approach can bring performance close to hand-tuned kernels on across latest GPU devices and multi-device environments and accelerator backends.

</details>


### [5] [KUBEDIRECT: Unleashing the Full Power of the Cluster Manager for Serverless Computing](https://arxiv.org/abs/2601.19160)
*Sheng Qi,Zhiquan Zhang,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: KUBEDIRECT 是一个基于 Kubernetes 的 FaaS 集群管理器，通过绕过 API Server 的直接消息传递提升效率，同时保持与现有生态系统的兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有的 FaaS 平台依赖 Kubernetes 进行资源管理，但在处理突发 FaaS 实例扩展时，控制器之间通过 API Server 的消息传递成为主要瓶颈。现有解决方案需要重新设计集群管理器，但会破坏与现有生态系统的兼容性并需要大量工程投入。

Method: KUBEDIRECT 发现 FaaS 平台中存在一个共同的"窄腰"结构，其顺序特性消除了对单一事实来源的需求，允许绕过 API Server 进行直接消息传递。系统采用新颖的状态管理方案，将窄腰作为分层写回缓存，确保一致性和收敛到期望状态。

Result: KUBEDIRECT 能够无缝集成到 Kubernetes 中，每个控制器仅需增加约 150 行代码。实验表明，KUBEDIRECT 相比 Knative 减少了 26.7 倍的延迟，性能与最先进的重新设计平台 Dirigent 相当。

Conclusion: KUBEDIRECT 通过在保持与 Kubernetes 生态系统兼容性的同时，通过绕过 API Server 的直接通信机制，显著提升了 FaaS 平台的性能，实现了效率与兼容性的平衡。

Abstract: FaaS platforms rely on cluster managers like Kubernetes for resource management. Kubernetes is popular due to its state-centric APIs that decouple the control plane into modular controllers. However, to scale out a burst of FaaS instances, message passing becomes the primary bottleneck as controllers have to exchange extensive state through the API Server. Existing solutions opt for a clean-slate redesign of cluster managers, but at the expense of compatibility with existing ecosystem and substantial engineering effort.
  We present KUBEDIRECT, a Kubernetes-based cluster manager for FaaS. We find that there exists a common narrow waist across FaaS platform that allows us to achieve both efficiency and external compatibility. Our insight is that the sequential structure of the narrow waist obviates the need for a single source of truth, allowing us to bypass the API Server and perform direct message passing for efficiency. However, our approach introduces a set of ephemeral states across controllers, making it challenging to enforce end-to-end semantics due to the absence of centralized coordination. KUBEDIRECT employs a novel state management scheme that leverages the narrow waist as a hierarchical write-back cache, ensuring consistency and convergence to the desired state. KUBEDIRECT can seamlessly integrate with Kubernetes, adding ~150 LoC per controller. Experiments show that KUBEDIRECT reduces serving latency by 26.7x over Knative, and has similar performance as the state-of-the-art clean-slate platform Dirigent.

</details>


### [6] [Revisiting Parameter Server in LLM Post-Training](https://arxiv.org/abs/2601.19362)
*Xinyi Wan,Penghui Qi,Guangxing Huang,Chaoyi Ruan,Min Lin,Jialin Li*

Main category: cs.DC

TL;DR: ODC将参数服务器范式融入FSDP，用点对点通信替代集体通信，解决LLM后训练中序列长度差异导致的负载不均衡问题，提升设备利用率和训练吞吐量


<details>
  <summary>Details</summary>
Motivation: 传统数据并行训练依赖集体通信，假设负载均衡。但在大语言模型后训练中，序列长度差异大导致负载不均衡，集体通信会造成同步障碍，使负载较小的设备利用率低下

Method: 提出按需通信(ODC)，将参数服务器范式融入全分片数据并行(FSDP)，用直接点对点通信替代集体all-gather和reduce-scatter操作，将同步障碍从每层一次减少到每小批次一次

Result: 在各种LLM后训练任务中，ODC持续提升设备利用率和训练吞吐量，相比标准FSDP实现高达36%的加速

Conclusion: ODC更适合LLM后训练中普遍存在的负载不均衡场景，是比传统集体通信更优的选择，相关实现已开源

Abstract: Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose \textbf{On-Demand Communication (ODC)}, which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc.

</details>


### [7] [Modular Foundation Model Inference at the Edge: Network-Aware Microservice Optimization](https://arxiv.org/abs/2601.19563)
*Juan Zhu,Zixin Wang,Shenghui Song,Jun Zhang,Khaled Ben Letaief*

Main category: cs.DC

TL;DR: 提出基于微服务的FM推理框架，通过核心服务与轻量服务的功能不对称性，实现边缘环境下的高效部署，在资源受限和网络不确定条件下保证服务质量。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型主要部署在云端，无法满足实时响应和用户隐私需求；而边缘设备资源有限且网络动态不确定，单一部署方案不可行。

Method: 采用两层部署策略：1) 核心服务通过长期网络感知整数规划静态部署，形成容错骨干；2) 轻量服务通过结合有效容量理论和Lyapunov优化的低复杂度在线控制器动态编排。

Result: 模拟实验显示，该框架平均按时任务完成率超过84%，部署成本适中，且随着系统负载增加仍保持强鲁棒性。

Conclusion: 提出的微服务框架有效解决了FM在边缘环境部署的挑战，通过功能不对称性和分层部署策略，在资源受限条件下实现了可靠的服务质量保证。

Abstract: Foundation models (FMs) unlock unprecedented multimodal and multitask intelligence, yet their cloud-centric deployment precludes real-time responsiveness and compromises user privacy. Meanwhile, monolithic execution at the edge remains infeasible under stringent resource limits and uncertain network dynamics. To bridge this gap, we propose a microservice-based FM inference framework that exploits the intrinsic functional asymmetry between heavyweight core services and agile light services. Our two-tier deployment strategy ensures robust Quality of Service (QoS) under resource contention. Specifically, core services are placed statically via a long-term network-aware integer program with sparsity constraints to form a fault-tolerant backbone. On the other hand, light services are orchestrated dynamically by a low-complexity online controller that integrates effective capacity theory with Lyapunov optimization, providing probabilistic latency guarantees under real-time workload fluctuations. Simulations demonstrate that our framework achieves over 84% average on-time task completion with moderate deployment costs and maintains strong robustness as the system load scales.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [M$^{\text{2}}$XFP: A Metadata-Augmented Microscaling Data Format for Efficient Low-bit Quantization](https://arxiv.org/abs/2601.19213)
*Weiming Hu,Zihan Zhang,Haoyan Zhang,Chen Zhang,Cong Guo,Yu Feng,Tianchi Hu,Guanglin Li,Guipeng Hu,Junsong Wang,Jingwen Leng*

Main category: cs.AR

TL;DR: 提出M2XFP方法，通过灵活的元数据和在线量化编码，显著减少低比特量化中的精度损失，在硬件加速器上实现性能提升和能耗节省。


<details>
  <summary>Details</summary>
Motivation: 现有的低比特MX格式（如MXFP4）由于使用共享缩放因子和Power-of-Two格式，导致显著的精度下降。需要在保持高比特效率的同时，通过最小化元数据来恢复量化过程中的精度损失。

Method: 提出基于灵活元数据的算法-硬件协同设计，包括在线量化和简单编码。实现轻量级硬件单元并集成到加速器中，支持所提出的方法。

Result: 在LLM基准测试中，相比MXFP4平均减少70.63%的精度损失，相比最新的NVFP4减少37.30%的精度损失。设计实现最高1.91倍加速和1.75倍能耗节省。

Conclusion: M2XFP方法通过灵活的元数据设计和硬件协同优化，在保持高比特效率的同时显著减少了低比特量化的精度损失，为大规模语言模型的部署提供了有效的解决方案。

Abstract: Existing low-bit Microscaling (MX) formats, such as MXFP4, often suffer from substantial accuracy degradation due to the use of a shared scaling factor with the Power-of-Two format. In this work, we explore strategies that introduce minimal metadata to recover accuracy lost during quantization while maintaining high bit efficiency across a wide range of large language models. We propose a complete algorithm-hardware co-design based on flexible metadata, featuring an online quantization with simple encoding. To support the proposed method efficiently, we implement a lightweight hardware unit and integrate it into the accelerator. Evaluation results demonstrate that our method substantially narrows the accuracy gap, achieving on average a 70.63% reduction in accuracy loss compared to MXFP4 and a 37.30% reduction relative to the latest NVFP4 on LLM benchmarks. Furthermore, our design delivers up to 1.91$\times$ speedup and 1.75$\times$ energy savings over state-of-the-art accelerators. Our code is available at https://github.com/SJTU-ReArch-Group/M2XFP_ASPLOS26.

</details>


### [9] [A Reconfigurable Framework for AI-FPGA Agent Integration and Acceleration](https://arxiv.org/abs/2601.19263)
*Aybars Yunusoglu,Talha Coskun,Hiruna Vishwamith,Murat Isik,I. Can Dikmen*

Main category: cs.AR

TL;DR: AI FPGA Agent框架通过软件代理动态分区AI模型、调度硬件卸载和管理数据传输，结合可参数化加速器核心，在FPGA上实现高效AI推理，相比CPU实现10倍延迟降低，相比GPU实现2-3倍能效提升。


<details>
  <summary>Details</summary>
Motivation: AI在实时和能耗受限环境中的部署需求日益增长，传统CPU和GPU由于通用性设计在严格延迟或功耗预算下效率低下。FPGA通过定制并行性和硬件级优化提供有前景的替代方案，但AI工作负载到FPGA的映射因硬件-软件协同设计和数据编排的复杂性而具有挑战性。

Method: 提出AI FPGA Agent框架，包含运行时软件代理动态分区AI模型、调度计算密集型层进行硬件卸载、管理数据传输；硬件组件包括针对量化算术优化的可参数化加速器核心，实现高吞吐量推理。

Result: 相比CPU基线实现超过10倍的延迟降低，相比GPU实现2-3倍更高的能效，同时保持分类精度在完整精度参考的0.2%范围内。

Conclusion: AI-FPGA协同设计在可扩展、高能效AI部署方面具有巨大潜力，AI FPGA Agent框架简化了深度神经网络推理在FPGA上的集成和加速。

Abstract: Artificial intelligence (AI) is increasingly deployed in real-time and energy-constrained environments, driving demand for hardware platforms that can deliver high performance and power efficiency. While central processing units (CPUs) and graphics processing units (GPUs) have traditionally served as the primary inference engines, their general-purpose nature often leads to inefficiencies under strict latency or power budgets. Field-Programmable Gate Arrays (FPGAs) offer a promising alternative by enabling custom-tailored parallelism and hardware-level optimizations. However, mapping AI workloads to FPGAs remains challenging due to the complexity of hardware-software co-design and data orchestration. This paper presents AI FPGA Agent, an agent-driven framework that simplifies the integration and acceleration of deep neural network inference on FPGAs. The proposed system employs a runtime software agent that dynamically partitions AI models, schedules compute-intensive layers for hardware offload, and manages data transfers with minimal developer intervention. The hardware component includes a parameterizable accelerator core optimized for high-throughput inference using quantized arithmetic. Experimental results demonstrate that the AI FPGA Agent achieves over 10x latency reduction compared to CPU baselines and 2-3x higher energy efficiency than GPU implementations, all while preserving classification accuracy within 0.2% of full-precision references. These findings underscore the potential of AI-FPGA co-design for scalable, energy-efficient AI deployment.

</details>


### [10] [GenPairX: A Hardware-Algorithm Co-Designed Accelerator for Paired-End Read Mapping](https://arxiv.org/abs/2601.19384)
*Julien Eudine,Chu Li,Zhuo Cheng,Renzo Andri,Can Firtina,Mohammad Sadrosadati,Nika Mansouri Ghiasi,Konstantina Koliogeorgi,Anirban Nag,Arash Tavakkol,Haiyu Mao,Onur Mutlu,Shai Bergman,Ji Zhang*

Main category: cs.AR

TL;DR: GenPairX：一种硬件算法协同设计的加速器，通过联合考虑双端测序读段对的新型过滤算法和轻量级比对算法，显著提升双端读段比对的性能和能效


<details>
  <summary>Details</summary>
Motivation: 读段比对是基因组分析中的主要性能瓶颈，现有方法对双端读段效果有限。现有过滤器通常独立评估每个读段，过滤率较低，且无法有效处理双端读段

Method: 提出GenPairX硬件算法协同设计加速器：1）新型过滤算法联合考虑读段对中的两个读段以提高过滤效果；2）轻量级比对算法替代大部分计算昂贵的动态规划操作；3）两个专用硬件机制支持所提算法

Result: GenPairX相比最先进的解决方案实现显著性能提升：与领先的基于CPU的读段比对器相比，每瓦吞吐量提高1575倍；与基于加速器的读段比对器相比，每瓦吞吐量提高1.43倍，且不损失准确性

Conclusion: GenPairX通过硬件算法协同设计，有效解决了双端读段比对的性能瓶颈问题，在保持准确性的同时大幅提升了计算效率和能效

Abstract: Genome sequencing has become a central focus in computational biology. A genome study typically begins with sequencing, which produces millions to billions of short DNA fragments known as reads. Read mapping aligns these reads to a reference genome. Read mapping for short reads comes in two forms: single-end and paired-end, with the latter being more prevalent due to its higher accuracy and support for advanced analysis. Read mapping remains a major performance bottleneck in genome analysis due to expensive dynamic programming. Prior efforts have attempted to mitigate this cost by employing filters to identify and potentially discard computationally expensive matches and leveraging hardware accelerators to speed up the computations. While partially effective, these approaches have limitations. In particular, existing filters are often ineffective for paired-end reads, as they evaluate each read independently and exhibit relatively low filtering ratios. In this work, we propose GenPairX, a hardware-algorithm co-designed accelerator that efficiently minimizes the computational load of paired-end read mapping while enhancing the throughput of memory-intensive operations. GenPairX introduces: (1) a novel filtering algorithm that jointly considers both reads in a pair to improve filtering effectiveness, and a lightweight alignment algorithm to replace most of the computationally expensive dynamic programming operations, and (2) two specialized hardware mechanisms to support the proposed algorithms. Our evaluations show that GenPairX delivers substantial performance improvements over state-of-the-art solutions, achieving 1575x and 1.43x higher throughput per watt compared to leading CPU-based and accelerator-based read mappers, respectively, all without compromising accuracy.

</details>


### [11] [Veri-Sure: A Contract-Aware Multi-Agent Framework with Temporal Tracing and Formal Verification for Correct RTL Code Generation](https://arxiv.org/abs/2601.19747)
*Jiale Liu,Taiyu Zhou,Tianqi Jiang*

Main category: cs.AR

TL;DR: Veri-Sure是一个多智能体框架，通过设计契约对齐智能体意图，使用静态依赖切片指导的补丁机制进行精准修复，结合多分支验证管道实现超越纯仿真的功能正确性，在RTL代码生成上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前EDA领域使用LLM进行RTL设计面临三大瓶颈：1) 仿真中心评估的测试覆盖率和可靠性有限；2) 迭代调试引入回归和修复幻觉；3) 智能体交接时的语义漂移。需要解决这些硅级正确性问题。

Method: 提出Veri-Sure多智能体框架：1) 建立设计契约对齐智能体意图；2) 使用静态依赖切片指导的补丁机制进行精准局部修复；3) 集成多分支验证管道，结合轨迹驱动的时序分析和形式验证（断言检查和布尔等价证明）。

Result: 1) 扩展了VerilogEval-v2-EXT基准，增加53个工业级设计任务和分层难度级别；2) Veri-Sure在已验证正确的RTL代码生成性能上达到SOTA，超越独立LLM和先前智能体系统。

Conclusion: Veri-Sure通过设计契约对齐和多分支验证，解决了LLM在RTL设计中的硅级正确性瓶颈，为EDA领域的可靠AI驱动设计提供了有效框架。

Abstract: In the rapidly evolving field of Electronic Design Automation (EDA), the deployment of Large Language Models (LLMs) for Register-Transfer Level (RTL) design has emerged as a promising direction. However, silicon-grade correctness remains bottlenecked by: (i) limited test coverage and reliability of simulation-centric evaluation, (ii) regressions and repair hallucinations introduced by iterative debugging, and (iii) semantic drift as intent is reinterpreted across agent handoffs. In this work, we propose Veri-Sure, a multi-agent framework that establishes a design contract to align agents' intent and uses a patching mechanism guided by static dependency slicing to perform precise, localized repairs. By integrating a multi-branch verification pipeline that combines trace-driven temporal analysis with formal verification consisting of assertion-based checking and boolean equivalence proofs, Veri-Sure enables functional correctness beyond pure simulations. We also introduce VerilogEval-v2-EXT, extending the original benchmark with 53 more industrial-grade design tasks and stratified difficulty levels, and show that Veri-Sure achieves state-of-the-art verified-correct RTL code generation performance, surpassing standalone LLMs and prior agentic systems.

</details>
