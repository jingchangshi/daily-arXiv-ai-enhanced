{"id": "2510.07449", "categories": ["cs.AR", "C.1.0"], "pdf": "https://arxiv.org/pdf/2510.07449", "abs": "https://arxiv.org/abs/2510.07449", "authors": ["Georgia Antoniou", "Haris Volos", "Jawad Haj Yahya", "Yiannakis Sazeides"], "title": "How long can you sleep? Idle Time System Inefficiencies and Opportunities", "comment": "3 pages, 3 figures, accepted at the 1st International Workshop on\n  Data Center Energy Efficiency (DCEE2025) 2025", "summary": "This work introduces a model-based framework that reveals the idle\nopportunity of modern servers running latency-critical applications.\nSpecifically, three queuing models, M/M/1, cxM/M/1, and M/M/c, are used to\nestimate the theoretical idle time distribution at the CPU core and system\n(package) level. A comparison of the actual idleness of a real server and that\nfrom the theoretical models reveals significant missed opportunities to enter\ndeep idle states. This inefficiency is attributed to the idle-governor\ninaccuracy and the high latency to transition to/from legacy deep-idle states.\nThe proposed methodology offers the means for an early-stage design exploration\nand insights into idle time behavior and opportunities for varying server\nsystem configurations and load.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6392\u961f\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63ed\u793a\u8fd0\u884c\u5ef6\u8fdf\u5173\u952e\u5e94\u7528\u7684\u73b0\u4ee3\u670d\u52a1\u5668\u7684\u7a7a\u95f2\u673a\u4f1a\uff0c\u53d1\u73b0\u5b9e\u9645\u7cfb\u7edf\u4e0e\u7406\u8bba\u6a21\u578b\u76f8\u6bd4\u5b58\u5728\u663e\u8457\u7684\u7a7a\u95f2\u72b6\u6001\u5229\u7528\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u670d\u52a1\u5668\u5728\u8fd0\u884c\u5ef6\u8fdf\u5173\u952e\u5e94\u7528\u65f6\u5b58\u5728\u7a7a\u95f2\u673a\u4f1a\u672a\u88ab\u5145\u5206\u5229\u7528\u7684\u95ee\u9898\uff0c\u9700\u8981\u91cf\u5316\u5206\u6790\u8fd9\u79cd\u673a\u4f1a\u5e76\u8bc6\u522b\u6548\u7387\u4f4e\u4e0b\u7684\u539f\u56e0\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u6392\u961f\u6a21\u578b\uff08M/M/1\u3001cxM/M/1\u548cM/M/c\uff09\u6765\u4f30\u8ba1CPU\u6838\u5fc3\u548c\u7cfb\u7edf\u7ea7\u522b\u7684\u7406\u8bba\u7a7a\u95f2\u65f6\u95f4\u5206\u5e03\uff0c\u5e76\u5c06\u5b9e\u9645\u670d\u52a1\u5668\u7a7a\u95f2\u60c5\u51b5\u4e0e\u7406\u8bba\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6bd4\u8f83\u53d1\u73b0\u5b9e\u9645\u670d\u52a1\u5668\u4e0e\u7406\u8bba\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5b58\u5728\u5927\u91cf\u8fdb\u5165\u6df1\u5ea6\u7a7a\u95f2\u72b6\u6001\u7684\u673a\u4f1a\u88ab\u9519\u8fc7\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7a7a\u95f2\u7ba1\u7406\u5668\u7684\u51c6\u786e\u6027\u548c\u4f20\u7edf\u6df1\u5ea6\u7a7a\u95f2\u72b6\u6001\u8f6c\u6362\u5ef6\u8fdf\u95ee\u9898\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65e9\u671f\u8bbe\u8ba1\u63a2\u7d22\u63d0\u4f9b\u4e86\u65b9\u6cd5\uff0c\u80fd\u591f\u6d1e\u5bdf\u4e0d\u540c\u670d\u52a1\u5668\u7cfb\u7edf\u914d\u7f6e\u548c\u8d1f\u8f7d\u4e0b\u7684\u7a7a\u95f2\u65f6\u95f4\u884c\u4e3a\u548c\u673a\u4f1a\u3002"}}
{"id": "2510.07719", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.07719", "abs": "https://arxiv.org/abs/2510.07719", "authors": ["Parker Hao Tian", "Zahra Yousefijamarani", "Alaa Alameldeen"], "title": "DL-PIM: Improving Data Locality in Processing-in-Memory Systems", "comment": null, "summary": "PIM architectures aim to reduce data transfer costs between processors and\nmemory by integrating processing units within memory layers. Prior PIM\narchitectures have shown potential to improve energy efficiency and\nperformance. However, such advantages rely on data proximity to the processing\nunits performing computations. Data movement overheads can degrade PIM's\nperformance and energy efficiency due to the need to move data between a\nprocessing unit and a distant memory location. %they face challenges due to the\noverhead of transferring data from remote memory locations to processing units\ninside memory for computation. In this paper, we demonstrate that a large\nfraction of PIM's latency per memory request is attributed to data transfers\nand queuing delays from remote memory accesses. To improve PIM's data locality,\nwe propose DL-PIM, a novel architecture that dynamically detects the overhead\nof data movement, and proactively moves data to a reserved area in the local\nmemory of the requesting processing unit. DL-PIM uses a distributed\naddress-indirection hardware lookup table to redirect traffic to the current\ndata location. We propose DL-PIM implementations on two 3D stacked memories:\nHMC and HBM. While some workloads benefit from DL-PIM, others are negatively\nimpacted by the additional latency due to indirection accesses. Therefore, we\npropose an adaptive mechanism that assesses the cost and benefit of indirection\nand dynamically enables or disables it to prevent degrading workloads that\nsuffer from indirection. Overall, DL-PIM reduces the average memory latency per\nrequest by 54% in HMC and 50% in HBM which resulted in performance improvement\nof 15% for workloads with substantial data reuse in HMC and 5% in HBM. For all\nrepresentative workloads, DL-PIM achieved a 6% speedup in HMC and a 3% speedup\nin HBM, showing that DL-PIM enhances data locality and overall system\nperformance.", "AI": {"tldr": "DL-PIM\u662f\u4e00\u79cd\u65b0\u578bPIM\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u68c0\u6d4b\u6570\u636e\u79fb\u52a8\u5f00\u9500\u5e76\u4e3b\u52a8\u5c06\u6570\u636e\u79fb\u52a8\u5230\u672c\u5730\u5185\u5b58\u7684\u4fdd\u7559\u533a\u57df\u6765\u6539\u5584\u6570\u636e\u5c40\u90e8\u6027\uff0c\u4f7f\u7528\u5206\u5e03\u5f0f\u5730\u5740\u95f4\u63a5\u786c\u4ef6\u67e5\u627e\u8868\u91cd\u5b9a\u5411\u6d41\u91cf\u3002", "motivation": "\u4f20\u7edfPIM\u67b6\u6784\u867d\u7136\u80fd\u51cf\u5c11\u5904\u7406\u5668\u4e0e\u5185\u5b58\u95f4\u7684\u6570\u636e\u4f20\u8f93\u6210\u672c\uff0c\u4f46\u5176\u4f18\u52bf\u4f9d\u8d56\u4e8e\u6570\u636e\u4e0e\u5904\u7406\u5355\u5143\u7684\u90bb\u8fd1\u6027\u3002\u6570\u636e\u79fb\u52a8\u5f00\u9500\u4f1a\u964d\u4f4ePIM\u7684\u6027\u80fd\u548c\u80fd\u6548\uff0c\u56e0\u4e3a\u9700\u8981\u5c06\u6570\u636e\u4ece\u8fdc\u7a0b\u5185\u5b58\u4f4d\u7f6e\u79fb\u52a8\u5230\u5185\u5b58\u5185\u7684\u5904\u7406\u5355\u5143\u8fdb\u884c\u8ba1\u7b97\u3002", "method": "\u63d0\u51faDL-PIM\u67b6\u6784\uff0c\u52a8\u6001\u68c0\u6d4b\u6570\u636e\u79fb\u52a8\u5f00\u9500\uff0c\u4e3b\u52a8\u5c06\u6570\u636e\u79fb\u52a8\u5230\u8bf7\u6c42\u5904\u7406\u5355\u5143\u7684\u672c\u5730\u5185\u5b58\u4fdd\u7559\u533a\u57df\u3002\u4f7f\u7528\u5206\u5e03\u5f0f\u5730\u5740\u95f4\u63a5\u786c\u4ef6\u67e5\u627e\u8868\u91cd\u5b9a\u5411\u6d41\u91cf\u5230\u5f53\u524d\u6570\u636e\u4f4d\u7f6e\u3002\u5728HMC\u548cHBM\u4e24\u79cd3D\u5806\u53e0\u5185\u5b58\u4e0a\u5b9e\u73b0\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u673a\u5236\u8bc4\u4f30\u95f4\u63a5\u8bbf\u95ee\u7684\u6210\u672c\u548c\u6536\u76ca\uff0c\u52a8\u6001\u542f\u7528\u6216\u7981\u7528\u4ee5\u9632\u6b62\u5bf9\u67d0\u4e9b\u5de5\u4f5c\u8d1f\u8f7d\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "result": "DL-PIM\u5c06HMC\u4e2d\u6bcf\u4e2a\u8bf7\u6c42\u7684\u5e73\u5747\u5185\u5b58\u5ef6\u8fdf\u964d\u4f4e54%\uff0cHBM\u4e2d\u964d\u4f4e50%\u3002\u5bf9\u4e8e\u5177\u6709\u5927\u91cf\u6570\u636e\u91cd\u7528\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0cHMC\u6027\u80fd\u63d0\u534715%\uff0cHBM\u63d0\u53475%\u3002\u6240\u6709\u4ee3\u8868\u6027\u5de5\u4f5c\u8d1f\u8f7d\u5728HMC\u4e2d\u5b9e\u73b06%\u52a0\u901f\uff0cHBM\u4e2d\u5b9e\u73b03%\u52a0\u901f\u3002", "conclusion": "DL-PIM\u901a\u8fc7\u589e\u5f3a\u6570\u636e\u5c40\u90e8\u6027\u6709\u6548\u63d0\u9ad8\u4e86\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\uff0c\u7279\u522b\u662f\u57283D\u5806\u53e0\u5185\u5b58\u67b6\u6784\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u79fb\u52a8\u5f00\u9500\u548c\u5185\u5b58\u8bbf\u95ee\u5ef6\u8fdf\u3002"}}
{"id": "2510.08137", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.08137", "abs": "https://arxiv.org/abs/2510.08137", "authors": ["Anastasios Petropoulos", "Theodore Antonakopoulos"], "title": "A Scalable FPGA Architecture With Adaptive Memory Utilization for GEMM-Based Operations", "comment": null, "summary": "Deep neural network (DNN) inference relies increasingly on specialized\nhardware for high computational efficiency. This work introduces a\nfield-programmable gate array (FPGA)-based dynamically configurable accelerator\nfeaturing systolic arrays, high-bandwidth memory, and UltraRAMs. We present two\nprocessing unit (PU) configurations with different computing capabilities using\nthe same interfaces and peripheral blocks. By instantiating multiple PUs and\nemploying a heuristic weight transfer schedule, the architecture achieves\nnotable throughput efficiency over prior works. Moreover, we outline how the\narchitecture can be extended to emulate analog in-memory computing (AIMC)\ndevices to aid next-generation heterogeneous AIMC chip designs and investigate\ndevice-level noise behavior. Overall, this brief presents a versatile DNN\ninference acceleration architecture adaptable to various models and future FPGA\ndesigns.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eFPGA\u7684\u53ef\u914d\u7f6eDNN\u63a8\u7406\u52a0\u901f\u5668\uff0c\u91c7\u7528\u8109\u52a8\u9635\u5217\u3001\u9ad8\u5e26\u5bbd\u5185\u5b58\u548cUltraRAM\uff0c\u652f\u6301\u591a\u79cd\u5904\u7406\u5355\u5143\u914d\u7f6e\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u6743\u91cd\u4f20\u8f93\u8c03\u5ea6\u5b9e\u73b0\u9ad8\u6548\u541e\u5410\u91cf\uff0c\u5e76\u53ef\u6269\u5c55\u6a21\u62df\u5b58\u5185\u8ba1\u7b97\u8bbe\u5907\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u5bf9\u4e13\u7528\u786c\u4ef6\u8ba1\u7b97\u6548\u7387\u9700\u6c42\u7684\u589e\u957f\uff0c\u9700\u8981\u5f00\u53d1\u7075\u6d3b\u53ef\u914d\u7f6e\u7684\u52a0\u901f\u5668\u67b6\u6784\u6765\u9002\u5e94\u4e0d\u540c\u6a21\u578b\u548c\u672a\u6765FPGA\u8bbe\u8ba1\u3002", "method": "\u8bbe\u8ba1FPGA\u52a8\u6001\u53ef\u914d\u7f6e\u52a0\u901f\u5668\uff0c\u5305\u542b\u8109\u52a8\u9635\u5217\u3001\u9ad8\u5e26\u5bbd\u5185\u5b58\u548cUltraRAM\uff0c\u63d0\u4f9b\u4e24\u79cd\u4e0d\u540c\u8ba1\u7b97\u80fd\u529b\u7684\u5904\u7406\u5355\u5143\u914d\u7f6e\uff0c\u4f7f\u7528\u76f8\u540c\u63a5\u53e3\u548c\u5916\u56f4\u6a21\u5757\uff0c\u901a\u8fc7\u591a\u5904\u7406\u5355\u5143\u5b9e\u4f8b\u5316\u548c\u542f\u53d1\u5f0f\u6743\u91cd\u4f20\u8f93\u8c03\u5ea6\u3002", "result": "\u8be5\u67b6\u6784\u76f8\u6bd4\u5148\u524d\u5de5\u4f5c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u541e\u5410\u6548\u7387\u63d0\u5347\uff0c\u5e76\u80fd\u6269\u5c55\u6a21\u62df\u6a21\u62df\u5b58\u5185\u8ba1\u7b97\u8bbe\u5907\uff0c\u7528\u4e8e\u7814\u7a76\u8bbe\u5907\u7ea7\u566a\u58f0\u884c\u4e3a\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u4e2a\u9002\u7528\u4e8e\u5404\u79cd\u6a21\u578b\u548c\u672a\u6765FPGA\u8bbe\u8ba1\u7684\u901a\u7528DNN\u63a8\u7406\u52a0\u901f\u67b6\u6784\uff0c\u5177\u6709\u9ad8\u5ea6\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.08351", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.08351", "abs": "https://arxiv.org/abs/2510.08351", "authors": ["Qingxiu Liu", "Jiazhen Cai", "Siyuan Sheng", "Yuhui Chen", "Lu Tang", "Zhirong Shen", "Patrick P. C. Lee"], "title": "FMCache: File-System Metadata Caching in Programmable Switches", "comment": "14 pages", "summary": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage.", "AI": {"tldr": "FMCache\u662f\u4e00\u4e2a\u57fa\u4e8e\u53ef\u7f16\u7a0b\u4ea4\u6362\u673a\u7684\u6587\u4ef6\u7cfb\u7edf\u5143\u6570\u636e\u7f13\u5b58\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u4ea4\u6362\u673a\u6570\u636e\u5e73\u9762\u5904\u7406\u5143\u6570\u636e\u8bf7\u6c42\uff0c\u663e\u8457\u63d0\u5347\u5206\u5e03\u5f0f\u6587\u4ef6\u7cfb\u7edf\u6027\u80fd", "motivation": "\u5206\u5e03\u5f0f\u6587\u4ef6\u7cfb\u7edf\u4e2d\u591a\u5ba2\u6237\u7aef\u8bbf\u95ee\u5143\u6570\u636e\u65f6\uff0c\u5ba2\u6237\u7aef\u7f13\u5b58\u4f1a\u5e26\u6765\u663e\u8457\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u7ef4\u62a4\u5f00\u9500\u548c\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5143\u6570\u636e\u7ba1\u7406\u65b9\u6848", "method": "\u5229\u7528\u53ef\u7f16\u7a0b\u4ea4\u6362\u673a\u5728\u6570\u636e\u5e73\u9762\u76f4\u63a5\u5904\u7406\u6587\u4ef6\u7cfb\u7edf\u5143\u6570\u636e\u8bf7\u6c42\uff0c\u89e3\u51b3\u6587\u4ef6\u7cfb\u7edf\u7279\u6709\u7684\u8def\u5f84\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u5728\u4e25\u683c\u4ea4\u6362\u673a\u8d44\u6e90\u7ea6\u675f\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7f13\u5b58", "result": "\u5728Tofino\u4ea4\u6362\u673a\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0cFMCache\u76f8\u6bd4\u539f\u751fHDFS\u541e\u5410\u91cf\u63d0\u5347\u6700\u9ad8181.6%\uff0c\u4e0e\u5ba2\u6237\u7aef\u7f13\u5b58\u7ed3\u5408\u65f6\u989d\u5916\u63d0\u5347139.6%\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u548c\u6709\u9650\u4ea4\u6362\u673a\u8d44\u6e90\u4f7f\u7528", "conclusion": "FMCache\u8bc1\u660e\u4e86\u5728\u4ea4\u6362\u673a\u5c42\u9762\u5b9e\u73b0\u6587\u4ef6\u7cfb\u7edf\u5143\u6570\u636e\u7f13\u5b58\u7684\u53ef\u884c\u6027\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5206\u5e03\u5f0f\u6587\u4ef6\u7cfb\u7edf\u6027\u80fd\u5e76\u8865\u5145\u5ba2\u6237\u7aef\u7f13\u5b58\u65b9\u6848"}}
{"id": "2510.07582", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.07582", "abs": "https://arxiv.org/abs/2510.07582", "authors": ["Yuyan Bao", "Tiark Rompf"], "title": "Type, Ability, and Effect Systems: Perspectives on Purity, Semantics, and Expressiveness", "comment": null, "summary": "Programming benefits from a clear separation between pure, mathematical\ncomputation and impure, effectful interaction with the world. Existing\napproaches to enforce this separation include monads, type-and-effect systems,\nand capability systems. All share a tension between precision and usability,\nand each one has non-obvious strengths and weaknesses.\n  This paper aims to raise the bar in assessing such systems. First, we propose\na semantic definition of purity, inspired by contextual equivalence, as a\nbaseline independent of any specific typing discipline. Second, we propose that\nexpressiveness should be measured by the degree of completeness, i.e., how many\nsemantically pure terms can be typed as pure. Using this measure, we focus on\nminimal meaningful effect and capability systems and show that they are\nincomparable, i.e., neither subsumes the other in terms of expressiveness.\n  Based on this result, we propose a synthesis and show that type, ability, and\neffect systems combine their respective strengths while avoiding their\nweaknesses. As part of our formal model, we provide a logical relation to\nfacilitate proofs of purity and other properties for a variety of effect typing\ndisciplines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7b49\u4ef7\u6027\u7684\u7eaf\u5ea6\u8bed\u4e49\u5b9a\u4e49\uff0c\u5e76\u5efa\u8bae\u7528\u5b8c\u6574\u6027\u6765\u8861\u91cf\u8868\u8fbe\u80fd\u529b\u3002\u7814\u7a76\u8868\u660e\u6700\u5c0f\u6548\u5e94\u7cfb\u7edf\u548c\u80fd\u529b\u7cfb\u7edf\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u4e0d\u53ef\u6bd4\u8f83\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u7ed3\u5408\u7c7b\u578b\u3001\u80fd\u529b\u548c\u6548\u5e94\u7cfb\u7edf\u7684\u7efc\u5408\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u5f3a\u5236\u6267\u884c\u7eaf\u8ba1\u7b97\u548c\u6548\u5e94\u4ea4\u4e92\u5206\u79bb\u7684\u65b9\u6cd5\uff08\u5982\u5355\u5b50\u3001\u7c7b\u578b-\u6548\u5e94\u7cfb\u7edf\u3001\u80fd\u529b\u7cfb\u7edf\uff09\u5728\u7cbe\u786e\u6027\u548c\u53ef\u7528\u6027\u4e4b\u95f4\u5b58\u5728\u5f20\u529b\uff0c\u5404\u6709\u4f18\u7f3a\u70b9\u3002\u8bba\u6587\u65e8\u5728\u4e3a\u8bc4\u4f30\u8fd9\u7c7b\u7cfb\u7edf\u8bbe\u7acb\u66f4\u9ad8\u6807\u51c6\u3002", "method": "\u9996\u5148\u63d0\u51fa\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7b49\u4ef7\u6027\u7684\u7eaf\u5ea6\u8bed\u4e49\u5b9a\u4e49\uff1b\u5176\u6b21\u7528\u5b8c\u6574\u6027\u6765\u8861\u91cf\u8868\u8fbe\u80fd\u529b\uff1b\u7136\u540e\u5206\u6790\u6700\u5c0f\u6548\u5e94\u7cfb\u7edf\u548c\u80fd\u529b\u7cfb\u7edf\u7684\u8868\u8fbe\u80fd\u529b\uff1b\u6700\u540e\u63d0\u51fa\u7efc\u5408\u65b9\u6848\u5e76\u63d0\u4f9b\u903b\u8f91\u5173\u7cfb\u6765\u8f85\u52a9\u7eaf\u5ea6\u8bc1\u660e\u3002", "result": "\u7814\u7a76\u8868\u660e\u6700\u5c0f\u6548\u5e94\u7cfb\u7edf\u548c\u80fd\u529b\u7cfb\u7edf\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u662f\u4e0d\u53ef\u6bd4\u8f83\u7684\uff0c\u5373\u6ca1\u6709\u4e00\u79cd\u7cfb\u7edf\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u5b8c\u5168\u5305\u542b\u53e6\u4e00\u79cd\u3002", "conclusion": "\u7c7b\u578b\u3001\u80fd\u529b\u548c\u6548\u5e94\u7cfb\u7edf\u7684\u7efc\u5408\u65b9\u6848\u80fd\u591f\u7ed3\u5408\u5404\u81ea\u4f18\u52bf\u5e76\u907f\u514d\u5f31\u70b9\uff0c\u8bba\u6587\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u6a21\u578b\u548c\u903b\u8f91\u5173\u7cfb\u6765\u652f\u6301\u5404\u79cd\u6548\u5e94\u7c7b\u578b\u7cfb\u7edf\u7684\u7eaf\u5ea6\u8bc1\u660e\u3002"}}
{"id": "2510.07811", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07811", "abs": "https://arxiv.org/abs/2510.07811", "authors": ["Aryan Poduri"], "title": "Adaptive Execution Scheduler for DataDios SmartDiff", "comment": "4 pages, 1 figure", "summary": "We present an adaptive scheduler for a single differencing engine (SmartDiff)\nwith two execution modes: (i) in-memory threads and (ii) Dask based\nparallelism. The scheduler continuously tunes batch size and worker/thread\ncount within fixed CPU and memory budgets to minimize p95 latency. A\nlightweight preflight profiler estimates bytes/row and I/O rate; an online\ncost/memory model prunes unsafe actions; and a guarded hill-climb policy favors\nlower latency with backpressure and straggler mitigation. Backend selection is\ngated by a conservative working-set estimate so that in-memory execution is\nchosen when safe, otherwise Dask is used. Across synthetic and public tabular\nbenchmarks, the scheduler reduces p95 latency by 23 to 28 percent versus a\ntuned warm-up heuristic (and by 35 to 40 percent versus fixed grid baselines),\nwhile lowering peak memory by 16 to 22 percent (25 to 32 percent vs. fixed)\nwith zero OOMs and comparable throughput.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5355\u673a\u5dee\u5206\u5f15\u64ce\u7684\u81ea\u9002\u5e94\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6279\u5904\u7406\u5927\u5c0f\u548c\u7ebf\u7a0b/\u5de5\u4f5c\u5668\u6570\u91cf\uff0c\u5728\u56fa\u5b9aCPU\u548c\u5185\u5b58\u9884\u7b97\u4e0b\u6700\u5c0f\u5316p95\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u8c03\u5ea6\u65b9\u6cd5\u5728\u5185\u5b58\u4f7f\u7528\u548c\u5ef6\u8fdf\u4f18\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u8c03\u6574\u8d44\u6e90\u5206\u914d\u4ee5\u907f\u514d\u5185\u5b58\u6ea2\u51fa\u540c\u65f6\u4f18\u5316\u5ef6\u8fdf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9884\u5206\u6790\u5668\u4f30\u8ba1\u6bcf\u884c\u5b57\u8282\u6570\u548cI/O\u901f\u7387\uff0c\u5728\u7ebf\u6210\u672c/\u5185\u5b58\u6a21\u578b\u4fee\u526a\u4e0d\u5b89\u5168\u64cd\u4f5c\uff0c\u91c7\u7528\u5e26\u9632\u62a4\u7684\u722c\u5c71\u7b56\u7565\u4f18\u5316\u5ef6\u8fdf\uff0c\u5e76\u5305\u542b\u80cc\u538b\u548c\u6162\u4efb\u52a1\u7f13\u89e3\u673a\u5236\u3002", "result": "\u5728\u5408\u6210\u548c\u516c\u5f00\u8868\u683c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u8c03\u4f18\u7684\u70ed\u8eab\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0cp95\u5ef6\u8fdf\u964d\u4f4e23-28%\uff1b\u76f8\u6bd4\u56fa\u5b9a\u7f51\u683c\u57fa\u7ebf\u964d\u4f4e35-40%\uff0c\u5cf0\u503c\u5185\u5b58\u964d\u4f4e16-22%\uff0c\u4e14\u65e0\u5185\u5b58\u6ea2\u51fa\u95ee\u9898\u3002", "conclusion": "\u8be5\u81ea\u9002\u5e94\u8c03\u5ea6\u5668\u5728\u4fdd\u6301\u53ef\u6bd4\u541e\u5410\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.08544", "categories": ["cs.AR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08544", "abs": "https://arxiv.org/abs/2510.08544", "authors": ["Hengrui Zhang", "Pratyush Patel", "August Ning", "David Wentzlaff"], "title": "SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM Inference", "comment": null, "summary": "Large Language Models (LLMs) have gained popularity in recent years, driving\nup the demand for inference. LLM inference is composed of two phases with\ndistinct characteristics: a compute-bound prefill phase followed by a\nmemory-bound decode phase. To efficiently serve LLMs, prior work proposes\nprefill-decode disaggregation to run each phase on separate hardware. However,\nexisting hardware poorly matches the different requirements of each phase.\nCurrent datacenter GPUs and TPUs follow a more-is-better design philosophy that\nmaximizes compute and memory resources, causing memory bandwidth\nunderutilization in the prefill phase and compute underutilization in the\ndecode phase. Such underutilization directly translates into increased serving\ncosts.\n  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting\na less-is-more methodology to design specialized chips tailored to the distinct\ncharacteristics of prefill and decode phases. The proposed Prefill Chips have\nlarger systolic arrays and use cost-effective GDDR memory, whereas the proposed\nDecode Chips retain high memory bandwidth but reduce compute capacity. Compared\nto modeled H100s, simulations show that the proposed Prefill Chips deliver 8%\nhigher prefill performance on average at 52% lower hardware cost, while the\nproposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.\n  End-to-end simulations on production traces show that SPAD reduces hardware\ncost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while\noffering the same performance. Even when models and workloads change, SPAD can\nreallocate either type of chip to run either phase and still achieve 11%-43%\nlower hardware costs, demonstrating the longevity of the SPAD design.", "AI": {"tldr": "SPAD\u63d0\u51fa\u4e13\u95e8\u9488\u5bf9LLM\u63a8\u7406\u7684\u4e24\u4e2a\u9636\u6bb5\uff08\u9884\u586b\u5145\u548c\u89e3\u7801\uff09\u8bbe\u8ba1\u4e13\u7528\u82af\u7247\uff0c\u901a\u8fc7\u9884\u586b\u5145\u82af\u7247\u548c\u89e3\u7801\u82af\u7247\u7684\u5206\u79bb\u8bbe\u8ba1\uff0c\u76f8\u6bd4\u4f20\u7edfGPU/TPU\u80fd\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u548c\u529f\u8017\u3002", "motivation": "\u4f20\u7edfGPU/TPU\u91c7\u7528\"\u8d8a\u591a\u8d8a\u597d\"\u7684\u8bbe\u8ba1\u7406\u5ff5\uff0c\u5bfc\u81f4\u5728LLM\u63a8\u7406\u7684\u4e24\u4e2a\u9636\u6bb5\uff08\u8ba1\u7b97\u5bc6\u96c6\u7684\u9884\u586b\u5145\u9636\u6bb5\u548c\u5185\u5b58\u5bc6\u96c6\u7684\u89e3\u7801\u9636\u6bb5\uff09\u90fd\u5b58\u5728\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u589e\u52a0\u4e86\u670d\u52a1\u6210\u672c\u3002", "method": "\u91c7\u7528\"\u5c11\u5373\u662f\u591a\"\u65b9\u6cd5\u8bbe\u8ba1\u4e13\u7528\u82af\u7247\uff1a\u9884\u586b\u5145\u82af\u7247\u4f7f\u7528\u66f4\u5927\u7684\u8109\u52a8\u9635\u5217\u548c\u6210\u672c\u6548\u76ca\u9ad8\u7684GDDR\u5185\u5b58\uff1b\u89e3\u7801\u82af\u7247\u4fdd\u6301\u9ad8\u5185\u5b58\u5e26\u5bbd\u4f46\u51cf\u5c11\u8ba1\u7b97\u5bb9\u91cf\u3002", "result": "\u76f8\u6bd4H100\uff0c\u9884\u586b\u5145\u82af\u7247\u6027\u80fd\u63d0\u9ad88%\uff0c\u786c\u4ef6\u6210\u672c\u964d\u4f4e52%\uff1b\u89e3\u7801\u82af\u7247\u8fbe\u523097%\u6027\u80fd\uff0cTDP\u964d\u4f4e28%\u3002\u7aef\u5230\u7aef\u6a21\u62df\u663e\u793a\u786c\u4ef6\u6210\u672c\u964d\u4f4e19%-41%\uff0cTDP\u964d\u4f4e2%-17%\u3002", "conclusion": "SPAD\u8bbe\u8ba1\u5177\u6709\u957f\u671f\u9002\u7528\u6027\uff0c\u5373\u4f7f\u6a21\u578b\u548c\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\uff0c\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u82af\u7247\u7c7b\u578b\u4ecd\u80fd\u5b9e\u73b011%-43%\u7684\u786c\u4ef6\u6210\u672c\u964d\u4f4e\u3002"}}
{"id": "2510.07851", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.07851", "abs": "https://arxiv.org/abs/2510.07851", "authors": ["Willem Heijltjes"], "title": "The Functional Machine Calculus III: Control", "comment": null, "summary": "The Functional Machine Calculus (Heijltjes 2022) is a new approach to\nunifying the imperative and functional programming paradigms. It extends the\nlambda-calculus, preserving the key features of confluent reduction and typed\ntermination, to embed computational effects, evaluation strategies, and control\nflow operations. The first instalment modelled sequential higher-order\ncomputation with global store, input/output, probabilities, and\nnon-determinism, and embedded both the call-by-name and call-by-value\nlambda-calculus, as well as Moggi's computational metalanguage and Levy's\ncall-by-push-value. The present paper extends the calculus from sequential to\nbranching and looping control flow. This allows the faithful embedding of a\nminimal but complete imperative language, including conditionals, exception\nhandling, and iteration, as well as constants and algebraic data types.\n  The calculus is defined through a simple operational semantics, extending the\n(simplified) Krivine machine for the lambda-calculus with multiple operand\nstacks to model effects and a continuation stack to model sequential,\nbranching, and looping computation. It features a confluent reduction relation\nand a system of simple types that guarantees termination of the machine and\nstrong normalization of reduction (in the absence of iteration). These\nproperties carry over to the embedded imperative language, providing a unified\nfunctional-imperative model of computation that supports simple types, a direct\nand intuitive operational semantics, and a confluent reduction semantics.", "AI": {"tldr": "Functional Machine Calculus\u6269\u5c55\u4e86lambda\u6f14\u7b97\uff0c\u7edf\u4e00\u4e86\u51fd\u6570\u5f0f\u548c\u547d\u4ee4\u5f0f\u7f16\u7a0b\u8303\u5f0f\uff0c\u652f\u6301\u8ba1\u7b97\u6548\u679c\u3001\u63a7\u5236\u6d41\u64cd\u4f5c\uff0c\u5e76\u4fdd\u8bc1\u6c47\u5408\u5f52\u7ea6\u548c\u7c7b\u578b\u7ec8\u6b62\u6027\u3002", "motivation": "\u4e3a\u4e86\u7edf\u4e00\u51fd\u6570\u5f0f\u548c\u547d\u4ee4\u5f0f\u7f16\u7a0b\u8303\u5f0f\uff0c\u5728\u4fdd\u6301lambda\u6f14\u7b97\u6838\u5fc3\u7279\u6027\u7684\u540c\u65f6\u5d4c\u5165\u8ba1\u7b97\u6548\u679c\u3001\u8bc4\u4f30\u7b56\u7565\u548c\u63a7\u5236\u6d41\u64cd\u4f5c\u3002", "method": "\u6269\u5c55\u7b80\u5316\u7684Krivine\u673a\u5668\uff0c\u4f7f\u7528\u591a\u4e2a\u64cd\u4f5c\u6570\u6808\u5efa\u6a21\u6548\u679c\uff0c\u4f7f\u7528\u5ef6\u7eed\u6808\u5efa\u6a21\u987a\u5e8f\u3001\u5206\u652f\u548c\u5faa\u73af\u8ba1\u7b97\uff0c\u5b9a\u4e49\u7b80\u5355\u64cd\u4f5c\u8bed\u4e49\u3002", "result": "\u5b9e\u73b0\u4e86\u6c47\u5408\u5f52\u7ea6\u5173\u7cfb\uff0c\u7b80\u5355\u7c7b\u578b\u7cfb\u7edf\u4fdd\u8bc1\u673a\u5668\u7ec8\u6b62\u548c\u5f3a\u89c4\u8303\u5316\uff08\u65e0\u8fed\u4ee3\u65f6\uff09\uff0c\u6210\u529f\u5d4c\u5165\u5b8c\u6574\u7684\u547d\u4ee4\u5f0f\u8bed\u8a00\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u529f\u80fd-\u547d\u4ee4\u5f0f\u8ba1\u7b97\u6a21\u578b\uff0c\u652f\u6301\u7b80\u5355\u7c7b\u578b\u3001\u76f4\u89c2\u64cd\u4f5c\u8bed\u4e49\u548c\u6c47\u5408\u5f52\u7ea6\u8bed\u4e49\u3002"}}
{"id": "2510.08164", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.08164", "abs": "https://arxiv.org/abs/2510.08164", "authors": ["Marco Picone", "Samuele Burattini", "Marco Melloni", "Prasad Talasila", "Davide Ziglioli", "Matteo Martinelli", "Nicola Bicocchi", "Alessandro Ricci", "Peter Gorm Larsen"], "title": "A Multi-Simulation Bridge for IoT Digital Twins", "comment": null, "summary": "The increasing capabilities of Digital Twins (DTs) in the context of the\nInternet of Things (IoT) and Industrial IoT (IIoT) call for seamless\nintegration with simulation platforms to support system design, validation, and\nreal-time operation. This paper introduces the concept, design, and\nexperimental evaluation of the DT Simulation Bridge - a software framework that\nenables diverse interaction patterns between active DTs and simulation\nenvironments. The framework supports both the DT development lifecycle and the\nincorporation of simulations during active operation. Through bidirectional\ndata exchange, simulations can update DT models dynamically, while DTs provide\nreal-time feedback to adapt simulation parameters. We describe the\narchitectural design and core software components that ensure flexible\ninteroperability and scalable deployment. Experimental results show that the DT\nSimulation Bridge enhances design agility, facilitates virtual commissioning,\nand supports live behavioral analysis under realistic conditions, demonstrating\nits effectiveness across a range of industrial scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86DT Simulation Bridge\u6846\u67b6\uff0c\u652f\u6301\u6570\u5b57\u5b6a\u751f\u4e0e\u4eff\u771f\u5e73\u53f0\u4e4b\u95f4\u7684\u53cc\u5411\u4ea4\u4e92\uff0c\u63d0\u5347\u5de5\u4e1a\u7269\u8054\u7f51\u7cfb\u7edf\u7684\u8bbe\u8ba1\u7075\u6d3b\u6027\u548c\u5b9e\u65f6\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u5b6a\u751f\u5728\u7269\u8054\u7f51\u548c\u5de5\u4e1a\u7269\u8054\u7f51\u4e2d\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u9700\u8981\u4e0e\u4eff\u771f\u5e73\u53f0\u65e0\u7f1d\u96c6\u6210\u4ee5\u652f\u6301\u7cfb\u7edf\u8bbe\u8ba1\u3001\u9a8c\u8bc1\u548c\u5b9e\u65f6\u64cd\u4f5c\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86DT Simulation Bridge\u8f6f\u4ef6\u6846\u67b6\uff0c\u652f\u6301\u6570\u5b57\u5b6a\u751f\u4e0e\u4eff\u771f\u73af\u5883\u4e4b\u95f4\u7684\u591a\u6837\u5316\u4ea4\u4e92\u6a21\u5f0f\uff0c\u901a\u8fc7\u53cc\u5411\u6570\u636e\u4ea4\u6362\u5b9e\u73b0\u52a8\u6001\u6a21\u578b\u66f4\u65b0\u548c\u5b9e\u65f6\u53cd\u9988\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u589e\u5f3a\u4e86\u8bbe\u8ba1\u7075\u6d3b\u6027\uff0c\u4fc3\u8fdb\u4e86\u865a\u62df\u8c03\u8bd5\uff0c\u5e76\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u652f\u6301\u5b9e\u65f6\u884c\u4e3a\u5206\u6790\uff0c\u5728\u591a\u79cd\u5de5\u4e1a\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "DT Simulation Bridge\u4e3a\u6570\u5b57\u5b6a\u751f\u4e0e\u4eff\u771f\u5e73\u53f0\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u4ece\u5f00\u53d1\u5230\u5b9e\u65f6\u64cd\u4f5c\u7684\u5168\u751f\u547d\u5468\u671f\u7ba1\u7406\u3002"}}
{"id": "2510.08180", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.08180", "abs": "https://arxiv.org/abs/2510.08180", "authors": ["Natalie Carl", "Tobias Pfandzelter", "David Bermbach"], "title": "Towards Energy-Efficient Serverless Computing with Hardware Isolation", "comment": null, "summary": "Serverless computing provides just-in-time infrastructure provisioning with\nrapid elasticity and a finely-grained pricing model. As full control of\nresource allocation is in the hands of the cloud provider and applications only\nconsume resources when they actually perform work, we believe that serverless\ncomputing is uniquely positioned to maximize energy efficiency.\n  However, the focus of current serverless platforms is to run hundreds or\nthousands of serverless functions from different tenants on traditional server\nhardware, requiring expensive software isolation mechanisms and a high degree\nof overprovisioning, i.e., idle servers, to anticipate load spikes. With shared\ncaches, high clock frequencies, and many-core architectures, servers today are\noptimized for large, singular workloads but not to run thousands of isolated\nfunctions.\n  We propose rethinking the serverless hardware architecture to align it with\nthe requirements of serverless software. Specifically, we propose using\nhardware isolation with individual processors per function instead of software\nisolation resulting in a serverless hardware stack that consumes energy only\nwhen an application actually performs work. In preliminary evaluation with real\nhardware and a typical serverless workload we find that this could reduce\nenergy consumption overheads by 90.63% or an average 70.8MW.", "AI": {"tldr": "\u63d0\u51fa\u91cd\u65b0\u8bbe\u8ba1\u65e0\u670d\u52a1\u5668\u786c\u4ef6\u67b6\u6784\uff0c\u4f7f\u7528\u786c\u4ef6\u9694\u79bb\u66ff\u4ee3\u8f6f\u4ef6\u9694\u79bb\uff0c\u4e3a\u6bcf\u4e2a\u51fd\u6570\u5206\u914d\u72ec\u7acb\u5904\u7406\u5668\uff0c\u53ef\u663e\u8457\u964d\u4f4e\u80fd\u8017", "motivation": "\u5f53\u524d\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u5728\u4f20\u7edf\u670d\u52a1\u5668\u786c\u4ef6\u4e0a\u8fd0\u884c\u6570\u5343\u4e2a\u51fd\u6570\uff0c\u9700\u8981\u6602\u8d35\u7684\u8f6f\u4ef6\u9694\u79bb\u673a\u5236\u548c\u9ad8\u5ea6\u7684\u8d44\u6e90\u8fc7\u5ea6\u914d\u7f6e\uff0c\u5bfc\u81f4\u80fd\u6548\u4f4e\u4e0b", "method": "\u4f7f\u7528\u786c\u4ef6\u9694\u79bb\u6280\u672f\uff0c\u4e3a\u6bcf\u4e2a\u51fd\u6570\u5206\u914d\u72ec\u7acb\u5904\u7406\u5668\uff0c\u6784\u5efa\u4ec5\u5728\u5b9e\u9645\u5de5\u4f5c\u65f6\u6d88\u8017\u80fd\u6e90\u7684\u65e0\u670d\u52a1\u5668\u786c\u4ef6\u6808", "result": "\u521d\u6b65\u8bc4\u4f30\u663e\u793a\u53ef\u51cf\u5c1190.63%\u7684\u80fd\u8017\u5f00\u9500\uff0c\u5e73\u5747\u8282\u770170.8MW", "conclusion": "\u786c\u4ef6\u9694\u79bb\u7684\u65e0\u670d\u52a1\u5668\u67b6\u6784\u80fd\u663e\u8457\u63d0\u5347\u80fd\u6548\uff0c\u66f4\u597d\u5730\u6ee1\u8db3\u65e0\u670d\u52a1\u5668\u8f6f\u4ef6\u7684\u9700\u6c42"}}
{"id": "2510.08228", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.08228", "abs": "https://arxiv.org/abs/2510.08228", "authors": ["Quentin Renau", "Amjad Ullah", "Emma Hart"], "title": "Distributed Resource Selection for Self-Organising Cloud-Edge Systems", "comment": "This paper is accepted for publication in the 23rd IEEE International\n  Symposium on Network Computing and Applications", "summary": "This paper presents a distributed resource selection mechanism for diverse\ncloud-edge environments, enabling dynamic and context-aware allocation of\nresources to meet the demands of complex distributed applications. By\ndistributing the decision-making process, our approach ensures efficiency,\nscalability, and resilience in highly dynamic cloud-edge environments where\ncentralised coordination becomes a bottleneck. The proposed mechanism aims to\nfunction as a core component of a broader, distributed, and self-organising\norchestration system that facilitates the intelligent placement and adaptation\nof applications in real-time. This work leverages a consensus-based mechanism\nutilising local knowledge and inter-agent collaboration to achieve efficient\nresults without relying on a central controller, thus paving the way for\ndistributed orchestration. Our results indicate that computation time is the\nkey factor influencing allocation decisions. Our approach consistently delivers\nrapid allocations without compromising optimality or incurring additional cost,\nachieving timely results at scale where exhaustive search is infeasible and\ncentralised heuristics run up to 30 times slower.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u8d44\u6e90\u9009\u62e9\u673a\u5236\uff0c\u7528\u4e8e\u4e91\u8fb9\u73af\u5883\u4e2d\u7684\u52a8\u6001\u8d44\u6e90\u5206\u914d\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u51b3\u7b56\u907f\u514d\u96c6\u4e2d\u5f0f\u534f\u8c03\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5728\u9ad8\u5ea6\u52a8\u6001\u7684\u4e91\u8fb9\u73af\u5883\u4e2d\uff0c\u96c6\u4e2d\u5f0f\u534f\u8c03\u6210\u4e3a\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u5206\u5e03\u5f0f\u3001\u81ea\u7ec4\u7ec7\u7684\u7f16\u6392\u7cfb\u7edf\u6765\u652f\u6301\u590d\u6742\u5206\u5e03\u5f0f\u5e94\u7528\u7684\u5b9e\u65f6\u90e8\u7f72\u548c\u9002\u5e94\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5171\u8bc6\u7684\u673a\u5236\uff0c\u5229\u7528\u672c\u5730\u77e5\u8bc6\u548c\u4ee3\u7406\u95f4\u534f\u4f5c\uff0c\u5b9e\u73b0\u65e0\u9700\u4e2d\u592e\u63a7\u5236\u5668\u7684\u5206\u5e03\u5f0f\u8d44\u6e90\u5206\u914d\u51b3\u7b56\u3002", "result": "\u8ba1\u7b97\u65f6\u95f4\u662f\u5f71\u54cd\u5206\u914d\u51b3\u7b56\u7684\u5173\u952e\u56e0\u7d20\u3002\u8be5\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u6700\u4f18\u6027\u6216\u589e\u52a0\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5feb\u901f\u5206\u914d\uff0c\u5728\u96c6\u4e2d\u5f0f\u542f\u53d1\u5f0f\u7b97\u6cd5\u8fd0\u884c\u616230\u500d\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u53ca\u65f6\u83b7\u5f97\u7ed3\u679c\u3002", "conclusion": "\u8be5\u5206\u5e03\u5f0f\u8d44\u6e90\u9009\u62e9\u673a\u5236\u4e3a\u5206\u5e03\u5f0f\u7f16\u6392\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u80fd\u591f\u5728\u4e91\u8fb9\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u5177\u6709\u5f39\u6027\u7684\u8d44\u6e90\u7ba1\u7406\u3002"}}
{"id": "2510.08244", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.08244", "abs": "https://arxiv.org/abs/2510.08244", "authors": ["Dominick Banasik", "Varsha Dani", "Fabien Dufoulon", "Aayush Gupta", "Thomas P. Hayes", "Gopal Pandurangan"], "title": "Energy-Efficient Maximal Independent Sets in Radio Networks", "comment": null, "summary": "The maximal independent set (MIS) is one of the most fundamental problems in\ndistributed computing, and it has been studied intensively for over four\ndecades. This paper focuses on the MIS problem in the Radio Network model, a\nstandard model widely used to model wireless networks, particularly ad hoc\nwireless and sensor networks. Energy is a premium resource in these networks,\nwhich are typically battery-powered. Hence, designing distributed algorithms\nthat use as little energy as possible is crucial. We use the well-established\nenergy model where a node can be sleeping or awake in a round, and only the\nawake rounds (when it can send or listen) determine the energy complexity of\nthe algorithm, which we want to minimize.\n  We present new, more energy-efficient MIS algorithms in radio networks with\narbitrary and unknown graph topology. We present algorithms for two popular\nvariants of the radio model -- with collision detection (CD) and without\ncollision detection (no-CD). Specifically, we obtain the following results:\n  1. CD model: We present a randomized distributed MIS algorithm with energy\ncomplexity $O(\\log n)$, round complexity $O(\\log^2 n)$, and failure probability\n$1 / poly(n)$, where $n$ is the network size. We show that our energy\ncomplexity is optimal by showing a matching $\\Omega(\\log n)$ lower bound.\n  2. no-CD model: In the more challenging no-CD model, we present a randomized\ndistributed MIS algorithm with energy complexity $O(\\log^2n \\log \\log n)$,\nround complexity $O(\\log^3 n \\log \\Delta)$, and failure probability $1 /\npoly(n)$. The energy complexity of our algorithm is significantly lower than\nthe round (and energy) complexity of $O(\\log^3 n)$ of the best known\ndistributed MIS algorithm of Davies [PODC 2023] for arbitrary graph topology.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u65e0\u7ebf\u7f51\u7edc\u4e2d\u66f4\u8282\u80fd\u7684\u6700\u5927\u72ec\u7acb\u96c6\uff08MIS\uff09\u7b97\u6cd5\uff0c\u5728\u78b0\u649e\u68c0\u6d4b\uff08CD\uff09\u6a21\u578b\u4e2d\u8fbe\u5230O(log n)\u80fd\u91cf\u590d\u6742\u5ea6\uff08\u6700\u4f18\uff09\uff0c\u5728\u65e0\u78b0\u649e\u68c0\u6d4b\uff08no-CD\uff09\u6a21\u578b\u4e2d\u8fbe\u5230O(log\u00b2n log log n)\u80fd\u91cf\u590d\u6742\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "motivation": "\u65e0\u7ebf\u7f51\u7edc\u901a\u5e38\u7531\u7535\u6c60\u4f9b\u7535\uff0c\u80fd\u91cf\u662f\u5b9d\u8d35\u8d44\u6e90\u3002\u9700\u8981\u8bbe\u8ba1\u80fd\u91cf\u590d\u6742\u5ea6\u5c3d\u53ef\u80fd\u4f4e\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u5176\u4e2d\u53ea\u6709\u5524\u9192\u8f6e\u6b21\u6d88\u8017\u80fd\u91cf\u3002", "method": "\u9488\u5bf9CD\u548cno-CD\u4e24\u79cd\u65e0\u7ebf\u7f51\u7edc\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e86\u968f\u673a\u5206\u5e03\u5f0fMIS\u7b97\u6cd5\u3002CD\u6a21\u578b\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u7ed3\u6784\u5b9e\u73b0\u4f4e\u80fd\u8017\uff0cno-CD\u6a21\u578b\u91c7\u7528\u66f4\u590d\u6742\u7684\u6280\u672f\u5904\u7406\u65e0\u78b0\u649e\u68c0\u6d4b\u7684\u6311\u6218\u3002", "result": "CD\u6a21\u578b\uff1a\u80fd\u91cf\u590d\u6742\u5ea6O(log n)\uff08\u6700\u4f18\uff09\uff0c\u8f6e\u590d\u6742\u5ea6O(log\u00b2 n)\uff0c\u5931\u8d25\u6982\u73871/poly(n)\u3002no-CD\u6a21\u578b\uff1a\u80fd\u91cf\u590d\u6742\u5ea6O(log\u00b2n log log n)\uff0c\u8f6e\u590d\u6742\u5ea6O(log\u00b3 n log \u0394)\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709O(log\u00b3 n)\u7684\u7b97\u6cd5\u3002", "conclusion": "\u672c\u6587\u5728\u65e0\u7ebf\u7f51\u7edcMIS\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u80fd\u91cf\u6548\u7387\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728CD\u6a21\u578b\u4e2d\u8fbe\u5230\u4e86\u7406\u8bba\u6700\u4f18\uff0c\u4e3a\u80fd\u91cf\u53d7\u9650\u7684\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08536", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08536", "abs": "https://arxiv.org/abs/2510.08536", "authors": ["Gregor Olenik", "Marcel Koch", "Hartwig Anzt"], "title": "Investigating Matrix Repartitioning to Address the Over- and Undersubscription Challenge for a GPU-based CFD Solver", "comment": "2025 Workshop: HPC on Heterogeneous Hardware (H3)", "summary": "Modern high-performance computing (HPC) increasingly relies on GPUs, but\nintegrating GPU acceleration into complex scientific frameworks like OpenFOAM\nremains a challenge. Existing approaches either fully refactor the codebase or\nuse plugin-based GPU solvers, each facing trade-offs between performance and\ndevelopment effort. In this work, we address the limitations of plugin-based\nGPU acceleration in OpenFOAM by proposing a repartitioning strategy that better\nbalances CPU matrix assembly and GPU-based linear solves. We present a detailed\ncomputational model, describe a novel matrix repartitioning and update\nprocedure, and evaluate its performance on large-scale CFD simulations. Our\nresults show that the proposed method significantly mitigates oversubscription\nissues, improving solver performance and resource utilization in heterogeneous\nCPU-GPU environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u65b0\u5206\u533a\u7b56\u7565\u6765\u6539\u8fdbOpenFOAM\u4e2d\u57fa\u4e8e\u63d2\u4ef6\u7684GPU\u52a0\u901f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861CPU\u77e9\u9635\u7ec4\u88c5\u548cGPU\u7ebf\u6027\u6c42\u89e3\uff0c\u7f13\u89e3\u8d44\u6e90\u8fc7\u5ea6\u8ba2\u9605\u95ee\u9898", "motivation": "\u73b0\u4ee3\u9ad8\u6027\u80fd\u8ba1\u7b97\u65e5\u76ca\u4f9d\u8d56GPU\uff0c\u4f46\u5728\u590d\u6742\u79d1\u5b66\u6846\u67b6\u5982OpenFOAM\u4e2d\u96c6\u6210GPU\u52a0\u901f\u4ecd\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5b8c\u5168\u91cd\u6784\u4ee3\u7801\u5e93\uff0c\u8981\u4e48\u4f7f\u7528\u57fa\u4e8e\u63d2\u4ef6\u7684GPU\u6c42\u89e3\u5668\uff0c\u5728\u6027\u80fd\u548c\u5f00\u53d1\u5de5\u4f5c\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861", "method": "\u63d0\u51fa\u91cd\u65b0\u5206\u533a\u7b56\u7565\uff0c\u5305\u62ec\u8be6\u7ec6\u7684\u8ba1\u7b97\u6a21\u578b\u3001\u65b0\u9896\u7684\u77e9\u9635\u91cd\u65b0\u5206\u533a\u548c\u66f4\u65b0\u8fc7\u7a0b\uff0c\u5728\u5f02\u6784CPU-GPU\u73af\u5883\u4e2d\u66f4\u597d\u5730\u5e73\u8861CPU\u77e9\u9635\u7ec4\u88c5\u548cGPU\u7ebf\u6027\u6c42\u89e3", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u7f13\u89e3\u4e86\u8fc7\u5ea6\u8ba2\u9605\u95ee\u9898\uff0c\u5728\u5927\u578bCFD\u6a21\u62df\u4e2d\u63d0\u9ad8\u4e86\u6c42\u89e3\u5668\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387", "conclusion": "\u6240\u63d0\u51fa\u7684\u91cd\u65b0\u5206\u533a\u65b9\u6cd5\u6709\u6548\u6539\u8fdb\u4e86OpenFOAM\u4e2d\u57fa\u4e8e\u63d2\u4ef6\u7684GPU\u52a0\u901f\uff0c\u4e3a\u5f02\u6784\u8ba1\u7b97\u73af\u5883\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6027\u80fd\u89e3\u51b3\u65b9\u6848"}}
