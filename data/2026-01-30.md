<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Optimal Software Pipelining using an SMT-Solver](https://arxiv.org/abs/2601.21842)
*Jan-Willem Roorda*

Main category: cs.PL

TL;DR: 基于SMT求解器的最优软件流水线方法，显著优于启发式算法和手工优化


<details>
  <summary>Details</summary>
Motivation: 软件流水线是VLIW处理器的重要循环优化技术，传统启发式方法可能无法找到最优解，需要更精确的优化方法

Method: 使用可满足性模理论（SMT）求解器构建最优软件流水线调度器，将调度问题转化为SMT约束求解问题

Result: 该方法显著优于启发式算法和手工优化，并能向程序员和处理器设计者提供关于特定启动间隔不可行的反馈信息

Conclusion: 基于SMT求解器的最优软件流水线方法是有效的，不仅能找到最优调度方案，还能提供有价值的调试和设计反馈

Abstract: Software Pipelining is a classic and important loop-optimization for VLIW processors. It improves instruction-level parallelism by overlapping multiple iterations of a loop and executing them in parallel. Typically, it is implemented using heuristics. In this paper, we present an optimal software pipeliner based on a Satisfiability Modulo Theories (SMT) Solver. We show that our approach significantly outperforms heuristic algorithms and hand-optimization. Furthermore, we show how the solver can be used to give feedback to programmers and processor designers on why a software pipelined schedule of a certain initiation interval is not feasible.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Deep Reinforcement Learning for Fault-Adaptive Routing in Eisenstein-Jacobi Interconnection Topologies](https://arxiv.org/abs/2601.21090)
*Mohammad Walid Charrwi,Zaid Hussain*

Main category: cs.DC

TL;DR: 该论文评估了三种路由算法在故障EJ网络中的表现：贪婪路由在故障下性能急剧下降，Dijkstra算法提供理论最优但需要全局拓扑信息，而基于强化学习的方法在分布式部署中实现了接近最优的性能。


<details>
  <summary>Details</summary>
Motivation: 随着多核架构密度的增加，需要高性能且容错的互连网络。EJ网络具有优越的拓扑特性，但在故障条件下对传统路由启发式算法构成挑战。

Method: 评估三种路由范式：确定性贪婪自适应路由、理论最优的Dijkstra算法，以及基于强化学习的方法。RL方法使用多目标奖励函数惩罚故障接近度并奖励路径效率，学习在故障集群周围导航。

Result: 在9个故障节点下：贪婪路由有效可达性和数据包投递率降至10%；Dijkstra算法证明52-54%是拓扑最优；RL代理实现94%有效可达性和91%数据包投递率。吞吐量评估显示RL在所有负载下维持超过90%的归一化吞吐量。

Conclusion: 基于强化学习的自适应策略是实用解决方案，在贪婪路由的效率和Dijkstra算法的最优性之间架起桥梁，提供鲁棒的自愈通信，无需全局拓扑知识或最优算法的计算开销。

Abstract: The increasing density of many-core architectures necessitates interconnection networks that are both high-performance and fault-resilient. Eisenstein-Jacobi (EJ) networks, with their symmetric 6-regular topology, offer superior topological properties but challenge traditional routing heuristics under fault conditions. This paper evaluates three routing paradigms in faulty EJ environments: deterministic Greedy Adaptive Routing, theoretically optimal Dijkstra's algorithm, and a reinforcement learning (RL)-based approach. Using a multi-objective reward function to penalize fault proximity and reward path efficiency, the RL agent learns to navigate around clustered failures that typically induce dead-ends in greedy geometric routing. Dijkstra's algorithm establishes the theoretical performance ceiling by computing globally optimal paths with complete topology knowledge, revealing the true connectivity limits of faulty networks. Quantitative analysis at nine faulty nodes shows greedy routing catastrophically degrades to 10% effective reachability and packet delivery, while Dijkstra proves 52-54% represents the topological optimum. The RL agent achieves 94% effective reachability and 91% packet delivery, making it suitable for distributed deployment. Furthermore, throughput evaluations demonstrate that RL sustains over 90% normalized throughput across all loads, actually outperforming Dijkstra under congestion through implicit load balancing strategies. These results establish RL-based adaptive policies as a practical solution that bridges the gap between greedy's efficiency and Dijkstra's optimality, providing robust, self-healing communication in fault-prone interconnection networks without requiring the global topology knowledge or computational overhead of optimal algorithms.

</details>


### [3] [Maxwait: A Generalized Mechanism for Distributed Time-Sensitive Systems](https://arxiv.org/abs/2601.21146)
*Francesco Paladino,Shulu Li,Edward A. Lee*

Main category: cs.DC

TL;DR: maxwait是一种简单但通用的协调机制，在分布式时间敏感系统中明确且可配置地权衡时序要求与一致性，涵盖多种经典分布式方法并支持实时行为。


<details>
  <summary>Details</summary>
Motivation: 分布式时间敏感系统需要在通信延迟和同步不确定性的情况下，平衡时序要求（可用性）和一致性。现有方法往往无法同时满足实时性和一致性需求。

Method: 提出maxwait协调机制，作为Lingua Franca协调语言的扩展实现。该机制在通信延迟有界时强制执行逻辑时间一致性，在边界被违反时提供结构化故障处理。

Result: maxwait机制能够涵盖PTIDES、Chandy-and-Misra、Time-Warp、Lamport时间故障检测等经典分布式方法，同时支持LET、发布订阅、actor、CRDTs、RPC with futures等常见分布式模式，并提供更好的时序控制、有界时间故障检测和确定性增强。

Conclusion: maxwait是一个简单但通用的协调机制，为分布式时间敏感系统提供了统一的语义框架，能够明确配置时序与一致性的权衡，同时支持实时行为和多种分布式模式。

Abstract: Distributed time-sensitive systems must balance timing requirements (availability) and consistency in the presence of communication delays and synchronization uncertainty. This paper presents maxwait, a simple coordination mechanism with surprising generality that makes these tradeoffs explicit and configurable. We demonstrate that this mechanism subsumes classical distributed system methods such as PTIDES, Chandy-and-Misra with or without null messages, Jefferson's Time-Warp, and Lamport's time-based fault detection, while enabling real-time behavior in distributed cyber-physical applications. The mechanism can also realize many commonly used distributed system patterns, including logical execution time (LET), publish and subscribe, actors, conflict-free replicated data types (CRDTs), and remote procedure calls with futures. More importantly, it adds to these mechanisms better control over timing, bounded time fault detection, and the option of making them more deterministic, all within a single semantic framework. Implemented as an extension of the Lingua Franca coordination language, maxwait enforces logical-time consistency when communication latencies are bounded and provides structured fault handling when bounds are violated.

</details>


### [4] [ZipMoE: Efficient On-Device MoE Serving via Lossless Compression and Cache-Affinity Scheduling](https://arxiv.org/abs/2601.21198)
*Yuchen Yang,Yaru Zhao,Pu Yang,Shaowei Wang,Zhi-Hua Zhou*

Main category: cs.DC

TL;DR: ZipMoE：一种高效且语义无损的端侧MoE服务系统，通过缓存调度协同设计，将端侧MoE推理从I/O瓶颈转变为计算中心化工作流，显著降低延迟并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 混合专家（MoE）架构虽然增强了大型语言模型的表达能力，但其巨大的内存占用严重阻碍了在资源受限的边缘设备上的实际部署，尤其是在需要保持模型行为而不依赖有损量化的情况下。

Method: ZipMoE采用缓存调度协同设计，利用边缘设备硬件特性与MoE参数固有统计冗余之间的协同效应，通过理论性能保证将端侧MoE推理范式从I/O瓶颈转变为计算中心化工作流，实现高效并行化。

Result: 在代表性边缘计算平台上使用开源MoE模型和真实工作负载进行实验，ZipMoE相比最先进系统实现了高达72.77%的推理延迟降低和高达6.76倍的吞吐量提升。

Conclusion: ZipMoE通过创新的缓存调度协同设计，有效解决了MoE模型在边缘设备部署中的内存瓶颈问题，实现了高效且语义无损的端侧推理服务，显著提升了边缘AI应用的实用性。

Abstract: While Mixture-of-Experts (MoE) architectures substantially bolster the expressive power of large-language models, their prohibitive memory footprint severely impedes the practical deployment on resource-constrained edge devices, especially when model behavior must be preserved without relying on lossy quantization. In this paper, we present ZipMoE, an efficient and semantically lossless on-device MoE serving system. ZipMoE exploits the synergy between the hardware properties of edge devices and the statistical redundancy inherent to MoE parameters via a caching-scheduling co-design with provable performance guarantee. Fundamentally, our design shifts the paradigm of on-device MoE inference from an I/O-bound bottleneck to a compute-centric workflow that enables efficient parallelization. We implement a prototype of ZipMoE and conduct extensive experiments on representative edge computing platforms using popular open-source MoE models and real-world workloads. Our evaluation reveals that ZipMoE achieves up to $72.77\%$ inference latency reduction and up to $6.76\times$ higher throughput than the state-of-the-art systems.

</details>


### [5] [Ira: Efficient Transaction Replay for Distributed Systems](https://arxiv.org/abs/2601.21286)
*Adithya Bhat,Harshal Bhadreshkumar Shah,Mohsen Minaei*

Main category: cs.DC

TL;DR: Ira框架通过传输紧凑提示来加速主备复制中的备份重放，在以太坊案例中实现25倍中位数加速


<details>
  <summary>Details</summary>
Motivation: 在主备复制中，共识延迟受限于备份节点重放主节点事务的时间。主节点已执行事务，拥有未来访问模式的精确信息，这正是最优重放所需的信息。

Method: 提出Ira框架，通过传输紧凑提示加速备份重放。针对以太坊提出具体协议Ira-L，主节点提供包含以太坊区块工作集密钥和每个密钥一字节元数据的提示，备份利用这些提示进行高效区块重放。

Result: 提示紧凑，每个区块中位数增加47KB压缩数据（约5%区块负载）。主节点顺序提示生成和区块执行带来28.6%时间开销，但提示直接成本仅为执行时间的10.9%。备份端Ira-L实现中位数每区块25倍加速，16个预取线程下总重放时间从6.5小时降至16分钟（23.6倍时间加速）。

Conclusion: Ira框架通过利用主节点的未来访问模式知识传输紧凑提示，显著加速备份重放，在以太坊主网测试中证明其有效性，提示开销小且可并行化部署。

Abstract: In primary-backup replication, consensus latency is bounded by the time for backup nodes to replay (re-execute) transactions proposed by the primary. In this work, we present Ira, a framework to accelerate backup replay by transmitting compact \emph{hints} alongside transaction batches. Our key insight is that the primary, having already executed transactions, possesses knowledge of future access patterns which is exactly the information needed for optimal replay.
  We use Ethereum for our case study and present a concrete protocol, Ira-L, within our framework to improve cache management of Ethereum block execution. The primaries implementing Ira-L provide hints that consist of the working set of keys used in an Ethereum block and one byte of metadata per key indicating the table to read from, and backups use these hints for efficient block replay.
  We evaluated Ira-L against the state-of-the-art Ethereum client reth over two weeks of Ethereum mainnet activity ($100,800$ blocks containing over $24$ million transactions). Our hints are compact, adding a median of $47$ KB compressed per block ($\sim5\%$ of block payload). We observe that the sequential hint generation and block execution imposes a $28.6\%$ wall-time overhead on the primary, though the direct cost from hints is $10.9\%$ of execution time; all of which can be pipelined and parallelized in production deployments. On the backup side, we observe that Ira-L achieves a median per-block speedup of $25\times$ over baseline reth. With $16$ prefetch threads, aggregate replay time drops from $6.5$ hours to $16$ minutes ($23.6\times$ wall-time speedup).

</details>


### [6] [EWSJF: An Adaptive Scheduler with Hybrid Partitioning for Mixed-Workload LLM Inference](https://arxiv.org/abs/2601.21758)
*Bronislav Sidik,Chaya Levi,Joseph Kampeas*

Main category: cs.DC

TL;DR: EWSJF是一种针对混合工作负载（短延迟敏感查询+长吞吐量请求）的自适应请求级调度器，通过实时学习工作负载结构，在vLLM中实现吞吐量提升30%以上，短请求平均首次令牌时间减少4倍。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型服务面临混合工作负载调度挑战：短延迟敏感交互查询与长吞吐量批量请求并存。传统的先到先服务策略存在严重的队头阻塞问题，导致高尾延迟和硬件利用率低下。

Method: EWSJF包含四个核心组件：1) Refine-and-Prune无监督分区算法，发现性能同质请求组；2) 动态队列路由，将请求分配到相应组；3) 密度加权评分，平衡紧急性和公平性的上下文感知优先级函数；4) 贝叶斯元优化，基于实时性能反馈持续调整评分和分区参数。

Result: 在vLLM中实现，相比FCFS，EWSJF将端到端吞吐量提升超过30%，并将短请求的平均首次令牌时间减少高达4倍。

Conclusion: 自适应、基于学习的请求调度是高效响应式LLM服务的关键缺失层，EWSJF通过实时学习工作负载结构，在公平性和吞吐量方面实现联合优化。

Abstract: Serving Large Language Models (LLMs) under mixed workloads--short, latency-sensitive interactive queries alongside long, throughput-oriented batch requests--poses a fundamental scheduling challenge. Standard First-Come, First-Served (FCFS) policies suffer from severe head-of-line blocking, leading to high tail latency and underutilized hardware. We introduce EWSJF (Effective Workload-based Shortest Job First), an adaptive request-level scheduler that learns workload structure in real time to jointly improve fairness and throughput. EWSJF operates upstream of execution-level schedulers and integrates four components: (1) Refine-and-Prune, an unsupervised partitioning algorithm that discovers performance-homogeneous request groups; (2) Dynamic Queue Routing for assigning requests to these groups; (3) Density-Weighted Scoring, a context-aware prioritization function balancing urgency and fairness; and (4) Bayesian Meta-Optimization, which continuously tunes scoring and partitioning parameters based on live performance feedback. Implemented in vLLM, EWSJF improves end-to-end throughput by over 30% and reduces average Time-To-First-Token for short requests by up to 4x compared to FCFS. These results demonstrate that adaptive, learning-based request scheduling is a critical missing layer for efficient and responsive LLM serving. Implementation available at https://anonymous.4open.science/r/vllm_0110-32D8.

</details>


### [7] [Self-Adaptive Probabilistic Skyline Query Processing in Distributed Edge Computing via Deep Reinforcement Learning](https://arxiv.org/abs/2601.21855)
*Chuan-Chi Lai*

Main category: cs.DC

TL;DR: SA-PSKY：一种用于边缘云协同系统的自适应概率天际线查询框架，通过强化学习动态调整过滤阈值，显著降低通信开销和响应时间。


<details>
  <summary>Details</summary>
Motivation: 在万物互联时代，边缘传感器数据爆炸式增长使得概率天际线查询处理面临严峻挑战。传统分布式方法依赖静态阈值过滤本地候选数据，但无法适应边缘计算环境的高度动态和异构特性，常导致通信瓶颈或计算延迟。

Method: 提出SA-PSKY自适应框架，将动态阈值调整问题形式化为连续马尔可夫决策过程，利用深度确定性策略梯度智能体实时优化过滤强度。通过智能分析多维系统状态（数据到达率、不确定性分布、瞬时资源可用性），最小化计算和通信成本的联合目标函数。

Result: 实验评估表明SA-PSKY持续优于最先进的静态和启发式基线方法。具体而言，通信开销降低高达60%，总响应时间减少40%，并在不同数据分布下保持强大的可扩展性。

Conclusion: SA-PSKY通过强化学习驱动的自适应阈值调整，有效解决了边缘计算环境中概率天际线查询的资源冲突问题，为分布式边缘云协同系统提供了高效、可扩展的解决方案。

Abstract: In the era of the Internet of Everything (IoE), the exponential growth of sensor-generated data at the network edge renders efficient Probabilistic Skyline Query (PSKY) processing a critical challenge. Traditional distributed PSKY methodologies predominantly rely on pre-defined static thresholds to filter local candidates. However, these rigid approaches are fundamentally ill-suited for the highly volatile and heterogeneous nature of edge computing environments, often leading to either severe communication bottlenecks or excessive local computational latency. To resolve this resource conflict, this paper presents SA-PSKY, a novel Self-Adaptive framework designed for distributed edge-cloud collaborative systems. We formalize the dynamic threshold adjustment problem as a continuous Markov Decision Process (MDP) and leverage a Deep Deterministic Policy Gradient (DDPG) agent to autonomously optimize filtering intensities in real-time. By intelligently analyzing multi-dimensional system states, including data arrival rates, uncertainty distributions, and instantaneous resource availability, our framework effectively minimizes a joint objective function of computation and communication costs. Comprehensive experimental evaluations demonstrate that SA-PSKY consistently outperforms state-of-the-art static and heuristic baselines. Specifically, it achieves a reduction of up to 60\% in communication overhead and 40\% in total response time, while ensuring robust scalability across diverse data distributions.

</details>


### [8] [Belief Propagation Converges to Gaussian Distributions in Sparsely-Connected Factor Graphs](https://arxiv.org/abs/2601.21935)
*Tom Yates,Yuzhou Cheng,Ignacio Alzugaray,Danyal Akarca,Pedro A. M. Mediano,Andrew J. Davison*

Main category: cs.DC

TL;DR: 论文证明了在满足特定假设的稀疏连接因子图中，即使面对非高斯问题，置信传播算法的变量信念也会收敛到高斯分布，为高斯置信传播在非高斯问题中的有效性提供了理论保证。


<details>
  <summary>Details</summary>
Motivation: 高斯置信传播（GBP）在计算机视觉和传感器网络等应用中表现出色，即使处理非高斯问题也有效，但缺乏理论解释。本文旨在为GBP在高度非高斯、稀疏连接的因子图中的有效性提供理论保证。

Method: 利用中心极限定理（CLT），在满足四个关键假设的复杂环状因子图中，数学证明置信传播算法的变量信念会收敛到高斯分布。并通过立体深度估计任务进行实验验证。

Result: 理论证明在满足假设的因子图中，变量信念会收敛到高斯分布；实验显示在立体深度估计任务中，仅需几次BP迭代后变量信念就变得高度高斯化。

Conclusion: 为高斯置信传播在非高斯问题中的有效性提供了坚实的理论基础，解释了为什么GBP在空间AI等实际应用中即使面对非高斯问题也能表现良好。

Abstract: Belief Propagation (BP) is a powerful algorithm for distributed inference in probabilistic graphical models, however it quickly becomes infeasible for practical compute and memory budgets. Many efficient, non-parametric forms of BP have been developed, but the most popular is Gaussian Belief Propagation (GBP), a variant that assumes all distributions are locally Gaussian. GBP is widely used due to its efficiency and empirically strong performance in applications like computer vision or sensor networks - even when modelling non-Gaussian problems. In this paper, we seek to provide a theoretical guarantee for when Gaussian approximations are valid in highly non-Gaussian, sparsely-connected factor graphs performing BP (common in spatial AI). We leverage the Central Limit Theorem (CLT) to prove mathematically that variables' beliefs under BP converge to a Gaussian distribution in complex, loopy factor graphs obeying our 4 key assumptions. We then confirm experimentally that variable beliefs become increasingly Gaussian after just a few BP iterations in a stereo depth estimation task.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [FireFly-P: FPGA-Accelerated Spiking Neural Network Plasticity for Robust Adaptive Control](https://arxiv.org/abs/2601.21222)
*Tenglong Li,Jindong Li,Guobin Shen,Dongcheng Zhao,Qian Zhang,Yi Zeng*

Main category: cs.AR

TL;DR: FireFly-P：基于FPGA的硬件加速器，实现SNN可塑性算法，用于机器人实时自适应控制，具有低延迟（8μs）和低功耗（0.713W）特性。


<details>
  <summary>Details</summary>
Motivation: 利用SNN的生物可塑性机制实现无监督自适应控制，避免反向传播的计算开销，为机器人提供在动态非结构化环境中的鲁棒性能。

Method: 设计FireFly-P FPGA硬件加速器，实现新型可塑性算法，通过片上可塑性增强网络泛化能力，支持实时推理和可塑性更新。

Result: 端到端延迟仅8μs，功耗0.713W，占用约10K LUTs（Cmod A7-35T FPGA），在嵌入式机器人平台上实现快速自适应。

Conclusion: 硬件加速的SNN可塑性是实现自适应、低延迟、高能效控制系统的可行路径，特别适合资源受限的嵌入式机器人平台。

Abstract: Spiking Neural Networks (SNNs) offer a biologically plausible learning mechanism through synaptic plasticity, enabling unsupervised adaptation without the computational overhead of backpropagation. To harness this capability for robotics, this paper presents FireFly-P, an FPGA-based hardware accelerator that implements a novel plasticity algorithm for real-time adaptive control. By leveraging on-chip plasticity, our architecture enhances the network's generalization, ensuring robust performance in dynamic and unstructured environments. The hardware design achieves an end-to-end latency of just 8~$μ$s for both inference and plasticity updates, enabling rapid adaptation to unseen scenarios. Implemented on a tiny Cmod A7-35T FPGA, FireFly-P consumes only 0.713~W and $\sim$10K~LUTs, making it ideal for power- and resource-constrained embedded robotic platforms. This work demonstrates that hardware-accelerated SNN plasticity is a viable path toward enabling adaptive, low-latency, and energy-efficient control systems.

</details>


### [10] [Frequency as Aperture: Enabling Embeddable Near-Field Sensing for 6G Wireless Radios](https://arxiv.org/abs/2601.21584)
*Pin-Han Ho,Limei Peng,Yiming Miao,Xu Fan,Kairan Liang,Haoran Mei,Wei Duan*

Main category: cs.AR

TL;DR: FaA利用频率敏捷性构建虚拟传感孔径，通过单个RF链和频率扫描漏波天线实现近场感知，将空间采样从天线域转移到频域，显著提升了架构效率。


<details>
  <summary>Details</summary>
Motivation: 当前毫米波传感大多依赖专用雷达硬件，与成本功耗受限的无线节点不兼容。需要一种无线优先的传感范式，能够将频率敏捷性重新用于虚拟传感孔径，实现近场感知。

Method: 提出频率即孔径(FaA)方法：使用单个RF链和频率扫描漏波天线，重用宽带通信中已有的本地振荡器频率扫描，将空间采样从天线域转移到频域，在通信RF链中嵌入雷达级空间指纹。

Result: FaA在相同物理和频谱约束下，相比传统多通道MIMO传感提供更精细的角度和距离分辨能力，同时功耗和单位成本更低，架构效率显著更高。

Conclusion: 近场传感可以无缝集成到频率敏捷的无线射频中，为智能家居、可穿戴设备和工业边缘部署实现硬件高效、可嵌入且保护隐私的ISAC节点。

Abstract: Integrated sensing and communication (ISAC) is expected to be natively supported by future 6G wireless radios, yet most mmWave sensing solutions still rely on dedicated radar hardware incompatible with cost and power constrained wireless nodes. This article introduces Frequency-as-Aperture (FaA), a wireless-first sensing paradigm that repurposes inherent frequency agility into a virtual sensing aperture, enabling near-field perception with minimal RF front end complexity. Using a single RF chain and a frequency-scanning leaky-wave antenna, FaA achieves two dimensional spatial sensing by reusing the local oscillator (LO) frequency sweep already employed for wideband communication. From a wireless-system perspective, this shifts spatial sampling from the antenna domain to the frequency domain, embedding radar-grade spatial fingerprints directly into the communication RF chain. A case study shows that FaA provides fine angular and range discrimination with low power consumption and unit cost, demonstrating significantly higher architectural efficiency than conventional multi-channel MIMO based sensing under identical physical and spectral constraints. These results indicate that near-field sensing can be seamlessly integrated into frequency-agile wireless radios, enabling hardware-efficient, embeddable, and privacy-preserving ISAC nodes for smart homes, wearables, and industrial edge deployments.

</details>
