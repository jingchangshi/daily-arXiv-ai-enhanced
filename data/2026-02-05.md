<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Pending Conflicts Make Progress Impossible](https://arxiv.org/abs/2602.04013)
*Petr Kuznetsov,Pierre Sutra,Guillermo Toyos-Marfurt*

Main category: cs.DC

TL;DR: 该论文研究共享对象的可交换感知线性化实现的进展条件，提出冲突阻塞自由的概念，但证明在异步读写共享内存模型中无法实现冲突阻塞自由的通用构造


<details>
  <summary>Details</summary>
Motivation: 观察到可交换操作可以并行执行，希望通过利用操作的交换性来改进共享对象实现的进展条件，减少不必要的同步开销

Method: 引入冲突阻塞自由的概念：如果一个进程在足够长的时间内运行而没有遇到与冲突（不可交换）操作的步骤争用，则保证完成其操作。这通过允许只要步骤争用仅由可交换操作引起就能取得进展，来推广阻塞自由和等待自由

Result: 证明在异步读写共享内存模型中，冲突阻塞自由的通用构造是不可能实现的。这一结果揭示了冲突感知通用构造的基本限制：仅仅是冲突操作的调用就会带来同步成本

Conclusion: 进展需要最终解决待处理的冲突，冲突感知的通用构造存在根本性限制，无法完全避免冲突操作带来的同步开销

Abstract: In this work, we study progress conditions for commutativity-aware, linearizable implementations of shared objects. Motivated by the observation that commuting operations can be executed in parallel, we introduce conflict-obstruction-freedom: a process is guaranteed to complete its operation if it runs for long enough without encountering step contention with conflicting (non-commuting) operations. This condition generalizes obstruction-freedom and wait-freedom by allowing progress as long as step contention is only induced by commuting operations. We prove that conflict-obstruction-free universal constructions are impossible to implement in the asynchronous read-write shared memory model. This result exposes a fundamental limitation of conflict-aware universal constructions: the mere invocation of conflicting operations imposes a synchronization cost. Progress requires eventual resolution of pending conflicts.

</details>


### [2] [Six Times to Spare: LDPC Acceleration on DGX Spark for AI-Native Open RAN](https://arxiv.org/abs/2602.04652)
*Ryan Barker,Fatemeh Afghah*

Main category: cs.DC

TL;DR: 论文实证量化了将5G LDPC解码从Grace CPU卸载到Blackwell GB10 GPU的性能优势，在DGX Spark平台上实现了约6倍的吞吐量加速，GPU解码延迟保持在0.5ms时隙内，而CPU解码会超时。


<details>
  <summary>Details</summary>
Motivation: 5G NR物理层中的LDPC解码是计算最密集的核心之一，必须在0.5ms传输时间间隔内完成。许多现有系统仍在通用CPU上执行LDPC解码，随着带宽、调制阶数和用户复用的增加，存在时隙错过和可扩展性受限的问题。

Method: 使用NVIDIA Sionna PHY/SYS在TensorFlow上构建NR类链路级链，包含LDPC5G编码器/解码器、16-QAM调制和AWGN信道。通过扫描并行解码的码字数量和置信传播迭代次数，仅测量解码阶段的性能，同时记录CPU和GPU利用率和功耗。

Result: GPU/CPU吞吐量加速比平均约为6倍。CPU解码延迟在20次迭代时达到约0.71ms（超过0.5ms时隙），而GB10 GPU在相同工作负载下保持在时隙的6-24%范围内。CPU解码通常消耗约10个Grace核心，而GPU解码仅比GPU空闲状态增加10-15W功耗。

Conclusion: GPU卸载LDPC解码能显著提升性能并满足时隙要求，同时释放CPU资源用于更高层任务。基于高级Sionna层的实现代表了可实现的加速器性能的保守下限，为未来平台上的物理层内核评估提供了可重用、可脚本化的方法。

Abstract: Low-density parity-check (LDPC) decoding is one of the most computationally intensive kernels in the 5G New Radio (NR) physical layer and must complete within a 0.5\,ms transmission time interval while sharing the budget with FFT, channel estimation, demapping, HARQ, and MAC scheduling. Many open and proprietary stacks still execute LDPC on general-purpose CPUs, raising concerns about missed-slot events and limited scalability as bandwidths, modulation orders, and user multiplexing increase. This paper empirically quantifies the benefit of offloading 5G-style LDPC5G decoding from a Grace CPU to the integrated Blackwell GB10 GPU on an NVIDIA DGX~Spark platform. Using NVIDIA Sionna PHY/SYS on TensorFlow, we construct an NR-like link-level chain with an LDPC5G encoder/decoder, 16-QAM modulation, and AWGN, and sweep both the number of codewords decoded in parallel and the number of belief-propagation iterations, timing only the decoding phase while logging CPU and GPU utilization and power. Across the sweep we observe an average GPU/CPU throughput speedup of approximately $6\times$, with per-codeword CPU latency reaching $\approx 0.71$\,ms at 20 iterations (exceeding the 0.5\,ms slot), while the GB10 GPU remains within 6--24\% of the slot for the same workloads. Resource-usage measurements show that CPU-based LDPC decoding often consumes around ten Grace cores, whereas GPU-based decoding adds only $\approx10-15$\,W over GPU idle while leaving most CPU capacity available for higher-layer tasks. Because our implementation relies on high-level Sionna layers rather than hand-tuned CUDA, these results represent conservative lower bounds on achievable accelerator performance and provide a reusable, scriptable methodology for evaluating LDPC and other physical-layer kernels on future Grace/Blackwell and Aerial/ACAR/AODT platforms.

</details>


### [3] [A TEE-based Approach for Preserving Data Secrecy in Process Mining with Decentralized Sources](https://arxiv.org/abs/2602.04697)
*Davide Basile,Valerio Goretti,Luca Barbaro,Hajo A. Reijers,Claudio Di Ciccio*

Main category: cs.DC

TL;DR: CONFINE：基于可信执行环境（TEE）的保密性保护跨组织流程挖掘方法，通过四阶段协议实现安全数据交换和处理，避免内存溢出，支持可扩展的现实工作负载。


<details>
  <summary>Details</summary>
Motivation: 跨组织流程挖掘面临重大挑战，特别是保密性问题：数据分析可能泄露参与组织不愿向彼此或第三方披露的信息。现有流程挖掘主要关注组织内场景，而现实业务流程常跨越多个独立组织，需要解决数据保密性限制。

Method: 提出CONFINE方法，利用可信执行环境（TEE）部署可信应用程序，安全地挖掘多方事件日志同时保持数据保密性。采用四阶段协议保护数据传输和处理，通过分段策略将事件日志分批传输到TEE以避免内存溢出。

Result: 对实现进行了形式化正确性验证和安全分析，在真实和合成数据上评估表明方法能处理现实工作负载。结果显示内存增长与事件日志大小呈对数关系，与供应组织数量呈线性关系，具有可扩展性。

Conclusion: CONFINE为跨组织流程挖掘提供了有效的保密性保护解决方案，利用TEE技术实现安全数据处理，分段策略解决了内存限制问题，展示了良好的可扩展性和进一步优化的潜力。

Abstract: Process mining techniques enable organizations to gain insights into their business processes through the analysis of execution records (event logs) stored by information systems. While most process mining efforts focus on intra-organizational scenarios, many real-world business processes span multiple independent organizations. Inter-organizational process mining, though, faces significant challenges, particularly regarding confidentiality guarantees: The analysis of data can reveal information that the participating organizations may not consent to disclose to one another, or to a third party hosting process mining services. To overcome this issue, this paper presents CONFINE, an approach for secrecy-preserving inter-organizational process mining. CONFINE leverages Trusted Execution Environments (TEEs) to deploy trusted applications that are capable of securely mining multi-party event logs while preserving data secrecy. We propose an architecture supporting a four-stage protocol to secure data exchange and processing, allowing for protected transfer and aggregation of unaltered process data across organizational boundaries. To avoid out-of-memory errors due to the limited capacity of TEEs, our protocol employs a segmentation-based strategy, whereby event logs are transmitted to TEEs in smaller batches. We conduct a formal verification of correctness and a security analysis of the guarantees provided by the TEE core. We evaluate our implementation on real-world and synthetic data, showing that the proposed approach can handle realistic workloads. The results indicate logarithmic memory growth with respect to the event log size and linear growth with the number of provisioning organizations, highlighting scalability properties and opportunities for further optimization.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [4] [SPPAM: Signature Pattern Prediction and Access-Map Prefetcher](https://arxiv.org/abs/2602.04100)
*Maccoy Merrell,Lei Wang,Stavros Kalafatis,Paul V. Gratz*

Main category: cs.AR

TL;DR: SPPAM是一种新的缓存预取方法，结合了SPP和AMPM的优点，通过在线学习构建访问模式图，并使用置信度调节的推测性前瞻机制，在二级缓存上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 处理器速度与内存系统性能之间的差距持续限制着许多工作负载的性能。现有的预取技术如SPP和AMPM各有局限性：SPP对高层缓存和乱序核心的引用重排序非常敏感；AMPM虽然对重排序有抵抗力，但无法推测超出区域范围。需要一种能结合两者优点并克服其局限性的新方法。

Method: SPPAM采用在线学习构建一组访问模式图（access-map patterns），这些模式图用于置信度调节的推测性前瞻（speculative lookahead）。该方法针对二级缓存，结合了SPP的推测能力和AMPM对重排序的抵抗力。

Result: SPPAM与最先进的预取器Berti和Bingo结合，在无预取基础上提升系统性能31.4%，在Berti和Pythia基线基础上提升6.2%。

Conclusion: SPPAM成功结合了SPP和AMPM的优点，通过在线学习访问模式图和置信度调节的推测机制，有效解决了现有预取技术的局限性，在二级缓存上实现了显著的性能改进。

Abstract: The discrepancy between processor speed and memory system performance continues to limit the performance of many workloads. To address the issue, one effective and well studied technique is cache prefetching. Many prefetching designs have been proposed, with varying approaches and effectiveness. For example, SPP is a popular prefetcher that leverages confidence throttled recursion to speculate on the future path of program's references, however it is very susceptible to the reference reordering of higher-level caches and the OoO core. Orthogonally, AMPM is another popular approach to prefetching which uses reordering-resistant access maps to identify patterns within a region, but is unable to speculate beyond that region. In this paper, we propose SPPAM, a new approach to prefetching, inspired by prior works such as SPP and AMPM, while addressing their limitations. SPPAM utilizes online-learning to build a set of access-map patterns. These patterns are used in a speculative lookahead which is throttled by a confidence metric. Targeting the second-level cache, SPPAM alongside state-of-the-art prefetchers Berti and Bingo improve system performance by 31.4% over no prefetching and 6.2% over the baseline of Berti and Pythia.

</details>


### [5] [Crypto-RV: High-Efficiency FPGA-Based RISC-V Cryptographic Co-Processor for IoT Security](https://arxiv.org/abs/2602.04415)
*Anh Kiet Pham,Van Truong Vo,Vu Trung Duong Le,Tuan Hai Vu,Hoai Luan Pham,Van Tinh Nguyen,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: Crypto-RV：一个统一的RISC-V协处理器架构，支持多种加密算法和抗量子密码，在资源受限的IoT环境中实现高性能、高能效的加密处理。


<details>
  <summary>Details</summary>
Motivation: 当前RISC-V平台缺乏对全面加密算法家族和抗量子密码的高效硬件支持，而加密操作对于IoT、边缘计算和自治系统的安全至关重要。

Method: 提出Crypto-RV架构，包含三个关键创新：高带宽内部缓冲区（128x64位）、专门化的加密执行单元（四阶段流水线数据路径）、以及针对大哈希优化的双缓冲机制和自适应调度。

Result: 在Xilinx ZCU102 FPGA上实现，运行频率160MHz，动态功耗0.851W，相比基线RISC-V核心获得165-1061倍加速，相比强大CPU获得5.8-17.4倍能效提升，仅占用34,704 LUTs、37,329 FFs和22 BRAMs。

Conclusion: Crypto-RV展示了在资源受限的IoT环境中实现高性能、高能效加密处理的可行性，为RISC-V平台提供了全面的加密硬件支持。

Abstract: Cryptographic operations are critical for securing IoT, edge computing, and autonomous systems. However, current RISC-V platforms lack efficient hardware support for comprehensive cryptographic algorithm families and post-quantum cryptography. This paper presents Crypto-RV, a RISC-V co-processor architecture that unifies support for SHA-256, SHA-512, SM3, SHA3-256, SHAKE-128, SHAKE-256 AES-128, HARAKA-256, and HARAKA-512 within a single 64-bit datapath. Crypto-RV introduces three key architectural innovations: a high-bandwidth internal buffer (128x64-bit), cryptography-specialized execution units with four-stage pipelined datapaths, and a double-buffering mechanism with adaptive scheduling optimized for large-hash. Implemented on Xilinx ZCU102 FPGA at 160 MHz with 0.851 W dynamic power, Crypto-RV achieves 165 times to 1,061 times speedup over baseline RISC-V cores, 5.8 times to 17.4 times better energy efficiency compared to powerful CPUs. The design occupies only 34,704 LUTs, 37,329 FFs, and 22 BRAMs demonstrating viability for high-performance, energy-efficient cryptographic processing in resource-constrained IoT environments.

</details>


### [6] [Harmonia: Algorithm-Hardware Co-Design for Memory- and Compute-Efficient BFP-based LLM Inference](https://arxiv.org/abs/2602.04595)
*Xinyu Wang,Jieyu Li,Yanan Sun,Weifeng He*

Main category: cs.AR

TL;DR: Harmonia：一种算法-硬件协同设计框架，通过在所有层使用可配置的块浮点数（BFP）激活，实现LLM的高效量化，特别解决了注意力层中BFP应用导致精度下降的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然强大，但内存和计算成本高昂。量化是有效的解决方案，但现有工作无法将块浮点数（BFP）激活扩展到注意力层，导致严重精度下降，限制了整体效率。

Method: 1. 系统探索BFP配置，在所有层实现精度与激活压缩的更好权衡；2. 引入非对称位分配策略和混合离线-在线异常值平滑技术，实现KV缓存从FP16到4位尾数BFP的激进压缩；3. 设计专用硬件组件，包括支持混合数据格式的可重构PE、实时FP16到BFP转换器和分块感知数据流。

Result: 在8个广泛使用的LLM的线性和注意力层GEMM操作中评估，相比先前工作，Harmonia平均实现3.84倍（最高5.05倍）的面积效率提升、2.03倍（最高3.90倍）的能效提升和3.08倍（最高4.62倍）的加速。

Conclusion: Harmonia成功解决了注意力层应用BFP激活的精度问题，通过算法-硬件协同设计实现了所有层BFP激活，显著提升了LLM推理的效率和性能。

Abstract: Large Language Models (LLMs) are powerful but incur high memory and computation costs. Quantization is an effective solution, with INT weights and FP activations being widely adopted to preserve accuracy. Prior works further reduce FP overhead by using block floating point (BFP) activations in linear layers, but fail to extend BFP to attention layers due to severe accuracy degradation, limiting overall efficiency. To address this challenge, we propose Harmonia, an algorithm-hardware co-design framework that enables all-layer BFP activations with a configurable hardware architecture. First, we systematically explore BFP configurations to achieve a better trade-off between accuracy and activation compression across all layers. Second, to reduce KV-cache storage and computation in attention layers, we introduce an asymmetric bit-allocation strategy and computations in attention layers,we introduce an asymmetric bit-allocation strategy combined with a hybrid offline-online outlier smoothing technique. This allow aggressive KV-cache compression from FP16 to 4-bit-mantissa BFP with only 0.3% average accuracy loss. Third, to fully exploit all-layer BFP activations, we design dedicated hardware components, including a reconfigurable PE supporting mixed data formats (BFP-INT and BPF-BFP), a real-time FP16-to-BFP converter, and a tiling-aware dataflow to reduce memory traffic. We evaluate Harmonia on GEMM operations in both linear and attention layers across eight widely used LLMs. Compared with prior works, Harmonia achieves 3.84x (up to 5.05x) higher area efficiency, 2.03x (up to 3.90x) better energy efficiency, and 3.08x (up to 4.62x) speedup on average.

</details>
