{"id": "2508.03837", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.03837", "abs": "https://arxiv.org/abs/2508.03837", "authors": ["Davide Zoni", "Andrea Galimberti", "Adriano Guarisco"], "title": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent Memory Subsystems", "comment": "9 pages, 13 figures, 1 table, accepted for presentation at 2025\n  International Conference on Computer-Aided Design (ICCAD), Munich, Germany,\n  October 26-30, 2025", "summary": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs."}
{"id": "2508.03866", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.03866", "abs": "https://arxiv.org/abs/2508.03866", "authors": ["Seock-Hwan Noh", "Hoyeon Lee", "Junkyum Kim", "Junsu Im", "Jay H. Park", "Sungjin Lee", "Sam H. Noh", "Yeseong Kim", "Jaeha Kung"], "title": "FlashVault: Versatile In-NAND Self-Encryption with Zero Area Overhead", "comment": "15 pages, 14 figures, Under submission", "summary": "We present FlashVault, an in-NAND self-encryption architecture that embeds a\nreconfigurable cryptographic engine into the unused silicon area of a\nstate-of-the-art 4D V-NAND structure. FlashVault supports not only block\nciphers for data encryption but also public-key and post-quantum algorithms for\ndigital signatures, all within the NAND flash chip. This design enables each\nNAND chip to operate as a self-contained enclave without incurring area\noverhead, while eliminating the need for off-chip encryption. We implement\nFlashVault at the register-transfer level (RTL) and perform place-and-route\n(P&R) for accurate power/area evaluation. Our analysis shows that the power\nbudget determines the number of cryptographic engines per NAND chip. We\nintegrate this architectural choice into a full-system simulation and evaluate\nits performance on a wide range of cryptographic algorithms. Our results show\nthat FlashVault consistently outperforms both CPU-based encryption (1.46~3.45x)\nand near-core processing architecture (1.02~2.01x), demonstrating its\neffectiveness as a secure SSD architecture that meets diverse cryptographic\nrequirements imposed by regulatory standards and enterprise policies."}
{"id": "2508.03900", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.03900", "abs": "https://arxiv.org/abs/2508.03900", "authors": ["Navaneeth Kunhi Purayil", "Diyou Shen", "Matteo Perotti", "Luca Benini"], "title": "TROOP: At-the-Roofline Performance for Vector Processors on Low Operational Intensity Workloads", "comment": "To be published in IEEE International Conference on Computer Design\n  (ICCD) 2025", "summary": "The fast evolution of Machine Learning (ML) models requires flexible and\nefficient hardware solutions as hardwired accelerators face rapid obsolescence.\nVector processors are fully programmable and achieve high energy efficiencies\nby exploiting data parallelism, amortizing instruction fetch and decoding\ncosts. Hence, a promising design choice is to build accelerators based on\nshared L1-memory clusters of streamlined Vector Processing Elements (VPEs).\nHowever, current state-of-the-art VPEs are limited in L1 memory bandwidth and\nachieve high efficiency only for computational kernels with high data reuse in\nthe Vector Register File (VRF), such as General Matrix Multiplication (GEMM).\nPerformance is suboptimal for workloads with lower data reuse like General\nMatrix-Vector Multiplication (GEMV). To fully exploit available bandwidth at\nthe L1 memory interface, the VPE micro-architecture must be optimized to\nachieve near-ideal utilization, i.e., to be as close as possible to the L1\nmemory roofline (at-the-roofline). In this work, we propose TROOP, a set of\nhardware optimizations that include decoupled load-store interfaces, improved\nvector chaining, shadow buffers to hide VRF conflicts, and address scrambling\ntechniques to achieve at-the-roofline performance for VPEs without compromising\ntheir area and energy efficiency. We implement TROOP on an open-source\nstreamlined vector processor in a 12nm FinFET technology. TROOP achieves\nsignificant speedups of 1.5x, 2.2x, and 2.6x, respectively, for key\nmemory-intensive kernels such as GEMV, DOTP and AXPY, achieving at-the-roofline\nperformance. Additionally, TROOP enhances the energy efficiency by up to 45%,\nreaching 38 DP-GFLOPs/W (1 GHz, TT, 0.8V) for DOTP while maintaining a high\nenergy efficiency of 61 DP-GFLOPs/W for GEMMs, incurring only a minor area\noverhead of less than 7%."}
{"id": "2508.04106", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.04106", "abs": "https://arxiv.org/abs/2508.04106", "authors": ["Shan Shen", "Xingyang Li", "Zhuohua Liu", "Yikai Wang", "Yiheng Wu", "Junhao Ma", "Yuquan Sun", "Wei W. Xing"], "title": "OpenYield: An Open-Source SRAM Yield Analysis and Optimization Benchmark Suite", "comment": "Accepted by The 43rd IEEE International Conference on Computer Design\n  (ICCD2025)", "summary": "Static Random-Access Memory (SRAM) yield analysis is essential for\nsemiconductor innovation, yet research progress faces a critical challenge: the\nsignificant disconnect between simplified academic models and complex\nindustrial realities. The absence of open, realistic benchmarks has created a\nreproducibility crisis, where promising academic techniques often fail to\ntranslate to industrial practice. We present \\textit{OpenYield}, a\ncomprehensive open-source ecosystem designed to address this critical gap\nthrough three core contributions: (1) A realistic SRAM circuit generator that\nuniquely incorporates critical second-order-effect parasitics, inter-cell\nleakage coupling, and peripheral circuit variations, which are typically\nomitted in academic studies but decisive in industrial designs. (2) A\nstandardized evaluation platform with a simple interface and implemented\nbaseline yield analysis algorithms, enabling fair comparisons and reproducible\nresearch. (3) A standardized SRAM optimization platform, demonstrating\nOpenYield's utility in enhancing SRAM design robustness and efficiency,\nproviding a comprehensive benchmark for optimization algorithms. OpenYield\ncreates a foundation for meaningful academia-industry collaboration,\naccelerating innovation in memory design. The framework is publicly available\non \\href{https://github.com/ShenShan123/OpenYield}{OpenYield:URL}"}
{"id": "2508.03830", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.03830", "abs": "https://arxiv.org/abs/2508.03830", "authors": ["Hanwen Guo", "Ben Greenman"], "title": "If-T: A Benchmark for Type Narrowing", "comment": null, "summary": "**Context:** The design of static type systems that can validate\ndynamically-typed programs (**gradually**) is an ongoing challenge. A key\ndifficulty is that dynamic code rarely follows datatype-driven design. Programs\ninstead use runtime tests to narrow down the proper usage of incoming data.\nType systems for dynamic languages thus need a **type narrowing** mechanism\nthat refines the type environment along individual control paths based on\ndominating tests, a form of flow-sensitive typing. In order to express\nrefinements, the type system must have some notion of sets and subsets. Since\nset-theoretic types are computationally and ergonomically complex, the need for\ntype narrowing raises design questions about how to balance precision and\nperformance. **Inquiry:** To date, the design of type narrowing systems has\nbeen driven by intuition, past experience, and examples from users in various\nlanguage communities. There is no standard that captures desirable and\nundesirable behaviors. Prior formalizations of narrowing are also significantly\nmore complex than a standard type system, and it is unclear how the extra\ncomplexity pays off in terms of concrete examples. This paper addresses the\nproblems through If-T, a language-agnostic **design benchmark** for type\nnarrowing that characterizes the abilities of implementations using simple\nprograms that draw attention to fundamental questions. Unlike a traditional\nperformance-focused benchmark, If-T measures a narrowing system's ability to\nvalidate correct code and reject incorrect code. Unlike a test suite, systems\nare not required to fully conform to If-T. Deviations are acceptable provided\nthey are justified by well-reasoned design considerations, such as compile-time\nperformance. **Approach:** If-T is guided by the literature on type narrowing,\nthe documentation of gradual languages such as TypeScript, and experiments with\ntypechecker implementations. We have identified a set of core technical\ndimensions for type narrowing. For each dimension, the benchmark contains a set\nof topics and (at least) two characterizing programs per topic: one that should\ntypecheck and one that should not typecheck. **Knowledge:** If-T provides a\nbaseline to measure type narrowing systems. For researchers, it provides\ncriteria to categorize future designs via its collection of positive and\nnegative examples. For language designers, the benchmark demonstrates the\npayoff of typechecker complexity in terms of concrete examples. Designers can\nuse the examples to decide whether supporting a particular example is\nworthwhile. Both the benchmark and its implementations are freely available\nonline. **Grounding:** We have implemented the benchmark for five typecheckers:\nTypeScript, Flow, Typed Racket, mypy, and Pyright. The results highlight\nimportant differences, such as the ability to track logical implications among\nprogram variables and typechecking for user-defined narrowing predicates.\n**Importance:** Type narrowing is essential for gradual type systems, but the\ntradeoffs between systems with different complexity have been unclear. If-T\nclarifies these tradeoffs by illustrating the benefits and limitations of each\nlevel of complexity. With If-T as a way to assess implementations in a fair,\ncross-language manner, future type system designs can strive for a better\nbalance among precision, annotation burden, and performance."}
{"id": "2508.03760", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03760", "abs": "https://arxiv.org/abs/2508.03760", "authors": ["Qingyuan Li", "Bo Zhang", "Hui Kang", "Tianhao Xu", "Yulei Qian", "Yuchen Xie", "Lin Ma"], "title": "FlashCommunication V2: Bit Splitting and Spike Reserving for Any Bit Communication", "comment": "9 pages, 8 figures", "summary": "Nowadays, communication bottlenecks have emerged as a critical challenge in\nthe distributed training and deployment of large language models (LLMs). This\npaper introduces FlashCommunication V2, a novel communication paradigm enabling\nefficient cross-GPU transmission at arbitrary bit widths. Its core innovations\nlie in the proposed bit splitting and spike reserving techniques, which address\nthe challenges of low-bit quantization. Bit splitting decomposes irregular bit\nwidths into basic units, ensuring compatibility with hardware capabilities and\nthus enabling transmission at any bit width. Spike reserving, on the other\nhand, retains numerical outliers (i.e., minima and maxima) as floating-point\nnumbers, which shrinks the dynamic numerical range and pushes the quantization\nlimits to 2-bit with acceptable losses. FlashCommunication V2 significantly\nenhances the flexibility and resource utilization of communication systems.\nThrough meticulous software-hardware co-design, it delivers robust performance\nand reduced overhead across both NVLink-based and PCIe-based architectures,\nachieving a maximum 3.2$\\times$ speedup in AllReduce and 2$\\times$ in All2All\ncommunication."}
{"id": "2508.04516", "categories": ["cs.AR", "cs.ET", "C.3; B.6; B.7.1"], "pdf": "https://arxiv.org/pdf/2508.04516", "abs": "https://arxiv.org/abs/2508.04516", "authors": ["Ishraq Tashdid", "Dewan Saiham", "Nafisa Anjum", "Tasnuva Farheen", "Sazadur Rahman"], "title": "ECOLogic: Enabling Circular, Obfuscated, and Adaptive Logic via eFPGA-Augmented SoCs", "comment": "10 pages, 7 figures. Extended version of accepted short paper at IEEE\n  International Conference on Computer Design (ICCD) 2025, Dallas, TX, USA", "summary": "Traditional hardware platforms - ASICs and FPGAs - offer competing trade-offs\namong performance, flexibility, and sustainability. ASICs provide high\nefficiency but are inflexible post-fabrication, require costly re-spins for\nupdates, and expose IPs to piracy risks. FPGAs offer reconfigurability and\nreuse, yet suffer from substantial area, power, and performance overheads,\nresulting in higher carbon footprints. We present ECOLogic, a hybrid design\nparadigm that embeds lightweight eFPGA fabric within ASICs to enable secure,\nupdatable, and resource-aware computation. Central to this architecture is\nECOScore, a quantitative scoring framework that evaluates IPs based on\nadaptability, piracy threat, performance tolerance, and resource fit to guide\nRTL partitioning. Evaluated across six diverse SoC modules, ECOLogic retains an\naverage of 90 percent ASIC-level performance (up to 2 GHz), achieves 9.8 ns\ntiming slack (versus 5.1 ns in FPGA), and reduces power by 480 times on\naverage. Moreover, sustainability analysis shows a 99.7 percent reduction in\ndeployment carbon footprint and 300 to 500 times lower emissions relative to\nFPGA-only implementations. These results position ECOLogic as a\nhigh-performance, secure, and environmentally sustainable solution for\nnext-generation reconfigurable systems."}
{"id": "2508.03831", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.03831", "abs": "https://arxiv.org/abs/2508.03831", "authors": ["Chinmayi Prabhu Baramashetru", "Paola Giannini", "Silvia Lizeth Tapia Tarifa", "Olaf Owe"], "title": "A Type System for Data Privacy Compliance in Active Object Languages", "comment": null, "summary": "Data protection laws such as GDPR aim to give users unprecedented control\nover their personal data. Compliance with these regulations requires\nsystematically considering information flow and interactions among entities\nhandling sensitive data. Privacy-by-design principles advocate embedding data\nprotection into system architectures as a default. However, translating these\nabstract principles into concrete, explicit methods remains a significant\nchallenge. This paper addresses this gap by proposing a language-based approach\nto privacy integration, combining static and runtime techniques. By employing\ntype checking and type inference in an active object language, the framework\nenables the tracking of authorised data flows and the automatic generation of\nconstraints checked at runtime based on user consent. This ensures that\npersonal data is processed in compliance with GDPR constraints. The key\ncontribution of this work is a type system that gather the compliance checks\nand the changes to users consent and integrates data privacy compliance\nverification into system execution. The paper demonstrates the feasibility of\nthis approach through a soundness proof and several examples, illustrating how\nthe proposed language addresses common GDPR requirements, such as user consent,\npurpose limitation, and data subject rights. This work advances the state of\nthe art in privacy-aware system design by offering a systematic and automated\nmethod for integrating GDPR compliance into programming languages. This\ncapability has implications for building trustworthy systems in domains such as\nhealthcare or finance, where data privacy is crucial."}
{"id": "2508.03854", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03854", "abs": "https://arxiv.org/abs/2508.03854", "authors": ["Xin Zhang", "Quanyu Zhu", "Liangbei Xu", "Zain Huda", "Wang Zhou", "Jin Fang", "Dennis van der Staay", "Yuxi Hu", "Jade Nie", "Jiyan Yang", "Chunzhi Yang"], "title": "Two-dimensional Sparse Parallelism for Large Scale Deep Learning Recommendation Model Training", "comment": null, "summary": "The increasing complexity of deep learning recommendation models (DLRM) has\nled to a growing need for large-scale distributed systems that can efficiently\ntrain vast amounts of data. In DLRM, the sparse embedding table is a crucial\ncomponent for managing sparse categorical features. Typically, these tables in\nindustrial DLRMs contain trillions of parameters, necessitating model\nparallelism strategies to address memory constraints. However, as training\nsystems expand with massive GPUs, the traditional fully parallelism strategies\nfor embedding table post significant scalability challenges, including\nimbalance and straggler issues, intensive lookup communication, and heavy\nembedding activation memory. To overcome these limitations, we propose a novel\ntwo-dimensional sparse parallelism approach. Rather than fully sharding tables\nacross all GPUs, our solution introduces data parallelism on top of model\nparallelism. This enables efficient all-to-all communication and reduces peak\nmemory consumption. Additionally, we have developed the momentum-scaled\nrow-wise AdaGrad algorithm to mitigate performance losses associated with the\nshift in training paradigms. Our extensive experiments demonstrate that the\nproposed approach significantly enhances training efficiency while maintaining\nmodel performance parity. It achieves nearly linear training speed scaling up\nto 4K GPUs, setting a new state-of-the-art benchmark for recommendation model\ntraining."}
{"id": "2508.04609", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.04609", "abs": "https://arxiv.org/abs/2508.04609", "authors": ["Osama Abdelaleim", "Arun Prakash", "Ayhan Irfanoglu", "Veljko Milutinovic"], "title": "Near instantaneous O(1) Analog Solver Circuit for Linear Symmetric Positive-Definite Systems", "comment": null, "summary": "Accelerating the solution of linear systems of equations is critical due to\ntheir central role in numerous applications, such as scientific simulations,\ndata analytics, and machine learning. This paper presents a general-purpose\nanalog direct solver circuit designed to accelerate the solution of positive\ndefinite symmetric linear systems of equations. The proposed design leverages\nnon-inverting operational amplifier configurations to create a negative\nresistance circuit, effectively modeling any symmetric system. The paper\ndetails the principles behind the design, optimizations of the system\narchitecture, and numerical results that demonstrate the robustness of the\ndesign. The findings reveal that the proposed system solves diagonally dominant\nsymmetric matrices with O(1) complexity, achieving the theoretical maximum\nspeed as the circuit relies solely on resistors. For non-diagonally dominant\nsymmetric positive-definite systems, the solution speed depends on matrix\nproperties such as eigenvalues and the maximum off-diagonal term, but remains\nindependent of matrix size."}
{"id": "2508.03832", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.03832", "abs": "https://arxiv.org/abs/2508.03832", "authors": ["Andreas Pointner", "Josef Pichler", "Herbert Prähofer"], "title": "Generating Inputs for Grammar Mining using Dynamic Symbolic Execution", "comment": null, "summary": "A vast number of software systems include components that parse and process\nstructured input. In addition to programming languages, which are analyzed by\ncompilers or interpreters, there are numerous components that process\nstandardized or proprietary data formats of varying complexity. Even if such\ncomponents were initially developed and tested based on a specification, such\nas a grammar, numerous modifications and adaptations over the course of\nsoftware evolution can make it impossible to precisely determine which inputs\nthey actually accept. In this situation, grammar mining can be used to\nreconstruct the specification in the form of a grammar. Established approaches\nalready produce useful results, provided that sufficient input data is\navailable to fully cover the input language. However, achieving this\ncompleteness is a major challenge. In practice, only input data recorded during\nthe operation of the software systems is available. If this data is used for\ngrammar mining, the resulting grammar reflects only the actual processed inputs\nbut not the complete grammar of the input language accepted by the software\ncomponent. As a result, edge cases or previously supported features that no\nlonger appear in the available input data are missing from the generated\ngrammar. This work addresses this challenge by introducing a novel approach for\nthe automatic generation of inputs for grammar mining. Although input\ngenerators have already been used for fuzz testing, it remains unclear whether\nthey are also suitable for grammar miners. Building on the grammar miner Mimid,\nthis work presents a fully automated approach to input generation. The approach\nleverages Dynamic Symbolic Execution (DSE) and extends it with two mechanisms\nto overcome the limitations of DSE regarding structured input parsers. First,\nthe search for new inputs is guided by an iterative expansion that starts with\na single-character input and gradually extends it. Second, input generation is\nstructured into a novel three-phase approach, which separates the generation of\ninputs for parser functions. The proposed method was evaluated against a\ndiverse set of eleven benchmark applications from the existing literature.\nResults demonstrate that the approach achieves precision and recall for\nextracted grammars close to those derived from state-of-the-art grammar miners\nsuch as Mimid. Notably, it successfully uncovers subtle features and edge cases\nin parsers that are typically missed by such grammar miners. The effectiveness\nof the method is supported by empirical evidence, showing that it can achieve\nhigh performance in various domains without requiring prior input samples. This\ncontribution is significant for researchers and practitioners in software\nengineering, offering an automated, scalable, and precise solution for grammar\nmining. By eliminating the need for manual input generation, the approach not\nonly reduces workload but also enhances the robustness and comprehensiveness of\nthe extracted grammars. Following this approach, software engineers can\nreconstruct specification from existing (legacy) parsers."}
{"id": "2508.03981", "categories": ["cs.DC", "cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.03981", "abs": "https://arxiv.org/abs/2508.03981", "authors": ["Zhikui Chen", "Muhammad Zeeshan Haider", "Naiwen Luo", "Shuo Yu", "Xu Yuan", "Yaochen Zhang", "Tayyaba Noreen"], "title": "Reputation-based partition scheme for IoT security", "comment": null, "summary": "With the popularity of smart terminals, such as the Internet of Things,\ncrowdsensing is an emerging data aggregation paradigm, which plays a pivotal\nrole in data-driven applications. There are some key issues in the development\nof crowdsensing such as platform security and privacy protection. As the\ncrowdsensing is usually managed by a centralized platform, centralized\nmanagement will bring various security vulnerabilities and scalability issues.\nTo solve these issues, an effective reputation-based partition scheme (RSPC) is\nproposed in this article. The partition scheme calculates the optimal partition\nsize by combining the node reputation value and divides the node into several\ndisjoint partitions according to the node reputation value. By selecting the\nappropriate partition size, RSPC provides a mechanism to ensure that each\npartition is valid, as long as themaximum permissible threshold for the failed\nnode is observed. At the same time, the RSPC reorganizes the network\nperiodically to avoid partition attacks. In addition, for cross-partition\ntransactions, this paper innovatively proposes a four-stage confirmation\nprotocol to ensure the efficient and safe completion of cross-partition\ntransactions. Finally, experiments show that RSPC improves scalability, low\nlatency, and high throughput for crowdsensing."}
{"id": "2508.04115", "categories": ["cs.PL", "A.1; C.1.2; D.3.1; F.3.1; F.3.2"], "pdf": "https://arxiv.org/pdf/2508.04115", "abs": "https://arxiv.org/abs/2508.04115", "authors": ["Roger C. Su", "Robert J. Colvin"], "title": "Weak Memory Model Formalisms: Introduction and Survey", "comment": null, "summary": "Memory consistency models define the order in which accesses to shared memory\nin a concurrent system may be observed to occur. Such models are a necessity\nsince program order is not a reliable indicator of execution order, due to\nmicroarchitectural features or compiler transformations. Concurrent\nprogramming, already a challenging task, is thus made even harder when weak\nmemory effects must be addressed. A rigorous specification of weak memory\nmodels is therefore essential to make this problem tractable for developers of\nsafety- and security-critical, low-level software.\n  In this paper we survey the field of formalisations of weak memory models,\nincluding their specification, their effects on execution, and tools and\ninference systems for reasoning about code. To assist the discussion we also\nprovide an introduction to two styles of formal representation found commonly\nin the literature (using a much simplified version of Intel's x86 as the\nexample): a step-by-step construction of traces of the system (operational\nsemantics); and with respect to relations between memory events (axiomatic\nsemantics). The survey covers some long-standing hardware features that lead to\nobservable weak behaviours, a description of historical developments in\npractice and in theory, an overview of computability and complexity results,\nand outlines current and future directions in the field."}
{"id": "2508.03984", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.03984", "abs": "https://arxiv.org/abs/2508.03984", "authors": ["Yuki Uchino", "Katsuhisa Ozaki", "Toshiyuki Imamura"], "title": "High-Performance and Power-Efficient Emulation of Matrix Multiplication using INT8 Matrix Engines", "comment": "8 pages, 9 figures", "summary": "Recent architectures integrate high-performance and power-efficient matrix\nengines. These engines demonstrate remarkable performance in low-precision\nmatrix multiplication, which is crucial in deep learning. Several techniques\nhave been proposed to emulate single- and double-precision general\nmatrix-matrix multiplication (SGEMM and DGEMM, respectively) by leveraging such\nlow-precision matrix engines. In this study, we present emulation methods that\nsignificantly outperforms conventional approaches. On a GH200 Grace Hopper\nSuperchip, the proposed DGEMM emulation achieves a 1.4x speedup and a 43\\%\nimprovement in power efficiency compared to native DGEMM for sufficiently large\nproblems. The proposed SGEMM emulation achieves a 3.0x speedup and a 154\\%\nimprovement in power efficiency compared to native SGEMM for sufficiently large\nproblems. Furthermore, compared to conventional emulation methods, the proposed\nemulation achieves more than 2x higher performance and superior power\nefficiency."}
{"id": "2508.04000", "categories": ["cs.DC", "cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.04000", "abs": "https://arxiv.org/abs/2508.04000", "authors": ["Tayyaba Noreen", "Qiufen Xia", "Muhammad Zeeshan Haider"], "title": "Advanced DAG-Based Ranking (ADR) Protocol for Blockchain Scalability", "comment": null, "summary": "In the past decade, blockchain has emerged as a promising solution for\nbuilding secure distributed ledgers and has attracted significant attention.\nHowever, current blockchain systems suffer from limited throughput, poor\nscalability, and high latency. Due to limitations in consensus mechanisms,\nespecially in managing node identities, blockchain is often considered\nunsuitable for applications such as the Internet of Things (IoT). This paper\nproposes the Advanced DAG-based Ranking (ADR) protocol to enhance blockchain\nscalability and throughput. ADR employs a directed acyclic graph (DAG)\nstructure where nodes are positioned based on their rankings. Unlike\ntraditional chains, ADR allows honest nodes to write blocks and verify\ntransactions using a DAG-based topology. The protocol follows a three-step\napproach to secure the network against double-spending and enhance performance.\nFirst, it verifies nodes using their public and private keys before granting\nentry. Second, it builds an advanced DAG ledger enabling block production and\ntransaction validation. Third, a ranking algorithm filters out malicious nodes,\nranks the remaining nodes based on performance, and arranges them\ntopologically. This process increases throughput and ensures robust\nscalability. We evaluated ADR on Amazon EC2 clusters with over 100 nodes,\nincluding scenarios with injected malicious nodes. Simulation results\ndemonstrate that ADR significantly improves transaction throughput and network\nliveness compared to existing DAG-based blockchains such as IOTA and ByteBall,\nmaking it well-suited for IoT applications."}
{"id": "2508.04013", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.04013", "abs": "https://arxiv.org/abs/2508.04013", "authors": ["Sameh Abdulah", "Mary Lai O. Salvana", "Ying Sun", "David E. Keyes", "Marc G. Genton"], "title": "High-Performance Statistical Computing (HPSC): Challenges, Opportunities, and Future Directions", "comment": null, "summary": "We recognize the emergence of a statistical computing community focused on\nworking with large computing platforms and producing software and applications\nthat exemplify high-performance statistical computing (HPSC). The statistical\ncomputing (SC) community develops software that is widely used across\ndisciplines. However, it remains largely absent from the high-performance\ncomputing (HPC) landscape, particularly on platforms such as those featured on\nthe Top500 or Green500 lists. Many disciplines already participate in HPC,\nmostly centered around simulation science, although data-focused efforts under\nthe artificial intelligence (AI) label are gaining popularity. Bridging this\ngap requires both community adaptation and technical innovation to align\nstatistical methods with modern HPC technologies. We can accelerate progress in\nfast and scalable statistical applications by building strong connections\nbetween the SC and HPC communities. We present a brief history of SC, a vision\nfor how its strengths can contribute to statistical science in the HPC\nenvironment (such as HPSC), the challenges that remain, and the opportunities\ncurrently available, culminating in a possible roadmap toward a thriving HPSC\ncommunity."}
{"id": "2508.04265", "categories": ["cs.DC", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.04265", "abs": "https://arxiv.org/abs/2508.04265", "authors": ["Borui Li", "Li Yan", "Jianmin Liu"], "title": "SelectiveShield: Lightweight Hybrid Defense Against Gradient Leakage in Federated Learning", "comment": "19 pages, 7 figures", "summary": "Federated Learning (FL) enables collaborative model training on decentralized\ndata but remains vulnerable to gradient leakage attacks that can reconstruct\nsensitive user information. Existing defense mechanisms, such as differential\nprivacy (DP) and homomorphic encryption (HE), often introduce a trade-off\nbetween privacy, model utility, and system overhead, a challenge that is\nexacerbated in heterogeneous environments with non-IID data and varying client\ncapabilities. To address these limitations, we propose SelectiveShield, a\nlightweight hybrid defense framework that adaptively integrates selective\nhomomorphic encryption and differential privacy. SelectiveShield leverages\nFisher information to quantify parameter sensitivity, allowing clients to\nidentify critical parameters locally. Through a collaborative negotiation\nprotocol, clients agree on a shared set of the most sensitive parameters for\nprotection via homomorphic encryption. Parameters that are uniquely important\nto individual clients are retained locally, fostering personalization, while\nnon-critical parameters are protected with adaptive differential privacy noise.\nExtensive experiments demonstrate that SelectiveShield maintains strong model\nutility while significantly mitigating gradient leakage risks, offering a\npractical and scalable defense mechanism for real-world federated learning\ndeployments."}
{"id": "2508.04271", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.04271", "abs": "https://arxiv.org/abs/2508.04271", "authors": ["JinYi Yoon", "JiHo Lee", "Ting He", "Nakjung Choi", "Bo Ji"], "title": "S2M3: Split-and-Share Multi-Modal Models for Distributed Multi-Task Inference on the Edge", "comment": "Accepted at IEEE International Conference on Distributed Computing\n  Systems (ICDCS 2025)", "summary": "With the advancement of Artificial Intelligence (AI) towards multiple\nmodalities (language, vision, speech, etc.), multi-modal models have\nincreasingly been used across various applications (e.g., visual question\nanswering or image generation/captioning). Despite the success of AI as a\nservice for multi-modal applications, it relies heavily on clouds, which are\nconstrained by bandwidth, latency, privacy concerns, and unavailability under\nnetwork or server failures. While on-device AI becomes popular, supporting\nmultiple tasks on edge devices imposes significant resource challenges. To\naddress this, we introduce S2M3, a split-and-share multi-modal architecture for\nmulti-task inference on edge devices. Inspired by the general-purpose nature of\nmulti-modal models, which are composed of multiple modules (encoder, decoder,\nclassifier, etc.), we propose to split multi-modal models at functional-level\nmodules; and then share common modules to reuse them across tasks, thereby\nreducing resource usage. To address cross-model dependency arising from module\nsharing, we propose a greedy module-level placement with per-request parallel\nrouting by prioritizing compute-intensive modules. Through experiments on a\ntestbed consisting of 14 multi-modal models across 5 tasks and 10 benchmarks,\nwe demonstrate that S2M3 can reduce memory usage by up to 50% and 62% in\nsingle-task and multi-task settings, respectively, without sacrificing\naccuracy. Furthermore, S2M3 achieves optimal placement in 89 out of 95\ninstances (93.7%) while reducing inference latency by up to 56.9% on\nresource-constrained devices, compared to cloud AI."}
{"id": "2508.04284", "categories": ["cs.DC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.04284", "abs": "https://arxiv.org/abs/2508.04284", "authors": ["Julius Irion", "Philipp Wiesner", "Jonathan Bader", "Odej Kao"], "title": "Optimizing Microgrid Composition for Sustainable Data Centers", "comment": null, "summary": "As computing energy demand continues to grow and electrical grid\ninfrastructure struggles to keep pace, an increasing number of data centers are\nbeing planned with colocated microgrids that integrate on-site renewable\ngeneration and energy storage. However, while existing research has examined\nthe tradeoffs between operational and embodied carbon emissions in the context\nof renewable energy certificates, there is a lack of tools to assess how the\nsizing and composition of microgrid components affects long-term sustainability\nand power reliability.\n  In this paper, we present a novel optimization framework that extends the\ncomputing and energy system co-simulator Vessim with detailed renewable energy\ngeneration models from the National Renewable Energy Laboratory's (NREL) System\nAdvisor Model (SAM). Our framework simulates the interaction between computing\nworkloads, on-site renewable production, and energy storage, capturing both\noperational and embodied emissions. We use a multi-horizon black-box\noptimization to explore efficient microgrid compositions and enable operators\nto make more informed decisions when planning energy systems for data centers."}
{"id": "2508.04334", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.04334", "abs": "https://arxiv.org/abs/2508.04334", "authors": ["Noor Islam S. Mohammad"], "title": "Data Scheduling Algorithm for Scalable and Efficient IoT Sensing in Cloud Computing", "comment": null, "summary": "The rapid growth of Internet of Things (IoT) devices produces massive,\nheterogeneous data streams, demanding scalable and efficient scheduling in\ncloud environments to meet latency, energy, and Quality-of-Service (QoS)\nrequirements. Existing scheduling methods often lack adaptability to dynamic\nworkloads and network variability inherent in IoT-cloud systems. This paper\npresents a novel hybrid scheduling algorithm combining deep Reinforcement\nLearning (RL) and Ant Colony Optimization (ACO) to address these challenges.\nThe deep RL agent utilizes a model-free policy-gradient approach to learn\nadaptive task allocation policies responsive to real-time workload fluctuations\nand network states. Simultaneously, the ACO metaheuristic conducts a global\ncombinatorial search to optimize resource distribution, mitigate congestion,\nand balance load across distributed cloud nodes. Extensive experiments on\nlarge-scale synthetic IoT datasets, reflecting diverse workloads and QoS\nconstraints, demonstrate that the proposed method achieves up to 18.4%\nreduction in average response time, 12.7% improvement in resource utilization,\nand 9.3% decrease in energy consumption compared to leading heuristics and\nRL-only baselines. Moreover, the algorithm ensures strict Service Level\nAgreement (SLA) compliance through deadline-aware scheduling and dynamic\nprioritization. The results confirm the effectiveness of integrating model-free\nRL with swarm intelligence for scalable, energy-efficient IoT data scheduling,\noffering a promising approach for next-generation IoT-cloud platforms."}
{"id": "2508.04596", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.04596", "abs": "https://arxiv.org/abs/2508.04596", "authors": ["Chuan-Chi Lai", "Yan-Lin Chen", "Bo-Xin Liu", "Chuan-Ming Liu"], "title": "Edge-assisted Parallel Uncertain Skyline Processing for Low-latency IoE Analysis", "comment": "18 pages, 15 figures, to appear in IEEE Internet of Things Journal", "summary": "Due to the Internet of Everything (IoE), data generated in our life become\nlarger. As a result, we need more effort to analyze the data and extract\nvaluable information. In the cloud computing environment, all data analysis is\ndone in the cloud, and the client only needs less computing power to handle\nsome simple tasks. However, with the rapid increase in data volume, sending all\ndata to the cloud via the Internet has become more expensive. The required\ncloud computing resources have also become larger. To solve this problem, edge\ncomputing is proposed. Edge is granted with more computation power to process\ndata before sending it to the cloud. Therefore, the data transmitted over the\nInternet and the computing resources required by the cloud can be effectively\nreduced. In this work, we proposed an Edge-assisted Parallel Uncertain Skyline\n(EPUS) algorithm for emerging low-latency IoE analytic applications. We use the\nconcept of skyline candidate set to prune data that are less likely to become\nthe skyline data on the parallel edge computing nodes. With the candidate\nskyline set, each edge computing node only sends the information required to\nthe server for updating the global skyline, which reduces the amount of data\nthat transfer over the internet. According to the simulation results, the\nproposed method is better than two comparative methods, which reduces the\nlatency of processing two-dimensional data by more than 50%. For\nhigh-dimensional data, the proposed EPUS method also outperforms the other\nexisting methods."}
