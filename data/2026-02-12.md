<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [KORAL: Knowledge Graph Guided LLM Reasoning for SSD Operational Analysis](https://arxiv.org/abs/2602.10246)
*Mayur Akewar,Sandeep Madireddy,Dongsheng Luo,Janki Bhimani*

Main category: cs.DC

TL;DR: KORAL是一个结合大型语言模型和知识图谱的推理框架，用于SSD性能与可靠性诊断，无需大量数据集和专家输入即可提供可解释的分析。


<details>
  <summary>Details</summary>
Motivation: SSD性能诊断困难，数据分散且时间不连续，现有方法需要大量数据集和专家输入，但只能提供有限洞察。性能下降不仅来自工作负载变化和架构演进，还受温度、湿度、振动等环境因素影响。

Method: KORAL框架整合LLM与结构化知识图谱：从碎片化遥测数据生成数据知识图谱，集成文献知识图谱（包含文献、报告、追踪数据），将非结构化源转为可查询图，引导LLM提供基于证据、可解释的分析。

Result: 使用真实生产追踪数据评估显示，KORAL提供专家级诊断和建议，支持基于证据的解释，提高推理透明度，指导操作决策，减少人工工作量，提供可操作的改进服务质量的见解。

Conclusion: 这是首个结合LLM和KG进行全频谱SSD推理（描述性、预测性、规范性、假设分析）的端到端系统，发布的SSD特定知识图谱将推动基于知识的存储系统分析的可重复研究。

Abstract: Solid State Drives (SSDs) are critical to datacenters, consumer platforms, and mission-critical systems. Yet diagnosing their performance and reliability is difficult because data are fragmented and time-disjoint, and existing methods demand large datasets and expert input while offering only limited insights. Degradation arises not only from shifting workloads and evolving architectures but also from environmental factors such as temperature, humidity, and vibration. We present KORAL, a knowledge driven reasoning framework that integrates Large Language Models (LLMs) with a structured Knowledge Graph (KG) to generate insights into SSD operations. Unlike traditional approaches that require extensive expert input and large datasets, KORAL generates a Data KG from fragmented telemetry and integrates a Literature KG that already organizes knowledge from literature, reports, and traces. This turns unstructured sources into a queryable graph and telemetry into structured knowledge, and both the Graphs guide the LLM to deliver evidence-based, explainable analysis aligned with the domain vocabulary and constraints. Evaluation using real production traces shows that the KORAL delivers expert-level diagnosis and recommendations, supported by grounded explanations that improve reasoning transparency, guide operator decisions, reduce manual effort, and provide actionable insights to improve service quality. To our knowledge, this is the first end-to-end system that combines LLMs and KGs for full-spectrum SSD reasoning including Descriptive, Predictive, Prescriptive, and What-if analysis. We release the generated SSD-specific KG to advance reproducible research in knowledge-based storage system analysis. GitHub Repository: https://github.com/Damrl-lab/KORAL

</details>


### [2] [Execution-Centric Characterization of FP8 Matrix Cores, Asynchronous Execution, and Structured Sparsity on AMD MI300A](https://arxiv.org/abs/2602.10262)
*Aaron Jarmusch,Connor Vitz,Sunita Chandrasekaran*

Main category: cs.DC

TL;DR: 本文对AMD MI300A APU的FP8矩阵执行、异步计算引擎并发和结构化稀疏性进行了执行中心化表征，通过微基准测试量化了占用阈值、公平性、并发吞吐量权衡和上下文相关的稀疏性收益，为MI300A类统一节点的调度决策提供实用指导。


<details>
  <summary>Details</summary>
Motivation: AMD MI300A APU集成了CDNA3 GPU、高带宽内存和先进加速器特性（FP8矩阵核心、异步计算引擎、2:4结构化稀疏性），这些特性被现代HPC和HPC-AI工作负载日益依赖，但其执行特性和系统级影响尚未被充分理解。

Method: 使用针对性的微基准测试对MI300A进行执行中心化表征，量化FP8矩阵执行、ACE并发和结构化稀疏性的性能特性，包括占用阈值、公平性、并发执行下的吞吐量权衡以及上下文相关的稀疏性收益。

Result: 通过代表性案例研究（transformer风格、并发和混合精度内核）展示了这些效应如何转化为应用级性能和可预测性，为占用感知调度、并发决策和稀疏性启用提供实用指导。

Conclusion: 研究结果为MI300A类统一节点上的占用感知调度、并发决策和稀疏性启用提供了实用指导，有助于优化现代HPC和HPC-AI工作负载的性能和可预测性。

Abstract: The AMD MI300A APU integrates CDNA3 GPUs with high-bandwidth memory and advanced accelerator features: FP8 matrix cores, asynchronous compute engines (ACE), and 2:4 structured sparsity. These capabilities are increasingly relied upon by modern HPC and HPC-AI workloads, yet their execution characteristics and system-level implications remain insufficiently understood. In this paper, we present an execution-centric characterization of FP8 matrix execution, ACE concurrency, and structured sparsity on MI300A using targeted microbenchmarks. We quantify occupancy thresholds, fairness, throughput trade-offs under concurrent execution, and context-dependent sparsity benefits. We evaluate representative case studies - transformer-style, concurrent, and mixed-precision kernels - to show how these effects translate into application-level performance and predictability. Our results provide practical guidance for occupancy-aware scheduling, concurrency decisions, and sparsity enablement on MI300A-class unified nodes.

</details>


### [3] [Flash-SD-KDE: Accelerating SD-KDE with Tensor Cores](https://arxiv.org/abs/2602.10378)
*Elliot L. Epstein,Rajat Vadiraj Dwaraknath,John Winnicki*

Main category: cs.DC

TL;DR: Flash-SD-KDE通过重新组织计算以利用Tensor Core，将得分去偏核密度估计的速度提升了47倍，使其能够处理百万级样本的高维数据。


<details>
  <summary>Details</summary>
Motivation: SD-KDE虽然比传统KDE有更好的渐近收敛率，但由于使用经验得分导致实际计算速度很慢，限制了其在大规模高维数据上的应用。

Method: 重新组织SD-KDE的计算流程，暴露矩阵乘法结构，从而利用GPU的Tensor Core进行加速。

Result: 在32k样本16维问题上，比GPU基线快47倍，比scikit-learn快3300倍；在1M样本16维任务上，131k个查询仅需2.3秒。

Conclusion: Flash-SD-KDE使得分去偏密度估计在以前不可行的规模上变得实用，显著提升了计算效率。

Abstract: Score-debiased kernel density estimation (SD-KDE) achieves improved asymptotic convergence rates over classical KDE, but its use of an empirical score has made it significantly slower in practice. We show that by re-ordering the SD-KDE computation to expose matrix-multiplication structure, Tensor Cores can be used to accelerate the GPU implementation. On a 32k-sample 16-dimensional problem, our approach runs up to $47\times$ faster than a strong SD-KDE GPU baseline and $3{,}300\times$ faster than scikit-learn's KDE. On a larger 1M-sample 16-dimensional task evaluated on 131k queries, Flash-SD-KDE completes in $2.3$ s on a single GPU, making score-debiased density estimation practical at previously infeasible scales.

</details>


### [4] [Computing Least Fixed Points with Overwrite Semantics in Parallel and Distributed Systems](https://arxiv.org/abs/2602.10486)
*Vijay K. Garg,Rohan Garg*

Main category: cs.DC

TL;DR: 提出在并行和分布式设置中计算多个单调膨胀函数最小不动点的方法，证明了三种渐进放松同步条件下的收敛定理，为基于覆盖的并行更新提供了首个精确的最小不动点收敛保证。


<details>
  <summary>Details</summary>
Motivation: 经典Knaster-Tarski定理处理单函数顺序迭代，而现代计算系统需要并行执行，具有覆盖语义、非原子更新和过时读取等特点。需要为并行和分布式环境中的不动点计算提供理论保证。

Method: 使用坐标覆盖方法（而非基于连接的合并），在三种渐进放松的同步条件下：1）交错语义与公平调度；2）仅当值变化时更新的并行执行；3）具有有限过时性和i-局部性的分布式执行。

Result: 证明了三种收敛定理，为基于覆盖的并行更新提供了首个精确的最小不动点收敛保证，无需连接操作或收缩假设。应用于传递闭包、稳定婚姻、最短路径和带补贴的公平分配等问题的并行分布式算法。

Conclusion: 该工作为并行和分布式环境中的不动点计算提供了新的理论框架，通过坐标覆盖方法和渐进放松的同步条件，实现了无需连接操作或收缩假设的精确收敛保证，扩展了经典不动点理论的应用范围。

Abstract: We present methods to compute least fixed points of multiple monotone inflationary functions in parallel and distributed settings. While the classic Knaster-Tarski theorem addresses a single function with sequential iteration, modern computing systems require parallel execution with overwrite semantics, non-atomic updates, and stale reads. We prove three convergence theorems under progressively relaxed synchronization: (1) Interleaving semantics with fair scheduling, (2) Parallel execution with update-only-on-change semantics (processes write only on those coordinates whose values change), and (3) Distributed execution with bounded staleness (updates propagate within $T$ rounds) and $i$-locality (each process modifies only its own component).
  Our approach differs from prior work in fundamental ways: Cousot-Cousot's chaotic iteration uses join-based merges that preserve information. Instead, we use coordinate-wise overwriting. Bertsekas's asynchronous methods assume contractions. We use coordinate-wise overwriting with structural constraints (locality, bounded staleness) instead. Applications include parallel and distributed algorithms for the transitive closure, stable marriage, shortest paths, and fair division with subsidy problems. Our results provide the first exact least-fixed-point convergence guarantees for overwrite-based parallel updates without join operations or contraction assumptions.

</details>


### [5] [BOute: Cost-Efficient LLM Serving with Heterogeneous LLMs and GPUs via Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2602.10729)
*Youhe Jiang,Fangcheng Fu,Eiko Yoneki*

Main category: cs.DC

TL;DR: BOute是一个质量感知调度系统，通过联合优化异构模型路由和GPU部署策略，实现成本高效的LLM服务。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型部署快速增长，需要成本高效的服务系统。现有方法分别从算法角度（异构查询路由）和系统角度（异构GPU部署）优化，但缺乏两者的协同设计，难以实现整体最优。

Method: 提出BOute系统，采用多目标贝叶斯优化框架，联合优化路由策略和模型部署配置，在保证响应质量的同时最大化系统成本效率。

Result: 在相同成本预算和质量要求下，BOute比现有最佳LLM服务系统性能提升最高157%，平均59%；或在保持相同性能目标下，服务成本降低15%-61%（平均38%）。

Conclusion: BOute通过算法-系统协同设计，有效实现了成本高效的LLM服务，验证了联合优化异构模型能力和GPU资源的价值。

Abstract: The rapid growth of large language model (LLM) deployments has made cost-efficient serving systems essential. Recent efforts to enhance system cost-efficiency adopt two main perspectives: (i) An algorithmic perspective that exploits heterogeneous model capabilities to route simpler queries to lower-cost models and complex queries to higher-cost models (i.e., heterogeneous query routing); and (ii) a systems perspective that utilizes heterogeneous GPU resources as cost-effective alternatives to homogeneous high-end GPUs (i.e., heterogeneous model deployment). However, algorithm-system co-design for cost-efficient LLM serving necessitates sophisticated management: (i) Determining optimal query routing strategies under latency and quality requirements, (ii) configuring model deployment across heterogeneous GPUs with appropriate resource allocation and parallelism strategies, and (iii) co-optimizing routing and deployment decisions to maximize overall system performance. To address these challenges, we present BOute, a quality-aware scheduling system that jointly exploits heterogeneous model and GPU capabilities for cost-efficient LLM serving. BOute employs a multi-objective Bayesian optimization (MOBO) framework to co-optimize the routing strategy and model deployment, thereby maximizing the cost-efficiency of the serving system while guaranteeing response quality. Evaluation results demonstrate that BOute outperforms state-of-the-art LLM serving systems by up to 157% and 59% on average under identical cost budgets and quality requirements, or reducing serving costs by 15%-61% (38% on average) while maintaining the same performance targets, validating its effectiveness in achieving cost-efficient LLM serving.

</details>


### [6] [Fine-Tuning GPT-5 for GPU Kernel Generation](https://arxiv.org/abs/2602.11000)
*Ali Tehrani,Yahya Emara,Essam Wissam,Wojciech Paluch,Waleed Atallah,Łukasz Dudziak,Mohamed S. Abdelfattah*

Main category: cs.DC

TL;DR: 使用强化学习微调GPT-5生成Triton GPU内核代码，显著提升正确率和性能，超越传统监督学习方法


<details>
  <summary>Details</summary>
Motivation: GPU内核开发复杂且需要专业知识，传统监督学习因高质量标注数据稀缺、编译器偏见和硬件泛化问题而受限，需要更有效的方法提升LLM在GPU代码生成领域的能力

Method: 开发Makora强化学习环境和工具，对GPT-5进行强化学习微调，专注于Triton代码生成，采用数据高效的自适应训练方法

Result: 单次尝试正确率从43.7%提升至77.0%，超越TorchInductor的问题比例从14.8%增至21.8%；完整编码代理可解决97.4%的问题，在72.9%问题上超越PyTorch编译器，几何平均加速2.12倍

Conclusion: 强化学习后训练能有效解锁LLM在专业领域的能力，为AI辅助加速器编程开辟新途径，特别是在传统监督学习受数据限制的领域

Abstract: Developing efficient GPU kernels is essential for scaling modern AI systems, yet it remains a complex task due to intricate hardware architectures and the need for specialized optimization expertise. Although Large Language Models (LLMs) demonstrate strong capabilities in general sequential code generation, they face significant challenges in GPU code generation because of the scarcity of high-quality labeled training data, compiler biases when generating synthetic solutions, and limited generalization across hardware generations. This precludes supervised fine-tuning (SFT) as a scalable methodology for improving current LLMs. In contrast, reinforcement learning (RL) offers a data-efficient and adaptive alternative but requires access to relevant tools, careful selection of training problems, and a robust evaluation environment. We present Makora's environment and tools for reinforcement learning finetuning of frontier models and report our results from fine-tuning GPT-5 for Triton code generation. In the single-attempt setting, our fine-tuned model improves kernel correctness from 43.7% to 77.0% (+33.3 percentage points) and increases the fraction of problems outperforming TorchInductor from 14.8% to 21.8% (+7 percentage points) compared to baseline GPT-5, while exceeding prior state-of-the-art models on KernelBench. When integrated into a full coding agent, it is able to solve up to 97.4% of problems in an expanded KernelBench suite, outperforming the PyTorch TorchInductor compiler on 72.9% of problems with a geometric mean speedup of 2.12x. Our work demonstrates that targeted post-training with reinforcement learning can unlock LLM capabilities in highly specialized technical domains where traditional supervised learning is limited by data availability, opening new pathways for AI-assisted accelerator programming.

</details>


### [7] [Min-Sum Uniform Coverage Problem by Autonomous Mobile Robots](https://arxiv.org/abs/2602.11125)
*Animesh Maiti,Abhinav Chakraborty,Bibhuti Das,Subhash Bhagat,Krishnendu Mukhopadhyaya*

Main category: cs.DC

TL;DR: 研究机器人在线段和圆上的最小总移动距离均匀覆盖问题，提出确定性分布式算法实现最优成本覆盖


<details>
  <summary>Details</summary>
Motivation: 研究自主、匿名、无记忆、无通信的机器人在异步调度下如何协调运动，以最小总移动距离实现均匀覆盖配置

Method: 提出确定性分布式算法：线段设置中实现最小总移动距离的均匀覆盖；圆设置中分析不可解配置并提供可解配置的最优算法

Result: 线段设置中实现最小总移动距离的均匀覆盖；圆设置中完整刻画了确定性可解和不可解的初始配置，并为可解配置提供最优算法

Conclusion: 该研究完整刻画了无记忆机器人在最小总移动距离均匀覆盖问题中的确定性可解性，并在可解情况下实现了最优成本覆盖

Abstract: We study the \textit{min-sum uniform coverage} problem for a swarm of $n$ mobile robots on a given finite line segment and on a circle having finite positive radius, where the circle is given as an input. The robots must coordinate their movements to reach a uniformly spaced configuration that minimizes the total distance traveled by all robots. The robots are autonomous, anonymous, identical, and homogeneous, and operate under the \textit{Look-Compute-Move} (LCM) model with \textit{non-rigid} motion controlled by a fair asynchronous scheduler. They are oblivious and silent, possessing neither persistent memory nor a means of explicit communication. In the \textbf{line-segment setting}, the \textit{min-sum uniform coverage} problem requires placing the robots at uniformly spaced points along the segment so as to minimize the total distance traveled by all robots. In the \textbf{circle setting} for this problem, the robots have to arrange themselves uniformly around the given circle to form a regular $n$-gon. There is no fixed orientation or designated starting vertex, and the goal is to minimize the total distance traveled by all the robots. We present a deterministic distributed algorithm that achieves uniform coverage in the line-segment setting with minimum total movement cost. For the circle setting, we characterize all initial configurations for which the \textit{min-sum uniform coverage} problem is deterministically unsolvable under the considered robot model. For all the other remaining configurations, we provide a deterministic distributed algorithm that achieves uniform coverage while minimizing the total distance traveled. These results characterize the deterministic solvability of min-sum coverage for oblivious robots and achieve optimal cost whenever solvable.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [ACE-RTL: When Agentic Context Evolution Meets RTL-Specialized LLMs](https://arxiv.org/abs/2602.10218)
*Chenhui Deng,Zhongzhi Yu,Guan-Ting Liu,Nathaniel Pinckney,Haoxing Ren*

Main category: cs.AR

TL;DR: ACE-RTL结合了领域专用RTL模型和前沿通用LLM的优势，通过生成器、反射器和协调器三个组件迭代优化RTL代码，在CVDP基准测试中显著提升了通过率。


<details>
  <summary>Details</summary>
Motivation: 当前硬件设计自动化中，基于LLM的方法存在两条独立路径：训练领域专用RTL模型和开发基于模拟反馈的代理系统。这两种方法各有优缺点，需要统一框架来结合两者的优势。

Method: 提出ACE-RTL框架，通过Agentic Context Evolution (ACE)整合RTL专用LLM（基于170万RTL样本训练）和前沿推理LLM。包含三个协同组件：生成器、反射器、协调器，迭代优化RTL代码。还引入了并行扩展策略减少迭代次数。

Result: 在Comprehensive Verilog Design Problems (CVDP)基准测试中，ACE-RTL相比14个竞争基线实现了高达44.87%的通过率提升，平均仅需4次迭代。

Conclusion: ACE-RTL成功统一了硬件设计自动化的两种主流方法，通过协同组件和并行策略实现了显著的性能提升，为LLM在硬件设计中的应用提供了有效框架。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest in applying them to hardware design automation, particularly for accurate RTL code generation. Prior efforts follow two largely independent paths: (i) training domain-adapted RTL models to internalize hardware semantics, (ii) developing agentic systems that leverage frontier generic LLMs guided by simulation feedback. However, these two paths exhibit complementary strengths and weaknesses. In this work, we present ACE-RTL that unifies both directions through Agentic Context Evolution (ACE). ACE-RTL integrates an RTL-specialized LLM, trained on a large-scale dataset of 1.7 million RTL samples, with a frontier reasoning LLM through three synergistic components: the generator, reflector, and coordinator. These components iteratively refine RTL code toward functional correctness. We further introduce a parallel scaling strategy that significantly reduces the number of iterations required to reach correct solutions. On the Comprehensive Verilog Design Problems (CVDP) benchmark, ACE-RTL achieves up to a 44.87% pass rate improvement over 14 competitive baselines while requiring only four iterations on average.

</details>


### [9] [Area-Efficient In-Memory Computing for Mixture-of-Experts via Multiplexing and Caching](https://arxiv.org/abs/2602.10254)
*Hanyuan Gao,Xiaoxuan Yang*

Main category: cs.AR

TL;DR: 提出面向MoE变压器的面积高效存内计算架构，通过交叉阵列复用、专家分组调度和门输出缓存技术，提升面积效率和生成性能。


<details>
  <summary>Details</summary>
Motivation: MoE层通过激活部分专家权重提升模型性能，适合存内计算架构部署，但存内计算芯片存在面积开销大的问题，特别是外围电路占用大量面积。

Method: 1) 交叉阵列级复用策略：利用MoE稀疏性，多个交叉阵列共享外围电路；2) 专家分组和组间调度：缓解共享导致的负载不平衡和争用开销；3) 门输出缓存：存储必要结果，避免专家选择路由器在生成时访问所有隐藏状态带来的额外计算开销。

Result: MoE部分面积效率相比SOTA架构提升2.2倍；生成8个token时，缓存使性能和能效分别提升4.2倍和10.1倍；总性能密度达到15.6 GOPS/W/mm²。

Conclusion: 提出的面积高效存内计算架构通过交叉阵列复用、专家分组调度和门输出缓存，有效解决了MoE在存内计算部署中的面积开销和性能瓶颈问题，显著提升了面积效率和生成效率。

Abstract: Mixture-of-Experts (MoE) layers activate a subset of model weights, dubbed experts, to improve model performance. MoE is particularly promising for deployment on process-in-memory (PIM) architectures, because PIM can naturally fit experts separately and provide great benefits for energy efficiency. However, PIM chips often suffer from large area overhead, especially in the peripheral circuits. In this paper, we propose an area-efficient in-memory computing architecture for MoE transformers. First, to reduce area, we propose a crossbar-level multiplexing strategy that exploits MoE sparsity: experts are deployed on crossbars and multiple crossbars share the same peripheral circuits. Second, we propose expert grouping and group-wise scheduling methods to alleviate the load imbalance and contention overhead caused by sharing. In addition, to address the problem that the expert choice router requires access to all hidden states during generation, we propose a gate-output (GO)cache to store necessary results and bypass expensive additional computation. Experiments show that our approaches improve the area efficiency of the MoE part by up to 2.2x compared to a SOTA architecture. During generation, the cache improves performance and energy efficiency by 4.2x and 10.1x, respectively, compared to the baseline when generating 8 tokens. The total performance density achieves 15.6 GOPS/W/mm2. The code is open source at https://github.com/superstarghy/MoEwithPIM.

</details>


### [10] [DRAMPyML: A Formal Description of DRAM Protocols with Timed Petri Nets](https://arxiv.org/abs/2602.10654)
*Derek Christ,Thomas Zimmermann,Philippe Barbie,Dmitri Saberi,Yao Yin,Matthias Jung*

Main category: cs.AR

TL;DR: 该论文提出了一种基于时间Petri网和Python的DRAM协议建模方法，以解决JEDEC标准中简化状态机无法反映内存bank并行操作的问题。


<details>
  <summary>Details</summary>
Motivation: JEDEC定义的DRAM标准协议日益复杂，时序图和命令表难以理解，现有简化状态机无法反映内存bank的并行操作，需要更精确的表达模型。

Method: 采用时间Petri网和Python相结合的建模方法，构建可直接执行的DRAM协议模型，能够准确反映协议行为。

Result: 该模型提供了更准确的DRAM协议表示，易于理解且可直接执行，能够评估性能指标并验证控制器RTL模型、DRAM逻辑和内存模拟器。

Conclusion: 基于时间Petri网和Python的建模方法能够有效解决JEDEC DRAM标准协议复杂性问题，提供更精确、可执行的协议表示，有助于协议理解和系统验证。

Abstract: The JEDEC committee defines various domain-specific DRAM standards. These standards feature increasingly complex and evolving protocol specifications, which are detailed in timing diagrams and command tables. Understanding these protocols is becoming progressively challenging as new features and complex device hierarchies are difficult to comprehend without an expressive model. While each JEDEC standard features a simplified state machine, this state machine fails to reflect the parallel operation of memory banks.
  In this paper, we present an evolved modeling approach based on timed Petri nets and Python. This model provides a more accurate representation of DRAM protocols, making them easier to understand and directly executable, which enables the evaluation of interesting metrics and the verification of controller RTL models, DRAM logic and memory simulators.

</details>


### [11] [Fault Tolerant Design of IGZO-based Binary Search ADCs](https://arxiv.org/abs/2602.10790)
*Paula Carolina Lozano Duarte,Sule Ozev,Mehdi Tahoori*

Main category: cs.AR

TL;DR: 提出分层故障注入框架分析IGZO等单极性技术中二进制搜索ADC的缺陷敏感性，通过选择性冗余策略将故障覆盖率从60%提升至92%，面积开销仅4.2%


<details>
  <summary>Details</summary>
Motivation: 柔性电子技术（如IGZO）在可穿戴传感和健康监测等新兴应用中前景广阔，但单极性技术相比成熟CMOS技术具有更高的缺陷密度和工艺变化。ADC作为关键传感器接口，其制造缺陷敏感性尚未得到充分理解。

Method: 提出分层故障注入框架，结合晶体管级缺陷表征和系统级故障传播分析，高效探索转换层次结构中的单故障和多故障场景，识别关键故障敏感电路组件。

Result: 通过选择性冗余策略，单故障注入下的故障覆盖率从60%提升至92%，多故障注入下从34%提升至77.6%，仅带来4.2%的面积开销和6%的功耗增加。

Conclusion: 该框架有效识别了二进制搜索ADC中的关键故障敏感组件，并通过选择性冗余实现了高效的缺陷容错设计。虽然基于IGZO-TFT验证，但方法适用于所有新兴单极性技术。

Abstract: Thin-film technologies such as Indium Gallium Zinc Oxide (IGZO) enable Flexible Electronics (FE) for emerging applications in wearable sensing, personal health monitoring, and large-area systems. Analog-to-digital converters (ADCs) serve as critical sensor interfaces in these systems. Yet, their vulnerability to manufacturing defects remains poorly understood despite unipolar technologies' inherently high defect densities and process variations compared to mature CMOS technologies. We present a hierarchical fault injection framework to characterize defect sensitivity in Binary Search ADCs implemented in n-type only technologies. Our methodology combines transistor-level defect characterization with system-level fault propagation analysis, enabling efficient exploration of both single and multiple fault scenarios across the conversion hierarchy. The framework identifies critical fault-sensitive circuit components and enables selective redundancy strategies targeting only the most sensitive components. The resulting defect-tolerant designs improve fault coverage from 60% to 92% under single-fault injections and from 34% to 77.6% under multi-fault injection, while incurring only 4.2% area overhead and 6% power increase. While validated on IGZO-TFTs, the methodology applies to all emerging unipolar technologies.

</details>


### [12] [From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design](https://arxiv.org/abs/2602.11016)
*Jinxin Yu,Yudong Pan,Mengdi Wang,Huawei Li,Yinhe Han,Xiaowei Li,Ying Wang*

Main category: cs.AR

TL;DR: 3D-Flow：一种混合键合3D堆叠空间加速器，通过垂直分区PE层间的寄存器到寄存器通信，减少Transformer模型中SRAM访问的能耗瓶颈，相比现有2D/3D设计实现46-93%能耗降低和1.4-7.6倍加速。


<details>
  <summary>Details</summary>
Motivation: Transformer模型因二次注意力复杂度和模型规模增长加剧内存瓶颈。现有加速器（如Groq、Cerebras）通过大容量片上缓存减少片外流量，FlashAttention等算法创新避免生成大注意力矩阵。但随着片外流量减少，测量显示长序列工作负载中片上SRAM访问能耗占比超60%，成为新瓶颈。

Method: 提出3D-Flow：混合键合3D堆叠空间加速器，支持垂直分区PE层间的寄存器到寄存器通信。利用亚10微米垂直TSV维持周期级算子流水线，相比2D多阵列架构受限于NoC路由器间传输，具有更低开销。在此基础上设计3D-FlashAttention：细粒度调度方法，平衡各层延迟，形成无气泡垂直数据流，避免片上SRAM往返访问。

Result: 在Transformer工作负载（OPT和QWEN模型）评估显示：3D空间加速器相比最先进的2D和3D设计，能耗降低46-93%，速度提升1.4-7.6倍。

Conclusion: 3D-Flow通过3D堆叠架构和垂直数据流设计，有效解决了Transformer加速中片上SRAM访问的能耗瓶颈问题，为长序列工作负载提供了高效的硬件加速方案。

Abstract: Transformer-based models dominate modern AI workloads but exacerbate memory bottlenecks due to their quadratic attention complexity and ever-growing model sizes. Existing accelerators, such as Groq and Cerebras, mitigate off-chip traffic with large on-chip caches, while algorithmic innovations such as FlashAttention fuse operators to avoid materializing large attention matrices. However, as off-chip traffic decreases, our measurements show that on-chip SRAM accesses account for over 60% of energy in long-sequence workloads, making cache access the new bottleneck. We propose 3D-Flow, a hybrid-bonded, 3D-stacked spatial accelerator that enables register-to-register communication across vertically partitioned PE tiers. Unlike 2D multi-array architectures limited by NoC-based router-to-router transfers, 3D-Flow leverages sub-10 um vertical TSVs to sustain cycle-level operator pipelining with minimal overhead. On top of this architecture, we design 3D-FlashAttention, a fine-grained scheduling method that balances latency across tiers, forming a bubble-free vertical dataflow without on-chip SRAM roundtrips. Evaluations on Transformer workloads (OPT and QWEN models) show that our 3D spatial accelerator reduces 46-93% energy consumption and achieves 1.4x-7.6x speedups compared to state-of-the-art 2D and 3D designs.

</details>
