{"id": "2511.16947", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.16947", "abs": "https://arxiv.org/abs/2511.16947", "authors": ["Chenqi Zhao", "Wenfei Wu", "Linhai Song", "Yuchen Xu"], "title": "MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling", "comment": "19 pages", "summary": "Mixture-of-Experts (MoE) has emerged as a promising approach to scale up deep learning models due to its significant reduction in computational resources. However, the dynamic nature of MoE leads to load imbalance among experts, severely impacting training efficiency. While previous research has attempted to address the load balancing challenge, existing solutions either compromise model accuracy or introduce additional system overhead. As a result, they fail to achieve fine-grained load balancing, which is crucial to optimizing training efficiency.\n  We propose MicroEP, a novel parallelization strategy to achieve fine-grained load balancing in MoE systems. MicroEP is capable of achieving optimal load balancing in every micro-batch through efficient token scheduling across GPUs. Furthermore, we propose MicroMoE, an efficient distributed MoE training system with MicroEP's load balancing capabilities. Our experimental results demonstrate that MicroMoE improves the end-to-end training throughput by up to 47.6% compared with the state-of-the-art system, and almost consistently achieves optimal load balance among GPUs.", "AI": {"tldr": "MicroEP\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5e76\u884c\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u5728MoE\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8d1f\u8f7d\u5747\u8861\u6765\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "MoE\u7684\u52a8\u6001\u7279\u6027\u5bfc\u81f4\u4e13\u5bb6\u95f4\u8d1f\u8f7d\u4e0d\u5e73\u8861\uff0c\u4e25\u91cd\u5f71\u54cd\u8bad\u7ec3\u6548\u7387\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u727a\u7272\u6a21\u578b\u7cbe\u5ea6\uff0c\u8981\u4e48\u5f15\u5165\u989d\u5916\u7cfb\u7edf\u5f00\u9500\u3002", "method": "\u63d0\u51faMicroEP\u5e76\u884c\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u8de8GPU\u7684\u9ad8\u6548token\u8c03\u5ea6\u5b9e\u73b0\u6bcf\u4e2a\u5fae\u6279\u6b21\u7684\u6700\u4f18\u8d1f\u8f7d\u5747\u8861\uff0c\u5e76\u6784\u5efa\u4e86MicroMoE\u5206\u5e03\u5f0f\u8bad\u7ec3\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMicroMoE\u76f8\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u5c06\u7aef\u5230\u7aef\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe47.6%\uff0c\u51e0\u4e4e\u59cb\u7ec8\u5b9e\u73b0GPU\u95f4\u6700\u4f18\u8d1f\u8f7d\u5747\u8861\u3002", "conclusion": "MicroEP\u80fd\u591f\u6709\u6548\u89e3\u51b3MoE\u7cfb\u7edf\u4e2d\u7684\u8d1f\u8f7d\u5747\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2511.17119", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.17119", "abs": "https://arxiv.org/abs/2511.17119", "authors": ["Gabriel Job Antunes Grabher", "Fumio Machida", "Thomas Ropars"], "title": "Modeling Anomaly Detection in Cloud Services: Analysis of the Properties that Impact Latency and Resource Consumption", "comment": null, "summary": "Detecting and resolving performance anomalies in Cloud services is crucial for maintaining desired performance objectives. Scaling actions triggered by an anomaly detector help achieve target latency at the cost of extra resource consumption. However, performance anomaly detectors make mistakes. This paper studies which characteristics of performance anomaly detection are important to optimize the trade-off between performance and cost. Using Stochastic Reward Nets, we model a Cloud service monitored by a performance anomaly detector. Using our model, we study the impact of detector characteristics, namely precision, recall and inspection frequency, on the average latency and resource consumption of the monitored service. Our results show that achieving a high precision and a high recall is not always necessary. If detection can be run frequently, a high precision is enough to obtain a good performance-to-cost trade-off, but if the detector is run infrequently, recall becomes the most important.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u6027\u80fd\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u7279\u6027\u5982\u4f55\u4f18\u5316\u4e91\u670d\u52a1\u6027\u80fd\u4e0e\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u53d1\u73b0\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u53ec\u56de\u7387\u5e76\u975e\u603b\u662f\u5fc5\u8981\uff0c\u68c0\u6d4b\u9891\u7387\u4f1a\u5f71\u54cd\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4e91\u670d\u52a1\u4e2d\u6027\u80fd\u5f02\u5e38\u7684\u68c0\u6d4b\u548c\u89e3\u51b3\u5bf9\u7ef4\u6301\u6027\u80fd\u76ee\u6807\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f02\u5e38\u68c0\u6d4b\u5668\u4f1a\u51fa\u9519\uff0c\u9700\u8981\u7814\u7a76\u54ea\u4e9b\u68c0\u6d4b\u7279\u6027\u5bf9\u4f18\u5316\u6027\u80fd\u4e0e\u6210\u672c\u6743\u8861\u6700\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u968f\u673a\u5956\u52b1\u7f51\u5bf9\u7531\u6027\u80fd\u5f02\u5e38\u68c0\u6d4b\u5668\u76d1\u63a7\u7684\u4e91\u670d\u52a1\u8fdb\u884c\u5efa\u6a21\uff0c\u5206\u6790\u68c0\u6d4b\u5668\u7279\u6027\uff08\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c\u68c0\u67e5\u9891\u7387\uff09\u5bf9\u5e73\u5747\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5982\u679c\u68c0\u6d4b\u53ef\u4ee5\u9891\u7e41\u8fd0\u884c\uff0c\u9ad8\u7cbe\u5ea6\u8db3\u4ee5\u83b7\u5f97\u826f\u597d\u7684\u6027\u80fd\u6210\u672c\u6743\u8861\uff1b\u4f46\u5982\u679c\u68c0\u6d4b\u5668\u8fd0\u884c\u4e0d\u9891\u7e41\uff0c\u53ec\u56de\u7387\u53d8\u5f97\u6700\u91cd\u8981\u3002\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u53ec\u56de\u7387\u5e76\u975e\u603b\u662f\u5fc5\u8981\u3002", "conclusion": "\u6027\u80fd\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u4f18\u5316\u5e94\u6839\u636e\u68c0\u6d4b\u9891\u7387\u6765\u6743\u8861\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u7684\u91cd\u8981\u6027\uff0c\u9891\u7e41\u68c0\u6d4b\u65f6\u7cbe\u5ea6\u66f4\u91cd\u8981\uff0c\u4e0d\u9891\u7e41\u68c0\u6d4b\u65f6\u53ec\u56de\u7387\u66f4\u5173\u952e\u3002"}}
{"id": "2511.16831", "categories": ["cs.AR", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.16831", "abs": "https://arxiv.org/abs/2511.16831", "authors": ["Yipeng Wang", "Mengtian Yang", "Chieh-pu Lo", "Jaydeep P. Kulkarni"], "title": "Vorion: A RISC-V GPU with Hardware-Accelerated 3D Gaussian Rendering and Training", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a foundational technique for real-time neural rendering, 3D scene generation, volumetric video (4D) capture. However, its rendering and training impose massive computation, making real-time rendering on edge devices and real-time 4D reconstruction on workstations currently infeasible. Given its fixed-function nature and similarity with traditional rasterization, 3DGS presents a strong case for dedicated hardware in the graphics pipeline of next-generation GPUs. This work, Vorion, presents the first GPGPU prototype with hardware-accelerated 3DGS rendering and training. Vorion features scalable architecture, minimal hardware change to traditional rasterizers, z-tiling to increase parallelism, and Gaussian/pixel-centric hybrid dataflow. We prototype the minimal system (8 SIMT cores, 2 Gaussian rasterizer) using TSMC 16nm FinFET technology, which achieves 19 FPS for rendering. The scaled design with 16 rasterizers achieves 38.6 iterations/s for training.", "AI": {"tldr": "Vorion\u662f\u9996\u4e2a\u652f\u6301\u786c\u4ef6\u52a0\u901f3D\u9ad8\u65af\u6cfc\u6e85\u6e32\u67d3\u548c\u8bad\u7ec3\u7684GPGPU\u539f\u578b\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u67b6\u6784\u3001\u6700\u5c0f\u786c\u4ef6\u6539\u52a8\u3001z-tiling\u548c\u6df7\u5408\u6570\u636e\u6d41\u5b9e\u73b0\u9ad8\u6548\u6027\u80fd\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u5b9e\u65f6\u795e\u7ecf\u6e32\u67d3\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8ba1\u7b97\u9700\u6c42\u5de8\u5927\uff0c\u5bfc\u81f4\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\uff0c\u5728\u5de5\u4f5c\u7ad9\u4e0a\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f64D\u91cd\u5efa\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u7528\u786c\u4ef6\u52a0\u901f\u3002", "method": "\u63d0\u51faVorion\u67b6\u6784\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u8bbe\u8ba1\uff0c\u5bf9\u4f20\u7edf\u5149\u6805\u5316\u5668\u8fdb\u884c\u6700\u5c0f\u786c\u4ef6\u6539\u52a8\uff0c\u5f15\u5165z-tiling\u589e\u52a0\u5e76\u884c\u6027\uff0c\u4f7f\u7528\u9ad8\u65af/\u50cf\u7d20\u4e2d\u5fc3\u6df7\u5408\u6570\u636e\u6d41\uff0c\u5728TSMC 16nm\u5de5\u827a\u4e0a\u5b9e\u73b0\u539f\u578b\u7cfb\u7edf\u3002", "result": "\u6700\u5c0f\u7cfb\u7edf\uff088\u4e2aSIMT\u6838\u5fc3\uff0c2\u4e2a\u9ad8\u65af\u5149\u6805\u5316\u5668\uff09\u5b9e\u73b019 FPS\u6e32\u67d3\u6027\u80fd\uff1b\u6269\u5c55\u8bbe\u8ba1\uff0816\u4e2a\u5149\u6805\u5316\u5668\uff09\u5b9e\u73b038.6\u6b21\u8fed\u4ee3/\u79d2\u7684\u8bad\u7ec3\u901f\u5ea6\u3002", "conclusion": "Vorion\u8bc1\u660e\u4e86\u786c\u4ef6\u52a0\u901f3D\u9ad8\u65af\u6cfc\u6e85\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4e0b\u4e00\u4ee3GPU\u56fe\u5f62\u6d41\u6c34\u7ebf\u4e2d\u7684\u4e13\u7528\u786c\u4ef6\u63d0\u4f9b\u4e86\u6709\u529b\u6848\u4f8b\u3002"}}
{"id": "2511.17123", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17123", "abs": "https://arxiv.org/abs/2511.17123", "authors": ["Jiaxun Fang", "Li Zhang", "Shaoyi Huang"], "title": "Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration", "comment": null, "summary": "Systolic array accelerators execute CNNs with energy dominated by the switching activity of multiply accumulate (MAC) units. Although prior work exploits weight dependent MAC power for compression, existing methods often use global activation models, coarse energy proxies, or layer-agnostic policies, which limits their effectiveness on real hardware. We propose an energy aware, layer-wise compression framework that explicitly leverages MAC and layer level energy characteristics. First, we build a layer-aware MAC energy model that combines per-layer activation statistics with an MSB-Hamming distance grouping of 22-bit partial sum transitions, and integrate it with a tile-level systolic mapping to estimate convolution-layer energy. On top of this model, we introduce an energy accuracy co-optimized weight selection algorithm within quantization aware training and an energy-prioritized layer-wise schedule that compresses high energy layers more aggressively under a global accuracy constraint. Experiments on different CNN models demonstrate up to 58.6\\% energy reduction with 2-3\\% accuracy drop, outperforming a state-of-the-art power-aware baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u80fd\u91cf\u611f\u77e5\u7684\u5c42\u95f4\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408MAC\u5355\u5143\u80fd\u91cf\u7279\u6027\u548c\u5c42\u95f4\u80fd\u91cf\u7279\u5f81\uff0c\u5728\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u4e2d\u8fdb\u884c\u80fd\u91cf-\u7cbe\u5ea6\u534f\u540c\u4f18\u5316\u7684\u6743\u91cd\u9009\u62e9\uff0c\u5b9e\u73b0CNN\u6a21\u578b\u5728\u8109\u52a8\u9635\u5217\u52a0\u901f\u5668\u4e0a\u7684\u80fd\u91cf\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u5168\u5c40\u6fc0\u6d3b\u6a21\u578b\u3001\u7c97\u7cd9\u80fd\u91cf\u4ee3\u7406\u6216\u5c42\u65e0\u5173\u7b56\u7565\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u7684\u6709\u6548\u6027\u3002\u9700\u8981\u66f4\u7cbe\u786e\u7684\u5c42\u95f4\u80fd\u91cf\u6a21\u578b\u548c\u538b\u7f29\u7b56\u7565\u6765\u4f18\u5316CNN\u5728\u8109\u52a8\u9635\u5217\u52a0\u901f\u5668\u4e0a\u7684\u80fd\u91cf\u6d88\u8017\u3002", "method": "1) \u6784\u5efa\u5c42\u611f\u77e5MAC\u80fd\u91cf\u6a21\u578b\uff0c\u7ed3\u5408\u6bcf\u5c42\u6fc0\u6d3b\u7edf\u8ba1\u548c22\u4f4d\u90e8\u5206\u548c\u8f6c\u6362\u7684MSB-\u6c49\u660e\u8ddd\u79bb\u5206\u7ec4\uff1b2) \u5728\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u4e2d\u5f15\u5165\u80fd\u91cf-\u7cbe\u5ea6\u534f\u540c\u4f18\u5316\u7684\u6743\u91cd\u9009\u62e9\u7b97\u6cd5\uff1b3) \u63d0\u51fa\u80fd\u91cf\u4f18\u5148\u7684\u5c42\u95f4\u8c03\u5ea6\u7b56\u7565\uff0c\u5728\u5168\u5c40\u7cbe\u5ea6\u7ea6\u675f\u4e0b\u66f4\u79ef\u6781\u5730\u538b\u7f29\u9ad8\u80fd\u8017\u5c42\u3002", "result": "\u5728\u4e0d\u540cCNN\u6a21\u578b\u4e0a\u5b9e\u9a8c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe58.6%\u7684\u80fd\u91cf\u964d\u4f4e\uff0c\u7cbe\u5ea6\u4e0b\u964d\u4ec5\u4e3a2-3%\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u529f\u7387\u611f\u77e5\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7cbe\u786e\u7684\u5c42\u95f4\u80fd\u91cf\u5efa\u6a21\u548c\u80fd\u91cf\u4f18\u5148\u7684\u538b\u7f29\u7b56\u7565\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u8109\u52a8\u9635\u5217\u52a0\u901f\u5668\u4e2dCNN\u6a21\u578b\u7684\u80fd\u91cf\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2511.17235", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.17235", "abs": "https://arxiv.org/abs/2511.17235", "authors": ["Rohit Prasad"], "title": "NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices", "comment": "This paper has been accepted for publication at the Design, Automation and Test in Europe (DATE) Conference 2026. 2026 IEEE. Personal use of this material is permitted", "summary": "The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets.", "AI": {"tldr": "NX-CGRA\u662f\u4e00\u79cd\u9488\u5bf9\u8fb9\u7f18\u8ba1\u7b97\u4e2dTransformer\u63a8\u7406\u7684\u53ef\u7f16\u7a0b\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u91c7\u7528\u7c97\u7c92\u5ea6\u53ef\u91cd\u6784\u9635\u5217\u67b6\u6784\uff0c\u652f\u6301\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u51fd\u6570\uff0c\u5728\u6027\u80fd\u3001\u80fd\u6548\u548c\u67b6\u6784\u7075\u6d3b\u6027\u4e4b\u95f4\u5b9e\u73b0\u5e73\u8861\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u4e2dTransformer\u5de5\u4f5c\u8d1f\u8f7d\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u9700\u8981\u5728\u6027\u80fd\u3001\u80fd\u6548\u548c\u67b6\u6784\u7075\u6d3b\u6027\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u70b9\uff0c\u800c\u56fa\u5b9a\u529f\u80fd\u52a0\u901f\u5668\u65e0\u6cd5\u9002\u5e94\u5e7f\u6cdb\u7684\u7528\u4f8b\u3002", "method": "\u91c7\u7528\u7c97\u7c92\u5ea6\u53ef\u91cd\u6784\u9635\u5217\u67b6\u6784\uff0c\u5177\u6709\u8f6f\u4ef6\u9a71\u52a8\u7684\u53ef\u7f16\u7a0b\u6027\uff0c\u80fd\u591f\u9ad8\u6548\u6267\u884c\u5404\u79cd\u5185\u6838\u6a21\u5f0f\uff0c\u652f\u6301\u7ebf\u6027\u548c\u975e\u7ebf\u6027Transformer\u63a8\u7406\u7b97\u6cd5\u3002", "result": "\u4f7f\u7528\u771f\u5b9eTransformer\u6a21\u578b\u7684\u4ee3\u8868\u6027\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u9ad8\u6574\u4f53\u6548\u7387\u548c\u5728\u4e0d\u540c\u64cd\u4f5c\u7c7b\u522b\u4e0a\u6709\u5229\u7684\u80fd\u8017-\u9762\u79ef\u6743\u8861\u3002", "conclusion": "NX-CGRA\u4f5c\u4e3a\u53ef\u6269\u5c55\u548c\u9002\u5e94\u6027\u5f3a\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u53d7\u9650\u7684\u529f\u7387\u548c\u7845\u9884\u7b97\u4e0b\u5177\u6709\u8fb9\u7f18Transformer\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.17265", "categories": ["cs.AR", "cs.AI", "cs.ET", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.17265", "abs": "https://arxiv.org/abs/2511.17265", "authors": ["Shady Agwa", "Yikang Shen", "Shiwei Wang", "Themis Prodromakis"], "title": "DISCA: A Digital In-memory Stochastic Computing Architecture Using A Compressed Bent-Pyramid Format", "comment": "6 pages, 5 figures", "summary": "Nowadays, we are witnessing an Artificial Intelligence revolution that dominates the technology landscape in various application domains, such as healthcare, robotics, automotive, security, and defense. Massive-scale AI models, which mimic the human brain's functionality, typically feature millions and even billions of parameters through data-intensive matrix multiplication tasks. While conventional Von-Neumann architectures struggle with the memory wall and the end of Moore's Law, these AI applications are migrating rapidly towards the edge, such as in robotics and unmanned aerial vehicles for surveillance, thereby adding more constraints to the hardware budget of AI architectures at the edge. Although in-memory computing has been proposed as a promising solution for the memory wall, both analog and digital in-memory computing architectures suffer from substantial degradation of the proposed benefits due to various design limitations. We propose a new digital in-memory stochastic computing architecture, DISCA, utilizing a compressed version of the quasi-stochastic Bent-Pyramid data format. DISCA inherits the same computational simplicity of analog computing, while preserving the same scalability, productivity, and reliability of digital systems. Post-layout modeling results of DISCA show an energy efficiency of 3.59 TOPS/W per bit at 500 MHz using a commercial 180nm CMOS technology. Therefore, DISCA significantly improves the energy efficiency for matrix multiplication workloads by orders of magnitude if scaled and compared to its counterpart architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDISCA\u7684\u65b0\u578b\u6570\u5b57\u5185\u5b58\u968f\u673a\u8ba1\u7b97\u67b6\u6784\uff0c\u91c7\u7528\u538b\u7f29\u7684\u51c6\u968f\u673aBent-Pyramid\u6570\u636e\u683c\u5f0f\uff0c\u5728\u4fdd\u6301\u6570\u5b57\u7cfb\u7edf\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6a21\u62df\u8ba1\u7b97\u7684\u8ba1\u7b97\u7b80\u5355\u6027\u3002", "motivation": "\u4f20\u7edf\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u9762\u4e34\u5185\u5b58\u5899\u548c\u6469\u5c14\u5b9a\u5f8b\u7ec8\u7ed3\u7684\u6311\u6218\uff0c\u800c\u73b0\u6709\u5185\u5b58\u8ba1\u7b97\u67b6\u6784\uff08\u5305\u62ec\u6a21\u62df\u548c\u6570\u5b57\uff09\u7531\u4e8e\u8bbe\u8ba1\u9650\u5236\u5bfc\u81f4\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002AI\u5e94\u7528\u5411\u8fb9\u7f18\u8fc1\u79fb\u5bf9\u786c\u4ef6\u9884\u7b97\u63d0\u51fa\u4e86\u66f4\u591a\u7ea6\u675f\u3002", "method": "\u4f7f\u7528\u538b\u7f29\u7684\u51c6\u968f\u673aBent-Pyramid\u6570\u636e\u683c\u5f0f\u7684\u6570\u5b57\u5185\u5b58\u968f\u673a\u8ba1\u7b97\u67b6\u6784DISCA\uff0c\u7ed3\u5408\u4e86\u6a21\u62df\u8ba1\u7b97\u7684\u8ba1\u7b97\u7b80\u5355\u6027\u548c\u6570\u5b57\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3001\u751f\u4ea7\u6027\u548c\u53ef\u9760\u6027\u3002", "result": "\u5728\u5546\u7528180nm CMOS\u6280\u672f\u4e0b\uff0cDISCA\u5728500MHz\u9891\u7387\u4e0b\u5b9e\u73b0\u4e86\u6bcf\u6bd4\u72793.59 TOPS/W\u7684\u80fd\u6548\uff0c\u76f8\u6bd4\u540c\u7c7b\u67b6\u6784\u5c06\u77e9\u9635\u4e58\u6cd5\u5de5\u4f5c\u8d1f\u8f7d\u7684\u80fd\u6548\u63d0\u9ad8\u4e86\u6570\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "DISCA\u67b6\u6784\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u683c\u5f0f\u548c\u8ba1\u7b97\u65b9\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u5b58\u5899\u95ee\u9898\uff0c\u4e3a\u8fb9\u7f18AI\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u80fd\u6548\u7684\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.17418", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.17418", "abs": "https://arxiv.org/abs/2511.17418", "authors": ["Houji Zhou", "Ling Yang", "Zhiwei Zhou", "Yi Li", "Xiangshui Miao"], "title": "MemIntelli: A Generic End-to-End Simulation Framework for Memristive Intelligent Computing", "comment": null, "summary": "Memristive in-memory computing (IMC) has emerged as a promising solution for addressing the bottleneck in the Von Neumann architecture. However, the couplingbetweenthecircuitandalgorithm in IMC makes computing reliability susceptible to non-ideal effects in devices and peripheral circuits. In this respect, efficient softwarehardwareco-simulationtoolsarehighlydesiredtoembedthedevice and circuit models into the algorithms. In this paper, for the first time, we proposed an end-to-end simulation framework supporting flexible variable-precision computing, named MemIntelli, to realize the pre-verification of diverse intelligent applications on memristive devices. At the device and circuit level, mathematical functions are employed to abstract the devices and circuits through meticulous equivalent circuit modeling. On the architecture level, MemIntelli achieves flexible variable-precision IMC supporting integer and floating data representation with bit-slicing. Moreover, MemIntelli is compatible with NumPy and PyTorch for seamless integration with applications. To demonstrate its capabilities, diverse intelligent algorithms, such as equation solving, data clustering, wavelet transformation, and neural network training and inference, were employed to showcase the robust processing ability of MemIntelli. This research presents a comprehensive simulation tool that facilitates the co-design of the IMC system, spanning from device to application.", "AI": {"tldr": "\u63d0\u51fa\u4e86MemIntelli\uff0c\u4e00\u4e2a\u652f\u6301\u7075\u6d3b\u53ef\u53d8\u7cbe\u5ea6\u8ba1\u7b97\u7684\u5185\u5b58\u8ba1\u7b97\u7aef\u5230\u7aef\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5fc6\u963b\u5668\u4ef6\u4e0a\u9884\u9a8c\u8bc1\u5404\u79cd\u667a\u80fd\u5e94\u7528\u3002", "motivation": "\u5fc6\u963b\u5185\u5b58\u8ba1\u7b97\u5b58\u5728\u7535\u8def\u4e0e\u7b97\u6cd5\u8026\u5408\u95ee\u9898\uff0c\u8ba1\u7b97\u53ef\u9760\u6027\u6613\u53d7\u5668\u4ef6\u548c\u5916\u56f4\u7535\u8def\u975e\u7406\u60f3\u6548\u5e94\u5f71\u54cd\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u4eff\u771f\u5de5\u5177\u5c06\u5668\u4ef6\u548c\u7535\u8def\u6a21\u578b\u5d4c\u5165\u7b97\u6cd5\u3002", "method": "\u5728\u5668\u4ef6\u548c\u7535\u8def\u5c42\u9762\u4f7f\u7528\u6570\u5b66\u51fd\u6570\u901a\u8fc7\u7b49\u6548\u7535\u8def\u5efa\u6a21\u62bd\u8c61\u5668\u4ef6\u548c\u7535\u8def\uff1b\u5728\u67b6\u6784\u5c42\u9762\u5b9e\u73b0\u652f\u6301\u6574\u6570\u548c\u6d6e\u70b9\u6570\u636e\u8868\u793a\u7684\u7075\u6d3b\u53ef\u53d8\u7cbe\u5ea6\u5185\u5b58\u8ba1\u7b97\uff1b\u517c\u5bb9NumPy\u548cPyTorch\u5b9e\u73b0\u4e0e\u5e94\u7528\u7684\u96c6\u6210\u3002", "result": "\u901a\u8fc7\u65b9\u7a0b\u6c42\u89e3\u3001\u6570\u636e\u805a\u7c7b\u3001\u5c0f\u6ce2\u53d8\u6362\u3001\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u548c\u63a8\u7406\u7b49\u667a\u80fd\u7b97\u6cd5\u9a8c\u8bc1\u4e86MemIntelli\u7684\u5f3a\u5927\u5904\u7406\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u4eff\u771f\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u4ece\u5668\u4ef6\u5230\u5e94\u7528\u7684\u5185\u5b58\u8ba1\u7b97\u7cfb\u7edf\u534f\u540c\u8bbe\u8ba1\u3002"}}
