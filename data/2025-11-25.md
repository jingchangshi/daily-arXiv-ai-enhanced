<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 7]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [TensorRight: Automated Verification of Tensor Graph Rewrites](https://arxiv.org/abs/2511.17838)
*Jai Arora,Sirui Lu,Devansh Jain,Tianfan Xu,Farzin Houshmand,Phitchaya Mangpo Phothilimthana,Mohsen Lesani,Praveen Narayanan,Karthik Srinivasa Murthy,Rastislav Bodik,Amit Sabne,Charith Mendis*

Main category: cs.PL

TL;DR: TensorRight是第一个能够验证任意秩和尺寸张量图重写的自动验证系统，通过引入聚合轴定义和边界验证方法，解决了现有系统无法处理无界张量的问题。


<details>
  <summary>Details</summary>
Motivation: 现有张量编译器重写优化缺乏对任意秩和尺寸张量的自动验证保证，只能验证具体秩的张量，无法提供无界设置下的正确性保证。

Method: 引入TensorRight DSL核心语言，使用聚合轴定义表示重写规则；通过证明存在秩边界，将无界验证转化为有限数量的边界验证任务；使用符号执行和SMT求解器自动验证。

Result: 在XLA代数简化器的175个重写规则中，TensorRight能够证明115个规则在完全通用性下的正确性，而最接近的自动边界验证系统只能表达18个规则。

Conclusion: TensorRight填补了张量图重写无界验证的空白，为张量编译器提供了首个能够验证任意秩和尺寸张量重写正确性的自动验证系统。

Abstract: Tensor compilers, essential for generating efficient code for deep learning models across various applications, employ tensor graph rewrites as one of the key optimizations. These rewrites optimize tensor computational graphs with the expectation of preserving semantics for tensors of arbitrary rank and size. Despite this expectation, to the best of our knowledge, there does not exist a fully automated verification system to prove the soundness of these rewrites for tensors of arbitrary rank and size. Previous works, while successful in verifying rewrites with tensors of concrete rank, do not provide guarantees in the unbounded setting.
  To fill this gap, we introduce TensorRight, the first automatic verification system that can verify tensor graph rewrites for input tensors of arbitrary rank and size. We introduce a core language, TensorRight DSL, to represent rewrite rules using a novel axis definition, called aggregated-axis, which allows us to reason about an unbounded number of axes. We achieve unbounded verification by proving that there exists a bound on tensor ranks, under which bounded verification of all instances implies the correctness of the rewrite rule in the unbounded setting. We derive an algorithm to compute this rank using the denotational semantics of TensorRight DSL. TensorRight employs this algorithm to generate a finite number of bounded-verification proof obligations, which are then dispatched to an SMT solver using symbolic execution to automatically verify the correctness of the rewrite rules. We evaluate TensorRight's verification capabilities by implementing rewrite rules present in XLA's algebraic simplifier. The results demonstrate that TensorRight can prove the correctness of 115 out of 175 rules in their full generality, while the closest automatic, bounded-verification system can express only 18 of these rules.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Pier: Efficient Large Language Model pretraining with Relaxed Global Communication](https://arxiv.org/abs/2511.17849)
*Shuyuan Fan,Zhao Zhang*

Main category: cs.DC

TL;DR: Pier是一个高效的分布式优化器，通过减少全局通信来加速大语言模型预训练，在保持模型性能的同时实现显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 全局通信（如all-reduce和allgather）是大语言模型预训练中的主要性能瓶颈，需要开发更高效的优化器来减少通信开销。

Method: 基于DiLoCo框架，在处理器组内使用内部优化器，全局通信使用外部优化器，并引入动量预热和动量衰减技术来保持收敛性。

Result: 在256个A100 GPU上，Pier将GPT-2 XL训练速度提升2.7x-3.7x；在64个GH200 Superchips上提升1.2x-1.9x；在128个A100上，GPT-2 7B训练时间减少54.5%，且验证损失和下游任务性能无下降。

Conclusion: Pier通过放松全局通信要求，在保持模型性能的同时显著加速大语言模型预训练，是解决分布式训练通信瓶颈的有效方案。

Abstract: Global communication, such as all-reduce and allgather, is the prominent performance bottleneck in large language model (LLM) pretraining. To address this issue, we present Pier, an efficient and scalable optimizer with relaxed global communication. Pier is built upon DiLoCo, which leverages an inner optimizer within groups of processors and an outer optimizer that requires global communication. To preserve the convergence and model performance, Pier incorporates two key techniques for the outer optimizer: momentum warmup and momentum decay. Pier employs an efficient and scalable system architecture to enable complex parallelization strategies in LLM pretraining. We examine the model performance and runtime reduction of Pier using the GPT model family (e.g., small, medium, XL, and 7B) and the OpenWebText dataset with a suite of thirteen downstream tasks. With data parallel strategy, Pier speeds up GPT-2 XL training by up to 2.7x-3.7x on 256 NVIDIA A100 GPUs and 1.2x-1.9x on 64 GH200 Superchips, respectively, without degradation of validation loss or downstream task performance. With data parallel and tensor parallel, Pier reduces the time cost GPT-2 7B model training by 54.5% on 128 A100s.

</details>


### [3] [SAGkit: A Python SAG Toolkit for Response Time Analysis of Hybrid-Triggered Jobs](https://arxiv.org/abs/2511.17882)
*Ruide Cao,Zhuyun Qi,Qinyang He,Chenxi Ling,Yi Wang,Guoming Tang*

Main category: cs.DC

TL;DR: SAGkit是一个Python工具包，实现了调度抽象图(SAG)框架，用于分布式控制系统的精确可持续响应时间分析，解决了传统方法在非抢占式系统中状态空间爆炸的问题。


<details>
  <summary>Details</summary>
Motivation: 现代延迟关键应用对实时性和鲁棒性要求越来越高，传统响应时间分析方法在处理非抢占式系统、释放抖动和执行时间变化时面临状态空间爆炸问题。

Method: 开发SAGkit工具包，基于调度抽象图框架，通过在SAG基础上允许作业缺席，实现混合触发作业的精确可持续响应时间分析。

Result: 实验表明SAGkit在可接受的运行时和内存开销下实现了精确性，能够分析复杂的分布式控制系统。

Conclusion: SAGkit作为轻量级开源工具包，为研究人员分析复杂分布式控制系统提供了有效工具，支持进一步开发。

Abstract: For distributed control systems, modern latency-critical applications are increasingly demanding real-time guarantees and robustness. Response-time analysis (RTA) is useful for this purpose, as it helps analyze and guarantee timing bounds. However, conventional RTA methods struggle with the state-space explosion problem, especially in non-preemptive systems with release jitter and execution time variations. In this paper, we introduce SAGkit, a Python toolkit that implements the schedule-abstraction graph (SAG) framework. SAGkit novelly enables exact and sustainable RTA of hybrid-triggered jobs by allowing job absence on the SAG basis. Our experiments demonstrate that SAGkit achieves exactness with acceptable runtime and memory overhead. This lightweight toolkit empowers researchers to analyze complex distributed control systems and is open-access for further development.

</details>


### [4] [MIDAS: Adaptive Proxy Middleware for Mitigating Metadata Hotspots in HPC I/O at Scale](https://arxiv.org/abs/2511.18124)
*Sangam Ghimire,Nigam Niraula,Nirjal Bhurtel,Paribartan Timalsina,Bishal Neupane,James Bhattarai,Sudan Jha*

Main category: cs.DC

TL;DR: MIDAS是一个自适应中间件层，通过智能负载均衡、协作缓存和自稳定控制循环来解决元数据热点问题，显著降低队列长度和热点影响。


<details>
  <summary>Details</summary>
Motivation: 元数据热点是HPC和云存储环境中可扩展I/O的主要障碍，会导致长队列、尾部延迟增加和系统吞吐量降低。现有解决方案过于僵化、部署侵入性强或在动态负载下不稳定。

Method: MIDAS包含三个机制：(1)基于实时遥测的命名空间感知负载均衡器，(2)通过租约、失效或自适应超时保持后端语义的协作缓存层，(3)动态调整路由攻击性和缓存寿命的自稳定控制循环。

Result: 与轮询调度相比，MIDAS将平均队列长度减少约23%，在最坏情况下热点缓解达80%。

Conclusion: 基于中间件的稳定性感知策略可以为元数据管理提供后端无关的改进，在突发场景中实现更好的可扩展性、更可预测的尾部延迟和更强的整体系统性能。

Abstract: Metadata hotspots remain one of the key obstacles to scalable Input/Output (I/O) in both High-Performance Computing (HPC) and cloud-scale storage environments. Situations such as job start-ups, checkpoint storms, or heavily skewed namespace access can trigger thousands of concurrent metadata requests against a small subset of servers. The result is long queues, inflated tail latencies, and reduced system throughput. Prior efforts including static namespace partitioning, backend-specific extensions, and kernel-level modifications address parts of the problem, but they often prove too rigid, intrusive to deploy, or unstable under shifting workloads. We present MIDAS, an adaptive middleware layer that operates transparently between clients and metadata servers, requiring no changes to kernels or storage backends. The design brings together three mechanisms: (i) a namespace-aware load balancer that enhances consistent hashing with power-of-d sampling informed by live telemetry, (ii) a cooperative caching layer that preserves backend semantics through leases, invalidations, or adaptive timeouts, and (iii) a self-stabilizing control loop that dynamically adjusts routing aggressiveness and cache lifetimes while avoiding oscillations under bursty workloads. Analysis of the model and controlled experiments show that MIDAS reduces average queue lengths by roughly 23% and mitigates worst-case hotspots by up to 80% when compared to round-robin scheduling. These findings highlight that a stability-aware, middleware-based strategy can provide backend-agnostic improvements to metadata management, enabling better scalability in bursty scenarios, more predictable tail latencies, and stronger overall system performance.

</details>


### [5] [Simulating Dynamic Cloud Marketspaces: Modeling Spot Instance Behavior and Scheduling with CloudSim Plus](https://arxiv.org/abs/2511.18137)
*Christoph Goldgruber,Benedikt Pittl,Erich Schikuta*

Main category: cs.DC

TL;DR: 扩展CloudSim Plus模拟框架以支持动态云定价环境中的实例生命周期管理，并评估改进的HLEM-VMP分配算法在波动工作负载下的性能表现


<details>
  <summary>Details</summary>
Motivation: 公共云环境中动态定价模型（如竞价实例）的日益普及给工作负载调度和可靠性带来了新挑战，现有分配算法和模拟工具未能充分处理这些模型引入的波动性和不确定性

Method: 扩展CloudSim Plus模拟框架以支持真实的竞价实例生命周期管理（包括中断、终止、休眠和重新分配），使用合成场景和基于Google Cluster Trace数据集的大规模模拟进行验证，并评估改进的HLEM-VMP分配算法在动态竞价市场条件下的性能

Result: 与基线分配策略相比，改进的HLEM-VMP算法减少了竞价实例中断次数和最大中断持续时间

Conclusion: 该工作提供了模拟动态云行为的框架，并对虚拟机分配性能和市场风险提供了分析见解，有助于实现更稳健和成本效益的云计算资源管理

Abstract: The increasing reliance on dynamic pricing models, such as spot instances, in public cloud environments presents new challenges for workload scheduling and reliability. While these models offer cost advantages, they introduce volatility and uncertainty that are not fully addressed by current allocation algorithms or simulation tools. This work contributes to the modeling and evaluation of such environments by extending the CloudSim Plus simulation framework to support realistic spot instance lifecycle management, including interruption, termination, hibernation, and reallocation. The enhanced simulator is validated using synthetic scenarios and large-scale simulations based on the Google Cluster Trace dataset. Building on this foundation, the HLEM-VMP allocation algorithm, originally proposed in earlier research, was adapted to operate under dynamic spot market conditions. Its performance was evaluated against baseline allocation strategies to assess its efficiency and resilience in volatile workload environments. The comparison demonstrated a reduction in the number of spot instance interruptions as well as a decrease in the maximum interruption duration. Overall, this work provides both a simulation framework for simulating dynamic cloud behavior and analytical insights into virtual machine allocation performance and market risk, contributing to more robust and cost-effective resource management in cloud computing.

</details>


### [6] [AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems](https://arxiv.org/abs/2511.18151)
*Rajat Bhattacharjya,Sing-Yao Wu,Hyunwoo Oh,Chaewon Nam,Suyeon Koo,Mohsen Imani,Elaheh Bozorgzadeh,Nikil Dutt*

Main category: cs.DC

TL;DR: AVERY是一个自适应分割计算框架，通过认知启发的双流分割将视觉语言模型（VLM）部署在无人机上，实现在灾难响应等低带宽环境下的实时可查询智能分析。


<details>
  <summary>Details</summary>
Motivation: 无人机在灾难响应中需要复杂的语义推理能力，但现有CNN无法提供可查询智能，而VLM虽然具备这种能力但资源需求过高，无法在设备上部署，且传统的云端卸载在低带宽灾难区域网络下失效。

Method: 提出功能性的认知启发双流分割方法：高频低分辨率的"上下文流"用于实时感知，低频高保真的"洞察流"用于深度分析。轻量级自感知控制器根据网络条件和操作意图动态选择预训练压缩模型。

Result: 在边缘-云场景下使用LISA-7B VLM评估，AVERY在波动网络条件下始终优于静态配置，相比原始图像压缩准确率提高11.2%，相比全边缘执行能耗降低93.98%。

Conclusion: AVERY框架显著提升了任务效率，在资源受限平台和动态环境中实现了实时可查询智能。

Abstract: Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution "context stream" for real-time awareness and a low-frequency, high-fidelity "insight stream" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.

</details>


### [7] [IOMMU Support for Virtual-Address Remote DMA in an ARMv8 environment](https://arxiv.org/abs/2511.19258)
*Antonis Psistakis*

Main category: cs.DC

TL;DR: 该论文成功测试并验证了ARM SMMU（IOMMU）在Xilinx Zynq UltraScale+ MPSoC平台上的功能，通过开发自定义内核模块实现了虚拟地址到物理地址的转换，支持DMA传输，并实现了动态地址翻译功能。


<details>
  <summary>Details</summary>
Motivation: 支持Unimem系统中通过IOMMU实现节点间高效一致性的方法，由于Linux对SMMU的文档有限且不清晰，需要验证SMMU在单节点中的功能。

Method: 开发自定义内核模块：1）在PS中插入虚拟到物理地址映射并触发DMA传输；2）从PL发起DMA事务验证SMMU翻译；3）配置SMMU使用用户进程页表指针实现动态地址翻译。

Result: 在所有测试场景中成功验证了SMMU的正确操作，包括PS和PL的DMA传输地址翻译，以及动态地址翻译功能。

Conclusion: 成功演示了SMMU的功能，为Unimem系统的实现提供了基础支持，高级SMMU特性的探索留待未来工作。

Abstract: In complex systems with many compute nodes containing multiple CPUs that are coherent within each node, a key challenge is maintaining efficient and correct coherence between nodes. The Unimem system addresses this by proposing a virtualized global address space that enables such coherence, relying on the I/O Memory Management Unit (IOMMU) in each node. The goal of this thesis is to support this approach by successfully testing and using the IOMMU of a single node. For this purpose, we used ARM's IOMMU, known as the System Memory Management Unit (SMMU), which translates virtual addresses to physical addresses. Because Linux documentation for the SMMU is limited and unclear, we implemented custom kernel modules to test and use its functionality.
  First, we tested the SMMU in the Processing System (PS) of the Xilinx Zynq UltraScale+ MPSoC by developing a module that inserted virtual-to-physical address mappings into the SMMU. We then triggered a DMA transfer to a virtual address and observed that the request passed through the SMMU for address translation. We repeated this experiment by initiating DMA transactions from the Programmable Logic (PL) and similarly confirmed that the transactions were translated by the SMMU. Finally, we developed a module that enables transactions from the PL without requiring explicit pre-mapping of virtual and physical address pairs. This was achieved by configuring the SMMU with the page table pointer of a user process, allowing it to translate all relevant virtual addresses dynamically.
  Overall, we successfully demonstrated the correct operation of the SMMU across all tested scenarios. Due to time constraints, further exploration of advanced SMMU features is left for future work.

</details>


### [8] [Monotone Decontamination of Arbitrary Dynamic Graphs with Mobile Agents](https://arxiv.org/abs/2511.18315)
*Rajashree Bar,Daibik Barik,Adri Bhattacharya,Partha Sarathi Mandal*

Main category: cs.DC

TL;DR: 本文研究了动态图中的单调去污问题，提出了两种动态性模型，并给出了所需移动代理数量的上下界。


<details>
  <summary>Details</summary>
Motivation: 网络去污问题在静态图中已有研究，但在动态图中尚未探索。本文旨在研究任意动态图中的单调去污问题，重点关注由于边突然消失或重新出现带来的困难。

Method: 设计了两种基于边重新出现时间的动态性模型，在每个模型中提出了完全单调去污所需代理数量的下界和上界。

Result: 获得了动态网络中单调去污问题所需代理数量的理论界限，揭示了边动态变化对去污过程的影响。

Conclusion: 本文首次系统研究了动态图中的单调去污问题，为优化所需代理数量提供了理论依据，并展示了边动态性带来的挑战。

Abstract: Network decontamination is a well-known problem, in which the aim of the mobile agents should be to decontaminate the network (i.e., both nodes and edges). This problem comes with an added constraint, i.e., of \emph{monotonicity}, in which whenever a node or an edge is decontaminated, it must not get recontaminated. Hence, the name comes \emph{monotone decontamination}. This problem has been relatively explored in static graphs, but nothing is known yet in dynamic graphs. We, in this paper, study the \emph{monotone decontamination} problem in arbitrary dynamic graphs. We designed two models of dynamicity, based on the time within which a disappeared edge must reappear. In each of these two models, we proposed lower bounds as well as upper bounds on the number of agents, required to fully decontaminate the underlying dynamic graph, monotonically. Our results also highlight the difficulties faced due to the sudden disappearance or reappearance of edges. Our aim in this paper has been to primarily optimize the number of agents required to solve monotone decontamination in these dynamic networks.

</details>


### [9] [An Online Fragmentation-Aware GPU Scheduler for Multi-Tenant MIG-based Clouds](https://arxiv.org/abs/2511.18906)
*Marco Zambianco,Lorenzo Fasol,Roberto Doriguzzi-Corin*

Main category: cs.DC

TL;DR: 提出了一种针对NVIDIA MIG GPU云平台的调度框架，通过最小化碎片化来最大化工作负载接受率，在重负载条件下平均增加10%的调度工作负载数量。


<details>
  <summary>Details</summary>
Motivation: GPU资源的爆炸性增长需求促使云提供商通过GPU即服务平台提供可租用资源。MIG技术虽然提供硬件级隔离，但其固定分区导致调度刚性，在多租户环境中造成严重的GPU碎片化问题，限制了可容纳的工作负载数量。

Method: 提出了一个新颖的调度框架，引入碎片化指标来量化资源效率并指导分配决策。基于此指标，设计贪心调度算法为每个传入工作负载选择最小化碎片化增长的GPU和MIG切片。

Result: 在多样化工作负载分布下与多种基线策略对比评估，结果表明该方法始终实现更高的工作负载接受率，在重负载条件下平均增加10%的调度工作负载数量，同时使用与基准方法大致相同数量的GPU。

Conclusion: 该调度框架有效解决了MIG云环境中的GPU碎片化问题，显著提高了资源利用效率和工作负载容纳能力。

Abstract: The explosive growth of AI applications has created unprecedented demand for GPU resources. Cloud providers meet this demand through GPU-as-a-Service platforms that offer rentable GPU resources for running AI workloads. In this context, the sharing of GPU resources between different tenants is essential to maximize the number of scheduled workloads. Among the various GPU sharing technologies, NVIDIA's Multi-Instance GPU (MIG) stands out by partitioning GPUs at hardware level into isolated slices with dedicated compute and memory, ensuring strong tenant isolation, preventing resource contention, and enhancing security. Despite these advantages, MIG's fixed partitioning introduces scheduling rigidity, leading to severe GPU fragmentation in multi-tenant environments, where workloads are continuously deployed and terminated. Fragmentation leaves GPUs underutilized, limiting the number of workloads that can be accommodated. To overcome this challenge, we propose a novel scheduling framework for MIG-based clouds that maximizes workload acceptance while mitigating fragmentation in an online, workload-agnostic setting. We introduce a fragmentation metric to quantify resource inefficiency and guide allocation decisions. Building on this metric, our greedy scheduling algorithm selects GPUs and MIG slices that minimize fragmentation growth for each incoming workload. We evaluate our approach against multiple baseline strategies under diverse workload distributions. Results demonstrate that our method consistently achieves higher workload acceptance rates, leading to an average 10% increase in the number of scheduled workloads in heavy load conditions, while using approximately the same number of GPUs as the benchmark methods.

</details>


### [10] [AME: An Efficient Heterogeneous Agentic Memory Engine for Smartphones](https://arxiv.org/abs/2511.19192)
*Xinkui Zhao,Qingyu Ma,Yifan Zhang,Hengxuan Lou,Guanjie Cheng,Shuiguang Deng,Jianwei Yin*

Main category: cs.DC

TL;DR: AME是一个专为智能手机SoC设计的设备端智能体记忆引擎，解决了现有向量数据库在移动设备上的性能瓶颈，通过硬件感知的矩阵流水线和智能调度机制，显著提升了查询、索引构建和插入操作的性能。


<details>
  <summary>Details</summary>
Motivation: 设备端智能体需要持续演进的记忆来支持个性化、上下文感知和长期行为，但现有向量数据库主要针对服务器环境设计，在移动设备上存在硬件约束不匹配和工作负载不匹配的问题。

Method: 提出了AME设备端智能体记忆引擎，包含两个关键技术：(1)硬件感知的高效矩阵流水线，最大化计算单元利用率并利用多级片上存储；(2)硬件和工作负载感知的调度方案，协调查询、插入和索引重建以最小化延迟。

Result: 在Snapdragon 8系列SoC上实现AME并在HotpotQA上评估，结果显示：查询吞吐量提升1.4倍，索引构建速度提升7倍，并发查询工作负载下插入吞吐量提升6倍。

Conclusion: AME成功解决了移动设备上向量数据库的性能瓶颈，为设备端智能体提供了高效、响应迅速的记忆管理解决方案。

Abstract: On-device agents on smartphones increasingly require continuously evolving memory to support personalized, context-aware, and long-term behaviors. To meet both privacy and responsiveness demands, user data is embedded as vectors and stored in a vector database for fast similarity search. However, most existing vector databases target server-class environments. When ported directly to smartphones, two gaps emerge: (G1) a mismatch between mobile SoC constraints and vector-database assumptions, including tight bandwidth budgets, limited on-chip memory, and stricter data type and layout constraints; and (G2) a workload mismatch, because on-device usage resembles a continuously learning memory, in which queries must coexist with frequent inserts, deletions, and ongoing index maintenance. To address these challenges, we propose AME, an on-device Agentic Memory Engine co-designed with modern smartphone SoCs. AME introduces two key techniques: (1) a hardware-aware, high-efficiency matrix pipeline that maximizes compute-unit utilization and exploits multi-level on-chip storage to sustain high throughput; and (2) a hardware- and workload-aware scheduling scheme that coordinates querying, insertion, and index rebuilding to minimize latency. We implement AME on Snapdragon 8-series SoCs and evaluate it on HotpotQA. In our experiments, AME improves query throughput by up to 1.4x at matched recall, achieves up to 7x faster index construction, and delivers up to 6x higher insertion throughput under concurrent query workloads.

</details>


### [11] [Constant-Size Certificates for Leader Election in Chordal Graphs and Related Classes](https://arxiv.org/abs/2511.19208)
*Jérémie Chalopin,Maria Kokkou*

Main category: cs.DC

TL;DR: 该论文提出了针对弦图和可拆卸图的领导选举和生成树构造的常数大小局部认证方案，并展示了如何将认证方案转换为自稳定算法。


<details>
  <summary>Details</summary>
Motivation: 研究分布式计算中的认证方案，使图节点能高效验证问题解决方案的正确性，特别关注领导选举和生成树构造这两个基本问题。

Method: 为弦图和K4-free可拆卸图提供领导选举认证方案，为可拆卸图提供生成树构造认证方案，使用常数大小证书和局部验证条件。

Result: 成功设计了针对特定图类的常数大小局部认证方案，弦图领导选举方案还确保无环定向，并提出了认证方案到自稳定算法的自动转换方法。

Conclusion: 这些是针对这些图类的首个局部认证结果，揭示了可用于验证其他问题的结构特性，认证方案到自稳定算法的转换具有独立研究价值。

Abstract: In distributed computing a certification scheme consists of a set of states and conditions over those states that enable each node of a graph to efficiently verify the correctness of a solution to a given problem. This work focuses on two fundamental problems: leader election and spanning tree construction. For each problem, we present a constant-size (per edge), local certification scheme, where the conditions available to each node can only refer to the graph induced by its one-hop neighborhood. In particular, we provide certification schemes for leader election in chordal and $K_4$-free dismantlable graphs and for spanning tree construction in dismantlable graphs, assuming a root is given. For chordal graphs, our leader election certification scheme additionally ensures an acyclic orientation, a property that is not generally verifiable using constant-size certificates in arbitrary graphs. To the best of our knowledge, these are the first local certification results tailored to these graph classes, potentially highlighting structural properties useful for verifying additional problems. Finally, we propose an algorithm that automatically transforms any certification scheme into a silent self-stabilizing algorithm (i.e., an algorithm that automatically recovers from faults) by adding only one extra state to the set of states of the certification scheme, assuming a Gouda fair scheduler. This transformation may be of independent interest.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [Optimized Memory Tagging on AmpereOne Processors](https://arxiv.org/abs/2511.17773)
*Shiv Kaushik,Mahesh Madhav,Nagi Aboulenein,Jason Bessette,Sandeep Brahmadathan,Ben Chaffin,Matthew Erler,Stephan Jourdan,Thomas Maciukenas,Ramya Masti,Jon Perry,Massimo Sutera,Scott Tetrick,Bret Toll,David Turley,Carl Worth,Atiq Bajwa*

Main category: cs.AR

TL;DR: ARM MTE内存标签扩展技术首次在AmpereOne数据中心处理器中实现，提供同步标签检查功能，能有效检测和预防内存安全漏洞，且不占用额外内存容量，性能影响仅为个位数百分比。


<details>
  <summary>Details</summary>
Motivation: 解决C/C++等指针语言中的内存安全漏洞问题，这些漏洞是众多安全攻击的根源。现有的编译器扩展和ISA扩展在开销和适用性方面存在限制，需要更高效的解决方案。

Method: 在AmpereOne处理器中实现ARM MTE内存标签扩展，采用同步标签检查模式，通过硬件优化实现零内存容量开销的标签存储。

Result: MTE能够确定性检测和预防顺序缓冲区溢出攻击，概率性检测和预防临时性释放后使用指针错误。性能影响仅为个位数百分比，适用于广泛的数据中心工作负载。

Conclusion: AmpereOne处理器的MTE实现为生产云环境部署提供了高效硬件基础，同时指出了应用内存管理作为主要开销来源的软件优化机会。

Abstract: Memory-safety escapes continue to form the launching pad for a wide range of security attacks, especially for the substantial base of deployed software that is coded in pointer-based languages such as C/C++. Although compiler and Instruction Set Architecture (ISA) extensions have been introduced to address elements of this issue, the overhead and/or comprehensive applicability have limited broad production deployment. The Memory Tagging Extension (MTE) to the ARM AArch64 Instruction Set Architecture is a valuable tool to address memory-safety escapes; when used in synchronous tag-checking mode, MTE provides deterministic detection and prevention of sequential buffer overflow attacks, and probabilistic detection and prevention of exploits resulting from temporal use-after-free pointer programming bugs. The AmpereOne processor, launched in 2024, is the first datacenter processor to support MTE. Its optimized MTE implementation uniquely incurs no memory capacity overhead for tag storage and provides synchronous tag-checking with single-digit performance impact across a broad range of datacenter class workloads. Furthermore, this paper analyzes the complete hardware-software stack, identifying application memory management as the primary remaining source of overhead and highlighting clear opportunities for software optimization. The combination of an efficient hardware foundation and a clear path for software improvement makes the MTE implementation of the AmpereOne processor highly attractive for deployment in production cloud environments.

</details>


### [13] [Comprehensive Design Space Exploration for Tensorized Neural Network Hardware Accelerators](https://arxiv.org/abs/2511.17971)
*Jinsong Zhang,Minghe Li,Jiayi Tian,Jinming Lu,Zheng Zhang*

Main category: cs.AR

TL;DR: 提出了一个统一的协同探索框架，将张量分解模型的收缩路径、硬件架构和数据流映射联合优化，以最大化边缘设备上的部署效率。


<details>
  <summary>Details</summary>
Motivation: 现有高阶张量分解研究主要关注算法优势（如精度和压缩比），而忽视了硬件部署效率。硬件无关的设计往往掩盖了张量化模型的实际延迟和能耗优势。

Method: 提出了一个协同探索框架，将收缩路径、硬件架构和数据流映射统一在一个设计空间中，通过面向延迟的搜索目标进行全局探索，实现端到端模型效率。

Result: 在可配置FPGA内核上实现优化配置，相比密集基线模型，推理延迟降低4倍，训练延迟降低3.85倍。

Conclusion: 收缩路径、硬件架构和数据流映射是紧密耦合的，必须在统一设计空间中联合优化，才能在真实设备上最大化部署效率。

Abstract: High-order tensor decomposition has been widely adopted to obtain compact deep neural networks for edge deployment. However, existing studies focus primarily on its algorithmic advantages such as accuracy and compression ratio-while overlooking the hardware deployment efficiency. Such hardware-unaware designs often obscure the potential latency and energy benefits of tensorized models. Although several works attempt to reduce computational cost by optimizing the contraction sequence based on the number of multiply-accumulate operations, they typically neglect the underlying hardware characteristics, resulting in suboptimal real-world performance. We observe that the contraction path, hardware architecture, and dataflow mapping are tightly coupled and must be optimized jointly within a unified design space to maximize deployment efficiency on real devices. To this end, we propose a co-exploration framework that unifies these dimensions within a unified design space for efficient training and inference of tensorized neural networks on edge platforms. The framework formulates a latency oriented search objective and solves it via a global latency-driven exploration across the unified design space to achieve end-to-end model efficiency. The optimized configurations are implemented on a configurable FPGA kernel, achieving up to 4 and 3.85 lower inference and training latency compared with the dense baseline.

</details>


### [14] [HDDB: Efficient In-Storage SQL Database Search Using Hyperdimensional Computing on Ferroelectric NAND Flash](https://arxiv.org/abs/2511.18234)
*Quanling Zhao,Yanru Chen,Runyang Tian,Sumukh Pinge,Weihong Xu,Augusto Vega,Steven Holmes,Saransh Gupta,Tajana Rosing*

Main category: cs.AR

TL;DR: HDDB是一个结合超维计算(HDC)和铁电NAND存储器的硬件-软件协同设计，用于在存储中执行SQL谓词评估和分析，具有大规模并行性和最小数据移动。


<details>
  <summary>Details</summary>
Motivation: 利用HDC的噪声容忍特性与新兴铁电NAND存储器的高密度和存储内计算能力相结合，解决传统SQL数据库在大型事实表上评估谓词和扫描时对低能耗和低延迟的需求。

Method: 提出新颖的HDC编码技术用于标准SQL数据表，将基于谓词的过滤和聚合公式化为高效的HDC操作，利用HDC内在冗余性在设备噪声下保持正确结果。

Result: 在TPC-DS事实表上的实验显示，HDDB相比传统CPU/GPU SQL数据库引擎实现了80.6倍的低延迟和12,636倍的低能耗。

Conclusion: HDDB为噪声鲁棒、内存中心的数据处理提供了一个实用基础，特别适合在具有高原始比特错误率的存储设备上执行数据库操作。

Abstract: Hyperdimensional Computing (HDC) encodes information and data into high-dimensional distributed vectors that can be manipulated using simple bitwise operations and similarity searches, offering parallelism, low-precision hardware friendliness, and strong robustness to noise. These properties are a natural fit for SQL database workloads dominated by predicate evaluation and scans, which demand low energy and low latency over large fact tables. Notably, HDC's noise-tolerance maps well onto emerging ferroelectric NAND (FeNAND) memories, which provide ultra-high density and in-storage compute capability but suffer from elevated raw bit-error rates. In this work, we propose HDDB, a hardware-software co-design that combines HDC with FeNAND multi-level cells (MLC) to perform in-storage SQL predicate evaluation and analytics with massive parallelism and minimal data movement. Particularly, we introduce novel HDC encoding techniques for standard SQL data tables and formulate predicate-based filtering and aggregation as highly efficient HDC operations that can happen in-storage. By exploiting the intrinsic redundancy of HDC, HDDB maintains correct predicate and decode outcomes under substantial device noise (up to 10% randomly corrupted TLC cells) without explicit error-correction overheads. Experiments on TPC-DS fact tables show that HDDB achieves up to 80.6x lower latency and 12,636x lower energy consumption compared to conventional CPU/GPU SQL database engines, suggesting that HDDB provides a practical substrate for noise-robust, memory-centric database processing.

</details>


### [15] [Evaluation of NVENC Split-Frame Encoding (SFE) for UHD Video Transcoding](https://arxiv.org/abs/2511.18687)
*Kasidis Arunruangsirilert,Jiro Katto*

Main category: cs.AR

TL;DR: NVIDIA Split-Frame Encoding (SFE) 技术通过将单个UHD帧分割到多个NVENC芯片并行编码，显著提升编码吞吐量，在实时应用中能以可忽略的RD性能损失实现近乎双倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 随着消费设备能够拍摄4K和8K超高清视频，需要高性能视频转码器进行互联网传输。NVIDIA GPU在数据中心广泛应用，NVENC将在UHD视频转码中发挥关键作用。

Method: 使用标准化测试序列评估SFE对RD性能、编码吞吐量、功耗和端到端延迟的影响。SFE将单个UHD帧分割到多个物理编码器并行编码，然后拼接结果。

Result: 在实时应用中，SFE使编码吞吐量近乎翻倍，RD性能损失可忽略，支持4K使用更高质量预设，使实时8K编码成为可能。4K无延迟增加，8K可减少延迟。

Conclusion: SFE是实现高吞吐量、实时UHD转码的关键技术，在吞吐量和效率之间实现了良好平衡。

Abstract: NVIDIA Encoder (NVENC) features in modern NVIDIA GPUs, offer significant advantages over software encoders by providing comparable Rate-Distortion (RD) performance while consuming considerably less power. The increasing capability of consumer devices to capture footage in Ultra High-Definition (UHD) at 4K and 8K resolutions necessitates high-performance video transcoders for internet-based delivery. To address this demand, NVIDIA introduced Split-Frame Encoding (SFE), a technique that leverages multiple on-die NVENC chips available in high-end GPUs. SFE splits a single UHD frame for parallel encoding across these physical encoders and subsequently stitches the results, which significantly improves encoding throughput. However, this approach is known to incur an RD performance penalty. The widespread adoption of NVIDIA GPUs in data centers, driven by the rise of Generative AI, means NVENC is poised to play a critical role in transcoding UHD video. To better understand the performance-efficiency tradeoff of SFE, this paper evaluates SFE's impact on RD performance, encoding throughput, power consumption, and end-to-end latency using standardized test sequences. The results show that for real-time applications, SFE nearly doubles encoding throughput with a negligible RD performance penalty, which enables the use of higher-quality presets for 4K and makes real-time 8K encoding feasible, effectively offsetting the minor RD penalty. Moreover, SFE adds no latency at 4K and can reduce it at 8K, positioning it as a key enabler for high-throughput, real-time UHD transcoding.

</details>


### [16] [Evaluation of GPU Video Encoder for Low-Latency Real-Time 4K UHD Encoding](https://arxiv.org/abs/2511.18688)
*Kasidis Arunruangsirilert,Jiro Katto*

Main category: cs.AR

TL;DR: 评估NVIDIA、Intel和AMD GPU上的低延迟编码模式，比较硬件编码器和软件编码器的性能，发现硬件编码器在实现超低延迟（83ms）的同时保持良好率失真性能。


<details>
  <summary>Details</summary>
Motivation: 随着4K超高清视频流需求增长，需要了解GPU硬件编码器在低延迟模式下的性能表现，为6G时代的实时视频应用提供参考。

Method: 从率失真性能和延迟两个角度评估三大厂商GPU的低延迟编码模式，并与硬件编码器的正常延迟模式及领先软件编码器进行对比。

Result: 硬件编码器相比软件解决方案实现显著更低的端到端延迟，且率失真性能略优；超低延迟模式可将延迟降至83ms（5帧）而不影响率失真性能；硬件编码器延迟对质量预设不敏感。

Conclusion: GPU硬件编码器能够实现高质量、低延迟的视频流传输，超低延迟模式在保持率失真性能的同时大幅降低延迟，为实时视频应用提供了理想解决方案。

Abstract: The demand for high-quality, real-time video streaming has grown exponentially, with 4K Ultra High Definition (UHD) becoming the new standard for many applications such as live broadcasting, TV services, and interactive cloud gaming. This trend has driven the integration of dedicated hardware encoders into modern Graphics Processing Units (GPUs). Nowadays, these encoders support advanced codecs like HEVC and AV1 and feature specialized Low-Latency and Ultra Low-Latency tuning, targeting end-to-end latencies of < 2 seconds and < 500 ms, respectively. As the demand for such capabilities grows toward the 6G era, a clear understanding of their performance implications is essential. In this work, we evaluate the low-latency encoding modes on GPUs from NVIDIA, Intel, and AMD from both Rate-Distortion (RD) performance and latency perspectives. The results are then compared against both the normal-latency tuning of hardware encoders and leading software encoders. Results show hardware encoders achieve significantly lower E2E latency than software solutions with slightly better RD performance. While standard Low-Latency tuning yields a poor quality-latency trade-off, the Ultra Low-Latency mode reduces E2E latency to 83 ms (5 frames) without additional RD impact. Furthermore, hardware encoder latency is largely insensitive to quality presets, enabling high-quality, low-latency streams without compromise.

</details>


### [17] [Splatonic: Architecture Support for 3D Gaussian Splatting SLAM via Sparse Processing](https://arxiv.org/abs/2511.18755)
*Xiaotong Huang,He Zhu,Tianrui Ma,Yuxiang Xiong,Fangxin Liu,Zhezhi He,Yiming Gan,Zihan Liu,Jingwen Leng,Yu Feng,Minyi Guo*

Main category: cs.AR

TL;DR: Splatonic是一个稀疏高效的3D高斯溅射SLAM算法-硬件协同设计，通过自适应稀疏像素采样和像素级渲染流水线，在移动设备上实现实时性能，相比移动GPU获得274.9倍加速和4738.5倍节能。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射SLAM算法在移动平台上计算成本过高，特别是跟踪过程，限制了其在资源受限设备上的实用性。

Method: 提出自适应稀疏像素采样算法减少渲染像素数达256倍；设计像素级渲染流水线，通过高斯并行渲染和抢占式α检查提高硬件利用率；提出流水线架构简化设计并解决投影和聚合瓶颈。

Result: 在四个3DGS-SLAM算法上评估，相比移动GPU获得274.9倍加速和4738.5倍节能，相比最先进加速器获得25.2倍加速和241.1倍节能，同时保持可比精度。

Conclusion: Splatonic通过算法-硬件协同设计成功解决了3D高斯溅射SLAM在移动设备上的计算瓶颈，实现了高效的实时性能。

Abstract: 3D Gaussian splatting (3DGS) has emerged as a promising direction for SLAM due to its high-fidelity reconstruction and rapid convergence. However, 3DGS-SLAM algorithms remain impractical for mobile platforms due to their high computational cost, especially for their tracking process.
  This work introduces Splatonic, a sparse and efficient real-time 3DGS-SLAM algorithm-hardware co-design for resource-constrained devices. Inspired by classical SLAMs, we propose an adaptive sparse pixel sampling algorithm that reduces the number of rendered pixels by up to 256$\times$ while retaining accuracy. To unlock this performance potential on mobile GPUs, we design a novel pixel-based rendering pipeline that improves hardware utilization via Gaussian-parallel rendering and preemptive $α$-checking. Together, these optimizations yield up to 121.7$\times$ speedup on the bottleneck stages and 14.6$\times$ end-to-end speedup on off-the-shelf GPUs. To further address new bottlenecks introduced by our rendering pipeline, we propose a pipelined architecture that simplifies the overall design while addressing newly emerged bottlenecks in projection and aggregation. Evaluated across four 3DGS-SLAM algorithms, Splatonic achieves up to 274.9$\times$ speedup and 4738.5$\times$ energy savings over mobile GPUs and up to 25.2$\times$ speedup and 241.1$\times$ energy savings over state-of-the-art accelerators, all with comparable accuracy.

</details>


### [18] [HeLEx: A Heterogeneous Layout Explorer for Spatial Elastic Coarse-Grained Reconfigurable Arrays](https://arxiv.org/abs/2511.19366)
*Alan Jia Bao Du,Tarek S. Abdelrahman*

Main category: cs.AR

TL;DR: HeLEx框架通过分支定界搜索优化异构粗粒度可重构阵列的功能布局，平均减少68.7%的操作数量，降低70%面积和51%功耗，接近理论最小CGRA的6.2%以内。


<details>
  <summary>Details</summary>
Motivation: 现有的异构CGRA设计需要优化功能布局以减少操作数量、面积和功耗，同时保持对输入数据流图的映射能力。

Method: 从全功能布局开始，使用分支定界搜索逐步从处理单元中消除操作，确保输入数据流图仍能成功映射到生成的CGRA上。

Result: 在12个数据流图和9个CGRA尺寸上的实验显示，平均减少68.7%操作，面积减少近70%，功耗降低超51%，优于现有方法达2.6倍。

Conclusion: HeLEx能有效生成接近理论最优的异构CGRA功能布局，显著降低硬件开销同时保持计算能力。

Abstract: We present HeLEx, a framework for determining the functional layout of heterogeneous spatially-configured elastic Coarse-Grained Reconfigurable Arrays (CGRAs). Given a collection of input data flow graphs (DFGs) and a target CGRA, the framework starts with a full layout in which every processing element (PE) supports every operation in the DFGs. It then employs a branch-and-bound (BB) search to eliminate operations out of PEs, ensuring that the input DFGs successfully map onto the resulting CGRAs, eventually returning an optimized heterogeneous CGRA. Experimental evaluation with 12 DFGs and 9 target CGRA sizes reveals that the framework reduces the number of operations by 68.7% on average, resulting in a reduction of CGRA area by almost 70% and of power by over 51%, all compared to the initial full layout. HeLEx generates CGRAs that are on average only within 6.2% of theoretically minimum CGRAs that support exactly the number of operations needed by the input DFGs. A comparison with functional layouts produced by two state-of-the-art frameworks indicates that HeLEx achieves better reduction in the number of operations, by up to 2.6X.

</details>
