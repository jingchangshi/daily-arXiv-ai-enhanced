<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Evaluate the Stack Management in Effect Handlers using the libseff C Library](https://arxiv.org/abs/2512.03083)
*ZeHao Yu*

Main category: cs.PL

TL;DR: 本文提出了一种基于用户级过量提交的栈管理方法，通过libseff C库实现，利用虚拟内存机制和基于保护延迟分配结合信号驱动内存提交，动态按需调整栈大小，提高内存利用率。


<details>
  <summary>Details</summary>
Motivation: 效应处理器在现代编程中日益重要，用于管理并发、异步操作和异常处理等复杂计算效应。然而，由于效应处理器引入的动态控制流变化，高效的栈管理仍然是一个重大挑战。传统栈管理方法存在内存浪费和性能问题。

Method: 采用用户级过量提交方法，在libseff C库中实现。利用虚拟内存机制和基于保护延迟分配结合信号驱动内存提交，动态按需调整栈大小。通过信号处理实现栈扩展，减少内存浪费。

Result: 实验结果表明：1) 内核级过量提交在性能和灵活性之间达到最佳平衡；2) 用户级实现虽然灵活，但引入了额外开销；3) 与传统固定大小栈、分段栈等方法相比，用户级过量提交在内存利用率方面有优势，但性能需要优化。

Conclusion: 本研究对各种栈管理策略进行了详细比较分析，提供了针对特定应用需求和操作约束的实用建议。未来工作将重点改进用户级过量提交机制，减轻非确定性行为，并扩展基准测试框架以包含真实场景。

Abstract: Effect handlers are increasingly prominent in modern programming for managing complex computational effects, including concurrency, asynchronous operations, and exception handling, in a modular and flexible manner. Efficient stack management remains a significant challenge for effect handlers due to the dynamic control flow changes they introduce. This paper explores a novel stack management approach using user-level overcommitting within the libseff C library, which leverages virtual memory mechanisms and protection-based lazy allocation combined with signal-driven memory commitment. Our user-level overcommitting implementation dynamically resizes stacks on-demand, improving memory utilization and reducing waste compared to traditional methods. We rigorously benchmark and evaluate this novel strategy against conventional fixed- size stacks, segmented stacks, and kernel-based overcommitting, using metrics such as context-switch latency, stack expansion efficiency, multi-threaded performance, and robustness under rapid stack growth conditions. Experimental results demonstrate that kernel-based overcommitting achieves an effective balance between performance and flexibility, whereas our user-level implementation, while flexible, incurs additional overheads, highlighting areas for optimization. This study provides a detailed comparative analysis of various stack management strate- gies, offering practical recommendations tailored to specific application requirements and operational constraints. Future work will focus on refining user-level overcommit- ting mechanisms, mitigating non-deterministic behaviors, and expanding benchmark frameworks to include real-world scenarios.

</details>


### [2] [Beyond Code Pairs: Dialogue-Based Data Generation for LLM Code Translation](https://arxiv.org/abs/2512.03086)
*Le Chen,Nuo Xu,Winson Chen,Bin Lei,Pei-Hung Lin,Dunzhi Zhou,Rajeev Thakur,Caiwen Ding,Ali Jannesari,Chunhua Liao*

Main category: cs.PL

TL;DR: 提出自动化数据集生成流水线，通过双LLM问答设计结合编译器反馈，为低资源编程语言翻译生成带单元测试的验证翻译和多轮对话数据，显著提升翻译功能正确性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码翻译方面表现出色，但在Fortran、CUDA等低资源编程领域性能下降，因为这些领域缺乏高质量的并行数据

Method: 采用自动化数据集生成流水线，设计双LLM问答系统（Questioner-Solver），结合编译器知识和运行时反馈，生成带单元测试的验证翻译和多轮对话数据

Result: 为Fortran->C++和C++->CUDA分别生成3.64k和3.93k对话数据，微调后功能正确性显著提升，C++-to-CUDA任务的单元测试成功率提高超过56%，7B开源模型在编译成功率等关键指标上超越大型专有系统

Conclusion: 提出的自动化数据集生成方法能有效解决低资源编程语言翻译的数据稀缺问题，通过生成高质量的训练数据显著提升翻译模型性能，使小型开源模型在特定任务上超越大型专有系统

Abstract: Large language models (LLMs) have shown remarkable capabilities in code translation, yet their performance deteriorates in low-resource programming domains such as Fortran and emerging frameworks like CUDA, where high-quality parallel data are scarce. We present an automated dataset generation pipeline featuring a dual-LLM Questioner-Solver design that incorporates external knowledge from compilers and runtime feedback. Beyond traditional source-target code pair datasets, our approach additionally generates (1) verified translations with unit tests for assessing functional consistency, and (2) multi-turn dialogues that capture the reasoning process behind translation refinement. Applied to Fortran -> C++ and C++ -> CUDA, the pipeline yields 3.64k and 3.93k dialogues, respectively. Fine-tuning on this data yields dramatic improvements in functional correctness, boosting unit test success rates by over 56% on the challenging C++-to-CUDA task. We show this data enables a 7B open-weight model to significantly outperform larger proprietary systems on key metrics like compilation success.

</details>


### [3] [OOPredictor: Predicting Object-Oriented Accesses using Static Analysis](https://arxiv.org/abs/2512.03972)
*Hassan Arafat,David Bremner,Kenneth B. Kent,Julian Wang*

Main category: cs.PL

TL;DR: 使用编译时静态分析预测Java程序运行时访问模式，通过马尔可夫链建模程序行为，为垃圾收集器提供更友好的内存布局建议


<details>
  <summary>Details</summary>
Motivation: 面向对象编程中的指针追逐导致缓存性能下降，现代硬件预取器难以处理这种不可预测的访问模式，现有软件方法需要运行时分析带来显著开销

Method: 在OpenJ9 JVM的OMR优化器基础设施中实现编译时静态分析器，预测程序最常见的访问模式，输出马尔可夫链模型

Result: 预测器表现出良好的准确性，能够为最小侵入性的负载停顿缓解策略提供信息，如指导复制式垃圾收集器采用更友好的内存复制顺序

Conclusion: 编译时静态分析可以有效预测Java程序的运行时访问模式，为优化内存布局和提升缓存性能提供可行方案

Abstract: Object-oriented Programming has become one of the most dominant design paradigms as the separation of concerns and adaptability of design reduce development and maintenance costs. However, the convenience is not without cost. The added indirection inherent in such designs causes excessive pointer chasing, negatively affecting locality, which in turn degrades the performance of cache structures. Furthermore, modern hardware prefetchers are mostly stride prefetchers that are ill-equipped to handle the unpredictability of access patterns generated by pointer chasing. Most software approaches that seek to address this problem resort to profiling the program as it runs, which comes with a significant run-time overhead or requires data from previous runs. In this paper, we propose the use of compile-time static analysis to predict the most common access patterns displayed by a program during run time. Since Java is one of the most popular object-oriented languages, we implement our prototype within the OpenJ9 JVM, inside the OMR optimizer infrastructure. The outputs of our proposed predictor are Markov chains that model the expected behavior of the program. The effectiveness of the proposed predictor is evaluated by comparing the model with the actual run-time behavior of the program measured using an instrumented interpreter. Our experiments show that the proposed predictor exhibits good accuracy and can be used to inform minimally intrusive load stall mitigation strategies, e.g. informing copying GCs on more locality-friendly copying orders

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [TokenScale: Timely and Accurate Autoscaling for Disaggregated LLM Serving with Token Velocity](https://arxiv.org/abs/2512.03416)
*Ruiqi Lai,Hongrui Liu,Chengzhi Lu,Zonghao Liu,Siyu Cao,Siyang Shao,Yixin Zhang,Luo Mai,Dmitrii Ustiugov*

Main category: cs.DC

TL;DR: TokenScale：一个针对LLM服务中prefill/decode解耦架构的自动扩缩容框架，通过Token Velocity指标和Convertible Decoders创新，显著提升SLO达成率并降低成本。


<details>
  <summary>Details</summary>
Motivation: LLM服务中的prefill/decode解耦架构提高了资源利用率，但现有自动扩缩容策略（如AIBrix和DistServe中的方案）基于GPU利用率或请求数等滞后指标，对突发负载反应慢，导致TTFT和TPOT SLO违规以及过度配置成本高。

Method: 1. 提出Token Velocity指标：统一prefill、网络和decode阶段的工作速率，作为系统背压的前导指标，实现主动扩缩容。2. 引入Convertible Decoders：允许解码GPU在流量高峰时动态执行prefill任务，创建快速响应缓冲区吸收突发流量，消除新prefiller的初始化延迟。

Result: 在GPU集群和生产环境trace上的评估显示，TokenScale将SLO达成率从50-88%提升到80-96%，相比DistServe、BlitzScale和AIBrix等先进系统，成本降低4-14%。

Conclusion: 通过结合预测性指标和灵活的系统设计，TokenScale显著提升了LLM解耦服务架构的性能和效率，解决了突发负载下的扩缩容挑战。

Abstract: The architectural shift to prefill/decode (PD) disaggregation in LLM serving improves resource utilization but struggles with the bursty nature of modern workloads. Existing autoscaling policies, often retrofitted from monolithic systems like those in AIBrix and DistServe, rely on lagging indicators such as GPU utilization or coarse-grained request counts. This results in slow reactions to load spikes, leading to significant Time-to First-Token (TTFT) and Time-Per-Output-Token (TPOT) SLO violations and costly over-provisioning. We introduce TokenScale, an autoscaling framework that resolves this performance mismatch through two innovations. First, we propose Token Velocity, a novel metric that unifies the prefill, network, and decode stages by quantifying their rate of work. As a leading indicator of system backpressure, it enables proactive scaling. Second, Convertible Decoders allow decoder GPUs to dynamically execute prefill tasks during traffic spikes, creating a rapid-response buffer that absorbs bursts and eliminates the initialization latency of new prefillers. Our evaluation on a GPU cluster with production traces shows TokenScale improves SLO attainment from 50-88% to 80-96% and reduces costs by 4-14% over state-of-the-art systems, including DistServe, BlitzScale, and AIBrix. By uniting a predictive metric with a flexible system design, TokenScale significantly boosts the performance and efficiency of disaggregated LLM serving infrastructure.

</details>


### [5] [Double-Edge-Assisted Computation Offloading and Resource Allocation for Space-Air-Marine Integrated Networks](https://arxiv.org/abs/2512.03487)
*Zhen Wang,Bin Lin,Qiang,Ye*

Main category: cs.DC

TL;DR: 提出了一种面向空天地海一体化网络的双边缘辅助计算卸载与资源分配方案，利用无人机和低轨卫星为海上自主船舶提供并行计算服务，通过优化算法最小化能耗


<details>
  <summary>Details</summary>
Motivation: 空天地海一体化网络中海事自主船舶的计算需求日益增长，需要高效的计算卸载和资源分配方案来满足低延迟和高能效的要求

Method: 采用双边缘计算架构，允许海事自主船舶同时向无人机和低轨卫星卸载部分计算任务；提出基于交替优化和分层方法的能量高效算法，联合优化卸载模式、卸载量和计算资源分配

Result: 通过仿真验证了所提方案在能耗和延迟方面的有效性，相比基准算法表现出更好的性能

Conclusion: 该双边缘辅助计算卸载方案能够有效降低空天地海一体化网络的能耗，同时满足海事自主船舶的计算延迟要求，为未来海洋物联网应用提供了可行的解决方案

Abstract: In this paper, we propose a double-edge-assisted computation offloading and resource allocation scheme tailored for space-air-marine integrated networks (SAMINs). Specifically, we consider a scenario where both unmanned aerial vehicles (UAVs) and a low earth orbit (LEO) satellite are equipped with edge servers, providing computing services for maritime autonomous surface ships (MASSs). Partial computation workloads of MASSs can be offloaded to both UAVs and the LEO satellite, concurrently, for processing via a multi-access approach. To minimize the energy consumption of SAMINs under latency constraints, we formulate an optimization problem and propose energy efficient algorithms to jointly optimize offloading mode, offloading volume, and computing resource allocation of the LEO satellite and the UAVs, respectively. We further exploit an alternating optimization (AO) method and a layered approach to decompose the original problem to attain the optimal solutions. Finally, we conduct simulations to validate the effectiveness and efficiency of the proposed scheme in comparison with benchmark algorithms.

</details>


### [6] [Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas](https://arxiv.org/abs/2512.03565)
*Luis Gall,Samuel James Newcome,Fabio Alexander Gratl,Markus Mühlhäußer,Manish Kumar Mishra,Hans-Joachim Bungartz*

Main category: cs.DC

TL;DR: 该研究探索了在AutoPas粒子模拟库中，通过不同的SIMD向量化技术优化分子间成对力计算，重点关注粒子值加载到向量寄存器的顺序对执行时间和能耗的影响。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟在原子尺度上为科学家提供有价值的物理过程洞察。先前研究表明最优MD算法可能在运行时发生变化，因此需要研究模拟特定参数（如粒子密度和邻居识别算法）的影响，并扩展AutoPas的动态调优机制以在运行时选择最优向量化顺序。

Method: 探索多种SIMD向量化技术，重点研究粒子值加载到向量寄存器的顺序优化。扩展AutoPas的动态调优机制，使其能够在运行时选择最优的向量化顺序。研究模拟特定参数如粒子密度和邻居识别算法的影响。

Result: 基准测试表明，在运行时考虑不同的粒子相互作用顺序，相比AutoPas先前的方法，能够显著提高力计算的性能。

Conclusion: 通过动态选择最优的粒子相互作用顺序，可以显著提升分子动力学模拟中成对力计算的性能，这为AutoPas库提供了重要的优化改进。

Abstract: Molecular Dynamics simulations can help scientists to gather valuable insights for physical processes on an atomic scale. This work explores various techniques for SIMD vectorization to improve the pairwise force calculation between molecules in the scope of the particle simulation library AutoPas. The focus lies on the order in which particle values are loaded into vector registers to achieve the most optimal performance regarding execution time or energy consumption.
  As previous work indicates that the optimal MD algorithm can change during runtime, this paper investigates simulation-specific parameters like particle density and the impact of the neighbor identification algorithms, which distinguishes this work from related projects. Furthermore, AutoPas' dynamic tuning mechanism is extended to choose the optimal vectorization order during runtime.
  The benchmarks show that considering different particle interaction orders during runtime can lead to a considerable performance improvement for the force calculation compared to AutoPas' previous approach.

</details>


### [7] [FFTrainer: Fast Failover in Large-Language Model Training with Almost-Free State Management](https://arxiv.org/abs/2512.03644)
*Bohan Zhao,Yuanhong Wang,Chenglin Liu,Jiagi Pan,Guang Yang,Ruitao Liu,Tingrui Zhang,Kai Luo,Wei Xu*

Main category: cs.DC

TL;DR: FFTrainer利用网络剩余带宽快速保存和加载状态，减少LLM训练中的回滚和恢复时间


<details>
  <summary>Details</summary>
Motivation: 随着LLM集群规模扩大，节点故障、恢复时间长和大检查点降低了训练效率。异步检查点要么触发昂贵的回滚，要么增加过高开销

Method: FFTrainer利用剩余网络容量快速保存和加载状态，防止回滚并加速恢复

Result: 相比现有检查点方法，FFTrainer减少恢复时间达98%，减少GPU利用率损失达68%，且不影响正常训练

Conclusion: FFTrainer通过利用网络剩余带宽实现了高效、鲁棒的LLM训练，显著减少了恢复时间和资源浪费

Abstract: Recent developments in large language models (LLMs) have introduced new requirements for efficient and robust training. As LLM clusters scale, node failures, lengthy recoveries, and bulky checkpoints erode efficiency. Infrequent asynchronous checkpoints trigger costly rollbacks, yet higher frequencies add prohibitive overhead. To address these challenges, we propose FFTrainer, a system designed for robust LLM training. FFTrainer leverages surplus network capacity to quickly save and load states, thereby preventing rollbacks and accelerating recovery. Compared with prior checkpointing approaches, FFTrainer reduces recovery time by up to 98% and mitigates GPU utilization loss by up to 68% without hindering normal training.

</details>


### [8] [On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs](https://arxiv.org/abs/2512.03697)
*Rafael Ravedutti Lucio Machado,Jan Eitzinger,Georg Hager,Gerhard Wellein*

Main category: cs.DC

TL;DR: 该论文探讨了在Fritz和Alex HPC集群上分析合成基准测试和Gromacs软件包能效时遇到的挑战，使用Intel Ice Lake/Sapphire Rapids CPU和Nvidia A40/A100 GPU进行实验，揭示了实验和分析中的问题并提出了最佳实践建议。


<details>
  <summary>Details</summary>
Motivation: 研究高性能计算集群中能效分析的挑战，特别是在使用不同硬件架构（CPU和GPU）和并行编程模型（MPI）时，为未来的能效研究提供指导。

Method: 在Fritz和Alex HPC集群上使用MPI并行化，在完整的Intel Ice Lake和Sapphire Rapids CPU插槽以及Nvidia A40和A100 GPU上运行合成基准测试和Gromacs软件包，使用Likwid和Nvidia性能分析工具进行测量。

Result: 展示了使用Likwid和Nvidia分析工具获得的指标和测量结果，揭示了在实验和分析过程中遇到的各种挑战和陷阱，包括测量准确性、工具使用复杂性等问题。

Conclusion: 提出了未来能效分析研究的最佳实践建议，强调了在高性能计算环境中进行能效评估时需要特别注意的方法论和技术挑战。

Abstract: This paper discusses the challenges encountered when analyzing the energy efficiency of synthetic benchmarks and the Gromacs package on the Fritz and Alex HPC clusters. Experiments were conducted using MPI parallelism on full sockets of Intel Ice Lake and Sapphire Rapids CPUs, as well as Nvidia A40 and A100 GPUs. The metrics and measurements obtained with the Likwid and Nvidia profiling tools are presented, along with the results. The challenges and pitfalls encountered during experimentation and analysis are revealed and discussed. Best practices for future energy efficiency analysis studies are suggested.

</details>


### [9] [Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods](https://arxiv.org/abs/2512.03825)
*Aingeru Ramos,Jose A Pascual,Javier Navaridas,Ivan Coluzza*

Main category: cs.DC

TL;DR: 本文提出了一个并行化的Metropolis-Hastings并行回火算法实现，使用OpenMP和CUDA分别在CPU和GPU上实现并行化，获得了最高52倍和986倍的加速比。


<details>
  <summary>Details</summary>
Motivation: 传统MCMC方法在处理复杂构型空间时采样精度不足，而并行回火技术虽然提高了精度但显著增加了计算成本。通过并行化可以抵消这种计算开销，使MCMC/PT技术更有效地运行并研究更大规模的模型。

Method: 开发了Metropolis-Hastings并行回火算法的并行实现，使用OpenMP在多核CPU上实现并行化，使用CUDA在GPU上实现并行化。该实现作为未来量子算法实现的基础基准。

Result: 使用OpenMP在48核CPU上获得了最高52倍的加速比，使用CUDA在GPU上获得了最高986倍的加速比。该结果可作为未来量子实现比较的基准。

Conclusion: 提出的并行化实现显著提升了MCMC/PT算法的计算效率，为研究更大规模物理/化学模型提供了有效工具，并为未来量子实现建立了性能基准。

Abstract: Markov Chain Monte Carlo methods are algorithms used to sample probability distributions, commonly used to sample the Boltzmann distribution of physical/chemical models (e.g., protein folding, Ising model, etc.). This allows us to study their properties by sampling the most probable states of those systems. However, the sampling capabilities of these methods are not sufficiently accurate when handling complex configuration spaces. This has resulted in the development of new techniques that improve sampling accuracy, usually at the expense of increasing the computational cost. One of such techniques is Parallel Tempering which improves accuracy by running several replicas which periodically exchange their states. Computationally, this imposes a significant slow-down, which can be counteracted by means of parallelization. These schemes enable MCMC/PT techniques to be run more effectively and allow larger models to be studied. In this work, we present a parallel implementation of Metropolis-Hastings with Parallel Tempering, using OpenMP and CUDA for the parallelization in modern CPUs and GPUs, respectively. The results show a maximum speed-up of 52x using OpenMP with 48 cores, and of 986x speed-up with the CUDA version. Furthermore, the results serve as a basic benchmark to compare a future quantum implementation of the same algorithm.

</details>


### [10] [OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference](https://arxiv.org/abs/2512.03927)
*Liujianfu Wang,Yuyang Du,Yuchen Pan,Soung Chang Liew,Jiacheng Liu,Kexin Chen*

Main category: cs.DC

TL;DR: OD-MoE：一种分布式MoE推理框架，通过按需加载专家参数，无需专家缓存，使MoE模型能在GPU内存小于1GB的边缘设备上运行


<details>
  <summary>Details</summary>
Motivation: MoE模型在内存受限的边缘设备上部署困难，现有专家卸载方法虽然将专家参数存储在CPU内存中，但GPU内存中的专家缓存利用率仍然较低，需要更好的解决方案

Method: 提出OD-MoE框架，包含两个关键技术：1）在分布式边缘节点上并行化专家加载和专家计算；2）超准确的模拟预测器，在专家计算进行时提前多层预测专家激活

Result: OD-MoE实现了99.94%的专家激活预测准确率，在仅使用1/3 GPU内存的情况下，达到完全GPU缓存MoE部署约75%的解码速度，使MoE能在小于1GB GPU内存的边缘节点上推理

Conclusion: OD-MoE通过消除专家缓存需求，为低成本物联网设备在边缘部署MoE模型铺平了道路，解决了边缘设备内存限制的关键挑战

Abstract: Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.

</details>


### [11] [A Chronological Analysis of the Evolution of SmartNICs](https://arxiv.org/abs/2512.04054)
*Olasupo Ajayi,Ryan Grant*

Main category: cs.DC

TL;DR: 本文对智能网卡（SNICs）进行了历时性分析，通过2009-2024年间370篇文献的系统回顾，探讨了SNICs的演变、制造商、使用场景和应用领域，旨在厘清其实际用途和适用性。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备的普及和对高速网络访问需求的增长，传统网卡已升级为智能网卡（SNICs），能够处理海量数据并实现近实时处理。然而，尽管SNICs日益流行，其确切用途和适用性仍存在争议，特别是随着加速器的集成使SNICs能够分担主机CPU任务，这些争议更加复杂化。

Method: 采用历时性分析方法，系统回顾了2009年至2024年15年间发表的370篇相关文献，对SNICs的演变过程、主要制造商、实际使用场景和应用领域进行了全面梳理和分析。

Result: 通过对大量文献的分析，揭示了SNICs的发展轨迹、主要制造商格局、具体使用场景以及在不同应用领域的适用性，为理解SNICs的实际价值和适用范围提供了实证依据。

Conclusion: 本研究通过系统文献回顾，阐明了智能网卡的发展历程、技术特点和应用现状，为学术界和工业界理解SNICs的实际用途和未来发展方向提供了重要参考，有助于解决当前关于SNICs适用性的争议。

Abstract: Network Interface Cards (NICs) are one of the key enablers of the modern Internet. They serve as gateways for connecting computing devices to networks for the exchange of data with other devices. Recently, the pervasive nature of Internet-enabled devices coupled with the growing demands for faster network access have necessitated the enhancement of NICs to Smart NICs (SNICs), capable of processing enormous volumes of data at near real-time speed. However, despite their popularity, the exact use and applicability of SNICs remains an ongoing debate. These debates are exacerbated by the incorporation of accelerators into SNIC, allowing them to relieve their host's CPUs of various tasks. In this work, we carry out a chronological analysis of SNICs, using 370 articles published in the past 15 years, from 2010 to 2024, to gain some insight into SNICs; and shed some light on their evolution, manufacturers, use cases, and application domains.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [Accelerating Detailed Routing Convergence through Offline Reinforcement Learning](https://arxiv.org/abs/2512.03594)
*Afsara Khan,Austin Rovinski*

Main category: cs.AR

TL;DR: 使用强化学习动态调整布线成本权重，显著提升详细布线效率，在保持或改进DRV数量的同时，实现1.56倍平均加速和最高3.01倍加速。


<details>
  <summary>Details</summary>
Motivation: 详细布线是现代物理设计中最复杂耗时的步骤之一，传统布线器使用静态成本权重调度，无法根据设计和工艺动态调整，导致收敛到零设计规则违规的解决方案耗时过长。

Method: 采用保守Q学习（CQL）模型，学习从先前设计中动态选择布线成本权重，以最小化算法迭代次数。该方法能够根据具体设计和工艺特性自适应调整布线策略。

Result: 在ISPD19基准测试中，该方法在保持或改进所有情况下的DRV数量的同时，实现了1.56倍平均加速和最高3.01倍的运行时间加速。学习还显示出跨工艺的泛化能力。

Conclusion: 强化学习能够有效优化详细布线过程，通过动态调整成本权重显著加速收敛，同时保持布线质量，并且这种学习具有跨工艺的泛化潜力。

Abstract: Detailed routing remains one of the most complex and time-consuming steps in modern physical design due to the challenges posed by shrinking feature sizes and stricter design rules. Prior detailed routers achieve state-of-the-art results by leveraging iterative pathfinding algorithms to route each net. However, runtimes are a major issue in detailed routers, as converging to a solution with zero design rule violations (DRVs) can be prohibitively expensive.
  In this paper, we propose leveraging reinforcement learning (RL) to enable rapid convergence in detailed routing by learning from previous designs. We make the key observation that prior detailed routers statically schedule the cost weights used in their routing algorithms, meaning they do not change in response to the design or technology. By training a conservative Q-learning (CQL) model to dynamically select the routing cost weights which minimize the number of algorithm iterations, we find that our work completes the ISPD19 benchmarks with 1.56x average and up to 3.01x faster runtime than the baseline router while maintaining or improving the DRV count in all cases. We also find that this learning shows signs of generalization across technologies, meaning that learning designs in one technology can translate to improved outcomes in other technologies.

</details>


### [13] [KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing](https://arxiv.org/abs/2512.03608)
*Lishuo Deng,Shaojie Xu,Jinwu Chen,Changwei Yan,Jiajie Wang,Zhe Jiang,Weiwei Shan*

Main category: cs.AR

TL;DR: KVNAND：首个完全基于计算型3D NAND闪存的DRAM-free架构，将模型权重和KV缓存都存储在闪存中，通过IFC技术、头组并行和页面级KV映射解决长上下文推理的内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备部署大语言模型面临内存瓶颈：单批次自回归推理的算术强度极低，造成严重的权重加载和带宽压力。现有IFC方案虽然将权重计算与闪存共置，但KV缓存仍需DRAM，随着上下文长度增长，KV缓存可能超过模型权重大小，导致DRAM成本过高且容量不足。

Method: 1) 完全基于计算型3D NAND闪存的DRAM-free架构；2) 利用IFC处理所有内存受限操作以减少数据传输开销；3) 引入头组并行提升吞吐量；4) 采用页面级KV缓存映射，使令牌访问模式与闪存组织对齐；5) 设计空间探索框架评估离散和紧凑变体，自动寻找最优设计权衡。

Result: 在MHA 7B和GQA 70B LLM上评估，KVNAND在128/1K/10K令牌上下文长度下分别实现1.98×/1.94×/2.05×几何平均加速比（相比DRAM-equipped IFC设计），并在100K上下文长度下解决了内存溢出问题。

Conclusion: KVNAND通过将模型权重和KV缓存完全存储在计算型闪存中，解决了长上下文LLM推理的内存瓶颈，使闪存成为KV存储的实用介质，显著提升了边缘设备上大语言模型的部署效率。

Abstract: Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.
  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\(\times\)/1.94\(\times\)/2.05\(\times\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.

</details>


### [14] [Lightweight Unified Sha-3/Shake Architecture with a Fault-Resilient State](https://arxiv.org/abs/2512.03616)
*Christian Ewert,Amrit Sharma Poudel,Mouadh Ayache,Andrija Neskovic,Rainer Buchty,Mladen Berekovic,Sebastian Berndt,Saleh Mulhem*

Main category: cs.AR

TL;DR: 提出统一哈希引擎支持Sha-3和Shake，采用字节级原地分区Keccak状态机制，并基于立方体结构设计二维奇偶校验故障检测，在保持100%三级故障检测的同时显著降低面积开销。


<details>
  <summary>Details</summary>
Motivation: 哈希函数已成为后量子密码学标准方案的关键部分，但在资源受限环境中需要轻量级实现。故障弹性设计对提升整个PQC系统可靠性至关重要，现有方案在面积开销和配置覆盖方面存在不足。

Method: 1) 提出统一哈希引擎，支持Sha-3和Shake，采用字节级原地分区机制处理Keccak状态；2) 基于Keccak立方体结构设计二维奇偶校验故障检测机制，实现状态保护。

Result: 实现100%三级故障检测和接近100%更高级别故障检测，面积开销降低3.7倍，整体故障弹性引擎设计缩小4.5倍。在RISC-V环境中集成时，面积开销小于8%。

Conclusion: 该方法为资源受限PQC应用中的哈希函数提供了鲁棒且轻量级的故障检测解决方案，在保持高故障检测率的同时显著优化了面积效率。

Abstract: Hash functions have become a key part of standard Post-quantum cryptography (PQC) schemes, especially Sha-3 and Shake, calling arXiv:submit/7045552 [cs.AR] 3 Dec 2025 for lightweight implementation. A fault-resilient design is always desirable to make the whole PQC system reliable. We, therefore, propose a) a unified hash engine supporting Sha-3 and Shake that follows a byte-wise in-place partitioning mechanism of the so-called Keccak state, and b) an according fault detection for Keccak state protection exploiting its cube structure by deploying two-dimensional parity checks. It outperforms the state-of-the-art (SoA) regarding area requirements at competitive register-level fault detection by achieving 100% detection of three and still near 100% of higher numbers of Keccak state faults. Unlike SoA solutions, the proposed unified hash engine covers all standard hash configurations. Moreover, the introduced multidimensional cross-parity check mechanism achieves a 3.7x improvement in area overhead, with an overall 4.5x smaller fault-resilient engine design as demonstrated in ASIC and FPGA implementations. Integrated into a RISC-V environment, the unified hash engine with the integrated fault-resilient mechanism introduced less than 8% area overhead. Our approach thus provides a robust and lightweight fault-detection solution for protecting hash functions deployed in resource-constrained PQC applications.

</details>


### [15] [The BrainScaleS-2 multi-chip system: Interconnecting continuous-time neuromorphic compute substrates](https://arxiv.org/abs/2512.03781)
*Joscha Ilmberger,Johannes Schemmel*

Main category: cs.AR

TL;DR: BrainScaleS-2系统通过FPGA互连扩展计算规模，实现芯片间低延迟通信


<details>
  <summary>Details</summary>
Motivation: 扩展BrainScaleS-2神经形态计算系统的计算规模，通过FPGA互连实现多芯片协同工作，构建更大规模的神经形态计算平台

Method: 使用基于FPGA的Aggregator单元进行互连，提供12个收发器链路连接Node-FPGA背板，通过标准19英寸4U机架集成系统控制器、以太网交换机和电源

Result: 在所有脉冲速率下，背板内芯片间延迟（经过三个FPGA的四跳）低于1.3微秒，实现了低延迟的芯片间通信

Conclusion: 通过FPGA互连成功扩展了BrainScaleS-2系统的计算规模，构建了可扩展的神经形态计算平台，为大规模神经形态计算应用奠定了基础

Abstract: The BrainScaleS-2 SoC integrates analog neuron and synapse circuits with digital periphery, including two CPUs with SIMD extensions. Each ASIC is connected to a Node-FPGA, providing experiment control and Ethernet connectivity. This work details the scaling of the compute substrate through FPGA-based interconnection via an additional Aggregator unit. The Aggregator provides up to 12 transceiver links to a backplane of Node-FPGAs, as well as 4 transceiver lanes for further extension. Two such interconnected backplanes are integrated into a standard 19in rack case with 4U height together with an Ethernet switch, system controller and power supplies. For all spike rates, chip-to-chip latencies -- consisting of four hops across three FPGAs -- below 1.3$μ$s are achieved within each backplane.

</details>
