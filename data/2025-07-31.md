<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A Compute-Matched Re-Evaluation of TroVE on MATH](https://arxiv.org/abs/2507.22069)
*Tobias Sesterhenn,Ian Berlot-Attwell,Janis Zenkner,Christian Bartelt*

Main category: cs.PL

TL;DR: TroVE声称通过工具重用提升性能，但实际改进可能源于更高的计算预算而非工具箱机制。修正后性能提升3%，但匹配计算后仅1%改进。


<details>
  <summary>Details</summary>
Motivation: 验证TroVE工具箱方法在MATH基准上的实际效果，分析其性能提升是否源于工具重用。

Method: 重新评估TroVE，分析其三种模式的影响，修正选择机制并匹配计算预算。

Result: TroVE性能提升3%，但匹配计算后仅1%改进，工具箱方法无显著优势。

Conclusion: TroVE的性能提升主要源于更高计算预算，工具箱机制对MATH基准无显著帮助。

Abstract: Reusing established theorems and formulas is central to mathematical problem
solving, serving as essential building blocks for tackling increasingly complex
challenges. Recent work, TroVE, argues that code-generating Large Language
Models (LLMs) can benefit similarly on the MATH benchmark by inducing and
reusing higher-level toolboxes. By allocating computational budget across an
ensemble of three modes -- directly generating code, creating tools, and
reusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only
performs direct generation. However, recent analysis (Berlot-Attwell et al.,
2024) casts doubt on these gains, noting that the tools created are often
trivial or rarely reused, suggesting that improvements may stem from
self-consistency or self-correction. In this work, we re-evaluate TroVE on
MATH, analyze the impact of each of its modes, and show that its benefit does
not come from these mechanisms, but simply from a higher computational budget
spent for TroVE compared to PRIMITIVE. To this end, we also perform a small
correction in the original implementation of TroVE's selection mechanism,
boosting TroVE's performance on MATH by 3\% in accuracy. After matching for
compute, the benefit of TroVE reduces to a marginal improvement of 1\%,
suggesting that this toolbox approach does not provide a significant benefit on
MATH.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Minimizing CGYRO HPC Communication Costs in Ensembles with XGYRO by Sharing the Collisional Constant Tensor Structure](https://arxiv.org/abs/2507.22245)
*Igor Sfiligoi,Emily A. Belli,Jeff Candy*

Main category: cs.DC

TL;DR: XGYRO工具通过将多个CGYRO模拟作为一个整体HPC任务执行，优化内存和通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决CGYRO模拟中因内存和通信开销高而导致的效率问题。

Method: 开发XGYRO工具，将模拟集合作为单一任务处理，共享数据结构以减少内存占用。

Result: 显著降低内存消耗和通信开销。

Conclusion: XGYRO通过集合优化提升了模拟效率。

Abstract: First-principles fusion plasma simulations are both compute and memory
intensive, and CGYRO is no exception. The use of many HPC nodes to fit the
problem in the available memory thus results in significant communication
overhead, which is hard to avoid for any single simulation. That said, most
fusion studies are composed of ensembles of simulations, so we developed a new
tool, named XGYRO, that executes a whole ensemble of CGYRO simulations as a
single HPC job. By treating the ensemble as a unit, XGYRO can alter the global
buffer distribution logic and apply optimizations that are not feasible on any
single simulation, but only on the ensemble as a whole. The main saving comes
from the sharing of the collisional constant tensor structure, since its values
are typically identical between parameter-sweep simulations. This data
structure dominates the memory consumption of CGYRO simulations, so
distributing it among the whole ensemble results in drastic memory savings for
each simulation, which in turn results in overall lower communication overhead.

</details>


### [3] [Towards Experiment Execution in Support of Community Benchmark Workflows for HPC](https://arxiv.org/abs/2507.22294)
*Gregor von Laszewski,Wesley Brewer,Sean R. Wilkinson,Andrew Shao,J. P. Fleischer,Harshad Pitkar,Christine R. Kirkpatrick,Geoffrey C. Fox*

Main category: cs.DC

TL;DR: 提出工作流模板作为解决计算资源能力展示问题的方案，并通过实验管理工具验证其适应性。


<details>
  <summary>Details</summary>
Motivation: 解决在有限基准测试下展示计算资源能力的难题。

Method: 提出工作流模板，结合HPC经验总结常见使用模式，并通过实验管理工具验证。

Result: 验证了工作流模板的适应性，特别是在教育领域，并通过两个独立工具（Cloudmesh和SmartSim）测试了多种科学应用。

Conclusion: 工作流模板和实验管理工具（如基准木工）能有效提升计算资源的适应性和应用范围。

Abstract: A key hurdle is demonstrating compute resource capability with limited
benchmarks. We propose workflow templates as a solution, offering adaptable
designs for specific scientific applications. Our paper identifies common usage
patterns for these templates, drawn from decades of HPC experience, including
recent work with the MLCommons Science working group.
  We found that focusing on simple experiment management tools within the
broader computational workflow improves adaptability, especially in education.
This concept, which we term benchmark carpentry, is validated by two
independent tools: Cloudmesh's Experiment Executor and Hewlett Packard
Enterprise's SmartSim. Both frameworks, with significant functional overlap,
have been tested across various scientific applications, including conduction
cloudmask, earthquake prediction, simulation-AI/ML interactions, and the
development of computational fluid dynamics surrogates.

</details>


### [4] [A Semi-Supervised Federated Learning Framework with Hierarchical Clustering Aggregation for Heterogeneous Satellite Networks](https://arxiv.org/abs/2507.22339)
*Zhuocheng Liu,Zhishu Shen,Qiushi Zheng,Tiehua Zhang,Zheng Lei,Jiong Jin*

Main category: cs.DC

TL;DR: 论文提出了一种针对LEO卫星网络的半监督联邦学习框架，通过分层聚类聚合和通信优化技术，显著减少了处理时间和能耗。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络资源受限且动态性强，传统联邦学习难以实现可靠收敛和高效能耗管理。

Method: 结合半监督学习、分层聚类聚合、稀疏化和自适应权重量化技术，分两阶段进行联邦学习聚类。

Result: 实验表明，该方法处理时间减少3倍，能耗降低4倍，同时保持模型准确性。

Conclusion: 该框架为LEO卫星网络中的分布式智能提供了高效解决方案。

Abstract: Low Earth Orbit (LEO) satellites are emerging as key components of 6G
networks, with many already deployed to support large-scale Earth observation
and sensing related tasks. Federated Learning (FL) presents a promising
paradigm for enabling distributed intelligence in these resource-constrained
and dynamic environments. However, achieving reliable convergence, while
minimizing both processing time and energy consumption, remains a substantial
challenge, particularly in heterogeneous and partially unlabeled satellite
networks. To address this challenge, we propose a novel semi-supervised
federated learning framework tailored for LEO satellite networks with
hierarchical clustering aggregation. To further reduce communication overhead,
we integrate sparsification and adaptive weight quantization techniques. In
addition, we divide the FL clustering into two stages: satellite cluster
aggregation stage and Ground Stations (GSs) aggregation stage. The supervised
learning at GSs guides selected Parameter Server (PS) satellites, which in turn
support fully unlabeled satellites during the federated training process.
Extensive experiments conducted on a satellite network testbed demonstrate that
our proposal can significantly reduce processing time (up to 3x) and energy
consumption (up to 4x) compared to other comparative methods while maintaining
model accuracy.

</details>


### [5] [Leveraging Caliper and Benchpark to Analyze MPI Communication Patterns: Insights from AMG2023, Kripke, and Laghos](https://arxiv.org/abs/2507.22372)
*Grace Nansamba,Evelyn Namugwanya,David Boehme,Dewi Yokelson,Riley Shipley,Derek Schafer,Michael McKinsey,Olga Pearce,Anthony Skjellum*

Main category: cs.DC

TL;DR: 在Caliper HPC分析工具中引入“通信区域”以捕获通信数据和MPI进程的详细指标，通过三个应用验证其有效性，并展示新的可视化方法和通信瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有Caliper工具无法捕获通信数据和MPI进程的详细指标，限制了性能分析的深度。

Method: 在Caliper中引入“通信区域”功能，结合Thicket工具生成新的可视化，分析三个代表性应用的通信行为。

Result: 揭示了通信瓶颈和详细行为，展示了CPU和GPU系统的扩展性差异。

Conclusion: 通信区域的引入显著提升了Caliper的实用性，为性能分析提供了更深入的见解。

Abstract: We introduce ``communication regions'' into the widely used Caliper HPC
profiling tool. A communication region is an annotation enabling capture of
metrics about the data being communicated (including statistics of these
metrics), and metrics about the MPI processes involved in the communications,
something not previously possible in Caliper. We explore the utility of
communication regions with three representative modeling and simulation
applications, AMG2023, Kripke, and Laghos, all part of the comprehensive
Benchpark suite that includes Caliper annotations. Enhanced Caliper reveals
detailed communication behaviors. Using Caliper and Thicket in tandem, we
create new visualizations of MPI communication patterns, including halo
exchanges. Our findings reveal communication bottlenecks and detailed
behaviors, indicating significant utility of the special-regions addition to
Caliper. The comparative scaling behavior of both CPU and GPU oriented systems
are shown; we are able to look at different regions within a given application,
and see how scalability and message-traffic metrics differ.

</details>


### [6] [DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic Space Partitioning with Erasure Code](https://arxiv.org/abs/2507.22801)
*Shubhradeep Roy,Suvarthi Sarkar,Vivek Verma,Aryabartta Sahu*

Main category: cs.DC

TL;DR: 提出了一种基于利润驱动的边缘存储系统框架，结合协作缓存、纠删码和弹性存储分区，优化动态工作负载下的数据访问。


<details>
  <summary>Details</summary>
Motivation: 边缘服务器存储容量有限，难以处理高容量和低延迟的数据访问请求，尤其是在动态工作负载下。

Method: 动态分区边缘服务器存储为私有和公共区域，私有区域进一步细分以控制数据局部性和所有权，设计数据放置和替换策略以最大化数据访问。

Result: 在Netflix和Spotify的真实数据上测试，系统利润提高了5%至8%。

Conclusion: 提出的动态空间分区和弹性缓存策略在动态工作负载下显著提升了系统利润。

Abstract: Edge Storage Systems have emerged as a critical enabler of low latency data
access in modern cloud networks by bringing storage and computation closer to
end users. However, the limited storage capacity of edge servers poses
significant challenges in handling high volume and latency sensitive data
access requests, particularly under dynamic workloads. In this work, we propose
a profit driven framework that integrates three key mechanisms which are
collaborative caching, erasure coding, and elastic storage partitioning. Unlike
traditional replication, erasure coding enables space efficient redundancy,
allowing data to be reconstructed from any subset of K out of K plus M coded
blocks. We dynamically partition each edge server s storage into private and
public regions. The private region is further subdivided among access points
based on their incoming request rates, enabling adaptive control over data
locality and ownership. We design a data placement and replacement policy that
determines how and where to store or evict coded data blocks to maximize data
access within deadlines. While the private region serves requests from local
APs, the public region handles cooperative storage requests from neighboring
servers. Our proposed Dynamic Space Partitioning and Elastic caching strategy
is evaluated on both synthetic and real world traces from Netflix and Spotify.
Experimental results show that our method improves overall system profitability
by approximately 5 to 8% compared to state of the art approaches under varied
workload conditions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [A Customized Memory-aware Architecture for Biological Sequence Alignment](https://arxiv.org/abs/2507.22221)
*Nasrin Akbari,Mehdi Modarressi,Alireza Khadem*

Main category: cs.AR

TL;DR: 论文提出了一种内存感知架构，用于减少序列比对算法的带宽需求，并通过3D DRAM中的处理内存架构提升性能，实验结果显示速度提升2.4倍，功耗降低37%。


<details>
  <summary>Details</summary>
Motivation: 随着生物信息学数据量的指数增长，序列比对算法的时间和资源需求增加，传统并行机器无法充分利用其并行性。

Method: 提出内存感知架构以减少带宽需求，并在3D DRAM的逻辑层集成处理内存架构。

Result: 实验结果显示速度提升2.4倍，功耗平均降低37%。

Conclusion: 通过将计算移至内存附近，有效解决了带宽瓶颈，提升了性能和能效。

Abstract: Sequence alignment is a fundamental process in computational biology which
identifies regions of similarity in biological sequences. With the exponential
growth in the volume of data in bioinformatics databases, the time, processing
power, and memory bandwidth for comparing a query sequence with the available
databases grows proportionally. The sequence alignment algorithms often involve
simple arithmetic operations and feature high degrees of inherent fine-grained
and coarse-grained parallelism. These features can be potentially exploited by
a massive parallel processor, such as a GPU, to increase throughput. In this
paper, we show that the excessive memory bandwidth demand of the sequence
alignment algorithms prevents exploiting the maximum achievable throughput on
conventional parallel machines. We then propose a memory-aware architecture to
reduce the bandwidth demand of the sequence alignment algorithms, effectively
pushing the memory wall to extract higher throughput. The design is integrated
at the logic layer of an emerging 3D DRAM as a processing-in-memory
architecture to further increase the available bandwidth. The experimental
results show that the proposed architecture results in up to 2.4x speedup over
a GPU-based design. Moreover, by moving the computation closer to the memory,
power consumption is reduced by 37%, on average.

</details>
