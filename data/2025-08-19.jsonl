{"id": "2508.11665", "categories": ["cs.PL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.11665", "abs": "https://arxiv.org/abs/2508.11665", "authors": ["Xinkui Zhao", "Yifan Zhang", "Zhengyi Zhou", "Yueshen Xu"], "title": "StackPilot: Autonomous Function Agents for Scalable and Environment-Free Code Execution", "comment": null, "summary": "Recent advances in large language models (LLMs) have substantially enhanced\nautomated code generation across a wide range of programming languages.\nNonetheless, verifying the correctness and executability of LLM-generated code\nremains a significant challenge, as traditional methods rely on\nlanguage-specific compilers and environment-dependent runtimes. To overcome\nthese limitations, we introduce StackPilot, an LLM-native, multi-agent\nframework designed for language-agnostic code verification and execution, which\noperates independently of conventional toolchains. StackPilot offers three\nprincipal innovations: (1) a Function-as-Agents paradigm, in which each\nfunction is modeled as an autonomous agent capable of fine-grained reasoning\nand collaborative verification; (2) an LLM-as-Executor strategy, which enables\nscalable verification via stack-based scheduling; and (3) a novel snapshot\nmechanism that preserves complete execution contexts, facilitating\ndeterministic and lossless context switching during verification. Empirical\nevaluations demonstrate that StackPilot achieves framework reliability rates\nbetween 89% and 97%, substantially outperforming baseline approaches. These\nresults indicate that StackPilot can reliably verify and execute a\nsignificantly larger proportion of LLM-generated code across diverse\nprogramming tasks compared to existing methods."}
{"id": "2508.12054", "categories": ["cs.PL", "11A51", "D.3.1"], "pdf": "https://arxiv.org/pdf/2508.12054", "abs": "https://arxiv.org/abs/2508.12054", "authors": ["Guilherme de Oliveira Silva", "Fernando Magno Quintão Pereira"], "title": "Certified Compilation based on Gödel Numbers", "comment": "32 pages, 19 figures", "summary": "In his 1984 Turing Award lecture, Ken Thompson showed that a compiler could\nbe maliciously altered to insert backdoors into programs it compiles and\nperpetuate this behavior by modifying any compiler it subsequently builds.\nThompson's hack has been reproduced in real-world systems for demonstration\npurposes. Several countermeasures have been proposed to defend against\nThompson-style backdoors, including the well-known {\\it Diverse\nDouble-Compiling} (DDC) technique, as well as methods like translation\nvalidation and CompCert-style compilation. However, these approaches ultimately\ncircle back to the fundamental question: \"How can we trust the compiler used to\ncompile the tools we rely on?\" In this paper, we introduce a novel approach to\ngenerating certificates to guarantee that a binary image faithfully represents\nthe source code. These certificates ensure that the binary contains all and\nonly the statements from the source code, preserves their order, and maintains\nequivalent def-use dependencies. The certificate is represented as an integer\nderivable from both the source code and the binary using a concise set of\nderivation rules, each applied in constant time. To demonstrate the\npracticality of our method, we present Charon, a compiler designed to handle a\nsubset of C expressive enough to compile FaCT, the Flexible and Constant Time\ncryptographic programming language."}
{"id": "2508.12427", "categories": ["cs.PL", "D.3.3; F.3.2; F.3.3"], "pdf": "https://arxiv.org/pdf/2508.12427", "abs": "https://arxiv.org/abs/2508.12427", "authors": ["Paul Downen"], "title": "Controlling Copatterns: There and Back Again (Extended Version)", "comment": "To find the detailed step-by-step process, which serves as their\n  proof of correctness, see https://github.com/pdownen/derive-copat", "summary": "Copatterns give functional programs a flexible mechanism for responding to\ntheir context, and composition can greatly enhance their expressiveness.\nHowever, that same expressive power makes it harder to precisely specify the\nbehavior of programs. Using Danvy's functional and syntactic correspondence\nbetween different semantic artifacts, we derive a full suite of semantics for\ncopatterns, twice. First, a calculus of monolithic copatterns is taken on a\njourney from small-step operational semantics to abstract machine to\ncontinuation-passing style. Then within continuation-passing style, we refactor\nthe semantics to derive a more general calculus of compositional copatterns,\nand take the return journey back to derive the other semantic artifacts in\nreverse order."}
{"id": "2508.12475", "categories": ["cs.PL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2508.12475", "abs": "https://arxiv.org/abs/2508.12475", "authors": ["Abhijit Paul"], "title": "Type-Driven Prompt Programming: From Typed Interfaces to a Calculus of Constraints", "comment": "Accepted as Extended Abstract in TyDe Workshop 2025,co-located with\n  ICFP", "summary": "Prompt programming treats large language model prompts as software components\nwith typed interfaces. Based on a literature survey of 15 recent works from\n2023 to 2025, we observe a consistent trend: type systems are central to\nemerging prompt programming frameworks. However, there are gaps in constraint\nexpressiveness and in supporting algorithms. To address these issues, we\nintroduce the notion of Lambda Prompt, a dependently typed calculus with\nprobabilistic refinements for syntactic and semantic constraints. While this is\nnot yet a full calculus, the formulation motivates a type-theoretic foundation\nfor prompt programming. Our catalog of 13 constraints highlights underexplored\nareas in constraint expressiveness (constraints 9 through 13). To address the\nalgorithmic gap, we propose a constraint-preserving optimization rule. Finally,\nwe outline research directions on developing a compiler for prompt programs."}
{"id": "2508.12308", "categories": ["cs.DC", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.12308", "abs": "https://arxiv.org/abs/2508.12308", "authors": ["Clément Aubert", "Cinzia Di Giusto", "Simon Fowler", "Violet Ka I Pun"], "title": "Proceedings 18th Interaction and Concurrency Experience", "comment": null, "summary": "This volume contains the proceedings of ICE'25, the 18th Interaction and\nConcurrency Experience, which was held on Friday 20th June 2025 at the \\'Ecole\nNational Sup\\'erieure des Arts et M\\'etiers in Lille, France, as a satellite\nworkshop of DisCoTec 2025. The ICE workshop series features a distinguishing\nreview and selection procedure: PC members are encouraged to interact,\nanonymously, with authors. The 2025 edition of ICE received 7 submissions, each\nreviewed by three PC members, and about 75 comments were exchanged during the\nreview process, witnessing very lively discussions. Four papers were accepted\nfor publication plus 1 oral communication, which was accepted for presentation\nat the workshop. We were proud to host one invited talk, by Kirstin Peters. The\nabstract of her talk is included in this volume, together with the final\nversions of the research papers, which take into account the discussion at the\nworkshop and during the review process."}
{"id": "2508.11935", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11935", "abs": "https://arxiv.org/abs/2508.11935", "authors": ["Yuannuo Feng", "Wenyong Zhou", "Yuexi Lyu", "Hanjie Liu", "Zhengwu Liu", "Ngai Wong", "Wang Kang"], "title": "HPD: Hybrid Projection Decomposition for Robust State Space Models on Analog CIM Hardware", "comment": "4 pages, 5 figures, conference", "summary": "State Space Models (SSMs) are efficient alternatives to traditional sequence\nmodels, excelling at processing long sequences with lower computational\ncomplexity. Their reliance on matrix multiplications makes them ideal for\ncompute-in-memory (CIM) architectures, which improve energy efficiency by\ncomputing within memory arrays. However, device non-idealities in CIM introduce\nweight perturbations that can degrade inference accuracy. In this paper, we\nsystematically analyze the robustness of SSMs under noisy conditions,\nidentifying that the final block and output projection layers are more\nsusceptible to perturbations compared to other components. Building on these\ninsights, we propose HPD, a Hybrid Projection Decomposition strategy for the\nlast output projection layer. We replace the original weight matrix with the\nmultiplication of U and {\\Sigma} in its SVD to ensure compatibility with\nexisting hardware architectures, while offloading V> to digital hardware for\nprecise and robust correction. Comprehensive tests on Mamba models show that\nour method reduces perplexity by up to 99.57% under various noise conditions\ncompared to baseline models, with accuracy gains of up to 96.67% on the PIQA\nbenchmark for commonsense reasoning."}
{"id": "2508.12386", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.12386", "abs": "https://arxiv.org/abs/2508.12386", "authors": ["Jundong Chen", "Honglei Zhang", "Chunxu Zhang", "Fangyuan Luo", "Yidong Li"], "title": "Breaking the Aggregation Bottleneck in Federated Recommendation: A Personalized Model Merging Approach", "comment": null, "summary": "Federated recommendation (FR) facilitates collaborative training by\naggregating local models from massive devices, enabling client-specific\npersonalization while ensuring privacy. However, we empirically and\ntheoretically demonstrate that server-side aggregation can undermine\nclient-side personalization, leading to suboptimal performance, which we term\nthe aggregation bottleneck. This issue stems from the inherent heterogeneity\nacross numerous clients in FR, which drives the globally aggregated model to\ndeviate from local optima. To this end, we propose FedEM, which elastically\nmerges the global and local models to compensate for impaired personalization.\nUnlike existing personalized federated recommendation (pFR) methods, FedEM (1)\ninvestigates the aggregation bottleneck in FR through theoretical insights,\nrather than relying on heuristic analysis; (2) leverages off-the-shelf local\nmodels rather than designing additional mechanisms to boost personalization.\nExtensive experiments on real-world datasets demonstrate that our method\npreserves client personalization during collaborative training, outperforming\nstate-of-the-art baselines."}
{"id": "2508.12195", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.12195", "abs": "https://arxiv.org/abs/2508.12195", "authors": ["Yifan Qin", "Zheyu Yan", "Wujie Wen", "Xiaobo Sharon Hu", "Yiyu Shi"], "title": "Special Session: Sustainable Deployment of Deep Neural Networks on Non-Volatile Compute-in-Memory Accelerators", "comment": "Published in 2024 International Conference on Hardware/Software\n  Codesign and System Synthesis (CODES+ISSS)", "summary": "Non-volatile memory (NVM) based compute-in-memory (CIM) accelerators have\nemerged as a sustainable solution to significantly boost energy efficiency and\nminimize latency for Deep Neural Networks (DNNs) inference due to their in-situ\ndata processing capabilities. However, the performance of NVCIM accelerators\ndegrades because of the stochastic nature and intrinsic variations of NVM\ndevices. Conventional write-verify operations, which enhance inference accuracy\nthrough iterative writing and verification during deployment, are costly in\nterms of energy and time. Inspired by negative feedback theory, we present a\nnovel negative optimization training mechanism to achieve robust DNN deployment\nfor NVCIM. We develop an Oriented Variational Forward (OVF) training method to\nimplement this mechanism. Experiments show that OVF outperforms existing\nstate-of-the-art techniques with up to a 46.71% improvement in inference\naccuracy while reducing epistemic uncertainty. This mechanism reduces the\nreliance on write-verify operations and thus contributes to the sustainable and\npractical deployment of NVCIM accelerators, addressing performance degradation\nwhile maintaining the benefits of sustainable computing with NVCIM\naccelerators."}
{"id": "2508.12671", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12671", "abs": "https://arxiv.org/abs/2508.12671", "authors": ["Dmitry Belousov", "Yury Yanovich"], "title": "DIT: Dimension Reduction View on Optimal NFT Rarity Meters", "comment": null, "summary": "Non-fungible tokens (NFTs) have become a significant digital asset class,\neach uniquely representing virtual entities such as artworks. These tokens are\nstored in collections within smart contracts and are actively traded across\nplatforms on Ethereum, Bitcoin, and Solana blockchains. The value of NFTs is\nclosely tied to their distinctive characteristics that define rarity, leading\nto a growing interest in quantifying rarity within both industry and academia.\nWhile there are existing rarity meters for assessing NFT rarity, comparing them\ncan be challenging without direct access to the underlying collection data. The\nRating over all Rarities (ROAR) benchmark addresses this challenge by providing\na standardized framework for evaluating NFT rarity. This paper explores a\ndimension reduction approach to rarity design, introducing new performance\nmeasures and meters, and evaluates them using the ROAR benchmark. Our\ncontributions to the rarity meter design issue include developing an optimal\nrarity meter design using non-metric weighted multidimensional scaling,\nintroducing Dissimilarity in Trades (DIT) as a performance measure inspired by\ndimension reduction techniques, and unveiling the non-interpretable rarity\nmeter DIT, which demonstrates superior performance compared to existing\nmethods."}
{"id": "2508.12251", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.12251", "abs": "https://arxiv.org/abs/2508.12251", "authors": ["Wenyong Zhou", "Yuan Ren", "Jiajun Zhou", "Tianshu Hou", "Ngai Wong"], "title": "A Time- and Energy-Efficient CNN with Dense Connections on Memristor-Based Chips", "comment": "4 pages, 7 figures", "summary": "Designing lightweight convolutional neural network (CNN) models is an active\nresearch area in edge AI. Compute-in-memory (CIM) provides a new computing\nparadigm to alleviate time and energy consumption caused by data transfer in\nvon Neumann architecture. Among competing alternatives, resistive random-access\nmemory (RRAM) is a promising CIM device owing to its reliability and multi-bit\nprogrammability. However, classical lightweight designs such as depthwise\nconvolution incurs under-utilization of RRAM crossbars restricted by their\ninherently dense weight-to-RRAM cell mapping. To build an RRAM-friendly yet\nefficient CNN, we evaluate the hardware cost of DenseNet which maintains a high\naccuracy vs other CNNs at a small parameter count. Observing the linearly\nincreasing channels in DenseNet leads to a low crossbar utilization and causes\nlarge latency and energy consumption, we propose a scheme that concatenates\nfeature maps of front layers to form the input of the last layer in each stage.\nExperiments show that our proposed model consumes less time and energy than\nconventional ResNet and DenseNet, while producing competitive accuracy on CIFAR\nand ImageNet datasets."}
{"id": "2508.12743", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.12743", "abs": "https://arxiv.org/abs/2508.12743", "authors": ["Jacob Wahlgren", "Gabin Schieffer", "Ruimin Shi", "Edgar A. León", "Roger Pearce", "Maya Gokhale", "Ivy Peng"], "title": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs", "comment": "To be published in IISWC 2025", "summary": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%."}
{"id": "2508.12294", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.12294", "abs": "https://arxiv.org/abs/2508.12294", "authors": ["Qijun Zhang", "Yao Lu", "Mengming Li", "Zhiyao Xie"], "title": "AutoPower: Automated Few-Shot Architecture-Level Power Modeling by Power Group Decoupling", "comment": "Published in DAC'25", "summary": "Power efficiency is a critical design objective in modern CPU design.\nArchitects need a fast yet accurate architecture-level power evaluation tool to\nperform early-stage power estimation. However, traditional analytical\narchitecture-level power models are inaccurate. The recently proposed machine\nlearning (ML)-based architecture-level power model requires sufficient data\nfrom known configurations for training, making it unrealistic.\n  In this work, we propose AutoPower targeting fully automated\narchitecture-level power modeling with limited known design configurations. We\nhave two key observations: (1) The clock and SRAM dominate the power\nconsumption of the processor, and (2) The clock and SRAM power correlate with\nstructural information available at the architecture level. Based on these two\nobservations, we propose the power group decoupling in AutoPower. First,\nAutoPower decouples across power groups to build individual power models for\neach group. Second, AutoPower designs power models by further decoupling the\nmodel into multiple sub-models within each power group. In our experiments,\nAutoPower can achieve a low mean absolute percentage error (MAPE) of 4.36\\% and\na high $R^2$ of 0.96 even with only two known configurations for training. This\nis 5\\% lower in MAPE and 0.09 higher in $R^2$ compared with McPAT-Calib, the\nrepresentative ML-based power model."}
{"id": "2508.12851", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.12851", "abs": "https://arxiv.org/abs/2508.12851", "authors": ["Tian Wu", "Liming Wang", "Zijian Wen", "Xiaoxi Zhang", "Jingpu Duan", "Xianwei Zhang", "Jinhang Zuo"], "title": "Accelerating Edge Inference for Distributed MoE Models with Latency-Optimized Expert Placement", "comment": null, "summary": "Mixture-of-Experts (MoE) have become a cornerstone for training and scaling\nlarge language models (LLMs), offering substantial gains in model capacity and\nefficiency through sparse expert activation. However, serving these models\nremains challenging in practice, particularly in resource-constrained edge\nenvironments, due to their large memory footprint and complex communication\ndemands. While centralized cloud inference is common, it incurs high\ninfrastructure costs, along with latency and privacy concerns. A few recent\nedge MoE works propose memory-efficient strategies but typically focus on\nsingle-device or homogeneous setups. This paper presents DanceMoE, an efficient\nMoE inference framework that enables activation-aware expert placement across\ncollaborative, heterogeneous, GPU-equipped edge servers. DanceMoE leverages the\ninherent sparsity of MoE models and workload locality to minimize cross-server\ncommunication and enable efficient expert placement under heterogeneous\nresource constraints. It introduces a data-driven, activation-aware placement\nalgorithm that balances local coverage and memory usage across servers,\nalongside a lightweight migration mechanism that adapts expert assignments\nunder evolving workloads. We evaluate DanceMoE on modern MoE models and widely\nused datasets, demonstrating up to 30.6\\% lower inference latency, and\nsubstantial communication reduction compared to state-of-the-art baselines,\nshowcasing the effectiveness of collaborative edge-based MoE inference."}
{"id": "2508.12345", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.12345", "abs": "https://arxiv.org/abs/2508.12345", "authors": ["Ali Jockar", "Mohsen Raji"], "title": "Soft Error Probability Estimation of Nano-scale Combinational Circuits", "comment": "6 pages", "summary": "As technology scales, nano-scale digital circuits face heightened\nsusceptibility to single event upsets (SEUs) and transients (SETs) due to\nshrinking feature sizes and reduced operating voltages. While logical,\nelectrical, and timing masking effects influence soft error probability (SEP),\nthe combined impact of process variation (PV) and aging-induced degradation\nfurther complicates SEP estimation. Existing approaches often address PV or\naging in isolation, or rely on computationally intensive methods like Monte\nCarlo simulations, limiting their practicality for large-scale circuit\noptimization. This paper introduces a novel framework for SEP analysis that\nholistically integrates PV and aging effects. We propose an enhanced electrical\nmasking model and a statistical methodology to quantify soft error probability\nunder process and aging variations. Experimental results demonstrate that the\nproposed approach achieves high accuracy while reducing computational overhead\nby approximately 2.5% compared to Monte Carlo-based methods. This work advances\nthe design of reliable nano-scale circuits by enabling efficient, accurate SEP\nestimation in the presence of manufacturing variability and long-term\ntransistor degradation."}
{"id": "2508.12961", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.12961", "abs": "https://arxiv.org/abs/2508.12961", "authors": ["Anshuman Das Mohapatra", "Kwangsung Oh"], "title": "WANify: Gauging and Balancing Runtime WAN Bandwidth for Geo-distributed Data Analytics", "comment": "This paper is accepted for publication at the 2025 IEEE International\n  Symposium on Workload Characterization (IISWC'25)", "summary": "Accurate wide area network (WAN) bandwidth (BW) is essential for\ngeo-distributed data analytics (GDA) systems to make optimal decisions such as\ndata and task placement to improve performance. Existing GDA systems, however,\nmeasure WAN BW statically and independently between data centers (DCs), while\ndata transfer occurs dynamically and simultaneously among DCs during workload\nexecution. Also, they use a single connection WAN BW that cannot capture actual\nWAN capacities between distant DCs. Such inaccurate WAN BWs yield sub-optimal\ndecisions, inflating overall query latency and cost. In this paper, we present\nWANify, a new framework that precisely and dynamically gauges achievable\nruntime WAN BW using a machine learning prediction scheme, decision tree-based\nRandom Forest. This helps GDA systems make better decisions yielding reduced\nlatency and costs including WAN BW monitoring costs. Based on predicted runtime\nWAN BW, WANify determines the optimal number of heterogeneous parallel\nconnections for data transfer among DCs. This approach improves performance\nwithout additional, or even at reduced cost, by fully exploiting available WAN\ncapacities. In addition, WANify considers dynamics like network and workloads,\nand heterogeneity like skewed data, heterogeneous compute resources, and a\nvarying number of DCs while making decisions. The WANify prototype running on\nstate-of-the-art GDA systems is evaluated on AWS with 8 geo-distributed DCs.\nResults show that WANify enhances WAN throughput by balancing between the\nstrongest and weakest WAN links, enabling GDA systems to reduce latency and\ncost by up to 26% and 16% respectively with minimal effort, all while handling\ndynamics and heterogeneity efficiently."}
{"id": "2508.12347", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.12347", "abs": "https://arxiv.org/abs/2508.12347", "authors": ["Mohsen Raji", "Mohammad Zaree", "Kimia Soroush"], "title": "An ECC-based Fault Tolerance Approach for DNNs", "comment": null, "summary": "Deep Neural Network (DNN) has achieve great success in solving a wide range\nof machine learning problems. Recently, they have been deployed in datacenters\n(potentially for business-critical or industrial applications) and\nsafety-critical systems such as self-driving cars. So, their correct\nfunctionality in the presence of potential bit-flip errors on DNN parameters\nstored in memories plays the key role in their applicability in safety-critical\napplications. In this paper, a fault tolerance approach based on Error\nCorrecting Codes (ECC), called SPW, is proposed to ensure the correct\nfunctionality of DNNs in the presence of bit-flip faults. In the proposed\napproach, error occurrence is detected by the stored ECC and then, it is\ncorrect in case of a single-bit error or the weight is completely set to zero\n(i.e. masked) otherwise. A statistical fault injection campaign is proposed and\nutilized to investigate the efficacy of the proposed approach. The experimental\nresults show that the accuracy of the DNN increases by more than 300% in the\npresence with Bit Error Rate of 10^(-1) in comparison to the case where ECC\ntechnique is applied, in expense of just 47.5% area overhead."}
{"id": "2508.13083", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2508.13083", "abs": "https://arxiv.org/abs/2508.13083", "authors": ["Joshua Z. Sobel"], "title": "Congested Clique Counting for Local Gibbs Distributions", "comment": null, "summary": "There are well established reductions between combinatorial sampling and\ncounting problems (Jerrum, Valiant, Vazirani TCS 1986). Building off of a very\nrecent parallel algorithm utilizing this connection (Liu, Yin, Zhang arxiv\n2024), we demonstrate the first approximate counting algorithm in the\nCongestedClique for a wide range of problems. Most interestingly, we present an\nalgorithm for approximating the number of $q$-colorings of a graph within\n$\\epsilon$-multiplicative error, when $q>\\alpha\\Delta$ for any constant\n$\\alpha>2$, in $\\Tilde{O}\\big(\\frac{n^{1/3}}{\\epsilon^2}\\big)$ rounds. More\ngenerally, we achieve a runtime of\n$\\Tilde{O}\\big(\\frac{n^{1/3}}{\\epsilon^2}\\big)$ rounds for approximating the\npartition function of Gibbs distributions defined over graphs when simple\nlocality and fast mixing conditions hold. Gibbs distributions are widely used\nin fields such as machine learning and statistical physics. We obtain our\nresult by providing an algorithm to draw $n$ random samples from a distributed\nMarkov chain in parallel, using similar ideas to triangle counting (Dolev,\nLenzen, Peled DISC 2012) and semiring matrix multiplication (Censor-Hillel,\nKaski, Korhonen, Lenzen, Paz, Suomela PODC 2015). Aside from counting problems,\nthis result may be interesting for other applications requiring a large number\nof samples. In the special case of estimating the partition function of the\nhardcore model, also known as counting weighted independent sets, we can do\neven better and achieve an $\\Tilde{O}\\big(\\frac{1}{\\epsilon^2}\\big)$ round\nalgorithm, when the fugacity $\\lambda \\leq \\frac{\\alpha}{\\Delta-1}$, where\n$\\alpha$ is an arbitrary constant less than $1$."}
{"id": "2508.12433", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.12433", "abs": "https://arxiv.org/abs/2508.12433", "authors": ["Wenkai Li", "Yao Lu", "Wenji Fang", "Jing Wang", "Qijun Zhang", "Zhiyao Xie"], "title": "ATLAS: A Self-Supervised and Cross-Stage Netlist Power Model for Fine-Grained Time-Based Layout Power Analysis", "comment": "Accepted by Design Automation Conference (DAC), 2025", "summary": "Accurate power prediction in VLSI design is crucial for effective power\noptimization, especially as designs get transformed from gate-level netlist to\nlayout stages. However, traditional accurate power simulation requires\ntime-consuming back-end processing and simulation steps, which significantly\nimpede design optimization. To address this, we propose ATLAS, which can\npredict the ultimate time-based layout power for any new design in the\ngate-level netlist. To the best of our knowledge, ATLAS is the first work that\nsupports both time-based power simulation and general cross-design power\nmodeling. It achieves such general time-based power modeling by proposing a new\npre-training and fine-tuning paradigm customized for circuit power. Targeting\ngolden per-cycle layout power from commercial tools, our ATLAS achieves the\nmean absolute percentage error (MAPE) of only 0.58%, 0.45%, and 5.12% for the\nclock tree, register, and combinational power groups, respectively, without any\nlayout information. Overall, the MAPE for the total power of the entire design\nis <1%, and the inference speed of a workload is significantly faster than the\nstandard flow of commercial tools."}
{"id": "2508.13084", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.13084", "abs": "https://arxiv.org/abs/2508.13084", "authors": ["Yuval Emek", "Shay Kutten", "Ido Rafael", "Gadi Taubenfeld"], "title": "Team Formation and Applications", "comment": "An extended abstract of this paper was accepted to DISC 2025", "summary": "A novel long-lived distributed problem, called Team Formation (TF), is\nintroduced together with a message- and time-efficient randomized algorithm.\nThe problem is defined over the asynchronous model with a complete\ncommunication graph, using bounded size messages, where a certain fraction of\nthe nodes may experience a generalized, strictly stronger, version of initial\nfailures. The goal of a TF algorithm is to assemble tokens injected by the\nenvironment, in a distributed manner, into teams of size $\\sigma$, where\n$\\sigma$ is a parameter of the problem.\n  The usefulness of TF is demonstrated by using it to derive efficient\nalgorithms for many distributed problems. Specifically, we show that various\n(one-shot as well as long-lived) distributed problems reduce to TF. This\nincludes well-known (and extensively studied) distributed problems such as\nseveral versions of leader election and threshold detection. For example, we\nare the first to break the linear message complexity bound for asynchronous\nimplicit leader election. We also improve the time complexity of\nmessage-optimal algorithms for asynchronous explicit leader election. Other\ndistributed problems that reduce to TF are new ones, including matching players\nin online gaming platforms, a generalization of gathering, constructing a\nperfect matching in an induced subgraph of the complete graph, quorum sensing\nin message-passing networks, and more. To complement our positive contribution,\nwe establish a tight lower bound on the message complexity of TF algorithms."}
{"id": "2508.12636", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.12636", "abs": "https://arxiv.org/abs/2508.12636", "authors": ["Ansh Chaurasia"], "title": "MemorySim: An RTL-level, timing accurate simulator model for the Chisel ecosystem", "comment": null, "summary": "The rapid growth of AI applications has driven increased demand for\nspecialized AI hardware, highlighting critical opportunities within the memory\nsubsystem, which often serves as a performance bottleneck in high-demand\nworkloads such as large language models (LLMs). Existing high-level memory\nsimulators, such as DRAMSim2 and DRAMSim3, offer timing simulations but\nfrequently compromise on correctness or integration at the register-transfer\nlevel (RTL). We present MemorySim, an RTL-level memory simulator designed to\ndeliver both accurate timing and functional correctness. MemorySim integrates\nseamlessly with existing Chisel and Verilog simulations and is fully compatible\nwith the Chisel/Chipyard ecosystem. This enables users to obtain precise\nperformance and power estimates, supporting downstream evaluation through\nsimulation platforms such as FireSim."}
{"id": "2508.12637", "categories": ["cs.AR", "cs.CV", "cs.ET", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.12637", "abs": "https://arxiv.org/abs/2508.12637", "authors": ["Shankaranarayanan H", "Satyapreet Singh Yadav", "Adithya Krishna", "Ajay Vikram P", "Mahesh Mehendale", "Chetan Singh Thakur"], "title": "HOMI: Ultra-Fast EdgeAI platform for Event Cameras", "comment": null, "summary": "Event cameras offer significant advantages for edge robotics applications due\nto their asynchronous operation and sparse, event-driven output, making them\nwell-suited for tasks requiring fast and efficient closed-loop control, such as\ngesture-based human-robot interaction. Despite this potential, existing event\nprocessing solutions remain limited, often lacking complete end-to-end\nimplementations, exhibiting high latency, and insufficiently exploiting event\ndata sparsity. In this paper, we present HOMI, an ultra-low latency, end-to-end\nedge AI platform comprising a Prophesee IMX636 event sensor chip with an Xilinx\nZynq UltraScale+MPSoC FPGA chip, deploying an in-house developed AI\naccelerator. We have developed hardware-optimized pre-processing pipelines\nsupporting both constant-time and constant-event modes for histogram\naccumulation, linear and exponential time surfaces. Our general-purpose\nimplementation caters to both accuracy-driven and low-latency applications.\nHOMI achieves 94% accuracy on the DVS Gesture dataset as a use case when\nconfigured for high accuracy operation and provides a throughput of 1000 fps\nfor low-latency configuration. The hardware-optimised pipeline maintains a\ncompact memory footprint and utilises only 33% of the available LUT resources\non the FPGA, leaving ample headroom for further latency reduction, model\nparallelisation, multi-task deployments, or integration of more complex\narchitectures."}
{"id": "2508.13049", "categories": ["cs.AR", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.13049", "abs": "https://arxiv.org/abs/2508.13049", "authors": ["Tejas Chaudhari", "Akarsh J.", "Tanushree Dewangan", "Mukul Lokhande", "Santosh Kumar Vishvakarma"], "title": "XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads", "comment": null, "summary": "This work proposes XR-NPE, a high-throughput Mixed-precision SIMD Neural\nProcessing Engine, designed for extended reality (XR) perception workloads like\nvisual inertial odometry (VIO), object classification, and eye gaze extraction.\nXR-NPE is first to support FP4, Posit (4,1), Posit (8,0), and Posit (16,1)\nformats, with layer adaptive hybrid-algorithmic implementation supporting\nultra-low bit precision to significantly reduce memory bandwidth requirements,\nand accompanied by quantization-aware training for minimal accuracy loss. The\nproposed Reconfigurable Mantissa Multiplication and Exponent processing\nCircuitry (RMMEC) reduces dark silicon in the SIMD MAC compute engine, assisted\nby selective power gating to reduce energy consumption, providing 2.85x\nimproved arithmetic intensity. XR-NPE achieves a maximum operating frequency of\n1.72 GHz, area 0.016 mm2 , and arithmetic intensity 14 pJ at CMOS 28nm,\nreducing 42% area, 38% power compared to the best of state-of-the-art MAC\napproaches. The proposed XR-NPE based AXI-enabled Matrix-multiplication\nco-processor consumes 1.4x fewer LUTs, 1.77x fewer FFs, and provides 1.2x\nbetter energy efficiency compared to SoTA accelerators on VCU129. The proposed\nco-processor provides 23% better energy efficiency and 4% better compute\ndensity for VIO workloads. XR-NPE establishes itself as a scalable,\nprecision-adaptive compute engine for future resource-constrained XR devices.\nThe complete set for codes for results reproducibility are released publicly,\nenabling designers and researchers to readily adopt and build upon them.\nhttps://github.com/mukullokhande99/XR-NPE."}
