{"id": "2507.17233", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.17233", "abs": "https://arxiv.org/abs/2507.17233", "authors": ["Marco Ciccal\u00e8", "Daniel Jurjo-Rivas", "Jose F. Morales", "Pedro L\u00f3pez-Garc\u00eda", "Manuel V. Hermenegildo"], "title": "Hiord: An Approach to the Specification and Verification of Higher-Order (C)LP Programs", "comment": "Accepted for publication in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "Higher-order constructs enable more expressive and concise code by allowing\nprocedures to be parameterized by other procedures. Assertions allow expressing\npartial program specifications, which can be verified either at compile time\n(statically) or run time (dynamically). In higher-order programs, assertions\ncan also describe higher-order arguments. While in the context of (C)LP,\nrun-time verification of higher-order assertions has received some attention,\ncompile-time verification remains relatively unexplored. We propose a novel\napproach for statically verifying higher-order (C)LP programs with higher-order\nassertions. Although we use the Ciao assertion language for illustration, our\napproach is quite general and we believe is applicable to similar contexts.\nHigher-order arguments are described using predicate properties -- a special\nkind of property which exploits the (Ciao) assertion language. We refine the\nsyntax and semantics of these properties and introduce an abstract criterion to\ndetermine conformance to a predicate property at compile time, based on a\nsemantic order relation comparing the predicate property with the predicate\nassertions. We then show how to handle these properties using an abstract\ninterpretation-based static analyzer for programs with first-order assertions\nby reducing predicate properties to first-order properties. Finally, we report\non a prototype implementation and evaluate it through various examples within\nthe Ciao system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9759\u6001\u9a8c\u8bc1\u9ad8\u9636(C)LP\u7a0b\u5e8f\u548c\u9ad8\u9636\u65ad\u8a00\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c13\u8bcd\u5c5e\u6027\u63cf\u8ff0\u9ad8\u9636\u53c2\u6570\uff0c\u5e76\u5c06\u5176\u7b80\u5316\u4e3a\u4e00\u9636\u5c5e\u6027\u6765\u8fdb\u884c\u7f16\u8bd1\u65f6\u9a8c\u8bc1\u3002", "motivation": "\u9ad8\u9636\u7a0b\u5e8f\u6784\u9020\u53ef\u4ee5\u8ba9\u4ee3\u7801\u66f4\u5177\u8868\u8fbe\u529b\u548c\u7b80\u6d01\u6027\uff0c\u4f46\u5728\u7ea6\u675f\u903b\u8f91\u7f16\u7a0b(CLP)\u4e2d\uff0c\u9ad8\u9636\u65ad\u8a00\u7684\u7f16\u8bd1\u65f6\u9a8c\u8bc1\u76f8\u5bf9\u7f3a\u4e4f\u7814\u7a76\u3002\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8fd0\u884c\u65f6\u9a8c\u8bc1\uff0c\u800c\u9759\u6001\u9a8c\u8bc1\u9ad8\u9636\u7a0b\u5e8f\u548c\u9ad8\u9636\u65ad\u8a00\u7684\u65b9\u6cd5\u4ecd\u7136\u4e0d\u591f\u6210\u719f\u3002", "method": "\u4f7f\u7528Ciao\u65ad\u8a00\u8bed\u8a00\uff0c\u63d0\u51fa\u8c13\u8bcd\u5c5e\u6027\u6765\u63cf\u8ff0\u9ad8\u9636\u53c2\u6570\uff1b\u5b8c\u5584\u8c13\u8bcd\u5c5e\u6027\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\uff1b\u5f15\u5165\u57fa\u4e8e\u8bed\u4e49\u987a\u5e8f\u5173\u7cfb\u7684\u62bd\u8c61\u51c6\u5219\u6765\u786e\u5b9a\u7f16\u8bd1\u65f6\u7684\u8c13\u8bcd\u5c5e\u6027\u4e00\u81f4\u6027\uff1b\u901a\u8fc7\u5c06\u8c13\u8bcd\u5c5e\u6027\u7b80\u5316\u4e3a\u4e00\u9636\u5c5e\u6027\uff0c\u4f7f\u5f97\u73b0\u6709\u7684\u57fa\u4e8e\u62bd\u8c61\u89e3\u91ca\u7684\u9759\u6001\u5206\u6790\u5668\u80fd\u591f\u5904\u7406\u9ad8\u9636\u65ad\u8a00\u3002", "result": "\u5f00\u53d1\u4e86\u539f\u578b\u5b9e\u73b0\u5e76\u5728Ciao\u7cfb\u7edf\u4e2d\u901a\u8fc7\u591a\u4e2a\u793a\u4f8b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5c06\u9ad8\u9636\u65ad\u8a00\u9a8c\u8bc1\u95ee\u9898\u8f6c\u5316\u4e3a\u5df2\u6709\u7684\u4e00\u9636\u65ad\u8a00\u5206\u6790\u6846\u67b6\u53ef\u4ee5\u5904\u7406\u7684\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u9636(C)LP\u7a0b\u5e8f\u7684\u9759\u6001\u9a8c\u8bc1\uff0c\u901a\u8fc7\u8c13\u8bcd\u5c5e\u6027\u548c\u8bed\u4e49\u987a\u5e8f\u5173\u7cfb\u5efa\u7acb\u4e86\u7f16\u8bd1\u65f6\u9a8c\u8bc1\u9ad8\u9636\u65ad\u8a00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u901a\u8fc7\u7b80\u5316\u4e3a\u4e00\u9636\u5c5e\u6027\u7684\u65b9\u5f0f\u4f7f\u5176\u80fd\u591f\u5728\u73b0\u6709\u5206\u6790\u6846\u67b6\u4e2d\u5b9e\u73b0\uff0c\u4e3a\u9ad8\u9636\u7a0b\u5e8f\u7684\u9759\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17087", "categories": ["cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.17087", "abs": "https://arxiv.org/abs/2507.17087", "authors": ["Anjiang Wei", "Rohan Yadav", "Hang Song", "Wonchan Lee", "Ke Wang", "Alex Aiken"], "title": "Mapple: A Domain-Specific Language for Mapping Distributed Heterogeneous Parallel Programs", "comment": null, "summary": "Optimizing parallel programs for distributed heterogeneous systems remains a\ncomplex task, often requiring significant code modifications. Task-based\nprogramming systems improve modularity by separating performance decisions from\ncore application logic, but their mapping interfaces are often too low-level.\nIn this work, we introduce Mapple, a high-level, declarative programming\ninterface for mapping distributed applications. Mapple provides transformation\nprimitives to resolve dimensionality mismatches between iteration and processor\nspaces, including a key primitive, decompose, that helps minimize communication\nvolume. We implement Mapple on top of the Legion runtime by translating Mapple\nmappers into its low-level C++ interface. Across nine applications, including\nsix matrix multiplication algorithms and three scientific computing workloads,\nMapple reduces mapper code size by 14X and enables performance improvements of\nup to 1.34X over expert-written C++ mappers. In addition, the decompose\nprimitive achieves up to 1.83X improvement over existing\ndimensionality-resolution heuristics. These results demonstrate that Mapple\nsimplifies the development of high-performance mappers for distributed\napplications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Mapple\uff0c\u4e00\u4e2a\u7528\u4e8e\u5206\u5e03\u5f0f\u5e94\u7528\u6620\u5c04\u7684\u9ad8\u7ea7\u58f0\u660e\u5f0f\u7f16\u7a0b\u63a5\u53e3\uff0c\u901a\u8fc7\u63d0\u4f9b\u8f6c\u6362\u539f\u8bed\uff08\u7279\u522b\u662fdecompose\u539f\u8bed\uff09\u6765\u89e3\u51b3\u8fed\u4ee3\u7a7a\u95f4\u548c\u5904\u7406\u5668\u7a7a\u95f4\u4e4b\u95f4\u7684\u7ef4\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u7b80\u5316\u4e86mapper\u4ee3\u7801\u5f00\u53d1\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5e03\u5f0f\u5f02\u6784\u7cfb\u7edf\u5e76\u884c\u7a0b\u5e8f\u4f18\u5316\u590d\u6742\u4e14\u9700\u8981\u5927\u91cf\u4ee3\u7801\u4fee\u6539\uff0c\u57fa\u4e8e\u4efb\u52a1\u7684\u7f16\u7a0b\u7cfb\u7edf\u867d\u7136\u63d0\u9ad8\u4e86\u6a21\u5757\u5316\u7a0b\u5ea6\uff0c\u4f46\u5176\u6620\u5c04\u63a5\u53e3\u5f80\u5f80\u8fc7\u4e8e\u5e95\u5c42\uff0c\u7f3a\u4e4f\u9ad8\u7ea7\u62bd\u8c61\uff0c\u5bfc\u81f4\u5f00\u53d1\u9ad8\u6027\u80fdmapper\u56f0\u96be\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86Mapple\uff0c\u4e00\u4e2a\u9ad8\u7ea7\u58f0\u660e\u5f0f\u7f16\u7a0b\u63a5\u53e3\uff0c\u63d0\u4f9b\u8f6c\u6362\u539f\u8bed\u6765\u89e3\u51b3\u8fed\u4ee3\u7a7a\u95f4\u548c\u5904\u7406\u5668\u7a7a\u95f4\u7684\u7ef4\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u7279\u522b\u662fdecompose\u539f\u8bed\u7528\u4e8e\u6700\u5c0f\u5316\u901a\u4fe1\u91cf\u3002\u5728Legion\u8fd0\u884c\u65f6\u4e0a\u5b9e\u73b0Mapple\uff0c\u5c06Mapple\u6620\u5c04\u5668\u8f6c\u6362\u4e3a\u5e95\u5c42C++\u63a5\u53e3\u3002", "result": "\u5728\u4e5d\u4e2a\u5e94\u7528\uff08\u5305\u62ec\u516d\u79cd\u77e9\u9635\u4e58\u6cd5\u7b97\u6cd5\u548c\u4e09\u4e2a\u79d1\u5b66\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMapple\u5c06mapper\u4ee3\u7801\u5927\u5c0f\u51cf\u5c11\u4e8614\u500d\uff0c\u76f8\u6bd4\u4e13\u5bb6\u7f16\u5199\u7684C++ mapper\u5b9e\u73b0\u4e86\u9ad8\u8fbe1.34\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002decompose\u539f\u8bed\u76f8\u6bd4\u73b0\u6709\u7ef4\u5ea6\u89e3\u51b3\u542f\u53d1\u5f0f\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe1.83\u500d\u7684\u6539\u8fdb\u3002", "conclusion": "Mapple\u901a\u8fc7\u63d0\u4f9b\u9ad8\u7ea7\u58f0\u660e\u5f0f\u63a5\u53e3\u548c\u6709\u6548\u7684\u8f6c\u6362\u539f\u8bed\uff0c\u6210\u529f\u7b80\u5316\u4e86\u5206\u5e03\u5f0f\u5e94\u7528\u9ad8\u6027\u80fdmapper\u7684\u5f00\u53d1\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u9ad8\u7ea7\u62bd\u8c61\u5728\u5e76\u884c\u7f16\u7a0b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.17094", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.17094", "abs": "https://arxiv.org/abs/2507.17094", "authors": ["Sukjin Kim", "Seongyeon Park", "Si Ung Noh", "Junguk Hong", "Taehee Kwon", "Hunseong Lim", "Jinho Lee"], "title": "PathWeaver: A High-Throughput Multi-GPU System for Graph-Based Approximate Nearest Neighbor Search", "comment": "ATC 2025", "summary": "Graph-based Approximate Nearest Neighbor Search (ANNS) is widely adopted in\nnumerous applications, such as recommendation systems, natural language\nprocessing, and computer vision. While recent works on GPU-based acceleration\nhave significantly advanced ANNS performance, the ever-growing scale of\ndatasets now demands efficient multi-GPU solutions. However, the design of\nexisting works overlooks multi-GPU scalability, resulting in naive approaches\nthat treat additional GPUs as a means to extend memory capacity for large\ndatasets. This inefficiency arises from partitioning the dataset and\nindependently searching for data points similar to the queries in each GPU. We\ntherefore propose PathWeaver, a novel multi-GPU framework designed to scale and\naccelerate ANNS for large datasets. First, we propose pipelining-based path\nextension, a GPU-aware pipelining mechanism that reduces prior work's redundant\nsearch iterations by leveraging GPU-to-GPU communication. Second, we design\nghost staging that leverages a representative dataset to identify optimal query\nstarting points, reducing the search space for challenging queries. Finally, we\nintroduce direction-guided selection, a data selection technique that filters\nirrelevant points early in the search process, minimizing unnecessary memory\naccesses and distance computations. Comprehensive evaluations across diverse\ndatasets demonstrate that PathWeaver achieves 3.24$\\times$ geomean speedup and\nup to 5.30$\\times$ speedup on 95% recall rate over state-of-the-art\nmulti-GPU-based ANNS frameworks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPathWeaver\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u591aGPU\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u6c34\u7ebf\u8def\u5f84\u6269\u5c55\u3001\u5e7d\u7075\u6682\u5b58\u548c\u65b9\u5411\u5f15\u5bfc\u9009\u62e9\u4e09\u9879\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u57fa\u4e8e\u56fe\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u591aGPU\u65b9\u6848\u5b9e\u73b03.24\u500d\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u548c\u6700\u9ad85.30\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709GPU\u52a0\u901f\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u65b9\u6cd5\u5728\u8bbe\u8ba1\u65f6\u5ffd\u7565\u4e86\u591aGPU\u53ef\u6269\u5c55\u6027\uff0c\u4ec5\u5c06\u989d\u5916GPU\u89c6\u4e3a\u6269\u5c55\u5185\u5b58\u5bb9\u91cf\u7684\u624b\u6bb5\uff0c\u91c7\u7528\u6734\u7d20\u7684\u6570\u636e\u96c6\u5206\u5272\u65b9\u6cd5\u5728\u5404GPU\u4e0a\u72ec\u7acb\u641c\u7d22\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u968f\u7740\u6570\u636e\u96c6\u89c4\u6a21\u4e0d\u65ad\u589e\u957f\uff0c\u8feb\u5207\u9700\u8981\u9ad8\u6548\u7684\u591aGPU\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPathWeaver\u591aGPU\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6280\u672f\uff1a1\uff09\u57fa\u4e8e\u6d41\u6c34\u7ebf\u7684\u8def\u5f84\u6269\u5c55-\u5229\u7528GPU\u95f4\u901a\u4fe1\u51cf\u5c11\u5197\u4f59\u641c\u7d22\u8fed\u4ee3\u7684GPU\u611f\u77e5\u6d41\u6c34\u7ebf\u673a\u5236\uff1b2\uff09\u5e7d\u7075\u6682\u5b58-\u5229\u7528\u4ee3\u8868\u6027\u6570\u636e\u96c6\u8bc6\u522b\u6700\u4f18\u67e5\u8be2\u8d77\u59cb\u70b9\uff0c\u51cf\u5c11\u56f0\u96be\u67e5\u8be2\u7684\u641c\u7d22\u7a7a\u95f4\uff1b3\uff09\u65b9\u5411\u5f15\u5bfc\u9009\u62e9-\u5728\u641c\u7d22\u8fc7\u7a0b\u65e9\u671f\u8fc7\u6ee4\u65e0\u5173\u70b9\u7684\u6570\u636e\u9009\u62e9\u6280\u672f\uff0c\u6700\u5c0f\u5316\u4e0d\u5fc5\u8981\u7684\u5185\u5b58\u8bbf\u95ee\u548c\u8ddd\u79bb\u8ba1\u7b97\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0cPathWeaver\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u591aGPU\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u6846\u67b6\u5b9e\u73b0\u4e863.24\u500d\u7684\u51e0\u4f55\u5e73\u5747\u52a0\u901f\uff0c\u572895%\u53ec\u56de\u7387\u4e0b\u6700\u9ad8\u8fbe\u52305.30\u500d\u52a0\u901f\u3002", "conclusion": "PathWeaver\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u591aGPU\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u9879\u521b\u65b0\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u641c\u7d22\u6027\u80fd\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u5e94\u7528\u4e2d\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u591aGPU\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17120", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17120", "abs": "https://arxiv.org/abs/2507.17120", "authors": ["Wanyi Zheng", "Minxian Xu", "Shengye Song", "Kejiang Ye"], "title": "BucketServe: Bucket-Based Dynamic Batching for Smart and Efficient LLM Inference Serving", "comment": "9 pages", "summary": "Large language models (LLMs) have become increasingly popular in various\nareas, traditional business gradually shifting from rule-based systems to\nLLM-based solutions. However, the inference of LLMs is resource-intensive or\nlatency-sensitive, posing significant challenges for serving systems. Existing\nLLM serving systems often use static or continuous batching strategies, which\ncan lead to inefficient GPU memory utilization and increased latency,\nespecially under heterogeneous workloads. These methods may also struggle to\nadapt to dynamic workload fluctuations, resulting in suboptimal throughput and\npotential service level objective (SLO) violations. In this paper, we introduce\nBucketServe, a bucket-based dynamic batching framework designed to optimize LLM\ninference performance. By grouping requests into size-homogeneous buckets based\non sequence length, BucketServe minimizes padding overhead and optimizes GPU\nmemory usage through real-time batch size adjustments preventing out-of-memory\n(OOM) errors. It introduces adaptive bucket splitting/merging and\npriority-aware scheduling to mitigate resource fragmentation and ensure SLO\ncompliance. Experiment shows that BucketServe significantly outperforms UELLM\nin throughput, achieving up to 3.58x improvement. It can also handle 1.93x more\nrequest load under the SLO attainment of 80% compared with DistServe and\ndemonstrates 1.975x higher system load capacity compared to the UELLM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BucketServe\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6876\u7684\u52a8\u6001\u6279\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8bf7\u6c42\u6309\u5e8f\u5217\u957f\u5ea6\u5206\u7ec4\u5230\u540c\u8d28\u5316\u6876\u4e2d\u6765\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u5e76\u51cf\u5c11\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u7cfb\u7edf\u4f7f\u7528\u9759\u6001\u6216\u8fde\u7eed\u6279\u5904\u7406\u7b56\u7565\uff0c\u5728\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u4f1a\u5bfc\u81f4GPU\u5185\u5b58\u5229\u7528\u7387\u4f4e\u6548\u548c\u5ef6\u8fdf\u589e\u52a0\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u6ce2\u52a8\uff0c\u9020\u6210\u541e\u5410\u91cf\u6b21\u4f18\u548cSLO\u8fdd\u53cd\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1BucketServe\u6846\u67b6\uff0c\u6839\u636e\u5e8f\u5217\u957f\u5ea6\u5c06\u8bf7\u6c42\u5206\u7ec4\u5230\u5927\u5c0f\u540c\u8d28\u7684\u6876\u4e2d\uff0c\u901a\u8fc7\u5b9e\u65f6\u6279\u5927\u5c0f\u8c03\u6574\u4f18\u5316GPU\u5185\u5b58\u4f7f\u7528\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u6876\u5206\u5272/\u5408\u5e76\u548c\u4f18\u5148\u7ea7\u611f\u77e5\u8c03\u5ea6\u673a\u5236\u3002", "result": "BucketServe\u5728\u541e\u5410\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8eUELLM\uff0c\u63d0\u5347\u9ad8\u8fbe3.58\u500d\uff1b\u572880% SLO\u8fbe\u6210\u7387\u4e0b\u80fd\u5904\u7406\u6bd4DistServe\u591a1.93\u500d\u7684\u8bf7\u6c42\u8d1f\u8f7d\uff1b\u7cfb\u7edf\u8d1f\u8f7d\u5bb9\u91cf\u6bd4UELLM\u9ad81.975\u500d\u3002", "conclusion": "BucketServe\u901a\u8fc7\u57fa\u4e8e\u6876\u7684\u52a8\u6001\u6279\u5904\u7406\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2d\u7684\u8d44\u6e90\u5229\u7528\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6027\u80fd\u6307\u6807\u4e0a\u90fd\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3aLLM\u670d\u52a1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17128", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.17128", "abs": "https://arxiv.org/abs/2507.17128", "authors": ["Minxian Xu", "Linfeng Wen", "Junhan Liao", "Huaming Wu", "Kejiang Ye", "Chengzhong Xu"], "title": "Auto-scaling Approaches for Cloud-native Applications: A Survey and Taxonomy", "comment": "14 pages", "summary": "The interactions within cloud-native applications are complex, with a\nconstantly changing number of services and loads, posing higher demands on\nauto-scaling approach. This mainly involves several challenges such as\nmicroservices dependency analysis, performance profiling, anomaly detection,\nworkload characterization and task co-location. Therefore, some advanced\nalgorithms have been investigated into auto-scaling cloud-native applications\nto optimize system and application performance. These algorithms can learn from\nhistorical data and appropriately adjust resource allocation based on the\ncurrent environment and load conditions to optimize resource utilization and\nsystem performance. In this paper, we systematically review the literature on\nstate-of-the-art auto-scaling approaches for cloud-native applications from\n2020, and further explore the technological evolution. Additionally, we propose\na detailed taxonomy to categorize current research from five perspectives,\nincluding infrastructure, architecture, scaling methods, optimization\nobjectives, and behavior modeling. Then, we provide a comprehensive comparison\nand in-depth discussion of the key features, advantages, limitations, and\napplication scenarios of each approach, considering their performance in\ndiverse environments and under various conditions. Finally, we summarize the\ncurrent state of research in this field, identify the gaps and unresolved\nchallenges, and emphasize promising directions for future exploration,\nparticularly in areas such as the application of large models, microservice\ndependency management, and the use of meta-learning techniques to enhance model\napplicability and adaptability across different environments.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u56de\u987e\u4e862020\u5e74\u4ee5\u6765\u4e91\u539f\u751f\u5e94\u7528\u81ea\u52a8\u6269\u7f29\u5bb9\u7684\u6700\u65b0\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4ece\u57fa\u7840\u8bbe\u65bd\u3001\u67b6\u6784\u3001\u6269\u7f29\u5bb9\u65b9\u6cd5\u3001\u4f18\u5316\u76ee\u6807\u548c\u884c\u4e3a\u5efa\u6a21\u4e94\u4e2a\u7ef4\u5ea6\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u7a7a\u767d\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u4e91\u539f\u751f\u5e94\u7528\u5177\u6709\u590d\u6742\u7684\u4ea4\u4e92\u5173\u7cfb\u3001\u670d\u52a1\u6570\u91cf\u548c\u8d1f\u8f7d\u4e0d\u65ad\u53d8\u5316\uff0c\u5bf9\u81ea\u52a8\u6269\u7f29\u5bb9\u65b9\u6cd5\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u6d89\u53ca\u5fae\u670d\u52a1\u4f9d\u8d56\u5206\u6790\u3001\u6027\u80fd\u5206\u6790\u3001\u5f02\u5e38\u68c0\u6d4b\u3001\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\u5316\u548c\u4efb\u52a1\u534f\u540c\u5b9a\u4f4d\u7b49\u591a\u9879\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u68b3\u7406\u73b0\u6709\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u4ece2020\u5e74\u5f00\u59cb\u6536\u96c6\u4e91\u539f\u751f\u5e94\u7528\u81ea\u52a8\u6269\u7f29\u5bb9\u7684\u6700\u65b0\u7814\u7a76\uff0c\u63d0\u51fa\u4e94\u7ef4\u5ea6\u5206\u7c7b\u6cd5\uff08\u57fa\u7840\u8bbe\u65bd\u3001\u67b6\u6784\u3001\u6269\u7f29\u5bb9\u65b9\u6cd5\u3001\u4f18\u5316\u76ee\u6807\u3001\u884c\u4e3a\u5efa\u6a21\uff09\uff0c\u5bf9\u5404\u79cd\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u6bd4\u8f83\u548c\u6df1\u5165\u8ba8\u8bba\uff0c\u5206\u6790\u5176\u5173\u952e\u7279\u5f81\u3001\u4f18\u52bf\u3001\u5c40\u9650\u6027\u548c\u5e94\u7528\u573a\u666f\u3002", "result": "\u5efa\u7acb\u4e86\u4e91\u539f\u751f\u5e94\u7528\u81ea\u52a8\u6269\u7f29\u5bb9\u65b9\u6cd5\u7684\u8be6\u7ec6\u5206\u7c7b\u4f53\u7cfb\uff0c\u5168\u9762\u6bd4\u8f83\u4e86\u5404\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u73af\u5883\u548c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u9886\u57df\u7684\u7a7a\u767d\u548c\u672a\u89e3\u51b3\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5927\u6a21\u578b\u5e94\u7528\u3001\u5fae\u670d\u52a1\u4f9d\u8d56\u7ba1\u7406\u548c\u5143\u5b66\u4e60\u6280\u672f\u65b9\u9762\u3002", "conclusion": "\u603b\u7ed3\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u5f3a\u8c03\u4e86\u672a\u6765\u6709\u524d\u666f\u7684\u63a2\u7d22\u65b9\u5411\uff0c\u7279\u522b\u662f\u5927\u6a21\u578b\u7684\u5e94\u7528\u3001\u5fae\u670d\u52a1\u4f9d\u8d56\u7ba1\u7406\u4ee5\u53ca\u5143\u5b66\u4e60\u6280\u672f\u7684\u4f7f\u7528\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.17133", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.17133", "abs": "https://arxiv.org/abs/2507.17133", "authors": ["Jianmin Hu", "Minxian Xu", "Kejiang Ye", "Chengzhong Xu"], "title": "BrownoutServe: SLO-Aware Inference Serving under Bursty Workloads for MoE-based LLMs", "comment": "12 pages", "summary": "In recent years, the Mixture-of-Experts (MoE) architecture has been widely\napplied to large language models (LLMs), providing a promising solution that\nactivates only a subset of the model's parameters during computation, thereby\nreducing overall memory requirements and allowing for faster inference compared\nto dense models. Despite these advantages, existing systems still face issues\nof low efficiency due to static model placement and lack of dynamic workloads\nadaptation. This leads to suboptimal resource utilization and increased\nlatency, especially during bursty requests periods.\n  To address these challenges, this paper introduces BrownoutServe, a novel\nserving framework designed to optimize inference efficiency and maintain\nservice reliability for MoE-based LLMs under dynamic computational demands and\ntraffic conditions. BrownoutServe introduces \"united experts\" that integrate\nknowledge from multiple experts, reducing the times of expert access and\ninference latency. Additionally, it proposes a dynamic brownout mechanism to\nadaptively adjust the processing of certain tokens, optimizing inference\nperformance while guaranteeing service level objectives (SLOs) are met. Our\nevaluations show the effectiveness of BrownoutServe under various workloads: it\nachieves up to 2.07x throughput improvement compared to vLLM and reduces SLO\nviolations by 90.28%, showcasing its robustness under bursty traffic while\nmaintaining acceptable inference accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BrownoutServe\u6846\u67b6\uff0c\u901a\u8fc7\"\u8054\u5408\u4e13\u5bb6\"\u548c\u52a8\u6001brownout\u673a\u5236\u4f18\u5316MoE\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u5728\u7a81\u53d1\u6d41\u91cf\u4e0b\u5b9e\u73b0\u4e86\u6700\u9ad82.07\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u548c90.28%\u7684SLO\u8fdd\u89c4\u51cf\u5c11\u3002", "motivation": "\u73b0\u6709MoE\u67b6\u6784\u7684\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\u5b58\u5728\u9759\u6001\u6a21\u578b\u90e8\u7f72\u548c\u7f3a\u4e4f\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u9002\u5e94\u6027\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\u3001\u5ef6\u8fdf\u589e\u52a0\uff0c\u7279\u522b\u662f\u5728\u7a81\u53d1\u8bf7\u6c42\u671f\u95f4\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faBrownoutServe\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6280\u672f\uff1a1\uff09\"\u8054\u5408\u4e13\u5bb6\"\u673a\u5236\uff0c\u6574\u5408\u591a\u4e2a\u4e13\u5bb6\u7684\u77e5\u8bc6\u4ee5\u51cf\u5c11\u4e13\u5bb6\u8bbf\u95ee\u6b21\u6570\u548c\u63a8\u7406\u5ef6\u8fdf\uff1b2\uff09\u52a8\u6001brownout\u673a\u5236\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u7279\u5b9atoken\u7684\u5904\u7406\u8fc7\u7a0b\uff0c\u5728\u4fdd\u8bc1\u670d\u52a1\u6c34\u5e73\u76ee\u6807\u7684\u540c\u65f6\u4f18\u5316\u63a8\u7406\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aBrownoutServe\u5728\u5404\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u8868\u73b0\u51fa\u8272\uff1a\u76f8\u6bd4vLLM\u5b9e\u73b0\u4e86\u6700\u9ad82.07\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0cSLO\u8fdd\u89c4\u51cf\u5c11\u4e8690.28%\uff0c\u5728\u7a81\u53d1\u6d41\u91cf\u4e0b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u63a5\u53d7\u7684\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "BrownoutServe\u6210\u529f\u89e3\u51b3\u4e86MoE\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u8ba1\u7b97\u9700\u6c42\u548c\u6d41\u91cf\u6761\u4ef6\u4e0b\u7684\u63a8\u7406\u6548\u7387\u548c\u670d\u52a1\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4e3aMoE\u67b6\u6784\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17301", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.17301", "abs": "https://arxiv.org/abs/2507.17301", "authors": ["Chi-Wei Chu", "Ding-Yong Hong", "Jan-Jan Wu"], "title": "Efficient Column-Wise N:M Pruning on RISC-V CPU", "comment": null, "summary": "In deep learning frameworks, weight pruning is a widely used technique for\nimproving computational efficiency by reducing the size of large models. This\nis especially critical for convolutional operators, which often act as\nperformance bottlenecks in convolutional neural networks (CNNs). However, the\neffectiveness of pruning heavily depends on how it is implemented, as different\nmethods can significantly impact both computational performance and memory\nfootprint. In this work, we propose a column-wise N:M pruning strategy applied\nat the tile level and modify XNNPACK to enable efficient execution of pruned\nmodels on the RISC-V vector architecture. Additionally, we propose fusing the\noperations of im2col and data packing to minimize redundant memory accesses and\nmemory overhead. To further optimize performance, we incorporate AITemplate's\nprofiling technique to identify the optimal implementation for each\nconvolutional operator. Our proposed approach effectively increases ResNet\ninference throughput by as much as 4.0x, and preserves ImageNet top-1 accuracy\nwithin 2.1\\% of the dense baseline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRISC-V\u5411\u91cf\u67b6\u6784\u7684\u5217\u7ea7N:M\u526a\u679d\u7b56\u7565\uff0c\u901a\u8fc7\u4fee\u6539XNNPACK\u6846\u67b6\u548c\u878d\u5408im2col\u4e0e\u6570\u636e\u6253\u5305\u64cd\u4f5c\uff0c\u5b9e\u73b0\u4e86ResNet\u63a8\u7406\u541e\u5410\u91cf4.0\u500d\u7684\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301ImageNet\u51c6\u786e\u7387\u635f\u5931\u57282.1%\u4ee5\u5185\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u6743\u91cd\u526a\u679d\u6280\u672f\u5bf9\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4f5c\u4e3aCNN\u6027\u80fd\u74f6\u9888\u7684\u5377\u79ef\u7b97\u5b50\u3002\u7136\u800c\uff0c\u4e0d\u540c\u7684\u526a\u679d\u5b9e\u73b0\u65b9\u6cd5\u4f1a\u663e\u8457\u5f71\u54cd\u8ba1\u7b97\u6027\u80fd\u548c\u5185\u5b58\u5360\u7528\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u526a\u679d\u7b56\u7565\u548c\u6267\u884c\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86tile\u7ea7\u522b\u7684\u5217\u7ea7N:M\u526a\u679d\u7b56\u7565\uff0c\u4fee\u6539XNNPACK\u4ee5\u652f\u6301RISC-V\u5411\u91cf\u67b6\u6784\u4e0a\u7684\u9ad8\u6548\u526a\u679d\u6a21\u578b\u6267\u884c\uff0c\u878d\u5408im2col\u548c\u6570\u636e\u6253\u5305\u64cd\u4f5c\u4ee5\u51cf\u5c11\u5197\u4f59\u5185\u5b58\u8bbf\u95ee\uff0c\u5e76\u7ed3\u5408AITemplate\u7684\u6027\u80fd\u5206\u6790\u6280\u672f\u4e3a\u6bcf\u4e2a\u5377\u79ef\u7b97\u5b50\u9009\u62e9\u6700\u4f18\u5b9e\u73b0\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7fResNet\u63a8\u7406\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe4.0\u500d\uff0c\u540c\u65f6ImageNet top-1\u51c6\u786e\u7387\u76f8\u6bd4\u7a20\u5bc6\u57fa\u7ebf\u6a21\u578b\u4ec5\u4e0b\u964d2.1%\u4ee5\u5185\uff0c\u6709\u6548\u5e73\u8861\u4e86\u6027\u80fd\u63d0\u5347\u548c\u7cbe\u5ea6\u4fdd\u6301\u3002", "conclusion": "\u901a\u8fc7\u5728RISC-V\u5411\u91cf\u67b6\u6784\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u5217\u7ea7N:M\u526a\u679d\u7b56\u7565\uff0c\u7ed3\u5408\u6846\u67b6\u4f18\u5316\u548c\u7b97\u5b50\u878d\u5408\u6280\u672f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u63a8\u7406\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17411", "categories": ["cs.DC", "90B35, 90C10, 68Q10, 68W10", "C.1.4"], "pdf": "https://arxiv.org/pdf/2507.17411", "abs": "https://arxiv.org/abs/2507.17411", "authors": ["P\u00e1l Andr\u00e1s Papp", "Toni B\u00f6hnlein", "A. N. Yzelman"], "title": "Multiprocessor Scheduling with Memory Constraints: Fundamental Properties and Finding Optimal Solutions", "comment": "Published in the 54th International Conference on Parallel Processing\n  (ICPP 2025)", "summary": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u53cc\u5c42\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u7684\u591a\u5904\u7406\u5668\u7cfb\u7edf\u4e0a\u8c03\u5ea6\u8ba1\u7b97DAG\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6574\u6570\u7ebf\u6027\u89c4\u5212(ILP)\u7684\u6574\u4f53\u8c03\u5ea6\u7b97\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u4f18\u5316\u8d1f\u8f7d\u5e73\u8861\u3001\u901a\u4fe1\u548c\u6570\u636e\u79fb\u52a8\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u5206\u79bb\u5f0f\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5e76\u884c\u5316\u548c\u5185\u5b58\u7ba1\u7406\u5206\u79bb\u4f18\u5316\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u4ea7\u751f\u4e0e\u6700\u4f18\u89e3\u76f8\u5dee\u7ebf\u6027\u500d\u6570\u7684\u7ed3\u679c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u8003\u8651\u5de5\u4f5c\u8d1f\u8f7d\u5e73\u8861\u3001\u901a\u4fe1\u5f00\u9500\u548c\u7f13\u5b58\u5927\u5c0f\u9650\u5236\u5bfc\u81f4\u7684\u6570\u636e\u79fb\u52a8\u7684\u6574\u4f53\u8c03\u5ea6\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6574\u6570\u7ebf\u6027\u89c4\u5212(ILP)\u65b9\u6cd5\u6765\u8868\u793a\u548c\u6c42\u89e3\u8c03\u5ea6\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eILP\u7684\u6574\u4f53\u8c03\u5ea6\u7b97\u6cd5\uff0c\u80fd\u591f\u7edf\u4e00\u5904\u7406\u5e76\u884c\u5316\u8c03\u5ea6\u548c\u5185\u5b58\u7ba1\u7406\uff0c\u800c\u4e0d\u662f\u5c06\u5b83\u4eec\u5206\u522b\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eILP\u7684\u65b9\u6cd5\u80fd\u591f\u627e\u5230\u6bd4\u7ed3\u5408\u7ecf\u5178\u8c03\u5ea6\u7b97\u6cd5\u548c\u5185\u5b58\u7ba1\u7406\u7b56\u7565\u7684\u57fa\u51c6\u65b9\u6cd5\u663e\u8457\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u6574\u4f53\u4f18\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5728\u53cc\u5c42\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u7684\u591a\u5904\u7406\u5668\u7cfb\u7edf\u4e0a\uff0c\u6574\u4f53\u8c03\u5ea6\u65b9\u6cd5\u4f18\u4e8e\u5206\u79bb\u5f0f\u4f18\u5316\u65b9\u6cd5\u3002\u57fa\u4e8eILP\u7684\u8c03\u5ea6\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u7ea6\u675f\u6761\u4ef6\uff0c\u4e3a\u8ba1\u7b97DAG\u8c03\u5ea6\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u601d\u8def\u3002"}}
{"id": "2507.17458", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.17458", "abs": "https://arxiv.org/abs/2507.17458", "authors": ["Marco Pulimeno", "Italo Epicoco", "Massimo Cafaro"], "title": "Distributed P2P quantile tracking with relative value error", "comment": null, "summary": "In this paper we present \\textsc{DUDDSketch}, a distributed version of the\n\\textsc{UDDSketch} algorithm for accurate tracking of quantiles. The algorithm\nis a fully decentralized, gossip-based distributed protocol working in the\ncontext of unstructured P2P networks. We discuss the algorithm's design and\nformally prove its correctness. We also show, through extensive experimental\nresults, that the algorithm converges to the results provided by the sequential\nalgorithm, which is a fundamental and highly desirable property.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DUDDSketch\u7b97\u6cd5\uff0c\u8fd9\u662fUDDSketch\u7b97\u6cd5\u7684\u5206\u5e03\u5f0f\u7248\u672c\uff0c\u7528\u4e8e\u5728\u975e\u7ed3\u6784\u5316P2P\u7f51\u7edc\u4e2d\u51c6\u786e\u8ffd\u8e2a\u5206\u4f4d\u6570\uff0c\u91c7\u7528\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684gossip\u534f\u8bae\u5b9e\u73b0", "motivation": "\u73b0\u6709\u7684\u5206\u4f4d\u6570\u8ffd\u8e2a\u7b97\u6cd5\u4e3b\u8981\u662f\u96c6\u4e2d\u5f0f\u7684\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e0b\uff0c\u7279\u522b\u662f\u5728\u975e\u7ed3\u6784\u5316P2P\u7f51\u7edc\u4e2d\u51c6\u786e\u8ffd\u8e2a\u5206\u4f4d\u6570\u7684\u7b97\u6cd5", "method": "\u8bbe\u8ba1\u4e86DUDDSketch\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u4e2a\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u3001\u57fa\u4e8egossip\u7684\u5206\u5e03\u5f0f\u534f\u8bae\uff0c\u80fd\u591f\u5728\u975e\u7ed3\u6784\u5316P2P\u7f51\u7edc\u73af\u5883\u4e2d\u5de5\u4f5c\uff0c\u5e76\u5bf9\u7b97\u6cd5\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\u6b63\u786e\u6027\u8bc1\u660e", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0cDUDDSketch\u7b97\u6cd5\u80fd\u591f\u6536\u655b\u5230\u4e0e\u987a\u5e8f\u7b97\u6cd5\u76f8\u540c\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027", "conclusion": "DUDDSketch\u7b97\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u5206\u5e03\u5f0f\u73af\u5883\u4e0b\u7684\u51c6\u786e\u5206\u4f4d\u6570\u8ffd\u8e2a\uff0c\u5177\u6709\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u548c\u6536\u655b\u6027\u4fdd\u8bc1\u7684\u7279\u70b9\uff0c\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5206\u4f4d\u6570\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
