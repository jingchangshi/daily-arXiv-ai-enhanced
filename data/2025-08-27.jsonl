{"id": "2508.18489", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.18489", "abs": "https://arxiv.org/abs/2508.18489", "authors": ["Haochen Pan", "Ryan Chard", "Reid Mello", "Christopher Grams", "Tanjin He", "Alexander Brace", "Owen Price Skelly", "Will Engler", "Hayden Holbrook", "Song Young Oh", "Maxime Gonthier", "Michael Papka", "Ben Blaiszik", "Kyle Chard", "Ian Foster"], "title": "Experiences with Model Context Protocol Servers for Science and High Performance Computing", "comment": "11 pages, including a 4-page appendix", "summary": "Large language model (LLM)-powered agents are increasingly used to plan and\nexecute scientific workflows, yet most research cyberinfrastructure (CI)\nexposes heterogeneous APIs and implements security models that present barriers\nfor use by agents. We report on our experience using the Model Context Protocol\n(MCP) as a unifying interface that makes research capabilities discoverable,\ninvokable, and composable. Our approach is pragmatic: we implement thin MCP\nservers over mature services, including Globus Transfer, Compute, and Search;\nstatus APIs exposed by computing facilities; Octopus event fabric; and\ndomain-specific tools such as Garden and Galaxy. We use case studies in\ncomputational chemistry, bioinformatics, quantum chemistry, and filesystem\nmonitoring to illustrate how this MCP-oriented architecture can be used in\npractice. We distill lessons learned and outline open challenges in evaluation\nand trust for agent-led science."}
{"id": "2508.18556", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.18556", "abs": "https://arxiv.org/abs/2508.18556", "authors": ["Abhijeet Saraha", "Yuanbo Li", "Chris Porter", "Santosh Pande"], "title": "Managing Multi Instance GPUs for High Throughput and Energy Savings", "comment": null, "summary": "Modern GPUs such as the Ampere series (A30, A100) as well as the Hopper\nseries (H100, H200) offer performance as well as security isolation features.\nThey also support a good amount of concurrency, but taking advantage of it can\nbe quite challenging due to the complex constraints on partitioning the chip.\n  In this work, we develop partitioning and scheduling schemes for a variety of\nworkloads, ranging from scientific to modern ML workloads, including LLMs. We\ndevelop several schemes involving dynamic memory estimation, partition fusion\nand partition fission. We also support process restart to recover from\nout-of-memory errors for workloads and early restart as an optimization. This\napproach yields up to 6.20x throughput and 5.93x energy improvements for\ngeneral workloads; and we see 1.59x and 1.12x improvement to throughput and\nenergy, respectively, for ML workloads on an A100 GPU. We leverage this\ntechnique on LLM workloads and show good improvements, including up to 1.43x\nthroughput improvement and 1.11x energy savings."}
{"id": "2508.18572", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.18572", "abs": "https://arxiv.org/abs/2508.18572", "authors": ["Zhiqiang Xie", "Ziyi Xu", "Mark Zhao", "Yuwei An", "Vikram Sharma Mailthody", "Scott Mahlke", "Michael Garland", "Christos Kozyrakis"], "title": "Strata: Hierarchical Context Caching for Long Context Language Model Serving", "comment": "13 pages, 14 figures, under peer review", "summary": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance."}
{"id": "2508.18667", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.18667", "abs": "https://arxiv.org/abs/2508.18667", "authors": ["Jiakun Yan", "Marc Snir", "Yanfei Guo"], "title": "Examining MPI and its Extensions for Asynchronous Multithreaded Communication", "comment": "This preprint has not undergone peer review. The Version of Record of\n  this contribution (with improvements after peer review) will be published in\n  EuroMPI25", "summary": "The increasing complexity of HPC architectures and the growing adoption of\nirregular scientific algorithms demand efficient support for asynchronous,\nmultithreaded communication. This need is especially pronounced with\nAsynchronous Many-Task (AMT) systems. This communication pattern was not a\nconsideration during the design of the original MPI specification. The MPI\ncommunity has recently introduced several extensions to address these evolving\nrequirements. This work evaluates two such extensions, the Virtual\nCommunication Interface (VCI) and the Continuation extensions, in the context\nof an established AMT runtime HPX. We begin by using an MPI-level\nmicrobenchmark, modeled from HPX's low-level communication mechanism, to\nmeasure the peak performance potential of these extensions. We then integrate\nthem into HPX to evaluate their effectiveness in real-world scenarios. Our\nresults show that while these extensions can enhance performance compared to\nstandard MPI, areas for improvement remain. The current continuation proposal\nlimits the maximum multithreaded message rate achievable in the multi-VCI\nsetting. Furthermore, the recommended one-VCI-per-thread mode proves\nineffective in real-world systems due to the attentiveness problem. These\nfindings underscore the importance of improving intra-VCI threading efficiency\nto achieve scalable multithreaded communication and fully realize the benefits\nof recent MPI extensions."}
{"id": "2508.18587", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18587", "abs": "https://arxiv.org/abs/2508.18587", "authors": ["Barış Bayazıt", "Yao Li", "Xujie Si"], "title": "A Case Study on the Effectiveness of LLMs in Verification with Proof Assistants", "comment": "Accepted by LMPL 2025", "summary": "Large language models (LLMs) can potentially help with verification using\nproof assistants by automating proofs. However, it is unclear how effective\nLLMs are in this task. In this paper, we perform a case study based on two\nmature Rocq projects: the hs-to-coq tool and Verdi. We evaluate the\neffectiveness of LLMs in generating proofs by both quantitative and qualitative\nanalysis. Our study finds that: (1) external dependencies and context in the\nsame source file can significantly help proof generation; (2) LLMs perform\ngreat on small proofs but can also generate large proofs; (3) LLMs perform\ndifferently on different verification projects; and (4) LLMs can generate\nconcise and smart proofs, apply classical techniques to new definitions, but\ncan also make odd mistakes."}
{"id": "2508.18924", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.18924", "abs": "https://arxiv.org/abs/2508.18924", "authors": ["Wei Xuan", "Zhongrui Wang", "Lang Feng", "Ning Lin", "Zihao Xuan", "Rongliang Fu", "Tsung-Yi Ho", "Yuzhong Jiao", "Luhong Liang"], "title": "SeDA: Secure and Efficient DNN Accelerators with Hardware/Software Synergy", "comment": "Accepted by Design Automation Conference (DAC), 2025", "summary": "Ensuring the confidentiality and integrity of DNN accelerators is paramount\nacross various scenarios spanning autonomous driving, healthcare, and finance.\nHowever, current security approaches typically require extensive hardware\nresources, and incur significant off-chip memory access overheads. This paper\nintroduces SeDA, which utilizes 1) a bandwidth-aware encryption mechanism to\nimprove hardware resource efficiency, 2) optimal block granularity through\nintra-layer and inter-layer tiling patterns, and 3) a multi-level integrity\nverification mechanism that minimizes, or even eliminates, memory access\noverheads. Experimental results show that SeDA decreases performance overhead\nby over 12% for both server and edge neural processing units (NPUs), while\nensuring robust scalability."}
{"id": "2508.18850", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18850", "abs": "https://arxiv.org/abs/2508.18850", "authors": ["Xinhao Luo", "Zihan Liu", "Yangjie Zhou", "Shihan Fang", "Ziyu Huang", "Yu Feng", "Chen Zhang", "Shixuan Sun", "Zhenzhe Zheng", "Jingwen Leng", "Minyi Guo"], "title": "ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive", "comment": null, "summary": "Large language model (LLM) decoding suffers from high latency due to\nfragmented execution across operators and heavy reliance on off-chip memory for\ndata exchange and reduction. This execution model limits opportunities for\nfusion and incurs significant memory traffic and kernel launch overhead. While\nmodern architectures such as NVIDIA Hopper provide distributed shared memory\nand low-latency intra-cluster interconnects, they expose only low-level data\nmovement instructions, lacking structured abstractions for collective on-chip\ncommunication. To bridge this software-hardware gap, we introduce two\ncluster-level communication primitives, ClusterReduce and ClusterGather, which\nabstract common communication patterns and enable structured, high-speed data\nexchange and reduction between thread blocks within a cluster, allowing\nintermediate results to be on-chip without involving off-chip memory. Building\non these abstractions, we design ClusterFusion, an execution framework that\nschedules communication and computation jointly to expand operator fusion scope\nby composing decoding stages such as QKV Projection, Attention, and Output\nProjection into a single fused kernels. Evaluations on H100 GPUs show that\nClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on\naverage in end-to-end latency across different models and configurations. The\nsource code is available at https://github.com/xinhao-luo/ClusterFusion."}
{"id": "2508.18961", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.18961", "abs": "https://arxiv.org/abs/2508.18961", "authors": ["Qianpeng Li", "Yu Song", "Xin Liu", "Wenna Song", "Boshi Zhao", "Zhichao Wang", "Aoxin Chen", "Tielin Zhang", "Liang Chen"], "title": "TaiBai: A fully programmable brain-inspired processor with topology-aware efficiency", "comment": null, "summary": "Brain-inspired computing has emerged as a promising paradigm to overcome the\nenergy-efficiency limitations of conventional intelligent systems by emulating\nthe brain's partitioned architecture and event-driven sparse computation.\nHowever, existing brain-inspired chips often suffer from rigid network topology\nconstraints and limited neuronal programmability, hindering their adaptability.\nTo address these challenges, we present TaiBai, an event-driven, programmable\nmany-core brain-inspired processor that leverages temporal and spatial spike\nsparsity to minimize bandwidth and computational overhead. TaiBai chip contains\nthree key features: First, a brain-inspired hierarchical topology encoding\nscheme is designed to flexibly support arbitrary network architectures while\nslashing storage overhead for large-scale networks; Second, a multi-granularity\ninstruction set enables programmability of brain-like spiking neuron or\nsynapses with various dynamics and on-chip learning rules; Third, a co-designed\ncompiler stack optimizes task mapping and resource allocation. After evaluating\nacross various tasks, such as speech recognition, ECG classification, and\ncross-day brain-computer interface decoding, we found spiking neural networks\nembedded on the TaiBai chip could achieve more than 200 times higher energy\nefficiency than a standard NVIDIA RTX 3090 GPU at a comparable accuracy. These\nresults demonstrated its high potentiation as a scalable, programmable, and\nultra-efficient solution for both multi-scale brain simulation and\nbrain-inspired computation."}
{"id": "2508.18950", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.18950", "abs": "https://arxiv.org/abs/2508.18950", "authors": ["Thomas Jakobsche", "Fredrik Robertsén", "Jessica R. Jones", "Utz-Uwe Haus", "Florina M. Ciorba"], "title": "SIREN: Software Identification and Recognition in HPC Systems", "comment": null, "summary": "HPC systems use monitoring and operational data analytics to ensure\nefficiency, performance, and orderly operations. Application-specific insights\nare crucial for analyzing the increasing complexity and diversity of HPC\nworkloads, particularly through the identification of unknown software and\nrecognition of repeated executions, which facilitate system optimization and\nsecurity improvements. However, traditional identification methods using job or\nfile names are unreliable for arbitrary user-provided names (a.out). Fuzzy\nhashing of executables detects similarities despite changes in executable\nversion or compilation approach while preserving privacy and file integrity,\novercoming these limitations. We introduce SIREN, a process-level data\ncollection framework for software identification and recognition. SIREN\nimproves observability in HPC by enabling analysis of process metadata,\nenvironment information, and executable fuzzy hashes. Findings from a first\nopt-in deployment campaign on LUMI show SIREN's ability to provide insights\ninto software usage, recognition of repeated executions of known applications,\nand similarity-based identification of unknown applications."}
{"id": "2508.19090", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.19090", "abs": "https://arxiv.org/abs/2508.19090", "authors": ["Rohan Juneja", "Pranav Dangi", "Thilini Kaushalya Bandara", "Zhaoying Li", "Dhananjaya Wijerathne", "Li-Shiuan Peh", "Tulika Mitra"], "title": "Building an Open CGRA Ecosystem for Agile Innovation", "comment": null, "summary": "Modern computing workloads, particularly in AI and edge applications, demand\nhardware-software co-design to meet aggressive performance and energy targets.\nSuch co-design benefits from open and agile platforms that replace closed,\nvertically integrated development with modular, community-driven ecosystems.\nCoarse-Grained Reconfigurable Architectures (CGRAs), with their unique balance\nof flexibility and efficiency are particularly well-suited for this paradigm.\nWhen built on open-source hardware generators and software toolchains, CGRAs\nprovide a compelling foundation for architectural exploration, cross-layer\noptimization, and real-world deployment. In this paper, we will present an open\nCGRA ecosystem that we have developed to support agile innovation across the\nstack. Our contributions include HyCUBE, a CGRA with a reconfigurable\nsingle-cycle multi-hop interconnect for efficient data movement; PACE, which\nembeds a power-efficient HyCUBE within a RISC-V SoC targeting edge computing;\nand Morpher, a fully open-source, architecture-adaptive CGRA design framework\nthat supports design space exploration, compilation, simulation, and\nvalidation. By embracing openness at every layer, we aim to lower barriers to\ninnovation, enable reproducible research, and demonstrate how CGRAs can anchor\nthe next wave of agile hardware development. We will conclude with a call for a\nunified abstraction layer for CGRAs and spatial accelerators, one that\ndecouples hardware specialization from software development. Such a\nrepresentation would unlock architectural portability, compiler innovation, and\na scalable, open foundation for spatial computing."}
{"id": "2508.18969", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.18969", "abs": "https://arxiv.org/abs/2508.18969", "authors": ["Zhuoqiang Guo", "Runze Mao", "Lijun Liu", "Guangming Tan", "Weile Jia", "Zhi X. Chen"], "title": "Deep Learning-Enabled Supercritical Flame Simulation at Detailed Chemistry and Real-Fluid Accuracy Towards Trillion-Cell Scale", "comment": "12 pages, 14 figures, conference: SC25", "summary": "For decades, supercritical flame simulations incorporating detailed chemistry\nand real-fluid transport have been limited to millions of cells, constraining\nthe resolved spatial and temporal scales of the physical system. We optimize\nthe supercritical flame simulation software DeepFlame -- which incorporates\ndeep neural networks while retaining the real-fluid mechanical and chemical\naccuracy -- from three perspectives: parallel computing, computational\nefficiency, and I/O performance. Our highly optimized DeepFlame achieves\nsupercritical liquid oxygen/methane (LOX/\\ce{CH4}) turbulent combustion\nsimulation of up to 618 and 154 billion cells with unprecedented\ntime-to-solution, attaining 439/1186 and 187/316 PFlop/s (32.3\\%/21.8\\% and\n37.4\\%/31.8\\% of the peak) in FP32/mixed-FP16 precision on Sunway (98,304\nnodes) and Fugaku (73,728 nodes) supercomputers, respectively. This\ncomputational capability surpasses existing capacities by three orders of\nmagnitude, enabling the first practical simulation of rocket engine combustion\nwith >100 LOX/\\ce{CH4} injectors. This breakthrough establishes high-fidelity\nsupercritical flame modeling as a critical design tool for next-generation\nrocket propulsion and ultra-high energy density systems."}
{"id": "2508.19073", "categories": ["cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.19073", "abs": "https://arxiv.org/abs/2508.19073", "authors": ["Ehsan Yousefzadeh-Asl-Miandoab", "Reza Karimzadeh", "Bulat Ibragimov", "Florina M. Ciorba", "Pınar Tözün"], "title": "CARMA: Collocation-Aware Resource Manager with GPU Memory Estimator", "comment": null, "summary": "Studies conducted on enterprise-scale infrastructure have shown that GPUs --\nthe core computational resource for deep learning (DL) training -- are often\nsignificantly underutilized. DL task collocation on GPUs is an opportunity to\naddress this challenge. However, it may result in (1) out-of-memory crashes for\nthe subsequently arriving task and (2) slowdowns for all tasks sharing the GPU\ndue to resource interference. The former challenge poses a threat to\nrobustness, while the latter affects the quality of service and energy\nefficiency.\n  We propose CARMA, a server-scale task-level collocation-aware resource\nmanagement system that handles both collocation challenges. CARMA encompasses\nGPUMemNet, a novel ML-based GPU memory estimator framework for DL training\ntasks, to minimize out-of-memory errors and introduces collocation policies\nthat cap GPU utilization to minimize interference. Furthermore, CARMA\nintroduces a recovery method to ensure robust restart of tasks that crash. Our\nevaluation on traces modeled after real-world DL training task traces shows\nthat CARMA increases the GPU utilization over time by 39.3\\%, decreases the\nend-to-end execution time by $\\sim$26.7\\%, and reduces the GPU energy use by\n$\\sim$14.2\\%."}
{"id": "2508.19078", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19078", "abs": "https://arxiv.org/abs/2508.19078", "authors": ["Fahao Chen", "Jie Wan", "Peng Li", "Zhou Su", "Dongxiao Yu"], "title": "Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices", "comment": "Accepted by EuroSys'26. The camera-ready version will be uploaded\n  later", "summary": "Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models\n(LLMs) is challenging due to their massive computational requirements and the\nresource constraints of participants. Existing working attempts to fill this\ngap through model quantization, computation offloading, or expert pruning.\nHowever, they cannot achieve desired performance due to impractical system\nassumptions and a lack of consideration for MoE-specific characteristics. In\nthis paper, we propose FLUX, a system designed to enable federated fine-tuning\nof MoE-based LLMs across participants with constrained computing resources\n(e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX\nintroduces three key innovations: (1) quantization-based local profiling to\nestimate expert activation with minimal overhead, (2) adaptive layer-aware\nexpert merging to reduce resource consumption while preserving accuracy, and\n(3) dynamic expert role assignment using an exploration-exploitation strategy\nto balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE\nand DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX\nsignificantly outperforms existing methods, achieving up to 4.75X speedup in\ntime-to-accuracy."}
{"id": "2508.19138", "categories": ["cs.DC", "cond-mat.mes-hall", "cs.CE"], "pdf": "https://arxiv.org/pdf/2508.19138", "abs": "https://arxiv.org/abs/2508.19138", "authors": ["Nicolas Vetsch", "Alexander Maeder", "Vincent Maillou", "Anders Winka", "Jiang Cao", "Grzegorz Kwasniewski", "Leonard Deuschle", "Torsten Hoefler", "Alexandros Nikolaos Ziogas", "Mathieu Luisier"], "title": "Ab-initio Quantum Transport with the GW Approximation, 42,240 Atoms, and Sustained Exascale Performance", "comment": null, "summary": "Designing nanoscale electronic devices such as the currently manufactured\nnanoribbon field-effect transistors (NRFETs) requires advanced modeling tools\ncapturing all relevant quantum mechanical effects. State-of-the-art approaches\ncombine the non-equilibrium Green's function (NEGF) formalism and density\nfunctional theory (DFT). However, as device dimensions do not exceed a few\nnanometers anymore, electrons are confined in ultra-small volumes, giving rise\nto strong electron-electron interactions. To account for these critical\neffects, DFT+NEGF solvers should be extended with the GW approximation, which\nmassively increases their computational intensity. Here, we present the first\nimplementation of the NEGF+GW scheme capable of handling NRFET geometries with\ndimensions comparable to experiments. This package, called QuaTrEx, makes use\nof a novel spatial domain decomposition scheme, can treat devices made of up to\n84,480 atoms, scales very well on the Alps and Frontier supercomputers (>80%\nweak scaling efficiency), and sustains an exascale FP64 performance on 42,240\natoms (1.15 Eflop/s)."}
