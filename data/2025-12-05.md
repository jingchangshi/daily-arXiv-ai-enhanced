<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Typing Fallback Functions: A Semantic Approach to Type Safe Smart Contracts](https://arxiv.org/abs/2512.04755)
*Stian Lybech,Daniele Gorla,Luca Aceto*

Main category: cs.PL

TL;DR: 论文提出在智能合约环境中使用语义类型化，通过证明携带代码确保包含不可静态类型化构造（如fallback函数）的代码类型安全，用户只需验证提供的类型安全证明证书。


<details>
  <summary>Details</summary>
Motivation: 智能合约中某些语言构造（如fallback函数）无法静态类型化，存在类型安全问题。区块链环境的不可变性使得证明携带代码方法特别适合，需要确保这类代码的类型安全。

Method: 采用语义类型化方法，合约创建者为包含不可静态类型化构造的代码提供形式化类型安全证明。基于TINYSOL语言（Solidity的精简版），通过安全类型确保信息流控制和非干扰性，使用共归纳定义的类型解释和up-to技术紧凑表示证明。

Result: 开发了适用于区块链/智能合约环境的理论框架，能够类型化基于fallback函数的典型指针到实现模式，确保信息流控制和非干扰性安全属性。

Conclusion: 论文的主要贡献不是安全定理本身，而是展示了使该方法在区块链/智能合约环境中工作的必要理论发展，为处理不可静态类型化的语言构造提供了可行的解决方案。

Abstract: This paper develops semantic typing in a smart-contract setting to ensure type safety of code that uses statically untypable language constructs, such as the fallback function. The idea is that the creator of a contract on the blockchain equips code containing such constructs with a formal proof of its type safety, given in terms of the semantics of types. Then, a user of the contract only needs to check the validity of the provided `proof certificate' of type safety. This is a form of proof-carrying code, which naturally fits with the immutable nature of the blockchain environment.
  As a concrete application of our approach, we focus on ensuring information flow control and non-interference for the language TINYSOL, a distilled version of the Solidity language, through security types. We provide the semantics of types in terms of a typed operational semantics of TINYSOL, and a way for expressing the proofs of safety as coinductively-defined typing interpretations and for representing them compactly via up-to techniques, similar to those used for bisimilarity. We also show how our machinery can be used to type the typical pointer-to-implementation pattern based on the fallback function. However, our main contribution is not the safety theorem per se (and so security properties different from non-interference can be considered as well), but rather the presentation of the theoretical developments necessary to make this approach work in a blockchain/smart-contract setting.

</details>


### [2] [Optimizations and extensions for fair join pattern matching](https://arxiv.org/abs/2512.04876)
*Ioannis Karras*

Main category: cs.PL

TL;DR: 该论文优化了Haller等人的状态树匹配算法，在特定基准测试中实现10倍性能提升，接近Rete算法性能，同时保持对复杂条件守卫的优势，并扩展了连接模式的功能和应用场景。


<details>
  <summary>Details</summary>
Motivation: 连接模式在并发和分布式系统编程中尚未充分探索，现有公平连接模式匹配算法的时间效率问题未得到充分研究。Haller等人的状态树算法在常规基准测试中表现不如Rete算法，而Rete算法需要大量手动适配才能应用于连接模式匹配。

Method: 优化和增强Haller等人的状态树匹配算法，改进基准测试套件（增加新功能、增强可扩展性和用户友好性），扩展连接模式实现（提供更少歧义的语法和动态模式切换），并展示连接模式在微服务Web架构中的新应用用例。

Result: 在特定基准测试中实现高达10倍的性能提升，接近Rete算法在常规基准测试中的性能，同时保持对复杂条件守卫的优异表现。扩展了连接模式的功能，展示了其在微服务架构中的实际应用价值。

Conclusion: 通过优化状态树匹配算法，显著提升了公平连接模式匹配的时间效率，使其在保持对复杂条件守卫优势的同时，接近Rete算法的性能。扩展的连接模式功能和新的应用案例进一步证明了连接模式在并发和分布式系统中的实用价值。

Abstract: Join patterns are an underexplored approach for the programming of concurrent and distributed systems. When applied to the actor model, join patterns offer the novel capability of matching combinations of messages in the mailbox of an actor. Previous work by Philipp Haller et al. in the paper "Fair Join Pattern Matching for Actors" (ECOOP 2024) explored join patterns with conditional guards in an actor-based setting with a specification of fair and deterministic matching semantics. Nevertheless, the question of time efficiency in fair join pattern matching has remained underexplored. The stateful tree-based matching algorithm of Haller et al. performs worse than an implementation that adapts the Rete algorithm to the regular version of a join pattern matching benchmark, while outperforming on a variant with heavy conditional guards, which take longer to evaluate. Nevertheless, conforming Rete to the problem of join pattern matching requires heavy manual adaptation.
  In this thesis, we enhance and optimize the stateful tree-based matching algorithm of Haller et al. to achieve up to tenfold performance improvements on certain benchmarks, approaching the performance of Rete on regular benchmarks while maintaining the advantages of versatility and performance with heavy guards. We also enhance the benchmark suite, adding new features and enhancing its extensibility and user-friendliness. We extend the join pattern implementation with a less ambiguous syntax as well as dynamic pattern switching. Finally, we present a new complex model use case for join patterns, showing their applicability in a microservice web architecture.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Toward Sustainability-Aware LLM Inference on Edge Clusters](https://arxiv.org/abs/2512.04088)
*Kolichala Rajashekar,Nafiseh Sharghivand,Radu Prodan,Reza Farahani*

Main category: cs.DC

TL;DR: 该论文提出了一种面向边缘集群的可持续性感知LLM推理框架，通过碳感知和延迟感知的路由策略，在NVIDIA Jetson Orin NX和Ada 2000设备上平衡推理延迟和碳足迹。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要大量计算资源，导致显著的碳排放和运营成本。虽然训练能耗高，但长期环境负担来自推理，特别是全球海量查询量。云端推理存在延迟和带宽限制，而边缘集群虽能实现本地化执行，但面临性能、能效和设备限制之间的权衡。

Method: 提出了可持续性感知的LLM推理框架，包含碳感知和延迟感知的路由策略。通过实证基准测试，测量不同提示和批处理配置下的能耗和执行时间，并比较贪心策略与碳感知、延迟感知策略在提示路由到特定硬件时的表现。

Result: 实验评估显示，批处理大小为4个提示时能在吞吐量和能效之间取得平衡，而更大的批处理可能导致GPU内存饱和。碳感知和延迟感知策略相比基线贪心策略能更好地平衡延迟和碳足迹。

Conclusion: 通过碳感知和延迟感知的路由策略，可以在边缘集群上实现可持续的LLM推理，在保持性能的同时减少环境影响。批处理大小需要仔细选择以避免内存饱和并优化能效。

Abstract: Large language models (LLMs) require substantial computational resources, leading to significant carbon emissions and operational costs. Although training is energy-intensive, the long-term environmental burden arises from inference, amplified by the massive global query volume. Cloud-based inference offers scalability but suffers from latency and bandwidth constraints due to centralized processing and continuous data transfer. Edge clusters instead can mitigate these limitations by enabling localized execution, yet they face trade-offs between performance, energy efficiency, and device constraints. This short paper presents a sustainability-aware LLM inference for edge clusters comprising NVIDIA Jetson Orin NX (8GB) and Nvidia Ada 2000 (16GB) devices. It aims to balance inference latency and carbon footprint through carbon- and latency-aware routing strategies, guided by empirical benchmarking of energy consumption and execution time across diverse prompts and batch (i.e., group of prompts) configurations. We compared baseline greedy strategies to carbon-aware and latency-aware strategies in prompt routing to specific hardware based on benchmarking information. Experimental evaluation shows that a batch size of four prompts achieves a trade-off between throughput, energy efficiency, while larger batches risk GPU memory saturation.

</details>


### [4] [Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud](https://arxiv.org/abs/2512.04089)
*Mario Colosi,Reza Farahani,Lauri Loven,Radu Prodan,Massimo Villari*

Main category: cs.DC

TL;DR: 评估WebAssembly在浏览器、边缘和云环境中执行无服务器工作流的性能表现，发现AOT编译和实例预热能显著降低启动延迟，小负载时浏览器性能有竞争力，大负载时边缘和云节点的AOT执行表现更优。


<details>
  <summary>Details</summary>
Motivation: WebAssembly（Wasm）作为二进制指令格式，具有可移植性、沙箱化和接近原生执行的特性，适合在浏览器、边缘节点和云服务器上执行无服务器工作流。但其性能和稳定性受启动开销、运行时执行模型（AOT/JIT编译）以及不同部署环境资源差异的影响，需要系统评估。

Method: 使用wasm32-wasi模块构建Wasm无服务器工作流，在浏览器中通过web worker执行，在边缘和云环境中通过HTTP shim流式传输帧到Wasm运行时。测量冷启动/热启动延迟、每步延迟、工作流总时长、吞吐量以及CPU/内存使用率，以获取跨环境的端到端行为数据。

Result: AOT编译和实例预热显著降低了启动延迟。对于小负载工作流，浏览器由于完全内存数据交换而具有竞争力。随着负载增大，工作流进入计算和内存密集型阶段，边缘和云节点的AOT执行性能明显超过浏览器。

Conclusion: Wasm无服务器工作流在不同环境中的性能表现存在差异：AOT编译和预热对启动性能至关重要；小负载场景下浏览器表现良好；大负载场景下边缘和云环境更适合计算密集型任务。这为Wasm工作流部署提供了环境选择指导。

Abstract: WebAssembly (Wasm) is a binary instruction format that enables portable, sandboxed, and near-native execution across heterogeneous platforms, making it well-suited for serverless workflow execution on browsers, edge nodes, and cloud servers. However, its performance and stability depend heavily on factors such as startup overhead, runtime execution model (e.g., Ahead-of-Time (AOT) and Just-in-Time (JIT) compilation), and resource variability across deployment contexts. This paper evaluates a Wasm-based serverless workflow executed consistently from the browser to edge and cloud instances. The setup uses wasm32-wasi modules: in the browser, execution occurs within a web worker, while on Edge and Cloud, an HTTP shim streams frames to the Wasm runtime. We measure cold- and warm-start latency, per-step delays, workflow makespan, throughput, and CPU/memory utilization to capture the end-to-end behavior across environments. Results show that AOT compilation and instance warming substantially reduce startup latency. For workflows with small payloads, the browser achieves competitive performance owing to fully in-memory data exchanges. In contrast, as payloads grow, the workflow transitions into a compute- and memory-intensive phase where AOT execution on edge and cloud nodes distinctly surpasses browser performance.

</details>


### [5] [Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions](https://arxiv.org/abs/2512.04093)
*Ali Akbar Vali,Sadoon Azizi,Mohammad Shojafar,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文对微服务雾边缘计算中的资源管理策略进行了全面综述，重点关注能源效率，系统分类了136项研究，识别了未解决挑战并提出了AI、量子计算等未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 物联网设备激增需要高效响应服务，雾边缘计算虽能降低延迟和能耗，但面临资源受限、异构性、动态负载等资源管理挑战，需要系统性的解决方案综述。

Method: 系统综述2020-2024年间136项研究，按服务放置、资源供给、任务调度与卸载、资源分配、实例选择五个子领域分类，基于优化技术、目标、优缺点进行分析。

Result: 建立了全面的分类框架，识别了现有资源管理组件间缺乏协同的关键问题，发现了文献中的研究空白，为未来研究提供了系统参考。

Conclusion: 本文提供了微服务雾边缘计算资源管理的统一能源感知视角，指出需加强基础组件协同，并展望了AI驱动优化、量子计算、无服务器计算等未来研究方向。

Abstract: The exponential growth of Internet of Things (IoT) devices has intensified the demand for efficient and responsive services. To address this demand, fog and edge computing have emerged as distributed paradigms that bring computational resources closer to end users, reducing latency, bandwidth limitations, and energy consumption. However, these paradigms present challenges in resource management due to resource constraints, computational heterogeneity, dynamic workloads, and diverse Quality of Service (QoS) requirements. This paper presents a comprehensive survey of state-of-the-art resource management strategies in microservices-based fog and edge computing, focusing on energy-efficient solutions. We systematically review and classify more than 136 studies (2020-2024) into five key subdomains: service placement, resource provisioning, task scheduling and offloading, resource allocation, and instance selection. Our categorization is based on optimization techniques, targeted objectives, and the strengths and limitations of each approach. In addition, we examine existing surveys and identify unresolved challenges and gaps in the literature. By highlighting the lack of synergy among fundamental resource management components, we outline promising research directions leveraging AI-driven optimization, quantum computing, and serverless computing. This survey serves as a comprehensive reference for researchers and practitioners by providing a unified and energy-aware perspective on resource management in microservices-based fog and edge computing, paving the way for more integrated, efficient, and sustainable future solutions.

</details>


### [6] [Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale](https://arxiv.org/abs/2512.04096)
*Sushant Kumar Gupta,Anil Raghunath Iyer,Chang Yu,Neel Bagora,Olivier Pomerleau,Vivek Kumar,Prunthaban Kanthakumar*

Main category: cs.DC

TL;DR: Fast ACS是一个文件式有序消息投递系统，使用RPC和RMA通信原语，支持数千消费者，实现秒级或亚秒级全球投递


<details>
  <summary>Details</summary>
Motivation: 实时系统需要低延迟消息投递，数据可能分布在跨大都市和洲际的集群中，可能有数千消费者，需要健壮的消息系统保证有序性和至少一次投递，同时避免消费者过载

Method: 设计Fast ACS文件式有序消息投递系统，结合双面（集群间）和单面（集群内）通信原语：远程过程调用（RPC）和远程内存访问（RMA）

Result: 系统已部署到数十个生产集群，每个集群可扩展到数千消费者，峰值时集群内消费者流量达Tbps级，基于消息量和消费者规模，可在几秒甚至亚秒内（p99）将消息投递给全球消费者，资源成本低

Conclusion: Fast ACS通过结合RPC和RMA通信原语，成功实现了高效、可扩展的低延迟消息投递系统，满足大规模实时系统的需求

Abstract: Low-latency message delivery is crucial for real-time systems. Data originating from a producer must be delivered to consumers, potentially distributed in clusters across metropolitan and continental boundaries. With the growing scale of computing, there can be several thousand consumers of the data. Such systems require a robust messaging system capable of transmitting messages containing data across clusters and efficiently delivering them to consumers. The system must offer guarantees like ordering and at-least-once delivery while avoiding overload on consumers, allowing them to consume messages at their own pace.
  This paper presents the design of Fast ACS (an abbreviation for Ads Copy Service), a file-based ordered message delivery system that leverages a combination of two-sided (inter-cluster) and one-sided (intra-cluster) communication primitives - namely, Remote Procedure Call and Remote Memory Access, respectively - to deliver messages. The system has been successfully deployed to dozens of production clusters and scales to accommodate several thousand consumers within each cluster, which amounts to Tbps-scale intra-cluster consumer traffic at peak. Notably, Fast ACS delivers messages to consumers across the globe within a few seconds or even sub-seconds (p99) based on the message volume and consumer scale, at a low resource cost.

</details>


### [7] [tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection](https://arxiv.org/abs/2512.04226)
*Ryan Swann,Muhammad Osama,Xiaohu Guo,Bryant Nelson,Lixun Zhang,Alex Brown,Yen Ong,Ali Yazdani,Sean Siddens,Ganesh Dasika,Alex Underwood*

Main category: cs.DC

TL;DR: tritonBLAS是一个基于架构参数的确定性分析模型，用于生成高性能GPU GEMM内核，无需运行时自动调优，性能可达自动调优方案的95%以上。


<details>
  <summary>Details</summary>
Motivation: 传统GPU GEMM内核通常依赖耗时的运行时自动调优来寻找最优配置，这增加了部署成本和时间开销。需要一种能够基于架构参数预测最优配置的确定性模型来替代经验性调优。

Method: 使用缓存层次结构、代码和数据相对位置等架构参数，显式建模架构拓扑、矩阵形状和算法分块行为之间的关系，基于此模型在Triton中实现轻量级GEMM框架。

Result: 在现代GPU上评估多种GEMM问题规模，tritonBLAS性能达到自动调优解决方案的95%以上，同时将自动调优时间降为零，成为生产HPC和ML工作负载中实用的经验调优替代方案。

Conclusion: tritonBLAS通过确定性分析模型成功实现了高性能GPU GEMM内核生成，无需运行时自动调优，为生产环境提供了高效实用的解决方案。

Abstract: We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.

</details>


### [8] [Scaling MPI Applications on Aurora](https://arxiv.org/abs/2512.04291)
*Huda Ibeid,Anthony-Trung Nguyen,Aditya Nishtala,Premanand Sakarda,Larry Kaplan,Nilakantan Mahadevan,Michael Woodacre,Victor Anisimov,Kalyan Kumaran,JaeHyuk Kwack,Vitali Morozov,Servesh Muralidharan,Scott Parker*

Main category: cs.DC

TL;DR: Aurora超算系统架构分析：采用Intel Max系列GPU/CPU和HPE Slingshot网络，实现Exascale性能，在Top500和HPL MxP基准测试中表现优异，支持AI和HPC科学计算。


<details>
  <summary>Details</summary>
Motivation: 介绍Aurora超算系统的设计细节，特别是网络架构和验证方法，展示其在Exascale计算中的性能表现，证明其能够支持大规模科学计算应用。

Method: 详细描述Aurora系统架构：超过1万个节点，每个节点包含6个Intel Data Center Max Series GPU和2个Intel Xeon Max Series CPU，采用HPE Slingshot高性能网络互连（包含近85,000个Cassini NIC和5,600个Rosetta交换机，采用dragonfly拓扑）。通过MPI基准测试和性能基准测试（HPL、HPL-MxP、Graph500、HPCG）验证系统性能，并展示多种应用（HACC、AMR-Wind、LAMMPS、FMM）的实际表现。

Result: Aurora在2024年6月Top500榜单中排名第二，在HPL MxP基准测试中排名第一。系统展示了优异的吞吐量、延迟和带宽性能，能够支持应用扩展到大规模节点数量，为突破性科学研究提供新的能力水平。

Conclusion: Aurora超算系统成功实现了Exascale性能，其Intel硬件和HPE Slingshot网络组合提供了卓越的计算能力，能够支持多样化的AI和HPC科学计算应用，为开放科学提供了强大的计算平台。

Abstract: The Aurora supercomputer, which was deployed at Argonne National Laboratory in 2024, is currently one of three Exascale machines in the world on the Top500 list. The Aurora system is composed of over ten thousand nodes each of which contains six Intel Data Center Max Series GPUs, Intel's first data center-focused discrete GPU, and two Intel Xeon Max Series CPUs, Intel's first Xeon processor to contain HBM memory. To achieve Exascale performance the system utilizes the HPE Slingshot high-performance fabric interconnect to connect the nodes. Aurora is currently the largest deployment of the Slingshot fabric to date with nearly 85,000 Cassini NICs and 5,600 Rosetta switches connected in a dragonfly topology. The combination of the Intel powered nodes and the Slingshot network enabled Aurora to become the second fastest system on the Top500 list in June of 2024 and the fastest system on the HPL MxP benchmark. The system is one of the most powerful systems in the world dedicated to AI and HPC simulations for open science. This paper presents details of the Aurora system design with a particular focus on the network fabric and the approach taken to validating it. The performance of the systems is demonstrated through the presentation of the results of MPI benchmarks as well as performance benchmarks including HPL, HPL-MxP, Graph500, and HPCG run on a large fraction of the system. Additionally results are presented for a diverse set of applications including HACC, AMR-Wind, LAMMPS, and FMM demonstrating that Aurora provides the throughput, latency, and bandwidth across system needed to allow applications to perform and scale to large node counts and providing new levels of capability and enabling breakthrough science.

</details>


### [9] [VLCs: Managing Parallelism with Virtualized Libraries](https://arxiv.org/abs/2512.04320)
*Yineng Yan,William Ruys,Hochan Lee,Ian Henriksen,Arthur Peters,Sean Stephens,Bozhi You,Henrique Fingler,Martin Burtscher,Milos Gligoric,Keshav Pingali,Mattan Erez,George Biros,Christopher J. Rossbach*

Main category: cs.DC

TL;DR: VLCs（虚拟库上下文）是一种进程子单元，能够封装库和资源分配，在不修改库代码的情况下控制资源使用，解决并行库组合时的资源争用问题。


<details>
  <summary>Details</summary>
Motivation: 随着并行机器复杂性和规模的增加，程序员越来越依赖软件库的组合来封装和利用并行性。然而，许多库在设计时没有考虑组合性，假设独占所有资源，导致并发使用时产生争用和性能下降。现有解决方案需要修改库或操作系统，通常不可行。

Method: 提出虚拟库上下文（VLCs），这是一种进程子单元，能够封装一组库及其关联的资源分配。VLCs可以在不修改库代码的情况下控制这些库的资源使用，允许用户在库之间划分资源以防止争用，或者加载同一库的多个副本以实现原本线程不安全代码的并行执行。

Result: 实现了C++和Python的VLCs原型，实验表明VLCs在包括使用OpenMP、OpenBLAS和LibTorch的应用程序基准测试中，能够实现高达2.85倍的加速。

Conclusion: VLCs提供了一种有效的解决方案，能够在不需要修改库代码或操作系统的情况下，解决并行库组合时的资源争用问题，显著提升性能。

Abstract: As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible.
  We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process.
  In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.

</details>


### [10] [Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity](https://arxiv.org/abs/2512.04355)
*Gregory Bolet,Giorgis Georgakoudis,Konstantinos Parasyris,Harshitha Menon,Niranjan Hasabnis,Kirk W. Cameron,Gal Oren*

Main category: cs.DC

TL;DR: gpuFLOPBench是一个评估LLM预测GPU内核浮点运算次数的基准测试，包含577个CUDA内核，揭示当前模型在涉及隐式FLOP操作时的严重误差。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在代码生成方面进展迅速，但缺乏对GPU性能前瞻性推理能力的测试，特别是预测浮点运算工作量这种影响调优、调度甚至硬件采购的关键能力。

Method: 构建包含577个CUDA内核的基准测试集，来自HeCBench，标注真实性能数据和8个执行属性，区分简单可分析代码与依赖编译器/运行时行为的复杂内核。

Result: 最新LLM在简单内核上能完美分类，但在涉及除法、内置数学函数或公共子表达式等隐式FLOP操作时，仍会出现数量级误差。

Conclusion: 现有代码助手存在核心局限：无法理解硬件特定的微码效应，gpuFLOPBench可作为开发能像经验丰富GPU开发者一样严格推理性能的LLM工具的重点测试平台。

Abstract: Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to "count without running" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench

</details>


### [11] [A Structure-Aware Irregular Blocking Method for Sparse LU Factorization](https://arxiv.org/abs/2512.04389)
*Zhen Hu,Dongliang Xiong,Kai Huang,Changjun Wu,Xiaowen Jiang*

Main category: cs.DC

TL;DR: 提出一种结构感知的不规则分块方法用于稀疏LU分解，通过对角线分块特征表征局部非零元分布，根据稀疏度调整分块大小，在GPU上实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 稀疏LU分解中，符号分解后的非零元倾向于分布在矩阵的对角线和右下区域，这种非均匀分布结构使得常规的2D分块方法会导致各分块间工作负载不平衡。现有的矩阵特征无法有效指导分块策略。

Method: 提出一种结构感知的不规则分块方法：1）引入新颖的对角线分块特征，有效表征稀疏矩阵的局部非零元分布；2）基于此特征提出不规则分块方法，根据局部非零元分布调整分块大小，在密集区域使用细粒度分块，在稀疏区域使用粗粒度分块，在依赖树的同一层级和跨层级间充分平衡各分块的非零元数量。

Result: 在单个NVIDIA A100 GPU上，相比PanguLU和最新的SuperLU_DIST，分别实现了平均1.50倍和3.32倍的加速。在4个NVIDIA A100 GPU上，相比PanguLU和SuperLU_DIST，分别实现了1.40倍和3.84倍的加速。

Conclusion: 提出的结构感知不规则分块方法能有效解决稀疏LU分解中的工作负载不平衡问题，通过自适应分块策略显著提升了GPU上的计算性能。

Abstract: In sparse LU factorization, nonzero elements after symbolic factorization tend to distribute in diagonal and right-bottom region of sparse matrices. However, regular 2D blocking on this non-uniform distribution structure may lead to workload imbalance across blocks. Besides, existing matrix features fail to guide us effectively in blocking. In this paper, we propose a structure-aware irregular blocking method for numerical factorization. A novel diagonal block-based feature is introduced to effectively characterize the local nonzero distribution of sparse matrices. Based on this, we further propose an irregular blocking method that adjusts block sizes according to the local distribution of nonzeros. The strategy utilizes fine-grained blocks in dense regions and coarse-grained blocks in sparse regions, adequately balancing the nonzeros of blocks both within the same level and across levels in the dependency tree. Experiments demonstrate that, on a single NVIDIA A100 GPU, our proposed irregular blocking method achieves average speedups of 1.50x and 3.32x over PanguLU and the latest SuperLU_DIST, respectively. In addition, it achieves speedups of 1.40x and 3.84x over PanguLU and SuperLU_DIST on 4 NVIDIA A100 GPUs.

</details>


### [12] [Offloading to CXL-based Computational Memory](https://arxiv.org/abs/2512.04449)
*Suyeon Lee,Kangkyu Park,Kwangsik Shin,Ada Gavrilovska*

Main category: cs.DC

TL;DR: KAI系统通过异步回传协议优化CXL计算内存，减少数据移动成本，提升性能50.4%


<details>
  <summary>Details</summary>
Motivation: 现有CXL计算内存的操作卸载机制无法充分利用不同CXL协议模型的权衡优势，无法有效处理多样化数据和处理需求的工作负载

Method: 提出"异步回传"协议，在底层CXL协议上分层数据和控制传输操作；设计KAI系统实现异步数据移动和轻量级流水线的主机-CCM交互

Result: KAI减少端到端运行时间最高达50.4%，CCM和主机空闲时间分别平均减少22.11倍和3.85倍

Conclusion: 异步回传协议和KAI系统能有效利用CXL协议权衡，显著提升解聚内存系统的性能和效率

Abstract: CXL-based Computational Memory (CCM) enables near-memory processing within expanded remote memory, presenting opportunities to address data movement costs associated with disaggregated memory systems and to accelerate overall performance. However, existing operation offloading mechanisms are not capable of leveraging the trade-offs of different models based on different CXL protocols. This work first examines these tradeoffs and demonstrates their impact on end-to-end performance and system efficiency for workloads with diverse data and processing requirements. We propose a novel 'Asynchronous Back-Streaming' protocol by carefully layering data and control transfer operations on top of the underlying CXL protocols. We design KAI, a system that realizes the asynchronous back-streaming model that supports asynchronous data movement and lightweight pipelining in host-CCM interactions. Overall, KAI reduces end-to-end runtime by up to 50.4%, and CCM and host idle times by average 22.11x and 3.85x, respectively.

</details>


### [13] [Federated Learning for Terahertz Wireless Communication](https://arxiv.org/abs/2512.04984)
*O. Tansel Baydas,Ozgur B. Akan*

Main category: cs.DC

TL;DR: THz通信与联邦学习结合时，宽带损伤（如波束倾斜、分子吸收）会导致收敛误差，传统聚合方法在频谱空洞处失效，需要SNR加权聚合来恢复收敛。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信与联邦学习的融合有望实现超快速分布式学习，但实际宽带损伤对优化动态的理论影响尚未得到充分研究。本文旨在填补这一空白，分析频率选择性THz效应对联邦学习收敛的影响。

Method: 开发了一个多载波随机框架，将本地梯度更新与频率选择性THz效应（包括波束倾斜、分子吸收和抖动）明确耦合。通过理论分析揭示了收敛误差与子载波SNR谐波均值的关系，并提出了SNR加权聚合策略来抑制频谱空洞处的方差奇异性。

Result: 分析发现存在"多样性陷阱"：在标准无偏聚合下，收敛误差由子载波SNR的谐波均值驱动，单个频谱空洞就可能导致整个带宽失效。同时识别出基本带宽限制，超过临界点会因热噪声和增益崩溃而降低收敛性能。SNR加权聚合能有效抑制方差奇异性，在高波束倾斜情况下恢复收敛。

Conclusion: THz-FL系统的性能受物理层参数显著影响，传统联邦学习聚合方法在宽带THz环境下可能失效。SNR加权聚合是必要的策略，能够克服频谱空洞问题，确保在现实THz损伤下的可靠收敛。该研究为设计鲁棒的THz联邦学习系统提供了理论指导。

Abstract: The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [FLEX: Leveraging FPGA-CPU Synergy for Mixed-Cell-Height Legalization Acceleration](https://arxiv.org/abs/2512.04527)
*Xingyu Liu,Jiawei Liang,Linfeng Du,Yipu Zhang,Chaofang Ma,Hanwei Fan,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: FLEX是一个FPGA-CPU混合加速器，用于混合单元高度合法化任务，通过优化任务分配、多粒度流水线技术和单元移位优化，实现了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: 解决混合单元高度合法化任务中的计算瓶颈，利用FPGA和CPU的互补优势来加速这一耗时的物理设计步骤。

Method: 1. 优化任务分配策略，在FPGA和CPU之间进行高效任务划分；2. 采用多粒度流水线技术加速最耗时的寻找最优放置位置步骤；3. 针对FOP中的计算密集型单元移位过程进行优化设计。

Result: 相比最先进的CPU-GPU和多线程CPU合法化器，FLEX实现了最高18.3倍和5.4倍的加速，同时具有更好的可扩展性，并将合法化质量分别提高了4%和1%。

Conclusion: FLEX通过FPGA-CPU协同加速策略，在混合单元高度合法化任务中实现了显著的性能提升和质量改进，展示了异构计算在物理设计自动化中的潜力。

Abstract: In this work, we present FLEX, an FPGA-CPU accelerator for mixed-cell-height legalization tasks. We address challenges from the following perspectives. First, we optimize the task assignment strategy and perform an efficient task partition between FPGA and CPU to exploit their complementary strengths. Second, a multi-granularity pipelining technique is employed to accelerate the most time-consuming step, finding optimal placement position (FOP), in legalization. At last, we particularly target the computationally intensive cell shifting process in FOP, optimizing the design to align it seamlessly with the multi-granularity pipelining framework for further speedup. Experimental results show that FLEX achieves up to 18.3x and 5.4x speedups compared to state-of-the-art CPU-GPU and multi-threaded CPU legalizers with better scalability, while improving legalization quality by 4% and 1%.

</details>


### [15] [Functional Stability of Software-Hardware Neural Network Implementation The NeuroComp Project](https://arxiv.org/abs/2512.04867)
*Bychkov Oleksii,Senysh Taras*

Main category: cs.AR

TL;DR: 提出一种通过神经元级硬件冗余确保神经网络功能稳定的创新方法，在硬件故障时仍能保持网络运行


<details>
  <summary>Details</summary>
Motivation: 传统Dropout方法仅用于训练阶段的正则化，无法应对网络运行时的硬件故障。需要一种能在硬件层面确保神经网络功能稳定性的方法。

Method: 将每个神经元部署在独立的微计算机（ESP32）上，通过硬件冗余实现容错。当单个计算节点故障时，系统仍能继续运行。

Result: 系统在硬件故障时仍能保持功能稳定，相比传统Dropout方法，该方法专门针对运行时的硬件故障提供保护。

Conclusion: 神经元级硬件冗余是确保神经网络功能稳定性的有效方法，特别适用于需要高可靠性的实时应用场景。

Abstract: This paper presents an innovative approach to ensuring functional stability of neural networks through hardware redundancy at the individual neuron level. Unlike the classical Dropout method, which is used during training for regularization purposes, the proposed system ensures resilience to hardware failures during network operation. Each neuron is implemented on a separate microcomputer (ESP32), allowing the system to continue functioning even when individual computational nodes fail.

</details>


### [16] [Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming](https://arxiv.org/abs/2512.04910)
*Fang Li*

Main category: cs.AR

TL;DR: 使用ASP实现自动条板电路布局设计，将布局问题转化为综合与多目标优化任务，同时生成可行布局并最小化板面积和元件条交叉。


<details>
  <summary>Details</summary>
Motivation: 传统条板电路布局设计复杂且耗时，需要自动化工具来简化电子原型制作和教育过程。ASP的声明式特性能够自然地表达复杂的几何和电气约束。

Method: 采用两阶段求解方法：首先确保布局可行性，然后优化布局质量。利用ASP的声明式编程表达几何约束（如元件位置、连接）和电气约束，同时最小化板面积和元件条交叉。

Result: 实验结果表明，该方法能为不同复杂度的电路生成紧凑、可制造的布局，验证了ASP在解决复杂设计自动化问题中的有效性。

Conclusion: 这项工作代表了自动条板布局设计的重大进展，为电子原型制作和教育提供了实用工具，同时展示了声明式编程在解决复杂设计自动化问题中的强大能力。

Abstract: This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.

</details>
