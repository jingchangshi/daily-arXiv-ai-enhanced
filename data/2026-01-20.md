<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Cutting Corners on Uncertainty: Zonotope Abstractions for Stream-based Runtime Monitoring](https://arxiv.org/abs/2601.11358)
*Bernd Finkbeiner,Martin Fränzle,Florian Kohn,Paul Kröger*

Main category: cs.PL

TL;DR: 该论文提出使用zonotopes作为在线监测RLola规范的抽象域，以解决传感器误差传播和内存无限增长的问题。


<details>
  <summary>Details</summary>
Motivation: 实时监测安全关键系统时，传感器存在校准和测量误差，这些误差会通过监测器的计算传播并扭曲最终判决。虽然仿射算术能精确跟踪这些误差，但独立的测量噪声会引入新的松弛变量，导致监测器状态表示随时间无限增长，因此需要一种有界内存的监测算法。

Method: 引入zonotopes作为在线监测RLola规范的抽象域。zonotopes能精确捕获监测器的仿射状态，通过其过近似产生一个有界内存的监测器。论文比较了不同zonotope过近似策略在运行时监测中的表现。

Result: zonotopes能精确捕获监测器的仿射状态，其过近似能产生一个可靠的有界内存监测器。通过比较不同zonotope过近似策略，评估了它们的性能和误报率。

Conclusion: zonotopes作为抽象域能有效解决传感器误差传播和内存无限增长的问题，为在线监测RLola规范提供了一种可靠的有界内存解决方案。

Abstract: Stream-based monitoring assesses the health of safety-critical systems by transforming input streams of sensor measurements into output streams that determine a verdict. These inputs are often treated as accurate representations of the physical state, although real sensors introduce calibration and measurement errors. Such errors propagate through the monitor's computations and can distort the final verdict. Affine arithmetic with symbolic slack variables can track these errors precisely, but independent measurement noise introduces a fresh slack variable upon each measurement event, causing the monitor's state representation to grow without bound over time. Therefore, any bounded-memory monitoring algorithm must unify slack variables at runtime in a way that generates a sound approximation.
  This paper introduces zonotopes as an abstract domain for online monitoring of RLola specifications. We demonstrate that zonotopes precisely capture the affine state of the monitor and that their over-approximation produces a sound bounded-memory monitor. We present a comparison of different zonotope over-approximation strategies in the context of runtime monitoring, evaluating their performance and false-positive rates.

</details>


### [2] [Qihe: A General-Purpose Static Analysis Framework for Verilog](https://arxiv.org/abs/2601.11408)
*Qinlin Chen,Nairen Zhang,Jinpeng Wang,Jiacai Cui,Tian Tan,Xiaoxing Ma,Chang Xu,Jian Lu,Yue Li*

Main category: cs.PL

TL;DR: Qihe是首个针对Verilog的通用静态分析框架，填补了硬件领域缺乏类似软件静态分析框架的空白，支持硬件特性分析并已发现多个真实bug和漏洞。


<details>
  <summary>Details</summary>
Motivation: 软件领域已有成熟的静态分析框架支持各种高级应用（bug检测、安全、程序理解），但硬件领域缺乏类似框架，限制了硬件静态分析的发展潜力。

Method: Qihe框架包含分析导向的前端、Verilog专用中间表示（IR）和一套基础分析组件，能够处理硬件特有特性如位向量运算、寄存器同步、数字组件并发等。

Result: 在真实硬件项目中发现了9个未知bug（已获开发者确认），识别了18个现有linter无法检测的bug，检测到16个真实硬件程序漏洞，框架包含10万+行代码并已开源。

Conclusion: Qihe填补了硬件静态分析框架的空白，展示了硬件静态分析的巨大潜力，通过开源旨在促进硬件分析生态的繁荣发展，达到软件分析生态的类似水平。

Abstract: In the past decades, static analysis has thrived in software, facilitating applications in bug detection, security, and program understanding. These advanced analyses are largely underpinned by general-purpose static analysis frameworks, which offer essential infrastructure to streamline their development. Conversely, hardware lacks such a framework, which overshadows the promising opportunities for sophisticated static analysis in hardware, hindering achievements akin to those witnessed in software. We thus introduce Qihe, the first general-purpose static analysis framework for Verilog -- a highly challenging endeavor given the absence of precedents in hardware. Qihe features an analysis-oriented front end, a Verilog-specific IR, and a suite of diverse fundamental analyses that capture essential hardware-specific characteristics -- such as bit-vector arithmetic, register synchronization, and digital component concurrency -- and enable the examination of intricate hardware data and control flows. These fundamental analyses are designed to support a wide array of hardware analysis clients. To validate Qihe's utility, we further developed a set of clients spanning bug detection, security, and program understanding. Our preliminary experimental results are highly promising; for example, Qihe uncovered 9 previously unknown bugs in popular real-world hardware projects (averaging 1.5K+ GitHub stars), all of which were confirmed by developers; moreover, Qihe successfully identified 18 bugs beyond the capabilities of existing static analyses for Verilog bug detection (i.e., linters), and detected 16 vulnerabilities in real-world hardware programs. By open-sourcing Qihe, which comprises over 100K lines of code, we aim to inspire further innovation and applications of sophisticated static analysis for hardware, aspiring to foster a similarly vibrant ecosystem that software analysis enjoys.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [AFLL: Real-time Load Stabilization for MMO Game Servers Based on Circular Causality Learning](https://arxiv.org/abs/2601.10998)
*Shinsuk Kang,Youngjae Kim*

Main category: cs.DC

TL;DR: AFLL系统通过实时学习服务器消息与客户端请求的因果关系，自适应调整消息权重，实现预测性限流，在保证关键消息的同时大幅降低服务器负载。


<details>
  <summary>Details</summary>
Motivation: MMO游戏服务器需要处理数千并发玩家并维持亚100ms响应时间。传统方法要么统一限流所有消息类型（损害游戏体验），要么使用固定的启发式规则无法适应动态工作负载。

Method: AFLL（自适应反馈循环学习）系统实时学习服务器发出消息与后续客户端请求之间的因果关系，使用反向传播持续调整消息类型权重，实现预测性限流，在过载发生前阻止低优先级消息，同时保证关键消息的传递。

Result: 在1000并发玩家的控制实验中，AFLL将平均CPU时间降低48.3%（13.2ms到6.8ms），峰值CPU时间降低51.7%（54.0ms到26.1ms），线程竞争降低64.4%（19.6%到7.0%）。系统通过后台计算和缓存优化实现零学习开销，并识别出消息阻塞到负载减少的三阶段因果链。

Conclusion: AFLL证明了循环因果学习能够为延迟关键系统提供实用的实时自适应能力，在保持高可重复性（所有指标CV<2%）的同时显著提升服务器性能。

Abstract: Massively Multiplayer Online (MMO) game servers must handle thousands of simultaneous players while maintaining sub-100ms response times. When server load exceeds capacity, traditional approaches either uniformly throttle all message types regardless of importance (damaging gameplay) or apply fixed heuristic rules that fail to adapt to dynamic workloads. This paper presents AFLL (Adaptive Feedback Loop Learning), a real-time load stabilization system that learns the causal relationship between outgoing server messages and subsequent incoming client requests. AFLL employs backpropagation to continuously adjust message type weights, enabling predictive throttling that blocks low-priority messages before overload occurs while guaranteeing critical message delivery. Through controlled experiments with 1,000 concurrent players, AFLL reduced average CPU time by 48.3% (13.2ms to 6.8ms), peak CPU time by 51.7% (54.0ms to 26.1ms), and thread contention by 64.4% (19.6% to 7.0%), while maintaining zero learning overhead through background computation and caching optimizations. The system achieved remarkable reproducibility (CV < 2% across all metrics) and identified a three-stage causal chain linking message blocking to load reduction. AFLL demonstrates that circular causality learning enables practical real-time adaptation for latency-critical systems.

</details>


### [4] [Konflux: Optimized Function Fusion for Serverless Applications](https://arxiv.org/abs/2601.11156)
*Niklas Kowallik,Trever Schirmer,David Bermbach*

Main category: cs.DC

TL;DR: 提出一个系统，通过模拟FaaS平台来分析所有可能的函数融合配置，以优化服务器less应用的成本和延迟，无需在生产环境中进行昂贵的基准测试。


<details>
  <summary>Details</summary>
Motivation: FaaS已成为服务器less云计算的核心范式，但优化FaaS部署仍然具有挑战性。函数融合可以将多个函数组合成单个部署单元，从而降低复杂服务器less应用的成本和延迟。然而，即使在小规模应用中，可能的融合配置数量也极其庞大，使得在生产环境中进行暴力基准测试既昂贵又耗时。

Method: 开发了一个系统，通过模拟FaaS平台来分析每个可能的融合配置。该系统支持本地实验，无需重新配置实时平台，显著降低了相关成本和时间。评估了多个示例FaaS应用和资源限制下的所有融合配置。

Result: 研究结果表明，在分析成本和延迟权衡时，只有有限的融合配置代表最优解决方案，这些配置受到特定定价模型的强烈影响。

Conclusion: 通过模拟FaaS平台进行本地分析的方法能够有效识别最优函数融合配置，为服务器less应用优化提供了实用且经济高效的解决方案，避免了在生产环境中进行昂贵基准测试的需求。

Abstract: Function-as-a-Service (FaaS) has become a central paradigm in serverless cloud computing, yet optimizing FaaS deployments remains challenging. Using function fusion, multiple functions can be combined into a single deployment unit, which can be used to reduce cost and latency of complex serverless applications comprising multiple functions. Even in small-scale applications, the number of possible fusion configurations is vast, making brute-force benchmarking in production both cost- and time-prohibitive.
  In this paper, we present a system that can analyze every possible fusion setup of complex applications. By emulating the FaaS platform, our system enables local experimentation, eliminating the need to reconfigure the live platform and significantly reducing associated cost and time. We evaluate all fusion configurations across a number of example FaaS applications and resource limits. Our results reveal that, when analyzing cost and latency trade-offs, only a limited set of fusion configurations represent optimal solutions, which are strongly influenced by the specific pricing model in use.

</details>


### [5] [Space-Optimal, Computation-Optimal, Topology-Agnostic, Throughput-Scalable Causal Delivery through Hybrid Buffering](https://arxiv.org/abs/2601.11487)
*Paulo Sérgio Almeida*

Main category: cs.DC

TL;DR: 提出一种新的因果顺序消息传递算法，结合发送方缓冲和接收方缓冲，实现恒定元数据开销和计算最优性能


<details>
  <summary>Details</summary>
Motivation: 现有的因果顺序消息传递方法存在严重缺陷：接收方缓冲方法需要大量元数据开销，不适用于大规模进程；发送方缓冲方法（如Cykas算法）存在吞吐量可扩展性和活性问题

Method: 提出SPS+FIFO策略：发送方缓冲确保发送许可（SPS），接收方缓冲确保FIFO顺序。设计新颖的混合算法，结合两种缓冲方式，使用精心选择的数据结构实现计算最优

Result: 算法克服了纯发送方缓冲的限制，实现每条消息恒定大小的元数据开销，具有摊销恒定处理开销，是计算最优的。据所知，这是首个具有这些特性的拓扑无关因果传递算法

Conclusion: 提出的混合缓冲算法解决了现有因果传递方法的局限性，实现了高效、可扩展的因果顺序消息传递，适用于大规模分布式系统

Abstract: Message delivery respecting causal ordering (causal delivery) is one of the most classic and widely useful abstraction for inter-process communication in a distributed system. Most approaches tag messages with causality information and buffer them at the receiver until they can be safely delivered. Except for specific approaches that exploit communication topology, therefore not generally applicable, they incur a metadata overhead which is prohibitive for a large number of processes. Much less used are the approaches that enforce causal order by buffering messages at the sender, until it is safe to release them to the network, as the classic algorithm has too many drawbacks. In this paper, first we discuss the limitations of sender-only buffering approaches and introduce the Sender Permission to Send (SPS) enforcement strategy, showing that SPS + FIFO implies Causal. We analyze a recent sender-buffering algorithm, Cykas, which follows SPS + FIFO, albeit very conservatively, pointing out throughput scalability and liveness issues. Then, we introduce a novel SPS + FIFO based algorithm, which adopts a new hybrid approach: enforcing causality by combining sender-buffering to enforce SPS and receiver-buffering to enforce FIFO. The algorithm overcomes limitations of sender-only buffering, and achieves effectively constant metadata size per message. By a careful choice of data-structures, the algorithm is also computationally-optimal, with amortized effectively constant processing overhead. As far as we know, there is no other topology-agnostic causal delivery algorithm with these properties.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [SwiftKV: An Edge-Oriented Attention Algorithm and Multi-Head Accelerator for Fast, Efficient LLM Decoding](https://arxiv.org/abs/2601.10953)
*Junming Zhang,Qinyan Zhang,Huajun Sun,Feiyang Gao,Sheng Hu,Rui Nie,Xiangshui Miao*

Main category: cs.AR

TL;DR: SwiftKV Attention：一种用于边缘加速器的单次注意力推理算法，通过每令牌流水线处理实现低延迟，无需分数物化或二次扫描，在边缘设备上实现7.16倍加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在边缘设备上的部署面临挑战：现有注意力推理算法在资源受限的边缘加速器上难以实现快速推理和高效解码，特别是多头并行解码支持有限。

Method: 提出SwiftKV Attention算法：每令牌流水线化的单次注意力推理，每个(k,v)对仅处理一次，无需分数物化、分块softmax或二次扫描。同时设计SwiftKV-MHA加速器，在同一处理器阵列上支持高精度注意力和低精度GEMV，实现多头并行解码。

Result: 在边缘加速器上，SwiftKV Attention相比原生注意力实现7.16倍加速；SwiftKV-MHA进一步将注意力延迟降低13.48倍，在相同设置下生成速度提升17.4%，令牌效率提高1.98倍。

Conclusion: SwiftKV Attention算法和SwiftKV-MHA加速器有效解决了边缘设备上LLM推理的挑战，实现了快速、高效的注意力计算和多头并行解码，为边缘AI应用提供了实用解决方案。

Abstract: Edge acceleration for large language models is crucial for their widespread application; however, achieving fast attention inference and efficient decoding on resource-constrained edge accelerators remains challenging. This paper presents SwiftKV Attention, a per-token pipelined, low-latency single-pass attention inference algorithm, where every (kt, vt) in the KV cache is processed exactly once in a uniform per-token pipeline without score materialization, blockwise softmax, or a second pass, thereby enabling fast execution on edge accelerators with a single hardware set and no resource-intensive parallelism. Furthermore, to address the limited support for multi-head LLM decoding in existing accelerators, we design the SwiftKV-MHA accelerator, which enables high precision attention and low precision GEMV on the same processor array, achieving fast and efficient multi-head parallel decoding. Experimental results show that, on the edge accelerator, the SwiftKV Attention algorithm achieves a 7.16* speedup over native attention and significantly outperforms other attention algorithms. SwiftKV-MHA further reduces attention latency by 13.48*; under the same settings, it improves generation speed by 17.4% and increases token efficiency by 1.98* compared with state-of-the-art works.

</details>


### [7] [RidgeWalker: Perfectly Pipelined Graph Random Walks on FPGAs](https://arxiv.org/abs/2601.11057)
*Hongshi Tan,Yao Chen,Xinyu Chen,Qizhen Zhang,Cheng Chen,Weng-Fai Wong,Bingsheng He*

Main category: cs.AR

TL;DR: RidgeWalker是一个基于FPGA的高性能图随机游走加速器，通过异步流水线架构和反馈驱动调度，实现了7.0倍于现有FPGA方案和8.1倍于GPU方案的加速效果。


<details>
  <summary>Details</summary>
Motivation: 图随机游走（GRW）在众多应用中用于近似计算图的关键属性，但其强数据依赖性、不规则内存访问模式和负载不均衡等特性使得加速困难。现有FPGA加速方案因流水线效率低下和静态调度而未能充分发挥硬件潜力。

Method: 基于GRW的马尔可夫性质，将任务分解为无状态的细粒度任务，支持乱序执行而不影响正确性。设计了异步流水线架构，采用基于排队论的反馈驱动调度器，实现完美流水化和自适应负载均衡。

Result: 在数据中心FPGA上原型实现，在多种GRW算法和真实图数据集上测试，平均加速比达到：相比最先进FPGA方案7.0倍，相比GPU方案8.1倍；峰值加速比分别达到71.0倍和22.9倍。

Conclusion: RidgeWalker通过创新的异步流水线架构和自适应调度机制，显著提升了图随机游走的计算效率，为数据中心FPGA上的图计算加速提供了有效解决方案。

Abstract: Graph Random Walks (GRWs) offer efficient approximations of key graph properties and have been widely adopted in many applications. However, GRW workloads are notoriously difficult to accelerate due to their strong data dependencies, irregular memory access patterns, and imbalanced execution behavior. While recent work explores FPGA-based accelerators for GRWs, existing solutions fall far short of hardware potential due to inefficient pipelining and static scheduling. This paper presents RidgeWalker, a high-performance GRW accelerator designed for datacenter FPGAs. The key insight behind RidgeWalker is that the Markov property of GRWs allows decomposition into stateless, fine-grained tasks that can be executed out-of-order without compromising correctness. Building on this, RidgeWalker introduces an asynchronous pipeline architecture with a feedback-driven scheduler grounded in queuing theory, enabling perfect pipelining and adaptive load balancing. We prototype RidgeWalker on datacenter FPGAs and evaluated it across a range of GRW algorithms and real-world graph datasets. Experimental results demonstrate that RidgeWalker achieves an average speedup of 7.0x over state-of-the-art FPGA solutions and 8.1x over GPU solutions, with peak speedups of up to 71.0x and 22.9x, respectively. The source code is publicly available at https://github.com/Xtra-Computing/RidgeWalker.

</details>


### [8] [OpenACM: An Open-Source SRAM-Based Approximate CiM Compiler](https://arxiv.org/abs/2601.11292)
*Yiqi Zhou,JunHao Ma,Xingyang Li,Yule Sheng,Yue Yuan,Yikai Wang,Bochang Wang,Yiheng Wu,Shan Shen,Wei Xing,Daying Sun,Li Li,Zhiqiang Xiao*

Main category: cs.AR

TL;DR: OpenACM是首个开源、精度感知的SRAM近似计算内存编译器，通过集成可配置精度乘法器库，在保证AI应用精度的同时实现高达64%的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 数据密集型AI工作负载加剧了"内存墙"瓶颈，SRAM数字计算内存(DCiM)虽提供可扩展解决方案，但其庞大设计空间使手动设计不切实际。现有DCiM编译器仅关注精确计算，未能利用AI应用的误差容忍特性进行近似计算优化。

Method: 开发OpenACM开源编译器，集成精度可配置乘法器库（精确、可调近似和对数），自动化生成DCiM架构。结合晶体管级可定制SRAM宏和变异感知特性分析，基于OpenROAD和FreePDK45库构建完整的开源物理设计流程。

Result: 在代表性卷积神经网络上的实验表明，OpenACM在保持应用精度可忽略损失的同时，实现了高达64%的能耗节省。该框架完全开源，确保可复现性和可访问性。

Conclusion: OpenACM填补了应用误差容忍与硬件自动化之间的空白，为SRAM近似DCiM架构提供了首个开源、精度感知的编译器解决方案，实现了细粒度的精度-能耗权衡，并消除了对专有工具的依赖。

Abstract: The rise of data-intensive AI workloads has exacerbated the ``memory wall'' bottleneck. Digital Compute-in-Memory (DCiM) using SRAM offers a scalable solution, but its vast design space makes manual design impractical, creating a need for automated compilers. A key opportunity lies in approximate computing, which leverages the error tolerance of AI applications for significant energy savings. However, existing DCiM compilers focus on exact arithmetic, failing to exploit this optimization. This paper introduces OpenACM, the first open-source, accuracy-aware compiler for SRAM-based approximate DCiM architectures. OpenACM bridges the gap between application error tolerance and hardware automation. Its key contribution is an integrated library of accuracy-configurable multipliers (exact, tunable approximate, and logarithmic), enabling designers to make fine-grained accuracy-energy trade-offs. The compiler automates the generation of the DCiM architecture, integrating a transistor-level customizable SRAM macro with variation-aware characterization into a complete, open-source physical design flow based on OpenROAD and the FreePDK45 library. This ensures full reproducibility and accessibility, removing dependencies on proprietary tools. Experimental results on representative convolutional neural networks (CNNs) demonstrate that OpenACM achieves energy savings of up to 64\% with negligible loss in application accuracy. The framework is available on \href{https://github.com/ShenShan123/OpenACM}{OpenACM:URL}

</details>
