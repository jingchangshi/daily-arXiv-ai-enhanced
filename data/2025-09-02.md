<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
*Nripesh Niketan,Vaatsalya Shrivastva*

Main category: cs.PL

TL;DR: CrossTL是一个通用的编程语言翻译器，通过统一的中间表示CrossGL实现多种编程语言之间的双向翻译，支持CUDA、HIP、Metal、HLSL、GLSL、SPIR-V、Rust和Mojo等语言。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要为每对语言单独开发翻译器，导致复杂度呈指数级增长。CrossTL旨在通过统一的中间表示解决这个问题，实现"一次编写，到处部署"的开发模式。

Method: 系统包含语言特定的词法分析器/解析器将源代码转换为AST，双向CrossGL翻译模块（ToCrossGLConverter类用于导入代码，CodeGen类用于目标生成），以及处理完整翻译流程的后端实现。

Result: 通过跨编程领域的全面评估，证明系统在所有支持的后端上都能成功编译和执行，验证了通用代码翻译的实际可行性。

Conclusion: CrossTL代表了向语言无关编程迈出的重要一步，统一的IR设计使得添加新语言只需最少的努力，仅需要语言特定的前端/后端组件。

Abstract: We present CrossTL, a universal programming language translator enabling
bidirectional translation between multiple languages through a unified
intermediate representation called CrossGL. Traditional approaches require
separate translators for each language pair, leading to exponential complexity
growth. CrossTL uses a single universal IR to facilitate translations between
CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,
with Slang support in development. Our system consists of: language-specific
lexers/parsers converting source code to ASTs, bidirectional CrossGL
translation modules implementing ToCrossGLConverter classes for importing code
and CodeGen classes for target generation, and comprehensive backend
implementations handling full translation pipelines. We demonstrate
effectiveness through comprehensive evaluation across programming domains,
achieving successful compilation and execution across all supported backends.
The universal IR design enables adding new languages with minimal effort,
requiring only language-specific frontend/backend components. Our contributions
include: (1) a unified IR capturing semantics of multiple programming
paradigms, (2) a modular architecture enabling extensibility, (3) a
comprehensive framework supporting GPU compute, graphics programming, and
systems languages, and (4) empirical validation demonstrating practical
viability of universal code translation. CrossTL represents a significant step
toward language-agnostic programming, enabling write-once, deploy-everywhere
development.

</details>


### [2] [Growing Mathlib: maintenance of a large scale mathematical library](https://arxiv.org/abs/2508.21593)
*Anne Baanen,Matthew Robert Ballard,Johan Commelin,Bryan Gin-ge Chen,Michael Rothgang,Damiano Testa*

Main category: cs.PL

TL;DR: Mathlib是增长最快的数学形式化库之一，本文描述了管理其增长、允许变更并避免维护者过载的各种策略。


<details>
  <summary>Details</summary>
Motivation: 随着Mathlib数学库的快速增长，需要有效的策略来管理库的扩展，同时保持代码质量和维护效率，避免维护者负担过重。

Method: 采用多种策略：通过弃用系统处理破坏性变更、使用代码质量分析工具（linter）提供用户反馈、通过有意识的库（重新）设计加速编译时间、处理技术债务以及编写自定义工具帮助审查和分类新贡献。

Result: 开发了一套综合的库管理方法，能够有效应对大型数学形式化库的增长挑战，提高了维护效率和代码质量。

Conclusion: 通过系统化的管理策略和工具支持，Mathlib能够在快速增长的同时保持高质量和可维护性，为大型形式化数学库的管理提供了有效解决方案。

Abstract: The Lean mathematical library Mathlib is one of the fastest-growing libraries
of formalised mathematics. We describe various strategies to manage this
growth, while allowing for change and avoiding maintainer overload. This
includes dealing with breaking changes via a deprecation system, using code
quality analysis tools (linters) to provide direct user feedback about common
pitfalls, speeding up compilation times through conscious library (re-)design,
dealing with technical debt as well as writing custom tooling to help with the
review and triage of new contributions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Fast and Scalable Mixed Precision Euclidean Distance Calculations Using GPU Tensor Cores](https://arxiv.org/abs/2508.21230)
*Brian Curless,Michael Gowanlock*

Main category: cs.DC

TL;DR: 提出FaSTED算法，利用GPU张量核心的FP16-32混合精度计算欧氏距离，相比现有FP64算法获得2.5-51倍加速，精度损失小于0.06%


<details>
  <summary>Details</summary>
Motivation: 现代GPU张量核心具有高计算吞吐量，但之前研究仅使用FP64精度。利用FP16-32混合精度可以显著提升欧氏距离计算性能，特别是在数据密集型应用中

Method: 设计FaSTED算法，通过层次化数据重用和最大化各级内存（全局内存、共享内存、寄存器）利用率来实现高计算吞吐量，应用于相似性搜索场景

Result: 在4个真实高维数据集（128-960维）上，混合精度方法相比现有FP64算法获得2.5-51倍加速，精度损失小于0.06%

Conclusion: 利用张量核心的FP16-32混合精度计算欧氏距离是高效可行的，在保持高精度的同时显著提升性能，适用于数据分析和相似性搜索应用

Abstract: Modern GPUs are equipped with tensor cores (TCs) that are commonly used for
matrix multiplication in artificial intelligence workloads. However, because
they have high computational throughput, they can lead to significant
performance gains in other algorithms if they can be successfully exploited. We
examine using TCs to compute Euclidean distance calculations, which are used in
many data analytics applications. Prior work has only investigated using 64 bit
floating point (FP64) data for computation; however, TCs can operate on lower
precision floating point data (i.e., 16 bit matrix multiplication and 32 bit
accumulation), which we refer to as FP16-32. FP16-32 TC peak throughput is so
high that TCs are easily starved of data. We propose a Fast and Scalable Tensor
core Euclidean Distance (FaSTED) algorithm. To achieve high computational
throughput, we design FaSTED for significant hierarchical reuse of data and
maximize memory utilization at every level (global memory, shared memory, and
registers). We apply FaSTED to the application of similarity searches, which
typically employ an indexing data structure to eliminate superfluous Euclidean
distance calculations. We compare to the state-of-the-art (SOTA) TC Euclidean
distance algorithm in the literature that employs FP64, as well as to two
single precision (FP32) CUDA core algorithms that both employ an index. We find
that across four real-world high-dimensional datasets spanning 128-960
dimensions, the mixed-precision brute force approach achieves a speedup over
the SOTA algorithms of 2.5-51x. We also quantify the accuracy loss of our mixed
precision algorithm to be less than <0.06% when compared to the FP64 baseline.

</details>


### [4] [Decentralized Federated Averaging via Random Walk](https://arxiv.org/abs/2508.21286)
*Changheng Wang,Zhiqing Wei,Lizhe Liu,Qiao Deng,Yingda Wu,Yangyang Niu,Yashan Pang,Zhiyong Feng*

Main category: cs.DC

TL;DR: 提出了一种基于随机游走的去中心化联邦平均算法DFedRW，通过随机游走更新替代多步本地更新，解决了异构数据下的收敛问题，并支持部分聚合和量化以提高通信效率。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在异构和不平衡数据下存在收敛慢和次优模型问题，且集中式架构有单点故障和隐私风险。去中心化联邦学习虽能提高鲁棒性和隐私性，但仍面临收敛效率挑战。

Method: 提出DFedRW算法，用随机游走更新替代多步本地更新；允许聚合部分随机游走更新以避免忽略掉队者；进一步提出量化版本以提高通信效率。

Result: 理论证明在凸条件下达到O(1/k^(1-q))收敛上界；数值分析显示在高异构性下测试准确率比传统FedAvg提高38.3%和37.5%。

Conclusion: DFedRW算法有效解决了去中心化联邦学习在异构数据下的收敛问题，同时通过量化平衡了通信效率和收敛性能，显著优于传统方法。

Abstract: Federated Learning (FL) is a communication-efficient distributed machine
learning method that allows multiple devices to collaboratively train models
without sharing raw data. FL can be categorized into centralized and
decentralized paradigms. The centralized paradigm relies on a central server to
aggregate local models, potentially resulting in single points of failure,
communication bottlenecks, and exposure of model parameters. In contrast, the
decentralized paradigm, which does not require a central server, provides
improved robustness and privacy. The essence of federated learning lies in
leveraging multiple local updates for efficient communication. However, this
approach may result in slower convergence or even convergence to suboptimal
models in the presence of heterogeneous and imbalanced data. To address this
challenge, we study decentralized federated averaging via random walk (DFedRW),
which replaces multiple local update steps on a single device with random walk
updates. Traditional Federated Averaging (FedAvg) and its decentralized
versions commonly ignore stragglers, which reduces the amount of training data
and introduces sampling bias. Therefore, we allow DFedRW to aggregate partial
random walk updates, ensuring that each computation contributes to the model
update. To further improve communication efficiency, we also propose a
quantized version of DFedRW. We demonstrate that (quantized) DFedRW achieves
convergence upper bound of order $\mathcal{O}(\frac{1}{k^{1-q}})$ under convex
conditions. Furthermore, we propose a sufficient condition that reveals when
quantization balances communication and convergence. Numerical analysis
indicates that our proposed algorithms outperform (decentralized) FedAvg in
both convergence rate and accuracy, achieving a 38.3\% and 37.5\% increase in
test accuracy under high levels of heterogeneities.

</details>


### [5] [Addressing Reproducibility Challenges in HPC with Continuous Integration](https://arxiv.org/abs/2508.21289)
*Valérie Hayot-Sasson,Nathaniel Hudson,André Bauer,Maxime Gonthier,Ian Foster,Kyle Chard*

Main category: cs.DC

TL;DR: 本文提出CORRECT工具，通过GitHub Action实现HPC应用的远程安全测试执行，以解决高性能计算领域因基础设施限制导致的复现性挑战


<details>
  <summary>Details</summary>
Motivation: HPC社区虽然设立了复现性激励机制，但由于独特的基础设施和严格访问限制，许多论文难以满足复现性要求。缺乏资源访问的情况下，需要替代方案来确保研究可复现

Method: 开发了CORRECT GitHub Action工具，支持在远程HPC资源上安全执行测试，结合持续集成(CI)和完整的溯源信息来替代直接资源访问

Result: 在三种不同类型的HPC应用上评估了CORRECT的可用性，证明该工具能有效自动化并记录复现性评估过程

Conclusion: 更好的HPC兼容CI解决方案将提高应用程序的复现性，CORRECT工具为解决HPC复现性障碍提供了实用方案

Abstract: The high-performance computing (HPC) community has adopted incentive
structures to motivate reproducible research, with major conferences awarding
badges to papers that meet reproducibility requirements. Yet, many papers do
not meet such requirements. The uniqueness of HPC infrastructure and software,
coupled with strict access requirements, may limit opportunities for
reproducibility. In the absence of resource access, we believe that regular
documented testing, through continuous integration (CI), coupled with complete
provenance information, can be used as a substitute. Here, we argue that better
HPC-compliant CI solutions will improve reproducibility of applications. We
present a survey of reproducibility initiatives and describe the barriers to
reproducibility in HPC. To address existing limitations, we present a GitHub
Action, CORRECT, that enables secure execution of tests on remote HPC
resources. We evaluate CORRECT's usability across three different types of HPC
applications, demonstrating the effectiveness of using CORRECT for automating
and documenting reproducibility evaluations.

</details>


### [6] [A Knowledge Distillation-empowered Adaptive Federated Reinforcement Learning Framework for Multi-Domain IoT Applications Scheduling](https://arxiv.org/abs/2508.21328)
*Zhiyu Wang,Mohammad Goudarzi,Mingming Gong,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 提出了KD-AFRL框架，通过知识蒸馏和自适应联邦强化学习解决IoT多域调度问题，在收敛速度、完成时间、能耗和成本方面显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: IoT应用在云边端异构环境中面临分布式调度优化的挑战，包括固定神经网络架构与计算异构性不兼容、非IID数据分布以及跨域协作机制不足等问题。

Method: 1. 资源感知的混合架构生成机制，创建双区域神经网络；2. 隐私保护的基于环境聚类的联邦学习方法，使用差分隐私和K-means聚类；3. 环境导向的跨架构知识蒸馏机制，通过温度调节软目标实现异构模型间的知识迁移。

Result: 实验显示比最佳基线方法收敛速度快21%，在完成时间、能耗和加权成本方面分别提升15.7%、10.8%和13.9%。可扩展性实验中，随着域数量增加，性能保持能力比现有解决方案好3-5倍。

Conclusion: KD-AFRL框架有效解决了IoT多域调度中的异构性、非IID数据和跨域协作挑战，在真实云边端基础设施中表现出优异的性能和可扩展性。

Abstract: The rapid proliferation of Internet of Things (IoT) applications across
heterogeneous Cloud-Edge-IoT environments presents significant challenges in
distributed scheduling optimization. Existing approaches face issues, including
fixed neural network architectures that are incompatible with computational
heterogeneity, non-Independent and Identically Distributed (non-IID) data
distributions across IoT scheduling domains, and insufficient cross-domain
collaboration mechanisms. This paper proposes KD-AFRL, a Knowledge
Distillation-empowered Adaptive Federated Reinforcement Learning framework that
addresses multi-domain IoT application scheduling through three core
innovations. First, we develop a resource-aware hybrid architecture generation
mechanism that creates dual-zone neural networks enabling heterogeneous devices
to participate in collaborative learning while maintaining optimal resource
utilization. Second, we propose a privacy-preserving environment-clustered
federated learning approach that utilizes differential privacy and K-means
clustering to address non-IID challenges and facilitate effective collaboration
among compatible domains. Third, we introduce an environment-oriented
cross-architecture knowledge distillation mechanism that enables efficient
knowledge transfer between heterogeneous models through temperature-regulated
soft targets. Comprehensive experiments with real Cloud-Edge-IoT infrastructure
demonstrate KD-AFRL's effectiveness using diverse IoT applications. Results
show significant improvements over the best baseline, with 21% faster
convergence and 15.7%, 10.8%, and 13.9% performance gains in completion time,
energy consumption, and weighted cost, respectively. Scalability experiments
reveal that KD-AFRL achieves 3-5 times better performance retention compared to
existing solutions as the number of domains increases.

</details>


### [7] [Unpacking Maximum Extractable Value on Polygon: A Study on Atomic Arbitrage](https://arxiv.org/abs/2508.21473)
*Daniil Vostrikov,Yash Madhwal,Andrey Seoev,Anastasiia Smirnova,Yury Yanovich,Alexey Smirnov,Vladimir Gorgadze*

Main category: cs.DC

TL;DR: 本文分析了Polygon区块链上的最大可提取价值(MEV)，特别关注原子套利交易，发现垃圾邮件式交易更普遍但拍卖式交易更盈利。


<details>
  <summary>Details</summary>
Motivation: 随着区块链技术从加密货币扩展到DeFi等更广泛应用，MEV成为重要挑战，需要深入研究其在Polygon等网络中的表现和影响。

Method: 使用22个月、2300万个区块的数据集，建立原子套利交易识别标准，分析搜索者行为、竞价动态和代币使用等关键因素，重点研究垃圾邮件式和拍卖式回跑策略。

Result: 垃圾邮件式交易更普遍，但拍卖式交易显示出更高的盈利能力。研究揭示了网络架构、交易排序和MEV提取之间的相互作用。

Conclusion: 研究结果强调了需要强大的交易排序机制，并指出了新兴MEV策略对区块链网络的影响，为去中心化生态系统中MEV的演变和挑战提供了全面见解。

Abstract: The evolution of blockchain technology, from its origins as a decentralized
ledger for cryptocurrencies to its broader applications in areas like
decentralized finance (DeFi), has significantly transformed financial
ecosystems while introducing new challenges such as Maximum Extractable Value
(MEV). This paper explores MEV on the Polygon blockchain, with a particular
focus on Atomic Arbitrage (AA) transactions. We establish criteria for
identifying AA transactions and analyze key factors such as searcher behavior,
bidding dynamics, and token usage. Utilizing a dataset spanning 22 months and
covering 23 million blocks, we examine MEV dynamics with a focus on Spam-based
and Auction-based backrunning strategies. Our findings reveal that while
Spam-based transactions are more prevalent, Auction-based transactions
demonstrate greater profitability. Through detailed examples and analysis, we
investigate the interactions between network architecture, transaction
sequencing, and MEV extraction, offering comprehensive insights into the
evolution and challenges of MEV in decentralized ecosystems. These results
emphasize the need for robust transaction ordering mechanisms and highlight the
implications of emerging MEV strategies for blockchain networks.

</details>


### [8] [Odyssey: Adaptive Policy Selection for Resilient Distributed Training](https://arxiv.org/abs/2508.21613)
*Yuhang Zhou,Zhibin Wang,Peng Jiang,Haoran Xia,Junhe Lu,Qianyu Jiang,Rong Gu,Hengxi Xu,Xinjing Huang,Guanghuan Fang,Zhiheng Hu,Jingyi Zhang,Yongjin Cai,Jian He,Chen Tian*

Main category: cs.DC

TL;DR: Odyssey是一个自适应容错系统，通过智能选择最优恢复策略来减少大语言模型训练中的中断影响，在32卡集群上实现恢复后性能与无故障训练仅11%的差距


<details>
  <summary>Details</summary>
Motivation: 现有无备份方法（冗余计算、动态并行、数据重路由）都存在性能损失问题，需要更高效的故障恢复方案

Method: 使用统一性能模型、快速执行计划搜索、准确性能估计和高效通信优化来智能选择最优恢复策略

Result: 在32卡集群上，恢复后训练性能与无故障训练仅差11.00%，平均吞吐量比Oobleck和Recycle分别高1.229倍和1.355倍

Conclusion: Odyssey系统能够有效处理训练中断，保持模型收敛性和内存使用效率，显著优于现有方法

Abstract: Training large language models faces frequent interruptions due to various
faults, demanding robust fault-tolerance. Existing backup-free methods, such as
redundant computation, dynamic parallelism, and data rerouting, each incur
performance penalties, whether from ongoing overhead, lengthy reconfigurations,
or post-recovery inefficiencies. We propose Odyssey, an adaptive fault-tolerant
system that intelligently selects optimal recovery strategies when a failure
occurs. Odyssey achieves this through a unified performance model, expedient
execution plan search, accurate performance estimation, and efficient
communication optimizations. Experiments on a 32-card cluster show that Odyssey
maintains a performance gap of within 11.00% between post-recovery and
failure-free training, while preserving model convergence and efficient memory
usage. Compared to state-of-the-art methods, Odyssey achieves up to 1.229x and
1.355x higher average throughput than Oobleck and Recycle, respectively.

</details>


### [9] [Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency with Speculative Decoding](https://arxiv.org/abs/2508.21706)
*Zhibin Wang,Zhonghui Zhang,Yuhang Zhou,Zibo Wang,Mo Zhou,Peng Jiang,Weilin Cai,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: SpecMoEOff利用推测解码技术扩大专家模型工作负载，通过理论分析和经验分析协调GPU和CPU资源，开发专用CPU注意力验证内核，并集成自动优化器调参，在MoE模型推理中实现最高2.5倍的解码吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 现有的MoE模型卸载技术由于I/O瓶颈和稀疏计算问题，导致硬件利用率低下，需要新的方法来充分利用硬件资源

Method: 提出SpecMoEOff方法，采用推测解码技术扩大专家工作负载，通过理论分析和经验分析协调GPU和CPU，开发专用CPU分块注意力验证内核，并集成自动优化器调参

Result: 实验结果显示，SpecMoEOff相比最先进的MoE卸载技术，实现了最高2.5倍的解码吞吐量提升

Conclusion: SpecMoEOff通过推测解码和硬件资源优化，有效解决了MoE模型推理中的硬件利用率问题，显著提升了推理性能

Abstract: Recent advancements in Mixture of Experts (MoE) models have significantly
increased their parameter scale as well as model performance. Extensive
offloading techniques have been proposed to address the GPU memory limitations
of MoE inference. However, due to the I/O bottleneck and sparse computation of
MoE models, existing offloading techniques still suffer from low hardware
utilization. To fully utilize the hardware resources, we propose SpecMoEOff,
which employs the speculative decoding technique to enlarge the workload of
each expert. SpecMoEOff orchestrates the GPU and CPU by both theoretical and
empirical roofline analysis. In addition, we develop a dedicated CPU chunked
attention verification kernel to fit the speculative decoding in offloading
scenarios as well as minimizing the additional overhead led by draft models.
SpecMoEOff further integrates an optimizer to automatically tune the
hyperparameters of speculative decoding under given hardware and workload.
Experimental results show that SpecMoEOff achieves up to 2.5x decode throughput
improvement over the state-of-the-art MoE offloading techniques.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [SCE-NTT: A Hardware Accelerator for Number Theoretic Transform Using Superconductor Electronics](https://arxiv.org/abs/2508.21265)
*Sasan Razmkhah,Mingye Li,Zeming Cheng,Robert S. Aviles,Kyle Jackman,Joey Delport,Lieze Schindler,Wenhui Luo,Takuya Suzuki,Mehdi Kamal,Christopher L. Ayala,Coenrad J. Fourie,Nabuyuki Yoshikawa,Peter A. Beerel,Sandeep Gupta,Massoud Pedram*

Main category: cs.AR

TL;DR: 基于超导电子学(SCE)的专用硬件加速器SCE-NTT，针对全同态加密中的数论变换(NTT)瓶颈，实现了531百万次NTT/秒的高性能，比最先进CMOS方案快100倍以上


<details>
  <summary>Details</summary>
Motivation: 全同态加密(FHE)中的数论变换(NTT)是计算瓶颈，传统CMOS技术面临性能限制，需要探索超导电子学来突破性能边界并实现高能效

Method: 提出基于超导单磁通量子(SFQ)逻辑和存储器的专用硬件加速器，采用深度流水线架构，使用移位寄存器内存(SRM)解决SFQ约束，开发了包含50多个参数化单元的新RSFQ单元库

Result: NTT-128单元在34GHz频率下达到531百万次NTT/秒，比现有CMOS方案快100倍以上；可扩展至2^14点NTT约482ns完成；密钥切换吞吐量估计为163万次操作/秒

Conclusion: SCE基加速器在后量子时代具有实现可扩展、高能效安全计算的巨大潜力，随着制造技术的进步预计将获得更大收益

Abstract: This research explores the use of superconductor electronics (SCE) for
accelerating fully homomorphic encryption (FHE), focusing on the
Number-Theoretic Transform (NTT), a key computational bottleneck in FHE
schemes. We present SCE-NTT, a dedicated hardware accelerator based on
superconductive single flux quantum (SFQ) logic and memory, targeting high
performance and energy efficiency beyond the limits of conventional CMOS. To
address SFQ constraints such as limited dense RAM and restricted fanin/fanout,
we propose a deeply pipelined NTT-128 architecture using shift register memory
(SRM). Designed for N=128 32-bit coefficients, NTT-128 comprises log2(N)=7
processing elements (PEs), each featuring a butterfly unit (BU), dual
coefficient memories operating in ping-pong mode via FIFO-based SRM queues, and
twiddle factor buffers. The BU integrates a Shoup modular multiplier optimized
for a small area, leveraging precomputed twiddle factors. A new RSFQ cell
library with over 50 parameterized cells, including compound logic units, was
developed for implementation. Functional and timing correctness were validated
using JoSIM analog simulations and Verilog models. A multiphase clocking scheme
was employed to enhance robustness and reduce path-balancing overhead,
improving circuit reliability. Fabricated results show the NTT-128 unit
achieves 531 million NTT/sec at 34 GHz, over 100x faster than state-of-the-art
CMOS equivalents. We also project that the architecture can scale to larger
sizes, such as a 2^14-point NTT in approximately 482 ns. Key-switch throughput
is estimated at 1.63 million operations/sec, significantly exceeding existing
hardware. These results demonstrate the strong potential of SCE-based
accelerators for scalable, energy-efficient secure computation in the
post-quantum era, with further gains anticipated through advances in
fabrication.

</details>


### [11] [Catwalk: Unary Top-K for Efficient Ramp-No-Leak Neuron Design for Temporal Neural Networks](https://arxiv.org/abs/2508.21267)
*Devon Lister,Prabhu Vellaisamy,John Paul Shen,Di Wu*

Main category: cs.AR

TL;DR: 提出Catwalk神经元实现，通过重排脉冲序列中的稀疏脉冲来降低硬件成本，在面积和功耗效率上相比现有SRM0-RNL神经元分别提升1.39倍和1.86倍


<details>
  <summary>Details</summary>
Motivation: 现有SRM-RNL神经元实现假设所有输入都携带脉冲，但实际脉冲序列中只有少量输入在计算周期内真正携带脉冲，这种稀疏性可以被利用来提高硬件效率

Method: 通过一元top-k方法将脉冲序列中的脉冲重排为排序后的子集簇，显著降低后续并行计数器(PC)积累响应函数的成本

Result: 布局布线结果显示，Catwalk在面积和功耗方面分别比现有SRM0-RNL神经元好1.39倍和1.86倍

Conclusion: Catwalk神经元实现通过利用脉冲序列的稀疏性，有效提高了RNL神经元实现的面积和功率效率

Abstract: Temporal neural networks (TNNs) are neuromorphic neural networks that utilize
bit-serial temporal coding. TNNs are composed of columns, which in turn employ
neurons as their building blocks. Each neuron processes volleys of input
spikes, modulated by associated synaptic weights, on its dendritic inputs.
Recently proposed neuron implementation in CMOS employs a Spike Response Model
(SRM) with a ramp-no-leak (RNL) response function and assumes all the inputs
can carry spikes. However, in actual spike volleys, only a small subset of the
dendritic inputs actually carry spikes in each compute cycle. This form of
sparsity can be exploited to achieve better hardware efficiency. In this paper,
we propose a Catwalk neuron implementation by relocating spikes in a spike
volley as a sorted subset cluster via unary top-k. Such relocation can
significantly reduce the cost of the subsequent parallel counter (PC) for
accumulating the response functions from the spiking inputs. This can lead to
improvements on area and power efficiency in RNL neuron implementation.
Place-and-route results show Catwalk is 1.39x and 1.86x better in area and
power, respectively, as compared to existing SRM0-RNL neurons.

</details>


### [12] [SIRA: Scaled-Integer Range Analysis for Optimizing FPGA Dataflow Neural Network Accelerators](https://arxiv.org/abs/2508.21493)
*Yaman Umuroglu,Christoph Berganski,Felix Jentzsch,Michal Danilowicz,Tomasz Kryjak,Charalampos Bezaitis,Magnus Sjalander,Ian Colbert,Thomas Preusser,Jakoba Petri-Koenig,Michaela Blott*

Main category: cs.AR

TL;DR: SIRA是一种静态分析技术，通过区间运算确定量化神经网络中张量的范围、尺度和偏置，用于优化FPGA数据流神经网络加速器的资源占用。


<details>
  <summary>Details</summary>
Motivation: 在嵌入式系统中，激进的量化会暴露非矩阵乘法操作作为显著的性能和资源瓶颈，需要全面的精度定制方法来解决这些问题。

Method: 引入scaled-integer range analysis (SIRA)静态分析技术，使用区间运算确定量化神经网络中张量的范围、尺度和偏置信息，并利用这些信息进行位宽适配、尺度和偏置聚合、连续元素操作转换为阈值操作等优化。

Result: 平均减少17%的LUT、66%的DSP和22%的累加器位宽，提供了详细的基准分析和分析模型来指导非矩阵层的实现风格。

Conclusion: SIRA优化能显著减少FPGA加速器的资源占用，已开源SIRA以便社区在各种应用和硬件平台上探索其优势。

Abstract: While neural network quantization effectively reduces the cost of matrix
multiplications, aggressive quantization can expose non-matrix-multiply
operations as significant performance and resource bottlenecks on embedded
systems. Addressing such bottlenecks requires a comprehensive approach to
tailoring the precision across operations in the inference computation. To this
end, we introduce scaled-integer range analysis (SIRA), a static analysis
technique employing interval arithmetic to determine the range, scale, and bias
for tensors in quantized neural networks. We show how this information can be
exploited to reduce the resource footprint of FPGA dataflow neural network
accelerators via tailored bitwidth adaptation for accumulators and downstream
operations, aggregation of scales and biases, and conversion of consecutive
elementwise operations to thresholding operations. We integrate SIRA-driven
optimizations into the open-source FINN framework, then evaluate their
effectiveness across a range of quantized neural network workloads and compare
implementation alternatives for non-matrix-multiply operations. We demonstrate
an average reduction of 17% for LUTs, 66% for DSPs, and 22% for accumulator
bitwidths with SIRA optimizations, providing detailed benchmark analysis and
analytical models to guide the implementation style for non-matrix layers.
Finally, we open-source SIRA to facilitate community exploration of its
benefits across various applications and hardware platforms.

</details>


### [13] [Binary Weight Multi-Bit Activation Quantization for Compute-in-Memory CNN Accelerators](https://arxiv.org/abs/2508.21524)
*Wenyong Zhou,Zhengwu Liu,Yuan Ren,Ngai Wong*

Main category: cs.AR

TL;DR: 提出了一种新型的二进制权重多比特激活(BWMA)方法，用于存内计算加速器上的CNN，在保持硬件效率的同时显著提升精度


<details>
  <summary>Details</summary>
Motivation: 现有方法要么使用二进制权重和激活以硬件效率为代价牺牲精度，要么使用多比特权重和激活获得更高精度但效率有限，需要找到平衡方案

Method: 推导每层权重量化的闭式解以提升二值化权重表示能力；开发可微分激活量化函数近似理想多比特函数，避免大量搜索最优设置

Result: 在CIFAR-10和ImageNet数据集上分别获得1.44%-5.46%和0.35%-5.37%的精度提升；硬件仿真显示4位激活量化在硬件成本和模型性能间达到最优平衡

Conclusion: BWMA方法成功解决了存内计算加速器中量化精度与硬件效率的权衡问题，为高效CNN部署提供了有效解决方案

Abstract: Compute-in-memory (CIM) accelerators have emerged as a promising way for
enhancing the energy efficiency of convolutional neural networks (CNNs).
Deploying CNNs on CIM platforms generally requires quantization of network
weights and activations to meet hardware constraints. However, existing
approaches either prioritize hardware efficiency with binary weight and
activation quantization at the cost of accuracy, or utilize multi-bit weights
and activations for greater accuracy but limited efficiency. In this paper, we
introduce a novel binary weight multi-bit activation (BWMA) method for CNNs on
CIM-based accelerators. Our contributions include: deriving closed-form
solutions for weight quantization in each layer, significantly improving the
representational capabilities of binarized weights; and developing a
differentiable function for activation quantization, approximating the ideal
multi-bit function while bypassing the extensive search for optimal settings.
Through comprehensive experiments on CIFAR-10 and ImageNet datasets, we show
that BWMA achieves notable accuracy improvements over existing methods,
registering gains of 1.44\%-5.46\% and 0.35\%-5.37\% on respective datasets.
Moreover, hardware simulation results indicate that 4-bit activation
quantization strikes the optimal balance between hardware cost and model
performance.

</details>
