<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Hybrid Game Control Envelope Synthesis](https://arxiv.org/abs/2508.05997)
*Aditi Kabra,Jonathan Laurent,Stefan Mitsch,André Platzer*

Main category: cs.PL

TL;DR: 本文研究了嵌入式系统（如汽车和火车）的控制问题，通过双玩家混合游戏建模，提出了一种尽可能宽松的非确定性获胜策略合成方法。


<details>
  <summary>Details</summary>
Motivation: 解决嵌入式系统中复杂控制问题的需求，通过混合游戏模型实现安全控制。

Method: 引入子值映射作为策略的组成表示，结合微分游戏逻辑（dGL）进行验证和合成。

Result: 证明了最大子值映射的存在性及其逻辑特性，并开发了相关算法。

Conclusion: 该方法在多样化控制挑战中表现出高效性和表达力。

Abstract: Control problems for embedded systems like cars and trains can be modeled by
two-player hybrid games. Control envelopes, which are families of safe control
solutions, correspond to nondeterministic winning policies of hybrid games,
where each deterministic specialization of the policy is a control solution.
This paper synthesizes nondeterministic winning policies for hybrid games that
are as permissive as possible. It introduces subvalue maps, a compositional
representation of such policies that enables verification and synthesis along
the structure of the game. An inductive logical characterization in
differential game logic (dGL) checks whether a subvalue map induces a sound
control envelope which always induces a winning play. A policy is said to win
if it always achieves the desirable outcome when the player follows it, no
matter what actions the opponent plays. The maximal subvalue map, which allows
the most action options while still winning, is shown to exist and satisfy a
logical characterization. A family of algorithms for nondeterministic policy
synthesis can be obtained from the inductive subvalue map soundness
characterization. An implementation of these findings is evaluated on examples
that use the expressivity of dGL to model a range of diverse control
challenges.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Accelerating Data Chunking in Deduplication Systems using Vector Instructions](https://arxiv.org/abs/2508.05797)
*Sreeharsha Udayashankar,Abdelrahman Baba,Samer Al-Kiswany*

Main category: cs.DC

TL;DR: VectorCDC利用向量CPU指令加速无哈希CDC算法，显著提升性能，不影响去重空间节省。


<details>
  <summary>Details</summary>
Motivation: CDC算法因需扫描整个文件而成为性能瓶颈，亟需加速方法。

Method: 采用向量CPU指令（如SSE/AVX）优化无哈希CDC算法。

Result: 在多种CPU上实现8.35x-26.2x的吞吐量提升，不影响去重效果。

Conclusion: VectorCDC是一种高效且兼容性强的CDC加速方案。

Abstract: Content-defined Chunking (CDC) algorithms dictate the overall space savings
that deduplication systems achieve. However, due to their need to scan each
file in its entirety, they are slow and often the main performance bottleneck
within data deduplication. We present VectorCDC, a method to accelerate
hashless CDC algorithms using vector CPU instructions, such as SSE / AVX. Our
evaluation shows that VectorCDC is effective on Intel, AMD, ARM, and IBM CPUs,
achieving 8.35x - 26.2x higher throughput than existing vector-accelerated
techniques without affecting the deduplication space savings.

</details>


### [3] [A Dynamic Approach to Load Balancing in Cloud Infrastructure: Enhancing Energy Efficiency and Resource Utilization](https://arxiv.org/abs/2508.05821)
*Shadman Sakib,Ajay Katangur,Rahul Dubey*

Main category: cs.DC

TL;DR: 论文提出了一种基于实时性能指标的动态负载均衡器（SBDLB），显著提升了云计算的资源利用率和系统效率。


<details>
  <summary>Details</summary>
Motivation: 随着云计算的快速发展，负载均衡成为关键挑战，需要动态分配用户请求以维持性能和防止服务器过载。

Method: 采用Score-Based Dynamic Load Balancer（SBDLB），基于实时性能指标分配工作负载，并在CloudSim 7G平台上与节流负载均衡策略进行比较。

Result: SBDLB在多种场景下表现优异，平均响应时间提升34%-37%，数据中心处理时间减少13%，24小时模拟中运营成本降低15%。

Conclusion: SBDLB通过动态适应工作负载波动，优化资源使用，显著提升了云计算的性能和能效。

Abstract: Cloud computing has grown rapidly in recent years, mainly due to the sharp
increase in data transferred over the internet. This growth makes load
balancing a key part of cloud systems, as it helps distribute user requests
across servers to maintain performance, prevent overload, and ensure a smooth
user experience. Despite its importance, managing server resources and keeping
workloads balanced over time remains a major challenge in cloud environments.
This paper introduces a novel Score-Based Dynamic Load Balancer (SBDLB) that
allocates workloads to virtual machines based on real-time performance metrics.
The objective is to enhance resource utilization and overall system efficiency.
The method was thoroughly tested using the CloudSim 7G platform, comparing its
performance against the throttled load balancing strategy. Evaluations were
conducted across a variety of workloads and scenarios, demonstrating the
SBDLB's ability to adapt dynamically to workload fluctuations while optimizing
resource usage. The proposed method outperformed the throttled strategy,
improving average response times by 34% and 37% in different scenarios. It also
reduced data center processing times by an average of 13%. Over a 24-hour
simulation, the method decreased operational costs by 15%, promoting a more
energy-efficient and sustainable cloud infrastructure through reduced energy
consumption.

</details>


### [4] [Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML Next To Your Data](https://arxiv.org/abs/2508.05904)
*Brandon Baker,Elliott Brossard,Chenwei Xie,Zihao Ye,Deen Liu,Yijun Xie,Arthur Zwiegincew,Nitya Kumar Sharma,Gaurav Jain,Eugene Retunsky,Mike Halcrow,Derek Denny-Brown,Istvan Cseri,Tyler Akidau,Yuxiong He*

Main category: cs.DC

TL;DR: Snowpark是Snowflake推出的一个托管解决方案，支持数据工程和AI/ML工作负载，具有高性能、强安全性和易用性。


<details>
  <summary>Details</summary>
Motivation: Snowflake希望通过Snowpark扩展其AI Data Cloud愿景，支持更多编程语言（如Python）和复杂工作负载。

Method: Snowpark采用弹性可扩展架构，与Snowflake核心计算基础设施无缝集成，利用控制平面进行分布式计算，并通过安全沙箱隔离SQL和Snowpark工作负载。

Result: Snowpark通过Python包缓存减少查询延迟，优化工作负载调度，并高效管理数据倾斜，提升了性能。

Conclusion: 实际案例展示了Snowpark在大规模数据工程和AI/ML任务中的高效性和有效性。

Abstract: Snowflake revolutionized data analytics with an elastic architecture that
decouples compute and storage, enabling scalable solutions supporting data
architectures like data lake, data warehouse, data lakehouse, and data mesh.
Building on this foundation, Snowflake has advanced its AI Data Cloud vision by
introducing Snowpark, a managed turnkey solution that supports data engineering
and AI and ML workloads using Python and other programming languages.
  This paper outlines Snowpark's design objectives towards high performance,
strong security and governance, and ease of use. We detail the architecture of
Snowpark, highlighting its elastic scalability and seamless integration with
Snowflake core compute infrastructure. This includes leveraging Snowflake
control plane for distributed computing and employing a secure sandbox for
isolating Snowflake SQL workloads from Snowpark executions. Additionally, we
present core innovations in Snowpark that drive further performance
enhancements, such as query initialization latency reduction through Python
package caching, improved workload scheduling for customized workloads, and
data skew management via efficient row redistribution. Finally, we showcase
real-world case studies that illustrate Snowpark's efficiency and effectiveness
for large-scale data engineering and AI and ML tasks.

</details>


### [5] [KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training](https://arxiv.org/abs/2508.06001)
*Kai Zhang,Peng Wang,Sai Bi,Jianming Zhang,Yuanjun Xiong*

Main category: cs.DC

TL;DR: KnapFormer是一个高效框架，结合工作负载平衡和序列并行性，用于分布式训练Diffusion Transformers（DiT）。它通过解决全局背包问题重新分配令牌，减少工作负载差异，显著提升训练速度。


<details>
  <summary>Details</summary>
Motivation: 在混合分辨率和图像-视频联合训练中，可变长度文本输入和视觉令牌数量导致工作负载不平衡，影响训练效率。KnapFormer旨在解决这一问题。

Method: KnapFormer通过收集序列长度元数据并解决全局背包问题，最小化每GPU工作负载的方差，同时结合序列并行性。

Result: 在真实训练任务中，KnapFormer实现小于1%的工作负载差异，消除拖尾效应，训练速度提升2至3倍。

Conclusion: KnapFormer通过高效负载平衡和序列并行性，显著提升Diffusion Transformers的训练效率，适用于复杂数据场景。

Abstract: We present KnapFormer, an efficient and versatile framework to combine
workload balancing and sequence parallelism in distributed training of
Diffusion Transformers (DiT). KnapFormer builds on the insight that strong
synergy exists between sequence parallelism and the need to address the
significant token imbalance across ranks. This imbalance arises from
variable-length text inputs and varying visual token counts in mixed-resolution
and image-video joint training. KnapFormer redistributes tokens by first
gathering sequence length metadata across all ranks in a balancing group and
solving a global knapsack problem. The solver aims to minimize the variances of
total workload per-GPU, while accounting for the effect of sequence
parallelism. By integrating DeepSpeed-Ulysees-based sequence parallelism in the
load-balancing decision process and utilizing a simple semi-empirical workload
model, KnapFormers achieves minimal communication overhead and less than 1%
workload discrepancy in real-world training workloads with sequence length
varying from a few hundred to tens of thousands. It eliminates straggler
effects and achieves 2x to 3x speedup when training state-of-the-art diffusion
models like FLUX on mixed-resolution and image-video joint data corpora. We
open-source the KnapFormer implementation at
https://github.com/Kai-46/KnapFormer/

</details>


### [6] [EC2MoE: Adaptive End-Cloud Pipeline Collaboration Enabling Scalable Mixture-of-Experts Inference](https://arxiv.org/abs/2508.06024)
*Zheming Yang,Yunqing Hu,Sheng Sun,Wen Ji*

Main category: cs.DC

TL;DR: EC2MoE是一个自适应框架，通过端云协作优化混合专家（MoE）模型的推理效率，解决了专家调度、通信开销和资源异构性问题。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型在异构端云环境中部署时的专家调度、通信开销和资源异构性挑战。

Method: 设计了硬件感知的轻量级组门网络和端云协作的流水线优化机制，包括低秩压缩和动态调度算法。

Result: 实验表明，EC2MoE在保持高精度的同时，吞吐量提升2.2x至5.1x，端到端延迟降低53%至67%。

Conclusion: EC2MoE在动态负载和网络环境下表现出良好的可扩展性和高效性。

Abstract: The Mixture-of-Experts (MoE) paradigm has emerged as a promising solution to
scale up model capacity while maintaining inference efficiency. However,
deploying MoE models across heterogeneous end-cloud environments poses new
challenges in expert scheduling, communication overhead, and resource
heterogeneity. In this paper, we propose EC2MoE, an adaptive framework for
scalable MoE inference via end-cloud pipeline collaboration. First, we design a
hardware-aware lightweight group gate network that enhances expert selection
and computational efficiency. By incorporating a hardware-aware local expert
selection mechanism, the system adaptively filters candidate experts based on
real-time device profiles. A lightweight group gate module then integrates
local and global gating outputs to achieve high-quality expert routing with
minimal overhead. Second, we develop a pipeline optimization mechanism based on
endcloud collaboration to accelerate MoE inference. This includes an
encoder-decoder structure based on low-rank compression, which reduces
transmission and computation costs. And a route-aware heuristic pipeline
scheduling algorithm that dynamically allocates inference stages across devices
according to workload and network topology. Extensive experiments show that
EC2MoE can increase throughput by 2.2x to 5.1x and reduce end-to-end latency by
53% to 67% while maintaining high accuracy compared to state-of-the-art
methods. It also maintains good scalability under dynamic load and network
environments.

</details>


### [7] [KV Cache Compression for Inference Efficiency in LLMs: A Review](https://arxiv.org/abs/2508.06297)
*Yanyu Liu,Jingying Fu,Sixiang Liu,Yitian Zou,You Fu,Jiehan Zhou,Shouhua Zhang*

Main category: cs.DC

TL;DR: 该论文综述了大型语言模型（LLMs）推理过程中键值（KV）缓存的优化技术，分析了压缩策略的优缺点，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs推理上下文长度的增加，KV缓存需求呈指数增长，导致内存瓶颈，影响推理效率和可扩展性。因此，优化KV缓存至关重要。

Method: 系统审查了当前KV缓存优化技术，包括选择性令牌策略、量化和注意力压缩等压缩策略，并评估其效果、权衡和应用场景。

Result: 分析了这些方法对内存使用和推理速度的影响，指出了现有方法的局限性和挑战，如与不同模型和任务的兼容性问题。

Conclusion: 提出了未来研究方向，包括混合优化技术、自适应动态策略和软硬件协同设计，以提高推理效率并促进LLMs的实际应用。

Abstract: Withtherapid advancement of large language models (LLMs), the context length
for inference has been continuously increasing, leading to an exponential
growth in the demand for Key-Value (KV) caching. This has resulted in a
significant memory bottleneck, limiting the inference efficiency and
scalability of the models. Therefore, optimizing the KV cache during inference
is crucial for enhancing performance and efficiency. This review systematically
examines current KV cache optimization techniques, including compression
strategies such as selective token strategies, quantization, and attention
compression. We evaluate the effectiveness, trade-offs, and application
scenarios of these methods, providing a comprehensive analysis of their impact
on memory usage and inference speed. We focus on identifying the limitations
and challenges of existing methods, such as compatibility issues with different
models and tasks. Additionally, this review highlights future research
directions, including hybrid optimization techniques, adaptive dynamic
strategies, and software-hardware co-design. These approaches aim to improve
inference efficiency and promote the practical application of large language
models.

</details>


### [8] [Performant Unified GPU Kernels for Portable Singular Value Computation Across Hardware and Precision](https://arxiv.org/abs/2508.06339)
*Evelyne Ringoot,Rabab Alomairy,Valentin Churavy,Alan Edelman*

Main category: cs.DC

TL;DR: 本文介绍了一种基于Julia的便携式、GPU加速的QR奇异值计算算法实现，支持多种GPU架构和数据类型，性能优于多数线性代数库。


<details>
  <summary>Details</summary>
Motivation: 奇异值分解（SVD）是科学计算和机器学习中的基础工具，尤其在大型机器学习模型（如LLMs）中用于低秩适应（LoRA）。现有实现缺乏对Apple Metal GPU和半精度的支持，且性能不足。

Method: 采用经典的两阶段QR约简算法，结合Julia的多重分派和元编程能力，集成GPUArrays和KernelAbstractions框架，实现硬件无关的统一函数。

Result: 在多种GPU后端和数据类型上，性能优于MAGMA、SLATE等库，对大型矩阵（>1024x1024）达到cuSOLVER的80%-90%性能。

Conclusion: 该实现证明了便携性不牺牲性能，首次支持Apple Metal GPU和半精度，为大规模SVD计算提供了高效解决方案。

Abstract: This paper presents a portable, GPU-accelerated implementation of a QR-based
singular value computation algorithm in Julia. The singular value ecomposition
(SVD) is a fundamental numerical tool in scientific computing and machine
learning, providing optimal low-rank matrix approximations. Its importance has
increased even more in large-scale machine learning pipelines, including large
language models (LLMs), where it enables low-rank adaptation (LoRA). The
implemented algorithm is based on the classic two-stage QR reduction,
consisting of successive matrix reduction to band form and bidiagonal form. Our
implementation leverages Julia's multiple dispatch and metaprogramming
capabilities, integrating with the GPUArrays and KernelAbstractions frameworks
to provide a unified type and hardware-agnostic function. It supports diverse
GPU architectures and data types, and is, to our knowledge, the first
GPU-accelerated singular value implementation to support Apple Metal GPUs and
half precision. Performance results on multiple GPU backends and data types
demonstrate that portability does not require sacrificing performance: the
unified function outperforms most linear algebra libraries (MAGMA, SLATE,
rocSOLVER, oneMKL) for matrix sizes larger than 1024x1024, and achieves 80%-90%
of the performance of cuSOLVER for large matrices.

</details>


### [9] [Blockchain-Enabled Federated Learning](https://arxiv.org/abs/2508.06406)
*Murtaza Rangwala,Venugopal K R,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 区块链联邦学习（BCFL）通过四维分类法分析架构，解决协作AI系统中的信任、隐私和协调问题。


<details>
  <summary>Details</summary>
Motivation: 解决协作AI系统中的信任、隐私和协调挑战。

Method: 通过四维分类法（协调结构、共识机制、存储架构和信任模型）分析BCFL系统，研究设计模式和共识机制。

Result: 展示了BCFL在医疗、金融和物联网中的实际应用，性能接近集中式方法，同时提供更强的安全保障。

Conclusion: BCFL系统在协作智能中实现了透明性和容错性，验证了其实际可行性。

Abstract: Blockchain-enabled federated learning (BCFL) addresses fundamental challenges
of trust, privacy, and coordination in collaborative AI systems. This chapter
provides comprehensive architectural analysis of BCFL systems through a
systematic four-dimensional taxonomy examining coordination structures,
consensus mechanisms, storage architectures, and trust models. We analyze
design patterns from blockchain-verified centralized coordination to fully
decentralized peer-to-peer networks, evaluating trade-offs in scalability,
security, and performance. Through detailed examination of consensus mechanisms
designed for federated learning contexts, including Proof of Quality and Proof
of Federated Learning, we demonstrate how computational work can be repurposed
from arbitrary cryptographic puzzles to productive machine learning tasks. The
chapter addresses critical storage challenges by examining multi-tier
architectures that balance blockchain's transaction constraints with neural
networks' large parameter requirements while maintaining cryptographic
integrity. A technical case study of the TrustMesh framework illustrates
practical implementation considerations in BCFL systems through distributed
image classification training, demonstrating effective collaborative learning
across IoT devices with highly non-IID data distributions while maintaining
complete transparency and fault tolerance. Analysis of real-world deployments
across healthcare consortiums, financial services, and IoT security
applications validates the practical viability of BCFL systems, achieving
performance comparable to centralized approaches while providing enhanced
security guarantees and enabling new models of trustless collaborative
intelligence.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [ConiQ: Enabling Concatenated Quantum Error Correction on Neutral Atom Arrays](https://arxiv.org/abs/2508.05779)
*Pengyu Liu,Mingkuan Xu,Hengyun Zhou,Hanrui Wang,Umut A. Acar,Yunong Shi*

Main category: cs.AR

TL;DR: 提出了一种针对中性原子阵列的高效编译方法，显著降低了时空开销和编译时间。


<details>
  <summary>Details</summary>
Motivation: 解决现有级联码在实现可寻址逻辑门和高并行性需求方面的挑战。

Method: 引入AHA逻辑CNOT门和VAIR中间表示，通过ConiQ编译器优化。

Result: 时空开销降低2000倍，编译时间减少10^6倍，AHA门额外降低20倍开销。

Conclusion: 级联码为近期的容错量子计算提供了可行方案。

Abstract: Recent progress on concatenated codes, especially many-hypercube codes,
achieves unprecedented space efficiency. Yet two critical challenges persist in
practice. First, these codes lack efficient implementations of addressable
logical gates. Second, the required high degree of parallelism and long-range
interactions pose significant challenges for current hardware platforms. In
this paper, we propose an efficient compilation approach for concatenated
codes, specifically many-hypercube codes, targeted at neutral atom arrays,
which provide the necessary parallelism and long-range interactions. Our
approach builds on two key innovations. First, we introduce
Automorphism-assisted Hierarchical Addressing (AHA) logical CNOT gates that
significantly reduce spacetime overhead compared to conventional
distillation-based methods. Second, we develop Virtual Atom Intermediate
Representation (VAIR) that enables level-wise optimization and legalization. We
implement these innovations in ConiQ, a hardware-aware quantum compiler
designed to compile fault-tolerant quantum circuits for neutral atom arrays
using many-hypercube codes. Our evaluation demonstrates that ConiQ achieves up
to 2000x reduction in spacetime overhead and up to 10^6x reduction in
compilation time compared to state-of-the-art compilers, with our AHA gates
providing an additional overhead reduction of up to 20x. These results
establish concatenated codes as a promising approach for fault-tolerant quantum
computing in the near future.

</details>


### [11] [ArchXBench: A Complex Digital Systems Benchmark Suite for LLM Driven RTL Synthesis](https://arxiv.org/abs/2508.06047)
*Suresh Purini,Siddhant Garg,Mudit Gaur,Sankalp Bhat,Sohan Mupparapu,Arun Ravindran*

Main category: cs.AR

TL;DR: ArchXBench是一个六级基准测试套件，用于评估LLM在复杂数字系统设计中的能力，结果显示当前LLM在高级别任务中表现不足。


<details>
  <summary>Details</summary>
Motivation: 现代SoC数据路径中，RTL实现和验证仍依赖手工，LLM在Verilog类RTL中的应用尚不成熟，需要更复杂的基准测试。

Method: 引入ArchXBench基准套件，包含复杂算术电路和高级数字子系统，提供问题描述、设计规范和测试平台。

Result: 在零样本提示下，o4-mini-high在30个基准中解决了16个（级别1-3），但从级别4开始所有模型均失败。

Conclusion: 当前LLM和提示/代理方法在复杂数字系统设计中的能力存在明显不足，需进一步研究。

Abstract: Modern SoC datapaths include deeply pipelined, domain-specific accelerators,
but their RTL implementation and verification are still mostly done by hand.
While large language models (LLMs) exhibit advanced code-generation abilities
for programming languages like Python, their application to Verilog-like RTL
remains in its nascent stage. This is reflected in the simple arithmetic and
control circuits currently used to evaluate generative capabilities in existing
benchmarks. In this paper, we introduce ArchXBench, a six-level benchmark suite
that encompasses complex arithmetic circuits and other advanced digital
subsystems drawn from domains such as cryptography, image processing, machine
learning, and signal processing. Architecturally, some of these designs are
purely combinational, others are multi-cycle or pipelined, and many require
hierarchical composition of modules. For each benchmark, we provide a problem
description, design specification, and testbench, enabling rapid research in
the area of LLM-driven agentic approaches for complex digital systems design.
  Using zero-shot prompting with Claude Sonnet 4, GPT 4.1, o4-mini-high, and
DeepSeek R1 under a pass@5 criterion, we observed that o4-mini-high
successfully solves the largest number of benchmarks, 16 out of 30, spanning
Levels 1, 2, and 3. From Level 4 onward, however, all models consistently fail,
highlighting a clear gap in the capabilities of current state-of-the-art LLMs
and prompting/agentic approaches.

</details>


### [12] [Nail: Not Another Fault-Injection Framework for Chisel-generated RTL](https://arxiv.org/abs/2508.06344)
*Robin Sehm,Christian Ewert,Rainer Buchty,Mladen Berekovic,Saleh Mulhem*

Main category: cs.AR

TL;DR: Nail是一个基于Chisel的开源故障注入框架，通过状态依赖的故障模型和运行时软件控制，提高了故障模拟的精确性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有Chisel故障注入框架在指令级控制上过于粗糙，限制了故障建模的精确性。

Method: Nail引入状态依赖的故障模型，支持运行时通过软件修改触发状态，并自动生成软件接口。

Result: 在RISC-V处理器上验证了状态依赖故障注入，仿真和FPGA模拟中资源开销低于1%。

Conclusion: Nail填补了仿真速度、软件易用性和模拟可控性之间的差距。

Abstract: Fault simulation and emulation are essential techniques for evaluating the
dependability of integrated circuits, enabling early-stage vulnerability
analysis and supporting the implementation of effective mitigation strategies.
High-level hardware description languages such as Chisel facilitate the rapid
development of complex fault scenarios with minimal modification to the design.
However, existing Chisel-based fault injection (FI) frameworks are limited by
coarse-grained, instruction-level controllability, restricting the precision of
fault modeling. This work introduces Nail, a Chisel-based open-source FI
framework that overcomes these limitations by introducing state-based faults.
This approach enables fault scenarios that depend on specific system states,
rather than solely on instruction-level triggers, thereby removing the need for
precise timing of fault activation. For greater controllability, Nail allows
users to arbitrarily modify internal trigger states via software at runtime. To
support this, Nail automatically generates a software interface, offering
straightforward access to the instrumented design. This enables fine-tuning of
fault parameters during active FI campaigns - a feature particularly beneficial
for FPGA emulation, where synthesis is time-consuming. Utilizing these
features, Nail narrows the gap between the high speed of emulation-based FI
frameworks, the usability of software-based approaches, and the controllability
achieved in simulation. We demonstrate Nail's state-based FI and software
framework by modeling a faulty general-purpose register in a RISC-V processor.
Although this might appear straightforward, it requires state-dependent FI and
was previously impossible without fundamental changes to the design. The
approach was validated in both simulation and FPGA emulation, where the
addition of Nail introduced less than 1% resource overhead.

</details>
