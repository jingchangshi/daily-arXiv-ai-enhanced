<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Doc2Spec: Synthesizing Formal Programming Specifications from Natural Language via Grammar Induction](https://arxiv.org/abs/2602.04892)
*Shihao Xia,Mengting He,Haomin Jia,Linhai Song*

Main category: cs.PL

TL;DR: Doc2Spec：使用多智能体框架和LLM从自然语言规则自动推导规范语法，并基于该语法生成形式化规范，提高API实现和使用的合规性验证效率。


<details>
  <summary>Details</summary>
Motivation: 确保API实现和使用符合自然语言编程规则对软件正确性、安全性和可靠性至关重要。形式化验证能提供强保证但需要精确的规范，而手动编写这些规范既困难又成本高昂。

Method: 提出Doc2Spec多智能体框架，使用LLM从自然语言规则自动推导规范语法，然后基于推导出的语法引导生成形式化规范。该语法捕获关键领域知识，约束规范空间，并强制执行一致表示。

Result: 在三种编程语言的七个基准测试上评估，Doc2Spec优于无语法推导的基线方法，并与使用手动构建语法的技术取得竞争性结果，证明了自动语法推导在形式化自然语言规则方面的有效性。

Conclusion: Doc2Spec通过自动语法推导显著提高了形式化规范生成的可靠性和质量，为解决手动编写规范的高成本问题提供了有效方案。

Abstract: Ensuring that API implementations and usage comply with natural language programming rules is critical for software correctness, security, and reliability. Formal verification can provide strong guarantees but requires precise specifications, which are difficult and costly to write manually. To address this challenge, we present Doc2Spec, a multi-agent framework that uses LLMs to automatically induce a specification grammar from natural-language rules and then generates formal specifications guided by the induced grammar. The grammar captures essential domain knowledge, constrains the specification space, and enforces consistent representations, thereby improving the reliability and quality of generated specifications. Evaluated on seven benchmarks across three programming languages, Doc2Spec outperforms a baseline without grammar induction and achieves competitive results against a technique with a manually crafted grammar, demonstrating the effectiveness of automated grammar induction for formalizing natural-language rules.

</details>


### [2] [Strong Normalisation for Asynchronous Effects](https://arxiv.org/abs/2602.05528)
*Danel Ahman,Ilja Sobolev*

Main category: cs.PL

TL;DR: 该论文研究了异步效应演算的归一化性质，证明了在移除一般递归后，该演算（包括顺序和并行部分）是强归一化的，并且顺序片段在重新引入受控的中断驱动递归行为后仍保持强归一化。


<details>
  <summary>Details</summary>
Motivation: Ahman和Pretnar的异步效应为代数计算效应提供了异步处理能力，能够建模抢占式多线程、可取消远程函数调用、多方应用等场景。然而，原始演算包含一般递归，无法保证终止性。因此需要研究其归一化性质，以理解在何种条件下异步效应演算能够保证终止。

Method: 采用Lindley和Stark基于⊤⊤-提升的方法的结构化扩展来证明强归一化。首先从原始演算中移除一般递归，证明剩余演算（包括顺序和并行部分）是强归一化的。然后，在顺序片段中重新引入受控的中断驱动递归行为，证明其仍保持强归一化。所有证明都在Agda中进行了形式化验证。

Result: 1. 移除一般递归后，整个异步效应演算（包括顺序和并行部分）是强归一化的；2. 顺序片段在重新引入受控的中断驱动递归行为后仍保持强归一化；3. 所有结果都在Agda中得到了形式化验证。

Conclusion: 该研究为异步效应演算提供了重要的终止性保证，扩展了Lindley和Stark的⊤⊤-提升方法，使其能够处理异步效应。结果表明，通过适当限制递归行为，可以在保持终止性的同时支持有意义的异步编程模式。形式化验证增强了结果的可靠性。

Abstract: Asynchronous effects of Ahman and Pretnar complement the conventional synchronous treatment of algebraic computational effects with asynchrony based on decoupling the execution of algebraic operation calls into signalling that an operation's implementation needs to be executed, and into interrupting a running computation with the operation's result, to which the computation can react by installing matching interrupt handlers. Beyond providing asynchrony for algebraic effects, the resulting core calculus also naturally models examples such as pre-emptive multi-threading, (cancellable) remote function calls, multi-party applications, and even a parallel variant of runners of algebraic effects. In this paper, we study the normalisation properties of this calculus. We prove that if one removes general recursion from the original calculus, then the remaining calculus is strongly normalising, including both its sequential and parallel parts. However, this only guarantees termination for very simple asynchronous examples. To improve on this result, we also prove that the sequential fragment of the calculus remains strongly normalising when a controlled amount of interrupt-driven recursive behaviour is reintroduced. Our strong normalisation proofs are structured compositionally as a natural extension of Lindley and Stark's $\top\top$-lifting based approach for proving strong normalisation of effectful languages. All our results are also formalised in Agda.

</details>


### [3] [An Equational Axiomatization of Dynamic Threads via Algebraic Effects: Presheaves on Finite Relations, Labelled Posets, and Parameterized Algebraic Theories](https://arxiv.org/abs/2602.05850)
*Ohad Kammar,Jack Liell-Cock,Sam Lindley,Cristina Matache,Sam Staton*

Main category: cs.PL

TL;DR: 使用代数效应理论为动态线程提供完整的等式公理化，基于参数化代数理论，通过fork和wait原语建模并发，证明模型完备性和语法完备性，并扩展到简单并发编程语言的语义分析。


<details>
  <summary>Details</summary>
Motivation: 为动态线程提供完整的等式公理化，使用代数效应理论来形式化并发编程中的fork和wait操作，建立严格的数学模型来验证并发程序的正确性。

Method: 基于参数化代数理论，构建包含fork和wait原语的代数理论，提供基本原子操作和定律（如fork的结合律），通过代数效应方法将语义分析集中在fork和wait的代数操作上。

Result: 实现了两个完备性：1）模型完备性：闭表达式完全捕获标记偏序集（pomsets）的相等性；2）语法完备性：开放表达式在所有闭化替换下相等时可证明相等。将分析扩展到简单并发编程语言，建立了操作语义和指称语义，证明了指称语义的可靠性、充分性和一阶完全抽象性。

Conclusion: 代数效应理论为动态线程提供了有效的等式公理化框架，参数化代数理论是处理名称和绑定的便捷工具，建立的完备性结果和语义分析为并发编程的形式化验证提供了坚实基础。

Abstract: We use the theory of algebraic effects to give a complete equational axiomatization for dynamic threads. Our method is based on parameterized algebraic theories, which give a concrete syntax for strong monads on functor categories, and are a convenient framework for names and binding. Our programs are built from the key primitives `fork' and `wait'. `Fork' creates a child thread and passes its name (thread ID) to the parent thread. `Wait' allows us to wait for given child threads to finish. We provide a parameterized algebraic theory built from fork and wait, together with basic atomic actions and laws such as associativity of `fork'. Our equational axiomatization is complete in two senses. First, for closed expressions, it completely captures equality of labelled posets (pomsets), an established model of concurrency: model complete. Second, any two open expressions are provably equal if they are equal under all closing substitutions: syntactically complete. The benefit of algebraic effects is that the semantic analysis can focus on the algebraic operations of fork and wait. We then extend the analysis to a simple concurrent programming language by giving operational and denotational semantics. The denotational semantics is built using the methods of parameterized algebraic theories and we show that it is sound, adequate, and fully abstract at first order for labelled-poset observations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [A novel scalable high performance diffusion solver for multiscale cell simulations](https://arxiv.org/abs/2602.05017)
*Jose-Luis Estragues-Muñoz,Carlos Alvarez,Arnau Montagud,Daniel Jimenez-Gonzalez,Alfonso Valencia*

Main category: cs.DC

TL;DR: 提出了一种用于分子扩散建模的可扩展HPC解决方案BioFVM，相比现有方案实现了近200倍加速和36%内存使用减少。


<details>
  <summary>Details</summary>
Motivation: 基于代理的细胞模型在模拟组织演化时需要处理细胞间相互作用和微环境响应，但将细胞分辨率模型扩展到真实规模肿瘤模拟面临计算挑战，需要高性能计算来处理每时间步数万亿次操作，这对于疾病数字孪生模型开发至关重要。

Method: 开发了可扩展的HPC解决方案，采用高效实现的最先进有限体积法框架，系统评估了新型可扩展生物有限体积法库BioFVM，并对可用解决方案进行了广泛的性能分析。

Result: HPC方案相比当前最先进解决方案实现了近200倍加速和高达36%的内存使用减少，为高效计算下一代生物问题铺平了道路。

Conclusion: 提出的BioFVM库为大规模生物模拟提供了高效的计算解决方案，显著提升了分子扩散建模的性能，有助于推动数字孪生疾病模型的发展。

Abstract: Agent-based cellular models simulate tissue evolution by capturing the behavior of individual cells, their interactions with neighboring cells, and their responses to the surrounding microenvironment. An important challenge in the field is scaling cellular resolution models to real-scale tumor simulations, which is critical for the development of digital twin models of diseases and requires the use of High-Performance Computing (HPC) since every time step involves trillions of operations. We hereby present a scalable HPC solution for the molecular diffusion modeling using an efficient implementation of state-of-the-art Finite Volume Method (FVM) frameworks. The paper systematically evaluates a novel scalable Biological Finite Volume Method (BioFVM) library and presents an extensive performance analysis of the available solutions. Results shows that our HPC proposal reach almost 200x speedup and up to 36% reduction in memory usage over the current state-of-the-art solutions, paving the way to efficiently compute the next generation of biological problems.

</details>


### [5] [Towards Advancing Research with Workflows: A perspective from the Workflows Community Summit -- Amsterdam, 2025](https://arxiv.org/abs/2602.05131)
*Irene Bonati,Silvina Caino-Lores,Tainã Coleman,Sagar Dolas,Sandro Fiore,Venkatesh Kannan,Marco Verdicchio,Sean R. Wilkinson,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 2025年科学工作流社区峰会识别了工作流采用的关键障碍，并提出了技术、政策和社区层面的解决方案，强调从计算性能转向科学影响评估，建立标准化模式，培育国际社区，以及投资人力资本。


<details>
  <summary>Details</summary>
Motivation: 科学工作流在现代研究中对于协调分布式计算、管理大数据和确保可重复性至关重要。然而，工作流采用面临多重障碍，包括系统通用性与领域特定性之间的张力、可持续性问题、开发者认可不足，以及标准化、资金、培训和跨学科合作的缺口。峰会旨在解决这些挑战，推动工作流在科学发现中的应用。

Method: 通过2025年6月6日在阿姆斯特丹举行的国际工作流社区峰会，汇集专家讨论当前挑战和机遇。采用社区参与的方式，识别关键障碍，并共同制定多维度解决方案。

Result: 识别了四大关键障碍：系统通用性与领域特定性的张力、工作流系统和服务的长期可持续性问题、工作流开发维护者认可不足、标准化/资金/培训/跨学科合作缺口。提出了技术、政策和社区层面的行动路线：从计算性能转向科学影响评估、形式化工作流模式和社区基准、培育国际社区、投资人力资本和培训。

Conclusion: 科学工作流社区需要系统性变革，包括评估指标的转变、标准化模式的建立、国际社区的培育，以及对工作流工程角色和培训的投资。这些行动将推动工作流在科学发现中的更有效应用，解决当前采用障碍，促进可重复性和跨学科合作。

Abstract: Scientific workflows have become essential for orchestrating complex computational processes across distributed resources, managing large datasets, and ensuring reproducibility in modern research. The Workflows Community Summit 2025, held in Amsterdam on June 6th, 2025, convened international experts to examine emerging challenges and opportunities in this domain. Participants identified key barriers to workflow adoption, including tensions between system generality and domain-specific utility, concerns over long-term sustainability of workflow systems and services, insufficient recognition for those who develop and maintain reproducible workflows, and gaps in standardization, funding, training, and cross-disciplinary collaboration. To address these challenges, the summit proposed action lines spanning technology, policy, and community dimensions: shifting evaluation metrics from raw computational performance toward measuring genuine scientific impact; formalizing workflow patterns and community-driven benchmarks to improve transparency, reproducibility, and usability; cultivating a cohesive international workflows community that engages funding bodies and research stakeholders; and investing in human capital through dedicated workflow engineering roles, career pathways, and integration of workflow concepts into educational curricula and long-term training initiatives. This document presents the summit's findings, beginning with an overview of the current computing ecosystem and the rationale for workflow-centric approaches, followed by a discussion of identified challenges and recommended action lines for advancing scientific discovery through workflows.

</details>


### [6] [ORACL: Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices](https://arxiv.org/abs/2602.05292)
*Haoyu Bai,Muhammed Tawfiqul Islam,Minxian Xu,Rajkumar Buyya*

Main category: cs.DC

TL;DR: ORACL使用大语言模型作为通用少样本资源分配器，通过思维链推理诊断微服务性能问题并推荐资源分配，无需部署特定训练即可适应快速演化的微服务部署。


<details>
  <summary>Details</summary>
Motivation: 微服务和Serverless架构成为主流，但现有自动扩缩策略存在局限性：要么是需要大量部署特定训练的不透明学习模型，要么是难以泛化的脆弱手动调优规则。需要一种能够适应快速演化微服务部署的通用资源分配方法。

Method: ORACL框架将运行时遥测数据（pods、副本、CPU/内存使用率、延迟、SLO、故障信号）转换为语义自然语言状态描述，调用LLM生成可解释的中间推理轨迹。该推理识别可能根本原因，剪枝动作空间，并在策略约束下发出安全的分配决策。

Result: 在代表性开源微服务工作负载实验中，ORACL将根本原因识别准确率提高15%，训练加速高达24倍，短期场景中服务质量提升6%，且无需部署特定重新训练。

Conclusion: 大语言模型可以作为通用少样本资源分配器，通过思维链推理有效适应快速演化的微服务部署，提供比现有方法更好的性能诊断和资源分配能力。

Abstract: Applications are moving away from monolithic designs to microservice and serverless architectures, where fleets of lightweight and independently deployable components run on public clouds. Autoscaling serves as the primary control mechanism for balancing resource utilization and quality of service, yet existing policies are either opaque learned models that require substantial per-deployment training or brittle hand-tuned rules that fail to generalize. We investigate whether large language models can act as universal few-shot resource allocators that adapt across rapidly evolving microservice deployments.
  We propose ORACL, Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices, a framework that leverages prior knowledge and chain-of-thought reasoning to diagnose performance regressions and recommend resource allocations. ORACL transforms runtime telemetry, including pods, replicas, CPU and memory usage, latency, service-level objectives, and fault signals, into semantic natural-language state descriptions and invokes an LLM to produce an interpretable intermediate reasoning trace. This reasoning identifies likely root causes, prunes the action space, and issues safe allocation decisions under policy constraints. Experiments on representative open-source microservice workloads show that ORACL improves root-cause identification accuracy by 15 percent, accelerates training by up to 24x, and improves quality of service by 6 percent in short-term scenarios, without deployment-specific retraining.

</details>


### [7] [Proteus: Append-Only Ledgers for (Mostly) Trusted Execution Environments](https://arxiv.org/abs/2602.05346)
*Shubham Mishra,João Gonçalves,Chawinphat Tankuranand,Neil Giridharan,Natacha Crooks,Heidi Howard,Chris Jensen*

Main category: cs.DC

TL;DR: Proteus是一个分布式共识协议，通过在CFT协议中嵌入BFT协议，谨慎信任TEE硬件，在保持高性能的同时确保TEE平台被攻破时的数据完整性。


<details>
  <summary>Details</summary>
Motivation: 分布式账本依赖TEE硬件提供信任保证，但TEE可能受到攻击，破坏分布式账本的设计保证。需要一种既能利用TEE性能优势，又能应对TEE被攻破情况的解决方案。

Method: Proteus通过精心重构CFT和BFT协议，使它们的结构对齐，将BFT协议嵌入到CFT协议中，无需额外消息。这种设计谨慎信任TEE的保证。

Result: Proteus实现了与常规TEE增强共识协议相当的性能，同时在TEE平台被攻破时仍能保证数据完整性。

Conclusion: Proteus提供了一种平衡方案，既能利用TEE的性能优势，又能应对TEE安全漏洞，为分布式账本提供了更强的弹性保障。

Abstract: Distributed ledgers are increasingly relied upon by industry to provide trustworthy accountability, strong integrity protection, and high availability for critical data without centralizing trust. Recently, distributed append-only logs are opting for a layered approach, combining crash-fault-tolerant (CFT) consensus with hardware-based Trusted Execution Environments (TEEs) for greater resiliency. Unfortunately, hardware TEEs can be subject to (rare) attacks, undermining the very guarantees that distributed ledgers are carefully designed to achieve. In response, we present Proteus, a new distributed consensus protocol that cautiously trusts the guarantees of TEEs. Proteus carefully embeds a Byzantine fault-tolerant (BFT) protocol inside of a CFT protocol with no additional messages. This is made possible through careful refactoring of both the CFT and BFT protocols such that their structure aligns. Proteus achieves performance in line with regular TEE-enabled consensus protocols, while guaranteeing integrity in the face of TEE platform compromises.

</details>


### [8] [Reaching Univalency with Subquadratic Communication](https://arxiv.org/abs/2602.05356)
*Andrew Lewis-Pye*

Main category: cs.DC

TL;DR: 论文挑战了Dolev-Reischuk下界，证明达到一致性（univalency）不需要二次通信，二次成本主要来自结果传播阶段。


<details>
  <summary>Details</summary>
Motivation: 研究Dolev-Reischuk下界中二次通信成本的根本原因：是达到协议结果确定性（univalency）困难，还是仅仅将结果传播给所有处理器困难？

Method: 引入ε-BA松弛概念，允许ε比例的正确处理器输出错误值，证明在f < n(1/3 - ε)时可以用O(n log n)通信复杂度解决。任何ε-BA协议都可以作为完整BA协议的第一阶段。

Result: 证明达到一致性不需要二次通信，二次成本完全来自传播阶段。在认证设置中定义可提取BA，证明可以用O(f log f)通信复杂度解决。

Conclusion: Dolev-Reischuk下界的二次通信成本主要源于结果传播，而非达到协议结果确定性。这为拜占庭协议设计提供了新的理论见解。

Abstract: The Dolev-Reischuk lower bound establishes that any deterministic Byzantine Agreement (BA) protocol for $n$ processors tolerating $f$ faults requires $Ω(f^2+n)$ messages. But what exactly does this quadratic cost pay for? Even the minimal requirement that every correct processor \emph{receive at least one message} already necessitates $Ω(f^2 + n)$ messages. This raises a fundamental question: is the Dolev-Reischuk bound about the difficulty of \emph{reaching univalency} -- the point at which the protocol's outcome is determined -- or merely about \emph{disseminating} the outcome to all processors afterward?
  We resolve this question by showing that reaching univalency does \emph{not} require quadratic communication. Specifically, we introduce $ε$-BA, a relaxation allowing an $ε$-fraction of correct processors to output incorrectly, and prove it can be solved deterministically with $O(n \log n)$ communication complexity when $f < n(1/3 - ε)$. Crucially, any $ε$-BA protocol can serve as the first phase of a full BA protocol: after $ε$-BA, a single all-to-all exchange and majority vote completes BA. Since the outcome is already determined after $ε$-BA, this demonstrates that the quadratic cost in Dolev-Reischuk stems entirely from dissemination, rather than from reaching univalency. We also define Extractable BA for authenticated settings, capturing when processors collectively hold enough signed messages to determine the agreed value, and show it can be solved with communication complexity $O(f \log f)$.

</details>


### [9] [TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism](https://arxiv.org/abs/2602.05754)
*Seonghye Cho,Jaemin Han,Hyunjin Kim,Euisoo Jung,Jae-Gil Lee*

Main category: cs.DC

TL;DR: TimelyFreeze：一种通过建模流水线调度为有向无环图并求解线性规划问题来计算最优冻结比例的方法，在保证精度的同时最大化训练吞吐量，相比现有方法减少过度冻结导致的精度损失。


<details>
  <summary>Details</summary>
Motivation: 流水线并行虽然能训练超出单设备内存的模型，但实际吞吐量受限于流水线气泡。现有的参数冻结方法虽然能通过自适应跳过反向计算来提高训练吞吐量，但往往过度冻结参数，导致不必要的精度下降。

Method: 将流水线调度建模为有向无环图，通过求解线性规划问题来计算最优冻结比例，在精度约束下最小化批次执行时间。

Result: 实验显示TimelyFreeze在LLaMA-8B上实现了高达40%的训练吞吐量提升，同时保持可比的精度。

Conclusion: TimelyFreeze能够在不影响收敛的情况下实现更快的大规模模型训练，并且能够泛化到不同的流水线并行设置中。

Abstract: Pipeline parallelism enables training models that exceed single-device memory, but practical throughput remains limited by pipeline bubbles. Although parameter freezing can improve training throughput by adaptively skipping backward computation, existing methods often over-freeze parameters, resulting in unnecessary accuracy degradation. To address this issue, we propose TimelyFreeze, which models the pipeline schedule as a directed acyclic graph and solves a linear program to compute optimal freeze ratios that minimize batch execution time under accuracy constraints. Experiments show that TimelyFreeze achieves up to 40% training throughput improvement on LLaMA-8B with comparable accuracy. Overall, it enables faster large-scale model training without compromising convergence and generalizes across diverse pipeline-parallel settings.

</details>


### [10] [Location-Aware Dispersion on Anonymous Graphs](https://arxiv.org/abs/2602.05948)
*Himani,Supantha Pandit,Gokarna Sharma*

Main category: cs.DC

TL;DR: 论文提出了位置感知分散问题，这是经典分散问题的新扩展，要求机器人根据颜色匹配移动到对应颜色的节点上，并开发了具有时间和内存保证的确定性算法。


<details>
  <summary>Details</summary>
Motivation: 经典分散问题假设机器人可以占据任何空闲节点，但在实际应用中，机器人可能需要移动到特定类型的位置（如充电站、特定资源点等）。位置感知分散问题通过引入颜色标签，使机器人需要移动到与其颜色匹配的节点，这更符合现实世界的应用场景。

Method: 作者开发了多个确定性算法来解决位置感知分散问题，这些算法在时间和内存使用上都有理论保证。同时，论文还给出了不可能性结果和任何确定性算法的下界。

Result: 研究结果表明位置感知分散问题在匿名网络中是可解的，但相比经典分散问题，获得高效解决方案面临更大挑战。提出的算法为这一问题提供了可行的解决方案。

Conclusion: 位置感知分散问题是分散问题的有意义的扩展，虽然比经典问题更具挑战性，但通过适当的算法设计是可以在匿名网络中解决的。该研究为分布式机器人协调提供了新的理论框架。

Abstract: The well-studied DISPERSION problem is a fundamental coordination problem in distributed robotics, where a set of mobile robots must relocate so that each occupies a distinct node of a network. DISPERSION assumes that a robot can settle at any node as long as no other robot settles on that node. In this work, we introduce LOCATION-AWARE DISPERSION, a novel generalization of DISPERSION that incorporates location awareness: Let $G = (V, E)$ be an anonymous, connected, undirected graph with $n = |V|$ nodes, each labeled with a color $\sf{col}(v) \in C = \{c_1, \dots, c_t\}, t\leq n$. A set $R = \{r_1, \dots, r_k\}$ of $k \leq n$ mobile robots is given, where each robot $r_i$ has an associated color $\mathsf{col}(r_i) \in C$. Initially placed arbitrarily on the graph, the goal is to relocate the robots so that each occupies a distinct node of the same color. When $|C|=1$, LOCATION-AWARE DISPERSION reduces to DISPERSION. There is a solution to DISPERSION in graphs with any $k\leq n$ without knowing $k,n$.
  Like DISPERSION, the goal is to solve LOCATION-AWARE DISPERSION minimizing both time and memory requirement at each agent. We develop several deterministic algorithms with guaranteed bounds on both time and memory requirement. We also give an impossibility and a lower bound for any deterministic algorithm for LOCATION-AWARE DISPERSION. To the best of our knowledge, the presented results collectively establish the algorithmic feasibility of LOCATION-AWARE DISPERSION in anonymous networks and also highlight the challenges on getting an efficient solution compared to the solutions for DISPERSION.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [CVA6-CFI: A First Glance at RISC-V Control-Flow Integrity Extensions](https://arxiv.org/abs/2602.04991)
*Simone Manoni,Emanuele Parisi,Riccardo Tedeschi,Davide Rossi,Andrea Acquaviva,Andrea Bartolini*

Main category: cs.AR

TL;DR: 本文首次设计、集成并评估了RISC-V的控制流完整性（CFI）标准扩展Zicfiss和Zicfilp，通过硬件实现影子栈和着陆点机制来防御控制流劫持攻击，在CVA6核心中集成后仅产生1.0%面积开销和最高15.6%性能开销。


<details>
  <summary>Details</summary>
Motivation: 保护易受攻击的程序免受控制流劫持攻击，通过硬件实现控制流完整性机制，为RISC-V架构提供标准化的安全扩展。

Method: 设计了两个独立的可配置硬件单元：一个实现前向边缘控制流保护（着陆点机制），另一个实现后向边缘控制流保护（影子栈机制），并将它们完全集成到开源的CVA6核心中。

Result: 在22nm FDX技术中合成时仅产生1.0%的面积开销，使用MiBench汽车基准测试子集评估显示最高15.6%的性能开销，并将完整实现作为开源发布。

Conclusion: 成功实现了RISC-V控制流完整性标准扩展的首次硬件设计，证明了在可接受的开销下通过硬件机制有效保护控制流的可行性，为RISC-V生态系统提供了重要的安全增强。

Abstract: This work presents the first design, integration, and evaluation of the standard RISC-V extensions for Control-Flow Integrity (CFI). The Zicfiss and Zicfilp extensions aim at protecting the execution of a vulnerable program from control-flow hijacking attacks through the implementation of security mechanisms based on shadow stack and landing pad primitives. We introduce two independent and configurable hardware units implementing forward-edge and backward-edge control-flow protection, fully integrated into the open-source CVA6 core. Our design incurs in only 1.0% area overhead when synthesized in 22 nm FDX technology, and up to 15.6% performance overhead based on evaluation with the MiBench automotive benchmark subset. We release the complete implementation as open source.

</details>


### [12] [COFFEE: A Carbon-Modeling and Optimization Framework for HZO-based FeFET eNVMs](https://arxiv.org/abs/2602.05018)
*Hongbang Wu,Xuesi Chen,Shubham Jadhav,Amit Lal,Lillian Pentecost,Udit Gupta*

Main category: cs.AR

TL;DR: COFFEE是首个针对HZO基FeFET eNVM的碳建模框架，涵盖硬件制造（体现碳）到使用（运营碳）的全生命周期。评估显示HZO-FeFET在单位面积体现碳比CMOS基线高11%，但单位MB体现碳比SRAM低4.3倍。在边缘ML加速器案例中，用HZO-FeFET eNVM替代SRAM权重缓冲区可减少42.3%体现碳和70%运营碳。


<details>
  <summary>Details</summary>
Motivation: 信息和通信技术对全球环境影响日益增长，新兴非易失性存储器（eNVM）虽能提供能效计算方案，但其端到端碳足迹尚未被充分理解。理解硬件系统全生命周期的环境影响是实现可持续计算的第一步。

Method: 提出COFFEE碳建模框架，基于真实半导体工厂数据和设备制造配方估算体现碳，使用架构级eNVM设计空间探索工具量化使用阶段性能和能耗。以HZO基铁电场效应晶体管（FeFET）为例进行详细研究。

Result: 在2MB容量下，HZO-FeFET的单位面积体现碳比CMOS基线高11%，但单位MB体现碳始终比SRAM低4.3倍。边缘ML加速器案例研究显示，用HZO-FeFET eNVM替代SRAM权重缓冲区可减少42.3%体现碳和高达70%运营碳。

Conclusion: COFFEE框架首次实现了对HZO基FeFET eNVM全生命周期碳足迹的量化分析，证明eNVM技术在减少计算系统碳足迹方面具有显著潜力，为实现可持续计算提供了重要工具和见解。

Abstract: Information and communication technologies account for a growing portion of global environmental impacts. While emerging technologies, such as emerging non-volatile memories (eNVM), offer a promising solution to energy efficient computing, their end-to-end footprint is not well understood. Understanding the environmental impact of hardware systems over their life cycle is the first step to realizing sustainable computing. This work conducts a detailed study of one example eNVM device: hafnium-zirconium-oxide (HZO)-based ferroelectric field-effect transistors (FeFETs). We present COFFEE, the first carbon modeling framework for HZO-based FeFET eNVMs across life cycle, from hardware manufacturing (embodied carbon) to use (operational carbon). COFFEE builds on data gathered from a real semiconductor fab and device fabrication recipes to estimate embodied carbon, and architecture level eNVM design space exploration tools to quantify use-phase performance and energy. Our evaluation shows that, at 2 MB capacity, the embodied carbon per unit area overhead of HZO-FeFETs can be up to 11% higher than the CMOS baseline, while the embodied carbon per MB remains consistently about 4.3x lower than SRAM across different memory capacity. A further case study applies COFFEE to an edge ML accelerator, showing that replacing the SRAM-based weight buffer with HZO-based FeFET eNVMs reduces embodied carbon by 42.3% and operational carbon by up to 70%.

</details>


### [13] [Balancing FP8 Computation Accuracy and Efficiency on Digital CIM via Shift-Aware On-the-fly Aligned-Mantissa Bitwidth Prediction](https://arxiv.org/abs/2602.05743)
*Liang Zhao,Kunming Shao,Zhipeng Liao,Xijie Huang,Tim Kwang-Ting Cheng,Chi-Ying Tsui,Yi Zou*

Main category: cs.AR

TL;DR: 提出一种灵活的FP8数字计算内存加速器，通过动态位宽预测和FIFO对齐单元，支持可变FP8格式，在28nm CMOS中实现20.4 TFLOPS/W能效，比现有工作提升2.8倍。


<details>
  <summary>Details</summary>
Motivation: 现有数字计算内存架构难以支持可变FP8对齐尾数位宽，统一对齐策略和固定精度MAC单元无法处理分布多样的输入数据，需要更灵活的解决方案。

Method: 提出三个创新：1) 动态移位感知位宽预测，自适应调整权重和输入对齐尾数精度；2) FIFO输入对齐单元替代复杂桶形移位器；3) 精度可扩展INT MAC阵列实现灵活权重精度。

Result: 在28nm CMOS中实现64×96 CIM阵列，达到20.4 TFLOPS/W能效，比现有FP8效率提升2.8倍，支持所有FP8格式。在Llama-7b上，DSBP在相同精度下比固定位宽模式效率更高。

Conclusion: 该工作提出了一种灵活的FP8 DCIM加速器架构，通过动态位宽预测和简化对齐单元，有效解决了可变FP8格式支持问题，在保持精度的同时显著提升能效。

Abstract: FP8 low-precision formats have gained significant adoption in Transformer inference and training. However, existing digital compute-in-memory (DCIM) architectures face challenges in supporting variable FP8 aligned-mantissa bitwidths, as unified alignment strategies and fixed-precision multiply-accumulate (MAC) units struggle to handle input data with diverse distributions. This work presents a flexible FP8 DCIM accelerator with three innovations: (1) a dynamic shift-aware bitwidth prediction (DSBP) with on-the-fly input prediction that adaptively adjusts weight (2/4/6/8b) and input (2$\sim$12b) aligned-mantissa precision; (2) a FIFO-based input alignment unit (FIAU) replacing complex barrel shifters with pointer-based control; and (3) a precision-scalable INT MAC array achieving flexible weight precision with minimal overhead. Implemented in 28nm CMOS with a 64$\times$96 CIM array, the design achieves 20.4 TFLOPS/W for fixed E5M7, demonstrating 2.8$\times$ higher FP8 efficiency than previous work while supporting all FP8 formats. Results on Llama-7b show that the DSBP achieves higher efficiency than fixed bitwidth mode at the same accuracy level on both BoolQ and Winogrande datasets, with configurable parameters enabling flexible accuracy-efficiency trade-offs.

</details>
