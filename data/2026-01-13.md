<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Employ SmartNICs' Data Path Accelerators for Ordered Key-Value Stores](https://arxiv.org/abs/2601.06231)
*Frederic Schimmelpfennig,Jan Sass,Reza Salkhordeh,Martin Kröning,Stefan Lankes,André Brinkmann*

Main category: cs.DC

TL;DR: 基于BlueField-3 SmartNIC的DPA加速KV存储，通过锁无关学习索引实现无状态客户端和范围查询，性能达3300万点查询/秒和1300万范围查询/秒


<details>
  <summary>Details</summary>
Motivation: 现有远程内存KV存储架构难以同时实现峰值效率、架构简单性和有序操作支持。传统主机中心框架受内核网络栈和总线延迟限制，哈希方案缺乏范围查询能力，RDMA系统依赖有状态客户端，SmartNIC方案则受DMA往返延迟影响。

Method: 利用BlueField-3 SmartNIC的DPA直接从NIC缓冲区处理网络请求，在加速器本地内存中维护锁无关学习索引。延迟从主机端树副本获取值直到到达叶节点，最小化PCIe穿越。写操作在DPA内存中暂存并批量迁移到主机，通过事务性缝合机制同步回SmartNIC，配合NIC驻留读缓存。

Result: 系统实现3300万操作/秒的点查询性能和1300万操作/秒的范围查询性能，匹配或超越当前最先进解决方案，同时识别了可进一步提升性能的硬件优化方向。

Conclusion: 基于SmartNIC DPA的KV存储架构成功消除了操作系统开销，支持无状态客户端和范围操作，在性能、简单性和功能支持方面取得了良好平衡，为未来硬件优化提供了方向。

Abstract: Remote in-memory key-value (KV) stores serve as a cornerstone for diverse modern workloads, and high-speed range scans are frequently a requirement. However, current architectures rarely achieve a simultaneous balance of peak efficiency, architectural simplicity, and native support for ordered operations. Conventional host-centric frameworks are restricted by kernel-space network stacks and internal bus latencies. While hash-based alternatives that utilize OS-bypass or run natively on SmartNICs offer high throughput, they lack the data structures necessary for range queries. Distributed RDMA-based systems provide performance and range functionality but often depend on stateful clients, which introduces complexity in scaling and error handling. Alternatively, SmartNIC implementations that traverse trees located in host memory are hampered by high DMA round-trip latencies.
  This paper introduces a KV store that leverages the on-path Data Path Accelerators (DPAs) of the BlueField-3 SmartNIC to eliminate operating system overhead while facilitating stateless clients and range operations. These DPAs ingest network requests directly from NIC buffers to navigate a lock-free learned index residing in the accelerator's local memory. By deferring value retrieval from the host-side tree replica until the leaf level is reached, the design minimizes PCIe crossings. Write operations are staged in DPA memory and migrated in batches to the host, where structural maintenance is performed before being transactionally stitched back to the SmartNIC. Coupled with a NIC-resident read cache, the system achieves 33 million operations per second (MOPS) for point lookups and 13 MOPS for range queries. Our analysis demonstrates that this architecture matches or exceeds the performance of contemporary state-of-the-art solutions, while we identify hardware refinements that could further accelerate performance.

</details>


### [2] [HiDVFS: A Hierarchical Multi-Agent DVFS Scheduler for OpenMP DAG Workloads](https://arxiv.org/abs/2601.06425)
*Mohammad Pivezhandi,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.DC

TL;DR: HiDVFS：针对OpenMP DAG工作负载的分层多智能体DVFS调度器，通过基于性能分析、核心温度和任务优先级的智能调度，在保证性能（makespan）的同时显著降低能耗和温度。


<details>
  <summary>Details</summary>
Motivation: 随着多核嵌入式系统的发展，漏电功耗（与芯片温度指数相关）已超过动态功耗。现有节能方案（如DVFS）缺乏核心级频率监控，无法解决核心活动不均导致的过热问题；任务分配方案缺乏详细性能分析，忽略了不规则执行模式。OpenMP DAG工作负载中，makespan、能耗和热目标常常冲突。

Method: 提出HiDVFS分层多智能体性能感知DVFS调度器：1）基于性能分析数据选择核心和频率的智能体；2）通过温度传感器管理核心组合的智能体；3）在资源竞争时设置任务优先级的智能体。使用以makespan为重点的奖励函数，加上能耗和温度正则化器，估计未来状态并提高样本效率。

Result: 在NVIDIA Jetson TX2上使用BOTS套件（9个基准测试）进行实验，多种子验证（42、123、456）。HiDVFS获得最佳微调性能：平均makespan 4.16±0.58秒（L10），比GearDVFS（14.32±2.61秒）快3.44倍，能耗降低50.4%（63.7 kJ vs 128.4 kJ）。在所有BOTS基准测试中，平均加速3.95倍，能耗降低47.1%。

Conclusion: HiDVFS通过分层多智能体架构有效解决了多核系统中性能、能耗和热管理的冲突问题，在优先保证性能的同时显著降低了能耗和温度，为OpenMP DAG工作负载提供了高效的调度解决方案。

Abstract: With advancements in multicore embedded systems, leakage power, exponentially tied to chip temperature, has surpassed dynamic power consumption. Energy-aware solutions use dynamic voltage and frequency scaling (DVFS) to mitigate overheating in performance-intensive scenarios, while software approaches allocate high-utilization tasks across core configurations in parallel systems to reduce power. However, existing heuristics lack per-core frequency monitoring, failing to address overheating from uneven core activity, and task assignments without detailed profiling overlook irregular execution patterns. We target OpenMP DAG workloads. Because makespan, energy, and thermal goals often conflict within a single benchmark, this work prioritizes performance (makespan) while reporting energy and thermal as secondary outcomes. To overcome these issues, we propose HiDVFS (a hierarchical multi-agent, performance-aware DVFS scheduler) for parallel systems that optimizes task allocation based on profiling data, core temperatures, and makespan-first objectives. It employs three agents: one selects cores and frequencies using profiler data, another manages core combinations via temperature sensors, and a third sets task priorities during resource contention. A makespan-focused reward with energy and temperature regularizers estimates future states and enhances sample efficiency. Experiments on the NVIDIA Jetson TX2 using the BOTS suite (9 benchmarks) compare HiDVFS against state-of-the-art approaches. With multi-seed validation (seeds 42, 123, 456), HiDVFS achieves the best finetuned performance with 4.16 plus/minus 0.58s average makespan (L10), representing a 3.44x speedup over GearDVFS (14.32 plus/minus 2.61s) and 50.4% energy reduction (63.7 kJ vs 128.4 kJ). Across all BOTS benchmarks, HiDVFS achieves an average 3.95x speedup and 47.1% energy reduction.

</details>


### [3] [SkyNomad: On Using Multi-Region Spot Instances to Minimize AI Batch Job Cost](https://arxiv.org/abs/2601.06520)
*Zhifei Li,Tian Xia,Ziming Mao,Zihan Zhou,Ethan J. Jackson,Jamison Kerney,Zhanghao Wu,Pratik Mishra,Yi Xu,Yifan Qiao,Scott Shenker,Ion Stoica*

Main category: cs.DC

TL;DR: SkyNomad是一个多区域调度系统，通过利用云服务商不同区域和时间的spot实例可用性、生命周期和价格差异，在保证截止时间的前提下最大化spot实例使用并最小化AI批处理作业成本。


<details>
  <summary>Details</summary>
Motivation: AI批处理作业（如模型训练、推理管道、数据分析）需要大量GPU资源且通常有截止时间要求。Spot实例成本比按需实例低3-10倍，但其不可预测的可用性使得难以保证截止时间。现有系统要么仅依赖spot实例而面临截止时间违规风险，要么在简化的单区域设置中运行，忽略了spot可用性、生命周期和价格在空间和时间上的显著异质性。

Method: SkyNomad采用多区域调度架构，通过轻量级探测估计spot可用性，预测spot实例生命周期，考虑迁移成本，并将区域特征和截止时间压力统一到一个指导调度决策的货币成本模型中。

Result: 在实际云部署中，SkyNomad实现了1.25-3.96倍的成本节省；在模拟中，其性能与最优策略的成本差异在10%以内，同时始终满足截止时间要求。

Conclusion: 通过利用spot实例在空间和时间上的异质性来获取更多spot容量，是降低作业执行成本的关键。SkyNomad证明了在多区域环境中智能调度spot实例可以在保证截止时间的前提下显著降低成本。

Abstract: AI batch jobs such as model training, inference pipelines, and data analytics require substantial GPU resources and often need to finish before a deadline. Spot instances offer 3-10x lower cost than on-demand instances, but their unpredictable availability makes meeting deadlines difficult. Existing systems either rely solely on spot instances and risk deadline violations, or operate in simplified single-region settings. These approaches overlook substantial spatial and temporal heterogeneity in spot availability, lifetimes, and prices. We show that exploiting such heterogeneity to access more spot capacity is the key to reduce the job execution cost. We present SkyNomad, a multi-region scheduling system that maximizes spot usage and minimizes cost while guaranteeing deadlines. SkyNomad uses lightweight probing to estimate availability, predicts spot lifetimes, accounts for migration cost, and unifies regional characteristics and deadline pressure into a monetary cost model that guides scheduling decisions. Our evaluation shows that SkyNomad achieves 1.25-3.96x cost savings in real cloud deployments and performs within 10% cost differences of an optimal policy in simulation, while consistently meeting deadlines.

</details>


### [4] [Resource-Aware Task Allocator Design: Insights and Recommendations for Distributed Satellite Constellations](https://arxiv.org/abs/2601.06706)
*Bharadwaj Veeravalli*

Main category: cs.DC

TL;DR: 本文提出了一种资源感知任务分配器(RATA)，用于分布式卫星系统(DSS)中的实时任务处理，通过实验分析发现系统性能随星座规模呈非线性变化，CPU可用性是任务阻塞的主要因素而非能量限制。


<details>
  <summary>Details</summary>
Motivation: 随着分布式卫星系统的发展，需要有效处理实时任务并优化资源利用。现有系统在应对计算密集型工作负载时面临性能挑战，特别是在资源受限的卫星环境中，需要分析系统在多种因素（计算、存储、带宽、电池、卫星遮挡等）综合影响下的性能极限。

Method: 采用基于单层树网络(SLTN)的协作任务分配架构，设计资源感知任务分配器(RATA)，监控关键参数如到达率、资源可用性（机载计算、存储、带宽、电池）和卫星遮挡影响。通过实验评估数万个任务在不同星座规模（LEO到Low-MEO）和流量负载下的性能指标，包括阻塞概率、响应时间、能耗和资源利用率。

Result: 结果显示明显的非线性缩放特性：虽然容量随星座规模增加，但阻塞和延迟迅速增长，而能量在太阳能感知调度下保持弹性。分析确定了基线SLTNs的实用卫星数量限制，并证明CPU可用性（而非能量）是阻塞的主要原因。研究识别了系统性能从优雅降级到崩溃的阈值。

Conclusion: 该研究为分布式卫星系统的任务分配提供了定量指导，识别了关键性能阈值。CPU资源而非能量成为系统瓶颈，太阳能感知调度能有效维持能量弹性。这些发现有助于设计更鲁棒的卫星任务处理系统。

Abstract: We present the design of a Resource-Aware Task Allocator (RATA) and an empirical analysis in handling real-time tasks for processing on Distributed Satellite Systems (DSS). We consider task processing performance across low Earth orbit (LEO) to Low-Medium Earth Orbit (Low-MEO) constellation sizes, under varying traffic loads. Using Single-Level Tree Network(SLTN)-based cooperative task allocation architecture, we attempt to evaluate some key performance metrics - blocking probabilities, response times, energy consumption, and resource utilization across several tens of thousands of tasks per experiment. Our resource-conscious RATA monitors key parameters such as arrival rate, resources (on-board compute, storage, bandwidth, battery) availability, satellite eclipses' influence in processing and communications. This study is an important step towards analyzing the performance under lighter to stress inducing levels of compute intense workloads to test the ultimate performance limits under the combined influence of the above-mentioned factors. Results show pronounced non-linear scaling: while capacity increases with constellation size, blocking and delay grow rapidly, whereas energy remains resilient under solar-aware scheduling. The analysis identifies a practical satellite-count limit for baseline SLTNs and demonstrates that CPU availability, rather than energy, is the primary cause of blocking. These findings provide quantitative guidance by identifying thresholds at which system performance shifts from graceful degradation to collapse.

</details>


### [5] [Learning-Augmented Performance Model for Tensor Product Factorization in High-Order FEM](https://arxiv.org/abs/2601.06886)
*Xuanzhengbo Ren,Yuta Kawai,Tetsuya Hoshino,Hirofumi Tomita,Takahiro Katagiri,Daichi Mukunoki,Seiya Nishizawa*

Main category: cs.DC

TL;DR: 提出一种结合依赖链分析和XGBoost学习的性能预测模型，针对高算术强度的张量n模积核，在A64FX和Xeon处理器上显著优于传统Roofline和ECM模型。


<details>
  <summary>Details</summary>
Motivation: 现有性能模型主要关注缓存和内存带宽，适用于内存受限工作负载，但不适用于高算术强度的张量n模积核（高阶有限元方法优化技术）。在SIMD延迟较高的处理器（如A64FX）上，这些核的性能受循环体分割策略影响很大，需要能直接反映指令级效率的模型。

Method: 开发基于依赖链的分析模型，将循环分割配置与张量n模积核中的指令依赖关系联系起来。使用XGBoost机器学习方法估计分析模型中难以显式建模的关键参数。

Result: 学习增强模型在A64FX处理器上对多项式阶数P=1-15的预测误差为1%-24%，而标准Roofline和ECM模型的误差分别为42%-256%和5%-117%。在Xeon Gold 6230上，学习增强模型对P=1-14的误差为1%-13%，P=15时为24%，而标准模型的误差分别为1%-73%和8%-112%。

Conclusion: 提出的学习增强依赖链模型能有效预测高算术强度张量n模积核的性能，特别是在SIMD延迟较高的处理器上，显著优于传统内存带宽导向的性能模型。

Abstract: Accurate performance prediction is essential for optimizing scientific applications on modern high-performance computing (HPC) architectures. Widely used performance models primarily focus on cache and memory bandwidth, which is suitable for many memory-bound workloads. However, it is unsuitable for highly arithmetic intensive cases such as the sum-factorization with tensor $n$-mode product kernels, which are an optimization technique for high-order finite element methods (FEM). On processors with relatively high single instruction multiple data (SIMD) instruction latency, such as the Fujitsu A64FX, the performance of these kernels is strongly influenced by loop-body splitting strategies. Memory-bandwidth-oriented models are therefore not appropriate for evaluating these splitting configurations, and a model that directly reflects instruction-level efficiency is required. To address this need, we develop a dependency-chain-based analytical formulation that links loop-splitting configurations to instruction dependencies in the tensor $n$-mode product kernel. We further use XGBoost to estimate key parameters in the analytical model that are difficult to model explicitly. Evaluations show that the learning-augmented model outperforms the widely used standard Roofline and Execution-Cache-Memory (ECM) models. On the Fujitsu A64FX processor, the learning-augmented model achieves mean absolute percentage errors (MAPE) between 1% and 24% for polynomial orders ($P$) from 1 to 15. In comparison, the standard Roofline and ECM models yield errors of 42%-256% and 5%-117%, respectively. On the Intel Xeon Gold 6230 processor, the learning-augmented model achieves MAPE values from 1% to 13% for $P$=1 to $P$=14, and 24% at $P$=15. In contrast, the standard Roofline and ECM models produce errors of 1%-73% and 8%-112% for $P$=1 to $P$=15, respectively.

</details>


### [6] [Divergence-Based Adaptive Aggregation for Byzantine Robust Federated Learning](https://arxiv.org/abs/2601.06903)
*Bingnan Xiao,Feng Zhu,Jingjing Zhang,Wei Ni,Xin Wang*

Main category: cs.DC

TL;DR: 提出DRAG和BR-DRAG框架，通过设计参考方向和发散度度量来缓解联邦学习中客户端漂移问题，并抵抗拜占庭攻击，加速训练收敛。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中存在数据异构性导致的客户端漂移问题，以及系统易受拜占庭攻击的脆弱性，这些因素阻碍了有效的模型训练和收敛。

Method: DRAG设计参考方向和发散度度量来量化本地更新的偏差，通过线性校准对齐本地更新；BR-DRAG在服务器端维护验证根数据集生成可信参考方向，校准更新以抵御恶意攻击。

Result: 理论证明DRAG和BR-DRAG在非凸模型、部分参与、数据异构和拜占庭攻击下实现快速收敛；实验验证DRAG在处理客户端漂移方面优于现有方法，BR-DRAG在数据异构和多样拜占庭攻击下保持鲁棒性。

Conclusion: 提出的DRAG和BR-DRAG框架能有效缓解联邦学习中的客户端漂移问题，并抵抗拜占庭攻击，加速训练收敛，在理论和实验上均表现出优越性能。

Abstract: Inherent client drifts caused by data heterogeneity, as well as vulnerability to Byzantine attacks within the system, hinder effective model training and convergence in federated learning (FL). This paper presents two new frameworks, named DiveRgence-based Adaptive aGgregation (DRAG) and Byzantine-Resilient DRAG (BR-DRAG), to mitigate client drifts and resist attacks while expediting training. DRAG designs a reference direction and a metric named divergence of degree to quantify the deviation of local updates. Accordingly, each worker can align its local update via linear calibration without extra communication cost. BR-DRAG refines DRAG under Byzantine attacks by maintaining a vetted root dataset at the server to produce trusted reference directions. The workers' updates can be then calibrated to mitigate divergence caused by malicious attacks. We analytically prove that DRAG and BR-DRAG achieve fast convergence for non-convex models under partial worker participation, data heterogeneity, and Byzantine attacks. Experiments validate the effectiveness of DRAG and its superior performance over state-of-the-art methods in handling client drifts, and highlight the robustness of BR-DRAG in maintaining resilience against data heterogeneity and diverse Byzantine attacks.

</details>


### [7] [SC-MII: Infrastructure LiDAR-based 3D Object Detection on Edge Devices for Split Computing with Multiple Intermediate Outputs Integration](https://arxiv.org/abs/2601.07119)
*Taisuke Noguchi,Takayuki Nishio,Takuya Azumi*

Main category: cs.DC

TL;DR: SC-MII：一种面向边缘设备的多基础设施LiDAR 3D目标检测框架，采用分割计算与多中间输出集成，在降低延迟和设备负载的同时保持高精度


<details>
  <summary>Details</summary>
Motivation: 当前基于LiDAR的3D目标检测模型在边缘设备部署面临计算需求高、能耗大的挑战，且单LiDAR设置存在盲区问题。需要一种既能降低设备负载又能保持检测性能的解决方案。

Method: 提出SC-MII框架：边缘设备处理本地点云数据的前几层DNN，将中间输出发送到边缘服务器；服务器集成多个设备的特征并完成最终推理。这种分割计算方式减少了设备计算负担，同时多设备集成解决了单LiDAR盲区问题。

Result: 在真实数据集上的实验结果显示：实现了2.19倍加速，边缘设备处理时间减少71.6%，精度下降最多仅1.09%。

Conclusion: SC-MII框架有效解决了边缘设备部署3D目标检测的挑战，在显著降低延迟和设备负载的同时保持了高精度，为自动驾驶系统的实际部署提供了可行方案。

Abstract: 3D object detection using LiDAR-based point cloud data and deep neural networks is essential in autonomous driving technology. However, deploying state-of-the-art models on edge devices present challenges due to high computational demands and energy consumption. Additionally, single LiDAR setups suffer from blind spots. This paper proposes SC-MII, multiple infrastructure LiDAR-based 3D object detection on edge devices for Split Computing with Multiple Intermediate outputs Integration. In SC-MII, edge devices process local point clouds through the initial DNN layers and send intermediate outputs to an edge server. The server integrates these features and completes inference, reducing both latency and device load while improving privacy. Experimental results on a real-world dataset show a 2.19x speed-up and a 71.6% reduction in edge device processing time, with at most a 1.09% drop in accuracy.

</details>


### [8] [Bringing Computation to the data: Interoperable serverless function execution for astrophysical data analysis in the SRCNet](https://arxiv.org/abs/2601.07308)
*Manuel Parra-Royón,Julián Garrido-Sánchez,Susana Sánchez-Expósito,María Ángeles Mendoza,Rob Barnsley,Anthony Moraghan,Jesús Sánchez,Laura Darriba,Carlos Ruíz-Monje,Edgar Joao,Javier Moldón,Jesús Salgado,Lourdes Verdes-Montenegro*

Main category: cs.DC

TL;DR: 论文研究了在SKAO天文台大数据背景下，将无服务器计算和FaaS模型应用于射电天文学工作流的可行性，展示了FaaS能够嵌入现有生态系统，实现数据近端计算。


<details>
  <summary>Details</summary>
Motivation: SKAO天文台每年将产生约700PB数据，分布在SRCNet网络中。传统集中式计算面临数据传输瓶颈，需要将计算带到数据所在位置。无服务器计算和FaaS模型提供了弹性资源分配和最小运维开销的解决方案。

Method: 研究了无服务器和FaaS计算原理，开发并部署了天体物理数据分析的代表性函数，包括从现有库派生的微函数和领域特定应用的包装器。特别实现了高斯卷积函数，并将其集成到SRCNet生态系统中。

Result: 用例证明FaaS可以嵌入现有SRCNet服务生态系统，允许函数直接在数据副本存储的站点运行。这减少了延迟，最小化了数据传输，提高了效率，符合联邦化、数据近端计算的需求。

Conclusion: 无服务器模型为应对SKA时代的数据量提供了可扩展且高效的途径，特别适用于需要将计算带到数据所在位置的大规模科学基础设施。

Abstract: Serverless computing is a paradigm in which the underlying infrastructure is fully managed by the provider, enabling applications and services to be executed with elastic resource provisioning and minimal operational overhead. A core model within this paradigm is Function-as-a-Service (FaaS), where lightweight functions are deployed and triggered on demand, scaling seamlessly with workload. FaaS offers flexibility, cost-effectiveness, and fine-grained scalability, qualities particularly relevant for large-scale scientific infrastructures where data volumes are too large to centralise and computation must increasingly occur close to the data. The Square Kilometre Array Observatory (SKAO) exemplifies this challenge. Once operational, it will generate about 700~PB of data products annually, distributed across the SKA Regional Centre Network (SRCNet), a federation of international centres providing storage, computing, and analysis services. In such a context, FaaS offers a mechanism to bring computation to the data. We studied the principles of serverless and FaaS computing and explored their application to radio astronomy workflows. Representative functions for astrophysical data analysis were developed and deployed, including micro-functions derived from existing libraries and wrappers around domain-specific applications. In particular, a Gaussian convolution function was implemented and integrated within the SRCNet ecosystem. The use case demonstrates that FaaS can be embedded into the existing SRCNet ecosystem of services, allowing functions to run directly at sites where data replicas are stored. This reduces latency, minimises transfers, and improves efficiency, aligning with federated, data-proximate computation. The results show that serverless models provide a scalable and efficient pathway to address the data volumes of the SKA era.

</details>


### [9] [MegaFlow: Large-Scale Distributed Orchestration System for the Agentic Era](https://arxiv.org/abs/2601.07526)
*Lei Zhang,Mouxiang Chen,Ruisheng Cao,Jiawei Chen,Fan Zhou,Yiheng Xu,Jiaxi Yang,Liang Chen,Changwei Luo,Kai Zhang,Fan Yan,KaShun Shum,Jiajun Zhang,Zeyu Cui,Hu Feng,Junyang Lin,Binyuan Hui,Min Yang*

Main category: cs.DC

TL;DR: MegaFlow是一个大规模分布式编排系统，专门用于复杂智能体任务的训练和评估，通过三服务架构实现高效调度和资源管理。


<details>
  <summary>Details</summary>
Motivation: 随着交互式和自主AI系统进入智能体时代，训练和评估复杂智能体任务（如软件工程和计算机使用）需要不仅高效模型计算，还需要能够协调大量智能体-环境交互的复杂基础设施。目前缺乏能够有效支持此类复杂智能体任务大规模训练和评估的开源基础设施。

Method: MegaFlow将智能体训练基础设施抽象为三个独立服务：模型服务、智能体服务和环境服务，通过统一接口交互，实现独立扩展和灵活资源分配。系统支持大规模分布式编排，能够高效调度、分配资源和管理细粒度任务。

Result: 在智能体训练部署中，MegaFlow成功协调了数万个并发智能体任务，同时保持高系统稳定性并实现高效资源利用。系统能够处理复杂的智能体-环境配置。

Conclusion: MegaFlow通过实现大规模智能体训练，填补了新兴智能体AI领域的关键基础设施空白，为复杂智能体任务的训练和评估提供了有效的解决方案。

Abstract: The rapid development of interactive and autonomous AI systems signals our entry into the agentic era. Training and evaluating agents on complex agentic tasks such as software engineering and computer use requires not only efficient model computation but also sophisticated infrastructure capable of coordinating vast agent-environment interactions. However, no open-source infrastructure can effectively support large-scale training and evaluation on such complex agentic tasks. To address this challenge, we present MegaFlow, a large-scale distributed orchestration system that enables efficient scheduling, resource allocation, and fine-grained task management for agent-environment workloads. MegaFlow abstracts agent training infrastructure into three independent services (Model Service, Agent Service, and Environment Service) that interact through unified interfaces, enabling independent scaling and flexible resource allocation across diverse agent-environment configurations. In our agent training deployments, MegaFlow successfully orchestrates tens of thousands of concurrent agent tasks while maintaining high system stability and achieving efficient resource utilization. By enabling such large-scale agent training, MegaFlow addresses a critical infrastructure gap in the emerging agentic AI landscape.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [DS-CIM: Digital Stochastic Computing-In-Memory Featuring Accurate OR-Accumulation via Sample Region Remapping for Edge AI Models](https://arxiv.org/abs/2601.06724)
*Kunming Shao,Liang Zhao,Jiangnan Yu,Zhipeng Liao,Xiaomeng Wang,Yi Zou,Tim Kwang-Ting Cheng,Chi-Ying Tsui*

Main category: cs.AR

TL;DR: 提出数字随机计算内存架构DS-CIM，通过改进数据表示和共享PRNG设计，在保持高精度的同时实现高能效，解决传统随机计算吞吐量低和数字CIM加法器成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统随机计算硬件简单但吞吐量低，而高吞吐量的数字计算内存架构在矩阵向量乘法中受限于昂贵的加法器逻辑。需要一种既能保持高精度又能实现高效率的解决方案。

Method: 1. 通过修改数据表示在紧凑的无符号OR电路中实现有符号乘累加；2. 低成本电路复制64倍仅增加1倍面积；3. 采用共享伪随机数生成器和2D分区策略实现单周期互斥激活；4. 通过随机过程分析和数据重映射解决1饱和问题。

Result: DS-CIM1在CIFAR-10上实现INT8 ResNet18 94.45%准确率，RMSE仅0.74%；DS-CIM2达到3566.1 TOPS/W能效和363.7 TOPS/mm²面积效率，RMSE为3.81%。在INT8 ResNet50和FP8 LLaMA-7B模型上进一步验证了扩展能力。

Conclusion: DS-CIM架构成功平衡了随机计算和数字CIM的优势，在保持高精度的同时实现了卓越的能效和面积效率，为大规模AI模型部署提供了有前景的硬件解决方案。

Abstract: Stochastic computing (SC) offers hardware simplicity but suffers from low throughput, while high-throughput Digital Computing-in-Memory (DCIM) is bottlenecked by costly adder logic for matrix-vector multiplication (MVM). To address this trade-off, this paper introduces a digital stochastic CIM (DS-CIM) architecture that achieves both high accuracy and efficiency. We implement signed multiply-accumulation (MAC) in a compact, unsigned OR-based circuit by modifying the data representation. Throughput is enhanced by replicating this low-cost circuit 64 times with only a 1x area increase. Our core strategy, a shared Pseudo Random Number Generator (PRNG) with 2D partitioning, enables single-cycle mutually exclusive activation to eliminate OR-gate collisions. We also resolve the 1s saturation issue via stochastic process analysis and data remapping, significantly improving accuracy and resilience to input sparsity. Our high-accuracy DS-CIM1 variant achieves 94.45% accuracy for INT8 ResNet18 on CIFAR-10 with a root-mean-squared error (RMSE) of just 0.74%. Meanwhile, our high-efficiency DS-CIM2 variant attains an energy efficiency of 3566.1 TOPS/W and an area efficiency of 363.7 TOPS/mm^2, while maintaining a low RMSE of 3.81%. The DS-CIM capability with larger models is further demonstrated through experiments with INT8 ResNet50 on ImageNet and the FP8 LLaMA-7B model.

</details>


### [11] [GRPO with State Mutations: Improving LLM-Based Hardware Test Plan Generation](https://arxiv.org/abs/2601.07593)
*Dimple Vijay Kochar,Nathaniel Pinckney,Guan-Ting Liu,Chia-Tung Ho,Chenhui Deng,Haoxing Ren,Brucek Khailany*

Main category: cs.AR

TL;DR: LLMs在RTL验证激励生成方面表现不佳（成功率仅15.7-21.7%），通过结合监督微调和创新的强化学习方法GRPO-SMu，7B参数模型实现了33.3%的黄金测试通过率，比基线提升17.6%。


<details>
  <summary>Details</summary>
Motivation: RTL设计早期严重依赖临时测试平台创建，虽然LLMs在RTL代码生成方面有潜力，但其在硬件规格推理和针对性测试计划生成方面的能力尚未充分探索。

Method: 建立两阶段框架：测试计划生成与测试平台执行分离。开发综合训练方法，结合监督微调和创新的强化学习方法GRPO-SMu（带状态突变的GRPO），采用树基分支突变策略构建训练数据，超越线性突变方法。

Result: 最先进模型（DeepSeek-R1和Claude-4.0-Sonnet）在生成通过黄金RTL设计的激励方面成功率仅15.7-21.7%。经过训练的7B参数模型达到33.3%的黄金测试通过率和13.9%的突变检测率，比基线绝对提升17.6%，并优于更大的通用模型。

Conclusion: 专业化训练方法能显著提升LLMs在硬件验证任务中的推理能力，为半导体设计工作流中的自动化子单元测试奠定了基础。

Abstract: RTL design often relies heavily on ad-hoc testbench creation early in the design cycle. While large language models (LLMs) show promise for RTL code generation, their ability to reason about hardware specifications and generate targeted test plans remains largely unexplored. We present the first systematic study of LLM reasoning capabilities for RTL verification stimuli generation, establishing a two-stage framework that decomposes test plan generation from testbench execution. Our benchmark reveals that state-of-the-art models, including DeepSeek-R1 and Claude-4.0-Sonnet, achieve only 15.7-21.7% success rates on generating stimuli that pass golden RTL designs. To improve LLM generated stimuli, we develop a comprehensive training methodology combining supervised fine-tuning with a novel reinforcement learning approach, GRPO with State Mutation (GRPO-SMu), which enhances exploration by varying input mutations. Our approach leverages a tree-based branching mutation strategy to construct training data comprising equivalent and mutated trees, moving beyond linear mutation approaches to provide rich learning signals. Training on this curated dataset, our 7B parameter model achieves a 33.3% golden test pass rate and a 13.9% mutation detection rate, representing a 17.6% absolute improvement over baseline and outperforming much larger general-purpose models. These results demonstrate that specialized training methodologies can significantly enhance LLM reasoning capabilities for hardware verification tasks, establishing a foundation for automated sub-unit testing in semiconductor design workflows.

</details>
