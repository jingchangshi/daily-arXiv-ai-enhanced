{"id": "2508.15919", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15919", "abs": "https://arxiv.org/abs/2508.15919", "authors": ["Zahra Yousefijamarani", "Xinglu Wang", "Qian Wang", "Morgan Lindsay Heisler", "Taha Shabani", "Niloofar Gholipour", "Parham Yassini", "Hong Chang", "Kan Chen", "Qiantao Zhang", "Xiaolong Bai", "Jiannan Wang", "Ying Xiong", "Yong Zhang", "Zhenan Fan"], "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO Serving and Fast Scaling", "comment": null, "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon.", "AI": {"tldr": "HyperFlexis\u662f\u4e00\u4e2a\u7edf\u4e00\u7684LLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u7b97\u6cd5\u548c\u7cfb\u7edf\u7ea7\u521b\u65b0\u8054\u5408\u4f18\u5316\u8c03\u5ea6\u548c\u6269\u5c55\uff0c\u652f\u6301\u591aSLO\u8c03\u5ea6\u3001\u6210\u672c\u6548\u76ca\u6269\u5c55\u51b3\u7b56\u548c\u5feb\u901f\u89d2\u8272\u8f6c\u6362\uff0c\u663e\u8457\u63d0\u5347SLO\u8fbe\u6210\u7387\u548c\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\u9762\u4e34\u8bf7\u6c42\u957f\u5ea6\u3001\u4f18\u5148\u7ea7\u548c\u9636\u6bb5\u7279\u5b9aSLO\u9ad8\u5ea6\u53ef\u53d8\u6027\u7684\u6311\u6218\uff0c\u9700\u8981\u5b9e\u65f6\u8c03\u5ea6\u3001\u5feb\u901f\u6210\u672c\u6548\u76ca\u6269\u5c55\u4ee5\u53ca\u652f\u6301\u5e76\u7f6e\u548c\u5206\u79bb\u7684Prefill/Decode\u67b6\u6784\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u591aSLO\u611f\u77e5\u8c03\u5ea6\u5668\uff0c\u5229\u7528\u9884\u7b97\u4f30\u8ba1\u548c\u8bf7\u6c42\u4f18\u5148\u7ea7\u786e\u4fdd\u4e3b\u52a8SLO\u5408\u89c4\uff1b\u652f\u6301P/D\u5206\u79bb\u67b6\u6784\u7684prefill\u548cdecode\u9636\u6bb5\u591aSLO\u8c03\u5ea6\uff1b\u63d0\u51fa\u8bbe\u5907\u5230\u8bbe\u5907\u6743\u91cd\u4f20\u8f93\u673a\u5236\u964d\u4f4e\u6743\u91cd\u52a0\u8f7d\u5f00\u9500\uff1b\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u6269\u5c55\u51b3\u7b56\u548c\u5feb\u901fP/D\u89d2\u8272\u8f6c\u6362\u3002", "result": "\u6743\u91cd\u52a0\u8f7d\u5f00\u9500\u964d\u4f4e\u9ad8\u8fbe19.39\u500d\uff0cSLO\u8fbe\u6210\u7387\u63d0\u5347\u9ad8\u8fbe4.44\u500d\uff0c\u8bf7\u6c42\u5ef6\u8fdf\u964d\u4f4e65.82%\uff0c\u4e0e\u6700\u5148\u8fdb\u57fa\u7ebf\u76f8\u6bd4\u6210\u672c\u76f8\u5f53\u3002", "conclusion": "HyperFlexis\u901a\u8fc7\u7edf\u4e00\u7684\u7cfb\u7edf\u8bbe\u8ba1\u548c\u591a\u9879\u4f18\u5316\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u670d\u52a1\u7cfb\u7edf\u4e2d\u7684\u8c03\u5ea6\u548c\u6269\u5c55\u6311\u6218\uff0c\u5728\u4fdd\u6301\u6210\u672c\u7ade\u4e89\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u670d\u52a1\u8d28\u91cf\u548c\u6027\u80fd\u3002"}}
{"id": "2508.16308", "categories": ["cs.DC", "cs.CC", "F.2.2; G.2.2"], "pdf": "https://arxiv.org/pdf/2508.16308", "abs": "https://arxiv.org/abs/2508.16308", "authors": ["Jan Bok", "Avinandan Das", "Anna Gujgiczer", "Nikola Jedli\u010dkov\u00e1"], "title": "Generalizing Brooks' theorem via Partial Coloring is Hard Classically and Locally", "comment": null, "summary": "We investigate the classical and distributed complexity of \\emph{$k$-partial\n$c$-coloring} where $c=k$, a natural generalization of Brooks' theorem where\neach vertex should be colored from the palette $\\{1,\\ldots,c\\} =\n\\{1,\\ldots,k\\}$ such that it must have at least $\\min\\{k, \\deg(v)\\}$ neighbors\ncolored differently. Das, Fraigniaud, and Ros{\\'{e}}n~[OPODIS 2023] showed that\nthe problem of $k$-partial $(k+1)$-coloring admits efficient centralized and\ndistributed algorithms and posed an open problem about the status of the\ndistributed complexity of $k$-partial $k$-coloring. We show that the problem\nbecomes significantly harder when the number of colors is reduced from $k+1$ to\n$k$ for every constant $k\\geq 3$.\n  In the classical setting, we prove that deciding whether a graph admits a\n$k$-partial $k$-coloring is NP-complete for every constant $k \\geq 3$,\nrevealing a sharp contrast with the linear-time solvable $(k+1)$-color case.\nFor the distributed LOCAL model, we establish an $\\Omega(n)$-round lower bound\nfor computing $k$-partial $k$-colorings, even when the graph is guaranteed to\nbe $k$-partial $k$-colorable. This demonstrates an exponential separation from\nthe $O(\\log^2 k \\cdot \\log n)$-round algorithms known for $(k+1)$-colorings.\n  Our results leverage novel structural characterizations of ``hard instances''\nwhere partial coloring reduces to proper coloring, and we construct intricate\ngraph gadgets to prove lower bounds via indistinguishability arguments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86k-partial k-coloring\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u53d1\u73b0\u5f53\u989c\u8272\u6570\u4ecek+1\u51cf\u5c11\u5230k\u65f6\uff0c\u95ee\u9898\u96be\u5ea6\u663e\u8457\u589e\u52a0\uff0c\u5728\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u6a21\u578b\u4e2d\u5747\u8868\u73b0\u51faNP\u5b8c\u5168\u6027\u548c\u9ad8\u65f6\u95f4\u4e0b\u754c\u3002", "motivation": "Das\u7b49\u4eba\u4e4b\u524d\u8bc1\u660e\u4e86k-partial (k+1)-coloring\u5b58\u5728\u9ad8\u6548\u7b97\u6cd5\uff0c\u4f46\u63d0\u51fa\u4e86k-partial k-coloring\u7684\u5206\u5e03\u5f0f\u590d\u6742\u6027\u5f00\u653e\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u989c\u8272\u6570\u51cf\u5c11\u5230k\u65f6\u7684\u8ba1\u7b97\u590d\u6742\u6027\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u6784\u9020\u65b0\u9896\u7684\u7ed3\u6784\u7279\u5f81\u6765\u63cf\u8ff0\"\u56f0\u96be\u5b9e\u4f8b\"\uff0c\u5176\u4e2d\u90e8\u5206\u7740\u8272\u7b80\u5316\u4e3a\u5b8c\u5168\u7740\u8272\uff0c\u5e76\u4f7f\u7528\u590d\u6742\u7684\u56fe\u6784\u9020\u548c\u4e0d\u53ef\u533a\u5206\u6027\u8bba\u8bc1\u6765\u8bc1\u660e\u4e0b\u754c\u3002", "result": "\u8bc1\u660e\u5bf9\u4e8ek\u22653\uff0c\u5224\u65ad\u56fe\u662f\u5426\u53efk-partial k-coloring\u662fNP\u5b8c\u5168\u7684\uff1b\u5728\u5206\u5e03\u5f0fLOCAL\u6a21\u578b\u4e2d\uff0c\u5373\u4f7f\u56fe\u4fdd\u8bc1\u53ef\u7740\u8272\uff0c\u4e5f\u9700\u8981\u03a9(n)\u8f6e\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "\u989c\u8272\u6570\u4ecek+1\u51cf\u5c11\u5230k\u5bfc\u81f4\u95ee\u9898\u590d\u6742\u6027\u53d1\u751f\u8d28\u53d8\uff0c\u4ece\u591a\u9879\u5f0f\u65f6\u95f4\u53ef\u89e3\u53d8\u4e3aNP\u5b8c\u5168\uff0c\u5206\u5e03\u5f0f\u590d\u6742\u5ea6\u4ece\u5bf9\u6570\u65f6\u95f4\u53d8\u4e3a\u7ebf\u6027\u65f6\u95f4\u4e0b\u754c\u3002"}}
{"id": "2508.16522", "categories": ["cs.PL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.16522", "abs": "https://arxiv.org/abs/2508.16522", "authors": ["Rohan Yadav", "Joseph Guman", "Sean Treichler", "Michael Garland", "Alex Aiken", "Fredrik Kjolstad", "Michael Bauer"], "title": "On the Duality of Task and Actor Programming Models", "comment": null, "summary": "Programming models for distributed and heterogeneous machines are rapidly\ngrowing in popularity to meet the demands of modern workloads. Task and actor\nmodels are common choices that offer different trade-offs between development\nproductivity and achieved performance. Task-based models offer better\nproductivity and composition of software, whereas actor-based models routinely\ndeliver better peak performance due to lower overheads. While task-based and\nactor-based models appear to be different superficially, we demonstrate these\nprogramming models are duals of each other. Importantly, we show that this\nduality extends beyond functionality to performance, and elucidate techniques\nthat let task-based systems deliver performance competitive with actor-based\nsystems without compromising productivity. We apply these techniques to both\nRealm, an explicitly parallel task-based runtime, as well as Legion, an\nimplicitly parallel task-based runtime. We show these techniques reduce Realm's\noverheads by between 1.7-5.3x, coming within a factor of two of the overheads\nimposed by heavily optimized actor-based systems like Charm++ and MPI. We\nfurther show that our techniques enable between 1.3-5.0x improved strong\nscaling of unmodified Legion applications.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u7d22\u4e86\u4efb\u52a1\u57fa\u548c\u6f14\u5458\u57fa\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\u4e4b\u95f4\u7684\u5bf9\u5076\u6027\uff0c\u5e76\u63d0\u51fa\u6280\u672f\u4f7f\u4efb\u52a1\u57fa\u7cfb\u7edf\u5728\u4fdd\u6301\u751f\u4ea7\u529b\u7684\u540c\u65f6\u8fbe\u5230\u7c7b\u4f3c\u6f14\u5458\u57fa\u7cfb\u7edf\u7684\u6027\u80fd\u6c34\u5e73\u3002", "motivation": "\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\u5728\u5f02\u6784\u8ba1\u7b97\u673a\u4e0a\u65e5\u76ca\u666e\u53ca\uff0c\u4efb\u52a1\u6a21\u578b\u548c\u6f14\u5458\u6a21\u578b\u5404\u6709\u4f18\u52bf\uff1a\u4efb\u52a1\u6a21\u578b\u751f\u4ea7\u529b\u9ad8\u4f46\u6027\u80fd\u8f83\u4f4e\uff0c\u6f14\u5458\u6a21\u578b\u6027\u80fd\u9ad8\u4f46\u5f00\u53d1\u96be\u5ea6\u5927\u3002\u9700\u8981\u627e\u5230\u4e24\u8005\u7684\u5e73\u8861\u70b9\u3002", "method": "\u8bc1\u660e\u4efb\u52a1\u57fa\u548c\u6f14\u5458\u57fa\u7f16\u7a0b\u6a21\u578b\u5b58\u5728\u5bf9\u5076\u6027\uff0c\u5e76\u5c06\u8fd9\u79cd\u5bf9\u5076\u6027\u6269\u5c55\u5230\u6027\u80fd\u5c42\u9762\u3002\u5728Realm\uff08\u663e\u5f0f\u5e76\u884c\u4efb\u52a1\u8fd0\u884c\u65f6\uff09\u548cLegion\uff08\u9690\u5f0f\u5e76\u884c\u4efb\u52a1\u8fd0\u884c\u65f6\uff09\u4e2d\u5e94\u7528\u76f8\u5173\u6280\u672f\u6765\u964d\u4f4e\u5f00\u9500\u3002", "result": "\u6280\u672f\u4f7fRealm\u7684\u5f00\u9500\u964d\u4f4e1.7-5.3\u500d\uff0c\u6027\u80fd\u63a5\u8fd1Charm++\u548cMPI\u7b49\u4f18\u5316\u6f14\u5458\u7cfb\u7edf\u76842\u500d\u5185\u3002Legion\u5e94\u7528\u7684\u5f3a\u7f29\u653e\u6027\u80fd\u63d0\u53471.3-5.0\u500d\u3002", "conclusion": "\u901a\u8fc7\u63ed\u793a\u4efb\u52a1\u57fa\u548c\u6f14\u5458\u57fa\u6a21\u578b\u7684\u5bf9\u5076\u6027\uff0c\u53ef\u4ee5\u5728\u4e0d\u4e22\u5931\u4efb\u52a1\u6a21\u578b\u751f\u4ea7\u529b\u4f18\u52bf\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u7c7b\u4f3c\u6f14\u5458\u6a21\u578b\u7684\u9ad8\u6027\u80fd\u3002"}}
{"id": "2508.15866", "categories": ["cs.PL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.15866", "abs": "https://arxiv.org/abs/2508.15866", "authors": ["Lingxiao Li", "Salar Rahili", "Yiwei Zhao"], "title": "Correctness-Guaranteed Code Generation via Constrained Decoding", "comment": "Published at COLM 2025", "summary": "Language Models (LMs) are increasingly being used for code generation, but\nensuring the correctness of generated programs remains a significant challenge.\nAlthough imperfect code may be acceptable during software development with\nhuman oversight, domains such as video games and robotics require one-shot\ncorrectness for runtime-critical components. We present a constrained decoding\nalgorithm for generating semantically correct programs that incorporates a\ncontext-sensitive parser, which, at each step, outputs a regular expression\nthat satisfies a critical non-extensible property to guide the generation of\nthe next token sequence that can continue to a correct program. To build such a\ncontext-sensitive parser, we propose a framework of a dynamic tree of parsers\n(ToP) during parsing, where each parser corresponds to a modular context-free\ngrammar enriched with contextual information such as variable scopes and type\nconstraints, with tree branches representing ambiguity in the future code\nsegment. We demonstrate our approach through sLua, a strongly typed variant of\nLua, showing that our method can generate semantically correct programs\nconforming to any prescribed scripting API. We further show that, with careful\ndesign, our semantic guarantees extend to runtime correctness, as validated in\nthe application of generating game mechanics for a roguelike video game.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u654f\u611f\u89e3\u6790\u5668\u7684\u7ea6\u675f\u89e3\u7801\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u89e3\u6790\u5668\u6811(ToP)\u6846\u67b6\u751f\u6210\u8bed\u4e49\u6b63\u786e\u7684\u7a0b\u5e8f\uff0c\u786e\u4fdd\u8fd0\u884c\u65f6\u5173\u952e\u7ec4\u4ef6\u7684\u6b63\u786e\u6027", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u96be\u4ee5\u4fdd\u8bc1\u7a0b\u5e8f\u6b63\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u89c6\u9891\u6e38\u620f\u548c\u673a\u5668\u4eba\u7b49\u9700\u8981\u4e00\u6b21\u6027\u6b63\u786e\u6027\u7684\u8fd0\u884c\u65f6\u5173\u952e\u9886\u57df", "method": "\u4f7f\u7528\u4e0a\u4e0b\u6587\u654f\u611f\u89e3\u6790\u5668\uff0c\u5728\u6bcf\u4e00\u6b65\u8f93\u51fa\u6ee1\u8db3\u5173\u952e\u4e0d\u53ef\u6269\u5c55\u5c5e\u6027\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u6765\u6307\u5bfc\u4e0b\u4e00\u4e2atoken\u5e8f\u5217\u7684\u751f\u6210\uff1b\u91c7\u7528\u52a8\u6001\u89e3\u6790\u5668\u6811(ToP)\u6846\u67b6\uff0c\u6bcf\u4e2a\u89e3\u6790\u5668\u5bf9\u5e94\u5e26\u6709\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6a21\u5757\u5316\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5", "result": "\u901a\u8fc7sLua\uff08\u5f3a\u7c7b\u578bLua\u53d8\u4f53\uff09\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u7b26\u5408\u4efb\u4f55\u89c4\u5b9a\u811a\u672cAPI\u7684\u8bed\u4e49\u6b63\u786e\u7a0b\u5e8f\uff0c\u5e76\u5728roguelike\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u8fd0\u884c\u65f6\u6b63\u786e\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4e3a\u8fd0\u884c\u65f6\u5173\u952e\u7ec4\u4ef6\u63d0\u4f9b\u8bed\u4e49\u4fdd\u8bc1\uff0c\u5e76\u53ef\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u6269\u5c55\u5230\u8fd0\u884c\u65f6\u6b63\u786e\u6027"}}
{"id": "2508.15940", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.15940", "abs": "https://arxiv.org/abs/2508.15940", "authors": ["Ahmed Allam", "Youssef Mansour", "Mohamed Shalan"], "title": "ASIC-Agent: An Autonomous Multi-Agent System for ASIC Design with Benchmark Evaluation", "comment": "2025 IEEE International Conference on LLM-Aided Design (ICLAD)", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nRegister Transfer Level (RTL) design, enabling high-quality code generation\nfrom natural language descriptions. However, LLMs alone face significant\nlimitations in real-world hardware design workflows, including the inability to\nexecute code, lack of debugging capabilities, and absence of long-term memory.\nTo address these challenges, we present ASIC-Agent, an autonomous system\ndesigned specifically for digital ASIC design tasks. ASIC-Agent enhances base\nLLMs with a multi-agent architecture incorporating specialized sub-agents for\nRTL generation, verification, OpenLane hardening, and Caravel chip integration,\nall operating within a comprehensive sandbox environment with access to\nessential hardware design tools. The system leverages a vector database\ncontaining documentation, API references, error knowledge, and curated insights\nfrom the open-source silicon community. To evaluate ASIC-Agent's performance,\nwe introduce ASIC-Agent-Bench, the first benchmark specifically designed to\nassess agentic systems in hardware design tasks. We evaluate ASIC-Agent with\nvarious base LLMs, providing quantitative comparisons and qualitative insights\ninto agent behavior across different design scenarios. Our results demonstrate\nthat ASIC-Agent, when powered by Claude 4 Sonnet, successfully automates a\nbroad range of ASIC design tasks spanning varying levels of complexity, showing\nthe potential of significantly accelerating the ASIC design workflow.", "AI": {"tldr": "ASIC-Agent\u662f\u4e00\u4e2a\u4e13\u4e3a\u6570\u5b57ASIC\u8bbe\u8ba1\u4efb\u52a1\u8bbe\u8ba1\u7684\u81ea\u4e3b\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u67b6\u6784\u589e\u5f3a\u57fa\u7840LLM\uff0c\u89e3\u51b3\u4e86LLM\u5728\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u6267\u884c\u3001\u8c03\u8bd5\u548c\u8bb0\u5fc6\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728RTL\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5b9e\u9645\u786c\u4ef6\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u4e2d\u5b58\u5728\u65e0\u6cd5\u6267\u884c\u4ee3\u7801\u3001\u7f3a\u4e4f\u8c03\u8bd5\u80fd\u529b\u548c\u957f\u671f\u8bb0\u5fc6\u7b49\u9650\u5236\uff0c\u9700\u8981\u4e13\u95e8\u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5305\u542bRTL\u751f\u6210\u3001\u9a8c\u8bc1\u3001OpenLane\u786c\u5316\u548cCaravel\u82af\u7247\u96c6\u6210\u7b49\u4e13\u95e8\u5b50\u667a\u80fd\u4f53\uff0c\u5728\u6c99\u76d2\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u5e76\u5229\u7528\u5305\u542b\u6587\u6863\u3001API\u53c2\u8003\u548c\u9519\u8bef\u77e5\u8bc6\u7684\u5411\u91cf\u6570\u636e\u5e93\u3002", "result": "\u5f53\u4f7f\u7528Claude 4 Sonnet\u4f5c\u4e3a\u57fa\u7840LLM\u65f6\uff0cASIC-Agent\u6210\u529f\u81ea\u52a8\u5316\u4e86\u5404\u79cd\u590d\u6742\u5ea6\u7684ASIC\u8bbe\u8ba1\u4efb\u52a1\uff0c\u663e\u793a\u51fa\u663e\u8457\u52a0\u901fASIC\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\u7684\u6f5c\u529b\u3002", "conclusion": "ASIC-Agent\u7cfb\u7edf\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u67b6\u6784\u5728\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3aASIC\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u52a0\u901f\u6f5c\u529b\u3002"}}
{"id": "2508.15898", "categories": ["cs.PL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15898", "abs": "https://arxiv.org/abs/2508.15898", "authors": ["Matthew Sotoudeh", "Zachary Yedidia"], "title": "Automated Formal Verification of a Software Fault Isolation System", "comment": "Short paper to appear at FMCAD 2025, https://fmcad.org/", "summary": "Software fault isolation (SFI) is a popular way to sandbox untrusted\nsoftware. A key component of SFI is the verifier that checks the untrusted code\nis written in a subset of the machine language that guarantees it never reads\nor writes outside of a region of memory dedicated to the sandbox. Soundness\nbugs in the SFI verifier would break the SFI security model and allow the\nsupposedly sandboxed code to read protected memory. In this paper, we address\nthe concern of SFI verifier bugs by performing an automated formal verification\nof a recent SFI system called Lightweight Fault Isolation (LFI). In particular,\nwe formally verify that programs accepted by the LFI verifier never read or\nwrite to memory outside of a designated sandbox region.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u81ea\u52a8\u5316\u5f62\u5f0f\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8f7b\u91cf\u7ea7\u6545\u969c\u9694\u79bb(LFI)\u7cfb\u7edf\u7684\u9a8c\u8bc1\u5668\u7684\u6b63\u786e\u6027\uff0c\u786e\u4fdd\u5176\u63a5\u53d7\u7684\u7a0b\u5e8f\u4e0d\u4f1a\u8bfb\u5199\u6c99\u76d2\u533a\u57df\u5916\u7684\u5185\u5b58\u3002", "motivation": "\u8f6f\u4ef6\u6545\u969c\u9694\u79bb(SFI)\u9a8c\u8bc1\u5668\u4e2d\u7684\u58f0\u97f3\u6027\u6f0f\u6d1e\u4f1a\u7834\u574fSFI\u5b89\u5168\u6a21\u578b\uff0c\u5141\u8bb8\u6c99\u76d2\u4ee3\u7801\u8bfb\u53d6\u53d7\u4fdd\u62a4\u5185\u5b58\uff0c\u56e0\u6b64\u9700\u8981\u9a8c\u8bc1\u9a8c\u8bc1\u5668\u7684\u6b63\u786e\u6027\u3002", "method": "\u91c7\u7528\u81ea\u52a8\u5316\u5f62\u5f0f\u9a8c\u8bc1\u6280\u672f\uff0c\u5bf9LFI\u7cfb\u7edf\u7684\u9a8c\u8bc1\u5668\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u8bc1\u660eLFI\u9a8c\u8bc1\u5668\u63a5\u53d7\u7684\u7a0b\u5e8f\u6c38\u8fdc\u4e0d\u4f1a\u8bfb\u5199\u6307\u5b9a\u6c99\u76d2\u533a\u57df\u5916\u7684\u5185\u5b58\u3002", "conclusion": "\u901a\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u53ef\u4ee5\u786e\u4fddSFI\u9a8c\u8bc1\u5668\u7684\u6b63\u786e\u6027\uff0c\u4ece\u800c\u4fdd\u969c\u8f6f\u4ef6\u6545\u969c\u9694\u79bb\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2508.16095", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.16095", "abs": "https://arxiv.org/abs/2508.16095", "authors": ["Vineet Kumar", "Ajay Kumar M", "Yike Li", "Shreejith Shanker", "Deepu John"], "title": "Bare-Metal RISC-V + NVDLA SoC for Efficient Deep Learning Inference", "comment": "Accepted paper in 2025 IEEE 38th International System-on-Chip\n  Conference (SOCC)", "summary": "This paper presents a novel System-on-Chip (SoC) architecture for\naccelerating complex deep learning models for edge computing applications\nthrough a combination of hardware and software optimisations. The hardware\narchitecture tightly couples the open-source NVIDIA Deep Learning Accelerator\n(NVDLA) to a 32-bit, 4-stage pipelined RISC-V core from Codasip called uRISC_V.\nTo offload the model acceleration in software, our toolflow generates\nbare-metal application code (in assembly), overcoming complex OS overheads of\nprevious works that have explored similar architectures. This tightly coupled\narchitecture and bare-metal flow leads to improvements in execution speed and\nstorage efficiency, making it suitable for edge computing solutions. We\nevaluate the architecture on AMD's ZCU102 FPGA board using NVDLA-small\nconfiguration and test the flow using LeNet-5, ResNet-18 and ResNet-50 models.\nOur results show that these models can perform inference in 4.8 ms, 16.2 ms and\n1.1 s respectively, at a system clock frequency of 100 MHz.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9898\u7684SoC\u67b6\u6784\uff0c\u901a\u8fc7\u786c\u4ef6\u8f6f\u4ef6\u4f18\u5316\u7ed3\u5408\u52a0\u901f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6267\u884c\u901f\u5ea6\u6162\u548c\u5b58\u50a8\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u514b\u670d\u4ee5\u5f80\u5de5\u4f5c\u4e2d\u590d\u6742\u64cd\u4f5c\u7cfb\u7edf\u5f00\u9500\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u786c\u4ef6\u8f6f\u4ef6\u7d27\u5bc6\u8026\u5408\u8bbe\u8ba1\uff1a\u5c06NVDLA\u52a0\u901f\u5668\u4e0eCodasip uRISC-V\u6838\u5fc3\u76f8\u7ed3\u5408\uff0c\u5e76\u4f7f\u7528\u751f\u6210\u7684\u7a7a\u7a0b\u5e94\u7528\u4ee3\u7801\uff08\u6c47\u7f16\uff09\u6765\u51cf\u5c11\u64cd\u4f5c\u7cfb\u7edf\u5f00\u9500\u3002", "result": "\u5728AMD ZCU102 FPGA\u677f\u4e0a\u4ee5100 MHz\u9891\u7387\u6d4b\u8bd5\uff0cLeNet-5\u3001ResNet-18\u548cResNet-50\u6a21\u578b\u7684\u63a8\u7406\u65f6\u95f4\u5206\u522b\u4e3a4.8 ms\u300116.2 ms\u548c1.1 s\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u901f\u5ea6\u548c\u5b58\u50a8\u6548\u7387\u3002", "conclusion": "\u8be5\u7d27\u5bc6\u8026\u5408\u67b6\u6784\u548c\u7a7a\u7a0b\u5de5\u5177\u6d41\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u52a0\u901f\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2508.16063", "categories": ["cs.PL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2508.16063", "abs": "https://arxiv.org/abs/2508.16063", "authors": ["Paul Krogmeier", "P. Madhusudan"], "title": "Synthesizing DSLs for Few-Shot Learning", "comment": null, "summary": "We study the problem of synthesizing domain-specific languages (DSLs) for\nfew-shot learning in symbolic domains. Given a base language and instances of\nfew-shot learning problems, where each instance is split into training and\ntesting samples, the DSL synthesis problem asks for a grammar over the base\nlanguage that guarantees that small expressions solving training samples also\nsolve corresponding testing samples. We prove that the problem is decidable for\na class of languages whose semantics over fixed structures can be evaluated by\ntree automata and when expression size corresponds to parse tree depth in the\ngrammar, and, furthermore, the grammars solving the problem correspond to a\nregular set of trees. We also prove decidability results for variants of the\nproblem where DSLs are only required to express solutions for input learning\nproblems and where DSLs are defined using macro grammars.", "AI": {"tldr": "\u7814\u7a76\u5982\u4f55\u4e3a\u7b26\u53f7\u9886\u57df\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u5408\u6210\u9886\u57df\u7279\u5b9a\u8bed\u8a00(DSL)\uff0c\u8bc1\u660e\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u8be5\u95ee\u9898\u662f\u53ef\u5224\u5b9a\u7684", "motivation": "\u89e3\u51b3\u7b26\u53f7\u9886\u57df\u4e2d\u4e3a\u5c0f\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u81ea\u52a8\u751f\u6210\u5408\u9002\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b", "method": "\u4f7f\u7528\u6811\u81ea\u52a8\u673a\u8bc4\u4f30\u8bed\u4e49\uff0c\u57fa\u4e8e\u8bed\u6cd5\u5206\u6790\u6811\u6df1\u5ea6\u5b9a\u4e49\u8868\u8fbe\u5f0f\u5927\u5c0f\uff0c\u7814\u7a76\u6b63\u5219\u6811\u96c6\u5bf9\u5e94\u7684\u8bed\u6cd5\u89e3\u51b3\u65b9\u6848", "result": "\u8bc1\u660e\u4e86\u5728\u6811\u81ea\u52a8\u673a\u53ef\u8bc4\u4f30\u8bed\u4e49\u4e14\u8868\u8fbe\u5f0f\u5927\u5c0f\u5bf9\u5e94\u8bed\u6cd5\u5206\u6790\u6811\u6df1\u5ea6\u7684\u6761\u4ef6\u4e0b\uff0cDSL\u5408\u6210\u95ee\u9898\u662f\u53ef\u5224\u5b9a\u7684", "conclusion": "\u4e3a\u7b26\u53f7\u5c0f\u6837\u672c\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8DSL\u5408\u6210\u6846\u67b6\uff0c\u5e76\u5efa\u7acb\u4e86\u76f8\u5173\u7406\u8bba\u7684\u53ef\u5224\u5b9a\u6027\u57fa\u7840"}}
{"id": "2508.16151", "categories": ["cs.AR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.16151", "abs": "https://arxiv.org/abs/2508.16151", "authors": ["Yang Liu", "Yi Chen", "Yongwei Zhao", "Yifan Hao", "Zifu Zheng", "Weihao Kong", "Zhangmai Li", "Dongchen Jiang", "Ruiyang Xia", "Zhihong Ma", "Zisheng Liu", "Zhaoyong Wan", "Yunqi Lu", "Ximing Liu", "Hongrui Guo", "Zhihao Yang", "Zhe Wang", "Tianrui Ma", "Mo Zou", "Rui Zhang", "Ling Li", "Xing Hu", "Zidong Du", "Zhiwei Xu", "Qi Guo", "Tianshi Chen", "Yunji Chen"], "title": "Hardwired-Neurons Language Processing Units as General-Purpose Cognitive Substrates", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has established\nlanguage as a core general-purpose cognitive substrate, driving the demand for\nspecialized Language Processing Units (LPUs) tailored for LLM inference. To\novercome the growing energy consumption of LLM inference systems, this paper\nproposes a Hardwired-Neurons Language Processing Unit (HNLPU), which physically\nhardwires LLM weight parameters into the computational fabric, achieving\nseveral orders of magnitude computational efficiency improvement by extreme\nspecialization. However, a significant challenge still lies in the scale of\nmodern LLMs. An ideal estimation on hardwiring gpt-oss 120 B requires\nfabricating at least 6 billion dollars of photomask sets, rendering the\nstraightforward solution economically impractical. Addressing this challenge,\nwe propose the novel Metal-Embedding methodology. Instead of embedding weights\nin a 2D grid of silicon device cells, Metal-Embedding embeds weight parameters\ninto the 3D topology of metal wires. This brings two benefits: (1) a 15x\nincrease in density, and (2) 60 out of 70 layers of photomasks are made\nhomogeneous across chips, including all EUV photomasks. In total,\nMetal-Embedding reduced the photomask cost by 112x, bringing the Non-Recurring\nEngineering (NRE) cost of HNLPU into an economically viable range. Experimental\nresults show that HNLPU achieved 249,960 tokens/s (5,555x/85x of GPU/WSE), 36\ntokens/J (1,047x/283x of GPU/WSE), 13,232 mm2 total die area (29% inscribed\nrectangular area in a 300 mm wafer), \\$184M estimated NRE at 5 nm technology.\nAnalysis shows that HNLPU achieved 8.57x cost-effectiveness and 230x carbon\nfootprint reduction compared to H100 clusters, under an annual weight updating\nassumption.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u8fde\u7ebf\u795e\u7ecf\u5143\u8bed\u8a00\u5904\u7406\u5355\u5143(HNLPU)\uff0c\u901a\u8fc7Metal-Embedding\u65b9\u6cd5\u5c06LLM\u6743\u91cd\u53c2\u6570\u7269\u7406\u786c\u8fde\u7ebf\u5230\u8ba1\u7b97\u7ed3\u6784\u4e2d\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u4e13\u7528AI\u82af\u7247\u7684\u5236\u9020\u6210\u672c\u548c\u80fd\u8017\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4f20\u7edfGPU\u63a8\u7406\u7cfb\u7edf\u7684\u80fd\u8017\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bed\u8a00\u5904\u7406\u5355\u5143\u6765\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u4f46\u76f4\u63a5\u786c\u8fde\u7ebf\u5927\u89c4\u6a21LLM\u53c2\u6570\u7684\u7ecf\u6d4e\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u91c7\u7528Metal-Embedding\u65b9\u6cd5\uff0c\u5c06\u6743\u91cd\u53c2\u6570\u5d4c\u5165\u5230\u91d1\u5c5e\u7ebf\u76843D\u62d3\u6251\u7ed3\u6784\u4e2d\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u76842D\u7845\u5668\u4ef6\u7f51\u683c\uff0c\u5b9e\u73b0\u4e8615\u500d\u5bc6\u5ea6\u63d0\u5347\u548c112\u500d\u5149\u7f69\u6210\u672c\u964d\u4f4e\u3002", "result": "HNLPU\u5b9e\u73b0\u4e86249,960 tokens/s\u7684\u5904\u7406\u901f\u5ea6(\u6bd4GPU\u5feb5,555\u500d)\uff0c36 tokens/J\u7684\u80fd\u6548(\u6bd4GPU\u9ad81,047\u500d)\uff0c\u603b\u82af\u7247\u9762\u79ef13,232 mm\u00b2\uff0cNRE\u6210\u672c1.84\u4ebf\u7f8e\u5143\u3002", "conclusion": "Metal-Embedding\u65b9\u6cd5\u4f7f\u4e13\u7528AI\u82af\u7247\u5728\u7ecf\u6d4e\u4e0a\u53ef\u884c\uff0cHNLPU\u76f8\u6bd4H100\u96c6\u7fa4\u5b9e\u73b0\u4e868.57\u500d\u6210\u672c\u6548\u76ca\u548c230\u500d\u78b3\u8db3\u8ff9\u51cf\u5c11\uff0c\u4e3a\u9ad8\u6548LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16125", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.16125", "abs": "https://arxiv.org/abs/2508.16125", "authors": ["Zhenyang Xu", "Hongxu Xu", "Yongqiang Tian", "Xintong Zhou", "Chengnian Sun"], "title": "Leveraging Large Language Models to Detect Missed Peephole Optimizations", "comment": null, "summary": "By replacing small, suboptimal instruction sequences within programs with a\nmore efficient equivalent, peephole optimization can not only directly optimize\ncode size and performance, but also potentially enables further transformations\nin the subsequent optimization pipeline. Although peephole optimization is a\ncritical class of compiler optimizations, discovering new and effective\npeephole optimizations is challenging as the instruction sets can be extremely\ncomplex and diverse. Previous methods either do not scale well or can only\ncapture a limited subset of peephole optimizations. In this work, we leverage\nLarge Language Models (LLMs) to detect missed peephole optimizations. We\npropose Lampo, a novel automated framework that synergistically combines the\ncreative but unreliable code optimization ability of LLMs with rigorous\ncorrectness verification performed by translation validation tools, integrated\nin a feedback-driven iterative process. Through a comprehensive evaluation\nwithin LLVM ecosystems, we show that Lampo can successfully detect up to 17 out\nof 25 previously reported missed optimizations in LLVM on average, and that 22\nout of 25 can potentially be found by Lampo with different LLMs. For\ncomparison, the state-of-the-art superoptimizer for LLVM, Souper, identified 15\nof them. Moreover, within seven months of development and intermittent\nexperiments, Lampo found 26 missed peephole optimizations, 15 of which have\nbeen confirmed and 6 already fixed. These results demonstrate Lampo's strong\npotential in continuously detecting missed peephole optimizations.", "AI": {"tldr": "Lampo\u662f\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u81ea\u52a8\u68c0\u6d4b\u7f16\u8bd1\u5668\u7aa5\u5b54\u4f18\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408LLMs\u7684\u4ee3\u7801\u4f18\u5316\u80fd\u529b\u548c\u4e25\u683c\u7684\u6b63\u786e\u6027\u9a8c\u8bc1\uff0c\u5728LLVM\u751f\u6001\u7cfb\u7edf\u4e2d\u6210\u529f\u53d1\u73b0\u4e86\u591a\u4e2a\u9057\u6f0f\u7684\u4f18\u5316\u673a\u4f1a\u3002", "motivation": "\u7aa5\u5b54\u4f18\u5316\u662f\u7f16\u8bd1\u5668\u4f18\u5316\u4e2d\u7684\u91cd\u8981\u7c7b\u522b\uff0c\u4f46\u7531\u4e8e\u6307\u4ee4\u96c6\u6781\u5176\u590d\u6742\u591a\u6837\uff0c\u53d1\u73b0\u65b0\u7684\u6709\u6548\u7aa5\u5b54\u4f18\u5316\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u6269\u5c55\u6027\u5dee\uff0c\u8981\u4e48\u53ea\u80fd\u6355\u83b7\u6709\u9650\u7684\u4f18\u5316\u5b50\u96c6\u3002", "method": "\u63d0\u51faLampo\u6846\u67b6\uff0c\u534f\u540c\u7ed3\u5408LLMs\u7684\u521b\u9020\u6027\u4ee3\u7801\u4f18\u5316\u80fd\u529b\u548c\u7ffb\u8bd1\u9a8c\u8bc1\u5de5\u5177\u6267\u884c\u7684\u4e25\u683c\u6b63\u786e\u6027\u9a8c\u8bc1\uff0c\u91c7\u7528\u53cd\u9988\u9a71\u52a8\u7684\u8fed\u4ee3\u8fc7\u7a0b\u3002", "result": "\u5728LLVM\u751f\u6001\u7cfb\u7edf\u4e2d\uff0cLampo\u5e73\u5747\u80fd\u6210\u529f\u68c0\u6d4b\u51fa25\u4e2a\u5148\u524d\u62a5\u544a\u7684\u9057\u6f0f\u4f18\u5316\u4e2d\u768417\u4e2a\uff0c\u5176\u4e2d22\u4e2a\u53ef\u4ee5\u901a\u8fc7\u4e0d\u540cLLMs\u627e\u5230\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6700\u5148\u8fdb\u7684\u8d85\u7ea7\u4f18\u5316\u5668Souper\u53ea\u8bc6\u522b\u4e8615\u4e2a\u3002\u57287\u4e2a\u6708\u5f00\u53d1\u4e2d\uff0cLampo\u53d1\u73b0\u4e8626\u4e2a\u9057\u6f0f\u4f18\u5316\uff0c15\u4e2a\u5df2\u786e\u8ba4\uff0c6\u4e2a\u5df2\u4fee\u590d\u3002", "conclusion": "Lampo\u5728\u6301\u7eed\u68c0\u6d4b\u9057\u6f0f\u7aa5\u5b54\u4f18\u5316\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u8bc1\u660e\u4e86LLMs\u4e0e\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.16376", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.16376", "abs": "https://arxiv.org/abs/2508.16376", "authors": ["Jiaping Tang", "Jianan Mu", "Zizhen Liu", "Ge Yu", "Tenghui Hua", "Bin Sun", "Silin Liu", "Jing Ye", "Huawei Li"], "title": "RIROS: A Parallel RTL Fault SImulation FRamework with TwO-Dimensional Parallelism and Unified Schedule", "comment": "Accepted by ICCAD 2025", "summary": "With the rapid development of safety-critical applications such as autonomous\ndriving and embodied intelligence, the functional safety of the corresponding\nelectronic chips becomes more critical. Ensuring chip functional safety\nrequires performing a large number of time-consuming RTL fault simulations\nduring the design phase, significantly increasing the verification cycle. To\nmeet time-to-market demands while ensuring thorough chip verification, parallel\nacceleration of RTL fault simulation is necessary. Due to the dynamic nature of\nfault propagation paths and varying fault propagation capabilities, task loads\nin RTL fault simulation are highly imbalanced, making traditional\nsingledimension parallel methods, such as structural-level parallelism,\nineffective. Through an analysis of fault propagation paths and task loads, we\nidentify two types of tasks in RTL fault simulation: tasks that are few in\nnumber but high in load, and tasks that are numerous but low in load. Based on\nthis insight, we propose a two-dimensional parallel approach that combines\nstructurallevel and fault-level parallelism to minimize bubbles in RTL fault\nsimulation. Structural-level parallelism combining with workstealing mechanism\nis used to handle the numerous low-load tasks, while fault-level parallelism is\napplied to split the high-load tasks. Besides, we deviate from the traditional\nserial execution model of computation and global synchronization in RTL\nsimulation by proposing a unified computation/global synchronization scheduling\napproach, which further eliminates bubbles. Finally, we implemented a parallel\nRTL fault simulation framework, RIROS. Experimental results show a performance\nimprovement of 7.0 times and 11.0 times compared to the state-of-the-art RTL\nfault simulation and a commercial tool.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e8c\u7ef4\u5e76\u884c\u65b9\u6cd5RIROS\uff0c\u7ed3\u5408\u7ed3\u6784\u7ea7\u548c\u6545\u969c\u7ea7\u5e76\u884c\uff0c\u901a\u8fc7\u7edf\u4e00\u8ba1\u7b97/\u5168\u5c40\u540c\u6b65\u8c03\u5ea6\u6d88\u9664RTL\u6545\u969c\u6a21\u62df\u4e2d\u7684\u6c14\u6ce1\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u5b9e\u73b07-11\u500d\u6027\u80fd\u63d0\u5347", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u7684\u53d1\u5c55\uff0c\u82af\u7247\u529f\u80fd\u5b89\u5168\u9a8c\u8bc1\u9700\u8981\u5927\u91cf\u8017\u65f6\u7684RTL\u6545\u969c\u6a21\u62df\uff0c\u4f20\u7edf\u5355\u7ef4\u5e76\u884c\u65b9\u6cd5\u56e0\u4efb\u52a1\u8d1f\u8f7d\u4e0d\u5747\u8861\u800c\u6548\u7387\u4f4e\u4e0b", "method": "\u5206\u6790\u6545\u969c\u4f20\u64ad\u8def\u5f84\u8bc6\u522b\u4e24\u7c7b\u4efb\u52a1\uff1a\u5c11\u91cf\u9ad8\u8d1f\u8f7d\u4efb\u52a1\u548c\u5927\u91cf\u4f4e\u8d1f\u8f7d\u4efb\u52a1\u3002\u4f7f\u7528\u7ed3\u6784\u7ea7\u5e76\u884c\u914d\u5408\u5de5\u4f5c\u7a83\u53d6\u673a\u5236\u5904\u7406\u4f4e\u8d1f\u8f7d\u4efb\u52a1\uff0c\u6545\u969c\u7ea7\u5e76\u884c\u62c6\u5206\u9ad8\u8d1f\u8f7d\u4efb\u52a1\u3002\u63d0\u51fa\u7edf\u4e00\u8ba1\u7b97/\u5168\u5c40\u540c\u6b65\u8c03\u5ea6\u65b9\u6cd5", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684RTL\u6545\u969c\u6a21\u62df\u5de5\u5177\u548c\u5546\u4e1a\u5de5\u5177\uff0c\u6027\u80fd\u5206\u522b\u63d0\u53477.0\u500d\u548c11.0\u500d", "conclusion": "\u4e8c\u7ef4\u5e76\u884c\u65b9\u6cd5\u548c\u7edf\u4e00\u8c03\u5ea6\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86RTL\u6545\u969c\u6a21\u62df\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u62df\u6548\u7387"}}
