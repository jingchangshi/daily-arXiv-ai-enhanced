<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Compiling to recurrent neurons](https://arxiv.org/abs/2511.14953)
*Joey Velez-Ginorio,Nada Amin,Konrad Kording,Steve Zdancewic*

Main category: cs.PL

TL;DR: 该论文提出了一种将离散结构（特别是迭代）转化为可微分形式的方法，通过将迭代编译为线性循环神经元，使迭代在可微分编程中成为一等公民。


<details>
  <summary>Details</summary>
Motivation: 当前可微分编程中离散结构是二等公民，缺乏显式导数，限制了可微分算法的表达范围，特别是在神经网络中使用条件语句和迭代时会影响梯度计算。

Method: 设计了一个最小化的类型化、高阶线性编程语言Cajal，包含迭代功能，并证明其程序可以正确编译为循环神经元。

Result: 通过实验验证，将循环神经元与神经网络结合用于迭代图像变换任务，网络学习速度更快且数据效率更高。

Conclusion: 循环神经元实现了学习与普通编程离散结构之间的丰富交互，使迭代在可微分编程中成为一等公民。

Abstract: Discrete structures are currently second-class in differentiable programming. Since functions over discrete structures lack overt derivatives, differentiable programs do not differentiate through them and limit where they can be used. For example, when programming a neural network, conditionals and iteration cannot be used everywhere; they can break the derivatives necessary for gradient-based learning to work. This limits the class of differentiable algorithms we can directly express, imposing restraints on how we build neural networks and differentiable programs more generally. However, these restraints are not fundamental. Recent work shows conditionals can be first-class, by compiling them into differentiable form as linear neurons. Similarly, this work shows iteration can be first-class -- by compiling to linear recurrent neurons. We present a minimal typed, higher-order and linear programming language with iteration called $\textsf{Cajal}\scriptstyle(\mathbb{\multimap}, \mathbb{2}, \mathbb{N})$. We prove its programs compile correctly to recurrent neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation, we conduct two experiments where we link these recurrent neurons against a neural network solving an iterative image transformation task. This determines part of its function prior to learning. As a result, the network learns faster and with greater data-efficiency relative to a neural network programmed without first-class iteration. A key lesson is that recurrent neurons enable a rich interplay between learning and the discrete structures of ordinary programming.

</details>


### [2] [Compiling Set Queries into Work-Efficient Tree Traversals](https://arxiv.org/abs/2511.15000)
*Alexander J Root,Christophe Gyurgyik,Purvi Goel,Kayvon Fatahalian,Jonathan Ragan-Kelley,Andrew Adams,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: 本文提出了一种自动生成树结构查询优化的通用方法，通过符号区间分析推导子树修剪条件，支持几何谓词和复合查询融合，能够自动生成高效的树遍历算法。


<details>
  <summary>Details</summary>
Motivation: 现有系统需要为每个查询谓词和数据结构手动实现修剪逻辑，这限制了查询优化的通用性和效率。

Method: 使用符号区间分析推导子树修剪条件，扩展处理几何谓词的新规则，并将复合查询融合为单个树遍历。

Result: 生成的遍历算法与专家编写的查询特定遍历代码行为一致，在手工编写案例不适用时，能渐进优于线性扫描和嵌套循环连接。

Conclusion: 该方法实现了树结构查询优化的通用化和机械化，支持超出标准相等和范围谓词的广泛连接谓词类别。

Abstract: Trees can accelerate queries that search or aggregate values over large collections. They achieve this by storing metadata that enables quick pruning (or inclusion) of subtrees when predicates on that metadata can prove that none (or all) of the data in a subtree affect the query result. Existing systems implement this pruning logic manually for each query predicate and data structure. We generalize and mechanize this class of optimization. Our method derives conditions for when subtrees can be pruned (or included wholesale), expressed in terms of the metadata available at each node. We efficiently generate these conditions using symbolic interval analysis, extended with new rules to handle geometric predicates (e.g., intersection, containment). Additionally, our compiler fuses compound queries (e.g., reductions on filters) into a single tree traversal. These techniques enable the automatic derivation of generalized single-index and dual-index tree joins that support a wide class of join predicates beyond standard equality and range predicates. The generated traversals match the behavior of expert-written code that implements query-specific traversals, and can asymptotically outperform the linear scans and nested-loop joins that existing systems fall back to when hand-written cases do not apply.

</details>


### [3] [Data Layout Polymorphism for Bounding Volume Hierarchies](https://arxiv.org/abs/2511.15028)
*Christophe Gyurgyik,Alexander J Root,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: Scion是一个用于边界体积层次结构数据布局的领域特定语言和编译器，将数据布局与遍历算法解耦，实现跨架构的性能优化。


<details>
  <summary>Details</summary>
Motivation: 现有系统中数据布局与遍历逻辑紧密耦合，阻碍了独立优化和跨上下文性能调优，导致性能与可移植性之间的错误二分法。

Method: 设计Scion语言和编译器，允许独立指定BVH数据布局，支持广泛的布局优化技术，同时保持架构无关性。

Result: 通过系统设计探索发现帕累托最优布局随算法、架构和工作负载特征而变化，并识别出一种结合先前优化技术的新型光线追踪布局。

Conclusion: Scion证明了数据布局与算法解耦的可行性，能够实现跨不同架构和场景的帕累托最优性能。

Abstract: Bounding volume hierarchies are ubiquitous acceleration structures in graphics, scientific computing, and data analytics. Their performance depends critically on data layout choices that affect cache utilization, memory bandwidth, and vectorization -- increasingly dominant factors in modern computing. Yet, in most programming systems, these layout choices are hopelessly entangled with the traversal logic. This entanglement prevents developers from independently optimizing data layouts and algorithms across different contexts, perpetuating a false dichotomy between performance and portability. We introduce Scion, a domain-specific language and compiler for specifying the data layouts of bounding volume hierarchies independent of tree traversal algorithms. We show that Scion can express a broad spectrum of layout optimizations used in high performance computing while remaining architecture-agnostic. We demonstrate empirically that Pareto-optimal layouts (along performance and memory footprint axes) vary across algorithms, architectures, and workload characteristics. Through systematic design exploration, we also identify a novel ray tracing layout that combines optimization techniques from prior work, achieving Pareto-optimality across diverse architectures and scenes.

</details>


### [4] [Cement2: Temporal Hardware Transactions for High-Level and Efficient FPGA Programming](https://arxiv.org/abs/2511.15073)
*Youwei Xiao,Zizhang Luo,Weijie Peng,Yuyang Zou,Yun Liang*

Main category: cs.PL

TL;DR: 提出了一种名为temporal hardware transactions的新抽象，将周期级时序感知引入事务性语言层面，支持描述跨多个时钟周期的规则，在Cement2中实现并验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 解决硬件设计中提高抽象级别以提升生产力与保持对低级细节控制之间的根本矛盾，传统RTL设计的行为正确性保证较弱，而现有高级抽象要么引入不可预测开销，要么限制设计通用性。

Method: 在Cement2（嵌入Rust的事务性HDL）中实现时间硬件事务抽象，通过多阶段分析和优化降低描述抽象级别，生成高效硬件。

Result: 使用Cement2编程了RISC-V软核处理器、自定义CPU指令、线性代数内核和脉动阵列加速器，评估表明与手写RTL设计相比不牺牲性能和资源。

Conclusion: 时间硬件事务抽象为通用FPGA设计任务提供了高适用性，能够在保持高性能的同时显著提升生产力。

Abstract: Hardware design faces a fundamental challenge: raising abstraction to improve productivity while maintaining control over low-level details like cycle accuracy. Traditional RTL design in languages like SystemVerilog composes modules through wiring-style connections that provide weak guarantees for behavioral correctness. While high-level synthesis (HLS) and emerging abstractions attempt to address this, they either introduce unpredictable overhead or restrict design generality. Although transactional HDLs provide a promising foundation by lifting design abstraction to atomic and composable rules, they solely model intra-cycle behavior and do not reflect the native temporal design characteristics, hindering applicability and productivity for FPGA programming scenarios.
  We propose temporal hardware transactions, a new abstraction that brings cycle-level timing awareness to designers at the transactional language level. Our approach models temporal relationships between rules and supports the description of rules whose actions span multiple clock cycles, providing intuitive abstraction to describe multi-cycle architectural behavior. We implement this in Cement2, a transactional HDL embedded in Rust, enabling programming hardware constructors to build both intra-cycle and temporal transactions. Cement2's synthesis framework lowers description abstraction through multiple analysis and optimization phases, generating efficient hardware. With Cement2's abstraction, we program a RISC-V soft-core processor, custom CPU instructions, linear algebra kernels, and systolic array accelerators, leveraging the high-level abstraction for boosted productivity. Evaluation shows that Cement2 does not sacrifice performance and resources compared to hand-coded RTL designs, demonstrating the high applicability for general FPGA design tasks.

</details>


### [5] [SkyEgg: Joint Implementation Selection and Scheduling for Hardware Synthesis using E-graphs](https://arxiv.org/abs/2511.15323)
*Youwei Xiao,Yuyang Zou,Yun Liang*

Main category: cs.PL

TL;DR: SkyEgg是一个新颖的硬件合成框架，通过e-graph数据结构联合优化实现选择与调度，解决了传统HLS工具中这两个步骤分离导致的次优设计问题。


<details>
  <summary>Details</summary>
Motivation: 传统硬件合成方法将实现选择与调度分离优化，导致无法充分利用现代FPGA异构架构的优势，产生次优设计。实现选择通常通过临时模式匹配完成，不考虑对调度的影响，而调度算法则在固定选择方案上使用不准确的延迟估计。

Method: 1. 从输入程序构建e-graph；2. 通过等式饱和应用代数和实现重写规则；3. 在饱和e-graph上构建混合整数线性规划问题来联合优化；4. 提供精确MILP求解和高效的ASAP启发式方法。

Result: 在针对Xilinx Kintex UltraScale+ FPGA的多样化应用基准测试中，SkyEgg相比Vitis HLS实现了平均3.01倍的加速，对于复杂表达式最高可达5.22倍的改进。

Conclusion: SkyEgg通过联合优化实现选择与调度，显著提升了硬件合成质量，证明了e-graph在硬件设计空间探索中的有效性。

Abstract: Hardware synthesis from high-level descriptions remains fundamentally limited by the sequential optimization of interdependent design decisions. Current methodologies, including state-of-the-art high-level synthesis (HLS) tools, artificially separate implementation selection from scheduling, leading to suboptimal designs that cannot fully exploit modern FPGA heterogeneous architectures. Implementation selection is typically performed by ad-hoc pattern matching on operations, a process that does not consider the impact on scheduling. Subsequently, scheduling algorithms operate on fixed selection solutions with inaccurate delay estimates, which misses critical optimization opportunities from appropriately configured FPGA blocks like DSP slices.
  We present SkyEgg, a novel hardware synthesis framework that jointly optimizes implementation selection and scheduling using the e-graph data structure. Our key insight is that both algebraic transformations and hardware implementation choices can be uniformly represented as rewrite rules within an e-graph, modeling the complete design space of implementation candidates to be selected and scheduled together. First, SkyEgg constructs an e-graph from the input program. It then applies both algebraic and implementation rewrites through equality saturation. Finally, it formulates the joint optimization as a mixed-integer linear programming (MILP) problem on the saturated e-graph. We provide both exact MILP solving and an efficient ASAP heuristic for scalable synthesis. Our evaluation on benchmarks from diverse applications targeting Xilinx Kintex UltraScale+ FPGAs demonstrates that SkyEgg achieves an average speedup of 3.01x over Vitis HLS, with improvements up to 5.22x for complex expressions.

</details>


### [6] [Graph Rewriting Language as a Platform for Quantum Diagrammatic Calculi](https://arxiv.org/abs/2511.15581)
*Kayo Tei,Haruto Mishina,Naoki Yamamoto,Kazunori Ueda*

Main category: cs.PL

TL;DR: 使用LMNtal语言构建量子电路简化的图变换验证平台，通过原生图重写规则操作ZX图，量化模式匹配简化规则规范，状态空间探索实现交互式可视化和优化路径验证


<details>
  <summary>Details</summary>
Motivation: 量子电路简化的优化路径系统性发现仍具挑战性，现有工具如PyZX和Quantomatic提供特定领域支持，但需要从不同视角研究量子电路变换的补充方法

Method: 利用通用层次图重写语言LMNtal建立图变换和验证平台，通过原生图变换规则操作ZX图，QLMNtal扩展实现量化模式匹配，状态空间探索支持交互式可视化和验证

Result: 案例研究表明该框架有助于理解优化路径和设计新算法策略，展示了LMNtal作为研究量子电路变换新平台的潜力

Conclusion: 声明式语言LMNtal及其工具链可作为从不同视角研究量子电路变换的新平台

Abstract: Systematic discovery of optimization paths in quantum circuit simplification remains a challenge. Today, ZX-calculus, a computing model for quantum circuit transformation, is attracting attention for its highly abstract graph-based approach. Whereas existing tools such as PyZX and Quantomatic offer domain-specific support for quantum circuit optimization, visualization and theorem-proving, we present a complementary approach using LMNtal, a general-purpose hierarchical graph rewriting language, to establish a diagrammatic transformation and verification platform with model checking. Our methodology shows three advantages: (1) manipulation of ZX-diagrams through native graph transformation rules, enabling direct implementation of basic rules; (2) quantified pattern matching via QLMNtal extensions, greatly simplifying rule specification; and (3) interactive visualization and validation of optimization paths through state space exploration. Through case studies, we demonstrate how our framework helps understand optimization paths and design new algorithms and strategies. This suggests that the declarative language LMNtal and its toolchain could serve as a new platform to investigate quantum circuit transformation from a different perspective.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [PolyKAN: Efficient Fused GPU Operators for Polynomial Kolmogorov-Arnold Network Variants](https://arxiv.org/abs/2511.14852)
*Mingkun Yu,Heming Zhong,Dan Huang,Yutong Lu,Jiazhi Jiang*

Main category: cs.DC

TL;DR: PolyKAN是一个GPU加速的KAN算子库，通过四种优化技术实现了比现有实现快1.2-12倍的推理和训练速度，解决了KAN在GPU上利用率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的Kolmogorov-Arnold Networks并行实现在GPU上利用率低，阻碍了其在实际应用中的采用，特别是在AI for Science领域。

Method: 提出了PolyKAN库，采用四种优化技术：查找表线性插值、2D平铺、两阶段归约和系数布局重排序，将多项式KAN层的前向和反向传播融合为优化的CUDA内核。

Result: 在高端GPU和消费级GPU上，PolyKAN相比Triton + cuBLAS基线实现了1.2-10倍更快的推理和1.4-12倍更快的训练，同时保持相同的精度。

Conclusion: PolyKAN是第一个通用的开源KAN实现，通过GPU加速显著提升了KAN的实际性能，促进了其在科学AI领域的应用。

Abstract: Kolmogorov-Arnold Networks (KANs) promise higher expressive capability and stronger interpretability than Multi-Layer Perceptron, particularly in the domain of AI for Science. However, practical adoption has been hindered by low GPU utilization of existing parallel implementations. To address this challenge, we present a GPU-accelerated operator library, named PolyKAN which is the first general open-source implementation of KAN and its variants. PolyKAN fuses the forward and backward passes of polynomial KAN layers into a concise set of optimized CUDA kernels. Four orthogonal techniques underpin the design: (i) \emph{lookup-table} with linear interpolation that replaces runtime expensive math-library functions; (ii) \emph{2D tiling} to expose thread-level parallelism with preserving memory locality; (iii) a \emph{two-stage reduction} scheme converting scattered atomic updates into a single controllable merge step; and (iv) \emph{coefficient-layout reordering} yielding unit-stride reads under the tiled schedule. Using a KAN variant, Chebyshev KAN, as a case-study, PolyKAN delivers $1.2$--$10\times$ faster inference and $1.4$--$12\times$ faster training than a Triton + cuBLAS baseline, with identical accuracy on speech, audio-enhancement, and tabular-regression workloads on both highend GPU and consumer-grade GPU.

</details>


### [8] [A Graph-Based, Distributed Memory, Modeling Abstraction for Optimization](https://arxiv.org/abs/2511.14966)
*David L. Cole,Jordan Jalving,Jonah Langlieb,Jesse D. Jenkins*

Main category: cs.DC

TL;DR: 提出了一个名为RemoteOptiGraph的分布式优化建模抽象，扩展了Plasmo.jl中的OptiGraph模型，支持在分布式内存环境中处理优化问题，通过Benders分解实现了7.5倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为分布式内存环境中的优化问题提供一个通用、灵活的建模抽象，避免定制化建模方法，并为开发利用分布式内存结构的通用元算法提供基础。

Method: 扩展OptiGraph模型，引入InterWorkerEdges管理跨越工作节点的链接约束，将优化问题表示为包含模块化子问题节点和代数链接约束边的超图。

Result: 在美国西部混合整数容量扩展模型（超过1200万个变量和约束）上测试，RemoteOptiGraph与Benders分解结合比无分解方法快7.5倍。

Conclusion: RemoteOptiGraph提供了一个统一的分布式优化建模框架，支持高效的大规模问题求解，为分布式优化算法开发奠定了基础。

Abstract: We present a general, flexible modeling abstraction for building and working with distributed optimization problems called a RemoteOptiGraph. This abstraction extends the OptiGraph model in Plasmo$.$jl, where optimization problems are represented as hypergraphs with nodes that define modular subproblems (variables, constraints, and objectives) and edges that encode algebraic linking constraints between nodes. The RemoteOptiGraph allows OptiGraphs to be utilized in distributed memory environments through InterWorkerEdges, which manage linking constraints that span workers. This abstraction offers a unified approach for modeling optimization problems on distributed memory systems (avoiding bespoke modeling approaches), and provides a basis for developing general-purpose meta-algorithms that can exploit distributed memory structure such as Benders or Lagrangian decompositions. We implement this abstraction in the open-source package, Plasmo$.$jl and we illustrate how it can be used by solving a mixed integer capacity expansion model for the western United States containing over 12 million variables and constraints. The RemoteOptiGraph abstraction together with Benders decomposition performs 7.5 times faster than solving the same problem without decomposition.

</details>


### [9] [GPU-Initiated Networking for NCCL](https://arxiv.org/abs/2511.15076)
*Khaled Hamidouche,John Bachan,Pak Markthub,Peter-Jan Gootzen,Elena Agostini,Sylvain Jeaugey,Aamir Shafi,Georgios Theodorakis,Manjunath Gorentla Venkata*

Main category: cs.DC

TL;DR: NCCL 2.28引入了设备API，支持设备发起的通信，包括GPU发起的网络(GIN)架构，为MoE等需要紧密计算通信集成的应用提供低延迟通信。


<details>
  <summary>Details</summary>
Motivation: 传统GPU通信由CPU协调，对于需要紧密集成计算和通信的应用，设备发起的通信可以消除CPU协调开销，提高性能。

Method: GIN采用三层架构：NCCL Core主机端API用于设备通信器设置和集体内存窗口注册；设备端API用于从CUDA内核调用远程内存操作；网络插件架构支持GPUDirect异步内核发起和代理两种语义。

Result: 通过集成DeepEP MoE通信库的基准测试显示，GIN在NCCL统一运行时中提供设备发起的通信，结合低延迟操作与NCCL的集体算法和生产基础设施。

Conclusion: GIN架构成功实现了设备发起的GPU通信，为现代AI工作负载特别是MoE架构提供了高效的通信解决方案。

Abstract: Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead.
  NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN's practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure.

</details>


### [10] [BlueBottle: Fast and Robust Blockchains through Subsystem Specialization](https://arxiv.org/abs/2511.15361)
*Preston Vander Vos,Alberto Sonnino,Giorgos Tsimos,Philipp Jovanovic,Lefteris Kokoris-Kogias*

Main category: cs.DC

TL;DR: BlueBottle是一个双层共识架构，通过BB-Core层实现高性能的乐观最终性，通过BB-Guard层提供去中心化时间戳和故障恢复，在保持强安全性和活跃性的同时实现亚秒级最终性和高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 区块链共识面临安全、延迟和去中心化的三难困境。高吞吐量系统通常需要牺牲去中心化或对强对手的鲁棒性，而高度去中心化和安全的系统往往性能较低。

Method: 采用双层共识架构：BB-Core层是n=5f+1协议，通过中等规模的核心验证者集以较低故障容忍度换取更低的最终性延迟；BB-Guard层提供去中心化时间戳、主动错误行为检测和同步恢复路径。

Result: 实验显示BB-Core相比Mysticeti减少20-25%的延迟。在核心层容忍f<3n/5故障节点的情况下，Guard验证者能够检测错误行为、达成共识排除恶意方，并重启核心协议或选择规范分叉。

Conclusion: 该架构在温和同步假设下，实现了乐观的亚秒级最终性和高吞吐量，同时保持了强安全性和活跃性。

Abstract: Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f<3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption.

</details>


### [11] [Multiple Sides of 36 Coins: Measuring Peer-to-Peer Infrastructure Across Cryptocurrencies](https://arxiv.org/abs/2511.15388)
*Lucianna Kiffer,Lioba Heimbach,Dennis Trautwein,Yann Vonlanthen,Oliver Gasser*

Main category: cs.DC

TL;DR: 对36个公共区块链网络进行了首次纵向跨网络测量研究，揭示了网络规模、IPv4/IPv6使用、自治系统和地理集中度等方面的显著差异，并提出了可扩展测量去中心化网络的通用框架。


<details>
  <summary>Details</summary>
Motivation: 区块链技术的底层对等网络层对于大多数生态系统来说仍然不透明，需要系统性的测量研究来理解这些网络的运行状态和特性。

Method: 部署15个主动爬虫，结合社区爬虫数据，进行9个月的纵向测量；利用以太坊发现协议推断19个辅助网络的元数据；开发互联网范围扫描方法，通过简单网络特定负载探测默认端口。

Result: 发现网络规模从不足10个到超过10,000个活跃节点的巨大差异；量化了IPv4与IPv6使用趋势；分析了自治系统和地理集中度；描述了节点流失、昼夜行为和发现协议的覆盖范围与冗余。

Conclusion: 研究揭示了区块链网络在弹性、去中心化和可观测性方面的关键差异，提出的方法为大规模测量去中心化网络提供了通用框架，支持持续监控和更透明的区块链基础设施评估。

Abstract: Blockchain technologies underpin an expanding ecosystem of decentralized applications, financial systems, and infrastructure. However, the fundamental networking layer that sustains these systems, the peer-to-peer layer, of all but the top few ecosystems remains largely opaque. In this paper, we present the first longitudinal, cross-network measurement study of 36 public blockchain networks. Over 9 months, we deployed 15 active crawlers, sourced data from two additional community crawlers, and conducted hourly connectivity probes to observe the evolving state of these networks. Furthermore, by leveraging Ethereum's discovery protocols, we inferred metadata for an additional 19 auxiliary networks that utilize the Ethereum peer discovery protocol. We also explored Internet-wide scans, which only require probing each protocol's default ports with a simple, network-specific payload. This approach allows us to rapidly identify responsive peers across the entire address space without having to implement custom discovery and handshake logic for every blockchain. We validated this method on Bitcoin and similar networks with known ground truth, then applied it to Cardano, which we could not crawl directly.
  Our study uncovers dramatic variation in network size from under 10 to more than 10,000 active nodes. We quantify trends in IPv4 versus IPv6 usage, analyze autonomous systems and geographic concentration, and characterize churn, diurnal behavior, and the coverage and redundancy of discovery protocols. These findings expose critical differences in network resilience, decentralization, and observability. Beyond characterizing each network, our methodology demonstrates a general framework for measuring decentralized networks at scale. This opens the door for continued monitoring, benchmarking, and more transparent assessments of blockchain infrastructure across diverse ecosystems.

</details>


### [12] [When Can You Trust Bitcoin? Value-Dependent Block Confirmation to Determine Transaction Finalit](https://arxiv.org/abs/2511.15421)
*Ethan Hicks,Joseph Oglio,Mikhail Nesterenko,Gokarna Sharma*

Main category: cs.DC

TL;DR: 分析比特币交易确认最终性与交易金额和用户风险承受能力的关系，提出基于区块深度和网络延迟的确认概率模型。


<details>
  <summary>Details</summary>
Motivation: 比特币交易确认通常采用固定区块深度（如6个区块），但这种经验方法没有考虑交易金额和用户风险偏好的差异，需要更科学的确认策略。

Method: 通过模拟不同网络延迟下的分叉情况，结合实际比特币数据分析，建立区块深度与确认撤销概率的关系，并运用前景理论将确认概率与交易金额和用户风险承受能力关联。

Result: 建立了交易确认概率与区块深度的定量关系，发现确认撤销概率随区块深度增加而降低但永不归零，为不同金额和风险偏好的交易提供了个性化确认策略。

Conclusion: 比特币交易确认不应采用固定区块深度，而应根据交易金额和用户风险承受能力动态调整确认策略，提高交易效率的同时控制风险。

Abstract: We study financial transaction confirmation finality in Bitcoin as a function of transaction amount and user risk tolerance. A transaction is recorded in a block on a blockchain. However, a transaction may be revoked due to a fork in the blockchain, the odds of which decrease over time but never reach zero. Therefore, a transaction is considered confirmed if its block is sufficiently deep in the blockchain. This depth is usually set empirically at some fixed number such as six blocks. We analyze forks under varying network delays in simulation and actual Bitcoin data. Based on this analysis, we establish a relationship between block depth and the probability of confirmation revocation due to a fork. We use prospect theory to relate transaction confirmation probability to transaction amount and user risk tolerance.

</details>


### [13] [Proving there is a leader without naming it](https://arxiv.org/abs/2511.15491)
*Laurent Feuilloley,Josef Erik Sedláček,Martin Slávik*

Main category: cs.DC

TL;DR: 该论文研究在局部认证框架下，如何在特定图类中实现亚对数位数的领导者唯一性认证，探讨了网络结构对认证复杂度的影响。


<details>
  <summary>Details</summary>
Motivation: 传统领导者唯一性认证需要O(log n)位证书，但在某些图类中可能不需要标识符编码。研究旨在探索在无环图的拓扑结构中是否可以实现亚对数位数的认证。

Method: 通过分析不同图类的结构特性，包括小直径图、弦图、网格图和稠密图，研究在这些特定拓扑中领导者认证的复杂度下界。

Result: 发现在某些图类中确实可以实现亚对数位数的领导者认证，并且在这些拓扑中标识符可能不是必需的。

Conclusion: 网络结构对局部认证复杂度有重要影响，在特定图类中可以突破传统对数位数的限制，实现更高效的认证方案。

Abstract: Local certification is a mechanism for certifying to the nodes of a network that a certain property holds. In this framework, nodes are assigned labels, called certificates, which are supposed to prove that the property holds. The nodes then communicate with their neighbors to verify the correctness of these certificates.
  Certifying that there is a unique leader in a network is one of the most classical problems in this setting. It is well-known that this can be done using certificates that encode node identifiers and distances in the graph. These require $O(\log n)$ and $O(\log D)$ bits respectively, where $n$ is the number of nodes and $D$ is the diameter. A matching lower bound is known in cycle graphs (where $n$ and $D$ are equal up to multiplicative constants).
  A recent line of work has shown that network structure greatly influences local certification. For example, certifying that a network does not contain triangles takes $Θ(n)$ bits in general graphs, but only $O(\log n)$ bits in graphs of bounded treewidth. This observation raises the question: Is it possible to achieve sublogarithmic leader certification in graph classes that do not contain cycle graphs? And since in that case we cannot write identifiers in a certificate, do we actually need identifiers at all in such topologies? [We answer these questions with results on small diameter graphs, chordal graphs, grids, and dense graphs. See full abstract in the paper.]

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [CoroAMU: Unleashing Memory-Driven Coroutines through Latency-Aware Decoupled Operations](https://arxiv.org/abs/2511.14990)
*Zhuolun Jiang,Songyue Wang,Xiaokun Pei,Tianyue Lu,Mingyu Chen*

Main category: cs.AR

TL;DR: CoroAMU是一个硬件-软件协同设计的系统，通过优化协程代码生成、减少上下文切换和合并内存请求，结合异步内存单元硬件支持，在分解内存系统中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代数据密集型应用面临分解内存系统带来的内存延迟挑战，现有协程方法在平衡延迟隐藏效率和运行时开销方面存在困难。

Method: 提出CoroAMU系统：编译器优化协程代码生成、最小化上下文、合并请求；硬件增强异步内存单元，支持协程特定内存操作和内存引导的分支预测机制。

Result: 在Intel服务器处理器上比最先进协程方法提速1.51倍；在FPGA仿真的分解系统中，结合优化硬件后，在200ns和800ns延迟下分别实现3.39倍和4.87倍平均性能提升。

Conclusion: CoroAMU通过硬件-软件协同设计有效解决了分解内存系统中的内存延迟问题，显著提升了数据密集型应用的性能。

Abstract: Modern data-intensive applications face memory latency challenges exacerbated by disaggregated memory systems. Recent work shows that coroutines are promising in effectively interleaving tasks and hiding memory latency, but they struggle to balance latency-hiding efficiency with runtime overhead. We present CoroAMU, a hardware-software co-designed system for memory-centric coroutines. It introduces compiler procedures that optimize coroutine code generation, minimize context, and coalesce requests, paired with a simple interface. With hardware support of decoupled memory operations, we enhance the Asynchronous Memory Unit to further exploit dynamic coroutine schedulers by coroutine-specific memory operations and a novel memory-guided branch prediction mechanism. It is implemented with LLVM and open-source XiangShan RISC-V processor over the FPGA platform. Experiments demonstrate that the CoroAMU compiler achieves a 1.51x speedup over state-of-the-art coroutine methods on Intel server processors. When combined with optimized hardware of decoupled memory access, it delivers 3.39x and 4.87x average performance improvements over the baseline processor on FPGA-emulated disaggregated systems under 200ns and 800ns latency respectively.

</details>


### [15] [DARE: An Irregularity-Tolerant Matrix Processing Unit with a Densifying ISA and Filtered Runahead Execution](https://arxiv.org/abs/2511.15367)
*Xin Yang,Xin Fan,Zengshi Wang,Jun Han*

Main category: cs.AR

TL;DR: DARE是一种针对稀疏DNN计算的不规则性容忍MPU，通过密度化ISA和过滤式预执行机制解决内存访问和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前DNN在CPU上运行时，硬件与算法协同优化不足导致性能不佳。稀疏DNN存在不规则内存访问模式导致高缓存未命中率，而现有Matrix ISA的步长约束限制了稀疏操作的密度化。

Method: 提出DARE MPU架构，扩展ISA支持稀疏操作的密度化，并配备轻量级过滤式预执行机制来减少预取冗余。

Result: 实验结果显示DARE相比基线性能提升1.04-4.44倍，能效提升1.00-22.8倍，硬件开销比NVR低3.91倍。

Conclusion: DARE通过硬件-算法协同设计有效解决了稀疏DNN计算中的不规则性问题，显著提升了性能和能效。

Abstract: Deep Neural Networks (DNNs) are widely applied across domains and have shown strong effectiveness. As DNN workloads increasingly run on CPUs, dedicated Matrix Processing Units (MPUs) and Matrix Instruction Set Architectures (ISAs) have been introduced. At the same time, sparsity techniques are widely adopted in algorithms to reduce computational cost.
  Despite these advances, insufficient hardware-algorithm co-optimization leads to suboptimal performance. On the memory side, sparse DNNs incur irregular access patterns that cause high cache miss rates. While runahead execution is a promising prefetching technique, its direct application to MPUs is often ineffective due to significant prefetch redundancy. On the compute side, stride constraints in current Matrix ISAs prevent the densification of multiple logically related sparse operations, resulting in poor utilization of MPU processing elements.
  To address these irregularities, we propose DARE, an irregularity-tolerant MPU with a Densifying ISA and filtered Runahead Execution. DARE extends the ISA to support densifying sparse operations and equips a lightweight runahead mechanism with filtering capability. Experimental results show that DARE improves performance by 1.04$\times$ to 4.44$\times$ and increases energy efficiency by 1.00$\times$ to 22.8$\times$ over the baseline, with 3.91$\times$ lower hardware overhead than NVR.

</details>


### [16] [Hemlet: A Heterogeneous Compute-in-Memory Chiplet Architecture for Vision Transformers with Group-Level Parallelism](https://arxiv.org/abs/2511.15397)
*Cong Wang,Zexin Fu,Jiayi Huang,Shanshi Huang*

Main category: cs.AR

TL;DR: Hemlet是一个异构CIM小芯片系统，旨在加速视觉Transformer，通过集成模拟CIM、数字CIM和中间数据处理小芯片来解决单片CIM设计的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer在性能上取得突破，但对内存和计算资源需求巨大，硬件部署面临挑战。单片CIM加速器存在可扩展性问题，而小芯片设计虽然更可扩展但通信成本较高。

Method: 设计异构CIM小芯片系统，集成模拟CIM、数字CIM和中间数据处理小芯片，实现灵活的资源扩展，同时优化通信以减少开销。

Result: 未在摘要中明确说明具体结果，但暗示该方法能够改善吞吐量并减少通信开销。

Conclusion: Hemlet系统为解决视觉Transformer加速的可扩展性和通信效率问题提供了一个有前景的异构CIM小芯片解决方案。

Abstract: Vision Transformers (ViTs) have established new performance benchmarks in vision tasks such as image recognition and object detection. However, these advancements come with significant demands for memory and computational resources, presenting challenges for hardware deployment. Heterogeneous compute-in-memory (CIM) accelerators have emerged as a promising solution for enabling energy-efficient deployment of ViTs. Despite this potential, monolithic CIM-based designs face scalability issues due to the size limitations of a single chip. To address this challenge, emerging chiplet-based techniques offer a more scalable alternative. However, chiplet designs come with their own costs, as they introduce more expensive communication through the network-on-package (NoP) compared to the network-on-chip (NoC), which can hinder improvements in throughput.
  This work introduces Hemlet, a heterogeneous CIM chiplet system designed to accelerate ViT. Hemlet facilitates flexible resource scaling through the integration of heterogeneous analog CIM (ACIM), digital CIM (DCIM), and Intermediate Data Process (IDP) chiplets. To improve throughput while reducing communication ove

</details>


### [17] [A Tensor Compiler for Processing-In-Memory Architectures](https://arxiv.org/abs/2511.15503)
*Peiming Yang,Sankeerth Durvasula,Ivan Fernandez,Mohammad Sadrosadati,Onur Mutlu,Gennady Pekhimenko,Christina Giannoula*

Main category: cs.AR

TL;DR: DCC是一个面向PIM系统的数据中心化ML编译器，通过联合优化数据重排和计算代码，在统一调优过程中解决主机处理器与PIM核心之间数据布局不匹配的问题，显著提升ML内核和LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: PIM设备与主机处理器需要不同的数据布局，导致数据重排成为ML内核执行中的关键性能瓶颈，现有编译方法缺乏对多样化PIM后端和ML内核的系统优化，且忽视了数据重排与计算代码优化的相互依赖性。

Method: 设计DCC编译器，集成多层PIM抽象，将数据分区策略映射到计算循环分区，应用PIM特定代码优化，并利用快速准确的性能预测模型选择最优配置，实现数据重排与计算代码的联合优化。

Result: 在HBM-PIM上实现最高7.68倍加速（平均2.7倍），在AttAcc PIM后端上实现最高13.17倍加速（平均5.75倍）；在端到端LLM推理中，GPT-3和LLaMA-2在AttAcc上最高加速7.71倍（平均4.88倍）。

Conclusion: DCC通过数据重排与计算代码的联合优化，有效解决了PIM系统中数据布局不匹配问题，显著提升了ML内核和LLM推理性能，证明了联合优化方法的重要性。

Abstract: Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.

</details>


### [18] [Instruction-Based Coordination of Heterogeneous Processing Units for Acceleration of DNN Inference](https://arxiv.org/abs/2511.15505)
*Anastasios Petropoulos,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: 提出了一种基于指令的FPGA多处理单元协调架构，用于加速DNN推理，支持可编程的多PU同步和灵活的模型分区，实现了高计算效率和吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决FPGA系统中多个高性能处理单元在加速DNN推理时的协调问题，需要一种灵活的架构来支持多PU同步和动态部署策略。

Method: 采用基于指令的协调架构，包含指令控制器和点对点指令同步单元，将指令分为加载、计算和存储功能组，并开发编译框架将DNN模型转换为可执行指令程序。

Result: 在ResNet-50上的实验结果显示，计算效率高达98%，吞吐量效率相比先前工作提升了2.7倍。

Conclusion: 该架构支持设计空间探索，能够在单批次和多批次性能之间进行动态权衡，无需FPGA重新配置即可实现运行时策略切换。

Abstract: This paper presents an instruction-based coordination architecture for Field-Programmable Gate Array (FPGA)-based systems with multiple high-performance Processing Units (PUs) for accelerating Deep Neural Network (DNN) inference. This architecture enables programmable multi-PU synchronization through instruction controller units coupled with peer-to-peer instruction synchronization units, utilizing instruction types organized into load, compute, and store functional groups. A compilation framework is presented that transforms DNN models into executable instruction programs, enabling flexible partitioning of DNN models into topologically contiguous subgraphs mapped to available PUs. Multiple deployment strategies are supported, enabling pipeline parallelism among PUs and batch-level parallelism across different PU subsets, with runtime switching among them without FPGA reconfiguration. The proposed approach enables design space exploration, supporting dynamic trade-offs between single-batch and multi-batch performance. Experimental results on ResNet-50 demonstrate notable compute efficiency, up to $98\%$, and throughput efficiency gains, up to $2.7\times$, over prior works across different configurations.

</details>


### [19] [Toward Open-Source Chiplets for HPC and AI: Occamy and Beyond](https://arxiv.org/abs/2511.15564)
*Paul Scheffler,Thomas Benz,Tim Fischer,Lorenzo Leone,Sina Arjmandpour,Luca Benini*

Main category: cs.AR

TL;DR: 提出开源chiplet-based RISC-V系统路线图，从Occamy双chiplet系统扩展到Ramora和Ogopogo四chiplet架构，追求高性能计算和AI应用，并探索将开放性扩展到仿真、EDA、PDK和PHY等领域。


<details>
  <summary>Details</summary>
Motivation: 缩小与专有设计之间的性能差距，为高性能计算和人工智能应用提供开源RISC-V解决方案。

Method: 采用chiplet方法，从12nm FinFET的双chiplet Occamy系统开始，扩展到基于mesh-NoC的Ramora双chiplet系统，再到7nm四chiplet的Ogopogo概念架构。

Result: 实现了首个开源、硅验证的双chiplet RISC-V多核系统，并展示了达到最先进计算密度的四chiplet架构。

Conclusion: 开源chiplet-based RISC-V系统在高性能计算和AI领域具有巨大潜力，未来需要将开放性扩展到更广泛的生态系统组件中。

Abstract: We present a roadmap for open-source chiplet-based RISC-V systems targeting high-performance computing and artificial intelligence, aiming to close the performance gap to proprietary designs. Starting with Occamy, the first open, silicon-proven dual-chiplet RISC-V manycore in 12nm FinFET, we scale to Ramora, a mesh-NoC-based dual-chiplet system, and to Ogopogo, a 7nm quad-chiplet concept architecture achieving state-of-the-art compute density. Finally, we explore possible avenues to extend openness beyond logic-core RTL into simulation, EDA, PDKs, and off-die PHYs.

</details>
