<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Development and Evaluation of Adaptive LearningSupport System Based on Ontology of MultipleProgramming Languages](https://arxiv.org/abs/2507.19728)
*Lalita Na Nongkhai,Jingyun Wang,Takahiko Mendori*

Main category: cs.PL

TL;DR: ADVENTURE系统通过基于本体的自适应学习支持系统，为编程学习者提供个性化练习，利用Elo评分系统动态调整难度，实验结果显示自适应模式优于随机模式。


<details>
  <summary>Details</summary>
Motivation: 为编程学习者提供个性化学习支持，通过自适应机制提升学习效果。

Method: 使用CONTINUOUS本体和Elo评分系统，动态调整练习难度，并通过实验比较自适应与随机模式。

Result: 自适应模式在正确提交和通过概念数量上显著优于随机模式。

Conclusion: ADVENTURE系统能有效支持编程学习者的练习。

Abstract: This paper introduces an ontology-based approach within an adaptive learning
support system for computer programming. This system (named ADVENTURE) is
designed to deliver personalized programming exercises that are tailored to
individual learners' skill levels. ADVENTURE utilizes an ontology, named
CONTINUOUS, which encompasses common concepts across multiple programming
languages. The system leverages this ontology not only to visualize programming
concepts but also to provide hints during practice programming exercises and
recommend subsequent programming concepts. The adaptive mechanism is driven by
the Elo Rating System, applied in an educational context to dynamically
estimate the most appropriate exercise difficulty for each learner. An
experimental study compared two instructional modes, adaptive and random, based
on six features derived from 1,186 code submissions across all the experimental
groups. The results indicate significant differences in four of six analyzed
features between these two modes. Notably, the adaptive mode demonstrates a
significant difference over the random mode in two features, the submission of
correct answers and the number of pass concepts. Therefore, these results
underscore that this adaptive learning support system may support learners in
practicing programming exercises.

</details>


### [2] [The Power of Negation in Higher-Order Datalog](https://arxiv.org/abs/2507.20251)
*Angelos Charalambidis,Babis Kostopoulos,Christos Nomikos,Panos Rondogiannis*

Main category: cs.PL

TL;DR: 论文研究了高阶Datalog$^\neg$在良基语义和稳定模型语义下的表达能力，揭示了其与复杂性类的紧密联系。


<details>
  <summary>Details</summary>
Motivation: 探索高阶逻辑编程的表达能力与复杂性类之间的关系，特别是在不同语义下的表现。

Method: 通过使用高阶Datalog$^\neg$的语言特性（如存在谓词变量、部分应用关系和关系枚举）进行理论证明。

Result: 在良基语义下，$(k+1)$-阶Datalog$^\neg$捕获k-EXP；在稳定模型语义下，捕获co-(k-NEXP)（谨慎推理）和k-NEXP（勇敢推理）。

Conclusion: 研究揭示了高阶逻辑编程中阶数与非确定性之间的权衡，表明高阶程序在良基语义下可能超越低阶程序在稳定模型语义下的表达能力。

Abstract: We investigate the expressive power of Higher-Order Datalog$^\neg$ under both
the well-founded and the stable model semantics, establishing tight connections
with complexity classes. We prove that under the well-founded semantics, for
all $k\geq 1$, $(k+1)$-Order Datalog$^\neg$ captures k-EXP, a result that holds
without explicit ordering of the input database. The proof of this fact can be
performed either by using the powerful existential predicate variables of the
language or by using partially applied relations and relation enumeration.
Furthermore, we demonstrate that this expressive power is retained within a
stratified fragment of the language. Under the stable model semantics, we show
that $(k+1)$-Order Datalog$^\neg$ captures co-(k-NEXP) using cautious reasoning
and k-NEXP using brave reasoning, again with analogous results for the
stratified fragment augmented with choice rules. Our results establish a
hierarchy of expressive power, highlighting an interesting trade-off between
order and non-determinism in the context of higher-order logic programming:
increasing the order of programs under the well-founded semantics can surpass
the expressive power of lower-order programs under the stable model semantics.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Quantifying the Performance Gap for Simple Versus Optimal Dynamic Server Allocation Policies](https://arxiv.org/abs/2507.19667)
*Niklas Carlsson,Derek Eager*

Main category: cs.DC

TL;DR: 论文研究了云计算中动态分配服务器资源的策略，分析了简单策略与最优策略的性能差距，并探讨了多站点系统中状态相关路由的潜在性能优势。


<details>
  <summary>Details</summary>
Motivation: 利用云计算的动态资源分配特性，需要一种策略来根据当前负载条件动态分配（和释放）服务器资源。

Method: 提出了几种简单的动态服务器分配策略，并开发了分析模型；设计了半马尔可夫决策模型以确定最优策略的性能。

Result: 量化了简单策略与最优策略之间的性能差距，并研究了多站点系统中状态相关路由的性能优势。

Conclusion: 研究结果对服务提供商在平衡云服务成本和延迟方面具有重要价值。

Abstract: Cloud computing enables the dynamic provisioning of server resources. To
exploit this opportunity, a policy is needed for dynamically allocating (and
deallocating) servers in response to the current load conditions. In this paper
we describe several simple policies for dynamic server allocation and develop
analytic models for their analysis. We also design semi-Markov decision models
that enable determination of the performance achieved with optimal policies,
allowing us to quantify the performance gap between simple, easily implemented
policies, and optimal policies. Finally, we apply our models to study the
potential performance benefits of state-dependent routing in multi-site systems
when using dynamic server allocation at each site. Insights from our results
are valuable to service providers wanting to balance cloud service costs and
delays.

</details>


### [4] [Oranits: Mission Assignment and Task Offloading in Open RAN-based ITS using Metaheuristic and Deep Reinforcement Learning](https://arxiv.org/abs/2507.19712)
*Ngoc Hung Nguyen,Nguyen Van Thieu,Quang-Trung Luu,Anh Tuan Nguyen,Senura Wanasekara,Nguyen Cong Luong,Fatemeh Kavehmadavani,Van-Dinh Nguyen*

Main category: cs.DC

TL;DR: 论文提出Oranits系统模型，通过双重优化方法（CGG-ARO和MA-DDQN）解决Open RAN智能交通系统中任务分配和卸载问题，显著提升任务完成率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视任务间依赖关系和边缘服务器卸载成本，导致决策次优。

Method: 提出CGG-ARO（混沌高斯全局ARO）作为单时隙优化基线，设计MA-DDQN（多智能体双深度Q网络）框架，结合多智能体协调和多动作选择机制。

Result: CGG-ARO提升任务完成率和整体效益约7.1%和7.7%；MA-DDQN进一步提升至11.0%和12.5%。

Conclusion: Oranits在动态ITS环境中实现更快、更自适应和高效的任务处理。

Abstract: In this paper, we explore mission assignment and task offloading in an Open
Radio Access Network (Open RAN)-based intelligent transportation system (ITS),
where autonomous vehicles leverage mobile edge computing for efficient
processing. Existing studies often overlook the intricate interdependencies
between missions and the costs associated with offloading tasks to edge
servers, leading to suboptimal decision-making. To bridge this gap, we
introduce Oranits, a novel system model that explicitly accounts for mission
dependencies and offloading costs while optimizing performance through vehicle
cooperation. To achieve this, we propose a twofold optimization approach.
First, we develop a metaheuristic-based evolutionary computing algorithm,
namely the Chaotic Gaussian-based Global ARO (CGG-ARO), serving as a baseline
for one-slot optimization. Second, we design an enhanced reward-based deep
reinforcement learning (DRL) framework, referred to as the Multi-agent Double
Deep Q-Network (MA-DDQN), that integrates both multi-agent coordination and
multi-action selection mechanisms, significantly reducing mission assignment
time and improving adaptability over baseline methods. Extensive simulations
reveal that CGG-ARO improves the number of completed missions and overall
benefit by approximately 7.1% and 7.7%, respectively. Meanwhile, MA-DDQN
achieves even greater improvements of 11.0% in terms of mission completions and
12.5% in terms of the overall benefit. These results highlight the
effectiveness of Oranits in enabling faster, more adaptive, and more efficient
task processing in dynamic ITS environments.

</details>


### [5] [Accelerating Matrix Multiplication: A Performance Comparison Between Multi-Core CPU and GPU](https://arxiv.org/abs/2507.19723)
*Mufakir Qamar Ansari,Mudabir Qamar Ansari*

Main category: cs.DC

TL;DR: 本文对现代异构平台上矩阵乘法的性能进行了实证分析，比较了顺序、多核CPU和GPU实现的性能表现。


<details>
  <summary>Details</summary>
Motivation: 矩阵乘法是科学计算和机器学习中的基础操作，但其计算复杂度使其成为大规模应用的瓶颈。研究异构平台的性能表现有助于优化实际应用。

Method: 实现了三种算法版本：顺序C++、基于OpenMP的多核CPU并行版本和基于CUDA的GPU并行版本，并在不同尺寸的方阵上进行了基准测试。

Result: 多核CPU实现了12-14倍的加速，而GPU的性能随问题规模显著提升，4096x4096矩阵上达到593倍加速。

Conclusion: 研究表明，即使在消费级硬件上，GPU架构也能显著加速数据并行工作负载。

Abstract: Matrix multiplication is a foundational operation in scientific computing and
machine learning, yet its computational complexity makes it a significant
bottleneck for large-scale applications. The shift to parallel architectures,
primarily multi-core CPUs and many-core GPUs, is the established solution, and
these systems are now ubiquitous from datacenters to consumer laptops. This
paper presents a direct, empirical performance analysis of matrix
multiplication on a modern, consumer-grade heterogeneous platform. We
implemented and benchmarked three versions of the algorithm: a baseline
sequential C++ implementation, a parallel version for its multi-core CPU using
OpenMP, and a massively parallel version for its discrete GPU using CUDA with
shared memory optimizations. The implementations were evaluated with square
matrices of varying dimensions, from 128x128 to 4096x4096. Our results show
that while the parallel CPU provides a consistent speedup of 12-14x over the
sequential version, the GPU's performance scales dramatically with problem
size. For a 4096x4096 matrix, the GPU implementation achieved a speedup of
approximately 593x over the sequential baseline and 45x over the optimized
parallel CPU version. These findings quantitatively demonstrate the profound
impact of many-core GPU architectures on accelerating data-parallel workloads,
underscoring that significant performance gains are readily accessible even on
consumer-level hardware.

</details>


### [6] [MegatronApp: Efficient and Comprehensive Management on Distributed LLM Training](https://arxiv.org/abs/2507.19845)
*Bohan Zhao,Guang Yang,Shuo Chen,Ruitao Liu,Tingrui Zhang,Yongchao He,Wei Xu*

Main category: cs.DC

TL;DR: MegatronApp是一个开源工具链，旨在解决大规模语言模型训练中的系统级挑战，通过四个模块提升训练可靠性、效率和透明度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型参数数量的激增，训练过程变得复杂且跨节点，现有框架虽支持万亿参数训练，但带来了性能优化、诊断和可解释性等新挑战。

Method: MegatronApp引入四个正交且可组合的模块（MegaScan、MegaFBD、MegaDPP和MegaScope），分别针对不同系统级问题。

Result: 这些模块协同工作，显著提升了Megatron-LM生态系统的可靠性、效率和透明度。

Conclusion: MegatronApp通过模块化设计有效解决了大规模语言模型训练中的系统级挑战，为生产级训练提供了强大支持。

Abstract: The rapid escalation in the parameter count of large language models (LLMs)
has transformed model training from a single-node endeavor into a highly
intricate, cross-node activity. While frameworks such as Megatron-LM
successfully integrate tensor (TP), pipeline (PP), and data (DP) parallelism to
enable trillion-parameter training, they simultaneously expose practitioners to
unprecedented systems-level challenges in performance optimization, diagnosis,
and interpretability. MegatronApp is an open-source toolchain expressly
designed to meet these challenges. It introduces four orthogonal, yet
seamlessly composable modules--MegaScan, MegaFBD, MegaDPP, and MegaScope--that
collectively elevate the reliability, efficiency, and transparency of
production-scale training. This paper presents the motivation, architecture,
and distinctive contributions of each module, and elucidates how their
synergistic integration augments the Megatron-LM ecosystem.

</details>


### [7] [A Fast Parallel Median Filtering Algorithm Using Hierarchical Tiling](https://arxiv.org/abs/2507.19926)
*Louis Sugy*

Main category: cs.DC

TL;DR: 提出了一种基于分层分块的新算法，通过减少冗余计算优化中值滤波的计算效率，实现了比现有方法更快的性能。


<details>
  <summary>Details</summary>
Motivation: 中值滤波在图像处理中广泛应用，但传统方法计算成本高，尤其是大核尺寸时效率低下。

Method: 利用排序问题的可分性，提出两种变体：一种基于寄存器操作的数据无关选择网络，另一种基于随机存取内存的数据感知方法。

Result: 算法在CUDA实现中比现有技术快5倍，是8、16、32位数据类型和3×3到75×75核尺寸下最快的。

Conclusion: 新算法显著提升了中值滤波的计算效率，适用于多种数据类型和核尺寸。

Abstract: Median filtering is a non-linear smoothing technique widely used in digital
image processing to remove noise while retaining sharp edges. It is
particularly well suited to removing outliers (impulse noise) or granular
artifacts (speckle noise). However, the high computational cost of median
filtering can be prohibitive. Sorting-based algorithms excel with small kernels
but scale poorly with increasing kernel diameter, in contrast to constant-time
methods characterized by higher constant factors but better scalability, such
as histogram-based approaches or the 2D wavelet matrix.
  This paper introduces a novel algorithm, leveraging the separability of the
sorting problem through hierarchical tiling to minimize redundant computations.
We propose two variants: a data-oblivious selection network that can operate
entirely within registers, and a data-aware version utilizing random-access
memory. These achieve per-pixel complexities of $O(k \log(k))$ and $O(k)$,
respectively, for a $k \times k$ kernel - unprecedented for sorting-based
methods. Our CUDA implementation is up to 5 times faster than the current state
of the art on a modern GPU and is the fastest median filter in most cases for
8-, 16-, and 32-bit data types and kernels from $3 \times 3$ to $75 \times 75$.

</details>


### [8] [Offloading tracing for real-time systems using a scalable cloud infrastructure](https://arxiv.org/abs/2507.19953)
*David Jannis Schmidt,Grigory Fridman,Florian von Zabiensky*

Main category: cs.DC

TL;DR: 本文提出了一种基于微服务和边缘计算的云架构，用于实时系统的软件追踪，解决了传统本地工具处理能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 实时嵌入式系统需要精确的时序和故障检测，但传统追踪工具受限于本地机器的处理能力，难以进行大规模分析。

Method: 采用云架构，将追踪处理任务从开发者机器转移到云端，通过WebSockets和Apache Kafka传输数据，支持长期监控和协作分析。

Result: 架构能够高效处理多并行追踪会话，尽管单会话吞吐量随负载增加略有下降，但整体吞吐量提升。

Conclusion: 该架构不仅适用于开发环境，还可用于具有网络连接的目标系统，实现运行时的现场监控。

Abstract: Real-time embedded systems require precise timing and fault detection to
ensure correct behavior. Traditional tracing tools often rely on local desktops
with limited processing and storage capabilities, which hampers large-scale
analysis. This paper presents a scalable, cloud-based architecture for software
tracing in real-time systems based on microservices and edge computing. Our
approach shifts the trace processing workload from the developer's machine to
the cloud, using a dedicated tracing component that captures trace data and
forwards it to a scalable backend via WebSockets and Apache Kafka. This enables
long-term monitoring and collaborative analysis of target executions, e.g., to
detect and investigate sporadic errors. We demonstrate how this architecture
supports scalable analysis of parallel tracing sessions and lays the foundation
for future integration of rule-based testing and runtime verification. The
evaluation results show that the architecture can handle many parallel tracing
sessions efficiently, although the per-session throughput decreases slightly as
the system load increases, while the overall throughput increases. Although the
design includes a dedicated tracer for analysis during development, this
approach is not limited to such setups. Target systems with network
connectivity can stream reduced trace data directly, enabling runtime
monitoring in the field.

</details>


### [9] [MTASet: A Tree-based Set for Efficient Range Queries in Update-heavy Workloads](https://arxiv.org/abs/2507.20041)
*Daniel Manor,Mor Perry,Moshe Sulamy*

Main category: cs.DC

TL;DR: MTASet是一种基于并发(a,b)-树的数据结构，针对更新密集型工作负载和原子范围查询优化，性能提升2倍，并保证线性一致性。


<details>
  <summary>Details</summary>
Motivation: 现有并发集合在更新密集型工作负载或原子范围查询中表现不佳，需要一种高效解决方案。

Method: 采用并发(a,b)-树实现MTASet，优化更新和范围查询操作。

Result: MTASet在范围查询操作中性能提升2倍，优于现有方案。

Conclusion: MTASet是一种高效且线性一致的并发集合，适用于更新密集和范围查询场景。

Abstract: In concurrent data structures, the efficiency of set operations can vary
significantly depending on the workload characteristics. Numerous concurrent
set implementations are optimized and fine-tuned to excel in scenarios
characterized by predominant read operations. However, they often perform
poorly when confronted with workloads that heavily prioritize updates.
Additionally, current leading-edge concurrent sets optimized for update-heavy
tasks typically lack efficiency in handling atomic range queries. This study
introduces the MTASet, which leverages a concurrent (a,b)-tree implementation.
Engineered to accommodate update-heavy workloads and facilitate atomic range
queries, MTASet surpasses existing counterparts optimized for tasks in range
query operations by up to 2x. Notably, MTASet ensures linearizability.

</details>


### [10] [Racing to Idle: Energy Efficiency of Matrix Multiplication on Heterogeneous CPU and GPU Architectures](https://arxiv.org/abs/2507.20063)
*Mufakir Qamar Ansari,Mudabir Qamar Ansari*

Main category: cs.DC

TL;DR: 论文通过实证测量三种计算架构（多核CPU、离散GPU和集成GPU）在矩阵乘法任务中的性能和能耗，发现离散GPU在性能和能效上均显著优于CPU。


<details>
  <summary>Details</summary>
Motivation: 异构计算系统在性能和功耗之间存在权衡，但量化这些权衡在广泛可用的硬件上仍是一个关键问题。

Method: 使用标准工具（Linux perf和nvidia-smi）测量4096x4096矩阵乘法在三种架构（AMD Ryzen CPU、NVIDIA GTX GPU和AMD Radeon GPU）上的性能和能耗。

Result: 离散GPU性能提升93.5倍，能耗仅为CPU的2%，能效提升50倍。

Conclusion: 离散GPU是性能和能效的最佳选择，为能源感知软件开发提供了明确指导。

Abstract: The paradigm shift towards multi-core and heterogeneous computing, driven by
the fundamental power and thermal limits of single-core processors, has
established energy efficiency as a first-class design constraint in
high-performance computing (HPC). Heterogeneous systems, integrating
traditional multi-core CPUs with specialized accelerators like discrete (dGPU)
and integrated (iGPU) graphics processing units, offer a compelling path to
navigating the trade-offs between performance and power. However, quantifying
these trade-offs on widely accessible hardware remains a critical area of
study. This paper presents a direct, empirical measurement of the performance
and energy-to-solution of a canonical HPC workload -- a 4096x4096 matrix-matrix
multiplication -- on three distinct compute architectures within a single
consumer-grade laptop: a multi-core AMD Ryzen 7 5800H CPU, a discrete NVIDIA
GeForce GTX 1650 GPU, and an integrated AMD Radeon Vega GPU. Using standard,
validated, and minimally intrusive tools such as Linux perf and nvidia-smi, we
find that the discrete GPU is not only the performance leader, achieving a
93.5x speedup over the CPU, but is also the most energy-efficient, consuming
only 2% of the energy used by the CPU, resulting in a 50-fold improvement in
energy efficiency. These findings provide a practical demonstration of the
"race to idle" principle and offer clear, quantitative guidance on
architectural choices for energy-aware software development.

</details>


### [11] [High-Performance Parallel Optimization of the Fish School Behaviour on the Setonix Platform Using OpenMP](https://arxiv.org/abs/2507.20173)
*Haitian Wang,Long Qin*

Main category: cs.DC

TL;DR: 论文研究了在Setonix超级计算平台上使用OpenMP框架对鱼群行为（FSB）算法进行高性能并行优化的方法。


<details>
  <summary>Details</summary>
Motivation: 由于复杂大规模计算的需求增加，需要优化并行算法和计算结构。FSB算法的迭代和计算密集型特性使其成为并行化的理想平台。

Method: 利用Setonix平台和OpenMP框架，分析多线程的线程数、调度策略和OpenMP构造，以提升程序性能。

Result: 实验测试了不同配置，结果为FSB在Setonix上的并行优化提供了见解，也为其他OpenMP并行计算研究提供了参考。

Conclusion: 未来可进一步探索缓存行为和线程调度策略等微观和宏观层面的优化。

Abstract: This paper presents an in-depth investigation into the high-performance
parallel optimization of the Fish School Behaviour (FSB) algorithm on the
Setonix supercomputing platform using the OpenMP framework. Given the
increasing demand for enhanced computational capabilities for complex,
large-scale calculations across diverse domains, there's an imperative need for
optimized parallel algorithms and computing structures. The FSB algorithm,
inspired by nature's social behavior patterns, provides an ideal platform for
parallelization due to its iterative and computationally intensive nature. This
study leverages the capabilities of the Setonix platform and the OpenMP
framework to analyze various aspects of multi-threading, such as thread counts,
scheduling strategies, and OpenMP constructs, aiming to discern patterns and
strategies that can elevate program performance. Experiments were designed to
rigorously test different configurations, and our results not only offer
insights for parallel optimization of FSB on Setonix but also provide valuable
references for other parallel computational research using OpenMP. Looking
forward, other factors, such as cache behavior and thread scheduling strategies
at micro and macro levels, hold potential for further exploration and
optimization.

</details>


### [12] [Ethereum Conflicts Graphed](https://arxiv.org/abs/2507.20196)
*Dvir David Biton,Roy Friedman,Yaron Hay*

Main category: cs.DC

TL;DR: 该论文研究了以太坊智能合约的交互方式，重点分析了调用图、读写集及冲突图的结构，以探索其并行化潜力。


<details>
  <summary>Details</summary>
Motivation: 理解智能合约的交互方式有助于优化性能，如预热对象和并发执行。

Method: 通过追踪超过200万个以太坊区块，分析交易分布、调用树结构及冲突图特性。

Result: 发现冲突图主要呈现星形结构，并揭示了其他重要结构特性。

Conclusion: 研究为以太坊智能合约的并行化提供了理论基础和实际数据支持。

Abstract: Ethereum, a leading blockchain platform, has revolutionized the digital
economy by enabling decentralized transactions and the execution of smart
contracts. Ethereum transactions form the backbone of its network, facilitating
peer-to-peer exchanges and interactions with complex decentralized
applications. Smart contracts extend Ethereum's capabilities by automating
processes and enabling trustless execution of agreements. Hence, understanding
how these smart contracts interact is important in order to facilitate various
performance optimizations, such as warming objects before they are being
accessed and enabling concurrent execution. Of particular interest to us are
the development of the calling graph, as well as the read sets and write sets
of invocations within the same block, and the properties of the associated
conflict graph that is derived from them. The latter is important for
understanding the parallelization potential of smart contracts on Ethereum. We
traced upwards of 2 million recent Ethereum blocks using call tracer and
prestate tracer, out of a total of 21.4 million blocks at the time of writing.
We report on the transactions per block distribution, the structure of call
trees in smart contract invocations, the ratio of value-transfer transactions
to smart contract invocations, as well as provide a comprehensive study of the
structure of blocks' conflict graphs. We find that conflict graphs
predominantly show a star like configuration, as well as other noteworthy
structural properties.

</details>


### [13] [Silent Self-Stabilising Leader Election in Programmable Matter Systems with Holes](https://arxiv.org/abs/2507.20201)
*Jérémie Chalopin,Shantanu Das,Maria Kokkou*

Main category: cs.DC

TL;DR: 本文研究了在可编程物质系统中自稳定领导者选举问题，提出了首个在非公平调度器下保证唯一领导者选举的算法。


<details>
  <summary>Details</summary>
Motivation: 在分布式计算中，领导者选举是关键问题，尤其在可编程物质系统中，协调简单计算实体对解决复杂任务至关重要。现有研究多集中于非自稳定环境，自稳定解决方案较少。

Method: 利用粒子移动能力（此前在自稳定环境中未被利用），结合网格操作，克服了Dolev等人提出的常数内存系统的不可能性结果。

Result: 提出了首个在非公平调度器下保证唯一领导者选举的自稳定算法，适用于连通但不一定单连通的配置。

Conclusion: 通过粒子移动和网格操作，本文克服了传统限制，为可编程物质系统提供了有效的自稳定领导者选举方案。

Abstract: Leader election is a fundamental problem in distributed computing,
particularly within programmable matter systems, where coordination among
simple computational entities is crucial for solving complex tasks. In these
systems, particles (i.e., constant memory computational entities) operate in a
regular triangular grid as described in the geometric Amoebot model. While
leader election has been extensively studied in non self-stabilising settings,
self-stabilising solutions remain more limited. In this work, we study the
problem of self-stabilising leader election in connected (but not necessarily
simply connected) configurations. We present the first self-stabilising
algorithm for programmable matter that guarantees the election of a unique
leader under an unfair scheduler, assuming particles share a common sense of
direction. Our approach leverages particle movement, a capability not
previously exploited in the self-stabilising context. We show that movement in
conjunction with particles operating in a grid can overcome classical
impossibility results for constant-memory systems established by Dolev et al.

</details>


### [14] [A Comparative Study of OpenMP Scheduling Algorithm Selection Strategies](https://arxiv.org/abs/2507.20312)
*Jonas H. Müller Korndörfer,Ali Mohammed,Ahmed Eleliemy,Quentin Guilloteau,Reto Krummenacher,Florina M. Ciorba*

Main category: cs.DC

TL;DR: 论文探讨了基于学习的方法（专家方法和强化学习方法）在OpenMP中选择调度算法的有效性，结合两者可提升性能和适应性。


<details>
  <summary>Details</summary>
Motivation: 随着科学和数据科学应用的复杂性增加，高效的调度和负载平衡技术对性能至关重要。OpenMP提供了多种调度算法，但如何选择最适合的算法是一个挑战。

Method: 提出了专家方法和强化学习方法，并在六个应用和三个系统上进行了性能分析。

Result: 强化学习方法能学习高性能调度决策，但需要大量探索；专家方法依赖先验知识，适应性较差。结合两者可提升性能。

Conclusion: 动态选择调度算法在OpenMP中是可行且有益的，并可扩展到MPI程序。

Abstract: Scientific and data science applications are becoming increasingly complex,
with growing computational and memory demands. Modern high performance
computing (HPC) systems provide high parallelism and heterogeneity across
nodes, devices, and cores. To achieve good performance, effective scheduling
and load balancing techniques are essential. Parallel programming frameworks
such as OpenMP now offer a variety of advanced scheduling algorithms to support
diverse applications and platforms. This creates an instance of the scheduling
algorithm selection problem, which involves identifying the most suitable
algorithm for a given combination of workload and system characteristics.
  In this work, we explore learning-based approaches for selecting scheduling
algorithms in OpenMP. We propose and evaluate expert-based and reinforcement
learning (RL)-based methods, and conduct a detailed performance analysis across
six applications and three systems. Our results show that RL methods are
capable of learning high-performing scheduling decisions, although they require
significant exploration, with the choice of reward function playing a key role.
Expert-based methods, in contrast, rely on prior knowledge and involve less
exploration, though they may not always identify the optimal algorithm for a
specific application-system pair. By combining expert knowledge with RL-based
learning, we achieve improved performance and greater adaptability.
  Overall, this work demonstrates that dynamic selection of scheduling
algorithms during execution is both viable and beneficial for OpenMP
applications. The approach can also be extended to MPI-based programs, enabling
optimization of scheduling decisions across multiple levels of parallelism.

</details>


### [15] [RIMMS: Runtime Integrated Memory Management System for Heterogeneous Computing](https://arxiv.org/abs/2507.20514)
*Serhan Gener,Aditya Ukarande,Shilpa Mysore Srinivasa Murthy,Sahil Hassan,Joshua Mack,Chaitali Chakrabarti,Umit Ogras,Ali Akoglu*

Main category: cs.DC

TL;DR: RIMMS是一个轻量级的运行时内存管理系统，用于异构系统，透明管理数据位置和一致性，显著提升性能并降低编程复杂度。


<details>
  <summary>Details</summary>
Motivation: 异构系统中内存管理的挑战日益增加，现有方法需要显式管理或静态映射，限制了可移植性和扩展性。

Method: RIMMS通过硬件无关的内存抽象层，动态跟踪数据位置、管理一致性，并支持高效内存分配，无需平台特定调整。

Result: 在CPU+GPU和CPU+FPGA平台上，RIMMS分别实现2.43倍和1.82倍加速，性能接近原生CUDA实现，且编程复杂度显著降低。

Conclusion: RIMMS在动态异构环境中提供高性能和编程效率，是一种低成本解决方案。

Abstract: Efficient memory management in heterogeneous systems is increasingly
challenging due to diverse compute architectures (e.g., CPU, GPU, FPGA) and
dynamic task mappings not known at compile time. Existing approaches often
require programmers to manage data placement and transfers explicitly, or
assume static mappings that limit portability and scalability. This paper
introduces RIMMS (Runtime Integrated Memory Management System), a lightweight,
runtime-managed, hardware-agnostic memory abstraction layer that decouples
application development from low-level memory operations. RIMMS transparently
tracks data locations, manages consistency, and supports efficient memory
allocation across heterogeneous compute elements without requiring
platform-specific tuning or code modifications. We integrate RIMMS into a
baseline runtime and evaluate with complete radar signal processing
applications across CPU+GPU and CPU+FPGA platforms. RIMMS delivers up to 2.43X
speedup on GPU-based and 1.82X on FPGA-based systems over the baseline.
Compared to IRIS, a recent heterogeneous runtime system, RIMMS achieves up to
3.08X speedup and matches the performance of native CUDA implementations while
significantly reducing programming complexity. Despite operating at a higher
abstraction level, RIMMS incurs only 1-2 cycles of overhead per memory
management call, making it a low-cost solution. These results demonstrate
RIMMS's ability to deliver high performance and enhanced programmer
productivity in dynamic, real-world heterogeneous environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [16] [MCP4EDA: LLM-Powered Model Context Protocol RTL-to-GDSII Automation with Backend Aware Synthesis Optimization](https://arxiv.org/abs/2507.19570)
*Yiting Wang,Wanghao Ye,Yexiao He,Yiran Chen,Gang Qu,Ang Li*

Main category: cs.AR

TL;DR: MCP4EDA是首个通过自然语言交互控制开源RTL-to-GDSII设计流程的模型上下文协议服务器，利用LLM优化合成脚本，实现闭环优化。


<details>
  <summary>Details</summary>
Motivation: 传统EDA流程依赖线负载模型，缺乏实际后端性能数据，导致合成估计与物理实现存在差距。MCP4EDA旨在通过LLM利用真实后端数据优化设计流程。

Method: 集成Yosys、OpenLane等工具，LLM通过分析实际布局后时序、功耗和面积数据，迭代优化合成TCL脚本，实现闭环优化。

Result: 实验显示，相比默认合成流程，时序收敛提升15-30%，面积减少10-20%。

Conclusion: MCP4EDA是首个实用的LLM控制端到端开源EDA自动化系统，填补了合成与物理实现间的鸿沟。

Abstract: This paper presents MCP4EDA, the first Model Context Protocol server that
enables Large Language Models (LLMs) to control and optimize the complete
open-source RTL-to-GDSII design flow through natural language interaction. The
system integrates Yosys synthesis, Icarus Verilog simulation, OpenLane
place-and-route, GTKWave analysis, and KLayout visualization into a unified
LLM-accessible interface, enabling designers to execute complex multi-tool EDA
workflows conversationally via AI assistants such as Claude Desktop and Cursor
IDE. The principal contribution is a backend-aware synthesis optimization
methodology wherein LLMs analyze actual post-layout timing, power, and area
metrics from OpenLane results to iteratively refine synthesis TCL scripts,
establishing a closed-loop optimization system that bridges the traditional gap
between synthesis estimates and physical implementation reality. In contrast to
conventional flows that rely on wire-load models, this methodology leverages
real backend performance data to guide synthesis parameter tuning, optimization
sequence selection, and constraint refinement, with the LLM functioning as an
intelligent design space exploration agent. Experimental evaluation on
representative digital designs demonstrates 15-30% improvements in timing
closure and 10-20% area reduction compared to default synthesis flows,
establishing MCP4EDA as the first practical LLM-controlled end-to-end
open-source EDA automation system. The code and demo are avaiable at:
http://www.agent4eda.com/

</details>


### [17] [ChipletPart: Scalable Cost-Aware Partitioning for 2.5D Systems](https://arxiv.org/abs/2507.19819)
*Alexander Graening,Puneet Gupta,Andrew B. Kahng,Bodhisatta Pramanik,Zhiang Wang*

Main category: cs.AR

TL;DR: ChipletPart是一种成本驱动的2.5D系统分区工具，通过遗传算法和模拟退火优化芯片分区，显著降低成本并支持异构制造技术。


<details>
  <summary>Details</summary>
Motivation: 随着芯片分区的需求增加，现有分区工具在成本和可行性上存在不足，需要一种更高效的解决方案。

Method: 结合遗传算法的技术分配与分区方法，以及模拟退火的芯片布局规划，集成复杂的成本模型。

Result: 相比现有工具，ChipletPart降低成本最高达58%，异构集成进一步降低成本43%。

Conclusion: ChipletPart为芯片分区提供了高效、低成本的解决方案，并开源工具以促进社区发展。

Abstract: Industry adoption of chiplets has been increasing as a cost-effective option
for making larger high-performance systems. Consequently, partitioning large
systems into chiplets is increasingly important. In this work, we introduce
ChipletPart - a cost-driven 2.5D system partitioner that addresses the unique
constraints of chiplet systems, including complex objective functions, limited
reach of inter-chiplet I/O transceivers, and the assignment of heterogeneous
manufacturing technologies to different chiplets. ChipletPart integrates a
sophisticated chiplet cost model with its underlying genetic algorithm-based
technology assignment and partitioning methodology, along with a simulated
annealing-based chiplet floorplanner. Our results show that: (i) ChipletPart
reduces chiplet cost by up to 58% (20% geometric mean) compared to
state-of-the-art min-cut partitioners, which often yield floorplan-infeasible
solutions; (ii) ChipletPart generates partitions with up to 47% (6% geometric
mean) lower cost as compared to the prior work Floorplet; and (iii) for the
testcases we study, heterogeneous integration reduces cost by up to 43% (15%
geometric mean) compared to homogeneous implementations. We also present case
studies that show how changes in packaging or inter-chiplet signaling
technologies can affect partitioning solutions. Finally, we make ChipletPart,
the underlying chiplet cost model, and a chiplet testcase generator available
as open-source tools for the community.

</details>


### [18] [AxOSyn: An Open-source Framework for Synthesizing Novel Approximate Arithmetic Operators](https://arxiv.org/abs/2507.20007)
*Siva Satyendra Sahoo,Salim Ullah,Akash Kumar*

Main category: cs.AR

TL;DR: AxOSyn是一个开源框架，用于近似算术算子的设计空间探索，支持多种抽象级别的选择和合成方法，旨在实现能源高效的近似计算。


<details>
  <summary>Details</summary>
Motivation: 边缘AI部署日益复杂，需要针对资源受限嵌入式系统的能源高效解决方案。近似计算通过允许计算中的可控不准确性，成为提高能源效率的有前景方法。

Method: 提出AxOSyn框架，支持选择和合成方法，允许自定义近似评估方法，并在算子级别和应用特定级别进行设计空间探索。

Result: AxOSyn提供了一个灵活且有效的方法，用于实现能源高效的近似算子。

Conclusion: AxOSyn框架为近似算术算子的设计空间探索提供了新的工具，有助于在边缘AI部署中实现更高的能源效率。

Abstract: Edge AI deployments are becoming increasingly complex, necessitating
energy-efficient solutions for resource-constrained embedded systems.
Approximate computing, which allows for controlled inaccuracies in
computations, is emerging as a promising approach for improving power and
energy efficiency. Among the key techniques in approximate computing are
approximate arithmetic operators (AxOs), which enable application-specific
optimizations beyond traditional computer arithmetic hardware reduction-based
methods, such as quantization and precision scaling. Existing design space
exploration (DSE) frameworks for approximate computing limit themselves to
selection-based approaches or custom synthesis at fixed abstraction levels,
which restricts the flexibility required for finding application-specific
optimal solutions. Further, the tools available for the DSE of AxOs are quite
limited in terms of exploring different approximation models and extending the
analysis to different granularities. To this end, we propose AxOSyn, an
open-source framework for the DSE of AxOs that supports both selection and
synthesis approaches at various abstraction levels. AxOSyn allows researchers
to integrate custom methods for evaluating approximations and facilitates DSE
at both the operator-level and application-specific. Our framework provides an
effective methodology for achieving energy-efficient, approximate operators.

</details>


### [19] [RoCE BALBOA: Service-enhanced Data Center RDMA for SmartNICs](https://arxiv.org/abs/2507.20412)
*Maximilian Jakob Heer,Benjamin Ramhorst,Yu Zhu,Luhao Liu,Zhiyi Hu,Jonas Dann,Gustavo Alonso*

Main category: cs.AR

TL;DR: RoCE BALBOA是一个开源的、兼容RoCE v2的RDMA协议栈，支持数百个队列对和100G速率，适用于构建加速器和智能网卡。它展示了与商用NIC相当的延迟和性能，并通过加密和深度学习预处理等用例验证了其潜力。


<details>
  <summary>Details</summary>
Motivation: 数据中心的数据密集型应用（如机器学习）使网络成为瓶颈，推动了高效网络协议和基础设施的发展。RoCE BALBOA旨在提供一种可定制、可扩展的RDMA协议栈，以支持智能网卡和加速器的开发。

Method: RoCE BALBOA是一个开源RDMA协议栈，支持RoCE v2，可扩展到数百个队列对和100G速率。通过FPGA部署，验证其性能与商用NIC相当，并探索了加密和深度学习预处理等用例。

Result: BALBOA在FPGA集群中部署，表现出与商用NIC相当的延迟和性能。通过加密和深度学习预处理用例展示了其在智能网卡和加速器中的潜力。

Conclusion: RoCE BALBOA为智能网卡和加速器的开发提供了灵活、高性能的基础设施，展示了其在网络数据流处理中的广泛应用潜力。

Abstract: Data-intensive applications in data centers, especially machine learning
(ML), have made the network a bottleneck, which in turn has motivated the
development of more efficient network protocols and infrastructure. For
instance, remote direct memory access (RDMA) has become the standard protocol
for data transport in the cloud as it minimizes data copies and reduces
CPU-utilization via host-bypassing. Similarly, an increasing amount of network
functions and infrastructure have moved to accelerators, SmartNICs, and
in-network computing to bypass the CPU. In this paper we explore the
implementation and deployment of RoCE BALBOA, an open-source, RoCE
v2-compatible, scalable up to hundreds of queue-pairs, and 100G-capable
RDMA-stack that can be used as the basis for building accelerators and
smartNICs. RoCE BALBOA is customizable, opening up a design space and offering
a degree of adaptability not available in commercial products. We have deployed
BALBOA in a cluster using FPGAs and show that it has latency and performance
characteristics comparable to commercial NICs. We demonstrate its potential by
exploring two classes of use cases. One involves enhancements to the protocol
for infrastructure purposes (encryption, deep packet inspection using ML). The
other showcases the ability to perform line-rate compute offloads with deep
pipelines by implementing commercial data preprocessing pipelines for
recommender systems that process the data as it arrives from the network before
transferring it directly to the GPU. These examples demonstrate how BALBOA
enables the exploration and development of SmartNICs and accelerators operating
on network data streams.

</details>


### [20] [Demystifying the 7-D Convolution Loop Nest for Data and Instruction Streaming in Reconfigurable AI Accelerators](https://arxiv.org/abs/2507.20420)
*Md Rownak Hossain Chowdhury,Mostafizur Rahman*

Main category: cs.AR

TL;DR: 提出了一种新的框架，通过将7维卷积循环嵌套重新解释为硬件中心的数据和指令流问题，优化AI加速中的卷积计算。


<details>
  <summary>Details</summary>
Motivation: 卷积在AI加速中占计算负载的80-90%，现有方法（如CGRAs和FPGAs）依赖循环展开或GEMM变换，导致数据移动和控制指令开销大。

Method: 将卷积循环嵌套视为由硬件参数（如计算单元分布、互连拓扑）决定的空间和时间映射，支持轻量级灵活部署。

Result: 在MAVeC加速器上实现，VGG-16推理中PE利用率超90%，吞吐量达1.56 TFLOPs/sec和12.7 KIPS。

Conclusion: 该方法有效减少控制开销，提升数据局部性，无需依赖传统变换方法即可高效执行大规模卷积。

Abstract: Convolution remains the most compute-intensive operation in AI acceleration,
often constituting over 80-90% of the workload. Existing approaches in spatial
architectures such as coarse-grained reconfigurable arrays (CGRAs) and
field-programmable gate arrays (FPGAs) frequently rely on loop unrolling or
GEMM-based matrix transformations, introducing significant overhead in both
data movement and instruction control. This paper presents a new framework
designed to systematically demystify the 7-dimensional convolution loop nest by
reinterpreting it as a hardware-centric data and instruction streaming problem.
Instead of treating the loop nest as a fixed computational construct, our
approach exposes its structure as a set of spatial and temporal mappings
governed by hardware parameters such as compute element distribution,
interconnect topology, and reconfigurability. This abstraction supports
lightweight, flexible deployment of convolution without reliance on heavyweight
transformations or reordering schemes. We demonstrate the application of our
approach on the MAVeC accelerator. We detail the implementation of convolution
operations in MAVeC and extend the framework to support full model execution on
VGG-16. Our profiling reveals high PE utilization (over 90%), significant fold
reuse, and scalable throughput up to 1.56 TFLOPs/sec and 12.7 KIPS for
end-to-end VGG-16 inference. These results validate the efficacy of our
approach in minimizing control overhead, improving data locality, and enabling
efficient large-scale convolution execution without reliance on conventional
transformation-based methods.

</details>
