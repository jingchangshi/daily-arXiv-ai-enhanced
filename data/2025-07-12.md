<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [On Propositional Program Equivalence (extended abstract)](https://arxiv.org/abs/2507.07480)
*Tobias Kappé*

Main category: cs.PL

TL;DR: 论文探讨了命题程序等价性在(G)KAT框架下的最新进展，指出在抽象语句语义后，等价性问题变得可判定且实用。


<details>
  <summary>Details</summary>
Motivation: 研究命题程序等价性，以解决一般程序等价性不可判定的问题。

Method: 基于(G)KAT（Guarded Kleene Algebra with Tests）理论框架进行分析。

Result: 命题程序等价性在抽象语义后是可判定的，且具有实际可行性。

Conclusion: (G)KAT为命题程序等价性提供了有效的理论工具，推动了该领域的发展。

Abstract: General program equivalence is undecidable. However, if we abstract away the
semantics of statements, then this problem becomes not just decidable, but
practically feasible. For instance, a program of the form "if $b$ then $e$ else
$f$" should be equivalent to "if not $b$ then $f$ else $e$" - no matter what
$b$, $e$ and $f$ are. This kind of equivalence is known as propositional
equivalence. In this extended abstract, we discuss recent developments in
propositional program equivalence from the perspective of (Guarded) Kleene
Algebra with Tests, or (G)KAT.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Distributed Training under Packet Loss](https://arxiv.org/abs/2507.07114)
*Erez Weintraub,Ron Banner,Ariel Orda*

Main category: cs.DC

TL;DR: 提出了一种新型分布式训练框架，可在不可靠连接下保持模型准确性和收敛性，无需修改模型代码或优化器。


<details>
  <summary>Details</summary>
Motivation: 现有分布式框架假设可靠连接，导致延迟和可扩展性问题，而不可靠连接可能影响模型准确性和收敛性。

Method: 采用两阶段防御机制：无偏梯度聚合和有界漂移参数广播，确保梯度估计一致性和模型参数稳定性。

Result: 在LLAMA2 7B模型上实验表明，容忍10%丢包率时困惑度变化不超过0.8%。

Conclusion: 该框架填补了高效通信协议与现代大模型训练需求之间的鸿沟，支持在普通或广域网络上进行鲁棒的高吞吐学习。

Abstract: State-of-the-art language and vision models are routinely trained across
thousands of GPUs, often spanning multiple data-centers, yet today's
distributed frameworks still assume reliable connections (e.g., InfiniBand or
RoCE). The resulting acknowledgment traffic and retransmissions inflate tail
latencies and limit scalability. Leveraging unreliable connections will reduce
latency but may sacrifice model accuracy and convergence once packets are
dropped. A principled, end-to-end solution that preserves accuracy and
convergence guarantees under genuine packet loss has previously been missing.
We address this critical gap by introducing a novel distributed training
framework capable of operating over unreliable connections, offering unbiased
gradient aggregation and bounded parameter drift without modifying model code
or optimizers. The key insight is a two-stage defense against missing messages:
(i) Unbiased gradient aggregation: each worker reconstructs a consistent
gradient estimate from whatever packets arrive, guaranteeing expectation-level
correctness; and (ii) Bounded-drift parameter broadcasts: we prove the
inter-worker model discrepancy remains O(1) even after arbitrarily many
iterations, preventing the unbounded divergence typical of asynchronous setups.
Analytical bounds are matched by experiments on the LLAMA2 7B model with 64
GPUs: tolerating 10% random packet loss yields at most 0.8% perplexity change.
This work bridges the gap between communication-efficient datacenter protocols
and the accuracy and generalization guarantees demanded by modern large-model
training, enabling robust, high-throughput learning on commodity or wide-area
networks.

</details>


### [3] [Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces](https://arxiv.org/abs/2507.07116)
*Juan Cano-Benito,Andrea Cimmino,Sven Hertling,Heiko Paulheim,Raúl García-Castro*

Main category: cs.DC

TL;DR: 论文系统评估了不同类型DLT（公有、私有、混合）在语义数据存储中的表现，发现私有DLT效率最高，混合DLT在审计与效率间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 解决数据空间中语义数据在DLT上高效存储的空白问题。

Method: 使用真实知识图谱对不同类型DLT进行性能、存储效率、资源消耗及语义数据更新查询能力的比较。

Result: 私有DLT在语义内容存储和管理上最有效，混合DLT在公共审计与操作效率间提供平衡。

Conclusion: 根据数据主权需求选择合适DLT基础设施对去中心化数据生态系统至关重要。

Abstract: Data spaces are emerging as decentralised infrastructures that enable
sovereign, secure, and trustworthy data exchange among multiple participants.
To achieve semantic interoperability within these environments, the use of
semantic web technologies and knowledge graphs has been proposed. Although
distributed ledger technologies (DLT) fit as the underlying infrastructure for
data spaces, there remains a significant gap in terms of the efficient storage
of semantic data on these platforms. This paper presents a systematic
evaluation of semantic data storage across different types of DLT (public,
private, and hybrid), using a real-world knowledge graph as an experimental
basis. The study compares performance, storage efficiency, resource
consumption, and the capabilities to update and query semantic data. The
results show that private DLTs are the most efficient for storing and managing
semantic content, while hybrid DLTs offer a balanced trade-off between public
auditability and operational efficiency. This research leads to a discussion on
the selection of the most appropriate DLT infrastructure based on the data
sovereignty requirements of decentralised data ecosystems.

</details>


### [4] [Collective Communication Profiling of Modern-day Machine Learning Workloads](https://arxiv.org/abs/2507.07117)
*Jit Gupta,Andrew Li,Tarun Banka,Ariel Cohen,T. Sridhar,Raj Yavatkar*

Main category: cs.DC

TL;DR: 分析分布式高性能系统中机器学习任务的集体通信行为，提出优化网络资源配置的建议。


<details>
  <summary>Details</summary>
Motivation: 机器学习任务中的集体通信操作可能导致网络拥塞和性能下降，需分析其行为以优化网络资源。

Method: 通过Nvidia Collective Communication Library记录通信行为，调整并行度、节点数和模型类型等参数。

Result: 分析显示现有集体通信框架和网络拓扑需改进以适应网络异常对工作负载的影响。

Conclusion: 建议重新设计集体通信框架和网络拓扑以提升机器学习任务的性能。

Abstract: Machine Learning jobs, carried out on large number of distributed high
performance systems, involve periodic communication using operations like
AllReduce, AllGather, and Broadcast. These operations may create high bandwidth
and bursty traffic patterns, leading to network congestion and packet loss,
thus impacting the performance of these jobs. Hence it is imperative to analyze
these patterns, which can be helpful in provisioning network resources
depending on the type of machine learning workloads. In this poster we carry
out extensive analysis of the collective communication behavior seen in a wide
variety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we
instrument Nvidia Collective Communication Library logging functionality for
richer context about the collectives and workloads. We adjust configuration
parameters that influence collective communication behavior, such as
parallelism, number of nodes, and model type. This overview presents and
discusses some of the results on the collective communication behavior for the
open source DeepSeek V3 inferencing model, which includes operation type and
count, transfer sizes per operation, and request size distribution. Our
analysis shows that it makes sense to rethink current collective communication
frameworks and network topologies so as to accommodate the effect of network
anomalies on the mentioned workloads.

</details>


### [5] [Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding](https://arxiv.org/abs/2507.07120)
*Nidhi Bhatia,Ankit More,Ritika Borkar,Tiyasa Mitra,Ramon Matas,Ritchie Zhao,Maximilian Golub,Dheevatsa Mudigere,Brian Pharris,Bita Darvish Rouhani*

Main category: cs.DC

TL;DR: Helix Parallelism是一种混合执行策略，通过KV并行和TP/EP并行优化LLM推理效率，减少延迟并支持更大批次。


<details>
  <summary>Details</summary>
Motivation: 随着LLM扩展到数百万token的KV历史，实时自回归解码面临KV缓存读取和FFN权重访问的瓶颈，传统并行方法效率有限。

Method: 提出Helix Parallelism，结合KV并行和TP/EP并行，通过轻量通信和Helix HOP-B技术最小化通信开销。

Result: 相比传统方法，Helix在固定批次下减少1.5倍TTL，支持32倍更大批次，提升吞吐-延迟平衡。

Conclusion: Helix推动了超长序列实时推理的实用性，显著优化了Blackwell架构的效率。

Abstract: As LLMs scale to multi-million-token KV histories, real-time autoregressive
decoding under tight Token-to-Token Latency (TTL) constraints faces growing
pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)
weights and reading long KV caches. While Tensor Parallelism (TP) helps
mitigate the cost of FFN weight reads, it does not scale well for attention.
When TP width exceeds the number of KV heads, it leads to inefficient KV
duplication, limits parallelism, and constrains batch size. Simultaneously,
DRAM reads for long KV histories scale linearly with batch size, further
capping efficiency.
  We introduce Helix Parallelism, a hybrid execution strategy that applies KV
parallelism during attention to shard KV caches across GPUs, then reuses the
same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN
computation. To preserve exact attention behavior, Helix includes a lightweight
communication step. To minimize the exposed communication cost, we introduce
Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through
batchwise overlap, preserving low TTL while improving GPU efficiency. Compared
to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at
fixed batch sizes and supports up to 32x larger batches under the same latency
budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on
Blackwell and making real-time inference with ultra-long-sequence practical.

</details>


### [6] [Ampere: Communication-Efficient and High-Accuracy Split Federated Learning](https://arxiv.org/abs/2507.07130)
*Zihan Zhang,Leon Wong,Blesson Varghese*

Main category: cs.DC

TL;DR: Ampere是一种新型的联邦学习系统，通过单向块间训练和轻量级辅助网络生成方法，显著减少设备计算和通信开销，同时提高模型准确性。


<details>
  <summary>Details</summary>
Motivation: 解决Split Federated Learning（SFL）中因频繁通信和非独立同分布（non-IID）数据导致的模型准确性下降和计算成本高的问题。

Method: 采用单向块间训练和轻量级辅助网络生成方法，减少梯度传输和中间交换，并通过整合设备块生成的激活来训练服务器块。

Result: Ampere在多个CNN和Transformer上表现优异，相比SFL基线系统，准确性提升13.26%，训练时间减少94.6%，通信开销减少99.1%，设备计算减少93.13%，非IID数据下的准确性标准差降低53.39%。

Conclusion: Ampere在减少计算和通信开销的同时，显著提高了模型准确性，尤其适用于非IID数据场景。

Abstract: A Federated Learning (FL) system collaboratively trains neural networks
across devices and a server but is limited by significant on-device computation
costs. Split Federated Learning (SFL) systems mitigate this by offloading a
block of layers of the network from the device to a server. However, in doing
so, it introduces large communication overheads due to frequent exchanges of
intermediate activations and gradients between devices and the server and
reduces model accuracy for non-IID data. We propose Ampere, a novel
collaborative training system that simultaneously minimizes on-device
computation and device-server communication while improving model accuracy.
Unlike SFL, which uses a global loss by iterative end-to-end training, Ampere
develops unidirectional inter-block training to sequentially train the device
and server block with a local loss, eliminating the transfer of gradients. A
lightweight auxiliary network generation method decouples training between the
device and server, reducing frequent intermediate exchanges to a single
transfer, which significantly reduces the communication overhead. Ampere
mitigates the impact of data heterogeneity by consolidating activations
generated by the trained device block to train the server block, in contrast to
SFL, which trains on device-specific, non-IID activations. Extensive
experiments on multiple CNNs and transformers show that, compared to
state-of-the-art SFL baseline systems, Ampere (i) improves model accuracy by up
to 13.26% while reducing training time by up to 94.6%, (ii) reduces
device-server communication overhead by up to 99.1% and on-device computation
by up to 93.13%, and (iii) reduces standard deviation of accuracy by 53.39% for
various non-IID degrees highlighting superior performance when faced with
heterogeneous data.

</details>


### [7] [Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure](https://arxiv.org/abs/2507.07223)
*Myoungsoo Jung*

Main category: cs.DC

TL;DR: 论文提出了一种基于CXL的模块化数据中心架构，以解决现代AI工作负载（如LLMs和RAG）在内存、通信带宽和资源灵活性方面的挑战。通过混合CXL-over-XLink设计和分层内存模型，提高了AI基础设施的可扩展性、吞吐量和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现代AI工作负载（如LLMs和RAG）对内存、通信带宽和资源灵活性提出了极高要求，传统GPU架构难以满足其扩展需求。

Method: 提出模块化数据中心架构，结合CXL和XLink（如UALink、NVLink）设计，引入分层内存模型，并评估轻量级CXL实现、HBM和硅光子技术。

Result: 评估结果显示，该架构显著提升了AI基础设施的可扩展性、吞吐量和灵活性。

Conclusion: 通过模块化设计和新型互连技术，能够有效解决大规模AI工作负载的扩展瓶颈。

Abstract: Modern AI workloads such as large language models (LLMs) and
retrieval-augmented generation (RAG) impose severe demands on memory,
communication bandwidth, and resource flexibility. Traditional GPU-centric
architectures struggle to scale due to growing inter-GPU communication
overheads. This report introduces key AI concepts and explains how Transformers
revolutionized data representation in LLMs. We analyze large-scale AI hardware
and data center designs, identifying scalability bottlenecks in hierarchical
systems. To address these, we propose a modular data center architecture based
on Compute Express Link (CXL) that enables disaggregated scaling of memory,
compute, and accelerators. We further explore accelerator-optimized
interconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink
Fusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance
data transfers while preserving memory coherence. We also propose a
hierarchical memory model that combines local and pooled memory, and evaluate
lightweight CXL implementations, HBM, and silicon photonics for efficient
scaling. Our evaluations demonstrate improved scalability, throughput, and
flexibility in AI infrastructure.

</details>


### [8] [M$^2$-MFP: A Multi-Scale and Multi-Level Memory Failure Prediction Framework for Reliable Cloud Infrastructure](https://arxiv.org/abs/2507.07144)
*Hongyi Xie,Min Zhou,Qiao Yu,Jialiang Yu,Zhenli Sheng,Hong Xie,Defu Lian*

Main category: cs.DC

TL;DR: M$^2$-MFP是一个多尺度分层内存故障预测框架，通过二进制空间特征提取器和双路径时间建模架构提升云基础设施的可靠性。


<details>
  <summary>Details</summary>
Motivation: 内存故障对系统稳定性构成威胁，现有预测方法存在泛化性差和性能不足的问题。

Method: 将可纠正错误转换为多级二进制矩阵表示，使用BSFE提取特征，并采用双路径时间建模架构。

Result: 在基准数据集和实际部署中，M$^2$-MFP显著优于现有方法。

Conclusion: M$^2$-MFP有效提升了内存故障预测的准确性和云基础设施的可靠性。

Abstract: As cloud services become increasingly integral to modern IT infrastructure,
ensuring hardware reliability is essential to sustain high-quality service.
Memory failures pose a significant threat to overall system stability, making
accurate failure prediction through the analysis of memory error logs (i.e.,
Correctable Errors) imperative. Existing memory failure prediction approaches
have notable limitations: rule-based expert models suffer from limited
generalizability and low recall rates, while automated feature extraction
methods exhibit suboptimal performance. To address these limitations, we
propose M$^2$-MFP: a Multi-scale and hierarchical memory failure prediction
framework designed to enhance the reliability and availability of cloud
infrastructure. M$^2$-MFP converts Correctable Errors (CEs) into multi-level
binary matrix representations and introduces a Binary Spatial Feature Extractor
(BSFE) to automatically extract high-order features at both DIMM-level and
bit-level. Building upon the BSFE outputs, we develop a dual-path temporal
modeling architecture: 1) a time-patch module that aggregates multi-level
features within observation windows, and 2) a time-point module that employs
interpretable rule-generation trees trained on bit-level patterns. Experiments
on both benchmark datasets and real-world deployment show the superiority of
M$^2$-MFP as it outperforms existing state-of-the-art methods by significant
margins. Code and data are available at this repository:
https://github.com/hwcloud-RAS/M2-MFP.

</details>


### [9] [Machine Learning-driven Multiscale MD Workflows: The Mini-MuMMI Experience](https://arxiv.org/abs/2507.07352)
*Loïc Pottier,Konstantia Georgouli,Timothy S. Carpenter,Fikret Aydin,Jeremy O. B. Tempkin,Dwight V. Nissley,Frederick H. Streitz,Thomas R. W. Scogland,Peer-Timo Bremer,Felice C. Lightstone,Helgi I. Ingólfsson*

Main category: cs.DC

TL;DR: 论文介绍了多尺度机器学习建模基础设施MuMMI及其轻量版mini-MuMMI，用于管理大规模并行多尺度模拟，特别是分子动力学模拟。


<details>
  <summary>Details</summary>
Motivation: 多尺度建模在复杂现象（如生物分子相互作用）中至关重要，但跨尺度建模和并行系统管理具有挑战性。

Method: 提出MuMMI和mini-MuMMI，后者适用于小型高性能计算系统或笔记本电脑，用于协调不同时间尺度的分子动力学模拟。

Result: 通过RAS-RAF膜相互作用验证了mini-MuMMI的实用性，并探讨了多尺度工作流通用化的挑战。

Conclusion: mini-MuMMI扩展了多尺度建模的应用范围，为更广泛的领域提供了可能性。

Abstract: Computational models have become one of the prevalent methods to model
complex phenomena. To accurately model complex interactions, such as detailed
biomolecular interactions, scientists often rely on multiscale models comprised
of several internal models operating at difference scales, ranging from
microscopic to macroscopic length and time scales. Bridging the gap between
different time and length scales has historically been challenging but the
advent of newer machine learning (ML) approaches has shown promise for tackling
that task. Multiscale models require massive amounts of computational power and
a powerful workflow management system. Orchestrating ML-driven multiscale
studies on parallel systems with thousands of nodes is challenging, the
workflow must schedule, allocate and control thousands of simulations operating
at different scales. Here, we discuss the massively parallel Multiscale
Machine-Learned Modeling Infrastructure (MuMMI), a multiscale workflow
management infrastructure, that can orchestrate thousands of molecular dynamics
(MD) simulations operating at different timescales, spanning from millisecond
to nanosecond. More specifically, we introduce a novel version of MuMMI called
"mini-MuMMI". Mini-MuMMI is a curated version of MuMMI designed to run on
modest HPC systems or even laptops whereas MuMMI requires larger HPC systems.
We demonstrate mini-MuMMI utility by exploring RAS-RAF membrane interactions
and discuss the different challenges behind the generalization of multiscale
workflows and how mini-MuMMI can be leveraged to target a broader range of
applications outside of MD and RAS-RAF interactions.

</details>


### [10] [KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows](https://arxiv.org/abs/2507.07400)
*Zaifeng Pan,Ajjkumar Patel,Zhengding Hu,Yipeng Shen,Yue Guan,Wan-Lu Li,Lianhui Qin,Yida Wang,Yufei Ding*

Main category: cs.DC

TL;DR: KVFlow是一个针对基于LLM的代理工作流程优化的KV缓存管理框架，通过预测代理的未来使用情况，减少缓存未命中和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM系统使用LRU策略管理KV缓存，无法预测代理的未来使用，导致频繁缓存未命中和高计算开销。

Method: KVFlow通过Agent Step Graph抽象代理执行计划，并基于步骤到执行的值进行细粒度缓存管理，同时引入重叠KV预取机制。

Result: KVFlow在单工作流程和多并发工作流程中分别实现了1.83倍和2.19倍的加速。

Conclusion: KVFlow显著提升了基于LLM的代理工作流程的效率，减少了缓存未命中和计算开销。

Abstract: Large language model (LLM) based agentic workflows have become a popular
paradigm for coordinating multiple specialized agents to solve complex tasks.
To improve serving efficiency, existing LLM systems employ prefix caching to
reuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby
avoiding redundant computation across repeated invocations. However, current
systems typically evict KV caches using a Least Recently Used (LRU) policy,
which fails to anticipate future agent usage and often discards KV caches
shortly before their reuse. This leads to frequent cache misses and substantial
recomputation or swapping overhead. We present KVFlow, a workflow-aware KV
cache management framework tailored for agentic workloads. KVFlow abstracts the
agent execution schedule as an Agent Step Graph and assigns each agent a
steps-to-execution value that estimates its temporal proximity to future
activation. These values guide a fine-grained eviction policy at the KV node
level, allowing KVFlow to preserve entries likely to be reused and efficiently
manage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a
fully overlapped KV prefetching mechanism, which proactively loads required
tensors from CPU to GPU in background threads for agents scheduled in the next
step, thereby avoiding cache miss stalls during generation. Compared to SGLang
with hierarchical radix cache, KVFlow achieves up to 1.83$\times$ speedup for
single workflows with large prompts, and up to 2.19$\times$ speedup for
scenarios with many concurrent workflows.

</details>


### [11] [Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems](https://arxiv.org/abs/2507.07671)
*Jovan Prodanov,Blaž Bertalanič,Carolina Fortuna,Shih-Kai Chou,Matjaž Branko Jurič,Ramon Sanchez-Iborra,Jernej Hribar*

Main category: cs.DC

TL;DR: 论文提出了一种基于多智能体强化学习的动态资源扩展引擎（MARLISE），用于优化边缘-云系统中的资源扩展问题，相比传统静态阈值方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现代边缘-云系统在处理动态和不可预测的工作负载时面临资源扩展效率低下的问题，传统静态阈值方法无法满足需求。

Method: 使用深度强化学习算法（DQN和PPO）开发了MARLISE引擎，实现动态、反应式的资源扩展控制。

Result: 实验表明，MARLISE在动态工作负载下能确保微服务的低响应时间，并显著提高资源效率。

Conclusion: MARLISE在资源弹性和性能优化方面优于传统启发式方法，适用于动态分布式环境。

Abstract: Modern edge-cloud systems face challenges in efficiently scaling resources to
handle dynamic and unpredictable workloads. Traditional scaling approaches
typically rely on static thresholds and predefined rules, which are often
inadequate for optimizing resource utilization and maintaining performance in
distributed and dynamic environments. This inefficiency hinders the
adaptability and performance required in edge-cloud infrastructures, which can
only be achieved through the newly proposed in-place scaling. To address this
problem, we propose the Multi-Agent Reinforcement Learning-based In-place
Scaling Engine (MARLISE) that enables seamless, dynamic, reactive control with
in-place resource scaling. We develop our solution using two Deep Reinforcement
Learning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization
(PPO). We analyze each version of the proposed MARLISE solution using dynamic
workloads, demonstrating their ability to ensure low response times of
microservices and scalability. Our results show that MARLISE-based approaches
outperform heuristic method in managing resource elasticity while maintaining
microservice response times and achieving higher resource efficiency.

</details>


### [12] [KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based Auto-Scaling](https://arxiv.org/abs/2507.07932)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Qiang Guan,Hailong Jiang*

Main category: cs.DC

TL;DR: KIS-S框架结合GPU感知的Kubernetes推理模拟器（KISim）和基于PPO的自动扩展器（KIScaler），显著提升动态流量下的GPU推理扩展性能。


<details>
  <summary>Details</summary>
Motivation: 解决Kubernetes中GPU推理工作负载的自动扩展问题，特别是针对动态和突发流量模式以及缺乏GPU级指标集成的挑战。

Method: 提出KIS-S框架，结合KISim（模拟器）和KIScaler（基于PPO的自动扩展器），在模拟中学习延迟感知和资源高效的扩展策略。

Result: 实验表明，KIScaler平均奖励提升75.2%，P95延迟降低6.7倍，且无需重新训练即可泛化。

Conclusion: KIS-S填补了反应式自动扩展与智能编排之间的差距，适用于可扩展的GPU加速环境。

Abstract: Autoscaling GPU inference workloads in Kubernetes remains challenging due to
the reactive and threshold-based nature of default mechanisms such as the
Horizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty
traffic patterns and lack integration with GPU-level metrics. We present KIS-S,
a unified framework that combines KISim, a GPU-aware Kubernetes Inference
Simulator, with KIScaler, a Proximal Policy Optimization (PPO)-based
autoscaler. KIScaler learns latency-aware and resource-efficient scaling
policies entirely in simulation, and is directly deployed without retraining.
Experiments across four traffic patterns show that KIScaler improves average
reward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and
generalizes without retraining. Our work bridges the gap between reactive
autoscaling and intelligent orchestration for scalable GPU-accelerated
environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [Accelerating Transposed Convolutions on FPGA-based Edge Devices](https://arxiv.org/abs/2507.07683)
*Jude Haris,José Cano*

Main category: cs.AR

TL;DR: 论文提出MM2IM加速器，通过结合矩阵乘法和col2IM优化转置卷积（TCONV）在边缘设备上的性能，显著提升速度和能效。


<details>
  <summary>Details</summary>
Motivation: 当前TCONV的主流方法IOM存在输出映射复杂、计算重叠和无效计算等问题，导致在资源受限的边缘设备上性能瓶颈加剧。

Method: 提出硬件-软件协同设计的MM2IM加速器，结合矩阵乘法和col2IM处理TCONV层，并通过SECDA-TFLite工具包实现和评估。

Result: 在261种TCONV配置中平均提速1.9倍，在知名生成模型上最高提速4.2倍，能效提升显著。

Conclusion: MM2IM在边缘设备上高效优化TCONV，显著提升生成模型的性能和能效。

Abstract: Transposed Convolutions (TCONV) enable the up-scaling mechanism within
generative Artificial Intelligence (AI) models. However, the predominant
Input-Oriented Mapping (IOM) method for implementing TCONV has complex output
mapping, overlapping sums, and ineffectual computations. These inefficiencies
further exacerbate the performance bottleneck of TCONV and generative models on
resource-constrained edge devices. To address this problem, in this paper we
propose MM2IM, a hardware-software co-designed accelerator that combines Matrix
Multiplication (MatMul) with col2IM to process TCONV layers on
resource-constrained edge devices efficiently. Using the SECDA-TFLite design
toolkit, we implement MM2IM and evaluate its performance across 261 TCONV
problem configurations, achieving an average speedup of 1.9x against a
dual-thread ARM Neon optimized CPU baseline. We then evaluate the performance
of MM2IM on a range of TCONV layers from well-known generative models achieving
up to 4.2x speedup, and compare it against similar resource-constrained TCONV
accelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate
MM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x
energy reduction against the CPU baseline.

</details>
