<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications](https://arxiv.org/abs/2509.20426)
*Mahmoud Samir Fayed*

Main category: cs.PL

TL;DR: 开发了PWCT2，一个双语言（阿拉伯语/英语）、通用、自托管的可视化编程语言，使用Ring文本编程语言构建，相比PWCT提供36倍更快的代码生成和20倍更少的存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有的通用可视化编程语言（如PWCT）使用文本编程语言开发，改进需要文本编程，限制了可视化编程的自举能力。

Method: 首先设计Ring文本编程语言，然后使用PWCT开发Ring编译器，最后用Ring开发PWCT2可视化编程语言，实现自托管。

Result: PWCT2包含92,000行Ring代码和394个可视化组件，在Steam平台有1772用户启动，总使用时间超过17,000小时，获得积极反馈。

Conclusion: 成功实现了自托管可视化编程语言PWCT2，证明了可视化编程语言可以自我开发，为未来研究和发展提供了基础。

Abstract: Most visual programming languages (VPLs) are domain-specific, with few
general-purpose VPLs like Programming Without Coding Technology (PWCT). These
general-purpose VPLs are developed using textual programming languages and
improving them requires textual programming. In this thesis, we designed and
developed PWCT2, a dual-language (Arabic/English), general-purpose,
self-hosting visual programming language. Before doing so, we specifically
designed a textual programming language called Ring for its development. Ring
is a dynamically typed language with a lightweight implementation, offering
syntax customization features. It permits the creation of domain-specific
languages through new features that extend object-oriented programming,
allowing for specialized languages resembling Cascading Style Sheets (CSS) or
Supernova language. The Ring Compiler and Virtual Machine are designed using
the PWCT visual programming language where the visual implementation is
composed of 18,945 components that generate 24,743 lines of C code, which
increases the abstraction level and hides unnecessary details. Using PWCT to
develop Ring allowed us to realize several issues in PWCT, which led to the
development of the PWCT2 visual programming language using the Ring textual
programming language. PWCT2 provides approximately 36 times faster code
generation and requires 20 times less storage for visual source files. It also
allows for the conversion of Ring code into visual code, enabling the creation
of a self-hosting VPL that can be developed using itself. PWCT2 consists of
approximately 92,000 lines of Ring code and comes with 394 visual components.
PWCT2 is distributed to many users through the Steam platform and has received
positive feedback, On Steam, 1772 users have launched the software, and the
total recorded usage time exceeds 17,000 hours, encouraging further research
and development.

</details>


### [2] [Efficient Symbolic Computation vis Hash Consing](https://arxiv.org/abs/2509.20534)
*Bowen Zhu,Aayush Sabharwal,Songchen Tan,Yingbo Ma,Alan Edelman,Christopher Rackauckas*

Main category: cs.PL

TL;DR: 在JuliaSymbolics中集成哈希合并技术，通过全局弱引用哈希表消除重复表达式存储，显著提升符号计算性能，内存使用减少2倍，计算速度提升3.2倍，代码生成加速5倍。


<details>
  <summary>Details</summary>
Motivation: 符号计算系统存在内存效率低下的问题，由于结构相同子表达式的冗余存储导致表达式膨胀，这影响了经典计算机代数和新兴AI驱动数学推理工具的性能。

Method: 将哈希合并技术首次集成到JuliaSymbolics中，使用全局弱引用哈希表对表达式进行规范化并消除重复，同时与Julia的元编程和即时编译基础设施无缝集成。

Result: 基准测试显示显著改进：符号计算加速3.2倍，内存使用减少2倍，代码生成加速5倍，函数编译加速10倍，大型模型的数值评估加速100倍。

Conclusion: 哈希合并对于扩展符号计算至关重要，为未来将哈希合并与e-graphs集成以在AI驱动管道中实现增强的等价感知表达式共享铺平了道路。

Abstract: Symbolic computation systems suffer from memory inefficiencies due to
redundant storage of structurally identical subexpressions, commonly known as
expression swell, which degrades performance in both classical computer algebra
and emerging AI-driven mathematical reasoning tools. In this paper, we present
the first integration of hash consing into JuliaSymbolics, a high-performance
symbolic toolkit in Julia, by employing a global weak-reference hash table that
canonicalizes expressions and eliminates duplication. This approach reduces
memory consumption and accelerates key operations such as differentiation,
simplification, and code generation, while seamlessly integrating with Julia's
metaprogramming and just-in-time compilation infrastructure. Benchmark
evaluations across different computational domains reveal substantial
improvements: symbolic computations are accelerated by up to 3.2 times, memory
usage is reduced by up to 2 times, code generation is up to 5 times faster,
function compilation up to 10 times faster, and numerical evaluation up to 100
times faster for larger models. While certain workloads with fewer duplicate
unknown-variable expressions show more modest gains or even slight overhead in
initial computation stages, downstream processing consistently benefits
significantly. These findings underscore the importance of hash consing in
scaling symbolic computation and pave the way for future work integrating hash
consing with e-graphs for enhanced equivalence-aware expression sharing in
AI-driven pipelines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [FZModules: A Heterogeneous Computing Framework for Customizable Scientific Data Compression Pipelines](https://arxiv.org/abs/2509.20563)
*Skyler Ruiter,Jiannan Tian,Fengguang Song*

Main category: cs.DC

TL;DR: FZModules是一个异构框架，用于通过高性能模块组装误差有界的自定义压缩流水线，支持快速实验和异步执行，在保持与融合内核GPU压缩器相当的端到端加速的同时，实现与高保真CPU或混合压缩器相似的率失真性能。


<details>
  <summary>Details</summary>
Motivation: 现代科学模拟和仪器生成的数据量超过内存和存储容量，限制了可扩展性。虽然有损压缩通过控制误差来减少存储占用和提高吞吐量，但最优流水线高度依赖于数据和目标，需要压缩专业知识。GPU压缩器提供原始吞吐量，但通常硬编码融合内核，阻碍快速实验，且在率失真方面表现不佳。

Method: 提出FZModules异构框架，通过简洁可扩展的接口从高性能模块组装误差有界的自定义压缩流水线。利用异步任务支持的执行库推断数据依赖、管理内存移动，并暴露分支和阶段级并发，实现强大的异步压缩流水线。

Result: 在四个代表性科学数据集上评估三个使用FZModules构建的流水线，显示它们能够实现与融合内核GPU压缩器相当的端到端加速，同时达到与高保真CPU或混合压缩器相似的率失真性能。

Conclusion: FZModules框架支持快速、领域定制的压缩流水线设计，在保持高性能的同时提供灵活性。

Abstract: Modern scientific simulations and instruments generate data volumes that
overwhelm memory and storage, throttling scalability. Lossy compression
mitigates this by trading controlled error for reduced footprint and throughput
gains, yet optimal pipelines are highly data and objective specific, demanding
compression expertise. GPU compressors supply raw throughput but often
hard-code fused kernels that hinder rapid experimentation, and underperform in
rate-distortion. We present FZModules, a heterogeneous framework for assembling
error-bounded custom compression pipelines from high-performance modules
through a concise extensible interface. We further utilize an asynchronous
task-backed execution library that infers data dependencies, manages memory
movement, and exposes branch and stage level concurrency for powerful
asynchronous compression pipelines. Evaluating three pipelines built with
FZModules on four representative scientific datasets, we show they can compare
end-to-end speedup of fused-kernel GPU compressors while achieving similar
rate-distortion to higher fidelity CPU or hybrid compressors, enabling rapid,
domain-tailored design.

</details>


### [4] [Experience Deploying Containerized GenAI Services at an HPC Center](https://arxiv.org/abs/2509.20603)
*Angel M. Beltre,Jeff Ogden,Kevin Pedretti*

Main category: cs.DC

TL;DR: 本文分享了在高性能计算中心部署生成式AI工作负载的经验，讨论了HPC与云计算环境的集成，提出了融合计算架构，并通过Llama大语言模型的案例研究展示了跨平台部署。


<details>
  <summary>Details</summary>
Motivation: 生成式AI应用通常由容器化组件构成并在云环境中部署，但这些能力在高性能计算中心仍在发展中，需要探索HPC与云计算的集成方案。

Method: 设计了融合计算架构，集成HPC和Kubernetes平台运行容器化GenAI工作负载，使用多种容器运行时在Kubernetes和HPC平台上部署vLLM推理服务器运行Llama大语言模型。

Result: 成功实现了跨平台的容器化GenAI工作负载部署，提高了可重复性，为HPC容器社区提供了实践经验和指导。

Conclusion: HPC中心部署GenAI工作负载具有实际可行性，融合架构为未来研究和工具开发提供了方向，促进了HPC与云计算的协同发展。

Abstract: Generative Artificial Intelligence (GenAI) applications are built from
specialized components -- inference servers, object storage, vector and graph
databases, and user interfaces -- interconnected via web-based APIs. While
these components are often containerized and deployed in cloud environments,
such capabilities are still emerging at High-Performance Computing (HPC)
centers. In this paper, we share our experience deploying GenAI workloads
within an established HPC center, discussing the integration of HPC and cloud
computing environments. We describe our converged computing architecture that
integrates HPC and Kubernetes platforms running containerized GenAI workloads,
helping with reproducibility. A case study illustrates the deployment of the
Llama Large Language Model (LLM) using a containerized inference server (vLLM)
across both Kubernetes and HPC platforms using multiple container runtimes. Our
experience highlights practical considerations and opportunities for the HPC
container community, guiding future research and tool development.

</details>


### [5] [Distributed-memory Algorithms for Sparse Matrix Permutation, Extraction, and Assignment](https://arxiv.org/abs/2509.20776)
*Elaheh Hassani,Md Taufique Hussain,Ariful Azad*

Main category: cs.DC

TL;DR: 提出了可扩展的分布式内存算法用于稀疏矩阵的置换、提取和赋值操作，采用Identify-Exchange-Build策略减少通信开销，在多个测试平台上性能优于现有库。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式稀疏矩阵操作（如基于SpGEMM的方法）存在通信开销大的问题，需要更高效的算法来提升性能。

Method: 采用Identify-Exchange-Build(IEB)策略：识别本地非零元素、交换所需数据、从接收元素构建本地子矩阵；使用无同步多线程算法加速本地计算。

Result: 在多个集群和超级计算机上的实验表明，该算法在矩阵置换、子图提取、流图应用等场景中性能显著优于CombBLAS和PETSc。

Conclusion: 该工作为稀疏矩阵置换、提取和赋值操作提供了全面的算法研究、软件实现和实验评估，展示了优越的性能表现。

Abstract: We present scalable distributed-memory algorithms for sparse matrix
permutation, extraction, and assignment. Our methods follow an
Identify-Exchange-Build (IEB) strategy where each process identifies the local
nonzeros to be sent, exchanges the required data, and then builds its local
submatrix from the received elements. This approach reduces communication
compared to SpGEMM-based methods in distributed memory. By employing
synchronization-free multithreaded algorithms, we further accelerate local
computations, achieving substantially better performance than existing
libraries such as CombBLAS and PETSc. We design efficient software for these
operations and evaluate their performance on two university clusters and the
Perlmutter supercomputer. Our experiments span a variety of application
scenarios, including matrix permutation for load balancing, matrix reordering,
subgraph extraction, and streaming graph applications. In all cases, we compare
our algorithms against CombBLAS, the most comprehensive distributed library for
these operations, and, in some scenarios, against PETSc. Overall, this work
provides a comprehensive study of algorithms, software implementations,
experimental evaluations, and applications for sparse matrix permutation,
extraction, and assignment.

</details>


### [6] [Integrating and Characterizing HPC Task Runtime Systems for hybrid AI-HPC workloads](https://arxiv.org/abs/2509.20819)
*Andre Merzky,Mikhail Titov,Matteo Turilli,Shantenu Jha*

Main category: cs.DC

TL;DR: RADICAL-Pilot与Flux和Dragon运行时系统集成，为混合AI-HPC工作负载提供高性能解决方案，相比传统Slurm的srun显著提升任务执行速率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 科学工作流日益结合HPC和机器学习任务，但传统启动器如Slurm的srun在并发性和吞吐量方面存在限制，无法适应动态异构工作负载。

Method: 将RADICAL-Pilot与Flux和Dragon两个互补的运行时系统集成，实现分层资源管理和高吞吐量函数执行，在Frontier系统上使用合成和生产级工作负载进行性能研究。

Result: RP+Flux维持930任务/秒，RP+Flux+Dragon超过1,500任务/秒，利用率超过99.6%；而srun峰值仅152任务/秒，利用率低于50%。在IMPECCABLE.v2药物发现应用中，RP+Flux相比srun/Slurm缩短完成时间30-60%，吞吐量提升超过4倍。

Conclusion: RP与运行时系统的混合集成为混合AI-HPC工作负载提供了可扩展的方法。

Abstract: Scientific workflows increasingly involve both HPC and machine-learning
tasks, combining MPI-based simulations, training, and inference in a single
execution. Launchers such as Slurm's srun constrain concurrency and throughput,
making them unsuitable for dynamic and heterogeneous workloads. We present a
performance study of RADICAL-Pilot (RP) integrated with Flux and Dragon, two
complementary runtime systems that enable hierarchical resource management and
high-throughput function execution. Using synthetic and production-scale
workloads on Frontier, we characterize the task execution properties of RP
across runtime configurations. RP+Flux sustains up to 930 tasks/s, and
RP+Flux+Dragon exceeds 1,500 tasks/s with over 99.6% utilization. In contrast,
srun peaks at 152 tasks/s and degrades with scale, with utilization below 50%.
For IMPECCABLE.v2 drug discovery campaign, RP+Flux reduces makespan by 30-60%
relative to srun/Slurm and increases throughput more than four times on up to
1,024. These results demonstrate hybrid runtime integration in RP as a scalable
approach for hybrid AI-HPC workloads.

</details>


### [7] [RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training](https://arxiv.org/abs/2509.21009)
*Wei Gao,Yuheng Zhao,Dakai An,Tianyuan Wu,Lunxi Cao,Shaopan Xiong,Ju Huang,Weixun Wang,Siran Yang,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: 提出了tail batching策略和RollPacker系统，通过将长尾响应集中调度到少数长轮次中，显著减少RL训练中的GPU空闲时间，实现2.03x-2.56x的端到端训练加速。


<details>
  <summary>Details</summary>
Motivation: 同步RL后训练存在GPU利用率低的问题，主要由rollout步骤中响应长度不平衡导致的'bubbles'引起。现有方法通过放宽同步性来缓解，但会牺牲训练准确性。

Method: 提出tail batching策略：将导致长尾响应的提示词集中到少数长轮次中，确保大多数轮次只涉及平衡的短rollout。同时开发RollPacker系统，在rollout阶段采用弹性并行适配，奖励阶段动态资源分配和调度，以及基于流的训练。

Result: 在128个H800 GPU上对Qwen2.5系列LLMs的测试显示，相比veRL实现2.03x-2.56x端到端训练时间减少，相比RLHFuse实现最高2.24x加速。

Conclusion: tail batching策略和RollPacker系统能够在不牺牲准确性的前提下，显著加速RL训练，有效解决了同步RL中的GPU利用率问题。

Abstract: Reinforcement Learning (RL) is a pivotal post-training technique for
enhancing the reasoning capabilities of Large Language Models (LLMs). However,
synchronous RL post-training often suffers from significant GPU
underutilization, referred to as bubbles, caused by imbalanced response lengths
within rollout steps. Many RL systems attempt to alleviate this problem by
relaxing synchronization, but this can compromise training accuracy. In this
paper, we introduce tail batching, a novel rollout scheduling strategy for
synchronous RL that systematically consolidates prompts leading to long-tail
responses into a small subset of rollout steps (long rounds), while ensuring
that the majority of steps (short rounds) involve only balanced, short
rollouts. By excluding long responses from short rounds and rescheduling them
into a few designated long rounds, tail batching effectively reduces GPU idle
time during rollouts and significantly accelerates RL training without
sacrificing accuracy. We present RollPacker, a system that fully harnesses the
benefits of tail batching through holistic optimizations across all three RL
stages: elastic parallelism adaptation for rollout, dynamic resource allocation
and scheduling for reward, and stream-based training. Empirical results show
that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction
compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5
family of LLMs on up to 128 H800 GPUs.

</details>


### [8] [Utilizing Sparsity in the GPU-accelerated Assembly of Schur Complement Matrices in Domain Decomposition Methods](https://arxiv.org/abs/2509.21037)
*Jakub Homola,Ondřej Meca,Lubomír Říha,Tomáš Brzobohatý*

Main category: cs.DC

TL;DR: 本文提出了一种通过智能利用输入矩阵稀疏性来改进GPU上Schur补矩阵组装的方法，在FETI方法中实现了5.1倍的GPU部分加速和3.3倍的整体组装加速。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算集群性能主要依赖于GPU，需要加速域分解方法中的Schur补矩阵计算。显式组装密集Schur补矩阵成本高昂，成为GPU加速的主要开销。

Method: 通过利用输入矩阵的稀疏性来改进GPU上的Schur补矩阵组装过程，减少显式组装的开销。

Result: 在FETI方法中，GPU部分代码实现了5.1倍加速，整个组装过程实现了3.3倍加速，使得从仅10次迭代开始就能获得加速效益。

Conclusion: 通过智能利用输入矩阵的稀疏性，可以显著改进GPU上的Schur补矩阵组装性能，使GPU加速在较少的迭代次数下即具有效益。

Abstract: Schur complement matrices emerge in many domain decomposition methods that
can solve complex engineering problems using supercomputers. Today, as most of
the high-performance clusters' performance lies in GPUs, these methods should
also be accelerated.
  Typically, the offloaded components are the explicitly assembled dense Schur
complement matrices used later in the iterative solver for multiplication with
a vector. As the explicit assembly is expensive, it represents a significant
overhead associated with this approach to acceleration. It has already been
shown that the overhead can be minimized by assembling the Schur complements
directly on the GPU.
  This paper shows that the GPU assembly can be further improved by wisely
utilizing the sparsity of the input matrices. In the context of FETI methods,
we achieved a speedup of 5.1 in the GPU section of the code and 3.3 for the
whole assembly, making the acceleration beneficial from as few as 10
iterations.

</details>


### [9] [Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem](https://arxiv.org/abs/2509.21039)
*William F. Godoy,Tatiana Melnichenko,Pedro Valero-Lara,Wael Elwasif,Philip Fackler,Rafael Ferreira Da Silva,Keita Teranishi,Jeffrey S. Vetter*

Main category: cs.DC

TL;DR: Mojo语言在GPU科学计算中的性能与可移植性评估，与CUDA和HIP相比在内存密集型任务上表现相当，但在原子操作和快速数学计算密集型任务上存在差距。


<details>
  <summary>Details</summary>
Motivation: 探索基于MLIR的新语言Mojo在科学计算工作负载中的性能和可移植性，旨在弥合Python生态系统在科学计算和AI融合中的性能与生产力差距。

Method: 针对四种科学计算工作负载（七点模板、BabelStream、miniBUDE、Hartree-Fock），在NVIDIA H100和AMD MI300A GPU上比较Mojo与供应商基线（CUDA/HIP）的性能。

Result: Mojo在内存密集型内核上的性能与CUDA和HIP相当，但在AMD GPU上的原子操作以及AMD和NVIDIA GPU上的快速数学计算密集型内核存在性能差距。

Conclusion: 尽管学习曲线和编程要求仍较低级，但Mojo可以在科学计算和AI融合的碎片化Python生态系统中弥合显著差距。

Abstract: We explore the performance and portability of the novel Mojo language for
scientific computing workloads on GPUs. As the first language based on the
LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure,
Mojo aims to close performance and productivity gaps by combining Python's
interoperability and CUDA-like syntax for compile-time portable GPU
programming. We target four scientific workloads: a seven-point stencil
(memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and
Hartree-Fock (compute-bound with atomic operations); and compare their
performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We
show that Mojo's performance is competitive with CUDA and HIP for memory-bound
kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math
compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve
and programming requirements are still fairly low-level, Mojo can close
significant gaps in the fragmented Python ecosystem in the convergence of
scientific computing and AI.

</details>


### [10] [From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient Method for Solving Large-Scale Linear Optimization Problem](https://arxiv.org/abs/2509.21137)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Gozde Tutuncuoglu,Junchi Yang,Feng Qiu,Murat Yildirim*

Main category: cs.DC

TL;DR: 提出了一种针对RRAM阵列的分布式内存原始-对偶混合梯度方法，解决了传统算法在内存计算中的矩阵重编程成本问题，实现了比GPU加速求解器高达三个数量级的能耗和延迟降低。


<details>
  <summary>Details</summary>
Motivation: 传统架构受限于基本物理极限，无法满足计算工作负载的指数增长需求。内存计算虽然提供了低延迟和低能耗的模拟计算能力，但现有算法特别是约束优化问题中的频繁矩阵重编程在内存计算中成本过高。

Method: 开发了分布式内存原始-对偶混合梯度方法，专门为RRAM设备阵列协同设计。该方法最小化昂贵的写入周期，包含对设备非理想性的鲁棒性，并利用对称块矩阵公式在分布式交叉阵列中统一操作。

Result: 与GPU加速求解器在大规模线性程序上的基准测试表明，基于RRAM的求解器实现了相当的精度，同时能耗和延迟降低了高达三个数量级。

Conclusion: 这是首个在RRAM上实现的PDHG线性规划求解器，展示了算法-硬件协同设计通过分布式内存计算解决大规模优化问题的变革潜力。

Abstract: The exponential growth of computational workloads is surpassing the
capabilities of conventional architectures, which are constrained by
fundamental limits. In-memory computing (IMC) with RRAM provides a promising
alternative by providing analog computations with significant gains in latency
and energy use. However, existing algorithms developed for conventional
architectures do not translate to IMC, particularly for constrained
optimization problems where frequent matrix reprogramming remains
cost-prohibitive for IMC applications. Here we present a distributed in-memory
primal-dual hybrid gradient (PDHG) method, specifically co-designed for arrays
of RRAM devices. Our approach minimizes costly write cycles, incorporates
robustness against device non-idealities, and leverages a symmetric
block-matrix formulation to unify operations across distributed crossbars. We
integrate a physics-based simulation framework called MELISO+ to evaluate
performance under realistic device conditions. Benchmarking against
GPU-accelerated solvers on large-scale linear programs demonstrates that our
RRAM-based solver achieves comparable accuracy with up to three orders of
magnitude reductions in energy consumption and latency. These results
demonstrate the first PDHG-based LP solver implemented on RRAMs, showcasing the
transformative potential of algorithm-hardware co-design for solving
large-scale optimization through distributed in-memory computing.

</details>


### [11] [Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training](https://arxiv.org/abs/2509.21275)
*Shiju Wang,Yujie Wang,Ao Sun,Fangcheng Fu,Zijian Zhu,Bin Cui,Xu Han,Kaisheng Ma*

Main category: cs.DC

TL;DR: 提出了弹性流水线并行(EPP)方法，通过协调token级和batch级流水线并行来适应资源和负载异质性，并构建了InfiniPipe系统实现1.69倍加速


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方法在长上下文训练中存在内存消耗与硬件利用率之间的权衡，且真实数据集序列长度分布不均导致负载不平衡问题

Method: 提出EPP方法，结合资源感知和负载均衡的序列处理器，以及联合优化流水线调度和梯度检查点的协同优化方法

Result: InfiniPipe系统相比最先进系统实现了1.69倍的加速

Conclusion: 弹性流水线并行能够有效解决长上下文训练中的内存和负载平衡问题，显著提升训练效率

Abstract: Long context training is crucial for LLM's context extension. Existing
schemes, such as sequence parallelism, incur substantial communication
overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness
hinges on partitioning granularity. Batch-level PP dividing input samples
exhibits high memory consumption in long-context scenario, whereas token-level
PP splitting sequences into slices alleviates memory overhead but may incur
hardware under-utilization. This trade-off motivates adaptively selecting PP
granularity to match resource and workload characteristics. Moreover, sequence
length distribution of the real-world dataset exhibits skewness, posing a
challenge on PP's workload balance and efficient scheduling. Current static PP
scheduling methods overlook the variance of sequence length, leading to
suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism
(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource
and workload heterogeneity. We build InfiniPipe, a distributed training system
that unleashes the potential of EPP via (1) a resource-aware and
workload-balanced sequence processor that splits long sequences and packs short
ones; and (2) a co-optimization methodology that jointly optimizes pipeline
schedule and gradient checkpointing via a mechanism named stage-aware
chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that
InfiniPipe achieves a 1.69x speedup over state-of-the-art systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [Pedagogically Motivated and Composable Open-Source RISC-V Processors for Computer Science Education](https://arxiv.org/abs/2509.20514)
*Ian McDougall,Harish Batchu,Michael Davies,Karthikeyan Sankaralingam*

Main category: cs.AR

TL;DR: 本文提出了一套评估RISC-V实现生态系统的标准，分析了现有开源实现，并开发了一个满足所有标准的可组合教学框架。


<details>
  <summary>Details</summary>
Motivation: RISC-V作为免费开源的指令集架构，为教学和业余使用提供了机会，但需要易于使用且健壮的开源实现。

Method: 1. 提出评估RISC-V实现生态系统的教学标准；2. 分析现有开源实现；3. 开发满足所有标准的可组合开源框架。

Result: 开发了一个全面的解决方案，所有组件都可以根据课程需求进行解构，并收集了有限的学⽣反馈。

Conclusion: 成功创建了一个满足教学需求的RISC-V实现框架，为其他教师提供了开源可用的教学工具。

Abstract: While most instruction set architectures (ISAs) are only available to use
through the purchase of a restrictive commercial license, the RISC-V ISA
presents a free and open-source alternative. Due to this availability, many
free and open-source implementations have been developed and can be accessed on
platforms such as GitHub. If an open source, easy-to-use, and robust RISC-V
implementation could be obtained, it could be easily adapted for pedagogical
and amateur use. In this work we accomplish three goals in relation to this
outlook. First, we propose a set of criteria for evaluating the components of a
RISC-V implementation's ecosystem from a pedagogical perspective. Second, we
analyze a number of existing open-source RISC-V implementations to determine
how many of the criteria they fulfill. We then develop a comprehensive solution
that meets all of these criterion and is released open-source for other
instructors to use. The framework is developed in a composable way that it's
different components can be disaggregated per individual course needs. Finally,
we also report on a limited study of student feedback.

</details>


### [13] [ZynqParrot: A Scale-Down Approach to Cycle-Accurate, FPGA-Accelerated Co-Emulation](https://arxiv.org/abs/2509.20543)
*Daniel Ruelas-Petrisko,Farzam Gilani,Anoop Mysore Nataraja,Zoe Taylor,Michael Taylor*

Main category: cs.AR

TL;DR: 提出了一种名为Scale-Down的建模和验证方法，通过将复杂系统分解为可管理的子组件进行独立原型设计，实现了FPGA加速的速度优势，同时避免了Scale-Out的不准确性和Scale-Up的高成本。


<details>
  <summary>Details</summary>
Motivation: 随着处理器复杂度的增加，功能验证和性能验证的成本急剧上升。传统的性能计数器推断和微架构模拟方法存在运行时间过长的问题，无法适用于长时间工作负载。

Method: 开发了ZynqParrot平台，采用Scale-Down方法将系统分解为子组件进行独立原型设计，通过精心设计原型接口确保被测设备的严格非干扰性，实现任意RTL设计的周期精确协同仿真。

Result: ZynqParrot能够以任意粒度验证功能和性能，并通过案例研究分析了开源RISC-V处理器的全栈性能。

Conclusion: Scale-Down方法比Scale-Up和Scale-Out方法更准确、更快、更经济，为架构师提供了FPGA加速的速度优势，同时消除了其他方法的不准确性和高成本问题。

Abstract: As processors increase in complexity, costs grow even more rapidly, both for
functional verification and performance validation. Most often, silicon
characterizations comprise simple performance counters, which are aggregated
and separated to tell a story. Based on these inferences, performance engineers
employ microarchitectural simulation to inspect deeply into the core.
Unfortunately, dramatically longer runtimes make simulation infeasible for long
workloads.
  We propose a Scale-Down approach to modelling and validation. Rather than
up-sizing a prototyping platform to fit large and complex system designs, we
show that it can be more accurate, faster, and more economical to decompose a
system into manageable sub-components that can be prototyped independently. By
carefully designing the prototyping interface, it is possible to adhere to
strict non-interference of the Device Under Test (DUT). This allows architects
to have the best of both worlds: the speed of FPGA acceleration while
eliminating the inaccuracies of Scale-Out and the inherent costs of Scale-Up.
  In this work, we present ZynqParrot: a Scale-Down FPGA-based modelling
platform, capable of executing non-interfering, cycle-accurate co-emulations of
arbitrary RTL designs. ZynqParrot is capable of verifying functionality and
performance with arbitrary granularity. We also provide case studies using
ZynqParrot to analyze the full-stack performance of an open-source RISC-V
processor.

</details>
