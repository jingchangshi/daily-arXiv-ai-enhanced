<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [From Dynamic to Lexical: A Comparative Exploration of Scoping Rules in SAS and R](https://arxiv.org/abs/2601.09808)
*Chen Ling,Yachen Wang*

Main category: cs.PL

TL;DR: 该论文比较了SAS的动态作用域和R的词法作用域机制，分析了两种语言中变量作用域规则的差异及其对代码行为的影响，并提供了调试和优化策略。


<details>
  <summary>Details</summary>
Motivation: 变量作用域在编程语言中对代码效率和结构组织至关重要。SAS和R作为广泛使用的统计编程语言，采用不同的作用域规则（SAS动态作用域 vs R词法作用域），理解这些差异对于编写高效、可靠的代码非常重要。

Method: 通过对比分析SAS的动态作用域（使用符号表，在运行时动态搜索活动宏层解析变量）和R的词法作用域（使用环境，基于函数定义结构解析变量）。提供具体示例展示不同作用域策略的影响，并介绍检查SAS符号表和R环境的方法，以及控制变量作用域的策略。

Result: 论文展示了SAS动态作用域和R词法作用域在变量解析机制上的根本差异，这些差异直接影响代码行为。提供了实用的调试工具和优化方法，帮助程序员更好地管理变量作用域。

Conclusion: 理解SAS和R的作用域规则差异对于优化变量管理至关重要。掌握这些知识能够帮助程序员编写更精确、可靠的代码，提高在两种语言中的编程实践水平。

Abstract: Variable scoping dictates how and where variables are accessible within programming languages, playing a crucial role in code efficiency and organization. This paper examines the distinct scoping rules in SAS and R, focusing on SAS's dynamic scoping and R's lexical scoping. In SAS, dynamic scoping utilizes symbol tables, resolving variables at runtime by dynamically searching through active macro layers. R, in contrast, employs lexical scoping, using environments to resolve variables based on the structure in which functions are defined. Illustrative examples highlight the differences between these scoping strategies, showcasing their impact on code behavior. Additionally, the paper outlines methods for inspecting variables in SAS's symbol tables and R's environments, offering practical insights for debugging and optimization. Strategies for controlling variable scope in both languages are discussed, enhancing code precision and reliability. This exploration equips programmers with critical understanding to optimize variable management, improving their programming practices in SAS and R.

</details>


### [2] [Lazy Evaluation: A Comparative Analysis of SAS MACROs and R Functions](https://arxiv.org/abs/2601.09839)
*Chen Ling,Yachen Wang*

Main category: cs.PL

TL;DR: 本文比较了SAS MACRO和R函数中的惰性求值机制，分析了两种语言在实现惰性求值时的不同策略及其对编程效率的影响。


<details>
  <summary>Details</summary>
Motivation: 随着制药行业从SAS向R的转型日益普遍，理解两种语言中惰性求值技术的差异对于优化代码效率至关重要。虽然R中的惰性求值已被广泛研究，但SAS中的相关应用尚未得到充分探索。

Method: 通过比较分析SAS MACRO和R函数的惰性求值机制：R使用Promise数据结构实现按需调用策略，而SAS通过符号表实现按名调用策略。论文通过示例说明这些差异如何影响编程结果。

Result: 研究发现R和SAS在惰性求值实现上存在根本差异：R的Promise结构延迟求值且不占用内存，而SAS的符号表需要存储参数。这些差异显著影响R函数和SAS MACRO的执行结果。

Conclusion: 理解R和SAS中惰性求值机制的不同实现方式，有助于程序员在两种语言中优化代码性能。随着制药行业向R的转型，这些知识对于提高编程效率和效果具有重要意义。

Abstract: Lazy evaluation is a powerful technique that can optimize code execution by deferring evaluations until their results are required, thus enhancing efficiency. In most modern programming languages, like R, lazy evaluation is commonly applied to function arguments. However, the application of lazy evaluation in SAS has not been extensively explored. This paper focuses on the mechanisms of lazy evaluation in SAS MACROs and R functions, offering a comparative analysis of the underlying principles that drive these processes.
  R's lazy evaluation is driven by a data structure called Promise, which postpones evaluation and does not occupy memory until the value is needed, utilizing a call-by-need strategy. SAS, on the other hand, achieves lazy evaluation through its symbol tables, employing memory to store parameters, and operates on a call-by-name basis. These discrepancies in lazy evaluation strategies can notably impact the results of R functions and SAS MACROs. By examining these distinct approaches, the paper illuminates the impact of lazy evaluation on programming efficiency, supported by illustrative examples. As the shift from SAS to R becomes increasingly prevalent in the pharmaceutical industry, understanding these techniques enables programmers to optimize their code for greater efficacy. This exploration serves as a guide to enhance programming capabilities and performance in both languages.

</details>


### [3] [Outrunning Big KATs: Efficient Decision Procedures for Variants of GKAT](https://arxiv.org/abs/2601.09986)
*Cheng Zhang,Qiancheng Fu,Hang Ji,Ines Santacruz Del Valle,Alexandra Silva,Marco Gaboardi*

Main category: cs.PL

TL;DR: 本文提出了几种高效的GKAT自动机迹等价判定过程，采用基于SAT求解器的符号化技术，在CF-GKAT系统中实现并验证了控制流转换，性能比现有实现提升数个数量级。


<details>
  <summary>Details</summary>
Motivation: GKAT自动机的迹等价判定在程序验证中很重要，但现有方法效率有限。需要开发更高效的决策过程来验证实际控制流转换系统。

Method: 提出了基于SAT求解器的符号化决策过程，设计了CF-GKAT的符号导数，实现了Rust实现，并在随机生成基准和真实控制流转换上进行了评估。

Result: 相比现有的KAT和CF-GKAT实现，获得了数量级的性能提升。实验还发现了行业标准反编译器Ghidra中的一个bug。

Conclusion: 提出的符号化决策过程高效实用，能够有效验证控制流转换，并在实际应用中发现了真实软件工具中的缺陷。

Abstract: This paper presents several efficient decision procedures for trace equivalence of GKAT automata, which make use of on-the-fly symbolic techniques via SAT solvers. To demonstrate applicability of our algorithms, we designed symbolic derivatives for CF-GKAT, a practical system based on GKAT designed to validate control-flow transformations. We implemented the algorithms in Rust and evaluated them on both randomly generated benchmarks and real-world control-flow transformations. Indeed, we observed order-of-magnitude performance improvements against existing implementations for both KAT and CF-GKAT. Notably, our experiments also revealed a bug in Ghidra, an industry-standard decompiler, highlighting the practical viability of these systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Federated Unlearning in Edge Networks: A Survey of Fundamentals, Challenges, Practical Applications and Future Directions](https://arxiv.org/abs/2601.09978)
*Jer Shyuan Ng,Wathsara Daluwatta,Shehan Edirimannage,Charitha Elvitigala,Asitha Kottahachchi Kankanamge Don,Ibrahim Khalil,Heng Zhang,Dusit Niyato*

Main category: cs.DC

TL;DR: 联邦遗忘学习（FUL）综述：在联邦学习框架下实现数据删除，满足隐私法规要求


<details>
  <summary>Details</summary>
Motivation: 随着连接设备和隐私敏感应用的激增，联邦学习（FL）被广泛采用，但FL本身不支持数据删除请求，而这是GDPR等法规（如被遗忘权）的要求。需要将机器学习遗忘（MU）概念扩展到联邦环境中。

Method: 本文是一篇综述论文，首先介绍FUL的基础知识，然后回顾解决三个主要实施挑战的FUL框架：通信成本、资源分配以及安全和隐私。还讨论了FUL在现代分布式计算机网络中的应用。

Result: 系统梳理了联邦遗忘学习的研究现状，总结了现有框架如何应对通信效率、资源优化和安全隐私等挑战，并展示了FUL在分布式网络中的实际应用场景。

Conclusion: FUL是一个新兴研究领域，旨在构建可信、合规、以用户为中心的联邦系统。本文通过整合现有知识和映射开放问题，为研究人员和实践者提供了基础参考，并指出了未来的研究方向。

Abstract: The proliferation of connected devices and privacy-sensitive applications has accelerated the adoption of Federated Learning (FL), a decentralized paradigm that enables collaborative model training without sharing raw data. While FL addresses data locality and privacy concerns, it does not inherently support data deletion requests that are increasingly mandated by regulations such as the Right to be Forgotten (RTBF). In centralized learning, this challenge has been studied under the concept of Machine Unlearning (MU), that focuses on efficiently removing the influence of specific data samples or clients from trained models. Extending this notion to federated settings has given rise to Federated Unlearning (FUL), a new research area concerned with eliminating the contributions of individual clients or data subsets from the global FL model in a distributed and heterogeneous environment. In this survey, we first introduce the fundamentals of FUL. Then, we review the FUL frameworks that are proposed to address the three main implementation challenges, i.e., communication cost, resource allocation as well as security and privacy. Furthermore, we discuss applications of FUL in the modern distributed computer networks. We also highlight the open challenges and future research opportunities. By consolidating existing knowledge and mapping open problems, this survey aims to serve as a foundational reference for researchers and practitioners seeking to advance FL to build trustworthy, regulation-compliant and user-centric federated systems.

</details>


### [5] [Distributed Linearly Separable Computation with Arbitrary Heterogeneous Data Assignment](https://arxiv.org/abs/2601.10177)
*Ziting Zhang,Kai Wan,Minquan Cheng,Shuo Shao,Giuseppe Caire*

Main category: cs.DC

TL;DR: 研究异构分布式线性可分计算问题，分析任意异构数据分配下任务函数可计算维度与通信成本之间的基本权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注同构设置，假设每个工作节点持有相同数量的数据集，数据分配由数据中心精心设计控制。本文考虑更一般的任意异构数据分配场景，其中数据分配预先给定且各工作节点可能持有不同数量的数据集，这更符合实际分布式系统的复杂性。

Method: 针对整数通信成本约束，提出通用计算方案和通用逆界，通过刻画数据分配结构来分析可计算维度与通信成本的权衡。然后将方案和逆界扩展到分数通信成本情况。

Result: 在任意异构数据分配下，建立了任务函数可计算维度与通信成本之间的基本权衡关系。在某些参数范围内，提出的计算方案和逆界完全一致，达到了最优性能。

Conclusion: 本文为异构分布式线性可分计算提供了理论框架，揭示了数据分配结构对系统性能的影响，为实际分布式计算系统的设计提供了理论指导。

Abstract: Distributed linearly separable computation is a fundamental problem in large-scale distributed systems, requiring the computation of linearly separable functions over different datasets across distributed workers. This paper studies a heterogeneous distributed linearly separable computation problem, including one master and N distributed workers. The linearly separable task function involves Kc linear combinations of K messages, where each message is a function of one dataset. Distinguished from the existing homogeneous settings that assume each worker holds the same number of datasets, where the data assignment is carefully designed and controlled by the data center (e.g., the cyclic assignment), we consider a more general setting with arbitrary heterogeneous data assignment across workers, where `arbitrary' means that the data assignment is given in advance and `heterogeneous' means that the workers may hold different numbers of datasets. Our objective is to characterize the fundamental tradeoff between the computable dimension of the task function and the communication cost under arbitrary heterogeneous data assignment. Under the constraint of integer communication costs, for arbitrary heterogeneous data assignment, we propose a universal computing scheme and a universal converse bound by characterizing the structure of data assignment, where they coincide under some parameter regimes. We then extend the proposed computing scheme and converse bound to the case of fractional communication costs.

</details>


### [6] [SCRamble: Adaptive Decentralized Overlay Construction for Blockchain Networks](https://arxiv.org/abs/2601.10277)
*Evangelos Kolyvas,Alexandros Antonov,Spyros Voulgaris*

Main category: cs.DC

TL;DR: SCRamble是一个去中心化协议，通过创新的链路选择策略显著减少区块链网络中的区块传播时间，从而提高交易吞吐量和系统安全性。


<details>
  <summary>Details</summary>
Motivation: 区块链发展15年来，交易吞吐量仍然是关键挑战，通常受限于每秒有限交易数。限制这一指标的根本因素是底层P2P网络中区块传播的网络延迟，这通常通过随机连接形成。加速区块传播不仅能提高交易速率，还能通过减少分叉概率来增强系统安全性。

Method: SCRamble协议采用创新的链路选择策略，整合两种启发式方法：1）评估来自相邻节点区块到达时间的评分机制；2）考虑网络延迟的第二启发式方法。

Result: SCRamble协议能显著减少区块链网络中的区块传播时间。

Conclusion: SCRamble通过其创新的链路选择策略，有效解决了区块链网络中的区块传播延迟问题，从而提高了交易吞吐量和系统安全性。

Abstract: Despite being under development for over 15 years, transaction throughput remains one of the key challenges confronting blockchains, which typically has a cap of a limited number of transactions per second. A fundamental factor limiting this metric is the network latency associated with the block propagation throughout of the underlying peer-to-peer network, typically formed through random connections. Accelerating the dissemination of blocks not only improves transaction rates, but also enhances system security by reducing the probability of forks. This paper introduces SCRamble: a decentralized protocol that significantly reduces block dissemination time in blockchain networks. SCRamble's effectiveness is attributed to its innovative link selection strategy, which integrates two heuristics: a scoring mechanism that assesses block arrival times from neighboring peers, and a second heuristic that takes network latency into account.

</details>


### [7] [Mitigating GIL Bottlenecks in Edge AI Systems](https://arxiv.org/abs/2601.10582)
*Mridankan Mandal,Smit Sanjay Shende*

Main category: cs.DC

TL;DR: 针对边缘设备上Python AI代理的运行时优化挑战，提出基于阻塞比(beta)度量的轻量级分析工具和自适应运行时系统，解决GIL导致的线程池扩展"饱和悬崖"问题。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署基于Python的AI代理面临运行时优化挑战：需要高线程数来掩盖I/O延迟，但Python的全局解释器锁(GIL)会序列化执行，导致线程池扩展出现"饱和悬崖"现象。

Method: 提出基于阻塞比(beta)度量的轻量级分析工具和自适应运行时系统，该度量能区分真正的I/O等待和GIL争用。采用库级解决方案，无需手动调优即可实现接近最优性能。

Result: 在边缘AI工作负载配置中，该方案达到96.5%的最优性能，平均效率为93.9%。相比多进程（受限于8倍内存开销）和asyncio（受CPU密集型阶段阻塞），表现更优。即使在Python 3.13t（无GIL）环境下，单核设备上的饱和悬崖问题仍然存在，验证了beta度量的普适性。

Conclusion: 提出的基于阻塞比度量的自适应运行时系统为边缘AI系统提供了实用的优化方案，既适用于GIL环境，也适用于无GIL环境，有效解决了线程池扩展的饱和悬崖问题。

Abstract: Deploying Python based AI agents on resource-constrained edge devices presents a runtime optimization challenge: high thread counts are needed to mask I/O latency, yet Python's Global Interpreter Lock (GIL) serializes execution. We demonstrate that naive thread-pool scaling causes a "saturation cliff": >= 20% throughput degradation at overprovisioned thread counts (N >= 512) on edge-representative configurations. We present a lightweight profiling tool and adaptive runtime system using a Blocking Ratio metric (beta) that distinguishes genuine I/O wait from GIL contention. Our library-based solution achieves 96.5% of optimal performance without manual tuning, outperforming multiprocessing (limited by ~8x memory overhead on devices with 512 MB-2 GB RAM) and asyncio (blocked by CPU-bound phases). Evaluation across seven edge AI workload profiles, including real ML inference with ONNX Runtime MobileNetV2, demonstrates 93.9% average efficiency. Comparative experiments with Python 3.13t (free threading) show that while GIL elimination enables ~4x throughput on multi-core edge devices, the saturation cliff persists on single-core devices, validating our beta metric for both GIL and no-GIL environments. This provides practical optimization for edge AI systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [Enhancing LUT-based Deep Neural Networks Inference through Architecture and Connectivity Optimization](https://arxiv.org/abs/2601.09773)
*Binglei Lou,Ruilin Wu,Philip Leong*

Main category: cs.AR

TL;DR: SparseLUT框架通过架构增强和训练算法优化，解决了LUT-based DNNs中LUT尺寸指数增长和稀疏连接效率低的问题，在保持精度的同时显著降低了硬件资源消耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 在FPGA等资源受限的边缘设备上部署DNN需要平衡延迟、功耗和硬件资源使用，同时保持高精度。现有的LUT-based DNNs面临两个关键挑战：LUT尺寸的指数增长和随机稀疏连接的低效率。

Method: 提出SparseLUT框架，包含两个正交优化：1) 架构增强：通过加法器聚合多个PolyLUT子神经元，显著减少LUT消耗；2) 非贪婪训练算法：通过选择性剪枝不重要输入和策略性重新生长更有效输入来优化神经元连接。

Result: 架构优化将LUT消耗减少2.0x-13.9x，推理延迟降低1.2x-1.6x，同时保持可比精度。训练优化在MNIST上实现2.13%精度提升，在Jet Substructure Classification上实现0.94%精度提升，且不增加额外面积和延迟开销。

Conclusion: SparseLUT通过架构和训练协同优化，有效解决了LUT-based DNNs的资源效率问题，为资源受限边缘设备上的高效DNN部署提供了可行方案。

Abstract: Deploying deep neural networks (DNNs) on resource-constrained edge devices such as FPGAs requires a careful balance among latency, power, and hardware resource usage, while maintaining high accuracy. Existing Lookup Table (LUT)-based DNNs -- such as LogicNets, PolyLUT, and NeuraLUT -- face two critical challenges: the exponential growth of LUT size and inefficient random sparse connectivity. This paper presents SparseLUT, a comprehensive framework that addresses these challenges through two orthogonal optimizations. First, we propose an architectural enhancement that aggregates multiple PolyLUT sub-neurons via an adder, significantly reducing LUT consumption by 2.0x-13.9x and lowering inference latency by 1.2x-1.6x, all while maintaining comparable accuracy. Building upon this foundation, we further introduce a non-greedy training algorithm that optimizes neuron connectivity by selectively pruning less significant inputs and strategically regrowing more effective ones. This training optimization, which incurs no additional area and latency overhead, delivers consistent accuracy improvements across benchmarks -- achieving up to a 2.13% gain on MNIST and 0.94% on Jet Substructure Classification compared to existing LUT-DNN approaches.

</details>


### [9] [Architectural Classification of XR Workloads: Cross-Layer Archetypes and Implications](https://arxiv.org/abs/2601.10463)
*Xinyu Shi,Simei Yang,Francky Catthoor*

Main category: cs.AR

TL;DR: 论文提出了一种跨层方法对XR工作负载进行架构分类，识别出容量受限和开销敏感等工作负载原型，为下一代XR SoC设计提供指导，强调需要从通用资源扩展转向阶段感知调度和弹性资源分配。


<details>
  <summary>Details</summary>
Motivation: XR平台需要在严格功耗和面积约束下提供确定性超低延迟性能，但XR工作负载多样性快速增加，具有异构算子类型和复杂数据流结构，这对传统CNN为中心的加速器架构构成挑战，而目前缺乏对完整XR流水线的系统性架构理解。

Method: 采用跨层方法整合基于模型的高层设计空间探索（DSE）与商用GPU和CPU硬件的经验分析，分析12个不同XR内核的代表性工作负载集，将复杂架构特征提炼为少量跨层工作负载原型。

Result: 识别出容量受限、开销敏感等工作负载原型，提取关键架构见解，为下一代XR SoC提供可操作的设计指南，发现XR架构设计需要从通用资源扩展转向阶段感知调度和弹性资源分配。

Conclusion: XR架构设计必须从通用资源扩展转向阶段感知调度和弹性资源分配，以实现未来XR系统更高的能效和性能，跨层工作负载原型方法为下一代XR SoC设计提供了系统性指导框架。

Abstract: Edge and mobile platforms for augmented and virtual reality, collectively referred to as extended reality (XR) must deliver deterministic ultra-low-latency performance under stringent power and area constraints. However, the diversity of XR workloads is rapidly increasing, characterized by heterogeneous operator types and complex dataflow structures. This trend poses significant challenges to conventional accelerator architectures centered around convolutional neural networks (CNNs), resulting in diminishing returns for traditional compute-centric optimization strategies. Despite the importance of this problem, a systematic architectural understanding of the full XR pipeline remains lacking. In this paper, we present an architectural classification of XR workloads using a cross-layer methodology that integrates model-based high-level design space exploration (DSE) with empirical profiling on commercial GPU and CPU hardware. By analyzing a representative set of workloads spanning 12 distinct XR kernels, we distill their complex architectural characteristics into a small set of cross-layer workload archetypes (e.g., capacity-limited and overhead-sensitive). Building on these archetypes, we further extract key architectural insights and provide actionable design guidelines for next-generation XR SoCs. Our study highlights that XR architecture design must shift from generic resource scaling toward phase-aware scheduling and elastic resource allocation in order to achieve greater energy efficiency and high performance in future XR systems.

</details>
