{"id": "2512.15028", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.15028", "abs": "https://arxiv.org/abs/2512.15028", "authors": ["Chin Fang", "Timothy Stitt", "Michael J. McManus", "Toshio Moriya"], "title": "Reexamining Paradigms of End-to-End Data Movement", "comment": "19 pages and 13 figures", "summary": "The pursuit of high-performance data transfer often focuses on raw network bandwidth, and international links of 100 Gbps or higher are frequently considered the primary enabler. While necessary, this network-centric view is incomplete, equating provisioned link speeds with practical, sustainable data movement capabilities across the entire edge-to-core spectrum. This paper investigates six common paradigms, from the often-cited constraints of network latency and TCP congestion control algorithms to host-side factors such as CPU performance and virtualization that critically impact data movement workflows. We validated our findings using a latency-emulation-capable testbed for high-speed WAN performance prediction and through extensive production measurements from resource-constrained edge environments to a 100 Gbps operational link connecting Switzerland and California, U.S. These results show that the principal bottlenecks often reside outside the network core, and that a holistic hardware-software co-design ensures consistent performance, whether moving data at 1 Gbps or 100 Gbps and faster. This approach effectively closes the fidelity gap between benchmark results and diverse and complex production environments.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u8ffd\u6c42\u9ad8\u6027\u80fd\u6570\u636e\u4f20\u8f93\u4e0d\u5e94\u4ec5\u5173\u6ce8\u7f51\u7edc\u5e26\u5bbd\uff0c\u800c\u9700\u91c7\u7528\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u6574\u4f53\u89c6\u89d2\uff0c\u89e3\u51b3\u4ece\u8fb9\u7f18\u5230\u6838\u5fc3\u5168\u94fe\u8def\u4e2d\u7684\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u9ad8\u6027\u80fd\u6570\u636e\u4f20\u8f93\u7814\u7a76\u8fc7\u5ea6\u5173\u6ce8\u7f51\u7edc\u5e26\u5bbd\uff0c\u7279\u522b\u662f100Gbps\u53ca\u4ee5\u4e0a\u56fd\u9645\u94fe\u8def\uff0c\u4f46\u4ec5\u5173\u6ce8\u7f51\u7edc\u5e26\u5bbd\u662f\u4e0d\u5b8c\u6574\u7684\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u89c6\u89d2\u6765\u7406\u89e3\u5b9e\u9645\u53ef\u6301\u7eed\u7684\u6570\u636e\u4f20\u8f93\u80fd\u529b\u3002", "method": "\u7814\u7a76\u516d\u79cd\u5e38\u89c1\u8303\u5f0f\uff1a\u7f51\u7edc\u5ef6\u8fdf\u3001TCP\u62e5\u585e\u63a7\u5236\u7b97\u6cd5\u7b49\u7f51\u7edc\u56e0\u7d20\uff0c\u4ee5\u53caCPU\u6027\u80fd\u3001\u865a\u62df\u5316\u7b49\u4e3b\u673a\u7aef\u56e0\u7d20\u3002\u4f7f\u7528\u652f\u6301\u5ef6\u8fdf\u4eff\u771f\u7684\u6d4b\u8bd5\u5e8a\u8fdb\u884c\u9ad8\u901f\u5e7f\u57df\u7f51\u6027\u80fd\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u4ece\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u73af\u5883\u5230100Gbps\u745e\u58eb-\u52a0\u5dde\u94fe\u8def\u7684\u5e7f\u6cdb\u751f\u4ea7\u6d4b\u91cf\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u4e3b\u8981\u74f6\u9888\u901a\u5e38\u4e0d\u5728\u7f51\u7edc\u6838\u5fc3\uff0c\u800c\u5728\u7f51\u7edc\u4e4b\u5916\u7684\u5176\u4ed6\u73af\u8282\u3002\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u80fd\u786e\u4fdd\u4ece1Gbps\u5230100Gbps\u53ca\u4ee5\u4e0a\u6570\u636e\u4f20\u8f93\u7684\u4e00\u81f4\u6027\u80fd\uff0c\u6709\u6548\u7f29\u5c0f\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u4e0e\u590d\u6742\u751f\u4ea7\u73af\u5883\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u9ad8\u6027\u80fd\u6570\u636e\u4f20\u8f93\u9700\u8981\u8d85\u8d8a\u7f51\u7edc\u4e2d\u5fc3\u7684\u89c6\u89d2\uff0c\u91c7\u7528\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u6574\u4f53\u65b9\u6cd5\uff0c\u624d\u80fd\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u6301\u7eed\u7684\u9ad8\u6027\u80fd\u6570\u636e\u4f20\u8f93\u3002"}}
{"id": "2512.15306", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15306", "abs": "https://arxiv.org/abs/2512.15306", "authors": ["Erik Schultheis", "Dan Alistarh"], "title": "LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs", "comment": null, "summary": "We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.", "AI": {"tldr": "LLMQ\u662f\u4e00\u4e2a\u9488\u5bf9\u4e2d\u7b49\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff083B-32B\u53c2\u6570\uff09\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u8bad\u7ec3\u7684\u7aef\u5230\u7aefCUDA/C++\u5b9e\u73b0\uff0c\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u548c\u901a\u4fe1\u74f6\u9888\uff0c\u80fd\u5728\u5355\u5f2016GB\u6e38\u620f\u5361\u4e0a\u8bad\u7ec37B\u6a21\u578b\uff0c\u6216\u57284\u5f20RTX 4090\u4e0a\u8bad\u7ec332B\u6a21\u578b\u3002", "motivation": "\u6d88\u8d39\u7ea7GPU\u5185\u5b58\u6709\u9650\u4e14\u901a\u4fe1\u901f\u5ea6\u6162\uff0c\u96be\u4ee5\u8bad\u7ec3\u4e2d\u7b49\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u3002\u73b0\u6709\u7cfb\u7edf\u4e3b\u8981\u9488\u5bf9\u6570\u636e\u4e2d\u5fc3\u7ea7GPU\u8bbe\u8ba1\uff0c\u9700\u8981\u4e3a\u666e\u901a\u7528\u6237\u63d0\u4f9b\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u9ad8\u6548\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6fc0\u6d3b\u68c0\u67e5\u70b9\u3001\u5378\u8f7d\u6280\u672f\u548c\u57fa\u4e8e\u590d\u5236\u5f15\u64ce\u7684\u96c6\u5408\u901a\u4fe1\u7b49\u4f18\u5316\u6280\u672f\uff0c\u9488\u5bf9\u5185\u5b58\u548c\u901a\u4fe1\u74f6\u9888\u8fdb\u884c\u4f18\u5316\u3002\u5b9e\u73b0\u6807\u51c6\u76848\u4f4d\u8bad\u7ec3\u6d41\u7a0b\uff0c\u65e0\u9700\u989d\u5916\u7684\u7b97\u6cd5\u8fd1\u4f3c\u3002", "result": "\u5728\u5355\u5f2016GB\u4e2d\u7aef\u6e38\u620f\u5361\u4e0a\u8bad\u7ec37B\u6a21\u578b\uff0c\u6216\u57284\u5f20RTX 4090\u5de5\u4f5c\u7ad9\u4e0a\u8bad\u7ec332B\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u7ea650%\u7684FLOP\u5229\u7528\u7387\u3002\u6548\u7387\u53ef\u4e0e\u5728\u66f4\u6602\u8d35\u7684\u4e91\u7ea7GPU\u4e0a\u7684\u751f\u4ea7\u7ea7\u7cfb\u7edf\u76f8\u5ab2\u7f8e\u3002", "conclusion": "LLMQ\u8bc1\u660e\u4e86\u5728\u6d88\u8d39\u7ea7GPU\u4e0a\u9ad8\u6548\u8bad\u7ec3\u4e2d\u7b49\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u666e\u901a\u7528\u6237\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u5b9e\u60e0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u6027\u80fd\u53ef\u4e0e\u6602\u8d35\u7684\u4e91\u7ea7\u7cfb\u7edf\u7ade\u4e89\u3002"}}
{"id": "2512.15595", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.15595", "abs": "https://arxiv.org/abs/2512.15595", "authors": ["Daniel J\u00fcnger", "Kevin Kristensen", "Yunsong Wang", "Xiangyao Yu", "Bertil Schmidt"], "title": "Optimizing Bloom Filters for Modern GPU Architectures", "comment": "13 pages, 12 figures", "summary": "Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.\n  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\\times$ ($15.4\\times$) for bulk filter lookup (construction), respectively, achieving above $92\\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86GPU\u4e0aBloom filter\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u901a\u8fc7\u5411\u91cf\u5316\u3001\u7ebf\u7a0b\u534f\u4f5c\u548c\u8ba1\u7b97\u5ef6\u8fdf\u4e09\u4e2a\u7ef4\u5ea6\u7684\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u541e\u5410\u91cf\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd511.35-15.4\u500d\u3002", "motivation": "Bloom filter\u662f\u8fd1\u4f3c\u6210\u5458\u67e5\u8be2\u7684\u57fa\u7840\u6570\u636e\u7ed3\u6784\uff0cGPU\u56e0\u5176\u5927\u89c4\u6a21\u7ebf\u7a0b\u7ea7\u5e76\u884c\u6027\u548c\u9ad8\u5e26\u5bbd\u5185\u5b58\uff0c\u662f\u52a0\u901fBloom filter\u7684\u7406\u60f3\u5e73\u53f0\u3002\u7136\u800c\uff0c\u5c3d\u7ba1CPU\u4f18\u5316\u5b9e\u73b0\u5df2\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0cGPU\u8bbe\u8ba1\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u63a2\u7d22GPU\u4e0a\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff1a1) \u5411\u91cf\u5316\uff0c2) \u7ebf\u7a0b\u534f\u4f5c\uff0c3) \u8ba1\u7b97\u5ef6\u8fdf\u3002\u901a\u8fc7\u6a21\u5757\u5316\u7684CUDA/C++\u5b9e\u73b0\uff0c\u5206\u6790\u4e0d\u540c\u53c2\u6570\u914d\u7f6e\u4e0b\u786c\u4ef6\u7684\u54cd\u5e94\uff0c\u5e76\u6d4b\u91cf\u6027\u80fd\u8d8b\u52bf\u3002", "result": "\u4f18\u5316\u8bbe\u8ba1\u7a81\u7834\u4e86\u901f\u5ea6\u4e0e\u7cbe\u5ea6\u4e4b\u95f4\u7684\u4f20\u7edf\u6743\u8861\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u914d\u7f6e\u4f18\u8d8a\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u901a\u5e38\u4ec5\u9650\u4e8e\u9ad8\u9519\u8bef\u7387\u53d8\u4f53\u7684\u541e\u5410\u91cf\u3002\u5728\u76f8\u540c\u9519\u8bef\u7387\u4e0b\uff0c\u6279\u91cf\u8fc7\u6ee4\u5668\u67e5\u627e\u6027\u80fd\u63d0\u534711.35\u500d\uff0c\u6784\u5efa\u6027\u80fd\u63d0\u534715.4\u500d\uff0c\u5728B200 GPU\u4e0a\u8fbe\u5230\u5b9e\u9645\u901f\u5ea6\u6781\u9650\u768492%\u4ee5\u4e0a\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86GPU\u4e0aBloom filter\u8bbe\u8ba1\u7684\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u5411\u91cf\u5316\u3001\u7ebf\u7a0b\u534f\u4f5c\u548c\u8ba1\u7b97\u5ef6\u8fdf\u7684\u4f18\u5316\u7ec4\u5408\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8fc7\u6ee4\u5668\u9002\u5408GPU\u7f13\u5b58\u57df\u65f6\u6548\u679c\u6700\u4f73\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u541e\u5410\u91cf\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002"}}
{"id": "2512.15659", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.15659", "abs": "https://arxiv.org/abs/2512.15659", "authors": ["A. Jesse Jiryu Davis", "Murat Demirbas", "Lingzhi Deng"], "title": "LeaseGuard: Raft Leases Done Right", "comment": null, "summary": "Raft is a leading consensus algorithm for replicating writes in distributed databases. However, distributed databases also require consistent reads. To guarantee read consistency, a Raft-based system must either accept the high communication overhead of a safety check for each read, or implement leader leases. Prior lease protocols are vaguely specified and hurt availability, so most Raft systems implement them incorrectly or not at all. We introduce LeaseGuard, a novel lease algorithm that relies on guarantees specific to Raft elections. LeaseGuard is simple, rigorously specified in TLA+, and includes two novel optimizations that maximize availability during leader failover. The first optimization restores write throughput quickly, and the second improves read availability. We evaluate LeaseGuard with a simulation in Python and an implementation in LogCabin, the C++ reference implementation of Raft. By replacing LogCabin's default consistency mechanism (quorum checks), LeaseGuard reduces the overhead of consistent reads from one to zero network roundtrips. It also improves write throughput from ~1000 to ~10,000 writes per second, by eliminating contention between writes and quorum reads. Whereas traditional leases ban all reads on a new leader while it waits for a lease, in our LeaseGuard test the new leader instantly allows 99% of reads to succeed.", "AI": {"tldr": "LeaseGuard\u662f\u4e00\u4e2a\u57fa\u4e8eRaft\u7684\u65b0\u578b\u79df\u7ea6\u7b97\u6cd5\uff0c\u901a\u8fc7\u9009\u4e3e\u4fdd\u8bc1\u5b9e\u73b0\u96f6\u7f51\u7edc\u5f80\u8fd4\u7684\u4e00\u81f4\u8bfb\u53d6\uff0c\u663e\u8457\u63d0\u5347\u8bfb\u5199\u6027\u80fd", "motivation": "Raft\u7cfb\u7edf\u9700\u8981\u4fdd\u8bc1\u8bfb\u53d6\u4e00\u81f4\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u901a\u4fe1\u5f00\u9500\u5927\uff08\u5b89\u5168\u68c0\u67e5\uff09\uff0c\u8981\u4e48\u79df\u7ea6\u534f\u8bae\u5b9a\u4e49\u6a21\u7cca\u4e14\u635f\u5bb3\u53ef\u7528\u6027\uff0c\u5bfc\u81f4\u591a\u6570\u7cfb\u7edf\u5b9e\u73b0\u4e0d\u6b63\u786e\u6216\u4e0d\u5b9e\u73b0\u79df\u7ea6", "method": "\u63d0\u51faLeaseGuard\u7b97\u6cd5\uff0c\u5229\u7528Raft\u9009\u4e3e\u7684\u7279\u5b9a\u4fdd\u8bc1\uff0c\u5728TLA+\u4e2d\u4e25\u683c\u89c4\u8303\uff0c\u5305\u542b\u4e24\u4e2a\u65b0\u9896\u4f18\u5316\uff1a\u5feb\u901f\u6062\u590d\u5199\u5165\u541e\u5410\u91cf\u548c\u63d0\u9ad8\u8bfb\u53d6\u53ef\u7528\u6027", "result": "\u5728LogCabin\u4e2d\u5b9e\u73b0LeaseGuard\uff0c\u5c06\u4e00\u81f4\u8bfb\u53d6\u5f00\u9500\u4ece1\u4e2a\u7f51\u7edc\u5f80\u8fd4\u964d\u4e3a0\uff0c\u5199\u5165\u541e\u5410\u91cf\u4ece~1000\u63d0\u5347\u5230~10,000\u6b21/\u79d2\uff0c\u65b0\u9886\u5bfc\u8005\u80fd\u7acb\u5373\u5141\u8bb899%\u7684\u8bfb\u53d6\u6210\u529f", "conclusion": "LeaseGuard\u63d0\u4f9b\u4e86\u7b80\u5355\u3001\u4e25\u683c\u89c4\u8303\u7684\u79df\u7ea6\u7b97\u6cd5\uff0c\u663e\u8457\u6539\u5584Raft\u7cfb\u7edf\u7684\u8bfb\u53d6\u4e00\u81f4\u6027\u548c\u6574\u4f53\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u79df\u7ea6\u7684\u53ef\u7528\u6027\u95ee\u9898"}}
{"id": "2512.14805", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14805", "abs": "https://arxiv.org/abs/2512.14805", "authors": ["Ellie Y. Cheng", "Logan Weber", "Tian Jin", "Michael Carbin"], "title": "Sharing State Between Prompts and Programs", "comment": null, "summary": "The rise of large language models (LLMs) has introduced a new type of programming: natural language programming. By writing prompts that direct LLMs to perform natural language processing, code generation, reasoning, etc., users are writing code in natural language -- natural language code -- for the LLM to execute.\n  An emerging area of research enables interoperability between natural language code and formal languages such as Python. We present a novel programming abstraction, shared program state, that removes the manual work required to enable interoperability between natural language code and program state. With shared program state, programmers can write natural code that directly writes program variables, computes with program objects, and implements control flow in the program. We present a schema for specifying natural function interfaces that extend programming systems to support natural code and leverage this schema to specify shared program state as a natural function interface.\n  We implement shared program state in the Nightjar programming system. Nightjar enables programmers to write Python programs that contain natural code that shares the Python program state. We show that Nightjar programs achieve comparable or higher task accuracy than manually written implementations (+4-19%), while decreasing the lines of code by 39.6% on average. The tradeoff to using Nightjar is that it may incur runtime overhead (0.4-4.3x runtime of manual implementations).", "AI": {"tldr": "Nightjar\u7f16\u7a0b\u7cfb\u7edf\u5f15\u5165\u5171\u4eab\u7a0b\u5e8f\u72b6\u6001\u62bd\u8c61\uff0c\u8ba9\u81ea\u7136\u8bed\u8a00\u4ee3\u7801\u53ef\u4ee5\u76f4\u63a5\u64cd\u4f5cPython\u7a0b\u5e8f\u53d8\u91cf\uff0c\u51cf\u5c11\u624b\u52a8\u96c6\u6210\u5de5\u4f5c\uff0c\u63d0\u9ad8\u4efb\u52a1\u51c6\u786e\u7387\u5e76\u51cf\u5c11\u4ee3\u7801\u91cf\uff0c\u4f46\u4f1a\u5e26\u6765\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\uff0c\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b\u6210\u4e3a\u65b0\u8303\u5f0f\uff0c\u4f46\u81ea\u7136\u8bed\u8a00\u4ee3\u7801\u4e0e\u5f62\u5f0f\u8bed\u8a00\uff08\u5982Python\uff09\u4e4b\u95f4\u7684\u4e92\u64cd\u4f5c\u6027\u9700\u8981\u5927\u91cf\u624b\u52a8\u5de5\u4f5c\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u8ba9\u81ea\u7136\u8bed\u8a00\u4ee3\u7801\u80fd\u76f4\u63a5\u64cd\u4f5c\u7a0b\u5e8f\u72b6\u6001\uff0c\u51cf\u5c11\u96c6\u6210\u8d1f\u62c5\u3002", "method": "\u63d0\u51fa\u5171\u4eab\u7a0b\u5e8f\u72b6\u6001\u7f16\u7a0b\u62bd\u8c61\uff0c\u901a\u8fc7\u81ea\u7136\u51fd\u6570\u63a5\u53e3\u6a21\u5f0f\u6269\u5c55\u7f16\u7a0b\u7cfb\u7edf\u4ee5\u652f\u6301\u81ea\u7136\u4ee3\u7801\u3002\u5728Nightjar\u7cfb\u7edf\u4e2d\u5b9e\u73b0\uff0c\u5141\u8bb8Python\u7a0b\u5e8f\u5305\u542b\u80fd\u5171\u4eabPython\u7a0b\u5e8f\u72b6\u6001\u7684\u81ea\u7136\u4ee3\u7801\u3002", "result": "Nightjar\u7a0b\u5e8f\u76f8\u6bd4\u624b\u52a8\u5b9e\u73b0\u7684\u4efb\u52a1\u51c6\u786e\u7387\u76f8\u5f53\u6216\u66f4\u9ad8\uff08+4-19%\uff09\uff0c\u5e73\u5747\u51cf\u5c1139.6%\u7684\u4ee3\u7801\u884c\u6570\u3002\u4ee3\u4ef7\u662f\u8fd0\u884c\u65f6\u5f00\u9500\u4e3a\u624b\u52a8\u5b9e\u73b0\u76840.4-4.3\u500d\u3002", "conclusion": "\u5171\u4eab\u7a0b\u5e8f\u72b6\u6001\u62bd\u8c61\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u7136\u8bed\u8a00\u4ee3\u7801\u4e0e\u5f62\u5f0f\u8bed\u8a00\u4e4b\u95f4\u7684\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u5f00\u53d1\u5de5\u4f5c\u91cf\uff0c\u867d\u7136\u5e26\u6765\u8fd0\u884c\u65f6\u5f00\u9500\uff0c\u4f46\u5728\u51c6\u786e\u7387\u548c\u4ee3\u7801\u7b80\u6d01\u6027\u65b9\u9762\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2512.15251", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.15251", "abs": "https://arxiv.org/abs/2512.15251", "authors": ["Michael Mecik", "Martin Kumm"], "title": "Implementation and Analysis of Thermometer Encoding in DWN FPGA Accelerators", "comment": "Accepted at the 2025 Asilomar Conference on Signals, Systems, and Computers", "summary": "Fully parallel neural network accelerators on field-programmable gate arrays (FPGAs) offer high throughput for latency-critical applications but face hardware resource constraints. Weightless neural networks (WNNs) efficiently replace arithmetic with logic-based inference. Differential weightless neural networks (DWN) further optimize resource usage by learning connections between encoders and LUT layers via gradient-based training. However, DWNs rely on thermometer encoding, and the associated hardware cost has not been fully evaluated. We present a DWN hardware generator that includes thermometer encoding explicitly. Experiments on the Jet Substructure Classification (JSC) task show that encoding can increase LUT usage by up to 3.20$\\times$, dominating costs in small networks and highlighting the need for encoding-aware hardware design in DWN accelerators.", "AI": {"tldr": "DWN\u786c\u4ef6\u751f\u6210\u5668\u5305\u542b\u6e29\u5ea6\u8ba1\u7f16\u7801\uff0c\u5b9e\u9a8c\u663e\u793a\u7f16\u7801\u6210\u672c\u5728\u5c0f\u578b\u7f51\u7edc\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u53ef\u8fbeLUT\u4f7f\u7528\u91cf\u76843.2\u500d", "motivation": "FPGA\u4e0a\u7684\u5168\u5e76\u884c\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u9762\u4e34\u786c\u4ef6\u8d44\u6e90\u9650\u5236\uff0cDWN\u901a\u8fc7\u68af\u5ea6\u8bad\u7ec3\u4f18\u5316\u8d44\u6e90\u4f7f\u7528\uff0c\u4f46\u4f9d\u8d56\u6e29\u5ea6\u8ba1\u7f16\u7801\u4e14\u5176\u786c\u4ef6\u6210\u672c\u672a\u5145\u5206\u8bc4\u4f30", "method": "\u63d0\u51fa\u4e86\u5305\u542b\u6e29\u5ea6\u8ba1\u7f16\u7801\u7684DWN\u786c\u4ef6\u751f\u6210\u5668\uff0c\u5728Jet Substructure Classification\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30", "result": "\u7f16\u7801\u53ef\u4f7fLUT\u4f7f\u7528\u91cf\u589e\u52a0\u8fbe3.20\u500d\uff0c\u5728\u5c0f\u578b\u7f51\u7edc\u4e2d\u6210\u672c\u5360\u4e3b\u5bfc\u5730\u4f4d", "conclusion": "DWN\u52a0\u901f\u5668\u9700\u8981\u7f16\u7801\u611f\u77e5\u7684\u786c\u4ef6\u8bbe\u8ba1\uff0c\u7f16\u7801\u6210\u672c\u662f\u91cd\u8981\u8003\u8651\u56e0\u7d20"}}
{"id": "2512.15705", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15705", "abs": "https://arxiv.org/abs/2512.15705", "authors": ["Xuting Liu", "Daniel Alexander", "Siva Kesava Reddy Kakarla", "Behnaz Arzani", "Vincent Liu"], "title": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX", "comment": null, "summary": "Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.", "AI": {"tldr": "DREX\u7cfb\u7edf\u901a\u8fc7\u52a8\u6001\u91cd\u7ec4\u6279\u5904\u7406\u6765\u4f18\u5316\u65e9\u671f\u9000\u51faLLM\u63a8\u7406\uff0c\u907f\u514d\u5f3a\u5236\u7edf\u4e00\u9000\u51fa\u51b3\u7b56\uff0c\u5728\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u541e\u5410\u91cf2-12%\u3002", "motivation": "\u4f20\u7edf\u6279\u5904\u7406\u6846\u67b6\u4e0d\u9002\u5408\u65e9\u671f\u9000\u51faLLM\u67b6\u6784\uff0c\u56e0\u4e3a\u6279\u5904\u7406\u4e2d\u4e0d\u540c\u8bf7\u6c42\u7684\u9000\u51fa\u65f6\u95f4\u4e0d\u540c\u3002\u73b0\u6709\u65b9\u6848\u8981\u4e48\u5f3a\u5236\u7edf\u4e00\u9000\u51fa\u51b3\u7b56\uff08\u9519\u8fc7\u65e9\u671f\u9000\u51fa\u673a\u4f1a\uff09\uff0c\u8981\u4e48\u5f3a\u5236\u63d0\u524d\u9000\u51fa\uff08\u964d\u4f4e\u8f93\u51fa\u8d28\u91cf\uff09\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u91cd\u7ec4\u6279\u5904\u7406\uff1a\u5728\u65e9\u671f\u9000\u51fa\u70b9\u52a8\u6001\u91cd\u7ec4\u6279\u5904\u7406\uff0c\u6ee1\u8db3\u9000\u51fa\u6761\u4ef6\u7684\u8bf7\u6c42\u7acb\u5373\u5904\u7406\uff0c\u7ee7\u7eed\u7684\u8bf7\u6c42\u653e\u5165\u7f13\u51b2\u533a\u91cd\u65b0\u5206\u7ec4\u540e\u8f6c\u53d1\u5230\u66f4\u6df1\u5c42\u3002DREX\u7cfb\u7edf\u5b9e\u73b0\u6b64\u65b9\u6848\uff0c\u5305\u542b\u65e0\u62f7\u8d1d\u91cd\u7ec4\u7f13\u51b2\u533a\u548c\u57fa\u4e8e\u5206\u6790\u7684\u8c03\u5ea6\u5668\u4f18\u5316\u3002", "result": "DREX\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u541e\u5410\u91cf2-12%\uff0c\u540c\u65f6\u5b8c\u5168\u6d88\u9664\u975e\u81ea\u613f\u9000\u51fa\uff0c\u4fdd\u6301\u65e9\u671f\u9000\u51fa\u6a21\u578b\u9884\u671f\u7684\u8f93\u51fa\u8d28\u91cf\u3002", "conclusion": "\u52a8\u6001\u91cd\u7ec4\u6279\u5904\u7406\u662f\u89e3\u51b3\u65e9\u671f\u9000\u51faLLM\u6279\u5904\u7406\u95ee\u9898\u7684\u6709\u6548\u65b9\u6848\uff0cDREX\u7cfb\u7edf\u5728\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2512.15515", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.15515", "abs": "https://arxiv.org/abs/2512.15515", "authors": ["Zhihan Xu", "Rajgopal Kannan", "Viktor K. Prasanna"], "title": "FAME: FPGA Acceleration of Secure Matrix Multiplication with Homomorphic Encryption", "comment": null, "summary": "Homomorphic Encryption (HE) enables secure computation on encrypted data, addressing privacy concerns in cloud computing. However, the high computational cost of HE operations, particularly matrix multiplication (MM), remains a major barrier to its practical deployment. Accelerating homomorphic encrypted MM (HE MM) is therefore crucial for applications such as privacy-preserving machine learning.\n  In this paper, we present a bandwidth-efficient FPGA implementation of HE MM. We first develop a cost model to evaluate the on-chip memory requirements for a given set of HE parameters and input matrix sizes. Our analysis shows that optimizing on-chip memory usage is critical for scalable and efficient HE MM. To this end, we design a novel datapath for Homomorphic Linear Transformation (HLT), the primary bottleneck in HE MM. The proposed datapath significantly reduces off-chip memory traffic and on-chip memory demand by enabling fine-grained data reuse. Leveraging this datapath, we introduce FAME, the first FPGA-based accelerator specifically tailored for HE MM. FAME supports arbitrary matrix shapes and is configurable across a wide range of HE parameter sets. We implement FAME on an Alveo U280 FPGA and evaluate its performance across diverse matrix sizes and shapes. Experimental results show that FAME achieves an average speedup of 221x over state-of-the-art CPU-based implementations, demonstrating its scalability and practicality for large-scale consecutive HE MM and real-world workloads.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFAME\uff0c\u4e00\u79cd\u9488\u5bf9\u540c\u6001\u52a0\u5bc6\u77e9\u9635\u4e58\u6cd5\u4f18\u5316\u7684FPGA\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u8def\u5f84\u8bbe\u8ba1\u51cf\u5c11\u5185\u5b58\u5e26\u5bbd\u9700\u6c42\uff0c\u5b9e\u73b0\u6bd4CPU\u65b9\u6848\u5e73\u5747221\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u540c\u6001\u52a0\u5bc6\uff08HE\uff09\u867d\u7136\u80fd\u5b9e\u73b0\u52a0\u5bc6\u6570\u636e\u7684\u5b89\u5168\u8ba1\u7b97\uff0c\u4f46\u5176\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u7279\u522b\u662f\u77e9\u9635\u4e58\u6cd5\uff08MM\uff09\u64cd\u4f5c\uff0c\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u52a0\u901f\u540c\u6001\u52a0\u5bc6\u77e9\u9635\u4e58\u6cd5\u5bf9\u4e8e\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "1. \u5f00\u53d1\u6210\u672c\u6a21\u578b\u8bc4\u4f30\u7ed9\u5b9aHE\u53c2\u6570\u548c\u8f93\u5165\u77e9\u9635\u5927\u5c0f\u7684\u7247\u4e0a\u5185\u5b58\u9700\u6c42\uff1b2. \u8bbe\u8ba1\u65b0\u9896\u7684\u540c\u6001\u7ebf\u6027\u53d8\u6362\uff08HLT\uff09\u6570\u636e\u8def\u5f84\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6570\u636e\u91cd\u7528\uff0c\u663e\u8457\u51cf\u5c11\u7247\u5916\u5185\u5b58\u6d41\u91cf\u548c\u7247\u4e0a\u5185\u5b58\u9700\u6c42\uff1b3. \u57fa\u4e8e\u6b64\u6570\u636e\u8def\u5f84\u6784\u5efaFAME\u52a0\u901f\u5668\uff0c\u652f\u6301\u4efb\u610f\u77e9\u9635\u5f62\u72b6\u548c\u5e7f\u6cdb\u7684HE\u53c2\u6570\u96c6\u3002", "result": "\u5728Alveo U280 FPGA\u4e0a\u5b9e\u73b0FAME\uff0c\u8bc4\u4f30\u4e0d\u540c\u77e9\u9635\u5927\u5c0f\u548c\u5f62\u72b6\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cFAME\u76f8\u6bd4\u6700\u5148\u8fdb\u7684CPU\u5b9e\u73b0\u5e73\u5747\u52a0\u901f221\u500d\uff0c\u8bc1\u660e\u5176\u5728\u5927\u89c4\u6a21\u8fde\u7eedHE MM\u548c\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "FAME\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u4f7f\u7528\u548c\u6570\u636e\u91cd\u7528\uff0c\u4e3a\u540c\u6001\u52a0\u5bc6\u77e9\u9635\u4e58\u6cd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684FPGA\u52a0\u901f\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86HE\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
