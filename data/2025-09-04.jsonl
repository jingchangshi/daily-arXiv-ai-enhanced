{"id": "2509.03318", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.03318", "abs": "https://arxiv.org/abs/2509.03318", "authors": ["Eduard Kamburjan", "Vidar Norstein Klungre", "Yuanwei Qu", "Rudolf Schlatte", "Egor V. Kostylev", "Martin Giese", "Einar Broch Johnsen"], "title": "Semantically Reflected Programs", "comment": null, "summary": "This paper addresses the dichotomy between the formalization of structural\nand the formalization of behavioral knowledge by means of semantically lifted\nprograms, which explore an intuitive connection between programs and knowledge\ngraphs. While knowledge graphs and ontologies are eminently useful to represent\nformal knowledge about a system's individuals and universals, programming\nlanguages are designed to describe the system's evolution. To address this\ndichotomy, we introduce a semantic lifting of the program states of an\nexecuting program into a knowledge graph, for an object-oriented programming\nlanguage. The resulting graph is exposed as a semantic reflection layer within\nthe programming language, allowing programmers to leverage knowledge of the\napplication domain in their programs. In this paper, we formalize semantic\nlifting and semantic reflection for a small programming language, SMOL, explain\nthe operational aspects of the language, and consider type correctness and\nvirtualisation for runtime program queries through the semantic reflection\nlayer. We illustrate semantic lifting and semantic reflection through a case\nstudy of geological modelling and discuss different applications of the\ntechnique. The language implementation is open source and available online."}
{"id": "2509.02873", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.02873", "abs": "https://arxiv.org/abs/2509.02873", "authors": ["Zhantong Qiu", "Mahyar Samani", "Jason Lowe-Power"], "title": "Portable Targeted Sampling Framework Using LLVM", "comment": null, "summary": "Comprehensive architectural evaluation of full workloads is throttled by slow\nsimulation and per-binary sampling pipelines. We present Nugget, a flexible\nframework for portable sampling across simulators and real hardware, ISAs, and\nlibraries. Nugget operates at the LLVM IR level to perform binary-agnostic\ninterval analysis, then emits lightweight, cross-platform\nexecutables--nuggets--that can be validated on real machines before driving\nsimulation. Across SPEC CPU2017, NPB, and LSMS, Nugget cuts interval-analysis\ncost by orders of magnitude relative to functional simulation (up to ~578X on\nmultithreaded NPB), keeps single-thread overhead low, and enables native-speed\nvalidation of selected samples. Case studies with gem5 show that nuggets\nsupport evaluation of system performance and model accuracy. Nugget makes\nsampling methodology research faster and more portable."}
{"id": "2509.02767", "categories": ["cs.DC", "cs.CY", "91-08", "J.1; H.1.m"], "pdf": "https://arxiv.org/pdf/2509.02767", "abs": "https://arxiv.org/abs/2509.02767", "authors": ["Benedikt Pittl", "Werner Mach", "Erich Schikuta"], "title": "A Novel IaaS Tax Model as Leverage Towards Green Cloud Computing", "comment": null, "summary": "The cloud computing technology uses datacenters, which require energy. Recent\ntrends show that the required energy for these datacenters will rise over time,\nor at least remain constant. Hence, the scientific community developed\ndifferent algorithms, architectures, and approaches for improving the energy\nefficiency of cloud datacenters, which are summarized under the umbrella term\nGreen Cloud computing. In this paper, we use an economic approach - taxes - for\nreducing the energy consumption of datacenters. We developed a tax model called\nGreenCloud tax, which penalizes energy-inefficient datacenters while fostering\ndatacenters that are energy-efficient. Hence, providers running\nenergy-efficient datacenters are able to offer cheaper prices to consumers,\nwhich consequently leads to a shift of workloads from energy-inefficient\ndatacenters to energy-efficient datacenters. The GreenCloud tax approach was\nimplemented using the simulation environment CloudSim. We applied real data\nsets published in the SPEC benchmark for the executed simulation scenarios,\nwhich we used for evaluating the GreenCloud tax."}
{"id": "2509.03103", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.03103", "abs": "https://arxiv.org/abs/2509.03103", "authors": ["Abdul Rahoof", "Vivek Chaturvedi", "Muhammad Shafique"], "title": "FastCaps: A Design Methodology for Accelerating Capsule Network on Field Programmable Gate Arrays", "comment": "2023 International Joint Conference on Neural Networks (IJCNN)", "summary": "Capsule Network (CapsNet) has shown significant improvement in understanding\nthe variation in images along with better generalization ability compared to\ntraditional Convolutional Neural Network (CNN). CapsNet preserves spatial\nrelationship among extracted features and apply dynamic routing to efficiently\nlearn the internal connections between capsules. However, due to the capsule\nstructure and the complexity of the routing mechanism, it is non-trivial to\naccelerate CapsNet performance in its original form on Field Programmable Gate\nArray (FPGA). Most of the existing works on CapsNet have achieved limited\nacceleration as they implement only the dynamic routing algorithm on FPGA,\nwhile considering all the processing steps synergistically is important for\nreal-world applications of Capsule Networks. Towards this, we propose a novel\ntwo-step approach that deploys a full-fledged CapsNet on FPGA. First, we prune\nthe network using a novel Look-Ahead Kernel Pruning (LAKP) methodology that\nuses the sum of look-ahead scores of the model parameters. Next, we simplify\nthe nonlinear operations, reorder loops, and parallelize operations of the\nrouting algorithm to reduce CapsNet hardware complexity. To the best of our\nknowledge, this is the first work accelerating a full-fledged CapsNet on FPGA.\nExperimental results on the MNIST and F-MNIST datasets (typical in Capsule\nNetwork community) show that the proposed LAKP approach achieves an effective\ncompression rate of 99.26% and 98.84%, and achieves a throughput of 82 FPS and\n48 FPS on Xilinx PYNQ-Z1 FPGA, respectively. Furthermore, reducing the hardware\ncomplexity of the routing algorithm increases the throughput to 1351 FPS and\n934 FPS respectively. As corroborated by our results, this work enables highly\nperformance-efficient deployment of CapsNets on low-cost FPGA that are popular\nin modern edge devices."}
{"id": "2509.03018", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03018", "abs": "https://arxiv.org/abs/2509.03018", "authors": ["Yangtao Deng", "Lei Zhang", "Qinlong Wang", "Xiaoyun Zhi", "Xinlei Zhang", "Zhuo Jiang", "Haohan Xu", "Lei Wang", "Zuquan Song", "Gaohong Liu", "Yang Bai", "Shuguang Wang", "Wencong Xiao", "Jianxi Ye", "Minlan Yu", "Hong Xu"], "title": "Mycroft: Tracing Dependencies in Collective Communication Towards Reliable LLM Training", "comment": null, "summary": "Reliability is essential for ensuring efficiency in LLM training. However,\nmany real-world reliability issues remain difficult to resolve, resulting in\nwasted resources and degraded model performance. Unfortunately, today's\ncollective communication libraries operate as black boxes, hiding critical\ninformation needed for effective root cause analysis. We propose Mycroft, a\nlightweight distributed tracing and root cause analysis system designed to\naddress previously hidden reliability issues in collective communication.\nMycroft's key idea is to trace collective communication states and leverage\ninternal control and data dependencies to resolve reliability problems in LLM\ntraining. Mycroft has been deployed at ByteDance for over six months to debug\ncollective communication related issues at runtime. It detected anomalies\nwithin 15 seconds in 90% of cases and identified the root cause within 20\nseconds in 60% of cases. We also conducted extensive fault injection\nexperiments to demonstrate Mycroft's capability and efficiency."}
{"id": "2509.03201", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.03201", "abs": "https://arxiv.org/abs/2509.03201", "authors": ["Abdul Rahoof", "Vivek Chaturvedi", "Mahesh Raveendranatha Panicker", "Muhammad Shafique"], "title": "CapsBeam: Accelerating Capsule Network based Beamformer for Ultrasound Non-Steered Plane Wave Imaging on Field Programmable Gate Array", "comment": null, "summary": "In recent years, there has been a growing trend in accelerating\ncomputationally complex non-real-time beamforming algorithms in ultrasound\nimaging using deep learning models. However, due to the large size and\ncomplexity these state-of-the-art deep learning techniques poses significant\nchallenges when deploying on resource-constrained edge devices. In this work,\nwe propose a novel capsule network based beamformer called CapsBeam, designed\nto operate on raw radio-frequency data and provide an envelope of beamformed\ndata through non-steered plane wave insonification. Experiments on in-vivo\ndata, CapsBeam reduced artifacts compared to the standard Delay-and-Sum (DAS)\nbeamforming. For in-vitro data, CapsBeam demonstrated a 32.31% increase in\ncontrast, along with gains of 16.54% and 6.7% in axial and lateral resolution\ncompared to the DAS. Similarly, in-silico data showed a 26% enhancement in\ncontrast, along with improvements of 13.6% and 21.5% in axial and lateral\nresolution, respectively, compared to the DAS. To reduce the parameter\nredundancy and enhance the computational efficiency, we pruned the model using\nour multi-layer LookAhead Kernel Pruning (LAKP-ML) methodology, achieving a\ncompression ratio of 85% without affecting the image quality. Additionally, the\nhardware complexity of the proposed model is reduced by applying quantization,\nsimplification of non-linear operations, and parallelizing operations. Finally,\nwe proposed a specialized accelerator architecture for the pruned and optimized\nCapsBeam model, implemented on a Xilinx ZU7EV FPGA. The proposed accelerator\nachieved a throughput of 30 GOPS for the convolution operation and 17.4 GOPS\nfor the dynamic routing operation."}
{"id": "2509.03047", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03047", "abs": "https://arxiv.org/abs/2509.03047", "authors": ["Haijun Zhang", "Jinxiang Wang", "Zhenhua Yu", "Yanyong Zhang", "Xuejie Ji", "Kaining Mao", "Jun Zhang", "Yaqing Zhang", "Ting Wu", "Fei Jie", "Xiemin Huang", "Zhifang Cai", "Junhua Cheng", "Shuwei Wang", "Wei Li", "Xiaoming Bao", "Hua Xu", "Shixiong Zhao", "Jun Li", "Hongwei Sun", "Ziyang Zhang", "Yi Xiong", "Chunsheng Li"], "title": "FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale Training of LLMs", "comment": null, "summary": "Large language models (LLMs) have made a profound impact across various\nfields due to their advanced capabilities. However, training these models at\nunprecedented scales requires extensive AI accelerator clusters and\nsophisticated parallelism strategies, which pose significant challenges in\nmaintaining system reliability over prolonged training periods. A major concern\nis the substantial loss of training time caused by inevitable hardware and\nsoftware failures. To address these challenges, we present FlashRecovery, a\nfast and low-cost failure recovery system comprising three core modules: (1)\nActive and real-time failure detection. This module performs continuous\ntraining state monitoring, enabling immediate identification of hardware and\nsoftware failures within seconds, thus ensuring rapid incident response; (2)\nScale-independent task restart. By employing different recovery strategies for\nnormal and faulty nodes, combined with an optimized communication group\nreconstruction protocol, our approach ensures that the recovery time remains\nnearly constant, regardless of cluster scale; (3) Checkpoint-free recovery\nwithin one step. Our novel recovery mechanism enables single-step restoration,\ncompletely eliminating dependence on traditional checkpointing methods and\ntheir associated overhead. Collectively, these innovations enable FlashRecovery\nto achieve optimal Recovery Time Objective (RTO) and Recovery Point Objective\n(RPO), substantially improving the reliability and efficiency of long-duration\nLLM training. Experimental results demonstrate that FlashRecovery system can\nachieve training restoration on training cluster with 4, 800 devices in 150\nseconds. We also verify that the time required for failure recovery is nearly\nconsistent for different scales of training tasks."}
{"id": "2509.03377", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.03377", "abs": "https://arxiv.org/abs/2509.03377", "authors": ["Rui Xie", "Asad Ul Haq", "Linsen Ma", "Yunhua Fang", "Zirak Burzin Engineer", "Liu Liu", "Tong Zhang"], "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via Transparent Near-Data Processing", "comment": null, "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."}
{"id": "2509.03104", "categories": ["cs.DC", "68", "D.4.8"], "pdf": "https://arxiv.org/pdf/2509.03104", "abs": "https://arxiv.org/abs/2509.03104", "authors": ["Leonid Kondrashov", "Boxi Zhou", "Hancheng Wang", "Dmitrii Ustiugov"], "title": "The High Cost of Keeping Warm: Characterizing Overhead in Serverless Autoscaling Policies", "comment": null, "summary": "Serverless computing is transforming cloud application development, but the\nperformance-cost trade-offs of control plane designs remain poorly understood\ndue to a lack of open, cross-platform benchmarks and detailed system analyses.\nIn this work, we address these gaps by designing a serverless system that\napproximates the scaling behaviors of commercial providers, including AWS\nLambda and Google Cloud Run. We systematically compare the performance and\ncost-efficiency of both synchronous and asynchronous autoscaling policies by\nreplaying real-world workloads and varying key autoscaling parameters.\n  We demonstrate that our open-source systems can closely replicate the\noperational characteristics of commercial platforms, enabling reproducible and\ntransparent experimentation. By evaluating how autoscaling parameters affect\nlatency, memory usage, and CPU overhead, we reveal several key findings. First,\nwe find that serverless systems exhibit significant computational overhead due\nto instance churn equivalent to 10-40% of the CPU cycles spent on request\nhandling, primarily originating from worker nodes. Second, we observe high\nmemory allocation due to scaling policy: 2-10 times more than actively used.\nFinally, we demonstrate that reducing these overheads typically results in\nsignificant performance degradation in the current systems, underscoring the\nneed for new, cost-efficient autoscaling strategies. Additionally, we employ a\nhybrid methodology that combines real control plane deployments with\nlarge-scale simulation to extend our evaluation closer to a production scale,\nthereby bridging the gap between small research clusters and real-world\nenvironments."}
{"id": "2509.03145", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.03145", "abs": "https://arxiv.org/abs/2509.03145", "authors": ["Pengkun Ren", "Hai Dong", "Zahir Tari", "Pengcheng Zhang"], "title": "Efficient and Secure Sleepy Model for BFT Consensus", "comment": "Accepted to ESORICS 2025, 20 pages, 7 figures", "summary": "Byzantine Fault Tolerant (BFT) consensus protocols for dynamically available\nsystems face a critical challenge: balancing latency and security in\nfluctuating node participation. Existing solutions often require multiple\nrounds of voting per decision, leading to high latency or limited resilience to\nadversarial behavior. This paper presents a BFT protocol integrating a\npre-commit mechanism with publicly verifiable secret sharing (PVSS) into\nmessage transmission. By binding users' identities to their messages through\nPVSS, our approach reduces communication rounds. Compared to other\nstate-of-the-art methods, our protocol typically requires only four network\ndelays (4$\\Delta$) in common scenarios while being resilient to up to 1/2\nadversarial participants. This integration enhances the efficiency and security\nof the protocol without compromising integrity. Theoretical analysis\ndemonstrates the robustness of the protocol against Byzantine attacks.\nExperimental evaluations show that, compared to traditional BFT protocols, our\nprotocol significantly prevents fork occurrences and improves chain stability.\nFurthermore, compared to longest-chain protocol, our protocol maintains\nstability and lower latency in scenarios with moderate participation\nfluctuations."}
{"id": "2509.03394", "categories": ["cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.03394", "abs": "https://arxiv.org/abs/2509.03394", "authors": ["Amirhossein Shahbazinia", "Darong Huang", "Luis Costero", "David Atienza"], "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds with Unknown Workload", "comment": null, "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."}
