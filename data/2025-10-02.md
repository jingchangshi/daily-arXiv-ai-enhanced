<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 12]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Lattica: A Decentralized Cross-NAT Communication Framework for Scalable AI Inference and Training](https://arxiv.org/abs/2510.00183)
*Ween Yang,Jason Liu,Suli Wang,Xinyuan Song,Lynn Ai,Eric Yang,Tianyu Shi*

Main category: cs.DC

TL;DR: Lattica是一个去中心化的跨NAT通信框架，专门为分布式AI系统设计，通过NAT穿越、CRDT数据存储和DHT内容发现三个核心组件，提供主权、弹性和可扩展的AI系统协议栈。


<details>
  <summary>Details</summary>
Motivation: 分布式AI工作负载扩展到集中式数据中心之外，需要在异构和无许可环境中可靠运行的通信基础架构，现有解决方案要么设计用于受控数据中心部署，要么是紧耦合的单一系统。

Method: 集成三个核心组件：1) 强大的NAT穿越机制建立全局可寻址的P2P网络；2) 基于CRDT的去中心化数据存储确保可验证和最终一致的状态复制；3) 利用DHT和优化RPC协议的内容发现层实现高效模型同步。

Result: Lattica提供了一个完整的协议栈，支持主权、弹性和可扩展的AI系统，无需依赖集中式中介，适用于边缘智能、协作强化学习等大规模分布式机器学习场景。

Conclusion: Lattica通过去中心化架构成功解决了分布式AI系统在异构环境中的通信挑战，为边缘计算和分布式机器学习提供了有效的解决方案。

Abstract: The rapid expansion of distributed Artificial Intelligence (AI) workloads
beyond centralized data centers creates a demand for new communication
substrates. These substrates must operate reliably in heterogeneous and
permissionless environments, where Network Address Translators (NATs) and
firewalls impose significant constraints. Existing solutions, however, are
either designed for controlled data center deployments or implemented as
monolithic systems that tightly couple machine learning logic with networking
code. To address these limitations, we present Lattica, a decentralized
cross-NAT communication framework designed to support distributed AI systems.
Lattica integrates three core components. First, it employs a robust suite of
NAT traversal mechanisms to establish a globally addressable peer-to-peer mesh.
Second, it provides a decentralized data store based on Conflict-free
Replicated Data Types (CRDTs), ensuring verifiable and eventually consistent
state replication. Third, it incorporates a content discovery layer that
leverages distributed hash tables (DHTs) together with an optimized RPC
protocol for efficient model synchronization. By integrating these components,
Lattica delivers a complete protocol stack for sovereign, resilient, and
scalable AI systems that operate independently of centralized intermediaries.
It is directly applicable to edge intelligence, collaborative reinforcement
learning, and other large-scale distributed machine learning scenarios.

</details>


### [2] [FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed Mixture-of-Experts Training](https://arxiv.org/abs/2510.00207)
*Yunqi Gao,Bing Hu,Mahdi Boloursaz Mashhadi,A-Long Jin,Yanfeng Zhang,Pei Xiao,Rahim Tafazolli,Merouane Debbah*

Main category: cs.DC

TL;DR: FlowMoE是一个用于稀疏激活混合专家模型训练的可扩展调度框架，通过统一流水线和张量分块优先级调度机制，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE训练方法主要关注专家计算和all-to-all通信的调度，但忽略了多头注意力计算、门控和all-reduce通信等其他关键操作，导致训练效率不高。

Method: 1. 构建统一流水线来一致调度MHA计算、门控、专家计算和A2A通信；2. 引入基于张量分块的优先级调度机制，将all-reduce通信与所有计算任务重叠。

Result: 在675个典型MoE层和4个真实MoE模型上的实验表明，FlowMoE相比现有最优框架减少训练时间13%-57%，能耗10%-39%，内存使用7%-32%。

Conclusion: FlowMoE作为一个自适应通用框架，能够有效提升MoE模型的分布式训练效率，在多个关键指标上都有显著改进。

Abstract: The parameter size of modern large language models (LLMs) can be scaled up
via the sparsely-activated Mixture-of-Experts (MoE) technique to avoid
excessive increase of the computational costs. To further improve training
efficiency, pipelining computation and communication has become a promising
solution for distributed MoE training. However, existing work primarily focuses
on scheduling tasks within the MoE layer, such as expert computing and
all-to-all (A2A) communication, while neglecting other key operations including
multi-head attention (MHA) computing, gating, and all-reduce communication. In
this paper, we propose FlowMoE, a scalable framework for scheduling multi-type
task pipelines. First, FlowMoE constructs a unified pipeline to consistently
scheduling MHA computing, gating, expert computing, and A2A communication.
Second, FlowMoE introduces a tensor chunk-based priority scheduling mechanism
to overlap the all-reduce communication with all computing tasks. We implement
FlowMoE as an adaptive and generic framework atop PyTorch. Extensive
experiments with 675 typical MoE layers and four real-world MoE models across
two GPU clusters demonstrate that our proposed FlowMoE framework outperforms
state-of-the-art MoE training frameworks, reducing training time by 13%-57%,
energy consumption by 10%-39%, and memory usage by 7%-32%.

</details>


### [3] [BlockSDN-VC: A SDN-Based Virtual Coordinate-Enhanced Transaction Broadcast Framework for High-Performance Blockchains](https://arxiv.org/abs/2510.00306)
*Wenyang Jia,Jingjing Wang,Kai Lei*

Main category: cs.DC

TL;DR: BlockSDN-VC是一种基于SDN控制器的交易广播协议，通过集中化坐标计算和转发控制，显著降低区块链传播延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代区块链需要快速可靠的传播来平衡安全性和吞吐量。现有的虚拟坐标方法依赖缓慢的迭代更新，导致节点不同步。

Method: 将坐标计算和转发控制集中到SDN控制器中，实现全局一致性、最小路径拉伸，并能快速响应节点变动或拥塞。

Result: 在地理分布式模拟中，BlockSDN-VC将中位延迟降低高达62%，收敛速度提高4倍，控制平面开销低于3%。在实际区块链环境中，在对抗性工作负载下将确认交易吞吐量提高17%。

Conclusion: BlockSDN-VC无需修改现有客户端即可显著提升区块链性能，提供了高效可靠的交易传播解决方案。

Abstract: Modern blockchains need fast, reliable propagation to balance security and
throughput. Virtual-coordinate methods speed dissemination but rely on slow
iterative updates, leaving nodes out of sync. We present BlockSDN-VC, a
transaction-broadcast protocol that centralises coordinate computation and
forwarding control in an SDN controller, delivering global consistency, minimal
path stretch and rapid response to churn or congestion. In geo-distributed
simulations, BlockSDN-VC cuts median latency by up to 62% and accelerates
convergence fourfold over state-of-the-art schemes with under 3% control-plane
overhead. In a real blockchain environment, BlockSDN-VC boosts
confirmed-transaction throughput by 17% under adversarial workloads, requiring
no modifications to existing clients.

</details>


### [4] [ThirstyFLOPS: Water Footprint Modeling and Analysis Toward Sustainable HPC Systems](https://arxiv.org/abs/2510.00471)
*Yankai Jiang,Raghavendra Kanakagiri,Rohan Basu Roy,Devesh Tiwari*

Main category: cs.DC

TL;DR: 提出了ThirstyFLOPS框架，用于分析高性能计算系统的水足迹，结合区域特定指标评估水消耗，并以四个代表性HPC系统为例提供管理启示。


<details>
  <summary>Details</summary>
Motivation: 高性能计算系统因依赖水冷和发电能耗而日益耗水，但水足迹研究相对不足，与碳排关注度形成对比。

Method: 开发ThirstyFLOPS框架，整合区域特定指标（水利用效率、功率利用效率、能源水因子），使用真实数据量化水消耗。

Result: 以Marconi、Fugaku、Polaris和Frontier四个HPC系统为例，分析了区域水资源短缺和核能策略对可持续性的影响。

Conclusion: 研究旨在推动开发具有水意识、环境友好的计算基础设施。

Abstract: High-performance computing (HPC) systems are becoming increasingly
water-intensive due to their reliance on water-based cooling and the energy
used in power generation. However, the water footprint of HPC remains
relatively underexplored-especially in contrast to the growing focus on carbon
emissions. In this paper, we present ThirstyFLOPS - a comprehensive water
footprint analysis framework for HPC systems. Our approach incorporates
region-specific metrics, including Water Usage Effectiveness, Power Usage
Effectiveness, and Energy Water Factor, to quantify water consumption using
real-world data. Using four representative HPC systems - Marconi, Fugaku,
Polaris, and Frontier - as examples, we provide implications for HPC system
planning and management. We explore the impact of regional water scarcity and
nuclear-based energy strategies on HPC sustainability. Our findings aim to
advance the development of water-aware, environmentally responsible computing
infrastructures.

</details>


### [5] [Towards Efficient VM Placement: A Two-Stage ACO-PSO Approach for Green Cloud Infrastructure](https://arxiv.org/abs/2510.00541)
*Ali M. Baydoun,Ahmed S. Zekri*

Main category: cs.DC

TL;DR: 提出了一种混合ACO-PSO算法(HAPSO)，用于绿色云数据中心中节能的虚拟机放置和迁移，通过两阶段优化显著降低了能耗和SLA违规。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗持续增长，需要可持续的资源管理方法来解决虚拟机放置和迁移的节能问题。

Method: 采用两阶段混合方法：第一阶段使用蚁群优化(ACO)进行节能的初始虚拟机放置；第二阶段使用离散粒子群优化(PSO)通过迁移过载或利用率不足主机上的虚拟机来优化分配。

Result: 在CloudSimPlus中的仿真显示，HAPSO相比传统启发式算法(BFD、FFD)、统一蚁群系统(UACS)和纯ACO表现更优，能耗降低达25%，SLA违规减少18%。

Conclusion: 两阶段生物启发式混合方法能有效应对云资源管理的动态和多目标特性，在保持成本和碳排放稳定的同时显著提升能效。

Abstract: Datacenters consume a growing share of energy, prompting the need for
sustainable resource management. This paper presents a Hybrid ACO-PSO (HAPSO)
algorithm for energy-aware virtual machine (VM) placement and migration in
green cloud datacenters. In the first stage, Ant Colony Optimization (ACO)
performs energy-efficient initial placement across physical hosts, ensuring
global feasibility. In the second stage, a discrete Particle Swarm Optimization
(PSO) refines allocations by migrating VMs from overloaded or underutilized
hosts. HAPSO introduces several innovations: sequential hybridization of
metaheuristics, system-informed particle initialization using ACO output,
heuristic-guided discretization for constraint handling, and a multi-objective
fitness function that minimizes active servers and resource wastage.
Implemented in CloudSimPlus, extensive simulations demonstrate that HAPSO
consistently outperforms classical heuristics (BFD, FFD), Unified Ant Colony
System (UACS), and ACO-only. Notably, HAPSO achieves up to 25% lower energy
consumption and 18% fewer SLA violations compared to UACS at large-scale
workloads, while sustaining stable cost and carbon emissions. These results
highlight the effectiveness of two-stage bio-inspired hybridization in
addressing the dynamic and multi-objective nature of cloud resource management.

</details>


### [6] [ElasWave: An Elastic-Native System for Scalable Hybrid-Parallel Training](https://arxiv.org/abs/2510.00606)
*Xueze Kang,Guangyu Xiang,Yuxin Wang,Hao Zhang,Yuchu Fang,Yuhang Zhou,Zhenheng Tang,Youhui Lv,Eliran Maman,Mark Wasserman,Alon Zameret,Zhipeng Bian,Shushu Chen,Zhiyou Yu,Jin Wang,Xiaoyu Wu,Yang Zheng,Chen Tian,Xiaowen Chu*

Main category: cs.DC

TL;DR: ElasWave是一个弹性原生的大规模LLM预训练系统，通过多维调度实现故障容错，在96个NPU上相比现有方法提升吞吐量1.35-1.60倍，恢复时间减少51%，收敛偏差降低78%。


<details>
  <summary>Details</summary>
Motivation: 大规模LLM预训练涉及10^5-10^6个加速器，故障频发且弹性不再可选。现有系统无法同时满足参数一致性、低恢复时间、高吞吐量和计算一致性的要求。

Method: 采用多维调度（图、数据流、频率、随机数生成），实现每步故障容错。通过在线流水线重分片、异步参数迁移、ZeRO分区交错、DVFS吸收流水线气泡、RNG重分片等技术。

Result: 在96个NPU上测试：吞吐量比ReCycle提升1.35倍，比TorchFT提升1.60倍；通信器恢复在1秒内完成（比完全/部分重建快82倍/3.6倍）；迁移恢复时间减少51%；收敛偏差降低约78%。

Conclusion: ElasWave成功实现了弹性原生训练系统的四个关键目标，为大规模LLM预训练提供了高效可靠的弹性解决方案。

Abstract: Large-scale LLM pretraining today spans $10^{5}$--$10^{6}$ accelerators,
making failures commonplace and elasticity no longer optional. We posit that an
elastic-native training system must simultaneously ensure (i) Parameter
Consistency, (ii) low Mean Time to Recovery (MTTR), (iii) high post-change
Throughput, and (iv) Computation Consistency. This objective set not has never
been jointly attained by prior work. To achieve these goals, we present
ElasWave, which provides per-step fault tolerance via multi-dimensional
scheduling across Graph, Dataflow, Frequency, and Random Number Generation.
ElasWave resizes and reshards micro-batch workloads while preserving the global
batch size and gradient scale; it performs online pipeline resharding with
asynchronous parameter migration, interleaving ZeRO partitions so recovery
reduces to disjoint rank-to-rank transfers. It further uses DVFS to absorb
pipeline bubbles and reshards RNG to keep consistent computations. A dynamic
communicator enables in-place communication group edits, while per-step
in-memory snapshots support online verification and redistribution. We
evaluated ElasWave on 96 NPUs and benchmarked against state-of-the-art
baselines: throughput improves by $1.35\times$ over ReCycle and $1.60\times$
over TorchFT; communicator recovery completes within one second (up to
$82\times/3.6\times$ faster than full/partial rebuilds); migration MTTR drops
by as much as $51\%$; and convergence deviation is reduced by approximately
$78\%$.

</details>


### [7] [Net-Zero 6G from Earth to Orbit: Sustainable Design of Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2510.00678)
*Muhammad Ali Jamshed,Malik Muhammad Saad,Muhammad Ahmed Mohsin,Dongkyun Kim,Octavia A. Dobre,Halim Yanikomeroglu,Lina Mohjazi*

Main category: cs.DC

TL;DR: 本文首次全面概述了在集成地面网络和非地面网络系统中实现净零能耗目标的设计挑战，提出了支持此类网络能源需求的关键使能技术，并通过AI用例分析提升能源效率，最后指出了可持续演进的未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 地面网络和非地面网络的集成对于弥合数字鸿沟和实现6G及更先进通信的泛在连接至关重要，但这种集成由于系统特性和运行环境的多样性带来了显著的能源挑战。

Method: 提出了支持净零能耗目标的关键使能技术，并利用人工智能进行用例分析，为不同部署场景提供适应性解决方案。

Result: 提供了集成TN和NTN系统能源效率提升的全面框架，包括技术路线和AI驱动的优化方法。

Conclusion: 指出了集成TN和NTN系统可持续演进的未来研究方向，为实现净零能耗目标提供了指导性建议。

Abstract: The integration of Terrestrial Networks (TN) and Non-Terrestrial Networks
(NTN) plays a crucial role in bridging the digital divide and enabling Sixth
Generation (6G) and beyond to achieve truly ubiquitous connectivity. However,
combining TN and NTN introduces significant energy challenges due to the
diverse characteristics and operational environments of these systems. In this
paper, we present for the first time a comprehensive overview of the design
challenges associated with achieving Net-Zero energy targets in integrated TN
and NTN systems. We outline a set of key enabling technologies that can support
the energy demands of such networks while aligning with Net-Zero objectives. To
enhance the Energy Efficiency (EE) of integrated TN and NTN systems, we provide
a use case analysis that leverages Artificial Intelligence (AI) to deliver
adaptable solutions across diverse deployment scenarios. Finally, we highlight
promising research directions that can guide the sustainable evolution of
integrated TN and NTN.

</details>


### [8] [Decentralized and Self-adaptive Core Maintenance on Temporal Graphs](https://arxiv.org/abs/2510.00758)
*Davide Rucci,Emanuele Carlini,Patrizio Dazzi,Hanna Kavalionak,Matteo Mordacchini*

Main category: cs.DC

TL;DR: 提出了一种新颖的去中心化增量算法，用于计算时序网络的核分解，通过利用先前计算的核度值来减少节点激活和消息交换量，在精度损失最小的情况下实现可扩展性。


<details>
  <summary>Details</summary>
Motivation: 图基问题在理解网络拓扑和发现同质及时序数据中的相似模式方面起着核心作用，这些模式可以通过分析节点形成的社区来揭示，而时序k-核可以有效建模这些社区。去中心化解决方案利用网络节点本地通信和协调能力，以可扩展、自适应和及时的方式解决复杂问题。

Method: 开发了一种去中心化增量算法，利用先前计算的核度值，在网络随时间变化时显著减少节点激活和消息交换量。

Result: 在大型真实世界网络上进行实验评估，结果表明该解决方案在节点激活、通信开销和收敛速度方面优于现有最优方法，特别是在不同动态水平下表现出高效性。

Conclusion: 该方法通过最小化精度损失实现了可扩展的时序网络核分解计算，为大规模动态网络分析提供了有效的解决方案。

Abstract: Key graph-based problems play a central role in understanding network
topology and uncovering patterns of similarity in homogeneous and temporal
data. Such patterns can be revealed by analyzing communities formed by nodes,
which in turn can be effectively modeled through temporal $k$-cores. This paper
introduces a novel decentralized and incremental algorithm for computing the
core decomposition of temporal networks. Decentralized solutions leverage the
ability of network nodes to communicate and coordinate locally, addressing
complex problems in a scalable, adaptive, and timely manner. By leveraging
previously computed coreness values, our approach significantly reduces the
activation of nodes and the volume of message exchanges when the network
changes over time. This enables scalability with only a minimal trade-off in
precision. Experimental evaluations on large real-world networks under varying
levels of dynamism demonstrate the efficiency of our solution compared to a
state-of-the-art approach, particularly in terms of active nodes, communication
overhead, and convergence speed.

</details>


### [9] [CGSim: A Simulation Framework for Large Scale Distributed Computing Environment](https://arxiv.org/abs/2510.00822)
*Sairam Sri Vatsavai,Raees Khan,Kuan-Chieh Hsu,Ozgur O. Kilic,Paul Nilsson,Tatiana Korchuganova,David K. Park,Sankha Dutta,Yihui Ren,Joseph Boudreau,Tasnuva Chowdhury,Shengyu Feng,Jaehyung Kim,Scott Klasky,Tadashi Maeno,Verena Ingrid Martinez,Norbert Podhorszki,Frédéric Suter,Wei Yang,Yiming Yang,Shinjae Yoo,Alexei Klimentov,Adolfy Hoisie*

Main category: cs.DC

TL;DR: CGSim是一个用于大规模分布式计算环境的仿真框架，基于SimGrid构建，解决了现有仿真工具在可扩展性、算法灵活性、实时监控和机器学习数据集生成方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模分布式计算基础设施仿真工具存在可扩展性有限、算法固化、缺乏实时监控以及无法生成适合现代机器学习方法的数据集等问题。

Method: 基于经过验证的SimGrid仿真框架，CGSim提供高层抽象来建模异构网格环境，同时保持准确性和可扩展性。关键特性包括模块化插件机制、交互式实时可视化仪表板以及自动生成事件级数据集。

Result: 使用生产ATLAS PanDA工作负载进行的全面评估显示，在WLCG计算站点上实现了显著的校准精度改进。可扩展性实验显示多站点仿真具有接近线性的扩展性，分布式工作负载相比单站点执行性能提升6倍。

Conclusion: 该框架使研究人员能够在商品硬件上，在实用时间预算约束内，仿真具有数百个站点和数千个并发作业的WLCG规模基础设施。

Abstract: Large-scale distributed computing infrastructures such as the Worldwide LHC
Computing Grid (WLCG) require comprehensive simulation tools for evaluating
performance, testing new algorithms, and optimizing resource allocation
strategies. However, existing simulators suffer from limited scalability,
hardwired algorithms, lack of real-time monitoring, and inability to generate
datasets suitable for modern machine learning approaches. We present CGSim, a
simulation framework for large-scale distributed computing environments that
addresses these limitations. Built upon the validated SimGrid simulation
framework, CGSim provides high-level abstractions for modeling heterogeneous
grid environments while maintaining accuracy and scalability. Key features
include a modular plugin mechanism for testing custom workflow scheduling and
data movement policies, interactive real-time visualization dashboards, and
automatic generation of event-level datasets suitable for AI-assisted
performance modeling. We demonstrate CGSim's capabilities through a
comprehensive evaluation using production ATLAS PanDA workloads, showing
significant calibration accuracy improvements across WLCG computing sites.
Scalability experiments show near-linear scaling for multi-site simulations,
with distributed workloads achieving 6x better performance compared to
single-site execution. The framework enables researchers to simulate WLCG-scale
infrastructures with hundreds of sites and thousands of concurrent jobs within
practical time budget constraints on commodity hardware.

</details>


### [10] [Data Management System Analysis for Distributed Computing Workloads](https://arxiv.org/abs/2510.00828)
*Kuan-Chieh Hsu,Sairam Sri Vatsavai,Ozgur O. Kilic,Tatiana Korchuganova,Paul Nilsson,Sankha Dutta,Yihui Ren,David K. Park,Joseph Boudreau,Tasnuva Chowdhury,Shengyu Feng,Raees Khan,Jaehyung Kim,Scott Klasky,Tadashi Maeno,Verena Ingrid Martinez Outschoorn,Norbert Podhorszki,Frédéric Suter,Wei Yang,Yiming Yang,Shinjae Yoo,Alexei Klimentov,Adolfy Hoisie*

Main category: cs.DC

TL;DR: 本文分析了ATLAS实验中PanDA工作流系统和Rucio数据管理系统在协同运行时出现的系统效率问题，提出了通过元数据匹配算法连接两个系统，识别异常传输模式，并制定改进策略。


<details>
  <summary>Details</summary>
Motivation: ATLAS实验中的PanDA工作流系统和Rucio数据管理系统各自优化但协同运行时存在系统效率低下问题，包括资源利用不足、冗余传输和错误分布改变等，需要端到端的改进。

Method: 开发元数据匹配算法，在文件级别连接PanDA作业和Rucio数据集，获得完整细粒度的数据访问和移动视图，识别违反PanDA数据中心作业分配原则的异常传输模式。

Result: 识别出空间和时间上不平衡的传输活动，发现了违反数据中心作业分配原则的异常传输模式，为系统优化提供了数据基础。

Conclusion: 提出了缓解这些模式的策略，并强调了加强PanDA-Rucio协调的机会，以改善资源利用、减少不必要的数据移动并增强整体系统弹性。

Abstract: Large-scale international collaborations such as ATLAS rely on globally
distributed workflows and data management to process, move, and store vast
volumes of data. ATLAS's Production and Distributed Analysis (PanDA) workflow
system and the Rucio data management system are each highly optimized for their
respective design goals. However, operating them together at global scale
exposes systemic inefficiencies, including underutilized resources, redundant
or unnecessary transfers, and altered error distributions. Moreover, PanDA and
Rucio currently lack shared performance awareness and coordinated, adaptive
strategies.
  This work charts a path toward co-optimizing the two systems by diagnosing
data-management pitfalls and prioritizing end-to-end improvements. With the
observation of spatially and temporally imbalanced transfer activities, we
develop a metadata-matching algorithm that links PanDA jobs and Rucio datasets
at the file level, yielding a complete, fine-grained view of data access and
movement. Using this linkage, we identify anomalous transfer patterns that
violate PanDA's data-centric job-allocation principle. We then outline
mitigation strategies for these patterns and highlight opportunities for
tighter PanDA-Rucio coordination to improve resource utilization, reduce
unnecessary data movement, and enhance overall system resilience.

</details>


### [11] [Towards Verifiable Federated Unlearning: Framework, Challenges, and The Road Ahead](https://arxiv.org/abs/2510.00833)
*Thanh Linh Nguyen,Marcela Tuler de Oliveira,An Braeken,Aaron Yi Ding,Quoc-Viet Pham*

Main category: cs.DC

TL;DR: 本文提出了veriFUL框架，用于解决联邦学习中数据遗忘的可验证性问题，确保客户数据影响能被可靠地验证移除。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习遗忘方法缺乏可靠的验证机制，客户无法确认其数据影响是否真正被移除，这削弱了隐私保护承诺。

Method: 提出veriFUL参考框架，形式化验证实体、目标、方法和指标，整合现有工作并贡献新见解和概念。

Result: 建立了可验证联邦遗忘的理论框架，明确了验证需求和实现路径。

Conclusion: 可验证遗忘应成为联邦学习生命周期的重要组成部分，特别是在医疗等高度监管领域具有重要应用价值。

Abstract: Federated unlearning (FUL) enables removing the data influence from the model
trained across distributed clients, upholding the right to be forgotten as
mandated by privacy regulations. FUL facilitates a value exchange where clients
gain privacy-preserving control over their data contributions, while service
providers leverage decentralized computing and data freshness. However, this
entire proposition is undermined because clients have no reliable way to verify
that their data influence has been provably removed, as current metrics and
simple notifications offer insufficient assurance. We envision unlearning
verification becoming a pivotal and trust-by-design part of the FUL life-cycle
development, essential for highly regulated and data-sensitive services and
applications like healthcare. This article introduces veriFUL, a reference
framework for verifiable FUL that formalizes verification entities, goals,
approaches, and metrics. Specifically, we consolidate existing efforts and
contribute new insights, concepts, and metrics to this domain. Finally, we
highlight research challenges and identify potential applications and
developments for verifiable FUL and veriFUL.

</details>


### [12] [An Efficient, Reliable and Observable Collective Communication Library in Large-scale GPU Training Clusters](https://arxiv.org/abs/2510.00991)
*Ziteng Chen,Xiaohe Hu,Menghao Zhang,Yanmin Jia,Yan Zhang,Mingjun Zhang,Da Liu,Fangzheng Jiao,Jun Chen,He Liu,Aohan Zeng,Shuaixing Duan,Ruya Gu,Yang Jing,Bowen Han,Jiahao Cao,Wei Chen,Wenqi Xie,Jinlong Hou,Yuan Cheng,Bohua Xu,Mingwei Xu,Chunming Hu*

Main category: cs.DC

TL;DR: ICCL是一个针对大规模LLM训练的高效、可靠、可观测的集体通信库，解决了NCCL在生产环境中遇到的P2P通信效率低、RNIC端口故障容忍差和通信异常观测不足等问题。


<details>
  <summary>Details</summary>
Motivation: 在大规模GPU训练集群中使用NCCL时面临三个主要挑战：1）P2P通信效率低且复杂；2）对频繁的RNIC端口故障容忍性差；3）对瞬态集体通信异常的可观测性不足。

Method: ICCL将P2P通信从GPU内核卸载到CPU线程以减少SM消耗，消除与通信过程无关的冗余内存拷贝，引入主备QP机制容忍NIC端口故障，并设计基于窗口的监控器在微秒级别观测网络异常。

Result: 与NCCL相比，ICCL在P2P吞吐量/延迟方面分别提升23.4%/28.5%，训练吞吐量提升6.02%，已在生产训练集群中部署数月。

Conclusion: ICCL有效解决了大规模LLM训练中集体通信的关键问题，并分享了在大规模集群中的运维经验，为社区提供了生产级集体通信库的实践洞察。

Abstract: Large-scale LLM training requires collective communication libraries to
exchange data among distributed GPUs. As a company dedicated to building and
operating large-scale GPU training clusters, we encounter several challenges
when using NCCL in production, including 1) limited efficiency with costly and
cumbersome P2P communication, 2) poor tolerance to frequent RNIC port failures,
and 3) insufficient observability of transient collective communication
anomalies. To address these issues, we propose ICCL, an efficient, reliable,
and observable collective communication library in large-scale GPU training
clusters. ICCL offloads the P2P communication from GPU kernels to CPU threads
for minimal SM consumption, and removes the redundant memory copies irrelevant
to the actual communicating process. ICCL also introduces a primary-backup QP
mechanism to tolerate frequent NIC port failures, and designs a window-based
monitor to observe network anomalies at O(us) level. We open-source ICCL and
deploy it in production training clusters for several months, with results
showing that compared to NCCL, ICCL achieves a 23.4%/28.5% improvement in P2P
throughput/latency as well as a 6.02% increase in training throughput. We also
share the operating experience of ICCL in large-scale clusters, hoping to give
the communities more insights on production-level collective communication
libraries in LLM training.

</details>
