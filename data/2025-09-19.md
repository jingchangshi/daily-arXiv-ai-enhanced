<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [DeliverC: Teaching Pointers through GenAI-Powered Game-Based Learning](https://arxiv.org/abs/2509.14496)
*Wyatt Petula,Anushcka Joshi,Peggy Tu,Amrutha Somasundar,Suman Saha*

Main category: cs.PL

TL;DR: DeliverC是一个集成GPT-4-mini的GenAI增强游戏，为C语言指针学习提供实时个性化提示和挑战生成。试点研究表明该系统能提升学生自信和反思能力，但AI生成反馈仍需改进。


<details>
  <summary>Details</summary>
Motivation: 解决编程教育中复杂主题（如C指针）缺乏实时自适应支持工具的问题，探索GenAI与游戏化学习结合在传统挑战性编程领域的潜力。

Method: 开发DeliverC游戏，集成GPT-4-mini提供个性化提示和实时生成指针相关挑战。通过25名本科生的试点研究，收集游戏数据和15项问卷调查（涵盖动机、自我效能、元认知和反馈质量等构念）。

Result: 大多数学生使用后感到更自信和善于反思，错误率随着脚手架式关卡进展而下降。但参与度随任务难度增加而降低，部分学生反馈AI生成的提示不够清晰。

Conclusion: DeliverC能够增强系统编程学习的参与度和理解，但需要改进AI生成反馈的质量。研究证明了GenAI与游戏化学习结合在支持个性化交互式编程实践方面的潜力。

Abstract: While game-based learning is widely used in programming education, few tools
offer adaptive, real-time support for complex topics, such as C pointers. We
present DeliverC, a GenAI-enhanced game that integrates GPT-4-mini to provide
personalized hints and generate pointer-related challenges on the fly. In a
pilot study involving 25 undergraduate students, we investigated the impact of
the system on learning through gameplay data and a 15-item survey that covered
constructs such as motivation, self-efficacy, metacognition, and feedback
quality. Results show that most students felt more confident and reflective
after using the tool, and error rates decreased as students progressed through
scaffolded levels. However, participation decreased with task difficulty, and
some students reported receiving unclear or vague feedback. These findings
suggest that DeliverC can enhance engagement and understanding in systems
programming, although refinement in AI-generated feedback is still needed. Our
study highlights the potential of combining GenAI with game-based learning to
support personalized and interactive practice in traditionally challenging
programming domains.

</details>


### [2] [Refinement-Types Driven Development: A study](https://arxiv.org/abs/2509.15005)
*Facundo Domínguez,Arnaud Spiwack*

Main category: cs.PL

TL;DR: 推广SMT求解器在普通编程中的应用，通过精细类型与编译器整合来提升程序组合能力


<details>
  <summary>Details</summary>
Motivation: 质疑SMT求解器仅适用于形式验证的传统观念，认为其在普通编程中也有广阔应用潜力

Method: 采用精细类型（Liquid Haskell）技术，将SMT求解器集成到编译器静态检查中，并通过绑定范围处理案例进行验证

Result: 实现了Liquid Haskell有限映射理论的原型实现，支持了案例研究，证明了技术的可行性

Conclusion: 精细类型和SMT求解器能够使普通编程更简单、更愉快，为日常编程带来新的可能性

Abstract: This paper advocates for the broader application of SMT solvers in everyday
programming, challenging the conventional wisdom that these tools are solely
for formal methods and verification. We claim that SMT solvers, when seamlessly
integrated into a compiler's static checks, significantly enhance the
capabilities of ordinary type checkers in program composition. Specifically, we
argue that refinement types, as embodied by Liquid Haskell, enable the use of
SMT solvers in mundane programming tasks. Through a case study on handling
binder scopes in compilers, we envision a future where ordinary programming is
made simpler and more enjoyable with the aid of refinement types and SMT
solvers. As a secondary contribution, we present a prototype implementation of
a theory of finite maps for Liquid Haskell's solver, developed to support our
case study.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless and GPU-Based Training Architectures](https://arxiv.org/abs/2509.14920)
*Amine Barrak,Fabio Petrillo,Fehmi Jaafar*

Main category: cs.DC

TL;DR: 该论文对比分析了多种无服务器分布式机器学习架构，发现SPIRT架构在训练效率、通信开销和效果方面显著优于传统方案


<details>
  <summary>Details</summary>
Motivation: 应对分布式机器学习对可扩展性和成本效果的日益增长需求，无服务器计算作为一种有前景的解决方案

Method: 对比分析SPIRT与ScatterReduce、AllReduce、MLLess等架构，重点考察训练时间效率、成本效果、通信开销和敌容能力

Result: SPIRT通过并行批处理和RedisAI数据库操作显著减少训练时间和通信开销，传统架构遇到扩展性挑战和敌容问题

Conclusion: SPIRT虽然初始成本较高，但具有长期经济效益，为未来研究结合各种架构优点的新模型奠定了基础

Abstract: The field of distributed machine learning (ML) faces increasing demands for
scalable and cost-effective training solutions, particularly in the context of
large, complex models. Serverless computing has emerged as a promising paradigm
to address these challenges by offering dynamic scalability and
resource-efficient execution. Building upon our previous work, which introduced
the Serverless Peer Integrated for Robust Training (SPIRT) architecture, this
paper presents a comparative analysis of several serverless distributed ML
architectures. We examine SPIRT alongside established architectures like
ScatterReduce, AllReduce, and MLLess, focusing on key metrics such as training
time efficiency, cost-effectiveness, communication overhead, and fault
tolerance capabilities. Our findings reveal that SPIRT provides significant
improvements in reducing training times and communication overhead through
strategies such as parallel batch processing and in-database operations
facilitated by RedisAI. However, traditional architectures exhibit scalability
challenges and varying degrees of vulnerability to faults and adversarial
attacks. The cost analysis underscores the long-term economic benefits of SPIRT
despite its higher initial setup costs. This study not only highlights the
strengths and limitations of current serverless ML architectures but also sets
the stage for future research aimed at developing new models that combine the
most effective features of existing systems.

</details>


### [4] [Conditional Prior-based Non-stationary Channel Estimation Using Accelerated Diffusion Models](https://arxiv.org/abs/2509.15182)
*Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Umer,Asad Aali,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 提出条件先验扩散方法用于非平稳无线信道估计，通过历史条件化分数学习来降噪，在3GPP基准测试中优于多种基线方法


<details>
  <summary>Details</summary>
Motivation: 移动丰富的城市微蜂窝环境中无线信道具有非平稳特性，传统和深度学习估计器性能会因信道分布随时间变化而下降

Method: 使用带跨时间注意力的时序编码器压缩观测窗口为上下文向量，通过特征调制引导去噪器；采用SNR匹配初始化和几何间隔的缩短采样计划

Result: 在3GPP基准测试中，所有SNR下均获得比LMMSE、GMM、LSTM和LDAMP基线更低的NMSE，表现出稳定性能和高SNR保真度

Conclusion: 条件先验扩散方法能有效处理非平稳信道估计问题，通过时序上下文学习和优化的扩散过程实现优越性能

Abstract: Wireless channels in motion-rich urban microcell (UMi) settings are
non-stationary; mobility and scatterer dynamics shift the distribution over
time, degrading classical and deep estimators. This work proposes conditional
prior diffusion for channel estimation, which learns a history-conditioned
score to denoise noisy channel snapshots. A temporal encoder with cross-time
attention compresses a short observation window into a context vector, which
captures the channel's instantaneous coherence and steers the denoiser via
feature-wise modulation. In inference, an SNR-matched initialization selects
the diffusion step whose marginal aligns with the measured input SNR, and the
process follows a shortened, geometrically spaced schedule, preserving the
signal-to-noise trajectory with far fewer iterations. Temporal
self-conditioning with the previous channel estimate and a training-only
smoothness penalty further stabilizes evolution without biasing the test-time
estimator. Evaluations on a 3GPP benchmark show lower NMSE across all SNRs than
LMMSE, GMM, LSTM, and LDAMP baselines, demonstrating stable performance and
strong high SNR fidelity.

</details>


### [5] [Channel Prediction under Network Distribution Shift Using Continual Learning-based Loss Regularization](https://arxiv.org/abs/2509.15192)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ibtsaam Qadir,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 通过控制损失正则化的持续学习框架，解决异构网络中频道预测的分布偏移问题，SI方法在资源约束环境下表现更优


<details>
  <summary>Details</summary>
Motivation: 现代无线网络中，移动用户穿越异构网络配置时遇到频道预测性能漏洞，传统预测器在分布偏移下NMSE上升37.5%

Method: 提出基于损失正则化的持续学习框架，通过添加惩罚项选择性保保关键网络参数，研究了EWC和SI两种正则化策略

Result: 在3GPP场景下，SI将高SNR的NMSE底器降低1.8dB(约32-34%)，EWC降低1.4dB(约17-28%)，SI具有更优的内存效率

Conclusion: 该持续学习框架有效解决了异构网络环境下的频道预测问题，SI方法在资源约束的无线经基础设施中具有更强的实用性

Abstract: Modern wireless networks face critical challenges when mobile users traverse
heterogeneous network configurations with varying antenna layouts, carrier
frequencies, and scattering statistics. Traditional predictors degrade under
distribution shift, with NMSE rising by 37.5\% during cross-configuration
handovers. This work addresses catastrophic forgetting in channel prediction by
proposing a continual learning framework based on loss regularization. The
approach augments standard training objectives with penalty terms that
selectively preserve network parameters essential for previous configurations
while enabling adaptation to new environments. Two prominent regularization
strategies are investigated: Elastic Weight Consolidation (EWC) and Synaptic
Intelligence (SI). Across 3GPP scenarios and multiple architectures, SI lowers
the high-SNR NMSE floor by up to 1.8 dB ($\approx$32--34\%), while EWC achieves
up to 1.4 dB ($\approx$17--28\%). Notably, standard EWC incurs
$\mathcal{O}(MK)$ complexity (storing $M$ Fisher diagonal entries and
corresponding parameter snapshots across $K$ tasks) unless consolidated,
whereas SI maintains $\mathcal{O}(M)$ memory complexity (storing $M$ model
parameters), independent of task sequence length, making it suitable for
resource-constrained wireless infrastructure

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations](https://arxiv.org/abs/2509.14388)
*Lennart Bamberg,Filippo Minnella,Roberto Bosio,Fabrizio Ottati,Yuebin Wang,Jongmin Lee,Luciano Lavagno,Adam Fuks*

Main category: cs.AR

TL;DR: 本文介绍了eIQ Neutron高效NPU架构，通过灵活的数据驱动设计和编译器优化，在同等TOPS和内存资源下比领先的嵌入式NPU性能提升1.8倍，即使面对计算和内存资源翻倍的NPU也能实现最高3.3倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统使用峰值TOPS来衡量NPU性能的方法不能准确反映实际性能，且通常与更高的硅成本相关。需要在保持灵活性的同时最大化计算利用率。

Method: 采用灵活的数据驱动架构设计，结合协同设计的编译器算法，使用约束编程方法根据工作负载特性优化计算和数据移动。

Result: 在标准AI基准测试中，相比领先的嵌入式NPU和编译器堆栈，在同等TOPS和内存资源下平均加速1.8倍（峰值4倍）。即使面对计算和内存资源翻倍的NPU，也能实现最高3.3倍的性能提升。

Conclusion: eIQ Neutron NPU通过架构和编译器的协同设计，在保持灵活性的同时显著提升了计算利用率，证明了在边缘AI推理中实现高效性能的可行性。

Abstract: Neural Processing Units (NPUs) are key to enabling efficient AI inference in
resource-constrained edge environments. While peak tera operations per second
(TOPS) is often used to gauge performance, it poorly reflects real-world
performance and typically rather correlates with higher silicon cost. To
address this, architects must focus on maximizing compute utilization, without
sacrificing flexibility. This paper presents the eIQ Neutron efficient-NPU,
integrated into a commercial flagship MPU, alongside co-designed compiler
algorithms. The architecture employs a flexible, data-driven design, while the
compiler uses a constrained programming approach to optimize compute and data
movement based on workload characteristics. Compared to the leading embedded
NPU and compiler stack, our solution achieves an average speedup of 1.8x (4x
peak) at equal TOPS and memory resources across standard AI-benchmarks. Even
against NPUs with double the compute and memory resources, Neutron delivers up
to 3.3x higher performance.

</details>


### [7] [Shift-Left Techniques in Electronic Design Automation: A Survey](https://arxiv.org/abs/2509.14551)
*Xinyue Wu,Zixuan Li,Fan Hu,Ting Lin,Xiaotian Zhao,Runxi Wang,Xinfei Guo*

Main category: cs.AR

TL;DR: 本文对EDA领域中的Shift-Left方法进行了全面综述，分析了该方法的进展、挑战和未来方向，重点关注AI技术和开源设计流程如何增强预测建模能力。


<details>
  <summary>Details</summary>
Motivation: 随着芯片设计复杂度增加，传统串行设计流程效率低下。Shift-Left方法通过创建数字孪生和融合多个设计步骤，使设计师能够更早建立强相关性并优化设计，但准确复制下游行为和确定采用时机仍存在挑战。

Method: 采用文献综述方法，系统梳理了EDA和更广泛设计生态系统中现有和新兴的Shift-Left研究范式，收集整理了相关论文并进行了分类分析。

Result: 研究发现AI技术和开源设计流程的兴起显著增强了预测和建模能力，使数据驱动方法在EDA社区中日益重要，从而增强了当前工具中的Shift-Left功能。

Conclusion: Shift-Left方法在EDA领域取得了显著进展，但仍面临挑战。AI和开源工具的融合为未来发展提供了新机遇，需要继续探索智能EDA工具和技术的发展方向。

Abstract: The chip design process involves numerous steps, beginning with defining
product requirements and progressing through architectural planning,
system-level design, and the physical layout of individual circuit blocks. As
the enablers of large-scale chip development, Electronic Design Automation
(EDA) tools play a vital role in helping designers achieve high-quality
results. The Shift-Left methodology introduces a pathway toward creating
digital twins and fusing multiple design steps, thereby transitioning
traditionally sequential, physically-aware processes into virtual design
environments. This shift allows designers to establish stronger correlations
earlier and optimize designs more effectively. However, challenges remain,
especially in accurately replicating downstream behaviors and determining the
right scope and timing for adoption. These challenges, in turn, have revealed
new opportunities for EDA vendors, physical designers, and logic designers
alike. As the industry advances toward intelligent EDA tools and techniques, it
is timely to reflect on Shift-Left progress made and the challenges that
remain. The rise of AI techniques and the momentum of open-source design flows
have significantly strengthened prediction and modeling capabilities, making
data-driven methods increasingly relevant to the EDA community. This, in turn,
enhances the ''Shift-Left'' features embedded in current tools. In this paper,
we present a comprehensive survey of existing and emerging paradigms in
Shift-Left research within EDA and the broader design ecosystem. Our goal is to
provide a unique perspective on the state of the field and its future
directions. Relevant papers mentioned are organized in
https://github.com/iCAS-SJTU/Shift-Left-EDA-Papers.

</details>


### [8] [DeepAssert: An LLM-Aided Verification Framework with Fine-Grained Assertion Generation for Modules with Extracted Module Specifications](https://arxiv.org/abs/2509.14668)
*Yonghao Wang,Jiaxin Zhou,Hongqin Lyu,Zhiteng Chao,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: DeepAssert是一个基于LLM的验证框架，能够分析模块间调用关系并自动生成细粒度深度断言，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有断言生成方法要么依赖设计规范（只能生成顶层断言），要么需要黄金RTL模型（难以获取），无法生成针对模块内部功能的深度断言。

Method: 通过分析模块间调用关系，提取各模块的独立规范及其I/O端口信息，然后利用LLM自动为这些模块生成细粒度的深度断言。

Result: DeepAssert在生成高质量模块深度断言方面显著优于AssertLLM和Spec2Assertion等方法，且能提升这些方法的整体断言质量。

Conclusion: DeepAssert提供了一个更全面有效的验证流程，解决了现有方法在生成模块级深度断言方面的局限性。

Abstract: Assertion-Based Verification (ABV) is a crucial method for ensuring that
logic designs conform to their architectural specifications. However, existing
assertion generation methods primarily rely on information either from the
design specification, or register-transfer level (RTL) code. The former methods
are typically limited to generating assertions for the top-level design. As the
top-level design is composed of different modules without module-level
specifications, they are unable to generate deep assertions that target the
internal functionality of modules. The latter methods often rely on a golden
RTL model, which is difficult to obtain. To address the above limitations, this
paper presents a novel large language model (LLM)-aided verification framework
named DeepAssert. DeepAssert is capable of analyzing the invocation
relationships between modules and extracting independent specifications for
each module with its I/O port information. These extracted specifications are
subsequently used to guide LLMs to automatically generate fine-grained deep
assertions for these modules. Our evaluation demonstrates that DeepAssert
significantly outperforms existing methods such as AssertLLM and Spec2Assertion
in generating high-quality deep assertions for modules. Furthermore, when
integrated with these methods, DeepAssert can enhance the overall quality of
the assertions generated. This allows for a more comprehensive and effective
verification process.

</details>


### [9] [LEAP: LLM Inference on Scalable PIM-NoC Architecture with Balanced Dataflow and Fine-Grained Parallelism](https://arxiv.org/abs/2509.14781)
*Yimin Wang,Yue Jiet Chong,Xuanyao Fong*

Main category: cs.AR

TL;DR: LEAP是一种非冯·诺依曼架构的LLM推理加速器，通过PIM和NoC协同设计，在Llama模型上相比A100 GPU实现了2.55倍吞吐量提升和71.94倍能效提升


<details>
  <summary>Details</summary>
Motivation: LLM推理需求日益增长，但大张量尺寸和计算复杂度给内存、计算和数据总线带来了挑战，需要新的硬件架构来解决这些问题

Method: 提出计算/内存/通信协同设计的非冯·诺依曼加速器LEAP，结合PIM和计算NoC，根据数据动态性分配矩阵乘法，采用启发式设计空间探索优化模型分区和映射，使用细粒度并行和分块技术实现高吞吐数据流

Result: 在Llama 1B/8B/13B模型上评估，相比A100 GPU实现了约2.55倍的吞吐量（tokens/sec）提升和约71.94倍的能效（tokens/Joule）提升

Conclusion: LEAP架构通过PIM和NoC的协同设计，有效解决了LLM推理中的内存、计算和通信瓶颈，显著提升了推理性能和能效

Abstract: Large language model (LLM) inference has been a prevalent demand in daily
life and industries. The large tensor sizes and computing complexities in LLMs
have brought challenges to memory, computing, and databus. This paper proposes
a computation/memory/communication co-designed non-von Neumann accelerator by
aggregating processing-in-memory (PIM) and computational network-on-chip (NoC),
termed LEAP. The matrix multiplications in LLMs are assigned to PIM or NoC
based on the data dynamicity to maximize data locality. Model partition and
mapping are optimized by heuristic design space exploration. Dedicated
fine-grained parallelism and tiling techniques enable high-throughput dataflow
across the distributed resources in PIM and NoC. The architecture is evaluated
on Llama 1B/8B/13B models and shows $\sim$2.55$\times$ throughput (tokens/sec)
improvement and $\sim$71.94$\times$ energy efficiency (tokens/Joule) boost
compared to the A100 GPU.

</details>


### [10] [NEURAL: An Elastic Neuromorphic Architecture with Hybrid Data-Event Execution and On-the-fly Attention Dataflow](https://arxiv.org/abs/2509.15036)
*Yuehai Chen,Farhad Merchant*

Main category: cs.AR

TL;DR: NEURAL是一种基于混合数据-事件执行范式的神经形态架构，通过解耦稀疏感知处理和神经元计算，使用弹性FIFO，实现了高效的脉冲神经网络加速，在资源利用和能效方面显著优于现有SNN加速器。


<details>
  <summary>Details</summary>
Motivation: 现有SNN硬件实现受限于脉冲稀疏性和多时间步执行，导致延迟增加和能效降低，需要新的架构设计来解决这些问题。

Method: 提出NEURAL架构：1）混合数据-事件执行范式；2）解耦稀疏感知处理与神经元计算；3）弹性FIFO；4）W2TTFS机制替代平均池化；5）基于知识蒸馏的训练框架构建单时间步SNN模型。

Result: 算法层面：VGG-11模型在CIFAR-10和CIFAR-100上准确率分别提升3.20%和5.13%；架构层面：相比现有SNN加速器，资源利用率降低50%，能效提升1.97倍。

Conclusion: NEURAL架构通过创新的混合执行范式和训练方法，有效解决了SNN硬件实现的延迟和能效问题，为高效神经形态计算提供了可行方案。

Abstract: Spiking neural networks (SNNs) have emerged as a promising alternative to
artificial neural networks (ANNs), offering improved energy efficiency by
leveraging sparse and event-driven computation. However, existing hardware
implementations of SNNs still suffer from the inherent spike sparsity and
multi-timestep execution, which significantly increase latency and reduce
energy efficiency. This study presents NEURAL, a novel neuromorphic
architecture based on a hybrid data-event execution paradigm by decoupling
sparsity-aware processing from neuron computation and using elastic
first-in-first-out (FIFO). NEURAL supports on-the-fly execution of spiking
QKFormer by embedding its operations within the baseline computing flow without
requiring dedicated hardware units. It also integrates a novel
window-to-time-to-first-spike (W2TTFS) mechanism to replace average pooling and
enable full-spike execution. Furthermore, we introduce a knowledge distillation
(KD)-based training framework to construct single-timestep SNN models with
competitive accuracy. NEURAL is implemented on a Xilinx Virtex-7 FPGA and
evaluated using ResNet-11, QKFResNet-11, and VGG-11. Experimental results
demonstrate that, at the algorithm level, the VGG-11 model trained with KD
improves accuracy by 3.20% on CIFAR-10 and 5.13% on CIFAR-100. At the
architecture level, compared to existing SNN accelerators, NEURAL achieves a
50% reduction in resource utilization and a 1.97x improvement in energy
efficiency.

</details>


### [11] [Voyager: An End-to-End Framework for Design-Space Exploration and Generation of DNN Accelerators](https://arxiv.org/abs/2509.15205)
*Kartik Prabhu,Jeffrey Yu,Xinyuan Allen Pan,Zhouhua Xie,Abigail Aleshire,Zihan Chen,Ammar Ali Ratnani,Priyanka Raina*

Main category: cs.AR

TL;DR: Voyager是一个基于HLS的DNN加速器自动生成框架，提供高度可配置的设计空间探索，支持多种数据类型和量化方案，并能生成高性能的tapeout-ready设计。


<details>
  <summary>Details</summary>
Motivation: 传统DNN加速器设计过程耗时费力且扩展性差，现有自动化工具存在参数化有限、不支持高性能设计、数据类型支持不足等问题。

Method: 基于高层次综合(HLS)框架，提供可配置的技术节点、时钟频率、处理单元数量、片上缓存大小等参数，支持浮点、posit和整数等多种数据类型及量化方案，集成PyTorch编译器进行端到端网络映射。

Result: 在先进视觉和语言模型上评估，设计利用率高达99.8%，相比现有生成器延迟降低61%、面积减少56%，性能与手工优化加速器相当但自动化程度更高。

Conclusion: Voyager框架实现了高效、自动化的DNN加速器设计与映射，解决了现有工具的局限性，为硬件加速器设计提供了可扩展的解决方案。

Abstract: While deep neural networks (DNNs) have achieved state-of-the-art performance
in fields from computer vision to natural language processing, efficiently
running these computationally demanding models requires hardware accelerators.
However, designing these accelerators is a time-consuming, labor-intensive
process that does not scale well. While prior efforts have sought to automate
DNN accelerator generation, they offer limited parameterization, cannot produce
high-performance, tapeout-ready designs, provide limited support for datatypes
and quantization schemes, and lack an integrated, end-to-end software compiler.
This work proposes Voyager, a high-level synthesis (HLS)-based framework for
design space exploration (DSE) and generation of DNN accelerators. Voyager
overcomes the limitations of prior work by offering extensive configurability
across technology nodes, clock frequencies, and scales, with customizable
parameters such as number of processing elements, on-chip buffer sizes, and
external memory bandwidth. Voyager supports a wider variety of datatypes and
quantization schemes versus prior work, including both built-in floating-point,
posit and integer formats, as well as user-defined formats with both per-tensor
scaling and microscaling quantization. Voyager's PyTorch-based compiler
efficiently maps networks end-to-end on the generated hardware, with support
for quantization, fusion, and tiling. We evaluate Voyager on state-of-the-art
vision and language models. Voyager enables fast DSE with full-dataset accuracy
evaluation for datatypes and quantization schemes. Generated designs achieve a
high utilization across models and scales, up to 99.8%, and outperform prior
generators with up to 61% lower latency and 56% lower area. Compared to
hand-optimized accelerators, Voyager achieves comparable performance, while
offering much greater automation in design and workload mapping.

</details>
