<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Dependent Multiplicities in Dependent Linear Type Theory](https://arxiv.org/abs/2507.08759)
*Maximilian Doré*

Main category: cs.PL

TL;DR: 提出了一种新颖的依赖线性类型理论，其中变量的多重性（即变量在程序中的使用次数）可以依赖于其他变量，从而为高阶函数提供精确的资源注释。


<details>
  <summary>Details</summary>
Motivation: 现有系统无法为某些高阶函数提供准确的资源注释，因此需要一种能够支持依赖多重性的类型理论。

Method: 通过将线性逻辑嵌入依赖类型理论，并指定嵌入逻辑与宿主理论的交互方式，构建了一个定量类型系统。

Result: 理论语义结合了依赖类型理论和线性逻辑的标准模型，并在Agda中实现了该系统。

Conclusion: 该理论为依赖类型语言提供了灵活的资源管理能力，扩展了其表达能力。

Abstract: We present a novel dependent linear type theory in which the multiplicity of
some variable - i.e., the number of times the variable can be used in a program
- can depend on other variables. This allows us to give precise resource
annotations to many higher-order functions that cannot be adequately typed in
any other system. Inspired by the Dialectica translation, our typing discipline
is obtained by embedding linear logic into dependent type theory and specifying
how the embedded logic interacts with the host theory. We can then use a
standard natural numbers type to obtain a quantitative typing system with
dependent multiplicities. We characterise the semantics for our theory as a
combination of standard models of dependent type theory and linear logic. Our
system can be added to any dependently typed language, which we demonstrate
with an implementation in Agda.

</details>


### [2] [Filter Equivariant Functions: A symmetric account of length-general extrapolation on lists](https://arxiv.org/abs/2507.08796)
*Owen Lewis,Neil Ghani,Andrew Dudzik,Christos Perivolaropoulos,Razvan Pascanu,Petar Veličković*

Main category: cs.PL

TL;DR: 论文探讨了如何设计一个能外推已知输入/输出示例的函数，提出了“过滤器等变函数”这一新语义类，并研究了其性质、几何解释及构造算法。


<details>
  <summary>Details</summary>
Motivation: 研究函数在外推时的行为规则，提出过滤器等变函数作为“好”外推函数的候选。

Method: 引入过滤器等变函数类，证明其基本定理，与映射等变函数类关联，并给出几何解释和构造算法。

Result: 证明了过滤器等变函数的性质，提出了一种完美外推的构造算法。

Conclusion: 过滤器等变函数为外推问题提供了有前景的解决方案，其几何和算法特性值得进一步研究。

Abstract: What should a function that extrapolates beyond known input/output examples
look like? This is a tricky question to answer in general, as any function
matching the outputs on those examples can in principle be a correct
extrapolant. We argue that a "good" extrapolant should follow certain kinds of
rules, and here we study a particularly appealing criterion for rule-following
in list functions: that the function should behave predictably even when
certain elements are removed. In functional programming, a standard way to
express such removal operations is by using a filter function. Accordingly, our
paper introduces a new semantic class of functions -- the filter equivariant
functions. We show that this class contains interesting examples, prove some
basic theorems about it, and relate it to the well-known class of map
equivariant functions. We also present a geometric account of filter
equivariants, showing how they correspond naturally to certain simplicial
structures. Our highlight result is the amalgamation algorithm, which
constructs any filter-equivariant function's output by first studying how it
behaves on sublists of the input, in a way that extrapolates perfectly.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Supporting Intel(r) SGX on Multi-Package Platforms](https://arxiv.org/abs/2507.08190)
*Simon Johnson,Raghunandan Makaram,Amy Santoni,Vinnie Scarlata*

Main category: cs.DC

TL;DR: 本文探讨了Intel SGX技术在云计算中的应用，并提出了多平台扩展的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着开发者对SGX技术的熟悉，其在云计算中的适用性被测试，需要进一步扩展以支持多平台。

Method: 描述了为支持多平台和云计算的SGX平台增强措施。

Result: 展示了SGX在云计算中的潜力，并提出了扩展需求。

Conclusion: SGX技术需要进一步扩展以实现可编程、高性能且安全的云计算环境。

Abstract: Intel(r) Software Guard Extensions (SGX) was originally released on client
platforms and later extended to single socket server platforms. As developers
have become familiar with the capabilities of the technology, the applicability
of this capability in the cloud has been tested. Various Cloud Service
Providers (CSPs) are demonstrating the value of using SGX based Trusted
Execution Environments (TEE) to create a new paradigm of Confidential Cloud
Computing. This paper describes the additional platform enhancements we believe
are necessary to deliver a user programmable Trusted Execution Environment that
scales to cloud usages, performs and is secure on multi-package platforms.

</details>


### [4] [Fast and Interactive Byzantine Fault-tolerant Web Services via Session-Based Consensus Decoupling](https://arxiv.org/abs/2507.08281)
*Ahmad Zaki Akmal,Azkario Rizky Pratama,Guntur Dharma Putra*

Main category: cs.DC

TL;DR: 提出了一种新型的两层架构，通过分离交互操作和共识最终化，解决了BFT系统中安全性和响应性之间的矛盾，实现了低延迟和高安全性。


<details>
  <summary>Details</summary>
Motivation: 解决BFT系统在提供关键完整性保证时面临的延迟问题，以提升交互式用户体验。

Method: 采用两层架构：会话感知的事务缓冲层（Layer 2）提供即时反馈，定期将批量操作提交到完全BFT共识层（Layer 1）。

Result: 系统实现了低于200ms的响应时间，同时保持强BFT安全性；Layer 2操作速度是Layer 1的四倍。

Conclusion: 该架构使BFT应用在延迟受限领域（如元宇宙）成为可能，满足了响应性和状态一致性的双重需求。

Abstract: Byzantine fault-tolerant (BFT) web services provide critical integrity
guarantees for distributed applications but face significant latency challenges
that hinder interactive user experiences. We propose a novel two-layer
architecture that addresses this fundamental tension between security and
responsiveness in BFT systems. Our approach introduces a session-aware
transaction buffer layer (Layer 2) that delivers immediate feedback to users
through consensus simulation, while periodically committing batched operations
to a fully Byzantine fault-tolerant consensus layer (Layer 1). By separating
interactive operations from consensus finalization, our system achieves
responsive user experiences of under 200ms, while maintaining strong BFT
security guarantees. We demonstrate the efficacy of our architecture through a
supply chain management implementation, where operators require both immediate
feedback during multi-step workflows and tamper-proof record keeping. Our
evaluation shows that our Layer 2 operations perform four times faster than the
Layer 1 counterpart, while substantially preserving the end-to-end transaction
integrity. Our approach enables BFT applications in domains previously
considered impractical due to latency constraints, such as metaverse
environments, where users require both responsive interaction and guaranteed
state consistency.

</details>


### [5] [Content-Oblivious Leader Election in 2-Edge-Connected Networks](https://arxiv.org/abs/2507.08348)
*Yi-Jun Chang,Lyuting Chen,Haoran Zhou*

Main category: cs.DC

TL;DR: 本文提出了一种在2边连通网络中异步无内容感知的领导者选举算法，消息复杂度为O(m⋅N⋅ID_min)，结合之前的模拟结果，完全反驳了原猜想。


<details>
  <summary>Details</summary>
Motivation: 研究完全缺陷异步网络中无需预设领导者即可进行非平凡计算的可能性，特别是针对2边连通网络。

Method: 设计了一种异步无内容感知的领导者选举算法，适用于2边连通网络，消息复杂度为O(m⋅N⋅ID_min)。

Result: 算法能够在2边连通网络中静默终止，结合之前的模拟结果，证明无需预设领导者即可模拟无噪声设置中的算法。

Conclusion: 完全反驳了原猜想，证明了在2边连通网络中无需预设领导者即可进行非平凡计算。

Abstract: Censor-Hillel, Cohen, Gelles, and Sela (PODC 2022 \& Distributed Computing
2023) studied fully-defective asynchronous networks, where communication
channels may suffer an extreme form of alteration errors, rendering messages
completely corrupted. The model is equivalent to content-oblivious computation,
where nodes communicate solely via pulses. They showed that if the network is
2-edge-connected, then any algorithm for a noiseless setting can be simulated
in the fully-defective setting; otherwise, no non-trivial computation is
possible in the fully-defective setting. However, their simulation requires a
predesignated leader, which they conjectured to be necessary for any
non-trivial content-oblivious task.
  Recently, Frei, Gelles, Ghazy, and Nolin (DISC 2024) refuted this conjecture
for the special case of oriented ring topology. They designed two asynchronous
content-oblivious leader election algorithms with message complexity $O(n \cdot
\mathsf{ID}_{\max})$, where $n$ is the number of nodes and $\mathsf{ID}_{\max}$
is the maximum $\mathsf{ID}$. The first algorithm stabilizes in unoriented
rings without termination detection. The second algorithm quiescently
terminates in oriented rings, thus enabling the execution of the simulation
algorithm after leader election.
  In this work, we present an asynchronous content-oblivious leader election
algorithm that quiescently terminates in any 2-edge connected network with
message complexity $O(m \cdot N \cdot \mathsf{ID}_{\min})$, where $m$ is the
number of edges, $N$ is a known upper bound on the number of nodes, and
$\mathsf{ID}_{\min}$ is the smallest $\mathsf{ID}$. Combined with the previous
simulation result, our finding implies that any algorithm from the noiseless
setting can be simulated in the fully-defective setting without assuming a
preselected leader, entirely refuting the original conjecture.

</details>


### [6] [Carbon-Aware Workflow Scheduling with Fixed Mapping and Deadline Constraint](https://arxiv.org/abs/2507.08725)
*Dominik Schweisgut,Anne Benoit,Yves Robert,Henning Meyerhenke*

Main category: cs.DC

TL;DR: 论文提出了一种调度框架CaWoSched，用于在混合能源数据中心中优化任务调度以减少碳排放。


<details>
  <summary>Details</summary>
Motivation: 大型数据中心能耗高，碳排放问题严重，尤其是任务依赖性强的工作流。通过优化调度任务到绿色能源充足时段，可减少碳排放。

Method: 将问题形式化为调度问题，提出多项式时间解（单处理器）和NP难解（多处理器）。提出启发式框架CaWoSched，结合贪婪算法和局部搜索。

Result: 实验表明，CaWoSched相比基线算法显著减少了碳排放。

Conclusion: CaWoSched在混合能源数据中心中有效减少碳排放，尤其适用于多处理器环境。

Abstract: Large data and computing centers consume a significant share of the world's
energy consumption. A prominent subset of the workloads in such centers are
workflows with interdependent tasks, usually represented as directed acyclic
graphs (DAGs). To reduce the carbon emissions resulting from executing such
workflows in centers with a mixed (renewable and non-renewable) energy supply,
it is advisable to move task executions to time intervals with sufficient green
energy when possible. To this end, we formalize the above problem as a
scheduling problem with a given mapping and ordering of the tasks. We show that
this problem can be solved in polynomial time in the uniprocessor case. For at
least two processors, however, the problem becomes NP-hard. Hence, we propose a
heuristic framework called CaWoSched that combines several greedy approaches
with local search. To assess the 16 heuristics resulting from different
combinations, we also devise a simple baseline algorithm and an exact ILP-based
solution. Our experimental results show that our heuristics provide significant
savings in carbon emissions compared to the baseline.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [CCSS: Hardware-Accelerated RTL Simulation with Fast Combinational Logic Computing and Sequential Logic Synchronization](https://arxiv.org/abs/2507.08406)
*Weigang Feng,Yijia Zhang,Zekun Wang,Zhengyang Wang,Yi Wang,Peijun Ma,Ningyi Xu*

Main category: cs.AR

TL;DR: CCSS是一个可扩展的多核RTL仿真平台，通过专用架构和编译策略加速组合逻辑计算和时序逻辑同步，比现有技术快12.9倍。


<details>
  <summary>Details</summary>
Motivation: 随着单芯片晶体管数量超过数百亿，RTL级仿真和验证的复杂性呈指数增长，仿真周期长达数月。CPU仿真速度成为功能调试的主要瓶颈。

Method: CCSS采用平衡的DAG分区方法和高效的布尔计算核心处理组合逻辑，并通过低延迟片上网络（NoC）设计同步时序状态。

Result: 实验结果表明，CCSS比最先进的多核仿真器快12.9倍。

Conclusion: CCSS通过优化架构和编译策略，显著提升了RTL仿真的速度和效率。

Abstract: As transistor counts in a single chip exceed tens of billions, the complexity
of RTL-level simulation and verification has grown exponentially, often
extending simulation campaigns to several months. In industry practice, RTL
simulation is divided into two phases: functional debug and system validation.
While system validation demands high simulation speed and is typically
accelerated using FPGAs, functional debug relies on rapid compilation-rendering
multi-core CPUs the primary choice. However, the limited simulation speed of
CPUs has become a major bottleneck. To address this challenge, we propose CCSS,
a scalable multi-core RTL simulation platform that achieves both fast
compilation and high simulation throughput. CCSS accelerates combinational
logic computation and sequential logic synchronization through specialized
architecture and compilation strategies. It employs a balanced DAG partitioning
method and efficient boolean computation cores for combinational logic, and
adopts a low-latency network-on-chip (NoC) design to synchronize sequential
states across cores efficiently. Experimental results show that CCSS delivers
up to 12.9x speedup over state-of-the-art multi-core simulators.

</details>


### [8] [Fast and Efficient Merge of Sorted Input Lists in Hardware Using List Offset Merge Sorters](https://arxiv.org/abs/2507.08658)
*Robert B. Kent,Marios S. Pattichis*

Main category: cs.AR

TL;DR: 介绍了新型硬件合并排序设备LOMS，通过列排序和行排序交替进行，高效合并多个有序输入列表。LOMS 2-way设备比现有设备更快且资源占用更少。


<details>
  <summary>Details</summary>
Motivation: 解决现有合并排序设备（如Bitonic和Odd-Even）在速度和资源效率上的不足，支持不同大小的输入列表。

Method: 使用输入2-D数组，通过列排序和行排序交替处理，利用S2MS设备作为第一阶段。

Result: LOMS 2-way设备合并32值列表仅需2.24 nS，速度提升2.63倍；3-way设备合并21值列表仅需3.4 nS，速度提升1.36倍。

Conclusion: LOMS设备在速度和资源效率上优于现有技术，适用于FPGA实现。

Abstract: A new set of hardware merge sort devices are introduced here, which merge
multiple sorted input lists into a single sorted output list in a fast and
efficient manner. In each merge sorter, the values from the sorted input lists
are arranged in an input 2-D setup array, but with the order of each sorted
input list offset from the order of each of the other sorted input lists. In
these new devices, called List Offset Merge Sorters (LOMS), a minimal set of
column sort stages alternating with row sort stages process the input setup
array into a final output array, now in the defined sorted order. LOMS 2-way
sorters, which merge 2 sorted input lists, require only 2 merge stages and are
significantly faster than Kenneth Batcher's previous state-of-the-art 2-way
merge devices, Bitonic Merge Sorters and Odd-Even Merge Sorters. LOMS 2-way
sorters utilize the recently-introduced Single-Stage 2-way Merge Sorters (S2MS)
in their first stage. Both LOMS and S2MS devices can merge any mixture of input
list sizes, while Batcher's merge sorters are difficult to design unless the 2
input lists are equal, and a power-of-2. By themselves, S2MS devices are the
fastest 2-way merge sorters when implemented in this study's target FPGA
devices, but they tend to use a large number of LUT resources. LOMS 2-way
devices use fewer resources than comparable S2MS devices, enabling some large
LOMS devices to be implemented in a given FPGA when comparable S2MS devices
cannot fit in that FPGA. A List Offset 2-way sorter merges 2 lists, each with
32 values, into a sorted output list of those 64 values in 2.24 nS, a speedup
of 2.63 versus a comparable Batcher device. A LOMS 3-way merge sorter, merging
3 sorted input lists with 7 values, fully merges the 21 values in 3.4 nS, a
speedup of 1.36 versus the comparable state-of-the-art 3-way merge device.

</details>
