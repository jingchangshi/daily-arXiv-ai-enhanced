<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.AR](#cs.AR) [Total: 10]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Nice to Meet You: Synthesizing Practical MLIR Abstract Transformers](https://arxiv.org/abs/2512.06442)
*Xuanyu Peng,Dominic Kennedy,Yuyou Fan,Ben Greenman,John Regehr,Loris D'Antoni*

Main category: cs.PL

TL;DR: NiceToMeetYou是一个程序合成框架，用于为编译器中的整数抽象域自动生成抽象转换器，通过分解合成问题并验证正确性，提高了转换器的精度和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 静态分析在编译中至关重要，但实现正确、精确且高效的抽象转换器具有挑战性。LLVM和GCC都曾因不正确的抽象转换器导致错误编译，且LLVM仍有数百条指令缺乏抽象转换器，需要自动化解决方案。

Method: 开发了NiceToMeetYou框架，采用分解合成技术：将每个抽象转换器分解为多个更简单的转换器的交集，每个新组件填补最终转换器的精度缺口。无需人工草图，通过将验证降低到MLIR的SMT方言来验证转换器。

Result: 合成的转换器被证明是正确的，其中17%比LLVM提供的转换器更精确，实现了批量自动化生成，解决了LLVM中抽象转换器覆盖率不足的问题。

Conclusion: NiceToMeetYou框架成功实现了抽象转换器的自动化合成，提高了精度和覆盖率，为编译器静态分析提供了可靠且高效的解决方案。

Abstract: Static analyses play a fundamental role during compilation: they discover facts that are true in all executions of the code being compiled, and then these facts are used to justify optimizations and diagnostics. Each static analysis is based on a collection of abstract transformers that provide abstract semantics for the concrete instructions that make up a program. It can be challenging to implement abstract transformers that are sound, precise, and efficient, and in fact both LLVM and GCC have suffered from miscompilations caused by unsound abstract transformers. Moreover, even after more than 20 years of development, LLVM lacks abstract transformers for hundreds of instructions in its intermediate representation (IR). We developed NiceToMeetYou, a program synthesis framework for abstract transformers that are aimed at the kinds of non-relational integer abstract domains that are heavily used by today's production compilers. It exploits a simple but novel technique for breaking the synthesis problem into parts: each of our transformers is the meet of a collection of simpler, sound transformers that are synthesized such that each new piece fills a gap in the precision of the final transformer. Our design point is bulk automation: no sketches are required. Transformers are verified by lowering to a previously created SMT dialect of MLIR. Each of our synthesized transformers is provably sound and some (17 percent) are more precise than those provided by LLVM.

</details>


### [2] [PIP: Making Andersen's Points-to Analysis Sound and Practical for Incomplete C Programs](https://arxiv.org/abs/2512.07299)
*Håvard Rognebakke Krogstie,Helge Bahmann,Magnus Själander,Nico Reissmann*

Main category: cs.PL

TL;DR: 针对不完整C程序的安德森式指针分析，通过隐式指针目标跟踪实现高效且可靠的解决方案，比现有技术快15倍，内存可扩展，适合实际编译器优化。


<details>
  <summary>Details</summary>
Motivation: 现代编译器通常单独编译文件以实现并行化，但这导致编译器必须在程序不完整的情况下工作。现有的指针分析技术需要完整的程序才能保证可靠性，而生产编译器很少提供摘要函数，同时可靠性和效率又是不可妥协的要求。

Method: 提出一种安德森式指针分析方法，通过隐式跟踪可从外部模块访问的内存位置和指针来实现可靠性。引入"偏好隐式指针目标"（PIP）技术进一步减少显式指针目标的使用，在约束图中隐式执行跟踪以提高效率。

Result: 隐式指针目标跟踪使约束求解器比使用显式跟踪的五种最先进技术组合快15倍。PIP技术提供额外1.9倍加速。在别名分析客户端评估中，相比LLVM的BasicAA单独使用，减少40%的MayAlias响应。分析在内存方面具有可扩展性。

Conclusion: 该方法为不完整C程序提供了高效可靠的指针分析解决方案，适合实际生产编译器使用，在保持可靠性的同时显著提升了分析效率。

Abstract: Compiling files individually lends itself well to parallelization, but forces the compiler to operate on incomplete programs. State-of-the-art points-to analyses guarantee sound solutions only for complete programs, requiring summary functions to describe any missing program parts. Summary functions are rarely available in production compilers, however, where soundness and efficiency are non-negotiable. This paper presents an Andersen-style points-to analysis that efficiently produces sound solutions for incomplete C programs. The analysis accomplishes soundness by tracking memory locations and pointers that are accessible from external modules, and efficiency by performing this tracking implicitly in the constraint graph. We show that implicit pointee tracking makes the constraint solver 15$\times$ faster than any combination of five different state-of-the-art techniques using explicit pointee tracking. We also present the Prefer Implicit Pointees (PIP) technique that further reduces the use of explicit pointees. PIP gives an additional speedup of 1.9$\times$, compared to the fastest solver configuration not benefiting from PIP. The precision of the analysis is evaluated in terms of an alias-analysis client, where it reduces the number of MayAlias-responses by 40% compared to LLVM's BasicAA pass alone. Finally, we show that the analysis is scalable in terms of memory, making it suitable for optimizing compilers in practice.

</details>


### [3] [Canonical bidirectional typechecking](https://arxiv.org/abs/2512.07511)
*Zanzi Mihejevs,Jules Hedges*

Main category: cs.PL

TL;DR: 该论文展示了双向类型检查中的可检查/可综合划分与极化系统L中的对偶性一致，建立了极化μμ̃演算、LNL演算和双向演算之间的三向对应关系。


<details>
  <summary>Details</summary>
Motivation: 探索双向类型检查中可检查与可综合的划分与极化系统L中对偶性之间的深层联系，建立不同计算模型之间的统一理论框架。

Method: 结合标准双向类型检查与Zeilberger的"共上下文"变体，扩展到使用McBride的共德布鲁因作用域表述的笛卡尔系统L，并在线性-非线性风格中组合两者。

Result: 证明了极化系统L中的正项和负余项是可检查的，负项和正余项是可综合的，建立了极化系统L的移位、LNL演算和双向演算之间的三向对应关系。

Conclusion: 双向类型检查的划分与极化系统L的对偶性本质相同，这一发现为类型理论中的不同计算模型提供了统一的数学基础。

Abstract: We demonstrate that the checkable/synthesisable split in bidirectional typechecking coincides with existing dualities in polarised System L, also known as polarised $μ\tildeμ$-calculus. Specifically, positive terms and negative coterms are checkable, and negative terms and positive coterms are synthesisable. This combines a standard formulation of bidirectional typechecking with Zeilberger's `cocontextual' variant. We extend this to ordinary `cartesian' System L using Mc Bride's co-de Bruijn formulation of scopes, and show that both can be combined in a linear-nonlinear style, where linear types are positive and cartesian types are negative. This yields a remarkable 3-way coincidence between the shifts of polarised System L, LNL calculi, and bidirectional calculi.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices](https://arxiv.org/abs/2512.06443)
*Xiangyu Li,Chengyu Yin,Weijun Wang,Jianyu Wei,Ting Cao,Yunxin Liu*

Main category: cs.DC

TL;DR: 提出向量查找表(Vec-LUT)方法，解决标量LUT在并行推理中内存带宽利用不足的问题，实现单次1→N查找，在边缘设备上性能提升最高达4.2倍。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在边缘设备部署需求增加，量化技术已发展到1.58位超低比特。虽然基于查找表(LUT)的推理使CPU运行速度超过NPU，但标量LUT范式在并行推理（如预填充、测试时缩放等场景）中存在内存带宽利用不足的问题，根源在于每个token都需要重复且非连续的内存访问。

Method: 提出向量查找表(Vec-LUT)新范式：1) 构建跨并行token的统一LUT，实现单次1→N查找；2) 向量LUT中心张量布局；3) 缓存感知流式查找技术。这些技术优化了内存访问模式，提高了并行推理效率。

Result: 在5种边缘设备和3个LLM上的评估显示，Vec-LUT比现有最优方法性能提升最高达4.2倍。该方法已集成到llama.cpp中，代码开源。

Conclusion: 向量查找表范式有效解决了标量LUT在并行推理中的内存带宽瓶颈，显著提升了超低比特LLM在边缘设备上的推理效率，为无处不在的设备端智能开辟了新机会。

Abstract: Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.
  However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.
  To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.

</details>


### [5] [Stable-MoE: Lyapunov-based Token Routing for Distributed Mixture-of-Experts Training over Edge Networks](https://arxiv.org/abs/2512.06784)
*Long Shi,Bingyan Ou,Kang Wei,Weihao Zhu,Zhe Wang,Zhiyong Chen*

Main category: cs.DC

TL;DR: 提出基于Lyapunov优化的分布式MoE训练令牌路由框架Stable-MoE，解决边缘网络中异构计算能力和随机令牌到达带来的负载积压、资源效率低下和性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 传统分布式MoE训练中的令牌路由在资源受限的边缘网络中面临挑战，包括异构计算能力、随机令牌到达导致的负载积压、资源效率低下和性能下降问题。

Method: 提出Lyapunov-based令牌路由框架Stable-MoE，通过优化令牌路由策略和计算资源分配来最大化系统吞吐量和门控一致性，同时确保边缘设备令牌和能量队列的长期稳定性。使用Lyapunov优化将长期优化问题转化为可处理的每时隙子问题，实现无需未来系统状态信息的在线决策。

Result: 在SVHN和CIFAR-100数据集上的实验结果表明，Stable-MoE在系统吞吐量和测试准确率方面分别比基线方法至少提升40%和5%。

Conclusion: Stable-MoE框架有效解决了资源异构边缘网络中分布式MoE训练的令牌路由问题，显著提升了系统性能和训练效率。

Abstract: The sparse activation mechanism of mixture of experts (MoE) model empowers edge intelligence with enhanced training efficiency and reduced computational resource consumption. However, traditional token routing in distributed MoE training faces significant challenges in resource-constrained edge networks characterized by heterogeneous computing capabilities and stochastic token arrivals, which inevitably suffer from workload backlog, resource inefficiency, and performance degradation. To address this issue, we propose a novel Lyapunov-based token routing framework for distributed MoE training over resource-heterogeneous edge networks, termed Stable-MoE. Specifically, we formulate a stochastic optimization problem to maximize both system throughput and gating consistency via optimizing the token routing strategy and computational resource allocation, while ensuring long-term stability of both token and energy queues at the edge devices. Using the Lyapunov optimization, we transform the intractable long-term optimization problem into tractable per-slot subproblems by enabling online decision-making of token routing and computation frequency utilization without the knowledge of future system states. Experimental results on the SVHN and CIFAR-100 datasets demonstrate that Stable-MoE outperforms the baselines with at least 40% and 5% gains in system throughput and test accuracy, respectively.

</details>


### [6] [Cloud Revolution: Tracing the Origins and Rise of Cloud Computing](https://arxiv.org/abs/2512.06800)
*Deepa Gurung,S M Zia Ur Rashid,Zain ul Abdeen,Suman Rath*

Main category: cs.DC

TL;DR: 本文重新审视了云计算的历史演变，从虚拟化、分布式系统等技术基础到现代全球云生态系统，分析了技术经济驱动力、组织变革影响、当前局限性和新兴趋势。


<details>
  <summary>Details</summary>
Motivation: 重新审视云计算数十年发展历程，理解其从技术基础到广泛应用的演变，分析技术经济驱动力如何改变组织计算习惯，并识别大规模采用带来的局限性和新兴趋势。

Method: 历史回顾与综合分析：通过重新审视云计算的历史演变，包括资源共享、效用计算等初始理念，分析技术经济力量，评估组织计算习惯变化，识别当前局限性和新兴趋势。

Result: 云计算已从虚拟化等技术发展为全球云生态系统，降低了数据密集和计算密集型应用的门槛，但面临安全配置漏洞、监管合规和供应商依赖等局限性，边缘计算、AI优化架构和量子计算服务正在重塑云环境。

Conclusion: 云计算是一个快速演变的范式，其未来发展方向需要在可扩展性、开放性和可信度之间取得平衡，新兴趋势将继续重塑云环境。

Abstract: The history behind the development of cloud computing is more than several decades of technological progress in the fields of virtualization, distributed systems, and high-speed networking, but its current application is much broader than the underlying technologies that made it possible. This paper reexamines the historical evolution of the field, including the initial ideas of resource sharing and utility-based computing approaches and the development of hyperscale data centers and modern globally federated cloud ecosystems. We also analyze the technological and economic forces and point to the way cloud platforms altered the organizational computing habits, decreasing the entrance-level to the data-intensive and computation-heavy apps. The study also takes into account the ongoing limitations which have come with the large-scale adoption of clouds which include exposure to security due to the weaknesses in configuration, particular establishment regulations, and structural reliance on the single vendors. Lastly, we address some of the new trends that are transforming the cloud environment, including the convergence of edge and cloud infrastructure, the increased prominence of AI-optimised architectures and the initial adoption of quantum computing services. Collectively, the developments above describe an emerging but quickly changing paradigm with its future direction being determined by a strike of balancing between scalability, openness, and trust.

</details>


### [7] [Optimizing video analytics inference pipelines: a case study](https://arxiv.org/abs/2512.07009)
*Saeid Ghafouri,Yuming Ding,Katerine Diaz Chito,Jesús Martinez del Rincón,Niamh O'Connell,Hans Vandierendonck*

Main category: cs.DC

TL;DR: 本文通过系统级优化（多级并行化、GPU加速、向量化聚类、内存高效后处理）实现了家禽福利监控系统2倍加速，为农业视频分析提供实用策略


<details>
  <summary>Details</summary>
Motivation: 商业养殖场的高分辨率视频和近实时监控需求产生巨大计算负载，需要成本效益高且可扩展的视频分析解决方案

Method: 采用系统级改进，包括多级并行化、用GPU加速代码替代CPU代码、向量化聚类和内存高效后处理，优化检测、跟踪、聚类和行为分析模块

Result: 在真实农场视频数据上评估，优化后的系统在保持模型精度的前提下，实现了高达2倍的管道加速

Conclusion: 研究结果为构建高吞吐量、低延迟的视频推理系统提供了实用策略，可降低农业和智能传感部署以及其他大规模视频分析应用的基础设施需求

Abstract: Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.

</details>


### [8] [PIR-DSN: A Decentralized Storage Network Supporting Private Information Retrieval](https://arxiv.org/abs/2512.07189)
*Jiahao Zhang,Minghui Xu,Hechuan Guo,Xiuzhen Cheng*

Main category: cs.DC

TL;DR: PIR-DSN：首个在去中心化存储网络中集成私有信息检索（PIR）的协议，通过安全映射和文件复制实现隐私保护的文件检索


<details>
  <summary>Details</summary>
Motivation: 去中心化存储网络（DSNs）作为Web 3.0的基础设施，在文件检索过程中存在用户隐私泄露的风险，敏感信息可能被暴露，需要解决这一关键漏洞

Method: 提出PIR-DSN协议，集成私有信息检索（PIR）技术，包括：1）新颖的安全映射方法，将稀疏文件标识符转换为紧凑整数索引；2）支持公共可验证的文件操作；3）通过跨多个矿工的文件复制实现拜占庭鲁棒的私有检索

Result: 实验评估表明：1）文件上传和删除的开销与现有DSN系统相当；2）PIR引入额外计算成本导致检索延迟较高，但吞吐量保持可比；3）在三个主流工业DSN系统上验证了可行性

Conclusion: PIR-DSN为DSN环境中的隐私敏感应用提供了实用的解决方案，首次实现了去中心化存储网络中的私有信息检索，平衡了隐私保护与系统性能

Abstract: Decentralized Storage Networks (DSNs) are emerging as a foundational infrastructure for Web 3.0, offering global peer-to-peer storage. However, a critical vulnerability persists: user privacy during file retrieval remains largely unaddressed, risking the exposure of sensitive information. To overcome this, we introduce PIR-DSN, the first DSN protocol to integrate Private Information Retrieval (PIR) for both single and multi-server settings. Our key innovations include a novel secure mapping method that transforms sparse file identifiers into compact integer indexes, enabling both public verifiability of file operations and efficient private retrieval. Furthermore, PIR-DSN guarantees Byzantine-robust private retrieval through file replication across multiple miners. We implement and rigorously evaluate PIR-DSN against three prominent industrial DSN systems. Experimental results demonstrate that PIR-DSN achieves comparable overhead for file upload and deletion. While PIR inherently introduces an additional computational cost leading to higher retrieval latency, PIR-DSN maintains comparable throughput. These findings underscore PIR-DSN's practical viability for privacy-sensitive applications within DSN environments.

</details>


### [9] [ContinuumConductor : Decentralized Process Mining on the Edge-Cloud Continuum](https://arxiv.org/abs/2512.07280)
*Hendrik Reiter,Janick Edinger,Martin Kabierski,Agnes Koschmider,Olaf Landsiedel,Arvid Lepsien,Xixi Lu,Andrea Marrella,Estefania Serral,Stefan Schulte,Florian Tschorsch,Matthias Weidlich,Wilhelm Hasselbring*

Main category: cs.DC

TL;DR: 提出ContinuumConductor框架，在边缘-云连续体上实现去中心化的流程挖掘，平衡隐私、响应性和资源效率


<details>
  <summary>Details</summary>
Motivation: 传统流程挖掘假设集中式事件数据收集和分析，但现代工业物联网系统在分布式、资源受限的边缘-云基础设施上运行，需要新的去中心化方法

Method: 提出ContinuumConductor分层决策框架，指导何时在流程挖掘管道（预处理、关联、发现等步骤）中执行集中或去中心化操作，分析各层去中心化与集中化的权衡并提出决策标准

Result: 在内河港口流程优化实际用例中演示了ContinuumConductor框架，为网络物理系统和工业物联网系统中的计算感知流程挖掘奠定基础

Conclusion: ContinuumConductor框架为工业物联网边缘-云连续体上的去中心化流程挖掘提供了结构化方法，实现了隐私保护、响应性和资源效率的平衡

Abstract: Process mining traditionally assumes centralized event data collection and analysis. However, modern Industrial Internet of Things systems increasingly operate over distributed, resource-constrained edge-cloud infrastructures. This paper proposes a structured approach for decentralizing process mining by enabling event data to be mined directly within the IoT systems edge-cloud continuum. We introduce ContinuumConductor a layered decision framework that guides when to perform process mining tasks such as preprocessing, correlation, and discovery centrally or decentrally. Thus, enabling privacy, responsive and resource-efficient process mining. For each step in the process mining pipeline, we analyze the trade-offs of decentralization versus centralization across these layers and propose decision criteria. We demonstrate ContinuumConductor at a real-world use-case of process optimazition in inland ports. Our contributions lay the foundation for computing-aware process mining in cyber-physical and IIoT systems.

</details>


### [10] [Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding](https://arxiv.org/abs/2512.07344)
*Shengyuan Ye,Bei Ouyang,Tianyi Qian,Liekang Zeng,Mu Yuan,Xiaowen Chu,Weijie Hong,Xu Chen*

Main category: cs.DC

TL;DR: Venus是一个用于高效在线视频理解的端侧内存与检索系统，采用边云分离架构，通过关键帧选择和渐进采样算法，在保持推理精度的同时实现15-131倍的延迟加速。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在在线视频理解应用中虽然表现出强大的多模态理解能力，但部署时忽略了系统开销问题，导致实际部署中产生过高的系统开销。

Method: 提出边云分离架构，将内存构建和关键帧检索从云端下沉到边缘。包含两个阶段：1) 摄入阶段：通过场景分割和聚类处理流式边缘视频，使用多模态嵌入模型构建分层内存；2) 查询阶段：索引查询并采用基于阈值的渐进采样算法进行关键帧选择，平衡系统成本和推理精度。

Result: 与最先进方法相比，Venus实现了15-131倍的总响应延迟加速，能够在几秒内实现实时响应，同时保持相当甚至更优的推理精度。

Conclusion: Venus系统通过创新的边云分离架构和高效的检索算法，成功解决了在线视频理解应用中的部署开销问题，实现了高效实时的视频理解能力。

Abstract: Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.

</details>


### [11] [Communication-Efficient Serving for Video Diffusion Models with Latent Parallelism](https://arxiv.org/abs/2512.07350)
*Zhiyuan Wu,Shuai Wang,Li Chen,Kaihui Gao,Dan Li,Yanyu Ren,Qiming Zhang,Yong Wang*

Main category: cs.DC

TL;DR: 提出Latent Parallelism (LP)并行策略，通过动态旋转潜在空间中的分区维度来减少视频扩散模型服务时的通信开销，相比基线方法减少高达97%的通信量。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型(VDMs)在3D时空域上进行注意力计算，相比处理1D序列的大语言模型(LLMs)，其内存消耗呈立方级增长，需要跨多个GPU并行服务。传统的并行策略划分计算图，需要频繁的高维激活传输，造成严重的通信瓶颈。

Method: 提出Latent Parallelism (LP)并行策略：1) 利用扩散去噪过程中固有的局部时空依赖性；2) 在扩散时间步中动态旋转潜在空间中的分区维度（时间、高度、宽度），将全局去噪问题分解为可并行子问题；3) 设计补丁对齐的重叠分区策略，使分区边界与视觉补丁匹配；4) 采用位置感知的潜在重建机制实现平滑拼接。

Result: 在三个基准测试上，LP相比基线方法减少通信开销高达97%，同时保持可比的生成质量。LP作为非侵入式插件范式，可以无缝集成到现有并行策略中。

Conclusion: LP是首个针对VDM服务定制的并行策略，通过利用扩散过程的局部依赖性，在紧凑潜在空间中动态旋转分区维度，显著减少通信开销，实现高效可扩展的视频生成服务。

Abstract: Video diffusion models (VDMs) perform attention computation over the 3D spatio-temporal domain. Compared to large language models (LLMs) processing 1D sequences, their memory consumption scales cubically, necessitating parallel serving across multiple GPUs. Traditional parallelism strategies partition the computational graph, requiring frequent high-dimensional activation transfers that create severe communication bottlenecks. To tackle this issue, we exploit the local spatio-temporal dependencies inherent in the diffusion denoising process and propose Latent Parallelism (LP), the first parallelism strategy tailored for VDM serving. \textcolor{black}{LP decomposes the global denoising problem into parallelizable sub-problems by dynamically rotating the partitioning dimensions (temporal, height, and width) within the compact latent space across diffusion timesteps, substantially reducing the communication overhead compared to prevailing parallelism strategies.} To ensure generation quality, we design a patch-aligned overlapping partition strategy that matches partition boundaries with visual patches and a position-aware latent reconstruction mechanism for smooth stitching. Experiments on three benchmarks demonstrate that LP reduces communication overhead by up to 97\% over baseline methods while maintaining comparable generation quality. As a non-intrusive plug-in paradigm, LP can be seamlessly integrated with existing parallelism strategies, enabling efficient and scalable video generation services.

</details>


### [12] [Otus Supercomputer](https://arxiv.org/abs/2512.07401)
*Sadaf Ehtesabi,Manoar Hossain,Tobias Kenter,Andreas Krawinkel,Holger Nitsche,Lukas Ostermann,Christian Plessl,Heinrich Riebler,Stefan Rohde,Robert Schade,Michael Schwarz,Jens Simon,Nils Winnwa,Alex Wiens,Xin Wu*

Main category: cs.DC

TL;DR: Otus是德国帕德博恩大学PC2中心于2025年推出的高性能计算集群，作为NHR计划的一部分，补充了Noctua 2系统，提供约两倍计算能力，在Top500中CPU分区排名164位，GPU分区排名255位，在Green500中GPU分区排名第5位。


<details>
  <summary>Details</summary>
Motivation: 本文旨在为Otus超级计算机提供全面的系统概述，包括硬件、软件、系统集成和能效设计，为使用该系统的科学家和运营HPC集群的其他中心提供独特见解。

Method: 文章采用系统描述性方法，详细介绍了Otus集群的硬件配置（包括三种节点类型：CPU计算节点、高端GPU节点和HPC级FPGA节点）、软件环境、系统集成策略以及数据中心能效优化设计。

Result: Otus在性能方面表现出色：CPU分区在Top500中排名164位，GPU分区排名255位；在能效方面，GPU分区在Green500中排名第5位，展现了卓越的能效表现。

Conclusion: Otus作为Noctua 2的补充系统，成功实现了约两倍的计算能力提升，同时在能效方面达到世界领先水平，为科学计算提供了高性能且节能的计算平台，文章将持续更新以反映最新的系统配置和测量数据。

Abstract: Otus is a high-performance computing cluster that was launched in 2025 and is operated by the Paderborn Center for Parallel Computing (PC2) at Paderborn University in Germany. The system is part of the National High Performance Computing (NHR) initiative. Otus complements the previous supercomputer Noctua 2, offering approximately twice the computing power while retaining the three node types that were characteristic of Noctua 2: 1) CPU compute nodes with different memory capacities, 2) high-end GPU nodes, and 3) HPC-grade FPGA nodes. On the Top500 list, which ranks the 500 most powerful supercomputers in the world, Otus is in position 164 with the CPU partition and in position 255 with the GPU partition (June 2025). On the Green500 list, ranking the 500 most energy-efficient supercomputers in the world, Otus is in position 5 with the GPU partition (June 2025).
  This article provides a comprehensive overview of the system in terms of its hardware, software, system integration, and its overall integration into the data center building to ensure energy-efficient operation. The article aims to provide unique insights for scientists using the system and for other centers operating HPC clusters. The article will be continuously updated to reflect the latest system setup and measurements.

</details>


### [13] [Bandwidth-Aware Network Topology Optimization for Decentralized Learning](https://arxiv.org/abs/2512.07536)
*Yipeng Shen,Zehan Zhu,Yan Huang,Changzhi Yan,Cheng Zhuo,Jinming Xu*

Main category: cs.DC

TL;DR: 提出带宽感知的网络拓扑优化框架，在边数约束下最大化共识速度，通过ADMM方法求解，实验显示在去中心化学习中显著加速训练


<details>
  <summary>Details</summary>
Motivation: 现有网络拓扑设计大多忽略带宽限制，而带宽对分布式学习中的参数同步效率至关重要。需要设计考虑带宽约束的拓扑优化方法以提高共识速度。

Method: 提出带宽感知网络拓扑优化框架，将问题重构为混合整数半定规划问题，采用ADMM方法高效求解。在ADMM子步骤中使用共轭梯度法解决大规模线性方程以提高可扩展性。针对异构带宽场景引入最大带宽分配策略。

Result: 实验结果表明，生成的网络拓扑在共识速度上优于基准拓扑，在真实数据集上去中心化学习任务中，为达到目标测试精度，同构和异构带宽设置下的训练时间分别减少超过1.11倍和1.21倍。

Conclusion: 带宽感知的网络拓扑优化能显著提高分布式学习的参数同步效率，ADMM方法结合共轭梯度法可有效解决大规模拓扑优化问题，在实际应用中带来显著性能提升。

Abstract: Network topology is critical for efficient parameter synchronization in distributed learning over networks. However, most existing studies do not account for bandwidth limitations in network topology design. In this paper, we propose a bandwidth-aware network topology optimization framework to maximize consensus speed under edge cardinality constraints. For heterogeneous bandwidth scenarios, we introduce a maximum bandwidth allocation strategy for the edges to ensure efficient communication among nodes. By reformulating the problem into an equivalent Mixed-Integer SDP problem, we leverage a computationally efficient ADMM-based method to obtain topologies that yield the maximum consensus speed. Within the ADMM substep, we adopt the conjugate gradient method to efficiently solve large-scale linear equations to achieve better scalability. Experimental results demonstrate that the resulting network topologies outperform the benchmark topologies in terms of consensus speed, and reduce the training time required for decentralized learning tasks on real-world datasets to achieve the target test accuracy, exhibiting speedups of more than $1.11\times$ and $1.21\times$ for homogeneous and heterogeneous bandwidth settings, respectively.

</details>


### [14] [A Performance Analyzer for a Public Cloud's ML-Augmented VM Allocator](https://arxiv.org/abs/2512.07750)
*Roozbeh Bostandoost,Pooria Namyar,Siva Kesava Reddy Kakarla,Ryan Beckett,Santiago Segarra,Eli Cortez,Ankur Mallick,Kevin Hsieh,Rodrigo Fonseca,Mohammad Hajiesmaili,Behnaz Arzani*

Main category: cs.DC

TL;DR: SANJESH是一个帮助云系统操作员理解多个机器学习模型如何影响端到端系统性能的工具，通过双层优化支持多种性能查询，并能快速解决先前工作无法在24小时内解决的优化问题。


<details>
  <summary>Details</summary>
Motivation: 当前云系统使用多个机器学习模型来提高效率和性能，但操作员缺乏工具来理解每个模型以及模型之间的交互如何影响端到端系统性能。

Method: SANJESH通过双层优化来回答性能相关查询，并发明了新颖的机制来加速优化求解过程。

Result: SANJESH能够解决先前工作无法在24小时内解决的优化问题。在虚拟机放置的案例中，SANJESH发现了这些模型导致比基于模拟方法检测到的性能差约4倍的情况。

Conclusion: SANJESH是一个有效的工具，能够帮助云系统操作员理解多个机器学习模型对系统性能的影响，并能识别出传统方法无法发现的性能问题。

Abstract: Many operational cloud systems use one or more machine learning models that help them achieve better efficiency and performance. But operators do not have tools to help them understand how each model and the interaction between them affect the end-to-end system performance. SANJESH is such a tool. SANJESH supports a diverse set of performance-related queries which we answer through a bi-level optimization. We invent novel mechanisms to solve this optimization more quickly. These techniques allow us to solve an optimization which prior work failed to solve even after $24$ hours.
  As a proof of concept, we apply SANJESH to an example production system that uses multiple ML models to optimize virtual machine (VM) placement. These models impact how many servers the operators uses to host VMs and the frequency with which it has to live-migrate them because the servers run out of resources. SANJESH finds scenarios where these models cause $~4\times$ worse performance than what simulation-based approaches detect.

</details>


### [15] [Designing Co-operation in Systems of Hierarchical, Multi-objective Schedulers for Stream Processing](https://arxiv.org/abs/2512.07792)
*Animesh Dangwal,Yufeng Jiang,Charlie Arnold,Jun Fan,Mohamed Bassem,Aish Rajagopal*

Main category: cs.DC

TL;DR: Meta开发了一个用于大规模流处理系统的分层负载均衡框架，通过集成新调度器到现有层次结构中，实现对计算资源的主动负载均衡


<details>
  <summary>Details</summary>
Motivation: 随着应用复杂性和用户需求的增长，Meta现有的流处理框架需要更鲁棒和主动的负载均衡机制来处理日益增长的数据量和计算需求

Method: 设计和构建专注于关键计算资源和应用属性的负载均衡系统，将新调度器集成到现有调度器层次结构中，实现多调度器协同工作

Result: 开发了一个能够处理TB级数据的分层负载均衡系统，支持多调度器在各自基础设施层面有效执行负载均衡

Conclusion: 通过分层调度器架构和主动负载均衡机制，Meta能够应对不断增长的应用复杂性，确保大规模流处理系统的高效运行

Abstract: Stream processing is a computing paradigm that supports real-time data processing for a wide variety of applications. At Meta, it's used across the company for various tasks such as deriving product insights, providing and improving user services, and enabling AI at scale for our ever-growing user base. Meta's current stream processing framework supports processing TerraBytes(TBs) of data in mere seconds. This is enabled by our efficient schedulers and multi-layered infrastructure, which allocate workloads across various compute resources, working together in hierarchies across various parts of the infrastructure. But with the ever growing complexity of applications, and user needs, areas of the infrastructure that previously required minimal load balancing, now must be made more robust and proactive to application load. In our work we explore how to build and design such a system that focuses on load balancing over key compute resources and properties of these applications. We also showcase how to integrate new schedulers into the hierarchy of the existing ones, allowing multiple schedulers to work together and perform load balancing, at their infrastructure level, effectively.

</details>


### [16] [Quantifying the Carbon Reduction of DAG Workloads: A Job Shop Scheduling Perspective](https://arxiv.org/abs/2512.07799)
*Roozbeh Bostandoost,Adam Lechowicz,Walid A. Hanafy,Prashant Shenoy,Mohammad Hajiesmaili*

Main category: cs.DC

TL;DR: 依赖感知的批处理调度器相比传统单任务调度器，在保持最优完工时间的同时，平均可降低25%碳排放，允许两倍完工时间时碳减排效果翻倍。


<details>
  <summary>Details</summary>
Motivation: 现有碳感知调度器大多将工作负载视为单一整体任务，忽略了视频编码、离线推理等作业由具有特定依赖关系和资源需求的小任务组成，这种结构知识为提升碳效率提供了机会。

Method: 将问题建模为灵活的作业车间调度变体，使用离线求解器计算碳和能源节约的上限，量化依赖感知方法对批处理工作负载的最大效益。

Result: 结果显示：在不增加最优完工时间的情况下，平均碳排放降低25%；在异构服务器设置中，这些调度可能比能源最优调度消耗更多能源；允许两倍最优完工时间时，碳节约几乎翻倍。

Conclusion: 依赖感知调度能显著降低数据中心碳排放，但存在碳、能源和完工时间之间的权衡关系，作业结构和服务器数量是影响可达成碳减排的关键因素。

Abstract: Carbon-aware schedulers aim to reduce the operational carbon footprint of data centers by running flexible workloads during periods of low carbon intensity. Most schedulers treat workloads as single monolithic tasks, ignoring that many jobs, like video encoding or offline inference, consist of smaller tasks with specific dependencies and resource needs; however, knowledge of this structure enables opportunities for greater carbon efficiency.
  We quantify the maximum benefit of a dependency-aware approach for batch workloads. We model the problem as a flexible job-shop scheduling variant and use an offline solver to compute upper bounds on carbon and energy savings. Results show up to $25\%$ lower carbon emissions on average without increasing the optimal makespan (total job completion time) compared to a makespan-only baseline. Although in heterogeneous server setup, these schedules may use more energy than energy-optimal ones. Our results also show that allowing twice the optimal makespan nearly doubles the carbon savings, underscoring the tension between carbon, energy, and makespan. We also highlight key factors such as job structure and server count influence the achievable carbon reductions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [Compass: Mapping Space Exploration for Multi-Chiplet Accelerators Targeting LLM Inference Serving Workloads](https://arxiv.org/abs/2512.06093)
*Boyu Li,Zongwei Zhu,Yi Xiong,Qianyue Cao,Jiawei Geng,Xiaonan Zhang,Xi Li*

Main category: cs.AR

TL;DR: 提出Compass框架，通过细粒度映射编码和遗传算法搜索，优化LLM推理在多芯片加速器上的部署，相比现有方法平均EDP降低63.12%


<details>
  <summary>Details</summary>
Motivation: 现有映射空间探索主要针对传统CNN/Transformer工作负载，无法充分支持真实LLM推理服务中混合请求类型和可变序列长度的动态行为

Method: 1) 提出基于计算执行图的映射编码方案，解耦微批次和层，实现异构芯片上的细粒度执行控制；2) 开发Compass框架，集成评估引擎和基于遗传算法的映射生成引擎

Result: 相比最先进方法，平均EDP（能量延迟积）降低63.12%

Conclusion: Compass框架通过细粒度映射编码和高效搜索算法，有效解决了LLM推理在多芯片加速器上的动态部署问题，显著提升了能效

Abstract: Large Language Models (LLMs) impose massive computational demands, driving the need for scalable multi-chiplet accelerators. However, existing mapping space exploration efforts for such accelerators primarily focus on traditional CNN/Transformer workloads and fail to adequately support the dynamic behaviors of mixed request types and variable sequence lengths in real-world LLM inference serving. To bridge this gap, we first propose a computation execution graph-based mapping encoding scheme that decouples micro-batches and layers, enabling fine-grained execution control on heterogeneous chiplets and flexibly representing various parallelism strategies. Second, building upon this scheme, we develop the Compass framework, which integrates an evaluation engine and a genetic algorithm-based mapping generation engine to achieve efficient mapping search. Compared to state-of-the-art works, our solution achieves an average EDP reduction of 63.12%.

</details>


### [18] [Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures](https://arxiv.org/abs/2512.06113)
*Bin Xu,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AR

TL;DR: MERINDA是一个基于FPGA加速的模型恢复框架，通过流式数据流管道重构计算，相比FPGA基准实现减少6.3倍周期数，为时间关键物理系统提供实时性能。


<details>
  <summary>Details</summary>
Motivation: 模型恢复是物理AI和实时数字孪生的核心原语，但GPU在执行时效率低下，存在迭代依赖、内核启动开销、内存带宽未充分利用和高数据移动延迟等问题。

Method: 将计算重构为流式数据流管道，利用BRAM分块、定点内核、LUT结构和进位链加法器的并发使用，暴露细粒度空间并行性，同时最小化片外通信。

Result: 在代表性模型恢复工作负载上，MERINDA相比基于FPGA的LTC基准实现减少了最多6.3倍的周期数，为时间关键物理系统实现了实时性能。

Conclusion: MERINDA通过硬件感知的流式数据流设计解决了模型恢复中的性能瓶颈，为物理AI系统提供了高效的FPGA加速解决方案。

Abstract: Model Recovery (MR) is a core primitive for physical AI and real-time digital twins, but GPUs often execute MR inefficiently due to iterative dependencies, kernel-launch overheads, underutilized memory bandwidth, and high data-movement latency. We present MERINDA, an FPGA-accelerated MR framework that restructures computation as a streaming dataflow pipeline. MERINDA exploits on-chip locality through BRAM tiling, fixed-point kernels, and the concurrent use of LUT fabric and carry-chain adders to expose fine-grained spatial parallelism while minimizing off-chip traffic. This hardware-aware formulation removes synchronization bottlenecks and sustains high throughput across the iterative updates in MR. On representative MR workloads, MERINDA delivers up to 6.3x fewer cycles than an FPGA-based LTC baseline, enabling real-time performance for time-critical physical systems.

</details>


### [19] [From PyTorch to Calyx: An Open-Source Compiler Toolchain for ML Accelerators](https://arxiv.org/abs/2512.06177)
*Jiahan Xie,Evan Williams,Adrian Sampson*

Main category: cs.AR

TL;DR: 开发了一个从PyTorch ML模型到可综合SystemVerilog的端到端开源编译器工具链，性能可与Vitis HLS等闭源工业工具相媲美。


<details>
  <summary>Details</summary>
Motivation: 为ML模型硬件加速提供开源解决方案，替代闭源工业工具如Vitis HLS，降低硬件设计门槛并促进开源硬件生态发展。

Method: 基于Allo加速器设计语言、Calyx硬件中间表示和LLVM的CIRCT项目构建工具链，实现内存分区编译器优化以提升内存密集型ML工作负载的并行性。

Result: 实验结果表明，该编译器能有效生成优化的FPGA可实现的硬件设计，性能与Vitis HLS等闭源工业级工具相当。

Conclusion: 成功开发了从PyTorch到SystemVerilog的完整开源编译器工具链，为ML硬件加速提供了可行的开源替代方案，性能达到工业级标准。

Abstract: We present an end-to-end open-source compiler toolchain that targets synthesizable SystemVerilog from ML models written in PyTorch. Our toolchain leverages the accelerator design language Allo, the hardware intermediate representation (IR) Calyx, and the CIRCT project under LLVM. We also implement a set of compiler passes for memory partitioning, enabling effective parallelism in memory-intensive ML workloads. Experimental results demonstrate that our compiler can effectively generate optimized FPGA-implementable hardware designs that perform reasonably well against closed-source industry-grade tools such as Vitis HLS.

</details>


### [20] [SparsePixels: Efficient Convolution for Sparse Data on FPGAs](https://arxiv.org/abs/2512.06208)
*Ho Fung Tsoi,Dylan Rankin,Vladimir Loncar,Philip Harris*

Main category: cs.AR

TL;DR: SparsePixels：针对空间稀疏图像数据的FPGA高效卷积框架，通过仅计算活跃像素实现73倍推理加速


<details>
  <summary>Details</summary>
Motivation: 传统CNN在FPGA上推理时，由于需要对所有输入像素进行密集卷积，导致高延迟和长启动间隔。许多图像数据具有空间稀疏性，大部分计算浪费在空区域上，特别是在需要微秒级延迟的约束环境中。

Method: 提出SparsePixels框架，实现特殊类别的CNN，仅选择性地保留和计算活跃像素子集，忽略其余像素。开发支持稀疏CNN构建和量化感知训练的库，以及用于FPGA部署的HLS实现。

Result: 在中微子物理数据集中，标准CNN推理延迟为48.665μs，而稀疏CNN仅计算不到1%的输入像素，实现73倍加速至0.665μs，资源利用率在片上预算内，仅带来微小性能损失。在类似稀疏图像数据集上也展示出至少一个数量级的加速。

Conclusion: SparsePixels框架为空间稀疏图像数据提供了高效的FPGA卷积解决方案，显著降低推理延迟，适用于CERN大型强子对撞机等现代实验中的触发和数据采集系统。提供了易于采用的工具链支持。

Abstract: Inference of standard CNNs on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value, especially when the image size is large. However, in some image data, input features can be spatially sparse, and semantic information may occupy only a small fraction of the input pixels. In this case most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework for efficient convolution for spatially sparse image data on FPGAs, targeting fast inference applications in constrained environments with latency requirements of microseconds or below. Our approach implements a special class of CNNs that selectively retain and compute on a small subset of pixels that are active while ignoring the rest. We show that, for example, in a neutrino physics dataset for identifying neutrino interactions in LArTPC images that have around 4k input pixels but are naturally very sparse, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $μ$s on an FPGA, whereas a sparse CNN of the same base architecture computing on less than 1% of the input pixels results in a $\times 73$ inference speedup to 0.665 $μ$s, with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. At least one-order-of magnitude speedups with comparable performance are also demonstrated in similar datasets with sparse image patterns. This work aims to benefit future algorithm developments for fast and efficient data readout in modern experiments such as the trigger and data acquisition systems at the CERN Large Hadron Collider. For easy adoption, we have developed a library to support building sparse CNNs with quantization-aware training, as well as an HLS implementation for FPGA deployment.

</details>


### [21] [A 33.6-136.2 TOPS/W Nonlinear Analog Computing-In-Memory Macro for Multi-bit LSTM Accelerator in 65 nm CMOS](https://arxiv.org/abs/2512.06362)
*Junyi Yang,Xinyu Luo,Ye Ke,Zheng Wang,Hongyang Shang,Shuai Dong,Zhengnan Fu,Xiaofeng Yang,Hongjie Liu,Arindam Basu*

Main category: cs.AR

TL;DR: 提出一种结合可重构非线性内存ADC的LSTM加速器，直接在模拟域计算非线性激活，显著提升能效和面积效率


<details>
  <summary>Details</summary>
Motivation: 传统模拟内存计算加速器在处理LSTM网络时，由于大量非线性操作需要数字处理，限制了能效提升

Method: 采用可重构(1-5位)非线性内存ADC，包含：1)双9T位单元支持有符号输入和三元权重；2)RUDC技术提升读取动态范围；3)双电源6T-SRAM阵列优化多比特权重操作

Result: 5位NLIM ADC实现非线性激活近似，平均误差<1 LSB；在12类关键词检测任务中达到92.0%片上推理准确率，系统级能效提升2.2倍，面积效率提升1.6倍

Conclusion: 提出的LSTM加速器通过模拟域直接计算非线性操作，显著提升了能效和面积效率，为RNN加速器设计提供了新思路

Abstract: The energy efficiency of analog computing-in-memory (ACIM) accelerator for recurrent neural networks, particularly long short-term memory (LSTM) network, is limited by the high proportion of nonlinear (NL) operations typically executed digitally. To address this, we propose an LSTM accelerator incorporating an ACIM macro with reconfigurable (1-5 bit) nonlinear in-memory (NLIM) analog-to-digital converter (ADC) to compute NL activations directly in the analog domain using: 1) a dual 9T bitcell with decoupled read/write paths for signed inputs and ternary weight operations; 2) a read-word-line underdrive Cascode (RUDC) technique achieving 2.8X higher read-bitline dynamic range than single-transistor designs (1.4X better over conventional Cascode structure with 7X lower current variation); 3) a dual-supply 6T-SRAM array for efficient multi-bit weight operations and reducing both bitcell count (7.8X) and latency (4X) for 5-bit weight operations. We experimentally demonstrate 5-bit NLIM ADC for approximating NL activations in LSTM cells, achieving average error <1 LSB. Simulation confirms the robustness of NLIM ADC against temperature variations thanks to the replica bias strategy. Our design achieves 92.0% on-chip inference accuracy for a 12-class keyword-spotting task while demonstrating 2.2X higher system-level normalized energy efficiency and 1.6X better normalized area efficiency than state-of-the-art works. The results combine physical measurements of a macro unit-accounting for the majority of LSTM operations (99% linear and 80% nonlinear operations)-with simulations of the remaining components, including additional LSTM and fully connected layers.

</details>


### [22] [Approximate Multiplier Induced Error Propagation in Deep Neural Networks](https://arxiv.org/abs/2512.06537)
*A. M. H. H. Alahakoon,Hassaan Saadat,Darshana Jayasinghe,Sri Parameswaran*

Main category: cs.AR

TL;DR: 该论文提出了一个分析框架，将近似乘法器的统计误差特性与DNN精度损失关联起来，发现误差均值（偏置）是影响精度的主要因素。


<details>
  <summary>Details</summary>
Motivation: DNN严重依赖密集算术运算，近似乘法器可降低硬件能耗，但缺乏对其误差分布如何影响DNN精度的严格数学分析。

Method: 开发分析框架连接AxM误差统计矩与GEMM失真，使用Frobenius范数推导闭式表达式，通过受控误差注入到GEMM和卷积层进行验证。

Result: 预测的失真与观测的精度下降强相关，FPGA上可配置误差的AxM案例研究证实了分析趋势，该框架为硬件模拟提供了轻量级替代方案。

Conclusion: 该框架能快速评估近似乘法器对DNN推理质量的影响，误差均值是影响精度的主要因素，为硬件设计提供了理论指导。

Abstract: Deep Neural Networks (DNNs) rely heavily on dense arithmetic operations, motivating the use of Approximate Multipliers (AxMs) to reduce energy consumption in hardware accelerators. However, a rigorous mathematical characterization of how AxMs error distributions influence DNN accuracy remains underdeveloped. This work presents an analytical framework that connects the statistical error moments of an AxM to the induced distortion in General Matrix Multiplication (GEMM). Using the Frobenius norm of the resulting error matrix, we derive a closed form expression for practical DNN dimensions that demonstrates the distortion is predominantly governed by the multiplier mean error (bias). To evaluate this model in realistic settings, we incorporate controlled error injection into GEMM and convolution layers and examine its effect on ImageNet scale networks. The predicted distortion correlates strongly with the observed accuracy degradation, and an error configurable AxM case study implemented on an FPGA further confirms the analytical trends. By providing a lightweight alternative to behavioral or hardware level simulations, this framework enables rapid estimation of AxM impact on DNN inference quality.

</details>


### [23] [ArchPower: Dataset for Architecture-Level Power Modeling of Modern CPU Design](https://arxiv.org/abs/2512.06854)
*Qijun Zhang,Yao Lu,Mengming Li,Shang Liu,Zhiyao Xie*

Main category: cs.AR

TL;DR: ArchPower是首个用于架构级处理器功耗建模的开源数据集，包含200个CPU数据样本，涵盖25种CPU配置和8种工作负载，提供超过100个架构特征和细粒度功耗标签。


<details>
  <summary>Details</summary>
Motivation: 当前CPU功耗评估需要耗时的IC实现过程，早期设计阶段的传统功耗模型不准确，而基于ML的架构级功耗模型缺乏开源数据集。现有数据集生成过程复杂耗时，且往往不能反映真实的CPU设计场景。

Method: 通过复杂且真实的设计流程收集CPU架构信息作为特征，使用仿真功耗作为真实标签。数据集包含200个CPU数据样本，来自25种不同CPU配置执行8种工作负载的情况，每个样本有超过100个架构特征。

Result: 创建了ArchPower开源数据集，提供细粒度功耗信息：包括总设计功耗和11个组件的功耗，每个功耗值进一步分解为组合逻辑功耗、时序逻辑功耗、存储器功耗和时钟功耗四类。

Conclusion: ArchPower填补了架构级处理器功耗建模领域开源数据集的空白，为ML-based功耗模型研究提供了高质量的真实数据集，有助于提升早期设计阶段的功耗评估准确性。

Abstract: Power is the primary design objective of large-scale integrated circuits (ICs), especially for complex modern processors (i.e., CPUs). Accurate CPU power evaluation requires designers to go through the whole time-consuming IC implementation process, easily taking months. At the early design stage (e.g., architecture-level), classical power models are notoriously inaccurate. Recently, ML-based architecture-level power models have been proposed to boost accuracy, but the data availability is a severe challenge. Currently, there is no open-source dataset for this important ML application. A typical dataset generation process involves correct CPU design implementation and repetitive execution of power simulation flows, requiring significant design expertise, engineering effort, and execution time. Even private in-house datasets often fail to reflect realistic CPU design scenarios. In this work, we propose ArchPower, the first open-source dataset for architecture-level processor power modeling. We go through complex and realistic design flows to collect the CPU architectural information as features and the ground-truth simulated power as labels. Our dataset includes 200 CPU data samples, collected from 25 different CPU configurations when executing 8 different workloads. There are more than 100 architectural features in each data sample, including both hardware and event parameters. The label of each sample provides fine-grained power information, including the total design power and the power for each of the 11 components. Each power value is further decomposed into four fine-grained power groups: combinational logic power, sequential logic power, memory power, and clock power. ArchPower is available at https://github.com/hkust-zhiyao/ArchPower.

</details>


### [24] [DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management](https://arxiv.org/abs/2512.07312)
*Zhongchun Zhou,Chengtao Lai,Yuhang Gu,Wei Zhang*

Main category: cs.AR

TL;DR: 该论文提出了一种面向大语言模型加速器的共享系统级缓存架构，通过应用感知的管理策略（包括缓存替换、死块预测和旁路决策）来简化编程并提升性能，相比传统缓存架构实现了最高1.8倍的加速。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速采用，AI加速器设计趋向于更强大和专业化，但复杂的层次化暂存器内存及其异步管理增加了软件开发难度。本文探索相反的设计方向：采用共享系统级缓存和简单编程模型，同时保持高性能。

Method: 提出多核AI加速器架构，配备共享系统级缓存和应用感知管理策略。利用软件栈中的数据流信息指导缓存替换（包括死块预测），结合旁路决策和缓解缓存颠簸的机制。通过周期精确模拟器评估，建立考虑实际重叠行为的分析模型，并在RTL中实现设计。

Result: 相比传统缓存架构，实现了最高1.80倍的性能提升。旁路和颠簸缓解策略能有效处理有无核间数据共享的场景。RTL实现面积0.064mm²（15nm工艺），可运行在2GHz时钟频率。分析模型成功将测量结果扩展到更大规模工作负载。

Conclusion: 共享缓存设计展示了简化AI加速器编程同时保持高性能的潜力，为未来AI加速器系统开发提供了有前景的方向，平衡了编程复杂度和性能需求。

Abstract: The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.
  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.
  Finally, we implement the design in RTL and the area of our design is $\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.

</details>


### [25] [aLEAKator: HDL Mixed-Domain Simulation for Masked Hardware \& Software Formal Verification](https://arxiv.org/abs/2512.07520)
*Noé Amiot,Quentin L. Meunier,Karine Heydemann,Emmanuelle Encrenaz*

Main category: cs.AR

TL;DR: aLEAKator：首个开源框架，用于从HDL描述自动形式化验证掩码加密加速器和CPU软件，支持多种泄漏模型和可变信号粒度


<details>
  <summary>Details</summary>
Motivation: 现有验证方法存在局限性：要么仅限于小型硬件模块或CPU上的小程序（如S盒），要么受限于泄漏模型，或需要硬件特定先验知识。在考虑毛刺、转换和CPU微架构特性的高级泄漏模型下，验证掩码硬件和软件实现的安全性仍然是一个重大挑战

Method: 提出aLEAKator框架，采用混合域仿真方法，能够精确建模和验证各种（包括鲁棒和宽松的）1探测泄漏模型，支持可变信号粒度而不限于1位线。支持查找表存在时的验证，且不需要目标CPU架构的先验知识

Result: 验证了与现有工具和实际测量的一致性，并提供了创新性结果，如在各种CPU上验证完整的一阶掩码AES实现

Conclusion: aLEAKator是一个强大的开源验证框架，能够克服现有方法的局限性，为掩码加密硬件和软件实现提供全面、自动化的形式化验证能力

Abstract: Verifying the security of masked hardware and software implementations, under advanced leakage models, remains a significant challenge, especially then accounting for glitches, transitions and CPU micro-architectural specifics. Existing verification approaches are either restricted to small hardware gadgets, small programs on CPUs such as Sboxes, limited leakage models, or require hardware-specific prior knowledge. In this work, we present aLEAKator, an open-source framework for the automated formal verification of masked cryptographic accelerators and software running on CPUs from their HDL descriptions. Our method introduces mixed-domain simulation, enabling precise modeling and verification under various (including robust and relaxed) 1-probing leakage models, and supports variable signal granularity without being restricted to 1-bit wires. aLEAKator also supports verification in the presence of lookup tables, and does not require prior knowledge of the target CPU architecture. Our approach is validated against existing tools and real-world measurements while providing innovative results such as the verification of a full, first-order masked AES on various CPUs

</details>


### [26] [Análisis de rendimiento y eficiencia energética en el cluster Raspberry Pi Cronos](https://arxiv.org/abs/2512.07622)
*Martha Semken,Mariano Vargas,Ignacio Tula,Giuliana Zorzoli,Andrés Rojas Paredes*

Main category: cs.AR

TL;DR: 评估基于Raspberry Pi4和3b的Cronos集群在HPL基准测试下的计算性能与能效，分析不同节点配置的扩展性、稳定性和功耗表现。


<details>
  <summary>Details</summary>
Motivation: 为教育目的设计低成本ARM集群，评估其在计算密集型工作负载下的性能、稳定性和能效，为教育研究环境提供实用参考。

Method: 使用High Performance Linpack (HPL)基准测试，在配置Slurm资源管理和Open MPI并行通信的环境中，对不同节点配置（同构和异构）进行实验测试。

Result: 6个Raspberry Pi4节点的同构配置达到6.91 GFLOPS性能；异构节点（包含Pi3b）会降低稳定性和效率；测量了系统总功耗并计算了性能功耗比(GFLOPS/W)。

Conclusion: 该研究为低成本ARM集群在教育研究环境中的设计、评估和使用提供了具体贡献，展示了其在教育场景下的可行性和局限性。

Abstract: This article presents an evaluation of the computational performance and energy efficiency of the Cronos cluster, composed of Raspberry Pi4 and 3b microcomputers designed for educational purposes. Experimental tests were performed using the High Performance Linpack (HPL) benchmark, under a resource management environment configured with Slurm and parallel communication via Open MPI. The study focuses on analyzing scalability, stability, and power consumption during the execution of computationally intensive workloads, considering different node configurations. The results show that the cluster achieves a performance of up to 6.91 GFLOPS in homogeneous configurations of 6 Raspberry Pi 4 nodes, and that the use of heterogeneous nodes (including Raspberry Pi 3b) can negatively impact stability and efficiency. Additionally, the total electrical consumption of the system was measured during the runs, allowing for the estimation of the performance-to-consumption ratio (GFLOPS/W) as a comparative metric. This study constitutes a concrete contribution to the design, evaluation, and utilization of low-cost ARM clusters in educational and research contexts.

</details>
