{"id": "2508.16592", "categories": ["cs.DC", "cs.MS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.16592", "abs": "https://arxiv.org/abs/2508.16592", "authors": ["Gregor Corbin"], "title": "Performance measurements of modern Fortran MPI applications with Score-P", "comment": null, "summary": "Version 3.0 of the Message-Passing Interface (MPI) standard, released in\n2012, introduced a new set of language bindings for Fortran 2008. By making use\nof modern language features and the enhanced interoperability with C, there was\nfinally a type safe and standard conforming method to call MPI from Fortran.\nThis highly recommended use mpi_f08 language binding has since then been widely\nadopted among developers of modern Fortran applications. However, tool support\nfor the F08 bindings is still lacking almost a decade later, forcing users to\nrecede to the less safe and convenient interfaces. Full support for the F08\nbindings was added to the performance measurement infrastructure Score-P by\nimplementing MPI wrappers in Fortran. Wrappers cover the latest MPI standard\nversion 4.1 in its entirety, matching the features of the C wrappers. By\nimplementing the wrappers in modern Fortran, we can provide full support for\nMPI procedures passing attributes, info objects, or callbacks. The\nimplementation is regularly tested under the MPICH test suite. The new F08\nwrappers were already used by two fluid dynamics simulation codes -- Neko, a\nspectral finite-element code derived from Nek5000, and EPIC (Elliptical\nParcel-In-Cell) -- to successfully generate performance measurements. In this\nwork, we additionally present our design considerations and sketch out the\nimplementation, discussing the challenges we faced in the process. The key\ncomponent of the implementation is a code generator that produces approximately\n50k lines of MPI wrapper code to be used by Score-P, relying on the Python\npympistandard module to provide programmatic access to the extracted data from\nthe MPI standard."}
{"id": "2508.16639", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.16639", "abs": "https://arxiv.org/abs/2508.16639", "authors": ["Louie Sinadjan"], "title": "GPU Acceleration for Faster Evolutionary Spatial Cyclic Game Systems", "comment": null, "summary": "This dissertation presents the design, implementation and evaluation of\nGPU-accelerated simulation frameworks for Evolutionary Spatial Cyclic Games\n(ESCGs), a class of agent-based models used to study ecological and\nevolutionary dynamics. Traditional single-threaded ESCG simulations are\ncomputationally expensive and scale poorly. To address this, high-performance\nimplementations were developed using Apple's Metal and Nvidia's CUDA, with a\nvalidated single-threaded C++ version serving as a baseline comparison point.\n  Benchmarking results show that GPU acceleration delivers significant\nspeedups, with the CUDA maxStep implementation achieving up to a 28x\nimprovement. Larger system sizes, up to 3200x3200, became tractable, while\nMetal faced scalability limits. The GPU frameworks also enabled replication and\ncritical extension of recent ESCG studies, revealing sensitivities to system\nsize and runtime not fully explored in prior work.\n  Overall, this project provides a configurable ESCG simulation platform that\nadvances the computational toolkit for this field of research. This\ndissertation forms the basis for a paper accepted for publication and\npresentation at the European Modelling and Simulation Symposium."}
{"id": "2508.16646", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.16646", "abs": "https://arxiv.org/abs/2508.16646", "authors": ["Zhixiang Wei", "James Yen", "Jingyi Chen", "Ziyang Zhang", "Zhibai Huang", "Chen Chen", "Xingzi Yu", "Yicheng Gu", "Chenggang Wu", "Yun Wang", "Mingyuan Xia", "Jie Wu", "Hao Wang", "Zhengwei Qi"], "title": "Equinox: Holistic Fair Scheduling in Serving Large Language Models", "comment": null, "summary": "We address the limitations of current LLM serving with a dual-counter\nframework separating user and operator perspectives. The User Fairness Counter\nmeasures quality of service via weighted tokens and latency; the Resource\nFairness Counter measures operational efficiency through throughput and GPU\nutilization. Since these metrics are only available post-execution, creating a\nscheduling paradox, we introduce a deterministic Mixture of Prediction Experts\n(MoPE) framework to predict user-perceived latency, output tokens, throughput,\nand GPU utilization. These predictions enable calculation of a unified Holistic\nFairness score that balances both counters through tunable parameters for\nproactive fairness-aware scheduling. We implement this in Equinox, an\nopen-source system with other optimizations like adaptive batching, and\nstall-free scheduling. Evaluations on production traces (ShareGPT, LMSYS) and\nsynthetic workloads demonstrate Equinox achieves up to $1.3\\times$ higher\nthroughput, 60\\% lower time-to-first-token latency, and 13\\% higher fairness\nversus VTC while maintaining 94\\% GPU utilization, proving fairness under\nbounded discrepancy across heterogeneous platforms."}
{"id": "2508.16792", "categories": ["cs.DC", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.16792", "abs": "https://arxiv.org/abs/2508.16792", "authors": ["Felix Wang", "Bradley H. Theilman", "Fred Rothganger", "William Severa", "Craig M. Vineyard", "James B. Aimone"], "title": "Neuromorphic Simulation of Drosophila Melanogaster Brain Connectome on Loihi 2", "comment": null, "summary": "We demonstrate the first-ever nontrivial, biologically realistic connectome\nsimulated on neuromorphic computing hardware. Specifically, we implement the\nwhole-brain connectome of the adult Drosophila melanogaster (fruit fly) from\nthe FlyWire Consortium containing 140K neurons and 50M synapses on the Intel\nLoihi 2 neuromorphic platform. This task is particularly challenging due to the\ncharacteristic connectivity structure of biological networks. Unlike artificial\nneural networks and most abstracted neural models, real biological circuits\nexhibit sparse, recurrent, and irregular connectivity that is poorly suited to\nconventional computing methods intended for dense linear algebra. Though\nneuromorphic hardware is architecturally better suited to discrete event-based\nbiological communication, mapping the connectivity structure to frontier\nsystems still faces challenges from low-level hardware constraints, such as\nfan-in and fan-out memory limitations. We describe solutions to these\nchallenges that allow for the full FlyWire connectome to fit onto 12 Loihi 2\nchips. We statistically validate our implementation by comparing network\nbehavior across multiple reference simulations. Significantly, we achieve a\nneuromorphic implementation that is orders of magnitude faster than numerical\nsimulations on conventional hardware, and we also find that performance\nadvantages increase with sparser activity. These results affirm that today's\nscalable neuromorphic platforms are capable of implementing and accelerating\nbiologically realistic models -- a key enabling technology for advancing\nneuro-inspired AI and computational neuroscience."}
{"id": "2508.16584", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.16584", "abs": "https://arxiv.org/abs/2508.16584", "authors": ["Zhongling Su", "Rong Fu", "Weihan Cao", "Jianfei Gao", "Minxi Jin", "Zhilin Pei", "Hui Wang"], "title": "TMA-Adaptive FP8 Grouped GEMM: Eliminating Padding Requirements in Low-Precision Training and Inference on Hopper", "comment": null, "summary": "Current FP8 grouped GEMM implementations require padding each group to a\nfixed alignment (e.g., 128), incurring memory and computational overhead. We\npropose \\textit{TMA-Adaptive FP8 Grouped GEMM}, which eliminates padding by\ndynamically adapting to variable group dimensions via (1) a TMA descriptor pool\nwith $\\log_2(block_M)$ preconfigured descriptors to handle all residual row\ncases through dynamic runtime selection and dual-phase load-store operations,\nachieving comprehensive coverage with minimal overhead, and (2)\nTMA-alignment-aware management to satisfy 16-byte global memory alignment and\n128-byte shared memory alignment. Experiments demonstrate 1.7\\% to 20.4\\% speed\nup with up to 23.8\\% memory reduction compared to padding operation plus\nstate-of-the-art FP8 grouped GEMM, while maintaining full numerical equivalence\nfor valid data. The source code is publicly available at an anonymous\nrepository: https://github.com/sukoncon/TMA-Adaptive-FP8-Grouped-GEMM."}
{"id": "2508.16746", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.16746", "abs": "https://arxiv.org/abs/2508.16746", "authors": ["Karuna Grewal", "P. Brighten Godfrey", "Justin Hsu"], "title": "SafeTree: Expressive Tree Policies for Microservices", "comment": null, "summary": "A microservice-based application is composed of multiple self-contained\ncomponents called microservices, and controlling inter-service communication is\nimportant for enforcing safety properties. Presently, inter-service\ncommunication is configured using microservice deployment tools. However, such\ntools only support a limited class of single-hop policies, which can be overly\npermissive because they ignore the rich service tree structure of microservice\ncalls. Policies that can express the service tree structure can offer\ndevelopment and security teams more fine-grained control over communication\npatterns.\n  To this end, we design an expressive policy language to specify service tree\nstructures, and we develop a visibly pushdown automata-based dynamic\nenforcement mechanism to enforce service tree policies. Our technique is\nnon-invasive: it does not require any changes to service implementations, and\ndoes not require access to microservice code. To realize our method, we build a\nruntime monitor on top of a service mesh, an emerging network infrastructure\nlayer that can control inter-service communication during deployment. In\nparticular, we employ the programmable network traffic filtering capabilities\nof Istio, a popular service mesh implementation, to implement an online and\ndistributed monitor. Our experiments show that our monitor can enforce rich\nsafety properties while adding minimal latency overhead on the order of\nmilliseconds."}
{"id": "2508.16809", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.16809", "abs": "https://arxiv.org/abs/2508.16809", "authors": ["Saverio Pasqualoni", "Lorenzo Piarulli", "Daniele De Sensi"], "title": "PICO: Performance Insights for Collective Operations", "comment": null, "summary": "Collective operations are cornerstones of both HPC application and\nlarge-scale AI training and inference. Yet, comprehensive, systematic and\nreproducible performance evaluation and benchmarking of said operations is not\nstraightforward. Existing frameworks do not provide sufficiently detailed\nprofiling information, nor they ensure reproducibility and extensibility. In\nthis paper, we present PICO (Performance Insights for Collective Operations), a\nnovel lightweight, extensible framework built with the aim of simplifying\ncollective operations benchmarking."}
{"id": "2508.16700", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.16700", "abs": "https://arxiv.org/abs/2508.16700", "authors": ["Deepak Kumar", "Divakar Yadav", "Yash Patel"], "title": "GPT-OSS-20B: A Comprehensive Deployment-Centric Analysis of OpenAI's Open-Weight Mixture of Experts Model", "comment": null, "summary": "We present a single-GPU (H100, bf16) evaluation of GPT-OSS-20B\n(Mixture-of-Experts; 20.9B total, approx. 3.61B active) against dense baselines\nQwen3-32B and Yi-34B across multiple dimensions. We measure true\ntime-to-first-token (TTFT), full-decode throughput (TPOT), end-to-end latency\npercentiles, peak VRAM with past key values (PKV) held, and energy via a\nconsistent nvidia-smi-based sampler. At a 2048-token context with 64-token\ndecode, GPT-OSS-20B delivers higher decode throughput and tokens per Joule than\ndense baselines Qwen3-32B and Yi-34B, while substantially reducing peak VRAM\nand energy per 1000 generated tokens; its TTFT is higher due to MoE routing\noverhead. With only 17.3% of parameters active (3.61B of 20.9B), GPT-OSS-20B\nprovides about 31.8% higher decode throughput and 25.8% lower energy per 1000\ngenerated tokens than Qwen3-32B at 2048/64, while using 31.7% less peak VRAM.\nNormalized by active parameters, GPT-OSS-20B shows markedly stronger\nper-active-parameter efficiency (APE), underscoring MoE's deployment\nadvantages. We do not evaluate accuracy; this is a deployment-focused study. We\nrelease code and consolidated results to enable replication and extension."}
{"id": "2508.16848", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.16848", "abs": "https://arxiv.org/abs/2508.16848", "authors": ["David Moon", "Andrew Blinn", "Thomas J. Porter", "Cyrus Omar"], "title": "Syntactic Completions with Material Obligations", "comment": null, "summary": "Code editors provide essential services that help developers understand,\nnavigate, and modify programs. However, these services often fail in the\npresence of syntax errors. Existing syntax error recovery techniques, like\npanic mode and multi-option repairs, are either too coarse, e.g. in deleting\nlarge swathes of code, or lead to a proliferation of possible completions. This\npaper introduces $\\texttt{tylr}$, a parser and editor generator that completes\narbitrarily malformed code by inserting obligations, which generalize holes to\ncover missing operands, operators, mixfix keywords, and sort transitions.\n$\\texttt{tylr}$ is backed by a novel theory of tile-based parsing, which\nextends operator-precedence parsing in two ways. First, traditional token\nprecedence comparisons are replaced by a notion of grammar walks, which form\nthe basis for generating obligations. Second, a distinct \"molding\" system based\non grammar zippers expand grammar expressivity by allowing the system to\ndisambiguate between possible parses and completions based on an obligation\nminimization criterion. In addition to serving as a novel approach to error\ncorrection, $\\texttt{tylr}$'s design enables the development of an editor that\nvisually materializes obligations to the human user, serving as a novel hybrid\nbetween a text editor and a structure editor. We introduce $\\texttt{tylr}$ by\nexample, then formalize its key ideas. Finally, we conduct a human subjects\nstudy to evaluate the extent to which an editor like $\\texttt{tylr}$ that\nmaterializes syntactic obligations might be usable and useful, finding both\npoints of positivity and interesting new avenues for future work."}
{"id": "2508.17209", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.17209", "abs": "https://arxiv.org/abs/2508.17209", "authors": ["Yebo Wu", "Jingguang Li", "Chunlin Tian", "Zhijiang Guo", "Li Li"], "title": "Memory-Efficient Federated Fine-Tuning of Large Language Models via Layer Pruning", "comment": null, "summary": "Federated fine-tuning enables privacy-preserving Large Language Model (LLM)\nadaptation, but its high memory cost limits participation from\nresource-constrained devices. We propose FedPruner, an innovative federated\nfine-tuning paradigm that tackles this via intelligent layer pruning. FedPruner\nflexibly prunes the global model, creating personalized submodels based on\ndevice memory constraints. It employs a macro-micro synergistic pruning\nframework: a macro-level functionality-driven layer orchestration mechanism\ngroups layers, while a micro-level importance-aware layer selection strategy\nprunes within groups to build device-specific submodels. We further introduce a\nfine-grained variant that independently prunes Multi-Head Attention and\nFeed-Forward Network components to precisely preserve critical architectural\nelements. Extensive experimental results demonstrate that FedPruner\nsignificantly outperforms state-of-the-art approaches, achieving up to a 1.98\\%\nimprovement in average model accuracy while reducing peak memory usage by 75\\%."}
{"id": "2508.16738", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.16738", "abs": "https://arxiv.org/abs/2508.16738", "authors": ["Alhad Daftardar", "Jianqiao Mo", "Joey Ah-kiow", "Benedikt Bünz", "Siddharth Garg", "Brandon Reagen"], "title": "zkPHIRE: A Programmable Accelerator for ZKPs over HIgh-degRee, Expressive Gates", "comment": "14 pages, 14 figures", "summary": "Zero-Knowledge Proofs (ZKPs) have emerged as powerful tools for secure and\nprivacy-preserving computation. ZKPs enable one party to convince another of a\nstatement's validity without revealing anything else. This capability has\nprofound implications in many domains, including: machine learning, blockchain,\nimage authentication, and electronic voting. Despite their potential, ZKPs have\nseen limited deployment because of their exceptionally high computational\noverhead, which manifests primarily during proof generation. To mitigate these\noverheads, a (growing) body of researchers has proposed hardware accelerators\nand GPU implementations for kernels and complete protocols. Prior art spans a\nwide variety of ZKP schemes that vary significantly in computational overhead,\nproof size, verifier cost, protocol setup, and trust. The latest, and widely\nused ZKP protocols are intentionally designed to balance these trade-offs. A\nparticular challenge in modern ZKP systems is supporting complex, high-degree\ngates using the SumCheck protocol. We address this challenge with a novel\nprogrammable accelerator that efficiently handles arbitrary custom gates via\nSumCheck. Our accelerator achieves upwards of $1000\\times$ geomean speedup over\nCPU-based SumChecks across a range of gate types. We integrate this unit into a\nfull-system accelerator, zkPHIRE, which achieves $1486\\times$ geomean speedup\nover CPU and $11.87\\times$ speedup over the state-of-the-art at iso-area.\nzkPHIRE is the first accelerator to scale to problem sizes of $2^{30}$ nominal\nconstraints while maintaining small proof sizes and programmability."}
{"id": "2508.17219", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.17219", "abs": "https://arxiv.org/abs/2508.17219", "authors": ["Bingyang Wu", "Zili Zhang", "Yinmin Zhong", "Guanzhe Huang", "Yibo Zhu", "Xuanzhe Liu", "Xin Jin"], "title": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained Elastic Long-Context LLM Serving", "comment": null, "summary": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively."}
{"id": "2508.16959", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.16959", "abs": "https://arxiv.org/abs/2508.16959", "authors": ["Simone Machetti", "Pasquale Davide Schiavone", "Giovanni Ansaloni", "Miguel Peón-Quirós", "David Atienza"], "title": "X-HEEP: An Open-Source, Configurable and Extendible RISC-V Platform for TinyAI Applications", "comment": null, "summary": "In this work, we present X-HEEP, an open-source, configurable, and extendible\nRISC-V platform for ultra-low-power edge applications (TinyAI). X-HEEP features\nthe eXtendible Accelerator InterFace (XAIF), which enables seamless integration\nof accelerators with varying requirements along with an extensive internal\nconfiguration of cores, memory, bus, and peripherals. Moreover, it supports\nvarious development flows, including FPGA prototyping, ASIC implementation, and\nmixed SystemC-RTL modeling, enabling efficient exploration and optimization.\nImplemented in TSMC's 65 nm CMOS technology (300 MHz, 0.8 V), X-HEEP achieves a\nminimal footprint of only 0.15 mm2 and consumes just 29 uW of leakage power. As\na demonstrator of the configurability and low overhead of X-HEEP as a host\nplatform, we present a study integrating it with near-memory accelerators\ntargeting early-exit dynamic network applications, achieving up to 7.3 x\nperformance speedup and 3.6 x energy improvement on the resulting heterogeneous\nsystem compared to CPU-only execution."}
{"id": "2508.17311", "categories": ["cs.DC", "cs.AI", "cs.PF", "C.2.4; C.5.1"], "pdf": "https://arxiv.org/pdf/2508.17311", "abs": "https://arxiv.org/abs/2508.17311", "authors": ["Daniele De Sensi", "Saverio Pasqualoni", "Lorenzo Piarulli", "Tommaso Bonato", "Seydou Ba", "Matteo Turisini", "Jens Domke", "Torsten Hoefler"], "title": "Bine Trees: Enhancing Collective Operations by Optimizing Communication Locality", "comment": null, "summary": "Communication locality plays a key role in the performance of collective\noperations on large HPC systems, especially on oversubscribed networks where\ngroups of nodes are fully connected internally but sparsely linked through\nglobal connections. We present Bine (binomial negabinary) trees, a family of\ncollective algorithms that improve communication locality. Bine trees maintain\nthe generality of binomial trees and butterflies while cutting global-link\ntraffic by up to 33%. We implement eight Bine-based collectives and evaluate\nthem on four large-scale supercomputers with Dragonfly, Dragonfly+,\noversubscribed fat-tree, and torus topologies, achieving up to 5x speedups and\nconsistent reductions in global-link traffic across different vector sizes and\nnode counts."}
{"id": "2508.16981", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.16981", "abs": "https://arxiv.org/abs/2508.16981", "authors": ["Simone Machetti", "Deniz Kasap", "Juan Sapriza", "Rubén Rodríguez Álvarez", "Hossein Taji", "José Miranda", "Miguel Peón-Quirós", "David Atienza"], "title": "Invited Paper: FEMU: An Open-Source and Configurable Emulation Framework for Prototyping TinyAI Heterogeneous Systems", "comment": null, "summary": "In this paper, we present the new FPGA EMUlation (FEMU), an open-source and\nconfigurable emulation framework for prototyping and evaluating TinyAI\nheterogeneous systems (HS). FEMU leverages the capability of system-on-chip\n(SoC)-based FPGAs to combine the under-development HS implemented in a\nreconfigurable hardware region (RH) for quick prototyping with a software\nenvironment running under a standard operating system in a control software\nregion (CS) for supervision and communication. To evaluate our approach, we\nbuilt the X-HEEP FPGA EMUlation (X-HEEP-FEMU) platform by instantiating the\nproposed framework with real-world hardware and software components.\nX-HEEP-FEMU is deployed on the Xilinx Zynq-7020 SoC and integrates the\neXtendible Heterogeneous Energy Efficient Platform (X-HEEP) host in the RH, a\nLinux-based Python environment on the ARM Cortex-A9 CS, and energy models\nderived from a TSMC 65 nm CMOS silicon implementation of X-HEEP, called\nHEEPocrates."}
{"id": "2508.17493", "categories": ["cs.DC", "cs.CE", "cs.MS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.17493", "abs": "https://arxiv.org/abs/2508.17493", "authors": ["Jeremy Kepner", "Chansup Byun", "LaToya Anderson", "William Arcand", "David Bestor", "William Bergeron", "Alex Bonn", "Daniel Burrill", "Vijay Gadepally", "Ryan Haney", "Michael Houle", "Matthew Hubbell", "Hayden Jananthan", "Michael Jones", "Piotr Luszczek", "Lauren Milechin", "Guillermo Morales", "Julie Mullen", "Andrew Prout", "Albert Reuther", "Antonio Rosa", "Charles Yee", "Peter Michaleas"], "title": "Easy Acceleration with Distributed Arrays", "comment": "8 pages, 4 figures, 2 tables, 2 algorithm listings, 2 code listings,\n  to appear in IEEE HPEC 2025", "summary": "High level programming languages and GPU accelerators are powerful enablers\nfor a wide range of applications. Achieving scalable vertical (within a compute\nnode), horizontal (across compute nodes), and temporal (over different\ngenerations of hardware) performance while retaining productivity requires\neffective abstractions. Distributed arrays are one such abstraction that\nenables high level programming to achieve highly scalable performance.\nDistributed arrays achieve this performance by deriving parallelism from data\nlocality, which naturally leads to high memory bandwidth efficiency. This paper\nexplores distributed array performance using the STREAM memory bandwidth\nbenchmark on a variety of hardware. Scalable performance is demonstrated within\nand across CPU cores, CPU nodes, and GPU nodes. Horizontal scaling across\nmultiple nodes was linear. The hardware used spans decades and allows a direct\ncomparison of hardware improvements for memory bandwidth over this time range;\nshowing a 10x increase in CPU core bandwidth over 20 years, 100x increase in\nCPU node bandwidth over 20 years, and 5x increase in GPU node bandwidth over 5\nyears. Running on hundreds of MIT SuperCloud nodes simultaneously achieved a\nsustained bandwidth $>$1 PB/s."}
{"id": "2508.17069", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.17069", "abs": "https://arxiv.org/abs/2508.17069", "authors": ["Mengyuan Yin", "Benjamin Chen Ming Choong", "Chuping Qu", "Rick Siow Mong Goh", "Weng-Fai Wong", "Tao Luo"], "title": "Optimizing Neural Networks with Learnable Non-Linear Activation Functions via Lookup-Based FPGA Acceleration", "comment": null, "summary": "Learned activation functions in models like Kolmogorov-Arnold Networks (KANs)\noutperform fixed-activation architectures in terms of accuracy and\ninterpretability; however, their computational complexity poses critical\nchallenges for energy-constrained edge AI deployments. Conventional CPUs/GPUs\nincur prohibitive latency and power costs when evaluating higher order\nactivations, limiting deployability under ultra-tight energy budgets. We\naddress this via a reconfigurable lookup architecture with edge FPGAs. By\ncoupling fine-grained quantization with adaptive lookup tables, our design\nminimizes energy-intensive arithmetic operations while preserving activation\nfidelity. FPGA reconfigurability enables dynamic hardware specialization for\nlearned functions, a key advantage for edge systems that require\npost-deployment adaptability. Evaluations using KANs - where unique activation\nfunctions play a critical role - demonstrate that our FPGA-based design\nachieves superior computational speed and over $10^4$ times higher energy\nefficiency compared to edge CPUs and GPUs, while maintaining matching accuracy\nand minimal footprint overhead. This breakthrough positions our approach as a\npractical enabler for energy-critical edge AI, where computational intensity\nand power constraints traditionally preclude the use of adaptive activation\nnetworks."}
{"id": "2508.17593", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.17593", "abs": "https://arxiv.org/abs/2508.17593", "authors": ["Aadesh Deshmukh", "Venkata Yaswanth Raparti", "Samuel Hsu"], "title": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD NPUs", "comment": null, "summary": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches."}
{"id": "2508.17562", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.17562", "abs": "https://arxiv.org/abs/2508.17562", "authors": ["Shota Konno", "Che-Kai Liu", "Sigang Ryu", "Samuel Spetalnick", "Arijit Raychowdhury"], "title": "A 28nm 1.80Mb/mm2 Digital/Analog Hybrid SRAM-CIM Macro Using 2D-Weighted Capacitor Array for Complex Number Mac Operations", "comment": "Asian Solid-State Circuits Conference (A-SSCC) 2025", "summary": "A 28nm dense 6T-SRAM Digital(D)/Analog(A) Hybrid compute-in-memory (CIM)\nmacro supporting complex num-ber MAC operation is presented. By introducing a\n2D-weighted Capacitor Array, a hybrid configuration is adopted where digital\nCIM is applied only to the upper bits and ana-log CIM is applied to the rest,\nwithout the need for input DACs resulting in improved accuracy and lower area\noverhead. The CIM prototype macro achieves 1.80 Mb/mm2 memory density and\n0.435% RMS error. Complex CIM unit outputs real and imaginary part with a\nsingle conversion to reduce latency."}
{"id": "2508.17624", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.17624", "abs": "https://arxiv.org/abs/2508.17624", "authors": ["Ge Shi", "Hanieh Sadri", "Qian Wang", "Yu Zhang", "Ying Xiong", "Yong Zhang", "Zhenan Fan"], "title": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters at Scale", "comment": null, "summary": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon."}
{"id": "2508.17820", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.17820", "abs": "https://arxiv.org/abs/2508.17820", "authors": ["Tingyu Ding", "Qunsong Zeng", "Kaibin Huang"], "title": "In-Memory Computing Enabled Deep MIMO Detection to Support Ultra-Low-Latency Communications", "comment": null, "summary": "The development of sixth-generation (6G) mobile networks imposes\nunprecedented latency and reliability demands on multiple-input multiple-output\n(MIMO) communication systems, a key enabler of high-speed radio access.\nRecently, deep unfolding-based detectors, which map iterative algorithms onto\nneural network architectures, have emerged as a promising approach, combining\nthe strengths of model-driven and data-driven methods to achieve high detection\naccuracy with relatively low complexity. However, algorithmic innovation alone\nis insufficient; software-hardware co-design is essential to meet the extreme\nlatency requirements of 6G (i.e., 0.1 milliseconds). This motivates us to\npropose leveraging in-memory computing, which is an analog computing technology\nthat integrates memory and computation within memristor circuits, to perform\nthe intensive matrix-vector multiplication (MVM) operations inherent in deep\nMIMO detection at the nanosecond scale. Specifically, we introduce a novel\narchitecture, called the deep in-memory MIMO (IM-MIMO) detector, characterized\nby two key features. First, each of its cascaded computational blocks is\ndecomposed into channel-dependent and channel-independent neural network\nmodules. Such a design minimizes the latency of memristor reprogramming in\nresponse to channel variations, which significantly exceeds computation time.\nSecond, we develop a customized detector-training method that exploits prior\nknowledge of memristor-value statistics to enhance robustness against\nprogramming noise. Furthermore, we conduct a comprehensive analysis of the\nIM-MIMO detector's performance, evaluating detection accuracy, processing\nlatency, and hardware complexity. Our study quantifies detection error as a\nfunction of various factors, including channel noise, memristor programming\nnoise, and neural network size."}
{"id": "2508.17814", "categories": ["cs.DC", "cs.AI", "68M20, 68T50", "C.4; D.4.7; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.17814", "abs": "https://arxiv.org/abs/2508.17814", "authors": ["Anderson de Lima Luiz", "Shubham Vijay Kurlekar", "Munir Georges"], "title": "Scalable Engine and the Performance of Different LLM Models in a SLURM based HPC architecture", "comment": "Accepted in ESSV 2025 - https://www.essv.de/paper.php?id=1265", "summary": "This work elaborates on a High performance computing (HPC) architecture based\non Simple Linux Utility for Resource Management (SLURM) [1] for deploying\nheterogeneous Large Language Models (LLMs) into a scalable inference engine.\nDynamic resource scheduling and seamless integration of containerized\nmicroservices have been leveraged herein to manage CPU, GPU, and memory\nallocations efficiently in multi-node clusters. Extensive experiments, using\nLlama 3.2 (1B and 3B parameters) [2] and Llama 3.1 (8B and 70B) [3], probe\nthroughput, latency, and concurrency and show that small models can handle up\nto 128 concurrent requests at sub-50 ms latency, while for larger models,\nsaturation happens with as few as two concurrent users, with a latency of more\nthan 2 seconds. This architecture includes Representational State Transfer\nApplication Programming Interfaces (REST APIs) [4] endpoints for single and\nbulk inferences, as well as advanced workflows such as multi-step \"tribunal\"\nrefinement. Experimental results confirm minimal overhead from container and\nscheduling activities and show that the approach scales reliably both for batch\nand interactive settings. We further illustrate real-world scenarios, including\nthe deployment of chatbots with retrievalaugmented generation, which helps to\ndemonstrate the flexibility and robustness of the architecture. The obtained\nresults pave ways for significantly more efficient, responsive, and\nfault-tolerant LLM inference on large-scale HPC infrastructures."}
{"id": "2508.17826", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.17826", "abs": "https://arxiv.org/abs/2508.17826", "authors": ["Kaiyan Chang", "Wenlong Zhu", "Shengwen Liang", "Huawei Li", "Ying Wang"], "title": "LLMulator: Generalizable Cost Modeling for Dataflow Accelerators with Input-Adaptive Control Flow", "comment": "Accepted by MICRO (IEEE/ACM International Symposium on\n  Microarchitecture) 2025", "summary": "Accurate and fast performance prediction for dataflow-based accelerators is\nvital for efficient hardware design and design space exploration, yet existing\nmethods struggle to generalize across architectures, applications, and\ninput-dependent control flows. We present LLMulator, a progressive numeric\nmodeling framework leveraging the program semantic knowledge of pre-trained\nlarge language models (LLMs) for robust, hardware- and application-aware\nprediction. Our numeric model treats performance values as categorical token\nsequences, enabling range-agnostic estimates and confidence-aware predictions\nfor unseen applications. To handle input-dependent control flows, we introduce\na reinforcement learning-based dynamic calibration method, reducing cycle\nprediction error by 9.7% over static models and converging to 11.2% error after\na few iterations. For cross-hardware generalization, we develop a progressive\ndata augmentation strategy that generates diverse datasets covering multi-level\ndataflow structures, memory parameters, and loop mapping primitives,\nsignificantly boosting prediction accuracy across architectures and\nconfigurations."}
{"id": "2508.18193", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.18193", "abs": "https://arxiv.org/abs/2508.18193", "authors": ["Petr Kuznetsov", "Maxence Perion", "Sara Tucci-Piergiovanni"], "title": "Wait-free Replicated Data Types and Fair Reconciliation", "comment": null, "summary": "Replication is a standard way to maintain availability of shared data in\nfault-prone distributed systems. To make sure that the data replicas are\nup-to-date, they need to synchronize, which typically means engaging the\nreplicas in waiting for coherent responses from each other. The amount of\nwaiting depends on the consistency and availability guarantees we impose on the\nsystem. The folklore CAP theory states that strong consistency (the set of\nreplicas create an illusion of one correct server) and strong availability (the\nreplicas' states are reachable despite network partitions) cannot be\nimplemented in the same system. A popular way to deal with this impossibility\nis to relax consistency to be only eventual: the replicas eventually converge\nto the same state. In return, the replicas can be wait-free, i.e., the clients\ncan get the data from the closest replica without waiting for other ones.\n  Wait-free data replication faces two important challenges. First, the\noperations issued by the clients may be constantly revoked, i.e., their effects\ncan be repeatedly recomputed due to asynchrony and concurrency. Second, even if\nsome operations eventually stabilize in their effects, a particular client may\nstill experience starvation if, from some point onward, each of its operations\nis later revoked. In this paper, we address these challenges through a general\nDAG-based framework for replicated data types, where replicas exchange their\nlocal views and merge them using a reconciliation function. Within this\nframework, we design reconciliation functions that implement a wait-free\neventually consistent replicated state machine ensuring both stable convergence\nand fair progress. Specifically, every replica maintains a growing sequence of\nclient operations, and we guarantee that: (1) all replicas share a common,\nmonotonically growing stable prefix of operations, and (2) no client starves."}
{"id": "2508.18043", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.18043", "abs": "https://arxiv.org/abs/2508.18043", "authors": ["Johan Söderström", "Yuan Yao"], "title": "Anatomy of the gem5 Simulator: AtomicSimpleCPU, TimingSimpleCPU, O3CPU, and Their Interaction with the Ruby Memory System", "comment": null, "summary": "gem5 is a popular modular-based computer system simulator, widely used in\ncomputer architecture research and known for its long simulation time and steep\nlearning curve. This report examines its three major CPU models: the\nAtomicSimpleCPU (AS CPU), the TimingSimpleCPU (TS CPU), the Out-of-order (O3)\nCPU, and their interactions with the memory subsystem. We provide a detailed\nanatomical overview of each CPU's function call-chains and present how gem5\npartitions its execution time for each simulated hardware layer.\n  We perform our analysis using a lightweight profiler built on Linux's\nperf_event interface, with user-configurable options to target specific\nfunctions and examine their interactions in detail. By profiling each CPU\nacross a wide selection of benchmarks, we identify their software bottlenecks.\nOur results show that the Ruby memory subsystem consistently accounts for the\nlargest share of execution time in the sequential AS and TS CPUs, primarily\nduring the instruction fetch stage. In contrast, the O3 CPU spends a relatively\nsmaller fraction of time in Ruby, with most of its time devoted to constructing\ninstruction instances and the various pipeline stages of the CPU.\n  We believe that the anatomical view of each CPU's execution flow is valuable\nfor educational purposes, as it clearly illustrates the interactions among\nsimulated components. These insights form a foundation for optimizing gem5's\nperformance, particularly for the AS, TS, and O3 CPUs. Moreover, our framework\ncan be readily applied to analyze other gem5 components or to develop and\nevaluate new models."}
{"id": "2508.18206", "categories": ["cs.DC", "cs.LG", "68T45, 86A32", "I.2.10; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.18206", "abs": "https://arxiv.org/abs/2508.18206", "authors": ["Ritvik Chaturvedi"], "title": "Practical GPU Choices for Earth Observation: ResNet-50 Training Throughput on Integrated, Laptop, and Cloud Accelerators", "comment": "10 pages, 5 figures", "summary": "This project implements a ResNet-based pipeline for land use and land cover\n(LULC) classification on Sentinel-2 imagery, benchmarked across three\nheterogeneous GPUs. The workflow automates data acquisition, geospatial\npreprocessing, tiling, model training, and visualization, and is fully\ncontainerized for reproducibility. Performance evaluation reveals up to a 2x\ntraining speed-up on an NVIDIA RTX 3060 and a Tesla T4 compared to the Apple M3\nPro baseline, while maintaining high classification accuracy on the EuroSAT\ndataset. These results demonstrate the feasibility of deploying deep learning\nLULC models on consumer and free cloud GPUs for scalable geospatial analytics."}
{"id": "2508.18224", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18224", "abs": "https://arxiv.org/abs/2508.18224", "authors": ["Ran Yan", "Youhe Jiang", "Binhang Yuan"], "title": "Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel", "comment": null, "summary": "Recent progress in sparse attention mechanisms has demonstrated strong\npotential for reducing the computational cost of long-context training and\ninference in large language models (LLMs). Native Sparse Attention (NSA), a\nstate-of-the-art approach, introduces natively trainable, hardware-aligned\nsparse attention that delivers substantial system-level performance gains while\nmaintaining accuracy comparable to full attention. However, the kernel\nimplementation of NSA relies on a query-grouping strategy that is efficient\nonly with large Grouped Query Attention (GQA) sizes, whereas modern LLMs\ntypically adopt much smaller GQA groups, which limits the applicability of this\nsparse algorithmic advance. In this work, we propose Flash Sparse Attention\n(FSA), which includes an alternative kernel design that enables efficient NSA\ncomputation across a wide range of popular LLMs with varied smaller GQA group\nsizes on modern GPUs. Compared to vanilla NSA kernel implementation, our\nempirical evaluation demonstrates that FSA achieves (i) up to 3.5$\\times$ and\non average 1.6$\\times$ kernel-level latency reduction, (ii) up to 1.25$\\times$\nand 1.09$\\times$ on average end-to-end training speedup on state-of-the-art\nLLMs, and (iii) up to 1.36$\\times$ and 1.11$\\times$ on average end-to-end\nprefill speedup on state-of-the-art LLMs. The source code is open-sourced and\npublicly available at\nhttps://github.com/Relaxed-System-Lab/Flash-Sparse-Attention."}
{"id": "2508.16700", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.16700", "abs": "https://arxiv.org/abs/2508.16700", "authors": ["Deepak Kumar", "Divakar Yadav", "Yash Patel"], "title": "GPT-OSS-20B: A Comprehensive Deployment-Centric Analysis of OpenAI's Open-Weight Mixture of Experts Model", "comment": null, "summary": "We present a single-GPU (H100, bf16) evaluation of GPT-OSS-20B\n(Mixture-of-Experts; 20.9B total, approx. 3.61B active) against dense baselines\nQwen3-32B and Yi-34B across multiple dimensions. We measure true\ntime-to-first-token (TTFT), full-decode throughput (TPOT), end-to-end latency\npercentiles, peak VRAM with past key values (PKV) held, and energy via a\nconsistent nvidia-smi-based sampler. At a 2048-token context with 64-token\ndecode, GPT-OSS-20B delivers higher decode throughput and tokens per Joule than\ndense baselines Qwen3-32B and Yi-34B, while substantially reducing peak VRAM\nand energy per 1000 generated tokens; its TTFT is higher due to MoE routing\noverhead. With only 17.3% of parameters active (3.61B of 20.9B), GPT-OSS-20B\nprovides about 31.8% higher decode throughput and 25.8% lower energy per 1000\ngenerated tokens than Qwen3-32B at 2048/64, while using 31.7% less peak VRAM.\nNormalized by active parameters, GPT-OSS-20B shows markedly stronger\nper-active-parameter efficiency (APE), underscoring MoE's deployment\nadvantages. We do not evaluate accuracy; this is a deployment-focused study. We\nrelease code and consolidated results to enable replication and extension."}
