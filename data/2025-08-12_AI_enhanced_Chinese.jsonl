{"id": "2508.06978", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.06978", "abs": "https://arxiv.org/abs/2508.06978", "authors": ["Kwanhee Kyung", "Sungmin Yun", "Jung Ho Ahn"], "title": "SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency", "comment": "4 pages, 6 figures, accepted at IEEE Computer Architecture Letters", "summary": "Large Language Models (LLMs) applying Mixture-of-Experts (MoE) scale to\ntrillions of parameters but require vast memory, motivating a line of research\nto offload expert weights from fast-but-small DRAM (HBM) to denser Flash SSDs.\nWhile SSDs provide cost-effective capacity, their read energy per bit is\nsubstantially higher than that of DRAM. This paper quantitatively analyzes the\nenergy implications of offloading MoE expert weights to SSDs during the\ncritical decode stage of LLM inference. Our analysis, comparing SSD, CPU memory\n(DDR), and HBM storage scenarios for models like DeepSeek-R1, reveals that\noffloading MoE weights to current SSDs drastically increases\nper-token-generation energy consumption (e.g., by up to ~12x compared to the\nHBM baseline), dominating the total inference energy budget. Although\ntechniques like prefetching effectively hide access latency, they cannot\nmitigate this fundamental energy penalty. We further explore future\ntechnological scaling, finding that the inherent sparsity of MoE models could\npotentially make SSDs energy-viable if Flash read energy improves\nsignificantly, roughly by an order of magnitude.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5c06MoE\u4e13\u5bb6\u6743\u91cd\u4eceDRAM\u5378\u8f7d\u5230SSD\u5bf9LLM\u63a8\u7406\u89e3\u7801\u9636\u6bb5\u80fd\u6e90\u6d88\u8017\u7684\u5f71\u54cd\uff0c\u53d1\u73b0SSD\u663e\u8457\u589e\u52a0\u80fd\u8017\uff0c\u672a\u6765\u9700Flash\u8bfb\u53d6\u80fd\u6e90\u5927\u5e45\u6539\u8fdb\u624d\u80fd\u4f7f\u5176\u53ef\u884c\u3002", "motivation": "\u7814\u7a76MoE\u6a21\u578b\u6743\u91cd\u4eceDRAM\u5378\u8f7d\u5230SSD\u7684\u80fd\u6e90\u5f71\u54cd\uff0c\u56e0SSD\u5bb9\u91cf\u5927\u4f46\u80fd\u8017\u9ad8\u3002", "method": "\u5b9a\u91cf\u5206\u6790SSD\u3001CPU\u5185\u5b58\u548cHBM\u5b58\u50a8\u573a\u666f\u4e0bMoE\u6743\u91cd\u5378\u8f7d\u7684\u80fd\u6e90\u6d88\u8017\uff0c\u4ee5DeepSeek-R1\u7b49\u6a21\u578b\u4e3a\u4f8b\u3002", "result": "SSD\u5378\u8f7dMoE\u6743\u91cd\u4f7f\u6bcf\u4ee4\u724c\u751f\u6210\u80fd\u8017\u589e\u52a0\u9ad8\u8fbe12\u500d\uff0c\u6210\u4e3a\u63a8\u7406\u603b\u80fd\u8017\u4e3b\u5bfc\u56e0\u7d20\u3002", "conclusion": "\u672a\u6765\u9700Flash\u8bfb\u53d6\u80fd\u6e90\u5927\u5e45\u6539\u8fdb\uff08\u7ea6\u4e00\u4e2a\u6570\u91cf\u7ea7\uff09\u624d\u80fd\u4f7fSSD\u5728MoE\u6a21\u578b\u4e2d\u80fd\u6e90\u53ef\u884c\u3002"}}
{"id": "2508.07110", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.07110", "abs": "https://arxiv.org/abs/2508.07110", "authors": ["Lorenzo Ruotolo", "Lara Orlandic", "Pengbo Yu", "Moritz Brunion", "Daniele Jahier Pagliari", "Dwaipayan Biswas", "Giovanni Ansaloni", "David Atienza", "Julien Ryckaert", "Francky Catthoor", "Yukai Chen"], "title": "Physical Design Exploration of a Wire-Friendly Domain-Specific Processor for Angstrom-Era Nodes", "comment": null, "summary": "This paper presents the physical design exploration of a domain-specific\nprocessor (DSIP) architecture targeted at machine learning (ML), addressing the\nchallenges of interconnect efficiency in advanced Angstrom-era technologies.\nThe design emphasizes reduced wire length and high core density by utilizing\nspecialized memory structures and SIMD (Single Instruction, Multiple Data)\nunits. Five configurations are synthesized and evaluated using the IMEC A10\nnanosheet node PDK. Key physical design metrics are compared across\nconfigurations and against VWR2A, a state-of-the-art (SoA) DSIP baseline.\nResults show that our architecture achieves over 2x lower normalized wire\nlength and more than 3x higher density than the SoA, with low variability in\nthe metrics across all configurations, making it a promising solution for\nnext-generation DSIP designs. These improvements are achieved with minimal\nmanual layout intervention, demonstrating the architecture's intrinsic physical\nefficiency and potential for low-cost wire-friendly implementation.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u4e00\u79cd\u9488\u5bf9\u673a\u5668\u5b66\u4e60\u7684\u9886\u57df\u7279\u5b9a\u5904\u7406\u5668\uff08DSIP\uff09\u67b6\u6784\u7684\u7269\u7406\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u5148\u8fdb\u57c3\u7c73\u65f6\u4ee3\u6280\u672f\u4e2d\u4e92\u8fde\u6548\u7387\u7684\u6311\u6218\u3002\u901a\u8fc7\u4f7f\u7528\u4e13\u7528\u5185\u5b58\u7ed3\u6784\u548cSIMD\u5355\u5143\uff0c\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u66f4\u77ed\u7684\u7ebf\u957f\u548c\u66f4\u9ad8\u7684\u6838\u5fc3\u5bc6\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5148\u8fdb\u6280\u672f\u8282\u70b9\u4e2d\u4e92\u8fde\u6548\u7387\u7684\u6311\u6218\uff0c\u63d0\u5347DSIP\u67b6\u6784\u7684\u7269\u7406\u8bbe\u8ba1\u6548\u7387\u3002", "method": "\u5229\u7528\u4e13\u7528\u5185\u5b58\u7ed3\u6784\u548cSIMD\u5355\u5143\uff0c\u8bbe\u8ba1\u4e86\u4e94\u79cd\u914d\u7f6e\uff0c\u5e76\u4f7f\u7528IMEC A10\u7eb3\u7c73\u7247\u8282\u70b9PDK\u8fdb\u884c\u5408\u6210\u548c\u8bc4\u4f30\u3002", "result": "\u4e0e\u73b0\u6709\u6700\u4f73DSIP\u57fa\u7ebf\uff08VWR2A\uff09\u76f8\u6bd4\uff0c\u8be5\u67b6\u6784\u5b9e\u73b0\u4e862\u500d\u4ee5\u4e0a\u7684\u7ebf\u957f\u964d\u4f4e\u548c3\u500d\u4ee5\u4e0a\u7684\u5bc6\u5ea6\u63d0\u5347\uff0c\u4e14\u5404\u914d\u7f6e\u95f4\u6307\u6807\u53d8\u5f02\u6027\u4f4e\u3002", "conclusion": "\u8be5\u67b6\u6784\u5177\u6709\u5185\u5728\u7269\u7406\u6548\u7387\u548c\u4f4e\u6210\u672c\u5b9e\u73b0\u7684\u6f5c\u529b\uff0c\u662f\u4e0b\u4e00\u4ee3DSIP\u8bbe\u8ba1\u7684\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07227", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.07227", "abs": "https://arxiv.org/abs/2508.07227", "authors": ["Siyuan He", "Zhantong Zhu", "Yandong He", "Tianyu Jia"], "title": "LP-Spec: Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization", "comment": "Accepted by ICCAD'2025", "summary": "LLM inference on mobile devices faces extraneous challenges due to limited\nmemory bandwidth and computational resources. To address these issues,\nspeculative inference and processing-in-memory (PIM) techniques have been\nexplored at the algorithmic and hardware levels. However, speculative inference\nresults in more compute-intensive GEMM operations, creating new design\ntrade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there\nexists a significant amount of redundant draft tokens in tree-based speculative\ninference, necessitating efficient token management schemes to minimize energy\nconsumption. In this work, we present LP-Spec, an architecture-dataflow\nco-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with\ndraft token pruning and dynamic workload scheduling to accelerate LLM\nspeculative inference. A near-data memory controller is proposed to enable data\nreallocation between DRAM and PIM banks. Furthermore, a data allocation unit\nbased on the hardware-aware draft token pruner is developed to minimize energy\nconsumption and fully exploit parallel execution opportunities. Compared to\nend-to-end LLM inference on other mobile solutions such as mobile NPUs or\nGEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x\nimprovements in performance, energy efficiency, and energy-delay-product (EDP).\nCompared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and\n415.31x EDP reduction benefits.", "AI": {"tldr": "LP-Spec\u662f\u4e00\u79cd\u67b6\u6784\u4e0e\u6570\u636e\u6d41\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7ed3\u5408LPDDR5 PIM\u67b6\u6784\u3001\u8349\u7a3f\u4ee4\u724c\u526a\u679d\u548c\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u8bbe\u5907\u4e0aLLM\u63a8\u6d4b\u63a8\u7406\u7684\u6027\u80fd\u548c\u80fd\u6548\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u4e0aLLM\u63a8\u7406\u9762\u4e34\u5185\u5b58\u5e26\u5bbd\u548c\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u7684\u95ee\u9898\uff0c\u73b0\u6709\u6280\u672f\u5982\u63a8\u6d4b\u63a8\u7406\u548cPIM\u5728\u7b97\u6cd5\u548c\u786c\u4ef6\u5c42\u9762\u5b58\u5728\u65b0\u7684\u8bbe\u8ba1\u6743\u8861\u548c\u5197\u4f59\u4ee4\u724c\u95ee\u9898\u3002", "method": "LP-Spec\u91c7\u7528\u6df7\u5408LPDDR5 PIM\u67b6\u6784\uff0c\u63d0\u51fa\u8fd1\u6570\u636e\u5185\u5b58\u63a7\u5236\u5668\u548c\u57fa\u4e8e\u786c\u4ef6\u611f\u77e5\u7684\u8349\u7a3f\u4ee4\u724c\u526a\u679d\u5668\uff0c\u4f18\u5316\u4ee4\u724c\u7ba1\u7406\u548c\u5e76\u884c\u6267\u884c\u3002", "result": "\u76f8\u6bd4\u5176\u4ed6\u79fb\u52a8\u89e3\u51b3\u65b9\u6848\uff0cLP-Spec\u5728\u6027\u80fd\u3001\u80fd\u6548\u548c\u80fd\u91cf\u5ef6\u8fdf\u79ef\uff08EDP\uff09\u4e0a\u5206\u522b\u63d0\u534713.21\u500d\u30017.56\u500d\u548c99.87\u500d\u3002", "conclusion": "LP-Spec\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u663e\u8457\u4f18\u5316\u4e86\u79fb\u52a8\u8bbe\u5907\u4e0aLLM\u63a8\u6d4b\u63a8\u7406\u7684\u6548\u7387\u548c\u80fd\u8017\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.07252", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.07252", "abs": "https://arxiv.org/abs/2508.07252", "authors": ["Siyuan He", "Peiran Yan", "Yandong He", "Youwei Zhuo", "Tianyu Jia"], "title": "Tasa: Thermal-aware 3D-Stacked Architecture Design with Bandwidth Sharing for LLM Inference", "comment": "Accepted by ICCAD'2025", "summary": "The autoregressive decoding in LLMs is the major inference bottleneck due to\nthe memory-intensive operations and limited hardware bandwidth. 3D-stacked\narchitecture is a promising solution with significantly improved memory\nbandwidth, which vertically stacked multi DRAM dies on top of logic die.\nHowever, our experiments also show the 3D-stacked architecture faces severer\nthermal issues compared to 2D architecture, in terms of thermal temperature,\ngradient and scalability. To better exploit the potential of 3D-stacked\narchitecture, we present Tasa, a heterogeneous architecture with cross-stack\nthermal optimizations to balance the temperature distribution and maximize the\nperformance under the thermal constraints. High-performance core is designed\nfor compute-intensive operations, while high-efficiency core is used for\nmemory-intensive operators, e.g. attention layers. Furthermore, we propose a\nbandwidth sharing scheduling to improve the bandwidth utilization in such\nheterogeneous architecture. Extensive thermal experiments show that our Tasa\narchitecture demonstrates greater scalability compared with the homogeneous\n3D-stacked architecture, i.e. up to 5.55 $\\tccentigrade$, 9.37 $\\tccentigrade$,\nand 7.91 $\\tccentigrade$ peak temperature reduction for 48, 60, and 72 core\nconfigurations. Our experimental for Llama-65B and GPT-3 66B inferences also\ndemonstrate 2.85x and 2.21x speedup are obtained over the GPU baselines and\nstate-of-the-art heterogeneous PIM-based LLM accelerator", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTasa\u67b6\u6784\uff0c\u901a\u8fc7\u5f02\u6784\u8bbe\u8ba1\u548c\u70ed\u4f18\u5316\u89e3\u51b33D\u5806\u53e0\u67b6\u6784\u7684\u6563\u70ed\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "3D\u5806\u53e0\u67b6\u6784\u867d\u80fd\u63d0\u5347\u5185\u5b58\u5e26\u5bbd\uff0c\u4f46\u9762\u4e34\u4e25\u91cd\u7684\u6563\u70ed\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u6f5c\u529b\u3002", "method": "\u63d0\u51faTasa\u5f02\u6784\u67b6\u6784\uff0c\u7ed3\u5408\u9ad8\u6027\u80fd\u548c\u9ad8\u6548\u7387\u6838\u5fc3\uff0c\u5e76\u91c7\u7528\u5e26\u5bbd\u5171\u4eab\u8c03\u5ea6\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793aTasa\u663e\u8457\u964d\u4f4e\u5cf0\u503c\u6e29\u5ea6\uff0c\u5e76\u5728Llama-65B\u548cGPT-3 66B\u63a8\u7406\u4e2d\u5b9e\u73b02.85x\u548c2.21x\u52a0\u901f\u3002", "conclusion": "Tasa\u67b6\u6784\u6709\u6548\u5e73\u8861\u6563\u70ed\u4e0e\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf3D\u5806\u53e0\u548cGPU\u57fa\u7ebf\u3002"}}
{"id": "2508.06526", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.06526", "abs": "https://arxiv.org/abs/2508.06526", "authors": ["Dong Liu", "Yanxuan Yu", "Ben Lengerich", "Ying Nian Wu", "Xuhong Wang"], "title": "PiKV: KV Cache Management System for Mixture of Experts", "comment": "Accepted to ICML ES-MoFo III WorkShop Paper Link:\n  https://openreview.net/pdf?id=hHoK1kBPd9 Github Link:\n  https://github.com/NoakLiu/PiKV", "summary": "As large language models continue to scale up in both size and context\nlength, the memory and communication cost of key-value (KV) cache storage has\nbecome a major bottleneck in multi-GPU and multi-node inference. While\nMoE-based architectures sparsify computation across experts, the corresponding\nKV caches remain dense and globally synchronized, resulting in significant\noverhead.\n  We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving\nframework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded\nKV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce\ntoken-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain\nquery-relevant entries. To further reduce memory usage, PiKV integrates\n\\textit{PiKV Compression} modules the caching pipeline for acceleration.\n  PiKV is recently publicly available as an open-source software library:\n\\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.\nExperiments details is recorded at:\n\\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}.\nWe also have PiKV integrated with Nvidia kvpress for acceleration, details see\n\\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.\nPiKV is still a living project, aiming to become a comprehesive KV Cache\nmanagement system for MoE Architectures.", "AI": {"tldr": "PiKV\u662f\u4e00\u4e2a\u4e13\u4e3aMoE\u67b6\u6784\u8bbe\u8ba1\u7684\u5e76\u884c\u5206\u5e03\u5f0fKV\u7f13\u5b58\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u5206\u7247\u5b58\u50a8\u3001\u8def\u7531\u4f18\u5316\u548c\u8c03\u5ea6\u7b56\u7565\u51cf\u5c11KV\u7f13\u5b58\u7684\u5f00\u9500\uff0c\u5e76\u96c6\u6210\u538b\u7f29\u6a21\u5757\u8fdb\u4e00\u6b65\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u589e\u52a0\uff0cKV\u7f13\u5b58\u7684\u5b58\u50a8\u548c\u901a\u4fe1\u6210\u672c\u6210\u4e3a\u591aGPU\u548c\u591a\u8282\u70b9\u63a8\u7406\u7684\u4e3b\u8981\u74f6\u9888\u3002MoE\u67b6\u6784\u867d\u7136\u7a00\u758f\u5316\u4e86\u8ba1\u7b97\uff0c\u4f46KV\u7f13\u5b58\u4ecd\u7136\u5bc6\u96c6\u4e14\u5168\u5c40\u540c\u6b65\uff0c\u5bfc\u81f4\u663e\u8457\u5f00\u9500\u3002", "method": "PiKV\u91c7\u7528\u4e13\u5bb6\u5206\u7247KV\u5b58\u50a8\u3001PiKV\u8def\u7531\u51cf\u5c11\u8bbf\u95ee\u3001PiKV\u8c03\u5ea6\u81ea\u9002\u5e94\u4fdd\u7559\u76f8\u5173\u6761\u76ee\uff0c\u5e76\u96c6\u6210\u538b\u7f29\u6a21\u5757\u4f18\u5316\u5185\u5b58\u4f7f\u7528\u3002", "result": "PiKV\u5df2\u5f00\u6e90\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u80fd\u6709\u6548\u51cf\u5c11KV\u7f13\u5b58\u7684\u5f00\u9500\uff0c\u5e76\u4e0eNvidia kvpress\u96c6\u6210\u52a0\u901f\u3002", "conclusion": "PiKV\u65e8\u5728\u6210\u4e3aMoE\u67b6\u6784\u7684\u5168\u9762KV\u7f13\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u76ee\u524d\u4ecd\u5728\u6301\u7eed\u5f00\u53d1\u4e2d\u3002"}}
{"id": "2508.07855", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.07855", "abs": "https://arxiv.org/abs/2508.07855", "authors": ["Parosh Aziz Abdulla", "Mohamed Faouzi Atig", "R. Govind", "Samuel Grahn", "Ramanathan S. Thinniyam"], "title": "Checking Consistency of Event-driven Traces", "comment": null, "summary": "Event-driven programming is a popular paradigm where the flow of execution is\ncontrolled by two features: (1) shared memory and (2) sending and receiving of\nmessages between multiple handler threads (just called handler). Each handler\nhas a mailbox (modelled as a queue) for receiving messages, with the constraint\nthat the handler processes its messages sequentially. Executions of messages by\ndifferent handlers may be interleaved. A central problem in this setting is\nchecking whether a candidate execution is consistent with the semantics of\nevent-driven programs. In this paper, we propose an axiomatic semantics for\neventdriven programs based on the standard notion of traces (also known as\nexecution graphs). We prove the equivalence of axiomatic and operational\nsemantics. This allows us to rephrase the consistency problem axiomatically,\nresulting in the event-driven consistency problem: checking whether a given\ntrace is consistent. We analyze the computational complexity of this problem\nand show that it is NP-complete, even when the number of handler threads is\nbounded. We then identify a tractable fragment: in the absence of nested\nposting, where handlers do not post new messages while processing a message,\nconsistency checking can be performed in polynomial time. Finally, we implement\nour approach in a prototype tool and report on experimental results on a wide\nrange of benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6267\u884c\u56fe\u7684\u516c\u7406\u8bed\u4e49\u65b9\u6cd5\uff0c\u7528\u4e8e\u9a8c\u8bc1\u4e8b\u4ef6\u9a71\u52a8\u7a0b\u5e8f\u7684\u6267\u884c\u4e00\u81f4\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u4e0e\u64cd\u4f5c\u8bed\u4e49\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u5206\u6790\u4e86\u8be5\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff08NP\u5b8c\u5168\u6027\uff09\uff0c\u540c\u65f6\u8bc6\u522b\u4e86\u4e00\u4e2a\u53ef\u9ad8\u6548\u6c42\u89e3\u7684\u7247\u6bb5\uff08\u65e0\u5d4c\u5957\u53d1\u5e03\uff09\u3002", "motivation": "\u4e8b\u4ef6\u9a71\u52a8\u7a0b\u5e8f\u7684\u6267\u884c\u4e00\u81f4\u6027\u9a8c\u8bc1\u662f\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5f62\u5f0f\u5316\u8bed\u4e49\u652f\u6301\u3002", "method": "\u57fa\u4e8e\u6267\u884c\u56fe\u7684\u516c\u7406\u8bed\u4e49\u65b9\u6cd5\uff0c\u8bc1\u660e\u5176\u4e0e\u64cd\u4f5c\u8bed\u4e49\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u5206\u6790\u8ba1\u7b97\u590d\u6742\u6027\u3002", "result": "\u4e00\u81f4\u6027\u9a8c\u8bc1\u95ee\u9898\u662fNP\u5b8c\u5168\u7684\uff0c\u4f46\u5728\u65e0\u5d4c\u5957\u53d1\u5e03\u7684\u60c5\u51b5\u4e0b\u53ef\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u6c42\u89e3\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u4e8b\u4ef6\u9a71\u52a8\u7a0b\u5e8f\u7684\u6267\u884c\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.07457", "categories": ["cs.AR", "A.1; C.1.1; G.3; I.6.1; I.6.8"], "pdf": "https://arxiv.org/pdf/2508.07457", "abs": "https://arxiv.org/abs/2508.07457", "authors": ["Janith Petangoda", "Chatura Samarakoon", "James Meech", "Divya Thekke Kanapram", "Hamid Toshani", "Nathaniel Tye", "Vasileios Tsoutsouras", "Phillip Stanley-Marbell"], "title": "The Monte Carlo Method and New Device and Architectural Techniques for Accelerating It", "comment": "15 pages, 4 figures (17 subfigures)", "summary": "Computing systems interacting with real-world processes must safely and\nreliably process uncertain data. The Monte Carlo method is a popular approach\nfor computing with such uncertain values. This article introduces a framework\nfor describing the Monte Carlo method and highlights two advances in the domain\nof physics-based non-uniform random variate generators (PPRVGs) to overcome\ncommon limitations of traditional Monte Carlo sampling. This article also\nhighlights recent advances in architectural techniques that eliminate the need\nto use the Monte Carlo method by leveraging distributional microarchitectural\nstate to natively compute on probability distributions. Unlike Monte Carlo\nmethods, uncertainty-tracking processor architectures can be said to be\nconvergence-oblivious.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u7684\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u975e\u5747\u5300\u968f\u673a\u53d8\u91cf\u751f\u6210\u5668\uff08PPRVGs\uff09\u7684\u6539\u8fdb\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u65e0\u9700\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u7684\u65b0\u578b\u67b6\u6784\u6280\u672f\u3002", "motivation": "\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4e0d\u786e\u5b9a\u6570\u636e\u9700\u8981\u5b89\u5168\u53ef\u9760\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u867d\u6d41\u884c\u4f46\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faPPRVGs\u6539\u8fdb\u8499\u7279\u5361\u6d1b\u91c7\u6837\uff0c\u5e76\u5229\u7528\u5206\u5e03\u5fae\u67b6\u6784\u72b6\u6001\u76f4\u63a5\u8ba1\u7b97\u6982\u7387\u5206\u5e03\u3002", "result": "\u65b0\u578b\u67b6\u6784\u6280\u672f\u907f\u514d\u4e86\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u7684\u6536\u655b\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6536\u655b\u65e0\u5173\u7684\u8ba1\u7b97\u3002", "conclusion": "\u901a\u8fc7PPRVGs\u548c\u65b0\u578b\u67b6\u6784\u6280\u672f\uff0c\u53ef\u4ee5\u66f4\u9ad8\u6548\u5730\u5904\u7406\u4e0d\u786e\u5b9a\u6570\u636e\uff0c\u514b\u670d\u4f20\u7edf\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u7684\u9650\u5236\u3002"}}
{"id": "2508.06948", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.06948", "abs": "https://arxiv.org/abs/2508.06948", "authors": ["Jinyuan Chen", "Jiuchen Shi", "Quan Chen", "Minyi Guo"], "title": "Kairos: Low-latency Multi-Agent Serving with Shared LLMs and Excessive Loads in the Public Cloud", "comment": null, "summary": "Multi-agent applications utilize the advanced capabilities of large language\nmodels (LLMs) for intricate task completion through agent collaboration in a\nworkflow. Under this situation, requests from different agents usually access\nthe same shared LLM to perform different kinds of tasks, forcing the shared LLM\nto suffer excessive loads. However, existing works have low serving performance\nfor these multi-agent applications, mainly due to the ignorance of inter-agent\nlatency and resource differences for request scheduling. We therefore propose\nKairos, a multi-agent orchestration system that optimizes end-to-end latency\nfor multi-agent applications. Kairos consists of a workflow orchestrator, a\nworkflow-aware priority scheduler, and a memory-aware dispatcher. The\norchestrator collects agent-specific information for online workflow analysis.\nThe scheduler decides the serving priority of the requests based on their\nlatency characteristics to reduce the overall queuing. The dispatcher\ndispatches the requests to different LLM instances based on their memory\ndemands to avoid GPU overloading. Experimental results show that Kairos reduces\nend-to-end latency by 17.8% to 28.4% compared to state-of-the-art works.", "AI": {"tldr": "Kairos\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u7f16\u6392\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u8c03\u5ea6\u548c\u5206\u53d1\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u591a\u4ee3\u7406\u5e94\u7528\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "\u591a\u4ee3\u7406\u5e94\u7528\u4e2d\uff0c\u5171\u4eabLLM\u56e0\u4efb\u52a1\u591a\u6837\u6027\u548c\u9ad8\u8d1f\u8f7d\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4ee3\u7406\u95f4\u7684\u5ef6\u8fdf\u548c\u8d44\u6e90\u5dee\u5f02\u3002", "method": "Kairos\u7531\u5de5\u4f5c\u6d41\u7f16\u6392\u5668\u3001\u4f18\u5148\u7ea7\u8c03\u5ea6\u5668\u548c\u5185\u5b58\u611f\u77e5\u5206\u53d1\u5668\u7ec4\u6210\uff0c\u5206\u522b\u8d1f\u8d23\u5de5\u4f5c\u6d41\u5206\u6790\u3001\u8bf7\u6c42\u4f18\u5148\u7ea7\u8c03\u5ea6\u548c\u5185\u5b58\u9700\u6c42\u5206\u53d1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKairos\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e\u4e8617.8%\u81f328.4%\u3002", "conclusion": "Kairos\u901a\u8fc7\u4f18\u5316\u8c03\u5ea6\u548c\u8d44\u6e90\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4ee3\u7406\u5e94\u7528\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07541", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.07541", "abs": "https://arxiv.org/abs/2508.07541", "authors": ["Kittiphon Phalakarn", "Athasit Surarerks"], "title": "A Matrix Decomposition Method for Odd-Type Gaussian Normal Basis Multiplication", "comment": null, "summary": "Normal basis is used in many applications because of the efficiency of the\nimplementation. However, most space complexity reduction techniques for binary\nfield multiplier are applicable for only optimal normal basis or Gaussian\nnormal basis of even type. There are 187 binary fields GF(2^k) for k from 2 to\n1,000 that use odd-type Gaussian normal basis. This paper presents a method to\nreduce the space complexity of odd-type Gaussian normal basis multipliers over\nbinary field GF(2^k). The idea is adapted from the matrix decomposition method\nfor optimal normal basis. The result shows that our space complexity reduction\nmethod can reduce the number of XOR gates used in the implementation comparing\nto previous works with a small trade-off in critical path delay.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u964d\u4f4e\u4e8c\u8fdb\u5236\u57dfGF(2^k)\u4e0a\u5947\u578b\u9ad8\u65af\u6b63\u89c4\u57fa\u4e58\u6cd5\u5668\u7a7a\u95f4\u590d\u6742\u5ea6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e9\u9635\u5206\u89e3\u6280\u672f\u51cf\u5c11XOR\u95e8\u6570\u91cf\uff0c\u4f46\u7565\u5fae\u589e\u52a0\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\u3002", "motivation": "\u5927\u591a\u6570\u7a7a\u95f4\u590d\u6742\u5ea6\u964d\u4f4e\u6280\u672f\u4ec5\u9002\u7528\u4e8e\u6700\u4f18\u6b63\u89c4\u57fa\u6216\u5076\u578b\u9ad8\u65af\u6b63\u89c4\u57fa\uff0c\u800c187\u4e2a\u4e8c\u8fdb\u5236\u57dfGF(2^k)\u4f7f\u7528\u5947\u578b\u9ad8\u65af\u6b63\u89c4\u57fa\uff0c\u56e0\u6b64\u9700\u8981\u9488\u5bf9\u6b64\u7c7b\u57fa\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u501f\u9274\u6700\u4f18\u6b63\u89c4\u57fa\u7684\u77e9\u9635\u5206\u89e3\u65b9\u6cd5\uff0c\u63d0\u51fa\u9002\u7528\u4e8e\u5947\u578b\u9ad8\u65af\u6b63\u89c4\u57fa\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u964d\u4f4e\u6280\u672f\u3002", "result": "\u4e0e\u4e4b\u524d\u5de5\u4f5c\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86XOR\u95e8\u7684\u4f7f\u7528\u6570\u91cf\uff0c\u4f46\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\u7565\u6709\u589e\u52a0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5947\u578b\u9ad8\u65af\u6b63\u89c4\u57fa\u4e58\u6cd5\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u4f18\u5316\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u7279\u5b9a\u4e8c\u8fdb\u5236\u57df\u3002"}}
{"id": "2508.06949", "categories": ["cs.DC", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.06949", "abs": "https://arxiv.org/abs/2508.06949", "authors": ["Arya Tanmay Gupta"], "title": "Convergence Sans Synchronization", "comment": "PhD thesis", "summary": "We currently see a steady rise in the usage and size of multiprocessor\nsystems, and so the community is evermore interested in developing fast\nparallel processing algorithms. However, most algorithms require a\nsynchronization mechanism, which is costly in terms of computational resources\nand time. If an algorithm can be executed in asynchrony, then it can use all\nthe available computation power, and the nodes can execute without being\nscheduled or locked. However, to show that an algorithm guarantees convergence\nin asynchrony, we need to generate the entire global state transition graph and\ncheck for the absence of cycles. This takes time exponential in the size of the\nglobal state space. In this dissertation, we present a theory that explains the\nnecessary and sufficient properties of a multiprocessor algorithm that\nguarantees convergence even without synchronization. We develop algorithms for\nvarious problems that do not require synchronization. Additionally, we show for\nseveral existing algorithms that they can be executed without any\nsynchronization mechanism. A significant theoretical benefit of our work is in\nproving that an algorithm can converge even in asynchrony. Our theory implies\nthat we can make such conclusions about an algorithm, by only showing that the\nlocal state transition graph of a computing node forms a partial order, rather\nthan generating the entire global state space and determining the absence of\ncycles in it. Thus, the complexity of rendering such proofs, formal or social,\nis phenomenally reduced. Experiments show a significant reduction in time taken\nto converge, when we compare the execution time of algorithms in the literature\nversus the algorithms that we design. We get similar results when we run an\nalgorithm, that guarantees convergence in asynchrony, under a scheduler versus\nin asynchrony.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\uff0c\u7528\u4e8e\u9a8c\u8bc1\u591a\u5904\u7406\u5668\u7b97\u6cd5\u5728\u5f02\u6b65\u73af\u5883\u4e0b\u4ecd\u80fd\u4fdd\u8bc1\u6536\u655b\uff0c\u65e0\u9700\u751f\u6210\u5168\u5c40\u72b6\u6001\u8f6c\u79fb\u56fe\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bc1\u660e\u590d\u6742\u5ea6\u3002", "motivation": "\u968f\u7740\u591a\u5904\u7406\u5668\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u5f00\u53d1\u5feb\u901f\u5e76\u884c\u5904\u7406\u7b97\u6cd5\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\uff0c\u4f46\u540c\u6b65\u673a\u5236\u6210\u672c\u9ad8\u6602\u3002\u5f02\u6b65\u6267\u884c\u80fd\u5145\u5206\u5229\u7528\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f46\u9a8c\u8bc1\u7b97\u6cd5\u6536\u655b\u6027\u9700\u68c0\u67e5\u5168\u5c40\u72b6\u6001\u8f6c\u79fb\u56fe\uff0c\u590d\u6742\u5ea6\u9ad8\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7406\u8bba\uff0c\u901a\u8fc7\u5206\u6790\u8ba1\u7b97\u8282\u70b9\u7684\u5c40\u90e8\u72b6\u6001\u8f6c\u79fb\u56fe\u662f\u5426\u5f62\u6210\u504f\u5e8f\u5173\u7cfb\uff0c\u6765\u5224\u65ad\u7b97\u6cd5\u5728\u5f02\u6b65\u73af\u5883\u4e0b\u7684\u6536\u655b\u6027\uff0c\u907f\u514d\u4e86\u5168\u5c40\u72b6\u6001\u7a7a\u95f4\u7684\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8bbe\u8ba1\u7684\u7b97\u6cd5\u5728\u6536\u655b\u65f6\u95f4\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u4e14\u5728\u5f02\u6b65\u73af\u5883\u4e0b\u8868\u73b0\u76f8\u4f3c\u3002", "conclusion": "\u8be5\u7406\u8bba\u7b80\u5316\u4e86\u5f02\u6b65\u7b97\u6cd5\u7684\u6536\u655b\u6027\u8bc1\u660e\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u5f02\u6b65\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.07725", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.07725", "abs": "https://arxiv.org/abs/2508.07725", "authors": ["Andreas Hager-Clukas", "Philipp van Kempen", "Stefan Wallentowitz"], "title": "ARISE: Automating RISC-V Instruction Set Extension", "comment": null, "summary": "RISC-V is an extendable Instruction Set Architecture, growing in popularity\nfor embedded systems. However, optimizing it to specific requirements, imposes\na great deal of manual effort. To bridge the gap between software and ISA, the\ntool ARISE is presented. It automates the generation of RISC-V instructions\nbased on assembly patterns, which are selected by an extendable set of metrics.\nThese metrics implement the optimization goals of code size and instruction\ncount reduction, both statically and dynamically. The instruction set\nextensions are generated using the ISA description language CoreDSL. Allowing\nseamless embedding in advanced tools such as the retargeting compiler Seal5 or\nthe instruction set simulator ETISS. ARISE improves the static code size by\n1.48% and the dynamic code size by 3.84%, as well as the number of instructions\nto be executed by 7.39% on average for Embench-Iot.", "AI": {"tldr": "ARISE\u5de5\u5177\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210RISC-V\u6307\u4ee4\uff0c\u57fa\u4e8e\u53ef\u6269\u5c55\u7684\u5ea6\u91cf\u6807\u51c6\u4f18\u5316\u4ee3\u7801\u5927\u5c0f\u548c\u6307\u4ee4\u6570\uff0c\u9759\u6001\u548c\u52a8\u6001\u6027\u80fd\u5747\u6709\u63d0\u5347\u3002", "motivation": "RISC-V\u7684\u53ef\u6269\u5c55\u6027\u4f7f\u5176\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u6d41\u884c\uff0c\u4f46\u624b\u52a8\u4f18\u5316\u5de5\u4f5c\u91cf\u5927\uff0cARISE\u65e8\u5728\u586b\u8865\u8f6f\u4ef6\u4e0eISA\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "ARISE\u5229\u7528CoreDSL\u8bed\u8a00\u751f\u6210\u6307\u4ee4\u96c6\u6269\u5c55\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u5ea6\u91cf\u6807\u51c6\u9009\u62e9\u6c47\u7f16\u6a21\u5f0f\uff0c\u4f18\u5316\u4ee3\u7801\u5927\u5c0f\u548c\u6307\u4ee4\u6570\u3002", "result": "\u5728Embench-Iot\u6d4b\u8bd5\u4e2d\uff0cARISE\u9759\u6001\u4ee3\u7801\u5927\u5c0f\u51cf\u5c111.48%\uff0c\u52a8\u6001\u4ee3\u7801\u5927\u5c0f\u51cf\u5c113.84%\uff0c\u6307\u4ee4\u6267\u884c\u6570\u5e73\u5747\u51cf\u5c117.39%\u3002", "conclusion": "ARISE\u6709\u6548\u81ea\u52a8\u5316\u4e86RISC-V\u6307\u4ee4\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u9ad8\u7ea7\u5de5\u5177\u96c6\u6210\u3002"}}
{"id": "2508.07071", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.07071", "abs": "https://arxiv.org/abs/2508.07071", "authors": ["Oscar Amoros", "Albert Andaluz", "Johnny Nunez", "Antonio J. Pena"], "title": "The Fused Kernel Library: A C++ API to Develop Highly-Efficient GPU Libraries", "comment": "16 pages", "summary": "Existing GPU libraries often struggle to fully exploit the parallel resources\nand on-chip memory (SRAM) of GPUs when chaining multiple GPU functions as\nindividual kernels. While Kernel Fusion (KF) techniques like Horizontal Fusion\n(HF) and Vertical Fusion (VF) can mitigate this, current library\nimplementations often require library developers to manually create fused\nkernels. Hence, library users rely on limited sets of pre-compiled or\ntemplate-based fused kernels. This limits the use cases that can benefit from\nHF and VF and increases development costs. In order to solve these issues, we\npresent a novel methodology for building GPU libraries that enables automatic\non-demand HF and VF for arbitrary combinations of GPU library functions. Our\nmethodology defines reusable, fusionable components that users combine via\nhigh-level programming interfaces. Leveraging C++17 metaprogramming features\navailable in compilers like nvcc, our methodology generates a single and\noptimized fused kernel tailored to the user's specific sequence of operations\nat compile time, without needing a custom compiler or manual development and\npre-compilation of kernel combinations. This approach abstracts low-level GPU\ncomplexities while maximizing GPU resource utilization and keeping intermediate\ndata in SRAM. We provide an open-source implementation demonstrating\nsignificant speedups compared to traditional libraries in various benchmarks,\nvalidating the effectiveness of this methodology for improving GPU performance\nin the range of 2x to more than 1000x, while preserving high-level\nprogrammability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5b9e\u73b0GPU\u5e93\u51fd\u6570\u6c34\u5e73\u878d\u5408\uff08HF\uff09\u548c\u5782\u76f4\u878d\u5408\uff08VF\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7C++17\u5143\u7f16\u7a0b\u751f\u6210\u4f18\u5316\u7684\u878d\u5408\u5185\u6838\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GPU\u5e93\u5728\u591a\u4e2aGPU\u51fd\u6570\u94fe\u5f0f\u8c03\u7528\u65f6\u96be\u4ee5\u5145\u5206\u5229\u7528\u5e76\u884c\u8d44\u6e90\u548cSRAM\uff0c\u4e14\u4f9d\u8d56\u624b\u52a8\u5f00\u53d1\u878d\u5408\u5185\u6838\uff0c\u9650\u5236\u4e86\u5e94\u7528\u573a\u666f\u5e76\u589e\u52a0\u5f00\u53d1\u6210\u672c\u3002", "method": "\u5b9a\u4e49\u53ef\u91cd\u7528\u3001\u53ef\u878d\u5408\u7684\u7ec4\u4ef6\uff0c\u7528\u6237\u901a\u8fc7\u9ad8\u7ea7\u7f16\u7a0b\u63a5\u53e3\u7ec4\u5408\uff0c\u5229\u7528C++17\u5143\u7f16\u7a0b\u5728\u7f16\u8bd1\u65f6\u81ea\u52a8\u751f\u6210\u4f18\u5316\u7684\u878d\u5408\u5185\u6838\u3002", "result": "\u5f00\u6e90\u5b9e\u73b0\u663e\u793a\u6027\u80fd\u63d0\u53472x\u81f31000x\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7ea7\u7f16\u7a0b\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86GPU\u8d44\u6e90\u5229\u7528\u7387\u548c\u6027\u80fd\uff0c\u540c\u65f6\u7b80\u5316\u4e86\u5f00\u53d1\u6d41\u7a0b\u3002"}}
{"id": "2508.07796", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.07796", "abs": "https://arxiv.org/abs/2508.07796", "authors": ["Dengke Han", "Duo Wang", "Mingyu Yan", "Xiaochun Ye", "Dongrui Fan"], "title": "TLV-HGNN: Thinking Like a Vertex for Memory-efficient HGNN Inference", "comment": "8 pages, 9 figures, accepted by ICCD 2025", "summary": "Heterogeneous graph neural networks (HGNNs) excel at processing heterogeneous\ngraph data and are widely applied in critical domains. In HGNN inference, the\nneighbor aggregation stage is the primary performance determinant, yet it\nsuffers from two major sources of memory inefficiency. First, the commonly\nadopted per-semantic execution paradigm stores intermediate aggregation results\nfor each semantic prior to semantic fusion, causing substantial memory\nexpansion. Second, the aggregation process incurs extensive redundant memory\naccesses, including repeated loading of target vertex features across semantics\nand repeated accesses to shared neighbors due to cross-semantic neighborhood\noverlap. These inefficiencies severely limit scalability and reduce HGNN\ninference performance.\n  In this work, we first propose a semantics-complete execution paradigm from a\nvertex perspective that eliminates per-semantic intermediate storage and\nredundant target vertex accesses. Building on this paradigm, we design\nTVL-HGNN, a reconfigurable hardware accelerator optimized for efficient\naggregation. In addition, we introduce a vertex grouping technique based on\ncross-semantic neighborhood overlap, with hardware implementation, to reduce\nredundant accesses to shared neighbors. Experimental results demonstrate that\nTVL-HGNN achieves average speedups of 7.85x and 1.41x over the NVIDIA A100 GPU\nand the state-of-the-art HGNN accelerator HiHGNN, respectively, while reducing\nenergy consumption by 98.79% and 32.61%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6267\u884c\u8303\u5f0fTVL-HGNN\uff0c\u901a\u8fc7\u6d88\u9664\u4e2d\u95f4\u5b58\u50a8\u548c\u5197\u4f59\u8bbf\u95ee\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HGNN\uff09\u7684\u63a8\u7406\u6027\u80fd\u3002", "motivation": "HGNN\u5728\u63a8\u7406\u9636\u6bb5\u7684\u90bb\u5c45\u805a\u5408\u9636\u6bb5\u5b58\u5728\u5185\u5b58\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u5305\u62ec\u4e2d\u95f4\u5b58\u50a8\u6269\u5c55\u548c\u5197\u4f59\u5185\u5b58\u8bbf\u95ee\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u9876\u70b9\u89c6\u89d2\u7684\u8bed\u4e49\u5b8c\u6574\u6267\u884c\u8303\u5f0f\uff0c\u8bbe\u8ba1TVL-HGNN\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u90bb\u5c45\u91cd\u53e0\u7684\u9876\u70b9\u5206\u7ec4\u6280\u672f\u3002", "result": "TVL-HGNN\u5728\u6027\u80fd\u4e0a\u6bd4NVIDIA A100 GPU\u548cHiHGNN\u52a0\u901f\u5668\u5206\u522b\u63d0\u5347\u4e867.85\u500d\u548c1.41\u500d\uff0c\u80fd\u8017\u964d\u4f4e\u4e8698.79%\u548c32.61%\u3002", "conclusion": "TVL-HGNN\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u8bbf\u95ee\u548c\u6267\u884c\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86HGNN\u7684\u63a8\u7406\u6548\u7387\u548c\u80fd\u6548\u3002"}}
{"id": "2508.07124", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.07124", "abs": "https://arxiv.org/abs/2508.07124", "authors": ["Shashwat Jaiswal", "Suman Raj", "Subhajit Sidhanta", "Yogesh Simmhan"], "title": "AerialDB: A Federated Peer-to-Peer Spatio-temporal Edge Datastore for Drone Fleets", "comment": null, "summary": "Recent years have seen an unprecedented growth in research that leverages the\nnewest computing paradigm of Internet of Drones, comprising a fleet of\nconnected Unmanned Aerial Vehicles (UAVs) used for a wide range of tasks such\nas monitoring and analytics in highly mobile and changing environments\ncharacteristic of disaster regions. Given that the typical data (i.e., videos\nand images) collected by the fleet of UAVs deployed in such scenarios can be\nconsiderably larger than what the onboard computers can process, the UAVs need\nto offload their data in real-time to the edge and the cloud for further\nprocessing. To that end, we present the design of AerialDB - a lightweight\ndecentralized data storage and query system that can store and process time\nseries data on a multi-UAV system comprising: A) a fleet of hundreds of UAVs\nfitted with onboard computers, and B) ground-based edge servers connected\nthrough a cellular link. Leveraging lightweight techniques for content-based\nreplica placement and indexing of shards, AerialDB has been optimized for\nefficient processing of different possible combinations of typical spatial and\ntemporal queries performed by real-world disaster management applications.\nUsing containerized deployment spanning up to 400 drones and 80 edges, we\ndemonstrate that AerialDB is able to scale efficiently while providing near\nreal-time performance with different realistic workloads. Further, AerialDB\ncomprises a decentralized and locality-aware distributed execution engine which\nprovides graceful degradation of performance upon edge failures with relatively\nlow latency while processing large spatio-temporal data. AerialDB exhibits\ncomparable insertion performance and 100 times improvement in query performance\nagainst state-of-the-art baseline. Moreover, it exhibits a 10 times and 100\ntimes improvement with insertion and query workloads respectively over the\ncloud baseline.", "AI": {"tldr": "AerialDB\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u53bb\u4e2d\u5fc3\u5316\u6570\u636e\u5b58\u50a8\u4e0e\u67e5\u8be2\u7cfb\u7edf\uff0c\u4e13\u4e3a\u65e0\u4eba\u673a\u7fa4\u8bbe\u8ba1\uff0c\u652f\u6301\u9ad8\u6548\u65f6\u7a7a\u6570\u636e\u5904\u7406\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u707e\u5bb3\u533a\u57df\u7b49\u52a8\u6001\u73af\u5883\u4e2d\u91c7\u96c6\u5927\u91cf\u6570\u636e\uff0c\u9700\u5b9e\u65f6\u5378\u8f7d\u5230\u8fb9\u7f18\u548c\u4e91\u7aef\u5904\u7406\uff0c\u73b0\u6709\u7cfb\u7edf\u96be\u4ee5\u6ee1\u8db3\u9ad8\u6548\u5b58\u50a8\u4e0e\u67e5\u8be2\u9700\u6c42\u3002", "method": "AerialDB\u91c7\u7528\u57fa\u4e8e\u5185\u5bb9\u7684\u526f\u672c\u653e\u7f6e\u548c\u5206\u7247\u7d22\u5f15\u6280\u672f\uff0c\u7ed3\u5408\u53bb\u4e2d\u5fc3\u5316\u6267\u884c\u5f15\u64ce\uff0c\u4f18\u5316\u65f6\u7a7a\u67e5\u8be2\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAerialDB\u5728400\u67b6\u65e0\u4eba\u673a\u548c80\u4e2a\u8fb9\u7f18\u8282\u70b9\u4e0a\u6269\u5c55\u9ad8\u6548\uff0c\u67e5\u8be2\u6027\u80fd\u63d0\u5347100\u500d\uff0c\u63d2\u5165\u6027\u80fd\u63d0\u534710\u500d\u3002", "conclusion": "AerialDB\u4e3a\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u6570\u636e\u5904\u7406\u65b9\u6848\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.07193", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.07193", "abs": "https://arxiv.org/abs/2508.07193", "authors": ["Haoyuan Zhang", "Yaqian Gao", "Xinxin Zhang", "Jialin Li", "Runfeng Jin", "Yidong Chen", "Feng Zhang", "Wu Yuan", "Wenpeng Ma", "Shan Liang", "Jian Zhang", "Zhonghua Lu"], "title": "FlashMP: Fast Discrete Transform-Based Solver for Preconditioning Maxwell's Equations on GPUs", "comment": null, "summary": "Efficiently solving large-scale linear systems is a critical challenge in\nelectromagnetic simulations, particularly when using the Crank-Nicolson\nFinite-Difference Time-Domain (CN-FDTD) method. Existing iterative solvers are\ncommonly employed to handle the resulting sparse systems but suffer from slow\nconvergence due to the ill-conditioned nature of the double-curl operator.\nApproximate preconditioners, like Successive Over-Relaxation (SOR) and\nIncomplete LU decomposition (ILU), provide insufficient convergence, while\ndirect solvers are impractical due to excessive memory requirements. To address\nthis, we propose FlashMP, a novel preconditioning system that designs a\nsubdomain exact solver based on discrete transforms. FlashMP provides an\nefficient GPU implementation that achieves multi-GPU scalability through domain\ndecomposition. Evaluations on AMD MI60 GPU clusters (up to 1000 GPUs) show that\nFlashMP reduces iteration counts by up to 16x and achieves speedups of 2.5x to\n4.9x compared to baseline implementations in state-of-the-art libraries such as\nHypre. Weak scalability tests show parallel efficiencies up to 84.1%.", "AI": {"tldr": "FlashMP\u662f\u4e00\u79cd\u65b0\u578b\u9884\u5904\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u79bb\u6563\u53d8\u6362\u8bbe\u8ba1\u5b50\u57df\u7cbe\u786e\u6c42\u89e3\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86CN-FDTD\u65b9\u6cd5\u4e2d\u5927\u89c4\u6a21\u7ebf\u6027\u7cfb\u7edf\u7684\u6c42\u89e3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u8fed\u4ee3\u6c42\u89e3\u5668\u5728\u5904\u7406\u53cc\u65cb\u5ea6\u7b97\u5b50\u5bfc\u81f4\u7684\u7a00\u758f\u7cfb\u7edf\u65f6\u6536\u655b\u7f13\u6162\uff0c\u800c\u8fd1\u4f3c\u9884\u5904\u7406\u65b9\u6cd5\u6548\u679c\u4e0d\u8db3\uff0c\u76f4\u63a5\u6c42\u89e3\u5668\u5185\u5b58\u9700\u6c42\u8fc7\u9ad8\u3002", "method": "\u63d0\u51faFlashMP\uff0c\u57fa\u4e8e\u79bb\u6563\u53d8\u6362\u8bbe\u8ba1\u5b50\u57df\u7cbe\u786e\u6c42\u89e3\u5668\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u7684\u591aGPU\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728AMD MI60 GPU\u96c6\u7fa4\u4e0a\uff0cFlashMP\u5c06\u8fed\u4ee3\u6b21\u6570\u51cf\u5c1116\u500d\uff0c\u901f\u5ea6\u63d0\u53472.5\u81f34.9\u500d\uff0c\u5e76\u884c\u6548\u7387\u8fbe84.1%\u3002", "conclusion": "FlashMP\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u7535\u78c1\u6a21\u62df\u4e2d\u7ebf\u6027\u7cfb\u7edf\u7684\u6c42\u89e3\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.07317", "categories": ["cs.DC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07317", "abs": "https://arxiv.org/abs/2508.07317", "authors": ["Pedro Carrinho", "Hamid Moghadaspour", "Oscar Ferraz", "Jo\u00e3o Dinis Ferreira", "Yann Falevoz", "Vitor Silva", "Gabriel Falcao"], "title": "An Experimental Exploration of In-Memory Computing for Multi-Layer Perceptrons", "comment": "19 pages, 1 figures, and 2 tables", "summary": "In modern computer architectures, the performance of many memory-bound\nworkloads (e.g., machine learning, graph processing, databases) is limited by\nthe data movement bottleneck that emerges when transferring large amounts of\ndata between the main memory and the central processing unit (CPU).\nProcessing-in-memory is an emerging computing paradigm that aims to alleviate\nthis data movement bottleneck by performing computation close to or within the\nmemory units, where data resides. One example of a prevalent workload whose\nperformance is bound by the data movement bottleneck is the training and\ninference process of artificial neural networks. In this work, we analyze the\npotential of modern general-purpose PiM architectures to accelerate neural\nnetworks. To this end, we selected the UPMEM PiM system, the first commercially\navailable real-world general-purpose PiM architecture. We compared the\nimplementation of multilayer perceptrons (MLPs) in PiM with a sequential\nbaseline running on an Intel Xeon CPU. The UPMEM implementation achieves up to\n$259\\times$ better performance for inference of large batch sizes when compared\nagainst the CPU that exploits the size of the available PiM memory.\nAdditionally, two smaller MLPs were implemented using UPMEM's working SRAM\n(WRAM), a scratchpad memory, to evaluate their performance against a low-power\nNvidia Jetson graphics processing unit (GPU), providing further insights into\nthe efficiency of UPMEM's PiM for neural network inference. Results show that\nusing WRAM achieves kernel execution times for MLP inference of under $3$ ms,\nwhich is within the same order of magnitude as low-power GPUs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u73b0\u4ee3\u901a\u7528PiM\u67b6\u6784\uff08\u5982UPMEM\uff09\u5728\u52a0\u901f\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edfCPU\uff0c\u5e76\u4e0e\u4f4e\u529f\u8017GPU\u76f8\u5f53\u3002", "motivation": "\u73b0\u4ee3\u8ba1\u7b97\u673a\u67b6\u6784\u4e2d\uff0c\u5185\u5b58\u5bc6\u96c6\u578b\u4efb\u52a1\uff08\u5982\u673a\u5668\u5b66\u4e60\uff09\u7684\u6027\u80fd\u53d7\u9650\u4e8e\u6570\u636e\u79fb\u52a8\u74f6\u9888\uff0cPiM\uff08\u5185\u5b58\u5185\u8ba1\u7b97\uff09\u65e8\u5728\u901a\u8fc7\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7814\u7a76\u9009\u62e9\u4e86UPMEM PiM\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u7684\u63a8\u7406\uff0c\u5e76\u4e0eIntel Xeon CPU\u548c\u4f4e\u529f\u8017Nvidia Jetson GPU\u8fdb\u884c\u4e86\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "UPMEM PiM\u5728\u5927\u6279\u91cf\u63a8\u7406\u4e2d\u6027\u80fd\u6bd4CPU\u5feb259\u500d\uff0c\u4f7f\u7528WRAM\u7684MLP\u63a8\u7406\u65f6\u95f4\u4f4e\u4e8e3ms\uff0c\u4e0e\u4f4e\u529f\u8017GPU\u76f8\u5f53\u3002", "conclusion": "PiM\u67b6\u6784\u5728\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u4e2d\u5177\u6709\u663e\u8457\u6027\u80fd\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u8ba1\u7b97\u8303\u5f0f\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.07472", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.07472", "abs": "https://arxiv.org/abs/2508.07472", "authors": ["Ramesh Adhikari", "Costas Busch", "Miroslav Popovic"], "title": "On the Efficiency of Dynamic Transaction Scheduling in Blockchain Sharding", "comment": "15 pages, 2 figures, accepted as a regular paper at 39th\n  International Symposium on Distributed Computing (DISC 2025)", "summary": "Sharding is a technique to speed up transaction processing in blockchains,\nwhere the $n$ processing nodes in the blockchain are divided into $s$ disjoint\ngroups (shards) that can process transactions in parallel. We study dynamic\nscheduling problems on a shard graph $G_s$ where transactions arrive online\nover time and are not known in advance. Each transaction may access at most $k$\nshards, and we denote by $d$ the worst distance between a transaction and its\naccessing (destination) shards (the parameter $d$ is unknown to the shards). To\nhandle different values of $d$, we assume a locality sensitive decomposition of\n$G_s$ into clusters of shards, where every cluster has a leader shard that\nschedules transactions for the cluster. We first examine the simpler case of\nthe stateless model, where leaders are not aware of the current state of the\ntransaction accounts, and we prove a $O(d \\log^2 s \\cdot \\min\\{k, \\sqrt{s}\\})$\ncompetitive ratio for latency. We then consider the stateful model, where\nleader shards gather the current state of accounts, and we prove a $O(\\log\ns\\cdot \\min\\{k, \\sqrt{s}\\}+\\log^2 s)$ competitive ratio for latency. Each\nleader calculates the schedule in polynomial time for each transaction that it\nprocesses. We show that for any $\\epsilon > 0$, approximating the optimal\nschedule within a $(\\min\\{k, \\sqrt{s}\\})^{1 -\\epsilon}$ factor is NP-hard.\nHence, our bound for the stateful model is within a poly-log factor from the\nbest possibly achievable. To the best of our knowledge, this is the first work\nto establish provably efficient dynamic scheduling algorithms for blockchain\nsharding systems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u533a\u5757\u94fe\u5206\u7247\u7cfb\u7edf\u4e2d\u7684\u52a8\u6001\u8c03\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5728\u65e0\u72b6\u6001\u548c\u6709\u72b6\u6001\u6a21\u578b\u4e0b\u7684\u7ade\u4e89\u6bd4\uff0c\u5e76\u8bc1\u660e\u4e86\u8fd1\u4f3c\u6700\u4f18\u8c03\u5ea6\u7684NP\u96be\u5ea6\u3002", "motivation": "\u533a\u5757\u94fe\u5206\u7247\u6280\u672f\u901a\u8fc7\u5e76\u884c\u5904\u7406\u4ea4\u6613\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u52a8\u6001\u8c03\u5ea6\u95ee\u9898\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5728\u5206\u7247\u56fe$G_s$\u4e0a\uff0c\u901a\u8fc7\u9886\u5bfc\u8005\u5206\u7247\u8c03\u5ea6\u4ea4\u6613\uff0c\u5206\u522b\u5206\u6790\u65e0\u72b6\u6001\u548c\u6709\u72b6\u6001\u6a21\u578b\u7684\u7ade\u4e89\u6bd4\u3002", "result": "\u65e0\u72b6\u6001\u6a21\u578b\u7ade\u4e89\u6bd4\u4e3a$O(d \\log^2 s \\cdot \\min\\{k, \\sqrt{s}\\})$\uff0c\u6709\u72b6\u6001\u6a21\u578b\u4e3a$O(\\log s\\cdot \\min\\{k, \\sqrt{s}\\}+\\log^2 s)$\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u4e3a\u533a\u5757\u94fe\u5206\u7247\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u8bc1\u660e\u9ad8\u6548\u7684\u52a8\u6001\u8c03\u5ea6\u7b97\u6cd5\uff0c\u7ade\u4e89\u6bd4\u63a5\u8fd1\u6700\u4f18\u3002"}}
{"id": "2508.07605", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.07605", "abs": "https://arxiv.org/abs/2508.07605", "authors": ["Zhong Zheng", "Michael E. Papka", "Zhiling Lan"], "title": "Coordinated Power Management on Heterogeneous Systems", "comment": null, "summary": "Performance prediction is essential for energy-efficient computing in\nheterogeneous computing systems that integrate CPUs and GPUs. However,\ntraditional performance modeling methods often rely on exhaustive offline\nprofiling, which becomes impractical due to the large setting space and the\nhigh cost of profiling large-scale applications. In this paper, we present\nOPEN, a framework consists of offline and online phases. The offline phase\ninvolves building a performance predictor and constructing an initial dense\nmatrix. In the online phase, OPEN performs lightweight online profiling, and\nleverages the performance predictor with collaborative filtering to make\nperformance prediction. We evaluate OPEN on multiple heterogeneous systems,\nincluding those equipped with A100 and A30 GPUs. Results show that OPEN\nachieves prediction accuracy up to 98.29\\%. This demonstrates that OPEN\neffectively reduces profiling cost while maintaining high accuracy, making it\npractical for power-aware performance modeling in modern HPC environments.\nOverall, OPEN provides a lightweight solution for performance prediction under\npower constraints, enabling better runtime decisions in power-aware computing\nenvironments.", "AI": {"tldr": "OPEN\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6027\u80fd\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u6784\u5efa\u6027\u80fd\u9884\u6d4b\u5668\u548c\u5728\u7ebf\u8f7b\u91cf\u7ea7\u5206\u6790\uff0c\u7ed3\u5408\u534f\u540c\u8fc7\u6ee4\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u5206\u6790\u6210\u672c\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff08\u6700\u9ad898.29%\uff09\u3002", "motivation": "\u5f02\u6784\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u4f20\u7edf\u6027\u80fd\u5efa\u6a21\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u6210\u672c\u7684\u79bb\u7ebf\u5206\u6790\uff0cOPEN\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "OPEN\u5206\u4e3a\u79bb\u7ebf\u548c\u5728\u7ebf\u9636\u6bb5\uff1a\u79bb\u7ebf\u9636\u6bb5\u6784\u5efa\u6027\u80fd\u9884\u6d4b\u5668\u548c\u521d\u59cb\u77e9\u9635\uff0c\u5728\u7ebf\u9636\u6bb5\u8fdb\u884c\u8f7b\u91cf\u7ea7\u5206\u6790\u5e76\u5229\u7528\u534f\u540c\u8fc7\u6ee4\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u5728\u914d\u5907A100\u548cA30 GPU\u7684\u5f02\u6784\u7cfb\u7edf\u4e0a\uff0cOPEN\u7684\u9884\u6d4b\u51c6\u786e\u7387\u9ad8\u8fbe98.29%\u3002", "conclusion": "OPEN\u4e3a\u73b0\u4ee3\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684\u6027\u80fd\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u529f\u8017\u611f\u77e5\u7684\u8ba1\u7b97\u73af\u5883\u3002"}}
{"id": "2508.07640", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.07640", "abs": "https://arxiv.org/abs/2508.07640", "authors": ["Chanh Nguyen", "Monowar Bhuyan", "Erik Elmroth"], "title": "Taming Cold Starts: Proactive Serverless Scheduling with Model Predictive Control", "comment": "8 pages, 8 figures, preprint accepted at MASCOTS 2025", "summary": "Serverless computing has transformed cloud application deployment by\nintroducing a fine-grained, event-driven execution model that abstracts away\ninfrastructure management. Its on-demand nature makes it especially appealing\nfor latency-sensitive and bursty workloads. However, the cold start problem,\ni.e., where the platform incurs significant delay when provisioning new\ncontainers, remains the Achilles' heel of such platforms.\n  This paper presents a predictive serverless scheduling framework based on\nModel Predictive Control to proactively mitigate cold starts, thereby improving\nend-to-end response time. By forecasting future invocations, the controller\njointly optimizes container prewarming and request dispatching, improving\nlatency while minimizing resource overhead.\n  We implement our approach on Apache OpenWhisk, deployed on a Kubernetes-based\ntestbed. Experimental results using real-world function traces and synthetic\nworkloads demonstrate that our method significantly outperforms\nstate-of-the-art baselines, achieving up to 85% lower tail latency and a 34%\nreduction in resource usage.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u9884\u6d4b\u6027\u65e0\u670d\u52a1\u5668\u8c03\u5ea6\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u8c03\u7528\u4e3b\u52a8\u7f13\u89e3\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u54cd\u5e94\u901f\u5ea6\u5e76\u51cf\u5c11\u8d44\u6e90\u5f00\u9500\u3002", "motivation": "\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u56e0\u5176\u7ec6\u7c92\u5ea6\u3001\u4e8b\u4ef6\u9a71\u52a8\u7684\u6267\u884c\u6a21\u5f0f\u53d7\u5230\u9752\u7750\uff0c\u4f46\u51b7\u542f\u52a8\u95ee\u9898\u4ecd\u662f\u5176\u5173\u952e\u74f6\u9888\uff0c\u5f71\u54cd\u5ef6\u8fdf\u654f\u611f\u548c\u7a81\u53d1\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6280\u672f\uff0c\u8054\u5408\u4f18\u5316\u5bb9\u5668\u9884\u70ed\u548c\u8bf7\u6c42\u8c03\u5ea6\uff0c\u9884\u6d4b\u672a\u6765\u8c03\u7528\u4ee5\u51cf\u5c11\u51b7\u542f\u52a8\u5ef6\u8fdf\u3002", "result": "\u5728Apache OpenWhisk\u548cKubernetes\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0c\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c3e\u90e8\u5ef6\u8fdf\u964d\u4f4e85%\uff0c\u8d44\u6e90\u4f7f\u7528\u51cf\u5c1134%\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u7684\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2508.07703", "categories": ["cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.07703", "abs": "https://arxiv.org/abs/2508.07703", "authors": ["Adri Bhattacharya", "Pritam Goswami", "Evangelos Bampas", "Partha Sarathi Mandal"], "title": "Perpetual exploration in anonymous synchronous networks with a Byzantine black hole", "comment": "This paper has been accepted at DISC 2025", "summary": "In this paper, we investigate: ``How can a group of initially co-located\nmobile agents perpetually explore an unknown graph, when one stationary node\noccasionally behaves maliciously, under an adversary's control?'' We call this\nnode a ``Byzantine black hole (BBH)'' and at any given round it may choose to\ndestroy all visiting agents, or none. This subtle power can drastically\nundermine classical exploration strategies designed for an always active black\nhole. We study this perpetual exploration problem in the presence of at most\none BBH, without initial knowledge of the network size. Since the underlying\ngraph may be 1-connected, perpetual exploration of the entire graph may be\ninfeasible. We thus define two variants: \\pbmPerpExpl\\ and \\pbmPerpExplHome. In\nthe former, the agents are tasked to perform perpetual exploration of at least\none component, obtained after the exclusion of the BBH. In the latter, the\nagents are tasked to perform perpetual exploration of the component which\ncontains the \\emph{home} node, where agents are initially co-located.\nNaturally, \\pbmPerpExplHome\\ is a special case of \\pbmPerpExpl. Agents operate\nunder a synchronous scheduler and communicate in a face-to-face model. Our goal\nis to determine the minimum number of agents necessary and sufficient to solve\nthese problems. In acyclic networks, we obtain optimal algorithms that solve\n\\pbmPerpExpl\\ with $4$ agents, and \\pbmPerpExplHome\\ with $6$ agents in trees.\nThe lower bounds hold even in path graphs. In general graphs, we give a\nnon-trivial lower bound of $2\\Delta-1$ agents for \\pbmPerpExpl, and an upper\nbound of $3\\Delta+3$ agents for \\pbmPerpExplHome. To our knowledge, this is the\nfirst study of a black-hole variant in arbitrary networks without initial\ntopological knowledge.", "AI": {"tldr": "\u7814\u7a76\u79fb\u52a8\u4ee3\u7406\u5728\u672a\u77e5\u56fe\u4e2d\u5982\u4f55\u6301\u7eed\u63a2\u7d22\uff0c\u5f53\u5b58\u5728\u4e00\u4e2a\u53ef\u80fd\u6076\u610f\u7684\u9759\u6b62\u8282\u70b9\uff08Byzantine black hole, BBH\uff09\u65f6\uff0c\u786e\u5b9a\u6240\u9700\u7684\u6700\u5c0f\u4ee3\u7406\u6570\u91cf\u3002", "motivation": "\u63a2\u7d22\u5728\u6076\u610f\u8282\u70b9\u5b58\u5728\u4e0b\u7684\u56fe\u63a2\u7d22\u95ee\u9898\uff0c\u586b\u8865\u4e86\u65e0\u521d\u59cb\u62d3\u6251\u77e5\u8bc6\u4e0b\u9ed1\u6d1e\u53d8\u4f53\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5b9a\u4e49\u4e86\u4e24\u79cd\u63a2\u7d22\u53d8\u4f53\uff08PerpExpl\u548cPerpExplHome\uff09\uff0c\u5728\u540c\u6b65\u8c03\u5ea6\u548c\u9762\u5bf9\u9762\u901a\u4fe1\u6a21\u578b\u4e0b\uff0c\u8bbe\u8ba1\u7b97\u6cd5\u5e76\u5206\u6790\u6240\u9700\u4ee3\u7406\u6570\u91cf\u3002", "result": "\u5728\u6811\u72b6\u7f51\u7edc\u4e2d\uff0cPerpExpl\u97004\u4e2a\u4ee3\u7406\uff0cPerpExplHome\u97006\u4e2a\uff1b\u4e00\u822c\u56fe\u4e2d\uff0cPerpExpl\u4e0b\u754c\u4e3a2\u0394-1\uff0cPerpExplHome\u4e0a\u754c\u4e3a3\u0394+3\u3002", "conclusion": "\u9996\u6b21\u5728\u65e0\u521d\u59cb\u62d3\u6251\u77e5\u8bc6\u7684\u4efb\u610f\u7f51\u7edc\u4e2d\u7814\u7a76\u9ed1\u6d1e\u53d8\u4f53\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u7b97\u6cd5\u652f\u6301\u3002"}}
{"id": "2508.07744", "categories": ["cs.DC", "cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07744", "abs": "https://arxiv.org/abs/2508.07744", "authors": ["Ingo Friese", "Jochen Klaffer", "Mandy Galkow-Schneider", "Sergiy Melnyk", "Qiuheng Zhou", "Hans Dieter Schotten"], "title": "Over-the-Top Resource Broker System for Split Computing: An Approach to Distribute Cloud Computing Infrastructure", "comment": null, "summary": "6G network architectures will usher in a wave of innovative services and\ncapabilities, introducing concepts like split computing and dynamic processing\nnodes. This implicates a paradigm where accessing resources seamlessly aligns\nwith diverse processing node characteristics, ensuring a uniform interface. In\nthis landscape, the identity of the operator becomes inconsequential, paving\nthe way for a collaborative ecosystem where multiple providers contribute to a\nshared pool of resources. At the core of this vision is the guarantee of\nspecific performance parameters, precisely tailored to the location and service\nrequirements. A consistent layer, as the abstraction of the complexities of\ndifferent infrastructure providers, is needed to simplify service deployment.\nOne promising approach is the introduction of an over-the-top broker for\nresource allocation, which streamlines the integration of these services into\nthe network and cloud infrastructure of the future. This paper explores the\nrole of the broker in two split computing scenarios. By abstracting the\ncomplexities of various infrastructures, the broker proves to be a versatile\nsolution applicable not only to cloud environments but also to networks and\nbeyond. Additionally, a detailed discussion of a proof-of-concept\nimplementation provides insights into the broker's actual architectural\nframework.", "AI": {"tldr": "6G\u7f51\u7edc\u67b6\u6784\u5c06\u5f15\u5165\u521b\u65b0\u670d\u52a1\u548c\u80fd\u529b\uff0c\u5982\u62c6\u5206\u8ba1\u7b97\u548c\u52a8\u6001\u5904\u7406\u8282\u70b9\uff0c\u901a\u8fc7\u7edf\u4e00\u63a5\u53e3\u7b80\u5316\u8d44\u6e90\u8bbf\u95ee\u3002\u8fd0\u8425\u5546\u8eab\u4efd\u4e0d\u518d\u91cd\u8981\uff0c\u591a\u63d0\u4f9b\u5546\u5171\u4eab\u8d44\u6e90\u6c60\u3002\u6027\u80fd\u53c2\u6570\u9700\u6839\u636e\u4f4d\u7f6e\u548c\u670d\u52a1\u9700\u6c42\u5b9a\u5236\uff0c\u62bd\u8c61\u5c42\u7b80\u5316\u90e8\u7f72\u3002\u672c\u6587\u63a2\u8ba8\u8d44\u6e90\u5206\u914d\u4ee3\u7406\u5728\u62c6\u5206\u8ba1\u7b97\u573a\u666f\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u5c55\u793a\u5176\u67b6\u6784\u6846\u67b6\u3002", "motivation": "6G\u7f51\u7edc\u9700\u8981\u652f\u6301\u591a\u6837\u5316\u670d\u52a1\u548c\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\uff0c\u540c\u65f6\u7b80\u5316\u590d\u6742\u57fa\u7840\u8bbe\u65bd\u7684\u96c6\u6210\uff0c\u4ee5\u4fc3\u8fdb\u591a\u63d0\u4f9b\u5546\u534f\u4f5c\u3002", "method": "\u5f15\u5165\u8d44\u6e90\u5206\u914d\u4ee3\u7406\uff0c\u62bd\u8c61\u4e0d\u540c\u57fa\u7840\u8bbe\u65bd\u7684\u590d\u6742\u6027\uff0c\u652f\u6301\u4e91\u73af\u5883\u548c\u7f51\u7edc\u3002\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\u5c55\u793a\u5176\u67b6\u6784\u3002", "result": "\u4ee3\u7406\u5728\u62c6\u5206\u8ba1\u7b97\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u591a\u529f\u80fd\u6027\uff0c\u9002\u7528\u4e8e\u4e91\u548c\u7f51\u7edc\u73af\u5883\u3002\u6982\u5ff5\u9a8c\u8bc1\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u8d44\u6e90\u5206\u914d\u4ee3\u7406\u662f\u7b80\u53166G\u7f51\u7edc\u8d44\u6e90\u7ba1\u7406\u548c\u670d\u52a1\u90e8\u7f72\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2508.07756", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.07756", "abs": "https://arxiv.org/abs/2508.07756", "authors": ["Hanze Zhang", "Rong Chen", "Haibo Chen"], "title": "Towards Lock Modularization for Heterogeneous Environments", "comment": null, "summary": "Modern hardware environments are becoming increasingly heterogeneous, leading\nto the emergence of applications specifically designed to exploit this\nheterogeneity. Efficiently adopting locks in these applications poses distinct\nchallenges. The uneven distribution of resources in such environments can\ncreate bottlenecks for lock operations, severely hindering application\nperformance. Existing solutions are often tailored to specific types of\nhardware, which underutilizes resources on other components within\nheterogeneous environments.\n  This paper introduces a new design principle: decomposing locks across\nhardware components to fully utilize unevenly distributed resources in\nheterogeneous environments. Following this principle, we propose lock\nmodularization, a systematic approach that decomposes a lock into independent\nmodules and assigns them to appropriate hardware components. This approach\naligns the resource requirements of lock modules with the attributes of\nspecific hardware components, maximizing strengths while minimizing weaknesses.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5f02\u6784\u786c\u4ef6\u73af\u5883\u4e2d\u5206\u89e3\u9501\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u901a\u8fc7\u9501\u6a21\u5757\u5316\u5145\u5206\u5229\u7528\u8d44\u6e90\u3002", "motivation": "\u73b0\u4ee3\u786c\u4ef6\u73af\u5883\u65e5\u76ca\u5f02\u6784\uff0c\u73b0\u6709\u9501\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u786c\u4ef6\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5f02\u6784\u73af\u5883\u7684\u8d44\u6e90\u3002", "method": "\u63d0\u51fa\u9501\u6a21\u5757\u5316\u65b9\u6cd5\uff0c\u5c06\u9501\u5206\u89e3\u4e3a\u72ec\u7acb\u6a21\u5757\u5e76\u5206\u914d\u5230\u5408\u9002\u7684\u786c\u4ef6\u7ec4\u4ef6\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u5339\u914d\u9501\u6a21\u5757\u7684\u8d44\u6e90\u9700\u6c42\u4e0e\u786c\u4ef6\u7279\u6027\uff0c\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u9501\u6a21\u5757\u5316\u662f\u5f02\u6784\u73af\u5883\u4e2d\u9ad8\u6548\u5229\u7528\u8d44\u6e90\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.07934", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.07934", "abs": "https://arxiv.org/abs/2508.07934", "authors": ["Lorenzo La Corte", "Syed Aftab Rashid", "Andrei-Marian Dan"], "title": "Performance Evaluation of Brokerless Messaging Libraries", "comment": "11 pages, 9 figures", "summary": "Messaging systems are essential for efficiently transferring large volumes of\ndata, ensuring rapid response times and high-throughput communication. The\nstate-of-the-art on messaging systems mainly focuses on the performance\nevaluation of brokered messaging systems, which use an intermediate broker to\nguarantee reliability and quality of service. However, over the past decade,\nbrokerless messaging systems have emerged, eliminating the single point of\nfailure and trading off reliability guarantees for higher performance. Still,\nthe state-of-the-art on evaluating the performance of brokerless systems is\nscarce. In this work, we solely focus on brokerless messaging systems. First,\nwe perform a qualitative analysis of several possible candidates, to find the\nmost promising ones. We then design and implement an extensive open-source\nbenchmarking suite to systematically and fairly evaluate the performance of the\nchosen libraries, namely, ZeroMQ, NanoMsg, and NanoMsg-Next-Generation (NNG).\nWe evaluate these libraries considering different metrics and workload\nconditions, and provide useful insights into their limitations. Our analysis\nenables practitioners to select the most suitable library for their\nrequirements.", "AI": {"tldr": "\u672c\u6587\u4e13\u6ce8\u4e8e\u65e0\u4ee3\u7406\u6d88\u606f\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u548c\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u8bc4\u4f30\u4e86ZeroMQ\u3001NanoMsg\u548cNNG\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u9009\u62e9\u4f9d\u636e\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u4ee3\u7406\u6d88\u606f\u7cfb\u7edf\uff0c\u800c\u65e0\u4ee3\u7406\u7cfb\u7edf\u7684\u6027\u80fd\u8bc4\u4f30\u7a00\u7f3a\uff0c\u672c\u6587\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u7b5b\u9009\u5019\u9009\u5e93\uff0c\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7cfb\u7edf\u8bc4\u4f30ZeroMQ\u3001NanoMsg\u548cNNG\u7684\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u4e86\u4e0d\u540c\u6307\u6807\u548c\u5de5\u4f5c\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u5404\u5e93\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u9009\u62e9\u5408\u9002\u65e0\u4ee3\u7406\u6d88\u606f\u7cfb\u7edf\u5e93\u7684\u4f9d\u636e\u3002"}}
{"id": "2508.08022", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08022", "abs": "https://arxiv.org/abs/2508.08022", "authors": ["Roopkatha Banerjee", "Sampath Koti", "Gyanendra Singh", "Anirban Chakraborty", "Gurunath Gurrala", "Bhushan Jagyasi", "Yogesh Simmhan"], "title": "Optimizing Federated Learning for Scalable Power-demand Forecasting in Microgrids", "comment": null, "summary": "Real-time monitoring of power consumption in cities and micro-grids through\nthe Internet of Things (IoT) can help forecast future demand and optimize grid\noperations. But moving all consumer-level usage data to the cloud for\npredictions and analysis at fine time scales can expose activity patterns.\nFederated Learning~(FL) is a privacy-sensitive collaborative DNN training\napproach that retains data on edge devices, trains the models on private data\nlocally, and aggregates the local models in the cloud. But key challenges\nexist: (i) clients can have non-independently identically distributed~(non-IID)\ndata, and (ii) the learning should be computationally cheap while scaling to\n1000s of (unseen) clients. In this paper, we develop and evaluate several\noptimizations to FL training across edge and cloud for time-series demand\nforecasting in micro-grids and city-scale utilities using DNNs to achieve a\nhigh prediction accuracy while minimizing the training cost. We showcase the\nbenefit of using exponentially weighted loss while training and show that it\nfurther improves the prediction of the final model. Finally, we evaluate these\nstrategies by validating over 1000s of clients for three states in the US from\nthe OpenEIA corpus, and performing FL both in a pseudo-distributed setting and\na Pi edge cluster. The results highlight the benefits of the proposed methods\nover baselines like ARIMA and DNNs trained for individual consumers, which are\nnot scalable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7684\u5b9e\u65f6\u7535\u529b\u6d88\u8017\u76d1\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316FL\u8bad\u7ec3\u89e3\u51b3\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u52a0\u6743\u635f\u5931\u51fd\u6570\u7684\u4f18\u52bf\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u5ba2\u6237\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u901a\u8fc7\u7269\u8054\u7f51\uff08IoT\uff09\u5b9e\u65f6\u76d1\u6d4b\u57ce\u5e02\u548c\u5fae\u7535\u7f51\u7684\u7535\u529b\u6d88\u8017\u6709\u52a9\u4e8e\u9884\u6d4b\u9700\u6c42\u548c\u4f18\u5316\u7535\u7f51\u8fd0\u8425\uff0c\u4f46\u5c06\u6570\u636e\u4e0a\u4f20\u81f3\u4e91\u7aef\u53ef\u80fd\u66b4\u9732\u9690\u79c1\u3002\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u80fd\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u9762\u4e34\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u9700\u6c42\u9884\u6d4b\u7684FL\u4f18\u5316\u65b9\u6cd5\uff0c\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\uff0c\u5f15\u5165\u52a0\u6743\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5728\u8fb9\u7f18\u8bbe\u5907\u548c\u4e91\u7aef\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u3002", "result": "\u5728OpenEIA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\uff0c\u8986\u76d6\u7f8e\u56fd\u4e09\u4e2a\u5dde\u76841000\u591a\u4e2a\u5ba2\u6237\uff0c\u7ed3\u679c\u663e\u793a\u5176\u4f18\u4e8eARIMA\u548c\u5355\u5ba2\u6237DNN\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684FL\u4f18\u5316\u65b9\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7535\u529b\u6d88\u8017\u76d1\u6d4b\u3002"}}
{"id": "2508.08064", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.08064", "abs": "https://arxiv.org/abs/2508.08064", "authors": ["Marco Bernardo", "Federico Calandra", "Andrea Esposito", "Francesco Fabris"], "title": "On the Operational Resilience of CBDC: Threats and Prospects of Formal Validation for Offline Payments", "comment": null, "summary": "Information and communication technologies are by now employed in most\nactivities, including economics and finance. Despite the extraordinary power of\nmodern computers and the vast amount of memory, some results of theoretical\ncomputer science imply the impossibility of certifying software quality in\ngeneral. With the exception of safety-critical systems, this has primarily\nconcerned the information processed by confined systems, with limited\nsocio-economic consequences. In the emerging era of technologies for exchanging\ndigital money and tokenized assets over the Internet - such as central bank\ndigital currencies (CBDCs) - even a minor bug could trigger a financial\ncollapse. Although the aforementioned impossibility results cannot be overcome\nin an absolute sense, there exist formal methods that can provide assertions of\ncomputing systems correctness. We advocate their use to validate the\noperational resilience of software infrastructures enabling CBDCs, with special\nemphasis on offline payments as they constitute a very critical issue.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u91d1\u878d\u6280\u672f\uff08\u5982CBDCs\uff09\u4e2d\u8f6f\u4ef6\u8d28\u91cf\u8ba4\u8bc1\u7684\u91cd\u8981\u6027\uff0c\u5c3d\u7ba1\u8ba1\u7b97\u673a\u79d1\u5b66\u7406\u8bba\u8868\u660e\u5b8c\u5168\u8ba4\u8bc1\u662f\u4e0d\u53ef\u80fd\u7684\uff0c\u4f46\u4ecd\u9700\u4f7f\u7528\u5f62\u5f0f\u5316\u65b9\u6cd5\u786e\u4fdd\u7cfb\u7edf\u6b63\u786e\u6027\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u91d1\u878d\u6280\u672f\u7684\u53d1\u5c55\uff08\u5982CBDCs\uff09\uff0c\u5373\u4f7f\u662f\u5c0f\u9519\u8bef\u4e5f\u53ef\u80fd\u5f15\u53d1\u91d1\u878d\u5d29\u6e83\uff0c\u56e0\u6b64\u9700\u8981\u786e\u4fdd\u8f6f\u4ef6\u57fa\u7840\u8bbe\u65bd\u7684\u64cd\u4f5c\u5f39\u6027\u3002", "method": "\u5efa\u8bae\u4f7f\u7528\u5f62\u5f0f\u5316\u65b9\u6cd5\u6765\u9a8c\u8bc1CBDCs\u8f6f\u4ef6\u57fa\u7840\u8bbe\u65bd\u7684\u6b63\u786e\u6027\uff0c\u7279\u522b\u662f\u79bb\u7ebf\u652f\u4ed8\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u3002", "result": "\u5f62\u5f0f\u5316\u65b9\u6cd5\u867d\u4e0d\u80fd\u5b8c\u5168\u514b\u670d\u7406\u8bba\u4e0a\u7684\u4e0d\u53ef\u80fd\u6027\uff0c\u4f46\u80fd\u63d0\u4f9b\u7cfb\u7edf\u6b63\u786e\u6027\u7684\u65ad\u8a00\u3002", "conclusion": "\u5728CBDCs\u7b49\u5173\u952e\u91d1\u878d\u6280\u672f\u4e2d\uff0c\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u5e94\u7528\u5bf9\u786e\u4fdd\u7cfb\u7edf\u7a33\u5065\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
