{"id": "2508.00341", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00341", "abs": "https://arxiv.org/abs/2508.00341", "authors": ["Shengheng Liu", "Ningning Fu", "Zhonghao Zhang", "Yongming Huang", "Tony Q. S. Quek"], "title": "Integrated user scheduling and beam steering in over-the-air federated learning for mobile IoT", "comment": "To appear in ACM TOIT. 24 pages, 8 figures", "summary": "The rising popularity of Internet of things (IoT) has spurred technological\nadvancements in mobile internet and interconnected systems. While offering\nflexible connectivity and intelligent applications across various domains, IoT\nservice providers must gather vast amounts of sensitive data from users, which\nnonetheless concomitantly raises concerns about privacy breaches. Federated\nlearning (FL) has emerged as a promising decentralized training paradigm to\ntackle this challenge. This work focuses on enhancing the aggregation\nefficiency of distributed local models by introducing over-the-air computation\ninto the FL framework. Due to radio resource scarcity in large-scale networks,\nonly a subset of users can participate in each training round. This highlights\nthe need for effective user scheduling and model transmission strategies to\noptimize communication efficiency and inference accuracy. To address this, we\npropose an integrated approach to user scheduling and receive beam steering,\nsubject to constraints on the number of selected users and transmit power.\nLeveraging the difference-of-convex technique, we decompose the primal\nnon-convex optimization problem into two sub-problems, yielding an iterative\nsolution. While effective, the computational load of the iterative method\nhampers its practical implementation. To overcome this, we further propose a\nlow-complexity user scheduling policy based on characteristic analysis of the\nwireless channel to directly determine the user subset without iteration.\nExtensive experiments validate the superiority of the proposed method in terms\nof aggregation error and learning performance over existing approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7a7a\u4e2d\u8ba1\u7b97\u548c\u8054\u90a6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u8c03\u5ea6\u548c\u63a5\u6536\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u901a\u4fe1\u6548\u7387\u548c\u63a8\u7406\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4f4e\u590d\u6742\u5ea6\u7684\u7528\u6237\u8c03\u5ea6\u7b56\u7565\u3002", "motivation": "\u7269\u8054\u7f51\uff08IoT\uff09\u7684\u666e\u53ca\u5e26\u6765\u4e86\u9690\u79c1\u95ee\u9898\uff0c\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4f5c\u4e3a\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u65b9\u6cd5\u88ab\u63d0\u51fa\uff0c\u4f46\u5927\u89c4\u6a21\u7f51\u7edc\u4e2d\u7684\u8d44\u6e90\u7a00\u7f3a\u6027\u9650\u5236\u4e86\u5176\u6548\u7387\u3002", "method": "\u5f15\u5165\u7a7a\u4e2d\u8ba1\u7b97\u5230FL\u6846\u67b6\uff0c\u63d0\u51fa\u7528\u6237\u8c03\u5ea6\u548c\u63a5\u6536\u6ce2\u675f\u6210\u5f62\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5dee\u5206\u51f8\u6280\u672f\u5206\u89e3\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u4f4e\u590d\u6742\u5ea6\u7528\u6237\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u805a\u5408\u8bef\u5dee\u548c\u5b66\u4e60\u6027\u80fd\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8d44\u6e90\u7a00\u7f3a\u4e0b\u7684FL\u6548\u7387\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f4e\u590d\u6742\u5ea6\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00426", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00426", "abs": "https://arxiv.org/abs/2508.00426", "authors": ["Rohan Gandhi", "Ankur Mallick", "Ken Sueda", "Rui Liang"], "title": "Tetris: Efficient Intra-Datacenter Calls Packing for Large Conferencing Services", "comment": null, "summary": "Conference services like Zoom, Microsoft Teams, and Google Meet facilitate\nmillions of daily calls, yet ensuring high performance at low costs remains a\nsignificant challenge. This paper revisits the problem of packing calls across\nMedia Processor (MP) servers that host the calls within individual datacenters\n(DCs). We show that the algorithm used in Teams -- a large scale conferencing\nservice as well as other state-of-art algorithms are prone to placing calls\nresulting in some of the MPs becoming hot (high CPU utilization) that leads to\ndegraded performance and/or elevated hosting costs. The problem arises from\ndisregarding the variability in CPU usage among calls, influenced by\ndifferences in participant numbers and media types (audio/video), compounded by\nbursty call arrivals. To tackle this, we propose Tetris, a multi-step framework\nwhich (a) optimizes initial call assignments by leveraging historical data and\n(b) periodically migrates calls from hot MPs using linear optimization, aiming\nto minimize hot MP usage. Evaluation based on a 24-hour trace of over 10\nmillion calls in one DC shows that Tetris reduces participant numbers on hot\nMPs by at least 2.5X.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTetris\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u547c\u53eb\u5206\u914d\u548c\u5b9a\u671f\u8fc1\u79fb\u70edMP\u4e0a\u7684\u547c\u53eb\uff0c\u663e\u8457\u51cf\u5c11\u70edMP\u7684\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u4f1a\u8bae\u670d\u52a1\uff08\u5982Teams\uff09\u7684\u547c\u53eb\u5206\u914d\u7b97\u6cd5\u5bb9\u6613\u5bfc\u81f4\u90e8\u5206MP\u8fc7\u70ed\uff0c\u5f71\u54cd\u6027\u80fd\u5e76\u589e\u52a0\u6210\u672c\u3002", "method": "Tetris\u6846\u67b6\u7ed3\u5408\u5386\u53f2\u6570\u636e\u4f18\u5316\u521d\u59cb\u5206\u914d\uff0c\u5e76\u5229\u7528\u7ebf\u6027\u4f18\u5316\u5b9a\u671f\u8fc1\u79fb\u70edMP\u4e0a\u7684\u547c\u53eb\u3002", "result": "\u5728\u5305\u542b1000\u4e07\u6b21\u547c\u53eb\u768424\u5c0f\u65f6\u8ffd\u8e2a\u4e2d\uff0cTetris\u5c06\u70edMP\u4e0a\u7684\u53c2\u4e0e\u8005\u6570\u91cf\u51cf\u5c11\u81f3\u5c112.5\u500d\u3002", "conclusion": "Tetris\u6709\u6548\u89e3\u51b3\u4e86MP\u8fc7\u70ed\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002"}}
{"id": "2508.00622", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00622", "abs": "https://arxiv.org/abs/2508.00622", "authors": ["Kapel Dev", "Yash Madhwal", "Sofia Shevelo", "Pavel Osinenko", "Yury Yanovich"], "title": "SwarnRaft: Leveraging Consensus for Robust Drone Swarm Coordination in GNSS-Degraded Environments", "comment": null, "summary": "Unmanned aerial vehicle (UAV) swarms are increasingly used in critical\napplications such as aerial mapping, environmental monitoring, and autonomous\ndelivery. However, the reliability of these systems is highly dependent on\nuninterrupted access to the Global Navigation Satellite Systems (GNSS) signals,\nwhich can be disrupted in real-world scenarios due to interference,\nenvironmental conditions, or adversarial attacks, causing disorientation,\ncollision risks, and mission failure. This paper proposes SwarnRaft, a\nblockchain-inspired positioning and consensus framework for maintaining\ncoordination and data integrity in UAV swarms operating under GNSS-denied\nconditions. SwarnRaft leverages the Raft consensus algorithm to enable\ndistributed drones (nodes) to agree on state updates such as location and\nheading, even in the absence of GNSS signals for one or more nodes. In our\nprototype, each node uses GNSS and local sensing, and communicates over WiFi in\na simulated swarm. Upon signal loss, consensus is used to reconstruct or verify\nthe position of the failed node based on its last known state and trajectory.\nOur system demonstrates robustness in maintaining swarm coherence and fault\ntolerance through a lightweight, scalable communication model. This work offers\na practical and secure foundation for decentralized drone operation in\nunpredictable environments.", "AI": {"tldr": "SwarnRaft\u662f\u4e00\u4e2a\u57fa\u4e8e\u533a\u5757\u94fe\u542f\u53d1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u65f6\u7ef4\u6301\u65e0\u4eba\u673a\u7fa4\u7684\u534f\u8c03\u548c\u6570\u636e\u5b8c\u6574\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\u7fa4\u5728\u5173\u952e\u5e94\u7528\u4e2d\u4f9d\u8d56GNSS\u4fe1\u53f7\uff0c\u4f46\u4fe1\u53f7\u53ef\u80fd\u56e0\u5e72\u6270\u6216\u653b\u51fb\u4e2d\u65ad\uff0c\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u3002", "method": "\u5229\u7528Raft\u5171\u8bc6\u7b97\u6cd5\uff0c\u4f7f\u65e0\u4eba\u673a\u5728GNSS\u4fe1\u53f7\u7f3a\u5931\u65f6\u4ecd\u80fd\u5c31\u72b6\u6001\u66f4\u65b0\u8fbe\u6210\u4e00\u81f4\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u5728\u6a21\u62df\u7fa4\u4e2d\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u901a\u4fe1\u6a21\u578b\uff0c\u4fdd\u6301\u4e86\u7fa4\u7684\u4e00\u81f4\u6027\u548c\u5bb9\u9519\u6027\u3002", "conclusion": "SwarnRaft\u4e3a\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u7684\u53bb\u4e2d\u5fc3\u5316\u65e0\u4eba\u673a\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u5b89\u5168\u7684\u57fa\u7840\u3002"}}
{"id": "2508.00475", "categories": ["cs.AR", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.00475", "abs": "https://arxiv.org/abs/2508.00475", "authors": ["Yunhao Ma", "Yanyu Lin", "Mingjing Li", "Puli Quan", "Chenlin Zhou", "Wenyue Zhang", "Zhiwei Zhong", "Wanyi Jia", "Xueke Zhu", "Qingyan Meng", "Huihui Zhou", "Fengwei An"], "title": "E2ATST: A Temporal-Spatial Optimized Energy-Efficient Architecture for Training Spiking Transformer", "comment": null, "summary": "(1) Pengcheng Laboratory, (2) Southern University of Science and Technology,\n(3) Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences,\n(4) University of Chinese Academy of Sciences", "AI": {"tldr": "\u6458\u8981\u672a\u63d0\u4f9b\u5177\u4f53\u5185\u5bb9\uff0c\u4ec5\u5217\u51fa\u673a\u6784\u4fe1\u606f\u3002", "motivation": "\u672a\u660e\u786e\u63d0\u53ca\u7814\u7a76\u52a8\u673a\u3002", "method": "\u672a\u63cf\u8ff0\u7814\u7a76\u65b9\u6cd5\u3002", "result": "\u672a\u63d0\u4f9b\u7814\u7a76\u7ed3\u679c\u3002", "conclusion": "\u672a\u7ed9\u51fa\u7ed3\u8bba\u3002"}}
{"id": "2508.00005", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00005", "abs": "https://arxiv.org/abs/2508.00005", "authors": ["Tilman Hinnerichs", "Bart Swinkels", "Jaap de Jong", "Reuben Gardos Reid", "Tudor Magirescu", "Neil Yorke-Smith", "Sebastijan Dumancic"], "title": "Modelling Program Spaces in Program Synthesis with Constraints", "comment": null, "summary": "A core challenge in program synthesis is taming the large space of possible\nprograms. Since program synthesis is essentially a combinatorial search, the\ncommunity has sought to leverage powerful combinatorial constraint solvers.\nHere, constraints are used to express the program semantics, but not as a\npotentially potent tool to remove unwanted programs. Recent inductive logic\nprogramming approaches introduce constraints on the program's syntax to be\nsynthesized. These syntactic constraints allow for checking and propagating a\nconstraint without executing the program, and thus for arbitrary operators. In\nthis work, we leverage syntactic constraints to model program spaces, defining\nnot just solutions that are feasible, but also ones that are likely useful. To\ndemonstrate this idea, we introduce BART, a solver that efficiently propagates\nand solves these constraints. We evaluate BART on program space enumeration\ntasks, finding that the constraints eliminate up to 99 percent of the program\nspace, and that modeling program spaces significantly reduces enumeration time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8bed\u6cd5\u7ea6\u675f\u4f18\u5316\u7a0b\u5e8f\u5408\u6210\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7BART\u6c42\u89e3\u5668\u9ad8\u6548\u4f20\u64ad\u548c\u89e3\u51b3\u7ea6\u675f\uff0c\u663e\u8457\u51cf\u5c11\u7a0b\u5e8f\u7a7a\u95f4\u548c\u679a\u4e3e\u65f6\u95f4\u3002", "motivation": "\u7a0b\u5e8f\u5408\u6210\u7684\u6838\u5fc3\u6311\u6218\u662f\u5904\u7406\u5e9e\u5927\u7684\u7a0b\u5e8f\u7a7a\u95f4\uff0c\u4f20\u7edf\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u7ea6\u675f\u53bb\u9664\u65e0\u7528\u7a0b\u5e8f\u3002", "method": "\u5f15\u5165\u8bed\u6cd5\u7ea6\u675f\u5efa\u6a21\u7a0b\u5e8f\u7a7a\u95f4\uff0c\u5f00\u53d1BART\u6c42\u89e3\u5668\u9ad8\u6548\u4f20\u64ad\u548c\u89e3\u51b3\u7ea6\u675f\u3002", "result": "\u7ea6\u675f\u6d88\u9664\u4e86\u9ad8\u8fbe99%\u7684\u7a0b\u5e8f\u7a7a\u95f4\uff0c\u663e\u8457\u51cf\u5c11\u679a\u4e3e\u65f6\u95f4\u3002", "conclusion": "\u8bed\u6cd5\u7ea6\u675f\u662f\u4f18\u5316\u7a0b\u5e8f\u5408\u6210\u7684\u6709\u6548\u5de5\u5177\uff0cBART\u6c42\u89e3\u5668\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u3002"}}
{"id": "2508.00013", "categories": ["cs.PL", "I.2.6; F.1.1"], "pdf": "https://arxiv.org/pdf/2508.00013", "abs": "https://arxiv.org/abs/2508.00013", "authors": ["Zurabi Kobaladze", "Anna Arnania", "Tamar Sanikidze"], "title": "From Provable Correctness to Probabilistic Generation: A Comparative Review of Program Synthesis Paradigms", "comment": "78 pages. Undergraduate thesis project submitted in partial\n  fulfillment of the requirements for the Bachelor's degree in Computer Science\n  at Kutaisi International University", "summary": "Program synthesis--the automated generation of executable code from\nhigh-level specifications--has been a central goal of computer science for over\nfifty years. This thesis provides a comparative literature review of the main\nparadigms that have shaped the field, tracing its evolution from formal logic\nbased methods to recent advances using large scale neural models. We examine\nfive key approaches: logic based (deductive) synthesis, inductive (example\nbased) synthesis, sketch/schema based synthesis, large language model based\nsynthesis, and neuro-symbolic hybrids. For each, we analyze foundational\nprinciples, notable systems, and practical applications, highlighting trade\noffs between correctness guarantees, specification requirements, search\ncomplexity, and expressive power. By reviewing developments from formally\nverified synthesis tools such as KIDS and Coq to data driven models generating\nprobabilistic code from natural language like Codex, we present a comprehensive\nnarrative of progress and ongoing challenges. This work emphasizes the\ntransition from symbolic to hybrid neuro-symbolic methods and outlines future\ndirections for reliable and scalable program synthesis.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u7a0b\u5e8f\u5408\u6210\u7684\u4e94\u79cd\u4e3b\u8981\u65b9\u6cd5\uff0c\u4ece\u903b\u8f91\u57fa\u7840\u5230\u795e\u7ecf\u7b26\u53f7\u6df7\u5408\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5176\u53d1\u5c55\u5386\u7a0b\u3001\u4f18\u7f3a\u70b9\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u7a0b\u5e8f\u5408\u6210\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u6838\u5fc3\u76ee\u6807\u4e4b\u4e00\uff0c\u672c\u6587\u65e8\u5728\u6bd4\u8f83\u548c\u603b\u7ed3\u8be5\u9886\u57df\u7684\u4e3b\u8981\u8303\u5f0f\uff0c\u63ed\u793a\u5176\u6f14\u53d8\u548c\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u4e94\u79cd\u65b9\u6cd5\uff1a\u903b\u8f91\u57fa\u7840\u5408\u6210\u3001\u5f52\u7eb3\u5408\u6210\u3001\u8349\u56fe/\u6a21\u5f0f\u5408\u6210\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5408\u6210\u548c\u795e\u7ecf\u7b26\u53f7\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u63ed\u793a\u4e86\u4ece\u7b26\u53f7\u65b9\u6cd5\u5230\u795e\u7ecf\u7b26\u53f7\u6df7\u5408\u65b9\u6cd5\u7684\u8f6c\u53d8\uff0c\u5f3a\u8c03\u4e86\u6b63\u786e\u6027\u3001\u641c\u7d22\u590d\u6742\u6027\u548c\u8868\u8fbe\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u672a\u6765\u65b9\u5411\u662f\u53d1\u5c55\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u795e\u7ecf\u7b26\u53f7\u6df7\u5408\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u7a0b\u5e8f\u5408\u6210\u7684\u6311\u6218\u3002"}}
{"id": "2508.00016", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.00016", "abs": "https://arxiv.org/abs/2508.00016", "authors": ["Matt Kaufmann", "Yahya Sohail", "Warren A. Hunt Jr"], "title": "Extended Abstract: Mutable Objects with Several Implementations", "comment": "In Proceedings ACL2 2025, arXiv:2507.18567", "summary": "This extended abstract outlines an ACL2 feature, attach-stobj, that first\nappeared in ACL2 Version 8.6 (October, 2024). This feature supports different\nexecutable operations for a given abstract stobj, without requiring\nrecertification of the book that introduces that stobj or theorems about it.\nThe paper provides background as well as a user-level overview and some\nimplementation notes.", "AI": {"tldr": "\u4ecb\u7ecdACL2\u7684attach-stobj\u529f\u80fd\uff0c\u652f\u6301\u5bf9\u62bd\u8c61stobj\u7684\u4e0d\u540c\u53ef\u6267\u884c\u64cd\u4f5c\uff0c\u65e0\u9700\u91cd\u65b0\u8ba4\u8bc1\u76f8\u5173\u4e66\u7c4d\u6216\u5b9a\u7406\u3002", "motivation": "\u7b80\u5316\u5bf9\u62bd\u8c61stobj\u7684\u64cd\u4f5c\uff0c\u907f\u514d\u56e0\u4fee\u6539\u64cd\u4f5c\u800c\u91cd\u65b0\u8ba4\u8bc1\u76f8\u5173\u4e66\u7c4d\u6216\u5b9a\u7406\u3002", "method": "\u901a\u8fc7attach-stobj\u529f\u80fd\u5b9e\u73b0\uff0c\u63d0\u4f9b\u7528\u6237\u7ea7\u6982\u8ff0\u548c\u5b9e\u73b0\u8bf4\u660e\u3002", "result": "\u6210\u529f\u652f\u6301\u5bf9\u62bd\u8c61stobj\u7684\u4e0d\u540c\u64cd\u4f5c\uff0c\u65e0\u9700\u91cd\u65b0\u8ba4\u8bc1\u3002", "conclusion": "attach-stobj\u529f\u80fd\u63d0\u9ad8\u4e86ACL2\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.00422", "categories": ["cs.PL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00422", "abs": "https://arxiv.org/abs/2508.00422", "authors": ["Varun Bharti", "Shashwat Jha", "Dhruv Kumar", "Pankaj Jalote"], "title": "Automated Type Annotation in Python Using Large Language Models", "comment": "Under Review", "summary": "Type annotations in Python enhance maintainability and error detection.\nHowever, generating these annotations manually is error prone and requires\nextra effort. Traditional automation approaches like static analysis, machine\nlearning, and deep learning struggle with limited type vocabularies, behavioral\nover approximation, and reliance on large labeled datasets. In this work, we\nexplore the use of LLMs for generating type annotations in Python. We develop a\ngenerate check repair pipeline: the LLM proposes annotations guided by a\nConcrete Syntax Tree representation, a static type checker (Mypy) verifies\nthem, and any errors are fed back for iterative refinement. We evaluate four\nLLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini\n(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.\nWe first measure the proportion of code snippets annotated by LLMs for which\nMyPy reported no errors (i.e., consistent results): GPT 4oMini achieved\nconsistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,\nand O4Mini each reached approximately 88.6% consistency (around 11.4%\nfailures). To measure annotation quality, we then compute exact-match and\nbase-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini\nperform the best, achieving up to 70.5% exact match and 79.1% base type\naccuracy, requiring under one repair iteration on average. Our results\ndemonstrate that general-purpose and reasoning optimized LLMs, without any task\nspecific fine tuning or additional training can be effective in generating\nconsistent type annotations.They perform competitively with traditional deep\nlearning techniques which require large labeled dataset for training. While our\nwork focuses on Python, the pipeline can be extended to other optionally typed\nimperative languages like Ruby", "AI": {"tldr": "\u4f7f\u7528LLMs\u751f\u6210Python\u7c7b\u578b\u6ce8\u91ca\uff0c\u901a\u8fc7\u751f\u6210-\u68c0\u67e5-\u4fee\u590d\u6d41\u7a0b\u63d0\u9ad8\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u624b\u52a8\u751f\u6210Python\u7c7b\u578b\u6ce8\u91ca\u6613\u51fa\u9519\u4e14\u8017\u65f6\uff0c\u4f20\u7edf\u81ea\u52a8\u5316\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u63a2\u7d22LLMs\u7684\u6f5c\u529b\u3002", "method": "\u5f00\u53d1\u751f\u6210-\u68c0\u67e5-\u4fee\u590d\u6d41\u7a0b\uff0c\u5229\u7528LLMs\u63d0\u51fa\u6ce8\u91ca\uff0c\u9759\u6001\u68c0\u67e5\u5668\u9a8c\u8bc1\u5e76\u8fed\u4ee3\u4fee\u6b63\u3002", "result": "GPT 4.1mini\u548cO3Mini\u8868\u73b0\u6700\u4f73\uff0c\u4e00\u81f4\u6027\u8fbe88.6%\uff0c\u51c6\u786e\u7387\u6700\u9ad870.5%\u3002", "conclusion": "\u65e0\u9700\u5fae\u8c03\u7684LLMs\u80fd\u6709\u6548\u751f\u6210\u7c7b\u578b\u6ce8\u91ca\uff0c\u6027\u80fd\u5ab2\u7f8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u53ef\u9009\u7c7b\u578b\u8bed\u8a00\u3002"}}
{"id": "2508.00482", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.00482", "abs": "https://arxiv.org/abs/2508.00482", "authors": ["Erdem Yildirim", "Albert Schimpf", "Stefan Wehr", "Annette Bieniusa"], "title": "Semantic Subtyping for Maps in Erlang", "comment": null, "summary": "In this paper we will construct a set-theoretic model of types featuring type\nvariables, base types, set-theoretic types and map types. Syntax of map types\nspans all the map types available in Erlang. The model of types is used to\ndefine a semantic subtyping relation based on set containment. The novelty of\nthis work is the definition of subtyping over parameteric map types.", "AI": {"tldr": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7c7b\u578b\u53d8\u91cf\u3001\u57fa\u7840\u7c7b\u578b\u3001\u96c6\u5408\u7c7b\u578b\u548c\u6620\u5c04\u7c7b\u578b\u7684\u96c6\u5408\u8bba\u6a21\u578b\uff0c\u5b9a\u4e49\u4e86\u57fa\u4e8e\u96c6\u5408\u5305\u542b\u7684\u8bed\u4e49\u5b50\u7c7b\u578b\u5173\u7cfb\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u53c2\u6570\u5316\u6620\u5c04\u7c7b\u578b\u7684\u5b50\u7c7b\u578b\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u7c7b\u578b\u7cfb\u7edf\u4e2d\u7684\u5b50\u7c7b\u578b\u5173\u7cfb\uff0c\u7279\u522b\u662f\u9488\u5bf9Erlang\u4e2d\u7684\u6620\u5c04\u7c7b\u578b\uff0c\u586b\u8865\u53c2\u6570\u5316\u6620\u5c04\u7c7b\u578b\u5b50\u7c7b\u578b\u5b9a\u4e49\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u96c6\u5408\u8bba\u6a21\u578b\uff0c\u6db5\u76d6\u591a\u79cd\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7\u96c6\u5408\u5305\u542b\u5b9a\u4e49\u8bed\u4e49\u5b50\u7c7b\u578b\u5173\u7cfb\u3002", "result": "\u6210\u529f\u5b9a\u4e49\u4e86\u53c2\u6570\u5316\u6620\u5c04\u7c7b\u578b\u7684\u5b50\u7c7b\u578b\u5173\u7cfb\uff0c\u6269\u5c55\u4e86\u7c7b\u578b\u7cfb\u7edf\u7684\u7406\u8bba\u6846\u67b6\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u7c7b\u578b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u53c2\u6570\u5316\u6620\u5c04\u7c7b\u578b\u65f6\u5177\u6709\u521b\u65b0\u6027\u3002"}}
{"id": "2508.00534", "categories": ["cs.PL", "cs.CL", "D.3.2; F.3.2; D.3.1"], "pdf": "https://arxiv.org/pdf/2508.00534", "abs": "https://arxiv.org/abs/2508.00534", "authors": ["Mikel Vandeloise"], "title": "Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations", "comment": "Preprint submitted to the Journal of Object Technology on July 29,\n  2025. Data available upon request until peer-review is completed", "summary": "The rise of multi-paradigm languages challenges traditional classification\nmethods, leading to practical software engineering issues like interoperability\ndefects. This systematic literature review (SLR) maps the formal foundations of\nprogramming paradigms. Our objective is twofold: (1) to assess the state of the\nart of classification formalisms and their limitations, and (2) to identify the\nconceptual primitives and mathematical frameworks for a more powerful,\nreconstructive approach.\n  Based on a synthesis of 74 primary studies, we find that existing taxonomies\nlack conceptual granularity, a unified formal basis, and struggle with hybrid\nlanguages. In response, our analysis reveals a strong convergence toward a\ncompositional reconstruction of paradigms. This approach identifies a minimal\nset of orthogonal, atomic primitives and leverages mathematical frameworks,\npredominantly Type theory, Category theory and Unifying Theories of Programming\n(UTP), to formally guarantee their compositional properties.\n  We conclude that the literature reflects a significant intellectual shift\naway from classification towards these promising formal, reconstructive\nframeworks. This review provides a map of this evolution and proposes a\nresearch agenda for their unification.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff08SLR\uff09\u63a2\u8ba8\u4e86\u591a\u8303\u5f0f\u8bed\u8a00\u7684\u5174\u8d77\u5bf9\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b66\u6846\u67b6\u7684\u91cd\u6784\u65b9\u6cd5\u3002", "motivation": "\u591a\u8303\u5f0f\u8bed\u8a00\u7684\u5174\u8d77\u5bfc\u81f4\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\uff0c\u5f15\u53d1\u4e86\u4e92\u64cd\u4f5c\u6027\u7f3a\u9677\u7b49\u5b9e\u9645\u95ee\u9898\u3002", "method": "\u57fa\u4e8e74\u9879\u4e3b\u8981\u7814\u7a76\u7684\u7efc\u5408\u5206\u6790\uff0c\u8bc4\u4f30\u73b0\u6709\u5206\u7c7b\u5f62\u5f0f\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u7d22\u57fa\u4e8e\u7c7b\u578b\u7406\u8bba\u3001\u8303\u7574\u8bba\u548c\u7edf\u4e00\u7f16\u7a0b\u7406\u8bba\uff08UTP\uff09\u7684\u91cd\u6784\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u5206\u7c7b\u7f3a\u4e4f\u6982\u5ff5\u7c92\u5ea6\u3001\u7edf\u4e00\u7684\u5f62\u5f0f\u57fa\u7840\uff0c\u5e76\u96be\u4ee5\u5e94\u5bf9\u6df7\u5408\u8bed\u8a00\u3002\u91cd\u6784\u65b9\u6cd5\u901a\u8fc7\u6b63\u4ea4\u539f\u5b50\u539f\u8bed\u548c\u6570\u5b66\u6846\u67b6\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\u3002", "conclusion": "\u6587\u732e\u53cd\u6620\u4e86\u4ece\u5206\u7c7b\u5411\u91cd\u6784\u6846\u67b6\u7684\u663e\u8457\u8f6c\u53d8\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u7edf\u4e00\u8fd9\u4e9b\u6846\u67b6\u7684\u7814\u7a76\u8bae\u7a0b\u3002"}}
