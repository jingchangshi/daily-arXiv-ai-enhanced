<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor](https://arxiv.org/abs/2512.16926)
*Oren Bell,Harun Teper,Mario Günzel,Chris Gill,Jian-Jia Chen*

Main category: cs.DC

TL;DR: 本文提出了一种基于事件执行器的固定作业级优先级调度方法，用于在单处理器系统上调度任意ROS2 DAG任务，将ROS2应用抽象为树森林并映射到传统实时DAG任务模型。


<details>
  <summary>Details</summary>
Motivation: 当前ROS2调度方法主要局限于简单的链式任务调度，缺乏对任意有向无环图（DAG）任务的分析能力，需要填补实时系统理论与ROS2调度分析之间的差距。

Method: 使用事件执行器实现固定作业级优先级调度器，将ROS2应用抽象为树森林结构，映射到传统实时DAG任务模型，需要特殊的事件队列实现和支持LIFO顺序消息传递的通信中间件。

Result: 该方法能够在缺乏通常所需的优先级信息的情况下，生成与传统固定优先级DAG任务调度器相同的调度结果，成功将ROS2应用映射到实时DAG任务模型。

Conclusion: 该方法进一步缩小了成熟的实时系统理论与ROS2调度分析之间的差距，为ROS2中任意DAG任务的调度提供了新的解决方案。

Abstract: This paper addresses limitations of current scheduling methods in the Robot Operating System (ROS)2, focusing on scheduling tasks beyond simple chains and analyzing arbitrary Directed Acyclic Graphs (DAGs). While previous research has focused mostly on chain-based scheduling with ad-hoc response time analyses, we propose a novel approach using the events executor to implement fixed-job-level-priority schedulers for arbitrary ROS2 graphs on uniprocessor systems. We demonstrate that ROS 2 applications can be abstracted as forests of trees, enabling the mapping of ROS 2 applications to traditional real-time DAG task models. Our usage of the events executor requires a special implementation of the events queue and a communication middleware that supports LIFO-ordered message delivery, features not yet standard in ROS2. We show that our implementation generates the same schedules as a conventional fixed-priority DAG task scheduler, in spite of lacking access to the precedence information that usually is required. This further closes the gap between established real-time systems theory and ROS2 scheduling analyses.

</details>


### [2] [LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation](https://arxiv.org/abs/2512.17023)
*Patrick Diehl,Noujoud Nader,Deepti Gupta*

Main category: cs.DC

TL;DR: 评估大型语言模型在生成高性能计算代码方面的能力，特别是针对Mandelbrot集在不同并行范式下的C++实现。


<details>
  <summary>Details</summary>
Motivation: 并行编程是高性能计算中最具挑战性的方面之一，需要深入了解同步、通信和内存模型。虽然现代C++标准和OpenMP、MPI等框架简化了并行性，但掌握这些范式仍然很复杂。大型语言模型在自动化代码生成方面显示出潜力，但它们在生成正确高效的高性能计算代码方面的有效性尚未得到充分理解。

Method: 系统评估包括ChatGPT 4和5、Claude和LLaMA在内的领先LLM，在生成使用共享内存、基于指令和分布式内存范式的Mandelbrot集C++实现方面的能力。每个生成的程序都使用GCC 11.5.0编译和执行，以评估其正确性、鲁棒性和可扩展性。

Result: 结果显示，ChatGPT-4和ChatGPT-5在语法精度和可扩展性能方面表现出色。

Conclusion: 大型语言模型，特别是ChatGPT-4和ChatGPT-5，在生成高性能计算代码方面具有实际应用价值，能够实现良好的语法精度和可扩展性能。

Abstract: Parallel programming remains one of the most challenging aspects of High-Performance Computing (HPC), requiring deep knowledge of synchronization, communication, and memory models. While modern C++ standards and frameworks like OpenMP and MPI have simplified parallelism, mastering these paradigms is still complex. Recently, Large Language Models (LLMs) have shown promise in automating code generation, but their effectiveness in producing correct and efficient HPC code is not well understood. In this work, we systematically evaluate leading LLMs including ChatGPT 4 and 5, Claude, and LLaMA on the task of generating C++ implementations of the Mandelbrot set using shared-memory, directive-based, and distributed-memory paradigms. Each generated program is compiled and executed with GCC 11.5.0 to assess its correctness, robustness, and scalability. Results show that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance.

</details>


### [3] [Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving](https://arxiv.org/abs/2512.17077)
*Jiakun Fan,Yanglin Zhang,Xiangchen Li,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: dLLM-Serve是一个针对扩散大语言模型的高效服务系统，通过内存优化、计算调度和生成质量协同优化，解决了扩散模型特有的内存危机和资源振荡问题，相比现有方法在吞吐量和延迟方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型研究主要关注内核级优化，缺乏针对生产环境中扩散过程独特内存动态的整体服务框架。作者识别出dLLM特有的"内存占用危机"，由单一对数张量和计算密集型"刷新"阶段与带宽密集型"重用"阶段之间的严重资源振荡驱动。

Method: dLLM-Serve提出三个关键技术：1) 对数感知激活预算，用于分解瞬态张量峰值；2) 阶段多路复用调度器，用于交错异构请求阶段；3) 头中心稀疏注意力，将逻辑稀疏性与物理存储解耦。

Result: 在多样化工作负载（LiveBench、Burst、OSC）和GPU（RTX 4090、L40S）上评估，相比最先进基线，dLLM-Serve在消费级RTX 4090上吞吐量提升1.61-1.81倍，在服务器级NVIDIA L40S上提升1.60-1.74倍，在重度争用下尾部延迟降低近4倍。

Conclusion: dLLM-Serve为可扩展的dLLM推理建立了首个蓝图，将理论算法稀疏性转化为跨异构硬件的实际时钟加速，解决了扩散模型服务的关键瓶颈。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to Autoregressive Models (ARMs), utilizing parallel decoding to overcome sequential bottlenecks. However, existing research focuses primarily on kernel-level optimizations, lacking a holistic serving framework that addresses the unique memory dynamics of diffusion processes in production. We identify a critical "memory footprint crisis" specific to dLLMs, driven by monolithic logit tensors and the severe resource oscillation between compute-bound "Refresh" phases and bandwidth-bound "Reuse" phases. To bridge this gap, we present dLLM-Serve, an efficient dLLM serving system that co-optimizes memory footprint, computational scheduling, and generation quality. dLLM-Serve introduces Logit-Aware Activation Budgeting to decompose transient tensor peaks, a Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and Head-Centric Sparse Attention to decouple logical sparsity from physical storage. We evaluate dLLM-Serve on diverse workloads (LiveBench, Burst, OSC) and GPUs (RTX 4090, L40S). Relative to the state-of-the-art baseline, dLLM-Serve improves throughput by 1.61$\times$-1.81$\times$ on the consumer-grade RTX 4090 and 1.60$\times$-1.74$\times$ on the server-grade NVIDIA L40S, while reducing tail latency by nearly 4$\times$ under heavy contention. dLLM-Serve establishes the first blueprint for scalable dLLM inference, converting theoretical algorithmic sparsity into tangible wall-clock acceleration across heterogeneous hardware.

</details>


### [4] [Scalable Distributed Vector Search via Accuracy Preserving Index Construction](https://arxiv.org/abs/2512.17264)
*Yuming Xu,Qianxi Zhang,Qi Chen,Baotong Lu,Menghao Li,Philip Adams,Mingqin Li,Zengzhong Li,Jing Liu,Cheng Li,Fan Yang*

Main category: cs.DC

TL;DR: SPIRE是一个可扩展的向量索引系统，通过平衡分区粒度和递归构建多级索引，在数十亿向量规模下实现高精度、低延迟和高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有分布式索引设计在扩展到数十亿向量时难以平衡准确性、延迟和吞吐量之间的权衡，需要新的索引设计来解决这一挑战。

Method: 1. 识别平衡的分区粒度以避免读取成本爆炸；2. 引入保持精度的递归构建方法，构建具有可预测搜索成本和稳定准确性的多级索引。

Result: 在46个节点上扩展到80亿向量的实验中，SPIRE实现了高可扩展性，比最先进系统的吞吐量提高了9.64倍。

Conclusion: SPIRE通过创新的分区策略和递归索引构建方法，成功解决了大规模向量搜索中的可扩展性挑战，在准确性、延迟和吞吐量之间取得了良好平衡。

Abstract: Scaling Approximate Nearest Neighbor Search (ANNS) to billions of vectors requires distributed indexes that balance accuracy, latency, and throughput. Yet existing index designs struggle with this tradeoff. This paper presents SPIRE, a scalable vector index based on two design decisions. First, it identifies a balanced partition granularity that avoids read-cost explosion. Second, it introduces an accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy. In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.

</details>


### [5] [The HEAL Data Platform](https://arxiv.org/abs/2512.17506)
*Brienna M. Larrick,L. Philip Schumm,Mingfei Shao,Craig Barnes,Anthony Juehne,Hara Prasad Juvvla,Michael B. Kranz,Michael Lukowski,Clint Malson,Jessica N. Mazerik,Christopher G. Meyer,Jawad Qureshi,Erin Spaniol,Andrea Tentner,Alexander VanTol,Peter Vassilatos,Sara Volk de Garcia,Robert L. Grossman*

Main category: cs.DC

TL;DR: 开发基于云的联邦数据平台，作为NIH HEAL计划数据的统一搜索、发现和分析入口，促进数据共享和二次利用。


<details>
  <summary>Details</summary>
Motivation: HEAL计划产生的数据分散在多个NIH和第三方数据仓库中，需要统一平台来发现和访问这些多样化数据资源。

Method: 基于开源Gen3平台构建，采用网状架构，集成认证授权、持久标识符、元数据管理等框架服务，与19个数据仓库互操作。

Result: 平台已收录1000多项HEAL研究，每月数百用户使用，提供丰富元数据，集成安全云计算环境支持二次分析。

Conclusion: HEAL数据平台通过统一发现和FAIR原则最大化数据价值，加速数据共享和二次利用，支持成瘾研究。

Abstract: Objective: The objective was to develop a cloud-based, federated system to serve as a single point of search, discovery and analysis for data generated under the NIH Helping to End Addiction Long-term (HEAL) Initiative.
  Materials and methods: The HEAL Data Platform is built on the open source Gen3 platform, utilizing a small set of framework services and exposed APIs to interoperate with both NIH and non-NIH data repositories. Framework services include those for authentication and authorization, creating persistent identifiers for data objects, and adding and updating metadata.
  Results: The HEAL Data Platform serves as a single point of discovery of over one thousand studies funded under the HEAL Initiative. With hundreds of users per month, the HEAL Data Platform provides rich metadata and interoperates with data repositories and commons to provide access to shared datasets. Secure, cloud-based compute environments that are integrated with STRIDES facilitate secondary analysis of HEAL data. The HEAL Data Platform currently interoperates with nineteen data repositories.
  Discussion: Studies funded under the HEAL Initiative generate a wide variety of data types, which are deposited across multiple NIH and third-party data repositories. The mesh architecture of the HEAL Data Platform provides a single point of discovery of these data resources, accelerating and facilitating secondary use.
  Conclusion: The HEAL Data Platform enables search, discovery, and analysis of data that are deposited in connected data repositories and commons. By ensuring that these data are fully Findable, Accessible, Interoperable and Reusable (FAIR), the HEAL Data Platform maximizes the value of data generated under the HEAL Initiative.

</details>


### [6] [Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing](https://arxiv.org/abs/2512.17574)
*Lingxiao Zhao,Haoran Zhou,Yuezhi Che,Dazhao Cheng*

Main category: cs.DC

TL;DR: FlashCodec和UnifiedServe联合优化多模态大语言模型推理系统，通过GPU协作解码降低延迟，解耦视觉编码与LLM推理消除阻塞，实现吞吐量提升4.4倍


<details>
  <summary>Details</summary>
Motivation: 现有MLLM推理系统存在两个主要瓶颈：1）多模态预处理（特别是视频解码）在CPU上执行导致首token延迟高；2）视觉编码器与LLM推理阶段异构，造成阻塞和资源利用率低

Method: 提出FlashCodec（多GPU协作视频解码）和UnifiedServe（逻辑解耦但物理共享GPU资源）两个互补设计，联合优化端到端MLLM流水线

Result: 系统可服务3.0倍更多请求或满足1.5倍更严格的SLO要求，相比SOTA系统实现高达4.4倍的吞吐量提升

Conclusion: 通过协同优化多模态预处理和推理阶段，显著提升MLLM推理系统的性能和资源利用率，为大规模多模态服务提供高效解决方案

Abstract: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.
  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\times$ more requests or enforce 1.5$\times$ tighter SLOs, while achieving up to 4.4$\times$ higher throughput compared to state-of-the-art systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [Torrent: A Distributed DMA for Efficient and Flexible Point-to-Multipoint Data Movement](https://arxiv.org/abs/2512.17589)
*Yunhao Deng,Fanchen Kong,Xiaoling Yi,Ryan Antonio,Marian Verhelst*

Main category: cs.AR

TL;DR: Torrent是一种分布式DMA架构，通过链式写入机制在不修改NoC硬件和互连协议的情况下实现高效的点对多点数据传输，相比单播基线最高可加速7.88倍。


<details>
  <summary>Details</summary>
Motivation: 现代SoC中计算能力与片上通信带宽之间的差距日益扩大，特别是对于AI等数据并行工作负载。高效的点对多点数据传输（如组播）对高性能至关重要，但标准互连协议缺乏原生组播支持。现有解决方案需要修改网络硬件和协议，影响可扩展性和兼容性。

Method: 提出Torrent分布式DMA架构，通过Chainwrite机制在NoC上形成逻辑链，数据像链表一样遍历目标节点。开发两种调度算法根据NoC拓扑确定最优链顺序以优化性能和能耗。

Result: RTL和FPGA原型评估显示，相比网络层组播在性能、灵活性和可扩展性方面有显著优势。相比单播基线最高加速7.88倍。16nm ASIC合成显示面积开销仅1.2%，功耗开销2.3%。每个目标节点的周期开销为82CC，面积开销为207um²。

Conclusion: Torrent通过Chainwrite机制实现了可扩展的点对多点数据传输，无需修改NoC硬件和互连协议，在保持最小硬件开销的同时提供了显著的性能提升。

Abstract: The growing disparity between computational power and on-chip communication bandwidth is a critical bottleneck in modern Systems-on-Chip (SoCs), especially for data-parallel workloads like AI. Efficient point-to-multipoint (P2MP) data movement, such as multicast, is essential for high performance. However, native multicast support is lacking in standard interconnect protocols. Existing P2MP solutions, such as multicast-capable Network-on-Chip (NoC), impose additional overhead to the network hardware and require modifications to the interconnect protocol, compromising scalability and compatibility.
  This paper introduces Torrent, a novel distributed DMA architecture that enables efficient P2MP data transfers without modifying NoC hardware and interconnect protocol. Torrent conducts P2MP data transfers by forming logical chains over the NoC, where the data traverses through targeted destinations resembling a linked list. This Chainwrite mechanism preserves the P2P nature of every data transfer while enabling flexible data transfers to an unlimited number of destinations. To optimize the performance and energy consumption of Chainwrite, two scheduling algorithms are developed to determine the optimal chain order based on NoC topology.
  Our RTL and FPGA prototype evaluations using both synthetic and real workloads demonstrate significant advantages in performance, flexibility, and scalability over network-layer multicast. Compared to the unicast baseline, Torrent achieves up to a 7.88x speedup. ASIC synthesis on 16nm technology confirms the architecture's minimal footprint in area (1.2%) and power (2.3%). Thanks to the Chainwrite, Torrent delivers scalable P2MP data transfers with a small cycle overhead of 82CC and area overhead of 207um2 per destination.

</details>


### [8] [A 14ns-Latency 9Gb/s 0.44mm$^2$ 62pJ/b Short-Blocklength LDPC Decoder ASIC in 22FDX](https://arxiv.org/abs/2512.17834)
*Darja Nonaca,Jérémy Guichemerre,Reinhard Wiesmayr,Nihat Engin Tunali,Christoph Studer*

Main category: cs.AR

TL;DR: 提出一种新型短块长多速率二进制LDPC码，在URLLC应用中优于5G-LDPC码，并通过ASIC实现14ns最低延迟解码


<details>
  <summary>Details</summary>
Motivation: URLLC需要短块长编码，传统polar码SCL解码延迟高、面积效率差，而LDPC码的MP解码在短块长下性能不足

Method: 设计新型短块长多速率二进制LDPC码，采用全并行消息传递解码架构，实现ASIC硬件实现

Result: 提出的LDPC码在相同块长下优于5G-LDPC码，ASIC解码器面积0.44mm²，延迟仅14ns，信息吞吐量9Gb/s，能效62pJ/b

Conclusion: 新型LDPC码结合全并行MP解码为URLLC应用提供了低延迟、高效率的解决方案，优于现有5G-LDPC方案

Abstract: Ultra-reliable low latency communication (URLLC) is a key part of 5G wireless systems. Achieving low latency necessitates codes with short blocklengths for which polar codes with successive cancellation list (SCL) decoding typically outperform message-passing (MP)-based decoding of low-density parity-check (LDPC) codes. However, SCL decoders are known to exhibit high latency and poor area efficiency. In this paper, we propose a new short-blocklength multi-rate binary LDPC code that outperforms the 5G-LDPC code for the same blocklength and is suitable for URLLC applications using fully parallel MP. To demonstrate our code's efficacy, we present a 0.44mm$^2$ GlobalFoundries 22FDX LDPC decoder ASIC which supports three rates and achieves the lowest-in-class decoding latency of 14ns while reaching an information throughput of 9Gb/s at 62pJ/b energy efficiency for a rate-1/2 code with 128-bit blocklength.

</details>
