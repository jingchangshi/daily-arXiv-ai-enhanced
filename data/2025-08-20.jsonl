{"id": "2508.13610", "categories": ["cs.PL", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.13610", "abs": "https://arxiv.org/abs/2508.13610", "authors": ["Basile Pesin", "Celia Picard", "Cyril Allignol"], "title": "Reactive Semantics for User Interface Description Languages", "comment": "In Proceedings ICE 2025, arXiv:2508.12308", "summary": "User Interface Description Languages (UIDLs) are high-level languages that\nfacilitate the development of Human-Machine Interfaces, such as Graphical User\nInterface (GUI) applications. They usually provide first-class primitives to\nspecify how the program reacts to an external event (user input, network\nmessage), and how data flows through the program. Although these\ndomain-specific languages are now widely used to implement safety-critical\nGUIs, little work has been invested in their formalization and verification.\n  In this paper, we propose a denotational semantic model for a core reactive\nUIDL, Smalite, which we argue is expressive enough to encode constructs from\nmore realistic languages. This preliminary work may be used as a stepping stone\nto produce a formally verified compiler for UIDLs."}
{"id": "2508.13611", "categories": ["cs.PL", "cs.LO", "D.3.1; F.3.2"], "pdf": "https://arxiv.org/pdf/2508.13611", "abs": "https://arxiv.org/abs/2508.13611", "authors": ["Clemens Grabmayer", "Maurizio Murgia"], "title": "Bisimilarity and Simulatability of Processes Parameterized by Join Interactions", "comment": "In Proceedings ICE 2025, arXiv:2508.12308", "summary": "Departing from Larsen's concept of parameterized bisimilarity of processes\nwith respect to interaction with environments, we start an exploration of its\nnatural weakening: bisimilarity of unrestricted join interactions with\nenvironments. Parameterized bisimilarity relates processes p and q with respect\nto an environment e if p and q behave bi-similarly while joining --\nrespectively the same -- transitions from e. The weakened variant relates\nprocesses p and q with respect to environment e if the join-interaction\nprocesses p & e and q & e of p and q with e are bisimilar. (Hereby join\ninteractions r & f facilitate a step with label a to r' & f' if and only if r\nand f permit a-steps to r' and f' , respectively.) Join-interaction\nparameterized (ji-parameterized) bisimilarity coincides with parameterized\nbisimilarity for deterministic environments, but that it is a coarser\nequivalence in general. We explain how Larsen's concept can be recovered from\nji-parameterized bisimilarity by 'determinizing' interactions. We show that by\nadaptation to simulatability (simulation preorder) the same concept arises:\nparameterized simulatability coincides with ji-parameterized simulatability.\nFor the discrimination preorder of (ji-)parameterized simulatability on\nenvironments we obtain the same result as Larsen did for parameterized\nbisimilarity. Also, we give a modal-logic characterization of\n(ji-)parameterized simulatability. Finally we gather open problems, and provide\nan outlook on our current related work."}
{"id": "2508.13156", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13156", "abs": "https://arxiv.org/abs/2508.13156", "authors": ["Ping Guo", "Yiting Wang", "Wanghao Ye", "Yexiao He", "Ziyao Wang", "Xiaopeng Dai", "Ang Li", "Qingfu Zhang"], "title": "EvoVerilog: Large Langugage Model Assisted Evolution of Verilog Code", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated great potential in automating\nthe generation of Verilog hardware description language code for hardware\ndesign. This automation is critical to reducing human effort in the complex and\nerror-prone process of hardware design.\n  However, existing approaches predominantly rely on human intervention and\nfine-tuning using curated datasets, limiting their scalability in automated\ndesign workflows.\n  Although recent iterative search techniques have emerged, they often fail to\nexplore diverse design solutions and may underperform simpler approaches such\nas repeated prompting.\n  To address these limitations, we introduce EvoVerilog, a novel framework that\ncombines the reasoning capabilities of LLMs with evolutionary algorithms to\nautomatically generate and refine Verilog code.\n  EvoVerilog utilizes a multiobjective, population-based search strategy to\nexplore a wide range of design possibilities without requiring human\nintervention.\n  Extensive experiments demonstrate that EvoVerilog achieves state-of-the-art\nperformance, with pass@10 scores of 89.1 and 80.2 on the VerilogEval-Machine\nand VerilogEval-Human benchmarks, respectively. Furthermore, the framework\nshowcases its ability to explore diverse designs by simultaneously generating a\nvariety of functional Verilog code while optimizing resource utilization."}
{"id": "2508.13298", "categories": ["cs.DC", "cs.AR", "cs.ET", "cs.PF", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.13298", "abs": "https://arxiv.org/abs/2508.13298", "authors": ["Huynh Q. N. Vo", "Md Tawsif Rahman Chowdhury", "Paritosh Ramanan", "Murat Yildirim", "Gozde Tutuncuoglu"], "title": "Harnessing the Full Potential of RRAMs through Scalable and Distributed In-Memory Computing with Integrated Error Correction", "comment": "Submitted to Nature Communication Contact authors for any info", "summary": "Exponential growth in global computing demand is exacerbated due to the\nhigher-energy requirements of conventional architectures, primarily due to\nenergy-intensive data movement. In-memory computing with Resistive Random\nAccess Memory (RRAM) addresses this by co-integrating memory and processing,\nbut faces significant hurdles related to device-level non-idealities and poor\nscalability for large computing tasks. Here, we introduce \\textbf{MELISO+}\n(In-\\textbf{Me}mory \\textbf{Li}near \\textbf{So}lver), a full-stack, distributed\nframework for energy-efficient in-memory computing. MELISO+ proposes a novel\ntwo-tier error correction mechanism to mitigate device non-idealities and\ndevelops a distributed RRAM computing framework to enable matrix computations\nexceeding dimensions of $65,000 \\times 65,000$. This approach reduces first-\nand second-order arithmetic errors due to device non-idealities by over 90\\%,\nenhances energy efficiency by three to five orders of magnitude, and decreases\nlatency 100-fold. Hence, MELISO+ allows lower-precision RRAM devices to\noutperform high-precision device alternatives in accuracy, energy and latency\nmetrics. By unifying algorithm-hardware co-design with scalable architecture,\nMELISO+ significantly advances sustainable, high-dimensional computing suitable\nfor applications like large language models and generative AI."}
{"id": "2508.13157", "categories": ["cs.AR", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.13157", "abs": "https://arxiv.org/abs/2508.13157", "authors": ["Haohang Xu", "Chengjie Liu", "Qihang Wang", "Wenhao Huang", "Yongjian Xu", "Weiyu Chen", "Anlan Peng", "Zhijun Li", "Bo Li", "Lei Qi", "Jun Yang", "Yuan Du", "Li Du"], "title": "Image2Net: Datasets, Benchmark and Hybrid Framework to Convert Analog Circuit Diagrams into Netlists", "comment": "10 pages, 12 figures, 6 tables", "summary": "Large Language Model (LLM) exhibits great potential in designing of analog\nintegrated circuits (IC) because of its excellence in abstraction and\ngeneralization for knowledge. However, further development of LLM-based analog\nICs heavily relies on textual description of analog ICs, while existing analog\nICs are mostly illustrated in image-based circuit diagrams rather than\ntext-based netlists. Converting circuit diagrams to netlists help LLMs to\nenrich the knowledge of analog IC. Nevertheless, previously proposed conversion\nframeworks face challenges in further application because of limited support of\nimage styles and circuit elements. Up to now, it still remains a challenging\ntask to effectively convert complex circuit diagrams into netlists. To this\nend, this paper constructs and opensources a new dataset with rich styles of\ncircuit diagrams as well as balanced distribution of simple and complex analog\nICs. And a hybrid framework, named Image2Net, is proposed for practical\nconversion from circuit diagrams to netlists. The netlist edit distance (NED)\nis also introduced to precisely assess the difference between the converted\nnetlists and ground truth. Based on our benchmark, Image2Net achieves 80.77\\%\nsuccessful rate, which is 34.62\\%-45.19\\% higher than previous works.\nSpecifically, the proposed work shows 0.116 averaged NED, which is\n62.1\\%-69.6\\% lower than state-of-the-arts."}
{"id": "2508.13370", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.13370", "abs": "https://arxiv.org/abs/2508.13370", "authors": ["Gerald Collom", "Jason Burmark", "Olga Pearce", "Amanda Bienz"], "title": "Persistent and Partitioned MPI for Stencil Communication", "comment": null, "summary": "Many parallel applications rely on iterative stencil operations, whose\nperformance are dominated by communication costs at large scales. Several MPI\noptimizations, such as persistent and partitioned communication, reduce\noverheads and improve communication efficiency through amortized setup costs\nand reduced synchronization of threaded sends. This paper presents the\nperformance of stencil communication in the Comb benchmarking suite when using\nnon blocking, persistent, and partitioned communication routines. The impact of\neach optimization is analyzed at various scales. Further, the paper presents an\nanalysis of the impact of process count, thread count, and message size on\npartitioned communication routines. Measured timings show that persistent MPI\ncommunication can provide a speedup of up to 37% over the baseline MPI\ncommunication, and partitioned MPI communication can provide a speedup of up to\n68%."}
{"id": "2508.13158", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.13158", "abs": "https://arxiv.org/abs/2508.13158", "authors": ["Yongxiang Liu", "Yuchun Ma", "Eren Kurshan", "Glenn Reinman", "Jason Cong"], "title": "Fine Grain 3D Integration for Microarchitecture Design Through Cube Packing Exploration", "comment": "Preprint", "summary": "Most previous 3D IC research focused on stacking traditional 2D silicon\nlayers, so the interconnect reduction is limited to inter-block delays. In this\npaper, we propose techniques that enable efficient exploration of the 3D design\nspace where each logical block can span more than one silicon layers. Although\nfurther power and performance improvement is achievable through fine grain 3D\nintegration, the necessary modeling and tool infrastructure has been mostly\nmissing. We develop a cube packing engine which can simultaneously optimize\nphysical and architectural design for effective utilization of 3D in terms of\nperformance, area and temperature. Our experimental results using a design\ndriver show 36% performance improvement (in BIPS) over 2D and 14% over 3D with\nsingle layer blocks. Additionally multi-layer blocks can provide up to 30%\nreduction in power dissipation compared to the single-layer alternatives. Peak\ntemperature of the design is kept within limits as a result of thermal-aware\nfloorplanning and thermal via insertion techniques."}
{"id": "2508.13374", "categories": ["cs.DC", "cs.ET", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.13374", "abs": "https://arxiv.org/abs/2508.13374", "authors": ["Zhouyu Li", "Zhijing Yang", "Huayue Gu", "Xiaojian Wang", "Yuchen Liu", "Ruozhou Yu"], "title": "OrbitChain: Orchestrating In-orbit Real-time Analytics of Earth Observation Data", "comment": "currently under review", "summary": "Earth observation analytics have the potential to serve many time-sensitive\napplications. However, due to limited bandwidth and duration of\nground-satellite connections, it takes hours or even days to download and\nanalyze data from existing Earth observation satellites, making real-time\ndemands like timely disaster response impossible. Toward real-time analytics,\nwe introduce OrbitChain, a collaborative analytics framework that orchestrates\ncomputational resources across multiple satellites in an Earth observation\nconstellation. OrbitChain decomposes analytics applications into microservices\nand allocates computational resources for time-constrained analysis. A traffic\nrouting algorithm is devised to minimize the inter-satellite communication\noverhead. OrbitChain adopts a pipeline workflow that completes Earth\nobservation tasks in real-time, facilitates time-sensitive applications and\ninter-constellation collaborations such as tip-and-cue. To evaluate OrbitChain,\nwe implement a hardware-in-the-loop orbital computing testbed. Experiments show\nthat our system can complete up to 60% analytics workload than existing Earth\nobservation analytics framework while reducing the communication overhead by up\nto 72%."}
{"id": "2508.13159", "categories": ["cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.13159", "abs": "https://arxiv.org/abs/2508.13159", "authors": ["Ruibai Tang", "Wenlai Zhao"], "title": "Accelerating Transistor-Level Simulation of Integrated Circuits via Equivalence of RC Long-Chain Structures", "comment": null, "summary": "Transistor-level simulation plays a vital role in validating the physical\ncorrectness of integrated circuits. However, such simulations are\ncomputationally expensive. This paper proposes three novel reduction methods\nspecifically tailored to RC long-chain structures with different scales of time\nconstant. Such structures account for an average of 6.34\\% (up to 12\\%) of the\ntotal nodes in the benchmark circuits. Experimental results demonstrate that\nour methods yields an average performance improvement of 8.8\\% (up to 22\\%) on\nsimulating benchmark circuits which include a variety of functional modules\nsuch as ALUs, adders, multipliers, SEC/DED checkers, and interrupt controllers,\nwith only 0.7\\% relative error."}
{"id": "2508.13397", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.13397", "abs": "https://arxiv.org/abs/2508.13397", "authors": ["Michael Adams", "Amanda Bienz"], "title": "Optimizing Allreduce Operations for Heterogeneous Architectures with Multiple Processes per GPU", "comment": null, "summary": "Large inter-GPU all-reduce operations, prevalent throughout deep learning,\nare bottlenecked by communication costs. Emerging heterogeneous architectures\nare comprised of complex nodes, often containing $4$ GPUs and dozens to\nhundreds of CPU cores per node. Parallel applications are typically accelerated\non the available GPUs, using only a single CPU core per GPU while the remaining\ncores sit idle. This paper presents novel optimizations to large GPU-aware\nall-reduce operations, extending lane-aware reductions to the GPUs, and notably\nusing multiple CPU cores per GPU to accelerate these operations. These\nmulti-CPU-accelerated GPU-aware lane all-reduces yield speedup of up to $2.45$x\nfor large MPI all-reduces across the NVIDIA A100 GPUs of NCSA's Delta\nsupercomputer. Finally, the approach is extended to NVIDIA's and AMD's\ncollective communication libraries, achieving speedup of up to $1.77$x and\n$1.71$x, respectively, across $2$ state-of-the-art supercomputers."}
{"id": "2508.13160", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.13160", "abs": "https://arxiv.org/abs/2508.13160", "authors": ["Yibo Chen", "Eren Kurshan", "Dave Motschman", "Charles Johnson", "Yuan Xie"], "title": "Through Silicon Via Aware Design Planning for Thermally Efficient 3-D Integrated Circuits", "comment": null, "summary": "3-D integrated circuits (3-D ICs) offer performance advantages due to their\nincreased bandwidth and reduced wire-length enabled by through-silicon-via\nstructures (TSVs). Traditionally TSVs have been considered to improve the\nthermal conductivity in the vertical direction. However, the lateral thermal\nblockage effect becomes increasingly important for TSV via farms (a cluster of\nTSV vias used for signal bus connections between layers) because the TSV size\nand pitch continue to scale in {\\mu}m range and the metal to insulator ratio\nbecomes smaller. Consequently, dense TSV farms can create lateral thermal\nblockages in thinned silicon substrate and exacerbate the local hotspots. In\nthis paper, we propose a thermal-aware via farm placement technique for 3-D ICs\nto minimize lateral heat blockages caused by dense signal bus TSV structures."}
{"id": "2508.13522", "categories": ["cs.DC", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13522", "abs": "https://arxiv.org/abs/2508.13522", "authors": ["Zain Ahmad", "Musab Ahmad", "Bilal Ahmad"], "title": "DDoS Attacks in Cloud Computing: Detection and Prevention", "comment": null, "summary": "DDoS attacks are one of the most prevalent and harmful cybersecurity threats\nfaced by organizations and individuals today. In recent years, the complexity\nand frequency of DDoS attacks have increased significantly, making it\nchallenging to detect and mitigate them effectively. The study analyzes various\ntypes of DDoS attacks, including volumetric, protocol, and application layer\nattacks, and discusses the characteristics, impact, and potential targets of\neach type. It also examines the existing techniques used for DDoS attack\ndetection, such as packet filtering, intrusion detection systems, and machine\nlearning-based approaches, and their strengths and limitations. Moreover, the\nstudy explores the prevention techniques employed to mitigate DDoS attacks,\nsuch as firewalls, rate limiting , CPP and ELD mechanism. It evaluates the\neffectiveness of each approach and its suitability for different types of\nattacks and environments. In conclusion, this study provides a comprehensive\noverview of the different types of DDoS attacks, their detection, and\nprevention techniques. It aims to provide insights and guidelines for\norganizations and individuals to enhance their cybersecurity posture and\nprotect against DDoS attacks."}
{"id": "2508.13161", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13161", "abs": "https://arxiv.org/abs/2508.13161", "authors": ["Zhexuan Xu", "Kexin Zhou", "Jie Wang", "Zijie Geng", "Siyuan Xu", "Shixiong Kai", "Mingxuan Yuan", "Feng Wu"], "title": "Piano: A Multi-Constraint Pin Assignment-Aware Floorplanner", "comment": null, "summary": "Floorplanning is a critical step in VLSI physical design, increasingly\ncomplicated by modern constraints such as fixed-outline requirements,\nwhitespace removal, and the presence of pre-placed modules. In addition, the\nassignment of pins on module boundaries significantly impacts the performance\nof subsequent stages, including detailed placement and routing. However,\ntraditional floorplanners often overlook pin assignment with modern constraints\nduring the floorplanning stage. In this work, we introduce Piano, a\nfloorplanning framework that simultaneously optimizes module placement and pin\nassignment under multiple constraints. Specifically, we construct a graph based\non the geometric relationships among modules and their netlist connections,\nthen iteratively search for shortest paths to determine pin assignments. This\ngraph-based method also enables accurate evaluation of feedthrough and unplaced\npins, thereby guiding overall layout quality. To further improve the design, we\nadopt a whitespace removal strategy and employ three local optimizers to\nenhance layout metrics under multi-constraint scenarios. Experimental results\non widely used benchmark circuits demonstrate that Piano achieves an average\n6.81% reduction in HPWL, a 13.39% decrease in feedthrough wirelength, a 16.36%\nreduction in the number of feedthrough modules, and a 21.21% drop in unplaced\npins, while maintaining zero whitespace."}
{"id": "2508.13523", "categories": ["cs.DC", "cs.PF", "physics.comp-ph", "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2"], "pdf": "https://arxiv.org/pdf/2508.13523", "abs": "https://arxiv.org/abs/2508.13523", "authors": ["Anders Johansson", "Evan Weinberg", "Christian R. Trott", "Megan J. McCarthy", "Stan G. Moore"], "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale Architectures", "comment": "14 pages, 6 figures", "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials."}
{"id": "2508.13162", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13162", "abs": "https://arxiv.org/abs/2508.13162", "authors": ["Mahmoud Nazzal", "Khoa Nguyen", "Deepak Vungarala", "Ramtin Zand", "Shaahin Angizi", "Hai Phan", "Abdallah Khreishah"], "title": "FedChip: Federated LLM for Artificial Intelligence Accelerator Chip Design", "comment": null, "summary": "AI hardware design is advancing rapidly, driven by the promise of design\nautomation to make chip development faster, more efficient, and more accessible\nto a wide range of users. Amongst automation tools, Large Language Models\n(LLMs) offer a promising solution by automating and streamlining parts of the\ndesign process. However, their potential is hindered by data privacy concerns\nand the lack of domain-specific training. To address this, we introduce\nFedChip, a Federated fine-tuning approach that enables multiple Chip design\nparties to collaboratively enhance a shared LLM dedicated for automated\nhardware design generation while protecting proprietary data. FedChip enables\nparties to train the model on proprietary local data and improve the shared\nLLM's performance. To exemplify FedChip's deployment, we create and release\nAPTPU-Gen, a dataset of 30k design variations spanning various performance\nmetric values such as power, performance, and area (PPA). To encourage the LLM\nto generate designs that achieve a balance across multiple quality metrics, we\npropose a new design evaluation metric, Chip@k, which statistically evaluates\nthe quality of generated designs against predefined acceptance criteria.\nExperimental results show that FedChip improves design quality by more than 77%\nover high-end LLMs while maintaining data privacy"}
{"id": "2508.13636", "categories": ["cs.DC", "68P30, 94A08, 37M05"], "pdf": "https://arxiv.org/pdf/2508.13636", "abs": "https://arxiv.org/abs/2508.13636", "authors": ["Laurent Duval", "Frédéric Payan", "Christophe Preux", "Lauriane Bouard"], "title": "LUNDIsim: model meshes for flow simulation and scientific data compression benchmarks", "comment": "11 pages, for associated LUNDIsim dataset, see\n  https://doi.org/10.5281/zenodo.14641958", "summary": "The volume of scientific data produced for and by numerical simulation\nworkflows is increasing at an incredible rate. This raises concerns either in\ncomputability, interpretability, and sustainability. This is especially\nnoticeable in earth science (geology, meteorology, oceanography, and\nastronomy), notably with climate studies.\n  We highlight five main evaluation issues: efficiency, discrepancy, diversity,\ninterpretability, availability.\n  Among remedies, lossless and lossy compression techniques are becoming\npopular to better manage dataset volumes. Performance assessment -- with\ncomparative benchmarks -- require open datasets shared under FAIR principles\n(Findable, Accessible, Interoperable, Reusable), with MRE (Minimal Reproducible\nExample) ancillary data for reuse. We share LUNDIsim, an exemplary faulted\ngeological mesh. It is inspired by SPE10 comparative Challenge. Enhanced by\nporosity/permeability datasets, this dataset proposes four distinct subsurface\nenvironments. They were primarily designed for flow simulation in porous media.\nSeveral consistent resolutions (with HexaShrink multiscale representations) are\nproposed for each model. We also provide a set of reservoir features for\nreproducing typical two-phase flow simulations on all LUNDIsim models in a\nreservoir engineering context. This dataset is chiefly meant for benchmarking\nand evaluating data size reduction (upscaling) or genuine composite mesh\ncompression algorithms. It is also suitable for other advanced mesh processing\nworkflows in geology and reservoir engineering, from visualization to machine\nlearning.\n  LUNDIsim meshes are available at https://doi.org/10.5281/zenodo.14641958"}
{"id": "2508.13163", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13163", "abs": "https://arxiv.org/abs/2508.13163", "authors": ["Yashasvi Makin", "Rahul Maliakkal"], "title": "Sustainable AI Training via Hardware-Software Co-Design on NVIDIA, AMD, and Emerging GPU Architectures", "comment": "IEEE CISOSE Industry Track 2025 Conference", "summary": "In particular, large-scale deep learning and artificial intelligence model\ntraining uses a lot of computational power and energy, so it poses serious\nsustainability issues. The fast rise in model complexity has resulted in\nexponential increases in energy consumption, increasing the demand for\ntechniques maximizing computational efficiency and lowering environmental\nimpact. This work explores environmentally driven performance optimization\nmethods especially intended for advanced GPU architectures from NVIDIA, AMD,\nand other emerging GPU architectures. Our main focus is on investigating\nhardware-software co-design techniques meant to significantly increase\nmemory-level and kernel-level operations, so improving performance-per-watt\nmeasures. Our thorough research encompasses evaluations of specialized tensor\nand matrix cores, advanced memory optimization methods, and creative\nintegration approaches that taken together result in notable energy efficiency\nincreases. We also discuss important software-level optimizations that augment\nhardware capability including mixed-precision arithmetic, advanced energy-aware\nscheduling algorithms, and compiler-driven kernel enhancements. Moreover, we\nmethodically point out important research gaps and suggest future directions\nnecessary to create really sustainable artificial intelligence systems. This\npaper emphasizes how major increases in training efficiency can be obtained by\nco-design of hardware and software, so lowering the environmental impact of\nartificial intelligence without compromising performance. To back up our\nanalysis, we use real-world case studies from top companies like Meta, Google,\nAmazon, and others that show how these sustainable AI training methods are used\nin the real world."}
{"id": "2508.13693", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.13693", "abs": "https://arxiv.org/abs/2508.13693", "authors": ["Gabriella Saraiva", "Miguel Vasconcelos", "Sarita Mazzini Bruschi", "Danilo Carastan-Santos", "Daniel Cordeiro"], "title": "Estimating CO$_2$ emissions of distributed applications and platforms with SimGrid/Batsim", "comment": null, "summary": "This work presents a carbon footprint plugin designed to extend the\ncapabilities of the Batsim simulator by allowing the calculation of CO$_2$\nemissions during simulation runs. The goal is to comprehensively assess the\nenvironmental impact associated with task and resource management strategies in\ndata centers. The plugin is developed within SimGrid -- the underlying\nsimulation framework of Batsim -- and computes carbon emissions based on the\nsimulated platform's energy consumption and carbon intensity factor of the\nsimulated machines. Once implemented, it is integrated into Batsim, ensuring\ncompatibility with existing simulation workflows and enabling researchers to\nassess the carbon efficiency of their scheduling strategies."}
{"id": "2508.13172", "categories": ["cs.AR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.13172", "abs": "https://arxiv.org/abs/2508.13172", "authors": ["Jianqiu Chen", "Siqi Li", "Xu He"], "title": "White-Box Reasoning: Synergizing LLM Strategy and gm/Id Data for Automated Analog Circuit Design", "comment": "8 pages, 4 figures, 7 Tables", "summary": "Analog IC design is a bottleneck due to its reliance on experience and\ninefficient simulations, as traditional formulas fail in advanced nodes.\nApplying Large Language Models (LLMs) directly to this problem risks mere\n\"guessing\" without engineering principles. We present a \"synergistic reasoning\"\nframework that integrates an LLM's strategic reasoning with the physical\nprecision of the gm/Id methodology. By empowering the LLM with gm/Id lookup\ntables, it becomes a quantitative, data-driven design partner.\n  We validated this on a two-stage op-amp, where our framework enabled the\nGemini model to meet all TT corner specs in 5 iterations and extended\noptimization to all PVT corners. A crucial ablation study proved gm/Id data is\nkey for this efficiency and precision; without it, the LLM is slower and\ndeviates. Compared to a senior engineer's design, our framework achieves\nquasi-expert quality with an order-of-magnitude improvement in efficiency. This\nwork validates a path for true analog design automation by combining LLM\nreasoning with scientific circuit design methodologies."}
{"id": "2508.13716", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.13716", "abs": "https://arxiv.org/abs/2508.13716", "authors": ["Xianfeng Song", "Yi Zou", "Zheng Shi"], "title": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint Caching and Resource-Aware Graph Partitioning", "comment": null, "summary": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments."}
{"id": "2508.13181", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13181", "abs": "https://arxiv.org/abs/2508.13181", "authors": ["Dominik Loroch", "Johannes Feldmann", "Vladimir Rybalkin", "Norbert Wehn"], "title": "Low-power, Energy-efficient, Cardiologist-level Atrial Fibrillation Detection for Wearable Devices", "comment": "2025 IEEE 38th International System-on-Chip Conference (SOCC)", "summary": "Atrial fibrillation (AF) is a common arrhythmia and major risk factor for\ncardiovascular complications. While commercially available devices and\nsupporting Artificial Intelligence (AI) algorithms exist for reliable detection\nof AF, the scaling of this technology to the amount of people who need this\ndiagnosis is still a major challenge. This paper presents a novel wearable\ndevice, designed specifically for the early and reliable detection of AF. We\npresent an FPGA-based patch-style wearable monitor with embedded deep\nlearning-based AF detection. Operating with 3.8mW system power, which is 1-3\norders of magnitude lower than the state-of-the-art, the device enables\ncontinuous AF detection for over three weeks while achieving 95% accuracy,\nsurpassing cardiologist-level performance. A key innovation is the combination\nof energy-efficient hardware-software co-design and optimized power management\nthrough the application of hardware-aware neural architecture search. This\nadvancement represents a significant step toward scalable, reliable, and\nsustainable AF monitoring."}
{"id": "2508.13840", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.13840", "abs": "https://arxiv.org/abs/2508.13840", "authors": ["Nick Brown"], "title": "Is RISC-V ready for High Performance Computing? An evaluation of the Sophon SG2044", "comment": "Preprint of paper submitted to RISC-V for HPC SC25 workshop", "summary": "The pace of RISC-V adoption continues to grow rapidly, yet for the successes\nenjoyed in areas such as embedded computing, RISC-V is yet to gain ubiquity in\nHigh Performance Computing (HPC). The Sophon SG2044 is SOPHGO's next generation\n64-core high performance CPU that has been designed for workstation and server\ngrade workloads. Building upon the SG2042, subsystems that were a bottleneck in\nthe previous generation have been upgraded.\n  In this paper we undertake the first performance study of the SG2044 for HPC.\nComparing against the SG2042 and other architectures, we find that the SG2044\nis most advantageous when running at higher core counts, delivering up to 4.91\ngreater performance than the SG2042 over 64-cores. Two of the most important\nupgrades in the SG2044 are support for RVV v1.0 and an enhanced memory\nsubsystem. This results in the SG2044 significantly closing the performance gap\nwith other architectures, especially for compute-bound workloads."}
{"id": "2508.13231", "categories": ["cs.AR", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.13231", "abs": "https://arxiv.org/abs/2508.13231", "authors": ["Yunhua Fang", "Rui Xie", "Asad Ul Haq", "Linsen Ma", "Kaoutar El Maghraoui", "Naigang Wang", "Meng Wang", "Liu Liu", "Tong Zhang"], "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in Heterogeneous Memory System", "comment": null, "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."}
{"id": "2508.13163", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13163", "abs": "https://arxiv.org/abs/2508.13163", "authors": ["Yashasvi Makin", "Rahul Maliakkal"], "title": "Sustainable AI Training via Hardware-Software Co-Design on NVIDIA, AMD, and Emerging GPU Architectures", "comment": "IEEE CISOSE Industry Track 2025 Conference", "summary": "In particular, large-scale deep learning and artificial intelligence model\ntraining uses a lot of computational power and energy, so it poses serious\nsustainability issues. The fast rise in model complexity has resulted in\nexponential increases in energy consumption, increasing the demand for\ntechniques maximizing computational efficiency and lowering environmental\nimpact. This work explores environmentally driven performance optimization\nmethods especially intended for advanced GPU architectures from NVIDIA, AMD,\nand other emerging GPU architectures. Our main focus is on investigating\nhardware-software co-design techniques meant to significantly increase\nmemory-level and kernel-level operations, so improving performance-per-watt\nmeasures. Our thorough research encompasses evaluations of specialized tensor\nand matrix cores, advanced memory optimization methods, and creative\nintegration approaches that taken together result in notable energy efficiency\nincreases. We also discuss important software-level optimizations that augment\nhardware capability including mixed-precision arithmetic, advanced energy-aware\nscheduling algorithms, and compiler-driven kernel enhancements. Moreover, we\nmethodically point out important research gaps and suggest future directions\nnecessary to create really sustainable artificial intelligence systems. This\npaper emphasizes how major increases in training efficiency can be obtained by\nco-design of hardware and software, so lowering the environmental impact of\nartificial intelligence without compromising performance. To back up our\nanalysis, we use real-world case studies from top companies like Meta, Google,\nAmazon, and others that show how these sustainable AI training methods are used\nin the real world."}
{"id": "2508.13244", "categories": ["cs.AR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.13244", "abs": "https://arxiv.org/abs/2508.13244", "authors": ["Marco Giordano", "Pietro Bonazzi", "Luca Benini", "Michele Magno"], "title": "Sub-Millisecond Event-Based Eye Tracking on a Resource-Constrained Microcontroller", "comment": null, "summary": "This paper presents a novel event-based eye-tracking system deployed on a\nresource-constrained microcontroller, addressing the challenges of real-time,\nlow-latency, and low-power performance in embedded systems. The system\nleverages a Dynamic Vision Sensor (DVS), specifically the DVXplorer Micro, with\nan average temporal resolution of 200 {\\mu}s, to capture rapid eye movements\nwith extremely low latency. The system is implemented on a novel low-power and\nhigh-performance microcontroller from STMicroelectronics, the STM32N6. The\nmicrocontroller features an 800 MHz Arm Cortex-M55 core and AI hardware\naccelerator, the Neural-ART Accelerator, enabling real-time inference with\nmilliwatt power consumption. The paper propose a hardware-aware and\nsensor-aware compact Convolutional Neuron Network (CNN) optimized for\nevent-based data, deployed at the edge, achieving a mean pupil prediction error\nof 5.99 pixels and a median error of 5.73 pixels on the Ini-30 dataset. The\nsystem achieves an end-to-end inference latency of just 385 {\\mu}s and a neural\nnetwork throughput of 52 Multiply and Accumulate (MAC) operations per cycle\nwhile consuming just 155 {\\mu}J of energy. This approach allows for the\ndevelopment of a fully embedded, energy-efficient eye-tracking solution\nsuitable for applications such as smart glasses and wearable devices."}
{"id": "2508.13257", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13257", "abs": "https://arxiv.org/abs/2508.13257", "authors": ["Wenhao Lv", "Yingjie Xia", "Xiyuan Chen", "Li Kuang"], "title": "ViTAD: Timing Violation-Aware Debugging of RTL Code using Large Language Models", "comment": null, "summary": "In modern Very Large Scale Integrated (VLSI) circuit design flow, the\nRegister-Transfer Level (RTL) stage presents a critical opportunity for timing\noptimization. Addressing timing violations at this early stage is essential, as\nmodern systems demand higher speeds, where even minor timing violations can\nlead to functional failures or system crashes. However, traditional timing\noptimization heavily relies on manual expertise, requiring engineers to\niteratively analyze timing reports and debug. To automate this process, this\npaper proposes ViTAD, a method that efficiently analyzes the root causes of\ntiming violations and dynamically generates targeted repair strategies.\nSpecifically, we first parse Verilog code and timing reports to construct a\nSignal Timing Dependency Graph (STDG). Based on the STDG, we perform violation\npath analysis and use large language models (LLMs) to infer the root causes of\nviolations. Finally, by analyzing the causes of violations, we selectively\nretrieve relevant debugging knowledge from a domain-specific knowledge base to\ngenerate customized repair solutions. To evaluate the effectiveness of our\nmethod, we construct a timing violation dataset based on real-world open-source\nprojects. This dataset contains 54 cases of violations. Experimental results\nshow that our method achieves a 73.68% success rate in repairing timing\nviolations, while the baseline using only LLM is 54.38%. Our method improves\nthe success rate by 19.30%."}
{"id": "2508.13298", "categories": ["cs.DC", "cs.AR", "cs.ET", "cs.PF", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.13298", "abs": "https://arxiv.org/abs/2508.13298", "authors": ["Huynh Q. N. Vo", "Md Tawsif Rahman Chowdhury", "Paritosh Ramanan", "Murat Yildirim", "Gozde Tutuncuoglu"], "title": "Harnessing the Full Potential of RRAMs through Scalable and Distributed In-Memory Computing with Integrated Error Correction", "comment": "Submitted to Nature Communication Contact authors for any info", "summary": "Exponential growth in global computing demand is exacerbated due to the\nhigher-energy requirements of conventional architectures, primarily due to\nenergy-intensive data movement. In-memory computing with Resistive Random\nAccess Memory (RRAM) addresses this by co-integrating memory and processing,\nbut faces significant hurdles related to device-level non-idealities and poor\nscalability for large computing tasks. Here, we introduce \\textbf{MELISO+}\n(In-\\textbf{Me}mory \\textbf{Li}near \\textbf{So}lver), a full-stack, distributed\nframework for energy-efficient in-memory computing. MELISO+ proposes a novel\ntwo-tier error correction mechanism to mitigate device non-idealities and\ndevelops a distributed RRAM computing framework to enable matrix computations\nexceeding dimensions of $65,000 \\times 65,000$. This approach reduces first-\nand second-order arithmetic errors due to device non-idealities by over 90\\%,\nenhances energy efficiency by three to five orders of magnitude, and decreases\nlatency 100-fold. Hence, MELISO+ allows lower-precision RRAM devices to\noutperform high-precision device alternatives in accuracy, energy and latency\nmetrics. By unifying algorithm-hardware co-design with scalable architecture,\nMELISO+ significantly advances sustainable, high-dimensional computing suitable\nfor applications like large language models and generative AI."}
