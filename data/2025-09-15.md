<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Setchain Algorithms for Blockchain Scalability](https://arxiv.org/abs/2509.09795)
*Arivarasan Karmegam,Gabina Luz Bianchi,Margarita Capretto,Martín Ceresa,Antonio Fernández Anta,César Sánchez*

Main category: cs.DC

TL;DR: Setchain通过放松交易严格全序要求来提高区块链可扩展性，提出了三种基于底层区块链的算法：Vanilla、Compresschain和Hashchain，实现了比底层区块链高几个数量级的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决区块链可扩展性问题，通过放松交易严格全序要求来提高吞吐量。

Method: 提出三种Setchain算法：Vanilla（基础实现）、Compresschain（批量压缩）和Hashchain（哈希批处理），基于CometBFT平台实现，使用epoch-proof机制确保安全性。

Result: 在4、7、10个服务器的集群测试中，Setchain算法达到比底层区块链高几个数量级的吞吐量，最终性延迟低于4秒。

Conclusion: Setchain算法通过放松排序要求有效提高了区块链的可扩展性，三种算法在不同场景下都有良好表现，Hashchain特别适合轻客户端应用。

Abstract: Setchain has been proposed to increase blockchain scalability by relaxing the
strict total order requirement among transactions. Setchain organizes elements
into a sequence of sets, referred to as epochs, so that elements within each
epoch are unordered. In this paper, we propose and evaluate three distinct
Setchain algorithms, that leverage an underlying block-based ledger. Vanilla is
a basic implementation that serves as a reference point. Compresschain
aggregates elements into batches, and compresses these batches before appending
them as epochs in the ledger. Hashchain converts batches into fixed-length
hashes which are appended as epochs in the ledger. This requires Hashchain to
use a distributed service to obtain the batch contents from its hash. To allow
light clients to safely interact with only one server, the proposed algorithms
maintain, as part of the Setchain, proofs for the epochs. An epoch-proof is the
hash of the epoch, cryptographically signed by a server. A client can verify
the correctness of an epoch with $f+1$ epoch-proofs (where $f$ is the maximum
number of Byzantine servers assumed). All three Setchain algorithms are
implemented on top of the CometBFT blockchain application platform. We
conducted performance evaluations across various configurations, using clusters
of four, seven, and ten servers. Our results show that the Setchain algorithms
reach orders of magnitude higher throughput than the underlying blockchain, and
achieve finality with latency below 4 seconds.

</details>


### [2] [Ordered Consensus with Equal Opportunity](https://arxiv.org/abs/2509.09868)
*Yunhao Zhang,Haobin Ni,Soumya Basu,Shir Cohen,Maofan Yin,Lorenzo Alvisi,Robbert van Renesse,Qi Chen,Lidong Zhou*

Main category: cs.DC

TL;DR: 这篇论文提出了一种新的排序共识协议Bercow，通过密密随机神谈(SRO)机制实现平等机会的公平性要求，有效防范SMR基础的区块链中的排序攻击。


<details>
  <summary>Details</summary>
Motivation: 传统状态机复制(SMR)协议对命令最终排序没有要求，但在区块链中排序致关金融奖励。虽然有排序共识来限制希腊节点影响，但实际中即使没有希腊节点，通过更快网络或更近距离等因素，某些客户仍可获得不公平优势。

Method: 提出扩展排序共识要求支持平等机会的公平性概念，并介绍了密密随机神谈(SRO)系统组件，能够以故障容错方式生成随机数。提出了基于信任硬件和阈值可验证随机函数的两种SRO设计，并在Bercow协议中实现。

Result: 开发了Bercow协议，能够在可配置因子范围内近似实现平等机会，有效减缓SMR基础区块链中的知名排序攻击。

Conclusion: 通过引入平等机会公平性概念和随机性机制，这项研究为SMR基础区块链提供了更公平的排序解决方案，能够在保持共识性能的同时有效应对实际中的排序欺诈问题。

Abstract: The specification of state machine replication (SMR) has no requirement on
the final total order of commands. In blockchains based on SMR, however, order
matters, since different orders could provide their clients with different
financial rewards. Ordered consensus augments the specification of SMR to
include specific guarantees on such order, with a focus on limiting the
influence of Byzantine nodes. Real-world ordering manipulations, however, can
and do happen even without Byzantine replicas, typically because of factors,
such as faster networks or closer proximity to the blockchain infrastructure,
that give some clients an unfair advantage. To address this challenge, this
paper proceeds to extend ordered consensus by requiring it to also support
equal opportunity, a concrete notion of fairness, widely adopted in social
sciences. Informally, equal opportunity requires that two candidates who,
according to a set of criteria deemed to be relevant, are equally qualified for
a position (in our case, a specific slot in the SMR total order), should have
an equal chance of landing it. We show how randomness can be leveraged to keep
bias in check, and, to this end, introduce the secret random oracle (SRO), a
system component that generates randomness in a fault-tolerant manner. We
describe two SRO designs based, respectively, on trusted hardware and threshold
verifiable random functions, and instantiate them in Bercow, a new ordered
consensus protocol that, by approximating equal opportunity up to within a
configurable factor, can effectively mitigate well-known ordering attacks in
SMR-based blockchains.

</details>


### [3] [Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective](https://arxiv.org/abs/2509.10371)
*Seokjin Go,Joongun Park,Spandan More,Hanjiang Wu,Irene Wang,Aaron Jezghani,Tushar Krishna,Divya Mahajan*

Main category: cs.DC

TL;DR: 大语言模型训练在多GPU系统中的性能特征化分析，研究了不同硬件平台和并行策略对硬件利用率、功耗和热行为的影响


<details>
  <summary>Details</summary>
Motivation: 随着LLM训练负荷越来越超出单节点能力，需要深入理解这些模型在大规模多GPU系统中的行为特征

Method: 在NVIDIA H100/H200和AMD MI250 GPU平台上，对密集和稀疏模型进行综合性能分析，测试了张量并行、水线并行、数据并行和专家并行策略，评估激活重计算和计算-通信重叠等优化技术

Result: 发现性能不仅仅取决于硬件规模，调试好的少数高内存GPU系统在通信缚口情况下更优，某些并行组合导致带宽利用率低，过大微批量会导致功耗峰值和热限制问题

Conclusion: 训练性能由硬件、系统拓扑结构和模型执行之间的复杂交互决定，建议从系统和硬件设计方面进行优化以提升未来LLM系统的可扩展性和可靠性

Abstract: The rapid scaling of Large Language Models (LLMs) has pushed training
workloads far beyond the limits of single-node analysis, demanding a deeper
understanding of how these models behave across large-scale, multi-GPU systems.
In this paper, we present a comprehensive characterization of LLM training
across diverse real-world workloads and hardware platforms, including NVIDIA
H100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various
parallelism strategies -- tensor, pipeline, data, and expert -- and evaluate
their effects on hardware utilization, power consumption, and thermal behavior.
We further evaluate the effectiveness of optimizations such as activation
recomputation and compute-communication overlap. Our findings show that
performance is not determined solely by scaling hardware capacity. Scale-up
systems with fewer, higher-memory GPUs can outperform scale-out systems in
communication-bound regimes, but only under carefully tuned configurations; in
other cases, scale-out deployments achieve superior throughput. We also show
that certain parallelism combinations, such as tensor with pipeline, lead to
bandwidth underutilization due to inefficient data chunking, while increasing
microbatch sizes beyond a certain point induces bursty execution and peak power
excursions that worsen thermal throttling. These insights reveal how training
performance is shaped by complex interactions between hardware, system
topology, and model execution. We conclude by offering recommendations for
system and hardware design to improve the scalability and reliability of future
LLM systems and workloads. The source code of this project is available at
https://github.com/sitar-lab/CharLLM-PPT.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [4] [Towards An Approach to Identify Divergences in Hardware Designs for HPC Workloads](https://arxiv.org/abs/2509.09774)
*Doru Thom Popovici,Mario Vega,Angelos Ioannou,Fabien Chaix,Dania Mosuli,Blair Reasoner,Tan Nguyen,Xiaokun Yang,John Shalf*

Main category: cs.AR

TL;DR: 提出了一种分层分解数学内核为基本原语的方法，在不同编程环境中实现这些原语并组装算法，通过自动分析频率和资源使用来比较不同设计方法的效率


<details>
  <summary>Details</summary>
Motivation: 传统硬件加速器开发需要低层Verilog编程和大量手动优化，现有高级设计工具生成的硬件可能不如专家设计优化，需要理解效率差距的来源

Method: 将数学内核（傅里叶变换、矩阵乘法、QR分解等）分层分解为通用构建块/原语，在不同编程环境中实现这些原语并组装算法，采用自动化方法分析可达频率和所需资源

Result: 通过在各个层级进行实验，提供了设计间更公平的比较，并为工具开发者和硬件设计者提供了改进实践的指导

Conclusion: 该方法有助于识别不同设计方法中的效率差距，为硬件设计工具和方法的改进提供有价值的见解

Abstract: Developing efficient hardware accelerators for mathematical kernels used in
scientific applications and machine learning has traditionally been a
labor-intensive task. These accelerators typically require low-level
programming in Verilog or other hardware description languages, along with
significant manual optimization effort. Recently, to alleviate this challenge,
high-level hardware design tools like Chisel and High-Level Synthesis have
emerged. However, as with any compiler, some of the generated hardware may be
suboptimal compared to expert-crafted designs. Understanding where these
inefficiencies arise is crucial, as it provides valuable insights for both
users and tool developers. In this paper, we propose a methodology to
hierarchically decompose mathematical kernels - such as Fourier transforms,
matrix multiplication, and QR factorization - into a set of common building
blocks or primitives. Then the primitives are implemented in the different
programming environments, and the larger algorithms get assembled. Furthermore,
we employ an automatic approach to investigate the achievable frequency and
required resources. Performing this experimentation at each level will provide
fairer comparisons between designs and offer guidance for both tool developers
and hardware designers to adopt better practices.

</details>


### [5] [Finesse: An Agile Design Framework for Pairing-based Cryptography via Software/Hardware Co-Design](https://arxiv.org/abs/2509.10051)
*Tianwei Pan,Tianao Dai,Jianlei Yang,Hongbin Jing,Yang Su,Zeyu Hao,Xiaotao Jia,Chunming Hu,Weisheng Zhao*

Main category: cs.AR

TL;DR: Finesse是一个基于协同设计方法的敏捷设计框架，用于配对密码学硬件加速器，通过专用编译器和多粒度硬件模拟器实现性能优化和设计空间探索，显著缩短设计周期并提高灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统配对密码学加速器设计方法面临设计周期长、性能与灵活性难以平衡、架构探索支持不足等挑战，需要新的敏捷设计框架来解决这些问题。

Method: 采用协同设计方法，结合专用编译器和多粒度硬件模拟器进行协同优化循环，采用模块化设计流程和通用抽象来支持不同曲线族和硬件架构。

Result: 编译时间缩短至分钟级，在流行曲线上实现34倍吞吐量提升和6.2倍面积效率提升，相比非灵活ASIC设计也有3倍吞吐量和3.2倍面积效率优势。

Conclusion: Finesse框架成功解决了配对密码学加速器设计中的关键挑战，提供了灵活性、高效性和快速原型设计能力，显著优于现有框架和ASIC设计。

Abstract: Pairing-based cryptography (PBC) is crucial in modern cryptographic
applications. With the rapid advancement of adversarial research and the
growing diversity of application requirements, PBC accelerators need regular
updates in algorithms, parameter configurations, and hardware design. However,
traditional design methodologies face significant challenges, including
prolonged design cycles, difficulties in balancing performance and flexibility,
and insufficient support for potential architectural exploration.
  To address these challenges, we introduce Finesse, an agile design framework
based on co-design methodology. Finesse leverages a co-optimization cycle
driven by a specialized compiler and a multi-granularity hardware simulator,
enabling both optimized performance metrics and effective design space
exploration. Furthermore, Finesse adopts a modular design flow to significantly
shorten design cycles, while its versatile abstraction ensures flexibility
across various curve families and hardware architectures.
  Finesse offers flexibility, efficiency, and rapid prototyping, comparing with
previous frameworks. With compilation times reduced to minutes, Finesse enables
faster iteration cycles and streamlined hardware-software co-design.
Experiments on popular curves demonstrate its effectiveness, achieving
$34\times$ improvement in throughput and $6.2\times$ increase in area
efficiency compared to previous flexible frameworks, while outperforming
state-of-the-art non-flexible ASIC designs with a $3\times$ gain in throughput
and $3.2\times$ improvement in area efficiency.

</details>


### [6] [MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging Bit-Slice-enabled Sparsity and Repetitiveness](https://arxiv.org/abs/2509.10372)
*Huizheng Wang,Zichuan Wang,Zhiheng Yue,Yousheng Long,Taiquan Wei,Jianxun Yang,Yang Wang,Chao Li,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: MCBP是一种基于比特粒度的算法-硬件协同设计，通过利用比特切片的重複性和稀疏性来加速大语言模型推理，在计算和内存效率方面实现显著提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理时面临GEMM操作、权重访问和KV缓存访问的低效问题，现有Transformer加速器难以同时优化计算和内存效率，需要一种更精细粒度的协同优化方案。

Method: 提出三种关键技术：1) BS重複性启用的计算减少(BRCR) - 通过比特切片向量间的冗余消除GEMM计算；2) BS稀疏性启用的两态编码(BSTC) - 利用高位比特切片权重中的稀疏性减少权重访问；3) 比特粒度渐进预测(BGPP) - 基于提前终止的比特粒度预测减少KV缓存访问。

Result: 在26个基准测试中，MCBP相比Nvidia A100 GPU实现9.43倍加速和31.1倍能效提升；相比SOTA Transformer加速器Spatten、FACT和SOFA，分别实现35倍、5.2倍和3.2倍的节能效果。

Conclusion: MCBP通过比特粒度的计算-内存高效协同设计，有效解决了LLM推理中的计算和内存瓶颈问题，为实时场景下的大语言模型推理提供了高效的加速解决方案。

Abstract: Large language models (LLMs) face significant inference latency due to
inefficiencies in GEMM operations, weight access, and KV cache access,
especially in real-time scenarios. This highlights the need for a versatile
compute-memory efficient accelerator. Unfortunately, existing Transformer
accelerators struggle to address both aspects simultaneously, as they focus on
value-level processing, missing fine-grained opportunities to optimize
computation and memory collaboratively. This paper introduces MCBP, a
bit-grained compute-memory efficient algorithm-hardware co-design that
leverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM
inference. MCBP features three key innovations: 1) BS-repetitiveness-enabled
computation reduction (BRCR), which eliminates redundant GEMM computations via
leveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state
coding (BSTC), which reduces weight access via exploiting significant sparsity
in high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),
which reduces KV cache access by leveraging early-termination-based bit-grained
prediction. These techniques, supported by custom accelerator designs,
effectively alleviate the burden in GEMM, weight access, and KV cache access.
Extensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up
and 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA
Transformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than
Spatten, FACT and SOFA, respectively.

</details>


### [7] [TurboFuzz: FPGA Accelerated Hardware Fuzzing for Processor Agile Verification](https://arxiv.org/abs/2509.10400)
*Yang Zhong,Haoran Wu,Xueqi Li,Sa Wang,David Boland,Yungang Bao,Kan Shi*

Main category: cs.AR

TL;DR: TurboFuzz是一个端到端的硬件加速验证框架，在单个FPGA上实现完整的测试生成-仿真-覆盖率反馈循环，相比软件模糊测试器在相同时间内实现2.23倍覆盖率提升，检测真实问题时达到571倍性能加速。


<details>
  <summary>Details</summary>
Motivation: 现代处理器设计复杂性增加和RISC-V等新ISA的出现，需要更敏捷高效的验证方法。传统仿真方法性能差、测试用例质量不足，硬件加速方案存在通信开销大、测试模式生成效率低等问题。

Method: 在单个FPGA上实现完整的测试生成-仿真-覆盖率反馈循环，通过优化的测试用例控制流、高效的种子间调度和混合模糊器集成来提高测试质量和执行效率，采用反馈驱动的生成机制加速覆盖率收敛。

Result: 在相同时间预算内比软件模糊测试器多收集2.23倍覆盖率，检测真实问题时达到571倍性能加速，同时保持完全的可视化和调试能力，面积开销适中。

Conclusion: TurboFuzz提供了一个高效的硬件加速验证解决方案，显著提升了处理器验证的覆盖率和性能，同时保持了良好的调试能力。

Abstract: Verification is a critical process for ensuring the correctness of modern
processors. The increasing complexity of processor designs and the emergence of
new instruction set architectures (ISAs) like RISC-V have created demands for
more agile and efficient verification methodologies, particularly regarding
verification efficiency and faster coverage convergence. While simulation-based
approaches now attempt to incorporate advanced software testing techniques such
as fuzzing to improve coverage, they face significant limitations when applied
to processor verification, notably poor performance and inadequate test case
quality. Hardware-accelerated solutions using FPGA or ASIC platforms have tried
to address these issues, yet they struggle with challenges including host-FPGA
communication overhead, inefficient test pattern generation, and suboptimal
implementation of the entire multi-step verification process.
  In this paper, we present TurboFuzz, an end-to-end hardware-accelerated
verification framework that implements the entire Test
Generation-Simulation-Coverage Feedback loop on a single FPGA for modern
processor verification. TurboFuzz enhances test quality through optimized test
case (seed) control flow, efficient inter-seed scheduling, and hybrid fuzzer
integration, thereby improving coverage and execution efficiency. Additionally,
it employs a feedback-driven generation mechanism to accelerate coverage
convergence. Experimental results show that TurboFuzz achieves up to 2.23x more
coverage collection than software-based fuzzers within the same time budget,
and up to 571x performance speedup when detecting real-world issues, while
maintaining full visibility and debugging capabilities with moderate area
overhead.

</details>
