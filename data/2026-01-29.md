<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 19]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A Data-Informed Local Subspaces Method for Error-Bounded Lossy Compression of Large-Scale Scientific Datasets](https://arxiv.org/abs/2601.20113)
*Arshan Khan,Rohit Deshmukh,Ben O'Neill*

Main category: cs.DC

TL;DR: Discontinuous DLS是一种基于数据驱动的科学数据压缩器，利用局部时空子空间提升压缩效率，在保证误差边界的同时显著减少存储需求。


<details>
  <summary>Details</summary>
Motivation: 科学模拟数据量快速增长给存储和传输带来巨大挑战，需要误差有界的有损压缩技术来减少数据大小，同时确保重建数据对科学分析有效。

Method: 提出Discontinuous DLS压缩器，利用数据驱动的局部时空子空间技术，根据底层数据结构信息增强压缩效率并保留关键特征。该方法在分布式计算环境中使用MPI实现。

Result: 与最先进的误差有界压缩方法相比，Discontinuous DLS在压缩比和重建精度方面表现优异，显著减少了存储需求而不损害关键数据保真度。

Conclusion: Discontinuous DLS是高性能计算环境中大规模科学数据压缩的有前景方法，为管理现代科学模拟日益增长的数据需求提供了稳健解决方案。

Abstract: The growing volume of scientific simulation data presents a significant challenge for storage and transfer. Error-bounded lossy compression has emerged as a critical solution for mitigating these challenges, providing a means to reduce data size while ensuring that reconstructed data remains valid for scientific analysis. In this paper, we present a data-driven scientific data compressor, called Discontinuous Data-informed Local Subspaces (Discontinuous DLS), to improve compression-to-error ratios over data-agnostic compressors. This error-bounded compressor leverages localized spatial and temporal subspaces, informed by the underlying data structure, to enhance compression efficiency and preserve key features. The presented technique is flexible and applicable to a wide range of scientific data, including fluid dynamics, environmental simulations, and other high-dimensional, time-dependent datasets. We describe the core principles of the method and demonstrate its ability to significantly reduce storage requirements without compromising critical data fidelity. The technique is implemented in a distributed computing environment using MPI, and its performance is evaluated against state-of-the-art error-bounded compression methods in terms of compression ratio and reconstruction accuracy. This study highlights discontinuous DLS as a promising approach for large-scale scientific data compression in high-performance computing environments, providing a robust solution for managing the growing data demands of modern scientific simulations.

</details>


### [2] [StreamFusion: Scalable Sequence Parallelism for Distributed Inference of Diffusion Transformers on GPUs](https://arxiv.org/abs/2601.20273)
*Jiacheng Yang,Jun Wu,Yaoyao Ding,Zhiying Xu,Yida Wang,Gennady Pekhimenko*

Main category: cs.DC

TL;DR: StreamFusion是一个针对扩散变换器(DiT)的高效服务引擎，通过拓扑感知序列并行、Torus Attention和单边通信技术，解决了现有并行方法在通信模式和延迟方面的瓶颈，实现了最高1.77倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着高分辨率图像和长视频生成需求的增长，单GPU推理因延迟增加和激活尺寸变大而效率低下。现有序列并行技术存在三个主要问题：1) 对现代GPU机器网络拓扑的通信模式不优；2) 机器间all-to-all操作造成延迟瓶颈；3) 使用双边通信库带来的GPU发送-接收同步和计算开销。

Method: StreamFusion包含三个关键技术：1) 考虑机器间和机器内带宽差异的拓扑感知序列并行技术；2) Torus Attention - 一种新颖的序列并行技术，允许机器间all-to-all操作与计算重叠；3) 最小化GPU发送-接收同步和计算开销的单边通信实现。

Result: 实验表明，StreamFusion平均性能优于最先进方法1.35倍，最高可达1.77倍。

Conclusion: StreamFusion通过创新的拓扑感知并行技术、通信-计算重叠机制和单边通信优化，有效解决了DiT大规模推理中的通信瓶颈问题，显著提升了高分辨率图像和长视频生成的推理效率。

Abstract: Diffusion Transformers (DiTs) have gained increasing adoption in high-quality image and video generation. As demand for higher-resolution images and longer videos increases, single-GPU inference becomes inefficient due to increased latency and large activation sizes. Current frameworks employ sequence parallelism (SP) techniques such as Ulysses Attention and Ring Attention to scale inference. However, these implementations have three primary limitations: (1) suboptimal communication patterns for network topologies on modern GPU machines, (2) latency bottlenecks from all-to-all operations in inter-machine communication, and (3) GPU sender-receiver synchronization and computation overheads from using two-sided communication libraries. To address these issues, we present StreamFusion, a topology-aware efficient DiT serving engine. StreamFusion incorporates three key innovations: (1) a topology-aware sequence parallelism technique that accounts for inter- and intra-machine bandwidth differences, (2) Torus Attention, a novel SP technique enabling overlapping of inter-machine all-to-all operations with computation, and (3) a one-sided communication implementation that minimizes GPU sender-receiver synchronization and computation overheads. Our experiments demonstrate that StreamFusion outperforms the state-of-the-art approach by an average of $1.35\times$ (up to $1.77\times$).

</details>


### [3] [SuperInfer: SLO-Aware Rotary Scheduling and Memory Management for LLM Inference on Superchips](https://arxiv.org/abs/2601.20309)
*Jiahuan Yu,Mingtao Hu,Zichao Lin,Minjia Zhang*

Main category: cs.DC

TL;DR: SuperInfer：基于Superchips的高性能LLM推理系统，通过RotaSched旋转调度器和DuplexKV优化引擎，在GPU内存受限时显著提升TTFT SLO达成率


<details>
  <summary>Details</summary>
Motivation: LLM推理面临严格的延迟SLO与有限的GPU内存容量之间的根本矛盾。当高请求率耗尽KV缓存预算时，现有系统会出现严重的队头阻塞问题。虽然已有研究探索PCIe卸载方案，但这些方法在高请求率下无法维持响应性，难以满足严格的TTFT和TBT SLO要求。

Method: SuperInfer针对新兴Superchips（如NVIDIA GH200）设计，利用NVLink-C2C紧密耦合的GPU-CPU架构。系统包含：1) RotaSched - 首个主动的、SLO感知的旋转调度器，通过旋转请求在Superchips上维持响应性；2) DuplexKV - 优化的旋转引擎，通过NVLink-C2C实现全双工传输。

Result: 在GH200上使用多种模型和数据集的评估显示，SuperInfer将TTFT SLO达成率提升高达74.7%，同时保持与最先进系统相当的TBT和吞吐量。

Conclusion: SLO感知调度与内存协同设计能够充分发挥Superchips在响应性LLM服务中的潜力，SuperInfer展示了通过新型硬件架构和系统设计解决LLM推理内存瓶颈的有效途径。

Abstract: Large Language Model (LLM) serving faces a fundamental tension between stringent latency Service Level Objectives (SLOs) and limited GPU memory capacity. When high request rates exhaust the KV cache budget, existing LLM inference systems often suffer severe head-of-line (HOL) blocking. While prior work explored PCIe-based offloading, these approaches cannot sustain responsiveness under high request rates, often failing to meet tight Time-To-First-Token (TTFT) and Time-Between-Tokens (TBT) SLOs. We present SuperInfer, a high-performance LLM inference system designed for emerging Superchips (e.g., NVIDIA GH200) with tightly coupled GPU-CPU architecture via NVLink-C2C. SuperInfer introduces RotaSched, the first proactive, SLO-aware rotary scheduler that rotates requests to maintain responsiveness on Superchips, and DuplexKV, an optimized rotation engine that enables full-duplex transfer over NVLink-C2C. Evaluations on GH200 using various models and datasets show that SuperInfer improves TTFT SLO attainment rates by up to 74.7% while maintaining comparable TBT and throughput compared to state-of-the-art systems, demonstrating that SLO-aware scheduling and memory co-design unlocks the full potential of Superchips for responsive LLM serving.

</details>


### [4] [Graph-Structured Deep Learning Framework for Multi-task Contention Identification with High-dimensional Metrics](https://arxiv.org/abs/2601.20389)
*Xiao Yang,Yinan Ni,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: 提出一个统一的多任务争用分类框架，通过表示变换、图结构建模和任务解耦机制，在高维系统环境中准确识别多种争用类型。


<details>
  <summary>Details</summary>
Motivation: 解决在高维系统环境中准确识别多任务争用类型的挑战，为复杂计算环境的性能管理提供可靠技术方案。

Method: 1) 从高维指标序列构建系统状态表示，应用非线性变换提取跨维度动态特征；2) 引入基于图的建模机制捕捉指标间潜在依赖关系；3) 设计任务特定映射结构建模争用类型差异；4) 采用自适应多任务损失权重策略平衡共享与特定特征学习。

Result: 在公开系统跟踪数据集上的实验显示，在准确率、召回率、精确率和F1分数方面具有优势；对批次大小、训练样本规模和指标维度的敏感性分析进一步证实了模型的稳定性和适用性。

Conclusion: 基于高维指标的结构化表示和多任务分类能显著改善争用模式识别，为复杂计算环境的性能管理提供了可靠技术途径。

Abstract: This study addresses the challenge of accurately identifying multi-task contention types in high-dimensional system environments and proposes a unified contention classification framework that integrates representation transformation, structural modeling, and a task decoupling mechanism. The method first constructs system state representations from high-dimensional metric sequences, applies nonlinear transformations to extract cross-dimensional dynamic features, and integrates multiple source information such as resource utilization, scheduling behavior, and task load variations within a shared representation space. It then introduces a graph-based modeling mechanism to capture latent dependencies among metrics, allowing the model to learn competitive propagation patterns and structural interference across resource links. On this basis, task-specific mapping structures are designed to model the differences among contention types and enhance the classifier's ability to distinguish multiple contention patterns. To achieve stable performance, the method employs an adaptive multi-task loss weighting strategy that balances shared feature learning with task-specific feature extraction and generates final contention predictions through a standardized inference process. Experiments conducted on a public system trace dataset demonstrate advantages in accuracy, recall, precision, and F1, and sensitivity analyses on batch size, training sample scale, and metric dimensionality further confirm the model's stability and applicability. The study shows that structured representations and multi-task classification based on high-dimensional metrics can significantly improve contention pattern recognition and offer a reliable technical approach for performance management in complex computing environments.

</details>


### [5] [Meeting SLOs, Slashing Hours: Automated Enterprise LLM Optimization with OptiKIT](https://arxiv.org/abs/2601.20408)
*Nicholas Santavas,Kareem Eissa,Patrycja Cieplicka,Piotr Florek,Matteo Nulli,Stefan Vasilev,Seyyed Hadi Hashemi,Antonios Gasteratos,Shahram Khadivi*

Main category: cs.DC

TL;DR: OptiKIT是一个分布式LLM优化框架，通过自动化复杂优化工作流程，使非专家团队能够进行模型压缩和调优，在GPU受限预算下实现2倍以上的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 企业LLM部署面临关键的可扩展性挑战：组织需要在有限的计算预算内系统化优化模型，但手动优化所需的专业知识稀缺且难以获取。特别是在异构基础设施中管理GPU利用率，同时让具有不同工作负载和有限LLM优化经验的团队高效部署模型。

Method: OptiKIT框架提供动态资源分配、分阶段管道执行（带自动清理）和无缝企业集成。通过资源分配算法、管道编排和集成模式，自动化复杂优化工作流程，使非专家团队能够进行模型压缩和调优。

Result: 在生产环境中，OptiKIT实现了超过2倍的GPU吞吐量提升，使应用团队能够在没有深厚LLM优化专业知识的情况下获得一致的性能改进。

Conclusion: OptiKIT通过自动化优化工作流程，民主化了模型优化，使非专家团队能够大规模部署优化后的LLM。系统已开源以促进外部贡献和更广泛的可重复性。

Abstract: Enterprise LLM deployment faces a critical scalability challenge: organizations must optimize models systematically to scale AI initiatives within constrained compute budgets, yet the specialized expertise required for manual optimization remains a niche and scarce skillset. This challenge is particularly evident in managing GPU utilization across heterogeneous infrastructure while enabling teams with diverse workloads and limited LLM optimization experience to deploy models efficiently.
  We present OptiKIT, a distributed LLM optimization framework that democratizes model compression and tuning by automating complex optimization workflows for non-expert teams. OptiKIT provides dynamic resource allocation, staged pipeline execution with automatic cleanup, and seamless enterprise integration.
  In production, it delivers more than 2x GPU throughput improvement while empowering application teams to achieve consistent performance improvements without deep LLM optimization expertise. We share both the platform design and key engineering insights into resource allocation algorithms, pipeline orchestration, and integration patterns that enable large-scale, production-grade democratization of model optimization. Finally, we open-source the system to enable external contributions and broader reproducibility.

</details>


### [6] [Rethinking Thread Scheduling under Oversubscription: A User-Space Framework for Coordinating Multi-runtime and Multi-process Workloads](https://arxiv.org/abs/2601.20435)
*Aleix Roca,Vicenç Beltran*

Main category: cs.DC

TL;DR: 提出用户空间调度框架USF和SCHED_COOP调度策略，通过用户空间协作调度减少OS调度器在过载场景下的干扰，提升并行应用性能


<details>
  <summary>Details</summary>
Motivation: HPC与AI融合导致复杂并行应用增多，多运行时共存给传统OS调度器带来压力。过载时OS调度器的周期性抢占会引入干扰，降低性能

Method: 开发用户空间调度框架USF，完全在用户空间实现，允许用户自定义调度算法。实现SCHED_COOP协作调度策略，仅在线程阻塞时切换，避免不必要的抢占

Result: 在过载多进程场景下性能提升达2.4倍，包括嵌套BLAS工作负载、多进程PyTorch推理（LLaMA-3）和分子动力学模拟

Conclusion: USF框架和SCHED_COOP策略能有效减少调度干扰，解决锁持有者抢占、锁等待者抢占和可扩展性崩溃等问题，提升并行应用性能

Abstract: The convergence of high-performance computing (HPC) and artificial intelligence (AI) is driving the emergence of increasingly complex parallel applications and workloads. These workloads often combine multiple parallel runtimes within the same application or across co-located jobs, creating scheduling demands that place significant stress on traditional OS schedulers. When oversubscribed (there are more ready threads than cores), OS schedulers rely on periodic preemptions to multiplex cores, often introducing interference that may degrade performance. In this paper, we present: (1) The User-space Scheduling Framework (USF), a novel seamless process scheduling framework completely implemented in user-space. USF enables users to implement their own process scheduling algorithms without requiring special permissions. We evaluate USF with its default cooperative policy, (2) SCHED_COOP, designed to reduce interference by switching threads only upon blocking. This approach mitigates well-known issues such as Lock-Holder Preemption (LHP), Lock-Waiter Preemption (LWP), and scalability collapse. We implement USF and SCHED_COOP by extending the GNU C library with the nOS-V runtime, enabling seamless coordination across multiple runtimes (e.g., OpenMP) without requiring invasive application changes. Evaluations show gains up to 2.4x in oversubscribed multi-process scenarios, including nested BLAS workloads, multi-process PyTorch inference with LLaMA-3, and Molecular Dynamics (MD) simulations.

</details>


### [7] [AutoOverlap: Enabling Fine-Grained Overlap of Computation and Communication with Chunk-Based Scheduling](https://arxiv.org/abs/2601.20595)
*Xinwei Qiang,Yue Guan,Zhengding Hu,Yufei Ding,Adnan Aziz*

Main category: cs.DC

TL;DR: AutoOverlap：一个编译器与运行时系统，通过细粒度通信块重叠技术，在单个融合内核内自动优化GPU通信，相比现有粗粒度流级重叠方法，平均获得1.3倍端到端加速，最高可达4.7倍。


<details>
  <summary>Details</summary>
Motivation: 大规模GPU工作负载中通信已成为首要瓶颈。现有分布式编译器主要通过流级重叠整个计算和通信内核，这种粗粒度方法存在额外内核启动开销、设备级同步问题，以及通信尾部因最慢瓦片/内核导致的空闲时间。

Method: 提出通信块抽象，解耦通信粒度与内核结构及后端机制；支持从现有分布式编译器移植块级计划、用户直接编写或可重用模板实例化。基于Triton实现源到源编译器，通过变换使计算与块可用性对齐。

Result: 在Triton上实现的原型系统，在多GPU工作负载上平均获得1.3倍端到端加速，最高可达4.7倍加速。

Conclusion: AutoOverlap通过细粒度通信块重叠技术，在单个融合内核内自动优化通信，有效解决了现有粗粒度方法的局限性，显著提升了大规模GPU工作负载的性能。

Abstract: Communication has become a first-order bottleneck in large-cale GPU workloads, and existing distributed compilers address it mainly by overlapping whole compute and communication kernels at the stream level. This coarse granularity incurs extra kernel launches, forces device-wide synchronizations at kernel boundaries, and leaves substantial slack when the slowest tile or kernel stretches the communication tail. We present AutoOverlap, a compiler and runtime that enables automatic fine-grained overlap inside a single fused kernel. AutoOverlap introduces a communication chunk abstraction that decouples communication granularity from kernel structure and backend mechanisms, allowing chunk-level plans to be ported from existing distributed compilers, written directly by users, or instantiated from reusable templates. Given a local Triton kernel and a chunk schedule, AutoOverlap performs transformations to align computation with chunk availability. Implemented as a source-to-source compiler on Triton, AutoOverlap delivers an average end-to-end speedup of 1.3$\times$ and up to 4.7$\times$ on multi-GPU workloads.

</details>


### [8] [OnePiece: A Large-Scale Distributed Inference System with RDMA for Complex AI-Generated Content (AIGC) Workflows](https://arxiv.org/abs/2601.20655)
*June Chen,Neal Xu,Gragas Huang,Bok Zhou,Stephen Liu*

Main category: cs.DC

TL;DR: OnePiece是一个基于RDMA优化的大规模分布式推理系统，专门针对多阶段AI生成内容工作流，通过微服务分解和单边RDMA通信显著降低延迟和CPU开销，提高GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成内容系统在并发工作负载下存在吞吐量、资源利用率和可扩展性方面的关键效率问题，需要更高效的分布式推理解决方案。

Method: 1) 将流水线分解为细粒度微服务；2) 利用单边RDMA通信减少节点间延迟和CPU开销；3) 采用新型双环缓冲区设计解决RDMA内存访问死锁；4) 动态节点管理器根据实时负载弹性分配资源。

Result: 在Wan2.1图像到视频生成任务中，相比单体推理流水线，OnePiece将GPU资源消耗降低了16倍，提供了可扩展、容错且高效的生产环境解决方案。

Conclusion: OnePiece通过分布式微服务架构、RDMA优化和动态资源管理，为生产级AIGC环境提供了显著更高效的推理系统，解决了现有系统的瓶颈问题。

Abstract: The rapid growth of AI-generated content (AIGC) has enabled high-quality creative production across diverse domains, yet existing systems face critical inefficiencies in throughput, resource utilization, and scalability under concurrent workloads. This paper introduces OnePiece, a large-scale distributed inference system with RDMA optimized for multi-stage AIGC workflows. By decomposing pipelines into fine-grained microservices and leveraging one-sided RDMA communication, OnePiece significantly reduces inter-node latency and CPU overhead while improving GPU utilization. The system incorporates a novel double-ring buffer design to resolve deadlocks in RDMA-aware memory access without CPU involvement. Additionally, a dynamic Node Manager allocates resources elastically across workflow stages in response to real-time load. Experimental results demonstrate that OnePiece reduces GPU resource consumption by 16x in Wan2.1 image-to-video generation compared to monolithic inference pipelines, offering a scalable, fault-tolerant, and efficient solution for production AIGC environments.

</details>


### [9] [Agentic Fog: A Policy-driven Framework for Distributed Intelligence in Fog Computing](https://arxiv.org/abs/2601.20764)
*Saeed Akbar,Muhammad Waqas,Rahmat Ullah*

Main category: cs.DC

TL;DR: 提出Agentic Fog（AF）模型，将雾节点表示为策略驱动的自主代理，通过基于共享内存和局部协调的p2p交互进行通信，确保在异步更新、有限理性最佳响应动态和节点故障下的收敛性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 雾计算和边缘计算需要能够处理部分可观测性、严格延迟要求和动态变化工作负载的自适应控制方案。现有的基于大语言模型的Agentic AI工具由于计算成本高、随机性强和形式化分析能力差，不适用于基础设施级系统。

Method: 提出Agentic Fog（AF）通用模型：1）将雾节点表示为策略驱动的自主代理；2）通过基于共享内存和局部协调的p2p交互进行通信；3）将系统目标分解为抽象策略指导；4）将去中心化雾协调形式化为精确势博弈；5）确保在异步更新、有限理性最佳响应动态和节点故障下的收敛性和稳定性。

Result: 仿真表明：1）AF系统相比贪婪启发式算法和整数线性规划，在动态条件下实现了更低的平均延迟；2）能够更高效地适应变化的需求；3）敏感性分析显示在不同内存和协调条件下都能保持最优性能。

Conclusion: Agentic Fog框架为雾计算和边缘计算提供了一种形式化可分析、计算效率高且稳定的自适应控制方案，克服了现有Agentic AI工具在基础设施系统中的局限性。

Abstract: Fog and edge computing require adaptive control schemes that can handle partial observability, severe latency requirements, and dynamically changing workloads. Recent research on Agentic AI (AAI) increasingly integrates reasoning systems powered by Large Language Models; however, these tools are not applicable to infrastructure-level systems due to their high computational cost, stochastic nature, and poor formal analyzability. In this paper, a generic model, Agentic Fog (AF), is presented, in which fog nodes are represented as policy-driven autonomous agents that communicate via p2p interactions based on shared memory and localized coordination. The suggested architecture decomposes a system's goals into abstract policy guidance and formalizes decentralized fog coordination as an exact potential game. The framework is guaranteed to converge and remain stable under asynchronous updates, bounded-rational best-response dynamics, and node failures. Simulations demonstrate that the AF system achieves lower average latency and adapts more efficiently to varying demand than greedy heuristics and integer linear programming under dynamic conditions. The sensitivity analysis also demonstrates the capability to perform optimally under different memory and coordination conditions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [Bench4HLS: End-to-End Evaluation of LLMs in High-Level Synthesis Code Generation](https://arxiv.org/abs/2601.19941)
*M Zafir Sadik Khan,Kimia Azar,Hadi Kamali*

Main category: cs.AR

TL;DR: Bench4HLS：首个针对LLM生成HLS设计的综合基准测试框架，包含170个案例，支持自动化评估编译、功能正确性、合成可行性及PPA分析。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在RTL硬件设计中展现强大能力，HLS应用相对滞后但兴趣增长迅速（HLS:RTL研究比例从1:10升至2:10），需要专门的基准测试框架来评估LLM生成的HLS设计。

Method: 构建包含170个手工起草验证案例的Bench4HLS框架，涵盖从简单内核到复杂加速器；支持自动化评估编译成功率、功能正确性（通过仿真）、合成可行性/优化；集成可插拔API用于PPA分析，兼容多种HLS工具链和架构。

Result: 框架已在Xilinx Vitis HLS上演示，并在Catapult HLS上验证；提供了结构化、可扩展、即插即用的测试平台，为HLS工作流中的LLM基准测试建立了基础方法。

Conclusion: Bench4HLS填补了LLM在HLS领域评估的空白，为未来LLM在高级设计入口的应用提供了标准化评估框架，支持跨工具链和架构的PPA分析能力。

Abstract: In last two years, large language models (LLMs) have shown strong capabilities in code generation, including hardware design at register-transfer level (RTL). While their use in high-level synthesis (HLS) remains comparatively less mature, the ratio of HLS- to RTL-focused studies has shifted from 1:10 to 2:10 in the past six months, indicating growing interest in leveraging LLMs for high-level design entry while relying on downstream synthesis for optimization. This growing trend highlights the need for a comprehensive benchmarking and evaluation framework dedicated to LLM-based HLS. To address this, We present Bench4HLS for evaluating LLM-generated HLS designs. Bench4HLS comprises 170 manually drafted and validated case studies, spanning small kernels to complex accelerators, curated from widely used public repositories. The framework supports fully automated assessment of compilation success, functional correctness via simulation, and synthesis feasibility/optimization. Crucially, Bench4HLS integrates a pluggable API for power, performance, and area (PPA) analysis across various HLS toolchains and architectures, demonstrated here with Xilinx Vitis HLS and validated on Catapult HLS. By providing a structured, extensible, and plug-and-play testbed, Bench4HLS establishes a foundational methodology for benchmarking LLMs in HLS workflows.

</details>


### [11] [Flexible Bit-Truncation Memory for Approximate Applications on the Edge](https://arxiv.org/abs/2601.19900)
*William Oswald,Mario Renteria-Pinon,Md. Sajjad Hossain,Kyle Mooney,Md. Bipul Hossain,Destinie Diggs,Yiwen Xu,Mohamed Shaban,Jinhui Wang,Na Gong*

Main category: cs.AR

TL;DR: 提出一种具有完全自适应灵活性的位截断存储器，可运行时截断任意位数以满足不同近似应用的质量-功耗权衡需求，应用于视频处理和深度学习，显著提升能效。


<details>
  <summary>Details</summary>
Motivation: 现有位截断存储器需要为特定应用定制设计，缺乏灵活性。需要一种能够适应多种近似应用、在运行时灵活调整位截断数量的存储器，以优化边缘环境中的能效。

Method: 开发了一种新型位截断存储器，具有完全自适应灵活性，可在运行时截断任意数量的数据位。将该存储器应用于两种数据密集型近似应用：视频处理和深度学习，支持三种不同的视频应用场景。

Result: 在视频处理应用中，相比最先进技术，功耗节省高达47.02%；在深度学习应用中，对基准和剪枝轻量级模型分别实现高达51.69%的功耗节省，且实现成本低（仅2.89%的硅面积开销）。

Conclusion: 提出的位截断存储器具有完全自适应灵活性，能够有效支持多种近似应用，显著提升能效，同时保持较低的实现成本，适合边缘环境部署。

Abstract: Bit truncation has demonstrated great potential to enable run-time quality-power adaptive data storage, thereby optimizing the power/energy efficiency of approximate applications and supporting their deployment in edge environments. However, existing bit-truncation memories require custom designs for a specific application. In this paper, we present a novel bit-truncation memory with full adaptation flexibility, which can truncate any number of data bits at run time to meet different quality and power trade-off requirements for various approximate applications. The developed bit-truncation memory has been applied to two representative data-intensive approximate applications: video processing and deep learning. Our experiments show that the proposed memory can support three different video applications (including luminance-aware, content-aware, and region-of-interest-aware) with enhanced power efficiency (up to 47.02% power savings) as compared to state-of-the-art. In addition, the proposed memory achieves significant (up to 51.69%) power savings for both baseline and pruned lightweight deep learning models, respectively, with a low implementation cost (2.89% silicon area overhead).

</details>


### [12] [A Flower-Inspired Solution for Computer Memory Wear-Leveling](https://arxiv.org/abs/2601.19902)
*Elizabeth Shen,Huiyang Zhou*

Main category: cs.AR

TL;DR: 提出基于黄金比例的双环磨损均衡方法，通过模拟花朵花瓣均匀接收阳光的原理，在不改变硬件、不降低程序性能的情况下延长内存寿命


<details>
  <summary>Details</summary>
Motivation: 延长计算机内存寿命对减少电子垃圾和可持续发展至关重要。内存磨损不均衡是主要障碍，而新兴内存技术（如相变存储器）寿命更短，问题更加紧迫。现有解决方案要么需要复杂的硬件扩展，要么仅适用于特定程序结构（如循环）。

Method: 提出双环磨损均衡方法，灵感来自黄金比例和花朵花瓣均匀接收阳光的自然规律。将内存建模为两个环，并与现有的内存管理和垃圾回收技术结合，实现有效的磨损均衡。

Result: 该方法具有确定性，能够自动适应内存大小，无需硬件改动，且不会给程序执行带来性能下降，有效减少内存磨损并延长内存寿命。

Conclusion: 双环磨损均衡方法为解决内存磨损不均衡问题提供了一种有效解决方案，通过借鉴自然规律和结合现有技术，实现了无需硬件改动、无性能损失的内存寿命延长方案。

Abstract: Lengthening a computer memory's lifespan is important for e-waste and sustainability. Uneven wear of memory is a major barrier. The problem is becoming even more urgent as emerging memory such as phase-change memory is subject to even shorter lifespan. Various solutions have been proposed, but they either require complicated hardware extensions or apply only to certain program constructs such as loops. This research proposes a new method, dual-ring wear leveling. It takes inspiration from the natural law known as the ``golden ratio" and how it helps flower petals evenly receive sun lights. By modeling memory as two rings and combines the idea with existing memory management, garbage collection, the new solution offers an effective way to reduce memory wear and hence lengthen memory lifespan. It is deterministic, able to automatically adapt to memory size, requiring no hardware changes, and adding no slowdown to program executions.

</details>


### [13] [STELLAR: Structure-guided LLM Assertion Retrieval and Generation for Formal Verification](https://arxiv.org/abs/2601.19903)
*Saeid Rajabi,Chengmo Yang,Satwik Patnaik*

Main category: cs.AR

TL;DR: STELLAR框架利用结构相似性指导LLM生成SystemVerilog断言，通过AST结构指纹检索相关知识库中的(RTL, SVA)对，显著提升断言质量


<details>
  <summary>Details</summary>
Motivation: 传统手动编写SystemVerilog断言(SVA)过程缓慢且易错，现有LLM方法要么从零生成断言，要么忽略硬件设计中的结构模式和专家编写的断言模式

Method: STELLAR将RTL模块表示为AST结构指纹，从知识库中检索结构相关的(RTL, SVA)对，并将它们集成到结构引导的提示中，指导LLM生成SVA

Result: 实验表明STELLAR在语法正确性、风格对齐和功能正确性方面表现优异，突显了结构感知检索在工业形式验证中的潜力

Conclusion: 结构相似性指导的LLM生成是提升SystemVerilog断言质量的有效方法，结构感知检索为工业形式验证提供了有前景的方向

Abstract: Formal Verification (FV) relies on high-quality SystemVerilog Assertions (SVAs), but the manual writing process is slow and error-prone. Existing LLM-based approaches either generate assertions from scratch or ignore structural patterns in hardware designs and expert-crafted assertions. This paper presents STELLAR, the first framework that guides LLM-based SVA generation with structural similarity. STELLAR represents RTL blocks as AST structural fingerprints, retrieves structurally relevant (RTL, SVA) pairs from a knowledge base, and integrates them into structure-guided prompts. Experiments show that STELLAR achieves superior syntax correctness, stylistic alignment, and functional correctness, highlighting structure-aware retrieval as a promising direction for industrial FV.

</details>


### [14] [DABench-LLM: Standardized and In-Depth Benchmarking of Post-Moore Dataflow AI Accelerators for LLMs](https://arxiv.org/abs/2601.19904)
*Ziyu Hu,Zhiqing Zhong,Weijian Zheng,Zhijing Ye,Xuwei Tan,Xueru Zhang,Zheng Xie,Rajkumar Kettimuthu,Xiaodong Yu*

Main category: cs.AR

TL;DR: DABench-LLM是首个针对数据流AI加速器的LLM训练基准测试框架，通过芯片内性能分析和芯片间可扩展性分析，全面评估资源分配、负载平衡和资源效率等关键指标。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的指数级增长，传统CPU/GPU架构受限于摩尔定律放缓，而数据流AI加速器虽然前景广阔，但缺乏深入的性能分析和标准化基准测试方法。

Method: 开发DABench-LLM框架，结合芯片内性能分析和芯片间可扩展性分析，提供全面的评估指标，包括资源分配、负载平衡和资源效率等。

Result: 在Cerebras WSE-2、SambaNova RDU和Graphcore IPU三种商用数据流加速器上验证了框架的有效性，揭示了性能瓶颈并提供了具体的优化策略。

Conclusion: DABench-LLM框架具有通用性和有效性，能够帮助研究人员快速理解底层硬件和系统行为，为性能优化提供指导，适用于多种数据流AI硬件平台。

Abstract: The exponential growth of large language models has outpaced the capabilities of traditional CPU and GPU architectures due to the slowdown of Moore's Law. Dataflow AI accelerators present a promising alternative; however, there remains a lack of in-depth performance analysis and standardized benchmarking methodologies for LLM training. We introduce DABench-LLM, the first benchmarking framework designed for evaluating LLM workloads on dataflow-based accelerators. By combining intra-chip performance profiling and inter-chip scalability analysis, DABench-LLM enables comprehensive evaluation across key metrics such as resource allocation, load balance, and resource efficiency. The framework helps researchers rapidly gain insights into underlying hardware and system behaviors, and provides guidance for performance optimizations. We validate DABench-LLM on three commodity dataflow accelerators, Cerebras WSE-2, SambaNova RDU, and Graphcore IPU. Our framework reveals performance bottlenecks and provides specific optimization strategies, demonstrating its generality and effectiveness across a diverse range of dataflow-based AI hardware platforms.

</details>


### [15] [Hardware-Aware Model Design and Training of Silicon-based Analog Neural Networks](https://arxiv.org/abs/2601.19905)
*Giulio Filippeschi,Mirko Brazzini,Cristhopher Mosquera,Marco Lanuzza,Alessandro Catania,Sebastiano Strangio,Giuseppe Iannaccone*

Main category: cs.AR

TL;DR: 通过物理感知硬件模型重新训练硅基模拟神经网络，可在存在显著非理想性的情况下完全恢复理想模型的推理精度，避免了传统方法的高成本校准和保守设计。


<details>
  <summary>Details</summary>
Motivation: 硅基模拟神经网络在物理实现中存在非理想性（如电容串扰和位线电压降），传统方法通过提高模拟精度来补偿，但会带来显著的能耗、面积和设计开销。需要一种更高效的方法来应对这些非理想性。

Method: 提出物理感知硬件感知模型，针对基于单晶体管浮栅存储单元的时间域向量矩阵乘法器，显式建模电容串扰和位线电压降。模型将操作离散化为自适应时隙，并行处理激活模式，累积贡献以预测有效乘法器输出。使用16x16硅阵列测量校准模型，改进权重提取程序，并通过硬件感知模型在前向传播中训练神经网络。

Result: 模型校准显示串扰是布局依赖且通常占主导地位；改进的权重提取程序使信噪比相比理想向量矩阵乘法器模型提高一倍；通过硬件感知训练，在三种架构（自定义MLP、LeNet-5、VGG-style CNN）上完全恢复了理想软件网络的精度。

Conclusion: 物理感知硬件感知模型能够有效补偿硅基模拟神经网络中的非理想性，建立了一个完整的时间域模拟神经形态芯片从设计到部署的工作流程，为可扩展性和集成密度提供了更优方案。

Abstract: Silicon-based analog neural networks physically embody the ideal neural network model in an approximate way. We show that by retraining the neural network using a physics-informed hardware-aware model one can fully recover the inference accuracy of the ideal network model even in the presence of significant non-idealities. This is way more promising for scalability and integration density than the default option of improving the fidelity of the analog neural network at the cost of significant energy, area, and design overhead, through extensive calibration and conservative analog design.
  We first present a physics-informed hardware-aware model for a time-domain vector-matrix multiplier implemented with single-transistor floating-gate memory cells that explicitly accounts for two dominant non-idealities of the physical implementation - capacitive crosstalk and bit-line voltage drop - and integrates seamlessly with modern deep-learning workflows. The model discretizes each operation into adaptive time slots, processes activation patterns in parallel, and accumulates their contributions to predict effective multiplier outputs. Using measurements from a 16x16 silicon array, we calibrate the model, show that crosstalk is layout-dependent and often dominant, and introduce an improved weight-extraction procedure that doubles signal-to-error ratio versus an ideal vector-matrix multiplier model. Finally, we show that by training silicon-based analog neural networks using an hardware-aware model in the forward pass we can recover the accuracy of the ideal software networks across three architectures -- custom MLP on low-resolution MNIST, LeNet-5 on MNIST, and a VGG-style CNN on CIFAR-10 - establishing a complete design-to-deployment workflow for time-domain analog neuromorphic chips.

</details>


### [16] [GTAC: A Generative Transformer for Approximate Circuits](https://arxiv.org/abs/2601.19906)
*Jingxin Wang,Shitong Guo,Ruicheng Dai,Wenhui Liang,Ruogu Ding,Xin Ning,Weikang Qian*

Main category: cs.AR

TL;DR: GTAC是一种基于生成式Transformer的近似电路设计模型，通过集成误差阈值约束，在PPA优化方面超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 针对容错应用，近似电路通过引入可控误差来显著改善电路的性能、功耗和面积（PPA）。现有方法在平衡误差约束和PPA优化方面仍有提升空间。

Method: 提出GTAC模型，这是一种基于生成式Transformer的近似电路设计方法。创新性地将误差阈值约束集成到设计过程中，结合近似计算和AI驱动的EDA技术。

Result: 与现有最先进方法相比，GTAC在误差率约束下进一步减少了6.4%的面积，同时设计速度提升了4.3倍。

Conclusion: GTAC证明了生成式Transformer模型在近似电路设计中的有效性，能够更好地平衡误差约束和PPA优化，为AI驱动的EDA提供了新思路。

Abstract: Targeting error-tolerant applications, approximate circuits introduce controlled errors to significantly improve performance, power, and area (PPA) of circuits. In this work, we introduce GTAC, a novel generative Transformer-based model for producing approximate circuits. By leveraging principles of approximate computing and AI-driven EDA, our model innovatively integrates error thresholds into the design process. Experimental results show that compared with a state-of-the-art method, GTAC further reduces 6.4% area under the error rate constraint, while being 4.3x faster.

</details>


### [17] [RAPID-Graph: Recursive All-Pairs Shortest Paths Using Processing-in-Memory for Dynamic Programming on Graphs](https://arxiv.org/abs/2601.19907)
*Yanru Chen,Zheyu Li,Keming Fan,Runyang Tian,John Hsu,Weihong Xu,Minxuan Zhou,Tajana Rosing*

Main category: cs.AR

TL;DR: RAPID-Graph是一个针对全对最短路径问题的存内计算系统，通过算法、架构和器件层面的协同设计，在2.45M节点的OGBN-Products数据集上相比GPU集群实现了5.8倍加速和1186倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 全对最短路径计算在大规模图分析中存在数据移动的立方复杂度瓶颈，传统内存层次带宽无法满足需求，需要新的计算范式来解决这一挑战。

Method: 采用存内计算系统协同设计：1) 算法层面引入递归感知分区器，将图分解为顶点瓦片，实现完全原地计算；2) 架构层面设计2.5D存内计算堆栈，集成相变存储器计算芯片、逻辑芯片和高带宽暂存器；3) 外部非易失存储堆栈持久存储大规模结果。

Result: 在2.45M节点的OGBN-Products数据集上，相比最先进GPU集群快5.8倍、能效高1186倍；相比现有存内计算加速器快8.3倍、能效高104倍；相比NVIDIA H100 GPU最高实现42.8倍加速和392倍节能。

Conclusion: RAPID-Graph通过算法-架构-器件的协同优化，成功解决了大规模图分析中的全对最短路径计算瓶颈，为存内计算系统设计提供了有效范例。

Abstract: All-pairs shortest paths (APSP) remains a major bottleneck for large-scale graph analytics, as data movement with cubic complexity overwhelms the bandwidth of conventional memory hierarchies. In this work, we propose RAPID-Graph to address this challenge through a co-designed processing-in-memory (PIM) system that integrates algorithm, architecture, and device-level optimizations. At the algorithm level, we introduce a recursion-aware partitioner that enables an exact APSP computation by decomposing graphs into vertex tiles to reduce data dependency, such that both Floyd-Warshall and Min-Plus kernels execute fully in-place within digital PIM arrays. At the architecture and device levels, we design a 2.5D PIM stack integrating two phase-change memory compute dies, a logic die, and high-bandwidth scratchpad memory within a unified advanced package. An external non-volatile storage stack stores large APSP results persistently. The design achieves both tile-level and unit-level parallel processing to sustain high throughput. On the 2.45M-node OGBN-Products dataset, RAPID-Graph is 5.8x faster and 1,186x more energy efficient than state-of-the-art GPU clusters, while exceeding prior PIM accelerators by 8.3x in speed and 104x in efficiency. It further delivers up to 42.8x speedup and 392x energy savings over an NVIDIA H100 GPU.

</details>


### [18] [CHIME: Chiplet-based Heterogeneous Near-Memory Acceleration for Edge Multimodal LLM Inference](https://arxiv.org/abs/2601.19908)
*Yanru Chen,Runyang Tian,Yue Pan,Zheyu Li,Weihong Xu,Tajana Rosing*

Main category: cs.AR

TL;DR: CHIME是一个基于chiplet的异构近内存加速器，用于边缘设备上的多模态大语言模型推理，通过结合M3D DRAM和RRAM芯片，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的多模态大语言模型推理面临延迟、能耗和连接不稳定的挑战，特别是视觉输入转换为大量token序列会膨胀KV缓存，增加数据移动开销。

Method: CHIME采用chiplet异构架构，结合M3D DRAM（提供低延迟带宽用于注意力计算）和RRAM（提供密集非易失性权重存储），并设计协同映射框架执行近数据融合内核，最小化跨chiplet通信。

Result: 在FastVLM和MobileVLM模型上，相比NVIDIA Jetson Orin NX，CHIME实现最高54倍加速和246倍能效提升，相比最先进的PIM加速器FACIL提供最高69.2倍吞吐量，相比纯M3D DRAM设计提升7%能效和2.4倍性能。

Conclusion: CHIME通过异构内存架构和协同映射设计，有效解决了边缘MLLM推理的带宽和能耗瓶颈，为边缘多模态AI提供了高效的硬件加速方案。

Abstract: The proliferation of large language models (LLMs) is accelerating the integration of multimodal assistants into edge devices, where inference is executed under stringent latency and energy constraints, often exacerbated by intermittent connectivity. These challenges become particularly acute in the context of multimodal LLMs (MLLMs), as high-dimensional visual inputs are transformed into extensive token sequences, thereby inflating the key-value (KV) cache and imposing substantial data movement overheads to the LLM backbone. To address these issues, we present CHIME, a chiplet-based heterogeneous near-memory acceleration for edge MLLMs inference. CHIME leverages the complementary strengths of integrated monolithic 3D (M3D) DRAM and RRAM chiplets: DRAM supplies low-latency bandwidth for attention, while RRAM offers dense, non-volatile storage for weights. This heterogeneous hardware is orchestrated by a co-designed mapping framework that executes fused kernels near data, minimizing cross-chiplet traffic to maximize effective bandwidth. On FastVLM (0.6B/1.7B) and MobileVLM (1.7B/3B), CHIME achieves up to 54x speedup and up to 246x better energy efficiency per inference as compared to the edge GPU NVIDIA Jetson Orin NX. It sustains 116.5-266.5 token/J compared to Jetson's 0.7-1.1 token/J. Furthermore, it delivers up to 69.2x higher throughput than the state-of-the-art PIM accelerator FACIL. Compared to the M3D DRAM-only design, CHIME's heterogeneous memory further improves energy efficiency by 7% and performance by 2.4x.

</details>


### [19] [Understanding Bottlenecks for Efficiently Serving LLM Inference With KV Offloading](https://arxiv.org/abs/2601.19910)
*William Meng,Benjamin Lee,Hong Wang*

Main category: cs.AR

TL;DR: KV缓存卸载技术通过将缓存存储在CPU DRAM中来支持长上下文LLM推理，但PCIe带宽限制造成严重瓶颈。本文开发分析框架推导临界缓存-预填充令牌比例κ_crit，发现典型工作负载远超此阈值，99%延迟来自数据传输，GPU仅使用28%额定功耗，提出硬件互连、模型架构和调度算法优化方案。


<details>
  <summary>Details</summary>
Motivation: KV缓存卸载技术虽然能支持长上下文LLM推理，但PCIe带宽限制导致严重性能瓶颈，需要系统性地分析和优化。

Method: 开发分析框架推导临界缓存-预填充令牌比例κ_crit，进行实证表征分析延迟分布和GPU功耗情况。

Result: 发现典型工作负载远超κ_crit阈值，99%延迟来自数据传输，GPU仅消耗28%额定TDP，表明系统存在严重瓶颈。

Conclusion: 需要针对硬件互连、模型架构和调度算法进行优化，以解决KV缓存卸载中的性能瓶颈问题。

Abstract: KV cache offloading enables long-context LLM inference by storing caches in CPU DRAM, but PCIe bandwidth limitations create severe bottlenecks. In this paper, we develops an analytical framework that derives $κ_{\text{crit}}$, the critical cached-to-prefill token ratio where execution becomes memory-bound and show typical workloads exceed this threshold by orders of magnitude. Empirical characterization reveals 99\% of latency spent on transfers and serving offloaded requests results in GPU's consuming only 28\% of their rated TDP, motivating our proposed optimizations for hardware interconnects, model architectures, and scheduling algorithms.

</details>


### [20] [GPU-Augmented OLAP Execution Engine: GPU Offloading](https://arxiv.org/abs/2601.19911)
*Ilsun Chang*

Main category: cs.AR

TL;DR: 论文提出混合架构，在向量化执行基础上选择性将高影响原语卸载到GPU，通过键值传输和风险感知门控机制优化OLAP系统性能。


<details>
  <summary>Details</summary>
Motivation: 现代OLAP系统通过存储计算分离和列式布局缓解了I/O瓶颈，但在执行层（特别是Top-K选择和连接探测）的CPU成本成为新的规模瓶颈。

Method: 1) 混合架构：在现有向量化执行基础上选择性卸载高影响原语到GPU；2) 键值传输：仅传输键和指针，延迟物化减少数据移动；3) 风险感知门控：基于输入大小、传输成本、内核成本、后处理成本和候选集复杂度(K,M)决定是否触发卸载。

Result: 使用PostgreSQL微基准测试和GPU代理测量，相比始终开启的GPU卸载，门控卸载改善了尾部延迟(P95/P99)。

Conclusion: 该工作将风险感知门控原则从优化器阶段的GPU辅助测量扩展到执行层的OLAP原语，为现代OLAP系统提供了一种有效的CPU-GPU混合执行策略。

Abstract: Modern OLAP systems have mitigated I/O bottlenecks via storage-compute separation and columnar layouts, but CPU costs in the execution layer (especially Top-K selection and join probe) are emerging as new bottlenecks at scale. This paper proposes a hybrid architecture that augments existing vectorized execution by selectively offloading only high-impact primitives to the GPU. To reduce data movement, we use key-only transfer (keys and pointers) with late materialization. We further introduce a Risky Gate (risk-aware gating) that triggers offloading only in gain/risk intervals based on input size, transfer, kernel and post-processing costs, and candidate-set complexity (K, M). Using PostgreSQL microbenchmarks and GPU proxy measurements, we observe improved tail latency (P95/P99) under gated offloading compared to always-on GPU offloading. This work extends the risk-aware gating principle used for optimizer-stage GPU-assisted measurement (arXiv:2512.19750) to execution-layer OLAP primitives.

</details>


### [21] [Analysis of LLM Vulnerability to GPU Soft Errors: An Instruction-Level Fault Injection Study](https://arxiv.org/abs/2601.19912)
*Duo Chai,Zizhen Liu,Shuhuai Wang,Songwei Pei,Cheng Liu,Huawei Li,Shangguang Wang*

Main category: cs.AR

TL;DR: 首次对大型语言模型推理进行指令级故障注入研究，分析模型架构、参数规模和任务复杂度对可靠性的影响


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对计算和内存需求高，依赖高性能GPU，而GPU技术发展使其更易受软错误影响。现有研究主要关注通用应用或传统视觉神经网络，缺乏对现代大规模LLM的系统性分析，而LLM的特性可能使其可靠性特征与早期模型显著不同。

Method: 采用指令级故障注入研究方法，从多个角度分析LLM推理的可靠性特征

Result: 研究揭示了模型架构、参数规模和任务复杂度对LLM可靠性的影响，为理解LLM可靠性提供了新见解

Conclusion: 这是首次对LLM推理进行指令级故障注入研究，研究结果有助于设计更有效的容错机制，填补了LLM可靠性分析的研究空白

Abstract: Large language models (LLMs) are highly compute- and memory-intensive, posing significant demands on high-performance GPUs. At the same time, advances in GPU technology driven by shrinking transistor sizes and lower operating voltages have made these devices increasingly susceptible to soft errors. While prior work has examined GPU reliability, most studies have focused on general-purpose applications or conventional neural networks mostly used for vision tasks such as classification and detection. In contrast, systematic analysis of modern large-scale LLMs remains limited, despite their rapid adoption in diverse application scenarios. Given the unique characteristics of LLMs, their resilience to soft errors may differ substantially from earlier models. To bridge this gap, we conduct the first instruction-level fault injection study of LLM inference. Our approach reveals reliability characteristics from multiple perspectives, highlighting the effects of model architecture, parameter scale, and task complexity. These findings provide new insights into LLM reliability and inform the design of more effective fault tolerance mechanisms.

</details>


### [22] [PiC-BNN: A 128-kbit 65 nm Processing-in-CAM-Based End-to-End Binary Neural Network Accelerator](https://arxiv.org/abs/2601.19920)
*Yuval Harary,Almog Sharoni,Esteban Garzón,Marco Lanuzza,Adam Teman,Leonid Yavits*

Main category: cs.AR

TL;DR: PiC-BNN是一个真正的端到端二进制神经网络加速器，使用汉明距离容错的内容寻址存储器，无需全精度操作，在MNIST和手势数据集上实现了高精度和高能效。


<details>
  <summary>Details</summary>
Motivation: 传统BNN虽然将线性层二值化，但仍需在批归一化、softmax、输出层等部分使用全精度操作，这限制了面积和能效优势，且需要全精度操作架构支持。

Method: 提出PiC-BNN，基于汉明距离容错的内容寻址存储器，利用大数定律实现准确分类，无需全精度操作。在65nm工艺中设计制造，实现端到端二进制处理。

Result: 在MNIST数据集上达到95.2%准确率，在手势数据集上达到93.5%准确率，吞吐量560K推理/秒，能效703M推理/秒/瓦。

Conclusion: PiC-BNN展示了真正的端到端二进制神经网络加速器的可行性，通过汉明距离容错机制实现了高精度和高能效，无需全精度操作支持。

Abstract: Binary Neural Networks (BNNs), where weights and activations are constrained to binary values (+1, -1), are a highly efficient alternative to traditional neural networks. Unfortunately, typical BNNs, while binarizing linear layers (matrix-vector multiplication), still implement other network layers (batch normalization, softmax, output layer, and sometimes the input layer of a convolutional neural network) in full precision. This limits the area and energy benefits and requires architectural support for full precision operations. We propose PiC-BNN, a true end-to-end binary in-approximate search (Hamming distance tolerant) Content Addressable Memory based BNN accelerator. PiC-BNN is designed and manufactured in a commercial 65nm process. PiC-BNN uses Hamming distance tolerance to apply the law of large numbers to enable accurate classification without implementing full precision operations. PiC-BNN achieves baseline software accuracy (95.2%) on the MNIST dataset and 93.5% on the Hand Gesture (HG) dataset, a throughput of 560K inferences/s, and presents a power efficiency of 703M inferences/s/W when implementing a binary MLP model for MNIST/HG dataset classification.

</details>


### [23] [Beyond GEMM-Centric NPUs: Enabling Efficient Diffusion LLM Sampling](https://arxiv.org/abs/2601.20706)
*Binglei Lou,Haoran Wu,Yao Lai,Jiayi Nie,Can Xiao,Xuan Guo,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: 本文针对扩散大语言模型(dLLMs)采样阶段的高延迟问题，提出了一种优化的NPU架构设计，通过轻量级非GEMM向量原语、内存复用策略和混合精度内存层次结构，实现了2.53倍的GPU加速。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型的采样阶段占用了高达70%的总推理延迟，主要原因是词汇级logits的大量内存读写、基于归约的token选择以及迭代掩码更新。这些操作需要大量片上SRAM且涉及不规则内存访问，传统NPU难以高效处理。

Method: 识别出dLLM采样阶段的关键指令集，设计优化的NPU架构：1) 轻量级非GEMM向量原语；2) 原地内存复用策略；3) 解耦的混合精度内存层次结构。开发了周期精确模拟器和RTL验证代码。

Result: 在同等nm技术节点下，相比NVIDIA RTX A6000 GPU实现了最高2.53倍的加速。开源了周期精确模拟器和后综合RTL验证代码，确认与当前dLLM PyTorch实现功能等价。

Conclusion: 针对dLLM采样阶段的独特计算特性，专门优化的NPU架构能够显著提升推理效率，为未来扩散模型硬件加速提供了有效解决方案。

Abstract: Diffusion Large Language Models (dLLMs) introduce iterative denoising to enable parallel token generation, but their sampling phase displays fundamentally different characteristics compared to GEMM-centric transformer layers. Profiling on modern GPUs reveals that sampling can account for up to 70% of total model inference latency-primarily due to substantial memory loads and writes from vocabulary-wide logits, reduction-based token selection, and iterative masked updates. These processes demand large on-chip SRAM and involve irregular memory accesses that conventional NPUs struggle to handle efficiently. To address this, we identify a set of critical instructions that an NPU architecture must specifically optimize for dLLM sampling. Our design employs lightweight non-GEMM vector primitives, in-place memory reuse strategies, and a decoupled mixed-precision memory hierarchy. Together, these optimizations deliver up to a 2.53x speedup over the NVIDIA RTX A6000 GPU under an equivalent nm technology node. We also open-source our cycle-accurate simulation and post-synthesis RTL verification code, confirming functional equivalence with current dLLM PyTorch implementations.

</details>


### [24] [Primitive-Driven Acceleration of Hyperdimensional Computing for Real-Time Image Classification](https://arxiv.org/abs/2601.20061)
*Dhruv Parikh,Jebacyril Arockiaraj,Viktor Prasanna*

Main category: cs.AR

TL;DR: 该论文提出了一种用于超维计算(HDC)的新型图像编码算法和FPGA加速器，在MNIST和Fashion-MNIST上实现了高精度和极低延迟推理。


<details>
  <summary>Details</summary>
Motivation: 超维计算使用高维低精度向量进行学习推理，具有轻量级和噪声容忍特性。但传统CPU/GPU执行HDC核心操作时存在维度高、稀疏性、数据移动频繁等问题，导致利用率低、内存瓶颈和实时性能受限。

Method: 1) 开发类似卷积神经网络的图像编码算法，将局部图像块映射到富含空间信息的超向量，再通过HDC基本操作合并为全局表示；2) 设计端到端FPGA加速器，采用流水线架构，在超向量维度和图像块集合上同时利用并行性。

Result: 图像编码器在MNIST上达到95.67%准确率，Fashion-MNIST上达到85.14%，优于现有HDC图像编码器。Alveo U280 FPGA实现实现0.09ms推理延迟，相比最先进CPU和GPU基线分别实现1300倍和60倍加速。

Conclusion: 该研究提出的HDC图像编码算法和FPGA加速器架构有效解决了传统处理器上的性能瓶颈，实现了高精度和极低延迟的图像分类，为实时边缘计算应用提供了有前景的解决方案。

Abstract: Hyperdimensional Computing (HDC) represents data using extremely high-dimensional, low-precision vectors, termed hypervectors (HVs), and performs learning and inference through lightweight, noise-tolerant operations. However, the high dimensionality, sparsity, and repeated data movement involved in HDC make these computations difficult to accelerate efficiently on conventional processors. As a result, executing core HDC operations: binding, permutation, bundling, and similarity search: on CPUs or GPUs often leads to suboptimal utilization, memory bottlenecks, and limits on real-time performance. In this paper, our contributions are two-fold. First, we develop an image-encoding algorithm that, similar in spirit to convolutional neural networks, maps local image patches to hypervectors enriched with spatial information. These patch-level hypervectors are then merged into a global representation using the fundamental HDC operations, enabling spatially sensitive and robust image encoding. This encoder achieves 95.67% accuracy on MNIST and 85.14% on Fashion-MNIST, outperforming prior HDC-based image encoders. Second, we design an end-to-end accelerator that implements these compute operations on an FPGA through a pipelined architecture that exploits parallelism both across the hypervector dimensionality and across the set of image patches. Our Alveo U280 implementation delivers 0.09ms inference latency, achieving up to 1300x and 60x speedup over state-of-the-art CPU and GPU baselines, respectively.

</details>


### [25] [A Paradigm for Generalized Multi-Level Priority Encoders](https://arxiv.org/abs/2601.20067)
*Maxwell Phillips,Firas Hassan,Ahmed Ammar*

Main category: cs.AR

TL;DR: 本文提出了一种新的多级优先级编码器设计范式，通过级联和组合技术扩展到三、四级结构，分析了不同架构在FPGA和ASIC实现中的复杂度与延迟权衡，为硬件设计者提供了优化设计工具包。


<details>
  <summary>Details</summary>
Motivation: 传统优先级编码器在高位精度（如512位以上）时硬件复杂度高，限制了其在高速整数运算和内容寻址存储器等关键应用中的加速潜力。需要降低复杂度以实现更广泛的应用。

Method: 将已有的两级优先级编码器结构推广到三、四级，采用级联和组合两种技术，并与传统单级结构、树形设计、递归设计等进行对比分析，评估不同输入长度下的复杂度和延迟。

Result: 两级架构在复杂度和延迟间取得平衡（复杂度降低约一半，延迟相应增加）；更多级别收益递减；树形和递归设计更快但更复杂；为不同输入长度和实现技术提供了设计选择建议。

Conclusion: 通过多级优先级编码器架构的全面分析和比较，为硬件设计者提供了根据具体需求（复杂度或延迟优先）选择最优设计的工具包，平衡了性能与资源消耗的权衡。

Abstract: Priority encoders are typically considered expensive hardware components in terms of complexity, especially at high bit precisions or input lengths (e.g., above 512 bits). However, if the complexity can be reduced, priority encoders can feasibly accelerate a variety of key applications, such as high-precision integer arithmetic and content-addressable memory. We propose a new paradigm for constructing priority encoders by generalizing the previously proposed two-level priority encoder structure. We extend this concept to three and four levels using two techniques -- cascading and composition -- and discuss further generalization. We then analyze the complexity and delay of new and existing priority encoder designs as a function of input length, for both FPGA and ASIC implementation technologies. In particular, we compare the multi-level structure to the traditional single-level priority encoder structure, a tree-based design, a recursive design, and the two-level structure. We find that the two-level architecture provides balanced performance -- reducing complexity by around half, but at the cost of a corresponding increase in delay. Additional levels have diminishing returns, highlighting a tradeoff between complexity and delay. Meanwhile, the tree and recursive designs are generally faster, but are more complex than the two-level and multi-level structures. We explore several characteristics and patterns of the designs across a wide range of input lengths. We then provide recommendations on which architecture to use for a given input length and implementation technology, based on which design factors -- such as complexity or delay -- are most important to the hardware designer. With this overview and analysis of various priority encoder architectures, we provide a priority encoder toolkit to assist hardware designers in creating the most optimal design.

</details>


### [26] [How Much Progress Has There Been in NVIDIA Datacenter GPUs?](https://arxiv.org/abs/2601.20115)
*Emanuele Del Sozzo,Martin Fleming,Kenneth Flamm,Neil Thompson*

Main category: cs.AR

TL;DR: 该研究分析了2000年代中期至今NVIDIA数据中心GPU的技术进步趋势，计算了各项性能指标的倍增时间，并评估了美国出口管制对AI芯片性能差距的影响。


<details>
  <summary>Details</summary>
Motivation: 随着GPU在AI等关键领域的重要性日益增加，了解其技术进步趋势对于预测未来科学研究的限制至关重要。特别是在美国实施AI芯片出口管制的背景下，分析GPU技术发展轨迹具有重要现实意义。

Method: 研究收集了从2000年代中期至今的NVIDIA数据中心GPU综合数据集，包含计算性能、价格等多项特征。通过分析主要GPU特征趋势，计算了每内存带宽、每美元和每瓦特的性能增长指标。

Result: FP16和FP32运算的倍增时间为1.44-1.69年，FP64为2.06-3.79年。内存大小和带宽增长较慢（3.32-3.53年）。数据中心GPU价格每5.1年翻倍，功耗每16年翻倍。出口管制调整可将性能差距从23.6倍缩小到3.54倍。

Conclusion: GPU计算性能增长远快于内存带宽和价格增长，出口管制政策对AI芯片性能差距有显著影响。这些发现为理解GPU技术发展趋势和政策影响提供了重要参考。

Abstract: Graphics Processing Units (GPUs) are the state-of-the-art architecture for essential tasks, ranging from rendering 2D/3D graphics to accelerating workloads in supercomputing centers and, of course, Artificial Intelligence (AI). As GPUs continue improving to satisfy ever-increasing performance demands, analyzing past and current progress becomes paramount in determining future constraints on scientific research. This is particularly compelling in the AI domain, where rapid technological advancements and fierce global competition have led the United States to recently implement export control regulations limiting international access to advanced AI chips. For this reason, this paper studies technical progress in NVIDIA datacenter GPUs released from the mid-2000s until today. Specifically, we compile a comprehensive dataset of datacenter NVIDIA GPUs comprising several features, ranging from computational performance to release price. Then, we examine trends in main GPU features and estimate progress indicators for per-memory bandwidth, per-dollar, and per-watt increase rates. Our main results identify doubling times of 1.44 and 1.69 years for FP16 and FP32 operations (without accounting for sparsity benefits), while FP64 doubling times range from 2.06 to 3.79 years. Off-chip memory size and bandwidth grew at slower rates than computing performance, doubling every 3.32 to 3.53 years. The release prices of datacenter GPUs have roughly doubled every 5.1 years, while their power consumption has approximately doubled every 16 years. Finally, we quantify the potential implications of current U.S. export control regulations in terms of the potential performance gaps that would result if implementation were assumed to be complete and successful. We find that recently proposed changes to export controls would shrink the potential performance gap from 23.6x to 3.54x.

</details>


### [27] [SATA: Sparsity-Aware Scheduling for Selective Token Attention](https://arxiv.org/abs/2601.20267)
*Zhenkun Fan,Zishen Wan,Che-Kai Liu,Ashwin Sanjay Lele,Win-San Khwa,Bo Zhang,Meng-Fan Chang,Arijit Raychowdhury*

Main category: cs.AR

TL;DR: SATA是一种面向稀疏访问模式的动态调度方案，通过重排序操作数流和利用数据局部性，提升选择性注意力机制在硬件实现中的效率和能效。


<details>
  <summary>Details</summary>
Motivation: Transformer的注意力机制存在二次方复杂度问题，给硬件高效实现带来挑战。虽然量化和剪枝等技术有所帮助，但选择性令牌注意力通过仅关注最相关令牌来减少计算和过滤噪声，为硬件优化提供了新方向。

Method: 提出SATA（局部性中心的动态调度方案），主动管理来自选择性Query-Key操作的稀疏分布访问模式。通过重排序操作数流和利用数据局部性，实现中间Query/Key向量的早期获取和释放，提高系统利用率。

Result: 实验结果显示，该方法在基于选择性注意力模型的运行时轨迹上，系统吞吐量提升最高达1.76倍，能效提升2.94倍，同时调度开销最小。

Conclusion: SATA通过有效的令牌管理和调度策略，显著提升了选择性注意力机制在硬件实现中的性能和能效，为解决Transformer注意力二次方复杂度问题提供了有效的硬件优化方案。

Abstract: Transformers have become the foundation of numerous state-of-the-art AI models across diverse domains, thanks to their powerful attention mechanism for modeling long-range dependencies. However, the quadratic scaling complexity of attention poses significant challenges for efficient hardware implementation. While techniques such as quantization and pruning help mitigate this issue, selective token attention offers a promising alternative by narrowing the attention scope to only the most relevant tokens, reducing computation and filtering out noise.
  In this work, we propose SATA, a locality-centric dynamic scheduling scheme that proactively manages sparsely distributed access patterns from selective Query-Key operations. By reordering operand flow and exploiting data locality, our approach enables early fetch and retirement of intermediate Query/Key vectors, improving system utilization. We implement and evaluate our token management strategy in a control and compute system, using runtime traces from selective-attention-based models. Experimental results show that our method improves system throughput by up to 1.76x and boosts energy efficiency by 2.94x, while incurring minimal scheduling overhead.

</details>


### [28] [VersaQ-3D: A Reconfigurable Accelerator Enabling Feed-Forward and Generalizable 3D Reconstruction via Versatile Quantization](https://arxiv.org/abs/2601.20317)
*Yipu Zhang,Jintao Cheng,Xingyu Liu,Zeyu Li,Carol Jingyi Li,Jin Wu,Lin Jiang,Yuan Xie,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: VersaQ-3D是一个算法-架构协同设计框架，通过无校准的4位量化和专用加速器，解决了VGGT模型在边缘设备部署中的内存和计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: VGGT模型虽然能实现无需逐场景优化的前馈式3D重建，但其十亿参数规模带来了高内存和计算需求，阻碍了在设备端的部署。现有LLM量化方法在VGGT上失效，且硬件上存在精度敏感的非线性算子和内存密集的全局注意力问题。

Method: 提出VersaQ-3D框架：算法层面，首次实现无需校准、场景无关的4位量化，利用正交变换解相关特征并抑制异常值；架构层面，设计支持BF16、INT8和INT4的可重构加速器，采用统一脉动数据路径处理线性和非线性算子，以及两阶段重计算分块缓解长序列注意力的内存压力。

Result: 在W4A8配置下保持98-99%的准确率；在W4A4配置下，相比先前方法在多样场景中性能提升1.61x-2.39x；加速器相比边缘GPU实现5.2x-10.8x的加速，功耗低，支持高效的即时3D重建。

Conclusion: VersaQ-3D通过算法-架构协同设计，成功解决了VGGT模型在边缘设备部署的挑战，实现了高效、低功耗的即时3D重建能力。

Abstract: The Visual Geometry Grounded Transformer (VGGT) enables strong feed-forward 3D reconstruction without per-scene optimization. However, its billion-parameter scale creates high memory and compute demands, hindering on-device deployment. Existing LLM quantization methods fail on VGGT due to saturated activation channels and diverse 3D semantics, which cause unreliable calibration. Furthermore, VGGT presents hardware challenges regarding precision-sensitive nonlinear operators and memory-intensive global attention. To address this, we propose VersaQ-3D, an algorithm-architecture co-design framework. Algorithmically, we introduce the first calibration-free, scene-agnostic quantization for VGGT down to 4-bit, leveraging orthogonal transforms to decorrelate features and suppress outliers. Architecturally, we design a reconfigurable accelerator supporting BF16, INT8, and INT4. A unified systolic datapath handles both linear and nonlinear operators, reducing latency by 60%, while two-stage recomputation-based tiling alleviates memory pressure for long-sequence attention. Evaluations show VersaQ-3D preserves 98-99% accuracy at W4A8. At W4A4, it outperforms prior methods by 1.61x-2.39x across diverse scenes. The accelerator delivers 5.2x-10.8x speedup over edge GPUs with low power, enabling efficient instant 3D reconstruction.

</details>
