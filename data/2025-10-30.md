<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [The Singularity Theory of Concurrent Programs: A Topological Characterization and Detection of Deadlocks and Livelocks](https://arxiv.org/abs/2510.25112)
*Di Zhang*

Main category: cs.PL

TL;DR: 提出并发程序分析的奇点理论，将执行空间建模为分支拓扑空间，使用代数拓扑工具检测死锁和活锁。


<details>
  <summary>Details</summary>
Motivation: 为并发程序验证建立几何和拓扑基础，超越传统模型检测的局限性。

Method: 将程序执行空间建模为分支拓扑空间，使用同伦和同调群等代数拓扑工具定义并发拓扑不变量。

Result: 能够系统性地检测和分类并发"奇点"（死锁和活锁），无需穷举遍历所有状态。

Conclusion: 奇点理论为并发程序验证提供了新的几何拓扑框架，有望改进传统验证方法。

Abstract: This paper introduces a novel paradigm for the analysis and verification of
concurrent programs -- the Singularity Theory. We model the execution space of
a concurrent program as a branched topological space, where program states are
points and state transitions are paths. Within this framework, we characterize
deadlocks as attractors and livelocks as non-contractible loops in the
execution space. By employing tools from algebraic topology, particularly
homotopy and homology groups, we define a series of concurrent topological
invariants to systematically detect and classify these concurrent
"singularities" without exhaustively traversing all states. This work aims to
establish a geometric and topological foundation for concurrent program
verification, transcending the limitations of traditional model checking.

</details>


### [2] [Have a thing? Reasoning around recursion with dynamic typing in grounded arithmetic](https://arxiv.org/abs/2510.25369)
*Elliot Bobrow,Bryan Ford,Stefan Milenković*

Main category: cs.PL

TL;DR: GA（基础算术）是一种支持任意递归函数定义的形式推理基础系统，通过调整推理规则使非终止计算无害地表示"底部"值，避免逻辑悖论。


<details>
  <summary>Details</summary>
Motivation: 传统逻辑系统（经典和直觉主义逻辑）无法直接表达任意一般递归函数而不导致不一致性，需要一种能安全处理非终止计算的形式推理基础。

Method: 调整传统推理规则，使表示非终止计算的项无害地表示语义值"底部"；通过"动态类型"或符号反向执行计算来证明递归函数终止性；在Isabelle/HOL中机械验证一致性。

Result: 开发了GA系统，支持任意递归定义而不引入不一致性；提供了机械验证的一致性证明；终止性证明后可使用经典推理规则。

Conclusion: GA展示了将任意递归定义的表达自由融入形式系统的可行性，为日常计算推理的手动或自动化应用奠定了基础。

Abstract: Neither the classical nor intuitionistic logic traditions are
perfectly-aligned with the purpose of reasoning about computation, in that
neither logical tradition can normally permit the direct expression of
arbitrary general-recursive functions without inconsistency. We introduce
grounded arithmetic or GA, a minimalistic but nonetheless powerful foundation
for formal reasoning that allows the direct expression of arbitrary recursive
definitions. GA adjusts the traditional inference rules such that terms that
express nonterminating computations harmlessly denote no semantic value (i.e.,
"bottom") instead of leading into logical paradox or inconsistency. Recursive
functions may be proven terminating in GA essentially by "dynamically typing"
terms, or equivalently, symbolically reverse-executing the computations they
denote via GA's inference rules. Once recursive functions have been proven
terminating, logical reasoning about their results reduce to the familiar
classical rules. A mechanically-checked consistency proof in Isabelle/HOL
exists for the basic quantifier-free fragment of GA. Quantifiers may be added
atop this foundation as ordinary computations, whose inference rules are thus
admissible and do not introduce new inconsistency risks. While GA is only a
first step towards richly-typed grounded deduction practical for everyday use
in manual or automated computational reasoning, it shows the promise that the
expressive freedom of arbitrary recursive definition can in principle be
incorporated into formal systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Radar DataTree: A FAIR and Cloud-Native Framework for Scalable Weather Radar Archives](https://arxiv.org/abs/2510.24943)
*Alfonso Ladino-Rincon,Stephen W. Nesbitt*

Main category: cs.DC

TL;DR: Radar DataTree是首个将WMO FM-301标准从单次雷达体扫描扩展到时间序列、分析就绪存档的数据集级框架，解决了雷达数据碎片化、不兼容FAIR原则的问题。


<details>
  <summary>Details</summary>
Motivation: 天气雷达数据是科学价值最高但结构利用不足的地球观测数据集之一。尽管公开可用，但雷达存档仍存在碎片化、供应商特定、不符合FAIR原则等问题，阻碍了大规模研究、可重复性和云原生计算。

Method: 基于FM-301/CfRadial 2.1标准，使用xarray DataTree构建可扩展的开源架构，将雷达体扫描组织为分层、元数据丰富的结构，并序列化为Zarr格式。结合Icechunk实现ACID兼容存储和版本控制。

Result: 在准垂直剖面(QVP)和降水累积工作流等案例研究中展示了显著的性能提升，所有工具和数据集通过Raw2Zarr仓库公开发布。

Conclusion: 这项工作为雷达数据管理、高性能地球科学和AI就绪的天气基础设施提供了可重复和可扩展的基础。

Abstract: We introduce Radar DataTree, the first dataset-level framework that extends
the WMO FM-301 standard from individual radar volume scans to time-resolved,
analysis-ready archives. Weather radar data are among the most scientifically
valuable yet structurally underutilized Earth observation datasets. Despite
widespread public availability, radar archives remain fragmented,
vendor-specific, and poorly aligned with FAIR (Findable, Accessible,
Interoperable, Reusable) principles, hindering large-scale research,
reproducibility, and cloud-native computation. Radar DataTree addresses these
limitations with a scalable, open-source architecture that transforms
operational radar archives into FAIR-compliant, cloud-optimized datasets. Built
on the FM-301/CfRadial 2.1 standard and implemented using xarray DataTree,
Radar DataTree organizes radar volume scans as hierarchical, metadata-rich
structures and serializes them to Zarr for scalable analysis. Coupled with
Icechunk for ACID-compliant storage and versioning, this architecture enables
efficient, parallel computation across thousands of radar scans with minimal
preprocessing. We demonstrate significant performance gains in case studies
including Quasi-Vertical Profile (QVP) and precipitation accumulation
workflows, and release all tools and datasets openly via the Raw2Zarr
repository. This work contributes a reproducible and extensible foundation for
radar data stewardship, high-performance geoscience, and AI-ready weather
infrastructure.

</details>


### [4] [Multi-Resolution Model Fusion for Accelerating the Convolutional Neural Network Training](https://arxiv.org/abs/2510.25170)
*Kewei Wang,Claire Songhyun Lee,Sunwoo Lee,Vishu Gupta,Jan Balewski,Alex Sim,Peter Nugent,Ankit Agrawal,Alok Choudhary,Kesheng Wu,Wei-keng Liao*

Main category: cs.DC

TL;DR: 提出多分辨率模型融合方法，通过结合低分辨率训练的模型和原始分辨率微调，显著减少神经网络训练时间而不影响精度。


<details>
  <summary>Details</summary>
Motivation: 神经网络在科学研究中应用广泛，但训练高维大样本数据时计算成本高昂，需要高效的训练方法来降低计算开销。

Method: 多分辨率模型融合方法：先训练降分辨率数据的模型，然后与原始分辨率数据融合精炼，通过加速每个融合阶段的收敛来减少训练时间。

Result: 在CosmoFlow和Neuron Inverter两个实际科学应用中，训练时间分别减少47%和44%，模型精度保持不变。

Conclusion: 多分辨率模型融合方法能显著降低端到端训练时间，同时保持模型精度，确保最终模型既保留高分辨率信息又受益于低分辨率训练的计算效率。

Abstract: Neural networks are rapidly gaining popularity in scientific research, but
training the models is often very time-consuming. Particularly when the
training data samples are large high-dimensional arrays, efficient training
methodologies that can reduce the computational costs are crucial. To reduce
the training cost, we propose a Multi-Resolution Model Fusion (MRMF) method
that combines models trained on reduced-resolution data and then refined with
data in the original resolution. We demonstrate that these reduced-resolution
models and datasets could be generated quickly. More importantly, the proposed
approach reduces the training time by speeding up the model convergence in each
fusion stage before switching to the final stage of finetuning with data in its
original resolution. This strategy ensures the final model retains
high-resolution insights while benefiting from the computational efficiency of
lower-resolution training. Our experiment results demonstrate that the
multi-resolution model fusion method can significantly reduce end-to-end
training time while maintaining the same model accuracy. Evaluated using two
real-world scientific applications, CosmoFlow and Neuron Inverter, the proposed
method improves the training time by up to 47% and 44%, respectively, as
compared to the original resolution training, while the model accuracy is not
affected.

</details>


### [5] [MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale Expert Parallel Inference](https://arxiv.org/abs/2510.25258)
*Xinru Tang,Jingxiang Hou,Dingcheng Jiang,Taiquan Wei,Jiaxin Liu,Jinyi Deng,Huizheng Wang,Qize Yang,Haoran Shang,Chao Li,Yang Hu,Shouyi Yin*

Main category: cs.DC

TL;DR: 该论文提出ER-Mapping和NI-Balancer方法，在晶圆级芯片上优化混合专家模型的通信效率和专家迁移开销，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 混合专家模型依赖专家并行技术，但GPU集群中跨节点通信开销大。晶圆级芯片提供高性能网络但受限于网格拓扑，存在通信压力不均衡和专家迁移开销高的问题。

Method: 提出ER-Mapping方法共同设计注意力层和MoE层的映射以平衡通信压力；提出NI-Balancer将完整专家迁移分解为多步骤，交替利用两层的冷链接来隐藏迁移开销。

Result: ER-Mapping实现高达62%的通信减少；NI-Balancer在MoE计算和通信方面分别带来54%和22%的改进；相比NVL72超级节点，晶圆级芯片平台平均提供39%更高的每设备MoE性能。

Conclusion: 通过ER-Mapping和NI-Balancer的协同设计，晶圆级芯片平台能够充分发挥其大规模专家并行的潜力，显著提升混合专家模型的性能。

Abstract: As large language models (LLMs) continue to scale up, mixture-of-experts
(MoE) has become a common technology in SOTA models. MoE models rely on expert
parallelism (EP) to alleviate memory bottleneck, which introduces all-to-all
communication to dispatch and combine tokens across devices. However, in
widely-adopted GPU clusters, high-overhead cross-node communication makes
all-to-all expensive, hindering the adoption of EP. Recently, wafer-scale chips
(WSCs) have emerged as a platform integrating numerous devices on a wafer-sized
interposer. WSCs provide a unified high-performance network connecting all
devices, presenting a promising potential for hosting MoE models. Yet, their
network is restricted to a mesh topology, causing imbalanced communication
pressure and performance loss. Moreover, the lack of on-wafer disk leads to
high-overhead expert migration on the critical path.
  To fully unleash this potential, we first propose Entwined Ring Mapping
(ER-Mapping), which co-designs the mapping of attention and MoE layers to
balance communication pressure and achieve better performance. We find that
under ER-Mapping, the distribution of cold and hot links in the attention and
MoE layers is complementary. Therefore, to hide the migration overhead, we
propose the Non-invasive Balancer (NI-Balancer), which splits a complete expert
migration into multiple steps and alternately utilizes the cold links of both
layers. Evaluation shows ER-Mapping achieves communication reduction up to 62%.
NI-Balancer further delivers 54% and 22% improvements in MoE computation and
communication, respectively. Compared with the SOTA NVL72 supernode, the WSC
platform delivers an average 39% higher per-device MoE performance owing to its
scalability to larger EP.

</details>


### [6] [A Privacy-Preserving Ecosystem for Developing Machine Learning Algorithms Using Patient Data: Insights from the TUM.ai Makeathon](https://arxiv.org/abs/2510.25277)
*Simon Süwer,Mai Khanh Mai,Christoph Klein,Nicola Götzenberger,Denis Dalić,Andreas Maier,Jan Baumbach*

Main category: cs.DC

TL;DR: 提出了一种多阶段的安全AI训练方法，通过模拟临床知识图谱设计模型，在联邦学习框架中训练，并在医院环境中进行实际训练，仅返回聚合性能指标，实现隐私保护的医疗AI开发。


<details>
  <summary>Details</summary>
Motivation: 临床数据整合对个性化医疗发展具有巨大潜力，但受GDPR严格限制，特别是对于罕见疾病的小型队列。高质量结构化数据对预测性医疗AI开发至关重要。

Method: 四阶段方法：(1)在模拟临床知识图谱上设计模型；(2)在FeatureCloud联邦学习框架中集成模型；(3)在医院环境中对真实知识图谱进行训练；(4)执行验证评估脚本仅返回聚合性能指标。

Result: 在TUM.ai Makeathon 2024挑战中成功验证：50名学生无需访问真实数据即可开发患者分类和诊断模型。

Conclusion: 通过联邦学习框架部署安全算法是实现医疗保健领域隐私保护AI的实用方法。

Abstract: The integration of clinical data offers significant potential for the
development of personalized medicine. However, its use is severely restricted
by the General Data Protection Regulation (GDPR), especially for small cohorts
with rare diseases. High-quality, structured data is essential for the
development of predictive medical AI. In this case study, we propose a novel,
multi-stage approach to secure AI training: (1) The model is designed on a
simulated clinical knowledge graph (cKG). This graph is used exclusively to
represent the structural characteristics of the real cKG without revealing any
sensitive content. (2) The model is then integrated into the FeatureCloud (FC)
federated learning framework, where it is prepared in a single-client
configuration within a protected execution environment. (3) Training then takes
place within the hospital environment on the real cKG, either under the direct
supervision of hospital staff or via a fully automated pipeline controlled by
the hospital. (4) Finally, verified evaluation scripts are executed, which only
return aggregated performance metrics. This enables immediate performance
feedback without sensitive patient data or individual predictions, leaving the
clinic. A fundamental element of this approach involves the incorporation of a
cKG, which serves to organize multi-omics and patient data within the context
of real-world hospital environments. This approach was successfully validated
during the TUM.ai Makeathon 2024 (TUMaiM24) challenge set by the Dr. von Hauner
Children's Hospital (HCH-LMU): 50 students developed models for patient
classification and diagnosis without access to real data. Deploying secure
algorithms via federated frameworks, such as the FC framework, could be a
practical way of achieving privacy-preserving AI in healthcare.

</details>


### [7] [Scheduling Data-Intensive Workloads in Large-Scale Distributed Systems: Trends and Challenges](https://arxiv.org/abs/2510.25362)
*Georgios L. Stavrinides,Helen D. Karatza*

Main category: cs.DC

TL;DR: 本章提出了数据密集型工作负载的分类，并概述了在大规模分布式系统中调度这些工作负载的常用方法，同时介绍了文献中的新策略并指出了开放挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大数据的爆炸式增长，工作负载变得更加复杂和计算密集，这些应用在分布式互连资源上处理，这些资源在规模和计算能力上变得越来越大。数据密集型应用具有不同的并行度，必须有效利用数据局部性，并可能施加多种服务质量要求，如时间约束和容错性，以及其他目标，如能效。这些工作负载的特性以及处理它们所需的计算资源的固有特性带来了重大挑战，需要采用有效的调度技术。

Method: 提出数据密集型工作负载的分类，并概述在大规模分布式系统中调度这些工作负载的常用方法。介绍文献中提出的新策略。

Result: 提供了数据密集型工作负载的分类框架和调度方法概述，识别了当前的研究进展和可用的调度策略。

Conclusion: 本章阐明了数据密集型工作负载调度领域的开放挑战和未来研究方向，强调了有效调度技术在处理复杂大数据应用中的重要性。

Abstract: With the explosive growth of big data, workloads tend to get more complex and
computationally demanding. Such applications are processed on distributed
interconnected resources that are becoming larger in scale and computational
capacity. Data-intensive applications may have different degrees of parallelism
and must effectively exploit data locality. Furthermore, they may impose
several Quality of Service requirements, such as time constraints and
resilience against failures, as well as other objectives, like energy
efficiency. These features of the workloads, as well as the inherent
characteristics of the computing resources required to process them, present
major challenges that require the employment of effective scheduling
techniques. In this chapter, a classification of data-intensive workloads is
proposed and an overview of the most commonly used approaches for their
scheduling in large-scale distributed systems is given. We present novel
strategies that have been proposed in the literature and shed light on open
challenges and future directions.

</details>


### [8] [Can Like Attract Like? A Study of Homonymous Gathering in Networks](https://arxiv.org/abs/2510.25451)
*Stéphane Devismes,Yoann Dieudonné,Arnaud Labourel*

Main category: cs.DC

TL;DR: 本文研究了移动智能体在分布式网络中的聚集问题，特别关注当智能体可能共享相同标签时的确定性聚集算法。给出了可聚集团队的完整特征描述，设计了多项式时间复杂度的算法，并证明了所需共享信息量的最优性。


<details>
  <summary>Details</summary>
Motivation: 传统确定性聚集算法假设智能体具有互不相同的标签来打破对称性。本文研究当智能体可能共享相同标签时，是否仍能保证确定性聚集，以及需要多少共享信息才能实现高效聚集。

Method: 首先完全刻画了可聚集团队的特征。然后设计了一个算法，在多项式时间内聚集所有可聚集团队，仅需O(log log log μ)比特的初始共享知识，其中μ是标签的最大重复次数。

Result: 获得了可聚集团队的完整特征描述；设计了poly(n,logλ)时间复杂度的聚集算法；证明了所需共享信息量的几乎最优性；得到了首个无需共享知识即可在多项式时间内聚集任意规模不同标签团队的确定性算法。

Conclusion: 即使智能体共享相同标签，只要满足特定条件，仍能实现确定性聚集。所需共享信息量极小且几乎最优，解决了终止检测这一主要挑战，技术方法具有独立价值。

Abstract: A team of mobile agents, starting from distinct nodes of a network, have to
meet at the same node and declare that they all met. Agents execute the same
algorithm, which they start when activated by an adversary or by an agent
entering their initial node. When activated, agents traverse edges of the
network in synchronous rounds. Their perception and communication are strictly
local. This task, known as gathering, is a central problem in distributed
mobile systems. Most prior work focuses on minimizing its time complexity,
i.e., the worst-case number of rounds between the start of the earliest agent
and the task completion. To break possible symmetries, deterministic solutions
typically assume that agents have pairwise distinct IDs, called labels, known
only to themselves. But must all labels be pairwise distinct to guarantee
deterministic gathering?
  We address this question by considering agents that may share the same label.
A team L is said to be gatherable if, for every initial setting of L, there is
an algorithm that solves gathering. Our contribution is threefold. (1) We give
a full characterization of the gatherable teams. (2) We design an algorithm
that gathers all of them in poly$(n,\log\lambda)$ time, where $n$ (resp.
$\lambda$) is the graph order (resp. the smallest label in L). This algorithm
requires the agents to initially share only $O(\log \log \log \mu)$ bits of
common knowledge, where $\mu$ is the largest label multiplicity in L. (3) We
show this dependency is almost optimal to get a poly$(n,\log\lambda)$-time
complexity.
  As a by-product, we get the first deterministic poly$(n,\log\lambda)$-time
algorithm requiring no common knowledge to gather any team when all labels are
distinct. Known to be achievable for two-agent teams, extending this to any
team size faced a major challenge: termination detection. Our techniques to
address it may be of independent interest.

</details>


### [9] [Holon Streaming: Global Aggregations with Windowed CRDTs](https://arxiv.org/abs/2510.25757)
*Jonas Spenger,Kolya Krafeld,Ruben van Gemeren,Philipp Haller,Paris Carbone*

Main category: cs.DC

TL;DR: Holon Streaming是一个支持精确一次语义的流处理系统，通过窗口化无冲突复制数据类型（Windowed CRDTs）实现可扩展的全局聚合计算，相比现有系统具有更低延迟和更高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有流处理系统在全局聚合计算时存在可扩展性瓶颈，要么在单个任务实例中计算，要么使用静态聚合树，导致延迟高且故障恢复时延迟峰值大。

Method: 提出确定性编程模型，使用窗口化无冲突复制数据类型（Windowed CRDTs）作为共享复制状态的新抽象，支持去中心化协调的高效故障恢复算法。

Result: 在全局聚合工作负载上，相比现有流处理系统，延迟降低5倍，吞吐量提高2倍，故障场景下延迟减少11倍。

Conclusion: 证明了确定性去中心化协调的有效性，以及Windowed CRDTs在全局聚合计算中的实用性。

Abstract: Scaling global aggregations is a challenge for exactly-once stream processing
systems. Current systems implement these either by computing the aggregation in
a single task instance, or by static aggregation trees, which limits
scalability and may become a bottleneck. Moreover, the end-to-end latency is
determined by the slowest path in the tree, and failures and reconfiguration
cause large latency spikes due to the centralized coordination. Towards these
issues, we present Holon Streaming, an exactly-once stream processing system
for global aggregations. Its deterministic programming model uses windowed
conflict-free replicated data types (Windowed CRDTs), a novel abstraction for
shared replicated state. Windowed CRDTs make computing global aggregations
scalable. Furthermore, their guarantees such as determinism and convergence
enable the design of efficient failure recovery algorithms by decentralized
coordination. Our evaluation shows a 5x lower latency and 2x higher throughput
than an existing stream processing system on global aggregation workloads, with
an 11x latency reduction under failure scenarios. The paper demonstrates the
effectiveness of decentralized coordination with determinism, and the utility
of Windowed CRDTs for global aggregations.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [DIRC-RAG: Accelerating Edge RAG with Robust High-Density and High-Loading-Bandwidth Digital In-ReRAM Computation](https://arxiv.org/abs/2510.25278)
*Kunming Shao,Zhipeng Liao,Jiangnan Yu,Liang Zhao,Qiwei Li,Xijie Huang,Jingyu He,Fengshi Tian,Yi Zou,Xiaomeng Wang,Tim Kwang-Ting Cheng,Chi-Ying Tsui*

Main category: cs.AR

TL;DR: DIRCRAG是一种基于数字In-ReRAM计算的新型边缘RAG加速架构，通过集成高密度多级ReRAM子阵列和SRAM单元，实现超低功耗、单周期数据加载，显著降低能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上RAG系统面临的高存储、高能耗和高延迟问题，同时克服现有CIM技术内存密度低或计算精度有限的问题。

Method: 采用数字In-ReRAM计算(DIRC)架构，集成ReRAM和SRAM，支持查询静止数据流，通过位级空间误差分布提取和针对性数据重映射进行误差优化，并实现误差检测电路。

Result: 在TSMC40nm工艺下，DIRC-RAG实现5.18Mb/mm²的片上非易失性内存密度和131 TOPS的吞吐量，4MB检索延迟为5.6μs/查询，能耗为0.956μJ/查询，同时保持检索精度。

Conclusion: DIRCRAG架构有效解决了边缘RAG系统的性能瓶颈，为边缘设备上的高效知识检索提供了可行的硬件解决方案。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating external knowledge retrieval but faces challenges on edge devices
due to high storage, energy, and latency demands. Computing-in-Memory (CIM)
offers a promising solution by storing document embeddings in CIM macros and
enabling in-situ parallel retrievals but is constrained by either low memory
density or limited computational accuracy. To address these challenges, we
present DIRCRAG, a novel edge RAG acceleration architecture leveraging Digital
In-ReRAM Computation (DIRC). DIRC integrates a high-density multi-level ReRAM
subarray with an SRAM cell, utilizing SRAM and differential sensing for robust
ReRAM readout and digital multiply-accumulate (MAC) operations. By storing all
document embeddings within the CIM macro, DIRC achieves ultra-low-power,
single-cycle data loading, substantially reducing both energy consumption and
latency compared to offchip DRAM. A query-stationary (QS) dataflow is supported
for RAG tasks, minimizing on-chip data movement and reducing SRAM buffer
requirements. We introduce error optimization for the DIRC ReRAM-SRAM cell by
extracting the bit-wise spatial error distribution of the ReRAM subarray and
applying targeted bit-wise data remapping. An error detection circuit is also
implemented to enhance readout resilience against deviceand circuit-level
variations. Simulation results demonstrate that DIRC-RAG under TSMC40nm process
achieves an on-chip non-volatile memory density of 5.18Mb/mm2 and a throughput
of 131 TOPS. It delivers a 4MB retrieval latency of 5.6{\mu}s/query and an
energy consumption of 0.956{\mu}J/query, while maintaining the retrieval
precision.

</details>
