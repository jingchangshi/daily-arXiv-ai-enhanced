<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Imperative Quantum Programming with Ownership and Borrowing in Guppy](https://arxiv.org/abs/2510.13082)
*Mark Koch,Agustín Borgna,Craig Roy,Alan Lawrence,Kartik Singhal,Seyon Sivarajah,Ross Duncan*

Main category: cs.PL

TL;DR: 开发量子类型系统，将线性类型与命令式语义结合，确保安全保证，已在Guppy编程语言中实现


<details>
  <summary>Details</summary>
Motivation: 线性类型在函数式量子编程中强制执行无克隆和无删除定理，但在命令式量子编程中尚未广泛应用

Method: 开发结合人体工程学线性类型和命令式语义的量子类型系统

Result: 所有想法已在Quantinuum的Guppy编程语言中实现

Conclusion: 成功创建了安全且实用的量子类型系统，适用于命令式量子编程

Abstract: Linear types enforce no-cloning and no-deleting theorems in functional
quantum programming. However, in imperative quantum programming, they have not
gained widespread adoption. This work aims to develop a quantum type system
that combines ergonomic linear typing with imperative semantics and maintains
safety guarantees. All ideas presented here have been implemented in
Quantinuum's Guppy programming language.

</details>


### [2] [Extensibility in Programming Languages: An overview](https://arxiv.org/abs/2510.13236)
*Sebastian mateos Nicolajsen*

Main category: cs.PL

TL;DR: 本文探讨编程语言可扩展性，通过文献综述识别宏、模块、类型和反射等关键主题，分析跨主题特性如参数化和一等公民行为，旨在启发未来语言设计者重视可扩展性。


<details>
  <summary>Details</summary>
Motivation: 作者在探索编程语言时缺乏对可扩展性组件的系统概述，因此希望提供一个入门指南，填补这一知识空白。

Method: 通过文献综述方法，识别和分析编程语言可扩展性的关键主题和跨主题特性。

Result: 识别出宏、模块、类型和反射四个关键可扩展性主题，以及参数化、一等公民行为等跨主题特性，强调了可定制性和灵活性的重要性。

Conclusion: 本文旨在启发未来编程语言设计者批判性地评估和考虑其设计的可扩展性，促进更灵活、可定制的语言构造。

Abstract: I here conduct an exploration of programming language extensibility, making
an argument for an often overlooked component of conventional language design.
Now, this is not a technical detailing of these components, rather, I attempt
to provide an overview as I myself have lacked during my time investigating
programming languages. Thus, read this as an introduction to the magical world
of extensibility. Through a literature review, I identify key extensibility
themes - Macros, Modules, Types, and Reflection - highlighting diverse
strategies for fostering extensibility. The analysis extends to cross-theme
properties such as Parametricism and First-class citizen behaviour, introducing
layers of complexity by highlighting the importance of customizability and
flexibility in programming language constructs. By outlining these facets of
existing programming languages and research, I aim to inspire future language
designers to assess and consider the extensibility of their creations
critically.

</details>


### [3] [Fast Trigonometric Functions using the RLIBM Approach](https://arxiv.org/abs/2510.13426)
*Sehyeok Park,Santosh Nagarakatte*

Main category: cs.PL

TL;DR: 开发针对三角函数的正确舍入多项式逼近方法，使用RLIBM方法处理多种表示形式和舍入模式，重点解决基于π的范围缩减中的精度问题。


<details>
  <summary>Details</summary>
Motivation: 三角函数计算中，基于π的范围缩减会放大π值的舍入误差，导致错误结果，需要开发能保持高精度π值的快速范围缩减技术。

Method: 使用RLIBM方法开发多项式逼近，实现快速范围缩减技术，通过浮点和整数计算保持π值的高精度，支持多种表示形式和舍入模式。

Result: 实现了快速的三角函数实现，能够为所有32位浮点输入产生正确舍入结果，单一实现支持多种表示形式。

Conclusion: 提出的方法成功解决了三角函数范围缩减中的精度问题，实现了高效且精确的三角函数计算，适用于多种32位浮点表示。

Abstract: This paper describes our experience developing polynomial approximations for
trigonometric functions that produce correctly rounded results for multiple
representations and rounding modes using the RLIBM approach. A key challenge
with trigonometric functions concerns range reduction with "pi", which reduces
a given input in the domain of a 32-bit float to a small domain. Any rounding
error in the value of "pi" is amplified during range reduction, which can
result in wrong results. We describe our experience implementing fast range
reduction techniques that maintain a large number of bits of "pi" both with
floating-point and integer computations. The resulting implementations for
trigonometric functions are fast and produce correctly rounded results for all
inputs for multiple representations up to 32-bits with a single implementation.

</details>


### [4] [A Complementary Approach to Incorrectness Typing](https://arxiv.org/abs/2510.13725)
*Celia Mengyue Li,Sophie Pull,Steven Ramsay*

Main category: cs.PL

TL;DR: 提出了一种新的双面类型系统，用于验证带有原子和模式匹配的函数式程序的正确性和错误性。核心思想是类型应该覆盖范式集合而非值集合，这允许定义类型上的补集运算符作为类型公式的否定。


<details>
  <summary>Details</summary>
Motivation: 需要验证函数式程序的正确性和错误性，特别是Erlang类程序的错误情况。传统类型系统难以有效处理程序错误验证。

Method: 使用基于范式集合的类型系统，引入补集运算符作为类型否定，通过子类型化的可判定公理化来表达补集操作，建立双面类型验证框架。

Result: 系统能够推导广泛的否定原则，包括类型理论中的共蕴含类似物，并成功验证多个Erlang类程序的错误情况。补集运算符的公理化被证明是可判定的，整个类型系统对范式是完备的。

Conclusion: 该双面类型系统不仅能验证程序正确性，还能有效验证程序错误性，为函数式程序的全面验证提供了新方法。

Abstract: We introduce a new two-sided type system for verifying the correctness and
incorrectness of functional programs with atoms and pattern matching. A key
idea in the work is that types should range over sets of normal forms, rather
than sets of values, and this allows us to define a complement operator on
types that acts as a negation on typing formulas. We show that the complement
allows us to derive a wide range of refutation principles within the system,
including the type-theoretic analogue of co-implication, and we use them to
certify that a number of Erlang-like programs go wrong. An expressive
axiomatisation of the complement operator via subtyping is shown decidable, and
the type system as a whole is shown to be not only sound, but also complete for
normal forms.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching for Heterogeneous Tasks and Clusters](https://arxiv.org/abs/2510.12889)
*Wei Da,Evangelia Kalyvianaki*

Main category: cs.DC

TL;DR: Dodoor是一种高效的随机化去中心化调度器，通过批量更新缓存服务器信息和新型负载评分机制，在现代数据中心任务调度中显著减少通信开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心需要高效的任务调度系统，但现有去中心化调度器依赖实时探测远程服务器，导致通信开销大。Dodoor旨在通过缓存服务器信息和批量更新来减少通信成本。

Method: 基于加权球-箱模型的b-batch设置，使用缓存服务器信息而非实时探测，采用新型负载评分机制衡量服务器与任务的反亲和性，替代传统的待处理任务计数方法。

Result: 在101节点异构集群上测试：调度消息减少55-66%，吞吐量提升最高33.2%，平均完成时间减少12.1%，尾部延迟改善24.6%。

Conclusion: Dodoor通过减少通信开销和优化负载平衡，在异构集群中实现了显著性能提升，证明了其在大规模数据中心任务调度中的有效性。

Abstract: This paper introduces Dodoor, an efficient randomized decentralized scheduler
designed for task scheduling in modern data centers. Dodoor leverages advanced
research on the weighted balls-into-bins model with b-batched setting. Unlike
other decentralized schedulers that rely on real-time probing of remote
servers, Dodoor makes scheduling decisions based on cached server information,
which is updated in batches, to reduce communication overheads. To schedule
tasks with dynamic, multidimensional resource requirements in heterogeneous
cluster, Dodoor uses a novel load score to measure servers' loads for each
scheduled task. This score captures the anti-affinity between servers and tasks
in contrast to the commonly used heuristic of counting pending tasks to balance
load. On a 101-node heterogeneous cluster, Dodoor is evaluated using two
workloads: (i) simulated Azure virtual machines placements and (ii) real
serverless Python functions executions in Docker. The evaluation shows that
Dodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can
also increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency
by 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two
workloads.

</details>


### [6] [Scrutiny new framework in integrated distributed reliable systems](https://arxiv.org/abs/2510.13203)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: 提出了一个新的集成分布式系统框架FDIRS，通过使用异构分布式数据库技术来提高系统性能、响应速度和可靠性，解决了之前框架的一些问题。


<details>
  <summary>Details</summary>
Motivation: 现有的集成分布式系统框架在性能、效率和可靠性方面存在不足，需要开发新的框架来改进这些方面。

Method: 提出了FDIRS框架，采用异构分布式数据库技术，通过三个组成部分来提升系统满意度和性能。

Result: 仿真结果显示，FDIRS框架在效率、性能和可靠性方面都有显著提升，响应速度得到改善。

Conclusion: FDIRS框架成功提高了集成系统的效率、性能和可靠性，解决了之前框架的一些问题。

Abstract: In this paper we represent a new framework for integrated distributed
systems. In the proposed framework we have used three parts to increase
Satisfaction and Performance of this framework. At first we analyse integrated
systems and their evolution process and also ERPSD and ERPDRT framework briefly
then we explain the new FDIRS framework. Finally we compare the results of
simulation of the new framework with presented frameworks. Result showed In
FIDRS framework, the technique of heterogeneous distributed data base is used
to improve Performance and speed in responding to users. Finally by using FDIRS
framework we succeeded to increase Efficiency, Performance and reliability of
integrated systems and remove some of previous frameworks problems.

</details>


### [7] [BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing Disaggregated LLM Serving in AI Infrastructure](https://arxiv.org/abs/2510.13223)
*Yiyuan He,Minxian Xu,Jingfeng Wu,Jianmin Hu,Chong Ma,Min Shen,Le Chen,Chengzhong Xu,Lin Qu,Kejiang Ye*

Main category: cs.DC

TL;DR: BanaServe是一个动态编排框架，通过层级权重迁移、注意力级KV缓存迁移和全局KV缓存共享，解决解耦LLM服务中的资源分配不平衡、负载不均和缓存热点问题，显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前解耦LLM服务系统面临三个关键限制：静态资源分配无法适应动态工作负载、预填充和解码阶段固有的负载不平衡、以及前缀缓存感知路由导致的负载分布倾斜，这些问题导致资源浪费或违反SLO。

Method: 引入层级权重迁移、注意力级KV缓存迁移和全局KV缓存共享，支持粗粒度（层级）和细粒度（注意力级）负载重分配，通过层间重叠传输最小化延迟开销，使路由器能够进行纯负载感知调度。

Result: 相比vLLM，BanaServe实现1.2x-3.9x更高的吞吐量，总处理时间降低3.9%-78.4%；相比DistServe，吞吐量提升1.1x-2.8x，延迟降低1.4%-70.1%。

Conclusion: BanaServe通过动态资源编排和缓存管理机制，有效解决了解耦LLM服务中的资源效率和负载平衡问题，显著提升了系统性能。

Abstract: Large language models (LLMs) are increasingly deployed in AI infrastructure,
driving the need for high throughput, resource efficient serving systems.
Disaggregated LLM serving, which separates prompt prefill from auto-regressive
decode, has emerged as a promising architecture by isolating their
heterogeneous compute and memory demands. However, current disaggregated
systems face three key limitations: (i) static resource allocation cannot adapt
to highly dynamic workloads, causing over-provisioning that wastes resources or
under-provisioning that violates service level objectives (SLOs); (ii) inherent
load imbalance between prefill and decode stages, where prefill is
compute-bound and decode is memory-bound, causes under-utilization in one tier
while the other becomes a bottleneck; and (iii) prefix cache aware routing
skews load distribution, as high cache hit rate prefill nodes attract
disproportionately more requests, further degrading balance and efficiency. To
address these issues, we present BanaServe, a dynamic orchestration framework
that continuously rebalances computational and memory resources across prefill
and decode instances while eliminating hotspots induced by cache. BanaServe
introduces layer level weight migration, attention level Key Value Cache (KV
Cache) migration, and Global KV Cache Store sharing with layer wise overlapped
transmission, enabling both coarse grained (layer level) and fine grained
(attention level) load redistribution with minimal latency overhead. These
mechanisms allow routers to perform purely load aware scheduling, unconstrained
by cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher
throughput with 3.9%-78.4% lower total processing time, and outperforms
DistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.

</details>


### [8] [Distributed Reductions for the Maximum Weight Independent Set Problem](https://arxiv.org/abs/2510.13306)
*Jannick Borowitz,Ernestine Großmann,Mattthias Schimek*

Main category: cs.DC

TL;DR: 本文提出了首个分布式内存并行缩减算法用于求解最大权重独立集问题，实现了在大规模图上的高效处理，相比顺序算法获得了显著加速。


<details>
  <summary>Details</summary>
Motivation: 最大权重独立集问题是一个重要的NP难优化问题，现有方法主要使用数据缩减规则来求解，但缺乏分布式并行算法来处理超大规模图。

Method: 开发了分布式内存并行缩减算法、分布式reduce-and-greedy和reduce-and-peel启发式算法，支持异步处理。

Result: 在1024个处理器上实验显示良好可扩展性，reduce-and-peel方法平均加速33倍，reduce-and-greedy方法平均加速50倍，能处理超过10亿顶点和170亿边的图。

Conclusion: 分布式并行方法显著提升了最大权重独立集问题的求解效率，能够处理超大规模图实例，为实际应用提供了有效解决方案。

Abstract: Finding maximum-weight independent sets in graphs is an important NP-hard
optimization problem. Given a vertex-weighted graph $G$, the task is to find a
subset of pairwise non-adjacent vertices of $G$ with maximum weight. Most
recently published practical exact algorithms and heuristics for this problem
use a variety of data-reduction rules to compute (near-)optimal solutions.
Applying these rules results in an equivalent instance of reduced size. An
optimal solution to the reduced instance can be easily used to construct an
optimal solution for the original input.
  In this work, we present the first distributed-memory parallel reduction
algorithms for this problem, targeting graphs beyond the scale of previous
sequential approaches. Furthermore, we propose the first distributed
reduce-and-greedy and reduce-and-peel algorithms for finding a maximum weight
independent set heuristically.
  In our practical evaluation, our experiments on up to $1024$ processors
demonstrate good scalability of our distributed reduce algorithms while
maintaining good reduction impact. Our asynchronous reduce-and-peel approach
achieves an average speedup of $33\times$ over a sequential state-of-the-art
reduce-and-peel approach on 36 real-world graphs with a solution quality close
to the sequential algorithm. Our reduce-and-greedy algorithms even achieve
average speedups of up to $50\times$ at the cost of a lower solution quality.
Moreover, our distributed approach allows us to consider graphs with more than
one billion vertices and 17 billion edges.

</details>


### [9] [Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices](https://arxiv.org/abs/2510.13447)
*Julian Legler,Sebastian Werner,Maria C. Borges,Stefan Tai*

Main category: cs.DC

TL;DR: 提出服务级能耗模型，考虑微服务跨容器交互中的网络和存储能耗，实验显示忽略这些因素会导致辅助服务能耗低估达63%。


<details>
  <summary>Details</summary>
Motivation: 微服务架构虽提供灵活性和可扩展性，但增加了云资源需求，导致更高能耗和碳排放。现有研究主要关注容器级CPU和内存能耗，忽略了跨容器服务交互（特别是网络和存储）的能耗影响。

Method: 引入服务级能耗模型，捕捉微服务在容器间分布式执行的特性，并开发实验工具测量CPU、内存、网络和存储组件的能耗。

Result: 实验验证显示，忽略网络和存储会导致辅助服务能耗低估高达63%，表明需要更全面的能耗评估。

Conclusion: 微服务架构的能耗评估必须包含网络和存储组件，以准确设计能效优化的系统。

Abstract: Microservice architectures have become the dominant paradigm for cloud-native
systems, offering flexibility and scalability. However, this shift has also led
to increased demand for cloud resources, contributing to higher energy
consumption and carbon emissions. While existing research has focused on
measuring fine-grained energy usage of CPU and memory at the container level,
or on system-wide assessments, these approaches often overlook the energy
impact of cross-container service interactions, especially those involving
network and storage for auxiliary services such as observability and system
monitoring. To address this gap, we introduce a service-level energy model that
captures the distributed nature of microservice execution across containers.
Our model is supported by an experimentation tool that accounts for energy
consumption not just in CPU and memory, but also in network and storage
components. We validate our approach through extensive experimentation with
diverse experiment configurations of auxiliary services for a popular
open-source cloud-native microservice application. Results show that omitting
network and storage can lead to an underestimation of auxiliary service energy
use by up to 63%, highlighting the need for more comprehensive energy
assessments in the design of energy-efficient microservice architectures.

</details>


### [10] [Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference](https://arxiv.org/abs/2510.13668)
*Zhibin Wang,Zetao Hong,Xue Li,Zibo Wang,Shipeng Li,Qingkai Meng,Qing Wang,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: ARES是一个基于长度预测的自适应解码重调度系统，通过预测LLM推理中的剩余生成长度来解决工作负载不平衡问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中LLM推理的输出长度变化导致解码阶段工作负载严重不平衡，特别是在长输出推理任务中。现有系统使用静态预填充到解码调度，在动态解码工作负载下经常导致SLO违规和OOM故障。

Method: 提出轻量级连续的LLM原生预测方法，利用LLM隐藏状态建模剩余生成长度；在解码阶段实施重调度解决方案，包含动态平衡机制整合当前和预测工作负载。

Result: 预测精度显著提升（MAE降低49.42%），开销大幅减少（预测器参数削减93.28%）；性能表现优异（P99 TPOT降低74.77%，吞吐量提升最高达2.24倍）。

Conclusion: ARES系统通过自适应解码重调度和精确的长度预测，有效解决了LLM推理中的工作负载不平衡问题，实现了显著的性能改进。

Abstract: Large Language Model (LLM) inference has emerged as a fundamental paradigm.
In real-world scenarios, variations in output length cause severe workload
imbalance in the decode phase, particularly for long-output reasoning tasks.
Existing systems, such as PD disaggregation architectures, rely on static
prefill-to-decode scheduling, which often results in SLO violations and OOM
failures under evolving decode workloads.
  In this paper, we propose ARES, an adaptive decoding rescheduling system
powered by length prediction to anticipate future workloads. Our core
contributions include: (1) A lightweight and continuous LLM-native prediction
method that leverages LLM hidden state to model remaining generation length
with high precision (reducing MAE by 49.42%) and low overhead (cutting
predictor parameters by 93.28%); (2) A rescheduling solution in decode phase
with : A dynamic balancing mechanism that integrates current and predicted
workloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher
goodput.

</details>


### [11] [FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access](https://arxiv.org/abs/2510.13724)
*Aditya Tanikanti,Benoit Côté,Yanfei Guo,Le Chen,Nickolaus Saint,Ryan Chard,Ken Raffenetti,Rajeev Thakur,Thomas Uram,Ian Foster,Michael E. Papka,Venkatram Vishwanath*

Main category: cs.DC

TL;DR: FIRST是一个联邦推理资源调度工具包，可在分布式HPC集群上提供推理即服务，支持多种AI模型和推理后端，通过OpenAI兼容API实现并行推理工作负载。


<details>
  <summary>Details</summary>
Motivation: 满足科学工作流中对私有、安全、可扩展AI推理日益增长的需求，让研究人员能够在本地基础设施上生成数十亿token，而无需依赖商业云服务。

Method: 利用Globus Auth和Globus Compute，在现有HPC基础设施上构建集群无关的API，支持多种推理后端（如vLLM），自动扩展资源，维护"热"节点以实现低延迟执行。

Result: 实现了在私有安全环境中通过OpenAI兼容API运行并行推理工作负载，支持高吞吐量批处理和交互模式，能够在本地基础设施上生成数十亿token。

Conclusion: FIRST框架成功解决了科学工作流中对私有、安全、可扩展AI推理的需求，为研究人员提供了在现有HPC基础设施上访问多样化AI模型的云式体验。

Abstract: We present the Federated Inference Resource Scheduling Toolkit (FIRST), a
framework enabling Inference-as-a-Service across distributed High-Performance
Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI
models, like Large Language Models (LLMs), on existing HPC infrastructure.
Leveraging Globus Auth and Globus Compute, the system allows researchers to run
parallel inference workloads via an OpenAI-compliant API on private, secure
environments. This cluster-agnostic API allows requests to be distributed
across federated clusters, targeting numerous hosted models. FIRST supports
multiple inference backends (e.g., vLLM), auto-scales resources, maintains
"hot" nodes for low-latency execution, and offers both high-throughput batch
and interactive modes. The framework addresses the growing demand for private,
secure, and scalable AI inference in scientific workflows, allowing researchers
to generate billions of tokens daily on-premises without relying on commercial
cloud infrastructure.

</details>


### [12] [Tight Conditions for Binary-Output Tasks under Crashes](https://arxiv.org/abs/2510.13755)
*Timothé Albouy,Antonio Fernández Anta,Chryssis Georgiou,Nicolas Nicolaou,Junlang Wang*

Main category: cs.DC

TL;DR: 本文为分布式系统中具有二进制输出的任务提供了完整的可解性条件特征化，统一了多个分布式计算问题。


<details>
  <summary>Details</summary>
Motivation: 探索分布式系统中具有二进制输出任务的必要和充分系统条件，重点关注任务可以产生的不同输出值集合，而忽略有效性和值多重性。

Method: 采用输出集方法，考虑在n个进程中最多t个可能崩溃的分布式系统，分析同步和异步系统下所有二进制输出任务类的可解性条件。

Result: 提供了关于n和t的紧条件，在这些条件下每个具有二进制输出的任务类都是可解的，结果具有高度通用性。

Conclusion: 输出集方法产生了高度通用的结果，统一了多个分布式计算问题，并为更强的任务表述提供了不可能性证明。

Abstract: This paper explores necessary and sufficient system conditions to solve
distributed tasks with binary outputs (\textit{i.e.}, tasks with output values
in $\{0,1\}$). We focus on the distinct output sets of values a task can
produce (intentionally disregarding validity and value multiplicity),
considering that some processes may output no value. In a distributed system
with $n$ processes, of which up to $t \leq n$ can crash, we provide a complete
characterization of the tight conditions on $n$ and $t$ under which every class
of tasks with binary outputs is solvable, for both synchronous and asynchronous
systems. This output-set approach yields highly general results: it unifies
multiple distributed computing problems, such as binary consensus and symmetry
breaking, and it produces impossibility proofs that hold for stronger task
formulations, including those that consider validity, account for value
multiplicity, or move beyond binary outputs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [D-com: Accelerating Iterative Processing to Enable Low-rank Decomposition of Activations](https://arxiv.org/abs/2510.13147)
*Faraz Tahmasebi,Michael Pelluer,Hyoukjun Kwon*

Main category: cs.AR

TL;DR: 本文提出D-com加速器架构，通过渐进式分解算法和硬件协同设计，解决了大语言模型分解过程中的延迟问题，实现了22%的端到端延迟改进。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的计算和内存成本持续增长，传统权重分解方法导致运行时分解延迟过高，超过了分解带来的收益。

Method: 采用渐进式分解算法(Lanczos算法)，设计协同加速器架构，引入计算复制方法解决内存瓶颈，开发输出形状保持计算方案，并使用多轨分解方法处理异常通道。

Result: 评估显示6.2倍加速，相比A100 GPU实现22%端到端延迟改进，模型质量损失较小(如AI2推理任务中3%下降)。

Conclusion: 通过合适的分解算法选择和硬件支持，输入分解可以显著有益，D-com加速器在保持模型质量的同时大幅提升性能。

Abstract: The computation and memory costs of large language models kept increasing
over last decade, which reached over the scale of 1T parameters. To address the
challenges from the large scale models, model compression techniques such as
low-rank decomposition have been explored. Previous model decomposition works
have focused on weight decomposition to avoid costly runtime decomposition,
whose latency often significantly exceeds the benefits from decomposition
(e.g., 38% more end-to-end latency when running Llama2-7b on A100 with 4K
sequence length with activation decomposition compared to no decomposition). In
this work, we debunk such observations and report that the input decomposition
can be significantly beneficial with a proper choice of decomposition algorithm
and hardware support. We adopt progressive decomposition algorithm, Lanczos
algorithm, and design a co-accelerator architecture for the decomposition
algorithm. To address the memory- boundness of the decomposition operation, we
introduce a novel compute replication methodology that moves the op- eration
toward compute-bound region, which enables 6.2x speedup in our evaluation. We
also develop an output shape- preserving computation scheme that eliminates
decomposi- tion costs in consecutive layers. To compensate model quality loss
from compression, we introduce a multi-track decom- position approach that
separately handles outlier channels for high accuracy and low perplexity with
minimal compu- tational costs. Combined together, our accelerator, D-com,
provides 22% end-to-end latency improvements compared to A100 GPU at the cost
of small model quality degradation (e.g., 3% on AI2 Reasoning Challenge task).

</details>


### [14] [Energy-Efficient FPGA Framework for Non-Quantized Convolutional Neural Networks](https://arxiv.org/abs/2510.13362)
*Angelos Athanasiadis,Nikolaos Tampouratzis,Ioannis Papaefstathiou*

Main category: cs.AR

TL;DR: 提出一个基于Darknet的框架，在FPGA上实现全精度CNN计算，在保持神经网络精度的同时达到与量化方法相似的性能和能效


<details>
  <summary>Details</summary>
Motivation: AI应用中CNN实时处理需求增长，传统处理器在性能、功耗和延迟方面难以平衡，特别是在嵌入式系统和边缘计算平台中

Method: 基于Darknet框架，使用类似Darknet的输入格式，在包含CPU和FPGA的异构系统中高效实现CNN，保持所有神经网络参数的全精度

Result: 与支持量化的FPGA框架相比，该解决方案在保持NN精度的同时，实现了相似的性能和/或能效

Conclusion: FPGA结合全精度计算为CNN提供了有前景的替代方案，在嵌入式系统和边缘计算中平衡性能、功耗和精度要求

Abstract: The growing demand for real-time processing in artificial intelligence
applications, particularly those involving Convolutional Neural Networks
(CNNs), has highlighted the need for efficient computational solutions.
Conventional processors, very often, fall short in balancing performance, power
consumption, and latency, especially in embedded systems and edge computing
platforms. Field-Programmable Gate Arrays (FPGAs) offer a promising
alternative, combining high performance with energy efficiency and
reconfigurability. The presented framework addresses the complex and demanding
computations of CNNs on FPGAs maintaining full precision in all neural network
parameters. Specifically, our framework is based on Darknet which is very
widely used for the design of CNNs and allows the designer, by using a similar
input to that given to Darknet, to efficiently implement a CNN in a
heterogeneous system comprising of CPUs and FPGAs. When compared with the FPGA
frameworks that support quantization, our solution aims to offer similar
performance and/or energy efficiency without any degradation on the NN
accuracy.

</details>


### [15] [F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs](https://arxiv.org/abs/2510.13401)
*Jude Haris,José Cano*

Main category: cs.AR

TL;DR: 本文提出了一种灵活块浮点量化（F-BFQ）加速器，用于在边缘设备上高效加速BFP量化的大型语言模型推理，相比Arm NEON CPU执行平均减少1.4倍推理时间。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在边缘设备上的部署需求增加，BFP量化成为减少内存占用和计算需求的关键技术。但由于模型层间使用混合BFP量化，需要支持不同BFP变体的专用加速器而无需重新配置。

Method: 提出F-BFQ加速器设计，能够动态切换两种BFP量化变体并执行矩阵乘法操作，在AMD Kria板上部署实现。

Result: 在三个BFP量化LLM上测试，相比Arm NEON CPU执行平均减少1.4倍推理时间，达到5.2 tokens/秒（约3.9 words/秒）的性能。

Conclusion: F-BFQ加速器有效解决了混合BFP量化LLM的加速需求，为边缘设备上的LLM部署提供了高效的硬件解决方案。

Abstract: Large Language Models (LLMs) have become increasingly prominent for daily
tasks, from improving sound-totext translation to generating additional frames
for the latest video games. With the help of LLM inference frameworks, such as
llama.cpp, which support optimizations such as KV-caching and quantization, it
is now easier than ever to deploy LLMs on edge devices. Quantization is
fundamental to enable LLMs on resource-constrained edge devices, and llama.cpp
utilizes block floating point (BFP) quantization to drastically reduce the bit
width of weights and input tensors, the memory footprint, and the computational
power required to run LLMs. LLMs are typically quantized with mixed BFP
quantization across the model layers to reduce the loss of model accuracy due
to quantization. Therefore, to efficiently accelerate across the layers of
BFP-quantized LLMs, specialized accelerators need to support different BFP
variants without reconfiguration. To address this issue, we propose a Flexible
Block FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically
switch between two BFP quantization variants and perform matrix multiplication
(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD
Kria board, reduces inference time by 1.4x on average over the Arm NEON-based
CPU execution across three BFP quantized LLMs while achieving 5.2 tokens per
second (~3.9 words per second).

</details>
