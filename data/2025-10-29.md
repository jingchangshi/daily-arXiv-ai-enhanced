<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [The SAP Cloud Infrastructure Dataset: A Reality Check of Scheduling and Placement of VMs in Cloud Computing](https://arxiv.org/abs/2510.23911)
*Arno Uhlig,Iris Braun,Matthias Wählisch*

Main category: cs.DC

TL;DR: 分析SAP云平台中虚拟机调度和放置问题，基于30天内1800个管理程序和48000个虚拟机的数据，发现资源调度存在多种次优情况，包括CPU资源争用、负载不均衡和资源过度配置等问题。


<details>
  <summary>Details</summary>
Motivation: 解决分布式环境中资源分配的基本挑战，特别是在企业级云平台中优化虚拟机调度和资源管理。

Method: 通过可观测性工具收集30天内1800个管理程序和48000个虚拟机的细粒度时间序列遥测数据，分析资源使用情况和性能指标。

Result: 发现CPU资源争用超过40%，CPU就绪时间高达220秒，计算主机负载严重不均衡（最高CPU利用率达99%），超过80%的虚拟机使用不到70%的分配资源。

Conclusion: 基于研究发现提出了新型放置和调度算法的设计要求，并为优化资源分配提供指导，同时公开数据集以支持未来大规模云基础设施调度方法的研究。

Abstract: Allocating resources in a distributed environment is a fundamental challenge.
In this paper, we analyze the scheduling and placement of virtual machines
(VMs) in the cloud platform of SAP, the world's largest enterprise resource
planning software vendor. Based on data from roughly 1,800 hypervisors and
48,000 VMs within a 30-day observation period, we highlight potential
improvements for workload management. The data was measured through
observability tooling that tracks resource usage and performance metrics across
the entire infrastructure. In contrast to existing datasets, ours uniquely
offers fine-grained time-series telemetry data of fully virtualized
enterprise-level workloads from both long-running and memory-intensive SAP
S/4HANA and diverse, general-purpose applications. Our key findings include
several suboptimal scheduling situations, such as CPU resource contention
exceeding 40%, CPU ready times of up to 220 seconds, significantly imbalanced
compute hosts with a maximum CPU~utilization on intra-building block hosts of
up to 99%, and overprovisioned CPU and memory resources resulting into over 80%
of VMs using less than 70% of the provided resources. Bolstered by these
findings, we derive requirements for the design and implementation of novel
placement and scheduling algorithms and provide guidance to optimize resource
allocations. We make the full dataset used in this study publicly available to
enable data-driven evaluations of scheduling approaches for large-scale cloud
infrastructures in future research.

</details>


### [2] [A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales](https://arxiv.org/abs/2510.23993)
*Anthony Carreon,Jagmohan Singh,Shivank Sharma,Shuzhi Zhang,Venkat Raman*

Main category: cs.DC

TL;DR: 开发了一个基于AMReX框架的高性能可压缩反应流求解器，针对多GPU环境优化，解决了GPU性能瓶颈，在燃烧应用中实现了2-5倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 高速化学反应流存在显著的计算挑战，现有GPU求解器在内存管理、负载均衡和化学反应局部性处理方面存在关键限制。

Method: 通过列优先存储优化内存访问模式，采用批量稀疏积分策略处理化学反应动力学计算负载变化，以及针对自适应网格细化的多GPU负载分配。

Result: 在1-96个NVIDIA H100 GPU上实现了2-5倍的性能提升和接近理想的弱扩展性，对流和化学反应的算术强度分别提高了约10倍和4倍。

Conclusion: 该求解器有效利用了GPU内存带宽和计算资源，为高速化学反应流的GPU加速计算提供了高效解决方案。

Abstract: High-speed chemically active flows present significant computational
challenges due to their disparate space and time scales, where stiff chemistry
often dominates simulation time. While modern supercomputing scientific codes
achieve exascale performance by leveraging graphics processing units (GPUs),
existing GPU-based compressible combustion solvers face critical limitations in
memory management, load balancing, and handling the highly localized nature of
chemical reactions. To this end, we present a high-performance compressible
reacting flow solver built on the AMReX framework and optimized for multi-GPU
settings. Our approach addresses three GPU performance bottlenecks: memory
access patterns through column-major storage optimization, computational
workload variability via a bulk-sparse integration strategy for chemical
kinetics, and multi-GPU load distribution for adaptive mesh refinement
applications. The solver adapts existing matrix-based chemical kinetics
formulations to multigrid contexts. Using representative combustion
applications including hydrogen-air detonations and jet in supersonic crossflow
configurations, we demonstrate $2-5\times$ performance improvements over
initial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA
H100 GPUs. Roofline analysis reveals substantial improvements in arithmetic
intensity for both convection ($\sim 10 \times$) and chemistry ($\sim 4
\times$) routines, confirming efficient utilization of GPU memory bandwidth and
computational resources.

</details>


### [3] [Towards Exascale Computing for Astrophysical Simulation Leveraging the Leonardo EuroHPC System](https://arxiv.org/abs/2510.24175)
*Nitin Shukla,Alessandro Romeo,Caterina Caravita,Michael Redenti,Radim Vavrik,Lubomir Riha,Andrea Mignone,Marco Rossazza,Stefano Truzzi,Luca Tornatore,Antonio Ragagnin,Tiago Castro,Geray S. Karademir,Klaus Dolag,Pranab J. Deka,Fabio Bacchini,Rostislav-Paul Wilhelm,Daniele Gregori,Elisabetta Boella*

Main category: cs.DC

TL;DR: SPACE中心通过协作优化天体物理、宇宙学和空间等离子体数值代码，在Leonardo系统上对三个旗舰代码进行性能分析，初步测试显示在1024个GPU上达到80%可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为现有和下一代加速器开发和重新设计天体物理、宇宙学和空间等离子体数值代码，以支持大规模模拟，满足百亿亿次计算时代的需求。

Method: 使用性能分析工具分析三个旗舰代码（gPLUTO、OpenGadget3和iPIC3D）在单节点和多节点上的性能表现。

Result: 所有三个代码都能高效扩展，在1024个GPU上达到80%的可扩展性。

Conclusion: SPACE中心的协作策略成功优化了代码性能，为百亿亿次计算时代的大规模模拟奠定了基础。

Abstract: Developing and redesigning astrophysical, cosmological, and space plasma
numerical codes for existing and next-generation accelerators is critical for
enabling large-scale simulations. To address these challenges, the SPACE Center
of Excellence (SPACE-CoE) fosters collaboration between scientists, code
developers, and high-performance computing experts to optimize applications for
the exascale era. This paper presents our strategy and initial results on the
Leonardo system at CINECA for three flagship codes, namely gPLUTO, OpenGadget3
and iPIC3D, using profiling tools to analyze performance on single and multiple
nodes. Preliminary tests show all three codes scale efficiently, reaching 80%
scalability up to 1,024 GPUs.

</details>


### [4] [CoMPSeT: A Framework for Comparing Multiparty Session Types](https://arxiv.org/abs/2510.24205)
*Telmo Ribeiro,José Proença,Mário Florido*

Main category: cs.DC

TL;DR: 提出了一个名为CoMPSeT的工具，用于分析和比较不同多会话类型(MPST)的特性，帮助研究人员和教师更好地理解全局编排协议。


<details>
  <summary>Details</summary>
Motivation: 并发系统设计复杂，现有的多会话类型(MPST)存在多种变体，各有特定功能和特性，需要一个工具来清晰展示和比较这些特性。

Method: 选择代表性MPST示例，提供机制来组合不同特性，并动画化比较具体示例的语义。工具开源，编译为JavaScript，可在浏览器中直接运行。

Result: 开发了CoMPSeT工具，能够展示不同MPST特性的组合效果，支持语义动画和比较。

Conclusion: CoMPSeT为研究人员理解MPST领域和教师讲解全局编排提供了实用工具，有助于更好地掌握多会话类型的不同特性。

Abstract: Concurrent systems are often complex and difficult to design. Choreographic
languages, such as Multiparty Session Types (MPST), allow the description of
global protocols of interactions by capturing valid patterns of interactions
between participants. Many variations of MPST exist, each one with its rather
specific features and idiosyncrasies. Here we propose a tool (CoMPSeT) that
provides clearer insights over different features in existing MPST. We select a
representative set of MPST examples and provide mechanisms to combine different
features and to animate and compare the semantics of concrete examples. CoMPSeT
is open-source, compiled into JavaScript, and can be directly executed from any
browser, becoming useful both for researchers who want to better understand the
landscape of MPST and for teachers who want to explain global choreographies.

</details>


### [5] [ARIMA_PLUS: Large-scale, Accurate, Automatic and Interpretable In-Database Time Series Forecasting and Anomaly Detection in Google BigQuery](https://arxiv.org/abs/2510.24452)
*Xi Cheng,Weijie Shen,Haoming Chen,Chaoyi Shen,Jean Ortega,Jiashang Liu,Steve Thomas,Honglin Zheng,Haoyun Wu,Yuxiang Li,Casey Lichtendahl,Jenny Ortiz,Gang Liu,Haiyang Qi,Omid Fatemieh,Chris Fry,Jing Jing Long*

Main category: cs.DC

TL;DR: ARIMA_PLUS是一个新颖的时间序列预测和异常检测框架，结合了准确可解释的模型与可扩展的云基础设施，在42个公共数据集上表现优于统计方法和神经网络模型。


<details>
  <summary>Details</summary>
Motivation: 解决大规模时间序列预测和异常检测的两个关键挑战：(1)高效准确地处理大量时间序列；(2)确保结果可解释性以融入业务洞察。

Method: 采用顺序模块化结构处理时间序列的不同组件（节假日效应、季节性、趋势、异常），每个模块都有新颖增强，并建立统一框架同时处理预测和异常检测任务。

Result: 在Monash预测库的42个公共数据集上表现优于统计方法（ETS、ARIMA、TBATS、Prophet）和神经网络模型（DeepAR、N-BEATS、PatchTST、TimeMixer）；在Google Cloud BigQuery中实现每秒18000+时间序列的吞吐量，1.5小时可预测1亿个时间序列。

Conclusion: ARIMA_PLUS通过结合准确可解释的模型与可扩展云基础设施，成功解决了大规模时间序列分析中的效率和可解释性问题，为行业实践提供了强大工具。

Abstract: Time series forecasting and anomaly detection are common tasks for
practitioners in industries such as retail, manufacturing, advertising and
energy. Two unique challenges stand out: (1) efficiently and accurately
forecasting time series or detecting anomalies in large volumes automatically;
and (2) ensuring interpretability of results to effectively incorporate
business insights. We present ARIMA_PLUS, a novel framework to overcome these
two challenges by a unique combination of (a) accurate and interpretable time
series models and (b) scalable and fully managed system infrastructure. The
model has a sequential and modular structure to handle different components of
the time series, including holiday effects, seasonality, trend, and anomalies,
which enables high interpretability of the results. Novel enhancements are made
to each module, and a unified framework is established to address both
forecasting and anomaly detection tasks simultaneously. In terms of accuracy,
its comprehensive benchmark on the 42 public datasets in the Monash forecasting
repository shows superior performance over not only well-established
statistical alternatives (such as ETS, ARIMA, TBATS, Prophet) but also newer
neural network models (such as DeepAR, N-BEATS, PatchTST, TimeMixer). In terms
of infrastructure, it is directly built into the query engine of BigQuery in
Google Cloud. It uses a simple SQL interface and automates tedious
technicalities such as data cleaning and model selection. It automatically
scales with managed cloud computational and storage resources, making it
possible to forecast 100 million time series using only 1.5 hours with a
throughput of more than 18000 time series per second. In terms of
interpretability, we present several case studies to demonstrate time series
insights it generates and customizability it offers.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [SlowPoke: Understanding and Detecting On-Chip Fail-Slow Failures in Many-Core Systems](https://arxiv.org/abs/2510.24112)
*Junchi Wu,Xinfei Wan,Zhuoran Li,Yuyang Jin,Guangyu Sun,Yun Liang,Diyu Zhou,Youwei Zhuo*

Main category: cs.AR

TL;DR: SlowPoke是一个轻量级、硬件感知的片上故障慢速检测框架，通过编译器插桩、实时跟踪压缩和拓扑感知排名算法，在多核架构中高效检测故障慢速问题。


<details>
  <summary>Details</summary>
Motivation: 多核架构对高性能计算至关重要，但故障慢速问题会严重削弱其性能。现有分布式系统方法因内存限制严格且无法跟踪硬件拓扑故障而不适用。

Method: 结合编译器插桩进行低开销监控，使用实时跟踪压缩在千字节内存内运行，并采用新颖的拓扑感知排名算法定位故障根本原因。

Result: 在代表性多核工作负载上评估显示，SlowPoke将检测跟踪的存储开销平均降低115.9倍，故障慢速检测准确率平均达86.77%，误报率为12.11%。

Conclusion: SlowPoke在不同多核架构上都能有效扩展，使其适用于大规模部署。

Abstract: Many-core architectures are essential for high-performance computing, but
their performance is undermined by widespread fail-slow failures. Detecting
such failures on-chip is challenging, as prior methods from distributed systems
are unsuitable due to strict memory limits and their inability to track
failures across the hardware topology. This paper introduces SlowPoke, a
lightweight, hardware-aware framework for practical on-chip fail-slow
detection. SlowPoke combines compiler-based instrumentation for low-overhead
monitoring, on-the-fly trace compression to operate within kilobytes of memory,
and a novel topology-aware ranking algorithm to pinpoint a failure's root
cause. We evaluate SlowPoke on a wide range of representative many-core
workloads, and the results demonstrate that SlowPoke reduces the storage
overhead of detection traces by an average of 115.9$\times$, while achieving an
average fail-slow detection accuracy of 86.77% and a false positive rate (FPR)
of 12.11%. More importantly, SlowPoke scales effectively across different
many-core architectures, making it practical for large-scale deployments.

</details>


### [7] [Taming the Tail: NoI Topology Synthesis for Mixed DL Workloads on Chiplet-Based Accelerators](https://arxiv.org/abs/2510.24113)
*Arnav Shukla,Harsh Sharma,Srikant Bharadwaj,Vinayak Abrol,Sujay Deb*

Main category: cs.AR

TL;DR: 提出PARL方法解决chiplet系统中内存驱动传输导致的延迟问题，通过多目标优化生成拓扑结构，在保持吞吐量的同时显著降低最坏情况延迟。


<details>
  <summary>Details</summary>
Motivation: 现代大模型推理中参数和激活值在HBM/DRAM间频繁移动，在interposer网络中产生突发性大流量，导致尾部延迟增加并违反SLA。

Method: 引入干扰评分量化最坏情况下的延迟增加，将NoI合成建模为多目标优化问题，开发PARL拓扑生成器平衡吞吐量、延迟和功耗。

Result: PARL生成的拓扑结构在内存切分处减少竞争，满足SLA要求，将最坏情况延迟降低至1.2倍，同时保持与富链接mesh相当的均值吞吐量。

Conclusion: 重新构建了面向异构chiplet加速器的NoI设计方法，强调工作负载感知的优化目标。

Abstract: Heterogeneous chiplet-based systems improve scaling by disag-gregating
CPUs/GPUs and emerging technologies (HBM/DRAM).However this on-package
disaggregation introduces a latency inNetwork-on-Interposer(NoI). We observe
that in modern large-modelinference, parameters and activations routinely move
backand forth from HBM/DRAM, injecting large, bursty flows into theinterposer.
These memory-driven transfers inflate tail latency andviolate Service Level
Agreements (SLAs) across k-ary n-cube base-line NoI topologies. To address this
gap we introduce an InterferenceScore (IS) that quantifies worst-case slowdown
under contention.We then formulate NoI synthesis as a multi-objective
optimization(MOO) problem. We develop PARL (Partition-Aware
ReinforcementLearner), a topology generator that balances throughput,
latency,and power. PARL-generated topologies reduce contention at the memory
cut, meet SLAs, and cut worst-case slowdown to 1.2 times while maintaining
competitive mean throughput relative to link-rich meshes. Overall, this
reframes NoI design for heterogeneouschiplet accelerators with workload-aware
objectives.

</details>
