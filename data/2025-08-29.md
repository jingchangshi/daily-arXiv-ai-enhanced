<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Solvable Tuple Patterns and Their Applications to Program Verification](https://arxiv.org/abs/2508.20365)
*Naoki Kobayashi,Ryosuke Sato,Ayumi Shinohara,Ryo Yoshinaka*

Main category: cs.PL

TL;DR: 使用可解元组模式(STPs)来表达递归数据结构的不变量，只需少量正样本即可高效推断，无需负样本。通过将STP推理集成到CHC求解器中，实现了对列表类递归数据结构的自动化验证。


<details>
  <summary>Details</summary>
Motivation: 虽然自动化程序验证技术有进步，但对操作递归数据结构的程序进行全自动验证仍面临挑战。需要一种能够高效表达和推断不变量的方法。

Method: 提出可解元组模式(STPs)来表达列表类递归数据结构间的不变量关系。STPs的特点是只需少量正样本即可高效推断，无需负样本。使用支持序列理论的SMT求解器检查STP是否为归纳不变量。将STP推理集成到支持列表类数据结构的CHC求解器中。

Result: 集成了STP推理的CHC求解器在CHC-COMP 2025的ADT-LIN类别中以较大优势获胜。

Conclusion: STPs提供了一种高效的方法来表达和推断递归数据结构的不变量，通过将其集成到CHC求解器中，实现了对列表类递归数据结构程序的自动化验证，为程序验证领域提供了有效的后端解决方案。

Abstract: Despite the recent progress of automated program verification techniques,
fully automated verification of programs manipulating recursive data structures
remains a challenge. We introduce the notion of solvable tuple patterns (STPs)
to express invariants between list-like recursive data structures. A
distinguishing feature of STPs is that they can be efficiently inferred from
only a small number of positive samples; no negative samples are required. An
SMT solver that supports the sequence theory can be used to check that an
inferred STP is indeed an inductive invariant. After presenting basic
properties of STPs and an STP inference algorithm, we show how to incorporate
the STP inference into a CHC (Constrained Horn Clauses) solver supporting
list-like data structures, which serves as a uniform backend for automated
program verification tools. A CHC solver incorporating the STP inference has
won the ADT-LIN category of CHC-COMP 2025 by a big margin.

</details>


### [2] [Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops](https://arxiv.org/abs/2508.20922)
*Markus Böck,Jürgen Cito*

Main category: cs.PL

TL;DR: 该论文解决了概率程序与贝叶斯网络之间的双向表示问题，特别针对包含采样语句和while循环的程序，提出了静态分析方法和程序切片技术来支持变分推断、Metropolis Hastings和顺序蒙特卡洛等优化。


<details>
  <summary>Details</summary>
Motivation: 虽然贝叶斯网络可以表示为概率程序，但反向表示（特别是包含用户标记采样语句和while循环的概率程序）的图形化表示仍是一个开放问题。需要为现代概率编程语言（如Gen、Turing、Pyro）中的这些特性提供图形表示方法。

Method: 扩展现有操作语义以支持语言特性，通过将程序转换为控制流图来定义静态分析，近似程序中随机变量的依赖结构，获得静态因子分解表示，并开发了程序切片技术。

Result: 获得了静态因子分解表示，对于无循环和常量标签的程序等价于贝叶斯网络因子分解，对于定义无界随机变量的程序提供了新颖的图形表示。程序切片技术成功实现了三种优化：降低变分推断梯度估计方差、加速单点Metropolis Hastings和顺序蒙特卡洛。

Conclusion: 提出的方法为包含循环和动态标签的概率程序提供了有效的图形表示，并通过程序切片技术实现了可证明正确的优化，实验表明这些优化匹配或优于现有技术。

Abstract: It is commonly known that any Bayesian network can be implemented as a
probabilistic program, but the reverse direction is not so clear. In this work,
we address the open question to what extent a probabilistic program with
user-labelled sample statements and while loops - features found in languages
like Gen, Turing, and Pyro - can be represented graphically. To this end, we
extend existing operational semantics to support these language features. By
translating a program to its control-flow graph, we define a sound static
analysis that approximates the dependency structure of the random variables in
the program. As a result, we obtain a static factorisation of the implicitly
defined program density, which is equivalent to the known Bayesian network
factorisation for programs without loops and constant labels, but constitutes a
novel graphical representation for programs that define an unbounded number of
random variables via loops or dynamic labels. We further develop a sound
program slicing technique to leverage this structure to statically enable three
well-known optimisations for the considered program class: we reduce the
variance of gradient estimates in variational inference and we speed up both
single-site Metropolis Hastings and sequential Monte Carlo. These optimisations
are proven correct and empirically shown to match or outperform existing
techniques.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [SpeedMalloc: Improving Multi-threaded Applications via a Lightweight Core for Memory Allocation](https://arxiv.org/abs/2508.20253)
*Ruihao Li,Qinzhe Wu,Krishna Kavi,Gayatri Mehta,Jonathan C. Beard,Neeraja J. Yadwadkar,Lizy K. John*

Main category: cs.DC

TL;DR: SpeedMalloc使用轻量级支持核心处理多线程应用的内存分配任务，通过专用核心管理分配器元数据，减少缓存冲突和跨核同步开销，相比现有软件和硬件分配器获得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 内存分配虽然只占代码的5%，但对程序性能有蝴蝶效应，可达2.7倍性能差异。现有加速器方案对多线程支持有限，跨核同步仍是挑战

Method: 采用轻量级可编程支持核心处理内存分配任务，将分配器元数据完全存放在支持核心的缓存中，避免与用户数据缓存冲突，消除跨核元数据同步需求

Result: 在多线程工作负载上相比Jemalloc、TCMalloc、Mimalloc、Mallacc和Memento分别获得1.75倍、1.18倍、1.15倍、1.23倍和1.18倍的加速

Conclusion: SpeedMalloc通过专用支持核心设计有效解决了多线程内存分配的缓存污染和同步开销问题，性能优于现有软件和硬件分配器方案

Abstract: Memory allocation, though constituting only a small portion of the executed
code, can have a "butterfly effect" on overall program performance, leading to
significant and far-reaching impacts. Despite accounting for just approximately
5% of total instructions, memory allocation can result in up to a 2.7x
performance variation depending on the allocator used. This effect arises from
the complexity of memory allocation in modern multi-threaded multi-core
systems, where allocator metadata becomes intertwined with user data, leading
to cache pollution or increased cross-thread synchronization overhead.
Offloading memory allocators to accelerators, e.g., Mallacc and Memento, is a
potential direction to improve the allocator performance and mitigate cache
pollution. However, these accelerators currently have limited support for
multi-threaded applications, and synchronization between cores and accelerators
remains a significant challenge.
  We present SpeedMalloc, using a lightweight support-core to process memory
allocation tasks in multi-threaded applications. The support-core is a
lightweight programmable processor with efficient cross-core data
synchronization and houses all allocator metadata in its own caches. This
design minimizes cache conflicts with user data and eliminates the need for
cross-core metadata synchronization. In addition, using a general-purpose core
instead of domain-specific accelerators makes SpeedMalloc capable of adopting
new allocator designs. We compare SpeedMalloc with state-of-the-art software
and hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and
Memento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on
multithreaded workloads over these five allocators, respectively.

</details>


### [4] [SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization](https://arxiv.org/abs/2508.20258)
*Arya Tschand,Muhammad Awad,Ryan Swann,Kesavan Ramakrishnan,Jeffrey Ma,Keith Lowery,Ganesh Dasika,Vijay Janapa Reddi*

Main category: cs.DC

TL;DR: SwizzlePerf利用LLMs实现硬件感知的GPU内核性能优化，自动生成空间优化模式，相比传统搜索方法大幅提升效率，在10个内核中9个实现最高2.06倍加速


<details>
  <summary>Details</summary>
Motivation: 现有基于搜索的LLM方法缺乏硬件感知能力，而人类性能工程师依赖硬件特性实现近最优性能。需要让LLMs具备硬件意识来进行软件级优化

Method: 通过提供工作负载的内存访问模式、架构规格、过滤的性能日志和历史性能反思，使LLMs具备硬件感知能力，自动生成GPU内核的空间优化模式

Result: GEMM内核优化时间从专家2周缩短到5分钟；10个内核中9个实现最高2.06倍加速和70% L2命中率提升

Conclusion: 这是首个系统性创建硬件感知LLM性能工程代理的工作，为未来自动化性能优化开辟了新方向

Abstract: Large language models (LLMs) have shown progress in GPU kernel performance
engineering using inefficient search-based methods that optimize around
runtime. Any existing approach lacks a key characteristic that human
performance engineers rely on for near-optimal utilization --
hardware-awareness. By leveraging the workload's specific memory access
patterns, architecture specifications, filtered profiling logs, and reflections
on historical performance, we can make software-level optimizations that are
tailored to the underlying hardware. SwizzlePerf automatically generates
spatial optimizations for GPU kernels on disaggregated architectures by giving
LLMs explicit hardware-awareness.
  For a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same
hardware-specific optimal swizzling pattern that took expert performance
engineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels,
SwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve
up to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the
first of many steps toward systematically creating hardware-aware LLM
performance engineering agents.

</details>


### [5] [Predictable LLM Serving on GPU Clusters](https://arxiv.org/abs/2508.20274)
*Erfan Darzi,Shreeanant Bharadwaj,Sree Bhargavi Balija*

Main category: cs.DC

TL;DR: 提出了一种主机级控制器，通过动态MIG重配置、PCIe感知放置和轻量级防护机制，在共享A100集群上减少PCIe噪声干扰，显著降低延迟敏感推理的尾部延迟和SLO违规率。


<details>
  <summary>Details</summary>
Motivation: 共享A100集群中PCIe架构的噪声邻居干扰导致尾部延迟增加和SLO违规，需要一种结构无关的解决方案来优化多租户推理性能。

Method: 结合动态多实例GPU(MIG)重配置、PCIe感知的虚拟机放置、轻量级防护机制(MPS配额、cgroup I/O)，采样每个租户的尾部延迟和系统信号，利用拓扑提示避免PCIe热点，并通过驻留/冷却机制防止操作抖动。

Result: 在单主机和2节点(16-GPU)集群上，SLO违规率降低约32%(约1.5倍)，p99延迟改善约15%，吞吐量成本≤5%；LLM服务评估显示TTFT p99改善10-15%，成本≤5%。

Conclusion: 该方法有效解决了PCIe噪声干扰问题，动态MIG和智能放置策略对性能提升贡献相当，控制器无需修改即可适用于LLM服务场景。

Abstract: Latency-sensitive inference on shared A100 clusters often suffers
noisy-neighbor interference on the PCIe fabric, inflating tail latency and SLO
violations. We present a fabric-agnostic, VM-deployable host-level controller
that combines dynamic Multi-Instance GPU (MIG) reconfiguration, PCIe-aware
placement, and lightweight guardrails (MPS quotas, cgroup I/O). It samples
per-tenant tails and system signals, uses topology hints to avoid PCIe hot
spots, and gates actions with dwell/cool-down to avoid thrash. On a single host
and a 2-node (16-GPU) cluster, SLO miss-rate is reduced by \(\approx\)32\%
(\(\approx\)1.5) and p99 latency improves \(\approx\)15\% with \(\leq\)5\%
throughput cost versus static MIG and naive placement; ablations show MIG and
placement contribute comparably. We also evaluate LLM serving with vLLM on OLMo
2 7B Instruct: TTFT p99 improves \(\approx\)10--15\% at \(\leq\)5\% cost
without changing the controller.

</details>


### [6] [CoFormer: Collaborating with Heterogeneous Edge Devices for Scalable Transformer Inference](https://arxiv.org/abs/2508.20375)
*Guanyu Xu,Zhiwei Hao,Li Shen,Yong Luo,Fuhui Sun,Xiaoyan Wang,Han Hu,Yonggang Wen*

Main category: cs.DC

TL;DR: CoFormer是一个协作推理系统，通过将大型transformer模型分解为多个小型模型在边缘设备上分布式推理，显著降低计算需求和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么将transformer计算卸载到其他设备导致通信开销大，要么在单个边缘设备上部署压缩模型导致精度和效率的次优权衡，需要新的解决方案。

Method: 利用transformer的可分割性和可集成性，将大型transformer分解为多个小型模型进行分布式推理，使用DeBo算法优化分解策略并校准模型性能。

Result: 实现3.1倍推理加速，GPT2-XL模型内存需求减少76.3%，能耗降低约40%，同时保持满意的推理性能。

Conclusion: CoFormer为在异构边缘设备上高效部署大型transformer模型提供了有效解决方案，平衡了延迟、精度和资源消耗。

Abstract: The impressive performance of transformer models has sparked the deployment
of intelligent applications on resource-constrained edge devices. However,
ensuring high-quality service for real-time edge systems is a significant
challenge due to the considerable computational demands and resource
requirements of these models. Existing strategies typically either offload
transformer computations to other devices or directly deploy compressed models
on individual edge devices. These strategies, however, result in either
considerable communication overhead or suboptimal trade-offs between accuracy
and efficiency. To tackle these challenges, we propose a collaborative
inference system for general transformer models, termed CoFormer. The central
idea behind CoFormer is to exploit the divisibility and integrability of
transformer. An off-the-shelf large transformer can be decomposed into multiple
smaller models for distributed inference, and their intermediate results are
aggregated to generate the final output. We formulate an optimization problem
to minimize both inference latency and accuracy degradation under heterogeneous
hardware constraints. DeBo algorithm is proposed to first solve the
optimization problem to derive the decomposition policy, and then progressively
calibrate decomposed models to restore performance. We demonstrate the
capability to support a wide range of transformer models on heterogeneous edge
devices, achieving up to 3.1$\times$ inference speedup with large transformer
models. Notably, CoFormer enables the efficient inference of GPT2-XL with 1.6
billion parameters on edge devices, reducing memory requirements by 76.3\%.
CoFormer can also reduce energy consumption by approximately 40\% while
maintaining satisfactory inference performance.

</details>


### [7] [pdGRASS: A Fast Parallel Density-Aware Algorithm for Graph Spectral Sparsification](https://arxiv.org/abs/2508.20403)
*Tiancheng Zhao,Zekun Yin,Huihai An,Xiaoyu Yang,Zhou Jin,Jiasi Shen,Helen Xu*

Main category: cs.DC

TL;DR: 提出了pdGRASS并行算法，解决了现有feGRASS方法在并行化和偏斜输入上的问题，实现了3.9-8.8倍的加速和更好的稀疏化质量


<details>
  <summary>Details</summary>
Motivation: 现有feGRASS方法存在两个主要问题：1）由于严格的数据依赖导致恢复步骤难以并行化；2）在偏斜输入上性能下降，通常需要多次遍历才能恢复足够的边

Method: 提出pdGRASS并行算法，将边组织成无数据依赖的互斥子任务，实现高效并行化并在单次遍历中充分恢复边

Result: pdGRASS实现了平均3.9-8.8倍的加速，稀疏化器质量在PCG迭代次数上表现更好（1.2倍高至1.8倍低），在极端情况下获得1000倍以上的加速

Conclusion: pdGRASS在图谱稀疏化问题上显著提升了可扩展性和性能

Abstract: Graph Spectral Sparsification (GSS) identifies an ultra-sparse subgraph, or
sparsifier, whose Laplacian matrix closely approximates the spectral properties
of the original graph, enabling substantial reductions in computational
complexity for computationally intensive problems in scientific computing. The
state-of-the-art method for efficient GSS is feGRASS, consisting of two steps:
1) spanning tree generation and 2) off-tree edge recovery. However, feGRASS
suffers from two main issues: 1) difficulties in parallelizing the recovery
step for strict data dependencies, and 2) performance degradation on skewed
inputs, often requiring multiple passes to recover sufficient edges. To address
these challenges, we propose parallel density-aware Graph Spectral
Sparsification (pdGRASS), a parallel algorithm that organizes edges into
disjoint subtasks without data dependencies between them, enabling efficient
parallelization and sufficient edge recovery in a single pass. We empirically
evaluate feGRASS and pdGRASS based on 1) off-tree edge-recovery runtime and 2)
sparsifier quality, measured by the iteration count required for convergence in
a preconditioned conjugate gradient (PCG) application. The evaluation
demonstrates that, depending on the number of edges recovered, pdGRASS achieves
average speedups ranging from 3.9x to 8.8x. The resulting sparsifiers also show
between 1.2x higher and 1.8x lower PCG iteration counts, with further
improvements as more edges are recovered. Additionally, pdGRASS mitigates the
worst-case runtimes of feGRASS with over 1000x speedup. These results highlight
pdGRASS's significant improvements in scalability and performance for the graph
spectral sparsification problem.

</details>


### [8] [Collaborative Evolution of Intelligent Agents in Large-Scale Microservice Systems](https://arxiv.org/abs/2508.20508)
*Yilin Li,Song Han,Sibo Wang,Ming Wang,Renzi Meng*

Main category: cs.DC

TL;DR: 提出基于多智能体协同进化机制的智能服务优化方法，解决大规模微服务架构中的治理挑战，通过图表示学习和博弈驱动的策略优化实现服务自适应协作。


<details>
  <summary>Details</summary>
Motivation: 解决大规模微服务架构中复杂的服务依赖、动态拓扑结构和波动工作负载等治理挑战，提高系统治理效率和运行稳定性。

Method: 将每个服务建模为智能体，使用图表示学习构建服务依赖图，采用集中训练分散执行框架，设计博弈驱动的策略优化机制，通过选择-变异过程动态调整策略分布。

Result: 在代表性微服务仿真平台上实验表明，该方法在协调效率、适应性和策略收敛性能等关键指标上优于其他先进方法，显著提升治理效率和运行稳定性。

Conclusion: 该方法具有强大的实用价值和工程可行性，能够快速响应突发工作负载峰值、拓扑重构或资源冲突等场景，实现稳定的策略收敛。

Abstract: This paper proposes an intelligent service optimization method based on a
multi-agent collaborative evolution mechanism to address governance challenges
in large-scale microservice architectures. These challenges include complex
service dependencies, dynamic topology structures, and fluctuating workloads.
The method models each service as an agent and introduces graph representation
learning to construct a service dependency graph. This enables agents to
perceive and embed structural changes within the system. Each agent learns its
policy based on a Markov Decision Process. A centralized training and
decentralized execution framework is used to integrate local autonomy with
global coordination. To enhance overall system performance and adaptability, a
game-driven policy optimization mechanism is designed. Through a
selection-mutation process, agent strategy distributions are dynamically
adjusted. This supports adaptive collaboration and behavioral evolution among
services. Under this mechanism, the system can quickly respond and achieve
stable policy convergence when facing scenarios such as sudden workload spikes,
topology reconfigurations, or resource conflicts. To evaluate the effectiveness
of the proposed method, experiments are conducted on a representative
microservice simulation platform. Comparative analyses are performed against
several advanced approaches, focusing on coordination efficiency, adaptability,
and policy convergence performance. Experimental results show that the proposed
method outperforms others in several key metrics. It significantly improves
governance efficiency and operational stability in large-scale microservice
systems. The method demonstrates strong practical value and engineering
feasibility.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [Testing and Fault Tolerance Techniques for Carbon Nanotube-Based FPGAs](https://arxiv.org/abs/2508.20304)
*Siyuan Lu,Kangwei Xu,Peng Xie,Rui Wang,Yuanqing Cheng*

Main category: cs.AR

TL;DR: 本文提出了一种基于环形振荡器的测试技术，用于检测由多壁碳纳米管互连线过程变异导致的延迟故障，并提出了一种有效的测试技术和算法来检测CLB中的金属性碳纳米管，同时通过重复预备行共享架构提高碳纳米管FPGA的良率。


<details>
  <summary>Details</summary>
Motivation: 随着半导体制造过程技术节点缩小至纳米级，CMOS基地现场可编程逸逸电闸阵(FPGA)在性能和功耗的可扩展性方面面临巨大挑战。多壁碳纳米管(MWCNT)和碳纳米管场效应管(CNFET)作为有前景的替代选择，但过程变异和非理想制造过程导致延迟故障和相关故障块。

Method: 提出基于环形振荡器(RO)的测试技术检测MWCNT互连线延迟故障；提出有效的进位链测试技术；基于查找表(LUT)的改进电路设计加快故障测试；提出检测m-CNT的测试算法；提出重复预备行共享架构提高良率。

Result: 实验结果显示，6输入LUT的测试时间可以比传统测试减少35.49%，提出的算法能够以少量开销实现高测试覆盖率，重复架构能够有效但故障段进行修复。

Conclusion: 该研究为碳纳米管基FPGA提供了一套完整的测试和修复方案，有效解决了由过程变异和非理想制造导致的故障问题，显著提高了碳纳米管基FPGA的可靠性和良率。

Abstract: As the semiconductor manufacturing process technology node shrinks into the
nanometer-scale, the CMOS-based Field Programmable Gate Arrays (FPGAs) face big
challenges in scalability of performance and power consumption. Multi-walled
Carbon Nanotube (MWCNT) serves as a promising candidate for Cu interconnects
thanks to the superior conductivity. Moreover, Carbon Nanotube Field Transistor
(CNFET) also emerges as a prospective alternative to the conventional CMOS
device because of high power efficiency and large noise margin. The combination
of MWCNT and CNFET enables the promising CNT-based FPGAs. However, the MWCNT
interconnects exhibit significant process variations due to immature
fabrication process, leading to delay faults. Also, the non-ideal CNFET
fabrication process may generate a few metallic CNTs (m-CNTs), rendering
correlated faulty blocks. In this article, we propose a ring oscillator (RO)
based testing technique to detect delay faults due to the process variation of
MWCNT interconnects. Furthermore, we propose an effective testing technique for
the carry chains in CLBs, and an improved circuit design based on the lookup
table (LUT) is applied to speed up the fault testing of CNT-based FPGAs. In
addition, we propose a testing algorithm to detect m-CNTs in CLBs. Finally, we
propose a redundant spare row sharing architecture to improve the yield of
CNT-based FPGA further. Experimental results show that the test time for a
6-input LUT can be reduced by 35.49% compared with conventional testing, and
the proposed algorithm can achieve a high test coverage with little overhead.
The proposed redundant architecture can repair the faulty segment effectively
and efficiently.

</details>


### [10] [The Future of Memory: Limits and Opportunities](https://arxiv.org/abs/2508.20425)
*Shuhan Liu,Samuel Dayo,Peijing Li,Philip Levis,Subhasish Mitra,Thierry Tambe,David Tennenhouse,H. -S. Philip Wong*

Main category: cs.AR

TL;DR: 这篇论文提出了一种新的计算-内存节点架构，通过将内存切片化并与计算元素紧密耦合，来解决大规模共享内存系统的缩放和信令挑战。


<details>
  <summary>Details</summary>
Motivation: 解决内存延迟、带宽、容量和能消耗对系统性能的限制，找到替代大规模共享内存架构的实用方案。

Method: 利用2.5D/3D集成技术，将内存切分为小型片段与计算元素紧密耦合，构建计算-内存节点，提供私有本地内存和包内共享内存。

Result: 实现了微米级距离的本地数据访问，显著降低访问成本，提供比DRAM更好的带宽和能效。

Conclusion: 硬件明确内存容量和距离的架构设计允许软件高效管理数据存放和移动，为解决内存性能问题提供了新方向。

Abstract: Memory latency, bandwidth, capacity, and energy increasingly limit
performance. In this paper, we reconsider proposed system architectures that
consist of huge (many-terabyte to petabyte scale) memories shared among large
numbers of CPUs. We argue two practical engineering challenges, scaling and
signaling, limit such designs. We propose the opposite approach. Rather than
create large, shared, homogenous memories, systems explicitly break memory up
into smaller slices more tightly coupled with compute elements. Leveraging
advances in 2.5D/3D integration, this compute-memory node provisions private
local memory, enabling accesses of node-exclusive data through micrometer-scale
distances, and dramatically reduced access cost. In-package memory elements
support shared state within a processor, providing far better bandwidth and
energy-efficiency than DRAM, which is used as main memory for large working
sets and cold data. Hardware making memory capacities and distances explicit
allows software to efficiently compose this hierarchy, managing data placement
and movement.

</details>


### [11] [Microarchitecture Design and Benchmarking of Custom SHA-3 Instruction for RISC-V](https://arxiv.org/abs/2508.20653)
*Alperen Bolat,Sakir Sezer,Kieran McLaughlin,Henry Hui*

Main category: cs.AR

TL;DR: 该研究探讨了在RISC-V CPU架构中集成SHA-3加密加速指令的微架构挑战，通过定制指令实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案主要依赖独立协处理器或软件优化，避免直接微架构集成的复杂性，而SHA-3由于其独特的基于置换的结构和内存访问模式，高效加速仍然是一个开放问题。

Method: 研究在通用处理器中嵌入SHA-3置换操作作为定制指令，重点关注流水线并行执行、存储利用和硬件成本。使用周期精确的GEM5模拟和FPGA原型验证。

Result: 结果显示：RISC-V优化SHA-3软件工作负载性能提升达8.02倍，Keccak特定软件工作负载提升达46.31倍，寄存器仅增加15.09%，LUT利用率仅增加11.51%。

Conclusion: 这些发现为微架构级别的SHA-3加速可行性和影响提供了关键见解，突出了未来密码指令集扩展的实际设计考虑。

Abstract: Integrating cryptographic accelerators into modern CPU architectures presents
unique microarchitectural challenges, particularly when extending instruction
sets with complex and multistage operations. Hardware-assisted cryptographic
instructions, such as Intel's AES-NI and ARM's custom instructions for
encryption workloads, have demonstrated substantial performance improvements.
However, efficient SHA-3 acceleration remains an open problem due to its
distinct permutation-based structure and memory access patterns. Existing
solutions primarily rely on standalone coprocessors or software optimizations,
often avoiding the complexities of direct microarchitectural integration. This
study investigates the architectural challenges of embedding a SHA-3
permutation operation as a custom instruction within a general-purpose
processor, focusing on pipelined simultaneous execution, storage utilization,
and hardware cost. In this paper, we investigated and prototyped a SHA-3 custom
instruction for the RISC-V CPU architecture. Using cycle-accurate GEM5
simulations and FPGA prototyping, our results demonstrate performance
improvements of up to 8.02x for RISC-V optimized SHA-3 software workloads and
up to 46.31x for Keccak-specific software workloads, with only a 15.09%
increase in registers and a 11.51% increase in LUT utilization. These findings
provide critical insights into the feasibility and impact of SHA-3 acceleration
at the microarchitectural level, highlighting practical design considerations
for future cryptographic instruction set extensions.

</details>
