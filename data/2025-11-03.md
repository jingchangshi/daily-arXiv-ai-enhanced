<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Dependence-Driven, Scalable Quantum Circuit Mapping with Affine Abstractions](https://arxiv.org/abs/2510.27067)
*Marouane Benbetka,Merwan Bekkar,Riyadh Baghdadi,Martin Kong*

Main category: cs.PL

TL;DR: 提出了一种基于传递依赖权重的量子比特映射算法，通过仿射抽象建模量子电路来计算传递依赖，从而优化SWAP门插入策略，显著减少电路深度和SWAP门数量。


<details>
  <summary>Details</summary>
Motivation: 现代量子处理器只支持相邻量子比特间的相互作用，需要插入SWAP门来修复非相邻量子比特间的连通性。现有方法未能充分利用依赖信息，导致SWAP门数量过多，增加了电路错误率。

Method: 使用仿射抽象建模量子电路，计算传递依赖关系，根据依赖距离对电路进行分层，并为每层计算不同的权重来优化SWAP门插入策略。

Result: 在IBM和Rigetti量子处理器上使用QUEKO和QASMBench基准测试集进行评估，相比QMAP、Sabre、Cirq和TKET等基线工具，在电路深度和SWAP门数量方面都有显著改进，同时保持了良好的可扩展性。

Conclusion: 基于传递依赖权重的映射算法能够有效利用依赖信息，显著优化量子电路的编译质量，为量子计算提供了更高效的编译解决方案。

Abstract: Qubit Mapping is a critical task in Quantum Compilation, as modern Quantum
Processing Units (QPUs) are constrained to nearest-neighbor interactions
defined by a qubit coupling graph. This compiler pass repairs the connectivity
of two-qubit gates whose operands are not adjacent by inserting SWAP gates that
move the state of qubits between directly connected qubits. Deciding when to
introduce SWAPs while minimizing their count is critical because the error in
quantum programs increases exponentially with the circuit latency, measured in
number of gates along the critical path of the circuit. Prior work for this
problem relied on heuristics and exact methods that partition the circuit into
two or more layers, but failed to exploit valuable dependence information in
any form.
  This paper introduces a novel qubit mapping algorithm based on the weight of
transitive dependences. The introduced mapper models quantum circuits with
affine abstractions thereby yielding the ability to compute transitive
dependences. In turn, the newfound information is used to partition circuits by
dependence distances and compute, efficiently, distinct weights for each layer.
We evaluate the efficiency of our mapper on IBM and Rigetti QPUs, using the
large datasets from the QUEKO and QASMBench benchmark suites, and against four
baseline tools (QMAP, Sabre, Cirq and TKET), demonstrating notable improvements
in circuit depth and swap count while delivering competitive scalability.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [FlowMesh: A Service Fabric for Composable LLM Workflows](https://arxiv.org/abs/2510.26913)
*Junyi Shen,Noppanat Wadlom,Lingfeng Zhou,Dequan Wang,Xu Miao,Lei Fang,Yao Lu*

Main category: cs.DC

TL;DR: FlowMesh是一个多租户服务架构，将AI工作流作为共享服务而非独立管道执行和优化，通过细粒度算子分解、去重和批处理实现成本降低和能效提升。


<details>
  <summary>Details</summary>
Motivation: AI部署正从单一LLM任务转向数据转换、微调和智能体交互的流水线模式，需要应对这种转变带来的效率和成本挑战。

Method: 将工作流分解为细粒度算子并记录血缘关系，通过全局控制平面管理算子池，使用单一效用函数选择批次和工作节点，数据平面采用无状态工作节点和内容寻址存储。

Result: 相比基线方案，FlowMesh实现高达3.8倍成本降低和2.0倍能耗降低，提供相似或更好的延迟性能，在动态和易故障环境下保持高效。

Conclusion: FlowMesh通过共享服务架构有效优化了现代AI工作流，在成本、能耗和性能方面均有显著提升。

Abstract: AI deployment increasingly resembles a pipeline of data transformation,
fine-tuning, and agent interactions rather than a monolithic LLM job; recent
examples include RLHF/RLAIF training and agentic workflows. To cope with this
shift, we propose FlowMesh, a multi-tenant service fabric that executes and
optimizes these workloads as one shared service instead of isolated pipelines.
It decomposes workflows into fine-grained operators with recorded lineage,
enabling de-duplication of work across users and batching requests on the same
hardware while preserving per-workflow provenance. A global control plane
maintains a cluster-wide pool of ready operators and uses a single utility
function to pick both the batch and the worker, balancing throughput, cost, and
data locality on heterogeneous GPUs. The data plane is an elastic fleet of
stateless workers backed by a content-addressable store, enabling rapid,
automatic scale-out, safe retry after preemption, and portability across
managed clusters such as Kubernetes and geo-distributed GPU marketplaces such
as Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost
reduction and 2.0x lower energy usage, provides a similar or better latency
profile, and remains efficient under dynamic and failure-prone conditions.

</details>


### [3] [A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration](https://arxiv.org/abs/2510.27039)
*Zhuo Zheng,Lingran Meng,Ziyu Lin*

Main category: cs.DC

TL;DR: 提出一种基于云的混合模型，结合时空图神经网络和Transformer架构进行交通流量预测，在云平台上实现可扩展性和实时适应性


<details>
  <summary>Details</summary>
Motivation: 传统模型难以有效捕捉大规模路网中的复杂时空依赖关系，特别是在天气、节假日、交通事故等外部因素影响下

Method: 集成时空图神经网络（ST-GNN）和Transformer架构，利用GNN建模道路网络空间相关性，Transformer捕捉长期时间依赖，并通过特征融合整合外部上下文特征

Result: 在数据集上的实验评估显示，该模型优于基线方法（LSTM、TCN、GCN、纯Transformer），RMSE仅为17.92，MAE仅为10.53

Conclusion: 混合GNN-Transformer方法为基于云的智能交通系统应用提供了有效且可扩展的解决方案，为交通流量预测提供了方法论进展，并为拥堵缓解提供了实际意义

Abstract: Accurate traffic flow forecasting is essential for the development of
intelligent transportation systems (ITS), supporting tasks such as traffic
signal optimization, congestion management, and route planning. Traditional
models often fail to effectively capture complex spatial-temporal dependencies
in large-scale road networks, especially under the influence of external
factors such as weather, holidays, and traffic accidents. To address this
challenge, this paper proposes a cloud-based hybrid model that integrates
Spatio-Temporal Graph Neural Networks (ST-GNN) with a Transformer architecture
for traffic flow prediction. The model leverages the strengths of GNNs in
modeling spatial correlations across road networks and the Transformers'
ability to capture long-term temporal dependencies. External contextual
features are incorporated via feature fusion to enhance predictive accuracy.
The proposed model is deployed on a cloud computing platform to achieve
scalability and real-time adaptability. Experimental evaluation of the dataset
shows that our model outperforms baseline methods (LSTM, TCN, GCN, pure
Transformer) with an RMSE of only 17.92 and a MAE of only 10.53. These findings
suggest that the hybrid GNN-Transformer approach provides an effective and
scalable solution for cloud-based ITS applications, offering methodological
advancements for traffic flow forecasting and practical implications for
congestion mitigation.

</details>


### [4] [Synergistic Tensor and Pipeline Parallelism](https://arxiv.org/abs/2510.27257)
*Mengshi Qi,Jiaxuan Peng,Jie Zhang,Juan Zhu,Yong Li,Huadong Ma*

Main category: cs.DC

TL;DR: 提出一种协同的张量并行和流水线并行调度方法，通过将前向和后向传播解耦为细粒度计算单元并交织编排，同时减少两种并行方式中的通信开销和流水线气泡。


<details>
  <summary>Details</summary>
Motivation: 现有混合并行训练方法中，张量并行引入显著的集体通信开销，流水线并行存在同步效率低下的流水线气泡问题，而现有工作往往只从孤立角度解决这些问题。

Method: 将流水线并行的前向和后向传播解耦为细粒度计算单元，然后交织编排形成复合计算序列，在此基础上设计流水线并行调度以最小化流水线气泡。

Result: 实验结果表明，相比现有调度方法，该方法对LLM训练吞吐量提升最高达12%，对MLLM提升最高达16%。

Conclusion: 提出的协同调度方法能有效同时减少张量并行和流水线并行中的通信开销和同步延迟，显著提升大规模模型训练效率。

Abstract: In the machine learning system, the hybrid model parallelism combining tensor
parallelism (TP) and pipeline parallelism (PP) has become the dominant solution
for distributed training of Large Language Models~(LLMs) and Multimodal LLMs
(MLLMs). However, TP introduces significant collective communication overheads,
while PP suffers from synchronization inefficiencies such as pipeline bubbles.
Existing works primarily address these challenges from isolated perspectives,
focusing either on overlapping TP communication or on flexible PP scheduling to
mitigate pipeline bubbles. In this paper, we propose a new synergistic tensor
and pipeline parallelism schedule that simultaneously reduces both types of
bubbles. Our proposed schedule decouples the forward and backward passes in PP
into fine-grained computation units, which are then braided to form a composite
computation sequence. This compositional structure enables near-complete
elimination of TP-related bubbles. Building upon this structure, we further
design the PP schedule to minimize PP bubbles. Experimental results demonstrate
that our approach improves training throughput by up to 12% for LLMs and 16%
for MLLMs compared to existing scheduling methods. Our source code is avaiable
at https://github.com/MICLAB-BUPT/STP.

</details>


### [5] [A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination](https://arxiv.org/abs/2510.27289)
*Zhengchang Hua,Panagiotis Oikonomou,Karim Djemame,Nikos Tziritas,Georgios Theodoropoulos*

Main category: cs.DC

TL;DR: 提出DT-MADDPG算法，将多智能体强化学习与协作数字孪生网络结合，在保护数据隐私的同时实现电动汽车V2G网络的协调控制。


<details>
  <summary>Details</summary>
Motivation: 解决大规模去中心化系统（如电动汽车V2G网络）的协调控制问题，同时保护个体智能体的隐私，避免传统多智能体学习算法需要集中收集敏感原始数据的问题。

Method: 提出数字孪生辅助多智能体深度确定性策略梯度算法（DT-MADDPG），通过协作构建的预测全局模型增强集中评论家，使用隐私保护数据而非原始数据。

Result: 在模拟V2G环境中的实验结果表明，DT-MADDPG能达到与标准MADDPG算法相当的协调性能，同时在数据隐私和架构去中心化方面具有显著优势。

Conclusion: 该工作为在复杂现实网络物理系统中部署基于学习的智能协调提供了一个实用且鲁棒的框架。

Abstract: The coordination of large-scale, decentralised systems, such as a fleet of
Electric Vehicles (EVs) in a Vehicle-to-Grid (V2G) network, presents a
significant challenge for modern control systems. While collaborative Digital
Twins have been proposed as a solution to manage such systems without
compromising the privacy of individual agents, deriving globally optimal
control policies from the high-level information they share remains an open
problem. This paper introduces Digital Twin Assisted Multi-Agent Deep
Deterministic Policy Gradient (DT-MADDPG) algorithm, a novel hybrid
architecture that integrates a multi-agent reinforcement learning framework
with a collaborative DT network. Our core contribution is a simulation-assisted
learning algorithm where the centralised critic is enhanced by a predictive
global model that is collaboratively built from the privacy-preserving data
shared by individual DTs. This approach removes the need for collecting
sensitive raw data at a centralised entity, a requirement of traditional
multi-agent learning algorithms. Experimental results in a simulated V2G
environment demonstrate that DT-MADDPG can achieve coordination performance
comparable to the standard MADDPG algorithm while offering significant
advantages in terms of data privacy and architectural decentralisation. This
work presents a practical and robust framework for deploying intelligent,
learning-based coordination in complex, real-world cyber-physical systems.

</details>


### [6] [Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing](https://arxiv.org/abs/2510.27317)
*Shuyi Chen,Panagiotis Oikonomou,Zhengchang Hua,Nikos Tziritas,Karim Djemame,Nan Zhang,Georgios Theodoropoulos*

Main category: cs.DC

TL;DR: 提出了一种面向可再生能源供电的多接入边缘计算系统的在线资源分配策略，通过动态调度计算任务和控制能耗来平衡间歇性可再生能源与动态用户需求。


<details>
  <summary>Details</summary>
Motivation: 多接入边缘计算系统越来越多地集成可再生能源收集技术，但在没有电网供电的情况下，如何平衡间歇性收集能源与动态用户需求成为重要的资源分配挑战。

Method: 提出在线策略，动态调度具有依赖关系的计算任务，并通过实时决策服务器频率缩放和服务模块迁移来控制能耗消耗。

Result: 使用真实世界数据集的实验表明，该算法在有效利用收集能源的同时保持了低服务延迟。

Conclusion: 该策略成功解决了可再生能源供电的边缘计算系统中资源分配的挑战，实现了能效和性能的平衡。

Abstract: Multi-access Edge Computing (MEC) delivers low-latency services by hosting
applications near end-users. To promote sustainability, these systems are
increasingly integrated with renewable Energy Harvesting (EH) technologies,
enabling operation where grid electricity is unavailable. However, balancing
the intermittent nature of harvested energy with dynamic user demand presents a
significant resource allocation challenge. This work proposes an online
strategy for an MEC system powered exclusively by EH to address this trade-off.
Our strategy dynamically schedules computational tasks with dependencies and
governs energy consumption through real-time decisions on server frequency
scaling and service module migration. Experiments using real-world datasets
demonstrate our algorithm's effectiveness in efficiently utilizing harvested
energy while maintaining low service latency.

</details>


### [7] [ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method](https://arxiv.org/abs/2510.27351)
*Milena Veneva*

Main category: cs.DC

TL;DR: 本文提出了一种基于机器学习的启发式方法，用于寻找并行分区算法CUDA实现的最佳子系统大小。通过kNN分类方法预测最佳子系统大小，并将该方法扩展到递归并行分区算法。


<details>
  <summary>Details</summary>
Motivation: 寻找并行分区算法在CUDA实现中的最佳子系统大小，以提高计算效率。

Method: 使用kNN分类方法预测最佳子系统大小，并通过统计分析与实际数据比较验证算法效果。将启发式方法扩展到递归并行分区算法，构建预测最佳递归步数的kNN模型。

Result: 通过比较预测值与实际数据，算法表现良好。成功构建了预测最佳子系统大小和递归步数的模型。

Conclusion: 基于机器学习的启发式方法能够有效预测并行分区算法的最佳参数配置，为CUDA实现提供了优化方案。

Abstract: This paper presents a machine learning (ML)-based heuristic for finding the
optimum sub-system size for the CUDA implementation of the parallel partition
algorithm. Computational experiments for different system of linear algebraic
equation (SLAE) sizes are conducted, and the optimum sub-system size for each
of them is found empirically. To estimate a model for the sub-system size, we
perform the k-nearest neighbors (kNN) classification method. Statistical
analysis of the results is done. By comparing the predicted values with the
actual data, the algorithm is deemed to be acceptably good. Next, the heuristic
is expanded to work for the recursive parallel partition algorithm as well. An
algorithm for determining the optimum sub-system size for each recursive step
is formulated. A kNN model for predicting the optimum number of recursive steps
for a particular SLAE size is built.

</details>


### [8] [RDMA Point-to-Point Communication for LLM Systems](https://arxiv.org/abs/2510.27656)
*Nandor Licker,Kevin Hu,Vladimir Zaytsev,Lequn Chen*

Main category: cs.DC

TL;DR: TransferEngine是一个统一的网络接口抽象层，解决了LLM系统中点对点通信的硬件锁定问题，支持多种NIC并提供高性能通信能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统实现被锁定在特定网络接口控制器(NIC)上，阻碍了推理引擎的集成和跨硬件提供商的移植性。

Method: TransferEngine桥接常见NIC功能，暴露统一的单边WriteImm操作和ImmCounter原语进行完成通知，透明管理每个GPU的多个NIC。

Result: 在NVIDIA ConnectX-7和AWS EFA上实现400Gbps峰值吞吐量；支持动态扩展的KvCache传输、万亿参数模型1.3秒RL权重更新、以及优于DeepEP的MoE调度/组合延迟。

Conclusion: TransferEngine的可移植点对点通信补充了集合操作，同时避免了硬件锁定问题。

Abstract: Emerging Large Language Model (LLM) system patterns, such as disaggregated
inference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement
fine-tuning, require flexible point-to-point communication beyond simple
collectives. Existing implementations are locked to specific Network Interface
Controllers (NICs), hindering integration into inference engines and
portability across hardware providers. We present TransferEngine, which bridges
the functionality of common NICs to expose a uniform interface. TransferEngine
exposes one-sided WriteImm operations with a ImmCounter primitive for
completion notification, without ordering assumptions of network transport,
transparently managing multiple NICs per GPU. We demonstrate peak throughput of
400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We
showcase TransferEngine through three production systems: (1) KvCache transfer
for disaggregated inference with dynamic scaling, (2) RL weight updates
achieving 1.3 seconds for trillion-parameter models, and (3) MoE
dispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,
with the first viable latencies on EFA. We demonstrate that our portable
point-to-point communication complements collectives while avoiding lock-in.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache Hierarchies](https://arxiv.org/abs/2510.26944)
*Hoa Nguyen,Pongstorn Maidee,Jason Lowe-Power,Alireza Kaviani*

Main category: cs.AR

TL;DR: Choreographer是一个用于细粒度加速器系统级评估的仿真框架，能够准确捕捉硬件软件交互开销，通过案例研究展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有框架无法全面捕捉核心-加速器和缓存-加速器交互中的硬件软件开销，需要更精确的系统级评估工具来优化延迟敏感任务的加速器设计。

Method: 开发了基于gem5的详细硬件栈（含AMBA CHI网状网络）和完整Linux软件栈，提供C++ API和模块化配置选项，包含精确的缓存模型来捕捉性能变化。

Result: 数据感知预取器在图形分析工作负载中实现1.08x-1.88x加速，快速排序加速器实现超过2x加速且地址转换开销最小，验证了框架的有效性。

Conclusion: Choreographer能够准确建模复杂硬件软件交互，在小任务卸载场景中优化性能，为细粒度加速器设计提供了有效的系统级评估工具。

Abstract: In this paper, we introduce Choreographer, a simulation framework that
enables a holistic system-level evaluation of fine-grained accelerators
designed for latency-sensitive tasks. Unlike existing frameworks, Choreographer
captures all hardware and software overheads in core-accelerator and
cache-accelerator interactions, integrating a detailed gem5-based hardware
stack featuring an AMBA coherent hub interface (CHI) mesh network and a
complete Linux-based software stack. To facilitate rapid prototyping, it offers
a C++ application programming interface and modular configuration options. Our
detailed cache model provides accurate insights into performance variations
caused by cache configurations, which are not captured by other frameworks. The
framework is demonstrated through two case studies: a data-aware prefetcher for
graph analytics workloads, and a quicksort accelerator. Our evaluation shows
that the prefetcher achieves speedups between 1.08x and 1.88x by reducing
memory access latency, while the quicksort accelerator delivers more than 2x
speedup with minimal address translation overhead. These findings underscore
the ability of Choreographer to model complex hardware-software interactions
and optimize performance in small task offloading scenarios.

</details>


### [10] [Practical Timing Closure in FPGA and ASIC Designs: Methods, Challenges, and Case Studies](https://arxiv.org/abs/2510.26985)
*Mostafa Darvishi*

Main category: cs.AR

TL;DR: 本文深入分析FPGA和ASIC中的时序收敛挑战，通过案例研究比较Xilinx Kintex UltraScale+ FPGA与7nm ASIC的性能差异，显示ASIC在时序性能上更优但FPGA仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 研究FPGA和ASIC在时序收敛方面的核心挑战和约束，理解两种技术在时序行为上的差异，为高性能设计选择提供指导。

Method: 分析核心时序原理、架构差异和设计方法学，通过案例研究比较XCKU040 FPGA与7nm ASIC的实际时序分析和性能权衡。

Result: 实验结果显示ASIC实现45ps建立时间和35ps保持时间的优越时序性能，而现代FPGA达到180ps建立时间和120ps保持时间，证明FPGA仍适用于高性能设计。

Conclusion: ASIC在时序性能上优于FPGA，但现代FPGA的时序表现仍具有竞争力，验证了其在高性能设计中的适用性。

Abstract: This paper presents an in-depth analysis of timing closure challenges and
constraints in Field Programmable Gate Arrays (FPGAs) and Application Specific
Integrated Circuits (ASICs). We examine core timing principles, architectural
distinctions, and design methodologies influencing timing behavior in both
technologies. A case study comparing the Xilinx Kintex UltraScale+ FPGA
(XCKU040) with a 7nm ASIC highlights practical timing analysis and performance
trade-offs. Experimental results show ASICs achieve superior timing of 45ps
setup and 35ps hold, while modern FPGAs remain competitive with 180ps setup and
120ps hold times, validating their suitability for high-performance designs.

</details>


### [11] [Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review](https://arxiv.org/abs/2510.27070)
*Dong Tong*

Main category: cs.AR

TL;DR: 该论文对基于描述符的对象感知内存系统进行了全面调查，这种架构范式旨在弥合硬件/软件接口的语义鸿沟，通过将描述符提升为一级架构抽象，使硬件能够动态获取和执行软件定义对象的丰富语义。


<details>
  <summary>Details</summary>
Motivation: 现代计算系统的安全性和效率因缺乏原生架构机制来传播高级程序语义（如对象身份、边界和生命周期）而受到根本性损害。

Method: 系统性地绘制了该方法的演进和现状，建立了内存对象和描述符的基础概念，并引入了描述符寻址模式的新分类法，为分析和比较不同实现提供了结构化框架。

Result: 统一分析揭示了该范式如何整体解决内存保护、管理和处理的相互关联挑战。通过CentroID模型的案例研究，展示了其混合标记指针编码和描述符处理机制如何体现实用高效的对象感知设计路径。

Conclusion: 明确的对象语义跨层通信为下一代缓存层次结构、统一虚拟内存甚至128位架构提供了基础研究方向。

Abstract: The security and efficiency of modern computing systems are fundamentally
undermined by the absence of a native architectural mechanism to propagate
high-level program semantics, such as object identity, bounds, and lifetime,
across the hardware/software interface. This paper presents a comprehensive
survey of the architectural paradigm designed to bridge this semantic gap:
descriptor-based, object-aware memory systems. By elevating the descriptor to a
first-class architectural abstraction, this paradigm enables hardware to
dynamically acquire and enforce the rich semantics of software-defined objects.
This survey systematically charts the evolution and current landscape of this
approach. We establish the foundational concepts of memory objects and
descriptors and introduce a novel taxonomy of descriptor addressing modes,
providing a structured framework for analyzing and comparing diverse
implementations. Our unified analysis reveals how this paradigm holistically
addresses the intertwined challenges of memory protection, management, and
processing. As a culminating case study, we re-examine the CentroID model,
demonstrating how its hybrid tagged-pointer encoding and descriptor processing
mechanisms embody the path toward practical and efficient object-aware designs.
Finally, we outline how the explicit cross-layer communication of object
semantics provides a foundational research direction for next-generation cache
hierarchies, unified virtual memory, and even 128-bit architectures.

</details>


### [12] [A Memory-Efficient Retrieval Architecture for RAG-Enabled Wearable Medical LLMs-Agents](https://arxiv.org/abs/2510.27107)
*Zhipeng Liao,Kunming Shao,Jiangnan Yu,Liang Zhao,Tim Kwang-Ting Cheng,Chi-Ying Tsui,Jie Yang,Mohamad Sawan*

Main category: cs.AR

TL;DR: 提出了一种用于边缘RAG的分层检索架构，通过两阶段检索方案显著降低能耗和内存访问，同时保持检索精度。


<details>
  <summary>Details</summary>
Motivation: 解决在边缘设备部署医疗AI代理时，RAG实现中检索阶段带来的高内存访问和能耗问题，同时保护隐私。

Method: 采用分层检索架构，结合近似检索生成候选集，然后在预选文档嵌入上进行高精度检索的两阶段方案。

Result: 在TSMC 28nm技术下，相比纯INT8检索，总体内存访问减少近50%，计算量减少75%，1MB数据检索的总能耗为177.76μJ/查询。

Conclusion: 该分层检索架构能有效降低边缘RAG的能耗和内存访问，同时维持检索准确性，适用于医疗AI代理的部署。

Abstract: With powerful and integrative large language models (LLMs), medical AI agents
have demonstrated unique advantages in providing personalized medical
consultations, continuous health monitoring, and precise treatment plans.
Retrieval-Augmented Generation (RAG) integrates personal medical documents into
LLMs by an external retrievable database to address the costly retraining or
fine-tuning issues in deploying customized agents. While deploying medical
agents in edge devices ensures privacy protection, RAG implementations impose
substantial memory access and energy consumption during the retrieval stage.
This paper presents a hierarchical retrieval architecture for edge RAG,
leveraging a two-stage retrieval scheme that combines approximate retrieval for
candidate set generation, followed by high-precision retrieval on pre-selected
document embeddings. The proposed architecture significantly reduces energy
consumption and external memory access while maintaining retrieval accuracy.
Simulation results show that, under TSMC 28nm technology, the proposed
hierarchical retrieval architecture has reduced the overall memory access by
nearly 50% and the computation by 75% compared to pure INT8 retrieval, and the
total energy consumption for 1 MB data retrieval is 177.76 {\mu}J/query.

</details>
