{"id": "2509.08207", "categories": ["cs.DC", "cs.AR", "cs.CE", "cs.PF", "C.0; C.4; C.5.1; B.8.0; D.1.3"], "pdf": "https://arxiv.org/pdf/2509.08207", "abs": "https://arxiv.org/abs/2509.08207", "authors": ["Benjamin S. Allen", "James Anchell", "Victor Anisimov", "Thomas Applencourt", "Abhishek Bagusetty", "Ramesh Balakrishnan", "Riccardo Balin", "Solomon Bekele", "Colleen Bertoni", "Cyrus Blackworth", "Renzo Bustamante", "Kevin Canada", "John Carrier", "Christopher Chan-nui", "Lance C. Cheney", "Taylor Childers", "Paul Coffman", "Susan Coghlan", "Michael D'Mello", "Murali Emani", "Kyle G. Felker", "Sam Foreman", "Olivier Franza", "Longfei Gao", "Marta Garc\u00eda", "Mar\u00eda Garzar\u00e1n", "Balazs Gerofi", "Yasaman Ghadar", "Neha Gupta", "Kevin Harms", "V\u00e4in\u00f6 Hatanp\u00e4\u00e4", "Brian Holland", "Carissa Holohan", "Brian Homerding", "Khalid Hossain", "Louise Huot", "Huda Ibeid", "Joseph A. Insley", "Sai Jayanthi", "Hong Jiang", "Wei Jiang", "Xiao-Yong Jin", "Jeongnim Kim", "Christopher Knight", "Kalyan Kumaran", "JaeHyuk Kwack", "Ti Leggett", "Ben Lenard", "Chris Lewis", "Nevin Liber", "Johann Lombardi", "Raymond M. Loy", "Ye Luo", "Bethany Lusch", "Nilakantan Mahadevan", "Victor A. Mateevitsi", "Gordon McPheeters", "Ryan Milner", "Vitali A. Morozov", "Servesh Muralidharan", "Tom Musta", "Mrigendra Nagar", "Vikram Narayana", "Marieme Ngom", "Anthony-Trung Nguyen", "Nathan Nichols", "Aditya Nishtala", "James C. Osborn", "Michael E. Papka", "Scott Parker", "Saumil S. Patel", "Adrian C. Pope", "Sucheta Raghunanda", "Esteban Rangel", "Paul M. Rich", "Silvio Rizzi", "Kris Rowe", "Varuni Sastry", "Adam Scovel", "Filippo Simini", "Haritha Siddabathuni Som", "Patrick Steinbrecher", "Rick Stevens", "Xinmin Tian", "Peter Upton", "Thomas Uram", "Archit K. Vasan", "\u00c1lvaro V\u00e1zquez-Mayagoitia", "Kaushik Velusamy", "Brice Videau", "Venkatram Vishwanath", "Brian Whitney", "Timothy J. Williams", "Michael Woodacre", "Sam Zeltner", "Gengbin Zheng", "Huihuo Zheng"], "title": "Aurora: Architecting Argonne's First Exascale Supercomputer for Accelerated Scientific Discovery", "comment": "40 pages, 10 figures. Submitted to J. Supercomputing", "summary": "Aurora is Argonne National Laboratory's pioneering Exascale supercomputer,\ndesigned to accelerate scientific discovery with cutting-edge architectural\ninnovations. Key new technologies include the Intel(TM) Xeon(TM) Data Center\nGPU Max Series (code-named Sapphire Rapids) with support for High Bandwidth\nMemory (HBM), alongside the Intel(TM) Data Center GPU Max Series (code-named\nPonte Vecchio) on each compute node. Aurora also integrates the Distributed\nAsynchronous Object Storage (DAOS), a novel exascale storage solution, and\nleverages Intel's oneAPI programming environment. This paper presents an\nin-depth exploration of Aurora's node architecture, the HPE Slingshot\ninterconnect, the supporting software ecosystem, and DAOS. We provide insights\ninto standard benchmark performance and applications readiness efforts via\nAurora's Early Science Program and the Exascale Computing Project.", "AI": {"tldr": "Aurora\u662f\u963f\u8d21\u56fd\u5bb6\u5b9e\u9a8c\u5ba4\u7684\u767e\u4ebf\u4ebf\u6b21\u8d85\u7ea7\u8ba1\u7b97\u673a\uff0c\u91c7\u7528\u82f1\u7279\u5c14Xeon Sapphire Rapids CPU\u548cPonte Vecchio GPU\uff0c\u96c6\u6210DAOS\u5b58\u50a8\u7cfb\u7edf\u548cHPE Slingshot\u4e92\u8fde\u6280\u672f\uff0c\u652f\u6301\u79d1\u5b66\u53d1\u73b0\u5e94\u7528\u3002", "motivation": "\u4e3a\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u800c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u6280\u672f\u5b9e\u73b0\u767e\u4ebf\u4ebf\u6b21\u8ba1\u7b97\u80fd\u529b\uff0c\u6ee1\u8db3\u73b0\u4ee3\u79d1\u5b66\u8ba1\u7b97\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u82f1\u7279\u5c14Xeon Sapphire Rapids CPU\uff08\u652f\u6301HBM\u9ad8\u5e26\u5bbd\u5185\u5b58\uff09\u548cPonte Vecchio GPU\u7684\u7ec4\u5408\u67b6\u6784\uff0c\u96c6\u6210\u5206\u5e03\u5f0f\u5f02\u6b65\u5bf9\u8c61\u5b58\u50a8(DAOS)\u7cfb\u7edf\uff0c\u4f7f\u7528HPE Slingshot\u4e92\u8fde\u6280\u672f\uff0c\u5e76\u57fa\u4e8eIntel oneAPI\u7f16\u7a0b\u73af\u5883\u3002", "result": "\u8bba\u6587\u6df1\u5165\u63a2\u8ba8\u4e86Aurora\u7684\u8282\u70b9\u67b6\u6784\u3001\u4e92\u8fde\u6280\u672f\u3001\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u548c\u5b58\u50a8\u7cfb\u7edf\uff0c\u63d0\u4f9b\u4e86\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u65e9\u671f\u79d1\u5b66\u8ba1\u5212\u548c\u767e\u4ebf\u4ebf\u6b21\u8ba1\u7b97\u9879\u76ee\u5c55\u793a\u4e86\u5e94\u7528\u51c6\u5907\u60c5\u51b5\u3002", "conclusion": "Aurora\u4ee3\u8868\u4e86\u8d85\u7ea7\u8ba1\u7b97\u6280\u672f\u7684\u91cd\u5927\u8fdb\u6b65\uff0c\u5176\u521b\u65b0\u7684\u786c\u4ef6\u67b6\u6784\u548c\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u4e3a\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u8ba1\u7b97\u5e73\u53f0\uff0c\u8bc1\u660e\u4e86\u767e\u4ebf\u4ebf\u6b21\u8ba1\u7b97\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u548c\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.08215", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.08215", "abs": "https://arxiv.org/abs/2509.08215", "authors": ["Bingbing Zhang", "Ziyu Lin", "Yingxin Su"], "title": "Design and Implementation of Code Completion System Based on LLM and CodeBERT Hybrid Subsystem", "comment": null, "summary": "In the rapidly evolving industry of software development, coding efficiency\nand accuracy play significant roles in delivering high-quality software.\nVarious code suggestion and completion tools, such as CodeBERT from Microsoft\nand GPT-3.5 from OpenAI, have been developed using deep learning techniques and\nintegrated into IDEs to assist software engineers' development. Research has\nshown that CodeBERT has outstanding performance in code summarization and\ncapturing code semantics, while GPT-3.5 demonstrated its adept capability at\ncode generation. This study focuses on implementing a hybrid model that\nintegrates CodeBERT and GPT-3.5 models to accomplish code suggestion and\nautocomplete tasks, leveraging the context-aware effectiveness of CodeBERT and\ntaking advantage of advanced code generation abilities of GPT-3.5. Evaluated in\nthree main metrics: accuracy, quality of generated code and performance\nefficiency with various software and hardware, the hybrid model outperforms\nbenchmarks, demonstrating its feasibility and effectiveness. Robustness testing\nfurther confirms the reliability and stability of the hybrid model. This study\nnot only emphasizes the importance of deep learning in the software development\nindustry, but also reveals the potential of synthesizing complementary deep\nlearning models to fully exploit strengths of each model.", "AI": {"tldr": "\u57fa\u4e8eCodeBERT\u548cGPT-3.5\u7684\u6df7\u5408\u6a21\u578b\u5728\u4ee3\u7801\u5efa\u8bae\u548c\u81ea\u52a8\u5b8c\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7ed3\u5408\u4e86\u4e24\u8005\u7684\u4f18\u52bf", "motivation": "\u63d0\u9ad8\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u7f16\u7801\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5229\u7528CodeBERT\u5728\u4ee3\u7801\u6458\u8981\u548c\u8bed\u4e49\u7406\u89e3\u4e0a\u7684\u4f18\u52bf\uff0c\u4ee5\u53caGPT-3.5\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b", "method": "\u5b9e\u73b0\u4e86\u4e00\u79cd\u6df7\u5408\u6a21\u578b\uff0c\u6574\u5408CodeBERT\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u548cGPT-3.5\u7684\u9ad8\u7ea7\u4ee3\u7801\u751f\u6210\u80fd\u529b", "result": "\u5728\u51c6\u786e\u6027\u3001\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u548c\u6027\u80fd\u6548\u7387\u4e09\u4e2a\u4e3b\u8981\u6307\u6807\u4e0a\u8d85\u8fc7\u4e86\u57fa\u51c6\u6a21\u578b\uff0c\u7a33\u5065\u6027\u6d4b\u8bd5\u8bc1\u660e\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u7a33\u5b9a\u6027", "conclusion": "\u6df7\u5408\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u5145\u5206\u53d1\u6325\u5404\u81ea\u4f18\u52bf\uff0c\u5728\u8f6f\u4ef6\u5f00\u53d1\u884c\u4e1a\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4e3a\u4ee3\u7801\u5efa\u8bae\u5de5\u5177\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2509.08309", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.08309", "abs": "https://arxiv.org/abs/2509.08309", "authors": ["Zizhao Mo", "Jianxiong Liao", "Huanle Xu", "Zhi Zhou", "Chengzhong Xu"], "title": "Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and Dynamic Parallelism", "comment": null, "summary": "The significant resource demands in LLM serving prompts production clusters\nto fully utilize heterogeneous hardware by partitioning LLM models across a mix\nof high-end and low-end GPUs. However, existing parallelization approaches\noften struggle to scale efficiently in heterogeneous environments due to their\ncoarse-grained and static parallelization strategies.\n  In this paper, we introduce Hetis, a new LLM system tailored for\nheterogeneous GPU clusters. Hetis addresses two critical challenges: (1) memory\ninefficiency caused by the mismatch between memory capacity and computational\npower in heterogeneous devices, and (2) computational inefficiency arising from\nperformance gaps across different LLM modules. To tackle these issues, Hetis\nemploys a fine-grained and dynamic parallelism design. Specifically, it\nselectively parallelizes compute-intensive operations to reduce latency and\ndynamically distributes Attention computations to low-end GPUs at a head\ngranularity, leveraging the distinct characteristics of each module.\nAdditionally, Hetis features an online load dispatching policy that\ncontinuously optimizes serving performance by carefully balancing network\nlatency, computational load, and memory intensity. Evaluation results\ndemonstrate that Hetis can improve serving throughput by up to $2.25\\times$ and\nreduce latency by $1.49\\times$ compared to existing systems.", "AI": {"tldr": "Hetis\u662f\u4e00\u4e2a\u9488\u5bf9\u5f02\u6784GPU\u96c6\u7fa4\u7684LLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u52a8\u6001\u5e76\u884c\u5316\u8bbe\u8ba1\u89e3\u51b3\u5185\u5b58\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u63d0\u5347\u670d\u52a1\u541e\u5410\u91cf2.25\u500d\uff0c\u964d\u4f4e\u5ef6\u8fdf1.49\u500d", "motivation": "LLM\u670d\u52a1\u9700\u8981\u5145\u5206\u5229\u7528\u5f02\u6784\u786c\u4ef6\u8d44\u6e90\uff0c\u4f46\u73b0\u6709\u5e76\u884c\u5316\u65b9\u6cd5\u5728\u5f02\u6784\u73af\u5883\u4e2d\u6269\u5c55\u6548\u7387\u4f4e\u4e0b\uff0c\u5b58\u5728\u5185\u5b58\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898", "method": "\u91c7\u7528\u7ec6\u7c92\u5ea6\u52a8\u6001\u5e76\u884c\u5316\u8bbe\u8ba1\uff1a\u9009\u62e9\u6027\u5e76\u884c\u5316\u8ba1\u7b97\u5bc6\u96c6\u578b\u64cd\u4f5c\uff0c\u4ee5\u5934\u90e8\u7c92\u5ea6\u52a8\u6001\u5206\u914dAttention\u8ba1\u7b97\u5230\u4f4e\u7aefGPU\uff0c\u5e76\u914d\u5907\u5728\u7ebf\u8d1f\u8f7d\u8c03\u5ea6\u7b56\u7565", "result": "\u76f8\u6bd4\u73b0\u6709\u7cfb\u7edf\uff0c\u670d\u52a1\u541e\u5410\u91cf\u63d0\u5347\u6700\u9ad82.25\u500d\uff0c\u5ef6\u8fdf\u964d\u4f4e1.49\u500d", "conclusion": "Hetis\u901a\u8fc7\u7cbe\u7ec6\u5316\u7684\u5f02\u6784\u8d44\u6e90\u7ba1\u7406\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u5f02\u6784GPU\u96c6\u7fa4\u4e2d\u7684\u670d\u52a1\u6548\u7387\u95ee\u9898"}}
{"id": "2509.08347", "categories": ["cs.DC", "B.m; C.4; J.0; K.0"], "pdf": "https://arxiv.org/pdf/2509.08347", "abs": "https://arxiv.org/abs/2509.08347", "authors": ["Andreas Herten", "Olga Pearce", "Filipe S. M. Guimar\u00e3es"], "title": "An HPC Benchmark Survey and Taxonomy for Characterization", "comment": null, "summary": "The field of High-Performance Computing (HPC) is defined by providing\ncomputing devices with highest performance for a variety of demanding\nscientific users. The tight co-design relationship between HPC providers and\nusers propels the field forward, paired with technological improvements,\nachieving continuously higher performance and resource utilization. A key\ndevice for system architects, architecture researchers, and scientific users\nare benchmarks, allowing for well-defined assessment of hardware, software, and\nalgorithms. Many benchmarks exist in the community, from individual niche\nbenchmarks testing specific features, to large-scale benchmark suites for whole\nprocurements. We survey the available HPC benchmarks, summarizing them in table\nform with key details and concise categorization, also through an interactive\nwebsite. For categorization, we present a benchmark taxonomy for well-defined\ncharacterization of benchmarks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u9886\u57df\u7684\u51e0\u4e4e\u6240\u6709\u91cf\u5316\u6d4b\u8bd5\u6807\u51c6\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8c03\u67e5\u548c\u5206\u7c7b\uff0c\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u8868\u683c\u603b\u7ed3\u548c\u4ea4\u4e92\u5f0f\u7f51\u7ad9\u5de5\u5177", "motivation": "\u9ad8\u6027\u80fd\u8ba1\u7b97\u9886\u57df\u9700\u8981\u51c6\u786e\u8bc4\u4f30\u786c\u4ef6\u3001\u8f6f\u4ef6\u548c\u7b97\u6cd5\u6027\u80fd\u7684\u6807\u51c6\u5316\u6d4b\u8bd5\u5de5\u5177\uff0c\u4f46\u73b0\u6709\u6d4b\u8bd5\u6807\u51c6\u5206\u6563\u4e14\u7f3a\u4e4f\u7edf\u4e00\u5206\u7c7b\u4f53\u7cfb", "method": "\u8c03\u67e5\u73b0\u6709HPC\u6d4b\u8bd5\u6807\u51c6\uff0c\u901a\u8fc7\u8868\u683c\u5f62\u5f0f\u6c47\u603b\u5173\u952e\u4fe1\u606f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6d4b\u8bd5\u6807\u51c6\u5206\u7c7b\u6cd5\u6765\u8fdb\u884c\u660e\u786e\u7684\u7279\u5f81\u5316\u63cf\u8ff0", "result": "\u5b8c\u6574\u7684HPC\u6d4b\u8bd5\u6807\u51c6\u8c03\u67e5\u7ed3\u679c\uff0c\u5305\u62ec\u4e92\u52a8\u5f0f\u7f51\u7ad9\u5de5\u5177\u548c\u8be6\u7ec6\u7684\u5206\u7c7b\u8868\u683c\uff0c\u4e3a\u7cfb\u7edf\u67b6\u6784\u5e08\u3001\u7814\u7a76\u4eba\u5458\u548c\u79d1\u5b66\u7528\u6237\u63d0\u4f9b\u4e86\u7efc\u5408\u6027\u7684\u53c2\u8003\u8d44\u6e90", "conclusion": "\u8be5\u7814\u7a76\u4e3aHPC\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6d4b\u8bd5\u6807\u51c6\u5206\u7c7b\u6846\u67b6\u548c\u5b8c\u6574\u7684\u8c03\u67e5\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u786c\u4ef6\u8f6f\u4ef6\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4fc3\u8fdbHPC\u9886\u57df\u7684\u53d1\u5c55"}}
{"id": "2509.08182", "categories": ["cs.PL", "cs.AI", "cs.CL", "03B70, 06B23, 47H10, 68T27, 68T50", "I.2.7; I.2.8; F.4.1; F.4.3; H.5.2"], "pdf": "https://arxiv.org/pdf/2509.08182", "abs": "https://arxiv.org/abs/2509.08182", "authors": ["Faruk Alpay", "Taylan Alpay"], "title": "XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols", "comment": "7 pages, multiple XML prompts", "summary": "Structured prompting with XML tags has emerged as an effective way to steer\nlarge language models (LLMs) toward parseable, schema-adherent outputs in\nreal-world systems. We develop a logic-first treatment of XML prompting that\nunifies (i) grammar-constrained decoding, (ii) fixed-point semantics over\nlattices of hierarchical prompts, and (iii) convergent human-AI interaction\nloops. We formalize a complete lattice of XML trees under a refinement order\nand prove that monotone prompt-to-prompt operators admit least fixed points\n(Knaster-Tarski) that characterize steady-state protocols; under a task-aware\ncontraction metric on trees, we further prove Banach-style convergence of\niterative guidance. We instantiate these results with context-free grammars\n(CFGs) for XML schemas and show how constrained decoding guarantees\nwell-formedness while preserving task performance. A set of multi-layer\nhuman-AI interaction recipes demonstrates practical deployment patterns,\nincluding multi-pass \"plan $\\to$ verify $\\to$ revise\" routines and agentic tool\nuse. We provide mathematically complete proofs and tie our framework to recent\nadvances in grammar-aligned decoding, chain-of-verification, and programmatic\nprompting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8eXML\u6807\u7b7e\u7684\u7ed3\u6784\u5316\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u683c\u7406\u8bba\u548c\u4e0d\u52a8\u70b9\u5b9a\u7406\u6765\u5f62\u5f0f\u5316\u5206\u6790\u63d0\u793a\u5de5\u7a0b\u7684\u6536\u655b\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u8bed\u6cd5\u7ea6\u675f\u89e3\u7801\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u683c\u5f0f\u4e0d\u53ef\u63a7\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u786e\u4fdd\u8f93\u51fa\u7b26\u5408\u9884\u5b9a\u6a21\u5f0f\uff08schema\uff09\u7684\u7ed3\u6784\u5316\u63d0\u793a\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u90e8\u7f72\u65b9\u6848\u3002", "method": "\u91c7\u7528\u903b\u8f91\u4f18\u5148\u7684\u65b9\u6cd5\uff0c\u5c06XML\u63d0\u793a\u5f62\u5f0f\u5316\u4e3a\u5b8c\u5168\u683c\u7ed3\u6784\uff0c\u5e94\u7528Knaster-Tarski\u4e0d\u52a8\u70b9\u5b9a\u7406\u548cBanach\u538b\u7f29\u6620\u5c04\u539f\u7406\u6765\u8bc1\u660e\u63d0\u793a\u64cd\u4f5c\u7684\u6536\u655b\u6027\uff0c\u5e76\u7ed3\u5408\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\u548c\u7ea6\u675f\u89e3\u7801\u6280\u672f\u3002", "result": "\u5efa\u7acb\u4e86XML\u63d0\u793a\u7684\u5b8c\u6574\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5355\u8c03\u63d0\u793a\u64cd\u4f5c\u5b58\u5728\u6700\u5c0f\u4e0d\u52a8\u70b9\uff0c\u5728\u4efb\u52a1\u611f\u77e5\u5ea6\u91cf\u4e0b\u5177\u6709\u8fed\u4ee3\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7CFG\u4fdd\u8bc1\u4e86\u8f93\u51fa\u7684\u826f\u597d\u683c\u5f0f\u548c\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7ed3\u6784\u5316\u63d0\u793a\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u6570\u5b66\u57fa\u7840\uff0c\u7edf\u4e00\u4e86\u8bed\u6cd5\u7ea6\u675f\u89e3\u7801\u3001\u56fa\u5b9a\u70b9\u8bed\u4e49\u548c\u4eba\u673a\u4ea4\u4e92\u5faa\u73af\uff0c\u4e3a\u5b9e\u9645\u7cfb\u7edf\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u8df5\u6a21\u5f0f\u3002"}}
{"id": "2509.08067", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.08067", "abs": "https://arxiv.org/abs/2509.08067", "authors": ["Rares Ifrim", "Decebal Popescu"], "title": "Analyzing the capabilities of HLS and RTL tools in the design of an FPGA Montgomery Multiplier", "comment": null, "summary": "We present the analysis of various FPGA design implementations of a\nMontgomery Modular Multiplier, compatible with the BLS12-381 elliptic curve,\nusing the Coarsely Integrated Operand Scanning approach of working with\ncomplete partial products on different digit sizes. The scope of the\nimplemented designs is to achieve a high-frequency, high-throughput solution\ncapable of computing millions of operations per second, which can provide a\nstrong foundation for different Elliptic Curve Cryptography operations such as\npoint addition and point multiplication. One important constraint for our\ndesigns was to only use FPGA DSP primitives for the arithmetic operations\nbetween digits employed in the CIOS algorithm as these primitives, when\npipelined properly, can operate at a high frequency while also relaxing the\nresource consumption of FPGA LUTs and FFs. The target of the analysis is to see\nhow different design choices and tool configurations influence the frequency,\nlatency and resource consumption when working with the latest AMD-Xilinx tools\nand Alveo FPGA boards in an RTL-HLS hybrid approach. We compare three\ncategories of designs: a Verilog naive approach where we rely on the Vivado\nsynthesizer to automatically choose when and where to use DSPs, a Verilog\noptimized approach by manually instantiating the DSP primitives ourselves and a\ncomplete High-Level Synthesis approach. We also compare the FPGA\nimplementations with an optimized software implementation of the same\nMontgomery multiplier written in Rust.", "AI": {"tldr": "\u5206\u6790\u4e86\u57fa\u4e8eCIOS\u7b97\u6cd5\u7684Montgomery\u6a21\u4e58\u6cd5\u5668\u5728FPGA\u4e0a\u7684\u591a\u79cd\u5b9e\u73b0\u65b9\u6848\uff0c\u6bd4\u8f83\u4e86Verilog\u7b80\u5355\u3001Verilog\u4f18\u5316\u548cHLS\u65b9\u6848\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u4e0eRust\u8f6f\u4ef6\u5b9e\u73b0\u8fdb\u884c\u5bf9\u6bd4", "motivation": "\u4e3aBLS12-381\u692d\u5706\u66f2\u7ebf\u5bc6\u7801\u64cd\u4f5c\u63d0\u4f9b\u9ad8\u9891\u7387\u3001\u9ad8\u541e\u5410\u91cf\u7684\u786c\u4ef6\u57fa\u7840\uff0c\u5b9e\u73b0\u6bcf\u79d2\u6570\u767e\u4e07\u6b21\u64cd\u4f5c\u7684\u76ee\u6807", "method": "\u91c7\u7528Coarsely Integrated Operand Scanning(CIOS)\u65b9\u6cd5\uff0c\u5728AMD-Xilinx\u5de5\u5177\u548cAlveo FPGA\u677f\u5361\u4e0a\u5b9e\u73b0\u4e09\u7c7b\u8bbe\u8ba1\uff1aVerilog\u7b80\u5355\u65b9\u6848\u3001\u624b\u52a8\u4f18\u5316DSP\u7684Verilog\u65b9\u6848\u3001\u9ad8\u7ea7\u7efc\u5408(HLS)\u65b9\u6848\uff0c\u5e76\u4e0eRust\u8f6f\u4ef6\u5b9e\u73b0\u8fdb\u884c\u5bf9\u6bd4", "result": "\u5206\u6790\u4e86\u4e0d\u540c\u8bbe\u8ba1\u9009\u62e9\u548c\u5de5\u5177\u914d\u7f6e\u5bf9\u9891\u7387\u3001\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u7684\u5f71\u54cd\uff0c\u4f18\u5316\u7684DSP\u4f7f\u7528\u80fd\u591f\u63d0\u9ad8\u6027\u80fd\u5e76\u51cf\u5c11LUT\u548cFF\u8d44\u6e90\u6d88\u8017", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684DSP\u539f\u8bed\u4f7f\u7528\u548c\u6d41\u6c34\u7ebf\u4f18\u5316\uff0cFPGA\u5b9e\u73b0\u80fd\u591f\u8fbe\u5230\u9ad8\u9891\u7387\u9ad8\u541e\u5410\u91cf\u7684\u76ee\u6807\uff0c\u4e3a\u692d\u5706\u66f2\u7ebf\u5bc6\u7801\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u786c\u4ef6\u57fa\u7840"}}
{"id": "2509.08409", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.08409", "abs": "https://arxiv.org/abs/2509.08409", "authors": ["Shilong Wang", "Jianchun Liu", "Hongli Xu", "Chenxia Tang", "Qianpiao Ma", "Liusheng Huang"], "title": "Towards Communication-Efficient Decentralized Federated Graph Learning over Non-IID Data", "comment": null, "summary": "Decentralized Federated Graph Learning (DFGL) overcomes potential bottlenecks\nof the parameter server in FGL by establishing a peer-to-peer (P2P)\ncommunication network among workers. However, while extensive cross-worker\ncommunication of graph node embeddings is crucial for DFGL training, it\nintroduces substantial communication costs. Most existing works typically\nconstruct sparse network topologies or utilize graph neighbor sampling methods\nto alleviate the communication overhead in DFGL. Intuitively, integrating these\nmethods may offer promise for doubly improving communication efficiency in\nDFGL. However, our preliminary experiments indicate that directly combining\nthese methods leads to significant training performance degradation if they are\njointly optimized. To address this issue, we propose Duplex, a unified\nframework that jointly optimizes network topology and graph sampling by\naccounting for their coupled relationship, thereby significantly reducing\ncommunication cost while enhancing training performance in DFGL. To overcome\npractical DFGL challenges, eg, statistical heterogeneity and dynamic network\nenvironments, Duplex introduces a learning-driven algorithm to adaptively\ndetermine optimal network topologies and graph sampling ratios for workers.\nExperimental results demonstrate that Duplex reduces completion time by\n20.1%--48.8% and communication costs by 16.7%--37.6% to achieve target\naccuracy, while improving accuracy by 3.3%--7.9% under identical resource\nbudgets compared to baselines.", "AI": {"tldr": "Duplex\u662f\u4e00\u4e2a\u8054\u5408\u4f18\u5316\u7f51\u7edc\u62d3\u6251\u548c\u56fe\u91c7\u6837\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u4e24\u8005\u7684\u8026\u5408\u5173\u7cfb\uff0c\u5728\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347DFGL\u8bad\u7ec3\u6027\u80fd\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u56fe\u5b66\u4e60(DFGL)\u867d\u7136\u907f\u514d\u4e86\u53c2\u6570\u670d\u52a1\u5668\u7684\u74f6\u9888\uff0c\u4f46\u56fe\u8282\u70b9\u5d4c\u5165\u7684\u8de8\u5de5\u4f5c\u8282\u70b9\u901a\u4fe1\u5e26\u6765\u4e86\u5de8\u5927\u7684\u901a\u4fe1\u5f00\u9500\u3002\u73b0\u6709\u65b9\u6cd5\u5355\u72ec\u4f7f\u7528\u7a00\u758f\u7f51\u7edc\u62d3\u6251\u6216\u56fe\u90bb\u5c45\u91c7\u6837\uff0c\u4f46\u76f4\u63a5\u7ec4\u5408\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51faDuplex\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u7f51\u7edc\u62d3\u6251\u548c\u56fe\u91c7\u6837\u7684\u8026\u5408\u5173\u7cfb\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002\u5f15\u5165\u5b66\u4e60\u9a71\u52a8\u7b97\u6cd5\u81ea\u9002\u5e94\u786e\u5b9a\u6700\u4f18\u7f51\u7edc\u62d3\u6251\u548c\u56fe\u91c7\u6837\u6bd4\u4f8b\uff0c\u4ee5\u5e94\u5bf9\u7edf\u8ba1\u5f02\u6784\u6027\u548c\u52a8\u6001\u7f51\u7edc\u73af\u5883\u7b49\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDuplex\u5728\u8fbe\u5230\u76ee\u6807\u7cbe\u5ea6\u65f6\u51cf\u5c11\u5b8c\u6210\u65f6\u95f420.1%-48.8%\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e16.7%-37.6%\uff0c\u5728\u76f8\u540c\u8d44\u6e90\u9884\u7b97\u4e0b\u7cbe\u5ea6\u63d0\u53473.3%-7.9%\u3002", "conclusion": "Duplex\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7f51\u7edc\u62d3\u6251\u548c\u56fe\u91c7\u6837\uff0c\u6709\u6548\u89e3\u51b3\u4e86DFGL\u4e2d\u7684\u901a\u4fe1\u6548\u7387\u95ee\u9898\uff0c\u5728\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u7684\u540c\u65f6\u63d0\u5347\u4e86\u8bad\u7ec3\u6027\u80fd\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2509.08193", "categories": ["cs.AR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.08193", "abs": "https://arxiv.org/abs/2509.08193", "authors": ["Shvetank Prakash", "Andrew Cheng", "Olof Kindgren", "Ashiq Ahamed", "Graham Knight", "Jed Kufel", "Francisco Rodriguez", "Arya Tschand", "David Kong", "Mariam Elgamal", "Jerry Huang", "Emma Chen", "Gage Hills", "Richard Price", "Emre Ozer", "Vijay Janapa Reddi"], "title": "Lifetime-Aware Design of Item-Level Intelligence", "comment": null, "summary": "We present FlexiFlow, a lifetime-aware design framework for item-level\nintelligence (ILI) where computation is integrated directly into disposable\nproducts like food packaging and medical patches. Our framework leverages\nnatively flexible electronics which offer significantly lower costs than\nsilicon but are limited to kHz speeds and several thousands of gates. Our\ninsight is that unlike traditional computing with more uniform deployment\npatterns, ILI applications exhibit 1000X variation in operational lifetime,\nfundamentally changing optimal architectural design decisions when considering\ntrillion-item deployment scales. To enable holistic design and optimization, we\nmodel the trade-offs between embodied carbon footprint and operational carbon\nfootprint based on application-specific lifetimes. The framework includes: (1)\nFlexiBench, a workload suite targeting sustainability applications from\nspoilage detection to health monitoring; (2) FlexiBits, area-optimized RISC-V\ncores with 1/4/8-bit datapaths achieving 2.65X to 3.50X better energy\nefficiency per workload execution; and (3) a carbon-aware model that selects\noptimal architectures based on deployment characteristics. We show that\nlifetime-aware microarchitectural design can reduce carbon footprint by 1.62X,\nwhile algorithmic decisions can reduce carbon footprint by 14.5X. We validate\nour approach through the first tape-out using a PDK for flexible electronics\nwith fully open-source tools, achieving 30.9kHz operation. FlexiFlow enables\nexploration of computing at the Extreme Edge where conventional design\nmethodologies must be reevaluated to account for new constraints and\nconsiderations.", "AI": {"tldr": "FlexiFlow\u662f\u4e00\u4e2a\u9762\u5411\u7269\u54c1\u7ea7\u667a\u80fd(ILI)\u7684\u751f\u547d\u5468\u671f\u611f\u77e5\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u67d4\u6027\u7535\u5b50\u6280\u672f\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u53ef\u6301\u7eed\u7684\u8fb9\u7f18\u8ba1\u7b97\uff0c\u5728\u78b3\u8db3\u8ff9\u4f18\u5316\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6548\u679c", "motivation": "\u4f20\u7edf\u8ba1\u7b97\u67b6\u6784\u65e0\u6cd5\u6ee1\u8db3\u4e07\u4ebf\u7ea7\u90e8\u7f72\u7684\u7269\u54c1\u7ea7\u667a\u80fd\u5e94\u7528\u9700\u6c42\uff0c\u8fd9\u4e9b\u5e94\u7528\u5b58\u57281000\u500d\u7684\u64cd\u4f5c\u5bff\u547d\u5dee\u5f02\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u67b6\u6784\u8bbe\u8ba1\u51b3\u7b56", "method": "\u63d0\u51fa\u5305\u542bFlexiBench\u5de5\u4f5c\u8d1f\u8f7d\u5957\u4ef6\u3001FlexiBits\u9762\u79ef\u4f18\u5316RISC-V\u6838\u5fc3(1/4/8\u4f4d\u6570\u636e\u8def\u5f84)\u548c\u78b3\u611f\u77e5\u6a21\u578b\u7684\u6574\u4f53\u6846\u67b6\uff0c\u4f7f\u7528\u67d4\u6027\u7535\u5b50PDK\u8fdb\u884c\u6d41\u7247\u9a8c\u8bc1", "result": "\u5b9e\u73b0\u4e862.65-3.50\u500d\u7684\u80fd\u6548\u63d0\u5347\uff0c\u751f\u547d\u5468\u671f\u611f\u77e5\u5fae\u67b6\u6784\u8bbe\u8ba1\u51cf\u5c11\u78b3\u8db3\u8ff91.62\u500d\uff0c\u7b97\u6cd5\u51b3\u7b56\u51cf\u5c11\u78b3\u8db3\u8ff914.5\u500d\uff0c\u8fbe\u523030.9kHz\u64cd\u4f5c\u9891\u7387", "conclusion": "FlexiFlow\u4e3a\u6781\u7aef\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u65b9\u6cd5\u8bba\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u4f20\u7edf\u8bbe\u8ba1\u65b9\u6cd5\u4ee5\u9002\u5e94\u65b0\u7684\u7ea6\u675f\u6761\u4ef6"}}
{"id": "2509.08608", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.08608", "abs": "https://arxiv.org/abs/2509.08608", "authors": ["Yichao Zhang", "Marco Bertuletti", "Sergio Mazzola", "Samuel Riedel", "Luca Benini"], "title": "A 410GFLOP/s, 64 RISC-V Cores, 204.8GBps Shared-Memory Cluster in 12nm FinFET with Systolic Execution Support for Efficient B5G/6G AI-Enhanced O-RAN", "comment": null, "summary": "We present HeartStream, a 64-RV-core shared-L1-memory cluster (410 GFLOP/s\npeak performance and 204.8 GBps L1 bandwidth) for energy-efficient AI-enhanced\nO-RAN. The cores and cluster architecture are customized for baseband\nprocessing, supporting complex (16-bit real&imaginary) instructions:\nmultiply&accumulate, division&square-root, SIMD instructions, and\nhardware-managed systolic queues, improving up to 1.89x the energy efficiency\nof key baseband kernels. At 800MHz@0.8V, HeartStream delivers up to 243GFLOP/s\non complex-valued wireless workloads. Furthermore, the cores also support\nefficient AI processing on received data at up to 72 GOP/s. HeartStream is\nfully compatible with base station power and processing latency limits: it\nachieves leading-edge software-defined PUSCH efficiency (49.6GFLOP/s/W) and\nconsumes just 0.68W (645MHz@0.65V), within the 4 ms end-to-end constraint for\nB5G/6G uplink.", "AI": {"tldr": "HeartStream\u662f\u4e00\u4e2a64\u6838\u5171\u4eabL1\u5185\u5b58\u96c6\u7fa4\uff0c\u4e13\u4e3aAI\u589e\u5f3a\u7684O-RAN\u8bbe\u8ba1\uff0c\u5728\u57fa\u5e26\u5904\u7406\u548cAI\u8ba1\u7b97\u65b9\u9762\u5177\u6709\u9ad8\u80fd\u6548\u8868\u73b0\uff0c\u6ee1\u8db35G/6G\u4e0a\u884c\u94fe\u8def\u7684\u65f6\u95f4\u548c\u529f\u8017\u7ea6\u675f\u3002", "motivation": "\u4e3a\u6ee1\u8db3B5G/6G\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u5bf9\u9ad8\u6548\u80fd\u57fa\u5e26\u5904\u7406\u548cAI\u589e\u5f3a\u529f\u80fd\u7684\u9700\u6c42\uff0c\u9700\u8981\u8bbe\u8ba1\u4e13\u95e8\u7684\u786c\u4ef6\u67b6\u6784\u6765\u63d0\u5347\u80fd\u6548\u5e76\u6ee1\u8db3\u4e25\u683c\u7684\u529f\u8017\u548c\u5ef6\u8fdf\u8981\u6c42\u3002", "method": "\u8bbe\u8ba164\u6838RV\u6838\u5fc3\u96c6\u7fa4\uff0c\u91c7\u7528\u5171\u4eabL1\u5185\u5b58\u67b6\u6784\uff0c\u652f\u6301\u590d\u6570\u6307\u4ee4\uff0816\u4f4d\u5b9e\u90e8\u548c\u865a\u90e8\uff09\u3001\u4e58\u79ef\u7d2f\u52a0\u3001\u9664\u6cd5\u548c\u5e73\u65b9\u6839\u3001SIMD\u6307\u4ee4\uff0c\u4ee5\u53ca\u786c\u4ef6\u7ba1\u7406\u7684\u8109\u52a8\u961f\u5217\u3002", "result": "\u5728800MHz@0.8V\u4e0b\u63d0\u4f9b243GFLOP/s\u7684\u590d\u6570\u65e0\u7ebf\u5de5\u4f5c\u8d1f\u8f7d\u5904\u7406\u80fd\u529b\uff0cAI\u5904\u7406\u8fbe72GOP/s\uff0c\u8f6f\u4ef6\u5b9a\u4e49PUSCH\u6548\u7387\u8fbe49.6GFLOP/s/W\uff0c\u529f\u8017\u4ec50.68W(645MHz@0.65V)\u3002", "conclusion": "HeartStream\u5728\u57fa\u5e26\u5904\u7406\u5173\u952e\u5185\u6838\u4e0a\u80fd\u6548\u63d0\u5347\u8fbe1.89\u500d\uff0c\u5b8c\u5168\u517c\u5bb9\u57fa\u7ad9\u529f\u7387\u548c\u5904\u7406\u5ef6\u8fdf\u9650\u5236\uff0c\u6ee1\u8db34ms\u7aef\u5230\u7aefB5G/6G\u4e0a\u884c\u94fe\u8def\u7ea6\u675f\uff0c\u662f\u9ad8\u6548\u7684AI\u589e\u5f3aO-RAN\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08405", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.08405", "abs": "https://arxiv.org/abs/2509.08405", "authors": ["Chengzhen Meng", "Xiuzhuang Chen", "Hongjun Dai"], "title": "FASE: FPGA-Assisted Syscall Emulation for Rapid End-to-End Processor Performance Validation", "comment": "14 pages, 19 figures, to be submitted to IEEE TCAD", "summary": "The rapid advancement of AI workloads and domain-specific architectures has\nled to increasingly diverse processor microarchitectures, whose design\nexploration requires fast and accurate performance validation. However,\ntraditional workflows defer validation process until RTL design and SoC\nintegration are complete, significantly prolonging development and iteration\ncycle.\n  In this work, we present FASE framework, FPGA-Assisted Syscall Emulation, the\nfirst work for adapt syscall emulation on FPGA platforms, enabling complex\nmulti-thread benchmarks to directly run on the processor design without\nintegrating SoC or target OS for early-stage performance validation. FASE\nintroduces three key innovations to address three critical challenges for\nadapting FPGA-based syscall emulation: (1) only a minimal CPU interface is\nexposed, with other hardware components untouched, addressing the lack of a\nunified hardware interface in FPGA systems; (2) a Host-Target Protocol (HTP) is\nproposed to minimize cross-device data traffic, mitigating the low-bandwidth\nand high-latency communication between FPGA and host; and (3) a host-side\nruntime is proposed to remotely handle Linux-style system calls, addressing the\nchallenge of cross-device syscall delegation.\n  Experiments ware conducted on Xilinx FPGA with open-sourced RISC-V SMP\nprocessor Rocket. With single-thread CoreMark, FASE introduces less than 1%\nperformance error and achieves over 2000x higher efficiency compared to Proxy\nKernel due to FPGA acceleration. With complex OpenMP benchmarks, FASE\ndemonstrates over 96% performance validation accuracy for most single-thread\nworkloads and over 91.5% for most multi-thread workloads compared to full SoC\nvalidation, significantly reducing development complexity and time-to-feedback.\nAll components of FASE framework are released as open-source.", "AI": {"tldr": "FASE\u6846\u67b6\u662f\u9996\u4e2a\u5728FPGA\u5e73\u53f0\u4e0a\u5b9e\u73b0\u7cfb\u7edf\u8c03\u7528\u4eff\u771f\u7684\u5de5\u4f5c\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u786c\u4ef6\u63a5\u53e3\u3001\u4f18\u5316\u901a\u4fe1\u534f\u8bae\u548c\u8fdc\u7a0b\u7cfb\u7edf\u8c03\u7528\u5904\u7406\uff0c\u4f7f\u590d\u6742\u591a\u7ebf\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u80fd\u76f4\u63a5\u5728\u5904\u7406\u5668\u8bbe\u8ba1\u4e0a\u8fd0\u884c\uff0c\u65e0\u9700\u96c6\u6210SoC\u6216\u76ee\u6807\u64cd\u4f5c\u7cfb\u7edf\uff0c\u5b9e\u73b0\u65e9\u671f\u6027\u80fd\u9a8c\u8bc1\u3002", "motivation": "\u4f20\u7edf\u5de5\u4f5c\u6d41\u7a0b\u5c06\u9a8c\u8bc1\u8fc7\u7a0b\u63a8\u8fdf\u5230RTL\u8bbe\u8ba1\u548cSoC\u96c6\u6210\u5b8c\u6210\u540e\uff0c\u663e\u8457\u5ef6\u957f\u4e86\u5f00\u53d1\u548c\u8fed\u4ee3\u5468\u671f\u3002AI\u5de5\u4f5c\u8d1f\u8f7d\u548c\u9886\u57df\u7279\u5b9a\u67b6\u6784\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u5904\u7406\u5668\u5fae\u67b6\u6784\u65e5\u76ca\u591a\u6837\u5316\uff0c\u9700\u8981\u5feb\u901f\u51c6\u786e\u7684\u6027\u80fd\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFASE\u6846\u67b6\uff08FPGA\u8f85\u52a9\u7cfb\u7edf\u8c03\u7528\u4eff\u771f\uff09\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u4ec5\u66b4\u9732\u6700\u5c0fCPU\u63a5\u53e3\uff1b2\uff09\u63d0\u51fa\u4e3b\u673a-\u76ee\u6807\u534f\u8bae\uff08HTP\uff09\u6700\u5c0f\u5316\u8de8\u8bbe\u5907\u6570\u636e\u6d41\u91cf\uff1b3\uff09\u63d0\u51fa\u4e3b\u673a\u7aef\u8fd0\u884c\u65f6\u8fdc\u7a0b\u5904\u7406Linux\u98ce\u683c\u7cfb\u7edf\u8c03\u7528\u3002", "result": "\u5728Xilinx FPGA\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5355\u7ebf\u7a0bCoreMark\u6027\u80fd\u8bef\u5dee\u5c0f\u4e8e1%\uff0c\u76f8\u6bd4Proxy Kernel\u6548\u7387\u63d0\u53472000\u500d\u4ee5\u4e0a\u3002\u590d\u6742OpenMP\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u5927\u591a\u6570\u5355\u7ebf\u7a0b\u5de5\u4f5c\u8d1f\u8f7d\u9a8c\u8bc1\u51c6\u786e\u7387\u8d85\u8fc796%\uff0c\u591a\u7ebf\u7a0b\u5de5\u4f5c\u8d1f\u8f7d\u8d85\u8fc791.5%\u3002", "conclusion": "FASE\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u5f00\u53d1\u590d\u6742\u6027\u548c\u53cd\u9988\u65f6\u95f4\uff0c\u6240\u6709\u7ec4\u4ef6\u5747\u5df2\u5f00\u6e90\u53d1\u5e03\uff0c\u4e3a\u65e9\u671f\u9636\u6bb5\u6027\u80fd\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08770", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.08770", "abs": "https://arxiv.org/abs/2509.08770", "authors": ["Muhammad Ali Jamshed", "Muhammad Ahmed Mohsin", "Hongliang Zhang", "Bushra Haq", "Aryan Kaushik", "Boya Di", "Weiwei Jiang"], "title": "Reconfigurable Holographic Surfaces and Near Field Communication for Non-Terrestrial Networks: Potential and Challenges", "comment": "SUBMITTED TO IEEE WIRELESS COMMUNICATION MAGAZINE", "summary": "To overcome the challenges of ultra-low latency, ubiquitous coverage, and\nsoaring data rates, this article presents a combined use of Near Field\nCommunication (NFC) and Reconfigurable Holographic Surfaces (RHS) for\nNon-Terrestrial Networks (NTN). A system architecture has been presented, which\nshows that the integration of RHS with NTN platforms such as satellites, High\nAltitute Platform Stations (HAPS), and Uncrewed Aerial Vehicles (UAV) can\nachieve precise beamforming and intelligent wavefront control in near-field\nregions, enhancing Energy Efficiency (EE), spectral utilization, and spatial\nresolution. Moreover, key applications, challenges, and future directions have\nbeen identified to fully adopt this integration. In addition, a use case\nanalysis has been presented to improve the EE of the system in a public safety\nuse case scenario, further strengthening the UAV-RHS fusion.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fd1\u573a\u901a\u4fe1(NFC)\u548c\u53ef\u91cd\u6784\u5168\u606f\u8868\u9762(RHS)\u7684\u65b9\u6848\uff0c\u7528\u4e8e\u6539\u5584\u975e\u5730\u9762\u7f51\u7edc(NTN)\u7684\u6027\u80fd\uff0c\u5305\u62ec\u536b\u661f\u3001\u9ad8\u7a7a\u5e73\u53f0\u548c\u65e0\u4eba\u673a\u7b49\u5e73\u53f0\u3002", "motivation": "\u5e94\u5bf9\u8d85\u4f4e\u5ef6\u8fdf\u3001\u6d41\u52a8\u8986\u76d6\u548c\u9ad8\u6570\u636e\u901f\u7387\u7684\u6311\u6218\uff0c\u63d0\u5347\u975e\u5730\u9762\u7f51\u7edc\u7684\u80fd\u91cf\u6548\u7387\u3001\u8c31\u6548\u7387\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7cfb\u7edf\u67b6\u6784\uff0c\u5c06RHS\u4e0eNTN\u5e73\u53f0(\u536b\u661f\u3001HAPS\u3001UAV)\u96c6\u6210\uff0c\u5b9e\u73b0\u8fd1\u573a\u533a\u57df\u7684\u7cbe\u786e\u653e\u5f62\u548c\u667a\u80fd\u6ce2\u524d\u6cbf\u63a7\u5236\u3002", "result": "\u8bc1\u660e\u8be5\u96c6\u6210\u65b9\u6848\u80fd\u591f\u63d0\u9ad8\u80fd\u91cf\u6548\u7387\u3001\u8c31\u6548\u7387\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u5e76\u901a\u8fc7\u516c\u5171\u5b89\u5168\u573a\u666f\u7684\u7528\u4f8b\u5206\u6790\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86UAV-RHS\u878d\u5408\u7684\u4f18\u52bf\u3002", "conclusion": "\u8fd9\u79cd\u65b0\u9898\u96c6\u6210\u65b9\u6848\u4e3a\u975e\u5730\u9762\u7f51\u7edc\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6027\u80fd\u63d0\u5347\u9014\u5f84\uff0c\u5e76\u6301\u7eed\u63a2\u7d22\u5176\u5e94\u7528\u6f5c\u529b\u3001\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2509.08416", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.08416", "abs": "https://arxiv.org/abs/2509.08416", "authors": ["Yan Tan", "Xiangchen Meng", "Zijun Jiang", "Yangdi Lyu"], "title": "AutoVeriFix: Automatically Correcting Errors and Enhancing Functional Correctness in LLM-Generated Verilog Code", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\ngenerating software code for high-level programming languages such as Python\nand C++. However, their application to hardware description languages, such as\nVerilog, is challenging due to the scarcity of high-quality training data.\nCurrent approaches to Verilog code generation using LLMs often focus on\nsyntactic correctness, resulting in code with functional errors. To address\nthese challenges, we present AutoVeriFix, a novel Python-assisted two-stage\nframework designed to enhance the functional correctness of LLM-generated\nVerilog code. In the first stage, LLMs are employed to generate high-level\nPython reference models that define the intended circuit behavior. In the\nsecond stage, these Python models facilitate the creation of automated tests\nthat guide the generation of Verilog RTL implementations. Simulation\ndiscrepancies between the reference model and the Verilog code are iteratively\nused to identify and correct errors, thereby improving the functional accuracy\nand reliability of the LLM-generated Verilog code. Experimental results\ndemonstrate that our approach significantly outperforms existing\nstate-of-the-art methods in improving the functional correctness of generated\nVerilog code.", "AI": {"tldr": "AutoVeriFix\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7Python\u53c2\u8003\u6a21\u578b\u548c\u81ea\u52a8\u5316\u6d4b\u8bd5\u6765\u63d0\u5347LLM\u751f\u6210\u7684Verilog\u4ee3\u7801\u529f\u80fd\u6b63\u786e\u6027", "motivation": "\u89e3\u51b3LLM\u5728\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00(\u5982Verilog)\u4ee3\u7801\u751f\u6210\u4e2d\u56e0\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u7684\u529f\u80fd\u9519\u8bef\u95ee\u9898", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u7528LLM\u751f\u6210Python\u53c2\u8003\u6a21\u578b\u5b9a\u4e49\u7535\u8def\u884c\u4e3a\uff0c\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8ePython\u6a21\u578b\u521b\u5efa\u81ea\u52a8\u5316\u6d4b\u8bd5\u6765\u6307\u5bfcVerilog RTL\u5b9e\u73b0\uff0c\u901a\u8fc7\u4eff\u771f\u5dee\u5f02\u8fed\u4ee3\u4fee\u6b63\u9519\u8bef", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u751f\u6210Verilog\u4ee3\u7801\u529f\u80fd\u6b63\u786e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "AutoVeriFix\u901a\u8fc7Python\u8f85\u52a9\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u751f\u6210Verilog\u4ee3\u7801\u7684\u529f\u80fd\u6b63\u786e\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u4ee3\u7801\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027"}}
{"id": "2509.08542", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.08542", "abs": "https://arxiv.org/abs/2509.08542", "authors": ["Wenlun Zhang", "Xinyu Li", "Shimpei Ando", "Kentaro Yoshioka"], "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter 1.58-bit LLM Inference", "comment": "Accepted to ASP-DAC 2026", "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.", "AI": {"tldr": "BitROM\u662f\u9996\u4e2a\u57fa\u4e8eCiROM\u7684LLM\u52a0\u901f\u5668\uff0c\u901a\u8fc71.58\u4f4d\u91cf\u5316\u3001\u53cc\u5411ROM\u9635\u5217\u548c\u4e09\u6a21\u5f0f\u7d2f\u52a0\u5668\u7b49\u521b\u65b0\uff0c\u572865nm\u5de5\u827a\u4e0b\u5b9e\u73b020.8 TOPS/W\u80fd\u6548\u548c4,967 kB/mm\u00b2\u7684\u6bd4\u7279\u5bc6\u5ea6\uff0c\u6bd4\u73b0\u6709\u6570\u5b57CiROM\u8bbe\u8ba1\u63d0\u534710\u500d\u9762\u79ef\u6548\u7387\u3002", "motivation": "\u4f20\u7edfCiROM\u52a0\u901f\u5668\u7531\u4e8eLLM\u53c2\u6570\u91cf\u5de8\u5927\u800c\u96be\u4ee5\u6269\u5c55\uff0c\u5982LLaMA-7B\u5728\u5148\u8fdbCMOS\u8282\u70b9\u4e0b\u9700\u8981\u8d85\u8fc71,000 cm\u00b2\u7684\u7845\u9762\u79ef\uff0c\u9650\u5236\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "1) \u65b0\u9896\u7684\u53cc\u5411ROM\u9635\u5217\uff0c\u6bcf\u4e2a\u6676\u4f53\u7ba1\u5b58\u50a8\u4e24\u4e2a\u4e09\u5143\u6743\u91cd\uff1b2) \u9488\u5bf9\u4e09\u5143\u6743\u91cd\u8ba1\u7b97\u4f18\u5316\u7684\u4e09\u6a21\u5f0f\u672c\u5730\u7d2f\u52a0\u5668\uff1b3) \u96c6\u6210\u89e3\u7801-\u5237\u65b0eDRAM\u652f\u6301\u7247\u4e0aKV\u7f13\u5b58\u7ba1\u7406\uff1b4) \u96c6\u6210LoRA\u9002\u914d\u5668\u652f\u6301\u8de8\u4e0b\u6e38\u4efb\u52a1\u7684\u9ad8\u6548\u8fc1\u79fb\u5b66\u4e60\u3002", "result": "\u572865nm CMOS\u5de5\u827a\u4e0b\u5b9e\u73b020.8 TOPS/W\u7684\u80fd\u6548\u548c4,967 kB/mm\u00b2\u7684\u6bd4\u7279\u5bc6\u5ea6\uff0c\u9762\u79ef\u6548\u7387\u6bd4\u73b0\u6709\u6570\u5b57CiROM\u8bbe\u8ba1\u63d0\u534710\u500d\uff0cDR eDRAM\u51cf\u5c1143.6%\u7684\u5916\u90e8DRAM\u8bbf\u95ee\u3002", "conclusion": "BitROM\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u548c\u591a\u9879\u521b\u65b0\uff0c\u6210\u529f\u514b\u670d\u4e86CiROM\u52a0\u901f\u5668\u5728LLM\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u4e3a\u8fb9\u7f18\u5e94\u7528\u4e2d\u7684\u9ad8\u6548LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
