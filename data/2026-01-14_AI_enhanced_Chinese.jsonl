{"id": "2601.08529", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.08529", "abs": "https://arxiv.org/abs/2601.08529", "authors": ["Thomas Bagrel"], "title": "Formalization and Implementation of Safe Destination Passing in Pure Functional Programming Settings", "comment": "PhD Manuscript, 148 pages. Sources: https://github.com/tweag/tbagrel-phd-manuscript/", "summary": "Destination-passing style programming introduces destinations, which represent the address of a write-once memory cell. These destinations can be passed as function parameters, allowing the caller to control memory management: the callee simply fills the cell instead of allocating space for a return value. While typically used in systems programming, destination passing also has applications in pure functional programming, where it enables programs that were previously unexpressible using usual immutable data structures.\n  In this thesis, we develop a core \u03bb-calculus with destinations, {\u03bb_d}. Our new calculus is more expressive than similar existing systems, with destination passing designed to be as flexible as possible. This is achieved through a modal type system combining linear types with a system of ages to manage scopes, in order to make destination-passing safe. Type safety of our core calculus was proved formally with the Coq proof assistant.\n  Then, we see how this core calculus can be adapted into an existing pure functional language, Haskell, whose type system is less powerful than our custom theoretical one. Retaining safety comes at the cost of removing some flexibility in the handling of destinations. We later refine the implementation to recover much of this flexibility, at the cost of increased user complexity.\n  The prototype implementation in Haskell shows encouraging results for adopting destination-passing style programming when traversing or mapping over large data structures such as lists or data trees.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e26\u6709\u76ee\u7684\u5730\u7684\u03bb\u6f14\u7b97\u7cfb\u7edf{\u03bb_d}\uff0c\u901a\u8fc7\u6a21\u6001\u7c7b\u578b\u7cfb\u7edf\u7ed3\u5408\u7ebf\u6027\u7c7b\u578b\u548c\u5e74\u9f84\u7cfb\u7edf\u6765\u5b89\u5168\u7ba1\u7406\u76ee\u7684\u5730\u4f20\u9012\uff0c\u5e76\u5728Haskell\u4e2d\u5b9e\u73b0\u539f\u578b\uff0c\u5c55\u793a\u4e86\u5728\u5904\u7406\u5927\u578b\u6570\u636e\u7ed3\u6784\u65f6\u7684\u4f18\u52bf\u3002", "motivation": "\u76ee\u7684\u5730\u4f20\u9012\u98ce\u683c\u7f16\u7a0b\u5141\u8bb8\u8c03\u7528\u8005\u63a7\u5236\u5185\u5b58\u7ba1\u7406\uff0c\u5728\u7eaf\u51fd\u6570\u5f0f\u7f16\u7a0b\u4e2d\u80fd\u591f\u5b9e\u73b0\u4f20\u7edf\u4e0d\u53ef\u53d8\u6570\u636e\u7ed3\u6784\u65e0\u6cd5\u8868\u8fbe\u7684\u7a0b\u5e8f\u3002\u7136\u800c\u73b0\u6709\u7cfb\u7edf\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u4e14\u5b89\u5168\u7684\u76ee\u7684\u5730\u4f20\u9012\u673a\u5236\u3002", "method": "\u5f00\u53d1\u6838\u5fc3\u03bb\u6f14\u7b97{\u03bb_d}\uff0c\u91c7\u7528\u6a21\u6001\u7c7b\u578b\u7cfb\u7edf\u7ed3\u5408\u7ebf\u6027\u7c7b\u578b\u548c\u5e74\u9f84\u7cfb\u7edf\u6765\u7ba1\u7406\u4f5c\u7528\u57df\uff0c\u786e\u4fdd\u76ee\u7684\u5730\u4f20\u9012\u7684\u5b89\u5168\u6027\u3002\u7136\u540e\u5728Haskell\u4e2d\u5b9e\u73b0\u8be5\u6f14\u7b97\uff0c\u901a\u8fc7\u7c7b\u578b\u7cfb\u7edf\u9002\u914d\u548c\u5b9e\u73b0\u4f18\u5316\u6765\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u7075\u6d3b\u6027\u3002", "result": "\u6838\u5fc3\u6f14\u7b97\u7684\u7c7b\u578b\u5b89\u5168\u6027\u5728Coq\u4e2d\u5f62\u5f0f\u5316\u8bc1\u660e\u3002Haskell\u539f\u578b\u5b9e\u73b0\u8868\u660e\uff0c\u5728\u904d\u5386\u6216\u6620\u5c04\u5927\u578b\u6570\u636e\u7ed3\u6784\uff08\u5982\u5217\u8868\u6216\u6570\u636e\u6811\uff09\u65f6\uff0c\u76ee\u7684\u5730\u4f20\u9012\u98ce\u683c\u7f16\u7a0b\u80fd\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "\u76ee\u7684\u5730\u4f20\u9012\u98ce\u683c\u7f16\u7a0b\u5728\u7eaf\u51fd\u6570\u5f0f\u8bed\u8a00\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7c7b\u578b\u7cfb\u7edf\u53ef\u4ee5\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u7075\u6d3b\u6027\u3002\u867d\u7136Haskell\u5b9e\u73b0\u9700\u8981\u5728\u5b89\u5168\u6027\u548c\u6613\u7528\u6027\u4e4b\u95f4\u6743\u8861\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u6548\u679c\u4ee4\u4eba\u9f13\u821e\u3002"}}
{"id": "2601.08368", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.08368", "abs": "https://arxiv.org/abs/2601.08368", "authors": ["Marie Bolzer", "S\u00e9bastien Duval", "Marine Minier"], "title": "A New Tool to Find Lightweight (And, Xor) Implementations of Quadratic Vectorial Boolean Functions up to Dimension 9", "comment": null, "summary": "The problem of finding a minimal circuit to implement a given function is one of the oldest in electronics. It is known to be NP-hard. Still, many tools exist to find sub-optimal circuits to implement a function. In electronics, such tools are known as synthesisers. However, these synthesisers aim to implement very large functions (a whole electronic chip). In cryptography, the focus is on small functions, hence the necessity for new dedicated tools for small functions. Several tools exist to implement small functions. They differ by their algorithmic approach (some are based on Depth-First-Search as introduced by Ullrich in 2011, some are based on SAT-solvers like the tool desgined by Stoffelen in 2016, some non-generic tools use subfield decomposition) and by their optimisation criteria (some optimise for circuit size, others for circuit depth, and some for side-channel-protected implementations). However, these tools are limited to functions operating on less than 5 bits, sometimes 6 bits for quadratic functions, or to very simple functions. The limitation lies in a high computing time. We propose a new tool (The tool is provided alongside the IEEE article with CodeOcean and at https://github.com/seduval/implem-quad-sbox) to implement quadratic functions up to 9 bits within AND-depth 1, minimising the number of AND gates. This tool is more time-efficient than previous ones, allowing to explore larger implementations than others on 6 bits or less and allows to reach larger sizes, up to 9 bits.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u5de5\u5177\u5b9e\u73b09\u4f4d\u4ee5\u5185\u4e8c\u6b21\u51fd\u6570\u7684AND-depth 1\u7535\u8def\uff0c\u4f18\u5316AND\u95e8\u6570\u91cf\uff0c\u6bd4\u73b0\u6709\u5de5\u5177\u66f4\u9ad8\u6548", "motivation": "\u73b0\u6709\u7535\u8def\u7efc\u5408\u5de5\u5177\u4e3b\u8981\u9488\u5bf9\u5927\u578b\u7535\u5b50\u82af\u7247\u8bbe\u8ba1\uff0c\u800c\u5bc6\u7801\u5b66\u9700\u8981\u5c0f\u578b\u51fd\u6570\u5b9e\u73b0\u5de5\u5177\u3002\u73b0\u6709\u5de5\u5177\u5c40\u9650\u4e8e5-6\u4f4d\u51fd\u6570\uff0c\u8ba1\u7b97\u65f6\u95f4\u957f\uff0c\u65e0\u6cd5\u5904\u7406\u66f4\u5927\u89c4\u6a21", "method": "\u5f00\u53d1\u65b0\u5de5\u5177\uff0c\u91c7\u7528\u7279\u5b9a\u7b97\u6cd5\u5b9e\u73b09\u4f4d\u4ee5\u5185\u4e8c\u6b21\u51fd\u6570\u7684AND-depth 1\u7535\u8def\uff0c\u6700\u5c0f\u5316AND\u95e8\u6570\u91cf\uff0c\u5de5\u5177\u901a\u8fc7CodeOcean\u548cGitHub\u63d0\u4f9b", "result": "\u65b0\u5de5\u5177\u6bd4\u73b0\u6709\u5de5\u5177\u65f6\u95f4\u6548\u7387\u66f4\u9ad8\uff0c\u80fd\u57286\u4f4d\u6216\u66f4\u5c0f\u89c4\u6a21\u4e0a\u63a2\u7d22\u66f4\u5927\u5b9e\u73b0\uff0c\u5e76\u80fd\u5904\u7406\u9ad8\u8fbe9\u4f4d\u7684\u51fd\u6570", "conclusion": "\u65b0\u5de5\u5177\u586b\u8865\u4e86\u5bc6\u7801\u5b66\u4e2d\u5c0f\u578b\u51fd\u6570\u7535\u8def\u7efc\u5408\u7684\u7a7a\u767d\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u5de5\u5177\u5728\u51fd\u6570\u4f4d\u6570\u548c\u8ba1\u7b97\u65f6\u95f4\u4e0a\u7684\u9650\u5236"}}
{"id": "2601.08025", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.08025", "abs": "https://arxiv.org/abs/2601.08025", "authors": ["Adiba Masud", "Nicholas Foley", "Pragathi Durga Rajarajan", "Palden Lama"], "title": "Where to Split? A Pareto-Front Analysis of DNN Partitioning for Edge Inference", "comment": null, "summary": "The deployment of deep neural networks (DNNs) on resource-constrained edge devices is frequently hindered by their significant computational and memory requirements. While partitioning and distributing a DNN across multiple devices is a well-established strategy to mitigate this challenge, prior research has largely focused on single-objective optimization, such as minimizing latency or maximizing throughput. This paper challenges that view by reframing DNN partitioning as a multi-objective optimization problem. We argue that in real-world scenarios, a complex trade-off between latency and throughput exists, which is further complicated by network variability. To address this, we introduce ParetoPipe, an open-source framework that leverages Pareto front analysis to systematically identify optimal partitioning strategies that balance these competing objectives.\n  Our contributions are threefold: we benchmark pipeline partitioned inference on a heterogeneous testbed of Raspberry Pis and a GPU-equipped edge server; we identify Pareto-optimal points to analyze the latency-throughput trade-off under varying network conditions; and we release a flexible, open-source framework to facilitate distributed inference and benchmarking. This toolchain features dual communication backends, PyTorch RPC and a custom lightweight implementation, to minimize overhead and support broad experimentation.", "AI": {"tldr": "ParetoPipe\uff1a\u4e00\u4e2a\u57fa\u4e8e\u5e15\u7d2f\u6258\u524d\u6cbf\u5206\u6790\u7684\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5e73\u8861DNN\u63a8\u7406\u7684\u5ef6\u8fdf\u4e0e\u541e\u5410\u91cf\u6743\u8861\u3002", "motivation": "\u73b0\u6709DNN\u5206\u533a\u7814\u7a76\u5927\u591a\u5173\u6ce8\u5355\u76ee\u6807\u4f18\u5316\uff08\u5982\u6700\u5c0f\u5316\u5ef6\u8fdf\u6216\u6700\u5927\u5316\u541e\u5410\u91cf\uff09\uff0c\u4f46\u5b9e\u9645\u8fb9\u7f18\u90e8\u7f72\u4e2d\u9700\u8981\u5728\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u4e4b\u95f4\u8fdb\u884c\u590d\u6742\u6743\u8861\uff0c\u4e14\u7f51\u7edc\u6761\u4ef6\u53d8\u5316\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51faParetoPipe\u6846\u67b6\uff0c\u5c06DNN\u5206\u533a\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5229\u7528\u5e15\u7d2f\u6258\u524d\u6cbf\u5206\u6790\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u6700\u4f18\u5206\u533a\u7b56\u7565\u3002\u6846\u67b6\u5305\u542b\u53cc\u901a\u4fe1\u540e\u7aef\uff08PyTorch RPC\u548c\u81ea\u5b9a\u4e49\u8f7b\u91cf\u7ea7\u5b9e\u73b0\uff09\u4ee5\u51cf\u5c11\u5f00\u9500\u3002", "result": "\u5728\u6811\u8393\u6d3e\u548cGPU\u8fb9\u7f18\u670d\u52a1\u5668\u7ec4\u6210\u7684\u5f02\u6784\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u6d41\u6c34\u7ebf\u5206\u533a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc6\u522b\u4e86\u4e0d\u540c\u7f51\u7edc\u6761\u4ef6\u4e0b\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u70b9\u6765\u5206\u6790\u5ef6\u8fdf-\u541e\u5410\u91cf\u6743\u8861\u3002", "conclusion": "ParetoPipe\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u63a8\u7406\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6709\u52a9\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0DNN\u90e8\u7f72\u7684\u5ef6\u8fdf\u4e0e\u541e\u5410\u91cf\u5e73\u8861\u3002"}}
{"id": "2601.08082", "categories": ["cs.DC", "cs.ET", "cs.MS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.08082", "abs": "https://arxiv.org/abs/2601.08082", "authors": ["Vicki Carrica", "Rabab Alomairy", "Evelyne Ringoot", "Alan Edelman"], "title": "Hierarchical Precision and Recursion for Accelerating Symmetric Linear Solves on MXUs", "comment": "10 pages, 11 figures", "summary": "Symmetric linear solves are fundamental to a wide range of scientific and engineering applications, from climate modeling and structural analysis to machine learning and optimization. These workloads often rely on Cholesky (POTRF) decomposition and its supporting operations, triangular solves (TRSM) and symmetric rank-k updates (SYRK), which together form the computational core for solving symmetric positive-definite systems. To accelerate these kernels, we present a portable, mixed-precision solver designed for Matrix Processing Units (MXUs), including NVIDIA Tensor Cores (H200) and AMD Matrix Cores (MI300X). Our algorithm builds on a nested recursive formulation in which Cholesky exposes parallelism through recursive decomposition of its TRSM and SYRK sub-problems. This structure yields a hierarchical recursion that maximizes GEMM throughput while enabling fine-grained control over numerical precision. We introduce a custom recursive data structure that assigns low-precision FP16 arithmetic to large off-diagonal blocks, while preserving high precision on diagonal blocks to ensure numerical stability. The solver is implemented in Julia, leveraging array programming, multiple dispatch, and dynamic type inference to enable seamless expression of mixed-precision computation. This design provides a high-level, hardware-agnostic interface while efficiently interfacing with low-level vendor libraries for backend portability. On H200, our recursive FP64 SYRK achieves a 14x speedup over cuBLAS, while mixed-precision delivers up to 27x speedup in SYRK and 5x in TRSM over full-precision baselines. This results in a 5x overall speedup for Cholesky versus cuSOLVER FP64, with 100x better accuracy than pure FP16 while retaining 88% of its peak speedup. Comparable performance and accuracy trends are observed on MI300X, demonstrating broad applicability across GPUs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMXU\u7684\u6df7\u5408\u7cbe\u5ea6\u5bf9\u79f0\u7ebf\u6027\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u9012\u5f52\u5206\u89e3\u548c\u7cbe\u5ea6\u63a7\u5236\uff0c\u5728NVIDIA H200\u548cAMD MI300X\u4e0a\u5b9e\u73b0\u663e\u8457\u52a0\u901f", "motivation": "\u5bf9\u79f0\u7ebf\u6027\u6c42\u89e3\u5728\u79d1\u5b66\u5de5\u7a0b\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u77e9\u9635\u5904\u7406\u5355\u5143\u4e0a\u6548\u7387\u6709\u9650\uff0c\u9700\u8981\u9488\u5bf9MXU\u4f18\u5316\u7684\u6df7\u5408\u7cbe\u5ea6\u7b97\u6cd5", "method": "\u91c7\u7528\u5d4c\u5957\u9012\u5f52\u5206\u89e3Cholesky\u4e3aTRSM\u548cSYRK\u5b50\u95ee\u9898\uff0c\u8bbe\u8ba1\u81ea\u5b9a\u4e49\u9012\u5f52\u6570\u636e\u7ed3\u6784\uff0c\u5bf9\u5927\u578b\u975e\u5bf9\u89d2\u5757\u4f7f\u7528FP16\u4f4e\u7cbe\u5ea6\uff0c\u5bf9\u89d2\u5757\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027", "result": "\u5728H200\u4e0a\uff0c\u9012\u5f52FP64 SYRK\u6bd4cuBLAS\u5feb14\u500d\uff0c\u6df7\u5408\u7cbe\u5ea6SYRK\u6bd4\u5168\u7cbe\u5ea6\u57fa\u7ebf\u5feb27\u500d\uff0cTRSM\u5feb5\u500d\uff0cCholesky\u6574\u4f53\u6bd4cuSOLVER FP64\u5feb5\u500d\uff0c\u7cbe\u5ea6\u6bd4\u7eafFP16\u9ad8100\u500d", "conclusion": "\u8be5\u6df7\u5408\u7cbe\u5ea6\u6c42\u89e3\u5668\u5728NVIDIA\u548cAMD GPU\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u4e3a\u5bf9\u79f0\u7ebf\u6027\u6c42\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u79fb\u690d\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.08277", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.08277", "abs": "https://arxiv.org/abs/2601.08277", "authors": ["Yizhuo Rao", "Xingjian Cui", "Jiabin Xie", "Shangzhi Pang", "Guangnan Feng", "Jinhui Wei", "Zhiguang Chen", "Yutong Lu"], "title": "Matrix-PIC: Harnessing Matrix Outer-product for High-Performance Particle-in-Cell Simulations", "comment": "Accepted for publication at EuroSys 2026", "summary": "Particle-in-Cell (PIC) simulations spend most of their execution time on particle--grid interactions, where fine-grained atomic updates become a major bottleneck on traditional many-core CPUs. Recent CPU architectures integrate specialized Matrix Processing Units (MPUs) that efficiently support matrix outer-product operations, offering new opportunities to overcome this limitation. Leveraging this architectural shift, this work focuses on redesigning the current deposition step of PIC simulations under a matrix-centric execution model.\n  We present MatrixPIC, the first holistic co-design of the deposition kernel, data layout, and incremental particle sorting tailored to the hybrid MPU--VPU SIMD model on modern CPUs. MatrixPIC introduces: (i)~a block-matrix formulation of the current deposition algorithm that maps naturally to MPU outer-product primitives; (ii)~a hybrid execution pipeline that combines MPU-based high-density accumulation with VPU-based data preparation and control flow; and (iii)~an $O(1)$-amortized incremental sorter based on a gapped packed-memory array to preserve data locality for efficient MPU execution.\n  Evaluated on a next-generation HPC platform, MatrixPIC achieves significant performance gains. In Laser-Wakefield Acceleration (LWFA) simulations, it delivers up to $2.63\\times$ speedup in total runtime. For third-order deposition, the core kernel is accelerated by $8.7\\times$ over the baseline and $2.0\\times$ over the best hand-optimized VPU implementation. Moreover, MatrixPIC reaches $83.08\\%$ of theoretical CPU peak performance, nearly $2.8\\times$ higher than a highly optimized CUDA kernel on a data center GPU. These results demonstrate the effectiveness of matrix-oriented co-design for accelerating PIC simulations on emerging CPU architectures.", "AI": {"tldr": "MatrixPIC\uff1a\u5229\u7528CPU\u4e0a\u7684\u77e9\u9635\u5904\u7406\u5355\u5143\uff08MPU\uff09\u91cd\u65b0\u8bbe\u8ba1PIC\u6a21\u62df\u4e2d\u7684\u7535\u6d41\u6c89\u79ef\u6b65\u9aa4\uff0c\u901a\u8fc7\u77e9\u9635\u5916\u79ef\u539f\u8bed\u5b9e\u73b0\u6027\u80fd\u663e\u8457\u63d0\u5347", "motivation": "\u4f20\u7edfPIC\u6a21\u62df\u4e2d\u7c92\u5b50-\u7f51\u683c\u4ea4\u4e92\u7684\u7ec6\u7c92\u5ea6\u539f\u5b50\u66f4\u65b0\u6210\u4e3aCPU\u591a\u6838\u6027\u80fd\u74f6\u9888\uff0c\u800c\u73b0\u4ee3CPU\u96c6\u6210\u7684\u4e13\u7528\u77e9\u9635\u5904\u7406\u5355\u5143\uff08MPU\uff09\u4e3a\u514b\u670d\u8fd9\u4e00\u9650\u5236\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a", "method": "\u63d0\u51faMatrixPIC\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u8bbe\u8ba1\uff1a1\uff09\u5c06\u7535\u6d41\u6c89\u79ef\u7b97\u6cd5\u91cd\u65b0\u8868\u8ff0\u4e3a\u5757\u77e9\u9635\u5f62\u5f0f\uff0c\u81ea\u7136\u6620\u5c04\u5230MPU\u5916\u79ef\u539f\u8bed\uff1b2\uff09\u6df7\u5408\u6267\u884c\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408MPU\u9ad8\u5bc6\u5ea6\u7d2f\u52a0\u548cVPU\u6570\u636e\u51c6\u5907\uff1b3\uff09\u57fa\u4e8e\u95f4\u9699\u6253\u5305\u5185\u5b58\u6570\u7ec4\u7684O(1)\u644a\u9500\u589e\u91cf\u6392\u5e8f\u5668\uff0c\u4fdd\u6301\u6570\u636e\u5c40\u90e8\u6027", "result": "\u5728\u4e0b\u4e00\u4ee3HPC\u5e73\u53f0\u4e0a\uff0cMatrixPIC\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff1aLWFA\u6a21\u62df\u603b\u8fd0\u884c\u65f6\u95f4\u52a0\u901f2.63\u500d\uff1b\u4e09\u9636\u6c89\u79ef\u6838\u5fc3\u5185\u6838\u52a0\u901f8.7\u500d\uff08\u76f8\u6bd4\u57fa\u7ebf\uff09\u548c2.0\u500d\uff08\u76f8\u6bd4\u6700\u4f73VPU\u5b9e\u73b0\uff09\uff1b\u8fbe\u5230CPU\u7406\u8bba\u5cf0\u503c\u6027\u80fd\u768483.08%\uff0c\u6bd4\u6570\u636e\u4e2d\u5fc3GPU\u4e0a\u9ad8\u5ea6\u4f18\u5316\u7684CUDA\u5185\u6838\u9ad8\u8fd12.8\u500d", "conclusion": "MatrixPIC\u5c55\u793a\u4e86\u9762\u5411\u77e9\u9635\u7684\u534f\u540c\u8bbe\u8ba1\u5728\u52a0\u901f\u65b0\u5174CPU\u67b6\u6784\u4e0aPIC\u6a21\u62df\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5229\u7528\u4e13\u7528\u786c\u4ef6\u52a0\u901f\u79d1\u5b66\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2601.08374", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.08374", "abs": "https://arxiv.org/abs/2601.08374", "authors": ["Dali Chang", "Chong Zhang", "Kaiqi Zhang", "Mingguan Yang", "Huiyuan Li", "Weiqiang Kong"], "title": "Shifting the Sweet Spot: High-Performance Matrix-Free Method for High-Order Elasticity", "comment": null, "summary": "In high-order finite element analysis for elasticity, matrix-free (PA) methods are a key technology for overcoming the memory bottleneck of traditional Full Assembly (FA). However, existing implementations fail to fully exploit the special structure of modern CPU architectures and tensor-product elements, causing their performance \"sweet spot\" to anomalously remain at the low order of $p \\approx 2$, which severely limits the potential of high-order methods. To address this challenge, we design and implement a highly optimized PA operator within the MFEM framework, deeply integrated with a Geometric Multigrid (GMG) preconditioner. Our multi-level optimization strategy includes replacing the original $O(p^6)$ generic algorithm with an efficient $O(p^4)$ one based on tensor factorization, exploiting Voigt symmetry to reduce redundant computations for the elasticity problem, and employing macro-kernel fusion to enhance data locality and break the memory bandwidth bottleneck. Extensive experiments on mainstream x86 and ARM architectures demonstrate that our method successfully shifts the performance \"sweet spot\" to the higher-order region of $p \\ge 6$. Compared to the MFEM baseline, the optimized core operator (kernel) achieves speedups of 7x to 83x, which translates to a 3.6x to 16.8x end-to-end performance improvement in the complete solution process. This paper provides a validated and efficient practical path for conducting large-scale, high-order elasticity simulations on mainstream CPU hardware.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u9ad8\u9636\u6709\u9650\u5143\u5f39\u6027\u5206\u6790\u4e2d\u7684\u77e9\u9635\u81ea\u7531\u65b9\u6cd5\u8fdb\u884c\u4f18\u5316\uff0c\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u3001Voigt\u5bf9\u79f0\u6027\u5229\u7528\u548c\u5b8f\u6838\u878d\u5408\u7b49\u6280\u672f\uff0c\u5c06\u6027\u80fd\u6700\u4f73\u70b9\u4ecep\u22482\u63d0\u5347\u5230p\u22656\uff0c\u5728\u4e3b\u6d41CPU\u67b6\u6784\u4e0a\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u77e9\u9635\u81ea\u7531\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u73b0\u4ee3CPU\u67b6\u6784\u548c\u5f20\u91cf\u79ef\u5355\u5143\u7684\u7279\u6b8a\u7ed3\u6784\uff0c\u5bfc\u81f4\u6027\u80fd\u6700\u4f73\u70b9\u5f02\u5e38\u505c\u7559\u5728\u4f4e\u9636p\u22482\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u9ad8\u9636\u65b9\u6cd5\u7684\u6f5c\u529b\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u6027\u80fd\u74f6\u9888\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u9ad8\u9636\u5f39\u6027\u6a21\u62df\u3002", "method": "\u5728MFEM\u6846\u67b6\u5185\u8bbe\u8ba1\u9ad8\u5ea6\u4f18\u5316\u7684\u77e9\u9635\u81ea\u7531\u7b97\u5b50\uff0c\u4e0e\u51e0\u4f55\u591a\u91cd\u7f51\u683c\u9884\u5904\u7406\u5668\u6df1\u5ea6\u96c6\u6210\u3002\u91c7\u7528\u591a\u7ea7\u4f18\u5316\u7b56\u7565\uff1a1) \u7528\u57fa\u4e8e\u5f20\u91cf\u5206\u89e3\u7684O(p^4)\u7b97\u6cd5\u66ff\u4ee3\u539f\u59cbO(p^6)\u901a\u7528\u7b97\u6cd5\uff1b2) \u5229\u7528Voigt\u5bf9\u79f0\u6027\u51cf\u5c11\u5f39\u6027\u95ee\u9898\u7684\u5197\u4f59\u8ba1\u7b97\uff1b3) \u4f7f\u7528\u5b8f\u6838\u878d\u5408\u589e\u5f3a\u6570\u636e\u5c40\u90e8\u6027\u5e76\u7a81\u7834\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u6210\u529f\u5c06\u6027\u80fd\u6700\u4f73\u70b9\u8f6c\u79fb\u5230\u9ad8\u9636\u533a\u57dfp\u22656\u3002\u4f18\u5316\u540e\u7684\u6838\u5fc3\u7b97\u5b50\u6bd4MFEM\u57fa\u7ebf\u52a0\u901f7-83\u500d\uff0c\u5b8c\u6574\u6c42\u89e3\u8fc7\u7a0b\u7aef\u5230\u7aef\u6027\u80fd\u63d0\u53473.6-16.8\u500d\u3002\u5728\u4e3b\u6d41x86\u548cARM\u67b6\u6784\u4e0a\u5747\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u5728\u4e3b\u6d41CPU\u786c\u4ef6\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u9ad8\u9636\u5f39\u6027\u6a21\u62df\u63d0\u4f9b\u4e86\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u9ad8\u6548\u5b9e\u8df5\u8def\u5f84\uff0c\u901a\u8fc7\u6df1\u5ea6\u4f18\u5316\u77e9\u9635\u81ea\u7531\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u5b9e\u73b0\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2601.08800", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.08800", "abs": "https://arxiv.org/abs/2601.08800", "authors": ["Bowen Zhou", "Jinrui Jia", "Wenhao He", "Yong Zhang", "Fang Dong"], "title": "MixServe: An Automatic Distributed Serving System for MoE Models with Hybrid Parallelism Based on Fused Communication Algorithm", "comment": "Submitted to ICDCS 2026", "summary": "The Mixture of Experts (MoE) models are emerging as the latest paradigm for Large Language Models (LLMs). However, due to memory constraints, MoE models with billions or even trillions of parameters can only be deployed in multi-GPU or even multi-node & multi-GPU based serving systems. Thus, communication has became a major bottleneck in distributed serving systems, especially inter-node communication. Contemporary distributed MoE models are primarily implemented using all-reduce (AR) based tensor parallelism (TP) and all-to-all (A2A) based expert parallelism (EP). However, TP generally exhibits low inter-node efficiency and is thus confined to high-speed intra-node bandwidth. In contrast, EP tends to suffer from load imbalance, especially when the parallel degree is high.\n  In this work, we introduce MixServe, a novel automatic distributed serving system for efficient deployment of MoE models by a novel TP-EP hybrid parallelism based on fused AR-A2A communication algorithm. MixServe begins by evaluating the communication overhead associated with various parallel strategies, taking into account the model hyperparameters and the configurations of network and hardware resources, and then automatically selects the most efficient parallel strategy. Then, we propose the TP-EP hybrid parallelism based on fused AR-A2A communication algorithm that overlaps intra-node AR communication and inter-node A2A communication. Extensive experiments on DeepSeek-R1 and Qwen3 models demonstrate that MixServe achieves superior inference performance, with 1.08~3.80x acceleration in time to first token (TTFT), 1.03~1.66x acceleration in inter-token latency (ITL), and 5.2%~50.3% throughput improvement compared to existing approaches.", "AI": {"tldr": "MixServe\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u6548\u90e8\u7f72MoE\u6a21\u578b\u7684\u81ea\u52a8\u5206\u5e03\u5f0f\u670d\u52a1\u7cfb\u7edf\uff0c\u91c7\u7528\u57fa\u4e8e\u878d\u5408AR-A2A\u901a\u4fe1\u7b97\u6cd5\u7684TP-EP\u6df7\u5408\u5e76\u884c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "MoE\u6a21\u578b\u7531\u4e8e\u53c2\u6570\u91cf\u5de8\u5927\uff0c\u901a\u5e38\u9700\u8981\u591aGPU\u6216\u591a\u8282\u70b9\u90e8\u7f72\uff0c\u901a\u4fe1\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709TP\u65b9\u6cd5\u8282\u70b9\u95f4\u6548\u7387\u4f4e\uff0cEP\u65b9\u6cd5\u8d1f\u8f7d\u4e0d\u5747\u8861\uff0c\u9700\u8981\u66f4\u597d\u7684\u5e76\u884c\u7b56\u7565\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faMixServe\u7cfb\u7edf\uff1a1\uff09\u81ea\u52a8\u8bc4\u4f30\u4e0d\u540c\u5e76\u884c\u7b56\u7565\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u6839\u636e\u6a21\u578b\u8d85\u53c2\u6570\u548c\u786c\u4ef6\u914d\u7f6e\u9009\u62e9\u6700\u4f18\u7b56\u7565\uff1b2\uff09\u63d0\u51fa\u57fa\u4e8e\u878d\u5408AR-A2A\u901a\u4fe1\u7b97\u6cd5\u7684TP-EP\u6df7\u5408\u5e76\u884c\uff0c\u91cd\u53e0\u8282\u70b9\u5185AR\u901a\u4fe1\u548c\u8282\u70b9\u95f4A2A\u901a\u4fe1\u3002", "result": "\u5728DeepSeek-R1\u548cQwen3\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMixServe\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff1a\u9996token\u5ef6\u8fdf\u52a0\u901f1.08~3.80\u500d\uff0ctoken\u95f4\u5ef6\u8fdf\u52a0\u901f1.03~1.66\u500d\uff0c\u541e\u5410\u91cf\u63d0\u53475.2%~50.3%\u3002", "conclusion": "MixServe\u901a\u8fc7\u521b\u65b0\u7684TP-EP\u6df7\u5408\u5e76\u884c\u548c\u878d\u5408\u901a\u4fe1\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86MoE\u6a21\u578b\u5206\u5e03\u5f0f\u90e8\u7f72\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
