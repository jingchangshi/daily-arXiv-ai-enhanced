{"id": "2602.08081", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.08081", "abs": "https://arxiv.org/abs/2602.08081", "authors": ["Brian Rojkov", "Shubham Ranjan", "Derek Wright", "Manoj Sachdev"], "title": "Investigating Energy Bounds of Analog Compute-in-Memory with Local Normalization", "comment": null, "summary": "Modern edge AI workloads demand maximum energy efficiency, motivating the pursuit of analog Compute-in-Memory (CIM) architectures. Simultaneously, the popularity of Large-Language-Models (LLMs) drives the adoption of low-bit floating-point formats which prioritize dynamic range. However, the conventional direct-accumulation CIM accommodates floating-points by normalizing them to a shared widened fixed-point scale. Consequently, hardware resolution is dictated by the input's dynamic range rather than its precision, and energy consumption is dominated by the ADC. We address this limitation by introducing local normalization for each input, weight, and multiply-accumulate (MAC) output via a Gain-Ranging MAC (GR-MAC). Normalization overhead is handled by low-power digital logic, enabling the computationally expensive MAC operation to remain in the energy-efficient low-precision analog regime. Energy modelling shows that the addition of a gain-ranging Stage to the MAC enables a 4-bit increase in input dynamic range without increased energy consumption at a 35 dB SQNR standard. Additionally, the ADC resolution requirement becomes invariant to input distribution assumptions, allowing construction of an upper bound with a 1.5-bit reduction compared to the conventional lower bound. These results establish a pathway towards unlocking favourable energy scaling trends of analog CIM for modern AI workloads."}
{"id": "2602.08323", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.08323", "abs": "https://arxiv.org/abs/2602.08323", "authors": ["Yousuf Choudhary", "Tosiron Adegbija"], "title": "Antiferromagnetic Tunnel Junctions (AFMTJs) for In-Memory Computing: Modeling and Case Study", "comment": "Design, Automation and Test in Europe (DATE) 2026", "summary": "Antiferromagnetic Tunnel Junctions (AFMTJs) enable picosecond switching and femtojoule writes through ultrafast sublattice dynamics. We present the first end-to-end AFMTJ simulation framework integrating multi-sublattice Landau-Lifshitz-Gilbert (LLG) dynamics with circuit-level modeling. SPICE-based simulations show that AFMTJs achieve ~8x lower write latency and ~9x lower write energy than conventional MTJs. When integrated into an in-memory computing architecture, AFMTJs deliver 17.5x average speedup and nearly 20x energy savings versus a CPU baseline-significantly outperforming MTJ-based IMC. These results establish AFMTJs as a compelling primitive for scalable, low-power computing."}
{"id": "2602.08842", "categories": ["cs.AR", "cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.08842", "abs": "https://arxiv.org/abs/2602.08842", "authors": ["Jean-Pierre Busch", "Lukas Ostendorf", "Guido Linden", "Lennart Reiher", "Till Beemelmanns", "Bastian Lampe", "Timo Woopen", "Lutz Eckstein"], "title": "karl. -- A Research Vehicle for Automated and Connected Driving", "comment": "8 pages; Accepted to be published as part of the 37th Intelligent Vehicles Symposium (IV), Detroit, MI, United States, June 22-25, 2026", "summary": "As highly automated driving is transitioning from single-vehicle closed-access testing to commercial deployments of public ride-hailing in selected areas (e.g., Waymo), automated driving and connected cooperative intelligent transport systems (C-ITS) remain active fields of research. Even though simulation is omnipresent in the development and validation life cycle of automated and connected driving technology, the complex nature of public road traffic and software that masters it still requires real-world integration and testing with actual vehicles. Dedicated vehicles for research and development allow testing and validation of software and hardware components under real-world conditions early on. They also enable collecting and publishing real-world datasets that let others conduct research without vehicle access, and support early demonstration of futuristic use cases. In this paper, we present karl., our new research vehicle for automated and connected driving. Apart from major corporations, few institutions worldwide have access to their own L4-capable research vehicles, restricting their ability to carry out independent research. This paper aims to help bridge that gap by sharing the reasoning, design choices, and technical details that went into making karl. a flexible and powerful platform for research, engineering, and validation in the context of automated and connected driving. More impressions of karl. are available at https://karl.ac."}
{"id": "2602.07324", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.07324", "abs": "https://arxiv.org/abs/2602.07324", "authors": ["Abdullah H. Rasheed"], "title": "Static Analysis Under Non-Deterministic Program Assumptions", "comment": null, "summary": "Static analyses overwhelmingly trade precision for soundness and automation. For this reason, their use-cases are restricted to situations where imprecision isn't prohibitive. In this paper, we propose and specify a static analysis that accepts user-supplied program assumptions that are local to program locations. Such assumptions can be used to counteract imprecision in static analyses, enabling their use in a much wider variety of applications. These assumptions are taken by the analyzer non-deterministically, resulting in a function from sets of accepted assumptions to the resulting analysis under those assumptions. We also demonstrate the utility of such a function in two ways, both of which showcase how it can enable optimization over a search space of assumptions that is otherwise infeasible without the specified analysis."}
{"id": "2602.07306", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.07306", "abs": "https://arxiv.org/abs/2602.07306", "authors": ["Chong Wang", "Nan Du", "Tom Gunter", "Tao Lei", "Kulin Seth", "Senyu Tong", "Jianyu Wang", "Guoli Yin", "Xiyou Zhou", "Kelvin Zou", "Ruoming Pang"], "title": "Parallel Track Transformers: Enabling Fast GPU Inference with Reduced Synchronization", "comment": null, "summary": "Efficient large-scale inference of transformer-based large language models (LLMs) remains a fundamental systems challenge, frequently requiring multi-GPU parallelism to meet stringent latency and throughput targets. Conventional tensor parallelism decomposes matrix operations across devices but introduces substantial inter-GPU synchronization, leading to communication bottlenecks and degraded scalability. We propose the Parallel Track (PT) Transformer, a novel architectural paradigm that restructures computation to minimize cross-device dependencies. PT achieves up to a 16x reduction in synchronization operations relative to standard tensor parallelism, while maintaining competitive model quality in our experiments. We integrate PT into two widely adopted LLM serving stacks-Tensor-RT-LLM and vLLM-and report consistent improvements in serving efficiency, including up to 15-30% reduced time to first token, 2-12% reduced time per output token, and up to 31.90% increased throughput in both settings."}
{"id": "2602.07455", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.07455", "abs": "https://arxiv.org/abs/2602.07455", "authors": ["Jinhua Wu", "Yuting Wang", "Liukun Yu", "Linglong Meng"], "title": "RustCompCert: A Verified and Verifying Compiler for a Sequential Subset of Rust", "comment": "Submitted to Rust Verify 2026", "summary": "We present our ongoing work on developing an end-to-end verified Rust compiler based on CompCert. It provides two guarantees: one is semantics preservation from Rust to assembly, i.e., the behaviors of source code includes the behaviors of target code, with which the properties verified at the source can be preserved down to the target; the other is memory safety ensured by the verifying compilation -- the borrow checking pass, which can simplify the verification of Rust programs, e.g., by allowing the verification tools focus on the functional correctness."}
{"id": "2602.07614", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.07614", "abs": "https://arxiv.org/abs/2602.07614", "authors": ["Rosario Napoli", "Gabriele Morabito", "Antonio Celesti", "Massimo Villari", "Maria Fazio"], "title": "Knowledge Graphs-Driven Intelligence for Distributed Decision Systems", "comment": "Accepted to the 18th IEEE/ACM International Conference on Utility and Cloud Computing (UCC 2025)", "summary": "Modern distributed decision-making systems face significant challenges arising from data heterogeneity, dynamic environments, and the need for decentralized coordination. This paper introduces the Knowledge Sharing paradigm as an innovative approach that uses the semantic richness of Knowledge Graphs (KGs) and the representational power of Graph Embeddings (GEs) to achieve decentralized intelligence. Our architecture empowers individual nodes to locally construct semantic representations of their operational context, iteratively aggregating embeddings through neighbor-based exchanges using GraphSAGE. This iterative local aggregation process results in a dynamically evolving global semantic abstraction called Knowledge Map, enabling coordinated decision-making without centralized control. To validate our approach, we conduct extensive experiments under a distributed resource orchestration use case. We simulate different network topologies and node workloads, analyzing the local semantic drift of individual nodes. Experimental results confirm that our distributed knowledge-sharing mechanism effectively maintains semantic coherence and adaptability, making it suitable for complex and dynamic environments such as Edge Computing, IoT, and multi-agent systems."}
{"id": "2602.07627", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.07627", "abs": "https://arxiv.org/abs/2602.07627", "authors": ["Xuran Cai", "Amir Goharshady", "S Hitarth", "Chun Kit Lam"], "title": "Series-Parallel-Loop Decompositions of Control-flow Graphs", "comment": null, "summary": "Control-flow graphs (CFGs) of structured programs are well known to exhibit strong sparsity properties. Traditionally, this sparsity has been modeled using graph parameters such as treewidth and pathwidth, enabling the development of faster parameterized algorithms for tasks in compiler optimization, model checking, and program analysis. However, these parameters only approximate the structural constraints of CFGs: although every structured CFG has treewidth at most~7, many graphs with treewidth at most~7 cannot arise as CFGs. As a result, existing parameterized techniques are optimized for a substantially broader class of graphs than those encountered in practice.\n  In this work, we introduce a new grammar-based decomposition framework that characterizes \\emph{exactly} the class of control-flow graphs generated by structured programs. Our decomposition is intuitive, mirrors the syntactic structure of programs, and remains fully compatible with the dynamic-programming paradigm of treewidth-based methods. Using this framework, we design improved algorithms for two classical compiler optimization problems: \\emph{Register Allocation} and \\emph{Lifetime-Optimal Speculative Partial Redundancy Elimination (LOSPRE)}. Extensive experimental evaluation demonstrates significant performance improvements over previous state-of-the-art approaches, highlighting the benefits of using decompositions tailored specifically to CFGs."}
{"id": "2602.07850", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.07850", "abs": "https://arxiv.org/abs/2602.07850", "authors": ["Shanuja Sasi"], "title": "Privacy-Preserving Coding Schemes for Multi-Access Distributed Computing Models", "comment": null, "summary": "Distributed computing frameworks such as MapReduce have become essential for large-scale data processing by decomposing tasks across multiple nodes. The multi-access distributed computing (MADC) model further advances this paradigm by decoupling mapper and reducer roles: dedicated mapper nodes store data and compute intermediate values, while reducer nodes are connected to multiple mappers and aggregate results to compute final outputs. This separation reduces communication bottlenecks without requiring file replication. In this paper, we introduce privacy constraints into MADC and develop private coded schemes for two specific connectivity models. We construct new families of extended placement delivery arrays and derive corresponding coding schemes that guarantee privacy of each reducer's assigned function."}
{"id": "2602.07742", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.07742", "abs": "https://arxiv.org/abs/2602.07742", "authors": ["Nat Karmios", "Sacha-Ã‰lie Ayoun", "Philippa Gardner"], "title": "Gillian Debugging: Swinging Through the (Compositional Symbolic Execution) Trees, Extended Version", "comment": "24 pages, 11 figures. To be published at TACAS 2026", "summary": "In recent years, compositional symbolic execution (CSE) tools have been growing in prominence and are becoming more and more applicable to real-world codebases. Still to this day, however, debugging the output of these tools remains difficult, even for specialist users. To address this, we introduce a debugging interface for symbolic execution tools, integrated with Visual Studio Code and the Gillian multi-language CSE platform, with strong focus on visualisation, interactivity, and intuitive representation of symbolic execution trees. We take care in making this interface tool-agnostic, easing its transfer to other symbolic analysis tools in future. We empirically evaluate our work with a user study, the results of which show the debugger's usefulness in helping early researchers understand the principles of CSE and verify fundamental data structure algorithms in Gillian."}
{"id": "2602.08257", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.08257", "abs": "https://arxiv.org/abs/2602.08257", "authors": ["Antonis Psistakis", "Burak Ocalan", "Fabien Chaix", "Ramnatthan Alagappan", "Josep Torrellas"], "title": "HEAL: Online Incremental Recovery for Leaderless Distributed Systems Across Persistency Models", "comment": null, "summary": "Ensuring resilience in distributed systems has become an acute concern. In today's environment, it is crucial to develop light-weight mechanisms that recover a distributed system from faults quickly and with only a small impact on the live-system throughput. To address this need, this paper proposes a new low-overhead, general recovery scheme for modern non-transactional leaderless distributed systems. We call our scheme HEAL. On a node failure, HEAL performs an optimized online incremental recovery. This paper presents HEAL's algorithms for settings with Linearizable consistency and different memory persistency models. We implement HEAL on a 6-node Intel cluster. Our experiments running TAOBench workloads show that HEAL is very effective. HEAL recovers the cluster in 120 milliseconds on average, while reducing the throughput of the running workload by an average of 8.7%. In contrast, a conventional recovery scheme for leaderless systems needs 360 seconds to recover, reducing the throughput of the system by 16.2%. Finally, compared to an incremental recovery scheme for a state-of-the-art leader-based system, HEAL reduces the average recovery latency by 20.7x and the throughput degradation by 62.4%."}
{"id": "2602.08271", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.08271", "abs": "https://arxiv.org/abs/2602.08271", "authors": ["Antonis Psistakis", "Burak Ocalan", "Chloe Alverti", "Fabien Chaix", "Ramnatthan Alagappan", "Josep Torrellas"], "title": "Towards CXL Resilience to CPU Failures", "comment": null, "summary": "Compute Express Link (CXL) 3.0 and beyond allows the compute nodes of a cluster to share data with hardware cache coherence and at the granularity of a cache line. This enables shared-memory semantics for distributed computing, but introduces new resilience challenges: a node failure leads to the loss of the dirty data in its caches, corrupting application state. Unfortunately, the CXL specification does not consider processor failures. Moreover, when a component fails, the specification tries to isolate it and continue application execution; there is no attempt to bring the application to a consistent state. To address these limitations, this paper extends the CXL specification to be resilient to node failures, and to correctly recover the application after node failures. We call the system ReCXL. To handle the failure of nodes, ReCXL augments the coherence transaction of a write with messages that propagate the update to a small set of other nodes (i.e., Replicas). Replicas save the update in a hardware Logging Unit. Such replication ensures resilience to node failures. Then, at regular intervals, the Logging Units dump the updates to memory. Recovery involves using the logs in the Logging Units to bring the directory and memory to a correct state. Our evaluation shows that ReCXL enables fault-tolerant execution with only a 30% slowdown over the same platform with no fault-tolerance support."}
{"id": "2602.08747", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.08747", "abs": "https://arxiv.org/abs/2602.08747", "authors": ["Zhixin Zhao", "Yitao Hu", "Simin Chen", "Mingfang Ji", "Wei Yang", "Yuhao Zhang", "Laiping Zhao", "Wenxin Li", "Xiulong Liu", "Wenyu Qu", "Hao Wang"], "title": "PARD: Enhancing Goodput for Inference Pipeline via Proactive Request Dropping", "comment": "Accepted by EuroSys'26", "summary": "Modern deep neural network (DNN) applications integrate multiple DNN models into inference pipelines with stringent latency requirements for customized tasks. To mitigate extensive request timeouts caused by accumulation, systems for inference pipelines commonly drop a subset of requests so the remaining ones can satisfy latency constraints. Since it is commonly believed that request dropping adversely affects goodput, existing systems only drop requests when they have to, which we call reactive dropping. However, this reactive policy can not maintain high goodput, as it neither makes timely dropping decisions nor identifies the proper set of requests to drop, leading to issues of dropping requests too late or dropping the wrong set of requests.\n  We propose that the inference system should proactively drop certain requests in advance to enhance the goodput across the entire workload. To achieve this, we design an inference system PARD. It enhances goodput with timely and precise dropping decisions by integrating a proactive dropping method that decides when to drop requests using runtime information of the inference pipeline, and an adaptive request priority mechanism that selects which specific requests to drop based on remaining latency budgets and workload intensity. Evaluation on a cluster of 64 GPUs over real-world workloads shows that PARD achieves $16\\%$-$176\\%$ higher goodput than the state of the art while reducing the drop rate and wasted computation resources by $1.6\\times$-$17\\times$ and $1.5\\times$-$62\\times$ respectively."}
