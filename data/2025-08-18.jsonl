{"id": "2508.11035", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11035", "abs": "https://arxiv.org/abs/2508.11035", "authors": ["Hasibul Jamil", "MD S Q Zulkar Nine", "Tevfik Kosar"], "title": "EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI Training", "comment": "SC25 Sustainable Supercomputing Workshop", "summary": "Large-scale deep learning workloads increasingly suffer from I/O bottlenecks\nas datasets grow beyond local storage capacities and GPU compute outpaces\nnetwork and disk latencies. While recent systems optimize data-loading time,\nthey overlook the energy cost of I/O - a critical factor at large scale. We\nintroduce EMLIO, an Efficient Machine Learning I/O service that jointly\nminimizes end-to-end data-loading latency T and I/O energy consumption E across\nvariable-latency networked storage. EMLIO deploys a lightweight data-serving\ndaemon on storage nodes that serializes and batches raw samples, streams them\nover TCP with out-of-order prefetching, and integrates seamlessly with\nGPU-accelerated (NVIDIA DALI) preprocessing on the client side. In exhaustive\nevaluations over local disk, LAN (0.05 ms & 10 ms RTT), and WAN (30 ms RTT)\nenvironments, EMLIO delivers up to 8.6X faster I/O and 10.9X lower energy use\ncompared to state-of-the-art loaders, while maintaining constant performance\nand energy profiles irrespective of network distance. EMLIO's service-based\narchitecture offers a scalable blueprint for energy-aware I/O in\nnext-generation AI clouds."}
{"id": "2508.11266", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11266", "abs": "https://arxiv.org/abs/2508.11266", "authors": ["Ailiya Borjigin", "Cong He", "Charles CC Lee", "Wei Zhou"], "title": "Element and Everything Tokens: Two-Tier Architecture for Mobilizing Alternative Assets", "comment": "8 Pages, Submitted to RASSE 2025", "summary": "Alternative assets such as mines, power plants, or infrastructure projects\nare often large, heterogeneous bundles of resources, rights, and outputs whose\nvalue is difficult to trade or fractionalize under traditional frameworks. This\npaper proposes a novel two-tier tokenization architecture to enhance the\nliquidity and transparency of such complex assets. We introduce the concepts of\nElement Tokens and Everything Tokens: elemental tokens represent standardized,\nfully collateralized components of an asset (e.g., outputs, rights, or\ncredits), while an everything token represents the entire asset as a fixed\ncombination of those elements. The architecture enables both fine-grained\npartial ownership and integrated whole-asset ownership through a system of\ntwo-way convertibility. We detail the design and mechanics of this system,\nincluding an arbitrage mechanism that keeps the price of the composite token\naligned with the net asset value of its constituents. Through illustrative\nexamples in the energy and industrial sectors, we demonstrate that our approach\nallows previously illiquid, high-value projects to be fractionalized and traded\nakin to stocks or exchange-traded funds (ETFs). We discuss the benefits for\ninvestors and asset owners, such as lower entry barriers, improved price\ndiscovery, and flexible financing, as well as the considerations for\nimplementation and regulation."}
{"id": "2508.11298", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11298", "abs": "https://arxiv.org/abs/2508.11298", "authors": ["Gabin Schieffer", "Jacob Wahlgren", "Ruimin Shi", "Edgar A. León", "Roger Pearce", "Maya Gokhale", "Ivy Peng"], "title": "Inter-APU Communication on AMD MI300A Systems via Infinity Fabric: a Deep Dive", "comment": null, "summary": "The ever-increasing compute performance of GPU accelerators drives up the\nneed for efficient data movements within HPC applications to sustain\nperformance. Proposed as a solution to alleviate CPU-GPU data movement, AMD\nMI300A Accelerated Processing Unit (APU) combines CPU, GPU, and high-bandwidth\nmemory (HBM) within a single physical package. Leadership supercomputers, such\nas El Capitan, group four APUs within a single compute node, using Infinity\nFabric Interconnect. In this work, we design specific benchmarks to evaluate\ndirect memory access from the GPU, explicit inter-APU data movement, and\ncollective multi-APU communication. We also compare the efficiency of HIP APIs,\nMPI routines, and the GPU-specialized RCCL library. Our results highlight key\ndesign choices for optimizing inter-APU communication on multi-APU AMD MI300A\nsystems with Infinity Fabric, including programming interfaces, allocators, and\ndata movement. Finally, we optimize two real HPC applications, Quicksilver and\nCloverLeaf, and evaluate them on a four MI100A APU system."}
{"id": "2508.11384", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11384", "abs": "https://arxiv.org/abs/2508.11384", "authors": ["Joel Rybicki", "Jakob Solnerzik", "Olivier Stietel", "Robin Vacus"], "title": "Space-efficient population protocols for exact majority in general graphs", "comment": null, "summary": "We study exact majority consensus in the population protocol model. In this\nmodel, the system is described by a graph $G = (V,E)$ with $n$ nodes, and in\neach time step, a scheduler samples uniformly at random a pair of adjacent\nnodes to interact. In the exact majority consensus task, each node is given a\nbinary input, and the goal is to design a protocol that almost surely reaches a\nstable configuration, where all nodes output the majority input value.\n  We give improved upper and lower bounds for the exact majority in general\ngraphs. First, we give asymptotically tight time lower bounds for general\n(unbounded space) protocols. Second, we obtain new upper bounds parameterized\nby the relaxation time $\\tau_{\\mathsf{rel}}$ of the random walk on $G$ induced\nby the scheduler and the degree imbalance $\\Delta/\\delta$ of $G$. Specifically,\nwe give a protocol that stabilizes in $O\\left( \\tfrac{\\Delta}{\\delta}\n\\tau_{\\mathsf{rel}} \\log^2 n \\right)$ steps in expectation and with high\nprobability and uses $O\\left( \\log n \\cdot \\left(\n\\log\\left(\\tfrac{\\Delta}{\\delta}\\right) + \\log\n\\left(\\tfrac{\\tau_{\\mathsf{rel}}}{n}\\right) \\right) \\right)$ states in any\ngraph with minimum degree at least $\\delta$ and maximum degree at most\n$\\Delta$.\n  For regular expander graphs, this matches the optimal space complexity of\n$\\Theta(\\log n)$ for fast protocols in complete graphs [Alistarh et al., SODA\n2016 and Doty et al., FOCS 2022] with a nearly optimal stabilization time of\n$O(n \\log^2 n)$ steps. Finally, we give a new upper bound of\n$O(\\tau_{\\mathsf{rel}} \\cdot n \\log n)$ for the stabilization time of a\nconstant-state protocol."}
{"id": "2508.11477", "categories": ["cs.AR", "cs.ET", "cs.OS"], "pdf": "https://arxiv.org/pdf/2508.11477", "abs": "https://arxiv.org/abs/2508.11477", "authors": ["Hyunsun Chung", "Junhyeok Park", "Taewan Noh", "Seonghoon Ahn", "Kihwan Kim", "Ming Zhao", "Youngjae Kim"], "title": "OpenCXD: An Open Real-Device-Guided Hybrid Evaluation Framework for CXL-SSDs", "comment": "This paper will be published in the proceedings of the 33rd\n  International Symposium on the Modeling, Analysis, and Simulation of Computer\n  and Telecommunication System (MASCOTS)", "summary": "The advent of Compute Express Link (CXL) enables SSDs to participate in the\nmemory hierarchy as large-capacity, byte-addressable memory devices. These\nCXL-enabled SSDs (CXL-SSDs) offer a promising new tier between DRAM and\ntraditional storage, combining NAND flash density with memory-like access\nsemantics. However, evaluating the performance of CXL-SSDs remains difficult\ndue to the lack of hardware that natively supports the CXL.mem protocol on\nSSDs. As a result, most prior work relies on hybrid simulators combining CPU\nmodels augmented with CXL.mem semantics and SSD simulators that approximate\ninternal flash behaviors. While effective for early-stage exploration, this\napproach cannot faithfully model firmware-level interactions and low-level\nstorage dynamics critical to CXL-SSD performance. In this paper, we present\nOpenCXD, a real-device-guided hybrid evaluation framework that bridges the gap\nbetween simulation and hardware. OpenCXD integrates a cycle-accurate CXL.mem\nsimulator on the host side with a physical OpenSSD platform running real\nfirmware. This enables in-situ firmware execution triggered by simulated memory\nrequests. Through these contributions, OpenCXD reflects device-level phenomena\nunobservable in simulation-only setups, providing critical insights for future\nfirmware design tailored to CXL-SSDs."}
{"id": "2508.11415", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11415", "abs": "https://arxiv.org/abs/2508.11415", "authors": ["Raïssa Nataf", "Yoram Moses"], "title": "Time, Fences and the Ordering of Events in TSO", "comment": null, "summary": "The Total Store Order (TSO) is arguably the most widely used relaxed memory\nmodel in multiprocessor architectures, widely implemented, for example in\nIntel's x86 and x64 platforms. It allows processes to delay the visibility of\nwrites through store buffering. While this supports hardware-level\noptimizations and makes a significant contribution to multiprocessor\nefficiency, it complicates reasoning about correctness, as executions may\nviolate sequential consistency. Ensuring correct behavior often requires\ninserting synchronization primitives such as memory fences ($F$) or atomic\nread-modify-write ($RMW$) operations, but this approach can incur significant\nperformance costs. In this work, we develop a semantic framework that precisely\ncharacterizes when such synchronization is necessary under TSO. We introduce a\nnovel TSO-specific occurs-before relation, which adapts Lamport's celebrated\nhappens-before relation from asynchronous message-passing systems to the TSO\nsetting. Our main result is a theorem that proves that the only way to ensure\nthat two events that take place at different sites are temporally ordered is by\nhaving the execution create an occurs-before chain between the events. By\nstudying the role of fences and $RMW$s in creating occurs-before chains, we are\nthen able to capture cases in which these costly synchronization operations are\nunavoidable. Since proper real-time ordering of events is a fundamental aspect\nof consistency conditions such as Linearizability, our analysis provides a\nsound theoretical understanding of essential aspects of the TSO model. In\nparticular, we are able to generalize prior lower bounds for linearizable\nimplementations of shared memory objects. Our results capture the structure of\ninformation flow and causality in the TSO model by extending the standard\ncommunication-based reasoning from asynchronous systems to the TSO memory\nmodel."}
{"id": "2508.11297", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.11297", "abs": "https://arxiv.org/abs/2508.11297", "authors": ["Casper Bach"], "title": "Generic Reduction-Based Interpreters (Extended Version)", "comment": null, "summary": "Reduction-based interpreters are traditionally defined in terms of a one-step\nreduction function which systematically decomposes a term into a potential\nredex and context, contracts the redex, and recomposes it to construct the new\nterm to be further reduced. While implementing such interpreters follows a\nsystematic recipe, they often require interpreter engineers to write a\nsubstantial amount of code -- much of it boilerplate. In this paper, we apply\nwell-known techniques from generic programming to reduce boilerplate code in\nreduction-based interpreters."}
{"id": "2508.11467", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.11467", "abs": "https://arxiv.org/abs/2508.11467", "authors": ["Shifang Liu", "Huiyuan Li", "Hongjiao Sheng", "Haoyuan Gui", "Xiaoyu Zhang"], "title": "Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method", "comment": null, "summary": "Singular Value Decomposition (SVD) is a fundamental matrix factorization\ntechnique in linear algebra, widely applied in numerous matrix-related\nproblems. However, traditional SVD approaches are hindered by slow panel\nfactorization and frequent CPU-GPU data transfers in heterogeneous systems,\ndespite advancements in GPU computational capabilities. In this paper, we\nintroduce a GPU-centered SVD algorithm, incorporating a novel GPU-based\nbidiagonal divide-and-conquer (BDC) method. We reformulate the algorithm and\ndata layout of different steps for SVD computation, performing all panel-level\ncomputations and trailing matrix updates entirely on GPU to eliminate CPU-GPU\ndata transfers. Furthermore, we integrate related computations to optimize BLAS\nutilization, thereby increasing arithmetic intensity and fully leveraging the\ncomputational capabilities of GPUs. Additionally, we introduce a newly\ndeveloped GPU-based BDC algorithm that restructures the workflow to eliminate\nmatrix-level CPU-GPU data transfers and enable asynchronous execution between\nthe CPU and GPU. Experimental results on AMD MI210 and NVIDIA V100 GPUs\ndemonstrate that our proposed method achieves speedups of up to 1293.64x/7.47x\nand 14.10x/12.38x compared to rocSOLVER/cuSOLVER and MAGMA, respectively."}
{"id": "2508.11443", "categories": ["cs.PL", "cs.DS"], "pdf": "https://arxiv.org/pdf/2508.11443", "abs": "https://arxiv.org/abs/2508.11443", "authors": ["William Henrich Due", "Martin Elsman", "Troels Henriksen"], "title": "Towards Efficient Hash Maps in Functional Array Languages", "comment": null, "summary": "We present a systematic derivation of a data-parallel implementation of\ntwo-level, static and collision-free hash maps, by giving a functional\nformulation of the Fredman et al. construction, and then flattening it. We\ndiscuss the challenges of providing a flexible, polymorphic, and abstract\ninterface to hash maps in a functional array language, with particular\nattention paid to the problem of dynamically sized keys, which we address by\nassociating each hash map with an arbitrary context. The algorithm is\nimplemented in Futhark, and the achieved GPU execution performance is compared\non simple benchmark problems. We find that our hash maps outperform\nconventional tree/search-based approaches. Furthermore, our implementation is\ncompared against the state-of-the-art cuCollections library, which is\nsignificantly faster for hash map construction, and to a lesser degree for\nlookups. We explain to which extent the performance difference is due to\nlow-level code generation limitation in the Futhark compiler, and to which\nextent it can be attributed to the data-parallel programming vocabulary not\nproviding the constructs necessary to express the equivalent of the algorithms\nused by cuCollections. We end by reflecting to which extent the functional\narray language programming model could, or should, be extended to address these\nweaknesses."}
