{"id": "2508.19373", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19373", "abs": "https://arxiv.org/abs/2508.19373", "authors": ["Haoran Lin", "Xianzhi Yu", "Kang Zhao", "Han Bao", "Zongyuan Zhan", "Ting Hu", "Wulong Liu", "Zekun Yin", "Xin Li", "Weiguo Liu"], "title": "HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference", "comment": null, "summary": "Current inference systems for Mixture-of-Experts (MoE) models primarily\nemploy static parallelization strategies. However, these static approaches\ncannot consistently achieve optimal performance across different inference\nscenarios, as they lack the flexibility to adapt to varying computational\nrequirements. In this work, we propose HAP (Hybrid Adaptive Parallelism), a\nnovel method that dynamically selects hybrid parallel strategies to enhance MoE\ninference efficiency. The fundamental innovation of HAP lies in hierarchically\ndecomposing MoE architectures into two distinct computational modules: the\nAttention module and the Expert module, each augmented with a specialized\ninference latency simulation model. This decomposition promotes the\nconstruction of a comprehensive search space for seeking model parallel\nstrategies. By leveraging Integer Linear Programming (ILP), HAP could solve the\noptimal hybrid parallel configurations to maximize inference efficiency under\nvarying computational constraints. Our experiments demonstrate that HAP\nconsistently determines parallel configurations that achieve comparable or\nsuperior performance to the TP strategy prevalent in mainstream inference\nsystems. Compared to the TP-based inference, HAP-based inference achieves\nspeedups of 1.68x, 1.77x, and 1.57x on A100, A6000, and V100 GPU platforms,\nrespectively. Furthermore, HAP showcases remarkable generalization capability,\nmaintaining performance effectiveness across diverse MoE model configurations,\nincluding Mixtral and Qwen series models."}
{"id": "2508.19299", "categories": ["cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.19299", "abs": "https://arxiv.org/abs/2508.19299", "authors": ["Rishov Sarkar", "Cong Hao"], "title": "OmniSim: Simulating Hardware with C Speed and RTL Accuracy for High-Level Synthesis Designs", "comment": "13 pages, 8 figures. Accepted at MICRO 2025", "summary": "High-Level Synthesis (HLS) is increasingly popular for hardware design using\nC/C++ instead of Register-Transfer Level (RTL). To express concurrent hardware\nbehavior in a sequential language like C/C++, HLS tools introduce constructs\nsuch as infinite loops and dataflow modules connected by FIFOs. However,\nefficiently and accurately simulating these constructs at C level remains\nchallenging. First, without hardware timing information, functional\nverification typically requires slow RTL synthesis and simulation, as the\ncurrent approaches in commercial HLS tools. Second, cycle-accurate performance\nmetrics, such as end-to-end latency, also rely on RTL simulation. No existing\nHLS tool fully overcomes the first limitation. For the second, prior work such\nas LightningSim partially improves simulation speed but lacks support for\nadvanced dataflow features like cyclic dependencies and non-blocking FIFO\naccesses.\n  To overcome both limitations, we propose OmniSim, a framework that\nsignificantly extends the simulation capabilities of both academic and\ncommercial HLS tools. First, OmniSim enables fast and accurate simulation of\ncomplex dataflow designs, especially those explicitly declared unsupported by\ncommercial tools. It does so through sophisticated software multi-threading,\nwhere threads are orchestrated by querying and updating a set of FIFO tables\nthat explicitly record exact hardware timing of each FIFO access. Second,\nOmniSim achieves near-C simulation speed with near-RTL accuracy for both\nfunctionality and performance, via flexibly coupled and overlapped\nfunctionality and performance simulations.\n  We demonstrate that OmniSim successfully simulates eleven designs previously\nunsupported by any HLS tool, achieving up to 35.9x speedup over traditional\nC/RTL co-simulation, and up to 6.61x speedup over the state-of-the-art yet less\ncapable simulator, LightningSim, on its own benchmark suite."}
{"id": "2508.19452", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19452", "abs": "https://arxiv.org/abs/2508.19452", "authors": ["Andrea Esposito", "Francesco P. Rossi", "Marco Bernardo", "Francesco Fabris", "Hubert Garavel"], "title": "Formal Modeling and Verification of the Algorand Consensus Protocol in CADP", "comment": null, "summary": "Algorand is a scalable and secure permissionless blockchain that achieves\nproof-of-stake consensus via cryptographic self-sortition and binary Byzantine\nagreement. In this paper, we present a process algebraic model of the Algorand\nconsensus protocol with the aim of enabling rigorous formal verification. Our\nmodel captures the behavior of participants with respect to the structured\nalternation of consensus steps toward a committee-based agreement by means of a\nprobabilistic process calculus. We validate the correctness of the protocol in\nthe absence of adversaries and then extend our model to capture the influence\nof coordinated malicious nodes that can force the commit of an empty block\ninstead of the proposed one. The adversarial scenario is analyzed by using an\nequivalence-checking-based noninterference framework that we have implemented\nin the CADP verification toolkit. In addition to highlighting both the\nrobustness and the limitations of the Algorand protocol under adversarial\nassumptions, this work illustrates the added value of using formal methods for\nthe analysis of blockchain consensus algorithms."}
{"id": "2508.19393", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19393", "abs": "https://arxiv.org/abs/2508.19393", "authors": ["Phuoc Pham", "Arun Venkitaraman", "Chia-Yu Hsieh", "Andrea Bonetti", "Stefan Uhlich", "Markus Leibl", "Simon Hofmann", "Eisaku Ohbuchi", "Lorenzo Servadei", "Ulf Schlichtmann", "Robert Wille"], "title": "GENIE-ASI: Generative Instruction and Executable Code for Analog Subcircuit Identification", "comment": null, "summary": "Analog subcircuit identification is a core task in analog design, essential\nfor simulation, sizing, and layout. Traditional methods often require extensive\nhuman expertise, rule-based encoding, or large labeled datasets. To address\nthese challenges, we propose GENIE-ASI, the first training-free, large language\nmodel (LLM)-based methodology for analog subcircuit identification. GENIE-ASI\noperates in two phases: it first uses in-context learning to derive natural\nlanguage instructions from a few demonstration examples, then translates these\ninto executable Python code to identify subcircuits in unseen SPICE netlists.\nIn addition, to evaluate LLM-based approaches systematically, we introduce a\nnew benchmark composed of operational amplifier netlists (op-amps) that cover a\nwide range of subcircuit variants. Experimental results on the proposed\nbenchmark show that GENIE-ASI matches rule-based performance on simple\nstructures (F1-score = 1.0), remains competitive on moderate abstractions\n(F1-score = 0.81), and shows potential even on complex subcircuits (F1-score =\n0.31). These findings demonstrate that LLMs can serve as adaptable,\ngeneral-purpose tools in analog design automation, opening new research\ndirections for foundation model applications in analog design automation."}
{"id": "2508.19495", "categories": ["cs.DC", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19495", "abs": "https://arxiv.org/abs/2508.19495", "authors": ["Muhammad Ahmed Mohsin", "Junaid Ahmad", "Muhammad Hamza Nawaz", "Muhammad Ali Jamshed"], "title": "Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks", "comment": "Submitted as a chapter to the book Ambient Intelligence for 6G", "summary": "Ambient intelligence (AmI) is a computing paradigm in which physical\nenvironments are embedded with sensing, computation, and communication so they\ncan perceive people and context, decide appropriate actions, and respond\nautonomously. Realizing AmI at global scale requires sixth generation (6G)\nwireless networks with capabilities for real time perception, reasoning, and\naction aligned with human behavior and mobility patterns. We argue that\nGenerative Artificial Intelligence (GenAI) is the creative core of such\nenvironments. Unlike traditional AI, GenAI learns data distributions and can\ngenerate realistic samples, making it well suited to close key AmI gaps,\nincluding generating synthetic sensor and channel data in under observed areas,\ntranslating user intent into compact, semantic messages, predicting future\nnetwork conditions for proactive control, and updating digital twins without\ncompromising privacy.\n  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,\nand generative transformers, and connects them to practical AmI use cases,\nincluding spectrum sharing, ultra reliable low latency communication,\nintelligent security, and context aware digital twins. We also examine how 6G\nenablers, such as edge and fog computing, IoT device swarms, intelligent\nreflecting surfaces (IRS), and non terrestrial networks, can host or accelerate\ndistributed GenAI. Finally, we outline open challenges in energy efficient on\ndevice training, trustworthy synthetic data, federated generative learning, and\nAmI specific standardization. We show that GenAI is not a peripheral addition,\nbut a foundational element for transforming 6G from a faster network into an\nambient intelligent ecosystem."}
{"id": "2508.19530", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.19530", "abs": "https://arxiv.org/abs/2508.19530", "authors": ["Yanyun Wang", "Dingcui Yu", "Yina Lv", "Yunpeng Song", "Yumiao Zhao", "Liang Shi"], "title": "RARO: Reliability-aware Conversion with Enhanced Read Performance for QLC SSDs", "comment": null, "summary": "Quad-level cell (QLC) flash offers significant benefits in cost and capacity,\nbut its limited reliability leads to frequent read retries, which severely\ndegrade read performance. A common strategy in high-density flash storage is to\nprogram selected blocks in a low-density mode (SLC), sacrificing some capacity\nto achieve higher I/O performance. This hybrid storage architecture has been\nwidely adopted in consumer-grade storage systems. However, existing hybrid\nstorage schemes typically focus on write performance and rely solely on data\ntemperature for migration decisions. This often results in excessive mode\nswitching, causing substantial capacity overhead.\n  In this paper, we present RARO (Reliability-Aware Read performance\nOptimization), a hybrid flash management scheme designed to improve read\nperformance with minimal capacity cost. The key insight behind RARO is that\nmuch of the read slowdown in QLC flash is caused by read retries. RARO triggers\ndata migration only when hot data resides in QLC blocks experiencing a high\nnumber of read retries, significantly reducing unnecessary conversions and\ncapacity loss. Moreover, RARO supports fine-grained multi-mode conversions\n(SLC-TLC-QLC) to further minimize capacity overhead. By leveraging real-time\nread retry statistics and flash characteristics, RARO mitigates over-conversion\nand optimizes I/O performance. Experiments on the FEMU platform demonstrate\nthat RARO significantly improves read performance across diverse workloads,\nwith negligible impact on usable capacity."}
{"id": "2508.19559", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19559", "abs": "https://arxiv.org/abs/2508.19559", "authors": ["Rongzhi Li", "Ruogu Du", "Zefang Chu", "Sida Zhao", "Chunlei Han", "Zuocheng Shi", "Yiwen Shao", "Huanle Han", "Long Huang", "Zherui Liu", "Shufan Liu"], "title": "Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference", "comment": null, "summary": "Serving Large Language Models (LLMs) is a GPU-intensive task where\ntraditional autoscalers fall short, particularly for modern Prefill-Decode\n(P/D) disaggregated architectures. This architectural shift, while powerful,\nintroduces significant operational challenges, including inefficient use of\nheterogeneous hardware, network bottlenecks, and critical imbalances between\nprefill and decode stages. We introduce HeteroScale, a coordinated autoscaling\nframework that addresses the core challenges of P/D disaggregated serving.\nHeteroScale combines a topology-aware scheduler that adapts to heterogeneous\nhardware and network constraints with a novel metric-driven policy derived from\nthe first large-scale empirical study of autoscaling signals in production. By\nleveraging a single, robust metric to jointly scale prefill and decode pools,\nHeteroScale maintains architectural balance while ensuring efficient, adaptive\nresource management. Deployed in a massive production environment on tens of\nthousands of GPUs, HeteroScale has proven its effectiveness, increasing average\nGPU utilization by a significant 26.6 percentage points and saving hundreds of\nthousands of GPU-hours daily, all while upholding stringent service level\nobjectives."}
{"id": "2508.19656", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.19656", "abs": "https://arxiv.org/abs/2508.19656", "authors": ["Polykarpos Vergos", "Theofanis Vergos", "Florentia Afentaki", "Konstantinos Balaskas", "Georgios Zervakis"], "title": "Support Vector Machines Classification on Bendable RISC-V", "comment": "Accepted for publication at the IEEE Computer Society Annual\n  Symposium on VLSI (ISVLSI '25)", "summary": "Flexible Electronics (FE) technology offers uniquecharacteristics in\nelectronic manufacturing, providing ultra-low-cost, lightweight, and\nenvironmentally-friendly alternatives totraditional rigid electronics. These\ncharacteristics enable a rangeof applications that were previously constrained\nby the costand rigidity of conventional silicon technology. Machine learning\n(ML) is essential for enabling autonomous, real-time intelligenceon devices\nwith smart sensing capabilities in everyday objects. However, the large feature\nsizes and high power consumption ofthe devices oppose a challenge in the\nrealization of flexible ML applications. To address the above, we propose an\nopen-source framework for developing ML co-processors for the Bendable RISC-V\ncore. In addition, we present a custom ML accelerator architecture for Support\nVector Machine (SVM), supporting both one-vs-one (OvO) and one-vs-rest (OvR)\nalgorithms. Our ML accelerator adopts a generic, precision-scalable design,\nsupporting 4-, 8-, and 16-bit weight representations. Experimental results\ndemonstrate a 21x improvement in both inference execution time and energy\nefficiency, on average, highlighting its potential for low-power, flexible\nintelligence on the edge."}
{"id": "2508.19670", "categories": ["cs.DC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19670", "abs": "https://arxiv.org/abs/2508.19670", "authors": ["Diogo Costa", "Jose Martins", "Sandro Pinto"], "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems", "comment": null, "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation."}
{"id": "2508.19868", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19868", "abs": "https://arxiv.org/abs/2508.19868", "authors": ["Geraldo F. Oliveira"], "title": "New Tools, Programming Models, and System Support for Processing-in-Memory Architectures", "comment": "Doctoral thesis", "summary": "Our goal in this dissertation is to provide tools, programming models, and\nsystem support for PIM architectures (with a focus on DRAM-based solutions), to\nease the adoption of PIM in current and future systems. To this end, we make at\nleast four new major contributions.\n  First, we introduce DAMOV, the first rigorous methodology to characterize\nmemory-related data movement bottlenecks in modern workloads, and the first\ndata movement benchmark suite. Second, we introduce MIMDRAM, a new\nhardware/software co-designed substrate that addresses the major current\nprogrammability and flexibility limitations of the bulk bitwise execution model\nof processing-using-DRAM (PUD) architectures. MIMDRAM enables the allocation\nand control of only the needed computing resources inside DRAM for PUD\ncomputing. Third, we introduce Proteus, the first hardware framework that\naddresses the high execution latency of bulk bitwise PUD operations in\nstate-of-the-art PUD architectures by implementing a data-aware runtime engine\nfor PUD. Proteus reduces the latency of PUD operations in three different ways:\n(i) Proteus concurrently executes independent in-DRAM primitives belong to a\nsingle PUD operation across DRAM arrays. (ii) Proteus dynamically reduces the\nbit-precision (and consequentially the latency and energy consumption) of PUD\noperations by exploiting narrow values (i.e., values with many leading zeros or\nones). (iii) Proteus chooses and uses the most appropriate data representation\nand arithmetic algorithm implementation for a given PUD instruction\ntransparently to the programmer. Fourth, we introduce DaPPA (data-parallel\nprocessing-in-memory architecture), a new programming framework that eases\nprogrammability for general-purpose PNM architectures by allowing the\nprogrammer to write efficient PIM-friendly code without the need to manage\nhardware resources explicitly."}
{"id": "2508.19805", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19805", "abs": "https://arxiv.org/abs/2508.19805", "authors": ["Shota Naito", "Tsukasa Ninomiya", "Koichi Wada"], "title": "Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers", "comment": null, "summary": "Understanding the computational power of mobile robot systems is a\nfundamental challenge in distributed computing. While prior work has focused on\npairwise separations between models, we explore how robot capabilities, light\nobservability, and scheduler synchrony interact in more complex ways.\n  We first show that the Exponential Times Expansion (ETE) problem is solvable\nonly in the strongest model -- fully-synchronous robots with full mutual lights\n($\\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and\nTAR(d)* problems to demonstrate how internal memory and lights interact with\nsynchrony: under weak synchrony, internal memory alone is insufficient, while\nfull synchrony can substitute for both lights and memory.\n  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and\nZCC to show fine-grained separations between $\\mathcal{FSTA}$ and\n$\\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and\nLeave Place Convergence (LP-Cv), illustrating the limitations of internal\nmemory in symmetric settings.\n  These results extend the known separation map of 14 canonical robot models,\nrevealing structural phenomena only visible through higher-order comparisons.\nOur work provides new impossibility criteria and deepens the understanding of\nhow observability, memory, and synchrony collectively shape the computational\npower of mobile robots."}
{"id": "2508.20016", "categories": ["cs.DC", "cs.AI", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.20016", "abs": "https://arxiv.org/abs/2508.20016", "authors": ["Matthias Maiterth", "Wesley H. Brewer", "Jaya S. Kuruvella", "Arunavo Dey", "Tanzima Z. Islam", "Kevin Menear", "Dmitry Duplyakin", "Rashadul Kabir", "Tapasya Patki", "Terry Jones", "Feiyi Wang"], "title": "HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling", "comment": null, "summary": "Schedulers are critical for optimal resource utilization in high-performance\ncomputing. Traditional methods to evaluate schedulers are limited to\npost-deployment analysis, or simulators, which do not model associated\ninfrastructure. In this work, we present the first-of-its-kind integration of\nscheduling and digital twins in HPC. This enables what-if studies to understand\nthe impact of parameter configurations and scheduling decisions on the physical\nassets, even before deployment, or regarching changes not easily realizable in\nproduction. We (1) provide the first digital twin framework extended with\nscheduling capabilities, (2) integrate various top-tier HPC systems given their\npublicly available datasets, (3) implement extensions to integrate external\nscheduling simulators. Finally, we show how to (4) implement and evaluate\nincentive structures, as-well-as (5) evaluate machine learning based\nscheduling, in such novel digital-twin based meta-framework to prototype\nscheduling. Our work enables what-if scenarios of HPC systems to evaluate\nsustainability, and the impact on the simulated system."}
{"id": "2508.19868", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19868", "abs": "https://arxiv.org/abs/2508.19868", "authors": ["Geraldo F. Oliveira"], "title": "New Tools, Programming Models, and System Support for Processing-in-Memory Architectures", "comment": "Doctoral thesis", "summary": "Our goal in this dissertation is to provide tools, programming models, and\nsystem support for PIM architectures (with a focus on DRAM-based solutions), to\nease the adoption of PIM in current and future systems. To this end, we make at\nleast four new major contributions.\n  First, we introduce DAMOV, the first rigorous methodology to characterize\nmemory-related data movement bottlenecks in modern workloads, and the first\ndata movement benchmark suite. Second, we introduce MIMDRAM, a new\nhardware/software co-designed substrate that addresses the major current\nprogrammability and flexibility limitations of the bulk bitwise execution model\nof processing-using-DRAM (PUD) architectures. MIMDRAM enables the allocation\nand control of only the needed computing resources inside DRAM for PUD\ncomputing. Third, we introduce Proteus, the first hardware framework that\naddresses the high execution latency of bulk bitwise PUD operations in\nstate-of-the-art PUD architectures by implementing a data-aware runtime engine\nfor PUD. Proteus reduces the latency of PUD operations in three different ways:\n(i) Proteus concurrently executes independent in-DRAM primitives belong to a\nsingle PUD operation across DRAM arrays. (ii) Proteus dynamically reduces the\nbit-precision (and consequentially the latency and energy consumption) of PUD\noperations by exploiting narrow values (i.e., values with many leading zeros or\nones). (iii) Proteus chooses and uses the most appropriate data representation\nand arithmetic algorithm implementation for a given PUD instruction\ntransparently to the programmer. Fourth, we introduce DaPPA (data-parallel\nprocessing-in-memory architecture), a new programming framework that eases\nprogrammability for general-purpose PNM architectures by allowing the\nprogrammer to write efficient PIM-friendly code without the need to manage\nhardware resources explicitly."}
