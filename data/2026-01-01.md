<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Enforcing Temporal Constraints for LLM Agents](https://arxiv.org/abs/2512.23738)
*Adharsh Kamath,Sishen Zhang,Calvin Xu,Shubham Ugare,Gagandeep Singh,Sasa Misailovic*

Main category: cs.PL

TL;DR: Agent-C：一个为LLM智能体提供运行时安全保障的框架，确保智能体遵守形式化的时序安全属性，通过SMT求解和约束生成技术实现100%安全合规。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体被部署在安全关键应用中，但现有的防护系统无法防止违反时序安全策略的行为。现有方法依赖不精确的自然语言指令或事后监控，无法提供形式化保证。

Method: Agent-C引入领域特定语言表达时序属性，将规范转换为一阶逻辑，使用SMT求解在token生成期间检测不合规动作，并利用约束生成技术确保所有动作符合规范。

Result: 在零售客服和机票预订系统中评估，Agent-C实现了完美安全（100%合规，0%伤害），同时提高了任务效用。在Claude Sonnet 4.5和GPT-5上，合规率分别从77.4%和83.7%提升到100%，效用也相应提高。

Conclusion: Agent-C为可靠的智能体推理设定了新的最先进前沿，通过形式化方法确保LLM智能体在安全关键应用中的时序安全合规性。

Abstract: LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning.

</details>


### [2] [Towards representation agnostic probabilistic programming](https://arxiv.org/abs/2512.23740)
*Ole Fenske,Maximilian Popko,Sebastian Bader,Thomas Kirste*

Main category: cs.PL

TL;DR: 提出一种因子抽象层作为概率编程的通用接口，支持混合不同表示形式，解决现有工具中模型表示与推理算法紧耦合的问题。


<details>
  <summary>Details</summary>
Motivation: 当前概率编程语言和工具将模型表示与特定推理算法紧密耦合，这限制了实验新的表示形式或混合离散-连续模型的能力。需要一种更灵活的框架来支持复杂混合模型的推理。

Method: 引入因子抽象概念，定义五个基本操作作为通用接口，可以操作各种底层表示形式的因子（如离散表、高斯分布、基于采样的方法等），实现表示无关的概率编程。

Result: 创建了一个统一的框架，允许用户在单个系统中自由混合不同表示形式，支持当前工具无法充分表达的复杂混合模型的实用推理。

Conclusion: 因子抽象层为概率编程提供了表示无关的通用接口，突破了现有工具的限制，使复杂混合模型的推理成为可能，为概率编程系统的设计提供了新的方向。

Abstract: Current probabilistic programming languages and tools tightly couple model representations with specific inference algorithms, preventing experimentation with novel representations or mixed discrete-continuous models. We introduce a factor abstraction with five fundamental operations that serve as a universal interface for manipulating factors regardless of their underlying representation. This enables representation-agnostic probabilistic programming where users can freely mix different representations (e.g. discrete tables, Gaussians distributions, sample-based approaches) within a single unified framework, allowing practical inference in complex hybrid models that current toolkits cannot adequately express.

</details>


### [3] [VGC: A High-Performance Zone-Based Garbage Collector Architecture for Python with Partitioning and Parallel Execution](https://arxiv.org/abs/2512.23768)
*Abdulla M*

Main category: cs.PL

TL;DR: VGC提出了一种新颖的双层垃圾收集框架，通过主动层（运行时并发标记清除）和被动层（编译时静态分配优化）相结合，显著提升了内存管理性能，减少暂停时间达30%，降低内存使用达25%。


<details>
  <summary>Details</summary>
Motivation: 传统垃圾收集器在多样化系统（从资源受限的嵌入式设备到高性能并行架构）中难以平衡性能与开销，需要一种更高效、可预测的内存管理方案。

Method: 采用双层架构：Active VGC（运行时）使用并发标记清除策略管理动态对象；Passive VGC（编译时）通过预测性内存映射优化静态对象分配，对齐缓存边界减少碎片。

Result: 相比分代收集器，多线程基准测试中暂停时间减少30%；内存使用减少25%；提供可预测的内存访问模式，提升现代并行应用的扩展性。

Conclusion: VGC通过整合编译时和运行时优化，为内存密集型系统提供了一个鲁棒且适应性强的解决方案，适用于从低级到高级的各种编程环境。

Abstract: The Virtual Garbage Collector (VGC) introduces a novel memory management framework designed to optimize performance across diverse systems, ranging from resource constrained embedded devices to high performance parallel architectures. Unlike conventional garbage collectors, VGC employs a dual layer architecture consisting of Active VGC and Passive VGC to enable efficient, low overhead memory management. Active VGC dynamically manages runtime objects using a concurrent mark and sweep strategy tailored for parallel workloads, reducing pause times by up to 30 percent compared to generational collectors in multithreaded benchmarks. Passive VGC operates at compile time and optimizes static object allocation through predictive memory mapping, minimizing fragmentation by aligning objects to cache boundaries. This separation of responsibilities ensures predictable memory access patterns, reduces total memory usage by up to 25 percent, and improves scalability for modern parallel applications. By integrating compile time and runtime optimizations, VGC provides a robust and adaptable solution for memory intensive systems across both low level and high level programming environments.

</details>


### [4] [State Space Estimation for DPOR-based Model Checkers](https://arxiv.org/abs/2512.23996)
*A. R. Balasubramanian,Mohammad Hossein Khoshechin Jorshari,Rupak Majumdar,Umang Mathur,Minjian Zhang*

Main category: cs.PL

TL;DR: 提出首个可证明的多项式时间无偏估计器，用于计算并发程序的Mazurkiewicz迹等价类数量，解决模型检查资源分配问题


<details>
  <summary>Details</summary>
Motivation: 并发程序的Mazurkiewicz迹等价类数量估计对基于枚举的模型检查至关重要：它决定了模型检查的运行时间和搜索空间覆盖程度。现有方法无法高效解决这个#P-hard问题

Method: 1) 证明计数问题是#P-hard且不可近似；2) 将无状态最优DPOR算法转换为无偏估计器，将其探索视为有界深度/宽度树；3) 应用Knuth估计器；4) 使用随机枚举控制方差，维护小规模部分路径种群；5) 在JMC模型检查器中实现

Result: 在共享内存基准测试中，即使状态空间有10^5-10^6个等价类，通过数百次试验即可获得稳定估计（通常在20%误差范围内）。同时展示了如何通过加权所有探索图来估计模型检查成本

Conclusion: 提出了首个可证明的多项式时间无偏估计器，用于计算并发程序迹数量，解决了模型检查资源分配中的重要问题。方法在实际基准测试中表现良好，为模型检查资源分配提供了实用工具

Abstract: We study the estimation problem for concurrent programs: given a bounded program $P$, estimate the number of Mazurkiewicz trace-equivalence classes induced by its interleavings. This quantity informs two practical questions for enumeration-based model checking: how long a model checking run is likely to take, and what fraction of the search space has been covered so far. We first show the counting problem is #P-hard even for restricted programs and, unless $P=NP$, inapproximable within any subexponential factor, ruling out efficient exact or randomized approximation algorithms. We give a Monte Carlo approach to obtain a poly-time unbiased estimator: we convert a stateless optimal DPOR algorithm into an unbiased estimator by viewing its exploration as a bounded-depth, bounded-width tree whose leaves are the maximal Mazurkiewicz traces. A classical estimator by Knuth, when run on this tree, yields an unbiased estimate. To control the variance, we apply stochastic enumeration by maintaining a small population of partial paths per depth whose evolution is coupled. We have implemented our estimator in the JMC model checker and evaluated it on shared-memory benchmarks. With modest budgets, our estimator yields stable estimates, typically within a 20% band, within a few hundred trials, even when the state space has $10^5$--$10^6$ classes. We also show how the same machinery estimates model-checking cost by weighting all explored graphs, not only complete traces. Our algorithms provide the first provable poly-time unbiased estimators for counting traces, a problem of considerable importance when allocating model checking resources.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Governing Cloud Data Pipelines with Agentic AI](https://arxiv.org/abs/2512.23737)
*Aswathnarayan Muthukrishnan Kirubakaran,Adithya Parthasarathy,Nitin Saksena,Ram Sekhar Bodala,Akshay Deshpande,Suhas Malempati,Shiva Carimireddy,Abhirup Mazumder*

Main category: cs.DC

TL;DR: 论文提出Agentic Cloud Data Engineering平台，通过集成有界AI代理到云数据管道的治理和控制平面，实现动态、策略感知的自动化管理，显著提升恢复效率、降低成本和减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 当前云数据管道在动态工作负载、演进模式、成本约束和严格治理要求下运行，但大多数生产管道依赖静态配置和反应式操作实践，导致恢复时间长、资源利用效率低、人工开销大。

Method: 提出Agentic Cloud Data Engineering平台，采用策略感知控制架构，集成有界AI代理到治理和控制平面。专门代理分析管道遥测和元数据，基于声明式成本和合规策略进行推理，提出受限操作行动（如自适应资源重配置、模式协调、自动化故障恢复），所有代理行动都经过治理策略验证以确保可预测和可审计行为。

Result: 实验使用代表性批处理和流分析工作负载构建自公共企业风格数据集。结果显示：与静态编排相比，平台将平均管道恢复时间减少高达45%，降低运营成本约25%，减少手动干预事件超过70%，同时保持数据新鲜度和策略合规性。

Conclusion: 策略有界代理控制为在企业环境中治理云数据管道提供了一种有效且实用的方法，能够显著改善运营效率、成本效益和自动化水平。

Abstract: Cloud data pipelines increasingly operate under dynamic workloads, evolving schemas, cost constraints, and strict governance requirements. Despite advances in cloud-native orchestration frameworks, most production pipelines rely on static configurations and reactive operational practices, resulting in prolonged recovery times, inefficient resource utilization, and high manual overhead. This paper presents Agentic Cloud Data Engineering, a policy-aware control architecture that integrates bounded AI agents into the governance and control plane of cloud data pipelines. In Agentic Cloud Data Engineering platform, specialized agents analyze pipeline telemetry and metadata, reason over declarative cost and compliance policies, and propose constrained operational actions such as adaptive resource reconfiguration, schema reconciliation, and automated failure recovery. All agent actions are validated against governance policies to ensure predictable and auditable behavior. We evaluate Agentic Cloud Data Engineering platform using representative batch and streaming analytics workloads constructed from public enterprise-style datasets. Experimental results show that Agentic Cloud Data Engineering platform reduces mean pipeline recovery time by up to 45%, lowers operational cost by approximately 25%, and decreases manual intervention events by over 70% compared to static orchestration, while maintaining data freshness and policy compliance. These results demonstrate that policy-bounded agentic control provides an effective and practical approach for governing cloud data pipelines in enterprise environments.

</details>


### [6] [Squeezing Edge Performance: A Sensitivity-Aware Container Management for Heterogeneous Tasks](https://arxiv.org/abs/2512.23952)
*Yongmin Zhang,Pengyu Huang,Mingyi Dong,Jing Yao*

Main category: cs.DC

TL;DR: 提出基于测量的容器资源管理框架CRMS，通过非线性拟合模型和队列延迟公式，将MINLP问题分解为凸子问题求解，在边缘服务器上降低延迟14%并提升能效


<details>
  <summary>Details</summary>
Motivation: 边缘计算需要处理延迟敏感应用，但任务异构性和资源有限性给高效编排带来挑战。现有方案难以在单节点上同时优化延迟和能耗，特别是在动态异构工作负载下

Method: 1) 通过大量剖析实验建立CPU/内存分配与处理延迟的非线性拟合模型；2) 结合队列延迟公式构建MINLP问题；3) 将NP-hard问题分解为凸子问题；4) 提出两阶段容器资源管理方案(CRMS)，结合凸优化和贪心细化

Result: CRMS相比启发式和搜索基线，降低延迟超过14%，提升能源效率。方案具有多项式时间复杂度，支持全局资源约束下的准动态执行，为异构边缘环境提供实用可扩展方案

Conclusion: 提出的测量驱动容器资源管理框架能有效解决边缘计算中异构应用的资源优化问题，通过模型驱动的优化方法在延迟和能耗间取得良好平衡，适用于动态工作负载特征的边缘环境

Abstract: Edge computing enables latency-critical applications to process data close to end devices, yet task heterogeneity and limited resources pose significant challenges to efficient orchestration. This paper presents a measurement-driven, container-based resource management framework for intra-node optimization on a single edge server hosting multiple heterogeneous applications. Extensive profiling experiments are conducted to derive a nonlinear fitting model that characterizes the relationship among CPU/memory allocations and processing latency across diverse workloads, enabling reliable estimation of performance under varying configurations and providing quantitative support for subsequent optimization. Using this model and a queueing-based delay formulation, we formulate a mixed-integer nonlinear programming (MINLP) problem to jointly minimize system latency and power consumption, which is shown to be NP-hard. The problem is decomposed into tractable convex subproblems and solved through a two-stage container-based resource management scheme (CRMS) combining convex optimization and greedy refinement. The proposed scheme achieves polynomial-time complexity and supports quasi-dynamic execution under global resource constraints. Simulation results demonstrate that CRMS reduces latency by over 14\% and improves energy efficiency compared with heuristic and search-based baselines, offering a practical and scalable solution for heterogeneous edge environments with dynamic workload characteristics.

</details>


### [7] [Data Heterogeneity-Aware Client Selection for Federated Learning in Wireless Networks](https://arxiv.org/abs/2512.24286)
*Yanbing Yang,Huiling Zhu,Wenchi Cheng,Jingqing Wang,Changrun Chen,Jiangzhou Wang*

Main category: cs.DC

TL;DR: 提出联合客户端选择与资源分配方案，解决联邦学习中数据异构性导致的泛化误差、延迟和能耗问题


<details>
  <summary>Details</summary>
Motivation: 联邦学习在无线网络中面临通信计算资源限制和客户端数据异构性问题，特别是大规模网络中数据异构性会导致泛化误差增大、重复训练周期、能耗增加和延迟延长

Method: 首先理论分析客户端数据异构性对全局模型泛化误差的影响，然后构建联合优化问题最小化学习延迟和能耗并约束泛化误差，提出联合客户端选择与资源分配方法，采用凸优化和松弛技术

Result: 仿真结果表明，相比不考虑数据异构性的基准方法，提出的CSRA方案实现了更高的测试精度、更低的学习延迟和更低的能耗

Conclusion: 通过理论分析和联合优化客户端选择与资源分配，能有效解决联邦学习中数据异构性带来的问题，提高系统效率

Abstract: Federated Learning (FL) enables mobile edge devices, functioning as clients, to collaboratively train a decentralized model while ensuring local data privacy. However, the efficiency of FL in wireless networks is limited not only by constraints on communication and computational resources but also by significant data heterogeneity among clients, particularly in large-scale networks. This paper first presents a theoretical analysis of the impact of client data heterogeneity on global model generalization error, which can result in repeated training cycles, increased energy consumption, and prolonged latency. Based on the theoretical insights, an optimization problem is formulated to jointly minimize learning latency and energy consumption while constraining generalization error. A joint client selection and resource allocation (CSRA) approach is then proposed, employing a series of convex optimization and relaxation techniques. Extensive simulation results demonstrate that the proposed CSRA scheme yields higher test accuracy, reduced learning latency, and lower energy consumption compared to baseline methods that do not account for data heterogeneity.

</details>


### [8] [PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression](https://arxiv.org/abs/2512.24449)
*Bo Jiang,Taolue Yang,Youyuan Liu,Xubin He,Sheng Di,Sian Jin*

Main category: cs.DC

TL;DR: PackKV是一个针对长上下文生成的KV缓存管理框架，通过专门设计的压缩算法和系统架构协同设计，显著减少内存占用并提高执行吞吐量。


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型的长上下文推理面临KV缓存内存需求巨大的挑战，随着序列长度和批次大小增加，KV缓存可达数GB，限制了实际应用。

Method: PackKV引入专门针对KV缓存数据特性的有损压缩技术，结合压缩算法和系统架构的协同设计，支持KV缓存的动态增长特性，同时保持高计算效率。

Result: 在相同精度损失下，PackKV相比现有量化方法平均实现K缓存153.2%、V缓存179.6%的内存减少率；在A100和RTX Pro 6000 GPU上，相比cuBLAS矩阵向量乘法核，平均吞吐量提升K为75.7%、V为171.7%。

Conclusion: PackKV是一个高效通用的KV缓存管理框架，通过创新的压缩技术和系统优化，有效解决了长上下文生成中的内存瓶颈问题，同时显著提升了计算性能。

Abstract: Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \textbf{153.2}\% higher memory reduction rate for the K cache and \textbf{179.6}\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \textbf{75.7}\% for K and \textbf{171.7}\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV

</details>


### [9] [Understanding LLM Checkpoint/Restore I/O Strategies and Patterns](https://arxiv.org/abs/2512.24511)
*Mikaila J. Gossman,Avinash Maurya,Bogdan Nicolae,Jon C. Calhoun*

Main category: cs.DC

TL;DR: 本文研究LLM训练中检查点/恢复的I/O性能问题，通过liburing优化实现比现有方案最高7.6倍的吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 随着LLM和基础模型规模扩大，3D并行训练中的检查点/恢复操作涉及大量进程管理各种形状大小的张量，频繁持久化到存储系统，形成了大数据I/O问题。现有方案在并发下存在性能瓶颈，需要探索kernel-accelerated I/O库如liburing在LLM检查点中的有效性。

Method: 开发微基准测试量化liburing的权衡，评估聚合、对齐和I/O合并在不同模式下的交互效果。研究文件系统感知的聚合策略，并与现有LLM检查点引擎进行对比。

Result: 未合并的小缓冲区操作使吞吐量减半，而文件系统感知的聚合恢复了带宽并减少了元数据开销。相比最先进的LLM检查点引擎，该方法实现了最高3.9倍于DataStates-LLM和7.6倍于TorchSnapshot的写入吞吐量提升。

Conclusion: LLM检查点需要与现代文件系统和I/O后端对齐的聚合和合并策略。liburing等kernel-accelerated I/O库能显著提升性能，但需要针对具体工作负载进行优化配置。

Abstract: As LLMs and foundation models scale, checkpoint/restore has become a critical pattern for training and inference. With 3D parallelism (tensor, pipeline, data), checkpointing involves many processes, each managing numerous tensors of varying shapes and sizes, that must be persisted frequently to stable storage (e.g., parallel file systems). This turns checkpoint/restore into a big-data I/O problem characterized by volume, variety, and velocity. The workflow must traverse the full storage stack -- from GPU memory through host memory and local storage to external repositories -- whose tiers differ by orders of magnitude in performance, creating bottlenecks under concurrency even with asynchronous flush/prefetch. Kernel-accelerated I/O libraries such as \texttt{liburing} may mitigate these issues versus POSIX, but their effectiveness for LLM checkpointing remains underexplored. We develop microbenchmarks to quantify trade-offs when using \texttt{liburing}, evaluating how aggregation, alignment, and I/O coalescing interact under buffered and direct I/O. We find that uncoalesced small-buffer operations halve throughput relative to synthetic workloads, while file system-aware aggregation restores bandwidth and reduces metadata overhead. Compared to state-of-the-art LLM checkpointing engines, our approach achieves up to $3.9\times$ higher write throughput than DataStates-LLM and $7.6\times$ higher than TorchSnapshot. These results highlight the need for aggregation and coalescing strategies that align with modern file systems and I/O backends.

</details>


### [10] [Distributed Bilevel Optimization with Dual Pruning for Resource-limited Clients](https://arxiv.org/abs/2512.24667)
*Mingyi Li,Xiao Zhang,Ruisheng Zheng,Hongjian Shi,Yuan Yuan,Xiuzhen Cheng,Dongxiao Yu*

Main category: cs.DC

TL;DR: 提出首个资源自适应的分布式双层优化框架，包含二阶自由超梯度估计器，使客户端能根据可用资源优化子模型，达到渐近最优收敛率


<details>
  <summary>Details</summary>
Motivation: 传统分布式双层优化算法无法直接应用于低资源客户端，主要原因是同时优化上下层函数涉及过多计算

Method: 提出资源自适应分布式双层优化框架，包含二阶自由超梯度估计器，允许客户端根据可用资源优化子模型，分析全局平均超梯度的上界

Result: RABO和RAFBO都能达到渐近最优收敛率O(1/√(C_x*Q))，由外层参数的最小覆盖率C_x*主导，实验证明方法的有效性和计算效率

Conclusion: 提出的资源自适应框架解决了低资源客户端下的分布式双层优化问题，在理论和实验上都表现出色

Abstract: With the development of large-scale models, traditional distributed bilevel optimization algorithms cannot be applied directly in low-resource clients. The key reason lies in the excessive computation involved in optimizing both the lower- and upper-level functions. Thus, we present the first resource-adaptive distributed bilevel optimization framework with a second-order free hypergradient estimator, which allows each client to optimize the submodels adapted to the available resources. Due to the coupled influence of partial outer parameters x and inner parameters y, it's challenging to theoretically analyze the upper bound regarding the globally averaged hypergradient for full model parameters. The error bound of inner parameter also needs to be reformulated since the local partial training. The provable theorems show that both RABO and RAFBO can achieve an asymptotically optimal convergence rate of $O(1/\sqrt{C_x^{\ast}Q})$, which is dominated by the minimum coverage of the outer parameter $C_x^{\ast}$. Extensive experiments on two different tasks demonstrate the effectiveness and computation efficiency of our proposed methods.

</details>


### [11] [AI-Driven Cloud Resource Optimization for Multi-Cluster Environments](https://arxiv.org/abs/2512.24914)
*Vinoth Punniyamoorthy,Akash Kumar Agarwal,Bikesh Kumar,Abhirup Mazumder,Kabilan Kannan,Sumit Saha*

Main category: cs.DC

TL;DR: 提出基于AI的多集群云系统自适应资源优化框架，通过预测学习和策略感知决策实现跨集群主动协调管理，相比传统反应式方法提高了资源效率并减少性能波动。


<details>
  <summary>Details</summary>
Motivation: 现代云原生系统依赖多集群部署实现可扩展性、弹性和地理分布，但现有资源管理方法主要是反应式和集群中心的，无法在动态负载下优化系统级行为，导致资源利用效率低、适应延迟和运维开销增加。

Method: 提出AI驱动的自适应资源优化框架，整合预测学习、策略感知决策和持续反馈机制，通过分析跨集群遥测数据和历史执行模式，动态调整资源分配以平衡性能、成本和可靠性目标。

Result: 原型实现表明，相比传统反应式方法，该框架提高了资源效率，在负载波动时实现更快稳定，并减少了性能变异性。

Conclusion: 智能、自适应的基础设施管理是构建可扩展和弹性云平台的关键使能技术，AI驱动的资源优化框架能有效解决多集群环境中的资源管理挑战。

Abstract: Modern cloud-native systems increasingly rely on multi-cluster deployments to support scalability, resilience, and geographic distribution. However, existing resource management approaches remain largely reactive and cluster-centric, limiting their ability to optimize system-wide behavior under dynamic workloads. These limitations result in inefficient resource utilization, delayed adaptation, and increased operational overhead across distributed environments. This paper presents an AI-driven framework for adaptive resource optimization in multi-cluster cloud systems. The proposed approach integrates predictive learning, policy-aware decision-making, and continuous feedback to enable proactive and coordinated resource management across clusters. By analyzing cross-cluster telemetry and historical execution patterns, the framework dynamically adjusts resource allocation to balance performance, cost, and reliability objectives. A prototype implementation demonstrates improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional reactive approaches. The results highlight the effectiveness of intelligent, self-adaptive infrastructure management as a key enabler for scalable and resilient cloud platforms.

</details>


### [12] [Reliable and Resilient Collective Communication Library for LLM Training and Serving](https://arxiv.org/abs/2512.25059)
*Wei Wang,Nengneng Yu,Sixian Xiong,Zaoxing Liu*

Main category: cs.DC

TL;DR: R²CCL是一个容错通信库，通过利用多NIC硬件实现无损、低开销的故障转移，显著减少GPU训练和推理中的网络故障恢复时间。


<details>
  <summary>Details</summary>
Motivation: 现代ML训练和推理扩展到成千上万个GPU，网络故障会导致10-15%的GPU时间浪费在缓慢的恢复过程中。常见的网络错误和链路波动会触发超时，通常导致整个作业终止，迫使在训练中进行昂贵的检查点回滚，在推理中重新处理请求。

Method: R²CCL通过利用多NIC硬件实现容错通信，包括快速连接迁移、带宽感知负载重新分配和弹性集体算法，以在故障下保持进度。

Result: R²CCL对NIC故障具有高度鲁棒性，训练开销小于1%，推理开销小于3%。在性能上分别优于基线AdapCC和DejaVu 12.18倍和47倍。

Conclusion: R²CCL通过创新的容错通信机制，显著减少了大规模ML系统中的网络故障恢复开销，提高了GPU利用率和系统可靠性。

Abstract: Modern ML training and inference now span tens to tens of thousands of GPUs, where network faults can waste 10--15\% of GPU hours due to slow recovery. Common network errors and link fluctuations trigger timeouts that often terminate entire jobs, forcing expensive checkpoint rollback during training and request reprocessing during inference. We present R$^2$CCL, a fault-tolerant communication library that provides lossless, low-overhead failover by exploiting multi-NIC hardware. R$^2$CCL performs rapid connection migration, bandwidth-aware load redistribution, and resilient collective algorithms to maintain progress under failures. We evaluate R$^2$CCL on two 8-GPU H100 InfiniBand servers and via large-scale ML simulators modeling hundreds of GPUs with diverse failure patterns. Experiments show that R$^2$CCL is highly robust to NIC failures, incurring less than 1\% training and less than 3\% inference overheads. R$^2$CCL outperforms baselines AdapCC and DejaVu by 12.18$\times$ and 47$\times$, respectively.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [HERO-Sign: Hierarchical Tuning and Efficient Compiler-Time GPU Optimizations for SPHINCS+ Signature Generation](https://arxiv.org/abs/2512.23969)
*Yaoyun Zhou,Qian Wang*

Main category: cs.AR

TL;DR: HERO Sign：基于GPU加速的SPHINCS+签名方案，通过层次化调优和编译时优化，显著提升后量子安全签名生成性能


<details>
  <summary>Details</summary>
Motivation: SPHINCS+作为无状态哈希签名方案具有强后量子安全性，但签名生成缓慢（大量哈希计算）。GPU具有大规模并行性可加速SPHINCS+，但现有GPU优化未能充分利用Merkle树结构并行性或缺乏细粒度编译器级定制。

Method: 1. 重新审视SPHINCS+组件（FORS、MSS、WOTS+）的数据独立性并行机会；2. 为FORS引入Tree Fusion策略，通过自动Tree Tuning搜索算法适应不同GPU架构；3. 采用自适应编译策略，根据FORS Sign、TREE Sign、WOTS+ Sign等内核效果选择PTX或原生代码路径；4. 批量签名生成时使用基于任务图的构造优化内核级重叠，减少多流空闲时间和内核启动开销。

Result: 在RTX 4090上，相比现有GPU实现，HERO Sign在SPHINCS+ 128f、192f、256f参数集下分别获得1.28-3.13、1.28-2.92、1.24-2.60倍的吞吐量提升。在A100、H100、GTX 2080上观察到类似增益，内核启动延迟降低两个数量级。

Conclusion: HERO Sign通过层次化调优和编译时优化，有效解决了SPHINCS+ GPU加速中的并行性利用不足和编译器定制缺乏问题，显著提升了后量子安全签名的生成性能。

Abstract: SPHINCS+ is a stateless hash-based signature scheme that provides strong post quantum security, but its signature generation is slow due to intensive hash computations. GPUs offer massive parallelism that can potentially accelerate SPHINCS+ signatures. However, existing GPU-based optimizations either fail to fully exploit the inherent parallelism of SPHINCS+'s Merkle tree structure or lack fine-grained, compiler-level customization across its diverse computational kernels. This paper proposes HERO Sign, a GPU-accelerated SPHINCS+ implementation that adopts hierarchical tuning and efficient compiler time optimizations. HERO Sign reexamines the parallelization opportunities enabled by data independence across SPHINCS+ components, including FORS, MSS, and WOTS+. It introduces a Tree Fusion strategy for FORS, which contains a large number of independent branches. The fusion strategy is guided by an automated Tree Tuning search algorithm that adapts fusion schemes to different GPU architectures. To further improve performance, HERO Sign employs an adaptive compilation strategy that accounts for the varying effectiveness of compiler optimizations across SPHINCS+ kernels such as FORS Sign, TREE Sign, and WOTS+ Sign. During compilation, the strategy automatically selects between PTX and native code paths to maximize efficiency. For batched signature generation, HERO Sign optimizes kernel-level overlapping using a task graph-based construction to reduce multi-stream idle time and kernel launch overhead. Experimental results show that, compared to state of the art GPU implementations, HERO Sign achieves throughput improvements of 1.28-3.13, 1.28-2.92, and 1.24-2.60 under the SPHINCS+ 128f, 192f, and 256f parameter sets on RTX 4090. Similar gains are observed on A100, H100, and GTX 2080, along with a two orders of magnitude reduction in kernel launch latency.

</details>
