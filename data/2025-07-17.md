<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Quantum circuits are just a phase](https://arxiv.org/abs/2507.11676)
*Chris Heunen,Louis Lemonnier,Christopher McNally,Alex Rice*

Main category: cs.PL

TL;DR: 论文提出了一种新型量子编程语言，通过相位操作和子空间选择提升抽象性和表达能力，支持更高级的量子算法设计。


<details>
  <summary>Details</summary>
Motivation: 当前量子编程语言抽象层次低，阻碍了可扩展性、清晰性和高级推理支持。

Method: 结合全局相位操作和量子版“if let”构造，聚焦于特征分解、共轭和控制酉操作。

Result: 证明了语言的通用性，展示了其在多种量子算法中的自然表达，并实现了原型编译器。

Conclusion: 该语言为更抽象和结构化的量子编程提供了原则性和实用的步骤。

Abstract: Quantum programs today are written at a low level of abstraction - quantum
circuits akin to assembly languages - and even advanced quantum programming
languages essentially function as circuit description languages. This state of
affairs impedes scalability, clarity, and support for higher-level reasoning.
More abstract and expressive quantum programming constructs are needed.
  To this end, we introduce a novel yet simple quantum programming language for
generating unitaries from "just a phase"; we combine a (global) phase operation
that captures phase shifts with a quantum analogue of the "if let" construct
that captures subspace selection via pattern matching. This minimal language
lifts the focus from quantum gates to eigendecomposition, conjugation, and
controlled unitaries; common building blocks in quantum algorithm design.
  We demonstrate several aspects of the expressive power of our language in
several ways. Firstly, we establish that our representation is universal by
deriving a universal quantum gate set. Secondly, we show that important quantum
algorithms can be expressed naturally and concisely, including Grover's search
algorithm, Hamiltonian simulation, Quantum Fourier Transform, Quantum Signal
Processing, and the Quantum Eigenvalue Transformation. Furthermore, we give
clean denotational semantics grounded in categorical quantum mechanics.
Finally, we implement a prototype compiler that efficiently translates terms of
our language to quantum circuits, and prove that it is sound with respect to
these semantics. Collectively, these contributions show that this construct
offers a principled and practical step toward more abstract and structured
quantum programming.

</details>


### [2] [Picat Through the Lens of Advent of Code](https://arxiv.org/abs/2507.11731)
*Neng-Fa Zhou,Cristian Grozea,Håkan Kjellerstrand,Oisín Mac Fhearaí*

Main category: cs.PL

TL;DR: 本文展示了Picat语言如何利用其逻辑、约束求解和动态编程特性高效解决2024 Advent of Code问题。


<details>
  <summary>Details</summary>
Motivation: 探讨Picat语言在多范式编程中的优势，特别是在解决特定类型问题（如逆向工程和路径查找）时的表现。

Method: 使用Picat的内置约束求解、模式匹配、回溯和动态编程功能，实现Advent of Code问题的解决方案。

Result: Picat的SAT约束求解和动态编程功能能够简洁高效地解决问题，相比命令式语言节省大量开发时间。

Conclusion: Picat的多范式特性使其成为解决复杂问题的理想选择，尤其在需要逻辑和约束求解的场景中。

Abstract: Picat is a logic-based, multi-paradigm programming language that integrates
features from logic, functional, constraint, and imperative programming
paradigms. This paper presents solutions to several problems from the 2024
Advent of Code (AoC). While AoC problems are not designed for any specific
programming language, certain problem types, such as reverse engineering and
path-finding, are particularly well-suited to Picat due to its built-in
constraint solving, pattern matching, backtracking, and dynamic programming
with tabling. This paper demonstrates that Picat's features, especially its
SAT-based constraint solving and tabling, enable concise, declarative, and
highly efficient implementations of problems that would require significantly
more effort in imperative languages.

</details>


### [3] [Universal Synthesis of Differentiably Tunable Numerical Abstract Transformers](https://arxiv.org/abs/2507.11827)
*Shaurya Gomber,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.PL

TL;DR: 提出了一种通用的抽象转换器合成算法，用于数值抽象解释，支持多面体数值域和二次有界保护操作符（QGO），并通过梯度引导搜索优化转换器选择。


<details>
  <summary>Details</summary>
Motivation: 现有数值抽象解释器依赖手工定制的转换器，缺乏通用算法，限制了扩展性和组合推理能力。

Method: 提出通用转换器合成算法，支持参数化转换器家族，并引入自适应梯度引导（AGG）搜索策略。

Result: 在Zones、Octagons和Polyhedra三个数值抽象域中验证了算法的有效性，USTAD框架显著提升了精度。

Conclusion: 通用转换器合成算法解决了现有方法的局限性，支持灵活、精确的组合推理。

Abstract: Numerical abstract interpretation is a widely used framework for the static
analysis of numerical programs. However, existing numerical abstract
interpreters rely on hand-crafted, instruction-specific transformers tailored
to each domain, with no general algorithm for handling common operations across
domains. This limits extensibility, prevents precise compositional reasoning
over instruction sequences, and forces all downstream tasks to use the same
fixed transformer regardless of their precision, efficiency, or task-specific
requirements. To address these limitations, we propose a universal transformer
synthesis algorithm that constructs a parametric family of sound abstract
transformers for any given polyhedral numerical domain and a concrete operator
from the class of Quadratic-Bounded Guarded Operators (QGO), which includes
both individual instructions and structured sequences. Each instantiation in
this family is sound by construction, enabling downstream analyses to adapt the
transformer to their particular needs. The space of transformers is
differentiable but complex. To efficiently explore this space of transformers,
we introduce the Adaptive Gradient Guidance (AGG) procedure, a gradient-guided
search strategy that steers the search process based on downstream analysis
objectives and runtime constraints. We implement these ideas in the USTAD
framework and evaluate their effectiveness across three numerical abstract
domains: Zones, Octagons, and Polyhedra. Our results demonstrate that the
universal synthesis algorithm successfully constructs sound families of
transformers across domains, and that USTAD achieves significant, tunable
precision gains over baselines by leveraging compositional reasoning and
efficient gradient-guided traversal of the transformer space.

</details>


### [4] [Towards Relational Contextual Equality Saturation](https://arxiv.org/abs/2507.11897)
*Tyler Hou,Shadaj Laddad,Joseph M. Hellerstein*

Main category: cs.PL

TL;DR: 本文探讨了如何将上下文等式饱和技术扩展到关系模型中，总结了现有方法、应用及挑战。


<details>
  <summary>Details</summary>
Motivation: 扩展上下文等式饱和技术以支持关系模型，提升程序优化能力。

Method: 结合现有上下文等式饱和技术，探索其在关系模型中的应用。

Result: 总结了现有方法、应用，并指出了在关系模型中实现的主要挑战。

Conclusion: 上下文等式饱和技术在关系模型中的应用具有潜力，但仍需解决关键挑战。

Abstract: Equality saturation is a powerful technique for program optimization.
Contextual equality saturation extends this to support rewrite rules that are
conditioned on where a term appears in an expression. Existing work has brought
contextual reasoning to egg; in this paper, we share our ongoing work to extend
this to relational equality saturation in egglog. We summarize the existing
approaches to contextual equality saturation, outline its main applications,
and identify key challenges in combining this approach with relational models.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [The AI Shadow War: SaaS vs. Edge Computing Architectures](https://arxiv.org/abs/2507.11545)
*Rhea Pritham Marpu,Kevin J McNamara,Preeti Gupta*

Main category: cs.DC

TL;DR: 本文分析了边缘AI与云端AI在计算能力、能效和数据隐私方面的竞争，指出边缘AI凭借高效能和数据主权优势，正在挑战云端AI的主导地位。


<details>
  <summary>Details</summary>
Motivation: 探讨边缘AI与云端AI的竞争格局，揭示边缘AI在能效、隐私和实时性方面的优势。

Method: 通过分析计算能力、能效和数据隐私等关键指标，结合最新技术突破（如测试时训练和专家混合架构），对比边缘AI与云端AI的性能。

Result: 边缘AI在能效上具有10,000倍优势，同时保障数据主权，市场预计从2025年的90亿美元增长到2030年的496亿美元（年复合增长率38.5%）。

Conclusion: 边缘AI的分布式架构与高效信息处理相契合，预示未来将形成混合边缘-云端生态系统。

Abstract: The very DNA of AI architecture presents conflicting paths: centralized
cloud-based models (Software-as-a-Service) versus decentralized edge AI (local
processing on consumer devices). This paper analyzes the competitive
battleground across computational capability, energy efficiency, and data
privacy. Recent breakthroughs show edge AI challenging cloud systems on
performance, leveraging innovations like test-time training and
mixture-of-experts architectures. Crucially, edge AI boasts a 10,000x
efficiency advantage: modern ARM processors consume merely 100 microwatts
forinference versus 1 watt for equivalent cloud processing. Beyond efficiency,
edge AI secures data sovereignty by keeping processing local, dismantling
single points of failure in centralized architectures. This democratizes access
throughaffordable hardware, enables offline functionality, and reduces
environmental impact by eliminating data transmission costs. The edge AI market
projects explosive growth from $9 billion in 2025 to $49.6 billion by 2030
(38.5% CAGR), fueled by privacy demands and real-time analytics. Critical
applications including personalized education, healthcare monitoring,
autonomous transport, and smart infrastructure rely on edge AI's ultra-low
latency (5-10ms versus 100-500ms for cloud). The convergence of architectural
innovation with fundamental physics confirms edge AI's distributed approach
aligns with efficient information processing, signaling the inevitable
emergence of hybrid edge-cloud ecosystems.

</details>


### [6] [A Model Aware AIGC Task Offloading Algorithm in IIoT Edge Computing](https://arxiv.org/abs/2507.11560)
*Xin Wang,Xiao Huan Li,Xun Wang*

Main category: cs.DC

TL;DR: 本文提出了一种针对IIoT边缘计算环境的AIGC任务卸载框架，首次考虑了模型切换带来的延迟和能耗，并通过MADDPG-MATO算法优化性能。


<details>
  <summary>Details</summary>
Motivation: IIoT与AIGC的结合为智能制造带来新机遇，但也面临计算密集和低延迟的挑战，传统云计算方法难以满足实时需求。

Method: 提出基于多智能体深度确定性策略梯度（MADDPG-MATO）的AIGC任务卸载算法，优化延迟和能耗。

Result: 实验表明，MADDPG-MATO在延迟、能耗和任务完成率上均优于基线算法。

Conclusion: 该算法在动态高负载IIoT环境中表现出鲁棒性和高效性。

Abstract: The integration of the Industrial Internet of Things (IIoT) with Artificial
Intelligence-Generated Content (AIGC) offers new opportunities for smart
manufacturing, but it also introduces challenges related to
computation-intensive tasks and low-latency demands. Traditional generative
models based on cloud computing are difficult to meet the real-time
requirements of AIGC tasks in IIoT environments, and edge computing can
effectively reduce latency through task offloading. However, the dynamic nature
of AIGC tasks, model switching delays, and resource constraints impose higher
demands on edge computing environments. To address these challenges, this paper
proposes an AIGC task offloading framework tailored for IIoT edge computing
environments, considering the latency and energy consumption caused by AIGC
model switching for the first time. IIoT devices acted as multi-agent
collaboratively offload their dynamic AIGC tasks to the most appropriate edge
servers deployed with different generative models. A model aware AIGC task
offloading algorithm based on Multi-Agent Deep Deterministic Policy Gradient
(MADDPG-MATO) is devised to minimize the latency and energy. Experimental
results show that MADDPG-MATO outperforms baseline algorithms, achieving an
average reduction of 6.98% in latency, 7.12% in energy consumption, and a 3.72%
increase in task completion rate across four sets of experiments with model
numbers ranging from 3 to 6, it is demonstrated that the proposed algorithm is
robust and efficient in dynamic, high-load IIoT environments.

</details>


### [7] [Environmentally-Conscious Cloud Orchestration Considering Geo-Distributed Data Centers](https://arxiv.org/abs/2507.11563)
*Giulio Attenni,Novella Bartolini*

Main category: cs.DC

TL;DR: 论文提出了一种理论框架，用于在云环境中实现环保意识的任务部署和迁移，旨在减少资源供应的环境影响并满足可持续性需求。


<details>
  <summary>Details</summary>
Motivation: 随着对可持续云服务需求的增长，客户需要基于可持续性指标选择数据中心运营商，并准确报告服务的生态足迹。

Method: 分析可持续性报告，定义数据中心的环境影响概况，建立优化模型以平衡多种环境因素和用户偏好。

Result: 模拟案例研究表明，该方法相比仅优化单一可持续性因素的基线策略具有潜力。

Conclusion: 该框架为云服务的可持续性优化提供了理论支持，展示了多因素平衡的优势。

Abstract: This paper presents a theoretical discussion for environmentally-conscious
job deployment and migration in cloud environments, aiming to minimize the
environmental impact of resource provisioning while incorporating
sustainability requirements. As the demand for sustainable cloud services
grows, it is crucial for cloud customers to select data center operators based
on sustainability metrics and to accurately report the ecological footprint of
their services. To this end, we analyze sustainability reports and define
comprehensive environmental impact profiles for data centers, incorporating key
sustainability indicators. We formalize the problem as an optimization model,
balancing multiple environmental factors while respecting user preferences. A
simulative case study demonstrates the {potential} of our approach compared to
baseline strategies that optimize for single sustainability factors.

</details>


### [8] [PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed Training](https://arxiv.org/abs/2507.11683)
*Seth Ockerman,Amal Gueroudji,Tanwi Mallick,Yixuan He,Line Pouchard,Robert Ross,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: PGT-I扩展了PyTorch Geometric Temporal，通过分布式数据并行训练和两种新策略（index-batching和distributed-index-batching）解决了ST-GNNs在大规模数据集上的内存限制问题，显著降低了内存开销并提升了训练速度。


<details>
  <summary>Details</summary>
Motivation: ST-GNNs在建模时空数据依赖方面表现强大，但受限于内存约束，主要应用于小规模数据集。分布式训练缺乏对时空模型的支持，且未充分利用时空数据的特性。

Method: 提出了PGT-I，集成了分布式数据并行训练和两种新策略：index-batching（动态构建快照以减少内存开销）和distributed-index-batching（支持多GPU扩展）。

Result: 在PeMS数据集上首次实现了无需图分割的ST-GNN训练，峰值内存使用降低89%，128 GPU下速度提升13.1倍。

Conclusion: PGT-I通过优化时空数据结构和分布式训练，显著提升了ST-GNNs在大规模数据集上的可扩展性和效率。

Abstract: Spatiotemporal graph neural networks (ST-GNNs) are powerful tools for
modeling spatial and temporal data dependencies. However, their applications
have been limited primarily to small-scale datasets because of memory
constraints. While distributed training offers a solution, current frameworks
lack support for spatiotemporal models and overlook the properties of
spatiotemporal data. Informed by a scaling study on a large-scale workload, we
present PyTorch Geometric Temporal Index (PGT-I), an extension to PyTorch
Geometric Temporal that integrates distributed data parallel training and two
novel strategies: index-batching and distributed-index-batching. Our index
techniques exploit spatiotemporal structure to construct snapshots dynamically
at runtime, significantly reducing memory overhead, while
distributed-index-batching extends this approach by enabling scalable
processing across multiple GPUs. Our techniques enable the first-ever training
of an ST-GNN on the entire PeMS dataset without graph partitioning, reducing
peak memory usage by up to 89\% and achieving up to a 13.1x speedup over
standard DDP with 128 GPUs.

</details>


### [9] [Arctic Inference with Shift Parallelism: Fast and Efficient Open Source Inference System for Enterprise AI](https://arxiv.org/abs/2507.11830)
*Samyam Rajbhandari,Mert Hidayetoglu,Aurick Qiao,Ye Wang,Juncheng Yang,Jeff Rasley,Michael Wyatt,Yuxiong He*

Main category: cs.DC

TL;DR: Arctic Inference通过动态并行策略和优化技术，显著提升了AI推理的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统在延迟、吞吐量和成本之间存在权衡，难以满足实际需求。

Method: 采用Shift Parallelism动态并行策略，结合推测解码、SwiftKV计算减少和优化的嵌入推理。

Result: 实现了请求完成速度提升3.4倍，生成速度提升1.75倍，嵌入推理达到1.6M tokens/sec每GPU。

Conclusion: Arctic Inference为企业和社区提供了高效、低成本的AI推理解决方案。

Abstract: Inference is now the dominant AI workload, yet existing systems force
trade-offs between latency, throughput, and cost. Arctic Inference, an
open-source vLLM plugin from Snowflake AI Research, introduces Shift
Parallelism, a dynamic parallelism strategy that adapts to real-world traffic
while integrating speculative decoding, SwiftKV compute reduction, and
optimized embedding inference. It achieves up to 3.4 times faster request
completion, 1.75 times faster generation, and 1.6M tokens/sec per GPU for
embeddings, outperforming both latency- and throughput-optimized deployments.
Already powering Snowflake Cortex AI, Arctic Inference delivers
state-of-the-art, cost-effective inference for enterprise AI and is now
available to the community.

</details>


### [10] [Performance Assessment of Load Balancing Methods in Cloud Computing: Analysis of Round Robin, Equally Spread, and Throttled Strategies Using Cloud Analyst](https://arxiv.org/abs/2507.11899)
*Saeid Aghasoleymani Najafabadi*

Main category: cs.DC

TL;DR: 论文研究了云计算中的负载均衡策略，比较了不同算法在集中式和分布式资源设置下的性能，发现分布式环境中响应时间显著降低，智能负载均衡对优化成本和性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 随着云环境中工作负载的动态性和不可预测性增加，传统的静态负载均衡方法已无法满足需求，需要更智能、自适应的策略来优化资源分配和服务质量。

Method: 使用Cloud Analyst模拟工具评估不同负载均衡算法（如Round Robin、Equally Spread和Throttled）在集中式和分布式资源设置下的性能表现。

Result: 在单数据中心中，Round Robin算法处理时间略优；而在分布式环境中，Equally Spread和Throttled算法表现更佳，显著降低响应时间和运营成本。

Conclusion: 智能、动态的负载均衡和资源管理策略对满足云需求、优化成本和保持竞争优势至关重要，需持续评估和整合新兴技术。

Abstract: Load balancing plays a pivotal role in cloud computing, ensuring that
resources are optimally allocated to maintain high service quality and
operational efficiency. As workloads in cloud environments become increasingly
dynamic and unpredictable, load balancing strategies are evolving from
traditional static methods to more adaptive and intelligent approaches. In this
study, the Cloud Analyst simulation tool was used to evaluate the performance
of different load balancing algorithms under various scenarios, including both
centralized and distributed resource setups. The results highlight that while
the Round Robin algorithm yields slightly better processing times within a
single data center, Equally Spread and Throttled techniques perform
competitively, especially when network latency is considered. More importantly,
when resources are distributed across multiple data centers, response times are
significantly reduced, emphasizing the value of proximity and efficient load
distribution. In these distributed environments, Equally Spread and Throttled
algorithms not only maintain quick response times but also contribute to lower
operational costs. These findings demonstrate the necessity of strategic
resource placement and proactive infrastructure planning to balance performance
and cost. Adopting intelligent, dynamic load balancing and resource management
practices can help organizations meet evolving cloud demands, optimize costs,
and maintain a competitive advantage. Continuous evaluation and integration of
emerging technologies are crucial for sustaining effective and scalable cloud
operations.

</details>


### [11] [Making Serverless Computing Extensible: A Case Study of Serverless Data Analytics](https://arxiv.org/abs/2507.11929)
*Minchen Yu,Yinghao Ren,Jiamu Zhao,Jiaqi Li*

Main category: cs.DC

TL;DR: 提出了一种可扩展的无服务器计算设计原则，通过Proteus平台实现，支持开发者自定义控制行为以优化性能。


<details>
  <summary>Details</summary>
Motivation: 解决通用无服务器平台性能不足与专用系统复杂性之间的矛盾。

Method: 提出可扩展设计原则，并在Proteus平台中实现，引入决策工作流抽象。

Result: Proteus原型优化了分析查询执行并支持细粒度资源共享。

Conclusion: 可扩展设计原则为无服务器计算提供了性能优化与通用性的平衡。

Abstract: Serverless computing has attracted a broad range of applications due to its
ease of use and resource elasticity. However, developing serverless
applications often poses a dilemma -- relying on general-purpose serverless
platforms can fall short of delivering satisfactory performance for complex
workloads, whereas building application-specific serverless systems undermines
the simplicity and generality. In this paper, we propose an extensible design
principle for serverless computing. We argue that a platform should enable
developers to extend system behaviors for domain-specialized optimizations
while retaining a shared, easy-to-use serverless environment. We take data
analytics as a representative serverless use case and realize this design
principle in Proteus. Proteus introduces a novel abstraction of decision
workflows, allowing developers to customize control-plane behaviors for
improved application performance. Preliminary results show that Proteus's
prototype effectively optimizes analytical query execution and supports
fine-grained resource sharing across diverse applications.

</details>


### [12] [NineToothed: A Triton-Based High-Level Domain-Specific Language for Machine Learning](https://arxiv.org/abs/2507.11978)
*Jiacheng Huang,Zimin Li,Yinghui Li,Haojie Wang*

Main category: cs.DC

TL;DR: NineToothed是一种支持串行语义的领域特定语言，通过自动将串行代码转换为并行代码，简化了深度学习计算内核的开发，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习领域特定语言（如Triton）要求开发者具备并行编程专业知识，且涉及大量底层细节，增加了开发和维护的复杂性。因此，开发支持串行编程的新模型至关重要。

Method: NineToothed提供了一种面向张量的元编程语言（TOM），采用“排列-应用”范式，无需管理底层细节即可表达分块计算，并包含一个高性能并行代码生成器。

Result: 评估结果表明，NineToothed能显著简化计算内核开发，同时性能与Triton相当。

Conclusion: NineToothed通过串行语义和自动并行化，为深度学习工作负载提供了一种更简单且高效的开发方式。

Abstract: The emergence of deep learning domain-specific languages (DSLs) has
substantially reduced the obstacles in developing high-performance,
cross-platform compute kernels. However, current DSLs, such as Triton, still
demand that developers possess expertise in parallel programming and expose
them to many low-level details. This requirement complicates the development
process and adds to the difficulty of maintaining compute kernels.
Consequently, developing a new programming model that supports serial
programming for deep learning workloads is crucial.
  This paper introduces NineToothed, a domain-specific language that offers
serial semantics for machine learning programming. Through the automatic
transformation of serial code into parallel code, NineToothed significantly
streamlines the development process while causing minimal performance
degradation. NineToothed encompasses (1) a language with tensor-oriented
metaprogramming (TOM) that adopts the arrange-and-apply paradigm, enabling the
expression of tiled computations without the need to manage low-level details
and (2) a code generator for generating high-performance parallel code. Our
evaluation results indicate that NineToothed can greatly simplify compute
kernel development while maintaining performance comparable to that of Triton.

</details>


### [13] [ARRC: Explainable, Workflow-Integrated Recommender for Sustainable Resource Optimization Across the Edge-Cloud Continuum](https://arxiv.org/abs/2507.12032)
*Brian-Frederik Jahnke,René Brinkhege,Jan Peter Meyer,Daniel Tebernum,Falk Howar*

Main category: cs.DC

TL;DR: ARRC是一个基于软件工程设计原则的推荐系统，用于在边缘-云系统中实现安全、透明且低成本的资源优化，显著减少操作员工作量并提高计算利用率。


<details>
  <summary>Details</summary>
Motivation: 解决边缘-云系统中资源过度配置和操作复杂性带来的问题，现有解决方案多为被动、单层抽象或需要侵入性平台更改，无法实现效率和可维护性的提升。

Method: 引入ARRC推荐系统，通过可审计的代理封装优化逻辑，提供跨层资源建议，直接集成到操作员工作流程中。

Result: 在多区域工业部署中，ARRC减少操作员工作量50%以上，计算利用率提升7.7倍，错误率低于5%。

Conclusion: ARRC展示了基于可解释推荐的架构可以在生产规模上实现可持续的效率和可维护性改进。

Abstract: Achieving sustainable, explainable, and maintainable automation for resource
optimization is a core challenge across the edge-cloud continuum. Persistent
overprovisioning and operational complexity often stem from heterogeneous
platforms and layered abstractions, while systems lacking explainability and
maintainability become fragile, impede safe recovery, and accumulate technical
debt. Existing solutions are frequently reactive, limited to single abstraction
layers, or require intrusive platform changes, leaving efficiency and
maintainability gains unrealized.
  This paper addresses safe, transparent, and low-effort resource optimization
in dynamic, multi-tenant edge-cloud systems, without disrupting operator
workflows or increasing technical debt. We introduce ARRC, a recommender system
rooted in software engineering design principles, which delivers explainable,
cross-layer resource recommendations directly into operator workflows (such as
tickets and GitOps pull requests). ARRC encapsulates optimization logic in
specialized, auditable agents coordinated via a shared interface, supporting
maintainability and extensibility through transparency and the ability to
inspect both recommendations and their rationale.
  Empirical evaluation in a multi-region industrial deployment shows that ARRC
reduces operator workload by over 50%, improves compute utilization by up to
7.7x, and maintains error rates below 5%, with most benefits achieved through
incremental, operator-approved changes. This demonstrates that explainable,
recommendation-based architectures can achieve sustainable efficiency and
maintainability improvements at production scale.
  ARRC provides an empirically evaluated framework for integrating explainable,
workflow-driven automation into resource management, intended to advance best
practices for robust, maintainable, and transparent edge-cloud continuum
platforms.

</details>


### [14] [Distributed Algorithms for Potential Problems](https://arxiv.org/abs/2507.12038)
*Alkida Balliu,Thomas Boudier,Francesco d'Amore,Dennis Olivetti,Gustav Schmid,Jukka Suomela*

Main category: cs.DC

TL;DR: 本文提出了一种快速分布式算法，用于解决局部势能问题，特别是局部最优割问题，填补了确定性LOCAL模型中算法复杂度的空白。


<details>
  <summary>Details</summary>
Motivation: 局部势能问题（如局部最优割）在分布式计算中的复杂度一直存在较大差距，本文旨在缩小这一差距。

Method: 提出了一种适用于有界度图的快速分布式算法，能够在确定性及随机LOCAL模型中高效解决局部势能问题。

Result: 算法在确定性及随机LOCAL模型中的复杂度为$\log^{O(1)} n$，解决了局部最优割问题的确定性复杂度问题。

Conclusion: 本文填补了局部势能问题在分布式算法复杂度上的空白，为相关问题提供了高效解决方案。

Abstract: In this work we present a fast distributed algorithm for local potential
problems: these are graph problems where the task is to find a locally optimal
solution where no node can unilaterally improve the utility in its local
neighborhood by changing its own label. A simple example of such a problem is
the task of finding a locally optimal cut, i.e., a cut where for each node at
least half of its incident edges are cut edges. The distributed round
complexity of locally optimal cut has been wide open; the problem is known to
require $\Omega(\log n)$ rounds in the deterministic LOCAL model and
$\Omega(\log \log n)$ rounds in the randomized LOCAL model, but the only known
upper bound is the trivial brute-force solution of $O(n)$ rounds. Locally
optimal cut in bounded-degree graphs is perhaps the simplest example of a
locally checkable labeling problem for which there is still such a large gap
between current upper and lower bounds. We show that in bounded-degree graphs,
all local potential problems, including locally optimal cut, can be solved in
$\log^{O(1)} n$ rounds, both in the deterministic and randomized LOCAL models.
In particular, the deterministic round complexity of the locally optimal cut
problem is now settled to $\log^{\Theta(1)} n$.

</details>


### [15] [Urban Green Governance: IoT-Driven Management and Enhancement of Urban Green Spaces in Campobasso](https://arxiv.org/abs/2507.12106)
*Antonio Salis,Gabriele Troina,Gianluca Boanelli,Marco Ottaviano,Paola Fortini,Soraya Versace*

Main category: cs.DC

TL;DR: 论文探讨了通过数字化和物联网技术优化城市绿地管理，提升居民生活质量。


<details>
  <summary>Details</summary>
Motivation: 公共绿地的有效管理对城市居民健康和福祉至关重要，世界卫生组织等机构也强调了这一点。

Method: 采用物联网系统和数据驱动平台，实时监测树木和绿地健康状况，结合机器学习优化灌溉。

Result: 开发了基于云的决策支持平台，通过实时数据和预测模型优化绿地管理。

Conclusion: 数字化和技术创新可支持可持续城市治理，增强环境韧性并改善居民生活质量。

Abstract: The efficient design and management of public green spaces is a key factor in
promoting the health and well-being of urban population, as emphasized by the
WHO, UNEP, and EEA. These areas serve as the "green lungs" of the urban
ecosystem, playing a vital role in enhancing quality of life thanks to the
provision of ecosystem services. In this context, the Smart Green City use case
in Campobasso municipality, funded by the Italian Ministry of Enterprises
(MIMIT), emerges as an innovative model for the sustainable management of green
urban areas through the adoption of an advanced system of emerging technologies
integrated and interoperable. The project integrates IoT systems and
data-driven governance platforms, enabling real-time monitoring of the health
status of trees and green areas via a Decision Support System (DSS). It also
facilitates the collection and analysis of data from diverse sources, including
weather conditions, air quality, soil moisture, pollution levels. The resulting
cloud-based platform supports a holistic real time decision making for green
urban managers, technical experts and operational staff. It enables intelligent
control and management of urban green spaces using Tree Talker sensors,
integrated with soil moisture and water potential monitoring systems. Thanks to
predictive models based on machine learning algorithms and real time data
provided by IoT sensors, irrigation of public parks can be optimized by
providing suggestions on when and how much water to apply. Customized alerts
layers are also activated warning users when monitored parameters, such as soil
temperature, humidity, or water potential, exceed predefined thresholds. This
Use Case demonstrates how digitalization, IoT sensors fusion and technological
innovation can support sustainable urban governance, fostering environmental
resilience and improving citizens quality of life.

</details>


### [16] [Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed Storage](https://arxiv.org/abs/2507.12205)
*Junqing Lin,Jingwei Sun,Mingge Lu,Guangzhong Sun*

Main category: cs.DC

TL;DR: EC-SpMV是一种针对稀疏大语言模型（LLM）推理优化的GPU加速方法，通过分层块提取算法和新型压缩稀疏格式（EC-CSR）显著提升性能并减少存储开销。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵-向量乘法（SpMV）在稀疏LLM推理中成为性能瓶颈，现有方法未充分利用稀疏LLM的结构特性，导致性能不佳和存储开销过大。

Method: 提出分层块提取算法捕获稀疏LLM的多粒度块结构，并设计EC-CSR格式通过增量索引减少存储开销和提升内存访问效率。

Result: 在LLaMA和OPT模型的稀疏权重矩阵上，EC-SpMV比现有SpMV库快6.44倍，存储开销比CSR减少55.4%。

Conclusion: EC-SpMV通过优化稀疏矩阵结构和存储格式，显著提升了稀疏LLM推理的性能和效率。

Abstract: Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance
bottleneck in the local deployment of sparse Large Language Models (LLMs),
where inference predominantly operates on workloads during the decoder phase
with a batch size of one. Existing SpMV kernels and sparse matrix formats,
originally designed for scientific computing, fail to exploit the unique
structure patterns inherent in sparse LLMs, resulting in suboptimal performance
and excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized
SpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a
hierarchical block extraction algorithm that captures multiple granularities of
block structures within sparse LLMs, and (2) a novel compressed sparse format
(EC-CSR) that employs delta indexing to reduce storage overhead and enhance
memory access efficiency. Evaluated on real sparse weight matrices from LLaMA
and OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV
libraries and reduces storage overhead by up to 55.4% compared to CSR.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [Double Duty: FPGA Architecture to Enable Concurrent LUT and Adder Chain Usage](https://arxiv.org/abs/2507.11709)
*Junius Pun,Xilai Dai,Grace Zgheib,Mahesh A. Iyer,Andrew Boutros,Vaughn Betz,Mohamed S. Abdelfattah*

Main category: cs.AR

TL;DR: 论文提出了一种名为Double Duty的逻辑块架构，通过允许加法器和LUT在逻辑单元中同时使用，提高了FPGA的算术密度，实现了显著的面积优化。


<details>
  <summary>Details</summary>
Motivation: 现代FPGA逻辑块架构限制了加法器和LUT的独立使用，阻碍了面积优化。研究旨在通过改进架构提升算术密度。

Method: 提出Double Duty架构，利用现有输入绕过LUT直接连接加法器链，未增加额外逻辑簇输入。

Result: 实验显示，在Stratix-10类架构上，面积减少了21.6%（Kratos基准）、9.3%（Koios基准）和8.2%（VTR基准），且不影响关键路径延迟。

Conclusion: Double Duty架构通过增加加法器链的灵活性，显著提升了FPGA的面积效率，平均面积-延迟积改善了9.7%。

Abstract: Flexibility and customization are key strengths of Field-Programmable Gate
Arrays (FPGAs) when compared to other computing devices. For instance, FPGAs
can efficiently implement arbitrary-precision arithmetic operations, and can
perform aggressive synthesis optimizations to eliminate ineffectual operations.
Motivated by sparsity and mixed-precision in deep neural networks (DNNs), we
investigate how to optimize the current logic block architecture to increase
its arithmetic density. We find that modern FPGA logic block architectures
prevent the independent use of adder chains, and instead only allow adder chain
inputs to be fed by look-up table (LUT) outputs. This only allows one of the
two primitives -- either adders or LUTs -- to be used independently in one
logic element and prevents their concurrent use, hampering area optimizations.
In this work, we propose the Double Duty logic block architecture to enable the
concurrent use of the adders and LUTs within a logic element. Without adding
expensive logic cluster inputs, we use 4 of the existing inputs to bypass the
LUTs and connect directly to the adder chain inputs. We accurately model our
changes at both the circuit and CAD levels using open-source FPGA development
tools. Our experimental evaluation on a Stratix-10-like architecture
demonstrates area reductions of 21.6% on adder-intensive circuits from the
Kratos benchmarks, and 9.3% and 8.2% on the more general Koios and VTR
benchmarks respectively. These area improvements come without an impact to
critical path delay, demonstrating that higher density is feasible on modern
FPGA architectures by adding more flexibility in how the adder chain is used.
Averaged across all circuits from our three evaluated benchmark set, our Double
Duty FPGA architecture improves area-delay product by 9.7%.

</details>


### [18] [MOFCO: Mobility- and Migration-Aware Task Offloading in Three-Layer Fog Computing Environments](https://arxiv.org/abs/2507.12028)
*Soheil Mahdizadeh,Elyas Oustad,Mohsen Ansari*

Main category: cs.AR

TL;DR: MOFCO是一种针对雾计算环境中任务卸载的新算法，通过结合进化博弈论和启发式方法，有效减少因用户设备移动性导致的系统成本。


<details>
  <summary>Details</summary>
Motivation: 解决雾计算环境中因用户设备移动性引发的服务迁移成本高和系统性能下降问题。

Method: 将任务卸载和资源分配建模为混合整数非线性规划问题，并采用启发式辅助的进化博弈论方法求解。

Result: 实验结果显示，MOFCO平均降低系统成本19%，某些场景下最高可达43%。

Conclusion: MOFCO显著优化了雾计算环境中的任务卸载性能，尤其在移动性强的场景中表现突出。

Abstract: Task offloading in three-layer fog computing environments presents a critical
challenge due to user equipment (UE) mobility, which frequently triggers costly
service migrations and degrades overall system performance. This paper
addresses this problem by proposing MOFCO, a novel Mobility- and
Migration-aware Task Offloading algorithm for Fog Computing environments. The
proposed method formulates task offloading and resource allocation as a
Mixed-Integer Nonlinear Programming (MINLP) problem and employs a
heuristic-aided evolutionary game theory approach to solve it efficiently. To
evaluate MOFCO, we simulate mobile users using SUMO, providing realistic
mobility patterns. Experimental results show that MOFCO reduces system cost,
defined as a combination of latency and energy consumption, by an average of
19% and up to 43% in certain scenarios compared to state-of-the-art methods.

</details>


### [19] [High-Performance Pipelined NTT Accelerators with Homogeneous Digit-Serial Modulo Arithmetic](https://arxiv.org/abs/2507.12418)
*George Alexakis,Dimitrios Schoinianakis,Giorgos Dimitrakopoulos*

Main category: cs.AR

TL;DR: 本文提出了一种基于数字串行模算术和冗余数据表示的流水线NTT加速器设计，以提高硬件效率和时钟频率。


<details>
  <summary>Details</summary>
Motivation: NTT在隐私保护技术（如全同态加密）中至关重要，但其硬件实现因大模数运算而受限，影响性能和效率。

Method: 采用数字串行模算术和冗余数据表示，设计无需中间（反）序列化的流水线NTT加速器。

Result: 实验表明，该方法在性能和硬件复杂度上优于现有技术。

Conclusion: 所提架构在保持并行性的同时提高了时钟频率，为实际FHE应用提供了高效硬件支持。

Abstract: The Number Theoretic Transform (NTT) is a fundamental operation in
privacy-preserving technologies, particularly within fully homomorphic
encryption (FHE). The efficiency of NTT computation directly impacts the
overall performance of FHE, making hardware acceleration a critical technology
that will enable realistic FHE applications. Custom accelerators, in FPGAs or
ASICs, offer significant performance advantages due to their ability to exploit
massive parallelism and specialized optimizations. However, the operation of
NTT over large moduli requires large word-length modulo arithmetic that limits
achievable clock frequencies in hardware and increases hardware area costs. To
overcome such deficits, digit-serial arithmetic has been explored for modular
multiplication and addition independently. The goal of this work is to leverage
digit-serial modulo arithmetic combined with appropriate redundant data
representation to design modular pipelined NTT accelerators that operate
uniformly on arbitrary small digits, without the need for intermediate
(de)serialization. The proposed architecture enables high clock frequencies
through regular pipelining while maintaining parallelism. Experimental results
demonstrate that the proposed approach outperforms state-of-the-art
implementations and reduces hardware complexity under equal performance and
input-output bandwidth constraints.

</details>


### [20] [Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length](https://arxiv.org/abs/2507.12442)
*Saptarshi Mitra,Rachid Karami,Haocheng Xu,Sitao Huang,Hyoukjun Kwon*

Main category: cs.AR

TL;DR: 论文比较了Transformer、SSM和混合模型在长上下文推理任务中的性能，发现SSM在长序列处理上优于Transformer，并提出硬件优化方向。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在处理长上下文输入时效率低下，需要研究新型架构（如SSM和混合模型）的实际性能以指导系统优化。

Method: 通过综合比较Transformer、SSM和混合模型在消费级和嵌入式GPU上的长上下文推理性能，并进行算子级分析。

Result: SSM在长序列处理上表现更优，能处理220K tokens（比Transformer长4倍），且在长上下文（~57K tokens）下快4倍。

Conclusion: SSM在长上下文任务中具有显著优势，未来硬件加速应关注定制SSM内核，论文将开源测试框架以促进研究。

Abstract: The demand for machine intelligence capable of processing continuous,
long-context inputs on local devices is growing rapidly. However, the quadratic
complexity and memory requirements of traditional Transformer architectures
make them inefficient and often unusable for these tasks. This has spurred a
paradigm shift towards new architectures like State Space Models (SSMs) and
hybrids, which promise near-linear scaling. While most current research focuses
on the accuracy and theoretical throughput of these models, a systematic
performance characterization on practical consumer hardware is critically
needed to guide system-level optimization and unlock new applications.
  To address this gap, we present a comprehensive, comparative benchmarking of
carefully selected Transformer, SSM, and hybrid models specifically for
long-context inference on consumer and embedded GPUs. Our analysis reveals that
SSMs are not only viable but superior for this domain, capable of processing
sequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than
comparable Transformers. While Transformers may be up to 1.8x faster at short
sequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x
faster at very long contexts (~57K tokens). Our operator-level analysis reveals
that custom, hardware-aware SSM kernels dominate the inference runtime,
accounting for over 55% of latency on edge platforms, identifying them as a
primary target for future hardware acceleration. We also provide detailed,
device-specific characterization results to guide system co-design for the
edge. To foster further research, we will open-source our characterization
framework.

</details>
