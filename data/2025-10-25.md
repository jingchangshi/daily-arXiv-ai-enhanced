<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs](https://arxiv.org/abs/2510.19850)
*Mostapha Kalami Heris*

Main category: cs.PL

TL;DR: 提出Prompt Decorators框架，通过声明式语法和紧凑控制令牌来标准化控制LLM的行为维度，实现可复用、可解释的提示设计。


<details>
  <summary>Details</summary>
Motivation: 传统提示工程依赖冗长的自然语言指令，限制了可重复性、模块化和可解释性，用户缺乏对LLM推理和输出表达的一致控制。

Method: 引入Prompt Decorators声明式语法，使用紧凑控制令牌（如+++Reasoning、+++Tone）来修改行为维度，定义20个核心装饰器，分为认知与生成、表达与系统两大功能族，建立统一语法、作用域模型和确定性处理流程。

Result: 演示用例显示提高了推理透明度、减少了提示复杂性，并实现了跨领域的标准化模型行为。

Conclusion: Prompt Decorators为可扩展AI系统开发声明式接口提供了基础，对互操作性、行为一致性具有重要影响。

Abstract: Large Language Models (LLMs) are central to reasoning, writing, and
decision-support workflows, yet users lack consistent control over how they
reason and express outputs. Conventional prompt engineering relies on verbose
natural-language instructions, limiting reproducibility, modularity, and
interpretability. This paper introduces Prompt Decorators, a declarative,
composable syntax that governs LLM behavior through compact control tokens such
as +++Reasoning, +++Tone(style=formal), and +++Import(topic="Systems
Thinking"). Each decorator modifies a behavioral dimension, such as reasoning
style, structure, or tone, without changing task content. The framework
formalizes twenty core decorators organized into two functional families
(Cognitive & Generative and Expressive & Systemic), each further decomposed
into subcategories that govern reasoning, interaction, expression, and
session-control. It defines a unified syntax, scoping model, and deterministic
processing pipeline enabling predictable and auditable behavior composition. By
decoupling task intent from execution behavior, Prompt Decorators create a
reusable and interpretable interface for prompt design. Illustrative use cases
demonstrate improved reasoning transparency, reduced prompt complexity, and
standardized model behavior across domains. The paper concludes with
implications for interoperability, behavioral consistency, and the development
of declarative interfaces for scalable AI systems.

</details>


### [2] [A Specification's Realm: Characterizing the Knowledge Required for Executing a Given Algorithm Specification](https://arxiv.org/abs/2510.19853)
*Assaf Marron,David Harel*

Main category: cs.PL

TL;DR: 本文提出了算法规范的"领域"概念，即执行算法所需的前提知识集合，包括语法语义、领域知识、实体关系、因果规则和操作指令等，这些知识可被系统化整理成实用文档。


<details>
  <summary>Details</summary>
Motivation: 为了让自然语言或伪代码的算法规范能够被机械执行，需要明确执行代理（人或机器）所需的前提知识，这些知识应该能够独立于具体系统实现而存在。

Method: 提出算法规范"领域"的概念，通过系统化分析过程生成包含语法语义、领域知识、实体关系、因果规则和操作指令的文档，部分过程可借助大语言模型和现有文档实现自动化。

Result: 算法规范的领域文档能够为算法在不同系统中的方法化实现和形式化验证提供支持，同时有助于评估执行忠实度（而非正确性）。

Conclusion: 算法规范的领域概念为算法执行提供了系统化的前提知识框架，有助于实现跨系统的标准化执行和验证，同时提出了执行忠实度评估这一重要问题。

Abstract: An algorithm specification in natural language or pseudocode is expected to
be clear and explicit enough to enable mechanical execution. In this position
paper we contribute an initial characterization of the knowledge that an
executing agent, human or machine, should possess in order to be able to carry
out the instructions of a given algorithm specification as a stand-alone
entity, independent of any system implementation. We argue that, for that
algorithm specification, such prerequisite knowledge, whether unique or shared
with other specifications, can be summarized in a document of practical size.
We term this document the realm of the algorithm specification. The generation
of such a realm is itself a systematic analytical process, significant parts of
which can be automated with the help of large language models and the reuse of
existing documents. The algorithm-specification's realm would consist of
specification language syntax and semantics, domain knowledge restricted to the
referenced entities, inter-entity relationships, relevant underlying
cause-and-effect rules, and detailed instructions and means for carrying out
certain operations. Such characterization of the realm can contribute to
methodological implementation of the algorithm specification in diverse systems
and to its formalization for mechanical verification. The paper also touches
upon the question of assessing execution faithfulness, which is distinct from
correctness: in the absence of a reference interpretation of natural language
or pseudocode specification with a given vocabulary, how can we determine if an
observed agent's execution indeed complies with the input specification.

</details>


### [3] [Deconstructed Proto-Quipper: A Rational Reconstruction](https://arxiv.org/abs/2510.20018)
*Ryan Kavanagh,Chuta Sano,Brigitte Pientka*

Main category: cs.PL

TL;DR: 提出了Proto-Quipper-A，这是Proto-Quipper量子编程语言家族的理性重构，使用线性λ演算和伴随逻辑基础来简化量子电路的静态生成和推理。


<details>
  <summary>Details</summary>
Motivation: Proto-Quipper语言具有复杂的操作语义，依赖于集合论操作和新鲜名称生成来操纵量子电路，这使得使用标准编程语言技术进行推理和机械化变得困难。

Method: 使用线性λ演算描述量子电路，其范式与盒线电路图紧密对应。通过伴随逻辑基础将电路语言与线性/非线性函数语言集成，重构Proto-Quipper的电路编程抽象。

Result: Proto-Quipper-A具有简单的按值调用归约语义，并且被证明是规范化的，展示了作为Proto-Quipper语言基础的易处理性。

Conclusion: 该方法避免了现有线性逻辑关系的固有复杂性，展示了如何使用标准逻辑关系证明线性和子结构系统的规范化。

Abstract: The Proto-Quipper family of programming languages aims to provide a formal
foundation for the Quipper quantum programming language. Unfortunately,
Proto-Quipper languages have complex operational semantics: they are inherently
effectful, and they rely on set-theoretic operations and fresh name generation
to manipulate quantum circuits. This makes them difficult to reason about using
standard programming language techniques and, ultimately, to mechanize. We
introduce Proto-Quipper-A, a rational reconstruction of Proto-Quipper languages
for static circuit generation. It uses a linear $\lambda$-calculus to describe
quantum circuits with normal forms that closely correspond to box-and-wire
circuit diagrams. Adjoint-logical foundations integrate this circuit language
with a linear/non-linear functional language and let us reconstruct
Proto-Quipper's circuit programming abstractions using more primitive
adjoint-logical operations. Proto-Quipper-A enjoys a simple call-by-value
reduction semantics, and to illustrate its tractability as a foundation for
Proto-Quipper languages, we show that it is normalizing. We show how to use
standard logical relations to prove normalization of linear and substructural
systems, thereby avoiding the inherent complexity of existing linear logical
relations.

</details>


### [4] [Deciding not to Decide: Sound and Complete Effect Inference in the Presence of Higher-Rank Polymorphism](https://arxiv.org/abs/2510.20532)
*Patrycja Balik,Szymon Jędras,Piotr Polesiuk*

Main category: cs.PL

TL;DR: 提出了一种用于类型和效应系统的效应推断算法，该系统包含子类型、高阶多态性和直观的集合式效应语义。通过将效应约束转换为命题逻辑公式来处理高阶多态性的作用域问题。


<details>
  <summary>Details</summary>
Motivation: 类型和效应系统尚未获得广泛应用，因为现有推断算法在表达能力、直观性和可判定性之间做出妥协。本文旨在开发一个更复杂但直观的效应推断算法。

Method: 将效应约束转换为命题逻辑公式，延迟求解以处理高阶多态性的作用域问题。算法基于具有子类型和高阶多态性的类型和效应系统。

Result: 证明了算法相对于声明式类型和效应系统的健全性和完备性。结果已在Rocq证明助手中形式化，并在实际编程语言中成功实现。

Conclusion: 提出的效应推断算法能够处理复杂的类型和效应系统，同时保持直观的语义，为类型和效应系统的更广泛应用提供了可行的解决方案。

Abstract: Type-and-effect systems help the programmer to organize data and
computational effects in a program. While for traditional type systems
expressive variants with sophisticated inference algorithms have been developed
and widely used in programming languages, type-and-effect systems did not yet
gain widespread adoption. One reason for this is that type-and-effect systems
are more complex and the existing inference algorithms make compromises between
expressiveness, intuitiveness, and decidability. In this work, we present an
effect inference algorithm for a type-and-effect system with subtyping,
expressive higher-rank polymorphism, and intuitive set-like semantics of
effects. In order to deal with scoping issues of higher-rank polymorphism, we
delay solving of effect constraints by transforming them into formulae of
propositional logic. We prove soundness and completeness of our algorithm with
respect to a declarative type-and-effect system. All the presented results have
been formalized in the Rocq proof assistant, and the algorithm has been
successfully implemented in a realistic programming language.

</details>


### [5] [Compiling the Mimosa programming language to RTOS tasks](https://arxiv.org/abs/2510.20547)
*Nikolaus Huber,Susanne Graf,Philipp Rümmer,Wang Yi*

Main category: cs.PL

TL;DR: 提出了Mimosa编程语言的编译方案，基于MIMOS计算模型，将嵌入式系统软件描述为通过FIFO队列通信的时间触发进程集合


<details>
  <summary>Details</summary>
Motivation: 为Mimosa语言开发正式的编译方案，将协调层映射到实时操作系统原语

Method: 基于Lustre编译方案的适配，针对Mimosa语义进行形式化描述

Result: 成功开发了Mimosa语言的编译方案

Conclusion: 该编译方案能够有效支持Mimosa语言在嵌入式系统中的实现

Abstract: This paper introduces a compilation scheme for programs written in the Mimosa
programming language, which builds upon the MIMOS model of computation. Mimosa
describes embedded systems software as a collection of time-triggered processes
which communicate through FIFO queues. We formally describe an adaptation of
the Lustre compilation scheme to the semantics of Mimosa and show how the
coordination layer can be mapped to real-time operating system primitives.

</details>


### [6] [SafeFFI: Efficient Sanitization at the Boundary Between Safe and Unsafe Code in Rust and Mixed-Language Applications](https://arxiv.org/abs/2510.20688)
*Oliver Braunsdorf,Tim Lange,Konrad Hohentanner,Julian Horsch,Johannes Kinder*

Main category: cs.PL

TL;DR: SafeFFI是一个优化Rust二进制文件中内存安全检测的系统，通过在unsafe和safe代码边界处进行检查，将内存安全执行从sanitizer转移到Rust类型系统，显著减少不必要的检查。


<details>
  <summary>Details</summary>
Motivation: Unsafe Rust代码在与C/C++库互操作和实现底层数据结构时是必要的，但可能导致内存安全违规。现有的sanitizer会引入许多不必要的检查，即使对于Rust类型系统保证安全的内存访问也是如此。

Method: SafeFFI系统优化内存安全检测，使得检查发生在unsafe和safe代码的边界处，将内存安全执行从sanitizer转移到Rust类型系统。该方法避免了昂贵的全程序分析，编译时开销更小。

Result: 在流行的Rust crate和已知易受攻击的Rust代码上，SafeFFI相比最先进系统实现了更优越的性能，将sanitizer检查减少了高达98%，同时保持正确性并标记所有空间和时间内存安全违规。

Conclusion: SafeFFI通过优化内存安全检测，在unsafe和safe代码边界处进行检查，显著减少了不必要的sanitizer检查，同时保持了内存安全保证，为Rust程序提供了更高效的内存安全检测方案。

Abstract: Unsafe Rust code is necessary for interoperability with C/C++ libraries and
implementing low-level data structures, but it can cause memory safety
violations in otherwise memory-safe Rust programs. Sanitizers can catch such
memory errors at runtime, but introduce many unnecessary checks even for memory
accesses guaranteed safe by the Rust type system. We introduce SafeFFI, a
system for optimizing memory safety instrumentation in Rust binaries such that
checks occur at the boundary between unsafe and safe code, handing over the
enforcement of memory safety from the sanitizer to the Rust type system. Unlike
previous approaches, our design avoids expensive whole-program analysis and
adds much less compile-time overhead (2.64x compared to over 8.83x). On a
collection of popular Rust crates and known vulnerable Rust code, SafeFFI
achieves superior performance compared to state-of-the-art systems, reducing
sanitizer checks by up to 98%, while maintaining correctness and flagging all
spatial and temporal memory safety violations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [New Hardness Results for the LOCAL Model via a Simple Self-Reduction](https://arxiv.org/abs/2510.19972)
*Alkida Balliu,Filippo Casagrande,Francesco d'Amore,Dennis Olivetti*

Main category: cs.DC

TL;DR: 本文简化了Khoury和Schild的轮消除自约简技术，并应用该技术证明了最大b匹配和边着色的随机LOCAL算法下界。


<details>
  <summary>Details</summary>
Motivation: Khoury和Schild提出的最大匹配下界证明长达25页且技术复杂，难以理解和推广。本文旨在简化该技术并扩展应用到其他图问题。

Method: 使用简化的轮消除自约简技术，构建了适用于最大b匹配和边着色问题的下界证明框架。

Result: 证明了最大b匹配需要Ω(min{log₁₊bΔ, log_Δ n})和Ω(√log₁₊b n)轮，边着色需要Ω(min{log Δ, log_Δ n})和Ω(√log n)轮的下界。

Conclusion: 简化后的轮消除自约简技术不仅重现了最大匹配下界，还成功推广到最大b匹配和边着色问题，为理解图问题复杂性提供了更简洁的工具。

Abstract: Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL
algorithm that solves maximal matching requires $\Omega(\min\{\log \Delta,
\log_\Delta n\})$ rounds, where $n$ is the number of nodes in the graph and
$\Delta$ is the maximum degree. This result is shown through a new technique,
called round elimination via self-reduction. The lower bound proof is beautiful
and presents very nice ideas. However, it spans more than 25 pages of technical
details, and hence it is hard to digest and generalize to other problems.
Historically, the simplification of proofs and techniques has marked an
important turning point in our understanding of the complexity of graph
problems. Our paper makes a step forward towards this direction, and provides
the following contributions.
  1. We present a short and simplified version of the round elimination via
self-reduction technique. The simplification of this technique enables us to
obtain the following two hardness results.
  2. We show that any randomized LOCAL algorithm that solves the maximal
$b$-matching problem requires $\Omega(\min\{\log_{1+b}\Delta, \log_\Delta n\})$
and $\Omega(\sqrt{\log_{1+b} n})$ rounds. We recall that the $b$-matching
problem is a generalization of the matching problem where each vertex can have
up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain
a short proof for the maximal matching lower bound shown by Khoury and Schild.
  3. Finally, we show that any randomized LOCAL algorithm that properly colors
the edges of a graph with $\Delta + k$ colors requires $\Omega(\min\{\log
\Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log n})$ rounds, for any $k\le
\Delta^{1-\varepsilon}$ and any constant $\varepsilon > 0$.

</details>


### [8] [AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training](https://arxiv.org/abs/2510.20111)
*Huawei Bai,Yifan Huang,Wenqi Shi,Ansheng You,Feifan Shao,Tengfei Han,Minghui Yu*

Main category: cs.DC

TL;DR: 提出AsyncHZP，一种异步分层零并行方法，通过自适应参数分片和多流异步调度，在保持内存效率的同时显著减少通信开销，在大规模训练中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在大规模集群上的训练效率和可扩展性存在瓶颈，主流ND并行方法复杂繁琐，而ZeRO等灵活方案又受通信开销限制。

Method: 采用自适应参数分片策略，在不同副本组间重新分片参数、梯度和优化器状态；设计多流异步调度方法，在后台线程执行参数收集和梯度分散操作，实现通信与计算重叠。

Result: 在密集模型和MoE模型上的实证评估表明，AsyncHZP在大规模下保持稳定，始终优于经典ND并行方法，无需复杂策略调优即可达到最先进性能。

Conclusion: AsyncHZP简化了高效大规模训练的路径，在保持简单性和内存效率的同时实现了卓越性能。

Abstract: The training efficiency and scalability of language models on massive
clusters currently remain a critical bottleneck. Mainstream approaches like ND
parallelism are often cumbersome and complex, while flexible alternatives such
as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by
communication overhead. In this paper, we propose Asynchronous Hierarchical
Zero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to
achieve superior performance while maintaining simplicity and memory
efficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding
that can lead to inefficient communication, AsyncHZP adaptively reshards
parameters, gradients, and optimizer states across different replica groups.
This strategy optimizes device memory utilization and significantly reduces
communication overhead. In addition, we also design a multi-stream asynchronous
scheduling method that executes parameter all-gather and gradient
reduce-scatter operations in dedicated background threads, effectively
overlapping communication with computation while incurring negligible memory
fragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)
models confirm that AsyncHZP maintains robust stability at scale. It
consistently outperforms classic ND parallelism, achieving state-of-the-art
performance without complex strategic tuning, thereby simplifying the path to
efficient large-scale training.

</details>


### [9] [A Full Stack Framework for High Performance Quantum-Classical Computing](https://arxiv.org/abs/2510.20128)
*Xin Zhan,K. Grace Johnson,Aniello Esposito,Barbara Chapman,Marco Fiorentino,Kirk M. Bresniker,Raymond G. Beausoleil,Masoud Mohseni*

Main category: cs.DC

TL;DR: 提出了一个HPC-QC全栈框架，通过模块化硬件/设备无关的软件集成方法，实现高性能计算与量子计算的混合工作负载开发。


<details>
  <summary>Details</summary>
Motivation: 解决可扩展的高性能计算与量子计算集成的需求，构建统一的量子-经典编程环境。

Method: 采用可扩展接口进行量子编程、调度和编译，在现有成熟HPC编程环境中实现量子内核的高层可移植调用，开发自适应电路编织虚拟机来分区大型量子电路。

Result: 在HPE EX超级计算机上成功演示了多个混合HPC-QC多节点多CPU和GPU工作负载，包括求解线性方程组、量子优化和模拟量子相变。

Conclusion: 这项工作为基于经典HPC软件栈的统一量子-经典编程环境提供了框架。

Abstract: To address the growing needs for scalable High Performance Computing (HPC)
and Quantum Computing (QC) integration, we present our HPC-QC full stack
framework and its hybrid workload development capability with modular
hardware/device-agnostic software integration approach. The latest development
in extensible interfaces for quantum programming, dispatching, and compilation
within existing mature HPC programming environment are demonstrated. Our HPC-QC
full stack enables high-level, portable invocation of quantum kernels from
commercial quantum SDKs within HPC meta-program in compiled languages (C/C++
and Fortran) as well as Python through a quantum programming interface library
extension. An adaptive circuit knitting hypervisor is being developed to
partition large quantum circuits into sub-circuits that fit on smaller noisy
quantum devices and classical simulators. At the lower-level, we leverage Cray
LLVM-based compilation framework to transform and consume LLVM IR and Quantum
IR (QIR) from commercial quantum software frontends in a retargetable fashion
to different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU
and GPU workloads (including solving linear system of equations, quantum
optimization, and simulating quantum phase transitions) have been demonstrated
on HPE EX supercomputers to illustrate functionality and execution viability
for all three components developed so far. This work provides the framework for
a unified quantum-classical programming environment built upon classical HPC
software stack (compilers, libraries, parallel runtime and process scheduling).

</details>


### [10] [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)
*Min Si,Pavan Balaji,Yongzhou Chen,Ching-Hsiang Chu,Adi Gangidi,Saif Hasan,Subodh Iyengar,Dan Johnson,Bingzhe Liu,Jingliang Ren,Ashmitha Jeevaraj Shetty,Greg Steinbrecher,Xinfeng Xie,Yulun Wang,Bruce Wu,Jingyi Yang,Mingran Yang,Minlan Yu,Cen Zhao,Wes Bland,Denis Boyda,Suman Gumudavelli,Cristian Lumezanu,Rui Miao,Zhe Qu,Venkat Ramesh,Maxim Samoylov,Jan Seidel,Feng Tian,Qiye Tan,Shuqiang Zhang,Yimeng Zhao,Shengbao Zheng,Art Zhu,Hongyi Zeng*

Main category: cs.DC

TL;DR: NCCLX是Meta开发的集体通信框架，旨在优化大规模语言模型在超10万GPU集群上的通信性能，显著提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，传统通信方法在数万GPU规模下面临吞吐量和延迟限制，阻碍了最先进模型的开发和部署。

Method: 开发NCCLX集体通信框架，专门针对超10万GPU集群的复杂工作负载进行优化，支持从大规模训练到低延迟推理的完整LLM生命周期。

Result: 在Llama4模型上的实证评估显示通信效率显著提升，实现了可靠、高吞吐量和低延迟的数据交换。

Conclusion: 该研究为下一代LLM在空前规模上运行提供了强大的通信解决方案。

Abstract: The increasing scale of large language models (LLMs) necessitates highly
efficient collective communication frameworks, particularly as training
workloads extend to hundreds of thousands of GPUs. Traditional communication
methods face significant throughput and latency limitations at this scale,
hindering both the development and deployment of state-of-the-art models. This
paper presents the NCCLX collective communication framework, developed at Meta,
engineered to optimize performance across the full LLM lifecycle, from the
synchronous demands of large-scale training to the low-latency requirements of
inference. The framework is designed to support complex workloads on clusters
exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency
data exchange. Empirical evaluation on the Llama4 model demonstrates
substantial improvements in communication efficiency. This research contributes
a robust solution for enabling the next generation of LLMs to operate at
unprecedented scales.

</details>


### [11] [FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services](https://arxiv.org/abs/2510.20388)
*Víctor Rampérez,Javier Soriano,David Lizcano,Juan A. Lara*

Main category: cs.DC

TL;DR: FLAS是一个结合了主动和被动方法的自动扩缩容系统，通过预测高维指标趋势和基于资源使用指标的被动应急系统，为分布式服务提供优化的扩缩容决策。


<details>
  <summary>Details</summary>
Motivation: 云计算弹性特性需要自动扩缩容系统来确保服务水平协议。现有方法需要改进以更好地预测SLA参数变化并减少必要的检测开销。

Method: FLAS结合主动预测和被动应急方法：使用预测模型预测高维指标趋势，以及基于资源使用指标估计高维指标的被动系统，减少检测需求。

Result: 在内容发布-订阅中间件上的评估显示，FLAS在99%以上的时间内确保性能要求得到满足，包括最坏场景测试。

Conclusion: FLAS是首个针对基于内容的发布-订阅分布式系统的自动扩缩容解决方案，具有通用性且能有效确保性能SLA。

Abstract: Cloud computing has established itself as the support for the vast majority
of emerging technologies, mainly due to the characteristic of elasticity it
offers. Auto-scalers are the systems that enable this elasticity by acquiring
and releasing resources on demand to ensure an agreed service level. In this
article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for
distributed services that combines the advantages of proactive and reactive
approaches according to the situation to decide the optimal scaling actions in
every moment. The main novelties introduced by FLAS are (i) a predictive model
of the high-level metrics trend which allows to anticipate changes in the
relevant SLA parameters (e.g. performance metrics such as response time or
throughput) and (ii) a reactive contingency system based on the estimation of
high-level metrics from resource use metrics, reducing the necessary
instrumentation (less invasive) and allowing it to be adapted agnostically to
different applications. We provide a FLAS implementation for the use case of a
content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone
of an event-driven architecture. To the best of our knowledge, this is the
first auto-scaling system for content-based publish-subscribe distributed
systems (although it is generic enough to fit any distributed service). Through
an evaluation based on several test cases recreating not only the expected
contexts of use, but also the worst possible scenarios (following the
Boundary-Value Analysis or BVA test methodology), we have validated our
approach and demonstrated the effectiveness of our solution by ensuring
compliance with performance requirements over 99% of the time.

</details>


### [12] [Accurate Performance Predictors for Edge Computing Applications](https://arxiv.org/abs/2510.20495)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 提出一种在动态边缘环境中自动构建和评估性能预测器的方法，通过联合优化准确性和推理时间来实现可预测的应用性能。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的动态边缘环境中，由于多个应用共存和节点异构性，实现可预测的性能具有挑战性，这对有效的调度和资源管理至关重要。

Method: 自动构建和评估各种性能预测器，优先考虑准确性和推理时间，选择最有效模型。预测器基于与应用程序性能最相关的监控指标历史状态进行训练，并在动态共置场景中评估。

Result: 预测器达到高达90%的准确率，同时保持推理时间小于往返时间的1%。在电子显微镜工作流的用例中验证了方法的有效性。

Conclusion: 需要系统化方法在动态共置场景中通过联合优化准确性和推理延迟来选择服务器特定的预测器，集成此类预测器可以改善资源利用率和实现可预测性能。

Abstract: Accurate prediction of application performance is critical for enabling
effective scheduling and resource management in resource-constrained dynamic
edge environments. However, achieving predictable performance in such
environments remains challenging due to the co-location of multiple
applications and the node heterogeneity. To address this, we propose a
methodology that automatically builds and assesses various performance
predictors. This approach prioritizes both accuracy and inference time to
identify the most efficient model. Our predictors achieve up to 90% accuracy
while maintaining an inference time of less than 1% of the Round Trip Time.
These predictors are trained on the historical state of the most correlated
monitoring metrics to application performance and evaluated across multiple
servers in dynamic co-location scenarios. As usecase we consider electron
microscopy (EM) workflows, which have stringent real-time demands and diverse
resource requirements. Our findings emphasize the need for a systematic
methodology that selects server-specific predictors by jointly optimizing
accuracy and inference latency in dynamic co-location scenarios. Integrating
such predictors into edge environments can improve resource utilization and
result in predictable performance.

</details>


### [13] [Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing](https://arxiv.org/abs/2510.20506)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 本文提出使用RTT预测器来改进负载均衡，通过预测应用延迟来优化请求路由决策，在Kubernetes管理的GPU集群中实现了高达95%的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 分布式应用对低端到端延迟的需求日益增长，传统负载均衡策略通常基于过时或粗粒度的指标，导致次优的路由决策和尾部延迟增加。

Method: 开发轻量级且准确的RTT预测器，基于从Kubernetes管理的GPU集群收集的时间序列监控数据进行训练，利用高度相关的监控指标子集来保持低开销。

Result: 预测器达到高达95%的准确率，预测延迟保持在应用RTT的10%以内。仿真评估显示性能感知负载均衡能显著降低应用RTT并最小化资源浪费。

Conclusion: 研究结果突显了将预测性负载均衡集成到未来生产系统中的可行性，并确定了确保在资源受限集群中有效部署预测器所需的最小预测准确率阈值和关键系统级因素。

Abstract: Distributed applications increasingly demand low end-to-end latency,
especially in edge and cloud environments where co-located workloads contend
for limited resources. Traditional load-balancing strategies are typically
reactive and rely on outdated or coarse-grained metrics, often leading to
suboptimal routing decisions and increased tail latencies. This paper
investigates the use of round-trip time (RTT) predictors to enhance request
routing by anticipating application latency. We develop lightweight and
accurate RTT predictors that are trained on time-series monitoring data
collected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of
highly correlated monitoring metrics, our approach maintains low overhead while
remaining adaptable to diverse co-location scenarios and heterogeneous
hardware. The predictors achieve up to 95% accuracy while keeping the
prediction delay within 10% of the application RTT. In addition, we identify
the minimum prediction accuracy threshold and key system-level factors required
to ensure effective predictor deployment in resource-constrained clusters.
Simulation-based evaluation demonstrates that performance-aware load balancing
can significantly reduce application RTT and minimize resource waste. These
results highlight the feasibility of integrating predictive load balancing into
future production systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [HALOC-AxA: An Area/-Energy-Efficient Approximate Adder for Image Processing Application](https://arxiv.org/abs/2510.20137)
*Hasnain A. Ziad,Ashiq A. Sakib*

Main category: cs.AR

TL;DR: 提出了一种新型近似加法器，在保持或提高计算精度的同时，比现有加法器具有更好的能量和面积效率，适用于图像处理等计算密集型多媒体应用。


<details>
  <summary>Details</summary>
Motivation: 为计算密集型多媒体应用（如图像、音频、视频处理）设计能量高效的硬件，需要平衡高性能、计算精度和能量效率之间的冲突需求。

Method: 设计了一种新型近似加法器，通过模拟结果验证其性能，并将其部署到图像处理任务中展示数字重建高质量图像的能力。

Result: 模拟结果表明，该加法器比现有加法器更节能且面积效率更高，同时实现改进或相当的计算精度。

Conclusion: 所提出的近似加法器在能量效率、面积效率和计算精度方面表现出色，特别适用于多媒体处理应用。

Abstract: The design of approximate adders has been widely researched to advance
energy-efficient hardware for computation-intensive multimedia applications,
such as image, audio, or video processing. The design of approximate adders has
been widely researched to advance energy-efficient hardware for computation
intensive multimedia applications, such as image/audio/video processing.
Several static and dynamic approximate adders exist in the literature, each of
which endeavors to balance the conflicting demands of high performance,
computational accuracy, and energy efficiency. This work introduces a novel
approximate adder that is more energy- and area-efficient than existing adders,
while achieving improved or comparable accuracy, as demonstrated by simulation
results. The proposed adder's ability to digitally reconstruct high quality
images is further demonstrated by the deployment of the design for an image
processing task.

</details>


### [15] [In-DRAM True Random Number Generation Using Simultaneous Multiple-Row Activation: An Experimental Study of Real DRAM Chips](https://arxiv.org/abs/2510.20269)
*Ismail Emir Yuksel,Ataberk Olgun,F. Nisa Bostanci,Oguzhan Canpolat,Geraldo F. Oliveira,Mohammad Sadrosadati,Abdullah Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: 本文通过实验证明，在商用DRAM芯片中利用同时多行激活(SiMRA)技术可以生成高质量的真随机数，在吞吐量和延迟方面优于现有DRAM-based TRNG方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种在商用DRAM芯片中实现高吞吐量、低延迟的真随机数生成器(TRNG)，利用DRAM的物理特性来产生高质量的随机性。

Method: 使用96个DDR4 DRAM芯片进行广泛表征，通过同时激活多个DRAM行(2、4、8、16、32行)来生成随机数，分析不同数据模式、温度水平和空间变化对熵的影响。

Result: 所有SiMRA-based TRNG设计都通过了NIST随机性测试；与现有DRAM-based TRNG相比，2、8、16、32行激活的吞吐量分别提高了1.15x、1.99x、1.82x和1.39x；熵随激活行数增加而增加；操作参数显著影响熵。

Conclusion: SiMRA技术为在商用DRAM芯片中实现高效真随机数生成提供了可行方案，其性能优于现有方法，且开源了相关基础设施以促进未来研究。

Abstract: In this work, we experimentally demonstrate that it is possible to generate
true random numbers at high throughput and low latency in commercial
off-the-shelf (COTS) DRAM chips by leveraging simultaneous multiple-row
activation (SiMRA) via an extensive characterization of 96 DDR4 DRAM chips. We
rigorously analyze SiMRA's true random generation potential in terms of
entropy, latency, and throughput for varying numbers of simultaneously
activated DRAM rows (i.e., 2, 4, 8, 16, and 32), data patterns, temperature
levels, and spatial variations. Among our 11 key experimental observations, we
highlight four key results. First, we evaluate the quality of our TRNG designs
using the commonly-used NIST statistical test suite for randomness and find
that all SiMRA-based TRNG designs successfully pass each test. Second, 2-, 8-,
16-, and 32-row activation-based TRNG designs outperform the state-of-theart
DRAM-based TRNG in throughput by up to 1.15x, 1.99x, 1.82x, and 1.39x,
respectively. Third, SiMRA's entropy tends to increase with the number of
simultaneously activated DRAM rows. Fourth, operational parameters and
conditions (e.g., data pattern and temperature) significantly affect entropy.
For example, for most of the tested modules, the average entropy of 32-row
activation is 2.51x higher than that of 2-row activation. For example,
increasing the temperature from 50{\deg}C to 90{\deg}C decreases SiMRA's
entropy by 1.53x for 32-row activation. To aid future research and development,
we open-source our infrastructure at https://github.com/CMU-SAFARI/SiMRA-TRNG.

</details>


### [16] [Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism on Dependency-Bound Kernels](https://arxiv.org/abs/2510.20400)
*Rubén Langarita,Jesús Alastruey-Benedé,Pablo Ibáñez-Marín,Santiago Marco-Sola,Miquel Moretó,Adrià Armejach*

Main category: cs.AR

TL;DR: Squire是一种通用加速器，旨在有效利用依赖绑定内核中的细粒度并行性，通过低功耗核心直接通信和L2缓存访问，在动态编程内核中实现最高7.64倍加速，端到端应用加速3.66倍，能耗降低56%，面积开销仅10.5%。


<details>
  <summary>Details</summary>
Motivation: 传统通用加速器（如SIMD、GPGPU）在处理复杂数据依赖模式时存在局限性，而定制FPGA/ASIC设计成本高且缺乏灵活性。需要一种能有效处理依赖绑定内核的通用加速器解决方案。

Method: 每个Squire加速器包含一组低功耗顺序核心，这些核心能快速相互通信并直接访问L2缓存。在典型多核系统中为每个核心集成一个Squire加速器，通过最小软件修改加速并行任务中的依赖绑定内核。

Result: 在实现复杂依赖模式的五个内核中，Squire在动态编程内核中获得最高7.64倍加速，端到端应用加速3.66倍。能耗降低高达56%，与Neoverse-N1基线相比面积开销仅10.5%。

Conclusion: Squire提供了一种高效处理依赖绑定内核的通用加速器解决方案，在保持低面积开销的同时显著提升性能和能效，为HPC应用中的计算密集型内核提供了实用加速方案。

Abstract: Multiple HPC applications are often bottlenecked by compute-intensive kernels
implementing complex dependency patterns (data-dependency bound). Traditional
general-purpose accelerators struggle to effectively exploit fine-grain
parallelism due to limitations in implementing convoluted data-dependency
patterns (like SIMD) and overheads due to synchronization and data transfers
(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved
performance and energy efficiency at a high cost in hardware design and
programming complexity and often lack the flexibility to process different
workloads. We propose Squire, a general-purpose accelerator designed to exploit
fine-grain parallelism effectively on dependency-bound kernels. Each Squire
accelerator has a set of general-purpose low-power in-order cores that can
rapidly communicate among themselves and directly access data from the L2
cache. Our proposal integrates one Squire accelerator per core in a typical
multicore system, allowing the acceleration of dependency-bound kernels within
parallel tasks with minimal software changes. As a case study, we evaluate
Squire's effectiveness by accelerating five kernels that implement complex
dependency patterns. We use three of these kernels to build an end-to-end
read-mapping tool that will be used to evaluate Squire. Squire obtains speedups
up to 7.64$\times$ in dynamic programming kernels. Overall, Squire provides an
acceleration for an end-to-end application of 3.66$\times$. In addition, Squire
reduces energy consumption by up to 56% with a minimal area overhead of 10.5%
compared to a Neoverse-N1 baseline.

</details>
