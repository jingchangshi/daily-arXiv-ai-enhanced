<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Optimal Software Pipelining using an SMT-Solver](https://arxiv.org/abs/2601.21842)
*Jan-Willem Roorda*

Main category: cs.PL

TL;DR: 基于SMT求解器的最优软件流水线方法，显著超越启发式算法和手工优化


<details>
  <summary>Details</summary>
Motivation: 软件流水线是VLIW处理器的重要循环优化技术，传统基于启发式的方法可能无法找到最优解。需要一种能保证最优性的方法，同时为程序员和处理器设计者提供反馈。

Method: 使用可满足性模理论（SMT）求解器构建最优软件流水线器，将软件流水线调度问题转化为SMT约束求解问题。

Result: 该方法显著优于启发式算法和手工优化，并能提供关于特定启动间隔不可行的反馈信息。

Conclusion: 基于SMT求解器的最优软件流水线方法是有效的，不仅能找到最优调度方案，还能为系统优化提供有价值的反馈。

Abstract: Software Pipelining is a classic and important loop-optimization for VLIW processors. It improves instruction-level parallelism by overlapping multiple iterations of a loop and executing them in parallel. Typically, it is implemented using heuristics. In this paper, we present an optimal software pipeliner based on a Satisfiability Modulo Theories (SMT) Solver. We show that our approach significantly outperforms heuristic algorithms and hand-optimization. Furthermore, we show how the solver can be used to give feedback to programmers and processor designers on why a software pipelined schedule of a certain initiation interval is not feasible.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Deep Reinforcement Learning for Fault-Adaptive Routing in Eisenstein-Jacobi Interconnection Topologies](https://arxiv.org/abs/2601.21090)
*Mohammad Walid Charrwi,Zaid Hussain*

Main category: cs.DC

TL;DR: 该论文评估了三种路由算法在故障EJ网络中的表现：贪婪路由在故障下性能严重下降，Dijkstra算法提供理论最优但需要全局拓扑知识，强化学习算法实现了接近最优的性能且适合分布式部署。


<details>
  <summary>Details</summary>
Motivation: 随着多核架构密度增加，需要高性能且容错的互连网络。EJ网络具有优越的拓扑特性，但在故障条件下对传统路由启发式算法构成挑战，需要评估不同路由范式在故障环境中的表现。

Method: 评估三种路由范式：确定性贪婪自适应路由、理论最优的Dijkstra算法、以及基于强化学习的方法。使用多目标奖励函数惩罚故障接近度并奖励路径效率，RL智能体学习在故障集群周围导航。

Result: 在9个故障节点下，贪婪路由有效可达性和数据包投递率降至10%，Dijkstra算法证明52-54%是拓扑最优值，而RL智能体实现了94%有效可达性和91%数据包投递率。吞吐量评估显示RL在所有负载下维持超过90%的归一化吞吐量。

Conclusion: 基于RL的自适应策略是实用解决方案，填补了贪婪路由效率和Dijkstra算法最优性之间的差距，在故障易发的互连网络中提供鲁棒、自愈的通信，无需全局拓扑知识或最优算法的计算开销。

Abstract: The increasing density of many-core architectures necessitates interconnection networks that are both high-performance and fault-resilient. Eisenstein-Jacobi (EJ) networks, with their symmetric 6-regular topology, offer superior topological properties but challenge traditional routing heuristics under fault conditions. This paper evaluates three routing paradigms in faulty EJ environments: deterministic Greedy Adaptive Routing, theoretically optimal Dijkstra's algorithm, and a reinforcement learning (RL)-based approach. Using a multi-objective reward function to penalize fault proximity and reward path efficiency, the RL agent learns to navigate around clustered failures that typically induce dead-ends in greedy geometric routing. Dijkstra's algorithm establishes the theoretical performance ceiling by computing globally optimal paths with complete topology knowledge, revealing the true connectivity limits of faulty networks. Quantitative analysis at nine faulty nodes shows greedy routing catastrophically degrades to 10% effective reachability and packet delivery, while Dijkstra proves 52-54% represents the topological optimum. The RL agent achieves 94% effective reachability and 91% packet delivery, making it suitable for distributed deployment. Furthermore, throughput evaluations demonstrate that RL sustains over 90% normalized throughput across all loads, actually outperforming Dijkstra under congestion through implicit load balancing strategies. These results establish RL-based adaptive policies as a practical solution that bridges the gap between greedy's efficiency and Dijkstra's optimality, providing robust, self-healing communication in fault-prone interconnection networks without requiring the global topology knowledge or computational overhead of optimal algorithms.

</details>


### [3] [Maxwait: A Generalized Mechanism for Distributed Time-Sensitive Systems](https://arxiv.org/abs/2601.21146)
*Francesco Paladino,Shulu Li,Edward A. Lee*

Main category: cs.DC

TL;DR: maxwait是一种简单但通用的协调机制，可在分布式时间敏感系统中显式配置时序要求与一致性之间的权衡，涵盖多种经典分布式系统方法并支持实时行为。


<details>
  <summary>Details</summary>
Motivation: 分布式时间敏感系统需要在通信延迟和同步不确定性的情况下，平衡时序要求（可用性）与一致性。现有方法往往难以同时满足这些需求。

Method: 提出maxwait协调机制，作为Lingua Franca协调语言的扩展实现。该机制在通信延迟有界时强制执行逻辑时间一致性，在边界被违反时提供结构化故障处理。

Result: maxwait机制能够涵盖PTIDES、Chandy-and-Misra（含/不含空消息）、Jefferson's Time-Warp、Lamport时间故障检测等经典方法，同时支持LET、发布订阅、actor、CRDT、RPC with futures等常见分布式模式，并提供更好的时序控制、有界时间故障检测和确定性增强。

Conclusion: maxwait提供了一种统一的语义框架，使分布式时间敏感系统能够显式配置时序与一致性的权衡，在单一框架内实现多种经典方法和模式，同时增强时序控制和确定性。

Abstract: Distributed time-sensitive systems must balance timing requirements (availability) and consistency in the presence of communication delays and synchronization uncertainty. This paper presents maxwait, a simple coordination mechanism with surprising generality that makes these tradeoffs explicit and configurable. We demonstrate that this mechanism subsumes classical distributed system methods such as PTIDES, Chandy-and-Misra with or without null messages, Jefferson's Time-Warp, and Lamport's time-based fault detection, while enabling real-time behavior in distributed cyber-physical applications. The mechanism can also realize many commonly used distributed system patterns, including logical execution time (LET), publish and subscribe, actors, conflict-free replicated data types (CRDTs), and remote procedure calls with futures. More importantly, it adds to these mechanisms better control over timing, bounded time fault detection, and the option of making them more deterministic, all within a single semantic framework. Implemented as an extension of the Lingua Franca coordination language, maxwait enforces logical-time consistency when communication latencies are bounded and provides structured fault handling when bounds are violated.

</details>


### [4] [ZipMoE: Efficient On-Device MoE Serving via Lossless Compression and Cache-Affinity Scheduling](https://arxiv.org/abs/2601.21198)
*Yuchen Yang,Yaru Zhao,Pu Yang,Shaowei Wang,Zhi-Hua Zhou*

Main category: cs.DC

TL;DR: ZipMoE：一种高效且语义无损的端侧MoE服务系统，通过缓存调度协同设计，将端侧MoE推理从I/O瓶颈转变为计算中心工作流，显著降低延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然增强了大型语言模型的表达能力，但其巨大的内存占用严重阻碍了在资源受限的边缘设备上的实际部署，尤其是在需要保持模型行为而不依赖有损量化的情况下。

Method: ZipMoE利用边缘设备硬件特性与MoE参数固有统计冗余之间的协同作用，通过具有可证明性能保证的缓存调度协同设计，将端侧MoE推理范式从I/O瓶颈转变为计算中心工作流，实现高效并行化。

Result: 在代表性边缘计算平台上使用开源MoE模型和真实工作负载进行实验，ZipMoE相比最先进系统实现了高达72.77%的推理延迟降低和高达6.76倍的吞吐量提升。

Conclusion: ZipMoE通过创新的缓存调度协同设计，有效解决了MoE模型在边缘设备部署时的内存瓶颈问题，实现了高效且语义无损的端侧推理服务。

Abstract: While Mixture-of-Experts (MoE) architectures substantially bolster the expressive power of large-language models, their prohibitive memory footprint severely impedes the practical deployment on resource-constrained edge devices, especially when model behavior must be preserved without relying on lossy quantization. In this paper, we present ZipMoE, an efficient and semantically lossless on-device MoE serving system. ZipMoE exploits the synergy between the hardware properties of edge devices and the statistical redundancy inherent to MoE parameters via a caching-scheduling co-design with provable performance guarantee. Fundamentally, our design shifts the paradigm of on-device MoE inference from an I/O-bound bottleneck to a compute-centric workflow that enables efficient parallelization. We implement a prototype of ZipMoE and conduct extensive experiments on representative edge computing platforms using popular open-source MoE models and real-world workloads. Our evaluation reveals that ZipMoE achieves up to $72.77\%$ inference latency reduction and up to $6.76\times$ higher throughput than the state-of-the-art systems.

</details>


### [5] [Ira: Efficient Transaction Replay for Distributed Systems](https://arxiv.org/abs/2601.21286)
*Adithya Bhat,Harshal Bhadreshkumar Shah,Mohsen Minaei*

Main category: cs.DC

TL;DR: Ira框架通过传输紧凑的提示信息加速主备复制中的备份重放，在以太坊案例中实现25倍加速


<details>
  <summary>Details</summary>
Motivation: 在主备复制中，共识延迟受限于备份节点重放主节点提议交易的时间。主节点已经执行过交易，拥有未来访问模式的知识，这正是优化重放所需的信息。

Method: 提出Ira框架，通过传输紧凑的提示信息加速备份重放。具体实现Ira-L协议用于以太坊：主节点提供包含以太坊区块中使用的键工作集和每个键一字节元数据的提示，备份节点利用这些提示进行高效的区块重放。

Result: 提示信息紧凑，每个区块中位数增加47KB压缩数据（约5%的区块负载）。主节点开销为28.6%的墙钟时间（提示直接成本为10.9%执行时间）。备份端实现中位数每区块25倍加速，16个预取线程下总重放时间从6.5小时降至16分钟（23.6倍墙钟加速）。

Conclusion: Ira框架通过利用主节点的执行知识生成紧凑提示，显著加速备份重放，在以太坊应用中实现显著性能提升，所有开销都可以在生产部署中流水线化和并行化。

Abstract: In primary-backup replication, consensus latency is bounded by the time for backup nodes to replay (re-execute) transactions proposed by the primary. In this work, we present Ira, a framework to accelerate backup replay by transmitting compact \emph{hints} alongside transaction batches. Our key insight is that the primary, having already executed transactions, possesses knowledge of future access patterns which is exactly the information needed for optimal replay.
  We use Ethereum for our case study and present a concrete protocol, Ira-L, within our framework to improve cache management of Ethereum block execution. The primaries implementing Ira-L provide hints that consist of the working set of keys used in an Ethereum block and one byte of metadata per key indicating the table to read from, and backups use these hints for efficient block replay.
  We evaluated Ira-L against the state-of-the-art Ethereum client reth over two weeks of Ethereum mainnet activity ($100,800$ blocks containing over $24$ million transactions). Our hints are compact, adding a median of $47$ KB compressed per block ($\sim5\%$ of block payload). We observe that the sequential hint generation and block execution imposes a $28.6\%$ wall-time overhead on the primary, though the direct cost from hints is $10.9\%$ of execution time; all of which can be pipelined and parallelized in production deployments. On the backup side, we observe that Ira-L achieves a median per-block speedup of $25\times$ over baseline reth. With $16$ prefetch threads, aggregate replay time drops from $6.5$ hours to $16$ minutes ($23.6\times$ wall-time speedup).

</details>


### [6] [EWSJF: An Adaptive Scheduler with Hybrid Partitioning for Mixed-Workload LLM Inference](https://arxiv.org/abs/2601.21758)
*Bronislav Sidik,Chaya Levi,Joseph Kampeas*

Main category: cs.DC

TL;DR: EWSJF是一个自适应请求级调度器，通过学习工作负载结构实时优化LLM服务中的公平性和吞吐量，相比FCFS提升吞吐量30%以上，短请求平均TTFT降低4倍。


<details>
  <summary>Details</summary>
Motivation: LLM服务中混合工作负载（短延迟敏感查询+长吞吐量导向批处理）的调度挑战：标准FCFS策略存在严重的队头阻塞问题，导致高尾延迟和硬件利用率低下。

Method: EWSJF包含四个组件：1) Refine-and-Prune无监督分区算法发现性能同质请求组；2) Dynamic Queue Routing将请求分配到这些组；3) Density-Weighted Scoring上下文感知优先级函数平衡紧急性和公平性；4) Bayesian Meta-Optimization基于实时性能反馈持续调优评分和分区参数。

Result: 在vLLM中实现，相比FCFS：端到端吞吐量提升超过30%，短请求平均TTFT降低高达4倍。

Conclusion: 自适应、基于学习的请求调度是高效响应LLM服务的关键缺失层，EWSJF通过实时学习工作负载结构显著改善了公平性和吞吐量。

Abstract: Serving Large Language Models (LLMs) under mixed workloads--short, latency-sensitive interactive queries alongside long, throughput-oriented batch requests--poses a fundamental scheduling challenge. Standard First-Come, First-Served (FCFS) policies suffer from severe head-of-line blocking, leading to high tail latency and underutilized hardware. We introduce EWSJF (Effective Workload-based Shortest Job First), an adaptive request-level scheduler that learns workload structure in real time to jointly improve fairness and throughput. EWSJF operates upstream of execution-level schedulers and integrates four components: (1) Refine-and-Prune, an unsupervised partitioning algorithm that discovers performance-homogeneous request groups; (2) Dynamic Queue Routing for assigning requests to these groups; (3) Density-Weighted Scoring, a context-aware prioritization function balancing urgency and fairness; and (4) Bayesian Meta-Optimization, which continuously tunes scoring and partitioning parameters based on live performance feedback. Implemented in vLLM, EWSJF improves end-to-end throughput by over 30% and reduces average Time-To-First-Token for short requests by up to 4x compared to FCFS. These results demonstrate that adaptive, learning-based request scheduling is a critical missing layer for efficient and responsive LLM serving. Implementation available at https://anonymous.4open.science/r/vllm_0110-32D8.

</details>


### [7] [Self-Adaptive Probabilistic Skyline Query Processing in Distributed Edge Computing via Deep Reinforcement Learning](https://arxiv.org/abs/2601.21855)
*Chuan-Chi Lai*

Main category: cs.DC

TL;DR: SA-PSKY：一种用于边缘-云协作系统的自适应概率天际线查询框架，使用DDPG智能体动态调整过滤阈值，显著降低通信开销和响应时间。


<details>
  <summary>Details</summary>
Motivation: 在万物互联时代，边缘传感器数据爆炸式增长使得概率天际线查询处理面临挑战。传统分布式方法依赖静态阈值，无法适应边缘计算环境的高度动态和异构特性，导致通信瓶颈或计算延迟。

Method: 提出SA-PSKY自适应框架，将动态阈值调整问题形式化为连续马尔可夫决策过程，利用深度确定性策略梯度智能体实时优化过滤强度。智能分析多维系统状态，包括数据到达率、不确定性分布和瞬时资源可用性。

Result: 实验评估表明，SA-PSKY持续优于最先进的静态和启发式基线方法，通信开销降低高达60%，总响应时间减少40%，并在不同数据分布下保持鲁棒可扩展性。

Conclusion: SA-PSKY通过自适应的动态阈值调整机制，有效解决了边缘计算环境中概率天际线查询的资源冲突问题，实现了通信和计算成本的最优化平衡。

Abstract: In the era of the Internet of Everything (IoE), the exponential growth of sensor-generated data at the network edge renders efficient Probabilistic Skyline Query (PSKY) processing a critical challenge. Traditional distributed PSKY methodologies predominantly rely on pre-defined static thresholds to filter local candidates. However, these rigid approaches are fundamentally ill-suited for the highly volatile and heterogeneous nature of edge computing environments, often leading to either severe communication bottlenecks or excessive local computational latency. To resolve this resource conflict, this paper presents SA-PSKY, a novel Self-Adaptive framework designed for distributed edge-cloud collaborative systems. We formalize the dynamic threshold adjustment problem as a continuous Markov Decision Process (MDP) and leverage a Deep Deterministic Policy Gradient (DDPG) agent to autonomously optimize filtering intensities in real-time. By intelligently analyzing multi-dimensional system states, including data arrival rates, uncertainty distributions, and instantaneous resource availability, our framework effectively minimizes a joint objective function of computation and communication costs. Comprehensive experimental evaluations demonstrate that SA-PSKY consistently outperforms state-of-the-art static and heuristic baselines. Specifically, it achieves a reduction of up to 60\% in communication overhead and 40\% in total response time, while ensuring robust scalability across diverse data distributions.

</details>


### [8] [Belief Propagation Converges to Gaussian Distributions in Sparsely-Connected Factor Graphs](https://arxiv.org/abs/2601.21935)
*Tom Yates,Yuzhou Cheng,Ignacio Alzugaray,Danyal Akarca,Pedro A. M. Mediano,Andrew J. Davison*

Main category: cs.DC

TL;DR: 论文证明了在满足四个关键假设的稀疏连接因子图中，即使是非高斯问题，BP算法的变量信念也会收敛到高斯分布，并通过立体深度估计实验验证了该理论。


<details>
  <summary>Details</summary>
Motivation: 虽然高斯信念传播(GBP)在计算机视觉和传感器网络等应用中表现出色，即使处理非高斯问题时也常被使用，但缺乏理论依据说明何时高斯近似在高度非高斯、稀疏连接的因子图中是有效的。

Method: 利用中心极限定理(CLT)从数学上证明，在满足四个关键假设的复杂环状因子图中，BP算法的变量信念会收敛到高斯分布。并通过立体深度估计任务进行实验验证。

Result: 理论证明表明在满足假设的条件下，变量信念会收敛到高斯分布。实验结果显示，在立体深度估计任务中，仅经过几次BP迭代后，变量信念就变得越来越高斯。

Conclusion: 该研究为GBP在高度非高斯、稀疏连接的因子图中的有效性提供了理论保证，解释了为什么GBP在实践中即使处理非高斯问题也能表现良好。

Abstract: Belief Propagation (BP) is a powerful algorithm for distributed inference in probabilistic graphical models, however it quickly becomes infeasible for practical compute and memory budgets. Many efficient, non-parametric forms of BP have been developed, but the most popular is Gaussian Belief Propagation (GBP), a variant that assumes all distributions are locally Gaussian. GBP is widely used due to its efficiency and empirically strong performance in applications like computer vision or sensor networks - even when modelling non-Gaussian problems. In this paper, we seek to provide a theoretical guarantee for when Gaussian approximations are valid in highly non-Gaussian, sparsely-connected factor graphs performing BP (common in spatial AI). We leverage the Central Limit Theorem (CLT) to prove mathematically that variables' beliefs under BP converge to a Gaussian distribution in complex, loopy factor graphs obeying our 4 key assumptions. We then confirm experimentally that variable beliefs become increasingly Gaussian after just a few BP iterations in a stereo depth estimation task.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [FireFly-P: FPGA-Accelerated Spiking Neural Network Plasticity for Robust Adaptive Control](https://arxiv.org/abs/2601.21222)
*Tenglong Li,Jindong Li,Guobin Shen,Dongcheng Zhao,Qian Zhang,Yi Zeng*

Main category: cs.AR

TL;DR: FireFly-P：基于FPGA的SNN硬件加速器，实现实时自适应控制，仅需8μs延迟和0.713W功耗


<details>
  <summary>Details</summary>
Motivation: 利用SNN的生物可塑性机制实现无监督自适应控制，避免反向传播的计算开销，为机器人提供实时适应动态非结构化环境的能力

Method: 设计FPGA硬件加速器FireFly-P，实现新型可塑性算法，在Cmod A7-35T FPGA上实现片上可塑性更新，优化网络泛化能力

Result: 端到端延迟仅8μs（推理和可塑性更新），功耗0.713W，占用约10K LUTs，在资源受限的嵌入式机器人平台上实现快速适应

Conclusion: 硬件加速的SNN可塑性是实现自适应、低延迟、高能效控制系统的可行路径，特别适合嵌入式机器人应用

Abstract: Spiking Neural Networks (SNNs) offer a biologically plausible learning mechanism through synaptic plasticity, enabling unsupervised adaptation without the computational overhead of backpropagation. To harness this capability for robotics, this paper presents FireFly-P, an FPGA-based hardware accelerator that implements a novel plasticity algorithm for real-time adaptive control. By leveraging on-chip plasticity, our architecture enhances the network's generalization, ensuring robust performance in dynamic and unstructured environments. The hardware design achieves an end-to-end latency of just 8~$μ$s for both inference and plasticity updates, enabling rapid adaptation to unseen scenarios. Implemented on a tiny Cmod A7-35T FPGA, FireFly-P consumes only 0.713~W and $\sim$10K~LUTs, making it ideal for power- and resource-constrained embedded robotic platforms. This work demonstrates that hardware-accelerated SNN plasticity is a viable path toward enabling adaptive, low-latency, and energy-efficient control systems.

</details>


### [10] [Frequency as Aperture: Enabling Embeddable Near-Field Sensing for 6G Wireless Radios](https://arxiv.org/abs/2601.21584)
*Pin-Han Ho,Limei Peng,Yiming Miao,Xu Fan,Kairan Liang,Haoran Mei,Wei Duan*

Main category: cs.AR

TL;DR: FaA利用频率敏捷性实现虚拟传感孔径，将空间采样从天线域转移到频域，用单射频链实现近场感知，显著提升架构效率


<details>
  <summary>Details</summary>
Motivation: 现有毫米波传感方案依赖专用雷达硬件，与成本功耗受限的无线节点不兼容，需要无线优先的传感范式

Method: 使用单射频链和频率扫描漏波天线，将本地振荡器频率扫描重用于传感，通过频率域虚拟孔径实现二维空间感知

Result: FaA在相同物理和频谱约束下，比传统多通道MIMO传感具有更高的架构效率，提供精细的角度和距离分辨能力

Conclusion: 近场感知可无缝集成到频率敏捷无线射频中，实现硬件高效、可嵌入、隐私保护的ISAC节点

Abstract: Integrated sensing and communication (ISAC) is expected to be natively supported by future 6G wireless radios, yet most mmWave sensing solutions still rely on dedicated radar hardware incompatible with cost and power constrained wireless nodes. This article introduces Frequency-as-Aperture (FaA), a wireless-first sensing paradigm that repurposes inherent frequency agility into a virtual sensing aperture, enabling near-field perception with minimal RF front end complexity. Using a single RF chain and a frequency-scanning leaky-wave antenna, FaA achieves two dimensional spatial sensing by reusing the local oscillator (LO) frequency sweep already employed for wideband communication. From a wireless-system perspective, this shifts spatial sampling from the antenna domain to the frequency domain, embedding radar-grade spatial fingerprints directly into the communication RF chain. A case study shows that FaA provides fine angular and range discrimination with low power consumption and unit cost, demonstrating significantly higher architectural efficiency than conventional multi-channel MIMO based sensing under identical physical and spectral constraints. These results indicate that near-field sensing can be seamlessly integrated into frequency-agile wireless radios, enabling hardware-efficient, embeddable, and privacy-preserving ISAC nodes for smart homes, wearables, and industrial edge deployments.

</details>
