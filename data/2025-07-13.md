<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [On Propositional Program Equivalence (extended abstract)](https://arxiv.org/abs/2507.07480)
*Tobias Kappé*

Main category: cs.PL

TL;DR: 论文讨论了命题程序等价性的最新进展，基于(G)KAT（Guarded Kleene Algebra with Tests）的视角。


<details>
  <summary>Details</summary>
Motivation: 尽管通用程序等价性是不可判定的，但通过抽象语句语义，命题等价性问题变得可判定且实际可行。

Method: 采用(G)KAT（Guarded Kleene Algebra with Tests）框架分析命题程序等价性。

Result: 研究表明，命题等价性在抽象语义下是可判定的，并且具有实际应用价值。

Conclusion: 通过(G)KAT框架，命题程序等价性问题得到了有效的理论支持和实践验证。

Abstract: General program equivalence is undecidable. However, if we abstract away the
semantics of statements, then this problem becomes not just decidable, but
practically feasible. For instance, a program of the form "if $b$ then $e$ else
$f$" should be equivalent to "if not $b$ then $f$ else $e$" - no matter what
$b$, $e$ and $f$ are. This kind of equivalence is known as propositional
equivalence. In this extended abstract, we discuss recent developments in
propositional program equivalence from the perspective of (Guarded) Kleene
Algebra with Tests, or (G)KAT.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Distributed Training under Packet Loss](https://arxiv.org/abs/2507.07114)
*Erez Weintraub,Ron Banner,Ariel Orda*

Main category: cs.DC

TL;DR: 提出了一种新型分布式训练框架，可在不可靠连接下保持模型精度和收敛性，通过无偏梯度聚合和有界参数漂移实现。


<details>
  <summary>Details</summary>
Motivation: 现有分布式框架依赖可靠连接，导致延迟和可扩展性问题，而不可靠连接可能影响模型精度。

Method: 采用两阶段防御机制：无偏梯度聚合和有界参数广播，确保梯度估计一致性和参数漂移有界。

Result: 在LLAMA2 7B模型上实验表明，容忍10%丢包率时困惑度变化不超过0.8%。

Conclusion: 该框架填补了高效通信协议与现代大模型训练需求之间的鸿沟，支持在普通或广域网络上进行鲁棒学习。

Abstract: State-of-the-art language and vision models are routinely trained across
thousands of GPUs, often spanning multiple data-centers, yet today's
distributed frameworks still assume reliable connections (e.g., InfiniBand or
RoCE). The resulting acknowledgment traffic and retransmissions inflate tail
latencies and limit scalability. Leveraging unreliable connections will reduce
latency but may sacrifice model accuracy and convergence once packets are
dropped. A principled, end-to-end solution that preserves accuracy and
convergence guarantees under genuine packet loss has previously been missing.
We address this critical gap by introducing a novel distributed training
framework capable of operating over unreliable connections, offering unbiased
gradient aggregation and bounded parameter drift without modifying model code
or optimizers. The key insight is a two-stage defense against missing messages:
(i) Unbiased gradient aggregation: each worker reconstructs a consistent
gradient estimate from whatever packets arrive, guaranteeing expectation-level
correctness; and (ii) Bounded-drift parameter broadcasts: we prove the
inter-worker model discrepancy remains O(1) even after arbitrarily many
iterations, preventing the unbounded divergence typical of asynchronous setups.
Analytical bounds are matched by experiments on the LLAMA2 7B model with 64
GPUs: tolerating 10% random packet loss yields at most 0.8% perplexity change.
This work bridges the gap between communication-efficient datacenter protocols
and the accuracy and generalization guarantees demanded by modern large-model
training, enabling robust, high-throughput learning on commodity or wide-area
networks.

</details>


### [3] [Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces](https://arxiv.org/abs/2507.07116)
*Juan Cano-Benito,Andrea Cimmino,Sven Hertling,Heiko Paulheim,Raúl García-Castro*

Main category: cs.DC

TL;DR: 论文系统评估了不同分布式账本技术（DLT）在语义数据存储中的表现，发现私有DLT效率最高，混合DLT在公开审计与效率间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 解决数据空间中语义数据在DLT上高效存储的空白问题。

Method: 使用真实知识图谱对不同类型DLT（公有、私有、混合）进行性能、存储效率、资源消耗及语义数据更新与查询能力的系统评估。

Result: 私有DLT在存储和管理语义内容上最有效，混合DLT在公开审计与效率间提供平衡。

Conclusion: 研究为去中心化数据生态系统基于数据主权需求选择合适DLT基础设施提供了依据。

Abstract: Data spaces are emerging as decentralised infrastructures that enable
sovereign, secure, and trustworthy data exchange among multiple participants.
To achieve semantic interoperability within these environments, the use of
semantic web technologies and knowledge graphs has been proposed. Although
distributed ledger technologies (DLT) fit as the underlying infrastructure for
data spaces, there remains a significant gap in terms of the efficient storage
of semantic data on these platforms. This paper presents a systematic
evaluation of semantic data storage across different types of DLT (public,
private, and hybrid), using a real-world knowledge graph as an experimental
basis. The study compares performance, storage efficiency, resource
consumption, and the capabilities to update and query semantic data. The
results show that private DLTs are the most efficient for storing and managing
semantic content, while hybrid DLTs offer a balanced trade-off between public
auditability and operational efficiency. This research leads to a discussion on
the selection of the most appropriate DLT infrastructure based on the data
sovereignty requirements of decentralised data ecosystems.

</details>


### [4] [Collective Communication Profiling of Modern-day Machine Learning Workloads](https://arxiv.org/abs/2507.07117)
*Jit Gupta,Andrew Li,Tarun Banka,Ariel Cohen,T. Sridhar,Raj Yavatkar*

Main category: cs.DC

TL;DR: 分析机器学习任务中集体通信行为对网络性能的影响，并提出优化框架和拓扑的建议。


<details>
  <summary>Details</summary>
Motivation: 机器学习任务中的集体通信操作可能导致网络拥塞和丢包，影响性能，因此需要分析这些行为以优化网络资源配置。

Method: 通过Nvidia集体通信库的日志功能分析多种模型的通信行为，调整并行度、节点数和模型类型等参数。

Result: 研究发现需要重新设计集体通信框架和网络拓扑以适应网络异常对工作负载的影响。

Conclusion: 当前集体通信框架和网络拓扑需优化以应对机器学习工作负载的网络需求。

Abstract: Machine Learning jobs, carried out on large number of distributed high
performance systems, involve periodic communication using operations like
AllReduce, AllGather, and Broadcast. These operations may create high bandwidth
and bursty traffic patterns, leading to network congestion and packet loss,
thus impacting the performance of these jobs. Hence it is imperative to analyze
these patterns, which can be helpful in provisioning network resources
depending on the type of machine learning workloads. In this poster we carry
out extensive analysis of the collective communication behavior seen in a wide
variety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we
instrument Nvidia Collective Communication Library logging functionality for
richer context about the collectives and workloads. We adjust configuration
parameters that influence collective communication behavior, such as
parallelism, number of nodes, and model type. This overview presents and
discusses some of the results on the collective communication behavior for the
open source DeepSeek V3 inferencing model, which includes operation type and
count, transfer sizes per operation, and request size distribution. Our
analysis shows that it makes sense to rethink current collective communication
frameworks and network topologies so as to accommodate the effect of network
anomalies on the mentioned workloads.

</details>


### [5] [Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding](https://arxiv.org/abs/2507.07120)
*Nidhi Bhatia,Ankit More,Ritika Borkar,Tiyasa Mitra,Ramon Matas,Ritchie Zhao,Maximilian Golub,Dheevatsa Mudigere,Brian Pharris,Bita Darvish Rouhani*

Main category: cs.DC

TL;DR: Helix Parallelism是一种混合执行策略，通过KV并行和TP/EP并行优化LLM的推理效率，降低延迟并支持更大批次。


<details>
  <summary>Details</summary>
Motivation: 随着LLM扩展到百万级token的KV历史，实时自回归解码面临KV缓存读取和FFN权重访问的瓶颈，传统TP方法在注意力机制中效率不足。

Method: 提出Helix Parallelism，结合KV并行和TP/EP并行，通过轻量级通信步骤保持注意力行为，并引入Helix HOP-B最小化通信开销。

Result: Helix在固定批次下降低TTL达1.5倍，支持32倍更大批次，提升了DeepSeek-R1的吞吐量-延迟Pareto前沿。

Conclusion: Helix Parallelism实现了超长序列实时推理的实用性，显著优化了GPU效率。

Abstract: As LLMs scale to multi-million-token KV histories, real-time autoregressive
decoding under tight Token-to-Token Latency (TTL) constraints faces growing
pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)
weights and reading long KV caches. While Tensor Parallelism (TP) helps
mitigate the cost of FFN weight reads, it does not scale well for attention.
When TP width exceeds the number of KV heads, it leads to inefficient KV
duplication, limits parallelism, and constrains batch size. Simultaneously,
DRAM reads for long KV histories scale linearly with batch size, further
capping efficiency.
  We introduce Helix Parallelism, a hybrid execution strategy that applies KV
parallelism during attention to shard KV caches across GPUs, then reuses the
same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN
computation. To preserve exact attention behavior, Helix includes a lightweight
communication step. To minimize the exposed communication cost, we introduce
Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through
batchwise overlap, preserving low TTL while improving GPU efficiency. Compared
to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at
fixed batch sizes and supports up to 32x larger batches under the same latency
budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on
Blackwell and making real-time inference with ultra-long-sequence practical.

</details>


### [6] [Ampere: Communication-Efficient and High-Accuracy Split Federated Learning](https://arxiv.org/abs/2507.07130)
*Zihan Zhang,Leon Wong,Blesson Varghese*

Main category: cs.DC

TL;DR: Ampere是一种新型的联邦学习系统，通过减少设备计算和通信开销，同时提高模型准确性，优于现有的Split Federated Learning (SFL)方法。


<details>
  <summary>Details</summary>
Motivation: 解决Split Federated Learning (SFL)中因频繁通信和设备计算成本高导致的效率低下和模型准确性下降问题。

Method: 采用单向块间训练和轻量级辅助网络生成方法，减少梯度传输和中间交换，同时通过整合设备块生成的激活来训练服务器块。

Result: 实验表明，Ampere在模型准确性、训练时间、通信开销和设备计算方面均显著优于SFL基线系统。

Conclusion: Ampere在非独立同分布数据下表现出色，显著提升了联邦学习的效率和准确性。

Abstract: A Federated Learning (FL) system collaboratively trains neural networks
across devices and a server but is limited by significant on-device computation
costs. Split Federated Learning (SFL) systems mitigate this by offloading a
block of layers of the network from the device to a server. However, in doing
so, it introduces large communication overheads due to frequent exchanges of
intermediate activations and gradients between devices and the server and
reduces model accuracy for non-IID data. We propose Ampere, a novel
collaborative training system that simultaneously minimizes on-device
computation and device-server communication while improving model accuracy.
Unlike SFL, which uses a global loss by iterative end-to-end training, Ampere
develops unidirectional inter-block training to sequentially train the device
and server block with a local loss, eliminating the transfer of gradients. A
lightweight auxiliary network generation method decouples training between the
device and server, reducing frequent intermediate exchanges to a single
transfer, which significantly reduces the communication overhead. Ampere
mitigates the impact of data heterogeneity by consolidating activations
generated by the trained device block to train the server block, in contrast to
SFL, which trains on device-specific, non-IID activations. Extensive
experiments on multiple CNNs and transformers show that, compared to
state-of-the-art SFL baseline systems, Ampere (i) improves model accuracy by up
to 13.26% while reducing training time by up to 94.6%, (ii) reduces
device-server communication overhead by up to 99.1% and on-device computation
by up to 93.13%, and (iii) reduces standard deviation of accuracy by 53.39% for
various non-IID degrees highlighting superior performance when faced with
heterogeneous data.

</details>


### [7] [Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure](https://arxiv.org/abs/2507.07223)
*Myoungsoo Jung*

Main category: cs.DC

TL;DR: 论文提出了一种基于CXL的模块化数据中心架构，结合XLink和分层内存模型，以解决AI工作负载中的扩展性和通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现代AI工作负载（如LLMs和RAG）对内存、通信带宽和资源灵活性提出了极高要求，传统GPU架构因通信开销难以扩展。

Method: 提出模块化数据中心架构，结合CXL和XLink技术，设计分层内存模型，并评估轻量级CXL实现、HBM和硅光子学。

Result: 评估显示该架构在AI基础设施中提高了扩展性、吞吐量和灵活性。

Conclusion: 通过CXL和XLink的混合设计，有效解决了AI工作负载中的扩展性问题，为未来数据中心提供了高效解决方案。

Abstract: Modern AI workloads such as large language models (LLMs) and
retrieval-augmented generation (RAG) impose severe demands on memory,
communication bandwidth, and resource flexibility. Traditional GPU-centric
architectures struggle to scale due to growing inter-GPU communication
overheads. This report introduces key AI concepts and explains how Transformers
revolutionized data representation in LLMs. We analyze large-scale AI hardware
and data center designs, identifying scalability bottlenecks in hierarchical
systems. To address these, we propose a modular data center architecture based
on Compute Express Link (CXL) that enables disaggregated scaling of memory,
compute, and accelerators. We further explore accelerator-optimized
interconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink
Fusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance
data transfers while preserving memory coherence. We also propose a
hierarchical memory model that combines local and pooled memory, and evaluate
lightweight CXL implementations, HBM, and silicon photonics for efficient
scaling. Our evaluations demonstrate improved scalability, throughput, and
flexibility in AI infrastructure.

</details>


### [8] [M$^2$-MFP: A Multi-Scale and Multi-Level Memory Failure Prediction Framework for Reliable Cloud Infrastructure](https://arxiv.org/abs/2507.07144)
*Hongyi Xie,Min Zhou,Qiao Yu,Jialiang Yu,Zhenli Sheng,Hong Xie,Defu Lian*

Main category: cs.DC

TL;DR: M$^2$-MFP是一种多尺度分层内存故障预测框架，通过将可纠正错误转换为多级二进制矩阵表示，并引入二进制空间特征提取器，显著提升了云基础设施的可靠性。


<details>
  <summary>Details</summary>
Motivation: 内存故障对系统稳定性构成重大威胁，现有预测方法在泛化性和性能上存在不足，需要更高效的解决方案。

Method: M$^2$-MFP将可纠正错误转换为多级二进制矩阵，使用二进制空间特征提取器（BSFE）提取特征，并采用双路径时间建模架构（时间补丁模块和时间点模块）。

Result: 在基准数据集和实际部署中，M$^2$-MFP显著优于现有最先进方法。

Conclusion: M$^2$-MFP通过多尺度特征提取和双路径建模，有效提升了内存故障预测的准确性和可靠性。

Abstract: As cloud services become increasingly integral to modern IT infrastructure,
ensuring hardware reliability is essential to sustain high-quality service.
Memory failures pose a significant threat to overall system stability, making
accurate failure prediction through the analysis of memory error logs (i.e.,
Correctable Errors) imperative. Existing memory failure prediction approaches
have notable limitations: rule-based expert models suffer from limited
generalizability and low recall rates, while automated feature extraction
methods exhibit suboptimal performance. To address these limitations, we
propose M$^2$-MFP: a Multi-scale and hierarchical memory failure prediction
framework designed to enhance the reliability and availability of cloud
infrastructure. M$^2$-MFP converts Correctable Errors (CEs) into multi-level
binary matrix representations and introduces a Binary Spatial Feature Extractor
(BSFE) to automatically extract high-order features at both DIMM-level and
bit-level. Building upon the BSFE outputs, we develop a dual-path temporal
modeling architecture: 1) a time-patch module that aggregates multi-level
features within observation windows, and 2) a time-point module that employs
interpretable rule-generation trees trained on bit-level patterns. Experiments
on both benchmark datasets and real-world deployment show the superiority of
M$^2$-MFP as it outperforms existing state-of-the-art methods by significant
margins. Code and data are available at this repository:
https://github.com/hwcloud-RAS/M2-MFP.

</details>


### [9] [Machine Learning-driven Multiscale MD Workflows: The Mini-MuMMI Experience](https://arxiv.org/abs/2507.07352)
*Loïc Pottier,Konstantia Georgouli,Timothy S. Carpenter,Fikret Aydin,Jeremy O. B. Tempkin,Dwight V. Nissley,Frederick H. Streitz,Thomas R. W. Scogland,Peer-Timo Bremer,Felice C. Lightstone,Helgi I. Ingólfsson*

Main category: cs.DC

TL;DR: 论文介绍了多尺度机器学习建模基础设施（MuMMI）及其轻量版mini-MuMMI，用于管理大规模并行多尺度分子动力学模拟，并展示了其在RAS-RAF膜相互作用中的应用。


<details>
  <summary>Details</summary>
Motivation: 多尺度建模在复杂现象（如生物分子相互作用）中至关重要，但跨尺度建模和并行系统管理具有挑战性。

Method: 提出MuMMI及其轻量版mini-MuMMI，支持从毫秒到纳秒的多尺度分子动力学模拟管理。

Result: mini-MuMMI在小型HPC系统或笔记本电脑上成功运行，并应用于RAS-RAF膜相互作用研究。

Conclusion: mini-MuMMI扩展了多尺度工作流的适用范围，为更广泛的应用提供了可能性。

Abstract: Computational models have become one of the prevalent methods to model
complex phenomena. To accurately model complex interactions, such as detailed
biomolecular interactions, scientists often rely on multiscale models comprised
of several internal models operating at difference scales, ranging from
microscopic to macroscopic length and time scales. Bridging the gap between
different time and length scales has historically been challenging but the
advent of newer machine learning (ML) approaches has shown promise for tackling
that task. Multiscale models require massive amounts of computational power and
a powerful workflow management system. Orchestrating ML-driven multiscale
studies on parallel systems with thousands of nodes is challenging, the
workflow must schedule, allocate and control thousands of simulations operating
at different scales. Here, we discuss the massively parallel Multiscale
Machine-Learned Modeling Infrastructure (MuMMI), a multiscale workflow
management infrastructure, that can orchestrate thousands of molecular dynamics
(MD) simulations operating at different timescales, spanning from millisecond
to nanosecond. More specifically, we introduce a novel version of MuMMI called
"mini-MuMMI". Mini-MuMMI is a curated version of MuMMI designed to run on
modest HPC systems or even laptops whereas MuMMI requires larger HPC systems.
We demonstrate mini-MuMMI utility by exploring RAS-RAF membrane interactions
and discuss the different challenges behind the generalization of multiscale
workflows and how mini-MuMMI can be leveraged to target a broader range of
applications outside of MD and RAS-RAF interactions.

</details>


### [10] [KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows](https://arxiv.org/abs/2507.07400)
*Zaifeng Pan,Ajjkumar Patel,Zhengding Hu,Yipeng Shen,Yue Guan,Wan-Lu Li,Lianhui Qin,Yida Wang,Yufei Ding*

Main category: cs.DC

TL;DR: KVFlow是一个针对基于LLM的代理工作流程优化的KV缓存管理框架，通过预测代理的未来使用情况，减少缓存未命中和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有系统使用LRU策略管理KV缓存，无法预测代理的未来使用，导致频繁缓存未命中和性能下降。

Method: KVFlow通过Agent Step Graph抽象代理执行计划，估计代理的未来激活时间，并采用细粒度驱逐策略和重叠KV预取机制。

Result: KVFlow在单工作流程和多并发工作流程场景下分别实现了1.83倍和2.19倍的加速。

Conclusion: KVFlow显著提升了LLM代理工作流程的效率和性能。

Abstract: Large language model (LLM) based agentic workflows have become a popular
paradigm for coordinating multiple specialized agents to solve complex tasks.
To improve serving efficiency, existing LLM systems employ prefix caching to
reuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby
avoiding redundant computation across repeated invocations. However, current
systems typically evict KV caches using a Least Recently Used (LRU) policy,
which fails to anticipate future agent usage and often discards KV caches
shortly before their reuse. This leads to frequent cache misses and substantial
recomputation or swapping overhead. We present KVFlow, a workflow-aware KV
cache management framework tailored for agentic workloads. KVFlow abstracts the
agent execution schedule as an Agent Step Graph and assigns each agent a
steps-to-execution value that estimates its temporal proximity to future
activation. These values guide a fine-grained eviction policy at the KV node
level, allowing KVFlow to preserve entries likely to be reused and efficiently
manage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a
fully overlapped KV prefetching mechanism, which proactively loads required
tensors from CPU to GPU in background threads for agents scheduled in the next
step, thereby avoiding cache miss stalls during generation. Compared to SGLang
with hierarchical radix cache, KVFlow achieves up to 1.83$\times$ speedup for
single workflows with large prompts, and up to 2.19$\times$ speedup for
scenarios with many concurrent workflows.

</details>


### [11] [Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems](https://arxiv.org/abs/2507.07671)
*Jovan Prodanov,Blaž Bertalanič,Carolina Fortuna,Shih-Kai Chou,Matjaž Branko Jurič,Ramon Sanchez-Iborra,Jernej Hribar*

Main category: cs.DC

TL;DR: 论文提出了一种基于多智能体强化学习的动态资源扩展引擎（MARLISE），用于优化边缘云系统中的资源利用和性能。


<details>
  <summary>Details</summary>
Motivation: 传统静态阈值和预定义规则的资源扩展方法在动态环境中效率低下，无法满足边缘云基础设施的需求。

Method: 使用两种深度强化学习算法（DQN和PPO）开发MARLISE，实现动态、反应式的资源扩展。

Result: 实验表明，MARLISE在管理资源弹性和保持微服务响应时间方面优于启发式方法，同时提高资源效率。

Conclusion: MARLISE为边缘云系统提供了一种高效的动态资源扩展解决方案。

Abstract: Modern edge-cloud systems face challenges in efficiently scaling resources to
handle dynamic and unpredictable workloads. Traditional scaling approaches
typically rely on static thresholds and predefined rules, which are often
inadequate for optimizing resource utilization and maintaining performance in
distributed and dynamic environments. This inefficiency hinders the
adaptability and performance required in edge-cloud infrastructures, which can
only be achieved through the newly proposed in-place scaling. To address this
problem, we propose the Multi-Agent Reinforcement Learning-based In-place
Scaling Engine (MARLISE) that enables seamless, dynamic, reactive control with
in-place resource scaling. We develop our solution using two Deep Reinforcement
Learning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization
(PPO). We analyze each version of the proposed MARLISE solution using dynamic
workloads, demonstrating their ability to ensure low response times of
microservices and scalability. Our results show that MARLISE-based approaches
outperform heuristic method in managing resource elasticity while maintaining
microservice response times and achieving higher resource efficiency.

</details>


### [12] [KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based Auto-Scaling](https://arxiv.org/abs/2507.07932)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Qiang Guan,Hailong Jiang*

Main category: cs.DC

TL;DR: KIS-S框架结合GPU感知的Kubernetes推理模拟器KISim和基于PPO的自动扩展器KIScaler，显著提升GPU推理工作负载的扩展性能。


<details>
  <summary>Details</summary>
Motivation: 解决Kubernetes中GPU推理工作负载的自动扩展问题，特别是针对动态和突发流量模式以及缺乏GPU级指标集成的挑战。

Method: 提出KIS-S框架，结合KISim（GPU感知模拟器）和KIScaler（基于PPO的自动扩展器），在模拟中学习延迟感知和资源高效的扩展策略。

Result: 实验显示KIScaler平均奖励提升75.2%，P95延迟降低6.7倍，且无需重新训练即可泛化。

Conclusion: KIS-S填补了反应式自动扩展与智能编排之间的差距，适用于可扩展的GPU加速环境。

Abstract: Autoscaling GPU inference workloads in Kubernetes remains challenging due to
the reactive and threshold-based nature of default mechanisms such as the
Horizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty
traffic patterns and lack integration with GPU-level metrics. We present KIS-S,
a unified framework that combines KISim, a GPU-aware Kubernetes Inference
Simulator, with KIScaler, a Proximal Policy Optimization (PPO)-based
autoscaler. KIScaler learns latency-aware and resource-efficient scaling
policies entirely in simulation, and is directly deployed without retraining.
Experiments across four traffic patterns show that KIScaler improves average
reward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and
generalizes without retraining. Our work bridges the gap between reactive
autoscaling and intelligent orchestration for scalable GPU-accelerated
environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [Accelerating Transposed Convolutions on FPGA-based Edge Devices](https://arxiv.org/abs/2507.07683)
*Jude Haris,José Cano*

Main category: cs.AR

TL;DR: 论文提出MM2IM，一种软硬件协同设计的加速器，用于高效处理资源受限边缘设备上的转置卷积（TCONV）层，相比现有方法显著提升性能和能效。


<details>
  <summary>Details</summary>
Motivation: 现有输入导向映射（IOM）方法在实现TCONV时存在输出映射复杂、计算重叠和无效计算等问题，导致性能瓶颈，尤其在资源受限的边缘设备上。

Method: 提出MM2IM，结合矩阵乘法（MatMul）与col2IM，通过软硬件协同设计优化TCONV层的处理。

Result: 在261种TCONV配置中平均加速1.9倍，在知名生成模型的TCONV层上最高加速4.2倍，性能优于同类加速器至少2倍GOPs/DSP。在DCGAN和pix2pix GAN模型上实现3倍加速和2.4倍能耗降低。

Conclusion: MM2IM显著提升了TCONV在边缘设备上的效率和性能，为生成模型的部署提供了实用解决方案。

Abstract: Transposed Convolutions (TCONV) enable the up-scaling mechanism within
generative Artificial Intelligence (AI) models. However, the predominant
Input-Oriented Mapping (IOM) method for implementing TCONV has complex output
mapping, overlapping sums, and ineffectual computations. These inefficiencies
further exacerbate the performance bottleneck of TCONV and generative models on
resource-constrained edge devices. To address this problem, in this paper we
propose MM2IM, a hardware-software co-designed accelerator that combines Matrix
Multiplication (MatMul) with col2IM to process TCONV layers on
resource-constrained edge devices efficiently. Using the SECDA-TFLite design
toolkit, we implement MM2IM and evaluate its performance across 261 TCONV
problem configurations, achieving an average speedup of 1.9x against a
dual-thread ARM Neon optimized CPU baseline. We then evaluate the performance
of MM2IM on a range of TCONV layers from well-known generative models achieving
up to 4.2x speedup, and compare it against similar resource-constrained TCONV
accelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate
MM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x
energy reduction against the CPU baseline.

</details>
