{"id": "2511.22075", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.22075", "abs": "https://arxiv.org/abs/2511.22075", "authors": ["Doruk Alp Mutlu"], "title": "Expanding Specification Capabilities of a Gradual Verifier with Pure Functions", "comment": "Submitted to the 53rd ACM SIGPLAN Symposium on Principles of Programming Languages (POPL 2026) Student Research Competition", "summary": "Gradual verification soundly combines static checking and dynamic checking to provide an incremental approach for software verification. With gradual verification, programs can be partially specified first, and then the full specification of a program can be achieved in incremental steps. The first and only practicable gradual verifier based on symbolic execution, Gradual C0, supports recursive heap data structures. Despite recent efforts to improve the expressivity of Gradual C0's specification language, Gradual C0's specification language is still limited in its capabilities for complex expressions. This work explores an extension to Gradual C0's design with a common construct supported by many static verification tools, pure functions, which both extend the specification capabilities of Gradual C0 and increase the ease of encoding observer methods in Gradual C0. Our approach addresses the technical challenges related to the axiomatisation of pure functions with imprecise specifications.", "AI": {"tldr": "\u6269\u5c55Gradual C0\u9a8c\u8bc1\u5668\uff0c\u652f\u6301\u7eaf\u51fd\u6570\u4ee5\u589e\u5f3a\u89c4\u8303\u8868\u8fbe\u80fd\u529b\u548c\u7b80\u5316\u89c2\u5bdf\u8005\u65b9\u6cd5\u7f16\u7801", "motivation": "\u5c3d\u7ba1Gradual C0\u662f\u76ee\u524d\u552f\u4e00\u57fa\u4e8e\u7b26\u53f7\u6267\u884c\u7684\u5b9e\u7528\u6e10\u8fdb\u9a8c\u8bc1\u5668\uff0c\u4f46\u5176\u89c4\u8303\u8bed\u8a00\u5728\u590d\u6742\u8868\u8fbe\u5f0f\u65b9\u9762\u4ecd\u6709\u9650\u5236\uff0c\u9700\u8981\u6269\u5c55\u8868\u8fbe\u80fd\u529b", "method": "\u5728Gradual C0\u8bbe\u8ba1\u4e2d\u5f15\u5165\u7eaf\u51fd\u6570\u652f\u6301\uff0c\u89e3\u51b3\u5177\u6709\u4e0d\u7cbe\u786e\u89c4\u8303\u7684\u7eaf\u51fd\u6570\u7684\u516c\u7406\u5316\u6280\u672f\u6311\u6218", "result": "\u6269\u5c55\u4e86Gradual C0\u7684\u89c4\u8303\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u89c2\u5bdf\u8005\u65b9\u6cd5\u7f16\u7801\u7684\u4fbf\u5229\u6027", "conclusion": "\u901a\u8fc7\u652f\u6301\u7eaf\u51fd\u6570\uff0c\u663e\u8457\u589e\u5f3a\u4e86Gradual C0\u9a8c\u8bc1\u5668\u7684\u8868\u8fbe\u80fd\u529b\u548c\u5b9e\u7528\u6027"}}
{"id": "2511.22419", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.22419", "abs": "https://arxiv.org/abs/2511.22419", "authors": ["Ken Sakayori", "Andrea Colledan", "Ugo Dal Lago"], "title": "On Circuit Description Languages, Indexed Monads, and Resource Analysis", "comment": "Extended version of a paper to be published at POPL 2026", "summary": "In this paper, a monad-based denotational model is introduced and shown adequate for the Proto-Quipper family of calculi, themselves being idealized versions of the Quipper programming language. The use of a monadic approach allows us to separate the value to which a term reduces from the circuit that the term itself produces as a side effect. In turn, this enables the denotational interpretation and validation of rich type systems in which the size of the produced circuit can be controlled. Notably, the proposed semantic framework, through the novel concept of circuit algebra, suggests forms of effect typing guaranteeing quantitative properties about the resulting circuit, even in presence of optimizations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5355\u5b50\u7684\u6307\u79f0\u8bed\u4e49\u6a21\u578b\uff0c\u9002\u7528\u4e8eProto-Quipper\u7cfb\u5217\u6f14\u7b97\uff0c\u80fd\u5206\u79bb\u8ba1\u7b97\u503c\u548c\u7535\u8def\u526f\u4f5c\u7528\uff0c\u652f\u6301\u63a7\u5236\u7535\u8def\u5927\u5c0f\u7684\u7c7b\u578b\u7cfb\u7edf\u9a8c\u8bc1\u3002", "motivation": "\u4e3aProto-Quipper\u6f14\u7b97\uff08Quipper\u7f16\u7a0b\u8bed\u8a00\u7684\u7406\u60f3\u5316\u7248\u672c\uff09\u5efa\u7acb\u5f62\u5f0f\u5316\u8bed\u4e49\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u5bf9\u91cf\u5b50\u7535\u8def\u751f\u6210\u8fdb\u884c\u7c7b\u578b\u63a7\u5236\u548c\u9a8c\u8bc1\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5355\u5b50\u7684\u6307\u79f0\u8bed\u4e49\u65b9\u6cd5\uff0c\u5f15\u5165\u7535\u8def\u4ee3\u6570\u65b0\u6982\u5ff5\uff0c\u5206\u79bb\u8ba1\u7b97\u503c\u548c\u7535\u8def\u526f\u4f5c\u7528\uff0c\u652f\u6301\u6548\u679c\u7c7b\u578b\u7cfb\u7edf\u3002", "result": "\u5efa\u7acb\u4e86Proto-Quipper\u6f14\u7b97\u7684\u5145\u5206\u6027\u6307\u79f0\u6a21\u578b\uff0c\u80fd\u9a8c\u8bc1\u63a7\u5236\u7535\u8def\u5927\u5c0f\u7684\u7c7b\u578b\u7cfb\u7edf\uff0c\u5373\u4f7f\u5728\u4f18\u5316\u5b58\u5728\u65f6\u4e5f\u80fd\u4fdd\u8bc1\u7535\u8def\u7684\u5b9a\u91cf\u6027\u8d28\u3002", "conclusion": "\u5355\u5b50\u65b9\u6cd5\u4e3a\u91cf\u5b50\u7f16\u7a0b\u8bed\u8a00\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bed\u4e49\u6846\u67b6\uff0c\u901a\u8fc7\u7535\u8def\u4ee3\u6570\u548c\u6548\u679c\u7c7b\u578b\u7cfb\u7edf\uff0c\u80fd\u5bf9\u751f\u6210\u7684\u7535\u8def\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u63a7\u5236\u3002"}}
{"id": "2511.22692", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.22692", "abs": "https://arxiv.org/abs/2511.22692", "authors": ["David Castro-Perez", "Francisco Ferreira", "Sung-Shik Jongmans"], "title": "A Synthetic Reconstruction of Multiparty Session Types (with Appendix)", "comment": null, "summary": "Multiparty session types (MPST) provide a rigorous foundation for verifying the safety and liveness of concurrent systems. However, existing approaches often force a difficult trade-off: classical, projection-based techniques are compositional but limited in expressiveness, while more recent techniques achieve higher expressiveness by relying on non-compositional, whole-system model checking, which scales poorly.\n  This paper introduces a new approach to MPST that delivers both expressiveness and compositionality, called the synthetic approach. Our key innovation is a type system that verifies each process directly against a global protocol specification, represented as a labelled transition system (LTS) in general, with global types as a special case. This approach uniquely avoids the need for intermediate local types and projection.\n  We demonstrate that our approach, while conceptually simpler, supports a benchmark of challenging protocols that were previously beyond the reach of compositional techniques in the MPST literature. We generalise our type system, showing that it can validate processes against any specification that constitutes a \"well-behaved\" LTS, supporting protocols not expressible with the standard global type syntax. The entire framework, including all theorems and many examples, has been formalised and mechanised in Agda, and we have developed a prototype implementation as an extension to VS Code.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u591a\u4f1a\u8bdd\u7c7b\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u9a8c\u8bc1\u8fdb\u7a0b\u4e0e\u5168\u5c40\u534f\u8bae\u89c4\u8303\uff08LTS\uff09\u6765\u540c\u65f6\u5b9e\u73b0\u8868\u8fbe\u6027\u548c\u7ec4\u5408\u6027\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6295\u5f71\u65b9\u6cd5\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u591a\u4f1a\u8bdd\u7c7b\u578b\u65b9\u6cd5\u9762\u4e34\u8868\u8fbe\u6027\u4e0e\u7ec4\u5408\u6027\u7684\u6743\u8861\uff1a\u57fa\u4e8e\u6295\u5f71\u7684\u7ecf\u5178\u65b9\u6cd5\u5177\u6709\u7ec4\u5408\u6027\u4f46\u8868\u8fbe\u6027\u6709\u9650\uff0c\u800c\u65b0\u65b9\u6cd5\u901a\u8fc7\u975e\u7ec4\u5408\u7684\u5168\u7cfb\u7edf\u6a21\u578b\u68c0\u67e5\u63d0\u9ad8\u8868\u8fbe\u6027\u4f46\u6269\u5c55\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u5408\u6210\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u7c7b\u578b\u7cfb\u7edf\u76f4\u63a5\u9a8c\u8bc1\u6bcf\u4e2a\u8fdb\u7a0b\u4e0e\u5168\u5c40\u534f\u8bae\u89c4\u8303\uff08\u6807\u8bb0\u8f6c\u79fb\u7cfb\u7edfLTS\uff09\uff0c\u907f\u514d\u4e2d\u95f4\u5c40\u90e8\u7c7b\u578b\u548c\u6295\u5f71\uff0c\u652f\u6301\u4e00\u822cLTS\u89c4\u8303\u800c\u4e0d\u4ec5\u662f\u5168\u5c40\u7c7b\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u652f\u6301\u5148\u524d\u7ec4\u5408\u6280\u672f\u65e0\u6cd5\u5904\u7406\u7684\u6311\u6218\u6027\u534f\u8bae\u57fa\u51c6\uff0c\u53ef\u9a8c\u8bc1\u4efb\u4f55\"\u884c\u4e3a\u826f\u597d\"LTS\u7684\u8fdb\u7a0b\uff0c\u652f\u6301\u6807\u51c6\u5168\u5c40\u7c7b\u578b\u8bed\u6cd5\u65e0\u6cd5\u8868\u8fbe\u7684\u534f\u8bae\uff0c\u5df2\u5728Agda\u4e2d\u5f62\u5f0f\u5316\u5e76\u5f00\u53d1VS Code\u539f\u578b\u3002", "conclusion": "\u5408\u6210\u65b9\u6cd5\u4e3a\u591a\u4f1a\u8bdd\u7c7b\u578b\u63d0\u4f9b\u4e86\u540c\u65f6\u5b9e\u73b0\u8868\u8fbe\u6027\u548c\u7ec4\u5408\u6027\u7684\u65b0\u9014\u5f84\uff0c\u6982\u5ff5\u66f4\u7b80\u5355\u4e14\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u534f\u8bae\u89c4\u8303\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.21844", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21844", "abs": "https://arxiv.org/abs/2511.21844", "authors": ["Murat Yaslioglu"], "title": "A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain", "comment": null, "summary": "In an age where sustainability is of paramount importance, the significance of both high-performance computing and intelligent algorithms cannot be understated. Yet, these domains often demand hefty computational power, translating to substantial energy usage and potentially sidelining less robust computing systems. It's evident that we need an approach that is more encompassing, scalable, and eco-friendly for intelligent algorithm development and implementation. The strategy we present in this paper offers a compelling answer to these issues. We unveil a fresh framework that seamlessly melds high-performance cluster computing with intelligent algorithms, all within a blockchain infrastructure. This promotes both efficiency and a broad-based participation. At its core, our design integrates an evolved proof-of-work consensus process, which links computational efforts directly to rewards for producing blocks. This ensures both optimal resource use and participation from a wide spectrum of computational capacities. Additionally, our approach incorporates a dynamic 'trust rating' that evolves based on a track record of accurate block validations. This rating determines the likelihood of a node being chosen for block generation, creating a merit-based system that recognizes and rewards genuine and precise contributions. To level the playing field further, we suggest a statistical 'draw' system, allowing even less powerful nodes a chance to be part of the block creation process.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u9ad8\u6027\u80fd\u96c6\u7fa4\u8ba1\u7b97\u3001\u667a\u80fd\u7b97\u6cd5\u4e0e\u533a\u5757\u94fe\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u7684PoW\u5171\u8bc6\u3001\u52a8\u6001\u4fe1\u4efb\u8bc4\u7ea7\u548c\u7edf\u8ba1\u62bd\u7b7e\u7cfb\u7edf\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u73af\u4fdd\u3001\u5305\u5bb9\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u3002", "motivation": "\u5f53\u524d\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u667a\u80fd\u7b97\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u5bfc\u81f4\u9ad8\u80fd\u8017\uff0c\u4e14\u6392\u9664\u4e86\u8ba1\u7b97\u80fd\u529b\u8f83\u5f31\u7684\u7cfb\u7edf\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5305\u5bb9\u3001\u53ef\u6269\u5c55\u4e14\u73af\u4fdd\u7684\u667a\u80fd\u7b97\u6cd5\u5f00\u53d1\u548c\u5b9e\u65bd\u65b9\u6cd5\u3002", "method": "1) \u63d0\u51fa\u65b0\u6846\u67b6\uff0c\u5c06\u9ad8\u6027\u80fd\u96c6\u7fa4\u8ba1\u7b97\u4e0e\u667a\u80fd\u7b97\u6cd5\u96c6\u6210\u5230\u533a\u5757\u94fe\u57fa\u7840\u8bbe\u65bd\u4e2d\uff1b2) \u6539\u8fdb\u7684PoW\u5171\u8bc6\u673a\u5236\uff0c\u5c06\u8ba1\u7b97\u5de5\u4f5c\u4e0e\u533a\u5757\u5956\u52b1\u76f4\u63a5\u6302\u94a9\uff1b3) \u52a8\u6001\u4fe1\u4efb\u8bc4\u7ea7\u7cfb\u7edf\uff0c\u57fa\u4e8e\u51c6\u786e\u9a8c\u8bc1\u5386\u53f2\u8c03\u6574\u8282\u70b9\u4fe1\u8a89\uff1b4) \u7edf\u8ba1\u62bd\u7b7e\u7cfb\u7edf\uff0c\u4e3a\u8ba1\u7b97\u80fd\u529b\u8f83\u5f31\u7684\u8282\u70b9\u63d0\u4f9b\u53c2\u4e0e\u673a\u4f1a\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u8d44\u6e90\u4f18\u5316\u5229\u7528\u548c\u5e7f\u6cdb\u53c2\u4e0e\uff0c\u521b\u5efa\u4e86\u57fa\u4e8e\u8d21\u732e\u8d28\u91cf\u7684\u5956\u52b1\u7cfb\u7edf\uff0c\u4f7f\u4e0d\u540c\u8ba1\u7b97\u80fd\u529b\u7684\u8282\u70b9\u90fd\u80fd\u53c2\u4e0e\u533a\u5757\u751f\u6210\u8fc7\u7a0b\u3002", "conclusion": "\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u4e3a\u53ef\u6301\u7eed\u3001\u5305\u5bb9\u4e14\u9ad8\u6548\u7684\u667a\u80fd\u7b97\u6cd5\u5b9e\u65bd\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u901a\u8fc7\u533a\u5757\u94fe\u6280\u672f\u5e73\u8861\u4e86\u6027\u80fd\u3001\u80fd\u8017\u548c\u53c2\u4e0e\u5ea6\u4e4b\u95f4\u7684\u77db\u76fe\u3002"}}
{"id": "2511.23283", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.23283", "abs": "https://arxiv.org/abs/2511.23283", "authors": ["Alexandre Moine", "Sam Westrick", "Joseph Tassarotti"], "title": "All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs", "comment": "32 pages, 26 figures, extended version of the same paper accepted at POPL 2026", "summary": "Nondeterminism makes parallel programs challenging to write and reason about. To avoid these challenges, researchers have developed techniques for internally deterministic parallel programming, in which the steps of a parallel computation proceed in a deterministic way. Internal determinism is useful because it lets a programmer reason about a program as if it executed in a sequential order. However, no verification framework exists to exploit this property and simplify formal reasoning about internally deterministic programs.\n  To capture the essence of why internally deterministic programs should be easier to reason about, this paper defines a property called schedule-independent safety. A program satisfies schedule-independent safety, if, to show that the program is safe across all orderings, it suffices to show that one terminating execution of the program is safe. We then present a separation logic called Musketeer for proving that a program satisfies schedule-independent safety. Once a parallel program has been shown to satisfy schedule-independent safety, we can verify it with a new logic called Angelic, which allows one to dynamically select and verify just one sequential ordering of the program.\n  Using Musketeer, we prove the soundness of MiniDet, an affine type system for enforcing internal determinism. MiniDet supports several core algorithmic primitives for internally deterministic programming that have been identified in the research literature, including a deterministic version of a concurrent hash set. Because any syntactically well-typed MiniDet program satisfies schedule-independent safety, we can apply Angelic to verify such programs.\n  All results in this paper have been verified in Rocq using the Iris separation logic framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Musketeer\u5206\u79bb\u903b\u8f91\u6765\u9a8c\u8bc1\u8c03\u5ea6\u65e0\u5173\u5b89\u5168\u6027\uff0c\u4ee5\u53caAngelic\u903b\u8f91\u6765\u7b80\u5316\u5185\u90e8\u786e\u5b9a\u6027\u5e76\u884c\u7a0b\u5e8f\u7684\u9a8c\u8bc1\u3002", "motivation": "\u5185\u90e8\u786e\u5b9a\u6027\u5e76\u884c\u7f16\u7a0b\u867d\u7136\u7b80\u5316\u4e86\u7a0b\u5e8f\u63a8\u7406\uff0c\u4f46\u7f3a\u4e4f\u5229\u7528\u8fd9\u4e00\u7279\u6027\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u6846\u67b6\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5185\u90e8\u786e\u5b9a\u6027\u6765\u7b80\u5316\u7a0b\u5e8f\u9a8c\u8bc1\u3002", "method": "1. \u5b9a\u4e49\u8c03\u5ea6\u65e0\u5173\u5b89\u5168\u6027\uff1a\u53ea\u9700\u9a8c\u8bc1\u4e00\u4e2a\u7ec8\u6b62\u6267\u884c\u7684\u5b89\u5168\u6027\u5373\u53ef\u4fdd\u8bc1\u6240\u6709\u987a\u5e8f\u7684\u5b89\u5168\u6027\uff1b2. \u63d0\u51faMusketeer\u5206\u79bb\u903b\u8f91\u8bc1\u660e\u8c03\u5ea6\u65e0\u5173\u5b89\u5168\u6027\uff1b3. \u63d0\u51faAngelic\u903b\u8f91\u52a8\u6001\u9009\u62e9\u548c\u9a8c\u8bc1\u5355\u4e00\u987a\u5e8f\uff1b4. \u8bc1\u660eMiniDet\u7c7b\u578b\u7cfb\u7edf\u7684\u6b63\u786e\u6027\u3002", "result": "1. \u5efa\u7acb\u4e86\u8c03\u5ea6\u65e0\u5173\u5b89\u5168\u6027\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\uff1b2. \u5f00\u53d1\u4e86Musketeer\u548cAngelic\u903b\u8f91\u6846\u67b6\uff1b3. \u8bc1\u660e\u4e86MiniDet\u7c7b\u578b\u7cfb\u7edf\u7684\u6b63\u786e\u6027\uff1b4. \u652f\u6301\u5305\u62ec\u786e\u5b9a\u6027\u5e76\u53d1\u54c8\u5e0c\u96c6\u5728\u5185\u7684\u6838\u5fc3\u7b97\u6cd5\u539f\u8bed\uff1b5. \u6240\u6709\u7ed3\u679c\u5728Rocq\u4e2d\u4f7f\u7528Iris\u5206\u79bb\u903b\u8f91\u6846\u67b6\u9a8c\u8bc1\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u9996\u4e2a\u5229\u7528\u5185\u90e8\u786e\u5b9a\u6027\u7b80\u5316\u5e76\u884c\u7a0b\u5e8f\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u5ea6\u65e0\u5173\u5b89\u5168\u6027\u548c\u65b0\u7684\u903b\u8f91\u7cfb\u7edf\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u90e8\u786e\u5b9a\u6027\u7a0b\u5e8f\u7684\u9a8c\u8bc1\u590d\u6742\u5ea6\u3002"}}
{"id": "2511.21859", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21859", "abs": "https://arxiv.org/abs/2511.21859", "authors": ["Hagit Attiya", "Armando Casta\u00f1eda", "Dhrubajyoti Ghosh", "Thomas Nowak"], "title": "Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models", "comment": "18 pages", "summary": "We revisit the relationship between two fundamental models of distributed computation: the asynchronous message-passing model with up to $f$ crash failures ($\\operatorname{AMP}_f$) and the Heard-Of model with up to $f$ message omissions ($\\operatorname{HO}_f$). We show that for $n > 2f$, the two models are equivalent with respect to the solvability of colorless tasks, and that for colored tasks the equivalence holds only when $f = 1$ (and $n > 2$). The separation for larger $f$ arises from the presence of silenced processes in $\\operatorname{HO}_f$, which may lead to incompatible decisions. The proofs proceed through bidirectional simulations between $\\operatorname{AMP}_f$ and $\\operatorname{HO}_f$ via an intermediate model that captures this notion of silencing. The results extend to randomized protocols against a non-adaptive adversary, indicating that the expressive limits of canonical rounds are structural rather than probabilistic. Together, these results delineate precisely where round-based abstractions capture asynchronous computation, and where they do not.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u5f02\u6b65\u6d88\u606f\u4f20\u9012\u6a21\u578b(AMP_f)\u548cHeard-Of\u6a21\u578b(HO_f)\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u5bf9\u4e8en>2f\u7684\u60c5\u51b5\uff0c\u4e24\u79cd\u6a21\u578b\u5728\u65e0\u8272\u4efb\u52a1\u7684\u53ef\u89e3\u6027\u4e0a\u662f\u7b49\u4ef7\u7684\uff0c\u4f46\u5bf9\u4e8e\u6709\u8272\u4efb\u52a1\uff0c\u4ec5\u5f53f=1\u65f6\u7b49\u4ef7\uff0cf\u66f4\u5927\u65f6\u5b58\u5728\u5206\u79bb\u3002", "motivation": "\u7814\u7a76\u4e24\u79cd\u57fa\u672c\u5206\u5e03\u5f0f\u8ba1\u7b97\u6a21\u578b\uff08\u5f02\u6b65\u6d88\u606f\u4f20\u9012\u6a21\u578b\u548cHeard-Of\u6a21\u578b\uff09\u4e4b\u95f4\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u660e\u786e\u754c\u5b9a\u57fa\u4e8e\u8f6e\u6b21\u7684\u62bd\u8c61\u5728\u4f55\u5904\u80fd\u591f\u6355\u83b7\u5f02\u6b65\u8ba1\u7b97\uff0c\u5728\u4f55\u5904\u4e0d\u80fd\u3002", "method": "\u901a\u8fc7\u53cc\u5411\u6a21\u62df\u65b9\u6cd5\uff0c\u5728AMP_f\u548cHO_f\u4e4b\u95f4\u5efa\u7acb\u7b49\u4ef7\u5173\u7cfb\u8bc1\u660e\uff0c\u4f7f\u7528\u4e00\u4e2a\u4e2d\u95f4\u6a21\u578b\u6765\u6355\u6349HO_f\u4e2d\u7684\"\u9759\u9ed8\u8fdb\u7a0b\"\u6982\u5ff5\u3002\u8bc1\u660e\u6269\u5c55\u5230\u9488\u5bf9\u975e\u81ea\u9002\u5e94\u5bf9\u624b\u7684\u968f\u673a\u5316\u534f\u8bae\u3002", "result": "\u5bf9\u4e8en>2f\uff0c\u4e24\u79cd\u6a21\u578b\u5728\u65e0\u8272\u4efb\u52a1\u53ef\u89e3\u6027\u4e0a\u7b49\u4ef7\uff1b\u5bf9\u4e8e\u6709\u8272\u4efb\u52a1\uff0c\u4ec5\u5f53f=1\u65f6\u7b49\u4ef7\uff08n>2\uff09\uff0cf\u66f4\u5927\u65f6\u5b58\u5728\u5206\u79bb\u3002\u8fd9\u79cd\u5206\u79bb\u6e90\u4e8eHO_f\u4e2d\u9759\u9ed8\u8fdb\u7a0b\u53ef\u80fd\u5bfc\u81f4\u51b3\u7b56\u4e0d\u517c\u5bb9\u3002", "conclusion": "\u7ed3\u679c\u7cbe\u786e\u754c\u5b9a\u4e86\u57fa\u4e8e\u8f6e\u6b21\u7684\u62bd\u8c61\u80fd\u591f\u6355\u83b7\u5f02\u6b65\u8ba1\u7b97\u7684\u8303\u56f4\uff0c\u8868\u660e\u89c4\u8303\u8f6e\u6b21\u7684\u8868\u8fbe\u80fd\u529b\u9650\u5236\u662f\u7ed3\u6784\u6027\u7684\u800c\u975e\u6982\u7387\u6027\u7684\uff0c\u4e3a\u5206\u5e03\u5f0f\u8ba1\u7b97\u6a21\u578b\u7684\u7406\u8bba\u7406\u89e3\u63d0\u4f9b\u4e86\u91cd\u8981\u6d1e\u89c1\u3002"}}
{"id": "2511.21910", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21910", "abs": "https://arxiv.org/abs/2511.21910", "authors": ["Haoxuan Shan", "Cong Guo", "Chiyue Wei", "Feng Cheng", "Junyao Zhang", "Hai", "Li", "Yiran Chen"], "title": "Platinum: Path-Adaptable LUT-Based Accelerator Tailored for Low-Bit Weight Matrix Multiplication", "comment": null, "summary": "The rapid scaling of large language models demands more efficient hardware. Quantization offers a promising trade-off between efficiency and performance. With ultra-low-bit quantization, there are abundant opportunities for results reuse, and thus it can be boosted with lookup tables (LUTs) based acceleration. However, existing LUT-based methods suffer from computation and hardware overheads for LUT construction, and rely solely on bit-serial computation, which is suboptimal for ternary-weight networks. We propose Platinum, a lightweight ASIC accelerator for integer weight mixed-precision matrix multiplication (mpGEMM) using LUTs. Platinum reduces LUT construction overhead via offline-generated construction paths and supports both general bit-serial and optimized ternary-weight execution through adaptive path switching. On BitNet b1.58-3B, Platinum achieves up to 73.6x, 4.09x, and 2.15x speedups over SpikingEyeriss, Prosperity, and 16-thread T-MAC (CPU), respectively, along with energy reductions of 32.4x, 3.23x, and 20.9x, all within a 0.96mm2 chip area. This demonstrates the potential of LUT-based ASICs as efficient, scalable solutions for ultra-low-bit neural networks on edge platforms.", "AI": {"tldr": "Platinum\uff1a\u4e00\u79cd\u57fa\u4e8e\u67e5\u627e\u8868\u7684\u8f7b\u91cf\u7ea7ASIC\u52a0\u901f\u5668\uff0c\u7528\u4e8e\u6574\u6570\u6743\u91cd\u6df7\u5408\u7cbe\u5ea6\u77e9\u9635\u4e58\u6cd5\uff0c\u901a\u8fc7\u79bb\u7ebf\u751f\u6210\u6784\u9020\u8def\u5f84\u51cf\u5c11\u5f00\u9500\uff0c\u652f\u6301\u6bd4\u7279\u4e32\u884c\u548c\u4e09\u5143\u6743\u91cd\u81ea\u9002\u5e94\u5207\u6362\uff0c\u5728BitNet b1.58-3B\u4e0a\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u548c\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u6269\u5c55\u9700\u8981\u66f4\u9ad8\u6548\u786c\u4ef6\uff0c\u8d85\u4f4e\u4f4d\u91cf\u5316\u63d0\u4f9b\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u5e73\u8861\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u67e5\u627e\u8868\u7684\u65b9\u6cd5\u5b58\u5728\u6784\u9020\u5f00\u9500\u5927\u3001\u4f9d\u8d56\u6bd4\u7279\u4e32\u884c\u8ba1\u7b97\u7b49\u95ee\u9898\uff0c\u5bf9\u4e09\u5143\u6743\u91cd\u7f51\u7edc\u4e0d\u591f\u4f18\u5316\u3002", "method": "\u63d0\u51faPlatinum\u52a0\u901f\u5668\uff0c\u91c7\u7528\u79bb\u7ebf\u751f\u6210\u7684\u6784\u9020\u8def\u5f84\u51cf\u5c11\u67e5\u627e\u8868\u6784\u5efa\u5f00\u9500\uff0c\u652f\u6301\u81ea\u9002\u5e94\u8def\u5f84\u5207\u6362\u5b9e\u73b0\u901a\u7528\u6bd4\u7279\u4e32\u884c\u548c\u4f18\u5316\u7684\u4e09\u5143\u6743\u91cd\u6267\u884c\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7ASIC\u67b6\u6784\u3002", "result": "\u5728BitNet b1.58-3B\u4e0a\uff0c\u76f8\u6bd4SpikingEyeriss\u3001Prosperity\u548c16\u7ebf\u7a0bT-MAC\uff08CPU\uff09\u5206\u522b\u5b9e\u73b073.6\u500d\u30014.09\u500d\u548c2.15\u500d\u52a0\u901f\uff0c\u80fd\u8017\u964d\u4f4e32.4\u500d\u30013.23\u500d\u548c20.9\u500d\uff0c\u82af\u7247\u9762\u79ef\u4ec50.96mm\u00b2\u3002", "conclusion": "Platinum\u5c55\u793a\u4e86\u57fa\u4e8e\u67e5\u627e\u8868\u7684ASIC\u4f5c\u4e3a\u8d85\u4f4e\u4f4d\u795e\u7ecf\u7f51\u7edc\u5728\u8fb9\u7f18\u5e73\u53f0\u4e0a\u7684\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u786c\u4ef6\u4f18\u5316\u8def\u5f84\u3002"}}
{"id": "2511.23358", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.23358", "abs": "https://arxiv.org/abs/2511.23358", "authors": ["Alexandre Moine", "Stephanie Balzer", "Alex Xu", "Sam Westrick"], "title": "TypeDis: A Type System for Disentanglement", "comment": "34 pages, 24 figures, extended version of the same paper accepted at POPL 2026", "summary": "Disentanglement is a runtime property of parallel programs guaranteeing that parallel tasks remain oblivious to each other's allocations. As demonstrated in the MaPLe compiler and run-time system, disentanglement can be exploited for fast automatic memory management, especially task-local garbage collection with no synchronization between parallel tasks. However, as a low-level property, disentanglement can be difficult to reason about for programmers. The only means of statically verifying disentanglement so far has been DisLog, an Iris-fueled variant of separation logic, mechanized in the Rocq proof assistant. DisLog is a fully-featured program logic, allowing for proof of functional correctness as well as verification of disentanglement. Yet its employment requires significant expertise and per-program proof effort.\n  This paper explores the route of automatic verification via a type system, ensuring that any well-typed program is disentangled and lifting the burden of carrying out manual proofs from the programmer. It contributes TypeDis, a type system inspired by region types, where each type is annotated with a timestamp, identifying the task that allocated it. TypeDis supports iso-recursive types as well as polymorphism over both types and timestamps. Crucially, timestamps are allowed to change during type-checking, at join points as well as via a form of subtyping, dubbed subtiming. The paper illustrates TypeDis and its features on a range of examples. The soundness of TypeDis and the examples are mechanized in the Rocq proof assistant, using an improved version of DisLog, dubbed DisLog2.", "AI": {"tldr": "TypeDis\uff1a\u57fa\u4e8e\u65f6\u95f4\u6233\u7c7b\u578b\u7cfb\u7edf\u81ea\u52a8\u9a8c\u8bc1\u5e76\u884c\u7a0b\u5e8f\u89e3\u8026\u6027\uff0c\u65e0\u9700\u624b\u52a8\u8bc1\u660e", "motivation": "\u89e3\u8026\u6027\u662f\u5e76\u884c\u7a0b\u5e8f\u7684\u91cd\u8981\u8fd0\u884c\u65f6\u5c5e\u6027\uff0c\u80fd\u5b9e\u73b0\u9ad8\u6548\u7684\u65e0\u540c\u6b65\u4efb\u52a1\u672c\u5730\u5783\u573e\u56de\u6536\u3002\u4f46\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5DisLog\u9700\u8981\u624b\u52a8\u8bc1\u660e\uff0c\u5bf9\u7a0b\u5e8f\u5458\u8981\u6c42\u9ad8\u4e14\u5de5\u4f5c\u91cf\u5927\uff0c\u9700\u8981\u66f4\u81ea\u52a8\u5316\u7684\u9a8c\u8bc1\u65b9\u6848\u3002", "method": "\u63d0\u51faTypeDis\u7c7b\u578b\u7cfb\u7edf\uff0c\u53d7\u533a\u57df\u7c7b\u578b\u542f\u53d1\uff0c\u6bcf\u4e2a\u7c7b\u578b\u90fd\u6807\u6ce8\u65f6\u95f4\u6233\u6807\u8bc6\u5206\u914d\u4efb\u52a1\u3002\u652f\u6301\u540c\u6784\u9012\u5f52\u7c7b\u578b\u3001\u7c7b\u578b\u548c\u65f6\u95f4\u6233\u591a\u6001\u6027\u3002\u5173\u952e\u521b\u65b0\u662f\u5141\u8bb8\u65f6\u95f4\u6233\u5728\u7c7b\u578b\u68c0\u67e5\u65f6\u53d8\u5316\uff0c\u5305\u62ec\u5728\u6c47\u5408\u70b9\u901a\u8fc7\"\u5b50\u65f6\u95f4\"\u5b50\u7c7b\u578b\u673a\u5236\u3002", "result": "TypeDis\u80fd\u81ea\u52a8\u9a8c\u8bc1\u4efb\u4f55\u826f\u7c7b\u578b\u7a0b\u5e8f\u90fd\u662f\u89e3\u8026\u7684\uff0c\u51cf\u8f7b\u4e86\u7a0b\u5e8f\u5458\u624b\u52a8\u8bc1\u660e\u7684\u8d1f\u62c5\u3002\u7cfb\u7edf\u5728Rocq\u8bc1\u660e\u52a9\u624b\u4e2d\u901a\u8fc7\u6539\u8fdb\u7684DisLog2\u8fdb\u884c\u4e86\u673a\u68b0\u5316\u9a8c\u8bc1\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u4e2a\u793a\u4f8b\u3002", "conclusion": "TypeDis\u4e3a\u5e76\u884c\u7a0b\u5e8f\u89e3\u8026\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u81ea\u52a8\u9a8c\u8bc1\u65b9\u6848\uff0c\u901a\u8fc7\u7c7b\u578b\u7cfb\u7edf\u66ff\u4ee3\u590d\u6742\u7684\u624b\u52a8\u8bc1\u660e\uff0c\u4f7f\u89e3\u8026\u6027\u9a8c\u8bc1\u66f4\u6613\u4e8e\u5e94\u7528\u3002"}}
{"id": "2511.21862", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21862", "abs": "https://arxiv.org/abs/2511.21862", "authors": ["Siyu Wu", "Zihan Tang", "Yuting Zeng", "Hui Chen", "Guiguang Ding", "Tongxuan Liu", "Ke Zhang", "Hailong Yang"], "title": "OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in both latency-sensitive online services and cost-sensitive offline workloads. Co-locating these workloads on shared serving instances can improve resource utilization, but directly applying this approach to Prefill/Decode (P/D) disaggregated systems introduces severe load imbalance, as fluctuating request mixes alter the intrinsic P/D ratio. Existing dynamic adjustment techniques cannot keep up with the bursty traffic patterns of online services.\n  We propose a latency-constraint disaggregated architecture, which separates cluster resources into latency-strict and latency-relaxed pools based on task latency requirements. This design enables flexible placement of offline decode tasks, mitigating P/D imbalance while preserving online performance. To fully exploit this flexibility, we propose (1) a bottleneck-based scheduler guided by a Roofline-based performance model for performance bottleneck based scheduling, and (2) a fast preemption mechanism that strictly enforces Service Level Objectives (SLOs) for online requests.\n  Experiments on real-world traces show that compared to existing offline system approaches, our method improves offline throughput by up to 3x, while maintaining online request SLOs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5ef6\u8fdf\u7ea6\u675f\u7684\u89e3\u8026\u67b6\u6784\uff0c\u5c06\u96c6\u7fa4\u8d44\u6e90\u5206\u4e3a\u5ef6\u8fdf\u4e25\u683c\u548c\u5ef6\u8fdf\u5bbd\u677e\u4e24\u4e2a\u6c60\uff0c\u901a\u8fc7\u74f6\u9888\u8c03\u5ea6\u5668\u548c\u5feb\u901f\u62a2\u5360\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u5728\u7ebf\u670d\u52a1SLO\u7684\u540c\u65f6\uff0c\u5c06\u79bb\u7ebf\u541e\u5410\u91cf\u63d0\u5347\u81f33\u500d\u3002", "motivation": "LLM\u5728\u5728\u7ebf\u670d\u52a1\u548c\u79bb\u7ebf\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u90e8\u7f72\u65e5\u76ca\u589e\u591a\uff0c\u5c06\u4e24\u8005\u5171\u7f6e\u5728\u5171\u4eab\u5b9e\u4f8b\u4e0a\u53ef\u4ee5\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4f46\u5728Prefill/Decode\u89e3\u8026\u7cfb\u7edf\u4e2d\uff0c\u8bf7\u6c42\u7ec4\u5408\u7684\u6ce2\u52a8\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\uff0c\u73b0\u6709\u52a8\u6001\u8c03\u6574\u6280\u672f\u65e0\u6cd5\u5e94\u5bf9\u5728\u7ebf\u670d\u52a1\u7684\u7a81\u53d1\u6d41\u91cf\u6a21\u5f0f\u3002", "method": "1) \u5ef6\u8fdf\u7ea6\u675f\u89e3\u8026\u67b6\u6784\uff1a\u6839\u636e\u4efb\u52a1\u5ef6\u8fdf\u8981\u6c42\u5c06\u96c6\u7fa4\u8d44\u6e90\u5206\u4e3a\u5ef6\u8fdf\u4e25\u683c\u548c\u5ef6\u8fdf\u5bbd\u677e\u4e24\u4e2a\u6c60\uff1b2) \u57fa\u4e8e\u74f6\u9888\u7684\u8c03\u5ea6\u5668\uff1a\u4f7f\u7528Roofline\u6027\u80fd\u6a21\u578b\u6307\u5bfc\u74f6\u9888\u611f\u77e5\u8c03\u5ea6\uff1b3) \u5feb\u901f\u62a2\u5360\u673a\u5236\uff1a\u4e25\u683c\u5f3a\u5236\u6267\u884c\u5728\u7ebf\u8bf7\u6c42\u7684SLO\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754ctrace\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u79bb\u7ebf\u7cfb\u7edf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5728\u7ebf\u8bf7\u6c42SLO\u7684\u540c\u65f6\uff0c\u5c06\u79bb\u7ebf\u541e\u5410\u91cf\u63d0\u5347\u81f33\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684\u5ef6\u8fdf\u7ea6\u675f\u89e3\u8026\u67b6\u6784\u548c\u8c03\u5ea6\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86Prefill/Decode\u89e3\u8026\u7cfb\u7edf\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u5728\u7ebf\u670d\u52a1\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u5de5\u4f5c\u8d1f\u8f7d\u7684\u541e\u5410\u91cf\u3002"}}
{"id": "2511.22166", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.22166", "abs": "https://arxiv.org/abs/2511.22166", "authors": ["Shuai Dong", "Junyi Yang", "Ye Ke", "Hongyang Shang", "Arindam Basu"], "title": "CADC: Crossbar-Aware Dendritic Convolution for Efficient In-memory Computing", "comment": null, "summary": "Convolutional neural networks (CNNs) are computationally intensive and often accelerated using crossbar-based in-memory computing (IMC) architectures. However, large convolutional layers must be partitioned across multiple crossbars, generating numerous partial sums (psums) that require additional buffer, transfer, and accumulation, thus introducing significant system-level overhead. Inspired by dendritic computing principles from neuroscience, we propose crossbar-aware dendritic convolution (CADC), a novel approach that dramatically increases sparsity in psums by embedding a nonlinear dendritic function (zeroing negative values) directly within crossbar computations. Experimental results demonstrate that CADC significantly reduces psums, eliminating 80% in LeNet-5 on MNIST, 54% in ResNet-18 on CIFAR-10, 66% in VGG-16 on CIFAR-100, and up to 88% in spiking neural networks (SNN) on the DVS Gesture dataset. The induced sparsity from CADC provides two key benefits: (1) enabling zero-compression and zero-skipping, thus reducing buffer and transfer overhead by 29.3% and accumulation overhead by 47.9%; (2) minimizing ADC quantization noise accumulation, resulting in small accuracy degradation - only 0.01% for LeNet-5, 0.1% for ResNet-18, 0.5% for VGG-16, and 0.9% for SNN. Compared to vanilla convolution (vConv), CADC exhibits accuracy changes ranging from +0.11% to +0.19% for LeNet-5, -0.04% to -0.27% for ResNet-18, +0.99% to +1.60% for VGG-16, and -0.57% to +1.32% for SNN, across crossbar sizes from 64x64 to 256x256. Ultimately, a SRAM-based IMC implementation of CADC achieves 2.15 TOPS and 40.8 TOPS/W for ResNet-18 (4/2/4b), realizing an 11x-18x speedup and 1.9x-22.9x improvement in energy efficiency compared to existing IMC accelerators.", "AI": {"tldr": "\u63d0\u51faCADC\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4ea4\u53c9\u9635\u5217\u8ba1\u7b97\u4e2d\u5d4c\u5165\u975e\u7ebf\u6027\u6811\u7a81\u51fd\u6570\uff08\u5c06\u8d1f\u503c\u5f52\u96f6\uff09\u6765\u5927\u5e45\u589e\u52a0\u90e8\u5206\u548c\u7684\u7a00\u758f\u6027\uff0c\u51cf\u5c11\u7cfb\u7edf\u5f00\u9500\u5e76\u63d0\u5347\u80fd\u6548", "motivation": "CNN\u5728\u4ea4\u53c9\u9635\u5217\u5185\u5b58\u8ba1\u7b97\u67b6\u6784\u4e2d\u52a0\u901f\u65f6\uff0c\u5927\u5377\u79ef\u5c42\u9700\u8981\u8de8\u591a\u4e2a\u4ea4\u53c9\u9635\u5217\u5206\u533a\uff0c\u4ea7\u751f\u5927\u91cf\u90e8\u5206\u548c\uff0c\u5bfc\u81f4\u989d\u5916\u7684\u7f13\u51b2\u3001\u4f20\u8f93\u548c\u7d2f\u52a0\u5f00\u9500\uff0c\u5f15\u5165\u663e\u8457\u7684\u7cfb\u7edf\u7ea7\u5f00\u9500", "method": "\u63d0\u51fa\u4ea4\u53c9\u9635\u5217\u611f\u77e5\u7684\u6811\u7a81\u5377\u79ef\uff08CADC\uff09\uff0c\u53d7\u795e\u7ecf\u79d1\u5b66\u4e2d\u6811\u7a81\u8ba1\u7b97\u539f\u7406\u542f\u53d1\uff0c\u5728\u4ea4\u53c9\u9635\u5217\u8ba1\u7b97\u4e2d\u76f4\u63a5\u5d4c\u5165\u975e\u7ebf\u6027\u6811\u7a81\u51fd\u6570\uff08\u5c06\u8d1f\u503c\u5f52\u96f6\uff09\uff0c\u4ece\u800c\u5927\u5e45\u589e\u52a0\u90e8\u5206\u548c\u7684\u7a00\u758f\u6027", "result": "CADC\u663e\u8457\u51cf\u5c11\u90e8\u5206\u548c\uff1aLeNet-5\u51cf\u5c1180%\uff0cResNet-18\u51cf\u5c1154%\uff0cVGG-16\u51cf\u5c1166%\uff0cSNN\u51cf\u5c1188%\u3002\u5b9e\u73b0\u96f6\u538b\u7f29\u548c\u96f6\u8df3\u8fc7\uff0c\u51cf\u5c1129.3%\u7684\u7f13\u51b2\u4f20\u8f93\u5f00\u9500\u548c47.9%\u7684\u7d2f\u52a0\u5f00\u9500\uff0c\u540c\u65f6\u6700\u5c0f\u5316ADC\u91cf\u5316\u566a\u58f0\u79ef\u7d2f\uff0c\u7cbe\u5ea6\u635f\u5931\u5f88\u5c0f", "conclusion": "CADC\u5728SRAM-based IMC\u5b9e\u73b0\u4e2d\u8fbe\u52302.15 TOPS\u548c40.8 TOPS/W\u7684\u80fd\u6548\uff0c\u76f8\u6bd4\u73b0\u6709IMC\u52a0\u901f\u5668\u5b9e\u73b011-18\u500d\u52a0\u901f\u548c1.9-22.9\u500d\u7684\u80fd\u6548\u63d0\u5347\uff0c\u4e3aCNN\u52a0\u901f\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.23472", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.23472", "abs": "https://arxiv.org/abs/2511.23472", "authors": ["Yusuke Matsushita", "Kengo Hirata", "Ryo Wakizaka", "Emanuele D'Osualdo"], "title": "RapunSL: Untangling Quantum Computing with Separation, Linear Combination and Mixing", "comment": "Full version of the conference paper at POPL 2026. The first two authors contributed equally to this work", "summary": "Quantum Separation Logic (QSL) has been proposed as an effective tool to improve the scalability of deductive reasoning for quantum programs. In QSL, separation is interpreted as disentanglement, and the frame rule brings a notion of entanglement-local specification (one that only talks about the qubits entangled with those acted upon by the program). In this paper, we identify two notions of locality unique to the quantum domain, and we construct a novel quantum separation logic, RapunSL, which is able to soundly reduce reasoning about superposition states to reasoning about pure states (basis-locality), and reasoning about mixed states arising from measurement to reasoning about pure states (outcome-locality). To do so, we introduce two connectives, linear combination and mixing, which together with separation provide a dramatic improvement in the scalability of reasoning, as we demonstrate on a series of challenging case studies.", "AI": {"tldr": "RapunSL\u662f\u4e00\u79cd\u65b0\u578b\u91cf\u5b50\u5206\u79bb\u903b\u8f91\uff0c\u901a\u8fc7\u5f15\u5165\u7ebf\u6027\u7ec4\u5408\u548c\u6df7\u5408\u8fde\u63a5\u8bcd\uff0c\u80fd\u591f\u5c06\u53e0\u52a0\u6001\u63a8\u7406\u7b80\u5316\u4e3a\u7eaf\u6001\u63a8\u7406\uff08\u57fa\u6001\u5c40\u90e8\u6027\uff09\uff0c\u5e76\u5c06\u6d4b\u91cf\u4ea7\u751f\u7684\u6df7\u5408\u6001\u63a8\u7406\u7b80\u5316\u4e3a\u7eaf\u6001\u63a8\u7406\uff08\u7ed3\u679c\u5c40\u90e8\u6027\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5b50\u7a0b\u5e8f\u63a8\u7406\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u91cf\u5b50\u5206\u79bb\u903b\u8f91(QSL)\u867d\u7136\u80fd\u63d0\u9ad8\u91cf\u5b50\u7a0b\u5e8f\u6f14\u7ece\u63a8\u7406\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u53e0\u52a0\u6001\u548c\u6d4b\u91cf\u4ea7\u751f\u7684\u6df7\u5408\u6001\u65f6\u4ecd\u6709\u5c40\u9650\u6027\u3002\u9700\u8981\u5f00\u53d1\u65b0\u7684\u903b\u8f91\u7cfb\u7edf\u6765\u66f4\u6709\u6548\u5730\u5904\u7406\u91cf\u5b50\u9886\u57df\u7279\u6709\u7684\u5c40\u90e8\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faRapunSL\u91cf\u5b50\u5206\u79bb\u903b\u8f91\uff0c\u5f15\u5165\u4e24\u4e2a\u65b0\u8fde\u63a5\u8bcd\uff1a\u7ebf\u6027\u7ec4\u5408\uff08\u5904\u7406\u53e0\u52a0\u6001\uff09\u548c\u6df7\u5408\uff08\u5904\u7406\u6d4b\u91cf\u4ea7\u751f\u7684\u6df7\u5408\u6001\uff09\uff0c\u7ed3\u5408\u5206\u79bb\u6982\u5ff5\uff0c\u5b9e\u73b0\u57fa\u6001\u5c40\u90e8\u6027\u548c\u7ed3\u679c\u5c40\u90e8\u6027\uff0c\u5c06\u590d\u6742\u91cf\u5b50\u6001\u63a8\u7406\u7b80\u5316\u4e3a\u7eaf\u6001\u63a8\u7406\u3002", "result": "RapunSL\u80fd\u591f\u5c06\u53e0\u52a0\u6001\u63a8\u7406\u7b80\u5316\u4e3a\u7eaf\u6001\u63a8\u7406\uff08\u57fa\u6001\u5c40\u90e8\u6027\uff09\uff0c\u5c06\u6d4b\u91cf\u4ea7\u751f\u7684\u6df7\u5408\u6001\u63a8\u7406\u7b80\u5316\u4e3a\u7eaf\u6001\u63a8\u7406\uff08\u7ed3\u679c\u5c40\u90e8\u6027\uff09\uff0c\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "RapunSL\u901a\u8fc7\u5f15\u5165\u7ebf\u6027\u7ec4\u5408\u548c\u6df7\u5408\u8fde\u63a5\u8bcd\uff0c\u89e3\u51b3\u4e86\u91cf\u5b50\u9886\u57df\u7279\u6709\u7684\u5c40\u90e8\u6027\u95ee\u9898\uff0c\u4e3a\u91cf\u5b50\u7a0b\u5e8f\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u548c\u53ef\u6269\u5c55\u7684\u903b\u8f91\u6846\u67b6\uff0c\u5728\u590d\u6742\u91cf\u5b50\u7a0b\u5e8f\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2511.21958", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21958", "abs": "https://arxiv.org/abs/2511.21958", "authors": ["Yiyan Zhai", "Bintang Dwi Marthen", "Sarath Balivada", "Vamsi Sudhakar Bojji", "Eric Knauft", "Jitender Rohilla", "Jiaqi Zuo", "Quanxing Liu", "Maxime Austruy", "Wenguang Wang", "Juncheng Yang"], "title": "Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN", "comment": "12 pages, 14 figures", "summary": "Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.", "AI": {"tldr": "Clock2Q+\u662f\u9488\u5bf9\u5143\u6570\u636e\u7f13\u5b58\u8bbe\u8ba1\u7684\u7f13\u5b58\u66ff\u6362\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u5c0fFIFO\u961f\u5217\u4e2d\u5f15\u5165\u76f8\u5173\u7a97\u53e3\u6765\u907f\u514d\u8bef\u5224\u70ed\u5757\uff0c\u5728\u5143\u6570\u636e\u8ddf\u8e2a\u4e2d\u6bd4S3-FIFO\u964d\u4f4e\u9ad8\u8fbe28.5%\u7684\u7f3a\u5931\u7387\u3002", "motivation": "\u5143\u6570\u636e\u7f13\u5b58\u5b58\u5728\u56fa\u6709\u7684\u76f8\u5173\u5f15\u7528\u7279\u6027\uff0c\u5373\u4f7f\u5bf9\u5e94\u7684\u6570\u636e\u8bbf\u95ee\u4e0d\u5305\u542b\u76f8\u5173\u5f15\u7528\u3002\u8fd9\u4e9b\u76f8\u5173\u5f15\u7528\u4f1a\u964d\u4f4e\u7f13\u5b58\u66ff\u6362\u7b97\u6cd5\u7684\u6548\u679c\uff0c\u56e0\u4e3a\u5b83\u4eec\u7ecf\u5e38\u88ab\u9519\u8bef\u5730\u5206\u7c7b\u4e3a\u70ed\u5757\u3002", "method": "Clock2Q+\u4f7f\u7528\u4e09\u4e2a\u961f\u5217\uff08\u7c7b\u4f3cS3-FIFO\uff09\uff0c\u4f46\u5728\u5c0fFIFO\u961f\u5217\u4e2d\u5f15\u5165\u4e86\u76f8\u5173\u7a97\u53e3\uff0c\u8be5\u7a97\u53e3\u5185\u7684\u5757\u4e0d\u8bbe\u7f6e\u5f15\u7528\u4f4d\u3002\u8fd9\u79cd\u7b80\u5355\u589e\u5f3a\u4f7f\u7b97\u6cd5\u80fd\u66f4\u597d\u5730\u533a\u5206\u771f\u6b63\u7684\u70ed\u5757\u548c\u76f8\u5173\u5f15\u7528\u3002", "result": "\u5728\u5143\u6570\u636e\u8ddf\u8e2a\u4e2d\uff0cClock2Q+\u6bd4\u7b2c\u4e8c\u4f73\u7b97\u6cd5S3-FIFO\u964d\u4f4e\u9ad8\u8fbe28.5%\u7684\u7f3a\u5931\u7387\u3002\u8be5\u7b97\u6cd5\u5df2\u5728VMware\u7684vSAN\u548cVDFS\u5b58\u50a8\u4ea7\u54c1\u4e2d\u5b9e\u73b0\uff0c\u5177\u6709\u4f4eCPU\u5f00\u9500\u3001\u4f4e\u5185\u5b58\u5f00\u9500\u3001\u591aCPU\u6269\u5c55\u6027\u597d\u3001\u6613\u4e8e\u8c03\u4f18\u548c\u5b9e\u73b0\u7b49\u4f18\u70b9\u3002", "conclusion": "Clock2Q+\u662f\u9488\u5bf9\u5143\u6570\u636e\u7f13\u5b58\u4f18\u5316\u7684\u9ad8\u6548\u7f13\u5b58\u66ff\u6362\u7b97\u6cd5\uff0c\u901a\u8fc7\u5904\u7406\u76f8\u5173\u5f15\u7528\u95ee\u9898\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u5177\u5907\u5927\u89c4\u6a21\u5b58\u50a8\u7cfb\u7edf\u6240\u9700\u7684\u5173\u952e\u7279\u6027\uff0c\u5728\u6570\u636e\u8ddf\u8e2a\u4e0a\u4e5f\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u7b97\u6cd5\u3002"}}
{"id": "2511.22267", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.22267", "abs": "https://arxiv.org/abs/2511.22267", "authors": ["Yuyang Zou", "Youwei Xiao", "Yansong Xu", "Chenyun Yin", "Yuhao Luo", "Yitian Sun", "Ruifan Xu", "Renze Chen", "Yun Liang"], "title": "Aquas: Enhancing Domain Specialization through Holistic Hardware-Software Co-Optimization based on MLIR", "comment": null, "summary": "Application-Specific Instruction-Set Processors (ASIPs) built on the RISC-V architecture offer specialization opportunities for various applications. However, existing frameworks from the open-source RISC-V ecosystem suffer from limited performance due to restricted hardware synthesis and rigid compiler support. To address these challenges, we introduce Aquas, a holistic hardware-software co-design framework built upon MLIR. Aquas enhances ASIP synthesis with fast memory access capability via a burst DMA engine and advanced high-level synthesis (HLS) optimizations. On the compiler side, we propose an e-graph based retargetable approach with a novel matching engine for efficient instruction matching. Evaluation demonstrates up to 9.27x speedup on real-world workloads, including point cloud processing and LLM inference.", "AI": {"tldr": "Aquas\u662f\u4e00\u4e2a\u57fa\u4e8eMLIR\u7684\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u7a81\u53d1DMA\u5f15\u64ce\u548c\u9ad8\u7ea7HLS\u4f18\u5316\u6765\u63d0\u5347RISC-V ASIP\u6027\u80fd\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8ee-graph\u7684\u53ef\u91cd\u5b9a\u5411\u7f16\u8bd1\u5668\u65b9\u6cd5\uff0c\u5728\u70b9\u4e91\u5904\u7406\u548cLLM\u63a8\u7406\u7b49\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u5b9e\u73b0\u6700\u9ad89.27\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90RISC-V\u751f\u6001\u7cfb\u7edf\u4e2d\u7684ASIP\u6846\u67b6\u5b58\u5728\u6027\u80fd\u53d7\u9650\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u786c\u4ef6\u5408\u6210\u80fd\u529b\u6709\u9650\u548c\u7f16\u8bd1\u5668\u652f\u6301\u50f5\u5316\uff0c\u8fd9\u963b\u788d\u4e86RISC-V ASIP\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u7684\u4e13\u4e1a\u5316\u6f5c\u529b\u3002", "method": "\u63d0\u51faAquas\u6846\u67b6\uff1a1\uff09\u786c\u4ef6\u65b9\u9762\uff1a\u901a\u8fc7\u7a81\u53d1DMA\u5f15\u64ce\u589e\u5f3a\u5feb\u901f\u5185\u5b58\u8bbf\u95ee\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u9ad8\u7ea7HLS\u4f18\u5316\uff1b2\uff09\u7f16\u8bd1\u5668\u65b9\u9762\uff1a\u63d0\u51fa\u57fa\u4e8ee-graph\u7684\u53ef\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u914d\u5907\u65b0\u578b\u5339\u914d\u5f15\u64ce\u4ee5\u5b9e\u73b0\u9ad8\u6548\u6307\u4ee4\u5339\u914d\u3002\u6574\u4e2a\u6846\u67b6\u57fa\u4e8eMLIR\u6784\u5efa\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u8bc4\u4f30\u4e2d\uff0c\u5305\u62ec\u70b9\u4e91\u5904\u7406\u548cLLM\u63a8\u7406\uff0cAquas\u6846\u67b6\u5b9e\u73b0\u4e86\u6700\u9ad89.27\u500d\u7684\u6027\u80fd\u52a0\u901f\u3002", "conclusion": "Aquas\u901a\u8fc7\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709RISC-V ASIP\u6846\u67b6\u7684\u6027\u80fd\u9650\u5236\u95ee\u9898\uff0c\u4e3a\u5e94\u7528\u7279\u5b9a\u5904\u7406\u5668\u5f00\u53d1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21969", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.21969", "abs": "https://arxiv.org/abs/2511.21969", "authors": ["Matteo Bjornsson", "Taylor Hardin", "Taylor Heinecke", "Marcin Furtak", "David L. Millman", "Mike P. Wittie"], "title": "ZipperChain: Transmuting Trusted Third-Party Services Into Trustless Atomic Broadcast", "comment": null, "summary": "Distributed ledger technologies (DLTs) rely on distributed consensus mechanisms to reach agreement over the order of transactions and to provide immutability and availability of transaction data. Distributed consensus suffers from performance limitations of network communication between participating nodes. BLOCKY ZipperChain guarantees immutability, agreement, and availability of transaction data, but without relying on distributed consensus. Instead, its construction process transfers trust from widely-used, third-party services onto ZipperChains's correctness guarantees. ZipperChain blocks are built by a pipeline of specialized services deployed on a small number of nodes connected by a fast data center network. As a result, ZipperChain transaction throughput approaches network line speeds and block finality is on the order of 500 ms. Finally, ZipperChain infrastructure creates blocks centrally and so does not need a native token to incentivize a community of verifiers.", "AI": {"tldr": "ZipperChain\u662f\u4e00\u79cd\u65e0\u9700\u5206\u5e03\u5f0f\u5171\u8bc6\u7684\u5206\u5e03\u5f0f\u8d26\u672c\u6280\u672f\uff0c\u901a\u8fc7\u4e13\u7528\u670d\u52a1\u6d41\u6c34\u7ebf\u5728\u5c11\u6570\u8282\u70b9\u4e0a\u6784\u5efa\u533a\u5757\uff0c\u5b9e\u73b0\u63a5\u8fd1\u7f51\u7edc\u7ebf\u8def\u901f\u5ea6\u7684\u4ea4\u6613\u541e\u5410\u91cf\u548c500\u6beb\u79d2\u7ea7\u7684\u6700\u7ec8\u786e\u8ba4\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf\u5206\u5e03\u5f0f\u5171\u8bc6\u673a\u5236\u56e0\u8282\u70b9\u95f4\u7f51\u7edc\u901a\u4fe1\u5b58\u5728\u6027\u80fd\u9650\u5236\uff0cZipperChain\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u65e0\u9700\u5206\u5e03\u5f0f\u5171\u8bc6\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e13\u7528\u670d\u52a1\u6d41\u6c34\u7ebf\u67b6\u6784\uff0c\u5728\u5c11\u91cf\u8282\u70b9\u4e0a\u90e8\u7f72\uff0c\u5229\u7528\u5feb\u901f\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\uff0c\u5c06\u4fe1\u4efb\u4ece\u5e7f\u6cdb\u4f7f\u7528\u7684\u7b2c\u4e09\u65b9\u670d\u52a1\u8f6c\u79fb\u5230ZipperChain\u7684\u6b63\u786e\u6027\u4fdd\u8bc1\u4e0a\u3002", "result": "\u4ea4\u6613\u541e\u5410\u91cf\u63a5\u8fd1\u7f51\u7edc\u7ebf\u8def\u901f\u5ea6\uff0c\u533a\u5757\u6700\u7ec8\u786e\u8ba4\u65f6\u95f4\u7ea6500\u6beb\u79d2\uff0c\u4e14\u65e0\u9700\u539f\u751f\u4ee3\u5e01\u6fc0\u52b1\u9a8c\u8bc1\u8005\u793e\u533a\u3002", "conclusion": "ZipperChain\u901a\u8fc7\u4e2d\u5fc3\u5316\u533a\u5757\u521b\u5efa\u67b6\u6784\uff0c\u5728\u4fdd\u8bc1\u4ea4\u6613\u6570\u636e\u4e0d\u53ef\u7be1\u6539\u3001\u4e00\u81f4\u6027\u548c\u53ef\u7528\u6027\u7684\u540c\u65f6\uff0c\u907f\u514d\u4e86\u5206\u5e03\u5f0f\u5171\u8bc6\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u5206\u5e03\u5f0f\u8d26\u672c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22348", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.22348", "abs": "https://arxiv.org/abs/2511.22348", "authors": ["Shuao Jia", "Zichao Ling", "Chen Bai", "Kang Zhao", "Jianwang Zhai"], "title": "FADiff: Fusion-Aware Differentiable Optimization for DNN Scheduling on Tensor Accelerators", "comment": "7 pages, 4 figures", "summary": "Efficient deployment of Deep Neural Networks (DNNs), such as Large Language Models (LLMs), on tensor accelerators is essential for maximizing computational efficiency in modern AI systems. However, achieving this is challenging due to the enormous and complex design space created by the interaction of intra-layer mapping and inter-layer fusion. In this work, we present FADiff, a gradient-based optimization framework capable of automatically identifying high-quality intra-layer mapping and inter-layer fusion strategies to accelerate inference for DNN workloads. We first construct a unified and differentiable analytical cost model, which accurately predicts the energy and latency of both single-layer mappings and various layer fusion strategies. Then, by encoding discrete constraints into the loss function, we employ a gradient-based approach to efficiently explore the vast design space, determining the optimal joint strategy for mapping and fusion. Experimental results demonstrate the superiority of FADiff, achieving better optimization in terms of energy and latency compared to existing methods.", "AI": {"tldr": "FADiff\u662f\u4e00\u4e2a\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5bfb\u627eDNN\u5728\u5f20\u91cf\u52a0\u901f\u5668\u4e0a\u7684\u6700\u4f18\u5c42\u5185\u6620\u5c04\u548c\u5c42\u95f4\u878d\u5408\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5728\u5f20\u91cf\u52a0\u901f\u5668\u4e0a\u9ad8\u6548\u90e8\u7f72DNN\uff08\u5982LLM\uff09\u5bf9\u6700\u5927\u5316\u8ba1\u7b97\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5c42\u5185\u6620\u5c04\u548c\u5c42\u95f4\u878d\u5408\u7684\u590d\u6742\u4ea4\u4e92\u5f62\u6210\u4e86\u5de8\u5927\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u9996\u5148\u6784\u5efa\u7edf\u4e00\u4e14\u53ef\u5fae\u7684\u5206\u6790\u6210\u672c\u6a21\u578b\uff0c\u51c6\u786e\u9884\u6d4b\u5355\u5c42\u6620\u5c04\u548c\u5404\u79cd\u5c42\u878d\u5408\u7b56\u7565\u7684\u80fd\u8017\u548c\u5ef6\u8fdf\uff1b\u7136\u540e\u5c06\u79bb\u6563\u7ea6\u675f\u7f16\u7801\u5230\u635f\u5931\u51fd\u6570\u4e2d\uff0c\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\u9ad8\u6548\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u786e\u5b9a\u6620\u5c04\u548c\u878d\u5408\u7684\u6700\u4f18\u8054\u5408\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eFADiff\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u80fd\u8017\u548c\u5ef6\u8fdf\u65b9\u9762\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4f18\u5316\u6548\u679c\u3002", "conclusion": "FADiff\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3DNN\u5728\u5f20\u91cf\u52a0\u901f\u5668\u4e0a\u7684\u90e8\u7f72\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u68af\u5ea6\u65b9\u6cd5\u81ea\u52a8\u5bfb\u627e\u6700\u4f18\u7684\u6620\u5c04\u548c\u878d\u5408\u7b56\u7565\u3002"}}
{"id": "2511.22010", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.22010", "abs": "https://arxiv.org/abs/2511.22010", "authors": ["Provakar Mondal", "Eli Tilevich"], "title": "An Empirical Study of Cross-Language Interoperability in Replicated Data Systems", "comment": null, "summary": "BACKGROUND: Modern distributed systems replicate data across multiple execution sites. Business requirements and resource constraints often necessitate mixing different languages across replica sites. To facilitate the management of replicated data, modern software engineering practices integrate special-purpose replicated data libraries (RDLs) that provide read-write access to the data and ensure its synchronization. Irrespective of the implementation languages, an RDL typically uses a single language or offers bindings to a designated one. Hence, integrating existing RDLs in multilingual environments requires special-purpose code, whose software quality and performance characteristics are poorly understood.\n  AIMS: We aim to bridge this knowledge gap to understand the software quality and performance characteristics of RDL integration in multilingual environments.\n  METHOD: We conduct an empirical study of two key strategies for integrating RDLs in the context of multilingual replicated data systems: foreign-function interface (FFI) and a common data format (CDF); we measure and compare their respective software metrics and performance to understand their suitability for the task at hand.\n  RESULTS: Our results reveal that adopting CDF for cross-language interaction offers software quality, latency, memory consumption, and throughput advantages. We further validate our findings by (1) creating a CDF-based RDL for mixing compiled, interpreted, and managed languages; and (2) enhancing our RDL with plug-in extensibility that enables adding functionality in a single language while maintaining integration within a multilingual environment.\n  CONCLUSIONS: With modern distributed systems utilizing multiple languages, our findings provide novel insights for designing RDLs in multilingual replicated data systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u8bed\u8a00\u590d\u5236\u6570\u636e\u7cfb\u7edf\u4e2d\u96c6\u6210\u590d\u5236\u6570\u636e\u5e93(RDL)\u7684\u4e24\u79cd\u7b56\u7565\uff1a\u5916\u90e8\u51fd\u6570\u63a5\u53e3(FFI)\u548c\u901a\u7528\u6570\u636e\u683c\u5f0f(CDF)\u3002\u7814\u7a76\u53d1\u73b0CDF\u5728\u8f6f\u4ef6\u8d28\u91cf\u3001\u5ef6\u8fdf\u3001\u5185\u5b58\u6d88\u8017\u548c\u541e\u5410\u91cf\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "motivation": "\u73b0\u4ee3\u5206\u5e03\u5f0f\u7cfb\u7edf\u9700\u8981\u5728\u591a\u4e2a\u6267\u884c\u7ad9\u70b9\u590d\u5236\u6570\u636e\uff0c\u4e1a\u52a1\u9700\u6c42\u548c\u8d44\u6e90\u7ea6\u675f\u5e38\u5bfc\u81f4\u4e0d\u540c\u7ad9\u70b9\u4f7f\u7528\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u3002\u73b0\u6709\u7684\u590d\u5236\u6570\u636e\u5e93(RDL)\u901a\u5e38\u53ea\u652f\u6301\u5355\u4e00\u8bed\u8a00\u6216\u7279\u5b9a\u7ed1\u5b9a\uff0c\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u96c6\u6210RDL\u9700\u8981\u4e13\u95e8\u7684\u4ee3\u7801\uff0c\u4f46\u8fd9\u4e9b\u4ee3\u7801\u7684\u8f6f\u4ef6\u8d28\u91cf\u548c\u6027\u80fd\u7279\u6027\u7f3a\u4e4f\u6df1\u5165\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u6bd4\u8f83\u4e24\u79cdRDL\u96c6\u6210\u7b56\u7565\uff1a\u5916\u90e8\u51fd\u6570\u63a5\u53e3(FFI)\u548c\u901a\u7528\u6570\u636e\u683c\u5f0f(CDF)\u3002\u6d4b\u91cf\u5e76\u6bd4\u8f83\u5b83\u4eec\u7684\u8f6f\u4ef6\u6307\u6807\u548c\u6027\u80fd\uff0c\u8bc4\u4f30\u5b83\u4eec\u5bf9\u591a\u8bed\u8a00\u590d\u5236\u6570\u636e\u7cfb\u7edf\u7684\u9002\u7528\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u91c7\u7528CDF\u8fdb\u884c\u8de8\u8bed\u8a00\u4ea4\u4e92\u5728\u8f6f\u4ef6\u8d28\u91cf\u3001\u5ef6\u8fdf\u3001\u5185\u5b58\u6d88\u8017\u548c\u541e\u5410\u91cf\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002\u7814\u7a76\u8fd8\u9a8c\u8bc1\u4e86\uff1a(1)\u521b\u5efa\u57fa\u4e8eCDF\u7684RDL\u652f\u6301\u6df7\u5408\u7f16\u8bd1\u3001\u89e3\u91ca\u548c\u6258\u7ba1\u8bed\u8a00\uff1b(2)\u901a\u8fc7\u63d2\u4ef6\u6269\u5c55\u6027\u589e\u5f3aRDL\uff0c\u5141\u8bb8\u5728\u5355\u4e00\u8bed\u8a00\u4e2d\u6dfb\u52a0\u529f\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u96c6\u6210\u3002", "conclusion": "\u968f\u7740\u73b0\u4ee3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4f7f\u7528\u591a\u79cd\u8bed\u8a00\uff0c\u672c\u7814\u7a76\u4e3a\u8bbe\u8ba1\u591a\u8bed\u8a00\u590d\u5236\u6570\u636e\u7cfb\u7edf\u4e2d\u7684RDL\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u8868\u660eCDF\u7b56\u7565\u5728\u8f6f\u4ef6\u8d28\u91cf\u548c\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684FFI\u65b9\u6cd5\u3002"}}
{"id": "2511.22551", "categories": ["cs.AR", "cs.ET", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.22551", "abs": "https://arxiv.org/abs/2511.22551", "authors": ["Elham Cheshmikhani", "Hamed Farbeh", "Hossein Asad"], "title": "3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison", "comment": null, "summary": "Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead.", "AI": {"tldr": "\u63d0\u51fa3RSeT\u65b9\u6848\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6807\u7b7e\u6bd4\u8f83\u51cf\u5c11STT-MRAM\u7f13\u5b58\u4e2d\u7684\u8bfb\u53d6\u5e72\u6270\u9519\u8bef\u7387\uff0c\u964d\u4f4e71.8%\u9519\u8bef\u7387\uff0c\u63d0\u53473.6\u500dMTTF\uff0c\u51cf\u5c1162.1%\u80fd\u8017\uff0c\u6027\u80fd\u65e0\u635f\u4e14\u9762\u79ef\u5f00\u9500\u5c0f\u4e8e0.4%\u3002", "motivation": "STT-MRAM\u4f5c\u4e3aSRAM\u66ff\u4ee3\u54c1\u5177\u6709\u4f4e\u529f\u8017\u3001\u9ad8\u5bc6\u5ea6\u7b49\u4f18\u52bf\uff0c\u4f46\u8bfb\u53d6\u5e72\u6270\u9519\u8bef\u662f\u4e25\u91cd\u53ef\u9760\u6027\u6311\u6218\u3002\u73b0\u6709\u5de5\u4f5c\u672a\u89e3\u51b3\u7f13\u5b58\u96c6\u5408\u4e2d\u5e76\u884c\u6bd4\u8f83\u64cd\u4f5c\u5bfc\u81f4\u7684\u6240\u6709\u6807\u7b7e\u540c\u65f6\u8bbf\u95ee\u95ee\u9898\uff0c\u8fd9\u4f1a\u6781\u5927\u589e\u52a0\u8bfb\u53d6\u5e72\u6270\u7387\u3002", "method": "\u63d0\u51fa3RSeT\u65b9\u6848\uff1a1\uff09\u4e3b\u52a8\u7981\u7528\u65e0\u547d\u4e2d\u673a\u4f1a\u7684\u6807\u7b7e\uff1b2\uff09\u5229\u7528\u8bbf\u95ee\u8bf7\u6c42\u4e2d\u6807\u7b7e\u7684\u4f4e\u6709\u6548\u4f4d\u8fdb\u884c\u9009\u62e9\u6027\u6bd4\u8f83\uff1b3\uff09\u6d88\u9664\u5927\u90e8\u5206\u6807\u7b7e\u8bfb\u53d6\u64cd\u4f5c\u3002", "result": "\u4f7f\u7528gem5\u5168\u7cfb\u7edf\u5468\u671f\u7cbe\u786e\u6a21\u62df\u5668\u8bc4\u4f30\uff1a\u6807\u7b7e\u9635\u5217\u8bfb\u53d6\u5e72\u6270\u7387\u964d\u4f4e71.8%\uff0c\u5e73\u5747\u6545\u969c\u65f6\u95f4\u63d0\u53473.6\u500d\uff0c\u80fd\u8017\u964d\u4f4e62.1%\uff0c\u6027\u80fd\u65e0\u635f\u5931\uff0c\u9762\u79ef\u5f00\u9500\u5c0f\u4e8e0.4%\u3002", "conclusion": "3RSeT\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86STT-MRAM\u7f13\u5b58\u4e2d\u7684\u8bfb\u53d6\u5e72\u6270\u95ee\u9898\uff0c\u901a\u8fc7\u4f4e\u6210\u672c\u7684\u9009\u62e9\u6027\u6807\u7b7e\u6bd4\u8f83\u673a\u5236\u663e\u8457\u63d0\u5347\u53ef\u9760\u6027\u5e76\u964d\u4f4e\u80fd\u8017\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.22333", "categories": ["cs.DC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22333", "abs": "https://arxiv.org/abs/2511.22333", "authors": ["Jinjun Yi", "Zhixin Zhao", "Yitao Hu", "Ke Yan", "Weiwei Sun", "Hao Wang", "Laiping Zhao", "Yuhao Zhang", "Wenxin Li", "Keqiu Li"], "title": "PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel", "comment": "Accepted by ASPLOS'26", "summary": "LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.\n  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.", "AI": {"tldr": "PAT\u63d0\u51fa\u4e86\u4e00\u79cd\u524d\u7f00\u611f\u77e5\u7684\u6ce8\u610f\u529b\u6838\u5b9e\u73b0\uff0c\u901a\u8fc7\u6253\u5305-\u524d\u5411-\u5408\u5e76\u8303\u5f0f\u51cf\u5c11\u91cd\u590d\u5185\u5b58\u8bbf\u95ee\uff0c\u663e\u8457\u964d\u4f4eLLM\u89e3\u7801\u6ce8\u610f\u529b\u5ef6\u8fdf", "motivation": "LLM\u670d\u52a1\u4e2d\u89e3\u7801\u6ce8\u610f\u529b\u6210\u4e3a\u5185\u5b58\u74f6\u9888\uff0c\u800c\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u5b58\u5728\u5927\u91cf\u8de8\u8bf7\u6c42\u7684\u5c42\u6b21\u5316\u5171\u4eab\u524d\u7f00\uff08\u5982\u7cfb\u7edf\u63d0\u793a\u3001\u5de5\u5177\u6a21\u677f\u3001RAG\uff09\u3002\u73b0\u6709\u6ce8\u610f\u529b\u5b9e\u73b0\u65e0\u6cd5\u5145\u5206\u5229\u7528\u524d\u7f00\u5171\u4eab\uff0c\u5bfc\u81f4\u91cd\u590d\u52a0\u8f7d\u5171\u4eabKV\u7f13\u5b58\uff0c\u52a0\u5267\u5185\u5b58\u5e26\u5bbd\u538b\u529b", "method": "\u91c7\u7528\u6253\u5305-\u524d\u5411-\u5408\u5e76\u8303\u5f0f\uff1a1) \u6309\u5171\u4eab\u524d\u7f00\u6253\u5305\u67e5\u8be2\u4ee5\u51cf\u5c11\u91cd\u590d\u5185\u5b58\u8bbf\u95ee\uff1b2) \u8fd0\u884c\u5b9a\u5236\u5316\u591atile\u6838\u5b9e\u73b0\u9ad8\u8d44\u6e90\u6548\u7387\uff1b3) \u5e94\u7528\u591a\u6d41\u524d\u5411\u548cKV\u5206\u5272\u51cf\u5c11\u8d44\u6e90\u6c14\u6ce1\uff1b4) \u6700\u7ec8\u5408\u5e76\u6267\u884c\u5728\u7ebfsoftmax", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u5de5\u4f5c\u8d1f\u8f7d\u8bc4\u4f30\u4e2d\uff0cPAT\u5e73\u5747\u51cf\u5c11\u6ce8\u610f\u529b\u5ef6\u8fdf67.4%\uff0c\u5728\u76f8\u540c\u914d\u7f6e\u4e0bTPOT\u964d\u4f4e13.6-83.4%\uff0c\u4f18\u4e8e\u73b0\u6709\u6ce8\u610f\u529b\u6838", "conclusion": "PAT\u901a\u8fc7\u524d\u7f00\u611f\u77e5\u7684\u6ce8\u610f\u529b\u6838\u5b9e\u73b0\uff0c\u6709\u6548\u5229\u7528\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u5171\u4eab\u524d\u7f00\uff0c\u663e\u8457\u63d0\u5347LLM\u89e3\u7801\u6548\u7387\uff0c\u53ef\u4f5c\u4e3avLLM\u7684\u5373\u63d2\u5373\u7528\u63d2\u4ef6"}}
{"id": "2511.22889", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.22889", "abs": "https://arxiv.org/abs/2511.22889", "authors": ["Fang Li"], "title": "The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference", "comment": "Code and data can be found here: https://github.com/fanglioc/ita-fpga-prototype", "summary": "The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the \"Memory Wall\" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a \"Split-Brain\" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.", "AI": {"tldr": "ITA\u67b6\u6784\u901a\u8fc7\u5c06LLM\u6743\u91cd\u7f16\u7801\u5230ASIC\u7269\u7406\u7535\u8def\u4e2d\uff0c\u6d88\u9664\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\uff0c\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\"\u5185\u5b58\u5899\"\u95ee\u9898", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6d88\u8d39\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u53d7\u5230\"\u5185\u5b58\u5899\"\u7684\u9650\u5236\u2014\u2014\u6bcf\u6b21\u751f\u6210token\u90fd\u9700\u8981\u4eceDRAM\u83b7\u53d6\u6570\u5341\u4ebf\u6a21\u578b\u6743\u91cd\uff0c\u5e26\u6765\u5de8\u5927\u7684\u5e26\u5bbd\u548c\u80fd\u8017\u6210\u672c\u3002\u5f53\u524d\u67b6\u6784\u5c06\u6a21\u578b\u6743\u91cd\u89c6\u4e3a\u53ef\u53d8\u8f6f\u4ef6\u6570\u636e\uff0c\u4e3a\u4fdd\u6301\u901a\u7528\u53ef\u7f16\u7a0b\u6027\u4ed8\u51fa\u4e86\u5de8\u5927\u80fd\u8017\u4ee3\u4ef7\u3002", "method": "\u63d0\u51fa\u4e0d\u53ef\u53d8\u5f20\u91cf\u67b6\u6784(ITA)\uff0c\u5c06\u6a21\u578b\u6743\u91cd\u89c6\u4e3a\u7269\u7406\u7535\u8def\u62d3\u6251\u800c\u975e\u6570\u636e\u3002\u901a\u8fc7\u5c06\u53c2\u6570\u76f4\u63a5\u7f16\u7801\u5230\u6210\u719f\u8282\u70b9ASIC(28nm/40nm)\u7684\u91d1\u5c5e\u4e92\u8fde\u548c\u903b\u8f91\u4e2d\uff0c\u5b8c\u5168\u6d88\u9664\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u3002\u91c7\u7528\"\u5206\u8111\"\u7cfb\u7edf\u8bbe\u8ba1\uff1a\u4e3b\u673aCPU\u7ba1\u7406\u52a8\u6001KV\u7f13\u5b58\u64cd\u4f5c\uff0cITA ASIC\u4f5c\u4e3a\u65e0\u72b6\u6001\u7684ROM\u5d4c\u5165\u5f0f\u6570\u636e\u6d41\u5f15\u64ce\u3002", "result": "\u8be5\u65b9\u6cd5\u7406\u8bba\u4e0a\u80fd\u591f\u6d88\u9664\u6a21\u578b\u6743\u91cd\u8bbf\u95ee\u7684DRAM\u5e26\u5bbd\u548c\u80fd\u8017\u6210\u672c\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8fb9\u7f18LLM\u90e8\u7f72\u3002", "conclusion": "ITA\u4ee3\u8868\u4e86\u4e00\u79cd\u8303\u5f0f\u8f6c\u53d8\uff0c\u5c06\u6a21\u578b\u6743\u91cd\u4ece\u8f6f\u4ef6\u6570\u636e\u91cd\u65b0\u5b9a\u4e49\u4e3a\u786c\u4ef6\u7535\u8def\u62d3\u6251\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u67b6\u6784\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22380", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.22380", "abs": "https://arxiv.org/abs/2511.22380", "authors": ["Kaya Alpturer", "Ron van der Meyden", "Sushmita Ruj", "Godfrey Wong"], "title": "Optimality of Simultaneous Consensus with Limited Information Exchange (Extended Abstract)", "comment": "In Proceedings TARK 2025, arXiv:2511.20540", "summary": "Work on the development of optimal fault-tolerant Agreement protocols using the logic of knowledge has concentrated on the \"full information\" approach to information exchange, which is costly with respect to message size. Alpturer, Halpern, and van der Meyden (PODC 2023) introduced the notion of optimality with respect to a limited information exchange, and studied the Eventual Agreement problem in the sending omissions failure model. The present paper studies the Simultaneous Agreement problem for the crash failures model, and a number of limited information exchanges from the literature. In particular, the paper considers information exchanges from a FloodSet protocol (Lynch, Distributed Algorithms 1996), a variant of this in which agents also count the number of failures (Casta\u00f1eda et al, NETYS 2017), and a variant in which agents associate each agent with a value (Raynal, PRDC 2002). A new information exchange is also introduced that enables decisions to be made at worst one round later than the optimal protocol of Dwork and Moses (I&C 88), but with lower computation cost and space requirements. By determining implementations of a knowledge based program, protocols are derived that are optimal amongst protocols for each of these information exchanges.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5728\u5d29\u6e83\u6545\u969c\u6a21\u578b\u4e0b\uff0c\u9488\u5bf9\u591a\u79cd\u6709\u9650\u4fe1\u606f\u4ea4\u6362\u7684\u540c\u6b65\u5171\u8bc6\u95ee\u9898\uff0c\u63d0\u51fa\u65b0\u4fe1\u606f\u4ea4\u6362\u673a\u5236\uff0c\u5b9e\u73b0\u6bd4\u6700\u4f18\u534f\u8bae\u665a\u4e00\u8f6e\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\uff0c\u5e76\u4e3a\u6bcf\u79cd\u4fe1\u606f\u4ea4\u6362\u63a8\u5bfc\u51fa\u6700\u4f18\u534f\u8bae\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u903b\u8f91\u7684\u6700\u4f18\u5bb9\u9519\u5171\u8bc6\u534f\u8bae\u591a\u91c7\u7528\"\u5168\u4fe1\u606f\"\u4ea4\u6362\u65b9\u5f0f\uff0c\u6d88\u606f\u5f00\u9500\u5927\u3002Alpturer\u7b49\u4eba\u63d0\u51fa\u4e86\u6709\u9650\u4fe1\u606f\u4ea4\u6362\u4e0b\u7684\u6700\u4f18\u6027\u6982\u5ff5\uff0c\u672c\u6587\u5728\u6b64\u57fa\u7840\u4e0a\u7814\u7a76\u5d29\u6e83\u6545\u969c\u6a21\u578b\u4e0b\u7684\u540c\u6b65\u5171\u8bc6\u95ee\u9898\uff0c\u63a2\u7d22\u591a\u79cd\u6587\u732e\u4e2d\u7684\u6709\u9650\u4fe1\u606f\u4ea4\u6362\u673a\u5236\u3002", "method": "1) \u5206\u6790\u6587\u732e\u4e2d\u7684\u6709\u9650\u4fe1\u606f\u4ea4\u6362\u673a\u5236\uff1aFloodSet\u534f\u8bae\u3001\u5e26\u6545\u969c\u8ba1\u6570\u7684\u53d8\u4f53\u3001\u5173\u8054\u4ee3\u7406\u503c\u7684\u53d8\u4f53\uff1b2) \u63d0\u51fa\u65b0\u7684\u4fe1\u606f\u4ea4\u6362\u673a\u5236\uff0c\u80fd\u5728\u6700\u4f18\u534f\u8bae\u57fa\u7840\u4e0a\u6700\u591a\u665a\u4e00\u8f6e\u8fbe\u6210\u5171\u8bc6\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u548c\u7a7a\u95f4\u9700\u6c42\u66f4\u4f4e\uff1b3) \u901a\u8fc7\u5b9e\u73b0\u77e5\u8bc6\u57fa\u7a0b\u5e8f\uff0c\u4e3a\u6bcf\u79cd\u4fe1\u606f\u4ea4\u6362\u63a8\u5bfc\u51fa\u6700\u4f18\u534f\u8bae\u3002", "result": "\u4e3a\u6bcf\u79cd\u6709\u9650\u4fe1\u606f\u4ea4\u6362\u673a\u5236\u63a8\u5bfc\u51fa\u4e86\u6700\u4f18\u534f\u8bae\uff0c\u7279\u522b\u662f\u65b0\u63d0\u51fa\u7684\u4fe1\u606f\u4ea4\u6362\u673a\u5236\u80fd\u5728Dwork\u548cMoses\u7684\u6700\u4f18\u534f\u8bae\u57fa\u7840\u4e0a\u6700\u591a\u665a\u4e00\u8f6e\u8fbe\u6210\u5171\u8bc6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u7a7a\u95f4\u9700\u6c42\u3002", "conclusion": "\u672c\u6587\u5728\u5d29\u6e83\u6545\u969c\u6a21\u578b\u4e0b\uff0c\u9488\u5bf9\u591a\u79cd\u6709\u9650\u4fe1\u606f\u4ea4\u6362\u673a\u5236\u89e3\u51b3\u4e86\u540c\u6b65\u5171\u8bc6\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u65b0\u4fe1\u606f\u4ea4\u6362\u673a\u5236\u5728\u5ef6\u8fdf\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u6bcf\u79cd\u4fe1\u606f\u4ea4\u6362\u627e\u5230\u4e86\u6700\u4f18\u534f\u8bae\u5b9e\u73b0\u3002"}}
{"id": "2511.23011", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.23011", "abs": "https://arxiv.org/abs/2511.23011", "authors": ["Yanjing Wang", "Lizhou Wu", "Sunfeng Gao", "Yibo Tang", "Junhui Luo", "Zicong Wang", "Yang Ou", "Dezun Dong", "Nong Xiao", "Mingche Lai"], "title": "Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation", "comment": "Accepted by HPCA 2026", "summary": "Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading.", "AI": {"tldr": "Cohet\uff1a\u9996\u4e2a\u57fa\u4e8eCXL\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u5f02\u6784\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8ba1\u7b97\u4e0e\u5185\u5b58\u8d44\u6e90\u5f62\u6210CPU\u548cXPU\u6c60\uff0c\u5171\u4eab\u7edf\u4e00\u5185\u5b58\u6c60\uff0c\u663e\u8457\u63d0\u5347\u5f02\u6784\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8ePCIe\u7684\u5f02\u6784\u8ba1\u7b97\u7cfb\u7edf\u5b58\u5728\u7ec6\u7c92\u5ea6\u4e3b\u673a-\u8bbe\u5907\u4ea4\u4e92\u6548\u7387\u4f4e\u4e0b\u548c\u7f16\u7a0b\u6a21\u578b\u590d\u6742\u7684\u95ee\u9898\u3002CXL\u4f5c\u4e3a\u65b0\u5174\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u4e92\u8fde\u6807\u51c6\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u53d7\u9650\u4e8e\u5e73\u53f0\u7a00\u7f3a\u3001\u751f\u6001\u4e0d\u6210\u719f\u548c\u5e94\u7528\u524d\u666f\u4e0d\u660e\u6717\u3002", "method": "\u63d0\u51faCohet\u6846\u67b6\uff0c\u5c06\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u89e3\u8026\u5f62\u6210CPU\u548cXPU\u6c60\uff0c\u5171\u4eab\u7edf\u4e00\u7f13\u5b58\u4e00\u81f4\u6027\u5185\u5b58\u6c60\u3002\u901a\u8fc7\u6807\u51c6malloc/mmap\u63a5\u53e3\u66b4\u9732\u7ed9\u8ba1\u7b97\u7ebf\u7a0b\uff0c\u7531\u64cd\u4f5c\u7cfb\u7edf\u667a\u80fd\u7ba1\u7406\u5f02\u6784\u8d44\u6e90\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u5168\u7cfb\u7edf\u5468\u671f\u7ea7\u6a21\u62df\u5668SimCXL\uff0c\u652f\u6301\u6240\u6709CXL\u5b50\u534f\u8bae\u548c\u8bbe\u5907\u7c7b\u578b\u5efa\u6a21\u3002", "result": "CXL.cache\u76f8\u6bd4DMA\u4f20\u8f93\u5728\u7f13\u5b58\u884c\u7c92\u5ea6\u4e0a\u5ef6\u8fdf\u964d\u4f4e68%\uff0c\u5e26\u5bbd\u63d0\u534714.4\u500d\u3002\u57fa\u4e8eCXL\u7684NIC\u76f8\u6bd4PCIe-NIC\u5728\u8fdc\u7a0b\u539f\u5b50\u64cd\u4f5c\u5378\u8f7d\u4e0a\u5b9e\u73b05.5-40.2\u500d\u52a0\u901f\uff0c\u5728RPC\u5e8f\u5217\u5316/\u53cd\u5e8f\u5217\u5316\u5378\u8f7d\u4e0a\u5e73\u5747\u52a0\u901f1.86\u500d\u3002", "conclusion": "Cohet\u662f\u9996\u4e2aCXL\u9a71\u52a8\u7684\u7f13\u5b58\u4e00\u81f4\u6027\u5f02\u6784\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u5185\u5b58\u6c60\u548c\u6807\u51c6\u63a5\u53e3\u7b80\u5316\u4e86\u5f02\u6784\u7f16\u7a0b\uff0cSimCXL\u6a21\u62df\u5668\u4e3aCXL\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u5c55\u793a\u4e86CXL\u5728\u63d0\u5347\u5f02\u6784\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.22481", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.22481", "abs": "https://arxiv.org/abs/2511.22481", "authors": ["Jun Wang", "Yunxiang Yao", "Wenwei Kuang", "Runze Mao", "Zhenhao Sun", "Zhuang Tao", "Ziyang Zhang", "Dengyu Li", "Jiajun Chen", "Zhili Wang", "Kai Cui", "Congzhi Cai", "Longwen Lan", "Ken Zhang"], "title": "OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency", "comment": "Project page: [this https URL](https://gitee.com/omniai/omniinfer)", "summary": "Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\\%, and the superimposition of OmniProxy further slashes TTFT by 38\\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).", "AI": {"tldr": "OmniInfer\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7cfb\u7edf\u7ea7\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u653e\u7f6e\u3001\u7f13\u5b58\u538b\u7f29\u548c\u8c03\u5ea6\u4f18\u5316\u6765\u6700\u5927\u5316LLM\u670d\u52a1\u6548\u7387\uff0c\u5728DeepSeek-R1\u4e0a\u5b9e\u73b0\u4e86616 QPM\uff0cTPOT\u51cf\u5c1136%\uff0cTTFT\u51cf\u5c1138%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8ba1\u7b97\u5bc6\u96c6\u3001\u5ef6\u8fdf\u4e25\u683c\u548c\u541e\u5410\u74f6\u9888\u65b9\u9762\u7ed9\u5927\u89c4\u6a21\u670d\u52a1\u7cfb\u7edf\u5e26\u6765\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u7ea7\u4f18\u5316\u6765\u63d0\u5347\u7aef\u5230\u7aef\u670d\u52a1\u6548\u7387\u3002", "method": "OmniInfer\u5305\u542b\u4e09\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1aOmniPlacement\u7528\u4e8e\u8d1f\u8f7d\u611f\u77e5\u7684\u4e13\u5bb6\u6df7\u5408\u8c03\u5ea6\uff0cOmniAttn\u7528\u4e8e\u7a00\u758f\u6ce8\u610f\u529b\u52a0\u901f\uff0cOmniProxy\u7528\u4e8e\u89e3\u8026\u611f\u77e5\u7684\u8bf7\u6c42\u8c03\u5ea6\uff0c\u57fa\u4e8evLLM\u6784\u5efa\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8d44\u6e90\u89e3\u8026\u3001\u9ad8\u6548\u7a00\u758f\u5229\u7528\u548c\u5168\u5c40\u534f\u8c03\u5b9e\u73b0\u4f18\u5316\u3002", "result": "\u572810\u8282\u70b9Ascend 910C\u96c6\u7fa4\u4e0a\u8bc4\u4f30DeepSeek-R1\uff0cOmniInfer\u8fbe\u5230616 QPM\uff0c\u7edf\u4e00\u6846\u67b6\u51cf\u5c11TPOT 36%\uff0cOmniProxy\u53e0\u52a0\u8fdb\u4e00\u6b65\u51cf\u5c11TTFT 38%\u3002", "conclusion": "OmniInfer\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\u663e\u8457\u63d0\u5347LLM\u670d\u52a1\u6548\u7387\uff0c\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2511.23203", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23203", "abs": "https://arxiv.org/abs/2511.23203", "authors": ["Jordi Fornt", "Pau Fontova-Must\u00e9", "Adrian Gras", "Omar Lahyani", "Mart\u00ed Caro", "Jaume Abella", "Francesc Moll", "Josep Altet"], "title": "GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration", "comment": "Presented in the 2025 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED). Conference proceedings pending to be published", "summary": "Voltage overscaling, or undervolting, is an enticing approximate technique in the context of energy-efficient Deep Neural Network (DNN) acceleration, given the quadratic relationship between power and voltage. Nevertheless, its very high error rate has thwarted its general adoption. Moreover, recent undervolting accelerators rely on 8-bit arithmetic and cannot compete with state-of-the-art low-precision (<8b) architectures. To overcome these issues, we propose a new technique called Guarded Aggressive underVolting (GAV), which combines the ideas of undervolting and bit-serial computation to create a flexible approximation method based on aggressively lowering the supply voltage on a select number of least significant bit combinations. Based on this idea, we implement GAVINA (GAV mIxed-precisioN Accelerator), a novel architecture that supports arbitrary mixed precision and flexible undervolting, with an energy efficiency of up to 89 TOP/sW in its most aggressive configuration. By developing an error model of GAVINA, we show that GAV can achieve an energy efficiency boost of 20% via undervolting, with negligible accuracy degradation on ResNet-18.", "AI": {"tldr": "\u63d0\u51faGAV\u6280\u672f\uff0c\u7ed3\u5408\u6b20\u538b\u548c\u4f4d\u4e32\u884c\u8ba1\u7b97\uff0c\u5b9e\u73b0\u7075\u6d3b\u8fd1\u4f3c\u8ba1\u7b97\uff0c\u5e76\u8bbe\u8ba1GAVINA\u67b6\u6784\uff0c\u5728ResNet-18\u4e0a\u5b9e\u73b020%\u80fd\u6548\u63d0\u5347\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "motivation": "\u7535\u538b\u8fc7\u7f29\u653e\uff08\u6b20\u538b\uff09\u4f5c\u4e3a\u8fd1\u4f3c\u8ba1\u7b97\u6280\u672f\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u9ad8\u9519\u8bef\u7387\u963b\u788d\u5176\u5e7f\u6cdb\u5e94\u7528\uff0c\u4e14\u73b0\u6709\u6b20\u538b\u52a0\u901f\u5668\u57fa\u4e8e8\u4f4d\u7b97\u672f\uff0c\u65e0\u6cd5\u4e0e\u5148\u8fdb\u4f4e\u7cbe\u5ea6\u67b6\u6784\u7ade\u4e89\u3002", "method": "\u63d0\u51faGAV\u6280\u672f\uff0c\u7ed3\u5408\u6b20\u538b\u548c\u4f4d\u4e32\u884c\u8ba1\u7b97\uff0c\u9009\u62e9\u6027\u5730\u5bf9\u6700\u4f4e\u6709\u6548\u4f4d\u7ec4\u5408\u8fdb\u884c\u6fc0\u8fdb\u964d\u538b\uff1b\u5b9e\u73b0GAVINA\u67b6\u6784\uff0c\u652f\u6301\u4efb\u610f\u6df7\u5408\u7cbe\u5ea6\u548c\u7075\u6d3b\u6b20\u538b\u3002", "result": "GAVINA\u5728\u6700\u6fc0\u8fdb\u914d\u7f6e\u4e0b\u80fd\u6548\u8fbe89 TOP/sW\uff1bGAV\u6280\u672f\u901a\u8fc7\u6b20\u538b\u5b9e\u73b020%\u80fd\u6548\u63d0\u5347\uff0c\u5728ResNet-18\u4e0a\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "GAV\u6280\u672f\u6210\u529f\u514b\u670d\u4e86\u6b20\u538b\u6280\u672f\u7684\u9ad8\u9519\u8bef\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u80fd\u6548\u663e\u8457\u63d0\u5347\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u63a5\u53d7\uff0c\u4e3a\u4f4e\u529f\u8017DNN\u52a0\u901f\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2511.22599", "categories": ["cs.DC", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22599", "abs": "https://arxiv.org/abs/2511.22599", "authors": ["Mohammadreza Malekabbasi", "Minghe Wang", "David Bermbach"], "title": "DisCEdge: Distributed Context Management for Large Language Models at the Edge", "comment": "author version", "summary": "Deploying Large Language Model (LLM) services at the edge benefits latency-sensitive and privacy-aware applications. However, the stateless nature of LLMs makes managing user context (e.g., sessions, preferences) across geo-distributed edge nodes challenging. Existing solutions, such as client-side context storage, often introduce network latency and bandwidth overhead, undermining the advantages of edge deployment.\n  We propose DisCEdge, a distributed context management system that stores and replicates user context in tokenized form across edge nodes. By maintaining context as token sequences rather than raw text, our system avoids redundant computation and enables efficient data replication. We implement and evaluate an open-source prototype in a realistic edge environment with commodity hardware. We show DisCEdge improves median response times by up to 14.46% and lowers median inter-node synchronization overhead by up to 15% compared to a raw-text-based system. It also reduces client request sizes by a median of 90% compared to client-side context management, while guaranteeing data consistency.", "AI": {"tldr": "DisCEdge\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u4e0a\u4e0b\u6587\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u7528\u6237\u4e0a\u4e0b\u6587\u4ee5token\u5e8f\u5217\u5f62\u5f0f\u5b58\u50a8\u548c\u590d\u5236\u5728\u8fb9\u7f18\u8282\u70b9\u4e4b\u95f4\uff0c\u89e3\u51b3\u4e86LLM\u5728\u8fb9\u7f18\u90e8\u7f72\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u54cd\u5e94\u65f6\u95f4\u548c\u964d\u4f4e\u4e86\u540c\u6b65\u5f00\u9500\u3002", "motivation": "\u5728\u8fb9\u7f18\u90e8\u7f72LLM\u670d\u52a1\u6709\u5229\u4e8e\u5ef6\u8fdf\u654f\u611f\u548c\u9690\u79c1\u4fdd\u62a4\u5e94\u7528\uff0c\u4f46LLM\u7684\u65e0\u72b6\u6001\u7279\u6027\u4f7f\u5f97\u8de8\u5730\u7406\u5206\u5e03\u5f0f\u8fb9\u7f18\u8282\u70b9\u7ba1\u7406\u7528\u6237\u4e0a\u4e0b\u6587\uff08\u5982\u4f1a\u8bdd\u3001\u504f\u597d\uff09\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982\u5ba2\u6237\u7aef\u4e0a\u4e0b\u6587\u5b58\u50a8\uff09\u901a\u5e38\u5f15\u5165\u7f51\u7edc\u5ef6\u8fdf\u548c\u5e26\u5bbd\u5f00\u9500\uff0c\u524a\u5f31\u4e86\u8fb9\u7f18\u90e8\u7f72\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faDisCEdge\u7cfb\u7edf\uff0c\u4ee5token\u5316\u5f62\u5f0f\u800c\u975e\u539f\u59cb\u6587\u672c\u5b58\u50a8\u548c\u590d\u5236\u7528\u6237\u4e0a\u4e0b\u6587\u3002\u901a\u8fc7\u5c06\u4e0a\u4e0b\u6587\u7ef4\u62a4\u4e3atoken\u5e8f\u5217\uff0c\u907f\u514d\u5197\u4f59\u8ba1\u7b97\u5e76\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u590d\u5236\u3002\u5728\u771f\u5b9e\u8fb9\u7f18\u73af\u5883\u4e2d\u4f7f\u7528\u5546\u7528\u786c\u4ef6\u5b9e\u73b0\u5e76\u8bc4\u4f30\u5f00\u6e90\u539f\u578b\u3002", "result": "\u76f8\u6bd4\u57fa\u4e8e\u539f\u59cb\u6587\u672c\u7684\u7cfb\u7edf\uff0cDisCEdge\u5c06\u4e2d\u4f4d\u54cd\u5e94\u65f6\u95f4\u63d0\u5347\u9ad8\u8fbe14.46%\uff0c\u4e2d\u4f4d\u8282\u70b9\u95f4\u540c\u6b65\u5f00\u9500\u964d\u4f4e\u9ad8\u8fbe15%\u3002\u76f8\u6bd4\u5ba2\u6237\u7aef\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u5c06\u5ba2\u6237\u7aef\u8bf7\u6c42\u5927\u5c0f\u4e2d\u4f4d\u51cf\u5c1190%\uff0c\u540c\u65f6\u4fdd\u8bc1\u6570\u636e\u4e00\u81f4\u6027\u3002", "conclusion": "DisCEdge\u901a\u8fc7token\u5316\u7684\u5206\u5e03\u5f0f\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18LLM\u90e8\u7f72\u4e2d\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6570\u636e\u4e00\u81f4\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2511.22779", "categories": ["cs.DC", "physics.med-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.22779", "abs": "https://arxiv.org/abs/2511.22779", "authors": ["Shijie Yan", "Douglas Dwyer", "David R. Kaeli", "Qianqian Fang"], "title": "Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware", "comment": null, "summary": "Significance: Monte Carlo (MC) methods are the gold-standard for modeling light-tissue interactions due to their accuracy. Mesh-based MC (MMC) offers enhanced precision for complex tissue structures using tetrahedral mesh models. Despite significant speedups achieved on graphics processing units (GPUs), MMC performance remains hindered by the computational cost of frequent ray-boundary intersection tests.\n  Aim: We propose a highly accelerated MMC algorithm, RT-MMC, that leverages the hardware-accelerated ray traversal and intersection capabilities of ray-tracing cores (RT-cores) on modern GPUs.\n  Approach: Implemented using NVIDIA's OptiX platform, RT-MMC extends graphics ray-tracing pipelines towards volumetric ray-tracing in turbid media, eliminating the need for challenging tetrahedral mesh generation while delivering significant speed improvements through hardware acceleration. It also intrinsically supports wide-field sources without complex mesh retesselation.\n  Results: RT-MMC demonstrates excellent agreement with traditional software-ray-tracing MMC algorithms while achieving 1.5x to 4.5x speedups across multiple GPU architectures. These performance gains significantly enhance the practicality of MMC for routine simulations.\n  Conclusion: Migration from software- to hardware-based ray-tracing not only greatly simplifies MMC simulation workflows, but also results in significant speedups that are expected to increase further as ray-tracing hardware rapidly gains adoption. Adoption of graphics ray-tracing pipelines in quantitative MMC simulations enables leveraging of emerging hardware resources and benefits a wide range of biophotonics applications.", "AI": {"tldr": "RT-MMC\uff1a\u5229\u7528GPU\u5149\u7ebf\u8ffd\u8e2a\u6838\u5fc3\u786c\u4ef6\u52a0\u901f\u7684\u8499\u7279\u5361\u6d1b\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u7f51\u683c\u8499\u7279\u5361\u6d1b\u6a21\u62df\u901f\u5ea6\uff0c\u7b80\u5316\u5de5\u4f5c\u6d41\u7a0b", "motivation": "\u4f20\u7edf\u7f51\u683c\u8499\u7279\u5361\u6d1b\uff08MMC\uff09\u65b9\u6cd5\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u9891\u7e41\u7684\u5c04\u7ebf-\u8fb9\u754c\u76f8\u4ea4\u6d4b\u8bd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002\u73b0\u4ee3GPU\u7684\u5149\u7ebf\u8ffd\u8e2a\u6838\u5fc3\uff08RT-cores\uff09\u63d0\u4f9b\u4e86\u786c\u4ef6\u52a0\u901f\u80fd\u529b\uff0c\u4f46\u5c1a\u672a\u88ab\u5145\u5206\u5229\u7528\u4e8e\u751f\u7269\u5149\u5b50\u5b66\u6a21\u62df\u3002", "method": "\u57fa\u4e8eNVIDIA OptiX\u5e73\u53f0\u5f00\u53d1RT-MMC\u7b97\u6cd5\uff0c\u5c06\u56fe\u5f62\u5149\u7ebf\u8ffd\u8e2a\u7ba1\u9053\u6269\u5c55\u5230\u6d51\u6d4a\u4ecb\u8d28\u4e2d\u7684\u4f53\u79ef\u5149\u7ebf\u8ffd\u8e2a\u3002\u5229\u7528RT-cores\u7684\u786c\u4ef6\u52a0\u901f\u80fd\u529b\uff0c\u907f\u514d\u590d\u6742\u7684\u56db\u9762\u4f53\u7f51\u683c\u751f\u6210\uff0c\u540c\u65f6\u652f\u6301\u5bbd\u573a\u5149\u6e90\u800c\u65e0\u9700\u590d\u6742\u7f51\u683c\u91cd\u5212\u5206\u3002", "result": "RT-MMC\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u5149\u7ebf\u8ffd\u8e2aMMC\u7b97\u6cd5\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5728\u4e0d\u540cGPU\u67b6\u6784\u4e0a\u5b9e\u73b01.5\u500d\u52304.5\u500d\u7684\u52a0\u901f\u3002\u6027\u80fd\u63d0\u5347\u663e\u8457\u589e\u5f3a\u4e86MMC\u5728\u5e38\u89c4\u6a21\u62df\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u4ece\u8f6f\u4ef6\u8f6c\u5411\u786c\u4ef6\u5149\u7ebf\u8ffd\u8e2a\u4e0d\u4ec5\u5927\u5927\u7b80\u5316\u4e86MMC\u6a21\u62df\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8fd8\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u968f\u7740\u5149\u7ebf\u8ffd\u8e2a\u786c\u4ef6\u7684\u5feb\u901f\u666e\u53ca\uff0c\u8fd9\u79cd\u52a0\u901f\u6548\u679c\u9884\u8ba1\u4f1a\u8fdb\u4e00\u6b65\u589e\u5f3a\u3002\u5728\u5b9a\u91cfMMC\u6a21\u62df\u4e2d\u91c7\u7528\u56fe\u5f62\u5149\u7ebf\u8ffd\u8e2a\u7ba1\u9053\u80fd\u591f\u5229\u7528\u65b0\u5174\u786c\u4ef6\u8d44\u6e90\uff0c\u60e0\u53ca\u5e7f\u6cdb\u7684\u751f\u7269\u5149\u5b50\u5b66\u5e94\u7528\u3002"}}
{"id": "2511.22880", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22880", "abs": "https://arxiv.org/abs/2511.22880", "authors": ["Shashwat Jaiswal", "Shrikara Arun", "Anjaly Parayil", "Ankur Mallick", "Spyros Mastorakis", "Alind Khare", "Chloi Alverti", "Renee St Amant", "Chetan Bansal", "Victor R\u00fchle", "Josep Torrellas"], "title": "Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems", "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\\times$ higher throughput, up to 9$\\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.", "AI": {"tldr": "LoRAServe\u662f\u4e00\u4e2a\u52a8\u6001\u9002\u914d\u5668\u653e\u7f6e\u548c\u8def\u7531\u6846\u67b6\uff0c\u4e13\u95e8\u89e3\u51b3LoRA\u670d\u52a1\u4e2d\u56e0\u9002\u914d\u5668\u5927\u5c0f\u5dee\u5f02\u5bfc\u81f4\u7684\u6027\u80fd\u503e\u659c\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u5e73\u8861\u548c\u8fdc\u7a0b\u8bbf\u95ee\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u5e76\u51cf\u5c11GPU\u9700\u6c42\u3002", "motivation": "\u5f53\u524dLoRA\u670d\u52a1\u7cfb\u7edf\u5728\u5904\u7406\u591a\u79df\u6237\u73af\u5883\u4e2d\u4e0d\u540c\u5927\u5c0f\u7684\u9002\u914d\u5668\u65f6\uff0c\u7531\u4e8e\u5ffd\u7565\u9002\u914d\u5668\u5927\u5c0f\uff08\u79e9\uff09\u7684\u5dee\u5f02\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u6027\u80fd\u503e\u659c\u548cGPU\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u591aGPU\u6765\u6ee1\u8db3\u670d\u52a1\u7ea7\u522b\u76ee\u6807\u3002", "method": "\u63d0\u51faLoRAServe\u6846\u67b6\uff0c\u91c7\u7528\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7684\u52a8\u6001\u9002\u914d\u5668\u653e\u7f6e\u548c\u8def\u7531\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u5e73\u8861\u9002\u914d\u5668\u5728GPU\u95f4\u7684\u5206\u5e03\uff0c\u5e76\u5229\u7528GPU Direct RDMA\u8fdb\u884c\u8fdc\u7a0b\u8bbf\u95ee\uff0c\u4ee5\u4f18\u5316\u8d44\u6e90\u5229\u7528\u3002", "result": "\u5728\u771f\u5b9e\u751f\u4ea7\u73af\u5883\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u7cfb\u7edf\uff0cLoRAServe\u5b9e\u73b0\u4e86\u6700\u9ad82\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u6700\u9ad89\u500d\u7684\u9996\u4ee4\u724c\u65f6\u95f4\u964d\u4f4e\uff0c\u5e76\u5728\u6ee1\u8db3SLO\u7ea6\u675f\u4e0b\u51cf\u5c11\u4e8650%\u7684GPU\u4f7f\u7528\u3002", "conclusion": "LoRAServe\u901a\u8fc7\u52a8\u6001\u5904\u7406\u9002\u914d\u5668\u5927\u5c0f\u5dee\u5f02\uff0c\u6709\u6548\u89e3\u51b3\u4e86LoRA\u670d\u52a1\u4e2d\u7684\u6027\u80fd\u503e\u659c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u79df\u6237\u73af\u5883\u4e0b\u7684\u8d44\u6e90\u5229\u7528\u6548\u7387\u548c\u670d\u52a1\u6027\u80fd\u3002"}}
{"id": "2511.23025", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.23025", "abs": "https://arxiv.org/abs/2511.23025", "authors": ["\u00c1lvaro Castro-Castilla", "Marcin Pawlowski", "Hong-Sheng Zhou"], "title": "Areon: Latency-Friendly and Resilient Multi-Proposer Consensus", "comment": null, "summary": "We present Areon, a family of latency-friendly, stake-weighted, multi-proposer proof-of-stake consensus protocols. By allowing multiple proposers per slot and organizing blocks into a directed acyclic graph (DAG), Areon achieves robustness under partial synchrony. Blocks reference each other within a sliding window, forming maximal antichains that represent parallel ``votes'' on history. Conflicting subDAGs are resolved by a closest common ancestor (CCA)-local, window-filtered fork choice that compares the weight of each subDAG -- the number of recent short references -- and prefers the heavier one. Combined with a structural invariant we call Tip-Boundedness (TB), this yields a bounded-width frontier and allows honest work to aggregate quickly.\n  We formalize an idealized protocol (Areon-Ideal) that abstracts away network delay and reference bounds, and a practical protocol (Areon-Base) that adds VRF-based eligibility, bounded short and long references, and application-level validity and conflict checks at the block level. On top of DAG analogues of the classical common-prefix, chain-growth, and chain-quality properties, we prove a backbone-style $(k,\\varepsilon)$-finality theorem that calibrates confirmation depth as a function of the window length and target tail probability. We focus on consensus at the level of blocks; extending the framework to richer transaction selection, sampling, and redundancy policies is left to future work.\n  Finally, we build a discrete-event simulator and compare Areon-Base against a chain-based baseline (Ouroboros Praos) under matched block-arrival rates. Across a wide range of adversarial stakes and network delays, Areon-Base achieves bounded-latency finality with consistently lower reorganization frequency and depth.", "AI": {"tldr": "Areon\u662f\u4e00\u4e2a\u57fa\u4e8eDAG\u7684\u591a\u63d0\u8bae\u8005PoS\u5171\u8bc6\u534f\u8bae\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u5185\u7684\u5757\u5f15\u7528\u5f62\u6210\u6700\u5927\u53cd\u94fe\u4f5c\u4e3a\u5e76\u884c\u6295\u7968\uff0c\u4f7f\u7528CCA-local fork choice\u89e3\u51b3\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u6700\u7ec8\u6027\u548c\u4f4e\u91cd\u7ec4\u9891\u7387\u3002", "motivation": "\u4f20\u7edf\u94fe\u5f0fPoS\u534f\u8bae\u5728\u90e8\u5206\u540c\u6b65\u7f51\u7edc\u4e0b\u5b58\u5728\u5ef6\u8fdf\u548c\u91cd\u7ec4\u95ee\u9898\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u9c81\u68d2\u3001\u4f4e\u5ef6\u8fdf\u7684\u5171\u8bc6\u673a\u5236\u6765\u6539\u5584\u6700\u7ec8\u6027\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eDAG\u7684\u591a\u63d0\u8bae\u8005\u534f\u8bae\uff0c\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u5185\u7684\u5757\u5f15\u7528\u5f62\u6210\u6700\u5927\u53cd\u94fe\u4f5c\u4e3a\u5e76\u884c\u6295\u7968\uff0c\u901a\u8fc7CCA-local\u3001\u7a97\u53e3\u8fc7\u6ee4\u7684\u5206\u53c9\u9009\u62e9\u89c4\u5219\u6bd4\u8f83\u5b50DAG\u6743\u91cd\uff0c\u7ed3\u5408Tip-Boundedness\u7ed3\u6784\u4e0d\u53d8\u91cf\u786e\u4fdd\u6709\u754c\u5bbd\u5ea6\u524d\u6cbf\u3002", "result": "\u8bc1\u660e\u4e86(k,\u03b5)-\u6700\u7ec8\u6027\u5b9a\u7406\uff0c\u901a\u8fc7\u79bb\u6563\u4e8b\u4ef6\u6a21\u62df\u663e\u793aAreon-Base\u76f8\u6bd4Ouroboros Praos\u5728\u5404\u79cd\u5bf9\u6297\u80a1\u6743\u548c\u7f51\u7edc\u5ef6\u8fdf\u4e0b\u5b9e\u73b0\u4e86\u6709\u754c\u5ef6\u8fdf\u6700\u7ec8\u6027\uff0c\u91cd\u7ec4\u9891\u7387\u548c\u6df1\u5ea6\u66f4\u4f4e\u3002", "conclusion": "Areon\u534f\u8bae\u901a\u8fc7DAG\u7ed3\u6784\u548c\u591a\u63d0\u8bae\u8005\u8bbe\u8ba1\u6709\u6548\u6539\u5584\u4e86PoS\u5171\u8bc6\u7684\u5ef6\u8fdf\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u4f4e\u5ef6\u8fdf\u6700\u7ec8\u6027\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u4f46\u66f4\u4e30\u5bcc\u7684\u4ea4\u6613\u9009\u62e9\u548c\u5197\u4f59\u7b56\u7565\u9700\u8981\u672a\u6765\u5de5\u4f5c\u3002"}}
{"id": "2511.23167", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.23167", "abs": "https://arxiv.org/abs/2511.23167", "authors": ["Chenyu Liu", "Zhaoyang Zhang", "Zirui Chen", "Zhaohui Yang"], "title": "Communication-Computation Pipeline Parallel Split Learning over Wireless Edge Networks", "comment": null, "summary": "Split learning (SL) offloads main computing tasks from multiple resource-constrained user equippments (UEs) to the base station (BS), while preserving local data privacy. However, its computation and communication processes remain sequential, resulting in limited system efficiency. To overcome this limitation, this paper applies pipeline parallelism (PP) of distributed training to SL in wireless networks, proposing the so-called communication-computation pipeline parallel split learning (C$^2$P$^2$SL). By considering the communicating and computing processes of UEs and BS as an overall pipeline, C$^2$P$^2$SL achieves pipeline parallelization among different micro-batches which are split from each batch of data samples. The overlap of communication and computation in this way significantly reduces the total training time. Given that training efficiency is affected by position of cutting layer and heterogeneity of the UEs, we formulate a joint optimization problem of task split and resource allocation, and design a solution based on alternating optimization. Experimental results demonstrate that C$^2$P$^2$SL significantly reduces system training time by over 38\\% while maintaining convergence accuracy under different communication conditions.", "AI": {"tldr": "\u63d0\u51faC\u00b2P\u00b2SL\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d41\u6c34\u7ebf\u5e76\u884c\u5316\u901a\u4fe1\u548c\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u5206\u5272\u5b66\u4e60\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u8bad\u7ec3\u65f6\u95f4", "motivation": "\u4f20\u7edf\u5206\u5272\u5b66\u4e60\u867d\u7136\u4fdd\u62a4\u4e86\u672c\u5730\u6570\u636e\u9690\u79c1\uff0c\u4f46\u5176\u8ba1\u7b97\u548c\u901a\u4fe1\u8fc7\u7a0b\u662f\u987a\u5e8f\u6267\u884c\u7684\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6548\u7387\u6709\u9650\u3002\u9700\u8981\u514b\u670d\u8fd9\u4e00\u9650\u5236\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u5c06\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u6d41\u6c34\u7ebf\u5e76\u884c\u6280\u672f\u5e94\u7528\u4e8e\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5206\u5272\u5b66\u4e60\uff0c\u5c06UE\u548cBS\u7684\u901a\u4fe1\u4e0e\u8ba1\u7b97\u8fc7\u7a0b\u89c6\u4e3a\u6574\u4f53\u6d41\u6c34\u7ebf\uff0c\u5728\u4e0d\u540c\u5fae\u6279\u6b21\u95f4\u5b9e\u73b0\u6d41\u6c34\u7ebf\u5e76\u884c\u5316\u3002\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4efb\u52a1\u5206\u5272\u548c\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u4ea4\u66ff\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cC\u00b2P\u00b2SL\u5728\u4e0d\u540c\u901a\u4fe1\u6761\u4ef6\u4e0b\u80fd\u4fdd\u6301\u6536\u655b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u7cfb\u7edf\u8bad\u7ec3\u65f6\u95f4\u8d85\u8fc738%\u3002", "conclusion": "C\u00b2P\u00b2SL\u901a\u8fc7\u6d41\u6c34\u7ebf\u5e76\u884c\u5316\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u5206\u5272\u5b66\u4e60\u4e2d\u7684\u6548\u7387\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.23297", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.23297", "abs": "https://arxiv.org/abs/2511.23297", "authors": ["Yi-Jun Chang", "Lyuting Chen", "Haoran Zhou"], "title": "Beyond 2-Edge-Connectivity: Algorithms and Impossibility for Content-Oblivious Leader Election", "comment": null, "summary": "The content-oblivious model, introduced by Censor-Hillel, Cohen, Gelles, and Sel (PODC 2022; Distributed Computing 2023), captures an extremely weak form of communication where nodes can only send asynchronous, content-less pulses. Censor-Hillel, Cohen, Gelles, and Sel showed that no non-constant function $f(x,y)$ can be computed correctly by two parties using content-oblivious communication over a single edge, where one party holds $x$ and the other holds $y$. This seemingly ruled out many natural graph problems on non-2-edge-connected graphs.\n  In this work, we show that, with the knowledge of network topology $G$, leader election is possible in a wide range of graphs.\n  Impossibility: Graphs symmetric about an edge admit no randomized terminating leader election algorithm, even when nodes have unique identifiers and full knowledge of $G$.\n  Leader election algorithms: Trees that are not symmetric about any edge admit a quiescently terminating leader election algorithm with topology knowledge, even in anonymous networks, using $O(n^2)$ messages, where $n$ is the number of nodes. Moreover, even-diameter trees admit a terminating leader election given only the knowledge of the network diameter $D = 2r$, with message complexity $O(nr)$.\n  Necessity of topology knowledge: In the family of graphs $\\mathcal{G} = \\{P_3, P_5\\}$, both the 3-path $P_3$ and the 5-path $P_5$ admit a quiescently terminating leader election if nodes know the topology exactly. However, if nodes only know that the underlying topology belongs to $\\mathcal{G}$, then terminating leader election is impossible.", "AI": {"tldr": "\u5728\u5185\u5bb9\u65e0\u5173\u901a\u4fe1\u6a21\u578b\u4e0b\uff0c\u5229\u7528\u62d3\u6251\u77e5\u8bc6\u53ef\u5b9e\u73b0\u67d0\u4e9b\u56fe\u4e2d\u7684\u9886\u5bfc\u8005\u9009\u4e3e\uff0c\u4f46\u62d3\u6251\u5bf9\u79f0\u6027\u548c\u62d3\u6251\u77e5\u8bc6\u7cbe\u786e\u5ea6\u662f\u5173\u952e\u9650\u5236\u56e0\u7d20\u3002", "motivation": "\u5185\u5bb9\u65e0\u5173\u901a\u4fe1\u6a21\u578b\u662f\u4e00\u79cd\u6781\u5f31\u7684\u901a\u4fe1\u5f62\u5f0f\uff0c\u8282\u70b9\u53ea\u80fd\u53d1\u9001\u5f02\u6b65\u3001\u65e0\u5185\u5bb9\u7684\u8109\u51b2\u3002\u5148\u524d\u7814\u7a76\u8868\u660e\uff0c\u5728\u5355\u8fb9\u4e0a\u65e0\u6cd5\u8ba1\u7b97\u4efb\u4f55\u975e\u5e38\u6570\u51fd\u6570\uff0c\u8fd9\u4f3c\u4e4e\u6392\u9664\u4e86\u8bb8\u591a\u81ea\u7136\u56fe\u95ee\u9898\u3002\u672c\u6587\u63a2\u7d22\u5728\u5df2\u77e5\u7f51\u7edc\u62d3\u6251\u7684\u60c5\u51b5\u4e0b\uff0c\u662f\u5426\u80fd\u5728\u66f4\u5e7f\u6cdb\u7684\u56fe\u4e2d\u5b9e\u73b0\u9886\u5bfc\u8005\u9009\u4e3e\u3002", "method": "1. \u8bc1\u660e\u5173\u4e8e\u8fb9\u5bf9\u79f0\u7684\u56fe\u65e0\u6cd5\u8fdb\u884c\u968f\u673a\u7ec8\u6b62\u9886\u5bfc\u8005\u9009\u4e3e\uff1b2. \u4e3a\u975e\u5bf9\u79f0\u6811\u8bbe\u8ba1\u57fa\u4e8e\u62d3\u6251\u77e5\u8bc6\u7684\u9759\u9ed8\u7ec8\u6b62\u7b97\u6cd5\uff1b3. \u5206\u6790\u62d3\u6251\u77e5\u8bc6\u7cbe\u786e\u5ea6\u7684\u5fc5\u8981\u6027\uff0c\u6bd4\u8f83\u7cbe\u786e\u62d3\u6251\u77e5\u8bc6\u4e0e\u62d3\u6251\u65cf\u77e5\u8bc6\u7684\u5dee\u5f02\u3002", "result": "1. \u8fb9\u5bf9\u79f0\u56fe\u65e0\u6cd5\u8fdb\u884c\u9886\u5bfc\u8005\u9009\u4e3e\uff1b2. \u975e\u5bf9\u79f0\u6811\u53ef\u5b9e\u73b0\u9886\u5bfc\u8005\u9009\u4e3e\uff0c\u6d88\u606f\u590d\u6742\u5ea6\u4e3aO(n\u00b2)\uff1b3. \u5076\u76f4\u5f84\u6811\u4ec5\u9700\u76f4\u5f84\u77e5\u8bc6\u5373\u53ef\u9009\u4e3e\uff0c\u6d88\u606f\u590d\u6742\u5ea6O(nr)\uff1b4. \u62d3\u6251\u77e5\u8bc6\u7cbe\u786e\u5ea6\u81f3\u5173\u91cd\u8981\uff1a\u5df2\u77e5\u786e\u5207\u62d3\u6251\u65f6\u53ef\u9009\u4e3e\uff0c\u4ec5\u77e5\u62d3\u6251\u65cf\u65f6\u4e0d\u53ef\u9009\u4e3e\u3002", "conclusion": "\u5728\u5185\u5bb9\u65e0\u5173\u901a\u4fe1\u6a21\u578b\u4e2d\uff0c\u62d3\u6251\u77e5\u8bc6\u662f\u5b9e\u73b0\u9886\u5bfc\u8005\u9009\u4e3e\u7684\u5173\u952e\u56e0\u7d20\u3002\u62d3\u6251\u5bf9\u79f0\u6027\u6784\u6210\u6839\u672c\u969c\u788d\uff0c\u800c\u62d3\u6251\u77e5\u8bc6\u7684\u7cbe\u786e\u5ea6\u76f4\u63a5\u5f71\u54cd\u9009\u4e3e\u53ef\u80fd\u6027\u3002\u8fd9\u4e3a\u5f31\u901a\u4fe1\u6a21\u578b\u4e0b\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u8fb9\u754c\u3002"}}
