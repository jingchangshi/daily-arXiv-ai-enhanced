<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD](https://arxiv.org/abs/2509.07003)
*Youjie Li,Cheng Wan,Zhiqi Lin,Hongyu Zhu,Jiacheng Yang,Ziang Song,Xinyi Di,Jiawei Wu,Huiyao Shu,Wenlei Bao,Yanghua Peng,Haibin Lin,Li-Wen Chang*

Main category: cs.PL

TL;DR: veScale是一个基于SPMD范式的分布式训练系统，解决了LLM训练中的一致性和性能问题，比现有系统快2.2倍，代码复杂度降低78.4%


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模和复杂度的快速增长，现有分布式训练方法（如3D并行）变得过于复杂，需要转向更简单、易调试的SPMD编程范式，但SPMD在eager执行中存在结果一致性和性能扩展的挑战

Method: 引入veScale系统，采用完全SPMD范式，提出分布式随机数生成算法确保任意分片算子的一致性，同时通过减少PyTorch原语开销和提升通信效率来优化性能

Result: 相比TorchTitan等先进训练系统，veScale实现了最高2.2倍的加速，代码复杂度降低78.4%，同时保持与单设备执行等效的结果

Conclusion: veScale成功证明了SPMD范式在LLM分布式训练中的可行性，为大规模模型训练提供了更简单、高效且可靠的解决方案

Abstract: Large Language Models (LLMs) have scaled rapidly in size and complexity,
requiring increasingly intricate parallelism for distributed training, such as
3D parallelism. This sophistication motivates a shift toward simpler, more
debuggable programming paradigm like Single Program Multiple Data (SPMD).
However, SPMD in eager execution introduces two key challenges: ensuring
consistency with single-device execution and achieving high performance at
scale. In this paper, we introduce veScale, an eager-mode training system that
fully embraces SPMD paradigm to democratize distributed tensor programming.
veScale addresses the prevalent issue of inconsistent results in systems like
PyTorch by introducing a novel algorithm of distributed Random Number
Generation (RNG) compatible with arbitrary sharded operators. veScale also
significantly boosts training performance by reducing PyTorch primitive's
overhead and improving communication efficiency. Evaluations show that veScale
delivers up to 2.2x speedup over the state-of-the-art training systems, like
TorchTitan, and cuts code complexity by 78.4%, while preserving
single-device-equivalent results.

</details>


### [2] [Fast and Extensible Hybrid Embeddings with Micros](https://arxiv.org/abs/2509.07551)
*Sean Bocirnea,William J. Bowman*

Main category: cs.PL

TL;DR: 小型嵌入技术通过语法到中间表示转换器提高编译性能，充分利用深嵌入的高效能和浅嵌入的扩展性，在保持类型语言扩展性的同时显著缩短编译时间。


<details>
  <summary>Details</summary>
Motivation: 宏嵌入技术虽然支持扩展性类型语言实现，但导致编译时性能问题，需要找到一种方法在保持扩展性的同时提升编译效率。

Method: 采用微嵌入技术，将语法转换为中间表示（IR），在IR层面进行高效的编译时函数运算，最后再浅嵌入回源码语法，并通过多种设计模式保证IR和其函数的扩展性。

Result: 实现了可扩展的混合嵌入静态类型语言，与宏嵌入方法相比，编译时间得到显著缩短。

Conclusion: 微嵌入技术通过结合深嵌入的高效能和浅嵌入的扩展性，成功解决了宏嵌入在编译性能方面的局限性，为构建高效可扩展类型语言提供了有效方案。

Abstract: Macro embedding is a popular approach to defining extensible shallow
embeddings of object languages in Scheme like host languages. While macro
embedding has even been shown to enable implementing extensible typed languages
in systems like Racket, it comes at a cost: compile-time performance. In this
paper, we revisit micros - syntax to intermediate representation (IR)
transformers, rather than source syntax to source syntax transformers (macros).
Micro embedding enables stopping at an IR, producing a deep embedding and
enabling high performance compile-time functions over an efficient IR, before
shallowly embedding the IR back into source syntax. Combining micros with
several design patterns to enable the IR and functions over it to be
extensible, we achieve extensible hybrid embedding of statically typed
languages with significantly improved compile-time compared to macro-embedding
approaches. We describe our design patterns and propose new abstractions
packaging these patterns.

</details>


### [3] [What's in the Box: Ergonomic and Expressive Capture Tracking over Generic Data Structures (Extended Version)](https://arxiv.org/abs/2509.07609)
*Yichen Xu,Oliver Bračevac,Cao Nguyen Pham,Martin Odersky*

Main category: cs.PL

TL;DR: System Capless通过引入reach capabilities机制，解决了Scala捕获类型在泛型数据结构中能力跟踪的限制，使捕获检查能够扩展到标准集合库，实现了最小化语法开销的效果多态性。


<details>
  <summary>Details</summary>
Motivation: 现有的Scala捕获类型在跟踪嵌入泛型数据结构中的能力方面表达能力不足，无法扩展到标准集合库，这阻碍了其更广泛的应用。主要限制在于无法在系统的box类型概念中命名能力。

Method: 开发了System Capless计算系统，引入reach capabilities机制来命名"盒子里的内容"。该系统将通用能力概念细化为具有存在性和通用性捕获集量化的新方案，支持可选显式捕获集量化以提高表达能力。

Result: 在Scala 3中基于System Capless完全重新实现了捕获检查，并迁移了整个Scala集合库和异步编程库进行评估。结果表明reach capabilities能够在生产代码中以最小更改和最小到零的语法开销实现捕获检查。

Conclusion: System Capless为捕获类型提供了新的理论基础，通过reach capabilities机制成功解决了泛型数据结构中能力跟踪的限制，使捕获检查能够实际应用于生产环境，具有很好的实用性和人机工程学特性。

Abstract: Capturing types in Scala unify static effect and resource tracking with
object capabilities, enabling lightweight effect polymorphism with minimal
notational overhead. However, their expressiveness has been insufficient for
tracking capabilities embedded in generic data structures, preventing them from
scaling to the standard collections library -- an essential prerequisite for
broader adoption. This limitation stems from the inability to name capabilities
within the system's notion of box types.
  This paper develops System Capless, a new foundation for capturing types that
provides the theoretical basis for reach capabilities (rcaps), a novel
mechanism for naming "what's in the box." The calculus refines the universal
capability notion into a new scheme with existential and universal capture set
quantification. Intuitively, rcaps witness existentially quantified capture
sets inside the boxes of generic types in a way that does not require exposing
existential capture types in the surface language. We have fully mechanized the
formal metatheory of System Capless in Lean, including proofs of type soundness
and scope safety. System Capless supports the same lightweight notation of
capturing types plus rcaps, as certified by a type-preserving translation, and
also enables fully optional explicit capture-set quantification to increase
expressiveness.
  Finally, we present a full reimplementation of capture checking in Scala 3
based on System Capless and migrate the entire Scala collections library and an
asynchronous programming library to evaluate its practicality and ergonomics.
Our results demonstrate that reach capabilities enable the adoption of capture
checking in production code with minimal changes and minimal-to-zero notational
overhead in a vast majority of cases.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Crossword: Adaptive Consensus for Dynamic Data-Heavy Workloads](https://arxiv.org/abs/2509.07157)
*Guanzhou Hu,Yiwei Chen,Andrea Arpaci-Dusseau,Remzi Arpaci-Dusseau*

Main category: cs.DC

TL;DR: Crossword是一种灵活的共识协议，针对动态数据密集型工作负载设计，通过智能分片分配和擦除编码显著减少关键路径数据传输，在动态场景下性能提升2.3倍


<details>
  <summary>Details</summary>
Motivation: 云环境中复制负载大小跨度大且带来间歇性带宽压力，现有协议静态分片分配无法适应动态工作负载和网络条件

Method: 采用每实例擦除编码，智能分发编码分片，支持分片分配和仲裁大小的自适应权衡，使用惰性跟随者gossip机制处理领导者故障转移

Result: 在静态场景下与现有协议性能相当，在动态工作负载和网络条件下性能提升达2.3倍，与CockroachDB集成使TPC-C聚合吞吐量提高1.32倍

Conclusion: Crossword协议有效解决了动态数据密集型工作负载的共识挑战，在保持经典协议可用性保证的同时显著提升性能

Abstract: We present Crossword, a flexible consensus protocol for dynamic data-heavy
workloads, a rising challenge in the cloud where replication payload sizes span
a wide spectrum and introduce sporadic bandwidth stress. Crossword applies
per-instance erasure coding and distributes coded shards intelligently to
reduce critical-path data transfer significantly when desirable. Unlike
previous approaches that statically assign shards to servers, Crossword enables
an adaptive tradeoff between the assignment of shards and quorum size in
reaction to dynamic workloads and network conditions, while always retaining
the availability guarantee of classic protocols. Crossword handles leader
failover gracefully by employing a lazy follower gossiping mechanism that
incurs minimal impact on critical-path performance. We implement Crossword
(along with relevant protocols) in Gazette, a distributed, replicated, and
protocol-generic key-value store written in async Rust. We evaluate Crossword
comprehensively to show that it matches the best performance among previous
protocols (MultiPaxos, Raft, RSPaxos, and CRaft) in static scenarios, and
outperforms them by up to 2.3x under dynamic workloads and network conditions.
Our integration of Crossword with CockroachDB brings 1.32x higher aggregate
throughput to TPC-C under 5-way replication. We will open-source Gazette upon
publication.

</details>


### [5] [Bodega: Serving Linearizable Reads Locally from Anywhere at Anytime via Roster Leases](https://arxiv.org/abs/2509.07158)
*Guanzhou Hu,Andrea Arpaci-Dusseau,Remzi Arpaci-Dusseau*

Main category: cs.DC

TL;DR: Bodega是首个能在任何节点本地提供线性化读取的共识协议，通过新型roster leases算法实现，相比现有方法在WAN集群中读取速度提升5.6-13.1倍


<details>
  <summary>Details</summary>
Motivation: 解决现有共识协议无法在任何节点本地提供线性化读取的问题，特别是在存在写入干扰的情况下

Method: 采用roster leases算法（全对全租赁机制）、optimistic holding、early accept notifications、smart roster coverage和lightweight heartbeats等技术

Result: 在真实WAN集群中，相比现有方法在中等写入干扰下平均客户端读取请求速度提升5.6-13.1倍，写入性能相当，支持快速主动roster变更和容错

Conclusion: Bodega通过创新的roster leases机制成功实现了在任何节点本地提供线性化读取的目标，是共识协议设计空间的新突破

Abstract: We present Bodega, the first consensus protocol that serves linearizable
reads locally from any desired node, regardless of interfering writes. Bodega
achieves this via a novel roster leases algorithm that safeguards the roster, a
new notion of cluster metadata. The roster is a generalization of leadership;
it tracks arbitrary subsets of replicas as responder nodes for local reads. A
consistent agreement on the roster is established through roster leases, an
all-to-all leasing mechanism that generalizes existing all-to-one leasing
approaches (Leader Leases, Quorum Leases), unlocking a new point in the
protocol design space. Bodega further employs optimistic holding and early
accept notifications to minimize interruption from interfering writes, and
incorporates smart roster coverage and lightweight heartbeats to maximize
practicality. Bodega is a non-intrusive extension to classic consensus; it
imposes no special requirements on writes other than a responder-covering
quorum. We implement Bodega and related works in Vineyard, a protocol-generic
replicated key-value store written in async Rust. We compare it to previous
protocols (Leader Leases, EPaxos, PQR, and Quorum Leases) and two production
coordination services (etcd and ZooKeeper). Bodega speeds up average client
read requests by 5.6x-13.1x on real WAN clusters versus previous approaches
under moderate write interference, delivers comparable write performance,
supports fast proactive roster changes as well as fault tolerance via leases,
and closely matches the performance of sequentially-consistent etcd and
ZooKeeper deployments across all YCSB workloads. We will open-source Vineyard
upon publication.

</details>


### [6] [A Study on Messaging Trade-offs in Data Streaming for Scientific Workflows](https://arxiv.org/abs/2509.07199)
*Anjus George,Michael J. Brim,Christopher Zimmer,Tyler J. Skluzacek,A. J. Ruckman,Gustav R. Jansen,Sarp Oral*

Main category: cs.DC

TL;DR: 研究分析了消息传递参数配置对科学工作流内存数据流性能的影响，通过RabbitMQ框架模拟Deleria和LCLS工作流，揭示了可靠消息传输的吞吐量权衡和最佳配置实践


<details>
  <summary>Details</summary>
Motivation: 现代科学工作流需要近实时数据分析和实验控制，内存到内存数据流可消除文件传输延迟，但需要低延迟、高吞吐量的可靠数据传输，现有消息框架需要合理配置才能满足需求

Method: 使用RabbitMQ消息框架，基于OLCF的HPC数据流基础设施，通过合成工作负载模拟Deleria和LCLS两种代表性科学工作流，研究消息参数配置对流性能的影响

Result: 模拟实验揭示了多个关键观察结果和实践见解，帮助用户理解哪些配置最能满足其流工作负载需求，特别是可靠消息传输相关的吞吐量权衡

Conclusion: 消息框架的有效使用需要仔细调整配置参数，研究为科学工作流提供了实用的配置指导，有助于优化内存数据流性能

Abstract: Memory-to-memory data streaming is essential for modern scientific workflows
that require near real-time data analysis, experimental steering, and informed
decision-making during experiment execution. It eliminates the latency
bottlenecks associated with file-based transfers to parallel storage, enabling
rapid data movement between experimental facilities and HPC systems. These
tightly coupled experimental-HPC workflows demand low latency, high throughput,
and reliable data delivery to support on-the-fly analysis and timely feedback
for experimental control. Off-the-shelf messaging frameworks are increasingly
considered viable solutions for enabling such direct memory streaming due to
their maturity, broad adoption, and ability to abstract core messaging and
reliability functionalities from the application layer. However, effectively
meeting the workflows' requirements depends on utilizing the framework's
capabilities and carefully tuning its configurations.
  In this paper, we present a study that investigates the messaging parameters,
and their configuration choices that impact the streaming requirements of two
representative scientific workflows. We specifically characterize throughput
trade-offs associated with reliable message transmission for these workflows.
Our study is conducted through streaming simulations using synthetic workloads
derived from the Deleria and LCLS workflows, employing the RabbitMQ messaging
framework within the context of the Data Streaming to HPC infrastructure at
OLCF. Our simulations reveal several key observations and practical insights
that help users understand which configurations best meet the needs of their
streaming workloads.

</details>


### [7] [Optimizing Task Scheduling in Fog Computing with Deadline Awareness](https://arxiv.org/abs/2509.07378)
*Mohammad Sadegh Sirjani,Somayeh Sobati-Moghadam*

Main category: cs.DC

TL;DR: 提出RIGEO算法，结合改进金鹰优化和强化学习，在雾计算环境中优化IoT任务调度，降低能耗并提升服务质量


<details>
  <summary>Details</summary>
Motivation: IoT设备增长需要低延迟响应，雾计算面临资源分配和任务调度挑战，需要设计高效算法来降低能耗并满足任务截止时间要求

Method: 将雾节点按流量分为高低两类：低流量节点使用改进金鹰优化算法(IGEO)处理低截止时间任务，高流量节点使用强化学习(RL)处理高截止时间任务，组合成RIGEO算法

Result: 实验结果表明，相比现有先进算法，RIGEO在系统响应时间、截止时间违反总量、资源和系统能耗方面都有优化

Conclusion: RIGEO算法通过分类调度策略有效解决了雾计算环境中的IoT任务调度问题，实现了能耗优化和服务质量提升

Abstract: The rise of Internet of Things (IoT) devices has led to the development of
numerous applications that require quick responses and low latency. Fog
computing has emerged as a solution for processing these IoT applications, but
it faces challenges such as resource allocation and job scheduling. Therefore,
it is crucial to determine how to assign and schedule tasks on Fog nodes. A
well-designed job scheduling algorithm can help decrease energy usage and
improve response times for application requests. This work aims to schedule
tasks in IoT while minimizing the total energy consumption of nodes and
enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into
account task deadlines. Initially, this paper classifies the Fog nodes into two
categories based on their traffic level: low and high. It schedules
low-deadline tasks on low-traffic-level nodes using an Improved Golden Eagle
Optimization (IGEO) algorithm, an enhancement of the Golden Eagle Optimization
Algorithm that utilizes genetic operators for discretization. High-deadline
tasks are processed on high-traffic nodes using reinforcement learning (RL).
This combined approach is called the Reinforcement Improved Golden Eagle
Optimization (RIGEO) algorithm. Experimental results demonstrate that the
proposed algorithms optimize system response time, total deadline violation
time, and resource and system energy consumption compared to other
state-of-the-art algorithms.

</details>


### [8] [DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for Efficient MoE LLM Inference](https://arxiv.org/abs/2509.07379)
*Yuning Zhang,Grant Pinkert,Nan Yang,Yanli Li,Dong Yuan*

Main category: cs.DC

TL;DR: DuoServe-MoE是一个专门针对MoE模型推理的优化系统，通过分离prefill和decode阶段并采用不同的专家调度策略，显著降低了GPU内存使用并提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然通过稀疏激活专家分支保持了推理效率，但大量专家权重带来了巨大的GPU内存压力，特别是在单GPU服务器等资源受限环境中。同时，MoE推理包含prefill（密集激活）和decode（稀疏激活）两个不同阶段，统一调度策略会导致延迟和内存使用不理想。

Method: 提出DuoServe-MoE系统：1）明确分离prefill和decode阶段；2）prefill阶段使用双流CUDA管道，重叠专家权重预取和非MoE层计算；3）decode阶段使用离线训练的轻量级层级预测器预取最可能激活的专家，无需修改模型。

Result: 在4位Mixtral-8x7B和8x22B模型上的实验显示，DuoServe-MoE将端到端延迟提升了1.42到7.54倍，同时将峰值内存使用保持在完整模型大小的仅15%。

Conclusion: DuoServe-MoE通过针对MoE推理不同阶段的特性设计专门的调度策略，有效解决了内存压力和延迟问题，为资源受限环境下的MoE模型部署提供了高效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
a wide range of deep learning tasks. Mixture of Experts (MoE) further enhances
their capabilities by increasing model width through sparsely activated expert
branches, which keeps inference computation efficient. However, the large
number of expert weights introduces significant GPU memory pressure, especially
in resource-constrained environments such as single-GPU servers. More
importantly, MoE inference consists of two fundamentally different stages: a
prefill stage where most experts are activated densely, and a decode stage
where only a few experts are triggered sparsely. Treating these stages with a
uniform scheduling strategy often leads to suboptimal latency and memory usage.
To address this, we propose DuoServe-MoE, an inference serving system that
explicitly separates prefill and decode stages and applies tailored expert
scheduling strategies to each. In the prefill stage, DuoServe-MoE uses a
two-stream CUDA pipeline that overlaps expert weight prefetching with the
computation of non-MoE layers, limiting expert residency in GPU memory. In the
decode stage, a lightweight layer-level predictor trained offline from
activation traces is used to prefetch only the most likely activated experts,
without requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B
and 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to
7.54 times while keeping peak memory usage at only 15 percent of the full model
size.

</details>


### [9] [Dependency-Aware Execution Mechanism in Hyperledger Fabric Architecture](https://arxiv.org/abs/2509.07425)
*Sanyam Kaul,Manaswini Piduguralla,Gayathri Shreeya Patnala,Sathya Peri*

Main category: cs.DC

TL;DR: 为Hyperledger Fabric提出依赖感知执行模型，通过依赖标记、优化区块构建、DAG依赖表示和并行执行，显著提升吞吐量和降低拒绝率


<details>
  <summary>Details</summary>
Motivation: 解决Fabric在高负载下交易吞吐量低和拒绝率高的问题，特别是乐观并发控制和延迟验证导致的资源低效和竞争问题

Method: 依赖标记系统、优化区块构建、DAG依赖表示、并行执行独立交易、按DAG顺序处理依赖交易

Result: 在Hyperledger Fabric v2.5中测试显示，高竞争场景下吞吐量提升40%，拒绝率显著降低

Conclusion: 依赖感知调度和DAG执行能大幅提升Fabric可扩展性，同时保持与现有共识和智能合约层的兼容性

Abstract: Hyperledger Fabric is a leading permissioned blockchain framework for
enterprise use, known for its modular design and privacy features. While it
strongly supports configurable consensus and access control, Fabric can face
challenges in achieving high transaction throughput and low rejection rates
under heavy workloads. These performance limitations are often attributed to
endorsement, ordering, and validation bottlenecks. Further, optimistic
concurrency control and deferred validation in Fabric may lead to resource
inefficiencies and contention, as conflicting transactions are identified only
during the commit phase. To address these challenges, we propose a
dependency-aware execution model for Hyperledger Fabric. Our approach includes:
(a) a dependency flagging system during endorsement, marking transactions as
independent or dependent using a hashmap; (b) an optimized block construction
in the ordering service that prioritizes independent transactions; (c) the
incorporation of a Directed Acyclic Graph (DAG) within each block to represent
dependencies; and (d) parallel execution of independent transactions at the
committer, with dependent transactions processed according to DAG order.
Incorporated in Hyperledger Fabric v2.5, our framework was tested on workloads
with varying dependency levels and system loads. Results show up to 40% higher
throughput and significantly reduced rejection rates in high-contention
scenarios. This demonstrates that dependency-aware scheduling and DAG-based
execution can substantially enhance Fabric's scalability while remaining
compatible with its existing consensus and smart contract layers.

</details>


### [10] [DREAMS: Decentralized Resource Allocation and Service Management across the Compute Continuum Using Service Affinity](https://arxiv.org/abs/2509.07497)
*Hai Dinh-Tuan,Tien Hung Nguyen,Sanjeet Raj Pandey*

Main category: cs.DC

TL;DR: DREAMS是一个去中心化的微服务部署框架，通过Raft共识算法和成本效益投票实现跨计算域的协作优化，特别适用于现代制造业的动态计算需求。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统需要适应动态工作负载和定制化生产需求，传统集中式资源分配方案存在扩展性差、延迟瓶颈和单点故障等问题，需要新的去中心化解决方案。

Method: 提出DREAMS框架，在每个计算域部署自主代理，通过Raft共识算法进行全局协调，采用成本效益投票机制实现协作式微服务部署决策。

Result: DREAMS在现代制造环境中实现了全局优化的服务部署，具有高容错性。关键协调操作（如LDM注册和迁移投票）随域数量呈亚线性扩展，证明了框架的高效性和可扩展性。

Conclusion: DREAMS提供了一个响应迅速、保护隐私、容错性强的去中心化协调框架，特别适合计算连续体中多利益相关方场景的微服务部署优化。

Abstract: Modern manufacturing systems require adaptive computing infrastructures that
can respond to highly dynamic workloads and increasingly customized production
demands. The compute continuum emerges as a promising solution, enabling
flexible deployment of microservices across distributed, heterogeneous domains.
However, this paradigm also requires a novel approach to resource allocation
and service placement, as traditional centralized solutions struggle to scale
effectively, suffer from latency bottlenecks, and introduce single points of
failure. In this paper, we present DREAMS, a decentralized framework that
optimizes microservice placement decisions collaboratively across different
computational domains. At its core, DREAMS introduces agents that operate
autonomously within each domain while coordinating globally through a
Raft-based consensus algorithm and cost-benefit voting. This decentralized
architecture enables responsive, privacy-preserving, and fault-tolerant
coordination, making it particularly suitable given the growing prevalence of
multi-stakeholder scenarios across the compute continuum. In particular, within
modern manufacturing environments, DREAMS achieves globally optimized service
placements while maintaining high fault tolerance. Further evaluations
demonstrate that key coordination operations, such as Local Domain Manager
(LDM) registration and migration voting, scale sub-linearly with the number of
domains, confirming the efficiency and scalability of our proposal.

</details>


### [11] [Astra: A Multi-Agent System for GPU Kernel Performance Optimization](https://arxiv.org/abs/2509.07506)
*Anjiang Wei,Tianran Sun,Yogesh Seenichamy,Hang Song,Anne Ouyang,Azalia Mirhoseini,Ke Wang,Alex Aiken*

Main category: cs.DC

TL;DR: Astra是首个基于LLM的多智能体系统，用于GPU内核优化，从现有CUDA代码出发而非PyTorch模块，通过多智能体协作实现平均1.32倍加速


<details>
  <summary>Details</summary>
Motivation: GPU内核优化对LLM训练和服务至关重要，但传统方法需要大量手动调优，现有编译器系统仍需大量人工设计，需要更自动化的优化方案

Method: 构建多智能体LLM系统，各智能体通过迭代代码生成、测试、性能分析和规划协作，从SGLang框架提取的CUDA实现开始优化

Result: 在SGLang内核上使用OpenAI o4-mini零样本提示实现平均1.32倍加速，能够自主应用循环变换、优化内存访问模式、利用CUDA内置函数和快速数学运算

Conclusion: 多智能体LLM系统为GPU内核优化提供了有前景的新范式，能够自动实现显著的性能提升

Abstract: GPU kernel optimization has long been a central challenge at the intersection
of high-performance computing and machine learning. Efficient kernels are
crucial for accelerating large language model (LLM) training and serving, yet
attaining high performance typically requires extensive manual tuning.
Compiler-based systems reduce some of this burden, but still demand substantial
manual design and engineering effort. Recently, researchers have explored using
LLMs for GPU kernel generation, though prior work has largely focused on
translating high-level PyTorch modules into CUDA code. In this work, we
introduce Astra, the first LLM-based multi-agent system for GPU kernel
optimization. Unlike previous approaches, Astra starts from existing CUDA
implementations extracted from SGLang, a widely deployed framework for serving
LLMs, rather than treating PyTorch modules as the specification. Within Astra,
specialized LLM agents collaborate through iterative code generation, testing,
profiling, and planning to produce kernels that are both correct and
high-performance. On kernels from SGLang, Astra achieves an average speedup of
1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study
further demonstrates that LLMs can autonomously apply loop transformations,
optimize memory access patterns, exploit CUDA intrinsics, and leverage fast
math operations to yield substantial performance gains. Our work highlights
multi-agent LLM systems as a promising new paradigm for GPU kernel
optimization.

</details>


### [12] [Navigating Energy Doldrums: Modeling the Impact of Energy Price Volatility on HPC Cost of Ownership](https://arxiv.org/abs/2509.07567)
*Peter Arzt,Felix Wolf*

Main category: cs.DC

TL;DR: 本文提出了一个简单模型来评估HPC系统中可变容量策略对总拥有成本的影响，通过动态调整计算资源来应对电价波动。


<details>
  <summary>Details</summary>
Motivation: 能源成本是高性能计算系统总拥有成本的主要因素，绿色能源的间歇性和电价波动使得能源预算复杂化，需要寻找有效的成本管理策略。

Method: 开发了一个简单模型，使用关键系统参数帮助运营商估算可变容量策略对TCO的影响，并将该模型应用于大学HPC集群的实际数据。

Result: 模型能够评估不同情景下可变容量策略的成本效益，为HPC系统运营商提供决策支持。

Conclusion: 可变容量策略是管理HPC能源成本的有效方法，但需要在节能和硬件利用率之间找到平衡，该模型为未来成本效益评估提供了实用工具。

Abstract: Energy costs are a major factor in the total cost of ownership (TCO) for
high-performance computing (HPC) systems. The rise of intermittent green energy
sources and reduced reliance on fossil fuels have introduced volatility into
electricity markets, complicating energy budgeting. This paper explores
variable capacity as a strategy for managing HPC energy costs - dynamically
adjusting compute resources in response to fluctuating electricity prices.
While this approach can lower energy expenses, it risks underutilizing costly
hardware. To evaluate this trade-off, we present a simple model that helps
operators estimate the TCO impact of variable capacity strategies using key
system parameters. We apply this model to real data from a university HPC
cluster and assess how different scenarios could affect the cost-effectiveness
of this approach in the future.

</details>


### [13] [AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with FaaS-hosted MCP Services](https://arxiv.org/abs/2509.07595)
*Shiva Sai Krishna Anand Tokal,Vaibhav Jha,Anand Eswaran,Praveen Jayachandran,Yogesh Simmhan*

Main category: cs.DC

TL;DR: AgentX是一种新型智能体工作流模式，由阶段设计器、规划器和执行器智能体组成，在复杂多步骤任务中表现优于现有最先进的智能体模式。


<details>
  <summary>Details</summary>
Motivation: 生成式AI虽然强大，但智能体系统在面对众多工具、复杂多步骤任务和长上下文管理时经常遇到困难，需要更好的工作流模式来解决这些问题。

Method: 提出AgentX工作流模式，包含阶段设计器、规划器和执行器三个智能体组件，并利用模型上下文协议(MCP)工具，提出了两种将MCP服务器部署为云函数即服务(FaaS)的替代方法。

Result: 通过三个实际应用的实证评估，比较了AgentX与ReAct、Magentic One两种当代智能体模式的成功率、延迟和成本，证明了AgentX的竞争优势。

Conclusion: AgentX工作流模式在设计和部署智能体工作流方面展现出显著优势，为智能体系统的发展提供了新的机遇和挑战解决方案。

Abstract: Generative Artificial Intelligence (GenAI) has rapidly transformed various
fields including code generation, text summarization, image generation and so
on. Agentic AI is a recent evolution that further advances this by coupling the
decision making and generative capabilities of LLMs with actions that can be
performed using tools. While seemingly powerful, Agentic systems often struggle
when faced with numerous tools, complex multi-step tasks,and long-context
management to track history and avoid hallucinations. Workflow patterns such as
Chain-of-Thought (CoT) and ReAct help address this. Here, we define a novel
agentic workflow pattern, AgentX, composed of stage designer, planner, and
executor agents that is competitive or better than the state-of-the-art agentic
patterns. We also leverage Model Context Protocol (MCP) tools, and propose two
alternative approaches for deploying MCP servers as cloud Functions as a
Service (FaaS). We empirically evaluate the success rate, latency and cost for
AgentX and two contemporary agentic patterns, ReAct and Magentic One, using
these the FaaS and local MCP server alternatives for three practical
applications. This highlights the opportunities and challenges of designing and
deploying agentic workflows.

</details>


### [14] [Scaling atomic ordering in shared memory](https://arxiv.org/abs/2509.07781)
*Lorenzo Martignetti,Eliã Batista,Gianpaolo Cugola,Fernando Pedone*

Main category: cs.DC

TL;DR: TRAM是一个专为共享内存系统设计的原子多播协议，采用覆盖树架构，性能显著优于现有协议


<details>
  <summary>Details</summary>
Motivation: 原子多播是可靠系统中的关键通信原语，但现有协议主要针对消息传递系统，共享内存系统的高性能原子多播协议较少

Method: 基于覆盖树架构设计TRAM协议，专门针对共享内存系统进行优化

Result: 相比最先进的共享内存协议，吞吐量提升3倍以上，延迟降低2.3倍以上；相比消息传递协议，吞吐量提升最高5.9倍，延迟降低最高106倍

Conclusion: TRAM协议通过简单实用的设计，在共享内存系统中实现了卓越的性能表现，为集成复制和分片的关键服务提供了高效的原子多播解决方案

Abstract: Atomic multicast is a communication primitive used in dependable systems to
ensure consistent ordering of messages delivered to a set of replica groups.
This primitive enables critical services to integrate replication and sharding
(i.e., state partitioning) to achieve fault tolerance and scalability. While
several atomic multicast protocols have been developed for message-passing
systems, only a few are designed for the shared memory system model. This paper
introduces TRAM, an atomic multicast protocol specifically designed for shared
memory systems, leveraging an overlay tree architecture. Due to its simple and
practical design, TRAM delivers exceptional performance, increasing throughput
by more than 3$\times$ and reducing latency by more than 2.3$\times$ compared
to state-of-the-art shared memory-based protocols. Additionally, it
significantly outperforms message-passing-based protocols, boosting throughput
by up to 5.9$\times$ and reducing latency by up to 106$\times$.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [15] [HYLU: Hybrid Parallel Sparse LU Factorization](https://arxiv.org/abs/2509.07690)
*Xiaoming Chen*

Main category: cs.AR

TL;DR: HYLU是一个混合并行LU分解求解器，在共享内存多核架构上高效求解稀疏线性系统，相比Intel MKL PARDISO在数值分解阶段有显著性能提升


<details>
  <summary>Details</summary>
Motivation: 针对多核共享内存架构设计高效的稀疏线性系统求解器，需要适应不同稀疏模式的系数矩阵

Method: 采用混合数值核的集成方法，结合并行LU分解技术，能够自适应处理各种稀疏模式

Result: 在SuiteSparse矩阵集的34个稀疏矩阵测试中，数值分解阶段相比Intel MKL PARDISO有1.74倍（单次求解）和2.26倍（重复求解）的几何平均性能提升

Conclusion: HYLU是一个高效的通用求解器，在稀疏线性系统求解方面表现出色，代码已在GitHub开源

Abstract: This article introduces HYLU, a hybrid parallel LU factorization-based
general-purpose solver designed for efficiently solving sparse linear systems
(Ax=b) on multi-core shared-memory architectures. The key technical feature of
HYLU is the integration of hybrid numerical kernels so that it can adapt to
various sparsity patterns of coefficient matrices. Tests on 34 sparse matrices
from SuiteSparse Matrix Collection reveal that HYLU outperforms Intel MKL
PARDISO in the numerical factorization phase by geometric means of 1.74X (for
one-time solving) and 2.26X (for repeated solving). HYLU can be downloaded from
https://github.com/chenxm1986/hylu.

</details>
