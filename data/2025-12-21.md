<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2512.15766)
*Yijie Zhi,Yayu Cao,Jianhua Dai,Xiaoyang Han,Jingwen Pu,Qingran Wu,Sheng Cheng,Ming Cai*

Main category: cs.PL

TL;DR: LOOPRAG是一个基于检索增强生成的框架，通过参数驱动方法生成多样化循环变换示例，结合循环感知检索算法和反馈迭代机制，显著提升LLMs在循环优化上的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管循环变换是广泛使用的语义保持优化技术，但找到最优的循环变换组合仍然具有挑战性。现有的大型语言模型在循环优化方面表现不佳，经常产生错误或次优结果，错失性能提升机会。

Method: 提出LOOPRAG框架：1) 参数驱动方法利用循环属性触发各种循环变换，生成多样化但合法的示例代码；2) 基于循环特征的循环感知检索算法，平衡相似性和多样性；3) 反馈式迭代机制，结合编译、测试和性能结果作为反馈指导LLMs；4) 通过变异、覆盖和差分测试进行等价性检查。

Result: 在PolyBench、TSVC和LORE基准测试中，LOOPRAG相比基础编译器分别达到11.20×、14.34×和9.29×的平均加速比，相比基础LLMs分别达到11.97×、5.61×和11.59×的加速比。

Conclusion: LOOPRAG通过检索增强生成框架有效解决了LLMs在循环优化中的局限性，显著提升了循环变换优化的性能，为代码优化领域提供了新的解决方案。

Abstract: Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\times$, 14.34$\times$, and 9.29$\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\times$, 5.61$\times$, and 11.59$\times$.

</details>


### [2] [Automated Formalization of Probabilistic Requirements from Structured Natural Language](https://arxiv.org/abs/2512.15788)
*Anastasia Mavridou,Marie Farrell,Gricel Vázquez,Tom Pressburger,Timothy E. Wang,Radu Calinescu,Michael Fisher*

Main category: cs.PL

TL;DR: 扩展NASA的FRET工具，支持用结构化自然语言编写概率需求，并自动转换为概率时序逻辑公式，使自主自适应系统的形式化分析更实用


<details>
  <summary>Details</summary>
Motivation: 自主自适应系统在软件开发中面临重大挑战，特别是需要明确捕捉环境或决策过程中的不确定性。安全关键和任务关键系统需要严格审查，但用概率构造编写需求很困难，直接使用复杂的形式化逻辑不现实且容易出错

Method: 扩展NASA形式化需求获取工具（FRET）的结构化自然语言，支持概率需求的明确规范，开发自动方法将这些需求转换为逻辑公式。提出形式化、组合式、自动化的方法，将结构化自然语言需求转换为概率时序逻辑公式

Result: 扩展的FRET工具使开发人员能够用结构化自然语言指定概率需求，并自动转换为概率时序逻辑。通过自动化验证框架和形式化证明确保生成的公式结构良好且符合预期语义

Conclusion: 该方法使自主自适应系统的形式化分析更加实用且减少错误，通过结构化自然语言和自动化转换降低了形式化需求编写的门槛

Abstract: Integrating autonomous and adaptive behavior into software-intensive systems presents significant challenges for software development, as uncertainties in the environment or decision-making processes must be explicitly captured. These challenges are amplified in safety- and mission-critical systems, which must undergo rigorous scrutiny during design and development. Key among these challenges is the difficulty of specifying requirements that use probabilistic constructs to capture the uncertainty affecting these systems. To enable formal analysis, such requirements must be expressed in precise mathematical notations such as probabilistic logics. However, expecting developers to write requirements directly in complex formalisms is unrealistic and highly error-prone. We extend the structured natural language used by NASA's Formal Requirement Elicitation Tool (FRET) with support for the specification of unambiguous and correct probabilistic requirements, and develop an automated approach for translating these requirements into logical formulas. We propose and develop a formal, compositional, and automated approach for translating structured natural-language requirements into formulas in probabilistic temporal logic. To increase trust in our formalizations, we provide assurance that the generated formulas are well-formed and conform to the intended semantics through an automated validation framework and a formal proof. The extended FRET tool enables developers to specify probabilistic requirements in structured natural language, and to automatically translate them into probabilistic temporal logic, making the formal analysis of autonomous and adaptive systems more practical and less error-prone.

</details>


### [3] [A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning](https://arxiv.org/abs/2512.15816)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.PL

TL;DR: NeuroInv：一种结合神经推理和符号验证的神经符号方法，用于循环不变式生成，在150个Java程序上达到99.5%的成功率


<details>
  <summary>Details</summary>
Motivation: 循环不变式生成是自动化程序验证的关键瓶颈。现有基于大语言模型的方法缺乏可靠的结构化方法，且很少参考现有的程序验证理论。需要一种结合神经推理和符号验证的混合方法来解决这个问题。

Method: NeuroInv包含两个关键模块：(1) 神经推理模块：利用LLMs和霍尔逻辑，通过后向链式最弱前置条件推理来推导和精化候选不变式；(2) 验证引导的符号模块：使用OpenJML的反例迭代修复不变式。

Result: 在包含150个Java程序的综合基准测试中（包括单循环、多循环、多数组、随机分支和噪声代码段），NeuroInv达到99.5%的成功率，显著优于其他评估方法。在包含10个更大规模多循环程序（平均每个程序7个循环）的困难基准测试中也表现出色。

Conclusion: NeuroInv通过神经符号方法成功解决了循环不变式生成问题，结合了LLMs的推理能力和符号验证的可靠性，能够扩展到更复杂的验证场景。

Abstract: Loop invariant generation remains a critical bottleneck in automated program verification. Recent work has begun to explore the use of Large Language Models (LLMs) in this area, yet these approaches tend to lack a reliable and structured methodology, with little reference to existing program verification theory. This paper presents NeuroInv, a neurosymbolic approach to loop invariant generation. NeuroInv comprises two key modules: (1) a neural reasoning module that leverages LLMs and Hoare logic to derive and refine candidate invariants via backward-chaining weakest precondition reasoning, and (2) a verification-guided symbolic module that iteratively repairs invariants using counterexamples from OpenJML. We evaluate NeuroInv on a comprehensive benchmark of 150 Java programs, encompassing single and multiple (sequential) loops, multiple arrays, random branching, and noisy code segments. NeuroInv achieves a $99.5\%$ success rate, substantially outperforming the other evaluated approaches. Additionally, we introduce a hard benchmark of $10$ larger multi-loop programs (with an average of $7$ loops each); NeuroInv's performance in this setting demonstrates that it can scale to more complex verification scenarios.

</details>


### [4] [Optimizing Agentic Language Model Inference via Speculative Tool Calls](https://arxiv.org/abs/2512.15834)
*Daniel Nichols,Prajwal Singhania,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.PL

TL;DR: 该论文提出针对语言模型工具调用性能瓶颈的系统优化方案，通过推测工具调用和保持序列驻留来提升推理吞吐量


<details>
  <summary>Details</summary>
Motivation: 语言模型越来越依赖外部工具（文件搜索、代码执行、API调用等），但工具调用会引入推理过程中的性能瓶颈，影响LM代理的吞吐量

Method: 提出两种系统优化：1）推测工具调用，提前准备资源；2）强制序列在推理引擎中保持驻留以减少开销。还提出"工具缓存"API端点便于LM提供商采用

Result: 优化方案使LM代理推理的吞吐量提升数百token/秒，并提供理论分析指导最佳推测配置

Conclusion: 通过系统优化解决LM工具调用性能瓶颈，显著提升推理吞吐量，为LM提供商提供实用的优化方案和API设计建议

Abstract: Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new "tool cache" API endpoint to enable LM providers to easily adopt these optimizations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines](https://arxiv.org/abs/2512.16038)
*Eric Simon,Renato B. Hoffmann,Lucas Alf,Dalvan Griebler*

Main category: cs.DC

TL;DR: LOG.io是一个为分布式数据流水线设计的解决方案，支持正确的回滚恢复和细粒度数据溯源捕获，在存在慢速操作符和中等吞吐量场景下表现优于ABS协议。


<details>
  <summary>Details</summary>
Motivation: 分布式数据流水线需要可靠的故障恢复机制，现有解决方案如Flink的异步屏障快照(ABS)协议在特定场景下存在性能限制，特别是在处理非确定性操作符、外部系统交互和自定义代码时。

Method: LOG.io采用基于日志的回滚恢复协议，支持通用编程模型，允许非确定性操作符、外部系统交互和任意自定义代码。它是非阻塞的，支持失败操作符独立恢复而不中断其他活动操作符，并支持流水线执行期间的动态扩展。

Result: 在SAP Data Intelligence系统中的性能评估显示：当流水线存在慢速操作符且事件吞吐量中等时，LOG.io在正常处理中与ABS相当，在恢复时优于ABS。其他情况下ABS表现更好，但数据并行化能显著减少LOG.io的开销。细粒度数据溯源捕获的开销在所有实验中均低于1.5%。

Conclusion: LOG.io为分布式数据流水线提供了一种有效的回滚恢复和细粒度数据溯源解决方案，在特定场景下优于现有方法，并通过数据并行化减少了性能开销。

Abstract: This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming model, accommodating non-deterministic operators, interactions with external systems, and arbitrary custom code. It is non-blocking, allowing failed operators to recover independently without interrupting other active operators, thereby leveraging data parallelization, and it facilitates dynamic scaling of operators during pipeline execution. Performance evaluations, conducted within the SAP Data Intelligence system, compare LOG.io with the Asynchronous Barrier Snapshotting (ABS) protocol, originally implemented in Flink. Our experiments show that when there are straggler operators in a data pipeline and the throughput of events is moderate (e.g., 1 event every 100 ms), LOG.io performs as well as ABS during normal processing and outperforms ABS during recovery. Otherwise, ABS performs better than LOG.io for both normal processing and recovery. However, we show that in these cases, data parallelization can largely reduce the overhead of LOG.io while ABS does not improve. Finally, we show that the overhead of data lineage capture, at the granularity of the event and between any two operators in a pipeline, is marginal, with less than 1.5% in all our experiments.

</details>


### [6] [MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services](https://arxiv.org/abs/2512.16056)
*Lingfeng Tang,Daoping Zhang,Junjie Chen,Peihao Huang,Feng Jin,Chengguang Xu,Yuxin Chen,Feiqiang Sun,Guo Chen*

Main category: cs.DC

TL;DR: MMA通过多路径内存访问技术解决PCIe带宽瓶颈，提升GPU与主机内存间数据传输带宽，加速LLM推理和模型切换


<details>
  <summary>Details</summary>
Motivation: PCIe带宽已成为大语言模型性能的关键瓶颈，特别是在前缀缓存获取和模型切换场景中。当前异构协议（PCIe和NVLink）限制了主机内存与GPU之间的带宽，导致服务器内带宽利用率不足。

Method: 提出多路径内存访问（MMA）方案，首次实现GPU与主机内存间的高效多路径数据传输。通过动态库注入实现无缝部署，无需修改LLM应用代码。

Result: 在测试平台上，MMA显著提升GPU与内存间数据传输带宽，峰值带宽达245GB/s，相比原生单路径带宽提升4.62倍。端到端评估显示，MMA将LLM服务的首令牌时间（TTFT）降低1.14-2.38倍，并将vLLM睡眠模式下的模型切换延迟降低1.12-2.48倍。

Conclusion: MMA有效解决了PCIe带宽瓶颈问题，通过多路径数据传输显著提升LLM推理性能和模型切换效率，且部署简单无需代码修改。

Abstract: The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.

</details>


### [7] [Twinning for Space-Air-Ground-Sea Integrated Networks: Beyond Conventional Digital Twin Towards Goal-Oriented Semantic Twin](https://arxiv.org/abs/2512.16058)
*Yifei Qiu,Tianle Liao,Xin Jin,Shaohua Wu,Dusit Niyato,Qinyu Zhang*

Main category: cs.DC

TL;DR: 本文提出了一种面向目标的语义孪生（GOST）框架，作为传统数字孪生的替代方案，用于解决空间-空中-地面-海洋一体化网络中的计算开销、同步延迟和语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 传统数字孪生在SAGSIN环境中面临高保真、全尺度建模的局限性，包括计算开销过大、模型同步延迟和跨系统语义鸿沟等问题，需要一种更轻量级、任务导向的孪生框架。

Method: 提出面向目标的语义孪生（GOST）框架，包含三层：基于知识的语义层、数据驱动的语义层和面向目标的原则层。该框架优先考虑"效用"而非"保真度"，利用语义技术和目标导向原则构建轻量级、任务特定的表示。

Result: 通过远程卫星-无人机网络中的协同跟踪任务案例研究，证明GOST在感知数据时效性和协同跟踪性能方面显著优于传统数字孪生。

Conclusion: GOST作为一种变革性的孪生范式，通过语义技术和目标导向原则解决了传统数字孪生在SAGSIN中的局限性，为6G系统的发展提供了新的指导方向。

Abstract: A space-air-ground-sea integrated network (SAGSIN) has emerged as a cornerstone of 6G systems, establishing a unified global architecture by integrating multi-domain network resources. Motivated by the demand for real-time situational awareness and intelligent operational maintenance, digital twin (DT) technology was initially regarded as a promising solution, owing to its capability to create virtual replicas and emulate physical system behaviors. However, in the context of SAGSIN, the high-fidelity, full-scale modeling paradigm inherent to conventional DTs encounters fundamental limitations, including prohibitive computational overhead, delayed model synchronization, and cross-system semantic gaps. To address these limitations, this survey paper proposes a novel twinning framework: goal-oriented semantic twin (GOST). Unlike DTs that pursue physical mirroring, GOST prioritizes ``utility'' over ``fidelity,'' leveraging semantic technologies and goal-oriented principles to construct lightweight, task-specific representations. This paper systematically articulates the GOST framework through three layers: knowledge-based semantics, data-driven semantics, and goal-oriented principles. Furthermore, we provide a comprehensive tutorial on constructing GOST by detailing its core enabling technologies and introduce a multidimensional evaluation framework for GOST. We present a case study targeting collaborative tracking tasks in remote satellite-UAV networks, demonstrating that GOST significantly outperforms conventional DTs in timeliness of perceptual data and collaborative tracking. Finally, we outline research directions, establishing GOST as a transformative twinning paradigm to guide the development of SAGSIN.

</details>


### [8] [Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study](https://arxiv.org/abs/2512.16066)
*Syed Salauddin Mohammad Tariq,Foyzul Hassan,Amiangshu Bosu,Probir Roy*

Main category: cs.DC

TL;DR: 该论文提出将冷启动视为开发者可见的设计问题，通过分析81个问题报告建立反模式分类，开发了SCABENCH基准测试和INITSCOPE分析框架，显著提高了诊断准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算简化了部署和扩展，但冷启动延迟仍然是主要性能瓶颈。现有工作将缓解措施视为黑盒优化，而本文认为冷启动应作为开发者可见的设计问题来研究。

Method: 1. 分析81个开源无服务器系统的问题报告，建立初始化反模式、修复策略和诊断挑战的分类体系；2. 开发SCABENCH可复现基准测试；3. 提出INITSCOPE轻量级分析框架，将加载的代码与执行的代码关联起来。

Result: 在SCABENCH上，INITSCOPE相比现有工具将定位准确性提高了40%，诊断工作量减少了64%。开发者研究表明，使用该工具的任务准确性和诊断速度都有显著提升。

Conclusion: 该研究为无服务器设计中的冷启动缓解提供了基于证据、性能感知的实践方法，相关研究工件已公开可用，支持未来研究和改进。

Abstract: Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.

</details>


### [9] [An Online Fragmentation-Aware Scheduler for Managing GPU-Sharing Workloads on Multi-Instance GPUs](https://arxiv.org/abs/2512.16099)
*Hsu-Tzu Ting,Jerry Chou,Ming-Hung Chen,I-Hsin Chung*

Main category: cs.DC

TL;DR: 提出在线调度框架解决MIG GPU共享中的资源争用和碎片化问题，通过负载均衡、动态分区和作业迁移提升系统效率，makespan最高改善35%


<details>
  <summary>Details</summary>
Motivation: 现代GPU工作负载需要高效资源共享，但NVIDIA MIG虽然提供硬件级分区，仍存在PCIe带宽等共享组件导致的资源争用问题，以及由于MIG配置有限和刚性配置约束导致的GPU碎片化问题

Method: 提出在线调度框架，整合条件负载均衡、动态分区和作业迁移技术，动态调整作业放置以减少争用，并重新组织GPU分配以应对内部和外部碎片化

Result: 实验结果显示该方法显著提升系统效率，当所有技术都应用时，makespan最高改善35%

Conclusion: 提出的在线调度框架能有效解决MIG GPU共享中的资源争用和碎片化问题，显著提升GPU资源利用率和系统性能

Abstract: Modern GPU workloads increasingly demand efficient resource sharing, as many jobs do not require the full capacity of a GPU. Among sharing techniques, NVIDIA's Multi-Instance GPU (MIG) offers strong resource isolation by enabling hardware-level GPU partitioning. However, leveraging MIG effectively introduces new challenges. First, resource contention persists due to shared components such as PCIe bandwidth. Second, GPU fragmentation becomes a critical issue, which is different from prior fine-grained GPU sharing work due to MIG's limited number of valid MIG configurations. Fragmentation arises not only from spatial discontinuity but also from rigid profile placement constraints, especially after job arrivals and terminations. To address these issues, we propose an online scheduling framework that integrates conditional load balancing, dynamic partitioning, and job migration. Our approach dynamically adapts job placement to minimize contention and reorganizes GPU allocations to combat both internal and external fragmentation. Experimental results show that our method significantly improves system efficiency. When all techniques are applied, the makespan improves by up to 35%.

</details>


### [10] [Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference](https://arxiv.org/abs/2512.16134)
*Jian Tian,Shuailong Li,Yang Cao,Wenbo Cui,Minghan Zhu,Wenkang Wu,Jianming Zhang,Yanpeng Wang,Zhiwen Xiao,Zhenyu Hou,Dou Shen*

Main category: cs.DC

TL;DR: 提出Staggered Batch Scheduling (SBS)调度机制，通过缓冲请求形成最优执行批次，解决DP+EP架构中的内部排队问题，降低TTFT 30%-40%，提升吞吐量15%-20%


<details>
  <summary>Details</summary>
Motivation: LLM服务向复杂分布式架构（特别是P/D分离的大规模DP+EP范式）演进，带来了独特的调度挑战。DP+EP架构具有高内部同步成本，传统调度将实例视为黑盒的方法不再适用。立即请求调度会导致严重的引擎内部排队和并行化气泡，降低首词延迟(TTFT)。

Method: 提出Staggered Batch Scheduling (SBS)机制，通过故意缓冲请求来形成最优执行批次，实现时间解耦，消除内部排队气泡。同时利用缓冲创建的调度窗口，引入负载感知全局分配策略，平衡DP单元在Prefill和Decode阶段的计算负载。

Result: 在生产H800集群上部署服务Deepseek-V3，相比最先进的立即调度基线，系统将TTFT降低了30%-40%，吞吐量提升了15%-20%。

Conclusion: SBS调度机制有效解决了DP+EP架构中的调度挑战，通过缓冲和负载感知调度显著改善了LLM服务的延迟和吞吐性能，为复杂分布式LLM服务提供了实用的调度解决方案。

Abstract: The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.

</details>


### [11] [Lotus: Optimizing Disaggregated Transactions with Disaggregated Locks](https://arxiv.org/abs/2512.16136)
*Zhisheng Hu,Pengfei Zuo,Junliang Hu,Yizou Chen,Yingjia Wang,Ming-Chang Yang*

Main category: cs.DC

TL;DR: Lotus提出了一种在分解内存架构上实现锁分解的可扩展分布式事务系统，通过将锁管理从内存节点转移到计算节点来消除RDMA网络接口卡瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有分解内存上的分布式事务系统中，内存节点的RDMA网络接口卡成为主要性能瓶颈，这源于大量用于锁的单边原子操作，限制了系统扩展能力。

Method: 1) 将锁从数据中分解出来，所有锁操作在计算节点执行；2) 应用感知的锁管理机制，利用OLTP工作负载的局部性进行锁分片并保持负载均衡；3) 锁优先事务协议，将锁获取作为读写事务执行的第一步；4) 无锁重建的故障恢复机制，将锁视为临时状态避免重建。

Result: 实验结果显示，与现有分解内存上的事务系统相比，Lotus将事务吞吐量提升最高2.1倍，延迟降低最高49.4%。

Conclusion: Lotus通过锁分解有效解决了分解内存架构中RDMA网络接口卡的瓶颈问题，实现了可扩展的分布式事务处理，显著提升了系统性能。

Abstract: Disaggregated memory (DM) separates compute and memory resources, allowing flexible scaling to achieve high resource utilization. To ensure atomic and consistent data access on DM, distributed transaction systems have been adapted, where compute nodes (CNs) rely on one-sided RDMA operations to access remote data in memory nodes (MNs). However, we observe that in existing transaction systems, the RDMA network interface cards at MNs become a primary performance bottleneck. This bottleneck arises from the high volume of one-sided atomic operations used for locks, which hinders the system's ability to scale efficiently.
  To address this issue, this paper presents Lotus, a scalable distributed transaction system with lock disaggregation on DM. The key innovation of Lotus is to disaggregate locks from data and execute all locks on CNs, thus eliminating the bottleneck at MN RNICs. To achieve efficient lock management on CNs, Lotus employs an application-aware lock management mechanism that leverages the locality of the OLTP workloads to shard locks while maintaining load balance. To ensure consistent transaction processing with lock disaggregation, Lotus introduces a lock-first transaction protocol, which separates the locking phase as the first step in each read-write transaction execution. This protocol allows the system to determine the success of lock acquisitions early and proactively abort conflicting transactions, improving overall efficiency. To tolerate lock loss during CN failures, Lotus employs a lock-rebuild-free recovery mechanism that treats locks as ephemeral and avoids their reconstruction, ensuring lightweight recovery for CN failures. Experimental results demonstrate that Lotus improves transaction throughput by up to 2.1$\times$ and reduces latency by up to 49.4% compared to state-of-the-art transaction systems on DM.

</details>


### [12] [FlexKV: Flexible Index Offloading for Memory-Disaggregated Key-Value Store](https://arxiv.org/abs/2512.16148)
*Zhisheng Hu,Jiacheng Shen,Ming-Chang Yang*

Main category: cs.DC

TL;DR: FlexKV通过索引代理技术，在内存解耦架构中动态将索引卸载到计算节点，解决现有KV存储性能瓶颈，提升吞吐量2.94倍，降低延迟85.2%


<details>
  <summary>Details</summary>
Motivation: 现有内存解耦KV存储存在两个关键问题：1) 索引处理过度依赖单边原子操作，2) 计算侧缓存效率受限，导致性能不佳

Method: 提出FlexKV，采用索引代理技术，包含三个核心机制：1) 基于秩的热度检测算法平衡负载，2) 两级计算节点内存优化方案，3) RPC聚合缓存管理机制降低一致性开销

Result: 相比最先进的内存解耦KV存储，FlexKV将吞吐量提升最高2.94倍，延迟降低最高85.2%

Conclusion: FlexKV通过索引代理有效解决了内存解耦KV存储的性能瓶颈，证明了动态索引卸载到计算节点的可行性

Abstract: Disaggregated memory (DM) is a promising data center architecture that decouples CPU and memory into independent resource pools to improve resource utilization. Building on DM, memory-disaggregated key-value (KV) stores are adopted to efficiently manage remote data. Unfortunately, existing approaches suffer from poor performance due to two critical issues: 1) the overdependence on one-sided atomic operations in index processing, and 2) the constrained efficiency in compute-side caches. To address these issues, we propose FlexKV, a memory-disaggregated KV store with index proxying. Our key idea is to dynamically offload the index to compute nodes, leveraging their powerful CPUs to accelerate index processing and maintain high-performance compute-side caches. Three challenges have to be addressed to enable efficient index proxying on DM, i.e., the load imbalance across compute nodes, the limited memory of compute nodes, and the expensive cache coherence overhead. FlexKV proposes: 1) a rank-aware hotness detection algorithm to continuously balance index load across compute nodes, 2) a two-level CN memory optimization scheme to efficiently utilize compute node memory, and 3) an RPC-aggregated cache management mechanism to reduce cache coherence overhead. The experimental results show that FlexKV improves throughput by up to 2.94$\times$ and reduces latency by up to 85.2%, compared with the state-of-the-art memory-disaggregated KV stores.

</details>


### [13] [AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research](https://arxiv.org/abs/2512.16455)
*Ignacio Heredia,Álvaro López García,Germán Moltó,Amanda Calatrava,Valentin Kozlov,Alessandro Costantini,Viet Tran,Mario David,Daniel San Martín,Marcin Płóciennik,Marta Obregón Ruiz,Saúl Fernandez,Judith Sáinz-Pardo Díaz,Miguel Caballer,Caterina Alarcón Marín,Stefan Dlugolinsky,Martin Šeleng,Lisana Berberi,Khadijeh Alibabaei,Borja Esteban Sanchis,Pedro Castro,Giacinto Donvito,Diego Aguirre,Sergio Langarita,Vicente Rodriguez,Leonhard Duda,Andrés Heredia Canales,Susana Rebolledo Ruiz,João Machado,Giang Nguyen,Fernando Aguilar Gómez,Jaime Díez*

Main category: cs.DC

TL;DR: 本文介绍了一个专门支持科学工作负载中人工智能的联邦计算平台，提供从模型开发、训练到部署的全生命周期集成体验，并注重可重复性和可定制性。


<details>
  <summary>Details</summary>
Motivation: 为科学工作负载中的人工智能应用提供一个可重复部署、透明访问的联邦计算平台，解决分布式e-基础设施的集成问题，降低外部社区采用门槛。

Method: 构建一个联邦计算平台，通过全面的服务目录提供完整的机器学习生命周期支持，包括交互式开发环境、GPU训练资源、注释工具、实验跟踪、联邦学习支持，以及覆盖云连续体的多种部署选项。

Result: 平台能够提供一致、透明的分布式e-基础设施访问，集成AI模型提供商、数据集和存储资源，提供AI模型的可追溯性和可重复性工具，并与更广泛的机器学习生态系统交互。

Conclusion: 该平台成功构建了一个支持科学AI工作负载的联邦计算解决方案，通过可定制性降低采用门槛，为科学社区提供完整的机器学习生命周期支持。

Abstract: In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.

</details>


### [14] [Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems](https://arxiv.org/abs/2512.16473)
*En-Ming Huang,Li-Shang Lin,Chun-Yi Lee*

Main category: cs.DC

TL;DR: 提出了一种用于MoE模型的CPU-GPU协同推理框架，通过GPU专家缓存机制减少数据传输，利用CPU多线程处理缓存未命中，在消费级硬件上实现更快的推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE模型通过选择性激活参数子集降低了计算需求，但最先进的MoE模型仍需要超出消费级GPU容量的内存。传统的卸载方法在CPU和GPU之间传输模型权重会引入延迟，限制了推理性能。

Method: 提出新颖的CPU-GPU协同推理框架，包含GPU上的专家缓存机制以减少数据传输需求，通过缓存命中实现更快推理。将计算卸载到CPU以高效处理缓存未命中，并利用CPU多线程优化。

Result: 评估显示该框架带来了性能改进，突显了CPU-GPU协作在消费级系统单请求推理场景中最大化硬件利用的潜力。

Conclusion: CPU-GPU协同推理框架通过专家缓存机制和CPU多线程优化，有效解决了MoE模型在消费级硬件上的部署挑战，实现了更高效的推理性能。

Abstract: Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.

</details>


### [15] [Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint](https://arxiv.org/abs/2512.16792)
*Endar Suprih Wihidayat,Sieteng Soh,Kwan-Wu Chin,Duc-son Pham*

Main category: cs.DC

TL;DR: 提出了多阶段边缘服务器升级（M-ESU）问题，通过混合整数线性规划和启发式算法解决多阶段MEC系统升级规划，在预算约束下最大化满足延迟需求的任务数量。


<details>
  <summary>Details</summary>
Motivation: 现有的多接入边缘计算（MEC）系统需要长期规划升级，以应对任务增长、任务规模增大和延迟要求更严格等挑战。传统方法缺乏多阶段规划框架，无法同时考虑服务器部署、升级和任务卸载决策。

Method: 提出M-ESU框架，包含两个关键决策：1）是否部署新服务器或升级现有服务器；2）如何卸载任务以最大化满足延迟需求的任务数量。提供两种解决方案：精确的混合整数线性规划（MILP）模型和高效的启发式算法M-ESU/H。

Result: 对于小型网络，M-ESU/H算法的解与最优解差距在1.25%以内，且运行速度快几个数量级。对于大型网络，相比仅考虑服务器部署或优先部署/升级的三种替代启发式算法，M-ESU/H在相同预算和需求增长条件下，任务满意度提升高达21.57%。

Conclusion: M-ESU框架为MEC系统的长期规划提供了有效的解决方案，M-ESU/H算法在保持接近最优性能的同时具有出色的可扩展性，适用于实际大规模网络部署。

Abstract: In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [16] [Workload Characterization for Branch Predictability](https://arxiv.org/abs/2512.15827)
*FNU Vikas,Paul Gratz,Daniel Jiménez*

Main category: cs.AR

TL;DR: 提出基于分支工作集大小和可预测性的分支预测性能分析方法，通过分析2451个负载轨迹，建立分支预测准确性与工作集特征的关联。


<details>
  <summary>Details</summary>
Motivation: 分支预测作为模式识别问题，准确预测能减少错误路径执行，提升性能和能效。现有研究缺乏系统的工作负载特征分析方法来理解分支预测准确性的根本来源。

Method: 提出分支预测工作负载特征化方法，定义两个新指标：分支工作集大小（基于分支地址、全局历史和局部历史的三元组）和分支可预测性。分析2451个负载轨迹，将其分为7个工作集大小类别和9个可预测性类别。

Result: 分支工作集大小和可预测性与现代分支预测方案（如TAGE和感知器）的误预测率高度相关。通过系统分类揭示了分支预测准确性的来源，为现代分支预测器提供了偏好的工作负载类别分析。

Conclusion: 提出的工作负载特征化方法能有效分析分支预测性能，分支工作集大小和可预测性是指示预测准确性的关键参数，为分支预测器设计和优化提供了新的分析框架。

Abstract: Conditional branch prediction predicts the likely direction of a conditional branch instruction to support ILP extraction. Branch prediction is a pattern recognition problem that learns mappings between a context to the branch outcome. An accurate predictor reduces the number of instructions executed on the wrong path resulting in an improvement of performance and energy consumption. In this paper, we present a workload characterization methodology for branch prediction. We propose two new workload-driven branch prediction accuracy identifiers -- branch working set size and branch predictability. These parameters are highly correlated with misprediction rates of modern branch prediction schemes (e.g. TAGE and perceptron). We define the branch working set of a trace as a group of most frequently occurring branch contexts, i.e. the 3-part tuple of branch address, and associated global and local history. We analyze the branch working set's size and predictability on a per-trace basis to study its relationship with a modern branch predictor's accuracy. We have characterized 2,451 workload traces into seven branch working set size and nine predictability categories after analyzing their branch behavior. We present further insights into the source of prediction accuracy and favored workload categories for modern branch predictors.

</details>


### [17] [Full System Architecture Modeling for Wearable Egocentric Contextual AI](https://arxiv.org/abs/2512.16045)
*Vincent T. Lee,Tanfer Alan,Sung Kim,Ecenur Ustun,Amr Suleiman,Ajit Krisshna,Tim Balbekov,Armin Alaghi,Richard Newcombe*

Main category: cs.AR

TL;DR: 本文介绍了面向下一代情境AI的可穿戴系统设计挑战，提出了Aria2系统架构，强调端到端全系统建模的重要性，以避免功耗瓶颈并实现全天候可穿戴情境AI。


<details>
  <summary>Details</summary>
Motivation: 下一代人机交互需要全天候、空间感知的可穿戴设备来捕获自我中心视觉和功能原语，构建用户个人情境，结合生成式AI实现强大的情境AI助手。但由于系统复杂性和严格的功耗限制（重量和电池限制），设计这样的可穿戴系统极具挑战性。

Method: 提出了首个完整的可穿戴情境AI系统（Aria2）架构视图，通过系统建模和设计空间探索过程，展示了端到端全系统模型的重要性。强调需要在全系统上下文中进行长期设计决策和功耗优化，避免因其他系统瓶颈（即应用于功耗的阿姆达尔定律）或瓶颈变化而遇到限制。

Result: 研究表明，对于这类可穿戴情境AI系统，端到端全系统视图至关重要，因为没有单一组件或类别会完全主导系统功耗。这意味着需要在全系统上下文中进行长期设计决策和功耗优化，以避免遇到由其他系统瓶颈或瓶颈变化引起的限制。

Conclusion: 本文提供了对未来道路的反思和见解，这些对于最终实现全天候、可穿戴、情境AI系统非常重要。强调了全系统建模在设计复杂可穿戴AI系统中的关键作用，以及避免功耗瓶颈的系统级优化策略。

Abstract: The next generation of human-oriented computing will require always-on, spatially-aware wearable devices to capture egocentric vision and functional primitives (e.g., Where am I? What am I looking at?, etc.). These devices will sense an egocentric view of the world around us to observe all human-relevant signals across space and time to construct and maintain a user's personal context. This personal context, combined with advanced generative AI, will unlock a powerful new generation of contextual AI personal assistants and applications. However, designing a wearable system to support contextual AI is a daunting task because of the system's complexity and stringent power constraints due to weight and battery restrictions. To understand how to guide design for such systems, this work provides the first complete system architecture view of one such wearable contextual AI system (Aria2), along with the lessons we have learned through the system modeling and design space exploration process. We show that an end-to-end full system model view of such systems is vitally important, as no single component or category overwhelmingly dominates system power. This means long-range design decisions and power optimizations need to be made in the full system context to avoid running into limits caused by other system bottlenecks (i.e., Amdahl's law as applied to power) or as bottlenecks change. Finally, we reflect on lessons and insights for the road ahead, which will be important toward eventually enabling all-day, wearable, contextual AI systems.

</details>
