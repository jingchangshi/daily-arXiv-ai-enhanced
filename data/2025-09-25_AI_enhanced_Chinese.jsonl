{"id": "2509.19478", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19478", "abs": "https://arxiv.org/abs/2509.19478", "authors": ["Ziwei Wang", "Cong Wu", "Paolo Tasca"], "title": "Investigating Sharding Advancements, Methodologies, and Adoption Potential in Hedera", "comment": null, "summary": "Sharding has emerged as a critical solution to address the scalability\nchallenges faced by blockchain networks, enabling them to achieve higher\ntransaction throughput, reduced latency, and optimized resource usage. This\npaper investigates the advancements, methodologies, and adoption potential of\nsharding in the context of Hedera, a distributed ledger technology known for\nits unique Gossip about Gossip protocol and asynchronous Byzantine Fault\nTolerance (ABFT). We explore various academic and industrial sharding\ntechniques, emphasizing their benefits and trade-offs. Building on these\ninsights, we propose a hybrid sharding solution for Hedera that partitions the\nnetwork into local and global committees, facilitating efficient cross-shard\ntransactions and ensuring robust security through dynamic reconfiguration. Our\nanalysis highlights significant reductions in storage and communication\noverhead, improved scalability, and enhanced fault tolerance, demonstrating the\nfeasibility and advantages of integrating sharding into Hedera's architecture.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5206\u7247\u6280\u672f\u5728Hedera\u533a\u5757\u94fe\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5206\u7247\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5c06\u7f51\u7edc\u5212\u5206\u4e3a\u672c\u5730\u548c\u5168\u5c40\u59d4\u5458\u4f1a\u6765\u63d0\u9ad8\u4ea4\u6613\u541e\u5410\u91cf\u3001\u964d\u4f4e\u5ef6\u8fdf\u5e76\u4f18\u5316\u8d44\u6e90\u4f7f\u7528\u3002", "motivation": "\u89e3\u51b3\u533a\u5757\u94fe\u7f51\u7edc\u9762\u4e34\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u63d0\u9ad8\u4ea4\u6613\u541e\u5410\u91cf\u3001\u964d\u4f4e\u5ef6\u8fdf\u5e76\u4f18\u5316\u8d44\u6e90\u4f7f\u7528\uff0c\u7279\u522b\u662f\u5728Hedera\u8fd9\u79cd\u91c7\u7528Gossip about Gossip\u534f\u8bae\u548c\u5f02\u6b65\u62dc\u5360\u5ead\u5bb9\u9519\uff08ABFT\uff09\u7684\u5206\u5e03\u5f0f\u8d26\u672c\u6280\u672f\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5206\u7247\u89e3\u51b3\u65b9\u6848\uff0c\u5c06\u7f51\u7edc\u5212\u5206\u4e3a\u672c\u5730\u548c\u5168\u5c40\u59d4\u5458\u4f1a\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u8de8\u5206\u7247\u4ea4\u6613\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u91cd\u914d\u7f6e\u786e\u4fdd\u5f3a\u5927\u7684\u5b89\u5168\u6027\u3002", "result": "\u5206\u6790\u663e\u793a\u5b58\u50a8\u548c\u901a\u4fe1\u5f00\u9500\u663e\u8457\u51cf\u5c11\uff0c\u53ef\u6269\u5c55\u6027\u5f97\u5230\u6539\u5584\uff0c\u5bb9\u9519\u80fd\u529b\u589e\u5f3a\uff0c\u8bc1\u660e\u4e86\u5c06\u5206\u7247\u6280\u672f\u96c6\u6210\u5230Hedera\u67b6\u6784\u4e2d\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\u3002", "conclusion": "\u5206\u7247\u6280\u672f\u53ef\u4ee5\u6709\u6548\u5730\u96c6\u6210\u5230Hedera\u7684\u67b6\u6784\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u5176\u6027\u80fd\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b89\u5168\u6027\uff0c\u4e3a\u533a\u5757\u94fe\u7f51\u7edc\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19790", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.19790", "abs": "https://arxiv.org/abs/2509.19790", "authors": ["Anthony Faure-Gignoux", "Kevin Delmas", "Adrien Gauffriau", "Claire Pagetti"], "title": "Open-source Stand-Alone Versatile Tensor Accelerator", "comment": null, "summary": "Machine Learning (ML) applications demand significant computational\nresources, posing challenges for safety-critical domains like aeronautics. The\nVersatile Tensor Accelerator (VTA) is a promising FPGA-based solution, but its\nadoption was hindered by its dependency on the TVM compiler and by other code\nnon-compliant with certification requirements. This paper presents an\nopen-source, standalone Python compiler pipeline for the VTA, developed from\nscratch and designed with certification requirements, modularity, and\nextensibility in mind. The compiler's effectiveness is demonstrated by\ncompiling and executing LeNet-5 Convolutional Neural Network (CNN) using the\nVTA simulators, and preliminary results indicate a strong potential for scaling\nits capabilities to larger CNN architectures. All contributions are publicly\navailable.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u72ec\u7acbPython\u7f16\u8bd1\u5668\u7ba1\u9053\uff0c\u7528\u4e8eVersatile Tensor Accelerator (VTA)\uff0c\u89e3\u51b3\u4e86VTA\u5bf9TVM\u7f16\u8bd1\u5668\u7684\u4f9d\u8d56\u548c\u4ee3\u7801\u4e0d\u7b26\u5408\u8ba4\u8bc1\u8981\u6c42\u7684\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5e94\u7528\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u5728\u822a\u7a7a\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u9762\u4e34\u6311\u6218\u3002VTA\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684FPGA\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u91c7\u7528\u53d7\u5230\u5bf9TVM\u7f16\u8bd1\u5668\u7684\u4f9d\u8d56\u4ee5\u53ca\u4ee3\u7801\u4e0d\u7b26\u5408\u8ba4\u8bc1\u8981\u6c42\u7684\u9650\u5236\u3002", "method": "\u4ece\u5934\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u7684\u72ec\u7acbPython\u7f16\u8bd1\u5668\u7ba1\u9053\uff0c\u8bbe\u8ba1\u65f6\u8003\u8651\u4e86\u8ba4\u8bc1\u8981\u6c42\u3001\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002\u4f7f\u7528VTA\u6a21\u62df\u5668\u7f16\u8bd1\u548c\u6267\u884cLeNet-5\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u9a8c\u8bc1\u7f16\u8bd1\u5668\u6548\u679c\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\u8be5\u7f16\u8bd1\u5668\u5177\u6709\u6269\u5c55\u5230\u66f4\u5927CNN\u67b6\u6784\u7684\u5f3a\u5927\u6f5c\u529b\u3002\u6240\u6709\u8d21\u732e\u90fd\u5df2\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u7f16\u8bd1\u5668\u7ba1\u9053\u6210\u529f\u89e3\u51b3\u4e86VTA\u7684\u91c7\u7528\u969c\u788d\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684FPGA\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19532", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.19532", "abs": "https://arxiv.org/abs/2509.19532", "authors": ["Flavio Castro", "Weijian Zheng", "Joaquin Chung", "Ian Foster", "Rajkumar Kettimuthu"], "title": "To Stream or Not to Stream: Towards A Quantitative Model for Remote HPC Processing Decisions", "comment": null, "summary": "Modern scientific instruments generate data at rates that increasingly exceed\nlocal compute capabilities and, when paired with the staging and I/O overheads\nof file-based transfers, also render file-based use of remote HPC resources\nimpractical for time-sensitive analysis and experimental steering. Real-time\nstreaming frameworks promise to reduce latency and improve system efficiency,\nbut lack a principled way to assess their feasibility. In this work, we\nintroduce a quantitative framework and an accompanying Streaming Speed Score to\nevaluate whether remote high-performance computing (HPC) resources can provide\ntimely data processing compared to local alternatives. Our model incorporates\nkey parameters including data generation rate, transfer efficiency, remote\nprocessing power, and file input/output overhead to compute total processing\ncompletion time and identify operational regimes where streaming is beneficial.\nWe motivate our methodology with use cases from facilities such as APS, FRIB,\nLCLS-II, and the LHC, and validate our approach through an illustrative case\nstudy based on LCLS-II data. Our measurements show that streaming can achieve\nup to 97% lower end-to-end completion time than file-based methods under high\ndata rates, while worst-case congestion can increase transfer times by over an\norder of magnitude, underscoring the importance of tail latency in streaming\nfeasibility decisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9a\u91cf\u6846\u67b6\u548cStreaming Speed Score\u6765\u8bc4\u4f30\u8fdc\u7a0bHPC\u8d44\u6e90\u662f\u5426\u80fd\u591f\u6bd4\u672c\u5730\u66ff\u4ee3\u65b9\u6848\u63d0\u4f9b\u53ca\u65f6\u7684\u6570\u636e\u5904\u7406\uff0c\u7279\u522b\u9488\u5bf9\u9ad8\u6570\u636e\u751f\u6210\u7387\u7684\u79d1\u5b66\u4eea\u5668\u573a\u666f\u3002", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u4eea\u5668\u751f\u6210\u6570\u636e\u7684\u901f\u5ea6\u8d85\u8fc7\u4e86\u672c\u5730\u8ba1\u7b97\u80fd\u529b\uff0c\u800c\u57fa\u4e8e\u6587\u4ef6\u7684\u4f20\u8f93\u65b9\u5f0f\u5728\u65f6\u95f4\u654f\u611f\u7684\u5206\u6790\u548c\u5b9e\u9a8c\u5f15\u5bfc\u4e2d\u53d8\u5f97\u4e0d\u5b9e\u7528\u3002\u5b9e\u65f6\u6d41\u5f0f\u5904\u7406\u6846\u67b6\u627f\u8bfa\u964d\u4f4e\u5ef6\u8fdf\u548c\u63d0\u9ad8\u7cfb\u7edf\u6548\u7387\uff0c\u4f46\u7f3a\u4e4f\u8bc4\u4f30\u5176\u53ef\u884c\u6027\u7684\u539f\u5219\u6027\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9a\u91cf\u6846\u67b6\uff0c\u5305\u542b\u6570\u636e\u751f\u6210\u7387\u3001\u4f20\u8f93\u6548\u7387\u3001\u8fdc\u7a0b\u5904\u7406\u80fd\u529b\u548c\u6587\u4ef6I/O\u5f00\u9500\u7b49\u5173\u952e\u53c2\u6570\uff0c\u8ba1\u7b97\u603b\u5904\u7406\u5b8c\u6210\u65f6\u95f4\uff0c\u5e76\u8bc6\u522b\u6d41\u5f0f\u5904\u7406\u6709\u5229\u7684\u64cd\u4f5c\u533a\u57df\u3002", "result": "\u6d4b\u91cf\u663e\u793a\uff0c\u5728\u9ad8\u6570\u636e\u901f\u7387\u4e0b\uff0c\u6d41\u5f0f\u5904\u7406\u53ef\u4ee5\u5b9e\u73b0\u6bd4\u57fa\u4e8e\u6587\u4ef6\u7684\u65b9\u6cd5\u4f4e97%\u7684\u7aef\u5230\u7aef\u5b8c\u6210\u65f6\u95f4\uff0c\u4f46\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u62e5\u585e\u53ef\u80fd\u4f7f\u4f20\u8f93\u65f6\u95f4\u589e\u52a0\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\u3002", "conclusion": "\u6d41\u5f0f\u5904\u7406\u5728\u9ad8\u6570\u636e\u901f\u7387\u573a\u666f\u4e0b\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u5c3e\u90e8\u5ef6\u8fdf\u5728\u6d41\u5f0f\u5904\u7406\u53ef\u884c\u6027\u51b3\u7b56\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7efc\u5408\u8003\u8651\u5404\u79cd\u64cd\u4f5c\u6761\u4ef6\u3002"}}
{"id": "2509.19873", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.19873", "abs": "https://arxiv.org/abs/2509.19873", "authors": ["Linfeng Zhong", "Songqiang Xu", "Huifeng Wen", "Tong Xie", "Qingyu Guo", "Yuan Wang", "Meng Li"], "title": "SpecMamba: Accelerating Mamba Inference on FPGA with Speculative Decoding", "comment": "Accepted by ICCAD'25", "summary": "The growing demand for efficient long-sequence modeling on edge devices has\npropelled widespread adoption of State Space Models (SSMs) like Mamba, due to\ntheir superior computational efficiency and scalability. As its autoregressive\ngeneration process remains memory-bound, speculative decoding has been proposed\nthat incorporates draft model generation and target model verification.\nHowever, directly applying speculative decoding to SSMs faces three key\nchallenges: (1) hidden state backtracking difficulties, (2) tree-based parallel\nverification incompatibility, and (3) hardware workload mismatch. To address\nthese challenges, we propose SpecMamba, the first FPGA-based accelerator for\nMamba with speculative decoding, which features system, algorithm, and hardware\nco-design. At the system level, we present a memory-aware hybrid backtracking\nstrategy to coordinate both models. At the algorithm level, we propose\nfirst-in-first-out (FIFO)-based tree verification with tiling to minimize\nmemory access. At the hardware level, we customize a dataflow that computes\nlinear layers in parallel and SSM layers in series to enable maximal\noverlapping. Implemented on AMD FPGA platforms (VHK158 and VCK190), SpecMamba\nachieves a 2.27x speedup over GPU baselines and a 2.85x improvement compared to\nprior FPGA solutions, while demonstrating 5.41x and 1.26x higher energy\nefficiency, respectively.", "AI": {"tldr": "SpecMamba\u662f\u9996\u4e2a\u57fa\u4e8eFPGA\u7684Mamba\u6a21\u578b\u52a0\u901f\u5668\uff0c\u91c7\u7528\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u89e3\u51b3\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u957f\u5e8f\u5217\u5efa\u6a21\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u7cfb\u7edf\u3001\u7b97\u6cd5\u548c\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u968f\u7740\u8fb9\u7f18\u8bbe\u5907\u5bf9\u957f\u5e8f\u5217\u5efa\u6a21\u9700\u6c42\u7684\u589e\u957f\uff0c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u5982Mamba\uff09\u56e0\u5176\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u800c\u5e7f\u6cdb\u5e94\u7528\u3002\u7136\u800c\uff0c\u5176\u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4ecd\u53d7\u5185\u5b58\u9650\u5236\uff0c\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u9762\u4e34\u9690\u85cf\u72b6\u6001\u56de\u6eaf\u56f0\u96be\u3001\u6811\u5f62\u5e76\u884c\u9a8c\u8bc1\u4e0d\u517c\u5bb9\u548c\u786c\u4ef6\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5339\u914d\u4e09\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faSpecMamba\u52a0\u901f\u5668\uff0c\u91c7\u7528\u4e09\u5c42\u6b21\u534f\u540c\u8bbe\u8ba1\uff1a\u7cfb\u7edf\u5c42\u9762\u4f7f\u7528\u5185\u5b58\u611f\u77e5\u6df7\u5408\u56de\u6eaf\u7b56\u7565\u534f\u8c03\u6a21\u578b\uff1b\u7b97\u6cd5\u5c42\u9762\u63d0\u51fa\u57fa\u4e8eFIFO\u7684\u6811\u5f62\u9a8c\u8bc1\u4e0e\u5206\u5757\u6280\u672f\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\uff1b\u786c\u4ef6\u5c42\u9762\u5b9a\u5236\u6570\u636e\u6d41\uff0c\u5e76\u884c\u8ba1\u7b97\u7ebf\u6027\u5c42\u3001\u4e32\u884c\u8ba1\u7b97SSM\u5c42\u4ee5\u5b9e\u73b0\u6700\u5927\u91cd\u53e0\u3002", "result": "\u5728AMD FPGA\u5e73\u53f0\uff08VHK158\u548cVCK190\uff09\u4e0a\u5b9e\u73b0\uff0c\u76f8\u6bd4GPU\u57fa\u7ebf\u83b7\u5f972.27\u500d\u52a0\u901f\uff0c\u76f8\u6bd4\u73b0\u6709FPGA\u89e3\u51b3\u65b9\u6848\u63d0\u53472.85\u500d\uff0c\u540c\u65f6\u80fd\u6548\u5206\u522b\u63d0\u9ad85.41\u500d\u548c1.26\u500d\u3002", "conclusion": "SpecMamba\u6210\u529f\u89e3\u51b3\u4e86SSM\u6a21\u578b\u63a8\u6d4b\u89e3\u7801\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\u5728FPGA\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u548c\u80fd\u6548\u63d0\u5347\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u957f\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19539", "categories": ["cs.DC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19539", "abs": "https://arxiv.org/abs/2509.19539", "authors": ["Raj Patel", "Umesh Biswas", "Surya Kodipaka", "Will Carroll", "Preston Peranich", "Maxwell Young"], "title": "A Survey of Recent Advancements in Secure Peer-to-Peer Networks", "comment": "30 pages, 4 figures, 2 tables", "summary": "Peer-to-peer (P2P) networks are a cornerstone of modern computing, and their\nsecurity is an active area of research. Many defenses with strong security\nguarantees have been proposed; however, the most-recent survey is over a decade\nold. This paper delivers an updated review of recent theoretical advances that\naddress classic threats, such as the Sybil and routing attacks, while\nhighlighting how emerging trends -- such as machine learning, social networks,\nand dynamic systems -- pose new challenges and drive novel solutions. We\nevaluate the strengths and weaknesses of these solutions and suggest directions\nfor future research.", "AI": {"tldr": "\u672c\u6587\u5bf9P2P\u7f51\u7edc\u5b89\u5168\u7684\u6700\u65b0\u7406\u8bba\u8fdb\u5c55\u8fdb\u884c\u4e86\u66f4\u65b0\u7efc\u8ff0\uff0c\u91cd\u70b9\u5173\u6ce8\u7ecf\u5178\u5a01\u80c1\uff08\u5982Sybil\u653b\u51fb\u548c\u8def\u7531\u653b\u51fb\uff09\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5206\u6790\u4e86\u673a\u5668\u5b66\u4e60\u3001\u793e\u4ea4\u7f51\u7edc\u548c\u52a8\u6001\u7cfb\u7edf\u7b49\u65b0\u5174\u8d8b\u52bf\u5e26\u6765\u7684\u65b0\u6311\u6218\u548c\u65b0\u65b9\u6cd5\u3002", "motivation": "P2P\u7f51\u7edc\u662f\u73b0\u4ee3\u8ba1\u7b97\u7684\u57fa\u77f3\uff0c\u5176\u5b89\u5168\u6027\u662f\u4e00\u4e2a\u6d3b\u8dc3\u7684\u7814\u7a76\u9886\u57df\u3002\u867d\u7136\u5df2\u7ecf\u63d0\u51fa\u4e86\u8bb8\u591a\u5177\u6709\u5f3a\u5927\u5b89\u5168\u4fdd\u8bc1\u7684\u9632\u5fa1\u63aa\u65bd\uff0c\u4f46\u6700\u8fd1\u7684\u8c03\u67e5\u5df2\u6709\u5341\u591a\u5e74\u5386\u53f2\uff0c\u9700\u8981\u66f4\u65b0\u7efc\u8ff0\u4ee5\u53cd\u6620\u6700\u65b0\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u56de\u987e\u548c\u5206\u6790\uff0c\u8bc4\u4f30\u9488\u5bf9\u7ecf\u5178\u5a01\u80c1\uff08Sybil\u653b\u51fb\u3001\u8def\u7531\u653b\u51fb\uff09\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u7814\u7a76\u65b0\u5174\u8d8b\u52bf\uff08\u673a\u5668\u5b66\u4e60\u3001\u793e\u4ea4\u7f51\u7edc\u3001\u52a8\u6001\u7cfb\u7edf\uff09\u5982\u4f55\u5f71\u54cdP2P\u7f51\u7edc\u5b89\u5168\u3002", "result": "\u8bc6\u522b\u4e86\u5404\u79cd\u89e3\u51b3\u65b9\u6848\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u63ed\u793a\u4e86\u65b0\u5174\u6280\u672f\u5e26\u6765\u7684\u65b0\u6311\u6218\uff0c\u540c\u65f6\u4e5f\u63a8\u52a8\u4e86\u65b0\u9896\u5b89\u5168\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "conclusion": "\u672c\u6587\u4e0d\u4ec5\u603b\u7ed3\u4e86\u5f53\u524dP2P\u7f51\u7edc\u5b89\u5168\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u8fd8\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\uff0c\u4e3a\u9886\u57df\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2509.19959", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.19959", "abs": "https://arxiv.org/abs/2509.19959", "authors": ["Antoine Plin", "Fr\u00e9d\u00e9ric Fauberteau", "Nga Nguyen"], "title": "OpenGL GPU-Based Rowhammer Attack (Work in Progress)", "comment": "Presented at HS3 2025 Workshop", "summary": "Rowhammer attacks have emerged as a significant threat to modern DRAM-based\nmemory systems, leveraging frequent memory accesses to induce bit flips in\nadjacent memory cells. This work-in-progress paper presents an adaptive,\nmany-sided Rowhammer attack utilizing GPU compute shaders to systematically\nachieve high-frequency memory access patterns. Our approach employs statistical\ndistributions to optimize row targeting and avoid current mitigations. The\nmethodology involves initializing memory with known patterns, iteratively\nhammering victim rows, monitoring for induced errors, and dynamically adjusting\nparameters to maximize success rates. The proposed attack exploits the parallel\nprocessing capabilities of GPUs to accelerate hammering operations, thereby\nincreasing the probability of successful bit flips within a constrained\ntimeframe. By leveraging OpenGL compute shaders, our implementation achieves\nhighly efficient row hammering with minimal software overhead. Experimental\nresults on a Raspberry Pi 4 demonstrate that the GPU-based approach attains a\nhigh rate of bit flips compared to traditional CPU-based hammering, confirming\nits effectiveness in compromising DRAM integrity. Our findings align with\nexisting research on microarchitectural attacks in heterogeneous systems that\nhighlight the susceptibility of GPUs to security vulnerabilities. This study\ncontributes to the understanding of GPU-assisted fault-injection attacks and\nunderscores the need for improved mitigation strategies in future memory\narchitectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u8ba1\u7b97\u7740\u8272\u5668\u7684\u81ea\u9002\u5e94\u591a\u9762Rowhammer\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u8ba1\u5206\u5e03\u4f18\u5316\u884c\u76ee\u6807\u9009\u62e9\u5e76\u89c4\u907f\u73b0\u6709\u7f13\u89e3\u63aa\u65bd\uff0c\u5728\u6811\u8393\u6d3e4\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edfCPU\u653b\u51fb\u66f4\u9ad8\u7684\u6bd4\u7279\u7ffb\u8f6c\u7387\u3002", "motivation": "Rowhammer\u653b\u51fb\u5df2\u6210\u4e3a\u73b0\u4ee3DRAM\u5185\u5b58\u7cfb\u7edf\u7684\u91cd\u5927\u5a01\u80c1\uff0c\u4f46\u4f20\u7edfCPU\u653b\u51fb\u6548\u7387\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528GPU\u7684\u5e76\u884c\u5904\u7406\u80fd\u529b\u63d0\u5347\u653b\u51fb\u6548\u7387\uff0c\u63a2\u7d22\u5f02\u6784\u7cfb\u7edf\u4e2dGPU\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u4f7f\u7528OpenGL\u8ba1\u7b97\u7740\u8272\u5668\u5b9e\u73b0\u9ad8\u6548\u884c\u9524\u51fb\uff0c\u901a\u8fc7\u521d\u59cb\u5316\u5df2\u77e5\u5185\u5b58\u6a21\u5f0f\u3001\u8fed\u4ee3\u9524\u51fb\u53d7\u5bb3\u884c\u3001\u76d1\u63a7\u8bf1\u5bfc\u9519\u8bef\u5e76\u52a8\u6001\u8c03\u6574\u53c2\u6570\u6765\u6700\u5927\u5316\u6210\u529f\u7387\u3002\u5229\u7528\u7edf\u8ba1\u5206\u5e03\u4f18\u5316\u76ee\u6807\u884c\u9009\u62e9\u3002", "result": "\u5728\u6811\u8393\u6d3e4\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eGPU\u7684\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edfCPU\u9524\u51fb\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6bd4\u7279\u7ffb\u8f6c\u7387\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u7834\u574fDRAM\u5b8c\u6574\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u52a0\u6df1\u4e86\u5bf9GPU\u8f85\u52a9\u6545\u969c\u6ce8\u5165\u653b\u51fb\u7684\u7406\u89e3\uff0c\u5f3a\u8c03\u4e86\u672a\u6765\u5185\u5b58\u67b6\u6784\u4e2d\u9700\u8981\u6539\u8fdb\u7684\u7f13\u89e3\u7b56\u7565\uff0c\u7a81\u663e\u4e86GPU\u5728\u5f02\u6784\u7cfb\u7edf\u4e2d\u5bf9\u5b89\u5168\u6f0f\u6d1e\u7684\u6613\u611f\u6027\u3002"}}
{"id": "2509.19701", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.19701", "abs": "https://arxiv.org/abs/2509.19701", "authors": ["Akash Poptani", "Alireza Khadem", "Scott Mahlke", "Jonah Miller", "Joshua Dolence", "Reetuparna Das"], "title": "Characterizing Adaptive Mesh Refinement on Heterogeneous Platforms with Parthenon-VIBE", "comment": "Accepted to appear at IISWC 2025", "summary": "Hero-class HPC simulations rely on Adaptive Mesh Refinement (AMR) to reduce\ncompute and memory demands while maintaining accuracy. This work analyzes the\nperformance of Parthenon, a block-structured AMR benchmark, on CPU-GPU systems.\nWe show that smaller mesh blocks and deeper AMR levels degrade GPU performance\ndue to increased communication, serial overheads, and inefficient GPU\nutilization. Through detailed profiling, we identify inefficiencies, low\noccupancy, and memory access bottlenecks. We further analyze rank scalability\nand memory constraints, and propose optimizations to improve GPU throughput and\nreduce memory footprint. Our insights can inform future AMR deployments on\nDepartment of Energy's upcoming heterogeneous supercomputers.", "AI": {"tldr": "\u5206\u6790Parthenon\uff08\u5757\u7ed3\u6784\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\u57fa\u51c6\u6d4b\u8bd5\uff09\u5728CPU-GPU\u7cfb\u7edf\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u5c0f\u7f51\u683c\u5757\u548c\u6df1AMR\u5c42\u7ea7\u4f1a\u964d\u4f4eGPU\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4f18\u5316\u5efa\u8bae", "motivation": "\u968f\u7740\u5f02\u6784\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u53d1\u5c55\uff0c\u9700\u8981\u4e86\u89e3\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\uff08AMR\uff09\u5728CPU-GPU\u7cfb\u7edf\u4e0a\u7684\u6027\u80fd\u7279\u5f81\uff0c\u4e3a\u7f8e\u56fd\u80fd\u6e90\u90e8\u5373\u5c06\u63a8\u51fa\u7684\u5f02\u6784\u8d85\u7ea7\u8ba1\u7b97\u673a\u63d0\u4f9b\u90e8\u7f72\u6307\u5bfc", "method": "\u901a\u8fc7\u8be6\u7ec6\u6027\u80fd\u5206\u6790\uff0c\u8bc6\u522bGPU\u6027\u80fd\u74f6\u9888\uff0c\u5305\u62ec\u901a\u4fe1\u5f00\u9500\u3001\u4e32\u884c\u5f00\u9500\u3001GPU\u5229\u7528\u7387\u4f4e\u3001\u5360\u7528\u7387\u4e0d\u8db3\u548c\u5185\u5b58\u8bbf\u95ee\u74f6\u9888\uff0c\u5e76\u5206\u6790\u7b49\u7ea7\u53ef\u6269\u5c55\u6027\u548c\u5185\u5b58\u7ea6\u675f", "result": "\u53d1\u73b0\u8f83\u5c0f\u7684\u7f51\u683c\u5757\u548c\u8f83\u6df1\u7684AMR\u5c42\u7ea7\u4f1a\u663e\u8457\u964d\u4f4eGPU\u6027\u80fd\uff0c\u4e3b\u8981\u7531\u4e8e\u901a\u4fe1\u589e\u52a0\u3001\u4e32\u884c\u5f00\u9500\u548cGPU\u5229\u7528\u7387\u4f4e\u4e0b", "conclusion": "\u63d0\u51fa\u4e86\u4f18\u5316\u5efa\u8bae\u4ee5\u63d0\u9ad8GPU\u541e\u5410\u91cf\u548c\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff0c\u8fd9\u4e9b\u89c1\u89e3\u53ef\u4e3a\u672a\u6765\u5728\u5f02\u6784\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u7684AMR\u90e8\u7f72\u63d0\u4f9b\u6307\u5bfc"}}
{"id": "2509.20182", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.20182", "abs": "https://arxiv.org/abs/2509.20182", "authors": ["Amulya Bhattaram", "Janani Ramamoorthy", "Ranit Gupta", "Diana Marculescu", "Dimitrios Stamoulis"], "title": "Automated Multi-Agent Workflows for RTL Design", "comment": "Accepted: ML for Systems Workshop NeurIPS 2025", "summary": "The rise of agentic AI workflows unlocks novel opportunities for computer\nsystems design and optimization. However, for specialized domains such as\nprogram synthesis, the relative scarcity of HDL and proprietary EDA resources\nonline compared to more common programming tasks introduces challenges, often\nnecessitating task-specific fine-tuning, high inference costs, and\nmanually-crafted agent orchestration. In this work, we present VeriMaAS, a\nmulti-agent framework designed to automatically compose agentic workflows for\nRTL code generation. Our key insight is to integrate formal verification\nfeedback from HDL tools directly into workflow generation, reducing the cost of\ngradient-based updates or prolonged reasoning traces. Our method improves\nsynthesis performance by 5-7% for pass@k over fine-tuned baselines, while\nrequiring only a few hundred training examples, representing an\norder-of-magnitude reduction in supervision cost.", "AI": {"tldr": "VeriMaAS\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5f62\u5f0f\u9a8c\u8bc1\u53cd\u9988\u6765\u81ea\u52a8\u5316RTL\u4ee3\u7801\u751f\u6210\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u5728\u51cf\u5c11\u76d1\u7763\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u5408\u6210\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u7a0b\u5e8f\u5408\u6210\u7b49\u4e13\u4e1a\u9886\u57df\uff0c\u7531\u4e8eHDL\u548c\u4e13\u6709EDA\u8d44\u6e90\u7684\u7a00\u7f3a\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\u3001\u9ad8\u63a8\u7406\u6210\u672c\u548c\u624b\u52a8\u7f16\u6392\u667a\u80fd\u4f53\uff0c\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51faVeriMaAS\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06HDL\u5de5\u5177\u7684\u5f62\u5f0f\u9a8c\u8bc1\u53cd\u9988\u76f4\u63a5\u96c6\u6210\u5230\u5de5\u4f5c\u6d41\u751f\u6210\u4e2d\uff0c\u51cf\u5c11\u57fa\u4e8e\u68af\u5ea6\u7684\u66f4\u65b0\u6216\u5197\u957f\u63a8\u7406\u8f68\u8ff9\u7684\u6210\u672c\u3002", "result": "\u5728pass@k\u6307\u6807\u4e0a\u6bd4\u5fae\u8c03\u57fa\u7ebf\u63d0\u53475-7%\uff0c\u4ec5\u9700\u6570\u767e\u4e2a\u8bad\u7ec3\u6837\u672c\uff0c\u76d1\u7763\u6210\u672c\u964d\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "VeriMaAS\u901a\u8fc7\u5f62\u5f0f\u9a8c\u8bc1\u53cd\u9988\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e13\u4e1a\u9886\u57df\u7a0b\u5e8f\u5408\u6210\u7684\u8d44\u6e90\u7a00\u7f3a\u548c\u6210\u672c\u95ee\u9898\u3002"}}
{"id": "2509.19607", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.19607", "abs": "https://arxiv.org/abs/2509.19607", "authors": ["William J. Bowman"], "title": "Macro-embedding Compiler Intermediate Languages in Racket", "comment": null, "summary": "We present the design and implementation of a macro-embedding of a family of\ncompiler intermediate languages, from a Scheme-like language to x86-64, into\nRacket. This embedding is used as part of a testing framework for a compilers\ncourse to derive interpreters for all the intermediate languages. The embedding\nimplements features including safe, functional abstractions as well as unsafe\nassembly features, and the interactions between the two at various intermediate\nstages.\n  This paper aims to demonstrate language-oriented techniques and abstractions\nfor implementing (1) a large family of languages and (2) interoperability\nbetween low- and high-level languages. The primary strength of this approach is\nthe high degree of code reuse and interoperability compared to implementing\neach interpreter separately. The design emphasizes modularity and\ncompositionality of an open set of language features by local macro expansion\ninto a single host language, rather than implementing a language pre-defined by\na closed set of features. This enables reuse from both the host language\n(Racket) and between intermediate languages, and enables interoperability\nbetween high- and low-level features, simplifying development of the\nintermediate language semantics. It also facilitates extending or redefining\nindividual language features in intermediate languages, and exposing multiple\ninterfaces to the embedded languages.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5728Racket\u4e2d\u5d4c\u5165\u7f16\u8bd1\u5668\u4e2d\u95f4\u8bed\u8a00\u5bb6\u65cf\u7684\u5b8f\u5d4c\u5165\u8bbe\u8ba1\u548c\u5b9e\u73b0\uff0c\u7528\u4e8e\u7f16\u8bd1\u5668\u8bfe\u7a0b\u7684\u6d4b\u8bd5\u6846\u67b6\uff0c\u5b9e\u73b0\u4eceScheme\u7c7b\u8bed\u8a00\u5230x86-64\u7684\u5b8c\u6574\u7f16\u8bd1\u94fe\u3002", "motivation": "\u65e8\u5728\u5c55\u793a\u9762\u5411\u8bed\u8a00\u7684\u6280\u672f\u548c\u62bd\u8c61\uff0c\u7528\u4e8e\u5b9e\u73b0(1)\u5927\u578b\u8bed\u8a00\u5bb6\u65cf\u548c(2)\u9ad8\u4f4e\u7ea7\u8bed\u8a00\u4e4b\u95f4\u7684\u4e92\u64cd\u4f5c\u6027\uff0c\u5f3a\u8c03\u4ee3\u7801\u91cd\u7528\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "method": "\u901a\u8fc7\u5c40\u90e8\u5b8f\u5c55\u5f00\u5230\u5355\u4e00\u5bbf\u4e3b\u8bed\u8a00\u4e2d\uff0c\u5b9e\u73b0\u8bed\u8a00\u7279\u5f81\u7684\u6a21\u5757\u5316\u548c\u7ec4\u5408\u6027\uff0c\u800c\u4e0d\u662f\u5b9e\u73b0\u7531\u5c01\u95ed\u7279\u5f81\u96c6\u9884\u5b9a\u4e49\u7684\u8bed\u8a00\u3002", "result": "\u5b9e\u73b0\u4e86\u5b89\u5168\u51fd\u6570\u62bd\u8c61\u4e0e\u4e0d\u5b89\u5168\u6c47\u7f16\u7279\u5f81\u53ca\u5176\u5728\u4e2d\u95f4\u9636\u6bb5\u7684\u4ea4\u4e92\uff0c\u652f\u6301\u4ece\u5bbf\u4e3b\u8bed\u8a00\u548c\u4e2d\u95f4\u8bed\u8a00\u4e4b\u95f4\u7684\u91cd\u7528\uff0c\u7b80\u5316\u4e86\u4e2d\u95f4\u8bed\u8a00\u8bed\u4e49\u7684\u5f00\u53d1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4fc3\u8fdb\u4e86\u4e2d\u95f4\u8bed\u8a00\u4e2d\u5355\u4e2a\u8bed\u8a00\u7279\u5f81\u7684\u6269\u5c55\u6216\u91cd\u5b9a\u4e49\uff0c\u5e76\u66b4\u9732\u4e86\u5d4c\u5165\u8bed\u8a00\u7684\u591a\u4e2a\u63a5\u53e3\uff0c\u5c55\u793a\u4e86\u8bed\u8a00\u5bfc\u5411\u7f16\u7a0b\u5728\u7f16\u8bd1\u5668\u5b9e\u73b0\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2509.19729", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19729", "abs": "https://arxiv.org/abs/2509.19729", "authors": ["Haoyu Chen", "Xue Li", "Kun Qian", "Yu Guan", "Jin Zhao", "Xin Wang"], "title": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient LLM Inference", "comment": "12 pages, 15 figures", "summary": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions.", "AI": {"tldr": "Gyges\u63d0\u51fa\u8de8\u5b9e\u4f8b\u5e76\u884c\u8f6c\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8fd0\u884c\u5b9e\u4f8b\u7684\u5e76\u884c\u7b56\u7565\u6765\u9002\u5e94\u8bf7\u6c42\u7684\u52a8\u6001\u53d8\u5316\uff0c\u5305\u62ecKV\u7f13\u5b58\u8f6c\u6362\u3001\u6743\u91cd\u8f6c\u6362\u548c\u8f6c\u6362\u611f\u77e5\u8c03\u5ea6\u5668\uff0c\u663e\u8457\u63d0\u5347LLM\u670d\u52a1\u541e\u5410\u91cf\u3002", "motivation": "LLM\u670d\u52a1\u4e2d\u5904\u7406\u8bf7\u6c42\u52a8\u6001\u53d8\u5316\uff08\u7279\u522b\u662f\u4e0a\u4e0b\u6587\u957f\u5ea6\u53d8\u5316\uff09\u5b58\u5728\u5185\u5728\u6743\u8861\uff1a\u4f7f\u7528\u5f20\u91cf\u5e76\u884c\u7b49\u7b56\u7565\u53ef\u4ee5\u534f\u8c03\u591a\u4e2aGPU\u5904\u7406\u66f4\u5927\u4e0a\u4e0b\u6587\uff0c\u4f46\u4f1a\u964d\u4f4e\u6574\u4f53\u541e\u5410\u91cf\u3002", "method": "\u8bbe\u8ba1(1)\u9875\u9762\u53cb\u597d\u7684\u5934\u4e2d\u5fc3\u5e03\u5c40\u52a0\u901fKV\u7f13\u5b58\u8f6c\u6362\uff1b(2)\u4e13\u7528\u6743\u91cd\u586b\u5145\u52a0\u901f\u6a21\u578b\u6743\u91cd\u8f6c\u6362\uff1b(3)\u8f6c\u6362\u611f\u77e5\u8c03\u5ea6\u5668\u534f\u540c\u8c03\u5ea6\u8bf7\u6c42\u548c\u5e76\u884c\u8f6c\u6362\u3002", "result": "\u4f7f\u7528\u771f\u5b9e\u4e16\u754ctrace\u8bc4\u4f30\u663e\u793a\uff0cGyges\u76f8\u6bd4\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u5c06\u541e\u5410\u91cf\u63d0\u53471.75\u500d\u52306.57\u500d\u3002", "conclusion": "Gyges\u901a\u8fc7\u52a8\u6001\u5e76\u884c\u7b56\u7565\u8f6c\u6362\u6709\u6548\u89e3\u51b3\u4e86LLM\u670d\u52a1\u4e2d\u4e0a\u4e0b\u6587\u957f\u5ea6\u53d8\u5316\u5e26\u6765\u7684\u541e\u5410\u91cf\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2509.19613", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.19613", "abs": "https://arxiv.org/abs/2509.19613", "authors": ["William J. Bowman"], "title": "Compilation as Multi-Language Semantics", "comment": null, "summary": "Modeling interoperability between programs in different languages is a key\nproblem when modeling verified and secure compilation, which has been\nsuccessfully addressed using multi-language semantics. Unfortunately, existing\nmodels of compilation using multi-language semantics define two variants of\neach compiler pass: a syntactic translation on open terms to model compilation,\nand a run-time translation of closed terms at multi-language boundaries to\nmodel interoperability.\n  In this talk, I discuss work-in-progress approach to uniformly model a\ncompiler entirely as a reduction system on open term in a multi-language\nsemantics, rather than as a syntactic translation. This simultaneously defines\nthe compiler and the interoperability semantics, reducing duplication. It also\nprovides interesting semantic insights. Normalization of the cross-language\nredexes performs ahead-of-time (AOT) compilation. Evaluation in the\nmulti-language models just-in-time (JIT) compilation. Confluence of\nmulti-language reduction implies compiler correctness, and part of the secure\ncompilation proof (full abstraction), enabling focus on the difficult part of\nthe proof. Subject reduction of the multi-language reduction implies\ntype-preservation of the compiler.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u8bed\u8a00\u8bed\u4e49\u65b9\u6cd5\uff0c\u5c06\u7f16\u8bd1\u5668\u5efa\u6a21\u4e3a\u5f00\u653e\u9879\u7684\u5f52\u7ea6\u7cfb\u7edf\uff0c\u800c\u975e\u8bed\u6cd5\u7ffb\u8bd1\uff0c\u4ece\u800c\u540c\u65f6\u5b9a\u4e49\u7f16\u8bd1\u5668\u548c\u4e92\u64cd\u4f5c\u6027\u8bed\u4e49\u3002", "motivation": "\u73b0\u6709\u591a\u8bed\u8a00\u8bed\u4e49\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u7f16\u8bd1\u901a\u9053\u5b9a\u4e49\u4e24\u4e2a\u53d8\u4f53\uff1a\u7528\u4e8e\u5efa\u6a21\u7f16\u8bd1\u7684\u5f00\u653e\u9879\u8bed\u6cd5\u7ffb\u8bd1\uff0c\u4ee5\u53ca\u7528\u4e8e\u5efa\u6a21\u4e92\u64cd\u4f5c\u6027\u7684\u591a\u8bed\u8a00\u8fb9\u754c\u5c01\u95ed\u9879\u8fd0\u884c\u65f6\u7ffb\u8bd1\uff0c\u5b58\u5728\u91cd\u590d\u5de5\u4f5c\u3002", "method": "\u5c06\u7f16\u8bd1\u5668\u5b8c\u5168\u5efa\u6a21\u4e3a\u591a\u8bed\u8a00\u8bed\u4e49\u4e2d\u5f00\u653e\u9879\u7684\u5f52\u7ea6\u7cfb\u7edf\uff0c\u901a\u8fc7\u8de8\u8bed\u8a00redex\u7684\u5f52\u4e00\u5316\u5b9e\u73b0AOT\u7f16\u8bd1\uff0c\u901a\u8fc7\u591a\u8bed\u8a00\u6c42\u503c\u5b9e\u73b0JIT\u7f16\u8bd1\u3002", "result": "\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u91cd\u590d\u5b9a\u4e49\uff0c\u63d0\u4f9b\u4e86\u8bed\u4e49\u6d1e\u5bdf\uff0c\u591a\u8bed\u8a00\u5f52\u7ea6\u7684\u6c47\u5408\u6027\u9690\u542b\u7f16\u8bd1\u5668\u6b63\u786e\u6027\uff0c\u4e3b\u9898\u5f52\u7ea6\u9690\u542b\u7c7b\u578b\u4fdd\u6301\u6027\u3002", "conclusion": "\u7edf\u4e00\u7684\u591a\u8bed\u8a00\u5f52\u7ea6\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u5b9a\u4e49\u7f16\u8bd1\u548c\u4e92\u64cd\u4f5c\u6027\uff0c\u7b80\u5316\u8bc1\u660e\u8fc7\u7a0b\uff0c\u4e3a\u5b89\u5168\u7f16\u8bd1\u9a8c\u8bc1\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2509.19836", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.19836", "abs": "https://arxiv.org/abs/2509.19836", "authors": ["Ao Sun", "Weilin Zhao", "Xu Han", "Cheng Yang", "Zhiyuan Liu", "Chuan Shi", "Maosong sun"], "title": "BurstEngine: an Efficient Distributed Framework for Training Transformers on Extremely Long Sequences of over 1M Tokens", "comment": null, "summary": "Existing methods for training LLMs on long-sequence data, such as Tensor\nParallelism and Context Parallelism, exhibit low Model FLOPs Utilization as\nsequence lengths and number of GPUs increase, especially when sequence lengths\nexceed 1M tokens. To address these challenges, we propose BurstEngine, an\nefficient framework designed to train LLMs on long-sequence data. BurstEngine\nintroduces BurstAttention, an optimized distributed attention with lower\ncommunication cost than RingAttention. BurstAttention leverages topology-aware\nring communication to fully utilize network bandwidth and incorporates\nfine-grained communication-computation overlap. Furthermore, BurstEngine\nintroduces sequence-level selective checkpointing and fuses the language\nmodeling head with the loss function to reduce memory cost. Additionally,\nBurstEngine introduces workload balance optimization for various types of\nattention masking. By integrating these optimizations, BurstEngine achieves a\n$1.2\\times$ speedup with much lower memory overhead than the state-of-the-art\nbaselines when training LLMs on extremely long sequences of over 1M tokens. We\nhave made our code publicly available on GitHub:\nhttps://github.com/thunlp/BurstEngine.", "AI": {"tldr": "BurstEngine\u662f\u4e00\u4e2a\u9ad8\u6548\u8bad\u7ec3LLM\u957f\u5e8f\u5217\u6570\u636e\u7684\u6846\u67b6\uff0c\u901a\u8fc7BurstAttention\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u7ed3\u5408\u5e8f\u5217\u7ea7\u9009\u62e9\u6027\u68c0\u67e5\u70b9\u548c\u8d1f\u8f7d\u5747\u8861\u4f18\u5316\uff0c\u5728\u8d85\u8fc7100\u4e07token\u7684\u8d85\u957f\u5e8f\u5217\u8bad\u7ec3\u4e2d\u5b9e\u73b01.2\u500d\u52a0\u901f\u548c\u66f4\u4f4e\u5185\u5b58\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982Tensor Parallelism\u548cContext Parallelism\u5728\u5e8f\u5217\u957f\u5ea6\u8d85\u8fc7100\u4e07token\u65f6\u6a21\u578bFLOPs\u5229\u7528\u7387\u4f4e\uff0c\u7279\u522b\u662fGPU\u6570\u91cf\u589e\u52a0\u65f6\u6548\u7387\u4e0b\u964d\u660e\u663e\u3002", "method": "\u63d0\u51faBurstAttention\u4f18\u5316\u5206\u5e03\u5f0f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528\u62d3\u6251\u611f\u77e5\u73af\u5f62\u901a\u4fe1\u548c\u7ec6\u7c92\u5ea6\u901a\u4fe1\u8ba1\u7b97\u91cd\u53e0\uff1b\u5f15\u5165\u5e8f\u5217\u7ea7\u9009\u62e9\u6027\u68c0\u67e5\u70b9\u3001\u8bed\u8a00\u5efa\u6a21\u5934\u4e0e\u635f\u5931\u51fd\u6570\u878d\u5408\u3001\u6ce8\u610f\u529b\u63a9\u7801\u8d1f\u8f7d\u5747\u8861\u4f18\u5316\u3002", "result": "\u5728\u8bad\u7ec3\u8d85\u8fc7100\u4e07token\u7684\u8d85\u957f\u5e8f\u5217\u65f6\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b01.2\u500d\u52a0\u901f\uff0c\u5185\u5b58\u5f00\u9500\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "BurstEngine\u901a\u8fc7\u7efc\u5408\u4f18\u5316\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u548c\u5185\u5b58\u95ee\u9898\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2509.20020", "categories": ["cs.PL", "cs.LG", "cs.MS", "cs.SC", "F.2.2; I.1.2; I.1.3"], "pdf": "https://arxiv.org/pdf/2509.20020", "abs": "https://arxiv.org/abs/2509.20020", "authors": ["Maurice Wenig", "Paul G. Rump", "Mark Blacher", "Joachim Giesen"], "title": "The Syntax and Semantics of einsum", "comment": "21 pages, 1 figure. Includes formal definitions, proofs of algebraic\n  properties, and nesting/denesting rules for the einsum notation", "summary": "In 2011, einsum was introduced to NumPy as a practical and convenient\nnotation for tensor expressions in machine learning, quantum circuit\nsimulation, and other fields. It has since been implemented in additional\nPython frameworks such as PyTorch and TensorFlow, as well as in other\nprogramming languages such as Julia. Despite its practical success, the einsum\nnotation still lacks a solid theoretical basis, and is not unified across the\ndifferent frameworks, limiting opportunities for formal reasoning and\nsystematic optimization. In this work, we discuss the terminology of tensor\nexpressions and provide a formal definition of the einsum language. Based on\nthis definition, we formalize and prove important equivalence rules for tensor\nexpressions and highlight their relevance in practical applications.", "AI": {"tldr": "\u672c\u6587\u4e3aeinsum\u7b26\u53f7\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5b9a\u4e49\u4e86einsum\u8bed\u8a00\u5e76\u8bc1\u660e\u4e86\u5f20\u91cf\u8868\u8fbe\u5f0f\u7684\u91cd\u8981\u7b49\u4ef7\u89c4\u5219", "motivation": "einsum\u7b26\u53f7\u867d\u7136\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f88\u6210\u529f\uff0c\u4f46\u7f3a\u4e4f\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u4e14\u5728\u4e0d\u540c\u6846\u67b6\u4e2d\u4e0d\u7edf\u4e00\uff0c\u9650\u5236\u4e86\u5f62\u5f0f\u5316\u63a8\u7406\u548c\u7cfb\u7edf\u4f18\u5316\u7684\u673a\u4f1a", "method": "\u8ba8\u8bba\u5f20\u91cf\u8868\u8fbe\u5f0f\u7684\u672f\u8bed\uff0c\u63d0\u4f9beinsum\u8bed\u8a00\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u57fa\u4e8e\u6b64\u5b9a\u4e49\u5f62\u5f0f\u5316\u5e76\u8bc1\u660e\u5f20\u91cf\u8868\u8fbe\u5f0f\u7684\u91cd\u8981\u7b49\u4ef7\u89c4\u5219", "result": "\u5efa\u7acb\u4e86einsum\u7b26\u53f7\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u91cd\u8981\u7684\u7b49\u4ef7\u89c4\u5219\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u89c4\u5219\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u76f8\u5173\u6027", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aeinsum\u7b26\u53f7\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u5f62\u5f0f\u5316\u63a8\u7406\u548c\u7cfb\u7edf\u4f18\u5316"}}
{"id": "2509.20160", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20160", "abs": "https://arxiv.org/abs/2509.20160", "authors": ["Prashanthi S. K.", "Sai Anuroop Kesanapalli", "Yogesh Simmhan"], "title": "Characterizing the Performance of Accelerated Jetson Edge Devices for Training Deep Learning Models", "comment": "Preprint of article in ACM SIGMETRICS 2023", "summary": "Deep Neural Networks (DNNs) have had a significant impact on domains like\nautonomous vehicles and smart cities through low-latency inferencing on edge\ncomputing devices close to the data source. However, DNN training on the edge\nis poorly explored. Techniques like federated learning and the growing capacity\nof GPU-accelerated edge devices like NVIDIA Jetson motivate the need for a\nholistic characterization of DNN training on the edge. Training DNNs is\nresource-intensive and can stress an edge's GPU, CPU, memory and storage\ncapacities. Edge devices also have different resources compared to workstations\nand servers, such as slower shared memory and diverse storage media. Here, we\nperform a principled study of DNN training on individual devices of three\ncontemporary Jetson device types: AGX Xavier, Xavier NX and Nano for three\ndiverse DNN model--dataset combinations. We vary device and training parameters\nsuch as I/O pipelining and parallelism, storage media, mini-batch sizes and\npower modes, and examine their effect on CPU and GPU utilization, fetch stalls,\ntraining time, energy usage, and variability. Our analysis exposes several\nresource inter-dependencies and counter-intuitive insights, while also helping\nquantify known wisdom. Our rigorous study can help tune the training\nperformance on the edge, trade-off time and energy usage on constrained\ndevices, and even select an ideal edge hardware for a DNN workload, and, in\nfuture, extend to federated learning too. As an illustration, we use these\nresults to build a simple model to predict the training time and energy per\nepoch for any given DNN across different power modes, with minimal additional\nprofiling.", "AI": {"tldr": "\u672c\u6587\u5bf9\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684DNN\u8bad\u7ec3\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u5206\u6790\u4e86\u4e0d\u540cJetson\u8bbe\u5907\u5728\u8bad\u7ec3\u53c2\u6570\u53d8\u5316\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u5efa\u7acb\u4e86\u9884\u6d4b\u6a21\u578b\u6765\u4f18\u5316\u8bad\u7ec3\u65f6\u95f4\u548c\u80fd\u8017\u3002", "motivation": "\u968f\u7740\u8054\u90a6\u5b66\u4e60\u7684\u53d1\u5c55\u548c\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u80fd\u529b\u7684\u63d0\u5347\uff0c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684DNN\u8bad\u7ec3\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u6b64\u5173\u6ce8\u4e0d\u8db3\u3002\u8fb9\u7f18\u8bbe\u5907\u4e0e\u670d\u52a1\u5668\u5728\u5de5\u4f5c\u7ad9\u8d44\u6e90\u7279\u6027\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6027\u80fd\u5206\u6790\u3002", "method": "\u5728\u4e09\u79cdJetson\u8bbe\u5907\uff08AGX Xavier\u3001Xavier NX\u548cNano\uff09\u4e0a\u5bf9\u4e09\u79cd\u4e0d\u540c\u7684DNN\u6a21\u578b-\u6570\u636e\u96c6\u7ec4\u5408\u8fdb\u884c\u8bad\u7ec3\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u6027\u5730\u8c03\u6574I/O\u6d41\u6c34\u7ebf\u3001\u5b58\u50a8\u4ecb\u8d28\u3001\u6279\u5927\u5c0f\u548c\u529f\u8017\u6a21\u5f0f\u7b49\u53c2\u6570\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u8d44\u6e90\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u548c\u53cd\u76f4\u89c9\u7684\u53d1\u73b0\uff0c\u91cf\u5316\u4e86\u5df2\u77e5\u7ecf\u9a8c\u3002\u901a\u8fc7\u5b9e\u9a8c\u7ed3\u679c\u5efa\u7acb\u4e86\u7b80\u5355\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u989d\u5916\u5206\u6790\u9884\u6d4b\u4e0d\u540c\u529f\u8017\u6a21\u5f0f\u4e0b\u7684\u8bad\u7ec3\u65f6\u95f4\u548c\u80fd\u8017\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f18\u5316\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u8bad\u7ec3\u6027\u80fd\u3001\u5728\u53d7\u9650\u8bbe\u5907\u4e0a\u6743\u8861\u65f6\u95f4\u548c\u80fd\u8017\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e76\u6709\u52a9\u4e8e\u4e3a\u7279\u5b9aDNN\u5de5\u4f5c\u8d1f\u8f7d\u9009\u62e9\u7406\u60f3\u7684\u8fb9\u7f18\u786c\u4ef6\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5230\u8054\u90a6\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2509.20189", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20189", "abs": "https://arxiv.org/abs/2509.20189", "authors": ["Prashanthi S. K.", "Kunal Kumar Sahoo", "Amartya Ranjan Saikia", "Pranav Gupta", "Atharva Vinay Joshi", "Priyanshu Pansari", "Yogesh Simmhan"], "title": "Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge Accelerators", "comment": null, "summary": "Edge accelerators such as Nvidia Jetsons are becoming an integral part of the\ncomputing continuum, and are often used for DNN inferencing and training.\nNvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power\nenvelope and offer $1000$s of power modes to customize CPU, GPU and memory\nfrequencies. Their widely varying power--performance trade-offs can be\nexploited for energy and power-constrained deployments. While data-driven\nmethods to predict the power and latency of DNN workloads for edge devices\nexist, there is a lack of principled study to understand why edge accelerators\nand their power modes perform the way they do. We develop a time roofline and a\nnovel energy roofline model for the Jetson Orin AGX for diverse power modes,\nand couple it with an analytical model of the compute (FLOP) and memory access\n(bytes) for DNN inference workloads to analyze them from first principles.\nThese reveal unique, sometimes counter-intuitive, insights into the power and\nperformance behavior of DNN workloads on edge accelerators, e.g., the default\npower mode MAXN is not the most energy efficient and time efficiency implies\nenergy efficiency for all power modes. We also extend our analytical roofline\nmodels to DNN training. Finally, we apply these methods to tune the power mode\n(and hence the roofline) of the edge device to optimize the latency and energy\nfor DNN inference, with up to $15\\%$ lower energy and minimal degradation in\ninference time.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86Jetson Orin AGX\u8fb9\u7f18\u52a0\u901f\u5668\u7684\u65f6\u95f4\u5c4b\u9876\u7ebf\u548c\u80fd\u91cf\u5c4b\u9876\u7ebf\u6a21\u578b\uff0c\u7ed3\u5408DNN\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5206\u6790\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u529f\u7387\u6a21\u5f0f\u7684\u72ec\u7279\u6027\u80fd\u7279\u5f81\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u9ad815%\u7684\u80fd\u8017\u4f18\u5316\u3002", "motivation": "\u8fb9\u7f18\u52a0\u901f\u5668\u5982Nvidia Jetson\u62e5\u67092000+ CUDA\u6838\u5fc3\u548c\u6570\u5343\u79cd\u529f\u7387\u6a21\u5f0f\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u529f\u7387-\u6027\u80fd\u6743\u8861\u539f\u7406\u7684\u7cfb\u7edf\u7814\u7a76\uff0c\u9700\u8981\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u5206\u6790DNN\u5de5\u4f5c\u8d1f\u8f7d\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u884c\u4e3a\u3002", "method": "\u5f00\u53d1\u65f6\u95f4\u5c4b\u9876\u7ebf\u548c\u65b0\u578b\u80fd\u91cf\u5c4b\u9876\u7ebf\u6a21\u578b\uff0c\u7ed3\u5408DNN\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8ba1\u7b97(FLOP)\u548c\u5185\u5b58\u8bbf\u95ee(\u5b57\u8282)\u5206\u6790\u6a21\u578b\uff0c\u5bf9Jetson Orin AGX\u7684\u4e0d\u540c\u529f\u7387\u6a21\u5f0f\u8fdb\u884c\u7b2c\u4e00\u6027\u539f\u7406\u5206\u6790\u3002", "result": "\u63ed\u793a\u4e86\u8fb9\u7f18\u52a0\u901f\u5668\u529f\u7387\u6027\u80fd\u884c\u4e3a\u7684\u72ec\u7279\u89c1\u89e3\uff1a\u9ed8\u8ba4MAXN\u529f\u7387\u6a21\u5f0f\u5e76\u975e\u6700\u8282\u80fd\uff0c\u65f6\u95f4\u6548\u7387\u5728\u6240\u6709\u529f\u7387\u6a21\u5f0f\u4e0b\u90fd\u610f\u5473\u7740\u80fd\u91cf\u6548\u7387\u3002\u901a\u8fc7\u529f\u7387\u6a21\u5f0f\u8c03\u4f18\u53ef\u5b9e\u73b0\u6700\u9ad815%\u7684\u80fd\u8017\u964d\u4f4e\uff0c\u4e14\u63a8\u7406\u65f6\u95f4\u9000\u5316\u6700\u5c0f\u3002", "conclusion": "\u5c4b\u9876\u7ebf\u6a21\u578b\u4e3a\u7406\u89e3\u8fb9\u7f18\u52a0\u901f\u5668\u6027\u80fd\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u529f\u7387\u6a21\u5f0f\u8c03\u4f18\u80fd\u663e\u8457\u4f18\u5316DNN\u63a8\u7406\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u5230DNN\u8bad\u7ec3\u573a\u666f\u3002"}}
{"id": "2509.20205", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20205", "abs": "https://arxiv.org/abs/2509.20205", "authors": ["Prashanthi S. K.", "Saisamarth Taluri", "Pranav Gupta", "Amartya Ranjan Saikia", "Kunal Kumar Sahoo", "Atharva Vinay Joshi", "Lakshya Karwa", "Kedar Dhule", "Yogesh Simmhan"], "title": "Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge Accelerators", "comment": null, "summary": "The proliferation of GPU accelerated edge devices like Nvidia Jetsons and the\nrise in privacy concerns are placing an emphasis on concurrent DNN training and\ninferencing on edge devices. Inference and training have different computing\nand QoS goals. But edge accelerators like Jetson do not support native GPU\nsharing and expose 1000s of power modes. This requires careful time-sharing of\nconcurrent workloads to meet power--performance goals, while limiting costly\nprofiling. In this paper, we design an intelligent time-slicing approach for\nconcurrent DNN training and inferencing on Jetsons. We formulate an\noptimization problem to interleave training and inferencing minibatches, and\ndecide the device power mode and inference minibatch size, while maximizing the\ntraining throughput and staying within latency and power budgets, with modest\nprofiling costs. We propose GMD, an efficient multi-dimensional gradient\ndescent search which profiles just $15$ power modes; and ALS, an Active\nLearning technique which identifies reusable Pareto-optimal power modes, but\nprofiles $50$--$150$ power modes. We evaluate these within our Fulcrum\nscheduler for $273,000+$ configurations across $15$ DNN workloads. We also\nevaluate our strategies on dynamic arrival inference and concurrent inferences.\nALS and GMD outperform simpler and more complex baselines with larger-scale\nprofiling. Their solutions satisfy the latency and power budget for $>97\\%$ of\nour runs, and on average are within $7\\%$ of the optimal throughput.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u65f6\u95f4\u5207\u7247\u65b9\u6cd5GMD\u548cALS\uff0c\u7528\u4e8e\u5728Jetson\u7b49\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f18\u5316\u5e76\u53d1DNN\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u6027\u80fd\uff0c\u5728\u6709\u9650\u7684\u5206\u6790\u6210\u672c\u4e0b\u5b9e\u73b0\u8bad\u7ec3\u541e\u5410\u91cf\u6700\u5927\u5316\u5e76\u6ee1\u8db3\u5ef6\u8fdf\u548c\u529f\u8017\u7ea6\u675f\u3002", "motivation": "\u968f\u7740GPU\u52a0\u901f\u8fb9\u7f18\u8bbe\u5907\u7684\u666e\u53ca\u548c\u9690\u79c1\u95ee\u9898\u7684\u589e\u52a0\uff0c\u9700\u8981\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u540c\u65f6\u8fdb\u884cDNN\u8bad\u7ec3\u548c\u63a8\u7406\u3002\u4f46\u8fb9\u7f18\u52a0\u901f\u5668\u4e0d\u652f\u6301\u539f\u751fGPU\u5171\u4eab\u4e14\u5177\u6709\u6570\u5343\u79cd\u529f\u8017\u6a21\u5f0f\uff0c\u9700\u8981\u667a\u80fd\u7684\u65f6\u95f4\u5171\u4eab\u7b56\u7565\u6765\u6ee1\u8db3\u529f\u8017-\u6027\u80fd\u76ee\u6807\u3002", "method": "\u8bbe\u8ba1\u4e86Fulcrum\u8c03\u5ea6\u5668\uff0c\u63d0\u51faGMD\uff08\u9ad8\u6548\u591a\u7ef4\u68af\u5ea6\u4e0b\u964d\u641c\u7d22\uff09\u4ec5\u5206\u679015\u79cd\u529f\u8017\u6a21\u5f0f\uff0c\u4ee5\u53caALS\uff08\u4e3b\u52a8\u5b66\u4e60\u6280\u672f\uff09\u8bc6\u522b\u53ef\u91cd\u7528\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u529f\u8017\u6a21\u5f0f\u4f46\u5206\u679050-150\u79cd\u6a21\u5f0f\u3002\u901a\u8fc7\u4f18\u5316\u95ee\u9898\u51b3\u5b9a\u8bad\u7ec3\u548c\u63a8\u7406\u5c0f\u6279\u91cf\u7684\u4ea4\u9519\u65b9\u5f0f\u3001\u8bbe\u5907\u529f\u8017\u6a21\u5f0f\u548c\u63a8\u7406\u5c0f\u6279\u91cf\u5927\u5c0f\u3002", "result": "\u572815\u4e2aDNN\u5de5\u4f5c\u8d1f\u8f7d\u7684273,000+\u914d\u7f6e\u4e2d\u8bc4\u4f30\uff0cALS\u548cGMD\u4f18\u4e8e\u7b80\u5355\u548c\u590d\u6742\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u89e3\u51b3\u65b9\u6848\u572897%\u4ee5\u4e0a\u7684\u8fd0\u884c\u4e2d\u6ee1\u8db3\u5ef6\u8fdf\u548c\u529f\u8017\u9884\u7b97\uff0c\u5e73\u5747\u541e\u5410\u91cf\u63a5\u8fd1\u6700\u4f18\u89e3\u76847%\u4ee5\u5185\u3002", "conclusion": "\u63d0\u51fa\u7684\u667a\u80fd\u65f6\u95f4\u5207\u7247\u65b9\u6cd5\u80fd\u591f\u5728\u6709\u9650\u7684\u6027\u80fd\u5206\u6790\u6210\u672c\u4e0b\uff0c\u6709\u6548\u4f18\u5316\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5e76\u53d1DNN\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u6027\u80fd\uff0c\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u7684\u7ea6\u675f\u8981\u6c42\u3002"}}
{"id": "2509.20223", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20223", "abs": "https://arxiv.org/abs/2509.20223", "authors": ["Md Jueal Mia", "M. Hadi Amini"], "title": "An Empirical Analysis of Secure Federated Learning for Autonomous Vehicle Applications", "comment": "i3CE 2024, 2024 ASCE International Conference on Computing in Civil\n  Engineering", "summary": "Federated Learning lends itself as a promising paradigm in enabling\ndistributed learning for autonomous vehicles applications and ensuring data\nprivacy while enhancing and refining predictive model performance through\ncollaborative training on edge client vehicles. However, it remains vulnerable\nto various categories of cyber-attacks, necessitating more robust security\nmeasures to effectively mitigate potential threats. Poisoning attacks and\ninference attacks are commonly initiated within the federated learning\nenvironment to compromise secure system performance. Secure aggregation can\nlimit the disclosure of sensitive information from outsider and insider\nattackers of the federated learning environment. In this study, our aim is to\nconduct an empirical analysis on the transportation image dataset (e.g., LISA\ntraffic light) using various secure aggregation techniques and multiparty\ncomputation in the presence of diverse categories of cyber-attacks. Multiparty\ncomputation serves as a state-of-the-art security mechanism, offering standard\nprivacy for secure aggregation of edge autonomous vehicles local model updates\nthrough various security protocols. The presence of adversaries can mislead the\nautonomous vehicle learning model, leading to the misclassification of traffic\nlights, and resulting in detrimental impacts. This empirical study explores the\nresilience of various secure federated learning aggregation techniques and\nmultiparty computation in safeguarding autonomous vehicle applications against\nvarious cyber threats during both training and inference times.", "AI": {"tldr": "\u672c\u6587\u5bf9\u8054\u90a6\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u7814\u7a76\u4e0d\u540c\u5b89\u5168\u805a\u5408\u6280\u672f\u548c\u591a\u65b9\u8ba1\u7b97\u5728\u9762\u5bf9\u5404\u7c7b\u7f51\u7edc\u653b\u51fb\u65f6\u7684\u9632\u62a4\u6548\u679c\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u867d\u7136\u80fd\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u5206\u5e03\u5f0f\u5b66\u4e60\u548c\u6570\u636e\u9690\u79c1\u4fdd\u62a4\uff0c\u4f46\u4ecd\u5bb9\u6613\u53d7\u5230\u6295\u6bd2\u653b\u51fb\u548c\u63a8\u7406\u653b\u51fb\u7b49\u7f51\u7edc\u5a01\u80c1\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u5b89\u5168\u63aa\u65bd\u6765\u4fdd\u969c\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u4ea4\u901a\u56fe\u50cf\u6570\u636e\u96c6\uff08\u5982LISA\u4ea4\u901a\u706f\u6570\u636e\u96c6\uff09\uff0c\u91c7\u7528\u591a\u79cd\u5b89\u5168\u805a\u5408\u6280\u672f\u548c\u591a\u65b9\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5728\u5b58\u5728\u4e0d\u540c\u7c7b\u578b\u7f51\u7edc\u653b\u51fb\u7684\u73af\u5883\u4e0b\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u653b\u51fb\u8005\u53ef\u80fd\u8bef\u5bfc\u81ea\u52a8\u9a7e\u9a76\u5b66\u4e60\u6a21\u578b\uff0c\u5bfc\u81f4\u4ea4\u901a\u706f\u9519\u8bef\u5206\u7c7b\u7b49\u4e25\u91cd\u540e\u679c\u3002\u591a\u65b9\u8ba1\u7b97\u4f5c\u4e3a\u5148\u8fdb\u7684\u5b89\u5168\u673a\u5236\uff0c\u80fd\u4e3a\u8fb9\u7f18\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u672c\u5730\u6a21\u578b\u66f4\u65b0\u63d0\u4f9b\u6807\u51c6\u9690\u79c1\u4fdd\u62a4\u3002", "conclusion": "\u8be5\u5b9e\u8bc1\u7814\u7a76\u63a2\u7d22\u4e86\u5404\u79cd\u5b89\u5168\u8054\u90a6\u5b66\u4e60\u805a\u5408\u6280\u672f\u548c\u591a\u65b9\u8ba1\u7b97\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u4fdd\u62a4\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u514d\u53d7\u7f51\u7edc\u5a01\u80c1\u7684\u97e7\u6027\u3002"}}
{"id": "2509.20340", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20340", "abs": "https://arxiv.org/abs/2509.20340", "authors": ["Liubov Kurafeeva", "Alan Subedi", "Ryan Hartung", "Michael Fay", "Avhishek Biswas", "Shantenu Jha", "Ozgur O. Kilic", "Chandra Krintz", "Andre Merzky", "Douglas Thain", "Mehmet C. Vuran", "Rich Wolski"], "title": "xGFabric: Coupling Sensor Networks and HPC Facilities with Private 5G Wireless Networks for Real-Time Digital Agriculture", "comment": "8 pages with 7 figures followed by 3 pages of reproducibility\n  appendix. This paper will be published following the SC 2025 conference on\n  November 16-21, 2025 at St Louis, MO, USA. ISBN: 978-8-4007-1871-7/2025/11", "summary": "Advanced scientific applications require coupling distributed sensor networks\nwith centralized high-performance computing facilities. Citrus Under Protective\nScreening (CUPS) exemplifies this need in digital agriculture, where citrus\nresearch facilities are instrumented with numerous sensors monitoring\nenvironmental conditions and detecting protective screening damage. CUPS\ndemands access to computational fluid dynamics codes for modeling environmental\nconditions and guiding real-time interventions like water application or\nrobotic repairs. These computing domains have contrasting properties: sensor\nnetworks provide low-performance, limited-capacity, unreliable data access,\nwhile high-performance facilities offer enormous computing power through\nhigh-latency batch processing. Private 5G networks present novel capabilities\naddressing this challenge by providing low latency, high throughput, and\nreliability necessary for near-real-time coupling of edge sensor networks with\nHPC simulations. This work presents xGFabric, an end-to-end system coupling\nsensor networks with HPC facilities through Private 5G networks. The prototype\nconnects remote sensors via 5G network slicing to HPC systems, enabling\nreal-time digital agriculture simulation.", "AI": {"tldr": "xGFabric\u662f\u4e00\u4e2a\u901a\u8fc7\u79c1\u67095G\u7f51\u7edc\u5c06\u4f20\u611f\u5668\u7f51\u7edc\u4e0e\u9ad8\u6027\u80fd\u8ba1\u7b97\u8bbe\u65bd\u8026\u5408\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u7528\u4e8e\u6570\u5b57\u519c\u4e1a\u4e2d\u7684\u5b9e\u65f6\u6a21\u62df", "motivation": "\u6570\u5b57\u519c\u4e1a\u4e2d\u7684\u67d1\u6a58\u4fdd\u62a4\u5c4f\u7814\u7a76\u9700\u8981\u5c06\u5206\u5e03\u5f0f\u4f20\u611f\u5668\u7f51\u7edc\u4e0e\u96c6\u4e2d\u5f0f\u9ad8\u6027\u80fd\u8ba1\u7b97\u8bbe\u65bd\u8026\u5408\uff0c\u4ee5\u8fdb\u884c\u73af\u5883\u6761\u4ef6\u5efa\u6a21\u548c\u5b9e\u65f6\u5e72\u9884\u6307\u5bfc", "method": "\u5f00\u53d1xGFabric\u539f\u578b\u7cfb\u7edf\uff0c\u901a\u8fc75G\u7f51\u7edc\u5207\u7247\u5c06\u8fdc\u7a0b\u4f20\u611f\u5668\u8fde\u63a5\u5230HPC\u7cfb\u7edf\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6570\u5b57\u519c\u4e1a\u6a21\u62df", "result": "\u6210\u529f\u6784\u5efa\u4e86\u80fd\u591f\u8026\u5408\u4f20\u611f\u5668\u7f51\u7edc\u548cHPC\u8bbe\u65bd\u7684\u539f\u578b\u7cfb\u7edf", "conclusion": "\u79c1\u67095G\u7f51\u7edc\u4e3a\u89e3\u51b3\u8fb9\u7f18\u4f20\u611f\u5668\u7f51\u7edc\u4e0eHPC\u6a21\u62df\u7684\u8fd1\u5b9e\u65f6\u8026\u5408\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u7684\u80fd\u529b"}}
