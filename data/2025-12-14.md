<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Intrinsically Correct Algorithms and Recursive Coalgebras](https://arxiv.org/abs/2512.10748)
*Cass Alexandru,Henning Urbat,Thorsten Wißmann*

Main category: cs.PL

TL;DR: 提出基于良基函子的框架，自动保证递归性，简化递归算法在证明助手的形式化


<details>
  <summary>Details</summary>
Motivation: 递归余代数虽能优雅建模递归算法，但证明余代数的递归性通常需要特设证明，阻碍了在证明助手中的形式化

Method: 引入良基函子的新概念，在良基关系索引的族范畴上定义，证明良基函子的每个余代数都是递归的

Result: 主要理论结果：良基函子的每个余代数都是递归的；该框架能涵盖排序函数等传统技术；案例研究包括快速排序、欧几里得算法和CYK解析

Conclusion: 提出的良基函子框架为递归算法提供了内在递归的余代数构造方法，简化了在证明助手中的形式化，已在Cubical Agda中实现

Abstract: Recursive coalgebras provide an elegant categorical tool for modelling recursive algorithms and analysing their termination and correctness. By considering coalgebras over categories of suitably indexed families, the correctness of the corresponding algorithms follows intrinsically just from the type of the computed maps. However, proving recursivity of the underlying coalgebras is non-trivial, and proofs are typically ad hoc. This layer of complexity impedes the formalization of coalgebraically defined recursive algorithms in proof assistants. We introduce a framework for constructing coalgebras which are intrinsically recursive in the sense that the type of the coalgebra guarantees recursivity from the outset. Our approach is based on the novel concept of a well-founded functor on a category of families indexed by a well-founded relation. We show as our main result that every coalgebra for a well-founded functor is recursive, and demonstrate that well-known techniques for proving recursivity and termination such as ranking functions are subsumed by this abstract setup. We present a number of case studies, including Quicksort, the Euclidian algorithm, and CYK parsing. Both the main theoretical result and selected case studies have been formalized in Cubical Agda.

</details>


### [2] [Towards Cumulative Abstract Semantics via Handlers](https://arxiv.org/abs/2512.10861)
*Cade Lueker,Andrew Fox,Bor-Yuh Evan Chang*

Main category: cs.PL

TL;DR: 使用作用域效应实现模块化抽象解释框架，支持多种路径/流敏感性和近似方向


<details>
  <summary>Details</summary>
Motivation: 现有抽象解释框架缺乏真正的灵活性，无法支持不同的路径/流敏感性、前进/后退分析以及过/欠近似。大多数解释器将语法和语义紧密耦合，不利于模块化设计。当前模块化方法需要复杂数据结构（如单子变换器），虽然提供模块化但使用笨重。

Method: 提出累积抽象语义方法，利用作用域效应在解释器中积累语义片段。将效应分为两类：语法消除处理程序和领域语义引入处理程序，从而在一个解释器中创建多个动态求值器和静态分析。

Result: 展示了使用效应作为设计工具的优势，能够创建干净、优雅且模块化的抽象解释框架，支持多种分析变体。

Conclusion: 作用域效应是实现模块化抽象解释框架的有效工具，能够解耦语法和语义，提供灵活性同时保持设计的简洁性。

Abstract: We consider the problem of modularizing control flow in a generic abstract interpretation framework. A generic abstract interpretation framework is not truly flexible if it does not allow interpreting with different path- and flow-sensitivities, by going forwards or backwards, and over- or under-approximately. Most interpreters inherently intertwine syntax and semantics, making the implementation antagonistic to modularity. Current approaches to modular designs require the use of complex data structures (e.g., monad transformers), providing modularity but often proving unwieldy (e.g., lifts). We observe that leveraging scoped effects within an interpreter facilitates the accumulation of semantic fragments against a fixed syntax. In this paper, we define cumulative abstract semantics, illustrating the potential for creating multiple dynamic evaluators and static analyses from one interpreter. This modularity is achieved by grouping effects into two categories: syntax elimination and domain-semantic introduction handlers. Our contribution shows the benefits of using effects as an instrument for designing a clean, elegant, and modular abstract interpretation framework.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [A study of the spectrum resource leasing method based on ERC4907 extension](https://arxiv.org/abs/2512.09942)
*Zhiming Liang,Bin Chen,Litao Ye,Chen Sun,Shuo Wang,Zhe Peng*

Main category: cs.DC

TL;DR: 本文提出了M-ERC4907扩展方法，通过支持多时间段批量配置和多用户同时授权，解决了ERC4907标准在去中心化多时段调度场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: ERC4907标准虽然支持可租赁NFT，但仅限于单用户、单时间段授权，这在去中心化多时段调度场景中严重限制了其适用性和效率。

Method: 提出了M-ERC4907扩展方法，引入支持多时间段批量配置和多用户同时授权的新功能，有效消除了ERC4907的严格顺序授权约束。

Result: 在Remix开发平台上进行的实验表明，M-ERC4907方法显著减少了链上交易和总体Gas消耗，从而提高了可扩展性和资源分配效率。

Conclusion: M-ERC4907扩展方法有效解决了ERC4907标准在多时段调度中的局限性，通过减少交易和Gas消耗提升了系统效率和可扩展性。

Abstract: The ERC4907 standard enables rentable Non-Fungible Tokens (NFTs) but is limited to single-user, single-time-slot authorization, which severely limits its applicability and efficiency in decentralized multi-slot scheduling scenarios. To address this limitation, this paper proposes Multi-slot ERC4907 (M-ERC4907) extension method. The M-ERC4907 method introduces novel functionalities to support the batch configuration of multiple time slots and simultaneous authorization of multiple users, thereby effectively eliminating the rigid sequential authorization constraint of ERC4907. The experiment was conducted on the Remix development platform. Experimental results show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency.

</details>


### [4] [Design Space Exploration of DMA based Finer-Grain Compute Communication Overlap](https://arxiv.org/abs/2512.10236)
*Shagnik Pal,Shaizeen Aga,Suchita Pati,Mahzabeen Islam,Lizy K. John*

Main category: cs.DC

TL;DR: 提出FiCCO（细粒度计算-通信重叠）技术，通过比分片级更深一层的细粒度重叠，为更广泛的网络拓扑和更细粒度数据流解锁计算/通信重叠，实现高达1.6倍加速


<details>
  <summary>Details</summary>
Motivation: 当前分布式ML训练和推理中，基于分片的并行化技术存在数据依赖的通信和计算操作，通信暴露导致性能损失高达理想性能的1.7倍。虽然现有工作通过分片级重叠有所改进，但仍有局限性

Method: 提出FiCCO细粒度计算-通信重叠技术，分析操作分解导致的效率损失，设计FiCCO调度设计空间，结合效率特征设计启发式算法，并将通信卸载到GPU DMA引擎以减少竞争

Result: 在真实ML部署场景中，定制化FiCCO调度实现高达1.6倍加速，启发式算法在81%的未见场景中提供准确指导

Conclusion: FiCCO通过细粒度重叠扩展了调度设计空间，结合效率分析和启发式选择，能有效提升分布式ML系统性能，为框架和运行时提供实用的调度指导

Abstract: As both ML training and inference are increasingly distributed, parallelization techniques that shard (divide) ML model across GPUs of a distributed system, are often deployed. With such techniques, there is a high prevalence of data-dependent communication and computation operations where communication is exposed, leaving as high as 1.7x ideal performance on the table. Prior works harness the fact that ML model state and inputs are already sharded, and employ careful overlap of individual computation/communication shards. While such coarse-grain overlap is promising, in this work, we instead make a case for finer-grain compute-communication overlap which we term FiCCO, where we argue for finer-granularity, one-level deeper overlap than at shard-level, to unlock compute/communication overlap for a wider set of network topologies, finer-grain dataflow and more. We show that FiCCO opens up a wider design space of execution schedules than possible at shard-level alone. At the same time, decomposition of ML operations into smaller operations (done in both shard-based and finer-grain techniques) causes operation-level inefficiency losses. To balance the two, we first present a detailed characterization of these inefficiency losses, then present a design space of FiCCO schedules, and finally overlay the schedules with concomitant inefficiency signatures. Doing so helps us design heuristics that frameworks and runtimes can harness to select bespoke FiCCO schedules based on the nature of underlying ML operations. Finally, to further minimize contention inefficiencies inherent with operation overlap, we offload communication to GPU DMA engines. We evaluate several scenarios from realistic ML deployments and demonstrate that our proposed bespoke schedules deliver up to 1.6x speedup and our heuristics provide accurate guidance in 81% of unseen scenarios.

</details>


### [5] [ELANA: A Simple Energy and Latency Analyzer for LLMs](https://arxiv.org/abs/2512.09946)
*Hung-Yueh Chiang,Bokun Wang,Diana Marculescu*

Main category: cs.DC

TL;DR: ELANA是一个开源轻量级LLM性能分析工具，用于评测模型大小、KV缓存、延迟（TTFT、TPOT、TTLT）和能耗，支持Hugging Face所有公开模型和多GPU/边缘GPU平台。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在移动边缘设备到云GPU集群等不同硬件平台上的延迟和功耗是主要限制因素，需要基准测试工具来优化模型部署效率和下一代模型开发。

Method: 开发ELANA作为轻量级学术友好的分析工具，支持分析模型大小、KV缓存大小、预填充延迟（TTFT）、生成延迟（TPOT）和端到端延迟（TTLT），兼容Hugging Face API，提供命令行界面和能耗日志功能。

Result: 开源了ELANA分析工具，支持所有Hugging Face公开模型，可在多GPU和边缘GPU平台上使用，易于定制和适配压缩或低比特模型，适合高效LLM研究和小规模概念验证。

Conclusion: ELANA为LLM效率优化提供了实用的分析工具，填补了学术界对轻量级、易用性分析工具的需求，有助于推动高效LLM的研究和发展。

Abstract: The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.

</details>


### [6] [CloudFix: Automated Policy Repair for Cloud Access Control Policies Using Large Language Models](https://arxiv.org/abs/2512.09957)
*Bethel Hall,Owen Ungaro,William Eiers*

Main category: cs.DC

TL;DR: CloudFix：首个结合形式化方法与LLM的云访问控制策略自动修复框架，通过形式化故障定位和LLM生成修复方案，显著提升策略修复准确率。


<details>
  <summary>Details</summary>
Motivation: 云环境中访问控制策略的手动编写和更新容易出错且耗时，可能导致安全漏洞。现有符号分析方法在云访问控制场景下泛化能力有限，而LLM在程序修复中的应用尚未探索用于云访问控制策略修复。

Method: CloudFix结合形式化方法与LLM：1) 使用形式化方法进行故障定位，识别策略中的错误语句；2) 利用LLM生成潜在修复方案；3) 使用SMT求解器验证修复方案的正确性。

Result: 在包含282个真实AWS访问控制策略的数据集上实验，CloudFix在不同请求规模下均优于基线方法，显著提升了修复准确率。

Conclusion: CloudFix是首个利用LLM进行策略修复的框架，展示了LLM在访问控制领域的有效性，实现了云访问控制策略的高效自动修复。工具和数据集已开源。

Abstract: Access control policies are vital for securing modern cloud computing, where organizations must manage access to sensitive data across thousands of users in distributed system settings. Cloud administrators typically write and update policies manually, which can be an error-prone and time-consuming process and can potentially lead to security vulnerabilities. Existing approaches based on symbolic analysis have demonstrated success in automated debugging and repairing access control policies; however, their generalizability is limited in the context of cloud-based access control. Conversely, Large Language Models (LLMs) have been utilized for automated program repair; however, their applicability to repairing cloud access control policies remains unexplored. In this work, we introduce CloudFix, the first automated policy repair framework for cloud access control that combines formal methods with LLMs. Given an access control policy and a specification of allowed and denied access requests, CloudFix employs Formal Methods-based Fault Localization to identify faulty statements in the policy and leverages LLMs to generate potential repairs, which are then verified using SMT solvers. To evaluate CloudFix, we curated a dataset of 282 real-world AWS access control policies extracted from forum posts and augmented them with synthetically generated request sets based on real scenarios. Our experimental results show that CloudFix improves repair accuracy over a Baseline implementation across varying request sizes. Our work is the first to leverage LLMs for policy repair, showcasing the effectiveness of LLMs for access control and enabling efficient and automated repair of cloud access control policies. We make our tool Cloudfix and AWS dataset publicly available.

</details>


### [7] [TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0](https://arxiv.org/abs/2512.09961)
*Jinyu Chen,Long Shi,Taotao Wang,Jiaheng Wang,Wei Zhang*

Main category: cs.DC

TL;DR: 提出TDC-Cache框架，通过两层架构（DON层作为可信中介）和DRL-DC算法优化Web3.0去中心化缓存，结合PoCL共识确保一致性，显著降低延迟、提高命中率和共识成功率。


<details>
  <summary>Details</summary>
Motivation: Web3.0从中心化向去中心化转型，用户获得数据自主权，但面临冗余数据复制导致的效率问题和数据不一致带来的安全漏洞，需要解决去中心化数据访问中的这些挑战。

Method: 开发TDC-Cache两层架构：DON层作为可信中介平台连接去中心化存储和用户请求；提出DRL-DC算法动态优化分布式预言机缓存策略；设计PoCL共识机制维护缓存决策一致性。

Result: 实验表明，相比现有方法，该框架平均访问延迟降低20%，缓存命中率最高提升18%，平均共识成功率提高10%。

Conclusion: 本文首次探索了Web3.0去中心化缓存框架和策略，通过TDC-Cache有效解决了效率和安全问题，为Web3.0去中心化数据访问提供了可行方案。

Abstract: The rapid growth of Web3.0 is transforming the Internet from a centralized structure to decentralized, which empowers users with unprecedented self-sovereignty over their own data. However, in the context of decentralized data access within Web3.0, it is imperative to cope with efficiency concerns caused by the replication of redundant data, as well as security vulnerabilities caused by data inconsistency. To address these challenges, we develop a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0 to ensure efficient caching and enhance system resilience against adversarial threats. This framework features a two-layer architecture, wherein the Decentralized Oracle Network (DON) layer serves as a trusted intermediary platform for decentralized caching, bridging the contents from decentralized storage and the content requests from users. In light of the complexity of Web3.0 network topologies and data flows, we propose a Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC) for TDC-Cache to dynamically optimize caching strategies of distributed oracles. Furthermore, we develop a Proof of Cooperative Learning (PoCL) consensus to maintain the consistency of decentralized caching decisions within DON. Experimental results show that, compared with existing approaches, the proposed framework reduces average access latency by 20%, increases the cache hit rate by at most 18%, and improves the average success consensus rate by 10%. Overall, this paper serves as a first foray into the investigation of decentralized caching framework and strategy for Web3.0.

</details>


### [8] [GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference](https://arxiv.org/abs/2512.09963)
*Phuong Tran,Tzu-Hao Liu,Long Tan Le,Tung-Anh Nguyen,Van Quan La,Eason Yu,Han Shu,Choong Seon Hong,Nguyen H. Tran*

Main category: cs.DC

TL;DR: GOODSPEED是一个分布式推理框架，通过自适应推测解码优化多服务器LLM推理的吞吐量和公平性


<details>
  <summary>Details</summary>
Motivation: LLMs的高计算需求对实时推理构成挑战，特别是在多用户服务器推测解码和资源受限环境中。现有推测解码技术难以同时保证高吞吐量和跨多个草稿服务器的公平性。

Method: GOODSPEED采用中央验证服务器协调异构草稿服务器的分布式框架，使用梯度调度算法动态分配令牌验证任务，通过并行处理所有草稿服务器的推测输出来最大化对数效用函数，确保比例公平性。

Result: 通过流体样本路径分析证明，GOODSPEED在稳态条件下收敛到最优吞吐量分配，在动态工作负载下保持接近最优性能且具有可证明的有界误差。

Conclusion: GOODSPEED为分布式LLM推理系统中的多服务器推测解码提供了可扩展、公平且高效的解决方案。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet their high computational demands pose significant challenges for real-time inference, especially in multi-user server speculative decoding and resource-constrained environments. Speculative decoding has emerged as a promising technique to accelerate LLM inference by using lightweight draft models to generate candidate tokens, which are subsequently verified by a larger, more accurate model. However, ensuring both high goodput (the effective rate of accepted tokens) and fairness across multiple draft servers cooperating with a central verification server remains an open challenge. This paper introduces GOODSPEED, a novel distributed inference framework that optimizes goodput through adaptive speculative decoding. GOODSPEED employs a central verification server that coordinates a set of heterogeneous draft servers, each running a small language model to generate speculative tokens. To manage resource allocation effectively, GOODSPEED incorporates a gradient scheduling algorithm that dynamically assigns token verification tasks, maximizing a logarithmic utility function to ensure proportional fairness across servers. By processing speculative outputs from all draft servers in parallel, the framework enables efficient collaboration between the verification server and distributed draft generators, streamlining both latency and throughput. Through rigorous fluid sample path analysis, we show that GOODSPEED converges to the optimal goodput allocation in steady-state conditions and maintains near-optimal performance with provably bounded error under dynamic workloads. These results demonstrate that GOODSPEED provides a scalable, fair and efficient solution for multi-server speculative decoding in distributed LLM inference systems.

</details>


### [9] [Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters](https://arxiv.org/abs/2512.10271)
*Shruti Dongare,Redwan Ibne Seraj Khan,Hadeel Albahar,Nannan Zhao,Diego Melendez Maita,Ali R. Butt*

Main category: cs.DC

TL;DR: RLTune是一个基于强化学习的深度学习作业调度框架，能在异构GPU集群上动态优化作业优先级和资源分配，无需作业特定分析即可提升GPU利用率、减少排队延迟和作业完成时间。


<details>
  <summary>Details</summary>
Motivation: 现代云平台面临大规模深度学习工作负载的调度挑战，GPU集群日益异构化且缺乏应用特性可见性，现有调度器依赖离线分析或应用特定假设，难以有效管理。

Method: RLTune结合强化学习驱动的优先级排序和基于混合整数线性规划（MILP）的作业到节点映射，以系统级目标（作业完成时间、排队延迟、资源利用率）进行优化，无需逐作业分析。

Result: 在微软Philly、Helios和阿里巴巴的大规模生产跟踪数据上训练，RLTune将GPU利用率提升高达20%，排队延迟降低高达81%，作业完成时间缩短高达70%。

Conclusion: RLTune无需逐作业分析即可泛化到多样化工作负载，为云提供商提供了可扩展部署的实用解决方案，实现更高效、公平和可持续的深度学习工作负载管理。

Abstract: Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.

</details>


### [10] [High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments](https://arxiv.org/abs/2512.10312)
*Julian Rodriguez,Piotr Lopez,Emiliano Lerma,Rafael Medrano,Jacobo Hernandez*

Main category: cs.DC

TL;DR: 该文档报告了大数据课程中实施的实践和方法序列，包括Epsilon数据集处理、文本分析分类、电影特征分析以及Apache Spark分布式集群的技术实现。


<details>
  <summary>Details</summary>
Motivation: 通过大数据课程实践，展示从数据处理到分布式计算的全流程技术实现，培养学生在实际项目中应用大数据技术的能力。

Method: 采用分阶段的工作流程：1) Epsilon数据集处理（小组和个人策略）；2) RestMex文本分析与分类；3) IMDb电影特征分析；4) 基于Apache Spark在Linux上使用Scala构建分布式计算集群。

Result: 成功实施了完整的大数据处理流程，从数据预处理到文本分类再到分布式计算集群的部署，展示了大数据技术栈的实际应用能力。

Conclusion: 该课程实践系统地覆盖了大数据处理的关键环节，包括数据处理、文本分析、特征工程和分布式计算，为学生提供了全面的大数据技术实践经验。

Abstract: This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.

</details>


### [11] [Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability](https://arxiv.org/abs/2512.10425)
*Fan Yu,Guodong Li,Si Wu,Weijun Fang,Sihuang Hu*

Main category: cs.DC

TL;DR: CP-LRCs是一种新型宽条带局部可修复编码，通过将全局奇偶校验块分解到所有局部奇偶校验块中，创建级联奇偶校验组，在保持MDS级别容错的同时实现低带宽单节点和多节点修复。


<details>
  <summary>Details</summary>
Motivation: 现有LRC在宽条带设置中存在结构限制：扩大的局部组增加了单节点修复成本，多节点故障频繁触发昂贵的全局修复，可靠性急剧下降。根本原因是局部和全局奇偶校验块独立设计，无法在修复过程中协作。

Method: 提出级联奇偶校验LRCs（CP-LRCs），通过在所有局部奇偶校验块中嵌入全局奇偶校验块的结构化依赖关系，创建级联奇偶校验组。提供通用系数生成框架，开发利用级联的修复算法，并实例化为CP-Azure和CP-Uniform两种实现。

Result: 在阿里云上的评估显示，单节点故障修复时间减少高达41%，两节点故障修复时间减少高达26%，同时保持MDS级别的容错能力。

Conclusion: CP-LRCs通过级联奇偶校验设计解决了宽条带LRC的关键限制，实现了局部和全局奇偶校验块的协同修复，显著降低了修复带宽和时间，同时保持高可靠性。

Abstract: Erasure coding with wide stripes is increasingly adopted to reduce storage overhead in large-scale storage systems. However, existing Locally Repairable Codes (LRCs) exhibit structural limitations in this setting: inflated local groups increase single-node repair cost, multi-node failures frequently trigger expensive global repair, and reliability degrades sharply. We identify a key root cause: local and global parity blocks are designed independently, preventing them from cooperating during repair. We present Cascaded Parity LRCs (CP-LRCs), a new family of wide stripe LRCs that embed structured dependency between parity blocks by decomposing a global parity block across all local parity blocks. This creates a cascaded parity group that preserves MDS-level fault tolerance while enabling low-bandwidth single-node and multi-node repairs. We provide a general coefficient-generation framework, develop repair algorithms exploiting cascading, and instantiate the design with CP-Azure and CP-Uniform. Evaluations on Alibaba Cloud show reductions in repair time of up to 41% for single-node failures and 26% for two-node failures.

</details>


### [12] [Clustered Federated Learning with Hierarchical Knowledge Distillation](https://arxiv.org/abs/2512.10443)
*Sabtain Ahmad,Meerzhan Kanatbekova,Ivona Brandic,Atakan Aral*

Main category: cs.DC

TL;DR: 提出CFLHKD方法，通过分层聚类联邦学习和多教师知识蒸馏，解决传统CFL中模型碎片化和缺乏集群间知识共享的问题，提升个性化模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统聚类联邦学习(CFL)存在两个主要问题：1) 为每个集群训练独立的全局模型导致学习碎片化；2) 未能利用集群间的集体知识。这限制了模型性能和训练效率。

Method: 提出CFLHKD方法，采用分层CFL框架进行双层聚合：在边缘训练集群特定模型，在云端训练统一全局模型。通过多教师知识蒸馏实现集群间知识共享，同时保持集群特定的个性化。

Result: 在标准基准数据集上的评估表明，CFLHKD在集群特定模型和全局模型准确率上均优于代表性基线方法，性能提升达到3.32-7.57%。

Conclusion: 分层CFL结合知识蒸馏能有效解决传统CFL的局限性，CFLHKD方法通过集群间知识共享和双层聚合，显著提升了联邦学习在异构IoT环境中的性能和效率。

Abstract: Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\%.

</details>


### [13] [ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp](https://arxiv.org/abs/2512.10576)
*Xinhang Chen,Chao Zhang,Jiahuan He,Wei Liu,Jianming Zhang,Wenlong Zhou,Xiao Li,Pai Zeng,Shiyong Li,Yuanpan Qian,Dong Li,Zhaogeng Li*

Main category: cs.DC

TL;DR: ESS系统通过将Latent-Cache卸载到CPU内存，解决了DeepSeek-V3.2-Exp在长上下文推理中的GPU内存瓶颈，显著提升了Decode阶段吞吐量。


<details>
  <summary>Details</summary>
Motivation: DeepSeek-V3.2-Exp虽然通过稀疏注意力机制降低了长上下文推理延迟，但Decode阶段仍然是主要瓶颈。问题根源在于Latent-Cache随序列长度线性增长与GPU内存容量有限之间的矛盾，这限制了批处理大小，从而抑制了Decode阶段吞吐量。

Method: 提出了ESS（Extended Sparse Server）系统，这是一个专门为DeepSeek-V3.2-Exp设计的卸载中心系统架构。ESS选择性地将Latent-Cache卸载到CPU内存，同时将延迟关键组件保留在GPU上。通过释放GPU内存，ESS有效地将批处理大小扩展与GPU内存约束解耦。

Result: 高保真模拟显示，ESS在32K上下文长度下实现了69.4%的吞吐量提升，在128K上下文长度下实现了高达123%的吞吐量提升。这些结果证明了ESS在大规模上下文推理工作负载中的有效性。

Conclusion: ESS是一个实用且可扩展的解决方案，能够显著提升长上下文LLM服务的Decode阶段吞吐量，从而降低实际部署成本。该系统通过智能内存管理解决了GPU内存瓶颈问题。

Abstract: DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.
  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.
  Our high-fidelity simulations show that ESS delivers 69.4\% throughput improvement at 32K context length and up to 123\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [Algorithm-Driven On-Chip Integration for High Density and Low Cost](https://arxiv.org/abs/2512.10089)
*Jeongeun Kim,Sabrina Yarzada,Paul Chen,Christopher Torng*

Main category: cs.AR

TL;DR: 提出了一种用于大规模半导体设计流片的新方法，通过自动化布局、共享窄区域架构和片上电源域技术，实现高密度项目集成，相比现有方法面积减少高达13倍。


<details>
  <summary>Details</summary>
Motivation: 随着半导体人才培养需求的增长，需要支持大量独立硬件设计的平台。传统的多项目晶圆服务在项目数量增加时扩展性不足，现有的高密度集成方法缺乏系统性的布局、连接和验证原则。

Method: 提出三种关键技术：1) 建立结构化设计空间公式化，实现自动化算法驱动的项目布局；2) 引入仅利用设计站点之间窄区域进行片外通信的架构；3) 提供片上电源域实用方法，支持每个项目的电源特性分析。

Result: 实验结果显示，该方法相比最先进的纯物理聚合方法，实现了高达13倍的面积减少，为大规模流片环境提供了可扩展且经济高效的解决方案。

Conclusion: 该方法解决了高密度半导体设计集成的系统性问题，通过自动化布局、共享窄区域架构和片上电源域技术，为大规模硬件设计和培训提供了可扩展的流片平台。

Abstract: Growing interest in semiconductor workforce development has generated demand for platforms capable of supporting large numbers of independent hardware designs for research and training without imposing high per-project overhead. Traditional multi-project wafer (MPW) services based solely on physical co-placement have historically met this need, yet their scalability breaks down as project counts rise. Recent efforts towards scalable chip tapeouts mitigate these limitations by integrating many small designs within a shared die and attempt to amortize costly resources such as IO pads and memory macros. However, foundational principles for arranging, linking, and validating such densely integrated design sites have received limited systematic investigation. This work presents a new approach with three key techniques to address this gap. First, we establish a structured formulation of the design space that enables automated, algorithm-driven packing of many projects, replacing manual layout practices. Second, we introduce an architecture that exploits only the narrow-area regions between sites to deliver on off-chip communication and other shared needs. Third, we provide a practical approach for on-chip power domains enabling per-project power characterization at a standard laboratory bench and requiring no expertise in low-power ASIC design. Experimental results show that our approach achieves substantial area reductions of up to 13x over state-of-the-art physical-only aggregation methods, offering a scalable and cost-effective path forward for large-scale tapeout environments.

</details>


### [15] [A Vertically Integrated Framework for Templatized Chip Design](https://arxiv.org/abs/2512.10155)
*Jeongeun Kim,Christopher Torng*

Main category: cs.AR

TL;DR: 提出一种从高级面向对象软件规范生成芯片的方法，为初学者提供软件到芯片设计的连续性映射，降低硬件设计门槛


<details>
  <summary>Details</summary>
Motivation: 软件开发者难以将定制硬件集成到应用中，尽管专用芯片能为机器学习和AI等领域带来显著优势。需要降低芯片设计门槛，让软件开发者也能参与芯片创建

Method: 采用模块化构建策略：1）软件对象对应芯片区域的一对一结构映射；2）垂直组合IP块实现软件行为协议；3）基于序列的形式类型系统检查硬件模块交互是否符合软件模型通信模式；4）开发适合对象对齐设计风格的布局技术

Result: 实现了从软件到芯片设计的思维连续性，支持实用的布局生成，降低了软件开发者参与芯片创建所需的专业知识

Conclusion: 通过保持软件抽象到硬件实现的映射连续性，该方法使软件开发者能够更轻松地参与芯片设计，降低了硬件设计的学习门槛

Abstract: Developers who primarily engage with software often struggle to incorporate custom hardware into their applications, even though specialized silicon can provide substantial benefits to machine learning and AI, as well as to the application domains that they enable. This work investigates how a chip can be generated from a high-level object-oriented software specification, targeting introductory-level chip design learners with only very light performance requirements, while maintaining mental continuity between the chip layout and the software source program. In our approach, each software object is represented as a corresponding region on the die, producing a one-to-one structural mapping that preserves these familiar abstractions throughout the design flow. To support this mapping, we employ a modular construction strategy in which vertically composed IP blocks implement the behavioral protocols expressed in software. A direct syntactic translation, however, cannot meet hardware-level efficiency or communication constraints. For this reason, we leverage formal type systems based on sequences that check whether interactions between hardware modules adhere to the communication patterns described in the software model. We further examine hardware interconnect strategies for composing many such modules and develop layout techniques suited to this object-aligned design style. Together, these contributions preserve mental continuity from software to chip design for new learners and enables practical layout generation, ultimately reducing the expertise required for software developers to participate in chip creation.

</details>


### [16] [Neuromorphic Processor Employing FPGA Technology with Universal Interconnections](https://arxiv.org/abs/2512.10180)
*Pracheta Harlikar,Abdel-Hameed A. Badawy,Prasanna Date*

Main category: cs.AR

TL;DR: 在Xilinx Zynq-7000 FPGA平台上实现低成本开源神经形态处理器，支持可配置连接和LIF神经元模型，通过UART接口实现运行时重配置，验证了Iris和MNIST数据集分类性能。


<details>
  <summary>Details</summary>
Motivation: 神经形态计算具有超低功耗和实时推理的潜力，但缺乏灵活的开源平台阻碍了广泛采用和实验。需要开发低成本、可访问的神经形态处理器平台。

Method: 在Xilinx Zynq-7000 FPGA平台上实现神经形态处理器，支持全连接可配置连接，采用泄漏积分发放(LIF)神经元模型，参数可定制（阈值、突触权重、不应期），通过UART接口与主机通信实现运行时重配置。

Result: 使用Iris分类和MNIST数字识别基准数据集验证架构，后综合结果显示设计的能效和可扩展性，证明其作为研究级神经形态平台的可行性。

Conclusion: 该低成本神经形态处理器平台既易于访问又适应实际脉冲神经网络应用，将在项目完成后作为开源发布，促进神经形态计算的广泛采用。

Abstract: Neuromorphic computing, inspired by biological neural systems, holds immense promise for ultra-low-power and real-time inference applications. However, limited access to flexible, open-source platforms continues to hinder widespread adoption and experimentation. In this paper, we present a low-cost neuromorphic processor implemented on a Xilinx Zynq-7000 FPGA platform. The processor supports all-to-all configurable connectivity and employs the leaky integrate-and-fire (LIF) neuron model with customizable parameters such as threshold, synaptic weights, and refractory period. Communication with the host system is handled via a UART interface, enabling runtime reconfiguration without hardware resynthesis. The architecture was validated using benchmark datasets including the Iris classification and MNIST digit recognition tasks. Post-synthesis results highlight the design's energy efficiency and scalability, establishing its viability as a research-grade neuromorphic platform that is both accessible and adaptable for real-world spiking neural network applications. This implementation will be released as open source following project completion.

</details>


### [17] [SemanticBBV: A Semantic Signature for Cross-Program Knowledge Reuse in Microarchitecture Simulation](https://arxiv.org/abs/2512.10231)
*Zhenguo Liu,Chengao Shi,Chen Ding,Jiang Xu*

Main category: cs.AR

TL;DR: SemanticBBV：一个两阶段框架，通过语义编码和集合变换器生成性能感知的程序签名，支持跨程序模拟重用，相比传统BBV方法实现7143倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的微架构模拟使用基本块向量(BBV)作为程序表示，但BBV存在两个根本限制：1) 顺序依赖的ID阻碍跨程序知识重用；2) 缺乏预测硬件性能的语义内容，导致优化潜力未被充分利用。

Method: 提出SemanticBBV两阶段框架：1) 轻量级RWKV语义编码器将汇编基本块转换为丰富的BBE嵌入，捕捉深度功能语义；2) 顺序不变的集合变换器聚合BBE，按执行频率加权生成最终签名。该阶段通过三重损失和CPI回归任务进行协同训练，使签名具有性能敏感性。

Result: 在单程序准确性上与传统BBV相当，同时实现前所未有的跨程序分析能力。仅模拟14个通用程序点，就能以86.3%的平均准确率估计10个SPEC CPU基准测试的性能，实现7143倍模拟加速。签名对新微架构表现出强大的适应性，仅需少量微调。

Conclusion: SemanticBBV通过语义感知的程序表示解决了传统BBV的局限性，实现了跨程序模拟重用，大幅提升了模拟效率，为微架构模拟开辟了新方向。

Abstract: For decades, sampling-based techniques have been the de facto standard for accelerating microarchitecture simulation, with the Basic Block Vector (BBV) serving as the cornerstone program representation. Yet, the BBV's fundamental limitations: order-dependent IDs that prevent cross-program knowledge reuse and a lack of semantic content predictive of hardware performance have left a massive potential for optimization untapped.
  To address these gaps, we introduce SemanticBBV, a novel, two-stage framework that generates robust, performance-aware signatures for cross-program simulation reuse. First, a lightweight RWKV-based semantic encoder transforms assembly basic blocks into rich Basic Block Embeddings (BBEs), capturing deep functional semantics. Second, an order-invariant Set Transformer aggregates these BBEs, weighted by execution frequency, into a final signature. Crucially, this stage is co-trained with a dual objective: a triplet loss for signature distinctiveness and a Cycles Per Instruction (CPI) regression task, directly imbuing the signature with performance sensitivity. Our evaluation demonstrates that SemanticBBV not only matches traditional BBVs in single-program accuracy but also enables unprecedented cross-program analysis. By simulating just 14 universal program points, we estimated the performance of ten SPEC CPU benchmarks with 86.3% average accuracy, achieving a 7143x simulation speedup. Furthermore, the signature shows strong adaptability to new microarchitectures with minimal fine-tuning.

</details>
