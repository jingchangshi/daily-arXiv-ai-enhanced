<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.AR](#cs.AR) [Total: 11]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A Data-driven Analysis of Code Optimizations](https://arxiv.org/abs/2511.06117)
*Yacine Hakimi,Riyadh Baghdadi*

Main category: cs.PL

TL;DR: 本文研究了自动代码优化中变换顺序对性能的影响，通过数据驱动方法分析固定顺序与任意顺序变换的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着计算需求增长，自动代码优化变得日益重要。编译器开发者面临设计选择：是允许任意顺序的代码变换（理论上性能最佳但搜索空间大），还是采用固定顺序（可能加速搜索但可能损失优化潜力）。

Method: 采用数据驱动方法：生成大量随机程序，应用随机优化序列，记录执行时间，并进行统计分析。

Result: 通过统计分析为自动代码优化算法的设计提供了指导性见解。

Conclusion: 研究为开发更高效的自动代码优化算法提供了数据支持，帮助在搜索空间和优化潜力之间找到平衡。

Abstract: As the demand for computational power grows, optimizing code through compilers becomes increasingly crucial. In this context, we focus on fully automatic code optimization techniques that automate the process of selecting and applying code transformations for better performance without manual intervention. Understanding how these transformations behave and interact is key to designing more effective optimization strategies. Compiler developers must make numerous design choices when constructing these heuristics. For instance, they may decide whether to allow transformations to be explored in any arbitrary order or to enforce a fixed sequence. While the former may theoretically offer the best performance gains, it significantly increases the search space. This raises an important question: Can a predefined, fixed order of applying transformations speed up the search without severely compromising optimization potential? In this paper, we address this and other related questions that arise in the design of automatic code optimization algorithms. Using a data-driven approach, we generate a large dataset of random programs, apply random optimization sequences, and record their execution times. Through statistical analysis, we provide insights that guide the development of more efficient automatic code optimization algorithms.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [DWM-RO: Decentralized World Models with Reasoning Offloading for SWIPT-enabled Satellite-Terrestrial HetNets](https://arxiv.org/abs/2511.05972)
*Guangyuan Liu,Yinqiu Liu,Ruichen Zhang,Dusit Niyato,Jiawen Kang,Sumei Sun,Abbas Jamalipour,Ping Zhang*

Main category: cs.DC

TL;DR: 提出DWM-RO框架解决卫星-地面网络中的SWIPT优化问题，通过世界模型和推理卸载机制显著提升样本效率和协调能力


<details>
  <summary>Details</summary>
Motivation: 无线网络向大规模连接和能效运行转变，卫星-地面架构与SWIPT集成面临时变信道和多层干扰等挑战，传统MARL存在样本效率低和协调差的问题

Method: 每个代理使用世界模型学习环境动态的紧凑预测表示，支持基于想象的策略训练；不确定性感知卸载门监控本地干扰和模型重建误差，触发选择性边缘协调；边缘轻量级潜在解相关机制优化代理策略表示

Result: DWM-RO收敛速度比最先进基线快5倍，频谱效率提高34.7%，约束违规减少40%；在10用户密集场景中，违规率保持在20%以下，而基线超过70%

Conclusion: DWM-RO框架通过世界模型和推理卸载有效解决了卫星-地面SWIPT网络中的协调和样本效率问题，展现了卓越的鲁棒性和性能优势

Abstract: Wireless networks are undergoing a paradigm shift toward massive connectivity with energy-efficient operation, driving the integration of satellite-terrestrial architectures with simultaneous wireless information and power transfer (SWIPT). Optimizing transmit beamforming and power splitting in such systems faces formidable challenges, e.g., time-varying channels and multi-tier interference, which create a complex decision landscape where conventional model-free multi-agent reinforcement learning (MARL) suffers from sample inefficiency due to rarely-encountered state transitions and poor coordination as decentralized agents act independently. This paper proposes the Decentralized World Model with Reasoning Offloading (DWM-RO) framework to address these fundamental limitations. Specifically, each agent employs a world model to learn compact predictive representations of environment dynamics, enabling imagination-based policy training that dramatically reduces required environment interactions. An uncertainty-aware offloading gate monitors local interference levels and model reconstruction errors to trigger selective edge coordination. When activated, a lightweight latent decorrelation mechanism at the edge refines agents' strategic representations, guiding them toward orthogonal actions that minimize resource conflicts. Extensive simulations demonstrate that DWM-RO converges 5 times faster than state-of-the-art baselines while achieving 34.7% higher spectral efficiency and reducing constraint violations by 40%. In dense network scenarios with 10 users, DWM-RO maintains violation rates below 20% while baselines exceed 70%, validating superior robustness.

</details>


### [3] [Inductive Loop Analysis for Practical HPC Application Optimization](https://arxiv.org/abs/2511.06052)
*Philipp Schaad,Tal Ben-Nun,Patrick Iff,Torsten Hoefler*

Main category: cs.DC

TL;DR: SILO是一种新颖的符号归纳循环优化技术，通过将数据访问和依赖关系建模为循环步长的函数，实现了顺序依赖循环的自动并行化以及数据移动优化，在科学计算应用中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 科学计算应用严重依赖多维数组的多级循环嵌套，但现有HPC框架的低级表示难以分析跨步数据访问和循环携带依赖等常见模式，限制了并行化和数据移动优化的潜力。

Method: 引入符号归纳循环优化(SILO)，将数据访问和依赖关系建模为循环步长的函数，支持自动并行化顺序依赖循环，并实现软件预取和指针递增等数据移动优化。

Result: 在科学计算应用的基础内核上，特别是大气模型和数值求解器，相比现有技术实现了高达12倍的加速。

Conclusion: SILO通过高级抽象有效解决了科学计算中循环优化的关键挑战，为自动并行化和数据移动优化提供了新的解决方案。

Abstract: Scientific computing applications heavily rely on multi-level loop nests operating on multidimensional arrays. This presents multiple optimization opportunities from exploiting parallelism to reducing data movement through prefetching and improved register usage. HPC frameworks often delegate fine-grained data movement optimization to compilers, but their low-level representations hamper analysis of common patterns, such as strided data accesses and loop-carried dependencies. In this paper, we introduce symbolic, inductive loop optimization (SILO), a novel technique that models data accesses and dependencies as functions of loop nest strides. This abstraction enables the automatic parallelization of sequentially-dependent loops, as well as data movement optimizations including software prefetching and pointer incrementation to reduce register spills. We demonstrate SILO on fundamental kernels from scientific applications with a focus on atmospheric models and numerical solvers, achieving up to 12$\times$ speedup over the state of the art.

</details>


### [4] [Elastic Data Transfer Optimization with Hybrid Reinforcement Learning](https://arxiv.org/abs/2511.06159)
*Rasman Mubtasim Swargo,Md Arifuzzaman*

Main category: cs.DC

TL;DR: 提出一种自适应数据传输方法，联合优化多个参数，结合启发式并行、无限流水线和深度强化学习并发优化器，通过轻量级网络模拟器大幅减少训练时间，在多种数据集上性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代科学数据采集产生PB级数据需要传输到地理上遥远的计算集群，现有工具要么依赖预配置会话（对非专业用户难以调优），要么仅自适应优化并发而忽略其他重要参数。

Method: 结合启发式并行、无限流水线和基于深度强化学习的并发优化器，使用轻量级网络模拟器将训练时间减少到4分钟以内，相比在线训练提供2750倍加速。

Result: 在多样化数据集上持续优于现有方法，相比最先进解决方案实现高达9.5倍的吞吐量提升。

Conclusion: 该方法通过联合考虑多个参数并提供高效的训练机制，显著提升了大规模科学数据传输的性能。

Abstract: Modern scientific data acquisition generates petabytes of data that must be transferred to geographically distant computing clusters. Conventional tools either rely on preconfigured sessions, which are difficult to tune for users without domain expertise, or they adaptively optimize only concurrency while ignoring other important parameters. We present \name, an adaptive data transfer method that jointly considers multiple parameters. Our solution incorporates heuristic-based parallelism, infinite pipelining, and a deep reinforcement learning based concurrency optimizer. To make agent training practical, we introduce a lightweight network simulator that reduces training time to less than four minutes and provides a $2750\times$ speedup compared to online training. Experimental evaluation shows that \name consistently outperforms existing methods across diverse datasets, achieving up to 9.5x higher throughput compared to state-of-the-art solutions.

</details>


### [5] [LiteCast: A Lightweight Forecaster for Carbon Optimizations](https://arxiv.org/abs/2511.06187)
*Mathew Joseph,Tanush Savadi,Abel Souza*

Main category: cs.DC

TL;DR: LiteCast是一种轻量级碳强度预测方法，仅需少量历史数据即可快速建模，在保持预测排名准确性的前提下实现近最优的碳减排效果。


<details>
  <summary>Details</summary>
Motivation: 传统碳强度预测方法需要大量历史数据和复杂模型，但精度提升并不总能转化为相应减排效益。研究发现保持预测排名比追求绝对精度更重要，因此需要更高效的预测策略。

Method: 提出LiteCast轻量级时间序列预测方法，仅需几天历史能源和天气数据，能够快速建模区域能源结构并估计碳强度，适应电网突变。

Result: 在50个全球区域的真实工作负载测试中，LiteCast优于现有最优方法，实现20%更高的减排效益，达到最大可获平均效益的97%，同时保持轻量高效。

Conclusion: LiteCast证明了碳强度预测无需高度精确的复杂模型，通过轻量级方法保持预测排名即可实现大部分减排效益，为碳感知优化提供了更实用的解决方案。

Abstract: Over recent decades, electricity demand has experienced sustained growth through widespread electrification of transportation and the accelerated expansion of Artificial Intelligence (AI). Grids have managed the resulting surges by scaling generation capacity, incorporating additional resources such as solar and wind, and implementing demand-response mechanisms. Altogether, these policies influence a region's carbon intensity by affecting its energy mix. To mitigate the environmental impacts of consumption, carbon-aware optimizations often rely on long-horizon, high-accuracy forecasts of the grid's carbon intensity that typically use compute intensive models with extensive historical energy mix data. In addition to limiting scalability, accuracy improvements do not necessarily translate into proportional increases in savings. Highlighting the need for more efficient forecasting strategies, we argue that carbon forecasting solutions can achieve the majority of savings without requiring highly precise and complex predictions. Instead, it is the preservation of the ranking of forecasts relative to the ground-truth that drives realized savings. In this paper, we present LiteCast, a lightweight time series forecasting method capable of quickly modeling a region's energy mix to estimate its carbon intensity. LiteCast requires only a few days of historical energy and weather data, delivering fast forecasts that can quickly adapt to sudden changes in the electrical grid. Our evaluation in 50 worldwide regions under various real-world workloads shows that LiteCast outperforms state-of-the-art forecasters, delivering 20% higher savings with near-optimal performance, achieving 97% of the maximum attainable average savings, while remaining lightweight, efficient to run, and adaptive to new data.

</details>


### [6] [Optimizing Long-context LLM Serving via Fine-grained Sequence Parallelism](https://arxiv.org/abs/2511.06247)
*Cong Li,Yuzhe Yang,Xuegui Zheng,Qifan Yang,Yijin Guan,Size Zheng,Li-Wen Chang,Shufan Liu,Xin Liu,Guangyu Sun*

Main category: cs.DC

TL;DR: 提出Chunkwise Dynamic Sequence Parallelism (CDSP)和Tetris系统，通过细粒度的序列并行分配策略，在LLM推理中实现更高效的资源利用和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有动态序列并行分配方法缺乏灵活性，无法支持LLM推理中阶段特定的并行需求，难以缓解过度并行分配导致的全局延迟恶化，以及无法有效利用因并行规模变化产生的资源碎片。

Method: 提出CDSP策略，在请求内部按token段分配序列并行大小；构建Tetris系统，将CDSP集成到解耦集群中满足并行异构性，基于实时负载动态调节并行大小扩展，自适应探索分块计划以利用碎片资源。

Result: 与最先进系统相比，Tetris在最大可持续负载下实现高达4.35倍的首token时间降低，中位token间隔时间减少高达40.1%，最大请求容量提升高达45%。

Conclusion: CDSP和Tetris系统通过细粒度的序列并行策略，显著提升了LLM推理服务的性能和资源利用率。

Abstract: With the advancement of large language models (LLMs), their context windows have rapidly expanded. To meet diverse demands from varying-length requests in online services, existing state-of-the-art systems tune the sequence parallelism (SP) allocation. However, current dynamic SP allocation lacks flexibility to (1) support stage-specific parallelism requirements in LLM inference, (2) mitigate the global latency degradation from excessive SP allocation, and (3) exploit resource fragments arising from SP size variation.
  To tackle this problem, we propose Chunkwise Dynamic Sequence Parallelism (CDSP), a fine-grained parallelism strategy that assigns SP sizes across \textit{intra-request} token segments. Based on CDSP, we build Tetris, an LLM serving system that (1) efficiently integrates CDSP into disaggregated cluster to satisfy parallelism heterogeneity, (2) dynamically regulates SP size expansion based on real-time load conditions, and (3) adaptively explores chunking plans to utilize fragmented resources while meeting per-request demands. Compared with state-of-the-art systems, Tetris achieves up to 4.35$\times$ lower time-to-first-token (TTFT) under max sustainable loads, reduces median time-between-tokens (TBT) by up to 40.1\%, and increases the max request capacity by up to 45\%.

</details>


### [7] [PRAGMA: A Profiling-Reasoned Multi-Agent Framework for Automatic Kernel Optimization](https://arxiv.org/abs/2511.06345)
*Kelun Lei,Hailong Yang,Huaitao Zhang,Xin You,Kaige Zhang,Zhongzhi Luan,Yi Liu,Depei Qian*

Main category: cs.DC

TL;DR: PRAGMA是一个基于性能分析的AI内核生成框架，通过将执行反馈和细粒度硬件性能分析整合到推理循环中，帮助LLM识别性能瓶颈并迭代优化代码质量。


<details>
  <summary>Details</summary>
Motivation: 现有AI内核生成系统仅依赖正确性或执行时间反馈，缺乏对底层性能瓶颈的推理能力，需要专家级调优和深度硬件理解。

Method: 引入PRAGMA框架，集成执行反馈和细粒度硬件性能分析，使LLM能够识别性能瓶颈、保留历史最佳版本并迭代优化代码。

Result: 在KernelBench上评估，PRAGMA始终优于未启用性能分析的基线AIKG，在CPU和GPU平台上分别实现2.81倍和2.30倍的平均加速比。

Conclusion: PRAGMA通过整合性能分析到AI内核生成过程中，显著提升了内核性能，证明了性能引导方法在自动化内核优化中的有效性。

Abstract: Designing high-performance kernels requires expert-level tuning and a deep understanding of hardware characteristics. Recent advances in large language models (LLMs) have enabled automated kernel generation, yet most existing systems rely solely on correctness or execution time feedback, lacking the ability to reason about low-level performance bottlenecks. In this paper, we introduce PRAGMA, a profile-guided AI kernel generation framework that integrates execution feedback and fine-grained hardware profiling into the reasoning loop. PRAGMA enables LLMs to identify performance bottlenecks, preserve historical best versions, and iteratively refine code quality. We evaluate PRAGMA on KernelBench, covering GPU and CPU backends. Results show that PRAGMA consistently outperforms baseline AIKG without profiling enabled and achieves 2.81$\times$ and 2.30$\times$ averaged speedups against Torch on CPU and GPU platforms, respectively.

</details>


### [8] [Saarthi: An End-to-End Intelligent Platform for Optimising Distributed Serverless Workloads](https://arxiv.org/abs/2511.06599)
*Siddharth Agarwal,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.DC

TL;DR: Saarthi是一个端到端的无服务器框架，通过输入感知的智能资源管理和多目标优化，解决了FaaS平台中的启动延迟、静态配置和资源分配问题，显著提升了吞吐量并降低了成本。


<details>
  <summary>Details</summary>
Motivation: FaaS平台虽然具有基础设施抽象、按需执行和无空闲资源定价等优势，但仍面临启动延迟、静态配置、资源分配和调度不优等问题，导致函数性能不一致和意外运营成本。

Method: Saarthi采用输入感知的方法，根据请求负载特征预测资源需求，实现函数规模调整和智能请求编排。集成了主动容错冗余机制，并使用多目标整数线性规划模型来维持最优函数数量。

Result: 在OpenFaaS上实现的Saarthi相比基线实现了1.45倍吞吐量提升、1.84倍成本降低，同时保持98.3%的服务水平目标，开销仅为0.2秒。

Conclusion: Saarthi代表了向自驱动无服务器平台迈出的重要一步，通过智能资源管理有效解决了FaaS平台的现有挑战。

Abstract: FaaS offers significant advantages with its infrastructure abstraction, on-demand execution, and attractive no idle resource pricing for modern cloud applications. Despite these benefits, challenges such as startup latencies, static configurations, sub-optimal resource allocation and scheduling still exist due to coupled resource offering and workload-agnostic generic scheduling behaviour. These issues often lead to inconsistent function performance and unexpected operational costs for users and service providers. This paper introduces Saarthi, a novel, end-to-end serverless framework that intelligently manages the dynamic resource needs of function workloads, representing a significant step toward self-driving serverless platforms. Unlike platforms that rely on static resource configurations, Saarthi is input-aware, allowing it to intelligently anticipate resource requirements based on the characteristics of an incoming request payload. This input-driven approach reinforces function right-sizing and enables smart request orchestration across available function configurations. Saarthi further integrates a proactive fault-tolerant redundancy mechanism and employs a multi-objective Integer Linear Programming (ILP) model to maintain an optimal function quantity. This optimisation aims to maximise system throughput while simultaneously reducing overall operational costs. We validate the effectiveness of Saarthi by implementing it as a framework atop OpenFaaS. Our results demonstrate Saarthi's ability to achieve up to 1.45x better throughput, 1.84x reduced costs, while maintaining up to 98.3% service level targets with an overhead of up to 0.2 seconds as compared to the baseline OpenFaaS.

</details>


### [9] [DMA Collectives for Efficient ML Communication Offloads](https://arxiv.org/abs/2511.06605)
*Suchita Pati,Mahzabeen Islam,Shaizeen Aga,Mohamed Assem Ibrahim*

Main category: cs.DC

TL;DR: 本文对将ML通信集合操作卸载到DMA引擎进行了全面分析，发现在大尺寸数据传输时DMA集合操作性能相当或更好，但在小尺寸时性能较差。通过优化实现显著缩小了性能差距。


<details>
  <summary>Details</summary>
Motivation: 将ML通信集合操作卸载到DMA引擎可以高效重叠计算和通信，释放GPU核心用于计算并减少内存子系统干扰。但之前的研究仅在有限背景下进行，需要更全面的分析。

Method: 在AMD Instinct MI300X GPU上对DMA集合操作进行性能、功耗/能量和同步成本分析，并与RCCL通信集合库比较。识别DMA传输延迟瓶颈，利用现有DMA架构创新构建优化实现。

Result: DMA集合操作在大尺寸（10MB到GB）时性能提升16%，功耗降低32%；但在小尺寸时性能较差（all-gather慢4.5倍，all-to-all慢2.5倍）。优化实现显著缩小了小尺寸性能差距（all-gather仅慢30%，all-to-all快20%），大尺寸性能进一步改善7%，功耗节省3-10%。

Conclusion: 这项工作使DMA集合操作更接近主流集合库的采用标准，通过优化实现了显著的性能改进和功耗节省。

Abstract: Offloading machine learning (ML) communication collectives to direct memory access (DMA) engines has emerged as an interesting and low-cost solution to efficiently overlap computation and communication in inference and training. Doing so delivers superior concurrent performance by freeing up all GPU cores for computation and also lowers interference in the memory sub-system (caches). While DMA collectives show strong promise, prior works have only studied them in limited context (bandwidth-bound transfer sizes only, performance-only).
  To address this, we provide a comprehensive performance, power/energy and synchronization costs analysis of offloading ML communication collectives (all-gather, all-to-all) to DMA engines on state-of-the-art AMD Instinct MI300X GPUs. Our analysis reveals that, compared to the state-of-the-art RCCL communication collectives library, DMA collectives are at-par or better for large sizes (10s of MB to GB) in terms of both performance (16% better) and power (32% better). However, they significantly lag for latency-bound small sizes; 4.5X and 2.5X slower for all-gather and all-to-all, respectively. We provide a detailed latency breakdown of a DMA transfer and identify that DMA command scheduling and synchronization costs can limit DMA collective performance. To tackle this, we harness existing DMA architecture innovations, hitherto untapped, to build optimized DMA collectives and demonstrate their efficacy on real hardware. Our optimized implementations considerably close the performance gap for DMA collectives at smaller sizes (30% slower and 20% faster all-gather and all-to-all, respectively) and further improves performance (by 7%) and power savings at larger sizes (3-10%). Overall, this work represents a significant step toward making DMA collectives suitable for adoption in mainstream collective libraries.

</details>


### [10] [Wireless Sensor Networks Nodes Clustering and Optimization Based on Fuzzy C-Means and Water Strider Algorithms](https://arxiv.org/abs/2511.06735)
*Raya Majid Alsharfa,Mahmood Mohassel Feghhi,Majid Hameed Majeed*

Main category: cs.DC

TL;DR: 提出了一种结合水黾算法和模糊C均值聚类的混合分簇协议，显著提升了无线传感器网络的能量效率和网络寿命。


<details>
  <summary>Details</summary>
Motivation: 无线传感器网络面临能量管理和网络寿命优化的关键挑战，主要受限于有限的电池资源和通信开销。

Method: 使用水黾算法进行簇头位置的全局优化，结合模糊C均值聚类进行节点成员分配的精细化处理，采用模糊边界。

Result: 相比现有混合方法，首节点死亡延迟16.1%，末节点死亡延长11.9%，剩余能量保留提高37.4%，簇内距离减少19.4%。

Conclusion: 该方法在能量效率、网络寿命和可扩展性方面均优于现有混合方法，具有收敛保证和近线性扩展特性。

Abstract: Wireless sensor networks (WSNs) face critical challenges in energy management and network lifetime optimization due to limited battery resources and communication overhead. This study introduces a novel hybrid clustering protocol that integrates the Water Strider Algorithm (WSA) with Fuzzy C-Means (FCM) clustering to achieve superior energy efficiency and network longevity. The proposed WSA-FCM method employs WSA for global optimization of cluster- head positions and FCM for refined node membership assignment with fuzzy boundaries. Through extensive experimentation across networks of 200-800 nodes with 10 independent simulation runs, the method demonstrates significant improvements: First Node Death (FND) delayed by 16.1% ($678\pm12$ vs $584\pm18$ rounds), Last Node Death (LND) extended by 11.9% ($1,262\pm8$ vs $1,128\pm11$ rounds), and 37.4% higher residual energy retention ($5.47\pm0.09$ vs $3.98\pm0.11$ J) compared to state-of-the-art hybrid methods. Intra-cluster distances are reduced by 19.4% with statistical significance (p < 0.001). Theoretical analysis proves convergence guarantees and complexity bounds of $O(n\times c\times T)$, while empirical scalability analysis demonstrates near-linear scaling behaviour. The method outperforms recent hybrid approaches including MOALO-FCM, MSSO-MST, Fuzzy+HHO, and GWO-FCM across all performance metrics with rigorous statistical validation.

</details>


### [11] [A GPU-boosted high-performance multi-working condition joint analysis framework for predicting dynamics of textured axial piston pump](https://arxiv.org/abs/2511.06824)
*Xin Yao,Yang Liu,Jin Jiang,Yesen Chen,Zhilong Chen,Hongkang Dong,Xiaofeng Wei,Teng Zhang,Dongyun Wang*

Main category: cs.DC

TL;DR: 提出了一种基于GPU加速的高性能多工况联合分析框架（GMAF），用于快速模拟轴向柱塞泵的动力学特性，特别是针对具有纹理表面的复杂情况。


<details>
  <summary>Details</summary>
Motivation: 传统CPU迭代方法在处理需要精细网格的纹理表面时效率低下，且无法进行多周期模拟，需要开发更高效的求解方法。

Method: 设计了GMAF框架，采用预条件共轭梯度法（PCG）和近似对称逐次超松弛预处理器（ASSOR），充分利用GPU的计算强度和并行计算能力。

Result: GMAF在模拟光滑和纹理表面轴向柱塞泵的多周期动力学方面表现出色，显著加速了压力场联合代数系统的建立和求解，以及油流力和力矩的数值积分。

Conclusion: 纹理表面能够提升压力承载能力和抗扭性能，压力场中出现与纹理对应的'台阶'结构，证明了纹理设计的有效性。

Abstract: Accurate simulation to dynamics of axial piston pump (APP) is essential for its design, manufacture and maintenance. However, limited by computation capacity of CPU device and traditional solvers, conventional iteration methods are inefficient in complicated case with textured surface requiring refined mesh, and could not handle simulation during multiple periods. To accelerate Picard iteration for predicting dynamics of APP, a GPU-boosted high-performance Multi-working condition joint Analysis Framework (GMAF) is designed, which adopts Preconditioned Conjugate Gradient method (PCG) using Approximate Symmetric Successive Over-Relaxation preconditioner (ASSOR). GMAF abundantly utilizes GPU device via elevating computational intensity and expanding scale of massive parallel computation. Therefore, it possesses novel performance in analyzing dynamics of both smooth and textured APPs during multiple periods, as the establishment and solution to joint algebraic system for pressure field are accelerated magnificently, as well as numerical integral for force and moment due to oil flow. Compared with asynchronized convergence strategy pursuing local convergence, synchronized convergence strategy targeting global convergence is adopted in PCG solver for the joint system. Revealed by corresponding results, oil force in axial direction and moment in circumferential directly respond to input pressure, while other components evolve in sinusoidal patterns. Specifically, force and moment due to normal pressure instantly reach their steady state initially, while ones due to viscous shear stress evolve during periods. After simulating dynamics of APP and pressure distribution via GMAF, the promotion of pressure capacity and torsion resistance due to textured surface is revealed numerically, as several 'steps' exist in the pressure field corresponding to textures.

</details>


### [12] [Resilient by Design - Active Inference for Distributed Continuum Intelligence](https://arxiv.org/abs/2511.07202)
*Praveen Kumar Donta,Alfreds Lapkovskis,Enzo Mingozzi,Schahram Dustdar*

Main category: cs.DC

TL;DR: 提出了一种概率主动推理弹性代理(PAIR-Agent)，用于在分布式计算连续体中实现系统弹性，通过构建因果故障图、管理确定性与不确定性以及自主修复来应对复杂异构设备中的故障。


<details>
  <summary>Details</summary>
Motivation: 在高度复杂和异构的分布式计算连续体设备中，故障是常态，确保跨层的可靠性和全局一致性仍然是一个主要挑战，特别是对于需要实时自适应协调的AI驱动工作负载。

Method: PAIR-Agent执行三个核心操作：(i)从设备日志构建因果故障图，(ii)使用马尔可夫毯和自由能原理识别故障并管理确定性与不确定性，(iii)通过主动推理自主修复问题。

Result: 通过持续监控和自适应重新配置，该代理在各种故障条件下保持服务连续性和稳定性。理论验证证实了所提出框架的可靠性和有效性。

Conclusion: PAIR-Agent框架为分布式计算连续体系统提供了一种有效的弹性解决方案，能够自主应对复杂故障场景，确保系统可靠运行。

Abstract: Failures are the norm in highly complex and heterogeneous devices spanning the distributed computing continuum (DCC), from resource-constrained IoT and edge nodes to high-performance computing systems. Ensuring reliability and global consistency across these layers remains a major challenge, especially for AI-driven workloads requiring real-time, adaptive coordination. This paper introduces a Probabilistic Active Inference Resilience Agent (PAIR-Agent) to achieve resilience in DCC systems. PAIR-Agent performs three core operations: (i) constructing a causal fault graph from device logs, (ii) identifying faults while managing certainties and uncertainties using Markov blankets and the free-energy principle, and (iii) autonomously healing issues through active inference. Through continuous monitoring and adaptive reconfiguration, the agent maintains service continuity and stability under diverse failure conditions. Theoretical validations confirm the reliability and effectiveness of the proposed framework.

</details>


### [13] [LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and Serving Techniques in LLM Infrastructure](https://arxiv.org/abs/2511.07229)
*Jaehong Cho,Hyunmin Choi,Jongse Park*

Main category: cs.DC

TL;DR: LLMServingSim2.0是一个用于探索大规模LLM服务系统中异构硬件的系统模拟器，解决了硬件模型集成困难和现有模拟器支持技术有限的问题。


<details>
  <summary>Details</summary>
Motivation: 解决前代系统的两个关键限制：(1) 由于缺乏清晰的抽象，将硬件模型集成到系统级模拟器中非常困难；(2) 现有模拟器仅支持一小部分服务技术，缺乏能够捕捉现代LLM服务广泛方法的工具。

Method: 采用基于轨迹的性能建模，配备算子级延迟分析器，支持通过单一命令集成新加速器。嵌入最新的服务技术，同时提供灵活的路由、缓存管理和调度策略接口。

Result: 在TPU案例研究中，分析器需要减少18.5倍的代码行数，性能优于前代的硬件模拟器集成。实验显示LLMServingSim2.0能够以1.9%的误差重现基于GPU的LLM服务，同时保持实用的模拟时间。

Conclusion: LLMServingSim2.0是一个全面的平台，为硬件开发者和LLM服务提供商提供了低成本的硬件扩展性和准确的性能模拟能力。

Abstract: This paper introduces LLMServingSim2.0, a system simulator designed for exploring heterogeneous hardware in large-scale LLM serving systems. LLMServingSim2.0 addresses two key limitations of its predecessor: (1) integrating hardware models into system-level simulators is non-trivial due to the lack of a clear abstraction, and (2) existing simulators support only a narrow subset of serving techniques, leaving no infrastructure that captures the breadth of approaches in modern LLM serving. To overcome these issues, LLMServingSim2.0 adopts trace-driven performance modeling, accompanied by an operator-level latency profiler, enabling the integration of new accelerators with a single command. It further embeds up-to-date serving techniques while exposing flexible interfaces for request routing, cache management, and scheduling policies. In a TPU case study, our profiler requires 18.5x fewer LoC and outperforms the predecessor's hardware-simulator integration, demonstrating LLMServingSim2.0's low-effort hardware extensibility. Our experiments further show that LLMServingSim2.0 reproduces GPU-based LLM serving with 1.9% error, while maintaining practical simulation time, making it a comprehensive platform for both hardware developers and LLM service providers.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [Production-Grade Local LLM Inference on Apple Silicon: A Comparative Study of MLX, MLC-LLM, Ollama, llama.cpp, and PyTorch MPS](https://arxiv.org/abs/2511.05502)
*Varun Rajesh,Om Jodhpurkar,Pooja Anbuselvan,Mantinder Singh,Ashok Jallepali,Shantanu Godbole,Pradeep Kumar Sharma,Hritvik Shrivastava*

Main category: cs.AR

TL;DR: 对苹果芯片上五个本地大语言模型运行时的系统性评估：MLX、MLC-LLM、llama.cpp、Ollama和PyTorch MPS，在M2 Ultra芯片上进行性能测试，涵盖生成吞吐量、首token延迟、长上下文处理等指标。


<details>
  <summary>Details</summary>
Motivation: 评估苹果芯片上不同LLM运行时的实际性能表现，为开发者在私有、设备端LLM推理场景下提供基于证据的框架选择建议。

Method: 在配备M2 Ultra处理器和192GB统一内存的Mac Studio上，使用Qwen-2.5模型家族，测试从几百到10万token的提示，测量首token时间、稳态吞吐量、延迟百分位数、长上下文行为、量化支持等指标。

Result: MLX实现最高持续生成吞吐量，MLC-LLM在中等提示大小下提供更低的首token延迟，llama.cpp在轻量级单流使用中高效，Ollama注重开发者体验但吞吐量和首token延迟较差，PyTorch MPS受限于大模型和长上下文的内存约束。

Conclusion: 苹果芯片推理框架虽然绝对性能仍落后于NVIDIA GPU系统，但正快速成熟为私有、设备端LLM推理的可行生产级解决方案，所有框架都在设备上完全执行且无遥测，确保强隐私保证。

Abstract: We present a systematic, empirical evaluation of five local large language model (LLM) runtimes on Apple Silicon: MLX, MLC-LLM, llama.cpp, Ollama, and PyTorch MPS. Experiments were conducted on a Mac Studio equipped with an M2 Ultra processor and 192 GB of unified memory. Using the Qwen-2.5 model family across prompts ranging from a few hundred to 100,000 tokens, we measure time-to-first-token (TTFT), steady-state throughput, latency percentiles, long-context behavior (key-value and prompt caching), quantization support, streaming performance, batching and concurrency behavior, and deployment complexity.
  Under our settings, MLX achieves the highest sustained generation throughput, while MLC-LLM delivers consistently lower TTFT for moderate prompt sizes and offers stronger out-of-the-box inference features. llama.cpp is highly efficient for lightweight single-stream use, Ollama emphasizes developer ergonomics but lags in throughput and TTFT, and PyTorch MPS remains limited by memory constraints on large models and long contexts.
  All frameworks execute fully on-device with no telemetry, ensuring strong privacy guarantees. We release scripts, logs, and plots to reproduce all results. Our analysis clarifies the design trade-offs in Apple-centric LLM deployments and provides evidence-based recommendations for interactive and long-context processing. Although Apple Silicon inference frameworks still trail NVIDIA GPU-based systems such as vLLM in absolute performance, they are rapidly maturing into viable, production-grade solutions for private, on-device LLM inference.

</details>


### [15] [STAR: Improving Lifetime and Performance of High-Capacity Modern SSDs Using State-Aware Randomizer](https://arxiv.org/abs/2511.06249)
*Omin Kwon,Kyungjun Oh,Jaeyong Lee,Myungsuk Kim,Jihong Kim*

Main category: cs.AR

TL;DR: 提出了一种名为STAR的新型数据随机化器，通过主动消除导致横向电荷扩散(LCS)保留错误的弱数据模式，显著提高3D NAND闪存的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着3D堆叠和多级单元技术的发展，NAND闪存面临新的可靠性挑战，特别是横向电荷扩散(LCS)，这显著增加了保留错误，影响采用高容量3D闪存的SSD寿命。

Method: 开发了状态感知随机化器(STAR)，通过主动消除导致LCS保留错误的弱数据模式，并采用优化方案将其高效集成到SSD控制器的现有I/O数据路径中。

Result: 基于160个真实3D NAND闪存芯片的表征结果，实验显示STAR可将SSD寿命提高至多2.3倍，并在实际工作负载下平均减少50%的读取延迟。

Conclusion: STAR方案能有效应对3D NAND闪存中的LCS问题，显著提升SSD可靠性和性能，且实现成本低。

Abstract: Although NAND flash memory has achieved continuous capacity improvements via advanced 3D stacking and multi-level cell technologies, these innovations introduce new reliability challenges, par- ticularly lateral charge spreading (LCS), absent in low-capacity 2D flash memory. Since LCS significantly increases retention errors over time, addressing this problem is essential to ensure the lifetime of modern SSDs employing high-capacity 3D flash memory. In this paper, we propose a novel data randomizer, STate-Aware Randomizer (STAR), which proactively eliminates the majority of weak data patterns responsible for retention errors caused by LCS. Unlike existing techniques that target only specific worst-case patterns, STAR effectively removes a broad spectrum of weak patterns, significantly enhancing reliability against LCS. By employing several optimization schemes, STAR can be efficiently integrated into the existing I/O datapath of an SSD controller with negligible timing overhead. To evaluate the proposed STAR scheme, we developed a STAR-aware SSD emulator based on characterization results from 160 real 3D NAND flash chips. Experimental results demonstrate that STAR improves SSD lifetime by up to 2.3x and reduces read latency by an average of 50% on real-world traces compared to conventional SSDs

</details>


### [16] [Precision-Scalable Microscaling Datapaths with Optimized Reduction Tree for Efficient NPU Integration](https://arxiv.org/abs/2511.06313)
*Stef Cuyckens,Xiaoling Yi,Robin Geens,Joren Dumoulin,Martin Wiesner,Chao Fang,Marian Verhelst*

Main category: cs.AR

TL;DR: 提出了一种混合精度可扩展的MX MAC设计，结合整数和浮点累加的优势，在SNAX NPU平台上实现高效混合精度计算


<details>
  <summary>Details</summary>
Motivation: 现有MX MAC设计面临关键权衡：整数累加需要昂贵的窄浮点乘积转换，而FP32累加存在量化损失和昂贵的归一化问题

Method: 设计混合精度可扩展的归约树用于MX MAC，结合两种方法的优势，实现高效混合精度累加；将8x8 MAC阵列集成到SNAX NPU平台

Result: 集成系统在MXINT8、MXFP8/6和MXFP4下分别实现657、1438-1675和4065 GOPS/W的能效，吞吐量分别为64、256和512 GOPS

Conclusion: 提出的混合精度可扩展MX MAC设计解决了现有方法的局限性，在NPU平台上实现了高效的混合精度计算性能

Abstract: Emerging continual learning applications necessitate next-generation neural processing unit (NPU) platforms to support both training and inference operations. The promising Microscaling (MX) standard enables narrow bit-widths for inference and large dynamic ranges for training. However, existing MX multiply-accumulate (MAC) designs face a critical trade-off: integer accumulation requires expensive conversions from narrow floating-point products, while FP32 accumulation suffers from quantization losses and costly normalization. To address these limitations, we propose a hybrid precision-scalable reduction tree for MX MACs that combines the benefits of both approaches, enabling efficient mixed-precision accumulation with controlled accuracy relaxation. Moreover, we integrate an 8x8 array of these MACs into the state-of-the-art (SotA) NPU integration platform, SNAX, to provide efficient control and data transfer to our optimized precision-scalable MX datapath. We evaluate our design both on MAC and system level and compare it to the SotA. Our integrated system achieves an energy efficiency of 657, 1438-1675, and 4065 GOPS/W, respectively, for MXINT8, MXFP8/6, and MXFP4, with a throughput of 64, 256, and 512 GOPS.

</details>


### [17] [Offloading Data Center Tax](https://arxiv.org/abs/2511.06558)
*Akshay Revankar,Charan Renganathan,Sartaj Wariah*

Main category: cs.AR

TL;DR: 该论文探讨了在数据中心中识别并共同卸载多个tax组件的机会，以MongoDB为例进行性能分析并提出卸载建议。


<details>
  <summary>Details</summary>
Motivation: 数据中心运行着多样化的负载，这些负载共享许多称为tax组件的底层功能。优化任何tax组件都能提高整个数据中心集群的性能。虽然通常通过将tax组件卸载到加速器来实现性能提升，但单独卸载每个组件并不现实。

Method: 使用DeathStarBench基准测试套件对MongoDB进行性能分析，识别其tax组件及其微架构影响，并基于分析结果提出卸载建议。

Result: 通过分析识别出了MongoDB中的tax组件，并提出了共同卸载多个tax组件的具体机会和建议。

Conclusion: 研究表明，通过识别和共同卸载多个tax组件，可以在数据中心环境中实现更有效的性能优化，而MongoDB作为一个广泛使用的微服务，是实施这种优化策略的理想案例。

Abstract: The data centers of today are running diverse workloads sharing many common lower level functions called tax components. Any optimization to any tax component will lead to performance improvements across the data center fleet. Typically, performance enhancements in tax components are achieved by offloading them to accelerators, however, it is not practical to offload every tax component. The goal of this paper is to identify opportunities to offload more than one tax component together. We focus on MongoDB which is a common microservice used in a large number of applications in the datacenter. We profile MongoDB running as part of the DeathStarBench benchmark suite, identifying its tax components and their microarchitectural implications. We make observations and suggestions based on the inferences made to offload a few of the tax components in this application.

</details>


### [18] [FPGA or GPU? Analyzing comparative research for application-specific guidance](https://arxiv.org/abs/2511.06565)
*Arnab A Purkayastha,Jay Tharwani,Shobhit Aggarwal*

Main category: cs.AR

TL;DR: 本文通过综合分析现有研究，为领域特定应用选择合适的硬件加速器（FPGA vs GPU）提供指导，重点关注各自的优势领域和应用场景。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注FPGA和GPU的性能指标比较，但缺乏对每种加速器最适合的特定应用类型的深入分析。本文旨在填补这一空白，帮助用户根据具体应用需求选择合适的硬件加速器。

Method: 通过分类整理各种研究文献，分析关键性能指标，系统评估FPGA和GPU在不同应用场景下的表现。

Result: 明确了FPGA和GPU各自的优势、局限性和理想使用场景，提供了基于性能、能效和可编程性权衡的实际建议。

Conclusion: 该研究为研究人员和从业者提供了实用的指导，帮助他们根据特定应用需求在FPGA和GPU之间做出明智的选择决策。

Abstract: The growing complexity of computational workloads has amplified the need for efficient and specialized hardware accelerators. Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) have emerged as prominent solutions, each excelling in specific domains. Although there is substantial research comparing FPGAs and GPUs, most of the work focuses primarily on performance metrics, offering limited insight into the specific types of applications that each accelerator benefits the most. This paper aims to bridge this gap by synthesizing insights from various research articles to guide users in selecting the appropriate accelerator for domain-specific applications. By categorizing the reviewed studies and analyzing key performance metrics, this work highlights the strengths, limitations, and ideal use cases for FPGAs and GPUs. The findings offer actionable recommendations, helping researchers and practitioners navigate trade-offs in performance, energy efficiency, and programmability.

</details>


### [19] [EONSim: An NPU Simulator for On-Chip Memory and Embedding Vector Operations](https://arxiv.org/abs/2511.06679)
*Sangun Choi,Yunho Oh*

Main category: cs.AR

TL;DR: EONSim是一个NPU模拟器，专门用于建模矩阵和嵌入向量操作，支持灵活的片上内存架构探索，验证显示其准确性高。


<details>
  <summary>Details</summary>
Motivation: 现有NPU模拟器主要关注矩阵计算，缺乏对嵌入向量操作中数据依赖性和非确定性内存访问的建模能力，而下一代NPU需要支持嵌入工作负载的灵活内存架构。

Method: 开发EONSim模拟器，整合已验证的矩阵计算性能模型和详细的嵌入访问内存模拟，支持多种片上内存管理策略。

Result: 与TPUv6e验证对比，EONSim实现了平均推理时间误差1.4%和平均片上内存访问计数误差2.2%。

Conclusion: EONSim能够全面建模矩阵和嵌入向量操作，为新兴NPU架构的灵活探索和设计提供了有效工具。

Abstract: Embedding vector operations are a key component of modern deep neural network workloads. Unlike matrix operations with deterministic access patterns, embedding vector operations exhibit input data-dependent and non-deterministic memory accesses. Existing neural processing unit (NPU) simulators focus on matrix computations with simple double-buffered on-chip memory systems, lacking the modeling capability for realistic embedding behavior. Next-generation NPUs, however, call for more flexible on-chip memory architectures that can support diverse access and management schemes required by embedding workloads. To enable flexible exploration and design of emerging NPU architectures, we present EONSim, an NPU simulator that holistically models both matrix and embedding vector operations. EONSim integrates a validated performance model for matrix computations with detailed memory simulation for embedding accesses, supporting various on-chip memory management policies. Validated against TPUv6e, EONSim achieves an average inference time error of 1.4\% and an average on-chip memory access count error of 2.2\%.

</details>


### [20] [Preemption-Enhanced Benchmark Suite for FPGAs](https://arxiv.org/abs/2511.06736)
*Arsalan Ali Malik,John Buchanan,Aydin Aysu*

Main category: cs.AR

TL;DR: 提出了首个开源FPGA抢占基准测试套件，包含27个多样化应用，支持抢占策略评估和调度算法测试，无需用户从头创建抢占工作负载。


<details>
  <summary>Details</summary>
Motivation: FPGA在云计算中日益重要，但缺乏标准化的基准测试框架来评估任务调度和抢占技术，现有研究使用专有或合成基准，限制了通用性和可比性。

Method: 开发包含27个应用的基准测试套件，涵盖密码学、AI/ML、计算密集型工作负载、通信系统和多媒体处理等领域，每个基准都集成完整的上下文保存和恢复机制。

Result: 提供了可重现研究和一致比较的基础，简化了FPGA调度策略测试，支持评估多租户FPGA系统中的调度公平性、资源分配效率和上下文切换性能。

Conclusion: 该基准套件不仅促进FPGA抢占和调度评估，还为操作系统研究提供支持，有助于开发更好的FPGA环境操作系统和调度策略，并提供了添加新基准的指南。

Abstract: Field-Programmable Gate Arrays (FPGAs) have become essential in cloud computing due to their reconfigurability, energy efficiency, and ability to accelerate domain-specific workloads. As FPGA adoption grows, research into task scheduling and preemption techniques has intensified. However, the field lacks a standardized benchmarking framework for consistent and reproducible evaluation. Many existing studies propose innovative scheduling or preemption mechanisms but often rely on proprietary or synthetic benchmarks, limiting generalizability and making comparison difficult. This methodical fragmentation hinders effective evaluation of scheduling strategies and preemption in multi-tenant FPGA environments.
  This paper presents the first open-source preemption-enabled benchmark suite for evaluating FPGA preemption strategies and testing new scheduling algorithms, without requiring users to create preemption workloads from scratch. The suite includes 27 diverse applications spanning cryptography, AI/ML, computation-intensive workloads, communication systems, and multimedia processing. Each benchmark integrates comprehensive context-saving and restoration mechanisms, facilitating reproducible research and consistent comparisons. Our suite not only simplifies testing FPGA scheduling policies but also benefits OS research by enabling the evaluation of scheduling fairness, resource allocation efficiency, and context-switching performance in multi-tenant FPGA systems, ultimately supporting the development of better operating systems and scheduling policies for FPGA-based environments. We also provide guidelines for adding new benchmarks, enabling future research to expand and refine FPGA preemption and scheduling evaluation.

</details>


### [21] [ASTER: Attention-based Spiking Transformer Engine for Event-driven Reasoning](https://arxiv.org/abs/2511.06770)
*Tamoghno Das,Khanh Phan Vu,Hanning Chen,Hyunwoo Oh,Mohsen Imani*

Main category: cs.AR

TL;DR: 提出了一种针对脉冲变压器的内存中心硬件加速器，采用混合模拟-数字PIM架构，通过输入稀疏优化和定制数据流，在边缘设备上实现低功耗、实时视觉处理。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络与变压器架构结合为边缘设备提供了生物启发的低功耗视觉推理机会，但高时间分辨率和二进制计算特性与传统数字硬件存在架构不匹配问题。

Method: 采用混合模拟-数字PIM架构，结合输入稀疏优化和定制数据流，最小化内存访问开销；提出基于贝叶斯优化的推理时软件优化，包括层跳过和时间步减少。

Result: 在图像和事件分类任务上，相比边缘GPU和先前PIM加速器分别实现约467倍和1.86倍能耗降低，同时在ImageNet上保持竞争力精度。

Conclusion: 这项工作通过脉冲变压器加速实现了新型智能边缘AI，为极端边缘的低功耗实时视觉处理提供了解决方案。

Abstract: The integration of spiking neural networks (SNNs) with transformer-based architectures has opened new opportunities for bio-inspired low-power, event-driven visual reasoning on edge devices. However, the high temporal resolution and binary nature of spike-driven computation introduce architectural mismatches with conventional digital hardware (CPU/GPU). Prior neuromorphic and Processing-in-Memory (PIM) accelerators struggle with high sparsity and complex operations prevalent in such models. To address these challenges, we propose a memory-centric hardware accelerator tailored for spiking transformers, optimized for deployment in real-time event-driven frameworks such as classification with both static and event-based input frames. Our design leverages a hybrid analog-digital PIM architecture with input sparsity optimizations, and a custom-designed dataflow to minimize memory access overhead and maximize data reuse under spatiotemporal sparsity, for compute and memory-efficient end-to-end execution of spiking transformers. We subsequently propose inference-time software optimizations for layer skipping, and timestep reduction, leveraging Bayesian Optimization with surrogate modeling to perform robust, efficient co-exploration of the joint algorithmic-microarchitectural design spaces under tight computational budgets. Evaluated on both image(ImageNet) and event-based (CIFAR-10 DVS, DVSGesture) classification, the accelerator achieves up to ~467x and ~1.86x energy reduction compared to edge GPU (Jetson Orin Nano) and previous PIM accelerators for spiking transformers, while maintaining competitive task accuracy on ImageNet dataset. This work enables a new class of intelligent ubiquitous edge AI, built using spiking transformer acceleration for low-power, real-time visual processing at the extreme edge.

</details>


### [22] [P3-LLM: An Integrated NPU-PIM Accelerator for LLM Inference Using Hybrid Numerical Formats](https://arxiv.org/abs/2511.06838)
*Yuzong Chen,Chao Fang,Xilai Dai,Yuheng Wu,Thierry Tambe,Marian Verhelst,Mohamed S. Abdelfattah*

Main category: cs.AR

TL;DR: P3-LLM是一个用于大语言模型推理的NPU-PIM集成加速器，采用混合数值格式和混合精度量化方案，通过轻量级计算单元和算子融合优化，在保持精度的同时显著提升计算吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有高精度PIM计算单元在DRAM技术中产生显著的面积和功耗开销，限制了有效的计算吞吐量，需要解决大语言模型推理中的内存带宽和计算需求挑战。

Method: 1. 提出灵活的混合精度量化方案，使用混合数值格式量化不同LLM操作数；2. 设计高效的PIM加速器协同设计，支持混合数值格式的轻量级计算单元；3. 通过算子融合优化低精度数据流，减少运行时反量化开销。

Result: 在代表性LLM和任务上，P3-LLM在KV缓存量化和权重激活量化方面达到最先进的量化精度，相比现有LLM加速器HBM-PIM、Ecco和Pimba分别实现4.9倍、2.0倍和3.4倍的平均加速。

Conclusion: P3-LLM通过混合数值格式和PIM架构协同设计，有效解决了LLM推理中的内存带宽和计算瓶颈，在保持精度的同时显著提升了推理效率。

Abstract: The substantial memory bandwidth and computational demand of large language models (LLMs) present critical challenges for efficient inference. To tackle this, the literature has explored heterogeneous systems that combine neural processing units (NPUs) with DRAM-based processing-in-memory (PIM) for LLM acceleration. However, existing high-precision (e.g., FP16) PIM compute units incur significant area and power overhead in DRAM technology, limiting the effective computation throughput. In this paper, we introduce P3-LLM, a novel NPU-PIM integrated accelerator for LLM inference using hybrid numerical formats. Our approach is threefold: First, we propose a flexible mixed-precision quantization scheme, which leverages hybrid numerical formats to quantize different LLM operands with high compression efficiency and minimal accuracy loss. Second, we architect an efficient PIM accelerator co-design for P3-LLM, featuring lightweight compute units to support our hybrid numerical formats. The enhanced PIM compute units significantly boost the computation throughput under iso-area constraints. Third, we optimize the low-precision dataflow of different LLM modules by applying operator fusion to minimize the overhead of runtime dequantization. Our evaluation on a diverse set of representative LLMs and tasks demonstrates that P3-LLM achieves state-of-the-art quantization accuracy in terms of both KV-cache-only quantization and weight-activation quantization. Combining the proposed quantization scheme with PIM architecture co-design, P3-LLM yields an average of $4.9\times$, $2.0\times$, and $3.4\times$ speedups over the state-of-the-art LLM accelerators HBM-PIM, Ecco, and Pimba, respectively. Our quantization code is available at https://github.com/yc2367/P3-LLM.git

</details>


### [23] [Optimizing GEMM for Energy and Performance on Versal ACAP Architectures](https://arxiv.org/abs/2511.06907)
*Ilias Papalamprou,Dimosthenis Masouros,Ioannis Loudaros,Francky Catthoor,Dimitrios Soudris*

Main category: cs.AR

TL;DR: 提出了一个针对AMD Versal ACAP的自动化框架，使用机器学习模型优化GEMM映射，在性能和能效方面相比现有方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: GEMM是深度学习和科学计算中的关键操作，在边缘环境中受限于性能和能效。AMD Versal ACAP的异构架构虽能解决这些问题，但GEMM映射复杂且现有方法缺乏对能效-性能权衡的考虑。

Method: 开发自动化框架，利用在约6000个板上实验数据训练的机器学习模型来指导设计空间探索，生成针对性能或能效优化的GEMM映射。

Result: 在Versal VCK190上的评估显示，相比最先进框架，吞吐量几何平均提升1.23倍（最高2.5倍），能效几何平均提升1.25倍（最高2.7倍）。

Conclusion: 基于机器学习的自动化框架能有效优化Versal ACAP上的GEMM映射，在性能和能效方面均优于传统分析方法。

Abstract: General Matrix Multiplication (GEMM) is a fundamental operation in many scientific workloads, signal processing, and particularly deep learning. It is often a bottleneck for performance and energy efficiency, especially in edge environments with tight resource and power constraints. AMD's Versal ACAP offers heterogeneous components (AIEs, PL, PS) that can address these challenges, but mapping GEMM across them is complex, with prior works largely overlooking energy-performance trade-offs. In this paper, we propose an automated framework for Versal ACAP that generates GEMM mappings optimized for either performance or energy efficiency. Unlike prior analytical approaches, our method leverages a Machine Learning (ML) model, trained on approximately 6000 on-board experiments of different GEMM mappings, to guide Design Space Exploration, yielding more efficient designs. Evaluation on the Versal VCK190 shows geomean improvements of 1.23x (up to 2.5x) in throughput and 1.25x (up to 2.7x) in energy efficiency over state-of-the-art frameworks.

</details>


### [24] [FPGA-Accelerated RISC-V ISA Extensions for Efficient Neural Network Inference on Edge Devices](https://arxiv.org/abs/2511.06955)
*Arya Parameshwara,Santosh Hanamappa Mokashi*

Main category: cs.AR

TL;DR: 本文提出了FPGA加速的RISC-V ISA扩展，用于在资源受限的边缘设备上实现高效的神经网络推理，在Xilinx PYNQ-Z2平台上实现了2.14倍的平均延迟加速和49.1%的能耗降低。


<details>
  <summary>Details</summary>
Motivation: 边缘AI部署面临计算性能、能效和资源约束之间的平衡挑战，需要为资源受限的边缘设备开发高效的神经网络推理解决方案。

Method: 设计了一个定制的RISC-V核心，包含四个新颖的ISA扩展（FPGA.VCONV、FPGA.GEMM、FPGA.RELU、FPGA.CUSTOM）和集成的神经网络加速器，在Xilinx PYNQ-Z2平台上实现和验证。

Result: 在四个基准模型（MobileNet V2、ResNet-18、EfficientNet Lite、YOLO Tiny）上，相比ARM Cortex-A9软件基准，实现了2.14倍平均延迟加速和49.1%能耗降低。硬件实现时钟频率50MHz，使用0.43% LUTs和11.4% BRAM。

Conclusion: 这项工作建立了一个可复现的ISA引导FPGA加速框架，通过牺牲峰值性能换取可编程性，补充了固定功能的ASIC解决方案。

Abstract: Edge AI deployment faces critical challenges balancing computational performance, energy efficiency, and resource constraints. This paper presents FPGA-accelerated RISC-V instruction set architecture (ISA) extensions for efficient neural network inference on resource-constrained edge devices. We introduce a custom RISC-V core with four novel ISA extensions (FPGA.VCONV, FPGA.GEMM, FPGA.RELU, FPGA.CUSTOM) and integrated neural network accelerators, implemented and validated on the Xilinx PYNQ-Z2 platform. The complete system achieves 2.14x average latency speedup and 49.1% energy reduction versus an ARM Cortex-A9 software baseline across four benchmark models (MobileNet V2, ResNet-18, EfficientNet Lite, YOLO Tiny). Hardware implementation closes timing with +12.793 ns worst negative slack at 50 MHz while using 0.43% LUTs and 11.4% BRAM for the base core and 38.8% DSPs when accelerators are active. Hardware verification confirms successful FPGA deployment with verified 64 KB BRAM memory interface and AXI interconnect functionality. All performance metrics are obtained from physical hardware measurements. This work establishes a reproducible framework for ISA-guided FPGA acceleration that complements fixed-function ASICs by trading peak performance for programmability.

</details>
