<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Enforcing Temporal Constraints for LLM Agents](https://arxiv.org/abs/2512.23738)
*Adharsh Kamath,Sishen Zhang,Calvin Xu,Shubham Ugare,Gagandeep Singh,Sasa Misailovic*

Main category: cs.PL

TL;DR: Agent-C是一个为LLM智能体提供运行时安全保障的框架，确保智能体遵守形式化的时序安全策略，通过SMT求解和约束生成技术实现100%的安全合规性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体被部署在安全关键应用中，但现有的护栏系统无法防止违反时序安全策略的行为（如先访问敏感数据后认证用户）。现有方法依赖不精确的自然语言指令或事后监控，缺乏形式化保证。

Method: Agent-C引入领域特定语言表达时序属性，将规范转换为一阶逻辑，使用SMT求解在token生成时检测不合规动作。当LLM试图生成不合规工具调用时，利用约束生成技术确保所有动作符合规范，并为不合规动作生成合规替代方案。

Result: 在零售客服和机票预订系统等实际应用中，Agent-C实现了完美安全（100%合规，0%伤害），同时提高了任务效用。在Claude Sonnet 4.5和GPT-5等模型上，合规率从77.4%/83.7%提升到100%，效用也分别从71.8%/66.1%提升到75.2%/70.6%。

Conclusion: Agent-C为可靠的智能体推理设定了新的最先进标准，通过形式化方法和运行时保证，在确保100%安全合规的同时提升任务效用，解决了现有护栏系统无法处理时序安全约束的根本问题。

Abstract: LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning.

</details>


### [2] [Towards representation agnostic probabilistic programming](https://arxiv.org/abs/2512.23740)
*Ole Fenske,Maximilian Popko,Sebastian Bader,Thomas Kirste*

Main category: cs.PL

TL;DR: 提出因子抽象作为概率编程的通用接口，解耦模型表示与推理算法，支持混合离散-连续模型的统一框架


<details>
  <summary>Details</summary>
Motivation: 当前概率编程语言和工具将模型表示与特定推理算法紧密耦合，限制了新表示形式或混合离散-连续模型的实验探索

Method: 引入包含五种基本操作的因子抽象，作为操作因子的通用接口，无论其底层表示形式如何

Result: 实现表示无关的概率编程，允许用户在统一框架内自由混合不同表示形式（如离散表、高斯分布、基于样本的方法）

Conclusion: 该框架支持当前工具无法充分表达的复杂混合模型的实用推理，为概率编程提供了更大的灵活性和表达能力

Abstract: Current probabilistic programming languages and tools tightly couple model representations with specific inference algorithms, preventing experimentation with novel representations or mixed discrete-continuous models. We introduce a factor abstraction with five fundamental operations that serve as a universal interface for manipulating factors regardless of their underlying representation. This enables representation-agnostic probabilistic programming where users can freely mix different representations (e.g. discrete tables, Gaussians distributions, sample-based approaches) within a single unified framework, allowing practical inference in complex hybrid models that current toolkits cannot adequately express.

</details>


### [3] [VGC: A High-Performance Zone-Based Garbage Collector Architecture for Python with Partitioning and Parallel Execution](https://arxiv.org/abs/2512.23768)
*Abdulla M*

Main category: cs.PL

TL;DR: VGC提出了一种新颖的双层垃圾收集框架，通过主动运行时管理和被动编译时优化，显著提升内存管理性能，减少暂停时间30%、内存使用25%。


<details>
  <summary>Details</summary>
Motivation: 传统垃圾收集器在从嵌入式设备到高性能并行架构的多样化系统中存在性能瓶颈，需要一种更高效、低开销的内存管理方案来满足现代并行应用的需求。

Method: 采用双层架构：Active VGC使用并发标记清除策略动态管理运行时对象；Passive VGC在编译时通过预测性内存映射优化静态对象分配，将对象对齐到缓存边界。

Result: 相比分代收集器，在多线程基准测试中暂停时间减少30%；内存碎片最小化，总内存使用减少25%；为现代并行应用提供更好的可扩展性。

Conclusion: VGC通过整合编译时和运行时优化，为从低级到高级编程环境的内存密集型系统提供了强大且适应性强的解决方案。

Abstract: The Virtual Garbage Collector (VGC) introduces a novel memory management framework designed to optimize performance across diverse systems, ranging from resource constrained embedded devices to high performance parallel architectures. Unlike conventional garbage collectors, VGC employs a dual layer architecture consisting of Active VGC and Passive VGC to enable efficient, low overhead memory management. Active VGC dynamically manages runtime objects using a concurrent mark and sweep strategy tailored for parallel workloads, reducing pause times by up to 30 percent compared to generational collectors in multithreaded benchmarks. Passive VGC operates at compile time and optimizes static object allocation through predictive memory mapping, minimizing fragmentation by aligning objects to cache boundaries. This separation of responsibilities ensures predictable memory access patterns, reduces total memory usage by up to 25 percent, and improves scalability for modern parallel applications. By integrating compile time and runtime optimizations, VGC provides a robust and adaptable solution for memory intensive systems across both low level and high level programming environments.

</details>


### [4] [State Space Estimation for DPOR-based Model Checkers](https://arxiv.org/abs/2512.23996)
*A. R. Balasubramanian,Mohammad Hossein Khoshechin Jorshari,Rupak Majumdar,Umang Mathur,Minjian Zhang*

Main category: cs.PL

TL;DR: 提出首个多项式时间无偏估计器，用于计算并发程序中的Mazurkiewicz迹等价类数量，解决基于枚举的模型检查中的资源分配问题。


<details>
  <summary>Details</summary>
Motivation: 在基于枚举的模型检查中，需要估计并发程序的迹等价类数量，以预测模型检查运行时间和评估搜索空间覆盖进度。现有方法要么计算复杂度高（#P-hard），要么无法有效近似。

Method: 1) 将无状态最优DPOR算法的探索过程视为有界深度和宽度的树；2) 应用Knuth经典估计器获得无偏估计；3) 使用随机枚举技术控制方差，通过维护每层部分路径的小种群来实现耦合演化。

Result: 在JMC模型检查器中实现该估计器，在共享内存基准测试中，即使状态空间有10^5-10^6个类，通过几百次试验也能获得稳定估计（通常在20%误差范围内）。

Conclusion: 提出了首个可证明的多项式时间无偏估计器，用于计算迹数量这一重要问题，为模型检查资源分配提供了实用工具，并能扩展到估计模型检查成本。

Abstract: We study the estimation problem for concurrent programs: given a bounded program $P$, estimate the number of Mazurkiewicz trace-equivalence classes induced by its interleavings. This quantity informs two practical questions for enumeration-based model checking: how long a model checking run is likely to take, and what fraction of the search space has been covered so far. We first show the counting problem is #P-hard even for restricted programs and, unless $P=NP$, inapproximable within any subexponential factor, ruling out efficient exact or randomized approximation algorithms. We give a Monte Carlo approach to obtain a poly-time unbiased estimator: we convert a stateless optimal DPOR algorithm into an unbiased estimator by viewing its exploration as a bounded-depth, bounded-width tree whose leaves are the maximal Mazurkiewicz traces. A classical estimator by Knuth, when run on this tree, yields an unbiased estimate. To control the variance, we apply stochastic enumeration by maintaining a small population of partial paths per depth whose evolution is coupled. We have implemented our estimator in the JMC model checker and evaluated it on shared-memory benchmarks. With modest budgets, our estimator yields stable estimates, typically within a 20% band, within a few hundred trials, even when the state space has $10^5$--$10^6$ classes. We also show how the same machinery estimates model-checking cost by weighting all explored graphs, not only complete traces. Our algorithms provide the first provable poly-time unbiased estimators for counting traces, a problem of considerable importance when allocating model checking resources.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Governing Cloud Data Pipelines with Agentic AI](https://arxiv.org/abs/2512.23737)
*Aswathnarayan Muthukrishnan Kirubakaran,Adithya Parthasarathy,Nitin Saksena,Ram Sekhar Bodala,Akshay Deshpande,Suhas Malempati,Shiva Carimireddy,Abhirup Mazumder*

Main category: cs.DC

TL;DR: Agentic Cloud Data Engineering平台通过AI代理实现云数据管道的智能控制，在动态工作负载下减少恢复时间45%、降低运营成本25%、减少人工干预70%，同时保持数据新鲜度和策略合规性。


<details>
  <summary>Details</summary>
Motivation: 当前云数据管道面临动态工作负载、演进模式、成本约束和严格治理要求，但大多数生产管道依赖静态配置和被动运维实践，导致恢复时间长、资源利用率低、人工开销大。需要更智能的自动化控制方案。

Method: 提出Agentic Cloud Data Engineering平台，采用策略感知控制架构，将有限AI代理集成到云数据管道的治理和控制平面。专门代理分析管道遥测和元数据，基于声明性成本和合规策略进行推理，提出受限操作动作（如自适应资源重配置、模式协调、自动化故障恢复），所有代理动作都经过治理策略验证以确保可预测和可审计行为。

Result: 使用公共企业风格数据集构建的代表性批处理和流分析工作负载进行评估。实验结果显示：与静态编排相比，平台将平均管道恢复时间减少高达45%，运营成本降低约25%，手动干预事件减少超过70%，同时保持数据新鲜度和策略合规性。

Conclusion: 策略受限的代理控制为在企业环境中治理云数据管道提供了有效且实用的方法，能够显著改善运维效率、降低成本并减少人工干预，同时确保合规性和可预测性。

Abstract: Cloud data pipelines increasingly operate under dynamic workloads, evolving schemas, cost constraints, and strict governance requirements. Despite advances in cloud-native orchestration frameworks, most production pipelines rely on static configurations and reactive operational practices, resulting in prolonged recovery times, inefficient resource utilization, and high manual overhead. This paper presents Agentic Cloud Data Engineering, a policy-aware control architecture that integrates bounded AI agents into the governance and control plane of cloud data pipelines. In Agentic Cloud Data Engineering platform, specialized agents analyze pipeline telemetry and metadata, reason over declarative cost and compliance policies, and propose constrained operational actions such as adaptive resource reconfiguration, schema reconciliation, and automated failure recovery. All agent actions are validated against governance policies to ensure predictable and auditable behavior. We evaluate Agentic Cloud Data Engineering platform using representative batch and streaming analytics workloads constructed from public enterprise-style datasets. Experimental results show that Agentic Cloud Data Engineering platform reduces mean pipeline recovery time by up to 45%, lowers operational cost by approximately 25%, and decreases manual intervention events by over 70% compared to static orchestration, while maintaining data freshness and policy compliance. These results demonstrate that policy-bounded agentic control provides an effective and practical approach for governing cloud data pipelines in enterprise environments.

</details>


### [6] [Squeezing Edge Performance: A Sensitivity-Aware Container Management for Heterogeneous Tasks](https://arxiv.org/abs/2512.23952)
*Yongmin Zhang,Pengyu Huang,Mingyi Dong,Jing Yao*

Main category: cs.DC

TL;DR: 提出基于测量的容器资源管理框架CRMS，通过非线性拟合模型和队列延迟建模，在边缘服务器上联合优化延迟和能耗，相比基线方法降低延迟14%以上。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中任务异构性和资源有限性给高效编排带来挑战，需要针对单边缘服务器上多异构应用的资源管理解决方案。

Method: 1) 通过大量剖析实验建立CPU/内存分配与处理延迟的非线性拟合模型；2) 结合队列延迟建模，构建MINLP问题；3) 分解为可处理的凸子问题，采用两阶段容器资源管理方案（CRMS），结合凸优化和贪心细化。

Result: CRMS相比启发式和基于搜索的基线方法，延迟降低超过14%，能效提升，具有多项式时间复杂度和准动态执行能力。

Conclusion: 提出的测量驱动容器资源管理框架为异构边缘环境提供实用、可扩展的解决方案，能有效处理动态工作负载特性。

Abstract: Edge computing enables latency-critical applications to process data close to end devices, yet task heterogeneity and limited resources pose significant challenges to efficient orchestration. This paper presents a measurement-driven, container-based resource management framework for intra-node optimization on a single edge server hosting multiple heterogeneous applications. Extensive profiling experiments are conducted to derive a nonlinear fitting model that characterizes the relationship among CPU/memory allocations and processing latency across diverse workloads, enabling reliable estimation of performance under varying configurations and providing quantitative support for subsequent optimization. Using this model and a queueing-based delay formulation, we formulate a mixed-integer nonlinear programming (MINLP) problem to jointly minimize system latency and power consumption, which is shown to be NP-hard. The problem is decomposed into tractable convex subproblems and solved through a two-stage container-based resource management scheme (CRMS) combining convex optimization and greedy refinement. The proposed scheme achieves polynomial-time complexity and supports quasi-dynamic execution under global resource constraints. Simulation results demonstrate that CRMS reduces latency by over 14\% and improves energy efficiency compared with heuristic and search-based baselines, offering a practical and scalable solution for heterogeneous edge environments with dynamic workload characteristics.

</details>


### [7] [Data Heterogeneity-Aware Client Selection for Federated Learning in Wireless Networks](https://arxiv.org/abs/2512.24286)
*Yanbing Yang,Huiling Zhu,Wenchi Cheng,Jingqing Wang,Changrun Chen,Jiangzhou Wang*

Main category: cs.DC

TL;DR: 提出联合客户端选择与资源分配方案，解决联邦学习中数据异构性导致的泛化误差、延迟和能耗问题


<details>
  <summary>Details</summary>
Motivation: 联邦学习在无线网络中面临通信计算资源限制和数据异构性挑战，特别是大规模网络中数据异构性会导致泛化误差增大、重复训练、能耗增加和延迟延长

Method: 首先理论分析客户端数据异构性对全局模型泛化误差的影响，然后构建联合优化问题最小化学习延迟和能耗同时约束泛化误差，提出联合客户端选择与资源分配方案，采用凸优化和松弛技术

Result: 仿真结果显示，相比不考虑数据异构性的基准方法，提出的CSRA方案获得更高的测试精度、更低的学习延迟和更低的能耗

Conclusion: 通过联合客户端选择和资源分配优化，能够有效应对联邦学习中的数据异构性挑战，提高系统效率

Abstract: Federated Learning (FL) enables mobile edge devices, functioning as clients, to collaboratively train a decentralized model while ensuring local data privacy. However, the efficiency of FL in wireless networks is limited not only by constraints on communication and computational resources but also by significant data heterogeneity among clients, particularly in large-scale networks. This paper first presents a theoretical analysis of the impact of client data heterogeneity on global model generalization error, which can result in repeated training cycles, increased energy consumption, and prolonged latency. Based on the theoretical insights, an optimization problem is formulated to jointly minimize learning latency and energy consumption while constraining generalization error. A joint client selection and resource allocation (CSRA) approach is then proposed, employing a series of convex optimization and relaxation techniques. Extensive simulation results demonstrate that the proposed CSRA scheme yields higher test accuracy, reduced learning latency, and lower energy consumption compared to baseline methods that do not account for data heterogeneity.

</details>


### [8] [PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression](https://arxiv.org/abs/2512.24449)
*Bo Jiang,Taolue Yang,Youyuan Liu,Xubin He,Sheng Di,Sian Jin*

Main category: cs.DC

TL;DR: PackKV是一个针对长上下文生成的KV缓存管理框架，通过创新的有损压缩技术显著减少内存占用，同时保持高计算效率


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型在长上下文推理时面临KV缓存内存占用过大的挑战，KV缓存可能达到数GB级别，限制了实际应用

Method: 提出PackKV框架，专门针对KV缓存数据特性设计有损压缩算法，结合系统架构协同设计，支持动态增长的KV缓存并保持计算效率

Result: 在相同精度损失下，相比现有量化方法，K缓存内存减少率平均提高153.2%，V缓存提高179.6%；在A100和RTX Pro 6000 GPU上，K矩阵吞吐量平均提高75.7%，V矩阵提高171.7%

Conclusion: PackKV为长上下文生成提供了高效的内存优化解决方案，显著减少KV缓存内存占用同时提升计算性能，代码已开源

Abstract: Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \textbf{153.2}\% higher memory reduction rate for the K cache and \textbf{179.6}\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \textbf{75.7}\% for K and \textbf{171.7}\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV

</details>


### [9] [Understanding LLM Checkpoint/Restore I/O Strategies and Patterns](https://arxiv.org/abs/2512.24511)
*Mikaila J. Gossman,Avinash Maurya,Bogdan Nicolae,Jon C. Calhoun*

Main category: cs.DC

TL;DR: 本文研究了LLM大规模训练中检查点/恢复的I/O性能问题，通过liburing库优化实现了比现有方案最高7.6倍的吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 随着LLM和基础模型规模扩大，检查点/恢复成为训练和推理的关键模式。在3D并行（张量、流水线、数据）环境下，检查点涉及众多进程管理各种形状和大小的张量，需要频繁持久化到稳定存储。这使检查点/恢复变成了具有数据量大、种类多、速度快特点的大数据I/O问题。工作流需要遍历完整的存储层次结构（从GPU内存到主机内存、本地存储再到外部存储库），各层级性能差异巨大，即使使用异步刷新/预取也会在高并发下产生瓶颈。

Method: 开发微基准测试来量化使用liburing时的权衡，评估聚合、对齐和I/O合并如何在缓冲I/O和直接I/O下相互作用。研究文件系统感知的聚合策略，分析未合并的小缓冲区操作与合成工作负载的性能差异。

Result: 未合并的小缓冲区操作使吞吐量相对于合成工作负载减半，而文件系统感知的聚合恢复了带宽并减少了元数据开销。与最先进的LLM检查点引擎相比，该方法实现了比DataStates-LLM高3.9倍的写入吞吐量，比TorchSnapshot高7.6倍。

Conclusion: 研究结果强调了需要与现代文件系统和I/O后端对齐的聚合和合并策略。liburing等内核加速I/O库在LLM检查点中的有效性值得进一步探索，文件系统感知的优化可以显著提升大规模模型训练的检查点性能。

Abstract: As LLMs and foundation models scale, checkpoint/restore has become a critical pattern for training and inference. With 3D parallelism (tensor, pipeline, data), checkpointing involves many processes, each managing numerous tensors of varying shapes and sizes, that must be persisted frequently to stable storage (e.g., parallel file systems). This turns checkpoint/restore into a big-data I/O problem characterized by volume, variety, and velocity. The workflow must traverse the full storage stack -- from GPU memory through host memory and local storage to external repositories -- whose tiers differ by orders of magnitude in performance, creating bottlenecks under concurrency even with asynchronous flush/prefetch. Kernel-accelerated I/O libraries such as \texttt{liburing} may mitigate these issues versus POSIX, but their effectiveness for LLM checkpointing remains underexplored. We develop microbenchmarks to quantify trade-offs when using \texttt{liburing}, evaluating how aggregation, alignment, and I/O coalescing interact under buffered and direct I/O. We find that uncoalesced small-buffer operations halve throughput relative to synthetic workloads, while file system-aware aggregation restores bandwidth and reduces metadata overhead. Compared to state-of-the-art LLM checkpointing engines, our approach achieves up to $3.9\times$ higher write throughput than DataStates-LLM and $7.6\times$ higher than TorchSnapshot. These results highlight the need for aggregation and coalescing strategies that align with modern file systems and I/O backends.

</details>


### [10] [Distributed Bilevel Optimization with Dual Pruning for Resource-limited Clients](https://arxiv.org/abs/2512.24667)
*Mingyi Li,Xiao Zhang,Ruisheng Zheng,Hongjian Shi,Yuan Yuan,Xiuzhen Cheng,Dongxiao Yu*

Main category: cs.DC

TL;DR: 提出首个资源自适应的分布式双层优化框架，通过二阶自由超梯度估计器，使客户端能根据可用资源优化子模型，达到渐进最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 随着大规模模型的发展，传统分布式双层优化算法无法直接应用于低资源客户端，主要原因是优化内外层函数涉及过多计算。

Method: 提出资源自适应分布式双层优化框架（RABO和RAFBO），采用二阶自由超梯度估计器，允许每个客户端根据可用资源优化子模型。

Result: 理论证明RABO和RAFBO都能达到渐进最优收敛率O(1/√(C_x*Q))，该收敛率由外层参数的最小覆盖率C_x*主导。在两个不同任务上的实验验证了方法的有效性和计算效率。

Conclusion: 提出的资源自适应框架解决了传统分布式双层优化在低资源环境下的计算瓶颈问题，实现了理论保证的高效收敛。

Abstract: With the development of large-scale models, traditional distributed bilevel optimization algorithms cannot be applied directly in low-resource clients. The key reason lies in the excessive computation involved in optimizing both the lower- and upper-level functions. Thus, we present the first resource-adaptive distributed bilevel optimization framework with a second-order free hypergradient estimator, which allows each client to optimize the submodels adapted to the available resources. Due to the coupled influence of partial outer parameters x and inner parameters y, it's challenging to theoretically analyze the upper bound regarding the globally averaged hypergradient for full model parameters. The error bound of inner parameter also needs to be reformulated since the local partial training. The provable theorems show that both RABO and RAFBO can achieve an asymptotically optimal convergence rate of $O(1/\sqrt{C_x^{\ast}Q})$, which is dominated by the minimum coverage of the outer parameter $C_x^{\ast}$. Extensive experiments on two different tasks demonstrate the effectiveness and computation efficiency of our proposed methods.

</details>


### [11] [AI-Driven Cloud Resource Optimization for Multi-Cluster Environments](https://arxiv.org/abs/2512.24914)
*Vinoth Punniyamoorthy,Akash Kumar Agarwal,Bikesh Kumar,Abhirup Mazumder,Kabilan Kannan,Sumit Saha*

Main category: cs.DC

TL;DR: 提出基于AI的多集群云系统自适应资源优化框架，通过预测学习和策略感知决策实现跨集群主动协调管理，相比传统反应式方法提升资源效率、稳定性和性能一致性。


<details>
  <summary>Details</summary>
Motivation: 现代云原生系统依赖多集群部署实现可扩展性、弹性和地理分布，但现有资源管理方法多为反应式和集群中心化，无法在动态工作负载下优化系统级行为，导致资源利用低效、适应延迟和运维开销增加。

Method: 集成预测学习、策略感知决策和持续反馈的AI驱动框架，通过分析跨集群遥测数据和历史执行模式，动态调整资源分配以平衡性能、成本和可靠性目标。

Result: 原型实现显示相比传统反应式方法，资源效率提升、工作负载波动时稳定更快、性能变异性降低，验证了智能自适应基础设施管理的有效性。

Conclusion: 智能自适应资源管理是构建可扩展和弹性云平台的关键使能技术，AI驱动框架能够实现跨集群的主动协调优化，提升整体系统效能。

Abstract: Modern cloud-native systems increasingly rely on multi-cluster deployments to support scalability, resilience, and geographic distribution. However, existing resource management approaches remain largely reactive and cluster-centric, limiting their ability to optimize system-wide behavior under dynamic workloads. These limitations result in inefficient resource utilization, delayed adaptation, and increased operational overhead across distributed environments. This paper presents an AI-driven framework for adaptive resource optimization in multi-cluster cloud systems. The proposed approach integrates predictive learning, policy-aware decision-making, and continuous feedback to enable proactive and coordinated resource management across clusters. By analyzing cross-cluster telemetry and historical execution patterns, the framework dynamically adjusts resource allocation to balance performance, cost, and reliability objectives. A prototype implementation demonstrates improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional reactive approaches. The results highlight the effectiveness of intelligent, self-adaptive infrastructure management as a key enabler for scalable and resilient cloud platforms.

</details>


### [12] [Reliable and Resilient Collective Communication Library for LLM Training and Serving](https://arxiv.org/abs/2512.25059)
*Wei Wang,Nengneng Yu,Sixian Xiong,Zaoxing Liu*

Main category: cs.DC

TL;DR: R²CCL是一个容错通信库，利用多网卡硬件实现无损、低开销的故障转移，显著减少GPU训练和推理中的网络故障恢复时间。


<details>
  <summary>Details</summary>
Motivation: 现代ML训练和推理扩展到成千上万个GPU，网络故障导致10-15%的GPU时间浪费在缓慢的恢复上。常见的网络错误和链路波动会触发超时，导致整个作业终止，迫使训练时进行昂贵的检查点回滚和推理时重新处理请求。

Method: R²CCL通过利用多网卡硬件实现容错通信，包括快速连接迁移、带宽感知负载重新分配和弹性集体算法，以在故障下保持进度。

Result: R²CCL对网卡故障具有高度鲁棒性，训练开销小于1%，推理开销小于3%。在8-GPU H100 InfiniBand服务器和模拟数百个GPU的大规模ML模拟器上评估，R²CCL分别比AdapCC和DejaVu基准性能提升12.18倍和47倍。

Conclusion: R²CCL通过硬件感知的容错设计，显著减少了ML训练和推理中的网络故障恢复开销，提高了大规模GPU集群的资源利用率。

Abstract: Modern ML training and inference now span tens to tens of thousands of GPUs, where network faults can waste 10--15\% of GPU hours due to slow recovery. Common network errors and link fluctuations trigger timeouts that often terminate entire jobs, forcing expensive checkpoint rollback during training and request reprocessing during inference. We present R$^2$CCL, a fault-tolerant communication library that provides lossless, low-overhead failover by exploiting multi-NIC hardware. R$^2$CCL performs rapid connection migration, bandwidth-aware load redistribution, and resilient collective algorithms to maintain progress under failures. We evaluate R$^2$CCL on two 8-GPU H100 InfiniBand servers and via large-scale ML simulators modeling hundreds of GPUs with diverse failure patterns. Experiments show that R$^2$CCL is highly robust to NIC failures, incurring less than 1\% training and less than 3\% inference overheads. R$^2$CCL outperforms baselines AdapCC and DejaVu by 12.18$\times$ and 47$\times$, respectively.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [HERO-Sign: Hierarchical Tuning and Efficient Compiler-Time GPU Optimizations for SPHINCS+ Signature Generation](https://arxiv.org/abs/2512.23969)
*Yaoyun Zhou,Qian Wang*

Main category: cs.AR

TL;DR: HERO Sign：一种GPU加速的SPHINCS+签名方案，通过层次化调优和编译时优化，显著提升后量子安全哈希签名性能


<details>
  <summary>Details</summary>
Motivation: SPHINCS+作为无状态哈希签名方案提供强后量子安全性，但其签名生成因密集哈希计算而缓慢。现有GPU优化未能充分利用Merkle树结构的内在并行性或缺乏跨不同计算内核的细粒度编译器级定制。

Method: 提出分层调优和高效编译时优化：1）重新审视SPHINCS+组件（FORS、MSS、WOTS+）间的数据独立性并行机会；2）针对FORS引入Tree Fusion策略，由自动Tree Tuning搜索算法指导；3）采用自适应编译策略，根据不同内核（FORS Sign、TREE Sign、WOTS+ Sign）自动选择PTX或本地代码路径；4）批量签名生成时使用基于任务图的构造优化内核级重叠。

Result: 在RTX 4090上，相比现有GPU实现，HERO Sign在SPHINCS+ 128f、192f、256f参数集下分别实现1.28-3.13、1.28-2.92、1.24-2.60倍的吞吐量提升。在A100、H100、GTX 2080上也观察到类似增益，内核启动延迟降低两个数量级。

Conclusion: HERO Sign通过层次化GPU调优和编译时优化，有效解决了SPHINCS+签名性能瓶颈，为后量子安全哈希签名提供了高效的GPU加速方案。

Abstract: SPHINCS+ is a stateless hash-based signature scheme that provides strong post quantum security, but its signature generation is slow due to intensive hash computations. GPUs offer massive parallelism that can potentially accelerate SPHINCS+ signatures. However, existing GPU-based optimizations either fail to fully exploit the inherent parallelism of SPHINCS+'s Merkle tree structure or lack fine-grained, compiler-level customization across its diverse computational kernels. This paper proposes HERO Sign, a GPU-accelerated SPHINCS+ implementation that adopts hierarchical tuning and efficient compiler time optimizations. HERO Sign reexamines the parallelization opportunities enabled by data independence across SPHINCS+ components, including FORS, MSS, and WOTS+. It introduces a Tree Fusion strategy for FORS, which contains a large number of independent branches. The fusion strategy is guided by an automated Tree Tuning search algorithm that adapts fusion schemes to different GPU architectures. To further improve performance, HERO Sign employs an adaptive compilation strategy that accounts for the varying effectiveness of compiler optimizations across SPHINCS+ kernels such as FORS Sign, TREE Sign, and WOTS+ Sign. During compilation, the strategy automatically selects between PTX and native code paths to maximize efficiency. For batched signature generation, HERO Sign optimizes kernel-level overlapping using a task graph-based construction to reduce multi-stream idle time and kernel launch overhead. Experimental results show that, compared to state of the art GPU implementations, HERO Sign achieves throughput improvements of 1.28-3.13, 1.28-2.92, and 1.24-2.60 under the SPHINCS+ 128f, 192f, and 256f parameter sets on RTX 4090. Similar gains are observed on A100, H100, and GTX 2080, along with a two orders of magnitude reduction in kernel launch latency.

</details>
