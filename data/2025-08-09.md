<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Consistent Updates for Scalable Microservices](https://arxiv.org/abs/2508.04829)
*Devora Chait-Roth,Kedar S. Namjoshi,Thomas Wies*

Main category: cs.PL

TL;DR: 论文提出了首个保证混合模式更新一致性的算法，基于服务行为的语义属性（如交换性），并证明语义感知是避免不一致的必要条件。


<details>
  <summary>Details</summary>
Motivation: 在线服务通常采用可扩展的微服务架构，但动态更新功能时，新旧版本工作进程的混合操作可能导致不一致。现有方法要么效率低下，要么无法保证一致性。

Method: 提出基于服务行为语义属性的算法，并开发理论框架，形式化原子更新的直觉，推导新算法并证明其正确性。

Result: 证明了语义感知是避免不一致的必要条件，并提出了首个保证混合模式更新一致性的算法。

Conclusion: 通过语义感知和理论框架，论文解决了混合模式更新的不一致问题，为动态服务更新提供了可靠方法。

Abstract: Online services are commonly implemented with a scalable microservice
architecture, where isomorphic worker processes service client requests,
recording persistent state in a backend data store. To maintain service, any
modifications to the service functionality must be made on the fly -- i.e., as
the service continues to process client requests -- but doing so is
challenging. The central difficulty is that of avoiding potential
inconsistencies caused by ''mixed mode'' operation, where workers of current
and new versions are concurrently active and interact via the data store. Some
update methods avoid mixed mode altogether, but only at the cost of substantial
inefficiency -- by doubling resources (memory and compute), or by halving
throughput. The alternative is a so-called ''rolling'' update, which is
uncontrolled and runs the risk of serious service failures arising from
inconsistent mixed-mode behavior.
  In this paper, we present the first algorithms that guarantee consistency for
mixed mode updates. The algorithms rely on semantic properties of service
actions, such as commutativity. We show that semantic awareness is required, by
proving that any semantically oblivious, mixed-mode update method cannot avoid
inconsistencies. Ideally, it should appear to every client that a service
update takes effect atomically; this ensures that a client is not exposed to
inconsistent mixed-mode behavior. We introduce a framework that formalizes this
intuition and develop foundational theory for reasoning about the consistency
of mixed-mode updates, applying that theory to derive the new algorithms and
establish their correctness.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [OPTIMUMP2P: Fast and Reliable Gossiping in P2P Networks](https://arxiv.org/abs/2508.04833)
*Nicolas Nicolaou,Onyeka Obi,Aayush Rajasekaran,Alejandro Bergasov,Aleksandr Bezobchuk,Kishori M. Konwar,Michael Meier,Santiago Paiva,Har Preet Singh,Swarnabha Sinha*

Main category: cs.DC

TL;DR: OPTIMUMP2P是一种新型的gossip算法，利用RLNC技术提升P2P网络中信息传播的性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 提升libp2p中现有gossip算法的性能和可靠性，尤其是在存在恶意行为的情况下。

Method: 引入OPTIMUMP2P算法，结合RLNC技术，优化信息传播速度和可靠性。

Result: 在仿真和实际环境中，OPTIMUMP2P的性能优于现有的Gossipsub协议。

Conclusion: OPTIMUMP2P通过RLNC技术显著提升了信息传播效率和可靠性，适用于P2P网络。

Abstract: Gossip algorithms are pivotal in the dissemination of information within
decentralized systems. Consequently, numerous gossip libraries have been
developed and widely utilized especially in blockchain protocols for the
propagation of blocks and transactions. A well-established library is libp2p,
which provides two gossip algorithms: floodsup and gossibsup. These algorithms
enable the delivery of published messages to a set of peers. In this work we
aim to enhance the performance and reliability of libp2p by introducing
OPTIMUMP2P, a novel gossip algorithm that leverages the capabilities of Random
Linear Network Coding (RLNC) to expedite the dissemination of information in a
peer-to-peer (P2P) network while ensuring reliable delivery, even in the
presence of malicious actors capable of corrupting the transmitted data.
Preliminary research from the Ethereum Foundation has demonstrated the use of
RLNC in the significant improvement in the block propagation time [14]. Here we
present extensive evaluation results both in simulation and real-world
environments that demonstrate the performance gains of OPTIMUMP2P over the
Gossipsub protocol.

</details>


### [3] [Linear Search for Capturing an Oblivious Mobile Target in the Sender/Receiver Model](https://arxiv.org/abs/2508.04870)
*Khaled Jawhar,Evangelos Kranakis*

Main category: cs.DC

TL;DR: 论文研究两个具有不同通信能力的自主机器人如何通过线性搜索捕获一个移动目标，分析了在不同通信模型下的竞争比。


<details>
  <summary>Details</summary>
Motivation: 探讨不对称通信能力如何影响线性搜索的效率，特别是在目标移动方向已知的情况下。

Method: 设计了新的线性搜索算法，考虑了目标的不同移动模型（远离或接近原点）以及机器人对环境的了解程度。

Result: 分析了不同场景下捕获目标所需时间的竞争比，揭示了不对称通信对搜索效率的影响。

Conclusion: 研究为理解不对称通信在线性搜索中的作用提供了新的见解。

Abstract: We consider linear search for capturing an oblivious moving target by two
autonomous robots with different communicating abilities. Both robots can
communicate Face-to-Face (F2F) when co-located but in addition one robot is a
Sender (can also send messages wirelessly) and the other also a Receiver (can
also receive messages wirelessly). This is known as Sender/Receiver (S/R, for
short) communication model. The robots can move with max speed $1$. The moving
target starts at distance $d$ from the origin and can move either with speed
$v<1$ away from the origin in the ``away'' model or with speed $v \geq 0$
toward the origin in the ``toward'' model. We assume that the direction of
motion of the target (i.e., whether it is the away or toward model) is known to
the robots in advance. To capture the target the two robots must be co-located
with it.
  We design new linear search algorithms and analyze the competitive ratio of
the time required to capture the target. The approach takes into account
various scenarios related to what the robots know about the search environment
(e.g., starting distance or speed of the mobile, away or toward model, or a
combination thereof). Our study contributes to understanding how asymmetric
communication affects the competitive ratio of linear search.

</details>


### [4] [Managing, Analyzing and Sharing Research Data with Gen3 Data Commons](https://arxiv.org/abs/2508.04944)
*Craig Barnes,Kyle Burton,Michael S. Fitzsimons,Hara Prasad Juvvala,Brienna Larrick,Christopher Meyer,Pauline Ribeyre,Ao Liu,Clint Malson,Noah Metoki-Shlubsky,Andrii Prokhorenkov,Jawad Qureshi,Radhika Reddy,L. Philip Schumm,Mingfei Shao,Trevar Simmons,Alexander VanTol,Peter Vassilatos,Aarti Venkat,Robert L. Grossman*

Main category: cs.DC

TL;DR: Gen3是一个开源数据平台，用于构建数据共享空间，支持管理、分析和共享研究数据。


<details>
  <summary>Details</summary>
Motivation: 为研究社区提供一个云数据平台，促进数据的共享与管理。

Method: 通过定义数据模型自动生成数据门户和FAIR API，基于标准化软件服务构建。

Result: 已支持构建十多个数据共享空间，管理28 PB数据和6400万个FAIR数据对象。

Conclusion: Gen3是一个灵活、可互操作的数据平台，适用于当前和未来的数据生态系统。

Abstract: Gen3 is an open-source data platform for building data commons. A data
commons is a cloud-based data platform for managing, analyzing, and sharing
data with a research community. Gen3 has been used to build over a dozen data
commons that in aggregate contain over 28 PB of data and 64 million FAIR data
objects. To set up a Gen3 data commons, you first define a data model. Gen3
then autogenerates 1) a data portal for searching and exploring data in the
commons; 2) a data portal for submitting data to the commons; and 3) FAIR APIs
for accessing the data programmatically. Gen3 is built over a small number of
standards-based software services, which are designed to support current and
future Gen3 components so that Gen3 can interoperate with other data platforms
and data ecosystems.

</details>


### [5] [Tesserae: Scalable Placement Policies for Deep Learning Workloads](https://arxiv.org/abs/2508.04953)
*Song Bian,Saurabh Agarwal,Md. Tareq Mahmood,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 论文提出了一种基于图匹配的调度策略，用于优化深度学习集群的资源利用率和任务调度性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型训练已成为数据中心的主要工作负载，提高资源利用率是调度器的关键目标。现有调度策略要么性能不佳，要么扩展性差。

Method: 将调度约束建模为图匹配问题，设计了新的调度策略以减少任务迁移开销和提高任务打包效率。

Result: 实验表明，Tesserae调度器平均任务完成时间（JCT）提升1.62倍，总完成时间（Makespan）提升1.15倍。

Conclusion: 基于图匹配的调度策略显著提升了深度学习集群的调度性能和资源利用率。

Abstract: Training deep learning (DL) models has become a dominant workload in
data-centers and improving resource utilization is a key goal of DL cluster
schedulers. In order to do this, schedulers typically incorporate placement
policies that govern where jobs are placed on the cluster. Existing placement
policies are either designed as ad-hoc heuristics or incorporated as
constraints within a complex optimization problem and thus either suffer from
suboptimal performance or poor scalability. Our key insight is that many
placement constraints can be formulated as graph matching problems and based on
that we design novel placement policies for minimizing job migration overheads
and job packing. We integrate these policies into Tesserae and describe how our
design leads to a scalable and effective GPU cluster scheduler. Our
experimental results show that Tesserae improves average JCT by up to 1.62x and
the Makespan by up to 1.15x compared with the existing schedulers.

</details>


### [6] [Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations](https://arxiv.org/abs/2508.05020)
*Anjiang Wei,Hang Song,Mert Hidayetoglu,Elliott Slaughter,Sanjiva K. Lele,Alex Aiken*

Main category: cs.DC

TL;DR: 开发了一种基于AMR的高阶求解器，解决了动态数据结构、网格有效性等问题，并通过任务融合和GPU优化显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 高阶求解器在科学计算中至关重要，AMR能降低计算成本，但实现中存在挑战。

Method: 使用Regent语言开发AMR求解器，解决动态数据结构、任务融合和GPU优化问题。

Result: 任务融合实现18倍加速，GPU优化提升9.7倍性能。

Conclusion: 通过模拟验证了方法的有效性，适用于可压缩流问题。

Abstract: High-order solvers for compressible flows are vital in scientific
applications. Adaptive mesh refinement (AMR) is a key technique for reducing
computational cost by concentrating resolution in regions of interest. In this
work, we develop an AMR-based numerical solver using Regent, a high-level
programming language for the Legion programming model. We address several
challenges associated with implementing AMR in Regent. These include dynamic
data structures for patch refinement/coarsening, mesh validity enforcement, and
reducing task launch overhead via task fusion. Experimental results show that
task fusion achieves 18x speedup, while automated GPU kernel generation via
simple annotations yields 9.7x speedup for the targeted kernel. We demonstrate
our approach through simulations of two canonical compressible flow problems
governed by the Euler equations.

</details>


### [7] [Theseus: A Distributed and Scalable GPU-Accelerated Query Processing Platform Optimized for Efficient Data Movement](https://arxiv.org/abs/2508.05029)
*Felipe Aramburú,William Malpica,Kaouther Abrougui,Amin Aramoon,Romulo Auccapuclla,Claude Brisson,Matthijs Brobbel,Colby Farrell,Pradeep Garigipati,Joost Hoozemans,Supun Kamburugamuve,Akhil Nair,Alexander Ocsa,Johan Peltenburg,Rubén Quesada López,Deepak Sihag,Ahmet Uyar,Dhruv Vats,Michael Wendt,Jignesh M. Patel,Rodrigo Aramburú*

Main category: cs.DC

TL;DR: Theseus是一个分布式加速器原生查询引擎，旨在优化数据移动、内存利用和计算，显著提升大规模数据分析性能。


<details>
  <summary>Details</summary>
Motivation: 降低大规模数据分析的成本并提高吞吐量，利用GPU等加速器优化查询处理。

Method: 设计Theseus引擎，采用异步控制机制、固定大小页锁定主机内存分配等技术，优化数据移动和计算。

Result: 在TPC-H基准测试中，Theseus性能优于Databricks Photon高达4倍，且能以2个DGX A100节点处理100TB规模的数据。

Conclusion: Theseus为大规模数据分析提供了一种高效、低成本的解决方案，适用于企业级应用。

Abstract: Online analytical processing of queries on datasets in the many-terabyte
range is only possible with costly distributed computing systems. To decrease
the cost and increase the throughput, systems can leverage accelerators such as
GPUs, which are now ubiquitous in the compute infrastructure. This introduces
many challenges, the majority of which are related to when, where, and how to
best move data around the system. We present Theseus -- a production-ready
enterprise-scale distributed accelerator-native query engine designed to
balance data movement, memory utilization, and computation in an
accelerator-based system context. Specialized asynchronous control mechanisms
are tightly coupled to the hardware resources for the purpose of network
communication, data pre-loading, data spilling across memories and storage, and
GPU compute tasks. The memory subsystem contains a mechanism for fixed-size
page-locked host memory allocations to increase throughput and reduce memory
fragmentation. For the TPC-H benchmarks at scale factors ranging from 1k to 30k
on cloud infrastructure, Theseus outperforms Databricks Photon by up to
$4\times$ at cost parity. Theseus is capable of processing all queries of the
TPC-H and TPC-DS benchmarks at scale factor 100k (100 TB scale) with as few as
2 DGX A100 640GB nodes.

</details>


### [8] [Simulating LLM training workloads for heterogeneous compute and network infrastructure](https://arxiv.org/abs/2508.05370)
*Sumit Kumar,Arjun Temura,Naman Sharma,Ramanjeet Singh,Meet Dadhania,Praveen Tammana,Satananda Burla,Abed Mohammad Kamaluddin,Rinku Shah*

Main category: cs.DC

TL;DR: 论文提出了一种异构感知的分布式LLM模拟器，用于预测训练时间并支持自定义设备配置，解决了现有模拟器假设同构基础设施的局限性。


<details>
  <summary>Details</summary>
Motivation: 大规模GPU集群在分布式模型训练中的需求增长，但现有LLM训练模拟器假设同构基础设施，无法应对实际中的设备异构性。

Method: 设计了一种异构感知的分布式LLM模拟器，支持自定义设备组配置和非均匀工作负载分区。

Result: 初步模拟结果表明异构性对模型计算和通信时间有显著影响。

Conclusion: 异构感知模拟器填补了现有技术与实际需求之间的差距，为分布式训练提供了更准确的预测工具。

Abstract: The growing demand for large-scale GPU clusters in distributed model training
presents a significant barrier to innovation, particularly in model
optimization, performance tuning, and system-level enhancements. To address
this challenge, LLM training simulators are employed to estimate training time
and guide design decisions. However, the state-of-the-art LLM training
simulators assume homogeneous compute and network infrastructure. In practice,
device heterogeneity is inevitable due to resource sharing in cloud
environments, frequent shifts in device generations, and inherent intra-chip
interconnect heterogeneity. To address the gap between state-of-the-art and
practical requirements, we propose the design of a heterogeneity-aware
distributed LLM simulator capable of predicting training time while enabling
abstractions to specify custom configurations for device groups and
device-to-parallelism mapping. We present the design requirements and
challenges in building a heterogeneity-aware distributed ML training simulator,
and design components such as non-uniform workload partitioning. Our initial
simulation results demonstrate the impact of heterogeneity on the model
computation and communication time.

</details>


### [9] [Adaptive Parallel Downloader for Large Genomic Datasets](https://arxiv.org/abs/2508.05511)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: FastBioDL是一种用于大型生物数据集的并行文件下载工具，通过动态调整并发连接数优化下载速度，比现有工具快4倍。


<details>
  <summary>Details</summary>
Motivation: 现有下载工具的静态并发设置无法适应动态网络条件，导致带宽利用低效和下载时间长。

Method: FastBioDL将下载过程建模为在线优化问题，使用效用函数和梯度下降实时动态调整并发流数。

Result: 在公共基因组数据集上，FastBioDL比现有工具快4倍；在高速网络中快2.1倍。

Conclusion: FastBioDL为大规模基因组数据获取提供了高效解决方案，无需专业商业软件或协议。

Abstract: Modern next-generation sequencing (NGS) projects routinely generate terabytes
of data, which researchers commonly download from public repositories such as
SRA or ENA. Existing download tools often employ static concurrency settings,
leading to inefficient bandwidth utilization and prolonged download times due
to their inability to adapt to dynamic network conditions. We introduce
FastBioDL, a parallel file downloader designed for large biological datasets,
featuring an adaptive concurrency controller. FastBioDL frames the download
process as an online optimization problem, utilizing a utility function and
gradient descent to adjust the number of concurrent socket streams in real-time
dynamically. This approach maximizes download throughput while minimizing
resource overhead. Comprehensive evaluations on public genomic datasets
demonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art
tools. Moreover, in high-speed network experiments, its adaptive design was up
to $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP
or FTP downloads on the client side, FastBioDL provides a robust and efficient
solution for large-scale genomic data acquisition, democratizing
high-performance data retrieval for researchers without requiring specialized
commercial software or protocols.

</details>


### [10] [Modular Architecture for High-Performance and Low Overhead Data Transfers](https://arxiv.org/abs/2508.05546)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: AutoMDT是一种基于深度强化学习的模块化数据传输架构，通过优化并发级别显著提升数据传输性能。


<details>
  <summary>Details</summary>
Motivation: 传统文件传输工具因固定配置或单一优化方法导致资源利用不足和不稳定，无法满足高性能应用的需求。

Method: 采用深度强化学习（PPO算法）和轻量级网络系统模拟器，离线训练优化并发级别，模块化设计分离I/O和网络任务。

Result: 在测试中，AutoMDT实现了8倍更快的收敛速度和68%的传输完成时间减少。

Conclusion: AutoMDT通过模块化和强化学习显著提升了数据传输效率和适应性。

Abstract: High-performance applications necessitate rapid and dependable transfer of
massive datasets across geographically dispersed locations. Traditional file
transfer tools often suffer from resource underutilization and instability
because of fixed configurations or monolithic optimization methods. We propose
AutoMDT, a novel modular data transfer architecture that employs a deep
reinforcement learning based agent to simultaneously optimize concurrency
levels for read, network, and write operations. Our solution incorporates a
lightweight network-system simulator, enabling offline training of a Proximal
Policy Optimization (PPO) agent in approximately 45 minutes on average, thereby
overcoming the impracticality of lengthy online training in production
networks. AutoMDT's modular design decouples I/O and network tasks, allowing
the agent to capture complex buffer dynamics precisely and to adapt quickly to
changing system and network conditions. Evaluations on production-grade
testbeds show that AutoMDT achieves up to 8x faster convergence and a 68%
reduction in transfer completion times compared with state-of-the-art
solutions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [Understanding and Mitigating Errors of LLM-Generated RTL Code](https://arxiv.org/abs/2508.05266)
*Jiazheng Zhang,Cheng Liu,Huawei Li*

Main category: cs.AR

TL;DR: 论文分析了基于大语言模型（LLM）的RTL代码生成中的错误原因，并提出针对性改进方法，显著提升了生成准确率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在RTL代码生成中的成功率较低，缺乏对错误原因的深入理解，阻碍了进一步优化。

Method: 通过错误分析和分类，提出针对性改进技术：构建领域知识库、引入设计描述规则、集成外部工具处理多模态输入，并采用迭代调试循环。

Result: 改进后的框架在VerilogEval基准测试中达到91.0%的准确率，比基线方法提升32.7%。

Conclusion: 提出的方法有效解决了LLM在RTL代码生成中的主要错误来源，显著提升了性能。

Abstract: Despite the promising potential of large language model (LLM) based
register-transfer-level (RTL) code generation, the overall success rate remains
unsatisfactory. Errors arise from various factors, with limited understanding
of specific failure causes hindering improvement. To address this, we conduct a
comprehensive error analysis and manual categorization. Our findings reveal
that most errors stem not from LLM reasoning limitations, but from insufficient
RTL programming knowledge, poor understanding of circuit concepts, ambiguous
design descriptions, or misinterpretation of complex multimodal inputs.
Leveraging in-context learning, we propose targeted error correction
techniques. Specifically, we construct a domain-specific knowledge base and
employ retrieval-augmented generation (RAG) to supply necessary RTL knowledge.
To mitigate ambiguity errors, we introduce design description rules and
implement a rule-checking mechanism. For multimodal misinterpretation, we
integrate external tools to convert inputs into LLM-compatible meta-formats.
For remaining errors, we adopt an iterative debugging loop (simulation-error
localization-correction). Integrating these techniques into an LLM-based
framework significantly improves performance. We incorporate these error
correction techniques into a foundational LLM-based RTL code generation
framework, resulting in significantly improved performance. Experimental
results show that our enhanced framework achieves 91.0\% accuracy on the
VerilogEval benchmark, surpassing the baseline code generation approach by
32.7\%, demonstrating the effectiveness of our methods.

</details>


### [12] [relOBI: A Reliable Low-latency Interconnect for Tightly-Coupled On-chip Communication](https://arxiv.org/abs/2508.05354)
*Michael Rogenmoser,Angelo Garofalo,Luca Benini*

Main category: cs.AR

TL;DR: 论文提出relOBI，结合TMR和ECC保护，显著提升SoC互连的可靠性，故障注入测试显示可靠性从34.85%提升至0%，面积增加2.6倍，时序影响1.4倍。


<details>
  <summary>Details</summary>
Motivation: 现代SoC中互连的软错误可能导致整个系统失效，需在辐射环境下特别关注。

Method: 扩展OBI协议，结合TMR和ECC保护，实现完全可靠的交叉开关设计。

Result: 故障注入测试显示可靠性提升至0%，面积增加2.6倍，时序影响1.4倍。

Conclusion: relOBI在可靠性和面积效率上优于文献中的细粒度三重化方法。

Abstract: On-chip communication is a critical element of modern systems-on-chip (SoCs),
allowing processor cores to interact with memory and peripherals. Interconnects
require special care in radiation-heavy environments, as any soft error within
the SoC interconnect is likely to cause a functional failure of the whole SoC.
This work proposes relOBI, an extension to Open Bus Interface (OBI) combining
triple modular redundancy (TMR) for critical handshake signals with error
correction codes (ECC) protection on other signals for complete reliability.
Implementing and testing a fully reliable crossbar shows improved reliability
to injected faults from a vulnerability of 34.85 % to 0 % compared to a
reference design, with an area increase of 2.6x and 1.4x timing impact. The
area overhead is 1.8x lower than that reported in the literature for
fine-grained triplication and voting.

</details>
