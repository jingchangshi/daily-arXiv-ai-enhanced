<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Catalpa: GC for a Low-Variance Software Stack](https://arxiv.org/abs/2509.13429)
*Anthony Arnold,Mark Marron*

Main category: cs.PL

TL;DR: 提出了Catalpa垃圾收集器，通过利用Bosque语言的不可变性和无引用循环特性，实现有界收集暂停、固定内存开销和无屏障同步的高性能垃圾回收


<details>
  <summary>Details</summary>
Motivation: 实际应用中性能往往是二元的（要么足够快用户无感知，要么明显延迟），工业界更关注95%和99%百分位尾延迟而非平均响应时间

Method: 设计Catalpa收集器，利用Bosque语言的不可变性和无引用循环特性，构建无需屏障或同步的有界暂停收集器

Result: 实现了最小化延迟和可变性，同时保持高吞吐量和小内存开销的垃圾收集器

Conclusion: 通过编程语言和运行时系统设计可以主动支持工业级性能需求，Catalpa收集器为此提供了有效解决方案

Abstract: The performance of an application/runtime is usually conceptualized as a
continuous function where, the lower the amount of memory/time used on a given
workload, then the better the compiler/runtime is. However, in practice, good
performance of an application is viewed as more of a binary function - either
the application responds in under, say 100 ms, and is fast enough for a user to
barely notice, or it takes a noticeable amount of time, leaving the user
waiting and potentially abandoning the task. Thus, performance really means how
often the application is fast enough to be usable, leading industrial
developers to focus on the 95th and 99th percentile tail-latencies as heavily,
or moreso, than average response time. Our vision is to create a software stack
that actively supports these needs via programming language and runtime system
design. In this paper we present a novel garbage-collector design, the Catalpa
collector, for the Bosque programming language and runtime. This allocator is
designed to minimize latency and variability while maintaining high-throughput
and incurring small memory overheads. To achieve these goals we leverage
various features of the Bosque language, including immutability and
reference-cycle freedom, to construct a collector that has bounded collection
pauses, incurs fixed-constant memory overheads, and does not require any
barriers or synchronization with application code.

</details>


### [2] [Extended Abstract: Towards a Performance Comparison of Syntax and Type-Directed NbE](https://arxiv.org/abs/2509.13489)
*Chester J. F. Gould,William J. Bowman*

Main category: cs.PL

TL;DR: 这是一个进行中的工作，研究如何在依赖类型检查器中直接比较语法指导和类型指导的类型相等性检查方法的性能和表达力。


<details>
  <summary>Details</summary>
Motivation: 虽然常见说法认为语法指导方法性能更好而类型指导方法表达力更强，但由于实现通常只选择其中一种方法，导致无法进行直接的平等比较。

Method: 开发一个实际的平台，进行直接的平等比较，量化类型指导方法的性能差异，并分析其原因和改进方法。

Result: 这是一个进行中的研究，尚未提供具体的实验结果和数据。

Conclusion: 该研究目标是为了提供一个可靠的平台来评估两种类型相等性检查方法的真实性能差异和表达力优势。

Abstract: A key part of any dependent type-checker is the method for checking whether
two types are equal. A common claim is that syntax-directed equality is more
performant, although type-directed equality is more expressive. However, this
claim is difficult to make precise, since implementations choose only one or
the other approach, making a direct comparison impossible. We present some
work-in-progress developing a realistic platform for direct, apples-to-apples,
comparison of the two approaches, quantifying how much slower type-directed
equality checking is, and analyzing why and how it can be improved.

</details>


### [3] [CLMTracing: Black-box User-level Watermarking for Code Language Model Tracing](https://arxiv.org/abs/2509.13982)
*Boyu Zhang,Ping He,Tianyu Du,Xuhong Zhang,Lei Yun,Kingsum Chow,Jianwei Yin*

Main category: cs.PL

TL;DR: CLMTracing是一个黑盒代码语言模型水印框架，通过规则水印和保持实用性的注入方法实现用户级追踪，具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着开源代码语言模型的广泛采用，知识产权保护变得日益重要。现有水印技术在面对黑盒设置下的用户级追踪需求时存在局限性。

Method: 采用基于规则的水印和保持实用性的注入方法，结合对鲁棒水印敏感的参数选择算法和对抗训练来增强抗攻击能力。

Result: 在多个最先进代码语言模型上评估显示，CLMTracing相比现有基线有显著无害改进，对各种移除攻击具有强鲁棒性。

Conclusion: CLMTracing为代码语言模型提供了有效的黑盒用户级追踪水印解决方案，解决了实际应用中的知识产权保护需求。

Abstract: With the widespread adoption of open-source code language models (code LMs),
intellectual property (IP) protection has become an increasingly critical
concern. While current watermarking techniques have the potential to identify
the code LM to protect its IP, they have limitations when facing the more
practical and complex demand, i.e., offering the individual user-level tracing
in the black-box setting. This work presents CLMTracing, a black-box code LM
watermarking framework employing the rule-based watermarks and
utility-preserving injection method for user-level model tracing. CLMTracing
further incorporates a parameter selection algorithm sensitive to the robust
watermark and adversarial training to enhance the robustness against watermark
removal attacks. Comprehensive evaluations demonstrate CLMTracing is effective
across multiple state-of-the-art (SOTA) code LMs, showing significant harmless
improvements compared to existing SOTA baselines and strong robustness against
various removal attacks.

</details>


### [4] [Parallelizable Feynman-Kac Models for Universal Probabilistic Programming](https://arxiv.org/abs/2509.14092)
*Michele Boreale,Luisa Collodi*

Main category: cs.PL

TL;DR: 本文为概率程序开发了基于Sequential Monte Carlo的向量化粒子滤波算法VPF，并证明了其在无限执行轨迹上的语义一致性和有限近似性。


<details>
  <summary>Details</summary>
Motivation: 研究概率程序的正式操作语义，需要处理从任意测度采样和在无界循环中条件化/重加权的通用概率程序，为这类程序提供可证明正确且高效的推理方法。

Method: 首先为概率程序图(PPGs)建立基于无限执行轨迹的期望语义，证明有限近似定理；然后构建Feynman-Kac模型框架，确保粒子滤波算法与语义的一致性；最后提出针对PPGs的向量化粒子滤波算法VPF。

Result: 实验证明VPF相比现有概率程序推理工具表现出非常有前景的结果，验证了方法的有效性。

Conclusion: 本文为通用概率程序提供了理论保证的SMC推理框架，提出的VPF算法在实验中表现出优越性能，为概率程序的形式化验证和高效推理提供了新途径。

Abstract: We study provably correct and efficient instantiations of Sequential Monte
Carlo (SMC) inference in the context of formal operational semantics of
Probabilistic Programs (PPs). We focus on universal PPs featuring sampling from
arbitrary measures and conditioning/reweighting in unbounded loops. We first
equip Probabilistic Program Graphs (PPGs), an automata-theoretic description
format of PPs, with an expectation-based semantics over infinite execution
traces, which also incorporates trace weights. We then prove a finite
approximation theorem that provides bounds to this semantics based on
expectations taken over finite, fixed-length traces. This enables us to frame
our semantics within a Feynman-Kac (FK) model, and ensures the consistency of
the Particle Filtering (PF) algorithm, an instance of SMC, with respect to our
semantics. Building on these results, we introduce VPF, a vectorized version of
the PF algorithm tailored to PPGs and our semantics. Experiments conducted with
a proof-of-concept implementation of VPF show very promising results compared
to state-of-the-art PP inference tools.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [A User-centric Kubernetes-based Architecture for Green Cloud Computing](https://arxiv.org/abs/2509.13325)
*Matteo Zanotto,Leonardo Vicentini,Redi Vreto,Francesco Lumpp,Diego Braga,Sandro Fiore*

Main category: cs.DC

TL;DR: 提出基于Kubernetes的用户中心绿色云计算架构，通过碳强度预测和绿色能源调度，在资源受限场景下实现13%的碳排放减少


<details>
  <summary>Details</summary>
Motivation: 数据中心规模增长导致电力消耗和CO2排放增加，云提供商虽接近最优能效但缺乏精确可持续性报告，需要在用户侧进一步改进

Method: 实现碳强度预测器，利用区域和时间变化性，基于绿色能源可用性调度工作负载，使用Kubernetes架构

Result: 使用真实云工作负载跟踪评估，与轮询调度基线相比，在资源严格受限场景下可实现高达13%的排放减少

Conclusion: 用户中心的绿色云计算架构能有效减少碳排放，碳感知调度是降低云计算环境影响的可行方案

Abstract: To meet the increasing demand for cloud computing services, the scale and
number of data centers keeps increasing worldwide. This growth comes at the
cost of increased electricity consumption, which directly correlates to CO2
emissions, the main driver of climate change. As such, researching ways to
reduce cloud computing emissions is more relevant than ever. However, although
cloud providers are reportedly already working near optimal power efficiency,
they fail in providing precise sustainability reporting. This calls for further
improvements on the cloud computing consumer's side. To this end, in this paper
we propose a user-centric, Kubernetes-based architecture for green cloud
computing. We implement a carbon intensity forecaster and we use it to schedule
workloads based on the availability of green energy, exploiting both regional
and temporal variations to minimize emissions. We evaluate our system using
real-world traces of cloud workloads execution comparing the achieved carbon
emission savings against a baseline round-robin scheduler. Our findings
indicate that our system can achieve up to a 13% reduction in emissions in a
strict scenario with heavy limitations on the available resources.

</details>


### [6] [Testing and benchmarking emerging supercomputers via the MFC flow solver](https://arxiv.org/abs/2509.13575)
*Benjamin Wilfong,Anand Radhakrishnan,Henry A. Le Berre,Tanush Prathi,Stephen Abbott,Spencer H. Bryngelson*

Main category: cs.DC

TL;DR: MFC是一个计算流体动力学代码，配备自动化工具链，用于评估不同编译器-硬件组合的性能和正确性，已测试约50种计算设备和5台超级计算机。


<details>
  <summary>Details</summary>
Motivation: 部署新超级计算机需要通过应用程序代码进行测试和评估，需要便携、用户友好的工具来简化这一过程。

Method: 使用MFC代码及其自动化工具链，包括输入生成、编译、批处理作业提交、回归测试和基准测试功能，测试不同GPU和CPU架构以及多种编译器。

Result: 测试了五代NVIDIA GPU、三代AMD GPU和各种CPU架构，发现了Frontier和El Capitan等新机器上的编译器错误和回归问题。

Conclusion: MFC工具链能够有效评估编译器-硬件组合，帮助发现系统问题，适用于超级计算机的性能测试和验证。

Abstract: Deploying new supercomputers requires testing and evaluation via application
codes. Portable, user-friendly tools enable evaluation, and the Multicomponent
Flow Code (MFC), a computational fluid dynamics (CFD) code, addresses this
need. MFC is adorned with a toolchain that automates input generation,
compilation, batch job submission, regression testing, and benchmarking. The
toolchain design enables users to evaluate compiler-hardware combinations for
correctness and performance with limited software engineering experience. As
with other PDE solvers, wall time per spatially discretized grid point serves
as a figure of merit. We present MFC benchmarking results for five generations
of NVIDIA GPUs, three generations of AMD GPUs, and various CPU architectures,
utilizing Intel, Cray, NVIDIA, AMD, and GNU compilers. These tests have
revealed compiler bugs and regressions on recent machines such as Frontier and
El Capitan. MFC has benchmarked approximately 50 compute devices and 5 flagship
supercomputers.

</details>


### [7] [Modeling the Carbon Footprint of HPC: The Top 500 and EasyC](https://arxiv.org/abs/2509.13583)
*Varsha Rao,Andrew A. Chien*

Main category: cs.DC

TL;DR: 使用EasyC工具分析Top 500超算系统的碳踹迹，估算出运营碳排放1393.7万吨CO2e，体现碳排放1881.8万吨CO2e，并预测2030年前增长趋势


<details>
  <summary>Details</summary>
Motivation: HPC系统碳排放量计算方法复杂且缺乏统一报告，需要一种简单的方法来估算全球HPC系统的碳踹迹

Method: 开发EasyC工具，利用Top500.org数据和公开信息，通过插值法建模超算系统的运营碳和体现碳排放

Result: 成功模型了391个系统的运营碳排放和283个系统的体现碳排放，总估算出Top 500系统年碳排放量达3275.5万吨CO2e

Conclusion: EasyC工具能够在数据有限的情况下有效估算HPC系统碳踹迹，为HPC行业碳排放管理提供了可行方法，并预览了未来碳排放增长趋势

Abstract: Climate change is a critical concern for HPC systems, but GHG protocol
carbon-emission accounting methodologies are difficult for a single system, and
effectively infeasible for a collection of systems. As a result, there is no
HPC-wide carbon reporting, and even the largest HPC sites do not do GHG
protocol reporting.
  We assess the carbon footprint of HPC, focusing on the Top 500 systems. The
key challenge lies in modeling the carbon footprint with limited data
availability.
  With the disclosed Top500.org data, and using a new tool, EasyC, we were able
to model the operational carbon of 391 HPC systems and the embodied carbon of
283 HPC systems. We further show how this coverage can be enhanced by
exploiting additional public information. With improved coverage, then
interpolation is used to produce the first carbon footprint estimates of the
Top 500 HPC systems. They are 1,393.7 million MT CO2e operational carbon (1
Year) and 1,881.8 million MT CO2e embodied carbon. We also project how the Top
500's carbon footprint will increase through 2030.
  A key enabler is the EasyC tool which models carbon footprint with only a few
data metrics. We explore availability of data and enhancement, showing that
coverage can be increased to 98% of Top 500 systems for operational and 80.8%
of the systems for embodied emissions.

</details>


### [8] [GPU Programming for AI Workflow Development on AWS SageMaker: An Instructional Approach](https://arxiv.org/abs/2509.13703)
*Sriram Srinivasan,Hamdan Alabsi,Rand Obeidat,Nithisha Ponnala,Azene Zenebe*

Main category: cs.DC

TL;DR: 这是一门专门讲授GPU架构、GPU编程和AI应用的课程，通过AWS云平台进行实践教学，证明了实践学习对提升技术技能和思维能力的效果


<details>
  <summary>Details</summary>
Motivation: 为了准备STEM学生满足现代计算密集领域的需求，整合并行计算到教育中，提升学生的技术精通度和解决问题能力

Method: 设计了从GPU/CPU硬件基础、并行计算到RAG开发的进阶课程，使用AWS云GPU实例进行实践，通过评估、课程评价和匿名调查评估学习效果

Result: 结果显示：(1)AWS是高效经济的GPU编程平台；(2)实践学习显著提升了技术技能和参与度；(3)课程增强了学生的问题解决和思维能力

Conclusion: 并行计算整合到STEM教育中具有重要教育价值，建议更广泛采用类似选修课程以满足现代计算密集领域的需求

Abstract: We present the design, implementation, and comprehensive evaluation of a
specialized course on GPU architecture, GPU programming, and how these are used
for developing AI agents. This course is offered to undergraduate and graduate
students during Fall 2024 and Spring 2025. The course began with foundational
concepts in GPU/CPU hardware and parallel computing and progressed to develop
RAG and optimizing them using GPUs. Students gained experience provisioning and
configuring cloud-based GPU instances, implementing parallel algorithms, and
deploying scalable AI solutions. We evaluated learning outcomes through
assessments, course evaluations, and anonymous surveys. The results reveal that
(1) AWS served as an effective and economical platform for practical GPU
programming, (2) experiential learning significantly enhanced technical
proficiency and engagement, and (3) the course strengthened students'
problem-solving and critical thinking skills through tools such as TensorBoard
and HPC profilers, which exposed performance bottlenecks and scaling issues.
Our findings underscore the pedagogical value of integrating parallel computing
into STEM education. We advocate for broader adoption of similar electives
across STEM curricula to prepare students for the demands of modern,
compute-intensive fields.

</details>


### [9] [LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology](https://arxiv.org/abs/2509.13978)
*Renan Souza,Timothy Poteet,Brian Etz,Daniel Rosendo,Amal Gueroudji,Woong Shin,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 基于LLM代理的交互式流程执行数据分析系统，通过自然语言转换为结构化查询，解决大规模流程执行追溯数据分析复杂性问题


<details>
  <summary>Details</summary>
Motivation: 现代科学发现依赖于在Edge、Cloud和HPC连续体上处理数据的流程。虽然流程执行追溯技术支持深入分析，但大规模下执行追溯数据变得复杂难以分析，现有系统依赖自定义脚本、结构化查询或静态仪表板，限制了数据交互能力

Method: 提出了一种评估方法、参考架构和开源实现，利用交互式大语言模型（LLM）代理进行运行时数据分析。采用轻量级、元数据驱动的设计，将自然语言翻译为结构化执行追溯查询

Result: 在LLaMA、GPT、Gemini和Claude等多个模型上进行评估，涉及多种查询类型和真实化学流程。结果显示，模块化设计、提示调整和检索增强生成（RAG）能够实现准确且有深度的LLM代理响应，超越了记录的执行追溯范围

Conclusion: 该研究提出的方法能够有效解决大规模流程执行追溯数据分析的复杂性问题，通过LLM代理和自然语言交互提供了更加灵活和深入的数据分析能力，为科学工作流提供了新的分析方法

Abstract: Modern scientific discovery increasingly relies on workflows that process
data across the Edge, Cloud, and High Performance Computing (HPC) continuum.
Comprehensive and in-depth analyses of these data are critical for hypothesis
validation, anomaly detection, reproducibility, and impactful findings.
Although workflow provenance techniques support such analyses, at large scale,
the provenance data become complex and difficult to analyze. Existing systems
depend on custom scripts, structured queries, or static dashboards, limiting
data interaction. In this work, we introduce an evaluation methodology,
reference architecture, and open-source implementation that leverages
interactive Large Language Model (LLM) agents for runtime data analysis. Our
approach uses a lightweight, metadata-driven design that translates natural
language into structured provenance queries. Evaluations across LLaMA, GPT,
Gemini, and Claude, covering diverse query classes and a real-world chemistry
workflow, show that modular design, prompt tuning, and Retrieval-Augmented
Generation (RAG) enable accurate and insightful LLM agent responses beyond
recorded provenance.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [MACO: A Multi-Agent LLM-Based Hardware/Software Co-Design Framework for CGRAs](https://arxiv.org/abs/2509.13557)
*Zesong Jiang,Yuqi Sun,Qing Zhong,Mahathi Krishna,Deepak Patil,Cheng Tan,Sriram Krishnamoorthy,Jeff Zhang*

Main category: cs.AR

TL;DR: MACO是一个基于多智能体大语言模型的开源框架，用于CGRA的硬件/软件协同设计，通过LLM推理自动生成和优化CGRA架构，显著减少人工设计工作量。


<details>
  <summary>Details</summary>
Motivation: CGRA设计面临设计空间巨大、架构参数独立、人工设计耗时等挑战，而大型语言模型的快速发展为自动化这一过程提供了新的机遇。

Method: 采用多智能体LLM框架，通过四个阶段进行HW/SW协同设计：硬件/软件协同设计、设计错误修正、最佳设计选择、评估与反馈，并引入LLM自学习机制选择最优CGRA。

Result: 实验结果表明，MACO能够高效生成高质量的CGRA架构，在性能、功耗和面积方面优于最先进的基于LLM的方法和人工CGRA设计。

Conclusion: 该框架展示了在实际CGRA设计中应用的潜力，通过迭代优化和智能体推理实现了更高的PPA设计点。

Abstract: Coarse-grained Reconfigurable Arrays (CGRAs) are a promising computing
architecture that can deliver high-performance, energy-efficient acceleration
across diverse domains. By supporting reconfiguration at the functional unit
level, CGRAs efficiently adapt to varying computational patterns and optimize
resource utilization. However, designing CGRAs is highly challenging due to the
vast design space, independent architectural parameters, and the time-consuming
nature of manual design. Fortunately, the rapid advancement of large language
models (LLMs) presents new opportunities to automate this process.
  In this work, we propose MACO -- an open-source multi-agent LLM-based
framework for Hardware/Software (HW/SW) co-design of CGRAs. The framework
employs LLM reasoning to generate CGRAs across four stages: HW/SW co-design,
Design error correction, Best design selection, and Evaluation & Feedback.
Furthermore, MACO iteratively optimizes the generated CGRAs, leveraging agent
reasoning and feedback to achieve higher PPA (that is, power, performance, and
area) design points for a given domain. In addition, we introduce an LLM
self-learning mechanism that employs LLM-driven decision making to select the
optimal CGRA to accelerate the design process.
  We evaluate the framework with state-of-the-art LLM-based methods and manual
CGRA design, in terms of performance, power consumption, and area. Experimental
results show that MACO efficiently generates high-quality CGRA architectures,
significantly reducing manual design effort and demonstrating the potential of
our framework for real-world CGRA design.

</details>


### [11] [StreamTensor: Make Tensors Stream in Dataflow Accelerators for LLMs](https://arxiv.org/abs/2509.13694)
*Hanchen Ye,Deming Chen*

Main category: cs.AR

TL;DR: StreamTensor是一个编译器框架，通过新颖的迭代张量类型系统和分层设计空间探索，自动构建和优化基于流的数据流加速器，在FPGA上实现比现有FPGA加速器和GPU更低的延迟和更高的能效


<details>
  <summary>Details</summary>
Motivation: 解决深度学习工作负载在数据流架构上执行时的内存瓶颈问题，现有方法在处理内核间相关性、外部内存访问管理和缓冲区优化方面存在困难

Method: 提出StreamTensor编译器框架，引入迭代张量类型系统显式编码流布局，系统探索张量平铺、内核融合和资源分配三个分层设计空间

Result: 在FPGA上的大型语言模型评估中，相比最先进的FPGA LLM加速器延迟降低0.76倍，相比GPU延迟降低0.64倍，能效比GPU高1.99倍

Conclusion: StreamTensor为可扩展的基于数据流的深度学习加速提供了一种有前景的方法

Abstract: Efficient execution of deep learning workloads on dataflow architectures is
crucial for overcoming memory bottlenecks and maximizing performance. While
streaming intermediate results between computation kernels can significantly
improve efficiency, existing approaches struggle with inter-kernel
correlations, external memory access management, and buffer optimization. In
this work, we propose StreamTensor, a compiler framework that automatically
constructs and optimizes stream-based dataflow accelerators. StreamTensor
introduces a novel iterative tensor type system to explicitly encode stream
layouts, enabling seamless kernel fusion, buffer allocation, and memory
optimization. By systematically exploring three hierarchical design spaces,
including tensor tiling, kernel fusion, and resource allocation, StreamTensor
balances computational intensity, memory efficiency, and data streaming to
maximize performance. Based on FPGA evaluations on Large Language Models (LLM),
StreamTensor achieves up to 0.76x and 0.64x lower latency compared to the
state-of-the-art FPGA LLM accelerators and GPUs, and up to 1.99x higher energy
efficiency compared to GPUs, making it a promising approach for scalable
dataflow-based deep learning acceleration.

</details>


### [12] [CompAir: Synergizing Complementary PIMs and In-Transit NoC Computation for Efficient LLM Acceleration](https://arxiv.org/abs/2509.13710)
*Hongyi Li,Songchen Ma,Huanyu Qu,Weihao Zhang,Jia Chen,Junfeng Lin,Fengbin Tu,Rong Zhao*

Main category: cs.AR

TL;DR: CompAir是一种新型混合PIM架构，通过DRAM-PIM和SRAM-PIM的异构集成以及创新的片上网络设计，显著提升大语言模型的推理效率和能效


<details>
  <summary>Details</summary>
Motivation: 大语言模型的计算和能耗需求巨大，现有PIM架构难以平衡灵活性、性能和成本效率，无法有效处理LLM的动态内存计算模式和算子多样性

Method: 提出CompAir混合PIM架构：1）通过混合键合集成DRAM-PIM和SRAM-PIM；2）开发CompAir-NoC片上网络，在数据传输过程中执行非线性运算；3）设计分层指令集架构确保灵活性和可编程性

Result: 相比最先进的全PIM架构，预填充性能提升1.83-7.98倍，解码性能提升1.95-6.28倍；相比混合A100和HBM-PIM系统，能耗降低3.52倍且吞吐量相当

Conclusion: 这是首个系统探索混合DRAM-PIM和SRAM-PIM架构并具备网络内计算能力的工作，为LLM提供了高效解决方案

Abstract: The rapid advancement of Large Language Models (LLMs) has revolutionized
various aspects of human life, yet their immense computational and energy
demands pose significant challenges for efficient inference. The memory wall,
the growing processor-memory speed disparity, remains a critical bottleneck for
LLM. Process-In-Memory (PIM) architectures overcome limitations by co-locating
compute units with memory, leveraging 5-20$\times$ higher internal bandwidth
and enabling greater energy efficiency than GPUs. However, existing PIMs
struggle to balance flexibility, performance, and cost-efficiency for LLMs'
dynamic memory-compute patterns and operator diversity. DRAM-PIM suffers from
inter-bank communication overhead despite its vector parallelism. SRAM-PIM
offers sub-10ns latency for matrix operation but is constrained by limited
capacity. This work introduces CompAir, a novel PIM architecture that
integrates DRAM-PIM and SRAM-PIM with hybrid bonding, enabling efficient linear
computations while unlocking multi-granularity data pathways. We further
develop CompAir-NoC, an advanced network-on-chip with an embedded arithmetic
logic unit that performs non-linear operations during data movement,
simultaneously reducing communication overhead and area cost. Finally, we
develop a hierarchical Instruction Set Architecture that ensures both
flexibility and programmability of the hybrid PIM. Experimental results
demonstrate that CompAir achieves 1.83-7.98$\times$ prefill and
1.95-6.28$\times$ decode improvement over the current state-of-the-art fully
PIM architecture. Compared to the hybrid A100 and HBM-PIM system, CompAir
achieves 3.52$\times$ energy consumption reduction with comparable throughput.
This work represents the first systematic exploration of hybrid DRAM-PIM and
SRAM-PIM architectures with in-network computation capabilities, offering a
high-efficiency solution for LLM.

</details>


### [13] [TENET: An Efficient Sparsity-Aware LUT-Centric Architecture for Ternary LLM Inference On Edge](https://arxiv.org/abs/2509.13765)
*Zhirui Huang,Rui Ma,Shijie Cao,Ran Shu,Ian Wang,Ting Cao,Chixiao Chen,Yongqiang Xiong*

Main category: cs.AR

TL;DR: TENET是一个针对三元量化LLM推理的稀疏感知LUT中心架构，通过算法、计算和内存协同优化，在FPGA和ASIC平台上分别实现4.3倍和21.1倍的能效提升。


<details>
  <summary>Details</summary>
Motivation: 传统GPU平台无法充分利用三元量化的优势，缺乏对三元算术和内存专业化的原生支持，在低批次实时推理场景下利用率严重不足。

Method: 提出Sparse Ternary LUT (STL)核心优化三元混合精度GEMM，采用动态激活N:M稀疏性，设计基于LUT的64B:80B权重解压缩模块，构建异构可编程加速器。

Result: TENET-FPGA和TENET-ASIC相比A100 GPU分别实现4.3倍和21.1倍的能效提升，TENET-ASIC在端到端推理延迟上实现2.7倍平均加速。

Conclusion: TENET架构通过算法-硬件协同设计，有效解决了三元量化LLM推理在传统硬件上的效率瓶颈，为实时推理部署提供了高效解决方案。

Abstract: Ternary quantization has emerged as a powerful technique for reducing both
computational and memory footprint of large language models (LLM), enabling
efficient real-time inference deployment without significantly compromising
model accuracy. Conventional LLM inference platforms (e.g GPUs) cannot
capitalize on its benefits, as they (i) lack native support for ternary
arithmetic and memory specialization and (ii) remain severely under-utilized in
low-batch, real-time scenarios. In this work, we propose TENET, a sparse-aware
LUT-centric architecture that co-optimizes algorithm, compute, and memory for
ternary LLM inference. To maximize the efficiency of Ternary Linear layer,
TENET introduces a Sparse Ternary LUT (STL) core that optimizes ternary
mixed-precision GEMM using a symmetric precompute lookup table. It also
features Dynamic Activation N:M Sparsity to exploit the sparsity within the
activation of each token. Additionally, we propose a LUT-based 64B:80B ternary
weight decompression module to fully exploit the memory efficiency of ternary
values. At the system level, we design a heterogeneous TENET accelerator with
full programmability that integrates STL cores with high-precision cores. An
associated Linear-Projection-aware Sparse Attention dataflow is introduced to
optimize memory access and hardware utilization. We implement TENET accelerator
prototype on both FPGA and ASIC platforms. Experiments across various model
sizes and workloads demonstrate that TENET-FPGA and TENET-ASIC improve energy
efficiency by 4.3$\times$ and 21.1$\times$, respectively, compared to the A100
GPU. Furthermore, TENET-ASIC achieves a 2.7$\times$ average speedup compared to
the A100 GPU in end-to-end inference latency.

</details>


### [14] [An RDMA-First Object Storage System with SmartNIC Offload](https://arxiv.org/abs/2509.13997)
*Yu Zhu,Aditya Dhakal,Pedro Bruel,Gourav Rattihalli,Yunming Xiao,Johann Lombardi,Dejan Milojicic*

Main category: cs.AR

TL;DR: ROS2是一个基于RDMA的智能网卡卸载对象存储系统，通过分离控制平面和数据平面，在GPU中心化AI训练管道中提供高性能存储访问，RDMA性能优于TCP且智能网卡卸载保持效率


<details>
  <summary>Details</summary>
Motivation: AI训练和推理产生持续、细粒度的I/O负载，给基于TCP的主机中介存储路径带来压力，需要重新审视POSIX兼容的对象存储方案

Method: 设计ROS2系统，将DAOS客户端卸载到NVIDIA BlueField-3智能网卡，保持存储服务器上的DAOS I/O引擎不变，分离轻量级控制平面(gRPC)和高吞吐数据平面(UCX/libfabric over RDMA/TCP)

Result: RDMA在服务器级CPU上始终优于TCP，智能网卡卸载的RDMA驱动DAOS客户端端到端性能与主机相当，而TCP在智能网卡上性能落后

Conclusion: RDMA优先、智能网卡卸载的对象存储堆栈是现代LLM训练环境中扩展数据交付的实用基础

Abstract: AI training and inference impose sustained, fine-grain I/O that stresses
host-mediated, TCP-based storage paths. Motivated by kernel-bypass networking
and user-space storage stacks, we revisit POSIX-compatible object storage for
GPU-centric pipelines. We present ROS2, an RDMA-first object storage system
design that offloads the DAOS client to an NVIDIA BlueField-3 SmartNIC while
leaving the DAOS I/O engine unchanged on the storage server. ROS2 separates a
lightweight control plane (gRPC for namespace and capability exchange) from a
high-throughput data plane (UCX/libfabric over RDMA or TCP) and removes host
mediation from the data path.
  Using FIO/DFS across local and remote configurations, we find that on
server-grade CPUs RDMA consistently outperforms TCP for both large sequential
and small random I/O. When the RDMA-driven DAOS client is offloaded to
BlueField-3, end-to-end performance is comparable to the host, demonstrating
that SmartNIC offload preserves RDMA efficiency while enabling DPU-resident
features such as multi-tenant isolation and inline services (e.g.,
encryption/decryption) close to the NIC. In contrast, TCP on the SmartNIC lags
host performance, underscoring the importance of RDMA for offloaded
deployments.
  Overall, our results indicate that an RDMA-first, SmartNIC-offloaded
object-storage stack is a practical foundation for scaling data delivery in
modern LLM training environments; integrating optional GPU-direct placement for
LLM tasks is left for future work.

</details>


### [15] [A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval Prediction For Instruction Caching](https://arxiv.org/abs/2509.14041)
*Henry Kao,Nikhil Sreekumar,Prabhdeep Singh Soni,Ali Sedaghati,Fang Su,Bryan Chan,Maziar Goudarzi,Reza Azimi*

Main category: cs.AR

TL;DR: TRRIP是一种软硬件协同设计方法，通过编译器分析代码温度（热/冷）并利用操作系统接口向硬件提供温度信息，优化指令缓存替换策略，减少热代码的驱逐率，在移动CPU上实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 移动CPU软件由于复杂的运行时行为和指令重用距离大，导致传统指令缓存替换策略效率低下，前端停顿严重，且代码复杂度增长快于片上内存容量增长，需要新的解决方案。

Method: 采用软硬件协同设计：编译器分析代码温度并分类转换，通过OS接口提供温度信息；硬件轻量级扩展利用温度属性优化缓存替换策略。

Result: L2指令MPKI降低26.5%，在已使用PGO优化的移动代码上实现3.9%的几何平均加速比。

Conclusion: TRRIP是一种实用且可采纳的解决方案，能有效优化移动系统的指令缓存管理，提升性能。

Abstract: Modern mobile CPU software pose challenges for conventional instruction cache
replacement policies due to their complex runtime behavior causing high reuse
distance between executions of the same instruction. Mobile code commonly
suffers from large amounts of stalls in the CPU frontend and thus starvation of
the rest of the CPU resources. Complexity of these applications and their code
footprint are projected to grow at a rate faster than available on-chip memory
due to power and area constraints, making conventional hardware-centric methods
for managing instruction caches to be inadequate. We present a novel
software-hardware co-design approach called TRRIP (Temperature-based
Re-Reference Interval Prediction) that enables the compiler to analyze,
classify, and transform code based on "temperature" (hot/cold), and to provide
the hardware with a summary of code temperature information through a
well-defined OS interface based on using code page attributes. TRRIP's
lightweight hardware extension employs code temperature attributes to optimize
the instruction cache replacement policy resulting in the eviction rate
reduction of hot code. TRRIP is designed to be practical and adoptable in real
mobile systems that have strict feature requirements on both the software and
hardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%
resulting in geomean speedup of 3.9%, on top of RRIP cache replacement running
mobile code already optimized using PGO.

</details>
