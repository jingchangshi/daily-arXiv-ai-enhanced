{"id": "2507.14403", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.14403", "abs": "https://arxiv.org/abs/2507.14403", "authors": ["Sarunas Kalade", "Graham Schelle"], "title": "NPUEval: Optimizing NPU Kernels with LLMs and Open Source Compilers", "comment": null, "summary": "Neural processing units (NPUs) are gaining prominence in power-sensitive\ndevices like client devices, with AI PCs being defined by their inclusion of\nthese specialized processors. Running AI workloads efficiently on these devices\nrequires libraries of optimized kernels. Creating efficient kernels demands\nexpertise in domain-specific C++ with vector intrinsics and in-depth knowledge\nof the target architecture. Unlike GPU programming, which has had years to\nmature, NPU programming is new, with smaller and more fragmented developer\ncommunities across hardware platforms. This fragmentation poses a challenge\nwhen utilizing LLMs to assist in writing NPU kernels, as domain-specific\noptimized code examples are underrepresented in LLM pre-training data.\n  In this paper we introduce NPUEval -- a benchmark for writing and evaluating\nNPU kernels, consisting of 102 common operators for machine learning workloads.\nWe evaluate LLM generated code on actual hardware based on both functional\ncorrectness and vectorization efficiency using open source compiler tools\ntargeting the AMD NPU. We evaluate a range of state-of-the-art LLMs with a mix\nof proprietary and open-weight models. Latest reasoning models like DeepSeek\nR1, show promising results achieving out-of-the-box 50%+ vectorization on\nselect kernels. However, the average score across the entire dataset remains\nroughly 10% even with compiler feedback and vectorized kernel examples --\nshowing that this is a challenging dataset even for frontier models. The\ndataset and evaluation code will be released with a permissive open source\nlicense, providing an essential benchmark for advancing research in code\ngeneration and NPU kernel optimization."}
{"id": "2507.14471", "categories": ["cs.PL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.14471", "abs": "https://arxiv.org/abs/2507.14471", "authors": ["Logan Kenwright", "Partha Roop", "Nathan Allen", "Călin Caşcaval", "Avinash Malik"], "title": "Timetide: A programming model for logically synchronous distributed systems", "comment": "25 Pages, 21 Figures", "summary": "Massive strides in deterministic models have been made using synchronous\nlanguages. They are mainly focused on centralised applications, as the\ntraditional approach is to compile away the concurrency. Time triggered\nlanguages such as Giotto and Lingua Franca are suitable for distribution albeit\nthat they rely on expensive physical clock synchronisation, which is both\nexpensive and may suffer from scalability. Hence, deterministic programming of\ndistributed systems remains challenging. We address the challenges of\ndeterministic distribution by developing a novel multiclock semantics of\nsynchronous programs. The developed semantics is amenable to seamless\ndistribution. Moreover, our programming model, Timetide, alleviates the need\nfor physical clock synchronisation by building on the recently proposed logical\nsynchrony model for distributed systems. We discuss the important aspects of\ndistributing computation, such as network communication delays, and explore the\nformal verification of Timetide programs. To the best of our knowledge,\nTimetide is the first multiclock synchronous language that is both amenable to\ndistribution and formal verification without the need for physical clock\nsynchronisation or clock gating."}
{"id": "2507.15007", "categories": ["cs.PL", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15007", "abs": "https://arxiv.org/abs/2507.15007", "authors": ["Sayed Mahbub Hasan Amiri", "Md. Mainul Islam", "Mohammad Shakhawat Hossen", "Sayed Majhab Hasan Amiri", "Mohammad Shawkat Ali Mamun", "Sk. Humaun Kabir", "Naznin Akter"], "title": "Hear Your Code Fail, Voice-Assisted Debugging for Python", "comment": "35 pages, 20 figures", "summary": "This research introduces an innovative voice-assisted debugging plugin for\nPython that transforms silent runtime errors into actionable audible\ndiagnostics. By implementing a global exception hook architecture with pyttsx3\ntext-to-speech conversion and Tkinter-based GUI visualization, the solution\ndelivers multimodal error feedback through parallel auditory and visual\nchannels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,\nn=50) compared to traditional stack-trace debugging, while enabling 78% faster\nerror identification through vocalized exception classification and\ncontextualization. The system achieves sub-1.2 second voice latency with under\n18% CPU overhead during exception handling, vocalizing error types and\nconsequences while displaying interactive tracebacks with documentation deep\nlinks. Criteria validate compatibility across Python 3.7+ environments on\nWindows, macOS, and Linux platforms. Needing only two lines of integration\ncode, the plugin significantly boosts availability for aesthetically impaired\ndesigners and supports multitasking workflows through hands-free error medical\ndiagnosis. Educational applications show particular promise, with pilot studies\nindicating 45% faster debugging skill acquisition among novice programmers.\nFuture development will incorporate GPT-based repair suggestions and real-time\nmultilingual translation to further advance auditory debugging paradigms. The\nsolution represents a fundamental shift toward human-centric error diagnostics,\nbridging critical gaps in programming accessibility while establishing new\nstandards for cognitive efficiency in software development workflows."}
{"id": "2507.15017", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.15017", "abs": "https://arxiv.org/abs/2507.15017", "authors": ["Xuran Cai", "Liqian Chen", "Hongfei Fu"], "title": "Invariant Generation for Floating-Point Programs via Constraint Solving", "comment": null, "summary": "In numeric-intensive computations, it is well known that the execution of\nfloating-point programs is imprecise as floating point arithmetics (e.g.,\naddition, subtraction, multiplication, division, etc.) incurs rounding errors.\nAlbeit the rounding error is small for every single floating-point operation,\nthe aggregation of such error in multiple operations may be dramatic and cause\ncatastrophic program failures. Therefore, to ensure the correctness of\nfloating-point programs, the effect of floating point error needs to be\ncarefully taken into account. In this work, we consider the invariant\ngeneration for floating point programs, whose aim is to generate tight\ninvariants under the perturbation of floating point errors. Our main\ncontribution is a theoretical framework on how to apply constraint solving\nmethods to address the invariant generation problem. In our framework, we\npropose a novel combination between the first-order differential\ncharacterization by FPTaylor (TOPLAS 2018) and constraint solving methods,\naiming to reduce the computational burden of constraint solving. Moreover, we\ndevise two polynomial invariant generation algorithms to instantiate the\nframework. The first algorithm is applicable to a wide range of floating-point\noperations but requires an initial (coarse) invariant as external input, while\nthe second does not require an initial invariant but is limited to polynomial\nprograms. Furthermore, we show how conditional branches, a difficult issue in\nfloating-point analysis, can be handled in our framework. Experimental results\nshow that our algorithms outperform SOTA approaches in both the time efficiency\nand the precision of the generated invariants over a variety of benchmarks."}
{"id": "2507.14392", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.14392", "abs": "https://arxiv.org/abs/2507.14392", "authors": ["Lang Xu", "Kaushik Kandadi Suresh", "Quentin Anthony", "Nawras Alnaasan", "Dhabaleswar K. Panda"], "title": "Characterizing Communication Patterns in Distributed Large Language Model Inference", "comment": "To be presented at Hot Interconnects 2025", "summary": "Large Language Models (LLMs) built on transformer architectures have\ntransformed natural language processing, achieving remarkable performance\nacross diverse applications. While distributed inference frameworks enable\npractical deployment of these models, inter-GPU communication creates\nsignificant performance constraints that limit service quality in real-world\nsystems. This paper investigates communication dynamics in distributed LLM\nserving-analyzing how various parallelization approaches coordinate data\nexchange between GPU workers during inference. We study dense transformer-based\nmodels as representative examples of contemporary architectures widely used in\noperational deployments. Our work combines detailed profiling measurements with\npredictive analytical models to characterize communication behavior across\ndifferent parallelization configurations. Results show that tensor parallelism\nincurs substantial network overhead but delivers superior response times for\nbrief sequences, pipeline parallelism minimizes data transfer requirements\nwhile increasing total latency, and combined approaches demand careful tuning\nto achieve balanced performance. These insights offer practical recommendations\nfor selecting appropriate parallelization schemes in production LLM services\nand identify key opportunities for optimizing inference frameworks and\ncommunication infrastructure."}
{"id": "2507.14139", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.14139", "abs": "https://arxiv.org/abs/2507.14139", "authors": ["Peipei Wang", "Wu Guan", "Liping Liang", "Zhijun Wang", "Hanqing Luo", "Zhibin Zhang"], "title": "SpeedLLM: An FPGA Co-design of Large Language Model Inference Accelerator", "comment": null, "summary": "This paper introduces SpeedLLM, a neural network accelerator designed on the\nXilinx Alevo U280 platform and optimized for the Tinyllama framework to enhance\nedge computing performance. Key innovations include data stream parallelism, a\nmemory reuse strategy, and Llama2 operator fusion, which collectively reduce\nlatency and energy consumption. SpeedLLM's data pipeline architecture optimizes\nthe read-compute-write cycle, while the memory strategy minimizes FPGA resource\ndemands. The operator fusion boosts computational density and throughput.\nResults show SpeedLLM outperforms traditional Tinyllama implementations,\nachieving up to 4.8* faster performance and 1.18* lower energy consumption,\noffering improvements in edge devices."}
{"id": "2507.15277", "categories": ["cs.PL", "D.3.4"], "pdf": "https://arxiv.org/pdf/2507.15277", "abs": "https://arxiv.org/abs/2507.15277", "authors": ["Robert Hochgraf", "Sreepathi Pai"], "title": "A Few Fit Most: Improving Performance Portability of SGEMM on GPUs using Multi-Versioning", "comment": "13 pages, 8 figures", "summary": "Hand-optimizing linear algebra kernels for different GPU devices and\napplications is complex and labor-intensive. Instead, many developers use\nautomatic performance tuning (autotuning) to achieve high performance on a\nvariety of devices. However, autotuning \"overfits\", and must be redone if any\npart of the environment changes, such as if the device or input characteristics\nchange.\n  In most non-trivial cases, a single compute kernel cannot maintain\nnear-optimal performance across all environments. Changing the kernel to\nspecialize it to the current execution environment is possible, but on GPUs,\nruntime tuning and compilation can be expensive.\n  In this work, we use multi-versioning -- producing several variants of the\nsame code -- as a way to generate performance portable code. We describe a\nframework called portability tuning that can automatically generate\nmulti-versioned code whose performance is portable, requiring no retuning.\n  We evaluate our framework on a dataset of execution times for GEMM kernels\nfrom the CLBlast linear algebra library. We find our portability tuning\ntechniques outperform CLBlast's default kernels -- often approaching within 10%\nof the theoretical maximum performance -- despite CLBlast using autotuning\ntechniques. Further, we find that our generated programs generalize well to new\nand unseen devices, matching the performance of autotuning without ever\nportability tuning for those devices."}
{"id": "2507.14597", "categories": ["cs.DC", "cs.CV", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.14597", "abs": "https://arxiv.org/abs/2507.14597", "authors": ["Eugene Armah", "Linda Amoako Bannning"], "title": "Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning", "comment": null, "summary": "Processing data at high speeds is becoming increasingly critical as digital\neconomies generate enormous data. The current paradigms for timely data\nprocessing are edge computing and data stream processing (DSP). Edge computing\nplaces resources closer to where data is generated, while stream processing\nanalyzes the unbounded high-speed data in motion. However, edge stream\nprocessing faces rapid workload fluctuations, complicating resource\nprovisioning. Inadequate resource allocation leads to bottlenecks, whereas\nexcess allocation results in wastage. Existing reactive methods, such as\nthreshold-based policies and queuing theory scale only after performance\ndegrades, potentially violating SLAs. Although reinforcement learning (RL)\noffers a proactive approach through agents that learn optimal runtime\nadaptation policies, it requires extensive simulation. Furthermore, predictive\nmachine learning models face online distribution and concept drift that\nminimize their accuracy. We propose a three-step solution to the proactive edge\nstream processing autoscaling problem. Firstly, a GRU neural network forecasts\nthe upstream load using real-world and synthetic DSP datasets. Secondly, a\ntransfer learning framework integrates the predictive model into an online\nstream processing system using the DTW algorithm and joint distribution\nadaptation to handle the disparities between offline and online domains.\nFinally, a horizontal autoscaling module dynamically adjusts the degree of\noperator parallelism, based on predicted load while considering edge resource\nconstraints. The lightweight GRU model for load predictions recorded up to\n1.3\\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and\nProphet on the SMAPE and RMSE evaluation metrics, with lower training time than\nthe computationally intensive RL models."}
{"id": "2507.14397", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.14397", "abs": "https://arxiv.org/abs/2507.14397", "authors": ["Michael Davies", "Neal Crago", "Karthikeyan Sankaralingam", "Christos Kozyrakis"], "title": "Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need", "comment": null, "summary": "This paper presents a limit study of transformer-based large language model\n(LLM) inference, focusing on the fundamental performance bottlenecks imposed by\nmemory bandwidth, memory capacity, and synchronization overhead in distributed\ninference systems. We develop a hardware-agnostic performance model that\nabstracts away implementation details, enabling the analysis of a wide range of\ncurrent and near-future hardware technologies. Our analysis spans from current\nHBM3 memory technology used in AI accelerators like GPUs and TPUs to systems\nbased on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers\nSRAM-based designs and scaling techniques from distributed clusters with\nvarying numbers of chips to wafer-scale integration. Our key findings for\nauto-regressive decoding are: i) serving LLMs requires 100s of GB per server to\nserve a model instance; ii) high memory bandwidth is critical for high per-user\nthroughput; iii) exposed synchronization latencies to achieve collective\ncommunication must be around 1us else they make the memory bandwidth\nineffective; iv) DRAM-based designs have a fundamental advantage in terms of\nsystem-level efficiency as measured in throughput per cost or watt; and v)\nhardware designs can easily reach 2000+ user token/sec but getting to 10,000+\ntokens/sec will need smaller models, smaller context, or other forms of\nalgorithmic advances. This study provides valuable insights into the\nfundamental performance limits of LLM inference, highlighting the potential\nbenefits of future hardware advancements and guiding the optimization of LLM\ndeployment strategies."}
{"id": "2507.15530", "categories": ["cs.PL", "cs.LO", "F.3.1; F.3.2"], "pdf": "https://arxiv.org/pdf/2507.15530", "abs": "https://arxiv.org/abs/2507.15530", "authors": ["Shing Hin Ho", "Nicolas Wu", "Azalea Raad"], "title": "Bayesian Separation Logic", "comment": null, "summary": "Bayesian probabilistic programming languages (BPPLs) let users denote\nstatistical models as code while the interpreter infers the posterior\ndistribution. The semantics of BPPLs are usually mathematically complex and\nunable to reason about desirable properties such as expected values and\nindependence of random variables. To reason about these properties in a\nnon-Bayesian setting, probabilistic separation logics such as PSL and Lilac\ninterpret separating conjunction as probabilistic independence of random\nvariables. However, no existing separation logic can handle Bayesian updating,\nwhich is the key distinguishing feature of BPPLs.\n  To close this gap, we introduce Bayesian separation logic (BaSL), a\nprobabilistic separation logic that gives semantics to BPPL. We prove an\ninternal version of Bayes' theorem using a result in measure theory known as\nthe Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model\nprobabilistic programming concepts such as Bayesian updating, unnormalised\ndistribution, conditional distribution, soft constraint, conjugate prior and\nimproper prior while maintaining modularity via the frame rule. The model of\nBaSL is based on a novel instantiation of Kripke resource monoid via\n$\\sigma$-finite measure spaces over the Hilbert cube, and the semantics of\nHoare triple is compatible with an existing denotational semantics of BPPL\nbased on the category of $s$-finite kernels. Using BaSL, we then prove\nproperties of statistical models such as the expected value of Bayesian coin\nflip, correlation of random variables in the collider Bayesian network, and the\nposterior distributions of the burglar alarm model, a parameter estimation\nalgorithm, and the Gaussian mixture model."}
{"id": "2507.14723", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.14723", "abs": "https://arxiv.org/abs/2507.14723", "authors": ["Brati Mondal", "Pritam Goswami", "Buddhadeb Sau"], "title": "Simulating Chirality: Solving Distance-$k$-Dispersion on an 1-Interval Connected Ring", "comment": null, "summary": "We study the Distance-$k$-Dispersion (D-$k$-D) problem for synchronous mobile\nagents in a 1-interval-connected ring network having $n$ nodes and with $l$\nagents where $3 \\le l \\le \\lfloor \\frac{n}{k}\\rfloor$, without the assumption\nof chirality (a common sense of direction for the agents). This generalizes the\nclassical dispersion problem by requiring that agents maintain a minimum\ndistance of $k$ hops from each other, with the special case $k=1$ corresponding\nto the standard dispersion.\n  The contribution in this work is threefold. Our first contribution is a novel\nmethod that enables agents to simulate chirality using only local information,\nvision and bounded memory. This technique demonstrates that chirality is not a\nfundamental requirement for coordination in this model.\n  Building on this, our second contribution partially resolves an open question\nposed by Agarwalla et al. (ICDCN, 2018), who considered the same model (1-\ninterval connected ring, synchronous agents, no chirality). We prove that\nD-$k$-D, and thus dispersion is solvable from any arbitrary configuration under\nthese assumptions (excluding vertex permutation dynamism)for any size of the\nring network which was earlier limited to only odd sized ring or to a ring of\nsize four.\n  Finally, we present an algorithm for D-$k$-D in this setting that works in\n$O(ln)$ rounds, completing the constructive side of our result.\n  Altogether, our findings significantly extend the theoretical understanding\nof mobile agent coordination in dynamic networks and clarify the role of\nchirality in distributed computation."}
{"id": "2507.14651", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.14651", "abs": "https://arxiv.org/abs/2507.14651", "authors": ["Joren Dumoulin", "Pouya Houshmand", "Vikram Jain", "Marian Verhelst"], "title": "Enabling Efficient Hardware Acceleration of Hybrid Vision Transformer (ViT) Networks at the Edge", "comment": null, "summary": "Hybrid vision transformers combine the elements of conventional neural\nnetworks (NN) and vision transformers (ViT) to enable lightweight and accurate\ndetection. However, several challenges remain for their efficient deployment on\nresource-constrained edge devices. The hybrid models suffer from a widely\ndiverse set of NN layer types and large intermediate data tensors, hampering\nefficient hardware acceleration. To enable their execution at the edge, this\npaper proposes innovations across the hardware-scheduling stack: a.) At the\nlowest level, a configurable PE array supports all hybrid ViT layer types; b.)\ntemporal loop re-ordering within one layer, enabling hardware support for\nnormalization and softmax layers, minimizing on-chip data transfers; c.)\nfurther scheduling optimization employs layer fusion across inverted bottleneck\nlayers to drastically reduce off-chip memory transfers. The resulting\naccelerator is implemented in 28nm CMOS, achieving a peak energy efficiency of\n1.39 TOPS/W at 25.6 GMACs/s."}
{"id": "2507.15596", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.15596", "abs": "https://arxiv.org/abs/2507.15596", "authors": ["Jaeseo Lee", "Kyungmin Bae"], "title": "Formal Analysis of Networked PLC Controllers Interacting with Physical Environments", "comment": "To appear in Proceedings of the Static Analysis Symposium (SAS) 2025", "summary": "Programmable Logic Controllers (PLCs) are widely used in industrial\nautomation to control physical systems. As PLC applications become increasingly\ncomplex, ensuring their correctness is crucial. Existing formal verification\ntechniques focus on individual PLC programs in isolation, often neglecting\ninteractions with physical environments and network communication between\ncontrollers. This limitation poses significant challenges in analyzing\nreal-world industrial systems, where continuous dynamics and communication\ndelays play a critical role. In this paper, we present a unified formal\nframework that integrates discrete PLC semantics, networked communication, and\ncontinuous physical behaviors. To mitigate state explosion, we apply partial\norder reduction, significantly reducing the number of explored states while\nmaintaining correctness. Our framework enables precise analysis of PLC-driven\nsystems with continuous dynamics and networked communication."}
{"id": "2507.14802", "categories": ["cs.DC", "cs.AI", "C.2.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.14802", "abs": "https://arxiv.org/abs/2507.14802", "authors": ["Ziming Dai", "Chao Qiu", "Fei Gao", "Yunfeng Zhao", "Xiaofei Wang"], "title": "ACME: Adaptive Customization of Large Models via Distributed Systems", "comment": "Accepted to IEEE ICDCS 2025. 11 pages, 13 figures", "summary": "Pre-trained Transformer-based large models have revolutionized personal\nvirtual assistants, but their deployment in cloud environments faces challenges\nrelated to data privacy and response latency. Deploying large models closer to\nthe data and users has become a key research area to address these issues.\nHowever, applying these models directly often entails significant difficulties,\nsuch as model mismatching, resource constraints, and energy inefficiency.\nAutomated design of customized models is necessary, but it faces three key\nchallenges, namely, the high cost of centralized model customization,\nimbalanced performance from user heterogeneity, and suboptimal performance from\ndata heterogeneity. In this paper, we propose ACME, an adaptive customization\napproach of Transformer-based large models via distributed systems. To avoid\nthe low cost-efficiency of centralized methods, ACME employs a bidirectional\nsingle-loop distributed system to progressively achieve fine-grained\ncollaborative model customization. In order to better match user heterogeneity,\nit begins by customizing the backbone generation and identifying the Pareto\nFront under model size constraints to ensure optimal resource utilization.\nSubsequently, it performs header generation and refines the model using data\ndistribution-based personalized architecture aggregation to match data\nheterogeneity. Evaluation on different datasets shows that ACME achieves\ncost-efficient models under model size constraints. Compared to centralized\nsystems, data transmission volume is reduced to 6 percent. Additionally, the\naverage accuracy improves by 10 percent compared to the baseline, with the\ntrade-off metrics increasing by nearly 30 percent."}
{"id": "2507.15300", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.15300", "abs": "https://arxiv.org/abs/2507.15300", "authors": ["Minnan Pei", "Gang Li", "Junwen Si", "Zeyu Zhu", "Zitao Mo", "Peisong Wang", "Zhuoran Song", "Xiaoyao Liang", "Jian Cheng"], "title": "GCC: A 3DGS Inference Architecture with Gaussian-Wise and Cross-Stage Conditional Processing", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading neural rendering\ntechnique for high-fidelity view synthesis, prompting the development of\ndedicated 3DGS accelerators for mobile applications. Through in-depth analysis,\nwe identify two major limitations in the conventional decoupled\npreprocessing-rendering dataflow adopted by existing accelerators: 1) a\nsignificant portion of preprocessed Gaussians are not used in rendering, and 2)\nthe same Gaussian gets repeatedly loaded across different tile renderings,\nresulting in substantial computational and data movement overhead. To address\nthese issues, we propose GCC, a novel accelerator designed for fast and\nenergy-efficient 3DGS inference. At the dataflow level, GCC introduces: 1)\ncross-stage conditional processing, which interleaves preprocessing and\nrendering to dynamically skip unnecessary Gaussian preprocessing; and 2)\nGaussian-wise rendering, ensuring that all rendering operations for a given\nGaussian are completed before moving to the next, thereby eliminating\nduplicated Gaussian loading. We also propose an alpha-based boundary\nidentification method to derive compact and accurate Gaussian regions, thereby\nreducing rendering costs. We implement our GCC accelerator in 28nm technology.\nExtensive experiments demonstrate that GCC significantly outperforms the\nstate-of-the-art 3DGS inference accelerator, GSCore, in both performance and\nenergy efficiency."}
{"id": "2507.15843", "categories": ["cs.PL", "D.3.1; F.3.1; F.3.2; D.2.4"], "pdf": "https://arxiv.org/pdf/2507.15843", "abs": "https://arxiv.org/abs/2507.15843", "authors": ["Beniamino Accattoli", "Dan Ghica", "Giulio Guerrieri", "Cláudio Belo Lourenço", "Claudio Sacerdoti Coen"], "title": "Closure Conversion, Flat Environments, and the Complexity of Abstract Machines", "comment": null, "summary": "Closure conversion is a program transformation at work in compilers for\nfunctional languages to turn inner functions into global ones, by building\nclosures pairing the transformed functions with the environment of their free\nvariables. Abstract machines rely on similar and yet different concepts of\nclosures and environments.\n  In this paper, we study the relationship between the two approaches. We adopt\na very simple {\\lambda}-calculus with tuples as source language and study\nabstract machines for both the source language and the target of closure\nconversion. Moreover, we focus on the simple case of flat\nclosures/environments, that is, with no sharing of environments. We provide\nthree contributions.\n  Firstly, a new simple proof technique for the correctness of closure\nconversion, inspired by abstract machines.\n  Secondly, we show how the closure invariants of the target language allow us\nto design a new way of handling environments in abstract machines, not\nsuffering the shortcomings of other styles.\n  Thirdly, we study the machines from the point of view of time complexity,\nadapting analyses by Accattoli and co-authors. We show that closure conversion\ndecreases various dynamic costs while increasing the size of the initial code.\nDespite these changes, the overall complexity of the machines before and after\nclosure conversion turns out to be the same."}
{"id": "2507.14928", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14928", "abs": "https://arxiv.org/abs/2507.14928", "authors": ["Yongrae Jo", "Chanik Park"], "title": "Byzantine-Robust Decentralized Coordination of LLM Agents", "comment": null, "summary": "Collaboration among multiple large language model (LLM) agents is a promising\napproach to overcome inherent limitations of single-agent systems, such as\nhallucinations and single points of failure. As LLM agents are increasingly\ndeployed on open blockchain platforms, multi-agent systems capable of\ntolerating malicious (Byzantine) agents have become essential.\n  Recent Byzantine-robust multi-agent systems typically rely on leader-driven\ncoordination, which suffers from two major drawbacks. First, they are\ninherently vulnerable to targeted attacks against the leader. If consecutive\nleaders behave maliciously, the system repeatedly fails to achieve consensus,\nforcing new consensus rounds, which is particularly costly given the high\nlatency of LLM invocations. Second, an underperforming proposal from the leader\ncan be accepted as the final answer even when higher-quality alternatives are\navailable, as existing methods finalize the leader's proposal once it receives\na quorum of votes.\n  To address these issues, we propose DecentLLMs, a novel decentralized\nconsensus approach for multi-agent LLM systems, where worker agents generate\nanswers concurrently and evaluator agents independently score and rank these\nanswers to select the best available one. This decentralized architecture\nenables faster consensus despite the presence of Byzantine agents and\nconsistently selects higher-quality answers through Byzantine-robust\naggregation techniques.\n  Experimental results demonstrate that DecentLLMs effectively tolerates\nByzantine agents and significantly improves the quality of selected answers."}
{"id": "2507.15465", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15465", "abs": "https://arxiv.org/abs/2507.15465", "authors": ["Sungmin Yun", "Seonyong Park", "Hwayong Nam", "Younjoo Lee", "Gunjun Lee", "Kwanhee Kyung", "Sangpyo Kim", "Nam Sung Kim", "Jongmin Kim", "Hyungyo Kim", "Juhwan Cho", "Seungmin Baek", "Jung Ho Ahn"], "title": "The New LLM Bottleneck: A Systems Perspective on Latent Attention and Mixture-of-Experts", "comment": "15 pages, 11 figures", "summary": "Computational workloads composing traditional Transformer models are starkly\nbifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic\nintensity, while feedforward layers are compute-bound. This dichotomy has long\nmotivated research into specialized hardware to mitigate the MHA bottleneck.\n  This paper argues that recent architectural shifts, namely Multi-head Latent\nAttention (MLA) and Mixture-of-Experts (MoE), challenge the premise of\nspecialized attention hardware. We make two key observations. First, the\narithmetic intensity of MLA is over two orders of magnitude greater than that\nof MHA, shifting it close to a compute-bound regime well-suited for modern\naccelerators like GPUs. Second, by distributing MoE experts across a pool of\naccelerators, their arithmetic intensity can be tuned through batching to match\nthat of the dense layers, creating a more balanced computational profile.\n  These findings reveal a diminishing need for specialized attention hardware.\nThe central challenge for next-generation Transformers is no longer\naccelerating a single memory-bound layer. Instead, the focus must shift to\ndesigning balanced systems with sufficient compute, memory capacity, memory\nbandwidth, and high-bandwidth interconnects to manage the diverse demands of\nlarge-scale models."}
{"id": "2507.15121", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.15121", "abs": "https://arxiv.org/abs/2507.15121", "authors": ["Sasindu Wijeratne", "Rajgopal Kannan", "Viktor Prasanna"], "title": "AMPED: Accelerating MTTKRP for Billion-Scale Sparse Tensor Decomposition on Multiple GPUs", "comment": null, "summary": "Matricized Tensor Times Khatri-Rao Product (MTTKRP) is the computational\nbottleneck in sparse tensor decomposition. As real-world sparse tensors grow to\nbillions of nonzeros, they increasingly demand higher memory capacity and\ncompute throughput from hardware accelerators. In this work, we present AMPED,\na multi-GPU parallel algorithm designed to accelerate MTTKRP on billion-scale\nsparse tensors. AMPED scales beyond the limits of a single GPU, meeting both\nthe memory and performance requirements of large-scale workloads. We introduce\na partitioning strategy combined with a dynamic load balancing scheme to\ndistribute computation and minimize GPU idle time. On real-world billion-scale\ntensors, AMPED achieves a 5.1x geometric mean speedup in total execution time\nover state-of-the-art GPU baselines using 4 GPUs on a single CPU node."}
{"id": "2507.15603", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.15603", "abs": "https://arxiv.org/abs/2507.15603", "authors": ["Haoxiong Ren", "Yangu He", "Kwunhang Wong", "Rui Bao", "Ning Lin", "Zhongrui Wang", "Dashan Shang"], "title": "When Pipelined In-Memory Accelerators Meet Spiking Direct Feedback Alignment: A Co-Design for Neuromorphic Edge Computing", "comment": "International Conference on Computer-Aided Design 2025", "summary": "Spiking Neural Networks (SNNs) are increasingly favored for deployment on\nresource-constrained edge devices due to their energy-efficient and\nevent-driven processing capabilities. However, training SNNs remains\nchallenging because of the computational intensity of traditional\nbackpropagation algorithms adapted for spike-based systems. In this paper, we\npropose a novel software-hardware co-design that introduces a hardware-friendly\ntraining algorithm, Spiking Direct Feedback Alignment (SDFA) and implement it\non a Resistive Random Access Memory (RRAM)-based In-Memory Computing (IMC)\narchitecture, referred to as PipeSDFA, to accelerate SNN training.\nSoftware-wise, the computational complexity of SNN training is reduced by the\nSDFA through the elimination of sequential error propagation. Hardware-wise, a\nthree-level pipelined dataflow is designed based on IMC architecture to\nparallelize the training process. Experimental results demonstrate that the\nPipeSDFA training accelerator incurs less than 2% accuracy loss on five\ndatasets compared to baselines, while achieving 1.1X~10.5X and 1.37X~2.1X\nreductions in training time and energy consumption, respectively compared to\nPipeLayer."}
{"id": "2507.15154", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.15154", "abs": "https://arxiv.org/abs/2507.15154", "authors": ["Kohya Shiozaki", "Junya Nakamura"], "title": "Dynatune: Dynamic Tuning of Raft Election Parameters Using Network Measurement", "comment": "This paper was accepted at the 27th International Workshop on\n  Advances in Parallel and Distributed Computational Models (APDCM 2025), held\n  in conjunction with IPDPS 2025", "summary": "Raft is a leader-based consensus algorithm that implements State Machine\nReplication (SMR), which replicates the service state across multiple servers\nto enhance fault tolerance. In Raft, the servers play one of three roles:\nleader, follower, or candidate. The leader receives client requests, determines\nthe processing order, and replicates them to the followers. When the leader\nfails, the service must elect a new leader to continue processing requests,\nduring which the service experiences an out-of-service (OTS) time. The OTS time\nis directly influenced by election parameters, such as heartbeat interval and\nelection timeout. However, traditional approaches, such as Raft, often struggle\nto effectively tune these parameters, particularly under fluctuating network\nconditions, leading to increased OTS time and reduced service responsiveness.\nTo address this, we propose Dynatune, a mechanism that dynamically adjusts\nRaft's election parameters based on network metrics such as round-trip time and\npacket loss rates measured via heartbeats. By adapting to changing network\nenvironments, Dynatune significantly reduces the leader failure detection and\nOTS time without altering Raft's core mechanisms or introducing additional\ncommunication overheads. Experimental results demonstrate that Dynatune reduces\nthe leader failure detection and OTS times by 80% and 45%, respectively,\ncompared with Raft, while maintaining high availability even under dynamic\nnetwork conditions. These findings confirm that Dynatune effectively enhances\nthe performance and reliability of SMR services in various network scenarios."}
{"id": "2507.15664", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.15664", "abs": "https://arxiv.org/abs/2507.15664", "authors": ["Haomin Qi", "Yuyang Du", "Lihao Zhang", "Soung Chang Liew", "Kexin Chen", "Yining Du"], "title": "VeriRAG: A Retrieval-Augmented Framework for Automated RTL Testability Repair", "comment": "8 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated immense potential in\ncomputer-aided design (CAD), particularly for automated debugging and\nverification within electronic design automation (EDA) tools. However, Design\nfor Testability (DFT) remains a relatively underexplored area. This paper\npresents VeriRAG, the first LLM-assisted DFT-EDA framework. VeriRAG leverages a\nRetrieval-Augmented Generation (RAG) approach to enable LLM to revise code to\nensure DFT compliance. VeriRAG integrates (1) an autoencoder-based similarity\nmeasurement model for precise retrieval of reference RTL designs for the LLM,\nand (2) an iterative code revision pipeline that allows the LLM to ensure DFT\ncompliance while maintaining synthesizability. To support VeriRAG, we introduce\nVeriDFT, a Verilog-based DFT dataset curated for DFT-aware RTL repairs. VeriRAG\nretrieves structurally similar RTL designs from VeriDFT, each paired with a\nrigorously validated correction, as references for code repair. With VeriRAG\nand VeriDFT, we achieve fully automated DFT correction -- resulting in a\n7.72-fold improvement in successful repair rate compared to the zero-shot\nbaseline (Fig. 5 in Section V). Ablation studies further confirm the\ncontribution of each component of the VeriRAG framework. We open-source our\ndata, models, and scripts at https://github.com/yuyangdu01/LLM4DFT."}
{"id": "2507.15230", "categories": ["cs.DC", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.15230", "abs": "https://arxiv.org/abs/2507.15230", "authors": ["Guoxi Liu", "Thomas Randall", "Rong Ge", "Federico Iuricich"], "title": "GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis", "comment": null, "summary": "Unstructured meshes present challenges in scientific data analysis due to\nirregular distribution and complex connectivity. Computing and storing\nconnectivity information is a major bottleneck for visualization algorithms,\naffecting both time and memory performance. Recent task-parallel data\nstructures address this by precomputing connectivity information at runtime\nwhile the analysis algorithm executes, effectively hiding computation costs and\nimproving performance. However, existing approaches are CPU-bound, forcing the\ndata structure and analysis algorithm to compete for the same computational\nresources, limiting potential speedups. To overcome this limitation, we\nintroduce a novel task-parallel approach optimized for heterogeneous CPU-GPU\nsystems. Specifically, we offload the computation of mesh connectivity\ninformation to GPU threads, enabling CPU threads to focus on executing the\nvisualization algorithm. Following this paradigm, we propose GALE (GPU-Aided\nLocalized data structurE), the first open-source CUDA-based data structure\ndesigned for heterogeneous task parallelism. Experiments on two 20-core CPUs\nand an NVIDIA V100 GPU show that GALE achieves up to 2.7x speedup over\nstate-of-the-art localized data structures while maintaining memory efficiency."}
{"id": "2507.15233", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.15233", "abs": "https://arxiv.org/abs/2507.15233", "authors": ["Jintao Liu", "Mohammad Goudarzi", "Adel Nadjaran Toosi"], "title": "An ML-Driven Participant Selection Technique for Federated Recommendation System in Edge-Cloud Computing", "comment": null, "summary": "Recommendation systems (RS) personalize content by analyzing user\npreferences, but typically require centralized collection of user data, raising\nprivacy and scalability concerns. Federated Recommendation Systems (FRS)\naddress these issues by enabling distributed, privacy-preserving model training\nacross edge devices, keeping raw data on-device. Although existing FRS\nframeworks benefit from on-device feature extraction and privacy preservation,\nthey suffer from heterogeneous device capabilities, non-independent and\nidentically distributed (non-IID) data, and communication bottlenecks. To\novercome these limitations, we propose a multi-objective reinforcement learning\n(RL) participant selection that jointly optimizes historical client performance\nreputation (CPR), data utility, and system efficiency. First, we define a\ncomposite client-utility function combining CPR, system capability, and data\nquality. Next, we embed this utility into a multi-armed bandit (MAB) framework\nand dynamically balance exploration-exploitation to select participants.\nFinally, we practically implement our approach using the PySyft framework on an\nedge-cloud testbed, and evaluate it on a multimodal movie-recommendation task\nbuilt from the MovieLens-100K dataset. Across four different skewed\ndata-partition scenarios, our MAB-based selection accelerates convergence by\n32-50% in time-to-target AUC and reduces total wall-clock training time by up\nto 46%, while matching or slightly improving final AUC, NDCG@50, and Recall@50\ncompared to existing FRS baselines. Our results demonstrate that adaptive,\nreward-driven client sampling can substantially enhance both efficiency and\nfairness in real-world federated deployments."}
{"id": "2507.15553", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.15553", "abs": "https://arxiv.org/abs/2507.15553", "authors": ["Shibo Yu", "Mohammad Goudarzi", "Adel Nadjaran Toosi"], "title": "Efficient Routing of Inference Requests across LLM Instances in Cloud-Edge Computing", "comment": null, "summary": "The rising demand for Large Language Model (LLM) inference services has\nintensified pressure on computational resources, resulting in latency and cost\nchallenges. This paper introduces a novel routing algorithm based on the\nNon-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference\nrequests across heterogeneous LLM instances in a cloud-edge computing\nenvironment. Formulated as a multi-objective optimization problem, the\nalgorithm balances response quality, response time, and inference cost,\nadapting to request heterogeneity (e.g., varying complexity and prompt lengths)\nand node diversity (e.g., edge vs. cloud resources). This adaptive routing\nalgorithm optimizes performance under dynamic workloads. We benchmark the\napproach using a testbed with datasets including Stanford Question Answering\nDataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With\nAdversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K).\nExperimental results show our solution, compared to the baselines, achieves up\nto 95.2% and 34.9% improvements in terms of response time and cost,\nrespectively. These findings validate the algorithm's effectiveness for\nscalable LLM deployments."}
{"id": "2507.14471", "categories": ["cs.PL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.14471", "abs": "https://arxiv.org/abs/2507.14471", "authors": ["Logan Kenwright", "Partha Roop", "Nathan Allen", "Călin Caşcaval", "Avinash Malik"], "title": "Timetide: A programming model for logically synchronous distributed systems", "comment": "25 Pages, 21 Figures", "summary": "Massive strides in deterministic models have been made using synchronous\nlanguages. They are mainly focused on centralised applications, as the\ntraditional approach is to compile away the concurrency. Time triggered\nlanguages such as Giotto and Lingua Franca are suitable for distribution albeit\nthat they rely on expensive physical clock synchronisation, which is both\nexpensive and may suffer from scalability. Hence, deterministic programming of\ndistributed systems remains challenging. We address the challenges of\ndeterministic distribution by developing a novel multiclock semantics of\nsynchronous programs. The developed semantics is amenable to seamless\ndistribution. Moreover, our programming model, Timetide, alleviates the need\nfor physical clock synchronisation by building on the recently proposed logical\nsynchrony model for distributed systems. We discuss the important aspects of\ndistributing computation, such as network communication delays, and explore the\nformal verification of Timetide programs. To the best of our knowledge,\nTimetide is the first multiclock synchronous language that is both amenable to\ndistribution and formal verification without the need for physical clock\nsynchronisation or clock gating."}
