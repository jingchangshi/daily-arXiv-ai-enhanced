{"id": "2510.19129", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.19129", "abs": "https://arxiv.org/abs/2510.19129", "authors": ["Qiancheng Fu", "Hongwei Xi", "Ankush Das"], "title": "Dependent Session Types for Verified Concurrent Programming", "comment": null, "summary": "We present TLLC which extends the Two-Level Linear dependent type theory\n(TLL) with session-based concurrency. Equipped with Martin-L\\\"{o}f style\ndependency, the session types of TLLC allow protocols to specify properties of\ncommunicated messages. When used in conjunction with the dependent type\nmachinery already present in TLL, dependent session types facilitate a form of\nrelational verification by relating concurrent programs with their idealized\nsequential counterparts. Correctness properties proven for sequential programs\ncan be easily lifted to their corresponding concurrent implementations. TLLC\nmakes session types a powerful tool for intrinsically verifying the correctness\nof data structures such as queues and concurrent algorithms such as map-reduce.\nTo extend TLL with session types, we develop a novel formulation of\nintuitionistic session type which we believe to be widely applicable for\nintegrating session types into other type systems beyond the context of TLLC.\nWe study the meta-theory of our language, proving its soundness as both a term\ncalculus and a process calculus. To demonstrate the practicality of TLLC, we\nhave implemented a prototype compiler that translates TLLC programs into\nconcurrent C code, which has been extensively evaluated.", "AI": {"tldr": "TLLC\u6269\u5c55\u4e86TLL\u7c7b\u578b\u7cfb\u7edf\uff0c\u589e\u52a0\u4e86\u57fa\u4e8e\u4f1a\u8bdd\u7684\u5e76\u53d1\u6027\u548c\u4f9d\u8d56\u4f1a\u8bdd\u7c7b\u578b\uff0c\u652f\u6301\u5173\u7cfb\u9a8c\u8bc1\u548c\u5e76\u53d1\u7a0b\u5e8f\u6b63\u786e\u6027\u9a8c\u8bc1", "motivation": "\u5c06\u4f9d\u8d56\u7c7b\u578b\u4e0e\u4f1a\u8bdd\u7c7b\u578b\u7ed3\u5408\uff0c\u4f7f\u5e76\u53d1\u7a0b\u5e8f\u80fd\u591f\u7ee7\u627f\u987a\u5e8f\u7a0b\u5e8f\u7684\u6b63\u786e\u6027\u8bc1\u660e\uff0c\u4e3a\u961f\u5217\u7b49\u6570\u636e\u7ed3\u6784\u548cmap-reduce\u7b49\u5e76\u53d1\u7b97\u6cd5\u63d0\u4f9b\u5185\u5728\u9a8c\u8bc1", "method": "\u5f00\u53d1\u4e86\u76f4\u89c9\u4f1a\u8bdd\u7c7b\u578b\u7684\u65b0\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u6269\u5c55TLL\u7c7b\u578b\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4f9d\u8d56\u4f1a\u8bdd\u7c7b\u578b\uff0c\u6784\u5efa\u539f\u578b\u7f16\u8bd1\u5668\u751f\u6210\u5e76\u53d1C\u4ee3\u7801", "result": "\u5efa\u7acb\u4e86TLLC\u7684\u5143\u7406\u8bba\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u9879\u6f14\u7b97\u548c\u8fdb\u7a0b\u6f14\u7b97\u7684\u53ef\u9760\u6027\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u5b9e\u73b0\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30", "conclusion": "TLLC\u6210\u529f\u5c06\u4f9d\u8d56\u7c7b\u578b\u4e0e\u4f1a\u8bdd\u7c7b\u578b\u96c6\u6210\uff0c\u4e3a\u5e76\u53d1\u7a0b\u5e8f\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u5176\u4f1a\u8bdd\u7c7b\u578b\u5f62\u5f0f\u5316\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027"}}
{"id": "2510.19012", "categories": ["cs.DC", "cs.DB", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.19012", "abs": "https://arxiv.org/abs/2510.19012", "authors": ["Ivan Borodii", "Illia Fedorovych", "Halyna Osukhivska", "Diana Velychko", "Roman Butsii"], "title": "Comparative analysis of large data processing in Apache Spark using Java, Python and Scala", "comment": "CITI 2025, 3rd International Workshop on Computer Information\n  Technologies in Industry 4.0, June 11-12, 2025, Ternopil, Ukraine. The\n  article includes 10 pages, 5 figures, 9 tables", "summary": "During the study, the results of a comparative analysis of the process of\nhandling large datasets using the Apache Spark platform in Java, Python, and\nScala programming languages were obtained. Although prior works have focused on\nindividual stages, comprehensive comparisons of full ETL workflows across\nprogramming languages using Apache Iceberg remain limited. The analysis was\nperformed by executing several operations, including downloading data from CSV\nfiles, transforming and loading it into an Apache Iceberg analytical table. It\nwas found that the performance of the Spark algorithm varies significantly\ndepending on the amount of data and the programming language used. When\nprocessing a 5-megabyte CSV file, the best result was achieved in Python: 6.71\nseconds, which is superior to Scala's score of 9.13 seconds and Java's time of\n9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming\nlanguages demonstrated similar results: the fastest performance was showed in\nPython: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56\nseconds, respectively. When performing a more complex operation that involved\ncombining two CSV files into a single dataset for further loading into an\nApache Iceberg table, Scala demonstrated the highest performance, at 374.42\nseconds. Java processing was completed in 379.8 seconds, while Python was the\nleast efficient, with a runtime of 398.32 seconds. It follows that the\nprogramming language significantly affects the efficiency of data processing by\nthe Apache Spark algorithm, with Scala and Java being more productive for\nprocessing large amounts of data and complex operations, while Python\ndemonstrates an advantage in working with small amounts of data. The results\nobtained can be useful for optimizing data handling processes depending on\nspecific performance requirements and the amount of information being\nprocessed.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u5728Apache Spark\u5e73\u53f0\u4e0a\u4f7f\u7528Java\u3001Python\u548cScala\u5904\u7406\u5927\u6570\u636e\u96c6\u7684\u6027\u80fd\u5dee\u5f02\u3002\u7ed3\u679c\u663e\u793a\u7f16\u7a0b\u8bed\u8a00\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff1aPython\u5728\u5c0f\u6570\u636e\u96c6\u5904\u7406\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800cScala\u548cJava\u5728\u590d\u6742\u64cd\u4f5c\u548c\u5927\u6570\u636e\u5904\u7406\u65b9\u9762\u66f4\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5355\u4e2a\u5904\u7406\u9636\u6bb5\uff0c\u7f3a\u4e4f\u5bf9\u5b8c\u6574ETL\u5de5\u4f5c\u6d41\u7a0b\u5728\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u4e0b\u7684\u5168\u9762\u6bd4\u8f83\uff0c\u7279\u522b\u662f\u4f7f\u7528Apache Iceberg\u65f6\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u6267\u884c\u591a\u4e2a\u64cd\u4f5c\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff0c\u5305\u62ec\u4eceCSV\u6587\u4ef6\u4e0b\u8f7d\u6570\u636e\u3001\u8f6c\u6362\u6570\u636e\u5e76\u52a0\u8f7d\u5230Apache Iceberg\u5206\u6790\u8868\u4e2d\u3002\u6d4b\u8bd5\u4e86\u4e0d\u540c\u5927\u5c0f\u7684\u6570\u636e\u96c6\uff085MB\u548c1.6GB\uff09\u4ee5\u53ca\u590d\u6742\u7684\u6570\u636e\u5408\u5e76\u64cd\u4f5c\u3002", "result": "\u5904\u74065MB\u6587\u4ef6\u65f6Python\u6700\u5feb\uff086.71\u79d2\uff09\uff0cScala\uff089.13\u79d2\uff09\u548cJava\uff089.62\u79d2\uff09\u8f83\u6162\u3002\u5904\u74061.6GB\u6587\u4ef6\u65f6\u4e09\u8005\u6027\u80fd\u76f8\u8fd1\uff0cPython\u4ecd\u6700\u5feb\uff0846.34\u79d2\uff09\u3002\u590d\u6742\u5408\u5e76\u64cd\u4f5c\u4e2dScala\u8868\u73b0\u6700\u4f73\uff08374.42\u79d2\uff09\uff0cPython\u6700\u6162\uff08398.32\u79d2\uff09\u3002", "conclusion": "\u7f16\u7a0b\u8bed\u8a00\u663e\u8457\u5f71\u54cdApache Spark\u6570\u636e\u5904\u7406\u6548\u7387\u3002Python\u9002\u5408\u5c0f\u6570\u636e\u96c6\uff0cScala\u548cJava\u66f4\u9002\u5408\u5927\u6570\u636e\u91cf\u548c\u590d\u6742\u64cd\u4f5c\u3002\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u6839\u636e\u5177\u4f53\u6027\u80fd\u9700\u6c42\u548c\u6570\u636e\u91cf\u4f18\u5316\u6570\u636e\u5904\u7406\u6d41\u7a0b\u3002"}}
{"id": "2510.19260", "categories": ["cs.AR", "cs.ET", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.19260", "abs": "https://arxiv.org/abs/2510.19260", "authors": ["Mukul Lokhande", "Narendra Singh Dhakad", "Seema Chouhan", "Akash Sankhe", "Santosh Kumar Vishvakarma"], "title": "Res-DPU: Resource-shared Digital Processing-in-memory Unit for Edge-AI Workloads", "comment": null, "summary": "Processing-in-memory (PIM) has emerged as the go to solution for addressing\nthe von Neumann bottleneck in edge AI accelerators. However, state-of-the-art\n(SoTA) digital PIM approaches suffer from low compute density, primarily due to\nthe use of bulky bit cells and transistor-heavy adder trees, which impose\nlimitations on macro scalability and energy efficiency. This work introduces\nRes-DPU, a resource-shared digital PIM unit, with a dual-port 5T SRAM latch and\nshared 2T AND compute logic. This reflects the per-bit multiplication cost to\njust 5.25T and reduced the transistor count of the PIM array by up to 56% over\nthe SoTA works. Furthermore, a Transistor-Reduced 2D Interspersed Adder Tree\n(TRAIT) with FA-7T and PG-FA-26T helps reduce the power consumption of the\nadder tree by up to 21.35% and leads to improved energy efficiency by 59%\ncompared to conventional 28T RCA designs. We propose a Cycle-controlled\nIterative Approximate-Accurate Multiplication (CIA2M) approach, enabling\nrun-time accuracy-latency trade-offs without requiring error-correction\ncircuitry. The 16 KB REP-DPIM macro achieves 0.43 TOPS throughput and 87.22\nTOPS/W energy efficiency in TSMC 65nm CMOS, with 96.85% QoR for ResNet-18 or\nVGG-16 on CIFAR-10, including 30% pruning. The proposed results establish a\nRes-DPU module for highly scalable and energy-efficient real-time edge AI\naccelerators.", "AI": {"tldr": "Res-DPU\u662f\u4e00\u79cd\u8d44\u6e90\u5171\u4eab\u7684\u6570\u5b57\u5185\u5b58\u5904\u7406\u5355\u5143\uff0c\u91c7\u7528\u53cc\u7aef\u53e35T SRAM\u9501\u5b58\u5668\u548c\u5171\u4eab2T AND\u8ba1\u7b97\u903b\u8f91\uff0c\u5c06\u6bcf\u6bd4\u7279\u4e58\u6cd5\u6210\u672c\u964d\u81f35.25T\uff0c\u6bd4\u73b0\u6709\u6280\u672f\u51cf\u5c1156%\u6676\u4f53\u7ba1\u6570\u91cf\uff0c\u5e76\u901a\u8fc7TRAIT\u52a0\u6cd5\u6811\u548cCIA2M\u65b9\u6cd5\u5b9e\u73b0\u80fd\u6548\u63d0\u5347\u548c\u7cbe\u5ea6-\u5ef6\u8fdf\u6743\u8861\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6570\u5b57\u5185\u5b58\u5904\u7406\u65b9\u6cd5\u56e0\u4f7f\u7528\u5e9e\u5927\u6bd4\u7279\u5355\u5143\u548c\u6676\u4f53\u7ba1\u5bc6\u96c6\u578b\u52a0\u6cd5\u6811\u5bfc\u81f4\u7684\u4f4e\u8ba1\u7b97\u5bc6\u5ea6\u3001\u5b8f\u53ef\u6269\u5c55\u6027\u53d7\u9650\u548c\u80fd\u6548\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faRes-DPU\u67b6\u6784\uff0c\u5305\u542b\u53cc\u7aef\u53e35T SRAM\u9501\u5b58\u5668\u3001\u5171\u4eab2T AND\u8ba1\u7b97\u903b\u8f91\u3001TRAIT\u52a0\u6cd5\u6811\uff08\u4f7f\u7528FA-7T\u548cPG-FA-26T\uff09\uff0c\u4ee5\u53caCIA2M\u8fd0\u884c\u65f6\u7cbe\u5ea6-\u5ef6\u8fdf\u6743\u8861\u65b9\u6cd5\u3002", "result": "16KB REP-DPIM\u5b8f\u5728TSMC 65nm\u5de5\u827a\u4e0b\u5b9e\u73b00.43 TOPS\u541e\u5410\u91cf\u548c87.22 TOPS/W\u80fd\u6548\uff0c\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\u5bf9ResNet-18\u548cVGG-16\uff08\u542b30%\u526a\u679d\uff09\u8fbe\u523096.85%\u8d28\u91cf\u7ed3\u679c\u3002", "conclusion": "Res-DPU\u4e3a\u9ad8\u5ea6\u53ef\u6269\u5c55\u548c\u80fd\u6548\u4f18\u5316\u7684\u5b9e\u65f6\u8fb9\u7f18AI\u52a0\u901f\u5668\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.18893", "categories": ["cs.DC", "cs.AI", "cs.SE", "I.2.11; D.2.11"], "pdf": "https://arxiv.org/pdf/2510.18893", "abs": "https://arxiv.org/abs/2510.18893", "authors": ["Sergey Pugachev"], "title": "CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation", "comment": "11 pages, 3 figures", "summary": "Multi-agent LLM systems fail to realize parallel speedups due to costly\ncoordination. We present CodeCRDT, an observation-driven coordination pattern\nwhere agents coordinate by monitoring a shared state with observable updates\nand deterministic convergence, rather than explicit message passing. Using\nConflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,\nconflict-free concurrent code generation with strong eventual consistency.\nEvaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits\nand trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on\nothers, and 100% convergence with zero merge failures. The study formalizes\nobservation-driven coordination for stochastic LLM agents, revealing semantic\nconflict rates (5-10%) and quality-performance tradeoffs, and provides\nempirical characterization of when parallel coordination succeeds versus fails\nbased on task structure.", "AI": {"tldr": "CodeCRDT\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c2\u5bdf\u9a71\u52a8\u7684\u534f\u8c03\u6a21\u5f0f\uff0c\u4f7f\u7528CRDT\u5b9e\u73b0\u65e0\u9501\u3001\u65e0\u51b2\u7a81\u7684\u5e76\u53d1\u4ee3\u7801\u751f\u6210\uff0c\u5728\u90e8\u5206\u4efb\u52a1\u4e0a\u83b7\u5f9721.1%\u52a0\u901f\uff0c\u4f46\u67d0\u4e9b\u4efb\u52a1\u670939.4%\u51cf\u901f\uff0c100%\u6536\u655b\u4e14\u65e0\u5408\u5e76\u5931\u8d25\u3002", "motivation": "\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u7531\u4e8e\u6602\u8d35\u7684\u534f\u8c03\u6210\u672c\u800c\u65e0\u6cd5\u5b9e\u73b0\u5e76\u884c\u52a0\u901f\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u534f\u8c03\u673a\u5236\u3002", "method": "\u4f7f\u7528\u51b2\u7a81\u65e0\u5173\u590d\u5236\u6570\u636e\u7c7b\u578b\uff08CRDTs\uff09\uff0c\u901a\u8fc7\u76d1\u63a7\u5171\u4eab\u72b6\u6001\u7684\u53ef\u89c2\u5bdf\u66f4\u65b0\u548c\u786e\u5b9a\u6027\u6536\u655b\u6765\u5b9e\u73b0\u534f\u8c03\uff0c\u800c\u975e\u663e\u5f0f\u6d88\u606f\u4f20\u9012\u3002", "result": "\u5728600\u6b21\u8bd5\u9a8c\u4e2d\uff0c\u90e8\u5206\u4efb\u52a1\u83b7\u5f9721.1%\u52a0\u901f\uff0c\u90e8\u5206\u4efb\u52a1\u670939.4%\u51cf\u901f\uff0c100%\u6536\u655b\u4e14\u65e0\u5408\u5e76\u5931\u8d25\uff0c\u8bed\u4e49\u51b2\u7a81\u7387\u4e3a5-10%\u3002", "conclusion": "\u7814\u7a76\u5f62\u5f0f\u5316\u4e86\u89c2\u5bdf\u9a71\u52a8\u534f\u8c03\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u8bed\u4e49\u51b2\u7a81\u7387\u548c\u8d28\u91cf-\u6027\u80fd\u6743\u8861\uff0c\u5e76\u57fa\u4e8e\u4efb\u52a1\u7ed3\u6784\u5b9e\u8bc1\u5206\u6790\u4e86\u5e76\u884c\u534f\u8c03\u7684\u6210\u529f\u4e0e\u5931\u8d25\u6761\u4ef6\u3002"}}
{"id": "2510.19577", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.19577", "abs": "https://arxiv.org/abs/2510.19577", "authors": ["Zuoming Fu", "Alex Manley", "Mohammad Alian"], "title": "gem5 Co-Pilot: AI Assistant Agent for Architectural Design Space Exploration", "comment": "Accepted by CAMS25, October, 2025, Seoul, Republic of Korea", "summary": "Generative AI is increasing the productivity of software and hardware\ndevelopment across many application domains. In this work, we utilize the power\nof Large Language Models (LLMs) to develop a co-pilot agent for assisting gem5\nusers with automating design space exploration. Computer architecture design\nspace exploration is complex and time-consuming, given that numerous parameter\nsettings and simulation statistics must be analyzed before improving the\ncurrent design. The emergence of LLMs has significantly accelerated the\nanalysis of long-text data as well as smart decision making, two key functions\nin a successful design space exploration task. In this project, we first build\ngem5 Co-Pilot, an AI agent assistant for gem5, which comes with a webpage-GUI\nfor smooth user interaction, agent automation, and result summarization. We\nalso implemented a language for design space exploration, as well as a Design\nSpace Database (DSDB). With DSDB, gem5 Co-Pilot effectively implements a\nRetrieval Augmented Generation system for gem5 design space exploration. We\nexperiment on cost-constraint optimization with four cost ranges and compare\nour results with two baseline models. Results show that gem5 Co-Pilot can\nquickly identify optimal parameters for specific design constraints based on\nperformance and cost, with limited user interaction.", "AI": {"tldr": "\u5f00\u53d1\u4e86gem5 Co-Pilot\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684AI\u52a9\u624b\uff0c\u7528\u4e8e\u81ea\u52a8\u5316gem5\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u901a\u8fc7\u7f51\u9875GUI\u3001\u8bbe\u8ba1\u7a7a\u95f4\u6570\u636e\u5e93\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u6765\u4f18\u5316\u8ba1\u7b97\u673a\u67b6\u6784\u53c2\u6570\u914d\u7f6e\u3002", "motivation": "\u8ba1\u7b97\u673a\u67b6\u6784\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u590d\u6742\u8017\u65f6\uff0c\u9700\u8981\u5206\u6790\u5927\u91cf\u53c2\u6570\u8bbe\u7f6e\u548c\u4eff\u771f\u7edf\u8ba1\u6570\u636e\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u52a0\u901f\u957f\u6587\u672c\u5206\u6790\u548c\u667a\u80fd\u51b3\u7b56\uff0c\u8fd9\u6b63\u662f\u6210\u529f\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u6240\u9700\u7684\u5173\u952e\u529f\u80fd\u3002", "method": "\u6784\u5efagem5 Co-Pilot AI\u52a9\u624b\uff0c\u5305\u542b\u7f51\u9875GUI\u7528\u4e8e\u7528\u6237\u4ea4\u4e92\u3001\u81ea\u52a8\u5316\u4ee3\u7406\u548c\u7ed3\u679c\u603b\u7ed3\uff1b\u5b9e\u73b0\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u8bed\u8a00\u548c\u8bbe\u8ba1\u7a7a\u95f4\u6570\u636e\u5e93(DSDB)\uff1b\u57fa\u4e8eDSDB\u5b9e\u73b0\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u3002", "result": "\u5728\u56db\u4e2a\u6210\u672c\u7ea6\u675f\u8303\u56f4\u5185\u8fdb\u884c\u6210\u672c\u7ea6\u675f\u4f18\u5316\u5b9e\u9a8c\uff0c\u4e0e\u4e24\u4e2a\u57fa\u7ebf\u6a21\u578b\u6bd4\u8f83\uff0c\u7ed3\u679c\u663e\u793agem5 Co-Pilot\u80fd\u591f\u57fa\u4e8e\u6027\u80fd\u548c\u6210\u672c\u5feb\u901f\u8bc6\u522b\u7279\u5b9a\u8bbe\u8ba1\u7ea6\u675f\u4e0b\u7684\u6700\u4f18\u53c2\u6570\uff0c\u4e14\u7528\u6237\u4ea4\u4e92\u6709\u9650\u3002", "conclusion": "gem5 Co-Pilot\u6210\u529f\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\uff0c\u6709\u6548\u52a0\u901f\u4e86\u8ba1\u7b97\u673a\u67b6\u6784\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u8fc7\u7a0b\uff0c\u80fd\u591f\u5728\u6709\u9650\u7528\u6237\u4ea4\u4e92\u4e0b\u5feb\u901f\u627e\u5230\u6700\u4f18\u53c2\u6570\u914d\u7f6e\u3002"}}
{"id": "2510.18897", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18897", "abs": "https://arxiv.org/abs/2510.18897", "authors": ["Jacopo Tagliabue"], "title": "AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators", "comment": "Pre-print IAAA workshop submission", "summary": "We explore AI-driven distributed-systems policy design by combining\nstochastic code generation from large language models (LLMs) with deterministic\nverification in a domain-specific simulator. Using a Function-as-a-Service\nruntime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we\nframe scheduler design as an iterative generate-and-verify loop: an LLM\nproposes a Python policy, the simulator evaluates it on standardized traces,\nand structured feedback steers subsequent generations. This setup preserves\ninterpretability while enabling targeted search over a large design space. We\ndetail the system architecture and report preliminary results on throughput\nimprovements across multiple models. Beyond early gains, we discuss the limits\nof the current setup and outline next steps; in particular, we conjecture that\nAI will be crucial for scaling this methodology by helping to bootstrap new\nsimulators.", "AI": {"tldr": "\u7ed3\u5408LLM\u968f\u673a\u4ee3\u7801\u751f\u6210\u4e0e\u786e\u5b9a\u6027\u9a8c\u8bc1\uff0c\u901a\u8fc7\u751f\u6210-\u9a8c\u8bc1\u5faa\u73af\u81ea\u52a8\u8bbe\u8ba1\u5206\u5e03\u5f0f\u7cfb\u7edf\u8c03\u5ea6\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63a2\u7d22\u5927\u89c4\u6a21\u8bbe\u8ba1\u7a7a\u95f4\u3002", "motivation": "\u63a2\u7d22AI\u9a71\u52a8\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\u7b56\u7565\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5c06LLM\u7684\u521b\u9020\u6027\u751f\u6210\u80fd\u529b\u4e0e\u6a21\u62df\u5668\u7684\u786e\u5b9a\u6027\u9a8c\u8bc1\u76f8\u7ed3\u5408\uff0c\u4ee5\u81ea\u52a8\u5316\u65b9\u5f0f\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u751f\u6210-\u9a8c\u8bc1\u5faa\u73af\uff1aLLM\u751f\u6210Python\u8c03\u5ea6\u7b56\u7565\uff0c\u6a21\u62df\u5668\u5728\u6807\u51c6\u5316trace\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u6784\u5316\u53cd\u9988\u6307\u5bfc\u540e\u7eed\u751f\u6210\u3002\u57fa\u4e8eBauplan FaaS\u8fd0\u884c\u65f6\u548cEudoxia\u6a21\u62df\u5668\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u5728\u591a\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u541e\u5410\u91cf\u63d0\u5347\u7684\u521d\u6b65\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7b56\u7565\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u6709\u6548\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u672a\u6765AI\u5c06\u5728\u6269\u5c55\u8be5\u65b9\u6cd5\u8bba\u4e2d\u53d1\u6325\u5173\u952e\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5e2e\u52a9\u5f15\u5bfc\u65b0\u6a21\u62df\u5668\u7684\u5f00\u53d1\u3002"}}
{"id": "2510.19151", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.19151", "abs": "https://arxiv.org/abs/2510.19151", "authors": ["Seri Khoury", "Manish Purohit", "Aaron Schild", "Joshua Wang"], "title": "On the Randomized Locality of Matching Problems in Regular Graphs", "comment": "DISC 2025. Abstract modified for arXiv", "summary": "The main goal in distributed symmetry-breaking is to understand the locality\nof problems; i.e., the radius of the neighborhood that a node needs to explore\nin order to arrive at its part of a global solution. In this work, we study the\nlocality of matching problems in the family of regular graphs, which is one of\nthe main benchmarks for establishing lower bounds on the locality of\nsymmetry-breaking problems, as well as for obtaining classification results.\nFor approximate matching, we develop randomized algorithms to show that $(1 +\n\\epsilon)$-approximate matching in regular graphs is truly local; i.e., the\nlocality depends only on $\\epsilon$ and is independent of all other graph\nparameters. Furthermore, as long as the degree $\\Delta$ is not very small\n(namely, as long as $\\Delta \\geq \\text{poly}(1/\\epsilon)$), this dependence is\nonly logarithmic in $1/\\epsilon$. This stands in sharp contrast to maximal\nmatching in regular graphs which requires some dependence on the number of\nnodes $n$ or the degree $\\Delta$. We show matching lower bounds for both\nresults. For maximal matching, our techniques further allow us to establish a\nstrong separation between the node-averaged complexity and worst-case\ncomplexity of maximal matching in regular graphs, by showing that the former is\nonly $O(1)$. Central to our main technical contribution is a novel\nmartingale-based analysis for the $\\approx 40$-year-old algorithm by Luby. In\nparticular, our analysis shows that applying one round of Luby's algorithm on\nthe line graph of a $\\Delta$-regular graph results in an almost\n$\\Delta/2$-regular graph.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6b63\u5219\u56fe\u4e2d\u5339\u914d\u95ee\u9898\u7684\u5c40\u90e8\u6027\uff0c\u8bc1\u660e\u4e86(1+\u03b5)-\u8fd1\u4f3c\u5339\u914d\u662f\u771f\u6b63\u5c40\u90e8\u7684\uff0c\u5176\u5c40\u90e8\u6027\u4ec5\u4f9d\u8d56\u4e8e\u03b5\u800c\u4e0e\u56fe\u53c2\u6570\u65e0\u5173\u3002\u540c\u65f6\u63ed\u793a\u4e86\u6700\u5927\u5339\u914d\u5728\u8282\u70b9\u5e73\u5747\u590d\u6742\u5ea6\u548c\u6700\u574f\u60c5\u51b5\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u5f3a\u5206\u79bb\u3002", "motivation": "\u7814\u7a76\u5206\u5e03\u5f0f\u5bf9\u79f0\u6027\u7834\u574f\u95ee\u9898\u4e2d\u7684\u5c40\u90e8\u6027\uff0c\u7279\u522b\u662f\u6b63\u5219\u56fe\u4e2d\u5339\u914d\u95ee\u9898\u7684\u5c40\u90e8\u6027\uff0c\u8fd9\u662f\u5efa\u7acb\u5bf9\u79f0\u6027\u7834\u574f\u95ee\u9898\u5c40\u90e8\u6027\u4e0b\u754c\u548c\u5206\u7c7b\u7ed3\u679c\u7684\u4e3b\u8981\u57fa\u51c6\u3002", "method": "\u5f00\u53d1\u4e86\u968f\u673a\u5316\u7b97\u6cd5\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u9785\u7684\u65b0\u9896\u5206\u6790\u65b9\u6cd5\u6765\u5206\u6790Luby\u5df2\u670940\u5e74\u5386\u53f2\u7684\u7b97\u6cd5\uff0c\u8bc1\u660e\u5728\u0394-\u6b63\u5219\u56fe\u7684\u7ebf\u56fe\u4e0a\u5e94\u7528\u4e00\u8f6eLuby\u7b97\u6cd5\u4f1a\u4ea7\u751f\u51e0\u4e4e\u0394/2-\u6b63\u5219\u7684\u56fe\u3002", "result": "\u8bc1\u660e\u4e86(1+\u03b5)-\u8fd1\u4f3c\u5339\u914d\u5728\u6b63\u5219\u56fe\u4e2d\u662f\u771f\u6b63\u5c40\u90e8\u7684\uff0c\u5c40\u90e8\u6027\u4ec5\u4f9d\u8d56\u4e8e\u03b5\uff1b\u5f53\u0394\u2265poly(1/\u03b5)\u65f6\uff0c\u8fd9\u79cd\u4f9d\u8d56\u5173\u7cfb\u4ec5\u4e3a1/\u03b5\u7684\u5bf9\u6570\uff1b\u6700\u5927\u5339\u914d\u7684\u8282\u70b9\u5e73\u5747\u590d\u6742\u5ea6\u4ec5\u4e3aO(1)\u3002", "conclusion": "\u6b63\u5219\u56fe\u4e2d\u7684\u8fd1\u4f3c\u5339\u914d\u5177\u6709\u771f\u6b63\u5c40\u90e8\u6027\uff0c\u800c\u6700\u5927\u5339\u914d\u5728\u8282\u70b9\u5e73\u5747\u590d\u6742\u5ea6\u548c\u6700\u574f\u60c5\u51b5\u590d\u6742\u5ea6\u4e4b\u95f4\u5b58\u5728\u5f3a\u5206\u79bb\uff0c\u8fd9\u4e3a\u5206\u5e03\u5f0f\u5bf9\u79f0\u6027\u7834\u574f\u95ee\u9898\u7684\u5c40\u90e8\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2510.19225", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19225", "abs": "https://arxiv.org/abs/2510.19225", "authors": ["Yongji Wu", "Xueshen Liu", "Haizhong Zheng", "Juncheng Gu", "Beidi Chen", "Z. Morley Mao", "Arvind Krishnamurthy", "Ion Stoica"], "title": "RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs", "comment": null, "summary": "Reinforcement learning (RL) has become essential for unlocking advanced\nreasoning capabilities in large language models (LLMs). RL workflows involve\ninterleaving rollout and training stages with fundamentally different resource\nrequirements. Rollout typically dominates overall execution time, yet scales\nefficiently through multiple independent instances. In contrast, training\nrequires tightly-coupled GPUs with full-mesh communication. Existing RL\nframeworks fall into two categories: co-located and disaggregated\narchitectures. Co-located ones fail to address this resource tension by forcing\nboth stages to share the same GPUs. Disaggregated architectures, without\nmodifications of well-established RL algorithms, suffer from resource\nunder-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances\non public clouds and spare capacity in production clusters, present significant\ncost-saving opportunities for accelerating RL workflows, if efficiently\nharvested for rollout.\n  In this paper, we present RLBoost, a systematic solution for cost-efficient\nRL training that harvests preemptible GPU resources. Our key insight is that\nrollout's stateless and embarrassingly parallel nature aligns perfectly with\npreemptible and often fragmented resources. To efficiently utilize these\nresources despite frequent and unpredictable availability changes, RLBoost\nadopts a hybrid architecture with three key techniques: (1) adaptive rollout\noffload to dynamically adjust workloads on the reserved (on-demand) cluster,\n(2) pull-based weight transfer that quickly provisions newly available\ninstances, and (3) token-level response collection and migration for efficient\npreemption handling and continuous load balancing. Extensive experiments show\nRLBoost increases training throughput by 1.51x-1.97x while improving cost\nefficiency by 28%-49% compared to using only on-demand GPU resources.", "AI": {"tldr": "RLBoost\u662f\u4e00\u4e2a\u5229\u7528\u53ef\u62a2\u5360GPU\u8d44\u6e90\u8fdb\u884c\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df7\u5408\u67b6\u6784\u548c\u4e09\u9879\u5173\u952e\u6280\u672f\uff0c\u5728\u63d0\u5347\u8bad\u7ec3\u541e\u5410\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u73b0\u6709RL\u6846\u67b6\u5728\u8d44\u6e90\u5229\u7528\u4e0a\u5b58\u5728\u77db\u76fe\uff1arollout\u9636\u6bb5\u9700\u8981\u5927\u91cf\u72ec\u7acbGPU\u5b9e\u4f8b\uff0c\u800c\u8bad\u7ec3\u9636\u6bb5\u9700\u8981\u7d27\u5bc6\u8026\u5408\u7684GPU\u3002\u540c\u65f6\uff0c\u4e91\u4e0a\u7684\u53ef\u62a2\u5360GPU\u8d44\u6e90\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6210\u672c\u8282\u7701\u673a\u4f1a\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u6548\u5229\u7528\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6df7\u5408\u67b6\u6784\uff0c\u5305\u542b\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a(1)\u81ea\u9002\u5e94rollout\u5378\u8f7d\u52a8\u6001\u8c03\u6574\u5de5\u4f5c\u8d1f\u8f7d\uff1b(2)\u57fa\u4e8e\u62c9\u53d6\u7684\u6743\u91cd\u4f20\u8f93\u5feb\u901f\u914d\u7f6e\u65b0\u5b9e\u4f8b\uff1b(3)\u4ee4\u724c\u7ea7\u54cd\u5e94\u6536\u96c6\u548c\u8fc1\u79fb\u5b9e\u73b0\u9ad8\u6548\u62a2\u5360\u5904\u7406\u548c\u8d1f\u8f7d\u5747\u8861\u3002", "result": "\u5b9e\u9a8c\u663e\u793aRLBoost\u5c06\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u53471.51x-1.97x\uff0c\u540c\u65f6\u5c06\u6210\u672c\u6548\u7387\u63d0\u9ad828%-49%\u3002", "conclusion": "RLBoost\u901a\u8fc7\u6709\u6548\u5229\u7528\u53ef\u62a2\u5360GPU\u8d44\u6e90\uff0c\u89e3\u51b3\u4e86RL\u8bad\u7ec3\u4e2d\u7684\u8d44\u6e90\u5229\u7528\u77db\u76fe\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\u63d0\u5347\u3002"}}
{"id": "2510.19262", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.19262", "abs": "https://arxiv.org/abs/2510.19262", "authors": ["Heng Xu", "Zhiwei Yu", "Chengze Du", "Ying Zhou", "Letian Li", "Haojie Wang", "Weiqiang Cheng", "Jialong Li"], "title": "RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training", "comment": null, "summary": "Training Mixture-of-Experts (MoE) models introduces sparse and highly\nimbalanced all-to-all communication that dominates iteration time. Conventional\nload-balancing methods fail to exploit the deterministic topology of Rail\narchitectures, leaving multi-NIC bandwidth underutilized. We present RailS, a\ndistributed load-balancing framework that minimizes all-to-all completion time\nin MoE training. RailS leverages the Rail topology's symmetry to prove that\nuniform sending ensures uniform receiving, transforming global coordination\ninto local scheduling. Each node independently executes a Longest Processing\nTime First (LPT) spraying scheduler to proactively balance traffic using local\ninformation. RailS activates N parallel rails for fine-grained, topology-aware\nmultipath transmission. Across synthetic and real-world MoE workloads, RailS\nimproves bus bandwidth by 20%--78% and reduces completion time by 17%--78%. For\nMixtral workloads, it shortens iteration time by 18%--40% and achieves\nnear-optimal load balance, fully exploiting architectural parallelism in\ndistributed training.", "AI": {"tldr": "RailS\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u8d1f\u8f7d\u5747\u8861\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528Rail\u67b6\u6784\u7684\u62d3\u6251\u5bf9\u79f0\u6027\uff0c\u5c06\u5168\u5c40\u534f\u8c03\u8f6c\u5316\u4e3a\u672c\u5730\u8c03\u5ea6\uff0c\u4f7f\u7528LPT\u55b7\u6d12\u8c03\u5ea6\u5668\u4e3b\u52a8\u5e73\u8861\u6d41\u91cf\uff0c\u663e\u8457\u63d0\u5347MoE\u8bad\u7ec3\u4e2d\u7684all-to-all\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "MoE\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u7a00\u758f\u4e14\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684all-to-all\u901a\u4fe1\u4e3b\u5bfc\u4e86\u8fed\u4ee3\u65f6\u95f4\uff0c\u4f20\u7edf\u8d1f\u8f7d\u5747\u8861\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528Rail\u67b6\u6784\u7684\u591aNIC\u5e26\u5bbd\u3002", "method": "RailS\u5229\u7528Rail\u62d3\u6251\u7684\u5bf9\u79f0\u6027\u8bc1\u660e\u5747\u5300\u53d1\u9001\u786e\u4fdd\u5747\u5300\u63a5\u6536\uff0c\u6bcf\u4e2a\u8282\u70b9\u72ec\u7acb\u6267\u884cLPT\u55b7\u6d12\u8c03\u5ea6\u5668\u8fdb\u884c\u672c\u5730\u8c03\u5ea6\uff0c\u6fc0\u6d3bN\u6761\u5e76\u884crail\u8fdb\u884c\u7ec6\u7c92\u5ea6\u3001\u62d3\u6251\u611f\u77e5\u7684\u591a\u8def\u5f84\u4f20\u8f93\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9eMoE\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\uff0cRailS\u5c06\u603b\u7ebf\u5e26\u5bbd\u63d0\u534720%-78%\uff0c\u5b8c\u6210\u65f6\u95f4\u51cf\u5c1117%-78%\u3002\u5bf9\u4e8eMixtral\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u8fed\u4ee3\u65f6\u95f4\u7f29\u77ed18%-40%\uff0c\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u8d1f\u8f7d\u5747\u8861\u3002", "conclusion": "RailS\u80fd\u591f\u5145\u5206\u5229\u7528\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u67b6\u6784\u5e76\u884c\u6027\uff0c\u663e\u8457\u63d0\u5347MoE\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2510.19301", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.19301", "abs": "https://arxiv.org/abs/2510.19301", "authors": ["Ziheng Deng", "Xue Liu", "Jiantong Jiang", "Yankai Li", "Qingxu Deng", "Xiaochun Yang"], "title": "FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems", "comment": "Accepted for ICDE 2026", "summary": "The Viterbi algorithm is a key operator for structured sequence inference in\nmodern data systems, with applications in trajectory analysis, online\nrecommendation, and speech recognition. As these workloads increasingly migrate\nto resource-constrained edge platforms, standard Viterbi decoding remains\nmemory-intensive and computationally inflexible. Existing methods typically\ntrade decoding time for space efficiency, but often incur significant runtime\noverhead and lack adaptability to various system constraints. This paper\npresents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly\nViterbi decoding operator that enhances adaptability and resource efficiency.\nFLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning\nand parallelization techniques to enhance both time and memory efficiency,\nmaking it well-suited for resource-constrained data systems.To further decouple\nspace complexity from the hidden state space size, we present FLASH-BS Viterbi,\na dynamic beam search variant built on a memory-efficient data structure. Both\nproposed algorithms exhibit strong adaptivity to diverse deployment scenarios\nby dynamically tuning internal parameters.To ensure practical deployment on\nedge devices, we also develop FPGA-based hardware accelerators for both\nalgorithms, demonstrating high throughput and low resource usage. Extensive\nexperiments show that our algorithms consistently outperform existing baselines\nin both decoding time and memory efficiency, while preserving adaptability and\nhardware-friendly characteristics essential for modern data systems. All codes\nare publicly available at https://github.com/Dzh-16/FLASH-Viterbi.", "AI": {"tldr": "FLASH Viterbi\u662f\u4e00\u79cd\u5feb\u901f\u3001\u8f7b\u91cf\u7ea7\u3001\u81ea\u9002\u5e94\u4e14\u786c\u4ef6\u53cb\u597d\u7684Viterbi\u89e3\u7801\u7b97\u5b50\uff0c\u901a\u8fc7\u975e\u9012\u5f52\u5206\u6cbb\u7b56\u7565\u3001\u526a\u679d\u548c\u5e76\u884c\u5316\u6280\u672f\u63d0\u5347\u65f6\u95f4\u548c\u5185\u5b58\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u6570\u636e\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u5de5\u4f5c\u8d1f\u8f7d\u8fc1\u79fb\u5230\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u5e73\u53f0\uff0c\u6807\u51c6Viterbi\u89e3\u7801\u4ecd\u7136\u5185\u5b58\u5bc6\u96c6\u4e14\u8ba1\u7b97\u4e0d\u7075\u6d3b\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5728\u89e3\u7801\u65f6\u95f4\u548c\u7a7a\u95f4\u6548\u7387\u4e4b\u95f4\u6743\u8861\uff0c\u4f46\u5b58\u5728\u663e\u8457\u7684\u8fd0\u884c\u65f6\u5f00\u9500\u548c\u7f3a\u4e4f\u9002\u5e94\u6027\u3002", "method": "\u7ed3\u5408\u975e\u9012\u5f52\u5206\u6cbb\u7b56\u7565\u4e0e\u526a\u679d\u548c\u5e76\u884c\u5316\u6280\u672f\uff0c\u63d0\u51faFLASH-BS Viterbi\u53d8\u4f53\u4f7f\u7528\u5185\u5b58\u9ad8\u6548\u6570\u636e\u7ed3\u6784\u89e3\u8026\u7a7a\u95f4\u590d\u6742\u5ea6\u4e0e\u9690\u85cf\u72b6\u6001\u7a7a\u95f4\u5927\u5c0f\uff0c\u4e24\u79cd\u7b97\u6cd5\u90fd\u80fd\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5185\u90e8\u53c2\u6570\u9002\u5e94\u4e0d\u540c\u90e8\u7f72\u573a\u666f\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u89e3\u7801\u65f6\u95f4\u548c\u5185\u5b58\u6548\u7387\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u9002\u5e94\u6027\u548c\u786c\u4ef6\u53cb\u597d\u7279\u6027\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8eFPGA\u7684\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u5c55\u793a\u9ad8\u541e\u5410\u91cf\u548c\u4f4e\u8d44\u6e90\u4f7f\u7528\u3002", "conclusion": "FLASH Viterbi\u7b97\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u6570\u636e\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u4e14\u786c\u4ef6\u53cb\u597d\u7684Viterbi\u89e3\u7801\u89e3\u51b3\u65b9\u6848\uff0c\u6240\u6709\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2510.19470", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19470", "abs": "https://arxiv.org/abs/2510.19470", "authors": ["Weihao Yang", "Hao Huang", "Donglei Wu", "Ningke Li", "Yanqi Pan", "Qiyang Zheng", "Wen Xia", "Shiyi Li", "Qiang Wang"], "title": "HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission", "comment": null, "summary": "Mixture-of-Experts (MoE) has become a popular architecture for scaling large\nmodels. However, the rapidly growing scale outpaces model training on a single\nDC, driving a shift toward a more flexible, cross-DC training paradigm. Under\nthis, Expert Parallelism (EP) of MoE faces significant scalability issues due\nto the limited cross-DC bandwidth. Specifically, existing EP optimizations\nattempt to overlap data communication and computation, which has little benefit\nin low-bandwidth scenarios due to a much longer data communication time.\nTherefore, the trends of cross-DC EP scaling is fast becoming a critical\nroadblock to the continued growth of MoE models.\n  To address this, we propose HybridEP, a modeling-guided framework to optimize\nEP under constrained bandwidth. Our key idea is to dynamically transform the\nspatial placement of experts to reduce data communication traffic and\nfrequency, thereby minimizing EP's communication overheads. However, it is\nnon-trivial to find the optimal solution because it complicates the original\ncommunication pattern by mixing data and expert communication. We therefore\nbuild a stream-based model to determine the optimal transmission ratio. Guided\nby this, we incorporate two techniques: (1) domain-based partition to construct\nthe mapping between hybrid patterns and specific communication topology at GPU\nlevel, and (2) parameter-efficient migration to further refine this topology by\nreducing expert transmission overhead and enlarging the domain size. Combining\nall these designs, HybridEP can be considered as a more general EP with better\nscalability. Experimental results show that HybridEP outperforms existing\nstate-of-the-art MoE training systems by up to 5.6x under constrained\nbandwidth. We further compare HybridEP and EP on large-scale simulations.\nHybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths.", "AI": {"tldr": "HybridEP\u662f\u4e00\u4e2a\u9488\u5bf9\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u8de8\u6570\u636e\u4e2d\u5fc3\u8bad\u7ec3\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4e13\u5bb6\u653e\u7f6e\u4f4d\u7f6e\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u5728\u53d7\u9650\u5e26\u5bbd\u4e0b\u6027\u80fd\u63d0\u5347\u53ef\u8fbe5.6\u500d", "motivation": "\u968f\u7740\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u89c4\u6a21\u5feb\u901f\u589e\u957f\uff0c\u5355\u6570\u636e\u4e2d\u5fc3\u8bad\u7ec3\u5df2\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u8f6c\u5411\u8de8\u6570\u636e\u4e2d\u5fc3\u8bad\u7ec3\u3002\u4f46\u5728\u4f4e\u5e26\u5bbd\u73af\u5883\u4e0b\uff0c\u4e13\u5bb6\u5e76\u884c\u9762\u4e34\u4e25\u91cd\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u73b0\u6709\u91cd\u53e0\u901a\u4fe1\u548c\u8ba1\u7b97\u7684\u4f18\u5316\u65b9\u6cd5\u6548\u679c\u6709\u9650", "method": "\u63d0\u51faHybridEP\u6846\u67b6\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u52a8\u6001\u8f6c\u6362\u4e13\u5bb6\u7684\u7a7a\u95f4\u653e\u7f6e\u4ee5\u51cf\u5c11\u901a\u4fe1\u6d41\u91cf\u548c\u9891\u7387\u3002\u6784\u5efa\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\u786e\u5b9a\u6700\u4f18\u4f20\u8f93\u6bd4\u4f8b\uff0c\u7ed3\u5408\u57df\u57fa\u5206\u533a\u548c\u53c2\u6570\u9ad8\u6548\u8fc1\u79fb\u6280\u672f\u4f18\u5316GPU\u7ea7\u901a\u4fe1\u62d3\u6251", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aHybridEP\u5728\u53d7\u9650\u5e26\u5bbd\u4e0b\u6bd4\u73b0\u6709MoE\u8bad\u7ec3\u7cfb\u7edf\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe5.6\u500d\u3002\u5927\u89c4\u6a21\u4eff\u771f\u4e2d\uff0c\u57281000\u4e2a\u6570\u636e\u4e2d\u5fc3\u4e0b\u4e0d\u540c\u5e26\u5bbd\u6761\u4ef6\u4e0b\u5b9e\u73b01.45\u500d\u52a0\u901f", "conclusion": "HybridEP\u53ef\u89c6\u4e3a\u5177\u6709\u66f4\u597d\u53ef\u6269\u5c55\u6027\u7684\u901a\u7528\u4e13\u5bb6\u5e76\u884c\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6570\u636e\u4e2d\u5fc3\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898"}}
{"id": "2510.19617", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.19617", "abs": "https://arxiv.org/abs/2510.19617", "authors": ["Eric Ding"], "title": "Propius: A Platform for Collaborative Machine Learning across the Edge and the Cloud", "comment": null, "summary": "Collaborative Machine Learning is a paradigm in the field of distributed\nmachine learning, designed to address the challenges of data privacy,\ncommunication overhead, and model heterogeneity. There have been significant\nadvancements in optimization and communication algorithm design and ML hardware\nthat enables fair, efficient and secure collaborative ML training. However,\nless emphasis is put on collaborative ML infrastructure development. Developers\nand researchers often build server-client systems for a specific collaborative\nML use case, which is not scalable and reusable. As the scale of collaborative\nML grows, the need for a scalable, efficient, and ideally multi-tenant resource\nmanagement system becomes more pressing. We propose a novel system, Propius,\nthat can adapt to the heterogeneity of client machines, and efficiently manage\nand control the computation flow between ML jobs and edge resources in a\nscalable fashion. Propius is comprised of a control plane and a data plane. The\ncontrol plane enables efficient resource sharing among multiple collaborative\nML jobs and supports various resource sharing policies, while the data plane\nimproves the scalability of collaborative ML model sharing and result\ncollection. Evaluations show that Propius outperforms existing resource\nmanagement techniques and frameworks in terms of resource utilization (up to\n$1.88\\times$), throughput (up to $2.76$), and job completion time (up to\n$1.26\\times$).", "AI": {"tldr": "Propius\u662f\u4e00\u4e2a\u7528\u4e8e\u534f\u4f5c\u673a\u5668\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u8d44\u6e90\u7ba1\u7406\u7cfb\u7edf\uff0c\u5305\u542b\u63a7\u5236\u5e73\u9762\u548c\u6570\u636e\u5e73\u9762\uff0c\u80fd\u591f\u9002\u5e94\u5ba2\u6237\u7aef\u673a\u5668\u7684\u5f02\u6784\u6027\uff0c\u5e76\u9ad8\u6548\u7ba1\u7406ML\u4f5c\u4e1a\u4e0e\u8fb9\u7f18\u8d44\u6e90\u4e4b\u95f4\u7684\u8ba1\u7b97\u6d41\u3002", "motivation": "\u534f\u4f5c\u673a\u5668\u5b66\u4e60\u9762\u4e34\u6570\u636e\u9690\u79c1\u3001\u901a\u4fe1\u5f00\u9500\u548c\u6a21\u578b\u5f02\u6784\u6027\u7b49\u6311\u6218\uff0c\u73b0\u6709\u7cfb\u7edf\u591a\u4e3a\u7279\u5b9a\u7528\u4f8b\u6784\u5efa\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u53ef\u590d\u7528\u6027\u3002\u968f\u7740\u534f\u4f5cML\u89c4\u6a21\u6269\u5927\uff0c\u9700\u8981\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u652f\u6301\u591a\u79df\u6237\u7684\u8d44\u6e90\u7ba1\u7406\u7cfb\u7edf\u3002", "method": "Propius\u7cfb\u7edf\u7531\u63a7\u5236\u5e73\u9762\u548c\u6570\u636e\u5e73\u9762\u7ec4\u6210\u3002\u63a7\u5236\u5e73\u9762\u652f\u6301\u591a\u4e2a\u534f\u4f5cML\u4f5c\u4e1a\u4e4b\u95f4\u7684\u9ad8\u6548\u8d44\u6e90\u5171\u4eab\u548c\u591a\u79cd\u8d44\u6e90\u5171\u4eab\u7b56\u7565\uff1b\u6570\u636e\u5e73\u9762\u63d0\u9ad8\u534f\u4f5cML\u6a21\u578b\u5171\u4eab\u548c\u7ed3\u679c\u6536\u96c6\u7684\u53ef\u6269\u5c55\u6027\u3002", "result": "\u8bc4\u4f30\u663e\u793aPropius\u5728\u8d44\u6e90\u5229\u7528\u7387\uff08\u6700\u9ad81.88\u500d\uff09\u3001\u541e\u5410\u91cf\uff08\u6700\u9ad82.76\u500d\uff09\u548c\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\uff08\u6700\u9ad81.26\u500d\uff09\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u8d44\u6e90\u7ba1\u7406\u6280\u672f\u3002", "conclusion": "Propius\u662f\u4e00\u4e2a\u80fd\u591f\u9002\u5e94\u5ba2\u6237\u7aef\u5f02\u6784\u6027\u3001\u9ad8\u6548\u7ba1\u7406\u8ba1\u7b97\u6d41\u5e76\u652f\u6301\u591a\u79df\u6237\u8d44\u6e90\u5171\u4eab\u7684\u534f\u4f5cML\u8d44\u6e90\u7ba1\u7406\u7cfb\u7edf\uff0c\u5728\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.19689", "categories": ["cs.DC", "cs.AI", "cs.LG", "C.2.4; H.3.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.19689", "abs": "https://arxiv.org/abs/2510.19689", "authors": ["Guilin Zhang", "Wulan Guo", "Ziqi Tan", "Srinivas Vippagunta", "Suchitra Raman", "Shreeshankar Chatterjee", "Ju Lin", "Shang Liu", "Mary Schladenhauffen", "Jeffrey Luo", "Hailong Jiang"], "title": "Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation", "comment": "10 pages, 7 figures, 4 tables. Accepted to IEEE BigData 2025", "summary": "Industrial and government organizations increasingly depend on data-driven\nanalytics for workforce, finance, and regulated decision processes, where\ntimeliness, cost efficiency, and compliance are critical. Distributed\nframeworks such as Spark and Flink remain effective for massive-scale batch or\nstreaming analytics but introduce coordination complexity and auditing\noverheads that misalign with moderate-scale, latency-sensitive inference.\nMeanwhile, cloud providers now offer serverless GPUs, and models such as TabNet\nenable interpretable tabular ML, motivating new deployment blueprints for\nregulated environments. In this paper, we present a production-oriented Big\nData as a Service (BDaaS) blueprint that integrates a single-node serverless\nGPU runtime with TabNet. The design leverages GPU acceleration for throughput,\nserverless elasticity for cost reduction, and feature-mask interpretability for\nIL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,\ncomparing our approach against Spark and CPU baselines. Our results show that\nGPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%\nlower cost per 1K inferences compared to Spark baselines, while compliance\nmechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains\nstable under peak load, ensuring reliable auditability. Taken together, these\nfindings provide a compliance-aware benchmark, a reproducible Helm-packaged\nblueprint, and a decision framework that demonstrate the practicality of\nsecure, interpretable, and cost-efficient serverless GPU analytics for\nregulated enterprise and government settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u751f\u4ea7\u7684BDaaS\u84dd\u56fe\uff0c\u96c6\u6210\u5355\u8282\u70b9\u65e0\u670d\u52a1\u5668GPU\u8fd0\u884c\u65f6\u4e0eTabNet\uff0c\u5728\u53d7\u76d1\u7ba1\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u6210\u672c\u4e14\u53ef\u89e3\u91ca\u7684\u8868\u683c\u6570\u636e\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u5206\u5e03\u5f0f\u6846\u67b6\u5982Spark\u548cFlink\u5728\u5927\u89c4\u6a21\u5206\u6790\u4e2d\u6709\u6548\uff0c\u4f46\u534f\u8c03\u590d\u6742\u6027\u548c\u5ba1\u8ba1\u5f00\u9500\u4e0d\u9002\u5408\u4e2d\u7b49\u89c4\u6a21\u3001\u5ef6\u8fdf\u654f\u611f\u7684\u63a8\u7406\u573a\u666f\u3002\u65e0\u670d\u52a1\u5668GPU\u548c\u53ef\u89e3\u91ca\u6a21\u578b\u5982TabNet\u4e3a\u53d7\u76d1\u7ba1\u73af\u5883\u63d0\u4f9b\u4e86\u65b0\u7684\u90e8\u7f72\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u96c6\u6210\u5355\u8282\u70b9\u65e0\u670d\u52a1\u5668GPU\u8fd0\u884c\u65f6\u4e0eTabNet\u7684BDaaS\u84dd\u56fe\uff0c\u5229\u7528GPU\u52a0\u901f\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u65e0\u670d\u52a1\u5668\u5f39\u6027\u964d\u4f4e\u6210\u672c\uff0c\u7279\u5f81\u63a9\u7801\u53ef\u89e3\u91ca\u6027\u6ee1\u8db3\u5408\u89c4\u8981\u6c42\u3002", "result": "\u5728HR\u3001Adult\u548cBLS\u6570\u636e\u96c6\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0cGPU\u6d41\u6c34\u7ebf\u76f8\u6bd4Spark\u57fa\u7ebf\u5b9e\u73b04.5\u500d\u541e\u5410\u91cf\u63d0\u5347\u300198\u500d\u5ef6\u8fdf\u964d\u4f4e\u548c90%\u6210\u672c\u964d\u4f4e\uff0c\u5408\u89c4\u673a\u5236\u4ec5\u589e\u52a0\u7ea65.7ms\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u53d7\u76d1\u7ba1\u7684\u4f01\u4e1a\u548c\u653f\u5e9c\u73af\u5883\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u53ef\u89e3\u91ca\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u65e0\u670d\u52a1\u5668GPU\u5206\u6790\u5b9e\u8df5\u65b9\u6848\uff0c\u5305\u542b\u5408\u89c4\u57fa\u51c6\u3001\u53ef\u590d\u73b0\u84dd\u56fe\u548c\u51b3\u7b56\u6846\u67b6\u3002"}}
{"id": "2510.19725", "categories": ["cs.DC", "cs.NI", "C.2.m; G.2.m"], "pdf": "https://arxiv.org/pdf/2510.19725", "abs": "https://arxiv.org/abs/2510.19725", "authors": ["Jingfan Meng", "Tianji Yang", "Jun Xu"], "title": "CommonSense: Efficient Set Intersection (SetX) Protocol Based on Compressed Sensing", "comment": null, "summary": "In the set reconciliation (\\textsf{SetR}) problem, two parties Alice and Bob,\nholding sets $\\mathsf{A}$ and $\\mathsf{B}$, communicate to learn the symmetric\ndifference $\\mathsf{A} \\Delta \\mathsf{B}$. In this work, we study a related but\nunder-explored problem: set intersection (\\textsf{SetX})~\\cite{Ozisik2019},\nwhere both parties learn $\\mathsf{A} \\cap \\mathsf{B}$ instead. However,\nexisting solutions typically reuse \\textsf{SetR} protocols due to the absence\nof dedicated \\textsf{SetX} protocols and the misconception that \\textsf{SetR}\nand \\textsf{SetX} have comparable costs. Observing that \\textsf{SetX} is\nfundamentally cheaper than \\textsf{SetR}, we developed a multi-round\n\\textsf{SetX} protocol that outperforms the information-theoretic lower bound\nof \\textsf{SetR} problem. In our \\textsf{SetX} protocol, Alice sends Bob a\ncompressed sensing (CS) sketch of $\\mathsf{A}$ to help Bob identify his unique\nelements (those in $\\mathsf{B \\setminus A}$). This solves the \\textsf{SetX}\nproblem, if $\\mathsf{A} \\subseteq \\mathsf{B}$. Otherwise, Bob sends a CS sketch\nof the residue (a set of elements he cannot decode) back to Alice for her to\ndecode her unique elements (those in $\\mathsf{A \\setminus B}$). As such, Alice\nand Bob communicate back and forth %with a set membership filter (SMF) of\nestimated $\\mathsf{B \\setminus A}$. Alice updates $\\mathsf{A}$ and\ncommunication repeats until both parties agrees on $\\mathsf{A} \\cap\n\\mathsf{B}$. On real world datasets, experiments show that our $\\mathsf{SetX}$\nprotocol reduces the communication cost by 8 to 10 times compared to the\nIBLT-based $\\mathsf{SetR}$ protocol.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u8f6e\u96c6\u5408\u4ea4\u96c6\u534f\u8bae(SetX)\uff0c\u901a\u8fc7\u538b\u7f29\u611f\u77e5\u6280\u672f\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u6210\u672c\uff0c\u6bd4\u57fa\u4e8eIBLT\u7684\u96c6\u5408\u534f\u8c03\u534f\u8bae\u51cf\u5c11\u4e868-10\u500d\u7684\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u96c6\u5408\u4ea4\u96c6\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u91cd\u7528\u96c6\u5408\u534f\u8c03\u534f\u8bae\uff0c\u4f46\u5b58\u5728\u8bef\u89e3\u8ba4\u4e3a\u4e24\u8005\u6210\u672c\u76f8\u5f53\u3002\u5b9e\u9645\u4e0a\u96c6\u5408\u4ea4\u96c6\u672c\u8d28\u4e0a\u6bd4\u96c6\u5408\u534f\u8c03\u66f4\u4fbf\u5b9c\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u4f18\u5316\u534f\u8bae\u3002", "method": "\u4f7f\u7528\u591a\u8f6e\u538b\u7f29\u611f\u77e5\u8349\u56fe\u4ea4\u6362\u534f\u8bae\uff1aAlice\u53d1\u9001A\u7684CS\u8349\u56fe\u7ed9Bob\uff0cBob\u8bc6\u522b\u5176\u72ec\u7279\u5143\u7d20\uff1b\u5982\u679cA\u4e0d\u662fB\u7684\u5b50\u96c6\uff0cBob\u5c06\u65e0\u6cd5\u89e3\u7801\u7684\u5143\u7d20\u53d1\u9001\u56deAlice\uff0cAlice\u89e3\u7801\u5176\u72ec\u7279\u5143\u7d20\u3002\u53cc\u65b9\u901a\u8fc7\u8fed\u4ee3\u66f4\u65b0\u76f4\u5230\u8fbe\u6210\u4ea4\u96c6\u5171\u8bc6\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5SetX\u534f\u8bae\u6bd4\u57fa\u4e8eIBLT\u7684SetR\u534f\u8bae\u51cf\u5c11\u4e868-10\u500d\u7684\u901a\u4fe1\u6210\u672c\u3002", "conclusion": "\u96c6\u5408\u4ea4\u96c6\u95ee\u9898\u5728\u901a\u4fe1\u6210\u672c\u4e0a\u6bd4\u96c6\u5408\u534f\u8c03\u95ee\u9898\u66f4\u4fbf\u5b9c\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u538b\u7f29\u611f\u77e5\u591a\u8f6e\u534f\u8bae\u53ef\u4ee5\u663e\u8457\u4f18\u5316\u6027\u80fd\uff0c\u7a81\u7834\u4e86\u96c6\u5408\u534f\u8c03\u95ee\u9898\u7684\u4fe1\u606f\u8bba\u4e0b\u754c\u3002"}}
{"id": "2510.19805", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.19805", "abs": "https://arxiv.org/abs/2510.19805", "authors": ["Carl-Johan Fauvelle Munck af Rosensch\"old", "Feras M. Awaysheh", "Ahmad Awad"], "title": "Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and Beyond", "comment": "10 pages, 5 figures, 2 algorithms, 4 tables", "summary": "In-memory key-value datastores have become indispensable building blocks of\nmodern cloud-native infrastructures, yet their evolution faces scalability,\ncompatibility, and sustainability constraints. The current literature lacks an\nexperimental evaluation of state-of-the-art tools in the domain. This study\naddressed this timely gap by benchmarking Redis alternatives and systematically\nevaluating Valkey, KeyDB, and Garnet under realistic workloads within\nKubernetes deployments. The results demonstrate clear trade-offs among the\nbenchmarked data systems. Our study presents a comprehensive performance and\nviability assessment of the emerging in-memory key-value stores. Metrics\ninclude throughput, tail latency, CPU and memory efficiency, and migration\ncomplexity. We highlight trade-offs between performance, compatibility, and\nlong-term viability, including project maturity, community support, and\nsustained development.", "AI": {"tldr": "\u5bf9Redis\u66ff\u4ee3\u54c1Valkey\u3001KeyDB\u548cGarnet\u5728Kubernetes\u73af\u5883\u4e0b\u7684\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u3001\u8d44\u6e90\u6548\u7387\u548c\u8fc1\u79fb\u590d\u6742\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9\u5185\u5b58\u952e\u503c\u6570\u636e\u5e93\u6700\u65b0\u5de5\u5177\u7684\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u9700\u8981\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u3001\u517c\u5bb9\u6027\u548c\u53ef\u6301\u7eed\u6027\u9650\u5236\u3002", "method": "\u5728Kubernetes\u90e8\u7f72\u4e2d\u4f7f\u7528\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u5bf9Valkey\u3001KeyDB\u548cGarnet\u8fdb\u884c\u7cfb\u7edf\u5316\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7ed3\u679c\u663e\u793a\u88ab\u6d4b\u8bd5\u7684\u6570\u636e\u7cfb\u7edf\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u7684\u6743\u8861\u53d6\u820d\uff0c\u5305\u62ec\u6027\u80fd\u3001\u517c\u5bb9\u6027\u548c\u957f\u671f\u53ef\u884c\u6027\u65b9\u9762\u7684\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5174\u5185\u5b58\u952e\u503c\u5b58\u50a8\u7684\u5168\u9762\u6027\u80fd\u548c\u53ef\u884c\u6027\u8bc4\u4f30\uff0c\u5f3a\u8c03\u4e86\u6027\u80fd\u3001\u517c\u5bb9\u6027\u548c\u9879\u76ee\u6210\u719f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
