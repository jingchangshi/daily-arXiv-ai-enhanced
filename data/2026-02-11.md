<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [ALPHA-PIM: Analysis of Linear Algebraic Processing for High-Performance Graph Applications on a Real Processing-In-Memory System](https://arxiv.org/abs/2602.09174)
*Marzieh Barkhordar,Alireza Tabatabaeian,Mohammad Sadrosadati,Christina Giannoula,Juan Gomez Luna,Izzat El Hajj,Onur Mutlu,Alaa R. Alameldeen*

Main category: cs.DC

TL;DR: 本文在UPMEM的真实PIM系统上实现并评估了图算法，发现数据分区策略对性能至关重要，并识别了当前PIM架构的硬件限制，提出了未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 大规模图数据处理计算密集且耗时，传统的CPU/GPU架构因数据移动量大而面临内存瓶颈。虽然存内计算(PIM)能缓解数据移动问题，但之前的研究多基于定制PIM方案，未利用真实PIM系统。

Method: 在UPMEM通用PIM架构上实现代表性图算法，分析其性能特征和瓶颈，与CPU/GPU基准进行比较，并推导对未来PIM硬件设计的指导见解。

Result: 研究发现选择最优的数据分区策略对最大化PIM性能至关重要，识别了当前PIM架构在计算、内存和通信子系统方面的关键硬件限制。

Conclusion: 未来PIM硬件需要在指令级并行性、改进的DMA引擎（支持非阻塞操作）以及PIM核心间的直接互连网络等方面进行增强，以减少数据传输开销。

Abstract: Processing large-scale graph datasets is computationally intensive and time-consuming. Processor-centric CPU and GPU architectures, commonly used for graph applications, often face bottlenecks caused by extensive data movement between the processor and memory units due to low data reuse. As a result, these applications are often memory-bound, limiting both performance and energy efficiency due to excessive data transfers. Processing-In-Memory (PIM) offers a promising approach to mitigate data movement bottlenecks by integrating computation directly within or near memory. Although several previous studies have introduced custom PIM proposals for graph processing, they do not leverage real-world PIM systems.
  This work aims to explore the capabilities and characteristics of common graph algorithms on a real-world PIM system to accelerate data-intensive graph workloads. To this end, we (1) implement representative graph algorithms on UPMEM's general-purpose PIM architecture; (2) characterize their performance and identify key bottlenecks; (3) compare results against CPU and GPU baselines; and (4) derive insights to guide future PIM hardware design.
  Our study underscores the importance of selecting optimal data partitioning strategies across PIM cores to maximize performance. Additionally, we identify critical hardware limitations in current PIM architectures and emphasize the need for future enhancements across computation, memory, and communication subsystems. Key opportunities for improvement include increasing instruction-level parallelism, developing improved DMA engines with non-blocking capabilities, and enabling direct interconnection networks among PIM cores to reduce data transfer overheads.

</details>


### [2] [LLM-CoOpt: A Co-Design and Optimization Framework for Efficient LLM Inference on Heterogeneous Platforms](https://arxiv.org/abs/2602.09323)
*Jie Kong,Wei Wang,Jiehan Zhou,Chen Yu*

Main category: cs.DC

TL;DR: LLM-CoOpt是一个算法-硬件协同设计框架，通过优化KV缓存、分组查询注意力和分页注意力，提升LLM推理的吞吐量和降低延迟，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理面临内存带宽瓶颈、计算冗余和长序列处理效率低下等主要挑战，需要综合优化方案来提升实际部署性能。

Method: 提出三合一优化策略：1) Opt-KV优化KV缓存读写路径并引入FP8量化；2) Opt-GQA将多头自注意力重构为分组查询注意力；3) Opt-Pa采用两步策略处理长序列，包括分块和惰性内存映射。

Result: 在LLaMa-13B-GPTQ模型上，推理吞吐量提升最高13.43%，延迟降低最高16.79%，同时保持模型精度。

Conclusion: LLM-CoOpt为大规模语言模型的实际推理提供了一条实用、高性能的优化路径，有效解决了内存带宽瓶颈和计算效率问题。

Abstract: Major challenges in LLMs inference remain frequent memory bandwidth bottlenecks, computational redundancy, and inefficiencies in long-sequence processing. To address these issues, we propose LLM-CoOpt, a comprehensive algorithmhardware co-design framework aimed at improving both throughput and latency in LLM inference. LLM-CoOpt integrates three key strategies: (1) Key-Value Cache Optimization, termed Opt-KV, which improves memory access efficiency by optimizing both KV cache write and read paths, and introduces FP8 quantization to reduce memory footprint while maintaining accuracy; (2) Grouped-Query Attention for Computational Efficiency, termed Opt-GQA, which reduces the overall computational complexity by restructuring multi-head self-attention into grouped-query attention with shared key-value projections, enabling higher throughput and lower resource consumption; (3) Paged Attention for Long- Sequence Processing, termed Opt-Pa, which adopts a two-step strategy to first segment long sequences into manageable chunks and then apply lazy memory mapping and computation, significantly reducing memory pressure and improving performance on long-context inputs.Experiments on the LLaMa-13BGPTQ model demonstrate that LLM-CoOpt increases inference throughput by up to 13.43%, reduces latency by up to 16.79%, and maintains model accuracy. These results confirm that LLM-CoOpt provides a practical, high-performance optimization path for real-world inference of large-scale language models.

</details>


### [3] [The Coordination Criterion](https://arxiv.org/abs/2602.09435)
*Joseph M. Hellerstein*

Main category: cs.DC

TL;DR: 该论文提出了一个协调准则，用于判断分布式规范何时本质上需要协调，而非特定协议或实现策略所强加。准则表明：在异步消息传递模型中，规范允许无协调实现当且仅当其在适当顺序下相对于历史扩展是单调的。


<details>
  <summary>Details</summary>
Motivation: 研究分布式规范何时本质上需要协调（而非特定实现策略所强加），为理解协调的必要性提供理论基础，统一解释CAP、CALM、一致性协议等多种分布式系统现象。

Method: 在异步消息传递模型中，基于Lamport历史（偏序执行）和规范定义的观察结果，提出协调准则：规范允许无协调实现当且仅当其在历史扩展顺序下是单调的。

Result: 建立了协调的明确边界：规范要么可无协调实现，要么协调不可避免。该准则统一解释了CAP不可能性、CALM无协调性、共识任务、快照任务、事务隔离级别和不变式汇合等经典结果。

Conclusion: 协调准则为分布式规范是否需要协调提供了通用判断标准，揭示了多种分布式系统现象背后的统一语义现象，为理解和设计分布式系统提供了理论基础。

Abstract: When is coordination intrinsically required by a distributed specification, rather than imposed by a particular protocol or implementation strategy? We give a general answer using minimal assumptions. In an asynchronous message-passing model, we show that a specification admits a coordination-free implementation if and only if it is monotone with respect to history extension under an appropriate order on observable outcomes.
  This Coordination Criterion is stated directly over Lamport histories -- partially ordered executions under happens-before -- and specification-defined observable outcomes, without assuming any particular programming language, object implementation, or protocol structure. It yields a sharp boundary between specifications that can be implemented without coordination and those for which coordination is unavoidable. The criterion provides a uniform explanation for a range of classical results, including CAP-style impossibility, CALM-style coordination-freedom, agreement and snapshot tasks, transactional isolation levels, and invariant confluence -- all instances of the same underlying semantic phenomenon.

</details>


### [4] [It's not a lie if you don't get caught: simplifying reconfiguration in SMR through dirty logs](https://arxiv.org/abs/2602.09441)
*Allen Clement,Natacha Crooks,Neil Giridharan,Alex Shamis*

Main category: cs.DC

TL;DR: Gauss是一个重新配置引擎，将共识协议视为可互换模块，通过分离内部日志和外部日志，实现独立升级成员资格、故障阈值和共识协议本身，最小化全局停机时间。


<details>
  <summary>Details</summary>
Motivation: 现有生产状态机复制实现复杂且多层，现有研究共识协议很少讨论重新配置，即使讨论也将成员变更与特定算法紧密耦合，这阻碍了独立升级构建块，并在切换到新协议实现时导致昂贵的停机时间。

Method: 提出Gauss重新配置引擎，通过区分共识协议内部日志和暴露给RSM节点的净化外部日志，将共识协议视为可互换模块，实现独立升级成员资格、故障阈值和共识协议本身。

Result: 在Rialo区块链上的初步评估表明，这种关注点分离使得SMR堆栈能够在多种协议实现序列中实现无缝演进。

Conclusion: 模块化对于生产部署的可维护性和系统演进至关重要，Gauss通过将共识协议作为可互换模块处理，实现了独立升级和最小化全局停机时间。

Abstract: Production state-machine replication (SMR) implementations are complex, multi-layered architectures comprising data dissemination, ordering, execution, and reconfiguration components. Existing research consensus protocols rarely discuss reconfiguration. Those that do tightly couple membership changes to a specific algorithm. This prevents the independent upgrade of individual building blocks and forces expensive downtime when transitioning to new protocol implementations. Instead, modularity is essential for maintainability and system evolution in production deployments. We present Gauss, a reconfiguration engine designed to treat consensus protocols as interchangeable modules. By introducing a distinction between a consensus protocol's inner log and a sanitized outer log exposed to the RSM node, Gauss allows engineers to upgrade membership, failure thresholds, and the consensus protocol itself independently and with minimal global downtime. Our initial evaluation on the Rialo blockchain shows that this separation of concerns enables a seamless evolution of the SMR stack across a sequence of diverse protocol implementations.

</details>


### [5] [High-performance Vector-length Agnostic Quantum Circuit Simulations on ARM Processors](https://arxiv.org/abs/2602.09604)
*Ruimin Shi,Gabin Schieffer,Pei-Hung Lin,Maya Gokhale,Andreas Herten,Ivy Peng*

Main category: cs.DC

TL;DR: 该研究探索了在ARM SVE和RISC-V RVV等可变向量长度架构上实现量子态向量模拟的高性能可移植性，提出了VLA设计和优化技术，在三种ARM处理器上实现了显著加速。


<details>
  <summary>Details</summary>
Motivation: 量子态向量模拟是量子计算的重要工作负载，而ARM SVE和RISC-V RVV是新兴的支持可变向量长度的高端处理器架构。研究旨在探索在这些向量长度无关架构上能否实现高性能可移植性。

Method: 提出了VLA设计和关键优化技术：VLEN自适应内存布局调整、加载缓冲、细粒度循环控制、基于门融合的算术强度适配。在Google的Qsim中实现，并在三种ARM处理器（NVIDIA Grace、AWS Graviton3、Fujitsu A64FX）上评估最多36量子比特的五个量子电路。

Result: 定义了新的指标和PMU事件来量化向量化活动，为未来VLA设计提供通用见解。单源VLA量子模拟实现：在A64FX上达到4.5倍加速，在Grace上2.5倍加速，在Graviton上1.5倍加速。

Conclusion: 研究表明在向量长度无关架构上可以实现量子态向量模拟的高性能可移植性，提出的VLA设计和优化技术在不同处理器上都能显著提升性能，为未来VLA架构设计提供了重要见解。

Abstract: ARM SVE and RISC-V RVV are emerging vector architectures in high-end processors that support vectorization of flexible vector length. In this work, we leverage an important workload for quantum computing, quantum state-vector simulations, to understand whether high-performance portability can be achieved in a vector-length agnostic (VLA) design. We propose a VLA design and optimization techniques critical for achieving high performance, including VLEN-adaptive memory layout adjustment, load buffering, fine-grained loop control, and gate fusion-based arithmetic intensity adaptation. We provide an implementation in Google's Qsim and evaluate five quantum circuits of up to 36 qubits on three ARM processors, including NVIDIA Grace, AWS Graviton3, and Fujitsu A64FX. By defining new metrics and PMU events to quantify vectorization activities, we draw generic insights for future VLA designs. Our single-source implementation of VLA quantum simulations achieves up to 4.5x speedup on A64FX, 2.5x speedup on Grace, and 1.5x speedup on Graviton.

</details>


### [6] [Revealing the Challenges of Attention-FFN Disaggregation for Modern MoE Models and Hardware Systems](https://arxiv.org/abs/2602.09721)
*Guowei Liu,Hongming Li,Yaning Guo,Yongxi Lyu,Mo Zhou,Yi Liu,Zhaogeng Li,Yanpeng Wang*

Main category: cs.DC

TL;DR: AFD架构在特定硬件-模型组合下表现良好，但非通用解决方案；标准集群存在死区，增加FFN实例无法提升硬件利用率。


<details>
  <summary>Details</summary>
Motivation: 大规模MoE模型部署面临内存容量和带宽挑战，AFD架构作为解耦计算和内存资源的潜在方案，其性能边界相比标准EP架构尚未充分探索。

Method: 通过将屋顶线模型扩展到通信层面，关联互连带宽、算术强度和硬件浮点运算利用率，对AFD进行系统分析。

Result: 标准集群存在死区：增加FFN实例无法提升HFU，因为计算工作负载受扩展带宽限制；AFD的离散节点级扩展比EP的连续批次调整产生更高不平衡惩罚；但在Superpod级硬件和粗粒度专家、低稀疏度模型下，AFD表现更好。

Conclusion: AFD是特定硬件-模型组合下的有前景方案，而非通用解决方案；需要充足互连带宽和合适的模型特性才能发挥优势。

Abstract: Deploying large-scale MoE models presents challenges in memory capacity and bandwidth for expert activation. While Attention-FFN Disaggregation (AFD) has emerged as a potential architecture to decouple compute and memory resources, its performance boundaries compared to standard large-scale Expert Parallelism (EP) remain underexplored. In this paper, we conduct a systematic analysis of AFD by extending the roofline model to the communication level, correlating interconnect bandwidth, arithmetic intensity, and Hardware FLOPS Utilization (HFU). Our analysis reveals a dead zone on standard clusters: increasing FFN instance count fails to improve HFU as computational workload is capped by scale-out bandwidth, causing operator active time to shrink relative to the fixed latency budget. We further show that AFD's discrete node-level scaling incurs higher imbalance penalties than EP's continuous batch adjustment. Nevertheless, these limitations diminish under specific conditions: Superpod-class hardware with abundant interconnect bandwidth and models with coarse-grained experts and lower sparsity are more likely to benefit from AFD. These findings position AFD as a promising approach for specific hardware-model combinations rather than a universal solution.

</details>


### [7] [Efficient Remote Prefix Fetching with GPU-native Media ASICs](https://arxiv.org/abs/2602.09725)
*Liang Mi,Weijun Wang,Jinghan Chen,Ting Cao,Haipeng Dai,Yunxin Liu*

Main category: cs.DC

TL;DR: KVFetcher：利用GPU原生视频编解码器实现高效的远程KV缓存重用，在带宽受限场景下显著降低首次令牌时间


<details>
  <summary>Details</summary>
Motivation: 远程KV缓存重用通过从远程存储获取相同上下文的KV缓存来避免重复计算，加速LLM推理。但在带宽受限场景下性能显著下降。现有方法通过传输压缩KV缓存来解决，但重量级解压缩抵消了重用优势。

Method: 提出KVFetcher系统，采用两种技术：1）编解码友好的张量布局，将KV缓存压缩为高度紧凑的视频格式实现快速传输；2）高效的KV获取器，以流水线方式协调压缩KV缓存的传输、解码和恢复，消除资源争用，掩盖网络波动。

Result: 在高、中、低端GPU上原型实现，实验显示相比SOTA方法，TTFT最多降低3.51倍，同时保持无损精度。

Conclusion: KVFetcher提供了一种高效且广泛可部署的远程KV缓存重用解决方案，利用GPU原生视频编解码器在带宽受限环境下显著提升LLM推理性能。

Abstract: Remote KV cache reuse fetches KV cache for identical contexts from remote storage, avoiding recomputation, accelerating LLM inference. While it excels in high-speed networks, its performance degrades significantly in bandwidth-limited scenarios. Recent studies address this by transmitting KV caches in compressed form, but the associated heavyweight decompression counteracts the KV reuse benefits. In this paper, we propose an efficient and widely deployable remote KV cache reuse solution that leverages GPU-native video codecs. Our system, KVFetcher, enables effective KV cache coding with two techniques. The codec-friendly tensor layout compresses the KV cache in a highly compact video format, enabling fast transmission. The efficient KV fetcher orchestrates the transmission, decoding, and restoration of compressed KV caches in an efficient pipelined manner, eliminating resource contention, masking network fluctuations, and achieving minimum time-to-first-token (TTFT). We prototype KVFetcher on diverse GPUs from high- to low-end. Experiments reveal that it reduces TTFT by up to 3.51 times while maintaining lossless accuracy, compared to SOTA methods.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [Accelerating Post-Quantum Cryptography via LLM-Driven Hardware-Software Co-Design](https://arxiv.org/abs/2602.09410)
*Yuchao Liao,Tosiron Adegbija,Roman Lysecky*

Main category: cs.AR

TL;DR: LLMs加速后量子密码硬件设计：提出基于LLM的框架，针对FALCON签名方案，实现FPGA加速器自动设计，相比传统HLS方法获得2.6倍加速。


<details>
  <summary>Details</summary>
Motivation: 后量子密码算法计算复杂，硬件实现困难，需要高效的设计方法。传统硬件-软件协同设计过程耗时且复杂，需要自动化工具来加速设计迭代。

Method: 提出基于LLM的框架，利用大语言模型分析PQC算法、识别性能关键组件、生成FPGA硬件描述。采用人机协同方式，对FALCON签名方案的低层计算密集型内核进行LLM驱动合成。

Result: LLM生成的加速器在核心执行时间上比传统HLS方法快2.6倍，关键路径更短。但在资源利用率和功耗方面存在权衡。LLM能显著减少设计工作和开发时间。

Conclusion: LLMs能够自动化PQC算法的FPGA加速器设计迭代，为快速自适应PQC加速器设计提供了有前景的新方向，特别是在FALCON等后量子密码方案的硬件实现中。

Abstract: Post-quantum cryptography (PQC) is crucial for securing data against emerging quantum threats. However, its algorithms are computationally complex and difficult to implement efficiently on hardware. In this paper, we explore the potential of Large Language Models (LLMs) to accelerate the hardware-software co-design process for PQC, with a focus on the FALCON digital signature scheme. We present a novel framework that leverages LLMs to analyze PQC algorithms, identify performance-critical components, and generate candidate hardware descriptions for FPGA implementation. We present the first quantitative comparison between LLM-driven synthesis and conventional HLS-based approaches for low-level compute-intensive kernels in FALCON, showing that human-in-the-loop LLM-generated accelerators can achieve up to 2.6x speedup in kernel execution time with shorter critical paths, while highlighting trade-offs in resource utilization and power consumption. Our results suggest that LLMs can minimize design effort and development time by automating FPGA accelerator design iterations for PQC algorithms, offering a promising new direction for rapid and adaptive PQC accelerator design on FPGAs.

</details>


### [9] [Development of an Energy-Efficient and Real-Time Data Movement Strategy for Next-Generation Heterogeneous Mixed-Criticality Systems](https://arxiv.org/abs/2602.09554)
*Thomas Benz*

Main category: cs.AR

TL;DR: 论文探讨了ACES（自动驾驶、互联、电动化、共享出行）趋势下，工业计算系统面临的计算性能、能效、内存带宽和混合关键性等挑战，强调需要内存系统与计算单元的协同设计。


<details>
  <summary>Details</summary>
Motivation: ACES趋势推动了对车载计算性能和通信基础设施的急剧增长需求，而摩尔定律和登纳德缩放定律的放缓使得计算系统需要转向异构化和专用化。同时，ACES应用需要高能效计算，处理大规模不规则数据集，并在同一平台上混合运行实时关键任务和通用计算任务，对互连系统提出了严峻挑战。

Method: 论文提出通过内存系统与用例、计算单元和加速器的协同设计来应对挑战。这包括优化片上/片外互连系统，减少不同关键级别任务间的争用，提高系统可预测性，并采用专用硬件加速器而非单纯依赖技术缩放。

Result: 分析表明，要满足ACES应用的性能、能效和混合关键性要求，必须采用系统级协同设计方法，特别是内存系统与计算架构的深度集成，以应对计算异构性、内存带宽需求和实时性约束。

Conclusion: ACES驱动的工业计算系统需要从根本上重新思考架构设计，通过内存系统与计算单元的协同设计来平衡性能、能效和混合关键性需求，这是应对当前计算技术限制和未来应用挑战的关键路径。

Abstract: Industrial domains such as automotive, robotics, and aerospace are rapidly evolving to satisfy the increasing demand for machine-learning-driven Autonomy, Connectivity, Electrification, and Shared mobility (ACES). This paradigm shift inherently and significantly increases the requirement for onboard computing performance and high-performance communication infrastructure. At the same time, Moore's Law and Dennard Scaling are grinding to a halt, in turn, driving computing systems to larger scales and higher levels of heterogeneity and specialization, through application-specific hardware accelerators, instead of relying on technological scaling only. Approaching ACES requires this substantial amount of compute at an increasingly high energy-efficiency, since most use cases are fundamentally resource-bound. This increase in compute performance and heterogeneity goes hand in hand with a growing demand for high memory bandwidth and capacity as the driving applications grow in complexity, operating on huge and progressively irregular data sets and further requiring a steady influx of sensor data, increasing pressure both on on-chip and off-chip interconnect systems. Further, ACES combines real-time time-critical with general compute tasks on the same physical platform, sharing communication, storage, and micro-architectural resources. These heterogeneous mixed-criticality systems (MCSs) place additional pressure on the interconnect, demanding minimal contention between the different criticality levels to sustain a high degree of predictability. Fulfilling the performance and energy-efficiency requirements across a wide range of industrial applications requires a carefully co-designed process of the memory system with the use cases as well as the compute units and accelerators.

</details>
