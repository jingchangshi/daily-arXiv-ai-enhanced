{"id": "2508.20365", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20365", "abs": "https://arxiv.org/abs/2508.20365", "authors": ["Naoki Kobayashi", "Ryosuke Sato", "Ayumi Shinohara", "Ryo Yoshinaka"], "title": "Solvable Tuple Patterns and Their Applications to Program Verification", "comment": null, "summary": "Despite the recent progress of automated program verification techniques,\nfully automated verification of programs manipulating recursive data structures\nremains a challenge. We introduce the notion of solvable tuple patterns (STPs)\nto express invariants between list-like recursive data structures. A\ndistinguishing feature of STPs is that they can be efficiently inferred from\nonly a small number of positive samples; no negative samples are required. An\nSMT solver that supports the sequence theory can be used to check that an\ninferred STP is indeed an inductive invariant. After presenting basic\nproperties of STPs and an STP inference algorithm, we show how to incorporate\nthe STP inference into a CHC (Constrained Horn Clauses) solver supporting\nlist-like data structures, which serves as a uniform backend for automated\nprogram verification tools. A CHC solver incorporating the STP inference has\nwon the ADT-LIN category of CHC-COMP 2025 by a big margin.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u53ef\u89e3\u5143\u7ec4\u6a21\u5f0f(STP)\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u8868\u8fbe\u5217\u8868\u5f0f\u9012\u5f52\u6570\u636e\u7ed3\u6784\u7684\u7a0b\u5e8f\u4e0d\u53d8\u91cf\uff0c\u65e0\u9700\u8d1f\u6837\u672c\u5373\u53ef\u4ece\u5c0f\u6837\u672c\u4e2d\u9ad8\u6548\u63a8\u65ad\uff0c\u5e76\u96c6\u6210\u5230CHC\u6c42\u89e3\u5668\u4e2d\u5b9e\u73b0\u81ea\u52a8\u5316\u7a0b\u5e8f\u9a8c\u8bc1\u3002", "motivation": "\u5c3d\u7ba1\u7a0b\u5e8f\u9a8c\u8bc1\u6280\u672f\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u5b8c\u5168\u81ea\u52a8\u5316\u9a8c\u8bc1\u64cd\u4f5c\u9012\u5f52\u6570\u636e\u7ed3\u6784\u7684\u7a0b\u5e8f\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5217\u8868\u7c7b\u6570\u636e\u7ed3\u6784\u7684\u4e0d\u53d8\u91cf\u63a8\u65ad\u65b9\u9762\u3002", "method": "\u5f15\u5165\u53ef\u89e3\u5143\u7ec4\u6a21\u5f0f(STP)\u8868\u8fbe\u6570\u636e\u7ed3\u6784\u4e0d\u53d8\u91cf\uff0c\u5f00\u53d1STP\u63a8\u65ad\u7b97\u6cd5\u4ec5\u9700\u6b63\u6837\u672c\uff0c\u5229\u7528\u652f\u6301\u5e8f\u5217\u7406\u8bba\u7684SMT\u6c42\u89e3\u5668\u9a8c\u8bc1\u4e0d\u53d8\u91cf\uff0c\u5e76\u5c06STP\u63a8\u65ad\u96c6\u6210\u5230\u652f\u6301\u5217\u8868\u7c7b\u6570\u636e\u7ed3\u6784\u7684CHC\u6c42\u89e3\u5668\u4e2d\u3002", "result": "\u96c6\u6210STP\u63a8\u65ad\u7684CHC\u6c42\u89e3\u5668\u5728CHC-COMP 2025\u7684ADT-LIN\u7c7b\u522b\u4e2d\u4ee5\u663e\u8457\u4f18\u52bf\u83b7\u80dc\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "STP\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u63a8\u65ad\u9012\u5f52\u6570\u636e\u7ed3\u6784\u4e0d\u53d8\u91cf\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u8d1f\u6837\u672c\u4e14\u53ef\u96c6\u6210\u5230\u73b0\u6709\u9a8c\u8bc1\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u7a0b\u5e8f\u9a8c\u8bc1\u80fd\u529b\u3002"}}
{"id": "2508.20922", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.20922", "abs": "https://arxiv.org/abs/2508.20922", "authors": ["Markus B\u00f6ck", "J\u00fcrgen Cito"], "title": "Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops", "comment": null, "summary": "It is commonly known that any Bayesian network can be implemented as a\nprobabilistic program, but the reverse direction is not so clear. In this work,\nwe address the open question to what extent a probabilistic program with\nuser-labelled sample statements and while loops - features found in languages\nlike Gen, Turing, and Pyro - can be represented graphically. To this end, we\nextend existing operational semantics to support these language features. By\ntranslating a program to its control-flow graph, we define a sound static\nanalysis that approximates the dependency structure of the random variables in\nthe program. As a result, we obtain a static factorisation of the implicitly\ndefined program density, which is equivalent to the known Bayesian network\nfactorisation for programs without loops and constant labels, but constitutes a\nnovel graphical representation for programs that define an unbounded number of\nrandom variables via loops or dynamic labels. We further develop a sound\nprogram slicing technique to leverage this structure to statically enable three\nwell-known optimisations for the considered program class: we reduce the\nvariance of gradient estimates in variational inference and we speed up both\nsingle-site Metropolis Hastings and sequential Monte Carlo. These optimisations\nare proven correct and empirically shown to match or outperform existing\ntechniques.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6982\u7387\u7a0b\u5e8f\u4e0e\u8d1d\u53f6\u65af\u7f51\u7edc\u4e4b\u95f4\u7684\u53cc\u5411\u8868\u793a\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u652f\u6301\u5faa\u73af\u548c\u52a8\u6001\u6807\u7b7e\u7684\u9759\u6001\u5206\u6790\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u7a0b\u5e8f\u5207\u7247\u6280\u672f\u6765\u4f18\u5316\u53d8\u5206\u63a8\u7406\u3001Metropolis Hastings\u548c\u987a\u5e8f\u8499\u7279\u5361\u7f57\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6982\u7387\u7a0b\u5e8f\uff08\u5305\u542b\u7528\u6237\u6807\u8bb0\u7684sample\u8bed\u53e5\u548cwhile\u5faa\u73af\uff09\u80fd\u5426\u88ab\u56fe\u5f62\u5316\u8868\u793a\u7684\u5f00\u6e90\u95ee\u9898\uff0c\u8fd9\u4e9b\u7279\u6027\u5728Gen\u3001Turing\u548cPyro\u7b49\u6982\u7387\u7f16\u7a0b\u8bed\u8a00\u4e2d\u5f88\u5e38\u89c1\u3002", "method": "\u6269\u5c55\u73b0\u6709\u64cd\u4f5c\u8bed\u4e49\u4ee5\u652f\u6301\u8bed\u8a00\u7279\u6027\uff0c\u901a\u8fc7\u5c06\u7a0b\u5e8f\u8f6c\u6362\u4e3a\u63a7\u5236\u6d41\u56fe\uff0c\u5b9a\u4e49\u9759\u6001\u5206\u6790\u6765\u8fd1\u4f3c\u7a0b\u5e8f\u4e2d\u968f\u673a\u53d8\u91cf\u7684\u4f9d\u8d56\u7ed3\u6784\uff0c\u83b7\u5f97\u9759\u6001\u56e0\u5b50\u5206\u89e3\uff0c\u5e76\u5f00\u53d1\u7a0b\u5e8f\u5207\u7247\u6280\u672f\u3002", "result": "\u83b7\u5f97\u4e86\u9690\u5f0f\u5b9a\u4e49\u7a0b\u5e8f\u5bc6\u5ea6\u7684\u9759\u6001\u56e0\u5b50\u5206\u89e3\uff0c\u5bf9\u4e8e\u65e0\u5faa\u73af\u548c\u5e38\u91cf\u6807\u7b7e\u7684\u7a0b\u5e8f\u7b49\u540c\u4e8e\u8d1d\u53f6\u65af\u7f51\u7edc\u56e0\u5b50\u5206\u89e3\uff0c\u5bf9\u4e8e\u901a\u8fc7\u5faa\u73af\u6216\u52a8\u6001\u6807\u7b7e\u5b9a\u4e49\u65e0\u9650\u968f\u673a\u53d8\u91cf\u7684\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u56fe\u5f62\u8868\u793a\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f18\u5316\u6280\u672f\u88ab\u8bc1\u660e\u662f\u6b63\u786e\u7684\uff0c\u5e76\u5728\u7ecf\u9a8c\u4e0a\u663e\u793a\u80fd\u591f\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u6210\u529f\u964d\u4f4e\u4e86\u53d8\u5206\u63a8\u7406\u4e2d\u68af\u5ea6\u4f30\u8ba1\u7684\u65b9\u5dee\uff0c\u5e76\u52a0\u901f\u4e86\u5355\u70b9Metropolis Hastings\u548c\u987a\u5e8f\u8499\u7279\u5361\u7f57\u7b97\u6cd5\u3002"}}
{"id": "2508.20304", "categories": ["cs.AR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.20304", "abs": "https://arxiv.org/abs/2508.20304", "authors": ["Siyuan Lu", "Kangwei Xu", "Peng Xie", "Rui Wang", "Yuanqing Cheng"], "title": "Testing and Fault Tolerance Techniques for Carbon Nanotube-Based FPGAs", "comment": "Accepted by Integration, VLSI Journal", "summary": "As the semiconductor manufacturing process technology node shrinks into the\nnanometer-scale, the CMOS-based Field Programmable Gate Arrays (FPGAs) face big\nchallenges in scalability of performance and power consumption. Multi-walled\nCarbon Nanotube (MWCNT) serves as a promising candidate for Cu interconnects\nthanks to the superior conductivity. Moreover, Carbon Nanotube Field Transistor\n(CNFET) also emerges as a prospective alternative to the conventional CMOS\ndevice because of high power efficiency and large noise margin. The combination\nof MWCNT and CNFET enables the promising CNT-based FPGAs. However, the MWCNT\ninterconnects exhibit significant process variations due to immature\nfabrication process, leading to delay faults. Also, the non-ideal CNFET\nfabrication process may generate a few metallic CNTs (m-CNTs), rendering\ncorrelated faulty blocks. In this article, we propose a ring oscillator (RO)\nbased testing technique to detect delay faults due to the process variation of\nMWCNT interconnects. Furthermore, we propose an effective testing technique for\nthe carry chains in CLBs, and an improved circuit design based on the lookup\ntable (LUT) is applied to speed up the fault testing of CNT-based FPGAs. In\naddition, we propose a testing algorithm to detect m-CNTs in CLBs. Finally, we\npropose a redundant spare row sharing architecture to improve the yield of\nCNT-based FPGA further. Experimental results show that the test time for a\n6-input LUT can be reduced by 35.49% compared with conventional testing, and\nthe proposed algorithm can achieve a high test coverage with little overhead.\nThe proposed redundant architecture can repair the faulty segment effectively\nand efficiently.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6d4b\u8bd5\u6280\u672f\u548c\u5907\u4efd\u67b6\u6784\uff0c\u7528\u4e8e\u68c0\u6d4b\u4fee\u590d\u57fa\u4e8e\u78b3\u7eb3\u7c73\u7ba1\u7684FPGA\u4e2d\u7684\u5ef6\u8fdf\u6545\u969c\u548c\u91d1\u5c5e\u6027\u78b3\u7eb3\u7c73\u7ba1\u6545\u969c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u6548\u7387\u548c\u4ea7\u54c1\u826f\u7387\u3002", "motivation": "\u968f\u7740\u534a\u5bfc\u4f53\u5236\u9020\u8fdb\u5165\u7eb3\u7c73\u7ea7\uff0c\u4f20\u7edfCMOS FPGA\u5728\u6027\u80fd\u548c\u529f\u8017\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002\u591a\u58c1\u78b3\u7eb3\u7c73\u7ba1(MWCNT)\u548c\u78b3\u7eb3\u7c73\u7ba1\u573a\u6548\u5e94\u7ebf\u6027\u7ba1(CNFET)\u4f5c\u4e3a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u8005\uff0c\u4f46\u56e0\u5236\u9020\u8fc7\u7a0b\u4e0d\u6210\u719f\u5bfc\u81f4\u4e86\u5ef6\u8fdf\u6545\u969c\u548c\u91d1\u5c5e\u6027\u78b3\u7eb3\u7c73\u7ba1(m-CNTs)\u6545\u969c\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u73af\u5f62\u632f\u8361\u5668(RO)\u7684\u6d4b\u8bd5\u6280\u672f\u68c0\u6d4bMWCNT\u4e92\u8fde\u7ebf\u5ef6\u8fdf\u6545\u969c\uff1b\u4e3aCLB\u4e2d\u7684\u8fd0\u884c\u94fe\u63d0\u51fa\u6709\u6548\u6d4b\u8bd5\u6280\u672f\uff1b\u91c7\u7528\u6539\u8fdb\u7684\u67e5\u627e\u8868(LUT)\u7535\u8def\u8bbe\u8ba1\u52a0\u5feb\u6d4b\u8bd5\u901f\u5ea6\uff1b\u63d0\u51fa\u68c0\u6d4bm-CNTs\u7684\u6d4b\u8bd5\u7b97\u6cd5\uff1b\u8bbe\u8ba1\u4e86\u5197\u4f59\u5907\u4efd\u884c\u5171\u4eab\u67b6\u6784\u63d0\u9ad8\u4ea7\u54c1\u826f\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c6\u8f93\u5165LUT\u7684\u6d4b\u8bd5\u65f6\u95f4\u6bd4\u4f20\u7edf\u6d4b\u8bd5\u51cf\u5c1135.49%\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u4ee5\u5c11\u91cf\u5f00\u9500\u5b9e\u73b0\u9ad8\u6d4b\u8bd5\u8986\u76d6\u7387\uff0c\u5197\u4f59\u67b6\u6784\u80fd\u591f\u9ad8\u6548\u5730\u4fee\u590d\u6545\u969c\u6bb5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u78b3\u7eb3\u7c73\u7ba1\u57faFPGA\u63d0\u4f9b\u4e86\u4e00\u5957\u5b8c\u6574\u7684\u6d4b\u8bd5\u548c\u4fee\u590d\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56e0\u5236\u9020\u8fc7\u7a0b\u4e0d\u6210\u719f\u5bfc\u81f4\u7684\u6545\u969c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u6548\u7387\u548c\u4ea7\u54c1\u826f\u7387\uff0c\u4fc3\u8fdb\u4e86\u78b3\u7eb3\u7c73\u7ba1\u6280\u672f\u5728FPGA\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2508.20253", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.20253", "abs": "https://arxiv.org/abs/2508.20253", "authors": ["Ruihao Li", "Qinzhe Wu", "Krishna Kavi", "Gayatri Mehta", "Jonathan C. Beard", "Neeraja J. Yadwadkar", "Lizy K. John"], "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight Core for Memory Allocation", "comment": null, "summary": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively.", "AI": {"tldr": "SpeedMalloc\u4f7f\u7528\u8f7b\u91cf\u7ea7\u652f\u6301\u6838\u5fc3\u5904\u7406\u591a\u7ebf\u7a0b\u5e94\u7528\u7684\u5185\u5b58\u5206\u914d\u4efb\u52a1\uff0c\u901a\u8fc7\u5c06\u5206\u914d\u5668\u5143\u6570\u636e\u9694\u79bb\u5728\u4e13\u7528\u6838\u5fc3\u7f13\u5b58\u4e2d\uff0c\u51cf\u5c11\u7f13\u5b58\u51b2\u7a81\u548c\u8de8\u6838\u540c\u6b65\u5f00\u9500\uff0c\u76f8\u6bd4\u73b0\u6709\u8f6f\u4ef6\u548c\u786c\u4ef6\u5206\u914d\u5668\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5185\u5b58\u5206\u914d\u867d\u7136\u53ea\u5360\u4ee3\u7801\u76845%\uff0c\u4f46\u5bf9\u7a0b\u5e8f\u6027\u80fd\u6709\u8774\u8776\u6548\u5e94\u5f71\u54cd\uff0c\u53ef\u8fbe2.7\u500d\u6027\u80fd\u5dee\u5f02\u3002\u73b0\u6709\u52a0\u901f\u5668\u65b9\u6848\u5bf9\u591a\u7ebf\u7a0b\u652f\u6301\u6709\u9650\uff0c\u8de8\u6838\u540c\u6b65\u4ecd\u662f\u6311\u6218\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u53ef\u7f16\u7a0b\u652f\u6301\u6838\u5fc3\u5904\u7406\u5185\u5b58\u5206\u914d\u4efb\u52a1\uff0c\u8be5\u6838\u5fc3\u5177\u6709\u9ad8\u6548\u8de8\u6838\u6570\u636e\u540c\u6b65\u80fd\u529b\uff0c\u5c06\u6240\u6709\u5206\u914d\u5668\u5143\u6570\u636e\u4fdd\u5b58\u5728\u81ea\u8eab\u7f13\u5b58\u4e2d\uff0c\u907f\u514d\u4e0e\u7528\u6237\u6570\u636e\u7f13\u5b58\u51b2\u7a81\u548c\u8de8\u6838\u5143\u6570\u636e\u540c\u6b65\u3002", "result": "\u76f8\u6bd4Jemalloc\u3001TCMalloc\u3001Mimalloc\u3001Mallacc\u548cMemento\u4e94\u79cd\u5148\u8fdb\u5206\u914d\u5668\uff0cSpeedMalloc\u5728\u591a\u7ebf\u7a0b\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u5206\u522b\u5b9e\u73b01.75\u500d\u30011.18\u500d\u30011.15\u500d\u30011.23\u500d\u548c1.18\u500d\u7684\u52a0\u901f\u3002", "conclusion": "SpeedMalloc\u901a\u8fc7\u4e13\u7528\u652f\u6301\u6838\u5fc3\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u591a\u7ebf\u7a0b\u5185\u5b58\u5206\u914d\u7684\u7f13\u5b58\u6c61\u67d3\u548c\u540c\u6b65\u5f00\u9500\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\u6027\u80fd\u66f4\u4f18\uff0c\u4e14\u80fd\u9002\u5e94\u65b0\u7684\u5206\u914d\u5668\u8bbe\u8ba1\u3002"}}
{"id": "2508.20425", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.20425", "abs": "https://arxiv.org/abs/2508.20425", "authors": ["Shuhan Liu", "Samuel Dayo", "Peijing Li", "Philip Levis", "Subhasish Mitra", "Thierry Tambe", "David Tennenhouse", "H. -S. Philip Wong"], "title": "The Future of Memory: Limits and Opportunities", "comment": "3 Pages, 2 Figures, 1 Table, Accepted to SOSP 25 BigMem Workshop", "summary": "Memory latency, bandwidth, capacity, and energy increasingly limit\nperformance. In this paper, we reconsider proposed system architectures that\nconsist of huge (many-terabyte to petabyte scale) memories shared among large\nnumbers of CPUs. We argue two practical engineering challenges, scaling and\nsignaling, limit such designs. We propose the opposite approach. Rather than\ncreate large, shared, homogenous memories, systems explicitly break memory up\ninto smaller slices more tightly coupled with compute elements. Leveraging\nadvances in 2.5D/3D integration, this compute-memory node provisions private\nlocal memory, enabling accesses of node-exclusive data through micrometer-scale\ndistances, and dramatically reduced access cost. In-package memory elements\nsupport shared state within a processor, providing far better bandwidth and\nenergy-efficiency than DRAM, which is used as main memory for large working\nsets and cold data. Hardware making memory capacities and distances explicit\nallows software to efficiently compose this hierarchy, managing data placement\nand movement.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97-\u5185\u5b58\u8282\u70b9\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u5185\u5b58\u5206\u5272\u6210\u66f4\u5c0f\u7684\u5207\u7247\u5e76\u4e0e\u8ba1\u7b97\u5143\u7d20\u7d27\u5bc6\u8026\u5408\uff0c\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u5171\u4eab\u5185\u5b58\u7cfb\u7edf\u7684\u6269\u5c55\u6027\u548c\u4fe1\u4ee4\u6311\u6218\u3002", "motivation": "\u5185\u5b58\u5ef6\u8fdf\u3001\u5e26\u5bbd\u3001\u5bb9\u91cf\u548c\u80fd\u8017\u95ee\u9898\u4e0d\u65ad\u5230\u5236\u7cfb\u7edf\u6027\u80fd\uff0c\u9700\u8981\u91cd\u65b0\u8003\u8651\u7cfb\u7edf\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u5229\u75282.5D/3D\u96c6\u6210\u6280\u672f\uff0c\u6784\u5efa\u8ba1\u7b97-\u5185\u5b58\u8282\u70b9\uff0c\u63d0\u4f9b\u79c1\u6709\u672c\u5730\u5185\u5b58\u548c\u5305\u5185\u5171\u4eab\u5185\u5b58\u5143\u7d20\uff0c\u8ba9\u8f6f\u4ef6\u660e\u786e\u7ba1\u7406\u6570\u636e\u4f4d\u7f6e\u548c\u79fb\u52a8\u3002", "result": "\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u5fae\u7c73\u7ea7\u8ddd\u79bb\u7684\u8282\u70b9\u72ec\u5360\u6570\u636e\u8bbf\u95ee\uff0c\u663e\u8457\u964d\u4f4e\u8bbf\u95ee\u6210\u672c\uff0c\u5e76\u63d0\u4f9b\u6bd4DRAM\u66f4\u597d\u7684\u5e26\u5bbd\u548c\u80fd\u91cf\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5185\u5b58\u5206\u5272\u6210\u5c0f\u7247\u4e0e\u8ba1\u7b97\u7d27\u5bc6\u8026\u5408\uff0c\u8ba9\u8f6f\u4ef6\u660e\u786e\u7ba1\u7406\u5185\u5b58\u5c42\u7ea7\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u89e3\u51b3\u5185\u5b58\u6027\u80fd\u548c\u80fd\u8017\u95ee\u9898\u3002"}}
{"id": "2508.20258", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20258", "abs": "https://arxiv.org/abs/2508.20258", "authors": ["Arya Tschand", "Muhammad Awad", "Ryan Swann", "Kesavan Ramakrishnan", "Jeffrey Ma", "Keith Lowery", "Ganesh Dasika", "Vijay Janapa Reddi"], "title": "SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization", "comment": null, "summary": "Large language models (LLMs) have shown progress in GPU kernel performance\nengineering using inefficient search-based methods that optimize around\nruntime. Any existing approach lacks a key characteristic that human\nperformance engineers rely on for near-optimal utilization --\nhardware-awareness. By leveraging the workload's specific memory access\npatterns, architecture specifications, filtered profiling logs, and reflections\non historical performance, we can make software-level optimizations that are\ntailored to the underlying hardware. SwizzlePerf automatically generates\nspatial optimizations for GPU kernels on disaggregated architectures by giving\nLLMs explicit hardware-awareness.\n  For a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same\nhardware-specific optimal swizzling pattern that took expert performance\nengineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels,\nSwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve\nup to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the\nfirst of many steps toward systematically creating hardware-aware LLM\nperformance engineering agents.", "AI": {"tldr": "SwizzlePerf\u662f\u4e00\u4e2a\u5229\u7528LLM\u8fdb\u884cGPU\u5185\u6838\u6027\u80fd\u4f18\u5316\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u63d0\u4f9b\u786c\u4ef6\u611f\u77e5\u80fd\u529b\uff0c\u81ea\u52a8\u751f\u6210\u7a7a\u95f4\u4f18\u5316\u65b9\u6848\uff0c\u76f8\u6bd4\u4f20\u7edf\u641c\u7d22\u65b9\u6cd5\u6548\u7387\u5927\u5e45\u63d0\u5347", "motivation": "\u73b0\u6709\u57fa\u4e8e\u641c\u7d22\u7684LLM\u6027\u80fd\u4f18\u5316\u65b9\u6cd5\u7f3a\u4e4f\u786c\u4ef6\u611f\u77e5\u80fd\u529b\uff0c\u800c\u4eba\u7c7b\u6027\u80fd\u5de5\u7a0b\u5e08\u4f9d\u8d56\u786c\u4ef6\u7279\u6027\u6765\u5b9e\u73b0\u8fd1\u6700\u4f18\u6027\u80fd\u3002\u9700\u8981\u8ba9LLM\u5177\u5907\u786c\u4ef6\u610f\u8bc6\u6765\u8fdb\u884c\u8f6f\u4ef6\u7ea7\u4f18\u5316", "method": "\u901a\u8fc7\u5206\u6790\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u3001\u67b6\u6784\u89c4\u683c\u3001\u8fc7\u6ee4\u7684\u6027\u80fd\u65e5\u5fd7\u548c\u5386\u53f2\u6027\u80fd\u53cd\u601d\uff0c\u4e3aLLM\u63d0\u4f9b\u660e\u786e\u7684\u786c\u4ef6\u611f\u77e5\u80fd\u529b\uff0c\u81ea\u52a8\u751f\u6210GPU\u5185\u6838\u7684\u7a7a\u95f4\u4f18\u5316\u65b9\u6848", "result": "\u5bf9\u4e8eGEMM\u5185\u6838\uff0cSwizzlePerf\u57285\u5206\u949f\u5185\u751f\u6210\u4e13\u5bb6\u9700\u89812\u5468\u624d\u80fd\u627e\u5230\u7684\u6700\u4f18swizzling\u6a21\u5f0f\u3002\u572810\u4e2a\u591a\u6837\u5316\u5185\u6838\u4e2d\uff0c9\u4e2a\u5b9e\u73b0\u4e86\u6700\u9ad82.06\u500d\u52a0\u901f\u548c70%\u7684L2\u547d\u4e2d\u7387\u63d0\u5347", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7cfb\u7edf\u6027\u5730\u521b\u5efa\u786c\u4ef6\u611f\u77e5LLM\u6027\u80fd\u5de5\u7a0b\u4ee3\u7406\u7684\u5de5\u4f5c\uff0c\u4e3a\u672a\u6765\u81ea\u52a8\u5316\u786c\u4ef6\u4f18\u5316\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411"}}
{"id": "2508.20653", "categories": ["cs.AR", "cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20653", "abs": "https://arxiv.org/abs/2508.20653", "authors": ["Alperen Bolat", "Sakir Sezer", "Kieran McLaughlin", "Henry Hui"], "title": "Microarchitecture Design and Benchmarking of Custom SHA-3 Instruction for RISC-V", "comment": "Extended version of IEEE ISVLSI Conference Paper", "summary": "Integrating cryptographic accelerators into modern CPU architectures presents\nunique microarchitectural challenges, particularly when extending instruction\nsets with complex and multistage operations. Hardware-assisted cryptographic\ninstructions, such as Intel's AES-NI and ARM's custom instructions for\nencryption workloads, have demonstrated substantial performance improvements.\nHowever, efficient SHA-3 acceleration remains an open problem due to its\ndistinct permutation-based structure and memory access patterns. Existing\nsolutions primarily rely on standalone coprocessors or software optimizations,\noften avoiding the complexities of direct microarchitectural integration. This\nstudy investigates the architectural challenges of embedding a SHA-3\npermutation operation as a custom instruction within a general-purpose\nprocessor, focusing on pipelined simultaneous execution, storage utilization,\nand hardware cost. In this paper, we investigated and prototyped a SHA-3 custom\ninstruction for the RISC-V CPU architecture. Using cycle-accurate GEM5\nsimulations and FPGA prototyping, our results demonstrate performance\nimprovements of up to 8.02x for RISC-V optimized SHA-3 software workloads and\nup to 46.31x for Keccak-specific software workloads, with only a 15.09%\nincrease in registers and a 11.51% increase in LUT utilization. These findings\nprovide critical insights into the feasibility and impact of SHA-3 acceleration\nat the microarchitectural level, highlighting practical design considerations\nfor future cryptographic instruction set extensions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728RISC-V CPU\u67b6\u6784\u4e2d\u96c6\u6210SHA-3\u52a0\u5bc6\u52a0\u901f\u6307\u4ee4\u7684\u5fae\u67b6\u6784\u6311\u6218\uff0c\u901a\u8fc7\u5b9a\u5236\u6307\u4ee4\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "SHA-3\u52a0\u5bc6\u7531\u4e8e\u5176\u72ec\u7279\u7684\u57fa\u4e8e\u7f6e\u6362\u7684\u7ed3\u6784\u548c\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u4f9d\u8d56\u72ec\u7acb\u534f\u5904\u7406\u5668\u6216\u8f6f\u4ef6\u4f18\u5316\uff0c\u7f3a\u4e4f\u76f4\u63a5\u7684\u5fae\u67b6\u6784\u96c6\u6210\u7814\u7a76\u3002", "method": "\u5728RISC-V CPU\u67b6\u6784\u4e2d\u8bbe\u8ba1\u548c\u539f\u578b\u5316SHA-3\u5b9a\u5236\u6307\u4ee4\uff0c\u4f7f\u7528GEM5\u5468\u671f\u7cbe\u786e\u6a21\u62df\u548cFPGA\u539f\u578b\u9a8c\u8bc1\uff0c\u91cd\u70b9\u5173\u6ce8\u6d41\u6c34\u7ebf\u5e76\u884c\u6267\u884c\u3001\u5b58\u50a8\u5229\u7528\u548c\u786c\u4ef6\u6210\u672c\u3002", "result": "\u6027\u80fd\u63d0\u5347\u8fbe8.02\u500d\uff08RISC-V\u4f18\u5316\u8f6f\u4ef6\uff09\u548c46.31\u500d\uff08Keccak\u7279\u5b9a\u8f6f\u4ef6\uff09\uff0c\u5bc4\u5b58\u5668\u4f7f\u7528\u4ec5\u589e\u52a015.09%\uff0cLUT\u5229\u7528\u7387\u589e\u52a011.51%\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u5fae\u67b6\u6784\u7ea7\u522b\u5b9e\u73b0SHA-3\u52a0\u901f\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u5bc6\u7801\u5b66\u6307\u4ee4\u96c6\u6269\u5c55\u63d0\u4f9b\u4e86\u5173\u952e\u8bbe\u8ba1\u89c1\u89e3\u3002"}}
{"id": "2508.20274", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.20274", "abs": "https://arxiv.org/abs/2508.20274", "authors": ["Erfan Darzi", "Shreeanant Bharadwaj", "Sree Bhargavi Balija"], "title": "Predictable LLM Serving on GPU Clusters", "comment": null, "summary": "Latency-sensitive inference on shared A100 clusters often suffers\nnoisy-neighbor interference on the PCIe fabric, inflating tail latency and SLO\nviolations. We present a fabric-agnostic, VM-deployable host-level controller\nthat combines dynamic Multi-Instance GPU (MIG) reconfiguration, PCIe-aware\nplacement, and lightweight guardrails (MPS quotas, cgroup I/O). It samples\nper-tenant tails and system signals, uses topology hints to avoid PCIe hot\nspots, and gates actions with dwell/cool-down to avoid thrash. On a single host\nand a 2-node (16-GPU) cluster, SLO miss-rate is reduced by \\(\\approx\\)32\\%\n(\\(\\approx\\)1.5) and p99 latency improves \\(\\approx\\)15\\% with \\(\\leq\\)5\\%\nthroughput cost versus static MIG and naive placement; ablations show MIG and\nplacement contribute comparably. We also evaluate LLM serving with vLLM on OLMo\n2 7B Instruct: TTFT p99 improves \\(\\approx\\)10--15\\% at \\(\\leq\\)5\\% cost\nwithout changing the controller.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u673a\u7ea7\u522b\u7684PCIe\u5e72\u6270\u63a7\u5236\u65b9\u6848\uff0c\u901a\u8fc7\u52a8\u6001MIG\u91cd\u914d\u7f6e\u3001PCIe\u611f\u77e5\u653e\u7f6e\u548c\u8f7b\u91cf\u7ea7\u9632\u62a4\u673a\u5236\uff0c\u6709\u6548\u964d\u4f4e\u5171\u4eabA100\u96c6\u7fa4\u4e2d\u7684\u5c3e\u90e8\u5ef6\u8fdf\u548cSLO\u8fdd\u89c4\u7387\u3002", "motivation": "\u5171\u4eabA100\u96c6\u7fa4\u4e2dPCIe\u7ed3\u6784\u4e0a\u7684\u566a\u58f0\u90bb\u5c45\u5e72\u6270\u4f1a\u5bfc\u81f4\u5c3e\u90e8\u5ef6\u8fdf\u589e\u52a0\u548cSLO\u8fdd\u89c4\uff0c\u5f71\u54cd\u5ef6\u8fdf\u654f\u611f\u578b\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u52a8\u6001\u591a\u5b9e\u4f8bGPU(MIG)\u91cd\u914d\u7f6e\u3001PCIe\u611f\u77e5\u7684\u865a\u62df\u673a\u653e\u7f6e\u7b56\u7565\uff0c\u7ed3\u5408MPS\u914d\u989d\u548ccgroup I/O\u63a7\u5236\u7b49\u8f7b\u91cf\u7ea7\u9632\u62a4\u673a\u5236\uff0c\u901a\u8fc7\u91c7\u6837\u79df\u6237\u5c3e\u90e8\u5ef6\u8fdf\u548c\u7cfb\u7edf\u4fe1\u53f7\uff0c\u5229\u7528\u62d3\u6251\u63d0\u793a\u907f\u514dPCIe\u70ed\u70b9\u3002", "result": "\u5728\u5355\u4e3b\u673a\u548c2\u8282\u70b9(16-GPU)\u96c6\u7fa4\u4e0a\uff0cSLO\u8fdd\u89c4\u7387\u964d\u4f4e\u7ea632%\uff0cp99\u5ef6\u8fdf\u6539\u5584\u7ea615%\uff0c\u541e\u5410\u91cf\u6210\u672c\u22645%\uff1bLLM\u670d\u52a1\u8bc4\u4f30\u663e\u793aTTFT p99\u6539\u558410-15%\uff0c\u6210\u672c\u22645%\u3002", "conclusion": "\u8be5\u63a7\u5236\u5668\u65b9\u6848\u80fd\u6709\u6548\u7f13\u89e3PCIe\u5e72\u6270\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4f4e\u541e\u5410\u91cf\u6210\u672c\u7684\u540c\u65f6\u663e\u8457\u6539\u5584\u5c3e\u90e8\u5ef6\u8fdf\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u63a7\u5236\u5668\u5373\u53ef\u9002\u7528\u4e8eLLM\u670d\u52a1\u573a\u666f\u3002"}}
{"id": "2508.20375", "categories": ["cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.20375", "abs": "https://arxiv.org/abs/2508.20375", "authors": ["Guanyu Xu", "Zhiwei Hao", "Li Shen", "Yong Luo", "Fuhui Sun", "Xiaoyan Wang", "Han Hu", "Yonggang Wen"], "title": "CoFormer: Collaborating with Heterogeneous Edge Devices for Scalable Transformer Inference", "comment": "Accepted by IEEE Transactions on Computers", "summary": "The impressive performance of transformer models has sparked the deployment\nof intelligent applications on resource-constrained edge devices. However,\nensuring high-quality service for real-time edge systems is a significant\nchallenge due to the considerable computational demands and resource\nrequirements of these models. Existing strategies typically either offload\ntransformer computations to other devices or directly deploy compressed models\non individual edge devices. These strategies, however, result in either\nconsiderable communication overhead or suboptimal trade-offs between accuracy\nand efficiency. To tackle these challenges, we propose a collaborative\ninference system for general transformer models, termed CoFormer. The central\nidea behind CoFormer is to exploit the divisibility and integrability of\ntransformer. An off-the-shelf large transformer can be decomposed into multiple\nsmaller models for distributed inference, and their intermediate results are\naggregated to generate the final output. We formulate an optimization problem\nto minimize both inference latency and accuracy degradation under heterogeneous\nhardware constraints. DeBo algorithm is proposed to first solve the\noptimization problem to derive the decomposition policy, and then progressively\ncalibrate decomposed models to restore performance. We demonstrate the\ncapability to support a wide range of transformer models on heterogeneous edge\ndevices, achieving up to 3.1$\\times$ inference speedup with large transformer\nmodels. Notably, CoFormer enables the efficient inference of GPT2-XL with 1.6\nbillion parameters on edge devices, reducing memory requirements by 76.3\\%.\nCoFormer can also reduce energy consumption by approximately 40\\% while\nmaintaining satisfactory inference performance.", "AI": {"tldr": "CoFormer\u662f\u4e00\u4e2a\u534f\u4f5c\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u5927\u578btransformer\u6a21\u578b\u5206\u89e3\u4e3a\u591a\u4e2a\u5c0f\u578b\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5206\u5e03\u5f0f\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u63a8\u7406\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7b56\u7565\u8981\u4e48\u5c06transformer\u8ba1\u7b97\u5378\u8f7d\u5230\u5176\u4ed6\u8bbe\u5907\u5bfc\u81f4\u901a\u4fe1\u5f00\u9500\u5927\uff0c\u8981\u4e48\u76f4\u63a5\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u538b\u7f29\u6a21\u578b\u5bfc\u81f4\u7cbe\u5ea6\u548c\u6548\u7387\u6743\u8861\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "method": "\u5229\u7528transformer\u7684\u53ef\u5206\u5272\u6027\u548c\u53ef\u96c6\u6210\u6027\uff0c\u5c06\u5927\u578btransformer\u5206\u89e3\u4e3a\u591a\u4e2a\u5c0f\u578b\u6a21\u578b\u8fdb\u884c\u5206\u5e03\u5f0f\u63a8\u7406\uff0c\u4e2d\u95f4\u7ed3\u679c\u805a\u5408\u751f\u6210\u6700\u7ec8\u8f93\u51fa\u3002\u4f7f\u7528DeBo\u7b97\u6cd5\u89e3\u51b3\u4f18\u5316\u95ee\u9898\uff0c\u6700\u5c0f\u5316\u63a8\u7406\u5ef6\u8fdf\u548c\u7cbe\u5ea6\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u6e10\u8fdb\u6821\u51c6\u6062\u590d\u6027\u80fd\u3002", "result": "\u652f\u6301\u591a\u79cdtransformer\u6a21\u578b\u5728\u5f02\u6784\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53473.1\u500d\uff0c\u5185\u5b58\u9700\u6c42\u51cf\u5c1176.3%\uff0c\u80fd\u8017\u964d\u4f4e\u7ea640%\uff0c\u540c\u65f6\u4fdd\u6301\u6ee1\u610f\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "CoFormer\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5927\u578btransformer\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u534f\u4f5c\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u8d44\u6e90\u6d88\u8017\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2508.20403", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.20403", "abs": "https://arxiv.org/abs/2508.20403", "authors": ["Tiancheng Zhao", "Zekun Yin", "Huihai An", "Xiaoyu Yang", "Zhou Jin", "Jiasi Shen", "Helen Xu"], "title": "pdGRASS: A Fast Parallel Density-Aware Algorithm for Graph Spectral Sparsification", "comment": null, "summary": "Graph Spectral Sparsification (GSS) identifies an ultra-sparse subgraph, or\nsparsifier, whose Laplacian matrix closely approximates the spectral properties\nof the original graph, enabling substantial reductions in computational\ncomplexity for computationally intensive problems in scientific computing. The\nstate-of-the-art method for efficient GSS is feGRASS, consisting of two steps:\n1) spanning tree generation and 2) off-tree edge recovery. However, feGRASS\nsuffers from two main issues: 1) difficulties in parallelizing the recovery\nstep for strict data dependencies, and 2) performance degradation on skewed\ninputs, often requiring multiple passes to recover sufficient edges. To address\nthese challenges, we propose parallel density-aware Graph Spectral\nSparsification (pdGRASS), a parallel algorithm that organizes edges into\ndisjoint subtasks without data dependencies between them, enabling efficient\nparallelization and sufficient edge recovery in a single pass. We empirically\nevaluate feGRASS and pdGRASS based on 1) off-tree edge-recovery runtime and 2)\nsparsifier quality, measured by the iteration count required for convergence in\na preconditioned conjugate gradient (PCG) application. The evaluation\ndemonstrates that, depending on the number of edges recovered, pdGRASS achieves\naverage speedups ranging from 3.9x to 8.8x. The resulting sparsifiers also show\nbetween 1.2x higher and 1.8x lower PCG iteration counts, with further\nimprovements as more edges are recovered. Additionally, pdGRASS mitigates the\nworst-case runtimes of feGRASS with over 1000x speedup. These results highlight\npdGRASS's significant improvements in scalability and performance for the graph\nspectral sparsification problem.", "AI": {"tldr": "\u63d0\u51fa\u4e86pdGRASS\u5e76\u884c\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709feGRASS\u65b9\u6cd5\u5728\u5e76\u884c\u5316\u548c\u504f\u659c\u8f93\u5165\u4e0a\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e863.9x-8.8x\u7684\u52a0\u901f\u6bd4\u548c\u66f4\u597d\u7684\u7a00\u758f\u5316\u8d28\u91cf", "motivation": "\u73b0\u6709feGRASS\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1)\u6062\u590d\u6b65\u9aa4\u96be\u4ee5\u5e76\u884c\u5316\uff08\u4e25\u683c\u6570\u636e\u4f9d\u8d56\uff092)\u5728\u504f\u659c\u8f93\u5165\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u591a\u6b21\u904d\u5386", "method": "pdGRASS\u5e76\u884c\u7b97\u6cd5\uff0c\u5c06\u8fb9\u7ec4\u7ec7\u6210\u65e0\u6570\u636e\u4f9d\u8d56\u7684\u72ec\u7acb\u5b50\u4efb\u52a1\uff0c\u652f\u6301\u9ad8\u6548\u5e76\u884c\u5316\u548c\u5355\u6b21\u904d\u5386\u7684\u5145\u5206\u8fb9\u6062\u590d", "result": "\u5e73\u5747\u52a0\u901f\u6bd43.9x-8.8x\uff0cPCG\u8fed\u4ee3\u6b21\u6570\u6539\u55841.2x-1.8x\uff0c\u6700\u574f\u60c5\u51b5\u4e0b\u5b9e\u73b01000x\u4ee5\u4e0a\u52a0\u901f", "conclusion": "pdGRASS\u5728\u56fe\u8c31\u7a00\u758f\u5316\u95ee\u9898\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd"}}
{"id": "2508.20508", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.20508", "abs": "https://arxiv.org/abs/2508.20508", "authors": ["Yilin Li", "Song Han", "Sibo Wang", "Ming Wang", "Renzi Meng"], "title": "Collaborative Evolution of Intelligent Agents in Large-Scale Microservice Systems", "comment": null, "summary": "This paper proposes an intelligent service optimization method based on a\nmulti-agent collaborative evolution mechanism to address governance challenges\nin large-scale microservice architectures. These challenges include complex\nservice dependencies, dynamic topology structures, and fluctuating workloads.\nThe method models each service as an agent and introduces graph representation\nlearning to construct a service dependency graph. This enables agents to\nperceive and embed structural changes within the system. Each agent learns its\npolicy based on a Markov Decision Process. A centralized training and\ndecentralized execution framework is used to integrate local autonomy with\nglobal coordination. To enhance overall system performance and adaptability, a\ngame-driven policy optimization mechanism is designed. Through a\nselection-mutation process, agent strategy distributions are dynamically\nadjusted. This supports adaptive collaboration and behavioral evolution among\nservices. Under this mechanism, the system can quickly respond and achieve\nstable policy convergence when facing scenarios such as sudden workload spikes,\ntopology reconfigurations, or resource conflicts. To evaluate the effectiveness\nof the proposed method, experiments are conducted on a representative\nmicroservice simulation platform. Comparative analyses are performed against\nseveral advanced approaches, focusing on coordination efficiency, adaptability,\nand policy convergence performance. Experimental results show that the proposed\nmethod outperforms others in several key metrics. It significantly improves\ngovernance efficiency and operational stability in large-scale microservice\nsystems. The method demonstrates strong practical value and engineering\nfeasibility.", "AI": {"tldr": "\u57fa\u4e8e\u591a\u4ee3\u7406\u534f\u540c\u8fdb\u5316\u673a\u5236\u7684\u5fae\u670d\u52a1\u667a\u80fd\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u8868\u793a\u5b66\u4e60\u548c\u6e38\u620f\u9a71\u52a8\u7b56\u7565\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u5fae\u670d\u52a1\u7cfb\u7edf\u7684\u7ba1\u7406\u6311\u6218", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u590d\u6742\u7684\u670d\u52a1\u4f9d\u8d56\u5173\u7cfb\u3001\u52a8\u6001\u62d3\u6251\u7ed3\u6784\u548c\u6ce2\u52a8\u5de5\u4f5c\u8d1f\u8377\u7b49\u7ba1\u7406\u6311\u6218", "method": "\u5c06\u6bcf\u4e2a\u670d\u52a1\u6a21\u578b\u5316\u4e3a\u4ee3\u7406\uff0c\u4f7f\u7528\u56fe\u8868\u793a\u5b66\u4e60\u6784\u5efa\u670d\u52a1\u4f9d\u8d56\u56fe\uff0c\u57fa\u4e8eMarkov\u51b3\u7b56\u8fc7\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u91c7\u7528\u4e2d\u592e\u8bad\u7ec3\u5206\u6563\u6267\u884c\u6846\u67b6\uff0c\u8bbe\u8ba1\u6e38\u620f\u9a71\u52a8\u7684\u7b56\u7565\u4f18\u5316\u673a\u5236", "result": "\u5728\u5fae\u670d\u52a1\u6a21\u62df\u5e73\u53f0\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5728\u591a\u4e2a\u5173\u952e\u6307\u6807\u4e0a\u8d85\u8fc7\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u5fae\u670d\u52a1\u7cfb\u7edf\u7684\u7ba1\u7406\u6548\u7387\u548c\u8fd0\u884c\u7a33\u5b9a\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5feb\u901f\u54cd\u5e94\u7a81\u53d1\u5de5\u4f5c\u8d1f\u8377\u3001\u62d3\u6251\u91cd\u914d\u7f6e\u6216\u8d44\u6e90\u51b2\u7a81\u7b49\u573a\u666f\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u7b56\u7565\u6536\u655b\uff0c\u5177\u6709\u5f3a\u70c8\u7684\u5b9e\u8df5\u4ef7\u503c\u548c\u5de5\u7a0b\u53ef\u884c\u6027"}}
