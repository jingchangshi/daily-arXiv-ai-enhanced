<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Type, Ability, and Effect Systems: Perspectives on Purity, Semantics, and Expressiveness](https://arxiv.org/abs/2510.07582)
*Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 该论文提出了基于上下文等价性的纯度语义定义，比较了不同类型系统的表达能力，发现最小效应系统和能力系统在表达力上不可比较，并提出将类型、能力和效应系统结合的综合方案。


<details>
  <summary>Details</summary>
Motivation: 现有强制纯计算与效应计算分离的方法（如单子、类型效应系统、能力系统）在精确性和可用性之间存在张力，各有优缺点，需要更好的评估标准。

Method: 提出基于上下文等价性的纯度语义定义，用完整性程度衡量表达能力，分析最小效应系统和能力系统的表达力，并开发逻辑关系来证明纯度和相关性质。

Result: 发现最小效应系统和能力系统在表达力上是不可比较的，即两者都无法完全包含对方；提出的综合系统能结合各系统的优势而避免其弱点。

Conclusion: 类型、能力和效应系统的结合能提供更好的纯度和效应分离方案，所提出的逻辑关系为多种效应类型系统的性质证明提供了便利工具。

Abstract: Programming benefits from a clear separation between pure, mathematical
computation and impure, effectful interaction with the world. Existing
approaches to enforce this separation include monads, type-and-effect systems,
and capability systems. All share a tension between precision and usability,
and each one has non-obvious strengths and weaknesses.
  This paper aims to raise the bar in assessing such systems. First, we propose
a semantic definition of purity, inspired by contextual equivalence, as a
baseline independent of any specific typing discipline. Second, we propose that
expressiveness should be measured by the degree of completeness, i.e., how many
semantically pure terms can be typed as pure. Using this measure, we focus on
minimal meaningful effect and capability systems and show that they are
incomparable, i.e., neither subsumes the other in terms of expressiveness.
  Based on this result, we propose a synthesis and show that type, ability, and
effect systems combine their respective strengths while avoiding their
weaknesses. As part of our formal model, we provide a logical relation to
facilitate proofs of purity and other properties for a variety of effect typing
disciplines.

</details>


### [2] [The Functional Machine Calculus III: Control](https://arxiv.org/abs/2510.07851)
*Willem Heijltjes*

Main category: cs.PL

TL;DR: Functional Machine Calculus从顺序计算扩展到分支和循环控制流，能够忠实嵌入包含条件、异常处理、迭代等的最小但完整的命令式语言，同时保持汇合归约和类型终止等关键特性。


<details>
  <summary>Details</summary>
Motivation: 统一函数式和命令式编程范式，在保持lambda演算汇合归约和类型终止特性的基础上，嵌入计算效应、求值策略和控制流操作。

Method: 通过扩展简化的Krivine机器，使用多个操作数栈建模效应，使用延续栈建模顺序、分支和循环计算，定义简单的操作语义。

Result: 实现了汇合归约关系和简单类型系统，保证机器终止和强规范化（无迭代时），这些特性可传递到嵌入的命令式语言中。

Conclusion: 提供了一个支持简单类型、直观操作语义和汇合归约语义的统一函数式-命令式计算模型。

Abstract: The Functional Machine Calculus (Heijltjes 2022) is a new approach to
unifying the imperative and functional programming paradigms. It extends the
lambda-calculus, preserving the key features of confluent reduction and typed
termination, to embed computational effects, evaluation strategies, and control
flow operations. The first instalment modelled sequential higher-order
computation with global store, input/output, probabilities, and
non-determinism, and embedded both the call-by-name and call-by-value
lambda-calculus, as well as Moggi's computational metalanguage and Levy's
call-by-push-value. The present paper extends the calculus from sequential to
branching and looping control flow. This allows the faithful embedding of a
minimal but complete imperative language, including conditionals, exception
handling, and iteration, as well as constants and algebraic data types.
  The calculus is defined through a simple operational semantics, extending the
(simplified) Krivine machine for the lambda-calculus with multiple operand
stacks to model effects and a continuation stack to model sequential,
branching, and looping computation. It features a confluent reduction relation
and a system of simple types that guarantees termination of the machine and
strong normalization of reduction (in the absence of iteration). These
properties carry over to the embedded imperative language, providing a unified
functional-imperative model of computation that supports simple types, a direct
and intuitive operational semantics, and a confluent reduction semantics.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Adaptive Execution Scheduler for DataDios SmartDiff](https://arxiv.org/abs/2510.07811)
*Aryan Poduri*

Main category: cs.DC

TL;DR: 提出了一种自适应调度器SmartDiff，通过动态调整批处理大小和线程/工作器数量，在固定CPU和内存预算下最小化p95延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的固定配置方法无法在动态工作负载下优化延迟和内存使用，需要一种能够自适应调整执行策略的调度器。

Method: 使用轻量级预分析器估计每行字节数和I/O速率，在线成本/内存模型修剪不安全操作，采用带保护的爬山策略优化延迟，并包含背压和慢任务缓解机制。

Result: 在合成和公共表格基准测试中，相比调优的预热启发式方法，p95延迟降低23-28%；相比固定网格基线降低35-40%，峰值内存降低16-22%（相比固定配置降低25-32%），零内存溢出且吞吐量相当。

Conclusion: 该自适应调度器能够有效平衡延迟和内存使用，在保证性能的同时避免内存溢出问题。

Abstract: We present an adaptive scheduler for a single differencing engine (SmartDiff)
with two execution modes: (i) in-memory threads and (ii) Dask based
parallelism. The scheduler continuously tunes batch size and worker/thread
count within fixed CPU and memory budgets to minimize p95 latency. A
lightweight preflight profiler estimates bytes/row and I/O rate; an online
cost/memory model prunes unsafe actions; and a guarded hill-climb policy favors
lower latency with backpressure and straggler mitigation. Backend selection is
gated by a conservative working-set estimate so that in-memory execution is
chosen when safe, otherwise Dask is used. Across synthetic and public tabular
benchmarks, the scheduler reduces p95 latency by 23 to 28 percent versus a
tuned warm-up heuristic (and by 35 to 40 percent versus fixed grid baselines),
while lowering peak memory by 16 to 22 percent (25 to 32 percent vs. fixed)
with zero OOMs and comparable throughput.

</details>


### [4] [A Multi-Simulation Bridge for IoT Digital Twins](https://arxiv.org/abs/2510.08164)
*Marco Picone,Samuele Burattini,Marco Melloni,Prasad Talasila,Davide Ziglioli,Matteo Martinelli,Nicola Bicocchi,Alessandro Ricci,Peter Gorm Larsen*

Main category: cs.DC

TL;DR: 本文提出了DT Simulation Bridge框架，支持数字孪生与仿真平台之间的双向数据交换和多样化交互模式，提升工业物联网系统的设计灵活性和实时操作能力。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生在物联网和工业物联网中能力的增强，需要与仿真平台无缝集成以支持系统设计、验证和实时操作。

Method: 设计了DT Simulation Bridge软件框架，支持数字孪生与仿真环境之间的双向数据交换，使仿真能动态更新数字孪生模型，同时数字孪生提供实时反馈来调整仿真参数。

Result: 实验结果表明，该框架增强了设计灵活性，促进了虚拟调试，并支持在真实条件下进行实时行为分析，在多种工业场景中表现出有效性。

Conclusion: DT Simulation Bridge框架成功实现了数字孪生与仿真平台的灵活互操作和可扩展部署，为工业物联网系统提供了有效的设计和操作支持。

Abstract: The increasing capabilities of Digital Twins (DTs) in the context of the
Internet of Things (IoT) and Industrial IoT (IIoT) call for seamless
integration with simulation platforms to support system design, validation, and
real-time operation. This paper introduces the concept, design, and
experimental evaluation of the DT Simulation Bridge - a software framework that
enables diverse interaction patterns between active DTs and simulation
environments. The framework supports both the DT development lifecycle and the
incorporation of simulations during active operation. Through bidirectional
data exchange, simulations can update DT models dynamically, while DTs provide
real-time feedback to adapt simulation parameters. We describe the
architectural design and core software components that ensure flexible
interoperability and scalable deployment. Experimental results show that the DT
Simulation Bridge enhances design agility, facilitates virtual commissioning,
and supports live behavioral analysis under realistic conditions, demonstrating
its effectiveness across a range of industrial scenarios.

</details>


### [5] [Towards Energy-Efficient Serverless Computing with Hardware Isolation](https://arxiv.org/abs/2510.08180)
*Natalie Carl,Tobias Pfandzelter,David Bermbach*

Main category: cs.DC

TL;DR: 提出重新设计无服务器硬件架构，使用硬件隔离替代软件隔离，为每个函数分配独立处理器，可显著降低能耗


<details>
  <summary>Details</summary>
Motivation: 当前无服务器平台在传统服务器硬件上运行数千个函数，需要昂贵的软件隔离机制和大量闲置服务器，导致能源效率低下

Method: 采用硬件隔离方法，为每个函数分配独立处理器，构建仅在实际工作时消耗能源的无服务器硬件堆栈

Result: 初步评估显示可减少90.63%的能耗开销，平均节省70.8兆瓦

Conclusion: 硬件隔离的无服务器架构能显著提升能源效率，使无服务器计算真正实现按需消耗能源

Abstract: Serverless computing provides just-in-time infrastructure provisioning with
rapid elasticity and a finely-grained pricing model. As full control of
resource allocation is in the hands of the cloud provider and applications only
consume resources when they actually perform work, we believe that serverless
computing is uniquely positioned to maximize energy efficiency.
  However, the focus of current serverless platforms is to run hundreds or
thousands of serverless functions from different tenants on traditional server
hardware, requiring expensive software isolation mechanisms and a high degree
of overprovisioning, i.e., idle servers, to anticipate load spikes. With shared
caches, high clock frequencies, and many-core architectures, servers today are
optimized for large, singular workloads but not to run thousands of isolated
functions.
  We propose rethinking the serverless hardware architecture to align it with
the requirements of serverless software. Specifically, we propose using
hardware isolation with individual processors per function instead of software
isolation resulting in a serverless hardware stack that consumes energy only
when an application actually performs work. In preliminary evaluation with real
hardware and a typical serverless workload we find that this could reduce
energy consumption overheads by 90.63% or an average 70.8MW.

</details>


### [6] [Distributed Resource Selection for Self-Organising Cloud-Edge Systems](https://arxiv.org/abs/2510.08228)
*Quentin Renau,Amjad Ullah,Emma Hart*

Main category: cs.DC

TL;DR: 提出了一种用于云边环境的分布式资源选择机制，通过分布式决策过程实现高效、可扩展和弹性的资源分配。


<details>
  <summary>Details</summary>
Motivation: 解决集中式协调在动态云边环境中成为瓶颈的问题，实现智能的实时应用部署和适应。

Method: 采用基于共识的机制，利用本地知识和代理间协作，无需中央控制器即可实现高效资源分配。

Result: 计算时间是影响分配决策的关键因素，该方法在保持最优性的同时实现快速分配，比集中式启发式方法快达30倍。

Conclusion: 该分布式机制为分布式编排系统奠定了基础，能够在大规模环境中实现及时的资源分配。

Abstract: This paper presents a distributed resource selection mechanism for diverse
cloud-edge environments, enabling dynamic and context-aware allocation of
resources to meet the demands of complex distributed applications. By
distributing the decision-making process, our approach ensures efficiency,
scalability, and resilience in highly dynamic cloud-edge environments where
centralised coordination becomes a bottleneck. The proposed mechanism aims to
function as a core component of a broader, distributed, and self-organising
orchestration system that facilitates the intelligent placement and adaptation
of applications in real-time. This work leverages a consensus-based mechanism
utilising local knowledge and inter-agent collaboration to achieve efficient
results without relying on a central controller, thus paving the way for
distributed orchestration. Our results indicate that computation time is the
key factor influencing allocation decisions. Our approach consistently delivers
rapid allocations without compromising optimality or incurring additional cost,
achieving timely results at scale where exhaustive search is infeasible and
centralised heuristics run up to 30 times slower.

</details>


### [7] [Energy-Efficient Maximal Independent Sets in Radio Networks](https://arxiv.org/abs/2510.08244)
*Dominick Banasik,Varsha Dani,Fabien Dufoulon,Aayush Gupta,Thomas P. Hayes,Gopal Pandurangan*

Main category: cs.DC

TL;DR: 本文提出了在无线电网络模型中更节能的最大独立集算法，分别针对有碰撞检测和无碰撞检测两种模型，实现了比现有算法更低的能量复杂度。


<details>
  <summary>Details</summary>
Motivation: 无线网络（特别是自组织和传感器网络）通常由电池供电，能量是宝贵资源。因此需要设计能量复杂度尽可能低的分布式算法来解决MIS问题。

Method: 针对CD模型提出了随机分布式MIS算法，能量复杂度为O(log n)；针对no-CD模型提出了能量复杂度为O(log²n log log n)的算法。两种算法都考虑了节点睡眠和唤醒的节能机制。

Result: CD模型：能量复杂度O(log n)（最优），轮复杂度O(log² n)，失败概率1/poly(n)。no-CD模型：能量复杂度O(log²n log log n)，轮复杂度O(log³ n log Δ)，显著优于现有O(log³ n)的算法。

Conclusion: 本文在无线电网络模型中实现了更节能的MIS算法，在CD模型中达到了最优能量复杂度，在no-CD模型中显著降低了能量消耗，为无线网络的节能设计提供了重要进展。

Abstract: The maximal independent set (MIS) is one of the most fundamental problems in
distributed computing, and it has been studied intensively for over four
decades. This paper focuses on the MIS problem in the Radio Network model, a
standard model widely used to model wireless networks, particularly ad hoc
wireless and sensor networks. Energy is a premium resource in these networks,
which are typically battery-powered. Hence, designing distributed algorithms
that use as little energy as possible is crucial. We use the well-established
energy model where a node can be sleeping or awake in a round, and only the
awake rounds (when it can send or listen) determine the energy complexity of
the algorithm, which we want to minimize.
  We present new, more energy-efficient MIS algorithms in radio networks with
arbitrary and unknown graph topology. We present algorithms for two popular
variants of the radio model -- with collision detection (CD) and without
collision detection (no-CD). Specifically, we obtain the following results:
  1. CD model: We present a randomized distributed MIS algorithm with energy
complexity $O(\log n)$, round complexity $O(\log^2 n)$, and failure probability
$1 / poly(n)$, where $n$ is the network size. We show that our energy
complexity is optimal by showing a matching $\Omega(\log n)$ lower bound.
  2. no-CD model: In the more challenging no-CD model, we present a randomized
distributed MIS algorithm with energy complexity $O(\log^2n \log \log n)$,
round complexity $O(\log^3 n \log \Delta)$, and failure probability $1 /
poly(n)$. The energy complexity of our algorithm is significantly lower than
the round (and energy) complexity of $O(\log^3 n)$ of the best known
distributed MIS algorithm of Davies [PODC 2023] for arbitrary graph topology.

</details>


### [8] [Investigating Matrix Repartitioning to Address the Over- and Undersubscription Challenge for a GPU-based CFD Solver](https://arxiv.org/abs/2510.08536)
*Gregor Olenik,Marcel Koch,Hartwig Anzt*

Main category: cs.DC

TL;DR: 提出了一种改进OpenFOAM中基于插件的GPU加速的重新分区策略，通过平衡CPU矩阵组装和GPU线性求解来缓解资源过度订阅问题。


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算越来越依赖GPU，但在复杂科学框架如OpenFOAM中集成GPU加速仍面临挑战。现有方法要么完全重构代码库，要么使用基于插件的GPU求解器，在性能和开发工作量之间存在权衡。

Method: 提出了重新分区策略，包括详细的计算模型、新颖的矩阵重新分区和更新程序，以更好地平衡CPU矩阵组装和基于GPU的线性求解。

Result: 在大型CFD模拟中，所提出的方法显著缓解了过度订阅问题，提高了异构CPU-GPU环境中的求解器性能和资源利用率。

Conclusion: 该重新分区策略有效改进了OpenFOAM中基于插件的GPU加速方法，在保持开发效率的同时提升了计算性能。

Abstract: Modern high-performance computing (HPC) increasingly relies on GPUs, but
integrating GPU acceleration into complex scientific frameworks like OpenFOAM
remains a challenge. Existing approaches either fully refactor the codebase or
use plugin-based GPU solvers, each facing trade-offs between performance and
development effort. In this work, we address the limitations of plugin-based
GPU acceleration in OpenFOAM by proposing a repartitioning strategy that better
balances CPU matrix assembly and GPU-based linear solves. We present a detailed
computational model, describe a novel matrix repartitioning and update
procedure, and evaluate its performance on large-scale CFD simulations. Our
results show that the proposed method significantly mitigates oversubscription
issues, improving solver performance and resource utilization in heterogeneous
CPU-GPU environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [How long can you sleep? Idle Time System Inefficiencies and Opportunities](https://arxiv.org/abs/2510.07449)
*Georgia Antoniou,Haris Volos,Jawad Haj Yahya,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: 该论文提出了一个基于排队论的模型框架，用于揭示运行延迟关键应用的现代服务器的空闲机会。通过比较实际服务器空闲时间与理论模型预测，发现存在大量未充分利用的深度空闲状态机会。


<details>
  <summary>Details</summary>
Motivation: 现代服务器在运行延迟关键应用时存在空闲时间利用不足的问题，这导致能源效率低下。需要量化分析这种空闲机会，为系统设计提供指导。

Method: 使用三种排队模型（M/M/1、cxM/M/1和M/M/c）来估计CPU核心和系统级别的理论空闲时间分布，并将实际服务器空闲情况与理论模型进行比较。

Result: 比较发现实际服务器与理论模型之间存在显著差异，揭示了大量未利用的深度空闲状态机会。这种低效归因于空闲管理器的准确性和传统深度空闲状态转换的高延迟。

Conclusion: 所提出的方法为早期设计探索提供了手段，能够洞察不同服务器系统配置和负载下的空闲时间行为和优化机会。

Abstract: This work introduces a model-based framework that reveals the idle
opportunity of modern servers running latency-critical applications.
Specifically, three queuing models, M/M/1, cxM/M/1, and M/M/c, are used to
estimate the theoretical idle time distribution at the CPU core and system
(package) level. A comparison of the actual idleness of a real server and that
from the theoretical models reveals significant missed opportunities to enter
deep idle states. This inefficiency is attributed to the idle-governor
inaccuracy and the high latency to transition to/from legacy deep-idle states.
The proposed methodology offers the means for an early-stage design exploration
and insights into idle time behavior and opportunities for varying server
system configurations and load.

</details>


### [10] [DL-PIM: Improving Data Locality in Processing-in-Memory Systems](https://arxiv.org/abs/2510.07719)
*Parker Hao Tian,Zahra Yousefijamarani,Alaa Alameldeen*

Main category: cs.AR

TL;DR: DL-PIM是一种新型PIM架构，通过动态检测数据移动开销并主动将数据移动到本地内存来改善数据局部性，使用分布式地址间接硬件查找表重定向流量，在HMC和HBM上分别实现了54%和50%的平均内存延迟降低。


<details>
  <summary>Details</summary>
Motivation: 传统PIM架构虽然能提高能效和性能，但其优势依赖于数据与处理单元的邻近性。数据移动开销会降低PIM的性能和能效，因为需要将数据从远程内存位置移动到内存内的处理单元进行计算。

Method: 提出DL-PIM架构，动态检测数据移动开销，主动将数据移动到请求处理单元的本地内存保留区域。使用分布式地址间接硬件查找表重定向流量到当前数据位置。在HMC和HBM两种3D堆叠内存上实现，并采用自适应机制评估间接访问的成本和收益，动态启用或禁用以防止对某些工作负载产生负面影响。

Result: DL-PIM在HMC上平均内存延迟降低54%，在HBM上降低50%。对于具有大量数据重用的工作负载，HMC性能提升15%，HBM提升5%。所有代表性工作负载在HMC上实现6%加速，在HBM上实现3%加速。

Conclusion: DL-PIM通过增强数据局部性提高了PIM架构的整体系统性能，证明了动态数据管理策略在减少数据移动开销方面的有效性。

Abstract: PIM architectures aim to reduce data transfer costs between processors and
memory by integrating processing units within memory layers. Prior PIM
architectures have shown potential to improve energy efficiency and
performance. However, such advantages rely on data proximity to the processing
units performing computations. Data movement overheads can degrade PIM's
performance and energy efficiency due to the need to move data between a
processing unit and a distant memory location. %they face challenges due to the
overhead of transferring data from remote memory locations to processing units
inside memory for computation. In this paper, we demonstrate that a large
fraction of PIM's latency per memory request is attributed to data transfers
and queuing delays from remote memory accesses. To improve PIM's data locality,
we propose DL-PIM, a novel architecture that dynamically detects the overhead
of data movement, and proactively moves data to a reserved area in the local
memory of the requesting processing unit. DL-PIM uses a distributed
address-indirection hardware lookup table to redirect traffic to the current
data location. We propose DL-PIM implementations on two 3D stacked memories:
HMC and HBM. While some workloads benefit from DL-PIM, others are negatively
impacted by the additional latency due to indirection accesses. Therefore, we
propose an adaptive mechanism that assesses the cost and benefit of indirection
and dynamically enables or disables it to prevent degrading workloads that
suffer from indirection. Overall, DL-PIM reduces the average memory latency per
request by 54% in HMC and 50% in HBM which resulted in performance improvement
of 15% for workloads with substantial data reuse in HMC and 5% in HBM. For all
representative workloads, DL-PIM achieved a 6% speedup in HMC and a 3% speedup
in HBM, showing that DL-PIM enhances data locality and overall system
performance.

</details>


### [11] [A Scalable FPGA Architecture With Adaptive Memory Utilization for GEMM-Based Operations](https://arxiv.org/abs/2510.08137)
*Anastasios Petropoulos,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA的可动态配置DNN推理加速器架构，采用脉动阵列、高带宽内存和UltraRAM，支持多种处理单元配置，并通过启发式权重传输调度实现高吞吐效率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络推理越来越依赖专用硬件以获得高计算效率，需要设计灵活可配置的加速器架构来适应不同模型和未来FPGA设计。

Method: 使用FPGA构建动态可配置加速器，包含脉动阵列、高带宽内存和UltraRAM；设计两种不同计算能力的处理单元配置；采用多PU实例化和启发式权重传输调度策略；可扩展模拟模拟内存计算设备。

Result: 该架构相比先前工作实现了显著的吞吐效率提升，并能模拟AIMC设备以辅助下一代异构AIMC芯片设计。

Conclusion: 该工作提出了一种适用于各种模型和未来FPGA设计的通用DNN推理加速架构，具有高度灵活性和可扩展性。

Abstract: Deep neural network (DNN) inference relies increasingly on specialized
hardware for high computational efficiency. This work introduces a
field-programmable gate array (FPGA)-based dynamically configurable accelerator
featuring systolic arrays, high-bandwidth memory, and UltraRAMs. We present two
processing unit (PU) configurations with different computing capabilities using
the same interfaces and peripheral blocks. By instantiating multiple PUs and
employing a heuristic weight transfer schedule, the architecture achieves
notable throughput efficiency over prior works. Moreover, we outline how the
architecture can be extended to emulate analog in-memory computing (AIMC)
devices to aid next-generation heterogeneous AIMC chip designs and investigate
device-level noise behavior. Overall, this brief presents a versatile DNN
inference acceleration architecture adaptable to various models and future FPGA
designs.

</details>


### [12] [SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM Inference](https://arxiv.org/abs/2510.08544)
*Hengrui Zhang,Pratyush Patel,August Ning,David Wentzlaff*

Main category: cs.AR

TL;DR: SPAD提出专门针对LLM推理中prefill和decode两个阶段的专用硬件设计，通过减少资源浪费来降低服务成本


<details>
  <summary>Details</summary>
Motivation: 现有数据中心GPU/TPU采用"越多越好"的设计理念，导致prefill阶段内存带宽利用不足，decode阶段计算资源利用不足，增加了LLM推理服务成本

Method: 设计专门的Prefill芯片（更大的systolic阵列和成本效益高的GDDR内存）和Decode芯片（保持高内存带宽但减少计算容量），采用"少即是多"的方法论

Result: 相比模拟的H100，Prefill芯片平均prefill性能提升8%，硬件成本降低52%；Decode芯片达到97%的decode性能，TDP降低28%。端到端模拟显示硬件成本降低19%-41%，TDP降低2%-17%

Conclusion: SPAD通过专门化硬件设计显著降低了LLM推理成本，即使模型和工作负载变化，也能通过重新分配芯片类型实现11%-43%的硬件成本降低，证明了设计的长期适用性

Abstract: Large Language Models (LLMs) have gained popularity in recent years, driving
up the demand for inference. LLM inference is composed of two phases with
distinct characteristics: a compute-bound prefill phase followed by a
memory-bound decode phase. To efficiently serve LLMs, prior work proposes
prefill-decode disaggregation to run each phase on separate hardware. However,
existing hardware poorly matches the different requirements of each phase.
Current datacenter GPUs and TPUs follow a more-is-better design philosophy that
maximizes compute and memory resources, causing memory bandwidth
underutilization in the prefill phase and compute underutilization in the
decode phase. Such underutilization directly translates into increased serving
costs.
  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting
a less-is-more methodology to design specialized chips tailored to the distinct
characteristics of prefill and decode phases. The proposed Prefill Chips have
larger systolic arrays and use cost-effective GDDR memory, whereas the proposed
Decode Chips retain high memory bandwidth but reduce compute capacity. Compared
to modeled H100s, simulations show that the proposed Prefill Chips deliver 8%
higher prefill performance on average at 52% lower hardware cost, while the
proposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.
  End-to-end simulations on production traces show that SPAD reduces hardware
cost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while
offering the same performance. Even when models and workloads change, SPAD can
reallocate either type of chip to run either phase and still achieve 11%-43%
lower hardware costs, demonstrating the longevity of the SPAD design.

</details>


### [13] [FMCache: File-System Metadata Caching in Programmable Switches](https://arxiv.org/abs/2510.08351)
*Qingxiu Liu,Jiazhen Cai,Siyuan Sheng,Yuhui Chen,Lu Tang,Zhirong Shen,Patrick P. C. Lee*

Main category: cs.AR

TL;DR: FMCache是一个利用可编程交换机在数据平面直接服务文件系统元数据请求的框架，解决了多客户端场景下元数据缓存一致性的问题，显著提升了分布式文件系统的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 分布式文件系统中，跨多个元数据服务器的快速可扩展元数据管理对于处理大量文件和目录至关重要。客户端缓存虽然能减轻服务器负载，但在客户端数量增加时会带来显著的缓存一致性维护开销和复杂性。

Method: 提出FMCache框架，利用可编程交换机在交换机数据平面直接服务来自多个客户端的文件系统元数据请求。与先前的交换机内键值缓存方法不同，FMCache在严格的交换机资源约束下解决了文件系统特定的路径依赖问题。

Result: 在Hadoop HDFS上实现FMCache，并在Tofino交换机测试平台上使用真实世界文件系统元数据工作负载进行评估。FMCache相比原始HDFS实现了高达181.6%的吞吐量提升，与客户端缓存结合时还能带来额外高达139.6%的吞吐量增益。同时保持了低延迟和有限的交换机资源使用。

Conclusion: FMCache通过利用可编程交换机实现了高效的元数据缓存，显著提升了分布式文件系统的性能，同时解决了多客户端场景下的缓存一致性问题。

Abstract: Fast and scalable metadata management across multiple metadata servers is
crucial for distributed file systems to handle numerous files and directories.
Client-side caching of frequently accessed metadata can mitigate server loads,
but incurs significant overhead and complexity in maintaining cache consistency
when the number of clients increases. We propose FMCache, an in-switch
file-system metadata caching framework that leverages programmable switches to
serve file-system metadata requests from multiple clients directly in the
switch data plane. Unlike prior in-switch key-value caching approaches, FMCache
addresses file-system-specific path dependencies under stringent switch
resource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on
a Tofino-switch testbed using real-world file-system metadata workloads.
FMCache achieves up to 181.6% higher throughput than vanilla HDFS and
complements client-side caching with additional throughput gains of up to
139.6%. It also incurs low latencies and limited switch resource usage.

</details>
