<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Filling the Gaps of Polarity: Implementing Dependent Data and Codata Types with Implicit Arguments](https://arxiv.org/abs/2511.15819)
*Bohdan Liesnikov,David Binder,Tim Süberkrüb*

Main category: cs.PL

TL;DR: 本文为Polarity语言提供了完整的算法类型系统和隐式参数推断算法，支持归纳类型和余归纳类型的对称处理，解决了表达式问题中的两种扩展性权衡。


<details>
  <summary>Details</summary>
Motivation: 解决依赖类型语言中归纳类型和余归纳类型支持不对称的问题，为Polarity语言提供隐式参数等现代化特性，同时保持语言核心对称性。

Method: 设计了完整的算法类型系统描述、统一算法、归约语义、转换检查和模式匹配统一算法，覆盖任意归纳和余归纳类型。

Result: 提供了Polarity语言的完整算法实现基础，包括工作进展中的实现，统一算法设计可作为其他依赖类型语言的蓝图。

Conclusion: 成功为Polarity语言开发了尊重核心对称性的隐式参数算法类型系统，其统一算法设计对其他支持对称归纳和余归纳类型的依赖类型语言具有参考价值。

Abstract: The expression problem describes a fundamental tradeoff between two types of extensibility: extending a type with new operations, such as by pattern matching on an algebraic data type in functional programming, and extending a type with new constructors, such as by adding a new object implementing an interface in object-oriented programming. Most dependently typed languages have good support for the former style through inductive types, but support for the latter style through coinductive types is usually much poorer. Polarity is a language that treats both kinds of types symmetrically and allows the developer to switch between type representations.However, it currently lacks several features expected of a state-of-the-art dependently typed language, such as implicit arguments. The central aim of this paper is to provide an algorithmic type system and inference algorithm for implicit arguments that respect the core symmetry of the language. Our work provides two key contributions: a complete algorithmic description of the type system backing Polarity, and a comprehensive description of a unification algorithm that covers arbitrary inductive and coinductive types. We give rules for reduction semantics, conversion checking, and a unification algorithm for pattern-matching, which are essential for a usable implementation. A work-in-progress implementation of the algorithms in this paper is available at https://polarity-lang.github.io/. We expect that the comprehensive account of the unification algorithm and our design decisions can serve as a blueprint for other dependently typed languages that support inductive and coinductive types symmetrically.

</details>


### [2] [Chorex: Restartable, Language-Integrated Choreographies](https://arxiv.org/abs/2511.15820)
*Ashton Wiersdorf,Ben Greenman*

Main category: cs.PL

TL;DR: Chorex是一个将编排编程引入Elixir的语言，通过元编程实现完整的编排功能，支持容错机制，在演员崩溃时自动恢复状态并更新网络配置。


<details>
  <summary>Details</summary>
Motivation: 构建健壮的分布式应用程序，通过编排编程确保分布式系统中的容错性和可靠性。

Method: 使用元编程在Elixir中实现编排语言，支持演员故障恢复（生成新进程、状态检查点恢复、网络配置更新），并通过静态分析检测编排需求与演员实现的不匹配。

Result: 成功实现了Chorex语言，在多个案例（如高阶书商和安全远程密码协议）中验证了其有效性，测量了检查点的开销。

Conclusion: Chorex的输出无状态函数集的投影策略是支持可重启演员的可行方法，可推广到其他语言。

Abstract: We built Chorex, a language that brings choreographic programming to Elixir as a path toward robust distributed applications. Chorex is unique among choreographic languages because it tolerates failure among actors: when an actor crashes, Chorex spawns a new process, restores state using a checkpoint, and updates the network configuration for all actors. Chorex also proves that full-featured choreographies can be implemented via metaprogramming, and that doing so achieves tight integration with the host language. For example, mismatches between choreography requirements and an actor implementation are reported statically and in terms of source code rather than macro-expanded code. This paper illustrates Chorex on several examples, ranging from a higher-order bookseller to a secure remote password protocol, details its implementation, and measures the overhead of checkpointing. We conjecture that Chorex's projection strategy, which outputs sets of stateless functions, is a viable approach for other languages to support restartable actors.

</details>


### [3] [BlueScript: A Disaggregated Virtual Machine for Microcontrollers](https://arxiv.org/abs/2511.15821)
*Fumika Mochizuki,Tetsuro Yamazaki,Shigeru Chiba*

Main category: cs.PL

TL;DR: 提出了一种分解式虚拟机，将尽可能多的组件卸载到主机上，为内存受限的微控制器提供丰富的功能特性。


<details>
  <summary>Details</summary>
Motivation: 微控制器虚拟机由于内存限制功能有限，现有虚拟机缺乏交互响应性和高执行速度，需要一种方法来利用主机丰富资源扩展微控制器虚拟机功能。

Method: 设计并实现了BlueScript虚拟机，将大部分组件卸载到主机，使用影子机器数据结构来镜像微控制器的执行状态以减少通信开销。

Result: 组件卸载不会严重损害预期收益，卸载的增量编译器比MicroPython和Espruino执行速度更快，同时保持与MicroPython相当的交互性，动态编译器提升了虚拟机性能。

Conclusion: 证明了即使在内存受限的微控制器虚拟机上也能提供丰富功能的可行性。

Abstract: Virtual machines (VMs) are highly beneficial for microcontroller development. 
In particular, interactive programming environments greatly facilitate iterative development processes, 
and higher execution speeds expand the range of applications that can be developed. 
However, due to their limited memory size, microcontroller VMs provide a limited set of features. 
Widely used VMs for microcontrollers often lack interactive responsiveness and/or high execution speed. 
While researchers have investigated offloading certain VM components to other machines,the types of components that can be offloaded are still restricted. 
In this paper, we propose a disaggregated VM that offloads as many components as possible to a host machine. 
This makes it possible to exploit the abundant memory of the host machine and its powerful processing capability to provide rich features through the VM. 
As an instance of a disaggregated VM, we design and implement a BlueScript VM. 
The BlueScript VM is a virtual machine for microcontrollers that provides an interactive development environment. 
We offload most of the components of the BlueScript VM to a host machine. 
To reduce communication overhead between the host machine and the microcontroller,  
we employed a data structure called a shadow machine on the host machine, 
which mirrors the execution state of the microcontroller. 
Through our experiments, we confirmed that offloading components does not seriously compromise their expected benefits.  
We assess that an offloaded incremental compiler results in faster execution speed than MicroPython and Espruino,  
while keeping interactivity comparable with MicroPython.  
In addition, our experiments observe that the offloaded dynamic compiler improves VM performance. 
Through this investigation, we demonstrate the feasibility of providing rich features even on VMs for memory-limited microcontrollers.

</details>


### [4] [Operon: Incremental Construction of Ragged Data via Named Dimensions](https://arxiv.org/abs/2511.16080)
*Sungbin Moon,Jiho Park,Suyoung Hwang,Donghyun Koh,Seunghyun Moon,Minhyeong Lee*

Main category: cs.PL

TL;DR: Operon是一个基于Rust的工作流引擎，专门处理不规则数据，通过命名维度和显式依赖关系提供静态验证和动态调度，在机器学习数据生成管道中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有工作流引擎缺乏对不规则数据的原生支持，用户需要手动管理复杂的索引和依赖关系，这在自然语言处理、科学测量和自主AI代理等领域造成困难。

Method: 提出命名维度的形式化方法，提供领域特定语言声明带维度注解的管道，静态验证正确性，运行时系统根据数据形状动态调度任务，采用每任务多队列架构实现高效并行。

Result: 实验评估显示Operon比现有工作流引擎减少14.94倍基线开销，在扩展工作负载时保持接近线性的端到端输出率。

Conclusion: Operon通过显式建模部分已知状态实现鲁棒的持久化和恢复机制，特别适用于机器学习应用中的大规模数据生成管道。

Abstract: Modern data processing workflows frequently encounter ragged data: collections with variable-length elements that arise naturally in domains like natural language processing, scientific measurements, and autonomous AI agents. Existing workflow engines lack native support for tracking the shapes and dependencies inherent to ragged data, forcing users to manage complex indexing and dependency bookkeeping manually. We present Operon, a Rust-based workflow engine that addresses these challenges through a novel formalism of named dimensions with explicit dependency relations. Operon provides a domain-specific language where users declare pipelines with dimension annotations that are statically verified for correctness, while the runtime system dynamically schedules tasks as data shapes are incrementally discovered during execution. We formalize the mathematical foundation for reasoning about partial shapes and prove that Operon's incremental construction algorithm guarantees deterministic and confluent execution in parallel settings. The system's explicit modeling of partially-known states enables robust persistence and recovery mechanisms, while its per-task multi-queue architecture achieves efficient parallelism across heterogeneous task types. Empirical evaluation demonstrates that Operon outperforms an existing workflow engine with 14.94x baseline overhead reduction while maintaining near-linear end-to-end output rates as workloads scale, making it particularly suitable for large-scale data generation pipelines in machine learning applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [A Scalable NorthPole System with End-to-End Vertical Integration for Low-Latency and Energy-Efficient LLM Inference](https://arxiv.org/abs/2511.15950)
*Michael V. DeBole,Rathinakumar Appuswamy,Neil McGlohon,Brian Taba,Steven K. Esser,Filipp Akopyan,John V. Arthur,Arnon Amir,Alexander Andreopoulos,Peter J. Carlson,Andrew S. Cassidy,Pallab Datta,Myron D. Flickner,Rajamohan Gandhasri,Guillaume J. Garreau,Megumi Ito,Jennifer L. Klamo,Jeffrey A. Kusnitz,Nathaniel J. McClatchey,Jeffrey L. McKinstry,Tapan K. Nayak,Carlos Ortega Otero,Hartmut Penner,William P. Risk,Jun Sawada,Jay Sivagnaname,Daniel F. Smith,Rafael Sousa,Ignacio Terrizzano,Takanori Ueda,Trent Gray-Donald,David Cox,Dharmendra S. Modha*

Main category: cs.DC

TL;DR: 一个端到端的垂直集成研究原型系统，结合288个NorthPole神经推理加速器卡，提供可扩展高效的云推理服务，支持多种模型规模和上下文长度。


<details>
  <summary>Details</summary>
Motivation: 为企业在现有数据中心环境中部署AI应用提供可扩展、模块化和可重新配置的推理服务解决方案。

Method: 集成288个NorthPole神经推理加速器卡，结合离线训练算法、高性能运行时堆栈和容器化推理流水线。

Result: 系统提供115 peta-ops的4位整数精度计算能力和3.7 PB/s内存带宽，功耗仅30kW，可在42U机架空间内运行多个模型实例。

Conclusion: 该系统展示了在现有数据中心环境中部署企业级AI应用的可行性和效率，支持从30亿到700亿参数的各种模型规模。

Abstract: A vertically integrated, end-to-end, research prototype system combines 288 NorthPole neural inference accelerator cards, offline training algorithms, a high-performance runtime stack, and a containerized inference pipeline to deliver a scalable and efficient cloud inference service. The system delivers 115 peta-ops at 4-bit integer precision and 3.7 PB/s of memory bandwidth across 18 2U servers, while consuming only 30 kW of power and weighing 730 kg in a 0.67 m^2 42U rack footprint. The system can run 3 simultaneous instances of the 8-billion-parameter open-source IBM Granite-3.3-8b-instruct model at 2,048 context length with 28 simultaneous users and a per-user inter-token latency of 2.8 ms. The system is scalable, modular, and reconfigurable, supporting various model sizes and context lengths, and is ideal for deploying agentic workflows for enterprise AI applications in existing data center (cloud, on-prem) environments. For example, the system can support 18 instances of a 3-billion-parameter model or a single instance of a 70-billion-parameter model.

</details>


### [6] [Optimizing Communication in Byzantine Agreement Protocols with Slim-HBBFT](https://arxiv.org/abs/2511.15957)
*Nasit S Sony,Xianzhong Ding*

Main category: cs.DC

TL;DR: Slim-HBBFT是一种异步原子广播协议，通过仅考虑部分节点的请求来降低通信复杂度，相比传统协议提升O(n)倍效率


<details>
  <summary>Details</summary>
Motivation: 传统异步拜占庭协议需要广播所有节点的请求，通信开销大。当请求重复时，这种昂贵协议的开销不合理。需要设计更高效的协议来减少通信负担。

Method: 提出优先可证明广播(P-PB)协议，只为选定节点生成广播证明。基于P-PB设计Slim-HBBFT原子广播协议，仅考虑n个节点中的部分请求。

Result: Slim-HBBFT将通信复杂度降低了O(n)倍，同时满足异步公共子集协议的安全属性要求。

Conclusion: Slim-HBBFT通过选择性处理节点请求，在保持安全性的同时显著降低了异步原子广播的通信开销，为拜占庭协议提供了更高效的解决方案。

Abstract: Byzantine agreement protocols in asynchronous networks have received renewed interest because they do not rely on network behavior to achieve termination. Conventional asynchronous Byzantine agreement protocols require every party to broadcast its requests (e.g., transactions), and at the end of the protocol, parties agree on one party's request. If parties agree on one party's requests while exchanging every party's request, the protocol becomes expensive. These protocols are used to design an atomic broadcast (ABC) protocol where parties agree on $\langle n-f \rangle$ parties' requests (assuming $n=3f+1$, where $n$ is the total number of parties, and $f$ is the number of Byzantine parties). Although the parties agree on a subset of requests in the ABC protocol, if the requests do not vary (are duplicated), investing in a costly protocol is not justified. We propose Slim-HBBFT, an atomic broadcast protocol that considers requests from a fraction of $n$ parties and improves communication complexity by a factor of $O(n)$. At the core of our design is a prioritized provable-broadcast (P-PB) protocol that generates proof of broadcast only for selected parties. We use the P-PB protocol to design the Slim-HBBFT atomic broadcast protocol. Additionally, we conduct a comprehensive security analysis to demonstrate that Slim-HBBFT satisfies the properties of the Asynchronous Common Subset protocol, ensuring robust security and reliability.

</details>


### [7] [Efficient Chromosome Parallelization for Precision Medicine Genomic Workflows](https://arxiv.org/abs/2511.15977)
*Daniel Mas Montserrat,Ray Verma,Míriam Barrabés,Francisco M. de la Vega,Carlos D. Bustamante,Alexander G. Ioannidis*

Main category: cs.DC

TL;DR: 提出多种自适应内存优化方法，用于并行化染色体级生物信息学工作流，通过预测内存使用和优化任务调度来减少内存溢出和提高资源利用率。


<details>
  <summary>Details</summary>
Motivation: 大规模基因组工作流处理数十到数百GB数据时会出现高内存峰值、磁盘I/O密集和内存不足错误，静态资源分配方法无法处理染色体间RAM需求变化，导致资源利用率低和运行时间长。

Method: 1. 开发符号回归模型估计每染色体内存消耗并引入插值偏置保守最小化过度分配；2. 提出动态调度器使用多项式回归模型自适应预测RAM使用，将任务打包视为背包问题；3. 提出静态调度器优化染色体处理顺序以最小化峰值内存。

Result: 在模拟和真实基因组流程中评估，新方法减少了内存溢出并在线程间平衡负载，实现了更快的端到端执行。

Conclusion: 所提方法展示了优化大规模基因组工作流的潜力，通过内存高效并行化提高执行效率。

Abstract: Large-scale genomic workflows used in precision medicine can process datasets spanning tens to hundreds of gigabytes per sample, leading to high memory spikes, intensive disk I/O, and task failures due to out-of-memory errors. Simple static resource allocation methods struggle to handle the variability in per-chromosome RAM demands, resulting in poor resource utilization and long runtimes. In this work, we propose multiple mechanisms for adaptive, RAM-efficient parallelization of chromosome-level bioinformatics workflows. First, we develop a symbolic regression model that estimates per-chromosome memory consumption for a given task and introduces an interpolating bias to conservatively minimize over-allocation. Second, we present a dynamic scheduler that adaptively predicts RAM usage with a polynomial regression model, treating task packing as a Knapsack problem to optimally batch jobs based on predicted memory requirements. Additionally, we present a static scheduler that optimizes chromosome processing order to minimize peak memory while preserving throughput. Our proposed methods, evaluated on simulations and real-world genomic pipelines, provide new mechanisms to reduce memory overruns and balance load across threads. We thereby achieve faster end-to-end execution, showcasing the potential to optimize large-scale genomic workflows.

</details>


### [8] [Can Asymmetric Tile Buffering Be Beneficial?](https://arxiv.org/abs/2511.16041)
*Chengyue Wang,Wesley Pang,Xinrui Wu,Gregory Jun,Luis Romero,Endri Taka,Diana Marculescu,Tony Nowatzki,Pranathi Vasireddy,Joseph Melber,Deming Chen,Jason Cong*

Main category: cs.DC

TL;DR: 本文提出非对称瓦片缓冲(ATB)技术，通过解耦输入和输出操作数的缓冲瓦片维度，显著提升GEMM计算性能，在AMD XDNA2 AIE上实现4.54倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统对称瓦片缓冲方法限制了GEMM计算效率，需要一种能够提高算术强度而不显著增加开销的新技术。

Method: 开发非对称瓦片缓冲(ATB)技术，解耦输入和输出操作数的缓冲瓦片维度，并建立包含ATB收益和开销的性能模型来指导瓦片因子选择。

Result: 在AMD XDNA2 AIE上应用ATB，混合精度BFP16-BF16 GEMM性能从4.8 TFLOPS提升至24.6 TFLOPS，创下XDNA2 AIE性能新纪录。

Conclusion: ATB是一种简单而强大的技术，首次证明其实用性和显著效益，为现代AI工作负载中的GEMM计算提供了有效的优化方法。

Abstract: General matrix multiplication (GEMM) is the computational backbone of modern AI workloads, and its efficiency is critically dependent on effective tiling strategies. Conventional approaches employ symmetric tile buffering, where the buffered tile size of the input $A$ along the dimension $M$ matches the output tile size of $C$.
  In this paper, we introduce asymmetric tile buffering (ATB), a simple but powerful technique that decouples the buffered tile dimensions of the input and output operands. We show, for the first time, that ATB is both practical and highly beneficial. To explain this effect, we develop a performance model that incorporates both the benefits of ATB (higher arithmetic intensity) and its overheads (higher kernel switching costs), providing insight into how to select effective ATB tiling factors. As a case study, we apply ATB to AMD's latest XDNA2 AI Engine (AIE), achieving up to a 4.54x speedup, from 4.8 to 24.6 TFLOPS on mixed-precision BFP16--BF16 GEMM, establishing a new performance record for XDNA2 AIE.

</details>


### [9] [Mitigating Shared Storage Congestion Using Control Theory](https://arxiv.org/abs/2511.16177)
*Thomas Collignon,Kouds Halitim,Raphaël Bleuse,Sophie Cerf,Bogdan Robu,Éric Rutten,Lionel Seinturier,Alexandre van Kempen*

Main category: cs.DC

TL;DR: 提出基于控制理论的自适应方法，动态调节客户端I/O速率以解决HPC系统中I/O拥塞问题，减少总运行时间达20%并降低尾延迟。


<details>
  <summary>Details</summary>
Motivation: 传统I/O栈优化方法通常针对特定工作负载且需要专业知识，难以通用化；在共享HPC环境中，资源拥塞会导致性能不可预测，造成减速和超时。

Method: 基于控制理论的自适应方法，利用少量运行时系统负载指标动态调节客户端I/O速率，减少拥塞并提升性能稳定性。

Result: 在多节点集群上实现控制器并在真实测试平台上评估，实验结果显示能有效缓解I/O拥塞，减少总运行时间达20%，降低尾延迟，同时保持稳定性能。

Conclusion: 基于控制理论的自适应I/O速率调节方法能够有效解决HPC系统中的I/O拥塞问题，提高性能稳定性。

Abstract: Efficient data access in High-Performance Computing (HPC) systems is essential to the performance of intensive computing tasks. Traditional optimizations of the I/O stack aim to improve peak performance but are often workload specific and require deep expertise, making them difficult to generalize or re-use. In shared HPC environments, resource congestion can lead to unpredictable performance, causing slowdowns and timeouts. To address these challenges, we propose a self-adaptive approach based on Control Theory to dynamically regulate client-side I/O rates. Our approach leverages a small set of runtime system load metrics to reduce congestion and enhance performance stability. We implement a controller in a multi-node cluster and evaluate it on a real testbed under a representative workload. Experimental results demonstrate that our method effectively mitigates I/O congestion, reducing total runtime by up to 20% and lowering tail latency, while maintaining stable performance.

</details>


### [10] [Fast LLM Post-training via Decoupled and Best-of-N Speculation](https://arxiv.org/abs/2511.16193)
*Rongxin Cheng,Kai Zhou,Xingda Wei,Siyuan Liu,Mingcong Han,Mingjing Ai,Yeju Zhou,Baoquan Zhong,Wencong Xiao,Xin Liu,Rong Chen,Haibo Chen*

Main category: cs.DC

TL;DR: SpecActor通过动态解耦推测和动态Best-of-N推测方法，在大型语言模型后训练中实现1.3-1.7倍的加速，解决了推测解码在大批量执行中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型后训练中的rollout阶段占用了大量训练时间，而推测解码技术在大批量执行配置下效率不高，需要解决GPU计算效率和推测准确性的挑战。

Method: 采用动态解耦推测执行方法最大化GPU计算效率，以及动态Best-of-N推测方法根据rollout进度选择和组合不同的草稿方法，提高推测准确性。

Result: SpecActor比常见的后训练基线快1.3-1.7倍，比简单采用推测解码的rollout快1.3-1.5倍。

Conclusion: SpecActor通过创新的推测执行方法，有效解决了大规模语言模型后训练中rollout阶段的性能瓶颈问题。

Abstract: Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. SpecActor achieves fast rollout with speculative decoding that deploys a fast path (e.g., a smaller model) to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges in speculative rollout by (1) a \emph{dynamic decoupled speculation} execution method that maximizes the GPU computational efficiency to realize speedup for large-batch execution -- a configuration common in training but unfriendly to speculative execution and (2) a \emph{dynamic Best-of-N speculation} method that selects and combines different drafting methods according to the rollout progress. It substantially improves the speculation accuracy even when the best drafting method is unknown a priori, meanwhile without requiring adding extra computation resources. {\sys} is {1.3--1.7}\,$\times$ faster than common post-training baselines, and is {1.3--1.5}\,$\times$ faster compared to naively adopting speculative decoding for rollout.

</details>


### [11] [Optimizing Federated Learning in the Era of LLMs: Message Quantization and Streaming](https://arxiv.org/abs/2511.16450)
*Ziyue Xu,Zhihong Zhang,Holger R. Roth,Chester Chen,Yan Cheng,Andrew Feng*

Main category: cs.DC

TL;DR: NVIDIA FLARE通过量化和流式传输技术解决联邦学习中大语言模型的通信和内存限制问题


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临通信开销和本地资源限制的挑战，特别是在数十亿参数的大语言模型时代，模型规模加剧了内存和通信约束

Method: 在现有大对象流式传输解决方案基础上，通过消息量化和容器/文件流式传输两种关键技术来增强联邦学习工作流

Result: 这些技术显著提升了联邦学习与大语言模型的鲁棒性和效率，在现实联邦学习场景中实现了更好的性能

Conclusion: NVIDIA FLARE的先进通信能力通过量化和流式传输有效解决了大语言模型在联邦学习中的可扩展性和集成问题

Abstract: Federated Learning (FL) offers a promising solution for training machine learning models across distributed data sources while preserving data privacy. However, FL faces critical challenges related to communication overhead and local resource constraints, especially in the era of Large Language Models (LLMs) with billions of parameters. The sheer size of these models exacerbates both memory and communication constraints, making efficient transmission and processing essential for practical deployment. NVIDIA FLARE, an open-source SDK for federated learning, addresses these challenges by introducing advanced communication capabilities. Building upon existing solutions for large object streaming, we enhance FL workflows for LLMs through two key techniques: message quantization and container/file streaming. Quantization reduces message size, while streaming enables efficient memory management, improving scalability and integration with existing workflows. These advancements significantly enhance the robustness and efficiency of FL with LLMs, ensuring better performance in real-world federated learning scenarios.

</details>


### [12] [Distributed MIS Algorithms for Rational Agents using Games](https://arxiv.org/abs/2511.16533)
*Nithin Salevemula,Shreyas Pai*

Main category: cs.DC

TL;DR: 该论文研究了在分布式网络中计算最大独立集（MIS）的问题，其中节点是理性代理，其收益取决于是否加入MIS。作者提出了两种基于效用模型的算法，通过邻居节点间的成对交互生成随机性，确保在理性行为下仍能打破对称性。


<details>
  <summary>Details</summary>
Motivation: 传统分布式算法假设节点遵循协议，但在理性环境中，节点可能为了增加期望效用而偏离协议。标准MIS算法依赖诚实随机性或唯一标识符，但理性代理可能操纵随机性，仅依赖标识符会导致不公平，使某些节点加入MIS的概率为零，从而缺乏参与动机。

Method: 提出了两种基于效用模型的算法，其中随机性通过邻居节点间的成对交互生成，这些交互被视为简单游戏，单个节点无法单方面影响结果。这种方法在保持对称性打破的同时与理性行为兼容。

Result: 证明在算法执行的每个阶段，给定任何历史，假设其他节点遵循算法，没有代理能通过单边偏离增加期望效用。当所有节点遵循协议时，每个节点都有正概率加入MIS，最终输出正确的MIS。在温和附加假设下，两种算法都以高概率在O(log n)轮内终止。

Conclusion: 所提出的算法在理性分布式环境中提供了强于颤抖手完美均衡的保证，确保了公平性和正确性，同时保持了高效的时间复杂度。

Abstract: We study the problem of computing a Maximal Independent Set (MIS) in distributed networks where each node is a rational agent whose payoff depends on whether it joins the MIS. Classical distributed algorithms assume that nodes follow the prescribed protocol, but this assumption fails when nodes are strategic and may deviate if doing so increases their expected utility.
  Standard MIS algorithms rely on honest randomness or unique identifiers to break symmetry. In rational settings, however, agents may manipulate randomness, and relying solely on identifiers can create unfairness, giving some nodes zero probability of joining the MIS and thus no incentive to participate. To address these issues, we propose two algorithms based on a utility model in which agents seek locally correct solutions while also having preferences over which solution is chosen. Randomness in our algorithms is generated through pairwise interactions between neighboring nodes, viewed as simple games in which no single node can unilaterally affect the outcome. This allows symmetry breaking while remaining compatible with rational behavior.
  For both algorithms, we prove that at every stage of the execution, given any history, no agent can increase its expected utility through a unilateral deviation, assuming others follow the algorithm. This gives a stronger guarantee than Trembling-Hand Perfect Equilibrium. When all nodes follow the protocol, every node has a positive probability of joining the MIS, and the final output is a correct MIS. Under mild additional assumptions, both algorithms terminate in $O(\log n)$ rounds with high probability.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [CIMinus: Empowering Sparse DNN Workloads Modeling and Exploration on SRAM-based CIM Architectures](https://arxiv.org/abs/2511.16368)
*Yingjie Qi,Jianlei Yang,Rubing Yang,Cenlin Duan,Xiaolin He,Ziyan He,Weitao Pan,Weisheng Zhao*

Main category: cs.AR

TL;DR: CIMinus是一个专门用于CIM架构上稀疏DNN工作负载成本建模的框架，提供组件级能耗分析和整体延迟评估。


<details>
  <summary>Details</summary>
Motivation: CIM系统在利用稀疏性方面面临挑战，缺乏统一的系统化视图和建模方法来处理多样化的稀疏DNN工作负载。

Method: 提出CIMinus框架，在组件级别进行能耗分析，评估整体工作负载延迟，并在当代CIM架构上验证其有效性。

Result: 验证了CIMinus在当代CIM架构上的适用性，通过两个用例展示了稀疏模式影响和映射策略有效性。

Conclusion: CIMinus填补了理论设计与实际实现之间的差距，为稀疏DNN在CIM系统中的优化提供了系统化建模方法。

Abstract: Compute-in-memory (CIM) has emerged as a pivotal direction for accelerating workloads in the field of machine learning, such as Deep Neural Networks (DNNs). However, the effective exploitation of sparsity in CIM systems presents numerous challenges, due to the inherent limitations in their rigid array structures. Designing sparse DNN dataflows and developing efficient mapping strategies also become more complex when accounting for diverse sparsity patterns and the flexibility of a multi-macro CIM structure. Despite these complexities, there is still an absence of a unified systematic view and modeling approach for diverse sparse DNN workloads in CIM systems. In this paper, we propose CIMinus, a framework dedicated to cost modeling for sparse DNN workloads on CIM architectures. It provides an in-depth energy consumption analysis at the level of individual components and an assessment of the overall workload latency. We validate CIMinus against contemporary CIM architectures and demonstrate its applicability in two use-cases. These cases provide valuable insights into both the impact of sparsity patterns and the effectiveness of mapping strategies, bridging the gap between theoretical design and practical implementation.

</details>


### [14] [Unsupervised Graph Neural Network Framework for Balanced Multipatterning in Advanced Electronic Design Automation Layouts](https://arxiv.org/abs/2511.16374)
*Abdelrahman Helaly,Nourhan Sakr,Kareem Madkour,Ilhami Torunoglu*

Main category: cs.AR

TL;DR: 提出了一种混合工作流，将多模式分解建模为约束图着色问题，主要目标是最小化特征违规，次要目标是平衡每个掩模上的特征数量。


<details>
  <summary>Details</summary>
Motivation: 虽然基于启发式的回溯和SAT求解器可以应对多模式分解挑战，但它们往往难以同时处理复杂约束和次要目标。

Method: 集成两个主要组件：(1) 无监督训练的GNN代理生成初始颜色预测，(2) 结合GNN启发式和模拟退火的优化策略来提升解决方案质量和平衡性。

Result: 在专有数据集和公开开源布局上的实验评估显示，实现了完全无冲突的分解和一致的色彩平衡。

Conclusion: 该框架为EDA工作流中的可扩展布局分解提供了可重现、数据高效且可部署的基准。

Abstract: Multipatterning is an essential decomposition strategy in electronic design automation (EDA) that overcomes lithographic limitations when printing dense circuit layouts. Although heuristic-based backtracking and SAT solvers can address these challenges, they often struggle to simultaneously handle both complex constraints and secondary objectives. In this study, we present a hybrid workflow that casts multipatterning as a variant of a constrained graph coloring problem with the primary objective of minimizing feature violations and a secondary objective of balancing the number of features on each mask. Our pipeline integrates two main components: (1) A GNN-based agent, trained in an unsupervised manner to generate initial color predictions, which are refined by (2) refinement strategies (a GNN-based heuristic and simulated annealing) that together enhance solution quality and balance. Experimental evaluation in both proprietary data sets and publicly available open source layouts demonstrate complete conflict-free decomposition and consistent color balancing. The proposed framework provides a reproducible, data-efficient and deployable baseline for scalable layout decomposition in EDA workflows.

</details>
