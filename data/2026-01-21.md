<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 10]
- [cs.DC](#cs.DC) [Total: 36]
- [cs.AR](#cs.AR) [Total: 11]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Context-Free Grammar Inference for Complex Programming Languages in Black Box Settings](https://arxiv.org/abs/2601.12385)
*Feifei Li,Xiao Chen,Xiaoyu Sun,Xi Xiao,Shaohua Wang,Yong Ding,Sheng Wen,Qing Li*

Main category: cs.PL

TL;DR: Crucio æ˜¯ä¸€ç§æ–°çš„è¯­æ³•æ¨æ–­æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºåˆ†è§£æ£®æ—æå–çŸ­ç¤ºä¾‹è¿›è¡Œè¯æ³•å’Œè¯­æ³•æ¨æ–­ï¼Œèƒ½å¤Ÿé«˜æ•ˆå¤„ç†å¤æ‚ç¼–ç¨‹è¯­è¨€ï¼Œåœ¨48å°æ—¶å†…æˆåŠŸæ¨æ–­Cã€C++ã€Javaç­‰è¯­è¨€çš„è¯­æ³•ï¼Œæ€§èƒ½ä¼˜äºç°æœ‰æ–¹æ³•ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰è¯­æ³•æ¨æ–­æ–¹æ³•ï¼ˆå¦‚Arvadaã€Treevadaã€Kedavraï¼‰æ— æ³•åœ¨48å°æ—¶å†…æ¨æ–­å¤æ‚ç¼–ç¨‹è¯­è¨€ï¼ˆCã€C++ã€Javaï¼‰çš„è¯­æ³•ã€‚è¿™äº›æ–¹æ³•è¦ä¹ˆç›´æ¥åœ¨å®Œæ•´è¾“å…¥ä¸Šæ“ä½œæ•ˆç‡ä½ä¸‹ï¼Œè¦ä¹ˆåˆ†è§£ç­–ç•¥ä¸å½»åº•ï¼Œä¸”ä¸¥æ ¼çº¦æŸé™åˆ¶äº†å¤æ‚è¯­æ³•çš„æ„å»ºã€‚

Method: æå‡ºCrucioæ–¹æ³•ï¼šæ„å»ºåˆ†è§£æ£®æ—æå–çŸ­ç¤ºä¾‹ï¼Œé€šè¿‡åˆ†å¸ƒçŸ©é˜µè¿›è¡Œè¯æ³•å’Œè¯­æ³•æ¨æ–­ã€‚è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆå¤„ç†å¤§å‹æ–‡ä»¶ï¼Œé¿å…ç°æœ‰æ–¹æ³•çš„å±€é™æ€§ã€‚

Result: Crucioæ˜¯å”¯ä¸€èƒ½åœ¨åˆç†æ—¶é—´å†…æˆåŠŸæ¨æ–­å¤æ‚ç¼–ç¨‹è¯­è¨€è¯­æ³•çš„æ–¹æ³•ï¼ˆéç»ˆç»“ç¬¦æ•°é‡æ¯”å…ˆå‰åŸºå‡†å¤š23å€ï¼‰ã€‚åœ¨ç®€å•åŸºå‡†ä¸Šï¼Œç›¸æ¯”Treevadaå’ŒKedavraï¼Œå¹³å‡å¬å›ç‡åˆ†åˆ«æé«˜1.37å€å’Œ1.19å€ï¼ŒF1åˆ†æ•°åˆ†åˆ«æé«˜1.21å€å’Œ1.13å€ã€‚

Conclusion: Crucioé€šè¿‡åˆ›æ–°çš„åˆ†è§£æ£®æ—å’Œåˆ†å¸ƒçŸ©é˜µæ–¹æ³•ï¼Œè§£å†³äº†å¤æ‚ç¼–ç¨‹è¯­è¨€è¯­æ³•æ¨æ–­çš„æ‰©å±•æ€§é—®é¢˜ï¼Œæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚

Abstract: Grammar inference for complex programming languages remains a significant challenge, as existing approaches fail to scale to real world datasets within practical time constraints. In our experiments, none of the state-of-the-art tools, including Arvada, Treevada and Kedavra were able to infer grammars for complex languages such as C, C++, and Java within 48 hours. Arvada and Treevada perform grammar inference directly on full-length input examples, which proves inefficient for large files commonly found in such languages. While Kedavra introduces data decomposition to create shorter examples for grammar inference, its lexical analysis still relies on the original inputs. Additionally, its strict no-overgeneralization constraint limits the construction of complex grammars.
  To overcome these limitations, we propose Crucio, which builds a decomposition forest to extract short examples for lexical and grammar inference via a distributional matrix. Experimental results show that Crucio is the only method capable of successfully inferring grammars for complex programming languages (where the number of nonterminals is up to 23x greater than in prior benchmarks) within reasonable time limits. On the prior simple benchmark, Crucio achieves an average recall improvement of 1.37x and 1.19x over Treevada and Kedavra, respectively, and improves F1 scores by 1.21x and 1.13x.

</details>


### [2] [An Introduction to Razborov's Flag Algebra as a Proof System for Extremal Graph Theory](https://arxiv.org/abs/2601.12741)
*Gyeongwon Jeong,Seonghun Park,Hongseok Yang*

Main category: cs.PL

TL;DR: æœ¬æ–‡æ˜¯ä¸€ç¯‡é¢å‘è®¡ç®—æœºç§‘å­¦å®¶çš„æ——ä»£æ•°å…¥é—¨ç»¼è¿°ï¼Œä»é€»è¾‘è§†è§’ä»‹ç»æ——ä»£æ•°çš„è¯­æ³•ã€è¯­ä¹‰å’Œè¯æ˜ç­–ç•¥ï¼Œç‰¹åˆ«è§£é‡Šäº†é€šè¿‡æ ‡è®°å˜ä½“æ¨å¯¼ä¸ç­‰å¼å†é€šè¿‡ä¼´éšå¯¹è½¬ç§»åˆ°æ— æ ‡è®°è®¾ç½®çš„è¯æ˜æ–¹æ³•ã€‚


<details>
  <summary>Details</summary>
Motivation: å°†Razborovçš„æ——ä»£æ•°æ¡†æ¶ä»‹ç»ç»™ä»äº‹é€»è¾‘ã€ç¼–ç¨‹è¯­è¨€ã€è‡ªåŠ¨éªŒè¯å’Œå½¢å¼åŒ–æ–¹æ³•çš„è®¡ç®—æœºç§‘å­¦å®¶ï¼Œå¸®åŠ©ä»–ä»¬ç†è§£è¿™ä¸€åœ¨æå€¼å›¾è®ºä¸­å–å¾—é‡è¦è¿›å±•çš„å¼ºå¤§å·¥å…·ã€‚

Method: é‡‡ç”¨é€»è¾‘è§†è§’ï¼Œå°†æ——ä»£æ•°è¡¨è¿°ä¸ºè¯­æ³•ã€è¯­ä¹‰å’Œè¯æ˜ç­–ç•¥çš„å½¢å¼ç³»ç»Ÿã€‚è¯¦ç»†è§£é‡Šäº†ä¸€ç§æµè¡Œçš„è¯æ˜ç­–ç•¥ï¼šå…ˆåœ¨æ ‡è®°å˜ä½“ä¸­è¯æ˜ä¸ç­‰å¼ï¼Œç„¶åé€šè¿‡ä¼´éšå¯¹ï¼ˆç±»ä¼¼Galoisè¿æ¥å’ŒèŒƒç•´ä¼´éšï¼‰ä½¿ç”¨å‘ä¸‹ç®—å­è½¬ç§»åˆ°åŸå§‹æ— æ ‡è®°è®¾ç½®ã€‚

Result: é€šè¿‡Mantelå®šç†å’ŒGoodmançš„Ramseyå¤šé‡æ€§ç•Œé™ç­‰ä»£è¡¨æ€§ä¾‹å­ï¼Œå±•ç¤ºäº†å¦‚ä½•åœ¨æ——ä»£æ•°æ¡†æ¶ä¸­ç¬¦å·åŒ–åœ°æ‰§è¡Œæ•°å­¦è®ºè¯ï¼Œä½¿è®¡ç®—æœºç§‘å­¦å®¶èƒ½å¤Ÿç†è§£å¹¶åº”ç”¨è¿™ä¸€å·¥å…·ã€‚

Conclusion: æ——ä»£æ•°ä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œä¸ä»…é€‚ç”¨äºæå€¼å›¾è®ºï¼Œå…¶é€»è¾‘ç»“æ„å’Œä¼´éšå¯¹çš„æ¦‚å¿µä¹Ÿä¸è®¡ç®—æœºç§‘å­¦ä¸­çš„å½¢å¼åŒ–æ–¹æ³•ã€è‡ªåŠ¨éªŒè¯ç­‰é¢†åŸŸæœ‰å¯†åˆ‡è”ç³»ï¼Œä¸ºè·¨å­¦ç§‘ç ”ç©¶æä¾›äº†æ¡¥æ¢ã€‚

Abstract: Razborov's flag algebra forms a powerful framework for deriving asymptotic inequalities between induced subgraph densities, underpinning many advances in extremal graph theory. This survey introduces flag algebra to computer scientists working in logic, programming languages, automated verification, and formal methods. We take a logical perspective on flag algebra and present it in terms of syntax, semantics, and proof strategies, in a style closer to formal logic. One popular proof strategy derives valid inequalities by first proving inequalities in a labelled variant of flag algebra and then transferring them to the original unlabelled setting using the so-called downward operator. We explain this strategy in detail and highlight that its transfer mechanism relies on the notion of what we call an adjoint pair, reminiscent of Galois connections and categorical adjunctions, which appear frequently in work on automated verification and programming languages. Along the way, we work through representative examples, including Mantel's theorem and Goodman's bound on Ramsey multiplicity, to illustrate how mathematical arguments can be carried out symbolically in the flag algebra framework.

</details>


### [3] [A Formally Verified Procedure for Width Inference in FIRRTL](https://arxiv.org/abs/2601.12813)
*Keyin Wang,Xiaomu Shi,Jiaxiang Liu,Zhilin Wu,Taolve Chen,Fu Song,David N. Jansen*

Main category: cs.PL

TL;DR: æœ¬æ–‡é’ˆå¯¹FIRRTLç¡¬ä»¶è®¾è®¡ä¸­é—´è¡¨ç¤ºè¯­è¨€çš„ä½å®½æ¨æ–­é—®é¢˜ï¼Œæå‡ºäº†é¦–ä¸ªå½¢å¼åŒ–éªŒè¯çš„è§£å†³æ–¹æ¡ˆï¼Œç›¸æ¯”å®˜æ–¹ç¼–è¯‘å™¨èƒ½å¤„ç†æ›´å¤šå®ä¾‹ä¸”æ•ˆç‡é«˜ã€‚


<details>
  <summary>Details</summary>
Motivation: FIRRTLç¼–è¯‘å™¨ä¸­çš„InferWidthså®½åº¦æ¨æ–­è¿‡ç¨‹å­˜åœ¨ç¼ºé™·ï¼Œå³ä½¿å¯¹äºç®€å•çš„FIRRTLç¨‹åºä¹Ÿå¯èƒ½å¤±è´¥ï¼Œéœ€è¦æ›´å¯é ã€å®Œæ•´çš„è§£å†³æ–¹æ¡ˆã€‚

Method: åŸºäºçº¦æŸå¯æ»¡è¶³æ€§å­˜åœ¨å”¯ä¸€æœ€å°è§£çš„ç†è®ºç»“æœï¼Œæå‡ºå®Œæ•´çš„å®½åº¦æ¨æ–­è¿‡ç¨‹ï¼Œåœ¨Rocqäº¤äº’å¼å®šç†è¯æ˜å™¨ä¸­å®ç°å¹¶è¯æ˜åŠŸèƒ½æ­£ç¡®æ€§ï¼Œç„¶åæå–ä¸ºOCamlå®ç°ã€‚

Result: å®ç°äº†é¦–ä¸ªå½¢å¼åŒ–éªŒè¯çš„InferWidthsè¿‡ç¨‹ï¼Œå®éªŒè¡¨æ˜è¯¥æ–¹æ³•æ¯”å®˜æ–¹firtoolç¼–è¯‘å™¨èƒ½è§£å†³æ›´å¤šå®ä¾‹ï¼Œé€šå¸¸å…·æœ‰é«˜æ•ˆç‡ã€‚

Conclusion: æœ¬æ–‡ä¸ºFIRRTLå®½åº¦æ¨æ–­é—®é¢˜æä¾›äº†ç†è®ºä¿è¯å’Œå½¢å¼åŒ–éªŒè¯çš„å®ç°ï¼Œæ˜¾è‘—æå‡äº†å®½åº¦æ¨æ–­çš„å¯é æ€§å’Œè¦†ç›–ç‡ã€‚

Abstract: FIRRTL is an intermediate representation language for Register Transfer Level (RTL) hardware designs. In FIRRTL programs, the bit widths of many components are not specified explicitly and must be inferred during compilation. In mainstream FIRRTL compilers, such as the official compiler firtool, width inference is conducted by a compilation pass referred to as InferWidths, which may fail even for simple FIRRTL programs. In this paper, we thoroughly investigate the width inference problem for FIRRTL programs. We show that, if the constraints obtained from a FIRRTL program are satisfiable, there exists a unique least solution. Based on this result, we propose a complete procedure for solving the width inference problem. We implement it in the interactive theorem prover Rocq and prove its functional correctness. From the Rocq implementation, we extract an OCaml implementation, which is the first formally verified implementation of the InferWidths pass. Extensive experiments demonstrate that our approach can solve more instances than the official InferWidths pass in firtool, normally with high efficiency.

</details>


### [4] [Dependently-Typed AARA: A Non-Affine Approach for Resource Analysis of Higher-Order Programs](https://arxiv.org/abs/2601.12943)
*Han Xu,Di Wang*

Main category: cs.PL

TL;DR: Î»_amor^na æ˜¯ä¸€ä¸ªéä»¿å°„çš„AARAé£æ ¼ä¾èµ–ç±»å‹ç³»ç»Ÿï¼Œç”¨äºé«˜é˜¶å‡½æ•°ç¨‹åºçš„èµ„æºåˆ†æï¼Œé€šè¿‡è§£è€¦ç±»å‹å’Œèµ„æºã€ä½¿ç”¨ä¾èµ–ç±»å‹å®ç°æ›´ç²¾ç¡®çš„èµ„æºæ¨ç†ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰ä»¿å°„ç±»å‹ç³»ç»Ÿåœ¨å¤„ç†é«˜é˜¶ç¨‹åºï¼ˆç‰¹åˆ«æ˜¯éƒ¨åˆ†åº”ç”¨ï¼‰æ—¶æ— æ³•æ¨å¯¼ç²¾ç¡®çš„èµ„æºè¡Œä¸ºï¼Œä¸»è¦é—®é¢˜åœ¨äºç±»å‹ä¸èµ„æºçš„ç´§å¯†è€¦åˆä»¥åŠä»¿å°„ä¸é«˜é˜¶ç±»å‹æœºåˆ¶çš„å†²çªã€‚

Method: æå‡ºÎ»_amor^naç³»ç»Ÿï¼Œé‡‡ç”¨éä»¿å°„ç±»å‹æœºåˆ¶ï¼Œé€šè¿‡ä¾èµ–ç±»å‹å°†ç±»å‹çº§åŠ¿å‡½æ•°ä¸æ™®é€šç±»å‹åˆ†ç¦»ï¼Œè§£è€¦èµ„æºä¸ç±»å‹ï¼Œæ”¯æŒé«˜é˜¶å‡½æ•°çš„ç²¾ç¡®èµ„æºæ¨ç†ã€‚

Result: å½¢å¼åŒ–äº†Î»_amor^naçš„è¯­æ³•å’Œè¯­ä¹‰ï¼Œè¯æ˜äº†å…¶å¯é æ€§ï¼ˆä¿è¯èµ„æºç•Œé™çš„æ­£ç¡®æ€§ï¼‰ï¼Œå¹¶é€šè¿‡å¤šä¸ªç»å…¸å’Œé«˜é˜¶ç¤ºä¾‹å±•ç¤ºäº†ç³»ç»Ÿçš„è¡¨è¾¾èƒ½åŠ›å’Œç»„åˆæ€§ã€‚

Conclusion: Î»_amor^naé€šè¿‡éä»¿å°„ä¾èµ–ç±»å‹ç³»ç»ŸæˆåŠŸè§£å†³äº†é«˜é˜¶å‡½æ•°èµ„æºåˆ†æçš„ç²¾åº¦é—®é¢˜ï¼Œä¸ºé«˜é˜¶ç¨‹åºæä¾›äº†æ›´ç²¾ç¡®å’Œç»„åˆæ€§çš„èµ„æºæ¨ç†èƒ½åŠ›ã€‚

Abstract: Static resource analysis determines the resource consumption (e.g., time complexity) of a program without executing it. Among the numerous existing approaches for resource analysis, affine type systems have been one dominant approach. However, these affine type systems fall short of deriving precise resource behavior of higher-order programs, particularly in cases that involve partial applications.
  This article presents Î»_\ms{amor}^\ms{na}}, a non-affine AARA-style dependent type system for resource reasoning about higher-order functional programs. The key observation is that the main issue in previous approaches comes from (i) the close coupling of types and resources, and (ii) the conflict between affine and higher-order typing mechanisms. To derive precise resource behavior of higher-order functions, Î»_\ms{amor}^\ms{na}} decouples resources from types and follows a non-affine typing mechanism. The non-affine type system of Î»_\ms{amor}^\ms{na}} achieves this by using dependent types, which allows expressing type-level potential functions separate from ordinary types. This article formalizes Î»_\ms{amor}^\ms{na}}'s syntax and semantics, and proves its soundness, which guarantees the correctness of resource bounds. Several challenging classic and higher-order examples are presented to demonstrate the expressiveness and compositionality of Î»_\ms{amor}^\ms{na}}'s reasoning capability.

</details>


### [5] [Functional Logic Program Transformations](https://arxiv.org/abs/2601.13224)
*Michael Hanus,Steven Libby*

Main category: cs.PL

TL;DR: ä½¿ç”¨å‡½æ•°é€»è¾‘ç¼–ç¨‹å®ç°ç¨‹åºè½¬æ¢ï¼Œé€šè¿‡éƒ¨åˆ†å®šä¹‰å’Œéç¡®å®šæ€§æ“ä½œç®€åŒ–è¯­æ³•æ ‘å˜æ¢


<details>
  <summary>Details</summary>
Motivation: ç¼–è¯‘å™¨ã€åˆ†æå™¨å’ŒéªŒè¯å™¨ç­‰å·¥å…·éœ€è¦åœ¨ä¸­é—´ç¨‹åºè¡¨ç¤ºï¼ˆå¦‚æŠ½è±¡è¯­æ³•æ ‘ï¼‰ä¸Šæ‰§è¡Œå˜æ¢ï¼Œä½†å®ç°è¿™äº›å˜æ¢å¾ˆå¤æ‚ï¼Œéœ€è¦éå†å®Œæ•´è¯­æ³•æ ‘å¹¶åœ¨èŠ‚ç‚¹åº”ç”¨å„ç§å˜æ¢

Method: æå‡ºå°†ç¨‹åºå˜æ¢ç¼–å†™ä¸ºéƒ¨åˆ†å®šä¹‰å’Œéç¡®å®šæ€§æ“ä½œï¼Œåˆ©ç”¨å‡½æ•°é€»è¾‘ç¼–ç¨‹çš„ç‰¹æ€§ï¼Œåœ¨Curryè¯­è¨€åŠå…¶ä¸­é—´è¡¨ç¤ºFlatCurryä¸Šå®ç°

Result: ä¸ç¡®å®šæ€§å˜æ¢æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œè¯„ä¼°éç¡®å®šæ€§å®ç°çš„å¼€é”€

Conclusion: å‡½æ•°é€»è¾‘ç¼–ç¨‹çš„ç‰¹æ€§æœ‰åŠ©äºä»¥ç´§å‡‘ä¸”æ˜“äºç†è§£çš„æ–¹å¼å®ç°ç¨‹åºå˜æ¢

Abstract: Many tools used to process programs, like compilers, analyzers, or verifiers, perform transformations on their intermediate program representation, like abstract syntax trees. Implementing such program transformations is a non-trivial task, since it is necessary to iterate over the complete syntax tree and apply various transformations at nodes in a tree. In this paper we show how the features of functional logic programming are useful to implement program transformations in a compact and comprehensible manner. For this purpose, we propose to write program transformations as partially defined and non-deterministic operations. Since the implementation of non-determinism usually causes some overhead compared to deterministically defined operations, we compare our approach to a deterministic transformation method. We evaluate these alternatives for the functional logic language Curry and its intermediate representation FlatCurry which is used in various analysis and verification tools and compilers.

</details>


### [6] [Reduction for Structured Concurrent Programs](https://arxiv.org/abs/2601.13341)
*Namratha Gangamreddypalli,Constantin Enea,Shaz Qadeer*

Main category: cs.PL

TL;DR: æå‡ºä¸€ç§æ–°çš„ç»“æ„åŒ–å¹¶å‘ç¨‹åºè§„çº¦æŠ€æœ¯ï¼Œç»Ÿä¸€äº†ä¸¤ç§å…³é”®è¿›å±•ï¼šå°†å¹¶è¡Œç»„åˆæ›¿æ¢ä¸ºé¡ºåºç»„åˆï¼Œä»¥åŠæ‰©å±•Liptonè§„çº¦ä»¥æ”¯æŒåŒ…å«è¿‡ç¨‹è°ƒç”¨çš„åŸå­æ®µã€‚


<details>
  <summary>Details</summary>
Motivation: åŸºäºLiptonç§»åŠ¨å­çš„äº¤æ¢æ€§æ¨ç†æ˜¯éªŒè¯å¹¶å‘ç¨‹åºçš„å¼ºå¤§æŠ€æœ¯ï¼Œä½†å°†å…¶æ‰©å±•åˆ°è½¯ä»¶ç³»ç»Ÿä¸­å¸¸ç”¨çš„ç‰¹æ€§ï¼ˆå¦‚è¿‡ç¨‹å’Œå¹¶è¡Œç»„åˆï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜ã€‚

Method: å¼•å…¥ä¸€ç§æ–°çš„è§„çº¦æŠ€æœ¯ï¼š1ï¼‰æå‡ºå°†å¹¶è¡Œç»„åˆæ›¿æ¢ä¸ºé¡ºåºç»„åˆçš„è§„çº¦ç­–ç•¥ï¼›2ï¼‰å°†Liptonè§„çº¦æ¨å¹¿åˆ°æ”¯æŒåŒ…å«ï¼ˆå¯èƒ½é€’å½’ï¼‰è¿‡ç¨‹è°ƒç”¨çš„åŸå­æ®µï¼›3ï¼‰è¿™ä¸¤ç§åŸºç¡€ç­–ç•¥å¯ä»¥ä»»æ„ç»„åˆã€‚

Result: åœ¨Civlä¸­å®ç°äº†è¯¥æŠ€æœ¯ï¼Œå¹¶åœ¨å¤šä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„æ¡ˆä¾‹ç ”ç©¶ä¸­è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬å¿«ç…§å¯¹è±¡ã€å®¹é”™çº¿æ€§åŒ–å¯„å­˜å™¨ã€FLASHç¼“å­˜ä¸€è‡´æ€§åè®®å’ŒTwo-Phase Commitçš„éå¹³å‡¡å˜ä½“ã€‚

Conclusion: è¯¥å·¥ä½œç»Ÿä¸€äº†ä¸¤ç§å…³é”®çš„è§„çº¦ç­–ç•¥ï¼Œæå¤§åœ°æ‰©å±•äº†åŸºäºè§„çº¦çš„æ¨ç†çš„èŒƒå›´å’Œçµæ´»æ€§ï¼Œä¸ºç»“æ„åŒ–å¹¶å‘ç¨‹åºçš„éªŒè¯æä¾›äº†æ›´å¼ºå¤§çš„å·¥å…·ã€‚

Abstract: Commutativity reasoning based on Lipton's movers is a powerful technique for verification of concurrent programs. The idea is to define a program transformation that preserves a subset of the initial set of interleavings, which is sound modulo reorderings of commutative actions. Scaling commutativity reasoning to routinely-used features in software systems, such as procedures and parallel composition, remains a significant challenge.
  In this work, we introduce a novel reduction technique for structured concurrent programs that unifies two key advances. First, we present a reduction strategy that soundly replaces parallel composition with sequential composition. Second, we generalize Lipton's reduction to support atomic sections containing (potentially recursive) procedure calls. Crucially, these two foundational strategies can be composed arbitrarily, greatly expanding the scope and flexibility of reduction-based reasoning. We implemented this technique in Civl and demonstrated its effectiveness on a number of challenging case studies, including a snapshot object, a fault-tolerant and linearizable register, the FLASH cache coherence protocol, and a non-trivial variant of Two-Phase Commit.

</details>


### [7] [Foundational VeriFast: Pragmatic Certification of Verification Tool Results through Hinted Mirroring](https://arxiv.org/abs/2601.13727)
*Bart Jacobs*

Main category: cs.PL

TL;DR: VeriFastå·¥å…·é€šè¿‡æ‰©å±•æ”¯æŒç”ŸæˆRocqè¯æ˜è„šæœ¬ï¼Œä¸ºRustç¨‹åºéªŒè¯æä¾›å½¢å¼åŒ–è¯æ˜ï¼Œå¢å¼ºå…¶åœ¨å®‰å…¨å…³é”®é¢†åŸŸçš„é€‚ç”¨æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: VeriFastä½œä¸ºé¢†å…ˆçš„å½¢å¼éªŒè¯å·¥å…·ï¼Œå…¶æœ¬èº«çº¦3ä¸‡è¡ŒOCamlä»£ç æœªç»å½¢å¼åŒ–éªŒè¯ï¼Œå¯èƒ½å­˜åœ¨å¯¼è‡´é”™è¯¯éªŒè¯ç»“æœçš„bugï¼Œè¿™é™åˆ¶äº†å…¶åœ¨å®‰å…¨å…³é”®é¢†åŸŸçš„åº”ç”¨ã€‚

Method: é‡‡ç”¨"æç¤ºé•œåƒ"æ–¹æ³•ï¼šè®°å½•VeriFastç¬¦å·æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œåœ¨Rocqä¸­é‡æ”¾æ‰§è¡Œè¿‡ç¨‹ï¼Œç”ŸæˆRocqè¯æ˜è„šæœ¬æ¥è¯æ˜ç¨‹åºç›¸å¯¹äºRocqç¼–ç çš„Rustå…¬ç†è¯­ä¹‰çš„æ­£ç¡®æ€§ã€‚

Result: æˆåŠŸæ‰©å±•VeriFastï¼Œä½¿å…¶åœ¨éªŒè¯Rustç¨‹åºæˆåŠŸåèƒ½å¤Ÿç”ŸæˆRocqè¯æ˜è„šæœ¬ï¼Œæ˜¾è‘—å¢å¼ºäº†å·¥å…·åœ¨å®‰å…¨å…³é”®é¢†åŸŸçš„é€‚ç”¨æ€§ã€‚

Conclusion: é€šè¿‡å°†VeriFastä¸Rocqè¯æ˜åŠ©æ‰‹é›†æˆï¼Œä¸ºRustç¨‹åºéªŒè¯æä¾›äº†å½¢å¼åŒ–è¯æ˜ï¼Œè§£å†³äº†å·¥å…·æœ¬èº«æœªç»éªŒè¯çš„é—®é¢˜ï¼Œæå‡äº†éªŒè¯ç»“æœçš„å¯ä¿¡åº¦ã€‚

Abstract: VeriFast is a leading tool for the modular formal verification of correctness properties of single-threaded and multi-threaded C and Rust programs. It verifies a program by symbolically executing each function in isolation, exploiting user-annotated preconditions, postconditions, and loop invariants written in a form of separation logic, and using a separation logic-based symbolic representation of memory. However, the tool itself, written in roughly 30K lines of OCaml code, has not been formally verified. Therefore, bugs in the tool could cause it to falsely report the correctness of the input program. We here report on an early result extending VeriFast to emit, upon successful verification of a Rust program, a Rocq proof script that proves correctness of the program with respect to a Rocq-encoded axiomatic semantics of Rust. This significantly enhances VeriFast's applicability in safety-critical domains. We apply hinted mirroring: we record key information from VeriFast's symbolic execution run, and use it to direct a replay of the run in Rocq.

</details>


### [8] [Generating Functions Meet Occupation Measures: Invariant Synthesis for Probabilistic Loops (Extended Version)](https://arxiv.org/abs/2601.13991)
*Darion Haase,Kevin Batz,Adrian Gallus,Benjamin Lucien Kaminski,Joost-Pieter Katoen,Lutz Klinkenberg,Tobias Winkler*

Main category: cs.PL

TL;DR: æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå ç”¨ä¸å˜é‡çš„ç²¾ç¡®æ¦‚ç‡æ¨ç†æ–¹æ³•ï¼Œç”¨äºå¤„ç†å¸¦å¾ªç¯çš„ç¨‹åºï¼Œé€šè¿‡è‡ªåŠ¨æ¨¡æ¿åˆæˆå®ç°


<details>
  <summary>Details</summary>
Motivation: æ¦‚ç‡ç¼–ç¨‹ä¸­çš„ä¸€ä¸ªåŸºæœ¬è®¡ç®—ä»»åŠ¡æ˜¯ä»ç»™å®šçš„å…ˆéªŒåˆ†å¸ƒæ¨æ–­ç¨‹åºçš„åéªŒåˆ†å¸ƒã€‚è¿™ä¸ªé—®é¢˜å¯¹äºå…·æœ‰å¾ªç¯æˆ–æ— ç•Œé€’å½’çš„è¡¨è¾¾æ€§è¯­è¨€å°¤å…¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ç°æœ‰æ–‡çŒ®å¤§å¤šå…³æ³¨ç»Ÿè®¡è¿‘ä¼¼æ–¹æ³•ï¼Œè€Œæœ¬æ–‡æ—¨åœ¨è§£å†³æ•°å­¦ä¸Šç²¾ç¡®æ¨ç†çš„é—®é¢˜ã€‚

Method: é’ˆå¯¹å¸¦å¾ªç¯çš„ç¨‹åºï¼Œå¼•å…¥äº†å ç”¨æµ‹åº¦ç›¸å…³çš„æ¦‚ç‡å¾ªç¯ä¸å˜é‡æ¦‚å¿µï¼ˆå ç”¨ä¸å˜é‡ï¼‰ã€‚è¿™ç§ä¸å˜é‡ä¸ç¨‹åºçŠ¶æ€çš„æœŸæœ›è®¿é—®æ¬¡æ•°ç›¸å…³ï¼Œèƒ½å¤Ÿè€ƒè™‘åˆå§‹åˆ†å¸ƒï¼Œå¹¶èƒ½æ¨å¯¼å‡ºæ­£å‡ ä¹å¿…ç„¶ç»ˆæ­¢æ€§ã€‚æå‡ºäº†ä¸€ç§åŸºäºæ¨¡æ¿çš„è‡ªåŠ¨ä¸å˜é‡åˆæˆæ–¹æ³•ï¼Œé€šè¿‡ç”Ÿæˆå‡½æ•°è¿›è¡Œç¼–ç ã€‚

Result: è¯¥æ–¹æ³•å·²å®ç°å¹¶åœ¨åŸºå‡†æµ‹è¯•é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

Conclusion: å ç”¨ä¸å˜é‡ä¸ºæ¦‚ç‡å¾ªç¯åˆ†ææä¾›äº†ä¸€ç§ä¸ç°æœ‰é…æ–¹æ³•äº’è¡¥çš„æ–°æŠ€æœ¯ï¼Œèƒ½å¤Ÿå®ç°ç²¾ç¡®æ¨ç†å¹¶è‡ªåŠ¨åˆæˆä¸å˜é‡è¯æ˜ã€‚

Abstract: A fundamental computational task in probabilistic programming is to infer a program's output (posterior) distribution from a given initial (prior) distribution. This problem is challenging, especially for expressive languages that feature loops or unbounded recursion. While most of the existing literature focuses on statistical approximation, in this paper we address the problem of mathematically exact inference.
  To achieve this for programs with loops, we rely on a relatively underexplored type of probabilistic loop invariant, which is linked to a loop's so-called occupation measure. The occupation measure associates program states with their expected number of visits, given the initial distribution. Based on this, we derive the notion of an occupation invariant. Such invariants are essentially dual to probabilistic martingales, the predominant technique for formal probabilistic loop analysis in the literature. A key feature of occupation invariants is that they can take the initial distribution into account and often yield a proof of positive almost sure termination as a by-product.
  Finally, we present an automatic, template-based invariant synthesis approach for occupation invariants by encoding them as generating functions. The approach is implemented and evaluated on a set of benchmarks.

</details>


### [9] [Verifying Floating-Point Programs in Stainless](https://arxiv.org/abs/2601.14059)
*Andrea Gilot,Axel BergstrÃ¶m,Eva Darulova*

Main category: cs.PL

TL;DR: StainlesséªŒè¯å™¨æ‰©å±•æ”¯æŒæµ®ç‚¹æ•°ï¼Œæˆä¸ºé¦–ä¸ªæ”¯æŒScalaå­é›†ï¼ˆå«å¤šæ€ã€é€’å½’ã€é«˜é˜¶å‡½æ•°ï¼‰çš„è‡ªåŠ¨åŒ–æµ®ç‚¹éªŒè¯å·¥å…·


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰éªŒè¯å·¥å…·ç¼ºä¹å¯¹æµ®ç‚¹æ•°çš„è‡ªåŠ¨åŒ–éªŒè¯æ”¯æŒï¼Œç‰¹åˆ«æ˜¯å¯¹äºåŒ…å«å¤šæ€ã€é€’å½’å’Œé«˜é˜¶å‡½æ•°çš„Scalaè¯­è¨€å­é›†

Method: é‡‡ç”¨KeYéªŒè¯å™¨çš„æ•°å­¦å‡½æ•°å…¬ç†åŒ–æ–¹æ³•ï¼Œæ‰©å±•æ”¯æŒScala math APIçš„æ‰€æœ‰å‡½æ•°ï¼Œå¹¶åœ¨Stainlessä¸­éªŒè¯å…¬ç†çš„æ­£ç¡®æ€§

Result: æˆåŠŸéªŒè¯äº†ä»GitHubçœŸå®ä»£ç é‡‡æ ·çš„åŸºå‡†æµ‹è¯•ï¼Œèƒ½å¤ŸéªŒè¯è¾“å‡ºèŒƒå›´ã€ç‰¹æ®Šå€¼ä¸å­˜åœ¨ç­‰è§„èŒƒï¼Œæˆ–åœ¨è§„èŒƒä¸æˆç«‹æ—¶ç”Ÿæˆåä¾‹

Conclusion: Stainlessæˆä¸ºé¦–ä¸ªæ”¯æŒæµ®ç‚¹æ•°è‡ªåŠ¨åŒ–éªŒè¯çš„ScalaéªŒè¯å·¥å…·ï¼Œä¸ºå®é™…ä»£ç ä¸­çš„æµ®ç‚¹è¿ç®—éªŒè¯æä¾›äº†å®ç”¨è§£å†³æ–¹æ¡ˆ

Abstract: We extend the Stainless deductive verifier with floating-point support, providing the first automated verification support for floating-point numbers for a subset of Scala that includes polymorphism, recursion and higher-order functions. We follow the recent approach in the KeY verifier to axiomatise reasoning about mathematical functions, but go further by supporting all functions from Scala's math API, and by verifying the correctness of the axioms against the actual implementation in Stainless itself. We validate Stainless' floating-point support on a new set of benchmarks sampled from real-world code from GitHub, showing that it can verify specifications about, e.g., ranges of output or absence of special values for most supported functions, or produce counter-examples when the specifications do not hold.

</details>


### [10] [Partial Reductions for Kleene Algebra with Linear Hypotheses](https://arxiv.org/abs/2601.14114)
*Liam Chung,Tobias KappÃ©*

Main category: cs.PL

TL;DR: æå‡ºä¸€ç§åŸºäºè‡ªåŠ¨æœºçš„æ„é€ æ–¹æ³•ï¼Œç”¨äºè‡ªåŠ¨æ¨å¯¼Kleeneä»£æ•°ä¸­å¹¿æ³›å‡è®¾ç±»çš„çº¦ç®€ï¼Œæ”¯æŒéƒ¨åˆ†çº¦ç®€ä»¥å®ç°éƒ¨åˆ†å®Œå¤‡æ€§ï¼Œæ‰©å±•äº†ç°æœ‰å·¥ä½œçš„å¯è¯æ˜ç­‰ä»·èŒƒå›´ã€‚


<details>
  <summary>Details</summary>
Motivation: Kleeneä»£æ•°ï¼ˆKAï¼‰è™½ç„¶å…·æœ‰å¯åˆ¤å®šä¸”å®Œå¤‡çš„ç­‰å¼ç†è®ºï¼Œä½†æ— æ³•è¯æ˜ç‰¹å®šç¨‹åºé—´çš„æ‰€æœ‰ç­‰ä»·æ€§ã€‚ä¼ ç»Ÿæ–¹æ³•éœ€è¦æ‰‹åŠ¨æ„é€ çº¦ç®€æ˜ å°„ï¼Œå·¥ä½œé‡å¤§ä¸”å—æ­£åˆ™æ€§çº¦æŸï¼ŒæŸäº›è¡¨è¾¾å¼å’Œå‡è®¾ç»„åˆå¯èƒ½ä¸å­˜åœ¨çº¦ç®€ã€‚

Method: æå‡ºåŸºäºè‡ªåŠ¨æœºçš„æ„é€ æ–¹æ³•ï¼Œèƒ½å¤Ÿæœºæ¢°åœ°æ¨å¯¼å‡ºå¹¿æ³›å‡è®¾ç±»çš„çº¦ç®€ã€‚è¿™äº›çº¦ç®€å¯ä»¥æ˜¯éƒ¨åˆ†çš„ï¼Œå½“çº¦ç®€ä¸ºéƒ¨åˆ†æ—¶ï¼Œå®ƒä»¬åœ¨å…¶å®šä¹‰åŸŸå†…æä¾›éƒ¨åˆ†å®Œå¤‡æ€§ã€‚

Result: è¯¥æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨å»ºç«‹æ¯”ç°æœ‰å·¥ä½œè¦†ç›–èŒƒå›´æ›´å¹¿çš„ç­‰ä»·æ€§çš„å¯è¯æ˜æ€§ï¼Œé€šè¿‡éƒ¨åˆ†çº¦ç®€å®ç°äº†éƒ¨åˆ†å®Œå¤‡æ€§ï¼Œçªç ´äº†ä¼ ç»Ÿæ–¹æ³•çš„æ­£åˆ™æ€§çº¦æŸé™åˆ¶ã€‚

Conclusion: æå‡ºçš„è‡ªåŠ¨æœºæ„é€ æ–¹æ³•èƒ½å¤Ÿè‡ªåŠ¨æ¨å¯¼Kleeneä»£æ•°ä¸­å¹¿æ³›å‡è®¾ç±»çš„çº¦ç®€ï¼Œæ”¯æŒéƒ¨åˆ†çº¦ç®€ä»¥å®ç°éƒ¨åˆ†å®Œå¤‡æ€§ï¼Œæ˜¾è‘—æ‰©å±•äº†å¯è‡ªåŠ¨è¯æ˜çš„ç¨‹åºç­‰ä»·æ€§èŒƒå›´ï¼Œå‡å°‘äº†æ‰‹åŠ¨æ„é€ çš„å·¥ä½œé‡ã€‚

Abstract: Kleene algebra (KA) is an important tool for reasoning about general program equivalences, with a decidable and complete equational theory. However, KA cannot always prove equivalences between specific programs. For this purpose, one adds hypotheses to KA that encode program-specific knowledge. Traditionally, a map on regular expressions called a reduction then lets us lift decidability and completeness to these more expressive systems. Explicitly constructing such a reduction requires significant labour. Moreover, due to regularity constraints, a reduction may not exist for all combinations of expression and hypothesis.
  We describe an automaton-based construction to mechanically derive reductions for a wide class of hypotheses. These reductions can be partial, in which case they yield partial completeness: completeness for expressions in their domain. This allows us to automatically establish the provability of more equivalences than what is covered in existing work.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [11] [PerCache: Predictive Hierarchical Cache for RAG Applications on Mobile Devices](https://arxiv.org/abs/2601.11553)
*Kaiwei Liu,Liekang Zeng,Lilin Xu,Bufang Yang,Zhenyu Yan*

Main category: cs.DC

TL;DR: PerCacheæ˜¯ä¸€ä¸ªé’ˆå¯¹ç§»åŠ¨RAGåº”ç”¨çš„å±‚æ¬¡åŒ–ç¼“å­˜ç³»ç»Ÿï¼Œé€šè¿‡æ¸è¿›å¼åŒ¹é…ç›¸ä¼¼æŸ¥è¯¢å’ŒQKVç¼“å­˜æ¥é‡ç”¨ä¸­é—´è®¡ç®—ç»“æœï¼Œç»“åˆé¢„æµ‹æ€§ç¼“å­˜å¡«å……å’Œè‡ªé€‚åº”é…ç½®ï¼Œæ˜¾è‘—é™ä½ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚


<details>
  <summary>Details</summary>
Motivation: ç§»åŠ¨è®¾å¤‡ä¸Šçš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)åº”ç”¨ï¼ˆå¦‚ç§»åŠ¨åŠ©æ‰‹å¤„ç†ä¸ªäººé‚®ä»¶æˆ–ä¼šè®®è®°å½•ï¼‰é¢ä¸´é«˜å»¶è¿Ÿé—®é¢˜ã€‚ç°æœ‰ç¼“å­˜æ–¹æ¡ˆï¼ˆå¦‚KVç¼“å­˜é‡ç”¨å’Œè¯­ä¹‰ç¼“å­˜é‡ç”¨ï¼‰ä¸»è¦é’ˆå¯¹äº‘ç«¯è®¾è®¡ï¼Œå¿½è§†äº†ç§»åŠ¨RAGçš„ç‹¬ç‰¹ç‰¹æ€§ï¼Œåœ¨ç§»åŠ¨ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ã€‚

Method: PerCacheé‡‡ç”¨å±‚æ¬¡åŒ–æ¶æ„ï¼Œæ¸è¿›å¼åŒ¹é…ç›¸ä¼¼æŸ¥è¯¢å’ŒQKVç¼“å­˜ï¼Œæœ€å¤§åŒ–ä¸åŒè®¡ç®—é˜¶æ®µçš„ä¸­é—´ç»“æœé‡ç”¨ã€‚ä½¿ç”¨é¢„æµ‹æ–¹æ³•å¡«å……æœªæ¥å¯èƒ½å‡ºç°çš„æŸ¥è¯¢ä»¥æé«˜ç¼“å­˜å‘½ä¸­ç‡ï¼Œå¹¶èƒ½æ ¹æ®åŠ¨æ€ç³»ç»Ÿè´Ÿè½½è‡ªé€‚åº”è°ƒæ•´é…ç½®ï¼Œä»¥æœ€å°èµ„æºæ¶ˆè€—æœ€å¤§åŒ–ç¼“å­˜æ•ˆç”¨ã€‚

Result: åœ¨ç°æœ‰ç§»åŠ¨LLMæ¨ç†å¼•æ“ä¸Šå®ç°PerCacheï¼Œå¹¶åœ¨å•†ç”¨æ‰‹æœºä¸Šè¯„ä¼°ã€‚å®éªŒè¡¨æ˜ï¼ŒPerCacheåœ¨å„ç§åº”ç”¨ä¸­ç›¸æ¯”æœ€ä½³åŸºçº¿èƒ½å‡å°‘34.4%çš„å»¶è¿Ÿï¼Œå¹¶åœ¨åŠ¨æ€èµ„æºå˜åŒ–ä¸‹ä¿æŒæœ€ä¼˜å»¶è¿Ÿæ€§èƒ½ã€‚

Conclusion: PerCacheæ˜¯é’ˆå¯¹ç§»åŠ¨RAGåº”ç”¨çš„æœ‰æ•ˆç¼“å­˜è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å±‚æ¬¡åŒ–ç¼“å­˜æ¶æ„ã€é¢„æµ‹æ€§å¡«å……å’Œè‡ªé€‚åº”é…ç½®ï¼Œæ˜¾è‘—é™ä½äº†ç«¯åˆ°ç«¯å»¶è¿Ÿï¼Œé€‚åº”ç§»åŠ¨ç¯å¢ƒçš„èµ„æºçº¦æŸå’ŒåŠ¨æ€ç‰¹æ€§ã€‚

Abstract: Retrieval-augmented generation (RAG) has been extensively used as a de facto paradigm in various large language model (LLM)-driven applications on mobile devices, such as mobile assistants leveraging personal emails or meeting records. However, due to the lengthy prompts and the resource constraints, mobile RAG systems exhibit significantly high response latency. On this issue, one promising approach is to reuse intermediate computational results across different queries to eliminate redundant computation. But most existing approaches, such as KV cache reuse and semantic cache reuse, are designed for cloud settings and perform poorly, overlooking the distinctive characteristics of mobile RAG.
  We propose PerCache, a novel hierarchical cache solution designed for reducing end-to-end latency of personalized RAG applications on mobile platforms. PerCache adopts a hierarchical architecture that progressively matches similar queries and QKV cache to maximize the reuse of intermediate results at different computing stages. To improve cache hit rate, PerCache applies a predictive method to populate cache with queries that are likely to be raised in the future. In addition, PerCache can adapt its configurations to dynamic system loads, aiming at maximizing the caching utility with minimal resource consumption. We implement PerCache on top of an existing mobile LLM inference engine with commodity mobile phones. Extensive evaluations show that PerCache can surpass the best-performing baseline by 34.4% latency reduction across various applications and maintain optimal latency performance under dynamic resource changes.

</details>


### [12] [Computation-Bandwidth-Memory Trade-offs: A Unified Paradigm for AI Infrastructure](https://arxiv.org/abs/2601.11577)
*Yuankai Fan,Qizhen Weng,Xuelong Li*

Main category: cs.DC

TL;DR: AI Trinityæå‡ºè®¡ç®—-å¸¦å®½-å†…å­˜ä¸‰è¦ç´ æƒè¡¡çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡åŠ¨æ€èµ„æºåˆ†é…ä¼˜åŒ–å¯æ‰©å±•AIç³»ç»Ÿæ€§èƒ½


<details>
  <summary>Details</summary>
Motivation: å¤§è§„æ¨¡AIæ¨¡å‹é¢ä¸´ç¡¬ä»¶é™åˆ¶ï¼Œè®¡ç®—ã€å¸¦å®½å’Œå†…å­˜ç›¸äº’åˆ¶çº¦ï¼Œå­¤ç«‹ä¼˜åŒ–æ•ˆæœæœ‰é™ï¼Œéœ€è¦ç»Ÿä¸€æ¡†æ¶æ¥å¹³è¡¡è¿™äº›èµ„æº

Method: æå‡ºAI Trinityæ¡†æ¶ï¼Œå°†è®¡ç®—ã€å¸¦å®½ã€å†…å­˜è§†ä¸ºåŒç­‰é‡è¦çš„æ”¯æŸ±ï¼Œè¯†åˆ«ä¸‰ç§åŸºæœ¬æƒè¡¡å…³ç³»ï¼šæ›´å¤šè®¡ç®—â†’æ›´å°‘å¸¦å®½ã€æ›´å¤šå¸¦å®½â†’æ›´å°‘å†…å­˜ã€æ›´å¤šå†…å­˜â†’æ›´å°‘è®¡ç®—

Result: é€šè¿‡è¾¹ç¼˜-äº‘é€šä¿¡ã€å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨¡å‹æ¨ç†ç­‰ä»£è¡¨æ€§ç³»ç»Ÿè®¾è®¡éªŒè¯äº†AI Trinityçš„æœ‰æ•ˆæ€§

Conclusion: AI Trinityä¸ºå¯æ‰©å±•AIåŸºç¡€è®¾æ–½æä¾›äº†æ–°çš„èŒƒå¼ï¼Œæ—¢æä¾›äº†æ¦‚å¿µåŸºç¡€ï¼Œåˆä¸ºå¹¿æ³›çš„åº”ç”¨åœºæ™¯æä¾›äº†å®è·µæŒ‡å¯¼

Abstract: Large-scale artificial intelligence models are transforming industries and redefining human machine collaboration. However, continued scaling exposes critical limitations in hardware, including constraints on computation, bandwidth, and memory. These dimensions are tightly interconnected, so improvements in one often create bottlenecks in others, making isolated optimizations less effective. Balancing them to maximize system efficiency remains a central challenge in scalable AI design. To address this challenge, we introduce {Computation-Bandwidth-Memory Trade-offs}, termed the {AI Trinity}, a unified paradigm that positions {computation}, {bandwidth}, and {memory} as coequal pillars for next-generation AI infrastructure. AI Trinity enables dynamic allocation of resources across these pillars, alleviating single-resource bottlenecks and adapting to diverse scenarios to optimize system performance. Within this framework, AI Trinity identifies three fundamental trade-offs: (1) {More Computation$\rightarrow$Less Bandwidth}, wherein computational resources are exploited to reduce data transmission under limited bandwidth conditions, (2) {More Bandwidth$\rightarrow$Less Memory}, which exploits abundant communication capacity to populate or refresh memory when local storage resources are constrained, and (3) {More Memory$\rightarrow$Less Computation}, whereby storage capacity are utilized to mitigate redundant computation when computational costs are prohibitive. We illustrate the effectiveness of AI Trinity through representative system designs spanning edge-cloud communication, large-scale distributed training, and model inference. The innovations embodied in AI Trinity advance a new paradigm for scalable AI infrastructure, providing both a conceptual foundation and practical guidance for a broad range of application scenarios.

</details>


### [13] [Cost-Aware Logging: Measuring the Financial Impact of Excessive Log Retention in Small-Scale Cloud Deployments](https://arxiv.org/abs/2601.11584)
*Jody Almaida Putra*

Main category: cs.DC

TL;DR: ç ”ç©¶å‘ç°ï¼Œå°†æ—¥å¿—ä¿ç•™æœŸä»90å¤©ç¼©çŸ­è‡³14å¤©å¯é™ä½78%å­˜å‚¨æˆæœ¬ï¼ŒåŒæ—¶ä¿ç•™97%ä»¥ä¸Šæ“ä½œæœ‰ç”¨æ—¥å¿—ï¼Œä¸ºå°å‹äº‘å›¢é˜Ÿæä¾›æˆæœ¬æ•ˆç›Šåˆ†ææ¡†æ¶ã€‚


<details>
  <summary>Details</summary>
Motivation: æ—©æœŸäº‘éƒ¨ç½²ä¸­ï¼Œæ—¥å¿—ä¿ç•™ç­–ç•¥å¸¸é»˜è®¤è®¾ç½®ä¸º90å¤©æˆ–æ›´é•¿ï¼Œæœªè€ƒè™‘è´¢åŠ¡å’Œæ€§èƒ½å½±å“ï¼Œå¯¼è‡´è¿‡åº¦ä¿ç•™æˆä¸ºéšè—çš„é‡å¤æˆæœ¬ã€‚ç ”ç©¶æ—¨åœ¨ä»æˆæœ¬æ„è¯†è§’åº¦åˆ†ææ—¥å¿—ä¿ç•™çª—å£é€‰æ‹©çš„è´¢åŠ¡å’Œè¿è¥å½±å“ã€‚

Method: ä½¿ç”¨åæ˜ çœŸå®ä¸–ç•Œæ—¥å¿—é‡å’Œè®¿é—®æ¨¡å¼å˜åŒ–çš„åˆæˆæ—¥å¿—æ•°æ®é›†ï¼Œè¯„ä¼°7ã€14ã€30å’Œ90å¤©ä¿ç•™çª—å£ï¼Œèšç„¦ä¸‰ä¸ªæŒ‡æ ‡ï¼šå­˜å‚¨æˆæœ¬ã€æ“ä½œæœ‰ç”¨æ—¥å¿—æ¯”ä¾‹ã€æ¯æœ‰ç”¨æ—¥å¿—æˆæœ¬ã€‚æ“ä½œæœ‰ç”¨æ€§å®šä¹‰ä¸ºæ¨¡æ‹Ÿè°ƒè¯•å’Œäº‹ä»¶åˆ†æä»»åŠ¡æœŸé—´è®¿é—®çš„æ—¥å¿—æ•°æ®ã€‚

Result: ç»“æœæ˜¾ç¤ºï¼Œå°†æ—¥å¿—ä¿ç•™æœŸä»90å¤©å‡å°‘åˆ°14å¤©å¯å°†æ—¥å¿—å­˜å‚¨æˆæœ¬é™ä½é«˜è¾¾78%ï¼ŒåŒæ—¶ä¿ç•™è¶…è¿‡97%çš„æ“ä½œæœ‰ç”¨æ—¥å¿—ã€‚æ›´é•¿ä¿ç•™çª—å£æä¾›é€’å‡çš„è¿è¥å›æŠ¥ï¼ŒåŒæ—¶ä¸æˆæ¯”ä¾‹åœ°å¢åŠ å­˜å‚¨æˆæœ¬å’ŒæŸ¥è¯¢å¼€é”€ã€‚

Conclusion: ç ”ç©¶æœªæå‡ºæ–°æ—¥å¿—æœºåˆ¶ï¼Œè€Œæ˜¯æä¾›è½»é‡çº§å¯è®¿é—®æ¡†æ¶ï¼Œå¸®åŠ©å°å‹å·¥ç¨‹å›¢é˜Ÿé€šè¿‡æˆæœ¬æ•ˆç›Šè§†è§’æ€è€ƒæ—¥å¿—ä¿ç•™ç­–ç•¥ã€‚æ—¨åœ¨é¼“åŠ±åœ¨èµ„æºå—é™äº‘ç¯å¢ƒä¸­è¿›è¡Œæ›´å®¡æ…çš„å¯è§‚æµ‹æ€§é…ç½®ã€‚

Abstract: Log data plays a critical role in observability, debugging, and performance monitoring in modern cloud-native systems. In small and early-stage cloud deployments, however, log retention policies are frequently configured far beyond operational requirements, often defaulting to 90 days or more, without explicit consideration of their financial and performance implications. As a result, excessive log retention becomes a hidden and recurring cost.
  This study examines the financial and operational impact of log retention window selection from a cost-aware perspective. Using synthetic log datasets designed to reflect real-world variability in log volume and access patterns, we evaluate retention windows of 7, 14, 30, and 90 days. The analysis focuses on three metrics: storage cost, operationally useful log ratio, and cost per useful log. Operational usefulness is defined as log data accessed during simulated debugging and incident analysis tasks.
  The results show that reducing log retention from 90 days to 14 days can lower log storage costs by up to 78 percent while preserving more than 97 percent of operationally useful logs. Longer retention windows provide diminishing operational returns while disproportionately increasing storage cost and query overhead. These findings suggest that modest configuration changes can yield significant cost savings without compromising system reliability.
  Rather than proposing new logging mechanisms, this work offers a lightweight and accessible framework to help small engineering teams reason about log retention policies through a cost-effectiveness lens. The study aims to encourage more deliberate observability configurations, particularly in resource-constrained cloud environments.

</details>


### [14] [PLA-Serve: A Prefill-Length-Aware LLM Serving System](https://arxiv.org/abs/2601.11589)
*Jianshu She,Zonghang Li,Hongchao Du,Shangyu Wu,Wenhao Zheng,Eric Xing,Zhengzhong Liu,Huaxiu Yao,Jason Xue,Qirong Ho*

Main category: cs.DC

TL;DR: PLA-Serveé€šè¿‡æ ¹æ®æç¤ºé•¿åº¦å¯¹LLMæœåŠ¡è¯·æ±‚è¿›è¡Œè§£è€¦å’Œæ™ºèƒ½è°ƒåº¦ï¼Œä¼˜åŒ–å¼‚æ„å·¥ä½œè´Ÿè½½æ€§èƒ½ï¼Œæ˜¾è‘—é™ä½å»¶è¿Ÿå¹¶æé«˜ååé‡ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰ç³»ç»Ÿè™½ç„¶è§£è€¦äº†é¢„å¡«å……å’Œè§£ç é˜¶æ®µï¼Œä½†ä»é‡‡ç”¨ç»Ÿä¸€è°ƒåº¦ç­–ç•¥ï¼Œæ— æ³•é€‚åº”æç¤ºé•¿åº¦å˜åŒ–å¸¦æ¥çš„å¼‚æ„å·¥ä½œè´Ÿè½½ç‰¹æ€§ï¼Œå¯¼è‡´æ€§èƒ½ç“¶é¢ˆã€‚

Method: 1. æ ¹æ®æç¤ºé•¿åº¦å°†å¤šè½®é•¿é¢„å¡«å……è¯·æ±‚ä¸çŸ­é¢„å¡«å……è¯·æ±‚è§£è€¦ï¼›2. ä¸ºçŸ­é¢„å¡«å……å·¥ä½œè´Ÿè½½å¼•å…¥é•¿åº¦æ„ŸçŸ¥æ™ºèƒ½æ‰¹å¤„ç†æœºåˆ¶ï¼›3. é‡‡ç”¨åŒé˜Ÿåˆ—è®¾è®¡ï¼Œæ”¯æŒå•å®ä¾‹æ—¶é—´è§£è€¦æˆ–å¤šå®ä¾‹ç©ºé—´è§£è€¦ï¼›4. ä½¿ç”¨æ‰¹å¤„ç†ç­‰å¾…çª—å£å’ŒCUDA Graphèšç±»å‡å°‘å¼‚æ„è®¡ç®—å¹²æ‰°ã€‚

Result: ç›¸æ¯”vanilla SGLangï¼Œé¢„å¡«å……å»¶è¿Ÿé™ä½30%ä»¥ä¸Šï¼›å¤šå®ä¾‹éƒ¨ç½²ä¸­SLOè¿è§„å‡å°‘28%ï¼›ç›¸æ¯”SGLangè·¯ç”±è´Ÿè½½å‡è¡¡ï¼Œå¤šGPUè®¾ç½®ä¸‹SLOè¿è§„è¿›ä¸€æ­¥é™ä½12%ï¼›åœ¨é«˜å¹¶å‘æ··åˆè¯·æ±‚åœºæ™¯ä¸‹ï¼ŒQwen2.5-32Bæ¨¡å‹é¢„å¡«å……å®ä¾‹ååé‡æé«˜35%ã€‚

Conclusion: PLA-Serveé€šè¿‡é•¿åº¦æ„ŸçŸ¥çš„è¯·æ±‚è§£è€¦å’Œè‡ªé€‚åº”è°ƒåº¦ç­–ç•¥ï¼Œæœ‰æ•ˆä¼˜åŒ–äº†å¼‚æ„LLMæœåŠ¡å·¥ä½œè´Ÿè½½ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿæ€§èƒ½å’Œèµ„æºåˆ©ç”¨ç‡ã€‚

Abstract: PLA-Serve identifies and disaggregates requests with different prompt lengths in LLM serving to reduce TTFT latency. While recent systems have decoupled the prefill and decode stages to improve throughput, they still rely on unified scheduling policies that fail to adapt to heterogeneous workload characteristics. We observe that prompt-length variations lead to distinct performance bottlenecks, motivating an adaptive scheduling strategy. PLA-Serve disaggregates multi-turn long-prefill requests from short-prefill ones and introduces a length-aware smart batching mechanism for short-prefill workloads. It adopts a dual-queue design that supports temporal disaggregation on a single prefill instance or spatial disaggregation across multiple instances. For short-prefill batches, a batch waiting window and CUDA Graph-based clustering mitigate interference from heterogeneous computation, reducing batching delay and lowering average latency. In real multi-turn workloads, PLA-Serve reduces prefill latency by over 30% compared to vanilla SGLang under prefill**--**decode disaggregation, and further decreases SLO violations by 28% in multi-instance deployments with vanilla data-parallel configuration. Compared to the SGLang router with load balancing, it further lowers SLO violations by 12% in multi-GPU settings. Under high concurrency and mixed-request scenarios, PLA-Serve improves request throughput by 35% serving Qwen2.5-32B model for prefill instance, demonstrating its effectiveness in optimizing heterogeneous LLM serving workloads.

</details>


### [15] [EPD-Serve: A Flexible Multimodal EPD Disaggregation Inference Serving System On Ascend](https://arxiv.org/abs/2601.11590)
*Fan Bai,Pai Peng,Zhengzhi Tang,Zhe Wang,Gong Chen,Xiang Lu,Yinuo Li,Huan Lin,Weizhe Lin,Yaoyuan Wang,Xiaosong Li*

Main category: cs.DC

TL;DR: EPD-Serveï¼šä¸€ç§é¢å‘å¤šæ¨¡æ€å¤§æ¨¡å‹æ¨ç†çš„é˜¶æ®µçº§è§£è€¦æœåŠ¡ç³»ç»Ÿï¼Œé€šè¿‡å°†æ¨ç†æµæ°´çº¿è§£è€¦ä¸ºç‹¬ç«‹çš„ç¼–ç ã€é¢„å¡«å……å’Œè§£ç é˜¶æ®µï¼Œå¹¶å¼•å…¥å¼‚æ­¥ç‰¹å¾é¢„å–å’Œåˆ†å±‚KVç¼“å­˜ä¼ è¾“æœºåˆ¶ï¼Œæ˜¾è‘—æå‡ç³»ç»Ÿååé‡ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰å¤šæ¨¡æ€æ¨ç†ç³»ç»Ÿé€šå¸¸é‡‡ç”¨å•ä½“æ¶æ„ï¼Œå°†ç¼–ç ã€é¢„å¡«å……å’Œè§£ç é˜¶æ®µç´§å¯†è€¦åˆåœ¨åŒæ„ç¡¬ä»¶ä¸Šï¼Œå¿½è§†äº†å„é˜¶æ®µçš„å¼‚æ„è®¡ç®—ç‰¹æ€§ï¼Œå¯¼è‡´èµ„æºåˆ©ç”¨ç‡ä½å’Œç³»ç»Ÿååé‡å—é™ã€‚

Method: æå‡ºEPD-Serveç³»ç»Ÿï¼š1ï¼‰å°†æ¨ç†æµæ°´çº¿è§£è€¦ä¸ºç‹¬ç«‹çš„ç¼–ç ã€é¢„å¡«å……å’Œè§£ç é˜¶æ®µï¼›2ï¼‰åˆ©ç”¨æ˜‡è…¾äº’è”æ‹“æ‰‘ï¼Œå¼•å…¥ç¼–ç ä¸é¢„å¡«å……é˜¶æ®µé—´çš„å¼‚æ­¥ç‰¹å¾é¢„å–æœºåˆ¶ï¼Œä»¥åŠé¢„å¡«å……ä¸è§£ç é˜¶æ®µé—´çš„åˆ†å±‚åˆ†ç»„KVç¼“å­˜ä¼ è¾“æœºåˆ¶ï¼›3ï¼‰ç»“åˆå¤šè·¯ç”±è°ƒåº¦ã€å®ä¾‹çº§è´Ÿè½½å‡è¡¡å’Œå¤šé˜¶æ®µç¡¬ä»¶å…±ç½®ç­‰ä¼˜åŒ–æŠ€æœ¯ã€‚

Result: åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹ï¼Œç›¸æ¯”PDè§£è€¦éƒ¨ç½²ï¼ŒEPD-Serveå°†ç«¯åˆ°ç«¯ååé‡æå‡äº†57.37-69.48%ï¼ŒåŒæ—¶æ»¡è¶³ä¸¥æ ¼çš„SLOçº¦æŸï¼šTTFTä½äº2000msï¼ŒTPOTä½äº50msã€‚

Conclusion: é˜¶æ®µçº§è§£è€¦æ¶æ„èƒ½æœ‰æ•ˆä¼˜åŒ–å¤šæ¨¡æ€å¤§æ¨¡å‹æ¨ç†ç³»ç»Ÿï¼ŒEPD-Serveé€šè¿‡è§£è€¦è®¾è®¡ã€é€šä¿¡ä¼˜åŒ–å’Œèµ„æºç®¡ç†ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†ç³»ç»Ÿæ€§èƒ½å’Œèµ„æºåˆ©ç”¨ç‡ã€‚

Abstract: With the widespread adoption of large multimodal models, efficient inference across text, image, audio, and video modalities has become critical. However, existing multimodal inference systems typically employ monolithic architectures that tightly couple the Encode, Prefill, and Decode stages on homogeneous hardware, neglecting the heterogeneous computational characteristics of each stage. This design leads to inefficient resource utilization and limited system throughput. To address these issues, we propose EPD-Serve, a stage-level disaggregated inference serving system for multimodal models. EPD-Serve decouples the inference pipeline into independent Encode, Prefill, and Decode stages, enabling logical isolation and flexible co-located deployment through dynamic orchestration. Leveraging the Ascend interconnect topology, EPD-Serve introduces asynchronous feature prefetching between Encode and Prefill stages and a hierarchical grouped KV cache transmission mechanism between Prefill and Decode stages to improve cross-node communication efficiency. In addition, EPD-Serve incorporates multi-route scheduling, instance-level load balancing, and multi-stage hardware co-location with spatial multiplexing to better support diverse multimodal workloads. Comprehensive experiments on multimodal understanding models demonstrate that, under high-concurrency scenarios, EPD-Serve improves end-to-end throughput by 57.37-69.48% compared to PD-disaggregated deployment, while satisfying strict SLO constraints, including TTFT below 2000 ms and TPOT below 50 ms. These results highlight the effectiveness of stage-level disaggregation for optimizing multimodal large model inference systems.

</details>


### [16] [Enhancing Model Context Protocol (MCP) with Context-Aware Server Collaboration](https://arxiv.org/abs/2601.11595)
*Meenakshi Amulya Jayanti,X. Y. Han*

Main category: cs.DC

TL;DR: æœ¬æ–‡æå‡ºäº†ä¸€ç§ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„MCPï¼ˆCA-MCPï¼‰ï¼Œé€šè¿‡å¼•å…¥å…±äº«ä¸Šä¸‹æ–‡å­˜å‚¨æ¥æ”¹è¿›ä¼ ç»Ÿçš„MCPæ¡†æ¶ï¼Œä½¿å¤šæ™ºèƒ½ä½“ç³»ç»Ÿèƒ½å¤Ÿæ›´é«˜æ•ˆåœ°åè°ƒå·¥ä½œã€‚


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»ŸMCPæ¡†æ¶ä¸­çš„æ™ºèƒ½ä½“ã€æ¨¡å‹å’ŒæœåŠ¡å™¨éƒ½æ˜¯æ— çŠ¶æ€çš„ï¼Œç¼ºä¹å…¨å±€ä¸Šä¸‹æ–‡è®¿é—®èƒ½åŠ›ã€‚åœ¨æ¶‰åŠLLMé©±åŠ¨çš„åè°ƒä»»åŠ¡ä¸­ï¼Œå…±äº«ä¸Šä¸‹æ–‡å­˜å‚¨ï¼ˆSCSï¼‰å¯ä»¥é€šè¿‡å‡å°‘å†—ä½™å’Œå®ç°æœåŠ¡å™¨é—´çŸ¥è¯†è½¬ç§»æ¥æé«˜å¤šæ™ºèƒ½ä½“å·¥ä½œæµçš„æ•ˆç‡å’Œä¸€è‡´æ€§ã€‚

Method: è®¾è®¡äº†ä¸Šä¸‹æ–‡æ„ŸçŸ¥MCPï¼ˆCA-MCPï¼‰ï¼Œå°†æ‰§è¡Œé€»è¾‘å¸è½½åˆ°ä¸“é—¨çš„MCPæœåŠ¡å™¨ï¼Œè¿™äº›æœåŠ¡å™¨å¯ä»¥è¯»å–å’Œå†™å…¥å…±äº«ä¸Šä¸‹æ–‡å†…å­˜ã€‚ä¸Šä¸‹æ–‡ç®¡ç†ä½œä¸ºæ ¸å¿ƒæœºåˆ¶ï¼Œé€šè¿‡è·Ÿè¸ªä¸­é—´çŠ¶æ€å’Œå…±äº«å˜é‡æ¥ç»´æŒä»»åŠ¡æ‰§è¡Œçš„è¿ç»­æ€§ã€‚

Result: å®éªŒè¡¨æ˜ï¼ŒCA-MCPåœ¨å¤æ‚ä»»åŠ¡ä¸­å‡å°‘äº†LLMè°ƒç”¨æ¬¡æ•°ï¼Œé™ä½äº†ä»»åŠ¡æ¡ä»¶ä¸æ»¡è¶³æ—¶çš„å“åº”å¤±è´¥é¢‘ç‡ã€‚åœ¨TravelPlannerå’ŒREALM-BenchåŸºå‡†æ•°æ®é›†ä¸Šè·å¾—äº†ç»Ÿè®¡æ˜¾è‘—çš„ç»“æœã€‚

Conclusion: CA-MCPé€šè¿‡å¼•å…¥å…±äº«ä¸Šä¸‹æ–‡å­˜å‚¨ï¼Œæ˜¾è‘—æé«˜äº†LLMé©±åŠ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„æ•ˆç‡å’Œå“åº”æ€§ï¼Œå±•ç¤ºäº†åœ¨å¤æ‚åè°ƒä»»åŠ¡ä¸­çš„æ½œåœ¨ä¼˜åŠ¿ã€‚

Abstract: The Model Context Protocol (MCP) has emerged as a widely used framework for enabling LLM-based agents to communicate with external tools and services. The most common implementation of MCP, proposed by Anthropic, heavily relies on a Large Language Model (LLM) to decompose tasks and issue instructions to servers, which act as stateless executors. In particular, the agents, models, and servers are stateless and do not have access to a global context. However, in tasks involving LLM-driven coordination, it is natural that a Shared Context Store (SCS) could improve the efficiency and coherence of multi-agent workflows by reducing redundancy and enabling knowledge transfer between servers. Thus, in this work, we design and assess the performance of a Context-Aware MCP (CA-MCP) that offloads execution logic to specialized MCP servers that read from and write to a shared context memory, allowing them to coordinate more autonomously in real time. In this design, context management serves as the central mechanism that maintains continuity across task executions by tracking intermediate states and shared variables, thereby enabling persistent collaboration among agents without repeated prompting. We present experiments showing that the CA-MCP can outperform the traditional MCP by reducing the number of LLM calls required for complex tasks and decreasing the frequency of response failures when task conditions are not satisfied, thereby improving overall efficiency and responsiveness. In particular, we conducted experiments on the TravelPlanner and REALM-Bench benchmark datasets and observed statistically significant results indicating the potential advantages of incorporating a shared context store via CA-MCP in LLM-driven multi-agent systems.

</details>


### [17] [Hardware-Aware Reformulation of Convolutions for Efficient Execution on Specialized AI Hardware: A Case Study on NVIDIA Tensor Cores](https://arxiv.org/abs/2601.11608)
*Ganesh Bikshandi*

Main category: cs.DC

TL;DR: æå‡ºä¸€ç§ç¡¬ä»¶æ„ŸçŸ¥çš„CNNè®¡ç®—é‡å†™æ–¹æ³•ï¼Œé€šè¿‡åè®­ç»ƒæ•°å­¦é‡æ„æ»¡è¶³ç¡¬ä»¶å¯¹é½è¦æ±‚ï¼Œæ— éœ€ä¿®æ”¹ç½‘ç»œæƒé‡


<details>
  <summary>Details</summary>
Motivation: ç°ä»£AIç¡¬ä»¶ï¼ˆå¦‚NVIDIA Tensor Coreså’ŒoneDNNæ¡†æ¶ï¼‰å¯¹è¾“å…¥é€šé“æœ‰å¯¹é½è¦æ±‚ï¼ˆå¦‚8æˆ–512çš„å€æ•°ï¼‰ï¼Œä¼ ç»Ÿé›¶å¡«å……æ–¹æ³•æ•ˆç‡ä½ä¸‹ï¼Œéœ€è¦æ›´ä¼˜çš„ç¡¬ä»¶å¯¹é½è§£å†³æ–¹æ¡ˆ

Method: ä½¿ç”¨é‡å†™è§„åˆ™å¯¹CNNè®¡ç®—è¿›è¡Œç¡¬ä»¶æ„ŸçŸ¥çš„é‡æ–°è¡¨è¿°ï¼Œé€šè¿‡æ•°å­¦é‡æ„æ»¡è¶³ç¡¬ä»¶å¯¹é½è¦æ±‚ï¼Œå®Œå…¨åœ¨åè®­ç»ƒé˜¶æ®µå®Œæˆï¼Œä¸ä¿®æ”¹ç½‘ç»œæƒé‡

Result: å½“å‰å®ç°ä¸“æ³¨äºTensor Coresçš„å•ä¸€å˜æ¢ï¼Œä½†è¯¥æ–¹æ³•å¯æ¨å¹¿åˆ°CPUå’Œå…¶ä»–åŠ é€Ÿå™¨çš„æ›´å¤šå˜æ¢ï¼Œä¸ºè¯­ä¹‰è°ƒä¼˜å¥ å®šåŸºç¡€

Conclusion: è¿™æ˜¯è¿ˆå‘è¯­ä¹‰è°ƒä¼˜çš„åˆæ­¥æ¢ç´¢ï¼Œä¸ºCNNæ¨¡å‹åœ¨ä¸“ç”¨AIç¡¬ä»¶ä¸Šçš„é«˜æ•ˆéƒ¨ç½²æä¾›ç³»ç»ŸåŒ–çš„ç¡¬ä»¶æ„ŸçŸ¥ä¼˜åŒ–ç­–ç•¥

Abstract: Convolutional Neural Networks (CNNs) are central to modern AI, but their performance is often limited by hardware constraints. NVIDIA Tensor Cores, for instance, require input channels to be multiples of 8 and sometimes 512 for efficient execution. {\em oneDNN} framework for CPU imposes such a requirement for the blocked format. Traditional approaches address such alignment issue using zero-padding, which can be inefficient. In this work, we present a first-step, hardware-aware reformulation of CNN computations using rewrite rules, restructuring the underlying math to satisfy hardware alignment entirely {\bf post-training} without modifying network weights. While our current implementation focuses on a single transformation for Tensor Cores, this approach is generalizable, laying the foundation to explore additional transformations for CPU and accelerators. This study represents an initial step toward {\em semantic tuning}, a systematic, hardware-aware optimization strategy for efficient deployment of CNN models on specialized AI hardware.

</details>


### [18] [Radio Labeling of Strong Prismatic Network With Star](https://arxiv.org/abs/2601.11624)
*Liming Wang,Feng Li,Linlin Cui*

Main category: cs.DC

TL;DR: æœ¬æ–‡ç ”ç©¶æ˜Ÿå½¢å¼ºæ£±æŸ±ç½‘ç»œçš„æ— çº¿é¢‘è°±åˆ†é…é—®é¢˜ï¼Œé€šè¿‡å›¾è®ºä¸­çš„æ— çº¿ç”µæ ‡å·æ¨¡å‹ï¼Œæå‡ºç›¸å…³å®šç†ã€ç¤ºä¾‹å’Œå¹¶è¡Œç®—æ³•ä»¥æé«˜å¤§è§„æ¨¡ç½‘ç»œçš„è®¡ç®—æ•ˆç‡ã€‚


<details>
  <summary>Details</summary>
Motivation: æ— çº¿é€šä¿¡çš„å¿«é€Ÿå‘å±•ä½¿å¾—é«˜æ•ˆé¢‘è°±åˆ†é…æˆä¸ºæå‡ç½‘ç»œæ€§èƒ½çš„å…³é”®å› ç´ ã€‚æ— çº¿ç”µæ ‡å·ä½œä¸ºä¿¡é“åˆ†é…çš„ç»„åˆä¼˜åŒ–æ¨¡å‹ï¼Œæ˜¯ä¸€ä¸ªNPéš¾é—®é¢˜ã€‚ç ”ç©¶ç‰¹å®šå›¾ç±»çš„æ— çº¿ç”µæ ‡å·å¯¹äºæ— çº¿ç½‘ç»œçš„æœ€ä¼˜ä¿¡é“åˆ†é…è®¾è®¡å…·æœ‰é‡è¦æ„ä¹‰ã€‚

Method: å°†é¢‘è°±åˆ†é…é—®é¢˜è½¬åŒ–ä¸ºå›¾çš„æ— çº¿ç”µæ ‡å·é—®é¢˜ï¼Œç ”ç©¶æ˜Ÿå½¢å¼ºæ£±æŸ±ç½‘ç»œçš„æ— çº¿ç”µæ ‡å·ã€‚æå‡ºç›¸å…³å®šç†å’Œç¤ºä¾‹ï¼Œå¹¶è®¾è®¡å¹¶è¡Œç®—æ³•ä»¥æé«˜å¤§è§„æ¨¡ç½‘ç»œåœºæ™¯ä¸‹çš„è®¡ç®—æ•ˆç‡ã€‚

Result: è®¨è®ºäº†æ˜Ÿå½¢å¼ºæ£±æŸ±ç½‘ç»œçš„æ— çº¿ç”µæ ‡å·ï¼Œæå‡ºäº†ç›¸å…³å®šç†å’Œç¤ºä¾‹ï¼Œå¹¶å¼€å‘äº†å¹¶è¡Œç®—æ³•æ¥æå‡å¤§è§„æ¨¡ç½‘ç»œçš„è®¡ç®—æ•ˆç‡ã€‚

Conclusion: å¼ºç§¯æ˜¯æ„å»ºæ­£åˆ™ç½‘ç»œçš„é‡è¦å·¥å…·ï¼Œç ”ç©¶å…¶æ— çº¿ç”µæ ‡å·å¯¹äºæ— çº¿ç½‘ç»œçš„æœ€ä¼˜ä¿¡é“åˆ†é…è®¾è®¡æ˜¯å¿…è¦çš„ã€‚æœ¬æ–‡çš„ç ”ç©¶ä¸ºå¤§è§„æ¨¡ç½‘ç»œåœºæ™¯ä¸‹çš„é«˜æ•ˆé¢‘è°±åˆ†é…æä¾›äº†ç†è®ºæ”¯æŒå’Œç®—æ³•è§£å†³æ–¹æ¡ˆã€‚

Abstract: The rapid development of wireless communication has made efficient spectrum assignment a crucial factor in enhancing network performance. As a combinatorial optimization model for channel assignment, the radio labeling is recognized as an NP-hard problem. Therefore, converting the spectrum assignment problem into the radio labeling of graphs and studying the radio labeling of specific graph classes is of great significance. For $G$, a radio labeling $\varphi: V(G) \to \{0, 1, 2, \ldots\}$ is required to satisfy $|\varphi(u) - \varphi(v)| \geq \text{diam}(G) + 1 -d_G(u, v)$, where ${diam(G)}$ and $d_G(u, v)$ are diameter and distance between $u$ and $v$. For a radio labeling $\varphi$, its $\text{span}$ is defined as the largest integer assigned by $\varphi$ to the vertices of $G$; the radio labeling specifically denotes the labeling with the minimal span among possible radio labeling. The strong product is a crucial tool for constructing regular networks, and studying its radio labeling is necessary for the design of optimal channel assignment in wireless networks. Within this manuscript, we discuss the radio labeling of strong prismatic network with star, present the relevant theorems and examples, and propose a parallel algorithm to improve computational efficiency in large-scale network scenarios.

</details>


### [19] [A Forward Simulation-Based Hierarchy of Linearizable Concurrent Objects](https://arxiv.org/abs/2601.11646)
*Chao Wang,Ruijia Li,Yang Zhou,Peng Wu,Yi Lv,Jianwei Liao,Jim Woodcock,Zhiming Liu*

Main category: cs.DC

TL;DR: æœ¬æ–‡ç³»ç»Ÿç ”ç©¶äº†çº¿æ€§åŒ–å¯¹è±¡ä¸å‘å‰æ¨¡æ‹Ÿä¹‹é—´çš„å…³ç³»ï¼Œè¯æ˜äº†åœ¨ä¸åŒæ´»æ€§çº¦æŸä¸‹çº¿æ€§åŒ–å¯¹è±¡é›†åˆåœ¨å‘å‰æ¨¡æ‹Ÿå…³ç³»ä¸‹å½¢æˆæœ‰ç•ŒåŠæ ¼æˆ–æ ¼ç»“æ„ï¼Œå¹¶æå‡ºäº†çº¿æ€§åŒ–çš„ç­‰ä»·ç‰¹å¾åŒ–æ–¹æ³•ã€‚


<details>
  <summary>Details</summary>
Motivation: ç ”ç©¶çº¿æ€§åŒ–å¯¹è±¡ä¸å‘å‰æ¨¡æ‹Ÿä¹‹é—´çš„ç†è®ºè”ç³»ï¼Œä¸ºå¹¶å‘å¯¹è±¡çš„å½¢å¼åŒ–éªŒè¯æä¾›ç†è®ºåŸºç¡€ã€‚å½“å‰ç¼ºä¹å¯¹çº¿æ€§åŒ–å¯¹è±¡åœ¨å‘å‰æ¨¡æ‹Ÿå…³ç³»ä¸‹ä»£æ•°ç»“æ„çš„ç³»ç»Ÿåˆ†æï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨è¿™ç§å…³ç³»ç®€åŒ–çº¿æ€§åŒ–éªŒè¯ã€‚

Method: é‡‡ç”¨å½¢å¼åŒ–æ–¹æ³•åˆ†æçº¿æ€§åŒ–å¯¹è±¡ä¸å‘å‰æ¨¡æ‹Ÿçš„å…³ç³»ï¼š1) è¯æ˜åœ¨ä¸åŒæ´»æ€§çº¦æŸä¸‹çº¿æ€§åŒ–å¯¹è±¡é›†åˆå½¢æˆæœ‰ç•ŒåŠæ ¼æˆ–æ ¼ç»“æ„ï¼›2) æå‡ºé€šè¿‡å¯¹è±¡ğ’°_ğ‘†ğ‘ğ‘’ğ‘å°†çº¿æ€§åŒ–éªŒè¯è½¬åŒ–ä¸ºå‘å‰æ¨¡æ‹Ÿæ£€æŸ¥çš„ç­‰ä»·ç‰¹å¾åŒ–æ–¹æ³•ï¼›3) é€šè¿‡å…·ä½“æ¡ˆä¾‹è¯æ˜å¯¹è±¡é—´çš„æ¨¡æ‹Ÿå…³ç³»ã€‚

Result: 1) è¯æ˜äº†ç­‰å¾…è‡ªç”±ã€é”è‡ªç”±å’Œéšœç¢è‡ªç”±çº¿æ€§åŒ–å¯¹è±¡é›†åˆåœ¨å‘å‰æ¨¡æ‹Ÿå…³ç³»ä¸‹å½¢æˆæœ‰ç•Œå¹¶åŠæ ¼ï¼›2) æ— æ´»æ€§çº¦æŸçš„çº¿æ€§åŒ–å¯¹è±¡é›†åˆå½¢æˆæœ‰ç•Œæ ¼ï¼›3) æå‡ºäº†çº¿æ€§åŒ–çš„ç­‰ä»·ç‰¹å¾åŒ–æ–¹æ³•ï¼›4) è¯æ˜äº†å¼ºçº¿æ€§åŒ–å¯¹è±¡é—´çš„ç›¸äº’æ¨¡æ‹Ÿå…³ç³»ï¼Œä»¥åŠæ—¶é—´æˆ³é˜Ÿåˆ—æ¨¡æ‹ŸHerlihy-Wingé˜Ÿåˆ—ï¼›5) è¯æ˜äº†Herlihy-Wingé˜Ÿåˆ—è¢«ğ’°_ğ‘†ğ‘ğ‘’ğ‘æ¨¡æ‹Ÿã€‚

Conclusion: çº¿æ€§åŒ–å¯¹è±¡åœ¨å‘å‰æ¨¡æ‹Ÿå…³ç³»ä¸‹å…·æœ‰ä¸°å¯Œçš„ä»£æ•°ç»“æ„ï¼Œæå‡ºçš„ç­‰ä»·ç‰¹å¾åŒ–æ–¹æ³•ä¸ºçº¿æ€§åŒ–éªŒè¯æä¾›äº†æ–°é€”å¾„ã€‚è¿™äº›ç†è®ºç»“æœä¸ºå¹¶å‘å¯¹è±¡çš„å½¢å¼åŒ–éªŒè¯å’Œè®¾è®¡æä¾›äº†é‡è¦ç†è®ºåŸºç¡€ã€‚

Abstract: In this paper, we systematically investigate the connection between linearizable objects and forward simulation. We prove that the sets of linearizable objects satisfying wait-freedom (resp., lock-freedom or obstruction-freedom) form a bounded join-semilattice under the forward simulation relation, and that the sets of linearizable objects without liveness constraints form a bounded lattice under the same relation. As part of our lattice result, we propose an equivalent characterization of linearizability by reducing checking linearizability w.r.t. sequential specification $Spec$ into checking forward simulation by an object $\mathcal{U}_{Spec}$. To demonstrate the forward simulation relation between linearizable objects, we prove that the objects that are strongly linearizable w.r.t. the same sequential specification and are wait-free (resp., lock-free, obstruction-free) simulate each other, and we prove that the time-stamped queue simulates the Herlihy-Wing queue. We also prove that the Herlihy-Wing queue is simulated by $\mathcal{U}_{Spec}$, and thus, our equivalent characterization of linearizability can be used in the verification of linearizability.

</details>


### [20] [WISP: Waste- and Interference-Suppressed Distributed Speculative LLM Serving at the Edge via Dynamic Drafting and SLO-Aware Batching](https://arxiv.org/abs/2601.11652)
*Xiangchen Li,Jiakun Fan,Qingyuan Wang,Dimitrios Spatharakis,Saeid Ghafouri,Hans Vandierendonck,Deepu John,Bo Ji,Ali R. Butt,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: WISPï¼šä¸€ç§é«˜æ•ˆçš„åˆ†å¸ƒå¼LLMæ¨ç†ç³»ç»Ÿï¼Œé€šè¿‡æ™ºèƒ½æ¨æµ‹è§£ç è§£å†³è¾¹ç¼˜è®¾å¤‡ä¸äº‘ç«¯è´Ÿè½½ä¸å‡è¡¡é—®é¢˜ï¼Œæå‡ç³»ç»Ÿå®¹é‡å’Œååé‡


<details>
  <summary>Details</summary>
Motivation: éšç€LLMè¶Šæ¥è¶Šæ™®åŠï¼Œå¤§é‡æ¨ç†è¯·æ±‚ä»è¾¹ç¼˜è®¾å¤‡å‘èµ·ä½†åœ¨ä¸­å¿ƒGPUé›†ç¾¤è®¡ç®—ï¼Œå¯¼è‡´æ•°æ®ä¸­å¿ƒè´Ÿè½½è¿‡é‡è€Œè¾¹ç¼˜è®¾å¤‡åˆ©ç”¨ç‡ä¸è¶³ï¼Œé€ æˆç½‘ç»œèµ„æºæ•ˆç‡ä½ä¸‹å’Œè´Ÿè½½ä¸å‡è¡¡

Method: æå‡ºWISPç³»ç»Ÿï¼ŒåŒ…å«æ™ºèƒ½æ¨æµ‹æ§åˆ¶å™¨ã€éªŒè¯æ—¶é—´ä¼°è®¡å™¨å’ŒéªŒè¯æ‰¹è°ƒåº¦å™¨ï¼Œé€šè¿‡è§£å†³"æµªè´¹çš„è‰ç¨¿æ—¶é—´"å’Œ"éªŒè¯å¹²æ‰°"ä¸¤ä¸ªå…³é”®ç“¶é¢ˆï¼Œä¼˜åŒ–è¾¹ç¼˜è®¾å¤‡è‰ç¨¿ç”Ÿæˆæ•ˆç‡å’Œäº‘ç«¯éªŒè¯è°ƒåº¦

Result: ç›¸æ¯”é›†ä¸­å¼æœåŠ¡å’ŒSLEDï¼ŒWISPåˆ†åˆ«å°†ç³»ç»Ÿå®¹é‡æå‡2.1å€å’Œ4.1å€ï¼Œç³»ç»Ÿæœ‰æ•ˆååé‡æå‡1.94å€å’Œ3.7å€

Conclusion: WISPé€šè¿‡åˆ†å¸ƒå¼æ¨æµ‹è§£ç æœ‰æ•ˆå¹³è¡¡è¾¹ç¼˜ä¸äº‘ç«¯è´Ÿè½½ï¼Œåœ¨ä¿æŒæ— æŸé¢„æµ‹ç²¾åº¦çš„åŒæ—¶æ˜¾è‘—æå‡LLMæ¨ç†ç³»ç»Ÿçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§

Abstract: As Large Language Models (LLMs) become increasingly accessible to end users, an ever-growing number of inference requests are initiated from edge devices and computed on centralized GPU clusters. However, the resulting exponential growth in computation workload is placing significant strain on data centers, while edge devices remain largely underutilized, leading to imbalanced workloads and resource inefficiency across the network. Integrating edge devices into the LLM inference process via speculative decoding helps balance the workload between the edge and the cloud, while maintaining lossless prediction accuracy. In this paper, we identify and formalize two critical bottlenecks that limit the efficiency and scalability of distributed speculative LLM serving: Wasted Drafting Time and Verification Interference. To address these challenges, we propose WISP, an efficient and SLO-aware distributed LLM inference system that consists of an intelligent speculation controller, a verification time estimator, and a verification batch scheduler. These components collaboratively enhance drafting efficiency and optimize verification request scheduling on the server. Extensive numerical results show that WISP improves system capacity by up to 2.1x and 4.1x, and increases system goodput by up to 1.94x and 3.7x, compared to centralized serving and SLED, respectively.

</details>


### [21] [HALO: Semantic-Aware Distributed LLM Inference in Lossy Edge Network](https://arxiv.org/abs/2601.11676)
*Peirong Zheng,Wenchao Xu,Haozhao Wang,Jinyu Chen,Xuemin Shen*

Main category: cs.DC

TL;DR: HALOæ˜¯ä¸€ä¸ªç”¨äºæå‡è¾¹ç¼˜ç½‘ç»œä¸­åˆ†å¸ƒå¼LLMæ¨ç†æ€§èƒ½çš„æ¡†æ¶ï¼Œé€šè¿‡æ¾å¼›åŒæ­¥ç­–ç•¥å’Œæ™ºèƒ½ç¥ç»å…ƒç»„åˆ†é…æ¥åº”å¯¹ä¸å¯é ç½‘ç»œæ¡ä»¶ã€‚


<details>
  <summary>Details</summary>
Motivation: è¾¹ç¼˜éƒ¨ç½²LLMæ¨ç†å¯ä»¥æé«˜æœåŠ¡å“åº”é€Ÿåº¦å¹¶ä¿æŠ¤ç”¨æˆ·éšç§ï¼Œä½†å•ä¸ªè¾¹ç¼˜èŠ‚ç‚¹çš„èµ„æºé™åˆ¶æ˜¯ä¸»è¦æŒ‘æˆ˜ã€‚åˆ†å¸ƒå¼æ¨ç†å¯ä»¥èšåˆå¤šä¸ªè®¾å¤‡çš„è®¡ç®—èµ„æºï¼Œä½†ç°æœ‰æ–¹æ³•éœ€è¦ä¸¥æ ¼åŒæ­¥ï¼Œè¿™åœ¨ä¸å¯é çš„ç½‘ç»œæ¡ä»¶ä¸‹å¾€å¾€ä¸å¯è¡Œã€‚

Method: HALOæ¡†æ¶åŒ…å«ä¸‰ä¸ªå…³é”®æœºåˆ¶ï¼š1) è¯­ä¹‰æ„ŸçŸ¥é¢„æµ‹å™¨ï¼Œåœ¨æ¿€æ´»å‰è¯„ä¼°ç¥ç»å…ƒç»„çš„é‡è¦æ€§ï¼›2) æ¨¡å‹æ¨ç†æœŸé—´çš„ç¥ç»å…ƒç»„åŠ è½½å¹¶è¡Œæ‰§è¡Œæ–¹æ¡ˆï¼›3) è´Ÿè½½å‡è¡¡è°ƒåº¦å™¨ï¼Œæœ‰æ•ˆåè°ƒå¼‚æ„èµ„æºçš„å¤šä¸ªè®¾å¤‡ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å°†ä¸å¤ªå…³é”®çš„ç¥ç»å…ƒç»„åˆ†é…ç»™ä¸ç¨³å®šçš„è®¾å¤‡ï¼Œå®ç°æ¾å¼›ä½†æœ‰æ•ˆçš„åŒæ­¥ï¼Œé¿å…å»¶è¿Ÿæ•°æ®åŒ…å¯¼è‡´çš„è¿‡åº¦ç­‰å¾…æ—¶é—´ã€‚

Result: åœ¨æ ‘è“æ´¾é›†ç¾¤ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒHALOåœ¨ä¸å¯é ç½‘ç»œæ¡ä»¶ä¸‹ä¸ºLLaMAç³»åˆ—LLMå®ç°äº†3.41å€çš„ç«¯åˆ°ç«¯åŠ é€Ÿã€‚å®ƒåœ¨å„ç§åœºæ™¯ä¸‹ä¿æŒä¸æœ€ä¼˜æ¡ä»¶ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ã€‚

Conclusion: HALOæ¡†æ¶æˆåŠŸè§£å†³äº†è¾¹ç¼˜ç½‘ç»œä¸­åˆ†å¸ƒå¼LLMæ¨ç†çš„åŒæ­¥ç“¶é¢ˆé—®é¢˜ï¼Œé€šè¿‡æ™ºèƒ½çš„ç¥ç»å…ƒç»„åˆ†é…å’Œæ¾å¼›åŒæ­¥ç­–ç•¥ï¼Œåœ¨ä¸å¯é ç½‘ç»œæ¡ä»¶ä¸‹å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ï¼Œä¸ºè¾¹ç¼˜AIæ¨ç†æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

Abstract: The deployment of large language models' (LLMs) inference at the edge can facilitate prompt service responsiveness while protecting user privacy. However, it is critically challenged by the resource constraints of a single edge node. Distributed inference has emerged to aggregate and leverage computational resources across multiple devices. Yet, existing methods typically require strict synchronization, which is often infeasible due to the unreliable network conditions. In this paper, we propose HALO, a novel framework that can boost the distributed LLM inference in lossy edge network. The core idea is to enable a relaxed yet effective synchronization by strategically allocating less critical neuron groups to unstable devices, thus avoiding the excessive waiting time incurred by delayed packets. HALO introduces three key mechanisms: (1) a semantic-aware predictor to assess the significance of neuron groups prior to activation. (2) a parallel execution scheme of neuron group loading during the model inference. (3) a load-balancing scheduler that efficiently orchestrates multiple devices with heterogeneous resources. Experimental results from a Raspberry Pi cluster demonstrate that HALO achieves a 3.41x end-to-end speedup for LLaMA-series LLMs under unreliable network conditions. It maintains performance comparable to optimal conditions and significantly outperforms the state-of-the-art in various scenarios.

</details>


### [22] [RAPID-Serve: Resource-efficient and Accelerated P/D Intra-GPU Disaggregation](https://arxiv.org/abs/2601.11822)
*Amna Masood,Pratishtha Gaur,Nuwan Jayasena*

Main category: cs.DC

TL;DR: RAPID-Serveï¼šä¸€ç§åœ¨ç›¸åŒGPUä¸Šå¹¶å‘æ‰§è¡Œé¢„å¡«å……å’Œè§£ç çš„æŠ€æœ¯ï¼Œåœ¨æ»¡è¶³å»¶è¿ŸSLOçš„åŒæ—¶ä¿æŒé«˜ååé‡å’Œèµ„æºåˆ©ç”¨ç‡ï¼Œç›¸æ¯”ç°æœ‰æŠ€æœ¯æœ‰æ˜¾è‘—æå‡ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰LLMæ¨ç†æœåŠ¡ç³»ç»Ÿçš„ä¸¤ç§ä¸»æµæŠ€æœ¯å„æœ‰ç¼ºé™·ï¼šæ··åˆæ‰¹å¤„ç†è™½ç„¶æé«˜èµ„æºåˆ©ç”¨ç‡å’Œååé‡ï¼Œä½†å¢åŠ æ¯tokenå»¶è¿Ÿï¼›è§£è€¦æœåŠ¡è™½ç„¶ä¼˜åŒ–SLOï¼Œä½†å¯¼è‡´èµ„æºåˆ©ç”¨ä¸è¶³å’ŒKVç¼“å­˜ä¼ è¾“å¼€é”€ã€‚éœ€è¦ä¸€ç§èƒ½åŒæ—¶æ»¡è¶³å»¶è¿ŸSLOã€é«˜ååé‡å’Œé«˜æ•ˆèµ„æºåˆ©ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚

Method: æå‡ºRAPID-ServeæŠ€æœ¯ï¼Œåœ¨ç›¸åŒGPUä¸Šå¹¶å‘æ‰§è¡Œé¢„å¡«å……å’Œè§£ç é˜¶æ®µï¼›é‡‡ç”¨è‡ªé€‚åº”èµ„æºç®¡ç†è¿›è¡Œè¿è¡Œæ—¶è®¡ç®—èµ„æºåˆ†é…ï¼›å¯é€‰åˆ©ç”¨AMD Instinctâ„¢ GPUä¸Šçš„CUæ©ç ï¼ˆç»†ç²’åº¦è®¡ç®—å•å…ƒåˆ†åŒºåŠŸèƒ½ï¼‰å®ç°æ›´ç²¾ç»†çš„èµ„æºæ§åˆ¶ã€‚

Result: RAPID-Serveæä¾›é«˜è¾¾4.1å€ï¼ˆå¹³å‡1.7å€ï¼‰çš„æ— çº¦æŸååé‡æå‡ï¼Œåœ¨SLOçº¦æŸä¸‹æä¾›32å€åŠä»¥ä¸Šï¼ˆå¹³å‡4.9å€ï¼‰çš„ååé‡æå‡ï¼Œåœ¨èµ„æºå—é™ç¯å¢ƒä¸­è¡¨ç°å°¤ä¸ºçªå‡ºã€‚

Conclusion: RAPID-Serveæ˜¯ä¸€ç§æœ‰æ•ˆçš„LLMæ¨ç†æœåŠ¡ç­–ç•¥ï¼Œç›¸æ¯”æœ€å…ˆè¿›æ–¹æ³•ï¼Œèƒ½åœ¨æ»¡è¶³å»¶è¿ŸSLOçš„åŒæ—¶å®ç°é«˜ååé‡å’Œé«˜æ•ˆèµ„æºåˆ©ç”¨ï¼Œç‰¹åˆ«é€‚åˆèµ„æºå—é™ç¯å¢ƒã€‚

Abstract: Two widely adopted techniques for LLM inference serving systems today are hybrid batching and disaggregated serving. A hybrid batch combines prefill and decode tokens of different requests in the same batch to improve resource utilization and throughput at the cost of increased latency per token. In contrast, disaggregated serving decouples compute-bound prefill and bandwidth-bound decode phases to optimize for service level objectives (SLOs) at the cost of resource under-utilization and KV-cache transfer overheads. To address the limitations of these techniques, we propose RAPID-Serve: a technique to concurrently execute prefill and decode on the same GPU(s) to meet latency SLOs while maintaining high throughput and efficient resource utilization. Furthermore, we propose Adaptive Resource Management for runtime compute resource allocation, optionally leveraging CU masking (a fine-grained Compute Unit partitioning feature on AMD Instinct\textsuperscript{TM} GPUs). RAPID-Serve provides up to 4.1x (average 1.7x) unconstrained throughput improvement and 32x and higher (average 4.9x) throughput improvement under SLO constraints, showing it as an effective strategy compared to the state-of-the-art approaches, particularly in resource-constrained environments.

</details>


### [23] [Big Data Workload Profiling for Energy-Aware Cloud Resource Management](https://arxiv.org/abs/2601.11935)
*Milan Parikh,Aniket Abhishek Soni,Sneja Mitinbhai Shah,Ayush Raj Jha*

Main category: cs.DC

TL;DR: æå‡ºä¸€ç§åŸºäºå·¥ä½œè´Ÿè½½æ„ŸçŸ¥çš„èŠ‚èƒ½è°ƒåº¦æ¡†æ¶ï¼Œé€šè¿‡åˆ†æCPUã€å†…å­˜å’Œå­˜å‚¨IOè¡Œä¸ºæ¥ä¼˜åŒ–è™šæ‹Ÿæœºæ”¾ç½®å†³ç­–ï¼Œåœ¨ä¿æŒSLAçš„åŒæ—¶å®ç°15-20%çš„èƒ½è€—èŠ‚çœã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€å¤§æ•°æ®å·¥ä½œè´Ÿè½½è§„æ¨¡å’Œå¤æ‚æ€§çš„å¢é•¿ï¼Œäº‘æ•°æ®ä¸­å¿ƒé¢ä¸´é™ä½è¿è¥èƒ½è€—çš„å‹åŠ›ã€‚ä¼ ç»Ÿè°ƒåº¦å™¨ç¼ºä¹å¯¹å·¥ä½œè´Ÿè½½ç‰¹æ€§çš„æ·±å…¥ç†è§£ï¼Œå¯¼è‡´èƒ½æºæ•ˆç‡ä½ä¸‹ã€‚

Method: å¼€å‘äº†ä¸€ä¸ªå·¥ä½œè´Ÿè½½æ„ŸçŸ¥çš„èŠ‚èƒ½è°ƒåº¦æ¡†æ¶ï¼Œé€šè¿‡åˆ†æCPUåˆ©ç”¨ç‡ã€å†…å­˜éœ€æ±‚å’Œå­˜å‚¨IOè¡Œä¸ºæ¥æŒ‡å¯¼è™šæ‹Ÿæœºæ”¾ç½®å†³ç­–ã€‚ç»“åˆå†å²æ‰§è¡Œæ—¥å¿—å’Œå®æ—¶é¥æµ‹æ•°æ®ï¼Œé¢„æµ‹å€™é€‰æ”¾ç½®æ–¹æ¡ˆçš„èƒ½è€—å’Œæ€§èƒ½å½±å“ï¼Œå®ç°è‡ªé€‚åº”æ•´åˆåŒæ—¶ä¿æŒSLAåˆè§„æ€§ã€‚

Result: åœ¨åŒ…å«Hadoop MapReduceã€Spark MLlibå’ŒETLå·¥ä½œè´Ÿè½½çš„å¤šèŠ‚ç‚¹äº‘æµ‹è¯•å¹³å°ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œç›¸æ¯”åŸºçº¿è°ƒåº¦å™¨å®ç°äº†15-20%çš„æŒç»­èƒ½è€—èŠ‚çœï¼Œæ€§èƒ½ä¸‹é™å¯å¿½ç•¥ä¸è®¡ã€‚

Conclusion: å·¥ä½œè´Ÿè½½åˆ†ææ˜¯æé«˜åŸºäºäº‘çš„å¤§æ•°æ®å¤„ç†ç¯å¢ƒå¯æŒç»­æ€§çš„å®ç”¨ä¸”å¯æ‰©å±•ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹æ˜¾è‘—é™ä½èƒ½è€—ã€‚

Abstract: Cloud data centers face increasing pressure to reduce operational energy consumption as big data workloads continue to grow in scale and complexity. This paper presents a workload aware and energy efficient scheduling framework that profiles CPU utilization, memory demand, and storage IO behavior to guide virtual machine placement decisions. By combining historical execution logs with real time telemetry, the proposed system predicts the energy and performance impact of candidate placements and enables adaptive consolidation while preserving service level agreement compliance. The framework is evaluated using representative Hadoop MapReduce, Spark MLlib, and ETL workloads deployed on a multi node cloud testbed. Experimental results demonstrate consistent energy savings of 15 to 20 percent compared to a baseline scheduler, with negligible performance degradation. These findings highlight workload profiling as a practical and scalable strategy for improving the sustainability of cloud based big data processing environments.

</details>


### [24] [DaggerFFT: A Distributed FFT Framework Using Task Scheduling in Julia](https://arxiv.org/abs/2601.12209)
*Sana Taghipour Anvari,Julian Samaroo,Matin Raayai Ardakani,David Kaeli*

Main category: cs.DC

TL;DR: DaggerFFTï¼šåŸºäºJuliaçš„åŠ¨æ€ä»»åŠ¡è°ƒåº¦åˆ†å¸ƒå¼FFTæ¡†æ¶ï¼Œåœ¨CPUå’ŒGPUé›†ç¾¤ä¸Šåˆ†åˆ«å®ç°2.6å€å’Œ1.35å€åŠ é€Ÿ


<details>
  <summary>Details</summary>
Motivation: éšç€ç§‘å­¦æ¨¡æ‹Ÿå‘ç™¾äº¿äº¿æ¬¡è®¡ç®—ç³»ç»Ÿå‘å±•ï¼Œéœ€è¦èƒ½æœ‰æ•ˆåˆ©ç”¨ç°ä»£å¼‚æ„é«˜æ€§èƒ½è®¡ç®—ç³»ç»Ÿçš„åˆ†å¸ƒå¼FFTç®—æ³•ã€‚ä¼ ç»ŸFFTç®—æ³•åœ¨å¼‚æ„å¹³å°ä¸Šå¸¸é‡åˆ°æ€§èƒ½ç“¶é¢ˆï¼Œç°æœ‰åˆ†å¸ƒå¼FFTæ–¹æ³•ä¾èµ–é™æ€ä»»åŠ¡åˆ†é…å’ŒåŒæ­¥å±éšœï¼Œé™åˆ¶äº†å¯æ‰©å±•æ€§å’Œèµ„æºåˆ©ç”¨ç‡ã€‚

Method: æå‡ºDaggerFFTæ¡†æ¶ï¼Œå°†é«˜åº¦å¹¶è¡Œçš„FFTè®¡ç®—è§†ä¸ºåŠ¨æ€è°ƒåº¦çš„ä»»åŠ¡å›¾ã€‚æ¯ä¸ªFFTé˜¶æ®µåœ¨å•ç‹¬å®šä¹‰çš„åˆ†å¸ƒå¼æ•°ç»„ä¸Šæ“ä½œï¼ŒFFTæ“ä½œè¡¨ç¤ºä¸ºåœ¨é“…ç¬”æˆ–å¹³æ¿åˆ†åŒºDArrayä¸Šè¿è¡Œçš„DTasksã€‚æ¯ä¸ªFFTé˜¶æ®µæ‹¥æœ‰è‡ªå·±çš„DArrayï¼Œè¿è¡Œæ—¶ä½¿ç”¨Daggerçš„åŠ¨æ€è°ƒåº¦å™¨ï¼ˆæ”¯æŒå·¥ä½œçªƒå–ï¼‰åœ¨è®¾å¤‡é—´åˆ†é…ä»»åŠ¡ã€‚

Result: DaggerFFTçš„åŠ¨æ€è°ƒåº¦å™¨åœ¨CPUå’ŒGPUåç«¯ä¸Šå‡ä¼˜äºæœ€å…ˆè¿›çš„åˆ†å¸ƒå¼FFTåº“ï¼Œåœ¨CPUé›†ç¾¤ä¸Šå®ç°é«˜è¾¾2.6å€åŠ é€Ÿï¼Œåœ¨GPUé›†ç¾¤ä¸Šå®ç°é«˜è¾¾1.35å€åŠ é€Ÿã€‚å·²æˆåŠŸé›†æˆåˆ°åœ°çƒç‰©ç†æµä½“åŠ¨åŠ›å­¦æ¡†æ¶Oceananigans.jlä¸­ã€‚

Conclusion: åŸºäºä»»åŠ¡çš„è¿è¡Œæ—¶ç³»ç»Ÿï¼ˆå¦‚DaggerFFTï¼‰èƒ½å¤Ÿåœ¨å¤§è§„æ¨¡å®é™…æ¨¡æ‹Ÿä¸­åŒæ—¶æä¾›å“è¶Šæ€§èƒ½å’Œæ¨¡å—åŒ–ï¼Œè¯æ˜äº†é«˜çº§ä»»åŠ¡è°ƒåº¦æ–¹æ³•åœ¨å¼‚æ„é«˜æ€§èƒ½è®¡ç®—ç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§ã€‚

Abstract: The Fast Fourier Transform (FFT) is a fundamental numerical technique with widespread application in a range of scientific problems. As scientific simulations attempt to exploit exascale systems, there has been a growing demand for distributed FFT algorithms that can effectively utilize modern heterogeneous high-performance computing (HPC) systems. Conventional FFT algorithms commonly encounter performance bottlenecks, especially when run on heterogeneous platforms. Most distributed FFT approaches rely on static task distribution and require synchronization barriers, limiting scalability and impacting overall resource utilization. In this paper we present DaggerFFT, a distributed FFT framework, developed in Julia, that treats highly parallel FFT computations as a dynamically scheduled task graph. Each FFT stage operates on a separately defined distributed array. FFT operations are expressed as DTasks operating on pencil or slab partitioned DArrays. Each FFT stage owns its own DArray, and the runtime assigns DTasks across devices using Dagger's dynamic scheduler that uses work stealing. We demonstrate how DaggerFFT's dynamic scheduler can outperform state-of-the-art distributed FFT libraries on both CPU and GPU backends, achieving up to a 2.6x speedup on CPU clusters and up to a 1.35x speedup on GPU clusters. We have integrated DaggerFFT into Oceananigans.jl, a geophysical fluid dynamics framework, demonstrating that high-level, task-based runtimes can deliver both superior performance and modularity in large-scale, real-world simulations.

</details>


### [25] [Power Aware Dynamic Reallocation For Inference](https://arxiv.org/abs/2601.12241)
*Yiwei Jiang,Sangeeta Chowdhary,Nathaniel Morris,Rutwik Jain,Srilatha Manne,Sam Bayliss*

Main category: cs.DC

TL;DR: RAPIDæ˜¯ä¸€ä¸ªé¢å‘å¤§è¯­è¨€æ¨¡å‹æ¨ç†çš„åŠŸç‡æ„ŸçŸ¥è§£è€¦æ¡†æ¶ï¼Œé€šè¿‡è”åˆç®¡ç†GPUè§’è‰²å’ŒåŠŸç‡é¢„ç®—ï¼Œåœ¨ä¸¥æ ¼åŠŸç‡é™åˆ¶ä¸‹æå‡æ€§èƒ½è¡¨ç°ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€æ¨¡å‹å’Œé›†ç¾¤è§„æ¨¡æ‰©å¤§ï¼ŒåŠŸç‡è€Œéè®¡ç®—èƒ½åŠ›å·²æˆä¸ºé™åˆ¶æ•´ä½“æ€§èƒ½å’Œæˆæœ¬æ•ˆç‡çš„ä¸»è¦å› ç´ ã€‚ç°æœ‰è§£è€¦æ–¹æ¡ˆè™½èƒ½æå‡ç¡¬ä»¶åˆ©ç”¨ç‡ï¼Œä½†æœªå……åˆ†è€ƒè™‘åŠŸç‡çº¦æŸä¸‹çš„ä¼˜åŒ–ã€‚

Method: RAPIDé‡‡ç”¨é™æ€å’ŒåŠ¨æ€åŠŸç‡é‡åˆ†é…ä»¥åŠGPUé‡åˆ†é…ç­–ç•¥ï¼Œåœ¨å›ºå®šåŠŸç‡é™åˆ¶ä¸‹è”åˆä¼˜åŒ–GPUè§’è‰²åˆ†é…å’ŒåŠŸç‡é¢„ç®—ç®¡ç†ã€‚

Result: ç›¸æ¯”é™æ€åˆ†é…æ–¹æ¡ˆï¼ŒRAPIDåœ¨å³°å€¼è´Ÿè½½ä¸‹å°†SLOè¾¾æˆç‡æå‡é«˜è¾¾2å€ï¼Œä¸”ä¸å¢åŠ å¤æ‚åº¦æˆ–æˆæœ¬ã€‚

Conclusion: åŠŸç‡æ„ŸçŸ¥çš„è§£è€¦æ¨ç†æ¡†æ¶RAPIDèƒ½æ˜¾è‘—æå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†åœ¨åŠŸç‡çº¦æŸä¸‹çš„æ€§èƒ½å’Œä¸€è‡´æ€§ï¼Œä¸ºå¤§è§„æ¨¡éƒ¨ç½²æä¾›æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚

Abstract: Disaggregation has emerged as a powerful strategy for optimizing large language model (LLM) inference by separating compute-intensive prefill and memory-bound decode phases across specialized GPUs. This separation improves utilization and throughput under fixed hardware capacity. However, as model and cluster scales grow, power, rather than compute, has become the dominant limiter of overall performance and cost efficiency. In this paper, we propose RAPID, a power-aware disaggregated inference framework that jointly manages GPU roles and power budgets to sustain goodput within strict power caps. RAPID utilizes static and dynamic power reallocation in addition to GPU reallocation to improve performance under fixed power bounds. RAPID improves overall performance and application consistency beyond what is achievable in current disaggregation solutions, resulting in up to a 2x improvement in SLO attainment at peak load when compared to a static assignment without an increase in complexity or cost.

</details>


### [26] [CPU-less parallel execution of lambda calculus in digital logic](https://arxiv.org/abs/2601.13040)
*Harry Fitchett,Charles Fox*

Main category: cs.DC

TL;DR: å°†çº¯å‡½æ•°å¼è¯­è¨€ï¼ˆÎ»æ¼”ç®—ï¼‰ç›´æ¥ç¼–è¯‘ä¸ºæ•°å­—é€»è¾‘ç”µè·¯ï¼Œç»•è¿‡ä¼ ç»ŸCPUå’Œå†¯Â·è¯ºä¾æ›¼ç“¶é¢ˆï¼Œå®ç°å¹¶è¡Œè®¡ç®—çš„æ–°æ¶æ„ã€‚


<details>
  <summary>Details</summary>
Motivation: æ™¶ä½“ç®¡å¯†åº¦æŒç»­å¢åŠ ä½†æ—¶é’Ÿé€Ÿåº¦åœæ»ï¼Œéœ€è¦å¯»æ‰¾æ–°çš„å¹¶è¡Œæ¶æ„ã€‚ä¼ ç»ŸCPUå¹¶è¡Œç¼–è¯‘ä»å—å†¯Â·è¯ºä¾æ›¼ç“¶é¢ˆé™åˆ¶ï¼Œè€Œçº¯å‡½æ•°å¼è¯­è¨€å¤©ç”Ÿå…·æœ‰å¹¶è¡Œæ€§ï¼Œç›´æ¥ç¼–è¯‘åˆ°ç¡¬ä»¶å¯æœ€å¤§åŒ–åˆ©ç”¨å¹¶è¡Œæ€§ã€‚

Method: ä½¿ç”¨Î»æ¼”ç®—ä½œä¸ºæºè¯­è¨€ï¼Œé‡‡ç”¨æ ‘å½¢è¡¨ç¤ºæ³•ï¼ŒèŠ‚ç‚¹å¯¹åº”Î»è¯­æ³•å½¢å¼ï¼Œæ•°æ®å±€éƒ¨åŒ–å­˜å‚¨ï¼Œé€šè¿‡æ€»çº¿ä¼ é€’æ¶ˆæ¯ã€‚èŠ‚ç‚¹ç±»å‹å’Œè¡Œä¸ºå¯¹åº”Î»è¯­æ³•å½¢å¼ï¼ŒÎ²å½’çº¦å¹¶è¡Œæ‰§è¡Œï¼Œç‹¬ç«‹åˆ†æ”¯å¯åŒæ—¶è¿›è¡Œå˜æ¢ã€‚

Result: å®ç°äº†æ¦‚å¿µéªŒè¯ç³»ç»Ÿï¼Œé€šè¿‡ä»¿çœŸå±•ç¤ºäº†Î»è¡¨è¾¾å¼çš„æˆåŠŸæ‰§è¡Œã€‚æµ‹è¯•å¥—ä»¶çš„æˆåŠŸæ‰§è¡Œè¡¨æ˜è¯¥æ–¹æ³•å¯æ‰©å±•åˆ°æ›´å¤§çš„å‡½æ•°å¼è¯­è¨€ã€‚

Conclusion: è¯¥æ–¹æ³•ä¸ºæ— CPUçš„å‡½æ•°å¼è®¡ç®—æä¾›äº†æ–°æ¨¡å‹åŸºç¡€ï¼Œè¯æ˜äº†å°†å‡½æ•°å¼è¯­è¨€ç›´æ¥ç¼–è¯‘åˆ°æ•°å­—é€»è¾‘çš„å¯è¡Œæ€§ï¼Œæœ‰æœ›æ‰©å±•åˆ°æ›´å®Œæ•´çš„ç°ä»£å‡½æ•°å¼è¯­è¨€ã€‚

Abstract: While transistor density is still increasing, clock speeds are not, motivating the search for new parallel architectures. One approach is to completely abandon the concept of CPU -- and thus serial imperative programming -- and instead to specify and execute tasks in parallel, compiling from programming languages to data flow digital logic. It is well-known that pure functional languages are inherently parallel, due to the Church-Rosser theorem, and CPU-based parallel compilers exist for many functional languages. However, these still rely on conventional CPUs and their von Neumann bottlenecks. An alternative is to compile functional languages directly into digital logic to maximize available parallelism. It is difficult to work with complete modern functional languages due to their many features, so we demonstrate a proof-of-concept system using lambda calculus as the source language and compiling to digital logic. We show how functional hardware can be tailored to a simplistic functional language, forming the ground for a new model of CPU-less functional computation. At the algorithmic level, we use a tree-based representation, with data localized within nodes and communicated data passed between them. This is implemented by physical digital logic blocks corresponding to nodes, and buses enabling message passing. Node types and behaviors correspond to lambda grammar forms, and beta-reductions are performed in parallel allowing branches independent from one another to perform transformations simultaneously. As evidence for this approach, we present an implementation, along with simulation results, showcasing successful execution of lambda expressions. This suggests that the approach could be scaled to larger functional languages. Successful execution of a test suite of lambda expressions suggests that the approach could be scaled to larger functional languages.

</details>


### [27] [Opportunistic Scheduling for Optimal Spot Instance Savings in the Cloud](https://arxiv.org/abs/2601.12266)
*Neelkamal Bhuyan,Randeep Bhatia,Murali Kodialam,TV Lakshman*

Main category: cs.DC

TL;DR: ç ”ç©¶åœ¨æ»¡è¶³å¹³å‡å»¶è¿Ÿçº¦æŸä¸‹ï¼Œä½¿ç”¨ç«ä»·å‹å’ŒæŒ‰éœ€å‹äº‘å®ä¾‹è°ƒåº¦å»¶è¿Ÿæ•æ„Ÿä½œä¸šä»¥æœ€å°åŒ–å¹³å‡æˆæœ¬çš„é—®é¢˜ï¼Œæå‡ºäº†åŸºäºæ’é˜Ÿè®ºå’Œä¼˜åŒ–çš„åˆ†ææ–¹æ³•åŠè‡ªé€‚åº”ç®—æ³•ã€‚


<details>
  <summary>Details</summary>
Motivation: äº‘ç¯å¢ƒä¸­å»¶è¿Ÿæ•æ„Ÿä½œä¸šçš„è°ƒåº¦éœ€è¦åœ¨æˆæœ¬ï¼ˆç«ä»·å®ä¾‹ä¾¿å®œä½†ä¸ç¨³å®šï¼‰å’Œå»¶è¿Ÿï¼ˆæŒ‰éœ€å®ä¾‹å¯é ä½†æ˜‚è´µï¼‰ä¹‹é—´æƒè¡¡ï¼Œç°æœ‰ç ”ç©¶ç¼ºä¹å¯¹æ­¤é—®é¢˜çš„ç³»ç»Ÿæ€§ç†è®ºåˆ†æã€‚

Method: ä½¿ç”¨æ’é˜Ÿè®ºã€éšæœºè¿‡ç¨‹å’Œä¼˜åŒ–å·¥å…·è¿›è¡Œç†è®ºåˆ†æï¼Œæ¨å¯¼ä¸€èˆ¬ç­–ç•¥çš„æˆæœ¬è¡¨è¾¾å¼ï¼Œè¯æ˜ä½ç›®æ ‡å»¶è¿Ÿæ—¶é˜Ÿåˆ—é•¿åº¦ä¸º1æœ€ä¼˜ï¼Œé«˜ç›®æ ‡å»¶è¿Ÿæ—¶åˆ©ç”¨èƒŒåŒ…ç»“æ„è®¾è®¡è°ƒåº¦ç­–ç•¥ï¼Œå¹¶æå‡ºè‡ªé€‚åº”ç®—æ³•å……åˆ†åˆ©ç”¨å…è®¸çš„å»¶è¿Ÿã€‚

Result: ç†è®ºåˆ†ææ­ç¤ºäº†æœ€ä¼˜ç­‰å¾…æ—¶é—´åˆ†å¸ƒç‰¹å¾ï¼Œè®¾è®¡çš„è°ƒåº¦ç­–ç•¥åœ¨é«˜å»¶è¿Ÿåœºæ™¯ä¸‹æœ‰æ•ˆï¼Œæå‡ºçš„è‡ªé€‚åº”ç®—æ³•åœ¨å®è¯ä¸­è¡¨ç°å‡ºæ¥è¿‘æœ€ä¼˜çš„æ€§èƒ½ã€‚

Conclusion: è¯¥ç ”ç©¶ä¸ºäº‘ç¯å¢ƒä¸­å»¶è¿Ÿæ•æ„Ÿä½œä¸šè°ƒåº¦æä¾›äº†é¦–ä¸ªç³»ç»Ÿæ€§ç†è®ºåˆ†ææ¡†æ¶ï¼Œæå‡ºçš„è‡ªé€‚åº”ç®—æ³•èƒ½æœ‰æ•ˆå¹³è¡¡æˆæœ¬å’Œå»¶è¿Ÿçº¦æŸï¼Œå…·æœ‰å®é™…åº”ç”¨ä»·å€¼ã€‚

Abstract: We study the problem of scheduling delay-sensitive jobs over spot and on-demand cloud instances to minimize average cost while meeting an average delay constraint. Jobs arrive as a general stochastic process, and incur different costs based on the instance type. This work provides the first analytical treatment of this problem using tools from queuing theory, stochastic processes, and optimization. We derive cost expressions for general policies, prove queue length one is optimal for low target delays, and characterize the optimal wait-time distribution. For high target delays, we identify a knapsack structure and design a scheduling policy that exploits it. An adaptive algorithm is proposed to fully utilize the allowed delay, and empirical results confirm its near-optimality.

</details>


### [28] [Multi-Partner Project: Multi-GPU Performance Portability Analysis for CFD Simulations at Scale](https://arxiv.org/abs/2601.14159)
*Panagiotis-Eleftherios Eleftherakis,George Anagnostopoulos,Anastassis Kapetanakis,Mohammad Umair,Jean-Yves Vet,Konstantinos Iliakis,Jonathan Vincent,Jing Gong,Akshay Patil,Clara GarcÃ­a-SÃ¡nchez,Gerardo Zampino,Ricardo Vinuesa,Sotirios Xydis*

Main category: cs.DC

TL;DR: è¯¥è®ºæ–‡åˆ†æäº†SOD2D CFDæ¡†æ¶åœ¨AMDå’ŒNVIDIA GPUæ¶æ„ä¸Šçš„æ€§èƒ½å¯ç§»æ¤æ€§ï¼Œå‘ç°å†…å­˜è®¿é—®ä¼˜åŒ–å¸¦æ¥0.69-3.91å€åŠ é€Ÿå·®å¼‚ï¼Œå¤šGPUæ‰©å±•å­˜åœ¨æ€§èƒ½æ³¢åŠ¨ï¼Œå¼ºè°ƒéœ€è¦å¤šå±‚æ¬¡è°ƒä¼˜ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€GPUå¼‚æ„è¶…ç®—æ¶æ„æˆä¸ºHPCæ ¸å¿ƒï¼ŒCFDæ¨¡æ‹Ÿéœ€è¦é«˜æ•ˆåˆ©ç”¨æ­¤ç±»ç¡¬ä»¶ã€‚æ€§èƒ½å¯ç§»æ¤æ€§ï¼ˆåœ¨ä¸åŒåŠ é€Ÿå™¨ä¸Šä¿æŒæ¥è¿‘æœ€ä¼˜æ€§èƒ½ï¼‰æ˜¯HPCä»£ç çš„å…³é”®æŒ‘æˆ˜ã€‚REFMAPé¡¹ç›®é’ˆå¯¹å¯æ‰©å±•çš„GPUå¤šä¿çœŸåº¦CFDåŸå¸‚æ°”æµé¢„æµ‹ï¼Œéœ€è¦åˆ†æSOD2Dæ¡†æ¶åœ¨AMDå’ŒNVIDIA GPUä¸Šçš„æ€§èƒ½å¯ç§»æ¤æ€§ã€‚

Method: é¦–å…ˆè®¨è®ºSOD2Dçš„ç‰©ç†å’Œæ•°å€¼æ¨¡å‹ï¼Œè¯†åˆ«è®¡ç®—çƒ­ç‚¹ã€‚ç„¶åä»¥å¤šå±‚æ¬¡æ–¹å¼åˆ†ææ€§èƒ½å’Œå¯æ‰©å±•æ€§ï¼šå®šä¹‰è·¨è¶Šåº”ç”¨ã€è½¯ä»¶å’Œç¡¬ä»¶åŸºç¡€è®¾æ–½å‚æ•°çš„å®Œæ•´è®¾è®¡ç©ºé—´ã€‚åœ¨æœåŠ¡å™¨çº§NVIDIAå’ŒAMD GPUæ¶æ„åŠä¾›åº”å•†ç‰¹å®šç¼–è¯‘å™¨æ ˆä¸Šè¿›è¡Œå•GPUæ€§èƒ½è¡¨å¾ï¼Œè¯„ä¼°å†…å­˜è®¿é—®ä¼˜åŒ–æ•ˆæœã€‚åœ¨LUMIå¤šGPUé›†ç¾¤ä¸Šè¿›è¡Œæ‰©å±•æ€§èƒ½åˆ†æï¼Œé€šè¿‡æ€§èƒ½å‰–æè¯†åˆ«ååé‡å˜åŒ–ã€‚

Result: å•GPUæ€§èƒ½è¡¨å¾æ˜¾ç¤ºå†…å­˜è®¿é—®ä¼˜åŒ–å¸¦æ¥æ˜¾è‘—å·®å¼‚ï¼š0.69-3.91å€çš„åŠ é€Ÿå˜åŒ–èŒƒå›´ã€‚å¤šGPUæ‰©å±•åˆ†ææ­ç¤ºç±»ä¼¼çš„ååé‡å˜åŒ–ï¼Œè¡¨æ˜æ€§èƒ½é¢„æµ‹å­˜åœ¨å±€é™æ€§ã€‚ä¸åŒGPUæ¶æ„å’Œç¼–è¯‘å™¨æ ˆè¡¨ç°å‡ºä¸åŒçš„ä¼˜åŒ–æ•ˆæœã€‚

Conclusion: SOD2Dåœ¨AMDå’ŒNVIDIA GPUä¸Šè¡¨ç°å‡ºæ€§èƒ½å¯ç§»æ¤æ€§æŒ‘æˆ˜ï¼Œå†…å­˜è®¿é—®ä¼˜åŒ–å¯¹æ€§èƒ½å½±å“æ˜¾è‘—ä¸”å¤šæ ·åŒ–ã€‚å¤šGPUæ‰©å±•å­˜åœ¨æ€§èƒ½æ³¢åŠ¨ï¼Œå¼ºè°ƒéœ€è¦åŸºäºå¤šå±‚æ¬¡ä¿¡æ¯çš„è°ƒä¼˜ï¼Œè€Œä¸èƒ½ä»…ä¾èµ–æ€§èƒ½é¢„æµ‹ã€‚è¿™å¯¹å¼‚æ„HPCæ¶æ„ä¸Šçš„CFDåº”ç”¨å¼€å‘å…·æœ‰é‡è¦å¯ç¤ºã€‚

Abstract: As heterogeneous supercomputing architectures leveraging GPUs become increasingly central to high-performance computing (HPC), it is crucial for computational fluid dynamics (CFD) simulations, a de-facto HPC workload, to efficiently utilize such hardware. One of the key challenges of HPC codes is performance portability, i.e. the ability to maintain near-optimal performance across different accelerators. In the context of the \textbf{REFMAP} project, which targets scalable, GPU-enabled multi-fidelity CFD for urban airflow prediction, this paper analyzes the performance portability of SOD2D, a state-of-the-art Spectral Elements simulation framework across AMD and NVIDIA GPU architectures. We first discuss the physical and numerical models underlying SOD2D, highlighting its computational hotspots. Then, we examine its performance and scalability in a multi-level manner, i.e. defining and characterizing an extensive full-stack design space spanning across application, software and hardware infrastructure related parameters. Single-GPU performance characterization across server-grade NVIDIA and AMD GPU architectures and vendor-specific compiler stacks, show the potential as well as the diverse effect of memory access optimizations, i.e. 0.69$\times$ - 3.91$\times$ deviations in acceleration speedup. Performance variability of SOD2D at scale is further examined on the LUMI multi-GPU cluster, where profiling reveals similar throughput variations, highlighting the limits of performance projections and the need for multi-level, informed tuning.

</details>


### [29] [RIPPLE++: An Incremental Framework for Efficient GNN Inference on Evolving Graphs](https://arxiv.org/abs/2601.12347)
*Pranjal Naman,Parv Agarwal,Hrishikesh Haritas,Yogesh Simmhan*

Main category: cs.DC

TL;DR: RIPPLE++ï¼šç”¨äºåŠ¨æ€å›¾ç¥ç»ç½‘ç»œæ¨ç†çš„æµå¼æ¡†æ¶ï¼Œé€šè¿‡å¢é‡æ›´æ–°é¿å…å†—ä½™è®¡ç®—ï¼Œåœ¨å•æœºå’Œåˆ†å¸ƒå¼ç¯å¢ƒä¸­æ˜¾è‘—æå‡æ€§èƒ½


<details>
  <summary>Details</summary>
Motivation: ç°å®ä¸–ç•Œä¸­çš„å›¾æ˜¯åŠ¨æ€å˜åŒ–çš„ï¼Œé¢‘ç¹çš„ç»“æ„å’Œç‰¹å¾æ›´æ–°ç»™å›¾ç¥ç»ç½‘ç»œæ¨ç†å¸¦æ¥æŒ‘æˆ˜ã€‚ç°æœ‰æ–¹æ³•åœ¨åŠ¨æ€å›¾åœºæ™¯ä¸‹å­˜åœ¨å†—ä½™è®¡ç®—ã€å¤§é‚»åŸŸéå†å’Œé«˜é€šä¿¡æˆæœ¬ç­‰é—®é¢˜ï¼Œä¸”é‡‡æ ·æ–¹æ³•å› éç¡®å®šæ€§ä¸é€‚åˆå…³é”®åº”ç”¨ï¼Œéš¾ä»¥æ»¡è¶³å®æ—¶ä½å»¶è¿Ÿéœ€æ±‚ã€‚

Method: æå‡ºRIPPLE++æ¡†æ¶ï¼Œå¼•å…¥é€šç”¨å¢é‡ç¼–ç¨‹æ¨¡å‹ï¼Œæ•è·GNNèšåˆå‡½æ•°è¯­ä¹‰ï¼Œå¢é‡ä¼ æ’­æ›´æ–°åˆ°å—å½±å“é‚»åŸŸã€‚æ”¯æŒæ‰€æœ‰å¸¸è§å›¾æ›´æ–°ï¼ˆé¡¶ç‚¹/è¾¹å¢åˆ ã€ç‰¹å¾æ›´æ–°ï¼‰ï¼Œé€‚ç”¨äºå•æœºå’Œåˆ†å¸ƒå¼éƒ¨ç½²ã€‚

Result: å•æœºç¯å¢ƒä¸‹ï¼šç¨€ç–å›¾Arxivï¼ˆ169Ké¡¶ç‚¹ï¼Œ1.2Mè¾¹ï¼‰è¾¾åˆ°56Kæ›´æ–°/ç§’ï¼Œç¨ å¯†å›¾Productsï¼ˆ2.5Mé¡¶ç‚¹ï¼Œ123.7Mè¾¹ï¼‰è¾¾åˆ°7.6Kæ›´æ–°/ç§’ï¼Œå»¶è¿Ÿ0.06-960msï¼Œæ¯”ç°æœ‰æ–¹æ³•æå‡2.2-24å€ååé‡ã€‚åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼šæ¯”é‡æ–°è®¡ç®—åŸºçº¿æå‡çº¦25å€ååé‡ï¼Œé™ä½20å€é€šä¿¡æˆæœ¬ã€‚

Conclusion: RIPPLE++ä¸ºåŠ¨æ€å›¾GNNæ¨ç†æä¾›äº†é«˜æ•ˆå‡†ç¡®çš„æµå¼è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡å¢é‡æ›´æ–°æœºåˆ¶æ˜¾è‘—æå‡æ€§èƒ½å¹¶é™ä½é€šä¿¡æˆæœ¬ï¼Œé€‚ç”¨äºå®æ—¶åº”ç”¨åœºæ™¯ã€‚

Abstract: Real-world graphs are dynamic, with frequent updates to their structure and features due to evolving vertex and edge properties. These continual changes pose significant challenges for efficient inference in graph neural networks (GNNs). Existing vertex-wise and layer-wise inference approaches are ill-suited for dynamic graphs, as they incur redundant computations, large neighborhood traversals, and high communication costs, especially in distributed settings. Additionally, while sampling-based approaches can be adopted to approximate final layer embeddings, these are often not preferred in critical applications due to their non-determinism. These limitations hinder low-latency inference required in real-time applications. To address this, we propose RIPPLE++, a framework for streaming GNN inference that efficiently and accurately updates embeddings in response to changes in the graph structure or features. RIPPLE++ introduces a generalized incremental programming model that captures the semantics of GNN aggregation functions and incrementally propagates updates to affected neighborhoods. RIPPLE++ accommodates all common graph updates, including vertex/edge addition/deletions and vertex feature updates. RIPPLE++ supports both single-machine and distributed deployments. On a single machine, it achieves up to $56$K updates/sec on sparse graphs like Arxiv ($169$K vertices, $1.2$M edges), and about $7.6$K updates/sec on denser graphs like Products ($2.5$M vertices, $123.7$M edges), with latencies of $0.06$--$960$ms, and outperforming state-of-the-art baselines by $2.2$--$24\times$ on throughput. In distributed settings, RIPPLE++ offers up to $\approx25\times$ higher throughput and $20\times$ lower communication costs compared to recomputing baselines.

</details>


### [30] [ASAS-BridgeAMM: Trust-Minimized Cross-Chain Bridge AMM with Failure Containment](https://arxiv.org/abs/2601.12434)
*Shengwei You,Aditya Joshi,Andrey Kuehlkamp,Jarek Nabrzyski*

Main category: cs.DC

TL;DR: ASAS-BridgeAMM æå‡ºäº†ä¸€ç§æ¡¥æ¥è€¦åˆçš„è‡ªåŠ¨åšå¸‚å•†ï¼Œé€šè¿‡"å—æ§é™çº§"æœºåˆ¶åœ¨æ£€æµ‹åˆ°æ”»å‡»ä¿¡å·æ—¶ä¼˜é›…é™ä½åŠŸèƒ½ï¼Œè€Œéå®Œå…¨å´©æºƒï¼Œæ˜¾è‘—é™ä½äº†è·¨é“¾æ¡¥çš„ç³»ç»Ÿæ€§é£é™©ã€‚


<details>
  <summary>Details</summary>
Motivation: è·¨é“¾æ¡¥æ˜¯DeFiä¸­ç³»ç»Ÿæ€§é£é™©çš„æœ€å¤§æ¥æºï¼Œè‡ª2021å¹´ä»¥æ¥å·²é€ æˆè¶…è¿‡28äº¿ç¾å…ƒæŸå¤±ã€‚ç°æœ‰æ¡¥æ¥å®‰å…¨æ¨¡å‹çš„æ ¹æœ¬ç¼ºé™·åœ¨äºå…¶äºŒå…ƒæ€§ï¼šè¦ä¹ˆå®Œå…¨è¿è¡Œï¼Œè¦ä¹ˆç¾éš¾æ€§å´©æºƒï¼Œç¼ºä¹å¤„ç†éƒ¨åˆ†æ•…éšœçš„ä¸­é—´çŠ¶æ€ã€‚

Method: æå‡ºASAS-BridgeAMMæ¡¥æ¥è€¦åˆè‡ªåŠ¨åšå¸‚å•†ï¼Œå¼•å…¥"å—æ§é™çº§"æœºåˆ¶ï¼Œå°†è·¨é“¾æ¶ˆæ¯å»¶è¿Ÿé‡åŒ–ä¸ºæ‰§è¡Œé£é™©ï¼ŒåŠ¨æ€è°ƒæ•´æŠµæŠ¼å“æŠ˜æ‰£ã€æ»‘ç‚¹è¾¹ç•Œå’Œææ¬¾é™åˆ¶ã€‚åœ¨æ‹œå åº­ä¸­ç»§æ¨¡å‹ä¸‹å½¢å¼åŒ–è¯æ˜å®‰å…¨æ€§ã€æ´»æ€§åŠæŠ—æ“çºµæ€§ã€‚

Result: åœ¨ä»¥å¤ªåŠå’Œä¸¤æ¡è¾…åŠ©é“¾ä¸Š18ä¸ªæœˆå†å²å›æ”¾æµ‹è¯•ä¸­ï¼Œç›¸æ¯”åŸºå‡†é“¸å¸-é”€æ¯æ¶æ„ï¼Œæœ€åæƒ…å†µä¸‹æ¡¥æ¥å¯¼è‡´çš„ç ´äº§å‡å°‘73%ï¼Œå‹åŠ›æœŸé—´äº¤æ˜“é‡ä¿æŒ104.5%ã€‚åœ¨å»¶è¿Ÿæœ€ç»ˆæ€§ã€é¢„è¨€æœºæ“çºµå’ŒæµåŠ¨æ€§æ”»å‡»çš„å¯¹æŠ—æ¨¡æ‹Ÿä¸­ï¼Œä¿æŒå¿ä»˜èƒ½åŠ›çš„æ¦‚ç‡>0.9999ï¼Œæ¯æ—¶æœŸåè´¦é™åˆ¶åœ¨æ€»æŠµæŠ¼å“çš„<0.2%ã€‚

Conclusion: ASAS-BridgeAMMé€šè¿‡å—æ§é™çº§æœºåˆ¶è§£å†³äº†è·¨é“¾æ¡¥çš„äºŒå…ƒå®‰å…¨æ¨¡å‹é—®é¢˜ï¼Œæ˜¾è‘—é™ä½äº†ç³»ç»Ÿæ€§é£é™©ï¼ŒåŒæ—¶ä¿æŒäº†é«˜å¯ç”¨æ€§ï¼Œä¸ºè·¨é“¾æ¡¥å®‰å…¨æä¾›äº†æ–°çš„èŒƒå¼ã€‚

Abstract: Cross-chain bridges constitute the single largest vector of systemic risk in Decentralized Finance (DeFi), accounting for over \$2.8 billion in losses since 2021. The fundamental vulnerability lies in the binary nature of existing bridge security models: a bridge is either fully operational or catastrophically compromised, with no intermediate state to contain partial failures. We present ASAS-BridgeAMM, a bridge-coupled automated market maker that introduces Contained Degradation: a formally specified operational state where the system gracefully degrades functionality in response to adversarial signals. By treating cross-chain message latency as a quantifiable execution risk, the protocol dynamically adjusts collateral haircuts, slippage bounds, and withdrawal limits. Across 18 months of historical replay on Ethereum and two auxiliary chains, ASAS-BridgeAMM reduces worst-case bridge-induced insolvency by 73% relative to baseline mint-and-burn architectures, while preserving 104.5% of transaction volume during stress periods. In rigorous adversarial simulations involving delayed finality, oracle manipulation, and liquidity griefing, the protocol maintains solvency with probability $>0.9999$ and bounds per-epoch bad debt to $<0.2%$ of total collateral. We provide a reference implementation in Solidity and formally prove safety (bounded debt), liveness (settlement completion), and manipulation resistance under a Byzantine relayer model.

</details>


### [31] [SGCP: A Self-Organized Game-Theoretic Framework For Collaborative Perception](https://arxiv.org/abs/2601.12524)
*Zechuan Gong,Hui Zhang,Yuquan Yang,Wenyu Lu*

Main category: cs.DC

TL;DR: æå‡ºå®Œå…¨å»ä¸­å¿ƒåŒ–çš„åä½œæ„ŸçŸ¥æ¡†æ¶ï¼Œé€šè¿‡ä¸¤é˜¶æ®µåšå¼ˆè®ºæ–¹æ³•è®©è½¦è¾†è‡ªç»„ç»‡æˆåä½œé›†ç¾¤ï¼Œåœ¨æœ‰é™é€šä¿¡å¸¦å®½ä¸‹å®ç°é«˜æ•ˆæ„ŸçŸ¥å…±äº«ã€‚


<details>
  <summary>Details</summary>
Motivation: è‡ªåŠ¨é©¾é©¶ä¸­åä½œæ„ŸçŸ¥èƒ½æå‡å®‰å…¨æ€§ï¼Œä½†åœ¨å¯†é›†äº¤é€šä¸­éƒ¨ç½²é¢ä¸´é€šä¿¡å¸¦å®½é™åˆ¶å’Œç¼ºä¹è·¯è¾¹åŸºç¡€è®¾æ–½çš„æŒ‘æˆ˜ï¼Œéœ€è¦å»ä¸­å¿ƒåŒ–è§£å†³æ–¹æ¡ˆã€‚

Method: ä¸¤é˜¶æ®µåšå¼ˆè®ºæ¡†æ¶ï¼šç¬¬ä¸€é˜¶æ®µè½¦è¾†åŸºäºæ„ŸçŸ¥äº’è¡¥æ€§å’Œè¿åŠ¨ä¸€è‡´æ€§å½¢æˆç¨³å®šé›†ç¾¤å¹¶é€‰ä¸¾åè°ƒè€…ï¼›ç¬¬äºŒé˜¶æ®µåè°ƒè€…å¼•å¯¼æˆå‘˜é€šè¿‡éåˆä½œåŠ¿åšå¼ˆé€‰æ‹©æ€§ä¼ è¾“ç‚¹äº‘ç‰‡æ®µï¼Œå®ç°å±€éƒ¨èåˆï¼Œé›†ç¾¤é—´äº¤æ¢ç´§å‡‘æ£€æµ‹æ¶ˆæ¯è€ŒéåŸå§‹æ•°æ®ã€‚

Result: åœ¨CARLA-OpenCDA-NS3ååŒä»¿çœŸå¹³å°ä¸Šï¼Œç›¸æ¯”ç°æœ‰åŸºçº¿æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å‡å°‘é€šä¿¡å¼€é”€çš„åŒæ—¶ï¼Œå®ç°äº†æ›´é«˜çš„æ„ŸçŸ¥ç²¾åº¦å’Œæ›´å¹¿çš„æœ‰æ•ˆè¦†ç›–èŒƒå›´ã€‚

Conclusion: æå‡ºçš„å®Œå…¨å»ä¸­å¿ƒåŒ–æ¡†æ¶é€šè¿‡è½¦è¾†è‡ªç»„ç»‡å’Œé€‰æ‹©æ€§æ•°æ®ä¼ è¾“ï¼Œæœ‰æ•ˆè§£å†³äº†æœ‰é™é€šä¿¡å¸¦å®½ä¸‹çš„åä½œæ„ŸçŸ¥é—®é¢˜ï¼Œä¸ºå¤§è§„æ¨¡è‡ªåŠ¨é©¾é©¶åä½œæ„ŸçŸ¥æä¾›äº†å¯è¡Œæ–¹æ¡ˆã€‚

Abstract: Collaborative perception holds great promise for improving safety in autonomous driving, particularly in dense traffic where vehicles can share sensory information to overcome individual blind spots and extend awareness. However, deploying such collaboration at scale remains difficult when communication bandwidth is limited and no roadside infrastructure is available. To overcome these limitations, we introduce a fully decentralized framework that enables vehicles to self organize into cooperative groups using only vehicle to vehicle communication. The approach decomposes the problem into two sequential game theoretic stages. In the first stage, vehicles form stable clusters by evaluating mutual sensing complementarity and motion coherence, and each cluster elects a coordinator. In the second stage, the coordinator guides its members to selectively transmit point cloud segments from perceptually salient regions through a non cooperative potential game, enabling efficient local fusion. Global scene understanding is then achieved by exchanging compact detection messages across clusters rather than raw sensor data. We design distributed algorithms for both stages that guarantee monotonic improvement of the system wide potential function. Comprehensive experiments on the CARLA-OpenCDA-NS3 co-simulation platform show that our method reduces communication overhead while delivering higher perception accuracy and wider effective coverage compared to existing baselines.

</details>


### [32] [Dynamic Detection of Inefficient Data Mapping Patterns in Heterogeneous OpenMP Applications](https://arxiv.org/abs/2601.12713)
*Luke Marzen,Junhyung Shim,Ali Jannesari*

Main category: cs.DC

TL;DR: OMPDataPerfï¼šåŸºäºåŠ¨æ€åˆ†ææ£€æµ‹å¼‚æ„OpenMPåº”ç”¨ä¸­æ•°æ®ç§»åŠ¨ä½æ•ˆé—®é¢˜çš„å·¥å…·ï¼Œä»…å¼•å…¥5%è¿è¡Œæ—¶å¼€é”€


<details>
  <summary>Details</summary>
Motivation: éšç€CPUä¸åŠ é€Ÿå™¨å¼‚æ„è®¡ç®—çš„æ™®åŠï¼Œè®¾å¤‡é—´æ•°æ®ç§»åŠ¨æˆä¸ºæ€§èƒ½ç“¶é¢ˆï¼Œç°æœ‰æ€§èƒ½å·¥å…·éœ€è¦å¤§é‡äººå·¥å¹²é¢„æ¥è¯Šæ–­æ•°æ®ä¼ è¾“ä½æ•ˆé—®é¢˜

Method: æå‡ºåŠ¨æ€åˆ†ææŠ€æœ¯æ£€æµ‹å¼‚æ„åº”ç”¨ä¸­çš„æ•°æ®ä¼ è¾“å’Œåˆ†é…æ¨¡å¼ä½æ•ˆé—®é¢˜ï¼Œå®ç°ä¸ºOMPDataPerfå·¥å…·ï¼Œåˆ©ç”¨OpenMP Tools Interface (OMPT)æä¾›è¯¦ç»†çš„é—®é¢˜æ•°æ®æ˜ å°„è¿½è¸ªã€æºä»£ç å½’å› å’Œä¼˜åŒ–æ½œåŠ›è¯„ä¼°

Result: OMPDataPerfèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«å¼‚æ„OpenMPåº”ç”¨ä¸­çš„æ•°æ®ç§»åŠ¨ä½æ•ˆæ¨¡å¼ï¼Œä»…å¼•å…¥5%çš„å‡ ä½•å¹³å‡è¿è¡Œæ—¶å¼€é”€ï¼Œä¸ºç¨‹åºå‘˜æä¾›è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯

Conclusion: OMPDataPerfé€šè¿‡åŠ¨æ€åˆ†ææŠ€æœ¯æœ‰æ•ˆè§£å†³äº†å¼‚æ„è®¡ç®—ä¸­æ•°æ®ç§»åŠ¨ç“¶é¢ˆçš„è¯Šæ–­é—®é¢˜ï¼Œæ˜¾è‘—é™ä½äº†ç¨‹åºå‘˜å¹²é¢„éœ€æ±‚ï¼Œä¸ºä¼˜åŒ–å¼‚æ„åº”ç”¨æ€§èƒ½æä¾›äº†å®ç”¨å·¥å…·

Abstract: With the growing prevalence of heterogeneous computing, CPUs are increasingly being paired with accelerators to achieve new levels of performance and energy efficiency. However, data movement between devices remains a significant bottleneck, complicating application development. Existing performance tools require considerable programmer intervention to diagnose and locate data transfer inefficiencies. To address this, we propose dynamic analysis techniques to detect and profile inefficient data transfer and allocation patterns in heterogeneous applications. We implemented these techniques into OMPDataPerf, which provides detailed traces of problematic data mappings, source code attribution, and assessments of optimization potential in heterogeneous OpenMP applications. OMPDataPerf uses the OpenMP Tools Interface (OMPT) and incurs only a 5 % geometric-mean runtime overhead.

</details>


### [33] [Efficient Local-to-Global Collaborative Perception via Joint Communication and Computation Optimization](https://arxiv.org/abs/2601.12749)
*Hui Zhang,Yuquan Yang,Zechuan Gong,Xiaohua Xu,Dan Keun Sung*

Main category: cs.DC

TL;DR: æå‡ºLGCPæ¡†æ¶ï¼Œé€šè¿‡åŒºåŸŸåˆ’åˆ†å’Œåˆ†ç»„åä½œï¼Œåœ¨ä¿æŒæ„ŸçŸ¥æ€§èƒ½çš„åŒæ—¶å¤§å¹…é™ä½é€šä¿¡å¼€é”€


<details>
  <summary>Details</summary>
Motivation: ååŒæ„ŸçŸ¥å­˜åœ¨é€šä¿¡å¼€é”€é«˜å’Œè®¡ç®—å»¶è¿Ÿå¤§çš„é—®é¢˜ï¼Œéœ€è¦æ›´é«˜æ•ˆçš„åä½œæ¡†æ¶

Method: å°†é“è·¯åˆ’åˆ†ä¸ºéé‡å åŒºåŸŸï¼Œæ¯ä¸ªåŒºåŸŸåˆ†é…ä¸“ç”¨CAVç»„è¿›è¡Œæœ¬åœ°æ„ŸçŸ¥ï¼Œé€šè¿‡RSUé›†ä¸­è°ƒåº¦å’Œèšåˆå…¨å±€è§†å›¾

Result: æ•°æ®ä¼ è¾“é‡å¹³å‡å‡å°‘44å€ï¼ŒåŒæ—¶ä¿æŒç”šè‡³æå‡ååŒæ„ŸçŸ¥æ€§èƒ½

Conclusion: LGCPæ¡†æ¶åœ¨é€šä¿¡å’Œè®¡ç®—æ•ˆç‡æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»ŸååŒæ„ŸçŸ¥æ–¹æ³•

Abstract: Autonomous driving relies on accurate perception to ensure safe driving. Collaborative perception improves accuracy by mitigating the sensing limitations of individual vehicles, such as limited perception range and occlusion-induced blind spots. However, collaborative perception often suffers from high communication overhead due to redundant data transmission, as well as increasing computation latency caused by excessive load with growing connected and autonomous vehicles (CAVs) participation. To address these challenges, we propose a novel local-to-global collaborative perception framework (LGCP) to achieve collaboration in a communication- and computation-efficient manner. The road of interest is partitioned into non-overlapping areas, each of which is assigned a dedicated CAV group to perform localized perception. A designated leader in each group collects and fuses perception data from its members, and uploads the perception result to the roadside unit (RSU), establishing a link between local perception and global awareness. The RSU aggregates perception results from all groups and broadcasts a global view to all CAVs. LGCP employs a centralized scheduling strategy via the RSU, which assigns CAV groups to each area, schedules their transmissions, aggregates area-level local perception results, and propagates the global view to all CAVs. Experimental results demonstrate that the proposed LGCP framework achieves an average 44 times reduction in the amount of data transmission, while maintaining or even improving the overall collaborative performance.

</details>


### [34] [Unleashing Efficient Asynchronous RL Post-Training via Staleness-Constrained Rollout Coordination](https://arxiv.org/abs/2601.12784)
*Haoyang Li,Sheng Lin,Fangcheng Fu,Yuming Zhou,Xiaodong Ji,Yanfeng Zhao,Lefeng Wang,Jie Jiang,Bin Cui*

Main category: cs.DC

TL;DR: StaleFlowï¼šä¸€ä¸ªè§£å†³RLåè®­ç»ƒä¸­æ•°æ®é™ˆæ—§æ€§å’Œåæ–œæ€§é—®é¢˜çš„ç³»ç»Ÿï¼Œé€šè¿‡å…¨å±€ä¸€è‡´æ€§åè®®å’Œé‡æ„æ¶æ„ï¼Œåœ¨ä¿è¯æ”¶æ•›çš„åŒæ—¶æå‡ååé‡ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°ä»£å¤§è§„æ¨¡æ¨¡å‹çš„RLåè®­ç»ƒä¸­ï¼Œå®Œå…¨è§£è€¦æ¶æ„å¯¼è‡´ä¸¤ä¸ªå…³é”®æ•°æ®é—®é¢˜ï¼šå¼‚æ­¥æ‰§è¡Œå¼•èµ·çš„æ•°æ®é™ˆæ—§æ€§ï¼ˆè½¨è¿¹è¿‡æ—¶ï¼‰å’Œè½¨è¿¹é•¿åº¦å˜åŒ–å¯¼è‡´çš„æ•°æ®åæ–œæ€§ï¼Œç°æœ‰ç³»ç»Ÿæ— æ³•ç»Ÿä¸€è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œéœ€è¦åœ¨æ”¶æ•›å’Œæ€§èƒ½ä¹‹é—´åšå‡ºæƒè¡¡ã€‚

Method: 1. å¼•å…¥å…¨å±€ä¸€è‡´æ€§åè®®è·Ÿè¸ªæ¯ä¸ªè½¨è¿¹çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸä»¥æ§åˆ¶é™ˆæ—§æ€§ï¼›2. é‡æ„RLç³»ç»Ÿæ¶æ„ï¼Œæ„å»ºè½¨è¿¹å’Œå‚æ•°çš„æ•°æ®æœåŠ¡å™¨å®ç°çµæ´»çš„rolloutåè°ƒï¼›3. å¼€å‘ä¸€å¥—é¢å‘ååé‡çš„é™ˆæ—§æ€§æ„ŸçŸ¥ç­–ç•¥æ¥æå‡ç³»ç»Ÿæ€§èƒ½ã€‚

Result: StaleFlowç›¸æ¯”æœ€å…ˆè¿›ç³»ç»Ÿå®ç°äº†1.42-2.68å€ï¼ˆå¹³å‡1.17-2.01å€ï¼‰çš„ååé‡æå‡ï¼ŒåŒæ—¶ä¸æŸå®³æ”¶æ•›æ€§èƒ½ã€‚

Conclusion: StaleFlowé€šè¿‡è”åˆè§£å†³æ•°æ®é™ˆæ—§æ€§å’Œåæ–œæ€§é—®é¢˜ï¼Œåœ¨RLåè®­ç»ƒä¸­å®ç°äº†æ”¶æ•›å’Œæ€§èƒ½çš„åŒé‡ä¼˜åŒ–ï¼Œä¸ºå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒæä¾›äº†æœ‰æ•ˆçš„ç³»ç»Ÿè§£å†³æ–¹æ¡ˆã€‚

Abstract: Reinforcement learning (RL) post-training has become pivotal for enhancing the capabilities of modern large models. A recent trend is to develop RL systems with a fully disaggregated architecture, which decouples the three RL phases (rollout, reward, and training) onto separate resources and executes them asynchronously. However, two critical data-level concerns arise: (1) asynchronous execution leads to data staleness in trajectories (the data generated by rollout) as the model parameters used in rollout may not be up to date, which impairs RL convergence; and (2) the length variation of trajectories introduces severe data skewness, leading to workload imbalance and degraded system performance.
  Existing systems fail to address these two concerns in a unified manner. Techniques that tightly control data staleness often constrain effective data skewness mitigation, while aggressive data skewness mitigation tends to exacerbate data staleness. As a result, systems are forced to trade off convergence for performance, or vice versa. To address this, we propose StaleFlow, an RL post-training system that jointly tackles data staleness and skewness. First, to control staleness, StaleFlow introduces a global consistency protocol that tracks the full lifecycle of each trajectory and constrains staleness. Second, to mitigate skewness, StaleFlow re-designs the RL system architecture by constructing data servers for trajectories and parameters to achieve flexible rollout coordination. Subsequently, we develop a suite of staleness-aware, throughput-oriented strategies to enhance system performance. Evaluations show that StaleFlow achieves up to 1.42-2.68$\times$ (1.17-2.01$\times$ on average) higher throughput than state-of-the-art systems, without compromising convergence.

</details>


### [35] [From Design to Deorbit: A Solar-Electric Autonomous Module for Multi-Debris Remediation](https://arxiv.org/abs/2601.12830)
*Om Mishra,Jayesh Patil,Sathwik Narkedimilli,G Srikantha Sharma,Ananda S,Manjunath K Vanahalli*

Main category: cs.DC

TL;DR: æå‡ºä¸€ç§æ–°å‹è½¨é“ç¢ç‰‡ä¸»åŠ¨æ¸…é™¤æ¶æ„ï¼Œç»“åˆæœºæ¢°å¤¹æŒç³»ç»Ÿã€å¤ªé˜³èƒ½æ¨è¿›å™¨å’Œè‡ªä¸»å¯¼èˆªåè®®ï¼Œå®ç°é«˜æ•ˆã€å¯æŒç»­çš„ç¢ç‰‡ç§»é™¤


<details>
  <summary>Details</summary>
Motivation: è½¨é“ç¢ç‰‡æ—¥ç›Šç´¯ç§¯å¨èƒå¤ªç©ºæ“ä½œå¯æŒç»­æ€§ï¼Œç°æœ‰ç‡ƒæ–™ä¾èµ–æ–¹æ³•å­˜åœ¨å±€é™æ€§ï¼Œéœ€è¦å¼€å‘æ›´é«˜æ•ˆçš„ä¸»åŠ¨æ¸…é™¤è§£å†³æ–¹æ¡ˆ

Method: æ•´åˆæœºæ¢°å¤¹æŒç³»ç»Ÿç”¨äºå®‰å…¨æ•è·ã€é«˜æ•ˆå¤ªé˜³èƒ½NASAè¿›åŒ–æ°™æ¨è¿›å™¨(NEXT)å’Œè‡ªä¸»å¯¼èˆªåè®®ï¼Œé€šè¿‡é«˜ä¿çœŸä»¿çœŸéªŒè¯æ¶æ„èƒ½åŠ›

Result: æˆåŠŸå®ç°ä»800å…¬é‡Œåˆ°100å…¬é‡Œçš„é€†è¡Œç¦»è½¨ï¼Œé›·è¾¾æ‰©å±•å¡å°”æ›¼æ»¤æ³¢å¯¼èˆªä½ç½®RMSEå°äº10ç±³ï¼Œå»¶è¿Ÿ/ä¸­æ–­å®¹å¿ç½‘ç»œåè®®æ•°æ®äº¤ä»˜æ•ˆç‡è¾¾93%

Conclusion: è¯¥æ¶æ„æ˜¾è‘—æ¨è¿›è½¨é“ç®¡ç†ï¼Œä¸ºå¯å†ç”Ÿå¤ªé˜³èƒ½æ¨è¿›å»ºç«‹åŸºå‡†ï¼Œå‡å°‘å¯¹ä¼ ç»Ÿç‡ƒæ–™ä¾èµ–ï¼Œå»¶é•¿å¤šç›®æ ‡æ¸…é™¤ä»»åŠ¡å¯¿å‘½

Abstract: The escalating accumulation of orbital debris threatens the sustainability of space operations, necessitating active removal solutions that overcome the limitations of current fuel-dependent methods. To address this, this study introduces a novel remediation architecture that integrates a mechanical clamping system for secure capture with a high-efficiency, solar-powered NASA Evolutionary Xenon Thruster (NEXT) and autonomous navigation protocols. High-fidelity simulations validate the architecture's capabilities, demonstrating a successful retrograde deorbit from 800 km to 100 km, <10m position Root Mean Square Errors (RMSE) via radar-based Extended Kalman Filter (EKF) navigation, and a 93\% data delivery efficiency within 1 second using Delay/Disruption Tolerant Network (DTN) protocols. This approach significantly advances orbital management by establishing a benchmark for renewable solar propulsion that minimizes reliance on conventional fuels and extends mission longevity for multi-target removal.

</details>


### [36] [On Resilient and Efficient Linear Secure Aggregation in Hierarchical Federated Learning](https://arxiv.org/abs/2601.12853)
*Shudi Weng,Xiang Zhang,Yizhou Zhao,Giuseppe Caire,Ming Xiao,Mikael Skoglund*

Main category: cs.DC

TL;DR: è¯¥è®ºæ–‡ç ”ç©¶äº†ä¸å¯é é€šä¿¡ä¸‹åˆ†å±‚å®‰å…¨èšåˆçš„åŸºæœ¬æé™ï¼Œæå‡ºäº†æœ€ä¼˜åè®®å¹¶å»ºç«‹äº†åŒ¹é…çš„é€†è¯æ˜


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰ä¿¡æ¯è®ºå®‰å…¨èšåˆåè®®ä¸å®é™…è”é‚¦å­¦ä¹ é—®é¢˜ä¹‹é—´å­˜åœ¨å·®è·ï¼Œéœ€è¦ç ”ç©¶ä¸å¯é é€šä¿¡ç¯å¢ƒä¸‹çš„åˆ†å±‚å®‰å…¨èšåˆåŸºæœ¬æé™

Method: è€ƒè™‘å®¢æˆ·ç«¯è¿æ¥å¤šä¸ªä¸­ç»§çš„åˆ†å±‚ç½‘ç»œï¼Œå®¢æˆ·ç«¯-ä¸­ç»§å’Œä¸­ç»§-æœåŠ¡å™¨é“¾è·¯éƒ½å¯èƒ½ä¸­æ–­ï¼Œæå‡ºæœ€ä¼˜åè®®å¹¶å»ºç«‹åŒ¹é…çš„é€†è¯æ˜

Result: åˆ»ç”»äº†å®ç°é²æ£’å®‰å…¨èšåˆæ‰€éœ€çš„æœ€å°é€šä¿¡å’Œéšæœºæ€§æˆæœ¬ï¼Œæå‡ºäº†è¾¾åˆ°è¿™äº›æœ€å°æˆæœ¬çš„æœ€ä¼˜åè®®

Conclusion: è¯¥ç ”ç©¶å¡«è¡¥äº†ä¿¡æ¯è®ºå®‰å…¨èšåˆåè®®ä¸å®é™…è”é‚¦å­¦ä¹ é—®é¢˜ä¹‹é—´çš„ç©ºç™½ï¼Œä¸ºä¸å¯é é€šä¿¡ç¯å¢ƒä¸‹çš„åˆ†å±‚å®‰å…¨èšåˆæä¾›äº†ç†è®ºåŸºç¡€å’Œæœ€ä¼˜è§£å†³æ–¹æ¡ˆ

Abstract: In this paper, we study the fundamental limits of hierarchical secure aggregation under unreliable communication. We consider a hierarchical network where each client connects to multiple relays, and both client-to-relay and relay-to-server links are intermittent. Under this setting, we characterize the minimum communication and randomness costs required to achieve robust secure aggregation. We then propose an optimal protocol that attains these minimum costs, and establish its optimality through a matching converse proof. In addition, we introduce an improved problem formulation that bridges the gap between existing information-theoretic secure aggregation protocols and practical real-world federated learning problems.

</details>


### [37] [Sutradhara: An Intelligent Orchestrator-Engine Co-design for Tool-based Agentic Inference](https://arxiv.org/abs/2601.12967)
*Anish Biswas,Kanishk Goel,Jayashree Mohan,Alind Khare,Anjaly Parayil,Ramachandran Ramjee,Chetan Bansal*

Main category: cs.DC

TL;DR: SUTRADHARAæ˜¯ä¸€ä¸ªååŒè®¾è®¡çš„æ™ºèƒ½ä½“æ¨ç†ç³»ç»Ÿï¼Œé€šè¿‡æ•´åˆç¼–æ’å’ŒLLMæœåŠ¡æ¥ä¼˜åŒ–å·¥å…·è°ƒç”¨å»¶è¿Ÿï¼Œå‡å°‘æœ€ç»ˆç­”æ¡ˆçš„é¦–ä»¤ç‰Œæ¸²æŸ“æ—¶é—´ã€‚


<details>
  <summary>Details</summary>
Motivation: åŸºäºå·¥å…·çš„æ™ºèƒ½ä½“åº”ç”¨å·²æˆä¸ºLLMéƒ¨ç½²çš„ä¸»æµèŒƒå¼ï¼Œä½†å¤šè½®LLMè°ƒç”¨å’Œå·¥å…·æ‰§è¡Œå¯¼è‡´é¦–ä»¤ç‰Œæ¸²æŸ“å»¶è¿Ÿæ˜¾è‘—å¢åŠ ã€‚ç ”ç©¶å‘ç°å·¥å…·è°ƒç”¨å 30-80%çš„å»¶è¿Ÿï¼ŒKVç¼“å­˜å‘½ä¸­ç‡å´©æºƒï¼Œé¡ºåºç¼–æ’æµªè´¹äº†æ½œåœ¨çš„å¹¶è¡Œæ€§ã€‚

Method: æå‡ºSUTRADHARAç³»ç»Ÿï¼Œé€šè¿‡è–„APIæ•´åˆç¼–æ’å’ŒLLMæœåŠ¡ï¼Œå®ç°ä¸‰ç§ä¼˜åŒ–ï¼š1) å·¥å…·æ„ŸçŸ¥æç¤ºåˆ†å‰²ï¼Œé‡å å·¥å…·æ‰§è¡Œä¸åç»­LLMé¢„å¡«å……ï¼›2) æµå¼å·¥å…·æ‰§è¡Œï¼Œåœ¨è§£ç æœŸé—´å¢é‡è°ƒåº¦å·¥å…·ï¼›3) ç¼–æ’å™¨æ„ŸçŸ¥ç¼“å­˜ç®¡ç†ï¼Œä½¿ç”¨è¯­ä¹‰æç¤ºæé«˜å‘½ä¸­ç‡ã€‚

Result: åœ¨vLLMä¸Šå®ç°SUTRADHARAï¼Œåœ¨A100 GPUä¸Šï¼Œä¸­ä½æ•°é¦–ä»¤ç‰Œæ¸²æŸ“å»¶è¿Ÿé™ä½15%ï¼Œç«¯åˆ°ç«¯å»¶è¿Ÿé™ä½10%ï¼Œè¯æ˜ååŒè®¾è®¡èƒ½ç³»ç»Ÿæ€§åœ°é™ä½æ™ºèƒ½ä½“ç³»ç»Ÿå»¶è¿Ÿã€‚

Conclusion: ç¼–æ’å™¨å’ŒLLMå¼•æ“çš„è§£è€¦é»‘ç›’è®¾è®¡å¯¼è‡´æ€§èƒ½ç“¶é¢ˆï¼ŒååŒè®¾è®¡é€šè¿‡è·¨å±‚ä¼˜åŒ–èƒ½æœ‰æ•ˆè§£å†³æ™ºèƒ½ä½“æ¨ç†ä¸­çš„å»¶è¿Ÿé—®é¢˜ï¼ŒSUTRADHARAå±•ç¤ºäº†è¿™ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

Abstract: Agentic applications are LLMs that iteratively invoke external tools to accomplish complex tasks. Such tool-based agents are rapidly becoming the dominant paradigm for deploying language models in production. Unlike traditional single-turn inference, agentic workloads chain together multiple LLM calls and tool executions before producing a final response, creating a new performance bottleneck that manifests as increased latency in First Token Rendered (FTR) of the final answer. Through analysis of synthetic requests at production scale, we reveal three critical challenges: tool calls account for 30-80% of FTR latency, KV cache hit rates collapse despite substantial context reuse across iterations, and sequential orchestration wastes potential intra-request parallelism by sequentially executing LLM calls and tools. These bottlenecks stem from a design gap in which orchestrators and LLM engines operate as decoupled black boxes, preventing cross-layer optimizations. We present SUTRADHARA, a co-designed agentic inference system that integrates orchestration with LLM serving through a thin API enabling three optimizations: overlap tool execution with subsequent LLM prefill using tool-aware prompt splitting, streaming tool execution to dispatch tools incrementally during decode rather than waiting for complete output, and orchestrator-aware cache management that uses semantic hints to improve hit rates and reduce thrashing. Implemented on vLLM, SUTRADHARA reduces median FTR latency by 15% and end-to-end latency by 10% across workloads on A100 GPUs, demonstrating that co-design can systematically tame latency in agentic systems.

</details>


### [38] [Enshrined Proposer Builder Separation in the presence of Maximal Extractable Value](https://arxiv.org/abs/2601.12989)
*Yitian Wang,Yebo Feng,Yingjiu Li,Jiahua Xu*

Main category: cs.DC

TL;DR: ePBSè™½ç„¶åˆ†ç¦»äº†åŒºå—æ„å»ºä¸æè®®ï¼Œä½†é€šè¿‡MEVé©±åŠ¨çš„æ‹å–æœºåˆ¶åŠ å‰§äº†åˆ©æ¶¦å’Œå†…å®¹é›†ä¸­åŒ–ï¼Œä½¿å°‘æ•°é«˜æ•ˆæ„å»ºè€…è·å–å¤§éƒ¨åˆ†ä»·å€¼ï¼ŒåŒæ—¶95.4%çš„åŒºå—ä»·å€¼æµå‘æè®®è€…ï¼Œå­˜åœ¨ç»æµåè§ã€‚


<details>
  <summary>Details</summary>
Motivation: PoSå…±è¯†ä¸­MEVå¼•å‘ç»æµé›†ä¸­åŒ–å’Œå†…å®¹æ“çºµæ‹…å¿§ï¼ŒEthereumç¤¾åŒºæå‡ºePBSï¼ˆEIP-7732ï¼‰å°†PBSåµŒå…¥å…±è¯†å±‚ä»¥åˆ†ç¦»åŒºå—æ„å»ºä¸æè®®ï¼Œä½†éœ€è¦è¯„ä¼°å…¶å®é™…æ•ˆæœã€‚

Method: å»ºç«‹ç»“åˆæ•°å­¦åˆ†æå’ŒåŸºäºä»£ç†æ¨¡æ‹Ÿçš„å½¢å¼åŒ–æ¡†æ¶ï¼Œè¯„ä¼°ePBSçš„æ‹å–å¼åŒºå—æ„å»ºæœºåˆ¶ï¼Œç‰¹åˆ«å…³æ³¨MEVåŠ¨æ€ã€‚

Result: ePBSæ˜¾è‘—åŠ å‰§åˆ©æ¶¦é›†ä¸­åŒ–ï¼šåŸºå°¼ç³»æ•°ä»æ ‡å‡†PoSçš„0.1749ä¸Šå‡åˆ°ePBSä¸‹çš„0.8358ï¼›95.4%çš„åŒºå—ä»·å€¼å¥–åŠ±ç»™æè®®è€…ï¼Œå°½ç®¡ä»–ä»¬åœ¨åŒºå—ç»„è£…ä¸­ä½œç”¨æœ‰é™ï¼›å°‘æ•°é«˜æ•ˆæ„å»ºè€…é€šè¿‡MEVé©±åŠ¨çš„æ‹å–æ•è·å¤§éƒ¨åˆ†ä»·å€¼ã€‚

Conclusion: ePBSè™½ç„¶é‡æ–°åˆ†é…äº†æ„å»ºè€…å’Œæè®®è€…çš„è´£ä»»ï¼Œä½†åŠ å‰§äº†æ„å»ºè€…é‡‡ç”¨æ¿€è¿›MEVç­–ç•¥çš„æ¿€åŠ±ï¼Œéœ€è¦æœªæ¥ç ”ç©¶æ›´å¥½çš„æœºåˆ¶è®¾è®¡æ¥å¹³è¡¡å»ä¸­å¿ƒåŒ–ã€å…¬å¹³æ€§å’ŒMEVç¼“è§£ã€‚

Abstract: In blockchain systems operating under the Proof-of-Stake (PoS) consensus mechanism, fairness in transaction processing is essential to preserving decentralization and maintaining user trust. However, with the emergence of Maximal Extractable Value (MEV), concerns about economic centralization and content manipulation have intensified. To address these vulnerabilities, the Ethereum community has introduced Proposer Builder Separation (PBS), which separates block construction from block proposal. Later, enshrined Proposer Builder Separation (ePBS) was also proposed in EIP-7732, which embeds PBS directly into the Ethereum consensus layer.
  Our work identifies key limitations of ePBS by developing a formal framework that combines mathematical analysis and agent-based simulations to evaluate its auction-based block-building mechanism, with particular emphasis on MEV dynamics. Our results reveal that, although ePBS redistributes responsibilities between builders and proposers, it significantly amplifies profit and content centralization: the Gini coefficient for profits rises from 0.1749 under standard PoS without ePBS to 0.8358 under ePBS. This sharp increase indicates that a small number of efficient builders capture most value via MEV-driven auctions. Moreover, 95.4% of the block value is rewarded to proposers in ePBS, revealing a strong economic bias despite their limited role in block assembly. These findings highlight that ePBS exacerbates incentives for builders to adopt aggressive MEV strategies, suggesting the need for future research into mechanism designs that better balance decentralization, fairness, and MEV mitigation.

</details>


### [39] [Exploration on Highly Dynamic Graphs](https://arxiv.org/abs/2601.13047)
*Ashish Saxena,Kaushik Mondal*

Main category: cs.DC

TL;DR: åœ¨åŠ¨æ€å›¾çš„æ¢ç´¢é—®é¢˜ä¸­ï¼Œæœ¬æ–‡æ”¹è¿›äº†1-Interval Connectivityæ¨¡å‹çš„ä¸å¯èƒ½æ€§ç»“æœï¼Œå¹¶è¯æ˜åœ¨Connectivity Timeæ¨¡å‹ä¸­ï¼Œå³ä½¿æœ‰(n-1)(n-2)/2ä¸ªç§»åŠ¨ä»£ç†ä¹Ÿæ— æ³•å®Œæˆæ¢ç´¢ï¼Œæ˜¾è‘—æ”¹è¿›äº†å…ˆå‰nä¸ªä»£ç†çš„ç•Œé™ã€‚åŒæ—¶æå‡ºäº†ä½¿ç”¨(n-1)(n-2)/2+1ä¸ªä»£ç†çš„æ¢ç´¢ç®—æ³•ã€‚


<details>
  <summary>Details</summary>
Motivation: ç ”ç©¶ç§»åŠ¨ä»£ç†åœ¨åŠ¨æ€å›¾ä¸­çš„æ¢ç´¢é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨1-Interval Connectivityå’ŒConnectivity Timeä¸¤ç§é‡è¦æ¨¡å‹ä¸­ã€‚å…ˆå‰ç ”ç©¶å·²å–å¾—ä¸€äº›ç»“æœï¼Œä½†å­˜åœ¨æ”¹è¿›ç©ºé—´ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»£ç†æ•°é‡ç•Œé™å’Œç®—æ³•è®¾è®¡æ–¹é¢ã€‚

Method: é¦–å…ˆå¼ºåŒ–1-Interval Connectivityæ¨¡å‹çš„ç°æœ‰ä¸å¯èƒ½æ€§ç»“æœã€‚ç„¶åè¯æ˜åœ¨Connectivity Timeæ¨¡å‹ä¸­ï¼Œå³ä½¿æœ‰(n-1)(n-2)/2ä¸ªä»£ç†ä¹Ÿæ— æ³•å®Œæˆæ¢ç´¢ã€‚æ¥ç€è¯æ˜è¦ä½¿ç”¨(n-1)(n-2)/2+1ä¸ªä»£ç†è§£å†³æ¢ç´¢é—®é¢˜ï¼Œéœ€è¦1è·³å¯è§æ€§ã€‚æœ€åæå‡ºä¸€ä¸ªä½¿ç”¨è¯¥æ•°é‡ä»£ç†çš„æ¢ç´¢ç®—æ³•ã€‚

Result: 1. æ”¹è¿›äº†1-Interval Connectivityæ¨¡å‹çš„ä¸å¯èƒ½æ€§ç»“æœï¼›2. è¯æ˜åœ¨Connectivity Timeæ¨¡å‹ä¸­ï¼Œå³ä½¿æœ‰(n-1)(n-2)/2ä¸ªä»£ç†ä¹Ÿæ— æ³•å®Œæˆæ¢ç´¢ï¼Œæ˜¾è‘—æ”¹è¿›äº†å…ˆå‰nä¸ªä»£ç†çš„ç•Œé™ï¼›3. è¯æ˜è¦ä½¿ç”¨(n-1)(n-2)/2+1ä¸ªä»£ç†ï¼Œéœ€è¦1è·³å¯è§æ€§ï¼›4. æå‡ºäº†ä½¿ç”¨è¯¥æ•°é‡ä»£ç†çš„æ¢ç´¢ç®—æ³•ï¼Œå‡è®¾å…¨å±€é€šä¿¡ã€1è·³å¯è§æ€§å’ŒO(log n)å†…å­˜ã€‚

Conclusion: æœ¬æ–‡åœ¨åŠ¨æ€å›¾æ¢ç´¢é—®é¢˜ä¸Šå–å¾—äº†é‡è¦è¿›å±•ï¼Œæ˜¾è‘—æ”¹è¿›äº†ä»£ç†æ•°é‡çš„ä¸å¯èƒ½æ€§ç•Œé™ï¼Œå¹¶æå‡ºäº†æ¥è¿‘æœ€ä¼˜çš„ç®—æ³•è§£å†³æ–¹æ¡ˆï¼Œä¸ºåŠ¨æ€ç½‘ç»œä¸­çš„ç§»åŠ¨ä»£ç†æ¢ç´¢é—®é¢˜æä¾›äº†æ–°çš„ç†è®ºè§è§£ã€‚

Abstract: We study the exploration problem by mobile agents in two prominent models of dynamic graphs: $1$-Interval Connectivity and Connectivity Time. The $1$-Interval Connectivity model was introduced by Kuhn et al.~[STOC 2010], and the Connectivity Time model was proposed by Michail et al.~[JPDC 2014]. Recently, Saxena et al.~[TCS 2025] investigated the exploration problem under both models. In this work, we first strengthen the existing impossibility results for the $1$-Interval Connectivity model. We then show that, in Connectivity Time dynamic graphs, exploration is impossible with $\frac{(n-1)(n-2)}{2}$ mobile agents, even when the agents have full knowledge of all system parameters, global communication, full visibility, and infinite memory. This significantly improves the previously known bound of $n$. Moreover, we prove that to solve exploration with $\frac{(n-1)(n-2)}{2}+1$ agents, $1$-hop visibility is necessary. Finally, we present an exploration algorithm that uses $\frac{(n-1)(n-2)}{2}+1$ agents, assuming global communication, $1$-hop visibility, and $O(\log n)$ memory per agent.

</details>


### [40] [OPTIMUM-DERAM: Highly Consistent, Scalable, and Secure Multi-Object Memory using RLNC](https://arxiv.org/abs/2601.13146)
*Nicolas Nicolaou,Kishori M. Konwar,Moritz Grundei,Aleksandr Bezobchuk,Muriel MÃ©dard,Sriram Vishwanath*

Main category: cs.DC

TL;DR: OPTIMUM-DERAMæ˜¯ä¸€ä¸ªé«˜åº¦ä¸€è‡´ã€å¯æ‰©å±•ã€å®‰å…¨ä¸”å»ä¸­å¿ƒåŒ–çš„å…±äº«å†…å­˜è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡éšæœºçº¿æ€§ç½‘ç»œç¼–ç å’Œä¸€è‡´æ€§å“ˆå¸Œç¯ç­‰æŠ€æœ¯ï¼Œè§£å†³äº†ä¼ ç»Ÿåˆ†å¸ƒå¼å…±äº«å†…å­˜èµ„æºæ¶ˆè€—å¤§çš„é—®é¢˜ã€‚


<details>
  <summary>Details</summary>
Motivation: ä¼ ç»Ÿåˆ†å¸ƒå¼å…±äº«å†…å­˜å®ç°é€šè¿‡åœ¨åŒä¸€ç»„æ•°æ®ä¸»æœºä¸Šå¤šçº¿ç¨‹åŒ–å•ä¸ªå¯¹è±¡å†…å­˜å®ä¾‹æ¥æ”¯æŒå¤šå¯¹è±¡ï¼Œç†è®ºä¸Šå¯è¡Œä½†èµ„æºéœ€æ±‚å·¨å¤§ï¼Œåœ¨å®é™…ç³»ç»Ÿä¸­æˆæœ¬è¿‡é«˜ã€‚

Method: æå‡ºå»ä¸­å¿ƒåŒ–ã€å¯é‡æ„çš„åŸå­è¯»å†™å…±äº«å†…å­˜(DeRAM)ï¼š1)åˆ©ç”¨éšæœºçº¿æ€§ç½‘ç»œç¼–ç (Random Linear Network Codes)æé«˜æ€§èƒ½å’Œå­˜å‚¨å¯æ‰©å±•æ€§ï¼›2)åŸºäºä¸€è‡´æ€§å“ˆå¸Œç¯å®ç°å¯¹è±¡æ”¾ç½®å’Œå‘ç°ï¼Œæ”¯æŒæ›´å¤šåŸå­å¯¹è±¡ï¼›3)é€šè¿‡åŒºå—é“¾é¢„è¨€æœºä½œä¸ºæ³¨å†ŒæœåŠ¡ï¼Œæ”¯æŒåŠ¨æ€èŠ‚ç‚¹åŠ å…¥/ç¦»å¼€ï¼›4)å®¹å¿æ‹œå åº­æ•…éšœï¼Œç¡®ä¿å®‰å…¨æ€§ã€‚

Result: åœ¨å…¨çƒåˆ†å¸ƒå¼èŠ‚ç‚¹ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒOPTIMUM-DERAMç›¸æ¯”ä¹‹å‰çš„åˆ†å¸ƒå¼å…±äº«å†…å­˜è§£å†³æ–¹æ¡ˆï¼ˆå¦‚ABDç®—æ³•ï¼‰åœ¨æ€§èƒ½å’Œå¯æ‰©å±•æ€§æ–¹é¢æœ‰æ˜¾è‘—æå‡ã€‚

Conclusion: OPTIMUM-DERAMæä¾›äº†ä¸€ä¸ªé«˜æ•ˆã€å¯æ‰©å±•ã€å®‰å…¨çš„å»ä¸­å¿ƒåŒ–å…±äº«å†…å­˜è§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•åœ¨å®é™…éƒ¨ç½²ä¸­çš„èµ„æºç“¶é¢ˆé—®é¢˜ã€‚

Abstract: This paper introduces OPTIMUM-DERAM, a highly consistent, scalable, secure, and decentralized shared memory solution. Traditional distributed shared memory implementations offer multi-object support by multi-threading a single object memory instance over the same set of data hosts. While theoretically sound, the amount of resources required made such solutions prohibitively expensive in practical systems. OPTIMUM-DERAM proposes a decentralized, reconfigurable, atomic read/write shared memory (DeRAM) that: (i) achieves improved performance and storage scalability by leveraging Random Linear Network Codes (RLNC); (ii) scales in the number of supported atomic objects by introducing a new object placement and discovery approach based on a consistent hashing ring; (iii) scales in the number of participants by allowing dynamic joins and departures leveraging a blockchain oracle to serve as a registry service; and (iv) is secure against malicious behavior by tolerating Byzantine failures.
  Experimental results over a globally distributed set of nodes, help us realize the performance and scalability gains of OPTIMUM-DERAM over previous distributed shared memory solutions (i.e., the ABD algorithm [3])

</details>


### [41] [Towards Scalable Federated Container Orchestration: The CODECO Approach](https://arxiv.org/abs/2601.13351)
*Rute C. Sofia,Josh Salomon,Ray Carrol,Luis GarcÃ©s-Erice,Peter Urbanetz,JÃ¼rgen Gesswein,Rizkallah Touma,Alejandro Espinosa,Luis M. Contreras,Vasileios Theodorou,George Papathanail,Georgios Koukis,Vassilis Tsaoussidis,Alberto del Rio,David Jimenez,Efterpi Paraskevoulakou,Panagiotis Karamolegkos,John Soldatos,Borja Dorado Nogales,Alejandro Tjaarda*

Main category: cs.DC

TL;DR: CODECOæ˜¯ä¸€ä¸ªé¢å‘Kubernetesçš„è”é‚¦ç¼–æ’æ¡†æ¶ï¼Œé‡‡ç”¨æ•°æ®-è®¡ç®—-ç½‘ç»œååŒç¼–æ’æ–¹æ³•ï¼Œæ”¯æŒå¼‚æ„åŸºç¡€è®¾æ–½ã€ç§»åŠ¨æ€§å’Œå¤šäº‘æä¾›å•†æ“ä½œã€‚


<details>
  <summary>Details</summary>
Motivation: è§£å†³äº‘ä¸­å¿ƒåŒ–éƒ¨ç½²çš„å±€é™æ€§ï¼Œæ”¯æŒåœ¨è”é‚¦è¾¹ç¼˜äº‘ç¯å¢ƒä¸­è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åº”ç”¨ç¼–æ’å’Œç®¡ç†ã€‚

Method: æ‰©å±•Kubernetesï¼Œå¼•å…¥è¯­ä¹‰åº”ç”¨æ¨¡å‹ã€åŸºäºåˆ†åŒºçš„è”é‚¦æœºåˆ¶å’ŒAIè¾…åŠ©å†³ç­–æ”¯æŒï¼›é‡‡ç”¨æ··åˆæ²»ç†æ¨¡å‹ï¼Œç»“åˆé›†ä¸­ç­–ç•¥æ‰§è¡Œä¸åˆ†æ•£æ‰§è¡Œå­¦ä¹ ã€‚

Result: å¼€å‘äº†CODECOæ¶æ„å’Œæ ¸å¿ƒç»„ä»¶ï¼Œè®¾è®¡äº†ä»£è¡¨æ€§ç¼–æ’å·¥ä½œæµï¼Œå¹¶å»ºç«‹äº†åŸºäºè½¯ä»¶çš„å¯é‡å¤å®éªŒæ¡†æ¶ã€‚

Conclusion: CODECOé€šè¿‡æ•°æ®-è®¡ç®—-ç½‘ç»œååŒç¼–æ’å’Œæ··åˆæ²»ç†æ¨¡å‹ï¼Œå®ç°äº†åœ¨è”é‚¦è¾¹ç¼˜äº‘ç¯å¢ƒä¸­ä¿æŒå…¨å±€ä¸€è‡´æ€§çš„åŒæ—¶æ”¯æŒè¾¹ç¼˜è‡ªä¸»æ€§çš„åº”ç”¨ç¼–æ’ã€‚

Abstract: This paper presents CODECO, a federated orchestration framework for Kubernetes that addresses the limitations of cloud-centric deployment. CODECO adopts a data-compute-network co-orchestration approach to support heterogeneous infrastructures, mobility, and multi-provider operation.
  CODECO extends Kubernetes with semantic application models, partition-based federation, and AI-assisted decision support, enabling context-aware placement and adaptive management of applications and their micro-services across federated environments. A hybrid governance model combines centralized policy enforcement with decentralized execution and learning to preserve global coherence while supporting far Edge autonomy. The paper describes the architecture and core components of CODECO, outlines representative orchestration workflows, and introduces a software-based experimentation framework for reproducible evaluation in federated Edge-Cloud infrastructure environments.

</details>


### [42] [Driving Computational Efficiency in Large-Scale Platforms using HPC Technologies](https://arxiv.org/abs/2601.13424)
*Alexander Martinez Mendez,Antonio J. Rubio-Montero,Carlos J. Barrios H.,HernÃ¡n Asorey,Rafael Mayo-GarcÃ­a,Luis A. NÃºÃ±ez*

Main category: cs.DC

TL;DR: LAGOé¡¹ç›®åˆ†æå…¶HPCèµ„æºåˆ©ç”¨æ•ˆç‡ï¼Œå‘ç°è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿä»»åŠ¡CPUæ•ˆç‡é«˜ï¼Œä½†çŸ­æµ‹è¯•ä½œä¸šä¼šæ‰­æ›²æ•´ä½“æŒ‡æ ‡ï¼Œæå‡ºäº†ä¼˜åŒ–èµ„æºè¯·æ±‚å’Œå·¥ä½œæµç®¡ç†çš„å»ºè®®ã€‚


<details>
  <summary>Details</summary>
Motivation: LAGOé¡¹ç›®ä½¿ç”¨å¤§é‡HPCèµ„æºè¿›è¡Œå¤æ‚çš„å®‡å®™çº¿ç‰©ç†æ¨¡æ‹Ÿï¼Œèµ„æºæ•ˆç‡å¯¹ç§‘å­¦äº§å‡ºå’Œå¯æŒç»­æ€§è‡³å…³é‡è¦ã€‚éœ€è¦é‡åŒ–å¹¶æ”¹è¿›HPCèµ„æºåˆ©ç”¨æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹LAGOç‰¹æœ‰çš„ç²—ç²’åº¦ã€ä»»åŠ¡å¹¶è¡Œè®¡ç®—è´Ÿè½½ã€‚

Method: åˆ†æEGI FedCloudå¹³å°çš„å†å²ä½œä¸šè®°è´¦æ•°æ®ï¼Œè¯†åˆ«ä¸»è¦å·¥ä½œè´Ÿè½½ç±»åˆ«ï¼ˆè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿã€æ•°æ®å¤„ç†ã€ç”¨æˆ·åˆ†æ/æµ‹è¯•ï¼‰ï¼Œä½¿ç”¨CPUåˆ©ç”¨ç‡ã€walltimeåˆ©ç”¨ç‡å’ŒI/Oæ¨¡å¼ç­‰å…³é”®æ•ˆç‡æŒ‡æ ‡è¯„ä¼°æ€§èƒ½ã€‚

Result: åˆ†ææ˜¾ç¤ºæ˜¾è‘—æ¨¡å¼ï¼šå•ä¸ªæ¨¡æ‹Ÿä»»åŠ¡å†…CPUæ•ˆç‡é«˜ï¼Œä½†çŸ­æµ‹è¯•ä½œä¸šå¯¹èšåˆæŒ‡æ ‡æœ‰æ‰­æ›²å½±å“ã€‚è¯†åˆ«äº†å…·ä½“ä½æ•ˆé—®é¢˜ï¼Œä¸ºLAGOçš„HPCä½¿ç”¨æä¾›äº†æ•°æ®é©±åŠ¨çš„è§è§£ã€‚

Conclusion: ç ”ç©¶ç»“æœç›´æ¥ä¸ºä¼˜åŒ–èµ„æºè¯·æ±‚ã€æ”¹è¿›å·¥ä½œæµç®¡ç†ç­–ç•¥æä¾›å»ºè®®ï¼ŒæŒ‡å¯¼æœªæ¥æå‡è®¡ç®—ååé‡çš„åŠªåŠ›ï¼Œæœ€ç»ˆæœ€å¤§åŒ–LAGO HPCæŠ•èµ„çš„ç§‘å­¦å›æŠ¥ã€‚

Abstract: The Latin American Giant Observatory (LAGO) project utilizes extensive High-Performance Computing (HPC) resources for complex astroparticle physics simulations, making resource efficiency critical for scientific productivity and sustainability. This article presents a detailed analysis focused on quantifying and improving HPC resource utilization efficiency specifically within the LAGO computational environment. The core objective is to understand how LAGO's distinct computational workloads-characterized by a prevalent coarse-grained, task-parallel execution model-consume resources in practice. To achieve this, we analyze historical job accounting data from the EGI FedCloud platform, identifying primary workload categories (Monte Carlo simulations, data processing, user analysis/testing) and evaluating their performance using key efficiency metrics (CPU utilization, walltime utilization, and I/O patterns). Our analysis reveals significant patterns, including high CPU efficiency within individual simulation tasks contrasted with the distorting impact of short test jobs on aggregate metrics. This work pinpoints specific inefficiencies and provides data-driven insights into LAGO's HPC usage. The findings directly inform recommendations for optimizing resource requests, refining workflow management strategies, and guiding future efforts to enhance computational throughput, ultimately maximizing the scientific return from LAGO's HPC investments.

</details>


### [43] [RASC: Enhancing Observability & Programmability in Smart Spaces](https://arxiv.org/abs/2601.13496)
*Anna Karanika,Kai-Siang Wang,Han-Ting Liang,Shalni Sundram,Indranil Gupta*

Main category: cs.DC

TL;DR: æå‡ºRASCæŠ½è±¡ï¼Œä¸ºç‰©è”ç½‘è®¾å¤‡åŠ¨ä½œæä¾›è¯·æ±‚-ç¡®è®¤-å¼€å§‹-å®Œæˆå››ä¸ªå…³é”®ç‚¹çš„ç¡®è®¤æœºåˆ¶ï¼Œæå‡ç‰©è”ç½‘ç³»ç»Ÿçš„å¯è§‚æµ‹æ€§å’Œå¯ç¼–ç¨‹æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰RPCæœºåˆ¶è™½ç„¶æ„æˆç³»ç»Ÿæ ˆçš„åŸºç¡€ï¼Œä½†å¯¹äºæ™ºèƒ½å®¶å±…ã€ä»“åº“ã€åŠå…¬æ¥¼ç­‰"é¢å‘ç”¨æˆ·"çš„ç‰©è”ç½‘è®¾å¤‡é›†åˆï¼Œéœ€è¦æ›´å¯Œè¡¨è¾¾åŠ›çš„æŠ½è±¡ã€‚ä»¥å¾€å·¥ä½œä¸»è¦å…³æ³¨ç‰©è”ç½‘é€šä¿¡çš„å¯é æ€§ï¼Œè€Œæœ¬ç ”ç©¶èšç„¦äºæå‡ç‰©è”ç½‘åŠ¨ä½œçš„å¯è§‚æµ‹æ€§å’Œå¯ç¼–ç¨‹æ€§ã€‚

Method: æå‡ºRASCï¼ˆè¯·æ±‚-ç¡®è®¤-å¼€å§‹-å®Œæˆï¼‰æŠ½è±¡ï¼Œåœ¨ç‰©è”ç½‘è®¾å¤‡åŠ¨ä½œå¯åŠ¨åçš„å…³é”®èŠ‚ç‚¹æä¾›ç¡®è®¤æœºåˆ¶ã€‚RASCè®¾è®¡ä¸ºåœ¨ç°æœ‰RPCæœºåˆ¶ä¹‹ä¸Šå®ç°ï¼Œè€Œéæ›¿ä»£æ–¹æ¡ˆã€‚å°†å…¶é›†æˆåˆ°æµè¡Œçš„å¼€æºç‰©è”ç½‘æ¡†æ¶Home Assistantä¸­ã€‚

Result: RASCèƒ½å¤Ÿæ»¡è¶³å»¶è¿ŸSLOï¼Œç‰¹åˆ«æ˜¯å¯¹äºæ™ºèƒ½ç©ºé—´ä¸­å¸¸è§çš„æŒç»­æ•°åˆ†é’Ÿçš„é•¿åŠ¨ä½œã€‚åŸºäºRASCçš„å®¶åº­è‡ªåŠ¨åŒ–è°ƒåº¦ç­–ç•¥æ¯”æœ€å…ˆè¿›çš„å¯¹åº”æ–¹æ¡ˆæ€§èƒ½æå‡10%-55%ã€‚

Conclusion: RASCæŠ½è±¡æ›´é€‚åˆç‰©è”ç½‘åŠ¨ä½œçš„è‡ªç„¶ç‰¹æ€§ï¼ˆè®¾å¤‡é—´ç©ºé—´å˜åŒ–å’Œæ—¶é—´ä¸Šæ—¶é—´å˜åŒ–ï¼‰ï¼Œæ”¯æŒå‡†ç¡®é¢„æµ‹åŠ¨ä½œå®Œæˆæ—¶é—´ã€æ›´å¿«æ£€æµ‹åŠ¨ä½œå¤±è´¥ã€ç»†ç²’åº¦ç¼–ç¨‹ä¾èµ–å’Œè°ƒåº¦ç­‰æ–°åŠŸèƒ½ã€‚

Abstract: While RPCs form the bedrock of systems stacks, we posit that IoT device collections in smart spaces like homes, warehouses, and office buildings--which are all "user-facing"--require a more expressive abstraction. Orthogonal to prior work, which improved the reliability of IoT communication, our work focuses on improving the observability and programmability of IoT actions. We present the RASC (Request-Acknowledge-Start-Complete) abstraction, which provides acknowledgments at critical points after an IoT device action is initiated. RASC is a better fit for IoT actions, which naturally vary in length spatially (across devices) and temporally (across time, for a given device). RASC also enables the design of several new features: predicting action completion times accurately, detecting failures of actions faster, allowing fine-grained dependencies in programming, and scheduling. RASC is intended to be implemented atop today's available RPC mechanisms, rather than as a replacement. We integrated RASC into a popular and open-source IoT framework called Home Assistant. Our trace-driven evaluation finds that RASC meets latency SLOs, especially for long actions that last O(mins), which are common in smart spaces. Our scheduling policies for home automations (e.g., routines) outperform state-of-the-art counterparts by 10%-55%.

</details>


### [44] [A Kubernetes custom scheduler based on reinforcement learning for compute-intensive pods](https://arxiv.org/abs/2601.13579)
*Hanlin Zhou,Huah Yong Chan,Shun Yao Zhang,Meie Lin,Jingfei Ni*

Main category: cs.DC

TL;DR: æå‡ºä¸¤ç§åŸºäºæ·±åº¦Qç½‘ç»œçš„Kubernetesè‡ªå®šä¹‰è°ƒåº¦å™¨SDQNå’ŒSDQN-nï¼Œåœ¨è®¡ç®—å¯†é›†å‹å·¥ä½œè´Ÿè½½ä¸‹æ¯”é»˜è®¤è°ƒåº¦å™¨æ€§èƒ½æ›´ä¼˜ï¼Œèƒ½é™ä½èŠ‚ç‚¹CPUåˆ©ç”¨ç‡10-20%ï¼Œå®ç°æ›´èŠ‚èƒ½çš„æ•°æ®ä¸­å¿ƒã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€äº‘è®¡ç®—å’Œè½»é‡çº§å®¹å™¨æŠ€æœ¯çš„å‘å±•ï¼ŒDockeræˆä¸ºå¿«é€ŸæœåŠ¡éƒ¨ç½²çš„ä¸»æµæŠ€æœ¯ï¼ŒKubernetesè´Ÿè´£podç¼–æ’ã€‚ä½†å¯¹äºè®¡ç®—å¯†é›†å‹å·¥ä½œè´Ÿè½½ï¼ˆç‰¹åˆ«æ˜¯æ‰§è¡Œå®¹å™¨åŒ–æœºå™¨å­¦ä¹ è®­ç»ƒçš„WebæœåŠ¡ï¼‰ï¼Œé»˜è®¤çš„Kubernetesè°ƒåº¦å™¨æ— æ³•å®ç°æœ€ä¼˜çš„èµ„æºåˆ†é…ã€‚

Method: æå‡ºäº†ä¸¤ç§åŸºäºæ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰æ¡†æ¶çš„è‡ªå®šä¹‰å¼ºåŒ–å­¦ä¹ è°ƒåº¦å™¨ï¼šSDQNå’ŒSDQN-nã€‚SDQN-né‡‡ç”¨äº†å°†podæ•´åˆåˆ°æ›´å°‘èŠ‚ç‚¹ä¸Šçš„ç­–ç•¥ï¼Œè¿›ä¸€æ­¥æ”¾å¤§èµ„æºèŠ‚çœæ•ˆæœã€‚

Result: åœ¨è®¡ç®—å¯†é›†å‹åœºæ™¯ä¸‹ï¼Œè¿™ä¸¤ç§æ¨¡å‹çš„è¡¨ç°ä¼˜äºé»˜è®¤Kubernetesè°ƒåº¦å™¨ä»¥åŠåŸºäºTransformerå’ŒLSTMçš„æ›¿ä»£æ–¹æ¡ˆï¼Œå°†é›†ç¾¤èŠ‚ç‚¹çš„å¹³å‡CPUåˆ©ç”¨ç‡é™ä½äº†10%ï¼Œä½¿ç”¨SDQN-næ—¶é™ä½è¶…è¿‡20%ã€‚SDQN-né€šè¿‡å°†podæ•´åˆåˆ°æ›´å°‘èŠ‚ç‚¹ä¸Šè¿›ä¸€æ­¥æ”¾å¤§äº†èµ„æºèŠ‚çœã€‚

Conclusion: podè°ƒåº¦å¿…é¡»æ ¹æ®ä¸åŒåœºæ™¯é‡‡ç”¨å®šåˆ¶åŒ–ç­–ç•¥ä»¥è·å¾—æ›´å¥½æ€§èƒ½ã€‚æœ¬æ–‡æå‡ºçš„SDQNå’ŒSDQN-næ¶æ„ä¸­çš„å¼ºåŒ–å­¦ä¹ ç»„ä»¶å¯ä»¥é€šè¿‡è°ƒæ•´å‚æ•°è½»æ¾è°ƒä¼˜ï¼Œèƒ½å¤Ÿé€‚åº”å„ç§æœªæ¥åœºæ™¯çš„éœ€æ±‚ï¼Œæœ‰åŠ©äºæ¨è¿›æ›´ç»¿è‰²ã€æ›´èŠ‚èƒ½çš„æ•°æ®ä¸­å¿ƒå‘å±•ã€‚

Abstract: With the rise of cloud computing and lightweight containers, Docker has emerged as a leading technology for rapid service deployment, with Kubernetes responsible for pod orchestration. However, for compute-intensive workloads-particularly web services executing containerized machine-learning training-the default Kubernetes scheduler does not always achieve optimal placement. To address this, we propose two custom, reinforcement-learning-based schedulers, SDQN and SDQN-n, both built on the Deep Q-Network (DQN) framework. In compute-intensive scenarios, these models outperform the default Kubernetes scheduler as well as Transformer-and LSTM-based alternatives, reducing average CPU utilization per cluster node by 10%, and by over 20% when using SDQN-n. Moreover, our results show that SDQN-n approach of consolidating pods onto fewer nodes further amplifies resource savings and helps advance greener, more energy-efficient data centers.Therefore, pod scheduling must employ different strategies tailored to each scenario in order to achieve better performance.Since the reinforcement-learning components of the SDQN and SDQN-n architectures proposed in this paper can be easily tuned by adjusting their parameters, they can accommodate the requirements of various future scenarios.

</details>


### [45] [Device Association and Resource Allocation for Hierarchical Split Federated Learning in Space-Air-Ground Integrated Network](https://arxiv.org/abs/2601.13817)
*Haitao Zhao,Xiaoyu Tang,Bo Xu,Jinlong Sun,Linghao Zhang*

Main category: cs.DC

TL;DR: æå‡ºåˆ†å±‚åˆ†å‰²è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡è”åˆä¼˜åŒ–è®¾å¤‡å…³è”ã€æ¨¡å‹åˆ†å‰²å±‚é€‰æ‹©å’Œèµ„æºåˆ†é…ï¼Œåœ¨SAGINä¸­å¹³è¡¡è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹ç²¾åº¦


<details>
  <summary>Details</summary>
Motivation: 6Gæ”¯æŒåœ¨ç©ºå¤©åœ°ä¸€ä½“åŒ–ç½‘ç»œä¸­éƒ¨ç½²è”é‚¦å­¦ä¹ ï¼Œä½†é¢ä¸´èµ„æºå—é™å’Œæ•°æ®åˆ†å¸ƒä¸å¹³è¡¡çš„æŒ‘æˆ˜ï¼Œéœ€è¦è®¾è®¡é«˜æ•ˆçš„å­¦ä¹ æ¡†æ¶

Method: æå‡ºåˆ†å±‚åˆ†å‰²è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œæ¨å¯¼æŸå¤±å‡½æ•°ä¸Šç•Œï¼Œå°†è”åˆä¼˜åŒ–é—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜ï¼Œè®¾è®¡åŸºäºæš´åŠ›æœç´¢åˆ†å‰²ç‚¹çš„è¿­ä»£ä¼˜åŒ–ç®—æ³•

Result: ä»¿çœŸç»“æœè¡¨æ˜ï¼Œæ‰€æç®—æ³•èƒ½æœ‰æ•ˆå¹³è¡¡SAGINä¸­è”é‚¦å­¦ä¹ çš„è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹ç²¾åº¦

Conclusion: HSFLæ¡†æ¶å’Œä¼˜åŒ–ç®—æ³•ä¸ºè§£å†³SAGINä¸­è”é‚¦å­¦ä¹ çš„èµ„æºçº¦æŸå’Œæ•°æ®ä¸å¹³è¡¡é—®é¢˜æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆ

Abstract: 6G facilitates deployment of Federated Learning (FL) in the Space-Air-Ground Integrated Network (SAGIN), yet FL confronts challenges such as resource constrained and unbalanced data distribution. To address these issues, this paper proposes a Hierarchical Split Federated Learning (HSFL) framework and derives its upper bound of loss function. To minimize the weighted sum of training loss and latency, we formulate a joint optimization problem that integrates device association, model split layer selection, and resource allocation. We decompose the original problem into several subproblems, where an iterative optimization algorithm for device association and resource allocation based on brute-force split point search is proposed. Simulation results demonstrate that the proposed algorithm can effectively balance training efficiency and model accuracy for FL in SAGIN.

</details>


### [46] [torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch](https://arxiv.org/abs/2601.13994)
*Mingyuan Chi*

Main category: cs.DC

TL;DR: TorchSLAæ˜¯ä¸€ä¸ªå¼€æºçš„PyTorchåº“ï¼Œæä¾›GPUåŠ é€Ÿã€å¯æ‰©å±•ä¸”å¯å¾®åˆ†çš„ç¨€ç–çº¿æ€§ä»£æ•°è®¡ç®—ï¼Œæ”¯æŒå¤§è§„æ¨¡ç§‘å­¦è®¡ç®—ã€‚


<details>
  <summary>Details</summary>
Motivation: å·¥ä¸šç§‘å­¦è®¡ç®—ä¸»è¦ä½¿ç”¨ç¨€ç–çŸ©é˜µè¡¨ç¤ºéç»“æ„åŒ–æ•°æ®ï¼ˆæœ‰é™å…ƒç½‘æ ¼ã€å›¾ã€ç‚¹äº‘ï¼‰ï¼Œä½†ç°æœ‰å·¥å…·ç¼ºä¹GPUåŠ é€Ÿã€å¤šGPUæ‰©å±•å’Œé«˜æ•ˆå¯å¾®åˆ†è®¡ç®—çš„èƒ½åŠ›ã€‚

Method: å¼€å‘TorchSLAåº“ï¼Œå®ç°ä¸‰ä¸ªå…³é”®æŠ€æœ¯ï¼š1) GPUåŠ é€Ÿçš„ç¨€ç–çº¿æ€§æ±‚è§£ã€éçº¿æ€§æ±‚è§£å’Œç‰¹å¾å€¼è®¡ç®—ï¼›2) é€šè¿‡åŸŸåˆ†è§£å’Œhaloäº¤æ¢å®ç°å¤šGPUæ‰©å±•ï¼›3) åŸºäºä¼´éšæ–¹æ³•çš„å¾®åˆ†ï¼Œå®ç°O(1)è®¡ç®—å›¾èŠ‚ç‚¹å’ŒO(nnz)å†…å­˜æ¶ˆè€—ã€‚

Result: åœ¨3ä¸ªGPUä¸Šå®ç°äº†4äº¿è‡ªç”±åº¦çº¿æ€§æ±‚è§£ï¼Œæ”¯æŒå¤šç§åç«¯ï¼ˆSciPyã€cuDSSã€PyTorch-nativeï¼‰ï¼Œå¹¶ä¸PyTorch autogradæ— ç¼é›†æˆï¼Œå®ç°ç«¯åˆ°ç«¯å¯å¾®åˆ†æ¨¡æ‹Ÿã€‚

Conclusion: TorchSLAä¸ºå¤§è§„æ¨¡ç§‘å­¦è®¡ç®—æä¾›äº†é«˜æ•ˆã€å¯æ‰©å±•ä¸”å¯å¾®åˆ†çš„ç¨€ç–çº¿æ€§ä»£æ•°è§£å†³æ–¹æ¡ˆï¼Œå¡«è¡¥äº†ç°æœ‰å·¥å…·çš„ç©ºç™½ã€‚

Abstract: Industrial scientific computing predominantly uses sparse matrices to represent unstructured data -- finite element meshes, graphs, point clouds. We present \torchsla{}, an open-source PyTorch library that enables GPU-accelerated, scalable, and differentiable sparse linear algebra. The library addresses three fundamental challenges: (1) GPU acceleration for sparse linear solves, nonlinear solves (Newton, Picard, Anderson), and eigenvalue computation; (2) Multi-GPU scaling via domain decomposition with halo exchange, reaching \textbf{400 million DOF linear solve on 3 GPUs}; and (3) Adjoint-based differentiation} achieving $\mathcal{O}(1)$ computational graph nodes (for autograd) and $\mathcal{O}(\text{nnz})$ memory -- independent of solver iterations. \torchsla{} supports multiple backends (SciPy, cuDSS, PyTorch-native) and seamlessly integrates with PyTorch autograd for end-to-end differentiable simulations. Code is available at https://github.com/walkerchi/torch-sla.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [47] [NuRedact: Non-Uniform eFPGA Architecture for Low-Overhead and Secure IP Redaction](https://arxiv.org/abs/2601.11770)
*Voktho Das,Kimia Azar,Hadi Kamali*

Main category: cs.AR

TL;DR: NuRedactæ˜¯ä¸€ä¸ªéå‡åŒ€eFPGAé‡åˆ æ¡†æ¶ï¼Œé€šè¿‡å®šåˆ¶åŒ–æ¶æ„å¹³è¡¡å®‰å…¨æ€§å’Œæ•ˆç‡ï¼Œç›¸æ¯”ä¼ ç»Ÿå‡åŒ€ç»“æ„å®ç°é«˜è¾¾9å€é¢ç§¯ç¼©å‡ï¼ŒåŒæ—¶ä¿æŒå¼ºéŸ§æ€§ã€‚


<details>
  <summary>Details</summary>
Motivation: ç°æœ‰åŸºäºå¯é‡æ„çš„é‡åˆ æŠ€æœ¯ï¼ˆå¦‚LUTå’ŒeFPGAæ–¹æ¡ˆï¼‰è™½ç„¶æé«˜äº†å®‰å…¨æ€§ï¼Œä½†å¸¦æ¥äº†å·¨å¤§å¼€é”€ï¼Œè¿™äº›å¼€é”€ä¸»è¦æ¥è‡ªä¸ºé˜»ç¢é€†å‘å·¥ç¨‹è€Œå¼•å…¥çš„äººå·¥å¤æ‚æ€§ï¼Œè€ŒéçœŸæ­£çš„åŠŸèƒ½å¯é‡æ„éœ€æ±‚ï¼Œå¯¼è‡´ç»“æ„åˆ©ç”¨ç‡ä½ä¸”å®‰å…¨æˆæœ¬è¿‡é«˜ã€‚

Method: åŸºäºOpenFPGAåŸºç¡€è®¾æ–½æ„å»ºçš„ä¸‰é˜¶æ®µæ–¹æ³•ï¼š1ï¼‰ç”Ÿæˆå…·æœ‰å¼•è„šæ˜ å°„ä¸è§„åˆ™æ€§çš„å®šåˆ¶ç»“æ„ï¼›2ï¼‰VPRçº§ä¿®æ”¹å®ç°åŸºäºPythonä¼˜åŒ–å™¨çš„éå‡åŒ€å¸ƒå±€ï¼›3ï¼‰ç›®æ ‡IPæ¨¡å—çš„é‡åˆ æ„ŸçŸ¥é‡é…ç½®å’Œæ˜ å°„ã€‚

Result: ç›¸æ¯”ä¼ ç»Ÿå‡åŒ€ç»“æ„å®ç°é«˜è¾¾9å€é¢ç§¯ç¼©å‡ï¼Œåœ¨ä¿æŒå¼ºéŸ§æ€§çš„åŒæ—¶ï¼Œè¾¾åˆ°ä¸LUTåŸºç”šè‡³æ™¶ä½“ç®¡çº§é‡åˆ æŠ€æœ¯ç«äº‰æ€§çš„æ•ˆç‡ã€‚åœ¨SATåŸºã€å¾ªç¯å’Œåºåˆ—å˜ä½“ç­‰å…ˆè¿›æ”»å‡»æ¨¡å‹ä¸‹è¡¨ç°å‡ºå¢å¼ºçš„éŸ§æ€§ï¼ŒåŒæ—¶ä¿æŒå®é™…è®¾è®¡å¼€é”€ã€‚

Conclusion: NuRedacté¦–æ¬¡å±•ç¤ºäº†é€šè¿‡æ¶æ„éå‡åŒ€æ€§å¹³è¡¡å®‰å…¨æ€§å’Œæ•ˆç‡çš„å®Œæ•´å®šåˆ¶eFPGAé‡åˆ æ¡†æ¶ï¼Œä¸ºé›†æˆç”µè·¯ä¾›åº”é“¾å®‰å…¨æä¾›äº†æ›´é«˜æ•ˆçš„é‡åˆ è§£å†³æ–¹æ¡ˆã€‚

Abstract: While logic locking has been extensively studied as a countermeasure against integrated circuit (IC) supply chain threats, recent research has shifted toward reconfigurable-based redaction techniques, e.g., LUT- and eFPGA-based schemes. While these approaches raise the bar against attacks, they incur substantial overhead, much of which arises not from genuine functional reconfigurability need, but from artificial complexity intended solely to frustrate reverse engineering (RE). As a result, fabrics are often underutilized, and security is achieved at disproportionate cost. This paper introduces NuRedact, the first full-custom eFPGA redaction framework that embraces architectural non-uniformity to balance security and efficiency. Built as an extension of the widely adopted OpenFPGA infrastructure, NuRedact introduces a three-stage methodology: (i) custom fabric generation with pin-mapping irregularity, (ii) VPR-level modifications to enable non-uniform placement guided by an automated Python-based optimizer, and (iii) redaction-aware reconfiguration and mapping of target IP modules. Experimental results show up to 9x area reduction compared to conventional uniform fabrics, achieving competitive efficiency with LUT-based and even transistor-level redaction techniques while retaining strong resilience. From a security perspective, NuRedact fabrics are evaluated against state-of-the-art attack models, including SAT-based, cyclic, and sequential variants, and show enhanced resilience while maintaining practical design overheads.

</details>


### [48] [Domain-specific Hardware Acceleration for Model Predictive Path Integral Control](https://arxiv.org/abs/2601.12089)
*Erwan Tanguy-Legac,Tommaso Belvedere,Gianluca Corsini,Marco Tognon,Marcello Traiola*

Main category: cs.AR

TL;DR: æå‡ºä¸€ç§ç”¨äºMPPIæ§åˆ¶çš„ç¡¬ä»¶åŠ é€Ÿå™¨ï¼Œç›¸æ¯”GPUå®ç°èƒ½æä¾›æ›´ç²¾ç¡®çš„è½¨è¿¹æ§åˆ¶


<details>
  <summary>Details</summary>
Motivation: æœºå™¨äººå®æ—¶æ§åˆ¶é¢ä¸´æŒ‘æˆ˜ï¼ŒMPCéš¾ä»¥åº”ç”¨äºéçº¿æ€§ç³»ç»Ÿï¼ŒMPPIè®¡ç®—è´Ÿè½½é‡ï¼ŒGPUå®ç°åŠŸè€—è¿‡é«˜ï¼Œè€ŒFPGAå®šåˆ¶è®¾è®¡èƒ½é™ä½èƒ½è€—ï¼Œä½†ç›®å‰å°šæ— MPPIä¸“ç”¨åŠ é€Ÿå™¨

Method: è®¾è®¡å¹¶æ¨¡æ‹Ÿæ‰§è¡ŒMPPIæ§åˆ¶çš„ç¡¬ä»¶åŠ é€Ÿå™¨ï¼Œé‡‡ç”¨å®šåˆ¶åŒ–æ¶æ„

Result: MPPIå®šåˆ¶åŠ é€Ÿå™¨ç›¸æ¯”åŸºäºGPUçš„MPPIå®ç°èƒ½å®ç°æ›´ç²¾ç¡®çš„è½¨è¿¹æ§åˆ¶

Conclusion: æå‡ºçš„MPPIç¡¬ä»¶åŠ é€Ÿå™¨åœ¨ä¿æŒä½èƒ½è€—çš„åŒæ—¶ï¼Œæé«˜äº†è½¨è¿¹æ§åˆ¶çš„ç²¾åº¦ï¼Œä¸ºæœºå™¨äººå®æ—¶æ§åˆ¶æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆ

Abstract: Accurately controlling a robotic system in real time is a challenging problem. To address this, the robotics community has adopted various algorithms, such as Model Predictive Control (MPC) and Model Predictive Path Integral (MPPI) control. The first is difficult to implement on non-linear systems such as unmanned aerial vehicles, whilst the second requires a heavy computational load. GPUs have been successfully used to accelerate MPPI implementations; however, their power consumption is often excessive for autonomous or unmanned targets, especially when battery-powered. On the other hand, custom designs, often implemented on FPGAs, have been proposed to accelerate robotic algorithms while consuming considerably less energy than their GPU (or CPU) implementation. However, no MPPI custom accelerator has been proposed so far. In this work, we present a hardware accelerator for MPPI control and simulate its execution. Results show that the MPPI custom accelerator allows more accurate trajectories than GPU-based MPPI implementations.

</details>


### [49] [Biological Intuition on Digital Hardware: An RTL Implementation of Poisson-Encoded SNNs for Static Image Classification](https://arxiv.org/abs/2601.12156)
*Debabrata Das,Yogeeth G. K.,Arnav Gupta*

Main category: cs.AR

TL;DR: è®¾è®¡äº†ä¸€ä¸ªåŸºäºSystemVerilogçš„ç²¾ç¡®å‘¨æœŸSNNæ ¸å¿ƒï¼Œé‡‡ç”¨LIFç¥ç»å…ƒæ¨¡å‹å’Œå®šç‚¹è¿ç®—ï¼Œé€šè¿‡åŠ¨æ€å‰ªæé™ä½åŠŸè€—ï¼Œåœ¨æ•°å­—åˆ†ç±»ä»»åŠ¡ä¸Šè¾¾åˆ°89%å‡†ç¡®ç‡


<details>
  <summary>Details</summary>
Motivation: è¾¹ç¼˜AIéƒ¨ç½²å—é™äºä¼ ç»ŸANNçš„é«˜åŠŸè€—å’Œå»¶è¿Ÿï¼Œè€Œç¥ç»å½¢æ€è®¡ç®—é€šè¿‡äº‹ä»¶é©±åŠ¨å¤„ç†æ¨¡æ‹Ÿç”Ÿç‰©æ•ˆç‡ï¼Œæä¾›äº†ä¸€ç§æœ‰å¸å¼•åŠ›çš„æ›¿ä»£æ–¹æ¡ˆ

Method: è®¾è®¡å¹¶å®ç°äº†ä¸€ä¸ªç²¾ç¡®å‘¨æœŸçš„ç¡¬ä»¶å¯¼å‘SNNæ ¸å¿ƒï¼Œä½¿ç”¨LIFç¥ç»å…ƒæ¨¡å‹é…åˆå®šç‚¹è¿ç®—å’Œä½çº§åŸè¯­ï¼ŒåŒ…å«ç‰‡ä¸Šæ³Šæ¾ç¼–ç å™¨å’ŒåŠ¨æ€å‰ªææœºåˆ¶

Result: åœ¨æ•°å­—åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œè¯¥è®¾è®¡åœ¨æœ‰é™æ—¶é—´æ­¥å†…å¿«é€Ÿæ”¶æ•›è¾¾åˆ°89%å‡†ç¡®ç‡ï¼ŒåŒæ—¶ç›¸æ¯”ä¼ ç»Ÿå¯†é›†æ¶æ„æ˜¾è‘—å‡å°‘äº†è®¡ç®—å¼€é”€

Conclusion: è¿™é¡¹å·¥ä½œä¸ºFPGAå’ŒASICå¹³å°ä¸Šå¯æ‰©å±•ã€é«˜èƒ½æ•ˆçš„ç¥ç»å½¢æ€ç¡¬ä»¶æä¾›äº†åŸºç¡€æ„å»ºæ¨¡å—

Abstract: The deployment of Artificial Intelligence on edge devices (TinyML) is often constrained by the high power consumption and latency associated with traditional Artificial Neural Networks (ANNs) and their reliance on intensive Matrix-Multiply (MAC) operations. Neuromorphic computing offers a compelling alternative by mimicking biological efficiency through event-driven processing. This paper presents the design and implementation of a cycle-accurate, hardware-oriented Spiking Neural Network (SNN) core implemented in SystemVerilog. Unlike conventional accelerators, this design utilizes a Leaky Integrate-and-Fire (LIF) neuron model powered by fixed-point arithmetic and bit-wise primitives (shifts and additions) to eliminate the need for complex floating-point hardware. The architecture features an on-chip Poisson encoder for stochastic spike generation and a novel active pruning mechanism that dynamically disables neurons post-classification to minimize dynamic power consumption. We demonstrate the hardware's efficacy through a fully connected layer implementation targeting digit classification. Simulation results indicate that the design achieves rapid convergence (89% accuracy) within limited timesteps while maintaining a significantly reduced computational footprint compared to traditional dense architectures. This work serves as a foundational building block for scalable, energy-efficient neuromorphic hardware on FPGA and ASIC platforms.

</details>


### [50] [CD-PIM: A High-Bandwidth and Compute-Efficient LPDDR5-Based PIM for Low-Batch LLM Acceleration on Edge-Device](https://arxiv.org/abs/2601.12298)
*Ye Lin,Chao Fang,Xiaoyong Song,Qi Wu,Anying Jiang,Yichuan Bai,Li Du*

Main category: cs.AR

TL;DR: CD-PIMæå‡ºäº†ä¸€ç§é’ˆå¯¹è¾¹ç¼˜ä½æ‰¹æ¬¡LLMéƒ¨ç½²çš„å†…å­˜è®¡ç®—æ¶æ„ï¼Œé€šè¿‡é«˜å¸¦å®½è®¡ç®—æ•ˆç‡æ¨¡å¼ã€ä½æ‰¹æ¬¡äº¤é”™æ¨¡å¼å’Œè®¡ç®—æ•ˆç‡å•å…ƒè®¾è®¡ï¼Œè§£å†³äº†ç°æœ‰PIMæ¶æ„çš„å¸¦å®½é™åˆ¶ã€ç»„ä»¶åˆ©ç”¨ç‡ä½å’Œè®¡ç®—èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚


<details>
  <summary>Details</summary>
Motivation: è¾¹ç¼˜éƒ¨ç½²ä½æ‰¹æ¬¡å¤§å‹è¯­è¨€æ¨¡å‹é¢ä¸´å†…å­˜å¸¦å®½ç“¶é¢ˆï¼Œç°æœ‰æ•°å­—å†…å­˜è®¡ç®—æ¶æ„å­˜åœ¨ä¸‰ä¸ªå…³é”®é™åˆ¶ï¼šå¸¦å®½æ”¹è¿›æœ‰é™ã€æ··åˆå·¥ä½œè´Ÿè½½ä¸­ç»„ä»¶åˆ©ç”¨ç‡ä½ã€è®¡ç®—å•å…ƒè®¡ç®—èƒ½åŠ›ä¸è¶³ã€‚

Method: 1. é«˜å¸¦å®½è®¡ç®—æ•ˆç‡æ¨¡å¼ï¼šé€šè¿‡åˆ†æ®µå…¨å±€ä½çº¿å°†æ¯ä¸ªå­˜å‚¨ä½“åˆ†ä¸ºå››ä¸ªä¼ªå­˜å‚¨ä½“ï¼›2. ä½æ‰¹æ¬¡äº¤é”™æ¨¡å¼ï¼šé‡å GEMVå’ŒGEMMæ“ä½œï¼›3. è®¡ç®—æ•ˆç‡å•å…ƒï¼šä»¥æµæ°´çº¿æ–¹å¼ä¸²è¡Œè¾“å…¥æƒé‡æ•°æ®ï¼›4. åˆ—å‘æ˜ å°„é”®ç¼“å­˜çŸ©é˜µå’Œè¡Œå‘æ˜ å°„å€¼ç¼“å­˜çŸ©é˜µã€‚

Result: åœ¨å•æ‰¹æ¬¡HBCEMæ¨¡å¼ä¸‹ï¼Œç›¸æ¯”GPUåŸºçº¿å’Œæœ€å…ˆè¿›PIMè®¾è®¡ï¼Œåˆ†åˆ«å®ç°11.42å€å’Œ4.25å€å¹³å‡åŠ é€Ÿï¼›åœ¨ä½æ‰¹æ¬¡ä¸‹ï¼ŒLBIMç›¸æ¯”HBCEMå®ç°1.12å€å¹³å‡åŠ é€Ÿã€‚

Conclusion: CD-PIMé€šè¿‡åˆ›æ–°çš„æ¶æ„è®¾è®¡æœ‰æ•ˆè§£å†³äº†è¾¹ç¼˜ä½æ‰¹æ¬¡LLMéƒ¨ç½²ä¸­çš„å†…å­˜å¸¦å®½ç“¶é¢ˆå’Œè®¡ç®—æ•ˆç‡é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ€§èƒ½ã€‚

Abstract: Edge deployment of low-batch large language models (LLMs) faces critical memory bandwidth bottlenecks when executing memory-intensive general matrix-vector multiplications (GEMV) operations. While digital processing-in-memory (PIM) architectures promise to accelerate GEMV operations, existing PIM-equipped edge devices still suffer from three key limitations: limited bandwidth improvement, component under-utilization in mixed workloads, and low compute capacity of computing units (CUs). In this paper, we propose CD-PIM to address these challenges through three key innovations. First, we introduce a high-bandwidth compute-efficient mode (HBCEM) that enhances bandwidth by dividing each bank into four pseudo-banks through segmented global bitlines. Second, we propose a low-batch interleaving mode (LBIM) to improve component utilization by overlapping GEMV operations with GEMM operations. Third, we design a compute-efficient CU that performs enhanced GEMV operations in a pipelined manner by serially feeding weight data into the computing core. Forth, we adopt a column-wise mapping for the key-cache matrix and row-wise mapping for the value-cache matrix, which fully utilizes CU resources. Our evaluation shows that compared to a GPU-only baseline and state-of-the-art PIM designs, our CD-PIM achieves 11.42x and 4.25x speedup on average within a single batch in HBCEM mode, respectively. Moreover, for low-batch sizes, the CD-PIM achieves an average speedup of 1.12x in LBIM compared to HBCEM.

</details>


### [51] [Best Practices for Large Load Interconnections: A North American Perspective on Data Centers](https://arxiv.org/abs/2601.12686)
*Rafi Zahedi,Amin Zamani,Rahul Anilkumar*

Main category: cs.AR

TL;DR: è¯¥è®ºæ–‡ç»¼è¿°äº†åŒ—ç¾å¤§å‹è´Ÿè½½ï¼ˆæ•°æ®ä¸­å¿ƒã€åŠ å¯†è´§å¸æŒ–çŸ¿ã€åˆ¶æ°¢è®¾æ–½ã€é‡å‹å……ç”µç«™ï¼‰å¹¶ç½‘çš„æœ€ä½³å®è·µï¼Œåˆ†æäº†æŠ€æœ¯æŒ‘æˆ˜å¹¶æå‡ºäº†å®ç”¨æŒ‡å¯¼å»ºè®®ã€‚


<details>
  <summary>Details</summary>
Motivation: åŒ—ç¾å¤§å‹è´Ÿè½½å¿«é€Ÿå¢é•¿ï¼Œç‰¹åˆ«æ˜¯AIé©±åŠ¨ä¸‹çš„æ•°æ®ä¸­å¿ƒæ‰©å¼ ï¼Œå¸¦æ¥äº†å¹¶ç½‘æŒ‘æˆ˜ã€‚è¿™äº›è´Ÿè½½çš„è§„æ¨¡ã€å·¥ä½œå‘¨æœŸå’Œå˜æµå™¨ä¸»å¯¼çš„æ¥å£å¯¹è¾“ç”µäº’è”æå‡ºäº†æ–°çš„æ‰°åŠ¨è¡Œä¸ºã€ç¨³æ€æ€§èƒ½å’Œè¿è¡Œå¯è§æ€§æ–¹é¢çš„æŒ‘æˆ˜ã€‚

Method: é€šè¿‡åˆ†æåŒ—ç¾å…¬ç”¨äº‹ä¸šå…¬å¸å’Œç³»ç»Ÿè¿è¥å•†çš„æŒ‡å—ï¼Œç»“åˆæ‰‹å†Œåˆ†æã€è·¨å…¬ç”¨äº‹ä¸šæ¯”è¾ƒå’Œæ¬§æ´²å‘å±•æ–¹å‘å±•æœ›ï¼Œç»¼åˆå½¢æˆä¸€å¥—è¿è´¯çš„æŠ€æœ¯è¦æ±‚æ¡†æ¶ã€‚

Result: è¯†åˆ«äº†åœ¨ç”µèƒ½è´¨é‡ã€é¥æµ‹ã€è°ƒè¯•æµ‹è¯•å’Œä¿æŠ¤åè°ƒæ–¹é¢çš„æŠ€æœ¯è¦æ±‚ï¼ŒåŒæ—¶æŒ‡å‡ºäº†åœ¨ç©¿è¶Šè§„èŒƒã€è´Ÿè½½å˜åŒ–ç®¡ç†å’Œæ‰°åŠ¨åæ¢å¤ç›®æ ‡æ–¹é¢çš„å·®è·ã€‚

Conclusion: åŸºäºç ”ç©¶å‘ç°ï¼Œä¸ºå¼€å‘å•†å’Œå…¬ç”¨äº‹ä¸šå…¬å¸æå‡ºäº†å®ç”¨çš„æŒ‡å¯¼å»ºè®®ï¼Œä»¥åº”å¯¹å¤§å‹è´Ÿè½½å¹¶ç½‘çš„æŠ€æœ¯æŒ‘æˆ˜ã€‚

Abstract: Large loads are expanding rapidly across North America, led by data centers, cryptocurrency mining, hydrogen production facilities, and heavy-duty charging stations. Each class presents distinct electrical characteristics, but data centers are drawing particular attention as AI deployment drives unprecedented capacity growth. Their scale, duty cycles, and converter-dominated interfaces introduce new challenges for transmission interconnections, especially regarding disturbance behavior, steady-state performance, and operational visibility. This paper reviews best practices for large-load interconnections across North America, synthesizing utility and system operator guidelines into a coherent set of technical requirements. The approach combines handbook and manual analysis with cross-utility comparisons and an outlook on European directions. The review highlights requirements on power quality, telemetry, commissioning tests, and protection coordination, while noting gaps in ride-through specifications, load-variation management, and post-disturbance recovery targets. Building on these findings, the paper proposes practical guidance for developers and utilities.

</details>


### [52] [PRIMAL: Processing-In-Memory Based Low-Rank Adaptation for LLM Inference Accelerator](https://arxiv.org/abs/2601.13628)
*Yue Jiet Chong,Yimin Wang,Zhen Wu,Xuanyao Fong*

Main category: cs.AR

TL;DR: PRIMALæ˜¯ä¸€ä¸ªåŸºäºå­˜å†…è®¡ç®—ï¼ˆPIMï¼‰çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†åŠ é€Ÿå™¨ï¼Œé›†æˆäº†ä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯ï¼Œé€šè¿‡å¼‚æ„PIMå¤„ç†å•å…ƒã€2Dç½‘æ ¼äº’è¿ç½‘ç»œå’Œåˆ›æ–°çš„SRAMé‡ç¼–ç¨‹ä¸ç”µæºé—¨æ§æ–¹æ¡ˆï¼Œå®ç°äº†é«˜æ•ˆèƒ½æ¨ç†ã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€å¤§è¯­è¨€æ¨¡å‹è§„æ¨¡ä¸æ–­æ‰©å¤§ï¼Œä¼ ç»ŸGPUæ¶æ„åœ¨æ¨ç†æ•ˆç‡å’Œèƒ½è€—æ–¹é¢é¢ä¸´æŒ‘æˆ˜ã€‚å­˜å†…è®¡ç®—ï¼ˆPIMï¼‰æŠ€æœ¯å¯ä»¥å‡å°‘æ•°æ®ç§»åŠ¨å¼€é”€ï¼Œè€Œä½ç§©è‡ªé€‚åº”ï¼ˆLoRAï¼‰æŠ€æœ¯èƒ½å¤Ÿé«˜æ•ˆåœ°å¾®è°ƒæ¨¡å‹ã€‚PRIMALæ—¨åœ¨ç»“åˆPIMå’ŒLoRAçš„ä¼˜åŠ¿ï¼Œæ„å»ºä¸€ä¸ªé«˜æ•ˆèƒ½çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†åŠ é€Ÿå™¨ã€‚

Method: PRIMALé‡‡ç”¨å¼‚æ„PIMå¤„ç†å•å…ƒï¼ˆPEsï¼‰ï¼Œé€šè¿‡2Dç½‘æ ¼äº’è¿ç½‘ç»œï¼ˆIPCNï¼‰è¿æ¥ã€‚åˆ›æ–°æ€§åœ°æå‡ºäº†SRAMé‡ç¼–ç¨‹ä¸ç”µæºé—¨æ§ï¼ˆSRPGï¼‰æ–¹æ¡ˆï¼Œæ”¯æŒæµæ°´çº¿åŒ–çš„LoRAæ›´æ–°å’Œäºšçº¿æ€§åŠŸè€—æ‰©å±•ã€‚é€šè¿‡ä¼˜åŒ–çš„ç©ºé—´æ˜ å°„å’Œæ•°æ®æµç¼–æ’æ¥æœ€å°åŒ–é€šä¿¡å¼€é”€ã€‚

Result: åœ¨Llama-13Bæ¨¡å‹ä¸Šï¼Œä½¿ç”¨LoRAç§©ä¸º8ï¼ˆQ,Vï¼‰æ—¶ï¼ŒPRIMALç›¸æ¯”NVIDIA H100å®ç°äº†1.5å€çš„ååé‡å’Œ25å€çš„èƒ½æ•ˆæå‡ã€‚

Conclusion: PRIMALæˆåŠŸå±•ç¤ºäº†PIMæ¶æ„ä¸LoRAæŠ€æœ¯ç»“åˆåœ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†åŠ é€Ÿæ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œé€šè¿‡åˆ›æ–°çš„ç¡¬ä»¶è®¾è®¡å’Œä¼˜åŒ–ç­–ç•¥ï¼Œæ˜¾è‘—æå‡äº†æ¨ç†æ€§èƒ½å’Œèƒ½æ•ˆã€‚

Abstract: This paper presents PRIMAL, a processing-in-memory (PIM) based large language model (LLM) inference accelerator with low-rank adaptation (LoRA). PRIMAL integrates heterogeneous PIM processing elements (PEs), interconnected by 2D-mesh inter-PE computational network (IPCN). A novel SRAM reprogramming and power gating (SRPG) scheme enables pipelined LoRA updates and sub-linear power scaling by overlapping reconfiguration with computation and gating idle resources. PRIMAL employs optimized spatial mapping and dataflow orchestration to minimize communication overhead, and achieves $1.5\times$ throughput and $25\times$ energy efficiency over NVIDIA H100 with LoRA rank 8 (Q,V) on Llama-13B.

</details>


### [53] [The Non-Predictability of Mispredicted Branches using Timing Information](https://arxiv.org/abs/2601.13804)
*Ioannis Constantinou,Arthur Perais,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: è®ºæ–‡ç ”ç©¶åˆ©ç”¨å¾®æ¶æ„æ—¶åºä¿¡æ¯æ”¹è¿›åˆ†æ”¯é¢„æµ‹ï¼Œæå‡ºSBRæ–¹æ³•ï¼Œä½†å®éªŒæ˜¾ç¤ºå¯¹æ•´ä½“æ€§èƒ½æå‡æœ‰é™ï¼Œä»…å¯¹ç‰¹å®šéš¾é¢„æµ‹åˆ†æ”¯æœ‰æ•ˆã€‚


<details>
  <summary>Details</summary>
Motivation: åˆ†æ”¯é¢„æµ‹é”™è¯¯æ˜¯ç°ä»£å¤„ç†å™¨æ€§èƒ½ä¸‹é™å’Œèƒ½è€—æµªè´¹çš„ä¸»è¦åŸå› ã€‚ç°æœ‰é¢„æµ‹å™¨è™½ç„¶è¡¨ç°è‰¯å¥½ï¼Œä½†å¯¹æŸäº›éš¾é¢„æµ‹åˆ†æ”¯ä»å­˜åœ¨é«˜è¯¯é¢„æµ‹ç‡ã€‚ä½œè€…æ¢ç´¢æ˜¯å¦å¯ä»¥é€šè¿‡ç»“åˆå¾®æ¶æ„æ—¶åºä¿¡æ¯æ¥æå‡é¢„æµ‹å‡†ç¡®æ€§ã€‚

Method: æå‡ºSpeculative Branch Resolution (SBR)æ–¹æ³•ï¼šåœ¨åˆ†æ”¯æŒ‡ä»¤è¿›å…¥é‡æ’åºç¼“å†²åŒº(ROB)åNä¸ªå‘¨æœŸï¼Œæ”¶é›†å„ç§æ—¶åºä¿¡æ¯ï¼ˆåŒ…æ‹¬ROBä¸­è¾ƒè€åˆ†æ”¯å’Œå·²æäº¤åˆ†æ”¯çš„è§£å†³å‘¨æœŸï¼Œä»¥åŠç›¸å¯¹äºå½“å‰é¢„æµ‹åˆ†æ”¯çš„è¾ƒå¹´è½»åˆ†æ”¯ä¿¡æ¯ï¼‰è¿›è¡Œé‡æ–°é¢„æµ‹ã€‚ä½¿ç”¨gem5æ¨¡æ‹Ÿå™¨å®ç°ï¼Œå¹¶åŸºäºTAGE-Likeé¢„æµ‹å™¨è¿›è¡Œæé™ç ”ç©¶ã€‚

Result: å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨çš„åˆ†é…åæ—¶åºä¿¡æ¯æœªèƒ½è¶…è¶Šæ— é™åˆ¶çš„TAGE-SCé¢„æµ‹å™¨çš„æ€§èƒ½ã€‚ä½†å‘ç°ä¸¤ä¸ªéš¾é¢„æµ‹åˆ†æ”¯ç¡®å®ä»æ—¶åºä¿¡æ¯ä¸­è·ç›Šï¼Œå¹¶æ·±å…¥åˆ†æäº†å…¶ä¸­ä¸€ä¸ªæ¡ˆä¾‹ä»¥ç†è§£åŸå› ã€‚

Conclusion: ç‰¹å®šå¾®æ¶æ„ä¿¡æ¯å¯èƒ½æœ‰åŠ©äºæå‡ç‰¹å®šéš¾é¢„æµ‹åˆ†æ”¯çš„å‡†ç¡®æ€§ï¼Œåç«¯è¦†ç›–é¢„æµ‹å¯èƒ½å¸¦æ¥æ€§èƒ½æ”¶ç›Šï¼Œä½†éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶ç¡®å®šæœ‰æ•ˆçš„ä¿¡æ¯å‘é‡ã€‚

Abstract: Branch misprediction latency is one of the most important contributors to performance degradation and wasted energy consumption in a modern core. State-of-the-art predictors generally perform very well but occasionally suffer from high Misprediction Per Kilo Instruction due to hard-to-predict branches. In this work, we investigate if predicting branches using microarchitectural information, in addition to traditional branch history, can improve prediction accuracy. Our approach considers branch timing information (resolution cycle) both for older branches in the Reorder Buffer (ROB) and recently committed, and for younger branches relative to the branch we re-predict. We propose Speculative Branch Resolution (SBR) in which, N cycles after a branch allocates in the ROB, various timing information is collected and used to re-predict. Using the gem5 simulator we implement and perform a limit-study of SBR using a TAGE-Like predictor. Our experiments show that the post-alloc timing information we used was not able to yield performance gains over an unbounded TAGE-SC. However, we find two hard to predict branches where timing information did provide an advantage and thoroughly analysed one of them to understand why. This finding suggests that predictors may benefit from specific microarchitectural information to increase accuracy on specific hard to predict branches and that overriding predictions in the backend may yet yield performance benefits, but that further research is needed to determine such information vectors.

</details>


### [54] [From RTL to Prompt Coding: Empowering the Next Generation of Chip Designers through LLMs](https://arxiv.org/abs/2601.13815)
*Lukas Krupp,Matthew Venn,Norbert Wehn*

Main category: cs.AR

TL;DR: é¦–ä¸ªåŸºäºLLMçš„èŠ¯ç‰‡è®¾è®¡æ•™è‚²å¹³å°ï¼Œé›†æˆèŠå¤©ä»£ç†åˆ°æµè§ˆå™¨å·¥ä½œæµï¼Œè®©æ— ç»éªŒçš„é«˜ä¸­ç”Ÿåœ¨90åˆ†é’Ÿå†…å®Œæˆå¯æµç‰‡çš„VGAèŠ¯ç‰‡è®¾è®¡


<details>
  <summary>Details</summary>
Motivation: é™ä½èŠ¯ç‰‡è®¾è®¡å…¥é—¨é—¨æ§›ï¼Œè®©åˆå­¦è€…æ— éœ€é¢å¯¹å¤æ‚æŠ€æœ¯ç»†èŠ‚ï¼ŒåŒæ—¶æä¾›è¦†ç›–å‰ç«¯å’Œåç«¯è®¾è®¡çš„å…¨é¢æ•™è‚²æ”¯æŒ

Method: å°†LLMèŠå¤©ä»£ç†é›†æˆåˆ°åŸºäºTiny Tapeoutç”Ÿæ€ç³»ç»Ÿçš„æµè§ˆå™¨å·¥ä½œæµä¸­ï¼Œå¼•å¯¼ç”¨æˆ·ä»è®¾è®¡æƒ³æ³•åˆ°RTLä»£ç ç”Ÿæˆå†åˆ°å¯æµç‰‡èŠ¯ç‰‡

Result: 18åé«˜ä¸­ç”Ÿåœ¨æ²¡æœ‰èŠ¯ç‰‡è®¾è®¡ç»éªŒçš„æƒ…å†µä¸‹ï¼Œ90åˆ†é’Ÿå†…å¼€å‘å‡º8ä¸ªåŠŸèƒ½æ­£å¸¸çš„130nmå·¥è‰ºVGAèŠ¯ç‰‡è®¾è®¡ï¼Œæ‰€æœ‰ç»„éƒ½æˆåŠŸå®ç°äº†å¯æµç‰‡é¡¹ç›®

Conclusion: LLMè¾…åŠ©çš„èŠ¯ç‰‡è®¾è®¡å…·æœ‰å¯è¡Œæ€§å’Œæ•™è‚²å½±å“åŠ›ï¼Œèƒ½å¤Ÿå¸å¼•å’Œæ¿€åŠ±æ—©æœŸå­¦ä¹ è€…ï¼Œæ˜¾è‘—æ‰©å¤§è¯¥é¢†åŸŸçš„å—ä¼—ç¾¤ä½“

Abstract: This paper presents an LLM-based learning platform for chip design education, aiming to make chip design accessible to beginners without overwhelming them with technical complexity. It represents the first educational platform that assists learners holistically across both frontend and backend design. The proposed approach integrates an LLM-based chat agent into a browser-based workflow built upon the Tiny Tapeout ecosystem. The workflow guides users from an initial design idea through RTL code generation to a tapeout-ready chip. To evaluate the concept, a case study was conducted with 18 high-school students. Within a 90-minute session they developed eight functional VGA chip designs in a 130 nm technology. Despite having no prior experience in chip design, all groups successfully implemented tapeout-ready projects. The results demonstrate the feasibility and educational impact of LLM-assisted chip design, highlighting its potential to attract and inspire early learners and significantly broaden the target audience for the field.

</details>


### [55] ['1'-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators](https://arxiv.org/abs/2601.14087)
*Ruichi Han,Yizhi Chen,Tong Lei,Jordi Altayo Gonzalez,Ahmed Hemani*

Main category: cs.AR

TL;DR: æå‡ºç¡¬ä»¶å®ç°çš„å…æ¯”è¾ƒæ’åºå•å…ƒï¼Œé€šè¿‡è¿‘ä¼¼è®¡ç®—å°†äººå£è®¡æ•°åˆ†ç»„åˆ°ç²—ç²’åº¦æ¡¶ä¸­ï¼Œåœ¨ä¿æŒæ•°æ®é‡æ’åºçš„é“¾è·¯åŠŸè€—ä¼˜åŠ¿çš„åŒæ—¶å‡å°‘ç¡¬ä»¶é¢ç§¯


<details>
  <summary>Details</summary>
Motivation: äº’è¿åŠŸè€—æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œåŠ é€Ÿå™¨çš„ä¸»è¦ç“¶é¢ˆï¼Œè™½ç„¶åŸºäº'1'ä½è®¡æ•°çš„æ•°æ®æ’åºå¯ä»¥å‡å°‘å¼€å…³æ´»åŠ¨ä»è€Œé™ä½åŠŸè€—ï¼Œä½†å®ç”¨çš„ç¡¬ä»¶æ’åºå®ç°ä»ç„¶ç¼ºä¹æ¢ç´¢

Method: è®¾è®¡é’ˆå¯¹å·ç§¯ç¥ç»ç½‘ç»œä¼˜åŒ–çš„å…æ¯”è¾ƒæ’åºå•å…ƒï¼Œåˆ©ç”¨è¿‘ä¼¼è®¡ç®—å°†äººå£è®¡æ•°åˆ†ç»„åˆ°ç²—ç²’åº¦æ¡¶ä¸­ï¼Œå®ç°ç¡¬ä»¶é¢ç§¯å‡å°‘åŒæ—¶ä¿æŒæ•°æ®é‡æ’åºçš„é“¾è·¯åŠŸè€—ä¼˜åŠ¿

Result: è¿‘ä¼¼æ’åºå•å…ƒå®ç°é«˜è¾¾35.4%çš„é¢ç§¯å‡å°‘ï¼ŒåŒæ—¶ä¿æŒ19.50%çš„BTå‡å°‘ï¼Œä¸ç²¾ç¡®å®ç°çš„20.42%ç›¸æ¯”æ€§èƒ½ç›¸è¿‘

Conclusion: æå‡ºçš„è¿‘ä¼¼æ’åºå•å…ƒåœ¨ä¿æŒé“¾è·¯åŠŸè€—ä¼˜åŠ¿çš„åŒæ—¶æ˜¾è‘—å‡å°‘ç¡¬ä»¶é¢ç§¯ï¼Œä¸ºDNNåŠ é€Ÿå™¨ä¸­çš„äº’è¿åŠŸè€—é—®é¢˜æä¾›äº†å®ç”¨çš„ç¡¬ä»¶è§£å†³æ–¹æ¡ˆ

Abstract: Interconnect power consumption remains a bottleneck in Deep Neural Network (DNN) accelerators. While ordering data based on '1'-bit counts can mitigate this via reduced switching activity, practical hardware sorting implementations remain underexplored. This work proposes the hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNN). By leveraging approximate computing to group population counts into coarse-grained buckets, our design achieves hardware area reductions while preserving the link power benefits of data reordering. Our approximate sorting unit achieves up to 35.4% area reduction while maintaining 19.50\% BT reduction compared to 20.42% of precise implementation.

</details>


### [56] [CREATE: Cross-Layer Resilience Characterization and Optimization for Efficient yet Reliable Embodied AI Systems](https://arxiv.org/abs/2601.14140)
*Tong Xie,Yijiahao Qi,Jinqi Wen,Zishen Wan,Yanchi Dong,Zihao Wang,Shaofei Cai,Yitao Liang,Tianyu Jia,Yuan Wang,Runsheng Wang,Meng Li*

Main category: cs.AR

TL;DR: CREATEæå‡ºäº†ä¸€ç§å¼‚æ„å¼¹æ€§è®¾è®¡åŸåˆ™ï¼Œé€šè¿‡ç”µè·¯å±‚ã€æ¨¡å‹å±‚å’Œåº”ç”¨å±‚çš„ååŒä¼˜åŒ–ï¼Œåœ¨ä¿æŒä»»åŠ¡è´¨é‡çš„åŒæ—¶æ˜¾è‘—é™ä½å…·èº«AIç³»ç»Ÿçš„èƒ½è€—


<details>
  <summary>Details</summary>
Motivation: éƒ¨ç½²å…·èº«AIç³»ç»Ÿé¢ä¸´é«˜è®¡ç®—éœ€æ±‚å’Œèƒ½è€—æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç”µæ± ä¾›ç”µè®¾å¤‡ã€‚è™½ç„¶é™ä½å·¥ä½œç”µå‹å¯ä»¥æé«˜èƒ½æ•ˆï¼Œä½†ä¼šå¼•å…¥æ¯”ç‰¹é”™è¯¯å¯¼è‡´ä»»åŠ¡å¤±è´¥ï¼Œéœ€è¦åœ¨èƒ½æ•ˆå’Œå¯é æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡

Method: 1) ç”µè·¯å±‚ï¼šå¼‚å¸¸æ£€æµ‹å’Œæ¸…é™¤æœºåˆ¶æ¶ˆé™¤å¼‚å¸¸é”™è¯¯ï¼›2) æ¨¡å‹å±‚ï¼šæƒé‡æ—‹è½¬å¢å¼ºè§„åˆ’ç®—æ³•æé«˜LLMè§„åˆ’å™¨çš„å®¹é”™èƒ½åŠ›ï¼›3) åº”ç”¨å±‚ï¼šè‡ªä¸»é€‚åº”æ€§ç”µå‹ç¼©æ”¾åŠ¨æ€è°ƒæ•´æ§åˆ¶å™¨å·¥ä½œç”µå‹ï¼›4) ååŒè®¾è®¡ç”µå‹ç¼©æ”¾ç”µè·¯æ”¯æŒåœ¨çº¿ç”µå‹è°ƒæ•´

Result: CREATEåœ¨ä¸å½±å“ä»»åŠ¡è´¨é‡çš„æƒ…å†µä¸‹ï¼Œç›¸æ¯”æ ‡ç§°ç”µå‹åŸºçº¿å¹³å‡èŠ‚çœ40.6%è®¡ç®—èƒ½è€—ï¼Œç›¸æ¯”ç°æœ‰æŠ€æœ¯èŠ‚çœ35.0%ã€‚èŠ¯ç‰‡çº§èƒ½è€—èŠ‚çœ29.5%-37.3%ï¼Œç”µæ± å¯¿å‘½æå‡çº¦15%-30%

Conclusion: CREATEé€šè¿‡åˆ©ç”¨ä¸åŒå±‚çš„å¼‚æ„å¼¹æ€§è¿›è¡ŒååŒèƒ½é‡-å¯é æ€§ä¼˜åŒ–ï¼Œä¸ºå…·èº«AIç³»ç»Ÿæä¾›äº†æœ‰æ•ˆçš„èƒ½æ•ˆæå‡æ–¹æ¡ˆï¼Œæ˜¾è‘—å»¶é•¿äº†ç”µæ± ä¾›ç”µè®¾å¤‡çš„è¿è¡Œæ—¶é—´

Abstract: Embodied Artificial Intelligence (AI) has recently attracted significant attention as it bridges AI with the physical world. Modern embodied AI systems often combine a Large Language Model (LLM)-based planner for high-level task planning and a reinforcement learning (RL)-based controller for low-level action generation, enabling embodied agents to tackle complex tasks in real-world environments. However, deploying embodied agents remains challenging due to their high computation requirements, especially for battery-powered local devices. Although techniques like lowering operating voltage can improve energy efficiency, they can introduce bit errors and result in task failures. In this work, we propose CREATE, a general design principle that leverages heterogeneous resilience at different layers for synergistic energy-reliability co-optimization. For the first time, we conduct a comprehensive error injection study on modern embodied AI systems and observe an inherent but heterogeneous fault tolerance. Building upon these insights, we develop an anomaly detection and clearance mechanism at the circuit level to eliminate outlier errors. At the model level, we propose a weight-rotation-enhanced planning algorithm to improve the fault tolerance of the LLM-based planner. Furthermore, we introduce an application-level technique, autonomy-adaptive voltage scaling, to dynamically adjust the operating voltage of the controllers. The voltage scaling circuit is co-designed to enable online voltage adjustment. Extensive experiments demonstrate that without compromising task quality, CREATE achieves 40.6% computational energy savings on average over nominal-voltage baselines and 35.0% over prior-art techniques. This further leads to 29.5% to 37.3% chip-level energy savings and approximately a 15% to 30% improvement in battery life.

</details>


### [57] [The Quest for Reliable AI Accelerators: Cross-Layer Evaluation and Design Optimization](https://arxiv.org/abs/2601.14148)
*Meng Li,Tong Xie,Zuodong Zhang,Runsheng Wang*

Main category: cs.AR

TL;DR: æœ¬æ–‡æå‡ºè·¨å±‚å¯é æ€§æ„ŸçŸ¥AIåŠ é€Ÿå™¨è®¾è®¡æ–¹æ³•ï¼Œè§£å†³çº³ç±³CMOSå·¥è‰ºä¸‹è€åŒ–æ•ˆåº”å’Œå·¥è‰ºå˜å¼‚å¸¦æ¥çš„å¯é æ€§æŒ‘æˆ˜ï¼Œé€šè¿‡åŠ¨æ€æ—¶åºåˆ†æã€æ•°æ®æµä¼˜åŒ–å’ŒLLMæ¶æ„è®¾è®¡å®ç°å¯é é«˜æ•ˆAIåŠ é€Ÿã€‚


<details>
  <summary>Details</summary>
Motivation: éšç€CMOSæŠ€æœ¯è¿›å…¥çº³ç±³å°ºåº¦ï¼Œè€åŒ–æ•ˆåº”å’Œå·¥è‰ºå˜å¼‚æ—¥ç›Šæ˜¾è‘—ï¼Œå¯¹AIåŠ é€Ÿå™¨çš„å¯é æ€§æ„æˆé‡å¤§æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„åŸºäºä¿æŠ¤å¸¦çš„è®¾è®¡æ–¹æ³•ä¾èµ–æ‚²è§‚æ—¶åºè£•åº¦ï¼Œç‰ºç‰²äº†å¤§é‡æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ï¼Œæ— æ³•æ»¡è¶³é«˜æ€§èƒ½AIè®¡ç®—éœ€æ±‚ã€‚å½“å‰å¯é æ€§æ„ŸçŸ¥AIåŠ é€Ÿå™¨è®¾è®¡é¢ä¸´ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ï¼šç¼ºä¹ç³»ç»Ÿæ€§çš„è·¨å±‚åˆ†æå·¥å…·æ¥æ•æ‰å™¨ä»¶ã€ç”µè·¯ã€æ¶æ„å’Œåº”ç”¨å±‚ä¹‹é—´çš„è€¦åˆå¯é æ€§æ•ˆåº”ï¼›ä»¥åŠä¼ ç»Ÿå¯é æ€§ä¼˜åŒ–ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´çš„æ ¹æœ¬æ€§æƒè¡¡ã€‚

Method: æœ¬æ–‡ç³»ç»Ÿæ€§åœ°æå‡ºäº†ä¸€ç³»åˆ—å¯é æ€§æ„ŸçŸ¥åŠ é€Ÿå™¨è®¾è®¡æ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼š(1) è€åŒ–å’Œå˜å¼‚æ„ŸçŸ¥çš„åŠ¨æ€æ—¶åºåˆ†æå™¨ï¼›(2) ä½¿ç”¨å…³é”®è¾“å…¥æ¨¡å¼å‡å°‘çš„åŠ é€Ÿå™¨æ•°æ®æµä¼˜åŒ–ï¼›(3) é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å¼¹æ€§ç‰¹å¾åˆ†æå’Œæ–°å‹æ¶æ„è®¾è®¡ã€‚é€šè¿‡ç´§å¯†é›†æˆè·¨å±‚å¯é æ€§å»ºæ¨¡å’ŒAIå·¥ä½œè´Ÿè½½ç‰¹æ€§ï¼Œè¿™äº›ååŒä¼˜åŒ–æ–¹æ³•æœ‰æ•ˆå®ç°äº†å¯é é«˜æ•ˆçš„AIåŠ é€Ÿã€‚

Result: é€šè¿‡æå‡ºçš„è·¨å±‚ååŒä¼˜åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³çº³ç±³CMOSå·¥è‰ºä¸‹çš„å¯é æ€§æŒ‘æˆ˜ï¼Œåœ¨ä¿æŒé«˜æ€§èƒ½çš„åŒæ—¶æå‡AIåŠ é€Ÿå™¨çš„å¯é æ€§ï¼Œé¿å…äº†ä¼ ç»Ÿä¿æŠ¤å¸¦è®¾è®¡æ–¹æ³•çš„æ€§èƒ½æŸå¤±ã€‚

Conclusion: æœ¬æ–‡æå‡ºçš„å¯é æ€§æ„ŸçŸ¥AIåŠ é€Ÿå™¨è®¾è®¡æ–¹æ³•é€šè¿‡è·¨å±‚ååŒä¼˜åŒ–ï¼ŒæˆåŠŸè§£å†³äº†çº³ç±³CMOSå·¥è‰ºä¸‹çš„å¯é æ€§æŒ‘æˆ˜ï¼Œå®ç°äº†å¯é æ€§å’Œè®¡ç®—æ•ˆç‡çš„å¹³è¡¡ï¼Œä¸ºé«˜æ€§èƒ½AIè®¡ç®—æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚

Abstract: As the CMOS technology pushes to the nanoscale, aging effects and process variations have become increasingly pronounced, posing significant reliability challenges for AI accelerators. Traditional guardband-based design approaches, which rely on pessimistic timing margin, sacrifice significant performance and computational efficiency, rendering them inadequate for high-performance AI computing demands. Current reliability-aware AI accelerator design faces two core challenges: (1) the lack of systematic cross-layer analysis tools to capture coupling reliability effects across device, circuit, architecture, and application layers; and (2) the fundamental trade-off between conventional reliability optimization and computational efficiency. To address these challenges, this paper systematically presents a series of reliability-aware accelerator designs, encompassing (1) aging and variation-aware dynamic timing analyzer, (2) accelerator dataflow optimization using critical input pattern reduction, and (3) resilience characterization and novel architecture design for large language models (LLMs). By tightly integrating cross-layer reliability modeling and AI workload characteristics, these co-optimization approaches effectively achieve reliable and efficient AI acceleration.

</details>
