<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Layered Modal ML: Syntax and Full Abstraction](https://arxiv.org/abs/2602.03033)
*Haoxuan Yin,Andrzej S. Murawski,C. -H. Luke Ong*

Main category: cs.PL

TL;DR: LMML是首个支持在强类型安全保证下存储和运行开放代码的元编程语言，并建立了首个命令式MetaML风格语言的完全抽象语义模型。


<details>
  <summary>Details</summary>
Motivation: 元编程语言中，在存在高阶代码引用时确保类型安全具有挑战性，因为自由变量可能逃逸其绑定器。同时，基于元编程的程序优化需要验证优化程序是否保持原始程序的意义，这需要程序等价性的概念和推理技术。

Method: 提出Layered Modal ML (LMML)，使用上下文模态类型显式跟踪和推理代码中的自由变量。基于操作博弈语义的迹建立语义模型，捕获上下文等价性，并建立新颖的封闭实例使用定理。

Result: LMML是首个支持在强类型安全保证下存储和运行开放代码的元编程语言。建立了首个命令式MetaML风格语言的完全抽象结果，语义模型能够捕获上下文等价性。

Conclusion: 该工作提出了类型安全的元编程语言LMML及其完全抽象语义模型，为元编程中的程序优化和等价性验证提供了理论基础。

Abstract: MetaML-style metaprogramming languages allow programmers to construct, manipulate and run code. In the presence of higher-order references for code, ensuring type safety is challenging, as free variables can escape their binders. In this paper, we present Layered Modal ML (LMML), \textit{the first metaprogramming language that supports storing and running open code under a strong type safety guarantee}. The type system utilises contextual modal types to track and reason about free variables in code explicitly.
  A crucial concern in metaprogramming-based program optimisations is whether the optimised program preserves the meaning of the original program. Addressing this question requires a notion of program equivalence and techniques to reason about it. In this paper, we provide a semantic model that captures contextual equivalence for LMML, establishing \textit{the first full abstraction result for an imperative MetaML-style language}. Our model is based on traces derived via operational game semantics, where the meaning of a program is modelled by its possible interactions with the environment. We also establish a novel closed instances of use theorem that accounts for both call-by-value and call-by-name closing substitutions.

</details>


### [2] [From Separate Compilation to Sound Language Composition](https://arxiv.org/abs/2602.03777)
*Federico Bruzzone,Walter Cazzola,Luca Favalli*

Main category: cs.PL

TL;DR: nlgcheck是一个基于数据流分析的静态分析工具，用于Neverlang语言工作台，能在编译时检测潜在运行时错误（如未定义属性访问），在保持单独编译的同时提供强静态正确性保证。


<details>
  <summary>Details</summary>
Motivation: 编程语言开发中，语言工作台为了支持模块化开发，常常放松单独编译这一关键约束，但这会损害工件可重用性和与依赖系统的集成。现有方法（如Neverlang中的动态映射）以灵活性为代价牺牲编译时正确性，导致潜在运行时错误。

Method: 提出nlgcheck工具，基于数据流分析进行静态分析，专门针对Neverlang语言工作台。该工具在编译时检测未定义属性访问等潜在运行时错误，保持单独编译的同时提供静态正确性保证。

Result: 通过基于Neverlang项目的突变测试实验评估表明，nlgcheck能有效增强鲁棒性，不牺牲模块化或灵活性，且性能水平不会阻碍日常开发活动中的采用。

Conclusion: nlgcheck解决了语言扩展中单独编译与静态正确性之间的权衡问题，为语言工作台提供了理论上可靠的静态分析工具，在保持模块化开发优势的同时防止运行时错误。

Abstract: The development of programming languages involves complex theoretical and practical challenges, particularly when addressing modularity and reusability through language extensions. While language workbenches aim to enable modular development under the constraints of the language extension problem, one critical constraint -- separate compilation -- is often relaxed due to its complexity. However, this relaxation undermines artifact reusability and integration with common dependency systems. A key difficulty under separate compilation arises from managing attribute grammars, as extensions may introduce new attributes that invalidate previously generated abstract syntax tree structures. Existing approaches, such as the use of dynamic maps in the Neverlang workbench, favor flexibility at the cost of compile-time correctness, leading to potential runtime errors due to undefined attributes. This work addresses this issue by introducing nlgcheck, a theoretically sound static analysis tool based on data-flow analysis for the Neverlang language workbench. nlgcheck detects potential runtime errors -- such as undefined attribute accesses -- at compile time, preserving separate compilation while maintaining strong static correctness guarantees. Experimental evaluation using mutation testing on Neverlang-based projects demonstrates that nlgcheck effectively enhances robustness without sacrificing modularity or flexibility and with a level of performance that does not impede its adoption in daily development activities.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Prefix Consensus For Censorship Resistant BFT](https://arxiv.org/abs/2602.02892)
*Zhuolun Xiang,Andrei Tonkikh,Alexander Spiegelman*

Main category: cs.DC

TL;DR: 提出Prefix Consensus新抽象和协议栈，解决BFT共识中的审查抵抗问题，实现无领导者、多提议者的抗审查BFT SMR协议


<details>
  <summary>Details</summary>
Motivation: 现有BFT共识在区块链中审查抵抗能力弱，领导者可以排除交易，这对交易和DeFi构成日益增长的威胁

Method: 1. 引入Prefix Consensus抽象，各方输入向量并输出高低边界；2. 定义Strong Prefix Consensus，要求在高输出上达成一致；3. 构建无领导者、多提议者的抗审查BFT SMR协议，使用确定性排名和降级规则

Result: 1. Prefix Consensus可在异步环境下解决并给出紧的轮复杂度界；2. Strong Prefix Consensus协议无领导者且部分同步；3. 抗审查BFT SMR协议在四轮内提交诚实提案；4. 将Prefix Consensus连接到分级和二进制/验证共识，获得最优延迟的分级共识

Conclusion: 通过Prefix Consensus新抽象构建了有效的抗审查BFT共识协议栈，解决了区块链中领导者审查交易的问题，实现了无领导者、多提议者的安全状态机复制

Abstract: Despite broad use of BFT consensus in blockchains, censorship resistance is weak: leaders can exclude transactions, a growing concern for trading and DeFi.
  We address this by introducing a new abstraction and protocol stack. First, we introduce \emph{Prefix Consensus}, where parties input vectors and output $(v^{\sf low},v^{\sf high})$ that (i) extend the maximum common prefix of honest inputs and (ii) satisfy $v_i^{\sf low}\preceq v_j^{\sf high}$ for all honest $i,j$. Unlike classical consensus, no single output is required. We show Prefix Consensus is solvable asynchronously and give tight round-complexity bounds.
  We then define \emph{Strong Prefix Consensus}, requiring agreement on the \emph{high} output. Our protocol is leaderless and partially synchronous: one Prefix Consensus instance decides (possibly different) lows, and additional instances yield a unique safe-to-extend high, even if an adversary can suspend one party per round.
  We lift this to a leaderless, multi-proposer, censorship-resistant BFT SMR protocol: per slot, all parties broadcast proposals, deterministically rank them, and run one Strong Prefix Consensus on proposal hashes, committing honest proposals in \emph{four rounds}. A deterministic demotion rule updates the ranking when a party's proposal is excluded, implying that after GST at most $f$ slots can miss an honest proposal while progress remains leaderless under suspension and up to $f{-}1$ Byzantine faults.
  Finally, we connect Prefix Consensus to graded and binary/validated consensus: we obtain an optimal-latency graded consensus (3 message delays) and leaderless Binary/Validated Consensus with worst-case message complexity $O(n^3)$ and communication $O(n^4)$.

</details>


### [4] [Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control](https://arxiv.org/abs/2602.02987)
*Ruihan Lin,Zezhen Ding,Zean Han,Jiheng Zhang*

Main category: cs.DC

TL;DR: 针对LLM推理中prefill和decode阶段资源争用问题，提出基于排队网络的随机控制框架，设计门控路由策略实现异构工作负载的最优调度


<details>
  <summary>Details</summary>
Motivation: LLM推理的两阶段特性（计算密集的prefill阶段和内存受限的decode阶段）在共享GPU资源时产生状态依赖的资源争用，加上工作负载的异构性（不同应用输入输出长度差异大），使得调度成为关键挑战

Method: 将LLM推理建模为具有状态依赖服务速率的多类多服务器排队网络，基于经验迭代时间测量，分析流体近似并求解稳态线性规划，设计调节prefill准入和decode路由的门控路由策略

Result: 在大量GPU限制下，证明所提策略在捆绑和分离token定价方案下都是渐近最优的，数值实验显示优于标准服务启发式方法

Conclusion: 提出的随机控制框架能有效解决LLM推理中的资源争用问题，支持服务级别指标约束，为异构工作负载调度提供通用方法

Abstract: Large Language Models (LLMs) are rapidly becoming critical infrastructure for enterprise applications, driving unprecedented demand for GPU-based inference services. A key operational challenge arises from the two-phase nature of LLM inference: a compute-intensive \emph{prefill} phase that processes user input, followed by a memory-bound \emph{decode} phase that generates output tokens. When these phases share GPU resources, prefill tasks throttle the processing speed of concurrent decodes, creating state-dependent contention. This contention is further complicated by workload heterogeneity, as different applications exhibit vastly different input and output lengths. We develop a stochastic control framework for scheduling heterogeneous LLM workloads across large GPU clusters. We formulate LLM inference as a multiclass many-server queueing network with state-dependent service rates, grounded in empirical iteration-time measurements. We analyze the fluid approximation of this system and solve steady-state linear programs that characterize optimal resource allocation. We design gate-and-route policies that regulate prefill admission and decode routing, and prove that they are asymptotically optimal in the many-GPU limit under both bundled and separate token-pricing schemes. We further extend the framework to incorporate Service Level Indicators (SLIs) such as latency and fairness, providing a general approach to constrained scheduling. Numerical experiments calibrated to empirical iteration-time data demonstrate that our policies outperform standard serving heuristics.

</details>


### [5] [Studying the Effect of Schedule Preemption on Dynamic Task Graph Scheduling](https://arxiv.org/abs/2602.03081)
*Mohammadali Khodabandehlou,Jared Coleman,Niranjan Suri,Bhaskar Krishnamachari*

Main category: cs.DC

TL;DR: 研究动态任务图调度中的受控抢占模型，提出Last-K抢占方法，选择性重调度近期任务图，在保持早期分配的同时优化性能指标


<details>
  <summary>Details</summary>
Motivation: 传统动态任务图调度通常不重新审视先前的任务分配，主要关注最小化完成时间。研究旨在探索受控的调度抢占，在保持公平性和低开销的同时获得性能提升

Method: 提出Last-K抢占模型，选择性重调度最近的任务图，同时保留早期分配。比较抢占式、非抢占式和部分抢占式策略，使用合成、RIoTBench、WFCommons和对抗性工作负载进行评估

Result: 结果显示适度的抢占可以在匹配完全抢占的大部分完成时间和利用率收益的同时，保持公平性和低运行时开销

Conclusion: 受控的调度抢占策略（如Last-K模型）能够在性能指标和系统开销之间取得良好平衡，为动态任务图调度提供了有效的解决方案

Abstract: Dynamic scheduling of task graphs is often addressed without revisiting prior task allocations, with a primary focus on minimizing makespan. We study controlled schedule preemption, introducing the Last-K Preemption model, which selectively reschedules recent task graphs while preserving earlier allocations. Using synthetic, RIoTBench, WFCommons, and adversarial workloads, we compare preemptive, non-preemptive, and partial-preemptive strategies across makespan, fairness, utilization, and runtime. Results show moderate preemption can match most makespan and utilization gains of full preemption while maintaining fairness and low overhead.

</details>


### [6] [Joint Network-and-Server Congestion in Multi-Source Traffic Allocation: A Convex Formulation and Price-Based Decentralization](https://arxiv.org/abs/2602.03246)
*Tamoghna Sarkar,Bhaskar Krishnamachari*

Main category: cs.DC

TL;DR: 研究多源多服务节点的稳态流量分配问题，考虑路径访问延迟和服务节点排队延迟，提出分布式定价算法实现系统最优


<details>
  <summary>Details</summary>
Motivation: 网络和分布式系统中存在重要的速率分配问题：当多个源节点向多个服务节点发送流量时，需要考虑路径访问延迟（与速率相关且凸）和服务节点排队延迟（与总负载相关）。现有研究通常单独处理这两种延迟，需要联合建模以获得系统最优分配。

Method: 将端到端延迟最小化建模为凸优化问题，基于KKT条件推导出最优条件：所有使用路径的总边际成本（路径边际访问项+节点拥塞价格）相等。提出轻量级分布式定价算法：服务节点根据观测负载计算并广播拥塞价格，源节点根据价格更新流量分配。

Result: 证明端到端延迟最小化是凸规划问题，存在全局最优解。分布式算法能够收敛到集中式最优解，数值实验验证了收敛性和性能。

Conclusion: 联合建模访问延迟和服务拥塞的流量分配问题具有凸结构，可通过分布式定价算法有效求解。最优条件具有Wardrop型解释，为实际网络系统提供了理论依据和实用算法。

Abstract: This paper studies an important rate allocation problem that arises in many networked and distributed systems: steady-state traffic rate allocation from multiple sources to multiple service nodes when both (i) the access-path delay on each source-node route is rate-dependent (capacity-constrained) and convex, and (ii) each service node (also capacity-constrained) experiences a load-dependent queueing delay driven by aggregate load from all sources. We show that the resulting flow-weighted end-to-end delay minimization is a convex program, yielding a global system-optimal solution characterized by KKT conditions that equalize total marginal costs (a path marginal access term plus a node congestion price) across all utilized routes. This condition admits a Wardrop-type interpretation: for each source, all utilized options equalize total marginal cost, while any option with strictly larger total marginal cost receives no flow. Building on this structure, we develop a lightweight distributed pricing-based algorithm in which each service node locally computes and broadcasts a scalar congestion price from its observed aggregate load, while each source updates its traffic split by solving a small separable convex allocation problem under the advertised prices. Numerical illustrations demonstrate convergence of the distributed iteration to the centralized optimum and highlight the trade-offs induced by jointly modeling access and service congestion.

</details>


### [7] [Exploiting Multi-Core Parallelism in Blockchain Validation and Construction](https://arxiv.org/abs/2602.03444)
*Arivarasan Karmegam,Lucianna Kiffer,Antonio Fernández Anta*

Main category: cs.DC

TL;DR: 该论文研究区块链验证器如何利用多核CPU并行处理区块，同时保持确定性执行和区块链语义。提出了两个优化问题：有序区块并行执行和区块构建交易选择，并开发了MILP模型和高效启发式算法。


<details>
  <summary>Details</summary>
Motivation: 区块链验证器可以利用多核CPU减少区块处理时间，但必须保持确定性执行，同时尊重交易冲突和每个区块的运行时间限制。需要系统研究如何在区块构建和执行中利用多核并行性而不违反区块链语义。

Method: 形式化了两个验证器端优化问题：1) 在p个核心上执行已排序区块以最小化完成时间，同时确保与顺序执行等价；2) 在运行时限制B下选择和调度内存池交易子集以最大化验证器奖励。为两者开发了精确的混合整数线性规划(MILP)公式，并提出可扩展到实际工作负载的快速确定性启发式算法。

Result: 使用以太坊主网追踪数据，包括Solana启发的声明访问基线(Sol)用于有序区块调度和简单奖励贪婪基线(RG)用于区块构建，实证量化了最优性和运行时间之间的权衡。

Conclusion: 该研究为区块链验证器在多核环境下的并行处理提供了系统框架，通过MILP模型和启发式算法在保持区块链语义的同时实现了性能优化，为实际部署提供了理论和方法基础。

Abstract: Blockchain validators can reduce block processing time by exploiting multi-core CPUs, but deterministic execution must preserve a given total order while respecting transaction conflicts and per-block runtime limits. This paper systematically examines how validators can exploit multi-core parallelism during both block construction and execution without violating blockchain semantics. We formalize two validator-side optimization problems: (i) executing an already ordered block on \(p\) cores to minimize makespan while ensuring equivalence to sequential execution; and (ii) selecting and scheduling a subset of mempool transactions under a runtime limit \(B\) to maximize validator reward. For both, we develop exact Mixed-Integer Linear Programming (MILP) formulations that capture conflict, order, and capacity constraints, and propose fast deterministic heuristics that scale to realistic workloads. Using Ethereum mainnet traces and including a Solana-inspired declared-access baseline (Sol) for ordered-block scheduling and a simple reward-greedy baseline (RG) for block construction, we empirically quantify the trade-offs between optimality and runtime.

</details>


### [8] [Recursive Energy Efficient Agreement](https://arxiv.org/abs/2602.03474)
*Shachar Meir,David Peleg*

Main category: cs.DC

TL;DR: 提出一种递归式共识算法，使每个参与者仅需参与O(log f)轮活跃通信，显著降低能耗


<details>
  <summary>Details</summary>
Motivation: 传统分布式共识算法中所有节点需要参与所有轮次通信，能耗高。近期研究提出能量高效共识概念，旨在减少每个参与者需要参与的轮次数，从而降低能耗成本

Method: 设计递归式共识算法，通过递归结构将参与者的活跃通信轮次限制在O(log f)级别，其中f<n表示系统中最大崩溃故障数

Result: 算法实现了每个参与者仅需O(log f)轮活跃参与，相比传统算法显著减少了能耗

Conclusion: 该递归算法为能量高效共识问题提供了有效解决方案，在保证共识正确性的同时大幅降低了参与者的能量消耗

Abstract: Agreement is a foundational problem in distributed computing that have been studied extensively for over four decades. Recently, Meir, Mirault, Peleg and Robinson introduced the notion of \emph{Energy Efficient Agreement}, where the goal is to solve Agreement while minimizing the number of round a party participates in, thereby reducing the energy cost per participant. We show a recursive Agreement algorithm that has $O(\log f)$ active rounds per participant, where $f<n$ represents the maximum number of crash faults in the system.

</details>


### [9] [DALI: A Workload-Aware Offloading Framework for Efficient MoE Inference on Local PCs](https://arxiv.org/abs/2602.03495)
*Zeyu Zhu,Gang Li,Peisong Wang,Zitao Mo,Minnan Pei,Zhuoran Song,Xiaoyao Liang,Jian Cheng*

Main category: cs.DC

TL;DR: DALI：一种面向本地PC上MoE推理的工作负载感知卸载框架，通过动态专家分配、残差预取和工作负载感知缓存替换，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然能大幅提升LLM容量而不成比例增加计算量，但参数规模庞大。现有卸载方法无法匹配专家工作负载的动态特性，导致CPU-GPU负载不平衡、预取不准确和GPU缓存效率低下等问题。

Method: 1) 将专家分配建模为0-1整数优化问题，采用贪心分配策略动态分配专家到CPU或GPU；2) 基于残差的预取方法，利用层间残差信息准确预测高工作负载专家；3) 工作负载感知的缓存替换策略，利用专家激活的时间相关性提升GPU缓存效率。

Result: 在各种MoE模型和设置下评估，DALI在预填充和解码阶段相比最先进的卸载框架都实现了显著加速。

Conclusion: DALI通过工作负载感知的卸载框架有效解决了MoE推理中的关键效率问题，为资源受限的本地PC平台提供了高效的MoE模型支持方案。

Abstract: Mixture of Experts (MoE) architectures significantly enhance the capacity of LLMs without proportional increases in computation, but at the cost of a vast parameter size. Offloading MoE expert parameters to host memory and leveraging both CPU and GPU computation has recently emerged as a promising direction to support such models on resourceconstrained local PC platforms. While promising, we notice that existing approaches mismatch the dynamic nature of expert workloads, which leads to three fundamental inefficiencies: (1) Static expert assignment causes severe CPUGPU load imbalance, underutilizing CPU and GPU resources; (2) Existing prefetching techniques fail to accurately predict high-workload experts, leading to costly inaccurate prefetches; (3) GPU cache policies neglect workload dynamics, resulting in poor hit rates and limited effectiveness. To address these challenges, we propose DALI, a workloaDAware offLoadIng framework for efficient MoE inference on local PCs. To fully utilize hardware resources, DALI first dynamically assigns experts to CPU or GPU by modeling assignment as a 0-1 integer optimization problem and solving it efficiently using a Greedy Assignment strategy at runtime. To improve prefetching accuracy, we develop a Residual-Based Prefetching method leveraging inter-layer residual information to accurately predict high-workload experts. Additionally, we introduce a Workload-Aware Cache Replacement policy that exploits temporal correlation in expert activations to improve GPU cache efficiency. By evaluating across various MoE models and settings, DALI achieves significant speedups in the both prefill and decoding phases over the state-of-the-art offloading frameworks.

</details>


### [10] [Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods](https://arxiv.org/abs/2602.03802)
*Grigory Begunov,Alexander Tyurin*

Main category: cs.DC

TL;DR: 同步SGD及其变体m-Synchronous SGD在异构计算场景中接近最优，尽管异步优化有进展但同步方法在许多实际场景中足够有效


<details>
  <summary>Details</summary>
Motivation: 尽管异步优化方法有显著进展，但现代分布式优化仍主要依赖传统同步方法。作者重新审视同步方法，研究其在异构计算场景中的表现，挑战了异步方法必然更优的普遍认知。

Method: 分析Synchronous SGD及其鲁棒变体m-Synchronous SGD，在随机计算时间和对抗性部分参与的工作节点条件下进行理论分析，评估其时间复杂性。

Result: 理论证明同步方法在许多异构计算场景中接近最优，其时间复杂性在许多实际场景中达到最优（最多相差对数因子）。同步方法虽然不是通用解决方案，但在许多现代异构计算场景中足够有效。

Conclusion: 同步SGD方法在异构计算环境中表现优于预期，在许多实际场景中接近最优，挑战了异步方法必然更优的假设，为分布式优化方法选择提供了新的理论依据。

Abstract: Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization. We revisit Synchronous SGD and its robust variant, called $m$-Synchronous SGD, and theoretically show that they are nearly optimal in many heterogeneous computation scenarios, which is somewhat unexpected. We analyze the synchronous methods under random computation times and adversarial partial participation of workers, and prove that their time complexities are optimal in many practical regimes, up to logarithmic factors. While synchronous methods are not universal solutions and there exist tasks where asynchronous methods may be necessary, we show that they are sufficient for many modern heterogeneous computation scenarios.

</details>
