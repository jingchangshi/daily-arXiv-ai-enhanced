<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Remarks on Algebraic Reconstruction of Types and Effects](https://arxiv.org/abs/2601.15455)
*Patrycja Balik,Szymon Jędras,Piotr Polesiuk*

Main category: cs.PL

TL;DR: 本文指出Jouvelot和Gifford1991年开创性论文中关于高阶多态性类型重建算法的细微bug，并重新审视其类型系统和重建算法。


<details>
  <summary>Details</summary>
Motivation: 虽然Jouvelot和Gifford的"Algebraic Reconstruction of Types and Effects"被认为是类型和效应系统的里程碑工作，但其原始算法考虑了具有高阶多态性的语言，这一特性难以正确实现。作者发现该算法在变量绑定方面存在细微bug，需要重新审视。

Method: 重新审视Jouvelot和Gifford的类型系统和重建算法，识别并描述与变量绑定相关的细微bug。

Result: 发现了原始算法中关于高阶多态性实现的细微bug，特别是在变量绑定方面的问题。

Conclusion: 即使是开创性的研究工作也可能存在实现细节上的问题，特别是对于高阶多态性等复杂特性的处理需要特别小心。本文为后续研究者提供了重要的修正和参考。

Abstract: In their 1991 paper "Algebraic Reconstruction of Types and Effects," Pierre Jouvelot and David Gifford presented a type-and-effect reconstruction algorithm based on an algebraic structure of effects. Their work is considered a milestone in the development of type-and-effect systems, and has inspired numerous subsequent works in the area of static analysis. However, unlike the later research it spawned, the original algorithm considered a language with higher-rank polymorphism, a feature which is challenging to implement correctly. In this note, we identify subtle bugs related to variable binding in their approach to this feature. We revisit their type system and reconstruction algorithm, and describe the discovered issues.

</details>


### [2] [Prioritizing Configuration Relevance via Compiler-Based Refined Feature Ranking](https://arxiv.org/abs/2601.16008)
*Federico Bruzzone,Walter Cazzola,Luca Favini*

Main category: cs.PL

TL;DR: 提出首个基于编译器的Rust配置优先级排序方法，通过图中心性分析特征重要性，生成有限数量的关键配置


<details>
  <summary>Details</summary>
Motivation: 现代编程语言（特别是Rust）支持高度可配置的软件系统，但配置组合爆炸使得程序分析、优化和测试变得困难，需要有效的配置优先级排序方法

Method: 1) 从Rust编译器提取定制中间表示；2) 构建两种互补的图数据结构；3) 使用中心性度量对特征排序；4) 根据代码影响范围细化排序；使用SAT求解器保证配置有效性

Result: 实现了原型工具RustyEx，在开源Rust项目上验证表明能高效生成指定数量的配置，确保构造正确性，中心性引导的配置优先级排序能有效探索大型配置空间

Conclusion: 该方法为配置感知的分析和优化研究铺平道路，证明编译器基础的配置优先级排序是实用且有效的解决方案

Abstract: Modern programming languages, most notably Rust, offer advanced linguistic constructs for building highly configurable software systems as aggregation of features -- identified by a configuration. However, they pose substantial challenges for program analysis, optimization, and testing, as the combinatorial explosion of configurations often makes exhaustive exploration infeasible. In this manuscript, we present the first compiler-based method for prioritizing configurations. Our approach consists of four main steps: 1. extracting a tailored intermediate representation from the Rust compiler, 2. constructing two complementary graph-based data structures, 3. using centrality measures to rank features, and 4. refining the ranking by considering the extent of code they impact. A fixed number of most relevant configurations are generated based on the achieved feature ranking. The validity of the generated configurations is guaranteed by using a SAT solver that takes a representation of this graph in conjunctive normal form. We formalized this approach and implemented it in a prototype, RustyEx, by instrumenting the Rust compiler. An empirical evaluation on higher-ranked open source Rust projects shows that RustyEx efficiently generates user-specified sets of configurations within bounded resources, while ensuring soundness by construction. The results demonstrate that centrality-guided configuration prioritization enables effective and practical exploration of large configuration spaces, paving the way for future research in configuration-aware analysis and optimization.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Securing LLM-as-a-Service for Small Businesses: An Industry Case Study of a Distributed Chatbot Deployment Platform](https://arxiv.org/abs/2601.15528)
*Jiazhu Xie,Bowen Li,Heyu Fu,Chong Gao,Ziqi Xu,Fengling Han*

Main category: cs.DC

TL;DR: 本文介绍了一个开源的多租户平台，帮助小企业通过无代码工作流部署定制的LLM支持聊天机器人，解决基础设施成本、工程复杂性和安全风险等挑战。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的问答系统在小企业中具有自动化客户支持和内部知识访问的潜力，但实际部署面临基础设施成本高、工程复杂、安全风险大（特别是在RAG场景下）等挑战。

Method: 开发了一个基于分布式轻量级k3s集群的开源多租户平台，跨异构低成本机器构建，通过加密覆盖网络连接，实现成本效益的资源池化，同时提供容器隔离和租户数据访问控制。平台还集成了针对RAG聊天机器人提示注入攻击的平台级防御机制。

Result: 通过实际电子商务部署评估，证明该平台能够在满足小企业现实成本、运营和安全约束的前提下，实现安全高效的LLM聊天机器人服务。

Conclusion: 该平台展示了小企业可以在现实约束下部署安全高效的LLM聊天机器人服务，为小企业采用LLM技术提供了可行的解决方案。

Abstract: Large Language Model (LLM)-based question-answering systems offer significant potential for automating customer support and internal knowledge access in small businesses, yet their practical deployment remains challenging due to infrastructure costs, engineering complexity, and security risks, particularly in retrieval-augmented generation (RAG)-based settings. This paper presents an industry case study of an open-source, multi-tenant platform that enables small businesses to deploy customised LLM-based support chatbots via a no-code workflow. The platform is built on distributed, lightweight k3s clusters spanning heterogeneous, low-cost machines and interconnected through an encrypted overlay network, enabling cost-efficient resource pooling while enforcing container-based isolation and per-tenant data access controls. In addition, the platform integrates practical, platform-level defences against prompt injection attacks in RAG-based chatbots, translating insights from recent prompt injection research into deployable security mechanisms without requiring model retraining or enterprise-scale infrastructure. We evaluate the proposed platform through a real-world e-commerce deployment, demonstrating that secure and efficient LLM-based chatbot services can be achieved under realistic cost, operational, and security constraints faced by small businesses.

</details>


### [4] [Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search](https://arxiv.org/abs/2601.15633)
*Enzo Meneses,Hugo Bec,Cristóbal A. Navarroa,Benoît Crespin,Felipe A. Quezada,Nancy Hitschfeld,Heinich Porro,Maxime Maria*

Main category: cs.DC

TL;DR: 该论文提出了三种改进基于RT核心的粒子FRNN物理模拟的方法：BVH更新/重建比率优化器、无需邻居列表的RT核心新用法（两种变体）、支持周期性边界条件的RT核心技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于RT核心的粒子FRNN物理模拟存在性能瓶颈，特别是在BVH管理、邻居列表内存开销和周期性边界条件支持方面需要改进。

Method: 1. 实时BVH更新/重建比率优化器，根据模拟动态自适应调整；2. 两种无需邻居列表的RT核心变体；3. 支持周期性边界条件的RT核心技术。

Result: BVH优化器使RT核心管道速度提升达3.4倍；新变体在小半径下提升1.3倍，对数正态半径分布下提升2.0倍；支持周期性边界条件无显著性能损失；方法在不同GPU代际上均能扩展性能和能效。

Conclusion: 提出的三种方法显著提升了RT核心在FRNN物理模拟中的性能和能效，同时识别了传统GPU计算仍占优势的场景，有助于理解RT核心的优缺点。

Abstract: In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\sim 3.4\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\sim1.3\times$ at small radius to $\sim2.0\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [5] [FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design](https://arxiv.org/abs/2601.15710)
*Jiahao Zhang,Zifan He,Nicholas Fraser,Michaela Blott,Yizhou Sun,Jason Cong*

Main category: cs.AR

TL;DR: FlexLLM是一个可组合的HLS库，用于快速开发专用LLM加速器，支持阶段定制化推理和量化，在FPGA上实现比GPU更好的性能与能效。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理加速方案通常缺乏灵活性，难以快速适配不同算法创新和硬件架构。需要一种能够简化专用LLM加速器开发、支持阶段定制化设计和高效量化的解决方案。

Method: 开发FlexLLM可组合HLS库，提供架构自由度以支持预填充和解码阶段的定制化设计，包含全面的量化套件，并实现层次化内存变换器插件用于长上下文处理。

Result: 在AMD U280 FPGA上，相比NVIDIA A100 GPU，实现了1.29倍端到端加速、1.64倍解码吞吐量和3.14倍能效提升；集成HMT插件后，预填充延迟降低23.23倍，上下文窗口扩展64倍。

Conclusion: FlexLLM能够以最小手动工作量桥接LLM推理算法创新与高性能加速器开发，显著提升开发效率和系统性能。

Abstract: We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\times$ end-to-end speedup, 1.64$\times$ higher decode throughput, and 3.14$\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\times$, 6.55$\times$, and 4.13$\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\times$ and extends the context window by 64$\times$, delivering 1.10$\times$/4.86$\times$ lower end-to-end latency and 5.21$\times$/6.27$\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.

</details>


### [6] [A Case for Hypergraphs to Model and Map SNNs on Neuromorphic Hardware](https://arxiv.org/abs/2601.16118)
*Marco Ronzani,Cristina Silvano*

Main category: cs.AR

TL;DR: 该论文提出将脉冲神经网络（SNN）从图抽象提升到超图，以更好地映射到神经形态硬件上，通过超边共成员关系捕捉核心内尖峰复制，从而减少通信流量和硬件资源使用。


<details>
  <summary>Details</summary>
Motivation: 将SNN映射到神经形态硬件面临两个NP-hard问题：将SNN图分区以适配核心，以及将分区放置到硬件网格上。随着SNN和硬件规模扩大到数十亿神经元，传统基于图的映射方法难以有效处理。

Method: 提出将SNN从图抽象提升到超图模型，利用超边共成员关系捕捉核心内尖峰复制。基于超边的重叠性和局部性设计映射算法，通过共享超边分组神经元。比较了多种分区和放置算法，包括新设计的和文献改编的。

Result: 超图基技术在不同执行时间范围内都能获得比现有技术更好的映射效果。超边重叠性和局部性与高质量映射强相关，利用这些特性可以直接减少通信流量和硬件资源使用。

Conclusion: 超图抽象能更准确地建模SNN在神经形态硬件上的执行，基于超边的映射算法在不同规模下都能实现有效映射。研究识别出了一组有前景的算法选择，可在任何规模下实现有效映射。

Abstract: Executing Spiking Neural Networks (SNNs) on neuromorphic hardware poses the problem of mapping neurons to cores. SNNs operate by propagating spikes between neurons that form a graph through synapses. Neuromorphic hardware mimics them through a network-on-chip, transmitting spikes, and a mesh of cores, each managing several neurons. Its operational cost is tied to spike movement and active cores. A mapping comprises two tasks: partitioning the SNN's graph to fit inside cores and placement of each partition on the hardware mesh. Both are NP-hard problems, and as SNNs and hardware scale towards billions of neurons, they become increasingly difficult to tackle effectively. In this work, we propose to raise the abstraction of SNNs from graphs to hypergraphs, redesigning mapping techniques accordingly. The resulting model faithfully captures the replication of spikes inside cores by exposing the notion of hyperedge co-membership between neurons. We further show that the overlap and locality of hyperedges strongly correlate with high-quality mappings, making these properties instrumental in devising mapping algorithms. By exploiting them directly, grouping neurons through shared hyperedges, communication traffic and hardware resource usage can be reduced be yond what just contracting individual connections attains. To substantiate this insight, we consider several partitioning and placement algorithms, some newly devised, others adapted from literature, and compare them over progressively larger and bio-plausible SNNs. Our results show that hypergraph based techniques can achieve better mappings than the state-of-the-art at several execution time regimes. Based on these observations, we identify a promising selection of algorithms to achieve effective mappings at any scale.

</details>
