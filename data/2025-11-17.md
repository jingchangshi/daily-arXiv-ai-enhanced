<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Data Race Detection by Digest-Driven Abstract Interpretation (Extended Version)](https://arxiv.org/abs/2511.11055)
*Michael Schwarz,Julian Erhard*

Main category: cs.PL

TL;DR: 本文提出了一种基于摘要的静态数据竞争检测方法，通过摘要捕获冲突访问不能并行发生的条件，显著提高了数据竞争检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的静态分析虽然能证明数据竞争的缺失，但需要更精确的方法来识别哪些冲突内存访问不会同时发生。本文旨在将摘要概念重新用于数据竞争检测，以捕获冲突访问不能并行发生的条件。

Method: 在局部迹语义中定义数据竞争，将冲突排除条件表示为摘要。在静态分析器Goblint中实现了摘要驱动的数据竞争检测，结合了锁集摘要、线程ID和线程连接推理。

Result: 在SV-COMP基准测试套件上评估，结合锁集摘要与线程ID和线程连接推理，相比单独使用锁集推理，正确解决的任务数量增加了五倍以上。

Conclusion: 摘要驱动的数据竞争检测方法显著提高了静态分析的准确性，证明了将摘要概念扩展到竞争检测的有效性。

Abstract: Sound static analysis can prove the absence of data races by establishing that no two conflicting memory accesses can occur at the same time. We repurpose the concept of digests -- summaries of computational histories originally introduced to bring tunable concurrency-sensitivity to thread-modular value analysis by abstract interpretation, extending this idea to race detection: We use digests to capture the conditions under which conflicting accesses may not happen in parallel. To formalize this, we give a definition of data races in the thread-modular local trace semantics and show how exclusion criteria for potential conflicts can be expressed as digests. We report on our implementation of digest-driven data race detection in the static analyzer Goblint, and evaluate it on the SV-COMP benchmark suite. Combining the lockset digest with digests reasoning on thread ids and thread joins increases the number of correctly solved tasks by more than a factor of five compared to lockset reasoning alone.

</details>


### [2] [Optimising Density Computations in Probabilistic Programs via Automatic Loop Vectorisation](https://arxiv.org/abs/2511.11070)
*Sangho Lim,Hyoungjin Lim,Wonyeol Lee,Xavier Rival,Hongseok Yang*

Main category: cs.PL

TL;DR: 提出了一种自动向量化概率程序中循环的方法，通过推测性并行执行提高性能，在Pyro PPL中实现并取得显著加速效果


<details>
  <summary>Details</summary>
Motivation: 概率推理成本高昂，其中一个瓶颈在于需要迭代大型数据集或长序列随机样本。现有向量化方法存在局限性，无法处理通用循环结构

Method: 使用推测性并行执行循环迭代，通过不动点检查保持原始循环语义，将命令式PPL转换为支持向量化的低级目标语言

Result: 在多种概率模型上测试，相比现有基线获得1.1-6倍加速，减少GPU内存使用，且能处理所有测试模型

Conclusion: 该方法能有效自动向量化概率程序中的循环，显著提升性能，解决了现有方法的局限性

Abstract: Probabilistic programming languages (PPLs) are a popular tool for high-level modelling across many fields. They provide a range of algorithms for probabilistic inference, which analyse models by learning their parameters from a dataset or estimating their posterior distributions. However, probabilistic inference is known to be very costly. One of the bottlenecks of probabilistic inference stems from the iteration over entries of a large dataset or a long series of random samples. Vectorisation can mitigate this cost, but manual vectorisation is error-prone, and existing automatic techniques are often ad-hoc and limited, unable to handle general repetition structures, such as nested loops and loops with data-dependent control flow, without significant user intervention. To address this bottleneck, we propose a sound and effective method for automatically vectorising loops in probabilistic programs. Our method achieves high throughput using speculative parallel execution of loop iterations, while preserving the semantics of the original loop through a fixed-point check. We formalise our method as a translation from an imperative PPL into a lower-level target language with primitives geared towards vectorisation. We implemented our method for the Pyro PPL and evaluated it on a range of probabilistic models. Our experiments show significant performance gains against an existing vectorisation baseline, achieving $1.1$--$6\times$ speedups and reducing GPU memory usage in many cases. Unlike the baseline, which is limited to a subset of models, our method effectively handled all the tested models.

</details>


### [3] [Kleene Algebra](https://arxiv.org/abs/2511.11264)
*Tobias Kappé,Alexandra Silva,Jana Wagemaker*

Main category: cs.PL

TL;DR: This booklet introduces Kleene Algebra (KA) as a framework for studying program equivalences using regular expressions and automata, with exercises to build understanding and an optional coalgebra-based approach to automata theory.


<details>
  <summary>Details</summary>
Motivation: To provide a foundational understanding of Kleene Algebra and its application in verifying program equivalences through the correspondence between regular expressions and automata.

Method: Modeling programs with regular expressions, establishing their correspondence to automata, and leveraging this to prove equivalences using KA laws.

Result: The central result is that an equivalence of regular expressions holds if and only if it can be proven using the laws of Kleene Algebra.

Conclusion: The booklet effectively introduces KA, reinforces concepts with exercises, and offers an alternative coalgebraic perspective on automata theory for deeper insight.

Abstract: This booklet serves as an introduction to Kleene Algebra (KA), a set of laws that can be used to study general equivalences between programs. It discusses how general programs can be modeled using regular expressions, how those expressions correspond to automata, and how this correspondence can be exploited to obtain the central result of KA, namely that an equivalence of regular expressions is true if and only if it can be proved using the laws of KA. Each chapter closes with a set of exercises to further build intuition and understanding, and there is an optional chapter that develops automata theory through the lens of coalgebra.

</details>


### [4] [The Jasmin Compiler Preserves Cryptographic Security](https://arxiv.org/abs/2511.11292)
*Santiago Arranz-Olmos,Gilles Barthe,Lionel Blatter,Benjamin Grégoire,Vincent Laporte,Paolo Torrini*

Main category: cs.PL

TL;DR: 本文显著增强了Jasmin编译器（用于开发高效、形式验证的密码学实现）的安全保证，证明其前端25个编译过程能够保持密码学安全性，包括IND-CCA安全性。


<details>
  <summary>Details</summary>
Motivation: Jasmin编译器虽然已在Rocq证明器中证明功能正确，但这种正确性声明不适用于密码学中必需的非终止或概率计算，因此需要增强其密码学安全性保证。

Method: 首先定义适用于编译器正确性证明的关系Hoare逻辑，并基于交互树的新指称语义证明其可靠性；然后使用该程序逻辑证明Jasmin编译器相对于该语义的功能正确性；最后用交互树形式化密码学安全性并证明编译器保持密码学安全。

Result: 成功证明Jasmin编译器前端25个编译过程能够保持密码学安全性，包括IND-CCA安全性，显著增强了编译器的安全保证。

Conclusion: 通过新的程序逻辑和语义框架，成功扩展了Jasmin编译器的验证范围，使其不仅保证功能正确性，还能保持密码学安全性，为开发形式验证的密码学实现提供了更强保障。

Abstract: Jasmin is a programming and verification framework for developing efficient, formally verified, cryptographic implementations. A main component of the framework is the Jasmin compiler, which empowers programmers to write efficient implementations of state-of-the-art cryptographic primitives, including post-quantum cryptographic standards. The Jasmin compiler is proven functionally correct in the Rocq prover. However, this functional correctness statement does not apply to nonterminating or probabilistic computations, which are essential features in cryptography.
  In this paper, we significantly enhance the guarantees of the compiler by showing, in the Rocq prover, that its front-end (25 out of 30 passes) preserves cryptographic security. To this end, we first define a Relational Hoare Logic tailored for compiler correctness proofs. We prove the soundness of our logic w.r.t. a new denotational semantics of Jasmin programs based on interaction trees. Secondly, we use our program logic to prove the functional correctness of the (unmodified) Jasmin compiler w.r.t. said semantics. Lastly, we formalize cryptographic security -- focusing on IND-CCA -- with interaction trees and prove that the Jasmin compiler preserves cryptographic security.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [FengHuang: Next-Generation Memory Orchestration for AI Inferencing](https://arxiv.org/abs/2511.10753)
*Jiamin Li,Lei Qu,Tao Zhang,Grigory Chirkov,Shuotao Xu,Peng Cheng,Lidong Zhou*

Main category: cs.DC

TL;DR: 提出凤凰平台，一种分解式AI基础设施，通过多级共享内存架构解决GPU内存和通信扩展限制，实现显著的内存容量减少、GPU计算节省和更快的通信速度。


<details>
  <summary>Details</summary>
Motivation: 传统GPU中心架构在推理工作负载上面临内存容量、带宽和互连扩展的限制，需要新的基础设施设计来支持大规模AI推理。

Method: 凤凰平台采用多级共享内存架构，结合高速本地内存和集中式分解远程内存，通过主动张量分页和近内存计算优化张量操作。

Result: 模拟显示凤凰平台实现高达93%本地内存容量减少、50%GPU计算节省，以及16倍到70倍更快的GPU间通信速度，在GPT-3、Grok-1等模型上可减少50%GPU数量同时保持性能。

Conclusion: 凤凰平台作为机架级AI基础设施扩展解决方案，提供可扩展、灵活且经济高效的AI推理基础设施，消除供应商锁定并显著降低基础设施和电力成本。

Abstract: This document presents a vision for a novel AI infrastructure design that has been initially validated through inference simulations on state-of-the-art large language models. Advancements in deep learning and specialized hardware have driven the rapid growth of large language models (LLMs) and generative AI systems. However, traditional GPU-centric architectures face scalability challenges for inference workloads due to limitations in memory capacity, bandwidth, and interconnect scaling. To address these issues, the FengHuang Platform, a disaggregated AI infrastructure platform, is proposed to overcome memory and communication scaling limits for AI inference. FengHuang features a multi-tier shared-memory architecture combining high-speed local memory with centralized disaggregated remote memory, enhanced by active tensor paging and near-memory compute for tensor operations. Simulations demonstrate that FengHuang achieves up to 93% local memory capacity reduction, 50% GPU compute savings, and 16x to 70x faster inter-GPU communication compared to conventional GPU scaling. Across workloads such as GPT-3, Grok-1, and QWEN3-235B, FengHuang enables up to 50% GPU reductions while maintaining end-user performance, offering a scalable, flexible, and cost-effective solution for AI inference infrastructure. FengHuang provides an optimal balance as a rack-level AI infrastructure scale-up solution. Its open, heterogeneous design eliminates vendor lock-in and enhances supply chain flexibility, enabling significant infrastructure and power cost reductions.

</details>


### [6] [HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation](https://arxiv.org/abs/2511.10860)
*Rabimba Karanjai,Lei Xu,Weidong Shi*

Main category: cs.DC

TL;DR: HPCAgentTester是一个基于多智能体LLM的框架，用于自动化生成HPC软件的单元测试，特别针对OpenMP和MPI并行程序，通过协作工作流提高测试质量和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理HPC中的非确定性行为和同步问题，需要更智能的测试生成方法来确保并行软件的可靠性。

Method: 采用多智能体LLM架构，包含配方智能体和测试智能体，通过迭代式的批评循环协作生成和优化测试用例，专门针对并行执行构造和复杂通信模式。

Result: 能够为OpenMP和MPI原语生成可编译且功能正确的测试，有效识别传统技术经常遗漏的细微bug，显著提高了测试编译率和正确性。

Conclusion: HPCAgentTester相比独立LLM提供了更强大和可扩展的解决方案，为并行软件系统的可靠性保障提供了有效工具。

Abstract: Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow where specialized LLM agents (Recipe Agent and Test Agent) iteratively generate and refine test cases through a critique loop. This architecture enables the generation of context-aware unit tests that specifically target parallel execution constructs, complex communication patterns, and hierarchical parallelism. We demonstrate HPCAgentTester's ability to produce compilable and functionally correct tests for OpenMP and MPI primitives, effectively identifying subtle bugs that are often missed by conventional techniques. Our evaluation shows that HPCAgentTester significantly improves test compilation rates and correctness compared to standalone LLMs, offering a more robust and scalable solution for ensuring the reliability of parallel software systems.

</details>


### [7] [UFO$^3$: Weaving the Digital Agent Galaxy](https://arxiv.org/abs/2511.11332)
*Chaoyun Zhang,Liqun Li,He Huang,Chiming Ni,Bo Qiao,Si Qin,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.DC

TL;DR: UFO$^3$是一个跨设备任务编排系统，将异构设备统一为单一编排结构，通过TaskConstellation模型实现异步执行、自适应恢复和动态优化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理框架大多局限于单一操作系统或设备，导致跨设备工作流程脆弱且主要依赖手动操作，需要统一的跨设备编排解决方案。

Method: 使用TaskConstellation模型，将用户请求建模为可变的分布式DAG，包含原子子任务和显式控制数据依赖关系，通过Constellation Orchestrator和Agent Interaction Protocol实现安全异步执行。

Result: 在包含55个跨设备任务的NebulaBench基准测试中，UFO$^3$实现了83.3%的子任务完成率、70.9%的任务成功率，并行度平均宽度1.72，端到端延迟比顺序基线减少31%。

Conclusion: UFO$^3$实现了跨异构设备的准确、高效和弹性任务编排，将孤立代理统一为连贯的自适应计算结构，扩展了普适计算的应用范围。

Abstract: Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO$^3$, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO$^3$ models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.
  We evaluate UFO$^3$ on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO$^3$ achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO$^3$ achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.

</details>


### [8] [Beyond Exascale: Dataflow Domain Translation on a Cerebras Cluster](https://arxiv.org/abs/2511.11542)
*Tomas Oppelstrup,Nicholas Giamblanco,Delyan Z. Kalchev,Ilya Sharapov,Mark Taylor,Dirk Van Essendelft,Sivasankaran Rajamanickam,Michael James*

Main category: cs.DC

TL;DR: 提出了一种新的算法来解决物理系统模拟中的性能瓶颈，在分布式计算环境中实现了极高的模拟速度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的域分解方法在分布式计算环境中无法实现高模拟速率或高利用率，特别是在Exascale系统中只能发挥峰值性能的一小部分。

Method: 引入了新颖的算法，通过在64个Cerebras CS-3系统集群上应用浅水方程来模拟行星尺度的海啸。

Result: 实现了超过160万时间步/秒的模拟速度，达到84 PFLOP/s的计算性能，在单节点和集群环境中都能达到峰值性能的90%。

Conclusion: 该算法成功克服了传统方法的局限性，在物理系统模拟中实现了前所未有的性能和效率。

Abstract: Simulation of physical systems is essential in many scientific and engineering domains. Commonly used domain decomposition methods are unable to deliver high simulation rate or high utilization in network computing environments. In particular, Exascale systems deliver only a small fraction their peak performance for these workloads. This paper introduces the novel \algorithmpropernoun{} algorithm, designed to overcome these limitations. We apply this method and show simulations running in excess of 1.6 million time steps per second and simulations achieving 84 PFLOP/s. Our implementation can achieve 90\% of peak performance in both single-node and clustered environments. We illustrate the method by applying the shallow-water equations to model a tsunami following an asteroid impact at 460m-resolution on a planetary scale running on a cluster of 64 Cerebras CS-3 systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [Tiny Chiplets Enabled by Packaging Scaling: Opportunities in ESD Protection and Signal Integrity](https://arxiv.org/abs/2511.10760)
*Emad Haque,Pragnya Sudershan Nalla,Jeff Zhang,Sachin S. Sapatnekar,Chaitali Chakrabarti,Yu Cao*

Main category: cs.AR

TL;DR: 本文重新评估了2.5D/3D异构集成中芯片接口设计的可靠性要求，提出可以大幅简化ESD保护和芯片间信号传输，为更小尺寸的chiplet铺平道路。


<details>
  <summary>Details</summary>
Motivation: 传统I/O电路（包括ESD保护和信号传输）在先进封装技术中引入了显著的面积开销，成为将chiplet尺寸减小到100mm²以下的主要限制因素。

Method: 通过寄生提取和SPICE仿真，从芯片接口设计的角度重新审视可靠性要求。

Result: 研究表明在未来的2.5D/3D封装技术中，ESD保护和芯片间信号传输可以大幅简化。

Conclusion: 这种简化为进一步chiplet小型化铺平了道路，提高了微小chiplet的可组合性和可重用性。

Abstract: The scaling of advanced packaging technologies provides abundant interconnection resources for 2.5D/3D heterogeneous integration (HI), thereby enabling the construction of larger-scale VLSI systems with higher energy efficiency in data movement. However, conventional I/O circuitry, including electrostatic discharge (ESD) protection and signaling, introduces significant area overhead. Prior studies have identified this overhead as a major constraint in reducing chiplet size below 100 mm2. In this study, we revisit reliability requirements from the perspective of chiplet interface design. Through parasitic extraction and SPICE simulations, we demonstrate that ESD protection and inter-chiplet signaling can be substantially simplified in future 2.5D/3D packaging technologies. Such simplification, in turn, paves the road for further chiplet miniaturization and improves the composability and reusability of tiny chiplets.

</details>


### [10] [MMA-Sim: Bit-Accurate Reference Model of Tensor Cores and Matrix Cores](https://arxiv.org/abs/2511.10909)
*Peichen Xie,Yang Wang,Fan Yang,Mao Yang*

Main category: cs.AR

TL;DR: MMA-Sim是首个精确到比特级别的参考模型，揭示了10种GPU架构中矩阵乘法加速器的详细算术行为，通过目标测试和随机测试推导出9种算法来模拟浮点矩阵乘法。


<details>
  <summary>Details</summary>
Motivation: 现代GPU中的矩阵乘法加速器由于不同的未文档化浮点矩阵乘法算术规范，可能导致数值不精确和不一致，影响DNN训练和推理的稳定性和可重现性。

Method: 结合目标测试和随机测试的方法，剖析矩阵乘法加速器的算术行为，推导出9种算法来模拟浮点矩阵乘法。

Result: 大规模验证确认MMA-Sim与真实硬件在比特级别完全等价，并识别出影响DNN训练稳定性的算术行为和可能导致显著错误的未文档化行为。

Conclusion: MMA-Sim提供了对GPU矩阵乘法加速器算术行为的深入理解，有助于解决DNN训练中的数值稳定性和可重现性问题。

Abstract: The rapidly growing computation demands of deep neural networks (DNNs) have driven hardware vendors to integrate matrix multiplication accelerators (MMAs), such as NVIDIA Tensor Cores and AMD Matrix Cores, into modern GPUs. However, due to distinct and undocumented arithmetic specifications for floating-point matrix multiplication, some MMAs can lead to numerical imprecision and inconsistency that can compromise the stability and reproducibility of DNN training and inference.
  This paper presents MMA-Sim, the first bit-accurate reference model that reveals the detailed arithmetic behaviors of the MMAs from ten GPU architectures (eight from NVIDIA and two from AMD). By dissecting the MMAs using a combination of targeted and randomized tests, our methodology derives nine arithmetic algorithms to simulate the floating-point matrix multiplication of the MMAs. Large-scale validation confirms bitwise equivalence between MMA-Sim and the real hardware. Using MMA-Sim, we investigate arithmetic behaviors that affect DNN training stability, and identify undocumented behaviors that could lead to significant errors.

</details>


### [11] [T-MAN: Enabling End-to-End Low-Bit LLM Inference on NPUs via Unified Table Lookup](https://arxiv.org/abs/2511.11248)
*Jianyu Wei,Qingtao Li,Shijie Cao,Lingxiao Ma,Zixu Hao,Yanyong Zhang,Xiaoyan Hu,Ting Cao*

Main category: cs.AR

TL;DR: 提出T-MAC方法，通过查表技术解决NPU上LLM推理性能差的问题，在预填充和解码阶段分别实现1.4倍和3.1倍加速，节省84%能耗。


<details>
  <summary>Details</summary>
Motivation: 当前NPU在LLM推理中性能不如CPU，主要因为NPU在非GEMM运算（如反量化）上表现不佳。现有方法要么分离预填充和解码到不同硬件，要么在NPU上运行但损失精度。

Method: 利用低比特位将目标计算编码到可接受大小的表中，通过查表替代硬件不支持的运算。具体包括：(1)融合两级表基反量化；(2)并发层次引导的平铺；(3)三阶段流水线实现预填充；(4)将基于查表的解码映射到NPU向量单元。

Result: 相比基线NPU方法，预填充阶段加速1.4倍，解码阶段加速3.1倍，能耗节省84%。

Conclusion: T-MAC方法通过查表技术有效解决了NPU上LLM推理的性能瓶颈，在保持精度的同时显著提升了推理速度和能效。

Abstract: Large language models (LLMs) are increasingly deployed on customer devices. To support them, current devices are adopting SoCs (System on Chip) with NPUs (Neural Processing Unit) installed. Although high performance is expected, LLM inference on NPUs is slower than its CPU counterpart. The reason is that NPUs have poor performance on computations other than GEMM, like dequantization. Current works either disaggregate prefill on the NPUs and decoding on the CPUs, or put both on the NPUs but with an accuracy loss. To solve this issue, based on the insight that low-bit can enable target computation encoded within an acceptably sized table, we propose table lookup to subsume hardware operations otherwise unsupported. To realize this, we overcome the conflicting hardware behavior of prefill and decoding to design a unified table layout and tiling through (1) fused two-level table-based dequantization and (2) concurrency-hierarchy-guided tiling. Based on that, we implement the prefill phase by three-stage pipeline and map the table-lookup-based decoding to NPU's vector units. Results show 1.4x and 3.1x speedup for prefill and decoding respectively, and 84% energy savings compared to the baseline NPU methods. The code is available at https://github.com/microsoft/T-MAC/tree/main/t-man.

</details>
