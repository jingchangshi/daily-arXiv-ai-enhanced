{"id": "2512.10748", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.10748", "abs": "https://arxiv.org/abs/2512.10748", "authors": ["Cass Alexandru", "Henning Urbat", "Thorsten Wi\u00dfmann"], "title": "Intrinsically Correct Algorithms and Recursive Coalgebras", "comment": null, "summary": "Recursive coalgebras provide an elegant categorical tool for modelling recursive algorithms and analysing their termination and correctness. By considering coalgebras over categories of suitably indexed families, the correctness of the corresponding algorithms follows intrinsically just from the type of the computed maps. However, proving recursivity of the underlying coalgebras is non-trivial, and proofs are typically ad hoc. This layer of complexity impedes the formalization of coalgebraically defined recursive algorithms in proof assistants. We introduce a framework for constructing coalgebras which are intrinsically recursive in the sense that the type of the coalgebra guarantees recursivity from the outset. Our approach is based on the novel concept of a well-founded functor on a category of families indexed by a well-founded relation. We show as our main result that every coalgebra for a well-founded functor is recursive, and demonstrate that well-known techniques for proving recursivity and termination such as ranking functions are subsumed by this abstract setup. We present a number of case studies, including Quicksort, the Euclidian algorithm, and CYK parsing. Both the main theoretical result and selected case studies have been formalized in Cubical Agda.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u826f\u57fa\u51fd\u5b50\u7684\u6846\u67b6\uff0c\u81ea\u52a8\u4fdd\u8bc1\u9012\u5f52\u6027\uff0c\u7b80\u5316\u9012\u5f52\u7b97\u6cd5\u5728\u8bc1\u660e\u52a9\u624b\u7684\u5f62\u5f0f\u5316", "motivation": "\u9012\u5f52\u4f59\u4ee3\u6570\u867d\u80fd\u4f18\u96c5\u5efa\u6a21\u9012\u5f52\u7b97\u6cd5\uff0c\u4f46\u8bc1\u660e\u5176\u9012\u5f52\u6027\u901a\u5e38\u9700\u8981\u7279\u8bbe\u65b9\u6cd5\uff0c\u8fd9\u963b\u788d\u4e86\u5728\u8bc1\u660e\u52a9\u624b\u4e2d\u7684\u5f62\u5f0f\u5316", "method": "\u5f15\u5165\u826f\u57fa\u51fd\u5b50\u6982\u5ff5\uff0c\u5728\u826f\u57fa\u5173\u7cfb\u7d22\u5f15\u7684\u65cf\u8303\u7574\u4e0a\u5de5\u4f5c\uff0c\u8bc1\u660e\u826f\u57fa\u51fd\u5b50\u7684\u6bcf\u4e2a\u4f59\u4ee3\u6570\u90fd\u662f\u9012\u5f52\u7684", "result": "\u4e3b\u8981\u7406\u8bba\u7ed3\u679c\uff1a\u6bcf\u4e2a\u826f\u57fa\u51fd\u5b50\u7684\u4f59\u4ee3\u6570\u90fd\u662f\u9012\u5f52\u7684\uff1b\u5df2\u5f62\u5f0f\u5316\u5728Cubical Agda\u4e2d\uff1b\u6848\u4f8b\u5305\u62ec\u5feb\u901f\u6392\u5e8f\u3001\u6b27\u51e0\u91cc\u5f97\u7b97\u6cd5\u3001CYK\u89e3\u6790", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u7c7b\u578b\u4fdd\u8bc1\u9012\u5f52\u6027\uff0c\u7b80\u5316\u4e86\u9012\u5f52\u7b97\u6cd5\u7684\u5f62\u5f0f\u5316\uff0c\u7edf\u4e00\u4e86\u591a\u79cd\u9012\u5f52\u6027\u8bc1\u660e\u6280\u672f"}}
{"id": "2512.10861", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.10861", "abs": "https://arxiv.org/abs/2512.10861", "authors": ["Cade Lueker", "Andrew Fox", "Bor-Yuh Evan Chang"], "title": "Towards Cumulative Abstract Semantics via Handlers", "comment": null, "summary": "We consider the problem of modularizing control flow in a generic abstract interpretation framework. A generic abstract interpretation framework is not truly flexible if it does not allow interpreting with different path- and flow-sensitivities, by going forwards or backwards, and over- or under-approximately. Most interpreters inherently intertwine syntax and semantics, making the implementation antagonistic to modularity. Current approaches to modular designs require the use of complex data structures (e.g., monad transformers), providing modularity but often proving unwieldy (e.g., lifts). We observe that leveraging scoped effects within an interpreter facilitates the accumulation of semantic fragments against a fixed syntax. In this paper, we define cumulative abstract semantics, illustrating the potential for creating multiple dynamic evaluators and static analyses from one interpreter. This modularity is achieved by grouping effects into two categories: syntax elimination and domain-semantic introduction handlers. Our contribution shows the benefits of using effects as an instrument for designing a clean, elegant, and modular abstract interpretation framework.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4f5c\u7528\u57df\u6548\u5e94\u7684\u7d2f\u79ef\u62bd\u8c61\u8bed\u4e49\u6846\u67b6\uff0c\u5b9e\u73b0\u63a7\u5236\u6d41\u6a21\u5757\u5316\uff0c\u652f\u6301\u591a\u79cd\u8def\u5f84/\u6d41\u654f\u611f\u6027\u548c\u65b9\u5411\u7684\u5206\u6790", "motivation": "\u73b0\u6709\u62bd\u8c61\u89e3\u91ca\u6846\u67b6\u7f3a\u4e4f\u771f\u6b63\u7684\u7075\u6d3b\u6027\uff0c\u96be\u4ee5\u652f\u6301\u4e0d\u540c\u8def\u5f84/\u6d41\u654f\u611f\u6027\u3001\u5206\u6790\u65b9\u5411\u548c\u8fd1\u4f3c\u65b9\u5f0f\uff0c\u4e14\u8bed\u6cd5\u4e0e\u8bed\u4e49\u7d27\u5bc6\u8026\u5408\uff0c\u5b9e\u73b0\u7f3a\u4e4f\u6a21\u5757\u6027", "method": "\u5229\u7528\u4f5c\u7528\u57df\u6548\u5e94\u5c06\u6548\u5e94\u5206\u4e3a\u4e24\u7c7b\uff1a\u8bed\u6cd5\u6d88\u9664\u5904\u7406\u5668\u548c\u9886\u57df\u8bed\u4e49\u5f15\u5165\u5904\u7406\u5668\uff0c\u901a\u8fc7\u7d2f\u79ef\u62bd\u8c61\u8bed\u4e49\u5b9e\u73b0\u5355\u4e00\u89e3\u91ca\u5668\u652f\u6301\u591a\u79cd\u52a8\u6001\u8bc4\u4f30\u548c\u9759\u6001\u5206\u6790", "result": "\u8bbe\u8ba1\u51fa\u5e72\u51c0\u3001\u4f18\u96c5\u3001\u6a21\u5757\u5316\u7684\u62bd\u8c61\u89e3\u91ca\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u4e00\u89e3\u91ca\u5668\u521b\u5efa\u591a\u4e2a\u52a8\u6001\u8bc4\u4f30\u5668\u548c\u9759\u6001\u5206\u6790\u5668", "conclusion": "\u6548\u5e94\u4f5c\u4e3a\u8bbe\u8ba1\u5de5\u5177\u80fd\u591f\u5b9e\u73b0\u6e05\u6670\u3001\u4f18\u96c5\u3001\u6a21\u5757\u5316\u7684\u62bd\u8c61\u89e3\u91ca\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u6846\u67b6\u7684\u7075\u6d3b\u6027\u548c\u6a21\u5757\u6027\u95ee\u9898"}}
{"id": "2512.10089", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.10089", "abs": "https://arxiv.org/abs/2512.10089", "authors": ["Jeongeun Kim", "Sabrina Yarzada", "Paul Chen", "Christopher Torng"], "title": "Algorithm-Driven On-Chip Integration for High Density and Low Cost", "comment": null, "summary": "Growing interest in semiconductor workforce development has generated demand for platforms capable of supporting large numbers of independent hardware designs for research and training without imposing high per-project overhead. Traditional multi-project wafer (MPW) services based solely on physical co-placement have historically met this need, yet their scalability breaks down as project counts rise. Recent efforts towards scalable chip tapeouts mitigate these limitations by integrating many small designs within a shared die and attempt to amortize costly resources such as IO pads and memory macros. However, foundational principles for arranging, linking, and validating such densely integrated design sites have received limited systematic investigation. This work presents a new approach with three key techniques to address this gap. First, we establish a structured formulation of the design space that enables automated, algorithm-driven packing of many projects, replacing manual layout practices. Second, we introduce an architecture that exploits only the narrow-area regions between sites to deliver on off-chip communication and other shared needs. Third, we provide a practical approach for on-chip power domains enabling per-project power characterization at a standard laboratory bench and requiring no expertise in low-power ASIC design. Experimental results show that our approach achieves substantial area reductions of up to 13x over state-of-the-art physical-only aggregation methods, offering a scalable and cost-effective path forward for large-scale tapeout environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u534a\u5bfc\u4f53\u591a\u9879\u76ee\u6676\u5706\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5e03\u5c40\u3001\u7a84\u533a\u57df\u4e92\u8fde\u67b6\u6784\u548c\u7247\u4e0a\u7535\u6e90\u57df\u6280\u672f\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u72ec\u7acb\u786c\u4ef6\u8bbe\u8ba1\u7684\u9ad8\u6548\u96c6\u6210\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u9762\u79ef\u51cf\u5c11\u9ad8\u8fbe13\u500d\u3002", "motivation": "\u968f\u7740\u534a\u5bfc\u4f53\u4eba\u624d\u57f9\u517b\u9700\u6c42\u7684\u589e\u957f\uff0c\u9700\u8981\u652f\u6301\u5927\u91cf\u72ec\u7acb\u786c\u4ef6\u8bbe\u8ba1\u7684\u7814\u7a76\u548c\u57f9\u8bad\u5e73\u53f0\u3002\u4f20\u7edf\u7684\u591a\u9879\u76ee\u6676\u5706\u670d\u52a1\u57fa\u4e8e\u7269\u7406\u5171\u7f6e\uff0c\u5728\u9879\u76ee\u6570\u91cf\u589e\u52a0\u65f6\u6269\u5c55\u6027\u53d7\u9650\u3002\u73b0\u6709\u65b9\u6cd5\u5c1d\u8bd5\u5c06\u591a\u4e2a\u5c0f\u8bbe\u8ba1\u96c6\u6210\u5230\u5171\u4eab\u82af\u7247\u4e2d\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u5e03\u5c40\u3001\u8fde\u63a5\u548c\u9a8c\u8bc1\u539f\u5219\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u5173\u952e\u6280\u672f\uff1a1) \u5efa\u7acb\u7ed3\u6784\u5316\u8bbe\u8ba1\u7a7a\u95f4\u516c\u5f0f\u5316\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u7b97\u6cd5\u9a71\u52a8\u7684\u9879\u76ee\u6253\u5305\uff0c\u66ff\u4ee3\u624b\u52a8\u5e03\u5c40\uff1b2) \u5f15\u5165\u4ec5\u5229\u7528\u8bbe\u8ba1\u7ad9\u70b9\u95f4\u7a84\u533a\u57df\u8fdb\u884c\u7247\u5916\u901a\u4fe1\u548c\u5176\u4ed6\u5171\u4eab\u9700\u6c42\u7684\u67b6\u6784\uff1b3) \u63d0\u4f9b\u7247\u4e0a\u7535\u6e90\u57df\u5b9e\u7528\u65b9\u6cd5\uff0c\u652f\u6301\u6bcf\u4e2a\u9879\u76ee\u5728\u6807\u51c6\u5b9e\u9a8c\u5ba4\u5de5\u4f5c\u53f0\u4e0a\u8fdb\u884c\u7535\u6e90\u7279\u6027\u5206\u6790\uff0c\u65e0\u9700\u4f4e\u529f\u8017ASIC\u8bbe\u8ba1\u4e13\u4e1a\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u7eaf\u7269\u7406\u805a\u5408\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8fbe13\u500d\u7684\u9762\u79ef\u51cf\u5c11\uff0c\u4e3a\u5927\u89c4\u6a21\u6d41\u7247\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u9ad8\u5bc6\u5ea6\u96c6\u6210\u8bbe\u8ba1\u7ad9\u70b9\u5e03\u5c40\u3001\u8fde\u63a5\u548c\u9a8c\u8bc1\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u7a7a\u767d\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u3001\u7a84\u533a\u57df\u4e92\u8fde\u548c\u7535\u6e90\u57df\u7ba1\u7406\u6280\u672f\uff0c\u4e3a\u5927\u89c4\u6a21\u534a\u5bfc\u4f53\u4eba\u624d\u57f9\u517b\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6d41\u7247\u5e73\u53f0\u3002"}}
{"id": "2512.09942", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09942", "abs": "https://arxiv.org/abs/2512.09942", "authors": ["Zhiming Liang", "Bin Chen", "Litao Ye", "Chen Sun", "Shuo Wang", "Zhe Peng"], "title": "A study of the spectrum resource leasing method based on ERC4907 extension", "comment": null, "summary": "The ERC4907 standard enables rentable Non-Fungible Tokens (NFTs) but is limited to single-user, single-time-slot authorization, which severely limits its applicability and efficiency in decentralized multi-slot scheduling scenarios. To address this limitation, this paper proposes Multi-slot ERC4907 (M-ERC4907) extension method. The M-ERC4907 method introduces novel functionalities to support the batch configuration of multiple time slots and simultaneous authorization of multiple users, thereby effectively eliminating the rigid sequential authorization constraint of ERC4907. The experiment was conducted on the Remix development platform. Experimental results show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency.", "AI": {"tldr": "\u63d0\u51faM-ERC4907\u6269\u5c55\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u65f6\u95f4\u69fd\u6279\u91cf\u914d\u7f6e\u548c\u591a\u7528\u6237\u540c\u65f6\u6388\u6743\uff0c\u89e3\u51b3ERC4907\u5355\u7528\u6237\u5355\u65f6\u95f4\u69fd\u9650\u5236\uff0c\u663e\u8457\u964d\u4f4e\u94fe\u4e0a\u4ea4\u6613\u548cGas\u6d88\u8017", "motivation": "ERC4907\u6807\u51c6\u867d\u7136\u652f\u6301\u53ef\u79df\u8d41NFT\uff0c\u4f46\u4ec5\u9650\u4e8e\u5355\u7528\u6237\u3001\u5355\u65f6\u95f4\u69fd\u6388\u6743\uff0c\u5728\u53bb\u4e2d\u5fc3\u5316\u591a\u69fd\u8c03\u5ea6\u573a\u666f\u4e2d\u9002\u7528\u6027\u548c\u6548\u7387\u4e25\u91cd\u53d7\u9650", "method": "\u63d0\u51faM-ERC4907\u6269\u5c55\u65b9\u6cd5\uff0c\u5f15\u5165\u652f\u6301\u591a\u65f6\u95f4\u69fd\u6279\u91cf\u914d\u7f6e\u548c\u591a\u7528\u6237\u540c\u65f6\u6388\u6743\u7684\u65b0\u529f\u80fd\uff0c\u6d88\u9664ERC4907\u7684\u521a\u6027\u987a\u5e8f\u6388\u6743\u7ea6\u675f", "result": "\u5728Remix\u5f00\u53d1\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cM-ERC4907\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u94fe\u4e0a\u4ea4\u6613\u548c\u603b\u4f53Gas\u6d88\u8017\uff0c\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u548c\u8d44\u6e90\u5206\u914d\u6548\u7387", "conclusion": "M-ERC4907\u6269\u5c55\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86ERC4907\u5728\u53bb\u4e2d\u5fc3\u5316\u591a\u69fd\u8c03\u5ea6\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u652f\u6301\u6279\u91cf\u914d\u7f6e\u548c\u540c\u65f6\u6388\u6743\u63d0\u9ad8\u4e86NFT\u79df\u8d41\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027"}}
{"id": "2512.10155", "categories": ["cs.AR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10155", "abs": "https://arxiv.org/abs/2512.10155", "authors": ["Jeongeun Kim", "Christopher Torng"], "title": "A Vertically Integrated Framework for Templatized Chip Design", "comment": null, "summary": "Developers who primarily engage with software often struggle to incorporate custom hardware into their applications, even though specialized silicon can provide substantial benefits to machine learning and AI, as well as to the application domains that they enable. This work investigates how a chip can be generated from a high-level object-oriented software specification, targeting introductory-level chip design learners with only very light performance requirements, while maintaining mental continuity between the chip layout and the software source program. In our approach, each software object is represented as a corresponding region on the die, producing a one-to-one structural mapping that preserves these familiar abstractions throughout the design flow. To support this mapping, we employ a modular construction strategy in which vertically composed IP blocks implement the behavioral protocols expressed in software. A direct syntactic translation, however, cannot meet hardware-level efficiency or communication constraints. For this reason, we leverage formal type systems based on sequences that check whether interactions between hardware modules adhere to the communication patterns described in the software model. We further examine hardware interconnect strategies for composing many such modules and develop layout techniques suited to this object-aligned design style. Together, these contributions preserve mental continuity from software to chip design for new learners and enables practical layout generation, ultimately reducing the expertise required for software developers to participate in chip creation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u9ad8\u7ea7\u9762\u5411\u5bf9\u8c61\u8f6f\u4ef6\u89c4\u8303\u751f\u6210\u82af\u7247\u7684\u65b9\u6cd5\uff0c\u4e3a\u5165\u95e8\u7ea7\u82af\u7247\u8bbe\u8ba1\u5b66\u4e60\u8005\u63d0\u4f9b\u8f6f\u4ef6\u5230\u786c\u4ef6\u7684\u76f4\u89c2\u6620\u5c04\uff0c\u964d\u4f4e\u8f6f\u4ef6\u5f00\u53d1\u8005\u53c2\u4e0e\u82af\u7247\u8bbe\u8ba1\u7684\u95e8\u69db\u3002", "motivation": "\u8f6f\u4ef6\u5f00\u53d1\u8005\u901a\u5e38\u96be\u4ee5\u5c06\u5b9a\u5236\u786c\u4ef6\u96c6\u6210\u5230\u5e94\u7528\u4e2d\uff0c\u5c3d\u7ba1\u4e13\u7528\u82af\u7247\u80fd\u4e3a\u673a\u5668\u5b66\u4e60\u548cAI\u7b49\u9886\u57df\u5e26\u6765\u663e\u8457\u4f18\u52bf\u3002\u9700\u8981\u964d\u4f4e\u82af\u7247\u8bbe\u8ba1\u95e8\u69db\uff0c\u8ba9\u8f6f\u4ef6\u5f00\u53d1\u8005\u80fd\u591f\u53c2\u4e0e\u82af\u7247\u521b\u5efa\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u6784\u5efa\u7b56\u7565\uff0c\u5c06\u8f6f\u4ef6\u5bf9\u8c61\u6620\u5c04\u4e3a\u82af\u7247\u4e0a\u7684\u5bf9\u5e94\u533a\u57df\uff0c\u5b9e\u73b0\u4e00\u4e00\u5bf9\u5e94\u7684\u7ed3\u6784\u6620\u5c04\u3002\u4f7f\u7528\u57fa\u4e8e\u5e8f\u5217\u7684\u5f62\u5f0f\u7c7b\u578b\u7cfb\u7edf\u68c0\u67e5\u786c\u4ef6\u6a21\u5757\u95f4\u7684\u901a\u4fe1\u6a21\u5f0f\u662f\u5426\u7b26\u5408\u8f6f\u4ef6\u6a21\u578b\u63cf\u8ff0\uff0c\u5e76\u5f00\u53d1\u9002\u5408\u8fd9\u79cd\u5bf9\u8c61\u5bf9\u9f50\u8bbe\u8ba1\u98ce\u683c\u7684\u5e03\u5c40\u6280\u672f\u3002", "result": "\u5b9e\u73b0\u4e86\u4ece\u8f6f\u4ef6\u5230\u82af\u7247\u8bbe\u8ba1\u7684\u5fc3\u7406\u8fde\u7eed\u6027\u4fdd\u6301\uff0c\u4e3a\u65b0\u624b\u5b66\u4e60\u8005\u63d0\u4f9b\u4e86\u76f4\u89c2\u7684\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u5e76\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u5e03\u5c40\u751f\u6210\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u8f6f\u4ef6\u5f00\u53d1\u8005\u53c2\u4e0e\u82af\u7247\u8bbe\u8ba1\u6240\u9700\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u901a\u8fc7\u4fdd\u6301\u8f6f\u4ef6\u62bd\u8c61\u5728\u786c\u4ef6\u8bbe\u8ba1\u6d41\u7a0b\u4e2d\u7684\u8fde\u7eed\u6027\uff0c\u4f7f\u8f6f\u4ef6\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u8f7b\u677e\u5730\u521b\u5efa\u5b9a\u5236\u82af\u7247\u3002"}}
{"id": "2512.09946", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09946", "abs": "https://arxiv.org/abs/2512.09946", "authors": ["Hung-Yueh Chiang", "Bokun Wang", "Diana Marculescu"], "title": "ELANA: A Simple Energy and Latency Analyzer for LLMs", "comment": null, "summary": "The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \\textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.", "AI": {"tldr": "ELANA\u662f\u4e00\u4e2a\u5f00\u6e90\u8f7b\u91cf\u7ea7LLM\u6027\u80fd\u5206\u6790\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5927\u5c0f\u3001KV\u7f13\u5b58\u3001\u63a8\u7406\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u652f\u6301Hugging Face\u6240\u6709\u516c\u5f00\u6a21\u578b\u548c\u591aGPU/\u8fb9\u7f18GPU\u5e73\u53f0\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u7c7b\u786c\u4ef6\u5e73\u53f0\uff08\u4ece\u79fb\u52a8\u8fb9\u7f18\u8bbe\u5907\u5230\u4e91\u7aefGPU\u96c6\u7fa4\uff09\u4e0a\u7684\u5ef6\u8fdf\u548c\u529f\u8017\u662f\u4e3b\u8981\u7ea6\u675f\uff0c\u9700\u8981\u6709\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u6765\u4f18\u5316\u6a21\u578b\u90e8\u7f72\u6548\u7387\u548c\u4e0b\u4e00\u4ee3\u6a21\u578b\u5f00\u53d1\u3002", "method": "\u5f00\u53d1\u4e86ELANA\u8fd9\u4e00\u8f7b\u91cf\u7ea7\u5b66\u672f\u53cb\u597d\u578b\u5206\u6790\u5de5\u5177\uff0c\u652f\u6301\u5206\u6790\u6a21\u578b\u5927\u5c0f\u3001KV\u7f13\u5b58\u5927\u5c0f\u3001\u9884\u586b\u5145\u5ef6\u8fdf\uff08TTFT\uff09\u3001\u751f\u6210\u5ef6\u8fdf\uff08TPOT\uff09\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\uff08TTLT\uff09\uff0c\u517c\u5bb9Hugging Face\u6240\u6709\u516c\u5f00\u6a21\u578b\uff0c\u63d0\u4f9b\u7b80\u5355\u547d\u4ee4\u884c\u754c\u9762\u548c\u53ef\u9009\u80fd\u8017\u65e5\u5fd7\u529f\u80fd\u3002", "result": "\u5f00\u6e90\u53d1\u5e03\u4e86ELANA\u5206\u6790\u5de5\u5177\uff0c\u652f\u6301\u591aGPU\u548c\u8fb9\u7f18GPU\u5e73\u53f0\uff0c\u5b8c\u5168\u517c\u5bb9\u6d41\u884c\u7684Hugging Face API\uff0c\u53ef\u8f7b\u677e\u5b9a\u5236\u6216\u9002\u914d\u538b\u7f29\u6216\u4f4e\u6bd4\u7279\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u9ad8\u6548LLM\u7814\u7a76\u6216\u5c0f\u89c4\u6a21\u6982\u5ff5\u9a8c\u8bc1\u7814\u7a76\u3002", "conclusion": "ELANA\u4e3aLLM\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u5b9e\u7528\u7684\u5f00\u6e90\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u6a21\u578b\u90e8\u7f72\u6548\u7387\u548c\u4fc3\u8fdb\u9ad8\u6548LLM\u7814\u7a76\uff0c\u7279\u522b\u9002\u5408\u5b66\u672f\u7814\u7a76\u548c\u5c0f\u89c4\u6a21\u6982\u5ff5\u9a8c\u8bc1\u3002"}}
{"id": "2512.10180", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.10180", "abs": "https://arxiv.org/abs/2512.10180", "authors": ["Pracheta Harlikar", "Abdel-Hameed A. Badawy", "Prasanna Date"], "title": "Neuromorphic Processor Employing FPGA Technology with Universal Interconnections", "comment": null, "summary": "Neuromorphic computing, inspired by biological neural systems, holds immense promise for ultra-low-power and real-time inference applications. However, limited access to flexible, open-source platforms continues to hinder widespread adoption and experimentation. In this paper, we present a low-cost neuromorphic processor implemented on a Xilinx Zynq-7000 FPGA platform. The processor supports all-to-all configurable connectivity and employs the leaky integrate-and-fire (LIF) neuron model with customizable parameters such as threshold, synaptic weights, and refractory period. Communication with the host system is handled via a UART interface, enabling runtime reconfiguration without hardware resynthesis. The architecture was validated using benchmark datasets including the Iris classification and MNIST digit recognition tasks. Post-synthesis results highlight the design's energy efficiency and scalability, establishing its viability as a research-grade neuromorphic platform that is both accessible and adaptable for real-world spiking neural network applications. This implementation will be released as open source following project completion.", "AI": {"tldr": "\u5728Xilinx Zynq-7000 FPGA\u4e0a\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u5f00\u6e90\u7684\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5668\uff0c\u652f\u6301\u53ef\u914d\u7f6e\u8fde\u63a5\u548cLIF\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u901a\u8fc7UART\u63a5\u53e3\u5b9e\u73b0\u8fd0\u884c\u65f6\u91cd\u914d\u7f6e\uff0c\u9a8c\u8bc1\u4e86Iris\u548cMNIST\u6570\u636e\u96c6\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5177\u6709\u8d85\u4f4e\u529f\u8017\u548c\u5b9e\u65f6\u63a8\u7406\u7684\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u7075\u6d3b\u7684\u5f00\u6e90\u5e73\u53f0\u963b\u788d\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u548c\u5b9e\u9a8c\u7814\u7a76\u3002", "method": "\u5728Xilinx Zynq-7000 FPGA\u5e73\u53f0\u4e0a\u5b9e\u73b0\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5668\uff0c\u91c7\u7528\u53ef\u914d\u7f6e\u7684\u5168\u8fde\u63a5\u7ed3\u6784\u548cLIF\u795e\u7ecf\u5143\u6a21\u578b\uff0c\u652f\u6301\u9608\u503c\u3001\u7a81\u89e6\u6743\u91cd\u3001\u4e0d\u5e94\u671f\u7b49\u53c2\u6570\u81ea\u5b9a\u4e49\uff0c\u901a\u8fc7UART\u63a5\u53e3\u4e0e\u4e3b\u673a\u901a\u4fe1\u5b9e\u73b0\u8fd0\u884c\u65f6\u91cd\u914d\u7f6e\u3002", "result": "\u4f7f\u7528Iris\u5206\u7c7b\u548cMNIST\u6570\u5b57\u8bc6\u522b\u57fa\u51c6\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u67b6\u6784\u6709\u6548\u6027\uff0c\u7efc\u5408\u540e\u7ed3\u679c\u663e\u793a\u4e86\u8bbe\u8ba1\u7684\u80fd\u6548\u548c\u53ef\u6269\u5c55\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u7814\u7a76\u7ea7\u795e\u7ecf\u5f62\u6001\u5e73\u53f0\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u4f4e\u6210\u672c\u3001\u5f00\u6e90\u7684\u795e\u7ecf\u5f62\u6001\u5904\u7406\u5668\u4e3a\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u8bbf\u95ee\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u7814\u7a76\u5e73\u53f0\uff0c\u9879\u76ee\u5b8c\u6210\u540e\u5c06\u5f00\u6e90\u53d1\u5e03\u3002"}}
{"id": "2512.09957", "categories": ["cs.DC", "cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09957", "abs": "https://arxiv.org/abs/2512.09957", "authors": ["Bethel Hall", "Owen Ungaro", "William Eiers"], "title": "CloudFix: Automated Policy Repair for Cloud Access Control Policies Using Large Language Models", "comment": "10 pages", "summary": "Access control policies are vital for securing modern cloud computing, where organizations must manage access to sensitive data across thousands of users in distributed system settings. Cloud administrators typically write and update policies manually, which can be an error-prone and time-consuming process and can potentially lead to security vulnerabilities. Existing approaches based on symbolic analysis have demon- strated success in automated debugging and repairing access control policies; however, their generalizability is limited in the context of cloud-based access control. Conversely, Large Language Models (LLMs) have been utilized for automated program repair; however, their applicability to repairing cloud access control policies remains unexplored. In this work, we introduce CloudFix, the first automated policy repair framework for cloud access control that combines formal methods with LLMs. Given an access control policy and a specification of allowed and denied access requests, CloudFix employs Formal Methods-based Fault Localization to identify faulty statements in the policy and leverages LLMs to generate potential repairs, which are then verified using SMT solvers. To evaluate CloudFix, we curated a dataset of 282 real-world AWS access control policies extracted from forum posts and augmented them with synthetically generated request sets based on real scenarios. Our experimental results show that CloudFix improves repair accuracy over a Baseline implementation across varying request sizes. Our work is the first to leverage LLMs for policy repair, showcasing the effectiveness of LLMs for access control and enabling efficient and automated repair of cloud access control policies. We make our tool Cloudfix and AWS dataset publicly available.", "AI": {"tldr": "CloudFix\uff1a\u9996\u4e2a\u7ed3\u5408\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e0eLLM\u7684\u4e91\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\u81ea\u52a8\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u6545\u969c\u5b9a\u4f4d\u548cLLM\u751f\u6210\u4fee\u590d\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u7b56\u7565\u4fee\u590d\u51c6\u786e\u7387\u3002", "motivation": "\u4e91\u73af\u5883\u4e2d\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\u7684\u624b\u52a8\u7f16\u5199\u548c\u66f4\u65b0\u5bb9\u6613\u51fa\u9519\u4e14\u8017\u65f6\uff0c\u53ef\u80fd\u5bfc\u81f4\u5b89\u5168\u6f0f\u6d1e\u3002\u73b0\u6709\u7b26\u53f7\u5206\u6790\u65b9\u6cd5\u5728\u4e91\u8bbf\u95ee\u63a7\u5236\u573a\u666f\u4e0b\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u800cLLM\u5728\u7a0b\u5e8f\u4fee\u590d\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u63a2\u7d22\u7528\u4e8e\u4e91\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\u4fee\u590d\u3002", "method": "CloudFix\u7ed3\u5408\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e0eLLM\uff1a\u9996\u5148\u4f7f\u7528\u57fa\u4e8e\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u6545\u969c\u5b9a\u4f4d\u8bc6\u522b\u7b56\u7565\u4e2d\u7684\u9519\u8bef\u8bed\u53e5\uff0c\u7136\u540e\u5229\u7528LLM\u751f\u6210\u6f5c\u5728\u4fee\u590d\u65b9\u6848\uff0c\u6700\u540e\u901a\u8fc7SMT\u6c42\u89e3\u5668\u9a8c\u8bc1\u4fee\u590d\u7684\u6b63\u786e\u6027\u3002", "result": "\u5728\u5305\u542b282\u4e2a\u771f\u5b9eAWS\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cCloudFix\u5728\u4e0d\u540c\u8bf7\u6c42\u89c4\u6a21\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u5b9e\u73b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4fee\u590d\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5229\u7528LLM\u8fdb\u884c\u7b56\u7565\u4fee\u590d\u7684\u5de5\u4f5c\uff0c\u5c55\u793a\u4e86LLM\u5728\u8bbf\u95ee\u63a7\u5236\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u4e91\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\u7684\u9ad8\u6548\u81ea\u52a8\u4fee\u590d\u3002\u5de5\u5177\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.10231", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.10231", "abs": "https://arxiv.org/abs/2512.10231", "authors": ["Zhenguo Liu", "Chengao Shi", "Chen Ding", "Jiang Xu"], "title": "SemanticBBV: A Semantic Signature for Cross-Program Knowledge Reuse in Microarchitecture Simulation", "comment": "Accepted by ASP-DAC 2026 conference", "summary": "For decades, sampling-based techniques have been the de facto standard for accelerating microarchitecture simulation, with the Basic Block Vector (BBV) serving as the cornerstone program representation. Yet, the BBV's fundamental limitations: order-dependent IDs that prevent cross-program knowledge reuse and a lack of semantic content predictive of hardware performance have left a massive potential for optimization untapped.\n  To address these gaps, we introduce SemanticBBV, a novel, two-stage framework that generates robust, performance-aware signatures for cross-program simulation reuse. First, a lightweight RWKV-based semantic encoder transforms assembly basic blocks into rich Basic Block Embeddings (BBEs), capturing deep functional semantics. Second, an order-invariant Set Transformer aggregates these BBEs, weighted by execution frequency, into a final signature. Crucially, this stage is co-trained with a dual objective: a triplet loss for signature distinctiveness and a Cycles Per Instruction (CPI) regression task, directly imbuing the signature with performance sensitivity. Our evaluation demonstrates that SemanticBBV not only matches traditional BBVs in single-program accuracy but also enables unprecedented cross-program analysis. By simulating just 14 universal program points, we estimated the performance of ten SPEC CPU benchmarks with 86.3% average accuracy, achieving a 7143x simulation speedup. Furthermore, the signature shows strong adaptability to new microarchitectures with minimal fine-tuning.", "AI": {"tldr": "SemanticBBV\uff1a\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u7f16\u7801\u548c\u96c6\u5408\u53d8\u6362\u5668\u751f\u6210\u6027\u80fd\u611f\u77e5\u7684\u7a0b\u5e8f\u7b7e\u540d\uff0c\u5b9e\u73b0\u8de8\u7a0b\u5e8f\u6a21\u62df\u91cd\u7528\uff0c\u76f8\u6bd4\u4f20\u7edfBBV\u65b9\u6cd5\u83b7\u5f977143\u500d\u52a0\u901f", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u91c7\u6837\u7684\u5fae\u67b6\u6784\u6a21\u62df\u4f7f\u7528\u57fa\u672c\u5757\u5411\u91cf\uff08BBV\uff09\u4f5c\u4e3a\u7a0b\u5e8f\u8868\u793a\uff0c\u4f46BBV\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u9650\u5236\uff1a1\uff09\u987a\u5e8f\u4f9d\u8d56\u7684ID\u963b\u788d\u8de8\u7a0b\u5e8f\u77e5\u8bc6\u91cd\u7528\uff1b2\uff09\u7f3a\u4e4f\u9884\u6d4b\u786c\u4ef6\u6027\u80fd\u7684\u8bed\u4e49\u5185\u5bb9\uff0c\u5bfc\u81f4\u5927\u91cf\u4f18\u5316\u6f5c\u529b\u672a\u88ab\u5f00\u53d1", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u8f7b\u91cf\u7ea7RWKV\u8bed\u4e49\u7f16\u7801\u5668\u5c06\u6c47\u7f16\u57fa\u672c\u5757\u8f6c\u6362\u4e3a\u4e30\u5bcc\u7684BBE\u5d4c\u5165\uff1b2\uff09\u987a\u5e8f\u4e0d\u53d8\u7684\u96c6\u5408\u53d8\u6362\u5668\u805a\u5408BBE\uff08\u6309\u6267\u884c\u9891\u7387\u52a0\u6743\uff09\u751f\u6210\u6700\u7ec8\u7b7e\u540d\uff0c\u901a\u8fc7\u4e09\u91cd\u635f\u5931\u548cCPI\u56de\u5f52\u4efb\u52a1\u8054\u5408\u8bad\u7ec3\uff0c\u4f7f\u7b7e\u540d\u5177\u6709\u6027\u80fd\u654f\u611f\u6027", "result": "\u4ec5\u6a21\u62df14\u4e2a\u901a\u7528\u7a0b\u5e8f\u70b9\u5373\u53ef\u4f30\u8ba110\u4e2aSPEC CPU\u57fa\u51c6\u6d4b\u8bd5\u7684\u6027\u80fd\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe86.3%\uff0c\u5b9e\u73b07143\u500d\u6a21\u62df\u52a0\u901f\uff1b\u7b7e\u540d\u5bf9\u65b0\u5fae\u67b6\u6784\u5177\u6709\u5f3a\u9002\u5e94\u6027\uff0c\u53ea\u9700\u5c11\u91cf\u5fae\u8c03", "conclusion": "SemanticBBV\u89e3\u51b3\u4e86\u4f20\u7edfBBV\u7684\u6839\u672c\u9650\u5236\uff0c\u4e0d\u4ec5\u4fdd\u6301\u5355\u7a0b\u5e8f\u51c6\u786e\u6027\uff0c\u8fd8\u5b9e\u73b0\u524d\u6240\u672a\u6709\u7684\u8de8\u7a0b\u5e8f\u5206\u6790\u80fd\u529b\uff0c\u663e\u8457\u52a0\u901f\u5fae\u67b6\u6784\u6a21\u62df\uff0c\u4e3a\u6027\u80fd\u611f\u77e5\u7684\u7a0b\u5e8f\u7b7e\u540d\u5f00\u8f9f\u65b0\u65b9\u5411"}}
{"id": "2512.09961", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09961", "abs": "https://arxiv.org/abs/2512.09961", "authors": ["Jinyu Chen", "Long Shi", "Taotao Wang", "Jiaheng Wang", "Wei Zhang"], "title": "TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0", "comment": null, "summary": "The rapid growth of Web3.0 is transforming the Internet from a centralized structure to decentralized, which empowers users with unprecedented self-sovereignty over their own data. However, in the context of decentralized data access within Web3.0, it is imperative to cope with efficiency concerns caused by the replication of redundant data, as well as security vulnerabilities caused by data inconsistency. To address these challenges, we develop a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0 to ensure efficient caching and enhance system resilience against adversarial threats. This framework features a two-layer architecture, wherein the Decentralized Oracle Network (DON) layer serves as a trusted intermediary platform for decentralized caching, bridging the contents from decentralized storage and the content requests from users. In light of the complexity of Web3.0 network topologies and data flows, we propose a Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC) for TDC-Cache to dynamically optimize caching strategies of distributed oracles. Furthermore, we develop a Proof of Cooperative Learning (PoCL) consensus to maintain the consistency of decentralized caching decisions within DON. Experimental results show that, compared with existing approaches, the proposed framework reduces average access latency by 20%, increases the cache hit rate by at most 18%, and improves the average success consensus rate by 10%. Overall, this paper serves as a first foray into the investigation of decentralized caching framework and strategy for Web3.0.", "AI": {"tldr": "\u63d0\u51faTDC-Cache\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u5c42\u67b6\u6784\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548cPoCL\u5171\u8bc6\u673a\u5236\uff0c\u89e3\u51b3Web3.0\u53bb\u4e2d\u5fc3\u5316\u6570\u636e\u8bbf\u95ee\u4e2d\u7684\u6548\u7387\u548c\u5b89\u5168\u95ee\u9898\u3002", "motivation": "Web3.0\u4ece\u4e2d\u5fc3\u5316\u5411\u53bb\u4e2d\u5fc3\u5316\u8f6c\u578b\uff0c\u7528\u6237\u83b7\u5f97\u6570\u636e\u81ea\u4e3b\u6743\uff0c\u4f46\u9762\u4e34\u6570\u636e\u5197\u4f59\u590d\u5236\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898\u548c\u6570\u636e\u4e0d\u4e00\u81f4\u5e26\u6765\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u5f00\u53d1TDC-Cache\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u5c42\u67b6\u6784\uff1aDON\u5c42\u4f5c\u4e3a\u53ef\u4fe1\u4e2d\u4ecb\u5e73\u53f0\uff1b\u63d0\u51faDRL-DC\u7b97\u6cd5\u52a8\u6001\u4f18\u5316\u5206\u5e03\u5f0f\u9884\u8a00\u673a\u7f13\u5b58\u7b56\u7565\uff1b\u8bbe\u8ba1PoCL\u5171\u8bc6\u673a\u5236\u7ef4\u62a4\u7f13\u5b58\u51b3\u7b56\u4e00\u81f4\u6027\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u8bbf\u95ee\u5ef6\u8fdf\u964d\u4f4e20%\uff0c\u7f13\u5b58\u547d\u4e2d\u7387\u6700\u591a\u63d0\u534718%\uff0c\u5e73\u5747\u5171\u8bc6\u6210\u529f\u7387\u63d0\u9ad810%\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u63a2\u7d22Web3.0\u53bb\u4e2d\u5fc3\u5316\u7f13\u5b58\u6846\u67b6\u548c\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2512.10236", "categories": ["cs.DC", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10236", "abs": "https://arxiv.org/abs/2512.10236", "authors": ["Shagnik Pal", "Shaizeen Aga", "Suchita Pati", "Mahzabeen Islam", "Lizy K. John"], "title": "Design Space Exploration of DMA based Finer-Grain Compute Communication Overlap", "comment": null, "summary": "As both ML training and inference are increasingly distributed, parallelization techniques that shard (divide) ML model across GPUs of a distributed system, are often deployed. With such techniques, there is a high prevalence of data-dependent communication and computation operations where communication is exposed, leaving as high as 1.7x ideal performance on the table. Prior works harness the fact that ML model state and inputs are already sharded, and employ careful overlap of individual computation/communication shards. While such coarse-grain overlap is promising, in this work, we instead make a case for finer-grain compute-communication overlap which we term FiCCO, where we argue for finer-granularity, one-level deeper overlap than at shard-level, to unlock compute/communication overlap for a wider set of network topologies, finer-grain dataflow and more. We show that FiCCO opens up a wider design space of execution schedules than possible at shard-level alone. At the same time, decomposition of ML operations into smaller operations (done in both shard-based and finer-grain techniques) causes operation-level inefficiency losses. To balance the two, we first present a detailed characterization of these inefficiency losses, then present a design space of FiCCO schedules, and finally overlay the schedules with concomitant inefficiency signatures. Doing so helps us design heuristics that frameworks and runtimes can harness to select bespoke FiCCO schedules based on the nature of underlying ML operations. Finally, to further minimize contention inefficiencies inherent with operation overlap, we offload communication to GPU DMA engines. We evaluate several scenarios from realistic ML deployments and demonstrate that our proposed bespoke schedules deliver up to 1.6x speedup and our heuristics provide accurate guidance in 81% of unseen scenarios.", "AI": {"tldr": "\u63d0\u51faFiCCO\u65b9\u6cd5\uff0c\u901a\u8fc7\u66f4\u7ec6\u7c92\u5ea6\u7684\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\u6765\u63d0\u5347\u5206\u5e03\u5f0fML\u8bad\u7ec3/\u63a8\u7406\u6027\u80fd\uff0c\u76f8\u6bd4\u4f20\u7edf\u5206\u7247\u7ea7\u91cd\u53e0\u80fd\u9002\u5e94\u66f4\u591a\u7f51\u7edc\u62d3\u6251\u548c\u7ec6\u7c92\u5ea6\u6570\u636e\u6d41", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0fML\u5e76\u884c\u5316\u6280\u672f\u4e2d\uff0c\u6570\u636e\u4f9d\u8d56\u7684\u901a\u4fe1\u548c\u8ba1\u7b97\u64cd\u4f5c\u666e\u904d\u5b58\u5728\uff0c\u901a\u4fe1\u66b4\u9732\u5bfc\u81f4\u6027\u80fd\u635f\u5931\u9ad8\u8fbe1.7\u500d\u7406\u60f3\u6027\u80fd\u3002\u4f20\u7edf\u5206\u7247\u7ea7\u91cd\u53e0\u867d\u7136\u6709\u6548\uff0c\u4f46\u7c92\u5ea6\u8f83\u7c97\uff0c\u9650\u5236\u4e86\u8bbe\u8ba1\u7a7a\u95f4", "method": "\u63d0\u51faFiCCO\uff08Finer-grain Compute-Communication Overlap\uff09\u65b9\u6cd5\uff0c\u5728\u6bd4\u5206\u7247\u7ea7\u66f4\u7ec6\u7684\u7c92\u5ea6\u4e0a\u5b9e\u73b0\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\uff1b\u5206\u6790\u64cd\u4f5c\u5206\u89e3\u5e26\u6765\u7684\u6548\u7387\u635f\u5931\uff1b\u8bbe\u8ba1FiCCO\u8c03\u5ea6\u7a7a\u95f4\u5e76\u5efa\u7acb\u6548\u7387\u7279\u5f81\u6620\u5c04\uff1b\u5f00\u53d1\u542f\u53d1\u5f0f\u7b97\u6cd5\u9009\u62e9\u5b9a\u5236\u5316\u8c03\u5ea6\uff1b\u5229\u7528GPU DMA\u5f15\u64ce\u5378\u8f7d\u901a\u4fe1\u4ee5\u51cf\u5c11\u7ade\u4e89", "result": "\u5728\u771f\u5b9eML\u90e8\u7f72\u573a\u666f\u4e2d\uff0c\u63d0\u51fa\u7684\u5b9a\u5236\u5316\u8c03\u5ea6\u65b9\u6848\u5b9e\u73b0\u4e86\u6700\u9ad81.6\u500d\u7684\u52a0\u901f\uff0c\u542f\u53d1\u5f0f\u7b97\u6cd5\u572881%\u7684\u672a\u89c1\u573a\u666f\u4e2d\u63d0\u4f9b\u51c6\u786e\u6307\u5bfc", "conclusion": "FiCCO\u901a\u8fc7\u66f4\u7ec6\u7c92\u5ea6\u7684\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\u6269\u5c55\u4e86\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u7ed3\u5408\u6548\u7387\u635f\u5931\u5206\u6790\u548c\u542f\u53d1\u5f0f\u8c03\u5ea6\u9009\u62e9\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5206\u5e03\u5f0fML\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u6846\u67b6\u548c\u8fd0\u884c\u65f6\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc"}}
{"id": "2512.09963", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09963", "abs": "https://arxiv.org/abs/2512.09963", "authors": ["Phuong Tran", "Tzu-Hao Liu", "Long Tan Le", "Tung-Anh Nguyen", "Van Quan La", "Eason Yu", "Han Shu", "Choong Seon Hong", "Nguyen H. Tran"], "title": "GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference", "comment": "Accepted at INFOCOM 2026", "summary": "Large language models (LLMs) have revolutionized natural language processing, yet their high computational demands pose significant challenges for real-time inference, especially in multi-user server speculative decoding and resource-constrained environments. Speculative decoding has emerged as a promising technique to accelerate LLM inference by using lightweight draft models to generate candidate tokens, which are subsequently verified by a larger, more accurate model. However, ensuring both high goodput (the effective rate of accepted tokens) and fairness across multiple draft servers cooperating with a central verification server remains an open challenge. This paper introduces GOODSPEED, a novel distributed inference framework that optimizes goodput through adaptive speculative decoding. GOODSPEED employs a central verification server that coordinates a set of heterogeneous draft servers, each running a small language model to generate speculative tokens. To manage resource allocation effectively, GOODSPEED incorporates a gradient scheduling algorithm that dynamically assigns token verification tasks, maximizing a logarithmic utility function to ensure proportional fairness across servers. By processing speculative outputs from all draft servers in parallel, the framework enables efficient collaboration between the verification server and distributed draft generators, streamlining both latency and throughput. Through rigorous fluid sample path analysis, we show that GOODSPEED converges to the optimal goodput allocation in steady-state conditions and maintains near-optimal performance with provably bounded error under dynamic workloads. These results demonstrate that GOODSPEED provides a scalable, fair and efficient solution for multi- in distributed LLM inference systems.", "AI": {"tldr": "GOODSPEED\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u6d4b\u89e3\u7801\u4f18\u5316\u591a\u7528\u6237LLM\u63a8\u7406\u7684\u597d\u541e\u5410\u91cf\uff0c\u786e\u4fdd\u670d\u52a1\u5668\u95f4\u7684\u516c\u5e73\u6027\u3002", "motivation": "LLM\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u5bf9\u5b9e\u65f6\u63a8\u7406\u6784\u6210\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u7528\u6237\u670d\u52a1\u5668\u63a8\u6d4b\u89e3\u7801\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u3002\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u9ad8\u597d\u541e\u5410\u91cf\uff08\u6709\u6548\u63a5\u53d7\u4ee4\u724c\u7387\uff09\u548c\u8de8\u591a\u4e2a\u8349\u7a3f\u670d\u52a1\u5668\u7684\u516c\u5e73\u6027\u3002", "method": "GOODSPEED\u91c7\u7528\u4e2d\u5fc3\u9a8c\u8bc1\u670d\u52a1\u5668\u534f\u8c03\u5f02\u6784\u8349\u7a3f\u670d\u52a1\u5668\u7684\u67b6\u6784\uff0c\u4f7f\u7528\u68af\u5ea6\u8c03\u5ea6\u7b97\u6cd5\u52a8\u6001\u5206\u914d\u4ee4\u724c\u9a8c\u8bc1\u4efb\u52a1\uff0c\u6700\u5927\u5316\u5bf9\u6570\u6548\u7528\u51fd\u6570\u4ee5\u786e\u4fdd\u6bd4\u4f8b\u516c\u5e73\u6027\uff0c\u5e76\u884c\u5904\u7406\u6240\u6709\u8349\u7a3f\u670d\u52a1\u5668\u7684\u63a8\u6d4b\u8f93\u51fa\u3002", "result": "\u901a\u8fc7\u4e25\u683c\u7684\u6d41\u4f53\u6837\u672c\u8def\u5f84\u5206\u6790\uff0cGOODSPEED\u5728\u7a33\u6001\u6761\u4ef6\u4e0b\u6536\u655b\u5230\u6700\u4f18\u597d\u541e\u5410\u91cf\u5206\u914d\uff0c\u5728\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u4e14\u6709\u754c\u8bef\u5dee\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u516c\u5e73\u4e14\u9ad8\u6548\u7684\u5206\u5e03\u5f0fLLM\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "GOODSPEED\u4e3a\u591a\u7528\u6237\u5206\u5e03\u5f0fLLM\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u516c\u5e73\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u6d4b\u89e3\u7801\u4f18\u5316\u597d\u541e\u5410\u91cf\u5e76\u786e\u4fdd\u670d\u52a1\u5668\u95f4\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2512.10271", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10271", "abs": "https://arxiv.org/abs/2512.10271", "authors": ["Shruti Dongare", "Redwan Ibne Seraj Khan", "Hadeel Albahar", "Nannan Zhao", "Diego Melendez Maita", "Ali R. Butt"], "title": "Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters", "comment": null, "summary": "Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.", "AI": {"tldr": "RLTune\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u8c03\u5ea6\u6846\u67b6\uff0c\u80fd\u591f\u5728\u5f02\u6784GPU\u96c6\u7fa4\u4e0a\u52a8\u6001\u4f18\u5316\u4efb\u52a1\u4f18\u5148\u7ea7\u548c\u8d44\u6e90\u5206\u914d\uff0c\u65e0\u9700\u9488\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u8fdb\u884c\u6027\u80fd\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347GPU\u5229\u7528\u7387\u5e76\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u6392\u961f\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u4ee3\u4e91\u5e73\u53f0\u627f\u8f7d\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u9700\u8981\u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u8fdf\u7684GPU\u8c03\u5ea6\u3002\u7136\u800c\uff0cGPU\u96c6\u7fa4\u7684\u5f02\u6784\u6027\u65e5\u76ca\u589e\u5f3a\uff0c\u4ee5\u53ca\u5bf9\u5e94\u7528\u7279\u6027\u7684\u6709\u9650\u53ef\u89c1\u6027\uff0c\u7ed9\u73b0\u6709\u8c03\u5ea6\u5668\u5e26\u6765\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u8c03\u5ea6\u5668\u901a\u5e38\u4f9d\u8d56\u79bb\u7ebf\u6027\u80fd\u5206\u6790\u6216\u5e94\u7528\u7279\u5b9a\u5047\u8bbe\u3002", "method": "RLTune\u91c7\u7528\u5e94\u7528\u65e0\u5173\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408RL\u9a71\u52a8\u7684\u4f18\u5148\u7ea7\u8c03\u5ea6\u548c\u57fa\u4e8e\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u7684\u4efb\u52a1\u5230\u8282\u70b9\u6620\u5c04\uff0c\u4f18\u5316\u7cfb\u7edf\u7ea7\u76ee\u6807\u5982\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3001\u6392\u961f\u5ef6\u8fdf\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002\u6846\u67b6\u57fa\u4e8e\u5fae\u8f6fPhilly\u3001Helios\u548c\u963f\u91cc\u5df4\u5df4\u7684\u5927\u89c4\u6a21\u751f\u4ea7\u8f68\u8ff9\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "RLTune\u5c06GPU\u5229\u7528\u7387\u63d0\u5347\u9ad8\u8fbe20%\uff0c\u6392\u961f\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe81%\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed\u9ad8\u8fbe70%\u3002\u8be5\u6846\u67b6\u80fd\u591f\u6cdb\u5316\u5230\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u65e0\u9700\u9488\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u8fdb\u884c\u6027\u80fd\u5206\u6790\u3002", "conclusion": "RLTune\u4e3a\u4e91\u63d0\u4f9b\u5546\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u3001\u53ef\u6269\u5c55\u7684\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u516c\u5e73\u548c\u53ef\u6301\u7eed\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7ba1\u7406\uff0c\u514b\u670d\u4e86\u73b0\u6709\u8c03\u5ea6\u5668\u5728\u5f02\u6784\u96c6\u7fa4\u548c\u5e94\u7528\u7279\u6027\u53ef\u89c1\u6027\u65b9\u9762\u7684\u9650\u5236\u3002"}}
{"id": "2512.10312", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10312", "abs": "https://arxiv.org/abs/2512.10312", "authors": ["Julian Rodriguez", "Piotr Lopez", "Emiliano Lerma", "Rafael Medrano", "Jacobo Hernandez"], "title": "High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments", "comment": "8 pages, 3 figures", "summary": "This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.", "AI": {"tldr": "\u5927\u6570\u636e\u8bfe\u7a0b\u5b9e\u8df5\u62a5\u544a\uff0c\u6db5\u76d6\u6570\u636e\u5904\u7406\u3001\u6587\u672c\u5206\u6790\u3001\u7535\u5f71\u7279\u5f81\u5206\u6790\u548cSpark\u96c6\u7fa4\u90e8\u7f72", "motivation": "\u8bb0\u5f55\u5927\u6570\u636e\u8bfe\u7a0b\u4e2d\u7684\u5b9e\u8df5\u6d41\u7a0b\u548c\u65b9\u6cd5\u8bba\uff0c\u5c55\u793a\u4ece\u6570\u636e\u5904\u7406\u5230\u5206\u5e03\u5f0f\u8ba1\u7b97\u96c6\u7fa4\u90e8\u7f72\u7684\u5b8c\u6574\u5b66\u4e60\u8fc7\u7a0b", "method": "1. Epsilon\u6570\u636e\u96c6\u5904\u7406\uff08\u5c0f\u7ec4\u548c\u4e2a\u4eba\u7b56\u7565\uff09 2. RestMex\u6587\u672c\u5206\u6790\u4e0e\u5206\u7c7b 3. IMDb\u7535\u5f71\u7279\u5f81\u5206\u6790 4. \u4f7f\u7528Scala\u5728Linux\u4e0a\u90e8\u7f72Apache Spark\u5206\u5e03\u5f0f\u8ba1\u7b97\u96c6\u7fa4", "result": "\u8be6\u7ec6\u8bb0\u5f55\u4e86\u5927\u6570\u636e\u5904\u7406\u548c\u5206\u6790\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u6570\u636e\u96c6\u5904\u7406\u3001\u6587\u672c\u5206\u7c7b\u3001\u7535\u5f71\u7279\u5f81\u5206\u6790\u4ee5\u53ca\u5206\u5e03\u5f0f\u8ba1\u7b97\u96c6\u7fa4\u7684\u6280\u672f\u5b9e\u73b0", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u5b9e\u8df5\uff0c\u638c\u63e1\u4e86\u5927\u6570\u636e\u5904\u7406\u7684\u5168\u6d41\u7a0b\u6280\u672f\u6808\uff0c\u4ece\u6570\u636e\u9884\u5904\u7406\u5230\u5206\u5e03\u5f0f\u8ba1\u7b97\u96c6\u7fa4\u90e8\u7f72\uff0c\u4e3a\u5927\u6570\u636e\u5206\u6790\u5e94\u7528\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.10425", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10425", "abs": "https://arxiv.org/abs/2512.10425", "authors": ["Fan Yu", "Guodong Li", "Si Wu", "Weijun Fang", "Sihuang Hu"], "title": "Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability", "comment": null, "summary": "Erasure coding with wide stripes is increasingly adopted to reduce storage overhead in large-scale storage systems. However, existing Locally Repairable Codes (LRCs) exhibit structural limitations in this setting: inflated local groups increase single-node repair cost, multi-node failures frequently trigger expensive global repair, and reliability degrades sharply. We identify a key root cause: local and global parity blocks are designed independently, preventing them from cooperating during repair. We present Cascaded Parity LRCs (CP-LRCs), a new family of wide stripe LRCs that embed structured dependency between parity blocks by decomposing a global parity block across all local parity blocks. This creates a cascaded parity group that preserves MDS-level fault tolerance while enabling low-bandwidth single-node and multi-node repairs. We provide a general coefficient-generation framework, develop repair algorithms exploiting cascading, and instantiate the design with CP-Azure and CP-Uniform. Evaluations on Alibaba Cloud show reductions in repair time of up to 41% for single-node failures and 26% for two-node failures.", "AI": {"tldr": "\u63d0\u51faCP-LRCs\uff08\u7ea7\u8054\u5947\u5076\u6821\u9a8cLRCs\uff09\uff0c\u901a\u8fc7\u5c06\u5168\u5c40\u5947\u5076\u6821\u9a8c\u5757\u5206\u89e3\u5230\u6240\u6709\u5c40\u90e8\u5947\u5076\u6821\u9a8c\u5757\u4e2d\uff0c\u5b9e\u73b0\u5bbd\u6761\u5e26\u4e0b\u7684\u9ad8\u6548\u4fee\u590d\uff0c\u76f8\u6bd4\u73b0\u6709LRCs\u663e\u8457\u964d\u4f4e\u4fee\u590d\u65f6\u95f4\u548c\u5e26\u5bbd\u3002", "motivation": "\u73b0\u6709\u5c40\u90e8\u53ef\u4fee\u590d\u7f16\u7801\uff08LRCs\uff09\u5728\u5bbd\u6761\u5e26\u573a\u666f\u4e0b\u5b58\u5728\u7ed3\u6784\u9650\u5236\uff1a\u6269\u5927\u7684\u5c40\u90e8\u7ec4\u589e\u52a0\u5355\u8282\u70b9\u4fee\u590d\u6210\u672c\uff0c\u591a\u8282\u70b9\u6545\u969c\u9891\u7e41\u89e6\u53d1\u6602\u8d35\u7684\u5168\u5c40\u4fee\u590d\uff0c\u53ef\u9760\u6027\u6025\u5267\u4e0b\u964d\u3002\u6839\u672c\u539f\u56e0\u662f\u5c40\u90e8\u548c\u5168\u5c40\u5947\u5076\u6821\u9a8c\u5757\u72ec\u7acb\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u5728\u4fee\u590d\u8fc7\u7a0b\u4e2d\u534f\u540c\u5de5\u4f5c\u3002", "method": "\u63d0\u51faCP-LRCs\uff08\u7ea7\u8054\u5947\u5076\u6821\u9a8cLRCs\uff09\uff0c\u901a\u8fc7\u5728\u5c40\u90e8\u5947\u5076\u6821\u9a8c\u5757\u4e4b\u95f4\u5d4c\u5165\u7ed3\u6784\u5316\u4f9d\u8d56\u5173\u7cfb\uff0c\u5c06\u5168\u5c40\u5947\u5076\u6821\u9a8c\u5757\u5206\u89e3\u5230\u6240\u6709\u5c40\u90e8\u5947\u5076\u6821\u9a8c\u5757\u4e2d\uff0c\u5f62\u6210\u7ea7\u8054\u5947\u5076\u6821\u9a8c\u7ec4\u3002\u63d0\u4f9b\u901a\u7528\u7684\u7cfb\u6570\u751f\u6210\u6846\u67b6\uff0c\u5f00\u53d1\u5229\u7528\u7ea7\u8054\u7279\u6027\u7684\u4fee\u590d\u7b97\u6cd5\uff0c\u5e76\u5b9e\u4f8b\u5316\u4e3aCP-Azure\u548cCP-Uniform\u4e24\u79cd\u5b9e\u73b0\u3002", "result": "\u5728\u963f\u91cc\u4e91\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCP-LRCs\u76f8\u6bd4\u73b0\u6709LRCs\uff0c\u5355\u8282\u70b9\u6545\u969c\u4fee\u590d\u65f6\u95f4\u6700\u591a\u51cf\u5c1141%\uff0c\u53cc\u8282\u70b9\u6545\u969c\u4fee\u590d\u65f6\u95f4\u6700\u591a\u51cf\u5c1126%\uff0c\u540c\u65f6\u4fdd\u6301MDS\u7ea7\u522b\u7684\u5bb9\u9519\u80fd\u529b\u3002", "conclusion": "CP-LRCs\u901a\u8fc7\u7ea7\u8054\u5947\u5076\u6821\u9a8c\u8bbe\u8ba1\u89e3\u51b3\u4e86\u5bbd\u6761\u5e26LRCs\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u5947\u5076\u6821\u9a8c\u5757\u7684\u534f\u540c\u4fee\u590d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4fee\u590d\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u5b58\u50a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u64e6\u9664\u7f16\u7801\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.10443", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10443", "abs": "https://arxiv.org/abs/2512.10443", "authors": ["Sabtain Ahmad", "Meerzhan Kanatbekova", "Ivona Brandic", "Atakan Aral"], "title": "Clustered Federated Learning with Hierarchical Knowledge Distillation", "comment": null, "summary": "Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\\%.", "AI": {"tldr": "\u63d0\u51faCFLHKD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u548c\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u5f02\u6784IoT\u73af\u5883\u4e2d\u7684\u6a21\u578b\u6027\u80fd", "motivation": "\u4f20\u7edf\u805a\u7c7b\u8054\u90a6\u5b66\u4e60(CFL)\u5b58\u5728\u5b66\u4e60\u788e\u7247\u5316\u95ee\u9898\uff0c\u4e3a\u6bcf\u4e2a\u805a\u7c7b\u72ec\u7acb\u8bad\u7ec3\u5168\u5c40\u6a21\u578b\uff0c\u65e0\u6cd5\u5229\u7528\u8de8\u805a\u7c7b\u7684\u96c6\u4f53\u77e5\u8bc6\uff0c\u4e14\u901a\u4fe1\u6548\u7387\u6709\u5f85\u63d0\u5347", "method": "\u63d0\u51faCFLHKD\u65b9\u6cd5\uff1a\u91c7\u7528\u5c42\u6b21\u5316CFL\u67b6\u6784\uff0c\u5728\u8fb9\u7f18\u7aef\u8bad\u7ec3\u805a\u7c7b\u7279\u5b9a\u6a21\u578b\uff0c\u5728\u4e91\u7aef\u8bad\u7ec3\u7edf\u4e00\u5168\u5c40\u6a21\u578b\uff1b\u901a\u8fc7\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u8de8\u805a\u7c7b\u77e5\u8bc6\u5171\u4eab\uff0c\u540c\u65f6\u4fdd\u6301\u805a\u7c7b\u4e2a\u6027\u5316\uff1b\u91c7\u7528\u53cc\u5c42\u805a\u5408\u673a\u5236\u8fde\u63a5\u672c\u5730\u548c\u5168\u5c40\u5b66\u4e60", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cCFLHKD\u5728\u805a\u7c7b\u7279\u5b9a\u6a21\u578b\u548c\u5168\u5c40\u6a21\u578b\u51c6\u786e\u7387\u4e0a\u5747\u4f18\u4e8e\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u8fbe3.32-7.57%", "conclusion": "CFLHKD\u901a\u8fc7\u5c42\u6b21\u5316\u67b6\u6784\u548c\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfCFL\u7684\u788e\u7247\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8de8\u805a\u7c7b\u77e5\u8bc6\u5171\u4eab\uff0c\u5728\u4fdd\u6301\u4e2a\u6027\u5316\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd"}}
{"id": "2512.10576", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10576", "abs": "https://arxiv.org/abs/2512.10576", "authors": ["Xinhang Chen", "Chao Zhang", "Jiahuan He", "Wei Liu", "Jianming Zhang", "Wenlong Zhou", "Xiao Li", "Pai Zeng", "Shiyong Li", "Yuanpan Qian", "Dong Li", "Zhaogeng Li"], "title": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp", "comment": null, "summary": "DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.\n  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.\n  Our high-fidelity simulations show that ESS delivers 69.4\\% throughput improvement at 32K context length and up to 123\\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.", "AI": {"tldr": "ESS\u7cfb\u7edf\u901a\u8fc7\u5c06Latent-Cache\u5378\u8f7d\u5230CPU\u5185\u5b58\uff0c\u89e3\u51b3\u4e86DeepSeek-V3.2-Exp\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684GPU\u5185\u5b58\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86Decode\u9636\u6bb5\u541e\u5410\u91cf\u3002", "motivation": "DeepSeek-V3.2-Exp\u867d\u7136\u901a\u8fc7\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u964d\u4f4e\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u5ef6\u8fdf\uff0c\u4f46Decode\u9636\u6bb5\u4ecd\u7136\u662f\u4e3b\u8981\u74f6\u9888\u3002\u95ee\u9898\u6e90\u4e8eLatent-Cache\u968f\u5e8f\u5217\u957f\u5ea6\u7ebf\u6027\u589e\u957f\u4e0eGPU\u5185\u5b58\u5bb9\u91cf\u6709\u9650\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u8fd9\u9650\u5236\u4e86\u6279\u5904\u7406\u5927\u5c0f\uff0c\u4ece\u800c\u6291\u5236\u4e86Decode\u9636\u6bb5\u541e\u5410\u91cf\u3002", "method": "\u63d0\u51faESS\uff08Extended Sparse Server\uff09\u7cfb\u7edf\uff0c\u91c7\u7528\u5378\u8f7d\u4e2d\u5fc3\u8bbe\u8ba1\uff1a1\uff09\u9009\u62e9\u6027\u5c06Latent-Cache\u5378\u8f7d\u5230CPU\u5185\u5b58\uff1b2\uff09\u4fdd\u7559\u5ef6\u8fdf\u5173\u952e\u7ec4\u4ef6\u5728GPU\u4e0a\uff1b3\uff09\u901a\u8fc7\u91ca\u653eGPU\u5185\u5b58\uff0c\u4f7f\u6279\u5904\u7406\u5927\u5c0f\u6269\u5c55\u4e0eGPU\u5185\u5b58\u7ea6\u675f\u89e3\u8026\u3002", "result": "\u9ad8\u4fdd\u771f\u6a21\u62df\u663e\u793a\uff1a\u572832K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\uff0cESS\u63d0\u4f9b69.4%\u7684\u541e\u5410\u91cf\u63d0\u5347\uff1b\u5728128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\uff0c\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe123%\u3002\u8fd9\u8bc1\u660e\u4e86ESS\u5728\u5927\u4e0a\u4e0b\u6587\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "ESS\u662f\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587LLM\u670d\u52a1\u7684Decode\u9636\u6bb5\u541e\u5410\u91cf\uff0c\u4ece\u800c\u964d\u4f4e\u5b9e\u9645\u90e8\u7f72\u6210\u672c\u3002"}}
