<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [HITrees: Higher-Order Interaction Trees](https://arxiv.org/abs/2510.14558)
*Amir Mohammad Fadaei Ayyam,Michael Sammler*

Main category: cs.PL

TL;DR: 提出了高阶交互树（HITrees），这是第一个在非守卫类型论中支持高阶效应的交互树变体，通过两种关键技术实现：设计效应概念使高阶输入的效应不动点可表示为归纳类型，以及使用去函数化将高阶输出编码为一阶表示。


<details>
  <summary>Details</summary>
Motivation: 交互树作为形式验证的组成语义基础很受欢迎，但其效应概念不支持高阶效应（即接受或返回单子计算的效应），而高阶效应对于建模并行组合和call/cc等复杂语义特征至关重要。

Method: 设计HITrees支持高阶效应：1）设计效应概念使高阶输入效应的不动点可表示为归纳类型；2）使用去函数化技术将高阶输出编码为一阶表示。在Lean证明助手中实现，并提供状态转换系统和单子程序两种解释。

Result: 在Lean中实现了HITrees，并构建了包含并发、递归和call/cc等效应的综合库。成功应用HITrees定义了具有并行组合和call/cc的语言语义。

Conclusion: HITrees是第一个在非守卫类型论中支持高阶效应的交互树变体，能够表达复杂的语义特征如并行组合和call/cc，为形式验证提供了更强大的组成语义基础。

Abstract: Recent years have witnessed the rise of compositional semantics as a
foundation for formal verification of complex systems. In particular,
interaction trees have emerged as a popular denotational semantics. Interaction
trees achieve compositionality by providing a reusable library of effects.
However, their notion of effects does not support higher-order effects, i.e.,
effects that take or return monadic computations. Such effects are essential to
model complex semantic features like parallel composition and call/cc.
  We introduce Higher-Order Interaction Trees (HITrees), the first variant of
interaction trees to support higher-order effects in a non-guarded type theory.
HITrees accomplish this through two key techniques: first, by designing the
notion of effects such that the fixpoints of effects with higher-order input
can be expressed as inductive types inside the type theory; and second, using
defunctionalization to encode higher-order outputs into a first-order
representation. We implement HITrees in the Lean proof assistant, accompanied
by a comprehensive library of effects including concurrency, recursion, and
call/cc. Furthermore, we provide two interpretations of HITrees, as state
transition systems and as monadic programs. To demonstrate the expressiveness
of HITrees, we apply them to define the semantics of a language with parallel
composition and call/cc.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Efficiently Executing High-throughput Lightweight LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management](https://arxiv.org/abs/2510.14024)
*Thanh Son Phung,Douglas Thain*

Main category: cs.DC

TL;DR: 提出了一种名为"Pervasive Context Management"的技术，通过将LLM初始化上下文与实际推理解耦，在GPU中保留上下文直到不再需要，从而解决HPC集群中LLM工作负载的排队等待和启动成本问题。


<details>
  <summary>Details</summary>
Motivation: 当前HPC集群设计无法有效支持集成轻量级LLM与传统高吞吐量应用的新型工作负载，要么面临静态批处理队列的长时间等待，要么在资源抢占时重复支付昂贵的LLM启动成本。

Method: 将LLM初始化上下文与实际LLM推理解耦，在GPU中持续保留上下文直到不再需要，实现"Pervasive Context Management"技术。

Result: 应用该技术后，事实核查应用的执行时间减少了72.1%（从3小时降至48分钟），使用相同数量的GPU；并能在集群32.8%的所有GPU上机会性扩展，进一步将执行时间缩短至13分钟。

Conclusion: Pervasive Context Management技术能有效解决HPC集群中LLM工作负载的性能瓶颈，显著减少执行时间并提高资源利用率。

Abstract: The rise of Generative AI introduces a new class of HPC workloads that
integrates lightweight LLMs with traditional high-throughput applications to
accelerate scientific discovery. The current design of HPC clusters is
inadequate to support this new class however, either incurring long wait times
on static batch queues or repeatedly paying expensive LLM startup costs upon
resource preemption. To circumvent both the long queues and high startup costs,
we propose to "decouple" the LLM initialization context from the actual LLM
inferences, and retain the context in GPUs until it is no longer needed, a
technique we term "Pervasive Context Management". We transform a fact
verification application to enable this technique, allowing it to reduce its
execution time by 72.1% (from 3 hours to 48 minutes) using the same amount of
GPUs, and scale opportunistically on 32.8% of all GPUs in the cluster and
further reduce the execution time to 13 minutes.

</details>


### [3] [Anonymized Network Sensing using C++26 std::execution on GPUs](https://arxiv.org/abs/2510.14050)
*Michael Mandulak,Sayan Ghosh,S M Ferdous,Mahantesh Halappanavar,George Slota*

Main category: cs.DC

TL;DR: 本文探讨了使用C++26 Senders模型在密集GPU系统上开发网络感知图挑战的实践方面，通过异步数据操作链实现高效的GPU任务部署，在8个NVIDIA A100 GPU上相比串行GraphBLAS基准实现了最高55倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着网络数据包数据规模不断增大，基于GPU的并行方法已成为网络分析的主流。但GPU实现仍面临主机-设备内存管理和复杂工作负载移植等启动挑战。可组合框架的出现旨在缓解这些挑战，特别是C++26 Senders模型为批量推送任务到不同设备执行上下文提供了简单接口。

Method: 采用C++26 Senders异步数据操作链模型，将GPU视为一等执行资源，通过表达性和标准化的异步语义开发多GPU应用工作负载。使用基于商品库的实现，在密集GPU系统上部署网络感知图挑战。

Result: 基于商品库的实现相比参考串行GraphBLAS基准，在8个NVIDIA A100 GPU上实现了最高55倍的性能提升。采用通用且高效的程序模型不会影响关键路径性能。

Conclusion: C++26 Senders模型为在密集GPU平台上开发网络分析应用提供了有效的编程方法，能够在保持高性能的同时简化开发过程，证明了现代异步编程模型在大规模网络感知任务中的实用性。

Abstract: Large-scale network sensing plays a vital role in network traffic analysis
and characterization. As network packet data grows increasingly large, parallel
methods have become mainstream for network analytics. While effective,
GPU-based implementations still face start-up challenges in host-device memory
management and porting complex workloads on devices, among others. To mitigate
these challenges, composable frameworks have emerged using modern C++
programming language, for efficiently deploying analytics tasks on GPUs.
Specifically, the recent C++26 Senders model of asynchronous data operation
chaining provides a simple interface for bulk pushing tasks to varied device
execution contexts.
  Considering the prominence of contemporary dense-GPU platforms and
vendor-leveraged software libraries, such a programming model consider GPUs as
first-class execution resources (compared to traditional host-centric
programming models), allowing convenient development of multi-GPU application
workloads via expressive and standardized asynchronous semantics. In this
paper, we discuss practical aspects of developing the Anonymized Network
Sensing Graph Challenge on dense-GPU systems using the recently proposed C++26
Senders model. Adopting a generic and productive programming model does not
necessarily impact the critical-path performance (as compared to low-level
proprietary vendor-based programming models): our commodity library-based
implementation achieves up to 55x performance improvements on 8x NVIDIA A100
GPUs as compared to the reference serial GraphBLAS baseline.

</details>


### [4] [Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic Serving](https://arxiv.org/abs/2510.14126)
*Nikos Pagonas,Yeounoh Chung,Kostis Kaffes,Arvind Krishnamurthy*

Main category: cs.DC

TL;DR: Cortex是一个面向智能体工作负载的工作流感知服务平台，通过阶段隔离策略为工作流的不同阶段分配专用资源池，提高性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决智能体工作流中不同阶段之间的计算和内存干扰问题，提升KV缓存利用率、吞吐量和性能可预测性。

Method: 采用阶段隔离策略，为智能体工作流的每个不同阶段配置专用资源池，并定制各阶段的资源分配和调度机制。

Result: 减轻了阶段间干扰，实现了更好的KV缓存利用、更高的吞吐量和更可预测的性能。

Conclusion: Cortex为更先进的智能体原生服务范式奠定了基础，包括可塑性资源管理、工作流分支的推测执行以及共享的多层智能体状态缓存。

Abstract: We introduce Cortex, a prototype workflow-aware serving platform designed for
agentic workloads. The core principle of Cortex is stage isolation: it
provisions dedicated resource pools for each distinct stage of an agentic
workflow. This simple yet powerful strategy mitigates inter-stage interference
in compute and memory, leading to better KV cache utilization, higher
throughput, and more predictable performance. By customizing resource
allocation and scheduling within each distinct stage of agentic workflows,
Cortex lays the groundwork for more advanced, agent-native serving paradigms,
including malleable resource management, speculative execution of workflow
branches, and a shared, multi-tiered cache for "agentic state."

</details>


### [5] [Distributed-Memory Parallel Algorithms for Fixed-Radius Near Neighbor Graph Construction](https://arxiv.org/abs/2510.14147)
*Gabriel Raulet,Dmitriy Morozov,Aydin Buluc,Katherine Yelick*

Main category: cs.DC

TL;DR: 本文提出了一种可扩展的分布式内存算法，使用覆盖树在一般度量空间中计算固定半径近邻图，在真实数据集上实现了高达1590倍的加速。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力和数据采集方法的发展，大型科学数据集需要可扩展的近邻图计算解决方案。现有并行近邻算法主要关注欧几里得空间和近似解，但许多应用需要精确解和非欧几里得度量。

Method: 提出了共享内存的覆盖树构建算法，以及两种分布式内存算法：简单的点分区策略和空间分区策略，在节点上利用覆盖树算法。

Result: 在100万个点的高维真实数据集上，使用1024核时对平均70个邻居的图实现了678.34倍加速，使用4096核时对平均500个邻居的图实现了1590.99倍加速。

Conclusion: 该算法在各种真实和合成数据集上表现出良好的并行扩展性，适用于传统和非传统度量空间，为大规模数据分析提供了高效的近邻图计算解决方案。

Abstract: Computing fixed-radius near-neighbor graphs is an important first step for
many data analysis algorithms. Near-neighbor graphs connect points that are
close under some metric, endowing point clouds with a combinatorial structure.
As computing power and data acquisition methods advance, diverse sources of
large scientific datasets would greatly benefit from scalable solutions to this
common subroutine for downstream analysis. Prior work on parallel nearest
neighbors has made great progress in problems like k-nearest and approximate
nearest neighbor search problems, with particular attention on Euclidean
spaces. Yet many applications need exact solutions and non-Euclidean metrics.
This paper presents a scalable sparsity-aware distributed memory algorithm
using cover trees to compute near-neighbor graphs in general metric spaces. We
provide a shared-memory algorithm for cover tree construction and demonstrate
its competitiveness with state-of-the-art fixed-radius search data structures.
We then introduce two distributed-memory algorithms for the near-neighbor graph
problem, a simple point-partitioning strategy and a spatial-partitioning
strategy, which leverage the cover tree algorithm on each node. Our algorithms
exhibit parallel scaling across a variety of real and synthetic datasets for
both traditional and non-traditional metrics. On real world high dimensional
datasets with one million points, we achieve speedups up to 678.34x over the
state-of-the-art using 1024 cores for graphs with 70 neighbors per vertex (on
average), and up to 1590.99x using 4096 cores for graphs with 500 neighbors per
vertex (on average).

</details>


### [6] [Deadlock-free routing for Full-mesh networks without using Virtual Channels](https://arxiv.org/abs/2510.14730)
*Alejandro Cano,Cristóbal Camarero,Carmen Martínez,Ramón Beivide*

Main category: cs.DC

TL;DR: 提出TERA路由算法，在Full-mesh网络中通过嵌入物理子网络实现无虚拟通道的死锁自由非最短路径路由，显著提升性能并减少缓冲区需求。


<details>
  <summary>Details</summary>
Motivation: 传统高基数低直径网络使用虚拟通道避免死锁，但VC带来面积、功耗和设计复杂性开销，限制了交换机可扩展性。

Method: TERA算法在Full-mesh网络中嵌入物理子网络，提供无虚拟通道的死锁自由非最短路径，避免传统链路排序方案在对抗性流量下的性能下降。

Result: 在Full-mesh网络中，TERA比链路排序算法在对抗性流量下性能提升80%，在应用内核中提升100%；相比VC方案减少50%缓冲区需求；在2D-HyperX中比最先进算法性能提升32%。

Conclusion: TERA通过物理子网络嵌入实现了高效的无虚拟通道死锁自由路由，在保持性能的同时显著降低了硬件开销。

Abstract: High-radix, low-diameter networks like HyperX and Dragonfly use a Full-mesh
core, and rely on multiple virtual channels (VCs) to avoid packet deadlocks in
adaptive routing. However, VCs introduce significant overhead in the switch in
terms of area, power, and design complexity, limiting the switch scalability.
This paper starts by revisiting VC-less routing through link ordering schemes
in Full-mesh networks, which offer implementation simplicity but suffer from
performance degradation under adversarial traffic. Thus, to overcome these
challenges, we propose TERA (Topology-Embedded Routing Algorithm), a novel
routing algorithm which employs an embedded physical subnetwork to provide
deadlock-free non-minimal paths without using VCs.
  In a Full-mesh network, TERA outperforms link ordering routing algorithms by
80% when dealing with adversarial traffic, and up to 100% in application
kernels. Furthermore, compared to other VC-based approaches, it reduces buffer
requirements by 50%, while maintaining comparable latency and throughput.
Lastly, early results from a 2D-HyperX evaluation show that TERA outperforms
state-of-the-art algorithms that use the same number of VCs, achieving
performance improvements of up to 32%.

</details>


### [7] [Privacy-Preserving and Incentive-Driven Relay-Based Framework for Cross-Domain Blockchain Interoperability](https://arxiv.org/abs/2510.14151)
*Saeed Moradi,Koosha Esmaeilzadeh Khorasani,Sara Rouhani*

Main category: cs.DC

TL;DR: 提出了一个区块链无关的框架，用于实现许可链和无许可链之间的互操作性，通过加密技术确保安全数据交换，并集成匿名协议增强交易隐私。


<details>
  <summary>Details</summary>
Motivation: 区块链互操作性对于将孤立的网络转变为协作生态系统至关重要。虽然公链互操作性已取得进展，但许可链和无许可链之间的桥接面临访问控制、架构和安全要求差异的独特挑战。

Method: 采用轻量级架构设计，利用加密技术确保安全数据交换，集成Clover和Dandelion++协议增强交易匿名性。

Result: 性能评估显示该框架在异构区块链生态系统中实现了安全高效的互操作性，测量了转发时间、吞吐量、可用性及其合谋影响。

Conclusion: 该区块链无关框架成功解决了许可链和无许可链之间的互操作性挑战，通过安全的数据交换和交易匿名性实现了有效的跨链协作。

Abstract: Interoperability is essential for transforming blockchains from isolated
networks into collaborative ecosystems, unlocking their full potential. While
significant progress has been made in public blockchain interoperability,
bridging permissioned and permissionless blockchains poses unique challenges
due to differences in access control, architectures, and security requirements.
This paper introduces a blockchain-agnostic framework to enable
interoperability between permissioned and permissionless networks. Leveraging
cryptographic techniques, the framework ensures secure data exchanges. Its
lightweight architectural design simplifies implementation and maintenance,
while the integration of Clover and Dandelion++ protocols enhances transaction
anonymity. Performance evaluations demonstrate the framework's effectiveness in
achieving secure and efficient interoperability by measuring the forwarding
time, the throughput, the availability, and their collusion impact of the
system across heterogeneous blockchain ecosystems.

</details>


### [8] [Proof-Carrying Fair Ordering: Asymmetric Verification for BFT via Incremental Graphs](https://arxiv.org/abs/2510.14186)
*Pengkun Ren,Hai Dong,Nasrin Sohrabi,Zahir Tari,Pengcheng Zhang*

Main category: cs.DC

TL;DR: AUTIG是一个高性能、可插拔的订单公平服务，通过非对称架构解决BFT共识协议中的订单公平问题，避免了传统对称协议中每个副本都需要重新计算排序的冗余开销。


<details>
  <summary>Details</summary>
Motivation: 现有的订单公平共识协议（如Themis）要求每个副本重新运行领导者昂贵的排序计算来进行验证，这种对称冗余的范式效率低下。AUTIG旨在打破这种对称性，通过验证而非重新计算来实现高效公平排序。

Method: AUTIG采用非对称架构：领导者维护持久化的未确认交易增量图（UTIG）来分摊图构建成本，并为每个提案生成结构化公平性证明；跟随者无需维护历史状态即可验证证明。关键创新包括增量图维护、解耦流水线设计和覆盖所有内部对及边界完整性的证明机制。

Result: 实验评估显示，AUTIG在部分同步条件下相比对称图基线具有更高的吞吐量和更低的端到端延迟，同时保持gamma批次订单公平性。

Conclusion: AUTIG证明了验证公平排序不需要重新计算，通过非对称架构和可验证断言实现了高性能的订单公平服务，为区块链系统提供了更有效的价值提取攻击防护方案。

Abstract: Byzantine Fault-Tolerant (BFT) consensus protocols ensure agreement on
transaction ordering despite malicious actors, but unconstrained ordering power
enables sophisticated value extraction attacks like front running and sandwich
attacks - a critical threat to blockchain systems. Order-fair consensus curbs
adversarial value extraction by constraining how leaders may order
transactions. While state-of-the-art protocols such as Themis attain strong
guarantees through graph-based ordering, they ask every replica to re-run the
leader's expensive ordering computation for validation - an inherently
symmetric and redundant paradigm. We present AUTIG, a high-performance,
pluggable order-fairness service that breaks this symmetry. Our key insight is
that verifying a fair order does not require re-computing it. Instead,
verification can be reduced to a stateless audit of succinct, verifiable
assertions about the ordering graph's properties. AUTIG realizes this via an
asymmetric architecture: the leader maintains a persistent
Unconfirmed-Transaction Incremental Graph (UTIG) to amortize graph construction
across rounds and emits a structured proof of fairness with each proposal;
followers validate the proof without maintaining historical state. AUTIG
introduces three critical innovations: (i) incremental graph maintenance driven
by threshold-crossing events and state changes; (ii) a decoupled pipeline that
overlaps leader-side collection/update/extraction with follower-side stateless
verification; and (iii) a proof design covering all internal pairs in the
finalized prefix plus a frontier completeness check to rule out hidden external
dependencies. We implement AUTIG and evaluate it against symmetric graph-based
baselines under partial synchrony. Experiments show higher throughput and lower
end-to-end latency while preserving gamma-batch-order-fairness.

</details>


### [9] [FairBatching: Fairness-Aware Batch Formation for LLM Inference](https://arxiv.org/abs/2510.14392)
*Hongtao Lyu,Boyue Liu,Mingyu Wu,Haibo Chen*

Main category: cs.DC

TL;DR: FairBatching是一种新型LLM推理调度器，通过公平分配预填充和解码任务的计算资源，解决了现有调度器中的计算不公平问题，显著降低了TTFT延迟并提高了系统容量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理调度器在最小化TTFT延迟和维持高令牌生成率之间存在矛盾，Sarathi提出的无停滞批处理调度器虽然防止了解码停滞，但引入了显著的计算不公平性，过度优先解码任务导致预填充队列延迟和系统QoS下降。

Method: 提出FairBatching调度器，包含自适应批容量确定机制、公平动态批形成算法和新颖的负载估计方法，动态调整计算预算，打破解码优先范式，实现全局公平。

Result: 在真实跟踪评估中，FairBatching将TTFT尾部延迟降低高达2.29倍，稳健维持TPOT服务水平目标，单节点容量提升20.0%，集群级容量提升54.3%。

Conclusion: FairBatching通过公平资源分配和自适应调度机制，有效解决了LLM推理系统中的计算不公平问题，显著提升了系统性能和容量。

Abstract: Large language model (LLM) inference systems face a fundamental tension
between minimizing Time-to-First-Token (TTFT) latency for new requests and
maintaining a high, steady token generation rate (low Time-Per-Output-Token, or
TPOT) for ongoing requests. Existing stall-free batching schedulers proposed by
Sarathi, while effective at preventing decode stalls, introduce significant
computational unfairness. They prioritize decode tasks excessively,
simultaneously leading to underutilized decode slack and unnecessary prefill
queuing delays, which collectively degrade the system's overall quality of
service (QoS).
  This work identifies the root cause of this unfairness: the non-monotonic
nature of Time-Between-Tokens (TBT) as a scheduling metric and the rigid
decode-prioritizing policy that fails to adapt to dynamic workload bursts. We
therefore propose FairBatching, a novel LLM inference scheduler that enforces
fair resource allocation between prefill and decode tasks. It features an
adaptive batch capacity determination mechanism, which dynamically adjusts the
computational budget to improve the GPU utilization without triggering SLO
violations. Its fair and dynamic batch formation algorithm breaks away from the
decode-prioritizing paradigm, allowing computation resources to be reclaimed
from bursting decode tasks to serve prefill surges, achieving global fairness.
Furthermore, FairBatching provides a novel load estimation method, enabling
more effective coordination with upper-level schedulers. Implemented and
evaluated on realistic traces, FairBatching significantly reduces TTFT tail
latency by up to 2.29x while robustly maintaining TPOT SLOs, achieving overall
20.0% improvement in single-node capacity and 54.3% improvement in
cluster-level capacity.

</details>


### [10] [ScalePool: Hybrid XLink-CXL Fabric for Composable Resource Disaggregation in Unified Scale-up Domains](https://arxiv.org/abs/2510.14580)
*Hyein Woo,Miryeong Kwon,Jiseon Kim,Eunjee Na,Hanjin Choi,Seonghyeon Jang,Myoungsoo Jung*

Main category: cs.DC

TL;DR: ScalePool是一种新型集群架构，通过统一硬件互连技术连接大量加速器，结合XLink和CXL构建混合结构，实现低延迟通信和可扩展内存共享，显著提升LLM训练性能。


<details>
  <summary>Details</summary>
Motivation: 传统长距离网络连接加速器存在延迟高、互操作性差的问题，需要一种能够统一硬件互连、支持异构集群操作和可组合资源解聚的架构。

Method: 采用XLink-CXL混合结构：XLink用于集群内低延迟加速器通信，分层CXL交换结构用于可扩展的集群间内存共享。引入显式内存分层：第1层结合加速器本地内存与CXL/XLink，第2层通过CXL互连专用内存节点。

Result: 相比传统RDMA环境，LLM训练平均加速1.22倍，最高达1.84倍；第2层内存解聚策略对内存密集型工作负载延迟降低最高4.5倍。

Conclusion: ScalePool通过统一硬件互连和内存分层设计，有效解决了加速器集群的互操作性和性能瓶颈问题，为大规模AI训练提供了高效解决方案。

Abstract: This paper proposes ScalePool, a novel cluster architecture designed to
interconnect numerous accelerators using unified hardware interconnects rather
than traditional long-distance networking. ScalePool integrates
Accelerator-Centric Links (XLink) and Compute Express Link (CXL) into a unified
XLink-CXL hybrid fabric. Specifically, ScalePool employs XLink for
intra-cluster, low-latency accelerator communication, while using hierarchical
CXL-based switching fabrics for scalable and coherent inter-cluster memory
sharing. By abstracting interfaces through CXL, ScalePool structurally resolves
interoperability constraints, enabling heterogeneous cluster operation and
composable resource disaggregation. In addition, ScalePool introduces explicit
memory tiering: the latency-critical tier-1 combines accelerator-local memory
with coherence-centric CXL and XLink, whereas the highcapacity tier-2 employs
dedicated memory nodes interconnected by a CXL-based fabric, achieving scalable
and efficient memory pooling. Evaluation results show that ScalePool
accelerates LLM training by 1.22x on average and up to 1.84x compared to
conventional RDMA-based environments. Furthermore, the proposed tier-2 memory
disaggregation strategy reduces latency by up to 4.5x for memory-intensive
workloads.

</details>


### [11] [JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job Atomization](https://arxiv.org/abs/2510.14599)
*Michal Konopa,Jan Fesl,Ladislav Ber ánek*

Main category: cs.DC

TL;DR: JASDA是一个基于拍卖理论和在线优化的去中心化GPU调度框架，通过双向迭代交互实现自适应资源管理，适用于MIG-enabled GPU环境。


<details>
  <summary>Details</summary>
Motivation: 传统集中式调度在MIG-enabled GPU上难以应对复杂多变的工作负载，需要更可扩展的调度方案。

Method: 扩展SJA概念，采用去中心化协商过程：作业主动生成和评分可行子作业，调度器执行策略驱动的清空操作，平衡利用率、公平性和时间响应性。

Result: JASDA在调度循环中嵌入反馈、校准和概率安全性，实现自适应和透明的决策制定。

Conclusion: JASDA为市场感知和公平驱动的资源管理提供了可扩展基础，弥合了理论调度模型与现代MIG-enabled环境实际部署之间的差距。

Abstract: The increasing complexity and temporal variability of workloads on
MIG-enabled GPUs challenge the scalability of traditional centralized
scheduling. Building upon the SJA concept, this paper introduces JASDA-a novel
paradigm that extends SJA from a largely centralized scheduling model toward a
fully decentralized negotiation process. In JASDA, jobs actively generate and
score feasible subjobs in response to scheduler-announced execution windows,
while the scheduler performs policy-driven clearing that balances utilization,
fairness, and temporal responsiveness. This bidirectional, iterative
interaction embeds feedback, calibration, and probabilistic safety directly
into the scheduling loop, enabling adaptive and transparent decision-making. By
coupling principles from auction theory and online optimization with the
temporal granularity of GPU workloads, JASDA provides a scalable foundation for
market-aware and fairness-driven resource management-bridging theoretical
scheduling models with practical deployment in modern MIG-enabled environments
relevant to Artificial Intelligence and Agriculture 4.0.

</details>


### [12] [MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC Systems](https://arxiv.org/abs/2510.14622)
*Miryeong Kwon,Donghyun Gouk,Hyein Woo,Junhee Kim,Jinwoo Baek,Kyungkuk Nam,Sangyoon Ji,Jiseon Kim,Hanyeoreum Bae,Junhyeok Jang,Hyunwoo You,Junseok Moon,Myoungsoo Jung*

Main category: cs.DC

TL;DR: MPI-over-CXL是一种新的MPI通信范式，利用CXL技术通过共享内存直接访问替代传统的数据拷贝方法，显著降低通信延迟和内存带宽使用。


<details>
  <summary>Details</summary>
Motivation: 传统MPI实现依赖显式内存拷贝操作，导致冗余数据移动和缓冲区管理的开销，影响了涉及密集处理器间通信的HPC工作负载性能。

Method: 利用CXL提供跨多主机的缓存一致性共享内存，将共享内存区域直接映射到MPI进程的虚拟地址空间，实现基于指针的高效通信，消除冗余拷贝操作。

Result: 使用代表性基准测试进行评估，相比传统MPI系统显示出显著的性能提升。

Conclusion: MPI-over-CXL有潜力在大规模HPC环境中提高效率和可扩展性。

Abstract: MPI implementations commonly rely on explicit memory-copy operations,
incurring overhead from redundant data movement and buffer management. This
overhead notably impacts HPC workloads involving intensive inter-processor
communication. In response, we introduce MPI-over-CXL, a novel MPI
communication paradigm leveraging CXL, which provides cache-coherent shared
memory across multiple hosts. MPI-over-CXL replaces traditional data-copy
methods with direct shared memory access, significantly reducing communication
latency and memory bandwidth usage. By mapping shared memory regions directly
into the virtual address spaces of MPI processes, our design enables efficient
pointer-based communication, eliminating redundant copying operations. To
validate this approach, we implement a comprehensive hardware and software
environment, including a custom CXL 3.2 controller, FPGA-based multi-host
emulation, and dedicated software stack. Our evaluations using representative
benchmarks demonstrate substantial performance improvements over conventional
MPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and
scalability in large-scale HPC environments.

</details>


### [13] [xLLM Technical Report](https://arxiv.org/abs/2510.14686)
*Tongxuan Liu,Tao Peng,Peijun Yang,Xiaoyang Zhao,Xiusheng Lu,Weizhe Huang,Zirui Liu,Xiaoyu Chen,Zhiwei Liang,Jun Xiong,Donghe Jin,Minchao Zhang,Jinrong Guo,Yingxu Deng,Xu Zhang,Xianzhe Dong,Siqi Wang,Siyu Wu,Yu Wu,Zihan Tang,Yuting Zeng,Yanshu Wang,Jinguang Liu,Meng Kang,Menxin Li,Yunlong Wang,Yiming Liu,Xiaolong Ma,Yifan Wang,Yichen Zhang,Jinrun Yin,Keyang Zheng,Jiawei Yin,Jun Zhang,Ziyue Wang,Xiaobo Lin,Liangyu Liu,Liwei Lan,Yang Liu,Chunhua Peng,Han Liu,Songcheng Ren,Xuezhu Wang,Yunheng Shen,Yi Wang,Guyue Liu,Hui Chen,Tong Yang,Hailong Yang,Jing Li,Guiguang Ding,Ke Zhang*

Main category: cs.DC

TL;DR: xLLM是一个高效的大型语言模型推理框架，采用解耦的服务-引擎架构，通过智能调度、分布式KV缓存管理和多层级优化，显著提升了推理吞吐量和资源效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决企业级大规模LLM服务中的性能瓶颈和资源利用率问题，特别是在多模态输入和多样化AI加速器环境下的高效推理需求。

Method: 构建解耦的服务-引擎架构：服务层采用智能调度模块处理多模态请求，实现在线离线任务协同调度；引擎层通过多层级执行流水线优化、自适应图模式和xTensor内存管理来饱和计算资源。

Result: 在相同TPOT约束下，xLLM的吞吐量达到MindIE的1.7倍和vLLM-Ascend的2.2倍（使用Qwen系列模型），在使用Deepseek系列模型时平均吞吐量为MindIE的1.7倍。

Conclusion: xLLM框架通过创新的架构设计和深度优化，为企业级LLM推理提供了高性能、高效率的解决方案，显著优于现有系统。

Abstract: We introduce xLLM, an intelligent and efficient Large Language Model (LLM)
inference framework designed for high-performance, large-scale enterprise-grade
serving, with deep optimizations for diverse AI accelerators. To address these
challenges, xLLM builds a novel decoupled service-engine architecture. At the
service layer, xLLM-Service features an intelligent scheduling module that
efficiently processes multimodal requests and co-locates online and offline
tasks through unified elastic scheduling to maximize cluster utilization. This
module also relies on a workload-adaptive dynamic Prefill-Decode (PD)
disaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation
policy designed for multimodal inputs. Furthermore, it incorporates a
distributed architecture to provide global KV Cache management and robust
fault-tolerant capabilities for high availability. At the engine layer,
xLLM-Engine co-optimizes system and algorithm designs to fully saturate
computing resources. This is achieved through comprehensive multi-layer
execution pipeline optimizations, an adaptive graph mode and an xTensor memory
management. xLLM-Engine also further integrates algorithmic enhancements such
as optimized speculative decoding and dynamic EPLB, collectively serving to
substantially boost throughput and inference efficiency. Extensive evaluations
demonstrate that xLLM delivers significantly superior performance and resource
efficiency. Under identical TPOT constraints, xLLM achieves throughput up to
1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while
maintaining an average throughput of 1.7x that of MindIE with Deepseek-series
models. xLLM framework is publicly available at
https://github.com/jd-opensource/xllm and
https://github.com/jd-opensource/xllm-service.

</details>


### [14] [Balls and Bins and the Infinite Process with Random Deletions](https://arxiv.org/abs/2510.14798)
*Petra Berenbrink,Tom Friedetzky,Peter Kling,Lars Nagel*

Main category: cs.DC

TL;DR: 本文分析了具有删除操作的无限球-箱过程，证明了在任意时刻，最大负载与平均负载的差异为O(log n)，过载为log log n+O(1)，并给出了匹配的下界。


<details>
  <summary>Details</summary>
Motivation: 研究具有动态插入和删除操作的负载均衡问题，分析在更一般场景下的负载分布特性，特别是最大负载与平均负载之间的差异。

Method: 使用分层归纳法（layered induction）和详细的势能分析，通过概率耦合简化分析过程，避免复杂的条件分析。

Result: 证明了在任意时刻，超过平均负载的球数为O(n)，差异为O(log n)，过载为log log n+O(1)，并为差异提供了匹配的下界。

Conclusion: 该模型在更一般的场景下仍能保持良好的负载均衡特性，最大负载与平均负载的差异和过载都有理论保证。

Abstract: We consider an infinite balls-into-bins process with deletions where in each
discrete step $t$ a coin is tossed as to whether, with probability $\beta(t)
\in (0,1)$, a new ball is allocated using the Greedy[2] strategy (which places
the ball in the lower loaded of two bins sampled uniformly at random) or, with
remaining probability $1-\beta(t)$, a ball is deleted from a non-empty bin
chosen uniformly at random. Let $n$ be the number of bins and $m(t)$ the total
load at time $t$. We are interested in bounding the discrepancy $x_{\max}(t) -
m(t)/n$ (current maximum load relative to current average) and the overload
$x_{\max}(t) - m_{\max}(t)/n$ (current maximum load relative to highest average
observed so far).
  We prove that at an arbitrarily chosen time $t$ the total number of balls
above the average is $O(n)$ and that the discrepancy is $ O(\log(n))$. For the
discrepancy, we provide a matching lower bound. Furthermore we prove that at an
arbitrarily chosen time $t$ the overload is $\log\log(n)+O(1)$. For "good"
insertion probability sequences (in which the average load of time intervals
with polynomial length increases in expectation) we show that even the
discrepancy is bounded by $\log\log(n)+O(1)$.
  One of our main analytical tools is a layered induction, as per [ABKU99].
Since our model allows for rather more general scenarios than what was
previously considered, the formal analysis requires some extra ingredients as
well, in particular a detailed potential analysis. Furthermore, we simplify the
setup by applying probabilistic couplings to obtain certain "recovery"
properties, which eliminate much of the need for intricate and careful
conditioning elsewhere in the analysis.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [15] [DIAMOND: Systolic Array Acceleration of Sparse Matrix Multiplication for Quantum Simulation](https://arxiv.org/abs/2510.14172)
*Yuchao Su,Srikar Chundury,Jiajia Li,Frank Mueller*

Main category: cs.AR

TL;DR: 提出了第一个针对量子模拟优化的对角线加速器，通过利用哈密顿矩阵中常见的对角线结构，将稀疏矩阵转换为密集计算，显著提升性能和能效。


<details>
  <summary>Details</summary>
Motivation: 哈密顿模拟是量子计算的关键工作负载，但希尔伯特空间维度随量子比特数指数增长，使得矩阵指数运算成本高昂。现有加速器主要针对机器学习负载设计，无法有效处理哈密顿模拟中常见的结构化对角线稀疏模式。

Method: 利用哈密顿矩阵中常见的对角线结构，通过重构脉动阵列数据流，将对角线稀疏矩阵转换为密集计算，实现高利用率和性能。

Result: 在HamLib基准测试中，相比SIGMA、外积和Gustavson算法，平均性能分别提升10.26倍、33.58倍和53.15倍，峰值加速达127.03倍，同时能耗平均降低471.55倍，最高达4630.58倍。

Conclusion: 该对角线优化加速器显著提升了经典哈密顿模拟的可扩展性，为量子系统研究和量子设备验证提供了高效的解决方案。

Abstract: Hamiltonian simulation is a key workload in quantum computing, enabling the
study of complex quantum systems and serving as a critical tool for classical
verification of quantum devices. However, it is computationally challenging
because the Hilbert space dimension grows exponentially with the number of
qubits. The growing dimensions make matrix exponentiation, the key kernel in
Hamiltonian simulations, increasingly expensive. Matrix exponentiation is
typically approximated by the Taylor series, which contains a series of matrix
multiplications. Since Hermitian operators are often sparse, sparse matrix
multiplication accelerators are essential for improving the scalability of
classical Hamiltonian simulation. Yet, existing accelerators are primarily
designed for machine learning workloads and tuned to their characteristic
sparsity patterns, which differ fundamentally from those in Hamiltonian
simulations that are often dominated by structured diagonals.
  In this work, we present \name, the first diagonal-optimized quantum
simulation accelerator. It exploits the diagonal structure commonly found in
problem-Hamiltonian (Hermitian) matrices and leverages a restructured systolic
array dataflow to transform diagonally sparse matrices into dense computations,
enabling high utilization and performance. Through detailed cycle-level
simulation of diverse benchmarks in HamLib, \name{} demonstrates average
performance improvements of $10.26\times$, $33.58\times$, and $53.15\times$
over SIGMA, Outer Product, and Gustavson's algorithm, respectively, with peak
speedups up to $127.03\times$ while reducing energy consumption by an average
of $471.55\times$ and up to $4630.58\times$ compared to SIGMA.

</details>


### [16] [Computing-In-Memory Aware Model Adaption For Edge Devices](https://arxiv.org/abs/2510.14379)
*Ming-Han Lin,Tian-Sheuan Chang*

Main category: cs.AR

TL;DR: 提出两阶段CIM感知模型适配方法，通过模型压缩、资源重分配和量化感知训练，在保持精度的同时提升CIM宏的利用率和吞吐量


<details>
  <summary>Details</summary>
Motivation: CIM宏在深度学习加速中面临宏尺寸限制和ADC精度问题，导致吞吐量和精度瓶颈

Method: 第一阶段：基于层重要性和宏尺寸约束进行模型压缩和资源重分配；第二阶段：结合部分和量化和ADC精度的量化感知训练

Result: CIM阵列利用率提升至90%，可同时激活256个字线，实现93%压缩率，精度与之前方法相当

Conclusion: 该方法有效解决了CIM宏的吞吐量和精度瓶颈，显著提升了资源利用效率

Abstract: Computing-in-Memory (CIM) macros have gained popularity for deep learning
acceleration due to their highly parallel computation and low power
consumption. However, limited macro size and ADC precision introduce throughput
and accuracy bottlenecks. This paper proposes a two-stage CIM-aware model
adaptation process. The first stage compresses the model and reallocates
resources based on layer importance and macro size constraints, reducing model
weight loading latency while improving resource utilization and maintaining
accuracy. The second stage performs quantization-aware training, incorporating
partial sum quantization and ADC precision to mitigate quantization errors in
inference. The proposed approach enhances CIM array utilization to 90\%,
enables concurrent activation of up to 256 word lines, and achieves up to 93\%
compression, all while preserving accuracy comparable to previous methods.

</details>


### [17] [Low Power Vision Transformer Accelerator with Hardware-Aware Pruning and Optimized Dataflow](https://arxiv.org/abs/2510.14393)
*Ching-Lin Hsiung,Tian-Sheuan Chang*

Main category: cs.AR

TL;DR: 提出了一个低功耗视觉Transformer加速器，通过算法-硬件协同设计优化，针对短token长度的视觉Transformer中FFN成为主要计算瓶颈的问题，采用动态token剪枝、ReLU替换GELU和动态FFN2剪枝等方法，显著减少计算量和权重，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer加速器主要关注自注意力机制的优化，但对于token长度较短的视觉Transformer，FFN往往成为主要计算瓶颈，需要专门优化。

Method: 采用算法-硬件协同设计：1）硬件友好的动态token剪枝；2）用ReLU替换GELU激活函数；3）动态FFN2剪枝；硬件采用行向数据流和输出导向数据访问，支持动态操作。

Result: 操作量减少61.5%，FFN2权重减少59.3%，精度损失小于2%；在28nm工艺下实现496.4K门面积和232KB SRAM，峰值吞吐量1024 GOPS@1GHz，能效2.31 TOPS/W，面积效率858.61 GOPS/mm²。

Conclusion: 通过算法-硬件协同设计，成功优化了视觉Transformer中FFN的计算瓶颈，在保持精度的同时显著提升了能效和面积效率。

Abstract: Current transformer accelerators primarily focus on optimizing self-attention
due to its quadratic complexity. However, this focus is less relevant for
vision transformers with short token lengths, where the Feed-Forward Network
(FFN) tends to be the dominant computational bottleneck. This paper presents a
low power Vision Transformer accelerator, optimized through algorithm-hardware
co-design. The model complexity is reduced using hardware-friendly dynamic
token pruning without introducing complex mechanisms. Sparsity is further
improved by replacing GELU with ReLU activations and employing dynamic FFN2
pruning, achieving a 61.5\% reduction in operations and a 59.3\% reduction in
FFN2 weights, with an accuracy loss of less than 2\%. The hardware adopts a
row-wise dataflow with output-oriented data access to eliminate data
transposition, and supports dynamic operations with minimal area overhead.
Implemented in TSMC's 28nm CMOS technology, our design occupies 496.4K gates
and includes a 232KB SRAM buffer, achieving a peak throughput of 1024 GOPS at
1GHz, with an energy efficiency of 2.31 TOPS/W and an area efficiency of 858.61
GOPS/mm2.

</details>


### [18] [ColumnDisturb: Understanding Column-based Read Disturbance in Real DRAM Chips and Implications for Future Systems](https://arxiv.org/abs/2510.14750)
*İsmail Emir Yüksel,Ataberk Olgun,F. Nisa Bostancı,Haocong Luo,A. Giray Yağlıkçı,Onur Mutlu*

Main category: cs.AR

TL;DR: 本文发现了一种新的DRAM读取干扰现象ColumnDisturb，通过重复打开或保持DRAM行（攻击行）打开，可以跨多个DRAM子阵列干扰共享相同列的DRAM单元，导致比特翻转。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注RowHammer/RowPress等行级干扰，但缺乏对跨子阵列的列级干扰现象的系统研究。随着DRAM技术节点缩小，这种干扰可能变得更加严重。

Method: 使用216个DDR4和4个HBM2芯片进行实验，在多种操作条件下对ColumnDisturb现象进行系统表征，得出27个关键实验观察结果。

Result: ColumnDisturb影响所有三大制造商的芯片，技术节点越小干扰越严重；在标准DDR4刷新窗口内即可引发多个比特翻转；干扰影响的行数比保留故障多198倍。

Conclusion: ColumnDisturb现象对基于保留时间异质性的刷新机制构成严重挑战，随着技术节点缩小，该问题在未来DRAM芯片中可能更加恶化。

Abstract: We experimentally demonstrate a new widespread read disturbance phenomenon,
ColumnDisturb, in real commodity DRAM chips. By repeatedly opening or keeping a
DRAM row (aggressor row) open, we show that it is possible to disturb DRAM
cells through a DRAM column (i.e., bitline) and induce bitflips in DRAM cells
sharing the same columns as the aggressor row (across multiple DRAM subarrays).
With ColumnDisturb, the activation of a single row concurrently disturbs cells
across as many as three subarrays (e.g., 3072 rows) as opposed to
RowHammer/RowPress, which affect only a few neighboring rows of the aggressor
row in a single subarray. We rigorously characterize ColumnDisturb and its
characteristics under various operational conditions using 216 DDR4 and 4 HBM2
chips from three major manufacturers. Among our 27 key experimental
observations, we highlight two major results and their implications.
  First, ColumnDisturb affects chips from all three major manufacturers and
worsens as DRAM technology scales down to smaller node sizes (e.g., the minimum
time to induce the first ColumnDisturb bitflip reduces by up to 5.06x). We
observe that, in existing DRAM chips, ColumnDisturb induces bitflips within a
standard DDR4 refresh window (e.g., in 63.6 ms) in multiple cells. We predict
that, as DRAM technology node size reduces, ColumnDisturb would worsen in
future DRAM chips, likely causing many more bitflips in the standard refresh
window. Second, ColumnDisturb induces bitflips in many (up to 198x) more rows
than retention failures. Therefore, ColumnDisturb has strong implications for
retention-aware refresh mechanisms that leverage the heterogeneity in cell
retention times: our detailed analyses show that ColumnDisturb greatly reduces
the benefits of such mechanisms.

</details>
