<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 5]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [On the Duality of Task and Actor Programming Models](https://arxiv.org/abs/2508.16522)
*Rohan Yadav,Joseph Guman,Sean Treichler,Michael Garland,Alex Aiken,Fredrik Kjolstad,Michael Bauer*

Main category: cs.PL

TL;DR: 这篇论文探索了任务基和演员基并行编程模型之间的对偶性，并提出技术使任务基系统在保持生产力的同时达到类似演员基系统的性能水平。


<details>
  <summary>Details</summary>
Motivation: 并行编程模型在异构计算机上日益普及，任务模型和演员模型各有优势：任务模型生产力高但性能较低，演员模型性能高但开发难度大。需要找到两者的平衡点。

Method: 证明任务基和演员基编程模型存在对偶性，并将这种对偶性扩展到性能层面。在Realm（显式并行任务运行时）和Legion（隐式并行任务运行时）中应用相关技术来降低开销。

Result: 技术使Realm的开销降低1.7-5.3倍，性能接近Charm++和MPI等优化演员系统的2倍内。Legion应用的强缩放性能提升1.3-5.0倍。

Conclusion: 通过揭示任务基和演员基模型的对偶性，可以在不丢失任务模型生产力优势的前提下，实现类似演员模型的高性能。

Abstract: Programming models for distributed and heterogeneous machines are rapidly
growing in popularity to meet the demands of modern workloads. Task and actor
models are common choices that offer different trade-offs between development
productivity and achieved performance. Task-based models offer better
productivity and composition of software, whereas actor-based models routinely
deliver better peak performance due to lower overheads. While task-based and
actor-based models appear to be different superficially, we demonstrate these
programming models are duals of each other. Importantly, we show that this
duality extends beyond functionality to performance, and elucidate techniques
that let task-based systems deliver performance competitive with actor-based
systems without compromising productivity. We apply these techniques to both
Realm, an explicitly parallel task-based runtime, as well as Legion, an
implicitly parallel task-based runtime. We show these techniques reduce Realm's
overheads by between 1.7-5.3x, coming within a factor of two of the overheads
imposed by heavily optimized actor-based systems like Charm++ and MPI. We
further show that our techniques enable between 1.3-5.0x improved strong
scaling of unmodified Legion applications.

</details>


### [2] [Correctness-Guaranteed Code Generation via Constrained Decoding](https://arxiv.org/abs/2508.15866)
*Lingxiao Li,Salar Rahili,Yiwei Zhao*

Main category: cs.PL

TL;DR: 提出了一种基于上下文敏感解析器的约束解码算法，通过动态解析器树(ToP)框架生成语义正确的程序，确保运行时关键组件的正确性


<details>
  <summary>Details</summary>
Motivation: 语言模型在代码生成中难以保证程序正确性，特别是在视频游戏和机器人等需要一次性正确性的运行时关键领域

Method: 使用上下文敏感解析器，在每一步输出满足关键不可扩展属性的正则表达式来指导下一个token序列的生成；采用动态解析器树(ToP)框架，每个解析器对应带有上下文信息的模块化上下文无关文法

Result: 通过sLua（强类型Lua变体）验证了方法能够生成符合任何规定脚本API的语义正确程序，并在roguelike游戏中验证了运行时正确性

Conclusion: 该方法能够为运行时关键组件提供语义保证，并可通过精心设计扩展到运行时正确性

Abstract: Language Models (LMs) are increasingly being used for code generation, but
ensuring the correctness of generated programs remains a significant challenge.
Although imperfect code may be acceptable during software development with
human oversight, domains such as video games and robotics require one-shot
correctness for runtime-critical components. We present a constrained decoding
algorithm for generating semantically correct programs that incorporates a
context-sensitive parser, which, at each step, outputs a regular expression
that satisfies a critical non-extensible property to guide the generation of
the next token sequence that can continue to a correct program. To build such a
context-sensitive parser, we propose a framework of a dynamic tree of parsers
(ToP) during parsing, where each parser corresponds to a modular context-free
grammar enriched with contextual information such as variable scopes and type
constraints, with tree branches representing ambiguity in the future code
segment. We demonstrate our approach through sLua, a strongly typed variant of
Lua, showing that our method can generate semantically correct programs
conforming to any prescribed scripting API. We further show that, with careful
design, our semantic guarantees extend to runtime correctness, as validated in
the application of generating game mechanics for a roguelike video game.

</details>


### [3] [Automated Formal Verification of a Software Fault Isolation System](https://arxiv.org/abs/2508.15898)
*Matthew Sotoudeh,Zachary Yedidia*

Main category: cs.PL

TL;DR: 本文通过自动化形式验证方法，验证了轻量级故障隔离(LFI)系统的验证器的正确性，确保其接受的程序不会读写沙盒区域外的内存。


<details>
  <summary>Details</summary>
Motivation: 软件故障隔离(SFI)验证器中的声音性漏洞会破坏SFI安全模型，允许沙盒代码读取受保护内存，因此需要验证验证器的正确性。

Method: 采用自动化形式验证技术，对LFI系统的验证器进行形式化验证。

Result: 成功证明LFI验证器接受的程序永远不会读写指定沙盒区域外的内存。

Conclusion: 通过形式化验证可以确保SFI验证器的正确性，从而保障软件故障隔离系统的安全性。

Abstract: Software fault isolation (SFI) is a popular way to sandbox untrusted
software. A key component of SFI is the verifier that checks the untrusted code
is written in a subset of the machine language that guarantees it never reads
or writes outside of a region of memory dedicated to the sandbox. Soundness
bugs in the SFI verifier would break the SFI security model and allow the
supposedly sandboxed code to read protected memory. In this paper, we address
the concern of SFI verifier bugs by performing an automated formal verification
of a recent SFI system called Lightweight Fault Isolation (LFI). In particular,
we formally verify that programs accepted by the LFI verifier never read or
write to memory outside of a designated sandbox region.

</details>


### [4] [Synthesizing DSLs for Few-Shot Learning](https://arxiv.org/abs/2508.16063)
*Paul Krogmeier,P. Madhusudan*

Main category: cs.PL

TL;DR: 研究如何为符号领域的小样本学习合成领域特定语言(DSL)，证明在特定条件下该问题是可判定的


<details>
  <summary>Details</summary>
Motivation: 解决符号领域中为小样本学习任务自动生成合适的领域特定语言的问题，以提高学习效率和泛化能力

Method: 使用树自动机评估语义，基于语法分析树深度定义表达式大小，研究正则树集对应的语法解决方案

Result: 证明了在树自动机可评估语义且表达式大小对应语法分析树深度的条件下，DSL合成问题是可判定的

Conclusion: 为符号小样本学习提供了一种自动DSL合成框架，并建立了相关理论的可判定性基础

Abstract: We study the problem of synthesizing domain-specific languages (DSLs) for
few-shot learning in symbolic domains. Given a base language and instances of
few-shot learning problems, where each instance is split into training and
testing samples, the DSL synthesis problem asks for a grammar over the base
language that guarantees that small expressions solving training samples also
solve corresponding testing samples. We prove that the problem is decidable for
a class of languages whose semantics over fixed structures can be evaluated by
tree automata and when expression size corresponds to parse tree depth in the
grammar, and, furthermore, the grammars solving the problem correspond to a
regular set of trees. We also prove decidability results for variants of the
problem where DSLs are only required to express solutions for input learning
problems and where DSLs are defined using macro grammars.

</details>


### [5] [Leveraging Large Language Models to Detect Missed Peephole Optimizations](https://arxiv.org/abs/2508.16125)
*Zhenyang Xu,Hongxu Xu,Yongqiang Tian,Xintong Zhou,Chengnian Sun*

Main category: cs.PL

TL;DR: Lampo是一个利用大型语言模型(LLMs)自动检测编译器窥孔优化的框架，通过结合LLMs的代码优化能力和严格的正确性验证，在LLVM生态系统中成功发现了多个遗漏的优化机会。


<details>
  <summary>Details</summary>
Motivation: 窥孔优化是编译器优化中的重要类别，但由于指令集极其复杂多样，发现新的有效窥孔优化具有挑战性。现有方法要么扩展性差，要么只能捕获有限的优化子集。

Method: 提出Lampo框架，协同结合LLMs的创造性代码优化能力和翻译验证工具执行的严格正确性验证，采用反馈驱动的迭代过程。

Result: 在LLVM生态系统中，Lampo平均能成功检测出25个先前报告的遗漏优化中的17个，其中22个可以通过不同LLMs找到。相比之下，最先进的超级优化器Souper只识别了15个。在7个月开发中，Lampo发现了26个遗漏优化，15个已确认，6个已修复。

Conclusion: Lampo在持续检测遗漏窥孔优化方面展现出强大潜力，证明了LLMs与形式化验证工具结合的有效性。

Abstract: By replacing small, suboptimal instruction sequences within programs with a
more efficient equivalent, peephole optimization can not only directly optimize
code size and performance, but also potentially enables further transformations
in the subsequent optimization pipeline. Although peephole optimization is a
critical class of compiler optimizations, discovering new and effective
peephole optimizations is challenging as the instruction sets can be extremely
complex and diverse. Previous methods either do not scale well or can only
capture a limited subset of peephole optimizations. In this work, we leverage
Large Language Models (LLMs) to detect missed peephole optimizations. We
propose Lampo, a novel automated framework that synergistically combines the
creative but unreliable code optimization ability of LLMs with rigorous
correctness verification performed by translation validation tools, integrated
in a feedback-driven iterative process. Through a comprehensive evaluation
within LLVM ecosystems, we show that Lampo can successfully detect up to 17 out
of 25 previously reported missed optimizations in LLVM on average, and that 22
out of 25 can potentially be found by Lampo with different LLMs. For
comparison, the state-of-the-art superoptimizer for LLVM, Souper, identified 15
of them. Moreover, within seven months of development and intermittent
experiments, Lampo found 26 missed peephole optimizations, 15 of which have
been confirmed and 6 already fixed. These results demonstrate Lampo's strong
potential in continuously detecting missed peephole optimizations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO Serving and Fast Scaling](https://arxiv.org/abs/2508.15919)
*Zahra Yousefijamarani,Xinglu Wang,Qian Wang,Morgan Lindsay Heisler,Taha Shabani,Niloofar Gholipour,Parham Yassini,Hong Chang,Kan Chen,Qiantao Zhang,Xiaolong Bai,Jiannan Wang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: HyperFlexis是一个统一的LLM服务系统，通过算法和系统级创新联合优化调度和扩展，支持多SLO调度、成本效益扩展决策和快速角色转换，显著提升SLO达成率和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型服务系统面临请求长度、优先级和阶段特定SLO高度可变性的挑战，需要实时调度、快速成本效益扩展以及支持并置和分离的Prefill/Decode架构。

Method: 系统采用多SLO感知调度器，利用预算估计和请求优先级确保主动SLO合规；支持P/D分离架构的prefill和decode阶段多SLO调度；提出设备到设备权重传输机制降低权重加载开销；实现成本效益扩展决策和快速P/D角色转换。

Result: 权重加载开销降低高达19.39倍，SLO达成率提升高达4.44倍，请求延迟降低65.82%，与最先进基线相比成本相当。

Conclusion: HyperFlexis通过统一的系统设计和多项优化技术，有效解决了LLM服务系统中的调度和扩展挑战，在保持成本竞争力的同时显著提升了服务质量和性能。

Abstract: Modern large language model (LLM) serving systems face challenges from highly
variable requests with diverse lengths, priorities, and stage-specific
service-level objectives (SLOs). Meeting these requires real-time scheduling,
rapid and cost-effective scaling, and support for both collocated and
disaggregated Prefill/Decode (P/D) architectures.
  We present \textbf{HyperFlexis}, a unified LLM serving system that integrates
algorithmic and system-level innovations to jointly optimize scheduling and
scaling under multiple SLOs. It features a multi-SLO-aware scheduler that
leverages budget estimation and request prioritization to ensure proactive SLO
compliance for both new and ongoing requests. The system supports prefill- and
decode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV
cache transfers. It also enables cost-effective scaling decisions,
prefill-decode instance linking during scaling, and rapid P/D role transitions.
To accelerate scaling and reduce cold-start latency, a device-to-device (D2D)
weight transfer mechanism is proposed that lowers weight loading overhead by up
to \textbf{19.39$\times$}. These optimizations allow the system to achieve up
to \textbf{4.44$\times$} higher SLO attainment, \textbf{65.82\%} lower request
latency, and cost parity with state-of-the-art baselines. The code will be
released soon.

</details>


### [7] [Generalizing Brooks' theorem via Partial Coloring is Hard Classically and Locally](https://arxiv.org/abs/2508.16308)
*Jan Bok,Avinandan Das,Anna Gujgiczer,Nikola Jedličková*

Main category: cs.DC

TL;DR: 本文研究了k-partial k-coloring问题的计算复杂性，发现当颜色数从k+1减少到k时，问题难度显著增加，在集中式和分布式模型中均表现出NP完全性和高时间下界。


<details>
  <summary>Details</summary>
Motivation: Das等人之前证明了k-partial (k+1)-coloring存在高效算法，但提出了k-partial k-coloring的分布式复杂性开放问题。本文旨在研究颜色数减少到k时的计算复杂性变化。

Method: 通过构造新颖的结构特征来描述"困难实例"，其中部分着色简化为完全着色，并使用复杂的图构造和不可区分性论证来证明下界。

Result: 证明对于k≥3，判断图是否可k-partial k-coloring是NP完全的；在分布式LOCAL模型中，即使图保证可着色，也需要Ω(n)轮计算时间。

Conclusion: 颜色数从k+1减少到k导致问题复杂性发生质变，从多项式时间可解变为NP完全，分布式复杂度从对数时间变为线性时间下界。

Abstract: We investigate the classical and distributed complexity of \emph{$k$-partial
$c$-coloring} where $c=k$, a natural generalization of Brooks' theorem where
each vertex should be colored from the palette $\{1,\ldots,c\} =
\{1,\ldots,k\}$ such that it must have at least $\min\{k, \deg(v)\}$ neighbors
colored differently. Das, Fraigniaud, and Ros{\'{e}}n~[OPODIS 2023] showed that
the problem of $k$-partial $(k+1)$-coloring admits efficient centralized and
distributed algorithms and posed an open problem about the status of the
distributed complexity of $k$-partial $k$-coloring. We show that the problem
becomes significantly harder when the number of colors is reduced from $k+1$ to
$k$ for every constant $k\geq 3$.
  In the classical setting, we prove that deciding whether a graph admits a
$k$-partial $k$-coloring is NP-complete for every constant $k \geq 3$,
revealing a sharp contrast with the linear-time solvable $(k+1)$-color case.
For the distributed LOCAL model, we establish an $\Omega(n)$-round lower bound
for computing $k$-partial $k$-colorings, even when the graph is guaranteed to
be $k$-partial $k$-colorable. This demonstrates an exponential separation from
the $O(\log^2 k \cdot \log n)$-round algorithms known for $(k+1)$-colorings.
  Our results leverage novel structural characterizations of ``hard instances''
where partial coloring reduces to proper coloring, and we construct intricate
graph gadgets to prove lower bounds via indistinguishability arguments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [ASIC-Agent: An Autonomous Multi-Agent System for ASIC Design with Benchmark Evaluation](https://arxiv.org/abs/2508.15940)
*Ahmed Allam,Youssef Mansour,Mohamed Shalan*

Main category: cs.AR

TL;DR: ASIC-Agent是一个专为数字ASIC设计任务设计的自主系统，通过多智能体架构增强基础LLM，解决了LLM在硬件设计中的执行、调试和记忆限制问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在RTL设计中表现出色，但在实际硬件设计工作流中存在无法执行代码、缺乏调试能力和长期记忆等限制，需要专门系统来解决这些挑战。

Method: 采用多智能体架构，包含RTL生成、验证、OpenLane硬化和Caravel芯片集成等专门子智能体，在沙盒环境中运行，并利用包含文档、API参考和错误知识的向量数据库。

Result: 当使用Claude 4 Sonnet作为基础LLM时，ASIC-Agent成功自动化了各种复杂度的ASIC设计任务，显示出显著加速ASIC设计工作流程的潜力。

Conclusion: ASIC-Agent系统展示了多智能体架构在硬件设计自动化中的有效性，为ASIC设计工作流程提供了重要的加速潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
Register Transfer Level (RTL) design, enabling high-quality code generation
from natural language descriptions. However, LLMs alone face significant
limitations in real-world hardware design workflows, including the inability to
execute code, lack of debugging capabilities, and absence of long-term memory.
To address these challenges, we present ASIC-Agent, an autonomous system
designed specifically for digital ASIC design tasks. ASIC-Agent enhances base
LLMs with a multi-agent architecture incorporating specialized sub-agents for
RTL generation, verification, OpenLane hardening, and Caravel chip integration,
all operating within a comprehensive sandbox environment with access to
essential hardware design tools. The system leverages a vector database
containing documentation, API references, error knowledge, and curated insights
from the open-source silicon community. To evaluate ASIC-Agent's performance,
we introduce ASIC-Agent-Bench, the first benchmark specifically designed to
assess agentic systems in hardware design tasks. We evaluate ASIC-Agent with
various base LLMs, providing quantitative comparisons and qualitative insights
into agent behavior across different design scenarios. Our results demonstrate
that ASIC-Agent, when powered by Claude 4 Sonnet, successfully automates a
broad range of ASIC design tasks spanning varying levels of complexity, showing
the potential of significantly accelerating the ASIC design workflow.

</details>


### [9] [Bare-Metal RISC-V + NVDLA SoC for Efficient Deep Learning Inference](https://arxiv.org/abs/2508.16095)
*Vineet Kumar,Ajay Kumar M,Yike Li,Shreejith Shanker,Deepu John*

Main category: cs.AR

TL;DR: 这篇论文提出了一种新题的SoC架构，通过硬件软件优化结合加速深度学习模型，适用于边缘计算应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决边缘计算中深度学习模型执行速度慢和存储效率低的问题，克服以往工作中复杂操作系统开销的限制。

Method: 采用硬件软件紧密耦合设计：将NVDLA加速器与Codasip uRISC-V核心相结合，并使用生成的空程应用代码（汇编）来减少操作系统开销。

Result: 在AMD ZCU102 FPGA板上以100 MHz频率测试，LeNet-5、ResNet-18和ResNet-50模型的推理时间分别为4.8 ms、16.2 ms和1.1 s，显著提升了执行速度和存储效率。

Conclusion: 该紧密耦合架构和空程工具流为边缘计算提供了高效的深度学习模型加速解决方案，具有良好的性能表现。

Abstract: This paper presents a novel System-on-Chip (SoC) architecture for
accelerating complex deep learning models for edge computing applications
through a combination of hardware and software optimisations. The hardware
architecture tightly couples the open-source NVIDIA Deep Learning Accelerator
(NVDLA) to a 32-bit, 4-stage pipelined RISC-V core from Codasip called uRISC_V.
To offload the model acceleration in software, our toolflow generates
bare-metal application code (in assembly), overcoming complex OS overheads of
previous works that have explored similar architectures. This tightly coupled
architecture and bare-metal flow leads to improvements in execution speed and
storage efficiency, making it suitable for edge computing solutions. We
evaluate the architecture on AMD's ZCU102 FPGA board using NVDLA-small
configuration and test the flow using LeNet-5, ResNet-18 and ResNet-50 models.
Our results show that these models can perform inference in 4.8 ms, 16.2 ms and
1.1 s respectively, at a system clock frequency of 100 MHz.

</details>


### [10] [Hardwired-Neurons Language Processing Units as General-Purpose Cognitive Substrates](https://arxiv.org/abs/2508.16151)
*Yang Liu,Yi Chen,Yongwei Zhao,Yifan Hao,Zifu Zheng,Weihao Kong,Zhangmai Li,Dongchen Jiang,Ruiyang Xia,Zhihong Ma,Zisheng Liu,Zhaoyong Wan,Yunqi Lu,Ximing Liu,Hongrui Guo,Zhihao Yang,Zhe Wang,Tianrui Ma,Mo Zou,Rui Zhang,Ling Li,Xing Hu,Zidong Du,Zhiwei Xu,Qi Guo,Tianshi Chen,Yunji Chen*

Main category: cs.AR

TL;DR: 本文提出了一种硬连线神经元语言处理单元(HNLPU)，通过Metal-Embedding方法将LLM权重参数物理硬连线到计算结构中，大幅降低了专用AI芯片的制造成本和能耗。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，传统GPU推理系统的能耗问题日益突出，需要专门的语言处理单元来提升计算效率，但直接硬连线大规模LLM参数的经济成本过高。

Method: 采用Metal-Embedding方法，将权重参数嵌入到金属线的3D拓扑结构中，而不是传统的2D硅器件网格，实现了15倍密度提升和112倍光罩成本降低。

Result: HNLPU实现了249,960 tokens/s的处理速度(比GPU快5,555倍)，36 tokens/J的能效(比GPU高1,047倍)，总芯片面积13,232 mm²，NRE成本1.84亿美元。

Conclusion: Metal-Embedding方法使专用AI芯片在经济上可行，HNLPU相比H100集群实现了8.57倍成本效益和230倍碳足迹减少，为高效LLM推理提供了可行解决方案。

Abstract: The rapid advancement of Large Language Models (LLMs) has established
language as a core general-purpose cognitive substrate, driving the demand for
specialized Language Processing Units (LPUs) tailored for LLM inference. To
overcome the growing energy consumption of LLM inference systems, this paper
proposes a Hardwired-Neurons Language Processing Unit (HNLPU), which physically
hardwires LLM weight parameters into the computational fabric, achieving
several orders of magnitude computational efficiency improvement by extreme
specialization. However, a significant challenge still lies in the scale of
modern LLMs. An ideal estimation on hardwiring gpt-oss 120 B requires
fabricating at least 6 billion dollars of photomask sets, rendering the
straightforward solution economically impractical. Addressing this challenge,
we propose the novel Metal-Embedding methodology. Instead of embedding weights
in a 2D grid of silicon device cells, Metal-Embedding embeds weight parameters
into the 3D topology of metal wires. This brings two benefits: (1) a 15x
increase in density, and (2) 60 out of 70 layers of photomasks are made
homogeneous across chips, including all EUV photomasks. In total,
Metal-Embedding reduced the photomask cost by 112x, bringing the Non-Recurring
Engineering (NRE) cost of HNLPU into an economically viable range. Experimental
results show that HNLPU achieved 249,960 tokens/s (5,555x/85x of GPU/WSE), 36
tokens/J (1,047x/283x of GPU/WSE), 13,232 mm2 total die area (29% inscribed
rectangular area in a 300 mm wafer), \$184M estimated NRE at 5 nm technology.
Analysis shows that HNLPU achieved 8.57x cost-effectiveness and 230x carbon
footprint reduction compared to H100 clusters, under an annual weight updating
assumption.

</details>


### [11] [RIROS: A Parallel RTL Fault SImulation FRamework with TwO-Dimensional Parallelism and Unified Schedule](https://arxiv.org/abs/2508.16376)
*Jiaping Tang,Jianan Mu,Zizhen Liu,Ge Yu,Tenghui Hua,Bin Sun,Silin Liu,Jing Ye,Huawei Li*

Main category: cs.AR

TL;DR: 提出了一种二维并行方法RIROS，结合结构级和故障级并行，通过统一计算/全局同步调度消除RTL故障模拟中的气泡，相比现有技术实现7-11倍性能提升


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶等安全关键应用的发展，芯片功能安全验证需要大量耗时的RTL故障模拟，传统单维并行方法因任务负载不均衡而效率低下

Method: 分析故障传播路径识别两类任务：少量高负载任务和大量低负载任务。使用结构级并行配合工作窃取机制处理低负载任务，故障级并行拆分高负载任务。提出统一计算/全局同步调度方法

Result: 实验结果显示，相比最先进的RTL故障模拟工具和商业工具，性能分别提升7.0倍和11.0倍

Conclusion: 二维并行方法和统一调度策略有效解决了RTL故障模拟中的负载不均衡问题，显著提高了模拟效率

Abstract: With the rapid development of safety-critical applications such as autonomous
driving and embodied intelligence, the functional safety of the corresponding
electronic chips becomes more critical. Ensuring chip functional safety
requires performing a large number of time-consuming RTL fault simulations
during the design phase, significantly increasing the verification cycle. To
meet time-to-market demands while ensuring thorough chip verification, parallel
acceleration of RTL fault simulation is necessary. Due to the dynamic nature of
fault propagation paths and varying fault propagation capabilities, task loads
in RTL fault simulation are highly imbalanced, making traditional
singledimension parallel methods, such as structural-level parallelism,
ineffective. Through an analysis of fault propagation paths and task loads, we
identify two types of tasks in RTL fault simulation: tasks that are few in
number but high in load, and tasks that are numerous but low in load. Based on
this insight, we propose a two-dimensional parallel approach that combines
structurallevel and fault-level parallelism to minimize bubbles in RTL fault
simulation. Structural-level parallelism combining with workstealing mechanism
is used to handle the numerous low-load tasks, while fault-level parallelism is
applied to split the high-load tasks. Besides, we deviate from the traditional
serial execution model of computation and global synchronization in RTL
simulation by proposing a unified computation/global synchronization scheduling
approach, which further eliminates bubbles. Finally, we implemented a parallel
RTL fault simulation framework, RIROS. Experimental results show a performance
improvement of 7.0 times and 11.0 times compared to the state-of-the-art RTL
fault simulation and a commercial tool.

</details>
