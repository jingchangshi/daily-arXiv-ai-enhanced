{"id": "2601.22557", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.22557", "abs": "https://arxiv.org/abs/2601.22557", "authors": ["Ke Du", "William Mansky", "Paolo G. Giarrusso", "Gregory Malecha"], "title": "Recursive Mutexes in Separation Logic", "comment": null, "summary": "Mutexes (i.e., locks) are well understood in separation logic, and can be specified in terms of either protecting an invariant or atomically changing the state of the lock. In this abstract, we develop the same styles of specifications for \\emph{recursive} mutexes, a common variant of mutexes in object-oriented languages such as C++ and Java. A recursive mutex can be acquired any number of times by the same thread, and our specifications treat all acquires/releases uniformly, with clients only needing to determine whether they hold the mutex when accessing the lock invariant.", "AI": {"tldr": "\u8bba\u6587\u4e3a\u9012\u5f52\u4e92\u65a5\u9501\uff08recursive mutex\uff09\u5f00\u53d1\u4e86\u5206\u79bb\u903b\u8f91\u89c4\u8303\uff0c\u7c7b\u4f3c\u4e8e\u666e\u901a\u4e92\u65a5\u9501\u7684\u4e24\u79cd\u89c4\u8303\u98ce\u683c\uff1a\u4fdd\u62a4\u4e0d\u53d8\u5f0f\u6216\u539f\u5b50\u6539\u53d8\u9501\u72b6\u6001\u3002", "motivation": "\u9012\u5f52\u4e92\u65a5\u9501\u5728C++\u3001Java\u7b49\u9762\u5411\u5bf9\u8c61\u8bed\u8a00\u4e2d\u5f88\u5e38\u89c1\uff0c\u4f46\u73b0\u6709\u7684\u5206\u79bb\u903b\u8f91\u89c4\u8303\u4e3b\u8981\u9488\u5bf9\u666e\u901a\u4e92\u65a5\u9501\u3002\u9700\u8981\u4e3a\u9012\u5f52\u4e92\u65a5\u9501\u5f00\u53d1\u7c7b\u4f3c\u7684\u89c4\u8303\uff0c\u4f7f\u5176\u80fd\u591f\u88ab\u540c\u4e00\u7ebf\u7a0b\u591a\u6b21\u83b7\u53d6\uff0c\u540c\u65f6\u4fdd\u6301\u89c4\u8303\u7684\u7b80\u6d01\u6027\u3002", "method": "\u4e3a\u9012\u5f52\u4e92\u65a5\u9501\u5f00\u53d1\u4e24\u79cd\u98ce\u683c\u7684\u5206\u79bb\u903b\u8f91\u89c4\u8303\uff1a1\uff09\u57fa\u4e8e\u4fdd\u62a4\u4e0d\u53d8\u5f0f\u7684\u89c4\u8303\uff1b2\uff09\u57fa\u4e8e\u539f\u5b50\u72b6\u6001\u53d8\u5316\u7684\u89c4\u8303\u3002\u89c4\u8303\u8bbe\u8ba1\u4f7f\u5f97\u6240\u6709\u83b7\u53d6/\u91ca\u653e\u64cd\u4f5c\u90fd\u88ab\u7edf\u4e00\u5904\u7406\uff0c\u5ba2\u6237\u7aef\u53ea\u9700\u5728\u8bbf\u95ee\u9501\u4e0d\u53d8\u5f0f\u65f6\u5224\u65ad\u81ea\u5df1\u662f\u5426\u6301\u6709\u8be5\u9501\u3002", "result": "\u6210\u529f\u4e3a\u9012\u5f52\u4e92\u65a5\u9501\u5f00\u53d1\u4e86\u5206\u79bb\u903b\u8f91\u89c4\u8303\uff0c\u8fd9\u4e9b\u89c4\u8303\u80fd\u591f\u5904\u7406\u540c\u4e00\u7ebf\u7a0b\u591a\u6b21\u83b7\u53d6\u9501\u7684\u60c5\u51b5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u666e\u901a\u4e92\u65a5\u9501\u89c4\u8303\u7c7b\u4f3c\u7684\u7ed3\u6784\u548c\u7b80\u6d01\u6027\u3002", "conclusion": "\u9012\u5f52\u4e92\u65a5\u9501\u53ef\u4ee5\u5728\u5206\u79bb\u903b\u8f91\u4e2d\u5f97\u5230\u4e0e\u666e\u901a\u4e92\u65a5\u9501\u7c7b\u4f3c\u7684\u89c4\u8303\u5904\u7406\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u83b7\u53d6/\u91ca\u653e\u64cd\u4f5c\u5904\u7406\uff0c\u7b80\u5316\u4e86\u5ba2\u6237\u7aef\u7684\u4f7f\u7528\uff0c\u53ea\u9700\u5728\u8bbf\u95ee\u9501\u4fdd\u62a4\u7684\u6570\u636e\u65f6\u5224\u65ad\u9501\u7684\u6301\u6709\u72b6\u6001\u3002"}}
{"id": "2601.22476", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.22476", "abs": "https://arxiv.org/abs/2601.22476", "authors": ["Ruizhe Zhong", "Xingbo Du", "Junchi Yan"], "title": "RulePlanner: All-in-One Reinforcement Learner for Unifying Design Rules in 3D Floorplanning", "comment": null, "summary": "Floorplanning determines the coordinate and shape of each module in Integrated Circuits. With the scaling of technology nodes, in floorplanning stage especially 3D scenarios with multiple stacked layers, it has become increasingly challenging to adhere to complex hardware design rules. Current methods are only capable of handling specific and limited design rules, while violations of other rules require manual and meticulous adjustment. This leads to labor-intensive and time-consuming post-processing for expert engineers. In this paper, we propose an all-in-one deep reinforcement learning-based approach to tackle these challenges, and design novel representations for real-world IC design rules that have not been addressed by previous approaches. Specifically, the processing of various hardware design rules is unified into a single framework with three key components: 1) novel matrix representations to model the design rules, 2) constraints on the action space to filter out invalid actions that cause rule violations, and 3) quantitative analysis of constraint satisfaction as reward signals. Experiments on public benchmarks demonstrate the effectiveness and validity of our approach. Furthermore, transferability is well demonstrated on unseen circuits. Our framework is extensible to accommodate new design rules, thus providing flexibility to address emerging challenges in future chip design. Code will be available at: https://github.com/Thinklab-SJTU/EDA-AI", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u74063D\u82af\u7247\u5e03\u5c40\u4e2d\u7684\u590d\u6742\u8bbe\u8ba1\u89c4\u5219\u7ea6\u675f\uff0c\u901a\u8fc7\u77e9\u9635\u8868\u793a\u3001\u52a8\u4f5c\u7a7a\u95f4\u7ea6\u675f\u548c\u5956\u52b1\u4fe1\u53f7\u5b9e\u73b0\u81ea\u52a8\u5316\u5e03\u5c40\u4f18\u5316\u3002", "motivation": "\u968f\u7740\u6280\u672f\u8282\u70b9\u7f29\u5c0f\uff0c3D\u591a\u5c42\u5806\u53e0\u82af\u7247\u5e03\u5c40\u9762\u4e34\u590d\u6742\u8bbe\u8ba1\u89c4\u5219\u7ea6\u675f\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u6709\u9650\u89c4\u5219\uff0c\u5176\u4ed6\u89c4\u5219\u8fdd\u53cd\u9700\u8981\u4e13\u5bb6\u624b\u52a8\u8c03\u6574\uff0c\u5bfc\u81f4\u540e\u5904\u7406\u5de5\u4f5c\u7e41\u91cd\u8017\u65f6\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u8bbe\u8ba1\u89c4\u5219\u7684\u77e9\u9635\u8868\u793a\uff1b2) \u52a8\u4f5c\u7a7a\u95f4\u7ea6\u675f\u4ee5\u8fc7\u6ee4\u5bfc\u81f4\u89c4\u5219\u8fdd\u53cd\u7684\u65e0\u6548\u52a8\u4f5c\uff1b3) \u7ea6\u675f\u6ee1\u8db3\u5ea6\u7684\u5b9a\u91cf\u5206\u6790\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6b63\u786e\u6027\uff0c\u5728\u672a\u89c1\u7535\u8def\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u6846\u67b6\u53ef\u6269\u5c55\u4ee5\u9002\u5e94\u65b0\u8bbe\u8ba1\u89c4\u5219\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u672a\u6765\u82af\u7247\u8bbe\u8ba1\u4e2d\u7684\u65b0\u5174\u6311\u6218\u63d0\u4f9b\u4e86\u7075\u6d3b\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u7edf\u4e00\u5904\u7406\u591a\u79cd\u786c\u4ef6\u8bbe\u8ba1\u89c4\u5219\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u63d0\u9ad8\u5e03\u5c40\u6548\u7387\u3002"}}
{"id": "2601.22438", "categories": ["cs.DC", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22438", "abs": "https://arxiv.org/abs/2601.22438", "authors": ["Shangshu Qian", "Kipling Liu", "P. C. Sruthi", "Lin Tan", "Yongle Zhang"], "title": "Towards Resiliency in Large Language Model Serving with KevlarFlow", "comment": null, "summary": "Large Language Model (LLM) serving systems remain fundamentally fragile, where frequent hardware faults in hyperscale clusters trigger disproportionate service outages in the software stack. Current recovery mechanisms are prohibitively slow, often requiring up to 10 minutes to reinitialize resources and reload massive model weights. We introduce KevlarFlow, a fault tolerant serving architecture designed to bridge the gap between hardware unreliability and service availability. KevlarFlow leverages 1) decoupled model parallelism initialization, 2) dynamic traffic rerouting, and 3) background KV cache replication to maintain high throughput during partial failures. Our evaluation demonstrates that KevlarFlow reduces mean-time-to-recovery (MTTR) by 20x and, under failure conditions, improves average latency by 3.1x, 99th percentile (p99) latency by 2.8x, average time-to-first-token (TTFT) by 378.9x, and p99 TTFT by 574.6x with negligible runtime overhead in comparison to state-of-the-art LLM serving systems.", "AI": {"tldr": "KevlarFlow\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7684\u5bb9\u9519\u67b6\u6784\uff0c\u901a\u8fc7\u89e3\u8026\u6a21\u578b\u5e76\u884c\u521d\u59cb\u5316\u3001\u52a8\u6001\u6d41\u91cf\u91cd\u8def\u7531\u548c\u540e\u53f0KV\u7f13\u5b58\u590d\u5236\uff0c\u663e\u8457\u964d\u4f4e\u6545\u969c\u6062\u590d\u65f6\u95f4\u5e76\u63d0\u5347\u670d\u52a1\u6027\u80fd\u3002", "motivation": "\u5f53\u524dLLM\u670d\u52a1\u7cfb\u7edf\u5728\u8d85\u5927\u89c4\u6a21\u96c6\u7fa4\u4e2d\u9762\u4e34\u786c\u4ef6\u6545\u969c\u9891\u7e41\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u670d\u52a1\u4e2d\u65ad\u3002\u73b0\u6709\u6062\u590d\u673a\u5236\u8fc7\u6162\uff08\u9700\u8981\u957f\u8fbe10\u5206\u949f\u91cd\u65b0\u521d\u59cb\u5316\u8d44\u6e90\u548c\u52a0\u8f7d\u6a21\u578b\u6743\u91cd\uff09\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u53ef\u7528\u6027\u9700\u6c42\u3002", "method": "\u63d0\u51faKevlarFlow\u67b6\u6784\uff0c\u91c7\u7528\u4e09\u79cd\u5173\u952e\u6280\u672f\uff1a1) \u89e3\u8026\u6a21\u578b\u5e76\u884c\u521d\u59cb\u5316\uff0c2) \u52a8\u6001\u6d41\u91cf\u91cd\u8def\u7531\uff0c3) \u540e\u53f0KV\u7f13\u5b58\u590d\u5236\uff0c\u4ee5\u5728\u90e8\u5206\u6545\u969c\u65f6\u7ef4\u6301\u9ad8\u541e\u5410\u91cf\u3002", "result": "KevlarFlow\u5c06\u5e73\u5747\u6062\u590d\u65f6\u95f4(MTTR)\u964d\u4f4e20\u500d\uff0c\u5728\u6545\u969c\u6761\u4ef6\u4e0b\uff1a\u5e73\u5747\u5ef6\u8fdf\u63d0\u53473.1\u500d\uff0cp99\u5ef6\u8fdf\u63d0\u53472.8\u500d\uff0c\u5e73\u5747\u9996\u6b21\u4ee4\u724c\u65f6\u95f4(TTFT)\u63d0\u5347378.9\u500d\uff0cp99 TTFT\u63d0\u5347574.6\u500d\uff0c\u8fd0\u884c\u65f6\u5f00\u9500\u53ef\u5ffd\u7565\u3002", "conclusion": "KevlarFlow\u6709\u6548\u89e3\u51b3\u4e86LLM\u670d\u52a1\u7cfb\u7edf\u7684\u5bb9\u9519\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u670d\u52a1\u53ef\u7528\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u8d85\u5927\u89c4\u6a21\u96c6\u7fa4\u4e2d\u7684\u53ef\u9760LLM\u670d\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.22862", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.22862", "abs": "https://arxiv.org/abs/2601.22862", "authors": ["Aurora Tom\u00e1s", "Juan Luis Arag\u00f3n", "Joan Manuel Parcerisa", "Antonio Gonz\u00e1lez"], "title": "Design of a GPU with Heterogeneous Cores for Graphics", "comment": null, "summary": "Heterogeneous architectures can deliver higher performance and energy efficiency than symmetric counterparts by using multiple architectures tuned to different types of workloads. While previous works focused on CPUs, this work extends the concept of heterogeneity to GPUs by proposing KHEPRI, a heterogeneous GPU architecture for graphics applications. Scenes in graphics applications showcase diversity, as they consist of many objects with varying levels of complexity. As a result, computational intensity and memory bandwidth requirements differ significantly across different regions of each scene. To address this variability, our proposal includes two types of cores: cores optimized for high ILP (compute-specialized) and cores that tolerate a higher number of simultaneously outstanding cache misses (memory-specialized). A key component of the proposed architecture is a novel work scheduler that dynamically assigns each part of a frame (i.e., a tile) to the most suitable core. Designing this scheduler is particularly challenging, as it must preserve data locality; otherwise, the benefits of heterogeneity may be offset by the penalty of additional cache misses. Additionally, the scheduler requires knowledge of each tile's characteristics before rendering it. For this purpose, KHEPRI leverages frame-to-frame coherence to predict the behavior of each tile based on that of the corresponding tile in the previous frame. Evaluations across a wide range of commercial animated graphics applications show that, compared to a traditional homogeneous GPU, KHEPRI achieves an average performance improvement of 9.2%, a throughput increase (frames per second) of 7.3%, and a total GPU energy reduction of 4.8%. Importantly, these benefits are achieved without any hardware overhead.", "AI": {"tldr": "KHEPRI\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784GPU\u67b6\u6784\uff0c\u5305\u542b\u8ba1\u7b97\u4e13\u7528\u548c\u5185\u5b58\u4e13\u7528\u4e24\u79cd\u6838\u5fc3\uff0c\u901a\u8fc7\u667a\u80fd\u8c03\u5ea6\u5668\u6839\u636e\u56fe\u5757\u7279\u6027\u52a8\u6001\u5206\u914d\u4efb\u52a1\uff0c\u5728\u4fdd\u6301\u6570\u636e\u5c40\u90e8\u6027\u7684\u540c\u65f6\u63d0\u5347\u56fe\u5f62\u5e94\u7528\u6027\u80fd\u3002", "motivation": "\u56fe\u5f62\u5e94\u7528\u4e2d\u7684\u573a\u666f\u5177\u6709\u591a\u6837\u6027\uff0c\u4e0d\u540c\u533a\u57df\u7684\u7269\u4f53\u590d\u6742\u5ea6\u5dee\u5f02\u5bfc\u81f4\u8ba1\u7b97\u5f3a\u5ea6\u548c\u5185\u5b58\u5e26\u5bbd\u9700\u6c42\u663e\u8457\u4e0d\u540c\u3002\u4f20\u7edf\u540c\u6784GPU\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u53d8\u5f02\u6027\uff0c\u9700\u8981\u5f02\u6784\u67b6\u6784\u6765\u4f18\u5316\u4e0d\u540c\u7c7b\u578b\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u63d0\u51faKHEPRI\u5f02\u6784GPU\u67b6\u6784\uff0c\u5305\u542b\u4e24\u79cd\u6838\u5fc3\uff1a\u4f18\u5316\u9ad8ILP\u7684\u8ba1\u7b97\u4e13\u7528\u6838\u5fc3\u548c\u5bb9\u5fcd\u66f4\u591a\u7f13\u5b58\u672a\u547d\u4e2d\u7684\u5185\u5b58\u4e13\u7528\u6838\u5fc3\u3002\u5173\u952e\u521b\u65b0\u662f\u667a\u80fd\u8c03\u5ea6\u5668\uff0c\u5229\u7528\u5e27\u95f4\u8fde\u8d2f\u6027\u9884\u6d4b\u6bcf\u4e2a\u56fe\u5757\u7279\u6027\uff0c\u52a8\u6001\u5206\u914d\u4efb\u52a1\u5230\u6700\u5408\u9002\u7684\u6838\u5fc3\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u5c40\u90e8\u6027\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u540c\u6784GPU\uff0cKHEPRI\u5728\u5546\u4e1a\u52a8\u753b\u56fe\u5f62\u5e94\u7528\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u53479.2%\uff0c\u5e27\u7387\u63d0\u53477.3%\uff0cGPU\u603b\u80fd\u8017\u964d\u4f4e4.8%\uff0c\u4e14\u65e0\u9700\u989d\u5916\u786c\u4ef6\u5f00\u9500\u3002", "conclusion": "\u5f02\u6784GPU\u67b6\u6784\u80fd\u6709\u6548\u5904\u7406\u56fe\u5f62\u5e94\u7528\u4e2d\u7684\u5de5\u4f5c\u8d1f\u8f7d\u591a\u6837\u6027\uff0c\u901a\u8fc7\u667a\u80fd\u8c03\u5ea6\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u548c\u80fd\u8017\u964d\u4f4e\uff0c\u8bc1\u660e\u4e86\u5f02\u6784\u6027\u5728GPU\u8bbe\u8ba1\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2601.22487", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.22487", "abs": "https://arxiv.org/abs/2601.22487", "authors": ["Ali Jahanshahi", "Sara Rashidi Golrouye", "Osten Anderson", "Nanpeng Yu", "Daniel Wong"], "title": "Coordinating Power Grid Frequency Regulation Service with Data Center Load Flexibility", "comment": null, "summary": "AI/ML data center growth have led to higher energy consumption and carbon emissions. The shift to renewable energy and growing data center energy demands can destabilize the power grid. Power grids rely on frequency regulation reserves, typically fossil-fueled power plants, to stabilize and balance the supply and demand of electricity. This paper sheds light on the hidden carbon emissions of frequency regulation service. Our work explores how modern GPU data centers can coordinate with power grids to reduce the need for fossil-fueled frequency regulation reserves. We first introduce a novel metric, Exogenous Carbon, to quantify grid-side carbon emission reductions resulting from data center participation in regulation service. We additionally introduce EcoCenter, a framework to maximize the amount of frequency regulation provision that GPU data centers can provide, and thus, reduce the amount of frequency regulation reserves necessary. We demonstrate that data center participation in frequency regulation can result in Exogenous carbon savings that oftentimes outweigh Operational carbon emissions.", "AI": {"tldr": "\u6570\u636e\u4e2d\u5fc3\u53c2\u4e0e\u7535\u7f51\u9891\u7387\u8c03\u8282\u53ef\u51cf\u5c11\u5316\u77f3\u71c3\u6599\u5907\u7528\u9700\u6c42\uff0c\u4ece\u800c\u964d\u4f4e\u78b3\u6392\u653e", "motivation": "AI/ML\u6570\u636e\u4e2d\u5fc3\u80fd\u8017\u589e\u957f\u5bfc\u81f4\u78b3\u6392\u653e\u589e\u52a0\uff0c\u800c\u53ef\u518d\u751f\u80fd\u6e90\u8f6c\u578b\u548c\u6570\u636e\u4e2d\u5fc3\u9700\u6c42\u589e\u957f\u53ef\u80fd\u7834\u574f\u7535\u7f51\u7a33\u5b9a\u3002\u7535\u7f51\u4f9d\u8d56\u5316\u77f3\u71c3\u6599\u53d1\u7535\u5382\u8fdb\u884c\u9891\u7387\u8c03\u8282\uff0c\u8fd9\u4f1a\u4ea7\u751f\u9690\u85cf\u78b3\u6392\u653e\u3002", "method": "\u63d0\u51fa\"\u5916\u6e90\u6027\u78b3\u6392\u653e\"\u6307\u6807\u91cf\u5316\u6570\u636e\u4e2d\u5fc3\u53c2\u4e0e\u8c03\u8282\u670d\u52a1\u5e26\u6765\u7684\u7535\u7f51\u4fa7\u78b3\u51cf\u6392\uff1b\u5f00\u53d1EcoCenter\u6846\u67b6\u6700\u5927\u5316GPU\u6570\u636e\u4e2d\u5fc3\u63d0\u4f9b\u9891\u7387\u8c03\u8282\u7684\u80fd\u529b\uff0c\u51cf\u5c11\u5316\u77f3\u71c3\u6599\u5907\u7528\u9700\u6c42\u3002", "result": "\u6570\u636e\u4e2d\u5fc3\u53c2\u4e0e\u9891\u7387\u8c03\u8282\u4ea7\u751f\u7684\u5916\u6e90\u6027\u78b3\u8282\u7ea6\u901a\u5e38\u8d85\u8fc7\u5176\u8fd0\u884c\u78b3\u6392\u653e\uff0c\u8868\u660e\u6570\u636e\u4e2d\u5fc3\u53ef\u4f5c\u4e3a\u7535\u7f51\u8c03\u8282\u8d44\u6e90\u51cf\u5c11\u6574\u4f53\u78b3\u6392\u653e\u3002", "conclusion": "GPU\u6570\u636e\u4e2d\u5fc3\u4e0e\u7535\u7f51\u534f\u8c03\u53c2\u4e0e\u9891\u7387\u8c03\u8282\u670d\u52a1\u53ef\u663e\u8457\u51cf\u5c11\u5bf9\u5316\u77f3\u71c3\u6599\u8c03\u8282\u5907\u7528\u7684\u9700\u6c42\uff0c\u964d\u4f4e\u6574\u4f53\u78b3\u6392\u653e\uff0c\u5b9e\u73b0\u6570\u636e\u4e2d\u5fc3\u4ece\u80fd\u6e90\u6d88\u8017\u8005\u5411\u7535\u7f51\u7a33\u5b9a\u8d21\u732e\u8005\u7684\u8f6c\u53d8\u3002"}}
{"id": "2601.23134", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.23134", "abs": "https://arxiv.org/abs/2601.23134", "authors": ["Zheyuan Hu", "Yifei Shi"], "title": "Machine Learning for Energy-Performance-aware Scheduling", "comment": "Zheyuan Hu and Yifei Shi contributed equally to this work", "summary": "In the post-Dennard era, optimizing embedded systems requires navigating complex trade-offs between energy efficiency and latency. Traditional heuristic tuning is often inefficient in such high-dimensional, non-smooth landscapes. In this work, we propose a Bayesian Optimization framework using Gaussian Processes to automate the search for optimal scheduling configurations on heterogeneous multi-core architectures. We explicitly address the multi-objective nature of the problem by approximating the Pareto Frontier between energy and time. Furthermore, by incorporating Sensitivity Analysis (fANOVA) and comparing different covariance kernels (e.g., Mat\u00e9rn vs. RBF), we provide physical interpretability to the black-box model, revealing the dominant hardware parameters driving system performance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5f02\u6784\u591a\u6838\u67b6\u6784\u4e0a\u81ea\u52a8\u641c\u7d22\u6700\u4f18\u8c03\u5ea6\u914d\u7f6e\uff0c\u5e73\u8861\u80fd\u8017\u4e0e\u5ef6\u8fdf\u7684\u6743\u8861\uff0c\u5e76\u901a\u8fc7\u654f\u611f\u6027\u5206\u6790\u63d0\u4f9b\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5728\u540e\u767b\u7eb3\u5fb7\u65f6\u4ee3\uff0c\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4f18\u5316\u9700\u8981\u5728\u80fd\u8017\u6548\u7387\u548c\u5ef6\u8fdf\u4e4b\u95f4\u8fdb\u884c\u590d\u6742\u6743\u8861\u3002\u4f20\u7edf\u542f\u53d1\u5f0f\u8c03\u4f18\u5728\u9ad8\u7ef4\u3001\u975e\u5e73\u6ed1\u7684\u4f18\u5316\u7a7a\u95f4\u4e2d\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8fd1\u4f3c\u5e15\u7d2f\u6258\u524d\u6cbf\u5904\u7406\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u654f\u611f\u6027\u5206\u6790\uff08fANOVA\uff09\u6bd4\u8f83\u4e0d\u540c\u534f\u65b9\u5dee\u6838\u51fd\u6570\uff08\u5982Mat\u00e9rn\u4e0eRBF\uff09\uff0c\u4e3a\u9ed1\u76d2\u6a21\u578b\u63d0\u4f9b\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u641c\u7d22\u5f02\u6784\u591a\u6838\u67b6\u6784\u4e0a\u7684\u6700\u4f18\u8c03\u5ea6\u914d\u7f6e\uff0c\u63ed\u793a\u9a71\u52a8\u7cfb\u7edf\u6027\u80fd\u7684\u4e3b\u5bfc\u786c\u4ef6\u53c2\u6570\uff0c\u63d0\u4f9b\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5d4c\u5165\u5f0f\u7cfb\u7edf\u5728\u591a\u7ef4\u4f18\u5316\u7a7a\u95f4\u4e2d\u7684\u8c03\u5ea6\u914d\u7f6e\u95ee\u9898\uff0c\u901a\u8fc7\u654f\u611f\u6027\u5206\u6790\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u786c\u4ef6\u53c2\u6570\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002"}}
{"id": "2601.22585", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.22585", "abs": "https://arxiv.org/abs/2601.22585", "authors": ["Heehoon Kim", "Jaehwan Lee", "Taejeoung Kim", "Jongwon Park", "Jinpyo Kim", "Pyongwon Suh", "Ryan H. Choi", "Sangwoo Lee", "Jaejin Lee"], "title": "HetCCL: Accelerating LLM Training with Heterogeneous GPUs", "comment": null, "summary": "The rapid growth of large language models is driving organizations to expand their GPU clusters, often with GPUs from multiple vendors. However, current deep learning frameworks lack support for collective communication across heterogeneous GPUs, leading to inefficiency and higher costs. We present HetCCL, a collective communication library that unifies vendor-specific backends and enables RDMA-based communication across GPUs without requiring driver modifications. HetCCL introduces two novel mechanisms that enable cross-vendor communication while leveraging optimized vendor libraries, NVIDIA NCCL and AMD RCCL. Evaluations on a multi-vendor GPU cluster show that HetCCL matches NCCL and RCCL performance in homogeneous setups while uniquely scaling in heterogeneous environments, enabling practical, high-performance training with both NVIDIA and AMD GPUs without changes to existing deep learning applications.", "AI": {"tldr": "HetCCL\u662f\u4e00\u4e2a\u5f02\u6784GPU\u96c6\u4f53\u901a\u4fe1\u5e93\uff0c\u652f\u6301\u8de8NVIDIA\u548cAMD GPU\u7684\u9ad8\u6027\u80fd\u901a\u4fe1\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7ec4\u7ec7\u9700\u8981\u6269\u5c55GPU\u96c6\u7fa4\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7f3a\u4e4f\u5bf9\u5f02\u6784GPU\u96c6\u4f53\u901a\u4fe1\u7684\u652f\u6301\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u6210\u672c\u589e\u52a0", "method": "HetCCL\u7edf\u4e00\u4e86\u5382\u5546\u7279\u5b9a\u7684\u540e\u7aef\uff0c\u652f\u6301\u57fa\u4e8eRDMA\u7684\u8de8GPU\u901a\u4fe1\uff0c\u65e0\u9700\u9a71\u52a8\u4fee\u6539\u3002\u5f15\u5165\u4e24\u79cd\u65b0\u9896\u673a\u5236\uff0c\u5728\u5229\u7528\u4f18\u5316\u7684\u5382\u5546\u5e93\uff08NVIDIA NCCL\u548cAMD RCCL\uff09\u7684\u540c\u65f6\u5b9e\u73b0\u8de8\u5382\u5546\u901a\u4fe1", "result": "\u5728\u591a\u5382\u5546GPU\u96c6\u7fa4\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cHetCCL\u5728\u540c\u7c7b\u8bbe\u7f6e\u4e2d\u4e0eNCCL\u548cRCCL\u6027\u80fd\u76f8\u5f53\uff0c\u5728\u5f02\u6784\u73af\u5883\u4e2d\u5177\u6709\u72ec\u7279\u7684\u6269\u5c55\u80fd\u529b", "conclusion": "HetCCL\u5b9e\u73b0\u4e86\u5b9e\u7528\u3001\u9ad8\u6027\u80fd\u7684NVIDIA\u548cAMD GPU\u6df7\u5408\u8bad\u7ec3\uff0c\u65e0\u9700\u4fee\u6539\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528"}}
{"id": "2601.23226", "categories": ["cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.23226", "abs": "https://arxiv.org/abs/2601.23226", "authors": ["Gourab Datta", "Sarah Safura Sharif", "Yaser Mike Banad"], "title": "Toward Digital Twins in 3D IC Packaging: A Critical Review of Physics, Data, and Hybrid Architectures", "comment": null, "summary": "Three-dimensional integrated circuit (3D IC) pack-aging and heterogeneous integration have emerged as central pillars of contemporary semiconductor scaling. Yet, the multi-physics coupling inherent to stacked architectures manifesting as thermal hot spots, warpage-induced stresses, and interconnect aging demands monitoring and control capabilities that surpass traditional offline metrology. Although Digital Twin (DT) technology provides a principled route to real-time reliability management, the existing literature remains fragmented and frequently blurs the distinction between static multiphysics simulation workflows and truly dynamic, closed-loop twins. This critical review distinguishes itself by addressing these deficiencies through three specific contributions. First, we clarify the Digital Twin hierarchy to resolve terminological ambiguity between digital models, shadows, and twins. Second, we synthesize three foundational enabling technologies: (1) physics-based modeling, emphasizing the shift from computationally intensive finite-element analysis (FEA) to real-time surrogate models; (2) data-driven paradigms, highlighting virtual metrology (VM) for inferring latent metrics; and (3) in-situ sensing, the nervous system coupling the physical stack to its virtual counterpart. Third, beyond a descriptive survey, we propose a unified hybrid DT architecture that leverages physics-informed machine learning (e.g., PINNs) to reconcile data scarcity with latency constraints. Finally, we outline a standards-aligned roadmap incorporating IEEE 1451 and UCIe protocols to accelerate the transition from passive digital shadows to autonomous, self-optimizing Digital Twins for 3D IC manufacturing and field operation.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u8bba\u6587\u6f84\u6e05\u4e863D IC\u6570\u5b57\u5b6a\u751f\u7684\u6982\u5ff5\u5c42\u6b21\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u7269\u7406\u5efa\u6a21\u3001\u6570\u636e\u9a71\u52a8\u548c\u539f\u4f4d\u4f20\u611f\u7684\u7edf\u4e00\u6df7\u5408\u67b6\u6784\uff0c\u5e76\u5236\u5b9a\u4e86\u6807\u51c6\u5316\u8def\u7ebf\u56fe\u3002", "motivation": "3D IC\u5c01\u88c5\u548c\u5f02\u6784\u96c6\u6210\u9762\u4e34\u70ed\u70ed\u70b9\u3001\u7fd8\u66f2\u5e94\u529b\u548c\u4e92\u8fde\u8001\u5316\u7b49\u591a\u7269\u7406\u8026\u5408\u6311\u6218\uff0c\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u79bb\u7ebf\u8ba1\u91cf\u5b66\u7684\u5b9e\u65f6\u53ef\u9760\u6027\u7ba1\u7406\u3002\u73b0\u6709\u6570\u5b57\u5b6a\u751f\u6587\u732e\u5b58\u5728\u6982\u5ff5\u6a21\u7cca\uff0c\u6df7\u6dc6\u4e86\u9759\u6001\u591a\u7269\u7406\u4eff\u771f\u4e0e\u771f\u6b63\u52a8\u6001\u95ed\u73af\u5b6a\u751f\u7684\u533a\u522b\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u5177\u4f53\u8d21\u732e\uff1a1) \u6f84\u6e05\u6570\u5b57\u5b6a\u751f\u5c42\u6b21\u7ed3\u6784\uff08\u6570\u5b57\u6a21\u578b\u3001\u6570\u5b57\u5f71\u5b50\u3001\u6570\u5b57\u5b6a\u751f\uff09\uff1b2) \u7efc\u5408\u4e09\u5927\u4f7f\u80fd\u6280\u672f\uff08\u7269\u7406\u5efa\u6a21\u3001\u6570\u636e\u9a71\u52a8\u8303\u5f0f\u3001\u539f\u4f4d\u4f20\u611f\uff09\uff1b3) \u63d0\u51fa\u7edf\u4e00\u6df7\u5408\u6570\u5b57\u5b6a\u751f\u67b6\u6784\uff0c\u5229\u7528\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u5ef6\u8fdf\u7ea6\u675f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u8def\u7ebf\u56fe\uff0c\u6574\u5408IEEE 1451\u548cUCIe\u534f\u8bae\uff0c\u52a0\u901f\u4ece\u88ab\u52a8\u6570\u5b57\u5f71\u5b50\u5411\u81ea\u4e3b\u81ea\u4f18\u5316\u6570\u5b57\u5b6a\u751f\u7684\u8f6c\u53d8\uff0c\u7528\u4e8e3D IC\u5236\u9020\u548c\u73b0\u573a\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u901a\u8fc7\u6f84\u6e05\u6982\u5ff5\u3001\u7efc\u5408\u6280\u672f\u548c\u63d0\u51fa\u7edf\u4e00\u67b6\u6784\uff0c\u4e3a3D IC\u6570\u5b57\u5b6a\u751f\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u65e8\u5728\u5b9e\u73b0\u4ece\u88ab\u52a8\u76d1\u63a7\u5230\u4e3b\u52a8\u4f18\u5316\u7684\u8f6c\u53d8\uff0c\u63d0\u53473D IC\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u7ba1\u7406\u3002"}}
{"id": "2601.22705", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.22705", "abs": "https://arxiv.org/abs/2601.22705", "authors": ["Qiaoling Chen", "Zhisheng Ye", "Tian Tang", "Peng Sun", "Boyu Tian", "Guoteng Wang", "Shenggui Li", "Yonggang Wen", "Zhenhua Han", "Tianwei Zhang"], "title": "CONCUR: High-Throughput Agentic Batch Inference of LLM via Congestion-Based Concurrency Control", "comment": null, "summary": "Batch inference for agentic workloads stresses the GPU key-value (KV) cache in a sustained and cumulative manner, often causing severe throughput degradation well before memory capacity is exhausted. We identify this phenomenon as middle-phase thrashing, a previously under-characterized pathology in which cache efficiency collapses as long-lived agents accumulate state over time.\n  We argue that mitigating this pathology requires moving beyond reactive, request-level cache management to proactive, agent-level admission control. Drawing inspiration from congestion control in distributed systems, we view the KV cache as a shared resource whose efficient utilization depends on feedback-driven regulation. Based on this insight, we present CONCUR, a lightweight control layer that regulates agent admission to bound aggregate cache pressure while preserving execution continuity. CONCUR adapts a cache-aware control algorithm to dynamically adjust the number of active agents using runtime cache signals.\n  Across large models and real-world agent workloads, CONCUR prevents middle-phase thrashing and improves batch inference throughput by up to 4.09x on Qwen3-32B and 1.9x on DeepSeek-V3, while remaining compatible with existing LLM serving systems.", "AI": {"tldr": "CONCUR\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u63a7\u5236\u5c42\uff0c\u901a\u8fc7\u57fa\u4e8e\u53cd\u9988\u7684\u4ee3\u7406\u51c6\u5165\u63a7\u5236\u6765\u9632\u6b62KV\u7f13\u5b58\u4e2d\u95f4\u9636\u6bb5\u6296\u52a8\uff0c\u663e\u8457\u63d0\u5347\u6279\u91cf\u63a8\u7406\u541e\u5410\u91cf\u3002", "motivation": "\u4ee3\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6279\u91cf\u63a8\u7406\u4f1a\u5bf9GPU\u952e\u503c\u7f13\u5b58\u9020\u6210\u6301\u7eed\u7d2f\u79ef\u7684\u538b\u529b\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u541e\u5410\u91cf\u4e0b\u964d\uff0c\u8fd9\u79cd\u73b0\u8c61\u88ab\u79f0\u4e3a\u4e2d\u95f4\u9636\u6bb5\u6296\u52a8\uff0c\u662f\u73b0\u6709\u7f13\u5b58\u7ba1\u7406\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u89e3\u51b3\u7684\u65b0\u95ee\u9898\u3002", "method": "CONCUR\u91c7\u7528\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u62e5\u585e\u63a7\u5236\u7684\u601d\u8def\uff0c\u5c06KV\u7f13\u5b58\u89c6\u4e3a\u5171\u4eab\u8d44\u6e90\uff0c\u901a\u8fc7\u57fa\u4e8e\u53cd\u9988\u7684\u8c03\u8282\u673a\u5236\u5b9e\u73b0\u4ee3\u7406\u7ea7\u522b\u7684\u51c6\u5165\u63a7\u5236\u3002\u5b83\u4f7f\u7528\u7f13\u5b58\u611f\u77e5\u63a7\u5236\u7b97\u6cd5\uff0c\u6839\u636e\u8fd0\u884c\u65f6\u7f13\u5b58\u4fe1\u53f7\u52a8\u6001\u8c03\u6574\u6d3b\u8dc3\u4ee3\u7406\u6570\u91cf\u3002", "result": "\u5728\u5927\u578b\u6a21\u578b\u548c\u5b9e\u9645\u4ee3\u7406\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\uff0cCONCUR\u9632\u6b62\u4e86\u4e2d\u95f4\u9636\u6bb5\u6296\u52a8\uff0c\u5728Qwen3-32B\u4e0a\u63d0\u5347\u6279\u91cf\u63a8\u7406\u541e\u5410\u91cf\u8fbe4.09\u500d\uff0c\u5728DeepSeek-V3\u4e0a\u63d0\u53471.9\u500d\uff0c\u4e14\u4e0e\u73b0\u6709LLM\u670d\u52a1\u7cfb\u7edf\u517c\u5bb9\u3002", "conclusion": "KV\u7f13\u5b58\u7ba1\u7406\u9700\u8981\u4ece\u53cd\u5e94\u5f0f\u7684\u8bf7\u6c42\u7ea7\u63a7\u5236\u8f6c\u5411\u4e3b\u52a8\u7684\u4ee3\u7406\u7ea7\u51c6\u5165\u63a7\u5236\uff0cCONCUR\u901a\u8fc7\u53cd\u9988\u9a71\u52a8\u7684\u8c03\u8282\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4e2d\u95f4\u9636\u6bb5\u6296\u52a8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6279\u91cf\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2601.22760", "categories": ["cs.DC", "cs.LG", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.22760", "abs": "https://arxiv.org/abs/2601.22760", "authors": ["Zhongzhen Wen", "Shudi Shao", "Zhong Li", "Yu Ge", "Tongtong Xu", "Yuanyi Lin", "Tian Zhang"], "title": "AscendCraft: Automatic Ascend NPU Kernel Generation via DSL-Guided Transcompilation", "comment": null, "summary": "The performance of deep learning models critically depends on efficient kernel implementations, yet developing high-performance kernels for specialized accelerators remains time-consuming and expertise-intensive. While recent work demonstrates that large language models (LLMs) can generate correct and performant GPU kernels, kernel generation for neural processing units (NPUs) remains largely underexplored due to domain-specific programming models, limited public examples, and sparse documentation. Consequently, directly generating AscendC kernels with LLMs yields extremely low correctness, highlighting a substantial gap between GPU and NPU kernel generation.\n  We present AscendCraft, a DSL-guided approach for automatic AscendC kernel generation. AscendCraft introduces a lightweight DSL that abstracts non-essential complexity while explicitly modeling Ascend-specific execution semantics. Kernels are first generated in the DSL using category-specific expert examples and then transcompiled into AscendC through structured, constraint-driven LLM lowering passes. Evaluated on MultiKernelBench across seven operator categories, AscendCraft achieves 98.1% compilation success and 90.4% functional correctness. Moreover, 46.2% of generated kernels match or exceed PyTorch eager execution performance, demonstrating that DSL-guided transcompilation can enable LLMs to generate both correct and competitive NPU kernels. Beyond benchmarks, AscendCraft further demonstrates its generality by successfully generating two correct kernels for newly proposed mHC architecture, achieving performance that substantially surpasses PyTorch eager execution.", "AI": {"tldr": "AscendCraft\uff1a\u4e00\u79cdDSL\u5f15\u5bfc\u7684\u81ea\u52a8AscendC\u5185\u6838\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7DSL\u62bd\u8c61\u590d\u6742\u6027\u5e76\u5efa\u6a21Ascend\u6267\u884c\u8bed\u4e49\uff0c\u5b9e\u73b0\u9ad8\u7f16\u8bd1\u6210\u529f\u7387\u548c\u529f\u80fd\u6b63\u786e\u6027\uff0c\u90e8\u5206\u5185\u6838\u6027\u80fd\u8d85\u8d8aPyTorch\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u4f9d\u8d56\u4e8e\u9ad8\u6548\u5185\u6838\u5b9e\u73b0\uff0c\u4f46\u4e3a\u4e13\u7528\u52a0\u901f\u5668\uff08\u5982NPU\uff09\u5f00\u53d1\u9ad8\u6027\u80fd\u5185\u6838\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002\u867d\u7136LLM\u80fd\u751f\u6210\u6b63\u786e\u7684GPU\u5185\u6838\uff0c\u4f46NPU\u5185\u6838\u751f\u6210\u56e0\u9886\u57df\u7279\u5b9a\u7f16\u7a0b\u6a21\u578b\u3001\u6709\u9650\u516c\u5f00\u793a\u4f8b\u548c\u7a00\u758f\u6587\u6863\u800c\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u76f4\u63a5\u751f\u6210AscendC\u5185\u6838\u7684\u6b63\u786e\u7387\u6781\u4f4e\u3002", "method": "\u63d0\u51faAscendCraft\u65b9\u6cd5\uff1a1\uff09\u5f15\u5165\u8f7b\u91cf\u7ea7DSL\uff0c\u62bd\u8c61\u975e\u5fc5\u8981\u590d\u6742\u6027\u5e76\u663e\u5f0f\u5efa\u6a21Ascend\u7279\u5b9a\u6267\u884c\u8bed\u4e49\uff1b2\uff09\u4f7f\u7528\u7c7b\u522b\u7279\u5b9a\u4e13\u5bb6\u793a\u4f8b\u5728DSL\u4e2d\u751f\u6210\u5185\u6838\uff1b3\uff09\u901a\u8fc7\u7ed3\u6784\u5316\u3001\u7ea6\u675f\u9a71\u52a8\u7684LLM\u964d\u4f4e\u4f20\u9012\u5c06DSL\u8f6c\u7f16\u8bd1\u4e3aAscendC\u3002", "result": "\u5728MultiKernelBench\u7684\u4e03\u4e2a\u7b97\u5b50\u7c7b\u522b\u4e0a\u8bc4\u4f30\uff1a\u8fbe\u523098.1%\u7f16\u8bd1\u6210\u529f\u7387\u548c90.4%\u529f\u80fd\u6b63\u786e\u6027\uff1b46.2%\u751f\u6210\u7684\u5185\u6838\u5339\u914d\u6216\u8d85\u8d8aPyTorch eager\u6267\u884c\u6027\u80fd\uff1b\u6210\u529f\u4e3a\u65b0\u63d0\u51fa\u7684mHC\u67b6\u6784\u751f\u6210\u4e24\u4e2a\u6b63\u786e\u5185\u6838\uff0c\u6027\u80fd\u5927\u5e45\u8d85\u8d8aPyTorch\u3002", "conclusion": "DSL\u5f15\u5bfc\u7684\u8f6c\u7f16\u8bd1\u65b9\u6cd5\u80fd\u4f7fLLM\u751f\u6210\u65e2\u6b63\u786e\u53c8\u5177\u6709\u7ade\u4e89\u529b\u7684NPU\u5185\u6838\uff0c\u89e3\u51b3\u4e86NPU\u5185\u6838\u751f\u6210\u7684\u6311\u6218\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2601.22963", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.22963", "abs": "https://arxiv.org/abs/2601.22963", "authors": ["Kegan Dougal"], "title": "ERA: Epoch-Resolved Arbitration for Duelling Admins in Group Management CRDTs", "comment": "7 pages, 8 figures, submitted to the 13th Workshop on Principles and Practice of Consistency for Distributed Data", "summary": "Conflict-Free Replicated Data Types (CRDTs) are used in a range of fields for their coordination-free replication with strong eventual consistency. By prioritising availability over consistency under partition, nodes accumulate events in different orders, and rely on an associative, commutative and idempotent merge function to present a materialised view of the CRDT. Under some circumstances, the state of the materialised view over time can appear to ''roll back'' previously applied events. When the materialised view is used to manage group permissions such as ones found in instant messaging applications, this can lead to surprising behaviour. This can occur when there are multiple concurrent events, such as in the Duelling Admins problem where two equally permissioned admins concurrently revoke each other's permissions. Who wins? This article argues that a Byzantine admin can exploit concurrency to win the duel. As a result, an external arbiter is required to arbitrate an immutable happens-before relation between concurrent events. Arbitration occurs asynchronously in batches via optional ''epoch events'', preserving availability. This introduces a bounded total order within epochs, and the resulting ''finality'' improves on the level of consistency CRDTs can provide.", "AI": {"tldr": "CRDTs\u5728\u5206\u533a\u65f6\u53ef\u80fd\u51fa\u73b0\u72b6\u6001\"\u56de\u6eda\"\uff0c\u5bfc\u81f4\u6743\u9650\u7ba1\u7406\u95ee\u9898\uff0c\u5982\"\u51b3\u6597\u7ba1\u7406\u5458\"\u573a\u666f\u3002\u6587\u7ae0\u63d0\u51fa\u901a\u8fc7\u5f02\u6b65\"\u7eaa\u5143\u4e8b\u4ef6\"\u4ef2\u88c1\u5efa\u7acb\u6709\u754c\u5168\u5e8f\uff0c\u589e\u5f3a\u4e00\u81f4\u6027\u3002", "motivation": "CRDTs\u5728\u5f3a\u6700\u7ec8\u4e00\u81f4\u6027\u4e0b\uff0c\u5f53\u8282\u70b9\u4ee5\u4e0d\u540c\u987a\u5e8f\u7d2f\u79ef\u4e8b\u4ef6\u65f6\uff0c\u7269\u5316\u89c6\u56fe\u53ef\u80fd\u51fa\u73b0\"\u56de\u6eda\"\u73b0\u8c61\u3002\u5728\u6743\u9650\u7ba1\u7406\u573a\u666f\uff08\u5982\u5373\u65f6\u901a\u8baf\u5e94\u7528\uff09\u4e2d\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u610f\u5916\u884c\u4e3a\uff0c\u5982\"\u51b3\u6597\u7ba1\u7406\u5458\"\u95ee\u9898\uff1a\u4e24\u4e2a\u7ba1\u7406\u5458\u540c\u65f6\u64a4\u9500\u5bf9\u65b9\u6743\u9650\u65f6\uff0c\u8c01\u83b7\u80dc\uff1f\u62dc\u5360\u5ead\u7ba1\u7406\u5458\u53ef\u80fd\u5229\u7528\u5e76\u53d1\u6027\u8d62\u5f97\u51b3\u6597\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u5916\u90e8\u4ef2\u88c1\u5668\u4ef2\u88c1\u5e76\u53d1\u4e8b\u4ef6\u95f4\u7684\u4e0d\u53ef\u53d8happens-before\u5173\u7cfb\u3002\u4ef2\u88c1\u901a\u8fc7\u53ef\u9009\u7684\"\u7eaa\u5143\u4e8b\u4ef6\"\u5f02\u6b65\u6279\u91cf\u8fdb\u884c\uff0c\u4fdd\u6301\u53ef\u7528\u6027\u3002\u8fd9\u5f15\u5165\u4e86\u7eaa\u5143\u5185\u7684\u6709\u754c\u5168\u5e8f\uff0c\u4ea7\u751f\"\u6700\u7ec8\u6027\"\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301CRDTs\u53ef\u7528\u6027\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u7eaa\u5143\u4e8b\u4ef6\u4ef2\u88c1\u63d0\u9ad8\u4e86CRDTs\u6240\u80fd\u63d0\u4f9b\u7684\u4e00\u81f4\u6027\u6c34\u5e73\uff0c\u89e3\u51b3\u4e86\u5e76\u53d1\u4e8b\u4ef6\u5bfc\u81f4\u7684\u6743\u9650\u7ba1\u7406\u95ee\u9898\u3002", "conclusion": "CRDTs\u5728\u6743\u9650\u7ba1\u7406\u7b49\u654f\u611f\u573a\u666f\u9700\u8981\u66f4\u5f3a\u7684\u987a\u5e8f\u4fdd\u8bc1\u3002\u901a\u8fc7\u5f02\u6b65\u7eaa\u5143\u4e8b\u4ef6\u4ef2\u88c1\u5efa\u7acb\u6709\u754c\u5168\u5e8f\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u53ef\u7528\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u6700\u7ec8\u6027\uff0c\u9632\u6b62\u62dc\u5360\u5ead\u7ba1\u7406\u5458\u5229\u7528\u5e76\u53d1\u6027\u3002"}}
