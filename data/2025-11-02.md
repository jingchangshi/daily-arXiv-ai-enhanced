<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Fair intersection of seekable iterators](https://arxiv.org/abs/2510.26016)
*Michael Arntzenius*

Main category: cs.PL

TL;DR: 该论文展示了通过限制工作量的公平性概念如何同时支撑了miniKanren的搜索策略和最优连接算法的实现。


<details>
  <summary>Details</summary>
Motivation: 探索如何在函数式语言中优雅地实现公平的搜索策略和最优连接算法，借鉴miniKanren的成功经验。

Method: 采用基于工作量限制的公平性方法，通过可搜索迭代器接口在函数式语言中浅层嵌入实现最坏情况最优连接。

Result: 证明了工作量限制的公平性概念可以统一应用于逻辑编程的搜索策略和数据库连接算法中。

Conclusion: 公平性通过工作量限制的概念为逻辑编程和数据库查询处理提供了统一的实现框架。

Abstract: miniKanren's key semantic advance over Prolog is to implement a complete yet
efficient search strategy, fairly interleaving execution between disjuncts.
This fairness is accomplished by bounding how much work is done exploring one
disjunct before switching to the next. We show that the same idea -- fairness
via bounded work -- underlies an elegant compositional approach to implementing
worst-case optimal joins using a seekable iterator interface, suitable for
shallow embedding in functional languages.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference](https://arxiv.org/abs/2510.26730)
*Zixu Shen,Kexin Chu,Yifan Zhang,Dawei Xiang,Runxin Wu,Wei Zhang*

Main category: cs.DC

TL;DR: ExpertFlow是一个用于MoE推理的运行时系统，通过自适应专家预取和缓存感知路由，解决了传统MoE推理中参数频繁传输导致的延迟问题。


<details>
  <summary>Details</summary>
Motivation: 现代GPU内存容量有限限制了大型语言模型的扩展，传统MoE推理方法因每层独立选择活跃专家而引入显著延迟，且现有的跨层预测策略缺乏对不同硬件平台和工作负载的适应性。

Method: 结合自适应专家预取和缓存感知路由，利用传输带宽、参数维度和模型反馈信号等运行时统计信息持续调整预测范围，采用混合跨层预测方案融合预门控信息和中间计算状态来预测未来专家需求。

Result: 评估显示ExpertFlow将模型停顿时间减少到基线的0.1%以下，显著优化了严格内存约束下的MoE推理性能。

Conclusion: ExpertFlow通过自适应预取决策和与实际使用行为的对齐，有效减少了缓存未命中并消除了专家交换引入的延迟，为内存受限环境下的MoE推理提供了高效解决方案。

Abstract: The expansion of large language models is increasingly limited by the
constrained memory capacity of modern GPUs. To mitigate this,
Mixture-of-Experts (MoE) architectures activate only a small portion of
parameters during inference, significantly lowering both memory demand and
computational overhead. However, conventional MoE inference approaches, which
select active experts independently at each layer, often introduce considerable
latency because of frequent parameter transfers between host and GPU memory. In
addition, current cross-layer prediction strategies, which are typically based
on fixed steps, lack adaptability across different hardware platforms and
workloads, thereby reducing their robustness and effectiveness.
  To address these challenges, we present ExpertFlow, a runtime system for MoE
inference that combines adaptive expert prefetching and cache-aware routing.
ExpertFlow continuously adjusts its prediction horizon for expert activation by
leveraging runtime statistics such as transfer bandwidth, parameter
dimensionality, and model feedback signals. Furthermore, it incorporates a
hybrid cross-layer prediction scheme that fuses pregating information with
intermediate computational states to anticipate future expert needs. By
adaptively refining prefetching decisions and aligning them with actual usage
behavior, ExpertFlow effectively decreases cache misses and removes latency
caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces
model stall time to less than 0.1% of the baseline, highlighting its capability
to optimize MoE inference under stringent memory constraints.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [3] [CHIPSIM: A Co-Simulation Framework for Deep Learning on Chiplet-Based Systems](https://arxiv.org/abs/2510.25958)
*Lukas Pfromm,Alish Kanani,Harsh Sharma,Janardhan Rao Doppa,Partha Pratim Pande,Umit Y. Ogras*

Main category: cs.AR

TL;DR: CHIPSIM是一个用于小芯片系统上并行DNN执行的协同仿真框架，能准确模拟网络争用和流水线效应，并提供微秒级功耗分析。


<details>
  <summary>Details</summary>
Motivation: 传统单片芯片由于制造良率下降，无法满足数据密集型应用（如DNN模型）的计算、存储和通信需求。小芯片架构通过片上网络集成提供了经济高效的解决方案，但现有仿真方法缺乏所需的精度、速度和灵活性。

Method: 开发CHIPSIM协同仿真框架，并行模拟计算和通信，准确捕捉网络争用和流水线效应，并以微秒级粒度分析小芯片和片上网络的功耗。

Result: 通过同构/异构小芯片和不同片上网络架构的广泛评估，证明该框架具有多功能性，精度提升高达340%，并具备功耗/热分析能力。

Conclusion: CHIPSIM为小芯片系统上的DNN执行提供了快速准确的仿真解决方案，解决了现有方法的局限性，支持精确的瞬态热分析。

Abstract: Due to reduced manufacturing yields, traditional monolithic chips cannot keep
up with the compute, memory, and communication demands of data-intensive
applications, such as rapidly growing deep neural network (DNN) models.
Chiplet-based architectures offer a cost-effective and scalable solution by
integrating smaller chiplets via a network-on-interposer (NoI). Fast and
accurate simulation approaches are critical to unlocking this potential, but
existing methods lack the required accuracy, speed, and flexibility. To address
this need, this work presents CHIPSIM, a comprehensive co-simulation framework
designed for parallel DNN execution on chiplet-based systems. CHIPSIM
concurrently models computation and communication, accurately capturing network
contention and pipelining effects that conventional simulators overlook.
Furthermore, it profiles the chiplet and NoI power consumptions at microsecond
granularity for precise transient thermal analysis. Extensive evaluations with
homogeneous/heterogeneous chiplets and different NoI architectures demonstrate
the framework's versatility, up to 340% accuracy improvement, and power/thermal
analysis capability.

</details>


### [4] [MIREDO: MIP-Driven Resource-Efficient Dataflow Optimization for Computing-in-Memory Accelerator](https://arxiv.org/abs/2510.26463)
*Xiaolin He,Cenlin Duan,Yingjie Qi,Xiao Ma,Jianlei Yang*

Main category: cs.AR

TL;DR: MIREDO框架通过将存内计算架构的数据流优化建模为混合整数规划问题，显著提升了深度神经网络在CIM加速器上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 存内计算架构虽然能缓解数据移动瓶颈，但由于设计空间庞大和架构约束严格，现有优化方法难以充分发挥CIM加速器的潜力，导致理论效率与实际系统效率之间存在明显差距。

Method: 提出MIREDO框架，采用分层硬件抽象和分析延迟模型，准确反映CIM系统内复杂的数据传输行为，通过联合建模工作负载特性、数据流策略和CIM特定约束，系统性地探索设计空间以确定最优数据流配置。

Result: 评估结果显示，MIREDO显著提升了性能，在各种DNN模型和硬件设置下实现了最高3.2倍的性能改进。

Conclusion: MIREDO框架通过将数据流优化问题形式化为混合整数规划，能够有效解决CIM架构中的数据流优化挑战，显著提升系统级效率。

Abstract: Computing-in-Memory (CIM) architectures have emerged as a promising solution
for accelerating Deep Neural Networks (DNNs) by mitigating data movement
bottlenecks. However, realizing the potential of CIM requires specialized
dataflow optimizations, which are challenged by an expansive design space and
strict architectural constraints. Existing optimization approaches often fail
to fully exploit CIM accelerators, leading to noticeable gaps between
theoretical and actual system-level efficiency. To address these limitations,
we propose the MIREDO framework, which formulates dataflow optimization as a
Mixed-Integer Programming (MIP) problem. MIREDO introduces a hierarchical
hardware abstraction coupled with an analytical latency model designed to
accurately reflect the complex data transfer behaviors within CIM systems. By
jointly modeling workload characteristics, dataflow strategies, and
CIM-specific constraints, MIREDO systematically navigates the vast design space
to determine the optimal dataflow configurations. Evaluation results
demonstrate that MIREDO significantly enhances performance, achieving up to
$3.2\times$ improvement across various DNN models and hardware setups.

</details>
