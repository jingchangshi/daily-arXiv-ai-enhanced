<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 16]
- [cs.AR](#cs.AR) [Total: 8]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Checking Consistency of Event-driven Traces](https://arxiv.org/abs/2508.07855)
*Parosh Aziz Abdulla,Mohamed Faouzi Atig,R. Govind,Samuel Grahn,Ramanathan S. Thinniyam*

Main category: cs.PL

TL;DR: 本文提出了一种基于执行图的公理语义方法，用于验证事件驱动程序的执行一致性，证明了其与操作语义的等价性，并分析了该问题的计算复杂性（NP完全性），同时识别了一个可高效求解的片段（无嵌套发布）。


<details>
  <summary>Details</summary>
Motivation: 事件驱动程序的执行一致性验证是一个核心问题，现有方法缺乏形式化语义支持。

Method: 基于执行图的公理语义方法，证明其与操作语义的等价性，并分析计算复杂性。

Result: 一致性验证问题是NP完全的，但在无嵌套发布的情况下可在多项式时间内求解。

Conclusion: 提出的方法为事件驱动程序的执行一致性提供了形式化验证工具，并在实验中验证了其有效性。

Abstract: Event-driven programming is a popular paradigm where the flow of execution is
controlled by two features: (1) shared memory and (2) sending and receiving of
messages between multiple handler threads (just called handler). Each handler
has a mailbox (modelled as a queue) for receiving messages, with the constraint
that the handler processes its messages sequentially. Executions of messages by
different handlers may be interleaved. A central problem in this setting is
checking whether a candidate execution is consistent with the semantics of
event-driven programs. In this paper, we propose an axiomatic semantics for
eventdriven programs based on the standard notion of traces (also known as
execution graphs). We prove the equivalence of axiomatic and operational
semantics. This allows us to rephrase the consistency problem axiomatically,
resulting in the event-driven consistency problem: checking whether a given
trace is consistent. We analyze the computational complexity of this problem
and show that it is NP-complete, even when the number of handler threads is
bounded. We then identify a tractable fragment: in the absence of nested
posting, where handlers do not post new messages while processing a message,
consistency checking can be performed in polynomial time. Finally, we implement
our approach in a prototype tool and report on experimental results on a wide
range of benchmarks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [PiKV: KV Cache Management System for Mixture of Experts](https://arxiv.org/abs/2508.06526)
*Dong Liu,Yanxuan Yu,Ben Lengerich,Ying Nian Wu,Xuhong Wang*

Main category: cs.DC

TL;DR: PiKV是一个专为MoE架构设计的并行分布式KV缓存框架，通过专家分片存储、路由优化和调度策略减少KV缓存的开销，并集成压缩模块进一步降低内存使用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模和上下文长度的增加，KV缓存的存储和通信成本成为多GPU和多节点推理的主要瓶颈。MoE架构虽然稀疏化了计算，但KV缓存仍然密集且全局同步，导致显著开销。

Method: PiKV采用专家分片KV存储、PiKV路由减少访问、PiKV调度自适应保留相关条目，并集成压缩模块优化内存使用。

Result: PiKV已开源，实验结果显示其能有效减少KV缓存的开销，并与Nvidia kvpress集成加速。

Conclusion: PiKV旨在成为MoE架构的全面KV缓存管理系统，目前仍在持续开发中。

Abstract: As large language models continue to scale up in both size and context
length, the memory and communication cost of key-value (KV) cache storage has
become a major bottleneck in multi-GPU and multi-node inference. While
MoE-based architectures sparsify computation across experts, the corresponding
KV caches remain dense and globally synchronized, resulting in significant
overhead.
  We introduce \textbf{PiKV}, a parallel and distributed KV cache serving
framework tailored for MoE architecture. PiKV leverages \textit{expert-sharded
KV storage} to partition caches across GPUs, \textit{PiKV routing} to reduce
token-to-KV access, and a \textit{PiKV Scheduling} to adaptively retain
query-relevant entries. To further reduce memory usage, PiKV integrates
\textit{PiKV Compression} modules the caching pipeline for acceleration.
  PiKV is recently publicly available as an open-source software library:
\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.
Experiments details is recorded at:
\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\_Results}.
We also have PiKV integrated with Nvidia kvpress for acceleration, details see
\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.
PiKV is still a living project, aiming to become a comprehesive KV Cache
management system for MoE Architectures.

</details>


### [3] [Kairos: Low-latency Multi-Agent Serving with Shared LLMs and Excessive Loads in the Public Cloud](https://arxiv.org/abs/2508.06948)
*Jinyuan Chen,Jiuchen Shi,Quan Chen,Minyi Guo*

Main category: cs.DC

TL;DR: Kairos是一个多代理编排系统，通过优化调度和分发策略，显著降低多代理应用的端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 多代理应用中，共享LLM因任务多样性和高负载导致性能下降，现有方法忽视了代理间的延迟和资源差异。

Method: Kairos由工作流编排器、优先级调度器和内存感知分发器组成，分别负责工作流分析、请求优先级调度和内存需求分发。

Result: 实验表明，Kairos将端到端延迟降低了17.8%至28.4%。

Conclusion: Kairos通过优化调度和资源分配，显著提升了多代理应用的性能。

Abstract: Multi-agent applications utilize the advanced capabilities of large language
models (LLMs) for intricate task completion through agent collaboration in a
workflow. Under this situation, requests from different agents usually access
the same shared LLM to perform different kinds of tasks, forcing the shared LLM
to suffer excessive loads. However, existing works have low serving performance
for these multi-agent applications, mainly due to the ignorance of inter-agent
latency and resource differences for request scheduling. We therefore propose
Kairos, a multi-agent orchestration system that optimizes end-to-end latency
for multi-agent applications. Kairos consists of a workflow orchestrator, a
workflow-aware priority scheduler, and a memory-aware dispatcher. The
orchestrator collects agent-specific information for online workflow analysis.
The scheduler decides the serving priority of the requests based on their
latency characteristics to reduce the overall queuing. The dispatcher
dispatches the requests to different LLM instances based on their memory
demands to avoid GPU overloading. Experimental results show that Kairos reduces
end-to-end latency by 17.8% to 28.4% compared to state-of-the-art works.

</details>


### [4] [Convergence Sans Synchronization](https://arxiv.org/abs/2508.06949)
*Arya Tanmay Gupta*

Main category: cs.DC

TL;DR: 论文提出了一种理论，用于验证多处理器算法在异步环境下仍能保证收敛，无需生成全局状态转移图，显著降低了证明复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着多处理器系统的普及，开发快速并行处理算法成为研究热点，但同步机制成本高昂。异步执行能充分利用计算资源，但验证算法收敛性需检查全局状态转移图，复杂度高。

Method: 提出一种理论，通过分析计算节点的局部状态转移图是否形成偏序关系，来判断算法在异步环境下的收敛性，避免了全局状态空间的生成。

Result: 实验表明，设计的算法在收敛时间上显著优于现有算法，且在异步环境下表现相似。

Conclusion: 该理论简化了异步算法的收敛性证明，为设计高效异步算法提供了理论基础。

Abstract: We currently see a steady rise in the usage and size of multiprocessor
systems, and so the community is evermore interested in developing fast
parallel processing algorithms. However, most algorithms require a
synchronization mechanism, which is costly in terms of computational resources
and time. If an algorithm can be executed in asynchrony, then it can use all
the available computation power, and the nodes can execute without being
scheduled or locked. However, to show that an algorithm guarantees convergence
in asynchrony, we need to generate the entire global state transition graph and
check for the absence of cycles. This takes time exponential in the size of the
global state space. In this dissertation, we present a theory that explains the
necessary and sufficient properties of a multiprocessor algorithm that
guarantees convergence even without synchronization. We develop algorithms for
various problems that do not require synchronization. Additionally, we show for
several existing algorithms that they can be executed without any
synchronization mechanism. A significant theoretical benefit of our work is in
proving that an algorithm can converge even in asynchrony. Our theory implies
that we can make such conclusions about an algorithm, by only showing that the
local state transition graph of a computing node forms a partial order, rather
than generating the entire global state space and determining the absence of
cycles in it. Thus, the complexity of rendering such proofs, formal or social,
is phenomenally reduced. Experiments show a significant reduction in time taken
to converge, when we compare the execution time of algorithms in the literature
versus the algorithms that we design. We get similar results when we run an
algorithm, that guarantees convergence in asynchrony, under a scheduler versus
in asynchrony.

</details>


### [5] [The Fused Kernel Library: A C++ API to Develop Highly-Efficient GPU Libraries](https://arxiv.org/abs/2508.07071)
*Oscar Amoros,Albert Andaluz,Johnny Nunez,Antonio J. Pena*

Main category: cs.DC

TL;DR: 提出了一种自动实现GPU库函数水平融合（HF）和垂直融合（VF）的新方法，通过C++17元编程生成优化的融合内核，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有GPU库在多个GPU函数链式调用时难以充分利用并行资源和SRAM，且依赖手动开发融合内核，限制了应用场景并增加开发成本。

Method: 定义可重用、可融合的组件，用户通过高级编程接口组合，利用C++17元编程在编译时自动生成优化的融合内核。

Result: 开源实现显示性能提升2x至1000x以上，同时保持高级编程能力。

Conclusion: 该方法显著提高了GPU资源利用率和性能，同时简化了开发流程。

Abstract: Existing GPU libraries often struggle to fully exploit the parallel resources
and on-chip memory (SRAM) of GPUs when chaining multiple GPU functions as
individual kernels. While Kernel Fusion (KF) techniques like Horizontal Fusion
(HF) and Vertical Fusion (VF) can mitigate this, current library
implementations often require library developers to manually create fused
kernels. Hence, library users rely on limited sets of pre-compiled or
template-based fused kernels. This limits the use cases that can benefit from
HF and VF and increases development costs. In order to solve these issues, we
present a novel methodology for building GPU libraries that enables automatic
on-demand HF and VF for arbitrary combinations of GPU library functions. Our
methodology defines reusable, fusionable components that users combine via
high-level programming interfaces. Leveraging C++17 metaprogramming features
available in compilers like nvcc, our methodology generates a single and
optimized fused kernel tailored to the user's specific sequence of operations
at compile time, without needing a custom compiler or manual development and
pre-compilation of kernel combinations. This approach abstracts low-level GPU
complexities while maximizing GPU resource utilization and keeping intermediate
data in SRAM. We provide an open-source implementation demonstrating
significant speedups compared to traditional libraries in various benchmarks,
validating the effectiveness of this methodology for improving GPU performance
in the range of 2x to more than 1000x, while preserving high-level
programmability.

</details>


### [6] [AerialDB: A Federated Peer-to-Peer Spatio-temporal Edge Datastore for Drone Fleets](https://arxiv.org/abs/2508.07124)
*Shashwat Jaiswal,Suman Raj,Subhajit Sidhanta,Yogesh Simmhan*

Main category: cs.DC

TL;DR: AerialDB是一个轻量级去中心化数据存储与查询系统，专为无人机群设计，支持高效时空数据处理，性能显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 无人机在灾害区域等动态环境中采集大量数据，需实时卸载到边缘和云端处理，现有系统难以满足高效存储与查询需求。

Method: AerialDB采用基于内容的副本放置和分片索引技术，结合去中心化执行引擎，优化时空查询处理。

Result: 实验表明，AerialDB在400架无人机和80个边缘节点上扩展高效，查询性能提升100倍，插入性能提升10倍。

Conclusion: AerialDB为多无人机系统提供了高效的实时数据处理方案，性能显著优于现有技术。

Abstract: Recent years have seen an unprecedented growth in research that leverages the
newest computing paradigm of Internet of Drones, comprising a fleet of
connected Unmanned Aerial Vehicles (UAVs) used for a wide range of tasks such
as monitoring and analytics in highly mobile and changing environments
characteristic of disaster regions. Given that the typical data (i.e., videos
and images) collected by the fleet of UAVs deployed in such scenarios can be
considerably larger than what the onboard computers can process, the UAVs need
to offload their data in real-time to the edge and the cloud for further
processing. To that end, we present the design of AerialDB - a lightweight
decentralized data storage and query system that can store and process time
series data on a multi-UAV system comprising: A) a fleet of hundreds of UAVs
fitted with onboard computers, and B) ground-based edge servers connected
through a cellular link. Leveraging lightweight techniques for content-based
replica placement and indexing of shards, AerialDB has been optimized for
efficient processing of different possible combinations of typical spatial and
temporal queries performed by real-world disaster management applications.
Using containerized deployment spanning up to 400 drones and 80 edges, we
demonstrate that AerialDB is able to scale efficiently while providing near
real-time performance with different realistic workloads. Further, AerialDB
comprises a decentralized and locality-aware distributed execution engine which
provides graceful degradation of performance upon edge failures with relatively
low latency while processing large spatio-temporal data. AerialDB exhibits
comparable insertion performance and 100 times improvement in query performance
against state-of-the-art baseline. Moreover, it exhibits a 10 times and 100
times improvement with insertion and query workloads respectively over the
cloud baseline.

</details>


### [7] [FlashMP: Fast Discrete Transform-Based Solver for Preconditioning Maxwell's Equations on GPUs](https://arxiv.org/abs/2508.07193)
*Haoyuan Zhang,Yaqian Gao,Xinxin Zhang,Jialin Li,Runfeng Jin,Yidong Chen,Feng Zhang,Wu Yuan,Wenpeng Ma,Shan Liang,Jian Zhang,Zhonghua Lu*

Main category: cs.DC

TL;DR: FlashMP是一种新型预处理系统，通过离散变换设计子域精确求解器，显著提高了CN-FDTD方法中大规模线性系统的求解效率。


<details>
  <summary>Details</summary>
Motivation: 现有迭代求解器在处理双旋度算子导致的稀疏系统时收敛缓慢，而近似预处理方法效果不足，直接求解器内存需求过高。

Method: 提出FlashMP，基于离散变换设计子域精确求解器，并实现高效的多GPU可扩展性。

Result: 在AMD MI60 GPU集群上，FlashMP将迭代次数减少16倍，速度提升2.5至4.9倍，并行效率达84.1%。

Conclusion: FlashMP显著提升了大规模电磁模拟中线性系统的求解效率和可扩展性。

Abstract: Efficiently solving large-scale linear systems is a critical challenge in
electromagnetic simulations, particularly when using the Crank-Nicolson
Finite-Difference Time-Domain (CN-FDTD) method. Existing iterative solvers are
commonly employed to handle the resulting sparse systems but suffer from slow
convergence due to the ill-conditioned nature of the double-curl operator.
Approximate preconditioners, like Successive Over-Relaxation (SOR) and
Incomplete LU decomposition (ILU), provide insufficient convergence, while
direct solvers are impractical due to excessive memory requirements. To address
this, we propose FlashMP, a novel preconditioning system that designs a
subdomain exact solver based on discrete transforms. FlashMP provides an
efficient GPU implementation that achieves multi-GPU scalability through domain
decomposition. Evaluations on AMD MI60 GPU clusters (up to 1000 GPUs) show that
FlashMP reduces iteration counts by up to 16x and achieves speedups of 2.5x to
4.9x compared to baseline implementations in state-of-the-art libraries such as
Hypre. Weak scalability tests show parallel efficiencies up to 84.1%.

</details>


### [8] [An Experimental Exploration of In-Memory Computing for Multi-Layer Perceptrons](https://arxiv.org/abs/2508.07317)
*Pedro Carrinho,Hamid Moghadaspour,Oscar Ferraz,João Dinis Ferreira,Yann Falevoz,Vitor Silva,Gabriel Falcao*

Main category: cs.DC

TL;DR: 论文探讨了现代通用PiM架构（如UPMEM）在加速神经网络训练和推理中的潜力，结果显示其性能显著优于传统CPU，并与低功耗GPU相当。


<details>
  <summary>Details</summary>
Motivation: 现代计算机架构中，内存密集型任务（如机器学习）的性能受限于数据移动瓶颈，PiM（内存内计算）旨在通过减少数据移动来解决这一问题。

Method: 研究选择了UPMEM PiM系统，实现了多层感知器（MLP）的推理，并与Intel Xeon CPU和低功耗Nvidia Jetson GPU进行了性能对比。

Result: UPMEM PiM在大批量推理中性能比CPU快259倍，使用WRAM的MLP推理时间低于3ms，与低功耗GPU相当。

Conclusion: PiM架构在神经网络推理中具有显著性能优势，尤其在减少数据移动方面表现出色，为未来计算范式提供了新方向。

Abstract: In modern computer architectures, the performance of many memory-bound
workloads (e.g., machine learning, graph processing, databases) is limited by
the data movement bottleneck that emerges when transferring large amounts of
data between the main memory and the central processing unit (CPU).
Processing-in-memory is an emerging computing paradigm that aims to alleviate
this data movement bottleneck by performing computation close to or within the
memory units, where data resides. One example of a prevalent workload whose
performance is bound by the data movement bottleneck is the training and
inference process of artificial neural networks. In this work, we analyze the
potential of modern general-purpose PiM architectures to accelerate neural
networks. To this end, we selected the UPMEM PiM system, the first commercially
available real-world general-purpose PiM architecture. We compared the
implementation of multilayer perceptrons (MLPs) in PiM with a sequential
baseline running on an Intel Xeon CPU. The UPMEM implementation achieves up to
$259\times$ better performance for inference of large batch sizes when compared
against the CPU that exploits the size of the available PiM memory.
Additionally, two smaller MLPs were implemented using UPMEM's working SRAM
(WRAM), a scratchpad memory, to evaluate their performance against a low-power
Nvidia Jetson graphics processing unit (GPU), providing further insights into
the efficiency of UPMEM's PiM for neural network inference. Results show that
using WRAM achieves kernel execution times for MLP inference of under $3$ ms,
which is within the same order of magnitude as low-power GPUs.

</details>


### [9] [On the Efficiency of Dynamic Transaction Scheduling in Blockchain Sharding](https://arxiv.org/abs/2508.07472)
*Ramesh Adhikari,Costas Busch,Miroslav Popovic*

Main category: cs.DC

TL;DR: 论文研究了区块链分片系统中的动态调度问题，提出了在无状态和有状态模型下的竞争比，并证明了近似最优调度的NP难度。


<details>
  <summary>Details</summary>
Motivation: 区块链分片技术通过并行处理交易提升性能，但动态调度问题尚未被充分研究。本文旨在解决这一问题。

Method: 在分片图$G_s$上，通过领导者分片调度交易，分别分析无状态和有状态模型的竞争比。

Result: 无状态模型竞争比为$O(d \log^2 s \cdot \min\{k, \sqrt{s}\})$，有状态模型为$O(\log s\cdot \min\{k, \sqrt{s}\}+\log^2 s)$。

Conclusion: 本文首次为区块链分片系统提供了可证明高效的动态调度算法，竞争比接近最优。

Abstract: Sharding is a technique to speed up transaction processing in blockchains,
where the $n$ processing nodes in the blockchain are divided into $s$ disjoint
groups (shards) that can process transactions in parallel. We study dynamic
scheduling problems on a shard graph $G_s$ where transactions arrive online
over time and are not known in advance. Each transaction may access at most $k$
shards, and we denote by $d$ the worst distance between a transaction and its
accessing (destination) shards (the parameter $d$ is unknown to the shards). To
handle different values of $d$, we assume a locality sensitive decomposition of
$G_s$ into clusters of shards, where every cluster has a leader shard that
schedules transactions for the cluster. We first examine the simpler case of
the stateless model, where leaders are not aware of the current state of the
transaction accounts, and we prove a $O(d \log^2 s \cdot \min\{k, \sqrt{s}\})$
competitive ratio for latency. We then consider the stateful model, where
leader shards gather the current state of accounts, and we prove a $O(\log
s\cdot \min\{k, \sqrt{s}\}+\log^2 s)$ competitive ratio for latency. Each
leader calculates the schedule in polynomial time for each transaction that it
processes. We show that for any $\epsilon > 0$, approximating the optimal
schedule within a $(\min\{k, \sqrt{s}\})^{1 -\epsilon}$ factor is NP-hard.
Hence, our bound for the stateful model is within a poly-log factor from the
best possibly achievable. To the best of our knowledge, this is the first work
to establish provably efficient dynamic scheduling algorithms for blockchain
sharding systems.

</details>


### [10] [Coordinated Power Management on Heterogeneous Systems](https://arxiv.org/abs/2508.07605)
*Zhong Zheng,Michael E. Papka,Zhiling Lan*

Main category: cs.DC

TL;DR: OPEN是一个轻量级性能预测框架，通过离线构建性能预测器和在线轻量级分析，结合协同过滤技术，显著减少分析成本并保持高精度（最高98.29%）。


<details>
  <summary>Details</summary>
Motivation: 异构计算系统中传统性能建模方法依赖高成本的离线分析，OPEN旨在解决这一问题。

Method: OPEN分为离线和在线阶段：离线阶段构建性能预测器和初始矩阵，在线阶段进行轻量级分析并利用协同过滤进行预测。

Result: 在配备A100和A30 GPU的异构系统上，OPEN的预测准确率高达98.29%。

Conclusion: OPEN为现代高性能计算环境提供了一种轻量级、高效的性能预测解决方案，适用于功耗感知的计算环境。

Abstract: Performance prediction is essential for energy-efficient computing in
heterogeneous computing systems that integrate CPUs and GPUs. However,
traditional performance modeling methods often rely on exhaustive offline
profiling, which becomes impractical due to the large setting space and the
high cost of profiling large-scale applications. In this paper, we present
OPEN, a framework consists of offline and online phases. The offline phase
involves building a performance predictor and constructing an initial dense
matrix. In the online phase, OPEN performs lightweight online profiling, and
leverages the performance predictor with collaborative filtering to make
performance prediction. We evaluate OPEN on multiple heterogeneous systems,
including those equipped with A100 and A30 GPUs. Results show that OPEN
achieves prediction accuracy up to 98.29\%. This demonstrates that OPEN
effectively reduces profiling cost while maintaining high accuracy, making it
practical for power-aware performance modeling in modern HPC environments.
Overall, OPEN provides a lightweight solution for performance prediction under
power constraints, enabling better runtime decisions in power-aware computing
environments.

</details>


### [11] [Taming Cold Starts: Proactive Serverless Scheduling with Model Predictive Control](https://arxiv.org/abs/2508.07640)
*Chanh Nguyen,Monowar Bhuyan,Erik Elmroth*

Main category: cs.DC

TL;DR: 论文提出了一种基于模型预测控制的预测性无服务器调度框架，旨在通过预测未来调用主动缓解冷启动问题，从而提升响应速度并减少资源开销。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算因其细粒度、事件驱动的执行模式受到青睐，但冷启动问题仍是其关键瓶颈，影响延迟敏感和突发工作负载的性能。

Method: 采用模型预测控制技术，联合优化容器预热和请求调度，预测未来调用以减少冷启动延迟。

Result: 在Apache OpenWhisk和Kubernetes测试平台上，实验显示该方法显著优于现有基线，尾部延迟降低85%，资源使用减少34%。

Conclusion: 该框架有效解决了冷启动问题，提升了无服务器平台的性能和资源效率。

Abstract: Serverless computing has transformed cloud application deployment by
introducing a fine-grained, event-driven execution model that abstracts away
infrastructure management. Its on-demand nature makes it especially appealing
for latency-sensitive and bursty workloads. However, the cold start problem,
i.e., where the platform incurs significant delay when provisioning new
containers, remains the Achilles' heel of such platforms.
  This paper presents a predictive serverless scheduling framework based on
Model Predictive Control to proactively mitigate cold starts, thereby improving
end-to-end response time. By forecasting future invocations, the controller
jointly optimizes container prewarming and request dispatching, improving
latency while minimizing resource overhead.
  We implement our approach on Apache OpenWhisk, deployed on a Kubernetes-based
testbed. Experimental results using real-world function traces and synthetic
workloads demonstrate that our method significantly outperforms
state-of-the-art baselines, achieving up to 85% lower tail latency and a 34%
reduction in resource usage.

</details>


### [12] [Perpetual exploration in anonymous synchronous networks with a Byzantine black hole](https://arxiv.org/abs/2508.07703)
*Adri Bhattacharya,Pritam Goswami,Evangelos Bampas,Partha Sarathi Mandal*

Main category: cs.DC

TL;DR: 研究移动代理在未知图中如何持续探索，当存在一个可能恶意的静止节点（Byzantine black hole, BBH）时，确定所需的最小代理数量。


<details>
  <summary>Details</summary>
Motivation: 探索在恶意节点存在下的图探索问题，填补了无初始拓扑知识下黑洞变体研究的空白。

Method: 定义了两种探索变体（PerpExpl和PerpExplHome），在同步调度和面对面通信模型下，设计算法并分析所需代理数量。

Result: 在树状网络中，PerpExpl需4个代理，PerpExplHome需6个；一般图中，PerpExpl下界为2Δ-1，PerpExplHome上界为3Δ+3。

Conclusion: 首次在无初始拓扑知识的任意网络中研究黑洞变体，提供了理论和算法支持。

Abstract: In this paper, we investigate: ``How can a group of initially co-located
mobile agents perpetually explore an unknown graph, when one stationary node
occasionally behaves maliciously, under an adversary's control?'' We call this
node a ``Byzantine black hole (BBH)'' and at any given round it may choose to
destroy all visiting agents, or none. This subtle power can drastically
undermine classical exploration strategies designed for an always active black
hole. We study this perpetual exploration problem in the presence of at most
one BBH, without initial knowledge of the network size. Since the underlying
graph may be 1-connected, perpetual exploration of the entire graph may be
infeasible. We thus define two variants: \pbmPerpExpl\ and \pbmPerpExplHome. In
the former, the agents are tasked to perform perpetual exploration of at least
one component, obtained after the exclusion of the BBH. In the latter, the
agents are tasked to perform perpetual exploration of the component which
contains the \emph{home} node, where agents are initially co-located.
Naturally, \pbmPerpExplHome\ is a special case of \pbmPerpExpl. Agents operate
under a synchronous scheduler and communicate in a face-to-face model. Our goal
is to determine the minimum number of agents necessary and sufficient to solve
these problems. In acyclic networks, we obtain optimal algorithms that solve
\pbmPerpExpl\ with $4$ agents, and \pbmPerpExplHome\ with $6$ agents in trees.
The lower bounds hold even in path graphs. In general graphs, we give a
non-trivial lower bound of $2\Delta-1$ agents for \pbmPerpExpl, and an upper
bound of $3\Delta+3$ agents for \pbmPerpExplHome. To our knowledge, this is the
first study of a black-hole variant in arbitrary networks without initial
topological knowledge.

</details>


### [13] [Over-the-Top Resource Broker System for Split Computing: An Approach to Distribute Cloud Computing Infrastructure](https://arxiv.org/abs/2508.07744)
*Ingo Friese,Jochen Klaffer,Mandy Galkow-Schneider,Sergiy Melnyk,Qiuheng Zhou,Hans Dieter Schotten*

Main category: cs.DC

TL;DR: 6G网络架构将引入创新服务和能力，如拆分计算和动态处理节点，通过统一接口简化资源访问。运营商身份不再重要，多提供商共享资源池。性能参数需根据位置和服务需求定制，抽象层简化部署。本文探讨资源分配代理在拆分计算场景中的作用，并通过概念验证展示其架构框架。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要支持多样化服务和动态资源管理，同时简化复杂基础设施的集成，以促进多提供商协作。

Method: 引入资源分配代理，抽象不同基础设施的复杂性，支持云环境和网络。通过概念验证实现展示其架构。

Result: 代理在拆分计算场景中表现出多功能性，适用于云和网络环境。概念验证验证了其实际可行性。

Conclusion: 资源分配代理是简化6G网络资源管理和服务部署的有效解决方案，具有广泛适用性。

Abstract: 6G network architectures will usher in a wave of innovative services and
capabilities, introducing concepts like split computing and dynamic processing
nodes. This implicates a paradigm where accessing resources seamlessly aligns
with diverse processing node characteristics, ensuring a uniform interface. In
this landscape, the identity of the operator becomes inconsequential, paving
the way for a collaborative ecosystem where multiple providers contribute to a
shared pool of resources. At the core of this vision is the guarantee of
specific performance parameters, precisely tailored to the location and service
requirements. A consistent layer, as the abstraction of the complexities of
different infrastructure providers, is needed to simplify service deployment.
One promising approach is the introduction of an over-the-top broker for
resource allocation, which streamlines the integration of these services into
the network and cloud infrastructure of the future. This paper explores the
role of the broker in two split computing scenarios. By abstracting the
complexities of various infrastructures, the broker proves to be a versatile
solution applicable not only to cloud environments but also to networks and
beyond. Additionally, a detailed discussion of a proof-of-concept
implementation provides insights into the broker's actual architectural
framework.

</details>


### [14] [Towards Lock Modularization for Heterogeneous Environments](https://arxiv.org/abs/2508.07756)
*Hanze Zhang,Rong Chen,Haibo Chen*

Main category: cs.DC

TL;DR: 提出了一种在异构硬件环境中分解锁的设计原则，通过锁模块化充分利用资源。


<details>
  <summary>Details</summary>
Motivation: 现代硬件环境日益异构，现有锁解决方案通常针对特定硬件，未能充分利用异构环境的资源。

Method: 提出锁模块化方法，将锁分解为独立模块并分配到合适的硬件组件。

Result: 该方法能更好地匹配锁模块的资源需求与硬件特性，提升性能。

Conclusion: 锁模块化是异构环境中高效利用资源的有效方法。

Abstract: Modern hardware environments are becoming increasingly heterogeneous, leading
to the emergence of applications specifically designed to exploit this
heterogeneity. Efficiently adopting locks in these applications poses distinct
challenges. The uneven distribution of resources in such environments can
create bottlenecks for lock operations, severely hindering application
performance. Existing solutions are often tailored to specific types of
hardware, which underutilizes resources on other components within
heterogeneous environments.
  This paper introduces a new design principle: decomposing locks across
hardware components to fully utilize unevenly distributed resources in
heterogeneous environments. Following this principle, we propose lock
modularization, a systematic approach that decomposes a lock into independent
modules and assigns them to appropriate hardware components. This approach
aligns the resource requirements of lock modules with the attributes of
specific hardware components, maximizing strengths while minimizing weaknesses.

</details>


### [15] [Performance Evaluation of Brokerless Messaging Libraries](https://arxiv.org/abs/2508.07934)
*Lorenzo La Corte,Syed Aftab Rashid,Andrei-Marian Dan*

Main category: cs.DC

TL;DR: 本文专注于无代理消息系统，通过定性分析和开源基准测试套件评估了ZeroMQ、NanoMsg和NNG的性能，为实践者提供选择依据。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注代理消息系统，而无代理系统的性能评估稀缺，本文填补这一空白。

Method: 通过定性分析筛选候选库，设计并实现开源基准测试套件，系统评估ZeroMQ、NanoMsg和NNG的性能。

Result: 评估了不同指标和工作负载条件下的性能，揭示了各库的局限性。

Conclusion: 为实践者提供了选择合适无代理消息系统库的依据。

Abstract: Messaging systems are essential for efficiently transferring large volumes of
data, ensuring rapid response times and high-throughput communication. The
state-of-the-art on messaging systems mainly focuses on the performance
evaluation of brokered messaging systems, which use an intermediate broker to
guarantee reliability and quality of service. However, over the past decade,
brokerless messaging systems have emerged, eliminating the single point of
failure and trading off reliability guarantees for higher performance. Still,
the state-of-the-art on evaluating the performance of brokerless systems is
scarce. In this work, we solely focus on brokerless messaging systems. First,
we perform a qualitative analysis of several possible candidates, to find the
most promising ones. We then design and implement an extensive open-source
benchmarking suite to systematically and fairly evaluate the performance of the
chosen libraries, namely, ZeroMQ, NanoMsg, and NanoMsg-Next-Generation (NNG).
We evaluate these libraries considering different metrics and workload
conditions, and provide useful insights into their limitations. Our analysis
enables practitioners to select the most suitable library for their
requirements.

</details>


### [16] [Optimizing Federated Learning for Scalable Power-demand Forecasting in Microgrids](https://arxiv.org/abs/2508.08022)
*Roopkatha Banerjee,Sampath Koti,Gyanendra Singh,Anirban Chakraborty,Gurunath Gurrala,Bhushan Jagyasi,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 论文提出了一种基于联邦学习（FL）的实时电力消耗监测方法，通过优化FL训练解决非独立同分布数据和计算成本问题，展示了加权损失函数的优势，并在大规模客户数据上验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 通过物联网（IoT）实时监测城市和微电网的电力消耗有助于预测需求和优化电网运营，但将数据上传至云端可能暴露隐私。联邦学习（FL）能保护隐私，但面临非独立同分布数据和计算成本高的挑战。

Method: 开发并评估了针对时间序列需求预测的FL优化方法，使用深度神经网络（DNN），引入加权损失函数，并在边缘设备和云端进行分布式训练。

Result: 在OpenEIA数据集上验证了方法，覆盖美国三个州的1000多个客户，结果显示其优于ARIMA和单客户DNN基线方法。

Conclusion: 提出的FL优化方法在保护隐私的同时提高了预测准确性，并降低了计算成本，适用于大规模电力消耗监测。

Abstract: Real-time monitoring of power consumption in cities and micro-grids through
the Internet of Things (IoT) can help forecast future demand and optimize grid
operations. But moving all consumer-level usage data to the cloud for
predictions and analysis at fine time scales can expose activity patterns.
Federated Learning~(FL) is a privacy-sensitive collaborative DNN training
approach that retains data on edge devices, trains the models on private data
locally, and aggregates the local models in the cloud. But key challenges
exist: (i) clients can have non-independently identically distributed~(non-IID)
data, and (ii) the learning should be computationally cheap while scaling to
1000s of (unseen) clients. In this paper, we develop and evaluate several
optimizations to FL training across edge and cloud for time-series demand
forecasting in micro-grids and city-scale utilities using DNNs to achieve a
high prediction accuracy while minimizing the training cost. We showcase the
benefit of using exponentially weighted loss while training and show that it
further improves the prediction of the final model. Finally, we evaluate these
strategies by validating over 1000s of clients for three states in the US from
the OpenEIA corpus, and performing FL both in a pseudo-distributed setting and
a Pi edge cluster. The results highlight the benefits of the proposed methods
over baselines like ARIMA and DNNs trained for individual consumers, which are
not scalable.

</details>


### [17] [On the Operational Resilience of CBDC: Threats and Prospects of Formal Validation for Offline Payments](https://arxiv.org/abs/2508.08064)
*Marco Bernardo,Federico Calandra,Andrea Esposito,Francesco Fabris*

Main category: cs.DC

TL;DR: 论文探讨了在金融技术（如CBDCs）中软件质量认证的重要性，尽管计算机科学理论表明完全认证是不可能的，但仍需使用形式化方法确保系统正确性。


<details>
  <summary>Details</summary>
Motivation: 随着数字金融技术的发展（如CBDCs），即使是小错误也可能引发金融崩溃，因此需要确保软件基础设施的操作弹性。

Method: 建议使用形式化方法来验证CBDCs软件基础设施的正确性，特别是离线支付这一关键问题。

Result: 形式化方法虽不能完全克服理论上的不可能性，但能提供系统正确性的断言。

Conclusion: 在CBDCs等关键金融技术中，形式化方法的应用对确保系统稳健性至关重要。

Abstract: Information and communication technologies are by now employed in most
activities, including economics and finance. Despite the extraordinary power of
modern computers and the vast amount of memory, some results of theoretical
computer science imply the impossibility of certifying software quality in
general. With the exception of safety-critical systems, this has primarily
concerned the information processed by confined systems, with limited
socio-economic consequences. In the emerging era of technologies for exchanging
digital money and tokenized assets over the Internet - such as central bank
digital currencies (CBDCs) - even a minor bug could trigger a financial
collapse. Although the aforementioned impossibility results cannot be overcome
in an absolute sense, there exist formal methods that can provide assertions of
computing systems correctness. We advocate their use to validate the
operational resilience of software infrastructures enabling CBDCs, with special
emphasis on offline payments as they constitute a very critical issue.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency](https://arxiv.org/abs/2508.06978)
*Kwanhee Kyung,Sungmin Yun,Jung Ho Ahn*

Main category: cs.AR

TL;DR: 论文分析了将MoE专家权重从DRAM卸载到SSD对LLM推理解码阶段能源消耗的影响，发现SSD显著增加能耗，未来需Flash读取能源大幅改进才能使其可行。


<details>
  <summary>Details</summary>
Motivation: 研究MoE模型权重从DRAM卸载到SSD的能源影响，因SSD容量大但能耗高。

Method: 定量分析SSD、CPU内存和HBM存储场景下MoE权重卸载的能源消耗，以DeepSeek-R1等模型为例。

Result: SSD卸载MoE权重使每令牌生成能耗增加高达12倍，成为推理总能耗主导因素。

Conclusion: 未来需Flash读取能源大幅改进（约一个数量级）才能使SSD在MoE模型中能源可行。

Abstract: Large Language Models (LLMs) applying Mixture-of-Experts (MoE) scale to
trillions of parameters but require vast memory, motivating a line of research
to offload expert weights from fast-but-small DRAM (HBM) to denser Flash SSDs.
While SSDs provide cost-effective capacity, their read energy per bit is
substantially higher than that of DRAM. This paper quantitatively analyzes the
energy implications of offloading MoE expert weights to SSDs during the
critical decode stage of LLM inference. Our analysis, comparing SSD, CPU memory
(DDR), and HBM storage scenarios for models like DeepSeek-R1, reveals that
offloading MoE weights to current SSDs drastically increases
per-token-generation energy consumption (e.g., by up to ~12x compared to the
HBM baseline), dominating the total inference energy budget. Although
techniques like prefetching effectively hide access latency, they cannot
mitigate this fundamental energy penalty. We further explore future
technological scaling, finding that the inherent sparsity of MoE models could
potentially make SSDs energy-viable if Flash read energy improves
significantly, roughly by an order of magnitude.

</details>


### [19] [Physical Design Exploration of a Wire-Friendly Domain-Specific Processor for Angstrom-Era Nodes](https://arxiv.org/abs/2508.07110)
*Lorenzo Ruotolo,Lara Orlandic,Pengbo Yu,Moritz Brunion,Daniele Jahier Pagliari,Dwaipayan Biswas,Giovanni Ansaloni,David Atienza,Julien Ryckaert,Francky Catthoor,Yukai Chen*

Main category: cs.AR

TL;DR: 本文探索了一种针对机器学习的领域特定处理器（DSIP）架构的物理设计，解决了先进埃米时代技术中互连效率的挑战。通过使用专用内存结构和SIMD单元，设计实现了更短的线长和更高的核心密度。


<details>
  <summary>Details</summary>
Motivation: 解决先进技术节点中互连效率的挑战，提升DSIP架构的物理设计效率。

Method: 利用专用内存结构和SIMD单元，设计了五种配置，并使用IMEC A10纳米片节点PDK进行合成和评估。

Result: 与现有最佳DSIP基线（VWR2A）相比，该架构实现了2倍以上的线长降低和3倍以上的密度提升，且各配置间指标变异性低。

Conclusion: 该架构具有内在物理效率和低成本实现的潜力，是下一代DSIP设计的有前景的解决方案。

Abstract: This paper presents the physical design exploration of a domain-specific
processor (DSIP) architecture targeted at machine learning (ML), addressing the
challenges of interconnect efficiency in advanced Angstrom-era technologies.
The design emphasizes reduced wire length and high core density by utilizing
specialized memory structures and SIMD (Single Instruction, Multiple Data)
units. Five configurations are synthesized and evaluated using the IMEC A10
nanosheet node PDK. Key physical design metrics are compared across
configurations and against VWR2A, a state-of-the-art (SoA) DSIP baseline.
Results show that our architecture achieves over 2x lower normalized wire
length and more than 3x higher density than the SoA, with low variability in
the metrics across all configurations, making it a promising solution for
next-generation DSIP designs. These improvements are achieved with minimal
manual layout intervention, demonstrating the architecture's intrinsic physical
efficiency and potential for low-cost wire-friendly implementation.

</details>


### [20] [LP-Spec: Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization](https://arxiv.org/abs/2508.07227)
*Siyuan He,Zhantong Zhu,Yandong He,Tianyu Jia*

Main category: cs.AR

TL;DR: LP-Spec是一种架构与数据流协同设计，通过结合LPDDR5 PIM架构、草稿令牌剪枝和动态工作负载调度，显著提升了移动设备上LLM推测推理的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 移动设备上LLM推理面临内存带宽和计算资源限制的问题，现有技术如推测推理和PIM在算法和硬件层面存在新的设计权衡和冗余令牌问题。

Method: LP-Spec采用混合LPDDR5 PIM架构，提出近数据内存控制器和基于硬件感知的草稿令牌剪枝器，优化令牌管理和并行执行。

Result: 相比其他移动解决方案，LP-Spec在性能、能效和能量延迟积（EDP）上分别提升13.21倍、7.56倍和99.87倍。

Conclusion: LP-Spec通过协同设计显著优化了移动设备上LLM推测推理的效率和能耗，为未来研究提供了新方向。

Abstract: LLM inference on mobile devices faces extraneous challenges due to limited
memory bandwidth and computational resources. To address these issues,
speculative inference and processing-in-memory (PIM) techniques have been
explored at the algorithmic and hardware levels. However, speculative inference
results in more compute-intensive GEMM operations, creating new design
trade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there
exists a significant amount of redundant draft tokens in tree-based speculative
inference, necessitating efficient token management schemes to minimize energy
consumption. In this work, we present LP-Spec, an architecture-dataflow
co-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with
draft token pruning and dynamic workload scheduling to accelerate LLM
speculative inference. A near-data memory controller is proposed to enable data
reallocation between DRAM and PIM banks. Furthermore, a data allocation unit
based on the hardware-aware draft token pruner is developed to minimize energy
consumption and fully exploit parallel execution opportunities. Compared to
end-to-end LLM inference on other mobile solutions such as mobile NPUs or
GEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x
improvements in performance, energy efficiency, and energy-delay-product (EDP).
Compared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and
415.31x EDP reduction benefits.

</details>


### [21] [Tasa: Thermal-aware 3D-Stacked Architecture Design with Bandwidth Sharing for LLM Inference](https://arxiv.org/abs/2508.07252)
*Siyuan He,Peiran Yan,Yandong He,Youwei Zhuo,Tianyu Jia*

Main category: cs.AR

TL;DR: 论文提出Tasa架构，通过异构设计和热优化解决3D堆叠架构的散热问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 3D堆叠架构虽能提升内存带宽，但面临严重的散热问题，限制了其潜力。

Method: 提出Tasa异构架构，结合高性能和高效率核心，并采用带宽共享调度优化。

Result: 实验显示Tasa显著降低峰值温度，并在Llama-65B和GPT-3 66B推理中实现2.85x和2.21x加速。

Conclusion: Tasa架构有效平衡散热与性能，优于传统3D堆叠和GPU基线。

Abstract: The autoregressive decoding in LLMs is the major inference bottleneck due to
the memory-intensive operations and limited hardware bandwidth. 3D-stacked
architecture is a promising solution with significantly improved memory
bandwidth, which vertically stacked multi DRAM dies on top of logic die.
However, our experiments also show the 3D-stacked architecture faces severer
thermal issues compared to 2D architecture, in terms of thermal temperature,
gradient and scalability. To better exploit the potential of 3D-stacked
architecture, we present Tasa, a heterogeneous architecture with cross-stack
thermal optimizations to balance the temperature distribution and maximize the
performance under the thermal constraints. High-performance core is designed
for compute-intensive operations, while high-efficiency core is used for
memory-intensive operators, e.g. attention layers. Furthermore, we propose a
bandwidth sharing scheduling to improve the bandwidth utilization in such
heterogeneous architecture. Extensive thermal experiments show that our Tasa
architecture demonstrates greater scalability compared with the homogeneous
3D-stacked architecture, i.e. up to 5.55 $\tccentigrade$, 9.37 $\tccentigrade$,
and 7.91 $\tccentigrade$ peak temperature reduction for 48, 60, and 72 core
configurations. Our experimental for Llama-65B and GPT-3 66B inferences also
demonstrate 2.85x and 2.21x speedup are obtained over the GPU baselines and
state-of-the-art heterogeneous PIM-based LLM accelerator

</details>


### [22] [The Monte Carlo Method and New Device and Architectural Techniques for Accelerating It](https://arxiv.org/abs/2508.07457)
*Janith Petangoda,Chatura Samarakoon,James Meech,Divya Thekke Kanapram,Hamid Toshani,Nathaniel Tye,Vasileios Tsoutsouras,Phillip Stanley-Marbell*

Main category: cs.AR

TL;DR: 论文介绍了蒙特卡洛方法的框架，并提出了两种基于物理的非均匀随机变量生成器（PPRVGs）的改进，同时展示了无需蒙特卡洛方法的新型架构技术。


<details>
  <summary>Details</summary>
Motivation: 处理现实世界中的不确定数据需要安全可靠的计算方法，蒙特卡洛方法虽流行但存在局限性。

Method: 提出PPRVGs改进蒙特卡洛采样，并利用分布微架构状态直接计算概率分布。

Result: 新型架构技术避免了蒙特卡洛方法的收敛问题，实现了收敛无关的计算。

Conclusion: 通过PPRVGs和新型架构技术，可以更高效地处理不确定数据，克服传统蒙特卡洛方法的限制。

Abstract: Computing systems interacting with real-world processes must safely and
reliably process uncertain data. The Monte Carlo method is a popular approach
for computing with such uncertain values. This article introduces a framework
for describing the Monte Carlo method and highlights two advances in the domain
of physics-based non-uniform random variate generators (PPRVGs) to overcome
common limitations of traditional Monte Carlo sampling. This article also
highlights recent advances in architectural techniques that eliminate the need
to use the Monte Carlo method by leveraging distributional microarchitectural
state to natively compute on probability distributions. Unlike Monte Carlo
methods, uncertainty-tracking processor architectures can be said to be
convergence-oblivious.

</details>


### [23] [A Matrix Decomposition Method for Odd-Type Gaussian Normal Basis Multiplication](https://arxiv.org/abs/2508.07541)
*Kittiphon Phalakarn,Athasit Surarerks*

Main category: cs.AR

TL;DR: 提出了一种降低二进制域GF(2^k)上奇型高斯正规基乘法器空间复杂度的方法，通过矩阵分解技术减少XOR门数量，但略微增加关键路径延迟。


<details>
  <summary>Details</summary>
Motivation: 大多数空间复杂度降低技术仅适用于最优正规基或偶型高斯正规基，而187个二进制域GF(2^k)使用奇型高斯正规基，因此需要针对此类基的优化方法。

Method: 借鉴最优正规基的矩阵分解方法，提出适用于奇型高斯正规基的空间复杂度降低技术。

Result: 与之前工作相比，该方法减少了XOR门的使用数量，但关键路径延迟略有增加。

Conclusion: 该方法为奇型高斯正规基乘法器提供了一种有效的空间复杂度优化方案，适用于特定二进制域。

Abstract: Normal basis is used in many applications because of the efficiency of the
implementation. However, most space complexity reduction techniques for binary
field multiplier are applicable for only optimal normal basis or Gaussian
normal basis of even type. There are 187 binary fields GF(2^k) for k from 2 to
1,000 that use odd-type Gaussian normal basis. This paper presents a method to
reduce the space complexity of odd-type Gaussian normal basis multipliers over
binary field GF(2^k). The idea is adapted from the matrix decomposition method
for optimal normal basis. The result shows that our space complexity reduction
method can reduce the number of XOR gates used in the implementation comparing
to previous works with a small trade-off in critical path delay.

</details>


### [24] [ARISE: Automating RISC-V Instruction Set Extension](https://arxiv.org/abs/2508.07725)
*Andreas Hager-Clukas,Philipp van Kempen,Stefan Wallentowitz*

Main category: cs.AR

TL;DR: ARISE工具通过自动化生成RISC-V指令，基于可扩展的度量标准优化代码大小和指令数，静态和动态性能均有提升。


<details>
  <summary>Details</summary>
Motivation: RISC-V的可扩展性使其在嵌入式系统中流行，但手动优化工作量大，ARISE旨在填补软件与ISA之间的鸿沟。

Method: ARISE利用CoreDSL语言生成指令集扩展，通过可扩展的度量标准选择汇编模式，优化代码大小和指令数。

Result: 在Embench-Iot测试中，ARISE静态代码大小减少1.48%，动态代码大小减少3.84%，指令执行数平均减少7.39%。

Conclusion: ARISE有效自动化了RISC-V指令优化，显著提升了性能，适用于高级工具集成。

Abstract: RISC-V is an extendable Instruction Set Architecture, growing in popularity
for embedded systems. However, optimizing it to specific requirements, imposes
a great deal of manual effort. To bridge the gap between software and ISA, the
tool ARISE is presented. It automates the generation of RISC-V instructions
based on assembly patterns, which are selected by an extendable set of metrics.
These metrics implement the optimization goals of code size and instruction
count reduction, both statically and dynamically. The instruction set
extensions are generated using the ISA description language CoreDSL. Allowing
seamless embedding in advanced tools such as the retargeting compiler Seal5 or
the instruction set simulator ETISS. ARISE improves the static code size by
1.48% and the dynamic code size by 3.84%, as well as the number of instructions
to be executed by 7.39% on average for Embench-Iot.

</details>


### [25] [TLV-HGNN: Thinking Like a Vertex for Memory-efficient HGNN Inference](https://arxiv.org/abs/2508.07796)
*Dengke Han,Duo Wang,Mingyu Yan,Xiaochun Ye,Dongrui Fan*

Main category: cs.AR

TL;DR: 论文提出了一种新的执行范式TVL-HGNN，通过消除中间存储和冗余访问，显著提升了异构图神经网络（HGNN）的推理性能。


<details>
  <summary>Details</summary>
Motivation: HGNN在推理阶段的邻居聚合阶段存在内存效率低下的问题，包括中间存储扩展和冗余内存访问，限制了其可扩展性和性能。

Method: 提出基于顶点视角的语义完整执行范式，设计TVL-HGNN硬件加速器，并引入基于邻居重叠的顶点分组技术。

Result: TVL-HGNN在性能上比NVIDIA A100 GPU和HiHGNN加速器分别提升了7.85倍和1.41倍，能耗降低了98.79%和32.61%。

Conclusion: TVL-HGNN通过优化内存访问和执行范式，显著提升了HGNN的推理效率和能效。

Abstract: Heterogeneous graph neural networks (HGNNs) excel at processing heterogeneous
graph data and are widely applied in critical domains. In HGNN inference, the
neighbor aggregation stage is the primary performance determinant, yet it
suffers from two major sources of memory inefficiency. First, the commonly
adopted per-semantic execution paradigm stores intermediate aggregation results
for each semantic prior to semantic fusion, causing substantial memory
expansion. Second, the aggregation process incurs extensive redundant memory
accesses, including repeated loading of target vertex features across semantics
and repeated accesses to shared neighbors due to cross-semantic neighborhood
overlap. These inefficiencies severely limit scalability and reduce HGNN
inference performance.
  In this work, we first propose a semantics-complete execution paradigm from a
vertex perspective that eliminates per-semantic intermediate storage and
redundant target vertex accesses. Building on this paradigm, we design
TVL-HGNN, a reconfigurable hardware accelerator optimized for efficient
aggregation. In addition, we introduce a vertex grouping technique based on
cross-semantic neighborhood overlap, with hardware implementation, to reduce
redundant accesses to shared neighbors. Experimental results demonstrate that
TVL-HGNN achieves average speedups of 7.85x and 1.41x over the NVIDIA A100 GPU
and the state-of-the-art HGNN accelerator HiHGNN, respectively, while reducing
energy consumption by 98.79% and 32.61%.

</details>
