{"id": "2507.19667", "categories": ["cs.DC", "cs.NI", "cs.PF", "68M20, 68M01", "C.4; C.5.5"], "pdf": "https://arxiv.org/pdf/2507.19667", "abs": "https://arxiv.org/abs/2507.19667", "authors": ["Niklas Carlsson", "Derek Eager"], "title": "Quantifying the Performance Gap for Simple Versus Optimal Dynamic Server Allocation Policies", "comment": "20 pages", "summary": "Cloud computing enables the dynamic provisioning of server resources. To\nexploit this opportunity, a policy is needed for dynamically allocating (and\ndeallocating) servers in response to the current load conditions. In this paper\nwe describe several simple policies for dynamic server allocation and develop\nanalytic models for their analysis. We also design semi-Markov decision models\nthat enable determination of the performance achieved with optimal policies,\nallowing us to quantify the performance gap between simple, easily implemented\npolicies, and optimal policies. Finally, we apply our models to study the\npotential performance benefits of state-dependent routing in multi-site systems\nwhen using dynamic server allocation at each site. Insights from our results\nare valuable to service providers wanting to balance cloud service costs and\ndelays."}
{"id": "2507.19712", "categories": ["cs.DC", "cs.AI", "cs.GT", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.19712", "abs": "https://arxiv.org/abs/2507.19712", "authors": ["Ngoc Hung Nguyen", "Nguyen Van Thieu", "Quang-Trung Luu", "Anh Tuan Nguyen", "Senura Wanasekara", "Nguyen Cong Luong", "Fatemeh Kavehmadavani", "Van-Dinh Nguyen"], "title": "Oranits: Mission Assignment and Task Offloading in Open RAN-based ITS using Metaheuristic and Deep Reinforcement Learning", "comment": "15 pages, 13 figures", "summary": "In this paper, we explore mission assignment and task offloading in an Open\nRadio Access Network (Open RAN)-based intelligent transportation system (ITS),\nwhere autonomous vehicles leverage mobile edge computing for efficient\nprocessing. Existing studies often overlook the intricate interdependencies\nbetween missions and the costs associated with offloading tasks to edge\nservers, leading to suboptimal decision-making. To bridge this gap, we\nintroduce Oranits, a novel system model that explicitly accounts for mission\ndependencies and offloading costs while optimizing performance through vehicle\ncooperation. To achieve this, we propose a twofold optimization approach.\nFirst, we develop a metaheuristic-based evolutionary computing algorithm,\nnamely the Chaotic Gaussian-based Global ARO (CGG-ARO), serving as a baseline\nfor one-slot optimization. Second, we design an enhanced reward-based deep\nreinforcement learning (DRL) framework, referred to as the Multi-agent Double\nDeep Q-Network (MA-DDQN), that integrates both multi-agent coordination and\nmulti-action selection mechanisms, significantly reducing mission assignment\ntime and improving adaptability over baseline methods. Extensive simulations\nreveal that CGG-ARO improves the number of completed missions and overall\nbenefit by approximately 7.1% and 7.7%, respectively. Meanwhile, MA-DDQN\nachieves even greater improvements of 11.0% in terms of mission completions and\n12.5% in terms of the overall benefit. These results highlight the\neffectiveness of Oranits in enabling faster, more adaptive, and more efficient\ntask processing in dynamic ITS environments."}
{"id": "2507.19723", "categories": ["cs.DC", "Primary 68W10, Secondary 65Y05, 68M20", "C.1.2; C.4; D.1.3"], "pdf": "https://arxiv.org/pdf/2507.19723", "abs": "https://arxiv.org/abs/2507.19723", "authors": ["Mufakir Qamar Ansari", "Mudabir Qamar Ansari"], "title": "Accelerating Matrix Multiplication: A Performance Comparison Between Multi-Core CPU and GPU", "comment": "13 pages, 3 figures. Complete C++/CUDA source code included in\n  Appendix", "summary": "Matrix multiplication is a foundational operation in scientific computing and\nmachine learning, yet its computational complexity makes it a significant\nbottleneck for large-scale applications. The shift to parallel architectures,\nprimarily multi-core CPUs and many-core GPUs, is the established solution, and\nthese systems are now ubiquitous from datacenters to consumer laptops. This\npaper presents a direct, empirical performance analysis of matrix\nmultiplication on a modern, consumer-grade heterogeneous platform. We\nimplemented and benchmarked three versions of the algorithm: a baseline\nsequential C++ implementation, a parallel version for its multi-core CPU using\nOpenMP, and a massively parallel version for its discrete GPU using CUDA with\nshared memory optimizations. The implementations were evaluated with square\nmatrices of varying dimensions, from 128x128 to 4096x4096. Our results show\nthat while the parallel CPU provides a consistent speedup of 12-14x over the\nsequential version, the GPU's performance scales dramatically with problem\nsize. For a 4096x4096 matrix, the GPU implementation achieved a speedup of\napproximately 593x over the sequential baseline and 45x over the optimized\nparallel CPU version. These findings quantitatively demonstrate the profound\nimpact of many-core GPU architectures on accelerating data-parallel workloads,\nunderscoring that significant performance gains are readily accessible even on\nconsumer-level hardware."}
{"id": "2507.19845", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.19845", "abs": "https://arxiv.org/abs/2507.19845", "authors": ["Bohan Zhao", "Guang Yang", "Shuo Chen", "Ruitao Liu", "Tingrui Zhang", "Yongchao He", "Wei Xu"], "title": "MegatronApp: Efficient and Comprehensive Management on Distributed LLM Training", "comment": null, "summary": "The rapid escalation in the parameter count of large language models (LLMs)\nhas transformed model training from a single-node endeavor into a highly\nintricate, cross-node activity. While frameworks such as Megatron-LM\nsuccessfully integrate tensor (TP), pipeline (PP), and data (DP) parallelism to\nenable trillion-parameter training, they simultaneously expose practitioners to\nunprecedented systems-level challenges in performance optimization, diagnosis,\nand interpretability. MegatronApp is an open-source toolchain expressly\ndesigned to meet these challenges. It introduces four orthogonal, yet\nseamlessly composable modules--MegaScan, MegaFBD, MegaDPP, and MegaScope--that\ncollectively elevate the reliability, efficiency, and transparency of\nproduction-scale training. This paper presents the motivation, architecture,\nand distinctive contributions of each module, and elucidates how their\nsynergistic integration augments the Megatron-LM ecosystem."}
{"id": "2507.19728", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.19728", "abs": "https://arxiv.org/abs/2507.19728", "authors": ["Lalita Na Nongkhai", "Jingyun Wang", "Takahiko Mendori"], "title": "Development and Evaluation of Adaptive LearningSupport System Based on Ontology of MultipleProgramming Languages", "comment": "This document provides corrections to the published article.\n  Corrections include clarifying figure legends and addressing grammatical\n  issues to enhance clarity. The authors state that the scientific conclusions\n  are unaffected", "summary": "This paper introduces an ontology-based approach within an adaptive learning\nsupport system for computer programming. This system (named ADVENTURE) is\ndesigned to deliver personalized programming exercises that are tailored to\nindividual learners' skill levels. ADVENTURE utilizes an ontology, named\nCONTINUOUS, which encompasses common concepts across multiple programming\nlanguages. The system leverages this ontology not only to visualize programming\nconcepts but also to provide hints during practice programming exercises and\nrecommend subsequent programming concepts. The adaptive mechanism is driven by\nthe Elo Rating System, applied in an educational context to dynamically\nestimate the most appropriate exercise difficulty for each learner. An\nexperimental study compared two instructional modes, adaptive and random, based\non six features derived from 1,186 code submissions across all the experimental\ngroups. The results indicate significant differences in four of six analyzed\nfeatures between these two modes. Notably, the adaptive mode demonstrates a\nsignificant difference over the random mode in two features, the submission of\ncorrect answers and the number of pass concepts. Therefore, these results\nunderscore that this adaptive learning support system may support learners in\npracticing programming exercises."}
{"id": "2507.19570", "categories": ["cs.AR", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.19570", "abs": "https://arxiv.org/abs/2507.19570", "authors": ["Yiting Wang", "Wanghao Ye", "Yexiao He", "Yiran Chen", "Gang Qu", "Ang Li"], "title": "MCP4EDA: LLM-Powered Model Context Protocol RTL-to-GDSII Automation with Backend Aware Synthesis Optimization", "comment": "7 pages, 5 figures Keywords: Model Context Protocol, Electronic\n  Design Automation, Large Language Models, Synthesis Optimization", "summary": "This paper presents MCP4EDA, the first Model Context Protocol server that\nenables Large Language Models (LLMs) to control and optimize the complete\nopen-source RTL-to-GDSII design flow through natural language interaction. The\nsystem integrates Yosys synthesis, Icarus Verilog simulation, OpenLane\nplace-and-route, GTKWave analysis, and KLayout visualization into a unified\nLLM-accessible interface, enabling designers to execute complex multi-tool EDA\nworkflows conversationally via AI assistants such as Claude Desktop and Cursor\nIDE. The principal contribution is a backend-aware synthesis optimization\nmethodology wherein LLMs analyze actual post-layout timing, power, and area\nmetrics from OpenLane results to iteratively refine synthesis TCL scripts,\nestablishing a closed-loop optimization system that bridges the traditional gap\nbetween synthesis estimates and physical implementation reality. In contrast to\nconventional flows that rely on wire-load models, this methodology leverages\nreal backend performance data to guide synthesis parameter tuning, optimization\nsequence selection, and constraint refinement, with the LLM functioning as an\nintelligent design space exploration agent. Experimental evaluation on\nrepresentative digital designs demonstrates 15-30% improvements in timing\nclosure and 10-20% area reduction compared to default synthesis flows,\nestablishing MCP4EDA as the first practical LLM-controlled end-to-end\nopen-source EDA automation system. The code and demo are avaiable at:\nhttp://www.agent4eda.com/"}
{"id": "2507.19926", "categories": ["cs.DC", "cs.CV", "I.3.1; I.4.3"], "pdf": "https://arxiv.org/pdf/2507.19926", "abs": "https://arxiv.org/abs/2507.19926", "authors": ["Louis Sugy"], "title": "A Fast Parallel Median Filtering Algorithm Using Hierarchical Tiling", "comment": "8 pages, 8 figures", "summary": "Median filtering is a non-linear smoothing technique widely used in digital\nimage processing to remove noise while retaining sharp edges. It is\nparticularly well suited to removing outliers (impulse noise) or granular\nartifacts (speckle noise). However, the high computational cost of median\nfiltering can be prohibitive. Sorting-based algorithms excel with small kernels\nbut scale poorly with increasing kernel diameter, in contrast to constant-time\nmethods characterized by higher constant factors but better scalability, such\nas histogram-based approaches or the 2D wavelet matrix.\n  This paper introduces a novel algorithm, leveraging the separability of the\nsorting problem through hierarchical tiling to minimize redundant computations.\nWe propose two variants: a data-oblivious selection network that can operate\nentirely within registers, and a data-aware version utilizing random-access\nmemory. These achieve per-pixel complexities of $O(k \\log(k))$ and $O(k)$,\nrespectively, for a $k \\times k$ kernel - unprecedented for sorting-based\nmethods. Our CUDA implementation is up to 5 times faster than the current state\nof the art on a modern GPU and is the fastest median filter in most cases for\n8-, 16-, and 32-bit data types and kernels from $3 \\times 3$ to $75 \\times 75$."}
{"id": "2507.20251", "categories": ["cs.PL", "cs.CC", "cs.DB", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.20251", "abs": "https://arxiv.org/abs/2507.20251", "authors": ["Angelos Charalambidis", "Babis Kostopoulos", "Christos Nomikos", "Panos Rondogiannis"], "title": "The Power of Negation in Higher-Order Datalog", "comment": null, "summary": "We investigate the expressive power of Higher-Order Datalog$^\\neg$ under both\nthe well-founded and the stable model semantics, establishing tight connections\nwith complexity classes. We prove that under the well-founded semantics, for\nall $k\\geq 1$, $(k+1)$-Order Datalog$^\\neg$ captures k-EXP, a result that holds\nwithout explicit ordering of the input database. The proof of this fact can be\nperformed either by using the powerful existential predicate variables of the\nlanguage or by using partially applied relations and relation enumeration.\nFurthermore, we demonstrate that this expressive power is retained within a\nstratified fragment of the language. Under the stable model semantics, we show\nthat $(k+1)$-Order Datalog$^\\neg$ captures co-(k-NEXP) using cautious reasoning\nand k-NEXP using brave reasoning, again with analogous results for the\nstratified fragment augmented with choice rules. Our results establish a\nhierarchy of expressive power, highlighting an interesting trade-off between\norder and non-determinism in the context of higher-order logic programming:\nincreasing the order of programs under the well-founded semantics can surpass\nthe expressive power of lower-order programs under the stable model semantics."}
{"id": "2507.19819", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.19819", "abs": "https://arxiv.org/abs/2507.19819", "authors": ["Alexander Graening", "Puneet Gupta", "Andrew B. Kahng", "Bodhisatta Pramanik", "Zhiang Wang"], "title": "ChipletPart: Scalable Cost-Aware Partitioning for 2.5D Systems", "comment": "14 pages, 13 figures", "summary": "Industry adoption of chiplets has been increasing as a cost-effective option\nfor making larger high-performance systems. Consequently, partitioning large\nsystems into chiplets is increasingly important. In this work, we introduce\nChipletPart - a cost-driven 2.5D system partitioner that addresses the unique\nconstraints of chiplet systems, including complex objective functions, limited\nreach of inter-chiplet I/O transceivers, and the assignment of heterogeneous\nmanufacturing technologies to different chiplets. ChipletPart integrates a\nsophisticated chiplet cost model with its underlying genetic algorithm-based\ntechnology assignment and partitioning methodology, along with a simulated\nannealing-based chiplet floorplanner. Our results show that: (i) ChipletPart\nreduces chiplet cost by up to 58% (20% geometric mean) compared to\nstate-of-the-art min-cut partitioners, which often yield floorplan-infeasible\nsolutions; (ii) ChipletPart generates partitions with up to 47% (6% geometric\nmean) lower cost as compared to the prior work Floorplet; and (iii) for the\ntestcases we study, heterogeneous integration reduces cost by up to 43% (15%\ngeometric mean) compared to homogeneous implementations. We also present case\nstudies that show how changes in packaging or inter-chiplet signaling\ntechnologies can affect partitioning solutions. Finally, we make ChipletPart,\nthe underlying chiplet cost model, and a chiplet testcase generator available\nas open-source tools for the community."}
{"id": "2507.19953", "categories": ["cs.DC", "D.2.5; C.3; D.2.11"], "pdf": "https://arxiv.org/pdf/2507.19953", "abs": "https://arxiv.org/abs/2507.19953", "authors": ["David Jannis Schmidt", "Grigory Fridman", "Florian von Zabiensky"], "title": "Offloading tracing for real-time systems using a scalable cloud infrastructure", "comment": "Submitted to ECRTS 2025 RT-Cloud Workshop proceedings", "summary": "Real-time embedded systems require precise timing and fault detection to\nensure correct behavior. Traditional tracing tools often rely on local desktops\nwith limited processing and storage capabilities, which hampers large-scale\nanalysis. This paper presents a scalable, cloud-based architecture for software\ntracing in real-time systems based on microservices and edge computing. Our\napproach shifts the trace processing workload from the developer's machine to\nthe cloud, using a dedicated tracing component that captures trace data and\nforwards it to a scalable backend via WebSockets and Apache Kafka. This enables\nlong-term monitoring and collaborative analysis of target executions, e.g., to\ndetect and investigate sporadic errors. We demonstrate how this architecture\nsupports scalable analysis of parallel tracing sessions and lays the foundation\nfor future integration of rule-based testing and runtime verification. The\nevaluation results show that the architecture can handle many parallel tracing\nsessions efficiently, although the per-session throughput decreases slightly as\nthe system load increases, while the overall throughput increases. Although the\ndesign includes a dedicated tracer for analysis during development, this\napproach is not limited to such setups. Target systems with network\nconnectivity can stream reduced trace data directly, enabling runtime\nmonitoring in the field."}
{"id": "2507.20007", "categories": ["cs.AR", "cs.ET", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.20007", "abs": "https://arxiv.org/abs/2507.20007", "authors": ["Siva Satyendra Sahoo", "Salim Ullah", "Akash Kumar"], "title": "AxOSyn: An Open-source Framework for Synthesizing Novel Approximate Arithmetic Operators", "comment": "Under review with ACM TRETS", "summary": "Edge AI deployments are becoming increasingly complex, necessitating\nenergy-efficient solutions for resource-constrained embedded systems.\nApproximate computing, which allows for controlled inaccuracies in\ncomputations, is emerging as a promising approach for improving power and\nenergy efficiency. Among the key techniques in approximate computing are\napproximate arithmetic operators (AxOs), which enable application-specific\noptimizations beyond traditional computer arithmetic hardware reduction-based\nmethods, such as quantization and precision scaling. Existing design space\nexploration (DSE) frameworks for approximate computing limit themselves to\nselection-based approaches or custom synthesis at fixed abstraction levels,\nwhich restricts the flexibility required for finding application-specific\noptimal solutions. Further, the tools available for the DSE of AxOs are quite\nlimited in terms of exploring different approximation models and extending the\nanalysis to different granularities. To this end, we propose AxOSyn, an\nopen-source framework for the DSE of AxOs that supports both selection and\nsynthesis approaches at various abstraction levels. AxOSyn allows researchers\nto integrate custom methods for evaluating approximations and facilitates DSE\nat both the operator-level and application-specific. Our framework provides an\neffective methodology for achieving energy-efficient, approximate operators."}
{"id": "2507.20041", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2507.20041", "abs": "https://arxiv.org/abs/2507.20041", "authors": ["Daniel Manor", "Mor Perry", "Moshe Sulamy"], "title": "MTASet: A Tree-based Set for Efficient Range Queries in Update-heavy Workloads", "comment": null, "summary": "In concurrent data structures, the efficiency of set operations can vary\nsignificantly depending on the workload characteristics. Numerous concurrent\nset implementations are optimized and fine-tuned to excel in scenarios\ncharacterized by predominant read operations. However, they often perform\npoorly when confronted with workloads that heavily prioritize updates.\nAdditionally, current leading-edge concurrent sets optimized for update-heavy\ntasks typically lack efficiency in handling atomic range queries. This study\nintroduces the MTASet, which leverages a concurrent (a,b)-tree implementation.\nEngineered to accommodate update-heavy workloads and facilitate atomic range\nqueries, MTASet surpasses existing counterparts optimized for tasks in range\nquery operations by up to 2x. Notably, MTASet ensures linearizability."}
{"id": "2507.20412", "categories": ["cs.AR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.20412", "abs": "https://arxiv.org/abs/2507.20412", "authors": ["Maximilian Jakob Heer", "Benjamin Ramhorst", "Yu Zhu", "Luhao Liu", "Zhiyi Hu", "Jonas Dann", "Gustavo Alonso"], "title": "RoCE BALBOA: Service-enhanced Data Center RDMA for SmartNICs", "comment": null, "summary": "Data-intensive applications in data centers, especially machine learning\n(ML), have made the network a bottleneck, which in turn has motivated the\ndevelopment of more efficient network protocols and infrastructure. For\ninstance, remote direct memory access (RDMA) has become the standard protocol\nfor data transport in the cloud as it minimizes data copies and reduces\nCPU-utilization via host-bypassing. Similarly, an increasing amount of network\nfunctions and infrastructure have moved to accelerators, SmartNICs, and\nin-network computing to bypass the CPU. In this paper we explore the\nimplementation and deployment of RoCE BALBOA, an open-source, RoCE\nv2-compatible, scalable up to hundreds of queue-pairs, and 100G-capable\nRDMA-stack that can be used as the basis for building accelerators and\nsmartNICs. RoCE BALBOA is customizable, opening up a design space and offering\na degree of adaptability not available in commercial products. We have deployed\nBALBOA in a cluster using FPGAs and show that it has latency and performance\ncharacteristics comparable to commercial NICs. We demonstrate its potential by\nexploring two classes of use cases. One involves enhancements to the protocol\nfor infrastructure purposes (encryption, deep packet inspection using ML). The\nother showcases the ability to perform line-rate compute offloads with deep\npipelines by implementing commercial data preprocessing pipelines for\nrecommender systems that process the data as it arrives from the network before\ntransferring it directly to the GPU. These examples demonstrate how BALBOA\nenables the exploration and development of SmartNICs and accelerators operating\non network data streams."}
{"id": "2507.20063", "categories": ["cs.DC", "cs.CC", "Primary 68W10, Secondary 65Y05, 68M20", "C.4; D.1.3"], "pdf": "https://arxiv.org/pdf/2507.20063", "abs": "https://arxiv.org/abs/2507.20063", "authors": ["Mufakir Qamar Ansari", "Mudabir Qamar Ansari"], "title": "Racing to Idle: Energy Efficiency of Matrix Multiplication on Heterogeneous CPU and GPU Architectures", "comment": "16 pages, 6 figures, 3 listings. A comprehensive empirical study on a\n  consumer-grade heterogeneous platform", "summary": "The paradigm shift towards multi-core and heterogeneous computing, driven by\nthe fundamental power and thermal limits of single-core processors, has\nestablished energy efficiency as a first-class design constraint in\nhigh-performance computing (HPC). Heterogeneous systems, integrating\ntraditional multi-core CPUs with specialized accelerators like discrete (dGPU)\nand integrated (iGPU) graphics processing units, offer a compelling path to\nnavigating the trade-offs between performance and power. However, quantifying\nthese trade-offs on widely accessible hardware remains a critical area of\nstudy. This paper presents a direct, empirical measurement of the performance\nand energy-to-solution of a canonical HPC workload -- a 4096x4096 matrix-matrix\nmultiplication -- on three distinct compute architectures within a single\nconsumer-grade laptop: a multi-core AMD Ryzen 7 5800H CPU, a discrete NVIDIA\nGeForce GTX 1650 GPU, and an integrated AMD Radeon Vega GPU. Using standard,\nvalidated, and minimally intrusive tools such as Linux perf and nvidia-smi, we\nfind that the discrete GPU is not only the performance leader, achieving a\n93.5x speedup over the CPU, but is also the most energy-efficient, consuming\nonly 2% of the energy used by the CPU, resulting in a 50-fold improvement in\nenergy efficiency. These findings provide a practical demonstration of the\n\"race to idle\" principle and offer clear, quantitative guidance on\narchitectural choices for energy-aware software development."}
{"id": "2507.20420", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.20420", "abs": "https://arxiv.org/abs/2507.20420", "authors": ["Md Rownak Hossain Chowdhury", "Mostafizur Rahman"], "title": "Demystifying the 7-D Convolution Loop Nest for Data and Instruction Streaming in Reconfigurable AI Accelerators", "comment": null, "summary": "Convolution remains the most compute-intensive operation in AI acceleration,\noften constituting over 80-90% of the workload. Existing approaches in spatial\narchitectures such as coarse-grained reconfigurable arrays (CGRAs) and\nfield-programmable gate arrays (FPGAs) frequently rely on loop unrolling or\nGEMM-based matrix transformations, introducing significant overhead in both\ndata movement and instruction control. This paper presents a new framework\ndesigned to systematically demystify the 7-dimensional convolution loop nest by\nreinterpreting it as a hardware-centric data and instruction streaming problem.\nInstead of treating the loop nest as a fixed computational construct, our\napproach exposes its structure as a set of spatial and temporal mappings\ngoverned by hardware parameters such as compute element distribution,\ninterconnect topology, and reconfigurability. This abstraction supports\nlightweight, flexible deployment of convolution without reliance on heavyweight\ntransformations or reordering schemes. We demonstrate the application of our\napproach on the MAVeC accelerator. We detail the implementation of convolution\noperations in MAVeC and extend the framework to support full model execution on\nVGG-16. Our profiling reveals high PE utilization (over 90%), significant fold\nreuse, and scalable throughput up to 1.56 TFLOPs/sec and 12.7 KIPS for\nend-to-end VGG-16 inference. These results validate the efficacy of our\napproach in minimizing control overhead, improving data locality, and enabling\nefficient large-scale convolution execution without reliance on conventional\ntransformation-based methods."}
{"id": "2507.20173", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20173", "abs": "https://arxiv.org/abs/2507.20173", "authors": ["Haitian Wang", "Long Qin"], "title": "High-Performance Parallel Optimization of the Fish School Behaviour on the Setonix Platform Using OpenMP", "comment": null, "summary": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization."}
{"id": "2507.20196", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.20196", "abs": "https://arxiv.org/abs/2507.20196", "authors": ["Dvir David Biton", "Roy Friedman", "Yaron Hay"], "title": "Ethereum Conflicts Graphed", "comment": "A slightly shorter version To appear in the Proceedings of the IEEE\n  International Conference on Blockchain and Cryptocurrency, ICBC 2025", "summary": "Ethereum, a leading blockchain platform, has revolutionized the digital\neconomy by enabling decentralized transactions and the execution of smart\ncontracts. Ethereum transactions form the backbone of its network, facilitating\npeer-to-peer exchanges and interactions with complex decentralized\napplications. Smart contracts extend Ethereum's capabilities by automating\nprocesses and enabling trustless execution of agreements. Hence, understanding\nhow these smart contracts interact is important in order to facilitate various\nperformance optimizations, such as warming objects before they are being\naccessed and enabling concurrent execution. Of particular interest to us are\nthe development of the calling graph, as well as the read sets and write sets\nof invocations within the same block, and the properties of the associated\nconflict graph that is derived from them. The latter is important for\nunderstanding the parallelization potential of smart contracts on Ethereum. We\ntraced upwards of 2 million recent Ethereum blocks using call tracer and\nprestate tracer, out of a total of 21.4 million blocks at the time of writing.\nWe report on the transactions per block distribution, the structure of call\ntrees in smart contract invocations, the ratio of value-transfer transactions\nto smart contract invocations, as well as provide a comprehensive study of the\nstructure of blocks' conflict graphs. We find that conflict graphs\npredominantly show a star like configuration, as well as other noteworthy\nstructural properties."}
{"id": "2507.20201", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.20201", "abs": "https://arxiv.org/abs/2507.20201", "authors": ["Jérémie Chalopin", "Shantanu Das", "Maria Kokkou"], "title": "Silent Self-Stabilising Leader Election in Programmable Matter Systems with Holes", "comment": "15 pages", "summary": "Leader election is a fundamental problem in distributed computing,\nparticularly within programmable matter systems, where coordination among\nsimple computational entities is crucial for solving complex tasks. In these\nsystems, particles (i.e., constant memory computational entities) operate in a\nregular triangular grid as described in the geometric Amoebot model. While\nleader election has been extensively studied in non self-stabilising settings,\nself-stabilising solutions remain more limited. In this work, we study the\nproblem of self-stabilising leader election in connected (but not necessarily\nsimply connected) configurations. We present the first self-stabilising\nalgorithm for programmable matter that guarantees the election of a unique\nleader under an unfair scheduler, assuming particles share a common sense of\ndirection. Our approach leverages particle movement, a capability not\npreviously exploited in the self-stabilising context. We show that movement in\nconjunction with particles operating in a grid can overcome classical\nimpossibility results for constant-memory systems established by Dolev et al."}
{"id": "2507.20312", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.20312", "abs": "https://arxiv.org/abs/2507.20312", "authors": ["Jonas H. Müller Korndörfer", "Ali Mohammed", "Ahmed Eleliemy", "Quentin Guilloteau", "Reto Krummenacher", "Florina M. Ciorba"], "title": "A Comparative Study of OpenMP Scheduling Algorithm Selection Strategies", "comment": "To appear at IEEE ACCESS", "summary": "Scientific and data science applications are becoming increasingly complex,\nwith growing computational and memory demands. Modern high performance\ncomputing (HPC) systems provide high parallelism and heterogeneity across\nnodes, devices, and cores. To achieve good performance, effective scheduling\nand load balancing techniques are essential. Parallel programming frameworks\nsuch as OpenMP now offer a variety of advanced scheduling algorithms to support\ndiverse applications and platforms. This creates an instance of the scheduling\nalgorithm selection problem, which involves identifying the most suitable\nalgorithm for a given combination of workload and system characteristics.\n  In this work, we explore learning-based approaches for selecting scheduling\nalgorithms in OpenMP. We propose and evaluate expert-based and reinforcement\nlearning (RL)-based methods, and conduct a detailed performance analysis across\nsix applications and three systems. Our results show that RL methods are\ncapable of learning high-performing scheduling decisions, although they require\nsignificant exploration, with the choice of reward function playing a key role.\nExpert-based methods, in contrast, rely on prior knowledge and involve less\nexploration, though they may not always identify the optimal algorithm for a\nspecific application-system pair. By combining expert knowledge with RL-based\nlearning, we achieve improved performance and greater adaptability.\n  Overall, this work demonstrates that dynamic selection of scheduling\nalgorithms during execution is both viable and beneficial for OpenMP\napplications. The approach can also be extended to MPI-based programs, enabling\noptimization of scheduling decisions across multiple levels of parallelism."}
{"id": "2507.20514", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.20514", "abs": "https://arxiv.org/abs/2507.20514", "authors": ["Serhan Gener", "Aditya Ukarande", "Shilpa Mysore Srinivasa Murthy", "Sahil Hassan", "Joshua Mack", "Chaitali Chakrabarti", "Umit Ogras", "Ali Akoglu"], "title": "RIMMS: Runtime Integrated Memory Management System for Heterogeneous Computing", "comment": null, "summary": "Efficient memory management in heterogeneous systems is increasingly\nchallenging due to diverse compute architectures (e.g., CPU, GPU, FPGA) and\ndynamic task mappings not known at compile time. Existing approaches often\nrequire programmers to manage data placement and transfers explicitly, or\nassume static mappings that limit portability and scalability. This paper\nintroduces RIMMS (Runtime Integrated Memory Management System), a lightweight,\nruntime-managed, hardware-agnostic memory abstraction layer that decouples\napplication development from low-level memory operations. RIMMS transparently\ntracks data locations, manages consistency, and supports efficient memory\nallocation across heterogeneous compute elements without requiring\nplatform-specific tuning or code modifications. We integrate RIMMS into a\nbaseline runtime and evaluate with complete radar signal processing\napplications across CPU+GPU and CPU+FPGA platforms. RIMMS delivers up to 2.43X\nspeedup on GPU-based and 1.82X on FPGA-based systems over the baseline.\nCompared to IRIS, a recent heterogeneous runtime system, RIMMS achieves up to\n3.08X speedup and matches the performance of native CUDA implementations while\nsignificantly reducing programming complexity. Despite operating at a higher\nabstraction level, RIMMS incurs only 1-2 cycles of overhead per memory\nmanagement call, making it a low-cost solution. These results demonstrate\nRIMMS's ability to deliver high performance and enhanced programmer\nproductivity in dynamic, real-world heterogeneous environments."}
