{"id": "2510.17889", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17889", "abs": "https://arxiv.org/abs/2510.17889", "authors": ["Eilene Tomkins-Flanagan", "Mary A. Kelly"], "title": "Hey Pentti, We Did It!: A Fully Vector-Symbolic Lisp", "comment": null, "summary": "Kanerva (2014) suggested that it would be possible to construct a complete\nLisp out of a vector-symbolic architecture. We present the general form of a\nvector-symbolic representation of the five Lisp elementary functions, lambda\nexpressions, and other auxiliary functions, found in the Lisp 1.5 specification\nMcCarthy (1960), which is near minimal and sufficient for Turing-completeness.\nOur specific implementation uses holographic reduced representations Plate\n(1995), with a lookup table cleanup memory. Lisp, as all Turing-complete\nlanguages, is a Cartesian closed category, unusual in its proximity to the\nmathematical abstraction. We discuss the mathematics, the purpose, and the\nsignificance of demonstrating vector-symbolic architectures' Cartesian-closure,\nas well as the importance of explicitly including cleanup memories in the\nspecification of the architecture.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5728\u5411\u91cf\u7b26\u53f7\u67b6\u6784\u4e2d\u5b9e\u73b0\u5b8c\u6574\u7684Lisp\u8bed\u8a00\uff0c\u5305\u62ec\u4e94\u4e2a\u57fa\u672c\u51fd\u6570\u3001lambda\u8868\u8fbe\u5f0f\u7b49\uff0c\u4f7f\u7528\u5168\u606f\u7b80\u5316\u8868\u793a\u548c\u67e5\u627e\u8868\u6e05\u7406\u5185\u5b58\u3002", "motivation": "\u9a8c\u8bc1Kanerva(2014)\u7684\u5047\u8bbe\uff0c\u8bc1\u660e\u5411\u91cf\u7b26\u53f7\u67b6\u6784\u80fd\u591f\u6784\u5efa\u5b8c\u6574\u7684Lisp\u8bed\u8a00\uff0c\u5e76\u63a2\u8ba8\u5176\u6570\u5b66\u610f\u4e49\u548c\u67b6\u6784\u89c4\u8303\u7684\u91cd\u8981\u6027\u3002", "method": "\u4f7f\u7528\u5168\u606f\u7b80\u5316\u8868\u793a(Plate, 1995)\u548c\u67e5\u627e\u8868\u6e05\u7406\u5185\u5b58\uff0c\u6784\u5efaLisp 1.5\u89c4\u8303\u4e2d\u4e94\u4e2a\u57fa\u672c\u51fd\u6570\u3001lambda\u8868\u8fbe\u5f0f\u53ca\u5176\u4ed6\u8f85\u52a9\u51fd\u6570\u7684\u5411\u91cf\u7b26\u53f7\u8868\u793a\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u5411\u91cf\u7b26\u53f7\u67b6\u6784\u4e0b\u7684\u5b8c\u6574Lisp\u8bed\u8a00\uff0c\u8bc1\u660e\u4e86\u8be5\u67b6\u6784\u7684\u7b1b\u5361\u5c14\u95ed\u5305\u6027\u8d28\u3002", "conclusion": "\u5411\u91cf\u7b26\u53f7\u67b6\u6784\u5177\u5907\u6784\u5efa\u56fe\u7075\u5b8c\u5907\u8bed\u8a00\u7684\u80fd\u529b\uff0c\u6e05\u7406\u5185\u5b58\u5e94\u660e\u786e\u5305\u542b\u5728\u67b6\u6784\u89c4\u8303\u4e2d\uff0c\u8fd9\u5bf9\u7406\u89e3\u5411\u91cf\u7b26\u53f7\u8ba1\u7b97\u7684\u6570\u5b66\u57fa\u7840\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.18479", "categories": ["cs.PL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2510.18479", "abs": "https://arxiv.org/abs/2510.18479", "authors": ["Samuel Chassot", "Viktor Kun\u010dak"], "title": "ZipLex: Verified Invertible Lexing with Memoized Derivatives and Zippers", "comment": null, "summary": "We present ZipLex, a verified framework for invertible lexical analysis.\nUnlike past verified lexers that focus only on satisfying the semantics of\nregular expressions and the maximal munch property, ZipLex also guarantees that\nlexing and printing are mutual inverses. Our design relies on two sets of\nideas: (1) a new abstraction of token sequences that captures the separability\nof tokens in a sequence while supporting their efficient manipulation, and (2)\na combination of verified data structures and optimizations, including Huet's\nzippers and memoized derivatives, to achieve practical performance. We\nimplemented ZipLex in Scala and verified its correctness, including\ninvertibility, using the Stainless verifier. Our evaluation demonstrates that\nZipLex supports realistic applications such as JSON processing and lexers of\nprogramming languages. In comparison to other verified lexers (which do not\nenforce invertibility), ZipLex is 4x slower than Coqlex and two orders of\nmagnitude faster than Verbatim++, showing that verified invertibility can be\nachieved without prohibitive cost.", "AI": {"tldr": "ZipLex\u662f\u4e00\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u53ef\u9006\u8bcd\u6cd5\u5206\u6790\u6846\u67b6\uff0c\u4e0d\u4ec5\u4fdd\u8bc1\u6b63\u5219\u8868\u8fbe\u5f0f\u8bed\u4e49\u548c\u6700\u5927\u5339\u914d\u5c5e\u6027\uff0c\u8fd8\u786e\u4fdd\u8bcd\u6cd5\u5206\u6790\u548c\u6253\u5370\u4e92\u4e3a\u9006\u64cd\u4f5c\u3002", "motivation": "\u4f20\u7edf\u9a8c\u8bc1\u8bcd\u6cd5\u5206\u6790\u5668\u53ea\u5173\u6ce8\u6b63\u5219\u8868\u8fbe\u5f0f\u8bed\u4e49\u548c\u6700\u5927\u5339\u914d\u5c5e\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8bcd\u6cd5\u5206\u6790\u548c\u6253\u5370\u4e92\u4e3a\u9006\u64cd\u4f5c\u7684\u4fdd\u8bc1\u3002", "method": "\u91c7\u7528\u65b0\u7684token\u5e8f\u5217\u62bd\u8c61\u6765\u6355\u83b7token\u53ef\u5206\u79bb\u6027\uff0c\u7ed3\u5408\u9a8c\u8bc1\u6570\u636e\u7ed3\u6784\uff08Huet\u62c9\u94fe\u548c\u8bb0\u5fc6\u5316\u5bfc\u6570\uff09\u8fdb\u884c\u4f18\u5316\u3002", "result": "ZipLex\u5728Scala\u4e2d\u5b9e\u73b0\u5e76\u901a\u8fc7Stainless\u9a8c\u8bc1\u5668\u9a8c\u8bc1\u6b63\u786e\u6027\uff0c\u652f\u6301JSON\u5904\u7406\u548c\u7f16\u7a0b\u8bed\u8a00\u8bcd\u6cd5\u5206\u6790\u7b49\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u9a8c\u8bc1\u53ef\u9006\u6027\u53ef\u4ee5\u5728\u4e0d\u4ed8\u51fa\u8fc7\u9ad8\u4ee3\u4ef7\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\uff0cZipLex\u6bd4Coqlex\u61624\u500d\u4f46\u6bd4Verbatim++\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002"}}
{"id": "2510.18651", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18651", "abs": "https://arxiv.org/abs/2510.18651", "authors": ["Uraz Odyurt", "\u00d6mer Sayilir", "Mari\u00eblle Stoelinga", "Vadim Zaytsev"], "title": "CPSLint: A Domain-Specific Language Providing Data Validation and Sanitisation for Industrial Cyber-Physical Systems", "comment": null, "summary": "Raw datasets are often too large and unstructured to work with directly, and\nrequire a data preparation process. The domain of industrial Cyber-Physical\nSystems (CPS) is no exception, as raw data typically consists of large amounts\nof time-series data logging the system's status in regular time intervals. Such\ndata has to be sanity checked and preprocessed to be consumable by data-centric\nworkflows. We introduce CPSLint, a Domain-Specific Language designed to provide\ndata preparation for industrial CPS. We build up on the fact that many raw data\ncollections in the CPS domain require similar actions to render them suitable\nfor Machine-Learning (ML) solutions, e.g., Fault Detection and Identification\n(FDI) workflows, yet still vary enough to hope for one universally applicable\nsolution.\n  CPSLint's main features include type checking and enforcing constraints\nthrough validation and remediation for data columns, such as imputing missing\ndata from surrounding rows. More advanced features cover inference of extra\nCPS-specific data structures, both column-wise and row-wise. For instance, as\nrow-wise structures, descriptive execution phases are an effective method of\ndata compartmentalisation are extracted and prepared for ML-assisted FDI\nworkflows. We demonstrate CPSLint's features through a proof of concept\nimplementation.", "AI": {"tldr": "CPSLint\u662f\u4e00\u4e2a\u9488\u5bf9\u5de5\u4e1a\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u7528\u4e8e\u6570\u636e\u9884\u5904\u7406\uff0c\u652f\u6301\u7c7b\u578b\u68c0\u67e5\u3001\u7ea6\u675f\u9a8c\u8bc1\u3001\u7f3a\u5931\u6570\u636e\u586b\u8865\u7b49\u64cd\u4f5c\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u51c6\u5907\u6570\u636e\u3002", "motivation": "\u5de5\u4e1aCPS\u7cfb\u7edf\u4ea7\u751f\u5927\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u8fd9\u4e9b\u539f\u59cb\u6570\u636e\u901a\u5e38\u8fc7\u4e8e\u5e9e\u5927\u4e14\u975e\u7ed3\u6784\u5316\uff0c\u9700\u8981\u6570\u636e\u9884\u5904\u7406\u624d\u80fd\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u5982\u6545\u969c\u68c0\u6d4b\u548c\u8bc6\u522b\u5de5\u4f5c\u6d41\u3002", "method": "\u5f00\u53d1CPSLint\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u63d0\u4f9b\u7c7b\u578b\u68c0\u67e5\u3001\u7ea6\u675f\u9a8c\u8bc1\u548c\u4fee\u590d\u529f\u80fd\uff0c\u652f\u6301\u5217\u7ea7\u548c\u884c\u7ea7\u7684\u6570\u636e\u7ed3\u6784\u63a8\u65ad\uff0c\u5305\u62ec\u7f3a\u5931\u6570\u636e\u586b\u8865\u548c\u63cf\u8ff0\u6027\u6267\u884c\u9636\u6bb5\u63d0\u53d6\u3002", "result": "\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\u5c55\u793a\u4e86CPSLint\u7684\u529f\u80fd\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406CPS\u539f\u59cb\u6570\u636e\uff0c\u4e3aML\u8f85\u52a9\u7684\u6545\u969c\u68c0\u6d4b\u548c\u8bc6\u522b\u5de5\u4f5c\u6d41\u51c6\u5907\u6570\u636e\u3002", "conclusion": "CPSLint\u4e3a\u5de5\u4e1aCPS\u6570\u636e\u9884\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u9886\u57df\u7279\u5b9a\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u76f8\u4f3c\u4f46\u5b58\u5728\u5dee\u5f02\u7684\u6570\u636e\u51c6\u5907\u9700\u6c42\uff0c\u652f\u6301\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u7684\u6570\u636e\u6d88\u8d39\u3002"}}
{"id": "2510.18525", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.18525", "abs": "https://arxiv.org/abs/2510.18525", "authors": ["Yushu Zhao", "Yubin Qin", "Yang Wang", "Xiaolong Yang", "Huiming Han", "Shaojun Wei", "Yang Hu", "Shouyi Yin"], "title": "From Quarter to All: Accelerating Speculative LLM Decoding via Floating-Point Exponent Remapping and Parameter Sharing", "comment": null, "summary": "Large language models achieve impressive performance across diverse tasks but\nexhibit high inference latency due to their large parameter sizes. While\nquantization reduces model size, it often leads to performance degradation\ncompared to the full model. Speculative decoding remains lossless but typically\nincurs extra overheads. We propose SPEQ, an algorithm-hardware co-designed\nspeculative decoding method that uses part of the full-model weight bits to\nform a quantized draft model, thereby eliminating additional training or\nstorage overhead. A reconfigurable processing element array enables efficient\nexecution of both the draft and verification passes. Experimental results\nacross 15 LLMs and tasks demonstrate that SPEQ achieves speedups of 2.07x,\n1.53x, and 1.45x compared over FP16, Olive, and Tender, respectively.", "AI": {"tldr": "SPEQ\u662f\u4e00\u79cd\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u4f7f\u7528\u5168\u6a21\u578b\u7684\u90e8\u5206\u6743\u91cd\u4f4d\u5f62\u6210\u91cf\u5316\u8349\u7a3f\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5b58\u50a8\u5f00\u9500\uff0c\u572815\u4e2aLLM\u548c\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u7531\u4e8e\u53c2\u6570\u89c4\u6a21\u5927\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3002\u91cf\u5316\u53ef\u4ee5\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u4f46\u4f1a\u5e26\u6765\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u63a8\u6d4b\u89e3\u7801\u867d\u7136\u65e0\u635f\u4f46\u901a\u5e38\u4f1a\u4ea7\u751f\u989d\u5916\u5f00\u9500\u3002", "method": "\u63d0\u51faSPEQ\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u7528\u5168\u6a21\u578b\u7684\u90e8\u5206\u6743\u91cd\u4f4d\u6784\u5efa\u91cf\u5316\u8349\u7a3f\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u91cd\u6784\u5904\u7406\u5355\u5143\u9635\u5217\u9ad8\u6548\u6267\u884c\u8349\u7a3f\u548c\u9a8c\u8bc1\u8fc7\u7a0b\u3002", "result": "\u572815\u4e2aLLM\u548c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSPEQ\u76f8\u6bd4FP16\u3001Olive\u548cTender\u5206\u522b\u5b9e\u73b0\u4e862.07\u500d\u30011.53\u500d\u548c1.45\u500d\u7684\u52a0\u901f\u3002", "conclusion": "SPEQ\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u5728\u4e0d\u589e\u52a0\u989d\u5916\u8bad\u7ec3\u6216\u5b58\u50a8\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.17852", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17852", "abs": "https://arxiv.org/abs/2510.17852", "authors": ["Yuze Sun", "Wentao Luo", "Yanfei Xiang", "Jiancheng Pan", "Jiahao Li", "Quan Zhang", "Xiaomeng Huang"], "title": "Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis", "comment": null, "summary": "With the growing role of artificial intelligence in climate and weather\nresearch, efficient model training and inference are in high demand. Current\nmodels like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware\nindependence, especially for Chinese domestic hardware and frameworks. To\naddress this issue, we present a framework for migrating large-scale\natmospheric and oceanic models from PyTorch to MindSpore and optimizing for\nChinese chips, and evaluating their performance against GPUs. The framework\nfocuses on software-hardware adaptation, memory optimization, and parallelism.\nFurthermore, the model's performance is evaluated across multiple metrics,\nincluding training speed, inference speed, model accuracy, and energy\nefficiency, with comparisons against GPU-based implementations. Experimental\nresults demonstrate that the migration and optimization process preserves the\nmodels' original accuracy while significantly reducing system dependencies and\nimproving operational efficiency by leveraging Chinese chips as a viable\nalternative for scientific computing. This work provides valuable insights and\npractical guidance for leveraging Chinese domestic chips and frameworks in\natmospheric and oceanic AI model development, offering a pathway toward greater\ntechnological independence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u5927\u6c14\u548c\u6d77\u6d0bAI\u6a21\u578b\u4ecePyTorch\u8fc1\u79fb\u5230MindSpore\u5e76\u9488\u5bf9\u4e2d\u56fd\u82af\u7247\u4f18\u5316\u7684\u6846\u67b6\uff0c\u65e8\u5728\u51cf\u5c11\u5bf9GPU\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u786c\u4ef6\u72ec\u7acb\u6027\u3002", "motivation": "\u5f53\u524dAI\u6c14\u5019\u548c\u5929\u6c14\u7814\u7a76\u6a21\u578b\uff08\u5982FourCastNet\u548cAI-GOMS\uff09\u4e25\u91cd\u4f9d\u8d56GPU\uff0c\u9650\u5236\u4e86\u786c\u4ef6\u72ec\u7acb\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e2d\u56fd\u56fd\u4ea7\u786c\u4ef6\u548c\u6846\u67b6\u7684\u652f\u6301\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u8f6f\u786c\u4ef6\u9002\u914d\u3001\u5185\u5b58\u4f18\u5316\u548c\u5e76\u884c\u5316\u7684\u8fc1\u79fb\u6846\u67b6\uff0c\u5c06\u6a21\u578b\u4ecePyTorch\u8fc1\u79fb\u5230MindSpore\uff0c\u5e76\u9488\u5bf9\u4e2d\u56fd\u82af\u7247\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fc1\u79fb\u548c\u4f18\u5316\u8fc7\u7a0b\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u539f\u59cb\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u7cfb\u7edf\u4f9d\u8d56\uff0c\u901a\u8fc7\u5229\u7528\u4e2d\u56fd\u82af\u7247\u4f5c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u7684\u53ef\u66ff\u4ee3\u65b9\u6848\u63d0\u9ad8\u4e86\u8fd0\u884c\u6548\u7387\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5728\u5927\u6c14\u548c\u6d77\u6d0bAI\u6a21\u578b\u5f00\u53d1\u4e2d\u5229\u7528\u4e2d\u56fd\u56fd\u4ea7\u82af\u7247\u548c\u6846\u67b6\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u4e3a\u5b9e\u73b0\u66f4\u5927\u7684\u6280\u672f\u72ec\u7acb\u6027\u63d0\u4f9b\u4e86\u9014\u5f84\u3002"}}
{"id": "2510.18152", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18152", "abs": "https://arxiv.org/abs/2510.18152", "authors": ["Zhuoyu Yao", "Yue Wang", "Songyang Zhang", "Yingshu Li", "Zhipeng Cai", "Zhi Tian"], "title": "Efficient Multi-Worker Selection based Distributed Swarm Learning via Analog Aggregation", "comment": "5 pages, 4 figures, conference", "summary": "Recent advances in distributed learning systems have introduced effective\nsolutions for implementing collaborative artificial intelligence techniques in\nwireless communication networks. Federated learning approaches provide a\nmodel-aggregation mechanism among edge devices to achieve collaborative\ntraining, while ensuring data security, communication efficiency, and sharing\ncomputational overheads. On the other hand, limited transmission resources and\ncomplex communication environments remain significant bottlenecks to the\nefficient collaborations among edge devices, particularly within large-scale\nnetworks. To address such issues, this paper proposes an over-the-air (OTA)\nanalog aggregation method designed for the distributed swarm learning (DSL),\ntermed DSL-OTA, aiming to enhance communication efficiency, enable effective\ncooperation, and ensure privacy preserving. Incorporating multi-worker\nselection strategy with over-the-air aggregation not only makes the standard\nDSL based on single best worker contributing to global model update to become\nmore federated, but also secures the aggregation from potential risks of data\nleakage. Our theoretical analyses verify the advantages of the proposed DSL-OTA\nalgorithm in terms of fast convergence rate and low communication costs.\nSimulation results reveal that our DSL-OTA outperforms the other existing\nmethods by achieving better learning performance under both homogeneous and\nheterogeneous dataset settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5206\u5e03\u5f0f\u7fa4\u4f53\u5b66\u4e60\u7684\u7a7a\u4e2d\u6a21\u62df\u805a\u5408\u65b9\u6cd5DSL-OTA\uff0c\u901a\u8fc7\u591a\u5de5\u4f5c\u8005\u9009\u62e9\u548c\u7a7a\u4e2d\u805a\u5408\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\u3001\u5b9e\u73b0\u6709\u6548\u5408\u4f5c\u5e76\u786e\u4fdd\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u5b66\u4e60\u4e2d\u6709\u9650\u4f20\u8f93\u8d44\u6e90\u548c\u590d\u6742\u901a\u4fe1\u73af\u5883\u5bfc\u81f4\u7684\u8fb9\u7f18\u8bbe\u5907\u95f4\u534f\u4f5c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u7f51\u7edc\u4e2d\u3002", "method": "\u7ed3\u5408\u591a\u5de5\u4f5c\u8005\u9009\u62e9\u7b56\u7565\u4e0e\u7a7a\u4e2d\u6a21\u62df\u805a\u5408\uff0c\u4f7f\u57fa\u4e8e\u5355\u4e00\u6700\u4f73\u5de5\u4f5c\u8005\u7684\u6807\u51c6DSL\u53d8\u5f97\u66f4\u52a0\u8054\u90a6\u5316\uff0c\u540c\u65f6\u4fdd\u62a4\u805a\u5408\u8fc7\u7a0b\u514d\u53d7\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002", "result": "\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u4e86DSL-OTA\u5728\u5feb\u901f\u6536\u655b\u548c\u4f4e\u901a\u4fe1\u6210\u672c\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u4eff\u771f\u7ed3\u679c\u663e\u793a\u5728\u5f02\u6784\u548c\u540c\u6784\u6570\u636e\u96c6\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DSL-OTA\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5206\u5e03\u5f0f\u7fa4\u4f53\u5b66\u4e60\u7684\u901a\u4fe1\u6548\u7387\u548c\u534f\u4f5c\u6548\u679c\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2510.18300", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18300", "abs": "https://arxiv.org/abs/2510.18300", "authors": ["Ankur Lahiry", "Ayush Pokharel", "Banooqa Banday", "Seth Ockerman", "Amal Gueroudji", "Mohammad Zaeed", "Tanzima Z. Islam", "Line Pouchard"], "title": "A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces", "comment": null, "summary": "Large-scale GPU traces play a critical role in identifying performance\nbottlenecks within heterogeneous High-Performance Computing (HPC)\narchitectures. However, the sheer volume and complexity of a single trace of\ndata make performance analysis both computationally expensive and\ntime-consuming. To address this challenge, we present an end-to-end parallel\nperformance analysis framework designed to handle multiple large-scale GPU\ntraces efficiently. Our proposed framework partitions and processes trace data\nconcurrently and employs causal graph methods and parallel coordinating chart\nto expose performance variability and dependencies across execution flows.\nExperimental results demonstrate a 67% improvement in terms of scalability,\nhighlighting the effectiveness of our pipeline for analyzing multiple traces\nindependently.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5e76\u884c\u6027\u80fd\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u591a\u4e2a\u5927\u89c4\u6a21GPU\u8ddf\u8e2a\u6570\u636e\uff0c\u901a\u8fc7\u5e76\u884c\u5904\u7406\u548c\u56e0\u679c\u56fe\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5206\u6790\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21GPU\u8ddf\u8e2a\u6570\u636e\u5bf9\u4e8e\u8bc6\u522b\u5f02\u6784\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5355\u4e00\u8ddf\u8e2a\u6570\u636e\u7684\u5e9e\u5927\u4f53\u79ef\u548c\u590d\u6742\u6027\u4f7f\u5f97\u6027\u80fd\u5206\u6790\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u5e76\u884c\u6027\u80fd\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u53d1\u5206\u533a\u548c\u5904\u7406\u8ddf\u8e2a\u6570\u636e\uff0c\u91c7\u7528\u56e0\u679c\u56fe\u65b9\u6cd5\u548c\u5e76\u884c\u534f\u8c03\u56fe\u6765\u63ed\u793a\u6267\u884c\u6d41\u4e2d\u7684\u6027\u80fd\u53d8\u5f02\u6027\u548c\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b9e\u73b0\u4e8667%\u7684\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u8be5\u6d41\u6c34\u7ebf\u5728\u72ec\u7acb\u5206\u6790\u591a\u4e2a\u8ddf\u8e2a\u65f6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5e76\u884c\u6027\u80fd\u5206\u6790\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21GPU\u8ddf\u8e2a\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5206\u6790\u6548\u7387\uff0c\u4e3a\u5f02\u6784HPC\u67b6\u6784\u7684\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.18544", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18544", "abs": "https://arxiv.org/abs/2510.18544", "authors": ["Pan Zhou", "Yiming Lei", "Ling Liu", "Xiaoqiong Xu", "Ying Cai", "Daji Ergu", "Hongfang Yu", "Yueyue Dai"], "title": "SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices", "comment": null, "summary": "Large Language Models (LLMs), as the foundational architecture for\nnext-generation interactive AI applications, not only power intelligent\ndialogue systems but also drive the evolution of embodied intelligence on edge\ndevices, including humanoid robots, smart vehicles, and other scenarios. The\napplications running on these edge devices impose differentiated Service Level\nObjectives (SLO) requirements on LLM services, specifically manifested as\ndistinct constraints on Time to First Token (TTFT) and Time Per Output Token\n(TPOT) as well as end-to-end latency. Notably, edge devices typically handle\nreal-time tasks that are extremely sensitive to latency, such as machine\ncontrol and navigation planning. However, existing scheduling service systems\nstill prioritize maximizing output token throughput as the sole optimization\nobjective, failing to adequately address the diversity of SLO requirements.\nThis ultimately results in persistently high violation rates for end-to-end\nlatency or TPOT related SLOs.\n  This paper proposes SLICE, an innovative scheduling solution designed for\nedge computing scenarios with differentiated SLO requirements. By combining a\nutility-maximizing request scheduling algorithm with a dynamic iterative\ncontrol mechanism for generation rates, SLICE significantly improves LLM\ninference service SLO attainment. Experimental results demonstrate that\ncompared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to\n35x higher SLO attainment and 3.4x advantage in task completion time than the\nother two solutions.", "AI": {"tldr": "SLICE\u662f\u4e00\u79cd\u9488\u5bf9\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u7684LLM\u63a8\u7406\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u6548\u7528\u6700\u5927\u5316\u8bf7\u6c42\u8c03\u5ea6\u7b97\u6cd5\u548c\u52a8\u6001\u8fed\u4ee3\u751f\u6210\u901f\u7387\u63a7\u5236\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86SLO\u8fbe\u6210\u7387\u3002", "motivation": "\u73b0\u6709\u8c03\u5ea6\u7cfb\u7edf\u4ec5\u4ee5\u6700\u5927\u5316\u8f93\u51fatoken\u541e\u5410\u91cf\u4e3a\u4f18\u5316\u76ee\u6807\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8fb9\u7f18\u8bbe\u5907\u5bf9TTFT\u3001TPOT\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u7b49\u591a\u6837\u5316SLO\u8981\u6c42\uff0c\u5bfc\u81f4SLO\u8fdd\u89c4\u7387\u5c45\u9ad8\u4e0d\u4e0b\u3002", "method": "\u63d0\u51faSLICE\u8c03\u5ea6\u65b9\u6848\uff0c\u7ed3\u5408\u6548\u7528\u6700\u5927\u5316\u8bf7\u6c42\u8c03\u5ea6\u7b97\u6cd5\u548c\u52a8\u6001\u8fed\u4ee3\u751f\u6210\u901f\u7387\u63a7\u5236\u673a\u5236\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848Orca\u548cFastServe\uff0cSLICE\u5b9e\u73b0\u4e86\u9ad8\u8fbe35\u500d\u7684SLO\u8fbe\u6210\u7387\u63d0\u5347\u548c3.4\u500d\u7684\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u4f18\u52bf\u3002", "conclusion": "SLICE\u80fd\u6709\u6548\u89e3\u51b3\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e2dLLM\u63a8\u7406\u670d\u52a1\u7684\u591a\u6837\u5316SLO\u9700\u6c42\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u670d\u52a1\u8d28\u91cf\u3002"}}
{"id": "2510.18586", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.18586", "abs": "https://arxiv.org/abs/2510.18586", "authors": ["Zhuohang Bian", "Feiyang Wu", "Teng Ma", "Youwei Zhuo"], "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.", "AI": {"tldr": "Tokencake\u662f\u4e00\u4e2a\u9488\u5bf9\u591a\u667a\u80fd\u4f53\u5e94\u7528\u4e2dKV\u7f13\u5b58\u4f18\u5316\u7684\u670d\u52a1\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u8c03\u5ea6\u548c\u65f6\u95f4\u8c03\u5ea6\u89e3\u51b3\u7f13\u5b58\u4e89\u7528\u548c\u5185\u5b58\u95f2\u7f6e\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8GPU\u5185\u5b58\u5229\u7528\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u5916\u90e8\u51fd\u6570\u8c03\u7528\uff0c\u5bfc\u81f4KV\u7f13\u5b58\u9762\u4e34\u4e25\u91cd\u6027\u80fd\u6311\u6218\uff1a\u7a7a\u95f4\u4e89\u7528\u5bfc\u81f4\u5173\u952e\u667a\u80fd\u4f53\u7f13\u5b58\u88ab\u9a71\u9010\uff0c\u65f6\u95f4\u5229\u7528\u4e0d\u8db3\u5bfc\u81f4\u7b49\u5f85\u51fd\u6570\u8c03\u7528\u7684\u667a\u80fd\u4f53\u7f13\u5b58\u95f2\u7f6e\u5728GPU\u5185\u5b58\u4e2d\u3002", "method": "Tokencake\u91c7\u7528\u667a\u80fd\u4f53\u611f\u77e5\u8bbe\u8ba1\uff0c\u7a7a\u95f4\u8c03\u5ea6\u5668\u4f7f\u7528\u52a8\u6001\u5185\u5b58\u5206\u533a\u4fdd\u62a4\u5173\u952e\u667a\u80fd\u4f53\u514d\u53d7\u4e89\u7528\uff0c\u65f6\u95f4\u8c03\u5ea6\u5668\u91c7\u7528\u4e3b\u52a8\u5378\u8f7d\u548c\u9884\u6d4b\u6027\u4e0a\u4f20\u673a\u5236\uff0c\u5728\u51fd\u6570\u8c03\u7528\u505c\u6ede\u671f\u95f4\u91cd\u65b0\u5229\u7528GPU\u5185\u5b58\u3002", "result": "\u5728\u4ee3\u8868\u6027\u591a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTokencake\u76f8\u6bd4vLLM\u53ef\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e\u8d85\u8fc747.06%\uff0c\u6709\u6548GPU\u5185\u5b58\u5229\u7528\u7387\u63d0\u9ad8\u8fbe16.9%\u3002", "conclusion": "Tokencake\u901a\u8fc7\u534f\u540c\u4f18\u5316\u8c03\u5ea6\u548c\u5185\u5b58\u7ba1\u7406\u7684\u667a\u80fd\u4f53\u611f\u77e5\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u5e94\u7528\u4e2dKV\u7f13\u5b58\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2510.18592", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.18592", "abs": "https://arxiv.org/abs/2510.18592", "authors": ["Yuval Gil", "Merav Parter"], "title": "Distributed Interactive Proofs for Planarity with Log-Star Communication", "comment": "To appear in SODA 26", "summary": "We provide new communication-efficient distributed interactive proofs for\nplanarity. The notion of a \\emph{distributed interactive proof (DIP)} was\nintroduced by Kol, Oshman, and Saxena (PODC 2018). In a DIP, the \\emph{prover}\nis a single centralized entity whose goal is to prove a certain claim regarding\nan input graph $G$. To do so, the prover communicates with a distributed\n\\emph{verifier} that operates concurrently on all $n$ nodes of $G$. A DIP is\nmeasured by the amount of prover-verifier communication it requires. Namely,\nthe goal is to design a DIP with a small number of interaction rounds and a\nsmall \\emph{proof size}, i.e., a small amount of communication per round. Our\nmain result is an $O(\\log ^{*}n)$-round DIP protocol for embedded planarity and\nplanarity with a proof size of $O(1)$ and $O(\\lceil\\log \\Delta/\\log\n^{*}n\\rceil)$, respectively. In fact, this result can be generalized as\nfollows. For any $1\\leq r\\leq \\log^{*}n$, there exists an $O(r)$-round protocol\nfor embedded planarity and planarity with a proof size of $O(\\log ^{(r)}n)$ and\n$O(\\log ^{(r)}n+\\log \\Delta /r)$, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u65b0\u7684\u901a\u4fe1\u9ad8\u6548\u5206\u5e03\u5f0f\u4ea4\u4e92\u8bc1\u660e\u534f\u8bae\u7528\u4e8e\u5e73\u9762\u6027\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86O(log*n)\u8f6e\u4ea4\u4e92\u548cO(1)\u8bc1\u660e\u5927\u5c0f\u7684\u5d4c\u5165\u5f0f\u5e73\u9762\u6027\u8bc1\u660e\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5e73\u9762\u6027\u9a8c\u8bc1\u7684\u901a\u4fe1\u6548\u7387\u95ee\u9898\uff0c\u51cf\u5c11\u8bc1\u660e\u8005\u4e0e\u9a8c\u8bc1\u8005\u4e4b\u95f4\u7684\u901a\u4fe1\u5f00\u9500\u3002", "method": "\u8bbe\u8ba1\u5206\u5e03\u5f0f\u4ea4\u4e92\u8bc1\u660e\u534f\u8bae\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u548c\u4f18\u5316\u7684\u8bc1\u660e\u5927\u5c0f\u6765\u9a8c\u8bc1\u56fe\u7684\u5e73\u9762\u6027\u7279\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u5d4c\u5165\u5f0f\u5e73\u9762\u6027\u9a8c\u8bc1\u7684O(log*n)\u8f6e\u4ea4\u4e92\u548cO(1)\u8bc1\u660e\u5927\u5c0f\uff0c\u4ee5\u53ca\u4e00\u822c\u5e73\u9762\u6027\u9a8c\u8bc1\u7684O(\u2308log\u0394/log*n\u2309)\u8bc1\u660e\u5927\u5c0f\u3002", "conclusion": "\u8be5\u534f\u8bae\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5e03\u5f0f\u5e73\u9762\u6027\u9a8c\u8bc1\u7684\u901a\u4fe1\u6548\u7387\uff0c\u4e3a\u5206\u5e03\u5f0f\u56fe\u6027\u8d28\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.18640", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.18640", "abs": "https://arxiv.org/abs/2510.18640", "authors": ["Nils Japke", "Sebastian Koch", "Helmut Lukasczyk", "David Bermbach"], "title": "Towards an Optimized Benchmarking Platform for CI/CD Pipelines", "comment": "Published in 2025 IEEE International Conference on Cloud Engineering\n  (IC2E)", "summary": "Performance regressions in large-scale software systems can lead to\nsubstantial resource inefficiencies, making their early detection critical.\nFrequent benchmarking is essential for identifying these regressions and\nmaintaining service-level agreements (SLAs). Performance benchmarks, however,\nare resource-intensive and time-consuming, which is a major challenge for\nintegration into Continuous Integration / Continuous Deployment (CI/CD)\npipelines. Although numerous benchmark optimization techniques have been\nproposed to accelerate benchmark execution, there is currently no practical\nsystem that integrates these optimizations seamlessly into real-world CI/CD\npipelines. In this vision paper, we argue that the field of benchmark\noptimization remains under-explored in key areas that hinder its broader\nadoption. We identify three central challenges to enabling frequent and\nefficient benchmarking: (a) the composability of benchmark optimization\nstrategies, (b) automated evaluation of benchmarking results, and (c) the\nusability and complexity of applying these strategies as part of CI/CD systems\nin practice. We also introduce a conceptual cloud-based benchmarking framework\nhandling these challenges transparently. By presenting these open problems, we\naim to stimulate research toward making performance regression detection in\nCI/CD systems more practical and effective.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u4f18\u5316\u5728CI/CD\u7cfb\u7edf\u4e2d\u5e94\u7528\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u4f18\u5316\u7b56\u7565\u7ec4\u5408\u6027\u3001\u7ed3\u679c\u81ea\u52a8\u8bc4\u4f30\u3001\u5b9e\u9645\u5e94\u7528\u590d\u6742\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e91\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u6982\u5ff5\u3002", "motivation": "\u5927\u578b\u8f6f\u4ef6\u7cfb\u7edf\u7684\u6027\u80fd\u56de\u5f52\u4f1a\u5bfc\u81f4\u8d44\u6e90\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u9891\u7e41\u57fa\u51c6\u6d4b\u8bd5\u6765\u68c0\u6d4b\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u8d44\u6e90\u6d88\u8017\u5927\u3001\u8017\u65f6\u957f\uff0c\u96be\u4ee5\u96c6\u6210\u5230CI/CD\u6d41\u6c34\u7ebf\u4e2d\u3002", "method": "\u8bc6\u522b\u4e86\u57fa\u51c6\u6d4b\u8bd5\u4f18\u5316\u7684\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u4f18\u5316\u7b56\u7565\u7684\u7ec4\u5408\u6027\u3001\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u7684\u81ea\u52a8\u8bc4\u4f30\u3001\u5728CI/CD\u7cfb\u7edf\u4e2d\u5e94\u7528\u7b56\u7565\u7684\u53ef\u7528\u6027\u548c\u590d\u6742\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e91\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u6982\u5ff5\u3002", "result": "\u63d0\u51fa\u4e86\u57fa\u51c6\u6d4b\u8bd5\u4f18\u5316\u9886\u57df\u7684\u5173\u952e\u5f00\u653e\u95ee\u9898\uff0c\u65e8\u5728\u63a8\u52a8\u7814\u7a76\u4f7fCI/CD\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\u56de\u5f52\u68c0\u6d4b\u66f4\u52a0\u5b9e\u7528\u548c\u6709\u6548\u3002", "conclusion": "\u57fa\u51c6\u6d4b\u8bd5\u4f18\u5316\u9886\u57df\u5728\u5173\u952e\u65b9\u9762\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u963b\u788d\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u89e3\u51b3\u7ec4\u5408\u6027\u3001\u81ea\u52a8\u8bc4\u4f30\u548c\u5b9e\u9645\u5e94\u7528\u590d\u6742\u6027\u7b49\u6311\u6218\u3002"}}
{"id": "2510.18838", "categories": ["cs.DC", "physics.comp-ph", "physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2510.18838", "abs": "https://arxiv.org/abs/2510.18838", "authors": ["Jacob S. Merson", "Cameron W. Smith", "Mark S. Shephard", "Fuad Hasan", "Abhiyan Paudel", "Angel Castillo-Crooke", "Joyal Mathew", "Mohammad Elahi"], "title": "PCMS: Parallel Coupler For Multimodel Simulations", "comment": null, "summary": "This paper presents the Parallel Coupler for Multimodel Simulations (PCMS), a\nnew GPU accelerated generalized coupling framework for coupling simulation\ncodes on leadership class supercomputers. PCMS includes distributed control and\nfield mapping methods for up to five dimensions. For field mapping PCMS can\nutilize discretization and field information to accommodate physics\nconstraints. PCMS is demonstrated with a coupling of the gyrokinetic\nmicroturbulence code XGC with a Monte Carlo neutral transport code DEGAS2 and\nwith a 5D distribution function coupling of an energetic particle transport\ncode (GNET) to a gyrokinetic microturbulence code (GTC). Weak scaling is also\ndemonstrated on up to 2,080 GPUs of Frontier with a weak scaling efficiency of\n85%.", "AI": {"tldr": "PCMS\u662f\u4e00\u4e2a\u65b0\u7684GPU\u52a0\u901f\u901a\u7528\u8026\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u8026\u5408\u6a21\u62df\u4ee3\u7801\uff0c\u652f\u6301\u9ad8\u8fbe\u4e94\u7ef4\u7684\u5206\u5e03\u5f0f\u63a7\u5236\u548c\u573a\u6620\u5c04\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5728\u9886\u5bfc\u7ea7\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u9ad8\u6548\u8026\u5408\u591a\u4e2a\u6a21\u62df\u4ee3\u7801\u7684\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u590d\u6742\u7684\u591a\u7269\u7406\u573a\u6a21\u62df\u3002", "method": "PCMS\u91c7\u7528GPU\u52a0\u901f\u7684\u5206\u5e03\u5f0f\u63a7\u5236\u548c\u573a\u6620\u5c04\u65b9\u6cd5\uff0c\u5229\u7528\u79bb\u6563\u5316\u548c\u573a\u4fe1\u606f\u6765\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\uff0c\u652f\u6301\u9ad8\u8fbe\u4e94\u7ef4\u7684\u8026\u5408\u3002", "result": "\u6210\u529f\u6f14\u793a\u4e86XGC\u4e0eDEGAS2\u7684\u8026\u5408\uff0c\u4ee5\u53caGNET\u4e0eGTC\u76845D\u5206\u5e03\u51fd\u6570\u8026\u5408\uff0c\u5728Frontier\u76842,080\u4e2aGPU\u4e0a\u5b9e\u73b0\u4e8685%\u7684\u5f31\u6269\u5c55\u6548\u7387\u3002", "conclusion": "PCMS\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u6a21\u578b\u8026\u5408\u6846\u67b6\uff0c\u80fd\u591f\u5728\u5927\u578bGPU\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u590d\u6742\u6a21\u62df\u4ee3\u7801\u8026\u5408\u3002"}}
