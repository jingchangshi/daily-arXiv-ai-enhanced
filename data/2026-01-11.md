<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Scalable Floating-Point Satisfiability via Staged Optimization](https://arxiv.org/abs/2601.04492)
*Yuanzhuo Zhang,Zhoulai Fu,Binoy Ravindran*

Main category: cs.PL

TL;DR: StageSAT：一种新的浮点可满足性求解方法，通过三阶段优化将SMT求解与数值优化结合，无需位级推理，在性能和正确性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有浮点可满足性求解方法存在性能瓶颈和可靠性问题，需要结合SMT求解的精确性和数值优化的高效性，同时避免复杂的位级推理。

Method: 采用三阶段优化方法：1）快速投影辅助下降引导搜索到可行区域；2）位级精度的ULP²优化；3）n-ULP格点精炼。通过正交投影引入部分单调下降特性，将复杂算术视为黑盒。

Result: 在SMT-COMP'25基准测试中，StageSAT比现有优化方法更具可扩展性和准确性，解决了更多公式，在可满足案例中达到99.4%召回率且0%误报，速度比传统位精确SMT和数值求解器快5-10倍。

Conclusion: 分阶段优化显著提高了浮点可满足性求解的性能和正确性，为浮点约束求解提供了可靠高效的解决方案。

Abstract: This work introduces StageSAT, a new approach to solving floating-point satisfiability that bridges SMT solving with numerical optimization. StageSAT reframes a floating-point formula as a series of optimization problems in three stages of increasing precision. It begins with a fast, projection-aided descent objective to guide the search toward a feasible region, proceeding to bit-level accuracy with ULP$^2$ optimization and a final $n$-ULP lattice refinement.
  By construction, the final stage uses a representing function that is zero if and only if a candidate satisfies all constraints. Thus, when optimization drives the objective to zero, the resulting assignment is a valid solution, providing a built-in guarantee of soundness.
  To improve search, StageSAT introduces a partial monotone descent property on linear constraints via orthogonal projection, preventing the optimizer from stalling on flat or misleading landscapes. Critically, this solver requires no heavy bit-level reasoning or specialized abstractions; it treats complex arithmetic as a black-box, using runtime evaluations to navigate the input space.
  We implement StageSAT and evaluate it on extensive benchmarks, including SMT-COMP'25 suites and difficult cases from prior work. StageSAT proved more scalable and accurate than state-of-the-art optimization-based alternatives. It solved strictly more formulas than any competing solver under the same time budget, finding most satisfiable instances without producing spurious models. This amounts to 99.4% recall on satisfiable cases with 0% false SAT, exceeding the reliability of prior optimization-based solvers. StageSAT also delivered significant speedups (often 5--10$\times$) over traditional bit-precise SMT and numeric solvers. These results demonstrate that staged optimization significantly improves performance and correctness of floating-point satisfiability solving.

</details>


### [2] [Lenses for Partially-Specified States (Extended Version)](https://arxiv.org/abs/2601.04573)
*Kazutaka Matsuda,Minh Nguyen,Meng Wang*

Main category: cs.PL

TL;DR: 提出部分状态透镜，通过部分指定源和视图状态来精确表示用户更新意图，支持多视图更新合并，并确保更新保持性。


<details>
  <summary>Details</summary>
Motivation: 在双向转换中，当多个视图共享一个源时，一个视图的更新会影响其他视图，难以在保持用户更新的同时维持对应关系，特别是在多个视图同时更改时。在组合框架中确保这些属性更具挑战性。

Method: 提出部分状态透镜，允许部分指定源和视图状态来精确表示用户更新意图。这些意图采用偏序关系，为合并来自多个视图的更新意图提供清晰语义，并定义与这种合并兼容的更新保持性概念。

Result: 形式化部分状态透镜，以及支持组合推理并确保更新保持性的部分指定感知良好行为性。通过示例展示了所提出系统的实用性。

Conclusion: 部分状态透镜为解决多视图双向转换中的更新合并和保持性问题提供了有效的形式化框架，支持组合推理并确保更新语义的精确性。

Abstract: A bidirectional transformation is a pair of transformations satisfying certain well-behavedness properties: one maps source data into view data, and the other translates changes on the view back to the source. However, when multiple views share a source, an update on one view may affect the others, making it hard to maintain correspondence while preserving the user's update, especially when multiple views are changed at once. Ensuring these properties within a compositional framework is even more challenging. In this paper, we propose partial-state lenses, which allow source and view states to be partially specified to precisely represent the user's update intentions. These intentions are partially ordered, providing clear semantics for merging intentions of updates coming from multiple views and a refined notion of update preservation compatible with this merging. We formalize partial-state lenses, together with partial-specifiedness-aware well-behavedness that supports compositional reasoning and ensures update preservation. In addition, we demonstrate the utility of the proposed system through examples.

</details>


### [3] [The Squirrel Parser: A Linear-Time PEG Packrat Parser Capable of Left Recursion and Optimal Error Recovery](https://arxiv.org/abs/2601.05012)
*Luke A. D. Hutchison*

Main category: cs.PL

TL;DR: Squirrel parser：一个PEG packrat解析器，能直接处理所有形式的左递归并实现最优错误恢复，同时保持线性时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统方法处理左递归需要语法重写或复杂算法扩展，现有解析器在处理左递归和错误恢复方面存在局限性。

Method: 基于第一原理推导最小算法：通过每位置状态跟踪进行循环检测，后代到祖先递归帧的O(1)通信，以及通过迭代扩展进行定点搜索。错误恢复方面，推导了四个公理和十二个约束，使用约束满足机制搜索所有可能性空间。

Result: 实现了能直接处理所有形式左递归的解析器，具有最优错误恢复能力，即使在任意数量错误存在时也能保持线性时间复杂度。

Conclusion: Squirrel解析器提供了一个理论上最优且鲁棒的错误恢复策略，在保持完美性能线性的同时，实现了完整的左递归处理和直观的行为。

Abstract: We present the squirrel parser, a PEG packrat parser that directly handles all forms of left recursion with optimal error recovery, while maintaining linear time complexity in the length of the input even in the presence of an arbitrary number of errors. Traditional approaches to handling left recursion in a recursive descent parser require grammar rewriting or complex algorithmic extensions. We derive a minimal algorithm from first principles: cycle detection via per-position state tracking and $O(1)$-per-LR-cycle communication from descendant to ancestor recursion frames, and fixed-point search via iterative expansion. For error recovery, we derived a set of four axioms and twelve constraints that must be imposed upon an optimal error recovery design to ensure completeness, correctness, optimality of performance, and intuitiveness of behavior. We utilized a constraint satisfaction mechanism to search the space of all possibilities, arriving at a provably optimal and robust error recovery strategy that maintains perfect performance linearity.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [ParaCodex: A Profiling-Guided Autonomous Coding Agent for Reliable Parallel Code Generation and Translation](https://arxiv.org/abs/2601.04327)
*Erel Kaplan,Tomer Bitan,Lian Ghrayeb,Le Chen,Tom Yotam,Niranjan Hasabnis,Gal Oren*

Main category: cs.DC

TL;DR: ParaCodex是一个基于Codex的自主编码代理系统，专门用于将串行CPU内核转换为OpenMP GPU卸载内核，通过分阶段热点分析、显式数据规划、正确性门控和性能分析引导的优化，在多个基准测试套件上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 并行编程对于高性能计算和人工智能至关重要，但编写正确且高效的代码仍然具有挑战性，特别是在OpenMP GPU卸载方面，数据移动和调优是主要难点。现有的自主编码代理虽然能在目标硬件上编译、测试和分析，但缺乏领域特定的支撑结构，输出结果脆弱。

Method: ParaCodex采用HPC工程师工作流程，将基于Codex的代理转变为自主的OpenMP GPU卸载系统。方法包括：分阶段热点分析识别关键代码区域、显式数据规划优化数据移动、正确性门控确保代码正确性、性能分析引导的细化优化。系统评估了从串行CPU内核到OpenMP GPU卸载内核的转换，以及CUDA到OpenMP卸载的转换。

Result: 在HeCBench、Rodinia和NAS基准测试套件上，ParaCodex成功处理了31个有效内核中的31个（排除了5个内核）。生成的GPU内核在25/31的情况下比参考OpenMP实现更快，在HeCBench上实现了3倍的几何平均加速比，在Rodinia上实现了5倍的几何平均加速比，在所有测试套件上都优于零样本Codex基线。在ParEval的CUDA到OpenMP卸载转换评估中，ParaCodex在纯代码和端到端设置中都保持了高编译和验证率。

Conclusion: ParaCodex通过结合领域特定的工程工作流程和基于Codex的自主编码代理，成功解决了OpenMP GPU卸载编程的挑战，实现了从串行CPU代码到高效GPU卸载代码的自动化转换，显著提升了性能，为高性能计算和AI领域的并行编程提供了有效的自动化解决方案。

Abstract: Parallel programming is central to HPC and AI, but producing code that is correct and fast remains challenging, especially for OpenMP GPU offload, where data movement and tuning dominate. Autonomous coding agents can compile, test, and profile on target hardware, but outputs are brittle without domain scaffolding.
  We present ParaCodex, an HPC-engineer workflow that turns a Codex-based agent into an autonomous OpenMP GPU offload system using staged hotspot analysis, explicit data planning, correctness gating, and profiling-guided refinement. We evaluate translation from serial CPU kernels to OpenMP GPU offload kernels on HeCBench, Rodinia, and NAS. After excluding five kernels, ParaCodex succeeded on all 31 valid kernels. The generated kernels improved GPU time over reference OpenMP implementations in 25/31 cases, achieving geometric-mean speedups of 3x on HeCBench and 5x on Rodinia, and outperforming a zero-shot Codex baseline on all suites. We also evaluate CUDA to OpenMP offload translation on ParEval, where ParaCodex maintains high compilation and validation rates in code-only and end-to-end settings.

</details>


### [5] [Hybrid Cloud Architectures for Research Computing: Applications and Use Cases](https://arxiv.org/abs/2601.04349)
*Xaver Stiensmeier,Alexander Kanitz,Jan Krüger,Santiago Insua,Adrián Rošinec,Viktória Spišáková,Lukáš Hejtmánek,David Yuan,Gavin Farrell,Jonathan Tedds,Juha Törnroos,Harald Wagener,Alex Sczyrba,Nils Hoffmann,Matej Antol*

Main category: cs.DC

TL;DR: 该论文综述了混合云在科研计算中的部署模型，重点分析了网格与云平台（OpenPBS、SLURM、OpenStack、Kubernetes）和工作流管理工具（Nextflow、Snakemake、CWL），探讨了联邦计算、多云编排和负载调度策略，并基于生命科学领域的实施案例提出了混合云在科研中加速采用的路线图。


<details>
  <summary>Details</summary>
Motivation: 随着科研对IT基础设施依赖的加深，研究人员面临分散的计算环境生态系统，需要在性能、成本、可扩展性和可访问性之间取得平衡。混合云架构通过整合多种计算环境，为解决这一挑战提供了灵活、高效且能访问专用硬件的解决方案。

Method: 论文采用综述分析方法，全面概述混合云部署模型，重点研究网格和云平台（OpenPBS、SLURM、OpenStack、Kubernetes）以及工作流管理工具（Nextflow、Snakemake、CWL）。探讨联邦计算、多云编排和负载调度策略，并基于ELIXIR计算平台在生命科学领域的实施案例进行分析。

Result: 论文识别了混合云部署中的关键挑战，包括互操作性、数据安全、可重复性和网络性能等问题。通过分析ELIXIR计算平台在EOSC更广泛背景下的集成案例，提出了解决这些挑战的技术方案和策略。

Conclusion: 论文提出了加速科研计算中混合云采用的路线图，强调治理框架和技术解决方案对于推动可持续和可扩展基础设施发展的重要性。混合云架构能够有效整合研究基础设施、NREN和商业云提供商的服务，为科学计算提供更灵活高效的解决方案。

Abstract: Scientific research increasingly depends on robust and scalable IT infrastructures to support complex computational workflows. With the proliferation of services provided by research infrastructures, NRENs, and commercial cloud providers, researchers must navigate a fragmented ecosystem of computing environments, balancing performance, cost, scalability, and accessibility. Hybrid cloud architectures offer a compelling solution by integrating multiple computing environments to enhance flexibility, resource efficiency, and access to specialised hardware.
  This paper provides a comprehensive overview of hybrid cloud deployment models, focusing on grid and cloud platforms (OpenPBS, SLURM, OpenStack, Kubernetes) and workflow management tools (Nextflow, Snakemake, CWL). We explore strategies for federated computing, multi-cloud orchestration, and workload scheduling, addressing key challenges such as interoperability, data security, reproducibility, and network performance. Drawing on implementations from life sciences, as coordinated by the ELIXIR Compute Platform and their integration into a wider EOSC context, we propose a roadmap for accelerating hybrid cloud adoption in research computing, emphasising governance frameworks and technical solutions that can drive sustainable and scalable infrastructure development.

</details>


### [6] [Sharded Elimination and Combining for Highly-Efficient Concurrent Stacks](https://arxiv.org/abs/2601.04523)
*Ajay Singh,Nikos Metaxakis,Panagiota Fatourou*

Main category: cs.DC

TL;DR: 提出一种新的阻塞线性化堆栈实现，通过分片和fetch&increment技术，性能比现有并发堆栈提升高达2倍


<details>
  <summary>Details</summary>
Motivation: 现有并发堆栈在大量线程和高竞争场景下性能受限，需要一种能提供更好并行性和更低竞争的新实现

Method: 采用分片技术、fetch&increment操作，结合新颖的消除机制和组合方法，有效混合这些技术以获得高性能

Result: 实验显示该实现比所有现有并发堆栈性能提升高达2倍，在支持大量线程的系统和高竞争场景中特别高效

Conclusion: 提出的堆栈实现通过创新的消除机制和组合方法，显著提升了并发堆栈的性能，特别是在高并发环境下

Abstract: We present a new blocking linearizable stack implementation which utilizes sharding and fetch&increment to achieve significantly better performance than all existing concurrent stacks. The proposed implementation is based on a novel elimination mechanism and a new combining approach that are efficiently blended to gain high performance. Our implementation results in enhanced parallelism and low contention when accessing the shared stack. Experiments show that the proposed stack implementation outperforms all existing concurrent stacks by up to 2X in most workloads. It is particularly efficient in systems supporting a large number of threads and in high contention scenarios.

</details>


### [7] [Quantifying Autoscaler Vulnerabilities: An Empirical Study of Resource Misallocation Induced by Cloud Infrastructure Faults](https://arxiv.org/abs/2601.04659)
*Gijun Park*

Main category: cs.DC

TL;DR: 研究通过模拟实验分析四种常见基础设施故障对云资源自动扩缩容的影响，发现存储故障导致最高成本开销，路由故障导致资源分配不足，水平扩缩对瞬态异常更敏感。


<details>
  <summary>Details</summary>
Motivation: 云环境中的资源自动扩缩机制依赖准确的性能指标来做出最优资源配置决策。当基础设施故障（包括硬件故障、网络中断和软件异常）损坏这些指标时，自动扩缩器可能会系统性地过度或不足配置资源，导致运营成本增加或服务可靠性下降。

Method: 通过受控模拟实验，测量四种常见故障类别对垂直和水平自动扩缩行为的影响，涵盖多种实例配置和服务水平目标（SLO）阈值。

Result: 实验结果显示：存储相关故障产生最大的成本开销，在水平扩缩策略下每月增加高达258美元；路由异常持续导致自动扩缩器偏向资源分配不足；不同扩缩策略对故障引起的指标扭曲的敏感性差异显著，水平扩缩对瞬态异常更敏感，特别是在阈值边界附近。

Conclusion: 这些基于实证的见解为设计容错的自动扩缩策略提供了可行建议，能够区分真实的工作负载波动和故障伪影。

Abstract: Resource autoscaling mechanisms in cloud environments depend on accurate performance metrics to make optimal provisioning decisions. When infrastructure faults including hardware malfunctions, network disruptions, and software anomalies corrupt these metrics, autoscalers may systematically over- or under-provision resources, resulting in elevated operational expenses or degraded service reliability. This paper conducts controlled simulation experiments to measure how four prevalent fault categories affect both vertical and horizontal autoscaling behaviors across multiple instance configurations and service level objective (SLO) thresholds. Experimental findings demonstrate that storage-related faults generate the largest cost overhead, adding up to $258 monthly under horizontal scaling policies, whereas routing anomalies consistently bias autoscalers toward insufficient resource allocation. The sensitivity to fault-induced metric distortions differs markedly between scaling strategies: horizontal autoscaling exhibits greater susceptibility to transient anomalies, particularly near threshold boundaries. These empirically-grounded insights offer actionable recommendations for designing fault-tolerant autoscaling policies that distinguish genuine workload fluctuations from failure artifacts.

</details>


### [8] [Cognitive Infrastructure: A Unified DCIM Framework for AI Data Centers](https://arxiv.org/abs/2601.04750)
*Krishna Chaitanya Sunkara*

Main category: cs.DC

TL;DR: DCIM 3.0是一个集成了语义推理、预测分析、自主编排和统一连接的下一代AI数据中心管理统一框架，通过知识图谱、热建模和统一设备连接协议解决基础设施自动化、可持续性和数字孪生设计挑战。


<details>
  <summary>Details</summary>
Motivation: 解决下一代AI数据中心在基础设施自动化、可持续性和数字孪生设计方面的关键挑战，传统数据中心管理方法难以应对GPU计算等新兴需求。

Method: 提出DCIM 3.0统一框架，集成四个核心组件：基于知识图谱的语义推理、预测分析、自主编排引擎和统一设备连接协议(UDCP)，结合热建模和数字孪生技术。

Result: 开发了一个能够处理AI数据中心复杂管理需求的综合框架，实现了基础设施的智能自动化管理、热管理优化和可持续性提升。

Conclusion: DCIM 3.0为下一代AI数据中心提供了一个全面的管理解决方案，通过统一框架解决了自动化、可持续性和数字孪生设计的关键问题，为高效、智能的数据中心运营奠定了基础。

Abstract: This work presents DCIM 3.0, a unified framework integrating semantic reasoning, predictive analytics, autonomous orchestration, and unified connectivity for next-generation AI data center management. The framework addresses critical challenges in infrastructure automation, sustainability, and digital-twin design through knowledge graph-based intelligence, thermal modeling, and the Unified Device Connectivity Protocol (UDCP).Keywords-Data Center Infrastructure Management, DCIM, AI Data Centers, Knowledge Graphs, Digital Twin, Thermal Management, Infrastructure Automation, Sustainability, GPU Computing, Data Center

</details>


### [9] [Proof of Commitment: A Human-Centric Resource for Permissionless Consensus](https://arxiv.org/abs/2601.04813)
*Homayoun Maleki,Nekane Sainz,Jon Legarda*

Main category: cs.DC

TL;DR: PoCmt是一种基于人类实时参与（非并行资源）的新型共识原语，通过人类挑战预言机强制执行身份绑定、时间敏感挑战，使维持多个活跃身份需要成比例的人类时间成本，从而在部分同步网络中实现安全、活性和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有基于并行资源（如算力或资本）的共识协议存在根本缺陷：一旦获得这些资源，可以以近乎零边际成本分配给多个身份，无法实现线性女巫攻击成本。需要一种基于非并行资源的新共识范式。

Method: 提出Proof of Commitment (PoCmt)共识原语，基于人类实时参与这一非并行资源。验证者维护包含累计人类努力、协议参与和在线可用性的承诺状态。通过人类挑战预言机强制执行身份绑定、时间敏感挑战，限制每个时间窗口内可解决的挑战数量。

Result: 理论分析显示：基于并行资源的协议允许零边际女巫成本，而PoCmt强制执行严格线性成本曲线。加权骨干分析证明PoCmt在部分同步网络中实现安全、活性和承诺比例公平性。仿真验证了人类时间容量作为唯一对抗瓶颈，以及承诺漂移和公平性特性。

Conclusion: PoCmt为共识设计空间提供了新方向，将无许可安全性建立在持续的人类努力而非计算或资本上，解决了现有范式中的女巫攻击成本问题。

Abstract: Permissionless consensus protocols require a scarce resource to regulate leader election and provide Sybil resistance. Existing paradigms such as Proof of Work and Proof of Stake instantiate this scarcity through parallelizable resources like computation or capital. Once acquired, these resources can be subdivided across many identities at negligible marginal cost, making linear Sybil cost fundamentally unattainable.
  We introduce Proof of Commitment (PoCmt), a consensus primitive grounded in a non-parallelizable resource: real-time human engagement. Validators maintain a commitment state capturing cumulative human effort, protocol participation, and online availability. Engagement is enforced through a Human Challenge Oracle that issues identity-bound, time-sensitive challenges, limiting the number of challenges solvable within each human window.
  Under this model, sustaining multiple active identities requires proportional human-time effort. We establish a cost-theoretic separation showing that protocols based on parallelizable resources admit zero marginal Sybil cost, whereas PoCmt enforces a strictly linear cost profile. Using a weighted-backbone analysis, we show that PoCmt achieves safety, liveness, and commitment-proportional fairness under partial synchrony.
  Simulations complement the analysis by isolating human-time capacity as the sole adversarial bottleneck and validating the predicted commitment drift and fairness properties. These results position PoCmt as a new point in the consensus design space, grounding permissionless security in sustained human effort rather than computation or capital.

</details>


### [10] [Parallel Quadratic Selected Inversion in Quantum Transport Simulation](https://arxiv.org/abs/2601.04904)
*Vincent Maillou,Matthias Bollhofer,Olaf Schenk,Alexandros Nikolaos Ziogas,Mathieu Luisier*

Main category: cs.DC

TL;DR: 提出分布式并行方法加速纳米器件量子输运模拟中的选择求逆和二次矩阵方程求解，相比现有方法在16个GPU上实现5.2倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着晶体管尺寸缩小到纳米尺度，需要精确的量子输运模拟。非平衡格林函数(NEGF)方法是理想选择，但计算量大，涉及矩阵选择求逆和二次矩阵方程求解。现有算法如递归格林函数(RGF)适合GPU加速，但通常是顺序执行、需要块三对角矩阵输入，且仅限于共享内存并行，限制了可模拟的器件尺寸。

Method: 基于RGF技术开发分布式并行方法，实现并行选择求逆和二次矩阵方程求解。扩展方法处理带箭头结构的块三对角矩阵，以支持多端晶体管结构研究。

Result: 在纳米带晶体管量子输运模拟的真实数据集上评估性能，与稀疏直接求解器PARDISO比较。在16个GPU上，融合的选择求逆和二次矩阵方程求解器比应用于16倍短器件的PARDISO选择求逆模块快5.2倍。

Conclusion: 该方法显著加速了基于NEGF的纳米器件模拟，展示了分布式并行方法在量子输运计算中的潜力，能够处理更大规模的器件模拟问题。

Abstract: Driven by Moore's Law, the dimensions of transistors have been pushed down to the nanometer scale. Advanced quantum transport (QT) solvers are required to accurately simulate such nano-devices. The non-equilibrium Green's function (NEGF) formalism lends itself optimally to these tasks, but it is computationally very intensive, involving the selected inversion (SI) of matrices and the selected solution of quadratic matrix (SQ) equations. Existing algorithms to tackle these numerical problems are ideally suited to GPU acceleration, e.g., the so-called recursive Green's function (RGF) technique, but they are typically sequential, require block-tridiagonal (BT) matrices as inputs, and their implementation has been so far restricted to shared memory parallelism, thus limiting the achievable device sizes. To address these shortcomings, we introduce distributed methods that build on RGF and enable parallel selected inversion and selected solution of the quadratic matrix equation. We further extend them to handle BT matrices with arrowhead, which allows for the investigation of multi-terminal transistor structures. We evaluate the performance of our approach on a real dataset from the QT simulation of a nano-ribbon transistor and compare it with the sparse direct package PARDISO. When scaling to 16 GPUs, our fused SI and SQ solver is 5.2x faster than the SI module of PARDISO applied to a device 16x shorter. These results highlight the potential of our method to accelerate NEGF-based nano-device simulations.

</details>


### [11] [Asynchronous Secure Federated Learning with Byzantine aggregators](https://arxiv.org/abs/2601.04930)
*Antonella Del Pozzo,Achille Desreumaux,Mathieu Gestin,Alexandre Rapetti,Sara Tucci-Piergiovanni*

Main category: cs.DC

TL;DR: 提出一种针对异步通信环境下恶意聚合服务器的联邦平均隐私保护方案，结合安全聚合和差分隐私，通过复制聚合器容忍拜占庭故障，无需聚合器间共识


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习隐私保护方案主要针对同步通信和可信聚合器，但在异步通信环境中，恶意聚合器可能操纵模型泄露客户端数据或停止计算，且快速客户端可能更频繁参与导致隐私泄露和训练偏差

Method: 1) 复制聚合器以容忍部分被腐蚀；2) 客户端使用掩码和高斯噪声保护模型更新；3) 通过复制服务器解掩模型，确保训练活性；4) 引入包含机制保证客户端均匀参与和隐私预算平衡；5) 无需聚合器间共识，避免异步环境下的共识不可能性

Result: 方案在所有指标上保持与现有最优方案相同的性能，同时提供对恶意聚合器的容忍、异步通信环境下的隐私保护，并避免共识瓶颈提高可用性

Conclusion: 该方案首次在异步通信环境中解决了恶意聚合器威胁下的联邦平均隐私保护问题，通过复制聚合器、安全聚合和差分隐私的组合，在无需共识的情况下实现了鲁棒的隐私保护联邦学习

Abstract: Privacy-preserving federated averaging is a central approach for protecting client privacy in federated learning. In this paper, we study this problem in an asynchronous communications setting with malicious aggregators. We propose a new solution to provide federated averaging in this model while protecting the client's data privacy through secure aggregation and differential privacy. Our solution maintains the same performance as the state of the art across all metrics. The main contributions of this paper are threefold. First, unlike existing single- or multi-server solutions, we consider malicious aggregation servers that may manipulate the model to leak clients' data or halt computation. To tolerate this threat, we replicate the aggregators, allowing a fraction of them to be corrupted. Second, we propose a new privacy preservation protocol for protocols in asynchronous communication models with Byzantine aggregators. In this protocol, clients mask their values and add Gaussian noise to their models. In contrast with previous works, we use the replicated servers to unmask the models, while ensuring the liveness of training even if aggregators misbehave. Third, the asynchronous communication model introduces new challenges not present in existing approaches. In such a setting, faster clients may contribute more frequently, potentially reducing their privacy and biasing the training. To address this, we introduce an inclusion mechanism that ensures uniform client participation and balanced privacy budgets. Interestingly, the solution presented in this paper does not rely on agreement between aggregators. Thus, we circumvent the known impossibility of consensus in asynchronous settings where processes might crash. Additionally, this feature increases availability, as a consensus-based algorithm only progresses in periods of low latency.

</details>


### [12] [Nalar: An agent serving framework](https://arxiv.org/abs/2601.05109)
*Marco Laju,Donghyun Son,Saurabh Agarwal,Nitin Kedia,Myungjin Lee,Jayanth Srinivasa,Aditya Akella*

Main category: cs.DC

TL;DR: Nalar是一个从头构建的智能体服务框架，通过分离工作流规范与执行、提供运行时可见性和控制，实现异构智能体应用的高效、可扩展服务。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的智能体应用在自动化复杂多步骤任务时面临挑战：异构组件、动态模型驱动的控制流、长运行状态和不可预测延迟，需要高效的服务框架。

Method: 1) 使用轻量级自动生成的存根将智能体和工具调用转换为携带依赖和上下文元数据的futures；2) 管理状态层解耦逻辑状态与物理放置；3) 两级控制架构结合全局策略计算与本地事件驱动执行。

Result: 在三个智能体工作负载上：尾部延迟降低34-74%，速度提升最高2.9倍，在基线失败时维持80 RPS，扩展到13万个futures且控制开销低于500ms。

Conclusion: Nalar能够在不增加开发者编排逻辑负担的情况下，提供可扩展、高效且策略驱动的异构智能体应用服务。

Abstract: LLM-driven agentic applications increasingly automate complex, multi-step tasks, but serving them efficiently remains challenging due to heterogeneous components, dynamic and model-driven control flow, long-running state, and unpredictable latencies. Nalar is a ground-up agent-serving framework that cleanly separates workflow specification from execution while providing the runtime visibility and control needed for robust performance. Nalar preserves full Python expressiveness, using lightweight auto-generated stubs that turn agent and tool invocations into futures carrying dependency and context metadata. A managed state layer decouples logical state from physical placement, enabling safe reuse, migration, and consistent retry behavior. A two-level control architecture combines global policy computation with local event-driven enforcement to support adaptive routing, scheduling, and resource management across evolving workflows. Together, these mechanisms allow Nalar to deliver scalable, efficient, and policy-driven serving of heterogeneous agentic applications without burdening developers with orchestration logic. Across three agentic workloads, Nalar cuts tail latency by 34--74\%, achieves up to $2.9\times$ speedups, sustains 80 RPS where baselines fail, and scales to 130K futures with sub-500 ms control overhead.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [Memory-Guided Unified Hardware Accelerator for Mixed-Precision Scientific Computing](https://arxiv.org/abs/2601.04476)
*Chuanzhen Wang,Leo Zhang,Eric Liu*

Main category: cs.AR

TL;DR: 提出Memory-Guided Unified Hardware Accelerator框架，通过内存引导的精度选择、经验驱动的位宽管理和课程学习自动稀疏模式发现，统一加速有限元方法、脉冲神经网络和稀疏张量计算，在单一平台上实现混合精度科学计算。


<details>
  <summary>Details</summary>
Motivation: 现有专用加速器存在根本性限制：有限元方法缺乏全面的舍入误差分析且使用固定精度策略；脉冲神经网络加速器无法处理非脉冲操作且位宽随网络深度增加而扩大；FPGA张量加速器仅优化密集计算且需要手动配置稀疏模式。这些限制阻碍了混合工作负载在统一平台上的高效处理。

Method: 提出Memory-Guided Unified Hardware Accelerator框架，包含三个增强模块：1) 内存引导的精度选择，克服固定精度限制；2) 经验驱动的位宽管理和动态并行性适应，增强脉冲神经网络加速；3) 课程学习自动稀疏模式发现。该框架在统一平台上集成处理有限元方法、脉冲神经网络和稀疏计算。

Result: 在FEniCS、COMSOL、ANSYS基准测试、MNIST、CIFAR-10、CIFAR-100、DVS-Gesture数据集和COCO 2017上的实验表明：数值精度提高2.8%，吞吐量增加47%，能耗降低34%，与专用加速器相比吞吐量提升45-65%。消除了单独单元间的数据传输开销。

Conclusion: 该工作实现了有限元方法、脉冲神经网络和稀疏计算在单一平台上的统一处理，通过内存引导的适应机制解决了现有专用加速器的根本限制，为混合精度科学计算提供了高效统一的硬件加速解决方案。

Abstract: Recent hardware acceleration advances have enabled powerful specialized accelerators for finite element computations, spiking neural network inference, and sparse tensor operations. However, existing approaches face fundamental limitations: (1) finite element methods lack comprehensive rounding error analysis for reduced-precision implementations and use fixed precision assignment strategies that cannot adapt to varying numerical conditioning; (2) spiking neural network accelerators cannot handle non-spike operations and suffer from bit-width escalation as network depth increases; and (3) FPGA tensor accelerators optimize only for dense computations while requiring manual configuration for each sparsity pattern. To address these challenges, we introduce \textbf{Memory-Guided Unified Hardware Accelerator for Mixed-Precision Scientific Computing}, a novel framework that integrates three enhanced modules with memory-guided adaptation for efficient mixed-workload processing on unified platforms. Our approach employs memory-guided precision selection to overcome fixed precision limitations, integrates experience-driven bit-width management and dynamic parallelism adaptation for enhanced spiking neural network acceleration, and introduces curriculum learning for automatic sparsity pattern discovery. Extensive experiments on FEniCS, COMSOL, ANSYS benchmarks, MNIST, CIFAR-10, CIFAR-100, DVS-Gesture datasets, and COCO 2017 demonstrate 2.8\% improvement in numerical accuracy, 47\% throughput increase, 34\% energy reduction, and 45-65\% throughput improvement compared to specialized accelerators. Our work enables unified processing of finite element methods, spiking neural networks, and sparse computations on a single platform while eliminating data transfer overhead between separate units.

</details>


### [14] [MPM-LLM4DSE: Reaching the Pareto Frontier in HLS with Multimodal Learning and LLM-Driven Exploration](https://arxiv.org/abs/2601.04801)
*Lei Xu,Shanshan Wang,Chenglong Xiao*

Main category: cs.AR

TL;DR: MPM-LLM4DSE框架：结合多模态预测模型和LLM优化器，提升HLS设计空间探索效率


<details>
  <summary>Details</summary>
Motivation: 现有HLS DSE方法中，GNN预测模型无法充分捕捉行为描述的语义特征，传统多目标优化算法缺乏对pragma指令如何影响QoR的领域知识考虑

Method: 提出MPM-LLM4DSE框架：1) 多模态预测模型(MPM)融合行为描述与控制数据流图特征；2) 使用LLM作为优化器，配合专门的提示工程方法，包含pragma对QoR影响分析

Result: 多模态预测模型比现有最佳方法ProgSG提升高达10.25倍；在DSE任务中，LLM4DSE比先前方法平均性能提升39.90%

Conclusion: MPM-LLM4DSE框架通过结合多模态特征融合和LLM驱动的优化，显著提升了HLS设计空间探索的效率和效果

Abstract: High-Level Synthesis (HLS) design space exploration (DSE) seeks Pareto-optimal designs within expansive pragma configuration spaces. To accelerate HLS DSE, graph neural networks (GNNs) are commonly employed as surrogates for HLS tools to predict quality of results (QoR) metrics, while multi-objective optimization algorithms expedite the exploration. However, GNN-based prediction methods may not fully capture the rich semantic features inherent in behavioral descriptions, and conventional multi-objective optimization algorithms often do not explicitly account for the domain-specific knowledge regarding how pragma directives influence QoR. To address these limitations, this paper proposes the MPM-LLM4DSE framework, which incorporates a multimodal prediction model (MPM) that simultaneously fuses features from behavioral descriptions and control and data flow graphs. Furthermore, the framework employs a large language model (LLM) as an optimizer, accompanied by a tailored prompt engineering methodology. This methodology incorporates pragma impact analysis on QoR to guide the LLM in generating high-quality configurations (LLM4DSE). Experimental results demonstrate that our multimodal predictive model significantly outperforms state-of-the-art work ProgSG by up to 10.25$\times$. Furthermore, in DSE tasks, the proposed LLM4DSE achieves an average performance gain of 39.90\% over prior methods, validating the effectiveness of our prompting methodology. Code and models are available at https://github.com/wslcccc/MPM-LLM4DSE.

</details>


### [15] [Challenges and Research Directions for Large Language Model Inference Hardware](https://arxiv.org/abs/2601.05047)
*Xiaoyu Ma,David Patterson*

Main category: cs.AR

TL;DR: 论文分析了LLM推理的挑战，提出了四种架构研究机会来解决内存和互连瓶颈问题


<details>
  <summary>Details</summary>
Motivation: LLM推理面临内存和互连瓶颈，而非计算瓶颈，这源于Transformer的自回归解码特性以及AI发展趋势

Method: 提出四种架构研究机会：高带宽闪存、近内存处理、3D内存逻辑堆叠、低延迟互连

Result: 这些架构创新可以显著提升内存容量和带宽，加速通信，解决LLM推理的核心瓶颈

Conclusion: 需要新的硬件架构来应对LLM推理的内存和互连挑战，这些方案既适用于数据中心也部分适用于移动设备

Abstract: Large Language Model (LLM) inference is hard. The autoregressive Decode phase of the underlying Transformer model makes LLM inference fundamentally different from training. Exacerbated by recent AI trends, the primary challenges are memory and interconnect rather than compute. To address these challenges, we highlight four architecture research opportunities: High Bandwidth Flash for 10X memory capacity with HBM-like bandwidth; Processing-Near-Memory and 3D memory-logic stacking for high memory bandwidth; and low-latency interconnect to speedup communication. While our focus is datacenter AI, we also review their applicability for mobile devices.

</details>
