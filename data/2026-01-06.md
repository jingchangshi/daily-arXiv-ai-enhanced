<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [BALI: Branch-Aware Loop Invariant Inference with Large Language Models](https://arxiv.org/abs/2601.00882)
*Mingxiu Wang,Jiawei Wang,Xiao Cheng*

Main category: cs.PL

TL;DR: BALI：一个结合大语言模型和分支感知静态分析的框架，用于增强循环不变式的推理和验证，通过路径级子句验证和组合来提高精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 循环不变式对于验证迭代算法的正确性至关重要，但推导合适的不变式仍然是一项具有挑战性且通常需要手动完成的任务，特别是对于复杂程序。现有基于LLM的猜测-检查方法存在局限性。

Method: BALI框架结合了自动化推理和分支感知静态程序分析。不同于先前的纯LLM方法，BALI首先使用SMT验证分支序列级（路径级）子句，然后将这些子句组合成程序级不变式。

Result: 论文展示了初步结果，表明该方法在提高循环不变式推理的精度和可扩展性方面具有潜力。

Conclusion: BALI为完全自动化的不变式发现提供了有前景的方向，通过结合LLM和形式化验证方法，改进了循环不变式的推理和验证过程。

Abstract: Loop invariants are fundamental for reasoning about the correctness of iterative algorithms. However, deriving suitable invariants remains a challenging and often manual task, particularly for complex programs. In this paper, we introduce BALI, a branch-aware framework that integrates large language models (LLMs) to enhance the inference and verification of loop invariants. Our approach combines automated reasoning with branch-aware static program analysis to improve both precision and scalability. Specifically, unlike prior LLM-only guess-and-check methods, BALI first verifies branch-sequence-level (path-level) clauses with SMT and then composes them into program-level invariants. We outline its key components, present preliminary results, and discuss future directions toward fully automated invariant discovery.

</details>


### [2] [The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers](https://arxiv.org/abs/2601.02045)
*Shuoming Zhang,Jiacheng Zhao,Qiuchu Yu,Chunwei Xia,Zheng Wang,Xiaobing Feng,Huimin Cui*

Main category: cs.PL

TL;DR: 本文对LLM赋能编译领域进行了系统性综述，提出了多维分类法，总结了三大优势，并指出了该领域的关键挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在代码生成和理解方面展现出强大能力，研究者开始探索如何将LLMs应用于编译领域，以开发新一代智能、自适应、协同的编译工具。本综述旨在系统梳理这一新兴领域的研究现状，回答关键研究问题，为研究者和实践者提供基础路线图。

Method: 通过提出一个全面的多维分类法来系统分析现有工作：1）设计哲学（选择器、翻译器、生成器）；2）LLM方法论；3）代码抽象层次；4）任务类型。同时识别了该领域的主要挑战和机遇。

Result: 识别了LLM赋能编译的三大主要优势：1）编译开发的民主化；2）新颖优化策略的发现；3）编译器传统范围的扩展。指出了关键挑战包括确保正确性和实现可扩展性，并认为开发混合系统是最有前景的发展方向。

Conclusion: 本综述为LLM赋能编译领域提供了系统性框架，通过多维分类法、优势分析和挑战识别，为新一代智能、自适应、协同编译工具的发展绘制了路线图，将成为研究者和实践者的重要参考。

Abstract: This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools.

</details>


### [3] [Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming](https://arxiv.org/abs/2601.02060)
*Nguyet-Anh H. Lang,Eric Lang,Thanh Le-Cong,Bach Le,Quyet-Thang Huynh*

Main category: cs.PL

TL;DR: FPEval是一个评估LLM在函数式编程语言中代码生成能力的框架，基于FPBench基准测试，发现LLM在纯函数式语言中错误率较高，且经常生成非惯用的命令式风格代码。


<details>
  <summary>Details</summary>
Motivation: 函数式编程虽然能提供可靠和安全的软件系统基础，但由于学习曲线陡峭，采用率不高。LLM为降低这一门槛提供了新机会，但现有评估主要关注命令式语言，对函数式编程语言的评估不足。

Method: 引入FPEval评估框架，基于FPBench基准（包含721个编程任务，涵盖三个难度级别和三种主流函数式语言：Haskell、OCaml和Scala）。该框架提供全面的评估基础设施，包括测试验证和静态分析工具，评估功能正确性、代码风格和可维护性。

Result: LLM在函数式编程中的性能随模型进步显著提升，但在纯函数式语言（Haskell和OCaml）中的错误率显著高于混合语言（Scala）或命令式语言（Java）。LLM经常生成遵循命令式模式的非惯用函数式代码，影响代码风格和长期可维护性。LLM在获得静态分析反馈和手工制作的常见问题指令后，能够部分自我修复正确性和质量问题。

Conclusion: 虽然LLM在函数式编程代码生成方面有进步，但在纯函数式语言中的表现仍有较大提升空间，且生成的代码风格问题需要关注。通过提供适当的反馈机制，LLM能够改善代码质量和正确性。

Abstract: Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.

</details>


### [4] [MLIR-Smith: A Novel Random Program Generator for Evaluating Compiler Pipelines](https://arxiv.org/abs/2601.02218)
*Berke Ates,Filip Dobrosavljević,Theodoros Theodoridis,Zhendong Su*

Main category: cs.PL

TL;DR: MLIR-Smith：专门为测试MLIR编译器优化设计的随机程序生成工具，填补了MLIR可扩展性测试工具的空白，发现了多个编译器错误。


<details>
  <summary>Details</summary>
Motivation: 编译器对软件性能和正确执行至关重要，但在MLIR这种可扩展中间表示框架中缺乏合适的测试工具。现有方法如Csmith无法适应MLIR的可扩展性，需要专门工具来测试MLIR编译器优化。

Method: 提出MLIR-Smith，一种专门为MLIR设计的随机程序生成器。该工具能够生成随机的MLIR程序，用于测试和评估MLIR编译器优化。通过差分测试方法，在MLIR、LLVM、DaCe和DCIR等多个编译器流水线中进行测试。

Result: 使用MLIR-Smith进行差分测试，在MLIR、LLVM、DaCe和DCIR编译器流水线中发现了多个错误。证明了该工具在编译器测试中的实用性和有效性。

Conclusion: MLIR-Smith填补了MLIR编译器测试工具的空白，强调了编译器系统全面测试的重要性。该工具不仅增强了评估和改进编译器的能力，还为未来软件测试和质量保证工具的发展奠定了基础。

Abstract: Compilers are essential for the performance and correct execution of software and hold universal relevance across various scientific disciplines. Despite this, there is a notable lack of tools for testing and evaluating them, especially within the adaptable Multi-Level Intermediate Representation (MLIR) context. This paper addresses the need for a tool that can accommodate MLIR's extensibility, a feature not provided by previous methods such as Csmith. Here we introduce MLIR-Smith, a novel random program generator specifically designed to test and evaluate MLIR-based compiler optimizations. We demonstrate the utility of MLIR-Smith by conducting differential testing on MLIR, LLVM, DaCe, and DCIR, which led to the discovery of multiple bugs in these compiler pipelines. The introduction of MLIR-Smith not only fills a void in the realm of compiler testing but also emphasizes the importance of comprehensive testing within these systems. By providing a tool that can generate random MLIR programs, this paper enhances our ability to evaluate and improve compilers and paves the way for future tools, potentially shaping the wider landscape of software testing and quality assurance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [A Multi-Port Concurrent Communication Model for handling Compute Intensive Tasks on Distributed Satellite System Constellations](https://arxiv.org/abs/2601.01031)
*Bharadwaj Veeravalli*

Main category: cs.DC

TL;DR: 提出了用于分布式卫星系统的多端口并发通信可分割负载理论框架，量化计算、通信和结果传输开销的联合影响，提供最优负载分配和完成时间的闭式解，并扩展了实时准入控制机制。


<details>
  <summary>Details</summary>
Motivation: 分布式卫星系统需要处理并发数据分发、并行计算和结果返回，但现有理论缺乏对异构机载处理和星间链路条件的综合分析，需要可分析的理论框架来指导应用感知调度和系统级设计。

Method: 开发了MPCC-DLT框架，建立了包含计算速度、链路带宽和结果大小开销的数学模型，推导出最优负载分配和完成时间的闭式表达式，并扩展了处理随机任务到达和截止时间约束的实时准入控制机制。

Result: 高度可分布的任务能显著降低延迟，而通信密集型任务由于结果传输开销而收益递减；推导出的截止时间可行性条件能明确确定协作卫星集群规模；实时仿真展示了任务结构和系统参数如何共同决定截止时间满足情况和运行状态。

Conclusion: 该工作为分布式卫星系统提供了首个可分析处理的MPCC-DLT模型，为未来卫星星座的应用感知调度和系统级设计提供了可操作的见解，弥合了理论与实践之间的差距。

Abstract: We develop an integrated Multi-Port Concurrent Communication Divisible Load Theory (MPCC-DLT) framework for relay-centric distributed satellite systems (DSS), capturing concurrent data dissemination, parallel computation, and result return under heterogeneous onboard processing and inter-satellite link conditions. We propose a formulation that yields closed-form expressions for optimal load allocation and completion time that explicitly quantify the joint impact of computation speed, link bandwidth, and result-size overhead. We further derive deadline feasibility conditions that enable explicit sizing of cooperative satellite clusters to meet time-critical task requirements. Extensive simulation results demonstrate that highly distributable tasks achieve substantial latency reduction, while communication-heavy tasks exhibit diminishing returns due to result-transfer overheads. To bridge theory and practice, we extend the MPCC-DLT framework with a real-time admission control mechanism that handles stochastic task arrivals and deadline constraints, enabling blocking-aware operation. Our real-time simulations illustrate how task structure and system parameters jointly govern deadline satisfaction and operating regimes. Overall, this work provides the first analytically tractable MPCC-DLT model for distributed satellite systems and offers actionable insights for application-aware scheduling and system-level design of future satellite constellations.

</details>


### [6] [Performance and Security Aware Distributed Service Placement in Fog Computing](https://arxiv.org/abs/2601.01125)
*Mohammad Goudarzi,Arash Shaghaghi,Zhiyu Wang,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 提出SPA-DDRL框架，使用分布式深度强化学习在雾计算中联合优化服务响应时间和安全合规性，通过三层安全评分体系和改进算法实现性能与安全平衡。


<details>
  <summary>Details</summary>
Motivation: 物联网应用快速增长对雾计算中的高效安全服务部署提出更高要求，但现有方案大多只关注性能指标而忽视安全影响，异构资源、动态负载和多样化安全需求使得最优服务部署极具挑战性。

Method: 提出安全与性能感知的分布式深度强化学习框架，将问题建模为加权多目标优化任务，最小化延迟同时最大化基于雾节点安全能力的安全评分。采用三层安全评分体系（配置级、能力级、控制级），使用分布式代理-学习者架构，集成LSTM网络、优先经验回放和离策略修正机制。

Result: 基于真实物联网负载的实验显示，SPA-DDRL相比现有方法显著改善服务响应时间和部署安全性，响应时间提升16.3%，收敛速度加快33%，在所有系统规模下保持一致、可行、安全合规的解决方案。

Conclusion: SPA-DDRL框架成功解决了雾计算中服务部署的性能与安全联合优化问题，通过创新的三层安全评分体系和改进的强化学习算法，在动态环境中实现了高效且安全合规的服务部署。

Abstract: The rapid proliferation of IoT applications has intensified the demand for efficient and secure service placement in Fog computing. However, heterogeneous resources, dynamic workloads, and diverse security requirements make optimal service placement highly challenging. Most solutions focus primarily on performance metrics while overlooking the security implications of deployment decisions. This paper proposes a Security and Performance-Aware Distributed Deep Reinforcement Learning (SPA-DDRL) framework for joint optimization of service response time and security compliance in Fog computing. The problem is formulated as a weighted multi-objective optimization task, minimizing latency while maximizing a security score derived from the security capabilities of Fog nodes. The security score features a new three-tier hierarchy, where configuration-level checks verify proper settings, capability-level assessments evaluate the resource security features, and control-level evaluations enforce stringent policies, thereby ensuring compliant solutions that align with performance objectives. SPA-DDRL adopts a distributed broker-learner architecture where multiple brokers perform autonomous service-placement decisions and a centralized learner coordinates global policy optimization through shared prioritized experiences. It integrates three key improvements, including Long Short-Term Memory networks, Prioritized Experience Replay, and off-policy correction mechanisms to improve the agent's performance. Experiments based on real IoT workloads show that SPA-DDRL significantly improves both service response time and placement security compared to current approaches, achieving a 16.3% improvement in response time and a 33% faster convergence rate. It also maintains consistent, feasible, security-compliant solutions across all system scales, while baseline techniques fail or show performance degradation.

</details>


### [7] [OrchestrRL: Dynamic Compute and Network Orchestration for Disaggregated RL](https://arxiv.org/abs/2601.01209)
*Xin Tan,Yicheng Feng,Yu Zhou,Yimin Jiang,Yibo Zhu,Hong Xu*

Main category: cs.DC

TL;DR: OrchestrRL是一个用于解耦RL训练和生成阶段的编排框架，通过自适应计算调度和可重构混合光电网络解决计算瓶颈和动态网络流量问题。


<details>
  <summary>Details</summary>
Motivation: 将RL训练解耦为并行异步流水线虽然能提高吞吐量，但面临两个关键挑战：1) 生成阶段因动态工作负载和严重执行不均衡成为瓶颈；2) 解耦阶段产生多样动态网络流量模式，压垮传统网络架构。

Method: OrchestrRL采用自适应计算调度器动态调整并行度以匹配工作负载特性，同时设计RFabric可重构混合光电网络架构，利用光路交换机实时重构拓扑，支持不同并行配置下的生成和训练通信。

Result: 在48个H800 GPU的物理测试平台上，OrchestrRL实现了最高1.40倍的吞吐量提升。RLSim模拟器显示RFabric相比静态Fat-Tree网络具有更优的性能成本效率。

Conclusion: OrchestrRL通过协调计算和网络节奏，有效解决了解耦RL训练中的瓶颈问题，RFabric为大规模RL工作负载提供了高效解决方案。

Abstract: Post-training with reinforcement learning (RL) has greatly enhanced the capabilities of large language models. Disaggregating the generation and training stages in RL into a parallel, asynchronous pipeline offers the potential for flexible scaling and improved throughput. However, it still faces two critical challenges. First, the generation stage often becomes a bottleneck due to dynamic workload shifts and severe execution imbalances. Second, the decoupled stages result in diverse and dynamic network traffic patterns that overwhelm conventional network fabrics. This paper introduces OrchestrRL, an orchestration framework that dynamically manages compute and network rhythms in disaggregated RL. To improve generation efficiency, OrchestrRL employs an adaptive compute scheduler that dynamically adjusts parallelism to match workload characteristics within and across generation steps. This accelerates execution while continuously rebalancing requests to mitigate stragglers. To address the dynamic network demands inherent in disaggregated RL -- further intensified by parallelism switching -- we co-design RFabric, a reconfigurable hybrid optical-electrical fabric. RFabric leverages optical circuit switches at selected network tiers to reconfigure the topology in real time, enabling workload-aware circuits for (i) layer-wise collective communication during training iterations, (ii) generation under different parallelism configurations, and (iii) periodic inter-cluster weight synchronization. We evaluate OrchestrRL on a physical testbed with 48 H800 GPUs, demonstrating up to a 1.40x throughput improvement. Furthermore, we develop RLSim, a high-fidelity simulator, to evaluate RFabric at scale. Our results show that RFabric achieves superior performance-cost efficiency compared to static Fat-Tree networks, establishing it as a highly effective solution for large-scale RL workloads.

</details>


### [8] [Making MoE based LLM inference resilient with Tarragon](https://arxiv.org/abs/2601.01310)
*Songyu Zhang,Aaron Tam,Myungjin Lee,Shixiong Qi,K. K. Ramakrishnan*

Main category: cs.DC

TL;DR: Tarragon是一个弹性的MoE推理框架，通过将注意力工作者和专家工作者作为独立的故障域，实现故障隔离和快速恢复，显著减少故障导致的推理停滞时间。


<details>
  <summary>Details</summary>
Motivation: 现有MoE推理系统在故障恢复方面表现不佳：单个工作者故障就会触发粗粒度的服务级重启，丢弃已累积的进度并暂停整个推理管道，这对于延迟敏感的LLM服务来说是不可接受的。

Method: 1. 将注意力工作者和专家工作者作为独立的故障域；2. 引入可重新配置的数据路径，通过重定向请求到健康工作者来屏蔽故障；3. 实现自愈机制，放松现有MoE框架的紧密同步执行；4. 对有状态的注意力工作者采用异步增量KV缓存检查点；5. 对无状态的专家工作者利用剩余GPU内存部署影子专家。

Result: 与最先进的MegaScale-Infer相比，Tarragon将故障导致的停滞时间减少了160-213倍（从约64秒降至0.3-0.4秒），同时在无故障情况下保持性能不变。

Conclusion: Tarragon通过创新的故障隔离和快速恢复机制，为大规模MoE推理服务提供了高弹性的解决方案，显著提高了服务的可用性和可靠性。

Abstract: Mixture-of-Experts (MoE) models are increasingly used to serve LLMs at scale, but failures become common as deployment scale grows. Existing systems exhibit poor failure resilience: even a single worker failure triggers a coarse-grained, service-wide restart, discarding accumulated progress and halting the entire inference pipeline during recovery--an approach clearly ill-suited for latency-sensitive, LLM services.
  We present Tarragon, a resilient MoE inference framework that confines the failures impact to individual workers while allowing the rest of the pipeline to continue making forward progress. Tarragon exploits the natural separation between the attention and expert computation in MoE-based transformers, treating attention workers (AWs) and expert workers (EWs) as distinct failure domains. Tarragon introduces a reconfigurable datapath to mask failures by rerouting requests to healthy workers. On top of this datapath, Tarragon implements a self-healing mechanism that relaxes the tightly synchronized execution of existing MoE frameworks. For stateful AWs, Tarragon performs asynchronous, incremental KV cache checkpointing with per-request restoration, and for stateless EWs, it leverages residual GPU memory to deploy shadow experts. These together keep recovery cost and recomputation overhead extremely low. Our evaluation shows that, compared to state-of-the-art MegaScale-Infer, Tarragon reduces failure-induced stalls by 160-213x (from ~64 s down to 0.3-0.4 s) while preserving performance when no failures occur.

</details>


### [9] [DiT-HC: Enabling Efficient Training of Visual Generation Model DiT on HPC-oriented CPU Cluster](https://arxiv.org/abs/2601.01500)
*Jinxiao Zhang,Yunpu Xu,Xiyong Wu,Runmin Dong,Shenggan Cheng,Yi Zhao,Mengxuan Chen,Qinrui Zheng,Jianting Liu,Haohuan Fu*

Main category: cs.DC

TL;DR: DiT-HC：首个在下一代HPC CPU集群上训练和扩展生成模型DiT的系统，通过通信自由张量并行、优化算子和自定义MPI后端实现显著加速


<details>
  <summary>Details</summary>
Motivation: 随着生成基础模型在科学计算中数据重建和模拟的重要性日益增加，以及新硬件特性（矩阵加速单元、高带宽内存）的发展，CPU集群为加速和扩展此类模型提供了机会，促进AI与科学计算的融合

Method: 提出DiT-HC系统，包含三个关键技术：1) 带AutoMem的通信自由张量并行，实现自动化内存感知数据流；2) HCOps优化算子套件，利用向量和矩阵加速单元；3) 自定义MPI后端，重叠计算、通信和内存移动

Result: 实验显示比原生或公共CPU库加速8.2到87.7倍，在256个节点上实现90.6%的弱扩展效率

Conclusion: 证明了在CPU集群上进行大规模生成模型训练的可行性，为未来HPC-AI协同设计提供了新见解

Abstract: Generative foundation models have become an important tool for data reconstruction and simulation in scientific computing, showing a tight integration with traditional numerical simulations. At the same time, with the development of new hardware features, such as matrix acceleration units and high-bandwidth memory, CPU-based clusters offer promising opportunities to accelerate and scale such models, facilitating the unification of artificial intelligence and scientific computing. We present DiT-HC, the first system to train and scale the generative model DiT on a next-generation HPC CPU cluster. DiT-HC introduces three key techniques: (1) communication-free tensor parallelism (CFTP) with AutoMem for automated memory-aware dataflow, (2) HCOps, a suite of optimized GEMM and operator kernels leveraging vector and matrix acceleration units, and (3) a custom MPI backend that overlaps computation, communication, and memory movement. Experiments show 8.2 to 87.7 times speedups over native or public CPU libraries and 90.6% weak scaling efficiency on 256 nodes. These results demonstrate the feasibility of large-scale generative model training on CPU clusters and provide new insights for future HPC-AI co-design.

</details>


### [10] [FFCz: Fast Fourier Correction for Spectrum-Preserving Lossy Compression of Scientific Data](https://arxiv.org/abs/2601.01596)
*Congrong Ren,Robert Underwood,Sheng Di,Emrecan Kutay,Zarija Lukic,Aylin Yener,Franck Cappello,Hanqi Guo*

Main category: cs.DC

TL;DR: 提出一种基于快速傅里叶校正算法的新型技术，在保持空间域精度的同时，通过校正现有压缩器的误差来保护频域特征，适用于宇宙学、燃烧模拟等需要频域分析的科学应用。


<details>
  <summary>Details</summary>
Motivation: 许多科学应用（如宇宙学模拟、湍流燃烧、X射线衍射）需要同时保留数据的空间和频域表示，因为两者提供互补的科学洞察。现有压缩方法只保证空间域精度，忽略了频域特征的保护，这限制了压缩数据在频域分析任务中的可用性。

Method: 提出一种快速傅里叶校正算法，将频域误差表达为空间域误差的线性组合，推导出同时约束两个域误差的区域。给定基础压缩器的空间误差和用户定义的空间/频域误差界限，通过迭代投影将空间误差向量投影到两个约束区域的交集内。利用GPU并行加速实现实用性能。

Result: 在宇宙学模拟、X射线衍射、燃烧模拟和脑电图等多个数据集上验证了方法的有效性，成功保留了空间和频域的关键科学信息，同时保持了压缩性能。

Conclusion: 该方法解决了现有压缩方法忽略频域特征保护的问题，通过校正基础压缩器的误差，实现了同时约束空间和频域误差的目标，为需要频域分析的科学应用提供了有效的压缩解决方案。

Abstract: This paper introduces a novel technique to preserve spectral features in lossy compression based on a novel fast Fourier correction algorithm\added{ for regular-grid data}. Preserving both spatial and frequency representations of data is crucial for applications such as cosmology, turbulent combustion, and X-ray diffraction, where spatial and frequency views provide complementary scientific insights. In particular, many analysis tasks rely on frequency-domain representations to capture key features, including the power spectrum of cosmology simulations, the turbulent energy spectrum in combustion, and diffraction patterns in reciprocal space for ptychography. However, existing compression methods guarantee accuracy only in the spatial domain while disregarding the frequency domain. To address this limitation, we propose an algorithm that corrects the errors produced by off-the-shelf ``base'' compressors such as SZ3, ZFP, and SPERR, thereby preserving both spatial and frequency representations by bounding errors in both domains. By expressing frequency-domain errors as linear combinations of spatial-domain errors, we derive a region that jointly bounds errors in both domains. Given as input the spatial errors from a base compressor and user-defined error bounds in the spatial and frequency domains, we iteratively project the spatial error vector onto the regions defined by the spatial and frequency constraints until it lies within their intersection. We further accelerate the algorithm using GPU parallelism to achieve practical performance. We validate our approach with datasets from cosmology simulations, X-ray diffraction, combustion simulation, and electroencephalography demonstrating its effectiveness in preserving critical scientific information in both spatial and frequency domains.

</details>


### [11] [RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference](https://arxiv.org/abs/2601.01712)
*Jiarui Wang,Huichao Chai,Yuanhang Zhang,Zongjin Zhou,Wei Guo,Xingkun Yang,Qiang Tang,Bo Pan,Jiawei Zhu,Ke Cheng,Yuting Yan,Shulan Wang,Yingjie Zhu,Zhengfan Yuan,Jiaqi Huang,Yuhan Zhang,Xiaosong Sun,Zhinan Zhang,Hong Zhu,Yongsheng Zhang,Tiantian Dong,Zhong Xiao,Deliang Liu,Chengzhou Lu,Yuan Sun,Zhiyuan Chen,Xinming Han,Zaizhu Liu,Yaoyuan Wang,Ziyang Zhang,Yong Liu,Jinxin Xu,Yajing Sun,Zhoujun Yu,Wenting Zhou,Qidong Zhang,Zhengyong Zhang,Zhonghai Gu,Yibo Jin,Yongxiang Feng,Pengfei Zuo*

Main category: cs.DC

TL;DR: RelayGR系统通过预推断和缓存用户行为序列前缀，在严格延迟SLO下显著提升生成式推荐模型的序列长度和吞吐量


<details>
  <summary>Details</summary>
Motivation: 生成式推荐模型需要处理长用户行为序列来提升推荐质量，但在生产环境中受到严格尾部延迟SLO的限制，只能处理很短的序列。研究发现大部分GR token编码的用户行为与候选物品无关，这提供了预推断和缓存的可能性

Method: RelayGR系统采用三种关键技术：1) 序列感知触发器，在有限缓存占用和预推断负载下只处理有风险的请求；2) 亲和感知路由器，将缓存生产和消费路由到同一实例；3) 内存感知扩展器，利用服务器本地DRAM捕获短期跨请求重用

Result: 在华为昇腾NPU上实现RelayGR，在固定P99 SLO下，支持1.5倍更长的序列长度，并将SLO合规吞吐量提升3.6倍

Conclusion: RelayGR通过创新的预推断和缓存机制，成功解决了生成式推荐模型在生产环境中的延迟瓶颈，显著提升了推荐质量和系统性能

Abstract: Real-time recommender systems execute multi-stage cascades (retrieval, pre-processing, fine-grained ranking) under strict tail-latency SLOs, leaving only tens of milliseconds for ranking. Generative recommendation (GR) models can improve quality by consuming long user-behavior sequences, but in production their online sequence length is tightly capped by the ranking-stage P99 budget. We observe that the majority of GR tokens encode user behaviors that are independent of the item candidates, suggesting an opportunity to pre-infer a user-behavior prefix once and reuse it during ranking rather than recomputing it on the critical path. Realizing this idea at industrial scale is non-trivial: the prefix cache must survive across multiple pipeline stages before the final ranking instance is determined, the user population implies cache footprints far beyond a single device, and indiscriminate pre-inference would overload shared resources under high QPS. We present RelayGR, a production system that enables in-HBM relay-race inference for GR. RelayGR selectively pre-infers long-term user prefixes, keeps their KV caches resident in HBM over the request lifecycle, and ensures the subsequent ranking can consume them without remote fetches. RelayGR combines three techniques: 1) a sequence-aware trigger that admits only at-risk requests under a bounded cache footprint and pre-inference load, 2) an affinity-aware router that co-locates cache production and consumption by routing both the auxiliary pre-infer signal and the ranking request to the same instance, and 3) a memory-aware expander that uses server-local DRAM to capture short-term cross-request reuse while avoiding redundant reloads. We implement RelayGR on Huawei Ascend NPUs and evaluate it with real queries. Under a fixed P99 SLO, RelayGR supports up to 1.5$\times$ longer sequences and improves SLO-compliant throughput by up to 3.6$\times$.

</details>


### [12] [pMSz: A Distributed Parallel Algorithm for Correcting Extrema and Morse Smale Segmentations in Lossy Compression](https://arxiv.org/abs/2601.01787)
*Yuxiao Li,Mingze Xia,Xin Liang,Bei Wang,Robert Underwood,Sheng Di,Hemant Sharma,Dishant Beniwal,Franck Cappello,Hanqi Guo*

Main category: cs.DC

TL;DR: 提出分布式并行算法，用于校正压缩后数据的拓扑特征（PLMSS），在128个GPU上实现超过90%的并行效率


<details>
  <summary>Details</summary>
Motivation: 有损压缩会扭曲数据中的关键特征，可能影响下游科学分析并导致错误结论。现有单GPU算法无法扩展到极端规模数据

Method: 简化MSz算法，通过保留所有位置的陡峭上升和下降方向来避免显式计算积分路径，减少进程间通信，采用松弛同步机制

Result: 在Perlmutter超级计算机的128个GPU上，对真实世界数据集实现了超过90%的并行效率

Conclusion: 提出的分布式算法成功解决了PLMSS校正的扩展瓶颈，为大规模科学数据的拓扑特征保护提供了高效解决方案

Abstract: Lossy compression, widely used by scientists to reduce data from simulations, experiments, and observations, can distort features of interest even under bounded error. Such distortions may compromise downstream analyses and lead to incorrect scientific conclusions in applications such as combustion and cosmology. This paper presents a distributed and parallel algorithm for correcting topological features, specifically, piecewise linear Morse Smale segmentations (PLMSS), which decompose the domain into monotone regions labeled by their corresponding local minima and maxima. While a single GPU algorithm (MSz) exists for PLMSS correction after compression, no methodology has been developed that scales beyond a single GPU for extreme scale data. We identify the key bottleneck in scaling PLMSS correction as the parallel computation of integral paths, a communication-intensive computation that is notoriously difficult to scale. Instead of explicitly computing and correcting integral paths, our algorithm simplifies MSz by preserving steepest ascending and descending directions across all locations, thereby minimizing interprocess communication while introducing negligible additional storage overhead. With this simplified algorithm and relaxed synchronization, our method achieves over 90% parallel efficiency on 128 GPUs on the Perlmutter supercomputer for real world datasets.

</details>


### [13] [Bringing computation to the data: A MOEA-driven approach for optimising data processing in the context of the SKA and SRCNet](https://arxiv.org/abs/2601.01980)
*Manuel Parra-Royón,Álvaro Rodríguez-Gallardo,Susana Sánchez-Expósito,Laura Darriba-Pol,Jesús Sánchez-Castañeda,M. Ángeles Mendoza,Julián Garrido,Javier Moldón,Lourdes Verdes-Montenegro*

Main category: cs.DC

TL;DR: 该论文提出在SKA天文台数据处理中采用分布式和原位计算，结合FaaS和进化算法优化数据密集型工作流，以解决传统集中式计算的数据传输瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: SKA天文台将产生前所未有的数据量，传统的数据中心计算模型由于网络和存储瓶颈已不可行。SRCNet需要在近艾级环境中运行，需要新的计算范式来解决数据传输问题。

Method: 提出向分布式和原位计算转变，将计算移至数据附近。集成函数即服务（FaaS）与基于进化算法（EAs）的智能决策实体，优化SRCNet中的数据密集型工作流。使用多目标进化算法（MOEAs）探索考虑执行时间和能耗的最优执行计划。

Result: 建立了一个高效且成本感知的计算到数据策略的基准框架，能够在考虑数据位置和传输成本约束的同时，优化执行时间和能耗。

Conclusion: 该工作为SRCNet架构中的计算到数据策略提供了可行的解决方案，通过FaaS和进化算法的结合，能够有效应对SKA数据处理中的大规模挑战。

Abstract: The Square Kilometre Array (SKA) will generate unprecedented data volumes, making efficient data processing a critical challenge. Within this context, the SKA Regional Centres Network (SRCNet) must operate in a near-exascale environment where traditional data-centric computing models based on moving large datasets to centralised resources are no longer viable due to network and storage bottlenecks.
  To address this limitation, this work proposes a shift towards distributed and in-situ computing, where computation is moved closer to the data. We explore the integration of Function-as-a-Service (FaaS) with an intelligent decision-making entity based on Evolutionary Algorithms (EAs) to optimise data-intensive workflows within SRCNet. FaaS enables lightweight and modular function execution near data sources while abstracting infrastructure management.
  The proposed decision-making entity employs Multi-Objective Evolutionary Algorithms (MOEAs) to explore near-optimal execution plans considering execution time and energy consumption, together with constraints related to data location and transfer costs. This work establishes a baseline framework for efficient and cost-aware computation-to-data strategies within the SRCNet architecture.

</details>


### [14] [SuperSFL: Resource-Heterogeneous Federated Split Learning with Weight-Sharing Super-Networks](https://arxiv.org/abs/2601.02092)
*Abdullah Al Asif,Sixing Yu,Juan Pablo Munoz,Arya Mazaheri,Ali Jannesari*

Main category: cs.DC

TL;DR: SuperSFL提出了一种联邦分割学习框架，通过权重共享超网络动态生成资源感知的客户端子网络，解决异构环境中的计算和通信差异问题，显著提升训练效率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统的SplitFed Learning在异构边缘设备环境中面临重大挑战，因为设备具有不同的计算和通信能力，导致训练效率低下和收敛缓慢。

Method: 1) 使用权重共享超网络动态生成资源感知的客户端子网络；2) 引入三阶段梯度融合(TPGF)机制协调本地更新、服务器端计算和梯度融合；3) 采用容错客户端分类器和协作式客户端-服务器聚合机制。

Result: 在CIFAR-10和CIFAR-100数据集上，与100个异构客户端测试，SuperSFL比基线SFL收敛速度快2-5倍，通信轮次减少，总通信成本降低20倍，训练时间缩短13倍，同时达到更高准确率。

Conclusion: SuperSFL通过创新的超网络架构和优化机制，有效解决了联邦分割学习在异构环境中的挑战，提供了更高效、更节能的解决方案，适用于实际边缘计算场景。

Abstract: SplitFed Learning (SFL) combines federated learning and split learning to enable collaborative training across distributed edge devices; however, it faces significant challenges in heterogeneous environments with diverse computational and communication capabilities. This paper proposes \textit{SuperSFL}, a federated split learning framework that leverages a weight-sharing super-network to dynamically generate resource-aware client-specific subnetworks, effectively mitigating device heterogeneity. SuperSFL introduces Three-Phase Gradient Fusion (TPGF), an optimization mechanism that coordinates local updates, server-side computation, and gradient fusion to accelerate convergence. In addition, a fault-tolerant client-side classifier and collaborative client--server aggregation enable uninterrupted training under intermittent communication failures. Experimental results on CIFAR-10 and CIFAR-100 with up to 100 heterogeneous clients show that SuperSFL converges $2$--$5\times$ faster in terms of communication rounds than baseline SFL while achieving higher accuracy, resulting in up to $20\times$ lower total communication cost and $13\times$ shorter training time. SuperSFL also demonstrates improved energy efficiency compared to baseline methods, making it a practical solution for federated learning in heterogeneous edge environments.

</details>


### [15] [BigSUMO: A Scalable Framework for Big Data Traffic Analytics and Parallel Simulation](https://arxiv.org/abs/2601.02286)
*Rahul Sengupta,Nooshin Yousefzadeh,Manav Sanghvi,Yash Ranjan,Anand Rangarajan,Sanjay Ranka,Yashaswi Karnati,Jeremy Dilmore,Tushar Patel,Ryan Casburn*

Main category: cs.DC

TL;DR: BigSUMO是一个端到端、可扩展的开源交通分析框架，结合描述性分析和SUMO微观仿真，用于交通中断检测和优化场景测试。


<details>
  <summary>Details</summary>
Motivation: 随着全球城市化进程加快，交通基础设施的高效管理对交通机构和城市规划者至关重要。需要能够分析大量存储交通数据并制定有效干预措施的工具。

Method: BigSUMO是一个模块化开源框架，首先摄入高分辨率环形检测器和信号状态数据以及稀疏探测轨迹数据，进行描述性分析和中断检测，然后使用SUMO微观仿真器进行规范性分析，测试数百种假设场景以优化交通性能。

Result: 该系统具有成本效益、可扩展且易于部署，能够有效处理交通数据分析、中断检测和优化场景测试，为智慧城市交通解决方案开发提供有价值的辅助工具。

Conclusion: BigSUMO框架有望成为开发智慧城市交通解决方案的重要工具，其开源、模块化设计使其具有广泛适用性和部署便利性。

Abstract: With growing urbanization worldwide, efficient management of traffic infrastructure is critical for transportation agencies and city planners. It is essential to have tools that help analyze large volumes of stored traffic data and make effective interventions. To address this need, we present ``BigSUMO", an end-to-end, scalable, open-source framework for analytics, interruption detection, and parallel traffic simulation. Our system ingests high-resolution loop detector and signal state data, along with sparse probe trajectory data. It first performs descriptive analytics and detects potential interruptions. It then uses the SUMO microsimulator for prescriptive analytics, testing hundreds of what-if scenarios to optimize traffic performance. The modular design allows integration of different algorithms for data processing and outlier detection. Built using open-source software and libraries, the pipeline is cost-effective, scalable, and easy to deploy. We hope BigSUMO will be a valuable aid in developing smart city mobility solutions.

</details>


### [16] [Placement Semantics for Distributed Deep Learning: A Systematic Framework for Analyzing Parallelism Strategies](https://arxiv.org/abs/2601.02311)
*Deep Pankajbhai Mehta*

Main category: cs.DC

TL;DR: 提出了placement semantics框架，通过状态放置模式统一分析各种并行策略的内存消耗和通信开销，无需实现细节即可精确预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型训练需要跨多个加速器分布计算，但实践者只能通过试错选择并行策略（数据、张量、流水线、ZeRO），缺乏统一的系统化框架来预测它们的行为。

Method: 引入placement semantics：每个策略通过五种模式（复制、分片、分片带聚集、物化、卸载）将四种训练状态（参数、优化器、梯度、激活）放置到设备上。仅从放置方式即可推导内存消耗和通信量。

Result: 预测结果与已发表结果完全匹配：ZeRO-3比数据并行少用8倍内存，通信成本增加1.5倍，与原始论文报告一致。证明了梯度完整性和状态一致性是分布式训练匹配单设备结果的充分必要条件。

Conclusion: 该框架将ZeRO Stages 1-3、FSDP、张量并行和流水线并行统一为具有不同放置选择的实例，提供了组合策略的安全组合规则。

Abstract: Training large language models requires distributing computation across many accelerators, yet practitioners select parallelism strategies (data, tensor, pipeline, ZeRO) through trial and error because no unified systematic framework predicts their behavior. We introduce placement semantics: each strategy is specified by how it places four training states (parameters, optimizer, gradients, activations) across devices using five modes (replicated, sharded, sharded-with-gather, materialized, offloaded). From placement alone, without implementation details, we derive memory consumption and communication volume. Our predictions match published results exactly: ZeRO-3 uses 8x less memory than data parallelism at 1.5x communication cost, as reported in the original paper. We prove two conditions (gradient integrity, state consistency) are necessary and sufficient for distributed training to match single-device results, and provide composition rules for combining strategies safely. The framework unifies ZeRO Stages 1-3, Fully Sharded Data Parallel (FSDP), tensor parallelism, and pipeline parallelism as instances with different placement choices.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [A System Architecture for Low Latency Multiprogramming Quantum Computing](https://arxiv.org/abs/2601.01158)
*Yilun Zhao,Yu Chen,Kaiyan Chang,He Li,Bing Li,Yinhe Han,Ying Wang*

Main category: cs.AR

TL;DR: FLAMENCO是一个保真度感知的多版本编译系统，通过离线编译生成多样化的可执行版本，运行时进行动态区域选择和冲突避免，消除了量子多程序计算中的在线编译开销，实现了5倍以上的运行时加速。


<details>
  <summary>Details</summary>
Motivation: 随着量子系统规模扩大，多程序量子计算（MPQC）对提高设备利用率和吞吐量至关重要。然而，当前的MPQC流水线依赖昂贵的在线编译来共同优化并发运行的程序，因为量子可执行文件是设备依赖的、跨量子比特区域不可移植的，并且对噪声和串扰高度敏感。这一在线步骤主导了运行时间，阻碍了未来实际工作负载（如重复调用的量子神经网络服务）的低延迟部署。

Method: 1）在架构层面，将设备抽象为计算单元以大幅缩小区域分配的搜索空间；2）在编译时，为每个程序生成绑定到不同量子比特区域的多样化可执行版本，实现运行时动态区域选择；3）在运行时，采用简化的编排器，利用编译后的保真度指标避免冲突并减轻串扰，实现无需在线共同优化的可靠协同执行。

Result: 与最先进的MPQC基线相比，FLAMENCO消除了在线编译开销，实现了超过5倍的运行时加速，提高了执行保真度，并在并发增加时保持了高利用率。

Conclusion: FLAMENCO通过保真度感知的多版本编译系统，实现了独立的离线编译和运行时低延迟、高保真度的多程序执行，解决了当前MPQC流水线中在线编译开销大的问题，为实际量子工作负载提供了可行的解决方案。

Abstract: As quantum systems scale, Multiprogramming Quantum Computing (MPQC) becomes essential to improve device utilization and throughput. However, current MPQC pipelines rely on expensive online compilation to co-optimize concurrently running programs, because quantum executables are device-dependent, non-portable across qubit regions, and highly susceptible to noise and crosstalk. This online step dominates runtime and impedes low-latency deployments for practical, real-world workloads in the future, such as repeatedly invoked Quantum Neural Network (QNN) services.
  We present FLAMENCO, a fidelity-aware multi-version compilation system that enables independent offline compilation and low-latency, high-fidelity multiprogramming at runtime. At the architecture level, FLAMENCO abstracts devices into compute units to drastically shrink the search space of region allocation. At compile time, it generates diverse executable versions for each program -- each bound to a distinct qubit region -- allowing dynamic region selection at runtime and overcoming non-portability. At runtime, FLAMENCO employs a streamlined orchestrator that leverages post-compilation fidelity metrics to avoid conflicts and mitigate crosstalk, achieving reliable co-execution without online co-optimization. Comprehensive evaluations against state-of-the-art MPQC baselines show that FLAMENCO removes online compilation overhead, achieves over 5$\times$ runtime speedup, improves execution fidelity, and maintains high utilization as concurrency increases.

</details>


### [18] [CounterPoint: Using Hardware Event Counters to Refute and Refine Microarchitectural Assumptions (Extended Version)](https://arxiv.org/abs/2601.01265)
*Nick Lindsay,Caroline Trippel,Anurag Khandelwal,Abhishek Bhattacharjee*

Main category: cs.AR

TL;DR: CounterPoint框架通过测试用户指定的微架构模型与性能计数器数据的一致性，帮助专家解释模糊的硬件事件计数器数据，并发现未文档化的微架构行为。


<details>
  <summary>Details</summary>
Motivation: 硬件事件计数器虽然能揭示性能瓶颈和微架构行为，但由于其规范模糊、设计不透明以及多路复用噪声，实际数据难以解释，需要工具来帮助专家理解这些计数器数据。

Method: 引入CounterPoint框架，测试用户指定的微架构模型（表示为μ路径决策图）与性能计数器数据的一致性。当出现不匹配时，使用多维计数器置信区域来减轻多路复用噪声，并定位可能的微架构特征来解释这些不匹配。

Result: 将CounterPoint应用于Haswell内存管理单元案例研究，揭示了多个未文档化或文档不足的微架构行为，包括负载存储队列侧TLB预取器、合并页表遍历器、可中止页表遍历等。

Conclusion: CounterPoint帮助专家将嘈杂的硬件性能计数器测量与他们对微架构的心理模型相协调，在此过程中发现了以前隐藏的微妙硬件特性。

Abstract: Hardware event counters offer the potential to reveal not only performance bottlenecks but also detailed microarchitectural behavior. In practice, this promise is undermined by their vague specifications, opaque designs, and multiplexing noise, making event counter data hard to interpret.
  We introduce CounterPoint, a framework that tests user-specified microarchitectural models - expressed as $μ$path Decision Diagrams - for consistency with performance counter data. When mismatches occur, CounterPoint pinpoints plausible microarchitectural features that could explain them, using multi-dimensional counter confidence regions to mitigate multiplexing noise. We apply CounterPoint to the Haswell Memory Management Unit as a case study, shedding light on multiple undocumented and underdocumented microarchitectural behaviors. These include a load-store queue-side TLB prefetcher, merging page table walkers, abortable page table walks, and more.
  Overall, CounterPoint helps experts reconcile noisy hardware performance counter measurements with their mental model of the microarchitecture - uncovering subtle, previously hidden hardware features along the way.

</details>


### [19] [Ageing Monitoring for Commercial Microcontrollers Based on Timing Windows](https://arxiv.org/abs/2601.02053)
*Leandro Lanzieri,Jiri Kral,Goerschwin Fey,Holger Schlarb,Thomas C. Schmidt*

Main category: cs.AR

TL;DR: 提出一种基于软件自测试的微控制器硬件老化监测方法，通过可变长度时间窗口确定设备最大工作频率，可现场部署检测温度引起的性能退化


<details>
  <summary>Details</summary>
Motivation: 微控制器在嵌入式系统和可靠性应用中日益普及，硬件老化导致的故障可能产生严重影响。目前缺乏可部署的老化监测技术，普遍采用静态防护带方法防止时序错误，但这会限制性能且可能导致设备老化时突然失效。

Method: 采用基于软件的自测试方法设计微控制器硬件退化监测技术。该方法利用可变长度时间窗口确定设备的最大工作频率，可在现场部署。

Result: 在真实硬件上实证验证该方法，发现在温度升高60°C时，能一致检测到最大工作频率高达13.79%的温度引起的退化，且在不同设备间表现一致。

Conclusion: 提出的软件自测试方法能够有效监测微控制器的硬件退化，解决了传统静态防护带方法的局限性，为嵌入式系统提供了可靠的老化监测解决方案。

Abstract: Microcontrollers are increasingly present in embedded deployments and dependable applications, for which malfunctions due to hardware ageing can have severe impact. The lack of deployable techniques for ageing monitoring on these devices has spread the application of guard bands to prevent timing errors due to degradation. Applying this static technique can limit performance and lead to sudden failures as devices age. In this paper, we follow a software-based self-testing approach to design monitoring of hardware degradation for microcontrollers. Deployable in the field, our technique leverages timing windows of variable lengths to determine the maximum operational frequency of the devices. We empirically validate the method on real hardware and find that it consistently detects temperature-induced degradations in maximum operating frequency of up to 13.79 % across devices for 60 °C temperature increase.

</details>


### [20] [HFRWKV: A High-Performance Fully On-Chip Hardware Accelerator for RWKV](https://arxiv.org/abs/2601.02135)
*Liu Shijie,Zeng Zhenghao,Jiao Han,Huang Yihua*

Main category: cs.AR

TL;DR: HFRWKV：针对RWKV模型的FPGA硬件加速器，通过混合精度量化、可复用架构和全片上计算系统，显著提升吞吐量和能效


<details>
  <summary>Details</summary>
Motivation: RWKV作为现代RNN架构，虽然能处理长上下文且内存成本线性增长，但其顺序计算模式无法充分利用GPU并行性，导致计算资源利用率低，且频繁的片外权重访问造成内存瓶颈

Method: 1. 提出硬件友好的混合精度量化策略；2. 针对指数和除法等复杂操作，采用可复用架构结合查找表或分段线性逼近；3. 采用全片上计算系统，集成并行矩阵向量处理阵列和高效流水线架构；4. 通过计算重排序和分块双缓冲消除数据传输瓶颈

Result: 在Alveo U50和U280平台上实现，相比CPU：吞吐量提升63.48倍，能效提升139.17倍；相比GPU：吞吐量提升32.33倍，能效提升171.36倍

Conclusion: HFRWKV成功解决了RWKV在GPU上的并行性不足和内存瓶颈问题，通过FPGA专用硬件加速实现了显著的性能提升和能效改进

Abstract: RWKV is a modern RNN architecture that approaches the performance of Transformers, with the advantage of processing long contexts at a linear memory cost. However, its sequential computation pattern struggles to efficiently leverage GPU parallelism, which leads to low compute resource utilization. Furthermore, frequent off-chip weight accesses create a memory bottleneck. To address these challenges, we propose HFRWKV, an FPGA-based hardware accelerator specifically designed for RWKV. Within the matrix operation module, we propose a novel hardware-friendly hybrid-precision quantization strategy, which enhances performance while maintaining acceptable accuracy. For the complex operations including exponentiation and division, we introduce a method featuring reusable architectures combined with lookup tables or piecewise linear approximation, which is algorithmically refined to effectively balance precision and hardware resource consumption. Based on this foundation, we adopt a fully on-chip computing system integrating parallel matrix-vector processing array and an efficient pipeline architecture. Through computation reordering and chunked double buffering, it effectively eliminates data transfer bottlenecks and improves overall throughput. We implement HFRWKV on the Alveo U50 and U280 platform. Experimental results show that compared to a CPU, a throughput improvement of 63.48$\times$ and an energy efficiency improvement of 139.17$\times$. Compared to GPUs, achieves a throughput improvement of 32.33$\times$ and an energy efficiency improvement of 171.36$\times$.

</details>
