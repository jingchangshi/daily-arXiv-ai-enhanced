{"id": "2512.16926", "categories": ["cs.DC", "cs.OS", "cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.16926", "abs": "https://arxiv.org/abs/2512.16926", "authors": ["Oren Bell", "Harun Teper", "Mario G\u00fcnzel", "Chris Gill", "Jian-Jia Chen"], "title": "Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor", "comment": "18 pages, 5 figure", "summary": "This paper addresses limitations of current scheduling methods in the Robot Operating System (ROS)2, focusing on scheduling tasks beyond simple chains and analyzing arbitrary Directed Acyclic Graphs (DAGs). While previous research has focused mostly on chain-based scheduling with ad-hoc response time analyses, we propose a novel approach using the events executor to implement fixed-job-level-priority schedulers for arbitrary ROS2 graphs on uniprocessor systems. We demonstrate that ROS 2 applications can be abstracted as forests of trees, enabling the mapping of ROS 2 applications to traditional real-time DAG task models. Our usage of the events executor requires a special implementation of the events queue and a communication middleware that supports LIFO-ordered message delivery, features not yet standard in ROS2. We show that our implementation generates the same schedules as a conventional fixed-priority DAG task scheduler, in spite of lacking access to the precedence information that usually is required. This further closes the gap between established real-time systems theory and ROS2 scheduling analyses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u6267\u884c\u5668\u7684\u56fa\u5b9a\u4f5c\u4e1a\u7ea7\u4f18\u5148\u7ea7\u8c03\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5355\u5904\u7406\u5668\u7cfb\u7edf\u4e0a\u8c03\u5ea6\u4efb\u610fROS2 DAG\u4efb\u52a1\uff0c\u5c06ROS2\u5e94\u7528\u62bd\u8c61\u4e3a\u6811\u68ee\u6797\u5e76\u6620\u5c04\u5230\u4f20\u7edf\u5b9e\u65f6DAG\u4efb\u52a1\u6a21\u578b\u3002", "motivation": "\u5f53\u524dROS2\u8c03\u5ea6\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u7b80\u5355\u7684\u94fe\u5f0f\u4efb\u52a1\u8c03\u5ea6\uff0c\u7f3a\u4e4f\u5bf9\u4efb\u610f\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u4efb\u52a1\u7684\u5206\u6790\u80fd\u529b\uff0c\u9700\u8981\u586b\u8865\u5b9e\u65f6\u7cfb\u7edf\u7406\u8bba\u4e0eROS2\u8c03\u5ea6\u5206\u6790\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u4e8b\u4ef6\u6267\u884c\u5668\u5b9e\u73b0\u56fa\u5b9a\u4f5c\u4e1a\u7ea7\u4f18\u5148\u7ea7\u8c03\u5ea6\u5668\uff0c\u5c06ROS2\u5e94\u7528\u62bd\u8c61\u4e3a\u6811\u68ee\u6797\u7ed3\u6784\uff0c\u6620\u5c04\u5230\u4f20\u7edf\u5b9e\u65f6DAG\u4efb\u52a1\u6a21\u578b\uff0c\u9700\u8981\u7279\u6b8a\u7684\u4e8b\u4ef6\u961f\u5217\u5b9e\u73b0\u548c\u652f\u6301LIFO\u987a\u5e8f\u6d88\u606f\u4f20\u9012\u7684\u901a\u4fe1\u4e2d\u95f4\u4ef6\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u7f3a\u4e4f\u901a\u5e38\u6240\u9700\u7684\u4f18\u5148\u7ea7\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u4e0e\u4f20\u7edf\u56fa\u5b9a\u4f18\u5148\u7ea7DAG\u4efb\u52a1\u8c03\u5ea6\u5668\u76f8\u540c\u7684\u8c03\u5ea6\u7ed3\u679c\uff0c\u6210\u529f\u5c06ROS2\u5e94\u7528\u6620\u5c04\u5230\u5b9e\u65f6DAG\u4efb\u52a1\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8fdb\u4e00\u6b65\u7f29\u5c0f\u4e86\u6210\u719f\u7684\u5b9e\u65f6\u7cfb\u7edf\u7406\u8bba\u4e0eROS2\u8c03\u5ea6\u5206\u6790\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3aROS2\u4e2d\u4efb\u610fDAG\u4efb\u52a1\u7684\u8c03\u5ea6\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.17023", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.17023", "abs": "https://arxiv.org/abs/2512.17023", "authors": ["Patrick Diehl", "Noujoud Nader", "Deepti Gupta"], "title": "LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation", "comment": null, "summary": "Parallel programming remains one of the most challenging aspects of High-Performance Computing (HPC), requiring deep knowledge of synchronization, communication, and memory models. While modern C++ standards and frameworks like OpenMP and MPI have simplified parallelism, mastering these paradigms is still complex. Recently, Large Language Models (LLMs) have shown promise in automating code generation, but their effectiveness in producing correct and efficient HPC code is not well understood. In this work, we systematically evaluate leading LLMs including ChatGPT 4 and 5, Claude, and LLaMA on the task of generating C++ implementations of the Mandelbrot set using shared-memory, directive-based, and distributed-memory paradigms. Each generated program is compiled and executed with GCC 11.5.0 to assess its correctness, robustness, and scalability. Results show that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u9ad8\u6027\u80fd\u8ba1\u7b97\u4ee3\u7801\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u9488\u5bf9Mandelbrot\u96c6\u5728\u4e0d\u540c\u5e76\u884c\u8303\u5f0f\u4e0b\u7684C++\u5b9e\u73b0\u3002", "motivation": "\u5e76\u884c\u7f16\u7a0b\u662f\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u6700\u5177\u6311\u6218\u6027\u7684\u65b9\u9762\u4e4b\u4e00\uff0c\u9700\u8981\u6df1\u5165\u4e86\u89e3\u540c\u6b65\u3001\u901a\u4fe1\u548c\u5185\u5b58\u6a21\u578b\u3002\u867d\u7136\u73b0\u4ee3C++\u6807\u51c6\u548cOpenMP\u3001MPI\u7b49\u6846\u67b6\u7b80\u5316\u4e86\u5e76\u884c\u6027\uff0c\u4f46\u638c\u63e1\u8fd9\u4e9b\u8303\u5f0f\u4ecd\u7136\u5f88\u590d\u6742\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u751f\u6210\u6b63\u786e\u9ad8\u6548\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u4ee3\u7801\u65b9\u9762\u7684\u6709\u6548\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u5305\u62ecChatGPT 4\u548c5\u3001Claude\u548cLLaMA\u5728\u5185\u7684\u9886\u5148LLM\uff0c\u5728\u751f\u6210\u4f7f\u7528\u5171\u4eab\u5185\u5b58\u3001\u57fa\u4e8e\u6307\u4ee4\u548c\u5206\u5e03\u5f0f\u5185\u5b58\u8303\u5f0f\u7684Mandelbrot\u96c6C++\u5b9e\u73b0\u65b9\u9762\u7684\u80fd\u529b\u3002\u6bcf\u4e2a\u751f\u6210\u7684\u7a0b\u5e8f\u90fd\u4f7f\u7528GCC 11.5.0\u7f16\u8bd1\u548c\u6267\u884c\uff0c\u4ee5\u8bc4\u4f30\u5176\u6b63\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cChatGPT-4\u548cChatGPT-5\u5728\u8bed\u6cd5\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7279\u522b\u662fChatGPT-4\u548cChatGPT-5\uff0c\u5728\u751f\u6210\u9ad8\u6027\u80fd\u8ba1\u7b97\u4ee3\u7801\u65b9\u9762\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u5b9e\u73b0\u826f\u597d\u7684\u8bed\u6cd5\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u80fd\u3002"}}
{"id": "2512.17077", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.17077", "abs": "https://arxiv.org/abs/2512.17077", "authors": ["Jiakun Fan", "Yanglin Zhang", "Xiangchen Li", "Dimitrios S. Nikolopoulos"], "title": "Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to Autoregressive Models (ARMs), utilizing parallel decoding to overcome sequential bottlenecks. However, existing research focuses primarily on kernel-level optimizations, lacking a holistic serving framework that addresses the unique memory dynamics of diffusion processes in production. We identify a critical \"memory footprint crisis\" specific to dLLMs, driven by monolithic logit tensors and the severe resource oscillation between compute-bound \"Refresh\" phases and bandwidth-bound \"Reuse\" phases. To bridge this gap, we present dLLM-Serve, an efficient dLLM serving system that co-optimizes memory footprint, computational scheduling, and generation quality. dLLM-Serve introduces Logit-Aware Activation Budgeting to decompose transient tensor peaks, a Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and Head-Centric Sparse Attention to decouple logical sparsity from physical storage. We evaluate dLLM-Serve on diverse workloads (LiveBench, Burst, OSC) and GPUs (RTX 4090, L40S). Relative to the state-of-the-art baseline, dLLM-Serve improves throughput by 1.61$\\times$-1.81$\\times$ on the consumer-grade RTX 4090 and 1.60$\\times$-1.74$\\times$ on the server-grade NVIDIA L40S, while reducing tail latency by nearly 4$\\times$ under heavy contention. dLLM-Serve establishes the first blueprint for scalable dLLM inference, converting theoretical algorithmic sparsity into tangible wall-clock acceleration across heterogeneous hardware.", "AI": {"tldr": "dLLM-Serve\u662f\u4e00\u4e2a\u9488\u5bf9\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5185\u5b58\u4f18\u5316\u3001\u8ba1\u7b97\u8c03\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u534f\u540c\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u7279\u6709\u7684\u5185\u5b58\u5371\u673a\u548c\u8d44\u6e90\u632f\u8361\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5185\u6838\u7ea7\u4f18\u5316\uff0c\u7f3a\u4e4f\u9488\u5bf9\u751f\u4ea7\u73af\u5883\u4e2d\u6269\u6563\u8fc7\u7a0b\u72ec\u7279\u5185\u5b58\u52a8\u6001\u7684\u6574\u4f53\u670d\u52a1\u6846\u67b6\u3002\u4f5c\u8005\u8bc6\u522b\u51fadLLM\u7279\u6709\u7684\"\u5185\u5b58\u5360\u7528\u5371\u673a\"\uff0c\u7531\u5355\u4e00\u5bf9\u6570\u5f20\u91cf\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\"\u5237\u65b0\"\u9636\u6bb5\u4e0e\u5e26\u5bbd\u5bc6\u96c6\u578b\"\u91cd\u7528\"\u9636\u6bb5\u4e4b\u95f4\u7684\u4e25\u91cd\u8d44\u6e90\u632f\u8361\u9a71\u52a8\u3002", "method": "dLLM-Serve\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1) \u5bf9\u6570\u611f\u77e5\u6fc0\u6d3b\u9884\u7b97\uff0c\u7528\u4e8e\u5206\u89e3\u77ac\u6001\u5f20\u91cf\u5cf0\u503c\uff1b2) \u9636\u6bb5\u591a\u8def\u590d\u7528\u8c03\u5ea6\u5668\uff0c\u7528\u4e8e\u4ea4\u9519\u5f02\u6784\u8bf7\u6c42\u9636\u6bb5\uff1b3) \u5934\u4e2d\u5fc3\u7a00\u758f\u6ce8\u610f\u529b\uff0c\u5c06\u903b\u8f91\u7a00\u758f\u6027\u4e0e\u7269\u7406\u5b58\u50a8\u89e3\u8026\u3002", "result": "\u5728\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\uff08LiveBench\u3001Burst\u3001OSC\uff09\u548cGPU\uff08RTX 4090\u3001L40S\uff09\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\uff0cdLLM-Serve\u5728\u6d88\u8d39\u7ea7RTX 4090\u4e0a\u541e\u5410\u91cf\u63d0\u53471.61-1.81\u500d\uff0c\u5728\u670d\u52a1\u5668\u7ea7NVIDIA L40S\u4e0a\u63d0\u53471.60-1.74\u500d\uff0c\u5728\u91cd\u5ea6\u4e89\u7528\u4e0b\u5c3e\u90e8\u5ef6\u8fdf\u964d\u4f4e\u8fd14\u500d\u3002", "conclusion": "dLLM-Serve\u4e3a\u53ef\u6269\u5c55\u7684dLLM\u63a8\u7406\u5efa\u7acb\u4e86\u9996\u4e2a\u84dd\u56fe\uff0c\u5c06\u7406\u8bba\u7b97\u6cd5\u7a00\u758f\u6027\u8f6c\u5316\u4e3a\u8de8\u5f02\u6784\u786c\u4ef6\u7684\u5b9e\u9645\u65f6\u949f\u52a0\u901f\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u670d\u52a1\u7684\u5173\u952e\u74f6\u9888\u3002"}}
{"id": "2512.17264", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.17264", "abs": "https://arxiv.org/abs/2512.17264", "authors": ["Yuming Xu", "Qianxi Zhang", "Qi Chen", "Baotong Lu", "Menghao Li", "Philip Adams", "Mingqin Li", "Zengzhong Li", "Jing Liu", "Cheng Li", "Fan Yang"], "title": "Scalable Distributed Vector Search via Accuracy Preserving Index Construction", "comment": null, "summary": "Scaling Approximate Nearest Neighbor Search (ANNS) to billions of vectors requires distributed indexes that balance accuracy, latency, and throughput. Yet existing index designs struggle with this tradeoff. This paper presents SPIRE, a scalable vector index based on two design decisions. First, it identifies a balanced partition granularity that avoids read-cost explosion. Second, it introduces an accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy. In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.", "AI": {"tldr": "SPIRE\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5411\u91cf\u7d22\u5f15\u7cfb\u7edf\uff0c\u901a\u8fc7\u5e73\u8861\u5206\u533a\u7c92\u5ea6\u548c\u9012\u5f52\u6784\u5efa\u591a\u7ea7\u7d22\u5f15\uff0c\u5728\u6570\u5341\u4ebf\u5411\u91cf\u89c4\u6a21\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0f\u7d22\u5f15\u8bbe\u8ba1\u5728\u6269\u5c55\u5230\u6570\u5341\u4ebf\u5411\u91cf\u65f6\u96be\u4ee5\u5e73\u8861\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u9700\u8981\u65b0\u7684\u7d22\u5f15\u8bbe\u8ba1\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "1. \u8bc6\u522b\u5e73\u8861\u7684\u5206\u533a\u7c92\u5ea6\u4ee5\u907f\u514d\u8bfb\u53d6\u6210\u672c\u7206\u70b8\uff1b2. \u5f15\u5165\u4fdd\u6301\u7cbe\u5ea6\u7684\u9012\u5f52\u6784\u5efa\u65b9\u6cd5\uff0c\u6784\u5efa\u5177\u6709\u53ef\u9884\u6d4b\u641c\u7d22\u6210\u672c\u548c\u7a33\u5b9a\u51c6\u786e\u6027\u7684\u591a\u7ea7\u7d22\u5f15\u3002", "result": "\u572846\u4e2a\u8282\u70b9\u4e0a\u6269\u5c55\u523080\u4ebf\u5411\u91cf\u7684\u5b9e\u9a8c\u4e2d\uff0cSPIRE\u5b9e\u73b0\u4e86\u9ad8\u53ef\u6269\u5c55\u6027\uff0c\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u7684\u541e\u5410\u91cf\u63d0\u9ad8\u4e869.64\u500d\u3002", "conclusion": "SPIRE\u901a\u8fc7\u521b\u65b0\u7684\u5206\u533a\u7b56\u7565\u548c\u9012\u5f52\u7d22\u5f15\u6784\u5efa\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5411\u91cf\u641c\u7d22\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u5728\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2512.17589", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.17589", "abs": "https://arxiv.org/abs/2512.17589", "authors": ["Yunhao Deng", "Fanchen Kong", "Xiaoling Yi", "Ryan Antonio", "Marian Verhelst"], "title": "Torrent: A Distributed DMA for Efficient and Flexible Point-to-Multipoint Data Movement", "comment": "7 pages, 11 figures, Proceeded by the 2026 Design, Automation and Test in Europe Conference (DATE 26)", "summary": "The growing disparity between computational power and on-chip communication bandwidth is a critical bottleneck in modern Systems-on-Chip (SoCs), especially for data-parallel workloads like AI. Efficient point-to-multipoint (P2MP) data movement, such as multicast, is essential for high performance. However, native multicast support is lacking in standard interconnect protocols. Existing P2MP solutions, such as multicast-capable Network-on-Chip (NoC), impose additional overhead to the network hardware and require modifications to the interconnect protocol, compromising scalability and compatibility.\n  This paper introduces Torrent, a novel distributed DMA architecture that enables efficient P2MP data transfers without modifying NoC hardware and interconnect protocol. Torrent conducts P2MP data transfers by forming logical chains over the NoC, where the data traverses through targeted destinations resembling a linked list. This Chainwrite mechanism preserves the P2P nature of every data transfer while enabling flexible data transfers to an unlimited number of destinations. To optimize the performance and energy consumption of Chainwrite, two scheduling algorithms are developed to determine the optimal chain order based on NoC topology.\n  Our RTL and FPGA prototype evaluations using both synthetic and real workloads demonstrate significant advantages in performance, flexibility, and scalability over network-layer multicast. Compared to the unicast baseline, Torrent achieves up to a 7.88x speedup. ASIC synthesis on 16nm technology confirms the architecture's minimal footprint in area (1.2%) and power (2.3%). Thanks to the Chainwrite, Torrent delivers scalable P2MP data transfers with a small cycle overhead of 82CC and area overhead of 207um2 per destination.", "AI": {"tldr": "Torrent\u662f\u4e00\u79cd\u5206\u5e03\u5f0fDMA\u67b6\u6784\uff0c\u901a\u8fc7\u94fe\u5f0f\u5199\u5165\u673a\u5236\u5728\u4e0d\u4fee\u6539NoC\u786c\u4ef6\u548c\u4e92\u8fde\u534f\u8bae\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u70b9\u5bf9\u591a\u70b9\u6570\u636e\u4f20\u8f93\uff0c\u76f8\u6bd4\u5355\u64ad\u57fa\u7ebf\u6700\u9ad8\u53ef\u52a0\u901f7.88\u500d\u3002", "motivation": "\u73b0\u4ee3SoC\u4e2d\u8ba1\u7b97\u80fd\u529b\u4e0e\u7247\u4e0a\u901a\u4fe1\u5e26\u5bbd\u4e4b\u95f4\u7684\u5dee\u8ddd\u65e5\u76ca\u6269\u5927\uff0c\u7279\u522b\u662f\u5bf9\u4e8eAI\u7b49\u6570\u636e\u5e76\u884c\u5de5\u4f5c\u8d1f\u8f7d\u3002\u9ad8\u6548\u7684\u70b9\u5bf9\u591a\u70b9\u6570\u636e\u4f20\u8f93\uff08\u5982\u7ec4\u64ad\uff09\u5bf9\u9ad8\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6807\u51c6\u4e92\u8fde\u534f\u8bae\u7f3a\u4e4f\u539f\u751f\u7ec4\u64ad\u652f\u6301\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u9700\u8981\u4fee\u6539\u7f51\u7edc\u786c\u4ef6\u548c\u534f\u8bae\uff0c\u5f71\u54cd\u53ef\u6269\u5c55\u6027\u548c\u517c\u5bb9\u6027\u3002", "method": "\u63d0\u51faTorrent\u5206\u5e03\u5f0fDMA\u67b6\u6784\uff0c\u901a\u8fc7Chainwrite\u673a\u5236\u5728NoC\u4e0a\u5f62\u6210\u903b\u8f91\u94fe\uff0c\u6570\u636e\u50cf\u94fe\u8868\u4e00\u6837\u904d\u5386\u76ee\u6807\u8282\u70b9\u3002\u5f00\u53d1\u4e24\u79cd\u8c03\u5ea6\u7b97\u6cd5\u6839\u636eNoC\u62d3\u6251\u786e\u5b9a\u6700\u4f18\u94fe\u987a\u5e8f\u4ee5\u4f18\u5316\u6027\u80fd\u548c\u80fd\u8017\u3002", "result": "RTL\u548cFPGA\u539f\u578b\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u7f51\u7edc\u5c42\u7ec4\u64ad\u5728\u6027\u80fd\u3001\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u6709\u663e\u8457\u4f18\u52bf\u3002\u76f8\u6bd4\u5355\u64ad\u57fa\u7ebf\u6700\u9ad8\u52a0\u901f7.88\u500d\u300216nm ASIC\u5408\u6210\u663e\u793a\u9762\u79ef\u5f00\u9500\u4ec51.2%\uff0c\u529f\u8017\u5f00\u95002.3%\u3002\u6bcf\u4e2a\u76ee\u6807\u8282\u70b9\u7684\u5468\u671f\u5f00\u9500\u4e3a82CC\uff0c\u9762\u79ef\u5f00\u9500\u4e3a207um\u00b2\u3002", "conclusion": "Torrent\u901a\u8fc7Chainwrite\u673a\u5236\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u70b9\u5bf9\u591a\u70b9\u6570\u636e\u4f20\u8f93\uff0c\u65e0\u9700\u4fee\u6539NoC\u786c\u4ef6\u548c\u4e92\u8fde\u534f\u8bae\uff0c\u5728\u4fdd\u6301\u6700\u5c0f\u786c\u4ef6\u5f00\u9500\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2512.17506", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.17506", "abs": "https://arxiv.org/abs/2512.17506", "authors": ["Brienna M. Larrick", "L. Philip Schumm", "Mingfei Shao", "Craig Barnes", "Anthony Juehne", "Hara Prasad Juvvla", "Michael B. Kranz", "Michael Lukowski", "Clint Malson", "Jessica N. Mazerik", "Christopher G. Meyer", "Jawad Qureshi", "Erin Spaniol", "Andrea Tentner", "Alexander VanTol", "Peter Vassilatos", "Sara Volk de Garcia", "Robert L. Grossman"], "title": "The HEAL Data Platform", "comment": "12 pages, 3 figures", "summary": "Objective: The objective was to develop a cloud-based, federated system to serve as a single point of search, discovery and analysis for data generated under the NIH Helping to End Addiction Long-term (HEAL) Initiative.\n  Materials and methods: The HEAL Data Platform is built on the open source Gen3 platform, utilizing a small set of framework services and exposed APIs to interoperate with both NIH and non-NIH data repositories. Framework services include those for authentication and authorization, creating persistent identifiers for data objects, and adding and updating metadata.\n  Results: The HEAL Data Platform serves as a single point of discovery of over one thousand studies funded under the HEAL Initiative. With hundreds of users per month, the HEAL Data Platform provides rich metadata and interoperates with data repositories and commons to provide access to shared datasets. Secure, cloud-based compute environments that are integrated with STRIDES facilitate secondary analysis of HEAL data. The HEAL Data Platform currently interoperates with nineteen data repositories.\n  Discussion: Studies funded under the HEAL Initiative generate a wide variety of data types, which are deposited across multiple NIH and third-party data repositories. The mesh architecture of the HEAL Data Platform provides a single point of discovery of these data resources, accelerating and facilitating secondary use.\n  Conclusion: The HEAL Data Platform enables search, discovery, and analysis of data that are deposited in connected data repositories and commons. By ensuring that these data are fully Findable, Accessible, Interoperable and Reusable (FAIR), the HEAL Data Platform maximizes the value of data generated under the HEAL Initiative.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u4e91\u7684\u8054\u90a6\u6570\u636e\u5e73\u53f0\uff0c\u4f5c\u4e3aNIH HEAL\u8ba1\u5212\u6570\u636e\u7684\u7edf\u4e00\u641c\u7d22\u3001\u53d1\u73b0\u548c\u5206\u6790\u5165\u53e3\uff0c\u4fc3\u8fdb\u6570\u636e\u5171\u4eab\u548c\u4e8c\u6b21\u5229\u7528\u3002", "motivation": "HEAL\u8ba1\u5212\u4ea7\u751f\u7684\u6570\u636e\u5206\u6563\u5728\u591a\u4e2aNIH\u548c\u7b2c\u4e09\u65b9\u6570\u636e\u4ed3\u5e93\u4e2d\uff0c\u9700\u8981\u7edf\u4e00\u5e73\u53f0\u6765\u53d1\u73b0\u548c\u8bbf\u95ee\u8fd9\u4e9b\u591a\u6837\u5316\u6570\u636e\u8d44\u6e90\u3002", "method": "\u57fa\u4e8e\u5f00\u6e90Gen3\u5e73\u53f0\u6784\u5efa\uff0c\u91c7\u7528\u7f51\u72b6\u67b6\u6784\uff0c\u96c6\u6210\u8ba4\u8bc1\u6388\u6743\u3001\u6301\u4e45\u6807\u8bc6\u7b26\u3001\u5143\u6570\u636e\u7ba1\u7406\u7b49\u6846\u67b6\u670d\u52a1\uff0c\u4e0e19\u4e2a\u6570\u636e\u4ed3\u5e93\u4e92\u64cd\u4f5c\u3002", "result": "\u5e73\u53f0\u5df2\u6536\u5f551000\u591a\u9879HEAL\u7814\u7a76\uff0c\u6bcf\u6708\u6570\u767e\u7528\u6237\u4f7f\u7528\uff0c\u63d0\u4f9b\u4e30\u5bcc\u5143\u6570\u636e\uff0c\u96c6\u6210\u5b89\u5168\u4e91\u8ba1\u7b97\u73af\u5883\u652f\u6301\u4e8c\u6b21\u5206\u6790\u3002", "conclusion": "HEAL\u6570\u636e\u5e73\u53f0\u901a\u8fc7\u7edf\u4e00\u53d1\u73b0\u548cFAIR\u539f\u5219\u6700\u5927\u5316\u6570\u636e\u4ef7\u503c\uff0c\u52a0\u901f\u6570\u636e\u5171\u4eab\u548c\u4e8c\u6b21\u5229\u7528\uff0c\u652f\u6301\u6210\u763e\u7814\u7a76\u3002"}}
{"id": "2512.17834", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.17834", "abs": "https://arxiv.org/abs/2512.17834", "authors": ["Darja Nonaca", "J\u00e9r\u00e9my Guichemerre", "Reinhard Wiesmayr", "Nihat Engin Tunali", "Christoph Studer"], "title": "A 14ns-Latency 9Gb/s 0.44mm$^2$ 62pJ/b Short-Blocklength LDPC Decoder ASIC in 22FDX", "comment": "Presented at the 2025 IEEE European Solid-State Electronics Research Conference (ESSERC)", "summary": "Ultra-reliable low latency communication (URLLC) is a key part of 5G wireless systems. Achieving low latency necessitates codes with short blocklengths for which polar codes with successive cancellation list (SCL) decoding typically outperform message-passing (MP)-based decoding of low-density parity-check (LDPC) codes. However, SCL decoders are known to exhibit high latency and poor area efficiency. In this paper, we propose a new short-blocklength multi-rate binary LDPC code that outperforms the 5G-LDPC code for the same blocklength and is suitable for URLLC applications using fully parallel MP. To demonstrate our code's efficacy, we present a 0.44mm$^2$ GlobalFoundries 22FDX LDPC decoder ASIC which supports three rates and achieves the lowest-in-class decoding latency of 14ns while reaching an information throughput of 9Gb/s at 62pJ/b energy efficiency for a rate-1/2 code with 128-bit blocklength.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u77ed\u5757\u957f\u591a\u901f\u7387\u4e8c\u8fdb\u5236LDPC\u7801\uff0c\u5728URLLC\u5e94\u7528\u4e2d\u4f18\u4e8e5G-LDPC\u7801\uff0c\u5e76\u901a\u8fc7ASIC\u5b9e\u73b014ns\u6700\u4f4e\u5ef6\u8fdf\u89e3\u7801", "motivation": "URLLC\u9700\u8981\u77ed\u5757\u957f\u7f16\u7801\uff0c\u4f20\u7edfpolar\u7801SCL\u89e3\u7801\u5ef6\u8fdf\u9ad8\u3001\u9762\u79ef\u6548\u7387\u5dee\uff0c\u800cLDPC\u7801\u7684MP\u89e3\u7801\u5728\u77ed\u5757\u957f\u4e0b\u6027\u80fd\u4e0d\u8db3", "method": "\u8bbe\u8ba1\u65b0\u578b\u77ed\u5757\u957f\u591a\u901f\u7387\u4e8c\u8fdb\u5236LDPC\u7801\uff0c\u91c7\u7528\u5168\u5e76\u884c\u6d88\u606f\u4f20\u9012\u89e3\u7801\u67b6\u6784\uff0c\u5b9e\u73b0ASIC\u786c\u4ef6\u5b9e\u73b0", "result": "\u63d0\u51fa\u7684LDPC\u7801\u5728\u76f8\u540c\u5757\u957f\u4e0b\u4f18\u4e8e5G-LDPC\u7801\uff0cASIC\u89e3\u7801\u5668\u9762\u79ef0.44mm\u00b2\uff0c\u5ef6\u8fdf\u4ec514ns\uff0c\u4fe1\u606f\u541e\u5410\u91cf9Gb/s\uff0c\u80fd\u654862pJ/b", "conclusion": "\u65b0\u578bLDPC\u7801\u7ed3\u5408\u5168\u5e76\u884cMP\u89e3\u7801\u4e3aURLLC\u5e94\u7528\u63d0\u4f9b\u4e86\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u73b0\u67095G-LDPC\u65b9\u6848"}}
{"id": "2512.17574", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.17574", "abs": "https://arxiv.org/abs/2512.17574", "authors": ["Lingxiao Zhao", "Haoran Zhou", "Yuezhi Che", "Dazhao Cheng"], "title": "Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing", "comment": null, "summary": "Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.\n  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\\times$ more requests or enforce 1.5$\\times$ tighter SLOs, while achieving up to 4.4$\\times$ higher throughput compared to state-of-the-art systems.", "AI": {"tldr": "FlashCodec\u548cUnifiedServe\u8054\u5408\u4f18\u5316\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7GPU\u534f\u4f5c\u89e3\u7801\u964d\u4f4e\u5ef6\u8fdf\uff0c\u89e3\u8026\u89c6\u89c9\u7f16\u7801\u4e0eLLM\u63a8\u7406\u6d88\u9664\u963b\u585e\uff0c\u5b9e\u73b0\u541e\u5410\u91cf\u63d0\u53474.4\u500d", "motivation": "\u73b0\u6709MLLM\u63a8\u7406\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u74f6\u9888\uff1a1\uff09\u591a\u6a21\u6001\u9884\u5904\u7406\uff08\u7279\u522b\u662f\u89c6\u9891\u89e3\u7801\uff09\u5728CPU\u4e0a\u6267\u884c\u5bfc\u81f4\u9996token\u5ef6\u8fdf\u9ad8\uff1b2\uff09\u89c6\u89c9\u7f16\u7801\u5668\u4e0eLLM\u63a8\u7406\u9636\u6bb5\u5f02\u6784\uff0c\u9020\u6210\u963b\u585e\u548c\u8d44\u6e90\u5229\u7528\u7387\u4f4e", "method": "\u63d0\u51faFlashCodec\uff08\u591aGPU\u534f\u4f5c\u89c6\u9891\u89e3\u7801\uff09\u548cUnifiedServe\uff08\u903b\u8f91\u89e3\u8026\u4f46\u7269\u7406\u5171\u4eabGPU\u8d44\u6e90\uff09\u4e24\u4e2a\u4e92\u8865\u8bbe\u8ba1\uff0c\u8054\u5408\u4f18\u5316\u7aef\u5230\u7aefMLLM\u6d41\u6c34\u7ebf", "result": "\u7cfb\u7edf\u53ef\u670d\u52a13.0\u500d\u66f4\u591a\u8bf7\u6c42\u6216\u6ee1\u8db31.5\u500d\u66f4\u4e25\u683c\u7684SLO\u8981\u6c42\uff0c\u76f8\u6bd4SOTA\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u8fbe4.4\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347", "conclusion": "\u901a\u8fc7\u534f\u540c\u4f18\u5316\u591a\u6a21\u6001\u9884\u5904\u7406\u548c\u63a8\u7406\u9636\u6bb5\uff0c\u663e\u8457\u63d0\u5347MLLM\u63a8\u7406\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u670d\u52a1\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
