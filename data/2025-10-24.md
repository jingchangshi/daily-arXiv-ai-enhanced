<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs](https://arxiv.org/abs/2510.19850)
*Mostapha Kalami Heris*

Main category: cs.PL

TL;DR: Prompt Decorators是一种声明式、可组合的语法框架，通过紧凑的控制令牌来管理LLM行为，将任务意图与执行行为解耦，提高提示设计的可重用性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统提示工程依赖冗长的自然语言指令，限制了可重现性、模块化和可解释性，用户缺乏对LLM推理和输出表达方式的一致控制。

Method: 引入Prompt Decorators框架，包含20个核心装饰器，分为认知与生成、表达与系统两个功能家族，定义统一语法、作用域模型和确定性处理流程。

Result: 使用案例显示提高了推理透明度、减少了提示复杂性，并实现了跨领域的标准化模型行为。

Conclusion: 该框架对互操作性、行为一致性以及可扩展AI系统声明式接口的开发具有重要意义。

Abstract: Large Language Models (LLMs) are central to reasoning, writing, and
decision-support workflows, yet users lack consistent control over how they
reason and express outputs. Conventional prompt engineering relies on verbose
natural-language instructions, limiting reproducibility, modularity, and
interpretability. This paper introduces Prompt Decorators, a declarative,
composable syntax that governs LLM behavior through compact control tokens such
as +++Reasoning, +++Tone(style=formal), and +++Import(topic="Systems
Thinking"). Each decorator modifies a behavioral dimension, such as reasoning
style, structure, or tone, without changing task content. The framework
formalizes twenty core decorators organized into two functional families
(Cognitive & Generative and Expressive & Systemic), each further decomposed
into subcategories that govern reasoning, interaction, expression, and
session-control. It defines a unified syntax, scoping model, and deterministic
processing pipeline enabling predictable and auditable behavior composition. By
decoupling task intent from execution behavior, Prompt Decorators create a
reusable and interpretable interface for prompt design. Illustrative use cases
demonstrate improved reasoning transparency, reduced prompt complexity, and
standardized model behavior across domains. The paper concludes with
implications for interoperability, behavioral consistency, and the development
of declarative interfaces for scalable AI systems.

</details>


### [2] [A Specification's Realm: Characterizing the Knowledge Required for Executing a Given Algorithm Specification](https://arxiv.org/abs/2510.19853)
*Assaf Marron,David Harel*

Main category: cs.PL

TL;DR: 本文提出了算法规范的"领域"概念，即执行算法所需的前提知识集合，包括语法语义、领域知识、实体关系等，并探讨了如何系统化生成这种领域文档以及评估执行忠实度的方法。


<details>
  <summary>Details</summary>
Motivation: 当前算法规范在自然语言或伪代码中往往不够明确，无法确保机械执行的一致性。本文旨在明确算法执行所需的前提知识，使算法规范能够独立于具体系统实现而被忠实执行。

Method: 提出了算法规范"领域"的概念，将其定义为执行算法所需的前提知识集合。开发了系统化的分析方法来生成领域文档，包括利用大语言模型和重用现有文档实现部分自动化。

Result: 建立了算法规范领域的理论框架，包含语法语义、领域知识、实体关系、因果关系规则等要素。提出了生成领域文档的系统方法，并探讨了执行忠实度的评估问题。

Conclusion: 算法规范领域的概念有助于实现算法在不同系统中的方法化实现和形式化验证。该框架为解决算法执行的一致性和忠实度问题提供了理论基础和实践路径。

Abstract: An algorithm specification in natural language or pseudocode is expected to
be clear and explicit enough to enable mechanical execution. In this position
paper we contribute an initial characterization of the knowledge that an
executing agent, human or machine, should possess in order to be able to carry
out the instructions of a given algorithm specification as a stand-alone
entity, independent of any system implementation. We argue that, for that
algorithm specification, such prerequisite knowledge, whether unique or shared
with other specifications, can be summarized in a document of practical size.
We term this document the realm of the algorithm specification. The generation
of such a realm is itself a systematic analytical process, significant parts of
which can be automated with the help of large language models and the reuse of
existing documents. The algorithm-specification's realm would consist of
specification language syntax and semantics, domain knowledge restricted to the
referenced entities, inter-entity relationships, relevant underlying
cause-and-effect rules, and detailed instructions and means for carrying out
certain operations. Such characterization of the realm can contribute to
methodological implementation of the algorithm specification in diverse systems
and to its formalization for mechanical verification. The paper also touches
upon the question of assessing execution faithfulness, which is distinct from
correctness: in the absence of a reference interpretation of natural language
or pseudocode specification with a given vocabulary, how can we determine if an
observed agent's execution indeed complies with the input specification.

</details>


### [3] [Deconstructed Proto-Quipper: A Rational Reconstruction](https://arxiv.org/abs/2510.20018)
*Ryan Kavanagh,Chuta Sano,Brigitte Pientka*

Main category: cs.PL

TL;DR: Proto-Quipper-A是一个重新设计的量子编程语言基础，使用线性λ演算和伴随逻辑来简化Proto-Quipper语言的复杂操作语义，使其具有简单的归约语义和可证明的归一化性质。


<details>
  <summary>Details</summary>
Motivation: Proto-Quipper语言具有复杂的操作语义，涉及集合论操作和新鲜名称生成，这使得使用标准编程语言技术进行推理和机械化变得困难。

Method: 使用线性λ演算描述量子电路，其范式与盒线电路图紧密对应；通过伴随逻辑基础将电路语言与线性/非线性函数语言集成，重构Proto-Quipper的电路编程抽象。

Result: Proto-Quipper-A具有简单的按值调用归约语义，并且被证明是归一化的；展示了如何使用标准逻辑关系来证明线性和子结构系统的归一化。

Conclusion: Proto-Quipper-A为Proto-Quipper语言提供了一个更易处理的基础，避免了现有线性逻辑关系的固有复杂性。

Abstract: The Proto-Quipper family of programming languages aims to provide a formal
foundation for the Quipper quantum programming language. Unfortunately,
Proto-Quipper languages have complex operational semantics: they are inherently
effectful, and they rely on set-theoretic operations and fresh name generation
to manipulate quantum circuits. This makes them difficult to reason about using
standard programming language techniques and, ultimately, to mechanize. We
introduce Proto-Quipper-A, a rational reconstruction of Proto-Quipper languages
for static circuit generation. It uses a linear $\lambda$-calculus to describe
quantum circuits with normal forms that closely correspond to box-and-wire
circuit diagrams. Adjoint-logical foundations integrate this circuit language
with a linear/non-linear functional language and let us reconstruct
Proto-Quipper's circuit programming abstractions using more primitive
adjoint-logical operations. Proto-Quipper-A enjoys a simple call-by-value
reduction semantics, and to illustrate its tractability as a foundation for
Proto-Quipper languages, we show that it is normalizing. We show how to use
standard logical relations to prove normalization of linear and substructural
systems, thereby avoiding the inherent complexity of existing linear logical
relations.

</details>


### [4] [Deciding not to Decide: Sound and Complete Effect Inference in the Presence of Higher-Rank Polymorphism](https://arxiv.org/abs/2510.20532)
*Patrycja Balik,Szymon Jędras,Piotr Polesiuk*

Main category: cs.PL

TL;DR: 提出了一种用于类型和效应系统的效应推断算法，该系统具有子类型、高阶多态性和直观的集合语义。通过将效应约束转换为命题逻辑公式来处理作用域问题，并证明了算法的完备性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统的类型和效应系统由于复杂性和现有推断算法在表达能力、直观性和可判定性之间的权衡，尚未获得广泛应用。

Method: 将效应约束转换为命题逻辑公式，延迟求解以处理高阶多态性的作用域问题。

Result: 算法在Rocq证明助手中形式化，并成功在现实编程语言中实现。

Conclusion: 该工作为类型和效应系统提供了一种具有表达力、直观性和可判定性的推断算法。

Abstract: Type-and-effect systems help the programmer to organize data and
computational effects in a program. While for traditional type systems
expressive variants with sophisticated inference algorithms have been developed
and widely used in programming languages, type-and-effect systems did not yet
gain widespread adoption. One reason for this is that type-and-effect systems
are more complex and the existing inference algorithms make compromises between
expressiveness, intuitiveness, and decidability. In this work, we present an
effect inference algorithm for a type-and-effect system with subtyping,
expressive higher-rank polymorphism, and intuitive set-like semantics of
effects. In order to deal with scoping issues of higher-rank polymorphism, we
delay solving of effect constraints by transforming them into formulae of
propositional logic. We prove soundness and completeness of our algorithm with
respect to a declarative type-and-effect system. All the presented results have
been formalized in the Rocq proof assistant, and the algorithm has been
successfully implemented in a realistic programming language.

</details>


### [5] [Compiling the Mimosa programming language to RTOS tasks](https://arxiv.org/abs/2510.20547)
*Nikolaus Huber,Susanne Graf,Philipp Rümmer,Wang Yi*

Main category: cs.PL

TL;DR: 提出了Mimosa编程语言的编译方案，基于MIMOS计算模型，将嵌入式系统软件描述为通过FIFO队列通信的时间触发进程集合


<details>
  <summary>Details</summary>
Motivation: 为Mimosa语言提供形式化的编译方案，将协调层映射到实时操作系统原语

Method: 采用Lustre编译方案的适配版本，针对Mimosa语义进行形式化描述

Result: 成功开发了Mimosa语言的编译方案，能够将程序映射到实时操作系统

Conclusion: 该编译方案为基于MIMOS模型的嵌入式系统软件开发提供了有效的实现途径

Abstract: This paper introduces a compilation scheme for programs written in the Mimosa
programming language, which builds upon the MIMOS model of computation. Mimosa
describes embedded systems software as a collection of time-triggered processes
which communicate through FIFO queues. We formally describe an adaptation of
the Lustre compilation scheme to the semantics of Mimosa and show how the
coordination layer can be mapped to real-time operating system primitives.

</details>


### [6] [SafeFFI: Efficient Sanitization at the Boundary Between Safe and Unsafe Code in Rust and Mixed-Language Applications](https://arxiv.org/abs/2510.20688)
*Oliver Braunsdorf,Tim Lange,Konrad Hohentanner,Julian Horsch,Johannes Kinder*

Main category: cs.PL

TL;DR: SafeFFI是一种优化Rust二进制文件中内存安全检测的系统，通过在unsafe和safe代码边界进行检查，将内存安全执行从检测器转移到Rust类型系统，显著减少检测开销。


<details>
  <summary>Details</summary>
Motivation: Unsafe Rust代码在与C/C++库互操作和实现底层数据结构时是必要的，但可能导致内存安全违规。现有检测器会引入大量不必要的检查，即使对Rust类型系统保证安全的访问也是如此。

Method: 设计了一种优化内存安全检测的系统，在unsafe和safe代码边界进行检查，避免昂贵的全程序分析，减少编译时开销。

Result: 与现有最先进系统相比，SafeFFI实现了更优的性能，减少了高达98%的检测器检查，同时保持正确性并标记所有空间和时间内存安全违规。

Conclusion: SafeFFI通过将内存安全检查集中在unsafe代码边界，显著提高了Rust程序的内存安全检测效率，同时保持了检测的完整性。

Abstract: Unsafe Rust code is necessary for interoperability with C/C++ libraries and
implementing low-level data structures, but it can cause memory safety
violations in otherwise memory-safe Rust programs. Sanitizers can catch such
memory errors at runtime, but introduce many unnecessary checks even for memory
accesses guaranteed safe by the Rust type system. We introduce SafeFFI, a
system for optimizing memory safety instrumentation in Rust binaries such that
checks occur at the boundary between unsafe and safe code, handing over the
enforcement of memory safety from the sanitizer to the Rust type system. Unlike
previous approaches, our design avoids expensive whole-program analysis and
adds much less compile-time overhead (2.64x compared to over 8.83x). On a
collection of popular Rust crates and known vulnerable Rust code, SafeFFI
achieves superior performance compared to state-of-the-art systems, reducing
sanitizer checks by up to 98%, while maintaining correctness and flagging all
spatial and temporal memory safety violations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [New Hardness Results for the LOCAL Model via a Simple Self-Reduction](https://arxiv.org/abs/2510.19972)
*Alkida Balliu,Filippo Casagrande,Francesco d'Amore,Dennis Olivetti*

Main category: cs.DC

TL;DR: 本文简化了Khoury和Schild的轮消除自约简技术，并应用该技术证明了最大b匹配问题和边着色问题的随机LOCAL算法下界。


<details>
  <summary>Details</summary>
Motivation: Khoury和Schild关于最大匹配的随机LOCAL算法下界证明虽然漂亮但技术复杂（超过25页），难以理解和推广。历史上证明简化对理解图问题复杂度有重要意义，本文旨在简化该技术并推广到其他问题。

Method: 提出了轮消除自约简技术的简化版本，并应用该技术分析最大b匹配和边着色问题。

Result: 1. 最大b匹配问题需要Ω(min{log₁₊bΔ, logΔn})和Ω(√log₁₊bn)轮；2. 边着色问题需要Ω(min{logΔ, logΔn})和Ω(√logn)轮。

Conclusion: 成功简化了轮消除自约简技术，并用该技术获得了最大b匹配和边着色问题的下界结果，为b=1时提供了最大匹配下界的简短证明。

Abstract: Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL
algorithm that solves maximal matching requires $\Omega(\min\{\log \Delta,
\log_\Delta n\})$ rounds, where $n$ is the number of nodes in the graph and
$\Delta$ is the maximum degree. This result is shown through a new technique,
called round elimination via self-reduction. The lower bound proof is beautiful
and presents very nice ideas. However, it spans more than 25 pages of technical
details, and hence it is hard to digest and generalize to other problems.
Historically, the simplification of proofs and techniques has marked an
important turning point in our understanding of the complexity of graph
problems. Our paper makes a step forward towards this direction, and provides
the following contributions.
  1. We present a short and simplified version of the round elimination via
self-reduction technique. The simplification of this technique enables us to
obtain the following two hardness results.
  2. We show that any randomized LOCAL algorithm that solves the maximal
$b$-matching problem requires $\Omega(\min\{\log_{1+b}\Delta, \log_\Delta n\})$
and $\Omega(\sqrt{\log_{1+b} n})$ rounds. We recall that the $b$-matching
problem is a generalization of the matching problem where each vertex can have
up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain
a short proof for the maximal matching lower bound shown by Khoury and Schild.
  3. Finally, we show that any randomized LOCAL algorithm that properly colors
the edges of a graph with $\Delta + k$ colors requires $\Omega(\min\{\log
\Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log n})$ rounds, for any $k\le
\Delta^{1-\varepsilon}$ and any constant $\varepsilon > 0$.

</details>


### [8] [AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training](https://arxiv.org/abs/2510.20111)
*Huawei Bai,Yifan Huang,Wenqi Shi,Ansheng You,Feifan Shao,Tengfei Han,Minghui Yu*

Main category: cs.DC

TL;DR: 提出AsyncHZP，一种异步分层零并行方法，通过自适应分片和异步调度优化内存利用和通信开销，在大规模训练中实现高性能。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在大规模集群上的训练效率和可扩展性存在瓶颈，主流ND并行方法复杂繁琐，而Zero Redundancy Optimizer (ZeRO)等方法受通信开销限制。

Method: AsyncHZP采用自适应参数分片策略，在不同副本组间重新分片参数、梯度和优化器状态；设计多流异步调度方法，在专用后台线程中执行参数all-gather和梯度reduce-scatter操作。

Result: 在Dense和MoE模型上的实证评估表明，AsyncHZP在大规模下保持稳健稳定性，持续优于经典ND并行，无需复杂策略调优即可实现最先进性能。

Conclusion: AsyncHZP通过简化大规模训练路径，在保持内存效率的同时实现了卓越性能，解决了现有并行方法的通信瓶颈问题。

Abstract: The training efficiency and scalability of language models on massive
clusters currently remain a critical bottleneck. Mainstream approaches like ND
parallelism are often cumbersome and complex, while flexible alternatives such
as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by
communication overhead. In this paper, we propose Asynchronous Hierarchical
Zero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to
achieve superior performance while maintaining simplicity and memory
efficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding
that can lead to inefficient communication, AsyncHZP adaptively reshards
parameters, gradients, and optimizer states across different replica groups.
This strategy optimizes device memory utilization and significantly reduces
communication overhead. In addition, we also design a multi-stream asynchronous
scheduling method that executes parameter all-gather and gradient
reduce-scatter operations in dedicated background threads, effectively
overlapping communication with computation while incurring negligible memory
fragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)
models confirm that AsyncHZP maintains robust stability at scale. It
consistently outperforms classic ND parallelism, achieving state-of-the-art
performance without complex strategic tuning, thereby simplifying the path to
efficient large-scale training.

</details>


### [9] [A Full Stack Framework for High Performance Quantum-Classical Computing](https://arxiv.org/abs/2510.20128)
*Xin Zhan,K. Grace Johnson,Aniello Esposito,Barbara Chapman,Marco Fiorentino,Kirk M. Bresniker,Raymond G. Beausoleil,Masoud Mohseni*

Main category: cs.DC

TL;DR: 提出了一个HPC-QC全栈框架，用于集成高性能计算和量子计算，支持混合工作负载开发，采用模块化硬件/设备无关的软件集成方法。


<details>
  <summary>Details</summary>
Motivation: 满足对可扩展高性能计算和量子计算集成日益增长的需求，解决大规模量子电路在较小噪声量子设备上的执行问题。

Method: 开发了自适应电路编织虚拟机，将大量子电路分割为适合小设备的子电路；利用Cray LLVM编译框架转换LLVM IR和量子IR；在HPE EX超级计算机上演示混合工作负载。

Result: 成功演示了多个混合HPC-QC多节点多CPU和GPU工作负载，包括解线性方程组、量子优化和模拟量子相变，验证了所有三个开发组件的功能和执行可行性。

Conclusion: 该工作为基于经典HPC软件栈的统一量子-经典编程环境提供了框架，实现了量子内核从商业量子SDK到HPC元程序的可移植调用。

Abstract: To address the growing needs for scalable High Performance Computing (HPC)
and Quantum Computing (QC) integration, we present our HPC-QC full stack
framework and its hybrid workload development capability with modular
hardware/device-agnostic software integration approach. The latest development
in extensible interfaces for quantum programming, dispatching, and compilation
within existing mature HPC programming environment are demonstrated. Our HPC-QC
full stack enables high-level, portable invocation of quantum kernels from
commercial quantum SDKs within HPC meta-program in compiled languages (C/C++
and Fortran) as well as Python through a quantum programming interface library
extension. An adaptive circuit knitting hypervisor is being developed to
partition large quantum circuits into sub-circuits that fit on smaller noisy
quantum devices and classical simulators. At the lower-level, we leverage Cray
LLVM-based compilation framework to transform and consume LLVM IR and Quantum
IR (QIR) from commercial quantum software frontends in a retargetable fashion
to different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU
and GPU workloads (including solving linear system of equations, quantum
optimization, and simulating quantum phase transitions) have been demonstrated
on HPE EX supercomputers to illustrate functionality and execution viability
for all three components developed so far. This work provides the framework for
a unified quantum-classical programming environment built upon classical HPC
software stack (compilers, libraries, parallel runtime and process scheduling).

</details>


### [10] [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)
*Min Si,Pavan Balaji,Yongzhou Chen,Ching-Hsiang Chu,Adi Gangidi,Saif Hasan,Subodh Iyengar,Dan Johnson,Bingzhe Liu,Jingliang Ren,Ashmitha Jeevaraj Shetty,Greg Steinbrecher,Xinfeng Xie,Yulun Wang,Bruce Wu,Jingyi Yang,Mingran Yang,Minlan Yu,Cen Zhao,Wes Bland,Denis Boyda,Suman Gumudavelli,Cristian Lumezanu,Rui Miao,Zhe Qu,Venkat Ramesh,Maxim Samoylov,Jan Seidel,Feng Tian,Qiye Tan,Shuqiang Zhang,Yimeng Zhao,Shengbao Zheng,Art Zhu,Hongyi Zeng*

Main category: cs.DC

TL;DR: NCCLX是一个专为超大规模LLM训练和推理优化的集体通信框架，支持超过10万GPU集群，显著提升通信效率。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模扩大，传统通信方法在数十万GPU规模下面临吞吐量和延迟限制，阻碍了最先进模型的开发和部署。

Method: 开发了NCCLX集体通信框架，针对LLM全生命周期优化，支持同步训练和低延迟推理的复杂工作负载。

Result: 在Llama4模型上的实证评估显示通信效率显著提升。

Conclusion: 该研究为下一代LLM在空前规模下运行提供了强大的解决方案。

Abstract: The increasing scale of large language models (LLMs) necessitates highly
efficient collective communication frameworks, particularly as training
workloads extend to hundreds of thousands of GPUs. Traditional communication
methods face significant throughput and latency limitations at this scale,
hindering both the development and deployment of state-of-the-art models. This
paper presents the NCCLX collective communication framework, developed at Meta,
engineered to optimize performance across the full LLM lifecycle, from the
synchronous demands of large-scale training to the low-latency requirements of
inference. The framework is designed to support complex workloads on clusters
exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency
data exchange. Empirical evaluation on the Llama4 model demonstrates
substantial improvements in communication efficiency. This research contributes
a robust solution for enabling the next generation of LLMs to operate at
unprecedented scales.

</details>


### [11] [FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services](https://arxiv.org/abs/2510.20388)
*Víctor Rampérez,Javier Soriano,David Lizcano,Juan A. Lara*

Main category: cs.DC

TL;DR: FLAS是一个结合了主动和被动方法的自动扩缩容系统，通过预测高层指标趋势和基于资源使用指标的被动应急系统，为分布式服务提供优化的扩缩容决策。


<details>
  <summary>Details</summary>
Motivation: 云计算弹性特性需要自动扩缩容系统来按需获取和释放资源，确保服务水平协议。现有方法在应对突发负载变化时存在不足。

Method: 结合主动预测（预测高层指标趋势）和被动应急（从资源使用指标估计高层指标）的方法，减少必要监测，适用于不同应用。

Result: 在多种测试场景下验证，包括最坏情况场景，确保99%以上的时间满足性能要求。

Conclusion: FLAS是首个面向内容发布订阅分布式系统的自动扩缩容系统，通用性强，能有效确保性能要求。

Abstract: Cloud computing has established itself as the support for the vast majority
of emerging technologies, mainly due to the characteristic of elasticity it
offers. Auto-scalers are the systems that enable this elasticity by acquiring
and releasing resources on demand to ensure an agreed service level. In this
article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for
distributed services that combines the advantages of proactive and reactive
approaches according to the situation to decide the optimal scaling actions in
every moment. The main novelties introduced by FLAS are (i) a predictive model
of the high-level metrics trend which allows to anticipate changes in the
relevant SLA parameters (e.g. performance metrics such as response time or
throughput) and (ii) a reactive contingency system based on the estimation of
high-level metrics from resource use metrics, reducing the necessary
instrumentation (less invasive) and allowing it to be adapted agnostically to
different applications. We provide a FLAS implementation for the use case of a
content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone
of an event-driven architecture. To the best of our knowledge, this is the
first auto-scaling system for content-based publish-subscribe distributed
systems (although it is generic enough to fit any distributed service). Through
an evaluation based on several test cases recreating not only the expected
contexts of use, but also the worst possible scenarios (following the
Boundary-Value Analysis or BVA test methodology), we have validated our
approach and demonstrated the effectiveness of our solution by ensuring
compliance with performance requirements over 99% of the time.

</details>


### [12] [Accurate Performance Predictors for Edge Computing Applications](https://arxiv.org/abs/2510.20495)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 提出一种在动态边缘环境中自动构建和评估性能预测器的方法，通过联合优化准确性和推理时间，实现高达90%的预测准确率，推理时间小于往返时间的1%。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的动态边缘环境中，由于多应用共置和节点异构性，实现可预测的性能具有挑战性，这对有效的调度和资源管理至关重要。

Method: 自动构建和评估各种性能预测器，优先考虑准确性和推理时间，选择最有效的模型。预测器基于与应用程序性能最相关的监控指标历史状态进行训练，并在动态共置场景下评估。

Result: 预测器达到高达90%的准确率，推理时间小于往返时间的1%。在电子显微镜工作流等实时需求严格的应用中表现良好。

Conclusion: 需要系统化方法在动态共置场景中通过联合优化准确性和推理延迟来选择服务器特定的预测器，集成此类预测器可提高资源利用率并实现可预测性能。

Abstract: Accurate prediction of application performance is critical for enabling
effective scheduling and resource management in resource-constrained dynamic
edge environments. However, achieving predictable performance in such
environments remains challenging due to the co-location of multiple
applications and the node heterogeneity. To address this, we propose a
methodology that automatically builds and assesses various performance
predictors. This approach prioritizes both accuracy and inference time to
identify the most efficient model. Our predictors achieve up to 90% accuracy
while maintaining an inference time of less than 1% of the Round Trip Time.
These predictors are trained on the historical state of the most correlated
monitoring metrics to application performance and evaluated across multiple
servers in dynamic co-location scenarios. As usecase we consider electron
microscopy (EM) workflows, which have stringent real-time demands and diverse
resource requirements. Our findings emphasize the need for a systematic
methodology that selects server-specific predictors by jointly optimizing
accuracy and inference latency in dynamic co-location scenarios. Integrating
such predictors into edge environments can improve resource utilization and
result in predictable performance.

</details>


### [13] [Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing](https://arxiv.org/abs/2510.20506)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 该论文提出使用RTT预测器来改进负载均衡，通过预测应用延迟来优化请求路由，在Kubernetes管理的GPU集群中实现了高达95%的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 分布式应用对低端到端延迟的需求日益增长，传统负载均衡策略通常基于过时或粗粒度的指标，导致次优路由决策和尾部延迟增加。

Method: 开发轻量级准确的RTT预测器，基于从Kubernetes管理的GPU集群收集的时间序列监控数据进行训练，利用高度相关的监控指标子集。

Result: 预测器达到95%的准确率，预测延迟控制在应用RTT的10%以内，性能感知负载均衡显著降低了应用RTT并最小化资源浪费。

Conclusion: 研究证明了将预测性负载均衡集成到未来生产系统中的可行性，并确定了在资源受限集群中确保有效部署所需的最小预测准确率阈值和关键系统级因素。

Abstract: Distributed applications increasingly demand low end-to-end latency,
especially in edge and cloud environments where co-located workloads contend
for limited resources. Traditional load-balancing strategies are typically
reactive and rely on outdated or coarse-grained metrics, often leading to
suboptimal routing decisions and increased tail latencies. This paper
investigates the use of round-trip time (RTT) predictors to enhance request
routing by anticipating application latency. We develop lightweight and
accurate RTT predictors that are trained on time-series monitoring data
collected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of
highly correlated monitoring metrics, our approach maintains low overhead while
remaining adaptable to diverse co-location scenarios and heterogeneous
hardware. The predictors achieve up to 95% accuracy while keeping the
prediction delay within 10% of the application RTT. In addition, we identify
the minimum prediction accuracy threshold and key system-level factors required
to ensure effective predictor deployment in resource-constrained clusters.
Simulation-based evaluation demonstrates that performance-aware load balancing
can significantly reduce application RTT and minimize resource waste. These
results highlight the feasibility of integrating predictive load balancing into
future production systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [HALOC-AxA: An Area/-Energy-Efficient Approximate Adder for Image Processing Application](https://arxiv.org/abs/2510.20137)
*Hasnain A. Ziad,Ashiq A. Sakib*

Main category: cs.AR

TL;DR: 提出了一种新型近似加法器，比现有加法器更节能、面积更小，同时保持或提高了计算精度，适用于图像处理等多媒体应用。


<details>
  <summary>Details</summary>
Motivation: 为计算密集型多媒体应用（如图像、音频、视频处理）设计能量高效的硬件，平衡高性能、计算精度和能效之间的冲突需求。

Method: 设计了一种新型近似加法器，通过仿真验证其性能，并将其部署到图像处理任务中测试图像重建质量。

Result: 仿真结果显示该加法器在能量和面积效率上优于现有加法器，同时保持或提高了计算精度，能够高质量地重建图像。

Conclusion: 该新型近似加法器在能量效率、面积效率和计算精度方面表现优异，适用于多媒体处理应用。

Abstract: The design of approximate adders has been widely researched to advance
energy-efficient hardware for computation-intensive multimedia applications,
such as image, audio, or video processing. The design of approximate adders has
been widely researched to advance energy-efficient hardware for computation
intensive multimedia applications, such as image/audio/video processing.
Several static and dynamic approximate adders exist in the literature, each of
which endeavors to balance the conflicting demands of high performance,
computational accuracy, and energy efficiency. This work introduces a novel
approximate adder that is more energy- and area-efficient than existing adders,
while achieving improved or comparable accuracy, as demonstrated by simulation
results. The proposed adder's ability to digitally reconstruct high quality
images is further demonstrated by the deployment of the design for an image
processing task.

</details>


### [15] [In-DRAM True Random Number Generation Using Simultaneous Multiple-Row Activation: An Experimental Study of Real DRAM Chips](https://arxiv.org/abs/2510.20269)
*Ismail Emir Yuksel,Ataberk Olgun,F. Nisa Bostanci,Oguzhan Canpolat,Geraldo F. Oliveira,Mohammad Sadrosadati,Abdullah Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: 通过同时激活多行DRAM（SiMRA）技术，在商用DDR4 DRAM芯片中实现了高吞吐量、低延迟的真随机数生成，性能优于现有DRAM-based TRNG方法。


<details>
  <summary>Details</summary>
Motivation: 利用商用DRAM芯片的物理特性开发高效的真随机数生成器（TRNG），满足现代计算系统对高质量随机数的需求。

Method: 在96个DDR4 DRAM芯片上进行广泛表征，通过同时激活2、4、8、16、32行DRAM，分析不同数据模式、温度水平和空间变化对随机性的影响。

Result: 所有SiMRA-based TRNG设计都通过了NIST随机性测试；2、8、16、32行激活的吞吐量分别比现有最佳DRAM-based TRNG提高1.15x、1.99x、1.82x、1.39x；熵值随激活行数增加而增加；操作参数显著影响熵值。

Conclusion: SiMRA技术为在商用DRAM中实现高性能真随机数生成提供了可行方案，熵值受激活行数和环境条件影响，为未来研究提供了开源基础设施。

Abstract: In this work, we experimentally demonstrate that it is possible to generate
true random numbers at high throughput and low latency in commercial
off-the-shelf (COTS) DRAM chips by leveraging simultaneous multiple-row
activation (SiMRA) via an extensive characterization of 96 DDR4 DRAM chips. We
rigorously analyze SiMRA's true random generation potential in terms of
entropy, latency, and throughput for varying numbers of simultaneously
activated DRAM rows (i.e., 2, 4, 8, 16, and 32), data patterns, temperature
levels, and spatial variations. Among our 11 key experimental observations, we
highlight four key results. First, we evaluate the quality of our TRNG designs
using the commonly-used NIST statistical test suite for randomness and find
that all SiMRA-based TRNG designs successfully pass each test. Second, 2-, 8-,
16-, and 32-row activation-based TRNG designs outperform the state-of-theart
DRAM-based TRNG in throughput by up to 1.15x, 1.99x, 1.82x, and 1.39x,
respectively. Third, SiMRA's entropy tends to increase with the number of
simultaneously activated DRAM rows. Fourth, operational parameters and
conditions (e.g., data pattern and temperature) significantly affect entropy.
For example, for most of the tested modules, the average entropy of 32-row
activation is 2.51x higher than that of 2-row activation. For example,
increasing the temperature from 50{\deg}C to 90{\deg}C decreases SiMRA's
entropy by 1.53x for 32-row activation. To aid future research and development,
we open-source our infrastructure at https://github.com/CMU-SAFARI/SiMRA-TRNG.

</details>


### [16] [Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism on Dependency-Bound Kernels](https://arxiv.org/abs/2510.20400)
*Rubén Langarita,Jesús Alastruey-Benedé,Pablo Ibáñez-Marín,Santiago Marco-Sola,Miquel Moretó,Adrià Armejach*

Main category: cs.AR

TL;DR: Squire是一种通用加速器，旨在有效利用依赖密集型内核中的细粒度并行性，通过集成低功耗核心和直接L2缓存访问，在动态编程内核中实现最高7.64倍加速，端到端应用加速3.66倍，能耗降低56%，面积开销仅10.5%。


<details>
  <summary>Details</summary>
Motivation: 传统通用加速器（如SIMD、GPGPU）在处理复杂数据依赖模式时存在局限性，而定制FPGA/ASIC设计成本高且缺乏灵活性。需要一种能够有效处理依赖密集型内核的通用加速器解决方案。

Method: 每个Squire加速器包含一组通用低功耗顺序核心，这些核心能够快速相互通信并直接访问L2缓存。在典型多核系统中为每个核心集成一个Squire加速器，以最小的软件更改加速依赖密集型内核。

Result: 在实现复杂依赖模式的五个内核中，Squire在动态编程内核中获得最高7.64倍加速，端到端应用加速3.66倍，能耗降低高达56%，相比Neoverse-N1基线仅增加10.5%面积开销。

Conclusion: Squire提供了一种高效处理依赖密集型内核的通用加速器架构，在保持低硬件设计复杂性的同时，实现了显著的性能提升和能耗降低。

Abstract: Multiple HPC applications are often bottlenecked by compute-intensive kernels
implementing complex dependency patterns (data-dependency bound). Traditional
general-purpose accelerators struggle to effectively exploit fine-grain
parallelism due to limitations in implementing convoluted data-dependency
patterns (like SIMD) and overheads due to synchronization and data transfers
(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved
performance and energy efficiency at a high cost in hardware design and
programming complexity and often lack the flexibility to process different
workloads. We propose Squire, a general-purpose accelerator designed to exploit
fine-grain parallelism effectively on dependency-bound kernels. Each Squire
accelerator has a set of general-purpose low-power in-order cores that can
rapidly communicate among themselves and directly access data from the L2
cache. Our proposal integrates one Squire accelerator per core in a typical
multicore system, allowing the acceleration of dependency-bound kernels within
parallel tasks with minimal software changes. As a case study, we evaluate
Squire's effectiveness by accelerating five kernels that implement complex
dependency patterns. We use three of these kernels to build an end-to-end
read-mapping tool that will be used to evaluate Squire. Squire obtains speedups
up to 7.64$\times$ in dynamic programming kernels. Overall, Squire provides an
acceleration for an end-to-end application of 3.66$\times$. In addition, Squire
reduces energy consumption by up to 56% with a minimal area overhead of 10.5%
compared to a Neoverse-N1 baseline.

</details>
