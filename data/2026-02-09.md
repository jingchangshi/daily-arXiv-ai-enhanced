<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Protean Compiler: An Agile Framework to Drive Fine-grain Phase Ordering](https://arxiv.org/abs/2602.06142)
*Amir H. Ashouri,Shayan Shirahmad Gale Bagi,Kavin Satheeskumar,Tejas Srikanth,Jonathan Zhao,Ibrahim Saidoun,Ziwen Wang,Bryan Chan,Tomasz S. Czajkowski*

Main category: cs.PL

TL;DR: Protean Compiler是一个集成到LLVM中的敏捷框架，通过机器学习实现细粒度代码段的编译器优化阶段排序，相比O3优化平均提升4.1%，特定应用最高提升15.7%。


<details>
  <summary>Details</summary>
Motivation: 编译器优化阶段排序问题自1970年代以来一直是个挑战，传统手工调优方法需要大量人力且难以适应不同基准测试集的变化。现有机器学习方法未能无缝集成到编译器中，也无法在细粒度代码段级别应用。

Method: 提出Protean Compiler框架，无缝集成到LLVM中，支持细粒度代码段的优化阶段排序。包含140多种手工设计的静态特征收集方法，支持与第三方机器学习框架和大型语言模型集成。

Result: 在Cbench基准测试中，相比LLVM O3优化，平均获得4.1%的性能提升，特定应用最高提升15.7%，仅增加几秒构建时间。与第三方ML集成后，Susan和Jpeg应用分别获得10.1%和8.5%的性能提升。

Conclusion: Protean Compiler成功解决了编译器优化阶段排序的长期挑战，实现了细粒度、可扩展的优化决策，可作为增强版编译器使用，并计划开源。

Abstract: The phase ordering problem has been a long-standing challenge since the late 1970s, yet it remains an open problem due to having a vast optimization space and an unbounded nature, making it an open-ended problem without a finite solution, one can limit the scope by reducing the number and the length of optimizations. Traditionally, such locally optimized decisions are made by hand-coded algorithms tuned for a small number of benchmarks, often requiring significant effort to be retuned when the benchmark suite changes. In the past 20 years, Machine Learning has been employed to construct performance models to improve the selection and ordering of compiler optimizations, however, the approaches are not baked into the compiler seamlessly and never materialized to be leveraged at a fine-grained scope of code segments. This paper presents Protean Compiler: An agile framework to enable LLVM with built-in phase-ordering capabilities at a fine-grained scope. The framework also comprises a complete library of more than 140 handcrafted static feature collection methods at varying scopes, and the experimental results showcase speedup gains of up to 4.1% on average and up to 15.7% on select Cbench applications wrt LLVM's O3 by just incurring a few extra seconds of build time on Cbench. Additionally, Protean compiler allows for an easy integration with third-party ML frameworks and other Large Language Models, and this two-step optimization shows a gain of 10.1% and 8.5% speedup wrt O3 on Cbench's Susan and Jpeg applications. Protean compiler is seamlessly integrated into LLVM and can be used as a new, enhanced, full-fledged compiler. We plan to release the project to the open-source community in the near future.

</details>


### [2] [Uniqueness is Separation](https://arxiv.org/abs/2602.06386)
*Liam O'Connor,Pilar Selene Linares Arevalo,Christine Rizkallah*

Main category: cs.PL

TL;DR: 该论文提出将值独立性作为分离逻辑中的断言，以在形式验证中结合唯一性类型系统的推理优势与可变语义的实际需求。


<details>
  <summary>Details</summary>
Motivation: 唯一性类型系统虽然能通过值独立性简化程序推理，但其限制过于严格，实际软件常需要"逃生舱口"来放宽限制，这导致推理优势丧失。需要在可变语义中表达值独立性以保证形式验证。

Method: 提出将值独立性作为分离逻辑中的断言，将唯一性类型系统的推理优势转化为可在可变、命令式语义中使用的形式化断言。

Result: 该方法允许在保持程序效率（使用可变操作）的同时，在分离逻辑框架中表达值独立性，从而支持对混合保证系统的形式验证。

Conclusion: 通过将值独立性表达为分离逻辑断言，可以在实际软件的形式验证中结合唯一性类型系统的推理优势与可变语义的灵活性。

Abstract: Value independence is enormously beneficial for reasoning about software systems at scale. These benefits carry over into the world of formal verification. Reasoning about programs algebraically is a simple affair in a proof assistant, whereas programs with unconstrained mutation necessitate much more complex techniques, such as Separation Logic, where invariants about memory safety, aliasing, and state changes must be established by manual proof. Uniqueness type systems allow programs to be compiled to code that uses mutation for efficiency, while retaining a semantics that enjoys value independence for reasoning. The restrictions of these type systems, however, are often too onerous for realistic software. Thus, most uniqueness type systems include some "escape hatch" where the benefits of value independence for reasoning are lost, but the restrictions of uniqueness types are lifted. To formally verify a system with such mixed guarantees, the value independence guarantees from uniqueness types must be expressed in terms of imperative, mutable semantics. In other words, we ought to express value independence as an assertion in Separation Logic.

</details>


### [3] [Auditing Rust Crates Effectively](https://arxiv.org/abs/2602.06466)
*Lydia Zoghbi,David Thien,Ranjit Jhala,Deian Stefan,Caleb Stanford*

Main category: cs.PL

TL;DR: Cargo Scan是首个用于审计第三方Rust代码的交互式程序分析工具，利用Rust类型系统识别潜在危险代码，可将审计工作量减少到代码量的0.2%


<details>
  <summary>Details</summary>
Motivation: Rust系统依赖大量第三方库，这些依赖与其他语言一样危险，但当前审计需要人工逐行检查所有代码，工作量巨大

Method: 将潜在危险代码建模为"效果"，进行针对Rust的副作用分析，跟踪效果跨越crate和模块边界，使用调用图跟踪上下文依赖信息，生成可组合重用的审计文件

Result: 69.2%情况下开发者可本地检查标记的效果；审计hyper及其依赖时，将潜在危险代码审计量减少到中位数0.2%的代码行数；可自动将约3.5K个顶级crate分类为安全

Conclusion: Cargo Scan能显著减少Rust第三方代码审计工作量，利用Rust类型系统特性有效识别和跟踪潜在危险代码，为Rust生态系统安全提供实用工具

Abstract: We introduce Cargo Scan, the first interactive program analysis tool designed to help developers audit third-party Rust code. Real systems written in Rust rely on thousands of transitive dependencies. These dependencies are as dangerous in Rust as they are in other languages (e.g., C or JavaScript) -- and auditing these dependencies today means manually inspecting every line of code. Unlike for most industrial languages, though, we can take advantage of Rust's type and module system to minimize the amount of code that developers need to inspect to the code that is potentially dangerous. Cargo Scan models such potentially dangerous code as effects and performs a side-effects analysis, tailored to Rust, to identify effects and track them across crate and module boundaries. In most cases (69.2%) developers can inspect flagged effects and decide whether the code is potentially dangerous locally. In some cases, however, the safety of an effect depends on the calling context -- how a function is called, potentially by a crate the developer imports later. Hence, Cargo Scan tracks context-dependent information using a call-graph, and collects audit results into composable and reusable audit files. In this paper, we describe our experience auditing Rust crates with Cargo Scan. In particular, we audit the popular client and server HTTP crate, hyper, and all of its dependencies; our experience shows that Cargo Scan can reduce the auditing burden of potentially dangerous code to a median of 0.2% of lines of code when compared to auditing whole crates. Looking at the Rust ecosystem more broadly, we find that Cargo Scan can automatically classify ~3.5K of the top 10K crates on crates.io as safe; of the crates that do require manual inspection, we find that most of the potentially dangerous side-effects are concentrated in roughly 3% of these crates.

</details>


### [4] [Same Engine, Multiple Gears: Parallelizing Fixpoint Iteration at Different Granularities (Extended Version)](https://arxiv.org/abs/2602.06680)
*Ali Rasim Kocal,Michael Schwarz,Simmo Saan,Helmut Seidl*

Main category: cs.PL

TL;DR: 提出了一种参数化任务粒度的并行不动点引擎，支持两种并行化哲学：即时方法和独立方法，并在Goblint静态分析框架中实现。


<details>
  <summary>Details</summary>
Motivation: 并行化不动点迭代可以显著减少静态分析时间。现有方法通常预先固定任务粒度（如程序线程或过程级别），导致引擎性能受限。需要一种更灵活的参数化任务粒度方法。

Method: 基于支持混合流敏感性的TD求解器，构建参数化任务粒度的并行不动点引擎。实现两种并行化哲学：1）即时方法：所有任务访问单个线程安全哈希表；2）独立方法：每个任务有自己的状态，通过发布/订阅数据结构交换数据。

Result: 在Goblint静态分析框架中实现了两种并行化方法，并在大型实际程序上进行了测试，验证了方法的有效性。

Conclusion: 提出的参数化任务粒度并行不动点引擎能够灵活适应不同分析需求，两种并行化哲学各有优势，为静态分析性能优化提供了新思路。

Abstract: Fixpoint iteration constitutes the algorithmic core of static analyzers. Parallelizing the fixpoint engine can significantly reduce analysis times. Previous approaches typically fix the granularity of tasks upfront, e.g., at the level of program threads or procedures - yielding an engine permanently stuck in one gear. Instead, we propose to parallelize a generic fixpoint engine in a way that is parametric in the task granularity - meaning that our engine can be run in different gears. We build on the top-down solver TD, extended with support for mixed-flow sensitivity, and realize two competing philosophies for parallelization, both building on a task pool that schedules tasks to a fixed number of workers. The nature of tasks differs between the philosophies. In the immediate approach, all tasks access a single thread-safe hash table maintaining solver state, while in the independent approach, each task has its own state and exchanges data with other tasks via a publish/subscribe data structure. We have equipped the fixpoint engine of the static analysis framework Goblint with implementations following both philosophies and report on our results for large real-world programs.

</details>


### [5] [Practical Refinement Session Type Inference (Extended Version)](https://arxiv.org/abs/2602.06715)
*Toby Ueno,Ankush Das*

Main category: cs.PL

TL;DR: 提出带算术精化的会话类型系统的类型推断算法，通过生成类型和算术约束并用Z3求解，减轻程序员标注负担


<details>
  <summary>Details</summary>
Motivation: 现有的带算术精化的会话类型系统虽然能更精细地描述通信协议，但给程序员增加了繁重的标注负担，需要自动推断来减轻这种负担

Method: 1. 建立会话类型的子类型理论，包括基于类型模拟的语义定义和算法；2. 开发形式化的推断算法，生成类型和算术约束；3. 使用Z3 SMT求解器解决约束；4. 在Rast语言上实现，包含3个关键优化

Result: 1. 实现了推断引擎并在6个挑战性基准测试上评估；2. 展示了从一元/二元自然数到线性λ演算的应用；3. 优化显著提高了Z3求解算术约束的效率

Conclusion: 提出的类型推断算法能有效减轻带算术精化的会话类型系统的标注负担，通过优化使推断在实际应用中可行

Abstract: Session types express and enforce safe communication in concurrent message-passing systems by statically capturing the interaction protocols between processes in the type. Recent works extend session types with arithmetic refinements, which enable additional fine-grained description of communication, but impose additional annotation burden on the programmer. To alleviate this burden, we propose a type inference algorithm for a session type system with arithmetic refinements. We develop a theory of subtyping for session types, including an algorithm which we prove sound with respect to a semantic definition based on type simulation. We also provide a formal inference algorithm that generates type and arithmetic constraints, which are then solved using the Z3 SMT solver. The algorithm has been implemented on top of the Rast language, and includes 3 key optimizations that make inference feasible and practical. We evaluate the efficacy of our inference engine by evaluating it on 6 challenging benchmarks, ranging from unary and binary natural numbers to linear $λ$-calculus. We show the performance benefits provided by our optimizations in coercing Z3 into solving the arithmetic constraints in reasonable time.

</details>


### [6] [Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI](https://arxiv.org/abs/2602.06934)
*Ehud Shapiro*

Main category: cs.PL

TL;DR: GLP是一种并发逻辑编程语言，为分布式草根平台设计，具有读者-写者变量分区，支持丰富的多向通信模式。本文开发了dGLP和madGLP作为实现就绪的确定性操作语义，用于工作站和智能手机实现。


<details>
  <summary>Details</summary>
Motivation: 为草根平台（分布式系统，各实例可独立运行并可合并成更大实例）设计编程语言，特别是针对智能手机点对点通信架构。需要为AI实现提供形式化规范。

Method: 开发了dGLP（单代理GLP的确定性操作语义）和madGLP（多代理GLP的操作语义），并证明它们相对于原始操作语义的正确性。这些确定性语义作为AI实现的正式规范。

Result: dGLP已成功用于AI开发基于工作站的GLP实现；madGLP正在被AI用于开发基于智能手机的maGLP实现。两种语义都经过正确性证明。

Conclusion: 通过开发实现就绪的确定性操作语义，为GLP语言在草根平台上的实现提供了可靠的形式化基础，支持从工作站到智能手机的分布式实现。

Abstract: Grassroots Logic Programs (GLP) is a concurrent logic programming language with variables partitioned into paired \emph{readers} and \emph{writers}, conjuring both linear logic and futures/promises: an assignment is produced at most once via the sole occurrence of a writer (promise) and consumed at most once via the sole occurrence of its paired reader (future), and may contain additional readers and/or writers, enabling the concise expression of rich multidirectional communication modalities.
  GLP was designed as a language for grassroots platforms -- distributed systems with multiple instances that can operate independently of each other and of any global resource, and can coalesce into ever larger instances -- with its target architecture being smartphones communicating peer-to-peer. The operational semantics of Concurrent (single-agent) GLP and of multiagent GLP (maGLP) were defined via transition systems/multiagent transition systems, respectively.
  Here, we describe the mathematics developed to facilitate the workstation- and smartphone-based implementations of GLP by AI in Dart. We developed dGLP -- implementation-ready deterministic operational semantics for single-agent GLP -- and proved it correct with respect to the Concurrent GLP operational semantics; dGLP was used by AI as a formal spec, from which it developed a workstation-based implementation of GLP. We developed madGLP -- an implementation-ready multiagent operational semantics for maGLP -- and proved it correct with respect to the maGLP operational semantics; madGLP is deterministic at the agent level (not at the system level due to communication asynchrony), and is being used by AI as a formal spec from which it develops a smartphone-based implementation of maGLP.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing](https://arxiv.org/abs/2602.06057)
*Satyam Kumar,Saurabh Jha*

Main category: cs.DC

TL;DR: QEIL框架通过推理时间缩放定律和异构加速器编排，在资源受限的边缘设备上实现高效本地LLM推理，显著提升能效和性能。


<details>
  <summary>Details</summary>
Motivation: 当前在资源受限的边缘设备上进行大语言模型推理面临重大挑战，现有解决方案严重依赖云或数据中心基础设施，无法满足低延迟智能系统的需求。

Method: 提出QEIL统一框架，包含三个优化维度：1) 推理时间缩放定律显示异构工作负载分配能获得超线性效率增益；2) 通过分析成本模型实现硬件感知路由；3) 使用新指标量化性能-能耗权衡。通过渐进式样本复用统一编排器整合这些组件。

Result: 在5个模型家族（125M到2.6B参数）上的广泛评估显示：pass@k覆盖率提升7-10.5个百分点，能耗降低35.6-78.2%，平均功耗降低68%满足边缘热预算，延迟改善15.8%，且无精度损失。

Conclusion: 推理时间缩放定律是通用且架构无关的，异构边缘编排是能耗受限智能系统的最优策略。

Abstract: Large language model inference on resource constrained edge devices remains a major challenge for low latency intelligent systems, as existing solutions depend heavily on cloud or datacenter infrastructure. This work introduces QEIL, Quantifying Edge Intelligence via Inference time Scaling Laws, a unified framework for efficient local LLM inference using principled scaling laws and heterogeneous orchestration across CPU, GPU, and NPU accelerators. We derive five architecture agnostic theorems that characterize how inference efficiency scales with model size, sample budget, and device level constraints. QEIL integrates three optimization dimensions. First, inference time scaling laws show that heterogeneous workload distribution achieves superlinear efficiency gains that are not observed in homogeneous execution. Second, hardware aware routing is enabled through analytical cost models that account for compute throughput, memory bandwidth, power consumption, and thermal limits. Third, performance energy trade offs are quantified using novel metrics including Intelligence Per Watt, Energy Coverage Efficiency, and Price Power Performance. A unified orchestrator combines these components through progressive sample multiplexing to improve coverage. Extensive evaluation across five model families from 125M to 2.6B parameters demonstrates consistent gains, including 7 to 10.5 percentage point improvement in pass at k coverage, 35.6 to 78.2 percent energy reduction, 68 percent average power reduction enabling edge thermal budgets, 15.8 percent latency improvement, and zero accuracy loss. Results confirm that inference time scaling laws are universal and architecture agnostic, establishing heterogeneous edge orchestration as the optimal strategy for energy constrained intelligent systems.

</details>


### [8] [Mapping Gemma3 onto an Edge Dataflow Architecture](https://arxiv.org/abs/2602.06063)
*Shouyu Du,Miaoxiang Yu,Zhiheng Ni,Jillian Cai,Qing Yang,Tao Wei,Zhenyu Xu*

Main category: cs.DC

TL;DR: 本文展示了在AMD Ryzen AI NPU上首次端到端部署Gemma3系列大语言和视觉模型，通过硬件感知优化技术实现了显著的性能提升和能效改进。


<details>
  <summary>Details</summary>
Motivation: 现代NPU能否在边缘设备上实现实用、低功耗的LLM和VLM推理，以及如何将基于transformer的模型映射到分块数据流加速器上。

Method: 提出了一系列硬件感知优化技术：预填充阶段采用高效反量化引擎、优化分块矩阵乘法内核和FlowQKV（分块流水线注意力机制）；解码阶段采用FusedDQP（融合反量化和投影）和FlowKV（重构注意力以维持高内存带宽利用率），配合紧凑的Q4NX 4位量化格式。

Result: 相比iGPU，预填充速度提升5.2倍，解码速度提升4.8倍；相比CPU，预填充提升33.5倍，解码提升2.2倍。能效相比iGPU提升67.2倍，相比CPU提升222.9倍。

Conclusion: 现代NPU能够在边缘设备上实现实用、低功耗的LLM和VLM推理，为将基于transformer的模型映射到分块数据流加速器提供了可推广的蓝图。

Abstract: We present the first end-to-end deployment of the Gemma3 family of large language and vision models on a tiled edge dataflow architecture (AMD Ryzen AI NPU). Our work introduces a set of hardware-aware techniques. For prefill, we introduce an efficient dequantization engine, optimize tiled matrix multiplication kernels, and propose FlowQKV, a chunked, pipelined attention mechanism. For decoding, we introduce FusedDQP, which fuses dequantization and projection into a single kernel, and FlowKV, which re-structures attention to sustain high memory bandwidth utilization. Together with a compact Q4NX 4-bit quantization format, these methods yield up to $5.2\times$ faster prefill and $4.8\times$ faster decoding versus the iGPU, and $33.5\times$ and $2.2\times$ over the CPU, respectively. Power efficiency improves by as much as $67.2\times$ and $222.9\times$ compared to the iGPU and CPU. The proposed approach demonstrates that modern NPUs can deliver practical, low-power LLM and VLM inference at the edge, and provides a generalizable blueprint for mapping transformer-based models onto tiled dataflow accelerators.

</details>


### [9] [iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems](https://arxiv.org/abs/2602.06064)
*Yi-Xiang Hu,Yuke Wang,Feng Wu,Zirui Huang,Shuli Zeng,Xiang-Yang Li*

Main category: cs.DC

TL;DR: iScheduler是一个基于强化学习的迭代调度框架，用于解决资源投资问题，通过将问题分解为子问题并顺序选择进程来加速优化，支持动态重新配置。


<details>
  <summary>Details</summary>
Motivation: 传统混合整数规划和约束规划方法在大规模实例上速度过慢，且动态更新需要在严格延迟预算下重新调度，需要更高效的解决方案。

Method: 将资源投资问题建模为马尔可夫决策过程，分解为子问题，通过强化学习驱动的顺序进程选择构建调度方案，支持重用未更改的进程调度。

Result: iScheduler在保持竞争力的资源成本的同时，将可行性时间减少了高达43倍，相比商业基线显著加速。

Conclusion: iScheduler为大规模资源投资问题提供了高效的强化学习解决方案，支持快速优化和动态重新配置，并发布了工业级基准数据集L-RIPLIB。

Abstract: Scheduling precedence-constrained tasks under shared renewable resources is central to modern computing platforms. The Resource Investment Problem (RIP) models this setting by minimizing the cost of provisioned renewable resources under precedence and timing constraints. Exact mixed-integer programming and constraint programming become impractically slow on large instances, and dynamic updates require schedule revisions under tight latency budgets. We present iScheduler, a reinforcement-learning-driven iterative scheduling framework that formulates RIP solving as a Markov decision process over decomposed subproblems and constructs schedules through sequential process selection. The framework accelerates optimization and supports reconfiguration by reusing unchanged process schedules and rescheduling only affected processes. We also release L-RIPLIB, an industrial-scale benchmark derived from cloud-platform workloads with 1,000 instances of 2,500-10,000 tasks. Experiments show that iScheduler attains competitive resource costs while reducing time to feasibility by up to 43$\times$ against strong commercial baselines.

</details>


### [10] [HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference](https://arxiv.org/abs/2602.06069)
*Dinesh Gopalan,Ratul Ali*

Main category: cs.DC

TL;DR: HQP框架通过敏感度感知的结构剪枝和8位后训练量化的协同优化，在保证精度下降不超过1.5%的前提下，实现3.12倍推理加速和55%模型压缩，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 边缘云环境中对高保真实时推理的需求日益增长，但面临严重的延迟和能耗限制，需要激进的模型优化方法。

Method: 提出混合量化与剪枝框架：1）基于Fisher信息矩阵高效近似的动态权重敏感度指标指导迭代剪枝；2）在满足最大允许精度下降后才进行8位后训练量化；3）确保稀疏模型结构对量化误差和硬件优化具有最大鲁棒性。

Result: 在NVIDIA Jetson边缘平台上，使用MobileNetV3和ResNet-18架构，实现峰值3.12倍推理加速和55%模型大小缩减，同时精度下降严格控制在1.5%以内。

Conclusion: HQP框架相比传统单目标压缩技术具有优越性，是资源受限边缘基础设施中部署超低延迟AI的硬件无关解决方案。

Abstract: The escalating demand for high-fidelity, real-time inference in distributed edge-cloud environments necessitates aggressive model optimization to counteract severe latency and energy constraints. This paper introduces the Hybrid Quantization and Pruning (HQP) framework, a novel, integrated methodology designed to achieve synergistic model acceleration while adhering to strict quality guarantees. We detail a sensitivity-aware structural pruning algorithm that employs a dynamic weight sensitivity metric, derived from a highly efficient approximation of the Fisher Information Matrix (FIM), to guide the iterative removal of redundant filters. This pruning is strictly conditional, enforcing an adherence to a maximum permissible accuracy drop (Delta ax) before the model proceeds to 8-bit post-training quantization. This rigorous coordination is critical, as it ensures the resultant sparse model structure is maximally robust to quantization error and hardware-specific kernel optimization. Exhaustive evaluation across heterogeneous NVIDIA Jetson edge platforms, utilizing resource-efficient architectures like MobileNetV3 and ResNet-18, demonstrates that the HQP framework achieves a peak performance gain of 3.12 times inference speedup and a 55 percent model size reduction, while rigorously containing the accuracy drop below the 1.5 percent constraint. A comprehensive comparative analysis against conventional single-objective compression techniques validates the HQP framework as a superior, hardware-agnostic solution for deploying ultra-low-latency AI in resource-limited edge infrastructures.

</details>


### [11] [Computationally Efficient Laplacian CL-colME](https://arxiv.org/abs/2602.06070)
*Nikola Stankovic*

Main category: cs.DC

TL;DR: 提出CL-colME，一种基于拉普拉斯共识的分散式协作均值估计新方法，相比现有C-colME方法避免了计算昂贵的归一化过程，同时保持了收敛性和精度。


<details>
  <summary>Details</summary>
Motivation: 分散式协作均值估计（colME）是异构网络中的基础任务。现有的基于图的B-colME和C-colME方法虽然实现了高可扩展性，但C-colME依赖双随机平均矩阵，需要进行计算昂贵的归一化过程，影响了计算效率。

Method: 提出CL-colME（基于拉普拉斯共识的协作均值估计），采用拉普拉斯基共识机制替代双随机平均矩阵，避免了复杂的归一化计算过程，同时保持了收敛到最优解的能力。

Result: 仿真结果表明，CL-colME在保持C-colME收敛行为和精度的同时，显著提高了计算效率。新方法能够在不牺牲性能的情况下减少计算开销。

Conclusion: CL-colME是一种有效的分散式协作均值估计方法，通过拉普拉斯共识机制避免了昂贵的归一化过程，在保持收敛性和精度的同时提高了计算效率，为大规模异构网络中的均值估计问题提供了更高效的解决方案。

Abstract: Decentralized collaborative mean estimation (colME) is a fundamental task in heterogeneous networks. Its graph-based variants B-colME and C-colME achieve high scalability of the problem. This paper evaluates the consensus-based C-colME framework, which relies on doubly stochastic averaging matrices to ensure convergence to the oracle solution. We propose CL-colME, a novel variant utilizing Laplacian-based consensus to avoid the computationally expensive normalization processes. Simulation results show that the proposed CL-colME maintains the convergence behavior and accuracy of C-colME while improving computational efficiency.

</details>


### [12] [FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs](https://arxiv.org/abs/2602.06071)
*Rajat Vadiraj Dwaraknath,Sungyoon Kim,Mert Pilanci*

Main category: cs.DC

TL;DR: 提出BlockPerm-SJLT稀疏草图与FlashSketch CUDA内核的协同设计，通过结构化稀疏模式提升GPU效率，在保持草图质量的同时获得1.7倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏随机草图虽然能降低计算成本，但其随机稀疏模式导致GPU内存访问不规则，严重影响内存带宽利用率，需要在GPU效率和草图鲁棒性之间找到平衡。

Method: 采用草图-内核协同设计方法：设计BlockPerm-SJLT稀疏草图家族，其稀疏结构专门为GPU优化；开发对应的FlashSketch CUDA内核实现高效计算；引入可调参数平衡GPU效率与草图质量。

Result: 理论证明BlockPerm-SJLT满足无意识子空间嵌入(OSE)保证；实验显示FlashSketch在RandNLA基准测试和GraSS数据归因管道中均表现优异，相比现有GPU草图获得约1.7倍几何平均加速。

Conclusion: 通过草图结构与计算内核的协同设计，成功解决了稀疏草图在GPU上的效率瓶颈，在草图质量与速度的帕累托前沿上取得显著进步，为GPU加速的随机数值线性代数提供了高效解决方案。

Abstract: Sparse sketches such as the sparse Johnson-Lindenstrauss transform are a core primitive in randomized numerical linear algebra because they leverage random sparsity to reduce the arithmetic cost of sketching, while still offering strong approximation guarantees. Their random sparsity, however, is at odds with efficient implementations on modern GPUs, since it leads to irregular memory access patterns that degrade memory bandwidth utilization. Motivated by this tension, we pursue a sketch-kernel co-design approach: we design a new family of sparse sketches, BlockPerm-SJLT, whose sparsity structure is chosen to enable FlashSketch, a corresponding optimized CUDA kernel that implements these sketches efficiently. The design of BlockPerm-SJLT introduces a tunable parameter that explicitly trades off the tension between GPU-efficiency and sketching robustness. We provide theoretical guarantees for BlockPerm-SJLT under the oblivious subspace embedding (OSE) framework, and also analyze the effect of the tunable parameter on sketching quality. We empirically evaluate FlashSketch on standard RandNLA benchmarks, as well as an end-to-end ML data attribution pipeline called GraSS. FlashSketch pushes the Pareto frontier of sketching quality versus speed, across a range of regimes and tasks, and achieves a global geomean speedup of roughly 1.7x over the prior state-of-the-art GPU sketches.

</details>


### [13] [PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference](https://arxiv.org/abs/2602.06072)
*Rui Ning,Wei Zhang,Fan Lai*

Main category: cs.DC

TL;DR: PackInfer是一个针对大语言模型推理的注意力框架，通过将异构批处理请求打包成负载均衡的执行组，优化计算和I/O效率，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 生产环境中LLM推理需要批处理不同长度的序列以获得高吞吐量，但现有注意力优化技术（如FlashAttention）主要针对单个请求，导致计算和I/O不平衡、拖尾效应严重，GPU资源利用率低。

Method: 1. 将批处理请求编排成负载均衡的执行组，将多个请求打包到统一的内核启动中；2. 直接在打包的查询-键区域上构建注意力内核，消除冗余计算并平衡线程块执行；3. 采用I/O感知分组，将共享前缀的请求共置，并将KV缓存重组为组连续布局，减少内存碎片和冗余数据移动。

Result: 在实际工作负载评估中，PackInfer相比最先进的FlashAttention，推理延迟降低13.0-20.1%，吞吐量提升20%。

Conclusion: PackInfer通过计算和I/O感知的异构批处理推理执行，有效解决了生产环境中LLM推理的注意力效率问题，显著提升了GPU利用率和整体性能。

Abstract: Attention efficiency is critical to large language model (LLM) inference. While prior advances optimize attention execution for individual requests (e.g., FlashAttention), production LLM serving relies on batching requests with highly heterogeneous sequence lengths for high serving throughput. This mismatch induces severe computation and I/O imbalance, exacerbates stragglers, and underutilizes GPU resources. We present PackInfer, a kernel-level attention framework that enables compute- and I/O-aware execution for heterogeneous batched inference. PackInfer orchestrates batched requests into load-balanced execution groups, effectively saturating GPU utilization by packing multiple requests into unified kernel launches. By constructing attention kernels directly over packed query-key regions, PackInfer eliminates redundant computation and balances thread-block execution. It then incorporates I/O-aware grouping that co-locates shared-prefix requests and reorganizes KV caches into group-contiguous layouts, reducing memory fragmentation and redundant data movement as generation evolves. Evaluations on real-world workloads show that PackInfer reduces inference latency by 13.0-20.1%, and improves throughput by 20% compared to the state-of-the-art FlashAttention.

</details>


### [14] [Experimental Analysis of Server-Side Caching for Web Performance](https://arxiv.org/abs/2602.06074)
*Mohammad Umar,Bharat Tripathi*

Main category: cs.DC

TL;DR: 实验研究表明，在小型Web应用中实施简单的内存缓存能显著降低响应时间，适合教育环境和小规模应用


<details>
  <summary>Details</summary>
Motivation: 尽管缓存技术已被广泛研究，但缺乏针对小型Web应用中简单内存缓存效果的实验研究。本文旨在填补这一研究空白，探索简单服务器端缓存在小型应用中的性能影响

Method: 实验比较两种服务器端配置：无缓存 vs 带内存缓存和固定生存时间。使用轻量级Web服务器框架，在相同环境条件下通过重复HTTP请求测量响应时间

Result: 缓存请求的响应时间显著降低，证明了简单服务器端缓存在提升Web应用性能方面的有效性

Conclusion: 简单服务器端缓存能有效改善Web应用性能，特别适合教育环境和小规模应用，因为其简单性和可重现性至关重要

Abstract: Performance in web applications is a key aspect of user experience and system scalability. Among the different techniques used to improve web application performance, caching has been widely used. While caching has been widely explored in web performance optimization literature, there is a lack of experimental work that explores the effect of simple inmemory caching in small-scale web applications. This paper fills this research gap by experimentally comparing the performance of two server-side web application configurations: one without caching and another with in-memory caching and a fixed time-tolive. The performance evaluation was conducted using a lightweight web server framework, and response times were measured using repeated HTTP requests under identical environmental conditions. The results show a significant reduction in response time for cached requests, and the findings of this paper provide valuable insights into the effectiveness of simple server-side caching in improving web application performance making it suitable for educational environments and small-scale web applications where simplicity and reproducibility are critical.

</details>


### [15] [MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments](https://arxiv.org/abs/2602.06075)
*Guangyi Liu,Pengxiang Zhao,Yaozhen Liang,Qinyi Luo,Shunye Tang,Yuxiang Chai,Weifeng Lin,Han Xiao,WenHao Wang,Siheng Chen,Zhengxi Lu,Gao Wu,Hao Wang,Liang Liu,Yong Liu*

Main category: cs.DC

TL;DR: MemGUI-Bench：首个专注于移动GUI代理记忆能力的基准测试，包含128个跨时空记忆任务，揭示了现有代理在记忆方面的显著缺陷


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI代理基准测试严重忽视记忆能力评估，仅有5.2-11.8%的任务涉及记忆，且缺乏跨会话学习评估。需要建立专门评估GUI代理记忆能力的基准

Method: 提出MemGUI-Bench基准：1) 系统化记忆分类法分析11种代理的5种架构；2) 128个跨26个应用的任务，89.8%挑战跨时空记忆；3) MemGUI-Eval自动化评估流水线，包含渐进审查和7个层次化指标；4) 基于研究问题的11种SOTA代理评估

Result: 实验显示所有评估系统都存在显著记忆缺陷，识别出5种不同的失败模式，并综合出5个可操作的设计启示

Conclusion: MemGUI-Bench填补了GUI代理记忆评估的空白，揭示了当前代理在记忆能力上的不足，为未来GUI代理设计提供了重要指导。所有资源将完全开源并持续维护

Abstract: Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \textbf{\textit{fully open-sourced and continuously maintained}} at https://lgy0404.github.io/MemGUI-Bench/.

</details>


### [16] [Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers](https://arxiv.org/abs/2602.06079)
*Liangyu Wang,Siqi Zhang,Junjie Wang,Yiming Dong,Bo Zheng,Zihan Qiu,Shengkun Tang,Di Wang,Rui Men,Dayiheng Liu*

Main category: cs.DC

TL;DR: Canzona是一个统一、异步、负载均衡的框架，用于在分布式训练中高效实现矩阵优化器，解决了现有方法在张量并行和数据并行下的冲突问题。


<details>
  <summary>Details</summary>
Motivation: 矩阵优化器（如Shampoo、Muon、SOAP）在大语言模型扩展中具有收敛效率优势，但其整体更新需求与Megatron等分布式框架中的张量碎片化存在冲突。现有同步方法存在计算冗余，而分层划分方法无法在不违反高效通信原语几何约束的情况下解决这一冲突。

Method: 提出Canzona框架，将逻辑优化器分配与物理参数分布解耦。针对数据并行，提出alpha平衡静态分区策略，在保持原子性的同时平衡负载。针对张量并行，设计异步计算流水线，利用微组调度批量处理碎片化更新并隐藏重构开销。

Result: 在256个GPU上对Qwen3模型家族（最高320亿参数）的评估表明，该方法保持了现有并行架构的效率，端到端迭代时间加速1.57倍，优化器步骤延迟降低5.8倍。

Conclusion: Canzona成功解决了矩阵优化器在分布式训练中的实现冲突，通过统一、异步、负载均衡的设计，在保持并行架构效率的同时显著提升了训练性能。

Abstract: The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.

</details>


### [17] [LAAFD: LLM-based Agents for Accelerated FPGA Design](https://arxiv.org/abs/2602.06085)
*Maxim Moraru,Kamalavasan Kamalakkannan,Jered Dominguez-Trujillo,Patrick Diehl,Atanu Barai,Julien Loiseau,Zachary Kent Baker,Howard Pritchard,Galen M Shipman*

Main category: cs.DC

TL;DR: LAAFD 是一个使用大语言模型将通用 C++ 代码自动转换为优化 Vitis HLS 内核的智能工作流，能实现深度流水线、向量化和数据流分区等关键优化，在保持高性能的同时大幅降低 FPGA 编程门槛。


<details>
  <summary>Details</summary>
Motivation: FPGA 具有高性能、低延迟和能效优势，但在科学计算和边缘计算中的应用受到硬件专业知识门槛的限制。虽然高级综合（HLS）比硬件描述语言（HDL）提高了生产力，但要获得有竞争力的设计仍需硬件感知优化和精细的数据流设计。

Method: LAAFD 采用基于大语言模型的智能工作流，将通用 C++ 代码转换为优化的 Vitis HLS 内核。该工作流自动化执行深度流水线、向量化和数据流分区等关键转换，并通过 HLS 协同仿真和综合反馈形成闭环，验证正确性并迭代改进执行周期。

Result: 在代表 HPC 常见计算模式的 15 个内核测试套件中，LAAFD 相比手动调优的 Vitis HLS 基准实现了 99.9% 的几何平均性能。对于模板计算工作负载，LAAFD 性能与最先进的基于 DSL 的 HLS 代码生成器 SODA 相当，同时生成更易读的内核。

Conclusion: LAAFD 在不牺牲效率的前提下，显著降低了 FPGA 加速的专业知识门槛，为更广泛地采用 FPGA 加速提供了可行方案。

Abstract: FPGAs offer high performance, low latency, and energy efficiency for accelerated computing, yet adoption in scientific and edge settings is limited by the specialized hardware expertise required. High-level synthesis (HLS) boosts productivity over HDLs, but competitive designs still demand hardware-aware optimizations and careful dataflow design. We introduce LAAFD, an agentic workflow that uses large language models to translate general-purpose C++ into optimized Vitis HLS kernels. LAAFD automates key transfor mations: deep pipelining, vectorization, and dataflow partitioning and closes the loop with HLS co-simulation and synthesis feedback to verify correctness while iteratively improving execution time in cycles. Over a suite of 15 kernels representing common compute patterns in HPC, LAFFD achieves 99.9% geomean performance when compared to the hand tuned baseline for Vitis HLS. For stencil workloads, LAAFD matches the performance of SODA, a state-of-the-art DSL-based HLS code generator for stencil solvers, while yielding more readable kernels. These results suggest LAAFD substantially lowers the expertise barrier to FPGA acceleration without sacrificing efficiency.

</details>


### [18] [BouquetFL: Emulating diverse participant hardware in Federated Learning](https://arxiv.org/abs/2602.06498)
*Arno Geimer*

Main category: cs.DC

TL;DR: BouquetFL是一个联邦学习框架，通过在单机上模拟异构客户端硬件来解决现有研究忽略硬件异质性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究大多在中央机器上进行模拟，忽略了实际部署中参与方硬件异质性对训练的影响，这导致实验条件与实际情况存在差距。

Method: 通过资源限制编程模拟不同的硬件配置，在单台物理机器上模拟异构客户端硬件，提供基于真实硬件流行度的自定义硬件采样器，支持用户按需配置联邦环境。

Result: 开发了BouquetFL框架，使研究者能够在不需要多台物理设备的情况下，在受控环境中研究系统异质性对联邦学习的影响。

Conclusion: BouquetFL填补了联邦学习研究的方法论空白，使实验条件更接近实际部署场景，特别适用于研究高度异构联邦的研究人员。

Abstract: In Federated Learning (FL), multiple parties collaboratively train a shared Machine Learning model to encapsulate all private knowledge without exchange of information. While it has seen application in several industrial projects, most FL research considers simulations on a central machine, without considering potential hardware heterogeneity between the involved parties. In this paper, we present BouquetFL, a framework designed to address this methodological gap by simulating heterogeneous client hardware on a single physical machine. By programmatically emulating diverse hardware configurations through resource restriction, BouquetFL enables controlled FL experimentation under realistic hardware diversity. Our tool provides an accessible way to study system heterogeneity in FL without requiring multiple physical devices, thereby bringing experimental practice closer to practical deployment conditions. The target audience are FL researchers studying highly heterogeneous federations. We include a wide range of profiles derived from commonly available consumer and small-lab devices, as well as a custom hardware sampler built on real-world hardware popularity, allowing users to configure the federation according to their preference.

</details>


### [19] [FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training](https://arxiv.org/abs/2602.06499)
*Gyeongseo Park,Eungyeong Lee,Song-woo Sok,Myung-Hoon Cha,Kwangwon Koh,Baik-Song An,Hongyeon Kim,Ki-Dong Kang*

Main category: cs.DC

TL;DR: FCDP是一种新的分布式训练方法，通过在主机内存中缓存前向传播参数并在反向传播时重用，减少50%的节点间通信，在带宽有限的集群上实现比ZeRO-3和ZeRO++更高的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有分布式训练方法在商用硬件上面临瓶颈：ZeRO-3在带宽有限的集群上存在严重的节点间通信瓶颈；ZeRO++等GPU内存缓存方法会触发内存不足问题；ZeRO-Offload等主机内存卸载方法则因PCIe开销导致吞吐量下降。

Method: FCDP将主机内存作为快速缓存层而非溢出层，在前向传播时将参数缓存到主机内存，反向传播时通过快速的节点内all-gather重用这些参数，从而消除冗余的节点间通信。对于参数高效微调，FCDP选择性通信仅训练参数以最大化缓存效果。

Result: 在商用集群设置中，FCDP相比ZeRO-3实现高达100倍的吞吐量提升，相比ZeRO++实现51倍的提升，同时保持ZeRO-3的最大批处理大小。对于参数高效微调，FCDP减少超过99%的节点间通信流量。

Conclusion: FCDP通过在主机内存中智能缓存参数，有效解决了带宽有限集群上的分布式训练瓶颈，在保持最小GPU内存占用的同时显著提升训练吞吐量，为资源受限环境中的大规模模型训练提供了实用解决方案。

Abstract: Training billion-parameter models requires distributing model states across GPUs using fully sharded data parallel (i.e., ZeRO-3). While ZeRO-3 succeeds on clusters with high-bandwidth NVLink and InfiniBand interconnects, researchers with commodity hardware face severe inter-node all-gather bottlenecks. Existing optimizations take two approaches: GPU memory caching (MiCS, ZeRO++) trades memory capacity for reduced communication, triggering out-of-memory failures on large models; host memory offloading (ZeRO-Offload, ZeRO-Infinity) extends capacity but degrades throughput due to PCIe overhead. We observe that on bandwidth-limited clusters, host memory can serve not as an overflow tier but as a fast caching layer that outperforms inter-node communication. Based on this insight, we propose FCDP, which eliminates redundant inter-node communication while preserving ZeRO-3's minimal GPU memory footprint. FCDP caches forward-pass parameters in host memory and reuses them during the backward pass via fast intra-node all-gather, reducing inter-node all-gather by 50%. For parameter-efficient fine-tuning (PEFT), FCDP selectively communicates only trainable parameters to maximize caching, reducing inter-node traffic by over 99%. In our commodity cluster setup, FCDP achieves up to 100x higher throughput than ZeRO-3 and 51x higher than ZeRO++, while maintaining ZeRO-3's maximum batch size.

</details>


### [20] [DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving](https://arxiv.org/abs/2602.06502)
*Ying Yuan,Pengfei Zuo,Bo Wang,Zhangyu Chen,Zhipeng Tan,Zhou Yu*

Main category: cs.DC

TL;DR: DualMap：一种用于分布式LLM服务的双映射调度策略，通过双重哈希映射和智能选择实现KV缓存亲和性与负载均衡的平衡


<details>
  <summary>Details</summary>
Motivation: 在LLM服务中，跨请求重用提示的KV缓存对于降低TTFT和服务成本至关重要。现有的调度器无法在缓存亲和性调度（将相同提示前缀的请求共置）和负载均衡调度（将请求均匀分布到计算实例）之间取得平衡，因为它们通常在单一映射空间中操作。

Method: 提出DualMap双映射调度策略：1）通过两个独立的哈希函数将每个请求映射到两个候选实例；2）基于当前系统状态智能选择更好的候选实例；3）包含三种技术：SLO感知请求路由、热点感知再平衡、轻量级双哈希环扩展。

Result: 在真实工作负载实验中，在相同的TTFT SLO约束下，DualMap相比最先进的工作将有效请求容量提高了最多2.25倍。

Conclusion: DualMap通过创新的双映射设计，成功解决了LLM服务中缓存亲和性与负载均衡之间的冲突，实现了更好的性能表现和系统效率。

Abstract: In LLM serving, reusing the KV cache of prompts across requests is critical for reducing TTFT and serving costs. Cache-affinity scheduling, which co-locates requests with the same prompt prefix to maximize KV cache reuse, often conflicts with load-balancing scheduling that distributes requests evenly across compute instances. Existing schedulers fail to reconcile this trade-off as they operate within a single mapping space, typically applying cache-affinity routing to a subset of requests and load-balanced routing to the rest, without a unified solution to achieve both goals. To address this limitation, we propose DualMap, a dual-mapping scheduling strategy for distributed LLM serving that achieves both cache affinity and load balancing. Its key idea is to map each request to two candidate instances via two independent hash functions based on the request prompt, then intelligently select the better candidate based on current system states. This design increases the likelihood that requests with shared prefixes are co-located, while evenly dispersing distinct prefixes across the cluster via ``the power of two choices''. To make DualMap robust under dynamic and skewed real-world workloads, we incorporate three techniques: 1) SLO-aware request routing, which prioritizes cache affinity but switches to load-aware scheduling when TTFT exceeds the SLO, enhancing load balance without sacrificing cache reuse; 2) hotspot-aware rebalancing, which dynamically migrates requests from overloaded to underloaded instances, mitigating hotspots and rebalancing the system; 3) lightweight dual-hash-ring scaling, which leverages a dual-hash-ring mapping to support fast and low-overhead instance scaling without costly global remapping. Experiments on real-world workloads show that DualMap improves effective request capacity by up to 2.25$\times$ under the same TTFT SLO constraints compared with SOTA work.

</details>


### [21] [Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms](https://arxiv.org/abs/2602.06555)
*Lanpei Li,Massimo Coppola,Malio Li,Valerio Besozzi,Jack Bell,Vincenzo Lomonaco*

Main category: cs.DC

TL;DR: 提出一个在无服务器平台上动态管理结构化并行处理骨架的框架，重点关注Farm模式，结合AI驱动的动态扩展来提升性能和资源效率。


<details>
  <summary>Details</summary>
Motivation: 将HPC级别的性能和弹性引入无服务器和连续计算环境，同时保持骨架编程的可编程性优势，解决无服务器平台上并行处理的性能管理问题。

Method: 基于OpenFaaS平台实现Farm模式，将工作池自动扩展作为QoS感知的资源管理问题。框架结合可复用的farm模板和基于Gymnasium的监控控制层，支持反应式和基于学习的控制器，研究AI驱动的动态扩展策略。

Result: AI基础的管理相比纯基于模型的性能引导能更好地适应平台特定限制，在保持高效资源使用和稳定扩展行为的同时改善QoS。

Conclusion: AI驱动的动态扩展策略在无服务器平台上管理并行处理骨架是有效的，能够平衡性能、资源效率和稳定性，为结构化并行计算在无服务器环境中的应用提供了有前景的方向。

Abstract: We present a framework for dynamic management of structured parallel processing skeletons on serverless platforms. Our goal is to bring HPC-like performance and resilience to serverless and continuum environments while preserving the programmability benefits of skeletons. As a first step, we focus on the well known Farm pattern and its implementation on the open-source OpenFaaS platform, treating autoscaling of the worker pool as a QoS-aware resource management problem. The framework couples a reusable farm template with a Gymnasium-based monitoring and control layer that exposes queue, timing, and QoS metrics to both reactive and learning-based controllers. We investigate the effectiveness of AI-driven dynamic scaling for managing the farm's degree of parallelism via the scalability of serverless functions on OpenFaaS. In particular, we discuss the autoscaling model and its training, and evaluate two reinforcement learning (RL) policies against a baseline of reactive management derived from a simple farm performance model. Our results show that AI-based management can better accommodate platform-specific limitations than purely model-based performance steering, improving QoS while maintaining efficient resource usage and stable scaling behaviour.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [22] [D-Legion: A Scalable Many-Core Architecture for Accelerating Matrix Multiplication in Quantized LLMs](https://arxiv.org/abs/2602.06252)
*Ahmed J. Abdelmaksoud,Cristian Sestito,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: D-Legion是一种新型可扩展多核架构，使用自适应精度脉动阵列核心加速量化LLM中的矩阵乘法，相比现有方案在延迟、内存和吞吐量方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的性能提升伴随着巨大的计算和内存需求，量化LLM具有显著优势，但需要专门的架构来加速其工作负载。

Method: 提出D-Legion架构，包含多个Legion，每个Legion有一组自适应精度脉动阵列。支持量化稀疏和稠密矩阵乘法，利用块结构化稀疏性，通过并行累加器减少部分和的内存访问，通过多播优化数据重用。

Result: 在BitNet模型的注意力工作负载上，相比最先进方案，延迟降低8.2倍，内存节省3.8倍，部分和内存节省3倍。8个Legion、64核心版本峰值吞吐量135.68 TOPS。32个Legion版本相比Google TPUv4i，延迟降低2.5倍，吞吐量提升2.3倍，内存节省2.7倍。

Conclusion: D-Legion是一种高效的可扩展架构，能够显著加速量化LLM的矩阵乘法运算，在性能、内存效率和吞吐量方面优于现有解决方案。

Abstract: The performance gains obtained by large language models (LLMs) are closely linked to their substantial computational and memory requirements. Quantized LLMs offer significant advantages with extremely quantized models, motivating the development of specialized architectures to accelerate their workloads. This paper proposes D-Legion, a novel scalable many-core architecture, designed using many adaptive-precision systolic array cores, to accelerate matrix multiplication in quantized LLMs. The proposed architecture consists of a set of Legions where each Legion has a group of adaptive-precision systolic arrays. D-Legion supports multiple computation modes, including quantized sparse and dense matrix multiplications. The block structured sparsity is exploited within a fully-sparse, or partially-sparse windows. In addition, memory accesses of partial summations (psums) are spatially reduced through parallel accumulators. Furthermore, data reuse is maximized through optimized scheduling techniques by multicasting matrix tiles across the Legions. A comprehensive design space exploration is performed in terms of Legion/core granularity to determine the optimal Legion configuration. Moreover, D-Legion is evaluated on attention workloads from two BitNet models, delivering up to 8.2$\times$ lower latency, up to 3.8$\times$ higher memory savings, and up to 3$\times$ higher psum memory savings compared to state-of-the-art work. D-Legion, with eight Legions and 64 total cores, achieves a peak throughput of 135,68 TOPS at a frequency of 1 GHz. A scaled version of D-Legion, with 32 Legions, is compared to Google TPUv4i, achieving up to 2.5$\times$ lower total latency, up to 2.3$\times$ higher total throughput, and up to 2.7$\times$ higher total memory savings.

</details>
