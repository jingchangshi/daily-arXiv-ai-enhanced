<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?](https://arxiv.org/abs/2509.21629)
*Anjiang Wei,Tarun Suresh,Tianran Sun,Haoze Wu,Ke Wang,Alex Aiken*

Main category: cs.PL

TL;DR: 本文提出了一个评估LLMs在循环不变式合成上的框架，通过验证器决策程序评估正确性和验证速度提升，发现当前LLM验证器相比传统求解器UAutomizer尚无显著优势，但监督微调和Best-of-N采样可提升性能。


<details>
  <summary>Details</summary>
Motivation: 程序验证依赖循环不变式，但自动发现强不变式仍是长期挑战，需要建立原则性框架来评估LLMs在不变式合成上的能力。

Method: 使用基于验证器的决策程序，具有形式化正确性保证，评估不变式正确性和验证速度提升。评估了7个最先进LLMs和现有LLM验证器，并与传统求解器UAutomizer对比。

Result: LLM验证器目前相比UAutomizer无显著优势，模型能力至关重要，不同模型的速度提升差异明显。监督微调（3589个实例）使Qwen3-Coder-480B的速度提升案例从8%提高到29.2%，Best-of-N采样（N=16）使Claude-sonnet-4从8.8%提升到22.1%。

Conclusion: LLM验证器是有前景的方向但尚未超越传统方法，当前基准对LLMs仍是开放挑战，监督微调和采样技术可显著提升性能。

Abstract: Program verification relies on loop invariants, yet automatically discovering
strong invariants remains a long-standing challenge. We introduce a principled
framework for evaluating LLMs on invariant synthesis. Our approach uses a
verifier-based decision procedure with a formal soundness guarantee and
assesses not only correctness but also the speedup that invariants provide in
verification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based
verifiers against the traditional solver UAutomizer. While LLM-based verifiers
represent a promising direction, they do not yet offer a significant advantage
over UAutomizer. Model capability also proves critical, as shown by sharp
differences in speedups across models, and our benchmark remains an open
challenge for current LLMs. Finally, we show that supervised fine-tuning and
Best-of-N sampling can improve performance: fine-tuning on 3589 instances
raises the percentage of speedup cases for Qwen3-Coder-480B from 8% to 29.2%,
and Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%.

</details>


### [2] [Compiling by Proving: Language-Agnostic Automatic Optimization from Formal Semantics](https://arxiv.org/abs/2509.21793)
*Jianhong Zhao,Everett Hildenbrandt,Juan Conejero,Yongwang Zhao*

Main category: cs.PL

TL;DR: 提出"通过证明编译"的新范式，将验证证明转化为优化的执行规则，通过符号执行构建全路径可达性证明并编译其图结构，在保持正确性的同时实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 验证证明编码了完整的程序行为，但在检查正确性后就被丢弃了，这些证明中蕴含的语义信息可以被利用来优化程序执行。

Method: 构建全路径可达性证明，通过符号执行和编译证明的图结构，将多个语义重写合并为单一规则，在K框架中实现语言无关的扩展。

Result: 评估显示在不同编译范围内都实现了性能提升：操作码级优化显示一致的速度提升，而全程序编译实现了数量级更高的性能增益。

Conclusion: 通过证明编译是一种有效的优化方法，能够利用验证证明中的语义信息生成优化的执行规则，在保持正确性的同时显著提升性能。

Abstract: Verification proofs encode complete program behavior, yet we discard them
after checking correctness. We present compiling by proving, a paradigm that
transforms these proofs into optimized execution rules. By constructing
All-Path Reachability Proofs through symbolic execution and compiling their
graph structure, we consolidate many semantic rewrites into single rules while
preserving correctness by construction. We implement this as a
language-agnostic extension to the K framework. Evaluation demonstrates
performance improvements across different compilation scopes: opcode-level
optimizations show consistent speedups, while whole-program compilation
achieves orders of magnitude greater performance gains.

</details>


### [3] [Committing to the bit: Relational programming with semiring arrays and SAT solving](https://arxiv.org/abs/2509.22614)
*Dmitri Volkov,Yafei Yang,Chung-chieh Shan*

Main category: cs.PL

TL;DR: 提出了semiringKanren关系编程语言，将关系表达式表示为半环数组，通过类型系统限制数组大小，并编译为位串表示。对于布尔半环，可使用SAT求解器高效运行，在解决数独问题时比miniKanren更高效。


<details>
  <summary>Details</summary>
Motivation: 开发一种基于半环的关系编程语言，通过类型系统和位串编译实现高效执行，特别是利用SAT求解器提升布尔半环情况下的性能。

Method: 定义semiringKanren语言，关系表达式表示半环数组；建立类型系统限制数组大小；设计参数化语义；将类型编译为位串表示；对布尔半环使用SAT求解器。

Result: 实验比较显示，在解决数独问题时，semiringKanren比miniKanren表现更高效。

Conclusion: semiringKanren可以成为miniKanren的更高效变体，特别是在利用SAT求解器处理布尔半环时。

Abstract: We propose semiringKanren, a relational programming language where each
relation expression denotes a semiring array. We formalize a type system that
restricts the arrays to finite size. We then define a semantics that is
parameterized by the semiring that the arrays draw their elements from. We
compile semiringKanren types to bitstring representations. For the Boolean
semiring, this compilation enables us to use an SAT solver to run
semiringKanren programs efficiently. We compare the performance of
semiringKanren and faster miniKanren for solving Sudoku puzzles. Our experiment
shows that semiringKanren can be a more efficient variant of miniKanren.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Redesigning GROMACS Halo Exchange: Improving Strong Scaling with GPU-initiated NVSHMEM](https://arxiv.org/abs/2509.21527)
*Mahesh Doijade,Andrey Alekseenko,Ania Brown,Alan Gray,Szilárd Páll*

Main category: cs.DC

TL;DR: 本文提出了一种基于NVSHMEM的GPU内核启动的GROMACS域分解halo交换算法重设计，通过GPU内核融合数据打包和通信，利用硬件延迟隐藏实现细粒度重叠，显著提升了分子动力学模拟的强扩展性能。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟需要强扩展性来解决固定大小问题，但GROMACS对延迟高度敏感，MPI的CPU中心特性在GPU驻留应用中引入了额外延迟，阻碍了GPU利用率和可扩展性。

Method: 采用NVSHMEM的GPU内核启动重设计，高度调优的GPU内核融合数据打包和通信，利用硬件延迟隐藏实现细粒度重叠，在重叠数据转发通信阶段进行内核融合，并通过NVLink使用异步复制引擎优化延迟和带宽。

Result: GPU驻留方案大幅增加了通信计算重叠，在NVLink上GROMACS强扩展性能提升高达1.5倍（节点内）和2倍（多节点），在NVLink+InfiniBand多节点环境下提升达1.3倍。

Conclusion: GPU启动通信对于强扩展各种延迟敏感应用具有显著优势。

Abstract: Improving time-to-solution in molecular dynamics simulations often requires
strong scaling due to fixed-sized problems. GROMACS is highly
latency-sensitive, with peak iteration rates in the sub-millisecond, making
scalability on heterogeneous supercomputers challenging. MPI's CPU-centric
nature introduces additional latencies on GPU-resident applications' critical
path, hindering GPU utilization and scalability. To address these limitations,
we present an NVSHMEM-based GPU kernel-initiated redesign of the GROMACS domain
decomposition halo-exchange algorithm. Highly tuned GPU kernels fuse data
packing and communication, leveraging hardware latency-hiding for fine-grained
overlap. We employ kernel fusion across overlapped data forwarding
communication phases and utilize the asynchronous copy engine over NVLink to
optimize latency and bandwidth. Our GPU-resident formulation greatly increases
communication-computation overlap, improving GROMACS strong scaling performance
across NVLink by up to 1.5x (intra-node) and 2x (multi-node), and up to 1.3x
multi-node over NVLink+InfiniBand. This demonstrates the profound benefits of
GPU-initiated communication for strong-scaling a broad range of
latency-sensitive applications.

</details>


### [5] [Zeppelin: Balancing Variable-length Workloads in Data Parallel Large Model Training](https://arxiv.org/abs/2509.21841)
*Chang Chen,Tiancheng Chen,Jiangfei Duan,Qianchao Zhu,Zerui Wang,Qinghao Hu,Peng Sun,Xiuhong Li,Chao Yang,Torsten Hoefler*

Main category: cs.DC

TL;DR: Zeppelin是一个针对长序列LLM训练的系统，通过层次化序列分区、路由层和重映射层技术，解决了计算通信不平衡问题，相比现有方法实现了2.80倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不同长度序列时存在计算通信成本比例变化、静态NIC-GPU亲和性与动态并行工作负载不匹配、以及注意力模块和线性模块需要不同分区策略的问题。

Method: 1. 注意力模块的层次化序列分区方法，减少通信开销并平衡计算；2. 路由层协调节点间传输以充分利用NIC带宽；3. 重映射层在注意力和线性模块间转换序列布局。

Result: 在各种配置下的综合评估显示，Zeppelin相比最先进方法实现了平均2.80倍的加速。

Conclusion: Zeppelin通过集成三种关键技术，有效解决了长序列LLM训练中的负载不平衡问题，显著提升了训练效率。

Abstract: Training large language models (LLMs) with increasingly long and varying
sequence lengths introduces severe load imbalance challenges in large-scale
data-parallel training. Recent frameworks attempt to mitigate these issues
through data reorganization or hybrid parallel strategies. However, they often
overlook how computational and communication costs scale with sequence length,
resulting in suboptimal performance. We identify three critical challenges: (1)
varying computation-to-communication ratios across sequences of different
lengths in distributed attention, (2) mismatch between static NIC-GPU affinity
and dynamic parallel workloads, and (3) distinct optimal partitioning
strategies required for quadratic attention versus linear components. To
address these challenges, we present Zeppelin, a novel training system that
integrates three key techniques: (1) a hierarchical sequence partitioning
method for the attention module that reduces communication overhead and
balances computation, supported by an efficient attention engine that applies
divergent parallel strategies; (2) a routing layer that orchestrates inter-node
transfers to fully utilize NIC bandwidth; and (3) a remapping layer that
transforms sequence layouts between attention and linear modules, ensuring high
computational efficiency across both. Comprehensive evaluations across diverse
configurations show that Zeppelin delivers an average 2.80x speedup over
state-of-the-art methods.

</details>


### [6] [Code once, Run Green: Automated Green Code Translation in Serverless Computing](https://arxiv.org/abs/2509.22068)
*Sebastian Werner,Mathis Kähler,Alireza Hakamian*

Main category: cs.DC

TL;DR: 利用LLMs将无服务器函数翻译为更节能的编程语言，可减少高达70%的调用能耗，但面临函数适用性和摊销阈值等挑战。


<details>
  <summary>Details</summary>
Motivation: 计算基础设施的碳排放问题日益严重，现有缓解策略依赖利益相关者干预，难以在遗留系统中实施，导致能源债务持续累积。

Method: 设计并实现ReFaaS系统，集成到Fission无服务器框架中，利用多种LLMs将函数代码翻译为更节能的编程语言，同时保持功能正确性。

Result: 翻译后的函数可减少高达70%的调用能耗，在约3000-5000次调用后实现净节能，但并非所有函数都适合翻译。

Conclusion: 虽然面临挑战，但识别出四个关键研究挑战，解决这些挑战可为无服务器计算中的能源债务提供长期自动化缓解方案。

Abstract: The rapid digitization and the increasing use of emerging technologies such
as AI models have significantly contributed to the emissions of computing
infrastructure. Efforts to mitigate this impact typically focus on the
infrastructure level such as powering data centers with renewable energy, or
through the specific design of energy-efficient software. However, both
strategies rely on stakeholder intervention, making their adoption in legacy
and already-deployed systems unlikely. As a result, past architectural and
implementation decisions continue to incur additional energy usage - a
phenomenon we refer to as energy debt.
  Hence, in this paper, we investigate the potential of serverless computing
platforms to automatically reduce energy debt by leveraging the unique access
to function source code. Specifically, we explore whether large language models
(LLMs) can translate serverless functions into more energy-efficient
programming languages while preserving functional correctness. To this end, we
design and implement ReFaaS and integrate it into the Fission serverless
framework. We evaluate multiple LLMs on their ability to perform such code
translations and analyze their impact on energy consumption.
  Our preliminary results indicate that translated functions can reduce
invocation energy by up to 70%, achieving net energy savings after
approximately 3,000 to 5,000 invocations, depending on the LLM used.
Nonetheless, the approach faces several challenges: not all functions are
suitable for translation, and for some, the amortization threshold is
significantly higher or unreachable. Despite these limitations, we identify
four key research challenges whose resolution could unlock long-term, automated
mitigation of energy debt in serverless computing.

</details>


### [7] [The AI_INFN Platform: Artificial Intelligence Development in the Cloud](https://arxiv.org/abs/2509.22117)
*Lucio Anderlini,Giulio Bianchini,Diego Ciangottini,Stefano Dal Pra,Diego Michelotto,Rosa Petrini,Daniele Spiga*

Main category: cs.DC

TL;DR: AI_INFN项目通过Kubernetes平台和云原生解决方案，为INFN的机器学习用例提供GPU加速器资源管理，支持跨不同计算资源（包括LHC计算网格和超级计算机）的工作流开发和扩展。


<details>
  <summary>Details</summary>
Motivation: 机器学习在科学软件中的应用日益增长，但对计算基础设施（特别是硬件加速器的配置和编排）提出了新挑战。AI_INFN项目旨在促进INFN内部ML技术的采用，通过提供AI定制的计算资源支持。

Method: 利用云原生解决方案和Kubernetes平台，结合Virtual Kubelet和InterLink API的卸载机制，管理跨不同资源提供商（包括LHC计算网格和超级计算机）的GPU加速工作流。

Result: 初步测试结果、案例研究和集成场景通过功能测试和基准测试进行了验证，展示了该平台在异构分布式计算资源上有效管理GPU加速工作流的能力。

Conclusion: 该平台为需要为工作负载不同部分提供专用基础设施的用例提供了一个模型，能够有效共享硬件加速器资源，同时确保研究所多样化的研究活动不受影响。

Abstract: Machine Learning (ML) is driving a revolution in the way scientists design,
develop, and deploy data-intensive software. However, the adoption of ML
presents new challenges for the computing infrastructure, particularly in terms
of provisioning and orchestrating access to hardware accelerators for
development, testing, and production. The INFN-funded project AI_INFN
(Artificial Intelligence at INFN) aims at fostering the adoption of ML
techniques within INFN use cases by providing support on multiple aspects,
including the provisioning of AI-tailored computing resources. It leverages
cloud-native solutions in the context of INFN Cloud, to share hardware
accelerators as effectively as possible, ensuring the diversity of the
Institute's research activities is not compromised. In this contribution, we
provide an update on the commissioning of a Kubernetes platform designed to
ease the development of GPU-powered data analysis workflows and their
scalability on heterogeneous distributed computing resources, also using the
offloading mechanism with Virtual Kubelet and InterLink API. This setup can
manage workflows across different resource providers, including sites of the
Worldwide LHC Computing Grid and supercomputers such as CINECA Leonardo,
providing a model for use cases requiring dedicated infrastructures for
different parts of the workload. Initial test results, emerging case studies,
and integration scenarios will be presented with functional tests and
benchmarks.

</details>


### [8] [Orientation does not help with 3-coloring a grid in online-LOCAL](https://arxiv.org/abs/2509.22233)
*Thomas Boudier,Filippo Casagrande,Avinandan Das,Massimo Equi,Henrik Lievonen,Augusto Modanese,Ronja Stimpert*

Main category: cs.DC

TL;DR: 本文证明了即使在算法明确获得网格的全局一致方向的情况下，3-着色网格问题在确定性online-LOCAL和随机化online-LOCAL模型中仍然需要Ω(log n)的局部性。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明3-着色网格在online-LOCAL模型中需要Ω(log n)局部性，但这些证明都依赖于算法无法访问底层网格方向的假设。本文旨在移除这一限制。

Method: 通过开发新的技术方法，使得即使算法被明确提供了网格的全局一致方向，仍然能够证明相同的下界成立。

Result: 成功证明了在确定性online-LOCAL和随机化online-LOCAL模型中，3-着色网格问题即使有全局方向信息，仍然需要Ω(log n)的局部性。

Conclusion: 网格方向信息并不能帮助降低3-着色问题在online-LOCAL模型中的计算复杂度，这强化了该问题的固有难度。

Abstract: The online-LOCAL and SLOCAL models are extensions of the LOCAL model where
nodes are processed in a sequential but potentially adversarial order. So far,
the only problem we know of where the global memory of the online-LOCAL model
has an advantage over SLOCAL is 3-coloring bipartite graphs. Recently, Chang et
al. [PODC 2024] showed that even in grids, 3-coloring requires $\Omega(\log n)$
locality in deterministic online-LOCAL. This result was subsequently extended
by Akbari et al. [STOC 2025] to also hold in randomized online-LOCAL. However,
both proofs heavily rely on the assumption that the algorithm does not have
access to the orientation of the underlying grid. In this paper, we show how to
lift this requirement and obtain the same lower bound (against either model)
even when the algorithm is explicitly given a globally consistent orientation
of the grid.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [Privacy-Preserving Performance Profiling of In-The-Wild GPUs](https://arxiv.org/abs/2509.21762)
*Ian McDougall,Michael Davies,Rahul Chatterjee,Somesh Jha,Karthikeyan Sankaralingam*

Main category: cs.AR

TL;DR: 提出一个行星级实时GPU性能分析系统，解决大规模GPU部署中的性能数据收集问题，同时保证零性能损失和用户隐私保护


<details>
  <summary>Details</summary>
Motivation: 随着GPU应用和硬件复杂性增加，制造商需要从真实部署环境中收集全面的性能数据来优化芯片设计和应用性能，但现有工具只能提供单个用户级别的数据收集

Method: 开发了一个系统，能够在数千个GPU上实时收集低层级硬件性能特征数据，同时确保零性能损失和用户隐私保护

Result: 在模拟的10万个GPU部署环境中运行Torchbench套件应用，证明系统能够有效解决所有三个核心问题

Conclusion: 该系统成功实现了行星级GPU性能分析，为GPU制造商提供了大规模部署环境中的宝贵性能数据，同时保护了用户隐私

Abstract: GPUs are the dominant platform for many important applications today
including deep learning, accelerated computing, and scientific simulation.
However, as the complexity of both applications and hardware increases, GPU
chip manufacturers face a significant challenge: how to gather comprehensive
performance characteristics and value profiles from GPUs deployed in real-world
scenarios. Such data, encompassing the types of kernels executed and the time
spent in each, is crucial for optimizing chip design and enhancing application
performance. Unfortunately, despite the availability of low-level tools like
NSYS and NCU, current methodologies fall short, offering data collection
capabilities only on an individual user basis rather than a broader, more
informative fleet-wide scale. This paper takes on the problem of realizing a
system that allows planet-scale real-time GPU performance profiling of
low-level hardware characteristics. The three fundamental problems we solve
are: i) user experience of achieving this with no slowdown; ii) preserving user
privacy, so that no 3rd party is aware of what applications any user runs; iii)
efficacy in showing we are able to collect data and assign it applications even
when run on 1000s of GPUs. Our results simulate a 100,000 size GPU deployment,
running applications from the Torchbench suite, showing our system addresses
all 3 problems.

</details>


### [10] [NeuroScalar: A Deep Learning Framework for Fast, Accurate, and In-the-Wild Cycle-Level Performance Prediction](https://arxiv.org/abs/2509.22410)
*Shayne Wadle,Yanxin Zhang,Vikas Singh,Karthikeyan Sankaralingam*

Main category: cs.AR

TL;DR: 提出基于深度学习的微处理器性能模拟框架，可在现有硬件上评估未来处理器设计，实现高保真度、大规模硬件A/B测试


<details>
  <summary>Details</summary>
Motivation: 传统微处理器设计评估受限于缓慢的周期精确模拟器，且依赖不具代表性的基准测试轨迹

Method: 使用微架构无关特征训练深度学习模型预测周期级性能，结合轻量级硬件轨迹收集器和采样策略

Result: 在商用GPU上实现5 MIPS模拟速度，仅0.1%性能开销；专用加速器Neutrino比GPU提升85倍性能

Conclusion: 该框架能够在真实应用上实现准确的性能分析和大规模硬件A/B测试

Abstract: The evaluation of new microprocessor designs is constrained by slow,
cycle-accurate simulators that rely on unrepresentative benchmark traces. This
paper introduces a novel deep learning framework for high-fidelity,
``in-the-wild'' simulation on production hardware. Our core contribution is a
DL model trained on microarchitecture-independent features to predict
cycle-level performance for hypothetical processor designs. This unique
approach allows the model to be deployed on existing silicon to evaluate future
hardware. We propose a complete system featuring a lightweight hardware trace
collector and a principled sampling strategy to minimize user impact. This
system achieves a simulation speed of 5 MIPS on a commodity GPU, imposing a
mere 0.1% performance overhead. Furthermore, our co-designed Neutrino on-chip
accelerator improves performance by 85x over the GPU. We demonstrate that this
framework enables accurate performance analysis and large-scale hardware A/B
testing on a massive scale using real-world applications.

</details>


### [11] [AxLLM: accelerator architecture for large language models with computation reuse capability](https://arxiv.org/abs/2509.22512)
*Soroush Ahadi,Mehdi Modarressi,Masoud Daneshtalab*

Main category: cs.AR

TL;DR: AxLLM是一种针对量化大语言模型的硬件加速器架构，通过消除冗余计算，实现了高达90%的计算量减少、28%的能耗降低和1.7倍的加速比。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要巨大的计算能力和内存资源，部署效率面临挑战。量化不仅减小模型尺寸，还能增加参数局部性，为计算重用创造机会。

Method: 提出AxLLM硬件加速器架构，采用新颖的冗余消除技术，缓存并重用重复权重值的乘法结果。架构包含双乘法和重用流水线，支持基础模型和LoRA微调模型，无需参数修改、重新训练或离线预处理。

Result: 实验结果显示，AxLLM实现了高达90%的计算量减少，能耗降低28%，速度提升1.7倍。

Conclusion: AxLLM为在专用硬件上加速大语言模型提供了一个可扩展且高效的解决方案。

Abstract: Large language models demand massive computational power and memory
resources, posing significant challenges for efficient deployment. While
quantization has been widely explored to reduce model size and computation,
this paper demonstrates an additional benefit: quantization increases parameter
locality, creating opportunities for computation reuse. Building on this
insight, we propose AxLLM, a hardware accelerator architecture designed for
quantized models. Axllm introduces a novel redundancy elimination technique
that caches and reuses multiplication results for repeated weight values,
substantially reducing redundant operations. The architecture features dual
multiply and reuse pipelines, efficiently supporting both base models and LoRA
fine-tuned models without altering parameters, retraining, or requiring offline
preprocessing. Experimental results show that AxLLM achieves up to 90%
reduction in computations, delivering 28% lower energy consumption and a 1.7x
speedup over baseline execution. These results highlight Axllm as a scalable
and efficient solution for accelerating LLMs on specialized hardware.

</details>
