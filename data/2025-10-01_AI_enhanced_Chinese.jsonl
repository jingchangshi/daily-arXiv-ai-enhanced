{"id": "2509.25391", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.25391", "abs": "https://arxiv.org/abs/2509.25391", "authors": ["Fernanda Zapata Bascu\u00f1\u00e1n", "Alan Ezequiel Fuster"], "title": "smallNet: Implementation of a convolutional layer in tiny FPGAs", "comment": null, "summary": "Since current neural network development systems in Xilinx and VLSI require\ncodevelopment with Python libraries, the first stage of a convolutional network\nhas been implemented by developing a convolutional layer entirely in Verilog.\nThis handcoded design, free of IP cores and based on a filter polynomial like\nstructure, enables straightforward deployment not only on low cost FPGAs but\nalso on SoMs, SoCs, and ASICs. We analyze the limitations of numerical\nrepresentations and compare our implemented architecture, smallNet, with its\ncomputer based counterpart, demonstrating a 5.1x speedup, over 81%\nclassification accuracy, and a total power consumption of just 1.5 W. The\nalgorithm is validated on a single-core Cora Z7, demonstrating its feasibility\nfor real time, resource-constrained embedded applications.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u5168\u7528Verilog\u5b9e\u73b0\u7684\u5377\u79ef\u5c42\uff0c\u65e0\u9700Python\u5e93\u548cIP\u6838\uff0c\u57fa\u4e8e\u6ee4\u6ce2\u5668\u591a\u9879\u5f0f\u7ed3\u6784\uff0c\u53ef\u5728\u4f4e\u6210\u672cFPGA\u3001SoM\u3001SoC\u548cASIC\u4e0a\u90e8\u7f72\u3002\u76f8\u6bd4\u8ba1\u7b97\u673a\u7248\u672c\u5b9e\u73b05.1\u500d\u52a0\u901f\uff0c81%\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u529f\u8017\u4ec51.5W\u3002", "motivation": "\u5f53\u524dXilinx\u548cVLSI\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u5f00\u53d1\u7cfb\u7edf\u9700\u8981\u4e0ePython\u5e93\u534f\u540c\u5f00\u53d1\uff0c\u9650\u5236\u4e86\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u5b8c\u5168\u7528Verilog\u624b\u5199\u8bbe\u8ba1\u5377\u79ef\u5c42\uff0c\u57fa\u4e8e\u6ee4\u6ce2\u5668\u591a\u9879\u5f0f\u7ed3\u6784\uff0c\u907f\u514d\u4f7f\u7528IP\u6838\uff0c\u5728\u5355\u6838Cora Z7\u4e0a\u9a8c\u8bc1\u3002", "result": "\u5b9e\u73b05.1\u500d\u901f\u5ea6\u63d0\u5347\uff0c\u8d85\u8fc781%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u603b\u529f\u8017\u4ec51.5W\u3002", "conclusion": "\u8be5\u67b6\u6784\u8bc1\u660e\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u5e94\u7528\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.25626", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.25626", "abs": "https://arxiv.org/abs/2509.25626", "authors": ["Yi Hu", "Huiyang Zhou"], "title": "LLM-Powered Code Analysis and Optimization for Gaussian Splatting Kernels", "comment": null, "summary": "3D Gaussian splatting (3DGS) is a transformative technique with profound\nimplications on novel view synthesis and real-time rendering. Given its\nimportance, there have been many attempts to improve its performance. However,\nwith the increasing complexity of GPU architectures and the vast search space\nof performance-tuning parameters, it is a challenging task. Although manual\noptimizations have achieved remarkable speedups, they require domain expertise\nand the optimization process can be highly time consuming and error prone. In\nthis paper, we propose to exploit large language models (LLMs) to analyze and\noptimize Gaussian splatting kernels. To our knowledge, this is the first work\nto use LLMs to optimize highly specialized real-world GPU kernels. We reveal\nthe intricacies of using LLMs for code optimization and analyze the code\noptimization techniques from the LLMs. We also propose ways to collaborate with\nLLMs to further leverage their capabilities. For the original 3DGS code on the\nMipNeRF360 datasets, LLMs achieve significant speedups, 19% with Deepseek and\n24% with GPT-5, demonstrating the different capabilities of different LLMs. By\nfeeding additional information from performance profilers, the performance\nimprovement from LLM-optimized code is enhanced to up to 42% and 38% on\naverage. In comparison, our best-effort manually optimized version can achieve\na performance improvement up to 48% and 39% on average, showing that there are\nstill optimizations beyond the capabilities of current LLMs. On the other hand,\neven upon a newly proposed 3DGS framework with algorithmic optimizations,\nSeele, LLMs can still further enhance its performance by 6%, showing that there\nare optimization opportunities missed by domain experts. This highlights the\npotential of collaboration between domain experts and LLMs.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u4f18\u53163D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u7684GPU\u5185\u6838\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5c55\u793a\u4e86LLMs\u4e0e\u9886\u57df\u4e13\u5bb6\u534f\u4f5c\u7684\u6f5c\u529b\u3002", "motivation": "3DGS\u5728\u65b0\u578b\u89c6\u56fe\u5408\u6210\u548c\u5b9e\u65f6\u6e32\u67d3\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46GPU\u67b6\u6784\u590d\u6742\u6027\u548c\u6027\u80fd\u8c03\u4f18\u53c2\u6570\u641c\u7d22\u7a7a\u95f4\u5927\uff0c\u624b\u52a8\u4f18\u5316\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u4e14\u8017\u65f6\u6613\u9519\u3002", "method": "\u5229\u7528LLMs\u5206\u6790\u548c\u4f18\u5316\u9ad8\u65af\u6cfc\u6e85\u5185\u6838\uff0c\u7ed3\u5408\u6027\u80fd\u5206\u6790\u5668\u63d0\u4f9b\u989d\u5916\u4fe1\u606f\uff0c\u5e76\u4e0e\u9886\u57df\u4e13\u5bb6\u624b\u52a8\u4f18\u5316\u7248\u672c\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5728MipNeRF360\u6570\u636e\u96c6\u4e0a\uff0cLLMs\u4f18\u5316\u4f7f\u539f\u59cb3DGS\u4ee3\u7801\u83b7\u5f9719-24%\u52a0\u901f\uff1b\u7ed3\u5408\u6027\u80fd\u5206\u6790\u5668\u4fe1\u606f\u540e\u63d0\u5347\u81f3\u5e73\u574738%\u3001\u6700\u9ad842%\uff1b\u624b\u52a8\u4f18\u5316\u53ef\u8fbe\u5e73\u574739%\u3001\u6700\u9ad848%\u3002\u5728\u4f18\u5316\u540e\u7684Seele\u6846\u67b6\u4e0a\uff0cLLMs\u4ecd\u80fd\u8fdb\u4e00\u6b65\u63d0\u53476%\u6027\u80fd\u3002", "conclusion": "LLMs\u5728GPU\u5185\u6838\u4f18\u5316\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u624b\u52a8\u4f18\u5316\u8d85\u8d8aLLMs\u7684\u60c5\u51b5\uff0c\u8868\u660eLLMs\u4e0e\u9886\u57df\u4e13\u5bb6\u534f\u4f5c\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.25853", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.25853", "abs": "https://arxiv.org/abs/2509.25853", "authors": ["Jingyao Zhang", "Jaewoo Park", "Jongeun Lee", "Elaheh Sadredini"], "title": "SAIL: SRAM-Accelerated LLM Inference System with Lookup-Table-based GEMV", "comment": null, "summary": "Large Language Model (LLM) inference requires substantial computational\nresources, yet CPU-based inference remains essential for democratizing AI due\nto the widespread availability of CPUs compared to specialized accelerators.\nHowever, efficient LLM inference on CPUs faces two fundamental challenges: (1)\nexisting CPU architectures struggle with low-precision arithmetic required by\nquantized models, where optimal bit precision varies across models and layers;\nand (2) the memory-bound nature of the token generation phase creates severe\nperformance bottlenecks. To address these challenges, we propose SAIL\n(SRAM-Accelerated Inference of LLMs), a CPU-based inference solution that\nefficiently supports arbitrary bit precisions with minimal overhead. SAIL\nintegrates three key innovations: First, we introduce Batched LUT-based General\nMatrix-Vector Multiplication (LUT-GEMV) with SRAM-based processing-in-memory,\nenabling high data reuse through lookup tables and reducing memory movement.\nSecond, our Pattern-Aware LUT optimization identifies and exploits redundancy\nin input activation patterns, reducing computation cycles by 13.8\\%. Third, we\ndevelop an in-memory type conversion algorithm that leverages PIM's parallelism\nfor efficient de-/quantization operations, alleviating pressure on CPU's vector\nunits. Our architecture requires only 2\\% hardware overhead and a single new\ninstruction, while maintaining dual functionality as both compute and storage\nunits. Experimental evaluations using a modified gem5 simulator demonstrate\nthat SAIL achieves up to 10.7x speedup and 19.9x higher tokens per dollar\ncompared to ARM Neoverse-N1 CPU baselines, and up to 7.04x better cost\nefficiency than NVIDIA V100 GPUs, establishing a practical path for efficient\nCPU-based LLM inference.", "AI": {"tldr": "SAIL\u662f\u4e00\u79cd\u57fa\u4e8eCPU\u7684LLM\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7SRAM\u52a0\u901f\u548c\u67e5\u627e\u8868\u6280\u672f\uff0c\u652f\u6301\u4efb\u610f\u6bd4\u7279\u7cbe\u5ea6\uff0c\u5728CPU\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u3002", "motivation": "CPU\u63a8\u7406\u5bf9\u4e8eAI\u6c11\u4e3b\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u73b0\u6709CPU\u67b6\u6784\u96be\u4ee5\u5904\u7406\u91cf\u5316\u6a21\u578b\u7684\u4f4e\u7cbe\u5ea6\u8fd0\u7b97\uff0c\u4ee5\u53catoken\u751f\u6210\u9636\u6bb5\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\u3002", "method": "\u63d0\u51faSAIL\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a\u57fa\u4e8eSRAM\u7684\u6279\u5904\u7406LUT-GEMV\u3001\u6a21\u5f0f\u611f\u77e5LUT\u4f18\u5316\u548c\u5185\u5b58\u5185\u7c7b\u578b\u8f6c\u6362\u7b97\u6cd5\uff0c\u4ec5\u97002%\u786c\u4ef6\u5f00\u9500\u548c\u4e00\u6761\u65b0\u6307\u4ee4\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0cSAIL\u76f8\u6bd4ARM Neoverse-N1 CPU\u57fa\u7ebf\u5b9e\u73b010.7\u500d\u52a0\u901f\u548c19.9\u500d\u6bcf\u7f8e\u5143token\u6570\u63d0\u5347\uff0c\u6bd4NVIDIA V100 GPU\u6210\u672c\u6548\u7387\u9ad87.04\u500d\u3002", "conclusion": "SAIL\u4e3a\u57fa\u4e8eCPU\u7684\u9ad8\u6548LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u5728\u4fdd\u6301\u4f4e\u786c\u4ef6\u5f00\u9500\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2509.26065", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.26065", "abs": "https://arxiv.org/abs/2509.26065", "authors": ["Alberto Scionti", "Paolo Savio", "Francesco Lubrano", "Olivier Terzo", "Marco Ferretti", "Florin Apopei", "Juri Bellucci", "Ennio Spano", "Luca Carriere"], "title": "Runtime Energy Monitoring for RISC-V Soft-Cores", "comment": null, "summary": "Energy efficiency is one of the major concern in designing advanced computing\ninfrastructures. From single nodes to large-scale systems (data centers),\nmonitoring the energy consumption of the computing system when applications run\nis a critical task. Designers and application developers often rely on software\ntools and detailed architectural models to extract meaningful information and\ndetermine the system energy consumption. However, when a design space\nexploration is required, designers may incur in continuous tuning of the models\nto match with the system under evaluation. To overcome such limitations, we\npropose a holistic approach to monitor energy consumption at runtime without\nthe need of running complex (micro-)architectural models. Our approach is based\non a measurement board coupled with a FPGA-based System-on-Module. The\nmeasuring board captures currents and voltages (up to tens measuring points)\ndriving the FPGA and exposes such values through a specific memory region. A\nrunning service reads and computes energy consumption statistics without\nconsuming extra resources on the FPGA device. Our approach is also scalable to\nmonitoring of multi-nodes infrastructures (clusters). We aim to leverage this\nframework to perform experiments in the context of an aeronautical design\napplication; specifically, we will look at optimizing performance and energy\nconsumption of a shallow artificial neural network on RISC-V based soft-cores.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u8fd0\u884c\u65f6\u80fd\u8017\u76d1\u63a7\u65b9\u6cd5\uff0c\u65e0\u9700\u590d\u6742\u67b6\u6784\u6a21\u578b\uff0c\u53ef\u6269\u5c55\u81f3\u591a\u8282\u70b9\u96c6\u7fa4\u76d1\u63a7", "motivation": "\u4f20\u7edf\u80fd\u8017\u76d1\u63a7\u65b9\u6cd5\u9700\u8981\u590d\u6742\u7684\u67b6\u6784\u6a21\u578b\u548c\u6301\u7eed\u8c03\u4f18\uff0c\u9650\u5236\u4e86\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u7684\u6548\u7387", "method": "\u4f7f\u7528\u6d4b\u91cf\u677f\u4e0eFPGA\u7cfb\u7edf\u6a21\u5757\u7ed3\u5408\uff0c\u6355\u83b7\u7535\u6d41\u7535\u538b\u503c\u5e76\u901a\u8fc7\u7279\u5b9a\u5185\u5b58\u533a\u57df\u66b4\u9732\uff0c\u8fd0\u884c\u670d\u52a1\u8bfb\u53d6\u5e76\u8ba1\u7b97\u80fd\u8017\u7edf\u8ba1", "result": "\u5b9e\u73b0\u4e86\u65e0\u9700\u6d88\u8017FPGA\u989d\u5916\u8d44\u6e90\u7684\u5b9e\u65f6\u80fd\u8017\u76d1\u63a7\uff0c\u652f\u6301\u6570\u5341\u4e2a\u6d4b\u91cf\u70b9", "conclusion": "\u8be5\u6846\u67b6\u53ef\u7528\u4e8e\u822a\u7a7a\u8bbe\u8ba1\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u4e0e\u80fd\u8017\u4f18\u5316\uff0c\u7279\u522b\u662f\u5728RISC-V\u8f6f\u6838\u4e0a\u7684\u6d45\u5c42\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"}}
{"id": "2509.25415", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.25415", "abs": "https://arxiv.org/abs/2509.25415", "authors": ["Jan Droll"], "title": "Permuting Transactions in Ethereum Blocks: An Empirical Study", "comment": "17 pages, 6 figures, experiment code available", "summary": "Several recent proposals implicitly or explicitly suggest making use of\nrandomized transaction ordering within a block to mitigate centralization\neffects and to improve fairness in the Ethereum ecosystem. However,\ntransactions and blocks are subject to gas limits and protocol rules. In a\nrandomized transaction order, the behavior of transactions may change depending\non other transactions in the same block, leading to invalid blocks and varying\ngas consumptions. In this paper, we quantify and characterize protocol\nviolations, execution errors and deviations in gas consumption of blocks and\ntransactions to examine technical deployability. For that, we permute and\nexecute the transactions of over 335,000 Ethereum Mainnet blocks multiple\ntimes. About 22% of block permutations are invalid due to protocol violations\ncaused by privately mined transactions or blocks close to their gas limit.\nAlso, almost all transactions which show execution errors under permutation but\nnot in the original order are privately mined transactions. Only 6% of\ntransactions show deviations in gas consumption and 98% of block permutations\ndeviate at most 10% from their original gas consumption. From a technical\nperspective, these results suggest that randomized transaction ordering may be\nfeasible if transaction selection is handled carefully.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u4ee5\u592a\u574a\u968f\u673a\u4ea4\u6613\u6392\u5e8f\u7684\u6280\u672f\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u7ea622%\u7684\u533a\u5757\u6392\u5217\u56e0\u534f\u8bae\u8fdd\u89c4\u800c\u65e0\u6548\uff0c\u4e3b\u8981\u6d89\u53ca\u79c1\u6709\u6316\u77ff\u4ea4\u6613\u548c\u63a5\u8fd1gas\u9650\u5236\u7684\u533a\u5757\uff0c\u4f46\u5927\u90e8\u5206\u533a\u5757\u7684gas\u6d88\u8017\u504f\u5dee\u4e0d\u8d85\u8fc710%\u3002", "motivation": "\u8bc4\u4f30\u968f\u673a\u4ea4\u6613\u6392\u5e8f\u5728\u4ee5\u592a\u574a\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u6280\u672f\u53ef\u90e8\u7f72\u6027\uff0c\u4ee5\u7f13\u89e3\u4e2d\u5fc3\u5316\u6548\u5e94\u548c\u63d0\u9ad8\u516c\u5e73\u6027\uff0c\u540c\u65f6\u8003\u8651gas\u9650\u5236\u548c\u534f\u8bae\u89c4\u5219\u7684\u5f71\u54cd\u3002", "method": "\u5bf9\u8d85\u8fc7335,000\u4e2a\u4ee5\u592a\u574a\u4e3b\u7f51\u533a\u5757\u7684\u4ea4\u6613\u8fdb\u884c\u591a\u6b21\u6392\u5217\u548c\u6267\u884c\uff0c\u91cf\u5316\u534f\u8bae\u8fdd\u89c4\u3001\u6267\u884c\u9519\u8bef\u548cgas\u6d88\u8017\u504f\u5dee\u3002", "result": "22%\u7684\u533a\u5757\u6392\u5217\u56e0\u534f\u8bae\u8fdd\u89c4\u800c\u65e0\u6548\uff1b\u51e0\u4e4e\u6240\u6709\u5728\u6392\u5217\u4e2d\u663e\u793a\u6267\u884c\u9519\u8bef\u4f46\u5728\u539f\u59cb\u987a\u5e8f\u4e2d\u6b63\u5e38\u7684\u4ea4\u6613\u90fd\u662f\u79c1\u6709\u6316\u77ff\u4ea4\u6613\uff1b\u4ec56%\u7684\u4ea4\u6613\u663e\u793agas\u6d88\u8017\u504f\u5dee\uff0c98%\u7684\u533a\u5757\u6392\u5217gas\u6d88\u8017\u504f\u5dee\u4e0d\u8d85\u8fc710%\u3002", "conclusion": "\u4ece\u6280\u672f\u89d2\u5ea6\u770b\uff0c\u5982\u679c\u4ea4\u6613\u9009\u62e9\u5904\u7406\u5f97\u5f53\uff0c\u968f\u673a\u4ea4\u6613\u6392\u5e8f\u53ef\u80fd\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2509.26043", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.26043", "abs": "https://arxiv.org/abs/2509.26043", "authors": ["Alberto Scionti", "Paolo Savio", "Francesco Lubrano", "Federico Stirano", "Antonino Nespola", "Olivier Terzo", "Corrado De Sio", "Luca Sterpone"], "title": "Enabling Time-Aware Priority Traffic Management over Distributed FPGA Nodes", "comment": null, "summary": "Network Interface Cards (NICs) greatly evolved from simple basic devices\nmoving traffic in and out of the network to complex heterogeneous systems\noffloading host CPUs from performing complex tasks on in-transit packets. These\nlatter comprise different types of devices, ranging from NICs accelerating\nfixed specific functions (e.g., on-the-fly data compression/decompression,\nchecksum computation, data encryption, etc.) to complex Systems-on-Chip (SoC)\nequipped with both general purpose processors and specialized engines\n(Smart-NICs). Similarly, Field Programmable Gate Arrays (FPGAs) moved from pure\nreprogrammable devices to modern heterogeneous systems comprising\ngeneral-purpose processors, real-time cores and even AI-oriented engines.\nFurthermore, the availability of high-speed network interfaces (e.g., SFPs)\nmakes modern FPGAs a good choice for implementing Smart-NICs. In this work, we\nextended the functionalities offered by an open-source NIC implementation\n(Corundum) by enabling time-aware traffic management in hardware, and using\nthis feature to control the bandwidth associated with different traffic\nclasses. By exposing dedicated control registers on the AXI bus, the driver of\nthe NIC can easily configure the transmission bandwidth of different\nprioritized queues. Basically, each control register is associated with a\nspecific transmission queue (Corundum can expose up to thousands of\ntransmission and receiving queues), and sets up the fraction of time in a\ntransmission window which the queue is supposed to get access the output port\nand transmit the packets. Queues are then prioritized and associated to\ndifferent traffic classes through the Linux QDISC mechanism. Experimental\nevaluation demonstrates that the approach allows to properly manage the\nbandwidth reserved to the different transmission flows.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u5f00\u6e90NIC\u5b9e\u73b0Corundum\u7684\u529f\u80fd\uff0c\u5728\u786c\u4ef6\u5c42\u9762\u5b9e\u73b0\u4e86\u65f6\u95f4\u611f\u77e5\u7684\u6d41\u91cf\u7ba1\u7406\uff0c\u901a\u8fc7\u63a7\u5236\u4e0d\u540c\u4f18\u5148\u7ea7\u961f\u5217\u7684\u4f20\u8f93\u5e26\u5bbd\u6765\u7ba1\u7406\u6d41\u91cf\u7c7b\u522b\u3002", "motivation": "\u73b0\u4ee3\u7f51\u7edc\u63a5\u53e3\u5361(NIC)\u5df2\u4ece\u7b80\u5355\u7684\u7f51\u7edc\u6d41\u91cf\u4f20\u8f93\u8bbe\u5907\u6f14\u53d8\u4e3a\u590d\u6742\u7684\u5f02\u6784\u7cfb\u7edf\uff0c\u80fd\u591f\u5378\u8f7d\u4e3b\u673aCPU\u7684\u590d\u6742\u4efb\u52a1\u3002FPGA\u4e5f\u4ece\u7eaf\u53ef\u7f16\u7a0b\u8bbe\u5907\u53d1\u5c55\u4e3a\u5305\u542b\u901a\u7528\u5904\u7406\u5668\u548c\u4e13\u7528\u5f15\u64ce\u7684\u5f02\u6784\u7cfb\u7edf\u3002\u4f5c\u8005\u65e8\u5728\u5229\u7528\u73b0\u4ee3FPGA\u5b9e\u73b0\u667a\u80fdNIC\uff0c\u5e76\u901a\u8fc7\u786c\u4ef6\u7ea7\u65f6\u95f4\u611f\u77e5\u6d41\u91cf\u7ba1\u7406\u6765\u7cbe\u786e\u63a7\u5236\u4e0d\u540c\u6d41\u91cf\u7c7b\u522b\u7684\u5e26\u5bbd\u5206\u914d\u3002", "method": "\u6269\u5c55Corundum\u5f00\u6e90NIC\u5b9e\u73b0\uff0c\u5728AXI\u603b\u7ebf\u4e0a\u66b4\u9732\u4e13\u7528\u63a7\u5236\u5bc4\u5b58\u5668\uff0c\u4f7fNIC\u9a71\u52a8\u7a0b\u5e8f\u80fd\u591f\u914d\u7f6e\u4e0d\u540c\u4f18\u5148\u7ea7\u961f\u5217\u7684\u4f20\u8f93\u5e26\u5bbd\u3002\u6bcf\u4e2a\u63a7\u5236\u5bc4\u5b58\u5668\u5bf9\u5e94\u7279\u5b9a\u4f20\u8f93\u961f\u5217\uff0c\u8bbe\u7f6e\u961f\u5217\u5728\u4f20\u8f93\u7a97\u53e3\u5185\u83b7\u5f97\u8f93\u51fa\u7aef\u53e3\u8bbf\u95ee\u6743\u9650\u7684\u65f6\u95f4\u6bd4\u4f8b\u3002\u901a\u8fc7Linux QDISC\u673a\u5236\u5c06\u961f\u5217\u4e0e\u4e0d\u540c\u6d41\u91cf\u7c7b\u522b\u5173\u8054\u5e76\u8bbe\u7f6e\u4f18\u5148\u7ea7\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7ba1\u7406\u4e0d\u540c\u4f20\u8f93\u6d41\u9884\u7559\u7684\u5e26\u5bbd\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u961f\u5217\u4f20\u8f93\u7684\u7cbe\u786e\u5e26\u5bbd\u63a7\u5236\u3002", "conclusion": "\u901a\u8fc7\u5728\u786c\u4ef6\u5c42\u9762\u5b9e\u73b0\u65f6\u95f4\u611f\u77e5\u7684\u6d41\u91cf\u7ba1\u7406\uff0c\u6210\u529f\u6269\u5c55\u4e86Corundum NIC\u7684\u529f\u80fd\uff0c\u4e3a\u4e0d\u540c\u6d41\u91cf\u7c7b\u522b\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u5e26\u5bbd\u63a7\u5236\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u73b0\u4ee3FPGA\u5728\u5b9e\u73b0\u667a\u80fdNIC\u65b9\u9762\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.25555", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25555", "abs": "https://arxiv.org/abs/2509.25555", "authors": ["Amirreza Sokhankhosh", "Khalid Hassan", "Sara Rouhani"], "title": "Enhancing Split Learning with Sharded and Blockchain-Enabled SplitFed Approaches", "comment": "Accepted by the 2025 IEEE International Conference on Blockchain\n  (Blockchain)", "summary": "Collaborative and distributed learning techniques, such as Federated Learning\n(FL) and Split Learning (SL), hold significant promise for leveraging sensitive\ndata in privacy-critical domains. However, FL and SL suffer from key\nlimitations -- FL imposes substantial computational demands on clients, while\nSL leads to prolonged training times. To overcome these challenges, SplitFed\nLearning (SFL) was introduced as a hybrid approach that combines the strengths\nof FL and SL. Despite its advantages, SFL inherits scalability, performance,\nand security issues from SL. In this paper, we propose two novel frameworks:\nSharded SplitFed Learning (SSFL) and Blockchain-enabled SplitFed Learning\n(BSFL). SSFL addresses the scalability and performance constraints of SFL by\ndistributing the workload and communication overhead of the SL server across\nmultiple parallel shards. Building upon SSFL, BSFL replaces the centralized\nserver with a blockchain-based architecture that employs a committee-driven\nconsensus mechanism to enhance fairness and security. BSFL incorporates an\nevaluation mechanism to exclude poisoned or tampered model updates, thereby\nmitigating data poisoning and model integrity attacks. Experimental evaluations\nagainst baseline SL and SFL approaches show that SSFL improves performance and\nscalability by 31.2% and 85.2%, respectively. Furthermore, BSFL increases\nresilience to data poisoning attacks by 62.7% while maintaining superior\nperformance under normal operating conditions. To the best of our knowledge,\nBSFL is the first blockchain-enabled framework to implement an end-to-end\ndecentralized SplitFed Learning system.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u65b0\u9896\u6846\u67b6SSFL\u548cBSFL\uff0c\u89e3\u51b3SplitFed Learning\u7684\u53ef\u6269\u5c55\u6027\u3001\u6027\u80fd\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002SSFL\u901a\u8fc7\u5206\u7247\u6280\u672f\u63d0\u5347\u6027\u80fd85.2%\uff0cBSFL\u7ed3\u5408\u533a\u5757\u94fe\u6280\u672f\u589e\u5f3a\u5b89\u5168\u6027\uff0c\u62b5\u5fa1\u6570\u636e\u4e2d\u6bd2\u653b\u51fb62.7%\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60(FL)\u548c\u5206\u5272\u5b66\u4e60(SL)\u5728\u9690\u79c1\u5173\u952e\u9886\u57df\u6709\u5e94\u7528\u524d\u666f\uff0c\u4f46FL\u8ba1\u7b97\u9700\u6c42\u5927\uff0cSL\u8bad\u7ec3\u65f6\u95f4\u957f\u3002SplitFed Learning(SFL)\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u4f46\u4ecd\u5b58\u5728\u53ef\u6269\u5c55\u6027\u3001\u6027\u80fd\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002", "method": "SSFL\u901a\u8fc7\u5c06SL\u670d\u52a1\u5668\u5de5\u4f5c\u8d1f\u8f7d\u548c\u901a\u4fe1\u5f00\u9500\u5206\u5e03\u5230\u591a\u4e2a\u5e76\u884c\u5206\u7247\u6765\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff1bBSFL\u5728SSFL\u57fa\u7840\u4e0a\u7528\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u67b6\u6784\u66ff\u4ee3\u96c6\u4e2d\u5f0f\u670d\u52a1\u5668\uff0c\u91c7\u7528\u59d4\u5458\u4f1a\u9a71\u52a8\u7684\u5171\u8bc6\u673a\u5236\uff0c\u5305\u542b\u8bc4\u4f30\u673a\u5236\u6392\u9664\u4e2d\u6bd2\u6a21\u578b\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793aSSFL\u76f8\u6bd4\u57fa\u7ebfSL\u548cSFL\u65b9\u6cd5\uff0c\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u5206\u522b\u63d0\u534731.2%\u548c85.2%\uff1bBSFL\u5bf9\u6570\u636e\u4e2d\u6bd2\u653b\u51fb\u7684\u62b5\u5fa1\u80fd\u529b\u63d0\u534762.7%\uff0c\u5728\u6b63\u5e38\u64cd\u4f5c\u6761\u4ef6\u4e0b\u4fdd\u6301\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "BSFL\u662f\u9996\u4e2a\u5b9e\u73b0\u7aef\u5230\u7aef\u53bb\u4e2d\u5fc3\u5316SplitFed Learning\u7cfb\u7edf\u7684\u533a\u5757\u94fe\u8d4b\u80fd\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86SFL\u7684\u53ef\u6269\u5c55\u6027\u3001\u6027\u80fd\u548c\u5b89\u5168\u6027\u9650\u5236\u3002"}}
{"id": "2509.25605", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.25605", "abs": "https://arxiv.org/abs/2509.25605", "authors": ["Brian Kelley", "Sivasankaran Rajamanickam"], "title": "LAPIS: A Performance Portable, High Productivity Compiler Framework", "comment": "14 pages (10 excluding references and appendices). 5 figures", "summary": "Portability, performance, and productivity are three critical dimensions for\nevaluating a programming model or compiler infrastructure. Several modern\nprogramming models for computational science focus on performance and\nportability. On the other end, several machine learning focused programming\nmodels focus on portability and productivity. A clear solution that is strong\nin all three dimensions has yet to emerge. A second related problem arises when\nuse cases from computational science converge with machine learning. The\ndisparate popular frameworks of these fields require programmers to manually\nintegrate codes written in different frameworks. Finally, several programming\nframeworks lack easy options for extensibility as any new computer architecture\nchange require complex changes to the programming models. We present LAPIS, an\nMLIR-based compiler that addresses all three of these challenges. We\ndemonstrate that LAPIS can automatically lower sparse and dense linear algebra\nkernels from computational science and artificial intelligence use cases. We\nalso show how LAPIS facilitates the integration of codes between PyTorch and\nKokkos. We compare kernel performance with the default MLIR implementations on\ndiverse architectures to demonstrate portability. By developing a dialect that\nis built on the principles of the Kokkos ecosystem, LAPIS also allows\nextensibility of the framework to new architectures.", "AI": {"tldr": "LAPIS\u662f\u4e00\u4e2a\u57fa\u4e8eMLIR\u7684\u7f16\u8bd1\u5668\uff0c\u89e3\u51b3\u4e86\u7f16\u7a0b\u6a21\u578b\u5728\u53ef\u79fb\u690d\u6027\u3001\u6027\u80fd\u548c\u751f\u4ea7\u529b\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u6311\u6218\uff0c\u80fd\u591f\u81ea\u52a8\u964d\u4f4e\u8ba1\u7b97\u79d1\u5b66\u548cAI\u4e2d\u7684\u7a00\u758f\u548c\u7a20\u5bc6\u7ebf\u6027\u4ee3\u6570\u5185\u6838\uff0c\u5e76\u4fc3\u8fdbPyTorch\u548cKokkos\u4e4b\u95f4\u7684\u4ee3\u7801\u96c6\u6210\u3002", "motivation": "\u5f53\u524d\u7f16\u7a0b\u6a21\u578b\u5728\u53ef\u79fb\u690d\u6027\u3001\u6027\u80fd\u548c\u751f\u4ea7\u529b\u8fd9\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u8ba1\u7b97\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e0d\u540c\u6846\u67b6\u9700\u8981\u624b\u52a8\u96c6\u6210\u4ee3\u7801\uff0c\u4e14\u73b0\u6709\u6846\u67b6\u7f3a\u4e4f\u5bf9\u65b0\u8ba1\u7b97\u673a\u67b6\u6784\u7684\u6613\u6269\u5c55\u6027\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eMLIR\u7684LAPIS\u7f16\u8bd1\u5668\uff0c\u521b\u5efa\u4e00\u4e2a\u57fa\u4e8eKokkos\u751f\u6001\u7cfb\u7edf\u539f\u5219\u7684\u65b9\u8a00\uff0c\u652f\u6301\u81ea\u52a8\u964d\u4f4e\u7a00\u758f\u548c\u7a20\u5bc6\u7ebf\u6027\u4ee3\u6570\u5185\u6838\uff0c\u5e76\u5b9e\u73b0PyTorch\u548cKokkos\u4e4b\u95f4\u7684\u4ee3\u7801\u96c6\u6210\u3002", "result": "LAPIS\u80fd\u591f\u5728\u591a\u79cd\u67b6\u6784\u4e0a\u5b9e\u73b0\u53ef\u79fb\u690d\u6027\uff0c\u5176\u5185\u6838\u6027\u80fd\u4e0e\u9ed8\u8ba4MLIR\u5b9e\u73b0\u76f8\u5f53\uff0c\u540c\u65f6\u652f\u6301\u6846\u67b6\u5411\u65b0\u67b6\u6784\u7684\u6269\u5c55\u3002", "conclusion": "LAPIS\u6210\u529f\u89e3\u51b3\u4e86\u7f16\u7a0b\u6a21\u578b\u5728\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u4e0a\u7684\u6311\u6218\uff0c\u4e3a\u8ba1\u7b97\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7f16\u8bd1\u5668\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.25700", "categories": ["cs.DC", "cs.GT", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.25700", "abs": "https://arxiv.org/abs/2509.25700", "authors": ["Houyi Qi", "Minghui Liwang", "Liqun Fu", "Sai Zou", "Xinlei Yi", "Wei Ni", "Huaiyu Dai"], "title": "PAST: Pilot and Adaptive Orchestration for Timely and Resilient Service Delivery in Edge-Assisted UAV Networks under Spatio-Temporal Dynamics", "comment": null, "summary": "Incentive-driven resource trading is essential for UAV applications with\nintensive, time-sensitive computing demands. Traditional spot trading suffers\nfrom negotiation delays and high energy costs, while conventional futures\ntrading struggles to adapt to the dynamic, uncertain UAV-edge environment. To\naddress these challenges, we propose PAST (pilot-and-adaptive stable trading),\na novel framework for edge-assisted UAV networks with spatio-temporal dynamism.\nPAST integrates two complementary mechanisms: PilotAO (pilot trading agreements\nwith overbooking), a risk-aware, overbooking-enabled early-stage\ndecision-making module that establishes long-term, mutually beneficial\nagreements and boosts resource utilization; and AdaptAO (adaptive trading\nagreements with overbooking rate update), an intelligent adaptation module that\ndynamically updates agreements and overbooking rates based on UAV mobility,\nsupply-demand variations, and agreement performance. Together, these mechanisms\nenable both stability and flexibility, guaranteeing individual rationality,\nstrong stability, competitive equilibrium, and weak Pareto optimality.\nExtensive experiments on real-world datasets show that PAST consistently\noutperforms benchmark methods in decision-making overhead, task completion\nlatency, resource utilization, and social welfare. By combining predictive\nplanning with real-time adjustments, PAST offers a valuable reference on robust\nand adaptive practice for improving low-altitude mission performance.", "AI": {"tldr": "PAST\u6846\u67b6\u7ed3\u5408PilotAO\u548cAdaptAO\u673a\u5236\uff0c\u901a\u8fc7\u98ce\u9669\u611f\u77e5\u7684\u65e9\u671f\u51b3\u7b56\u548c\u667a\u80fd\u81ea\u9002\u5e94\u66f4\u65b0\uff0c\u5728\u52a8\u6001\u65e0\u4eba\u673a-\u8fb9\u7f18\u7f51\u7edc\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u7075\u6d3b\u7684\u8d44\u6e90\u4ea4\u6613\u3002", "motivation": "\u4f20\u7edf\u73b0\u8d27\u4ea4\u6613\u5b58\u5728\u534f\u5546\u5ef6\u8fdf\u548c\u9ad8\u80fd\u8017\u95ee\u9898\uff0c\u4f20\u7edf\u671f\u8d27\u4ea4\u6613\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u4e0d\u786e\u5b9a\u7684\u65e0\u4eba\u673a-\u8fb9\u7f18\u73af\u5883\uff0c\u9700\u8981\u89e3\u51b3\u6fc0\u52b1\u9a71\u52a8\u7684\u8d44\u6e90\u4ea4\u6613\u6311\u6218\u3002", "method": "\u63d0\u51faPAST\u6846\u67b6\uff0c\u5305\u542bPilotAO\uff08\u5e26\u8d85\u989d\u9884\u8ba2\u7684\u98ce\u9669\u611f\u77e5\u65e9\u671f\u51b3\u7b56\u6a21\u5757\uff09\u548cAdaptAO\uff08\u57fa\u4e8e\u65e0\u4eba\u673a\u79fb\u52a8\u6027\u3001\u4f9b\u9700\u53d8\u5316\u548c\u534f\u8bae\u6027\u80fd\u52a8\u6001\u66f4\u65b0\u534f\u8bae\u548c\u8d85\u989d\u9884\u8ba2\u7387\u7684\u667a\u80fd\u9002\u5e94\u6a21\u5757\uff09\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPAST\u5728\u51b3\u7b56\u5f00\u9500\u3001\u4efb\u52a1\u5b8c\u6210\u5ef6\u8fdf\u3001\u8d44\u6e90\u5229\u7528\u7387\u548c\u793e\u4f1a\u798f\u5229\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u9884\u6d4b\u89c4\u5212\u548c\u5b9e\u65f6\u8c03\u6574\uff0cPAST\u4e3a\u63d0\u5347\u4f4e\u7a7a\u4efb\u52a1\u6027\u80fd\u63d0\u4f9b\u4e86\u7a33\u5065\u81ea\u9002\u5e94\u7684\u5b9e\u8df5\u53c2\u8003\u3002"}}
{"id": "2509.25919", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.25919", "abs": "https://arxiv.org/abs/2509.25919", "authors": ["Jay H. Park", "Youngju Cho", "Choungsol Lee", "Moonwook Oh", "Euiseong Seo"], "title": "Accelerating LLM Inference with Precomputed Query Storage", "comment": null, "summary": "Large language model (LLM) inference often suffers from high latency,\nparticularly in resource-constrained environments such as on-device or edge\ndeployments. To address this challenge, we present StorInfer, a novel\nstorage-assisted LLM inference system that accelerates response time by\nprecomputing and storing predictable query-response pairs offline. When a user\nquery semantically matches a precomputed query, StorInfer bypasses expensive\nGPU inference and instantly returns the stored response, significantly reducing\nlatency and compute costs. To maximize coverage and effectiveness, StorInfer\nemploys an LLM-driven generator that adaptively produces diverse and\ndeduplicated queries based on a given knowledge base. This is achieved via two\ntechniques: adaptive query masking, which prevents regeneration of similar\nqueries, and adaptive sampling, which dynamically tunes generation parameters\nto promote semantic diversity. The resulting query-response pairs are embedded\nand indexed using a disk-backed vector database to enable fast,\nsimilarity-based retrieval at runtime. Using this approach, we generated 150K\nunique precomputed pairs (taking up to 830 MB of storage space), achieving up\nto 17.3% latency reduction with no loss in response quality. Our evaluation\nacross multiple QA datasets demonstrates the practicality and scalability of\nstorage-assisted inference, especially in scenarios with predictable query\ndistributions. StorInfer highlights a promising direction in leveraging storage\nas a primary enabler for efficient, low-latency LLM deployment.", "AI": {"tldr": "StorInfer\u662f\u4e00\u4e2a\u5b58\u50a8\u8f85\u52a9\u7684LLM\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u548c\u5b58\u50a8\u53ef\u9884\u6d4b\u7684\u67e5\u8be2-\u54cd\u5e94\u5bf9\u6765\u52a0\u901f\u63a8\u7406\uff0c\u5728\u67e5\u8be2\u8bed\u4e49\u5339\u914d\u65f6\u7ed5\u8fc7GPU\u63a8\u7406\u76f4\u63a5\u8fd4\u56de\u5b58\u50a8\u7684\u54cd\u5e94\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff08\u5982\u8bbe\u5907\u7aef\u6216\u8fb9\u7f18\u90e8\u7f72\uff09\u4e2d\u63a8\u7406\u5ef6\u8fdf\u9ad8\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5b58\u50a8\u9884\u8ba1\u7b97\u7ed3\u679c\u6765\u4f18\u5316\u63a8\u7406\u6548\u7387\u3002", "method": "\u4f7f\u7528LLM\u9a71\u52a8\u7684\u751f\u6210\u5668\u81ea\u9002\u5e94\u751f\u6210\u591a\u6837\u5316\u548c\u53bb\u91cd\u7684\u67e5\u8be2\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u67e5\u8be2\u63a9\u7801\u9632\u6b62\u76f8\u4f3c\u67e5\u8be2\u518d\u751f\uff0c\u4ee5\u53ca\u81ea\u9002\u5e94\u91c7\u6837\u52a8\u6001\u8c03\u6574\u751f\u6210\u53c2\u6570\u4ee5\u4fc3\u8fdb\u8bed\u4e49\u591a\u6837\u6027\u3002\u751f\u6210\u7684\u67e5\u8be2-\u54cd\u5e94\u5bf9\u901a\u8fc7\u78c1\u76d8\u652f\u6301\u7684\u5411\u91cf\u6570\u636e\u5e93\u8fdb\u884c\u5d4c\u5165\u548c\u7d22\u5f15\uff0c\u5b9e\u73b0\u5feb\u901f\u7684\u76f8\u4f3c\u6027\u68c0\u7d22\u3002", "result": "\u751f\u6210\u4e8615\u4e07\u4e2a\u72ec\u7279\u7684\u9884\u8ba1\u7b97\u5bf9\uff08\u5360\u7528830MB\u5b58\u50a8\u7a7a\u95f4\uff09\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe17.3%\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u4e14\u54cd\u5e94\u8d28\u91cf\u65e0\u635f\u5931\u3002\u5728\u591a\u4e2aQA\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "StorInfer\u5c55\u793a\u4e86\u5229\u7528\u5b58\u50a8\u4f5c\u4e3a\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdfLLM\u90e8\u7f72\u4e3b\u8981\u63a8\u52a8\u56e0\u7d20\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5177\u6709\u53ef\u9884\u6d4b\u67e5\u8be2\u5206\u5e03\u7684\u573a\u666f\u3002"}}
{"id": "2509.26092", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.26092", "abs": "https://arxiv.org/abs/2509.26092", "authors": ["Kuan-Wei Lu", "Ding-Yong Hong", "Pangfeng Liu", "Jan-Jan Wu"], "title": "Efficient Distributed Training via Dual Batch Sizes and Cyclic Progressive Learning", "comment": null, "summary": "Distributed machine learning is critical for training deep learning models on\nlarge datasets and with numerous parameters. Current research primarily focuses\non leveraging additional hardware resources and powerful computing units to\naccelerate the training process. As a result, larger batch sizes are often\nemployed to speed up training. However, training with large batch sizes can\nlead to lower accuracy due to poor generalization. To address this issue, we\npropose the dual batch size learning scheme, a distributed training method\nbuilt on the parameter server framework. This approach maximizes training\nefficiency by utilizing the largest batch size that the hardware can support\nwhile incorporating a smaller batch size to enhance model generalization. By\nusing two different batch sizes simultaneously, this method reduces testing\nloss and enhances generalization, with minimal extra training time.\nAdditionally, to mitigate the time overhead caused by dual batch size learning,\nwe propose the cyclic progressive learning scheme. This technique gradually\nadjusts image resolution from low to high during training, significantly\nboosting training speed. By combining cyclic progressive learning with dual\nbatch size learning, our hybrid approach improves both model generalization and\ntraining efficiency. Experimental results using ResNet-18 show that, compared\nto conventional training methods, our method can improve accuracy by 3.3% while\nreducing training time by 10.6% on CIFAR-100, and improve accuracy by 0.1%\nwhile reducing training time by 35.7% on ImageNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53cc\u6279\u6b21\u5927\u5c0f\u5b66\u4e60\u548c\u5faa\u73af\u6e10\u8fdb\u5b66\u4e60\u7684\u6df7\u5408\u5206\u5e03\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u4f7f\u7528\u5927\u3001\u5c0f\u6279\u6b21\u5927\u5c0f\u6765\u5e73\u8861\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4f7f\u7528\u6e10\u8fdb\u5206\u8fa8\u7387\u8c03\u6574\u6765\u51cf\u5c11\u65f6\u95f4\u5f00\u9500\u3002", "motivation": "\u5f53\u524d\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u4e3b\u8981\u4f9d\u8d56\u589e\u52a0\u786c\u4ef6\u8d44\u6e90\u548c\u5927\u6279\u6b21\u8bad\u7ec3\u6765\u52a0\u901f\uff0c\u4f46\u5927\u6279\u6b21\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u548c\u51c6\u786e\u7387\u964d\u4f4e\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u65e2\u80fd\u4fdd\u6301\u8bad\u7ec3\u6548\u7387\u53c8\u80fd\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u53c2\u6570\u670d\u52a1\u5668\u6846\u67b6\u7684\u53cc\u6279\u6b21\u5927\u5c0f\u5b66\u4e60\u65b9\u6848\uff1a\u540c\u65f6\u4f7f\u7528\u786c\u4ef6\u652f\u6301\u7684\u6700\u5927\u6279\u6b21\u5927\u5c0f\u548c\u8f83\u5c0f\u6279\u6b21\u5927\u5c0f\uff1b\u7ed3\u5408\u5faa\u73af\u6e10\u8fdb\u5b66\u4e60\u65b9\u6848\uff1a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ece\u4f4e\u5230\u9ad8\u6e10\u8fdb\u8c03\u6574\u56fe\u50cf\u5206\u8fa8\u7387\u3002", "result": "\u5728ResNet-18\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u5728CIFAR-100\u4e0a\u51c6\u786e\u7387\u63d0\u53473.3%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1110.6%\uff1b\u5728ImageNet\u4e0a\u51c6\u786e\u7387\u63d0\u53470.1%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1135.7%\u3002", "conclusion": "\u8be5\u6df7\u5408\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u6279\u6b21\u8bad\u7ec3\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u51c6\u786e\u7387\uff0c\u4e3a\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26120", "categories": ["cs.DC", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.26120", "abs": "https://arxiv.org/abs/2509.26120", "authors": ["Leszek Sliwko", "Vladimir Getov"], "title": "AGOCS -- Accurate Google Cloud Simulator Framework", "comment": "This is the accepted author's version of the paper. The final\n  published version is available in the Proceedings of the 2016 IEEE\n  International Conferences on Ubiquitous Intelligence and Computing (UIC),\n  Advanced and Trusted Computing (ATC), Scalable Computing and Communications\n  (ScalCom), Cloud and Big Data Computing (CBDCom), Internet of People (IoP),\n  and Smart World Congress (SmartWorld)", "summary": "This paper presents the Accurate Google Cloud Simulator (AGOCS) - a novel\nhigh-fidelity Cloud workload simulator based on parsing real workload traces,\nwhich can be conveniently used on a desktop machine for day-to-day research.\nOur simulation is based on real-world workload traces from a Google Cluster\nwith 12.5K nodes, over a period of a calendar month. The framework is able to\nreveal very precise and detailed parameters of the executed jobs, tasks and\nnodes as well as to provide actual resource usage statistics. The system has\nbeen implemented in Scala language with focus on parallel execution and an\neasy-to-extend design concept. The paper presents the detailed structural\nframework for AGOCS and discusses our main design decisions, whilst also\nsuggesting alternative and possibly performance enhancing future approaches.\nThe framework is available via the Open Source GitHub repository.", "AI": {"tldr": "AGOCS\u662f\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9eGoogle\u96c6\u7fa4\u5de5\u4f5c\u8d1f\u8f7d\u8f68\u8ff9\u7684\u9ad8\u4fdd\u771f\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u62df\u5668\uff0c\u53ef\u5728\u684c\u9762\u673a\u5668\u4e0a\u7528\u4e8e\u65e5\u5e38\u7814\u7a76\uff0c\u63d0\u4f9b\u7cbe\u786e\u7684\u4f5c\u4e1a\u3001\u4efb\u52a1\u548c\u8282\u70b9\u53c2\u6570\u5206\u6790\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u57fa\u4e8e\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u8f68\u8ff9\u8fdb\u884c\u9ad8\u4fdd\u771f\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u62df\u7684\u5de5\u5177\uff0c\u65b9\u4fbf\u7814\u7a76\u4eba\u5458\u5728\u684c\u9762\u73af\u5883\u4e2d\u8fdb\u884c\u65e5\u5e38\u7814\u7a76\u3002", "method": "\u57fa\u4e8eGoogle\u96c6\u7fa412.5K\u8282\u70b9\u4e00\u4e2a\u6708\u7684\u5de5\u4f5c\u8d1f\u8f7d\u8f68\u8ff9\uff0c\u4f7f\u7528Scala\u8bed\u8a00\u5b9e\u73b0\uff0c\u6ce8\u91cd\u5e76\u884c\u6267\u884c\u548c\u6613\u4e8e\u6269\u5c55\u7684\u8bbe\u8ba1\u6982\u5ff5\u3002", "result": "\u5b9e\u73b0\u4e86\u80fd\u591f\u63ed\u793a\u4f5c\u4e1a\u3001\u4efb\u52a1\u548c\u8282\u70b9\u7cbe\u786e\u53c2\u6570\u7684\u6846\u67b6\uff0c\u63d0\u4f9b\u5b9e\u9645\u8d44\u6e90\u4f7f\u7528\u7edf\u8ba1\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90GitHub\u4ed3\u5e93\u63d0\u4f9b\u3002", "conclusion": "AGOCS\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u9ad8\u4fdd\u771f\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u62df\u5668\uff0c\u4e3a\u4e91\u7814\u7a76\u63d0\u4f9b\u4e86\u4fbf\u5229\u5de5\u5177\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6027\u80fd\u589e\u5f3a\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002"}}
{"id": "2509.26182", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.26182", "abs": "https://arxiv.org/abs/2509.26182", "authors": ["Chris Tong", "Youhe Jiang", "Gufeng Chen", "Tianyi Zhao", "Sibian Lu", "Wenjie Qu", "Eric Yang", "Lynn Ai", "Binhang Yuan"], "title": "Parallax: Efficient LLM Inference Service over Decentralized Environment", "comment": null, "summary": "Deploying a large language model (LLM) inference service remains costly\nbecause centralized serving depends on specialized GPU clusters and\nhigh-bandwidth interconnects in datacenters. An appealing alternative is to\nleverage collaborative decentralized GPU pools. However, heterogeneity in GPU\nand limited interconnected network bandwidth, along with potentially dynamic\navailability, make efficient scheduling the central challenge in this scenario.\nIn this paper, we present Parallax, a decentralized LLM serving system that\nturns a pool of heterogeneous GPUs into an efficient inference platform via a\ntwo-phase scheduler. Parallax decomposes planning into (i) model allocation,\nwhich places layers of each replica across diverse GPUs to jointly optimize\nlatency and throughput under memory and link-bandwidth constraints, and (ii)\nrequest-time GPU pipeline selection, which stitches layers from different\nreplicas into end-to-end execution chains that balance load and adapt to\ncurrent conditions. We implement Parallax and evaluate it on open-source LLMs\ndeployed over real volunteer nodes. Parallax consistently reduces latency and\nincreases throughput relative to decentralized baselines, demonstrating that\nprincipled scheduling can make volunteer compute a practical, affordable\nsubstrate for LLM inference.\n  Github Repo at: https://github.com/GradientHQ/parallax.", "AI": {"tldr": "Parallax\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684LLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8c03\u5ea6\u5668\u5c06\u5f02\u6784GPU\u6c60\u8f6c\u5316\u4e3a\u9ad8\u6548\u63a8\u7406\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86GPU\u5f02\u6784\u6027\u548c\u7f51\u7edc\u5e26\u5bbd\u9650\u5236\u7684\u6311\u6218\u3002", "motivation": "\u96c6\u4e2d\u5f0fLLM\u63a8\u7406\u670d\u52a1\u6210\u672c\u9ad8\u6602\uff0c\u4f9d\u8d56\u4e13\u7528GPU\u96c6\u7fa4\u548c\u9ad8\u5e26\u5bbd\u4e92\u8054\u3002\u5229\u7528\u53bb\u4e2d\u5fc3\u5316GPU\u6c60\u662f\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46GPU\u5f02\u6784\u6027\u3001\u6709\u9650\u7f51\u7edc\u5e26\u5bbd\u548c\u52a8\u6001\u53ef\u7528\u6027\u4f7f\u9ad8\u6548\u8c03\u5ea6\u6210\u4e3a\u6838\u5fc3\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8c03\u5ea6\u5668\uff1a(1)\u6a21\u578b\u5206\u914d\u9636\u6bb5\uff0c\u5c06\u6bcf\u4e2a\u526f\u672c\u7684\u5c42\u5206\u914d\u5230\u4e0d\u540cGPU\u4e0a\uff0c\u5728\u5185\u5b58\u548c\u94fe\u8def\u5e26\u5bbd\u7ea6\u675f\u4e0b\u8054\u5408\u4f18\u5316\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\uff1b(2)\u8bf7\u6c42\u65f6GPU\u6d41\u6c34\u7ebf\u9009\u62e9\u9636\u6bb5\uff0c\u5c06\u4e0d\u540c\u526f\u672c\u7684\u5c42\u62fc\u63a5\u6210\u7aef\u5230\u7aef\u6267\u884c\u94fe\uff0c\u5e73\u8861\u8d1f\u8f7d\u5e76\u9002\u5e94\u5f53\u524d\u6761\u4ef6\u3002", "result": "\u5728\u771f\u5b9e\u5fd7\u613f\u8005\u8282\u70b9\u4e0a\u90e8\u7f72\u5f00\u6e90LLM\u8fdb\u884c\u8bc4\u4f30\uff0cParallax\u76f8\u6bd4\u53bb\u4e2d\u5fc3\u5316\u57fa\u7ebf\u6301\u7eed\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "conclusion": "\u539f\u5219\u6027\u8c03\u5ea6\u53ef\u4ee5\u4f7f\u5fd7\u613f\u8005\u8ba1\u7b97\u6210\u4e3aLLM\u63a8\u7406\u7684\u5b9e\u7528\u4e14\u7ecf\u6d4e\u53ef\u884c\u7684\u57fa\u7840\u3002"}}
{"id": "2509.26193", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.26193", "abs": "https://arxiv.org/abs/2509.26193", "authors": ["Fabian Czappa", "Marvin Kaster", "Felix Wolf"], "title": "I Like To Move It -- Computation Instead of Data in the Brain", "comment": null, "summary": "The detailed functioning of the human brain is still poorly understood. Brain\nsimulations are a well-established way to complement experimental research, but\nmust contend with the computational demands of the approximately $10^{11}$\nneurons and the $10^{14}$ synapses connecting them, the network of the latter\nreferred to as the connectome. Studies suggest that changes in the connectome\n(i.e., the formation and deletion of synapses, also known as structural\nplasticity) are essential for critical tasks such as memory formation and\nlearning. The connectivity update can be efficiently computed using a\nBarnes-Hut-inspired approximation that lowers the computational complexity from\n$O(n^2)$ to $O(n log n)$, where n is the number of neurons. However, updating\nsynapses, which relies heavily on RMA, and the spike exchange between neurons,\nwhich requires all-to-all communication at every time step, still hinder\nscalability. We present a new algorithm that significantly reduces the\ncommunication overhead by moving computation instead of data. This shrinks the\ntime it takes to update connectivity by a factor of six and the time it takes\nto exchange spikes by more than two orders of magnitude.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u79fb\u52a8\u8ba1\u7b97\u800c\u975e\u6570\u636e\u6765\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u5c06\u8fde\u63a5\u6027\u66f4\u65b0\u65f6\u95f4\u7f29\u77ed6\u500d\uff0c\u5c16\u5cf0\u4ea4\u6362\u65f6\u95f4\u7f29\u77ed\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u5927\u8111\u6a21\u62df\u9762\u4e34\u8ba1\u7b97\u9700\u6c42\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u7ea610^11\u4e2a\u795e\u7ecf\u5143\u548c10^14\u4e2a\u7a81\u89e6\u7684\u8fde\u63a5\u7ec4\u65f6\u3002\u7ed3\u6784\u53ef\u5851\u6027\uff08\u7a81\u89e6\u5f62\u6210\u548c\u5220\u9664\uff09\u5bf9\u8bb0\u5fc6\u5f62\u6210\u548c\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u8fde\u63a5\u6027\u66f4\u65b0\u548c\u5c16\u5cf0\u4ea4\u6362\u65b9\u6cd5\u5b58\u5728\u901a\u4fe1\u74f6\u9888\u3002", "method": "\u4f7f\u7528Barnes-Hut\u8fd1\u4f3c\u7b97\u6cd5\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(n^2)\u964d\u4f4e\u5230O(n log n)\uff0c\u5e76\u63d0\u51fa\u65b0\u7b97\u6cd5\u901a\u8fc7\u79fb\u52a8\u8ba1\u7b97\u800c\u975e\u6570\u636e\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "result": "\u65b0\u7b97\u6cd5\u4f7f\u8fde\u63a5\u6027\u66f4\u65b0\u65f6\u95f4\u7f29\u77ed6\u500d\uff0c\u5c16\u5cf0\u4ea4\u6362\u65f6\u95f4\u7f29\u77ed\u8d85\u8fc7\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u8ba1\u7b97\u548c\u901a\u4fe1\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u8111\u6a21\u62df\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.26253", "categories": ["cs.DC", "cs.PF", "C.4; I.m; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.26253", "abs": "https://arxiv.org/abs/2509.26253", "authors": ["Floris-Jan Willemsen", "Rob V. van Nieuwpoort", "Ben van Werkhoven"], "title": "Efficient Construction of Large Search Spaces for Auto-Tuning", "comment": null, "summary": "Automatic performance tuning, or auto-tuning, accelerates high-performance\ncodes by exploring vast spaces of code variants. However, due to the large\nnumber of possible combinations and complex constraints, constructing these\nsearch spaces can be a major bottleneck. Real-world applications have been\nencountered where the search space construction takes minutes to hours or even\ndays. Current state-of-the-art techniques for search space construction, such\nas chain-of-trees, lack a formal foundation and only perform adequately on a\nspecific subset of search spaces.\n  We show that search space construction for constraint-based auto-tuning can\nbe reformulated as a Constraint Satisfaction Problem (CSP). Building on this\ninsight with a CSP solver, we develop a runtime parser that translates\nuser-defined constraint functions into solver-optimal expressions, optimize the\nsolver to exploit common structures in auto-tuning constraints, and integrate\nthese and other advances in open-source tools. These contributions\nsubstantially improve performance and accessibility while preserving\nflexibility.\n  We evaluate our approach using a diverse set of benchmarks, demonstrating\nthat our optimized solver reduces construction time by four orders of magnitude\nversus brute-force enumeration, three orders of magnitude versus an unoptimized\nCSP solver, and one to two orders of magnitude versus leading auto-tuning\nframeworks built on chain-of-trees. We thus eliminate a critical scalability\nbarrier for auto-tuning and provide a drop-in solution that enables the\nexploration of previously unattainable problem scales in auto-tuning and\nrelated domains.", "AI": {"tldr": "\u5c06\u7ea6\u675f\u5f0f\u81ea\u52a8\u8c03\u4f18\u7684\u641c\u7d22\u7a7a\u95f4\u6784\u5efa\u91cd\u65b0\u8868\u8ff0\u4e3a\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316CSP\u6c42\u89e3\u5668\u663e\u8457\u63d0\u5347\u6784\u5efa\u6548\u7387\u3002", "motivation": "\u81ea\u52a8\u8c03\u4f18\u4e2d\u7684\u641c\u7d22\u7a7a\u95f4\u6784\u5efa\u7531\u4e8e\u7ec4\u5408\u6570\u91cf\u5e9e\u5927\u548c\u7ea6\u675f\u590d\u6742\uff0c\u53ef\u80fd\u8017\u65f6\u6570\u5206\u949f\u5230\u6570\u5929\uff0c\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u4e14\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u5b50\u96c6\u3002", "method": "\u5c06\u641c\u7d22\u7a7a\u95f4\u6784\u5efa\u91cd\u65b0\u8868\u8ff0\u4e3aCSP\u95ee\u9898\uff0c\u5f00\u53d1\u8fd0\u884c\u65f6\u89e3\u6790\u5668\u5c06\u7528\u6237\u7ea6\u675f\u8f6c\u6362\u4e3a\u6c42\u89e3\u5668\u4f18\u5316\u8868\u8fbe\u5f0f\uff0c\u4f18\u5316\u6c42\u89e3\u5668\u4ee5\u5229\u7528\u81ea\u52a8\u8c03\u4f18\u7ea6\u675f\u7684\u5e38\u89c1\u7ed3\u6784\u3002", "result": "\u4f18\u5316\u6c42\u89e3\u5668\u76f8\u6bd4\u66b4\u529b\u679a\u4e3e\u51cf\u5c114\u4e2a\u6570\u91cf\u7ea7\u6784\u5efa\u65f6\u95f4\uff0c\u76f8\u6bd4\u672a\u4f18\u5316CSP\u6c42\u89e3\u5668\u51cf\u5c113\u4e2a\u6570\u91cf\u7ea7\uff0c\u76f8\u6bd4\u57fa\u4e8e\u94fe\u5f0f\u6811\u7684\u9886\u5148\u6846\u67b6\u51cf\u5c111-2\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u6d88\u9664\u4e86\u81ea\u52a8\u8c03\u4f18\u7684\u5173\u952e\u53ef\u6269\u5c55\u6027\u969c\u788d\uff0c\u4e3a\u63a2\u7d22\u5148\u524d\u65e0\u6cd5\u8fbe\u5230\u7684\u95ee\u9898\u89c4\u6a21\u63d0\u4f9b\u4e86\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.26529", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.26529", "abs": "https://arxiv.org/abs/2509.26529", "authors": ["Shangshu Qian", "Lin Tan", "Yongle Zhang"], "title": "CSnake: Detecting Self-Sustaining Cascading Failure via Causal Stitching of Fault Propagations", "comment": "Accepted by EuroSys 2026", "summary": "Recent studies have revealed that self-sustaining cascading failures in\ndistributed systems frequently lead to widespread outages, which are\nchallenging to contain and recover from. Existing failure detection techniques\nstruggle to expose such failures prior to deployment, as they typically require\na complex combination of specific conditions to be triggered. This challenge\nstems from the inherent nature of cascading failures, as they typically involve\na sequence of fault propagations, each activated by distinct conditions.\n  This paper presents CSnake, a fault injection framework to expose\nself-sustaining cascading failures in distributed systems. CSnake uses the\nnovel idea of causal stitching, which causally links multiple single-fault\ninjections in different tests to simulate complex fault propagation chains. To\nidentify these chains, CSnake designs a counterfactual causality analysis of\nfault propagations - fault causality analysis (FCA): FCA compares the execution\ntrace of a fault injection run with its corresponding profile run (i.e., same\ntest w/o the injection) and identifies any additional faults triggered, which\nare considered to have a causal relationship with the injected fault.\n  To address the large search space of fault and workload combinations, CSnake\nemploys a three-phase allocation protocol of test budget that prioritizes\nfaults with unique and diverse causal consequences, increasing the likelihood\nof uncovering conditional fault propagations. Furthermore, to avoid incorrectly\nconnecting fault propagations from workloads with incompatible conditions,\nCSnake performs a local compatibility check that approximately checks the\ncompatibility of the path constraints associated with connected fault\npropagations with low overhead.\n  CSnake detected 15 bugs that cause self-sustaining cascading failures in five\nsystems, five of which have been confirmed with two fixed.", "AI": {"tldr": "CSnake\u662f\u4e00\u4e2a\u6545\u969c\u6ce8\u5165\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u62fc\u63a5\u6280\u672f\u6a21\u62df\u590d\u6742\u7684\u6545\u969c\u4f20\u64ad\u94fe\uff0c\u6765\u66b4\u9732\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u81ea\u7ef4\u6301\u7ea7\u8054\u6545\u969c\u3002", "motivation": "\u73b0\u6709\u6545\u969c\u68c0\u6d4b\u6280\u672f\u96be\u4ee5\u5728\u90e8\u7f72\u524d\u66b4\u9732\u81ea\u7ef4\u6301\u7ea7\u8054\u6545\u969c\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6545\u969c\u9700\u8981\u7279\u5b9a\u6761\u4ef6\u7684\u590d\u6742\u7ec4\u5408\u624d\u80fd\u89e6\u53d1\uff0c\u6d89\u53ca\u591a\u4e2a\u6545\u969c\u4f20\u64ad\u5e8f\u5217\u3002", "method": "\u91c7\u7528\u56e0\u679c\u62fc\u63a5\u6280\u672f\u5c06\u4e0d\u540c\u6d4b\u8bd5\u4e2d\u7684\u591a\u4e2a\u5355\u6545\u969c\u6ce8\u5165\u56e0\u679c\u8fde\u63a5\uff0c\u8bbe\u8ba1\u6545\u969c\u56e0\u679c\u5206\u6790(FCA)\u6765\u8bc6\u522b\u6545\u969c\u4f20\u64ad\u94fe\uff0c\u4f7f\u7528\u4e09\u9636\u6bb5\u6d4b\u8bd5\u9884\u7b97\u5206\u914d\u534f\u8bae\u548c\u5c40\u90e8\u517c\u5bb9\u6027\u68c0\u67e5\u6765\u4f18\u5316\u641c\u7d22\u3002", "result": "\u5728\u4e94\u4e2a\u7cfb\u7edf\u4e2d\u68c0\u6d4b\u523015\u4e2a\u5bfc\u81f4\u81ea\u7ef4\u6301\u7ea7\u8054\u6545\u969c\u7684bug\uff0c\u5176\u4e2d5\u4e2a\u5df2\u786e\u8ba4\uff0c2\u4e2a\u5df2\u4fee\u590d\u3002", "conclusion": "CSnake\u6846\u67b6\u80fd\u6709\u6548\u66b4\u9732\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u81ea\u7ef4\u6301\u7ea7\u8054\u6545\u969c\uff0c\u901a\u8fc7\u56e0\u679c\u62fc\u63a5\u548c\u4f18\u5316\u641c\u7d22\u7b56\u7565\u63d0\u9ad8\u4e86\u6545\u969c\u68c0\u6d4b\u6548\u7387\u3002"}}
