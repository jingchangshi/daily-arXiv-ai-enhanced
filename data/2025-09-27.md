<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications](https://arxiv.org/abs/2509.20426)
*Mahmoud Samir Fayed*

Main category: cs.PL

TL;DR: 本文开发了PWCT2，这是一个双语言（阿拉伯语/英语）、通用、自托管的可视化编程语言，通过Ring文本编程语言实现，显著提升了代码生成速度和存储效率。


<details>
  <summary>Details</summary>
Motivation: 大多数可视化编程语言是领域特定的，通用VPL如PWCT需要文本编程来改进。本文旨在开发一个自托管的通用VPL，能够自我改进。

Method: 首先设计Ring文本编程语言，然后用PWCT开发Ring编译器，最后用Ring开发PWCT2。PWCT2包含92,000行Ring代码和394个可视化组件。

Result: PWCT2代码生成速度提升36倍，存储需求减少20倍。在Steam平台有1772用户使用，总使用时间超过17,000小时。

Conclusion: PWCT2成功实现了自托管VPL，证明了可视化编程语言可以自我改进，为未来研究提供了基础。

Abstract: Most visual programming languages (VPLs) are domain-specific, with few
general-purpose VPLs like Programming Without Coding Technology (PWCT). These
general-purpose VPLs are developed using textual programming languages and
improving them requires textual programming. In this thesis, we designed and
developed PWCT2, a dual-language (Arabic/English), general-purpose,
self-hosting visual programming language. Before doing so, we specifically
designed a textual programming language called Ring for its development. Ring
is a dynamically typed language with a lightweight implementation, offering
syntax customization features. It permits the creation of domain-specific
languages through new features that extend object-oriented programming,
allowing for specialized languages resembling Cascading Style Sheets (CSS) or
Supernova language. The Ring Compiler and Virtual Machine are designed using
the PWCT visual programming language where the visual implementation is
composed of 18,945 components that generate 24,743 lines of C code, which
increases the abstraction level and hides unnecessary details. Using PWCT to
develop Ring allowed us to realize several issues in PWCT, which led to the
development of the PWCT2 visual programming language using the Ring textual
programming language. PWCT2 provides approximately 36 times faster code
generation and requires 20 times less storage for visual source files. It also
allows for the conversion of Ring code into visual code, enabling the creation
of a self-hosting VPL that can be developed using itself. PWCT2 consists of
approximately 92,000 lines of Ring code and comes with 394 visual components.
PWCT2 is distributed to many users through the Steam platform and has received
positive feedback, On Steam, 1772 users have launched the software, and the
total recorded usage time exceeds 17,000 hours, encouraging further research
and development.

</details>


### [2] [Efficient Symbolic Computation vis Hash Consing](https://arxiv.org/abs/2509.20534)
*Bowen Zhu,Aayush Sabharwal,Songchen Tan,Yingbo Ma,Alan Edelman,Christopher Rackauckas*

Main category: cs.PL

TL;DR: 本文首次将哈希一致性技术集成到JuliaSymbolics中，通过全局弱引用哈希表规范化表达式并消除重复存储，显著提升了符号计算的性能和内存效率。


<details>
  <summary>Details</summary>
Motivation: 符号计算系统存在内存效率低下的问题，由于结构相同子表达式的冗余存储（表达式膨胀），这影响了经典计算机代数和新兴AI驱动数学推理工具的性能。

Method: 在JuliaSymbolics中集成哈希一致性技术，使用全局弱引用哈希表来规范化表达式并消除重复存储，同时与Julia的元编程和即时编译基础设施无缝集成。

Result: 基准测试显示显著改进：符号计算加速达3.2倍，内存使用减少达2倍，代码生成快达5倍，函数编译快达10倍，大型模型的数值评估快达100倍。

Conclusion: 哈希一致性对于扩展符号计算至关重要，为未来将哈希一致性与e-graphs集成以增强AI驱动管道中的等价感知表达式共享铺平了道路。

Abstract: Symbolic computation systems suffer from memory inefficiencies due to
redundant storage of structurally identical subexpressions, commonly known as
expression swell, which degrades performance in both classical computer algebra
and emerging AI-driven mathematical reasoning tools. In this paper, we present
the first integration of hash consing into JuliaSymbolics, a high-performance
symbolic toolkit in Julia, by employing a global weak-reference hash table that
canonicalizes expressions and eliminates duplication. This approach reduces
memory consumption and accelerates key operations such as differentiation,
simplification, and code generation, while seamlessly integrating with Julia's
metaprogramming and just-in-time compilation infrastructure. Benchmark
evaluations across different computational domains reveal substantial
improvements: symbolic computations are accelerated by up to 3.2 times, memory
usage is reduced by up to 2 times, code generation is up to 5 times faster,
function compilation up to 10 times faster, and numerical evaluation up to 100
times faster for larger models. While certain workloads with fewer duplicate
unknown-variable expressions show more modest gains or even slight overhead in
initial computation stages, downstream processing consistently benefits
significantly. These findings underscore the importance of hash consing in
scaling symbolic computation and pave the way for future work integrating hash
consing with e-graphs for enhanced equivalence-aware expression sharing in
AI-driven pipelines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Experience Deploying Containerized GenAI Services at an HPC Center](https://arxiv.org/abs/2509.20603)
*Angel M. Beltre,Jeff Ogden,Kevin Pedretti*

Main category: cs.DC

TL;DR: 本文分享了在高性能计算中心部署生成式AI工作负载的经验，讨论了HPC与云计算环境的集成，提出了融合计算架构，并通过Llama大语言模型的案例研究展示了跨Kubernetes和HPC平台的容器化部署。


<details>
  <summary>Details</summary>
Motivation: 生成式AI应用通常基于容器化组件构建，但在HPC中心的部署能力仍在发展中。本文旨在探索如何在传统HPC环境中有效部署和管理容器化的GenAI工作负载。

Method: 提出融合计算架构，集成HPC和Kubernetes平台来运行容器化的GenAI工作负载。通过案例研究，使用vLLM容器化推理服务器在Kubernetes和HPC平台上部署Llama大语言模型，并测试多种容器运行时。

Result: 成功实现了跨平台的GenAI工作负载部署，验证了融合架构的可行性。经验表明容器化技术有助于提高工作负载的可重复性。

Conclusion: 本文为HPC容器社区提供了实用的部署考虑因素和发展机会，指导未来的研究和工具开发，证明了在HPC环境中有效部署容器化GenAI应用的可行性。

Abstract: Generative Artificial Intelligence (GenAI) applications are built from
specialized components -- inference servers, object storage, vector and graph
databases, and user interfaces -- interconnected via web-based APIs. While
these components are often containerized and deployed in cloud environments,
such capabilities are still emerging at High-Performance Computing (HPC)
centers. In this paper, we share our experience deploying GenAI workloads
within an established HPC center, discussing the integration of HPC and cloud
computing environments. We describe our converged computing architecture that
integrates HPC and Kubernetes platforms running containerized GenAI workloads,
helping with reproducibility. A case study illustrates the deployment of the
Llama Large Language Model (LLM) using a containerized inference server (vLLM)
across both Kubernetes and HPC platforms using multiple container runtimes. Our
experience highlights practical considerations and opportunities for the HPC
container community, guiding future research and tool development.

</details>


### [4] [From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient Method for Solving Large-Scale Linear Optimization Problem](https://arxiv.org/abs/2509.21137)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Gozde Tutuncuoglu,Junchi Yang,Feng Qiu,Murat Yildirim*

Main category: cs.DC

TL;DR: 本文提出了一种专为RRAM阵列设计的分布式内存原始-对偶混合梯度（PDHG）方法，用于解决大规模线性规划问题，相比GPU加速求解器实现了三个数量级的能耗和延迟降低。


<details>
  <summary>Details</summary>
Motivation: 传统架构受限于基本物理限制，无法满足计算工作量的指数级增长需求。内存计算（IMC）与RRAM提供了有前景的替代方案，但现有算法不适用于IMC，特别是在需要频繁矩阵重编程的约束优化问题中。

Method: 开发了分布式内存PDHG方法，最小化昂贵的写入周期，包含对设备非理想性的鲁棒性，并利用对称块矩阵公式统一分布式交叉开关上的操作。使用MELISO+物理模拟框架评估实际设备条件下的性能。

Result: 与GPU加速求解器在大规模线性程序上的基准测试表明，基于RRAM的求解器实现了相当的精度，同时能耗和延迟降低了三个数量级。

Conclusion: 这是首个在RRAM上实现的PDHG基LP求解器，展示了算法-硬件协同设计通过分布式内存计算解决大规模优化问题的变革潜力。

Abstract: The exponential growth of computational workloads is surpassing the
capabilities of conventional architectures, which are constrained by
fundamental limits. In-memory computing (IMC) with RRAM provides a promising
alternative by providing analog computations with significant gains in latency
and energy use. However, existing algorithms developed for conventional
architectures do not translate to IMC, particularly for constrained
optimization problems where frequent matrix reprogramming remains
cost-prohibitive for IMC applications. Here we present a distributed in-memory
primal-dual hybrid gradient (PDHG) method, specifically co-designed for arrays
of RRAM devices. Our approach minimizes costly write cycles, incorporates
robustness against device non-idealities, and leverages a symmetric
block-matrix formulation to unify operations across distributed crossbars. We
integrate a physics-based simulation framework called MELISO+ to evaluate
performance under realistic device conditions. Benchmarking against
GPU-accelerated solvers on large-scale linear programs demonstrates that our
RRAM-based solver achieves comparable accuracy with up to three orders of
magnitude reductions in energy consumption and latency. These results
demonstrate the first PDHG-based LP solver implemented on RRAMs, showcasing the
transformative potential of algorithm-hardware co-design for solving
large-scale optimization through distributed in-memory computing.

</details>


### [5] [FZModules: A Heterogeneous Computing Framework for Customizable Scientific Data Compression Pipelines](https://arxiv.org/abs/2509.20563)
*Skyler Ruiter,Jiannan Tian,Fengguang Song*

Main category: cs.DC

TL;DR: FZModules是一个异构框架，用于通过高性能模块组装误差有界的自定义压缩流水线，支持快速实验和异步执行，在保持高压缩率的同时实现与GPU压缩器相当的端到端加速。


<details>
  <summary>Details</summary>
Motivation: 现代科学模拟和仪器生成的数据量过大，超过内存和存储容量，限制了可扩展性。虽然有损压缩通过控制误差来减少存储占用和提高吞吐量，但最优流水线高度依赖数据和目标，需要压缩专业知识。现有的GPU压缩器虽然提供高吞吐量，但通常采用固定内核，阻碍快速实验，且在率失真性能上表现不佳。

Method: 提出FZModules异构框架，通过简洁可扩展的接口从高性能模块组装误差有界的自定义压缩流水线。利用异步任务支持的执行库，推断数据依赖关系，管理内存移动，并暴露分支和阶段级并发，实现强大的异步压缩流水线。

Result: 在四个代表性科学数据集上评估三个使用FZModules构建的流水线，结果显示它们可以实现与融合内核GPU压缩器相当的端到端加速，同时达到与更高保真度的CPU或混合压缩器相似的率失真性能。

Conclusion: FZModules框架能够实现快速、针对特定领域定制的压缩流水线设计，在保持高压缩质量的同时提供显著的性能提升。

Abstract: Modern scientific simulations and instruments generate data volumes that
overwhelm memory and storage, throttling scalability. Lossy compression
mitigates this by trading controlled error for reduced footprint and throughput
gains, yet optimal pipelines are highly data and objective specific, demanding
compression expertise. GPU compressors supply raw throughput but often
hard-code fused kernels that hinder rapid experimentation, and underperform in
rate-distortion. We present FZModules, a heterogeneous framework for assembling
error-bounded custom compression pipelines from high-performance modules
through a concise extensible interface. We further utilize an asynchronous
task-backed execution library that infers data dependencies, manages memory
movement, and exposes branch and stage level concurrency for powerful
asynchronous compression pipelines. Evaluating three pipelines built with
FZModules on four representative scientific datasets, we show they can compare
end-to-end speedup of fused-kernel GPU compressors while achieving similar
rate-distortion to higher fidelity CPU or hybrid compressors, enabling rapid,
domain-tailored design.

</details>


### [6] [Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem](https://arxiv.org/abs/2509.21039)
*William F. Godoy,Tatiana Melnichenko,Pedro Valero-Lara,Wael Elwasif,Philip Fackler,Rafael Ferreira Da Silva,Keita Teranishi,Jeffrey S. Vetter*

Main category: cs.DC

TL;DR: Mojo语言在GPU科学计算工作负载中的性能与可移植性评估，显示其在内存密集型任务上与CUDA/HIP竞争，但在原子操作和快速数学计算密集型任务上存在差距


<details>
  <summary>Details</summary>
Motivation: 探索基于MLIR的新型Mojo语言在GPU科学计算中的表现，旨在解决Python生态系统在科学计算与AI融合中的性能和生产效率差距

Method: 针对四种科学计算工作负载（七点模板、BabelStream、miniBUDE、Hartree-Fock）在NVIDIA H100和AMD MI300A GPU上比较Mojo与CUDA/HIP的性能

Result: Mojo在内存密集型内核上性能与CUDA/HIP相当，但在AMD GPU上的原子操作以及两平台上的快速数学计算密集型内核存在性能差距

Conclusion: 尽管学习曲线和编程要求仍较底层，Mojo能在科学计算与AI融合的碎片化Python生态系统中填补重要空白

Abstract: We explore the performance and portability of the novel Mojo language for
scientific computing workloads on GPUs. As the first language based on the
LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure,
Mojo aims to close performance and productivity gaps by combining Python's
interoperability and CUDA-like syntax for compile-time portable GPU
programming. We target four scientific workloads: a seven-point stencil
(memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and
Hartree-Fock (compute-bound with atomic operations); and compare their
performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We
show that Mojo's performance is competitive with CUDA and HIP for memory-bound
kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math
compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve
and programming requirements are still fairly low-level, Mojo can close
significant gaps in the fragmented Python ecosystem in the convergence of
scientific computing and AI.

</details>


### [7] [Distributed-memory Algorithms for Sparse Matrix Permutation, Extraction, and Assignment](https://arxiv.org/abs/2509.20776)
*Elaheh Hassani,Md Taufique Hussain,Ariful Azad*

Main category: cs.DC

TL;DR: 提出了可扩展的分布式内存算法用于稀疏矩阵的置换、提取和赋值操作，采用Identify-Exchange-Build策略减少通信开销，并通过无同步多线程算法提升本地计算性能


<details>
  <summary>Details</summary>
Motivation: 现有的分布式库如CombBLAS和PETSc在稀疏矩阵操作上存在通信开销大和性能不足的问题，需要更高效的算法来支持大规模稀疏矩阵处理

Method: 采用Identify-Exchange-Build三阶段策略：识别要发送的本地非零元素、交换所需数据、从接收元素构建本地子矩阵，结合无同步多线程算法加速本地计算

Result: 在多个集群上的实验表明，该算法相比CombBLAS和PETSc实现了显著性能提升，适用于负载均衡、矩阵重排序、子图提取和流图应用等多种场景

Conclusion: 本研究为稀疏矩阵置换、提取和赋值操作提供了完整的算法、软件实现和实验评估，为大规模稀疏矩阵处理提供了高效解决方案

Abstract: We present scalable distributed-memory algorithms for sparse matrix
permutation, extraction, and assignment. Our methods follow an
Identify-Exchange-Build (IEB) strategy where each process identifies the local
nonzeros to be sent, exchanges the required data, and then builds its local
submatrix from the received elements. This approach reduces communication
compared to SpGEMM-based methods in distributed memory. By employing
synchronization-free multithreaded algorithms, we further accelerate local
computations, achieving substantially better performance than existing
libraries such as CombBLAS and PETSc. We design efficient software for these
operations and evaluate their performance on two university clusters and the
Perlmutter supercomputer. Our experiments span a variety of application
scenarios, including matrix permutation for load balancing, matrix reordering,
subgraph extraction, and streaming graph applications. In all cases, we compare
our algorithms against CombBLAS, the most comprehensive distributed library for
these operations, and, in some scenarios, against PETSc. Overall, this work
provides a comprehensive study of algorithms, software implementations,
experimental evaluations, and applications for sparse matrix permutation,
extraction, and assignment.

</details>


### [8] [Integrating and Characterizing HPC Task Runtime Systems for hybrid AI-HPC workloads](https://arxiv.org/abs/2509.20819)
*Andre Merzky,Mikhail Titov,Matteo Turilli,Shantenu Jha*

Main category: cs.DC

TL;DR: 本文研究了RADICAL-Pilot与Flux和Dragon运行时系统集成，用于高性能计算和机器学习混合工作负载的性能优化，相比传统Slurm的srun在任务执行速率和资源利用率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 科学工作流越来越多地涉及HPC和机器学习任务的结合，但传统的启动器如Slurm的srun在并发性和吞吐量方面存在限制，不适合动态和异构的工作负载。

Method: 将RADICAL-Pilot与Flux和Dragon两个互补的运行时系统集成，实现分层资源管理和高吞吐量函数执行，使用合成和生产规模的工作负载在Frontier系统上进行性能测试。

Result: RP+Flux可持续达到930任务/秒，RP+Flux+Dragon超过1,500任务/秒，利用率超过99.6%；而srun峰值仅为152任务/秒，利用率低于50%。在IMPEccABLE.v2药物发现应用中，RP+Flux相比srun/Slurm将makespan减少了30-60%，吞吐量提高了四倍以上。

Conclusion: RADICAL-Pilot与运行时系统的混合集成为混合AI-HPC工作负载提供了一种可扩展的方法。

Abstract: Scientific workflows increasingly involve both HPC and machine-learning
tasks, combining MPI-based simulations, training, and inference in a single
execution. Launchers such as Slurm's srun constrain concurrency and throughput,
making them unsuitable for dynamic and heterogeneous workloads. We present a
performance study of RADICAL-Pilot (RP) integrated with Flux and Dragon, two
complementary runtime systems that enable hierarchical resource management and
high-throughput function execution. Using synthetic and production-scale
workloads on Frontier, we characterize the task execution properties of RP
across runtime configurations. RP+Flux sustains up to 930 tasks/s, and
RP+Flux+Dragon exceeds 1,500 tasks/s with over 99.6% utilization. In contrast,
srun peaks at 152 tasks/s and degrades with scale, with utilization below 50%.
For IMPECCABLE.v2 drug discovery campaign, RP+Flux reduces makespan by 30-60%
relative to srun/Slurm and increases throughput more than four times on up to
1,024. These results demonstrate hybrid runtime integration in RP as a scalable
approach for hybrid AI-HPC workloads.

</details>


### [9] [RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training](https://arxiv.org/abs/2509.21009)
*Wei Gao,Yuheng Zhao,Dakai An,Tianyuan Wu,Lunxi Cao,Shaopan Xiong,Ju Huang,Weixun Wang,Siran Yang,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: 本文提出了一种称为tail batching的新型rollout调度策略，通过将长尾响应整合到少量专门的long rounds中，有效减少GPU空闲时间，显著加速RL训练而不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 同步RL后训练存在GPU利用率不足的问题，主要由于rollout步骤中响应长度不平衡导致的气泡现象。现有系统通过放松同步性来缓解此问题，但会损害训练准确性。

Method: 引入tail batching策略，系统地将导致长尾响应的提示整合到少量rollout步骤（long rounds）中，确保大多数步骤（short rounds）仅涉及平衡的短rollout。同时提出RollPacker系统，在三个RL阶段进行全面优化。

Result: 实验结果表明，RollPacker在128个H800 GPU上对Qwen2.5系列LLMs实现了2.03x-2.56x的端到端训练时间减少，相比veRL和RLHFuse分别达到2.24x的加速。

Conclusion: tail batching策略和RollPacker系统能够有效解决同步RL训练中的GPU利用率问题，在保持训练准确性的同时显著提升训练效率。

Abstract: Reinforcement Learning (RL) is a pivotal post-training technique for
enhancing the reasoning capabilities of Large Language Models (LLMs). However,
synchronous RL post-training often suffers from significant GPU
underutilization, referred to as bubbles, caused by imbalanced response lengths
within rollout steps. Many RL systems attempt to alleviate this problem by
relaxing synchronization, but this can compromise training accuracy. In this
paper, we introduce tail batching, a novel rollout scheduling strategy for
synchronous RL that systematically consolidates prompts leading to long-tail
responses into a small subset of rollout steps (long rounds), while ensuring
that the majority of steps (short rounds) involve only balanced, short
rollouts. By excluding long responses from short rounds and rescheduling them
into a few designated long rounds, tail batching effectively reduces GPU idle
time during rollouts and significantly accelerates RL training without
sacrificing accuracy. We present RollPacker, a system that fully harnesses the
benefits of tail batching through holistic optimizations across all three RL
stages: elastic parallelism adaptation for rollout, dynamic resource allocation
and scheduling for reward, and stream-based training. Empirical results show
that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction
compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5
family of LLMs on up to 128 H800 GPUs.

</details>


### [10] [Utilizing Sparsity in the GPU-accelerated Assembly of Schur Complement Matrices in Domain Decomposition Methods](https://arxiv.org/abs/2509.21037)
*Jakub Homola,Ondřej Meca,Lubomír Říha,Tomáš Brzobohatý*

Main category: cs.DC

TL;DR: 本文提出了一种通过利用输入矩阵稀疏性来改进GPU上Schur补矩阵组装的方法，在FETI方法中实现了5.1倍的GPU部分加速和3.3倍的整体组装加速。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算集群主要依赖GPU，需要加速域分解方法中的Schur补矩阵组装过程。显式组装Schur补矩阵成本高昂，是GPU加速的主要开销。

Method: 通过明智利用输入矩阵的稀疏性来优化GPU上的Schur补矩阵组装过程，在FETI方法框架下实现更高效的GPU计算。

Result: 在GPU代码部分实现了5.1倍的加速，整体组装过程实现了3.3倍的加速，使得从仅10次迭代开始就能获得加速收益。

Conclusion: 利用输入矩阵稀疏性可以显著改进GPU上的Schur补矩阵组装性能，使GPU加速在更少的迭代次数下就变得有益。

Abstract: Schur complement matrices emerge in many domain decomposition methods that
can solve complex engineering problems using supercomputers. Today, as most of
the high-performance clusters' performance lies in GPUs, these methods should
also be accelerated.
  Typically, the offloaded components are the explicitly assembled dense Schur
complement matrices used later in the iterative solver for multiplication with
a vector. As the explicit assembly is expensive, it represents a significant
overhead associated with this approach to acceleration. It has already been
shown that the overhead can be minimized by assembling the Schur complements
directly on the GPU.
  This paper shows that the GPU assembly can be further improved by wisely
utilizing the sparsity of the input matrices. In the context of FETI methods,
we achieved a speedup of 5.1 in the GPU section of the code and 3.3 for the
whole assembly, making the acceleration beneficial from as few as 10
iterations.

</details>


### [11] [Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training](https://arxiv.org/abs/2509.21275)
*Shiju Wang,Yujie Wang,Ao Sun,Fangcheng Fu,Zijian Zhu,Bin Cui,Xu Han,Kaisheng Ma*

Main category: cs.DC

TL;DR: 本文提出弹性管道并行（EPP）方法，通过协调token级和batch级管道并行来适应资源和负载异质性，并构建了InfiniPipe系统，相比现有最优系统实现了1.69倍加速。


<details>
  <summary>Details</summary>
Motivation: 长上下文训练对LLM上下文扩展至关重要。现有方案如序列并行会产生大量通信开销。管道并行（PP）能降低此成本，但其效果取决于分区粒度。batch级PP在长上下文场景下内存消耗高，token级PP能缓解内存开销但可能导致硬件利用率不足。此外，真实数据集序列长度分布的偏斜性对PP的负载平衡和高效调度构成挑战。

Method: 提出弹性管道并行（EPP），构建InfiniPipe分布式训练系统，包含：（1）资源感知和负载平衡的序列处理器，用于分割长序列和打包短序列；（2）协同优化方法，通过阶段感知的块级自适应检查点机制联合优化管道调度和梯度检查点。

Result: 综合实验表明，InfiniPipe相比现有最优系统实现了1.69倍的加速。

Conclusion: EPP方法能有效适应资源和负载异质性，InfiniPipe系统在长上下文训练中表现出显著性能优势。

Abstract: Long context training is crucial for LLM's context extension. Existing
schemes, such as sequence parallelism, incur substantial communication
overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness
hinges on partitioning granularity. Batch-level PP dividing input samples
exhibits high memory consumption in long-context scenario, whereas token-level
PP splitting sequences into slices alleviates memory overhead but may incur
hardware under-utilization. This trade-off motivates adaptively selecting PP
granularity to match resource and workload characteristics. Moreover, sequence
length distribution of the real-world dataset exhibits skewness, posing a
challenge on PP's workload balance and efficient scheduling. Current static PP
scheduling methods overlook the variance of sequence length, leading to
suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism
(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource
and workload heterogeneity. We build InfiniPipe, a distributed training system
that unleashes the potential of EPP via (1) a resource-aware and
workload-balanced sequence processor that splits long sequences and packs short
ones; and (2) a co-optimization methodology that jointly optimizes pipeline
schedule and gradient checkpointing via a mechanism named stage-aware
chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that
InfiniPipe achieves a 1.69x speedup over state-of-the-art systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [Pedagogically Motivated and Composable Open-Source RISC-V Processors for Computer Science Education](https://arxiv.org/abs/2509.20514)
*Ian McDougall,Harish Batchu,Michael Davies,Karthikeyan Sankaralingam*

Main category: cs.AR

TL;DR: 本文提出了一个评估RISC-V实现生态系统的标准，分析了现有开源实现，并开发了一个满足所有标准的可组合教学框架。


<details>
  <summary>Details</summary>
Motivation: RISC-V ISA作为免费开源替代方案，需要一个易于使用且稳健的实现用于教学和业余用途。

Method: 首先制定RISC-V实现生态系统的教学评估标准，然后分析现有开源实现，最后开发满足所有标准的可组合框架。

Result: 开发了一个全面的开源解决方案，其组件可以根据课程需求进行分解，并收集了学生反馈。

Conclusion: 成功创建了一个满足教学需求的RISC-V实现框架，为其他教育工作者提供了可用的开源工具。

Abstract: While most instruction set architectures (ISAs) are only available to use
through the purchase of a restrictive commercial license, the RISC-V ISA
presents a free and open-source alternative. Due to this availability, many
free and open-source implementations have been developed and can be accessed on
platforms such as GitHub. If an open source, easy-to-use, and robust RISC-V
implementation could be obtained, it could be easily adapted for pedagogical
and amateur use. In this work we accomplish three goals in relation to this
outlook. First, we propose a set of criteria for evaluating the components of a
RISC-V implementation's ecosystem from a pedagogical perspective. Second, we
analyze a number of existing open-source RISC-V implementations to determine
how many of the criteria they fulfill. We then develop a comprehensive solution
that meets all of these criterion and is released open-source for other
instructors to use. The framework is developed in a composable way that it's
different components can be disaggregated per individual course needs. Finally,
we also report on a limited study of student feedback.

</details>


### [13] [ZynqParrot: A Scale-Down Approach to Cycle-Accurate, FPGA-Accelerated Co-Emulation](https://arxiv.org/abs/2509.20543)
*Daniel Ruelas-Petrisko,Farzam Gilani,Anoop Mysore Nataraja,Zoe Taylor,Michael Taylor*

Main category: cs.AR

TL;DR: 提出了一种Scale-Down的FPGA建模平台ZynqParrot，通过将复杂系统分解为可管理的子组件进行独立原型设计，实现快速、准确且经济的处理器验证。


<details>
  <summary>Details</summary>
Motivation: 随着处理器复杂度增加，传统验证方法（性能计数器和微架构模拟）面临运行时间过长和成本过高的问题，需要一种更高效的验证方法。

Method: 采用Scale-Down方法，将系统分解为子组件进行独立原型设计，通过精心设计的原型接口确保被测设备的严格非干扰性，实现周期精确的协同仿真。

Result: 开发了ZynqParrot平台，能够执行非干扰的周期精确协同仿真，验证功能和性能，并通过开源RISC-V处理器的案例研究证明了其有效性。

Conclusion: Scale-Down方法结合FPGA加速，在保持速度的同时消除了Scale-Out的不准确性和Scale-Up的高成本，为架构师提供了最佳解决方案。

Abstract: As processors increase in complexity, costs grow even more rapidly, both for
functional verification and performance validation. Most often, silicon
characterizations comprise simple performance counters, which are aggregated
and separated to tell a story. Based on these inferences, performance engineers
employ microarchitectural simulation to inspect deeply into the core.
Unfortunately, dramatically longer runtimes make simulation infeasible for long
workloads.
  We propose a Scale-Down approach to modelling and validation. Rather than
up-sizing a prototyping platform to fit large and complex system designs, we
show that it can be more accurate, faster, and more economical to decompose a
system into manageable sub-components that can be prototyped independently. By
carefully designing the prototyping interface, it is possible to adhere to
strict non-interference of the Device Under Test (DUT). This allows architects
to have the best of both worlds: the speed of FPGA acceleration while
eliminating the inaccuracies of Scale-Out and the inherent costs of Scale-Up.
  In this work, we present ZynqParrot: a Scale-Down FPGA-based modelling
platform, capable of executing non-interfering, cycle-accurate co-emulations of
arbitrary RTL designs. ZynqParrot is capable of verifying functionality and
performance with arbitrary granularity. We also provide case studies using
ZynqParrot to analyze the full-stack performance of an open-source RISC-V
processor.

</details>
