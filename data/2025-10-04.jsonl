{"id": "2510.01216", "categories": ["cs.DC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.01216", "abs": "https://arxiv.org/abs/2510.01216", "authors": ["Preston Vander Vos"], "title": "Odontoceti: Ultra-Fast DAG Consensus with Two Round Commitment", "comment": "MSc thesis. Supervisors: Philipp Jovanovic and Alberto Sonnino", "summary": "Users of blockchains value scalability, expecting fast confirmations and\nimmediate transaction processing. Odontoceti, the latest in DAG-based\nconsensus, addresses these concerns by prioritizing low latency and high\nthroughput, making a strategic trade-off in security by operating with a 20%\nfault tolerance instead of the established 33% level. It is the first DAG-based\nprotocol to achieve commitment in just two communication rounds, delivering\nmedian latency of 300 milliseconds while processing 10,000 transactions per\nsecond under realistic network conditions. Odontoceti operates with n = 5f + 1\nvalidators and creates an uncertified DAG with a novel decision rule for\ncommitting blocks. The protocol includes an optimization that advances progress\nwhen participants are slow, benefiting crash fault scenarios which are more\ncommon in practice than Byzantine faults. Evaluation results demonstrate 20-25%\nlatency improvements compared to an existing production protocol, validating\nthat reducing wave length from three rounds to two rounds yields meaningful\nperformance benefits. This paper establishes the practical viability of lower\nfault tolerance consensus protocols for blockchains."}
{"id": "2510.01256", "categories": ["cs.DC", "cs.AI", "cs.IT", "cs.LG", "math.IT", "I.2.6; I.2.7; C.2.4; C.1.4"], "pdf": "https://arxiv.org/pdf/2510.01256", "abs": "https://arxiv.org/abs/2510.01256", "authors": ["Lingling Zeng", "Gen Zhang", "Jialin Peng", "Xiang Xu", "Yuan Xu", "Lijun Ma"], "title": "Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters", "comment": "25 pages,15 figures", "summary": "As AI cluster sizes continue to expand and the demand for\nlarge-language-model (LLM) training and inference workloads grows rapidly,\ntraditional scheduling systems face significant challenges in balancing\nresource utilization, scheduling efficiency, and service quality. This paper\npresents and evaluates Kant: an efficient unified scheduling platform designed\nfor large-scale AI container clusters, supporting the co-scheduling of both\ntraining and inference jobs. Based on the practical implementation of the Kant\nsystem, we systematically define a set of key evaluation metrics for AI\nclusters, including GPU Allocation Ratio (GAR), Scheduling Occupancy Rate\n(SOR), GPU Node Fragmentation Ratio (GFR), Job Waiting Time Distribution\n(JWTD), and Job Training Time Estimation Distribution (JTTED), providing a\nfoundation for quantitative performance analysis. Experimental results\ndemonstrate that Kant achieves exceptional performance in clusters ranging from\nhundreds to tens of thousands of GPUs. By leveraging scheduling strategies such\nas Backfill and Enhanced Binpack (E-Binpack), the system significantly improves\nresource utilization and scheduling efficiency, while effectively reducing\nresource fragmentation and communication overhead in distributed training. The\nsystem has been deployed in multiple AI data center clusters, where it stably\nsupports large-scale intelligent computing workloads. This work provides a\npractical engineering approach for building high-performance, highly available,\nAI-native scheduling infrastructure."}
{"id": "2510.01260", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.01260", "abs": "https://arxiv.org/abs/2510.01260", "authors": ["Ningyuan Yang", "Guanliang Lyu", "Mingchen Ma", "Yiyi Lu", "Yiming Li", "Zhihui Gao", "Hancheng Ye", "Jianyi Zhang", "Tingjun Chen", "Yiran Chen"], "title": "IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol", "comment": null, "summary": "The integration of Large Language Models (LLMs) with Internet-of-Things (IoT)\nsystems faces significant challenges in hardware heterogeneity and control\ncomplexity. The Model Context Protocol (MCP) emerges as a critical enabler,\nproviding standardized communication between LLMs and physical devices. We\npropose IoT-MCP, a novel framework that implements MCP through edge-deployed\nservers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we\nintroduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g.,\n``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel\nso hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation\nacross 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100%\ntask success rate to generate tool calls that fully meet expectations and\nobtain completely accurate results, 205ms average response time, and 74KB peak\nmemory footprint. This work delivers both an open-source integration framework\n(https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized\nevaluation methodology for LLM-IoT systems."}
{"id": "2510.01730", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.01730", "abs": "https://arxiv.org/abs/2510.01730", "authors": ["Ashiyana Abdul Majeed", "Mahmoud Meribout", "Safa Mohammed Sali"], "title": "Edge GPU Aware Multiple AI Model Pipeline for Accelerated MRI Reconstruction and Analysis", "comment": "11 pages. 14 figures. This work has been submitted to IEEE for\n  possible publication", "summary": "Advancements in AI have greatly enhanced the medical imaging process, making\nit quicker to diagnose patients. However, very few have investigated the\noptimization of a multi-model system with hardware acceleration. As specialized\nedge devices emerge, the efficient use of their accelerators is becoming\nincreasingly crucial. This paper proposes a hardware-accelerated method for\nsimultaneous reconstruction and diagnosis of \\ac{MRI} from \\ac{CT} images.\nReal-time performance of achieving a throughput of nearly 150 frames per second\nwas achieved by leveraging hardware engines available in modern NVIDIA edge\nGPU, along with scheduling techniques. This includes the GPU and the \\ac{DLA}\navailable in both Jetson AGX Xavier and Jetson AGX Orin, which were considered\nin this paper. The hardware allocation of different layers of the multiple AI\nmodels was done in such a way that the ideal time between the hardware engines\nis reduced. In addition, the AI models corresponding to the \\ac{GAN} model were\nfine-tuned in such a way that no fallback execution into the GPU engine is\nrequired without compromising accuracy. Indeed, the accuracy corresponding to\nthe fine-tuned edge GPU-aware AI models exhibited an accuracy enhancement of\n5\\%. A further hardware allocation of two fine-tuned GPU-aware GAN models\nproves they can double the performance over the original model, leveraging\nadequate partitioning on the NVIDIA Jetson AGX Xavier and Orin devices. The\nresults prove the effectiveness of employing hardware-aware models in parallel\nfor medical image analysis and diagnosis."}
{"id": "2510.01536", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.01536", "abs": "https://arxiv.org/abs/2510.01536", "authors": ["Hasan Heydari", "Alysson Bessani", "Kartik Nayak"], "title": "QScale: Probabilistic Chained Consensus for Moderate-Scale Systems", "comment": null, "summary": "Existing distributed ledger protocols either incur a high communication\ncomplexity and are thus suited to systems with a small number of processes\n(e.g., PBFT), or rely on committee-sampling-based approaches that only work for\na very large number of processes (e.g., Algorand). Neither of these lines of\nwork is well-suited for moderate-scale distributed ledgers ranging from a few\nhundred to a thousand processes, which are common in production (e.g, Redbelly,\nSui). The goal of this work is to design a distributed ledger with sub-linear\ncommunication complexity per process, sub-quadratic total communication\ncomplexity, and low latency for finalizing a block into the ledger, such that\nit can be used for moderate-scale systems. We propose QScale, a protocol in\nwhich every process incurs only $\\widetilde{O}(\\kappa \\sqrt{n})$ communication\ncomplexity per-block in expectation, $\\widetilde{O}(n\\kappa)$ total\ncommunication complexity per-block in expectation, and a best-case latency of\n$O(\\kappa)$ rounds while ensuring safety and liveness with overwhelming\nprobability, with $\\kappa$ being a small security parameter."}
{"id": "2510.02099", "categories": ["cs.AR", "cs.NE", "B.3; B.7; I.4"], "pdf": "https://arxiv.org/pdf/2510.02099", "abs": "https://arxiv.org/abs/2510.02099", "authors": ["Felix Zeller", "John Reuben", "Dietmar Fey"], "title": "Multiplier-free In-Memory Vector-Matrix Multiplication Using Distributed Arithmetic", "comment": "9 pages, 10 figures", "summary": "Vector-Matrix Multiplication (VMM) is the fundamental and frequently required\ncomputation in inference of Neural Networks (NN). Due to the large data\nmovement required during inference, VMM can benefit greatly from in-memory\ncomputing. However, ADC/DACs required for in-memory VMM consume significant\npower and area. `Distributed Arithmetic (DA)', a technique in computer\narchitecture prevalent in 1980s was used to achieve inner product or dot\nproduct of two vectors without using a hard-wired multiplier when one of the\nvectors is a constant. In this work, we extend the DA technique to multiply an\ninput vector with a constant matrix. By storing the sum of the weights in\nmemory, DA achieves VMM using shift-and-add circuits in the periphery of ReRAM\nmemory. We verify functional and also estimate non-functional properties\n(latency, energy, area) by performing transistor-level simulations. Using\nenergy-efficient sensing and fine grained pipelining, our approach achieves 4.5\nx less latency and 12 x less energy than VMM performed in memory conventionally\nby bit slicing. Furthermore, DA completely eliminated the need for power-hungry\nADCs which are the main source of area and energy consumption in the current\nVMM implementations in memory."}
{"id": "2510.01885", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.01885", "abs": "https://arxiv.org/abs/2510.01885", "authors": ["Jamie Cotter", "Ignacio Castineiras", "Victor Cionca"], "title": "Accuracy vs Performance: An abstraction model for deadline constrained offloading at the mobile-edge", "comment": "Presented at in Irish Signals and Systems Conference 2025", "summary": "In this paper, we present a solution for low-latency deadline-constrained DNN\noffloading on mobile edge devices. We design a scheduling algorithm with\nlightweight network state representation, considering device availability,\ncommunication on the network link, priority-aware pre-emption, and task\ndeadlines. The scheduling algorithm aims to reduce latency by designing a\nresource availability representation, as well as a network discretisation and a\ndynamic bandwidth estimation mechanism. We implement the scheduling algorithm\ninto a system composed of four Raspberry Pi 2 (model Bs) mobile edge devices,\nsampling a waste classification conveyor belt at a set frame rate. The system\nis evaluated and compared to a previous approach of ours, which was proven to\noutcompete work-stealers and a non-pre-emption based scheduling heuristic under\nthe aforementioned waste classification scenario. Our findings show the novel\nlower latency abstraction models yield better performance under high-volume\nworkloads, with the dynamic bandwidth estimation assisting the task placement\nwhile, ultimately, increasing task throughput in times of resource scarcity."}
{"id": "2510.02170", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.02170", "abs": "https://arxiv.org/abs/2510.02170", "authors": ["Nick Brown", "Jake Davies", "Felix LeClair"], "title": "Programming RISC-V accelerators via Fortran", "comment": "Accepted extended abstract to the RISC-V Summit Europe 2025", "summary": "A range of RISC-V based accelerators are available and coming to market, and\nthere is strong potential for these to be used for High Performance Computing\n(HPC) workloads. However, such accelerators tend to provide bespoke programming\nmodels and APIs that require codes to be rewritten. In scientific computing,\nwhere many of the simulation code are highly complex, extensive, and written in\nFortran, this is not realistic. In this extended abstract we present an\napproach that enables driving such architectures via Fortran, avoiding code\nredevelopment."}
