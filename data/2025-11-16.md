<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 5]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Cyclotron: Compilation of Recurrences to Distributed and Systolic Architectures](https://arxiv.org/abs/2511.09987)
*Shiv Sundram,Akhilesh Balasingam,Nathan Zhang,Kunle Olukotun,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: Cyclotron是一个用于表达流式数据流算法的框架和编译器，使用递归方程来定义算法，然后可移植地编译到分布式处理器拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 为了解决在分布式系统中高效执行流式数据流算法的需求，提供一种能够优化内存访问、减少数据移动的可移植编译框架。

Method: 使用递归方程在逻辑张量上表达算法，通过中间表示层逐步降低到特定处理器的发送、接收和计算操作，提供调度语言定义数据流和广播方式。

Result: 框架能够生成在模拟可重构系统和分布式CPU集群上运行的代码，对于矩阵乘法和三角求解等算法，生成的分布式实现与ScaLAPACK性能相当。

Conclusion: Cyclotron提供了一个有效的框架，通过递归方程和优化编译技术，实现了流式数据流算法在分布式系统中的高效可移植执行。

Abstract: We present Cyclotron, a framework and compiler for using recurrence equations to express streaming dataflow algorithms, which then get portably compiled to distributed topologies of interlinked processors. Our framework provides an input language of recurrences over logical tensors, which then gets lowered into an intermediate language of recurrences over logical iteration spaces, and finally into programs of send, receive, and computation operations specific to each individual processor. In Cyclotron's IR, programs are optimized such that external memory interactions are confined to the boundaries of the iteration space. Within inner iteration spaces, all data accesses become local: data accesses target values residing in local fast memory or on neighboring processing units, avoiding costly memory movement. We provide a scheduling language allowing users to define how data gets streamed and broadcasted between processors, enabling pipelined execution of computation kernels over distributed topologies of processing elements. We demonstrate the portability of our approach by compiling our IR to a reconfigurable simulator of systolic arrays and chiplet style distributed hardware, as well as to distributed-memory CPU clusters. In the simulated reconfigurable setting, we use our compiler for hardware design space exploration in which link costs and latencies can be specified. In the distributed CPU setting, we show how to use recurrences and our scheduling language to express various matrix multiplication routines (Cannon, SUMMA, PUMMA, weight stationary) and solvers (Triangular solve and Cholesky). For matrix multiplication and the triangular solve, we generate distributed implementations competitive with ScaLAPACK.

</details>


### [2] [Omnidirectional type inference for ML: principality any way](https://arxiv.org/abs/2511.10343)
*Alistair O'Brien,Didier Rémy,Gabriel Scherer*

Main category: cs.PL

TL;DR: 本文提出了一种称为"全向类型推断"的新方法，通过动态信息流和挂起匹配约束来解决ML类型系统扩展中的主要性问题，相比静态顺序推断方法具有更好的表达能力。


<details>
  <summary>Details</summary>
Motivation: ML类型系统的成功归功于主要性（每个良类型表达式都有唯一最一般类型），但GADTs、高阶多态和静态重载等扩展会破坏主要性。现有的方向性推断算法采用固定顺序传播类型信息，往往拒绝本应良类型的程序。

Method: 提出全向类型推断，允许类型信息以动态顺序流动，使用挂起匹配约束在需要已知类型信息时暂停并恢复。引入增量实例化机制，允许包含挂起约束的部分求解类型方案被实例化，并在方案细化时增量更新实例。

Result: 该方法在OCaml的两个根本不同特性上验证了有效性：记录标签和数据构造器的静态重载，以及半显式一等多态。在这两种情况下都获得了比OCaml当前类型检查器更具表达力的主要类型推断算法。

Conclusion: 全向性为在存在脆弱特性时恢复主要性提供了一个通用框架，能够处理ML类型系统扩展带来的主要性挑战。

Abstract: The Damas-Hindley-Milner (ML) type system owes its success to principality, the property that every well-typed expression has a unique most general type. This makes inference predictable and efficient. Unfortunately, many extensions of ML (GADTs, higher-rank polymorphism, and static overloading) endanger princpality by introducing _fragile_ constructs that resist principal inference. Existing approaches recover principality through directional inference algorithms, which propagate _known_ type information in a fixed (or static) order (e.g. as in bidirectional typing) to disambiguate such constructs. However, the rigidity of a static inference order often causes otherwise well-typed programs to be rejected.
  We propose _omnidirectional_ type inference, where type information flows in a dynamic order. Typing constraints may be solved in any order, suspending when progress requires known type information and resuming once it becomes available, using _suspended match constraints_. This approach is straightforward for simply typed systems, but extending it to ML is challenging due to let-generalization. Existing ML inference algorithms type let-bindings (let x = e1 in e2) in a fixed order: type e1, generalize its type, and then type e2. To overcome this, we introduce _incremental instantiation_, allowing partially solved type schemes containing suspended constraints to be instantiated, with a mechanism to incrementally update instances as the scheme is refined.
  Omnidirectionality provides a general framework for restoring principality in the presence of fragile features. We demonstrate its versatility on two fundamentally different features of OCaml: static overloading of record labels and datatype constructors and semi-explicit first-class polymorphism. In both cases, we obtain a principal type inference algorithm that is more expressive than OCaml's current typechecker.

</details>


### [3] [Lazy Linearity for a Core Functional Language](https://arxiv.org/abs/2511.10361)
*Rodrigo Mesquita,Bernardo Toninho*

Main category: cs.PL

TL;DR: 本文提出了Linear Core系统，在惰性求值语言中重新定义线性类型语义，使其与编译器优化兼容，解决了传统语法线性在惰性求值环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统线性类型系统中，资源的消耗与语法出现直接对应，但在惰性求值语言中，语法出现不一定意味着实际使用资源。Haskell优化编译器会重写程序，破坏语法线性但保持语义，需要新的线性类型系统来适应这种特性。

Method: 设计了Linear Core系统，基于惰性语义重新定义线性类型，使其能够静态接受惰性语义，适用于GHC的Core中间语言。证明了系统的正确性，并验证了多种优化转换在Linear Core中保持线性性。

Result: Linear Core被实现为编译器插件，在线性资源密集的库（包括linear-base）中验证了系统的有效性。系统能够保证线性资源使用，同时允许编译器优化而不破坏线性性。

Conclusion: Linear Core成功解决了惰性求值语言中线性类型与编译器优化的冲突问题，为线性类型在惰性语言中的实际应用提供了可行的解决方案。

Abstract: Traditionally, in linearly typed languages, consuming a linear resource is synonymous with its syntactic occurrence in the program. However, under the lens of non-strict evaluation, linearity can be further understood semantically, where a syntactic occurrence of a resource does not necessarily entail using that resource when the program is executed. While this distinction has been largely unexplored, it turns out to be inescapable in Haskell's optimising compiler, which heavily rewrites the source program in ways that break syntactic linearity but preserve the program's semantics. We introduce Linear Core, a novel system which accepts the lazy semantics of linearity statically and is suitable for lazy languages such as the Core intermediate language of the Glasgow Haskell Compiler. We prove that Linear Core is sound, guaranteeing linear resource usage, and that multiple optimising transformations preserve linearity in Linear Core while failing to do so in Core. We have implemented Linear Core as a compiler plugin to validate the system against linearity-heavy libraries, including linear-base.

</details>


### [4] [Modeling Layout Abstractions Using Integer Set Relations](https://arxiv.org/abs/2511.10374)
*Somashekaracharya G Bhaskaracharya,Aravind Acharya,Bastian Hagedorn,Vinod Grover*

Main category: cs.PL

TL;DR: 使用整数集合库(ISL)为CuTe和Triton两种布局系统创建统一的数学表示，通过整数集合关系实现形式化分析和正确性验证。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习编译器依赖布局抽象来管理逻辑张量结构与物理内存安排之间的复杂映射。CuTe和Triton线性布局是广泛采用的行业标准，但它们具有不同的数学基础，阻碍了统一的形式化分析和跨系统推理。

Method: 通过整数集合关系建模CuTe布局（使用基于步长的计算将多维坐标转换为线性索引，包括复杂的swizzle操作）和Triton线性布局（在有限域F_2规则下建模二进制向量空间变换）。使用ISL内置操作实现完整的布局操作算法套件。

Result: 实验评估表明，该系统能够处理从基本恒等变换到具有复杂步长配置和swizzle模式的多维张量排列的完整布局复杂度谱系，验证了不同布局范式的数学建模方法。

Conclusion: 该方法为CuTe和Triton布局系统提供了统一的数学表示，实现了严格的形式化分析、正确性验证，并为未来的跨系统优化策略奠定了基础。

Abstract: Modern deep learning compilers rely on layout abstractions to manage the complex mapping between logical tensor structures and physical memory arrangements. CuTe layouts and Triton linear layouts are widely adopted industry standards. However, these layout systems operate independently with distinct mathematical underpinnings, preventing unified formal analysis and cross-system reasoning. We bridge this gap by introducing a novel approach that leverages the Integer Set Library (ISL) to create a unified mathematical representation for both layout systems through integer set relations, thereby enabling rigorous formal analysis, correctness verification, and the foundation for future cross-system optimization strategies. Our approach models CuTe layouts through integer set relations that encode the transformation from multi-dimensional coordinates to linear indices using stride-based calculations, including sophisticated swizzle operations that perform bit-level manipulations for enhanced memory access patterns. For Triton linear layouts, we construct integer set relations that model the binary vector space transformations where arithmetic operations follow finite field F_2 rules. We implement a complete suite of layout manipulation algorithms for composition, inversion, complement using built-in operations in ISL to ensure mathematical correctness and preserve layout semantics. Experimental evaluation shows that the system handles the full spectrum of layout complexity, from elementary identity transformations to sophisticated multi-dimensional tensor arrangements with complex stride configurations and swizzle patterns, validating the mathematical modeling approach across different layout paradigms.

</details>


### [5] [zkStruDul: Programming zkSNARKs with Structural Duality](https://arxiv.org/abs/2511.10565)
*Rahul Krishnan,Ashley Samuelson,Emily Yao,Ethan Cecchetti*

Main category: cs.PL

TL;DR: zkStruDul是一个统一输入转换和谓词定义的语言，通过单一抽象消除重复代码和安全漏洞，构建在现有NIZK技术之上。


<details>
  <summary>Details</summary>
Motivation: 现有NIZK工具将谓词定义和输入转换分开实现，导致逻辑重复和潜在的安全漏洞，需要统一抽象来解决这个问题。

Method: 开发zkStruDul语言，将输入转换和谓词定义统一为单一抽象，编译器可从中投影出两个过程，消除重复代码。

Result: 实现了zkStruDul语言，提供高层抽象支持递归证明等特性，并证明了源级语义与投影语义的行为一致性。

Conclusion: zkStruDul成功解决了NIZK应用中输入转换和谓词定义分离带来的问题，提供了更安全高效的开发方式。

Abstract: Non-Interactive Zero Knowledge (NIZK) proofs, such as zkSNARKS, let one prove knowledge of private data without revealing it or interacting with a verifier. While existing tooling focuses on specifying the predicate to be proven, real-world applications optimize predicate definitions to minimize proof generation overhead, but must correspondingly transform predicate inputs. Implementing these two steps separately duplicates logic that must precisely match to avoid catastrophic security flaws. We address this shortcoming with zkStruDul, a language that unifies input transformations and predicate definitions into a single combined abstraction from which a compiler can project both procedures, eliminating duplicate code and problematic mismatches. zkStruDul provides a high-level abstraction to layer on top of existing NIZK technology and supports important features like recursive proofs. We provide a source-level semantics and prove its behavior is identical to the projected semantics, allowing straightforward standard reasoning.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Ksurf-Drone: Attention Kalman Filter for Contextual Bandit Optimization in Cloud Resource Allocation](https://arxiv.org/abs/2511.09766)
*Michael Dang'ana,Yuqiu Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: Ksurf方法在云环境中作为上下文多臂老虎机的目标函数模型，显著降低了延迟方差，减少了CPU和内存使用，并节省了成本。


<details>
  <summary>Details</summary>
Motivation: 云数据中心中容器基础设施的资源编排和配置参数搜索面临大规模配置空间和云不确定性的挑战，现有方法如Drone编排器在处理高度可变的云数据时准确性不足。

Method: 将Ksurf（一种针对高可变性云数据设计的最优方差最小化估计方法）作为上下文多臂老虎机的目标函数模型，集成到Drone编排器中进行资源编排。

Result: Ksurf实现了p95延迟方差降低41%，p99延迟方差降低47%，CPU使用减少4%，主节点内存使用减少7MB，在VarBench Kubernetes基准测试中平均工作节点pod数量节省7%成本。

Conclusion: Ksurf方法在高度可变的云工作负载环境下能够显著提升资源编排的性能和效率，降低延迟方差并节省资源成本。

Abstract: Resource orchestration and configuration parameter search are key concerns for container-based infrastructure in cloud data centers. Large configuration search space and cloud uncertainties are often mitigated using contextual bandit techniques for resource orchestration including the state-of-the-art Drone orchestrator. Complexity in the cloud provider environment due to varying numbers of virtual machines introduces variability in workloads and resource metrics, making orchestration decisions less accurate due to increased nonlinearity and noise. Ksurf, a state-of-the-art variance-minimizing estimator method ideal for highly variable cloud data, enables optimal resource estimation under conditions of high cloud variability.
  This work evaluates the performance of Ksurf on estimation-based resource orchestration tasks involving highly variable workloads when employed as a contextual multi-armed bandit objective function model for cloud scenarios using Drone. Ksurf enables significantly lower latency variance of $41\%$ at p95 and $47\%$ at p99, demonstrates a $4\%$ reduction in CPU usage and 7 MB reduction in master node memory usage on Kubernetes, resulting in a $7\%$ cost savings in average worker pod count on VarBench Kubernetes benchmark.

</details>


### [7] [A Poly-Log Approximation for Transaction Scheduling in Fog-Cloud Computing and Beyond](https://arxiv.org/abs/2511.09776)
*Ramesh Adhikari,Costas Busch,Pavan Poudel*

Main category: cs.DC

TL;DR: 本文研究了雾-云计算网络中事务调度问题，提出了在常数倍维网络中最小化总成本的调度算法，包括单对象和多对象访问场景，并提供分布式实现。


<details>
  <summary>Details</summary>
Motivation: 在分布式系统中，事务调度对于高效分配共享资源至关重要。雾-云计算模型中事务和共享对象可以在网络中移动，需要找到最优调度方案来最小化总成本。

Method: 针对常数倍维网络，提出了两种调度算法：单对象访问场景给出O(log n·log D)近似比算法，多对象访问场景给出O(k·log n·log D)近似比算法，并提供了无需全局知识的分布式版本。

Result: 所提算法在理论上保证了近似比性能，单对象场景为O(log n·log D)，多对象场景为O(k·log n·log D)，其中n为节点数，D为网络直径，k为每个事务访问的对象数。

Conclusion: 该研究为雾-云计算网络中的事务调度问题提供了有效的近似算法和分布式解决方案，适用于实际应用中常见的常数倍维网络结构。

Abstract: Transaction scheduling is crucial to efficiently allocate shared resources in a conflict-free manner in distributed systems. We investigate the efficient scheduling of transactions in a network of fog-cloud computing model, where transactions and their associated shared objects can move within the network. The schedule may require objects to move to transaction nodes, or the transactions to move to the object nodes. Moreover, the schedule may determine intermediate nodes where both objects and transactions meet. Our goal is to minimize the total combined cost of the schedule. We focus on networks of constant doubling dimension, which appear frequently in practice. We consider a batch problem where an arbitrary set of nodes has transactions that need to be scheduled. First, we consider a single shared object required by all the transactions and present a scheduling algorithm that gives an $O(\log n \cdot \log D)$ approximation of the optimal schedule, where $n$ is the number of nodes and $D$ is the diameter of the network. Later, we consider transactions accessing multiple shared objects (at most $k$ objects per transaction) and provide a scheduling algorithm that gives an $O(k \cdot \log n \cdot \log D)$ approximation. We also provide a fully distributed version of the scheduling algorithms where the nodes do not need global knowledge of transactions.

</details>


### [8] [MoFa: A Unified Performance Modeling Framework for LLM Pretraining](https://arxiv.org/abs/2511.09837)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Shangchao Su,Ziqing Yin,Zhiyan Cui,Hongfeng Sun,Baoguo He,Yueqiang Chen,Liang Dong,Xiyuan Li,Lingbin Wang,Lijun Ma,Qiang Huang,Ting Liu,Chong Wang,Can Wei*

Main category: cs.DC

TL;DR: MoFa是一个统一的预训练性能建模框架，整合了多维优化特征和容错机制，通过增强的成本模型和基于历史集群可靠性数据的容错模型，实现高精度性能预测和优化调优。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模从数十亿扩展到数万亿参数，分布式预训练面临巨大优化挑战。传统手动调优成本高昂，现有性能建模方法无法全面考虑优化特征和容错机制开销。

Method: 提出MoFa框架，包含增强的成本模型准确捕捉关键优化效果，集成基于历史集群可靠性数据的容错模型，并开发基于MoFa的调优系统探索最优预训练性能。

Result: 广泛的建模评估显示MoFa在各种场景下都能实现高预测精度，调优实验系统揭示了不同配置下影响预训练性能的关键因素。

Conclusion: MoFa为LLM预训练系统设计和部署提供了可靠的先验指导，能够有效解决大规模分布式预训练的性能优化挑战。

Abstract: The exponential growth in LLM scales, with parameters soaring from billions to trillions, has necessitated distributed pretraining across large clusters comprising thousands to tens of thousands of devices. While hybrid parallelization strategies enable such pretraining, the vast combinatorial strategy space introduces significant optimization challenges. Traditional manual tuning methods incur prohibitive trial-and-error costs, and existing performance modeling approaches exhibit critical limitations: they fail to comprehensively account for prevalent optimization features and ignore the substantial overhead imposed by essential fault tolerance mechanisms like checkpoint recovery in long-duration pretraining. To address these gaps, we propose MoFa, a novel pretraining performance modeling framework that unifies multi-dimensional optimization features and fault tolerance. MoFa incorporates an enhanced cost model to accurately capture the effects of key optimizations and integrates a fault tolerance model based on historical cluster reliability data. Besides, a MoFa-based tuning system is developed to explore optimal pretraining performance and potential bottlenecks in various scenarios. Extensive modeling evaluations demonstrate that MoFa can achieve high prediction accuracy across various scenarios. In addition, through comprehensive tuning experiments, our framework systematically reveals the key factors influencing pretraining performance under different configurations, which provides solid a priori guidance for LLM pretraining system design and deployment.

</details>


### [9] [Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs](https://arxiv.org/abs/2511.09861)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: GPU系统在数据中心中存在性能变异问题，特别是在运行大语言模型训练时。研究发现热不平衡导致的"Lit Silicon"效应是主要原因，即热引发的慢速GPU会拖累快速GPU。论文提出了检测和缓解方法，通过功率管理优化可获得6%性能提升和4%功率改善。


<details>
  <summary>Details</summary>
Motivation: GPU系统在现代数据中心中广泛使用，但存在节点和集群级别的性能变异问题，严重影响高性能计算和AI工作负载（如大语言模型训练）的效率。需要理解性能变异的根本原因并提出解决方案。

Method: 分析了单节点多GPU系统运行LLM训练的性能，发现内核级性能变异与并发计算通信(C3)高度相关。提出Lit Silicon效应理论，设计检测和缓解技术，评估三种功率管理解决方案：GPU热设计功率优化、节点级GPU功率限制优化、节点级CPU功率转移优化。

Result: 在两个AMD Instinct MI300X GPU系统和两个LLM训练框架上进行实验，观察到最高6%的性能提升和4%的功率改善，可能为数据中心节省数亿美元成本。

Conclusion: Lit Silicon效应是GPU性能变异的重要根源，提出的解决方案几乎零成本，可作为新的节点级功率管理层在数据中心中轻松部署，有效解决性能变异问题。

Abstract: GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, such as cutting-edge large language models (LLMs). We analyze the performance of a single-node multi-GPU system running LLM training, and observe that the kernel-level performance variation is highly correlated with concurrent computation communication (C3), a technique to overlap computation and communication across GPUs for performance gains. We then take a further step to reason that thermally induced straggling coupling with C3 impacts performance variation, coined as the Lit Silicon effect. Lit Silicon describes that in a multi-GPU node, thermal imbalance across GPUs introduces node-level straggler GPUs, which in turn slow down the leader GPUs. Lit Silicon leads to node-level performance variation and inefficiency, impacting the entire datacenter from the bottom up. We propose analytical performance and power models for Lit Silicon, to understand the potential system-level gains. We further design simple detection and mitigation techniques to effectively address the Lit Silicon problem, and evaluate three different power management solutions, including power optimization under GPU thermal design power, performance optimization under node-level GPU power capping, and performance optimization under node-level CPU power sloshing. We conduct experiments on two workloads on two AMD InstinctTM MI300X GPU systems under two LLM training frameworks, and observe up to 6% performance and 4% power improvements, potentially saving hundreds of millions of dollars in datacenters. Our solution is almost free lunch and can be effortlessly adopted in datacenters as a new node-level power management layer.

</details>


### [10] [Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction](https://arxiv.org/abs/2511.09956)
*Mani Tofigh,Edward Guo,Weiwei Jia,Xiaoning Ding,Jianchen Shan*

Main category: cs.DC

TL;DR: CacheX：一种在云虚拟机中探测缓存抽象的新方法，通过驱逐集技术实现细粒度缓存感知，无需硬件或hypervisor支持，可提升缓存利用率。


<details>
  <summary>Details</summary>
Motivation: 云虚拟机中的缓存优化往往无效，因为VM无法获知和控制CPU缓存的配置细节，包括缓存分区、共享情况以及内存到缓存的映射关系。

Method: 提出CacheX解决方案，使用驱逐集技术在VM内部探测准确细粒度的缓存抽象，并开发了LLC竞争感知的任务调度和虚拟颜色感知的页面缓存管理两种新技术。

Result: 在x86 Linux内核中实现的CacheX评估表明，它能有效提高公共云虚拟机中各种工作负载的缓存利用率。

Conclusion: CacheX提供了一种无需硬件或hypervisor支持的缓存抽象探测方法，能够显著改善云环境中的缓存性能问题。

Abstract: This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.

</details>


### [11] [Dynamic Edge Server Selection in Time-Varying Environments: A Reliability-Aware Predictive Approach](https://arxiv.org/abs/2511.10146)
*Jaime Sebastian Burbano,Arnova Abdullah,Eldiyar Zhantileuov,Mohan Liyanage,Rolf Schuster*

Main category: cs.DC

TL;DR: 提出MO-HAN方法，通过融合延迟预测、自适应可靠性和基于滞后的切换机制，在边缘计算中实现轻量级服务器选择，降低延迟并减少切换次数


<details>
  <summary>Details</summary>
Motivation: 多服务器架构中的动态网络拥塞给边缘服务器选择带来挑战，需要为延迟敏感的嵌入式应用提供有效的服务器选择方案

Method: 使用被动测量数据（到达率、利用率、负载大小）和指数调制有理延迟模型，计算平衡预测延迟和可靠性的得分，仅在有意义增益时进行切换

Result: MO-HAN在降低平均延迟和尾部延迟方面优于静态和公平分布基准方法，同时将切换次数减少近50%

Conclusion: MO-HAN无需侵入式检测或重型学习基础设施，适用于资源受限的嵌入式设备，实现了实用的边缘服务器选择方案

Abstract: Latency-sensitive embedded applications increasingly rely on edge computing, yet dynamic network congestion in multi-server architectures challenges proper edge server selection. This paper proposes a lightweight server-selection method for edge applications that fuses latency prediction with adaptive reliability and hysteresis-based handover. Using passive measurements (arrival rate, utilization, payload size) and an exponentially modulated rational delay model, the proposed Moderate Handover (MO-HAN) method computes a score that balances predicted latency and reliability to ensure handovers occur only when the expected gain is meaningful and maintain reduced end-to-end latency. Results show that MO-HAN consistently outperforms static and fair-distribution baselines by lowering mean and tail latencies, while reducing handovers by nearly 50% compared to pure opportunistic selection. These gains arise without intrusive instrumentation or heavy learning infrastructure, making MO-HAN practical for resource-constrained embedded devices.

</details>


### [12] [Selection of Supervised Learning-based Sparse Matrix Reordering Algorithms](https://arxiv.org/abs/2511.10180)
*Tao Tang,Youfu Jiang,Yingbo Cui,Jianbin Fang,Peng Zhang,Lin Peng,Chun Huang*

Main category: cs.DC

TL;DR: 提出基于监督学习的稀疏矩阵重排序算法选择模型，通过理解矩阵特征与常用重排序算法的关联，实现自动化智能选择，在Florida稀疏矩阵数据集上相比仅使用AMD算法减少55.37%求解时间。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏矩阵排序算法选择依赖暴力搜索或经验知识，缺乏适应不同稀疏矩阵结构的能力，需要更智能的自适应方法。

Method: 使用监督学习模型，学习矩阵特征与常用重排序算法之间的关联关系，实现自动化算法选择。

Result: 在Florida稀疏矩阵数据集上，模型能准确预测最优重排序算法，相比仅使用AMD算法减少55.37%求解时间，平均加速比为1.45。

Conclusion: 基于监督学习的稀疏矩阵重排序算法选择模型能有效提升求解效率，实现智能化的算法选择。

Abstract: Sparse matrix ordering is a vital optimization technique often employed for solving large-scale sparse matrices. Its goal is to minimize the matrix bandwidth by reorganizing its rows and columns, thus enhancing efficiency. Conventional methods for algorithm selection usually depend on brute-force search or empirical knowledge, lacking the ability to adjust to diverse sparse matrix structures.As a result, we have introduced a supervised learning-based model for choosing sparse matrix reordering algorithms. This model grasps the correlation between matrix characteristics and commonly utilized reordering algorithms, facilitating the automated and intelligent selection of the suitable sparse matrix reordering algorithm. Experiments conducted on the Florida sparse matrix dataset reveal that our model can accurately predict the optimal reordering algorithm for various matrices, leading to a 55.37% reduction in solution time compared to solely using the AMD reordering algorithm, with an average speedup ratio of 1.45.

</details>


### [13] [Workload Schedulers -- Genesis, Algorithms and Differences](https://arxiv.org/abs/2511.10258)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: 本文提出了一种新颖的现代工作负载调度器分类方法，描述了操作系统进程调度器、集群系统作业调度器和大数据调度器三类调度器，分析了它们从早期采用到现代实现的演变过程。


<details>
  <summary>Details</summary>
Motivation: 现代工作负载调度器种类繁多且不断发展，需要系统性的分类方法来理解不同类型调度器的特征和演变规律。

Method: 通过描述三类调度器的演化历程，分析它们使用的算法特征，比较不同类别调度器之间的差异。

Result: 建立了三类调度器的分类框架，揭示了它们从早期到现代的演变路径，并识别了不同类别调度器之间的关键差异。

Conclusion: 尽管调度器应用于不同规模系统（本地和分布式），但在调度策略设计上存在相似的重点关注方向。

Abstract: This paper presents a novel approach to categorization of modern workload schedulers. We provide descriptions of three classes of schedulers: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers and Big Data Schedulers. We describe their evolution from early adoptions to modern implementations, considering both the use and features of algorithms. In summary, we discuss differences between all presented classes of schedulers and discuss their chronological development. In conclusion we highlight similarities in the focus of scheduling strategies design, applicable to both local and distributed systems.

</details>


### [14] [FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing](https://arxiv.org/abs/2511.10442)
*Aarush Agarwal,Raymond He,Jan Kieseler,Matteo Cremonesi,Shah Rukh Qasim*

Main category: cs.DC

TL;DR: FastGraph是一种针对GPU优化的k近邻算法，专门用于加速低维空间（2-10维）的图构建，为高性能图神经网络提供支持。


<details>
  <summary>Details</summary>
Motivation: 现有的图构建方法在低维空间中效率不足，限制了图神经网络在计算密集型应用中的性能。

Method: 采用GPU驻留的桶分区方法，具有完整梯度流支持和自适应参数调优，显著提升计算和内存效率。

Result: 在维度小于10的情况下，相比FAISS、ANNOY和SCANN等先进库，FastGraph实现了20-40倍的加速，且几乎没有内存开销。

Conclusion: FastGraph显著提升了GNN工作流程的性能，特别适用于高能物理中的粒子聚类、视觉目标跟踪和图聚类等低维计算密集型应用。

Abstract: We introduce FastGraph, a novel GPU-optimized k-nearest neighbor algorithm specifically designed to accelerate graph construction in low-dimensional spaces (2-10 dimensions), critical for high-performance graph neural networks. Our method employs a GPU-resident, bin-partitioned approach with full gradient-flow support and adaptive parameter tuning, significantly enhancing both computational and memory efficiency. Benchmarking demonstrates that FastGraph achieves a 20-40x speedup over state-of-the-art libraries such as FAISS, ANNOY, and SCANN in dimensions less than 10 with virtually no memory overhead. These improvements directly translate into substantial performance gains for GNN-based workflows, particularly benefiting computationally intensive applications in low dimensions such as particle clustering in high-energy physics, visual object tracking, and graph clustering.

</details>


### [15] [Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs](https://arxiv.org/abs/2511.10480)
*Changhai Man,Joongun Park,Hanjiang Wu,Huan Xu,Srinivas Sridharan,Tushar Krishna*

Main category: cs.DC

TL;DR: STAGE是一个框架，能够合成高保真执行轨迹来准确建模LLM工作负载，支持全面的并行化策略，可扩展到32K GPU规模。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可扩展且表达力强的机制来建模分布式工作负载执行，且大型基础设施访问受限，现有平台轨迹难以适应未来更大规模系统配置。

Method: 引入Symbolic Tensor grAph GEnerator(STAGE)框架，通过合成高保真执行轨迹来建模LLM工作负载，支持多种并行化策略。

Result: STAGE展示了其可扩展性，能够合成覆盖32K GPU的高保真LLM轨迹，在计算、内存和通信方面保持张量级精度。

Conclusion: STAGE将公开可用，以促进分布式机器学习系统的进一步研究。

Abstract: Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [16] [History-Aware Trajectory k-Anonymization Using an FPGA-Based Hardware Accelerator for Real-Time Location Services](https://arxiv.org/abs/2511.09688)
*Hiroshi Nakano,Hiroaki Nishi*

Main category: cs.AR

TL;DR: 提出了一种结合历史轨迹搜索和最短路径的FPGA实时轨迹k-匿名化方法，相比之前仅基于最短路径的方法，能更好地保留行为准确性。


<details>
  <summary>Details</summary>
Motivation: 之前的FPGA轨迹匿名化方法仅依赖最短路径计算，无法捕捉真实旅行行为，降低了匿名化数据的实用性。

Method: 开发了新型FPGA硬件架构，集成并行历史轨迹搜索和传统最短路径查找，使用自定义定点计数模块准确权衡历史数据贡献。

Result: 新架构实现超过6,000条记录/秒的实时吞吐量，数据保留率比之前设计提高1.2%，更有效地保留主要干道。

Conclusion: 该成果是在LBS严格延迟约束下实现高保真度、历史感知匿名化的关键进展，能同时保护隐私和保持行为准确性。

Abstract: Our previous work established the feasibility of FPGA-based real-time trajectory anonymization, a critical task for protecting user privacy in modern location-based services (LBS). However, that pioneering approach relied exclusively on shortest-path computations, which can fail to capture re- alistic travel behavior and thus reduce the utility of the anonymized data. To address this limitation, this paper introduces a novel, history-aware trajectory k-anonymization methodology and presents an advanced FPGA-based hardware architecture to implement it. Our proposed architecture uniquely integrates par- allel history-based trajectory searches with conventional shortest- path finding, using a custom fixed-point counting module to ac- curately weigh contributions from historical data. This approach enables the system to prioritize behaviorally common routes over geometrically shorter but less-traveled paths. The FPGA implementation demonstrates that our new architecture achieves a real-time throughput of over 6,000 records/s, improves data retention by up to 1.2% compared to our previous shortest-path- only design, and preserves major arterial roads more effectively. These results signify a key advancement, enabling high-fidelity, history-aware anonymization that preserves both privacy and behavioral accuracy under the strict latency constraints of LBS.

</details>


### [17] [AssertMiner: Module-Level Spec Generation and Assertion Mining using Static Analysis Guided LLMs](https://arxiv.org/abs/2511.10007)
*Hongqin Lyu,Yonghao Wang,Jiaxin Zhou,Zhiteng Chao,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: AssertMiner是一个模块级断言生成框架，利用AST静态信息辅助LLM挖掘断言，解决了现有方法只生成顶层断言而忽略微架构级模块验证需求的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于设计规范的断言生成方法通常只产生顶层断言，忽视了微架构级模块实现细节的验证需求，而这些地方设计错误更频繁发生。

Method: 通过AST结构提取获取模块调用图、I/O表和数据流图，引导LLM生成模块级规范并挖掘模块级断言。

Result: 评估显示AssertMiner在生成高质量模块断言方面优于AssertLLM和Spec2Assertion等方法，集成后可显著提升结构覆盖率和错误检测能力。

Conclusion: AssertMiner能够实现更全面高效的验证过程，解决了模块级断言生成的局限性。

Abstract: Assertion-based verification (ABV) is a key approach to checking whether a logic design complies with its architectural specifications. Existing assertion generation methods based on design specifications typically produce only top-level assertions, overlooking verification needs on the implementation details in the modules at the micro-architectural level, where design errors occur more frequently. To address this limitation, we present AssertMiner, a module-level assertion generation framework that leverages static information generated from abstract syntax tree (AST) to assist LLMs in mining assertions. Specifically, it performs AST-based structural extraction to derive the module call graph, I/O table, and dataflow graph, guiding the LLM to generate module-level specifications and mine module-level assertions. Our evaluation demonstrates that AssertMiner outperforms existing methods such as AssertLLM and Spec2Assertion in generating high-quality assertions for modules. When integrated with these methods, AssertMiner can enhance the structural coverage and significantly improve the error detection capability, enabling a more comprehensive and efficient verification process.

</details>


### [18] [The Role of Advanced Computer Architectures in Accelerating Artificial Intelligence Workloads](https://arxiv.org/abs/2511.10010)
*Shahid Amin,Syed Pervez Hussnain Shah*

Main category: cs.AR

TL;DR: 本文综述了AI与计算机架构的协同演化，分析了GPU、ASIC和FPGA等主流AI加速器架构的设计理念、性能权衡，以及数据流优化、内存层次、稀疏性和量化等核心原则，并展望了存内计算和神经形态计算等新兴技术。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型（特别是深度神经网络）复杂度增加，其巨大的计算需求已推动传统架构达到极限，需要研究专门为AI工作负载设计的架构。

Method: 通过结构化回顾和定量性能数据分析，探索主流AI加速器架构（GPU、ASIC、FPGA）的设计哲学、关键特征和性能权衡，分析数据流优化、内存层次、稀疏性和量化等核心原则。

Result: 通过行业标准基准测试的定量数据，展示了不同AI加速器架构的性能表现，揭示了硬件-软件协同设计对性能和能效的重要性。

Conclusion: AI与计算机架构处于共生关系中，硬件-软件协同设计不再是优化选项，而是未来计算进步的必要条件。

Abstract: The remarkable progress in Artificial Intelligence (AI) is foundation-ally linked to a concurrent revolution in computer architecture. As AI models, particularly Deep Neural Networks (DNNs), have grown in complexity, their massive computational demands have pushed traditional architectures to their limits. This paper provides a structured review of this co-evolution, analyzing the architectural landscape designed to accelerate modern AI workloads. We explore the dominant architectural paradigms Graphics Processing Units (GPUs), Appli-cation-Specific Integrated Circuits (ASICs), and Field-Programmable Gate Ar-rays (FPGAs) by breaking down their design philosophies, key features, and per-formance trade-offs. The core principles essential for performance and energy efficiency, including dataflow optimization, advanced memory hierarchies, spar-sity, and quantization, are analyzed. Furthermore, this paper looks ahead to emerging technologies such as Processing-in-Memory (PIM) and neuromorphic computing, which may redefine future computation. By synthesizing architec-tural principles with quantitative performance data from industry-standard benchmarks, this survey presents a comprehensive picture of the AI accelerator landscape. We conclude that AI and computer architecture are in a symbiotic relationship, where hardware-software co-design is no longer an optimization but a necessity for future progress in computing.

</details>


### [19] [Combined power management and congestion control in High-Speed Ethernet-based Networks for Supercomputers and Data Centers](https://arxiv.org/abs/2511.10159)
*Miguel Sánchez de la Rosa,Francisco J. andújar,Jesus Escudero-Sahuquillo,José L. Sánchez,Francisco J. Alfaro-Cortés*

Main category: cs.AR

TL;DR: 本文探讨数据中心和超级计算机网络的两个关键方面：高负载下的拥塞控制和空闲时的节能管理，以及两者之间的相互作用。


<details>
  <summary>Details</summary>
Motivation: 随着数据中心和超级计算机的普及，网络互联成为系统性能的关键瓶颈。需要先进技术来确保系统在高负载下不降级，同时在空闲时能有效节能。

Method: 研究网络的两个关键方面：拥塞控制（防止高负载时性能下降）和电源管理（空闲时节能），并分析两者之间的相互作用。

Result: 探索了网络拥塞控制和电源管理之间的关系，为优化大规模计算系统的网络性能提供了理论框架。

Conclusion: 网络拥塞控制和电源管理是确保大规模计算系统高效运行的两个关键因素，需要综合考虑两者的相互作用以实现最佳性能。

Abstract: The demand for computer in our daily lives has led to the proliferation of Datacenters that power indispensable many services. On the other hand, computing has become essential for some research for various scientific fields, that require Supercomputers with vast computing capabilities to produce results in reasonable time. The scale and complexity of these systems, compared to our day-to-day devices, are like comparing a cell to a living organism. To make them work properly, we need state-of-the-art technology and engineering, not just raw resources. Interconnecting the different computer nodes that make up a whole is a delicate task, as it can become the bottleneck for the whole infrastructure. In this work, we explore two aspects of the network: how to prevent degradation under heavy use with congestion control, and how to save energy when idle with power management; and how the two may interact.

</details>


### [20] [Beamspace Equalization for mmWave Massive MIMO: Algorithms and VLSI Implementations](https://arxiv.org/abs/2511.10563)
*Seyed Hadi Mirfarshbafan,Christoph Studer*

Main category: cs.AR

TL;DR: 提出了一种基于波束空间的复杂稀疏自适应均衡器(CSPADE)及其VLSI架构，相比天线域均衡可节省高达66%的功耗，在22nm FDSOI工艺中实现了最高吞吐量和更好的能效。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO和毫米波通信在部署时面临基带处理硬件成本和功耗过高的问题，需要利用毫米波信道的稀疏性来降低处理复杂度。

Method: 提出复杂稀疏自适应均衡器(CSPADE)算法，并设计了相应的VLSI架构，包括全并行实现和基于顺序MAC的架构。

Result: 在22nm FDSOI工艺中实现，全并行CSPADE相比天线域均衡节省54%功耗，MAC架构节省66%功耗，实现了现有大规模MIMO数据检测器中最高的吞吐量。

Conclusion: 波束空间处理结合稀疏自适应算法能显著降低大规模MIMO系统的功耗，提出的VLSI架构在吞吐量和能效方面表现优异。

Abstract: Massive multiuser multiple-input multiple-output (MIMO) and millimeter-wave (mmWave) communication are key physical layer technologies in future wireless systems. Their deployment, however, is expected to incur excessive baseband processing hardware cost and power consumption. Beamspace processing leverages the channel sparsity at mmWave frequencies to reduce baseband processing complexity. In this paper, we review existing beamspace data detection algorithms and propose new algorithms as well as corresponding VLSI architectures that reduce data detection power. We present VLSI implementation results for the proposed architectures in a 22nm FDSOI process. Our results demonstrate that a fully-parallelized implementation of the proposed complex sparsity-adaptive equalizer (CSPADE) achieves up to 54% power savings compared to antenna-domain equalization. Furthermore, our fully-parallelized designs achieve the highest reported throughput among existing massive MIMO data detectors, while achieving better energy and area efficiency. We also present a sequential multiply-accumulate (MAC)-based architecture for CSPADE, which enables even higher power savings, i.e., up to 66%, compared to a MAC-based antenna-domain equalizer.

</details>
