<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Static Analysis Under Non-Deterministic Program Assumptions](https://arxiv.org/abs/2602.07324)
*Abdullah H. Rasheed*

Main category: cs.PL

TL;DR: 提出一种接受用户提供程序假设的静态分析方法，这些假设是程序位置局部的，用于抵消静态分析的不精确性，从而扩展应用范围。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析在精确性、可靠性和自动化之间存在权衡，导致其应用场景受限。当不精确性不可接受时，静态分析的使用受到限制。本文旨在通过引入用户提供的程序假设来克服这一限制。

Method: 提出并规范了一种静态分析方法，该方法接受用户提供的程序位置局部假设。分析器非确定性地接受这些假设，产生一个从接受的假设集合到相应分析结果的函数。

Result: 展示了这种函数的实用性，通过两种方式证明它能够在假设搜索空间上实现优化，这在没有指定分析的情况下是不可行的。

Conclusion: 通过允许用户提供局部程序假设，可以显著扩展静态分析的应用范围，使其能够处理更多原本因不精确性而无法应用的情况。

Abstract: Static analyses overwhelmingly trade precision for soundness and automation. For this reason, their use-cases are restricted to situations where imprecision isn't prohibitive. In this paper, we propose and specify a static analysis that accepts user-supplied program assumptions that are local to program locations. Such assumptions can be used to counteract imprecision in static analyses, enabling their use in a much wider variety of applications. These assumptions are taken by the analyzer non-deterministically, resulting in a function from sets of accepted assumptions to the resulting analysis under those assumptions. We also demonstrate the utility of such a function in two ways, both of which showcase how it can enable optimization over a search space of assumptions that is otherwise infeasible without the specified analysis.

</details>


### [2] [RustCompCert: A Verified and Verifying Compiler for a Sequential Subset of Rust](https://arxiv.org/abs/2602.07455)
*Jinhua Wu,Yuting Wang,Liukun Yu,Linglong Meng*

Main category: cs.PL

TL;DR: 开发基于CompCert的端到端验证Rust编译器，提供语义保持和内存安全保证


<details>
  <summary>Details</summary>
Motivation: 现有Rust编译器缺乏形式化验证，无法保证从源代码到汇编的语义保持，也无法确保编译过程本身的内存安全性

Method: 基于CompCert构建端到端验证的Rust编译器，包含借用检查器验证，确保语义保持（源程序行为包含目标程序行为）和内存安全

Result: 提供两个关键保证：1）从Rust到汇编的语义保持，源程序验证的属性可传递到目标代码；2）验证编译确保的内存安全，简化Rust程序验证

Conclusion: 通过构建形式化验证的Rust编译器，可同时获得语义保持和内存安全保证，为Rust程序验证提供更可靠的基础

Abstract: We present our ongoing work on developing an end-to-end verified Rust compiler based on CompCert. It provides two guarantees: one is semantics preservation from Rust to assembly, i.e., the behaviors of source code includes the behaviors of target code, with which the properties verified at the source can be preserved down to the target; the other is memory safety ensured by the verifying compilation -- the borrow checking pass, which can simplify the verification of Rust programs, e.g., by allowing the verification tools focus on the functional correctness.

</details>


### [3] [Series-Parallel-Loop Decompositions of Control-flow Graphs](https://arxiv.org/abs/2602.07627)
*Xuran Cai,Amir Goharshady,S Hitarth,Chun Kit Lam*

Main category: cs.PL

TL;DR: 本文提出了一种新的基于语法的分解框架，能够精确描述结构化程序生成的控制流图（CFG），并基于此改进了寄存器分配和生命周期最优投机性部分冗余消除（LOSPRE）算法，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用树宽和路径宽等图参数来建模CFG的稀疏性，但这些参数只能近似结构化CFG的结构约束。虽然每个结构化CFG的树宽最多为7，但许多树宽≤7的图并不能作为CFG出现，导致现有参数化技术针对的图类比实践中遇到的要广泛得多。

Method: 引入一种新的基于语法的分解框架，能够精确描述结构化程序生成的控制流图类别。该分解直观、反映程序的语法结构，并且与基于树宽的动态规划方法完全兼容。

Result: 使用该框架为两个经典编译器优化问题设计了改进算法：寄存器分配和生命周期最优投机性部分冗余消除（LOSPRE）。广泛的实验评估显示，相比之前的最先进方法，性能有显著提升。

Conclusion: 针对CFG量身定制的分解方法比通用的树宽方法更有效，能够为编译器优化问题带来显著的性能改进，证明了专门针对CFG结构特性的分解框架的价值。

Abstract: Control-flow graphs (CFGs) of structured programs are well known to exhibit strong sparsity properties. Traditionally, this sparsity has been modeled using graph parameters such as treewidth and pathwidth, enabling the development of faster parameterized algorithms for tasks in compiler optimization, model checking, and program analysis. However, these parameters only approximate the structural constraints of CFGs: although every structured CFG has treewidth at most~7, many graphs with treewidth at most~7 cannot arise as CFGs. As a result, existing parameterized techniques are optimized for a substantially broader class of graphs than those encountered in practice.
  In this work, we introduce a new grammar-based decomposition framework that characterizes \emph{exactly} the class of control-flow graphs generated by structured programs. Our decomposition is intuitive, mirrors the syntactic structure of programs, and remains fully compatible with the dynamic-programming paradigm of treewidth-based methods. Using this framework, we design improved algorithms for two classical compiler optimization problems: \emph{Register Allocation} and \emph{Lifetime-Optimal Speculative Partial Redundancy Elimination (LOSPRE)}. Extensive experimental evaluation demonstrates significant performance improvements over previous state-of-the-art approaches, highlighting the benefits of using decompositions tailored specifically to CFGs.

</details>


### [4] [Gillian Debugging: Swinging Through the (Compositional Symbolic Execution) Trees, Extended Version](https://arxiv.org/abs/2602.07742)
*Nat Karmios,Sacha-Élie Ayoun,Philippa Gardner*

Main category: cs.PL

TL;DR: 为符号执行工具开发了一个可视化调试界面，集成到VS Code和Gillian平台，通过用户研究验证其有效性


<details>
  <summary>Details</summary>
Motivation: 组合符号执行工具虽然日益成熟，但其输出调试仍然困难，即使是专家用户也难以理解，需要更好的调试工具来改善用户体验

Method: 开发了一个与Visual Studio Code和Gillian多语言CSE平台集成的调试界面，重点关注可视化、交互性和符号执行树的直观表示，并确保工具无关性以便未来移植到其他符号分析工具

Result: 通过用户研究进行实证评估，结果显示该调试器能有效帮助早期研究人员理解CSE原理，并在Gillian中验证基本数据结构算法

Conclusion: 成功开发了一个可视化、交互式的符号执行调试界面，解决了现有工具调试困难的问题，并通过用户研究验证了其实际效用

Abstract: In recent years, compositional symbolic execution (CSE) tools have been growing in prominence and are becoming more and more applicable to real-world codebases. Still to this day, however, debugging the output of these tools remains difficult, even for specialist users. To address this, we introduce a debugging interface for symbolic execution tools, integrated with Visual Studio Code and the Gillian multi-language CSE platform, with strong focus on visualisation, interactivity, and intuitive representation of symbolic execution trees. We take care in making this interface tool-agnostic, easing its transfer to other symbolic analysis tools in future. We empirically evaluate our work with a user study, the results of which show the debugger's usefulness in helping early researchers understand the principles of CSE and verify fundamental data structure algorithms in Gillian.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Parallel Track Transformers: Enabling Fast GPU Inference with Reduced Synchronization](https://arxiv.org/abs/2602.07306)
*Chong Wang,Nan Du,Tom Gunter,Tao Lei,Kulin Seth,Senyu Tong,Jianyu Wang,Guoli Yin,Xiyou Zhou,Kelvin Zou,Ruoming Pang*

Main category: cs.DC

TL;DR: 提出Parallel Track Transformer架构，通过重构计算减少跨设备同步操作，相比传统张量并行减少16倍同步，在LLM推理服务中提升效率


<details>
  <summary>Details</summary>
Motivation: 传统张量并行在多GPU推理中存在大量跨设备同步操作，导致通信瓶颈和可扩展性下降，需要新的架构来减少同步开销

Method: 提出Parallel Track Transformer架构，重新组织计算结构以最小化跨设备依赖，集成到Tensor-RT-LLM和vLLM两个主流LLM服务框架中

Result: 相比标准张量并行减少16倍同步操作，在Tensor-RT-LLM和vLLM中实现：首token时间减少15-30%，输出token时间减少2-12%，吞吐量提升最高31.90%

Conclusion: PT Transformer通过减少跨设备同步有效解决了LLM大规模推理中的通信瓶颈问题，在保持模型质量的同时显著提升了服务效率

Abstract: Efficient large-scale inference of transformer-based large language models (LLMs) remains a fundamental systems challenge, frequently requiring multi-GPU parallelism to meet stringent latency and throughput targets. Conventional tensor parallelism decomposes matrix operations across devices but introduces substantial inter-GPU synchronization, leading to communication bottlenecks and degraded scalability. We propose the Parallel Track (PT) Transformer, a novel architectural paradigm that restructures computation to minimize cross-device dependencies. PT achieves up to a 16x reduction in synchronization operations relative to standard tensor parallelism, while maintaining competitive model quality in our experiments. We integrate PT into two widely adopted LLM serving stacks-Tensor-RT-LLM and vLLM-and report consistent improvements in serving efficiency, including up to 15-30% reduced time to first token, 2-12% reduced time per output token, and up to 31.90% increased throughput in both settings.

</details>


### [6] [Knowledge Graphs-Driven Intelligence for Distributed Decision Systems](https://arxiv.org/abs/2602.07614)
*Rosario Napoli,Gabriele Morabito,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.DC

TL;DR: 提出基于知识图谱和图嵌入的知识共享范式，通过局部语义表示构建和邻居迭代聚合，实现去中心化智能决策，适用于边缘计算、物联网等动态环境。


<details>
  <summary>Details</summary>
Motivation: 现代分布式决策系统面临数据异构性、动态环境和去中心化协调的挑战，需要一种能够在不依赖集中控制的情况下实现语义一致性和适应性的解决方案。

Method: 采用知识图谱的语义丰富性和图嵌入的表征能力，让各节点本地构建操作上下文的语义表示，通过GraphSAGE进行邻居间的迭代嵌入聚合，形成动态演化的全局语义抽象（知识地图）。

Result: 在分布式资源编排用例中进行广泛实验，模拟不同网络拓扑和节点工作负载，分析节点局部语义漂移。结果表明该分布式知识共享机制能有效维持语义一致性和适应性。

Conclusion: 知识共享范式利用知识图谱和图嵌入实现了去中心化智能，适用于边缘计算、物联网和多智能体系统等复杂动态环境，为分布式决策提供了有效解决方案。

Abstract: Modern distributed decision-making systems face significant challenges arising from data heterogeneity, dynamic environments, and the need for decentralized coordination. This paper introduces the Knowledge Sharing paradigm as an innovative approach that uses the semantic richness of Knowledge Graphs (KGs) and the representational power of Graph Embeddings (GEs) to achieve decentralized intelligence. Our architecture empowers individual nodes to locally construct semantic representations of their operational context, iteratively aggregating embeddings through neighbor-based exchanges using GraphSAGE. This iterative local aggregation process results in a dynamically evolving global semantic abstraction called Knowledge Map, enabling coordinated decision-making without centralized control. To validate our approach, we conduct extensive experiments under a distributed resource orchestration use case. We simulate different network topologies and node workloads, analyzing the local semantic drift of individual nodes. Experimental results confirm that our distributed knowledge-sharing mechanism effectively maintains semantic coherence and adaptability, making it suitable for complex and dynamic environments such as Edge Computing, IoT, and multi-agent systems.

</details>


### [7] [Privacy-Preserving Coding Schemes for Multi-Access Distributed Computing Models](https://arxiv.org/abs/2602.07850)
*Shanuja Sasi*

Main category: cs.DC

TL;DR: 在MADC模型中引入隐私约束，为两种特定连接模型开发私有编码方案，保护reducer分配函数的隐私性


<details>
  <summary>Details</summary>
Motivation: MapReduce等分布式计算框架已成为大规模数据处理的关键，而MADC模型通过解耦mapper和reducer角色进一步提升了性能。然而，现有的MADC模型缺乏隐私保护机制，需要引入隐私约束来保护reducer分配函数的机密性。

Method: 1. 在MADC模型中引入隐私约束；2. 针对两种特定连接模型开发私有编码方案；3. 构建扩展的放置交付数组新家族；4. 推导相应的编码方案以保证每个reducer分配函数的隐私性

Result: 成功开发了能够保护reducer函数隐私的私有编码方案，通过构建扩展的放置交付数组家族，为MADC模型中的隐私保护提供了理论框架和实现方法

Conclusion: 该研究将隐私约束引入MADC模型，通过创新的编码方案解决了分布式计算中的隐私保护问题，为隐私保护的分布式计算系统设计提供了新的理论工具和实践方法

Abstract: Distributed computing frameworks such as MapReduce have become essential for large-scale data processing by decomposing tasks across multiple nodes. The multi-access distributed computing (MADC) model further advances this paradigm by decoupling mapper and reducer roles: dedicated mapper nodes store data and compute intermediate values, while reducer nodes are connected to multiple mappers and aggregate results to compute final outputs. This separation reduces communication bottlenecks without requiring file replication. In this paper, we introduce privacy constraints into MADC and develop private coded schemes for two specific connectivity models. We construct new families of extended placement delivery arrays and derive corresponding coding schemes that guarantee privacy of each reducer's assigned function.

</details>


### [8] [HEAL: Online Incremental Recovery for Leaderless Distributed Systems Across Persistency Models](https://arxiv.org/abs/2602.08257)
*Antonis Psistakis,Burak Ocalan,Fabien Chaix,Ramnatthan Alagappan,Josep Torrellas*

Main category: cs.DC

TL;DR: HEAL：一种用于无主分布式系统的低开销通用恢复方案，能在120毫秒内完成恢复，仅降低吞吐量8.7%


<details>
  <summary>Details</summary>
Motivation: 分布式系统对弹性要求日益提高，需要开发轻量级机制来快速从故障中恢复，同时对系统吞吐量影响最小

Method: 提出HEAL方案，在节点故障时执行优化的在线增量恢复，支持线性化一致性和不同内存持久性模型

Result: 在6节点Intel集群上实现，TAOBench工作负载测试显示：平均恢复时间120毫秒，吞吐量仅降低8.7%；相比传统无主系统恢复方案快3000倍，吞吐影响减半

Conclusion: HEAL是一种高效的无主分布式系统恢复方案，显著优于传统方法和现有基于主节点的增量恢复方案

Abstract: Ensuring resilience in distributed systems has become an acute concern. In today's environment, it is crucial to develop light-weight mechanisms that recover a distributed system from faults quickly and with only a small impact on the live-system throughput. To address this need, this paper proposes a new low-overhead, general recovery scheme for modern non-transactional leaderless distributed systems. We call our scheme HEAL. On a node failure, HEAL performs an optimized online incremental recovery. This paper presents HEAL's algorithms for settings with Linearizable consistency and different memory persistency models. We implement HEAL on a 6-node Intel cluster. Our experiments running TAOBench workloads show that HEAL is very effective. HEAL recovers the cluster in 120 milliseconds on average, while reducing the throughput of the running workload by an average of 8.7%. In contrast, a conventional recovery scheme for leaderless systems needs 360 seconds to recover, reducing the throughput of the system by 16.2%. Finally, compared to an incremental recovery scheme for a state-of-the-art leader-based system, HEAL reduces the average recovery latency by 20.7x and the throughput degradation by 62.4%.

</details>


### [9] [Towards CXL Resilience to CPU Failures](https://arxiv.org/abs/2602.08271)
*Antonis Psistakis,Burak Ocalan,Chloe Alverti,Fabien Chaix,Ramnatthan Alagappan,Josep Torrellas*

Main category: cs.DC

TL;DR: ReCXL扩展CXL 3.0规范以实现节点故障容错，通过硬件日志单元复制缓存行更新，确保分布式共享内存系统的故障恢复能力。


<details>
  <summary>Details</summary>
Motivation: CXL 3.0及更高版本支持集群节点间硬件缓存一致性的数据共享，但存在节点故障时缓存中脏数据丢失导致应用状态损坏的问题。当前CXL规范未考虑处理器故障，且故障时仅隔离组件而不恢复应用一致性状态。

Method: ReCXL扩展CXL规范，在写操作的一致性事务中添加消息，将更新传播到少量副本节点。副本节点在硬件日志单元中保存更新，定期将日志转储到内存。故障恢复时使用日志单元中的日志将目录和内存恢复到正确状态。

Result: 评估显示ReCXL能够实现容错执行，相比无容错支持平台仅带来30%的性能开销。

Conclusion: ReCXL成功解决了CXL分布式共享内存系统的节点故障容错问题，通过硬件辅助的日志复制机制实现了高效的应用状态恢复。

Abstract: Compute Express Link (CXL) 3.0 and beyond allows the compute nodes of a cluster to share data with hardware cache coherence and at the granularity of a cache line. This enables shared-memory semantics for distributed computing, but introduces new resilience challenges: a node failure leads to the loss of the dirty data in its caches, corrupting application state. Unfortunately, the CXL specification does not consider processor failures. Moreover, when a component fails, the specification tries to isolate it and continue application execution; there is no attempt to bring the application to a consistent state. To address these limitations, this paper extends the CXL specification to be resilient to node failures, and to correctly recover the application after node failures. We call the system ReCXL. To handle the failure of nodes, ReCXL augments the coherence transaction of a write with messages that propagate the update to a small set of other nodes (i.e., Replicas). Replicas save the update in a hardware Logging Unit. Such replication ensures resilience to node failures. Then, at regular intervals, the Logging Units dump the updates to memory. Recovery involves using the logs in the Logging Units to bring the directory and memory to a correct state. Our evaluation shows that ReCXL enables fault-tolerant execution with only a 30% slowdown over the same platform with no fault-tolerance support.

</details>


### [10] [PARD: Enhancing Goodput for Inference Pipeline via Proactive Request Dropping](https://arxiv.org/abs/2602.08747)
*Zhixin Zhao,Yitao Hu,Simin Chen,Mingfang Ji,Wei Yang,Yuhao Zhang,Laiping Zhao,Wenxin Li,Xiulong Liu,Wenyu Qu,Hao Wang*

Main category: cs.DC

TL;DR: PARD是一个用于深度学习推理管道的主动请求丢弃系统，通过提前丢弃部分请求来提高整体吞吐量，相比现有方法实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有推理管道系统采用被动丢弃策略，只在必要时丢弃请求，导致丢弃决策不及时或丢弃错误请求，无法维持高吞吐量。需要更智能的请求管理机制来优化系统性能。

Method: 设计PARD系统，包含两个核心组件：1) 主动丢弃方法，利用推理管道的运行时信息决定何时丢弃请求；2) 自适应请求优先级机制，基于剩余延迟预算和工作负载强度选择具体要丢弃的请求。

Result: 在64个GPU集群上的真实工作负载评估显示，PARD相比现有技术实现了16%-176%的吞吐量提升，同时将丢弃率和浪费的计算资源分别降低了1.6-17倍和1.5-62倍。

Conclusion: 主动请求丢弃策略能够显著提升深度学习推理管道的吞吐量，PARD系统通过智能的丢弃决策机制有效解决了传统被动丢弃方法的问题，为高延迟要求的推理任务提供了更优的解决方案。

Abstract: Modern deep neural network (DNN) applications integrate multiple DNN models into inference pipelines with stringent latency requirements for customized tasks. To mitigate extensive request timeouts caused by accumulation, systems for inference pipelines commonly drop a subset of requests so the remaining ones can satisfy latency constraints. Since it is commonly believed that request dropping adversely affects goodput, existing systems only drop requests when they have to, which we call reactive dropping. However, this reactive policy can not maintain high goodput, as it neither makes timely dropping decisions nor identifies the proper set of requests to drop, leading to issues of dropping requests too late or dropping the wrong set of requests.
  We propose that the inference system should proactively drop certain requests in advance to enhance the goodput across the entire workload. To achieve this, we design an inference system PARD. It enhances goodput with timely and precise dropping decisions by integrating a proactive dropping method that decides when to drop requests using runtime information of the inference pipeline, and an adaptive request priority mechanism that selects which specific requests to drop based on remaining latency budgets and workload intensity. Evaluation on a cluster of 64 GPUs over real-world workloads shows that PARD achieves $16\%$-$176\%$ higher goodput than the state of the art while reducing the drop rate and wasted computation resources by $1.6\times$-$17\times$ and $1.5\times$-$62\times$ respectively.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [Investigating Energy Bounds of Analog Compute-in-Memory with Local Normalization](https://arxiv.org/abs/2602.08081)
*Brian Rojkov,Shubham Ranjan,Derek Wright,Manoj Sachdev*

Main category: cs.AR

TL;DR: 论文提出GR-MAC架构，通过局部归一化解决模拟存内计算中浮点格式动态范围与硬件分辨率不匹配的问题，实现能效提升。


<details>
  <summary>Details</summary>
Motivation: 现代边缘AI工作负载需要最大能效，推动模拟存内计算架构发展。同时，大语言模型采用低比特浮点格式优先考虑动态范围。传统直接累加CIM通过归一化到共享扩展定点尺度来适应浮点，导致硬件分辨率由输入动态范围而非精度决定，且ADC能耗占主导。

Method: 引入GR-MAC架构，为每个输入、权重和MAC输出进行局部归一化。归一化开销由低功耗数字逻辑处理，使计算密集的MAC操作保持在能效高的低精度模拟域。

Result: 能量建模显示，在MAC中添加增益范围级可使输入动态范围增加4位而不增加能耗（35 dB SQNR标准）。ADC分辨率要求对输入分布假设不变，与传统下界相比减少1.5位。

Conclusion: GR-MAC为现代AI工作负载解锁模拟存内计算有利的能量缩放趋势提供了途径。

Abstract: Modern edge AI workloads demand maximum energy efficiency, motivating the pursuit of analog Compute-in-Memory (CIM) architectures. Simultaneously, the popularity of Large-Language-Models (LLMs) drives the adoption of low-bit floating-point formats which prioritize dynamic range. However, the conventional direct-accumulation CIM accommodates floating-points by normalizing them to a shared widened fixed-point scale. Consequently, hardware resolution is dictated by the input's dynamic range rather than its precision, and energy consumption is dominated by the ADC. We address this limitation by introducing local normalization for each input, weight, and multiply-accumulate (MAC) output via a Gain-Ranging MAC (GR-MAC). Normalization overhead is handled by low-power digital logic, enabling the computationally expensive MAC operation to remain in the energy-efficient low-precision analog regime. Energy modelling shows that the addition of a gain-ranging Stage to the MAC enables a 4-bit increase in input dynamic range without increased energy consumption at a 35 dB SQNR standard. Additionally, the ADC resolution requirement becomes invariant to input distribution assumptions, allowing construction of an upper bound with a 1.5-bit reduction compared to the conventional lower bound. These results establish a pathway towards unlocking favourable energy scaling trends of analog CIM for modern AI workloads.

</details>


### [12] [Antiferromagnetic Tunnel Junctions (AFMTJs) for In-Memory Computing: Modeling and Case Study](https://arxiv.org/abs/2602.08323)
*Yousuf Choudhary,Tosiron Adegbija*

Main category: cs.AR

TL;DR: 本文提出了首个端到端反铁磁隧道结（AFMTJ）仿真框架，将多子晶格LLG动力学与电路级建模结合，证明AFMTJ相比传统MTJ具有显著更低的写入延迟和能耗，在存内计算架构中实现17.5倍加速和近20倍节能。


<details>
  <summary>Details</summary>
Motivation: 反铁磁隧道结（AFMTJs）通过超快子晶格动力学实现皮秒级切换和飞焦级写入，但缺乏端到端的仿真框架来评估其在计算系统中的实际性能潜力。

Method: 开发了首个端到端AFMTJ仿真框架，集成多子晶格Landau-Lifshitz-Gilbert（LLG）动力学与电路级建模，使用SPICE进行仿真，并将AFMTJ集成到存内计算架构中进行评估。

Result: AFMTJ相比传统MTJ实现约8倍写入延迟降低和约9倍写入能耗降低；在存内计算架构中，相比CPU基准实现17.5倍平均加速和近20倍节能，显著优于MTJ基存内计算。

Conclusion: AFMTJ是构建可扩展、低功耗计算系统的有前景的基础元件，其超快切换和低能耗特性为下一代计算架构提供了重要技术路径。

Abstract: Antiferromagnetic Tunnel Junctions (AFMTJs) enable picosecond switching and femtojoule writes through ultrafast sublattice dynamics. We present the first end-to-end AFMTJ simulation framework integrating multi-sublattice Landau-Lifshitz-Gilbert (LLG) dynamics with circuit-level modeling. SPICE-based simulations show that AFMTJs achieve ~8x lower write latency and ~9x lower write energy than conventional MTJs. When integrated into an in-memory computing architecture, AFMTJs deliver 17.5x average speedup and nearly 20x energy savings versus a CPU baseline-significantly outperforming MTJ-based IMC. These results establish AFMTJs as a compelling primitive for scalable, low-power computing.

</details>


### [13] [karl. -- A Research Vehicle for Automated and Connected Driving](https://arxiv.org/abs/2602.08842)
*Jean-Pierre Busch,Lukas Ostendorf,Guido Linden,Lennart Reiher,Till Beemelmanns,Bastian Lampe,Timo Woopen,Lutz Eckstein*

Main category: cs.AR

TL;DR: 本文介绍了karl.——一款用于自动驾驶和网联驾驶研究的新型研究车辆平台，旨在为缺乏L4级研究车辆的研究机构提供灵活强大的研究工具。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶从封闭测试转向商业部署，虽然仿真技术广泛应用，但真实世界的集成测试仍然必不可少。目前只有少数大型企业拥有L4级研究车辆，限制了其他研究机构的独立研究能力。

Method: 设计并开发了karl.研究车辆平台，详细阐述了其设计理念、技术选择和实现细节，旨在创建一个灵活强大的自动驾驶和网联驾驶研究平台。

Result: 成功开发了karl.研究车辆，该平台能够支持真实世界条件下的软硬件测试验证、数据收集发布，以及未来用例的早期演示。

Conclusion: karl.研究车辆为缺乏L4级研究车辆的研究机构提供了重要的研究工具，通过分享设计细节帮助缩小研究差距，促进自动驾驶和网联驾驶领域的独立研究。

Abstract: As highly automated driving is transitioning from single-vehicle closed-access testing to commercial deployments of public ride-hailing in selected areas (e.g., Waymo), automated driving and connected cooperative intelligent transport systems (C-ITS) remain active fields of research. Even though simulation is omnipresent in the development and validation life cycle of automated and connected driving technology, the complex nature of public road traffic and software that masters it still requires real-world integration and testing with actual vehicles. Dedicated vehicles for research and development allow testing and validation of software and hardware components under real-world conditions early on. They also enable collecting and publishing real-world datasets that let others conduct research without vehicle access, and support early demonstration of futuristic use cases. In this paper, we present karl., our new research vehicle for automated and connected driving. Apart from major corporations, few institutions worldwide have access to their own L4-capable research vehicles, restricting their ability to carry out independent research. This paper aims to help bridge that gap by sharing the reasoning, design choices, and technical details that went into making karl. a flexible and powerful platform for research, engineering, and validation in the context of automated and connected driving. More impressions of karl. are available at https://karl.ac.

</details>
