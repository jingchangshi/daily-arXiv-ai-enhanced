{"id": "2511.09766", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09766", "abs": "https://arxiv.org/abs/2511.09766", "authors": ["Michael Dang'ana", "Yuqiu Zhang", "Hans-Arno Jacobsen"], "title": "Ksurf-Drone: Attention Kalman Filter for Contextual Bandit Optimization in Cloud Resource Allocation", "comment": "14 pages, 22 figures, 2 tables", "summary": "Resource orchestration and configuration parameter search are key concerns for container-based infrastructure in cloud data centers. Large configuration search space and cloud uncertainties are often mitigated using contextual bandit techniques for resource orchestration including the state-of-the-art Drone orchestrator. Complexity in the cloud provider environment due to varying numbers of virtual machines introduces variability in workloads and resource metrics, making orchestration decisions less accurate due to increased nonlinearity and noise. Ksurf, a state-of-the-art variance-minimizing estimator method ideal for highly variable cloud data, enables optimal resource estimation under conditions of high cloud variability.\n  This work evaluates the performance of Ksurf on estimation-based resource orchestration tasks involving highly variable workloads when employed as a contextual multi-armed bandit objective function model for cloud scenarios using Drone. Ksurf enables significantly lower latency variance of $41\\%$ at p95 and $47\\%$ at p99, demonstrates a $4\\%$ reduction in CPU usage and 7 MB reduction in master node memory usage on Kubernetes, resulting in a $7\\%$ cost savings in average worker pod count on VarBench Kubernetes benchmark."}
{"id": "2511.09776", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.09776", "abs": "https://arxiv.org/abs/2511.09776", "authors": ["Ramesh Adhikari", "Costas Busch", "Pavan Poudel"], "title": "A Poly-Log Approximation for Transaction Scheduling in Fog-Cloud Computing and Beyond", "comment": "14 pages, 3 figures, accepted for the proceedings at The 27th International Symposium on Stabilization, Safety, and Security of Distributed Systems", "summary": "Transaction scheduling is crucial to efficiently allocate shared resources in a conflict-free manner in distributed systems. We investigate the efficient scheduling of transactions in a network of fog-cloud computing model, where transactions and their associated shared objects can move within the network. The schedule may require objects to move to transaction nodes, or the transactions to move to the object nodes. Moreover, the schedule may determine intermediate nodes where both objects and transactions meet. Our goal is to minimize the total combined cost of the schedule. We focus on networks of constant doubling dimension, which appear frequently in practice. We consider a batch problem where an arbitrary set of nodes has transactions that need to be scheduled. First, we consider a single shared object required by all the transactions and present a scheduling algorithm that gives an $O(\\log n \\cdot \\log D)$ approximation of the optimal schedule, where $n$ is the number of nodes and $D$ is the diameter of the network. Later, we consider transactions accessing multiple shared objects (at most $k$ objects per transaction) and provide a scheduling algorithm that gives an $O(k \\cdot \\log n \\cdot \\log D)$ approximation. We also provide a fully distributed version of the scheduling algorithms where the nodes do not need global knowledge of transactions."}
{"id": "2511.09837", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.09837", "abs": "https://arxiv.org/abs/2511.09837", "authors": ["Lu Zhao", "Rong Shi", "Shaoqing Zhang", "Shangchao Su", "Ziqing Yin", "Zhiyan Cui", "Hongfeng Sun", "Baoguo He", "Yueqiang Chen", "Liang Dong", "Xiyuan Li", "Lingbin Wang", "Lijun Ma", "Qiang Huang", "Ting Liu", "Chong Wang", "Can Wei"], "title": "MoFa: A Unified Performance Modeling Framework for LLM Pretraining", "comment": null, "summary": "The exponential growth in LLM scales, with parameters soaring from billions to trillions, has necessitated distributed pretraining across large clusters comprising thousands to tens of thousands of devices. While hybrid parallelization strategies enable such pretraining, the vast combinatorial strategy space introduces significant optimization challenges. Traditional manual tuning methods incur prohibitive trial-and-error costs, and existing performance modeling approaches exhibit critical limitations: they fail to comprehensively account for prevalent optimization features and ignore the substantial overhead imposed by essential fault tolerance mechanisms like checkpoint recovery in long-duration pretraining. To address these gaps, we propose MoFa, a novel pretraining performance modeling framework that unifies multi-dimensional optimization features and fault tolerance. MoFa incorporates an enhanced cost model to accurately capture the effects of key optimizations and integrates a fault tolerance model based on historical cluster reliability data. Besides, a MoFa-based tuning system is developed to explore optimal pretraining performance and potential bottlenecks in various scenarios. Extensive modeling evaluations demonstrate that MoFa can achieve high prediction accuracy across various scenarios. In addition, through comprehensive tuning experiments, our framework systematically reveals the key factors influencing pretraining performance under different configurations, which provides solid a priori guidance for LLM pretraining system design and deployment."}
{"id": "2511.09861", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.09861", "abs": "https://arxiv.org/abs/2511.09861", "authors": ["Marco Kurzynski", "Shaizeen Aga", "Di Wu"], "title": "Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs", "comment": null, "summary": "GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, such as cutting-edge large language models (LLMs). We analyze the performance of a single-node multi-GPU system running LLM training, and observe that the kernel-level performance variation is highly correlated with concurrent computation communication (C3), a technique to overlap computation and communication across GPUs for performance gains. We then take a further step to reason that thermally induced straggling coupling with C3 impacts performance variation, coined as the Lit Silicon effect. Lit Silicon describes that in a multi-GPU node, thermal imbalance across GPUs introduces node-level straggler GPUs, which in turn slow down the leader GPUs. Lit Silicon leads to node-level performance variation and inefficiency, impacting the entire datacenter from the bottom up. We propose analytical performance and power models for Lit Silicon, to understand the potential system-level gains. We further design simple detection and mitigation techniques to effectively address the Lit Silicon problem, and evaluate three different power management solutions, including power optimization under GPU thermal design power, performance optimization under node-level GPU power capping, and performance optimization under node-level CPU power sloshing. We conduct experiments on two workloads on two AMD InstinctTM MI300X GPU systems under two LLM training frameworks, and observe up to 6% performance and 4% power improvements, potentially saving hundreds of millions of dollars in datacenters. Our solution is almost free lunch and can be effortlessly adopted in datacenters as a new node-level power management layer."}
{"id": "2511.09987", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.09987", "abs": "https://arxiv.org/abs/2511.09987", "authors": ["Shiv Sundram", "Akhilesh Balasingam", "Nathan Zhang", "Kunle Olukotun", "Fredrik Kjolstad"], "title": "Cyclotron: Compilation of Recurrences to Distributed and Systolic Architectures", "comment": null, "summary": "We present Cyclotron, a framework and compiler for using recurrence equations to express streaming dataflow algorithms, which then get portably compiled to distributed topologies of interlinked processors. Our framework provides an input language of recurrences over logical tensors, which then gets lowered into an intermediate language of recurrences over logical iteration spaces, and finally into programs of send, receive, and computation operations specific to each individual processor. In Cyclotron's IR, programs are optimized such that external memory interactions are confined to the boundaries of the iteration space. Within inner iteration spaces, all data accesses become local: data accesses target values residing in local fast memory or on neighboring processing units, avoiding costly memory movement. We provide a scheduling language allowing users to define how data gets streamed and broadcasted between processors, enabling pipelined execution of computation kernels over distributed topologies of processing elements. We demonstrate the portability of our approach by compiling our IR to a reconfigurable simulator of systolic arrays and chiplet style distributed hardware, as well as to distributed-memory CPU clusters. In the simulated reconfigurable setting, we use our compiler for hardware design space exploration in which link costs and latencies can be specified. In the distributed CPU setting, we show how to use recurrences and our scheduling language to express various matrix multiplication routines (Cannon, SUMMA, PUMMA, weight stationary) and solvers (Triangular solve and Cholesky). For matrix multiplication and the triangular solve, we generate distributed implementations competitive with ScaLAPACK."}
{"id": "2511.09688", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.09688", "abs": "https://arxiv.org/abs/2511.09688", "authors": ["Hiroshi Nakano", "Hiroaki Nishi"], "title": "History-Aware Trajectory k-Anonymization Using an FPGA-Based Hardware Accelerator for Real-Time Location Services", "comment": null, "summary": "Our previous work established the feasibility of FPGA-based real-time trajectory anonymization, a critical task for protecting user privacy in modern location-based services (LBS). However, that pioneering approach relied exclusively on shortest-path computations, which can fail to capture re- alistic travel behavior and thus reduce the utility of the anonymized data. To address this limitation, this paper introduces a novel, history-aware trajectory k-anonymization methodology and presents an advanced FPGA-based hardware architecture to implement it. Our proposed architecture uniquely integrates par- allel history-based trajectory searches with conventional shortest- path finding, using a custom fixed-point counting module to ac- curately weigh contributions from historical data. This approach enables the system to prioritize behaviorally common routes over geometrically shorter but less-traveled paths. The FPGA implementation demonstrates that our new architecture achieves a real-time throughput of over 6,000 records/s, improves data retention by up to 1.2% compared to our previous shortest-path- only design, and preserves major arterial roads more effectively. These results signify a key advancement, enabling high-fidelity, history-aware anonymization that preserves both privacy and behavioral accuracy under the strict latency constraints of LBS."}
{"id": "2511.09956", "categories": ["cs.DC", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.09956", "abs": "https://arxiv.org/abs/2511.09956", "authors": ["Mani Tofigh", "Edward Guo", "Weiwei Jia", "Xiaoning Ding", "Jianchen Shan"], "title": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction", "comment": null, "summary": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs."}
{"id": "2511.10343", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.10343", "abs": "https://arxiv.org/abs/2511.10343", "authors": ["Alistair O'Brien", "Didier Rémy", "Gabriel Scherer"], "title": "Omnidirectional type inference for ML: principality any way", "comment": "39 pages + appendices", "summary": "The Damas-Hindley-Milner (ML) type system owes its success to principality, the property that every well-typed expression has a unique most general type. This makes inference predictable and efficient. Unfortunately, many extensions of ML (GADTs, higher-rank polymorphism, and static overloading) endanger princpality by introducing _fragile_ constructs that resist principal inference. Existing approaches recover principality through directional inference algorithms, which propagate _known_ type information in a fixed (or static) order (e.g. as in bidirectional typing) to disambiguate such constructs. However, the rigidity of a static inference order often causes otherwise well-typed programs to be rejected.\n  We propose _omnidirectional_ type inference, where type information flows in a dynamic order. Typing constraints may be solved in any order, suspending when progress requires known type information and resuming once it becomes available, using _suspended match constraints_. This approach is straightforward for simply typed systems, but extending it to ML is challenging due to let-generalization. Existing ML inference algorithms type let-bindings (let x = e1 in e2) in a fixed order: type e1, generalize its type, and then type e2. To overcome this, we introduce _incremental instantiation_, allowing partially solved type schemes containing suspended constraints to be instantiated, with a mechanism to incrementally update instances as the scheme is refined.\n  Omnidirectionality provides a general framework for restoring principality in the presence of fragile features. We demonstrate its versatility on two fundamentally different features of OCaml: static overloading of record labels and datatype constructors and semi-explicit first-class polymorphism. In both cases, we obtain a principal type inference algorithm that is more expressive than OCaml's current typechecker."}
{"id": "2511.10007", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.10007", "abs": "https://arxiv.org/abs/2511.10007", "authors": ["Hongqin Lyu", "Yonghao Wang", "Jiaxin Zhou", "Zhiteng Chao", "Tiancheng Wang", "Huawei Li"], "title": "AssertMiner: Module-Level Spec Generation and Assertion Mining using Static Analysis Guided LLMs", "comment": "6 pages, 8 figures", "summary": "Assertion-based verification (ABV) is a key approach to checking whether a logic design complies with its architectural specifications. Existing assertion generation methods based on design specifications typically produce only top-level assertions, overlooking verification needs on the implementation details in the modules at the micro-architectural level, where design errors occur more frequently. To address this limitation, we present AssertMiner, a module-level assertion generation framework that leverages static information generated from abstract syntax tree (AST) to assist LLMs in mining assertions. Specifically, it performs AST-based structural extraction to derive the module call graph, I/O table, and dataflow graph, guiding the LLM to generate module-level specifications and mine module-level assertions. Our evaluation demonstrates that AssertMiner outperforms existing methods such as AssertLLM and Spec2Assertion in generating high-quality assertions for modules. When integrated with these methods, AssertMiner can enhance the structural coverage and significantly improve the error detection capability, enabling a more comprehensive and efficient verification process."}
{"id": "2511.10146", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.10146", "abs": "https://arxiv.org/abs/2511.10146", "authors": ["Jaime Sebastian Burbano", "Arnova Abdullah", "Eldiyar Zhantileuov", "Mohan Liyanage", "Rolf Schuster"], "title": "Dynamic Edge Server Selection in Time-Varying Environments: A Reliability-Aware Predictive Approach", "comment": null, "summary": "Latency-sensitive embedded applications increasingly rely on edge computing, yet dynamic network congestion in multi-server architectures challenges proper edge server selection. This paper proposes a lightweight server-selection method for edge applications that fuses latency prediction with adaptive reliability and hysteresis-based handover. Using passive measurements (arrival rate, utilization, payload size) and an exponentially modulated rational delay model, the proposed Moderate Handover (MO-HAN) method computes a score that balances predicted latency and reliability to ensure handovers occur only when the expected gain is meaningful and maintain reduced end-to-end latency. Results show that MO-HAN consistently outperforms static and fair-distribution baselines by lowering mean and tail latencies, while reducing handovers by nearly 50% compared to pure opportunistic selection. These gains arise without intrusive instrumentation or heavy learning infrastructure, making MO-HAN practical for resource-constrained embedded devices."}
{"id": "2511.10361", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.10361", "abs": "https://arxiv.org/abs/2511.10361", "authors": ["Rodrigo Mesquita", "Bernardo Toninho"], "title": "Lazy Linearity for a Core Functional Language", "comment": "Extended version of POPL 2026 paper", "summary": "Traditionally, in linearly typed languages, consuming a linear resource is synonymous with its syntactic occurrence in the program. However, under the lens of non-strict evaluation, linearity can be further understood semantically, where a syntactic occurrence of a resource does not necessarily entail using that resource when the program is executed. While this distinction has been largely unexplored, it turns out to be inescapable in Haskell's optimising compiler, which heavily rewrites the source program in ways that break syntactic linearity but preserve the program's semantics. We introduce Linear Core, a novel system which accepts the lazy semantics of linearity statically and is suitable for lazy languages such as the Core intermediate language of the Glasgow Haskell Compiler. We prove that Linear Core is sound, guaranteeing linear resource usage, and that multiple optimising transformations preserve linearity in Linear Core while failing to do so in Core. We have implemented Linear Core as a compiler plugin to validate the system against linearity-heavy libraries, including linear-base."}
{"id": "2511.10010", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10010", "abs": "https://arxiv.org/abs/2511.10010", "authors": ["Shahid Amin", "Syed Pervez Hussnain Shah"], "title": "The Role of Advanced Computer Architectures in Accelerating Artificial Intelligence Workloads", "comment": "16 Pages, 2 Figures", "summary": "The remarkable progress in Artificial Intelligence (AI) is foundation-ally linked to a concurrent revolution in computer architecture. As AI models, particularly Deep Neural Networks (DNNs), have grown in complexity, their massive computational demands have pushed traditional architectures to their limits. This paper provides a structured review of this co-evolution, analyzing the architectural landscape designed to accelerate modern AI workloads. We explore the dominant architectural paradigms Graphics Processing Units (GPUs), Appli-cation-Specific Integrated Circuits (ASICs), and Field-Programmable Gate Ar-rays (FPGAs) by breaking down their design philosophies, key features, and per-formance trade-offs. The core principles essential for performance and energy efficiency, including dataflow optimization, advanced memory hierarchies, spar-sity, and quantization, are analyzed. Furthermore, this paper looks ahead to emerging technologies such as Processing-in-Memory (PIM) and neuromorphic computing, which may redefine future computation. By synthesizing architec-tural principles with quantitative performance data from industry-standard benchmarks, this survey presents a comprehensive picture of the AI accelerator landscape. We conclude that AI and computer architecture are in a symbiotic relationship, where hardware-software co-design is no longer an optimization but a necessity for future progress in computing."}
{"id": "2511.10180", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.10180", "abs": "https://arxiv.org/abs/2511.10180", "authors": ["Tao Tang", "Youfu Jiang", "Yingbo Cui", "Jianbin Fang", "Peng Zhang", "Lin Peng", "Chun Huang"], "title": "Selection of Supervised Learning-based Sparse Matrix Reordering Algorithms", "comment": "14pages", "summary": "Sparse matrix ordering is a vital optimization technique often employed for solving large-scale sparse matrices. Its goal is to minimize the matrix bandwidth by reorganizing its rows and columns, thus enhancing efficiency. Conventional methods for algorithm selection usually depend on brute-force search or empirical knowledge, lacking the ability to adjust to diverse sparse matrix structures.As a result, we have introduced a supervised learning-based model for choosing sparse matrix reordering algorithms. This model grasps the correlation between matrix characteristics and commonly utilized reordering algorithms, facilitating the automated and intelligent selection of the suitable sparse matrix reordering algorithm. Experiments conducted on the Florida sparse matrix dataset reveal that our model can accurately predict the optimal reordering algorithm for various matrices, leading to a 55.37% reduction in solution time compared to solely using the AMD reordering algorithm, with an average speedup ratio of 1.45."}
{"id": "2511.10374", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.10374", "abs": "https://arxiv.org/abs/2511.10374", "authors": ["Somashekaracharya G Bhaskaracharya", "Aravind Acharya", "Bastian Hagedorn", "Vinod Grover"], "title": "Modeling Layout Abstractions Using Integer Set Relations", "comment": null, "summary": "Modern deep learning compilers rely on layout abstractions to manage the complex mapping between logical tensor structures and physical memory arrangements. CuTe layouts and Triton linear layouts are widely adopted industry standards. However, these layout systems operate independently with distinct mathematical underpinnings, preventing unified formal analysis and cross-system reasoning. We bridge this gap by introducing a novel approach that leverages the Integer Set Library (ISL) to create a unified mathematical representation for both layout systems through integer set relations, thereby enabling rigorous formal analysis, correctness verification, and the foundation for future cross-system optimization strategies. Our approach models CuTe layouts through integer set relations that encode the transformation from multi-dimensional coordinates to linear indices using stride-based calculations, including sophisticated swizzle operations that perform bit-level manipulations for enhanced memory access patterns. For Triton linear layouts, we construct integer set relations that model the binary vector space transformations where arithmetic operations follow finite field F_2 rules. We implement a complete suite of layout manipulation algorithms for composition, inversion, complement using built-in operations in ISL to ensure mathematical correctness and preserve layout semantics. Experimental evaluation shows that the system handles the full spectrum of layout complexity, from elementary identity transformations to sophisticated multi-dimensional tensor arrangements with complex stride configurations and swizzle patterns, validating the mathematical modeling approach across different layout paradigms."}
{"id": "2511.10159", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.10159", "abs": "https://arxiv.org/abs/2511.10159", "authors": ["Miguel Sánchez de la Rosa", "Francisco J. andújar", "Jesus Escudero-Sahuquillo", "José L. Sánchez", "Francisco J. Alfaro-Cortés"], "title": "Combined power management and congestion control in High-Speed Ethernet-based Networks for Supercomputers and Data Centers", "comment": "Early version of journal paper", "summary": "The demand for computer in our daily lives has led to the proliferation of Datacenters that power indispensable many services. On the other hand, computing has become essential for some research for various scientific fields, that require Supercomputers with vast computing capabilities to produce results in reasonable time. The scale and complexity of these systems, compared to our day-to-day devices, are like comparing a cell to a living organism. To make them work properly, we need state-of-the-art technology and engineering, not just raw resources. Interconnecting the different computer nodes that make up a whole is a delicate task, as it can become the bottleneck for the whole infrastructure. In this work, we explore two aspects of the network: how to prevent degradation under heavy use with congestion control, and how to save energy when idle with power management; and how the two may interact."}
{"id": "2511.10258", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10258", "abs": "https://arxiv.org/abs/2511.10258", "authors": ["Leszek Sliwko", "Vladimir Getov"], "title": "Workload Schedulers -- Genesis, Algorithms and Differences", "comment": null, "summary": "This paper presents a novel approach to categorization of modern workload schedulers. We provide descriptions of three classes of schedulers: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers and Big Data Schedulers. We describe their evolution from early adoptions to modern implementations, considering both the use and features of algorithms. In summary, we discuss differences between all presented classes of schedulers and discuss their chronological development. In conclusion we highlight similarities in the focus of scheduling strategies design, applicable to both local and distributed systems."}
{"id": "2511.10565", "categories": ["cs.PL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.10565", "abs": "https://arxiv.org/abs/2511.10565", "authors": ["Rahul Krishnan", "Ashley Samuelson", "Emily Yao", "Ethan Cecchetti"], "title": "zkStruDul: Programming zkSNARKs with Structural Duality", "comment": null, "summary": "Non-Interactive Zero Knowledge (NIZK) proofs, such as zkSNARKS, let one prove knowledge of private data without revealing it or interacting with a verifier. While existing tooling focuses on specifying the predicate to be proven, real-world applications optimize predicate definitions to minimize proof generation overhead, but must correspondingly transform predicate inputs. Implementing these two steps separately duplicates logic that must precisely match to avoid catastrophic security flaws. We address this shortcoming with zkStruDul, a language that unifies input transformations and predicate definitions into a single combined abstraction from which a compiler can project both procedures, eliminating duplicate code and problematic mismatches. zkStruDul provides a high-level abstraction to layer on top of existing NIZK technology and supports important features like recursive proofs. We provide a source-level semantics and prove its behavior is identical to the projected semantics, allowing straightforward standard reasoning."}
{"id": "2511.10563", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.10563", "abs": "https://arxiv.org/abs/2511.10563", "authors": ["Seyed Hadi Mirfarshbafan", "Christoph Studer"], "title": "Beamspace Equalization for mmWave Massive MIMO: Algorithms and VLSI Implementations", "comment": "14 pages", "summary": "Massive multiuser multiple-input multiple-output (MIMO) and millimeter-wave (mmWave) communication are key physical layer technologies in future wireless systems. Their deployment, however, is expected to incur excessive baseband processing hardware cost and power consumption. Beamspace processing leverages the channel sparsity at mmWave frequencies to reduce baseband processing complexity. In this paper, we review existing beamspace data detection algorithms and propose new algorithms as well as corresponding VLSI architectures that reduce data detection power. We present VLSI implementation results for the proposed architectures in a 22nm FDSOI process. Our results demonstrate that a fully-parallelized implementation of the proposed complex sparsity-adaptive equalizer (CSPADE) achieves up to 54% power savings compared to antenna-domain equalization. Furthermore, our fully-parallelized designs achieve the highest reported throughput among existing massive MIMO data detectors, while achieving better energy and area efficiency. We also present a sequential multiply-accumulate (MAC)-based architecture for CSPADE, which enables even higher power savings, i.e., up to 66%, compared to a MAC-based antenna-domain equalizer."}
{"id": "2511.10442", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.10442", "abs": "https://arxiv.org/abs/2511.10442", "authors": ["Aarush Agarwal", "Raymond He", "Jan Kieseler", "Matteo Cremonesi", "Shah Rukh Qasim"], "title": "FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing", "comment": null, "summary": "We introduce FastGraph, a novel GPU-optimized k-nearest neighbor algorithm specifically designed to accelerate graph construction in low-dimensional spaces (2-10 dimensions), critical for high-performance graph neural networks. Our method employs a GPU-resident, bin-partitioned approach with full gradient-flow support and adaptive parameter tuning, significantly enhancing both computational and memory efficiency. Benchmarking demonstrates that FastGraph achieves a 20-40x speedup over state-of-the-art libraries such as FAISS, ANNOY, and SCANN in dimensions less than 10 with virtually no memory overhead. These improvements directly translate into substantial performance gains for GNN-based workflows, particularly benefiting computationally intensive applications in low dimensions such as particle clustering in high-energy physics, visual object tracking, and graph clustering."}
{"id": "2511.09861", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.09861", "abs": "https://arxiv.org/abs/2511.09861", "authors": ["Marco Kurzynski", "Shaizeen Aga", "Di Wu"], "title": "Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs", "comment": null, "summary": "GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, such as cutting-edge large language models (LLMs). We analyze the performance of a single-node multi-GPU system running LLM training, and observe that the kernel-level performance variation is highly correlated with concurrent computation communication (C3), a technique to overlap computation and communication across GPUs for performance gains. We then take a further step to reason that thermally induced straggling coupling with C3 impacts performance variation, coined as the Lit Silicon effect. Lit Silicon describes that in a multi-GPU node, thermal imbalance across GPUs introduces node-level straggler GPUs, which in turn slow down the leader GPUs. Lit Silicon leads to node-level performance variation and inefficiency, impacting the entire datacenter from the bottom up. We propose analytical performance and power models for Lit Silicon, to understand the potential system-level gains. We further design simple detection and mitigation techniques to effectively address the Lit Silicon problem, and evaluate three different power management solutions, including power optimization under GPU thermal design power, performance optimization under node-level GPU power capping, and performance optimization under node-level CPU power sloshing. We conduct experiments on two workloads on two AMD InstinctTM MI300X GPU systems under two LLM training frameworks, and observe up to 6% performance and 4% power improvements, potentially saving hundreds of millions of dollars in datacenters. Our solution is almost free lunch and can be effortlessly adopted in datacenters as a new node-level power management layer."}
{"id": "2511.10480", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10480", "abs": "https://arxiv.org/abs/2511.10480", "authors": ["Changhai Man", "Joongun Park", "Hanjiang Wu", "Huan Xu", "Srinivas Sridharan", "Tushar Krishna"], "title": "Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs", "comment": null, "summary": "Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems."}
