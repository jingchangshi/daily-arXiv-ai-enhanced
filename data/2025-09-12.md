<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs](https://arxiv.org/abs/2509.09019)
*Mohit Tekriwal,John Sarracino*

Main category: cs.PL

TL;DR: 利用Rocq定理证明器中的Verified LLVM框架，证明LLVM IR级别FMA优化在基本块a*b+c表达式中的正确性，并计划扩展更多程序特性和浮点优化。


<details>
  <summary>Details</summary>
Motivation: 科学计算程序需要激进编译器优化来获得高性能，但必须确保这些优化的正确性，特别是浮点优化和fast math优化。

Method: 基于Rocq定理证明器中的Verified LLVM框架，对基本块中a*b+c算术表达式进行FMA优化的正确性证明。

Result: 提出了初步工作，成功证明了FMA优化在特定算术表达式中的正确性。

Conclusion: 这项工作为浮点优化正确性验证提供了基础，并计划扩展到更多程序特性和fast math浮点优化。

Abstract: Scientific computing programs often undergo aggressive compiler optimization
to achieve high performance and efficient resource utilization. While
performance is critical, we also need to ensure that these optimizations are
correct. In this paper, we focus on a specific class of optimizations,
floating-point optimizations, notably due to fast math, at the LLVM IR level.
We present a preliminary work, which leverages the Verified LLVM framework in
the Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)
optimization for a basic block implementing the arithmetic expression $a * b +
c$ . We then propose ways to extend this preliminary results by adding more
program features and fast math floating-point optimizations.

</details>


### [2] [Dependent-Type-Preserving Memory Allocation](https://arxiv.org/abs/2509.09059)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: 依赖类型语言在编译后类型规范会被擦除，导致外部程序可能违反原始程序的规范。本文提出通过类型保持编译和依赖内存分配的中间语言来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 依赖类型语言如Coq、Agda等允许程序编写详细规范并证明程序符合规范，但这些规范在编译时会被擦除，外部链接程序可能违反这些规范，即使使用验证编译器编译也是如此。

Method: 开发支持依赖内存分配的类型化中间语言，以及用于内存分配的依赖类型保持编译器传递，通过类型保持编译在链接过程中进行类型检查。

Result: 本文是进行中的工作，提出了解决方案框架但尚未报告具体实验结果。

Conclusion: 通过类型保持编译和依赖内存分配的中间语言，可以在链接过程中防止与类型不正确的程序链接，从而保持程序规范的完整性。

Abstract: Dependently typed programming languages such as Coq, Agda, Idris, and F*,
allow programmers to write detailed specifications of their programs and prove
their programs meet these specifications. However, these specifications can be
violated during compilation since they are erased after type checking. External
programs linked with the compiled program can violate the specifications of the
original program and change the behavior of the compiled program -- even when
compiled with a verified compiler. For example, since Coq does not allow
explicitly allocating memory, a programmer might link their Coq program with a
C program that can allocate memory. Even if the Coq program is compiled with a
verified compiler, the external C program can still violate the memory-safe
specification of the Coq program by providing an uninitialized pointer to
memory. This error could be ruled out by type checking in a language expressive
enough to indicate whether memory is initialized versus uninitialized. Linking
with a program with an uninitialized pointer could be considered ill-typed, and
our linking process could prevent linking with ill-typed programs. To
facilitate type checking during linking, we can use type-preserving
compilation, which preserves the types through the compilation process. In this
ongoing work, we develop a typed intermediate language that supports dependent
memory allocation, as well as a dependent-type-preserving compiler pass for
memory allocation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems](https://arxiv.org/abs/2509.08969)
*Nima Karimian Kakolaki*

Main category: cs.DC

TL;DR: 本文对分布式标识符方案进行综合分析，比较了传统自增键、UUIDv4、UUIDv7和ULIDs的性能表现，发现ULIDs在网络开销、生成速度和碰撞风险方面均显著优于其他方案


<details>
  <summary>Details</summary>
Motivation: 分布式系统需要健壮、可扩展的标识符方案来确保数据唯一性和跨多节点的高效索引

Method: 结合碰撞概率的数学计算与在模拟分布式环境中测量生成速度和网络传输开销的实证实验

Result: ULIDs显著优于UUIDv4和UUIDv7，网络开销减少83.7%，生成速度提高97.32%，碰撞风险比UUIDv7低98.42%，即使在高生成率下也保持可忽略的碰撞概率

Conclusion: ULIDs是高性能分布式系统的最佳选择，提供高效、时间有序且字典可排序的标识符，适合可扩展应用

Abstract: Distributed systems require robust, scalable identifier schemes to ensure
data uniqueness and efficient indexing across multiple nodes. This paper
presents a comprehensive analysis of the evolution of distributed identifiers,
comparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We
combine mathematical calculation of collision probabilities with empirical
experiments measuring generation speed and network transmission overhead in a
simulated distributed environment. Results demonstrate that ULIDs significantly
outperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing
generation speed by 97.32%. statistical analysis further shows ULIDs offer a
98.42% lower collision risk compared to UUIDv7, while maintaining negligible
collision probabilities even at high generation rates. These findings highlight
ULIDs as an optimal choice for high-performance distributed systems, providing
efficient, time-ordered, and lexicographically sortable identifiers suitable
for scalable applications. All source code, datasets, and analysis scripts
utilized in this research are publicly available in our dedicated repository at
https://github.com/nimakarimiank/uids-comparison. This repository contains
comprehensive documentation of the experimental setup, including configuration
files for the distributed environment, producer and consumer implementations,
and message broker integration. Additionally, it provides the data scripts and
datasets. Researchers and practitioners are encouraged to explore the
repository for full reproducibility of the experiments and to facilitate
further investigation or extension of the presented work.

</details>


### [4] [Optimizing the Variant Calling Pipeline Execution on Human Genomes Using GPU-Enabled Machines](https://arxiv.org/abs/2509.09058)
*Ajay Kumar,Praveen Rao,Peter Sanders*

Main category: cs.DC

TL;DR: 这篇论文提出了一种基于机器学习的优化方法，通过预测变异检测流水线各阶段执行时间和生成最优执行计划，在GPU设备上实现了人类基因组变异检测工作负载的高效执行。


<details>
  <summary>Details</summary>
Motivation: 由于变异检测计算密集性较高，基因数据通常在云环境中处理。论文解决在GPU机器上高效执行变异检测流水线的问题，以最小化总执行时间。

Method: 提出两种关键技术：1）使用ML根据基因组序列特征预测流水线各阶段执行时间；2）受灵活工作店排程问题启发，使用预测时间生成最优执行计划，通过精心同步在不同机器上执行。

Result: 在公开基因组序列工作负载上评估，ML预测模型在序列大小、读质量、重复读百分比等特征上预测准确。与贪心方法相比实现2倍加速，与动态资源分配方法相比实现1.6倍加速。

Conclusion: 该研究提供了一种有效的ML基于时间预测和优化排程的方法，能够在GPU设备上显著提升基因组变异检测工作负载的执行效率。

Abstract: Variant calling is the first step in analyzing a human genome and aims to
detect variants in an individual's genome compared to a reference genome. Due
to the computationally-intensive nature of variant calling, genomic data are
increasingly processed in cloud environments as large amounts of compute and
storage resources can be acquired with the pay-as-you-go pricing model. In this
paper, we address the problem of efficiently executing a variant calling
pipeline for a workload of human genomes on graphics processing unit
(GPU)-enabled machines. We propose a novel machine learning (ML)-based approach
for optimizing the workload execution to minimize the total execution time. Our
approach encompasses two key techniques: The first technique employs ML to
predict the execution times of different stages in a variant calling pipeline
based on the characteristics of a genome sequence. Using the predicted times,
the second technique generates optimal execution plans for the machines by
drawing inspiration from the flexible job shop scheduling problem. The plans
are executed via careful synchronization across different machines. We
evaluated our approach on a workload of publicly available genome sequences
using a testbed with different types of GPU hardware. We observed that our
approach was effective in predicting the execution times of variant calling
pipeline stages using ML on features such as sequence size, read quality,
percentage of duplicate reads, and average read length. In addition, our
approach achieved 2X speedup (on an average) over a greedy approach that also
used ML for predicting the execution times on the tested workload of sequences.
Finally, our approach achieved 1.6X speedup (on an average) over a dynamic
approach that executed the workload based on availability of resources without
using any ML-based time predictions.

</details>


### [5] [Coherence-Aware Task Graph Modeling for Realistic Application](https://arxiv.org/abs/2509.09094)
*Guochu Xiong,Xiangzhong Luo,Weichen Liu*

Main category: cs.DC

TL;DR: CoTAM是一个缓存一致性感知的任务图建模框架，通过解耦一致性影响、量化其权重并推断任务间依赖关系，为真实工作负载构建反映运行时行为的统一任务图。


<details>
  <summary>Details</summary>
Motivation: 随着多核系统扩展，缓存一致性成为系统性能的关键因素。现有任务图建模方法要么依赖隐式技术，要么生成针对固定调度模型的图，且往往忽略一致性交互，导致设计假设与实际运行时行为存在差距。

Method: CoTAM框架通过解耦缓存一致性影响与整体执行，通过学习的加权方案量化一致性影响，并推断任务间依赖关系来生成一致性感知的任务图。

Result: 大量实验表明，CoTAM优于隐式方法，弥合了动态工作负载行为与现有设计之间的差距。

Conclusion: 将缓存一致性纳入任务图建模对于准确和可推广的系统级分析至关重要，CoTAM框架为此提供了有效解决方案。

Abstract: As multicore systems continue to scale, cache coherence has emerged as a
critical determinant of system performance, with coherence behavior and task
execution closely intertwined, reshaping inter-task dependencies. Task graph
modeling provides a structured way to capture such dependencies and serves as
the foundation for many system-level design strategies. However, these
strategies typically rely on predefined task graphs, while many real-world
applications lack explicit graphs and exhibit dynamic, data-dependent behavior,
limiting the effectiveness of static approaches. To address this, several task
graph modeling methods for realistic workloads have been developed. Yet, they
either rely on implicit techniques that use application-specific features
without producing explicit graphs, or they generate graphs tailored to fixed
scheduling models, which limits generality. More importantly, they often
overlook coherence interactions, creating a gap between design assumptions and
actual runtime behavior. To overcome these limitations, we propose CoTAM, a
Coherence-Aware Task Graph Modeling framework for realistic workloads that
constructs a unified task graph reflecting runtime behavior. CoTAM analyzes the
impact of coherence by decoupling its effects from overall execution,
quantifies its influence through a learned weighting scheme, and infers
inter-task dependencies for coherence-aware graph generation. Extensive
experiments show that CoTAM outperforms implicit methods, bridging the gap
between dynamic workload behavior and existing designs while demonstrating the
importance of incorporating cache coherence into task graph modeling for
accurate and generalizable system-level analysis.

</details>


### [6] [WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge](https://arxiv.org/abs/2509.09400)
*Valerio Besozzi,Enrico Fiasco,Marco Danelutto,Patrizio Dazzi*

Main category: cs.DC

TL;DR: 这篇论文比较了WebAssembly和unikernel基微虚拟机在边缘计算中的性能，发现WebAssembly在轻量函数冷启动时间更低，而Firecracker在复杂工作负载和I/O密集任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 边缘服务器无计算需要轻量级执行环境来最小化冷启动延迟，特别是在紧急边缘计算(UEC)场景下。需要比较不同技术方案的性能特性。

Method: 研究提出了Limes，一个基于Wasmtime的WebAssembly运行时，并将其与基于Firecracker的SPARE环境进行性能比较分析。测试了不同类型的服务器无函数工作负载。

Result: 结果显示WebAssembly在轻量函数上拥有更低的冷启动时间，但在复杂工作负载下表现得很差。Firecracker提供了更高但更稳定的冷启动时间，并在执行性能上表玹更好，特别是在I/O密集型任务中。

Conclusion: 两种技术各有优势：WebAssembly适合轻量函数和快速冷启场景，而Firecracker更适合复杂工作负载和高性能要求的应用。根据具体需求选择适合的技术方案。

Abstract: Serverless computing at the edge requires lightweight execution environments
to minimize cold start latency, especially in Urgent Edge Computing (UEC). This
paper compares WebAssembly and unikernel-based MicroVMs for serverless
workloads. We present Limes, a WebAssembly runtime built on Wasmtime, and
evaluate it against the Firecracker-based environment used in SPARE. Results
show that WebAssembly offers lower cold start times for lightweight functions
but suffers with complex workloads, while Firecracker provides higher, but
stable, cold starts and better execution performance, particularly for
I/O-heavy tasks.

</details>


### [7] [Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing](https://arxiv.org/abs/2509.09435)
*Houming Qiu,Kun Zhu,Dusit Niyato,Nguyen Cong Luong,Changyan Yi,Chen Dai*

Main category: cs.DC

TL;DR: 基于重心有理插值的近似编码分布式计算方案，充分利用任意工作节点返回结果进行解码，解决了传统CDC方案的固定阈值限制和数值不稳定问题


<details>
  <summary>Details</summary>
Motivation: 解决协同移动边缘计算中延迟节点（stragglers）造成的性能下降问题，充分利用编码分布式计算技术来减轻straggler效应

Method: 采用重心有理插值方法构建近似CDC方案，设计无极点的编码/解码函数，支持有限域和实数域计算，并集成BRI基础的梯度编码算法

Result: 实验结果显示，该方案在等待时间和近似精度方面都优于现有CDC方案，能够使用任意数量的工作节点返回结果进行解码

Conclusion: 该研究提出的重心有理插值基于CDC方案有效解决了传统方案的两大限制，实现了灵活的解码能力和数值稳定性，为协同移动边缘计算提供了更高效的straggler忍容方案

Abstract: Collaborative mobile edge computing (MEC) has emerged as a promising paradigm
to enable low-capability edge nodes to cooperatively execute
computation-intensive tasks. However, straggling edge nodes (stragglers)
significantly degrade the performance of MEC systems by prolonging computation
latency. While coded distributed computing (CDC) as an effective technique is
widely adopted to mitigate straggler effects, existing CDC schemes exhibit two
critical limitations: (i) They cannot successfully decode the final result
unless the number of received results reaches a fixed recovery threshold, which
seriously restricts their flexibility; (ii) They suffer from inherent poles in
their encoding/decoding functions, leading to decoding inaccuracies and
numerical instability in the computational results. To address these
limitations, this paper proposes an approximated CDC scheme based on
barycentric rational interpolation. The proposed CDC scheme offers several
outstanding advantages. Firstly, it can decode the final result leveraging any
returned results from workers. Secondly, it supports computations over both
finite and real fields while ensuring numerical stability. Thirdly, its
encoding/decoding functions are free of poles, which not only enhances
approximation accuracy but also achieves flexible accuracy tuning. Fourthly, it
integrates a novel BRI-based gradient coding algorithm accelerating the
training process while providing robustness against stragglers. Finally,
experimental results reveal that the proposed scheme is superior to existing
CDC schemes in both waiting time and approximate accuracy.

</details>


### [8] [Weaker Assumptions for Asymmetric Trust](https://arxiv.org/abs/2509.09493)
*Ignacio Amores-Sesar,Christian Cachin,Juan Villacis*

Main category: cs.DC

TL;DR: 这篇论文研究不对称信任分布式系统中的基础问题，提出了更弱假设的可靠广播和共识算法，充分利用了不对称信任的优势。


<details>
  <summary>Details</summary>
Motivation: 传统对称信任模型在不对称信任分布式系统中存在限制，现有方案的假设过于严格，影响了不对称信任的优势。需要更弱假设的解决方案。

Method: 提出了一种新的方法来特征化不对称问题，并基于此设计了可靠广播和共识算法，这些算法需要比之前方案更弱的假设。

Result: 新算法在保持系统可靠性的同时，充分利用了不对称信任的灵活性，避免了现有方案的过度限制问题。

Conclusion: 该研究为不对称信任分布式系统提供了更为实用的基础解决方案，方法具有普遍性，可扩展到其他核心问题的解决。

Abstract: In distributed systems with asymmetric trust, each participant is free to
make its own trust assumptions about others, captured by an asymmetric quorum
system. This contrasts with ordinary, symmetric quorum systems and threshold
models, where trust assumptions are uniformly shared among participants.
Fundamental problems like reliable broadcast and consensus are unsolvable in
the asymmetric model if quorum systems satisfy only the classical properties of
consistency and availability. Existing approaches overcome this by introducing
stronger assumptions. We show that some of these assumptions are overly
restrictive, so much so that they effectively eliminate the benefits of
asymmetric trust. To address this, we propose a new approach to characterize
asymmetric problems and, building upon it, present algorithms for reliable
broadcast and consensus that require weaker assumptions than previous
solutions. Our methods are general and can be extended to other core problems
in systems with asymmetric trust.

</details>


### [9] [TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes](https://arxiv.org/abs/2509.09525)
*Jialiang Huang,Teng Ma,Zheng Liu,Sixing Lin,Kang Chen,Jinlei Jiang,Xia Liao,Yingdi Shan,Yongwei Wu,Ning Zhang,Mengting Lu,Tao Ma,Haifeng Gong,Mingxing Zhang*

Main category: cs.DC

TL;DR: TrEnv是一个针对LLM代理优化的无服务器平台，通过可重用沙箱和内存模板等技术，显著降低了启动延迟和内存使用，在容器和虚拟机环境中都实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有无服务器计算基础设施在处理LLM代理等新兴工作负载时存在瓶颈，其开销可能达到LLM API调用成本的70%，需要更高效的高密度无服务器平台。

Method: TrEnv采用协同设计方法，支持容器和虚拟机环境，通过可重用沙箱、内存模板、浏览器共享和页面缓存绕过机制来优化LLM代理的执行环境。

Result: 评估显示TrEnv在容器环境中将P99延迟降低7倍，内存使用减少48%；在虚拟机环境中P99延迟降低58%，内存节省61%，优于E2B等现有系统。

Conclusion: TrEnv通过创新的环境复用和优化技术，为LLM代理等新兴工作负载提供了高效的无服务器计算解决方案，显著降低了基础设施开销。

Abstract: Serverless computing provides dynamic scalability, but its infrastructure
overhead becomes a bottleneck for emerging workloads such as LLM agents, which
exhibit unpredictable invocation patterns and variable resource demands. Our
analysis shows that for these agents, the cost of running on serverless
platforms can reach up to 70% of the cost of LLM API calls. This finding
motivates the need for a more efficient, high-density serverless platform. We
present TrEnv, a co-designed serverless platform that supports both container-
and VM-based environments, optimized for the unique demands of LLM agents.
TrEnv reduces startup latency and memory usage through repurposable sandboxes
and memory templates, which enable fast reuse and restoration of execution
environments. To further reduce overhead in VM-based agent workloads, TrEnv
leverages browser sharing and a page cache bypassing mechanism. Evaluations
show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in
container-based settings, and achieves up to 58% lower P99 latency and 61%
memory savings for VM-based agents compared to state-of-the-art systems like
E2B.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [Implementation of a 8-bit Wallace Tree Multiplier](https://arxiv.org/abs/2509.09178)
*Ayan Biswas,Jimmy Jin*

Main category: cs.AR

TL;DR: 本文概述了Wallace树乘法器的设计、进展和方法论，包括在gpdk45技术上使用Cadence Virtuoso设计的8位Wallace树乘法器的原理图和版图，以及最终实现的16位组合乘法累加(MAC)单元。


<details>
  <summary>Details</summary>
Motivation: 设计并行数字乘法器架构，通过全加器和半加器电路减少每级部分积数量，实现相对于输入大小的最优最坏情况时间复杂度O(log(n))。

Method: 使用Cadence Virtuoso在gpdk45技术上进行Wallace树8位乘法器的原理图和版图设计，并实现16位组合乘法累加单元。

Result: 成功设计了Wallace树8位乘法器，并实现了16位组合乘法累加(MAC)单元，达到了预期的电路深度优化目标。

Conclusion: Wallace树乘法器架构能有效优化乘法运算的时间复杂度，通过并行设计和适当的电路实现，可以在现代半导体工艺上实现高效的乘法运算单元。

Abstract: Wallace tree multipliers are a parallel digital multiplier architecture
designed to minimize the worst-case time complexity of the circuit depth
relative to the input size [1]. In particular, it seeks to perform long
multiplication in the binary sense, reducing as many partial products per stage
as possible through full and half adders circuits, achieving O(log(n)) where n
= bit length of input. This paper provides an overview of the design, progress
and methodology in the final project of ECE 55900, consisting of the schematic
and layout of a Wallace tree 8-bit input multiplier on the gpdk45 technology in
Cadence Virtuoso, as well as any design attempts prior to the final product.
This also includes our endeavors in designing the final MAC (Multiply
Accumulate) unit with undefined targets, which we chose to implement as a 16
bit combinational multiply-add.

</details>


### [11] [Combating the Memory Walls: Optimization Pathways for Long-Context Agentic LLM Inference](https://arxiv.org/abs/2509.09505)
*Haoran Wu,Can Xiao,Jiayi Nie,Xuan Guo,Binglei Lou,Jeffrey T. H. Wong,Zhiwen Mo,Cheng Zhang,Przemyslaw Forys,Wayne Luk,Hongxiang Fan,Jianyi Cheng,Timothy M. Jones,Rika Antonova,Robert Mullins,Aaron Zhao*

Main category: cs.AR

TL;DR: PLENA是一个硬件软件协同设计的系统，专门针对长上下文LLM推理中的内存带宽和容量瓶颈问题，通过非对称量化方案、扁平化脉动阵列架构和完整软件栈优化，实现了比现有加速器更高的利用率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理应用（如网页DOM处理、复杂工具调用）需要处理超长上下文，导致推理阶段产生大量片外内存流量，受到内存带宽和容量两个内存墙的限制，使得计算单元无法达到高利用率。

Method: PLENA采用三条核心优化路径：1）支持非对称量化方案的高效硬件实现；2）具有FlashAttention原生支持的扁平化脉动阵列架构；3）完整的软件栈包括自定义ISA、编译器、周期仿真器和自动化设计空间探索流程。

Result: 仿真结果显示，PLENA比现有加速器实现高达8.5倍的利用率提升，在相同乘法器数量和内存配置下，比A100 GPU提供2.24倍吞吐量，比TPU v6e提供3.85倍吞吐量。

Conclusion: PLENA系统有效解决了长上下文LLM推理中的内存墙问题，显著提升了硬件利用率和推理性能，整个系统将开源发布。

Abstract: LLMs now form the backbone of AI agents for a diverse array of applications,
including tool use, command-line agents, and web or computer use agents. These
agentic LLM inference tasks are fundamentally different from chatbot-focused
inference -- they often have much larger context lengths to capture complex,
prolonged inputs, such as entire webpage DOMs or complicated tool call
trajectories. This, in turn, generates significant off-chip memory traffic for
the underlying hardware at the inference stage and causes the workload to be
constrained by two memory walls, namely the bandwidth and capacity memory
walls, preventing the on-chip compute units from achieving high utilization.
  In this paper, we introduce PLENA, a hardware-software co-designed system
that applies three core optimization pathways to tackle these challenges. PLENA
includes an efficient hardware implementation of compute and memory units
supporting an asymmetric quantization scheme. PLENA also features a novel
flattened systolic array architecture that has native support for
FlashAttention to tackle these memory walls in the scenario of inference
serving for long-context LLMs. Additionally, PLENA is developed with a complete
stack, including a custom ISA, a compiler, a cycle-emulated simulator, and an
automated design space exploration flow. The simulated results show that PLENA
achieves up to 8.5x higher utilization than existing accelerators, and delivers
2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the
TPU v6e, under the same multiplier count and memory settings. The full PLENA
system will also be open-sourced.

</details>
