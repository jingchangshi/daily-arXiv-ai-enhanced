<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Remarks on Algebraic Reconstruction of Types and Effects](https://arxiv.org/abs/2601.15455)
*Patrycja Balik,Szymon Jędras,Piotr Polesiuk*

Main category: cs.PL

TL;DR: 该论文指出Jouvelot和Gifford 1991年开创性类型与效应系统算法中存在与高阶多态性相关的变量绑定错误，并重新审视了其类型系统和重建算法。


<details>
  <summary>Details</summary>
Motivation: Jouvelot和Gifford 1991年的论文《Algebraic Reconstruction of Types and Effects》是类型与效应系统领域的里程碑工作，启发了大量后续研究。然而，原始算法处理高阶多态性时存在实现挑战，作者发现其中存在与变量绑定相关的微妙错误。

Method: 重新审视Jouvelot和Gifford的类型系统和重建算法，分析其处理高阶多态性的方法，识别变量绑定相关的错误。

Result: 发现了原始算法中与变量绑定相关的微妙错误，这些错误源于处理高阶多态性时的实现挑战。

Conclusion: 虽然Jouvelot和Gifford的开创性工作具有重要意义，但其算法在处理高阶多态性时存在需要修正的错误，这为后续研究提供了改进方向。

Abstract: In their 1991 paper "Algebraic Reconstruction of Types and Effects," Pierre Jouvelot and David Gifford presented a type-and-effect reconstruction algorithm based on an algebraic structure of effects. Their work is considered a milestone in the development of type-and-effect systems, and has inspired numerous subsequent works in the area of static analysis. However, unlike the later research it spawned, the original algorithm considered a language with higher-rank polymorphism, a feature which is challenging to implement correctly. In this note, we identify subtle bugs related to variable binding in their approach to this feature. We revisit their type system and reconstruction algorithm, and describe the discovered issues.

</details>


### [2] [Prioritizing Configuration Relevance via Compiler-Based Refined Feature Ranking](https://arxiv.org/abs/2601.16008)
*Federico Bruzzone,Walter Cazzola,Luca Favini*

Main category: cs.PL

TL;DR: 提出RustyEx：首个基于编译器的Rust配置优先级排序方法，通过图中心性分析特征重要性，生成有限但关键的配置子集，解决配置组合爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 现代编程语言（特别是Rust）支持通过特征组合构建高度可配置软件系统，但配置组合爆炸使得程序分析、优化和测试的穷举探索不可行，需要有效的配置优先级排序方法。

Method: 1) 从Rust编译器提取定制中间表示；2) 构建两种互补的图数据结构；3) 使用中心性度量对特征排序；4) 根据特征影响的代码范围细化排序；5) 使用SAT求解器生成有效配置。

Result: 在开源Rust项目上的实证评估表明，RustyEx能在有限资源内高效生成用户指定的配置集，同时通过构造保证正确性，中心性引导的配置优先级排序能有效探索大型配置空间。

Conclusion: 该方法为配置感知分析和优化开辟了新途径，证明了基于编译器的方法在配置优先级排序中的可行性和有效性，为解决配置组合爆炸问题提供了实用解决方案。

Abstract: Modern programming languages, most notably Rust, offer advanced linguistic constructs for building highly configurable software systems as aggregation of features -- identified by a configuration. However, they pose substantial challenges for program analysis, optimization, and testing, as the combinatorial explosion of configurations often makes exhaustive exploration infeasible. In this manuscript, we present the first compiler-based method for prioritizing configurations. Our approach consists of four main steps: 1. extracting a tailored intermediate representation from the Rust compiler, 2. constructing two complementary graph-based data structures, 3. using centrality measures to rank features, and 4. refining the ranking by considering the extent of code they impact. A fixed number of most relevant configurations are generated based on the achieved feature ranking. The validity of the generated configurations is guaranteed by using a SAT solver that takes a representation of this graph in conjunctive normal form. We formalized this approach and implemented it in a prototype, RustyEx, by instrumenting the Rust compiler. An empirical evaluation on higher-ranked open source Rust projects shows that RustyEx efficiently generates user-specified sets of configurations within bounded resources, while ensuring soundness by construction. The results demonstrate that centrality-guided configuration prioritization enables effective and practical exploration of large configuration spaces, paving the way for future research in configuration-aware analysis and optimization.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Securing LLM-as-a-Service for Small Businesses: An Industry Case Study of a Distributed Chatbot Deployment Platform](https://arxiv.org/abs/2601.15528)
*Jiazhu Xie,Bowen Li,Heyu Fu,Chong Gao,Ziqi Xu,Fengling Han*

Main category: cs.DC

TL;DR: 本文介绍了一个开源的多租户平台，帮助小企业通过无代码工作流部署定制的LLM支持聊天机器人，解决了基础设施成本、工程复杂性和安全风险等挑战。


<details>
  <summary>Details</summary>
Motivation: LLM问答系统在小企业的客户支持和内部知识访问方面具有巨大潜力，但实际部署面临基础设施成本高、工程复杂度大、安全风险（特别是在RAG场景下）等挑战，阻碍了小企业的采用。

Method: 开发了一个基于分布式轻量级k3s集群的开源多租户平台，跨异构低成本机器构建，通过加密覆盖网络连接，实现成本效益的资源池化，同时提供容器隔离和租户数据访问控制。平台还集成了针对RAG聊天机器人提示注入攻击的实用平台级防御机制。

Result: 通过一个真实的电子商务部署评估，证明该平台能够在现实的小企业成本、运营和安全约束下，实现安全高效的LLM聊天机器人服务。

Conclusion: 该平台展示了小企业可以在不依赖模型重训练或企业级基础设施的情况下，通过创新的分布式架构和平台级安全机制，成功部署安全高效的LLM支持聊天机器人。

Abstract: Large Language Model (LLM)-based question-answering systems offer significant potential for automating customer support and internal knowledge access in small businesses, yet their practical deployment remains challenging due to infrastructure costs, engineering complexity, and security risks, particularly in retrieval-augmented generation (RAG)-based settings. This paper presents an industry case study of an open-source, multi-tenant platform that enables small businesses to deploy customised LLM-based support chatbots via a no-code workflow. The platform is built on distributed, lightweight k3s clusters spanning heterogeneous, low-cost machines and interconnected through an encrypted overlay network, enabling cost-efficient resource pooling while enforcing container-based isolation and per-tenant data access controls. In addition, the platform integrates practical, platform-level defences against prompt injection attacks in RAG-based chatbots, translating insights from recent prompt injection research into deployable security mechanisms without requiring model retraining or enterprise-scale infrastructure. We evaluate the proposed platform through a real-world e-commerce deployment, demonstrating that secure and efficient LLM-based chatbot services can be achieved under realistic cost, operational, and security constraints faced by small businesses.

</details>


### [4] [Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search](https://arxiv.org/abs/2601.15633)
*Enzo Meneses,Hugo Bec,Cristóbal A. Navarroa,Benoît Crespin,Felipe A. Quezada,Nancy Hitschfeld,Heinich Porro,Maxime Maria*

Main category: cs.DC

TL;DR: 本文提出三种改进RT Core上粒子FRNN物理模拟的方法：BVH更新/重建优化器、消除邻居列表的RT Core新用法、周期性边界条件支持技术，显著提升性能和能效。


<details>
  <summary>Details</summary>
Motivation: 当前基于RT Core的粒子FRNN物理模拟存在BVH管理效率低、需要内存密集的邻居列表、不支持周期性边界条件等问题，需要进一步优化以充分发挥RT Core潜力。

Method: 1) 实时BVH更新/重建比例优化器；2) 两种消除邻居列表的RT Core新用法；3) 支持周期性边界条件的RT Core技术。以Lennard-Jones FRNN相互作用模型为案例进行研究。

Result: BVH优化器使RT Core流水线速度提升~3.4倍；新方法使模拟速度从半径较小时的~1.3倍提升到对数正态半径分布的~2.0倍；支持周期性边界条件无显著性能损失；方法在不同GPU代际上均能扩展。

Conclusion: 提出的三种方法显著提升了RT Core在FRNN物理模拟中的性能和能效，同时明确了RT Core与传统GPU计算的适用场景，为硬件加速粒子模拟提供了实用指导。

Abstract: In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\sim 3.4\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\sim1.3\times$ at small radius to $\sim2.0\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [5] [FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design](https://arxiv.org/abs/2601.15710)
*Jiahao Zhang,Zifan He,Nicholas Fraser,Michaela Blott,Yizhou Sun,Jason Cong*

Main category: cs.AR

TL;DR: FlexLLM是一个可组合的HLS库，用于快速开发领域特定的LLM加速器，支持阶段定制化推理和量化部署，在FPGA上实现比GPU更高的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理需要高性能加速器，但开发过程复杂耗时。需要一种方法能够快速将算法创新转化为高效硬件实现，同时支持阶段定制化设计和量化部署。

Method: FlexLLM提供可组合的HLS库，暴露关键架构自由度，支持prefill和decode阶段的定制化设计，包含全面的量化套件，并引入Hierarchical Memory Transformer插件处理长上下文。

Result: 在AMD U280 FPGA上，相比NVIDIA A100 GPU，实现1.29倍端到端加速、1.64倍解码吞吐量和3.14倍能效提升。集成HMT插件后，在长上下文场景中减少23.23倍prefill延迟，扩展64倍上下文窗口。

Conclusion: FlexLLM能够以最小的人工努力，将LLM推理的算法创新与高性能加速器连接起来，显著加速开发过程并实现优越的性能和能效。

Abstract: We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\times$ end-to-end speedup, 1.64$\times$ higher decode throughput, and 3.14$\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\times$, 6.55$\times$, and 4.13$\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\times$ and extends the context window by 64$\times$, delivering 1.10$\times$/4.86$\times$ lower end-to-end latency and 5.21$\times$/6.27$\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.

</details>


### [6] [A Case for Hypergraphs to Model and Map SNNs on Neuromorphic Hardware](https://arxiv.org/abs/2601.16118)
*Marco Ronzani,Cristina Silvano*

Main category: cs.AR

TL;DR: 该论文提出将脉冲神经网络(SNN)从图抽象提升到超图抽象，以改进在神经形态硬件上的映射算法，通过利用超边的重叠和局部性来减少通信流量和硬件资源使用。


<details>
  <summary>Details</summary>
Motivation: 在神经形态硬件上执行SNN面临神经元到核心的映射问题。随着SNN和硬件规模扩展到数十亿神经元，现有的图分区和放置方法变得越来越难以有效处理，需要更高效的抽象模型。

Method: 将SNN从图抽象提升到超图抽象，利用超边共成员关系捕捉核心内脉冲复制的特性。基于超边的重叠和局部性设计新的分区和放置算法，包括新设计的算法和文献改编的算法。

Result: 超图技术在不同执行时间机制下都能实现比现有技术更好的映射效果，通过共享超边分组神经元，能够比单纯压缩单个连接更有效地减少通信流量和硬件资源使用。

Conclusion: 超图抽象为大规模SNN在神经形态硬件上的映射提供了更有效的模型，基于超边特性的算法能够在各种规模下实现高质量的映射，为大规模部署提供了有前景的解决方案。

Abstract: Executing Spiking Neural Networks (SNNs) on neuromorphic hardware poses the problem of mapping neurons to cores. SNNs operate by propagating spikes between neurons that form a graph through synapses. Neuromorphic hardware mimics them through a network-on-chip, transmitting spikes, and a mesh of cores, each managing several neurons. Its operational cost is tied to spike movement and active cores. A mapping comprises two tasks: partitioning the SNN's graph to fit inside cores and placement of each partition on the hardware mesh. Both are NP-hard problems, and as SNNs and hardware scale towards billions of neurons, they become increasingly difficult to tackle effectively. In this work, we propose to raise the abstraction of SNNs from graphs to hypergraphs, redesigning mapping techniques accordingly. The resulting model faithfully captures the replication of spikes inside cores by exposing the notion of hyperedge co-membership between neurons. We further show that the overlap and locality of hyperedges strongly correlate with high-quality mappings, making these properties instrumental in devising mapping algorithms. By exploiting them directly, grouping neurons through shared hyperedges, communication traffic and hardware resource usage can be reduced be yond what just contracting individual connections attains. To substantiate this insight, we consider several partitioning and placement algorithms, some newly devised, others adapted from literature, and compare them over progressively larger and bio-plausible SNNs. Our results show that hypergraph based techniques can achieve better mappings than the state-of-the-art at several execution time regimes. Based on these observations, we identify a promising selection of algorithms to achieve effective mappings at any scale.

</details>
