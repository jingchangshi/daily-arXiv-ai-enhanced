{"id": "2509.05504", "categories": ["cs.PL", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.05504", "abs": "https://arxiv.org/abs/2509.05504", "authors": ["Karl Aaron Rudkowski", "Sallar Ahmadi-Pour", "Rolf Drechsler"], "title": "Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution", "comment": null, "summary": "Virtual Prototypes (VPs) are important tools in modern hardware development.\nAt high abstractions, they are often implemented in SystemC and offer early\nanalysis of increasingly complex designs. These complex designs often combine\none or more processors, interconnects, and peripherals to perform tasks in\nhardware or interact with the environment. Verifying these subsystems is a\nwell-suited task for VPs, as they allow reasoning across different abstraction\nlevels. While modern verification techniques like symbolic execution can be\nseamlessly integrated into VP-based workflows, they require modifications in\nthe SystemC kernel. Hence, existing approaches therefore modify and replace the\nSystemC kernel, or ignore the opportunity of cross-level scenarios completely,\nand would not allow focusing on special challenges of particular subsystems\nlike peripherals. We propose CrosSym and SEFOS, two opposing approaches for a\nversatile symbolic execution of peripherals. CrosSym modifies the SystemC\nkernel, while SEFOS instead modifies a modern symbolic execution engine. Our\nextensive evaluation applies our tools to various peripherals on different\nlevels of abstractions. Both tools extensive sets of features are demonstrated\nfor (1) different verification scenarios, and (2) identifying 300+ mutants. In\ncomparison with each other, SEFOS convinces with the unmodified SystemC kernel\nand peripheral, while CrosSym offers slightly better runtime and memory usage.\nIn comparison to the state-of-the-art, that is limited to Transaction Level\nModelling (TLM), our tools offered comparable runtime, while enabling\ncross-level verification with symbolic execution."}
{"id": "2509.05586", "categories": ["cs.PL", "cs.CC"], "pdf": "https://arxiv.org/pdf/2509.05586", "abs": "https://arxiv.org/abs/2509.05586", "authors": ["Lee Zheng Han", "Umang Mathur"], "title": "Fixed Parameter Tractable Linearizability Monitoring for Stack, Queue and Anagram Agnostic Data Types", "comment": null, "summary": "Verifying linearizability of concurrent data structures is NP-hard, even for\nsimple types. We present fixed-parameter tractable algorithms for monitoring\nstacks, queues, and anagram-agnostic data types (AADTs), parameterized by the\nmaximum concurrency. Our approach leverages frontier graphs and partition\nstates to bound the search space. For AADTs, equivalence of linearizations\nenables monitoring in log-linear time. For stacks, we introduce a grammar-based\nmethod with a sub-cubic reduction to matrix multiplication, and for queues, a\nsplit-sequence transition system supporting efficient dynamic programming.\nThese results unify tractability guarantees for both order-sensitive and\nanagram-agnostic data types under bounded concurrency."}
{"id": "2509.06724", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.06724", "abs": "https://arxiv.org/abs/2509.06724", "authors": ["Florian Kohn", "Arthur Correnson", "Jan Baumeister", "Bernd Finkbeiner"], "title": "Pacing Types: Safe Monitoring of Asynchronous Streams", "comment": null, "summary": "Stream-based monitoring is a real-time safety assurance mechanism for complex\ncyber-physical systems such as unmanned aerial vehicles. In this context, a\nmonitor aggregates streams of input data from sensors and other sources to give\nreal-time statistics and assessments of the system's health. Since monitors are\nsafety-critical components, it is crucial to ensure that they are free of\npotential runtime errors. One of the central challenges in designing reliable\nstream-based monitors is to deal with the asynchronous nature of data streams:\nin concrete applications, the different sensors being monitored produce values\nat different speeds, and it is the monitor's responsibility to correctly react\nto the asynchronous arrival of different streams of values. To ease this\nprocess, modern frameworks for stream-based monitoring such as RTLola feature\nan expressive specification language that allows to finely specify data\nsynchronization policies. While this feature dramatically simplifies the design\nof monitors, it can also lead to subtle runtime errors. To mitigate this issue,\nthis paper presents pacing types, a novel type system implemented in RTLola to\nensure that monitors for asynchronous streams are well-behaved at runtime. We\nformalize the essence of pacing types for a core fragment of RTLola, and\npresent a soundness proof of the pacing type system using a new logical\nrelation."}
{"id": "2509.06752", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.06752", "abs": "https://arxiv.org/abs/2509.06752", "authors": ["Amir M. Ben-Amram", "Samir Genaim", "JoÃ«l Ouaknine", "James Worrell"], "title": "Termination Analysis of Linear-Constraint Programs", "comment": null, "summary": "This Survey provides an overview of techniques in termination analysis for\nprograms with numerical variables and transitions defined by linear\nconstraints. This subarea of program analysis is challenging due to the\nexistence of undecidable problems, and this Survey systematically explores\napproaches that mitigate this inherent difficulty. These include foundational\ndecidability results, the use of ranking functions, and disjunctive\nwell-founded transition invariants. The Survey also discusses non-termination\nwitnesses, used to prove that a program will not halt. We examine the\nalgorithmic and complexity aspects of these methods, showing how different\napproaches offer a trade-off between expressive power and computational\ncomplexity. The Survey does not discuss how termination analysis is performed\non real-world programming languages, nor does it consider more expressive\nabstract models that include non-linear arithmetic, probabilistic choice, or\nterm rewriting systems."}
{"id": "2509.05451", "categories": ["cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.05451", "abs": "https://arxiv.org/abs/2509.05451", "authors": ["Niansong Zhang", "Wenbo Zhu", "Courtney Golden", "Dan Ilan", "Hongzheng Chen", "Christopher Batten", "Zhiru Zhang"], "title": "Characterizing and Optimizing Realistic Workloads on a Commercial Compute-in-SRAM Device", "comment": "MICRO 2025", "summary": "Compute-in-SRAM architectures offer a promising approach to achieving higher\nperformance and energy efficiency across a range of data-intensive\napplications. However, prior evaluations have largely relied on simulators or\nsmall prototypes, limiting the understanding of their real-world potential. In\nthis work, we present a comprehensive performance and energy characterization\nof a commercial compute-in-SRAM device, the GSI APU, under realistic workloads.\nWe compare the GSI APU against established architectures, including CPUs and\nGPUs, to quantify its energy efficiency and performance potential. We introduce\nan analytical framework for general-purpose compute-in-SRAM devices that\nreveals fundamental optimization principles by modeling performance trade-offs,\nthereby guiding program optimizations.\n  Exploiting the fine-grained parallelism of tightly integrated memory-compute\narchitectures requires careful data management. We address this by proposing\nthree optimizations: communication-aware reduction mapping, coalesced DMA, and\nbroadcast-friendly data layouts. When applied to retrieval-augmented generation\n(RAG) over large corpora (10GB--200GB), these optimizations enable our\ncompute-in-SRAM system to accelerate retrieval by 4.8$\\times$--6.6$\\times$ over\nan optimized CPU baseline, improving end-to-end RAG latency by\n1.1$\\times$--1.8$\\times$. The shared off-chip memory bandwidth is modeled using\na simulated HBM, while all other components are measured on the real\ncompute-in-SRAM device. Critically, this system matches the performance of an\nNVIDIA A6000 GPU for RAG while being significantly more energy-efficient\n(54.4$\\times$-117.9$\\times$ reduction). These findings validate the viability\nof compute-in-SRAM for complex, real-world applications and provide guidance\nfor advancing the technology."}
{"id": "2509.05303", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05303", "abs": "https://arxiv.org/abs/2509.05303", "authors": ["Sam Davidson", "Li Sun", "Bhavana Bhasker", "Laurent Callot", "Anoop Deoras"], "title": "Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across Multiple Formats", "comment": null, "summary": "Infrastructure as Code (IaC) is fundamental to modern cloud computing,\nenabling teams to define and manage infrastructure through machine-readable\nconfiguration files. However, different cloud service providers utilize diverse\nIaC formats. The lack of a standardized format requires cloud architects to be\nproficient in multiple IaC languages, adding complexity to cloud deployment.\nWhile Large Language Models (LLMs) show promise in automating IaC creation and\nmaintenance, progress has been limited by the lack of comprehensive benchmarks\nacross multiple IaC formats. We present Multi-IaC-Bench, a novel benchmark\ndataset for evaluating LLM-based IaC generation and mutation across AWS\nCloudFormation, Terraform, and Cloud Development Kit (CDK) formats. The dataset\nconsists of triplets containing initial IaC templates, natural language\nmodification requests, and corresponding updated templates, created through a\nsynthetic data generation pipeline with rigorous validation. We evaluate\nseveral state-of-the-art LLMs on Multi-IaC-Bench, demonstrating that while\nmodern LLMs can achieve high success rates (>95%) in generating syntactically\nvalid IaC across formats, significant challenges remain in semantic alignment\nand handling complex infrastructure patterns. Our ablation studies highlight\nthe importance of prompt engineering and retry mechanisms in successful IaC\ngeneration. We release Multi-IaC-Bench to facilitate further research in\nAI-assisted infrastructure management and establish standardized evaluation\nmetrics for this crucial domain."}
{"id": "2509.06794", "categories": ["cs.PL", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06794", "abs": "https://arxiv.org/abs/2509.06794", "authors": ["Shihan Fang", "Hongzheng Chen", "Niansong Zhang", "Jiajie Li", "Han Meng", "Adrian Liu", "Zhiru Zhang"], "title": "Dato: A Task-Based Programming Model for Dataflow Accelerators", "comment": null, "summary": "Recent deep learning workloads increasingly push computational demand beyond\nwhat current memory systems can sustain, with many kernels stalling on data\nmovement rather than computation. While modern dataflow accelerators\nincorporate on-chip streaming to mitigate off-chip bandwidth limitations,\nexisting programming models struggle to harness these capabilities effectively.\nLow-level interfaces provide fine-grained control but impose significant\ndevelopment overhead, whereas high-level tile-based languages abstract away\ncommunication details, restricting optimization and forcing compilers to\nreconstruct the intended dataflow. We present Dato, a Python-embedded,\ntask-based programming model for dataflow accelerators that elevates data\ncommunication and sharding to first-class type constructs. Developers write\nprograms as a graph of tasks connected via explicit stream types, with sharded\ninputs specified using layout types. These tasks are first mapped virtually\nonto the accelerator's spatial fabric, and the compiler then generates a\nphysical mapping that respects hardware constraints. Experimental results on\nboth AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves\nhigh performance while significantly reducing the burden of writing optimized\ncode. On the NPU, Dato attains up to 84% hardware utilization for GEMM and\ndelivers a 2.81x speedup on attention kernels compared to a state-of-the-art\ncommercial framework. On the FPGA, Dato surpasses leading frameworks in\nperformance when generating custom systolic arrays, achieving 98% of the\ntheoretical peak performance."}
{"id": "2509.05688", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.05688", "abs": "https://arxiv.org/abs/2509.05688", "authors": ["Kuan-Ting Lin", "Ching-Te Chiu", "Jheng-Yi Chang", "Shi-Zong Huang", "Yu-Ting Li"], "title": "High Utilization Energy-Aware Real-Time Inference Deep Convolutional Neural Network Accelerator", "comment": null, "summary": "Deep convolution Neural Network (DCNN) has been widely used in computer\nvision tasks. However, for edge devices even inference has too large\ncomputational complexity and data access amount. The inference latency of\nstate-of-the-art models are impractical for real-world applications. In this\npaper, we propose a high utilization energy-aware real-time inference deep\nconvolutional neural network accelerator, which improves the performance of the\ncurrent accelerators. First, we use the 1x1 size convolution kernel as the\nsmallest unit of the computing unit. Then we design suitable computing unit\nbased on the requirements of each model. Secondly, we use Reuse Feature SRAM to\nstore the output of the current layer in the chip and use the value as the\ninput of the next layer. Moreover, we import Output Reuse Strategy and Ring\nStream Dataflow to reduce the amount of data exchange between chips and DRAM.\nFinally, we present On-fly Pooling Module to let the calculation of the Pooling\nlayer directly complete in the chip. With the aid of the proposed method, the\nimplemented acceleration chip has an extremely high hardware utilization rate.\nWe reduce a generous amount of data transfer on the specific module, ECNN.\nCompared to the methods without reuse strategy, we can reduce 533 times of data\naccess amount. At the same time, we have enough computing power to perform\nreal-time execution of the existing image classification model, VGG16 and\nMobileNet. Compared with the design in VWA, we can speed up 7.52 times and have\n1.92x energy efficiency"}
{"id": "2509.05870", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2509.05870", "abs": "https://arxiv.org/abs/2509.05870", "authors": ["Edith Cohen", "Moshe Shechner", "Uri Stemmer"], "title": "A Simple and Robust Protocol for Distributed Counting", "comment": null, "summary": "We revisit the distributed counting problem, where a server must continuously\napproximate the total number of events occurring across $k$ sites while\nminimizing communication. The communication complexity of this problem is known\nto be $\\Theta(\\frac{k}{\\epsilon}\\log N)$ for deterministic protocols. Huang,\nYi, and Zhang (2012) showed that randomization can reduce this to\n$\\Theta(\\frac{\\sqrt{k}}{\\epsilon}\\log N)$, but their analysis is restricted to\nthe {\\em oblivious setting}, where the stream of events is independent of the\nprotocol's outputs.\n  Xiong, Zhu, and Huang (2023) presented a robust protocol for distributed\ncounting that removes the oblivious assumption. However, their communication\ncomplexity is suboptimal by a $polylog(k)$ factor and their protocol is\nsubstantially more complex than the oblivious protocol of Huang et al. (2012).\nThis left open a natural question: could it be that the simple protocol of\nHuang et al. (2012) is already robust?\n  We resolve this question with two main contributions. First, we show that the\nprotocol of Huang et al. (2012) is itself not robust by constructing an\nexplicit adaptive attack that forces it to lose its accuracy. Second, we\npresent a new, surprisingly simple, robust protocol for distributed counting\nthat achieves the optimal communication complexity of\n$O(\\frac{\\sqrt{k}}{\\epsilon} \\log N)$. Our protocol is simpler than that of\nXiong et al. (2023), perhaps even simpler than that of Huang et al. (2012), and\nis the first to match the optimal oblivious complexity in the adaptive setting."}
{"id": "2509.06845", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.06845", "abs": "https://arxiv.org/abs/2509.06845", "authors": ["Tom Lauwaerts", "Maarten Steevens", "Christophe Scholliers"], "title": "MIO: Multiverse Debugging in the Face of Input/Output -- Extended Version with Additional Appendices", "comment": "This extended version provides auxiliary material to the article of\n  the same title that will appear in the ACM Digital Library as part of the\n  PACMPL issue for OOPSLA 2025", "summary": "Debugging non-deterministic programs on microcontrollers is notoriously\nchallenging, especially when bugs manifest in unpredictable, input-dependent\nexecution paths. A recent approach, called multiverse debugging, makes it\neasier to debug non-deterministic programs by allowing programmers to explore\nall potential execution paths. Current multiverse debuggers enable both forward\nand backward traversal of program paths, and some facilitate jumping to any\npreviously visited states, potentially branching into alternative execution\npaths within the state space.\n  Unfortunately, debugging programs that involve input/output operations using\nexisting multiverse debuggers can reveal inaccessible program states, i.e.\nstates which are not encountered during regular execution. This can\nsignificantly hinder the debugging process, as the programmer may spend\nsubstantial time exploring and examining inaccessible program states, or worse,\nmay mistakenly assume a bug is present in the code, when in fact, the issue is\ncaused by the debugger.\n  This paper presents a novel approach to multiverse debugging, which can\naccommodate a broad spectrum of input/output operations. We provide the\nsemantics of our approach and prove the correctness of our debugger, ensuring\nthat despite having support for a wide range of input/output operations the\ndebugger will only explore those program states which can be reached during\nregular execution.\n  We have developed a prototype, called MIO, leveraging the WARDuino\nWebAssembly virtual machine to demonstrate the feasibility and efficiency of\nour techniques. As a demonstration of the approach we highlight a color dial\nbuilt with a Lego Mindstorms motor, and color sensor, providing a tangible\nexample of how our approach enables multiverse debugging for programs running\non an STM32 microcontroller."}
{"id": "2509.05937", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.05937", "abs": "https://arxiv.org/abs/2509.05937", "authors": ["Wei-Hsing Huang", "Jianwei Jia", "Yuyao Kong", "Faaiq Waqar", "Tai-Hao Wen", "Meng-Fan Chang", "Shimeng Yu"], "title": "Hardware Acceleration of Kolmogorov-Arnold Network (KAN) in Large-Scale Systems", "comment": null, "summary": "Recent developments have introduced Kolmogorov-Arnold Networks (KAN), an\ninnovative architectural paradigm capable of replicating conventional deep\nneural network (DNN) capabilities while utilizing significantly reduced\nparameter counts through the employment of parameterized B-spline functions\nwith trainable coefficients. Nevertheless, the B-spline functional components\ninherent to KAN architectures introduce distinct hardware acceleration\ncomplexities. While B-spline function evaluation can be accomplished through\nlook-up table (LUT) implementations that directly encode functional mappings,\nthus minimizing computational overhead, such approaches continue to demand\nconsiderable circuit infrastructure, including LUTs, multiplexers, decoders,\nand related components. This work presents an algorithm-hardware co-design\napproach for KAN acceleration. At the algorithmic level, techniques include\nAlignment-Symmetry and PowerGap KAN hardware aware quantization, KAN sparsity\naware mapping strategy, and circuit-level techniques include N:1 Time\nModulation Dynamic Voltage input generator with analog-compute-in-memory (ACIM)\ncircuits. This work conducts evaluations on large-scale KAN networks to\nvalidate the proposed methodologies. Non-ideality factors, including partial\nsum deviations from process variations, have been evaluated with statistics\nmeasured from the TSMC 22nm RRAM-ACIM prototype chips. Utilizing optimally\ndetermined KAN hyperparameters in conjunction with circuit optimizations\nfabricated at the 22nm technology node, despite the parameter count for\nlarge-scale tasks in this work increasing by 500Kx to 807Kx compared to\ntiny-scale tasks in previous work, the area overhead increases by only 28Kx to\n41Kx, with power consumption rising by merely 51x to 94x, while accuracy\ndegradation remains minimal at 0.11% to 0.23%, demonstrating the scaling\npotential of our proposed architecture."}
{"id": "2509.06046", "categories": ["cs.DC", "cs.DS", "cs.IR", "E.1; H.3.3"], "pdf": "https://arxiv.org/pdf/2509.06046", "abs": "https://arxiv.org/abs/2509.06046", "authors": ["Philip Adams", "Menghao Li", "Shi Zhang", "Li Tan", "Qi Chen", "Mingqin Li", "Zengzhong Li", "Knut Risvik", "Harsha Vardhan Simhadri"], "title": "DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers", "comment": null, "summary": "We present DISTRIBUTEDANN, a distributed vector search service that makes it\npossible to search over a single 50 billion vector graph index spread across\nover a thousand machines that offers 26ms median query latency and processes\nover 100,000 queries per second. This is 6x more efficient than existing\npartitioning and routing strategies that route the vector query to a subset of\npartitions in a scale out vector search system. DISTRIBUTEDANN is built using\ntwo well-understood components: a distributed key-value store and an in-memory\nANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for\nserving the Bing search engine, and we share our experience from making this\ntransition."}
{"id": "2509.06872", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.06872", "abs": "https://arxiv.org/abs/2509.06872", "authors": ["Zachary Kent", "Ugur Y. Yavuz", "Siddhartha Jayanti", "Stephanie Balzer", "Guy Blelloch"], "title": "Mechanized Metatheory of Forward Reasoning for End-to-End Linearizability Proofs", "comment": null, "summary": "In the past decade, many techniques have been developed to prove\nlinearizability, the gold standard of correctness for concurrent data\nstructures. Intuitively, linearizability requires that every operation on a\nconcurrent data structure appears to take place instantaneously, even when\ninterleaved with other operations. Most recently, Jayanti et al. presented the\nfirst sound and complete \"forward reasoning\" technique for proving\nlinearizability that relates the behavior of a concurrent data structure to a\nreference atomic data structure as time moves forward. This technique can be\nused to produce machine-checked proofs of linearizability in TLA+. However,\nwhile Jayanti et al.'s approach is shown to be sound and complete, a\nmechanization of this important metatheoretic result is still outstanding. As a\nresult, it is not possible to produce verified end-to-end proofs of\nlinearizability. To reduce the size of this trusted computing base, we\nformalize this forward reasoning technique and mechanize proofs of its\nsoundness and completeness in Rocq. As a case study, we use the approach to\nproduce a verified end-to-end proof of linearizability for a simple concurrent\nregister."}
{"id": "2509.06101", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.06101", "abs": "https://arxiv.org/abs/2509.06101", "authors": ["Fan Li", "Mimi Xie", "Yanan Guo", "Huize Li", "Xin Xin"], "title": "SCREME: A Scalable Framework for Resilient Memory Design", "comment": null, "summary": "The continuing advancement of memory technology has not only fueled a surge\nin performance, but also substantially exacerbate reliability challenges.\nTraditional solutions have primarily focused on improving the efficiency of\nprotection schemes, i.e., Error Correction Codes (ECC), under the assumption\nthat allocating additional memory space for parity data is always expensive and\ntherefore not a scalable solution.\n  We break the stereotype by proposing an orthogonal approach that provides\nadditional, cost-effective memory space for resilient memory design. In\nparticular, we recognize that ECC chips (used for parity storage) do not\nnecessarily require the same performance level as regular data chips. This\noffers two-fold benefits: First, the bandwidth originally provisioned for a\nregular-performance ECC chip can instead be used to accommodate multiple\nlow-performance chips. Second, the cost of ECC chips can be effectively\nreduced, as lower performance often correlates with lower expense. In addition,\nwe observe that server-class memory chips are often provisioned with ample, yet\nunderutilized I/O resources. This further offers the opportunity to repurpose\nthese resources to enable flexible on-DIMM interconnections. Based on the above\ntwo insights, we finally propose SCREME, a scalable memory framework leverages\ncost-effective, albeit slower, chips -- naturally produced during rapid\ntechnology evolution -- to meet the growing reliability demands driven by this\nevolution."}
{"id": "2509.06064", "categories": ["cs.DC", "math.CO"], "pdf": "https://arxiv.org/pdf/2509.06064", "abs": "https://arxiv.org/abs/2509.06064", "authors": ["Serafino Cicerone", "Alessia Di Fonso", "Gabriele Di Stefano", "Alfredo Navarra"], "title": "Gathering in Non-Vertex-Transitive Graphs Under Round Robin", "comment": "16 pages, 3 figures", "summary": "The Gathering problem for a swarm of robots asks for a distributed algorithm\nthat brings such entities to a common place, not known in advance. We consider\nthe well-known OBLOT model with robots constrained to move along the edges of a\ngraph, hence gathering in one vertex, eventually. Despite the classical setting\nunder which the problem has been usually approached, we consider the `hostile'\ncase where: i) the initial configuration may contain multiplicities, i.e. more\nthan one robot may occupy the same vertex; ii) robots cannot detect\nmultiplicities. As a scheduler for robot activation, we consider the\n\"favorable\" round-robin case, where robots are activated one at a time.\n  Our objective is to achieve a complete characterization of the problem in the\nbroad context of non-vertex-transitive graphs, i.e., graphs where the vertices\nare partitioned into at least two different classes of equivalence. We provide\na resolution algorithm for any configuration of robots moving on such graphs,\nalong with its correctness. Furthermore, we analyze its time complexity."}
{"id": "2509.06365", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.06365", "abs": "https://arxiv.org/abs/2509.06365", "authors": ["Omar Al Habsi", "Safa Mohammed Sali", "Anis Meribout", "Mahmoud Meribout", "Saif Almazrouei", "Mohamed Seghier"], "title": "Hardware Acceleration in Portable MRIs: State of the Art and Future Prospects", "comment": null, "summary": "There is a growing interest in portable MRI (pMRI) systems for point-of-care\nimaging, particularly in remote or resource-constrained environments. However,\nthe computational complexity of pMRI, especially in image reconstruction and\nmachine learning (ML) algorithms for enhanced imaging, presents significant\nchallenges. Such challenges can be potentially addressed by harnessing hardware\napplication solutions, though there is little focus in the current pMRI\nliterature on hardware acceleration. This paper bridges that gap by reviewing\nrecent developments in pMRI, emphasizing the role and impact of hardware\nacceleration to speed up image acquisition and reconstruction. Key technologies\nsuch as Graphics Processing Units (GPUs), Field-Programmable Gate Arrays\n(FPGAs), and Application-Specific Integrated Circuits (ASICs) offer excellent\nperformance in terms of reconstruction speed and power consumption. This review\nalso highlights the promise of AI-powered reconstruction, open low-field pMRI\ndatasets, and innovative edge-based hardware solutions for the future of pMRI\ntechnology. Overall, hardware acceleration can enhance image quality, reduce\npower consumption, and increase portability for next-generation pMRI\ntechnology. To accelerate reproducible AI for portable MRI, we propose forming\na Low-Field MRI Consortium and an evidence ladder (analytic/phantom validation,\nretrospective multi-center testing, prospective reader and non-inferiority\ntrials) to provide standardized datasets, benchmarks, and regulator-ready\ntestbeds."}
{"id": "2509.06229", "categories": ["cs.DC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.06229", "abs": "https://arxiv.org/abs/2509.06229", "authors": ["Karolina Skrivankova", "Mark Handley", "Stephen Hailes"], "title": "20 Years in Life of a Smart Building: A retrospective", "comment": null, "summary": "Operating an intelligent smart building automation system in 2025 is met with\nmany challenges: hardware failures, vendor obsolescence, evolving security\nthreats and more. None of these have been comprehensibly addressed by the\nindustrial building nor home automation industries, limiting feasibility of\noperating large, truly smart automation deployments. This paper introduces\nKaOS, a distributed control platform for constructing robust and evolvable\nsmart building automation systems using affordable, off-the-shelf IoT hardware.\nSupporting control applications and distributed system operations by leveraging\ncontainerisation and managed resource access, KaOS seeks to achieve\nflexibility, security, and fault tolerance without sacrificing\ncost-effectiveness. Initial evaluation confirms the practical feasibility of\nour approach, highlighting its potential to sustainably maintain and\nincrementally evolve building control functionalities over extended timeframes."}
{"id": "2509.06698", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.06698", "abs": "https://arxiv.org/abs/2509.06698", "authors": ["Leidy Mabel Alvero-Gonzalez", "Matias Miguez", "Eric Gutierrez", "Juan Sapriza", "Susana PatÃ³n", "David Atienza", "JosÃ© Miranda"], "title": "VCO-CARE: VCO-based Calibration-free Analog Readout for Electrodermal activity sensing", "comment": null, "summary": "Continuous monitoring of electrodermal activity (EDA) through wearable\ndevices has attracted much attention in recent times. However, the persistent\nchallenge demands analog front-end (AFE) systems with high sensitivity, low\npower consumption, and minimal calibration requirements to ensure practical\nusability in wearable technologies. In response to this challenge, this\nresearch introduces VCO-CARE, a Voltage-Controlled Oscillator-based Analog\nReadout tailored for continuous EDA sensing. The results show that our system\nachieves an exceptional average sensitivity of up to 40 pS within a 0-20 uS\nrange and a negligible relative error of less than 0.0025% for\nfixed-resistance. Furthermore, the proposed system consumes only an average of\n2.3 uW based on post-layout validations and introduces a low noise\ncontribution, measuring only 0.8 uVrms across the 0-1.5 Hz EDA signal band.\nThis research aims to drive the evolution of wearable sensors characterized by\nseamless adaptability to diverse users, minimal power consumption, and\noutstanding noise resilience."}
{"id": "2509.06261", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06261", "abs": "https://arxiv.org/abs/2509.06261", "authors": ["Kyungmin Bin", "Seungbeom Choi", "Jimyoung Son", "Jieun Choi", "Daseul Bae", "Daehyeon Baek", "Kihyo Moon", "Minsung Jang", "Hyojung Lee"], "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving", "comment": null, "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."}
{"id": "2509.05504", "categories": ["cs.PL", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.05504", "abs": "https://arxiv.org/abs/2509.05504", "authors": ["Karl Aaron Rudkowski", "Sallar Ahmadi-Pour", "Rolf Drechsler"], "title": "Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution", "comment": null, "summary": "Virtual Prototypes (VPs) are important tools in modern hardware development.\nAt high abstractions, they are often implemented in SystemC and offer early\nanalysis of increasingly complex designs. These complex designs often combine\none or more processors, interconnects, and peripherals to perform tasks in\nhardware or interact with the environment. Verifying these subsystems is a\nwell-suited task for VPs, as they allow reasoning across different abstraction\nlevels. While modern verification techniques like symbolic execution can be\nseamlessly integrated into VP-based workflows, they require modifications in\nthe SystemC kernel. Hence, existing approaches therefore modify and replace the\nSystemC kernel, or ignore the opportunity of cross-level scenarios completely,\nand would not allow focusing on special challenges of particular subsystems\nlike peripherals. We propose CrosSym and SEFOS, two opposing approaches for a\nversatile symbolic execution of peripherals. CrosSym modifies the SystemC\nkernel, while SEFOS instead modifies a modern symbolic execution engine. Our\nextensive evaluation applies our tools to various peripherals on different\nlevels of abstractions. Both tools extensive sets of features are demonstrated\nfor (1) different verification scenarios, and (2) identifying 300+ mutants. In\ncomparison with each other, SEFOS convinces with the unmodified SystemC kernel\nand peripheral, while CrosSym offers slightly better runtime and memory usage.\nIn comparison to the state-of-the-art, that is limited to Transaction Level\nModelling (TLM), our tools offered comparable runtime, while enabling\ncross-level verification with symbolic execution."}
{"id": "2509.06362", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.06362", "abs": "https://arxiv.org/abs/2509.06362", "authors": ["Mo Xuan", "Zhang yue", "Wu Weigang"], "title": "MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS", "comment": null, "summary": "Model-as-a-Service (MaaS) platforms face diverse Service Level Objective\n(SLO) requirements stemming from various large language model (LLM)\napplications, manifested in contextual complexity, first-token latency, and\nbetween-token latency. On the other hand, an LLM instance, when configured with\ndifferent parallelism strategies and inference batch sizes, exhibits distinct\nperformance characteristics and can thus be used to serve different SLO\nrequirements. However, current LLM inference systems typically deploy instances\nof the same model with identical configurations, lacking mechanisms to leverage\nsuch heterogeneity. To fill this research gap, we propose MaaSO, the first MaaS\nOrchestrator, which comprises three modules: (1) a profiler characterizing\ninstance performance under diverse parallelism strategies and inference batch\nsizes; (2) a placer optimizing heterogeneous instance configurations; (3) a\ndistributor enabling SLO-aware request distribution and preventing cascaded\ntimeouts in continuous batching. Experiments show that MaaSO improves the SLO\nsatisfaction ratio by 15 to 30% and reduces response latency by 40 to 60%\ncompared to existing approaches, and significantly lowers overall orchestration\noverhead."}
{"id": "2509.06794", "categories": ["cs.PL", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06794", "abs": "https://arxiv.org/abs/2509.06794", "authors": ["Shihan Fang", "Hongzheng Chen", "Niansong Zhang", "Jiajie Li", "Han Meng", "Adrian Liu", "Zhiru Zhang"], "title": "Dato: A Task-Based Programming Model for Dataflow Accelerators", "comment": null, "summary": "Recent deep learning workloads increasingly push computational demand beyond\nwhat current memory systems can sustain, with many kernels stalling on data\nmovement rather than computation. While modern dataflow accelerators\nincorporate on-chip streaming to mitigate off-chip bandwidth limitations,\nexisting programming models struggle to harness these capabilities effectively.\nLow-level interfaces provide fine-grained control but impose significant\ndevelopment overhead, whereas high-level tile-based languages abstract away\ncommunication details, restricting optimization and forcing compilers to\nreconstruct the intended dataflow. We present Dato, a Python-embedded,\ntask-based programming model for dataflow accelerators that elevates data\ncommunication and sharding to first-class type constructs. Developers write\nprograms as a graph of tasks connected via explicit stream types, with sharded\ninputs specified using layout types. These tasks are first mapped virtually\nonto the accelerator's spatial fabric, and the compiler then generates a\nphysical mapping that respects hardware constraints. Experimental results on\nboth AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves\nhigh performance while significantly reducing the burden of writing optimized\ncode. On the NPU, Dato attains up to 84% hardware utilization for GEMM and\ndelivers a 2.81x speedup on attention kernels compared to a state-of-the-art\ncommercial framework. On the FPGA, Dato surpasses leading frameworks in\nperformance when generating custom systolic arrays, achieving 98% of the\ntheoretical peak performance."}
{"id": "2509.06514", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.06514", "abs": "https://arxiv.org/abs/2509.06514", "authors": ["Mpoki Mwaisela", "Peterson Yuhala", "Pascal Felber", "Valerio Schiavoni"], "title": "IM-PIR: In-Memory Private Information Retrieval", "comment": null, "summary": "Private information retrieval (PIR) is a cryptographic primitive that allows\na client to securely query one or multiple servers without revealing their\nspecific interests. In spite of their strong security guarantees, current PIR\nconstructions are computationally costly. Specifically, most PIR\nimplementations are memory-bound due to the need to scan extensive databases\n(in the order of GB), making them inherently constrained by the limited memory\nbandwidth in traditional processor-centric computing\narchitectures.Processing-in-memory (PIM) is an emerging computing paradigm that\naugments memory with compute capabilities, addressing the memory bandwidth\nbottleneck while simultaneously providing extensive parallelism.Recent research\nhas demonstrated PIM's potential to significantly improve performance across a\nrange of data-intensive workloads, including graph processing, genome analysis,\nand machine learning.\n  In this work, we propose the first PIM-based architecture for multi-server\nPIR. We discuss the algorithmic foundations of the latter and show how its\noperations align with the core strengths of PIM architectures: extensive\nparallelism and high memory bandwidth. Based on this observation, we design and\nimplement IM-PIR, a PIM-based multi-server PIR approach on top of UPMEM PIM,\nthe first openly commercialized PIM architecture. Our evaluation demonstrates\nthat a PIM-based multi-server PIR implementation significantly improves query\nthroughput by more than 3.7x when compared to a standard CPU-based PIR\napproach."}
{"id": "2509.06616", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.06616", "abs": "https://arxiv.org/abs/2509.06616", "authors": ["Anton Paramonov", "Yann Vonlanthen", "Quentin Kniep", "Jakub Sliwinski", "Roger Wattenhofer"], "title": "Mangrove: Fast and Parallelizable State Replication for Blockchains", "comment": null, "summary": "Mangrove is a novel scaling approach to building blockchains with parallel\nsmart contract support. Unlike in monolithic blockchains, where a single\nconsensus mechanism determines a strict total order over all transactions,\nMangrove uses separate consensus instances per smart contract, without a global\norder. To allow multiple instances to run in parallel while ensuring that no\nconflicting transactions are committed, we propose a mechanism called Parallel\nOptimistic Agreement. Additionally, for simple transactions, we leverage a\nlightweight Byzantine Reliable Broadcast primitive to reduce latency. Mangrove\nis optimized for performance under optimistic conditions, where there is no\nmisbehavior and the network is synchronous. Under these conditions, our\nprotocol can achieve a latency of 2 communication steps between creating and\nexecuting a transaction."}
