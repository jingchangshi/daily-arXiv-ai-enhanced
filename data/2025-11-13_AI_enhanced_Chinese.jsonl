{"id": "2511.08842", "categories": ["cs.AR", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.08842", "abs": "https://arxiv.org/abs/2511.08842", "authors": ["Eren Kurshan", "Yuan Xie", "Paul Franzon"], "title": "3D Guard-Layer: An Integrated Agentic AI Safety System for Edge Artificial Intelligence", "comment": "Resubmitting Re: Arxiv Committee Approval", "summary": "AI systems have found a wide range of real-world applications in recent years. The adoption of edge artificial intelligence, embedding AI directly into edge devices, is rapidly growing. Despite the implementation of guardrails and safety mechanisms, security vulnerabilities and challenges have become increasingly prevalent in this domain, posing a significant barrier to the practical deployment and safety of AI systems. This paper proposes an agentic AI safety architecture that leverages 3D to integrate a dedicated safety layer. It introduces an adaptive AI safety infrastructure capable of dynamically learning and mitigating attacks against the AI system. The system leverages the inherent advantages of co-location with the edge computing hardware to continuously monitor, detect and proactively mitigate threats to the AI system. The integration of local processing and learning capabilities enhances resilience against emerging network-based attacks while simultaneously improving system reliability, modularity, and performance, all with minimal cost and 3D integration overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u96c6\u6210\u7684\u4ee3\u7406AI\u5b89\u5168\u67b6\u6784\uff0c\u901a\u8fc7\u4e13\u7528\u5b89\u5168\u5c42\u52a8\u6001\u5b66\u4e60\u548c\u7f13\u89e3\u9488\u5bf9AI\u7cfb\u7edf\u7684\u653b\u51fb\uff0c\u5229\u7528\u8fb9\u7f18\u8ba1\u7b97\u786c\u4ef6\u7684\u5171\u4f4d\u4f18\u52bf\u6301\u7eed\u76d1\u63a7\u3001\u68c0\u6d4b\u548c\u4e3b\u52a8\u7f13\u89e3\u5a01\u80c1\u3002", "motivation": "\u5c3d\u7ba1\u5b9e\u65bd\u4e86\u62a4\u680f\u548c\u5b89\u5168\u673a\u5236\uff0c\u8fb9\u7f18AI\u9886\u57df\u7684\u5b89\u5168\u6f0f\u6d1e\u548c\u6311\u6218\u65e5\u76ca\u666e\u904d\uff0c\u6210\u4e3aAI\u7cfb\u7edf\u5b9e\u9645\u90e8\u7f72\u548c\u5b89\u5168\u6027\u7684\u91cd\u8981\u969c\u788d\u3002", "method": "\u91c7\u7528\u4ee3\u7406AI\u5b89\u5168\u67b6\u6784\uff0c\u901a\u8fc73D\u96c6\u6210\u4e13\u7528\u5b89\u5168\u5c42\uff0c\u6784\u5efa\u81ea\u9002\u5e94AI\u5b89\u5168\u57fa\u7840\u8bbe\u65bd\uff0c\u5229\u7528\u8fb9\u7f18\u8ba1\u7b97\u786c\u4ef6\u7684\u5171\u4f4d\u4f18\u52bf\u8fdb\u884c\u6301\u7eed\u76d1\u63a7\u548c\u5a01\u80c1\u7f13\u89e3\u3002", "result": "\u7cfb\u7edf\u589e\u5f3a\u4e86\u5bf9\u6297\u65b0\u5174\u7f51\u7edc\u653b\u51fb\u7684\u97e7\u6027\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u3001\u6a21\u5757\u5316\u548c\u6027\u80fd\uff0c\u4e14\u6210\u672c\u548c3D\u96c6\u6210\u5f00\u9500\u6700\u5c0f\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u8fb9\u7f18AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b89\u5168\u4fdd\u62a4\uff0c\u89e3\u51b3\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u6311\u6218\u3002"}}
{"id": "2511.09131", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.09131", "abs": "https://arxiv.org/abs/2511.09131", "authors": ["Li Lu", "Jianan Wen", "Milos Krstic"], "title": "FsimNNs: An Open-Source Graph Neural Network Platform for SEU Simulation-based Fault Injection", "comment": null, "summary": "Simulation-based fault injection is a widely adopted methodology for assessing circuit vulnerability to Single Event Upsets (SEUs); however, its computational cost grows significantly with circuit complexity. To address this limitation, this work introduces an open-source platform that exploits Spatio-Temporal Graph Neural Networks (STGNNs) to accelerate SEU fault simulation. The platform includes three STGNN architectures incorporating advanced components such as Atrous Spatial Pyramid Pooling (ASPP) and attention mechanisms, thereby improving spatio-temporal feature extraction. In addition, SEU fault simulation datasets are constructed from six open-source circuits with varying levels of complexity, providing a comprehensive benchmark for performance evaluation. The predictive capability of the STGNN models is analyzed and compared on these datasets. Moreover, to further investigate the efficiency of the approach, we evaluate the predictive capability of STGNNs across multiple test cases and discuss their generalization capability. The developed platform and datasets are released as open-source to support reproducibility and further research on https://github.com/luli2021/FsimNNs.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5f00\u6e90\u5e73\u53f0\uff0c\u7528\u4e8e\u52a0\u901f\u5355\u7c92\u5b50\u7ffb\u8f6c\u6545\u969c\u6a21\u62df\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u62df\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8e\u6a21\u62df\u7684\u6545\u969c\u6ce8\u5165\u65b9\u6cd5\u5728\u8bc4\u4f30\u7535\u8def\u5bf9\u5355\u7c92\u5b50\u7ffb\u8f6c\u7684\u8106\u5f31\u6027\u65f6\uff0c\u968f\u7740\u7535\u8def\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u589e\u957f\u3002", "method": "\u5f15\u5165\u4e86\u4e09\u79cd\u5305\u542b\u7a7a\u6d1e\u7a7a\u95f4\u91d1\u5b57\u5854\u6c60\u5316\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u6539\u8fdb\u4e86\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u5f00\u6e90\u7535\u8def\u4e0a\u6784\u5efa\u4e86SEU\u6545\u969c\u6a21\u62df\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86STGNN\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5f00\u53d1\u7684\u5f00\u6e90\u5e73\u53f0\u548c\u6570\u636e\u96c6\u652f\u6301\u53ef\u91cd\u590d\u6027\u7814\u7a76\uff0c\u4e3a\u52a0\u901fSEU\u6545\u969c\u6a21\u62df\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.08729", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.08729", "abs": "https://arxiv.org/abs/2511.08729", "authors": ["Sacha-\u00c9lie Ayoun", "Opale Sj\u00f6stedt", "Azalea Raad"], "title": "Soteria: Efficient Symbolic Execution as a Functional Library", "comment": null, "summary": "Symbolic execution (SE) tools often rely on intermediate languages (ILs) to support multiple programming languages, promising reusability and efficiency. In practice, this approach introduces trade-offs between performance, accuracy, and language feature support. We argue that building SE engines \\emph{directly} for each source language is both simpler and more effective. We present Soteria, a lightweight OCaml library for writing SE engines in a functional style, without compromising on performance, accuracy or feature support. Soteria enables developers to construct SE engines that operate directly over source-language semantics, offering \\emph{configurability}, compositional reasoning, and ease of implementation. Using Soteria, we develop Soteria$^{\\text{Rust}}$, the \\emph{first} Rust SE engine supporting Tree Borrows (the intricate aliasing model of Rust), and Soteria$^{\\text{C}}$, a compositional SE engine for C. Both tools are competitive with or outperform state-of-the-art tools such as Kani, Pulse, CBMC and Gillian-C in performance and the number of bugs detected. We formalise the theoretical foundations of Soteria and prove its soundness, demonstrating that sound, efficient, accurate, and expressive SE can be achieved without the compromises of ILs.", "AI": {"tldr": "Soteria\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684OCaml\u5e93\uff0c\u7528\u4e8e\u76f4\u63a5\u4e3a\u6e90\u8bed\u8a00\u6784\u5efa\u7b26\u53f7\u6267\u884c\u5f15\u64ce\uff0c\u907f\u514d\u4e86\u4e2d\u95f4\u8bed\u8a00\u7684\u6027\u80fd\u3001\u51c6\u786e\u6027\u548c\u8bed\u8a00\u7279\u6027\u652f\u6301\u65b9\u9762\u7684\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u7b26\u53f7\u6267\u884c\u5de5\u5177\u4f9d\u8d56\u4e2d\u95f4\u8bed\u8a00\u6765\u652f\u6301\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u4f46\u8fd9\u5728\u6027\u80fd\u3001\u51c6\u786e\u6027\u548c\u8bed\u8a00\u7279\u6027\u652f\u6301\u65b9\u9762\u5f15\u5165\u4e86\u6743\u8861\u3002\u4f5c\u8005\u8ba4\u4e3a\u76f4\u63a5\u4e3a\u6bcf\u79cd\u6e90\u8bed\u8a00\u6784\u5efa\u7b26\u53f7\u6267\u884c\u5f15\u64ce\u66f4\u7b80\u5355\u6709\u6548\u3002", "method": "\u5f00\u53d1\u4e86Soteria\u5e93\uff0c\u91c7\u7528\u51fd\u6570\u5f0f\u7f16\u7a0b\u98ce\u683c\u6784\u5efa\u7b26\u53f7\u6267\u884c\u5f15\u64ce\uff0c\u76f4\u63a5\u64cd\u4f5c\u6e90\u8bed\u8a00\u8bed\u4e49\uff0c\u63d0\u4f9b\u53ef\u914d\u7f6e\u6027\u3001\u7ec4\u5408\u63a8\u7406\u548c\u6613\u4e8e\u5b9e\u73b0\u7684\u7279\u6027\u3002", "result": "\u57fa\u4e8eSoteria\u5f00\u53d1\u4e86Soteria^Rust\uff08\u9996\u4e2a\u652f\u6301Rust Tree Borrows\u522b\u540d\u6a21\u578b\u7684\u7b26\u53f7\u6267\u884c\u5f15\u64ce\uff09\u548cSoteria^C\uff08C\u8bed\u8a00\u7684\u7ec4\u5408\u7b26\u53f7\u6267\u884c\u5f15\u64ce\uff09\uff0c\u5728\u6027\u80fd\u548cbug\u68c0\u6d4b\u6570\u91cf\u4e0a\u4f18\u4e8e\u6216\u5ab2\u7f8eKani\u3001Pulse\u3001CBMC\u548cGillian-C\u7b49\u6700\u5148\u8fdb\u5de5\u5177\u3002", "conclusion": "\u8bc1\u660e\u4e86\u65e0\u9700\u4e2d\u95f4\u8bed\u8a00\u7684\u59a5\u534f\uff0c\u4e5f\u80fd\u5b9e\u73b0\u58f0\u97f3\u3001\u9ad8\u6548\u3001\u51c6\u786e\u548c\u8868\u8fbe\u529b\u5f3a\u7684\u7b26\u53f7\u6267\u884c\uff0c\u5e76\u5f62\u5f0f\u5316\u4e86Soteria\u7684\u7406\u8bba\u57fa\u7840\u5e76\u8bc1\u660e\u4e86\u5176\u58f0\u97f3\u6027\u3002"}}
{"id": "2511.08713", "categories": ["cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.08713", "abs": "https://arxiv.org/abs/2511.08713", "authors": ["Gabriel Rodriguez-Canal", "David Katz", "Nick Brown"], "title": "An MLIR pipeline for offloading Fortran to FPGAs via OpenMP", "comment": "Author accepted version of paper published in SC25 LLVM workshop", "summary": "With the slowing of Moore's Law, heterogeneous computing platforms such as Field Programmable Gate Arrays (FPGAs) have gained increasing interest for accelerating HPC workloads. In this work we present, to the best of our knowledge, the first implementation of selective code offloading to FPGAs via the OpenMP target directive within MLIR. Our approach combines the MLIR OpenMP dialect with a High-Level Synthesis (HLS) dialect to provide a portable compilation flow targeting FPGAs. Unlike prior OpenMP FPGA efforts that rely on custom compilers, by contrast we integrate with MLIR and so support any MLIR-compatible front end, demonstrated here with Flang. Building upon a range of existing MLIR building blocks significantly reduces the effort required and demonstrates the composability benefits of the MLIR ecosystem. Our approach supports manual optimisation of offloaded kernels through standard OpenMP directives, and this work establishes a flexible and extensible path for directive-based FPGA acceleration integrated within the MLIR ecosystem.", "AI": {"tldr": "\u5728MLIR\u4e2d\u901a\u8fc7OpenMP\u76ee\u6807\u6307\u4ee4\u5b9e\u73b0\u9009\u62e9\u6027\u4ee3\u7801\u5378\u8f7d\u5230FPGA\u7684\u9996\u4e2a\u5b9e\u73b0\uff0c\u7ed3\u5408OpenMP\u65b9\u8a00\u548cHLS\u65b9\u8a00\u63d0\u4f9b\u53ef\u79fb\u690d\u7684FPGA\u7f16\u8bd1\u6d41\u7a0b\u3002", "motivation": "\u968f\u7740\u6469\u5c14\u5b9a\u5f8b\u653e\u7f13\uff0cFPGA\u7b49\u5f02\u6784\u8ba1\u7b97\u5e73\u53f0\u5728\u52a0\u901fHPC\u5de5\u4f5c\u8d1f\u8f7d\u65b9\u9762\u53d7\u5230\u5173\u6ce8\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684FPGA\u52a0\u901f\u65b9\u6cd5\u3002", "method": "\u5c06MLIR OpenMP\u65b9\u8a00\u4e0e\u9ad8\u7ea7\u7efc\u5408(HLS)\u65b9\u8a00\u7ed3\u5408\uff0c\u652f\u6301\u4efb\u4f55MLIR\u517c\u5bb9\u524d\u7aef(\u5982Flang)\uff0c\u5229\u7528\u73b0\u6709MLIR\u6784\u5efa\u5757\u51cf\u5c11\u5f00\u53d1\u5de5\u4f5c\u91cf\u3002", "result": "\u5b9e\u73b0\u4e86\u57fa\u4e8e\u6307\u4ee4\u7684FPGA\u52a0\u901f\uff0c\u652f\u6301\u901a\u8fc7\u6807\u51c6OpenMP\u6307\u4ee4\u624b\u52a8\u4f18\u5316\u5378\u8f7d\u5185\u6838\uff0c\u5c55\u793a\u4e86MLIR\u751f\u6001\u7cfb\u7edf\u7684\u53ef\u7ec4\u5408\u6027\u4f18\u52bf\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u5c06\u57fa\u4e8e\u6307\u4ee4\u7684FPGA\u52a0\u901f\u96c6\u6210\u5230MLIR\u751f\u6001\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2511.09203", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.09203", "abs": "https://arxiv.org/abs/2511.09203", "authors": ["Robert Atkey", "Roly Perera"], "title": "Galois Slicing as Automatic Differentiation", "comment": null, "summary": "Galois slicing is a technique for program slicing for provenance, developed by Perera and collaborators. Galois slicing aims to explain program executions by demonstrating how to track approximations of the input and output forwards and backwards along a particular execution. In this paper, we explore an analogy between Galois slicing and differentiable programming, seeing the implementation of forwards and backwards slicing as a kind of automatic differentiation. Using the CHAD approach to automatic differentiation due to V\u00e1k\u00e1r and collaborators, we reformulate Galois slicing via a categorical semantics. In doing so, we are able to explore extensions of the Galois slicing idea to quantitative interval analysis, and to clarify the implicit choices made in existing instantiations of this approach.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7CHAD\u81ea\u52a8\u5fae\u5206\u65b9\u6cd5\u91cd\u65b0\u6784\u5efaGalois\u5207\u7247\uff0c\u5efa\u7acb\u5176\u4e0e\u53ef\u5fae\u5206\u7f16\u7a0b\u7684\u7c7b\u6bd4\u5173\u7cfb\uff0c\u5e76\u6269\u5c55\u4e86\u5b9a\u91cf\u533a\u95f4\u5206\u6790\u7684\u5e94\u7528\u3002", "motivation": "\u63a2\u7d22Galois\u5207\u7247\u4e0e\u53ef\u5fae\u5206\u7f16\u7a0b\u4e4b\u95f4\u7684\u7c7b\u6bd4\u5173\u7cfb\uff0c\u5c06\u524d\u5411\u548c\u540e\u5411\u5207\u7247\u89c6\u4e3a\u4e00\u79cd\u81ea\u52a8\u5fae\u5206\u8fc7\u7a0b\uff0c\u4ee5\u6f84\u6e05\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u9690\u5f0f\u9009\u62e9\u3002", "method": "\u4f7f\u7528V\u00e1k\u00e1r\u7b49\u4eba\u63d0\u51fa\u7684CHAD\u81ea\u52a8\u5fae\u5206\u65b9\u6cd5\uff0c\u901a\u8fc7\u8303\u7574\u8bed\u4e49\u91cd\u65b0\u6784\u5efaGalois\u5207\u7247\uff0c\u5e76\u6269\u5c55\u5230\u5b9a\u91cf\u533a\u95f4\u5206\u6790\u3002", "result": "\u6210\u529f\u5efa\u7acb\u4e86Galois\u5207\u7247\u4e0e\u81ea\u52a8\u5fae\u5206\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u6f84\u6e05\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u9009\u62e9\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u5b9a\u91cf\u533a\u95f4\u5206\u6790\u7684\u6269\u5c55\u3002", "conclusion": "Galois\u5207\u7247\u53ef\u4ee5\u6709\u6548\u5730\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u6846\u67b6\u91cd\u65b0\u6784\u5efa\uff0c\u8fd9\u79cd\u89c6\u89d2\u4e0d\u4ec5\u6f84\u6e05\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd8\u542f\u53d1\u4e86\u65b0\u7684\u5e94\u7528\u6269\u5c55\u3002"}}
{"id": "2511.08936", "categories": ["cs.DC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.08936", "abs": "https://arxiv.org/abs/2511.08936", "authors": ["Liuzixuan Lin", "Andrew A. Chien"], "title": "Distribution and Management of Datacenter Load Decoupling", "comment": null, "summary": "The exploding power consumption of AI and cloud datacenters (DCs) intensifies the long-standing concerns about their carbon footprint, especially because DCs' need for constant power clashes with volatile renewable generation needed for grid decarbonization. DC flexibility (a.k.a. load adaptation) is a key to reducing DC carbon emissions by improving grid renewable absorption.\n  DC flexibility can be created, without disturbing datacenter capacity by decoupling a datacenter's power capacity and grid load with a collection of energy resources. Because decoupling can be costly, we study how to best distribute and manage decoupling to maximize benefits for all. Key considerations include site variation and datacenter-grid cooperation.\n  We first define and compute the power and energy needs of datacenter load decoupling, and then we evaluate designed distribution and management approaches. Evaluation shows that optimized distribution can deliver >98% of the potential grid carbon reduction with 70% of the total decoupling need. For management, DC-grid cooperation (2-way sharing and control vs. 1-way info sharing) enables 1.4x grid carbon reduction. Finally, we show that decoupling may be economically viable, as on average datacenters can get power cost and carbon emissions benefits greater than their local costs of decoupling. However, skew across sites suggests grid intervention may be required.", "AI": {"tldr": "\u6570\u636e\u4e2d\u5fc3\u901a\u8fc7\u80fd\u6e90\u8d44\u6e90\u89e3\u8026\u529f\u7387\u5bb9\u91cf\u4e0e\u7535\u7f51\u8d1f\u8f7d\uff0c\u5b9e\u73b0\u7075\u6d3b\u6027\u4ee5\u964d\u4f4e\u78b3\u6392\u653e\uff0c\u4f18\u5316\u5206\u5e03\u548c\u7ba1\u7406\u7b56\u7565\u53ef\u663e\u8457\u63d0\u5347\u53ef\u518d\u751f\u80fd\u6e90\u5438\u6536\u548c\u7ecf\u6d4e\u6548\u76ca\u3002", "motivation": "AI\u548c\u4e91\u6570\u636e\u4e2d\u5fc3\u80fd\u8017\u7206\u70b8\u5f0f\u589e\u957f\u52a0\u5267\u4e86\u78b3\u8db3\u8ff9\u95ee\u9898\uff0c\u6570\u636e\u4e2d\u5fc3\u6052\u5b9a\u7535\u529b\u9700\u6c42\u4e0e\u6ce2\u52a8\u6027\u53ef\u518d\u751f\u80fd\u6e90\u7684\u77db\u76fe\u9700\u8981\u901a\u8fc7\u8d1f\u8f7d\u9002\u5e94\u6027\u6765\u89e3\u51b3\u3002", "method": "\u5b9a\u4e49\u5e76\u8ba1\u7b97\u6570\u636e\u4e2d\u5fc3\u8d1f\u8f7d\u89e3\u8026\u7684\u529f\u7387\u548c\u80fd\u91cf\u9700\u6c42\uff0c\u8bc4\u4f30\u4f18\u5316\u5206\u5e03\u548c\u7ba1\u7406\u65b9\u6cd5\uff0c\u5305\u62ec\u7ad9\u70b9\u5dee\u5f02\u548c\u7535\u7f51\u5408\u4f5c\u7b56\u7565\u3002", "result": "\u4f18\u5316\u5206\u5e03\u53ef\u5b9e\u73b098%\u7684\u6f5c\u5728\u7535\u7f51\u78b3\u51cf\u6392\uff0c\u4ec5\u970070%\u7684\u603b\u89e3\u8026\u9700\u6c42\uff1b\u7535\u7f51\u5408\u4f5c\u7ba1\u7406\u4f7f\u78b3\u51cf\u6392\u6548\u679c\u63d0\u53471.4\u500d\uff1b\u7ecf\u6d4e\u4e0a\u5e73\u5747\u6536\u76ca\u5927\u4e8e\u672c\u5730\u6210\u672c\u3002", "conclusion": "\u6570\u636e\u4e2d\u5fc3\u8d1f\u8f7d\u89e3\u8026\u662f\u964d\u4f4e\u78b3\u6392\u653e\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u7ad9\u70b9\u95f4\u5dee\u5f02\u53ef\u80fd\u9700\u8981\u7535\u7f51\u5e72\u9884\u4ee5\u786e\u4fdd\u7ecf\u6d4e\u53ef\u884c\u6027\u3002"}}
{"id": "2511.08948", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.08948", "abs": "https://arxiv.org/abs/2511.08948", "authors": ["Jay Tharwani", "Shobhit Aggarwal", "Arnab A Purkayastha"], "title": "Evaluating HPC-Style CPU Performance and Cost in Virtualized Cloud Infrastructures", "comment": "7 pages", "summary": "This paper evaluates HPC-style CPU performance and cost in virtualized cloud infrastructures using a subset of OpenMP workloads in the SPEC ACCEL suite. Four major cloud providers by market share AWS, Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI) are compared across Intel, AMD, and ARM general purpose instance types under both on-demand and one-year discounted pricing. AWS consistently delivers the shortest runtime in all three instance types, yet charges a premium, especially for on-demand usage. OCI emerges as the most economical option across all CPU families, although it generally runs workloads more slowly than AWS. Azure often exhibits mid-range performance and cost, while GCP presents a mixed profile: it sees a notable boost when moving from Intel to AMD. On the other hand, its ARM instance is more than twice as slow as its own AMD offering and remains significantly more expensive. AWS's internal comparisons reveal that its ARM instance can outperform its Intel and AMD siblings by up to 49 percent in runtime. These findings highlight how instance choices and provider selection can yield substantial variations in both runtime and price, indicating that workload priorities, whether raw speed or cost minimization, should guide decisions on instance types.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u56db\u79cd\u4e3b\u8981\u4e91\u63d0\u4f9b\u5546\uff08AWS\u3001Azure\u3001GCP\u3001OCI\uff09\u5728\u865a\u62df\u5316\u4e91\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684HPC\u98ce\u683cCPU\u6027\u80fd\u548c\u6210\u672c\uff0c\u4f7f\u7528SPEC ACCEL\u5957\u4ef6\u4e2d\u7684OpenMP\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u6bd4\u8f83\u4e86Intel\u3001AMD\u548cARM\u5b9e\u4f8b\u7c7b\u578b\u5728\u6309\u9700\u548c\u6298\u6263\u5b9a\u4ef7\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u4e91\u73af\u5883\u4e2d\u4e0d\u540cCPU\u67b6\u6784\u5b9e\u4f8b\u7684\u6027\u80fd\u548c\u6210\u672c\u5dee\u5f02\uff0c\u4e3aHPC\u5de5\u4f5c\u8d1f\u8f7d\u9009\u62e9\u6700\u4f18\u4e91\u5b9e\u4f8b\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4f7f\u7528SPEC ACCEL\u5957\u4ef6\u4e2d\u7684OpenMP\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5728AWS\u3001Azure\u3001GCP\u548cOCI\u56db\u79cd\u4e91\u5e73\u53f0\u4e0a\u6d4b\u8bd5Intel\u3001AMD\u548cARM\u5b9e\u4f8b\u7c7b\u578b\uff0c\u6bd4\u8f83\u6309\u9700\u548c\u4e00\u5e74\u6298\u6263\u5b9a\u4ef7\u4e0b\u7684\u6027\u80fd\u548c\u6210\u672c\u3002", "result": "AWS\u5728\u6240\u6709\u4e09\u79cd\u5b9e\u4f8b\u7c7b\u578b\u4e2d\u8fd0\u884c\u65f6\u95f4\u6700\u77ed\u4f46\u6536\u8d39\u6700\u9ad8\uff1bOCI\u662f\u6700\u7ecf\u6d4e\u7684\u9009\u62e9\u4f46\u8fd0\u884c\u8f83\u6162\uff1bAzure\u8868\u73b0\u4e2d\u7b49\uff1bGCP\u4eceIntel\u5207\u6362\u5230AMD\u65f6\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5176ARM\u5b9e\u4f8b\u6bd4\u81ea\u8eabAMD\u5b9e\u4f8b\u6162\u4e24\u500d\u4e14\u66f4\u8d35\uff1bAWS\u7684ARM\u5b9e\u4f8b\u6bd4\u5176Intel\u548cAMD\u5b9e\u4f8b\u5feb\u8fbe49%\u3002", "conclusion": "\u5b9e\u4f8b\u9009\u62e9\u548c\u4e91\u63d0\u4f9b\u5546\u9009\u62e9\u4f1a\u663e\u8457\u5f71\u54cd\u8fd0\u884c\u65f6\u95f4\u548c\u6210\u672c\uff0c\u5e94\u6839\u636e\u5de5\u4f5c\u8d1f\u8f7d\u4f18\u5148\u7ea7\uff08\u539f\u59cb\u901f\u5ea6\u6216\u6210\u672c\u6700\u5c0f\u5316\uff09\u6765\u6307\u5bfc\u5b9e\u4f8b\u7c7b\u578b\u51b3\u7b56\u3002"}}
{"id": "2511.09447", "categories": ["cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.09447", "abs": "https://arxiv.org/abs/2511.09447", "authors": ["Lukas Gianinazzi", "Tal Ben-Nun", "Torsten Hoefler"], "title": "SPADA: A Spatial Dataflow Architecture Programming Language", "comment": null, "summary": "Spatial dataflow architectures like the Cerebras Wafer-Scale Engine achieve exceptional performance in AI and scientific applications by leveraging distributed memory across processing elements (PEs) and localized computation. However, programming these architectures remains challenging due to the need for explicit orchestration of data movement through reconfigurable networks-on-chip and asynchronous computation triggered by data arrival. Existing FPGA and CGRA programming models emphasize loop scheduling but overlook the unique capabilities of spatial dataflow architectures, particularly efficient dataflow over regular grids and intricate routing management.\n  We present SPADA, a programming language that provides precise control over data placement, dataflow patterns, and asynchronous operations while abstracting architecture-specific low-level details. We introduce a rigorous dataflow semantics framework for SPADA that defines routing correctness, data races, and deadlocks. Additionally, we design and implement a compiler targeting Cerebras CSL with multi-level lowering.\n  SPADA serves as both a high-level programming interface and an intermediate representation for domain-specific languages (DSLs), which we demonstrate with the GT4Py stencil DSL. SPADA enables developers to express complex parallel patterns -- including pipelined reductions and multi-dimensional stencils -- in 6--8x less code than CSL with near-ideal weak scaling across three orders of magnitude. By unifying programming for spatial dataflow architectures under a single model, SPADA advances both the theoretical foundations and practical usability of these emerging high-performance computing platforms.", "AI": {"tldr": "SPADA\u662f\u4e00\u79cd\u9762\u5411\u7a7a\u95f4\u6570\u636e\u6d41\u67b6\u6784\u7684\u7f16\u7a0b\u8bed\u8a00\uff0c\u901a\u8fc7\u62bd\u8c61\u5e95\u5c42\u67b6\u6784\u7ec6\u8282\uff0c\u63d0\u4f9b\u5bf9\u6570\u636e\u653e\u7f6e\u3001\u6570\u636e\u6d41\u6a21\u5f0f\u548c\u5f02\u6b65\u64cd\u4f5c\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u663e\u8457\u7b80\u5316\u7f16\u7a0b\u590d\u6742\u5ea6\u3002", "motivation": "\u7a7a\u95f4\u6570\u636e\u6d41\u67b6\u6784\uff08\u5982Cerebras\u6676\u5706\u7ea7\u5f15\u64ce\uff09\u5728AI\u548c\u79d1\u5b66\u8ba1\u7b97\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f16\u7a0b\u56f0\u96be\uff0c\u9700\u8981\u663e\u5f0f\u534f\u8c03\u6570\u636e\u79fb\u52a8\u548c\u5f02\u6b65\u8ba1\u7b97\u3002\u73b0\u6709FPGA\u548cCGRA\u7f16\u7a0b\u6a21\u578b\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u72ec\u7279\u80fd\u529b\u3002", "method": "\u63d0\u51faSPADA\u7f16\u7a0b\u8bed\u8a00\uff0c\u5efa\u7acb\u4e25\u683c\u7684\u6570\u636e\u6d41\u8bed\u4e49\u6846\u67b6\uff0c\u5b9a\u4e49\u8def\u7531\u6b63\u786e\u6027\u3001\u6570\u636e\u7ade\u4e89\u548c\u6b7b\u9501\uff0c\u5e76\u8bbe\u8ba1\u9488\u5bf9Cerebras CSL\u7684\u591a\u7ea7\u7f16\u8bd1\u5668\u3002", "result": "SPADA\u53ef\u5c06\u590d\u6742\u5e76\u884c\u6a21\u5f0f\uff08\u5982\u6d41\u6c34\u7ebf\u5f52\u7ea6\u548c\u591a\u7ef4\u6a21\u677f\uff09\u7684\u4ee3\u7801\u91cf\u51cf\u5c116-8\u500d\uff0c\u5728\u4e09\u4e2a\u6570\u91cf\u7ea7\u4e0a\u5b9e\u73b0\u63a5\u8fd1\u7406\u60f3\u7684\u5f31\u6269\u5c55\u6027\u80fd\u3002", "conclusion": "SPADA\u901a\u8fc7\u7edf\u4e00\u7a7a\u95f4\u6570\u636e\u6d41\u67b6\u6784\u7684\u7f16\u7a0b\u6a21\u578b\uff0c\u63a8\u8fdb\u4e86\u8fd9\u4e9b\u65b0\u5174\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e73\u53f0\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u53ef\u7528\u6027\u3002"}}
{"id": "2511.08998", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.08998", "abs": "https://arxiv.org/abs/2511.08998", "authors": ["Zilinghan Li", "Aditya Sinha", "Yijiang Li", "Kyle Chard", "Kibaek Kim", "Ravi Madduri"], "title": "Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science", "comment": null, "summary": "Federated learning (FL) is a promising approach to enabling collaborative model training without centralized data sharing, a crucial requirement in scientific domains where data privacy, ownership, and compliance constraints are critical. However, building user-friendly enterprise-level FL frameworks that are both scalable and privacy-preserving remains challenging, especially when bridging the gap between local prototyping and distributed deployment across heterogeneous client computing infrastructures. In this paper, based on our experiences building the Advanced Privacy-Preserving Federated Learning (APPFL) framework, we present our vision for an enterprise-grade, privacy-preserving FL framework designed to scale seamlessly across computing environments. We identify several key capabilities that such a framework must provide: (1) Scalable local simulation and prototyping to accelerate experimentation and algorithm design; (2) seamless transition from simulation to deployment; (3) distributed deployment across diverse, real-world infrastructures, from personal devices to cloud clusters and HPC systems; (4) multi-level abstractions that balance ease of use and research flexibility; and (5) comprehensive privacy and security through techniques such as differential privacy, secure aggregation, robust authentication, and confidential computing. We further discuss architectural designs to realize these goals. This framework aims to bridge the gap between research prototypes and enterprise-scale deployment, enabling scalable, reliable, and privacy-preserving AI for science.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4f01\u4e1a\u7ea7\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u6846\u67b6APPFL\u7684\u8bbe\u8ba1\u613f\u666f\uff0c\u65e8\u5728\u89e3\u51b3\u4ece\u672c\u5730\u539f\u578b\u5230\u5206\u5e03\u5f0f\u90e8\u7f72\u7684\u89c4\u6a21\u5316\u6311\u6218\uff0c\u652f\u6301\u8de8\u5f02\u6784\u8ba1\u7b97\u73af\u5883\u7684\u65e0\u7f1d\u6269\u5c55\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u867d\u7136\u80fd\u5728\u4e0d\u5171\u4eab\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u534f\u4f5c\u8bad\u7ec3\uff0c\u4f46\u6784\u5efa\u65e2\u7528\u6237\u53cb\u597d\u53c8\u5177\u5907\u4f01\u4e1a\u7ea7\u53ef\u6269\u5c55\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u7684\u6846\u67b6\u4ecd\u7136\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u8fde\u63a5\u672c\u5730\u539f\u578b\u548c\u5f02\u6784\u5ba2\u6237\u7aef\u90e8\u7f72\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u57fa\u4e8eAPPFL\u6846\u67b6\u5f00\u53d1\u7ecf\u9a8c\uff0c\u63d0\u51fa\u4f01\u4e1a\u7ea7\u9690\u79c1\u4fdd\u62a4FL\u6846\u67b6\u7684\u5173\u952e\u80fd\u529b\uff1a\u53ef\u6269\u5c55\u672c\u5730\u4eff\u771f\u3001\u65e0\u7f1d\u90e8\u7f72\u8fc7\u6e21\u3001\u8de8\u5f02\u6784\u57fa\u7840\u8bbe\u65bd\u90e8\u7f72\u3001\u591a\u7ea7\u62bd\u8c61\u3001\u7efc\u5408\u9690\u79c1\u5b89\u5168\u6280\u672f\uff08\u5dee\u5206\u9690\u79c1\u3001\u5b89\u5168\u805a\u5408\u3001\u8ba4\u8bc1\u3001\u673a\u5bc6\u8ba1\u7b97\uff09\u3002", "result": "\u63d0\u51fa\u4e86\u5b9e\u73b0\u4f01\u4e1a\u7ea7\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u80fd\u591f\u5f25\u5408\u7814\u7a76\u539f\u578b\u4e0e\u4f01\u4e1a\u7ea7\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e8\u5728\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u53ef\u9760\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u79d1\u5b66AI\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u4ece\u7814\u7a76\u8d70\u5411\u4f01\u4e1a\u5e94\u7528\u63d0\u4f9b\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.09143", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.09143", "abs": "https://arxiv.org/abs/2511.09143", "authors": ["Myungsu Kim", "Ikjun Yeom", "Younghoon Kim"], "title": "Flex-MIG: Enabling Distributed Execution on MIG", "comment": "13 pages, 11 figures, under review for MLSys 2026", "summary": "GPU clusters in multi-tenant settings often suffer from underutilization, making GPU-sharing technologies essential for efficient resource use. Among them, NVIDIA Multi-Instance GPU (MIG) has gained traction for providing hardware-level isolation that enables concurrent workloads without interference. However, MIG's hardware rigidity and the conventional one-to-one allocation model jointly lead to severe fragmentation and cluster-wide underutilization. We present Flex-MIG, a software-only framework that replaces one-to-one with a one-to-many allocation model and enables host-shared-memory collectives across MIG instances without hardware modification. Flex-MIG eliminates drain-required reconfiguration, reduces fragmentation, and improves makespan by up to 17% across diverse traces, showing that rethinking MIG's operational model as a software-coordinated layer substantially improves cluster efficiency.", "AI": {"tldr": "Flex-MIG\u662f\u4e00\u4e2a\u8f6f\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7\u5c06MIG\u4ece\u4e00\u5bf9\u4e00\u5206\u914d\u6a21\u578b\u6539\u4e3a\u4e00\u5bf9\u591a\u5206\u914d\u6a21\u578b\uff0c\u5e76\u652f\u6301\u8de8MIG\u5b9e\u4f8b\u7684\u4e3b\u673a\u5171\u4eab\u5185\u5b58\u96c6\u5408\u64cd\u4f5c\uff0c\u89e3\u51b3\u4e86GPU\u96c6\u7fa4\u4e2d\u7684\u788e\u7247\u5316\u548c\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u591a\u79df\u6237\u73af\u5883\u4e2d\u7684GPU\u96c6\u7fa4\u7ecf\u5e38\u5b58\u5728\u5229\u7528\u7387\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u800cNVIDIA MIG\u867d\u7136\u63d0\u4f9b\u786c\u4ef6\u7ea7\u9694\u79bb\uff0c\u4f46\u5176\u786c\u4ef6\u521a\u6027\u548c\u4f20\u7edf\u7684\u4e00\u5bf9\u4e00\u5206\u914d\u6a21\u578b\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u788e\u7247\u5316\u548c\u96c6\u7fa4\u8303\u56f4\u7684\u5229\u7528\u7387\u4f4e\u4e0b\u3002", "method": "Flex-MIG\u91c7\u7528\u7eaf\u8f6f\u4ef6\u65b9\u6cd5\uff0c\u7528\u4e00\u5bf9\u591a\u5206\u914d\u6a21\u578b\u66ff\u4ee3\u4e00\u5bf9\u4e00\u6a21\u578b\uff0c\u5e76\u5b9e\u73b0\u8de8MIG\u5b9e\u4f8b\u7684\u4e3b\u673a\u5171\u4eab\u5185\u5b58\u96c6\u5408\u64cd\u4f5c\uff0c\u65e0\u9700\u786c\u4ef6\u4fee\u6539\u3002", "result": "Flex-MIG\u6d88\u9664\u4e86\u9700\u8981\u6392\u7a7a\u7684\u91cd\u914d\u7f6e\u8fc7\u7a0b\uff0c\u51cf\u5c11\u4e86\u788e\u7247\u5316\uff0c\u5728\u4e0d\u540c\u8ddf\u8e2a\u6570\u636e\u4e0b\u5c06makespan\u63d0\u9ad8\u4e86\u9ad8\u8fbe17%\u3002", "conclusion": "\u5c06MIG\u7684\u64cd\u4f5c\u6a21\u578b\u91cd\u65b0\u8bbe\u8ba1\u4e3a\u8f6f\u4ef6\u534f\u8c03\u5c42\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u96c6\u7fa4\u6548\u7387\u3002"}}
{"id": "2511.09194", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.09194", "abs": "https://arxiv.org/abs/2511.09194", "authors": ["Simon K\u00f6nig", "Lukas Epple", "Christian Becker"], "title": "Minimize Your Critical Path with Combine-and-Exchange Locks", "comment": "19 pages, 15 figures", "summary": "Coroutines are experiencing a renaissance as many modern programming languages support the use of cooperative multitasking for highly parallel or asynchronous applications. One of the greatest advantages of this is that concurrency and synchronization is manged entirely in the userspace, omitting heavy-weight system calls. However, we find that state-of-the-art userspace synchronization primitives approach synchronization in the userspace from the perspective of kernel-level scheduling. This introduces unnecessary delays on the critical path of the application, limiting throughput. In this paper, we re-think synchronization for tasks that are scheduled entirely in the userspace (e.g., coroutines, fibers, etc.). We develop Combine-and-Exchange Scheduling (CES), a novel scheduling approach that ensures contended critical sections stay on the same thread of execution while parallelizable work is evenly spread across the remaining threads. We show that our approach can be applied to many existing languages and libraries, resulting in 3-fold performance improvements in application benchmarks as well as 8-fold performance improvements in microbenchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7528\u6237\u7a7a\u95f4\u4efb\u52a1\u8c03\u5ea6\u65b9\u6cd5CES\uff0c\u901a\u8fc7\u7ec4\u5408\u4ea4\u6362\u8c03\u5ea6\u6765\u4f18\u5316\u534f\u7a0b\u540c\u6b65\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u7528\u6237\u7a7a\u95f4\u540c\u6b65\u539f\u8bed\u5e26\u6765\u7684\u4e0d\u5fc5\u8981\u5ef6\u8fdf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u7f16\u7a0b\u8bed\u8a00\u5e7f\u6cdb\u652f\u6301\u534f\u7a0b\u7528\u4e8e\u9ad8\u5e76\u884c\u6216\u5f02\u6b65\u5e94\u7528\uff0c\u7528\u6237\u7a7a\u95f4\u540c\u6b65\u907f\u514d\u4e86\u91cd\u91cf\u7ea7\u7cfb\u7edf\u8c03\u7528\uff0c\u4f46\u73b0\u6709\u7528\u6237\u7a7a\u95f4\u540c\u6b65\u539f\u8bed\u4ecd\u4ece\u5185\u6838\u7ea7\u8c03\u5ea6\u89c6\u89d2\u5904\u7406\u540c\u6b65\uff0c\u5f15\u5165\u4e86\u4e0d\u5fc5\u8981\u7684\u5ef6\u8fdf\u5e76\u9650\u5236\u4e86\u541e\u5410\u91cf\u3002", "method": "\u5f00\u53d1\u4e86Combine-and-Exchange Scheduling (CES)\u65b9\u6cd5\uff0c\u786e\u4fdd\u7ade\u4e89\u4e34\u754c\u533a\u4fdd\u6301\u5728\u540c\u4e00\u4e2a\u6267\u884c\u7ebf\u7a0b\u4e0a\uff0c\u540c\u65f6\u5c06\u53ef\u5e76\u884c\u5de5\u4f5c\u5747\u5300\u5206\u5e03\u5230\u5176\u4ed6\u7ebf\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u73b0\u6709\u8bed\u8a00\u548c\u5e93\uff0c\u5728\u5e94\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e863\u500d\u6027\u80fd\u63d0\u5347\uff0c\u5728\u5fae\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e868\u500d\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CES\u4e3a\u5b8c\u5168\u5728\u7528\u6237\u7a7a\u95f4\u8c03\u5ea6\u7684\u4efb\u52a1\u91cd\u65b0\u8bbe\u8ba1\u4e86\u540c\u6b65\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u534f\u7a0b\u540c\u6b65\u7684\u6027\u80fd\u548c\u541e\u5410\u91cf\u3002"}}
{"id": "2511.09410", "categories": ["cs.DC", "cs.DS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.09410", "abs": "https://arxiv.org/abs/2511.09410", "authors": ["Yusuf Motiwala"], "title": "No Cords Attached: Coordination-Free Concurrent Lock-Free Queues", "comment": "10 pages, 2 figures, 3 tables. Lock-free concurrent queue with coordination-free memory reclamation", "summary": "The queue is conceptually one of the simplest data structures-a basic FIFO container. However, ensuring correctness in the presence of concurrency makes existing lock-free implementations significantly more complex than their original form. Coordination mechanisms introduced to prevent hazards such as ABA, use-after-free, and unsafe reclamation often dominate the design, overshadowing the queue itself. Many schemes compromise strict FIFO ordering, unbounded capacity, or lock-free progress to mask coordination overheads. Yet the true source of complexity lies in the pursuit of infinite protection against reclamation hazards--theoretically sound but impractical and costly. This pursuit not only drives unnecessary complexity but also creates a protection paradox where excessive protection reduces system resilience rather than improving it. While such costs may be tolerable in conventional workloads, the AI era has shifted the paradigm: training and inference pipelines involve hundreds to thousands of concurrent threads per node, and at this scale, protection and coordination overheads dominate, often far heavier than the basic queue operations themselves.\n  This paper introduces Cyclic Memory Protection (CMP), a coordination-free queue that preserves strict FIFO semantics, unbounded capacity, and lock-free progress while restoring simplicity. CMP reclaims the strict FIFO that other approaches sacrificed through bounded protection windows that provide practical reclamation guarantees. We prove strict FIFO and safety via linearizability and bounded reclamation analysis, and show experimentally that CMP outperforms state-of-the-art lock-free queues by up to 1.72-4x under high contention while maintaining scalability to hundreds of threads. Our work demonstrates that highly concurrent queues can return to their fundamental simplicity without weakening queue semantics.", "AI": {"tldr": "\u63d0\u51fa\u4e86Cyclic Memory Protection (CMP)\u65e0\u9501\u961f\u5217\uff0c\u901a\u8fc7\u6709\u754c\u4fdd\u62a4\u7a97\u53e3\u5b9e\u73b0\u4e25\u683cFIFO\u8bed\u4e49\u3001\u65e0\u754c\u5bb9\u91cf\u548c\u534f\u8c03\u81ea\u7531\uff0c\u5728\u9ad8\u5ea6\u5e76\u53d1\u573a\u666f\u4e0b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u68481.72-4\u500d\u3002", "motivation": "\u73b0\u6709\u65e0\u9501\u961f\u5217\u5b9e\u73b0\u8fc7\u4e8e\u590d\u6742\uff0c\u534f\u8c03\u673a\u5236\u5f00\u9500\u8fdc\u5927\u4e8e\u961f\u5217\u64cd\u4f5c\u672c\u8eab\uff0c\u7279\u522b\u662f\u5728AI\u65f6\u4ee3\u9700\u8981\u6570\u767e\u5230\u6570\u5343\u5e76\u53d1\u7ebf\u7a0b\u7684\u573a\u666f\u4e0b\uff0c\u8fd9\u79cd\u5f00\u9500\u53d8\u5f97\u4e0d\u53ef\u63a5\u53d7\u3002", "method": "\u4f7f\u7528\u5faa\u73af\u5185\u5b58\u4fdd\u62a4(CMP)\u673a\u5236\uff0c\u901a\u8fc7\u6709\u754c\u4fdd\u62a4\u7a97\u53e3\u63d0\u4f9b\u5b9e\u9645\u7684\u5185\u5b58\u56de\u6536\u4fdd\u8bc1\uff0c\u907f\u514d\u65e0\u9650\u4fdd\u62a4\u5e26\u6765\u7684\u590d\u6742\u6027\u548c\u5f00\u9500\u3002", "result": "CMP\u5728\u9ad8\u5ea6\u7ade\u4e89\u6761\u4ef6\u4e0b\u6027\u80fd\u6bd4\u6700\u5148\u8fdb\u7684\u65e0\u9501\u961f\u5217\u63d0\u53471.72-4\u500d\uff0c\u80fd\u591f\u6269\u5c55\u5230\u6570\u767e\u4e2a\u7ebf\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e25\u683cFIFO\u8bed\u4e49\u3002", "conclusion": "\u9ad8\u5ea6\u5e76\u53d1\u7684\u961f\u5217\u53ef\u4ee5\u56de\u5f52\u5176\u57fa\u672c\u7b80\u5355\u6027\uff0c\u800c\u65e0\u9700\u524a\u5f31\u961f\u5217\u8bed\u4e49\uff0c\u6709\u754c\u4fdd\u62a4\u6bd4\u65e0\u9650\u4fdd\u62a4\u66f4\u5b9e\u7528\u548c\u9ad8\u6548\u3002"}}
{"id": "2511.09485", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.09485", "abs": "https://arxiv.org/abs/2511.09485", "authors": ["Miroslav Popovic", "Marko Popovic", "Pavle Vasiljevic", "Miodrag Djukic"], "title": "Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links", "comment": "4 pages, 1 figure, 3 tables", "summary": "The Python Testbed for Federated Learning Algorithms is a simple FL framework targeting edge systems, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the universal TDM communication in the current time slot. The first two were formally verified in a previous paper using the CSP process algebra, and in this paper, we use the same approach to formally verify the third one, in two phases. In the first phase, we construct the CSP model as a faithful representation of the real Python code. In the second phase, the model checker PAT automatically proves correctness of the third generic algorithm by proving its deadlock freeness (safety property) and successful termination (liveness property).", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528CSP\u8fc7\u7a0b\u4ee3\u6570\u5bf9\u8054\u90a6\u5b66\u4e60Python\u6d4b\u8bd5\u5e73\u53f0\u7684\u7b2c\u4e09\u4e2a\u901a\u7528\u7b97\u6cd5\uff08TDM\u901a\u4fe1\uff09\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u5206\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u6784\u5efaCSP\u6a21\u578b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528PAT\u6a21\u578b\u68c0\u67e5\u5668\u9a8c\u8bc1\u5176\u65e0\u6b7b\u9501\u548c\u6210\u529f\u7ec8\u6b62\u3002", "motivation": "\u5bf9\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u7b2c\u4e09\u4e2a\u901a\u7528\u7b97\u6cd5\uff08TDM\u901a\u4fe1\uff09\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u786e\u4fdd\u5176\u6b63\u786e\u6027\uff0c\u6b64\u524d\u5df2\u5bf9\u524d\u4e24\u4e2a\u7b97\u6cd5\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "method": "\u91c7\u7528CSP\u8fc7\u7a0b\u4ee3\u6570\u6784\u5efa\u6a21\u578b\uff0c\u5206\u4e24\u9636\u6bb5\uff1a1\uff09\u6784\u5efa\u4e0ePython\u4ee3\u7801\u5bf9\u5e94\u7684CSP\u6a21\u578b\uff1b2\uff09\u4f7f\u7528PAT\u6a21\u578b\u68c0\u67e5\u5668\u9a8c\u8bc1\u7b97\u6cd5\u7684\u6b7b\u9501\u81ea\u7531\u6027\uff08\u5b89\u5168\u6027\uff09\u548c\u6210\u529f\u7ec8\u6b62\u6027\uff08\u6d3b\u6027\uff09\u3002", "result": "PAT\u6a21\u578b\u68c0\u67e5\u5668\u81ea\u52a8\u8bc1\u660e\u4e86\u7b2c\u4e09\u4e2a\u901a\u7528\u7b97\u6cd5\u7684\u6b63\u786e\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u65e0\u6b7b\u9501\u548c\u6210\u529f\u7ec8\u6b62\u3002", "conclusion": "\u6210\u529f\u5b8c\u6210\u4e86\u5bf9\u8054\u90a6\u5b66\u4e60Python\u6d4b\u8bd5\u5e73\u53f0\u4e2dTDM\u901a\u4fe1\u7b97\u6cd5\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u7684\u5b89\u5168\u6027\u548c\u6d3b\u6027\u3002"}}
{"id": "2511.09557", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09557", "abs": "https://arxiv.org/abs/2511.09557", "authors": ["Prajwal Singhania", "Siddharth Singh", "Lannie Dalton Hough", "Akarsh Srivastava", "Harshitha Menon", "Charles Fredrick Jekel", "Abhinav Bhatele"], "title": "LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication", "comment": "12 Figures", "summary": "As large language models (LLMs) continue to grow in size, distributed inference has become increasingly important. Model-parallel strategies must now efficiently scale not only across multiple GPUs but also across multiple nodes. In this work, we present a detailed performance study of multi-node distributed inference using LLMs on GPU-based supercomputers. We conduct experiments with several state-of-the-art inference engines alongside YALIS, a research-oriented prototype engine designed for controlled experimentation. We analyze the strong-scaling behavior of different model-parallel schemes and identify key bottlenecks. Since all-reduce operations are a common performance bottleneck, we develop NVRAR, a hierarchical all-reduce algorithm based on recursive doubling with NVSHMEM. NVRAR achieves up to 1.9x-3.6x lower latency than NCCL for message sizes between 128 KB and 2 MB on HPE Slingshot and InfiniBand interconnects. Integrated into YALIS, NVRAR achieves up to a 1.72x reduction in end-to-end batch latency for the Llama 3.1 405B model in multi-node decode-heavy workloads using tensor parallelism.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u8282\u70b9\u5206\u5e03\u5f0f\u63a8\u7406\u7684\u6027\u80fd\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8eNVSHMEM\u7684\u5206\u5c42all-reduce\u7b97\u6cd5NVRAR\uff0c\u5728Llama 3.1 405B\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad81.72\u500d\u7684\u5ef6\u8fdf\u964d\u4f4e\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u4e0d\u65ad\u589e\u5927\uff0c\u5206\u5e03\u5f0f\u63a8\u7406\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u9700\u8981\u8de8\u591a\u4e2aGPU\u548c\u8282\u70b9\u8fdb\u884c\u9ad8\u6548\u6269\u5c55\u3002", "method": "\u4f7f\u7528\u591a\u4e2a\u5148\u8fdb\u63a8\u7406\u5f15\u64ce\u548cYALIS\u539f\u578b\u5f15\u64ce\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u4e0d\u540c\u6a21\u578b\u5e76\u884c\u65b9\u6848\u7684\u5f3a\u6269\u5c55\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u9012\u5f52\u500d\u589e\u548cNVSHMEM\u7684NVRAR\u5206\u5c42all-reduce\u7b97\u6cd5\u3002", "result": "NVRAR\u5728HPE Slingshot\u548cInfiniBand\u4e92\u8fde\u4e0a\uff0c\u5bf9\u4e8e128KB\u52302MB\u7684\u6d88\u606f\u5927\u5c0f\uff0c\u6bd4NCCL\u5ef6\u8fdf\u964d\u4f4e1.9x-3.6x\uff1b\u5728Llama 3.1 405B\u6a21\u578b\u7684\u591a\u8282\u70b9\u89e3\u7801\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\uff0c\u7aef\u5230\u7aef\u6279\u91cf\u5ef6\u8fdf\u6700\u9ad8\u964d\u4f4e1.72\u500d\u3002", "conclusion": "NVRAR\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86all-reduce\u64cd\u4f5c\u8fd9\u4e00\u5e38\u89c1\u6027\u80fd\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8282\u70b9\u5206\u5e03\u5f0f\u63a8\u7406\u7684\u6548\u7387\u3002"}}
