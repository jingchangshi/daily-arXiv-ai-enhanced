<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Typing Fallback Functions: A Semantic Approach to Type Safe Smart Contracts](https://arxiv.org/abs/2512.04755)
*Stian Lybech,Daniele Gorla,Luca Aceto*

Main category: cs.PL

TL;DR: 该论文提出了一种在智能合约环境中使用语义类型来确保类型安全的方法，通过证明携带代码机制，让合约创建者提供类型安全的形式化证明，用户只需验证证明证书。


<details>
  <summary>Details</summary>
Motivation: 智能合约中某些语言构造（如fallback函数）在静态类型检查中无法被类型化，存在类型安全风险。区块链环境的不可变性使得证明携带代码机制特别适合。

Method: 1) 为TINYSOL语言（Solidity的简化版）提供类型化操作语义；2) 使用共归纳定义的类型解释来表达安全证明；3) 采用类似双相似性的up-to技术来紧凑表示证明；4) 将方法应用于基于fallback函数的典型指针到实现模式。

Result: 开发了一套理论框架，能够在区块链/智能合约环境中实现语义类型检查，确保信息流控制和非干扰性安全属性，并展示了如何类型化fallback函数模式。

Conclusion: 主要贡献不是安全定理本身，而是提出了在区块链/智能合约环境中实现证明携带代码方法所需的理论发展，为其他安全属性的验证提供了基础框架。

Abstract: This paper develops semantic typing in a smart-contract setting to ensure type safety of code that uses statically untypable language constructs, such as the fallback function. The idea is that the creator of a contract on the blockchain equips code containing such constructs with a formal proof of its type safety, given in terms of the semantics of types. Then, a user of the contract only needs to check the validity of the provided `proof certificate' of type safety. This is a form of proof-carrying code, which naturally fits with the immutable nature of the blockchain environment.
  As a concrete application of our approach, we focus on ensuring information flow control and non-interference for the language TINYSOL, a distilled version of the Solidity language, through security types. We provide the semantics of types in terms of a typed operational semantics of TINYSOL, and a way for expressing the proofs of safety as coinductively-defined typing interpretations and for representing them compactly via up-to techniques, similar to those used for bisimilarity. We also show how our machinery can be used to type the typical pointer-to-implementation pattern based on the fallback function. However, our main contribution is not the safety theorem per se (and so security properties different from non-interference can be considered as well), but rather the presentation of the theoretical developments necessary to make this approach work in a blockchain/smart-contract setting.

</details>


### [2] [Optimizations and extensions for fair join pattern matching](https://arxiv.org/abs/2512.04876)
*Ioannis Karras*

Main category: cs.PL

TL;DR: 该论文优化了Haller等人的公平连接模式匹配算法，实现了10倍性能提升，接近Rete算法性能，同时增强了基准测试套件和连接模式语法，并展示了在微服务架构中的应用。


<details>
  <summary>Details</summary>
Motivation: 连接模式在并发和分布式系统编程中探索不足，现有公平连接模式匹配算法在时间效率方面研究不够，性能不如Rete算法，且Rete算法需要大量手动适配。

Method: 优化Haller等人的状态树基匹配算法，增强基准测试套件（增加新特性、可扩展性和用户友好性），扩展连接模式实现（改进语法和动态模式切换）。

Result: 在某些基准测试中实现高达10倍的性能提升，接近Rete算法在常规基准测试中的性能，同时保持处理复杂条件守卫时的优势。

Conclusion: 优化的状态树基匹配算法在性能上显著提升，接近Rete算法水平，同时保持了灵活性和处理复杂条件守卫的优势，并在微服务架构中展示了实际应用价值。

Abstract: Join patterns are an underexplored approach for the programming of concurrent and distributed systems. When applied to the actor model, join patterns offer the novel capability of matching combinations of messages in the mailbox of an actor. Previous work by Philipp Haller et al. in the paper "Fair Join Pattern Matching for Actors" (ECOOP 2024) explored join patterns with conditional guards in an actor-based setting with a specification of fair and deterministic matching semantics. Nevertheless, the question of time efficiency in fair join pattern matching has remained underexplored. The stateful tree-based matching algorithm of Haller et al. performs worse than an implementation that adapts the Rete algorithm to the regular version of a join pattern matching benchmark, while outperforming on a variant with heavy conditional guards, which take longer to evaluate. Nevertheless, conforming Rete to the problem of join pattern matching requires heavy manual adaptation.
  In this thesis, we enhance and optimize the stateful tree-based matching algorithm of Haller et al. to achieve up to tenfold performance improvements on certain benchmarks, approaching the performance of Rete on regular benchmarks while maintaining the advantages of versatility and performance with heavy guards. We also enhance the benchmark suite, adding new features and enhancing its extensibility and user-friendliness. We extend the join pattern implementation with a less ambiguous syntax as well as dynamic pattern switching. Finally, we present a new complex model use case for join patterns, showing their applicability in a microservice web architecture.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Toward Sustainability-Aware LLM Inference on Edge Clusters](https://arxiv.org/abs/2512.04088)
*Kolichala Rajashekar,Nafiseh Sharghivand,Radu Prodan,Reza Farahani*

Main category: cs.DC

TL;DR: 该论文提出了一种面向边缘集群的可持续性感知LLM推理框架，通过碳感知和延迟感知的路由策略，在NVIDIA Jetson Orin NX和Ada 2000设备上平衡推理延迟与碳足迹。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理需要大量计算资源，导致显著的碳排放和运营成本。云端推理存在延迟和带宽限制，而边缘集群虽然能实现本地化执行，但在性能、能效和设备约束之间存在权衡。需要一种可持续的LLM推理方法。

Method: 提出了面向边缘集群的可持续性感知LLM推理框架，包含碳感知和延迟感知的路由策略。通过经验基准测试测量不同提示和批处理配置下的能耗和执行时间，将基准贪婪策略与碳感知、延迟感知策略进行比较。

Result: 实验评估显示，批处理大小为4个提示时能在吞吐量和能效之间取得平衡，而更大的批处理可能导致GPU内存饱和。碳感知和延迟感知策略相比基线策略能更好地平衡延迟和碳足迹。

Conclusion: 通过碳感知和延迟感知的路由策略，可以在边缘集群上实现可持续的LLM推理，平衡推理延迟和碳足迹，批处理大小为4是较优配置。

Abstract: Large language models (LLMs) require substantial computational resources, leading to significant carbon emissions and operational costs. Although training is energy-intensive, the long-term environmental burden arises from inference, amplified by the massive global query volume. Cloud-based inference offers scalability but suffers from latency and bandwidth constraints due to centralized processing and continuous data transfer. Edge clusters instead can mitigate these limitations by enabling localized execution, yet they face trade-offs between performance, energy efficiency, and device constraints. This short paper presents a sustainability-aware LLM inference for edge clusters comprising NVIDIA Jetson Orin NX (8GB) and Nvidia Ada 2000 (16GB) devices. It aims to balance inference latency and carbon footprint through carbon- and latency-aware routing strategies, guided by empirical benchmarking of energy consumption and execution time across diverse prompts and batch (i.e., group of prompts) configurations. We compared baseline greedy strategies to carbon-aware and latency-aware strategies in prompt routing to specific hardware based on benchmarking information. Experimental evaluation shows that a batch size of four prompts achieves a trade-off between throughput, energy efficiency, while larger batches risk GPU memory saturation.

</details>


### [4] [Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud](https://arxiv.org/abs/2512.04089)
*Mario Colosi,Reza Farahani,Lauri Loven,Radu Prodan,Massimo Villari*

Main category: cs.DC

TL;DR: 本文评估了WebAssembly在浏览器、边缘节点和云服务器上执行无服务器工作流的性能表现，发现AOT编译和实例预热能显著降低启动延迟，小负载时浏览器性能有竞争力，大负载时边缘和云节点的AOT执行表现更优。


<details>
  <summary>Details</summary>
Motivation: WebAssembly作为一种可移植、沙箱化且接近原生执行的二进制指令格式，适合在异构平台上执行无服务器工作流。但其性能和稳定性受启动开销、运行时执行模型（AOT/JIT编译）以及不同部署环境资源差异等因素影响，需要系统评估。

Method: 使用wasm32-wasi模块在浏览器、边缘节点和云服务器上一致执行WebAssembly无服务器工作流。浏览器中在web worker内执行，边缘和云环境中通过HTTP shim将帧流式传输到Wasm运行时。测量冷/热启动延迟、每步延迟、工作流完成时间、吞吐量和CPU/内存利用率。

Result: AOT编译和实例预热显著降低了启动延迟。对于小负载工作流，浏览器因完全内存数据交换而具有竞争力性能。随着负载增大，工作流进入计算和内存密集型阶段，边缘和云节点的AOT执行性能明显超越浏览器。

Conclusion: WebAssembly在不同环境中的性能表现取决于负载特性和执行模型。AOT编译和预热优化对启动性能至关重要，而工作负载大小决定了最佳执行环境选择——小负载适合浏览器，大负载更适合边缘/云节点。

Abstract: WebAssembly (Wasm) is a binary instruction format that enables portable, sandboxed, and near-native execution across heterogeneous platforms, making it well-suited for serverless workflow execution on browsers, edge nodes, and cloud servers. However, its performance and stability depend heavily on factors such as startup overhead, runtime execution model (e.g., Ahead-of-Time (AOT) and Just-in-Time (JIT) compilation), and resource variability across deployment contexts. This paper evaluates a Wasm-based serverless workflow executed consistently from the browser to edge and cloud instances. The setup uses wasm32-wasi modules: in the browser, execution occurs within a web worker, while on Edge and Cloud, an HTTP shim streams frames to the Wasm runtime. We measure cold- and warm-start latency, per-step delays, workflow makespan, throughput, and CPU/memory utilization to capture the end-to-end behavior across environments. Results show that AOT compilation and instance warming substantially reduce startup latency. For workflows with small payloads, the browser achieves competitive performance owing to fully in-memory data exchanges. In contrast, as payloads grow, the workflow transitions into a compute- and memory-intensive phase where AOT execution on edge and cloud nodes distinctly surpasses browser performance.

</details>


### [5] [Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions](https://arxiv.org/abs/2512.04093)
*Ali Akbar Vali,Sadoon Azizi,Mohammad Shojafar,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 该论文对微服务架构的雾计算和边缘计算中的资源管理策略进行了全面综述，重点关注能源效率，系统分类了136+篇研究，识别了关键挑战并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 物联网设备激增需要高效响应服务，雾计算和边缘计算作为分布式范式能降低延迟和能耗，但面临资源受限、异构性、动态负载和多样化QoS需求等资源管理挑战。

Method: 系统综述2020-2024年间136+项研究，将其分类为五个关键子领域：服务放置、资源供应、任务调度与卸载、资源分配、实例选择，基于优化技术、目标及方法优缺点进行分类。

Result: 识别出现有资源管理组件之间缺乏协同的问题，发现文献中的未解决挑战和空白，为研究人员和从业者提供了统一的能源感知视角。

Conclusion: 通过强调AI驱动优化、量子计算和无服务器计算等研究方向，为未来更集成、高效和可持续的微服务雾边缘计算资源管理解决方案铺平道路。

Abstract: The exponential growth of Internet of Things (IoT) devices has intensified the demand for efficient and responsive services. To address this demand, fog and edge computing have emerged as distributed paradigms that bring computational resources closer to end users, reducing latency, bandwidth limitations, and energy consumption. However, these paradigms present challenges in resource management due to resource constraints, computational heterogeneity, dynamic workloads, and diverse Quality of Service (QoS) requirements. This paper presents a comprehensive survey of state-of-the-art resource management strategies in microservices-based fog and edge computing, focusing on energy-efficient solutions. We systematically review and classify more than 136 studies (2020-2024) into five key subdomains: service placement, resource provisioning, task scheduling and offloading, resource allocation, and instance selection. Our categorization is based on optimization techniques, targeted objectives, and the strengths and limitations of each approach. In addition, we examine existing surveys and identify unresolved challenges and gaps in the literature. By highlighting the lack of synergy among fundamental resource management components, we outline promising research directions leveraging AI-driven optimization, quantum computing, and serverless computing. This survey serves as a comprehensive reference for researchers and practitioners by providing a unified and energy-aware perspective on resource management in microservices-based fog and edge computing, paving the way for more integrated, efficient, and sustainable future solutions.

</details>


### [6] [Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale](https://arxiv.org/abs/2512.04096)
*Sushant Kumar Gupta,Anil Raghunath Iyer,Chang Yu,Neel Bagora,Olivier Pomerleau,Vivek Kumar,Prunthaban Kanthakumar*

Main category: cs.DC

TL;DR: Fast ACS是一个基于文件的顺序消息传递系统，利用RPC和RMA通信原语，实现跨集群的低延迟消息传递，支持数千消费者，达到Tbps级流量。


<details>
  <summary>Details</summary>
Motivation: 实时系统需要低延迟消息传递，数据生产者需要将消息传递给可能分布在跨大都市和大陆边界的集群中的数千消费者。系统需要保证顺序性和至少一次交付，同时避免消费者过载。

Method: 设计Fast ACS（Ads Copy Service），一个基于文件的顺序消息传递系统，结合使用双向（集群间）和单向（集群内）通信原语：远程过程调用（RPC）和远程内存访问（RMA）。

Result: 系统已成功部署到数十个生产集群，每个集群可扩展到数千消费者，峰值时达到Tbps级集群内消费者流量。在全球范围内，根据消息量和消费者规模，可在几秒甚至亚秒级（p99）内将消息传递给消费者，资源成本低。

Conclusion: Fast ACS通过结合文件系统和RPC/RMA通信原语，实现了高效、可扩展的低延迟消息传递系统，满足大规模实时系统的需求。

Abstract: Low-latency message delivery is crucial for real-time systems. Data originating from a producer must be delivered to consumers, potentially distributed in clusters across metropolitan and continental boundaries. With the growing scale of computing, there can be several thousand consumers of the data. Such systems require a robust messaging system capable of transmitting messages containing data across clusters and efficiently delivering them to consumers. The system must offer guarantees like ordering and at-least-once delivery while avoiding overload on consumers, allowing them to consume messages at their own pace.
  This paper presents the design of Fast ACS (an abbreviation for Ads Copy Service), a file-based ordered message delivery system that leverages a combination of two-sided (inter-cluster) and one-sided (intra-cluster) communication primitives - namely, Remote Procedure Call and Remote Memory Access, respectively - to deliver messages. The system has been successfully deployed to dozens of production clusters and scales to accommodate several thousand consumers within each cluster, which amounts to Tbps-scale intra-cluster consumer traffic at peak. Notably, Fast ACS delivers messages to consumers across the globe within a few seconds or even sub-seconds (p99) based on the message volume and consumer scale, at a low resource cost.

</details>


### [7] [tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection](https://arxiv.org/abs/2512.04226)
*Ryan Swann,Muhammad Osama,Xiaohu Guo,Bryant Nelson,Lixun Zhang,Alex Brown,Yen Ong,Ali Yazdani,Sean Siddens,Ganesh Dasika,Alex Underwood*

Main category: cs.DC

TL;DR: tritonBLAS是一个基于架构参数的确定性分析模型，用于生成高性能GPU GEMM内核，无需运行时自动调优即可达到接近最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统GPU GEMM内核通常依赖耗时的运行时自动调优来获得高性能，这在大规模生产环境中成本高昂。需要一种能够预测最优配置而不需要实际运行调优的方法。

Method: 使用缓存层次结构、代码和数据相对位置等架构参数建立分析模型，明确建模架构拓扑、矩阵形状和算法分块行为之间的关系，基于此模型在Triton中实现轻量级GEMM框架。

Result: 在现代GPU上评估多种GEMM问题规模，tritonBLAS能够达到自动调优解决方案95%以上的性能，同时将自动调优时间降为零。

Conclusion: tritonBLAS是一个实用的即插即用替代方案，可用于生产环境的高性能计算和机器学习工作负载，消除了传统自动调优的开销。

Abstract: We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.

</details>


### [8] [Scaling MPI Applications on Aurora](https://arxiv.org/abs/2512.04291)
*Huda Ibeid,Anthony-Trung Nguyen,Aditya Nishtala,Premanand Sakarda,Larry Kaplan,Nilakantan Mahadevan,Michael Woodacre,Victor Anisimov,Kalyan Kumaran,JaeHyuk Kwack,Vitali Morozov,Servesh Muralidharan,Scott Parker*

Main category: cs.DC

TL;DR: Aurora超算系统是2024年部署在阿贡国家实验室的三大百亿亿次级系统之一，采用Intel Max系列GPU和CPU，配备HPE Slingshot高性能互连网络，在Top500和HPL MxP基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 介绍Aurora超算系统的设计细节，特别是网络架构和验证方法，展示其在AI和HPC模拟方面的强大能力，为开放科学研究提供新的计算能力水平。

Method: 详细描述Aurora系统架构：超过一万个节点，每个节点包含6个Intel数据中心Max系列GPU和2个Intel Xeon Max系列CPU，采用HPE Slingshot高性能互连网络（包含近85,000个Cassini NIC和5,600个Rosetta交换机），使用蜻蜓拓扑结构。

Result: Aurora在2024年6月Top500榜单中排名第二，在HPL MxP基准测试中排名第一；通过MPI基准测试和多种应用（HACC、AMR-Wind、LAMMPS、FMM等）展示了优异的吞吐量、延迟和带宽性能，能够扩展到大规模节点数量。

Conclusion: Aurora超算系统通过创新的硬件架构和网络设计，为AI和HPC模拟提供了前所未有的计算能力，能够支持大规模科学应用，实现突破性科学研究。

Abstract: The Aurora supercomputer, which was deployed at Argonne National Laboratory in 2024, is currently one of three Exascale machines in the world on the Top500 list. The Aurora system is composed of over ten thousand nodes each of which contains six Intel Data Center Max Series GPUs, Intel's first data center-focused discrete GPU, and two Intel Xeon Max Series CPUs, Intel's first Xeon processor to contain HBM memory. To achieve Exascale performance the system utilizes the HPE Slingshot high-performance fabric interconnect to connect the nodes. Aurora is currently the largest deployment of the Slingshot fabric to date with nearly 85,000 Cassini NICs and 5,600 Rosetta switches connected in a dragonfly topology. The combination of the Intel powered nodes and the Slingshot network enabled Aurora to become the second fastest system on the Top500 list in June of 2024 and the fastest system on the HPL MxP benchmark. The system is one of the most powerful systems in the world dedicated to AI and HPC simulations for open science. This paper presents details of the Aurora system design with a particular focus on the network fabric and the approach taken to validating it. The performance of the systems is demonstrated through the presentation of the results of MPI benchmarks as well as performance benchmarks including HPL, HPL-MxP, Graph500, and HPCG run on a large fraction of the system. Additionally results are presented for a diverse set of applications including HACC, AMR-Wind, LAMMPS, and FMM demonstrating that Aurora provides the throughput, latency, and bandwidth across system needed to allow applications to perform and scale to large node counts and providing new levels of capability and enabling breakthrough science.

</details>


### [9] [VLCs: Managing Parallelism with Virtualized Libraries](https://arxiv.org/abs/2512.04320)
*Yineng Yan,William Ruys,Hochan Lee,Ian Henriksen,Arthur Peters,Sean Stephens,Bozhi You,Henrique Fingler,Martin Burtscher,Milos Gligoric,Keshav Pingali,Mattan Erez,George Biros,Christopher J. Rossbach*

Main category: cs.DC

TL;DR: VLCs（虚拟库上下文）是一种进程子单元，用于封装库和资源分配，无需修改库代码即可控制资源使用，避免库间资源争用，提升并行性能。


<details>
  <summary>Details</summary>
Motivation: 随着并行机复杂性增加，程序员依赖库组合来利用并行性，但许多库设计时未考虑组合使用，假设独占所有资源，导致并发使用时产生争用和性能下降。现有方案需要修改库或操作系统，通常不可行。

Method: 提出虚拟库上下文（VLCs）作为进程子单元，封装库集合和相关资源分配。VLCs无需修改库代码即可控制库的资源使用，允许用户在不同库间划分资源以防止争用，或加载同一库的多个副本以支持并行执行线程不安全的代码。

Result: 实现了C++和Python的VLCs原型，实验表明VLCs在使用OpenMP、OpenBLAS和LibTorch的应用中可实现最高2.85倍的加速。

Conclusion: VLCs提供了一种无需修改库代码即可管理库资源使用的方法，有效解决了库组合时的资源争用问题，显著提升了并行应用的性能。

Abstract: As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible.
  We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process.
  In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.

</details>


### [10] [Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity](https://arxiv.org/abs/2512.04355)
*Gregory Bolet,Giorgis Georgakoudis,Konstantinos Parasyris,Harshitha Menon,Niranjan Hasabnis,Kirk W. Cameron,Gal Oren*

Main category: cs.DC

TL;DR: gpuFLOPBench是一个评估LLM预测GPU内核FLOP性能的基准测试，包含577个CUDA内核，要求模型在不运行代码的情况下预测单双精度FLOP计数，揭示现有模型在硬件特定微码效应推理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现代GPU软件开发需要开发者在运行内核前就能预测性能瓶颈，但当前的大型语言模型很少测试这种前瞻性推理能力。现有代码助手无法内化硬件特定的微码效应，需要专门的基准测试来评估和改进LLM在性能推理方面的能力。

Method: 创建gpuFLOPBench基准测试，包含从HeCBench中提取的577个CUDA内核，每个内核都标注了真实性能分析和八个执行属性。这些属性区分了可简单分析的代码和那些FLOP依赖于隐藏编译器或运行时行为的内核。评估当前闭源推理模型在这些内核上的FLOP预测能力。

Result: 最新LLM在简单内核上实现了完美分类，但当涉及除法、内在数学函数或公共子表达式等产生隐式FLOP的情况时，仍然会出现多个数量级的错误。这表明现有模型在推理硬件特定微码效应方面存在核心局限性。

Conclusion: gpuFLOPBench作为一个专注的测试平台，可用于开发能够像经验丰富的GPU开发者一样严谨推理性能的LLM工具。研究揭示了当前代码助手在硬件特定性能推理方面的不足，为未来改进提供了方向。

Abstract: Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to "count without running" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench

</details>


### [11] [A Structure-Aware Irregular Blocking Method for Sparse LU Factorization](https://arxiv.org/abs/2512.04389)
*Zhen Hu,Dongliang Xiong,Kai Huang,Changjun Wu,Xiaowen Jiang*

Main category: cs.DC

TL;DR: 提出一种针对稀疏LU分解的结构感知不规则分块方法，通过基于对角块的特征表征局部非零分布，动态调整块大小以平衡负载，在GPU上相比现有方法获得显著加速。


<details>
  <summary>Details</summary>
Motivation: 稀疏LU分解中，符号分解后的非零元素倾向于分布在矩阵对角线和右下区域，这种非均匀分布使得常规2D分块会导致块间负载不均衡，现有矩阵特征无法有效指导分块策略。

Method: 提出一种结构感知的不规则分块方法：1）引入新颖的对角块特征来有效表征稀疏矩阵的局部非零分布；2）基于此特征提出不规则分块方法，根据局部非零分布动态调整块大小；3）在密集区域使用细粒度块，稀疏区域使用粗粒度块，充分平衡依赖树中同层和跨层的块非零元素。

Result: 在单个NVIDIA A100 GPU上，相比PanguLU和最新SuperLU_DIST分别获得平均1.50倍和3.32倍的加速；在4个NVIDIA A100 GPU上，分别获得1.40倍和3.84倍的加速。

Conclusion: 提出的结构感知不规则分块方法能有效解决稀疏LU分解中的负载不均衡问题，通过基于对角块的特征表征和动态块大小调整，在GPU上实现了显著的性能提升。

Abstract: In sparse LU factorization, nonzero elements after symbolic factorization tend to distribute in diagonal and right-bottom region of sparse matrices. However, regular 2D blocking on this non-uniform distribution structure may lead to workload imbalance across blocks. Besides, existing matrix features fail to guide us effectively in blocking. In this paper, we propose a structure-aware irregular blocking method for numerical factorization. A novel diagonal block-based feature is introduced to effectively characterize the local nonzero distribution of sparse matrices. Based on this, we further propose an irregular blocking method that adjusts block sizes according to the local distribution of nonzeros. The strategy utilizes fine-grained blocks in dense regions and coarse-grained blocks in sparse regions, adequately balancing the nonzeros of blocks both within the same level and across levels in the dependency tree. Experiments demonstrate that, on a single NVIDIA A100 GPU, our proposed irregular blocking method achieves average speedups of 1.50x and 3.32x over PanguLU and the latest SuperLU_DIST, respectively. In addition, it achieves speedups of 1.40x and 3.84x over PanguLU and SuperLU_DIST on 4 NVIDIA A100 GPUs.

</details>


### [12] [Offloading to CXL-based Computational Memory](https://arxiv.org/abs/2512.04449)
*Suyeon Lee,Kangkyu Park,Kwangsik Shin,Ada Gavrilovska*

Main category: cs.DC

TL;DR: KAI系统通过异步回传协议优化CXL计算内存，减少数据移动开销，提升异构工作负载性能


<details>
  <summary>Details</summary>
Motivation: CXL计算内存(CCM)虽然能减少数据移动开销，但现有卸载机制无法充分利用不同CXL协议模型的权衡，需要更灵活的异步数据移动和流水线支持

Method: 提出异步回传协议，在底层CXL协议上分层数据和控制传输操作；设计KAI系统实现异步数据移动和轻量级流水线的主机-CCM交互

Result: KAI将端到端运行时间减少高达50.4%，CCM和主机空闲时间分别平均减少22.11倍和3.85倍

Conclusion: 异步回传协议和KAI系统能有效利用CXL协议权衡，显著提升异构工作负载在计算内存系统中的性能和效率

Abstract: CXL-based Computational Memory (CCM) enables near-memory processing within expanded remote memory, presenting opportunities to address data movement costs associated with disaggregated memory systems and to accelerate overall performance. However, existing operation offloading mechanisms are not capable of leveraging the trade-offs of different models based on different CXL protocols. This work first examines these tradeoffs and demonstrates their impact on end-to-end performance and system efficiency for workloads with diverse data and processing requirements. We propose a novel 'Asynchronous Back-Streaming' protocol by carefully layering data and control transfer operations on top of the underlying CXL protocols. We design KAI, a system that realizes the asynchronous back-streaming model that supports asynchronous data movement and lightweight pipelining in host-CCM interactions. Overall, KAI reduces end-to-end runtime by up to 50.4%, and CCM and host idle times by average 22.11x and 3.85x, respectively.

</details>


### [13] [Federated Learning for Terahertz Wireless Communication](https://arxiv.org/abs/2512.04984)
*O. Tansel Baydas,Ozgur B. Akan*

Main category: cs.DC

TL;DR: 本文分析了太赫兹通信与联邦学习结合时，宽带损伤对优化动态的影响，发现了频谱空洞会导致收敛误差，并提出了SNR加权聚合策略来解决问题。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信与联邦学习的结合有望实现超快速分布式学习，但实际宽带损伤对优化动态的理论影响尚未得到充分研究。本文旨在填补这一空白，分析频率选择性太赫兹效应（如波束倾斜、分子吸收和抖动）对联邦学习收敛性的影响。

Method: 开发了一个多载波随机框架，将本地梯度更新与频率选择性太赫兹效应（包括波束倾斜、分子吸收和抖动）显式耦合。通过理论分析揭示了收敛误差与子载波SNR谐波均值的关系，并提出了SNR加权聚合策略来抑制频谱空洞处的方差奇点。

Result: 研究发现存在一个关键的多样性陷阱：在标准无偏聚合下，收敛误差由子载波SNR的谐波均值驱动，单个频谱空洞（由严重波束倾斜引起）可能导致整个带宽对可靠模型更新无效。同时识别了基本带宽限制，表明频谱扩展超过临界点会因热噪声积分和带边增益崩溃而降低收敛性。SNR加权聚合策略能有效抑制频谱空洞处的方差奇点，在标准平均法失败的高倾斜区域恢复收敛。

Conclusion: 太赫兹联邦学习系统的性能受到物理层参数的显著影响，特别是波束倾斜引起的频谱空洞会严重损害收敛性。通过采用SNR加权聚合策略可以克服这些限制，在宽带太赫兹通信环境下实现可靠的联邦学习收敛。数值结果验证了所讨论物理层参数对系统性能的预期影响。

Abstract: The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [FLEX: Leveraging FPGA-CPU Synergy for Mixed-Cell-Height Legalization Acceleration](https://arxiv.org/abs/2512.04527)
*Xingyu Liu,Jiawei Liang,Linfeng Du,Yipu Zhang,Chaofang Ma,Hanwei Fan,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: FLEX是用于混合单元高度合法化任务的FPGA-CPU加速器，通过优化任务分配策略、多粒度流水线技术和针对计算密集型单元移位过程的优化，实现了显著的加速效果。


<details>
  <summary>Details</summary>
Motivation: 混合单元高度合法化任务在电子设计自动化中计算密集且耗时，现有CPU-GPU和多线程CPU解决方案在性能和可扩展性方面存在不足，需要更高效的异构加速方案。

Method: 1. 优化任务分配策略，在FPGA和CPU之间进行高效任务划分以发挥互补优势；2. 采用多粒度流水线技术加速最耗时的寻找最优放置位置步骤；3. 针对FOP中的计算密集型单元移位过程进行优化设计，使其与多粒度流水线框架无缝集成。

Result: FLEX相比最先进的CPU-GPU和多线程CPU合法化器，分别实现了最高18.3倍和5.4倍的加速，同时具有更好的可扩展性，并将合法化质量分别提高了4%和1%。

Conclusion: FLEX通过创新的FPGA-CPU异构加速架构，在混合单元高度合法化任务中实现了显著的性能提升和更好的可扩展性，同时提高了合法化质量，为电子设计自动化中的合法化问题提供了高效的解决方案。

Abstract: In this work, we present FLEX, an FPGA-CPU accelerator for mixed-cell-height legalization tasks. We address challenges from the following perspectives. First, we optimize the task assignment strategy and perform an efficient task partition between FPGA and CPU to exploit their complementary strengths. Second, a multi-granularity pipelining technique is employed to accelerate the most time-consuming step, finding optimal placement position (FOP), in legalization. At last, we particularly target the computationally intensive cell shifting process in FOP, optimizing the design to align it seamlessly with the multi-granularity pipelining framework for further speedup. Experimental results show that FLEX achieves up to 18.3x and 5.4x speedups compared to state-of-the-art CPU-GPU and multi-threaded CPU legalizers with better scalability, while improving legalization quality by 4% and 1%.

</details>


### [15] [Functional Stability of Software-Hardware Neural Network Implementation The NeuroComp Project](https://arxiv.org/abs/2512.04867)
*Bychkov Oleksii,Senysh Taras*

Main category: cs.AR

TL;DR: 提出一种通过神经元级硬件冗余确保神经网络功能稳定性的创新方法，使用独立微计算机实现每个神经元，保证硬件故障时的系统韧性


<details>
  <summary>Details</summary>
Motivation: 传统Dropout方法仅用于训练阶段的正则化，无法保证神经网络在运行时的硬件故障恢复能力。需要一种能在实际部署中应对硬件失效的稳定机制。

Method: 将每个神经元实现在独立的ESP32微计算机上，通过硬件冗余设计，即使单个计算节点故障，系统仍能继续运行。

Result: 系统能够在单个神经元硬件故障时保持功能稳定，提供比传统软件方法更强的硬件容错能力。

Conclusion: 神经元级硬件冗余是确保神经网络功能稳定性的有效方法，特别适用于需要高可靠性的实际部署场景。

Abstract: This paper presents an innovative approach to ensuring functional stability of neural networks through hardware redundancy at the individual neuron level. Unlike the classical Dropout method, which is used during training for regularization purposes, the proposed system ensures resilience to hardware failures during network operation. Each neuron is implemented on a separate microcomputer (ESP32), allowing the system to continue functioning even when individual computational nodes fail.

</details>


### [16] [Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming](https://arxiv.org/abs/2512.04910)
*Fang Li*

Main category: cs.AR

TL;DR: 使用ASP进行条板电路自动布局设计，将布局问题转化为合成与多目标优化任务，同时生成可行布局并最小化板面积和元件条交叉。


<details>
  <summary>Details</summary>
Motivation: 为电子原型制作和教育提供实用的自动化条板布局工具，同时展示声明式编程在解决复杂设计自动化问题中的能力。

Method: 采用答案集编程（ASP）方法，将布局问题形式化为合成与多目标优化任务，利用ASP的声明式特性自然简洁地表达几何和电气约束，采用两阶段求解方法（先保证可行性再优化质量）。

Result: 实验结果表明，该方法能为各种复杂度的电路生成紧凑、可制造的布局，在自动条板布局方面取得了显著进展。

Conclusion: 该工作代表了自动条板布局的重要进展，为电子原型制作和教育提供了实用工具，同时展示了声明式编程在解决复杂设计自动化问题中的强大能力。

Abstract: This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.

</details>
