<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Generating Compilers for Qubit Mapping and Routing](https://arxiv.org/abs/2508.10781)
*Abtin Molavi,Amanda Xu,Ethan Cecchetti,Swamit Tannu,Aws Albarghouthi*

Main category: cs.PL

TL;DR: 提出了一种自动生成量子比特映射和路由（QMR）编译器的方法，适用于任意量子架构，通过领域特定语言Marol简化问题定义，并展示了其性能与手工编译器相当。


<details>
  <summary>Details</summary>
Motivation: 量子计算机的架构多样且快速演变，需要高效的QMR编译器以优化资源使用和降低错误率。

Method: 通过识别QMR问题的共同核心结构（设备状态机），提出抽象QMR问题，并开发领域特定语言Marol用于问题定义。使用参数化求解器解决Marol定义的问题。

Result: 生成的编译器在运行时和解质量上与手工编译器相当，适用于多种量子架构。

Conclusion: 该方法简化了未来量子编译器的开发，适应新架构的出现。

Abstract: Quantum computers promise to solve important problems faster than classical
computers, potentially unlocking breakthroughs in materials science, chemistry,
and beyond. Optimizing compilers are key to realizing this potential, as they
minimize expensive resource usage and limit error rates. A critical compilation
step is qubit mapping and routing (QMR), which finds mappings from circuit
qubits to qubits on a target device and plans instruction execution while
satisfying the device's connectivity constraints. The challenge is that the
landscape of quantum architectures is incredibly diverse and fast-evolving.
Given this diversity, hundreds of papers have addressed the QMR problem for
different qubit hardware, connectivity constraints, and quantum error
correction schemes.
  We present an approach for automatically generating qubit mapping and routing
compilers for arbitrary quantum architectures. Though each QMR problem is
different, we identify a common core structure-device state machine-that we use
to formulate an abstract QMR problem. Our formulation naturally leads to a
domain-specific language, Marol, for specifying QMR problems-for example, the
well-studied NISQ mapping and routing problem requires only 12 lines of Marol.
We demonstrate that QMR problems, defined in Marol, can be solved with a
powerful parametric solver that can be instantiated for any Marol program. We
evaluate our approach through case studies of important QMR problems from prior
and recent work, covering noisy and fault-tolerant quantum architectures on all
major hardware platforms. Our thorough evaluation shows that generated
compilers are competitive with handwritten, specialized compilers in terms of
runtime and solution quality. We envision that our approach will simplify
development of future quantum compilers as new quantum architectures continue
to emerge.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Hard Shell, Reliable Core: Improving Resilience in Replicated Systems with Selective Hybridization](https://arxiv.org/abs/2508.10141)
*Laura Lawniczak,Tobias Distler*

Main category: cs.DC

TL;DR: ShellFT框架通过微复制技术，灵活选择需要抵御拜占庭故障的复制逻辑部分，显著降低多样化成本。


<details>
  <summary>Details</summary>
Motivation: 现有混合故障模型在灵活性和多样化开销上存在不足，需改进。

Method: 提出ShellFT框架，利用微复制技术，允许自由选择拜占庭容错部分。

Result: 相比传统方法，ShellFT减少多样化成本超70%。

Conclusion: ShellFT为定制化混合解决方案提供了高效灵活的方法。

Abstract: Hybrid fault models are known to be an effective means for enhancing the
robustness of consensus-based replicated systems. However, existing
hybridization approaches suffer from limited flexibility with regard to the
composition of crash-tolerant and Byzantine fault-tolerant system parts and/or
are associated with a significant diversification overhead. In this paper we
address these issues with ShellFT, a framework that leverages the concept of
micro replication to allow system designers to freely choose the parts of the
replication logic that need to be resilient against Byzantine faults. As a key
benefit, such a selective hybridization makes it possible to develop hybrid
solutions that are tailored to the specific characteristics and requirements of
individual use cases. To illustrate this flexibility, we present three custom
ShellFT protocols and analyze the complexity of their implementations. Our
evaluation shows that compared with traditional hybridization approaches,
ShellFT is able to decrease diversification costs by more than 70%.

</details>


### [3] [Mixed-Precision Performance Portability of FFT-Based GPU-Accelerated Algorithms for Block-Triangular Toeplitz Matrices](https://arxiv.org/abs/2508.10202)
*Sreeram Venkat,Kasia Swirydowicz,Noah Wolfe,Omar Ghattas*

Main category: cs.DC

TL;DR: 论文提出了一种基于Hipify的性能可移植框架，并将其应用于FFTMatvec，使其能在AMD GPU上高效运行。同时，提出了动态混合精度框架，通过Pareto前沿分析确定最佳配置，并在OLCF Frontier超级计算机上扩展到2,048个GPU。


<details>
  <summary>Details</summary>
Motivation: 领导级计算设施的硬件多样性及GPU在低精度计算中的性能提升，促使科学HPC工作流采用混合精度算法和性能可移植模型。

Method: 使用Hipify框架实现性能可移植性，应用于FFTMatvec；通过动态混合精度框架和Pareto前沿分析优化配置。

Result: FFTMatvec在AMD GPU上表现优异，性能优化集成到rocBLAS库中；混合精度配置在AMD MI250X、MI300X和MI355X GPU上验证。

Conclusion: 性能可移植的混合精度FFTMatvec在OLCF Frontier上成功扩展到2,048个GPU，展示了其高效性和可扩展性。

Abstract: The hardware diversity displayed in leadership-class computing facilities,
alongside the immense performance boosts exhibited by today's GPUs when
computing in lower precision, provide a strong incentive for scientific HPC
workflows to adopt mixed-precision algorithms and performance portability
models. We present an on-the-fly framework using Hipify for performance
portability and apply it to FFTMatvec-an HPC application that computes
matrix-vector products with block-triangular Toeplitz matrices. Our approach
enables FFTMatvec, initially a CUDA-only application, to run seamlessly on AMD
GPUs with excellent observed performance. Performance optimizations for AMD
GPUs are integrated directly into the open-source rocBLAS library, keeping the
application code unchanged. We then present a dynamic mixed-precision framework
for FFTMatvec; a Pareto front analysis determines the optimal mixed-precision
configuration for a desired error tolerance. Results are shown for AMD Instinct
MI250X, MI300X, and the newly launched MI355X GPUs. The performance-portable,
mixed-precision FFTMatvec is scaled to 2,048 GPUs on the OLCF Frontier
supercomputer.

</details>


### [4] [GPZ: GPU-Accelerated Lossy Compressor for Particle Data](https://arxiv.org/abs/2508.10305)
*Ruoyu Li,Yafan Huang,Longtao Zhang,Zhuoxun Yang,Sheng Di,Jiajun Huang,Jinyang Liu,Jiannan Tian,Xin Liang,Guanpeng Li,Hanqi Guo,Franck Cappello,Kai Zhao*

Main category: cs.DC

TL;DR: GPZ是一种高性能、误差有界的无损压缩器，专为现代GPU上的大规模粒子数据设计，显著提升了压缩效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统压缩技术在处理不规则粒子分布和GPU架构限制时表现不佳，导致吞吐量有限和压缩比不理想。

Method: GPZ采用四阶段并行流水线，结合计算、内存访问和GPU占用的优化，实现接近硬件极限的吞吐量。

Result: 在三种GPU架构和六个大规模科学数据集上，GPZ比五种先进GPU压缩器性能提升高达8倍，同时压缩比和数据质量更优。

Conclusion: GPZ为大规模粒子数据提供了一种高效、高性能的压缩解决方案，适用于多种GPU架构和科学领域。

Abstract: Particle-based simulations and point-cloud applications generate massive,
irregular datasets that challenge storage, I/O, and real-time analytics.
Traditional compression techniques struggle with irregular particle
distributions and GPU architectural constraints, often resulting in limited
throughput and suboptimal compression ratios. In this paper, we present GPZ, a
high-performance, error-bounded lossy compressor designed specifically for
large-scale particle data on modern GPUs. GPZ employs a novel four-stage
parallel pipeline that synergistically balances high compression efficiency
with the architectural demands of massively parallel hardware. We introduce a
suite of targeted optimizations for computation, memory access, and GPU
occupancy that enables GPZ to achieve near-hardware-limit throughput. We
conduct an extensive evaluation on three distinct GPU architectures
(workstation, data center, and edge) using six large-scale, real-world
scientific datasets from five distinct domains. The results demonstrate that
GPZ consistently and significantly outperforms five state-of-the-art GPU
compressors, delivering up to 8x higher end-to-end throughput while
simultaneously achieving superior compression ratios and data quality.

</details>


### [5] [Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models](https://arxiv.org/abs/2508.10349)
*Tianjun Yuan,Jiaxiang Geng,Pengchao Han,Xianhao Chen,Bing Luo*

Main category: cs.DC

TL;DR: 提出了一种灵活个性化联邦学习范式FlexP-SFL，通过分割学习和资源分配优化，提升个性化微调效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决有限客户端数据和异构数据分布对协作学习的阻碍，同时适应客户端的计算资源限制。

Method: 基于分割学习，FlexP-SFL允许客户端根据资源限制本地训练部分模型，其余部分卸载到服务器，并提出对齐策略优化性能。

Result: 实验结果表明，FlexP-SFL在个性化微调效率和最终准确性上优于基线模型。

Conclusion: FlexP-SFL是一种有效的个性化联邦学习范式，能够适应资源限制并提升模型性能。

Abstract: Fine-tuning foundation models is critical for superior performance on
personalized downstream tasks, compared to using pre-trained models.
Collaborative learning can leverage local clients' datasets for fine-tuning,
but limited client data and heterogeneous data distributions hinder effective
collaboration. To address the challenge, we propose a flexible personalized
federated learning paradigm that enables clients to engage in collaborative
learning while maintaining personalized objectives. Given the limited and
heterogeneous computational resources available on clients, we introduce
\textbf{flexible personalized split federated learning (FlexP-SFL)}. Based on
split learning, FlexP-SFL allows each client to train a portion of the model
locally while offloading the rest to a server, according to resource
constraints. Additionally, we propose an alignment strategy to improve
personalized model performance on global data. Experimental results show that
FlexP-SFL outperforms baseline models in personalized fine-tuning efficiency
and final accuracy.

</details>


### [6] [Dalek: An Unconventional and Energy-Aware Heterogeneous Cluster](https://arxiv.org/abs/2508.10481)
*Adrien Cassagne,Noé Amiot,Manuel Bouyer*

Main category: cs.DC

TL;DR: Dalek是一个实验性计算集群，旨在评估异构消费级硬件在软件设计、原型开发和算法开发中的性能，提供低成本且多功能的平台。


<details>
  <summary>Details</summary>
Motivation: 传统计算中心依赖昂贵的服务器级组件，而Dalek通过整合消费级硬件（如迷你PC、笔记本电脑和游戏台式机中的CPU和GPU），提供更具成本效益的解决方案。

Method: 文档详细描述了集群的架构和软件堆栈，并通过合成基准测试展示性能。此外，还介绍了一个自定义的高精度能源监控平台。

Result: 能源监控平台能够以毫瓦级分辨率提供每秒1000次平均采样，支持广泛的能源感知研究实验。

Conclusion: Dalek为计算机科学应用中的能源感知研究提供了一个高效且经济的实验平台。

Abstract: Dalek is an experimental compute cluster designed to evaluate the performance
of heterogeneous, consumer-grade hardware for software design, prototyping, and
algorithm development. In contrast to traditional computing centers that rely
on costly, server-class components, Dalek integrates CPUs and GPUs typically
found in mini-PCs, laptops, and gaming desktops, providing a cost-effective yet
versatile platform. This document details the cluster's architecture and
software stack, and presents results from synthetic benchmarks. Furthermore, it
introduces a custom energy monitoring platform capable of delivering 1000
averaged samples per second with milliwatt-level resolution. This
high-precision monitoring capability enables a wide range of energy-aware
research experiments in applied Computer Science.

</details>


### [7] [Introducing CQ: A C-like API for Quantum Accelerated HPC](https://arxiv.org/abs/2508.10854)
*Oliver Thomson Brown,Mateusz Meller,James Richings*

Main category: cs.DC

TL;DR: 本文介绍了CQ，一种用于量子加速高性能计算（HPC）的类C API规范，以及其参考实现CQ-SimBE。CQ支持从C和Fortran等语言运行时卸载量子计算，提供细粒度的数据控制。


<details>
  <summary>Details</summary>
Motivation: 量子计算在HPC中的应用需要与经典代码的无缝集成，CQ旨在解决这一问题。

Method: 提出CQ规范及其参考实现CQ-SimBE，基于QuEST模拟器，支持运行时卸载和严格类型语言兼容。

Result: CQ和CQ-SimBE开源可用，展示了量子计算与经典HPC代码的集成潜力。

Conclusion: CQ为量子计算在HPC中的渐进式集成提供了可行方案，并支持未来功能扩展。

Abstract: In this paper we present CQ, a specification for a C-like API for quantum
accelerated HPC, as well as CQ-SimBE, a reference implementation of CQ written
in C99, and built on top of the statevector simulator QuEST. CQ focuses on
enabling the incremental integration of quantum computing into classical HPC
codes by supporting runtime offloading from languages such as C and Fortran. It
provides a way of describing and offloading quantum computations which is
compatible with strictly and strongly typed compiled languages, and gives the
programmer fine-grained control over classical data movement. The CQ Simulated
Backend (CQ-SimBE) provides both a way to demonstrate the usage and utility of
CQ, and a space to experiment with new features such as support for analogue
quantum computing. Both the CQ specification and CQ-SimBE are open-source, and
available in public repositories.

</details>


### [8] [Minimmit: Fast Finality with Even Faster Blocks](https://arxiv.org/abs/2508.10862)
*Brendan Kobayashi Chou,Andrew Lewis-Pye,Patrick O'Grady*

Main category: cs.DC

TL;DR: Minimmit是一种新的状态机复制协议，通过优化视图切换进一步降低延迟，扩展了Alpenglow等协议的'2轮最终性'方法。


<details>
  <summary>Details</summary>
Motivation: 旨在减少状态机复制协议的延迟，提升效率。

Method: 基于'2轮最终性'方法，优化视图切换机制，提供伪代码和一致性及活性证明。

Result: 初步草案展示了协议的一致性和活性，后续将补充乐观响应性证明和实验。

Conclusion: Minimmit通过优化视图切换降低延迟，未来将进一步验证其性能。

Abstract: Minimmit is a new protocol for State-Machine-Replication (SMR) that extends
the '2-round finality' approach of protocols such as Alpenglow to further
reduce latency, by allowing for faster progression through 'views'. This
preliminary draft provides motivation and pseudocode, together with proofs of
consistency and liveness. An updated draft with a proof of optimistic
responsiveness, suggested optimizations, and experiments, is to follow.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [DiffAxE: Diffusion-driven Hardware Accelerator Generation and Design Space Exploration](https://arxiv.org/abs/2508.10303)
*Arkapravo Ghosh,Abhishek Moitra,Abhiroop Bhattacharjee,Ruokai Yin,Priyadarshini Panda*

Main category: cs.AR

TL;DR: 提出了一种生成式方法，将硬件设计建模为基于目标性能的1-D图像合成，显著提升了设计空间探索的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载（如DNN和LLM）的复杂性增加，设计空间变得巨大且不规则，传统优化方法效率低下且难以应对。

Method: 采用生成式方法，将硬件设计建模为1-D图像合成，以学习非可微、非双射的硬件-性能映射。

Result: 在O(10^17)设计空间中，比贝叶斯优化误差低0.86%，速度快17000倍；在LLM推理中，EDP降低3.37x（ASIC）和7.75x（FPGA）。

Conclusion: 该方法显著提升了大规模设计空间探索的效率和性能，适用于复杂AI硬件优化。

Abstract: Design space exploration (DSE) is critical for developing optimized hardware
architectures, especially for AI workloads such as deep neural networks (DNNs)
and large language models (LLMs), which require specialized acceleration. As
model complexity grows, accelerator design spaces have expanded to O(10^17),
becoming highly irregular, non-convex, and exhibiting many-to-one mappings from
design configurations to performance metrics. This complexity renders direct
inverse derivation infeasible and necessitates heuristic or sampling-based
optimization. Conventional methods - including Bayesian optimization, gradient
descent, reinforcement learning, and genetic algorithms - depend on iterative
sampling, resulting in long runtimes and sensitivity to initialization. Deep
learning-based approaches have reframed DSE as classification using
recommendation models, but remain limited to small-scale (O(10^3)), less
complex design spaces. To overcome these constraints, we propose a generative
approach that models hardware design as 1-D image synthesis conditioned on
target performance, enabling efficient learning of non-differentiable,
non-bijective hardware-performance mappings. Our framework achieves 0.86% lower
generation error than Bayesian optimization with a 17000x speedup, and
outperforms GANDSE with 30% lower error at only 1.83x slower search. We further
extend the method to a structured DSE setting, attaining 9.8% lower
energy-delay product (EDP) and 6% higher performance, with up to 145.6x and
1312x faster search compared to existing optimization methods on O(10^17)
design spaces. For LLM inference, our method achieves 3.37x and 7.75x lower EDP
on a 32nm ASIC and Xilinx Ultrascale+ VPU13 FPGA, respectively, compared to the
state-of-the-art DOSA framework.

</details>


### [10] [AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design](https://arxiv.org/abs/2508.10409)
*Zihao Chen,Ji Zhuang,Jinyi Shen,Xiaoyue Ke,Xinyi Yang,Mingjie Zhou,Zhuoyao Du,Xu Yan,Zhouyang Wu,Zhenyu Xu,Jiangli Huang,Li Shang,Xuan Zeng,Fan Yang*

Main category: cs.AR

TL;DR: AnalogSeeker是一个开源的基础语言模型，专注于模拟电路设计，通过领域知识蒸馏和精细数据集构建提升设计辅助能力。


<details>
  <summary>Details</summary>
Motivation: 解决模拟电路设计领域数据稀缺和知识复杂性的问题，提供高效的设计辅助工具。

Method: 采用领域知识框架构建语料库，多代理框架蒸馏知识为问答对，并开发定制化微调算法。

Result: 在AMSBench-TQA基准测试中达到85.04%准确率，比原模型提升15.67%，并在下游任务中表现优异。

Conclusion: AnalogSeeker为模拟电路设计提供了有效的开源解决方案，具有实际应用和研究价值。

Abstract: In this paper, we propose AnalogSeeker, an effort toward an open-source
foundation language model for analog circuit design, with the aim of
integrating domain knowledge and giving design assistance. To overcome the
scarcity of data in this field, we employ a corpus collection strategy based on
the domain knowledge framework of analog circuits. High-quality, accessible
textbooks across relevant subfields are systematically curated and cleaned into
a textual domain corpus. To address the complexity of knowledge of analog
circuits, we introduce a granular domain knowledge distillation method. Raw,
unlabeled domain corpus is decomposed into typical, granular learning nodes,
where a multi-agent framework distills implicit knowledge embedded in
unstructured text into question-answer data pairs with detailed reasoning
processes, yielding a fine-grained, learnable dataset for fine-tuning. To
address the unexplored challenges in training analog circuit foundation models,
we explore and share our training methods through both theoretical analysis and
experimental validation. We finally establish a fine-tuning-centric training
paradigm, customizing and implementing a neighborhood self-constrained
supervised fine-tuning algorithm. This approach enhances training outcomes by
constraining the perturbation magnitude between the model's output
distributions before and after training. In practice, we train the
Qwen2.5-32B-Instruct model to obtain AnalogSeeker, which achieves 85.04%
accuracy on AMSBench-TQA, the analog circuit knowledge evaluation benchmark,
with a 15.67% point improvement over the original model and is competitive with
mainstream commercial models. Furthermore, AnalogSeeker also shows
effectiveness in the downstream operational amplifier design task. AnalogSeeker
is open-sourced at https://huggingface.co/analogllm/analogseeker for research
use.

</details>


### [11] [THERMOS: Thermally-Aware Multi-Objective Scheduling of AI Workloads on Heterogeneous Multi-Chiplet PIM Architectures](https://arxiv.org/abs/2508.10691)
*Alish Kanani,Lukas Pfromm,Harsh Sharma,Janardhan Rao Doppa,Partha Pratim Pande,Umit Y. Ogras*

Main category: cs.AR

TL;DR: THERMOS是一个针对异构多芯片PIM架构的AI工作负载调度框架，通过多目标强化学习实现性能、能耗和热管理的优化。


<details>
  <summary>Details</summary>
Motivation: 异构芯片PIM架构在AI推理中具有潜力，但调度面临性能、能耗和热管理的挑战，需要智能调度方案。

Method: 提出THERMOS框架，利用多目标强化学习（MORL）动态优化执行时间、能耗和热管理。

Result: THERMOS比基线算法平均执行时间快89%，能耗降低57%，运行时和能耗开销极低。

Conclusion: THERMOS为异构PIM架构提供高效、灵活的调度方案，显著提升性能和能效。

Abstract: Chiplet-based integration enables large-scale systems that combine diverse
technologies, enabling higher yield, lower costs, and scalability, making them
well-suited to AI workloads. Processing-in-Memory (PIM) has emerged as a
promising solution for AI inference, leveraging technologies such as ReRAM,
SRAM, and FeFET, each offering unique advantages and trade-offs. A
heterogeneous chiplet-based PIM architecture can harness the complementary
strengths of these technologies to enable higher performance and energy
efficiency. However, scheduling AI workloads across such a heterogeneous system
is challenging due to competing performance objectives, dynamic workload
characteristics, and power and thermal constraints. To address this need, we
propose THERMOS, a thermally-aware, multi-objective scheduling framework for AI
workloads on heterogeneous multi-chiplet PIM architectures. THERMOS trains a
single multi-objective reinforcement learning (MORL) policy that is capable of
achieving Pareto-optimal execution time, energy, or a balanced objective at
runtime, depending on the target preferences. Comprehensive evaluations show
that THERMOS achieves up to 89% faster average execution time and 57% lower
average energy consumption than baseline AI workload scheduling algorithms with
only 0.14% runtime and 0.022% energy overhead.

</details>
