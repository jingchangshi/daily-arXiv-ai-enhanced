<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Generating Compilers for Qubit Mapping and Routing](https://arxiv.org/abs/2508.10781)
*Abtin Molavi,Amanda Xu,Ethan Cecchetti,Swamit Tannu,Aws Albarghouthi*

Main category: cs.PL

TL;DR: 提出了一种自动生成量子比特映射和路由编译器的方法，适用于任意量子架构，通过领域特定语言Marol简化问题定义，并展示了其竞争力。


<details>
  <summary>Details</summary>
Motivation: 量子计算机的架构多样且快速演进，需要一种通用方法解决量子比特映射和路由问题，以简化编译器开发。

Method: 使用设备状态机作为核心结构，提出抽象问题定义，并开发领域特定语言Marol，结合参数化求解器解决问题。

Result: 生成的编译器在运行时间和解决方案质量上与手工编写的专用编译器相当，适用于多种量子架构。

Conclusion: 该方法为未来量子编译器的开发提供了简化途径，适应不断涌现的新量子架构。

Abstract: Quantum computers promise to solve important problems faster than classical
computers, potentially unlocking breakthroughs in materials science, chemistry,
and beyond. Optimizing compilers are key to realizing this potential, as they
minimize expensive resource usage and limit error rates. A critical compilation
step is qubit mapping and routing (QMR), which finds mappings from circuit
qubits to qubits on a target device and plans instruction execution while
satisfying the device's connectivity constraints. The challenge is that the
landscape of quantum architectures is incredibly diverse and fast-evolving.
Given this diversity, hundreds of papers have addressed the QMR problem for
different qubit hardware, connectivity constraints, and quantum error
correction schemes.
  We present an approach for automatically generating qubit mapping and routing
compilers for arbitrary quantum architectures. Though each QMR problem is
different, we identify a common core structure-device state machine-that we use
to formulate an abstract QMR problem. Our formulation naturally leads to a
domain-specific language, Marol, for specifying QMR problems-for example, the
well-studied NISQ mapping and routing problem requires only 12 lines of Marol.
We demonstrate that QMR problems, defined in Marol, can be solved with a
powerful parametric solver that can be instantiated for any Marol program. We
evaluate our approach through case studies of important QMR problems from prior
and recent work, covering noisy and fault-tolerant quantum architectures on all
major hardware platforms. Our thorough evaluation shows that generated
compilers are competitive with handwritten, specialized compilers in terms of
runtime and solution quality. We envision that our approach will simplify
development of future quantum compilers as new quantum architectures continue
to emerge.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Hard Shell, Reliable Core: Improving Resilience in Replicated Systems with Selective Hybridization](https://arxiv.org/abs/2508.10141)
*Laura Lawniczak,Tobias Distler*

Main category: cs.DC

TL;DR: ShellFT框架通过微复制技术提升共识系统的灵活性，减少多样化成本70%以上。


<details>
  <summary>Details</summary>
Motivation: 现有混合容错模型在灵活性和多样化成本上存在不足。

Method: 提出ShellFT框架，支持选择性混合容错设计。

Result: 实现三种定制协议，多样化成本降低70%。

Conclusion: ShellFT为特定用例提供高效灵活的混合容错解决方案。

Abstract: Hybrid fault models are known to be an effective means for enhancing the
robustness of consensus-based replicated systems. However, existing
hybridization approaches suffer from limited flexibility with regard to the
composition of crash-tolerant and Byzantine fault-tolerant system parts and/or
are associated with a significant diversification overhead. In this paper we
address these issues with ShellFT, a framework that leverages the concept of
micro replication to allow system designers to freely choose the parts of the
replication logic that need to be resilient against Byzantine faults. As a key
benefit, such a selective hybridization makes it possible to develop hybrid
solutions that are tailored to the specific characteristics and requirements of
individual use cases. To illustrate this flexibility, we present three custom
ShellFT protocols and analyze the complexity of their implementations. Our
evaluation shows that compared with traditional hybridization approaches,
ShellFT is able to decrease diversification costs by more than 70%.

</details>


### [3] [Mixed-Precision Performance Portability of FFT-Based GPU-Accelerated Algorithms for Block-Triangular Toeplitz Matrices](https://arxiv.org/abs/2508.10202)
*Sreeram Venkat,Kasia Swirydowicz,Noah Wolfe,Omar Ghattas*

Main category: cs.DC

TL;DR: 论文提出了一种基于Hipify的性能可移植框架，并将其应用于FFTMatvec，使其能在AMD GPU上高效运行。同时，提出了动态混合精度框架，通过Pareto前沿分析确定最佳精度配置，并在多个AMD GPU上展示了性能。


<details>
  <summary>Details</summary>
Motivation: 领导级计算设施的硬件多样性及GPU在低精度计算中的性能提升，促使科学HPC工作流采用混合精度算法和性能可移植模型。

Method: 使用Hipify框架实现性能可移植性，并开发动态混合精度框架，通过Pareto前沿分析优化精度配置。

Result: FFTMatvec在AMD GPU上表现优异，性能优化直接集成到rocBLAS库中，并在OLCF Frontier超级计算机上扩展到2,048个GPU。

Conclusion: 该框架成功实现了性能可移植性和混合精度优化，为HPC应用提供了高效解决方案。

Abstract: The hardware diversity displayed in leadership-class computing facilities,
alongside the immense performance boosts exhibited by today's GPUs when
computing in lower precision, provide a strong incentive for scientific HPC
workflows to adopt mixed-precision algorithms and performance portability
models. We present an on-the-fly framework using Hipify for performance
portability and apply it to FFTMatvec-an HPC application that computes
matrix-vector products with block-triangular Toeplitz matrices. Our approach
enables FFTMatvec, initially a CUDA-only application, to run seamlessly on AMD
GPUs with excellent observed performance. Performance optimizations for AMD
GPUs are integrated directly into the open-source rocBLAS library, keeping the
application code unchanged. We then present a dynamic mixed-precision framework
for FFTMatvec; a Pareto front analysis determines the optimal mixed-precision
configuration for a desired error tolerance. Results are shown for AMD Instinct
MI250X, MI300X, and the newly launched MI355X GPUs. The performance-portable,
mixed-precision FFTMatvec is scaled to 2,048 GPUs on the OLCF Frontier
supercomputer.

</details>


### [4] [GPZ: GPU-Accelerated Lossy Compressor for Particle Data](https://arxiv.org/abs/2508.10305)
*Ruoyu Li,Yafan Huang,Longtao Zhang,Zhuoxun Yang,Sheng Di,Jiajun Huang,Jinyang Liu,Jiannan Tian,Xin Liang,Guanpeng Li,Hanqi Guo,Franck Cappello,Kai Zhao*

Main category: cs.DC

TL;DR: GPZ是一种专为GPU设计的高性能、误差有界的粒子数据压缩器，通过四阶段并行流水线实现高效压缩，显著优于现有GPU压缩器。


<details>
  <summary>Details</summary>
Motivation: 粒子模拟和点云应用生成的大规模不规则数据集对存储、I/O和实时分析提出了挑战，传统压缩技术难以满足需求。

Method: GPZ采用四阶段并行流水线，结合计算、内存访问和GPU占用优化，实现高效压缩。

Result: 在三种GPU架构和六个真实数据集上，GPZ比五种先进GPU压缩器吞吐量高8倍，压缩比和数据质量更优。

Conclusion: GPZ为大规模粒子数据提供了一种高效、高性能的压缩解决方案。

Abstract: Particle-based simulations and point-cloud applications generate massive,
irregular datasets that challenge storage, I/O, and real-time analytics.
Traditional compression techniques struggle with irregular particle
distributions and GPU architectural constraints, often resulting in limited
throughput and suboptimal compression ratios. In this paper, we present GPZ, a
high-performance, error-bounded lossy compressor designed specifically for
large-scale particle data on modern GPUs. GPZ employs a novel four-stage
parallel pipeline that synergistically balances high compression efficiency
with the architectural demands of massively parallel hardware. We introduce a
suite of targeted optimizations for computation, memory access, and GPU
occupancy that enables GPZ to achieve near-hardware-limit throughput. We
conduct an extensive evaluation on three distinct GPU architectures
(workstation, data center, and edge) using six large-scale, real-world
scientific datasets from five distinct domains. The results demonstrate that
GPZ consistently and significantly outperforms five state-of-the-art GPU
compressors, delivering up to 8x higher end-to-end throughput while
simultaneously achieving superior compression ratios and data quality.

</details>


### [5] [Flexible Personalized Split Federated Learning for On-Device Fine-Tuning of Foundation Models](https://arxiv.org/abs/2508.10349)
*Tianjun Yuan,Jiaxiang Geng,Pengchao Han,Xianhao Chen,Bing Luo*

Main category: cs.DC

TL;DR: 提出了一种灵活的个性化联邦学习范式FlexP-SFL，通过分割学习和资源分配优化，提升个性化微调效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决客户端数据有限和分布异构性对协作学习的挑战。

Method: FlexP-SFL结合分割学习，客户端根据资源约束本地训练部分模型，其余部分卸载到服务器，并提出对齐策略优化全局数据性能。

Result: 实验表明FlexP-SFL在个性化微调效率和最终准确性上优于基线模型。

Conclusion: FlexP-SFL为资源受限的个性化联邦学习提供了有效解决方案。

Abstract: Fine-tuning foundation models is critical for superior performance on
personalized downstream tasks, compared to using pre-trained models.
Collaborative learning can leverage local clients' datasets for fine-tuning,
but limited client data and heterogeneous data distributions hinder effective
collaboration. To address the challenge, we propose a flexible personalized
federated learning paradigm that enables clients to engage in collaborative
learning while maintaining personalized objectives. Given the limited and
heterogeneous computational resources available on clients, we introduce
\textbf{flexible personalized split federated learning (FlexP-SFL)}. Based on
split learning, FlexP-SFL allows each client to train a portion of the model
locally while offloading the rest to a server, according to resource
constraints. Additionally, we propose an alignment strategy to improve
personalized model performance on global data. Experimental results show that
FlexP-SFL outperforms baseline models in personalized fine-tuning efficiency
and final accuracy.

</details>


### [6] [Dalek: An Unconventional and Energy-Aware Heterogeneous Cluster](https://arxiv.org/abs/2508.10481)
*Adrien Cassagne,Noé Amiot,Manuel Bouyer*

Main category: cs.DC

TL;DR: Dalek是一个实验性计算集群，旨在评估异构消费级硬件在软件设计、原型开发和算法开发中的性能，提供低成本且多功能的平台。


<details>
  <summary>Details</summary>
Motivation: 传统计算中心依赖昂贵的服务器级组件，Dalek通过整合消费级硬件（如迷你PC、笔记本电脑和游戏台式机中的CPU和GPU），提供更具成本效益和灵活性的解决方案。

Method: 详细描述了集群的架构和软件堆栈，并通过合成基准测试展示性能。此外，还开发了一个高精度的能源监控平台，支持毫瓦级分辨率的实时监测。

Result: Dalek展示了消费级硬件的潜力，同时高精度能源监控平台为能源感知研究提供了强大工具。

Conclusion: Dalek为计算机科学应用中的能源感知研究提供了一个经济高效且功能丰富的实验平台。

Abstract: Dalek is an experimental compute cluster designed to evaluate the performance
of heterogeneous, consumer-grade hardware for software design, prototyping, and
algorithm development. In contrast to traditional computing centers that rely
on costly, server-class components, Dalek integrates CPUs and GPUs typically
found in mini-PCs, laptops, and gaming desktops, providing a cost-effective yet
versatile platform. This document details the cluster's architecture and
software stack, and presents results from synthetic benchmarks. Furthermore, it
introduces a custom energy monitoring platform capable of delivering 1000
averaged samples per second with milliwatt-level resolution. This
high-precision monitoring capability enables a wide range of energy-aware
research experiments in applied Computer Science.

</details>


### [7] [Introducing CQ: A C-like API for Quantum Accelerated HPC](https://arxiv.org/abs/2508.10854)
*Oliver Thomson Brown,Mateusz Meller,James Richings*

Main category: cs.DC

TL;DR: CQ是一个用于量子加速HPC的C类API规范，CQ-SimBE是其参考实现，支持从C和Fortran等语言运行时卸载量子计算。


<details>
  <summary>Details</summary>
Motivation: 为逐步将量子计算集成到经典HPC代码中提供支持，兼容强类型编译语言，并允许程序员精细控制数据移动。

Method: 提出CQ规范和基于QuEST的CQ-SimBE实现，支持运行时卸载和实验新功能。

Result: CQ和CQ-SimBE开源，可用于演示和实验量子计算功能。

Conclusion: CQ为量子计算与经典HPC的集成提供了实用工具和实验平台。

Abstract: In this paper we present CQ, a specification for a C-like API for quantum
accelerated HPC, as well as CQ-SimBE, a reference implementation of CQ written
in C99, and built on top of the statevector simulator QuEST. CQ focuses on
enabling the incremental integration of quantum computing into classical HPC
codes by supporting runtime offloading from languages such as C and Fortran. It
provides a way of describing and offloading quantum computations which is
compatible with strictly and strongly typed compiled languages, and gives the
programmer fine-grained control over classical data movement. The CQ Simulated
Backend (CQ-SimBE) provides both a way to demonstrate the usage and utility of
CQ, and a space to experiment with new features such as support for analogue
quantum computing. Both the CQ specification and CQ-SimBE are open-source, and
available in public repositories.

</details>


### [8] [Minimmit: Fast Finality with Even Faster Blocks](https://arxiv.org/abs/2508.10862)
*Brendan Kobayashi Chou,Andrew Lewis-Pye,Patrick O'Grady*

Main category: cs.DC

TL;DR: Minimmit是一种新的状态机复制协议，通过允许更快地切换视图来进一步降低延迟，扩展了Alpenglow等协议的'2轮最终性'方法。


<details>
  <summary>Details</summary>
Motivation: 旨在通过优化视图切换机制，进一步减少状态机复制的延迟。

Method: 扩展了'2轮最终性'方法，允许更快地切换视图，提供了伪代码和一致性、活跃性证明。

Result: 初步草案展示了协议的一致性和活跃性，后续将补充乐观响应性证明、优化建议和实验。

Conclusion: Minimmit协议通过优化视图切换降低了延迟，未来工作将进一步完善其性能和实验验证。

Abstract: Minimmit is a new protocol for State-Machine-Replication (SMR) that extends
the '2-round finality' approach of protocols such as Alpenglow to further
reduce latency, by allowing for faster progression through 'views'. This
preliminary draft provides motivation and pseudocode, together with proofs of
consistency and liveness. An updated draft with a proof of optimistic
responsiveness, suggested optimizations, and experiments, is to follow.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [DiffAxE: Diffusion-driven Hardware Accelerator Generation and Design Space Exploration](https://arxiv.org/abs/2508.10303)
*Arkapravo Ghosh,Abhishek Moitra,Abhiroop Bhattacharjee,Ruokai Yin,Priyadarshini Panda*

Main category: cs.AR

TL;DR: 提出了一种生成式方法，将硬件设计建模为基于目标性能的1-D图像合成，显著提升了设计空间探索的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载（如DNN和LLM）的复杂性增加，设计空间变得庞大且不规则，传统优化方法效率低下且难以应对。

Method: 采用生成式方法，将硬件设计建模为1-D图像合成，学习硬件与性能之间的非可微、非双射映射。

Result: 在O(10^17)设计空间中，生成误差比贝叶斯优化低0.86%，速度提升17000倍；在LLM推理中，EDP显著降低。

Conclusion: 该方法在高效性和准确性上优于现有技术，适用于大规模复杂设计空间的优化。

Abstract: Design space exploration (DSE) is critical for developing optimized hardware
architectures, especially for AI workloads such as deep neural networks (DNNs)
and large language models (LLMs), which require specialized acceleration. As
model complexity grows, accelerator design spaces have expanded to O(10^17),
becoming highly irregular, non-convex, and exhibiting many-to-one mappings from
design configurations to performance metrics. This complexity renders direct
inverse derivation infeasible and necessitates heuristic or sampling-based
optimization. Conventional methods - including Bayesian optimization, gradient
descent, reinforcement learning, and genetic algorithms - depend on iterative
sampling, resulting in long runtimes and sensitivity to initialization. Deep
learning-based approaches have reframed DSE as classification using
recommendation models, but remain limited to small-scale (O(10^3)), less
complex design spaces. To overcome these constraints, we propose a generative
approach that models hardware design as 1-D image synthesis conditioned on
target performance, enabling efficient learning of non-differentiable,
non-bijective hardware-performance mappings. Our framework achieves 0.86% lower
generation error than Bayesian optimization with a 17000x speedup, and
outperforms GANDSE with 30% lower error at only 1.83x slower search. We further
extend the method to a structured DSE setting, attaining 9.8% lower
energy-delay product (EDP) and 6% higher performance, with up to 145.6x and
1312x faster search compared to existing optimization methods on O(10^17)
design spaces. For LLM inference, our method achieves 3.37x and 7.75x lower EDP
on a 32nm ASIC and Xilinx Ultrascale+ VPU13 FPGA, respectively, compared to the
state-of-the-art DOSA framework.

</details>


### [10] [AnalogSeeker: An Open-source Foundation Language Model for Analog Circuit Design](https://arxiv.org/abs/2508.10409)
*Zihao Chen,Ji Zhuang,Jinyi Shen,Xiaoyue Ke,Xinyi Yang,Mingjie Zhou,Zhuoyao Du,Xu Yan,Zhouyang Wu,Zhenyu Xu,Jiangli Huang,Li Shang,Xuan Zeng,Fan Yang*

Main category: cs.AR

TL;DR: AnalogSeeker是一个开源的基础语言模型，旨在为模拟电路设计提供领域知识集成和设计辅助。通过领域知识蒸馏和多代理框架，将非结构化文本转化为可学习的数据集，最终训练出性能优越的模型。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计领域数据稀缺且知识复杂，需要一种能够整合领域知识并提供设计辅助的模型。

Method: 采用领域知识蒸馏方法，将非结构化文本分解为细粒度学习节点，并通过多代理框架生成问答对。使用定制的邻域自约束监督微调算法进行模型训练。

Result: AnalogSeeker在AMSBench-TQA基准测试中达到85.04%准确率，比原始模型提升15.67%，并在下游任务中表现优异。

Conclusion: AnalogSeeker为模拟电路设计提供了有效的开源解决方案，性能优于原始模型且与主流商业模型竞争。

Abstract: In this paper, we propose AnalogSeeker, an effort toward an open-source
foundation language model for analog circuit design, with the aim of
integrating domain knowledge and giving design assistance. To overcome the
scarcity of data in this field, we employ a corpus collection strategy based on
the domain knowledge framework of analog circuits. High-quality, accessible
textbooks across relevant subfields are systematically curated and cleaned into
a textual domain corpus. To address the complexity of knowledge of analog
circuits, we introduce a granular domain knowledge distillation method. Raw,
unlabeled domain corpus is decomposed into typical, granular learning nodes,
where a multi-agent framework distills implicit knowledge embedded in
unstructured text into question-answer data pairs with detailed reasoning
processes, yielding a fine-grained, learnable dataset for fine-tuning. To
address the unexplored challenges in training analog circuit foundation models,
we explore and share our training methods through both theoretical analysis and
experimental validation. We finally establish a fine-tuning-centric training
paradigm, customizing and implementing a neighborhood self-constrained
supervised fine-tuning algorithm. This approach enhances training outcomes by
constraining the perturbation magnitude between the model's output
distributions before and after training. In practice, we train the
Qwen2.5-32B-Instruct model to obtain AnalogSeeker, which achieves 85.04%
accuracy on AMSBench-TQA, the analog circuit knowledge evaluation benchmark,
with a 15.67% point improvement over the original model and is competitive with
mainstream commercial models. Furthermore, AnalogSeeker also shows
effectiveness in the downstream operational amplifier design task. AnalogSeeker
is open-sourced at https://huggingface.co/analogllm/analogseeker for research
use.

</details>


### [11] [THERMOS: Thermally-Aware Multi-Objective Scheduling of AI Workloads on Heterogeneous Multi-Chiplet PIM Architectures](https://arxiv.org/abs/2508.10691)
*Alish Kanani,Lukas Pfromm,Harsh Sharma,Janardhan Rao Doppa,Partha Pratim Pande,Umit Y. Ogras*

Main category: cs.AR

TL;DR: THERMOS是一个热感知、多目标调度框架，用于异构多芯片PIM架构上的AI工作负载，通过强化学习实现性能与能耗的优化。


<details>
  <summary>Details</summary>
Motivation: 异构芯片PIM架构结合多种技术优势，但调度AI工作负载面临性能、能耗和热约束的挑战。

Method: 提出THERMOS框架，利用多目标强化学习策略，动态优化执行时间、能耗或平衡目标。

Result: THERMOS比基线算法平均执行时间快89%，能耗降低57%，且运行时和能耗开销极低。

Conclusion: THERMOS有效解决了异构PIM架构的调度问题，显著提升了性能和能效。

Abstract: Chiplet-based integration enables large-scale systems that combine diverse
technologies, enabling higher yield, lower costs, and scalability, making them
well-suited to AI workloads. Processing-in-Memory (PIM) has emerged as a
promising solution for AI inference, leveraging technologies such as ReRAM,
SRAM, and FeFET, each offering unique advantages and trade-offs. A
heterogeneous chiplet-based PIM architecture can harness the complementary
strengths of these technologies to enable higher performance and energy
efficiency. However, scheduling AI workloads across such a heterogeneous system
is challenging due to competing performance objectives, dynamic workload
characteristics, and power and thermal constraints. To address this need, we
propose THERMOS, a thermally-aware, multi-objective scheduling framework for AI
workloads on heterogeneous multi-chiplet PIM architectures. THERMOS trains a
single multi-objective reinforcement learning (MORL) policy that is capable of
achieving Pareto-optimal execution time, energy, or a balanced objective at
runtime, depending on the target preferences. Comprehensive evaluations show
that THERMOS achieves up to 89% faster average execution time and 57% lower
average energy consumption than baseline AI workload scheduling algorithms with
only 0.14% runtime and 0.022% energy overhead.

</details>
