<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [MHRC-Bench: A Multilingual Hardware Repository-Level Code Completion benchmark](https://arxiv.org/abs/2601.03708)
*Qingyun Zou,Jiahao Cui,Nuo Chen,Bingsheng He,Weng-Fai Wong*

Main category: cs.PL

TL;DR: MHRC-Bench是首个针对硬件描述语言的多语言仓库级代码补全基准，包含训练和评估两部分，覆盖三种主要硬件设计编码风格，并提供了代码结构和硬件语义标注。


<details>
  <summary>Details</summary>
Motivation: 现有仓库级代码补全基准主要关注软件代码，忽视了硬件描述语言，而LLMs在通用编程语言上表现良好，但在硬件代码补全方面缺乏专门的评估基准。

Method: 创建MHRC-Bench基准，包含MHRC-Bench-Train和MHRC-Bench-Eval两部分，针对三种主要硬件设计编码风格，通过具体语法树分析为每个补全目标标注代码结构级和硬件导向的语义标签。

Result: 在MHRC-Bench-Eval上对模型进行了全面评估，评估结果和分析证明了MHRC-Bench的有效性。

Conclusion: MHRC-Bench填补了硬件描述语言仓库级代码补全基准的空白，为评估和改进LLMs在硬件代码补全方面的性能提供了重要工具。

Abstract: Large language models (LLMs) have achieved strong performance on code completion tasks in general-purpose programming languages. However, existing repository-level code completion benchmarks focus almost exclusively on software code and largely overlook hardware description languages. In this work, we present \textbf{MHRC-Bench}, consisting of \textbf{MHRC-Bench-Train} and \textbf{MHRC-Bench-Eval}, the first benchmark designed for multilingual hardware code completion at the repository level. Our benchmark targets completion tasks and covers three major hardware design coding styles. Each completion target is annotated with code-structure-level and hardware-oriented semantic labels derived from concrete syntax tree analysis. We conduct a comprehensive evaluation of models on MHRC-Bench-Eval. Comprehensive evaluation results and analysis demonstrate the effectiveness of MHRC-Bench.

</details>


### [2] [Agentic Proof Automation: A Case Study](https://arxiv.org/abs/2601.03768)
*Yichen Xu,Martin Odersky*

Main category: cs.PL

TL;DR: LLM代理在人类指导下完成大部分证明工程工作，成功实现复杂系统类型安全性的形式化验证，显著提升证明工程效率


<details>
  <summary>Details</summary>
Motivation: 证明工程劳动密集，传统证明脚本冗长；LLM的进步为证明自动化提供了新机会，通过代理行为可以探索代码库并迭代优化输出

Method: 提出"代理式证明自动化"方案：人类提供数学洞察（定义、定理、证明策略），LLM代理处理证明开发的机械工作；使用现成的LLM代理和轻量级证明检查工具，在Lean 4中形式化System Capless的语义类型安全性

Result: 代理完成了189个证明工程任务，成功率87%，仅16%需要人工干预；实现了超过14,000行代码的形式化系统

Conclusion: 代理是能力强的证明工程师，能显著提高生产力，但在创造性推理方面仍有不足，某些情况下仍需人类指导

Abstract: Proof engineering is notoriously labor-intensive: proofs that are straightforward on paper often require lengthy scripts in theorem provers. Recent advances in large language models (LLMs) create new opportunities for proof automation: modern LLMs not only generate proof scripts, but also support agentic behavior, exploring codebases and iteratively refining their outputs against prover feedback. These advances enable an emerging scheme where LLM-based agents undertake most proof engineering under human guidance. Humans provide mathematical insight (definitions, theorems, proof strategies); agents handle the mechanical work of proof development. We call this scheme agentic proof automation. We present this scheme through a case study: mechanizing the semantic type soundness of a sophisticated formal system, System Capless, in Lean 4, comprising over 14,000 lines of code. Using off-the-shelf LLM agents with a single lightweight proof-checking tool, the agents completed 189 proof engineering tasks with an 87% success rate, only 16% requiring human intervention. The case study demonstrates that agents are capable proof engineers that substantially boost productivity, though they fall short in creative reasoning and still require human guidance in certain cases. We release an interactive explorer where readers can examine all agent interactions; the mechanization is open-sourced for experiments and extensions.

</details>


### [3] [Logic Programming with Extensible Types](https://arxiv.org/abs/2601.03836)
*Ivan Perez,Angel Herranz*

Main category: cs.PL

TL;DR: 在Haskell中实现逻辑编程的集成方法，通过可扩展类型、通用统一算法和领域特定语言，保持静态类型和函数式编程优势


<details>
  <summary>Details</summary>
Motivation: 逻辑编程语言在声明性和简洁性方面有明显优势，但其思想在其他编程社区受到抵制，未能被其他范式和语言普遍采用。本文旨在探索如何在现有类型化函数式编程代码库中融入逻辑编程。

Method: 结合三个核心思想：1) 使用可扩展类型技术，允许宿主语言值包含逻辑变量；2) 实现适用于任何支持特定操作的数据结构的统一算法；3) 引入领域特定语言来定义和查询谓词。

Result: 通过一系列示例展示了该方法的可行性，并提供了使符号表示对用户更方便的辅助工具，表明该方法不仅在技术上可行，而且实用。该思想已在Haskell语言中实现并取得良好效果。

Conclusion: 提出了一种在类型化函数式编程语言中集成逻辑编程的新方法，该方法与宿主语言无缝集成，不牺牲静态类型，同时利用类型化函数式编程的优势如多态性和高阶函数，实现了技术可行性和实用性的平衡。

Abstract: Logic programming languages present clear advantages in terms of declarativeness and conciseness. However, the ideas of logic programming have been met with resistance in other programming communities, and have not generally been adopted by other paradigms and languages. This paper proposes a novel way to incorporate logic programming in an existing codebase in a typed functional programming language. Our approach integrates with the host language without sacrificing static typing, and leverages strengths of typed functional programming such as polymorphism and higher-order. We do so by combining three ideas. First, we use the extensible types technique to allow values of the host language to contain logic variables. Second, we implement a unification algorithm that works for any data structure that supports certain operations.Third, we introduce a domain-specific language to define and query predicates. We demonstrate our proposal via a series of examples, and provide aids to make the notation convenient for users, showing that the proposed approach is not just technically possible but also practical. Our ideas have been implemented in the language Haskell with very good results.

</details>


### [4] [Inductive First-Order Formula Synthesis by ASP: A Case Study in Invariant Inference](https://arxiv.org/abs/2601.03854)
*Ziyi Yang,George Pîrlea,Ilya Sergey*

Main category: cs.PL

TL;DR: 提出了一个从示例合成一阶逻辑公式的框架，统一并推进了推理转换系统不变量的最新方法，通过正交切片技术显著加速了分布式系统不变量的推理。


<details>
  <summary>Details</summary>
Motivation: 现有的一阶逻辑公式合成方法在推理转换系统不变量时存在效率问题，需要统一现有方法并开发更高效的搜索空间划分技术来加速推理过程。

Method: 1. 研究并分类现有方法，通过答案集编程（ASP）编码公式合成；2. 提出正交切片技术，将搜索空间划分为可管理的块；3. 实现两种增量候选剪枝方法；4. 在FORCE框架中结合现有FO不变量合成技术和正交切片。

Result: 1. 显著加速了分布式系统不变量推理的最先进算法；2. 促进了不同不变量推理框架的组合，实现了新颖的优化。

Conclusion: 提出的框架统一了现有方法，通过正交切片技术有效加速了一阶逻辑公式合成，为不变量推理提供了更高效的解决方案，并支持不同框架的组合优化。

Abstract: We present a framework for synthesising formulas in first-order logic (FOL) from examples, which unifies and advances state-of-the-art approaches for inference of transition system invariants. To do so, we study and categorise the existing methodologies, encoding techniques in their formula synthesis via answer set programming (ASP). Based on the derived categorisation, we propose orthogonal slices, a new technique for formula enumeration that partitions the search space into manageable chunks, enabling two approaches for incremental candidate pruning. Using a combination of existing techniques for first-order (FO) invariant synthesis and the orthogonal slices implemented in our framework FORCE, we significantly accelerate a state-of-the-art algorithm for distributed system invariant inference. We also show that our approach facilitates composition of different invariant inference frameworks, allowing for novel optimisations.

</details>


### [5] [Implementing Binary Search Trees in GP 2 (Extended Abstract)](https://arxiv.org/abs/2601.03897)
*Ziad Ismaili Alaoui,Detlef Plump*

Main category: cs.PL

TL;DR: 在GP 2图编程语言中实现二叉搜索树，支持插入、删除和查询操作，时间复杂度与命令式语言实现相当


<details>
  <summary>Details</summary>
Motivation: 探索如何在规则基础的图编程语言GP 2中实现高效的数据结构，特别是二叉搜索树，以验证图编程语言在实现经典算法数据结构方面的能力

Method: 使用GP 2的根图转换规则来实现二叉搜索树，通过图变换规则来执行插入、删除和查询操作

Result: 实现了二叉搜索树的基本操作，最坏情况时间复杂度为O(n)，平均情况下为O(log n)，与命令式语言实现的时间复杂度相匹配

Conclusion: 在GP 2图编程语言中成功实现了二叉搜索树，证明了图编程语言能够实现与传统命令式语言相当效率的数据结构算法

Abstract: We present an approach to implement binary search trees in the rule-based graph programming language GP 2. Our implementation uses GP 2's rooted graph transformation rules to be fast and supports insertion, deletion and query operations. We argue that the worst-case runtime for each of the operations is O(n) for a tree with n nodes. In addition, we expect that, on average, the operations run in time O(log(n)). Hence the implementation would match the time complexity of binary search trees implementations in imperative languages.

</details>


### [6] [CSSG: Measuring Code Similarity with Semantic Graphs](https://arxiv.org/abs/2601.04085)
*Jingwen Xu,Yiyang Lu,Changze Lv,Zisu Huang,Zhengkang Guo,Zhengyuan Wang,Muzhao Tian,Xuanjing Huang,Xiaoqing Zheng*

Main category: cs.PL

TL;DR: CSSG是一种基于程序依赖图的新型代码相似性度量方法，相比基于字符串重叠或语法树的传统方法，能更好地捕捉代码的深层语义关系。


<details>
  <summary>Details</summary>
Motivation: 现有代码相似性度量方法（如BLEU、CodeBLEU、TSED）主要依赖表面字符串重叠或抽象语法树结构，往往无法捕捉程序之间的深层语义关系，需要一种更语义感知的代码表示方法。

Method: 提出CSSG（基于语义图的代码相似性）度量方法，利用程序依赖图显式建模控制依赖和变量交互，提供语义感知的代码表示。

Result: 在CodeContests+数据集上的实验表明，CSSG在单语言和跨语言设置下，在区分更相似代码和较不相似代码方面始终优于现有度量方法。

Conclusion: 依赖感知的图表示相比表面级或基于语法的相似性度量提供了更有效的替代方案，能够更好地捕捉代码的语义相似性。

Abstract: Existing code similarity metrics, such as BLEU, CodeBLEU, and TSED, largely rely on surface-level string overlap or abstract syntax tree structures, and often fail to capture deeper semantic relationships between programs.We propose CSSG (Code Similarity using Semantic Graphs), a novel metric that leverages program dependence graphs to explicitly model control dependencies and variable interactions, providing a semantics-aware representation of code.Experiments on the CodeContests+ dataset show that CSSG consistently outperforms existing metrics in distinguishing more similar code from less similar code under both monolingual and cross-lingual settings, demonstrating that dependency-aware graph representations offer a more effective alternative to surface-level or syntax-based similarity measures.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [Revisiting Speculative Leaderless Protocols for Low-Latency BFT Replication](https://arxiv.org/abs/2601.03390)
*Daniel Qian,Xiyu Hao,Jinkun Geng,Yuncheng Yao,Aurojit Panda,Jinyang Li,Anirudh Sivaraman*

Main category: cs.DC

TL;DR: Aspen是一个无领导者的拜占庭容错协议，通过基于松散同步时钟和网络延迟估计的尽力排序层，在无冲突条件下实现接近最优的2Δ+ε延迟，比现有协议快1.2-3.3倍。


<details>
  <summary>Details</summary>
Motivation: 随着BFT协议在面向用户的许可区块链应用（如支付）中使用，低延迟变得至关重要。现有的无领导者乐观快速路径协议只在无冲突条件下工作，并发冲突请求会导致副本分歧并触发昂贵的恢复过程。

Method: Aspen采用无领导者架构，通过基于松散同步时钟和网络延迟估计的尽力排序层来移除无冲突条件。协议需要n=3f+2p+1个副本以容忍f个拜占庭节点，其中2p个额外副本允许快速路径在最多p个副本因网络延迟而分歧时继续工作。当乐观条件不满足时，回退到PBFT风格协议。

Result: 在广域分布式副本实验中，Aspen在75毫秒内提交请求，比先前协议快1.2-3.3倍，同时支持每秒19,000个请求。

Conclusion: Aspen通过创新的尽力排序层实现了无领导者BFT协议的低延迟，在保持安全性和活跃性的同时显著提升了性能，适用于需要低延迟的许可区块链应用。

Abstract: As Byzantine Fault Tolerant (BFT) protocols begin to be used in permissioned blockchains for user-facing applications such as payments, it is crucial that they provide low latency. In pursuit of low latency, some recently proposed BFT consensus protocols employ a leaderless optimistic fast path, in which clients broadcast their requests directly to replicas without first serializing requests at a leader, resulting in an end-to-end commit latency of 2 message delays ($2Δ$) during fault-free, synchronous periods. However, such a fast path only works if there is no contention: concurrent contending requests can cause replicas to diverge if they receive conflicting requests in different orders, triggering costly recovery procedures.
  In this work, we present Aspen, a leaderless BFT protocol that achieves a near-optimal latency of $2Δ+ \varepsilon$, where $\varepsilon$ indicates a short waiting delay. Aspen removes the no-contention condition by utilizing a best-effort sequencing layer based on loosely synchronized clocks and network delay estimates. Aspen requires $n = 3f + 2p + 1$ replicas to cope with up to $f$ Byzantine nodes. The $2p$ extra nodes allow Aspen's fast path to proceed even if up to $p$ replicas diverge due to unpredictable network delays. When its optimistic conditions do not hold, Aspen falls back to PBFT-style protocol, guaranteeing safety and liveness under partial synchrony. In experiments with wide-area distributed replicas, Aspen commits requests in less than 75 ms, a 1.2 to 3.3$\times$ improvement compared to previous protocols, while supporting 19,000 requests per second.

</details>


### [8] [Majorum: Ebb-and-Flow Consensus with Dynamic Quorums](https://arxiv.org/abs/2601.03862)
*Francesco D'Amato,Roberto Saltini,Thanh-Hai Tran,Yann Vonlanthen,Luca Zanolini*

Main category: cs.DC

TL;DR: Majorum是一种ebb-and-flow协议，结合了动态可用性协议和部分同步最终性协议，在乐观条件下只需3个时隙即可最终确定区块，每个时隙仅需一轮投票。


<details>
  <summary>Details</summary>
Motivation: 解决动态可用性协议在网络分区或异步期间无法提供强安全性保证的局限性，通过结合动态可用性和最终性协议来提供更好的安全性和性能。

Method: 采用ebb-and-flow架构，结合基于仲裁的动态可用性协议（TOB-SVD）和部分同步最终性协议，在乐观条件下每个时隙只需一轮投票即可最终确定区块。

Result: 在乐观条件下，Majorum只需3个时隙即可最终确定区块，每个时隙仅需一轮投票，且能连续最终化扩展先前已最终确定的区块。

Conclusion: Majorum通过ebb-and-flow设计有效解决了动态可用性协议的安全限制，在保持动态可用性的同时提供了高效的最终性保证。

Abstract: Dynamic availability is the ability of a consensus protocol to remain live despite honest participants going offline and later rejoining. A well-known limitation is that dynamically available protocols, on their own, cannot provide strong safety guarantees during network partitions or extended asynchrony. Ebb-and-flow protocols [SP21] address this by combining a dynamically available protocol with a partially synchronous finality protocol that irrevocably finalizes a prefix.
  We present Majorum, an ebb-and-flow construction whose dynamically available component builds on a quorum-based protocol (TOB-SVD). Under optimistic conditions, Majorum finalizes blocks in as few as three slots while requiring only a single voting phase per slot. In particular, when conditions remain favourable, each slot finalizes the next block extending the previously finalized one.

</details>


### [9] [A Scheduling Framework for Efficient MoE Inference on Edge GPU-NDP Systems](https://arxiv.org/abs/2601.03992)
*Qi Wu,Chao Fang,Jiayuan Chen,Ye Lin,Yueqi Zhang,Yichuan Bai,Yuan Du,Li Du*

Main category: cs.DC

TL;DR: 提出一个针对边缘GPU-NDP系统的MoE推理优化框架，通过张量并行、负载均衡调度和无数据集预取策略，实现2.41倍平均加速


<details>
  <summary>Details</summary>
Motivation: MoE模型在边缘部署时面临三大挑战：1) NDP单元间负载不均衡（专家选择不均匀和专家并行）；2) NDP单元内GPU利用率不足；3) 需要大量数据预分析来预测不可预测的专家激活模式

Method: 提出高效推理框架，包含三个关键优化：1) 利用MoE推理中未充分探索的张量并行，在边缘低批量场景下跨多个NDP单元分区计算大型专家参数；2) 负载均衡感知调度算法，在NDP单元和GPU间分配专家计算以最大化资源利用率；3) 无数据集预取策略，主动加载频繁访问的专家以最小化激活延迟

Result: 实验结果显示，该框架使GPU-NDP系统在端到端延迟上相比最先进方法平均加速2.41倍，最高达2.56倍，显著提升了资源受限环境中的MoE推理效率

Conclusion: 提出的优化框架有效解决了边缘GPU-NDP系统中MoE推理的关键挑战，通过张量并行、负载均衡调度和智能预取策略，显著提升了推理性能和资源利用率

Abstract: Mixture-of-Experts (MoE) models facilitate edge deployment by decoupling model capacity from active computation, yet their large memory footprint drives the need for GPU systems with near-data processing (NDP) capabilities that offload experts to dedicated processing units. However, deploying MoE models on such edge-based GPU-NDP systems faces three critical challenges: 1) severe load imbalance across NDP units due to non-uniform expert selection and expert parallelism, 2) insufficient GPU utilization during expert computation within NDP units, and 3) extensive data pre-profiling necessitated by unpredictable expert activation patterns for pre-fetching. To address these challenges, this paper proposes an efficient inference framework featuring three key optimizations. First, the underexplored tensor parallelism in MoE inference is exploited to partition and compute large expert parameters across multiple NDP units simultaneously towards edge low-batch scenarios. Second, a load-balancing-aware scheduling algorithm distributes expert computations across NDP units and GPU to maximize resource utilization. Third, a dataset-free pre-fetching strategy proactively loads frequently accessed experts to minimize activation delays. Experimental results show that our framework enables GPU-NDP systems to achieve 2.41x on average and up to 2.56x speedup in end-to-end latency compared to state-of-the-art approaches, significantly enhancing MoE inference efficiency in resource-constrained environments.

</details>


### [10] [Hummingbird: SLO-Oriented GPU Preemption at Microsecond-scale](https://arxiv.org/abs/2601.04071)
*Tiancheng Hu,Chenxi Wang,Ting Cao,Jin Qin,Lei Chen,Xinyu Xiao,Junhao Hu,Hongliang Tian,Shoumeng Yan,Huimin Cui,Quan Chen,Tao Xie*

Main category: cs.DC

TL;DR: Hummingbird是一个面向SLO的GPU调度系统，通过在闭源GPU上实现微秒级抢占，同时有效利用空闲GPU时间片，显著提升高优先级任务的SLO达成率，同时提高低优先级任务的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有GPU共享技术（包括空间和时间共享）在提高利用率的同时，由于缺乏对闭源GPU的细粒度任务调度，难以同时确保SLO（服务水平目标）遵守和最大化效率。

Method: Hummingbird通过在闭源GPU上实现微秒级抢占机制，同时有效收集和利用空闲GPU时间片，实现SLO导向的GPU调度。

Result: 与最先进的空间和时间共享方法相比，Hummingbird将高优先级任务的SLO达成率分别提高了9.7倍和3.5倍。高优先级任务与低优先级任务共存时，SLO达成率仅下降不到1%。低优先级任务的吞吐量比最先进的时间共享方法高出2.4倍。

Conclusion: Hummingbird在确保SLO的同时显著提高了GPU利用率，证明了其在GPU调度方面的显著有效性。

Abstract: Existing GPU-sharing techniques, including spatial and temporal sharing, aim to improve utilization but face challenges in simultaneously ensuring SLO adherence and maximizing efficiency due to the lack of fine-grained task scheduling on closed-source GPUs. This paper presents Hummingbird, an SLO-oriented GPU scheduling system that overcomes these challenges by enabling microsecond-scale preemption on closed-source GPUs while effectively harvesting idle GPU time slices. Comprehensive evaluations across diverse GPU architectures reveal that Hummingbird improves the SLO attainment of high-priority tasks by 9.7x and 3.5x compared to the state-of-the-art spatial and temporal-sharing approaches. When compared to executing exclusively, the SLO attainment of the high-priority task, collocating with low-priority tasks on Hummingbird, only drops by less than 1%. Meanwhile, the throughput of the low-priority task outperforms the state-of-the-art temporal-sharing approaches by 2.4x. Hummingbird demonstrates significant effectiveness in ensuring the SLO while enhancing GPU utilization.

</details>


### [11] [Failure-Resilient and Carbon-Efficient Deployment of Microservices over the Cloud-Edge Continuum](https://arxiv.org/abs/2601.04123)
*Francisco Ponce,Simone Gazza,Andrea D'Iapico,Roberto Amadini,Antonio Brogi,Stefano Forti,Saverio Giallorenzo,Pierluigi Plebani,Davide Usai,Monica Vitali,Gianluigi Zavattaro,Jacopo Soldani*

Main category: cs.DC

TL;DR: FREEDA工具链用于在云边连续体上自动化部署微服务应用，平衡故障恢复力、性能和碳效率等冲突目标


<details>
  <summary>Details</summary>
Motivation: 在异构动态的云边基础设施上部署微服务应用需要平衡冲突目标：故障恢复力、性能和环境可持续性

Method: 开发FREEDA工具链，持续适应变化的操作条件、资源可用性和可持续性约束，使用模拟和仿真场景验证有效性

Result: FREEDA能够自主重新配置部署，通过迁移服务、调整配置或重新平衡工作负载，成功实现恢复力、效率和环境影响之间的最优平衡

Conclusion: FREEDA工具链有效解决了云边连续体上微服务应用的故障恢复和碳效率部署问题，展示了自主适应变化条件的能力

Abstract: Deploying microservice-based applications (MSAs) on heterogeneous and dynamic Cloud-Edge infrastructures requires balancing conflicting objectives, such as failure resilience, performance, and environmental sustainability. In this article, we introduce the FREEDA toolchain, designed to automate the failure-resilient and carbon-efficient deployment of MSAs over the Cloud-Edge Continuum.
  The FREEDA toolchain continuously adapts deployment configurations to changing operational conditions, resource availability, and sustainability constraints, aiming to maintain the MSA quality and service continuity while reducing carbon emissions. We also introduce an experimental suite using diverse simulated and emulated scenarios to validate the effectiveness of the toolchain against real-world challenges, including resource exhaustion, node failures, and carbon intensity fluctuations. The results demonstrate FREEDA's capability to autonomously reconfigure deployments by migrating services, adjusting flavour selections, or rebalancing workloads, successfully achieving an optimal balance among resilience, efficiency, and environmental impact.

</details>
