{"id": "2601.09773", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.09773", "abs": "https://arxiv.org/abs/2601.09773", "authors": ["Binglei Lou", "Ruilin Wu", "Philip Leong"], "title": "Enhancing LUT-based Deep Neural Networks Inference through Architecture and Connectivity Optimization", "comment": "arXiv admin note: substantial text overlap with arXiv:2503.12829, arXiv:2406.04910", "summary": "Deploying deep neural networks (DNNs) on resource-constrained edge devices such as FPGAs requires a careful balance among latency, power, and hardware resource usage, while maintaining high accuracy. Existing Lookup Table (LUT)-based DNNs -- such as LogicNets, PolyLUT, and NeuraLUT -- face two critical challenges: the exponential growth of LUT size and inefficient random sparse connectivity. This paper presents SparseLUT, a comprehensive framework that addresses these challenges through two orthogonal optimizations. First, we propose an architectural enhancement that aggregates multiple PolyLUT sub-neurons via an adder, significantly reducing LUT consumption by 2.0x-13.9x and lowering inference latency by 1.2x-1.6x, all while maintaining comparable accuracy. Building upon this foundation, we further introduce a non-greedy training algorithm that optimizes neuron connectivity by selectively pruning less significant inputs and strategically regrowing more effective ones. This training optimization, which incurs no additional area and latency overhead, delivers consistent accuracy improvements across benchmarks -- achieving up to a 2.13% gain on MNIST and 0.94% on Jet Substructure Classification compared to existing LUT-DNN approaches.", "AI": {"tldr": "SparseLUT\u6846\u67b6\u901a\u8fc7\u67b6\u6784\u589e\u5f3a\u548c\u8bad\u7ec3\u4f18\u5316\u89e3\u51b3LUT-DNN\u4e2dLUT\u5c3a\u5bf8\u6307\u6570\u589e\u957f\u548c\u7a00\u758f\u8fde\u63a5\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u548c\u5ef6\u8fdf\uff0c\u540c\u65f6\u63d0\u5347\u7cbe\u5ea6\u3002", "motivation": "\u5728FPGA\u7b49\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72DNN\u9700\u8981\u5e73\u8861\u5ef6\u8fdf\u3001\u529f\u8017\u3001\u786c\u4ef6\u8d44\u6e90\u548c\u7cbe\u5ea6\u3002\u73b0\u6709LUT-based DNN\uff08\u5982LogicNets\u3001PolyLUT\u3001NeuraLUT\uff09\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1aLUT\u5c3a\u5bf8\u7684\u6307\u6570\u589e\u957f\u548c\u968f\u673a\u7a00\u758f\u8fde\u63a5\u7684\u4f4e\u6548\u7387\u3002", "method": "\u63d0\u51faSparseLUT\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6b63\u4ea4\u4f18\u5316\uff1a1\uff09\u67b6\u6784\u589e\u5f3a\uff1a\u901a\u8fc7\u52a0\u6cd5\u5668\u805a\u5408\u591a\u4e2aPolyLUT\u5b50\u795e\u7ecf\u5143\uff0c\u663e\u8457\u51cf\u5c11LUT\u6d88\u8017\uff1b2\uff09\u975e\u8d2a\u5a6a\u8bad\u7ec3\u7b97\u6cd5\uff1a\u901a\u8fc7\u9009\u62e9\u6027\u526a\u679d\u4e0d\u91cd\u8981\u7684\u8f93\u5165\u5e76\u7b56\u7565\u6027\u5730\u91cd\u65b0\u751f\u957f\u66f4\u6709\u6548\u7684\u8fde\u63a5\u6765\u4f18\u5316\u795e\u7ecf\u5143\u8fde\u63a5\u3002", "result": "\u67b6\u6784\u589e\u5f3a\u5c06LUT\u6d88\u8017\u51cf\u5c112.0x-13.9x\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e1.2x-1.6x\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7cbe\u5ea6\u3002\u8bad\u7ec3\u4f18\u5316\u5728MNIST\u4e0a\u83b7\u5f97\u6700\u9ad82.13%\u7cbe\u5ea6\u63d0\u5347\uff0c\u5728Jet Substructure Classification\u4e0a\u83b7\u5f970.94%\u63d0\u5347\uff0c\u4e14\u4e0d\u589e\u52a0\u989d\u5916\u9762\u79ef\u548c\u5ef6\u8fdf\u5f00\u9500\u3002", "conclusion": "SparseLUT\u901a\u8fc7\u67b6\u6784\u548c\u8bad\u7ec3\u534f\u540c\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86LUT-DNN\u7684\u8d44\u6e90\u6548\u7387\u548c\u8fde\u63a5\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548DNN\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.10463", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.10463", "abs": "https://arxiv.org/abs/2601.10463", "authors": ["Xinyu Shi", "Simei Yang", "Francky Catthoor"], "title": "Architectural Classification of XR Workloads: Cross-Layer Archetypes and Implications", "comment": null, "summary": "Edge and mobile platforms for augmented and virtual reality, collectively referred to as extended reality (XR) must deliver deterministic ultra-low-latency performance under stringent power and area constraints. However, the diversity of XR workloads is rapidly increasing, characterized by heterogeneous operator types and complex dataflow structures. This trend poses significant challenges to conventional accelerator architectures centered around convolutional neural networks (CNNs), resulting in diminishing returns for traditional compute-centric optimization strategies. Despite the importance of this problem, a systematic architectural understanding of the full XR pipeline remains lacking. In this paper, we present an architectural classification of XR workloads using a cross-layer methodology that integrates model-based high-level design space exploration (DSE) with empirical profiling on commercial GPU and CPU hardware. By analyzing a representative set of workloads spanning 12 distinct XR kernels, we distill their complex architectural characteristics into a small set of cross-layer workload archetypes (e.g., capacity-limited and overhead-sensitive). Building on these archetypes, we further extract key architectural insights and provide actionable design guidelines for next-generation XR SoCs. Our study highlights that XR architecture design must shift from generic resource scaling toward phase-aware scheduling and elastic resource allocation in order to achieve greater energy efficiency and high performance in future XR systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u5c42\u65b9\u6cd5\u5bf9XR\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u67b6\u6784\u5206\u7c7b\uff0c\u8bc6\u522b\u51fa\u5bb9\u91cf\u53d7\u9650\u548c\u5f00\u9500\u654f\u611f\u7b49\u5de5\u4f5c\u8d1f\u8f7d\u539f\u578b\uff0c\u4e3a\u4e0b\u4e00\u4ee3XR SoC\u63d0\u4f9b\u8bbe\u8ba1\u6307\u5357\uff0c\u5f3a\u8c03\u9700\u8981\u4ece\u901a\u7528\u8d44\u6e90\u6269\u5c55\u8f6c\u5411\u9636\u6bb5\u611f\u77e5\u8c03\u5ea6\u548c\u5f39\u6027\u8d44\u6e90\u5206\u914d\u3002", "motivation": "XR\u5e73\u53f0\u9700\u8981\u5728\u4e25\u683c\u7684\u529f\u8017\u548c\u9762\u79ef\u7ea6\u675f\u4e0b\u63d0\u4f9b\u786e\u5b9a\u6027\u7684\u8d85\u4f4e\u5ef6\u8fdf\u6027\u80fd\uff0c\u4f46XR\u5de5\u4f5c\u8d1f\u8f7d\u7684\u591a\u6837\u6027\u4e0d\u65ad\u589e\u52a0\uff0c\u5177\u6709\u5f02\u6784\u7b97\u5b50\u7c7b\u578b\u548c\u590d\u6742\u6570\u636e\u6d41\u7ed3\u6784\uff0c\u8fd9\u5bf9\u4f20\u7edfCNN\u4e3a\u4e2d\u5fc3\u7684\u52a0\u901f\u5668\u67b6\u6784\u6784\u6210\u6311\u6218\uff0c\u7f3a\u4e4f\u5bf9\u5b8c\u6574XR\u7ba1\u9053\u7684\u7cfb\u7edf\u6027\u67b6\u6784\u7406\u89e3\u3002", "method": "\u91c7\u7528\u8de8\u5c42\u65b9\u6cd5\uff0c\u7ed3\u5408\u57fa\u4e8e\u6a21\u578b\u7684\u9ad8\u5c42\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u548c\u5728\u5546\u7528GPU\u548cCPU\u786c\u4ef6\u4e0a\u7684\u7ecf\u9a8c\u5206\u6790\uff0c\u5206\u679012\u4e2a\u4e0d\u540cXR\u5185\u6838\u7684\u4ee3\u8868\u6027\u5de5\u4f5c\u8d1f\u8f7d\u96c6\uff0c\u5c06\u5176\u590d\u6742\u67b6\u6784\u7279\u5f81\u63d0\u70bc\u4e3a\u5c11\u91cf\u8de8\u5c42\u5de5\u4f5c\u8d1f\u8f7d\u539f\u578b\u3002", "result": "\u8bc6\u522b\u51fa\u5bb9\u91cf\u53d7\u9650\u3001\u5f00\u9500\u654f\u611f\u7b49\u5de5\u4f5c\u8d1f\u8f7d\u539f\u578b\uff0c\u63d0\u53d6\u5173\u952e\u67b6\u6784\u6d1e\u5bdf\uff0c\u4e3a\u4e0b\u4e00\u4ee3XR SoC\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u6307\u5357\uff0c\u53d1\u73b0XR\u67b6\u6784\u8bbe\u8ba1\u9700\u8981\u4ece\u901a\u7528\u8d44\u6e90\u6269\u5c55\u8f6c\u5411\u9636\u6bb5\u611f\u77e5\u8c03\u5ea6\u548c\u5f39\u6027\u8d44\u6e90\u5206\u914d\u3002", "conclusion": "XR\u67b6\u6784\u8bbe\u8ba1\u5fc5\u987b\u4ece\u901a\u7528\u8d44\u6e90\u6269\u5c55\u8f6c\u5411\u9636\u6bb5\u611f\u77e5\u8c03\u5ea6\u548c\u5f39\u6027\u8d44\u6e90\u5206\u914d\uff0c\u4ee5\u5b9e\u73b0\u672a\u6765XR\u7cfb\u7edf\u66f4\u9ad8\u7684\u80fd\u6548\u548c\u6027\u80fd\uff0c\u8de8\u5c42\u5de5\u4f5c\u8d1f\u8f7d\u539f\u578b\u4e3a\u4e0b\u4e00\u4ee3XR SoC\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u6307\u5bfc\u6846\u67b6\u3002"}}
{"id": "2601.09978", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.09978", "abs": "https://arxiv.org/abs/2601.09978", "authors": ["Jer Shyuan Ng", "Wathsara Daluwatta", "Shehan Edirimannage", "Charitha Elvitigala", "Asitha Kottahachchi Kankanamge Don", "Ibrahim Khalil", "Heng Zhang", "Dusit Niyato"], "title": "Federated Unlearning in Edge Networks: A Survey of Fundamentals, Challenges, Practical Applications and Future Directions", "comment": null, "summary": "The proliferation of connected devices and privacy-sensitive applications has accelerated the adoption of Federated Learning (FL), a decentralized paradigm that enables collaborative model training without sharing raw data. While FL addresses data locality and privacy concerns, it does not inherently support data deletion requests that are increasingly mandated by regulations such as the Right to be Forgotten (RTBF). In centralized learning, this challenge has been studied under the concept of Machine Unlearning (MU), that focuses on efficiently removing the influence of specific data samples or clients from trained models. Extending this notion to federated settings has given rise to Federated Unlearning (FUL), a new research area concerned with eliminating the contributions of individual clients or data subsets from the global FL model in a distributed and heterogeneous environment. In this survey, we first introduce the fundamentals of FUL. Then, we review the FUL frameworks that are proposed to address the three main implementation challenges, i.e., communication cost, resource allocation as well as security and privacy. Furthermore, we discuss applications of FUL in the modern distributed computer networks. We also highlight the open challenges and future research opportunities. By consolidating existing knowledge and mapping open problems, this survey aims to serve as a foundational reference for researchers and practitioners seeking to advance FL to build trustworthy, regulation-compliant and user-centric federated systems.", "AI": {"tldr": "\u672c\u6587\u662f\u5173\u4e8e\u8054\u90a6\u9057\u5fd8\u5b66\u4e60\uff08FUL\uff09\u7684\u7efc\u8ff0\uff0c\u63a2\u8ba8\u5982\u4f55\u5728\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\u5b9e\u73b0\u6570\u636e\u5220\u9664\u4ee5\u7b26\u5408\u9690\u79c1\u6cd5\u89c4\u8981\u6c42\u3002", "motivation": "\u968f\u7740\u8fde\u63a5\u8bbe\u5907\u548c\u9690\u79c1\u654f\u611f\u5e94\u7528\u7684\u666e\u53ca\uff0c\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u6210\u4e3a\u91cd\u8981\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u8303\u5f0f\u3002\u7136\u800c\uff0cFL\u672c\u8eab\u4e0d\u652f\u6301\u6570\u636e\u5220\u9664\u8bf7\u6c42\uff0c\u800c\u300a\u88ab\u9057\u5fd8\u6743\u300b\uff08RTBF\uff09\u7b49\u6cd5\u89c4\u8981\u6c42\u5fc5\u987b\u80fd\u591f\u5220\u9664\u7279\u5b9a\u6570\u636e\u3002\u73b0\u6709\u96c6\u4e2d\u5f0f\u673a\u5668\u5b66\u4e60\u9057\u5fd8\uff08MU\uff09\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u8054\u90a6\u73af\u5883\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u8054\u90a6\u9057\u5fd8\u5b66\u4e60\uff08FUL\uff09\u3002", "method": "\u672c\u6587\u662f\u4e00\u7bc7\u7efc\u8ff0\u6027\u7814\u7a76\uff0c\u9996\u5148\u4ecb\u7ecdFUL\u7684\u57fa\u7840\u77e5\u8bc6\uff0c\u7136\u540e\u7cfb\u7edf\u56de\u987e\u73b0\u6709\u7684FUL\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u4e09\u4e2a\u4e3b\u8981\u5b9e\u73b0\u6311\u6218\uff1a\u901a\u4fe1\u6210\u672c\u3001\u8d44\u6e90\u5206\u914d\u4ee5\u53ca\u5b89\u5168\u4e0e\u9690\u79c1\u3002\u540c\u65f6\u63a2\u8ba8FUL\u5728\u73b0\u4ee3\u5206\u5e03\u5f0f\u8ba1\u7b97\u673a\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u672c\u6587\u6574\u5408\u4e86\u73b0\u6709\u77e5\u8bc6\uff0c\u68b3\u7406\u4e86FUL\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u5305\u62ec\u5e94\u5bf9\u901a\u4fe1\u6210\u672c\u3001\u8d44\u6e90\u5206\u914d\u548c\u5b89\u5168\u9690\u79c1\u6311\u6218\u7684\u5404\u79cd\u6846\u67b6\uff0c\u4ee5\u53ca\u5728\u5206\u5e03\u5f0f\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u573a\u666f\u3002", "conclusion": "FUL\u662f\u4e00\u4e2a\u65b0\u5174\u7684\u7814\u7a76\u9886\u57df\uff0c\u65e8\u5728\u4f7f\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7b26\u5408\u9690\u79c1\u6cd5\u89c4\u8981\u6c42\u3002\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u73b0\u6709\u5de5\u4f5c\uff0c\u6307\u51fa\u4e86\u5f00\u653e\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1\u3001\u5408\u89c4\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8054\u90a6\u7cfb\u7edf\u63d0\u4f9b\u57fa\u7840\u53c2\u8003\u3002"}}
{"id": "2601.09808", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.09808", "abs": "https://arxiv.org/abs/2601.09808", "authors": ["Chen Ling", "Yachen Wang"], "title": "From Dynamic to Lexical: A Comparative Exploration of Scoping Rules in SAS and R", "comment": "This paper was originally published in the SESUG 2025 Conference Proceedings. Cary, NC", "summary": "Variable scoping dictates how and where variables are accessible within programming languages, playing a crucial role in code efficiency and organization. This paper examines the distinct scoping rules in SAS and R, focusing on SAS's dynamic scoping and R's lexical scoping. In SAS, dynamic scoping utilizes symbol tables, resolving variables at runtime by dynamically searching through active macro layers. R, in contrast, employs lexical scoping, using environments to resolve variables based on the structure in which functions are defined. Illustrative examples highlight the differences between these scoping strategies, showcasing their impact on code behavior. Additionally, the paper outlines methods for inspecting variables in SAS's symbol tables and R's environments, offering practical insights for debugging and optimization. Strategies for controlling variable scope in both languages are discussed, enhancing code precision and reliability. This exploration equips programmers with critical understanding to optimize variable management, improving their programming practices in SAS and R.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u6bd4\u5206\u6790\u4e86SAS\u7684\u52a8\u6001\u4f5c\u7528\u57df\u548cR\u7684\u8bcd\u6cd5\u4f5c\u7528\u57df\u89c4\u5219\uff0c\u63a2\u8ba8\u4e86\u5b83\u4eec\u5728\u53d8\u91cf\u8bbf\u95ee\u3001\u89e3\u6790\u673a\u5236\u4e0a\u7684\u5dee\u5f02\uff0c\u5e76\u63d0\u4f9b\u4e86\u8c03\u8bd5\u548c\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u53d8\u91cf\u4f5c\u7528\u57df\u5bf9\u4ee3\u7801\u6548\u7387\u548c\u7ed3\u6784\u81f3\u5173\u91cd\u8981\uff0c\u4f46SAS\u548cR\u91c7\u7528\u4e0d\u540c\u7684\u4f5c\u7528\u57df\u89c4\u5219\uff08\u52a8\u6001vs\u8bcd\u6cd5\uff09\uff0c\u7406\u89e3\u8fd9\u4e9b\u5dee\u5f02\u5bf9\u7a0b\u5e8f\u5458\u4f18\u5316\u53d8\u91cf\u7ba1\u7406\u3001\u63d0\u9ad8\u4ee3\u7801\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\u975e\u5e38\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790SAS\u7684\u52a8\u6001\u4f5c\u7528\u57df\uff08\u4f7f\u7528\u7b26\u53f7\u8868\uff0c\u8fd0\u884c\u65f6\u52a8\u6001\u641c\u7d22\u5b8f\u5c42\uff09\u548cR\u7684\u8bcd\u6cd5\u4f5c\u7528\u57df\uff08\u4f7f\u7528\u73af\u5883\uff0c\u57fa\u4e8e\u51fd\u6570\u5b9a\u4e49\u7ed3\u6784\u89e3\u6790\u53d8\u91cf\uff09\uff0c\u63d0\u4f9b\u5177\u4f53\u793a\u4f8b\u5c55\u793a\u5dee\u5f02\uff0c\u5e76\u4ecb\u7ecd\u4e24\u79cd\u8bed\u8a00\u4e2d\u68c0\u67e5\u53d8\u91cf\uff08SAS\u7b26\u53f7\u8868\u548cR\u73af\u5883\uff09\u7684\u65b9\u6cd5\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u4e24\u79cd\u4f5c\u7528\u57df\u7b56\u7565\u5bf9\u4ee3\u7801\u884c\u4e3a\u7684\u5177\u4f53\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8c03\u8bd5\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u63a7\u5236\u53d8\u91cf\u4f5c\u7528\u57df\u7684\u7b56\u7565\uff0c\u5e2e\u52a9\u7a0b\u5e8f\u5458\u66f4\u597d\u5730\u7ba1\u7406\u53d8\u91cf\u3002", "conclusion": "\u901a\u8fc7\u6df1\u5165\u7406\u89e3SAS\u548cR\u7684\u4f5c\u7528\u57df\u673a\u5236\u5dee\u5f02\uff0c\u7a0b\u5e8f\u5458\u80fd\u591f\u4f18\u5316\u53d8\u91cf\u7ba1\u7406\uff0c\u6539\u8fdb\u5728\u8fd9\u4e24\u79cd\u8bed\u8a00\u4e2d\u7684\u7f16\u7a0b\u5b9e\u8df5\uff0c\u63d0\u9ad8\u4ee3\u7801\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.10177", "categories": ["cs.DC", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.10177", "abs": "https://arxiv.org/abs/2601.10177", "authors": ["Ziting Zhang", "Kai Wan", "Minquan Cheng", "Shuo Shao", "Giuseppe Caire"], "title": "Distributed Linearly Separable Computation with Arbitrary Heterogeneous Data Assignment", "comment": null, "summary": "Distributed linearly separable computation is a fundamental problem in large-scale distributed systems, requiring the computation of linearly separable functions over different datasets across distributed workers. This paper studies a heterogeneous distributed linearly separable computation problem, including one master and N distributed workers. The linearly separable task function involves Kc linear combinations of K messages, where each message is a function of one dataset. Distinguished from the existing homogeneous settings that assume each worker holds the same number of datasets, where the data assignment is carefully designed and controlled by the data center (e.g., the cyclic assignment), we consider a more general setting with arbitrary heterogeneous data assignment across workers, where `arbitrary' means that the data assignment is given in advance and `heterogeneous' means that the workers may hold different numbers of datasets. Our objective is to characterize the fundamental tradeoff between the computable dimension of the task function and the communication cost under arbitrary heterogeneous data assignment. Under the constraint of integer communication costs, for arbitrary heterogeneous data assignment, we propose a universal computing scheme and a universal converse bound by characterizing the structure of data assignment, where they coincide under some parameter regimes. We then extend the proposed computing scheme and converse bound to the case of fractional communication costs.", "AI": {"tldr": "\u7814\u7a76\u5f02\u6784\u5206\u5e03\u5f0f\u7ebf\u6027\u53ef\u5206\u8ba1\u7b97\u95ee\u9898\uff0c\u9488\u5bf9\u4efb\u610f\u5f02\u6784\u6570\u636e\u5206\u914d\uff0c\u63d0\u51fa\u901a\u7528\u8ba1\u7b97\u65b9\u6848\u548c\u9006\u754c\uff0c\u523b\u753b\u4efb\u52a1\u51fd\u6570\u53ef\u8ba1\u7b97\u7ef4\u5ea6\u4e0e\u901a\u4fe1\u6210\u672c\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u540c\u6784\u8bbe\u7f6e\uff08\u6bcf\u4e2a\u5de5\u4f5c\u8005\u6301\u6709\u76f8\u540c\u6570\u91cf\u7684\u6570\u636e\u96c6\uff09\uff0c\u800c\u5b9e\u9645\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u6570\u636e\u5206\u914d\u5f80\u5f80\u662f\u5f02\u6784\u7684\u3002\u672c\u6587\u7814\u7a76\u66f4\u4e00\u822c\u7684\u4efb\u610f\u5f02\u6784\u6570\u636e\u5206\u914d\u573a\u666f\uff0c\u5176\u4e2d\u6570\u636e\u5206\u914d\u662f\u9884\u5148\u7ed9\u5b9a\u7684\uff0c\u4e14\u5de5\u4f5c\u8005\u53ef\u80fd\u6301\u6709\u4e0d\u540c\u6570\u91cf\u7684\u6570\u636e\u96c6\u3002", "method": "\u9488\u5bf9\u6574\u6570\u901a\u4fe1\u6210\u672c\u7ea6\u675f\u4e0b\u7684\u4efb\u610f\u5f02\u6784\u6570\u636e\u5206\u914d\uff0c\u63d0\u51fa\u901a\u7528\u8ba1\u7b97\u65b9\u6848\u548c\u901a\u7528\u9006\u754c\uff0c\u901a\u8fc7\u523b\u753b\u6570\u636e\u5206\u914d\u7ed3\u6784\u6765\u5206\u6790\u53ef\u8ba1\u7b97\u7ef4\u5ea6\u4e0e\u901a\u4fe1\u6210\u672c\u7684\u6743\u8861\u3002\u7136\u540e\u5c06\u65b9\u6848\u548c\u9006\u754c\u6269\u5c55\u5230\u5206\u6570\u901a\u4fe1\u6210\u672c\u60c5\u51b5\u3002", "result": "\u63d0\u51fa\u7684\u901a\u7528\u8ba1\u7b97\u65b9\u6848\u548c\u9006\u754c\u5728\u67d0\u4e9b\u53c2\u6570\u8303\u56f4\u5185\u662f\u4e00\u81f4\u7684\uff0c\u4e3a\u5f02\u6784\u5206\u5e03\u5f0f\u7ebf\u6027\u53ef\u5206\u8ba1\u7b97\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6027\u80fd\u754c\u9650\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u5f02\u6784\u5206\u5e03\u5f0f\u7ebf\u6027\u53ef\u5206\u8ba1\u7b97\u95ee\u9898\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5728\u4efb\u610f\u5f02\u6784\u6570\u636e\u5206\u914d\u4e0b\u523b\u753b\u4e86\u53ef\u8ba1\u7b97\u7ef4\u5ea6\u4e0e\u901a\u4fe1\u6210\u672c\u7684\u57fa\u672c\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u5b9e\u9645\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2601.09839", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.09839", "abs": "https://arxiv.org/abs/2601.09839", "authors": ["Chen Ling", "Yachen Wang"], "title": "Lazy Evaluation: A Comparative Analysis of SAS MACROs and R Functions", "comment": "This paper was originally published in SESUG 2025 Conference Proceedings. Cary, NC: SouthEast SAS Users Group", "summary": "Lazy evaluation is a powerful technique that can optimize code execution by deferring evaluations until their results are required, thus enhancing efficiency. In most modern programming languages, like R, lazy evaluation is commonly applied to function arguments. However, the application of lazy evaluation in SAS has not been extensively explored. This paper focuses on the mechanisms of lazy evaluation in SAS MACROs and R functions, offering a comparative analysis of the underlying principles that drive these processes.\n  R's lazy evaluation is driven by a data structure called Promise, which postpones evaluation and does not occupy memory until the value is needed, utilizing a call-by-need strategy. SAS, on the other hand, achieves lazy evaluation through its symbol tables, employing memory to store parameters, and operates on a call-by-name basis. These discrepancies in lazy evaluation strategies can notably impact the results of R functions and SAS MACROs. By examining these distinct approaches, the paper illuminates the impact of lazy evaluation on programming efficiency, supported by illustrative examples. As the shift from SAS to R becomes increasingly prevalent in the pharmaceutical industry, understanding these techniques enables programmers to optimize their code for greater efficacy. This exploration serves as a guide to enhance programming capabilities and performance in both languages.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86SAS MACROs\u548cR\u51fd\u6570\u4e2d\u7684\u60f0\u6027\u6c42\u503c\u673a\u5236\uff0c\u5206\u6790\u4e86\u4e24\u79cd\u8bed\u8a00\u5728\u5b9e\u73b0\u60f0\u6027\u6c42\u503c\u65f6\u7684\u4e0d\u540c\u7b56\u7565\u53ca\u5176\u5bf9\u7f16\u7a0b\u6548\u7387\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u5236\u836f\u884c\u4e1a\u4eceSAS\u5411R\u7684\u8f6c\u578b\u65e5\u76ca\u666e\u904d\uff0c\u7406\u89e3\u4e24\u79cd\u8bed\u8a00\u4e2d\u60f0\u6027\u6c42\u503c\u6280\u672f\u7684\u5dee\u5f02\u5bf9\u4e8e\u7a0b\u5e8f\u5458\u4f18\u5316\u4ee3\u7801\u3001\u63d0\u9ad8\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136R\u4e2d\u7684\u60f0\u6027\u6c42\u503c\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46SAS\u4e2d\u7684\u76f8\u5173\u5e94\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5206\u6790SAS MACROs\u548cR\u51fd\u6570\u7684\u60f0\u6027\u6c42\u503c\u673a\u5236\uff1aR\u4f7f\u7528Promise\u6570\u636e\u7ed3\u6784\u5b9e\u73b0\u6309\u9700\u8c03\u7528\u7b56\u7565\uff0c\u800cSAS\u901a\u8fc7\u7b26\u53f7\u8868\u5b9e\u73b0\u6309\u540d\u8c03\u7528\u7b56\u7565\u3002\u8bba\u6587\u901a\u8fc7\u793a\u4f8b\u8bf4\u660e\u8fd9\u4e9b\u5dee\u5f02\u5982\u4f55\u5f71\u54cd\u7f16\u7a0b\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0R\u548cSAS\u5728\u60f0\u6027\u6c42\u503c\u5b9e\u73b0\u4e0a\u5b58\u5728\u6839\u672c\u5dee\u5f02\uff1aR\u7684Promise\u7ed3\u6784\u5ef6\u8fdf\u6c42\u503c\u4e14\u4e0d\u5360\u7528\u5185\u5b58\uff0c\u800cSAS\u7684\u7b26\u53f7\u8868\u9700\u8981\u5b58\u50a8\u53c2\u6570\u5360\u7528\u5185\u5b58\u3002\u8fd9\u4e9b\u5dee\u5f02\u663e\u8457\u5f71\u54cdR\u51fd\u6570\u548cSAS MACROs\u7684\u6267\u884c\u7ed3\u679c\u548c\u6548\u7387\u3002", "conclusion": "\u7406\u89e3SAS\u548cR\u4e2d\u60f0\u6027\u6c42\u503c\u6280\u672f\u7684\u5dee\u5f02\u6709\u52a9\u4e8e\u7a0b\u5e8f\u5458\u5728\u4e24\u79cd\u8bed\u8a00\u4e2d\u4f18\u5316\u4ee3\u7801\uff0c\u63d0\u9ad8\u7f16\u7a0b\u6548\u7387\u548c\u6027\u80fd\u3002\u8bba\u6587\u4e3a\u5236\u836f\u884c\u4e1a\u4eceSAS\u8f6c\u5411R\u7684\u7a0b\u5e8f\u5458\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e2e\u52a9\u4ed6\u4eec\u66f4\u597d\u5730\u5229\u7528\u4e24\u79cd\u8bed\u8a00\u7684\u7279\u6027\u3002"}}
{"id": "2601.10277", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.10277", "abs": "https://arxiv.org/abs/2601.10277", "authors": ["Evangelos Kolyvas", "Alexandros Antonov", "Spyros Voulgaris"], "title": "SCRamble: Adaptive Decentralized Overlay Construction for Blockchain Networks", "comment": "5 pages, 3 figures, The 27th ACM International Conference on Distributed Computing and Networking, ACM ICDCN 2026", "summary": "Despite being under development for over 15 years, transaction throughput remains one of the key challenges confronting blockchains, which typically has a cap of a limited number of transactions per second. A fundamental factor limiting this metric is the network latency associated with the block propagation throughout of the underlying peer-to-peer network, typically formed through random connections. Accelerating the dissemination of blocks not only improves transaction rates, but also enhances system security by reducing the probability of forks. This paper introduces SCRamble: a decentralized protocol that significantly reduces block dissemination time in blockchain networks. SCRamble's effectiveness is attributed to its innovative link selection strategy, which integrates two heuristics: a scoring mechanism that assesses block arrival times from neighboring peers, and a second heuristic that takes network latency into account.", "AI": {"tldr": "SCRamble\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u534f\u8bae\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u94fe\u8def\u9009\u62e9\u7b56\u7565\u663e\u8457\u51cf\u5c11\u533a\u5757\u94fe\u7f51\u7edc\u4e2d\u7684\u533a\u5757\u4f20\u64ad\u65f6\u95f4\uff0c\u4ece\u800c\u63d0\u9ad8\u4ea4\u6613\u541e\u5410\u91cf\u548c\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "motivation": "\u533a\u5757\u94fe\u53d1\u5c5515\u5e74\u6765\uff0c\u4ea4\u6613\u541e\u5410\u91cf\u4ecd\u7136\u662f\u5173\u952e\u6311\u6218\uff0c\u901a\u5e38\u53d7\u9650\u4e8e\u6bcf\u79d2\u6709\u9650\u4ea4\u6613\u6570\u3002\u9650\u5236\u8fd9\u4e00\u6307\u6807\u7684\u6839\u672c\u56e0\u7d20\u662f\u5e95\u5c42P2P\u7f51\u7edc\u4e2d\u533a\u5757\u4f20\u64ad\u7684\u7f51\u7edc\u5ef6\u8fdf\uff0c\u8fd9\u901a\u5e38\u901a\u8fc7\u968f\u673a\u8fde\u63a5\u5f62\u6210\u3002\u52a0\u901f\u533a\u5757\u4f20\u64ad\u4e0d\u4ec5\u80fd\u63d0\u9ad8\u4ea4\u6613\u901f\u7387\uff0c\u8fd8\u80fd\u901a\u8fc7\u51cf\u5c11\u5206\u53c9\u6982\u7387\u589e\u5f3a\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "method": "SCRamble\u91c7\u7528\u521b\u65b0\u7684\u94fe\u8def\u9009\u62e9\u7b56\u7565\uff0c\u6574\u5408\u4e24\u79cd\u542f\u53d1\u5f0f\u65b9\u6cd5\uff1a1) \u8bc4\u4f30\u6765\u81ea\u76f8\u90bb\u8282\u70b9\u7684\u533a\u5757\u5230\u8fbe\u65f6\u95f4\u7684\u8bc4\u5206\u673a\u5236\uff1b2) \u8003\u8651\u7f51\u7edc\u5ef6\u8fdf\u7684\u7b2c\u4e8c\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "SCRamble\u534f\u8bae\u80fd\u663e\u8457\u51cf\u5c11\u533a\u5757\u94fe\u7f51\u7edc\u4e2d\u7684\u533a\u5757\u4f20\u64ad\u65f6\u95f4\u3002", "conclusion": "SCRamble\u901a\u8fc7\u4f18\u5316\u7684\u94fe\u8def\u9009\u62e9\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u533a\u5757\u94fe\u7f51\u7edc\u4e2d\u7684\u533a\u5757\u4f20\u64ad\u5ef6\u8fdf\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u4ea4\u6613\u541e\u5410\u91cf\u548c\u7cfb\u7edf\u5b89\u5168\u6027\u3002"}}
{"id": "2601.09986", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.09986", "abs": "https://arxiv.org/abs/2601.09986", "authors": ["Cheng Zhang", "Qiancheng Fu", "Hang Ji", "Ines Santacruz Del Valle", "Alexandra Silva", "Marco Gaboardi"], "title": "Outrunning Big KATs: Efficient Decision Procedures for Variants of GKAT", "comment": "Conditionally Accepted at ESOP 2026", "summary": "This paper presents several efficient decision procedures for trace equivalence of GKAT automata, which make use of on-the-fly symbolic techniques via SAT solvers. To demonstrate applicability of our algorithms, we designed symbolic derivatives for CF-GKAT, a practical system based on GKAT designed to validate control-flow transformations. We implemented the algorithms in Rust and evaluated them on both randomly generated benchmarks and real-world control-flow transformations. Indeed, we observed order-of-magnitude performance improvements against existing implementations for both KAT and CF-GKAT. Notably, our experiments also revealed a bug in Ghidra, an industry-standard decompiler, highlighting the practical viability of these systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u51e0\u79cd\u9ad8\u6548\u7684GKAT\u81ea\u52a8\u673a\u8ff9\u7b49\u4ef7\u5224\u5b9a\u7a0b\u5e8f\uff0c\u91c7\u7528\u57fa\u4e8eSAT\u6c42\u89e3\u5668\u7684\u5373\u65f6\u7b26\u53f7\u6280\u672f\uff0c\u5e76\u5728CF-GKAT\u7cfb\u7edf\u4e2d\u5b9e\u73b0\uff0c\u6027\u80fd\u6bd4\u73b0\u6709\u5b9e\u73b0\u63d0\u5347\u6570\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "GKAT\u81ea\u52a8\u673a\u7684\u8ff9\u7b49\u4ef7\u5224\u5b9a\u5728\u7a0b\u5e8f\u9a8c\u8bc1\u548c\u7f16\u8bd1\u5668\u4f18\u5316\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u5224\u5b9a\u7b97\u6cd5\uff0c\u5e76\u5e94\u7528\u4e8e\u5b9e\u9645\u7684CF-GKAT\u7cfb\u7edf\u9a8c\u8bc1\u63a7\u5236\u6d41\u8f6c\u6362\u3002", "method": "1. \u5f00\u53d1\u4e86\u591a\u79cd\u9ad8\u6548\u7684GKAT\u81ea\u52a8\u673a\u8ff9\u7b49\u4ef7\u5224\u5b9a\u7b97\u6cd5\uff1b2. \u91c7\u7528\u57fa\u4e8eSAT\u6c42\u89e3\u5668\u7684\u5373\u65f6\u7b26\u53f7\u6280\u672f\uff1b3. \u4e3aCF-GKAT\u7cfb\u7edf\u8bbe\u8ba1\u4e86\u7b26\u53f7\u5bfc\u6570\uff1b4. \u5728Rust\u4e2d\u5b9e\u73b0\u7b97\u6cd5\uff1b5. \u5728\u968f\u673a\u751f\u6210\u57fa\u51c6\u548c\u771f\u5b9e\u63a7\u5236\u6d41\u8f6c\u6362\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "1. \u76f8\u6bd4\u73b0\u6709\u7684KAT\u548cCF-GKAT\u5b9e\u73b0\uff0c\u6027\u80fd\u63d0\u5347\u6570\u4e2a\u6570\u91cf\u7ea7\uff1b2. \u5728\u5b9e\u9a8c\u4e2d\u53d1\u73b0\u4e86\u884c\u4e1a\u6807\u51c6\u53cd\u7f16\u8bd1\u5668Ghidra\u4e2d\u7684\u4e00\u4e2abug\uff1b3. \u8bc1\u660e\u4e86\u8fd9\u4e9b\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8eSAT\u6c42\u89e3\u5668\u7684\u5373\u65f6\u7b26\u53f7\u6280\u672f\u80fd\u663e\u8457\u63d0\u9ad8GKAT\u81ea\u52a8\u673a\u8ff9\u7b49\u4ef7\u5224\u5b9a\u7684\u6548\u7387\uff0c\u5728CF-GKAT\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u63a7\u5236\u6d41\u8f6c\u6362\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u751a\u81f3\u80fd\u53d1\u73b0\u5546\u4e1a\u5de5\u5177\u4e2d\u7684\u9519\u8bef\u3002"}}
{"id": "2601.10582", "categories": ["cs.DC", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.10582", "abs": "https://arxiv.org/abs/2601.10582", "authors": ["Mridankan Mandal", "Smit Sanjay Shende"], "title": "Mitigating GIL Bottlenecks in Edge AI Systems", "comment": null, "summary": "Deploying Python based AI agents on resource-constrained edge devices presents a runtime optimization challenge: high thread counts are needed to mask I/O latency, yet Python's Global Interpreter Lock (GIL) serializes execution. We demonstrate that naive thread-pool scaling causes a \"saturation cliff\": >= 20% throughput degradation at overprovisioned thread counts (N >= 512) on edge-representative configurations. We present a lightweight profiling tool and adaptive runtime system using a Blocking Ratio metric (beta) that distinguishes genuine I/O wait from GIL contention. Our library-based solution achieves 96.5% of optimal performance without manual tuning, outperforming multiprocessing (limited by ~8x memory overhead on devices with 512 MB-2 GB RAM) and asyncio (blocked by CPU-bound phases). Evaluation across seven edge AI workload profiles, including real ML inference with ONNX Runtime MobileNetV2, demonstrates 93.9% average efficiency. Comparative experiments with Python 3.13t (free threading) show that while GIL elimination enables ~4x throughput on multi-core edge devices, the saturation cliff persists on single-core devices, validating our beta metric for both GIL and no-GIL environments. This provides practical optimization for edge AI systems.", "AI": {"tldr": "\u9488\u5bf9\u8fb9\u7f18\u8bbe\u5907Python AI\u4ee3\u7406\u7684\u8fd0\u884c\u65f6\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u963b\u585e\u6bd4(beta)\u5ea6\u91cf\u7684\u8f7b\u91cf\u7ea7\u5206\u6790\u548c\u81ea\u9002\u5e94\u7cfb\u7edf\uff0c\u89e3\u51b3\u7ebf\u7a0b\u6c60\u6269\u5c55\u4e2d\u7684\"\u9971\u548c\u60ac\u5d16\"\u95ee\u9898\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72Python AI\u4ee3\u7406\u9762\u4e34\u8fd0\u884c\u65f6\u4f18\u5316\u6311\u6218\uff1a\u9700\u8981\u9ad8\u7ebf\u7a0b\u6570\u6765\u63a9\u76d6I/O\u5ef6\u8fdf\uff0c\u4f46Python\u7684\u5168\u5c40\u89e3\u91ca\u5668\u9501(GIL)\u4f1a\u5e8f\u5217\u5316\u6267\u884c\uff0c\u5bfc\u81f4\u7ebf\u7a0b\u6c60\u6269\u5c55\u51fa\u73b0\"\u9971\u548c\u60ac\u5d16\"\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u5206\u6790\u5de5\u5177\u548c\u81ea\u9002\u5e94\u8fd0\u884c\u65f6\u7cfb\u7edf\uff0c\u4f7f\u7528\u963b\u585e\u6bd4(beta)\u5ea6\u91cf\u533a\u5206\u771f\u6b63\u7684I/O\u7b49\u5f85\u548cGIL\u4e89\u7528\u3002\u57fa\u4e8e\u5e93\u7684\u89e3\u51b3\u65b9\u6848\u65e0\u9700\u624b\u52a8\u8c03\u4f18\uff0c\u907f\u514d\u591a\u8fdb\u7a0b\u7684\u5185\u5b58\u5f00\u9500\u548c\u5f02\u6b65IO\u7684CPU\u963b\u585e\u95ee\u9898\u3002", "result": "\u7cfb\u7edf\u8fbe\u523096.5%\u7684\u6700\u4f18\u6027\u80fd\uff0c\u5728\u4e03\u4e2a\u8fb9\u7f18AI\u5de5\u4f5c\u8d1f\u8f7d\u914d\u7f6e\uff08\u5305\u62ecONNX Runtime MobileNetV2\u5b9e\u9645ML\u63a8\u7406\uff09\u4e0a\u5e73\u5747\u6548\u7387\u8fbe93.9%\u3002\u4e0ePython 3.13t\uff08\u65e0GIL\uff09\u6bd4\u8f83\u663e\u793a\uff0c\u867d\u7136\u65e0GIL\u5728\u591a\u6838\u8bbe\u5907\u4e0a\u63d0\u53474\u500d\u541e\u5410\u91cf\uff0c\u4f46\u5355\u6838\u8bbe\u5907\u4e0a\u9971\u548c\u60ac\u5d16\u95ee\u9898\u4f9d\u7136\u5b58\u5728\u3002", "conclusion": "\u63d0\u51fa\u7684beta\u5ea6\u91cf\u5728GIL\u548c\u65e0GIL\u73af\u5883\u4e0b\u90fd\u6709\u6548\uff0c\u4e3a\u8fb9\u7f18AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8fd0\u884c\u65f6\u4f18\u5316\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0aPython AI\u4ee3\u7406\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
