<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [When Lifetimes Liberate: A Type System for Arenas with Higher-Order Reachability Tracking](https://arxiv.org/abs/2509.04253)
*Siyuan He,Songlin Jia,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 这篇论文提出了一种新的类型系统，统一了区域系统、所有权类型和可达性类型的优点，在高阶函数语言中实现灵活的资源共享和生命周期控制。


<details>
  <summary>Details</summary>
Motivation: 解决高阶函数语言中静态资源管理的挑战，统一不同系统的优点：区域系统的词法作用域生命周期控制和共享能力，Rust所有权类型的非词法生命周期，以及可达性类型的分离性推理能力。

Method: 在可达性类型基础上提出两个新扩展：A<:通过二维存储模型支持粒度粗糕的可达性跟踪，{A}<:实现词法作用域生命周期控制。两者都保持了语言的高阶参数性质。

Result: 形式化了两个新的算法（A<:和{A}<:），并在Rocq中证明了类型安全性。这是首个用于生命周期控制的可达性形式系统，避免了流效感性推理的复杂性。

Conclusion: 该工作成功统一了不同资源管理方法的优点，在高阶函数语言中实现了既有灵活的资源共享又有静态生命周期控制的方案，为静态资源管理领域提供了重要推进。

Abstract: Static resource management in higher-order functional languages remains
elusive due to tensions between control, expressiveness, and flexibility.
Region-based systems [Grossman et al. 2002; Tofte et al. 2001] offer control
over lifetimes and expressive in-region sharing, but restrict resources to
lexical scopes. Rust, an instance of ownership types [Clarke et al. 2013],
offers non-lexical lifetimes and robust safety guarantees, yet its global
invariants make common sharing patterns hard to express. Reachability types
[Wei et al. 2024] enable reasoning about sharing and separation, but lack
practical tools for controlling resource lifetimes.
  In this work, we try to unify their strengths. Our solution enables grouping
resources as arenas for arbitrary sharing and static guarantees of lexically
scoped lifetimes. Crucially, arenas and lexical lifetimes are not the only
choice: users may also manage resources individually, with non-lexical
lifetimes. Regardless of mode, resources share the same type, preserving the
higher-order parametric nature of the language.
  Obtaining static safety guarantee in a higher-order language with flexible
sharing is nontrivial. To this end, we propose two new extensions atop
reachability types [Wei et al. 2024]. First, A<: features a novel
two-dimensional store model to enable coarse-grained reachability tracking for
arbitrarily shared resources within arenas. Building on this, {A}<: establishes
lexical lifetime control with static guarantees. As the first reachability
formalism presented for lifetime control, {A}<: avoids the complication of
flow-sensitive reasoning and retains expressive power and simplicity. Both
calculi are formalized and proven type safe in Rocq.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Combining Performance and Productivity: Accelerating the Network Sensing Graph Challenge with GPUs and Commodity Data Science Software](https://arxiv.org/abs/2509.03653)
*Siddharth Samsi,Dan Campbell,Emanuel Scoullos,Oded Green*

Main category: cs.DC

TL;DR: HPEC图挑战赛引入新的匿名网络感知图基准测试，作者使用数据科学方法重新解释GraphBLAS公式，通过RAPIDS生态系统实现147-2185倍加速


<details>
  <summary>Details</summary>
Motivation: 传统基准测试如LINPACK无法充分测试HPC系统的复杂工作负载，新的图挑战需要更全面的软件组件支持

Method: 使用数据科学语言重新解释GraphBLAS公式，利用NVIDIA RAPIDS生态系统中的现成ETL工具（cuDF和cupy）实现

Result: 在NVIDIA GPU上实现显著加速：A100达147-509倍，H100达243-1269倍，H200达332-2185倍，相比CPU上的Pandas

Conclusion: 使用现成企业软件可以实现显著的软件加速，无需编写特定的HPC代码，为图分析工作负载提供了高效的解决方案

Abstract: The HPEC Graph Challenge is a collection of benchmarks representing complex
workloads that test the hardware and software components of HPC systems, which
traditional benchmarks, such as LINPACK, do not. The first benchmark, Subgraph
Isomorphism, focused on several compute-bound and memory-bound kernels. The
most recent of the challenges, the Anonymized Network Sensing Graph Challenge,
represents a shift in direction, as it represents a longer end-to-end workload
that requires many more software components, including, but not limited to,
data I/O, data structures for representing graph data, and a wide range of
functions for data preparation and network analysis. A notable feature of this
new graph challenge is the use of GraphBLAS to represent the computational
aspects of the problem statement. In this paper, we show an alternative
interpretation of the GraphBLAS formulations using the language of data
science. With this formulation, we show that the new graph challenge can be
implemented using off-the-shelf ETL tools available in open-source, enterprise
software such as NVIDIA's RAPIDS ecosystem. Using off-the-shelf software,
RAPIDS cuDF and cupy, we enable significant software acceleration without
requiring any specific HPC code and show speedups, over the same code running
with Pandas on the CPU, of 147x-509x on an NVIDIA A100 GPU, 243x-1269X for an
NVIDIA H100 GPU, and 332X-2185X for an NVIDIA H200 GPU.

</details>


### [3] [Distributed Download from an External Data Source in Asynchronous Faulty Settings](https://arxiv.org/abs/2509.03755)
*John Augustine,Soumyottam Chatterjee,Valerie King,Manish Kumar,Shachar Meir,David Peleg*

Main category: cs.DC

TL;DR: 本文首次将分布式数据获取研究扩展到异步通信网络，提出了在Byzantine和crash故障模型下的查询复杂度最优解决方案，并得到了相应的上下界结果。


<details>
  <summary>Details</summary>
Motivation: 之前的研究主要集中在同步通信网络上，本文首次将分布式数据获取问题扩展到异步通信网络环境，以满足现实系统中的实际需求。

Method: 在异步通信模型下，研究了Byzantine和crash两种故障模型。对于crash故障，提出了查询复杂度最优的确定性协议；对于Byzantine故障，分析了确定性协议的下界并展开到随机协议，进而提出了近最优的随机协议。

Result: 在crash故障模型下，实现了可容忍任意固定比例β<1故障的查询最优确定性解。在Byzantine模型下，证明了当β≥1/2时随机协议的查询复杂度下界为Ω(n)，而当β<1/2时存在查询复杂度近最优的随机协议。

Conclusion: 本文成功将分布式数据获取问题扩展到异步通信网络，并在两种主要故障模型下得到了查询复杂度的最优或近最优解决方案，为异步环境下的数据获取提供了重要的理论基础。

Abstract: The distributedData Retrieval (DR) model consists of $k$ peers connected by a
complete peer-to-peer communication network, and a trusted external data source
that stores an array $\textbf{X}$ of $n$ bits ($n \gg k$). Up to $\beta k$ of
the peers might fail in any execution (for $\beta \in [0, 1)$). Peers can
obtain the information either by inexpensive messages passed among themselves
or through expensive queries to the source array $\textbf{X}$. In the DR model,
we focus on designing protocols that minimize the number of queries performed
by any nonfaulty peer (a measure referred to as query complexity) while
maximizing the resilience parameter $\beta$.
  The Download problem requires each nonfaulty peer to correctly learn the
entire array $\textbf{X}$. Earlier work on this problem focused on synchronous
communication networks and established several deterministic and randomized
upper and lower bounds. Our work is the first to extend the study of
distributed data retrieval to asynchronous communication networks. We address
the Download problem under both the Byzantine and crash failure models. We
present query-optimal deterministic solutions in an asynchronous model that can
tolerate any fixed fraction $\beta<1$ of crash faults. In the Byzantine failure
model, it is known that deterministic protocols incur a query complexity of
$\Omega(n)$ per peer, even under synchrony. We extend this lower bound to
randomized protocols in the asynchronous model for $\beta \geq 1/2$, and
further show that for $\beta < 1/2$, a randomized protocol exists with
near-optimal query complexity. To the best of our knowledge, this is the first
work to address the Download problem in asynchronous communication networks.

</details>


### [4] [Gathering of asynchronous robots on circle with limited visibility using finite communication](https://arxiv.org/abs/2509.04004)
*Avisek Sharma,Satakshi Ghosh,Buddhadeb Sau*

Main category: cs.DC

TL;DR: 本文解决了在连续圆环上具有有限可见度的自主、匿名、同质机器人的聚集问题，提出了在π可见度模型下使用有限通信能力的非刚性移动机器人的聚集算法。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，在π/2可见度下聚集是不可能的，而即使移除一个点的可见度也会使聚集变得困难。现有算法要么需要半同步调度器，要么需要特殊异步调度器。本文旨在在完全异步调度器下解决π可见度模型的聚集问题。

Method: 使用具有有限通信能力（FCOM）的机器人，在π可见度模型下采用非刚性移动方式，在完全异步调度器下运行。机器人初始位置形成旋转不对称配置，并同意顺时针方向。

Result: 提出了一个算法，能够在π可见度模型下成功解决聚集问题，机器人具有有限通信能力且采用非刚性移动。

Conclusion: 该工作证明了在完全异步调度器下，使用有限通信能力和非刚性移动的机器人可以在π可见度模型中实现聚集，这比先前需要更严格条件的方法更具实用性。

Abstract: This work addresses the gathering problem for a set of autonomous, anonymous,
and homogeneous robots with limited visibility operating in a continuous
circle. The robots are initially placed at distinct positions, forming a
rotationally asymmetric configuration. The robots agree on the clockwise
direction. In the $\theta$-visibility model, a robot can only see those robots
on the circle that are at an angular distance $<\theta$ from it. Di Luna
\textit{et. al.} [DISC'20] have shown that, in $\pi/2$ visibility, gathering is
impossible. In addition, they provided an algorithm for robots with $\pi$
visibility, operating under a semi-synchronous scheduler. In the $\pi$
visibility model, only one point, the point at the angular distance $\pi$ is
removed from the visibility. Ghosh \textit{et. al.} [SSS'23] provided a
gathering algorithm for $\pi$ visibility model with robot having finite memory
($\mathcal{FSTA}$), operating under a special asynchronous scheduler.
  If the robots can see all points on the circle, then the gathering can be
done by electing a leader in the weakest robot model under a fully asynchronous
scheduler. However, previous works have shown that even the removal of one
point from the visibility makes gathering difficult. In both works, the robots
had rigid movement. In this work, we propose an algorithm that solves the
gathering problem under the $\pi$-visibility model for robots that have finite
communication ability ($\mathcal{FCOM}$). In this work the robot movement is
non-rigid and the robots work under a fully asynchronous scheduler.

</details>


### [5] [Counterfactual simulations for large scale systems with burnout variables](https://arxiv.org/abs/2509.04038)
*Benjamin Heymann*

Main category: cs.DC

TL;DR: 提出基于不确定性松弛的新算法，用于高效并行计算具有burnout变量系统的反事实估计


<details>
  <summary>Details</summary>
Motivation: 大规模系统中存在burnout变量（激活后不可逆失活的状态变量），传统顺序处理方法计算效率低，难以进行有效的反事实分析

Method: 引入不确定性松弛算法，将顺序处理转换为并行计算，显著提升计算效率

Result: 新算法能够显著提高具有burnout变量系统的反事实估计可扩展性

Conclusion: 不确定性松弛方法为解决具有burnout变量系统的计算瓶颈提供了有效解决方案

Abstract: We consider large-scale systems influenced by burnout variables - state
variables that start active, shape dynamics, and irreversibly deactivate once
certain conditions are met. Simulating what-if scenarios in such systems is
computationally demanding, as alternative trajectories often require sequential
processing, which does not scale very well. This challenge arises in settings
like online advertising, because of campaigns budgets, complicating
counterfactual analysis despite rich data availability. We introduce a new type
of algorithms based on what we refer to as uncertainty relaxation, that enables
efficient parallel computation, significantly improving scalability for
counterfactual estimation in systems with burnout variables.

</details>


### [6] [LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for High-Performance Distributed Training Systems](https://arxiv.org/abs/2509.04084)
*Chenxuan Yao,Yuchong Hu,Feifan Liu,Zhengyu Liu,Dan Feng*

Main category: cs.DC

TL;DR: LowDiff是一个高效的分布式训练检查点框架，通过重用压缩梯度作为差异检查点来降低成本，支持每迭代检查点频率且运行时开销低于3.1%。


<details>
  <summary>Details</summary>
Motivation: 分布式大模型训练经常失败需要检查点恢复，但频繁检查点会产生大量成本。现有差异检查点方法仅限于推荐系统，无法应用于通用分布式训练系统。

Method: 提出LowDiff框架：1）重用压缩梯度作为差异检查点；2）批量梯度写入优化；3）动态调整检查点频率和批处理大小；4）分层梯度重用和快照；5）CPU异步持久化策略。

Result: 在各种工作负载上的实验表明，LowDiff可以实现每迭代检查点频率，运行时开销低于3.1%。

Conclusion: LowDiff通过创新的梯度重用和优化策略，实现了高效的频繁检查点，显著降低了分布式训练的成本和性能开销。

Abstract: Distributed training of large deep-learning models often leads to failures,
so checkpointing is commonly employed for recovery. State-of-the-art studies
focus on frequent checkpointing for fast recovery from failures. However, it
generates numerous checkpoints, incurring substantial costs and thus degrading
training performance. Recently, differential checkpointing has been proposed to
reduce costs, but it is limited to recommendation systems, so its application
to general distributed training systems remains unexplored.
  This paper proposes LowDiff, an efficient frequent checkpointing framework
that \textit{reuses} compressed gradients, serving as differential checkpoints
to reduce cost. Furthermore, LowDiff incorporates a batched gradient write
optimization to persist these differentials to storage efficiently. It also
dynamically tunes both the checkpoint frequency and the batching size to
maximize performance. We further enhance LowDiff with a layer-wise gradient
reusing and snapshotting approach and a CPU-based asynchronous persistence
strategy, enabling frequent checkpointing without gradient compression.
Experiments on various workloads show that LowDiff can achieve checkpointing
frequency up to per iteration with less than 3.1\% runtime overhead.

</details>


### [7] [Trustworthy Second-hand Marketplace for Built Environment](https://arxiv.org/abs/2509.04085)
*Stanly Wilson,Kwabena Adu-Duodu,Yinhao Li,Ringo Sham,Yingli Wang,Ellis Solaiman,Charith Perera,Rajiv Ranjan,Omer Rana*

Main category: cs.DC

TL;DR: 基于区块链和IPFS的数字化市场体系，促进建筑材料可持续重复利用，提高透明度和可追溯性


<details>
  <summary>Details</summary>
Motivation: 建筑行业面临材料浪费和可持续性挑战，需要通过自动化、可追溯性和去中心化决策来实现材料高效重复利用

Method: 开发了一种基于区块链的数字化市场体系，采用InterPlanetary File System (IPFS)确保透明度和可追溯性，构建了市场运营过程框架

Result: 证明了该市场体系能够促进可重复利用材料的高效、可信任交易，展示了其实际应用效果

Conclusion: 该研究为建筑行业提供了一种可持续实践的创新解决方案，在工业自动化和循环供应链方面取得了重要进展

Abstract: The construction industry faces significant challenges regarding material
waste and sustainable practices, necessitating innovative solutions that
integrate automation, traceability, and decentralised decision-making to enable
efficient material reuse. This paper presents a blockchain-enabled digital
marketplace for sustainable construction material reuse, ensuring transparency
and traceability using InterPlanetary File System (IPFS). The proposed
framework enhances trust and accountability in material exchange, addressing
key challenges in industrial automation and circular supply chains. A framework
has been developed to demonstrate the operational processes of the marketplace,
illustrating its practical application and effectiveness. Our contributions
show how the marketplace can facilitate the efficient and trustworthy exchange
of reusable materials, representing a substantial step towards more sustainable
construction practices.

</details>


### [8] [On the impact of unlimited computational power in OBLOT: consequences for synchronous robots on graphs](https://arxiv.org/abs/2509.04383)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 本文证明在同步机器人在有限图上移动时，无限计算能力对OBLOT模型有显著影响，提出了一个适用于广泛问题类别的最优移动和轮次算法


<details>
  <summary>Details</summary>
Motivation: OBLOT模型中机器人能力有限（匿名、无方向感、无记忆、无声），算法设计具有挑战性。现有研究主要关注移动次数和轮次成本，而忽略了计算能力的影响

Method: 利用同步机器人在有限图上的无限计算能力，开发了一个确定性解决方案算法

Result: 该算法能够应用于广泛的问题类别，同时保证最小的移动次数和轮次

Conclusion: 在OBLOT模型中，无限计算能力对同步机器人在有限图上的性能有重要影响，可以实现最优的移动和轮次效率

Abstract: The OBLOT model has been extensively studied in theoretical swarm robotics.
It assumes weak capabilities for the involved mobile robots, such as they are
anonymous, disoriented, no memory of past events (oblivious), and silent. Their
only means of (implicit) communication is transferred to their positioning,
i.e., stigmergic information. These limited capabilities make the design of
distributed algorithms a challenging task. Over the last two decades, numerous
research papers have addressed the question of which tasks can be accomplished
within this model. Nevertheless, as it usually happens in distributed
computing, also in OBLOT the computational power available to the robots is
neglected as the main cost measures for the designed algorithms refer to the
number of movements or the number of rounds required. In this paper, we prove
that for synchronous robots moving on finite graphs, the unlimited
computational power (other than finite time) has a significant impact. In fact,
by exploiting it, we provide a definitive resolution algorithm that applies to
a wide class of problems while guaranteeing the minimum number of moves and
rounds.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [Hardware-Aware Data and Instruction Mapping for AI Tasks: Balancing Parallelism, I/O and Memory Tradeoffs](https://arxiv.org/abs/2509.03846)
*Md Rownak Hossain Chowdhury,Mostafizur Rahman*

Main category: cs.AR

TL;DR: 通过预先规划计算和通信，构建统一的指令数据流，减少主机干领和片外内存使用，实现高效深度学习推理


<details>
  <summary>Details</summary>
Motivation: 解决深度学习推理中常见的I/O瓶颈、片外内存访问和主机干领问题，通过预测程序行为来优化计算和通信

Method: 使用统一指令数据流框架，基于粒度消息传递和编程计算架构，采用静态权重重用、数组内多播和阶段性约简等技术

Result: 在VGG-19上实现88-92%利用率，97%消息内部生成，89%时间用于片内传输，计算吞吐量超1 TFLOP/s，重用和局部聚合减少达100MB/层

Conclusion: 流式计算方式高效可行，通过紧密协调数据和指令流可在硬件上实现这种执行模式

Abstract: We introduce a mapping framework for deep learning inference that takes
advantage of predictable neural network behavior to plan both computation and
communication ahead of time. The framework generates a unified stream of
instructions and data, enabling the hardware to execute operations and route
information on its own, without frequent involvement from the host and with
minimal off-chip memory use. This naturally reduces reliance on I/O, off-chip
memory, and host control. By leveraging fine-grained message passing on a
programmable, message-based compute architecture, the framework keeps data
movement local and coordinates computation across the array using techniques
such as stationary-weight reuse, in-array multicasting, and staged reductions.
Applied to VGG-19, the framework sustains high utilization (88 to 92 percent),
with over 97 percent of messages generated internally and nearly 89 percent of
time consumed on-chip transfers. Computation throughput scales beyond 1 TFLOP/s
on larger arrays, while traffic reductions from reuse and local aggregation
reach up to 100 MB per layer. Overall, the results highlight the effectiveness
of streaming-based computation and show how our mapper enables this execution
style by tightly coordinating data and instruction flow across the hardware.

</details>


### [10] [Real Time FPGA Based CNNs for Detection, Classification, and Tracking in Autonomous Systems: State of the Art Designs and Optimizations](https://arxiv.org/abs/2509.04153)
*Safa Mohammed Sali,Mahmoud Meribout,Ashiyana Abdul Majeed*

Main category: cs.AR

TL;DR: 本文综述了FPGA上CNN目标检测、分类和跟踪的最新进展，重点讨论了FPGA在实时计算机视觉应用中的优势、硬件加速技术、优化策略以及软硬件协同设计方法。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶、机器人和监控等领域对实时计算机视觉应用需求的增长，FPGA因其可重构性、低功耗和确定性延迟等优势，成为GPU和ASIC的有力替代方案。

Method: 通过批判性审查最先进的FPGA实现，涵盖算法创新、硬件加速技术、剪枝、量化和稀疏感知等优化策略，以及现代FPGA平台和软件开发工具的分析。

Result: 提供了对FPGA部署CNN的全面技术洞察，包括混合架构、软硬件协同设计实践、数据流优化和流水线处理技术，为开发下一代高效能视觉系统奠定基础。

Conclusion: 本研究为研究人员和工程师提供了开发面向边缘和嵌入式应用的、功耗高效且高性能的FPGA优化视觉系统的关键见解和方法论。

Abstract: This paper presents a comprehensive review of recent advances in deploying
convolutional neural networks (CNNs) for object detection, classification, and
tracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand
for real-time computer vision applications in domains such as autonomous
vehicles, robotics, and surveillance, FPGAs have emerged as a powerful
alternative to GPUs and ASICs due to their reconfigurability, low power
consumption, and deterministic latency. We critically examine state-of-the-art
FPGA implementations of CNN-based vision tasks, covering algorithmic
innovations, hardware acceleration techniques, and the integration of
optimization strategies like pruning, quantization, and sparsity-aware methods
to maximize performance within hardware constraints. This survey also explores
the landscape of modern FPGA platforms, including classical LUT-DSP based
architectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration
Platforms (ACAPs), comparing their capabilities in handling deep learning
workloads. Furthermore, we review available software development tools such as
Vitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the
design and deployment of AI models on FPGAs. The paper uniquely discusses
hybrid architecture that combine GPUs and FPGAs for collaborative acceleration
of AI inference, addressing challenges related to energy efficiency and
throughput. Additionally, we highlight hardware-software co-design practices,
dataflow optimizations, and pipelined processing techniques essential for
real-time inference on resource-constrained devices. Through this survey,
researchers and engineers are equipped with insights to develop
next-generation, power-efficient, and high-performance vision systems optimized
for FPGA deployment in edge and embedded applications.

</details>


### [11] [Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs and Optimizations](https://arxiv.org/abs/2509.04162)
*Safa Mohammed Sali,Mahmoud Meribout,Ashiyana Abdul Majeed*

Main category: cs.AR

TL;DR: 本文全面综述了基于FPGA的Transformer和视觉语言模型推理的设计权衡、优化策略和实现挑战，旨在解决这些模型在延迟和功耗受限环境中的部署问题。


<details>
  <summary>Details</summary>
Motivation: Transformer和视觉语言模型虽然性能优异，但计算复杂度高、内存占用大、数据访问模式不规则，在延迟和功耗受限环境中部署面临重大挑战。FPGA因其可重构性、细粒度并行性和能效优势成为理想的硬件平台。

Method: 通过综合分析设计权衡、优化策略和实现挑战，包括设备类别选择、内存子系统约束、数据流编排、量化策略、稀疏性利用、工具链选择，以及视觉语言模型特有的异构计算平衡和交叉注意力内存管理等模态特定问题。

Result: 提出了硬件算法协同设计的新兴趋势，包括注意力机制创新、压缩技术和模块化覆盖层，以提高效率和适应性。同时考虑了运行时灵活性、验证开销和缺乏标准化FPGA多模态基准等实际问题。

Conclusion: 为先进多模态AI模型与高效FPGA部署之间的鸿沟搭建技术基础，并展望了可扩展、可移植和可重构FPGA解决方案的未来发展方向，以适配不断演进的模型架构并保持高利用率和可预测性能。

Abstract: Transformers and vision-language models (VLMs) have emerged as dominant
architectures in computer vision and multimodal AI, offering state-of-the-art
performance in tasks such as image classification, object detection, visual
question answering, and caption generation. However, their high computational
complexity, large memory footprints, and irregular data access patterns present
significant challenges for deployment in latency- and power-constrained
environments. Field-programmable gate arrays (FPGAs) provide an attractive
hardware platform for such workloads due to their reconfigurability,
fine-grained parallelism, and potential for energy-efficient acceleration. This
paper presents a comprehensive review of design trade-offs, optimization
strategies, and implementation challenges for FPGA-based inference of
transformers and VLMs. We examine critical factors such as device-class
selection, memory subsystem constraints, dataflow orchestration, quantization
strategies, sparsity exploitation, and toolchain choices, alongside
modality-specific issues unique to VLMs, including heterogeneous compute
balancing and cross-attention memory management. Additionally, we discuss
emerging trends in hardware-algorithm co-design, highlighting innovations in
attention mechanisms, compression, and modular overlays to improve efficiency
and adaptability. Practical issues such as runtime flexibility, verification
overhead, and the absence of standardized FPGA multimodal benchmarks are also
considered. Finally, we outline future directions toward scalable, portable,
and reconfigurable FPGA solutions that adapt to evolving model architectures
while sustaining high utilization and predictable performance. This synthesis
offers both a technical foundation and a forward-looking perspective to help
bridge the gap between advanced multimodal AI models and efficient FPGA
deployment.

</details>


### [12] [Real-time Object Detection and Associated Hardware Accelerators Targeting Autonomous Vehicles: A Review](https://arxiv.org/abs/2509.04173)
*Safa Sali,Anis Meribout,Ashiyana Majeed,Mahmoud Meribout,Juan Pablo,Varun Tiwari,Asma Baobaid*

Main category: cs.AR

TL;DR: 这篇评论文章系统调研了自主驾驶汽车中的实时物体检测算法和硬件加速器，分析了从非神经网络算法向CNN模型的转变，讨论了商业系统与学术研究之间的差距，以便为未来全自动驾驶汽车的设计提供参考。


<details>
  <summary>Details</summary>
Motivation: 自主驾驶汽车对实时物体检测的急需急需高效率和实时性，而目前商业系统与学术研究之间存在信息隔离，需要统一调研以提供全面参考。

Method: 通过系统性的文献调研方法，综述实时物体检测算法的发展轮应，分析CNN模型在边缘硬件上的部署优势，对比GPU和ASIC等加速器的性能表现。

Result: 识别出当前实时检测算法已能达到每秒百幅的处理速度，但对于处理多摄像头数据仍需更多硬件和算法改进；同时指出商业保密性导致研究数据限制的挑战。

Conclusion: 本文填补了以往调研在商业AV技术与学术研究之间的空白，为研究人员设计未来全自动驾驶汽车提供了实用的技术参考案例。

Abstract: The efficiency of object detectors depends on factors like detection
accuracy, processing time, and computational resources. Processing time is
crucial for real-time applications, particularly for autonomous vehicles (AVs),
where instantaneous responses are vital for safety. This review paper provides
a concise yet comprehensive survey of real-time object detection (OD)
algorithms for autonomous cars delving into their hardware accelerators (HAs).
Non-neural network-based algorithms, which use statistical image processing,
have been entirely substituted by AI algorithms, such as different models of
convolutional neural networks (CNNs). Their intrinsically parallel features led
them to be deployable into edge-based HAs of various types, where GPUs and, to
a lesser extent, ASIC (application-specific integrated circuit) remain the most
widely used. Throughputs of hundreds of frames/s (fps) could be reached;
however, handling object detection for all the cameras available in a typical
AV requires further hardware and algorithmic improvements. The intensive
competition between AV providers has limited the disclosure of algorithms,
firmware, and even hardware platform details. This remains a hurdle for
researchers, as commercial systems provide valuable insights while academics
undergo lengthy training and testing on restricted datasets and road scenarios.
Consequently, many AV research papers may not be reflected in end products,
being developed under limited conditions. This paper surveys state-of-the-art
OD algorithms and aims to bridge the gap with technologies in commercial AVs.
To our knowledge, this aspect has not been addressed in earlier surveys. Hence,
the paper serves as a tangible reference for researchers designing future
generations of vehicles, expected to be fully autonomous for comfort and
safety.

</details>
