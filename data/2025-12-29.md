<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Quantitative Verification of Omega-regular Properties in Probabilistic Programming](https://arxiv.org/abs/2512.21596)
*Peixin Wang,Jianhao Bai,Min Zhang,C. -H. Luke Ong*

Main category: cs.PL

TL;DR: 提出TPI框架，将概率编程与时态逻辑统一，计算满足ω-正则规范的执行轨迹的后验分布，并提供严格的概率上下界保证


<details>
  <summary>Details</summary>
Motivation: 现有概率编程推理技术通常只计算程序终止时的后验分布，无法捕捉概率行为的时序演化，需要支持时态规范和观测的推理框架

Method: 开发TPI框架，将Rabin接受条件分解为持久性和循环性组件，构建随机屏障证书来严格界定每个组件的概率上下界

Result: 实现原型工具TPInfer，在多个基准测试中展示了对概率模型中丰富时态属性进行有效高效推理的能力

Conclusion: TPI框架成功统一了概率编程与时态逻辑，为具有时态规范和观测的概率模型提供了严格的定量推理方法

Abstract: Probabilistic programming provides a high-level framework for specifying statistical models as executable programs with built-in randomness and conditioning. Existing inference techniques, however, typically compute posterior distributions over program states at fixed time points, most often at termination, thereby failing to capture the temporal evolution of probabilistic behaviors. We introduce temporal posterior inference (TPI), a new framework that unifies probabilistic programming with temporal logic by computing posterior distributions over execution traces that satisfy omega-regular specifications, conditioned on possibly temporal observations. To obtain rigorous quantitative guarantees, we develop a new method for computing upper and lower bounds on the satisfaction probabilities of omega-regular properties. Our approach decomposes Rabin acceptance conditions into persistence and recurrence components and constructs stochastic barrier certificates that soundly bound each component. We implement our approach in a prototype tool, TPInfer, and evaluate it on a suite of benchmarks, demonstrating effective and efficient inference over rich temporal properties in probabilistic models.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum](https://arxiv.org/abs/2512.21340)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Nikolaos Tsironis,Souvik Sengupta,Kostas Ramantas,Jhofre Ojeda*

Main category: cs.DC

TL;DR: 智慧城市采用数据驱动架构提升城市服务效率、可持续性和韧性


<details>
  <summary>Details</summary>
Motivation: 智慧城市需要更高效、可持续和韧性的城市服务，传统方法难以满足现代城市复杂需求

Method: 采用数据中心的架构设计，整合城市各类数据资源

Result: 数据驱动架构能够显著提升城市服务的运行效率、环境可持续性和系统韧性

Conclusion: 数据中心的架构是智慧城市发展的关键方向，能够有效应对城市复杂挑战

Abstract: Smart cities are increasingly adopting data-centric architectures to enhance the efficiency, sustainability, and resilience of urban services.

</details>


### [3] [Demystifying ARM SME to Optimize General Matrix Multiplications](https://arxiv.org/abs/2512.21473)
*Chencheng Deng,Weiling Yang,Jianbin Fang,Dezun Dong*

Main category: cs.DC

TL;DR: MpGEMM是一个针对ARM SME架构优化的开源GEMM库，在Apple M4 Pro上比Apple Accelerate库平均加速1.23倍


<details>
  <summary>Details</summary>
Motivation: 现代架构如ARM SME引入了专门的矩阵运算硬件，但现有线性代数库未能充分利用其潜力，特别是对于大型矩阵。需要开发能够充分利用SME架构特性的GEMM实现。

Method: 通过系统性的SME架构特性分析，制定优化指导原则。采用缓存感知的分区策略、高效的数据打包与即时转置、以及利用多向量加载和所有可用tile寄存器的专用微内核。

Result: 在Apple M4 Pro上使用DeepSeek和LLaMA的真实工作负载进行评估，MpGEMM相比供应商优化的Apple Accelerate库平均获得1.23倍加速，显著优于其他开源替代方案。

Conclusion: MpGEMM成功展示了如何通过充分利用ARM SME架构特性来优化GEMM性能，为高性能计算和深度学习中的矩阵乘法提供了高效的实现方案。

Abstract: General Matrix Multiplication (GEMM) is a critical kernel in high-performance computing and deep learning. While modern architectures like ARM's Scalable Matrix Extension (SME) introduce dedicated hardware for matrix operations, existing linear algebra libraries fail to fully exploit its potential, particularly for large matrices. This paper presents MpGEMM, an open-source library that leverages key architectural features of SME to optimize GEMM across multiple precisions. Through a systematic characterization of SME, we derive optimization guidelines that inform our design. MpGEMM employs cache-aware partitioning, efficient data packing with on-the-fly transposition, and specialized micro-kernels that utilize multi-vector loads and all available tile registers. Evaluated on an Apple M4 Pro with real-world workloads from DeepSeek and LLaMA, MpGEMM achieves an average speedup of 1.23x over the vendor-optimized Apple Accelerate library and significantly outperforms other open-source alternatives.

</details>


### [4] [Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism](https://arxiv.org/abs/2512.21487)
*Xinglin Pan,Shaohuai Shi,Wenxiang Lin,Yuxin Wang,Zhenheng Tang,Wei Wang,Xiaowen Chu*

Main category: cs.DC

TL;DR: FinDEP：针对混合专家模型推理的细粒度任务调度算法，通过优化计算/通信任务划分和调度，提升推理吞吐量


<details>
  <summary>Details</summary>
Motivation: 混合专家架构虽然能以亚线性计算增长扩展模型规模，但推理时因KV缓存和稀疏专家激活导致内存密集。现有的解耦专家并行方法缺乏对共享专家的支持和高效的任务调度，限制了性能。

Method: 提出FinDEP算法：1）将计算/通信划分为更小任务实现细粒度流水线；2）制定支持可变粒度和顺序的调度优化问题；3）开发针对大搜索空间的高效求解器。

Result: 在四个GPU系统上使用DeepSeek-V2和Qwen3-MoE测试，FinDEP相比先前方法提升吞吐量最高达1.61倍，在32-GPU系统上实现最高1.24倍加速。

Conclusion: FinDEP通过细粒度任务调度有效解决了混合专家模型推理中的内存瓶颈和调度效率问题，显著提升了推理性能。

Abstract: The mixture-of-experts (MoE) architecture scales model size with sublinear computational increase but suffers from memory-intensive inference due to KV caches and sparse expert activation. Recent disaggregated expert parallelism (DEP) distributes attention and experts to dedicated GPU groups but lacks support for shared experts and efficient task scheduling, limiting performance.
  We propose FinDEP, a fine-grained task scheduling algorithm for DEP that maximizes task overlap to improve MoE inference throughput. FinDEP introduces three innovations: 1) partitioning computation/communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization supporting variable granularity and ordering, and 3) developing an efficient solver for this large search space.
  Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system.

</details>


### [5] [nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures](https://arxiv.org/abs/2512.21571)
*Hui Guo,Qihang Zheng,Chenghai Huo,Dongliang Guo,Haoqi Yang,Yang Zhang*

Main category: cs.DC

TL;DR: nncase是一个开源的端到端编译框架，通过基于e-graph的项重写引擎解决内存架构异构性问题，统一优化跨不同目标平台，在Qwen3系列模型上超越主流框架，CPU性能接近手工优化的llama.cpp。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的高效部署受到内存架构异构性的阻碍，传统编译器存在工作流程碎片化和高适配成本的问题，需要统一的编译框架来优化跨平台部署。

Method: 基于e-graph的项重写引擎解决阶段排序问题，实现计算和数据移动策略的全局探索；包含三个核心模块：Auto Vectorize适配异构计算单元，Auto Distribution搜索并行策略并进行成本感知通信优化，Auto Schedule最大化片上缓存局部性；最后通过缓冲区感知的Codegen阶段实现高效内核实例化。

Result: nncase在Qwen3系列模型上超越了MLC LLM和Intel IPEX等主流框架，在CPU上达到了与手工优化的llama.cpp相当的性能，证明了自动化编译在高性能LLM部署中的可行性。

Conclusion: nncase通过统一的端到端编译框架有效解决了LLM部署中的内存架构异构性问题，展示了自动化编译在实现高性能LLM部署方面的潜力。

Abstract: The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase.

</details>


### [6] [Embedding Samples Dispatching for Recommendation Model Training in Edge Environments](https://arxiv.org/abs/2512.21615)
*Guopeng Li,Haisheng Tan,Chi Zhang,Hongqiu Ni,Zilong Wang,Xinyue Zhang,Yang Xu,Han Tian*

Main category: cs.DC

TL;DR: ESD机制通过优化嵌入样本分配到边缘工作节点，显著减少嵌入传输成本，加速DLRM训练


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上训练DLRM有隐私保护、低延迟和个性化等优势，但嵌入表巨大，传统框架使用参数服务器导致嵌入传输成本过高，成为训练瓶颈

Method: 提出ESD机制，基于预期嵌入传输成本优化输入嵌入样本分配到边缘工作节点；开发HybridDis分配决策方法，结合资源密集型最优算法和启发式算法平衡决策质量和资源消耗

Result: 实验显示ESD减少嵌入传输成本最高达36.76%，端到端DLRM训练加速最高达1.74倍

Conclusion: ESD能有效解决边缘DLRM训练中的嵌入传输瓶颈，显著提升训练效率

Abstract: Training deep learning recommendation models (DLRMs) on edge workers brings several benefits, particularly in terms of data privacy protection, low latency and personalization. However, due to the huge size of embedding tables, typical DLRM training frameworks adopt one or more parameter servers to maintain global embedding tables, while leveraging the edge workers cache part of them. This incurs significant transmission cost for embedding transmissions between workers and parameter servers, which can dominate the training cycle. In this paper, we investigate how to dispatch input embedding samples to appropriate edge workers to minimize the total embedding transmission cost when facing edge-specific challenges such as heterogeneous networks and limited resources. We develop ESD, a novel mechanism that optimizes the dispatch of input embedding samples to edge workers based on expected embedding transmission cost. We propose HybridDis as the dispatch decision method within ESD, which combines a resource-intensive optimal algorithm and a heuristic algorithm to balance decision quality and resource consumption. We implement a prototype of ESD and compare it with state-of-the-art mechanisms on real-world workloads. Extensive experimental results show that ESD reduces the embedding transmission cost by up to 36.76% and achieves up to 1.74 times speedup in end-to-end DLRM training.

</details>


### [7] [Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference](https://arxiv.org/abs/2512.21730)
*Linyi Jiang,Yifei Zhu,Hao Yin,Bo Li*

Main category: cs.DC

TL;DR: Hyperion是一个云-端协同框架，用于在动态网络上对超高清视频进行低延迟的视觉Transformer推理，通过识别关键区域、动态调度和加权融合来提升处理速度和准确率。


<details>
  <summary>Details</summary>
Motivation: 阵列相机技术能够实时捕捉超高清视频，但使用现有的Transformer视觉基础模型处理这些数据面临计算开销大（设备端）或传输开销大（云端）的问题，需要一种低延迟的解决方案。

Method: Hyperion框架包含三个核心组件：1) 协作感知的重要性评分器，在patch级别识别关键区域；2) 动态调度器，根据网络条件自适应调整patch传输质量以平衡延迟和准确率；3) 加权集成器，融合边缘和云端结果以提高准确率。

Result: 实验结果表明，在各种网络环境下，Hyperion相比现有最优基线方法，帧处理率最高提升1.61倍，准确率最高提升20.2%。

Conclusion: Hyperion是首个支持在动态网络上使用现成视觉Transformer对超高清视觉数据进行低延迟推理的云-端协同框架，有效解决了计算和传输瓶颈问题。

Abstract: Recent advancements in array-camera videography enable real-time capturing of ultra-high-definition (Ultra-HD) videos, providing rich visual information in a large field of view. However, promptly processing such data using state-of-the-art transformer-based vision foundation models faces significant computational overhead in on-device computing or transmission overhead in cloud computing. In this paper, we present Hyperion, the first cloud-device collaborative framework that enables low-latency inference on Ultra-HD vision data using off-the-shelf vision transformers over dynamic networks. Hyperion addresses the computational and transmission bottleneck of Ultra-HD vision transformers by exploiting the intrinsic property in vision Transformer models. Specifically, Hyperion integrates a collaboration-aware importance scorer that identifies critical regions at the patch level, a dynamic scheduler that adaptively adjusts patch transmission quality to balance latency and accuracy under dynamic network conditions, and a weighted ensembler that fuses edge and cloud results to improve accuracy. Experimental results demonstrate that Hyperion enhances frame processing rate by up to 1.61 times and improves the accuracy by up to 20.2% when compared with state-of-the-art baselines under various network environments.

</details>


### [8] [LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices](https://arxiv.org/abs/2512.21835)
*Mingyu Sun,Xiao Zhang,Shen Qu,Yan Li,Mengbai Xiao,Yuan Yuan,Dongxiao Yu*

Main category: cs.DC

TL;DR: LIME是一个协作系统，可在有限网络带宽下，在多个内存受限的边缘设备上实现大模型的无损推理，通过交错流水线并行和模型卸载来平衡计算与通信。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在边缘设备上部署面临巨大挑战：参数规模大、资源需求高，而边缘设备计算能力和内存容量有限，网络带宽瓶颈进一步限制了分布式部署和实时响应。现有轻量化优化技术往往导致模型精度显著下降。

Method: LIME采用交错流水线并行结合模型卸载，动态平衡计算与通信。引入细粒度离线分配调度器和在线内存适配策略，优化设备计算和存储资源，最小化推理延迟。

Result: 在四个异构Nvidia Jetson边缘设备上部署LLaMA3.3-70B-Instruct模型推理，相比最先进基线，在零星和突发请求模式下分别实现1.7倍和3.7倍加速，且不损失模型精度。

Conclusion: LIME系统有效解决了大模型在边缘设备上的高效无损推理问题，通过创新的协作架构和资源优化策略，在保持模型精度的同时显著提升推理性能。

Abstract: Large language models (LLMs) have emerged as a powerful foundation for intelligent reasoning and decision-making, demonstrating substantial impact across a wide range of domains and applications. However, their massive parameter scales and substantial resource demands pose critical challenges for efficient inference on edge devices. These devices are inherently constrained by limited computational power and memory capacity, while bandwidth bottlenecks at the network edge further restrict distributed deployment and real-time responsiveness. Although existing research has explored lightweight optimization techniques to mitigate memory limitations, such approaches often incur significant degradation in model accuracy and performance. To address these challenges, we propose LIME, a collaborative system that enables lossless inference for large models across multiple memory-constrained edge devices under limited network bandwidth. LIME employs an interleaved pipeline parallelism in conjunction with model offloading to dynamically balance computation and communication. Furthermore, a fine-grained offline allocation scheduler and online memory adaptation strategy are introduced to enhance the device's computing and storage resources while minimizing inference latency. Extensive experiments demonstrate that LIME, deployed on four heterogeneous Nvidia Jetson edge devices for LLaMA3.3-70B-Instruct model inference, achieves 1.7$\times$ and 3.7$\times$ speedups over state-of-the-art baselines under sporadic and bursty request patterns respectively, without compromising model accuracy.

</details>


### [9] [Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models](https://arxiv.org/abs/2512.21884)
*Tingyang Sun,Ting He,Bo Ji,Parimal Parag*

Main category: cs.DC

TL;DR: 本文系统研究了分布式LLM推理中的资源分配问题，提出了性能预测模型、优化算法和在线适应方案，显著提升了推理效率。


<details>
  <summary>Details</summary>
Motivation: 虽然PETALS等分布式系统降低了LLM部署门槛，但其性能严重依赖资源分配策略，而如何最优分配仍未知。现有方案无法充分利用分布式资源，导致推理效率低下。

Method: 1) 建立实验验证的性能预测模型；2) 将块放置和请求路由离线优化建模为混合整数线性规划问题，证明其NP难并设计多项式复杂度算法；3) 为在线场景设计具有相同性能保证的适应算法；4) 开发轻量级CPU模拟器。

Result: 通过实验和模拟验证，相比现有最优方案，所提方法在不同地理分布服务器设置下能显著减少推理时间。CPU模拟器能准确预测GPU服务器性能，为资源有限的研究者提供研究工具。

Conclusion: 本文首次系统研究了分布式LLM推理的资源分配问题，提出了理论保证的优化方案和实用工具，为高效分布式LLM部署提供了重要基础。

Abstract: Large language models have demonstrated extraordinary performance in many AI tasks but are expensive to use, even after training, due to their requirement of high-end GPUs. Recently, a distributed system called PETALS was developed to lower the barrier for deploying LLMs by splitting the model blocks across multiple servers with low-end GPUs distributed over the Internet, which was much faster than swapping the model parameters between the GPU memory and other cheaper but slower local storage media. However, the performance of such a distributed system critically depends on the resource allocation, and how to do so optimally remains unknown. In this work, we present the first systematic study of the resource allocation problem in distributed LLM inference, with focus on two important decisions: block placement and request routing. Our main results include: experimentally validated performance models that can predict the inference performance under given block placement and request routing decisions, a formulation of the offline optimization of block placement and request routing as a mixed integer linear programming problem together with the NP-hardness proof and a polynomial-complexity algorithm with guaranteed performance, and an adaptation of the offline algorithm for the online setting with the same performance guarantee under bounded load. Through both experiments and experimentally-validated simulations, we have verified that the proposed solution can substantially reduce the inference time compared to the state-of-the-art solution in diverse settings with geographically-distributed servers. As a byproduct, we have also developed a light-weighted CPU-only simulator capable of predicting the performance of distributed LLM inference on GPU servers, which can evaluate large deployments and facilitate future research for researchers with limited GPU access.

</details>


### [10] [BLEST: Blazingly Efficient BFS using Tensor Cores](https://arxiv.org/abs/2512.21967)
*Deniz Elbek,Kamer Kaya*

Main category: cs.DC

TL;DR: BLEST是一个利用GPU Tensor Core加速BFS的框架，通过位图导向结构、负载均衡策略和图重排序技术，在多种真实图上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代GPU的Tensor Core虽然提供极高吞吐量，但主要针对密集运算，难以有效应用于不规则、非结构化的图计算（如BFS）。需要解决负载不均衡、冗余计算和同步开销等问题。

Method: 1. 采用基于位图的结构和执行布局重构pull-based BFS流水线；2. 引入二值化虚拟切片集(BVSS)实现warp级负载均衡；3. 应用两种图重排序策略：社交类图用压缩导向排序，非社交类图用带宽减少排序；4. 开发批量SpMSpV乘法模式，利用位运算TC瓦片处理点积；5. 结合内核融合和惰性顶点更新方案减少同步开销。

Result: 在多种真实图上，BLEST相比BerryBees、Gunrock和GSWITCH分别实现了平均3.58倍、4.64倍和4.9倍的加速。

Conclusion: BLEST成功将GPU Tensor Core有效应用于BFS计算，通过创新的负载均衡、图重排序和计算模式优化，显著提升了不规则图计算的性能。

Abstract: Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit them for irregular, unstructured graph computations. In particular, fully utilising them for a BFS requires an efficient mapping of the edge operations onto TCs while avoiding redundancy, load imbalance, and synchronisation. We present BLEST, a TC-accelerated framework that reformulates the pull-based BFS pipeline around a bitmap-oriented structure and a carefully engineered execution layout. BLEST introduces Binarised Virtual Slice Sets (BVSS) to enforce warp-level load balancing and to eliminate frontier-oblivious work assignment. To improve both memory efficiency and update locality across diverse graphs, we apply two complementary graph reordering strategies: a compression-oriented ordering for social-like graphs and a bandwidth-reducing ordering for non-social graphs. At the compute level, we develop a batched SpMSpV multiplication pattern that uses the bitwise TC tiles to handle dot products without wasting output entries, thereby reducing the number of required MMA calls. Finally, BLEST combines kernel fusion with a lazy vertex update scheme to reduce host-side synchronisation, mitigate atomic overheads, and improve cache locality. Experiments show that BLEST delivers, on average, $3.58\times$, $4.64\times$ and $4.9\times$ speedup over BerryBees, Gunrock, and GSWITCH, respectively, across a broad set of real-world graphs.

</details>


### [11] [Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View](https://arxiv.org/abs/2512.22035)
*Yanmeng Wang,Zhiwen Dai,Shuai Wang,Jian Zhou,Fu Xiao,Tony Q. S. Quek,Tsung-Hui Chang*

Main category: cs.DC

TL;DR: FedAuto：一种无需先验网络知识的自适应聚合联邦微调框架，有效应对连接失败和数据异构问题


<details>
  <summary>Details</summary>
Motivation: 联邦微调(FFT)结合服务器和客户端数据提升模型泛化能力并保护隐私，但现有方法假设网络条件同质或需要连接失败先验知识，不适用于现实世界中多种通信标准和异构故障模式的网络环境。

Method: 提出FedAuto框架，通过自适应聚合缓解连接失败和数据异构的联合影响。该框架无需网络条件先验知识或基础设施修改，支持即插即用部署。采用新颖的每轮聚合视角进行分析，无需连接失败概率或客户端选择策略假设。

Result: FedAuto在各种连接失败场景下（包括全参数和部分参数微调如LoRA）始终优于最先进的基线方法，甚至超过依赖复杂通信资源优化的策略。建立了严格的收敛保证，为每个单独实现提供收敛性。

Conclusion: FedAuto是首个无需先验网络知识即可应对连接失败和数据异构问题的联邦微调框架，具有强大的理论保证和实际性能优势，适用于现实世界的异构网络环境。

Abstract: Federated Fine-Tuning (FFT) has attracted growing interest as it leverages both server- and client-side data to enhance global model generalization while preserving privacy, and significantly reduces the computational burden on edge devices by avoiding training from scratch. Despite these advantages, FFT performance is often degraded by unreliable server-client connections and heterogeneous client data distributions. Most existing methods assume homogeneous network conditions or require prior knowledge of connection failures. However, these assumptions are impractical in real-world networks characterized by diverse communication standards (e.g., wired, Wi-Fi, 4G, and 5G) and heterogeneous failure patterns. To address these limitations, we propose FedAuto, a novel FFT framework that mitigates the combined effects of connection failures and data heterogeneity via adaptive aggregation. FedAuto operates without prior knowledge of network conditions or modifications to existing infrastructure, enabling seamless plug-and-play deployment. Moreover, we establish a rigorous convergence guarantee for FedAuto. By adopting a novel per-round aggregation perspective, our analysis removes the need for assumptions on connection failures probabilities or client selection strategies commonly imposed in prior work, and guarantees convergence of FedAuto for each individual realization, providing a stronger theoretical assurance. Extensive experiments demonstrate that FedAuto consistently outperforms state-of-the-art baselines under diverse connection failure scenarios for both full-parameter and partial-parameter fine-tuning (e.g., LoRA), and even surpasses strategies that rely on complex communication resource optimization.

</details>


### [12] [FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion](https://arxiv.org/abs/2512.22036)
*Zhuoran Zhu,Chunyang Zhu,Hao Lin,Xu Fu,Yiming Zhou,Quanlu Zhang,Zhenhua Li,Feng Qian,Chao Yu,Boxun Li,Guohao Dai,Yu Wang*

Main category: cs.DC

TL;DR: FUSCO是一个专为MoE模型设计的通信库，通过融合数据转换和通信优化专家并行中的数据重排，相比现有方案显著提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有通信库在处理MoE模型的专家并行数据重排时效率低下，其开销可能占端到端运行时间的一半以上。MoE的专家主数据布局与通信操作期望的设备主布局存在冲突，导致性能瓶颈。

Method: FUSCO通过捕获细粒度数据布局，由流水线通信引擎在通信路径上高效执行数据重排。结合轻量级规划和负载均衡机制，消除冗余通信并分散流量。

Result: 在代表性基准测试中，FUSCO相比NCCL和DeepEP分别实现最高3.84倍和2.01倍加速。在端到端MoE任务中，训练延迟降低1.17-1.39倍和1.10-1.19倍，推理首令牌生成延迟降低1.09-1.25倍和1.06-1.16倍。

Conclusion: FUSCO通过专门针对MoE数据布局特点设计的通信优化，有效解决了专家并行中的数据重排瓶颈，显著提升了大规模MoE模型的训练和推理效率。

Abstract: Large-scale Mixture-of-Experts (MoE) models rely on \emph{expert parallelism} for efficient training and inference, which splits experts across devices and necessitates distributed data shuffling to route each token to its assigned experts. However, existing communication libraries handle this shuffling poorly; its overhead can account for over half of end-to-end runtime. We present FUSCO, an MoE-friendly communication library that achieves efficient and lightweight data shuffling through fused data transformation and communication, based on the key observation that MoE's expert-major data layout conflicts with the device-major layout expected by communication operations. FUSCO captures the fine-grained data layout, which is then interpreted by a pipelined communication engine that performs the required shuffling efficiently along the communication path. Lightweight planning and load-balancing mechanisms complement the engine by eliminating redundant communication and dispersing traffic. Evaluations on representative benchmarks illustrate that FUSCO achieves up to 3.84$\times$ and 2.01$\times$ speedups over NCCL and DeepEP (the state-of-the-art MoE communication library), respectively. In end-to-end MoE tasks, compared to NCCL and DeepEP, FUSCO reduces the training latency by 1.17-1.39$\times$ and 1.10-1.19$\times$, and lowers the first-token generation latency in inference by 1.09-1.25$\times$ and 1.06-1.16$\times$.

</details>


### [13] [Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications](https://arxiv.org/abs/2512.22113)
*Shengkun Cui,Rahul Krishna,Saurabh Jha,Ravishankar K. Iyer*

Main category: cs.DC

TL;DR: PRAXIS是一个基于LLM的云事故诊断编排器，通过遍历服务依赖图和程序依赖图来定位代码和配置问题，相比现有方法将RCA准确率提升3.1倍，同时减少3.8倍的token消耗。


<details>
  <summary>Details</summary>
Motivation: 云事故造成巨大经济损失（平均每小时超过200万美元），而代码和配置问题是云事故的主要根本原因，需要更有效的诊断方法。

Method: PRAXIS采用LLM驱动的结构化遍历策略，在两种图上进行遍历：1) 服务依赖图(SDG)捕获微服务级依赖关系；2) 程序依赖图(PDG)捕获每个微服务的代码级依赖关系。LLM作为遍历策略在这些图上移动，定位和解释故障。

Result: 相比最先进的ReAct基线方法，PRAXIS将根本原因分析(RCA)准确率提高了3.1倍，同时减少了3.8倍的token消耗。在30个真实世界事故案例上进行了验证。

Conclusion: PRAXIS通过结合服务级和代码级依赖图的LLM驱动遍历，有效提高了云事故诊断的准确性和效率，为云事故根本原因分析提供了新的解决方案。

Abstract: Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [Online Learning Extreme Learning Machine with Low-Complexity Predictive Plasticity Rule and FPGA Implementation](https://arxiv.org/abs/2512.21777)
*Zhenya Zang,Xingda Li,David Day Uei Li*

Main category: cs.AR

TL;DR: 提出一种简化的生物启发式预测局部学习规则，无需全局反向传播和事件驱动训练中的膜积分，仅基于预测误差触发权重更新，通过稀疏二进制驱动向量加法实现。将该规则集成到极限学习机中，替代计算密集的矩阵求逆，将训练复杂度从O(M³)降至O(M)，精度损失很小（训练和测试集分别下降3.6%和2.0%），FPGA实现显示计算和内存需求显著降低，适合低功耗边缘设备在线学习。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络需要全局反向传播，计算复杂度高；事件驱动训练需要膜积分，实现复杂。现有方法不适合资源受限的边缘设备进行在线学习，需要开发更高效、低功耗的学习算法。

Method: 提出生物启发的预测局部学习规则：1）仅当出现预测误差时触发权重更新；2）使用稀疏二进制驱动向量加法进行更新；3）将该规则集成到极限学习机中，替代传统的矩阵求逆操作；4）实现FPGA硬件加速。

Result: 1）训练复杂度从O(M³)降至O(M)，M为隐藏层节点数；2）精度损失很小：训练集下降3.6%，测试集下降2.0%；3）FPGA实现显示计算和内存需求显著降低；4）与现有研究相比，在计算效率和资源使用方面有明显优势。

Conclusion: 提出的简化预测局部学习规则结合极限学习机，实现了高效低功耗的在线学习，特别适合资源受限的边缘设备应用，在保持可接受精度损失的同时大幅降低计算复杂度。

Abstract: We propose a simplified, biologically inspired predictive local learning rule that eliminates the need for global backpropagation in conventional neural networks and membrane integration in event-based training. Weight updates are triggered only on prediction errors and are performed using sparse, binary-driven vector additions. We integrate this rule into an extreme learning machine (ELM), replacing the conventional computationally intensive matrix inversion. Compared to standard ELM, our approach reduces the complexity of the training from O(M^3) to O(M), in terms of M nodes in the hidden layer, while maintaining comparable accuracy (within 3.6% and 2.0% degradation on training and test datasets, respectively). We demonstrate an FPGA implementation and compare it with existing studies, showing significant reductions in computational and memory requirements. This design demonstrates strong potential for energy-efficient online learning on low-cost edge devices.

</details>


### [15] [Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling](https://arxiv.org/abs/2512.22066)
*Hannah Atmer,Yuan Yao,Thiemo Voigt,Stefanos Kaxiras*

Main category: cs.AR

TL;DR: 研究LLM推理中SRAM大小和运行频率对能效的影响，发现小缓冲区（32-64KB）和高频率（1200-1400MHz）组合能实现最佳能耗延迟积。


<details>
  <summary>Details</summary>
Motivation: LLM的能耗直接影响部署成本和环境影响，需要研究硬件配置（SRAM大小、运行频率）如何影响推理能效，特别针对计算密集型预填充和内存密集型解码两个阶段的不同特性。

Method: 采用OpenRAM进行能耗建模、LLMCompass进行延迟模拟、ScaleSIM进行脉动阵列操作强度分析的仿真方法，研究SRAM大小和运行频率对LLM推理能效的影响。

Result: 总能耗主要由SRAM大小决定，大缓冲区因漏电显著增加静态能耗；高频率虽增加动态功耗但能减少执行时间，从而降低总能耗；内存带宽成为性能瓶颈，计算频率提升只在内存带宽允许范围内有效。

Conclusion: 针对模拟工作负载，最佳硬件配置为小本地缓冲区（32-64KB）和高运行频率（1200-1400MHz），该组合能实现最佳能耗延迟积，为设计能效型LLM加速器提供具体架构指导。

Abstract: Energy consumption dictates the cost and environmental impact of deploying Large Language Models. This paper investigates the impact of on-chip SRAM size and operating frequency on the energy efficiency and performance of LLM inference, focusing on the distinct behaviors of the compute-bound prefill and memory-bound decode phases. Our simulation methodology combines OpenRAM for energy modeling, LLMCompass for latency simulation, and ScaleSIM for systolic array operational intensity. Our findings show that total energy use is predominantly determined by SRAM size in both phases, with larger buffers significantly increasing static energy due to leakage, which is not offset by corresponding latency benefits. We quantitatively explore the memory-bandwidth bottleneck, demonstrating that while high operating frequencies reduce prefill latency, their positive impact on memory-bound decode latency is capped by the external memory bandwidth. Counter-intuitively, high compute frequency can reduce total energy by reducing execution time and consequently decreasing static energy consumption more than the resulting dynamic power increase. We identify an optimal hardware configuration for the simulated workload: high operating frequencies (1200MHz-1400MHz) and a small local buffer size of 32KB to 64KB. This combination achieves the best energy-delay product, balancing low latency with high energy efficiency. Furthermore, we demonstrate how memory bandwidth acts as a performance ceiling, and that increasing compute frequency only yields performance gains up to the point where the workload becomes memory-bound. This analysis provides concrete architectural insights for designing energy-efficient LLM accelerators, especially for datacenters aiming to minimize their energy overhead.

</details>
