<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Semantically Reflected Programs](https://arxiv.org/abs/2509.03318)
*Eduard Kamburjan,Vidar Norstein Klungre,Yuanwei Qu,Rudolf Schlatte,Egor V. Kostylev,Martin Giese,Einar Broch Johnsen*

Main category: cs.PL

TL;DR: 该论文提出通过语义提升程序来解决结构知识和行为知识形式化之间的二分法，将程序状态转换为知识图谱，并在编程语言中提供语义反射层。


<details>
  <summary>Details</summary>
Motivation: 解决知识图谱（表示系统个体和普遍知识）与编程语言（描述系统演化）之间的形式化差异，让程序员能够在程序中利用应用领域的知识。

Method: 为面向对象编程语言引入语义提升技术，将执行程序的状态转换为知识图谱，并在编程语言SMOL中实现语义反射层，支持运行时程序查询。

Result: 开发了SMOL语言并实现了语义提升和语义反射的正式化，通过地质建模案例研究展示了该技术的应用，语言实现已开源。

Conclusion: 语义提升和语义反射技术成功弥合了结构知识和行为知识形式化之间的差距，为程序员提供了在程序中直接利用领域知识的能力。

Abstract: This paper addresses the dichotomy between the formalization of structural
and the formalization of behavioral knowledge by means of semantically lifted
programs, which explore an intuitive connection between programs and knowledge
graphs. While knowledge graphs and ontologies are eminently useful to represent
formal knowledge about a system's individuals and universals, programming
languages are designed to describe the system's evolution. To address this
dichotomy, we introduce a semantic lifting of the program states of an
executing program into a knowledge graph, for an object-oriented programming
language. The resulting graph is exposed as a semantic reflection layer within
the programming language, allowing programmers to leverage knowledge of the
application domain in their programs. In this paper, we formalize semantic
lifting and semantic reflection for a small programming language, SMOL, explain
the operational aspects of the language, and consider type correctness and
virtualisation for runtime program queries through the semantic reflection
layer. We illustrate semantic lifting and semantic reflection through a case
study of geological modelling and discuss different applications of the
technique. The language implementation is open source and available online.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [A Novel IaaS Tax Model as Leverage Towards Green Cloud Computing](https://arxiv.org/abs/2509.02767)
*Benedikt Pittl,Werner Mach,Erich Schikuta*

Main category: cs.DC

TL;DR: 这篇论文提出了一种称为GreenCloud税的经济模型，通过对能源效率低的数据中心征税来促进云计算数据中心的能源效率提升。


<details>
  <summary>Details</summary>
Motivation: 云计算数据中心的能源消耗趋势上升，需要有效的方法来提高能源效率，以实现绿色云计算。

Method: 开发了GreenCloud税模型，对能源效率低的数据中心进行罚款，而促进能源效率高的数据中心。使用CloudSim模拟环境实现，并采用SPEC净化数据进行模拟评估。

Result: 能源效率高的数据中心能够提供更便宜的服务价格，导致工作负载从能源效率低的数据中心转移到能源效率高的数据中心。

Conclusion: GreenCloud税模型是一种有效的经济手段，可以通过市场机制促进云计算数据中心的能源效率提升，为绿色云计算提供了新的解决方案。

Abstract: The cloud computing technology uses datacenters, which require energy. Recent
trends show that the required energy for these datacenters will rise over time,
or at least remain constant. Hence, the scientific community developed
different algorithms, architectures, and approaches for improving the energy
efficiency of cloud datacenters, which are summarized under the umbrella term
Green Cloud computing. In this paper, we use an economic approach - taxes - for
reducing the energy consumption of datacenters. We developed a tax model called
GreenCloud tax, which penalizes energy-inefficient datacenters while fostering
datacenters that are energy-efficient. Hence, providers running
energy-efficient datacenters are able to offer cheaper prices to consumers,
which consequently leads to a shift of workloads from energy-inefficient
datacenters to energy-efficient datacenters. The GreenCloud tax approach was
implemented using the simulation environment CloudSim. We applied real data
sets published in the SPEC benchmark for the executed simulation scenarios,
which we used for evaluating the GreenCloud tax.

</details>


### [3] [Mycroft: Tracing Dependencies in Collective Communication Towards Reliable LLM Training](https://arxiv.org/abs/2509.03018)
*Yangtao Deng,Lei Zhang,Qinlong Wang,Xiaoyun Zhi,Xinlei Zhang,Zhuo Jiang,Haohan Xu,Lei Wang,Zuquan Song,Gaohong Liu,Yang Bai,Shuguang Wang,Wencong Xiao,Jianxi Ye,Minlan Yu,Hong Xu*

Main category: cs.DC

TL;DR: Mycroft是一个轻量级分布式追踪和根因分析系统，用于解决LLM训练中集体通信的可靠性问题，通过追踪通信状态和利用内部依赖关系来快速检测和诊断问题。


<details>
  <summary>Details</summary>
Motivation: LLM训练中的可靠性问题导致资源浪费和性能下降，现有集体通信库作为黑盒运行，缺乏有效的根因分析所需的关键信息。

Method: 设计Mycroft系统，通过追踪集体通信状态并利用内部控制和数据依赖关系来解决可靠性问题，支持运行时调试。

Result: 在字节跳动部署6个月，90%情况下15秒内检测到异常，60%情况下20秒内识别根因，故障注入实验验证了其能力和效率。

Conclusion: Mycroft有效解决了LLM训练中集体通信的隐藏可靠性问题，提供了快速异常检测和根因分析能力。

Abstract: Reliability is essential for ensuring efficiency in LLM training. However,
many real-world reliability issues remain difficult to resolve, resulting in
wasted resources and degraded model performance. Unfortunately, today's
collective communication libraries operate as black boxes, hiding critical
information needed for effective root cause analysis. We propose Mycroft, a
lightweight distributed tracing and root cause analysis system designed to
address previously hidden reliability issues in collective communication.
Mycroft's key idea is to trace collective communication states and leverage
internal control and data dependencies to resolve reliability problems in LLM
training. Mycroft has been deployed at ByteDance for over six months to debug
collective communication related issues at runtime. It detected anomalies
within 15 seconds in 90% of cases and identified the root cause within 20
seconds in 60% of cases. We also conducted extensive fault injection
experiments to demonstrate Mycroft's capability and efficiency.

</details>


### [4] [FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale Training of LLMs](https://arxiv.org/abs/2509.03047)
*Haijun Zhang,Jinxiang Wang,Zhenhua Yu,Yanyong Zhang,Xuejie Ji,Kaining Mao,Jun Zhang,Yaqing Zhang,Ting Wu,Fei Jie,Xiemin Huang,Zhifang Cai,Junhua Cheng,Shuwei Wang,Wei Li,Xiaoming Bao,Hua Xu,Shixiong Zhao,Jun Li,Hongwei Sun,Ziyang Zhang,Yi Xiong,Chunsheng Li*

Main category: cs.DC

TL;DR: FlashRecovery是一个针对大规模LLM训练的高效故障恢复系统，通过实时故障检测、规模无关的任务重启和无检查点单步恢复三大模块，显著降低训练中断时间


<details>
  <summary>Details</summary>
Motivation: 大规模LLM训练需要庞大的AI加速器集群和复杂并行策略，硬件软件故障导致大量训练时间损失，传统检查点方法开销巨大

Method: 1) 主动实时故障检测模块进行持续训练状态监控；2) 规模无关任务重启，对正常和故障节点采用不同恢复策略；3) 无检查点单步恢复机制，完全消除对传统检查点的依赖

Result: 在4800个设备的训练集群上实现150秒内训练恢复，不同规模训练任务的故障恢复时间几乎一致

Conclusion: FlashRecovery系统实现了最优的恢复时间目标(RTO)和恢复点目标(RPO)，大幅提升了长周期LLM训练的可靠性和效率

Abstract: Large language models (LLMs) have made a profound impact across various
fields due to their advanced capabilities. However, training these models at
unprecedented scales requires extensive AI accelerator clusters and
sophisticated parallelism strategies, which pose significant challenges in
maintaining system reliability over prolonged training periods. A major concern
is the substantial loss of training time caused by inevitable hardware and
software failures. To address these challenges, we present FlashRecovery, a
fast and low-cost failure recovery system comprising three core modules: (1)
Active and real-time failure detection. This module performs continuous
training state monitoring, enabling immediate identification of hardware and
software failures within seconds, thus ensuring rapid incident response; (2)
Scale-independent task restart. By employing different recovery strategies for
normal and faulty nodes, combined with an optimized communication group
reconstruction protocol, our approach ensures that the recovery time remains
nearly constant, regardless of cluster scale; (3) Checkpoint-free recovery
within one step. Our novel recovery mechanism enables single-step restoration,
completely eliminating dependence on traditional checkpointing methods and
their associated overhead. Collectively, these innovations enable FlashRecovery
to achieve optimal Recovery Time Objective (RTO) and Recovery Point Objective
(RPO), substantially improving the reliability and efficiency of long-duration
LLM training. Experimental results demonstrate that FlashRecovery system can
achieve training restoration on training cluster with 4, 800 devices in 150
seconds. We also verify that the time required for failure recovery is nearly
consistent for different scales of training tasks.

</details>


### [5] [The High Cost of Keeping Warm: Characterizing Overhead in Serverless Autoscaling Policies](https://arxiv.org/abs/2509.03104)
*Leonid Kondrashov,Boxi Zhou,Hancheng Wang,Dmitrii Ustiugov*

Main category: cs.DC

TL;DR: 本文分析了无服务器计算中控制平面设计的性能-成本权衡，通过开源系统模拟商业平台行为，发现实例周转导致10-40%的CPU开销和2-10倍的内存浪费，现有系统难以同时优化开销和性能。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算正在改变云应用开发，但控制平面设计的性能-成本权衡缺乏深入理解，主要原因是缺少跨平台的开放基准测试和详细系统分析。

Method: 设计近似商业提供商（如AWS Lambda和Google Cloud Run）扩展行为的无服务器系统，通过回放真实工作负载和调整自动扩展参数，系统比较同步和异步自动扩展策略的性能和成本效益。

Result: 发现无服务器系统因实例周转产生显著计算开销（相当于请求处理CPU周期的10-40%），内存分配比实际使用多2-10倍，且减少这些开销通常会导致性能显著下降。

Conclusion: 当前系统在开销和性能之间存在根本性权衡，需要开发新的成本效益优化的自动扩展策略，混合方法（真实部署+大规模模拟）有助于弥合研究集群与实际环境之间的差距。

Abstract: Serverless computing is transforming cloud application development, but the
performance-cost trade-offs of control plane designs remain poorly understood
due to a lack of open, cross-platform benchmarks and detailed system analyses.
In this work, we address these gaps by designing a serverless system that
approximates the scaling behaviors of commercial providers, including AWS
Lambda and Google Cloud Run. We systematically compare the performance and
cost-efficiency of both synchronous and asynchronous autoscaling policies by
replaying real-world workloads and varying key autoscaling parameters.
  We demonstrate that our open-source systems can closely replicate the
operational characteristics of commercial platforms, enabling reproducible and
transparent experimentation. By evaluating how autoscaling parameters affect
latency, memory usage, and CPU overhead, we reveal several key findings. First,
we find that serverless systems exhibit significant computational overhead due
to instance churn equivalent to 10-40% of the CPU cycles spent on request
handling, primarily originating from worker nodes. Second, we observe high
memory allocation due to scaling policy: 2-10 times more than actively used.
Finally, we demonstrate that reducing these overheads typically results in
significant performance degradation in the current systems, underscoring the
need for new, cost-efficient autoscaling strategies. Additionally, we employ a
hybrid methodology that combines real control plane deployments with
large-scale simulation to extend our evaluation closer to a production scale,
thereby bridging the gap between small research clusters and real-world
environments.

</details>


### [6] [Efficient and Secure Sleepy Model for BFT Consensus](https://arxiv.org/abs/2509.03145)
*Pengkun Ren,Hai Dong,Zahir Tari,Pengcheng Zhang*

Main category: cs.DC

TL;DR: 通过结合PVSS预提交机制的BFT协议，在动态可用系统中实现低延迟和高安全性，只需4网络延迟并承受1/2恶意参与者


<details>
  <summary>Details</summary>
Motivation: 解决动态可用系统中BFT协议在延迟和安全性之间的平衡挑战，现有方案多需多轮投票导致高延迟或安全性不足

Method: 集成预提交机制和公开可验证秘密分享(PVSS)，通过PVSS绑定用户身份和消息，减少通信轮数

Result: 在常见场景下仅需4网络延迟(4Δ)，能承受至1/2恶意参与者，显著减少分叉发生并提升链稳定性

Conclusion: 该协议在保持完整性的前提下提升了效率和安全性，在中等参与波动场景中保持稳定性和低延迟

Abstract: Byzantine Fault Tolerant (BFT) consensus protocols for dynamically available
systems face a critical challenge: balancing latency and security in
fluctuating node participation. Existing solutions often require multiple
rounds of voting per decision, leading to high latency or limited resilience to
adversarial behavior. This paper presents a BFT protocol integrating a
pre-commit mechanism with publicly verifiable secret sharing (PVSS) into
message transmission. By binding users' identities to their messages through
PVSS, our approach reduces communication rounds. Compared to other
state-of-the-art methods, our protocol typically requires only four network
delays (4$\Delta$) in common scenarios while being resilient to up to 1/2
adversarial participants. This integration enhances the efficiency and security
of the protocol without compromising integrity. Theoretical analysis
demonstrates the robustness of the protocol against Byzantine attacks.
Experimental evaluations show that, compared to traditional BFT protocols, our
protocol significantly prevents fork occurrences and improves chain stability.
Furthermore, compared to longest-chain protocol, our protocol maintains
stability and lower latency in scenarios with moderate participation
fluctuations.

</details>


### [7] [CloudFormer: An Attention-based Performance Prediction for Public Clouds with Unknown Workload](https://arxiv.org/abs/2509.03394)
*Amirhossein Shahbazinia,Darong Huang,Luis Costero,David Atienza*

Main category: cs.DC

TL;DR: 云平台多租户环境中虚拟机性能干扰预测难题，CloudFormer使用双分支Transformer模型，在黑盒环境下精确预测性能降级，MAE仅7.8%，显著提升预测准确性


<details>
  <summary>Details</summary>
Motivation: 多租户云环境中虚拟机共享资源导致性能干扰，现有管理技术需要准确性能预测，但公有云黑盒特性和动态工作负载使得这一挑战困难

Method: 设计CloudFormer双分支Transformer模型，聚合建模时间动态性和系统级交互作用，利用206个系统指标以秒级分辨率，无需场景特定调优

Result: CloudFormer在多租户评估指标上持续超过最先进基线模型，实现了跨工作负载的稳健泛化能力，MAE仅7.8%，比现有方法提升至少28%

Conclusion: 该研究提出的CloudFormer模型能够有效解决云平台性能干扰预测难题，为云资源管理提供了更准确的性能预测能力，具有重要的实践价值

Abstract: Cloud platforms are increasingly relied upon to host diverse,
resource-intensive workloads due to their scalability, flexibility, and
cost-efficiency. In multi-tenant cloud environments, virtual machines are
consolidated on shared physical servers to improve resource utilization. While
virtualization guarantees resource partitioning for CPU, memory, and storage,
it cannot ensure performance isolation. Competition for shared resources such
as last-level cache, memory bandwidth, and network interfaces often leads to
severe performance degradation. Existing management techniques, including VM
scheduling and resource provisioning, require accurate performance prediction
to mitigate interference. However, this remains challenging in public clouds
due to the black-box nature of VMs and the highly dynamic nature of workloads.
To address these limitations, we propose CloudFormer, a dual-branch
Transformer-based model designed to predict VM performance degradation in
black-box environments. CloudFormer jointly models temporal dynamics and
system-level interactions, leveraging 206 system metrics at one-second
resolution across both static and dynamic scenarios. This design enables the
model to capture transient interference effects and adapt to varying workload
conditions without scenario-specific tuning. Complementing the methodology, we
provide a fine-grained dataset that significantly expands the temporal
resolution and metric diversity compared to existing benchmarks. Experimental
results demonstrate that CloudFormer consistently outperforms state-of-the-art
baselines across multiple evaluation metrics, achieving robust generalization
across diverse and previously unseen workloads. Notably, CloudFormer attains a
mean absolute error (MAE) of just 7.8%, representing a substantial improvement
in predictive accuracy and outperforming existing methods at least by 28%.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [Portable Targeted Sampling Framework Using LLVM](https://arxiv.org/abs/2509.02873)
*Zhantong Qiu,Mahyar Samani,Jason Lowe-Power*

Main category: cs.AR

TL;DR: Nugget是一个灵活的采样框架，可在模拟器和真实硬件上进行跨平台采样，大幅降低区间分析成本并支持原生速度验证


<details>
  <summary>Details</summary>
Motivation: 全面的架构评估受到缓慢模拟和每二进制采样流程的限制，需要更高效的采样方法

Method: 在LLVM IR级别进行二进制无关的区间分析，生成轻量级可执行文件(nuggets)，支持跨模拟器、硬件、ISA和库的便携采样

Result: 在SPEC CPU2017、NPB和LSMS上，相比功能模拟将区间分析成本降低数个数量级(多线程NPB最高578倍)，单线程开销低，支持原生速度验证

Conclusion: Nugget使采样方法学研究更快、更便携，支持系统性能和模型准确性的评估

Abstract: Comprehensive architectural evaluation of full workloads is throttled by slow
simulation and per-binary sampling pipelines. We present Nugget, a flexible
framework for portable sampling across simulators and real hardware, ISAs, and
libraries. Nugget operates at the LLVM IR level to perform binary-agnostic
interval analysis, then emits lightweight, cross-platform
executables--nuggets--that can be validated on real machines before driving
simulation. Across SPEC CPU2017, NPB, and LSMS, Nugget cuts interval-analysis
cost by orders of magnitude relative to functional simulation (up to ~578X on
multithreaded NPB), keeps single-thread overhead low, and enables native-speed
validation of selected samples. Case studies with gem5 show that nuggets
support evaluation of system performance and model accuracy. Nugget makes
sampling methodology research faster and more portable.

</details>


### [9] [FastCaps: A Design Methodology for Accelerating Capsule Network on Field Programmable Gate Arrays](https://arxiv.org/abs/2509.03103)
*Abdul Rahoof,Vivek Chaturvedi,Muhammad Shafique*

Main category: cs.AR

TL;DR: 这篇论文提出了一种在FPGA上加速完整Capsule网络的方法，通过新的剪枝技术和路由算法优化，在保持性能的同时大幅提升了速度。


<details>
  <summary>Details</summary>
Motivation: Capsule网络虽然在图像理解方面优于传统CNN，但其复杂的结构和路由机制导致在FPGA上加速困难，现有方案大多只采用部分加速，需要完整的解决方案来支持实际应用。

Method: 提出两步方法：1）使用新的Look-Ahead Kernel Pruning(LAKP)剪枝技术，基于模型参数的预先评分进行网络剪枝；2）简化非线性操作、重新排序循环和并行化操作来降低路由算法的硬件复杂度。

Result: 在MNIST和F-MNIST数据集上，LAKP方法达到了99.26%和98.84%的压缩率，在PYNQ-Z1 FPGA上分别实现82 FPS和48 FPS的速度。通过进一步优化路由算法，速度提升到1351 FPS和934 FPS。

Conclusion: 这是首个在FPGA上完整加速CapsNet的工作，为在现代边缘设备中高效部署CapsNet提供了可行的解决方案，具有重要的实际应用价值。

Abstract: Capsule Network (CapsNet) has shown significant improvement in understanding
the variation in images along with better generalization ability compared to
traditional Convolutional Neural Network (CNN). CapsNet preserves spatial
relationship among extracted features and apply dynamic routing to efficiently
learn the internal connections between capsules. However, due to the capsule
structure and the complexity of the routing mechanism, it is non-trivial to
accelerate CapsNet performance in its original form on Field Programmable Gate
Array (FPGA). Most of the existing works on CapsNet have achieved limited
acceleration as they implement only the dynamic routing algorithm on FPGA,
while considering all the processing steps synergistically is important for
real-world applications of Capsule Networks. Towards this, we propose a novel
two-step approach that deploys a full-fledged CapsNet on FPGA. First, we prune
the network using a novel Look-Ahead Kernel Pruning (LAKP) methodology that
uses the sum of look-ahead scores of the model parameters. Next, we simplify
the nonlinear operations, reorder loops, and parallelize operations of the
routing algorithm to reduce CapsNet hardware complexity. To the best of our
knowledge, this is the first work accelerating a full-fledged CapsNet on FPGA.
Experimental results on the MNIST and F-MNIST datasets (typical in Capsule
Network community) show that the proposed LAKP approach achieves an effective
compression rate of 99.26% and 98.84%, and achieves a throughput of 82 FPS and
48 FPS on Xilinx PYNQ-Z1 FPGA, respectively. Furthermore, reducing the hardware
complexity of the routing algorithm increases the throughput to 1351 FPS and
934 FPS respectively. As corroborated by our results, this work enables highly
performance-efficient deployment of CapsNets on low-cost FPGA that are popular
in modern edge devices.

</details>


### [10] [CapsBeam: Accelerating Capsule Network based Beamformer for Ultrasound Non-Steered Plane Wave Imaging on Field Programmable Gate Array](https://arxiv.org/abs/2509.03201)
*Abdul Rahoof,Vivek Chaturvedi,Mahesh Raveendranatha Panicker,Muhammad Shafique*

Main category: cs.AR

TL;DR: 本文提出CapsBeam胎团网络基于的波束成型器，通过剪枝和量化优化，在FPGA上实现高效加速，在图像质量和计算效率方面都显著优于传统DAS方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习在超声波束成型中应用遍历，但现有模型过大过复杂，难以部署到边缘设备上。需要一种新的轻量级方案来解决这个问题。

Method: 设计了CapsBeam胎团网络波束成型器，使用LAKP-ML剪枝技术减少参数重复，通过量化、非线性操作简化和并行化降低硬件复杂度，最后在FPGA上实现专门的加速器架构。

Result: 实验结果显示：在in-vivo数据上减少了伪影；in-vitro数据对比度提升32.31%，轴向和横向分辨率分别提升16.54%和6.7%；in-silico数据对比度提升26%，轴向和横向分辨率分别提升13.6%和21.5%。剪枝后压缩比85%但图像质量不变，FPGA加速器达到卷积运30 GOPS和动态路由运17.4 GOPS的速度。

Conclusion: CapsBeam通过胎团网络结构和多层次优化技术，成功实现了高性能、轻量化的超声波束成型解决方案，适合部署在边缘设备上，为实时超声图像处理提供了可行的技术路径。

Abstract: In recent years, there has been a growing trend in accelerating
computationally complex non-real-time beamforming algorithms in ultrasound
imaging using deep learning models. However, due to the large size and
complexity these state-of-the-art deep learning techniques poses significant
challenges when deploying on resource-constrained edge devices. In this work,
we propose a novel capsule network based beamformer called CapsBeam, designed
to operate on raw radio-frequency data and provide an envelope of beamformed
data through non-steered plane wave insonification. Experiments on in-vivo
data, CapsBeam reduced artifacts compared to the standard Delay-and-Sum (DAS)
beamforming. For in-vitro data, CapsBeam demonstrated a 32.31% increase in
contrast, along with gains of 16.54% and 6.7% in axial and lateral resolution
compared to the DAS. Similarly, in-silico data showed a 26% enhancement in
contrast, along with improvements of 13.6% and 21.5% in axial and lateral
resolution, respectively, compared to the DAS. To reduce the parameter
redundancy and enhance the computational efficiency, we pruned the model using
our multi-layer LookAhead Kernel Pruning (LAKP-ML) methodology, achieving a
compression ratio of 85% without affecting the image quality. Additionally, the
hardware complexity of the proposed model is reduced by applying quantization,
simplification of non-linear operations, and parallelizing operations. Finally,
we proposed a specialized accelerator architecture for the pruned and optimized
CapsBeam model, implemented on a Xilinx ZU7EV FPGA. The proposed accelerator
achieved a throughput of 30 GOPS for the convolution operation and 17.4 GOPS
for the dynamic routing operation.

</details>


### [11] [Amplifying Effective CXL Memory Bandwidth for LLM Inference via Transparent Near-Data Processing](https://arxiv.org/abs/2509.03377)
*Rui Xie,Asad Ul Haq,Linsen Ma,Yunhua Fang,Zirak Burzin Engineer,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: CXL-NDP是一种透明近数据处理架构，通过动态量化和无损压缩技术，在不改变CXL.mem接口或AI模型的情况下，显著提升CXL内存带宽效率，改善LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理受限于CXL扩展内存的有限带宽，需要通过创新架构来提升带宽利用效率。

Method: 集成精度可扩展的位平面布局实现动态量化，并在CXL设备内部对权重和KV缓存进行透明无损压缩。

Result: 端到端服务中吞吐量提升43%，最大上下文长度扩展87%，KV缓存占用减少46.9%，且无精度损失。硬件合成证实其硅面积占用适中。

Conclusion: CXL-NDP为生成式AI基础设施采用高效、可扩展的CXL内存降低了技术门槛，具有实际应用价值。

Abstract: Large language model (LLM) inference is bottlenecked by the limited bandwidth
of CXL-based memory used for capacity expansion. We introduce CXL-NDP, a
transparent near-data processing architecture that amplifies effective CXL
bandwidth without requiring changes to the CXL.mem interface or AI models.
CXL-NDP integrates a precision-scalable bit-plane layout for dynamic
quantization with transparent lossless compression of weights and KV caches
directly within the CXL device. In end-to-end serving, CXL-NDP improves
throughput by 43%, extends the maximum context length by 87%, and reduces the
KV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms
its practicality with a modest silicon footprint, lowering the barrier for
adopting efficient, scalable CXL-based memory in generative AI infrastructure.

</details>
