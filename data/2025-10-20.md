<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Visualizing miniKanren Search with a Fine-Grained Small-Step Semantics](https://arxiv.org/abs/2510.15178)
*Brysen Pfingsten,Jason Hemann*

Main category: cs.PL

TL;DR: 提出了一个确定性小步操作语义来显式表示miniKanren执行过程中的搜索树演化，并基于此实现了一个交互式可视化工具。


<details>
  <summary>Details</summary>
Motivation: 为了精确建模miniKanren的交错搜索和目标调度机制，帮助用户理解其公平搜索行为和操作效果，特别是令人惊讶的答案顺序。

Method: 开发了确定性小步操作语义，显式表示搜索树的演化过程，包括目标激活、挂起、恢复和成功等步骤，并基于此语义实现交互式可视化器。

Result: 通过基于属性的测试验证了语义和工具的正确性，并通过多个示例展示了工具的教学价值。

Conclusion: 该语义和可视化工具为miniKanren提供了一个有效的教学概念机器，帮助用户深入理解其搜索行为和操作语义。

Abstract: We present a deterministic small-step operational semantics for miniKanren
that explicitly represents the evolving search tree during execution. This
semantics models interleaving and goal scheduling at fine granularity, allowing
each evaluation step-goal activation, suspension, resumption, and success -- to
be visualized precisely. Building on this model, we implement an interactive
visualizer that renders the search tree as it develops and lets users step
through execution. The tool acts as a pedagogical notional machine for
reasoning about miniKanren's fair search behavior, helping users understand
surprising answer orders and operational effects. Our semantics and tool are
validated through property-based testing and illustrated with several examples.

</details>


### [2] [Grassroots Logic Programs: A Secure, Multiagent, Concurrent, Logic Programming Language](https://arxiv.org/abs/2510.15747)
*Ehud Shapiro*

Main category: cs.PL

TL;DR: 提出Grassroots Logic Programs (GLP)，一种安全的、多智能体的并发逻辑编程语言，用于实现去中心化的基层平台，解决恶意参与者问题。


<details>
  <summary>Details</summary>
Motivation: 基层平台面临恶意参与者的挑战，需要安全编程支持来确保正确参与者能够可靠识别彼此、建立安全通信并验证代码完整性。

Method: 扩展逻辑程序，添加配对的单读单写逻辑变量，通过加密、签名和认证消息提供安全通信通道，支持身份和代码完整性验证。

Result: 证明了GLP的安全属性，包括计算是演绎的、SRSW保持性、无环性和单调性，多智能体GLP具有基层特性，GLP流实现区块链安全属性。

Conclusion: GLP为基层平台提供了安全实现基础，展示了安全的基层社交网络应用。

Abstract: Grassroots platforms are distributed applications run by\linebreak
cryptographically-identified people on their networked personal devices, where
multiple disjoint platform instances emerge independently and coalesce when
they interoperate. Their foundation is the grassroots social graph, upon which
grassroots social networks, grassroots cryptocurrencies, and grassroots
democratic federations can be built.
  Grassroots platforms have yet to be implemented, the key challenge being
faulty and malicious participants: without secure programming support, correct
participants cannot reliably identify each other, establish secure
communication, or verify each other's code integrity.
  We present Grassroots Logic Programs (GLP), a secure, multiagent, concurrent,
logic programming language for implementing grassroots platforms. GLP extends
logic programs with paired single-reader/single-writer (SRSW) logic variables,
providing secure communication channels among cryptographically-identified
people through encrypted, signed and attested messages, which enable identity
and code integrity verification. We present GLP progressively: logic programs,
concurrent GLP, multiagent GLP, augmenting it with cryptographic security, and
providing smartphone implementation-ready specifications. We prove safety
properties including that GLP computations are deductions, SRSW preservation,
acyclicity, and monotonicity. We prove multiagent GLP is grassroots and that
GLP streams achieve blockchain security properties. We present a grassroots
social graph protocol establishing authenticated peer-to-peer connections and
demonstrate secure grassroots social networking applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table for GPUs](https://arxiv.org/abs/2510.15095)
*Md Sabbir Hossain Polak,David Troendle,Byunghyun Jang*

Main category: cs.DC

TL;DR: Hive哈希表是一种高性能、支持动态调整大小的GPU哈希表，通过缓存对齐的桶布局、warp同步并发协议和负载感知动态调整策略，在保持95%高负载因子的同时，实现比现有GPU哈希表高1.5-2倍的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有GPU哈希表在并发更新、高负载因子和不规则内存访问模式方面存在性能瓶颈，需要设计能够适应变化工作负载且无需全局重哈希的高性能解决方案。

Method: 1. 缓存对齐的打包桶布局，将键值对存储为64位字，支持合并内存访问和单CAS原子操作；2. Warp同步并发协议(WABC和WCME)，将争用减少到每个warp一个原子操作；3. 负载因子感知的动态调整策略，使用线性哈希以warp并行K桶批次扩展或收缩容量；4. 四步插入策略处理高争用情况。

Result: 在NVIDIA RTX 4090上的实验表明，Hive哈希表在混合插入-删除-查找工作负载下，比最先进的GPU哈希表(Slab-Hash、DyCuckoo、WarpCore)吞吐量高1.5-2倍，负载因子可达95%。在平衡工作负载下，达到35亿次更新/秒和近40亿次查找/秒。

Conclusion: Hive哈希表通过创新的warp协作设计和动态调整机制，为GPU加速数据处理提供了可扩展且高效的哈希表解决方案，显著提升了并发性能和负载能力。

Abstract: Hash tables are essential building blocks in data-intensive applications, yet
existing GPU implementations often struggle with concurrent updates, high load
factors, and irregular memory access patterns. We present Hive hash table, a
high-performance, warp-cooperative and dynamically resizable GPU hash table
that adapts to varying workloads without global rehashing.
  Hive hash table makes three key contributions. First, a cache-aligned packed
bucket layout stores key-value pairs as 64-bit words, enabling coalesced memory
access and atomic updates via single-CAS operations. Second, warp-synchronous
concurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and
Warp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic
operation per warp while ensuring lock-free progress. Third, a
load-factor-aware dynamic resizing strategy expands or contracts capacity in
warp-parallel K-bucket batches using linear hashing, maintaining balanced
occupancy. To handle insertions under heavy contention, Hive hash table employs
a four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and
overflow-stash fallback. This design provides lock-free fast paths and bounded
recovery cost under contention determined by a fixed eviction depth, while
eliminating ABA hazards during concurrent updates.
  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains
load factors up to 95% while delivering 1.5-2x higher throughput than
state-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed
insert-delete-lookup workloads. On balanced workload, Hive hash table reaches
3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability
and efficiency for GPU-accelerated data processing.

</details>


### [4] [NEMO: Faster Parallel Execution for Highly Contended Blockchain Workloads (Full version)](https://arxiv.org/abs/2510.15122)
*François Ezard,Can Umut Ileri,Jérémie Decouchant*

Main category: cs.DC

TL;DR: NEMO是一个新的区块链执行引擎，结合乐观并发控制(OCC)和对象数据模型来解决高竞争工作负载下的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着区块链共识算法效率提升，执行层成为新的性能瓶颈，特别是在高竞争场景下。现有的并行执行框架(乐观或悲观并发控制)在高竞争工作负载下性能都会下降。

Method: NEMO引入四个核心创新：1) 使用自有对象的贪婪提交规则；2) 精炼依赖处理以减少重执行；3) 使用静态可推导的读/写提示指导执行；4) 基于优先级的调度器，优先处理能解锁其他交易的事务。

Result: 模拟执行实验表明，NEMO显著减少了冗余计算，在16个工作线程下，吞吐量比最先进的OCC方法Block-STM高出42%，比悲观并发控制基线高出61%。

Conclusion: NEMO通过结合OCC和对象数据模型，有效解决了高竞争工作负载下的区块链执行性能问题，显著提升了吞吐量。

Abstract: Following the design of more efficient blockchain consensus algorithms, the
execution layer has emerged as the new performance bottleneck of blockchains,
especially under high contention. Current parallel execution frameworks either
rely on optimistic concurrency control (OCC) or on pessimistic concurrency
control (PCC), both of which see their performance decrease when workloads are
highly contended, albeit for different reasons. In this work, we present NEMO,
a new blockchain execution engine that combines OCC with the object data model
to address this challenge. NEMO introduces four core innovations: (i) a greedy
commit rule for transactions using only owned objects; (ii) refined handling of
dependencies to reduce re-executions; (iii) the use of incomplete but
statically derivable read/write hints to guide execution; and (iv) a
priority-based scheduler that favors transactions that unblock others. Through
simulated execution experiments, we demonstrate that NEMO significantly reduces
redundant computation and achieves higher throughput than representative
approaches. For example, with 16 workers NEMO's throughput is up to 42% higher
than the one of Block-STM, the state-of-the-art OCC approach, and 61% higher
than the pessimistic concurrency control baseline used.

</details>


### [5] [An Elastic Job Scheduler for HPC Applications on the Cloud](https://arxiv.org/abs/2510.15147)
*Aditya Bhosale,Kavitha Chandrasekar,Laxmikant Kale,Sara Kokkila-Schumacher*

Main category: cs.DC

TL;DR: 开发了一个Kubernetes操作符来运行Charm++应用，并提出了基于优先级的弹性作业调度器，能够在Kubernetes集群中动态调整HPC作业规模以最大化资源利用率。


<details>
  <summary>Details</summary>
Motivation: 随着HPC应用在云端的采用增加，需要专门的编程模型和调度器来高效利用云资源，特别是支持动态伸缩的能力。传统MPI等并行编程模型缺乏原生自动伸缩支持，而Charm++通过其可迁移对象范式原生支持动态重伸缩。

Method: 开发了Kubernetes操作符来运行Charm++应用，并设计了基于优先级的弹性作业调度器，根据集群状态动态调整作业规模。

Result: 弹性调度器能够以最小开销重伸缩HPC作业，相比传统静态调度器展现出显著的性能提升，在最大化集群利用率的同时最小化高优先级作业的响应时间。

Conclusion: Charm++与Kubernetes的结合以及弹性调度器为云端HPC应用提供了高效的动态资源管理解决方案，显著优于传统静态调度方法。

Abstract: The last few years have seen an increase in adoption of the cloud for running
HPC applications. The pay-as-you-go cost model of these cloud resources has
necessitated the development of specialized programming models and schedulers
for HPC jobs for efficient utilization of cloud resources. A key aspect of
efficient utilization is the ability to rescale applications on the fly to
maximize the utilization of cloud resources. Most commonly used parallel
programming models like MPI have traditionally not supported autoscaling either
in a cloud environment or on supercomputers. While more recent work has been
done to implement this functionality in MPI, it is still nascent and requires
additional programmer effort. Charm++ is a parallel programming model that
natively supports dynamic rescaling through its migratable objects paradigm. In
this paper, we present a Kubernetes operator to run Charm++ applications on a
Kubernetes cluster. We then present a priority-based elastic job scheduler that
can dynamically rescale jobs based on the state of a Kubernetes cluster to
maximize cluster utilization while minimizing response time for high-priority
jobs. We show that our elastic scheduler, with the ability to rescale HPC jobs
with minimal overhead, demonstrates significant performance improvements over
traditional static schedulers.

</details>


### [6] [Spatiotemporal Traffic Prediction in Distributed Backend Systems via Graph Neural Networks](https://arxiv.org/abs/2510.15215)
*Zhimin Qiu,Feng Liu,Yuxiao Wang,Chenrui Hu,Ziyu Cheng,Di Wu*

Main category: cs.DC

TL;DR: 提出基于图神经网络的分布式后端系统流量预测方法，通过图卷积和门控循环结构整合时空特征，显著提升预测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以捕捉分布式系统中复杂的依赖关系和动态特征，需要更有效的建模方法来提升流量预测准确性。

Method: 将系统抽象为图结构，使用图卷积机制进行节点特征传播聚合，结合门控循环结构建模历史序列，通过时空联合建模模块融合图表示与时序依赖，解码器生成未来流量预测。

Result: 在公开分布式系统日志上的实验表明，该方法在不同预测时域和模型深度下均表现稳定且误差较低，显著优于主流基线方法。

Conclusion: 验证了图神经网络在复杂系统建模中的潜力，为分布式后端系统流量预测提供了准确可靠的技术方案。

Abstract: This paper addresses the problem of traffic prediction in distributed backend
systems and proposes a graph neural network based modeling approach to overcome
the limitations of traditional models in capturing complex dependencies and
dynamic features. The system is abstracted as a graph with nodes and edges,
where node features represent traffic and resource states, and adjacency
relations describe service interactions. A graph convolution mechanism enables
multi order propagation and aggregation of node features, while a gated
recurrent structure models historical sequences dynamically, thus integrating
spatial structures with temporal evolution. A spatiotemporal joint modeling
module further fuses graph representation with temporal dependency, and a
decoder generates future traffic predictions. The model is trained with mean
squared error to minimize deviations from actual values. Experiments based on
public distributed system logs construct combined inputs of node features,
topology, and sequences, and compare the proposed method with mainstream
baselines using MSE, RMSE, MAE, and MAPE. Results show that the proposed method
achieves stable performance and low error across different prediction horizons
and model depths, significantly improving the accuracy and robustness of
traffic forecasting in distributed backend systems and verifying the potential
of graph neural networks in complex system modeling.

</details>


### [7] [BeLLMan: Controlling LLM Congestion](https://arxiv.org/abs/2510.15330)
*Tella Rajashekhar Reddy,Atharva Deshmukh,Karan Tandon,Rohan Gandhi,Anjaly Parayil,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: beLLMan控制器通过主动调整LLM应用的输出长度来应对系统负载变化，在真实测试环境中显著降低推理延迟（最高8倍）并减少25%的能耗，同时处理更多请求。


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用对底层基础设施负载不敏感，在系统负载高时会导致推理延迟增加和用户体验下降，需要一种机制来动态调整输出长度以应对负载变化。

Method: 开发beLLMan控制器，使LLM基础设施能够主动向第一方LLM应用发送信号，根据系统负载动态调整输出长度。

Result: 在配备H100 GPU的真实测试平台上，beLLMan在拥塞期间将推理延迟控制在较低水平（端到端延迟最高降低8倍），能耗减少25%，同时处理请求量增加19%。

Conclusion: beLLMan通过主动调整LLM输出长度，有效解决了系统负载导致的延迟和能耗问题，显著提升了LLM推理服务的性能和能效。

Abstract: Large language model (LLM) applications are blindfolded to the infrastructure
underneath and generate tokens autoregressively, indifferent to the system
load, thus risking inferencing latency inflation and poor user experience. Our
first-cut controller, named beLLMan, enables the LLM infrastructure to actively
and progressively signal the first-party LLM application to adjust the output
length in response to changing system load. On a real testbed with H100 GPUs,
beLLMan helps keep inferencing latency under control (upto 8X lower end-to-end
latency) and reduces energy consumption by 25% (while serving 19% more
requests) during periods of congestion for a summarization workload.

</details>


### [8] [Cloud-Enabled Virtual Prototypes](https://arxiv.org/abs/2510.15355)
*Tim Kraus,Axel Sauer,Ingo Feldner*

Main category: cs.DC

TL;DR: 本文探讨了本地与云端仿真环境之间的权衡，重点关注可扩展性与隐私保护，旨在提高远程仿真的可信度并促进虚拟原型技术的采用。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统和AI算法的快速发展需要强大的硬件/软件协同设计方法，而市场上多样化的仿真解决方案各有优缺点，加上远程计算资源的普及，使得选择合适的主机基础设施变得至关重要。

Method: 通过分析本地与云端仿真环境的对比，研究计算基础设施设置对执行性能和数据安全的影响，并强调嵌入式AI开发工作流程中高效仿真的关键作用。

Result: 提出了一个解决方案，旨在可持续地提高对远程仿真的信任度，并促进虚拟原型设计实践的采用。

Conclusion: 在嵌入式AI开发中，需要在本地和云端仿真环境之间做出权衡，平衡可扩展性与隐私保护，通过优化计算基础设施设置来提升性能和数据安全。

Abstract: The rapid evolution of embedded systems, along with the growing variety and
complexity of AI algorithms, necessitates a powerful hardware/software
co-design methodology based on virtual prototyping technologies. The market
offers a diverse range of simulation solutions, each with its unique
technological approach and therefore strengths and weaknesses. Additionally,
with the increasing availability of remote on-demand computing resources and
their adaptation throughout the industry, the choice of the host infrastructure
for execution opens even more new possibilities for operational strategies.
This work explores the dichotomy between local and cloud-based simulation
environments, focusing on the trade-offs between scalability and privacy. We
discuss how the setup of the compute infrastructure impacts the performance of
the execution and security of data involved in the process. Furthermore, we
highlight the development workflow associated with embedded AI and the critical
role of efficient simulations in optimizing these algorithms. With the proposed
solution, we aim to sustainably improve trust in remote simulations and
facilitate the adoption of virtual prototyping practices.

</details>


### [9] [(Almost) Perfect Discrete Iterative Load Balancing](https://arxiv.org/abs/2510.15473)
*Petra Berenbrink,Robert Elsässer,Tom Friedetzky,Hamed Hosseinpour,Dominik Kaaser,Peter Kling,Thomas Sauerwald*

Main category: cs.DC

TL;DR: 该论文研究离散负载平衡问题，通过匹配机制在任意图上进行迭代负载均衡，证明了离散负载平衡可以达到与连续负载平衡相同的性能，最终实现常数为3的负载差异。


<details>
  <summary>Details</summary>
Motivation: 研究离散负载平衡问题的动机在于理解在任意图上通过匹配机制进行负载均衡的性能，探索离散负载平衡是否与连续负载平衡具有相同的复杂度，并改进之前工作中较大的常数差异和非显式结果。

Method: 提出一类简单的局部平衡方案，通过匹配机制在每轮中平衡节点间的令牌数量。当两个匹配节点的令牌总和为奇数时，随机选择接收多余令牌的节点。该方法涵盖三种流行模型：匹配模型、平衡电路模型和异步模型。

Result: 主要结果表明，该离散平衡方案以高概率在渐进匹配连续负载平衡谱界所需的轮数内达到负载差异为3的结果。这一结果不仅实现了小的常数差异，而且适用于任意图而非仅限于正则图。

Conclusion: 该研究证明了在一般模型中，离散负载平衡并不比连续负载平衡更难，离散方案可以达到与连续方案相同的性能，为负载平衡理论提供了重要进展。

Abstract: We consider discrete, iterative load balancing via matchings on arbitrary
graphs. Initially each node holds a certain number of tokens, defining the load
of the node, and the objective is to redistribute the tokens such that
eventually each node has approximately the same number of tokens. We present
results for a general class of simple local balancing schemes where the tokens
are balanced via matchings. In each round the process averages the tokens of
any two matched nodes. If the sum of their tokens is odd, the node to receive
the one excess token is selected at random. Our class covers three popular
models: in the matching model a new matching is generated randomly in each
round, in the balancing circuit model a fixed sequence of matchings is applied
periodically, and in the asynchronous model the load is balanced over a
randomly chosen edge.
  We measure the quality of a load vector by its discrepancy, defined as the
difference between the maximum and minimum load across all nodes. As our main
result we show that with high probability our discrete balancing scheme reaches
a discrepancy of $3$ in a number of rounds which asymptotically matches the
spectral bound for continuous load balancing with fractional load.
  This result improves and tightens a long line of previous works, by not only
achieving a small constant discrepancy (instead of a non-explicit, large
constant) but also holding for arbitrary instead of regular graphs. The result
also demonstrates that in the general model we consider, discrete load
balancing is no harder than continuous load balancing.

</details>


### [10] [Balancing Fairness and Performance in Multi-User Spark Workloads with Dynamic Scheduling (extended version)](https://arxiv.org/abs/2510.15485)
*Dāvis Kažemaks,Laurens Versluis,Burcu Kulahcioglu Ozkan,Jérémie Decouchant*

Main category: cs.DC

TL;DR: 提出了UWFQ调度器，在Spark框架中实现用户级公平调度，通过虚拟公平队列系统和运行时分区技术，显著降低小作业响应时间并确保用户间资源公平分配。


<details>
  <summary>Details</summary>
Motivation: Spark内置调度器在工业分析环境中难以同时维持用户级公平性和低平均响应时间，现有解决方案偏向提交更多作业的用户，缺乏对动态用户工作负载的适应性。

Method: 设计UWFQ调度器，模拟虚拟公平队列系统，基于有界公平模型按预估完成时间调度作业；引入运行时分区技术动态调整任务粒度以解决任务偏斜和优先级反转问题。

Result: 使用多用户合成工作负载和Google集群跟踪进行评估，UWFQ相比现有Spark调度器和先进公平调度算法，将小作业的平均响应时间降低高达74%。

Conclusion: UWFQ调度器有效解决了Spark环境中用户级公平性和性能优化之间的平衡问题，显著提升了小作业性能并确保资源公平分配。

Abstract: Apache Spark is a widely adopted framework for large-scale data processing.
However, in industrial analytics environments, Spark's built-in schedulers,
such as FIFO and fair scheduling, struggle to maintain both user-level fairness
and low mean response time, particularly in long-running shared applications.
Existing solutions typically focus on job-level fairness which unintentionally
favors users who submit more jobs. Although Spark offers a built-in fair
scheduler, it lacks adaptability to dynamic user workloads and may degrade
overall job performance. We present the User Weighted Fair Queuing (UWFQ)
scheduler, designed to minimize job response times while ensuring equitable
resource distribution across users and their respective jobs. UWFQ simulates a
virtual fair queuing system and schedules jobs based on their estimated finish
times under a bounded fairness model. To further address task skew and reduce
priority inversions, which are common in Spark workloads, we introduce runtime
partitioning, a method that dynamically refines task granularity based on
expected runtime. We implement UWFQ within the Spark framework and evaluate its
performance using multi-user synthetic workloads and Google cluster traces. We
show that UWFQ reduces the average response time of small jobs by up to 74%
compared to existing built-in Spark schedulers and to state-of-the-art fair
scheduling algorithms.

</details>


### [11] [Retrofitting Service Dependency Discovery in Distributed Systems](https://arxiv.org/abs/2510.15490)
*Diogo Landau,Gijs Blanken,Jorge Barbosa,Nishant Saurabh*

Main category: cs.DC

TL;DR: 提出了一种名为XXXX的新型运行时系统，用于构建进程级服务依赖图，能够在复杂网络路由机制（包括NAT）下准确推断服务依赖关系，无需源代码插桩。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统中复杂的服务依赖关系容易导致级联故障，但现有方法在NAT等复杂路由技术下无法准确构建服务依赖图，因为NAT会模糊实际运行服务的主机信息。

Method: XXXX采用非破坏性方法，在TCP包头中注入元数据，保持跨主机边界的协议正确性。如果没有接收代理，插桩不会影响现有TCP连接，确保在部分部署时也能无中断运行。

Result: 在9个场景下与3个最先进系统进行比较，涉及3种网络配置（无NAT、内部NAT、外部NAT）和3个微服务基准测试。XXXX是唯一在所有网络配置下表现一致的方法，在大多数场景下精确度和召回率达到100%。

Conclusion: XXXX系统能够有效解决NAT环境下服务依赖图构建的挑战，提供准确且非破坏性的依赖关系推断，在分布式系统故障定位中具有重要价值。

Abstract: Modern distributed systems rely on complex networks of interconnected
services, creating direct or indirect dependencies that can propagate faults
and cause cascading failures. To localize the root cause of performance
degradation in these environments, constructing a service dependency graph is
highly beneficial. However, building an accurate service dependency graph is
impaired by complex routing techniques, such as Network Address Translation
(NAT), an essential mechanism for connecting services across networks. NAT
obfuscates the actual hosts running the services, causing existing run-time
approaches that passively observe network metadata to fail in accurately
inferring service dependencies. To this end, this paper introduces XXXX, a
novel run-time system for constructing process-level service dependency graphs.
It operates without source code instrumentation and remains resilient under
complex network routing mechanisms, including NAT. XXXX implements a
non-disruptive method of injecting metadata onto a TCP packet's header that
maintains protocol correctness across host boundaries. In other words, if no
receiving agent is present, the instrumentation leaves existing TCP connections
unaffected, ensuring non-disruptive operation when it is partially deployed
across hosts. We evaluated XXXX extensively against three state-of-the-art
systems across nine scenarios, involving three network configurations
(NAT-free, internal-NAT, external-NAT) and three microservice benchmarks. XXXX
was the only approach that performed consistently across networking
configurations. With regards to correctness, it performed on par with, or
better than, the state-of-the-art with precision and recall values of 100% in
the majority of the scenarios.

</details>


### [12] [PRISM: Probabilistic Runtime Insights and Scalable Performance Modeling for Large-Scale Distributed Training](https://arxiv.org/abs/2510.15596)
*Alicia Golden,Michael Kuchnik,Samuel Hsia,Zachary DeVito,Gu-Yeon Wei,David Brooks,Carole-Jean Wu*

Main category: cs.DC

TL;DR: PRISM是一个性能建模框架，用于分析大规模分布式训练中的性能变异性，通过统计方法提供训练时间的概率保证，并优化并行化策略以减少变异性影响。


<details>
  <summary>Details</summary>
Motivation: 随着GPU训练规模扩展到数万个GPU，性能变异性成为必然问题，在64k GPU规模下已观察到9%的GPU时间变异性，需要系统化方法来理解和优化这种随机性能波动。

Method: 开发PRISM性能建模框架，使用统计方法量化训练时间的概率保证，分析GPU微基准测试，探索并行化方法和训练系统的设计空间。

Result: PRISM验证显示训练时间预测准确度达到20.8% KS距离，通过优化节点布局可获得1.26倍性能提升潜力，发现AllGather和ReduceScatter通信内核优化对减少训练步骤时间变异性贡献最大。

Conclusion: 大规模分布式训练必须考虑性能变异性，PRISM框架能有效预测和优化训练性能，通信内核优化是减少变异性的关键，为下一代训练系统设计提供指导。

Abstract: Large model training beyond tens of thousands of GPUs is an uncharted
territory. At such scales, disruptions to the training process are not a matter
of if, but a matter of when -- a stochastic process degrading training
productivity. Dynamic runtime variation will become increasingly more frequent
as training scales up and GPUs are operated in increasingly power-limited and
thermally-stressed environments. At the 64k GPU scale, we already observed 9%
GPU time variability for frontier foundation model training. To understand
potential causes of variability, we analyze GPU microbenchmarks at scale across
a variety of platforms, showing up to 14% variation in GPU performance on GEMM
workloads depending on training hardware and deployed environment.
  Motivated by our analysis and the large design space around performance
variability, we present PRISM -- a performance modeling framework that
considers the stochastic nature of the large-scale distributed training. The
core of PRISM is the statistical method that provides a quantifiable measure
for probabilistic guarantees on training time. Using PRISM, we explore the
design and optimization space of distributed training, from parallelization
methods to next-generation training systems. PRISM is validated with
real-system measurement, showing training time prediction accuracy with 20.8%
Kolmogorov-Smirnov distance. Using PRISM, we demonstrate that, depending on
computation node placement, up to 1.26x performance improvement potential is
available if we factor in sensitivities of parallelization strategies to
variation. In addition, we use PRISM to identify kernels to optimize for
reducing performance variability and predict probability of slow-down for
large-scale jobs where variation is magnified. We find optimizing communication
kernels, such as AllGather and ReduceScatter, contribute most to minimizing
variability in training step time.

</details>


### [13] [GOGH: Correlation-Guided Orchestration of GPUs in Heterogeneous Clusters](https://arxiv.org/abs/2510.15652)
*Ahmad Raeisi,Mahdi Dolati,Sina Darabi,Sadegh Talebi,Patrick Eugster,Ahmad Khonsari*

Main category: cs.DC

TL;DR: 提出基于学习的异构集群资源分配架构，通过两个神经网络在线优化机器学习工作负载分配，降低能耗并满足性能要求


<details>
  <summary>Details</summary>
Motivation: 机器学习计算需求增长，异构硬件集群中设备能力、年限和能效各异，升级最新硬件不可行，需要可持续利用现有混合代际资源

Method: 使用两个神经网络：第一个提供新模型在不同硬件上的利用率估计和共置影响，优化器据此分配资源；部署后监控实际性能，用第二个神经网络改进预测

Result: 开发出自适应迭代方法，随时间学习在异构深度学习集群中做出更有效的资源分配决策

Conclusion: 该学习型架构能够有效管理异构集群中的机器学习工作负载，在满足性能要求的同时最小化能耗

Abstract: The growing demand for computational resources in machine learning has made
efficient resource allocation a critical challenge, especially in heterogeneous
hardware clusters where devices vary in capability, age, and energy efficiency.
Upgrading to the latest hardware is often infeasible, making sustainable use of
existing, mixed-generation resources essential. In this paper, we propose a
learning-based architecture for managing machine learning workloads in
heterogeneous clusters. The system operates online, allocating resources to
incoming training or inference requests while minimizing energy consumption and
meeting performance requirements. It uses two neural networks: the first
provides initial estimates of how well a new model will utilize different
hardware types and how it will affect co-located models. An optimizer then
allocates resources based on these estimates. After deployment, the system
monitors real performance and uses this data to refine its predictions via a
second neural network. This updated model improves estimates not only for the
current hardware but also for hardware not initially allocated and for
co-location scenarios not yet observed. The result is an adaptive, iterative
approach that learns over time to make more effective resource allocation
decisions in heterogeneous deep learning clusters.

</details>


### [14] [A Post-Quantum Lower Bound for the Distributed Lovász Local Lemma](https://arxiv.org/abs/2510.15698)
*Sebastian Brandt,Tim Göttlicher*

Main category: cs.DC

TL;DR: 该论文在分布式量子计算领域研究了Lovász局部引理问题，证明了量子LOCAL模型中分布式LLL的复杂度下界为2^Ω(log* n)，这是该问题在多个模型中的首个超常数下界。


<details>
  <summary>Details</summary>
Motivation: 研究分布式量子计算中的Lovász局部引理问题，解决近期提出的开放性问题，为分布式量子算法提供理论下界分析。

Method: 通过研究LLL的特殊情况sinkless orientation问题，在比量子LOCAL更强的随机在线LOCAL模型中建立下界，并开发了新的下界证明技术。

Result: 证明了分布式LLL和sinkless orientation在量子LOCAL模型中的复杂度下界为2^Ω(log* n)，这是该问题在多个模型中的首个超常数下界。

Conclusion: 该工作不仅解决了开放性问题，还开发了可能成为后量子下界证明的通用技术，对分布式量子计算的理论研究具有重要意义。

Abstract: In this work, we study the Lov\'asz local lemma (LLL) problem in the area of
distributed quantum computing, which has been the focus of attention of recent
advances in quantum computing [STOC'24, STOC'25, STOC'25]. We prove a lower
bound of $2^{\Omega(\log^* n)}$ for the complexity of the distributed LLL in
the quantum-LOCAL model. More specifically, we obtain our lower bound already
for a very well-studied special case of the LLL, called sinkless orientation,
in a stronger model than quantum-LOCAL, called the randomized online-LOCAL
model. As a consequence, we obtain the same lower bounds for sinkless
orientation and the distributed LLL also in a variety of other models studied
across different research communities.
  Our work provides the first superconstant lower bound for sinkless
orientation and the distributed LLL in all of these models, addressing recently
stated open questions. Moreover, to obtain our results, we develop an entirely
new lower bound technique that we believe has the potential to become the first
generic technique for proving post-quantum lower bounds for many of the most
important problems studied in the context of locality.

</details>


### [15] [Funky: Cloud-Native FPGA Virtualization and Orchestration](https://arxiv.org/abs/2510.15755)
*Atsushi Koshiba,Charalampos Mainas,Pramod Bhatotia*

Main category: cs.DC

TL;DR: Funky是一个面向云原生应用的FPGA感知编排引擎，通过FPGA虚拟化、状态管理和编排组件解决了FPGA在云环境中的编排难题。


<details>
  <summary>Details</summary>
Motivation: FPGA在云原生环境中的采用面临障碍，因为FPGA缺乏虚拟化、隔离和抢占支持，导致云提供商无法提供FPGA编排服务，造成低可扩展性、灵活性和弹性。

Method: Funky通过三个主要贡献实现：(1) FPGA虚拟化创建轻量级沙箱；(2) FPGA状态管理支持任务抢占和检查点；(3) 遵循行业标准CRI/OCI规范的FPGA感知编排组件。

Result: 在4台x86服务器上评估显示：Funky只需修改3.4%源代码即可移植23个OpenCL应用，OCI镜像比AMD的FPGA Docker容器小28.7倍，性能开销仅7.4%，同时提供强隔离和分布式FPGA编排。

Conclusion: Funky展示了在大型集群中的可扩展性、容错性和调度效率，为云原生环境中的FPGA编排提供了完整解决方案。

Abstract: The adoption of FPGAs in cloud-native environments is facing impediments due
to FPGA limitations and CPU-oriented design of orchestrators, as they lack
virtualization, isolation, and preemption support for FPGAs. Consequently,
cloud providers offer no orchestration services for FPGAs, leading to low
scalability, flexibility, and resiliency.
  This paper presents Funky, a full-stack FPGA-aware orchestration engine for
cloud-native applications. Funky offers primary orchestration services for FPGA
workloads to achieve high performance, utilization, scalability, and fault
tolerance, accomplished by three contributions: (1) FPGA virtualization for
lightweight sandboxes, (2) FPGA state management enabling task preemption and
checkpointing, and (3) FPGA-aware orchestration components following the
industry-standard CRI/OCI specifications.
  We implement and evaluate Funky using four x86 servers with Alveo U50 FPGA
cards. Our evaluation highlights that Funky allows us to port 23 OpenCL
applications from the Xilinx Vitis and Rosetta benchmark suites by modifying
3.4% of the source code while keeping the OCI image sizes 28.7 times smaller
than AMD's FPGA-accessible Docker containers. In addition, Funky incurs only
7.4% performance overheads compared to native execution, while providing
virtualization support with strong hypervisor-enforced isolation and
cloud-native orchestration for a set of distributed FPGAs. Lastly, we evaluate
Funky's orchestration services in a large-scale cluster using Google production
traces, showing its scalability, fault tolerance, and scheduling efficiency.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [16] [Cleaning up the Mess](https://arxiv.org/abs/2510.15744)
*Haocong Luo,Ataberk Olgun,Maria Makeenkova,F. Nisa Bostanci,Geraldo F. Oliveira,A. Giray Yaglikci,Onur Mutlu*

Main category: cs.AR

TL;DR: 本文指出MICRO 2024最佳论文亚军（Mess论文）中关于Ramulator 2.0模拟器的结果存在错误且不可复现，通过正确配置后显示该模拟器能准确反映真实系统特性，从而反驳了Mess论文的关键贡献。


<details>
  <summary>Details</summary>
Motivation: 纠正Mess论文中关于Ramulator 2.0模拟器的错误结果，防止不准确和误导性结果的传播，维护科学记录的可靠性。

Method: 通过重新配置和使用Ramulator 2.0模拟器，对比Mess论文中的错误配置，验证模拟结果的准确性。

Result: 发现Mess论文在Ramulator 2.0的配置和使用上存在多个简单人为错误，正确配置后模拟结果与真实系统特性相符；同时发现DAMOV模拟结果使用了错误的统计指标。

Conclusion: 强调仔细验证模拟结果的重要性，建议与模拟器开发者沟通确保正确使用，呼吁社区纠正Mess论文的错误，并质疑评审和制品评估过程的完整性。

Abstract: A MICRO 2024 best paper runner-up publication (the Mess paper) with all three
artifact badges awarded (including "Reproducible") proposes a new benchmark to
evaluate real and simulated memory system performance. In this paper, we
demonstrate that the Ramulator 2.0 simulation results reported in the Mess
paper are incorrect and, at the time of the publication of the Mess paper,
irreproducible. We find that the authors of Mess paper made multiple trivial
human errors in both the configuration and usage of the simulators. We show
that by correctly configuring Ramulator 2.0, Ramulator 2.0's simulated memory
system performance actually resembles real system characteristics well, and
thus a key claimed contribution of the Mess paper is factually incorrect. We
also identify that the DAMOV simulation results in the Mess paper use wrong
simulation statistics that are unrelated to the simulated DRAM performance.
Moreover, the Mess paper's artifact repository lacks the necessary sources to
fully reproduce all the Mess paper's results.
  Our work corrects the Mess paper's errors regarding Ramulator 2.0 and
identifies important issues in the Mess paper's memory simulator evaluation
methodology. We emphasize the importance of both carefully and rigorously
validating simulation results and contacting simulator authors and developers,
in true open source spirit, to ensure these simulators are used with correct
configurations and as intended. We encourage the computer architecture
community to correct the Mess paper's errors. This is necessary to prevent the
propagation of inaccurate and misleading results, and to maintain the
reliability of the scientific record. Our investigation also opens up questions
about the integrity of the review and artifact evaluation processes. To aid
future work, our source code and scripts are openly available at https:
//github.com/CMU-SAFARI/ramulator2/tree/mess.

</details>
