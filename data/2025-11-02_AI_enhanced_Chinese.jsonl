{"id": "2510.26730", "categories": ["cs.DC", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.26730", "abs": "https://arxiv.org/abs/2510.26730", "authors": ["Zixu Shen", "Kexin Chu", "Yifan Zhang", "Dawei Xiang", "Runxin Wu", "Wei Zhang"], "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference", "comment": "12 pages, 11 figures", "summary": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints.", "AI": {"tldr": "ExpertFlow\u662f\u4e00\u4e2a\u7528\u4e8eMoE\u63a8\u7406\u7684\u8fd0\u884c\u65f6\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4e13\u5bb6\u9884\u53d6\u548c\u7f13\u5b58\u611f\u77e5\u8def\u7531\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMoE\u63a8\u7406\u4e2d\u53c2\u6570\u9891\u7e41\u4f20\u8f93\u5bfc\u81f4\u7684\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3GPU\u5185\u5b58\u5bb9\u91cf\u6709\u9650\u9650\u5236\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6269\u5c55\uff0c\u4f20\u7edfMoE\u63a8\u7406\u65b9\u6cd5\u56e0\u6bcf\u5c42\u72ec\u7acb\u9009\u62e9\u6d3b\u8dc3\u4e13\u5bb6\u800c\u5f15\u5165\u663e\u8457\u5ef6\u8fdf\uff0c\u4e14\u73b0\u6709\u7684\u8de8\u5c42\u9884\u6d4b\u7b56\u7565\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u548c\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9002\u5e94\u6027\u3002", "method": "\u7ed3\u5408\u81ea\u9002\u5e94\u4e13\u5bb6\u9884\u53d6\u548c\u7f13\u5b58\u611f\u77e5\u8def\u7531\uff0c\u5229\u7528\u4f20\u8f93\u5e26\u5bbd\u3001\u53c2\u6570\u7ef4\u5ea6\u548c\u6a21\u578b\u53cd\u9988\u4fe1\u53f7\u7b49\u8fd0\u884c\u65f6\u7edf\u8ba1\u4fe1\u606f\u6301\u7eed\u8c03\u6574\u9884\u6d4b\u8303\u56f4\uff0c\u91c7\u7528\u6df7\u5408\u8de8\u5c42\u9884\u6d4b\u65b9\u6848\u878d\u5408\u9884\u95e8\u63a7\u4fe1\u606f\u548c\u4e2d\u95f4\u8ba1\u7b97\u72b6\u6001\u6765\u9884\u6d4b\u672a\u6765\u4e13\u5bb6\u9700\u6c42\u3002", "result": "\u8bc4\u4f30\u663e\u793aExpertFlow\u5c06\u6a21\u578b\u505c\u987f\u65f6\u95f4\u51cf\u5c11\u5230\u57fa\u7ebf\u76840.1%\u4ee5\u4e0b\uff0c\u663e\u8457\u4f18\u5316\u4e86\u4e25\u683c\u5185\u5b58\u7ea6\u675f\u4e0b\u7684MoE\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "ExpertFlow\u901a\u8fc7\u81ea\u9002\u5e94\u9884\u53d6\u51b3\u7b56\u548c\u4e0e\u5b9e\u9645\u4f7f\u7528\u884c\u4e3a\u7684\u5bf9\u9f50\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u7f13\u5b58\u672a\u547d\u4e2d\u5e76\u6d88\u9664\u4e86\u4e13\u5bb6\u4ea4\u6362\u5f15\u5165\u7684\u5ef6\u8fdf\uff0c\u4e3a\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u7684MoE\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26016", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2510.26016", "abs": "https://arxiv.org/abs/2510.26016", "authors": ["Michael Arntzenius"], "title": "Fair intersection of seekable iterators", "comment": "8 pages, 2 figures, published in miniKanren 2025", "summary": "miniKanren's key semantic advance over Prolog is to implement a complete yet\nefficient search strategy, fairly interleaving execution between disjuncts.\nThis fairness is accomplished by bounding how much work is done exploring one\ndisjunct before switching to the next. We show that the same idea -- fairness\nvia bounded work -- underlies an elegant compositional approach to implementing\nworst-case optimal joins using a seekable iterator interface, suitable for\nshallow embedding in functional languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u901a\u8fc7\u9650\u5236\u5de5\u4f5c\u91cf\u7684\u516c\u5e73\u6027\u6982\u5ff5\u5982\u4f55\u540c\u65f6\u652f\u6491\u4e86miniKanren\u7684\u641c\u7d22\u7b56\u7565\u548c\u6700\u4f18\u8fde\u63a5\u7b97\u6cd5\u7684\u5b9e\u73b0\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u51fd\u6570\u5f0f\u8bed\u8a00\u4e2d\u4f18\u96c5\u5730\u5b9e\u73b0\u516c\u5e73\u7684\u641c\u7d22\u7b56\u7565\u548c\u6700\u4f18\u8fde\u63a5\u7b97\u6cd5\uff0c\u501f\u9274miniKanren\u7684\u6210\u529f\u7ecf\u9a8c\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5de5\u4f5c\u91cf\u9650\u5236\u7684\u516c\u5e73\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u641c\u7d22\u8fed\u4ee3\u5668\u63a5\u53e3\u5728\u51fd\u6570\u5f0f\u8bed\u8a00\u4e2d\u6d45\u5c42\u5d4c\u5165\u5b9e\u73b0\u6700\u574f\u60c5\u51b5\u6700\u4f18\u8fde\u63a5\u3002", "result": "\u8bc1\u660e\u4e86\u5de5\u4f5c\u91cf\u9650\u5236\u7684\u516c\u5e73\u6027\u6982\u5ff5\u53ef\u4ee5\u7edf\u4e00\u5e94\u7528\u4e8e\u903b\u8f91\u7f16\u7a0b\u7684\u641c\u7d22\u7b56\u7565\u548c\u6570\u636e\u5e93\u8fde\u63a5\u7b97\u6cd5\u4e2d\u3002", "conclusion": "\u516c\u5e73\u6027\u901a\u8fc7\u5de5\u4f5c\u91cf\u9650\u5236\u7684\u6982\u5ff5\u4e3a\u903b\u8f91\u7f16\u7a0b\u548c\u6570\u636e\u5e93\u67e5\u8be2\u5904\u7406\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5b9e\u73b0\u6846\u67b6\u3002"}}
{"id": "2510.25958", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.25958", "abs": "https://arxiv.org/abs/2510.25958", "authors": ["Lukas Pfromm", "Alish Kanani", "Harsh Sharma", "Janardhan Rao Doppa", "Partha Pratim Pande", "Umit Y. Ogras"], "title": "CHIPSIM: A Co-Simulation Framework for Deep Learning on Chiplet-Based Systems", "comment": "Accepted at IEEE Open Journal of the Solid-State Circuits Society", "summary": "Due to reduced manufacturing yields, traditional monolithic chips cannot keep\nup with the compute, memory, and communication demands of data-intensive\napplications, such as rapidly growing deep neural network (DNN) models.\nChiplet-based architectures offer a cost-effective and scalable solution by\nintegrating smaller chiplets via a network-on-interposer (NoI). Fast and\naccurate simulation approaches are critical to unlocking this potential, but\nexisting methods lack the required accuracy, speed, and flexibility. To address\nthis need, this work presents CHIPSIM, a comprehensive co-simulation framework\ndesigned for parallel DNN execution on chiplet-based systems. CHIPSIM\nconcurrently models computation and communication, accurately capturing network\ncontention and pipelining effects that conventional simulators overlook.\nFurthermore, it profiles the chiplet and NoI power consumptions at microsecond\ngranularity for precise transient thermal analysis. Extensive evaluations with\nhomogeneous/heterogeneous chiplets and different NoI architectures demonstrate\nthe framework's versatility, up to 340% accuracy improvement, and power/thermal\nanalysis capability.", "AI": {"tldr": "CHIPSIM\u662f\u4e00\u4e2a\u7528\u4e8e\u5c0f\u82af\u7247\u7cfb\u7edf\u4e0a\u5e76\u884cDNN\u6267\u884c\u7684\u534f\u540c\u4eff\u771f\u6846\u67b6\uff0c\u80fd\u51c6\u786e\u6a21\u62df\u7f51\u7edc\u4e89\u7528\u548c\u6d41\u6c34\u7ebf\u6548\u5e94\uff0c\u5e76\u63d0\u4f9b\u5fae\u79d2\u7ea7\u529f\u8017\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u5355\u7247\u82af\u7247\u7531\u4e8e\u5236\u9020\u826f\u7387\u4e0b\u964d\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\uff08\u5982DNN\u6a21\u578b\uff09\u7684\u8ba1\u7b97\u3001\u5b58\u50a8\u548c\u901a\u4fe1\u9700\u6c42\u3002\u5c0f\u82af\u7247\u67b6\u6784\u901a\u8fc7\u7247\u4e0a\u7f51\u7edc\u96c6\u6210\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u4eff\u771f\u65b9\u6cd5\u7f3a\u4e4f\u6240\u9700\u7684\u7cbe\u5ea6\u3001\u901f\u5ea6\u548c\u7075\u6d3b\u6027\u3002", "method": "\u5f00\u53d1CHIPSIM\u534f\u540c\u4eff\u771f\u6846\u67b6\uff0c\u5e76\u884c\u6a21\u62df\u8ba1\u7b97\u548c\u901a\u4fe1\uff0c\u51c6\u786e\u6355\u6349\u7f51\u7edc\u4e89\u7528\u548c\u6d41\u6c34\u7ebf\u6548\u5e94\uff0c\u5e76\u4ee5\u5fae\u79d2\u7ea7\u7c92\u5ea6\u5206\u6790\u5c0f\u82af\u7247\u548c\u7247\u4e0a\u7f51\u7edc\u7684\u529f\u8017\u3002", "result": "\u901a\u8fc7\u540c\u6784/\u5f02\u6784\u5c0f\u82af\u7247\u548c\u4e0d\u540c\u7247\u4e0a\u7f51\u7edc\u67b6\u6784\u7684\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u8bc1\u660e\u8be5\u6846\u67b6\u5177\u6709\u591a\u529f\u80fd\u6027\uff0c\u7cbe\u5ea6\u63d0\u5347\u9ad8\u8fbe340%\uff0c\u5e76\u5177\u5907\u529f\u8017/\u70ed\u5206\u6790\u80fd\u529b\u3002", "conclusion": "CHIPSIM\u4e3a\u5c0f\u82af\u7247\u7cfb\u7edf\u4e0a\u7684DNN\u6267\u884c\u63d0\u4f9b\u4e86\u5feb\u901f\u51c6\u786e\u7684\u4eff\u771f\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u652f\u6301\u7cbe\u786e\u7684\u77ac\u6001\u70ed\u5206\u6790\u3002"}}
{"id": "2510.26463", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.26463", "abs": "https://arxiv.org/abs/2510.26463", "authors": ["Xiaolin He", "Cenlin Duan", "Yingjie Qi", "Xiao Ma", "Jianlei Yang"], "title": "MIREDO: MIP-Driven Resource-Efficient Dataflow Optimization for Computing-in-Memory Accelerator", "comment": "7 pages, accepted by ASP-DAC 2026", "summary": "Computing-in-Memory (CIM) architectures have emerged as a promising solution\nfor accelerating Deep Neural Networks (DNNs) by mitigating data movement\nbottlenecks. However, realizing the potential of CIM requires specialized\ndataflow optimizations, which are challenged by an expansive design space and\nstrict architectural constraints. Existing optimization approaches often fail\nto fully exploit CIM accelerators, leading to noticeable gaps between\ntheoretical and actual system-level efficiency. To address these limitations,\nwe propose the MIREDO framework, which formulates dataflow optimization as a\nMixed-Integer Programming (MIP) problem. MIREDO introduces a hierarchical\nhardware abstraction coupled with an analytical latency model designed to\naccurately reflect the complex data transfer behaviors within CIM systems. By\njointly modeling workload characteristics, dataflow strategies, and\nCIM-specific constraints, MIREDO systematically navigates the vast design space\nto determine the optimal dataflow configurations. Evaluation results\ndemonstrate that MIREDO significantly enhances performance, achieving up to\n$3.2\\times$ improvement across various DNN models and hardware setups.", "AI": {"tldr": "MIREDO\u6846\u67b6\u901a\u8fc7\u5c06\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\u7684\u6570\u636e\u6d41\u4f18\u5316\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u89c4\u5212\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728CIM\u52a0\u901f\u5668\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\u867d\u7136\u80fd\u7f13\u89e3\u6570\u636e\u79fb\u52a8\u74f6\u9888\uff0c\u4f46\u7531\u4e8e\u8bbe\u8ba1\u7a7a\u95f4\u5e9e\u5927\u548c\u67b6\u6784\u7ea6\u675f\u4e25\u683c\uff0c\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u53d1\u6325CIM\u52a0\u901f\u5668\u7684\u6f5c\u529b\uff0c\u5bfc\u81f4\u7406\u8bba\u6548\u7387\u4e0e\u5b9e\u9645\u7cfb\u7edf\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u5dee\u8ddd\u3002", "method": "\u63d0\u51faMIREDO\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u786c\u4ef6\u62bd\u8c61\u548c\u5206\u6790\u5ef6\u8fdf\u6a21\u578b\uff0c\u51c6\u786e\u53cd\u6620CIM\u7cfb\u7edf\u5185\u590d\u6742\u7684\u6570\u636e\u4f20\u8f93\u884c\u4e3a\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\u3001\u6570\u636e\u6d41\u7b56\u7565\u548cCIM\u7279\u5b9a\u7ea6\u675f\uff0c\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\u4ee5\u786e\u5b9a\u6700\u4f18\u6570\u636e\u6d41\u914d\u7f6e\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cMIREDO\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5728\u5404\u79cdDNN\u6a21\u578b\u548c\u786c\u4ef6\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u9ad83.2\u500d\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "MIREDO\u6846\u67b6\u901a\u8fc7\u5c06\u6570\u636e\u6d41\u4f18\u5316\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u6df7\u5408\u6574\u6570\u89c4\u5212\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3CIM\u67b6\u6784\u4e2d\u7684\u6570\u636e\u6d41\u4f18\u5316\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u7ea7\u6548\u7387\u3002"}}
