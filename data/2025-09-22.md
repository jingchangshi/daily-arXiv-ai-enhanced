<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Automatic layout of railroad diagrams](https://arxiv.org/abs/2509.15834)
*Shardul Chiplunkar,Clément Pit-Claudel*

Main category: cs.PL

TL;DR: 本文提出了铁路图布局的第一个形式化处理方法，包括一个原则性的实用实现，将图语言编译为布局语言，支持自动换行、对齐和调整等优化功能。


<details>
  <summary>Details</summary>
Motivation: 铁路图是常见的语法可视化工具，但由于缺乏形式化布局方法和工具支持，目前主要局限于手绘文档。

Method: 将问题定义为从图语言（指定概念组件及其连接组合方式）到布局语言（指定基本图形形状及其尺寸位置）的编译过程，实现包含换行、对齐和调整策略的编译器。

Result: 通过将正则表达式和巴科斯范式编译到图语言，验证了方法的适用性；通过与其他工具和手绘图的比较，证明了编译器的实用性。

Conclusion: 该研究为铁路图布局提供了首个形式化框架和实用实现，解决了现有工具不足的问题，为语法可视化提供了更好的支持。

Abstract: Railroad diagrams (also called "syntax diagrams") are a common, intuitive
visualization of grammars, but limited tooling and a lack of formal attention
to their layout mostly confines them to hand-drawn documentation. We present
the first formal treatment of railroad diagram layout along with a principled,
practical implementation. We characterize the problem as compiling a *diagram
language* (specifying conceptual components and how they connect and compose)
to a *layout language* (specifying basic graphical shapes and their sizes and
positions). We then implement a compiler that performs *line wrapping* to meet
a target width, as well as vertical *alignment* and horizontal *justification*
per user-specified policies. We frame line wrapping as an optimization problem,
where we describe principled dimensions of optimality and implement
corresponding heuristics. For front-end evaluation, we show that our diagram
language is well-suited for common applications by describing how regular
expressions and Backus-Naur form can be compiled to it. For back-end
evaluation, we argue that our compiler is practical by comparing its output to
diagrams laid out by hand and by other tools.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [PCCL: Photonic circuit-switched collective communication for distributed ML](https://arxiv.org/abs/2509.15450)
*Abhishek Vijaya Kumar,Arjun Devraj,Rachee Singh*

Main category: cs.DC

TL;DR: PCCL是一个光子集体通信库，通过重新配置网络拓扑来匹配集体通信算法的通信模式，消除拥塞和延迟，在128个GPU上实现3倍加速。


<details>
  <summary>Details</summary>
Motivation: 现代分布式ML存在理论性能与实际性能之间的差距，主要由于GPU集群中的拥塞和跳数导致的延迟问题。

Method: PCCL采用硬件无关的优化框架，智能决策何时重新配置网络，权衡网络重配置延迟与拥塞/延迟成本，为不同集体算法创建直接、无争用的电路。

Result: 在128个GPU上的评估显示，PCCL在各种工作负载、缓冲区大小和拓扑结构下比最先进算法快3倍，端到端训练吞吐量提升1.3倍。

Conclusion: PCCL通过动态网络重配置有效解决了分布式ML中的通信瓶颈，为不同光学硬件提供了实用的解决方案。

Abstract: Modern distributed ML suffers from a fundamental gap between the theoretical
and realized performance of collective communication algorithms due to
congestion and hop-count induced dilation in practical GPU clusters. We present
PCCL, a Photonic Collective Communication Library that reconfigures the network
topology to match the communication patterns of collective algorithms, thereby
eliminating congestion and dilation by creating direct, contention-free
circuits between communicating GPUs. Unlike prior approaches that synthesize
algorithms for specific network topologies and collectives, PCCL generalizes to
any collective primitive and any topology by adapting the network to match each
algorithm's communication pattern. PCCL's key innovation lies in its
hardware-agnostic optimization framework that intelligently decides when to
reconfigure based on the trade-off between network reconfiguration delay and
congestion/dilation costs, making it practical across different optical
hardware with varying switching speeds. Our evaluation demonstrates that PCCL
achieves up to 3X speedup over state-of-the-art algorithms on 128 GPUs across
various workloads, buffer sizes, and topologies, translating to a 1.3X speedup
in end-to-end training throughput.

</details>


### [3] [Angelfish: Consensus with Optimal Throughput and Latency Across the Leader-DAG Spectrum](https://arxiv.org/abs/2509.15847)
*Qianyu Yu,Giuliano Losa,Nibesh Shrestha,Xuechao Wang*

Main category: cs.DC

TL;DR: Angelfish是一种混合共识协议，结合了基于领导者和基于DAG的共识方法，在保持高吞吐量的同时实现了低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有区块链共识协议在延迟和吞吐量之间存在权衡：基于领导者的协议延迟低但吞吐量有限，基于DAG的协议吞吐量高但延迟较高。需要一种能同时优化两者的解决方案。

Method: 使用动态调整的节点子集通过最佳努力广播发送轻量级投票，而不是可靠广播代价更高的DAG顶点。结合了领导者的低延迟优势和DAG的高吞吐量特性。

Result: 实验评估显示Angelfish达到了最先进的峰值吞吐量，同时在中等吞吐量下匹配了基于领导者协议的延迟性能。

Conclusion: Angelfish成功实现了两种设计方法的优势结合，在区块链共识协议设计空间上提供了平滑的自适应能力。

Abstract: To maximize performance, many modern blockchain systems rely on
eventually-synchronous, Byzantine fault-tolerant (BFT) consensus protocols. Two
protocol designs have emerged in this space: protocols that minimize latency
using a leader that drives both data dissemination and consensus, and protocols
that maximize throughput using a separate, asynchronous data dissemination
layer. Recent protocols such as Partially-Synchronous Bullshark and Sailfish
combine elements of both approaches by using a DAG to enable parallel data
dissemination and a leader that paces DAG formation. This improves latency
while achieving state-of-the-art throughput. Yet the latency of leader-based
protocols is still better under moderate loads.
  We present Angelfish, a hybrid protocol that adapts smoothly across this
design space, from leader-based to Sailfish-like DAG-based consensus. Angelfish
lets a dynamically-adjusted subset of parties use best-effort broadcast to
issue lightweight votes instead of reliably broadcasting costlier DAG vertices.
This reduces communication, helps lagging nodes catch up, and lowers latency in
practice compared to prior DAG-based protocols. Our empirical evaluation shows
that Angelfish attains state-of-the-art peak throughput while matching the
latency of leader-based protocols under moderate throughput, delivering the
best of both worlds.

</details>


### [4] [Efficient Pre-Training of LLMs via Topology-Aware Communication Alignment on More Than 9600 GPUs](https://arxiv.org/abs/2509.15940)
*Guoliang He,Youhe Jiang,Wencong Xiao,Kaihua Jiang,Shuguang Wang,Jun Wang,Zixian Du,Zhuo Jiang,Xinlei Zhang,Binhang Yuan,Eiko Yoneki*

Main category: cs.DC

TL;DR: 本文提出了Arnold调度系统，通过将LLM预训练作业的通信模式与数据中心物理网络拓扑对齐，解决大规模GPU集群中的带宽争用问题，提升训练性能。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型预训练需要数千个计算节点，但复杂的通信模式（稀疏但高容量的突发数据传输）会导致带宽争用，造成训练性能下降。

Method: 进行深入的特性研究分析物理网络拓扑对LLM预训练的影响，开发调度算法将通信模式与数据中心拓扑对齐，通过仿真实验验证算法有效性。

Result: 仿真实验显示通信组最大传播范围减少1.67倍，生产环境中9600+GPU训练时端到端性能提升10.6%。

Conclusion: Arnold调度系统能有效优化LLM预训练性能，为大模型训练提供了实用的调度解决方案。

Abstract: The scaling law for large language models (LLMs) depicts that the path
towards machine intelligence necessitates training at large scale. Thus,
companies continuously build large-scale GPU clusters, and launch training jobs
that span over thousands of computing nodes. However, LLM pre-training presents
unique challenges due to its complex communication patterns, where GPUs
exchange data in sparse yet high-volume bursts within specific groups.
Inefficient resource scheduling exacerbates bandwidth contention, leading to
suboptimal training performance. This paper presents Arnold, a scheduling
system summarizing our experience to effectively align LLM communication
patterns with data center topology at scale. An in-depth characteristic study
is performed to identify the impact of physical network topology to LLM
pre-training jobs. Based on the insights, we develop a scheduling algorithm to
effectively align communication patterns with the physical network topology in
modern data centers. Through simulation experiments, we show the effectiveness
of our algorithm in reducing the maximum spread of communication groups by up
to $1.67$x. In production training, our scheduling system improves the
end-to-end performance by $10.6\%$ when training with more than $9600$ GPUs, a
significant improvement for our training pipeline.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [5] [Automatic Microarchitecture-Aware Custom Instruction Design for RISC-V Processors](https://arxiv.org/abs/2509.15782)
*Evgenii Rezunov,Niko Zurstraßen,Lennart M. Reimann,Rainer Leupers*

Main category: cs.AR

TL;DR: CIDRE是一个自动化工具，用于RISC-V ASIP设计，能自动分析应用热点并生成自定义指令建议，实现最高2.47倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: ASIP设计目前主要依赖手动工作，缺乏自动化工具来高效寻找平衡性能与成本的定制指令。

Method: 开发CIDRE工具，自动分析RISC-V应用热点，生成自定义指令建议和对应的nML描述，可与其他EDA工具集成评估成本效益。

Result: 在RISC-V基准测试中，Embench和MiBench嵌入式基准性能提升最高达2.47倍，面积增加小于24%，整个过程完全自动化。

Conclusion: CIDRE成功实现了ASIP设计的自动化，显著提高了设计效率，为RISC-V处理器定制化提供了有效解决方案。

Abstract: An Application-Specific Instruction Set Processor(ASIP) is a specialized
microprocessor that provides a trade-off between the programmability of a
General Purpose Processor (GPP) and the performance and energy-efficiency of
dedicated hardware accelerators. ASIPs are often derived from off-the-shelf
GPPs extended by custom instructions tailored towards a specific software
workload. One of the most important challenges of designing an ASIP is to find
said custom instructions that help to increase performance without being too
costly in terms of area and power consumption. To date, solving this challenge
is relatively labor-intensive and typically performed manually. Addressing the
lack of automation, we present Custom Instruction Designer for RISC-V
Extensions (CIDRE), a front-to-back tool for ASIP design. CIDRE automatically
analyzes hotspots in RISC-V applications and generates custom instruction
suggestions with a corresponding nML description. The nML description can be
used with other electronic design automation tools to accurately assess the
cost and benefits of the found suggestions. In a RISC-V benchmark study, we
were able to accelerate embedded benchmarks from Embench and MiBench by up to
2.47x with less than 24% area increase. The entire process was conducted
completely automatically.

</details>
