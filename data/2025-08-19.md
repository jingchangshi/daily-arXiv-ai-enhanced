<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 10]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [StackPilot: Autonomous Function Agents for Scalable and Environment-Free Code Execution](https://arxiv.org/abs/2508.11665)
*Xinkui Zhao,Yifan Zhang,Zhengyi Zhou,Yueshen Xu*

Main category: cs.PL

TL;DR: StackPilot是一个基于LLM的多智能体框架，用于语言无关的代码验证和执行，无需传统工具链，通过函数即智能体、LLM作为执行器和快照机制实现高可靠性验证。


<details>
  <summary>Details</summary>
Motivation: 传统代码验证方法依赖语言特定编译器和环境相关运行时，无法有效验证LLM生成代码的正确性和可执行性，需要语言无关的验证解决方案。

Method: 采用Function-as-Agents范式将每个函数建模为自主智能体，使用LLM-as-Executor策略进行基于堆栈的调度验证，并引入快照机制保存完整执行上下文。

Result: 实证评估显示StackPilot框架可靠性达到89%-97%，显著优于基线方法，能够验证和执行更多LLM生成代码。

Conclusion: StackPilot提供了一种有效的语言无关代码验证方法，显著提高了LLM生成代码的验证可靠性和执行成功率。

Abstract: Recent advances in large language models (LLMs) have substantially enhanced
automated code generation across a wide range of programming languages.
Nonetheless, verifying the correctness and executability of LLM-generated code
remains a significant challenge, as traditional methods rely on
language-specific compilers and environment-dependent runtimes. To overcome
these limitations, we introduce StackPilot, an LLM-native, multi-agent
framework designed for language-agnostic code verification and execution, which
operates independently of conventional toolchains. StackPilot offers three
principal innovations: (1) a Function-as-Agents paradigm, in which each
function is modeled as an autonomous agent capable of fine-grained reasoning
and collaborative verification; (2) an LLM-as-Executor strategy, which enables
scalable verification via stack-based scheduling; and (3) a novel snapshot
mechanism that preserves complete execution contexts, facilitating
deterministic and lossless context switching during verification. Empirical
evaluations demonstrate that StackPilot achieves framework reliability rates
between 89% and 97%, substantially outperforming baseline approaches. These
results indicate that StackPilot can reliably verify and execute a
significantly larger proportion of LLM-generated code across diverse
programming tasks compared to existing methods.

</details>


### [2] [Certified Compilation based on Gödel Numbers](https://arxiv.org/abs/2508.12054)
*Guilherme de Oliveira Silva,Fernando Magno Quintão Pereira*

Main category: cs.PL

TL;DR: 这篇论文提出了一种新的证书生成方法，用于确保二进制文件实际上代表源代码，以防范Thompson类型的编译器后门攻击。


<details>
  <summary>Details</summary>
Motivation: 解决Thompson后门攻击的根本问题：如何信任用于编译工具的编译器。现有的多重编译、翻译验证等方法都无法彻底解决这个信任问题。

Method: 设计了一种证书生成方法，通过整数形式的证书来保证二进制文件包含源代码的所有语句、保持语句顺序、维护等价的def-use依赖关系。证书可以通过简洁的求导规则从源代码和二进制文件中派生，每个步骤都在常数时间内完成。

Result: 实现了Charon编译器，能够处理足够编译FaCT（灵活且常数时间的加密编程语言）的C语言子集，证明了该方法的实用性。

Conclusion: 该方法为解决Thompson后门攻击提供了一种新的视角，通过数学化的证书验证来保证编译过程的可信质，从根本上解决了编译器信任问题。

Abstract: In his 1984 Turing Award lecture, Ken Thompson showed that a compiler could
be maliciously altered to insert backdoors into programs it compiles and
perpetuate this behavior by modifying any compiler it subsequently builds.
Thompson's hack has been reproduced in real-world systems for demonstration
purposes. Several countermeasures have been proposed to defend against
Thompson-style backdoors, including the well-known {\it Diverse
Double-Compiling} (DDC) technique, as well as methods like translation
validation and CompCert-style compilation. However, these approaches ultimately
circle back to the fundamental question: "How can we trust the compiler used to
compile the tools we rely on?" In this paper, we introduce a novel approach to
generating certificates to guarantee that a binary image faithfully represents
the source code. These certificates ensure that the binary contains all and
only the statements from the source code, preserves their order, and maintains
equivalent def-use dependencies. The certificate is represented as an integer
derivable from both the source code and the binary using a concise set of
derivation rules, each applied in constant time. To demonstrate the
practicality of our method, we present Charon, a compiler designed to handle a
subset of C expressive enough to compile FaCT, the Flexible and Constant Time
cryptographic programming language.

</details>


### [3] [Controlling Copatterns: There and Back Again (Extended Version)](https://arxiv.org/abs/2508.12427)
*Paul Downen*

Main category: cs.PL

TL;DR: 通过Danvy的功能和语法对应关系，为copatterns汇编了一套完整的语义定义，包括从小步驱动操作语义到抽象机器再到续体传递风格，然后通过重构得到更一般化的组合式copatterns计算法


<details>
  <summary>Details</summary>
Motivation: Copatterns为函数式程序提供了灵活的上下文响应机制，组合性大大提高了表达力，但这种表达力也使得精确规范程序行为更加困难

Method: 利用Danvy的功能和语法对应关系，先从单一的copatterns计算法出发，从小步操作语义到抽象机器再到续体传递风格，然后在续体传递风格中重构语义，得到更一般化的组合式copatterns计算法，最后反向汇编回其他语义实体

Result: 得到了一套完整的copatterns语义定义，包括了单一和组合式的两种计算法，以及对应的操作语义、抽象机器和续体传递风格

Conclusion: 通过Danvy的对应关系方法，成功为copatterns汇编了一套完整且一致的语义定义，解决了其高度表达力带来的规范困难，为函数式程序设计提供了坚实的理论基础

Abstract: Copatterns give functional programs a flexible mechanism for responding to
their context, and composition can greatly enhance their expressiveness.
However, that same expressive power makes it harder to precisely specify the
behavior of programs. Using Danvy's functional and syntactic correspondence
between different semantic artifacts, we derive a full suite of semantics for
copatterns, twice. First, a calculus of monolithic copatterns is taken on a
journey from small-step operational semantics to abstract machine to
continuation-passing style. Then within continuation-passing style, we refactor
the semantics to derive a more general calculus of compositional copatterns,
and take the return journey back to derive the other semantic artifacts in
reverse order.

</details>


### [4] [Type-Driven Prompt Programming: From Typed Interfaces to a Calculus of Constraints](https://arxiv.org/abs/2508.12475)
*Abhijit Paul*

Main category: cs.PL

TL;DR: Prompt Programming将语言模型提示视为具有类型接口的软件组件，基于2023-2025年15篇文献调查发现类型系统是新兴框架的核心，但存在约束表达能力和算法支持的不足。


<details>
  <summary>Details</summary>
Motivation: 现有提示编程框架在约束表达能力和算法支持方面存在不足，需要建立类型理论基础来解决这些问题。

Method: 引入Lambda Prompt概念——一个具有概率精化的依赖类型演算，用于句法和语义约束，提出约束保持优化规则，并规划提示程序编译器研究方向。

Result: 识别出13种约束类型，其中约束9-13是未被充分探索的领域，提出了类型理论基础和优化方法。

Conclusion: 为提示编程建立了类型理论基础，指出了约束表达和算法支持的研究方向，为开发提示程序编译器奠定了基础。

Abstract: Prompt programming treats large language model prompts as software components
with typed interfaces. Based on a literature survey of 15 recent works from
2023 to 2025, we observe a consistent trend: type systems are central to
emerging prompt programming frameworks. However, there are gaps in constraint
expressiveness and in supporting algorithms. To address these issues, we
introduce the notion of Lambda Prompt, a dependently typed calculus with
probabilistic refinements for syntactic and semantic constraints. While this is
not yet a full calculus, the formulation motivates a type-theoretic foundation
for prompt programming. Our catalog of 13 constraints highlights underexplored
areas in constraint expressiveness (constraints 9 through 13). To address the
algorithmic gap, we propose a constraint-preserving optimization rule. Finally,
we outline research directions on developing a compiler for prompt programs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Proceedings 18th Interaction and Concurrency Experience](https://arxiv.org/abs/2508.12308)
*Clément Aubert,Cinzia Di Giusto,Simon Fowler,Violet Ka I Pun*

Main category: cs.DC

TL;DR: ICE'25会议论文集，包含18届Interaction and Concurrency Experience的4篇论文、1个口头报告和1个特邀报告，采用独特的匿名互动评审机制


<details>
  <summary>Details</summary>
Motivation: 促进并发与交互领域的研究交流，通过创新的匿名互动评审机制提升论文质量和学术讨论

Method: 采用PC成员与作者匿名互动的评审流程，每篇论文由3名PC成员评审，共交换约75条评论

Result: 收到7篇投稿，接受4篇论文发表和1个口头报告，包含Kirstin Peters的特邀报告

Conclusion: ICE会议通过创新的互动评审机制成功促进了学术交流，论文集收录了经过充分讨论和修改的最终版本论文

Abstract: This volume contains the proceedings of ICE'25, the 18th Interaction and
Concurrency Experience, which was held on Friday 20th June 2025 at the \'Ecole
National Sup\'erieure des Arts et M\'etiers in Lille, France, as a satellite
workshop of DisCoTec 2025. The ICE workshop series features a distinguishing
review and selection procedure: PC members are encouraged to interact,
anonymously, with authors. The 2025 edition of ICE received 7 submissions, each
reviewed by three PC members, and about 75 comments were exchanged during the
review process, witnessing very lively discussions. Four papers were accepted
for publication plus 1 oral communication, which was accepted for presentation
at the workshop. We were proud to host one invited talk, by Kirstin Peters. The
abstract of her talk is included in this volume, together with the final
versions of the research papers, which take into account the discussion at the
workshop and during the review process.

</details>


### [6] [Breaking the Aggregation Bottleneck in Federated Recommendation: A Personalized Model Merging Approach](https://arxiv.org/abs/2508.12386)
*Jundong Chen,Honglei Zhang,Chunxu Zhang,Fangyuan Luo,Yidong Li*

Main category: cs.DC

TL;DR: FedEM通过弹性融合全局和局部模型来解决联邦推荐中的聚合瓶颈问题，在保持隐私的同时提升个性化推荐性能


<details>
  <summary>Details</summary>
Motivation: 发现联邦推荐中服务器端聚合会削弱客户端个性化，导致次优性能（聚合瓶颈问题），这源于客户端间的异构性使全局模型偏离局部最优

Method: 提出FedEM方法，弹性合并全局和局部模型来补偿受损的个性化，利用现成的局部模型而非设计额外机制

Result: 在真实数据集上的大量实验表明，该方法在协作训练过程中保持了客户端个性化，优于最先进的基线方法

Conclusion: FedEM通过理论驱动的弹性融合方法有效解决了联邦推荐中的聚合瓶颈问题，为个性化联邦推荐提供了新的解决方案

Abstract: Federated recommendation (FR) facilitates collaborative training by
aggregating local models from massive devices, enabling client-specific
personalization while ensuring privacy. However, we empirically and
theoretically demonstrate that server-side aggregation can undermine
client-side personalization, leading to suboptimal performance, which we term
the aggregation bottleneck. This issue stems from the inherent heterogeneity
across numerous clients in FR, which drives the globally aggregated model to
deviate from local optima. To this end, we propose FedEM, which elastically
merges the global and local models to compensate for impaired personalization.
Unlike existing personalized federated recommendation (pFR) methods, FedEM (1)
investigates the aggregation bottleneck in FR through theoretical insights,
rather than relying on heuristic analysis; (2) leverages off-the-shelf local
models rather than designing additional mechanisms to boost personalization.
Extensive experiments on real-world datasets demonstrate that our method
preserves client personalization during collaborative training, outperforming
state-of-the-art baselines.

</details>


### [7] [DIT: Dimension Reduction View on Optimal NFT Rarity Meters](https://arxiv.org/abs/2508.12671)
*Dmitry Belousov,Yury Yanovich*

Main category: cs.DC

TL;DR: 本文提出了基于维度缩减的NFT稀有度评估新方法DIT，在ROAR基准测试中表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: NFT稀有度评估缺乏标准化比较框架，现有稀有度计量器难以直接对比，需要统一的评估基准和方法改进

Method: 采用非度量加权多维尺度分析进行最优稀有度计量器设计，引入基于维度缩减技术的DIT（交易差异度）作为性能衡量指标

Result: 开发的DIT稀有度计量器在ROAR基准测试中显示出优于现有方法的性能表现

Conclusion: 维度缩减方法为NFT稀有度计量提供了有效的设计框架，DIT指标在稀有度评估中具有优越性能

Abstract: Non-fungible tokens (NFTs) have become a significant digital asset class,
each uniquely representing virtual entities such as artworks. These tokens are
stored in collections within smart contracts and are actively traded across
platforms on Ethereum, Bitcoin, and Solana blockchains. The value of NFTs is
closely tied to their distinctive characteristics that define rarity, leading
to a growing interest in quantifying rarity within both industry and academia.
While there are existing rarity meters for assessing NFT rarity, comparing them
can be challenging without direct access to the underlying collection data. The
Rating over all Rarities (ROAR) benchmark addresses this challenge by providing
a standardized framework for evaluating NFT rarity. This paper explores a
dimension reduction approach to rarity design, introducing new performance
measures and meters, and evaluates them using the ROAR benchmark. Our
contributions to the rarity meter design issue include developing an optimal
rarity meter design using non-metric weighted multidimensional scaling,
introducing Dissimilarity in Trades (DIT) as a performance measure inspired by
dimension reduction techniques, and unveiling the non-interpretable rarity
meter DIT, which demonstrates superior performance compared to existing
methods.

</details>


### [8] [Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs](https://arxiv.org/abs/2508.12743)
*Jacob Wahlgren,Gabin Schieffer,Ruimin Shi,Edgar A. León,Roger Pearce,Maya Gokhale,Ivy Peng*

Main category: cs.DC

TL;DR: MI300A APU的UPM架构首次实现CPU和GPU物理内存统一，性能可匹配甚至超越显式内存管理，同时减少高达44%的内存成本。


<details>
  <summary>Details</summary>
Motivation: 传统离散GPU需要管理分离的CPU和GPU内存空间，UVM方案性能代价高，需要研究新型UPM架构的性能特征和应用优势。

Method: 首先分析UPM系统特性（内存延迟、带宽、一致性开销），评估系统软件效率，提出应用移植策略，并在MI300A上评估6个应用。

Result: UPM架构下的统一内存模型应用性能可匹配或超越显式管理模型，同时内存成本降低高达44%。

Conclusion: MI300A的UPM架构为HPC系统提供了高性能且内存效率更高的解决方案，统一物理内存模型具有显著优势。

Abstract: Discrete GPUs are a cornerstone of HPC and data center systems, requiring
management of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)
has been proposed to ease the burden of memory management; however, at a high
cost in performance. The recent introduction of AMD's MI300A Accelerated
Processing Units (APUs)--as deployed in the El Capitan supercomputer--enables
HPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)
for the first time. This work presents the first comprehensive characterization
of the UPM architecture on MI300A. We first analyze the UPM system properties,
including memory latency, bandwidth, and coherence overhead. We then assess the
efficiency of the system software in memory allocation, page fault handling,
TLB management, and Infinity Cache utilization. We propose a set of porting
strategies for transforming applications for the UPM architecture and evaluate
six applications on the MI300A APU. Our results show that applications on UPM
using the unified memory model can match or outperform those in the explicitly
managed model--while reducing memory costs by up to 44%.

</details>


### [9] [Accelerating Edge Inference for Distributed MoE Models with Latency-Optimized Expert Placement](https://arxiv.org/abs/2508.12851)
*Tian Wu,Liming Wang,Zijian Wen,Xiaoxi Zhang,Jingpu Duan,Xianwei Zhang,Jinhang Zuo*

Main category: cs.DC

TL;DR: DanceMoE是一个高效的MoE推理框架，通过激活感知的专家放置和轻量级迁移机制，在异构边缘服务器上实现协作推理，显著降低延迟和通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型在资源受限的边缘环境中部署的挑战，包括大内存占用、复杂通信需求，以及现有方法主要关注单设备或同构设置的局限性。

Method: 利用MoE模型的固有稀疏性和工作负载局部性，提出数据驱动的激活感知放置算法，平衡服务器间的本地覆盖和内存使用，并设计轻量级迁移机制适应动态工作负载。

Result: 在现代MoE模型和广泛使用的数据集上评估，相比最先进的基线方法，推理延迟降低高达30.6%，通信开销显著减少。

Conclusion: DanceMoE展示了协作式边缘MoE推理的有效性，为资源受限环境中的大型语言模型部署提供了实用解决方案。

Abstract: Mixture-of-Experts (MoE) have become a cornerstone for training and scaling
large language models (LLMs), offering substantial gains in model capacity and
efficiency through sparse expert activation. However, serving these models
remains challenging in practice, particularly in resource-constrained edge
environments, due to their large memory footprint and complex communication
demands. While centralized cloud inference is common, it incurs high
infrastructure costs, along with latency and privacy concerns. A few recent
edge MoE works propose memory-efficient strategies but typically focus on
single-device or homogeneous setups. This paper presents DanceMoE, an efficient
MoE inference framework that enables activation-aware expert placement across
collaborative, heterogeneous, GPU-equipped edge servers. DanceMoE leverages the
inherent sparsity of MoE models and workload locality to minimize cross-server
communication and enable efficient expert placement under heterogeneous
resource constraints. It introduces a data-driven, activation-aware placement
algorithm that balances local coverage and memory usage across servers,
alongside a lightweight migration mechanism that adapts expert assignments
under evolving workloads. We evaluate DanceMoE on modern MoE models and widely
used datasets, demonstrating up to 30.6\% lower inference latency, and
substantial communication reduction compared to state-of-the-art baselines,
showcasing the effectiveness of collaborative edge-based MoE inference.

</details>


### [10] [WANify: Gauging and Balancing Runtime WAN Bandwidth for Geo-distributed Data Analytics](https://arxiv.org/abs/2508.12961)
*Anshuman Das Mohapatra,Kwangsung Oh*

Main category: cs.DC

TL;DR: WANify是一个基于机器学习的新型框架，通过随机森林算法动态预测广域网带宽，帮助地理分布式数据分析系统优化数据传输决策，减少延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 现有的地理分布式数据分析系统静态测量WAN带宽，无法准确反映运行时动态和同时传输的情况，导致次优决策，增加查询延迟和成本。

Method: 使用基于决策树的随机森林机器学习算法动态预测运行时WAN带宽，确定数据中心间异构并行连接的最优数量，充分利用可用WAN容量。

Result: 在AWS 8个地理分布式数据中心的测试中，WANify通过平衡最强和最弱WAN链路，将延迟降低26%，成本减少16%，同时高效处理动态性和异构性。

Conclusion: WANify框架能够精确动态测量WAN带宽，为GDA系统提供更好的决策支持，在最小化成本的同时显著提升性能。

Abstract: Accurate wide area network (WAN) bandwidth (BW) is essential for
geo-distributed data analytics (GDA) systems to make optimal decisions such as
data and task placement to improve performance. Existing GDA systems, however,
measure WAN BW statically and independently between data centers (DCs), while
data transfer occurs dynamically and simultaneously among DCs during workload
execution. Also, they use a single connection WAN BW that cannot capture actual
WAN capacities between distant DCs. Such inaccurate WAN BWs yield sub-optimal
decisions, inflating overall query latency and cost. In this paper, we present
WANify, a new framework that precisely and dynamically gauges achievable
runtime WAN BW using a machine learning prediction scheme, decision tree-based
Random Forest. This helps GDA systems make better decisions yielding reduced
latency and costs including WAN BW monitoring costs. Based on predicted runtime
WAN BW, WANify determines the optimal number of heterogeneous parallel
connections for data transfer among DCs. This approach improves performance
without additional, or even at reduced cost, by fully exploiting available WAN
capacities. In addition, WANify considers dynamics like network and workloads,
and heterogeneity like skewed data, heterogeneous compute resources, and a
varying number of DCs while making decisions. The WANify prototype running on
state-of-the-art GDA systems is evaluated on AWS with 8 geo-distributed DCs.
Results show that WANify enhances WAN throughput by balancing between the
strongest and weakest WAN links, enabling GDA systems to reduce latency and
cost by up to 26% and 16% respectively with minimal effort, all while handling
dynamics and heterogeneity efficiently.

</details>


### [11] [Congested Clique Counting for Local Gibbs Distributions](https://arxiv.org/abs/2508.13083)
*Joshua Z. Sobel*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: There are well established reductions between combinatorial sampling and
counting problems (Jerrum, Valiant, Vazirani TCS 1986). Building off of a very
recent parallel algorithm utilizing this connection (Liu, Yin, Zhang arxiv
2024), we demonstrate the first approximate counting algorithm in the
CongestedClique for a wide range of problems. Most interestingly, we present an
algorithm for approximating the number of $q$-colorings of a graph within
$\epsilon$-multiplicative error, when $q>\alpha\Delta$ for any constant
$\alpha>2$, in $\Tilde{O}\big(\frac{n^{1/3}}{\epsilon^2}\big)$ rounds. More
generally, we achieve a runtime of
$\Tilde{O}\big(\frac{n^{1/3}}{\epsilon^2}\big)$ rounds for approximating the
partition function of Gibbs distributions defined over graphs when simple
locality and fast mixing conditions hold. Gibbs distributions are widely used
in fields such as machine learning and statistical physics. We obtain our
result by providing an algorithm to draw $n$ random samples from a distributed
Markov chain in parallel, using similar ideas to triangle counting (Dolev,
Lenzen, Peled DISC 2012) and semiring matrix multiplication (Censor-Hillel,
Kaski, Korhonen, Lenzen, Paz, Suomela PODC 2015). Aside from counting problems,
this result may be interesting for other applications requiring a large number
of samples. In the special case of estimating the partition function of the
hardcore model, also known as counting weighted independent sets, we can do
even better and achieve an $\Tilde{O}\big(\frac{1}{\epsilon^2}\big)$ round
algorithm, when the fugacity $\lambda \leq \frac{\alpha}{\Delta-1}$, where
$\alpha$ is an arbitrary constant less than $1$.

</details>


### [12] [Team Formation and Applications](https://arxiv.org/abs/2508.13084)
*Yuval Emek,Shay Kutten,Ido Rafael,Gadi Taubenfeld*

Main category: cs.DC

TL;DR: 提出了一种新的长生命分布式问题Team Formation(TF)，并设计了消息和时间效率高的随机算法。通过将多种分布式问题缩减到TF，在异步隐式领导者选举等问题中突破了线性消息复杂度限制，并提高了时间效能。


<details>
  <summary>Details</summary>
Motivation: 解决在异步模型下的长生命分布式问题，应对节点初始故障挑战，通过统一的TF问题模型来缩减多种分布式问题，提高算法效率和性能。

Method: 在完全通信图的异步模型中，使用有界消息大小，设计随机化算法将环境注入的token组织成团队（团队大小为参数σ）。通过将多种分布式问题缩减到TF问题来实现效率提升。

Result: 首次突破了异步隐式领导者选举的线性消息复杂度限制，提高了消息最优算法的时间复杂度，并证明了TF算法消息复杂度的紧下界。

Conclusion: Team Formation作为一种基础分布式问题，能够高效地缩凍多种分布式问题，在消息和时间复杂度方面实现了重要突破，为分布式系统设计提供了新的解决思路。

Abstract: A novel long-lived distributed problem, called Team Formation (TF), is
introduced together with a message- and time-efficient randomized algorithm.
The problem is defined over the asynchronous model with a complete
communication graph, using bounded size messages, where a certain fraction of
the nodes may experience a generalized, strictly stronger, version of initial
failures. The goal of a TF algorithm is to assemble tokens injected by the
environment, in a distributed manner, into teams of size $\sigma$, where
$\sigma$ is a parameter of the problem.
  The usefulness of TF is demonstrated by using it to derive efficient
algorithms for many distributed problems. Specifically, we show that various
(one-shot as well as long-lived) distributed problems reduce to TF. This
includes well-known (and extensively studied) distributed problems such as
several versions of leader election and threshold detection. For example, we
are the first to break the linear message complexity bound for asynchronous
implicit leader election. We also improve the time complexity of
message-optimal algorithms for asynchronous explicit leader election. Other
distributed problems that reduce to TF are new ones, including matching players
in online gaming platforms, a generalization of gathering, constructing a
perfect matching in an induced subgraph of the complete graph, quorum sensing
in message-passing networks, and more. To complement our positive contribution,
we establish a tight lower bound on the message complexity of TF algorithms.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [HPD: Hybrid Projection Decomposition for Robust State Space Models on Analog CIM Hardware](https://arxiv.org/abs/2508.11935)
*Yuannuo Feng,Wenyong Zhou,Yuexi Lyu,Hanjie Liu,Zhengwu Liu,Ngai Wong,Wang Kang*

Main category: cs.AR

TL;DR: 本文分析了状态空间模型在内存计算架构中的噪声鲁棒性问题，发现最终块和输出投影层对扰动最敏感，提出了混合投影分解策略HPD来提升模型在噪声条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型虽然适合内存计算架构，但设备非理想性导致的权重扰动会严重影响推理精度，需要系统分析其鲁棒性并提出有效的解决方案。

Method: 提出HPD混合投影分解策略，将输出投影层的权重矩阵分解为U和Σ的乘积（保持与现有硬件兼容），同时将V^T卸载到数字硬件进行精确校正。

Result: 在Mamba模型上的测试显示，该方法在各种噪声条件下将困惑度降低了99.57%，在PIQA常识推理基准上的准确率提升了96.67%。

Conclusion: HPD方法有效提升了状态空间模型在内存计算架构中的噪声鲁棒性，为实际部署提供了可行的解决方案。

Abstract: State Space Models (SSMs) are efficient alternatives to traditional sequence
models, excelling at processing long sequences with lower computational
complexity. Their reliance on matrix multiplications makes them ideal for
compute-in-memory (CIM) architectures, which improve energy efficiency by
computing within memory arrays. However, device non-idealities in CIM introduce
weight perturbations that can degrade inference accuracy. In this paper, we
systematically analyze the robustness of SSMs under noisy conditions,
identifying that the final block and output projection layers are more
susceptible to perturbations compared to other components. Building on these
insights, we propose HPD, a Hybrid Projection Decomposition strategy for the
last output projection layer. We replace the original weight matrix with the
multiplication of U and {\Sigma} in its SVD to ensure compatibility with
existing hardware architectures, while offloading V> to digital hardware for
precise and robust correction. Comprehensive tests on Mamba models show that
our method reduces perplexity by up to 99.57% under various noise conditions
compared to baseline models, with accuracy gains of up to 96.67% on the PIQA
benchmark for commonsense reasoning.

</details>


### [14] [Special Session: Sustainable Deployment of Deep Neural Networks on Non-Volatile Compute-in-Memory Accelerators](https://arxiv.org/abs/2508.12195)
*Yifan Qin,Zheyu Yan,Wujie Wen,Xiaobo Sharon Hu,Yiyu Shi*

Main category: cs.AR

TL;DR: 基于负反馈理论的新题向变分前向训练方法(OVF)，通过减少对写入验证操作的依赖来提升NVCIM加速器的推理准确性和稳定性


<details>
  <summary>Details</summary>
Motivation: 非易失性存储器(NVM)基于内存计算(CIM)的加速器因其原位数据处理能力而具有高能效，但NVM设备的随机性和本质变异导致性能降级。传统的写入验证操作能力提升准确性但耗能耗时。

Method: 提出了一种新的负优化训练机制，开发了题向变分前向(OVF)训练方法来实现该机制，通过负反馈理论来提升DNN在NVCIM上的那米性部署。

Result: 实验结果显示OVF方法在推理准确性上超过现有最先进技术达46.71%，同时减少认知不确定性。该方法减少了对耗能耗时的写入验证操作的依赖。

Conclusion: 该方法有助于NVCIM加速器的可持续和实际部署，在保持NVCIM加速器可持续计算优势的同时解决性能降级问题。

Abstract: Non-volatile memory (NVM) based compute-in-memory (CIM) accelerators have
emerged as a sustainable solution to significantly boost energy efficiency and
minimize latency for Deep Neural Networks (DNNs) inference due to their in-situ
data processing capabilities. However, the performance of NVCIM accelerators
degrades because of the stochastic nature and intrinsic variations of NVM
devices. Conventional write-verify operations, which enhance inference accuracy
through iterative writing and verification during deployment, are costly in
terms of energy and time. Inspired by negative feedback theory, we present a
novel negative optimization training mechanism to achieve robust DNN deployment
for NVCIM. We develop an Oriented Variational Forward (OVF) training method to
implement this mechanism. Experiments show that OVF outperforms existing
state-of-the-art techniques with up to a 46.71% improvement in inference
accuracy while reducing epistemic uncertainty. This mechanism reduces the
reliance on write-verify operations and thus contributes to the sustainable and
practical deployment of NVCIM accelerators, addressing performance degradation
while maintaining the benefits of sustainable computing with NVCIM
accelerators.

</details>


### [15] [A Time- and Energy-Efficient CNN with Dense Connections on Memristor-Based Chips](https://arxiv.org/abs/2508.12251)
*Wenyong Zhou,Yuan Ren,Jiajun Zhou,Tianshu Hou,Ngai Wong*

Main category: cs.AR

TL;DR: 这篇论文提出了一种RRAM友好的轻量级卷积神经网络设计，通过改进DenseNet的特征图连接方式来提高存储计算交叉矩阵的利用率，降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决传统轻量设计如深度卷积在RRAM存储计算交叉矩阵中利用率低的问题，并在保持精度的同时提高硬件效能。

Method: 评估DenseNet的硬件成本，发现线性增长的通道导致交叉矩阵利用率低，提出将前置层特征图连接作为每个阶段末层输入的方案。

Result: 实验结果显示，提出的模型比传统ResNet和DenseNet更节省时间和能量，同时在CIFAR和ImageNet数据集上保持竞争性的准确度。

Conclusion: 该方案成功地在保持CNN模型高准确度的前提下，优化了RRAM存储计算交叉矩阵的利用率，为边缘AI设备提供了更高效的解决方案。

Abstract: Designing lightweight convolutional neural network (CNN) models is an active
research area in edge AI. Compute-in-memory (CIM) provides a new computing
paradigm to alleviate time and energy consumption caused by data transfer in
von Neumann architecture. Among competing alternatives, resistive random-access
memory (RRAM) is a promising CIM device owing to its reliability and multi-bit
programmability. However, classical lightweight designs such as depthwise
convolution incurs under-utilization of RRAM crossbars restricted by their
inherently dense weight-to-RRAM cell mapping. To build an RRAM-friendly yet
efficient CNN, we evaluate the hardware cost of DenseNet which maintains a high
accuracy vs other CNNs at a small parameter count. Observing the linearly
increasing channels in DenseNet leads to a low crossbar utilization and causes
large latency and energy consumption, we propose a scheme that concatenates
feature maps of front layers to form the input of the last layer in each stage.
Experiments show that our proposed model consumes less time and energy than
conventional ResNet and DenseNet, while producing competitive accuracy on CIFAR
and ImageNet datasets.

</details>


### [16] [AutoPower: Automated Few-Shot Architecture-Level Power Modeling by Power Group Decoupling](https://arxiv.org/abs/2508.12294)
*Qijun Zhang,Yao Lu,Mengming Li,Zhiyao Xie*

Main category: cs.AR

TL;DR: AutoPower是一个自动化架构级功耗建模工具，通过功耗组分离开法，在仅有少量已知配置的情况下实现高精度功耗预测


<details>
  <summary>Details</summary>
Motivation: 现代CPU设计中功耗效率至关重要，但传统分析方法不准确，而基于机器学习的方法需要大量训练数据，不切实际

Method: 提出功耗组分离开法：首先跨功耗组分离建立独立模型，然后在每个功耗组内进一步分解为多个子模型，重点关注时钟和SRAM功耗与架构级结构信息的关联

Result: 仅用2个已知配置训练即可达到4.36%的MAPE和0.96的R²，比代表性ML模型McPAT-Calib的MAPE低5%，R²高0.09

Conclusion: AutoPower能够在极有限训练数据下实现高精度架构级功耗建模，解决了传统方法和ML方法在数据需求方面的局限性

Abstract: Power efficiency is a critical design objective in modern CPU design.
Architects need a fast yet accurate architecture-level power evaluation tool to
perform early-stage power estimation. However, traditional analytical
architecture-level power models are inaccurate. The recently proposed machine
learning (ML)-based architecture-level power model requires sufficient data
from known configurations for training, making it unrealistic.
  In this work, we propose AutoPower targeting fully automated
architecture-level power modeling with limited known design configurations. We
have two key observations: (1) The clock and SRAM dominate the power
consumption of the processor, and (2) The clock and SRAM power correlate with
structural information available at the architecture level. Based on these two
observations, we propose the power group decoupling in AutoPower. First,
AutoPower decouples across power groups to build individual power models for
each group. Second, AutoPower designs power models by further decoupling the
model into multiple sub-models within each power group. In our experiments,
AutoPower can achieve a low mean absolute percentage error (MAPE) of 4.36\% and
a high $R^2$ of 0.96 even with only two known configurations for training. This
is 5\% lower in MAPE and 0.09 higher in $R^2$ compared with McPAT-Calib, the
representative ML-based power model.

</details>


### [17] [Soft Error Probability Estimation of Nano-scale Combinational Circuits](https://arxiv.org/abs/2508.12345)
*Ali Jockar,Mohsen Raji*

Main category: cs.AR

TL;DR: 这篇论文提出了一种新的软错误概率分析框架，统一考虑制造变异和老化效应，在保持高精度的同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 随着技术缩小，纳米级数字电路更容易受单事件故障影响。现有方法多独立考虑制造变异或老化效应，或依赖计算费用高的蒙特卡罗模拟，限制了大规模电路优化的实用性。

Method: 提出了一种新的SEP分析框架，整体集成制造变异和老化效应。包括提出增强型电气屏蔽模型和统计方法来量化过程变异和老化变异下的软错误概率。

Result: 实验结果表明，该方法在保持高精度的同时，计算开销约减少了2.5%（与蒙特卡罗模拟方法相比）。

Conclusion: 这项工作通过提供高效、准确的SEP估计方法，在存在制造变异和长期线性老化的情况下，推动了可靠纳米级电路的设计。

Abstract: As technology scales, nano-scale digital circuits face heightened
susceptibility to single event upsets (SEUs) and transients (SETs) due to
shrinking feature sizes and reduced operating voltages. While logical,
electrical, and timing masking effects influence soft error probability (SEP),
the combined impact of process variation (PV) and aging-induced degradation
further complicates SEP estimation. Existing approaches often address PV or
aging in isolation, or rely on computationally intensive methods like Monte
Carlo simulations, limiting their practicality for large-scale circuit
optimization. This paper introduces a novel framework for SEP analysis that
holistically integrates PV and aging effects. We propose an enhanced electrical
masking model and a statistical methodology to quantify soft error probability
under process and aging variations. Experimental results demonstrate that the
proposed approach achieves high accuracy while reducing computational overhead
by approximately 2.5% compared to Monte Carlo-based methods. This work advances
the design of reliable nano-scale circuits by enabling efficient, accurate SEP
estimation in the presence of manufacturing variability and long-term
transistor degradation.

</details>


### [18] [An ECC-based Fault Tolerance Approach for DNNs](https://arxiv.org/abs/2508.12347)
*Mohsen Raji,Mohammad Zaree,Kimia Soroush*

Main category: cs.AR

TL;DR: 提出基于ECC的SPW容错方法，通过错误检测和校正/屏蔽机制保护DNN免受位翻转错误影响，在BER为10^(-1)时准确率提升300%以上，面积开销47.5%。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在安全关键系统中部署时，内存中的位翻转错误可能影响其正确功能，需要有效的容错机制来确保可靠性。

Method: 采用错误校正码(ECC)技术，检测位翻转错误：单比特错误时进行校正，多比特错误时将权重置零(屏蔽)。通过统计故障注入实验验证方法有效性。

Result: 在比特错误率10^(-1)的情况下，相比应用ECC技术的情况，DNN准确率提升了300%以上，仅带来47.5%的面积开销。

Conclusion: SPW方法能有效提高DNN在存在位翻转错误时的容错能力，以合理的硬件开销显著提升系统可靠性，适用于安全关键应用。

Abstract: Deep Neural Network (DNN) has achieve great success in solving a wide range
of machine learning problems. Recently, they have been deployed in datacenters
(potentially for business-critical or industrial applications) and
safety-critical systems such as self-driving cars. So, their correct
functionality in the presence of potential bit-flip errors on DNN parameters
stored in memories plays the key role in their applicability in safety-critical
applications. In this paper, a fault tolerance approach based on Error
Correcting Codes (ECC), called SPW, is proposed to ensure the correct
functionality of DNNs in the presence of bit-flip faults. In the proposed
approach, error occurrence is detected by the stored ECC and then, it is
correct in case of a single-bit error or the weight is completely set to zero
(i.e. masked) otherwise. A statistical fault injection campaign is proposed and
utilized to investigate the efficacy of the proposed approach. The experimental
results show that the accuracy of the DNN increases by more than 300% in the
presence with Bit Error Rate of 10^(-1) in comparison to the case where ECC
technique is applied, in expense of just 47.5% area overhead.

</details>


### [19] [ATLAS: A Self-Supervised and Cross-Stage Netlist Power Model for Fine-Grained Time-Based Layout Power Analysis](https://arxiv.org/abs/2508.12433)
*Wenkai Li,Yao Lu,Wenji Fang,Jing Wang,Qijun Zhang,Zhiyao Xie*

Main category: cs.AR

TL;DR: ATLAS是一个能够在门级网表阶段预测最终布局功耗的模型，无需布局信息即可实现高精度的时间基功耗预测，显著加速设计优化流程。


<details>
  <summary>Details</summary>
Motivation: 传统精确功耗仿真需要耗时的后端处理和仿真步骤，严重阻碍设计优化效率。需要一种能够在早期设计阶段准确预测布局功耗的方法。

Method: 提出ATLAS模型，采用专门为电路功耗定制的新预训练和微调范式，支持时间基功耗仿真和跨设计功耗建模，无需布局信息即可预测最终布局功耗。

Result: 在时钟树、寄存器和组合逻辑功耗组上的MAPE分别仅为0.58%、0.45%和5.12%，整体设计总功耗的MAPE<1%，推理速度显著快于商业工具标准流程。

Conclusion: ATLAS首次实现了同时支持时间基功耗仿真和通用跨设计功耗建模，为VLSI设计提供了高效的早期功耗预测解决方案，大幅提升了设计优化效率。

Abstract: Accurate power prediction in VLSI design is crucial for effective power
optimization, especially as designs get transformed from gate-level netlist to
layout stages. However, traditional accurate power simulation requires
time-consuming back-end processing and simulation steps, which significantly
impede design optimization. To address this, we propose ATLAS, which can
predict the ultimate time-based layout power for any new design in the
gate-level netlist. To the best of our knowledge, ATLAS is the first work that
supports both time-based power simulation and general cross-design power
modeling. It achieves such general time-based power modeling by proposing a new
pre-training and fine-tuning paradigm customized for circuit power. Targeting
golden per-cycle layout power from commercial tools, our ATLAS achieves the
mean absolute percentage error (MAPE) of only 0.58%, 0.45%, and 5.12% for the
clock tree, register, and combinational power groups, respectively, without any
layout information. Overall, the MAPE for the total power of the entire design
is <1%, and the inference speed of a workload is significantly faster than the
standard flow of commercial tools.

</details>


### [20] [MemorySim: An RTL-level, timing accurate simulator model for the Chisel ecosystem](https://arxiv.org/abs/2508.12636)
*Ansh Chaurasia*

Main category: cs.AR

TL;DR: MemorySim是一个RTL级内存模拟器，提供精确的时序和功能正确性，与Chisel/Verilog模拟无缝集成，支持性能功耗评估。


<details>
  <summary>Details</summary>
Motivation: AI硬件需求增长，内存子系统成为性能瓶颈，现有模拟器在时序模拟上妥协了正确性或RTL集成度。

Method: 开发RTL级内存模拟器MemorySim，与Chisel/Verilog模拟无缝集成，兼容Chisel/Chipyard生态系统。

Result: MemorySim能够提供精确的性能和功耗估计，支持通过FireSim等模拟平台进行下游评估。

Conclusion: MemorySim解决了现有内存模拟器的局限性，为AI硬件开发提供了更准确的内存子系统模拟解决方案。

Abstract: The rapid growth of AI applications has driven increased demand for
specialized AI hardware, highlighting critical opportunities within the memory
subsystem, which often serves as a performance bottleneck in high-demand
workloads such as large language models (LLMs). Existing high-level memory
simulators, such as DRAMSim2 and DRAMSim3, offer timing simulations but
frequently compromise on correctness or integration at the register-transfer
level (RTL). We present MemorySim, an RTL-level memory simulator designed to
deliver both accurate timing and functional correctness. MemorySim integrates
seamlessly with existing Chisel and Verilog simulations and is fully compatible
with the Chisel/Chipyard ecosystem. This enables users to obtain precise
performance and power estimates, supporting downstream evaluation through
simulation platforms such as FireSim.

</details>


### [21] [HOMI: Ultra-Fast EdgeAI platform for Event Cameras](https://arxiv.org/abs/2508.12637)
*Shankaranarayanan H,Satyapreet Singh Yadav,Adithya Krishna,Ajay Vikram P,Mahesh Mehendale,Chetan Singh Thakur*

Main category: cs.AR

TL;DR: HOMI是一个超低延迟的端到端边缘AI平台，使用事件相机和FPGA芯片，通过硬件优化的预处理管道实现了94%的手势识别准确率和1000fps的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 事件相机在边缘机器人应用中具有异步操作和稀疏事件驱动的优势，但现有解决方案缺乏完整的端到端实现，延迟高且未能充分利用事件数据的稀疏性。

Method: 开发了HOMI平台，包含Prophesee IMX636事件传感器和Xilinx Zynq UltraScale+ MPSoC FPGA芯片，部署了内部开发的AI加速器，支持恒定时间和恒定事件模式的直方图累积、线性和指数时间表面的硬件优化预处理管道。

Result: 在DVS Gesture数据集上达到94%的准确率（高精度配置），低延迟配置下提供1000fps的吞吐量，仅使用33%的FPGA LUT资源，内存占用紧凑。

Conclusion: HOMI平台为事件相机处理提供了高效的端到端解决方案，在保持高精度的同时实现了超低延迟，为更复杂的架构集成和多任务部署留下了充足的空间。

Abstract: Event cameras offer significant advantages for edge robotics applications due
to their asynchronous operation and sparse, event-driven output, making them
well-suited for tasks requiring fast and efficient closed-loop control, such as
gesture-based human-robot interaction. Despite this potential, existing event
processing solutions remain limited, often lacking complete end-to-end
implementations, exhibiting high latency, and insufficiently exploiting event
data sparsity. In this paper, we present HOMI, an ultra-low latency, end-to-end
edge AI platform comprising a Prophesee IMX636 event sensor chip with an Xilinx
Zynq UltraScale+MPSoC FPGA chip, deploying an in-house developed AI
accelerator. We have developed hardware-optimized pre-processing pipelines
supporting both constant-time and constant-event modes for histogram
accumulation, linear and exponential time surfaces. Our general-purpose
implementation caters to both accuracy-driven and low-latency applications.
HOMI achieves 94% accuracy on the DVS Gesture dataset as a use case when
configured for high accuracy operation and provides a throughput of 1000 fps
for low-latency configuration. The hardware-optimised pipeline maintains a
compact memory footprint and utilises only 33% of the available LUT resources
on the FPGA, leaving ample headroom for further latency reduction, model
parallelisation, multi-task deployments, or integration of more complex
architectures.

</details>


### [22] [XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads](https://arxiv.org/abs/2508.13049)
*Tejas Chaudhari,Akarsh J.,Tanushree Dewangan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: XR-NPE是一个面向XR感知工作负载的高吞吐量混合精度SIMD神经网络处理引擎，支持多种低精度格式，通过创新的硬件设计和量化感知训练实现高性能和低功耗。


<details>
  <summary>Details</summary>
Motivation: 为扩展现实(XR)设备开发高效的神经网络处理引擎，解决传统方法在内存带宽、功耗和计算密度方面的限制，满足资源受限XR设备的计算需求。

Method: 提出支持FP4、Posit(4,1)、Posit(8,0)、Posit(16,1)等格式的混合精度架构，采用可重构尾数乘法和指数处理电路(RMMEC)，结合选择性电源门控和量化感知训练技术。

Result: 在28nm CMOS工艺下达到1.72GHz最大工作频率，面积0.016mm²，算术强度14pJ，相比现有最佳MAC方法减少42%面积和38%功耗，在VCU129上相比SoTA加速器减少1.4x LUTs和1.77x FFs，能效提升1.2x。

Conclusion: XR-NPE是一个可扩展、精度自适应的计算引擎，为未来资源受限的XR设备提供了高效的神经网络处理解决方案，具有优异的性能和能效表现。

Abstract: This work proposes XR-NPE, a high-throughput Mixed-precision SIMD Neural
Processing Engine, designed for extended reality (XR) perception workloads like
visual inertial odometry (VIO), object classification, and eye gaze extraction.
XR-NPE is first to support FP4, Posit (4,1), Posit (8,0), and Posit (16,1)
formats, with layer adaptive hybrid-algorithmic implementation supporting
ultra-low bit precision to significantly reduce memory bandwidth requirements,
and accompanied by quantization-aware training for minimal accuracy loss. The
proposed Reconfigurable Mantissa Multiplication and Exponent processing
Circuitry (RMMEC) reduces dark silicon in the SIMD MAC compute engine, assisted
by selective power gating to reduce energy consumption, providing 2.85x
improved arithmetic intensity. XR-NPE achieves a maximum operating frequency of
1.72 GHz, area 0.016 mm2 , and arithmetic intensity 14 pJ at CMOS 28nm,
reducing 42% area, 38% power compared to the best of state-of-the-art MAC
approaches. The proposed XR-NPE based AXI-enabled Matrix-multiplication
co-processor consumes 1.4x fewer LUTs, 1.77x fewer FFs, and provides 1.2x
better energy efficiency compared to SoTA accelerators on VCU129. The proposed
co-processor provides 23% better energy efficiency and 4% better compute
density for VIO workloads. XR-NPE establishes itself as a scalable,
precision-adaptive compute engine for future resource-constrained XR devices.
The complete set for codes for results reproducibility are released publicly,
enabling designers and researchers to readily adopt and build upon them.
https://github.com/mukullokhande99/XR-NPE.

</details>
