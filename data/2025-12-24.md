<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Error Localization, Certificates, and Hints for Probabilistic Program Verification via Slicing (Extended Version)](https://arxiv.org/abs/2512.20214)
*Philipp Schröer,Darion Haase,Joost-Pieter Katoen*

Main category: cs.PL

TL;DR: 本文提出基于切片技术的概率程序演绎验证用户诊断方法，包括错误报告、证明简化和验证结果保持三种诊断概念，并在Caesar验证器中实现为Brutus工具。


<details>
  <summary>Details</summary>
Motivation: 概率程序验证中需要有效的用户诊断来帮助理解验证失败原因、简化证明过程，并保持已验证的部分结果。现有方法缺乏针对定量断言的错误定位机制。

Method: 基于HeyVL定量中间验证语言，形式化定义三种诊断切片概念：错误报告切片、证明简化切片和验证结果保持切片。实现Brutus工具，采用二分搜索算法最小化错误见证切片，并比较基于不可满足核、最小不可满足子集枚举和直接SMT编码的多种算法。

Result: 在现有和新基准测试上的实证评估表明，Brutus能够找到既小又信息丰富的切片。错误报告切片为定量断言提供了新颖的错误定位机制，支持多种证明规则如定量规范语句和基于不变量的循环规则。

Conclusion: 提出的切片诊断方法为概率程序验证提供了有效的用户诊断工具，能够生成专门的错误消息和验证提示，并形式化证明了其正确性。该方法可应用于多种概率编程语言的诊断。

Abstract: This paper focuses on effective user diagnostics generated during the deductive verification of probabilistic programs. Our key principle is based on providing slices for (1) error reporting, (2) proof simplification, and (3) preserving successful verification results. By formally defining these different notions on HeyVL, an existing quantitative intermediate verification language (IVL), our concepts (and implementation) can be used to obtain diagnostics for a range of probabilistic programming languages. Slicing for error reporting is a novel notion of error localization for quantitative assertions. We demonstrate slicing-based diagnostics on a variety of proof rules such as quantitative versions of the specification statement and invariant-based loop rules, and formally prove the correctness of specialized error messages and verification hints.
  We implemented our user diagnostics into the deductive verifier Caesar. Our novel implementation -- called \emph{Brutus} -- can search for slices which do or do not verify, corresponding to each of the three diagnostic notions. For error reporting (1), it exploits a binary search-based algorithm that minimizes error-witnessing slices. To solve for slices that verify (2 and 3), we empirically compare different algorithms based on unsatisfiable cores, minimal unsatisfiable subset enumeration, and a direct SMT encoding of the slicing problem. Our empirical evaluation of Brutus on existing and new benchmarks shows that we can find slices that are both small and informative.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform](https://arxiv.org/abs/2512.19842)
*Andrea Sordello,Marco Mellia,Idilio Drago,Rodolfo Valentim,Francesco Musumeci,Massimo Tornatore,Federico Cerutti,Martino Trevisan,Alessio Botta,Willen Borges Coelho*

Main category: cs.DC

TL;DR: Holoscope是一个轻量级云原生平台，用于简化分布式望远镜和蜜罐传感器的部署与管理，支持跨多机构网络的安全攻击监控。


<details>
  <summary>Details</summary>
Motivation: 互联网攻击的复杂性和规模需要能够跨多样化网络监控恶意流量的分布式、协作式观测平台。现有解决方案在部署、管理和安全连接方面存在挑战。

Method: 基于K3s和WireGuard构建的云原生平台，采用模块化设计和基础设施即代码原则，提供安全连接、自动化节点加入、弹性操作，支持动态传感器编排、自动化恢复和处理。

Result: 在欧洲和巴西的多个机构和云网络中成功部署和运行Holoscope，实现了对大规模攻击现象的统一可见性，同时保持了易集成性和安全合规性。

Conclusion: Holoscope提供了一个有效的分布式攻击监控平台，简化了传感器部署管理，支持跨机构协作，为大规模网络攻击分析提供了实用解决方案。

Abstract: The complexity and scale of Internet attacks call for distributed, cooperative observatories capable of monitoring malicious traffic across diverse networks. Holoscope is a lightweight, cloud-native platform designed to simplify the deployment and management of distributed telescope (passive) and honeypot (active) sensors, used to collect and analyse attack traffic by exposing or simulating vulnerable systems. Built upon K3s and WireGuard, Holoscope offers secure connectivity, automated node onboarding, and resilient operation even in resource-constrained environments. Through modular design and Infrastructure-as-Code principles, it supports dynamic sensor orchestration, automated recovery and processing. We build, deploy and operate Holoscope across multiple institutions and cloud networks in Europe and Brazil, enabling unified visibility into large-scale attack phenomena while maintaining ease of integration and security compliance.

</details>


### [3] [UCCL-EP: Portable Expert-Parallel Communication](https://arxiv.org/abs/2512.19849)
*Ziming Mao,Yihan Zhang,Chihan Cui,Kaichao You,Zhongjie Chen,Zhiying Xu,Scott Shenker,Costin Raiciu,Yang Zhou,Ion Stoica*

Main category: cs.DC

TL;DR: UCCL-EP：一种可移植的专家并行通信系统，通过GPU-CPU控制通道替代GPU发起的RDMA，在异构GPU和NIC平台上实现高性能。


<details>
  <summary>Details</summary>
Motivation: 现有专家并行通信系统（如DeepEP）虽然性能优秀，但跨异构GPU和NIC平台的移植性差，因为其GPU发起的令牌级RDMA通信需要GPU与NIC之间的紧密垂直集成。

Method: 1) 用高吞吐量的GPU-CPU控制通道替代GPU发起的RDMA，将紧凑的令牌路由命令传输给多线程CPU代理，由代理代表GPU执行GPUDirect RDMA操作；2) 使用RDMA即时数据模拟各种排序语义，确保在缺乏此类排序支持的NIC（如AWS EFA）上的正确性。

Result: 在EFA上，UCCL-EP的调度和组合吞吐量比现有最佳EP解决方案提升高达2.1倍；在NVIDIA平台上，性能与原始DeepEP相当；在NVIDIA+EFA平台上，SGLang令牌吞吐量提升高达40%；在16节点AMD+Broadcom平台上，DeepSeek-V3训练吞吐量提升高达45%。

Conclusion: UCCL-EP通过创新的GPU-CPU控制通道设计和排序语义模拟，实现了跨异构硬件平台的高性能专家并行通信，解决了现有系统移植性差的问题。

Abstract: Mixture-of-Experts (MoE) workloads rely on expert parallelism (EP) to achieve high GPU efficiency. State-of-the-art EP communication systems such as DeepEP demonstrate strong performance but exhibit poor portability across heterogeneous GPU and NIC platforms. The poor portability is rooted in architecture: GPU-initiated token-level RDMA communication requires tight vertical integration between GPUs and NICs, e.g., GPU writes to NIC driver/MMIO interfaces.
  We present UCCL-EP, a portable EP communication system that delivers DeepEP-level performance across heterogeneous GPU and NIC hardware. UCCL-EP replaces GPU-initiated RDMA with a high-throughput GPU-CPU control channel: compact token-routing commands are transferred to multithreaded CPU proxies, which then issue GPUDirect RDMA operations on behalf of GPUs. UCCL-EP further emulates various ordering semantics required by specialized EP communication modes using RDMA immediate data, enabling correctness on NICs that lack such ordering, e.g., AWS EFA. We implement UCCL-EP on NVIDIA and AMD GPUs with EFA and Broadcom NICs. On EFA, it outperforms the best existing EP solution by up to $2.1\times$ for dispatch and combine throughput. On NVIDIA-only platform, UCCL-EP achieves comparable performance to the original DeepEP. UCCL-EP also improves token throughput on SGLang by up to 40% on the NVIDIA+EFA platform, and improves DeepSeek-V3 training throughput over the AMD Primus/Megatron-LM framework by up to 45% on a 16-node AMD+Broadcom platform.

</details>


### [4] [An Adaptive Distributed Stencil Abstraction for GPUs](https://arxiv.org/abs/2512.19851)
*Aditya Bhosale,Laxmikant Kale*

Main category: cs.DC

TL;DR: 提出基于Charm++运行时的自适应分布式抽象CharmTyles，用于多节点GPU上的模板计算，提供NumPy-like语法，支持动态资源弹性伸缩，性能优于专用DSL和通用NumPy替代方案。


<details>
  <summary>Details</summary>
Motivation: Python科学计算生态系统主要局限于单节点并行，NumPy原型与超级计算机高性能执行之间存在鸿沟。硬件加速器普及和能效需求使得资源自适应性成为关键需求，但传统HPC抽象仍然僵化。

Method: 基于自适应Charm++运行时构建CharmTyles框架，创建支持多节点GPU的分布式模板计算抽象，采用熟悉的NumPy-like语法以减少从原型到生产代码的移植工作。

Result: 展示了资源弹性能力：动态调整运行应用程序在不同节点数间的伸缩；性能分析显示相关开销可控；性能显著优于专用高性能模板DSL和通用NumPy替代方案。

Conclusion: CharmTyles填补了Python科学计算中单节点原型与多节点高性能执行之间的鸿沟，通过自适应分布式抽象和NumPy-like语法，实现了资源弹性和性能优势。

Abstract: The scientific computing ecosystem in Python is largely confined to single-node parallelism, creating a gap between high-level prototyping in NumPy and high-performance execution on modern supercomputers. The increasing prevalence of hardware accelerators and the need for energy efficiency have made resource adaptivity a critical requirement, yet traditional HPC abstractions remain rigid. To address these challenges, we present an adaptive, distributed abstraction for stencil computations on multi-node GPUs. This abstraction is built using CharmTyles, a framework based on the adaptive Charm++ runtime, and features a familiar NumPy-like syntax to minimize the porting effort from prototype to production code. We showcase the resource elasticity of our abstraction by dynamically rescaling a running application across a different number of nodes and present a performance analysis of the associated overheads. Furthermore, we demonstrate that our abstraction achieves significant performance improvements over both a specialized, high-performance stencil DSL and a generalized NumPy replacement.

</details>


### [5] [Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions](https://arxiv.org/abs/2512.19972)
*Pengchao Han,Xi Huang,Yi Fang,Guojun Han*

Main category: cs.DC

TL;DR: 本文对协作学习中知识蒸馏（KD）进行了全面综述，重点关注记忆与知识在KD过程中的作用机制，分析了不同协作学习模式下的知识提取、存储与共享，并探讨了任务异质性等挑战。


<details>
  <summary>Details</summary>
Motivation: 协作学习已成为大规模智能系统的关键范式，但知识蒸馏在协作学习中如何利用记忆和知识的机制尚未得到充分探索。本文旨在填补这一空白，深入理解知识在协作环境中的提取、存储和共享过程。

Method: 通过定义和分类KD过程中的记忆与知识，探索其相互关系；分析分布式、分层和去中心化等不同协作学习模式；特别关注分布式学习模式中的任务异质性，涵盖联邦学习、多智能体域适应、联邦多模态学习等多种场景。

Result: 提供了对协作学习中KD技术的系统性理解框架，明确了记忆与知识在知识转移中的作用机制；分析了模型异质性、数据异质性、资源异质性和隐私问题等关键挑战；对现有工作基于记忆与知识处理方法进行了分类。

Conclusion: 本文为协作学习中知识蒸馏的研究提供了全面的理论基础和分析框架，指出了当前面临的挑战，并提出了未来研究方向，有助于推动KD技术在协作学习中的进一步发展。

Abstract: Collaborative learning has emerged as a key paradigm in large-scale intelligent systems, enabling distributed agents to cooperatively train their models while addressing their privacy concerns. Central to this paradigm is knowledge distillation (KD), a technique that facilitates efficient knowledge transfer among agents. However, the underlying mechanisms by which KD leverages memory and knowledge across agents remain underexplored. This paper aims to bridge this gap by offering a comprehensive review of KD in collaborative learning, with a focus on the roles of memory and knowledge. We define and categorize memory and knowledge within the KD process and explore their interrelationships, providing a clear understanding of how knowledge is extracted, stored, and shared in collaborative settings. We examine various collaborative learning patterns, including distributed, hierarchical, and decentralized structures, and provide insights into how memory and knowledge dynamics shape the effectiveness of KD in collaborative learning. Particularly, we emphasize task heterogeneity in distributed learning pattern covering federated learning (FL), multi-agent domain adaptation (MADA), federated multi-modal learning (FML), federated continual learning (FCL), federated multi-task learning (FMTL), and federated graph knowledge embedding (FKGE). Additionally, we highlight model heterogeneity, data heterogeneity, resource heterogeneity, and privacy concerns of these tasks. Our analysis categorizes existing work based on how they handle memory and knowledge. Finally, we discuss existing challenges and propose future directions for advancing KD techniques in the context of collaborative learning.

</details>


### [6] [Scaling Point-based Differentiable Rendering for Large-scale Reconstruction](https://arxiv.org/abs/2512.20017)
*Hexu Zhao,Xiaoteng Liu,Xiwen Min,Jianhao Huang,Youming Deng,Yanfei Li,Ang Li,Jinyang Li,Aurojit Panda*

Main category: cs.DC

TL;DR: Gaian是一个用于点基可微渲染的通用分布式训练系统，通过优化数据局部性减少通信开销，提升训练效率


<details>
  <summary>Details</summary>
Motivation: 现有的点基可微渲染分布式训练系统存在两个主要问题：1) 与特定方法紧密耦合，缺乏通用性；2) 数据局部性差导致严重的通信开销。需要开发一个既能支持多种PBDR方法又能优化通信效率的系统。

Method: Gaian提供了一个统一的API，足够表达现有PBDR方法，同时暴露丰富的数据访问信息。系统利用这些信息来优化数据局部性，减少通信开销。通过实现4种PBDR算法来评估系统性能。

Result: 在6个数据集和最多128个GPU上，Gaian减少了高达91%的通信开销，训练吞吐量提升了1.50倍到3.71倍，实现了高性能和资源效率。

Conclusion: Gaian是一个通用的分布式PBDR训练系统，通过统一的API和优化的数据局部性，显著减少了通信开销并提高了训练效率，为大规模高分辨率3D场景重建提供了有效的解决方案。

Abstract: Point-based Differentiable Rendering (PBDR) enables high-fidelity 3D scene reconstruction, but scaling PBDR to high-resolution and large scenes requires efficient distributed training systems. Existing systems are tightly coupled to a specific PBDR method. And they suffer from severe communication overhead due to poor data locality. In this paper, we present Gaian, a general distributed training system for PBDR. Gaian provides a unified API expressive enough to support existing PBDR methods, while exposing rich data-access information, which Gaian leverages to optimize locality and reduce communication. We evaluated Gaian by implementing 4 PBDR algorithms. Our implementations achieve high performance and resource efficiency: across six datasets and up to 128 GPUs, it reduces communication by up to 91% and improves training throughput by 1.50x-3.71x.

</details>


### [7] [FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling](https://arxiv.org/abs/2512.20064)
*Yaojian Chen,Si-Qiu Gong,Lin Gan,Yanfei Liu,An Yang,Yinuo Wang,Chao-yang Lu,Guangwen Yang*

Main category: cs.DC

TL;DR: Fast-MPS是一个用于可扩展MPS采样的多级并行框架，结合了跨样本的数据并行和沿键维度的张量并行，在Gaussian Boson Sampling中实现了10倍以上的加速，可扩展到数千个进程。


<details>
  <summary>Details</summary>
Motivation: 随着问题复杂度增加，MPS规模迅速扩大。传统数据并行受限于内存和I/O，而模型并行缺乏可扩展性，需要新的并行框架来处理大规模MPS采样。

Method: 提出Fast-MPS多级并行框架，结合跨样本的数据并行和沿键维度的张量并行，通过压缩和重叠技术消除内存和I/O压力，在大规模MPS采样中重新启用数据并行。

Result: 在Gaussian Boson Sampling中实现超过10倍加速，可扩展到数千个进程，支持8,176个位点和键维度chi=10^4的模拟，显著优于现有技术。

Conclusion: Fast-MPS在高性能张量网络应用中展现出巨大潜力，解决了大规模MPS采样的并行化挑战。

Abstract: Matrix Product State (MPS) is a versatile tensor network representation widely applied in quantum physics, quantum chemistry, and machine learning, etc. MPS sampling serves as a critical fundamental operation in these fields. As the problems become more complex, the scale of MPS is rapidly increasing. Traditional data parallelism is limited by memory and heavy I/O in large-scale MPS. Model parallelism that can handle large-scale MPS imposes rigid process bindings and lacks scalability. This work proposes Fast-MPS, a multi-level parallel framework for scalable MPS sampling. Our design combines data parallelism across samples with tensor parallelism along bond dimensions. We eliminate memory and I/O pressure through compression and overlapping, and revive data parallel in large-scale MPS sampling. We evaluate our approach on Gaussian Boson Sampling, a representative and demanding application. Fast-MPS achieves over 10x speedup compared to existing simulators, scales to thousands of processes, and enables simulations with 8,176 sites and bond dimension chi = 10^4, significantly outperforming the state of the art. Fast-MPS has demonstrated great potential in high-performance tensor network applications.

</details>


### [8] [Population Protocols Revisited: Parity and Beyond](https://arxiv.org/abs/2512.20163)
*Leszek Gąsieniec,Tytus Grodzicki,Tomasz Jurdziński,Jakub Kowalski,Grzegorz Stachowiak*

Main category: cs.DC

TL;DR: 本文提出了首个在时间和空间上都高效的奇偶性和同余性群体协议，使用O(log³ n)状态在O(log³ n)时间内实现稳定计算。


<details>
  <summary>Details</summary>
Motivation: 群体协议研究近20年来，虽然对领导者选举和多数计算等问题有高效解决方案，但对奇偶性和同余性等Presburger算术中的互补谓词，一直没有同时实现时间和空间高效的协议。这一空白构成了该领域的重要挑战。

Method: 提出新的计算范式，整合群体权重系统、鲁棒的时钟机制、高效异常检测和切换机制。采用多阶段稳定群体协议设计，允许协议不完全优化，更注重通用性、鲁棒性和概率保证。

Result: 首次实现了高效的奇偶性和同余性协议，使用O(log³ n)状态在O(log³ n)时间内实现静默稳定。权重系统还支持一元和二进制表示之间的隐式转换，可应用于其他问题。

Conclusion: 通过放松完全优化的要求，专注于通用性和鲁棒性，成功填补了群体协议中奇偶性和同余性计算的时间-空间效率空白，为其他问题提供了新的计算范式。

Abstract: For nearly two decades, population protocols have been extensively studied, yielding efficient solutions for central problems in distributed computing, including leader election, and majority computation, a predicate type in Presburger Arithmetic closely tied to population protocols. Surprisingly, no protocols have achieved both time- and space-efficiency for congruency predicates, such as parity computation, which are complementary in this arithmetic framework. This gap highlights a significant challenge in the field. To address this gap, we explore the parity problem, where agents are tasked with computing the parity of the given sub-population size. Then we extend the solution for parity to compute congruences modulo an arbitrary $m$.
  Previous research on efficient population protocols has focused on protocols that minimise both stabilisation time and state utilisation for specific problems. In contrast, this work slightly relaxes this expectation, permitting protocols to place less emphasis on full optimisation and more on universality, robustness, and probabilistic guarantees. This allows us to propose a novel computing paradigm that integrates population weights (or simply weights), a robust clocking mechanism, and efficient anomaly detection coupled with a switching mechanism (which ensures slow but always correct solutions). This paradigm facilitates universal design of efficient multistage stable population protocols. Specifically, the first efficient parity and congruence protocols introduced here use both $O(\log^3 n)$ states and achieve silent stabilisation in $O(\log^3 n)$ time. We conclude by discussing the impact of implicit conversion between unary and binary representations enabled by the weight system, with applications to other problems, including the computation and representation of (sub-)population sizes.

</details>


### [9] [SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication](https://arxiv.org/abs/2512.20178)
*Chen Zhuang,Lingqi Zhang,Benjamin Brock,Du Wu,Peng Chen,Toshio Endo,Satoshi Matsuoka,Mohamed Wahib*

Main category: cs.DC

TL;DR: 提出分布式稀疏矩阵乘法优化框架，通过细粒度稀疏感知通信策略和分层通信策略，在128个GPU上实现最高221.5倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 分布式稀疏矩阵乘法是高性能计算和深度学习中的基础操作，现有实现存在显著的通信开销问题，限制了性能和可扩展性。

Method: 1) 细粒度稀疏感知通信策略：利用稀疏矩阵的稀疏模式减少通信开销；2) 分层通信策略：将稀疏感知策略与GPU加速系统中常见的两层网络架构结合，减少跨慢速网络链路的冗余通信。

Result: 在真实数据集上的评估显示，框架在128个GPU上表现出强大的可扩展性，相对于四个最先进的基线方法（CAGNET、SPA、BCL、CoLa）分别实现了221.5倍、56.0倍、23.4倍和8.8倍的几何平均加速。

Conclusion: 通过分析现有分布式SpMM实现中的通信低效问题，并提出针对性的优化策略，显著提升了分布式稀疏矩阵乘法的性能和可扩展性。

Abstract: Distributed Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental operation in numerous high-performance computing and deep learning applications. The major performance bottleneck in distributed SpMM lies in the substantial communication overhead, which limits both performance and scalability. In this paper, we identify and analyze sources of inefficient communication in existing distributed SpMM implementations at two levels and address these inefficiencies by proposing: (1) a fine-grained, sparsity-aware communication strategy that reduces communication overhead by exploiting the sparsity pattern of the sparse matrix, and (2) a hierarchical communication strategy that integrates the sparsity-aware strategy with the common two-tier network architectures in GPU-accelerated systems, to reduce redundant communication across slow network links. We implement these optimizations in a comprehensive distributed SpMM framework, \method{}. Extensive evaluations on real-world datasets show that our framework demonstrates strong scalability up to 128 GPUs, achieving geometric mean speedups of 221.5$\times$, 56.0$\times$, 23.4$\times$, and 8.8$\times$ over four state-of-the-art baselines (CAGNET, SPA, BCL, and CoLa, respectively) at this scale.

</details>


### [10] [Reaching Agreement Among Reasoning LLM Agents](https://arxiv.org/abs/2512.20184)
*Chaoyi Ruan,Yiliang Wang,Ziji Shi,Jialin Li*

Main category: cs.DC

TL;DR: 该论文提出Aegean，一个为随机推理智能体设计的共识协议，解决多智能体精炼问题，通过增量仲裁检测实现早期终止，显著降低延迟同时保持答案质量。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体编排依赖静态启发式工作流（如固定循环限制和屏障同步），这些临时方法浪费计算资源、因落后节点产生高延迟，并可能最终化瞬时协议。可靠的多智能体推理需要类似经典分布式共识问题的形式化基础。

Method: 提出多智能体精炼问题的形式化模型，包括正确性保证的定义和智能体推理的形式语义。然后引入Aegean——为随机推理智能体设计的共识协议，解决多智能体精炼问题。在Aegean-Serve中实现该协议，这是一个共识感知的服务引擎，在并发智能体执行中进行增量仲裁检测，实现充分智能体收敛时的早期终止。

Result: 在四个数学推理基准测试中，Aegean提供可证明的安全性和活性保证，同时相比最先进的基线降低延迟1.2-20倍，保持答案质量在2.5%以内。在本地GPU部署和商业API提供商上的一致增益验证了基于共识的编排消除了落后节点延迟而不牺牲正确性。

Conclusion: 该研究为多智能体推理提供了形式化基础，通过共识协议Aegean解决了现有静态工作流的局限性，实现了延迟显著降低和资源效率提升，同时保持答案质量，为可靠的多智能体系统提供了新的理论基础和实用解决方案。

Abstract: Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.
  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.

</details>


### [11] [Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs](https://arxiv.org/abs/2512.20210)
*Yinan Ni,Xiao Yang,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: P-LoRA：一种面向LoRA微调LLM的服务器无感知推理系统，通过预测性适配器预取和页面式内存管理，显著降低冷启动延迟并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在服务器无感知环境中部署基于LoRA的LLM推理面临两大挑战：反应式适配器加载导致显著的冷启动延迟，以及频繁的适配器交换造成严重的GPU内存碎片化。

Method: 提出P-LoRA系统，包含两个关键技术：1）轻量级LSTM流量预测器，预测适配器需求并主动从主机内存预取到GPU；2）受操作系统虚拟内存启发的页面式适配器内存管理机制，保持高GPU内存利用率。

Result: 实验使用Azure Functions跟踪的生产级工作负载，P-LoRA相比S-LoRA实现1.52倍吞吐量提升，在高并发场景下平均TTFT降低35%，冷启动延迟最高减少68%，GPU内存利用率保持在87%以上。

Conclusion: P-LoRA通过预测性适配器预取和智能内存管理，有效解决了服务器无感知环境中LoRA-based LLM推理的冷启动和内存碎片问题，显著提升了系统性能。

Abstract: The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.

</details>


### [12] [Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults](https://arxiv.org/abs/2512.20394)
*Mohammad Walid Charrwi,Zaid Hussain*

Main category: cs.DC

TL;DR: 提出一种基于强化学习的容错路由方案，用于高斯互连网络，通过PPO代理学习绕过故障区域，显著提升包投递率


<details>
  <summary>Details</summary>
Motivation: 随着NoC和无线传感器网络规模扩大，网络拓扑对性能至关重要。高斯互连网络具有直径和对称性优势，但自适应路由技术易受节点和链路故障影响，导致通信可靠性快速下降。特别是遵循高斯分布的节点故障（如热热点或物理损坏集群）对传统确定性路由构成严重挑战。

Method: 提出一种针对高斯互连网络的故障感知强化学习路由方案。使用PPO（近端策略优化）代理，设计特定的奖励结构来惩罚接近故障区域，系统动态学习绕过故障区域。与贪婪自适应最短路径路由算法进行比较。

Result: 实验结果表明，RL代理显著优于自适应路由：在40%故障密度下，RL维持0.95的包投递率，而贪婪算法仅为0.66。在20%低网络负载下，RL投递率为0.57，贪婪算法为0.43，显示RL在管理拥塞方面更有效。

Conclusion: 基于强化学习的路由方案在随机、易故障的拓扑结构中表现出色，验证了其在处理高斯互连网络故障方面的有效性，特别是在高故障密度和低网络负载条件下优于传统贪婪自适应路由。

Abstract: As Network-on-Chip (NoC) and Wireless Sensor Network architectures continue to scale, the topology of the underlying network becomes a critical factor in performance. Gaussian Interconnected Networks based on the arithmetic of Gaussian integers, offer attractive properties regarding diameter and symmetry. Despite their attractive theoretical properties, adaptive routing techniques in these networks are vulnerable to node and link faults, leading to rapid degradation in communication reliability. Node failures (particularly those following Gaussian distributions, such as thermal hotspots or physical damage clusters) pose severe challenges to traditional deterministic routing. This paper proposes a fault-aware Reinforcement Learning (RL) routing scheme tailored for Gaussian Interconnected Networks. By utilizing a PPO (Proximal Policy Optimization) agent with a specific reward structure designed to penalize fault proximity, the system dynamically learns to bypass faulty regions. We compare our proposed RL-based routing protocol against a greedy adaptive shortest-path routing algorithm. Experimental results demonstrate that the RL agent significantly outperforms the adaptive routing sustaining a Packet Delivery Ratio (PDR) of 0.95 at 40% fault density compared to 0.66 for the greedy. Furthermore, the RL approach exhibits effective delivery rates compared to the greedy adaptive routing, particularly under low network load of 20% at 0.57 vs. 0.43, showing greater proficiency in managing congestion, validating its efficacy in stochastic, fault-prone topologies

</details>


### [13] [WOC: Dual-Path Weighted Object Consensus Made Efficient](https://arxiv.org/abs/2512.20485)
*Tanisha Fonseca,Gengrui Zhang*

Main category: cs.DC

TL;DR: WOC提出了一种双路径共识协议，根据操作访问模式动态路由：独立操作走快速路径（对象特定加权仲裁，1个网络往返），冲突/共享对象走慢速路径（节点加权共识），在保持高竞争性能的同时，对>70%独立对象的工作负载实现比Cabinet高4倍的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有共识协议要么针对节点异构性优化（如Cabinet使用加权仲裁但串行化所有操作），要么针对工作负载独立性优化（如EPaxos支持并行执行但忽略节点性能差异），无法同时处理节点异构性和工作负载独立性。

Method: WOC采用双路径共识协议：1）快速路径处理独立操作，使用对象特定加权仲裁，1个网络往返完成；2）慢速路径处理冲突或共享对象，采用节点加权共识，由领导者协调。根据操作访问模式动态路由到相应路径。

Result: 评估显示：对于独立对象比例>70%的工作负载，WOC比Cabinet实现高达4倍的吞吐量提升；在高竞争场景下，WOC保持与Cabinet相当的性能。

Conclusion: WOC成功解决了现有共识协议无法同时优化节点异构性和工作负载独立性的问题，通过双路径设计实现了高性能和适应性，为现代分布式系统提供了更优的共识解决方案。

Abstract: Modern distributed systems face a critical challenge: existing consensus protocols optimize for either node heterogeneity or workload independence, but not both. For example, Cabinet leverages weighted quorums to handle node heterogeneity but serializes all operations through a global leader, limiting parallelism. EPaxos enables parallel execution for independent operations but treats all nodes uniformly, ignoring performance differences. To tackle this problem, we present WOC, a dual-path consensus protocol that dynamically routes operations into two paths based on their access patterns. Independent operations execute through a fast path that uses object-specific weighted quorums and completes in one network round-trip. Conflicting or shared objects route through a leader-coordinated slow path employing node-weighted consensus. Our evaluation demonstrates that WOC achieves up to 4X higher throughput than Cabinet for workloads with >70% independent objects, while maintaining equivalent performance under high contention.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [3D Stack In-Sensor-Computing (3DS-ISC): Accelerating Time-Surface Construction for Neuromorphic Event Cameras](https://arxiv.org/abs/2512.20073)
*Hongyang Shang,Shuai Dong,Ye Ke,Arindam Basu*

Main category: cs.AR

TL;DR: 提出3D堆叠传感器内计算架构，利用DRAM漏电特性实现时间表面实时归一化，显著降低功耗和面积，在事件视觉任务中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统事件视觉处理面临内存墙问题，功耗和延迟较高，需要更高效的传感器内计算架构来支持实时处理

Method: 采用3D堆叠集成传感、存储和计算，利用DRAM漏电特性实现指数衰减时间归一化，使用MOMCAP存储电荷和低漏电开关延长存储时间

Result: 相比2D架构降低功耗69倍、延迟2.2倍、面积1.9倍；相比16位SRAM降低功耗三个数量级；在多个数据集上达到SOTA分类精度，图像重建SSIM最高

Conclusion: 3D传感器内计算架构为实时高效事件视觉处理奠定基础，未来可集成更先进计算电路扩展应用范围

Abstract: This work proposes a 3D Stack In-Sensor-Computing (3DS-ISC) architecture for efficient event-based vision processing. A real-time normalization method using an exponential decay function is introduced to construct the time-surface, reducing hardware usage while preserving temporal information. The circuit design utilizes the leakage characterization of Dynamic Random Access Memory(DRAM) for timestamp normalization. Custom interdigitated metal-oxide-metal capacitor (MOMCAP) is used to store the charge and low leakage switch (LL switch) is used to extend the effective charge storage time. The 3DS-ISC architecture integrates sensing, memory, and computation to overcome the memory wall problem, reducing power, latency, and reducing area by 69x, 2.2x and 1.9x, respectively, compared with its 2D counterpart. Moreover, compared to works using a 16-bit SRAM to store timestamps, the ISC analog array can reduce power consumption by three orders of magnitude. In real computer vision (CV) tasks, we applied the spatial-temporal correlation filter (STCF) for denoise, and 3D-ISC achieved almost equivalent accuracy compared to the digital implementation using high precision timestamps. As for the image classification, time-surface constructed by 3D-ISC is used as the input of GoogleNet, achieving 99% on N-MNIST, 85% on N-Caltech101, 78% on CIFAR10-DVS, and 97% on DVS128 Gesture, comparable with state-of-the-art results on each dataset. Additionally, the 3D-ISC method is also applied to image reconstruction using the DAVIS240C dataset, achieving the highest average SSIM (0.62) among three methods. This work establishes a foundation for real-time, resource-efficient event-based processing and points to future integration of advanced computational circuits for broader applications.

</details>


### [15] [Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling](https://arxiv.org/abs/2512.20198)
*Huizheng Wang,Taiquan Wei,Hongbin Wang,Zichuan Wang,Xinru Tang,Zhiheng Yue,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: STAR提出了一种针对大语言模型长序列并行处理的跨阶段协同稀疏加速算法-硬件协同设计，通过预测稀疏性、分布式排序和协调分块策略，显著提升计算和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有动态稀疏加速器在长序列并行处理场景下表现不佳，因为它们采用阶段隔离的优化方法。研究发现跨阶段协同可以大幅减少冗余计算和内存访问，这是被忽视的机会。

Method: 1) 基于前导零的稀疏预测，使用对数域仅加法操作最小化预测开销；2) 分布式排序和排序更新FlashAttention机制；3) 协调分块策略实现细粒度阶段交互；4) 专用STAR加速器架构；5) 多核空间架构部署优化数据流和执行编排。

Result: STAR相比A100实现9.2倍加速和71.2倍能效提升，相比最先进加速器实现16.1倍能效和27.1倍面积效率提升。空间架构版本相比基线设计实现20.1倍吞吐量提升。

Conclusion: STAR通过跨阶段协同的算法-硬件协同设计，有效解决了长序列并行处理场景下的Transformer推理效率问题，为超长序列处理提供了高效的解决方案。

Abstract: Large language models (LLMs) rely on self-attention for contextual understanding, demanding high-throughput inference and large-scale token parallelism (LTPP). Existing dynamic sparsity accelerators falter under LTPP scenarios due to stage-isolated optimizations. Revisiting the end-to-end sparsity acceleration flow, we identify an overlooked opportunity: cross-stage coordination can substantially reduce redundant computation and memory access. We propose STAR, a cross-stage compute- and memory-efficient algorithm-hardware co-design tailored for Transformer inference under LTPP. STAR introduces a leading-zero-based sparsity prediction using log-domain add-only operations to minimize prediction overhead. It further employs distributed sorting and a sorted updating FlashAttention mechanism, guided by a coordinated tiling strategy that enables fine-grained stage interaction for improved memory efficiency and latency. These optimizations are supported by a dedicated STAR accelerator architecture, achieving up to 9.2$\times$ speedup and 71.2$\times$ energy efficiency over A100, and surpassing SOTA accelerators by up to 16.1$\times$ energy and 27.1$\times$ area efficiency gains. Further, we deploy STAR onto a multi-core spatial architecture, optimizing dataflow and execution orchestration for ultra-long sequence processing. Architectural evaluation shows that, compared to the baseline design, Spatial-STAR achieves a 20.1$\times$ throughput improvement.

</details>


### [16] [Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization](https://arxiv.org/abs/2512.20495)
*He Zhu,Zheng Liu,Xingyang Li,Anbang Wu,Jieru Zhao,Fangxin Liu,Yiming Gan,Jingwen Leng,Yu Feng*

Main category: cs.AR

TL;DR: Nebula是一个用于大规模3D高斯泼溅协同渲染的加速框架，通过流式传输中间结果而非视频，显著降低云客户端带宽需求，并提升VR体验。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯泼溅架构设计忽视可扩展性，难以处理超大规模场景；同时VR带宽需求使得云端无法传输高保真、流畅的VR内容。

Method: 1) 流式传输LoD搜索后的中间结果而非视频；2) 云端引入时间感知LoD搜索，利用帧间时间一致性减少不规则内存访问；3) 客户端提出新颖的立体光栅化，让双眼共享大部分计算；4) 最小硬件增强。

Result: 相比有损视频流，Nebula实现2.7倍运动到光子延迟加速，减少1925%带宽需求。

Conclusion: Nebula通过创新的协同渲染框架，有效解决了大规模3D高斯泼溅渲染的扩展性和VR带宽瓶颈问题。

Abstract: 3D Gaussian splatting (3DGS) has drawn significant attention in the architectural community recently. However, current architectural designs often overlook the 3DGS scalability, making them fragile for extremely large-scale 3DGS. Meanwhile, the VR bandwidth requirement makes it impossible to deliver high-fidelity and smooth VR content from the cloud.
  We present Nebula, a coherent acceleration framework for large-scale 3DGS collaborative rendering. Instead of streaming videos, Nebula streams intermediate results after the LoD search, reducing 1925% data communication between the cloud and the client. To further enhance the motion-to-photon experience, we introduce a temporal-aware LoD search in the cloud that tames the irregular memory access and reduces redundant data access by exploiting temporal coherence across frames. On the client side, we propose a novel stereo rasterization that enables two eyes to share most computations during the stereo rendering with bit-accurate quality. With minimal hardware augmentations, Nebula achieves 2.7$\times$ motion-to-photon speedup and reduces 1925% bandwidth over lossy video streaming.

</details>


### [17] [Composing Mini Oscilloscope on Embedded Systems](https://arxiv.org/abs/2512.20571)
*Brennan Romero,D. G. Perera*

Main category: cs.AR

TL;DR: 使用Nuvoton NUC-140嵌入式平台实现基本示波器功能，包括自动、边沿触发、单次模式，波形显示和探头校准


<details>
  <summary>Details</summary>
Motivation: 利用嵌入式系统平台实现常规示波器的基本功能，创建一个经济实用的调试工具

Method: 使用Nuvoton NUC-140作为前端和显示平台，通过定制子板连接BNC探头接口、外部九键键盘和校准信号，利用开发板LCD显示波形

Result: 系统实现了90%的常用示波器功能，包括自动、边沿触发、单次模式，垂直和水平缩放波形可视化，以及探头校准，成为一个非常称职的调试工具

Conclusion: 基于NUC-140的嵌入式示波器系统成功实现了常规示波器的核心功能，验证了嵌入式平台在测试测量仪器开发中的可行性

Abstract: In this paper, our goal is to reproduce the basic functionalities of a regular oscilloscope, using the Nuvoton NUC-140 embedded systems development platform as the front-end and display method. A custom-built daughter board connects the NUC-140 to a variety of peripherals, including two BNC scope-probe connections, an external nine-button keypad, and a calibration signal. The LCD of the NUC-140 development board serves as the waveform display. From the experimental results, it is demonstrated that our proposed system became a very competent debugging tool. It implements 90% of the features we typically use on original oscilloscopes, including: automatic, edge-triggered, and single modes; waveform visualization using vertical and horizontal scaling; probe calibration.

</details>
