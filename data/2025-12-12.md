<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Intrinsically Correct Algorithms and Recursive Coalgebras](https://arxiv.org/abs/2512.10748)
*Cass Alexandru,Henning Urbat,Thorsten Wißmann*

Main category: cs.PL

TL;DR: 提出基于良基函子的框架，自动保证递归性，简化递归算法在证明助手的形式化


<details>
  <summary>Details</summary>
Motivation: 递归余代数虽能优雅建模递归算法，但证明其递归性通常需要特设方法，这阻碍了在证明助手中的形式化

Method: 引入良基函子概念，在良基关系索引的族范畴上工作，证明良基函子的每个余代数都是递归的

Result: 主要理论结果：每个良基函子的余代数都是递归的；已形式化在Cubical Agda中；案例包括快速排序、欧几里得算法、CYK解析

Conclusion: 提出的框架通过类型保证递归性，简化了递归算法的形式化，统一了多种递归性证明技术

Abstract: Recursive coalgebras provide an elegant categorical tool for modelling recursive algorithms and analysing their termination and correctness. By considering coalgebras over categories of suitably indexed families, the correctness of the corresponding algorithms follows intrinsically just from the type of the computed maps. However, proving recursivity of the underlying coalgebras is non-trivial, and proofs are typically ad hoc. This layer of complexity impedes the formalization of coalgebraically defined recursive algorithms in proof assistants. We introduce a framework for constructing coalgebras which are intrinsically recursive in the sense that the type of the coalgebra guarantees recursivity from the outset. Our approach is based on the novel concept of a well-founded functor on a category of families indexed by a well-founded relation. We show as our main result that every coalgebra for a well-founded functor is recursive, and demonstrate that well-known techniques for proving recursivity and termination such as ranking functions are subsumed by this abstract setup. We present a number of case studies, including Quicksort, the Euclidian algorithm, and CYK parsing. Both the main theoretical result and selected case studies have been formalized in Cubical Agda.

</details>


### [2] [Towards Cumulative Abstract Semantics via Handlers](https://arxiv.org/abs/2512.10861)
*Cade Lueker,Andrew Fox,Bor-Yuh Evan Chang*

Main category: cs.PL

TL;DR: 提出基于作用域效应的累积抽象语义框架，实现控制流模块化，支持多种路径/流敏感性和方向的分析


<details>
  <summary>Details</summary>
Motivation: 现有抽象解释框架缺乏真正的灵活性，难以支持不同路径/流敏感性、分析方向和近似方式，且语法与语义紧密耦合，实现缺乏模块性

Method: 利用作用域效应将效应分为两类：语法消除处理器和领域语义引入处理器，通过累积抽象语义实现单一解释器支持多种动态评估和静态分析

Result: 设计出干净、优雅、模块化的抽象解释框架，能够从单一解释器创建多个动态评估器和静态分析器

Conclusion: 效应作为设计工具能够实现清晰、优雅、模块化的抽象解释框架，解决现有框架的灵活性和模块性问题

Abstract: We consider the problem of modularizing control flow in a generic abstract interpretation framework. A generic abstract interpretation framework is not truly flexible if it does not allow interpreting with different path- and flow-sensitivities, by going forwards or backwards, and over- or under-approximately. Most interpreters inherently intertwine syntax and semantics, making the implementation antagonistic to modularity. Current approaches to modular designs require the use of complex data structures (e.g., monad transformers), providing modularity but often proving unwieldy (e.g., lifts). We observe that leveraging scoped effects within an interpreter facilitates the accumulation of semantic fragments against a fixed syntax. In this paper, we define cumulative abstract semantics, illustrating the potential for creating multiple dynamic evaluators and static analyses from one interpreter. This modularity is achieved by grouping effects into two categories: syntax elimination and domain-semantic introduction handlers. Our contribution shows the benefits of using effects as an instrument for designing a clean, elegant, and modular abstract interpretation framework.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [A study of the spectrum resource leasing method based on ERC4907 extension](https://arxiv.org/abs/2512.09942)
*Zhiming Liang,Bin Chen,Litao Ye,Chen Sun,Shuo Wang,Zhe Peng*

Main category: cs.DC

TL;DR: 提出M-ERC4907扩展方法，支持多时间槽批量配置和多用户同时授权，解决ERC4907单用户单时间槽限制，显著降低链上交易和Gas消耗


<details>
  <summary>Details</summary>
Motivation: ERC4907标准虽然支持可租赁NFT，但仅限于单用户、单时间槽授权，在去中心化多槽调度场景中适用性和效率严重受限

Method: 提出M-ERC4907扩展方法，引入支持多时间槽批量配置和多用户同时授权的新功能，消除ERC4907的刚性顺序授权约束

Result: 在Remix开发平台上的实验结果显示，M-ERC4907方法显著减少链上交易和总体Gas消耗，提升了可扩展性和资源分配效率

Conclusion: M-ERC4907扩展方法有效解决了ERC4907在去中心化多槽调度场景中的局限性，通过支持批量配置和同时授权提高了NFT租赁系统的效率和可扩展性

Abstract: The ERC4907 standard enables rentable Non-Fungible Tokens (NFTs) but is limited to single-user, single-time-slot authorization, which severely limits its applicability and efficiency in decentralized multi-slot scheduling scenarios. To address this limitation, this paper proposes Multi-slot ERC4907 (M-ERC4907) extension method. The M-ERC4907 method introduces novel functionalities to support the batch configuration of multiple time slots and simultaneous authorization of multiple users, thereby effectively eliminating the rigid sequential authorization constraint of ERC4907. The experiment was conducted on the Remix development platform. Experimental results show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency.

</details>


### [4] [ELANA: A Simple Energy and Latency Analyzer for LLMs](https://arxiv.org/abs/2512.09946)
*Hung-Yueh Chiang,Bokun Wang,Diana Marculescu*

Main category: cs.DC

TL;DR: ELANA是一个开源轻量级LLM性能分析工具，用于评估模型大小、KV缓存、推理延迟和能耗，支持Hugging Face所有公开模型和多GPU/边缘GPU平台。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在各类硬件平台（从移动边缘设备到云端GPU集群）上的延迟和功耗是主要约束，需要有效的基准测试工具来优化模型部署效率和下一代模型开发。

Method: 开发了ELANA这一轻量级学术友好型分析工具，支持分析模型大小、KV缓存大小、预填充延迟（TTFT）、生成延迟（TPOT）和端到端延迟（TTLT），兼容Hugging Face所有公开模型，提供简单命令行界面和可选能耗日志功能。

Result: 开源发布了ELANA分析工具，支持多GPU和边缘GPU平台，完全兼容流行的Hugging Face API，可轻松定制或适配压缩或低比特模型，适用于高效LLM研究或小规模概念验证研究。

Conclusion: ELANA为LLM性能评估提供了一个简单实用的开源工具，有助于优化模型部署效率和促进高效LLM研究，特别适合学术研究和小规模概念验证。

Abstract: The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.

</details>


### [5] [CloudFix: Automated Policy Repair for Cloud Access Control Policies Using Large Language Models](https://arxiv.org/abs/2512.09957)
*Bethel Hall,Owen Ungaro,William Eiers*

Main category: cs.DC

TL;DR: CloudFix：首个结合形式化方法与LLM的云访问控制策略自动修复框架，通过形式化故障定位和LLM生成修复方案，显著提升策略修复准确率。


<details>
  <summary>Details</summary>
Motivation: 云环境中访问控制策略的手动编写和更新容易出错且耗时，可能导致安全漏洞。现有符号分析方法在云访问控制场景下泛化能力有限，而LLM在程序修复中的应用尚未探索用于云访问控制策略修复。

Method: CloudFix结合形式化方法与LLM：首先使用基于形式化方法的故障定位识别策略中的错误语句，然后利用LLM生成潜在修复方案，最后通过SMT求解器验证修复的正确性。

Result: 在包含282个真实AWS访问控制策略的数据集上实验表明，CloudFix在不同请求规模下均优于基线实现，显著提升了修复准确率。

Conclusion: 这是首个利用LLM进行策略修复的工作，展示了LLM在访问控制领域的有效性，能够实现云访问控制策略的高效自动修复。工具和数据集已开源。

Abstract: Access control policies are vital for securing modern cloud computing, where organizations must manage access to sensitive data across thousands of users in distributed system settings. Cloud administrators typically write and update policies manually, which can be an error-prone and time-consuming process and can potentially lead to security vulnerabilities. Existing approaches based on symbolic analysis have demon- strated success in automated debugging and repairing access control policies; however, their generalizability is limited in the context of cloud-based access control. Conversely, Large Language Models (LLMs) have been utilized for automated program repair; however, their applicability to repairing cloud access control policies remains unexplored. In this work, we introduce CloudFix, the first automated policy repair framework for cloud access control that combines formal methods with LLMs. Given an access control policy and a specification of allowed and denied access requests, CloudFix employs Formal Methods-based Fault Localization to identify faulty statements in the policy and leverages LLMs to generate potential repairs, which are then verified using SMT solvers. To evaluate CloudFix, we curated a dataset of 282 real-world AWS access control policies extracted from forum posts and augmented them with synthetically generated request sets based on real scenarios. Our experimental results show that CloudFix improves repair accuracy over a Baseline implementation across varying request sizes. Our work is the first to leverage LLMs for policy repair, showcasing the effectiveness of LLMs for access control and enabling efficient and automated repair of cloud access control policies. We make our tool Cloudfix and AWS dataset publicly available.

</details>


### [6] [TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0](https://arxiv.org/abs/2512.09961)
*Jinyu Chen,Long Shi,Taotao Wang,Jiaheng Wang,Wei Zhang*

Main category: cs.DC

TL;DR: 提出TDC-Cache框架，通过两层架构结合深度强化学习和PoCL共识机制，解决Web3.0去中心化数据访问中的效率和安全问题。


<details>
  <summary>Details</summary>
Motivation: Web3.0从中心化向去中心化转型，用户获得数据自主权，但面临数据冗余复制导致的效率问题和数据不一致带来的安全漏洞。

Method: 开发TDC-Cache框架，采用两层架构：DON层作为可信中介平台；提出DRL-DC算法动态优化分布式预言机缓存策略；设计PoCL共识机制维护缓存决策一致性。

Result: 相比现有方法，平均访问延迟降低20%，缓存命中率最多提升18%，平均共识成功率提高10%。

Conclusion: 该研究首次探索Web3.0去中心化缓存框架和策略，有效提升系统效率和安全性。

Abstract: The rapid growth of Web3.0 is transforming the Internet from a centralized structure to decentralized, which empowers users with unprecedented self-sovereignty over their own data. However, in the context of decentralized data access within Web3.0, it is imperative to cope with efficiency concerns caused by the replication of redundant data, as well as security vulnerabilities caused by data inconsistency. To address these challenges, we develop a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0 to ensure efficient caching and enhance system resilience against adversarial threats. This framework features a two-layer architecture, wherein the Decentralized Oracle Network (DON) layer serves as a trusted intermediary platform for decentralized caching, bridging the contents from decentralized storage and the content requests from users. In light of the complexity of Web3.0 network topologies and data flows, we propose a Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC) for TDC-Cache to dynamically optimize caching strategies of distributed oracles. Furthermore, we develop a Proof of Cooperative Learning (PoCL) consensus to maintain the consistency of decentralized caching decisions within DON. Experimental results show that, compared with existing approaches, the proposed framework reduces average access latency by 20%, increases the cache hit rate by at most 18%, and improves the average success consensus rate by 10%. Overall, this paper serves as a first foray into the investigation of decentralized caching framework and strategy for Web3.0.

</details>


### [7] [Design Space Exploration of DMA based Finer-Grain Compute Communication Overlap](https://arxiv.org/abs/2512.10236)
*Shagnik Pal,Shaizeen Aga,Suchita Pati,Mahzabeen Islam,Lizy K. John*

Main category: cs.DC

TL;DR: 提出FiCCO方法，通过更细粒度的计算-通信重叠来提升分布式ML训练/推理性能，相比传统分片级重叠能适应更多网络拓扑和细粒度数据流


<details>
  <summary>Details</summary>
Motivation: 现有分布式ML并行化技术中，数据依赖的通信和计算操作普遍存在，通信暴露导致性能损失高达1.7倍理想性能。传统分片级重叠虽然有效，但粒度较粗，限制了设计空间

Method: 提出FiCCO（Finer-grain Compute-Communication Overlap）方法，在比分片级更细的粒度上实现计算-通信重叠；分析操作分解带来的效率损失；设计FiCCO调度空间并建立效率特征映射；开发启发式算法选择定制化调度；利用GPU DMA引擎卸载通信以减少竞争

Result: 在真实ML部署场景中，提出的定制化调度方案实现了最高1.6倍的加速，启发式算法在81%的未见场景中提供准确指导

Conclusion: FiCCO通过更细粒度的计算-通信重叠扩展了设计空间，结合效率损失分析和启发式调度选择，能显著提升分布式ML系统性能，为框架和运行时提供实用指导

Abstract: As both ML training and inference are increasingly distributed, parallelization techniques that shard (divide) ML model across GPUs of a distributed system, are often deployed. With such techniques, there is a high prevalence of data-dependent communication and computation operations where communication is exposed, leaving as high as 1.7x ideal performance on the table. Prior works harness the fact that ML model state and inputs are already sharded, and employ careful overlap of individual computation/communication shards. While such coarse-grain overlap is promising, in this work, we instead make a case for finer-grain compute-communication overlap which we term FiCCO, where we argue for finer-granularity, one-level deeper overlap than at shard-level, to unlock compute/communication overlap for a wider set of network topologies, finer-grain dataflow and more. We show that FiCCO opens up a wider design space of execution schedules than possible at shard-level alone. At the same time, decomposition of ML operations into smaller operations (done in both shard-based and finer-grain techniques) causes operation-level inefficiency losses. To balance the two, we first present a detailed characterization of these inefficiency losses, then present a design space of FiCCO schedules, and finally overlay the schedules with concomitant inefficiency signatures. Doing so helps us design heuristics that frameworks and runtimes can harness to select bespoke FiCCO schedules based on the nature of underlying ML operations. Finally, to further minimize contention inefficiencies inherent with operation overlap, we offload communication to GPU DMA engines. We evaluate several scenarios from realistic ML deployments and demonstrate that our proposed bespoke schedules deliver up to 1.6x speedup and our heuristics provide accurate guidance in 81% of unseen scenarios.

</details>


### [8] [GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference](https://arxiv.org/abs/2512.09963)
*Phuong Tran,Tzu-Hao Liu,Long Tan Le,Tung-Anh Nguyen,Van Quan La,Eason Yu,Han Shu,Choong Seon Hong,Nguyen H. Tran*

Main category: cs.DC

TL;DR: GOODSPEED是一个分布式推理框架，通过自适应推测解码优化多用户LLM推理的好吞吐量，确保服务器间的公平性。


<details>
  <summary>Details</summary>
Motivation: LLM的高计算需求对实时推理构成挑战，特别是在多用户服务器推测解码和资源受限环境中。现有推测解码技术难以同时保证高好吞吐量（有效接受令牌率）和跨多个草稿服务器的公平性。

Method: GOODSPEED采用中心验证服务器协调异构草稿服务器的架构，使用梯度调度算法动态分配令牌验证任务，最大化对数效用函数以确保比例公平性，并行处理所有草稿服务器的推测输出。

Result: 通过严格的流体样本路径分析，GOODSPEED在稳态条件下收敛到最优好吞吐量分配，在动态工作负载下保持接近最优性能且有界误差，提供可扩展、公平且高效的分布式LLM推理解决方案。

Conclusion: GOODSPEED为多用户分布式LLM推理系统提供了可扩展、公平且高效的解决方案，通过自适应推测解码优化好吞吐量并确保服务器间的公平性。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet their high computational demands pose significant challenges for real-time inference, especially in multi-user server speculative decoding and resource-constrained environments. Speculative decoding has emerged as a promising technique to accelerate LLM inference by using lightweight draft models to generate candidate tokens, which are subsequently verified by a larger, more accurate model. However, ensuring both high goodput (the effective rate of accepted tokens) and fairness across multiple draft servers cooperating with a central verification server remains an open challenge. This paper introduces GOODSPEED, a novel distributed inference framework that optimizes goodput through adaptive speculative decoding. GOODSPEED employs a central verification server that coordinates a set of heterogeneous draft servers, each running a small language model to generate speculative tokens. To manage resource allocation effectively, GOODSPEED incorporates a gradient scheduling algorithm that dynamically assigns token verification tasks, maximizing a logarithmic utility function to ensure proportional fairness across servers. By processing speculative outputs from all draft servers in parallel, the framework enables efficient collaboration between the verification server and distributed draft generators, streamlining both latency and throughput. Through rigorous fluid sample path analysis, we show that GOODSPEED converges to the optimal goodput allocation in steady-state conditions and maintains near-optimal performance with provably bounded error under dynamic workloads. These results demonstrate that GOODSPEED provides a scalable, fair and efficient solution for multi- in distributed LLM inference systems.

</details>


### [9] [Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters](https://arxiv.org/abs/2512.10271)
*Shruti Dongare,Redwan Ibne Seraj Khan,Hadeel Albahar,Nannan Zhao,Diego Melendez Maita,Ali R. Butt*

Main category: cs.DC

TL;DR: RLTune是一个基于强化学习的深度学习任务调度框架，能够在异构GPU集群上动态优化任务优先级和资源分配，无需针对每个任务进行性能分析，显著提升GPU利用率并减少任务完成时间和排队延迟。


<details>
  <summary>Details</summary>
Motivation: 现代云平台承载大规模深度学习工作负载，需要高吞吐、低延迟的GPU调度。然而，GPU集群的异构性日益增强，以及对应用特性的有限可见性，给现有调度器带来重大挑战。现有调度器通常依赖离线性能分析或应用特定假设。

Method: RLTune采用应用无关的强化学习方法，结合RL驱动的优先级调度和基于混合整数线性规划（MILP）的任务到节点映射，优化系统级目标如任务完成时间、排队延迟和资源利用率。框架基于微软Philly、Helios和阿里巴巴的大规模生产轨迹进行训练。

Result: RLTune将GPU利用率提升高达20%，排队延迟降低高达81%，任务完成时间缩短高达70%。该框架能够泛化到多样化工作负载，无需针对每个任务进行性能分析。

Conclusion: RLTune为云提供商提供了一个实用的、可扩展的调度解决方案，能够实现更高效、公平和可持续的深度学习工作负载管理，克服了现有调度器在异构集群和应用特性可见性方面的限制。

Abstract: Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.

</details>


### [10] [High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments](https://arxiv.org/abs/2512.10312)
*Julian Rodriguez,Piotr Lopez,Emiliano Lerma,Rafael Medrano,Jacobo Hernandez*

Main category: cs.DC

TL;DR: 大数据课程实践报告，涵盖数据处理、文本分析、电影特征分析和Spark集群部署


<details>
  <summary>Details</summary>
Motivation: 记录大数据课程中的实践流程和方法论，展示从数据处理到分布式计算集群部署的完整学习过程

Method: 1. Epsilon数据集处理（小组和个人策略） 2. RestMex文本分析与分类 3. IMDb电影特征分析 4. 使用Scala在Linux上部署Apache Spark分布式计算集群

Result: 详细记录了大数据处理和分析的完整工作流程，包括数据集处理、文本分类、电影特征分析以及分布式计算集群的技术实现

Conclusion: 通过系统性的实践，掌握了大数据处理的全流程技术栈，从数据预处理到分布式计算集群部署，为大数据分析应用提供了完整的技术解决方案

Abstract: This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.

</details>


### [11] [Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability](https://arxiv.org/abs/2512.10425)
*Fan Yu,Guodong Li,Si Wu,Weijun Fang,Sihuang Hu*

Main category: cs.DC

TL;DR: 提出CP-LRCs（级联奇偶校验LRCs），通过将全局奇偶校验块分解到所有局部奇偶校验块中，实现宽条带下的高效修复，相比现有LRCs显著降低修复时间和带宽。


<details>
  <summary>Details</summary>
Motivation: 现有局部可修复编码（LRCs）在宽条带场景下存在结构限制：扩大的局部组增加单节点修复成本，多节点故障频繁触发昂贵的全局修复，可靠性急剧下降。根本原因是局部和全局奇偶校验块独立设计，无法在修复过程中协同工作。

Method: 提出CP-LRCs（级联奇偶校验LRCs），通过在局部奇偶校验块之间嵌入结构化依赖关系，将全局奇偶校验块分解到所有局部奇偶校验块中，形成级联奇偶校验组。提供通用的系数生成框架，开发利用级联特性的修复算法，并实例化为CP-Azure和CP-Uniform两种实现。

Result: 在阿里云上的评估显示，CP-LRCs相比现有LRCs，单节点故障修复时间最多减少41%，双节点故障修复时间最多减少26%，同时保持MDS级别的容错能力。

Conclusion: CP-LRCs通过级联奇偶校验设计解决了宽条带LRCs的关键限制，实现了局部和全局奇偶校验块的协同修复，显著提高了修复效率，为大规模存储系统提供了更优的擦除编码解决方案。

Abstract: Erasure coding with wide stripes is increasingly adopted to reduce storage overhead in large-scale storage systems. However, existing Locally Repairable Codes (LRCs) exhibit structural limitations in this setting: inflated local groups increase single-node repair cost, multi-node failures frequently trigger expensive global repair, and reliability degrades sharply. We identify a key root cause: local and global parity blocks are designed independently, preventing them from cooperating during repair. We present Cascaded Parity LRCs (CP-LRCs), a new family of wide stripe LRCs that embed structured dependency between parity blocks by decomposing a global parity block across all local parity blocks. This creates a cascaded parity group that preserves MDS-level fault tolerance while enabling low-bandwidth single-node and multi-node repairs. We provide a general coefficient-generation framework, develop repair algorithms exploiting cascading, and instantiate the design with CP-Azure and CP-Uniform. Evaluations on Alibaba Cloud show reductions in repair time of up to 41% for single-node failures and 26% for two-node failures.

</details>


### [12] [Clustered Federated Learning with Hierarchical Knowledge Distillation](https://arxiv.org/abs/2512.10443)
*Sabtain Ahmad,Meerzhan Kanatbekova,Ivona Brandic,Atakan Aral*

Main category: cs.DC

TL;DR: 提出CFLHKD方法，通过层次化聚类联邦学习和多教师知识蒸馏，在保护隐私的同时提升异构IoT环境中的模型性能


<details>
  <summary>Details</summary>
Motivation: 传统聚类联邦学习(CFL)存在学习碎片化问题，为每个聚类独立训练全局模型，无法利用跨聚类的集体知识，且通信效率有待提升

Method: 提出CFLHKD方法：采用层次化CFL架构，在边缘端训练聚类特定模型，在云端训练统一全局模型；通过多教师知识蒸馏实现跨聚类知识共享，同时保持聚类个性化；采用双层聚合机制连接本地和全局学习

Result: 在标准基准数据集上的广泛评估表明，CFLHKD在聚类特定模型和全局模型准确率上均优于代表性基线方法，性能提升达3.32-7.57%

Conclusion: CFLHKD通过层次化架构和多教师知识蒸馏，有效解决了传统CFL的碎片化学习问题，实现了跨聚类知识共享，在保持个性化的同时显著提升了模型性能

Abstract: Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\%.

</details>


### [13] [ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp](https://arxiv.org/abs/2512.10576)
*Xinhang Chen,Chao Zhang,Jiahuan He,Wei Liu,Jianming Zhang,Wenlong Zhou,Xiao Li,Pai Zeng,Shiyong Li,Yuanpan Qian,Dong Li,Zhaogeng Li*

Main category: cs.DC

TL;DR: ESS系统通过将Latent-Cache卸载到CPU内存，解决了DeepSeek-V3.2-Exp在长上下文推理中的GPU内存瓶颈，显著提升了Decode阶段吞吐量。


<details>
  <summary>Details</summary>
Motivation: DeepSeek-V3.2-Exp虽然通过稀疏注意力机制降低了长上下文推理延迟，但Decode阶段仍然是主要瓶颈。问题源于Latent-Cache随序列长度线性增长与GPU内存容量有限之间的矛盾，这限制了批处理大小，从而抑制了Decode阶段吞吐量。

Method: 提出ESS（Extended Sparse Server）系统，采用卸载中心设计：1）选择性将Latent-Cache卸载到CPU内存；2）保留延迟关键组件在GPU上；3）通过释放GPU内存，使批处理大小扩展与GPU内存约束解耦。

Result: 高保真模拟显示：在32K上下文长度下，ESS提供69.4%的吞吐量提升；在128K上下文长度下，吞吐量提升高达123%。这证明了ESS在大上下文推理工作负载中的有效性。

Conclusion: ESS是一个实用且可扩展的解决方案，能够显著提升长上下文LLM服务的Decode阶段吞吐量，从而降低实际部署成本。

Abstract: DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.
  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.
  Our high-fidelity simulations show that ESS delivers 69.4\% throughput improvement at 32K context length and up to 123\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [Algorithm-Driven On-Chip Integration for High Density and Low Cost](https://arxiv.org/abs/2512.10089)
*Jeongeun Kim,Sabrina Yarzada,Paul Chen,Christopher Torng*

Main category: cs.AR

TL;DR: 提出一种新的半导体多项目晶圆集成方法，通过自动化布局、窄区域互连架构和片上电源域技术，实现大规模独立硬件设计的高效集成，相比现有方法面积减少高达13倍。


<details>
  <summary>Details</summary>
Motivation: 随着半导体人才培养需求的增长，需要支持大量独立硬件设计的研究和培训平台。传统的多项目晶圆服务基于物理共置，在项目数量增加时扩展性受限。现有方法尝试将多个小设计集成到共享芯片中，但缺乏系统性的布局、连接和验证原则。

Method: 提出三种关键技术：1) 建立结构化设计空间公式化，实现自动化算法驱动的项目打包，替代手动布局；2) 引入仅利用设计站点间窄区域进行片外通信和其他共享需求的架构；3) 提供片上电源域实用方法，支持每个项目在标准实验室工作台上进行电源特性分析，无需低功耗ASIC设计专业知识。

Result: 实验结果显示，该方法相比最先进的纯物理聚合方法实现了高达13倍的面积减少，为大规模流片环境提供了可扩展且经济高效的解决方案。

Conclusion: 该工作填补了高密度集成设计站点布局、连接和验证的系统性研究空白，通过自动化、窄区域互连和电源域管理技术，为大规模半导体人才培养和研究提供了可扩展的流片平台。

Abstract: Growing interest in semiconductor workforce development has generated demand for platforms capable of supporting large numbers of independent hardware designs for research and training without imposing high per-project overhead. Traditional multi-project wafer (MPW) services based solely on physical co-placement have historically met this need, yet their scalability breaks down as project counts rise. Recent efforts towards scalable chip tapeouts mitigate these limitations by integrating many small designs within a shared die and attempt to amortize costly resources such as IO pads and memory macros. However, foundational principles for arranging, linking, and validating such densely integrated design sites have received limited systematic investigation. This work presents a new approach with three key techniques to address this gap. First, we establish a structured formulation of the design space that enables automated, algorithm-driven packing of many projects, replacing manual layout practices. Second, we introduce an architecture that exploits only the narrow-area regions between sites to deliver on off-chip communication and other shared needs. Third, we provide a practical approach for on-chip power domains enabling per-project power characterization at a standard laboratory bench and requiring no expertise in low-power ASIC design. Experimental results show that our approach achieves substantial area reductions of up to 13x over state-of-the-art physical-only aggregation methods, offering a scalable and cost-effective path forward for large-scale tapeout environments.

</details>


### [15] [A Vertically Integrated Framework for Templatized Chip Design](https://arxiv.org/abs/2512.10155)
*Jeongeun Kim,Christopher Torng*

Main category: cs.AR

TL;DR: 提出一种从高级面向对象软件规范生成芯片的方法，为入门级芯片设计学习者提供软件到硬件的直观映射，降低软件开发者参与芯片设计的门槛。


<details>
  <summary>Details</summary>
Motivation: 软件开发者通常难以将定制硬件集成到应用中，尽管专用芯片能为机器学习和AI等领域带来显著优势。需要降低芯片设计门槛，让软件开发者能够参与芯片创建。

Method: 采用模块化构建策略，将软件对象映射为芯片上的对应区域，实现一一对应的结构映射。使用基于序列的形式类型系统检查硬件模块间的通信模式是否符合软件模型描述，并开发适合这种对象对齐设计风格的布局技术。

Result: 实现了从软件到芯片设计的心理连续性保持，为新手学习者提供了直观的设计流程，并实现了实用的布局生成能力。

Conclusion: 该方法降低了软件开发者参与芯片设计所需的专业知识，通过保持软件抽象在硬件设计流程中的连续性，使软件开发者能够更轻松地创建定制芯片。

Abstract: Developers who primarily engage with software often struggle to incorporate custom hardware into their applications, even though specialized silicon can provide substantial benefits to machine learning and AI, as well as to the application domains that they enable. This work investigates how a chip can be generated from a high-level object-oriented software specification, targeting introductory-level chip design learners with only very light performance requirements, while maintaining mental continuity between the chip layout and the software source program. In our approach, each software object is represented as a corresponding region on the die, producing a one-to-one structural mapping that preserves these familiar abstractions throughout the design flow. To support this mapping, we employ a modular construction strategy in which vertically composed IP blocks implement the behavioral protocols expressed in software. A direct syntactic translation, however, cannot meet hardware-level efficiency or communication constraints. For this reason, we leverage formal type systems based on sequences that check whether interactions between hardware modules adhere to the communication patterns described in the software model. We further examine hardware interconnect strategies for composing many such modules and develop layout techniques suited to this object-aligned design style. Together, these contributions preserve mental continuity from software to chip design for new learners and enables practical layout generation, ultimately reducing the expertise required for software developers to participate in chip creation.

</details>


### [16] [Neuromorphic Processor Employing FPGA Technology with Universal Interconnections](https://arxiv.org/abs/2512.10180)
*Pracheta Harlikar,Abdel-Hameed A. Badawy,Prasanna Date*

Main category: cs.AR

TL;DR: 在Xilinx Zynq-7000 FPGA上实现低成本、开源的神经形态处理器，支持可配置连接和LIF神经元模型，通过UART接口实现运行时重配置，验证了Iris和MNIST数据集分类性能。


<details>
  <summary>Details</summary>
Motivation: 神经形态计算具有超低功耗和实时推理的潜力，但缺乏灵活的开源平台阻碍了其广泛应用和实验研究。

Method: 在Xilinx Zynq-7000 FPGA平台上实现神经形态处理器，采用可配置的全连接结构和LIF神经元模型，支持阈值、突触权重、不应期等参数自定义，通过UART接口与主机通信实现运行时重配置。

Result: 使用Iris分类和MNIST数字识别基准数据集验证了架构有效性，综合后结果显示了设计的能效和可扩展性，证明了其作为研究级神经形态平台的可行性。

Conclusion: 该低成本、开源的神经形态处理器为脉冲神经网络应用提供了可访问且适应性强的研究平台，项目完成后将开源发布。

Abstract: Neuromorphic computing, inspired by biological neural systems, holds immense promise for ultra-low-power and real-time inference applications. However, limited access to flexible, open-source platforms continues to hinder widespread adoption and experimentation. In this paper, we present a low-cost neuromorphic processor implemented on a Xilinx Zynq-7000 FPGA platform. The processor supports all-to-all configurable connectivity and employs the leaky integrate-and-fire (LIF) neuron model with customizable parameters such as threshold, synaptic weights, and refractory period. Communication with the host system is handled via a UART interface, enabling runtime reconfiguration without hardware resynthesis. The architecture was validated using benchmark datasets including the Iris classification and MNIST digit recognition tasks. Post-synthesis results highlight the design's energy efficiency and scalability, establishing its viability as a research-grade neuromorphic platform that is both accessible and adaptable for real-world spiking neural network applications. This implementation will be released as open source following project completion.

</details>


### [17] [SemanticBBV: A Semantic Signature for Cross-Program Knowledge Reuse in Microarchitecture Simulation](https://arxiv.org/abs/2512.10231)
*Zhenguo Liu,Chengao Shi,Chen Ding,Jiang Xu*

Main category: cs.AR

TL;DR: SemanticBBV：一个两阶段框架，通过语义编码和集合变换器生成性能感知的程序签名，实现跨程序模拟重用，相比传统BBV方法获得7143倍加速


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的微架构模拟使用基本块向量（BBV）作为程序表示，但BBV存在两个根本限制：1）顺序依赖的ID阻碍跨程序知识重用；2）缺乏预测硬件性能的语义内容，导致大量优化潜力未被开发

Method: 两阶段框架：1）轻量级RWKV语义编码器将汇编基本块转换为丰富的BBE嵌入；2）顺序不变的集合变换器聚合BBE（按执行频率加权）生成最终签名，通过三重损失和CPI回归任务联合训练，使签名具有性能敏感性

Result: 仅模拟14个通用程序点即可估计10个SPEC CPU基准测试的性能，平均准确率达86.3%，实现7143倍模拟加速；签名对新微架构具有强适应性，只需少量微调

Conclusion: SemanticBBV解决了传统BBV的根本限制，不仅保持单程序准确性，还实现前所未有的跨程序分析能力，显著加速微架构模拟，为性能感知的程序签名开辟新方向

Abstract: For decades, sampling-based techniques have been the de facto standard for accelerating microarchitecture simulation, with the Basic Block Vector (BBV) serving as the cornerstone program representation. Yet, the BBV's fundamental limitations: order-dependent IDs that prevent cross-program knowledge reuse and a lack of semantic content predictive of hardware performance have left a massive potential for optimization untapped.
  To address these gaps, we introduce SemanticBBV, a novel, two-stage framework that generates robust, performance-aware signatures for cross-program simulation reuse. First, a lightweight RWKV-based semantic encoder transforms assembly basic blocks into rich Basic Block Embeddings (BBEs), capturing deep functional semantics. Second, an order-invariant Set Transformer aggregates these BBEs, weighted by execution frequency, into a final signature. Crucially, this stage is co-trained with a dual objective: a triplet loss for signature distinctiveness and a Cycles Per Instruction (CPI) regression task, directly imbuing the signature with performance sensitivity. Our evaluation demonstrates that SemanticBBV not only matches traditional BBVs in single-program accuracy but also enables unprecedented cross-program analysis. By simulating just 14 universal program points, we estimated the performance of ten SPEC CPU benchmarks with 86.3% average accuracy, achieving a 7143x simulation speedup. Furthermore, the signature shows strong adaptability to new microarchitectures with minimal fine-tuning.

</details>
