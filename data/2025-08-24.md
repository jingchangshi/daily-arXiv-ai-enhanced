<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 8]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Homomorphism Calculus for User-Defined Aggregations](https://arxiv.org/abs/2508.15109)
*Ziteng Wang,Ruijie Fang,Linus Zheng,Dixin Tang,Isil Dillig*

Main category: cs.PL

TL;DR: 这篇论文提出了一种新的同态演算法，用于验证和否定UDAF是否满足同态性质，并为满足条件的函数构造合并运算符以支持增量计算和并行执行。


<details>
  <summary>Details</summary>
Motivation: 虽然Spark和Flink等框架支持用户定义聚合函数，但要实现高效执行需要函数满足同态性质，这是当前的技术挑战。

Method: 设计了一种新的同态演算法，能够自动化地验证或否定UDAF的同态性质，并为满足条件的函数生成相应的合并运算符。

Result: 在真实世界的UDAF上评估证明，该方法在性能上显著超过了两个领先的综合器。

Conclusion: 该同态演算法为数据处理框架提供了一种有效的方法来验证和构造高效的聚合函数，从而支持更好的增量计算和并行执行能力。

Abstract: Data processing frameworks like Apache Spark and Flink provide built-in
support for user-defined aggregation functions (UDAFs), enabling the
integration of domain-specific logic. However, for these frameworks to support
\emph{efficient} UDAF execution, the function needs to satisfy a
\emph{homomorphism property}, which ensures that partial results from
independent computations can be merged correctly. Motivated by this problem,
this paper introduces a novel \emph{homomorphism calculus} that can both verify
and refute whether a UDAF is a dataframe homomorphism. If so, our calculus also
enables the construction of a corresponding merge operator which can be used
for incremental computation and parallel execution. We have implemented an
algorithm based on our proposed calculus and evaluate it on real-world UDAFs,
demonstrating that our approach significantly outperforms two leading
synthesizers.

</details>


### [2] [Software Model Checking via Summary-Guided Search (Extended Version)](https://arxiv.org/abs/2508.15137)
*Ruijie Fang,Zachary Kincaid,Thomas Reps*

Main category: cs.PL

TL;DR: GPS是一种新的软件模型检测算法，结合了定向搜索和静态分析，能够高效发现程序安全证明和反例，在性能上优于现有最先进的模型检测器。


<details>
  <summary>Details</summary>
Motivation: 现有的软件模型检测方法在处理具有长输入依赖错误路径的程序时效率不高，需要一种既能证明安全性又能高效发现错误的新方法。

Method: GPS采用组合式、基于摘要的静态分析来指导程序状态的定向搜索，使用静态分析摘要剪除不可行路径并驱动测试生成，采用双层搜索策略和插桩技术实现反驳完备性。

Result: 在包括SV-COMP和先前文献的基准测试中，GPS在解决的基准数量和运行时间方面都优于最先进的软件模型检测器（包括SV-COMP ReachSafety-Loops类别的顶级性能工具）。

Conclusion: GPS通过结合静态分析和定向搜索，实现了高效的错误检测和安全证明，特别擅长处理具有长输入依赖错误路径的程序，同时保持了反驳完备性。

Abstract: In this work, we describe a new software model-checking algorithm called GPS.
GPS treats the task of model checking a program as a directed search of the
program states, guided by a compositional, summary-based static analysis. The
summaries produced by static analysis are used both to prune away infeasible
paths and to drive test generation to reach new, unexplored program states. GPS
can find both proofs of safety and counter-examples to safety (i.e., inputs
that trigger bugs), and features a novel two-layered search strategy that
renders it particularly efficient at finding bugs in programs featuring long,
input-dependent error paths. To make GPS refutationally complete (in the sense
that it will find an error if one exists, if it is allotted enough time), we
introduce an instrumentation technique and show that it helps GPS achieve
refutation-completeness without sacrificing overall performance. We benchmarked
GPS on a suite of benchmarks including both programs from the Software
Verification Competition (SV-COMP) and from prior literature, and found that
our implementation of GPS outperforms state-of-the-art software model checkers
(including the top performers in SV-COMP ReachSafety-Loops category), both in
terms of the number of benchmarks solved and in terms of running time.

</details>


### [3] [Big-Stop Semantics: A Simple Way to Get the Benefits of Small-Step Semantics in a Big-Step Judgment](https://arxiv.org/abs/2508.15157)
*David M Kahn,Jan Hoffmann,Runming Li*

Main category: cs.PL

TL;DR: 大步步法与小步步法的结合，通过归纳定义扩展大步步法以捕获分散计算，保持大步步法的简洁性同时获得小步步法的完整性。


<details>
  <summary>Details</summary>
Motivation: 大步步法在实践中更受欢迎，因为它更简洁且直接显示计算结果，但缺乏对分散计算的描述能力。需要找到一种方法在保留大步步法优点的同时扩展其能力。

Method: 提出大停步步法，通过归纳定义扩展标准大步步法的推理规则，定义一种与小步步法反射-传递闭包等价的评估判断。在类型化、非类型化和效果化PCF以及命令式语言中进行实验。

Result: 大停步步法能够捕获分散计算而不引入错误状态，且仅需添加少量额外规则。这种方法比其他解决方案更简洁，避免了全局状态和循环归纳等复杂的推理原则。

Conclusion: 大停步步法提供了一种简洁而强大的方法，在保持大步步法优雅性的同时获得了小步步法的完整性，为程序语言语义描述提供了更好的工具。

Abstract: As evident in the programming language literature, many practitioners favor
specifying dynamic program behavior using big-step over small-step semantics.
Unlike small-step semantics, which must dwell on every intermediate program
state, big-step semantics conveniently jump directly to the ever-important
result of the computation. Big-step semantics also typically involve fewer
inference rules than their small-step counterparts. However, in exchange for
ergonomics, big-step semantics give up power: Small-step semantics describes
program behaviors that are outside the grasp of big-step semantics, notably
divergence. This work presents a little-known extension of big-step semantics
with inductive definitions that captures diverging computations without
introducing error states. This big-stop semantics is illustrated for typed,
untyped, and effectful variants of PCF, as well as a while-loop-based
imperative language. Big-stop semantics extends the standard big-step inference
rules with a few additional rules to define an evaluation judgment that is
equivalent to the reflexive-transitive closure of small-step transitions. This
simple extension contrasts with other solutions in the literature which
sacrifice ergonomics by introducing many additional inference rules, global
state, and/or less-commonly-understood reasoning principles like coinduction.

</details>


### [4] [Probabilistic Inference for Datalog with Correlated Inputs](https://arxiv.org/abs/2508.15166)
*Jingbo Wang,Shashin Halalingaiah,Weiyi Chen,Chao Wang,Isil Dillig*

Main category: cs.PL

TL;DR: Praline是Datalog的新扩展，用于处理输入事实间统计相关性下的精确概率推理，通过约束优化和δ-精确算法提供可扩展的紧致概率边界。


<details>
  <summary>Details</summary>
Motivation: 现有的概率逻辑编程语言（如ProbLog）在评估输出关系概率时未考虑输入事实间可能存在的统计相关性，这限制了推理的精确性。

Method: 将推理任务建模为约束优化问题，提出δ-精确推理算法，结合约束求解、静态分析和迭代优化来提高可扩展性。

Result: 在包括侧信道分析在内的真实基准测试中，该方法不仅有效扩展，还能提供紧致的概率边界。

Conclusion: Praline通过处理输入相关性并采用高效算法，实现了可扩展的精确概率推理，为复杂应用提供了可靠的概率边界估计。

Abstract: Probabilistic extensions of logic programming languages, such as ProbLog,
integrate logical reasoning with probabilistic inference to evaluate
probabilities of output relations; however, prior work does not account for
potential statistical correlations among input facts. This paper introduces
Praline, a new extension to Datalog designed for precise probabilistic
inference in the presence of (partially known) input correlations. We formulate
the inference task as a constrained optimization problem, where the solution
yields sound and precise probability bounds for output facts. However, due to
the complexity of the resulting optimization problem, this approach alone often
does not scale to large programs. To address scalability, we propose a more
efficient $\delta$-exact inference algorithm that leverages constraint solving,
static analysis, and iterative refinement. Our empirical evaluation on
challenging real-world benchmarks, including side-channel analysis,
demonstrates that our method not only scales effectively but also delivers
tight probability bounds.

</details>


### [5] [Exploring the Theory and Practice of Concurrency in the Entity-Component-System Pattern](https://arxiv.org/abs/2508.15264)
*Patrick Redmond,Jonathan Castello,José Manuel Calderón Trilla,Lindsey Kuper*

Main category: cs.PL

TL;DR: 本文提出了一个名为Core ECS的正式模型，用于抽象实体-组件-系统（ECS）设计模式的本质，并识别出一类具有确定性行为的ECS程序，无论调度如何都能保持确定性。


<details>
  <summary>Details</summary>
Motivation: ECS模式虽然在游戏开发中广泛使用，但在其他领域认知度不高，现有解释往往局限于具体框架细节或不完善的比喻。需要建立一个严谨的形式化模型来理解ECS模式的本质。

Method: 设计了一个名为Core ECS的正式模型，抽象出具体实现的细节，揭示ECS软件的本质。通过该模型识别出具有确定性行为的ECS程序类别。

Result: 发现了一类Core ECS程序无论调度如何都能保持确定性行为，使ECS模式可以作为确定性构造的并发编程模型使用。调查发现现有ECS框架都未能充分利用确定性并发的机会。

Conclusion: 研究结果指出了新的ECS实现技术的空间，可以更好地利用确定性并发的机会，为ECS模式在更广泛领域的应用提供了理论基础。

Abstract: The Entity-Component-System (ECS) software design pattern, long used in game
development, encourages a clean separation of identity (entities), data
properties (components), and computational behaviors (systems). Programs
written using the ECS pattern are naturally concurrent, and the pattern offers
modularity, flexibility, and performance benefits that have led to a
proliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known
and not well understood outside of a few domains. Existing explanations of the
ECS pattern tend to be mired in the concrete details of particular ECS
frameworks, or they explain the pattern in terms of imperfect metaphors or in
terms of what it is not. We seek a rigorous understanding of the ECS pattern
via the design of a formal model, Core ECS, that abstracts away the details of
specific implementations to reveal the essence of software using the ECS
pattern. We identify a class of Core ECS programs that behave deterministically
regardless of scheduling, enabling use of the ECS pattern as a
deterministic-by-construction concurrent programming model. With Core ECS as a
point of comparison, we then survey several real-world ECS frameworks and find
that they all leave opportunities for deterministic concurrency unexploited.
Our findings point out a space for new ECS implementation techniques that
better leverage such opportunities.

</details>


### [6] [Fair Termination for Resource-Aware Active Objects](https://arxiv.org/abs/2508.15333)
*Francesco Dagnino,Paola Giannini,Violet Ka I Pun,Ulises Torrella*

Main category: cs.PL

TL;DR: 开发了一个资源感知的主动对象核心演算和类型系统，确保良好类型化程序能够公平终止


<details>
  <summary>Details</summary>
Motivation: 主动对象系统用于分布式系统和业务流程建模，需要资源感知的形式化方法来处理并发和资源管理

Method: 结合分级语义和类型系统技术（适用于顺序程序）与公平终止技术（适用于同步会话）

Result: 提出了一个核心演算和类型系统，能够保证程序的公平终止性

Conclusion: 成功将顺序程序的分级语义技术与同步会话的公平终止技术相结合，为资源感知主动对象模型提供了形式化基础

Abstract: Active object systems are a model of distributed computation that has been
adopted for modelling distributed systems and business process workflows. This
field of modelling is, in essence, concurrent and resource-aware, motivating
the development of resource-aware formalisations on the active object model.
The contributions of this work are the development of a core calculus for
resource-aware active objects together with a type system ensuring that
well-typed programs are fairly terminating, i.e., they can always eventually
terminate. To achieve this, we combine techniques from graded semantics and
type systems, which are quite well understood for sequential programs, with
those for fair termination, which have been developed for synchronous~sessions.

</details>


### [7] [Compositional Symbolic Execution for the Next 700 Memory Models (Extended Version)](https://arxiv.org/abs/2508.15576)
*Andreas Lööw,Seung Hoon Park,Daniele Nantes-Sobrinho,Sacha-Élie Ayoun,Opale Sjöstedt,Philippa Gardner*

Main category: cs.PL

TL;DR: 本文为内存模型参数化组合符号执行平台提供了新的形式化基础，支持多种内存模型和分离逻辑/不正确性分离逻辑分析


<details>
  <summary>Details</summary>
Motivation: 现有组合符号执行工具缺乏对内存模型参数化的满意形式化基础，限制了平台的灵活性、语言支持范围和性能优化

Method: 基于Gillian平台的启发，在Rocq定理证明器中机械化形式化基础，支持C和CHERI等多种内存模型，涵盖SL和ISL分析

Result: 建立了首个同时支持SL和ISL分析的内存模型参数化形式化框架，验证了多种内存模型的实例化

Conclusion: 该形式化基础为组合符号执行平台提供了更灵活、标准化的内存模型支持，有助于提升多语言分析和工具互操作性

Abstract: Multiple successful compositional symbolic execution (CSE) tools and
platforms exploit separation logic (SL) for compositional verification and/or
incorrectness separation logic (ISL) for compositional bug-finding, including
VeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian
platform, the only CSE platform that is parametric on the memory model, meaning
that it can be instantiated to different memory models, suggests that the
ability to use custom memory models allows for more flexibility in supporting
analysis of a wide range of programming languages, for implementing custom
automation, and for improving performance. However, the literature lacks a
satisfactory formal foundation for memory-model-parametric CSE platforms.
  In this paper, inspired by Gillian, we provide a new formal foundation for
memory-model-parametric CSE platforms. Our foundation advances the state of the
art in four ways. First, we mechanise our foundation (in the interactive
theorem prover Rocq). Second, we validate our foundation by instantiating it to
a broad range of memory models, including models for C and CHERI. Third,
whereas previous memory-model-parametric work has only covered SL analyses, we
cover both SL and ISL analyses. Fourth, our foundation is based on standard
definitions of SL and ISL (including definitions of function specification
validity, to ensure sound interoperation with other tools and platforms also
based on standard definitions).

</details>


### [8] [Active Learning for Neurosymbolic Program Synthesis](https://arxiv.org/abs/2508.15750)
*Celeste Barnaby,Qiaochu Chen,Ramya Ramalingam,Osbert Bastani,Isil Dillig*

Main category: cs.PL

TL;DR: 提出了一种新的主动学习方法SmartLabel，通过约束符合性评估(CCE)处理神经符号程序合成中的神经网络误预测问题，显著提高了合成准确率


<details>
  <summary>Details</summary>
Motivation: 传统主动学习方法在神经符号程序合成中由于神经网络误预测会导致返回错误程序，需要新的技术来处理这一独特挑战

Method: 基于约束符合性评估(CCE)的新策略，考虑神经误预测和用户反馈，迭代优化CCE精度直到所有剩余程序保证观测等价

Result: SmartLabel在三个神经符号领域中98%的基准测试识别出真实程序，平均需要不到5轮用户交互，而先前技术最多只能达到65%

Conclusion: 该方法有效解决了神经符号程序合成中的主动学习挑战，显著提升了合成准确性和用户交互效率

Abstract: The goal of active learning for program synthesis is to synthesize the
desired program by asking targeted questions that minimize user interaction.
While prior work has explored active learning in the purely symbolic setting,
such techniques are inadequate for the increasingly popular paradigm of
neurosymbolic program synthesis, where the synthesized program incorporates
neural components. When applied to the neurosymbolic setting, such techniques
can -- and, in practice, do -- return an unintended program due to
mispredictions of neural components. This paper proposes a new active learning
technique that can handle the unique challenges posed by neural network
mispredictions. Our approach is based upon a new evaluation strategy called
constrained conformal evaluation (CCE), which accounts for neural
mispredictions while taking into account user-provided feedback. Our proposed
method iteratively makes CCE more precise until all remaining programs are
guaranteed to be observationally equivalent. We have implemented this method in
a tool called SmartLabel and experimentally evaluated it on three neurosymbolic
domains. Our results demonstrate that SmartLabel identifies the ground truth
program for 98% of the benchmarks, requiring under 5 rounds of user interaction
on average. In contrast, prior techniques for active learning are only able to
converge to the ground truth program for at most 65% of the benchmarks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [9] [Declarative Data Pipeline for Large Scale ML Services](https://arxiv.org/abs/2508.15105)
*Yunzhao Yang,Runhui Wang,Xuanqing Liu,Adit Krishnan,Yefan Tao,Yuqian Deng,Kuangyou Yao,Peiyuan Sun,Henrik Johnson,Aditi sinha,Davor Golac,Gerald Friedland,Usman Shakeel,Daryl Cooke,Joe Sullivan,Chris Kong*

Main category: cs.DC

TL;DR: 声明式数据管道架构通过模块化设计在Apache Spark中集成机器学习功能，实现了系统性能和开发效率的重大提升


<details>
  <summary>Details</summary>
Motivation: 解决分布式数据处理系统在维护性、开发效率和机器学习集成方面的挑战，特别是在大规模协作环境中的高通信开销和复杂协调问题

Method: 提出了一种新的"声明式数据管道"架构，采用模块化框架在Apache Spark中集成机器学习功能，通过逻辑计算单元（Pipes）替代传统微服务方案，建立清晰组件边界和标准接口

Result: 企业案例显示：开发效率提升50%，协作/故障排除从周缩减到天，性能提升500倍扩展性和10倍吞吐量。学术实验证明通用性至少5.7倍更快，CPU利用率达99%

Conclusion: 该架构通过架构决策、实现策略和性能优化，为构建可扩展、可维护的数据处理系统提供了有价值的见解，有效平衡了系统性能和开发速度

Abstract: Modern distributed data processing systems face significant challenges in
balancing system performance with code maintainability and developer
productivity, particularly when integrating machine learning capabilities at
scale. In large collaborative environments, these challenges are amplified by
high communication overhead between teams and the complexity of coordinating
development across multiple groups. This paper presents a novel "Declarative
Data Pipeline" architecture that addresses these challenges while processing
billions of records with high accuracy and efficiency. Our architecture
introduces a modular framework that seamlessly integrates machine learning
capabilities within Apache Spark by combining logical computation units that we
refer as Pipes, departing from traditional microservice-based approaches. By
establishing clear component boundaries and standardized interfaces, we achieve
both modularity and system optimization without sacrificing maintainability.
The enterprise case study demonstrate substantial improvements in multiple
dimensions: development efficiency improved by 50%,
collaboration/troubleshooting efforts compressed from weeks to days,
performance improved by 500x in scalability and by 10x in throughput. The
academic experiment also proves at least 5.7x faster in throughput with 99% CPU
utilization than non-framework implementations. This paper details the
architectural decisions, implementation strategies, and performance
optimizations that enable these improvements, providing insights for building
scalable, maintainable data processing systems that effectively balance system
performance with development velocity.

</details>


### [10] [Databelt: A Continuous Data Path for Serverless Workflows in the 3D Compute Continuum](https://arxiv.org/abs/2508.15351)
*Cynthia Marcelino,Leonard Guelmino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Databelt是一个面向3D计算连续体动态环境的无服务器工作流状态管理框架，通过SLO感知状态传播和函数状态融合机制，显著降低状态访问延迟和网络传输开销


<details>
  <summary>Details</summary>
Motivation: 在3D边缘-云-空间计算连续体的动态环境中，无服务器函数依赖远程存储服务会导致高延迟和网络通信开销，特别是在卫星频繁进出地面站范围导致网络拓扑变化时，函数需要多跳访问云服务

Method: Databelt引入SLO感知状态传播机制，主动将函数状态卸载到最合适的节点；采用函数状态融合机制，为共享同一运行时的函数抽象状态管理，减少冗余网络和存储操作

Result: 实验结果显示，Databelt将工作流执行时间减少高达66%，吞吐量提高50%；函数状态融合将存储操作延迟降低20%，通过减少同一运行时的重复存储请求

Conclusion: Databelt框架有效解决了动态网络环境中无服务器工作流的状态管理挑战，通过智能状态传播和融合机制，确保了在3D计算连续体等高度动态环境中的高效执行

Abstract: Typically, serverless functions rely on remote storage services for managing
state, which can result in increased latency and network communication
overhead. In a dynamic environment such as the 3D (Edge-Cloud-Space) Compute
Continuum, serverless functions face additional challenges due to frequent
changes in network topology. As satellites move in and out of the range of
ground stations, functions must make multiple hops to access cloud services,
leading to high-latency state access and unnecessary data transfers. In this
paper, we present Databelt, a state management framework for serverless
workflows designed for the dynamic environment of the 3D Compute Continuum.
Databelt introduces an SLO-aware state propagation mechanism that enables the
function state to move continuously in orbit. Databelt proactively offloads
function states to the most suitable node, such that when functions execute,
the data is already present on the execution node or nearby, thus minimizing
state access latency and reducing the number of network hops. Additionally,
Databelt introduces a function state fusion mechanism that abstracts state
management for functions sharing the same serverless runtime. When functions
are fused, Databelt seamlessly retrieves their state as a group, reducing
redundant network and storage operations and improving overall workflow
efficiency. Our experimental results show that Databelt reduces workflow
execution time by up to 66% and increases throughput by 50% compared to the
baselines. Furthermore, our results show that Databelt function state fusion
reduces storage operations latency by up to 20%, by reducing repetitive storage
requests for functions within the same runtime, ensuring efficient execution of
serverless workflows in highly dynamic network environments such as the 3D
Continuum.

</details>


### [11] [Universal Dancing by Luminous Robots under Sequential Schedulers](https://arxiv.org/abs/2508.15484)
*Caterina Feletti,Paola Flocchini,Debasish Pattanayak,Giuseppe Prencipe,Nicola Santoro*

Main category: cs.DC

TL;DR: 本文提出了一种在LUMI模型下解决通用舞蹈问题的新方法，通过分布式计数器机制实现任意模式的序列化编排，突破了传统舞蹈问题中的对称性和周期性限制。


<details>
  <summary>Details</summary>
Motivation: 传统舞蹈问题对模式序列和初始配置有严格限制（如重复模式、周期性、对称性等），这些限制在实际应用中可能过于严格。本文旨在探索如何在LUMI模型下消除这些限制，实现更通用的舞蹈编排。

Method: 采用LUMI模型（机器人配备有限颜色调色板的灯光），在顺序调度器下实现分布式计数器机制。算法利用机器人灯光颜色来编码状态信息，通过非刚性移动确保空间同质性。

Result: 证明了在LUMI模型下，通用舞蹈问题是可解的，但可行编排序列的长度受限于可用颜色数量对n的组合。提供了具体的算法实现，能够从任意初始配置开始执行任意模式序列。

Conclusion: LUMI模型配合顺序调度器能够有效解决通用舞蹈问题，突破了传统限制。分布式计数器机制是实现这一目标的关键技术，为自主移动机器人的复杂协同任务提供了新的解决方案。

Abstract: The Dancing problem requires a swarm of $n$ autonomous mobile robots to form
a sequence of patterns, aka perform a choreography. Existing work has proven
that some crucial restrictions on choreographies and initial configurations
(e.g., on repetitions of patterns, periodicity, symmetries,
contractions/expansions) must hold so that the Dancing problem can be solved
under certain robot models. Here, we prove that these necessary constraints can
be dropped by considering the LUMI model (i.e., where robots are endowed with a
light whose color can be chosen from a constant-size palette) under the quite
unexplored sequential scheduler. We formalize the class of Universal Dancing
problems which require a swarm of $n$ robots starting from any initial
configuration to perform a (periodic or finite) sequence of arbitrary patterns,
only provided that each pattern consists of $n$ vertices (including
multiplicities). However, we prove that, to be solvable under LUMI, the length
of the feasible choreographies is bounded by the compositions of $n$ into the
number of colors available to the robots. We provide an algorithm solving the
Universal Dancing problem by exploiting the peculiar capability of sequential
robots to implement a distributed counter mechanism. Even assuming non-rigid
movements, our algorithm ensures spatial homogeneity of the performed
choreography.

</details>


### [12] [Lower Bounds for $k$-Set Agreement in Fault-Prone Networks](https://arxiv.org/abs/2508.15562)
*Pierre Fraigniaud,Minh Hang Nguyen,Ami Paz,Ulrich Schmid,Hugo Rincon Galeana*

Main category: cs.DC

TL;DR: 本文提出了同步消息传递系统中k-set agreement问题的新下界，推广了先前在完全网络和任意无向网络中的已知结果，并提供了基于拓扑证明的简洁方法。


<details>
  <summary>Details</summary>
Motivation: 现有的k-set agreement下界主要针对完全网络或特定网络拓扑，缺乏对任意有向通信网络的通用下界分析。本文旨在填补这一空白，为任意网络拓扑提供统一的下界理论框架。

Method: 采用拓扑证明方法，基于一系列可壳化载体映射，从可壳化输入复形出发分析协议复形的演化。在前t/k轮使用每轮崩溃k个进程的载体映射保持高连通性，之后采用新颖载体映射维持连通性，并运用Sperner引理式论证。

Result: 成功推导出任意有向通信网络中k-set agreement的新下界，推广了Chaudhuri等人的t/k+1下界，并统一了Castaneda和Fraigniaud等人的无向网络结果。同时证明了可以使用基于Kuhn三角剖分的指数级更小输入复形。

Conclusion: 该研究为分布式系统中k-set agreement问题提供了更通用的下界理论，拓扑证明方法简洁有效，且输入复形的优化显著降低了证明复杂度，对分布式计算理论有重要贡献。

Abstract: We develop a new lower bound for k-set agreement in synchronous
message-passing systems connected by an arbitrary directed communication
network, where up to t processes may crash. Our result thus generalizes the
t/k+1 lower bound for complete networks in the t-resilient model by Chaudhuri,
Herlihy, Lynch, and Tuttle [JACM'00]. Moreover, it generalizes two lower bounds
for oblivious algorithms in synchronous systems connected by an arbitrary
undirected communication network known to the processes, namely, the domination
number-based lower bound by Castaneda, Fraigniaud, Paz, Rajsbaum, Roy, and
Travers [TCS'21] for failure-free processes, and the radius-based lower bound
in the t-resilient model by Fraigniaud, Nguyen, and Paz [STACS'24].
  Our topological proof non-trivially generalizes and extends the
connectivity-based approach for the complete network, as presented in the book
by Herlihy, Kozlov, and Rajsbaum (2013). It is based on a sequence of shellable
carrier maps that, starting from a shellable input complex, determine the
evolution of the protocol complex: During the first t/k rounds, carrier maps
that crash exactly k processes per round are used, ensuring high connectivity
of their images. A Sperner's lemma style argument is used to prove that k-set
agreement is still impossible by that round. From round t/k+1 up to our lower
bound, we employ a novel carrier map that maintains high connectivity. Our
proof also provides a strikingly simple lower bound for k-set agreement in
synchronous systems with an arbitrary communication network with initial
crashes. We express the resulting additional agreement overhead via an
appropriately defined radius of the communication graphs. Finally, we prove
that the usual input pseudosphere complex for k-set agreement can be replaced
by an exponentially smaller input complex based on Kuhn triangulations, which
we prove to be also shellable.

</details>


### [13] [Efficient Mixed-Precision Large Language Model Inference with TurboMind](https://arxiv.org/abs/2508.15601)
*Li Zhang,Youhe Jiang,Guoliang He,Xin Chen,Han Lv,Qian Yao,Fangcheng Fu,Kai Chen*

Main category: cs.DC

TL;DR: 这篇论文提出了一种混合精度的大语言模型推理技术，通过优化硬件利用率和存储等级，在多种GPU架构上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 减少大语言模型的内存和计算需求，通过混合精度格式优化模型权重、激活值和KV缓存，以提高推理效率。

Method: 提出了两种新的混合精度流水线：GEMM流水线（通过离线权重打包和在线加速优化矩阵运算）和注意力流水线（支持任意精度组合的Query、Key、Value计算），包含硬件感知权重打包、适应性头部对齐、指令级并行和KV内存加载等关键技术。

Result: 在16个流行LLM和4种GPU架构上进行综合评估，与现有混合精度框架相比，延迟最高降低61%（平均30%），吞吐量最高提升156%（平均58%），在所有测试配置和硬件类型上都实现了一致的性能提升。

Conclusion: 该方法成功集成到TurboMind高性能推理引擎中，通过系统化的内存和计算优化，为混合精度LLM推理提供了高效的解决方案，并开源可用。

Abstract: Mixed-precision inference techniques reduce the memory and computational
demands of Large Language Models (LLMs) by applying hybrid precision formats to
model weights, activations, and KV caches. This work introduces mixed-precision
LLM inference techniques that encompass (i) systematic memory and compute
optimization across hierarchical storage and tensor core architectures, and
(ii) comprehensive end-to-end mixed-precision optimization across diverse
precision formats and hardware configurations. Our approach features two novel
mixed-precision pipelines designed for optimal hardware utilization: a General
Matrix Multiply (GEMM) pipeline that optimizes matrix operations through
offline weight packing and online acceleration, and an attention pipeline that
enables efficient attention computation with arbitrary Query, Key, and Value
precision combinations. The key implementation of the pipelines includes (i)
hardware-aware weight packing for automatic format optimization, (ii) adaptive
head alignment for efficient attention computation, (iii) instruction-level
parallelism for memory hierarchy exploitation, and (iv) KV memory loading
pipeline for enhanced inference efficiency. We conduct comprehensive
evaluations across 16 popular LLMs and 4 representative GPU architectures.
Results demonstrate that our approach achieves up to 61% lower serving latency
(30% on average) and up to 156% higher throughput (58% on average) in
mixed-precision workloads compared to existing mixed-precision frameworks,
establishing consistent performance improvements across all tested
configurations and hardware types. This work is integrated into TurboMind, a
high-performance inference engine of the LMDeploy project, which is
open-sourced and publicly available at https://github.com/InternLM/lmdeploy.

</details>


### [14] [CausalMesh: A Formally Verified Causal Cache for Stateful Serverless Computing](https://arxiv.org/abs/2508.15647)
*Haoran Zhang,Zihao Zhang,Shuai Mu,Sebastian Angel,Vincent Liu*

Main category: cs.DC

TL;DR: CausalMesh是一个为无服务器工作流设计的因果一致性缓存系统，支持客户端在多服务器间漫游时的无协调、无中止读写操作和读事务


<details>
  <summary>Details</summary>
Motivation: 在无服务器环境中，工作流中的函数可能被调度到不同节点的不同缓存上，这会导致非直观的异常。现有缓存系统无法很好地处理客户端漫游场景

Method: 提出了CausalMesh系统，采用因果一致性缓存协议，支持客户端在多服务器间漫游时的无协调读写操作。使用Dafny进行形式化验证

Result: 实验评估显示CausalMesh比现有方案具有更低的延迟和更高的吞吐量

Conclusion: CausalMesh是第一个支持客户端漫游场景下无协调、无中止操作的缓存系统，为无服务器工作流提供了有效的状态管理解决方案

Abstract: Stateful serverless workflows consist of multiple serverless functions that
access state on a remote database. Developers sometimes add a cache layer
between the serverless runtime and the database to improve I/O latency.
However, in a serverless environment, functions in the same workflow may be
scheduled to different nodes with different caches, which can cause
non-intuitive anomalies. This paper presents CausalMesh, a novel approach to
causally consistent caching in environments where a computation may migrate
from one machine to another, such as in serverless computing. CausalMesh is the
first cache system that supports coordination-free and abort-free read/write
operations and read transactions when clients roam among multiple servers.
CausalMesh also supports read-write transactional causal consistency in the
presence of client roaming, but at the cost of abort-freedom.
  We have formally verified CausalMesh's protocol in Dafny, and our
experimental evaluation shows that CausalMesh has lower latency and higher
throughput than existing proposals

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [15] [Accelerating GenAI Workloads by Enabling RISC-V Microkernel Support in IREE](https://arxiv.org/abs/2508.14899)
*Adeel Ahmad,Ahmad Tameem Kamal,Nouman Amir,Bilal Zafar,Saad Bin Nasir*

Main category: cs.AR

TL;DR: 在IREE编译器中为RISC-V微内核添加支持，通过优化MLIR附加指令和开发高性能微内核，在Llama-3.2-1B模型上对比了IREE和Llama.cpp的性能收益


<details>
  <summary>Details</summary>
Motivation: 为了在RISC-V平台上提高机器学习模型的运行效率，通过微内核技术优化IREE编译器在RISC-V64目标上的性能

Method: 首先在IREE编译途径中启用MLIR linalg dialect缩约操作向linalg.mmt4d操作的降级，然后为RISC-V开发优化的微内核

Result: 在Llama-3.2-1B-Instruct模型上进行了性能测试，并与上游IREE和Llama.cpp进行了性能对比

Conclusion: 该方法成功在IREE中实现了RISC-V微内核支持，为RISC-V平台上的机器学习工作负载提供了性能优化潜力

Abstract: This project enables RISC-V microkernel support in IREE, an MLIR-based
machine learning compiler and runtime. The approach begins by enabling the
lowering of MLIR linalg dialect contraction ops to linalg.mmt4d op for the
RISC-V64 target within the IREE pass pipeline, followed by the development of
optimized microkernels for RISC-V. The performance gains are compared with
upstream IREE and Llama.cpp for the Llama-3.2-1B-Instruct model.

</details>


### [16] [Improving Chip Design Enablement for Universities in Europe -- A Position Paper](https://arxiv.org/abs/2508.14907)
*Lukas Krupp,Ian O'Connor,Luca Benini,Christoph Studer,Joachim Rodrigues,Norbert Wehn*

Main category: cs.AR

TL;DR: 欧洲半导体行业面临芯片设计能力不足和人才短缺问题，本文分析大学和学术机构在提升芯片设计教育和研究方面的作用，提出战略建议。


<details>
  <summary>Details</summary>
Motivation: 欧洲半导体行业对经济至关重要，但在芯片设计领域存在严重的人才短缺和价值链贡献不足问题，需要通过学术机构来弥补这些缺陷。

Method: 对欧洲现有芯片设计计划进行全面概述，分析招聘、生产力、技术获取和设计支持方面的主要挑战，识别学术机构中的战略机遇。

Result: 识别了欧洲芯片设计教育研究面临的关键挑战和机遇，为制定解决方案提供了基础分析。

Conclusion: 需要协调努力和战略投资来克服挑战，加强学术机构的芯片设计能力，以支持欧洲半导体行业的发展。

Abstract: The semiconductor industry is pivotal to Europe's economy, especially within
the industrial and automotive sectors. However, Europe faces a significant
shortfall in chip design capabilities, marked by a severe skilled labor
shortage and lagging contributions in the design value chain segment. This
paper explores the role of European universities and academic initiatives in
enhancing chip design education and research to address these deficits. We
provide a comprehensive overview of current European chip design initiatives,
analyze major challenges in recruitment, productivity, technology access, and
design enablement, and identify strategic opportunities to strengthen chip
design capabilities within academic institutions. Our analysis leads to a
series of recommendations that highlight the need for coordinated efforts and
strategic investments to overcome these challenges.

</details>


### [17] [Scalable FPGA Framework for Real-Time Denoising in High-Throughput Imaging: A DRAM-Optimized Pipeline using High-Level Synthesis](https://arxiv.org/abs/2508.14917)
*Weichien Liao*

Main category: cs.AR

TL;DR: 基于FPGA的高速实时图像预处理流水线，通过HLS实现防止过量数据冲击并提高PRISM成像效率


<details>
  <summary>Details</summary>
Motivation: 高速成像技术如PRISM产生的数据速率超过传统实时处理能力，需要可扩展的预处理方案来减轻下游CPU/GPU的负担

Method: 使用高级综合(HLS)开发可扩展的FPGA预处理流氵线，通过爆炸模式AXI4接口最小化延迟，直接在流式图像数据上执行帧减法和平均操作

Result: 核心操作在帧间间隔之内完成，实现了内联去噪和数据集缩减，在PRISM规模量采集下验证有效

Conclusion: 该模块化FPGA框架为光谱学和显微镜预镜中延迟敏感的成像流程提供了实用的解决方案

Abstract: High-throughput imaging workflows, such as Parallel Rapid Imaging with
Spectroscopic Mapping (PRISM), generate data at rates that exceed conventional
real-time processing capabilities. We present a scalable FPGA-based
preprocessing pipeline for real-time denoising, implemented via High-Level
Synthesis (HLS) and optimized for DRAM-backed buffering. Our architecture
performs frame subtraction and averaging directly on streamed image data,
minimizing latency through burst-mode AXI4 interfaces. The resulting kernel
operates below the inter-frame interval, enabling inline denoising and reducing
dataset size for downstream CPU/GPU analysis. Validated under PRISM-scale
acquisition, this modular FPGA framework offers a practical solution for
latency-sensitive imaging workflows in spectroscopy and microscopy.

</details>


### [18] [Row-Column Hybrid Grouping for Fault-Resilient Multi-Bit Weight Representation on IMC Arrays](https://arxiv.org/abs/2508.15685)
*Kang Eun Jeon,Sangheum Yeon,Jinhee Kim,Hyeonsu Bang,Johnny Rhe,Jong Hwan Ko*

Main category: cs.AR

TL;DR: 这篇论文提出了一种新的模拟内存计算系统故障容锐方案，通过行列混合分组技术和优化编译流程，实现了更高的计算准确性、更快的编译速度和更好的能消效果。


<details>
  <summary>Details</summary>
Motivation: 解决模拟内存计算系统中的故障问题：固定故障导致的计算不可靠性，以及现有故障容锐算法编译过程超出过高的问题，这两者限制了系统的可扩展性和部署性。

Method: 1）提出行列混合分组技术：在行和列两个维度上引入冗余度，提高故障容锐能力；2）设计优化编译流程：将故障感知权重分解问题重构为整数线性规划任务，利用商用求解器实现快速编译；3）通过理论分析识别可直接解决的故障模式，进一步加速计算。

Result: 在卷积神经网络和小型语言模型上的实验结果显示：达到了超过现有基线方法的性能，准确度提升8%，编译速度提升150倍，能消效率提升2倍。

Conclusion: 该方法有效解决了模拟内存计算系统的故障容锐和编译效率问题，为实际部署提供了可扩展的解决方案，在保持高准确性的同时显著提升了系统性能。

Abstract: This paper addresses two critical challenges in analog In-Memory Computing
(IMC) systems that limit their scalability and deployability: the computational
unreliability caused by stuck-at faults (SAFs) and the high compilation
overhead of existing fault-mitigation algorithms, namely Fault-Free (FF). To
overcome these limitations, we first propose a novel multi-bit weight
representation technique, termed row-column hybrid grouping, which generalizes
conventional column grouping by introducing redundancy across both rows and
columns. This structural redundancy enhances fault tolerance and can be
effectively combined with existing fault-mitigation solutions. Second, we
design a compiler pipeline that reformulates the fault-aware weight
decomposition problem as an Integer Linear Programming (ILP) task, enabling
fast and scalable compilation through off-the-shelf solvers. Further
acceleration is achieved through theoretical insights that identify fault
patterns amenable to trivial solutions, significantly reducing computation.
Experimental results on convolutional networks and small language models
demonstrate the effectiveness of our approach, achieving up to 8%p improvement
in accuracy, 150x faster compilation, and 2x energy efficiency gain compared to
existing baselines.

</details>
