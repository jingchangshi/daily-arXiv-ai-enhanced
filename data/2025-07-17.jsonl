{"id": "2507.11709", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.11709", "abs": "https://arxiv.org/abs/2507.11709", "authors": ["Junius Pun", "Xilai Dai", "Grace Zgheib", "Mahesh A. Iyer", "Andrew Boutros", "Vaughn Betz", "Mohamed S. Abdelfattah"], "title": "Double Duty: FPGA Architecture to Enable Concurrent LUT and Adder Chain Usage", "comment": "accepted at FPL 2025", "summary": "Flexibility and customization are key strengths of Field-Programmable Gate\nArrays (FPGAs) when compared to other computing devices. For instance, FPGAs\ncan efficiently implement arbitrary-precision arithmetic operations, and can\nperform aggressive synthesis optimizations to eliminate ineffectual operations.\nMotivated by sparsity and mixed-precision in deep neural networks (DNNs), we\ninvestigate how to optimize the current logic block architecture to increase\nits arithmetic density. We find that modern FPGA logic block architectures\nprevent the independent use of adder chains, and instead only allow adder chain\ninputs to be fed by look-up table (LUT) outputs. This only allows one of the\ntwo primitives -- either adders or LUTs -- to be used independently in one\nlogic element and prevents their concurrent use, hampering area optimizations.\nIn this work, we propose the Double Duty logic block architecture to enable the\nconcurrent use of the adders and LUTs within a logic element. Without adding\nexpensive logic cluster inputs, we use 4 of the existing inputs to bypass the\nLUTs and connect directly to the adder chain inputs. We accurately model our\nchanges at both the circuit and CAD levels using open-source FPGA development\ntools. Our experimental evaluation on a Stratix-10-like architecture\ndemonstrates area reductions of 21.6% on adder-intensive circuits from the\nKratos benchmarks, and 9.3% and 8.2% on the more general Koios and VTR\nbenchmarks respectively. These area improvements come without an impact to\ncritical path delay, demonstrating that higher density is feasible on modern\nFPGA architectures by adding more flexibility in how the adder chain is used.\nAveraged across all circuits from our three evaluated benchmark set, our Double\nDuty FPGA architecture improves area-delay product by 9.7%."}
{"id": "2507.12028", "categories": ["cs.AR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.12028", "abs": "https://arxiv.org/abs/2507.12028", "authors": ["Soheil Mahdizadeh", "Elyas Oustad", "Mohsen Ansari"], "title": "MOFCO: Mobility- and Migration-Aware Task Offloading in Three-Layer Fog Computing Environments", "comment": null, "summary": "Task offloading in three-layer fog computing environments presents a critical\nchallenge due to user equipment (UE) mobility, which frequently triggers costly\nservice migrations and degrades overall system performance. This paper\naddresses this problem by proposing MOFCO, a novel Mobility- and\nMigration-aware Task Offloading algorithm for Fog Computing environments. The\nproposed method formulates task offloading and resource allocation as a\nMixed-Integer Nonlinear Programming (MINLP) problem and employs a\nheuristic-aided evolutionary game theory approach to solve it efficiently. To\nevaluate MOFCO, we simulate mobile users using SUMO, providing realistic\nmobility patterns. Experimental results show that MOFCO reduces system cost,\ndefined as a combination of latency and energy consumption, by an average of\n19% and up to 43% in certain scenarios compared to state-of-the-art methods."}
{"id": "2507.12418", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.12418", "abs": "https://arxiv.org/abs/2507.12418", "authors": ["George Alexakis", "Dimitrios Schoinianakis", "Giorgos Dimitrakopoulos"], "title": "High-Performance Pipelined NTT Accelerators with Homogeneous Digit-Serial Modulo Arithmetic", "comment": "28th Euromicro Conference Series on Digital System Design (DSD 2025)", "summary": "The Number Theoretic Transform (NTT) is a fundamental operation in\nprivacy-preserving technologies, particularly within fully homomorphic\nencryption (FHE). The efficiency of NTT computation directly impacts the\noverall performance of FHE, making hardware acceleration a critical technology\nthat will enable realistic FHE applications. Custom accelerators, in FPGAs or\nASICs, offer significant performance advantages due to their ability to exploit\nmassive parallelism and specialized optimizations. However, the operation of\nNTT over large moduli requires large word-length modulo arithmetic that limits\nachievable clock frequencies in hardware and increases hardware area costs. To\novercome such deficits, digit-serial arithmetic has been explored for modular\nmultiplication and addition independently. The goal of this work is to leverage\ndigit-serial modulo arithmetic combined with appropriate redundant data\nrepresentation to design modular pipelined NTT accelerators that operate\nuniformly on arbitrary small digits, without the need for intermediate\n(de)serialization. The proposed architecture enables high clock frequencies\nthrough regular pipelining while maintaining parallelism. Experimental results\ndemonstrate that the proposed approach outperforms state-of-the-art\nimplementations and reduces hardware complexity under equal performance and\ninput-output bandwidth constraints."}
{"id": "2507.12442", "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12442", "abs": "https://arxiv.org/abs/2507.12442", "authors": ["Saptarshi Mitra", "Rachid Karami", "Haocheng Xu", "Sitao Huang", "Hyoukjun Kwon"], "title": "Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length", "comment": "12 pages, 7 figures", "summary": "The demand for machine intelligence capable of processing continuous,\nlong-context inputs on local devices is growing rapidly. However, the quadratic\ncomplexity and memory requirements of traditional Transformer architectures\nmake them inefficient and often unusable for these tasks. This has spurred a\nparadigm shift towards new architectures like State Space Models (SSMs) and\nhybrids, which promise near-linear scaling. While most current research focuses\non the accuracy and theoretical throughput of these models, a systematic\nperformance characterization on practical consumer hardware is critically\nneeded to guide system-level optimization and unlock new applications.\n  To address this gap, we present a comprehensive, comparative benchmarking of\ncarefully selected Transformer, SSM, and hybrid models specifically for\nlong-context inference on consumer and embedded GPUs. Our analysis reveals that\nSSMs are not only viable but superior for this domain, capable of processing\nsequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than\ncomparable Transformers. While Transformers may be up to 1.8x faster at short\nsequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x\nfaster at very long contexts (~57K tokens). Our operator-level analysis reveals\nthat custom, hardware-aware SSM kernels dominate the inference runtime,\naccounting for over 55% of latency on edge platforms, identifying them as a\nprimary target for future hardware acceleration. We also provide detailed,\ndevice-specific characterization results to guide system co-design for the\nedge. To foster further research, we will open-source our characterization\nframework."}
{"id": "2507.11676", "categories": ["cs.PL", "cs.LO", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.11676", "abs": "https://arxiv.org/abs/2507.11676", "authors": ["Chris Heunen", "Louis Lemonnier", "Christopher McNally", "Alex Rice"], "title": "Quantum circuits are just a phase", "comment": "43 pages, 5 figures", "summary": "Quantum programs today are written at a low level of abstraction - quantum\ncircuits akin to assembly languages - and even advanced quantum programming\nlanguages essentially function as circuit description languages. This state of\naffairs impedes scalability, clarity, and support for higher-level reasoning.\nMore abstract and expressive quantum programming constructs are needed.\n  To this end, we introduce a novel yet simple quantum programming language for\ngenerating unitaries from \"just a phase\"; we combine a (global) phase operation\nthat captures phase shifts with a quantum analogue of the \"if let\" construct\nthat captures subspace selection via pattern matching. This minimal language\nlifts the focus from quantum gates to eigendecomposition, conjugation, and\ncontrolled unitaries; common building blocks in quantum algorithm design.\n  We demonstrate several aspects of the expressive power of our language in\nseveral ways. Firstly, we establish that our representation is universal by\nderiving a universal quantum gate set. Secondly, we show that important quantum\nalgorithms can be expressed naturally and concisely, including Grover's search\nalgorithm, Hamiltonian simulation, Quantum Fourier Transform, Quantum Signal\nProcessing, and the Quantum Eigenvalue Transformation. Furthermore, we give\nclean denotational semantics grounded in categorical quantum mechanics.\nFinally, we implement a prototype compiler that efficiently translates terms of\nour language to quantum circuits, and prove that it is sound with respect to\nthese semantics. Collectively, these contributions show that this construct\noffers a principled and practical step toward more abstract and structured\nquantum programming."}
{"id": "2507.11545", "categories": ["cs.DC", "cs.ET", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.11545", "abs": "https://arxiv.org/abs/2507.11545", "authors": ["Rhea Pritham Marpu", "Kevin J McNamara", "Preeti Gupta"], "title": "The AI Shadow War: SaaS vs. Edge Computing Architectures", "comment": null, "summary": "The very DNA of AI architecture presents conflicting paths: centralized\ncloud-based models (Software-as-a-Service) versus decentralized edge AI (local\nprocessing on consumer devices). This paper analyzes the competitive\nbattleground across computational capability, energy efficiency, and data\nprivacy. Recent breakthroughs show edge AI challenging cloud systems on\nperformance, leveraging innovations like test-time training and\nmixture-of-experts architectures. Crucially, edge AI boasts a 10,000x\nefficiency advantage: modern ARM processors consume merely 100 microwatts\nforinference versus 1 watt for equivalent cloud processing. Beyond efficiency,\nedge AI secures data sovereignty by keeping processing local, dismantling\nsingle points of failure in centralized architectures. This democratizes access\nthroughaffordable hardware, enables offline functionality, and reduces\nenvironmental impact by eliminating data transmission costs. The edge AI market\nprojects explosive growth from $9 billion in 2025 to $49.6 billion by 2030\n(38.5% CAGR), fueled by privacy demands and real-time analytics. Critical\napplications including personalized education, healthcare monitoring,\nautonomous transport, and smart infrastructure rely on edge AI's ultra-low\nlatency (5-10ms versus 100-500ms for cloud). The convergence of architectural\ninnovation with fundamental physics confirms edge AI's distributed approach\naligns with efficient information processing, signaling the inevitable\nemergence of hybrid edge-cloud ecosystems."}
{"id": "2507.11731", "categories": ["cs.PL", "68N15", "D.3.2"], "pdf": "https://arxiv.org/pdf/2507.11731", "abs": "https://arxiv.org/abs/2507.11731", "authors": ["Neng-Fa Zhou", "Cristian Grozea", "Håkan Kjellerstrand", "Oisín Mac Fhearaí"], "title": "Picat Through the Lens of Advent of Code", "comment": "14 pages", "summary": "Picat is a logic-based, multi-paradigm programming language that integrates\nfeatures from logic, functional, constraint, and imperative programming\nparadigms. This paper presents solutions to several problems from the 2024\nAdvent of Code (AoC). While AoC problems are not designed for any specific\nprogramming language, certain problem types, such as reverse engineering and\npath-finding, are particularly well-suited to Picat due to its built-in\nconstraint solving, pattern matching, backtracking, and dynamic programming\nwith tabling. This paper demonstrates that Picat's features, especially its\nSAT-based constraint solving and tabling, enable concise, declarative, and\nhighly efficient implementations of problems that would require significantly\nmore effort in imperative languages."}
{"id": "2507.11560", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11560", "abs": "https://arxiv.org/abs/2507.11560", "authors": ["Xin Wang", "Xiao Huan Li", "Xun Wang"], "title": "A Model Aware AIGC Task Offloading Algorithm in IIoT Edge Computing", "comment": "6 pages, 4 figures, accepted by ICCC 2025", "summary": "The integration of the Industrial Internet of Things (IIoT) with Artificial\nIntelligence-Generated Content (AIGC) offers new opportunities for smart\nmanufacturing, but it also introduces challenges related to\ncomputation-intensive tasks and low-latency demands. Traditional generative\nmodels based on cloud computing are difficult to meet the real-time\nrequirements of AIGC tasks in IIoT environments, and edge computing can\neffectively reduce latency through task offloading. However, the dynamic nature\nof AIGC tasks, model switching delays, and resource constraints impose higher\ndemands on edge computing environments. To address these challenges, this paper\nproposes an AIGC task offloading framework tailored for IIoT edge computing\nenvironments, considering the latency and energy consumption caused by AIGC\nmodel switching for the first time. IIoT devices acted as multi-agent\ncollaboratively offload their dynamic AIGC tasks to the most appropriate edge\nservers deployed with different generative models. A model aware AIGC task\noffloading algorithm based on Multi-Agent Deep Deterministic Policy Gradient\n(MADDPG-MATO) is devised to minimize the latency and energy. Experimental\nresults show that MADDPG-MATO outperforms baseline algorithms, achieving an\naverage reduction of 6.98% in latency, 7.12% in energy consumption, and a 3.72%\nincrease in task completion rate across four sets of experiments with model\nnumbers ranging from 3 to 6, it is demonstrated that the proposed algorithm is\nrobust and efficient in dynamic, high-load IIoT environments."}
{"id": "2507.11827", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.11827", "abs": "https://arxiv.org/abs/2507.11827", "authors": ["Shaurya Gomber", "Debangshu Banerjee", "Gagandeep Singh"], "title": "Universal Synthesis of Differentiably Tunable Numerical Abstract Transformers", "comment": "42 pages, 8 figures", "summary": "Numerical abstract interpretation is a widely used framework for the static\nanalysis of numerical programs. However, existing numerical abstract\ninterpreters rely on hand-crafted, instruction-specific transformers tailored\nto each domain, with no general algorithm for handling common operations across\ndomains. This limits extensibility, prevents precise compositional reasoning\nover instruction sequences, and forces all downstream tasks to use the same\nfixed transformer regardless of their precision, efficiency, or task-specific\nrequirements. To address these limitations, we propose a universal transformer\nsynthesis algorithm that constructs a parametric family of sound abstract\ntransformers for any given polyhedral numerical domain and a concrete operator\nfrom the class of Quadratic-Bounded Guarded Operators (QGO), which includes\nboth individual instructions and structured sequences. Each instantiation in\nthis family is sound by construction, enabling downstream analyses to adapt the\ntransformer to their particular needs. The space of transformers is\ndifferentiable but complex. To efficiently explore this space of transformers,\nwe introduce the Adaptive Gradient Guidance (AGG) procedure, a gradient-guided\nsearch strategy that steers the search process based on downstream analysis\nobjectives and runtime constraints. We implement these ideas in the USTAD\nframework and evaluate their effectiveness across three numerical abstract\ndomains: Zones, Octagons, and Polyhedra. Our results demonstrate that the\nuniversal synthesis algorithm successfully constructs sound families of\ntransformers across domains, and that USTAD achieves significant, tunable\nprecision gains over baselines by leveraging compositional reasoning and\nefficient gradient-guided traversal of the transformer space."}
{"id": "2507.11563", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11563", "abs": "https://arxiv.org/abs/2507.11563", "authors": ["Giulio Attenni", "Novella Bartolini"], "title": "Environmentally-Conscious Cloud Orchestration Considering Geo-Distributed Data Centers", "comment": "LOCO 2024, December 3, 2024, Glasgow/Online", "summary": "This paper presents a theoretical discussion for environmentally-conscious\njob deployment and migration in cloud environments, aiming to minimize the\nenvironmental impact of resource provisioning while incorporating\nsustainability requirements. As the demand for sustainable cloud services\ngrows, it is crucial for cloud customers to select data center operators based\non sustainability metrics and to accurately report the ecological footprint of\ntheir services. To this end, we analyze sustainability reports and define\ncomprehensive environmental impact profiles for data centers, incorporating key\nsustainability indicators. We formalize the problem as an optimization model,\nbalancing multiple environmental factors while respecting user preferences. A\nsimulative case study demonstrates the {potential} of our approach compared to\nbaseline strategies that optimize for single sustainability factors."}
{"id": "2507.11897", "categories": ["cs.PL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.11897", "abs": "https://arxiv.org/abs/2507.11897", "authors": ["Tyler Hou", "Shadaj Laddad", "Joseph M. Hellerstein"], "title": "Towards Relational Contextual Equality Saturation", "comment": "Appeared at EGRAPHS 2024", "summary": "Equality saturation is a powerful technique for program optimization.\nContextual equality saturation extends this to support rewrite rules that are\nconditioned on where a term appears in an expression. Existing work has brought\ncontextual reasoning to egg; in this paper, we share our ongoing work to extend\nthis to relational equality saturation in egglog. We summarize the existing\napproaches to contextual equality saturation, outline its main applications,\nand identify key challenges in combining this approach with relational models."}
{"id": "2507.11683", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11683", "abs": "https://arxiv.org/abs/2507.11683", "authors": ["Seth Ockerman", "Amal Gueroudji", "Tanwi Mallick", "Yixuan He", "Line Pouchard", "Robert Ross", "Shivaram Venkataraman"], "title": "PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed Training", "comment": "To be published in the 2025 International Conference for High\n  Performance Computing, Networking, Storage, and Analysis", "summary": "Spatiotemporal graph neural networks (ST-GNNs) are powerful tools for\nmodeling spatial and temporal data dependencies. However, their applications\nhave been limited primarily to small-scale datasets because of memory\nconstraints. While distributed training offers a solution, current frameworks\nlack support for spatiotemporal models and overlook the properties of\nspatiotemporal data. Informed by a scaling study on a large-scale workload, we\npresent PyTorch Geometric Temporal Index (PGT-I), an extension to PyTorch\nGeometric Temporal that integrates distributed data parallel training and two\nnovel strategies: index-batching and distributed-index-batching. Our index\ntechniques exploit spatiotemporal structure to construct snapshots dynamically\nat runtime, significantly reducing memory overhead, while\ndistributed-index-batching extends this approach by enabling scalable\nprocessing across multiple GPUs. Our techniques enable the first-ever training\nof an ST-GNN on the entire PeMS dataset without graph partitioning, reducing\npeak memory usage by up to 89\\% and achieving up to a 13.1x speedup over\nstandard DDP with 128 GPUs."}
{"id": "2507.11830", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11830", "abs": "https://arxiv.org/abs/2507.11830", "authors": ["Samyam Rajbhandari", "Mert Hidayetoglu", "Aurick Qiao", "Ye Wang", "Juncheng Yang", "Jeff Rasley", "Michael Wyatt", "Yuxiong He"], "title": "Arctic Inference with Shift Parallelism: Fast and Efficient Open Source Inference System for Enterprise AI", "comment": null, "summary": "Inference is now the dominant AI workload, yet existing systems force\ntrade-offs between latency, throughput, and cost. Arctic Inference, an\nopen-source vLLM plugin from Snowflake AI Research, introduces Shift\nParallelism, a dynamic parallelism strategy that adapts to real-world traffic\nwhile integrating speculative decoding, SwiftKV compute reduction, and\noptimized embedding inference. It achieves up to 3.4 times faster request\ncompletion, 1.75 times faster generation, and 1.6M tokens/sec per GPU for\nembeddings, outperforming both latency- and throughput-optimized deployments.\nAlready powering Snowflake Cortex AI, Arctic Inference delivers\nstate-of-the-art, cost-effective inference for enterprise AI and is now\navailable to the community."}
{"id": "2507.11899", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11899", "abs": "https://arxiv.org/abs/2507.11899", "authors": ["Saeid Aghasoleymani Najafabadi"], "title": "Performance Assessment of Load Balancing Methods in Cloud Computing: Analysis of Round Robin, Equally Spread, and Throttled Strategies Using Cloud Analyst", "comment": null, "summary": "Load balancing plays a pivotal role in cloud computing, ensuring that\nresources are optimally allocated to maintain high service quality and\noperational efficiency. As workloads in cloud environments become increasingly\ndynamic and unpredictable, load balancing strategies are evolving from\ntraditional static methods to more adaptive and intelligent approaches. In this\nstudy, the Cloud Analyst simulation tool was used to evaluate the performance\nof different load balancing algorithms under various scenarios, including both\ncentralized and distributed resource setups. The results highlight that while\nthe Round Robin algorithm yields slightly better processing times within a\nsingle data center, Equally Spread and Throttled techniques perform\ncompetitively, especially when network latency is considered. More importantly,\nwhen resources are distributed across multiple data centers, response times are\nsignificantly reduced, emphasizing the value of proximity and efficient load\ndistribution. In these distributed environments, Equally Spread and Throttled\nalgorithms not only maintain quick response times but also contribute to lower\noperational costs. These findings demonstrate the necessity of strategic\nresource placement and proactive infrastructure planning to balance performance\nand cost. Adopting intelligent, dynamic load balancing and resource management\npractices can help organizations meet evolving cloud demands, optimize costs,\nand maintain a competitive advantage. Continuous evaluation and integration of\nemerging technologies are crucial for sustaining effective and scalable cloud\noperations."}
{"id": "2507.11929", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11929", "abs": "https://arxiv.org/abs/2507.11929", "authors": ["Minchen Yu", "Yinghao Ren", "Jiamu Zhao", "Jiaqi Li"], "title": "Making Serverless Computing Extensible: A Case Study of Serverless Data Analytics", "comment": null, "summary": "Serverless computing has attracted a broad range of applications due to its\nease of use and resource elasticity. However, developing serverless\napplications often poses a dilemma -- relying on general-purpose serverless\nplatforms can fall short of delivering satisfactory performance for complex\nworkloads, whereas building application-specific serverless systems undermines\nthe simplicity and generality. In this paper, we propose an extensible design\nprinciple for serverless computing. We argue that a platform should enable\ndevelopers to extend system behaviors for domain-specialized optimizations\nwhile retaining a shared, easy-to-use serverless environment. We take data\nanalytics as a representative serverless use case and realize this design\nprinciple in Proteus. Proteus introduces a novel abstraction of decision\nworkflows, allowing developers to customize control-plane behaviors for\nimproved application performance. Preliminary results show that Proteus's\nprototype effectively optimizes analytical query execution and supports\nfine-grained resource sharing across diverse applications."}
{"id": "2507.11978", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11978", "abs": "https://arxiv.org/abs/2507.11978", "authors": ["Jiacheng Huang", "Zimin Li", "Yinghui Li", "Haojie Wang"], "title": "NineToothed: A Triton-Based High-Level Domain-Specific Language for Machine Learning", "comment": null, "summary": "The emergence of deep learning domain-specific languages (DSLs) has\nsubstantially reduced the obstacles in developing high-performance,\ncross-platform compute kernels. However, current DSLs, such as Triton, still\ndemand that developers possess expertise in parallel programming and expose\nthem to many low-level details. This requirement complicates the development\nprocess and adds to the difficulty of maintaining compute kernels.\nConsequently, developing a new programming model that supports serial\nprogramming for deep learning workloads is crucial.\n  This paper introduces NineToothed, a domain-specific language that offers\nserial semantics for machine learning programming. Through the automatic\ntransformation of serial code into parallel code, NineToothed significantly\nstreamlines the development process while causing minimal performance\ndegradation. NineToothed encompasses (1) a language with tensor-oriented\nmetaprogramming (TOM) that adopts the arrange-and-apply paradigm, enabling the\nexpression of tiled computations without the need to manage low-level details\nand (2) a code generator for generating high-performance parallel code. Our\nevaluation results indicate that NineToothed can greatly simplify compute\nkernel development while maintaining performance comparable to that of Triton."}
{"id": "2507.12032", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.12032", "abs": "https://arxiv.org/abs/2507.12032", "authors": ["Brian-Frederik Jahnke", "René Brinkhege", "Jan Peter Meyer", "Daniel Tebernum", "Falk Howar"], "title": "ARRC: Explainable, Workflow-Integrated Recommender for Sustainable Resource Optimization Across the Edge-Cloud Continuum", "comment": null, "summary": "Achieving sustainable, explainable, and maintainable automation for resource\noptimization is a core challenge across the edge-cloud continuum. Persistent\noverprovisioning and operational complexity often stem from heterogeneous\nplatforms and layered abstractions, while systems lacking explainability and\nmaintainability become fragile, impede safe recovery, and accumulate technical\ndebt. Existing solutions are frequently reactive, limited to single abstraction\nlayers, or require intrusive platform changes, leaving efficiency and\nmaintainability gains unrealized.\n  This paper addresses safe, transparent, and low-effort resource optimization\nin dynamic, multi-tenant edge-cloud systems, without disrupting operator\nworkflows or increasing technical debt. We introduce ARRC, a recommender system\nrooted in software engineering design principles, which delivers explainable,\ncross-layer resource recommendations directly into operator workflows (such as\ntickets and GitOps pull requests). ARRC encapsulates optimization logic in\nspecialized, auditable agents coordinated via a shared interface, supporting\nmaintainability and extensibility through transparency and the ability to\ninspect both recommendations and their rationale.\n  Empirical evaluation in a multi-region industrial deployment shows that ARRC\nreduces operator workload by over 50%, improves compute utilization by up to\n7.7x, and maintains error rates below 5%, with most benefits achieved through\nincremental, operator-approved changes. This demonstrates that explainable,\nrecommendation-based architectures can achieve sustainable efficiency and\nmaintainability improvements at production scale.\n  ARRC provides an empirically evaluated framework for integrating explainable,\nworkflow-driven automation into resource management, intended to advance best\npractices for robust, maintainable, and transparent edge-cloud continuum\nplatforms."}
{"id": "2507.12038", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.12038", "abs": "https://arxiv.org/abs/2507.12038", "authors": ["Alkida Balliu", "Thomas Boudier", "Francesco d'Amore", "Dennis Olivetti", "Gustav Schmid", "Jukka Suomela"], "title": "Distributed Algorithms for Potential Problems", "comment": "28 pages, 4 figures", "summary": "In this work we present a fast distributed algorithm for local potential\nproblems: these are graph problems where the task is to find a locally optimal\nsolution where no node can unilaterally improve the utility in its local\nneighborhood by changing its own label. A simple example of such a problem is\nthe task of finding a locally optimal cut, i.e., a cut where for each node at\nleast half of its incident edges are cut edges. The distributed round\ncomplexity of locally optimal cut has been wide open; the problem is known to\nrequire $\\Omega(\\log n)$ rounds in the deterministic LOCAL model and\n$\\Omega(\\log \\log n)$ rounds in the randomized LOCAL model, but the only known\nupper bound is the trivial brute-force solution of $O(n)$ rounds. Locally\noptimal cut in bounded-degree graphs is perhaps the simplest example of a\nlocally checkable labeling problem for which there is still such a large gap\nbetween current upper and lower bounds. We show that in bounded-degree graphs,\nall local potential problems, including locally optimal cut, can be solved in\n$\\log^{O(1)} n$ rounds, both in the deterministic and randomized LOCAL models.\nIn particular, the deterministic round complexity of the locally optimal cut\nproblem is now settled to $\\log^{\\Theta(1)} n$."}
{"id": "2507.12106", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.12106", "abs": "https://arxiv.org/abs/2507.12106", "authors": ["Antonio Salis", "Gabriele Troina", "Gianluca Boanelli", "Marco Ottaviano", "Paola Fortini", "Soraya Versace"], "title": "Urban Green Governance: IoT-Driven Management and Enhancement of Urban Green Spaces in Campobasso", "comment": "18 pages, 6 Figures", "summary": "The efficient design and management of public green spaces is a key factor in\npromoting the health and well-being of urban population, as emphasized by the\nWHO, UNEP, and EEA. These areas serve as the \"green lungs\" of the urban\necosystem, playing a vital role in enhancing quality of life thanks to the\nprovision of ecosystem services. In this context, the Smart Green City use case\nin Campobasso municipality, funded by the Italian Ministry of Enterprises\n(MIMIT), emerges as an innovative model for the sustainable management of green\nurban areas through the adoption of an advanced system of emerging technologies\nintegrated and interoperable. The project integrates IoT systems and\ndata-driven governance platforms, enabling real-time monitoring of the health\nstatus of trees and green areas via a Decision Support System (DSS). It also\nfacilitates the collection and analysis of data from diverse sources, including\nweather conditions, air quality, soil moisture, pollution levels. The resulting\ncloud-based platform supports a holistic real time decision making for green\nurban managers, technical experts and operational staff. It enables intelligent\ncontrol and management of urban green spaces using Tree Talker sensors,\nintegrated with soil moisture and water potential monitoring systems. Thanks to\npredictive models based on machine learning algorithms and real time data\nprovided by IoT sensors, irrigation of public parks can be optimized by\nproviding suggestions on when and how much water to apply. Customized alerts\nlayers are also activated warning users when monitored parameters, such as soil\ntemperature, humidity, or water potential, exceed predefined thresholds. This\nUse Case demonstrates how digitalization, IoT sensors fusion and technological\ninnovation can support sustainable urban governance, fostering environmental\nresilience and improving citizens quality of life."}
{"id": "2507.12205", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.12205", "abs": "https://arxiv.org/abs/2507.12205", "authors": ["Junqing Lin", "Jingwei Sun", "Mingge Lu", "Guangzhong Sun"], "title": "Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed Storage", "comment": "11 pages", "summary": "Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance\nbottleneck in the local deployment of sparse Large Language Models (LLMs),\nwhere inference predominantly operates on workloads during the decoder phase\nwith a batch size of one. Existing SpMV kernels and sparse matrix formats,\noriginally designed for scientific computing, fail to exploit the unique\nstructure patterns inherent in sparse LLMs, resulting in suboptimal performance\nand excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized\nSpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a\nhierarchical block extraction algorithm that captures multiple granularities of\nblock structures within sparse LLMs, and (2) a novel compressed sparse format\n(EC-CSR) that employs delta indexing to reduce storage overhead and enhance\nmemory access efficiency. Evaluated on real sparse weight matrices from LLaMA\nand OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV\nlibraries and reduces storage overhead by up to 55.4% compared to CSR."}
{"id": "2507.12028", "categories": ["cs.AR", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.12028", "abs": "https://arxiv.org/abs/2507.12028", "authors": ["Soheil Mahdizadeh", "Elyas Oustad", "Mohsen Ansari"], "title": "MOFCO: Mobility- and Migration-Aware Task Offloading in Three-Layer Fog Computing Environments", "comment": null, "summary": "Task offloading in three-layer fog computing environments presents a critical\nchallenge due to user equipment (UE) mobility, which frequently triggers costly\nservice migrations and degrades overall system performance. This paper\naddresses this problem by proposing MOFCO, a novel Mobility- and\nMigration-aware Task Offloading algorithm for Fog Computing environments. The\nproposed method formulates task offloading and resource allocation as a\nMixed-Integer Nonlinear Programming (MINLP) problem and employs a\nheuristic-aided evolutionary game theory approach to solve it efficiently. To\nevaluate MOFCO, we simulate mobile users using SUMO, providing realistic\nmobility patterns. Experimental results show that MOFCO reduces system cost,\ndefined as a combination of latency and energy consumption, by an average of\n19% and up to 43% in certain scenarios compared to state-of-the-art methods."}
