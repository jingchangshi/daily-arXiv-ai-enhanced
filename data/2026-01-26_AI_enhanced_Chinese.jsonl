{"id": "2601.16294", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.16294", "abs": "https://arxiv.org/abs/2601.16294", "authors": ["Evangelos Georganas", "Alexander Heinecke", "Pradeep Dubey"], "title": "Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple", "comment": null, "summary": "General Matrix Multiplication (GEMM) is the cornerstone of Deep Learning and HPC workloads; accordingly, academia and industry have heavily optimized this kernel. Modern platforms with matrix multiplication accelerators exhibit high FLOP/Byte machine balance, which makes implementing optimal matrix multiplication challenging. On modern CPU platforms with matrix engines, state-of-the-art vendor libraries tune input tensor layouts, parallelization schemes, and cache blocking to minimize data movement across the memory hierarchy and maximize throughput. However, the best settings for these parameters depend strongly on the target platform (number of cores, memory hierarchy, cache sizes) and on the shapes of the matrices, making exhaustive tuning infeasible; in practice this leads to performance \"glass jaws\". In this work we revisit space filling curves (SFC) to alleviate the problem of this cumbersome tuning. SFC convert multi-dimensional coordinates (e.g. 2D) into a single dimension (1D), keeping nearby points in the high-dimensional space close in the 1D order. We partition the Matrix Multiplication computation space using recent advancements in generalized SFC (Generalized Hilbert Curves), and we obtain platform-oblivious and shape-oblivious matrix-multiplication schemes that exhibit inherently high degree of data locality. Furthermore, we extend the SFC-based work partitioning to implement Communication-Avoiding (CA) algorithms that replicate the input tensors and provably minimize communication/data-movement on the critical path. The integration of CA-algorithms is seamless and yields compact code (~30 LOC), yet it achieves state-of-the-art results on multiple CPU platforms, outperforming vendor libraries by up to 2x(geometric-mean speedup) for a range of GEMM shapes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u5e7f\u4e49\u5e0c\u5c14\u4f2f\u7279\u66f2\u7ebf\u7b49\u7a7a\u95f4\u586b\u5145\u66f2\u7ebf\u6765\u5212\u5206\u77e9\u9635\u4e58\u6cd5\u8ba1\u7b97\u7a7a\u95f4\uff0c\u5b9e\u73b0\u5e73\u53f0\u65e0\u5173\u548c\u5f62\u72b6\u65e0\u5173\u7684\u9ad8\u6570\u636e\u5c40\u90e8\u6027GEMM\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u901a\u4fe1\u907f\u514d\u7b97\u6cd5\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u5728\u591a\u4e2aCPU\u5e73\u53f0\u4e0a\u8d85\u8d8a\u5382\u5546\u5e93\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3CPU\u5e73\u53f0\u4e0a\u7684\u77e9\u9635\u4e58\u6cd5\u52a0\u901f\u5668\u5177\u6709\u9ad8FLOP/Byte\u673a\u5668\u5e73\u8861\uff0c\u4f7f\u5f97\u5b9e\u73b0\u6700\u4f18\u77e9\u9635\u4e58\u6cd5\u5177\u6709\u6311\u6218\u6027\u3002\u5382\u5546\u5e93\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u5e73\u53f0\uff08\u6838\u5fc3\u6570\u3001\u5185\u5b58\u5c42\u6b21\u3001\u7f13\u5b58\u5927\u5c0f\uff09\u548c\u77e9\u9635\u5f62\u72b6\u8fdb\u884c\u7e41\u7410\u8c03\u4f18\uff0c\u5bfc\u81f4\u6027\u80fd\"\u73bb\u7483\u4e0b\u5df4\"\u95ee\u9898\uff0c\u5373\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u5e7f\u4e49\u5e0c\u5c14\u4f2f\u7279\u66f2\u7ebf\u7b49\u7a7a\u95f4\u586b\u5145\u66f2\u7ebf\u5c06\u591a\u7ef4\u8ba1\u7b97\u7a7a\u95f4\uff08\u59822D\uff09\u8f6c\u6362\u4e3a1D\u987a\u5e8f\uff0c\u4fdd\u6301\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u76f8\u90bb\u70b9\u57281D\u987a\u5e8f\u4e2d\u7684\u63a5\u8fd1\u6027\u3002\u57fa\u4e8e\u6b64\u5212\u5206\u77e9\u9635\u4e58\u6cd5\u8ba1\u7b97\u7a7a\u95f4\uff0c\u5b9e\u73b0\u5e73\u53f0\u65e0\u5173\u548c\u5f62\u72b6\u65e0\u5173\u7684\u6570\u636e\u5c40\u90e8\u6027\u65b9\u6848\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u4e3a\u901a\u4fe1\u907f\u514d\u7b97\u6cd5\uff0c\u590d\u5236\u8f93\u5165\u5f20\u91cf\u5e76\u8bc1\u660e\u6700\u5c0f\u5316\u5173\u952e\u8def\u5f84\u4e0a\u7684\u901a\u4fe1/\u6570\u636e\u79fb\u52a8\u3002", "result": "\u5728\u591a\u4e2aCPU\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4ee3\u7801\u7d27\u51d1\uff08\u7ea630\u884c\uff09\uff0c\u5bf9\u4e8e\u4e00\u7cfb\u5217GEMM\u5f62\u72b6\uff0c\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u6bd4\u8d85\u8d8a\u5382\u5546\u5e93\u9ad8\u8fbe2\u500d\u3002", "conclusion": "\u7a7a\u95f4\u586b\u5145\u66f2\u7ebf\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u77e9\u9635\u4e58\u6cd5\u4e2d\u7e41\u7410\u7684\u5e73\u53f0\u548c\u5f62\u72b6\u7279\u5b9a\u8c03\u4f18\u95ee\u9898\uff0c\u901a\u8fc7\u9ad8\u6570\u636e\u5c40\u90e8\u6027\u548c\u901a\u4fe1\u907f\u514d\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u5e73\u53f0\u65e0\u5173\u7684GEMM\u5b9e\u73b0\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5382\u5546\u5e93\u3002"}}
{"id": "2601.16460", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.16460", "abs": "https://arxiv.org/abs/2601.16460", "authors": ["Ivan Klianev"], "title": "Consensus In Asynchrony", "comment": null, "summary": "We demonstrate sufficiency of events-based synchronisation for solving deterministic fault-tolerant consensus in asynchrony. Main result is an algorithm that terminates with valid vector agreement, hence operates with safety, liveness, and tolerance to one crash. Reconciling with the FLP impossibility result, we identified: i) existence of two types of agreements: data-independent and data-dependent; and ii) dependence of FLP theorem correctness on three implicit assumptions. Consensus impossibility with data-dependent agreement is contingent on two of them. The theorem-stated impossibility with every agreement type hinges entirely on the third. We provide experimental results showing that the third assumption has no evidence in support.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u57fa\u4e8e\u4e8b\u4ef6\u7684\u540c\u6b65\u8db3\u4ee5\u5728\u5f02\u6b65\u73af\u5883\u4e2d\u89e3\u51b3\u786e\u5b9a\u6027\u5bb9\u9519\u5171\u8bc6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u7ec8\u6b62\u5e76\u8fbe\u6210\u6709\u6548\u5411\u91cf\u534f\u8bae\u7684\u7b97\u6cd5\uff0c\u540c\u65f6\u8bc6\u522b\u4e86FLP\u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u4e2d\u7684\u4e09\u4e2a\u9690\u542b\u5047\u8bbe\u3002", "motivation": "\u4f20\u7edfFLP\u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u8868\u660e\u5728\u5b8c\u5168\u5f02\u6b65\u7cfb\u7edf\u4e2d\u65e0\u6cd5\u5b9e\u73b0\u786e\u5b9a\u6027\u5bb9\u9519\u5171\u8bc6\uff0c\u4f46\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5728\u4e8b\u4ef6\u540c\u6b65\u6761\u4ef6\u4e0b\u662f\u5426\u53ef\u80fd\u7ed5\u8fc7\u8fd9\u4e00\u9650\u5236\uff0c\u5b9e\u73b0\u5b89\u5168\u3001\u6d3b\u6027\u548c\u5bb9\u9519\u6027\u517c\u5907\u7684\u5171\u8bc6\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u540c\u6b65\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u7ec8\u6b62\u5e76\u8fbe\u6210\u6709\u6548\u7684\u5411\u91cf\u534f\u8bae\u3002\u901a\u8fc7\u5206\u6790FLP\u5b9a\u7406\uff0c\u8bc6\u522b\u4e86\u4e09\u79cd\u9690\u542b\u5047\u8bbe\uff1a1) \u5b58\u5728\u4e24\u79cd\u534f\u8bae\u7c7b\u578b\uff08\u6570\u636e\u65e0\u5173\u548c\u6570\u636e\u76f8\u5173\uff09\uff1b2) FLP\u5b9a\u7406\u7684\u6b63\u786e\u6027\u4f9d\u8d56\u4e8e\u4e09\u4e2a\u9690\u542b\u5047\u8bbe\uff1b3) \u6570\u636e\u76f8\u5173\u534f\u8bae\u7684\u5171\u8bc6\u4e0d\u53ef\u80fd\u6027\u4f9d\u8d56\u4e8e\u5176\u4e2d\u4e24\u4e2a\u5047\u8bbe\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5b89\u5168\u3001\u6d3b\u6027\u548c\u5bb9\u5fcd\u4e00\u6b21\u5d29\u6e83\u7684\u7279\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFLP\u5b9a\u7406\u6240\u4f9d\u8d56\u7684\u7b2c\u4e09\u4e2a\u9690\u542b\u5047\u8bbe\u7f3a\u4e4f\u5b9e\u8bc1\u652f\u6301\uff0c\u4ece\u800c\u4e3a\u5728\u4e8b\u4ef6\u540c\u6b65\u6761\u4ef6\u4e0b\u5b9e\u73b0\u786e\u5b9a\u6027\u5bb9\u9519\u5171\u8bc6\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002", "conclusion": "\u57fa\u4e8e\u4e8b\u4ef6\u7684\u540c\u6b65\u8db3\u4ee5\u89e3\u51b3\u5f02\u6b65\u73af\u5883\u4e2d\u7684\u786e\u5b9a\u6027\u5bb9\u9519\u5171\u8bc6\u95ee\u9898\u3002FLP\u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u4f9d\u8d56\u4e8e\u4e09\u4e2a\u9690\u542b\u5047\u8bbe\uff0c\u5176\u4e2d\u7b2c\u4e09\u4e2a\u5047\u8bbe\u7f3a\u4e4f\u5b9e\u8bc1\u8bc1\u636e\uff0c\u8fd9\u4e3a\u7ed5\u8fc7FLP\u9650\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2601.16536", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.16536", "abs": "https://arxiv.org/abs/2601.16536", "authors": ["Yuanhong He", "Peiyu Niu", "Jun Chen", "Chenchen Zhang", "Chao Yang"], "title": "W4A16 Mixed-Precision Matrix Multiplication on Decoupled Architecture: Kernel Design and Memory Bottleneck Analysis for Ascend NPUs", "comment": null, "summary": "As Large Language Models (LLMs) scale, weight-only quantization (W4A16: 4-bit weights, 16-bit activations) becomes critical for reducing memory footprint with minimal accuracy loss. However, its efficient deployment on Huawei's Ascend 910 Neural Processing Unit (NPU) is challenging due to limited native mixed-precision support and the accelerator's decoupled compute architecture. To enable quantization on such architecture, we present the first practical W4A16 matrix multiplication kernel tailored for the Ascend 910 NPU. Our design leverages vector cores for on-the-fly INT4-to-FP16 dequantization, cube cores for high-throughput GEMM, and Split-K parallelization to mitigate memory latency. Performance evaluations across diverse matrix shapes and batch sizes show our method outperforms data-parallel approaches when K >> N, a typical scenario in LLM decoding. Specially, our method can achieve a speedup ranging from 1.01x to 1.74x. In addition, our profile reveals the primary bottleneck is not dequantization compution itself, but extra global memory transfer for the weight, making W4A16 only reaching a maximum speedup of 1.48x over native FP16xFP16 matrix multiplication in PyTorch. In the long run, our method lays a solid foundation and provides insightful views for the efficient deployment of quantized large language models on various domain-specific accelerators.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9762\u5411\u534e\u4e3a\u6607\u817e910 NPU\u7684W4A16\u77e9\u9635\u4e58\u6cd5\u5185\u6838\uff0c\u901a\u8fc7\u5411\u91cf\u6838\u5fc3\u5b9e\u65f6\u53cd\u91cf\u5316\u3001\u7acb\u65b9\u6838\u5fc3GEMM\u548cSplit-K\u5e76\u884c\u5316\uff0c\u5728LLM\u89e3\u7801\u573a\u666f\u4e2d\u5b9e\u73b01.01-1.74\u500d\u52a0\u901f", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0cW4A16\u91cf\u5316\u5bf9\u51cf\u5c11\u5185\u5b58\u5360\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u534e\u4e3a\u6607\u817e910 NPU\u4e0a\u9ad8\u6548\u90e8\u7f72\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u8be5\u67b6\u6784\u7f3a\u4e4f\u539f\u751f\u6df7\u5408\u7cbe\u5ea6\u652f\u6301\u4e14\u91c7\u7528\u89e3\u8026\u8ba1\u7b97\u67b6\u6784", "method": "\u8bbe\u8ba1\u9488\u5bf9\u6607\u817e910 NPU\u7684W4A16\u77e9\u9635\u4e58\u6cd5\u5185\u6838\uff1a1) \u5229\u7528\u5411\u91cf\u6838\u5fc3\u8fdb\u884cINT4\u5230FP16\u7684\u5b9e\u65f6\u53cd\u91cf\u5316\uff1b2) \u4f7f\u7528\u7acb\u65b9\u6838\u5fc3\u8fdb\u884c\u9ad8\u541e\u5410\u91cfGEMM\u8ba1\u7b97\uff1b3) \u91c7\u7528Split-K\u5e76\u884c\u5316\u6765\u7f13\u89e3\u5185\u5b58\u5ef6\u8fdf", "result": "\u5728\u4e0d\u540c\u77e9\u9635\u5f62\u72b6\u548c\u6279\u91cf\u5927\u5c0f\u4e0b\uff0c\u5f53K>>N\uff08LLM\u89e3\u7801\u5178\u578b\u573a\u666f\uff09\u65f6\u4f18\u4e8e\u6570\u636e\u5e76\u884c\u65b9\u6cd5\uff0c\u52a0\u901f\u6bd41.01-1.74\u500d\uff1b\u5206\u6790\u663e\u793a\u4e3b\u8981\u74f6\u9888\u662f\u6743\u91cd\u989d\u5916\u5168\u5c40\u5185\u5b58\u4f20\u8f93\u800c\u975e\u53cd\u91cf\u5316\u8ba1\u7b97\uff0cW4A16\u76f8\u6bd4\u539f\u751fFP16\u6700\u5927\u52a0\u901f1.48\u500d", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u5404\u7c7b\u9886\u57df\u4e13\u7528\u52a0\u901f\u5668\u4e0a\u9ad8\u6548\u90e8\u7f72\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u5e76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3"}}
{"id": "2601.16635", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.16635", "abs": "https://arxiv.org/abs/2601.16635", "authors": ["Julian Legler"], "title": "Artifact for Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices", "comment": "Accepted Artifact Paper at ICSOC2025", "summary": "Recent advancements enable fine-grained energy measurements in cloud-native environments (e.g., at container or process level) beyond traditional coarse-grained scopes. However, service-level energy measurement for microservice-based applications remains underexplored. Such measurements must include compute, network, and storage energy to avoid underestimating consumption in distributed setups. We present GOXN (Green Observability eXperiment eNginE), an energy experimentation engine for Kubernetes-based microservices that quantifies compute, network, and storage energy at the service level. Using GOXN, we evaluated the OpenTelemetry Demo under varying configurations (monitoring, tracing, service mesh) and steady synthetic load, collecting metrics from Kepler and cAdvisor. Our additive energy model derives service-level energy from container-level data. Results show that excluding network and storage can underestimate auxiliary-service energy by up to 63%, and that high tracing loads shift energy dominance toward network and storage.", "AI": {"tldr": "GOXN\u662f\u4e00\u4e2aKubernetes\u5fae\u670d\u52a1\u80fd\u6e90\u5b9e\u9a8c\u5f15\u64ce\uff0c\u80fd\u591f\u91cf\u5316\u670d\u52a1\u7ea7\u522b\u7684\u8ba1\u7b97\u3001\u7f51\u7edc\u548c\u5b58\u50a8\u80fd\u8017\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u5206\u5e03\u5f0f\u73af\u5883\u80fd\u8017\u7684\u95ee\u9898\u3002", "motivation": "\u867d\u7136\u4e91\u539f\u751f\u73af\u5883\u5df2\u7ecf\u80fd\u591f\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u80fd\u6e90\u6d4b\u91cf\uff08\u5982\u5bb9\u5668\u6216\u8fdb\u7a0b\u7ea7\u522b\uff09\uff0c\u4f46\u5fae\u670d\u52a1\u5e94\u7528\u7684\u670d\u52a1\u5668\u80fd\u6e90\u6d4b\u91cf\u4ecd\u7136\u7814\u7a76\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u8ba1\u7b97\u80fd\u8017\uff0c\u5ffd\u7565\u4e86\u7f51\u7edc\u548c\u5b58\u50a8\u80fd\u8017\uff0c\u8fd9\u5728\u5206\u5e03\u5f0f\u8bbe\u7f6e\u4e2d\u4f1a\u5bfc\u81f4\u80fd\u8017\u4f4e\u4f30\u3002", "method": "\u63d0\u51fa\u4e86GOXN\uff08Green Observability eXperiment eNginE\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8eKubernetes\u5fae\u670d\u52a1\u7684\u80fd\u6e90\u5b9e\u9a8c\u5f15\u64ce\u3002\u5b83\u4f7f\u7528Kepler\u548ccAdvisor\u6536\u96c6\u6307\u6807\uff0c\u901a\u8fc7\u52a0\u6027\u80fd\u6e90\u6a21\u578b\u4ece\u5bb9\u5668\u7ea7\u6570\u636e\u63a8\u5bfc\u51fa\u670d\u52a1\u5668\u80fd\u6e90\u6d88\u8017\uff0c\u80fd\u591f\u91cf\u5316\u8ba1\u7b97\u3001\u7f51\u7edc\u548c\u5b58\u50a8\u4e09\u65b9\u9762\u7684\u80fd\u8017\u3002", "result": "\u5728OpenTelemetry Demo\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff1a1\uff09\u5ffd\u7565\u7f51\u7edc\u548c\u5b58\u50a8\u80fd\u8017\u4f1a\u5bfc\u81f4\u8f85\u52a9\u670d\u52a1\u80fd\u8017\u4f4e\u4f30\u9ad8\u8fbe63%\uff1b2\uff09\u5728\u9ad8\u8ffd\u8e2a\u8d1f\u8f7d\u4e0b\uff0c\u7f51\u7edc\u548c\u5b58\u50a8\u80fd\u8017\u6210\u4e3a\u4e3b\u5bfc\u56e0\u7d20\uff0c\u6539\u53d8\u4e86\u80fd\u8017\u5206\u5e03\u683c\u5c40\u3002", "conclusion": "\u5fae\u670d\u52a1\u5e94\u7528\u7684\u670d\u52a1\u5668\u80fd\u6e90\u6d4b\u91cf\u5fc5\u987b\u5305\u542b\u8ba1\u7b97\u3001\u7f51\u7edc\u548c\u5b58\u50a8\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5426\u5219\u4f1a\u4e25\u91cd\u4f4e\u4f30\u5b9e\u9645\u80fd\u8017\u3002GOXN\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5de5\u5177\u6765\u5168\u9762\u8bc4\u4f30\u5fae\u670d\u52a1\u67b6\u6784\u7684\u80fd\u6e90\u6548\u7387\u3002"}}
{"id": "2601.16935", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2601.16935", "abs": "https://arxiv.org/abs/2601.16935", "authors": ["Wei Wei", "Jingye Xu", "Sahidul Islam", "Dakai Zhu", "Chen Pan", "Mimi Xie"], "title": "AERO: Adaptive and Efficient Runtime-Aware OTA Updates for Energy-Harvesting IoT", "comment": "Accepted at DATE 2026", "summary": "Energy-harvesting (EH) Internet of Things (IoT) devices operate under intermittent energy availability, which disrupts task execution and makes energy-intensive over-the-air (OTA) updates particularly challenging. Conventional OTA update mechanisms rely on reboots and incur significant overhead, rendering them unsuitable for intermittently powered systems. Recent live OTA update techniques reduce reboot overhead but still lack mechanisms to ensure consistency when updates interact with runtime execution. This paper presents AERO, an Adaptive and Efficient Runtime-Aware OTA update mechanism that integrates update tasks into the device's Directed Acyclic Graph (DAG) and schedules them alongside routine tasks under energy and timing constraints. By identifying update-affected execution regions and dynamically adjusting dependencies, AERO ensures consistent up date integration while adapting to intermittent energy availability. Experiments on representative workloads demonstrate improved update reliability and efficiency compared to existing live update approaches.", "AI": {"tldr": "AERO\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u3001\u9ad8\u6548\u7684\u8fd0\u884c\u65f6\u611f\u77e5OTA\u66f4\u65b0\u673a\u5236\uff0c\u4e13\u4e3a\u80fd\u91cf\u6536\u96c6\u7269\u8054\u7f51\u8bbe\u5907\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5c06\u66f4\u65b0\u4efb\u52a1\u96c6\u6210\u5230\u8bbe\u5907DAG\u4e2d\u5e76\u4e0e\u5e38\u89c4\u4efb\u52a1\u534f\u540c\u8c03\u5ea6\uff0c\u89e3\u51b3\u95f4\u6b47\u4f9b\u7535\u73af\u5883\u4e0b\u7684\u66f4\u65b0\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u80fd\u91cf\u6536\u96c6\u7269\u8054\u7f51\u8bbe\u5907\u5728\u95f4\u6b47\u6027\u80fd\u6e90\u4f9b\u5e94\u4e0b\u8fd0\u884c\uff0c\u4f20\u7edfOTA\u66f4\u65b0\u673a\u5236\u4f9d\u8d56\u91cd\u542f\u4e14\u5f00\u9500\u5927\uff0c\u4e0d\u9002\u5408\u95f4\u6b47\u4f9b\u7535\u7cfb\u7edf\u3002\u73b0\u6709\u5b9e\u65f6OTA\u66f4\u65b0\u6280\u672f\u867d\u51cf\u5c11\u91cd\u542f\u5f00\u9500\uff0c\u4f46\u7f3a\u4e4f\u786e\u4fdd\u66f4\u65b0\u4e0e\u8fd0\u884c\u65f6\u6267\u884c\u4ea4\u4e92\u4e00\u81f4\u6027\u7684\u673a\u5236\u3002", "method": "\u63d0\u51faAERO\u673a\u5236\uff1a\u5c06\u66f4\u65b0\u4efb\u52a1\u96c6\u6210\u5230\u8bbe\u5907\u7684\u6709\u5411\u65e0\u73af\u56fe(DAG)\u4e2d\uff0c\u5728\u80fd\u91cf\u548c\u65f6\u95f4\u7ea6\u675f\u4e0b\u4e0e\u5e38\u89c4\u4efb\u52a1\u534f\u540c\u8c03\u5ea6\uff1b\u8bc6\u522b\u66f4\u65b0\u5f71\u54cd\u7684\u6267\u884c\u533a\u57df\uff0c\u52a8\u6001\u8c03\u6574\u4f9d\u8d56\u5173\u7cfb\uff0c\u786e\u4fdd\u4e00\u81f4\u7684\u66f4\u65b0\u96c6\u6210\u5e76\u9002\u5e94\u95f4\u6b47\u6027\u80fd\u6e90\u53ef\u7528\u6027\u3002", "result": "\u5728\u4ee3\u8868\u6027\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u5b9e\u65f6\u66f4\u65b0\u65b9\u6cd5\uff0cAERO\u5728\u66f4\u65b0\u53ef\u9760\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u6709\u6539\u8fdb\u3002", "conclusion": "AERO\u901a\u8fc7\u8fd0\u884c\u65f6\u611f\u77e5\u7684\u66f4\u65b0\u96c6\u6210\u548c\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u95f4\u6b47\u4f9b\u7535\u7269\u8054\u7f51\u8bbe\u5907OTA\u66f4\u65b0\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u548c\u9ad8\u6548\u7684\u66f4\u65b0\u673a\u5236\u3002"}}
{"id": "2601.16637", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.16637", "abs": "https://arxiv.org/abs/2601.16637", "authors": ["Jun Doi", "Tomonori Shirakawa", "Yukio Kawashima", "Seiji Yunoki", "Hiroshi Horii"], "title": "GPU-Accelerated Selected Basis Diagonalization with Thrust for SQD-based Algorithms", "comment": null, "summary": "Selected Basis Diagonalization (SBD) plays a central role in Sample-based Quantum Diagonalization (SQD), where iterative diagonalization of the Hamiltonian in selected configuration subspaces forms the dominant classical workload. We present a GPU-accelerated implementation of SBD using the Thrust library. By restructuring key components -- including configuration processing, excitation generation, and matrix-vector operations -- around fine-grained data-parallel primitives and flattened GPU-friendly data layouts, the proposed approach efficiently exploits modern GPU architectures. In our experiments, the Thrust-based SBD achieves up to $\\sim$40$\\times$ speedup over CPU execution and substantially reduces the total runtime of SQD iterations. These results demonstrate that GPU-native parallel primitives provide a simple, portable, and high-performance foundation for accelerating SQD-based quantum-classical workflows.", "AI": {"tldr": "GPU\u52a0\u901f\u7684Selected Basis Diagonalization (SBD)\u5b9e\u73b0\uff0c\u4f7f\u7528Thrust\u5e93\uff0c\u5728\u91cf\u5b50\u5bf9\u89d2\u5316\u8ba1\u7b97\u4e2d\u83b7\u5f9740\u500d\u52a0\u901f", "motivation": "SBD\u5728\u57fa\u4e8e\u6837\u672c\u7684\u91cf\u5b50\u5bf9\u89d2\u5316(SQD)\u4e2d\u8d77\u6838\u5fc3\u4f5c\u7528\uff0c\u5176\u8fed\u4ee3\u5bf9\u89d2\u5316\u8ba1\u7b97\u662f\u4e3b\u8981\u7ecf\u5178\u8ba1\u7b97\u8d1f\u62c5\uff0c\u9700\u8981GPU\u52a0\u901f\u6765\u63d0\u9ad8\u6548\u7387", "method": "\u4f7f\u7528Thrust\u5e93\u5b9e\u73b0GPU\u52a0\u901f\u7684SBD\uff0c\u91cd\u6784\u914d\u7f6e\u5904\u7406\u3001\u6fc0\u53d1\u751f\u6210\u548c\u77e9\u9635\u5411\u91cf\u8fd0\u7b97\u7b49\u5173\u952e\u7ec4\u4ef6\uff0c\u91c7\u7528\u7ec6\u7c92\u5ea6\u6570\u636e\u5e76\u884c\u539f\u8bed\u548c\u6241\u5e73\u5316\u7684GPU\u53cb\u597d\u6570\u636e\u5e03\u5c40", "result": "Thrust-based SBD\u76f8\u6bd4CPU\u6267\u884c\u83b7\u5f97\u9ad8\u8fbe40\u500d\u7684\u52a0\u901f\uff0c\u663e\u8457\u51cf\u5c11\u4e86SQD\u8fed\u4ee3\u7684\u603b\u8fd0\u884c\u65f6\u95f4", "conclusion": "GPU\u539f\u751f\u5e76\u884c\u539f\u8bed\u4e3a\u52a0\u901f\u57fa\u4e8eSQD\u7684\u91cf\u5b50-\u7ecf\u5178\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u7b80\u5355\u3001\u53ef\u79fb\u690d\u4e14\u9ad8\u6027\u80fd\u7684\u57fa\u7840"}}
{"id": "2601.16956", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.16956", "abs": "https://arxiv.org/abs/2601.16956", "authors": ["Avinash Maurya", "M. Mustafa Rafique", "Franck Cappello", "Bogdan Nicolae"], "title": "DataStates-LLM: Scalable Checkpointing for Transformer Models Using Composable State Providers", "comment": null, "summary": "The rapid growth of Large Transformer-based models, specifically Large Language Models (LLMs), now scaling to trillions of parameters, has necessitated training across thousands of GPUs using complex hybrid parallelism strategies (e.g., data, tensor, and pipeline parallelism). Checkpointing this massive, distributed state is critical for a wide range of use cases, such as resilience, suspend-resume, investigating undesirable training trajectories, and explaining model evolution. However, existing checkpointing solutions typically treat model state as opaque binary blobs, ignoring the ``3D heterogeneity'' of the underlying data structures--varying by memory location (GPU vs. Host), number of ``logical'' objects sharded and split across multiple files, data types (tensors vs. Python objects), and their serialization requirements. This results in significant runtime overheads due to blocking device-to-host transfers, data-oblivious serialization, and storage I/O contention. In this paper, we introduce DataStates-LLM, a novel checkpointing architecture that leverages State Providers to decouple state abstraction from data movement. DataStates-LLM exploits the immutability of model parameters during the forward and backward passes to perform ``lazy'', non-blocking asynchronous snapshots. By introducing State Providers, we efficiently coalesce fragmented, heterogeneous shards and overlap the serialization of metadata with bulk tensor I/O. We evaluate DataStates-LLM on models up to 70B parameters on 256 A100-40GB GPUs. Our results demonstrate that DataStates-LLM achieves up to 4$\\times$ higher checkpointing throughput and reduces end-to-end training time by up to 2.2$\\times$ compared to state-of-the-art solutions, effectively mitigating the serialization and heterogeneity bottlenecks in extreme-scale LLM training.", "AI": {"tldr": "DataStates-LLM\uff1a\u4e00\u79cd\u9488\u5bf9\u5927\u89c4\u6a21Transformer\u6a21\u578b\u8bad\u7ec3\u7684\u65b0\u578b\u68c0\u67e5\u70b9\u67b6\u6784\uff0c\u901a\u8fc7\u72b6\u6001\u63d0\u4f9b\u8005\u89e3\u8026\u72b6\u6001\u62bd\u8c61\u4e0e\u6570\u636e\u79fb\u52a8\uff0c\u5229\u7528\u53c2\u6570\u4e0d\u53d8\u6027\u5b9e\u73b0\u5f02\u6b65\u61d2\u5feb\u7167\uff0c\u663e\u8457\u63d0\u5347\u68c0\u67e5\u70b9\u541e\u5410\u91cf\u5e76\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u68c0\u67e5\u70b9\u65b9\u6848\u5c06\u6a21\u578b\u72b6\u6001\u89c6\u4e3a\u4e0d\u900f\u660e\u7684\u4e8c\u8fdb\u5236\u6570\u636e\u5757\uff0c\u5ffd\u7565\u4e86\u5e95\u5c42\u6570\u636e\u7ed3\u6784\u7684\"3D\u5f02\u8d28\u6027\"\uff08\u5185\u5b58\u4f4d\u7f6e\u3001\u5206\u7247\u6570\u91cf\u3001\u6570\u636e\u7c7b\u578b\u3001\u5e8f\u5217\u5316\u9700\u6c42\u5dee\u5f02\uff09\uff0c\u5bfc\u81f4\u8fd0\u884c\u65f6\u5f00\u9500\u5927\uff0c\u5305\u62ec\u963b\u585e\u7684\u8bbe\u5907\u5230\u4e3b\u673a\u4f20\u8f93\u3001\u6570\u636e\u65e0\u5173\u7684\u5e8f\u5217\u5316\u4ee5\u53ca\u5b58\u50a8I/O\u4e89\u7528\u3002", "method": "\u5f15\u5165DataStates-LLM\u67b6\u6784\uff0c\u901a\u8fc7\u72b6\u6001\u63d0\u4f9b\u8005\uff08State Providers\uff09\u89e3\u8026\u72b6\u6001\u62bd\u8c61\u4e0e\u6570\u636e\u79fb\u52a8\u3002\u5229\u7528\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u671f\u95f4\u6a21\u578b\u53c2\u6570\u7684\u4e0d\u53d8\u6027\uff0c\u6267\u884c\"\u61d2\"\u975e\u963b\u585e\u5f02\u6b65\u5feb\u7167\u3002\u6709\u6548\u5408\u5e76\u788e\u7247\u5316\u3001\u5f02\u6784\u7684\u5206\u7247\uff0c\u5e76\u5c06\u5143\u6570\u636e\u5e8f\u5217\u5316\u4e0e\u6279\u91cf\u5f20\u91cfI/O\u91cd\u53e0\u3002", "result": "\u5728256\u4e2aA100-40GB GPU\u4e0a\u8bc4\u4f30\u9ad8\u8fbe700\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u3002DataStates-LLM\u76f8\u6bd4\u6700\u5148\u8fdb\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u9ad8\u8fbe4\u500d\u7684\u68c0\u67e5\u70b9\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5e76\u5c06\u7aef\u5230\u7aef\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe2.2\u500d\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6781\u7aef\u89c4\u6a21LLM\u8bad\u7ec3\u4e2d\u7684\u5e8f\u5217\u5316\u548c\u5f02\u8d28\u6027\u74f6\u9888\u3002", "conclusion": "DataStates-LLM\u901a\u8fc7\u521b\u65b0\u7684\u72b6\u6001\u63d0\u4f9b\u8005\u67b6\u6784\u548c\u5f02\u6b65\u61d2\u5feb\u7167\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21Transformer\u6a21\u578b\u8bad\u7ec3\u4e2d\u68c0\u67e5\u70b9\u64cd\u4f5c\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u6781\u7aef\u89c4\u6a21LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u68c0\u67e5\u70b9\u89e3\u51b3\u65b9\u6848\u3002"}}
