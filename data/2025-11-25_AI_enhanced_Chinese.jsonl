{"id": "2511.17773", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.17773", "abs": "https://arxiv.org/abs/2511.17773", "authors": ["Shiv Kaushik", "Mahesh Madhav", "Nagi Aboulenein", "Jason Bessette", "Sandeep Brahmadathan", "Ben Chaffin", "Matthew Erler", "Stephan Jourdan", "Thomas Maciukenas", "Ramya Masti", "Jon Perry", "Massimo Sutera", "Scott Tetrick", "Bret Toll", "David Turley", "Carl Worth", "Atiq Bajwa"], "title": "Optimized Memory Tagging on AmpereOne Processors", "comment": "12 pages, 8 figures", "summary": "Memory-safety escapes continue to form the launching pad for a wide range of security attacks, especially for the substantial base of deployed software that is coded in pointer-based languages such as C/C++. Although compiler and Instruction Set Architecture (ISA) extensions have been introduced to address elements of this issue, the overhead and/or comprehensive applicability have limited broad production deployment. The Memory Tagging Extension (MTE) to the ARM AArch64 Instruction Set Architecture is a valuable tool to address memory-safety escapes; when used in synchronous tag-checking mode, MTE provides deterministic detection and prevention of sequential buffer overflow attacks, and probabilistic detection and prevention of exploits resulting from temporal use-after-free pointer programming bugs. The AmpereOne processor, launched in 2024, is the first datacenter processor to support MTE. Its optimized MTE implementation uniquely incurs no memory capacity overhead for tag storage and provides synchronous tag-checking with single-digit performance impact across a broad range of datacenter class workloads. Furthermore, this paper analyzes the complete hardware-software stack, identifying application memory management as the primary remaining source of overhead and highlighting clear opportunities for software optimization. The combination of an efficient hardware foundation and a clear path for software improvement makes the MTE implementation of the AmpereOne processor highly attractive for deployment in production cloud environments.", "AI": {"tldr": "ARM MTE\u5185\u5b58\u6807\u7b7e\u6269\u5c55\u6280\u672f\u9996\u6b21\u5728AmpereOne\u6570\u636e\u4e2d\u5fc3\u5904\u7406\u5668\u4e2d\u5b9e\u73b0\uff0c\u63d0\u4f9b\u540c\u6b65\u6807\u7b7e\u68c0\u67e5\u529f\u80fd\uff0c\u80fd\u6709\u6548\u68c0\u6d4b\u548c\u9884\u9632\u5185\u5b58\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e14\u4e0d\u5360\u7528\u989d\u5916\u5185\u5b58\u5bb9\u91cf\uff0c\u6027\u80fd\u5f71\u54cd\u4ec5\u4e3a\u4e2a\u4f4d\u6570\u767e\u5206\u6bd4\u3002", "motivation": "\u89e3\u51b3C/C++\u7b49\u6307\u9488\u8bed\u8a00\u4e2d\u7684\u5185\u5b58\u5b89\u5168\u6f0f\u6d1e\u95ee\u9898\uff0c\u8fd9\u4e9b\u6f0f\u6d1e\u662f\u4f17\u591a\u5b89\u5168\u653b\u51fb\u7684\u6839\u6e90\u3002\u73b0\u6709\u7684\u7f16\u8bd1\u5668\u6269\u5c55\u548cISA\u6269\u5c55\u5728\u5f00\u9500\u548c\u9002\u7528\u6027\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5728AmpereOne\u5904\u7406\u5668\u4e2d\u5b9e\u73b0ARM MTE\u5185\u5b58\u6807\u7b7e\u6269\u5c55\uff0c\u91c7\u7528\u540c\u6b65\u6807\u7b7e\u68c0\u67e5\u6a21\u5f0f\uff0c\u901a\u8fc7\u786c\u4ef6\u4f18\u5316\u5b9e\u73b0\u96f6\u5185\u5b58\u5bb9\u91cf\u5f00\u9500\u7684\u6807\u7b7e\u5b58\u50a8\u3002", "result": "MTE\u80fd\u591f\u786e\u5b9a\u6027\u68c0\u6d4b\u548c\u9884\u9632\u987a\u5e8f\u7f13\u51b2\u533a\u6ea2\u51fa\u653b\u51fb\uff0c\u6982\u7387\u6027\u68c0\u6d4b\u548c\u9884\u9632\u4e34\u65f6\u6027\u91ca\u653e\u540e\u4f7f\u7528\u6307\u9488\u9519\u8bef\u3002\u6027\u80fd\u5f71\u54cd\u4ec5\u4e3a\u4e2a\u4f4d\u6570\u767e\u5206\u6bd4\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u6570\u636e\u4e2d\u5fc3\u5de5\u4f5c\u8d1f\u8f7d\u3002", "conclusion": "AmpereOne\u5904\u7406\u5668\u7684MTE\u5b9e\u73b0\u4e3a\u751f\u4ea7\u4e91\u73af\u5883\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u786c\u4ef6\u57fa\u7840\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u5e94\u7528\u5185\u5b58\u7ba1\u7406\u4f5c\u4e3a\u4e3b\u8981\u5f00\u9500\u6765\u6e90\u7684\u8f6f\u4ef6\u4f18\u5316\u673a\u4f1a\u3002"}}
{"id": "2511.17971", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17971", "abs": "https://arxiv.org/abs/2511.17971", "authors": ["Jinsong Zhang", "Minghe Li", "Jiayi Tian", "Jinming Lu", "Zheng Zhang"], "title": "Comprehensive Design Space Exploration for Tensorized Neural Network Hardware Accelerators", "comment": null, "summary": "High-order tensor decomposition has been widely adopted to obtain compact deep neural networks for edge deployment. However, existing studies focus primarily on its algorithmic advantages such as accuracy and compression ratio-while overlooking the hardware deployment efficiency. Such hardware-unaware designs often obscure the potential latency and energy benefits of tensorized models. Although several works attempt to reduce computational cost by optimizing the contraction sequence based on the number of multiply-accumulate operations, they typically neglect the underlying hardware characteristics, resulting in suboptimal real-world performance. We observe that the contraction path, hardware architecture, and dataflow mapping are tightly coupled and must be optimized jointly within a unified design space to maximize deployment efficiency on real devices. To this end, we propose a co-exploration framework that unifies these dimensions within a unified design space for efficient training and inference of tensorized neural networks on edge platforms. The framework formulates a latency oriented search objective and solves it via a global latency-driven exploration across the unified design space to achieve end-to-end model efficiency. The optimized configurations are implemented on a configurable FPGA kernel, achieving up to 4 and 3.85 lower inference and training latency compared with the dense baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u534f\u540c\u63a2\u7d22\u6846\u67b6\uff0c\u5c06\u5f20\u91cf\u5206\u89e3\u6a21\u578b\u7684\u6536\u7f29\u8def\u5f84\u3001\u786c\u4ef6\u67b6\u6784\u548c\u6570\u636e\u6d41\u6620\u5c04\u8054\u5408\u4f18\u5316\uff0c\u4ee5\u6700\u5927\u5316\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u9ad8\u9636\u5f20\u91cf\u5206\u89e3\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7b97\u6cd5\u4f18\u52bf\uff08\u5982\u7cbe\u5ea6\u548c\u538b\u7f29\u6bd4\uff09\uff0c\u800c\u5ffd\u89c6\u4e86\u786c\u4ef6\u90e8\u7f72\u6548\u7387\u3002\u786c\u4ef6\u65e0\u5173\u7684\u8bbe\u8ba1\u5f80\u5f80\u63a9\u76d6\u4e86\u5f20\u91cf\u5316\u6a21\u578b\u7684\u5b9e\u9645\u5ef6\u8fdf\u548c\u80fd\u8017\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u534f\u540c\u63a2\u7d22\u6846\u67b6\uff0c\u5c06\u6536\u7f29\u8def\u5f84\u3001\u786c\u4ef6\u67b6\u6784\u548c\u6570\u636e\u6d41\u6620\u5c04\u7edf\u4e00\u5728\u4e00\u4e2a\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\uff0c\u901a\u8fc7\u9762\u5411\u5ef6\u8fdf\u7684\u641c\u7d22\u76ee\u6807\u8fdb\u884c\u5168\u5c40\u63a2\u7d22\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u6a21\u578b\u6548\u7387\u3002", "result": "\u5728\u53ef\u914d\u7f6eFPGA\u5185\u6838\u4e0a\u5b9e\u73b0\u4f18\u5316\u914d\u7f6e\uff0c\u76f8\u6bd4\u5bc6\u96c6\u57fa\u7ebf\u6a21\u578b\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e4\u500d\uff0c\u8bad\u7ec3\u5ef6\u8fdf\u964d\u4f4e3.85\u500d\u3002", "conclusion": "\u6536\u7f29\u8def\u5f84\u3001\u786c\u4ef6\u67b6\u6784\u548c\u6570\u636e\u6d41\u6620\u5c04\u662f\u7d27\u5bc6\u8026\u5408\u7684\uff0c\u5fc5\u987b\u5728\u7edf\u4e00\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\u8054\u5408\u4f18\u5316\uff0c\u624d\u80fd\u5728\u771f\u5b9e\u8bbe\u5907\u4e0a\u6700\u5927\u5316\u90e8\u7f72\u6548\u7387\u3002"}}
{"id": "2511.18234", "categories": ["cs.AR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.18234", "abs": "https://arxiv.org/abs/2511.18234", "authors": ["Quanling Zhao", "Yanru Chen", "Runyang Tian", "Sumukh Pinge", "Weihong Xu", "Augusto Vega", "Steven Holmes", "Saransh Gupta", "Tajana Rosing"], "title": "HDDB: Efficient In-Storage SQL Database Search Using Hyperdimensional Computing on Ferroelectric NAND Flash", "comment": null, "summary": "Hyperdimensional Computing (HDC) encodes information and data into high-dimensional distributed vectors that can be manipulated using simple bitwise operations and similarity searches, offering parallelism, low-precision hardware friendliness, and strong robustness to noise. These properties are a natural fit for SQL database workloads dominated by predicate evaluation and scans, which demand low energy and low latency over large fact tables. Notably, HDC's noise-tolerance maps well onto emerging ferroelectric NAND (FeNAND) memories, which provide ultra-high density and in-storage compute capability but suffer from elevated raw bit-error rates. In this work, we propose HDDB, a hardware-software co-design that combines HDC with FeNAND multi-level cells (MLC) to perform in-storage SQL predicate evaluation and analytics with massive parallelism and minimal data movement. Particularly, we introduce novel HDC encoding techniques for standard SQL data tables and formulate predicate-based filtering and aggregation as highly efficient HDC operations that can happen in-storage. By exploiting the intrinsic redundancy of HDC, HDDB maintains correct predicate and decode outcomes under substantial device noise (up to 10% randomly corrupted TLC cells) without explicit error-correction overheads. Experiments on TPC-DS fact tables show that HDDB achieves up to 80.6x lower latency and 12,636x lower energy consumption compared to conventional CPU/GPU SQL database engines, suggesting that HDDB provides a practical substrate for noise-robust, memory-centric database processing.", "AI": {"tldr": "HDDB\u662f\u4e00\u4e2a\u7ed3\u5408\u8d85\u7ef4\u8ba1\u7b97(HDC)\u548c\u94c1\u7535NAND\u5b58\u50a8\u5668\u7684\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u7528\u4e8e\u5728\u5b58\u50a8\u4e2d\u6267\u884cSQL\u8c13\u8bcd\u8bc4\u4f30\u548c\u5206\u6790\uff0c\u5177\u6709\u5927\u89c4\u6a21\u5e76\u884c\u6027\u548c\u6700\u5c0f\u6570\u636e\u79fb\u52a8\u3002", "motivation": "\u5229\u7528HDC\u7684\u566a\u58f0\u5bb9\u5fcd\u7279\u6027\u4e0e\u65b0\u5174\u94c1\u7535NAND\u5b58\u50a8\u5668\u7684\u9ad8\u5bc6\u5ea6\u548c\u5b58\u50a8\u5185\u8ba1\u7b97\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u4f20\u7edfSQL\u6570\u636e\u5e93\u5728\u5927\u578b\u4e8b\u5b9e\u8868\u4e0a\u8bc4\u4f30\u8c13\u8bcd\u548c\u626b\u63cf\u65f6\u5bf9\u4f4e\u80fd\u8017\u548c\u4f4e\u5ef6\u8fdf\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684HDC\u7f16\u7801\u6280\u672f\u7528\u4e8e\u6807\u51c6SQL\u6570\u636e\u8868\uff0c\u5c06\u57fa\u4e8e\u8c13\u8bcd\u7684\u8fc7\u6ee4\u548c\u805a\u5408\u516c\u5f0f\u5316\u4e3a\u9ad8\u6548\u7684HDC\u64cd\u4f5c\uff0c\u5229\u7528HDC\u5185\u5728\u5197\u4f59\u6027\u5728\u8bbe\u5907\u566a\u58f0\u4e0b\u4fdd\u6301\u6b63\u786e\u7ed3\u679c\u3002", "result": "\u5728TPC-DS\u4e8b\u5b9e\u8868\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cHDDB\u76f8\u6bd4\u4f20\u7edfCPU/GPU SQL\u6570\u636e\u5e93\u5f15\u64ce\u5b9e\u73b0\u4e8680.6\u500d\u7684\u4f4e\u5ef6\u8fdf\u548c12,636\u500d\u7684\u4f4e\u80fd\u8017\u3002", "conclusion": "HDDB\u4e3a\u566a\u58f0\u9c81\u68d2\u3001\u5185\u5b58\u4e2d\u5fc3\u7684\u6570\u636e\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u57fa\u7840\uff0c\u7279\u522b\u9002\u5408\u5728\u5177\u6709\u9ad8\u539f\u59cb\u6bd4\u7279\u9519\u8bef\u7387\u7684\u5b58\u50a8\u8bbe\u5907\u4e0a\u6267\u884c\u6570\u636e\u5e93\u64cd\u4f5c\u3002"}}
{"id": "2511.18687", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.18687", "abs": "https://arxiv.org/abs/2511.18687", "authors": ["Kasidis Arunruangsirilert", "Jiro Katto"], "title": "Evaluation of NVENC Split-Frame Encoding (SFE) for UHD Video Transcoding", "comment": "2025 Picture Coding Symposium (PCS 2025), 8-11 December 2025, Aachen, Germany", "summary": "NVIDIA Encoder (NVENC) features in modern NVIDIA GPUs, offer significant advantages over software encoders by providing comparable Rate-Distortion (RD) performance while consuming considerably less power. The increasing capability of consumer devices to capture footage in Ultra High-Definition (UHD) at 4K and 8K resolutions necessitates high-performance video transcoders for internet-based delivery. To address this demand, NVIDIA introduced Split-Frame Encoding (SFE), a technique that leverages multiple on-die NVENC chips available in high-end GPUs. SFE splits a single UHD frame for parallel encoding across these physical encoders and subsequently stitches the results, which significantly improves encoding throughput. However, this approach is known to incur an RD performance penalty. The widespread adoption of NVIDIA GPUs in data centers, driven by the rise of Generative AI, means NVENC is poised to play a critical role in transcoding UHD video. To better understand the performance-efficiency tradeoff of SFE, this paper evaluates SFE's impact on RD performance, encoding throughput, power consumption, and end-to-end latency using standardized test sequences. The results show that for real-time applications, SFE nearly doubles encoding throughput with a negligible RD performance penalty, which enables the use of higher-quality presets for 4K and makes real-time 8K encoding feasible, effectively offsetting the minor RD penalty. Moreover, SFE adds no latency at 4K and can reduce it at 8K, positioning it as a key enabler for high-throughput, real-time UHD transcoding.", "AI": {"tldr": "NVIDIA Split-Frame Encoding (SFE) \u6280\u672f\u901a\u8fc7\u5c06\u5355\u4e2aUHD\u5e27\u5206\u5272\u5230\u591a\u4e2aNVENC\u82af\u7247\u5e76\u884c\u7f16\u7801\uff0c\u663e\u8457\u63d0\u5347\u7f16\u7801\u541e\u5410\u91cf\uff0c\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u80fd\u4ee5\u53ef\u5ffd\u7565\u7684RD\u6027\u80fd\u635f\u5931\u5b9e\u73b0\u8fd1\u4e4e\u53cc\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u968f\u7740\u6d88\u8d39\u8bbe\u5907\u80fd\u591f\u62cd\u64444K\u548c8K\u8d85\u9ad8\u6e05\u89c6\u9891\uff0c\u9700\u8981\u9ad8\u6027\u80fd\u89c6\u9891\u8f6c\u7801\u5668\u8fdb\u884c\u4e92\u8054\u7f51\u4f20\u8f93\u3002NVIDIA GPU\u5728\u6570\u636e\u4e2d\u5fc3\u5e7f\u6cdb\u5e94\u7528\uff0cNVENC\u5c06\u5728UHD\u89c6\u9891\u8f6c\u7801\u4e2d\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u5316\u6d4b\u8bd5\u5e8f\u5217\u8bc4\u4f30SFE\u5bf9RD\u6027\u80fd\u3001\u7f16\u7801\u541e\u5410\u91cf\u3001\u529f\u8017\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u7684\u5f71\u54cd\u3002SFE\u5c06\u5355\u4e2aUHD\u5e27\u5206\u5272\u5230\u591a\u4e2a\u7269\u7406\u7f16\u7801\u5668\u5e76\u884c\u7f16\u7801\uff0c\u7136\u540e\u62fc\u63a5\u7ed3\u679c\u3002", "result": "\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\uff0cSFE\u4f7f\u7f16\u7801\u541e\u5410\u91cf\u8fd1\u4e4e\u7ffb\u500d\uff0cRD\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565\uff0c\u652f\u63014K\u4f7f\u7528\u66f4\u9ad8\u8d28\u91cf\u9884\u8bbe\uff0c\u4f7f\u5b9e\u65f68K\u7f16\u7801\u6210\u4e3a\u53ef\u80fd\u30024K\u65e0\u5ef6\u8fdf\u589e\u52a0\uff0c8K\u53ef\u51cf\u5c11\u5ef6\u8fdf\u3002", "conclusion": "SFE\u662f\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u3001\u5b9e\u65f6UHD\u8f6c\u7801\u7684\u5173\u952e\u6280\u672f\uff0c\u5728\u541e\u5410\u91cf\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2511.17849", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.17849", "abs": "https://arxiv.org/abs/2511.17849", "authors": ["Shuyuan Fan", "Zhao Zhang"], "title": "Pier: Efficient Large Language Model pretraining with Relaxed Global Communication", "comment": null, "summary": "Global communication, such as all-reduce and allgather, is the prominent performance bottleneck in large language model (LLM) pretraining. To address this issue, we present Pier, an efficient and scalable optimizer with relaxed global communication. Pier is built upon DiLoCo, which leverages an inner optimizer within groups of processors and an outer optimizer that requires global communication. To preserve the convergence and model performance, Pier incorporates two key techniques for the outer optimizer: momentum warmup and momentum decay. Pier employs an efficient and scalable system architecture to enable complex parallelization strategies in LLM pretraining. We examine the model performance and runtime reduction of Pier using the GPT model family (e.g., small, medium, XL, and 7B) and the OpenWebText dataset with a suite of thirteen downstream tasks. With data parallel strategy, Pier speeds up GPT-2 XL training by up to 2.7x-3.7x on 256 NVIDIA A100 GPUs and 1.2x-1.9x on 64 GH200 Superchips, respectively, without degradation of validation loss or downstream task performance. With data parallel and tensor parallel, Pier reduces the time cost GPT-2 7B model training by 54.5% on 128 A100s.", "AI": {"tldr": "Pier\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u51cf\u5c11\u5168\u5c40\u901a\u4fe1\u6765\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u5168\u5c40\u901a\u4fe1\uff08\u5982all-reduce\u548callgather\uff09\u662f\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u7684\u4e3b\u8981\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u5668\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "method": "\u57fa\u4e8eDiLoCo\u6846\u67b6\uff0c\u5728\u5904\u7406\u5668\u7ec4\u5185\u4f7f\u7528\u5185\u90e8\u4f18\u5316\u5668\uff0c\u5168\u5c40\u901a\u4fe1\u4f7f\u7528\u5916\u90e8\u4f18\u5316\u5668\uff0c\u5e76\u5f15\u5165\u52a8\u91cf\u9884\u70ed\u548c\u52a8\u91cf\u8870\u51cf\u6280\u672f\u6765\u4fdd\u6301\u6536\u655b\u6027\u3002", "result": "\u5728256\u4e2aA100 GPU\u4e0a\uff0cPier\u5c06GPT-2 XL\u8bad\u7ec3\u901f\u5ea6\u63d0\u53472.7x-3.7x\uff1b\u572864\u4e2aGH200 Superchips\u4e0a\u63d0\u53471.2x-1.9x\uff1b\u5728128\u4e2aA100\u4e0a\uff0cGPT-2 7B\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1154.5%\uff0c\u4e14\u9a8c\u8bc1\u635f\u5931\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u65e0\u4e0b\u964d\u3002", "conclusion": "Pier\u901a\u8fc7\u653e\u677e\u5168\u5c40\u901a\u4fe1\u8981\u6c42\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\uff0c\u662f\u89e3\u51b3\u5206\u5e03\u5f0f\u8bad\u7ec3\u901a\u4fe1\u74f6\u9888\u7684\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2511.17838", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.17838", "abs": "https://arxiv.org/abs/2511.17838", "authors": ["Jai Arora", "Sirui Lu", "Devansh Jain", "Tianfan Xu", "Farzin Houshmand", "Phitchaya Mangpo Phothilimthana", "Mohsen Lesani", "Praveen Narayanan", "Karthik Srinivasa Murthy", "Rastislav Bodik", "Amit Sabne", "Charith Mendis"], "title": "TensorRight: Automated Verification of Tensor Graph Rewrites", "comment": "61 pages, 13 figures, published in POPL 2025", "summary": "Tensor compilers, essential for generating efficient code for deep learning models across various applications, employ tensor graph rewrites as one of the key optimizations. These rewrites optimize tensor computational graphs with the expectation of preserving semantics for tensors of arbitrary rank and size. Despite this expectation, to the best of our knowledge, there does not exist a fully automated verification system to prove the soundness of these rewrites for tensors of arbitrary rank and size. Previous works, while successful in verifying rewrites with tensors of concrete rank, do not provide guarantees in the unbounded setting.\n  To fill this gap, we introduce TensorRight, the first automatic verification system that can verify tensor graph rewrites for input tensors of arbitrary rank and size. We introduce a core language, TensorRight DSL, to represent rewrite rules using a novel axis definition, called aggregated-axis, which allows us to reason about an unbounded number of axes. We achieve unbounded verification by proving that there exists a bound on tensor ranks, under which bounded verification of all instances implies the correctness of the rewrite rule in the unbounded setting. We derive an algorithm to compute this rank using the denotational semantics of TensorRight DSL. TensorRight employs this algorithm to generate a finite number of bounded-verification proof obligations, which are then dispatched to an SMT solver using symbolic execution to automatically verify the correctness of the rewrite rules. We evaluate TensorRight's verification capabilities by implementing rewrite rules present in XLA's algebraic simplifier. The results demonstrate that TensorRight can prove the correctness of 115 out of 175 rules in their full generality, while the closest automatic, bounded-verification system can express only 18 of these rules.", "AI": {"tldr": "TensorRight\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u9a8c\u8bc1\u4efb\u610f\u79e9\u548c\u5c3a\u5bf8\u5f20\u91cf\u56fe\u91cd\u5199\u7684\u81ea\u52a8\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u805a\u5408\u8f74\u5b9a\u4e49\u548c\u8fb9\u754c\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u65e0\u6cd5\u5904\u7406\u65e0\u754c\u5f20\u91cf\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f20\u91cf\u7f16\u8bd1\u5668\u91cd\u5199\u4f18\u5316\u7f3a\u4e4f\u5bf9\u4efb\u610f\u79e9\u548c\u5c3a\u5bf8\u5f20\u91cf\u7684\u81ea\u52a8\u9a8c\u8bc1\u4fdd\u8bc1\uff0c\u53ea\u80fd\u9a8c\u8bc1\u5177\u4f53\u79e9\u7684\u5f20\u91cf\uff0c\u65e0\u6cd5\u63d0\u4f9b\u65e0\u754c\u8bbe\u7f6e\u4e0b\u7684\u6b63\u786e\u6027\u4fdd\u8bc1\u3002", "method": "\u5f15\u5165TensorRight DSL\u6838\u5fc3\u8bed\u8a00\uff0c\u4f7f\u7528\u805a\u5408\u8f74\u5b9a\u4e49\u8868\u793a\u91cd\u5199\u89c4\u5219\uff1b\u901a\u8fc7\u8bc1\u660e\u5b58\u5728\u79e9\u8fb9\u754c\uff0c\u5c06\u65e0\u754c\u9a8c\u8bc1\u8f6c\u5316\u4e3a\u6709\u9650\u6570\u91cf\u7684\u8fb9\u754c\u9a8c\u8bc1\u4efb\u52a1\uff1b\u4f7f\u7528\u7b26\u53f7\u6267\u884c\u548cSMT\u6c42\u89e3\u5668\u81ea\u52a8\u9a8c\u8bc1\u3002", "result": "\u5728XLA\u4ee3\u6570\u7b80\u5316\u5668\u7684175\u4e2a\u91cd\u5199\u89c4\u5219\u4e2d\uff0cTensorRight\u80fd\u591f\u8bc1\u660e115\u4e2a\u89c4\u5219\u5728\u5b8c\u5168\u901a\u7528\u6027\u4e0b\u7684\u6b63\u786e\u6027\uff0c\u800c\u6700\u63a5\u8fd1\u7684\u81ea\u52a8\u8fb9\u754c\u9a8c\u8bc1\u7cfb\u7edf\u53ea\u80fd\u8868\u8fbe18\u4e2a\u89c4\u5219\u3002", "conclusion": "TensorRight\u586b\u8865\u4e86\u5f20\u91cf\u56fe\u91cd\u5199\u65e0\u754c\u9a8c\u8bc1\u7684\u7a7a\u767d\uff0c\u4e3a\u5f20\u91cf\u7f16\u8bd1\u5668\u63d0\u4f9b\u4e86\u9996\u4e2a\u80fd\u591f\u9a8c\u8bc1\u4efb\u610f\u79e9\u548c\u5c3a\u5bf8\u5f20\u91cf\u91cd\u5199\u6b63\u786e\u6027\u7684\u81ea\u52a8\u9a8c\u8bc1\u7cfb\u7edf\u3002"}}
{"id": "2511.18688", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.18688", "abs": "https://arxiv.org/abs/2511.18688", "authors": ["Kasidis Arunruangsirilert", "Jiro Katto"], "title": "Evaluation of GPU Video Encoder for Low-Latency Real-Time 4K UHD Encoding", "comment": "2025 IEEE International Conference on Visual Communications and Image Processing (VCIP 2025), 1-4 December 2025, Klagenfurt, Austria", "summary": "The demand for high-quality, real-time video streaming has grown exponentially, with 4K Ultra High Definition (UHD) becoming the new standard for many applications such as live broadcasting, TV services, and interactive cloud gaming. This trend has driven the integration of dedicated hardware encoders into modern Graphics Processing Units (GPUs). Nowadays, these encoders support advanced codecs like HEVC and AV1 and feature specialized Low-Latency and Ultra Low-Latency tuning, targeting end-to-end latencies of < 2 seconds and < 500 ms, respectively. As the demand for such capabilities grows toward the 6G era, a clear understanding of their performance implications is essential. In this work, we evaluate the low-latency encoding modes on GPUs from NVIDIA, Intel, and AMD from both Rate-Distortion (RD) performance and latency perspectives. The results are then compared against both the normal-latency tuning of hardware encoders and leading software encoders. Results show hardware encoders achieve significantly lower E2E latency than software solutions with slightly better RD performance. While standard Low-Latency tuning yields a poor quality-latency trade-off, the Ultra Low-Latency mode reduces E2E latency to 83 ms (5 frames) without additional RD impact. Furthermore, hardware encoder latency is largely insensitive to quality presets, enabling high-quality, low-latency streams without compromise.", "AI": {"tldr": "\u8bc4\u4f30NVIDIA\u3001Intel\u548cAMD GPU\u4e0a\u7684\u4f4e\u5ef6\u8fdf\u7f16\u7801\u6a21\u5f0f\uff0c\u6bd4\u8f83\u786c\u4ef6\u7f16\u7801\u5668\u548c\u8f6f\u4ef6\u7f16\u7801\u5668\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u786c\u4ef6\u7f16\u7801\u5668\u5728\u5b9e\u73b0\u8d85\u4f4e\u5ef6\u8fdf\uff0883ms\uff09\u7684\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7387\u5931\u771f\u6027\u80fd\u3002", "motivation": "\u968f\u77404K\u8d85\u9ad8\u6e05\u89c6\u9891\u6d41\u9700\u6c42\u589e\u957f\uff0c\u9700\u8981\u4e86\u89e3GPU\u786c\u4ef6\u7f16\u7801\u5668\u5728\u4f4e\u5ef6\u8fdf\u6a21\u5f0f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3a6G\u65f6\u4ee3\u7684\u5b9e\u65f6\u89c6\u9891\u5e94\u7528\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u4ece\u7387\u5931\u771f\u6027\u80fd\u548c\u5ef6\u8fdf\u4e24\u4e2a\u89d2\u5ea6\u8bc4\u4f30\u4e09\u5927\u5382\u5546GPU\u7684\u4f4e\u5ef6\u8fdf\u7f16\u7801\u6a21\u5f0f\uff0c\u5e76\u4e0e\u786c\u4ef6\u7f16\u7801\u5668\u7684\u6b63\u5e38\u5ef6\u8fdf\u6a21\u5f0f\u53ca\u9886\u5148\u8f6f\u4ef6\u7f16\u7801\u5668\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u786c\u4ef6\u7f16\u7801\u5668\u76f8\u6bd4\u8f6f\u4ef6\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u663e\u8457\u66f4\u4f4e\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u4e14\u7387\u5931\u771f\u6027\u80fd\u7565\u4f18\uff1b\u8d85\u4f4e\u5ef6\u8fdf\u6a21\u5f0f\u53ef\u5c06\u5ef6\u8fdf\u964d\u81f383ms\uff085\u5e27\uff09\u800c\u4e0d\u5f71\u54cd\u7387\u5931\u771f\u6027\u80fd\uff1b\u786c\u4ef6\u7f16\u7801\u5668\u5ef6\u8fdf\u5bf9\u8d28\u91cf\u9884\u8bbe\u4e0d\u654f\u611f\u3002", "conclusion": "GPU\u786c\u4ef6\u7f16\u7801\u5668\u80fd\u591f\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u89c6\u9891\u6d41\u4f20\u8f93\uff0c\u8d85\u4f4e\u5ef6\u8fdf\u6a21\u5f0f\u5728\u4fdd\u6301\u7387\u5931\u771f\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u5ef6\u8fdf\uff0c\u4e3a\u5b9e\u65f6\u89c6\u9891\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u60f3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.17882", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.17882", "abs": "https://arxiv.org/abs/2511.17882", "authors": ["Ruide Cao", "Zhuyun Qi", "Qinyang He", "Chenxi Ling", "Yi Wang", "Guoming Tang"], "title": "SAGkit: A Python SAG Toolkit for Response Time Analysis of Hybrid-Triggered Jobs", "comment": "6 pages, 5 figures, ICDCS 2025 Demo Paper", "summary": "For distributed control systems, modern latency-critical applications are increasingly demanding real-time guarantees and robustness. Response-time analysis (RTA) is useful for this purpose, as it helps analyze and guarantee timing bounds. However, conventional RTA methods struggle with the state-space explosion problem, especially in non-preemptive systems with release jitter and execution time variations. In this paper, we introduce SAGkit, a Python toolkit that implements the schedule-abstraction graph (SAG) framework. SAGkit novelly enables exact and sustainable RTA of hybrid-triggered jobs by allowing job absence on the SAG basis. Our experiments demonstrate that SAGkit achieves exactness with acceptable runtime and memory overhead. This lightweight toolkit empowers researchers to analyze complex distributed control systems and is open-access for further development.", "AI": {"tldr": "SAGkit\u662f\u4e00\u4e2aPython\u5de5\u5177\u5305\uff0c\u5b9e\u73b0\u4e86\u8c03\u5ea6\u62bd\u8c61\u56fe(SAG)\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u63a7\u5236\u7cfb\u7edf\u7684\u7cbe\u786e\u53ef\u6301\u7eed\u54cd\u5e94\u65f6\u95f4\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u975e\u62a2\u5360\u5f0f\u7cfb\u7edf\u4e2d\u72b6\u6001\u7a7a\u95f4\u7206\u70b8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u5ef6\u8fdf\u5173\u952e\u5e94\u7528\u5bf9\u5b9e\u65f6\u6027\u548c\u9c81\u68d2\u6027\u8981\u6c42\u8d8a\u6765\u8d8a\u9ad8\uff0c\u4f20\u7edf\u54cd\u5e94\u65f6\u95f4\u5206\u6790\u65b9\u6cd5\u5728\u5904\u7406\u975e\u62a2\u5360\u5f0f\u7cfb\u7edf\u3001\u91ca\u653e\u6296\u52a8\u548c\u6267\u884c\u65f6\u95f4\u53d8\u5316\u65f6\u9762\u4e34\u72b6\u6001\u7a7a\u95f4\u7206\u70b8\u95ee\u9898\u3002", "method": "\u5f00\u53d1SAGkit\u5de5\u5177\u5305\uff0c\u57fa\u4e8e\u8c03\u5ea6\u62bd\u8c61\u56fe\u6846\u67b6\uff0c\u901a\u8fc7\u5728SAG\u57fa\u7840\u4e0a\u5141\u8bb8\u4f5c\u4e1a\u7f3a\u5e2d\uff0c\u5b9e\u73b0\u6df7\u5408\u89e6\u53d1\u4f5c\u4e1a\u7684\u7cbe\u786e\u53ef\u6301\u7eed\u54cd\u5e94\u65f6\u95f4\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSAGkit\u5728\u53ef\u63a5\u53d7\u7684\u8fd0\u884c\u65f6\u548c\u5185\u5b58\u5f00\u9500\u4e0b\u5b9e\u73b0\u4e86\u7cbe\u786e\u6027\uff0c\u80fd\u591f\u5206\u6790\u590d\u6742\u7684\u5206\u5e03\u5f0f\u63a7\u5236\u7cfb\u7edf\u3002", "conclusion": "SAGkit\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u5206\u6790\u590d\u6742\u5206\u5e03\u5f0f\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u5f00\u53d1\u3002"}}
{"id": "2511.18755", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.18755", "abs": "https://arxiv.org/abs/2511.18755", "authors": ["Xiaotong Huang", "He Zhu", "Tianrui Ma", "Yuxiang Xiong", "Fangxin Liu", "Zhezhi He", "Yiming Gan", "Zihan Liu", "Jingwen Leng", "Yu Feng", "Minyi Guo"], "title": "Splatonic: Architecture Support for 3D Gaussian Splatting SLAM via Sparse Processing", "comment": null, "summary": "3D Gaussian splatting (3DGS) has emerged as a promising direction for SLAM due to its high-fidelity reconstruction and rapid convergence. However, 3DGS-SLAM algorithms remain impractical for mobile platforms due to their high computational cost, especially for their tracking process.\n  This work introduces Splatonic, a sparse and efficient real-time 3DGS-SLAM algorithm-hardware co-design for resource-constrained devices. Inspired by classical SLAMs, we propose an adaptive sparse pixel sampling algorithm that reduces the number of rendered pixels by up to 256$\\times$ while retaining accuracy. To unlock this performance potential on mobile GPUs, we design a novel pixel-based rendering pipeline that improves hardware utilization via Gaussian-parallel rendering and preemptive $\u03b1$-checking. Together, these optimizations yield up to 121.7$\\times$ speedup on the bottleneck stages and 14.6$\\times$ end-to-end speedup on off-the-shelf GPUs. To further address new bottlenecks introduced by our rendering pipeline, we propose a pipelined architecture that simplifies the overall design while addressing newly emerged bottlenecks in projection and aggregation. Evaluated across four 3DGS-SLAM algorithms, Splatonic achieves up to 274.9$\\times$ speedup and 4738.5$\\times$ energy savings over mobile GPUs and up to 25.2$\\times$ speedup and 241.1$\\times$ energy savings over state-of-the-art accelerators, all with comparable accuracy.", "AI": {"tldr": "Splatonic\u662f\u4e00\u4e2a\u7a00\u758f\u9ad8\u6548\u76843D\u9ad8\u65af\u6e85\u5c04SLAM\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7a00\u758f\u50cf\u7d20\u91c7\u6837\u548c\u50cf\u7d20\u7ea7\u6e32\u67d3\u6d41\u6c34\u7ebf\uff0c\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c\u76f8\u6bd4\u79fb\u52a8GPU\u83b7\u5f97274.9\u500d\u52a0\u901f\u548c4738.5\u500d\u8282\u80fd\u3002", "motivation": "3D\u9ad8\u65af\u6e85\u5c04SLAM\u7b97\u6cd5\u5728\u79fb\u52a8\u5e73\u53f0\u4e0a\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u7279\u522b\u662f\u8ddf\u8e2a\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u7a00\u758f\u50cf\u7d20\u91c7\u6837\u7b97\u6cd5\u51cf\u5c11\u6e32\u67d3\u50cf\u7d20\u6570\u8fbe256\u500d\uff1b\u8bbe\u8ba1\u50cf\u7d20\u7ea7\u6e32\u67d3\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u9ad8\u65af\u5e76\u884c\u6e32\u67d3\u548c\u62a2\u5360\u5f0f\u03b1\u68c0\u67e5\u63d0\u9ad8\u786c\u4ef6\u5229\u7528\u7387\uff1b\u63d0\u51fa\u6d41\u6c34\u7ebf\u67b6\u6784\u7b80\u5316\u8bbe\u8ba1\u5e76\u89e3\u51b3\u6295\u5f71\u548c\u805a\u5408\u74f6\u9888\u3002", "result": "\u5728\u56db\u4e2a3DGS-SLAM\u7b97\u6cd5\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u79fb\u52a8GPU\u83b7\u5f97274.9\u500d\u52a0\u901f\u548c4738.5\u500d\u8282\u80fd\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u52a0\u901f\u5668\u83b7\u5f9725.2\u500d\u52a0\u901f\u548c241.1\u500d\u8282\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7cbe\u5ea6\u3002", "conclusion": "Splatonic\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6210\u529f\u89e3\u51b3\u4e863D\u9ad8\u65af\u6e85\u5c04SLAM\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u6027\u80fd\u3002"}}
{"id": "2511.18124", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.18124", "abs": "https://arxiv.org/abs/2511.18124", "authors": ["Sangam Ghimire", "Nigam Niraula", "Nirjal Bhurtel", "Paribartan Timalsina", "Bishal Neupane", "James Bhattarai", "Sudan Jha"], "title": "MIDAS: Adaptive Proxy Middleware for Mitigating Metadata Hotspots in HPC I/O at Scale", "comment": null, "summary": "Metadata hotspots remain one of the key obstacles to scalable Input/Output (I/O) in both High-Performance Computing (HPC) and cloud-scale storage environments. Situations such as job start-ups, checkpoint storms, or heavily skewed namespace access can trigger thousands of concurrent metadata requests against a small subset of servers. The result is long queues, inflated tail latencies, and reduced system throughput. Prior efforts including static namespace partitioning, backend-specific extensions, and kernel-level modifications address parts of the problem, but they often prove too rigid, intrusive to deploy, or unstable under shifting workloads. We present MIDAS, an adaptive middleware layer that operates transparently between clients and metadata servers, requiring no changes to kernels or storage backends. The design brings together three mechanisms: (i) a namespace-aware load balancer that enhances consistent hashing with power-of-d sampling informed by live telemetry, (ii) a cooperative caching layer that preserves backend semantics through leases, invalidations, or adaptive timeouts, and (iii) a self-stabilizing control loop that dynamically adjusts routing aggressiveness and cache lifetimes while avoiding oscillations under bursty workloads. Analysis of the model and controlled experiments show that MIDAS reduces average queue lengths by roughly 23% and mitigates worst-case hotspots by up to 80% when compared to round-robin scheduling. These findings highlight that a stability-aware, middleware-based strategy can provide backend-agnostic improvements to metadata management, enabling better scalability in bursty scenarios, more predictable tail latencies, and stronger overall system performance.", "AI": {"tldr": "MIDAS\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u4e2d\u95f4\u4ef6\u5c42\uff0c\u901a\u8fc7\u667a\u80fd\u8d1f\u8f7d\u5747\u8861\u3001\u534f\u4f5c\u7f13\u5b58\u548c\u81ea\u7a33\u5b9a\u63a7\u5236\u5faa\u73af\u6765\u89e3\u51b3\u5143\u6570\u636e\u70ed\u70b9\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u961f\u5217\u957f\u5ea6\u548c\u70ed\u70b9\u5f71\u54cd\u3002", "motivation": "\u5143\u6570\u636e\u70ed\u70b9\u662fHPC\u548c\u4e91\u5b58\u50a8\u73af\u5883\u4e2d\u53ef\u6269\u5c55I/O\u7684\u4e3b\u8981\u969c\u788d\uff0c\u4f1a\u5bfc\u81f4\u957f\u961f\u5217\u3001\u5c3e\u90e8\u5ef6\u8fdf\u589e\u52a0\u548c\u7cfb\u7edf\u541e\u5410\u91cf\u964d\u4f4e\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8fc7\u4e8e\u50f5\u5316\u3001\u90e8\u7f72\u4fb5\u5165\u6027\u5f3a\u6216\u5728\u52a8\u6001\u8d1f\u8f7d\u4e0b\u4e0d\u7a33\u5b9a\u3002", "method": "MIDAS\u5305\u542b\u4e09\u4e2a\u673a\u5236\uff1a(1)\u57fa\u4e8e\u5b9e\u65f6\u9065\u6d4b\u7684\u547d\u540d\u7a7a\u95f4\u611f\u77e5\u8d1f\u8f7d\u5747\u8861\u5668\uff0c(2)\u901a\u8fc7\u79df\u7ea6\u3001\u5931\u6548\u6216\u81ea\u9002\u5e94\u8d85\u65f6\u4fdd\u6301\u540e\u7aef\u8bed\u4e49\u7684\u534f\u4f5c\u7f13\u5b58\u5c42\uff0c(3)\u52a8\u6001\u8c03\u6574\u8def\u7531\u653b\u51fb\u6027\u548c\u7f13\u5b58\u5bff\u547d\u7684\u81ea\u7a33\u5b9a\u63a7\u5236\u5faa\u73af\u3002", "result": "\u4e0e\u8f6e\u8be2\u8c03\u5ea6\u76f8\u6bd4\uff0cMIDAS\u5c06\u5e73\u5747\u961f\u5217\u957f\u5ea6\u51cf\u5c11\u7ea623%\uff0c\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u70ed\u70b9\u7f13\u89e3\u8fbe80%\u3002", "conclusion": "\u57fa\u4e8e\u4e2d\u95f4\u4ef6\u7684\u7a33\u5b9a\u6027\u611f\u77e5\u7b56\u7565\u53ef\u4ee5\u4e3a\u5143\u6570\u636e\u7ba1\u7406\u63d0\u4f9b\u540e\u7aef\u65e0\u5173\u7684\u6539\u8fdb\uff0c\u5728\u7a81\u53d1\u573a\u666f\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3001\u66f4\u53ef\u9884\u6d4b\u7684\u5c3e\u90e8\u5ef6\u8fdf\u548c\u66f4\u5f3a\u7684\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2511.19366", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.19366", "abs": "https://arxiv.org/abs/2511.19366", "authors": ["Alan Jia Bao Du", "Tarek S. Abdelrahman"], "title": "HeLEx: A Heterogeneous Layout Explorer for Spatial Elastic Coarse-Grained Reconfigurable Arrays", "comment": null, "summary": "We present HeLEx, a framework for determining the functional layout of heterogeneous spatially-configured elastic Coarse-Grained Reconfigurable Arrays (CGRAs). Given a collection of input data flow graphs (DFGs) and a target CGRA, the framework starts with a full layout in which every processing element (PE) supports every operation in the DFGs. It then employs a branch-and-bound (BB) search to eliminate operations out of PEs, ensuring that the input DFGs successfully map onto the resulting CGRAs, eventually returning an optimized heterogeneous CGRA. Experimental evaluation with 12 DFGs and 9 target CGRA sizes reveals that the framework reduces the number of operations by 68.7% on average, resulting in a reduction of CGRA area by almost 70% and of power by over 51%, all compared to the initial full layout. HeLEx generates CGRAs that are on average only within 6.2% of theoretically minimum CGRAs that support exactly the number of operations needed by the input DFGs. A comparison with functional layouts produced by two state-of-the-art frameworks indicates that HeLEx achieves better reduction in the number of operations, by up to 2.6X.", "AI": {"tldr": "HeLEx\u6846\u67b6\u901a\u8fc7\u5206\u652f\u5b9a\u754c\u641c\u7d22\u4f18\u5316\u5f02\u6784\u7c97\u7c92\u5ea6\u53ef\u91cd\u6784\u9635\u5217\u7684\u529f\u80fd\u5e03\u5c40\uff0c\u5e73\u5747\u51cf\u5c1168.7%\u7684\u64cd\u4f5c\u6570\u91cf\uff0c\u964d\u4f4e70%\u9762\u79ef\u548c51%\u529f\u8017\uff0c\u63a5\u8fd1\u7406\u8bba\u6700\u5c0fCGRA\u76846.2%\u4ee5\u5185\u3002", "motivation": "\u73b0\u6709\u7684\u5f02\u6784CGRA\u8bbe\u8ba1\u9700\u8981\u4f18\u5316\u529f\u80fd\u5e03\u5c40\u4ee5\u51cf\u5c11\u64cd\u4f5c\u6570\u91cf\u3001\u9762\u79ef\u548c\u529f\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u8f93\u5165\u6570\u636e\u6d41\u56fe\u7684\u6620\u5c04\u80fd\u529b\u3002", "method": "\u4ece\u5168\u529f\u80fd\u5e03\u5c40\u5f00\u59cb\uff0c\u4f7f\u7528\u5206\u652f\u5b9a\u754c\u641c\u7d22\u9010\u6b65\u4ece\u5904\u7406\u5355\u5143\u4e2d\u6d88\u9664\u64cd\u4f5c\uff0c\u786e\u4fdd\u8f93\u5165\u6570\u636e\u6d41\u56fe\u4ecd\u80fd\u6210\u529f\u6620\u5c04\u5230\u751f\u6210\u7684CGRA\u4e0a\u3002", "result": "\u572812\u4e2a\u6570\u636e\u6d41\u56fe\u548c9\u4e2aCGRA\u5c3a\u5bf8\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5e73\u5747\u51cf\u5c1168.7%\u64cd\u4f5c\uff0c\u9762\u79ef\u51cf\u5c11\u8fd170%\uff0c\u529f\u8017\u964d\u4f4e\u8d8551%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u8fbe2.6\u500d\u3002", "conclusion": "HeLEx\u80fd\u6709\u6548\u751f\u6210\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u7684\u5f02\u6784CGRA\u529f\u80fd\u5e03\u5c40\uff0c\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u5f00\u9500\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u80fd\u529b\u3002"}}
{"id": "2511.18137", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.18137", "abs": "https://arxiv.org/abs/2511.18137", "authors": ["Christoph Goldgruber", "Benedikt Pittl", "Erich Schikuta"], "title": "Simulating Dynamic Cloud Marketspaces: Modeling Spot Instance Behavior and Scheduling with CloudSim Plus", "comment": null, "summary": "The increasing reliance on dynamic pricing models, such as spot instances, in public cloud environments presents new challenges for workload scheduling and reliability. While these models offer cost advantages, they introduce volatility and uncertainty that are not fully addressed by current allocation algorithms or simulation tools. This work contributes to the modeling and evaluation of such environments by extending the CloudSim Plus simulation framework to support realistic spot instance lifecycle management, including interruption, termination, hibernation, and reallocation. The enhanced simulator is validated using synthetic scenarios and large-scale simulations based on the Google Cluster Trace dataset. Building on this foundation, the HLEM-VMP allocation algorithm, originally proposed in earlier research, was adapted to operate under dynamic spot market conditions. Its performance was evaluated against baseline allocation strategies to assess its efficiency and resilience in volatile workload environments. The comparison demonstrated a reduction in the number of spot instance interruptions as well as a decrease in the maximum interruption duration. Overall, this work provides both a simulation framework for simulating dynamic cloud behavior and analytical insights into virtual machine allocation performance and market risk, contributing to more robust and cost-effective resource management in cloud computing.", "AI": {"tldr": "\u6269\u5c55CloudSim Plus\u6a21\u62df\u6846\u67b6\u4ee5\u652f\u6301\u52a8\u6001\u4e91\u5b9a\u4ef7\u73af\u5883\u4e2d\u7684\u5b9e\u4f8b\u751f\u547d\u5468\u671f\u7ba1\u7406\uff0c\u5e76\u8bc4\u4f30\u6539\u8fdb\u7684HLEM-VMP\u5206\u914d\u7b97\u6cd5\u5728\u6ce2\u52a8\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u516c\u5171\u4e91\u73af\u5883\u4e2d\u52a8\u6001\u5b9a\u4ef7\u6a21\u578b\uff08\u5982\u7ade\u4ef7\u5b9e\u4f8b\uff09\u7684\u65e5\u76ca\u666e\u53ca\u7ed9\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u548c\u53ef\u9760\u6027\u5e26\u6765\u4e86\u65b0\u6311\u6218\uff0c\u73b0\u6709\u5206\u914d\u7b97\u6cd5\u548c\u6a21\u62df\u5de5\u5177\u672a\u80fd\u5145\u5206\u5904\u7406\u8fd9\u4e9b\u6a21\u578b\u5f15\u5165\u7684\u6ce2\u52a8\u6027\u548c\u4e0d\u786e\u5b9a\u6027", "method": "\u6269\u5c55CloudSim Plus\u6a21\u62df\u6846\u67b6\u4ee5\u652f\u6301\u771f\u5b9e\u7684\u7ade\u4ef7\u5b9e\u4f8b\u751f\u547d\u5468\u671f\u7ba1\u7406\uff08\u5305\u62ec\u4e2d\u65ad\u3001\u7ec8\u6b62\u3001\u4f11\u7720\u548c\u91cd\u65b0\u5206\u914d\uff09\uff0c\u4f7f\u7528\u5408\u6210\u573a\u666f\u548c\u57fa\u4e8eGoogle Cluster Trace\u6570\u636e\u96c6\u7684\u5927\u89c4\u6a21\u6a21\u62df\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5e76\u8bc4\u4f30\u6539\u8fdb\u7684HLEM-VMP\u5206\u914d\u7b97\u6cd5\u5728\u52a8\u6001\u7ade\u4ef7\u5e02\u573a\u6761\u4ef6\u4e0b\u7684\u6027\u80fd", "result": "\u4e0e\u57fa\u7ebf\u5206\u914d\u7b56\u7565\u76f8\u6bd4\uff0c\u6539\u8fdb\u7684HLEM-VMP\u7b97\u6cd5\u51cf\u5c11\u4e86\u7ade\u4ef7\u5b9e\u4f8b\u4e2d\u65ad\u6b21\u6570\u548c\u6700\u5927\u4e2d\u65ad\u6301\u7eed\u65f6\u95f4", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u6a21\u62df\u52a8\u6001\u4e91\u884c\u4e3a\u7684\u6846\u67b6\uff0c\u5e76\u5bf9\u865a\u62df\u673a\u5206\u914d\u6027\u80fd\u548c\u5e02\u573a\u98ce\u9669\u63d0\u4f9b\u4e86\u5206\u6790\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u7a33\u5065\u548c\u6210\u672c\u6548\u76ca\u7684\u4e91\u8ba1\u7b97\u8d44\u6e90\u7ba1\u7406"}}
{"id": "2511.18151", "categories": ["cs.DC", "cs.AR", "cs.CV", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.18151", "abs": "https://arxiv.org/abs/2511.18151", "authors": ["Rajat Bhattacharjya", "Sing-Yao Wu", "Hyunwoo Oh", "Chaewon Nam", "Suyeon Koo", "Mohsen Imani", "Elaheh Bozorgzadeh", "Nikil Dutt"], "title": "AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems", "comment": "8 pages, 5 figures. Paper is currently under review. Authors' version posted for personal use and not for redistribution", "summary": "Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution \"context stream\" for real-time awareness and a low-frequency, high-fidelity \"insight stream\" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.", "AI": {"tldr": "AVERY\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u5206\u5272\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u8ba4\u77e5\u542f\u53d1\u7684\u53cc\u6d41\u5206\u5272\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u90e8\u7f72\u5728\u65e0\u4eba\u673a\u4e0a\uff0c\u5b9e\u73b0\u5728\u707e\u96be\u54cd\u5e94\u7b49\u4f4e\u5e26\u5bbd\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u53ef\u67e5\u8be2\u667a\u80fd\u5206\u6790\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u707e\u96be\u54cd\u5e94\u4e2d\u9700\u8981\u590d\u6742\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u73b0\u6709CNN\u65e0\u6cd5\u63d0\u4f9b\u53ef\u67e5\u8be2\u667a\u80fd\uff0c\u800cVLM\u867d\u7136\u5177\u5907\u8fd9\u79cd\u80fd\u529b\u4f46\u8d44\u6e90\u9700\u6c42\u8fc7\u9ad8\uff0c\u65e0\u6cd5\u5728\u8bbe\u5907\u4e0a\u90e8\u7f72\uff0c\u4e14\u4f20\u7edf\u7684\u4e91\u7aef\u5378\u8f7d\u5728\u4f4e\u5e26\u5bbd\u707e\u96be\u533a\u57df\u7f51\u7edc\u4e0b\u5931\u6548\u3002", "method": "\u63d0\u51fa\u529f\u80fd\u6027\u7684\u8ba4\u77e5\u542f\u53d1\u53cc\u6d41\u5206\u5272\u65b9\u6cd5\uff1a\u9ad8\u9891\u4f4e\u5206\u8fa8\u7387\u7684\"\u4e0a\u4e0b\u6587\u6d41\"\u7528\u4e8e\u5b9e\u65f6\u611f\u77e5\uff0c\u4f4e\u9891\u9ad8\u4fdd\u771f\u7684\"\u6d1e\u5bdf\u6d41\"\u7528\u4e8e\u6df1\u5ea6\u5206\u6790\u3002\u8f7b\u91cf\u7ea7\u81ea\u611f\u77e5\u63a7\u5236\u5668\u6839\u636e\u7f51\u7edc\u6761\u4ef6\u548c\u64cd\u4f5c\u610f\u56fe\u52a8\u6001\u9009\u62e9\u9884\u8bad\u7ec3\u538b\u7f29\u6a21\u578b\u3002", "result": "\u5728\u8fb9\u7f18-\u4e91\u573a\u666f\u4e0b\u4f7f\u7528LISA-7B VLM\u8bc4\u4f30\uff0cAVERY\u5728\u6ce2\u52a8\u7f51\u7edc\u6761\u4ef6\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u9759\u6001\u914d\u7f6e\uff0c\u76f8\u6bd4\u539f\u59cb\u56fe\u50cf\u538b\u7f29\u51c6\u786e\u7387\u63d0\u9ad811.2%\uff0c\u76f8\u6bd4\u5168\u8fb9\u7f18\u6267\u884c\u80fd\u8017\u964d\u4f4e93.98%\u3002", "conclusion": "AVERY\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6548\u7387\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5b9e\u65f6\u53ef\u67e5\u8be2\u667a\u80fd\u3002"}}
{"id": "2511.19258", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.19258", "abs": "https://arxiv.org/abs/2511.19258", "authors": ["Antonis Psistakis"], "title": "IOMMU Support for Virtual-Address Remote DMA in an ARMv8 environment", "comment": "Antonis Psistakis, B.Sc. Thesis (2017). Abstract revised in 2025 to comply with arXiv character limits", "summary": "In complex systems with many compute nodes containing multiple CPUs that are coherent within each node, a key challenge is maintaining efficient and correct coherence between nodes. The Unimem system addresses this by proposing a virtualized global address space that enables such coherence, relying on the I/O Memory Management Unit (IOMMU) in each node. The goal of this thesis is to support this approach by successfully testing and using the IOMMU of a single node. For this purpose, we used ARM's IOMMU, known as the System Memory Management Unit (SMMU), which translates virtual addresses to physical addresses. Because Linux documentation for the SMMU is limited and unclear, we implemented custom kernel modules to test and use its functionality.\n  First, we tested the SMMU in the Processing System (PS) of the Xilinx Zynq UltraScale+ MPSoC by developing a module that inserted virtual-to-physical address mappings into the SMMU. We then triggered a DMA transfer to a virtual address and observed that the request passed through the SMMU for address translation. We repeated this experiment by initiating DMA transactions from the Programmable Logic (PL) and similarly confirmed that the transactions were translated by the SMMU. Finally, we developed a module that enables transactions from the PL without requiring explicit pre-mapping of virtual and physical address pairs. This was achieved by configuring the SMMU with the page table pointer of a user process, allowing it to translate all relevant virtual addresses dynamically.\n  Overall, we successfully demonstrated the correct operation of the SMMU across all tested scenarios. Due to time constraints, further exploration of advanced SMMU features is left for future work.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6210\u529f\u6d4b\u8bd5\u5e76\u9a8c\u8bc1\u4e86ARM SMMU\uff08IOMMU\uff09\u5728Xilinx Zynq UltraScale+ MPSoC\u5e73\u53f0\u4e0a\u7684\u529f\u80fd\uff0c\u901a\u8fc7\u5f00\u53d1\u81ea\u5b9a\u4e49\u5185\u6838\u6a21\u5757\u5b9e\u73b0\u4e86\u865a\u62df\u5730\u5740\u5230\u7269\u7406\u5730\u5740\u7684\u8f6c\u6362\uff0c\u652f\u6301DMA\u4f20\u8f93\uff0c\u5e76\u5b9e\u73b0\u4e86\u52a8\u6001\u5730\u5740\u7ffb\u8bd1\u529f\u80fd\u3002", "motivation": "\u652f\u6301Unimem\u7cfb\u7edf\u4e2d\u901a\u8fc7IOMMU\u5b9e\u73b0\u8282\u70b9\u95f4\u9ad8\u6548\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\uff0c\u7531\u4e8eLinux\u5bf9SMMU\u7684\u6587\u6863\u6709\u9650\u4e14\u4e0d\u6e05\u6670\uff0c\u9700\u8981\u9a8c\u8bc1SMMU\u5728\u5355\u8282\u70b9\u4e2d\u7684\u529f\u80fd\u3002", "method": "\u5f00\u53d1\u81ea\u5b9a\u4e49\u5185\u6838\u6a21\u5757\uff1a1\uff09\u5728PS\u4e2d\u63d2\u5165\u865a\u62df\u5230\u7269\u7406\u5730\u5740\u6620\u5c04\u5e76\u89e6\u53d1DMA\u4f20\u8f93\uff1b2\uff09\u4ecePL\u53d1\u8d77DMA\u4e8b\u52a1\u9a8c\u8bc1SMMU\u7ffb\u8bd1\uff1b3\uff09\u914d\u7f6eSMMU\u4f7f\u7528\u7528\u6237\u8fdb\u7a0b\u9875\u8868\u6307\u9488\u5b9e\u73b0\u52a8\u6001\u5730\u5740\u7ffb\u8bd1\u3002", "result": "\u5728\u6240\u6709\u6d4b\u8bd5\u573a\u666f\u4e2d\u6210\u529f\u9a8c\u8bc1\u4e86SMMU\u7684\u6b63\u786e\u64cd\u4f5c\uff0c\u5305\u62ecPS\u548cPL\u7684DMA\u4f20\u8f93\u5730\u5740\u7ffb\u8bd1\uff0c\u4ee5\u53ca\u52a8\u6001\u5730\u5740\u7ffb\u8bd1\u529f\u80fd\u3002", "conclusion": "\u6210\u529f\u6f14\u793a\u4e86SMMU\u7684\u529f\u80fd\uff0c\u4e3aUnimem\u7cfb\u7edf\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u57fa\u7840\u652f\u6301\uff0c\u9ad8\u7ea7SMMU\u7279\u6027\u7684\u63a2\u7d22\u7559\u5f85\u672a\u6765\u5de5\u4f5c\u3002"}}
{"id": "2511.18315", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.18315", "abs": "https://arxiv.org/abs/2511.18315", "authors": ["Rajashree Bar", "Daibik Barik", "Adri Bhattacharya", "Partha Sarathi Mandal"], "title": "Monotone Decontamination of Arbitrary Dynamic Graphs with Mobile Agents", "comment": "Published in CALDAM 2026", "summary": "Network decontamination is a well-known problem, in which the aim of the mobile agents should be to decontaminate the network (i.e., both nodes and edges). This problem comes with an added constraint, i.e., of \\emph{monotonicity}, in which whenever a node or an edge is decontaminated, it must not get recontaminated. Hence, the name comes \\emph{monotone decontamination}. This problem has been relatively explored in static graphs, but nothing is known yet in dynamic graphs. We, in this paper, study the \\emph{monotone decontamination} problem in arbitrary dynamic graphs. We designed two models of dynamicity, based on the time within which a disappeared edge must reappear. In each of these two models, we proposed lower bounds as well as upper bounds on the number of agents, required to fully decontaminate the underlying dynamic graph, monotonically. Our results also highlight the difficulties faced due to the sudden disappearance or reappearance of edges. Our aim in this paper has been to primarily optimize the number of agents required to solve monotone decontamination in these dynamic networks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u52a8\u6001\u56fe\u4e2d\u7684\u5355\u8c03\u53bb\u6c61\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u52a8\u6001\u6027\u6a21\u578b\uff0c\u5e76\u7ed9\u51fa\u4e86\u6240\u9700\u79fb\u52a8\u4ee3\u7406\u6570\u91cf\u7684\u4e0a\u4e0b\u754c\u3002", "motivation": "\u7f51\u7edc\u53bb\u6c61\u95ee\u9898\u5728\u9759\u6001\u56fe\u4e2d\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u5728\u52a8\u6001\u56fe\u4e2d\u5c1a\u672a\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u4efb\u610f\u52a8\u6001\u56fe\u4e2d\u7684\u5355\u8c03\u53bb\u6c61\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u7531\u4e8e\u8fb9\u7a81\u7136\u6d88\u5931\u6216\u91cd\u65b0\u51fa\u73b0\u5e26\u6765\u7684\u56f0\u96be\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cd\u57fa\u4e8e\u8fb9\u91cd\u65b0\u51fa\u73b0\u65f6\u95f4\u7684\u52a8\u6001\u6027\u6a21\u578b\uff0c\u5728\u6bcf\u4e2a\u6a21\u578b\u4e2d\u63d0\u51fa\u4e86\u5b8c\u5168\u5355\u8c03\u53bb\u6c61\u6240\u9700\u4ee3\u7406\u6570\u91cf\u7684\u4e0b\u754c\u548c\u4e0a\u754c\u3002", "result": "\u83b7\u5f97\u4e86\u52a8\u6001\u7f51\u7edc\u4e2d\u5355\u8c03\u53bb\u6c61\u95ee\u9898\u6240\u9700\u4ee3\u7406\u6570\u91cf\u7684\u7406\u8bba\u754c\u9650\uff0c\u63ed\u793a\u4e86\u8fb9\u52a8\u6001\u53d8\u5316\u5bf9\u53bb\u6c61\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u52a8\u6001\u56fe\u4e2d\u7684\u5355\u8c03\u53bb\u6c61\u95ee\u9898\uff0c\u4e3a\u4f18\u5316\u6240\u9700\u4ee3\u7406\u6570\u91cf\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u5c55\u793a\u4e86\u8fb9\u52a8\u6001\u6027\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2511.18906", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.18906", "abs": "https://arxiv.org/abs/2511.18906", "authors": ["Marco Zambianco", "Lorenzo Fasol", "Roberto Doriguzzi-Corin"], "title": "An Online Fragmentation-Aware GPU Scheduler for Multi-Tenant MIG-based Clouds", "comment": null, "summary": "The explosive growth of AI applications has created unprecedented demand for GPU resources. Cloud providers meet this demand through GPU-as-a-Service platforms that offer rentable GPU resources for running AI workloads. In this context, the sharing of GPU resources between different tenants is essential to maximize the number of scheduled workloads. Among the various GPU sharing technologies, NVIDIA's Multi-Instance GPU (MIG) stands out by partitioning GPUs at hardware level into isolated slices with dedicated compute and memory, ensuring strong tenant isolation, preventing resource contention, and enhancing security. Despite these advantages, MIG's fixed partitioning introduces scheduling rigidity, leading to severe GPU fragmentation in multi-tenant environments, where workloads are continuously deployed and terminated. Fragmentation leaves GPUs underutilized, limiting the number of workloads that can be accommodated. To overcome this challenge, we propose a novel scheduling framework for MIG-based clouds that maximizes workload acceptance while mitigating fragmentation in an online, workload-agnostic setting. We introduce a fragmentation metric to quantify resource inefficiency and guide allocation decisions. Building on this metric, our greedy scheduling algorithm selects GPUs and MIG slices that minimize fragmentation growth for each incoming workload. We evaluate our approach against multiple baseline strategies under diverse workload distributions. Results demonstrate that our method consistently achieves higher workload acceptance rates, leading to an average 10% increase in the number of scheduled workloads in heavy load conditions, while using approximately the same number of GPUs as the benchmark methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9NVIDIA MIG GPU\u4e91\u5e73\u53f0\u7684\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u788e\u7247\u5316\u6765\u6700\u5927\u5316\u5de5\u4f5c\u8d1f\u8f7d\u63a5\u53d7\u7387\uff0c\u5728\u91cd\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u5e73\u5747\u589e\u52a010%\u7684\u8c03\u5ea6\u5de5\u4f5c\u8d1f\u8f7d\u6570\u91cf\u3002", "motivation": "GPU\u8d44\u6e90\u7684\u7206\u70b8\u6027\u589e\u957f\u9700\u6c42\u4fc3\u4f7f\u4e91\u63d0\u4f9b\u5546\u901a\u8fc7GPU\u5373\u670d\u52a1\u5e73\u53f0\u63d0\u4f9b\u53ef\u79df\u7528\u8d44\u6e90\u3002MIG\u6280\u672f\u867d\u7136\u63d0\u4f9b\u786c\u4ef6\u7ea7\u9694\u79bb\uff0c\u4f46\u5176\u56fa\u5b9a\u5206\u533a\u5bfc\u81f4\u8c03\u5ea6\u521a\u6027\uff0c\u5728\u591a\u79df\u6237\u73af\u5883\u4e2d\u9020\u6210\u4e25\u91cd\u7684GPU\u788e\u7247\u5316\u95ee\u9898\uff0c\u9650\u5236\u4e86\u53ef\u5bb9\u7eb3\u7684\u5de5\u4f5c\u8d1f\u8f7d\u6570\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u8c03\u5ea6\u6846\u67b6\uff0c\u5f15\u5165\u788e\u7247\u5316\u6307\u6807\u6765\u91cf\u5316\u8d44\u6e90\u6548\u7387\u5e76\u6307\u5bfc\u5206\u914d\u51b3\u7b56\u3002\u57fa\u4e8e\u6b64\u6307\u6807\uff0c\u8bbe\u8ba1\u8d2a\u5fc3\u8c03\u5ea6\u7b97\u6cd5\u4e3a\u6bcf\u4e2a\u4f20\u5165\u5de5\u4f5c\u8d1f\u8f7d\u9009\u62e9\u6700\u5c0f\u5316\u788e\u7247\u5316\u589e\u957f\u7684GPU\u548cMIG\u5207\u7247\u3002", "result": "\u5728\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\u4e0b\u4e0e\u591a\u79cd\u57fa\u7ebf\u7b56\u7565\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u59cb\u7ec8\u5b9e\u73b0\u66f4\u9ad8\u7684\u5de5\u4f5c\u8d1f\u8f7d\u63a5\u53d7\u7387\uff0c\u5728\u91cd\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u5e73\u5747\u589e\u52a010%\u7684\u8c03\u5ea6\u5de5\u4f5c\u8d1f\u8f7d\u6570\u91cf\uff0c\u540c\u65f6\u4f7f\u7528\u4e0e\u57fa\u51c6\u65b9\u6cd5\u5927\u81f4\u76f8\u540c\u6570\u91cf\u7684GPU\u3002", "conclusion": "\u8be5\u8c03\u5ea6\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86MIG\u4e91\u73af\u5883\u4e2d\u7684GPU\u788e\u7247\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d44\u6e90\u5229\u7528\u6548\u7387\u548c\u5de5\u4f5c\u8d1f\u8f7d\u5bb9\u7eb3\u80fd\u529b\u3002"}}
{"id": "2511.19192", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.19192", "abs": "https://arxiv.org/abs/2511.19192", "authors": ["Xinkui Zhao", "Qingyu Ma", "Yifan Zhang", "Hengxuan Lou", "Guanjie Cheng", "Shuiguang Deng", "Jianwei Yin"], "title": "AME: An Efficient Heterogeneous Agentic Memory Engine for Smartphones", "comment": null, "summary": "On-device agents on smartphones increasingly require continuously evolving memory to support personalized, context-aware, and long-term behaviors. To meet both privacy and responsiveness demands, user data is embedded as vectors and stored in a vector database for fast similarity search. However, most existing vector databases target server-class environments. When ported directly to smartphones, two gaps emerge: (G1) a mismatch between mobile SoC constraints and vector-database assumptions, including tight bandwidth budgets, limited on-chip memory, and stricter data type and layout constraints; and (G2) a workload mismatch, because on-device usage resembles a continuously learning memory, in which queries must coexist with frequent inserts, deletions, and ongoing index maintenance. To address these challenges, we propose AME, an on-device Agentic Memory Engine co-designed with modern smartphone SoCs. AME introduces two key techniques: (1) a hardware-aware, high-efficiency matrix pipeline that maximizes compute-unit utilization and exploits multi-level on-chip storage to sustain high throughput; and (2) a hardware- and workload-aware scheduling scheme that coordinates querying, insertion, and index rebuilding to minimize latency. We implement AME on Snapdragon 8-series SoCs and evaluate it on HotpotQA. In our experiments, AME improves query throughput by up to 1.4x at matched recall, achieves up to 7x faster index construction, and delivers up to 6x higher insertion throughput under concurrent query workloads.", "AI": {"tldr": "AME\u662f\u4e00\u4e2a\u4e13\u4e3a\u667a\u80fd\u624b\u673aSoC\u8bbe\u8ba1\u7684\u8bbe\u5907\u7aef\u667a\u80fd\u4f53\u8bb0\u5fc6\u5f15\u64ce\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5411\u91cf\u6570\u636e\u5e93\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u6027\u80fd\u74f6\u9888\uff0c\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u7684\u77e9\u9635\u6d41\u6c34\u7ebf\u548c\u667a\u80fd\u8c03\u5ea6\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u3001\u7d22\u5f15\u6784\u5efa\u548c\u63d2\u5165\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "motivation": "\u8bbe\u5907\u7aef\u667a\u80fd\u4f53\u9700\u8981\u6301\u7eed\u6f14\u8fdb\u7684\u8bb0\u5fc6\u6765\u652f\u6301\u4e2a\u6027\u5316\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u957f\u671f\u884c\u4e3a\uff0c\u4f46\u73b0\u6709\u5411\u91cf\u6570\u636e\u5e93\u4e3b\u8981\u9488\u5bf9\u670d\u52a1\u5668\u73af\u5883\u8bbe\u8ba1\uff0c\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b58\u5728\u786c\u4ef6\u7ea6\u675f\u4e0d\u5339\u914d\u548c\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86AME\u8bbe\u5907\u7aef\u667a\u80fd\u4f53\u8bb0\u5fc6\u5f15\u64ce\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6280\u672f\uff1a(1)\u786c\u4ef6\u611f\u77e5\u7684\u9ad8\u6548\u77e9\u9635\u6d41\u6c34\u7ebf\uff0c\u6700\u5927\u5316\u8ba1\u7b97\u5355\u5143\u5229\u7528\u7387\u5e76\u5229\u7528\u591a\u7ea7\u7247\u4e0a\u5b58\u50a8\uff1b(2)\u786c\u4ef6\u548c\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u7684\u8c03\u5ea6\u65b9\u6848\uff0c\u534f\u8c03\u67e5\u8be2\u3001\u63d2\u5165\u548c\u7d22\u5f15\u91cd\u5efa\u4ee5\u6700\u5c0f\u5316\u5ef6\u8fdf\u3002", "result": "\u5728Snapdragon 8\u7cfb\u5217SoC\u4e0a\u5b9e\u73b0AME\u5e76\u5728HotpotQA\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\uff1a\u67e5\u8be2\u541e\u5410\u91cf\u63d0\u53471.4\u500d\uff0c\u7d22\u5f15\u6784\u5efa\u901f\u5ea6\u63d0\u53477\u500d\uff0c\u5e76\u53d1\u67e5\u8be2\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u63d2\u5165\u541e\u5410\u91cf\u63d0\u53476\u500d\u3002", "conclusion": "AME\u6210\u529f\u89e3\u51b3\u4e86\u79fb\u52a8\u8bbe\u5907\u4e0a\u5411\u91cf\u6570\u636e\u5e93\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u8bbe\u5907\u7aef\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u54cd\u5e94\u8fc5\u901f\u7684\u8bb0\u5fc6\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.19208", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.19208", "abs": "https://arxiv.org/abs/2511.19208", "authors": ["J\u00e9r\u00e9mie Chalopin", "Maria Kokkou"], "title": "Constant-Size Certificates for Leader Election in Chordal Graphs and Related Classes", "comment": "16 pages, 3 figures, under review", "summary": "In distributed computing a certification scheme consists of a set of states and conditions over those states that enable each node of a graph to efficiently verify the correctness of a solution to a given problem. This work focuses on two fundamental problems: leader election and spanning tree construction. For each problem, we present a constant-size (per edge), local certification scheme, where the conditions available to each node can only refer to the graph induced by its one-hop neighborhood. In particular, we provide certification schemes for leader election in chordal and $K_4$-free dismantlable graphs and for spanning tree construction in dismantlable graphs, assuming a root is given. For chordal graphs, our leader election certification scheme additionally ensures an acyclic orientation, a property that is not generally verifiable using constant-size certificates in arbitrary graphs. To the best of our knowledge, these are the first local certification results tailored to these graph classes, potentially highlighting structural properties useful for verifying additional problems. Finally, we propose an algorithm that automatically transforms any certification scheme into a silent self-stabilizing algorithm (i.e., an algorithm that automatically recovers from faults) by adding only one extra state to the set of states of the certification scheme, assuming a Gouda fair scheduler. This transformation may be of independent interest.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u5f26\u56fe\u548c\u53ef\u62c6\u5378\u56fe\u7684\u9886\u5bfc\u9009\u4e3e\u548c\u751f\u6210\u6811\u6784\u9020\u7684\u5e38\u6570\u5927\u5c0f\u5c40\u90e8\u8ba4\u8bc1\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u8ba4\u8bc1\u65b9\u6848\u8f6c\u6362\u4e3a\u81ea\u7a33\u5b9a\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u5206\u5e03\u5f0f\u8ba1\u7b97\u4e2d\u7684\u8ba4\u8bc1\u65b9\u6848\uff0c\u4f7f\u56fe\u8282\u70b9\u80fd\u9ad8\u6548\u9a8c\u8bc1\u95ee\u9898\u89e3\u51b3\u65b9\u6848\u7684\u6b63\u786e\u6027\uff0c\u7279\u522b\u5173\u6ce8\u9886\u5bfc\u9009\u4e3e\u548c\u751f\u6210\u6811\u6784\u9020\u8fd9\u4e24\u4e2a\u57fa\u672c\u95ee\u9898\u3002", "method": "\u4e3a\u5f26\u56fe\u548cK4-free\u53ef\u62c6\u5378\u56fe\u63d0\u4f9b\u9886\u5bfc\u9009\u4e3e\u8ba4\u8bc1\u65b9\u6848\uff0c\u4e3a\u53ef\u62c6\u5378\u56fe\u63d0\u4f9b\u751f\u6210\u6811\u6784\u9020\u8ba4\u8bc1\u65b9\u6848\uff0c\u4f7f\u7528\u5e38\u6570\u5927\u5c0f\u8bc1\u4e66\u548c\u5c40\u90e8\u9a8c\u8bc1\u6761\u4ef6\u3002", "result": "\u6210\u529f\u8bbe\u8ba1\u4e86\u9488\u5bf9\u7279\u5b9a\u56fe\u7c7b\u7684\u5e38\u6570\u5927\u5c0f\u5c40\u90e8\u8ba4\u8bc1\u65b9\u6848\uff0c\u5f26\u56fe\u9886\u5bfc\u9009\u4e3e\u65b9\u6848\u8fd8\u786e\u4fdd\u65e0\u73af\u5b9a\u5411\uff0c\u5e76\u63d0\u51fa\u4e86\u8ba4\u8bc1\u65b9\u6848\u5230\u81ea\u7a33\u5b9a\u7b97\u6cd5\u7684\u81ea\u52a8\u8f6c\u6362\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u662f\u9488\u5bf9\u8fd9\u4e9b\u56fe\u7c7b\u7684\u9996\u4e2a\u5c40\u90e8\u8ba4\u8bc1\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u53ef\u7528\u4e8e\u9a8c\u8bc1\u5176\u4ed6\u95ee\u9898\u7684\u7ed3\u6784\u7279\u6027\uff0c\u8ba4\u8bc1\u65b9\u6848\u5230\u81ea\u7a33\u5b9a\u7b97\u6cd5\u7684\u8f6c\u6362\u5177\u6709\u72ec\u7acb\u7814\u7a76\u4ef7\u503c\u3002"}}
