<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 8]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Grammar-Aware Literate Generative Mathematical Programming with Compiler-in-the-Loop](https://arxiv.org/abs/2601.17670)
*Roberto Rossi,Steven D. Prestwich*

Main category: cs.PL

TL;DR: SyntAGM是一个通过生成-编译-评估-修订循环将自然语言问题描述转换为PyOPL数学规划模型的端到端系统，利用编译器反馈和LLM对齐判断，在准确性和效率方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究生成式数学编程，探索如何通过代数建模语言和编译器引导的模型合成，将自然语言问题描述自动转换为可执行的数学规划模型，提高建模效率和可访问性。

Method: 开发SyntAGM系统，基于PyOPL代数建模语言编译器，采用生成-编译-评估-修订循环，结合语法感知（通过BNF语法上下文学习）、少样本检索示例、编译器反馈和LLM对齐判断来生成和优化模型。

Result: 与现有提示基准相比，SyntAGM实现了具有竞争力的准确性，同时在令牌使用、成本和延迟方面表现出更优的性能。

Conclusion: SyntAGM通过结合编译器反馈和LLM对齐判断，有效实现了从自然语言到数学规划模型的自动转换，为生成式数学编程提供了实用且高效的解决方案。

Abstract: This work investigates generative mathematical programming through the lens of Algebraic Modelling Languages (AMLs) and compiler-guided model synthesis. By leveraging PyOPL, an OPL-like AML compiler that provides detailed syntax diagnostics, we introduce SyntAGM, an end-to-end system that translates natural language problem descriptions into PyOPL models via a generate--compile--assess--revise loop. SyntAGM is grammar-aware thanks to in-context exposure to the PyOPL BNF grammar, and benefits from few-shot retrieval of literate PyOPL model exemplars. To obtain a valid PyOPL model that matches the problem description, SyntAGM mobilises compiler feedback and an LLM-based alignment judge. In a comparative study against established prompting baselines SyntAGM achieves competitive accuracy with superior token, cost, and latency profiles.

</details>


### [2] [Types for Grassroots Logic Programs](https://arxiv.org/abs/2601.17957)
*Ehud Shapiro*

Main category: cs.PL

TL;DR: 论文提出了Typed GLP系统，为Grassroots Logic Programs添加类型系统，通过模式化路径定义类型，支持交互式部分计算的类型检查，并展示了AI辅助开发流程。


<details>
  <summary>Details</summary>
Motivation: GLP作为无类型并发逻辑编程语言，在表达复杂通信模式时存在挑战。特别是当使用AI编程时，缺乏类型系统使得编写正确代码变得困难。需要类型系统来确保AI生成的代码符合预期通信模式。

Method: 定义类型为模式化路径的正则集合，其中模式捕获通信方向性（消费或产生）。提供语法上的良类型定义，并证明程序良类型当且仅当其模式原子语义的路径抽象满足协变和逆变条件。

Result: 实现了GLP类型系统，通过AI从数学规范生成英文规范，再生成Dart代码。建立了人机协作开发流程：先定义GLP类型、过程类型声明、非正式描述，再让AI编写代码。

Conclusion: Typed GLP为并发逻辑编程提供了实用的类型系统，特别适合AI辅助编程场景。通过类型系统约束AI生成的代码，提高了复杂通信模式编程的可靠性。

Abstract: Grassroots Logic Programs (GLP) is a concurrent logic programming language in which logic variables are partitioned into paired readers and writers. An assignment is produced at most once via a writer and consumed at most once via its paired reader, and may contain additional readers and/or writers. This enables the concise expression of rich multidirectional communication modalities.
  ``Logic Programs as Types for Logic Programs'' (LICS'91) defined types as regular sets of paths over derivable ground atoms. Here, we define types to be regular sets of moded paths, where a mode captures directionality of communication -- whether a subterm is consumed from or produced to the environment -- enabling the typing of interactive partial computations including those that eventually deadlock or fail, or never terminate. We provide a syntactic definition of well-typing and prove that a program is well-typed iff the path abstraction of its moded-atom semantics satisfies covariance and contravariance conditions with respect to its type.
  The GLP type system was implemented in Dart by AI, starting from a mathematical specification of Typed GLP (this paper), deriving from it an English spec (written by AI), and from the spec deriving Dart code (by AI). While GLP is naturally untyped, the motivation for Typed GLP comes from programming with AI: Asking AI to program complex communication modalities in GLP (and in general) and hoping for the best is a tenuous strategy. The emerging discipline we advocate and employ is for the human designer and AI to jointly develop and agree upon (1)~GLP types; (2)~GLP procedure type declarations; (3)~informal (English) descriptions of the procedures; and only then let AI attempt to write (4)~GLP code based on those.

</details>


### [3] [Handling Scope Checks (Extended Version)](https://arxiv.org/abs/2601.18793)
*Michael Lee,Ningning Xie,Oleg Kiselyov,Jeremy Yallop*

Main category: cs.PL

TL;DR: 首次形式化研究动态作用域外泄检查，提出λ⟨⟨op⟩⟩演算描述和评估检查，引入新的"Cause-for-Concern"动态检查并证明其正确性，同时扩展框架包含静态作用域分类器。


<details>
  <summary>Details</summary>
Motivation: 元编程和效应处理器的交互会产生意外问题，如作用域外泄。静态类型系统存在实现和可用性问题，而动态检查虽在实践中存在（如MetaOCaml）但缺乏理论研究，金属语言设计者缺乏设计和实现检查的指导。

Method: 提出λ⟨⟨op⟩⟩演算形式化框架来描述和评估动态作用域外泄检查；引入新的"Cause-for-Concern"动态检查方法；扩展框架包含精化的环境分类器用于静态防止作用域外泄。

Result: 证明"Cause-for-Concern"检查的正确性，无需参考其实现即可描述其特征，论证其结合了现有动态检查的优点；比较动态检查与静态环境分类器的表达能力。

Conclusion: 首次对动态作用域外泄检查进行形式化研究，为金属语言设计者提供了设计和实现检查的理论指导，展示了动态检查与静态方法的互补关系。

Abstract: Metaprogramming and effect handlers interact in unexpected, and sometimes undesirable, ways. One example is scope extrusion: the generation of ill-scoped code. Scope extrusion can either be preemptively prevented, via static type systems, or retroactively detected, via dynamic checks. Static type systems exist in theory, but struggle with a range of implementation and usability problems in practice. In contrast, dynamic checks exist in practice (e.g. in MetaOCaml), but are understudied in theory. Designers of metalanguages are thus given little guidance regarding the design and implementation of checks. We present the first formal study of dynamic scope extrusion checks, introducing a calculus ($λ_{\langle\langle\text{op}\rangle\rangle}$) for describing and evaluating checks. Further, we introduce a novel dynamic check $\unicode{x2014}$ the "Cause-for-Concern" check $\unicode{x2014}$ which we prove correct, characterise without reference to its implementation, and argue combines the advantages of existing dynamic checks. Finally, we extend our framework with refined environment classifiers, which statically prevent scope extrusion, and compare their expressivity with the dynamic checks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Communication-Avoiding Linear Algebraic Kernel K-Means on GPUs](https://arxiv.org/abs/2601.17136)
*Julian Bellavita,Matthew Rubino,Nakul Iyer,Andrew Chang,Aditya Devarakonda,Flavio Vella,Giulia Guidi*

Main category: cs.DC

TL;DR: 提出了分布式内存并行算法，在多GPU系统上实现大规模核K-means聚类，能够处理百万级数据集，相比单GPU方法实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 传统K-means无法处理非线性可分聚类，核K-means解决了这个问题但计算和内存开销大。现有单GPU方法受限于内存，无法处理超过约8万个样本的数据集。

Method: 设计分布式内存并行算法，将核K-means计算密集型组件映射到通信高效的分布式线性代数原语上，采用专门为核K-means定制的分区方案，包括1D和1.5D算法。

Result: 1.5D算法性能最优，能将核K-means扩展到比之前实用规模大1-2个数量级的数据。在256个GPU上，几何平均弱扩展效率达79.7%，强扩展加速比达4.2倍，相比1D算法最高加速3.6倍。

Conclusion: 针对特定应用设计的分布式算法结合线性代数公式化能实现显著性能提升，使大规模核K-means聚类变得实用，将聚类时间从超过1小时减少到2秒以内。

Abstract: Clustering is an important tool in data analysis, with K-means being popular for its simplicity and versatility. However, it cannot handle non-linearly separable clusters. Kernel K-means addresses this limitation but requires a large kernel matrix, making it computationally and memory intensive. Prior work has accelerated Kernel K-means by formulating it using sparse linear algebra primitives and implementing it on a single GPU. However, that approach cannot run on datasets with more than approximately 80,000 samples due to limited GPU memory.
  In this work, we address this issue by presenting a suite of distributed-memory parallel algorithms for large-scale Kernel K-means clustering on multi-GPU systems. Our approach maps the most computationally expensive components of Kernel K-means onto communication-efficient distributed linear algebra primitives uniquely tailored for Kernel K-means, enabling highly scalable implementations that efficiently cluster million-scale datasets. Central to our work is the design of partitioning schemes that enable communication-efficient composition of the linear algebra primitives that appear in Kernel K-means.
  Our 1.5D algorithm consistently achieves the highest performance, enabling Kernel K-means to scale to data one to two orders of magnitude larger than previously practical. On 256 GPUs, it achieves a geometric mean weak scaling efficiency of $79.7\%$ and a geometric mean strong scaling speedup of $4.2\times$. Compared to our 1D algorithm, the 1.5D approach achieves up to a $3.6\times$ speedup on 256 GPUs and reduces clustering time from over an hour to under two seconds relative to a single-GPU sliding window implementation. Our results show that distributed algorithms designed with application-specific linear algebraic formulations can achieve substantial performance improvement.

</details>


### [5] [Push Down Optimization for Distributed Multi Cloud Data Integration](https://arxiv.org/abs/2601.17546)
*Ravi Kiran Kodali,Vinoth Punniyamoorthy,Akash Kumar Agarwal,Bikesh Kumar,Balakrishna Pothineni,Aswathnarayan Muthukrishnan Kirubakaran,Sumit Saha,Nachiappan Chockalingam*

Main category: cs.DC

TL;DR: 该论文探讨了在多云环境中ETL管道的下推优化可行性，分析了其优势与限制，并通过Redshift和BigQuery的案例研究展示了性能提升和成本效益。


<details>
  <summary>Details</summary>
Motivation: 企业越来越多地采用多云架构以利用不同的数据库引擎、区域可用性和成本模型。在这些环境中，ETL管道需要处理大量分布式数据集，同时最小化延迟和传输成本。虽然下推优化在单云系统中已被证明非常有效，但在多云环境中面临数据移动、异构SQL引擎、编排复杂性和碎片化安全控制等挑战。

Method: 论文研究了多云ETL管道中的下推优化可行性，评估了本地化下推、混合模型和数据联邦技术，这些技术可以减少跨云流量同时提高性能。通过Redshift和BigQuery的案例研究来验证效果。

Result: 案例研究表明了可衡量的收益，包括降低端到端运行时间、减少传输量以及提高成本效率。研究突出了组织可以在分布式云环境中采用的实际策略，以改善ETL的可扩展性和可靠性。

Conclusion: 多云环境中的下推优化虽然面临挑战，但通过适当的策略（如本地化下推、混合模型和数据联邦技术）可以实现显著的性能提升和成本节约，为组织提供了改善ETL管道效率的实际解决方案。

Abstract: Enterprises increasingly adopt multi cloud architectures to take advantage of diverse database engines, regional availability, and cost models. In these environments, ETL pipelines must process large, distributed datasets while minimizing latency and transfer cost. Push down optimization, which executes transformation logic within database engines rather than within the ETL tool, has proven highly effective in single cloud systems. However, when applied across multiple clouds, it faces challenges related to data movement, heterogeneous SQL engines, orchestration complexity, and fragmented security controls. This paper examines the feasibility of push down optimization in multi cloud ETL pipelines and analyzes its benefits and limitations. It evaluates localized push down, hybrid models, and data federation techniques that reduce cross cloud traffic while improving performance. A case study across Redshift and BigQuery demonstrates measurable gains, including lower end to end runtime, reduced transfer volume, and improved cost efficiency. The study highlights practical strategies that organizations can adopt to improve ETL scalability and reliability in distributed cloud environments.

</details>


### [6] [A Unified Approach to Concurrent, Parallel Map-Reduce in R using Futures](https://arxiv.org/abs/2601.17578)
*Henrik Bengtsson*

Main category: cs.DC

TL;DR: futurize包提供单一函数futurize()，将顺序map-reduce表达式转换为future生态系统的并行版本，简化R中并行计算


<details>
  <summary>Details</summary>
Motivation: R生态系统有丰富的map-reduce API，但并行化需要学习多种不兼容的并行API，导致代码移植困难

Method: 通过futurize()函数将顺序map-reduce表达式转译为并行版本，利用R原生管道操作符，支持base R、purrr、crossmap、foreach、plyr、BiocParallel等多种框架

Result: 用户只需在现有代码后添加`|> futurize()`即可并行化，支持多种领域特定包如boot、caret、glmnet等，开发者声明并行内容，用户通过plan()选择并行方式

Conclusion: futurize包通过抽象底层并行机制，统一处理future选项，简化了R中的并行计算，使并行化变得简单直观

Abstract: The R ecosystem offers a rich variety of map-reduce application programming interfaces (APIs) for iterative computations, yet parallelizing code across these diverse frameworks requires learning multiple, often incompatible, parallel APIs. The futurize package addresses this challenge by providing a single function, futurize(), which transpiles sequential map-reduce expressions into their parallel equivalents in the future ecosystem, which performs all the heavy lifting. By leveraging R's native pipe operator, users can parallelize existing code with minimal refactoring -- often by simply appending `|> futurize()' to an expression. The package supports classical map-reduce functions from base R, purrr, crossmap, foreach, plyr, BiocParallel, e.g., lapply(xs, fcn) |> futurize() and map(xs, fcn) |> futurize(), as well as a growing set of domain-specific packages, e.g., boot, caret, glmnet, lme4, mgcv, and tm. By abstracting away the underlying parallel machinery, and unifying handling of future options, the package enables developers to declare what to parallelize via futurize(), and end-users to choose how via plan(). This article describes the philosophy, design, and implementation of futurize, demonstrates its usage across various map-reduce paradigms, and discusses its role in simplifying parallel computing in R.

</details>


### [7] [Lightspeed Data Compute for the Space Era](https://arxiv.org/abs/2601.17589)
*Thomas Sandholm,Bernardo A. Huberman,Klas Segeljakt,Paris Carbone*

Main category: cs.DC

TL;DR: SpaceCoMP：一种受MapReduce启发的LEO卫星网络处理模型，利用星间激光链路在轨道上进行协同数据处理，仅返回结果而非原始数据，解决了下行带宽瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 每天数千颗卫星拍摄地球数据，但由于下行带宽限制，大部分数据无法传回地面。在低地球轨道区域进行数据处理有望克服这一限制。

Method: 提出SpaceCoMP处理模型：地面站提交感兴趣区域查询；卫星收集传感器数据，利用星间激光链路进行协同处理；采用距离感知路由协议利用轨道几何特性；使用二分图匹配调度策略在轨道区域内放置map和reduce任务以最小化聚合成本。

Result: 在1000-10000颗卫星的星座模拟中，相比基线方法map放置效率提升61-79%，相比贪婪分配提升18-28%，聚合成本降低67-72%。

Conclusion: SpaceCoMP证明轨道网状网络不仅可作为通信中继（如当前应用），还能为天空之上的更快数据处理提供基础。

Abstract: While thousands of satellites photograph Earth every day, most of that data never makes it to the ground because downlink bandwidth simply cannot keep up. Processing data in the Low Earth Orbit (LEO) zone offers promising capabilities to overcome this limitation. We propose SpaceCoMP, a MapReduce-inspired processing model for LEO satellite mesh networks. Ground stations submit queries over an area of interest; satellites collect sensor data, process it cooperatively at light-speed using inter-satellite laser links, and return only the results. Our compute model leverages space physics to accelerate computations on LEO megaconstellations. Our distance-aware routing protocol exploits orbital geometry. In addition, our bipartite match scheduling strategy places map and reduce tasks within orbital regions while minimizing aggregation costs. We have simulated constellations of 1,000-10,000 satellites showcasing 61-79% improvement in map placement efficiency over baselines, 18-28% over greedy allocation, and 67-72% reduction in aggregation cost. SpaceCoMP demonstrates that the orbital mesh is not merely useful as a communication relay, as seen today, but can provide the foundations for faster data processing above the skies.

</details>


### [8] [Scaling All-to-all Operations Across Emerging Many-Core Supercomputers](https://arxiv.org/abs/2601.17606)
*Shannon Kinkead,Jackson Wesley,Whit Schonbein,David DeBonis,Matthew G. F. Dosanjh,Amanda Bienz*

Main category: cs.DC

TL;DR: 该论文提出了针对新兴众核系统的创新all-to-all算法，在Sapphire Rapids系统上相比系统MPI实现了最高3倍的性能加速。


<details>
  <summary>Details</summary>
Motivation: MPI中的高性能all-to-all集体操作对快速傅里叶变换、转置和机器学习应用至关重要。现有实现在新兴系统上的性能受消息大小、进程数、架构和并行系统分区等多种因素影响，需要针对新兴众核系统进行优化。

Method: 论文提出了针对新兴众核系统的创新all-to-all算法，并对现有算法和系统MPI进行了性能分析比较。

Result: 在32个节点的先进Sapphire Rapids系统上，新算法相比系统MPI实现了最高3倍的性能加速。

Conclusion: 针对新兴众核系统设计的专用all-to-all算法能够显著提升MPI集体操作的性能，对科学计算和机器学习应用具有重要意义。

Abstract: Performant all-to-all collective operations in MPI are critical to fast Fourier transforms, transposition, and machine learning applications. There are many existing implementations for all-to-all exchanges on emerging systems, with the achieved performance dependent on many factors, including message size, process count, architecture, and parallel system partition. This paper presents novel all-to-all algorithms for emerging many-core systems. Further, the paper presents a performance analysis against existing algorithms and system MPI, with novel algorithms achieving up to 3x speedup over system MPI at 32 nodes of state-of-the-art Sapphire Rapids systems.

</details>


### [9] [Multi-core & GPU-based Balanced Butterfly Counting in Signed Bipartite Graphs](https://arxiv.org/abs/2601.17707)
*Mekala Kiran,Apurba Das,Suman Banerjee,Tathagata Ray*

Main category: cs.DC

TL;DR: 提出了针对平衡蝴蝶计数的并行算法，包括多核CPU的M-BBC和GPU的G-BBC/G-BBC++，在真实数据集上实现了显著加速。


<details>
  <summary>Details</summary>
Motivation: 平衡蝴蝶计数是分析符号二分图的基础原语，用于研究高阶结构特性，但在大规模图上的计算成本仍然是主要瓶颈。

Method: 开发了多核CPU算法M-BBC（细粒度顶点级并行）和GPU算法G-BBC（基于tile的并行方法），以及改进版G-BBC++（集成动态调度）。

Result: M-BBC相比顺序基线BB2K最高加速71.13倍（平均38.13倍）；GPU算法最高加速13,320倍（平均2,600倍），比M-BBC最高快186倍（平均50倍）。

Conclusion: 提出的并行算法具有显著的可扩展性和效率，为大规模二分图上的高性能符号模体分析建立了坚实基础。

Abstract: Balanced butterfly counting, corresponding to counting balanced (2, 2)-bicliques, is a fundamental primitive in the analysis of signed bipartite graphs and provides a basis for studying higher-order structural properties such as clustering coefficients and community structure. Although prior work has proposed an efficient CPU-based serial method for counting balanced (2, k)-bicliques. The computational cost of balanced butterfly counting remains a major bottleneck on large-scale graphs. In this work, we present the highly parallel implementations for balanced butterfly counting for both multicore CPUs and GPUs. The proposed multi-core algorithm (M-BBC) employs fine-grained vertex-level parallelism to accelerate wedge-based counting while eliminating the generation of unbalanced substructures. To improve scalability, we develop a GPU-based method (G-BBC) that uses a tile-based parallel approach to effectively leverage shared memory while handling large vertex sets. We then present an improved variation, G-BBC++, which integrates dynamic scheduling to mitigate workload imbalance and maximize throughput. We conduct an experimental assessment of the proposed methods across 15 real-world datasets. Experimental results exhibit that M-BBC achieves speedups of up to 71.13x (average 38.13x) over the sequential baseline BB2K. The GPU-based algorithms deliver even greater improvements, achieving up to 13,320x speedup (average 2,600x) over BB2K and outperforming M-BBC by up to 186x (average 50x). These results indicate the substantial scalability and efficiency of our parallel algorithms and establish a robust foundation for high-performance signed motif analysis on massive bipartite graphs.

</details>


### [10] [An MLIR Lowering Pipeline for Stencils at Wafer-Scale](https://arxiv.org/abs/2601.17754)
*Nicolai Stawinoga,David Katz,Anton Lydike,Justs Zarins,Nick Brown,George Bisbas,Tobias Grosser*

Main category: cs.DC

TL;DR: 开发了一个编译器流水线，能够自动将基于模板的计算内核转换为针对Cerebras晶圆级引擎优化的代码，无需修改应用层代码，在WSE3上性能比128个NVIDIA A100 GPU快约14倍。


<details>
  <summary>Details</summary>
Motivation: Cerebras晶圆级引擎(WSE)虽然性能强大，但其分布式异步编程模型与传统顺序或批量同步编程模型差异巨大，移植现有代码需要定制化重实现。缺乏编译器支持使得自动化这一过程几乎不可能，而模板计算在HPC中普遍存在。

Method: 提出了一个编译器流水线，利用模板计算的领域特定信息，自动将基于模板的内核转换为高度优化的CSL代码，弥合问题数学表示与WSE异步执行模型之间的语义鸿沟。

Result: 基于三个HPC编程技术的五个基准测试，在Cerebras WSE2和WSE3上运行，该方法提供了与手动优化代码相当甚至略优的性能。无需应用层代码修改，WSE3上的性能比128个NVIDIA A100 GPU快约14倍，比128个CPU节点的Cray-EX超级计算机快20倍。

Conclusion: 通过利用模板计算的领域特定信息，编译器可以自动为目标硬件生成高效代码，无需应用层代码修改，为在Cerebras WSE等新型架构上部署HPC应用提供了可行的自动化解决方案。

Abstract: The Cerebras Wafer-Scale Engine (WSE) delivers performance at an unprecedented scale of over 900,000 compute units, all connected via a single-wafer on-chip interconnect. Initially designed for AI, the WSE architecture is also well-suited for High Performance Computing (HPC). However, its distributed asynchronous programming model diverges significantly from the simple sequential or bulk-synchronous programs that one would typically derive for a given mathematical program description. Targeting the WSE requires a bespoke re-implementation when porting existing code. The absence of WSE support in compilers such as MLIR, meant that there was little hope for automating this process.
  Stencils are ubiquitous in HPC, and in this paper we explore the hypothesis that domain specific information about stencils can be leveraged by the compiler to automatically target the WSE without requiring application-level code changes. We present a compiler pipeline that transforms stencil-based kernels into highly optimized CSL code for the WSE, bridging the semantic gap between the mathematical representation of the problem and the WSE's asynchronous execution model. Based upon five benchmarks across three HPC programming technologies, running on both the Cerebras WSE2 and WSE3, our approach delivers comparable, if not slightly better, performance than manually optimized code. Furthermore, without requiring any application level code changes, performance on the WSE3 is around 14 times faster than 128 Nvidia A100 GPUs and 20 times faster than 128 nodes of a CPU-based Cray-EX supercomputer when using our approach.

</details>


### [11] [CondenseGraph: Communication-Efficient Distributed GNN Training via On-the-Fly Graph Condensation](https://arxiv.org/abs/2601.17774)
*Zizhao Zhang,Yihan Xue,Haotian Zhu,Sijia Li,Zhijun Wang,Yujie Xiao*

Main category: cs.DC

TL;DR: CondenseGraph：一种通过动态图压缩和误差反馈机制减少分布式GNN训练通信开销的高效框架


<details>
  <summary>Details</summary>
Motivation: 分布式图神经网络训练存在严重的通信瓶颈问题，由于图数据的邻域依赖性，需要频繁交换边界节点特征，现有静态图划分方法无法适应动态网络条件

Method: 提出CondenseGraph框架，核心创新包括：1) 动态图压缩机制，将边界节点特征压缩为紧凑的超节点后再传输；2) 基于梯度的误差反馈机制，补偿压缩带来的信息损失并保证收敛性

Result: 在四个基准数据集上的实验表明，CondenseGraph在保持与全精度基线相当准确度的同时，通信量减少40-60%，训练时间显著降低

Conclusion: CondenseGraph通过动态特征压缩和误差反馈机制，有效解决了分布式GNN训练的通信瓶颈问题，实现了通信效率与模型精度的良好平衡

Abstract: Distributed Graph Neural Network (GNN) training suffers from substantial communication overhead due to the inherent neighborhood dependency in graph-structured data. This neighbor explosion problem requires workers to frequently exchange boundary node features across partitions, creating a communication bottleneck that severely limits training scalability. Existing approaches rely on static graph partitioning strategies that cannot adapt to dynamic network conditions. In this paper, we propose CondenseGraph, a novel communication-efficient framework for distributed GNN training. Our key innovation is an on-the-fly graph condensation mechanism that dynamically compresses boundary node features into compact super nodes before transmission. To compensate for the information loss introduced by compression, we develop a gradient-based error feedback mechanism that maintains convergence guarantees while reducing communication volume by 40-60%. Extensive experiments on four benchmark datasets demonstrate that CondenseGraph achieves comparable accuracy to full-precision baselines while significantly reducing communication costs and training time.

</details>


### [12] [A Universal Load Balancing Principle and Its Application to Large Language Model Serving](https://arxiv.org/abs/2601.17855)
*Zixi Chen,Tianci Bu,Chendong Song,Xin Lu,Yinyu Ye,Zijie Zhou*

Main category: cs.DC

TL;DR: 论文提出了一种通用负载均衡原则，针对屏障同步系统中的异构工作负载漂移问题，通过逐步有限时域整数优化减少空闲时间，在LLM服务等场景中显著提升吞吐量并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)服务中出现的屏障同步、有状态系统负载均衡瓶颈：工作无法自由迁移，进度受最慢参与者限制，异构性和工作负载的时间漂移导致持续落后者和大量空闲时间。生产跟踪显示屏障引起的空闲时间可超过每个解码步骤计算时间的40%。

Method: 开发了通用负载均衡原则，采用逐步有限时域整数优化公式，为LLM解码模型和更广泛的非递减工作负载漂移过程提供最坏情况保证，减少长期不平衡。

Result: 理论分析显示该方法能减少长期不平衡，减少因子随批处理大小和系统规模增长。大量实验证实了理论，显示吞吐量和延迟显著改善，同时能耗降低。

Conclusion: 为负载均衡提供了通用、理论基础的框架，对可持续LLM服务有直接意义，并对其他同步门控资源分配问题具有广泛相关性。

Abstract: Load balancing-the allocation of work across parallel resources to reduce delay, energy and cost-is a pervasive challenge in science and engineering, from large-scale simulation and data processing to cloud and manufacturing operations. Motivated by the emerging bottleneck in large language model (LLM) serving, we study a particularly stringent regime of load balancing that arises in barrier-synchronized, stateful systems: work cannot be freely migrated and progress is gated by the slowest participant at each step, so heterogeneity and temporal drift in workloads create persistent stragglers and substantial idle time. LLM serving under data-parallel decoding provides a prominent modern instance: in production traces, barrier-induced idle can exceed 40% of compute time per decode step. Here we develop a universal load-balancing principle, which admits a step-wise finite-horizon integer-optimization formulation and yields worst-case guarantees: across LLM decode models and a broader class of non-decreasing workload drift processes, it reduces long-run imbalance by a factor that grows with batch size and system scale. Extensive experiments corroborate the theory, showing substantial improvements in throughput and latency together with reductions in energy consumption. These results provide a general, theoretically grounded framework for load balancing, with immediate implications for sustainable LLM serving and broad relevance to other synchronization-gated resource-allocation problems.

</details>


### [13] [An Initial Evaluation of Distributed Graph Algorithms using NWGraph and HPX](https://arxiv.org/abs/2601.18158)
*Karame Mohammadiporshokooh,Panagiotis Syskakis,Hartmut Kaiser*

Main category: cs.DC

TL;DR: 本文提出将NWGraph库与HPX运行时系统结合的分布式图处理框架，旨在通过异步多任务模型减少同步开销、改善负载均衡，为分布式图分析提供基础。实验显示BFS性能优于分布式BGL，但PageRank尚未超越BGL。


<details>
  <summary>Details</summary>
Motivation: 随着图数据规模不断增长，单节点内存和计算能力已无法满足需求，需要分布式解决方案。现有分布式图框架面临延迟受限、内存访问不规则、同步成本高等挑战，限制了可扩展性和效率。

Method: 将NWGraph库与HPX运行时系统集成，利用HPX的异步多任务模型来减少同步开销、改善负载均衡，为分布式图分析提供基础框架。使用BFS和PageRank两种代表性算法进行评估。

Result: BFS算法在性能上优于分布式Boost Graph Library (BGL)，但PageRank算法目前尚未超越BGL，显示出异步任务运行时在图处理中的潜力和挑战。

Conclusion: 异步任务运行时在图处理中具有应用前景，但仍有优化空间。BFS的成功和PageRank的挑战表明需要进一步优化和扩展，为未来研究指明了方向。

Abstract: Graphs are central to modeling relationships in scientific computing, data analysis, and AI/ML, but their growing scale can exceed the memory and compute capacity of single nodes, requiring distributed solutions. Existing distributed graph framework, however, face fundamental challenges: graph algorithms are latency-bound, suffer from irregular memory access, and often impose synchronization costs that limit scalability and efficiency. In this work, we present a distributed implementation of the NWGraph library integrated with the HPX runtime system. By leveraging HPX's asynchronous many-task model, our approach aims to reduce synchronization overhead, improve load balance, and provide a foundation for distributed graph analytics. We evaluate this approach using two representative algorithms: Breadth-First-Search (BFS) and (PageRank). Our initial results show that BFS achieves better performance than the distributed Boost Graph Library (BGL), while PageRank remains more challenging, with current implementation not yet outperforming BGL. These findings highlight both the promise and the open challenges of applying asynchronous task-based runtimes to graph processing, and point to opportunities for future optimizations and extensions.

</details>


### [14] [On the Bandwidth Consumption of Blockchains](https://arxiv.org/abs/2601.18400)
*Andrei Lebedev,Vincent Gramoli*

Main category: cs.DC

TL;DR: 首次对五种区块链协议（Algorand、Aptos、Avalanche、Redbelly、Solana）进行带宽消耗的实证比较，发现传输协议是影响网络流量的主要因素


<details>
  <summary>Details</summary>
Motivation: 区块链提案数量激增导致节点托管成本增加，但目前缺乏对区块链带宽消耗的比较研究，需要填补这一空白

Method: 测量五种区块链协议网络节点的网络流量，研究时间变化、接收与发送流量差异，分析流量如何随节点和验证者数量变化

Result: 传输协议是影响网络流量的主要因素，节点角色分离有助于减少流量，不同区块链受网络规模影响程度不同

Conclusion: 首次提供了区块链带宽消耗的实证比较，为区块链网络设计提供了重要参考，特别是传输协议选择和节点角色分离的重要性

Abstract: With the advent of blockchain technology, the number of proposals has boomed. The network traffic imposed by these blockchain proposals increases the cost of hosting nodes. Unfortunately, as of today, we are not aware of any comparative study of the bandwidth consumption of blockchains.
  In this paper, we propose the first empirical comparison of blockchain bandwidth consumption. To this end, we measure the network traffic of blockchain network nodes of five blockchain protocols: Algorand, Aptos, Avalanche, Redbelly and Solana. We study the variation over time, differentiate the receiving and sending traffic and analyze how this traffic varies with the number of nodes and validators.
  We conclude that the transport protocol is the main factor impacting the network traffic, segregating node roles helps reduce traffic and different blockchains are differently impacted by the network size.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [15] [SPADE: A SIMD Posit-enabled compute engine for Accelerating DNN Efficiency](https://arxiv.org/abs/2601.17279)
*Sonu Kumar,Lavanya Vinnakota,Mukul Lokhande,Santosh Kumar Vishvakarma,Adam Teman*

Main category: cs.AR

TL;DR: SPADE是一个统一的多精度SIMD Posit MAC架构，支持Posit(8,0)、(16,1)、(32,2)格式，通过分层复用子模块减少硬件开销，在FPGA和ASIC上实现高效能边缘AI计算。


<details>
  <summary>Details</summary>
Motivation: 边缘AI系统需要平衡数值精度、能效和硬件紧凑性的算术单元。Posit算术相比浮点和定点表示具有锥形精度、宽动态范围和更好的数值鲁棒性优势，但现有方案多为单精度或浮点/定点SIMD MAC，缺乏统一的多精度Posit支持。

Method: 提出SPADE架构：1) 采用基于regime-aware的lane-fused SIMD Posit数据通路；2) 分层复用Posit专用子模块(LOD、补码器、移位器、乘法器)跨8/16/32位精度；3) 避免数据通路复制，支持三种Posit格式的统一框架。

Result: FPGA实现：Xilinx Virtex-7上，Posit(8,0)减少45.13% LUT和80% slice；Posit(16,1)和(32,2)分别提升28.44%和17.47%；多精度支持仅增加6.9% LUT和14.9%寄存器开销。ASIC：28nm TSMC节点达1.38GHz@6.1mW。MNIST、CIFAR-10/100等数据集上保持竞争性推理精度。

Conclusion: SPADE通过创新的统一多精度SIMD Posit MAC架构，在保持数值精度的同时显著减少硬件资源，为边缘AI系统提供了高效、紧凑的算术单元解决方案，平衡了精度、能效和硬件成本的需求。

Abstract: The growing demand for edge-AI systems requires arithmetic units that balance numerical precision, energy efficiency, and compact hardware while supporting diverse formats. Posit arithmetic offers advantages over floating- and fixed-point representations through its tapered precision, wide dynamic range, and improved numerical robustness. This work presents SPADE, a unified multi-precision SIMD Posit-based multiplyaccumulate (MAC) architecture supporting Posit (8,0), Posit (16,1), and Posit (32,2) within a single framework. Unlike prior single-precision or floating/fixed-point SIMD MACs, SPADE introduces a regime-aware, lane-fused SIMD Posit datapath that hierarchically reuses Posit-specific submodules (LOD, complementor, shifter, and multiplier) across 8/16/32-bit precisions without datapath replication. FPGA implementation on a Xilinx Virtex-7 shows 45.13% LUT and 80% slice reduction for Posit (8,0), and up to 28.44% and 17.47% improvement for Posit (16,1) and Posit (32,2) over prior work, with only 6.9% LUT and 14.9% register overhead for multi-precision support. ASIC results across TSMC nodes achieve 1.38 GHz at 6.1 mW (28 nm). Evaluation on MNIST, CIFAR-10/100, and alphabet datasets confirms competitive inference accuracy.

</details>


### [16] [Athena: Synergizing Data Prefetching and Off-Chip Prediction via Online Reinforcement Learning](https://arxiv.org/abs/2601.17615)
*Rahul Bera,Zhenrong Lang,Caroline Hengartner,Konstantinos Kanellopoulos,Rakesh Kumar,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.AR

TL;DR: Athena：一个基于强化学习的框架，用于协调多级缓存预取器和片外预测器，以优化内存访问性能


<details>
  <summary>Details</summary>
Motivation: 预取和片外预测是隐藏内存访问延迟的两种技术，但现有方法在协调这两种技术时存在不足，无法充分发挥它们的性能潜力，需要更智能的协调机制

Method: 将预取器和片外预测器的协调建模为强化学习问题，Athena作为RL代理，观察系统级特征（如准确率、带宽使用），选择协调动作（启用/禁用组件、调整预取器激进性），通过奖励信号持续学习优化策略

Result: Athena在多种内存密集型工作负载下，始终优于现有最先进的协调策略，适用于各种系统配置（不同预取器、OCP、内存带宽组合），且存储开销适中

Conclusion: Athena提供了一个有效的强化学习框架，能够自主学习和协调预取器与片外预测器，显著提升内存访问性能，代码已开源

Abstract: Prefetching and off-chip prediction are two techniques proposed to hide long memory access latencies in high-performance processors. In this work, we demonstrate that: (1) prefetching and off-chip prediction often provide complementary performance benefits, yet (2) naively combining them often fails to realize their full performance potential, and (3) existing prefetcher control policies leave significant room for performance improvement behind.
  Our goal is to design a holistic framework that can autonomously learn to coordinate an off-chip predictor with multiple prefetchers employed at various cache levels. To this end, we propose a new technique called Athena, which models the coordination between prefetchers and off-chip predictor (OCP) as a reinforcement learning (RL) problem. Athena acts as the RL agent that observes multiple system-level features (e.g., prefetcher/OCP accuracy, bandwidth usage) over an epoch of program execution, and uses them as state information to select a coordination action (i.e., enabling the prefetcher and/or OCP, and adjusting prefetcher aggressiveness). At the end of every epoch, Athena receives a numerical reward that measures the change in multiple system-level metrics (e.g., number of cycles taken to execute an epoch). Athena uses this reward to autonomously and continuously learn a policy to coordinate prefetchers with OCP.
  Our extensive evaluation using a diverse set of memory-intensive workloads shows that Athena consistently outperforms prior state-of-the-art coordination policies across a wide range of system configurations with various combinations of underlying prefetchers, OCPs, and main memory bandwidths, while incurring only modest storage overhead. Athena is freely available at https://github.com/CMU-SAFARI/Athena.

</details>


### [17] [Conduit: Programmer-Transparent Near-Data Processing Using Multiple Compute-Capable Resources in Solid State Drives](https://arxiv.org/abs/2601.17633)
*Rakesh Nadig,Vamanan Arulchelvan,Mayank Kabra,Harshita Gupta,Rahul Bera,Nika Mansouri Ghiasi,Nanditha Rao,Qingcai Jiang,Andreas Kosmas Kakolyris,Yu Liang,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.AR

TL;DR: Conduit是一个通用的、对程序员透明的SSD近数据处理框架，通过利用SSD的多种计算资源，实现指令粒度的卸载决策，相比现有技术性能提升1.8倍，能耗降低46%。


<details>
  <summary>Details</summary>
Motivation: 现有SSD近数据处理技术存在三个主要问题：1）只针对特定工作负载或内核；2）未能充分利用SSD的全部计算潜力；3）缺乏对程序员的透明度。同时，现有主机与近内存加速器之间的计算分区技术也不适用于SSD，因为它们忽略了SSD资源的异构性，且卸载决策基于有限因素。

Method: Conduit采用编译时和运行时相结合的方案。编译时通过自定义编译器（如LLVM）将合适的应用代码段向量化为与SSD页面布局对齐的SIMD操作，并在向量化指令中嵌入元数据。运行时在SSD内部基于六个关键特征评估指令粒度卸载，使用成本函数选择最合适的SSD资源。

Result: 在六个数据密集型工作负载上评估，Conduit相比最佳现有卸载策略性能提升1.8倍，能耗降低46%。

Conclusion: Conduit通过充分利用SSD的多种计算资源，实现了通用的、对程序员透明的近数据处理框架，显著提升了性能和能效。

Abstract: Solid-state drives (SSDs) are well suited for near-data processing (NDP) because they: (1) store large application datasets, and (2) support three NDP paradigms: in-storage processing (ISP), processing using DRAM in the SSD (PuD-SSD), and in-flash processing (IFP). A large body of prior SSD-based NDP techniques operate in isolation, mapping computations to only one or two NDP paradigms (i.e., ISP, PuD-SSD, or IFP) within the SSD. These techniques (1) are tailored to specific workloads or kernels, (2) do not exploit the full computational potential of an SSD, and (3) lack programmer-transparency. While several prior works propose techniques to partition computation between the host and near-memory accelerators, adapting these techniques to SSDs has limited benefits because they (1) ignore the heterogeneity of the SSD resources, and (2) make offloading decisions based on limited factors such as bandwidth utilization, or data movement cost. We propose Conduit, a general-purpose, programmer-transparent NDP framework for SSDs that leverages multiple SSD computation resources. At compile time, Conduit executes a custom compiler (e.g., LLVM) pass that (i) vectorizes suitable application code segments into SIMD operations that align with the SSD's page layout, and (ii) embeds metadata (e.g., operation type, operand sizes) into the vectorized instructions to guide runtime offloading decisions. At runtime, within the SSD, Conduit performs instruction-granularity offloading by evaluating six key features, and uses a cost function to select the most suitable SSD resource. We evaluate Conduit and two prior NDP offloading techniques using an in-house event-driven SSD simulator on six data-intensive workloads. Conduit outperforms the best-performing prior offloading policy by 1.8x and reduces energy consumption by 46%.

</details>


### [18] [Late Breaking Results: Boosting Efficient Dual-Issue Execution on Lightweight RISC-V Cores](https://arxiv.org/abs/2601.17940)
*Luca Colagrande,Luca Benini*

Main category: cs.AR

TL;DR: COPIFTv2在Snitch RISC-V核心上引入轻量级队列，改进了双发射执行，相比COPIFT获得1.49倍加速和1.47倍能效提升，简化了编程模型。


<details>
  <summary>Details</summary>
Motivation: 大规模ML加速器需要大量PE，每个PE的面积和能耗预算严格。现有COPIFT方法虽然能提升性能（IPC 1.6x，能效1.3x），但编程模型复杂且容易出错，需要繁琐的平铺和软件流水线步骤。

Method: 在Snitch RISC-V核心上引入轻量级队列，实现整数和浮点线程间的直接细粒度通信和同步。消除COPIFT中的平铺和软件流水线步骤，简化编程模型。

Result: 相比COPIFT获得1.49倍加速和1.47倍能效提升，峰值IPC达到1.81。显著提升了轻量级核心上双发射执行的效率和可编程性。

Conclusion: COPIFTv2通过轻量级队列机制显著改进了双发射执行，在保持性能提升的同时大大简化了编程模型，实现了开源实现和可复现的性能实验。

Abstract: Large-scale ML accelerators rely on large numbers of PEs, imposing strict bounds on the area and energy budget of each PE. Prior work demonstrates that limited dual-issue capabilities can be efficiently integrated into a lightweight in-order open-source RISC-V core (Snitch), with a geomean IPC boost of 1.6x and a geomean energy efficiency gain of 1.3x, obtained by concurrently executing integer and FP instructions. Unfortunately, this required a complex and error-prone low level programming model (COPIFT). We introduce COPIFTv2 which augments Snitch with lightweight queues enabling direct, fine-grained communication and synchronization between integer and FP threads. By eliminating the tiling and software pipelining steps of COPIFT, we can remove much of its complexity and software overheads. As a result, COPIFTv2 achieves up to a 1.49x speedup and a 1.47x energy-efficiency gain over COPIFT, and a peak IPC of 1.81. Overall, COPIFTv2 significantly enhances the efficiency and programmability of dual-issue execution on lightweight cores. Our implementation is fully open source and performance experiments are reproducible using free software.

</details>


### [19] [Memory-Efficient FPGA Implementation of Stochastic Simulated Annealing](https://arxiv.org/abs/2601.18007)
*Duckgyu Shin,Naoya Onizawa,Warren J. Gross,Takahiro Hanyu*

Main category: cs.AR

TL;DR: 提出硬件感知的随机模拟退火算法（HA-SSA），在FPGA上实现内存高效实现，相比传统SA加速114倍，相比SSA内存效率提升6倍。


<details>
  <summary>Details</summary>
Motivation: 模拟退火（SA）算法在解决组合优化问题时，随着问题规模增大计算时间急剧增加。虽然随机模拟退火（SSA）比传统SA收敛更快，但在FPGA实现中需要存储中间结果，内存使用效率不高。

Method: 提出硬件感知的随机模拟退火算法（HA-SSA），针对FPGA实现进行优化，减少存储中间结果的内存使用，同时保持SSA的计算速度。在最大割组合优化问题上进行评估，使用G-set数据集。

Result: HA-SSA在最大割问题上比传统SA算法收敛速度快114倍（取决于具体问题）。在Xilinx Kintex-7 FPGA上实现，相比传统SSA内存效率提升6倍，同时保持高解质量。

Conclusion: HA-SSA算法在FPGA上实现了内存高效的模拟退火算法，显著提升了计算速度和内存效率，为组合优化问题的硬件加速提供了有效解决方案。

Abstract: Simulated annealing (SA) is a well-known algorithm for solving combinatorial optimization problems. However, the computation time of SA increases rapidly, as the size of the problem grows. Recently, a stochastic simulated annealing (SSA) algorithm that converges faster than conventional SA has been reported. In this paper, we present a hardware-aware SSA (HA- SSA) algorithm for memory-efficient FPGA implementations. HA-SSA can reduce the memory usage of storing intermediate results while maintaining the computing speed of SSA. For evaluation purposes, the proposed algorithm is compared with the conventional SSA and SA approaches on maximum cut combinatorial optimization problems. HA-SSA achieves a convergence speed that is up to 114-times faster than that of the conventional SA algorithm depending on the maximum cut problem selected from the G-set which is a dataset of the maximum cut problems. HA-SSA is implemented on a field-programmable gate array (FPGA) (Xilinx Kintex-7), and it achieves up to 6-times the memory efficiency of conventional SSA while maintaining high solution quality for optimization problems.

</details>


### [20] [CIM-Tuner: Balancing the Compute and Storage Capacity of SRAM-CIM Accelerator via Hardware-mapping Co-exploration](https://arxiv.org/abs/2601.18070)
*Jinwu Chen,Yuhui Shi,He Wang,Zhe Jiang,Jun Yang,Xin Si,Zhenhua Zhu*

Main category: cs.AR

TL;DR: CIM-Tuner是一个自动工具，通过硬件-映射协同探索，在面积约束下实现SRAM-CIM加速器的硬件平衡和最优映射策略


<details>
  <summary>Details</summary>
Motivation: SRAM计算内存(CIM)加速器具有高能效和高吞吐量，但各种CIM设计和未充分探索的映射策略阻碍了计算和存储平衡的全面探索，可能导致显著的性能下降

Method: 通过CIM宏的矩阵抽象和通用加速器模板确保跨各种CIM设计的通用性；采用细粒度两级策略（加速器级调度和宏级分块）实现不同硬件配置下的高效映射

Result: 相比先前的CIM映射，CIM-Tuner的扩展策略空间实现了1.58倍更高的能效和2.11倍更高的吞吐量；应用于具有相同面积预算的SOTA CIM加速器时也提供了相当的改进

Conclusion: CIM-Tuner是一个有效的自动工具，通过硬件-映射协同探索解决了SRAM-CIM加速器的硬件平衡和映射优化问题，仿真精度经过硅验证，工具已开源

Abstract: As an emerging type of AI computing accelerator, SRAM Computing-In-Memory (CIM) accelerators feature high energy efficiency and throughput. However, various CIM designs and under-explored mapping strategies impede the full exploration of compute and storage balancing in SRAM-CIM accelerator, potentially leading to significant performance degradation. To address this issue, we propose CIM-Tuner, an automatic tool for hardware balancing and optimal mapping strategy under area constraint via hardware-mapping co-exploration. It ensures universality across various CIM designs through a matrix abstraction of CIM macros and a generalized accelerator template. For efficient mapping with different hardware configurations, it employs fine-grained two-level strategies comprising accelerator-level scheduling and macro-level tiling. Compared to prior CIM mapping, CIM-Tuner's extended strategy space achieves 1.58$\times$ higher energy efficiency and 2.11$\times$ higher throughput. Applied to SOTA CIM accelerators with identical area budget, CIM-Tuner also delivers comparable improvements. The simulation accuracy is silicon-verified and CIM-Tuner tool is open-sourced at https://github.com/champloo2878/CIM-Tuner.git.

</details>


### [21] [RTeAAL Sim: Using Tensor Algebra to Represent and Accelerate RTL Simulation (Extended Version)](https://arxiv.org/abs/2601.18140)
*Yan Zhu,Boru Chen,Christopher W. Fletcher,Nandeeka Nayak*

Main category: cs.AR

TL;DR: RTeAAL Sim将RTL仿真重新定义为稀疏张量代数问题，通过张量表示电路和仿真过程，解决了传统仿真器的编译时间长和CPU前端瓶颈问题，性能可与高度优化的Verilator竞争。


<details>
  <summary>Details</summary>
Motivation: 传统RTL仿真器将电路直接嵌入仿真二进制文件，导致编译时间长，执行时受CPU前端限制严重，存在指令缓存压力大的问题。

Method: 将RTL仿真重新定义为稀疏张量代数问题：将RTL电路表示为张量，仿真过程作为稀疏张量代数内核，使仿真行为与二进制大小解耦，并应用成熟的张量代数优化技术。

Result: 原型系统即使只应用部分优化，已能缓解编译开销和前端压力，在不同CPU和ISA上性能与高度优化的Verilator仿真器相当。

Conclusion: 张量代数方法为RTL仿真提供了新范式，解决了传统方法的编译和性能瓶颈，展现了在硬件仿真领域的潜力。

Abstract: RTL simulation on CPUs remains a persistent bottleneck in hardware design. State-of-the-art simulators embed the circuit directly into the simulation binary, resulting in long compilation times and execution that is fundamentally CPU frontend-bound, with severe instruction-cache pressure.
  This work proposes RTeAAL Sim, which reformulates RTL simulation as a sparse tensor algebra problem. By representing RTL circuits as tensors and simulation as a sparse tensor algebra kernel, RTeAAL Sim decouples simulation behavior from binary size and makes RTL simulation amenable to well-studied tensor algebra optimizations. We demonstrate that a prototype of our tensor-based simulator, even with a subset of these optimizations, already mitigates the compilation overhead and frontend pressure and achieves performance competitive with the highly optimized Verilator simulator across multiple CPUs and ISAs.

</details>


### [22] [Lifecycle Cost-Effectiveness Modeling for Redundancy-Enhanced Multi-Chiplet Architectures](https://arxiv.org/abs/2601.18159)
*Zizhen Liu,Fangzhiyi Wang,Mengdi Wang,Jing Ye,Hayden Kwok-Hay So,Cheng Liu,Huawei Li*

Main category: cs.AR

TL;DR: 提出多芯片架构生命周期成本效益框架，引入LCE指标，通过联合优化制造成本和运行寿命来评估摊销计算成本，揭示模块与芯片级冗余协同优化策略。


<details>
  <summary>Details</summary>
Motivation: 计算密集型应用需求增长使多芯片架构成为有前景的替代方案，但现有成本模型要么忽略芯片运行寿命内的计算价值摊销，要么未能评估冗余策略对长期成本效益的影响。

Method: 提出全面的成本效益框架，引入生命周期成本效益(LCE)指标，集成：(1)跨芯片内和芯片间级别的冗余感知成本建模，(2)可靠性驱动的寿命估计，(3)冗余配置对整体经济效益的定量分析。

Result: 广泛的权衡研究和多目标优化证明了模型的有效性，揭示了模块级和芯片级冗余之间的必要协同优化策略，以实现成本效益高的多芯片架构设计。

Conclusion: 该框架为多芯片架构提供了全面的成本效益评估方法，通过联合优化制造成本和运行寿命，以及冗余配置的协同优化，能够实现更经济有效的设计。

Abstract: The growing demand for compute-intensive applications has made multi-chiplet architectures a promising alternative to monolithic designs, offering improved scalability and manufacturing flexibility. However, effectively managing the economic effectiveness remains challenging. Existing cost models either overlook the amortization of compute value over a chip's operational lifetime or fail to evaluate how redundancy strategies, which are widely adopted to enhance yield and fault tolerance, impact long-term cost efficiency. This paper presents a comprehensive cost-effectiveness framework for multi-chiplet architectures, introducing a novel Lifecycle Cost Effectiveness (LCE) metric that evaluates amortized compute costs by jointly optimizing manufacturing expenses and operational lifetime. Our approach uniquely integrates: (1) redundancy-aware cost modeling spanning both intra- and inter-chiplet levels, (2) reliability-driven lifetime estimation, and (3) quantitative analysis of how redundancy configurations on overall economic effectiveness. Extensive trade-off and multi-objective optimization studies demonstrate the effectiveness of the model and reveal essential co-optimization strategies between module and chiplet-level redundancy to achieve cost-efficient multi-chiplet architecture designs.

</details>
