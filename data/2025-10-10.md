<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Type, Ability, and Effect Systems: Perspectives on Purity, Semantics, and Expressiveness](https://arxiv.org/abs/2510.07582)
*Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 该论文提出了基于上下文等价性的纯度语义定义，并建议用完整性来衡量表达能力。研究表明最小有效系统和能力系统在表达能力上不可比较，因此提出了结合类型、能力和效果系统的综合方案。


<details>
  <summary>Details</summary>
Motivation: 现有强制纯计算和副作用计算分离的方法（如单子、类型和效果系统、能力系统）在精度和可用性之间存在张力，各有优缺点。论文旨在提升评估这类系统的标准。

Method: 提出了基于上下文等价性的纯度语义定义；用完整性来衡量表达能力；分析最小有效系统和能力系统的表达能力；提出结合类型、能力和效果系统的综合方案；提供逻辑关系来促进纯度和其他属性的证明。

Result: 发现最小有效系统和能力系统在表达能力上是不可比较的，即没有哪个系统在表达能力上完全包含另一个。

Conclusion: 类型、能力和效果系统的结合能够发挥各自的优势同时避免弱点，为各种效果类型学科提供了统一的框架和证明工具。

Abstract: Programming benefits from a clear separation between pure, mathematical
computation and impure, effectful interaction with the world. Existing
approaches to enforce this separation include monads, type-and-effect systems,
and capability systems. All share a tension between precision and usability,
and each one has non-obvious strengths and weaknesses.
  This paper aims to raise the bar in assessing such systems. First, we propose
a semantic definition of purity, inspired by contextual equivalence, as a
baseline independent of any specific typing discipline. Second, we propose that
expressiveness should be measured by the degree of completeness, i.e., how many
semantically pure terms can be typed as pure. Using this measure, we focus on
minimal meaningful effect and capability systems and show that they are
incomparable, i.e., neither subsumes the other in terms of expressiveness.
  Based on this result, we propose a synthesis and show that type, ability, and
effect systems combine their respective strengths while avoiding their
weaknesses. As part of our formal model, we provide a logical relation to
facilitate proofs of purity and other properties for a variety of effect typing
disciplines.

</details>


### [2] [The Functional Machine Calculus III: Control](https://arxiv.org/abs/2510.07851)
*Willem Heijltjes*

Main category: cs.PL

TL;DR: Functional Machine Calculus扩展了lambda演算，统一了函数式和命令式编程范式，支持计算效应、求值策略和控制流操作，具有汇合归约和类型终止特性。


<details>
  <summary>Details</summary>
Motivation: 统一函数式和命令式编程范式，在保持lambda演算核心特性的基础上嵌入计算效应、求值策略和控制流操作。

Method: 扩展Krivine机器，增加多个操作数栈来建模效应，以及续体栈来建模顺序、分支和循环计算，定义简单操作语义。

Result: 成功嵌入最小但完整的命令式语言，包括条件、异常处理和迭代，以及常量和代数数据类型，具有汇合归约关系和类型系统保证终止性。

Conclusion: 提供了一个统一的功能-命令式计算模型，支持简单类型、直观操作语义和汇合归约语义。

Abstract: The Functional Machine Calculus (Heijltjes 2022) is a new approach to
unifying the imperative and functional programming paradigms. It extends the
lambda-calculus, preserving the key features of confluent reduction and typed
termination, to embed computational effects, evaluation strategies, and control
flow operations. The first instalment modelled sequential higher-order
computation with global store, input/output, probabilities, and
non-determinism, and embedded both the call-by-name and call-by-value
lambda-calculus, as well as Moggi's computational metalanguage and Levy's
call-by-push-value. The present paper extends the calculus from sequential to
branching and looping control flow. This allows the faithful embedding of a
minimal but complete imperative language, including conditionals, exception
handling, and iteration, as well as constants and algebraic data types.
  The calculus is defined through a simple operational semantics, extending the
(simplified) Krivine machine for the lambda-calculus with multiple operand
stacks to model effects and a continuation stack to model sequential,
branching, and looping computation. It features a confluent reduction relation
and a system of simple types that guarantees termination of the machine and
strong normalization of reduction (in the absence of iteration). These
properties carry over to the embedded imperative language, providing a unified
functional-imperative model of computation that supports simple types, a direct
and intuitive operational semantics, and a confluent reduction semantics.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Adaptive Execution Scheduler for DataDios SmartDiff](https://arxiv.org/abs/2510.07811)
*Aryan Poduri*

Main category: cs.DC

TL;DR: 提出了一个用于SmartDiff差分引擎的自适应调度器，通过动态调整批处理大小和线程/工作器数量，在固定CPU和内存预算下最小化p95延迟。


<details>
  <summary>Details</summary>
Motivation: 现有调度方法在批处理大小和并行度选择上缺乏自适应能力，导致延迟和内存使用效率低下，需要智能调度来优化性能。

Method: 使用轻量级预分析器估计字节/行和I/O速率，在线成本/内存模型修剪不安全操作，采用带防护的爬山策略优化延迟，并包含背压和慢任务缓解机制。

Result: 在合成和公开表格基准测试中，调度器将p95延迟降低23-28%（相比调优预热启发式方法），降低峰值内存16-22%，且无内存溢出问题。

Conclusion: 该自适应调度器能有效平衡延迟和内存使用，在保持吞吐量的同时显著提升性能表现。

Abstract: We present an adaptive scheduler for a single differencing engine (SmartDiff)
with two execution modes: (i) in-memory threads and (ii) Dask based
parallelism. The scheduler continuously tunes batch size and worker/thread
count within fixed CPU and memory budgets to minimize p95 latency. A
lightweight preflight profiler estimates bytes/row and I/O rate; an online
cost/memory model prunes unsafe actions; and a guarded hill-climb policy favors
lower latency with backpressure and straggler mitigation. Backend selection is
gated by a conservative working-set estimate so that in-memory execution is
chosen when safe, otherwise Dask is used. Across synthetic and public tabular
benchmarks, the scheduler reduces p95 latency by 23 to 28 percent versus a
tuned warm-up heuristic (and by 35 to 40 percent versus fixed grid baselines),
while lowering peak memory by 16 to 22 percent (25 to 32 percent vs. fixed)
with zero OOMs and comparable throughput.

</details>


### [4] [A Multi-Simulation Bridge for IoT Digital Twins](https://arxiv.org/abs/2510.08164)
*Marco Picone,Samuele Burattini,Marco Melloni,Prasad Talasila,Davide Ziglioli,Matteo Martinelli,Nicola Bicocchi,Alessandro Ricci,Peter Gorm Larsen*

Main category: cs.DC

TL;DR: 提出DT Simulation Bridge框架，实现数字孪生与仿真平台的双向交互，支持系统设计、验证和实时操作。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生在物联网和工业物联网中能力增强，需要与仿真平台无缝集成来支持系统设计、验证和实时操作。

Method: 设计DT Simulation Bridge软件框架，支持数字孪生与仿真环境之间的多样化交互模式，通过双向数据交换实现动态模型更新和实时反馈。

Result: 实验结果表明，该框架提高了设计灵活性，促进了虚拟调试，并支持在真实条件下的实时行为分析。

Conclusion: DT Simulation Bridge在多种工业场景中表现出有效性，能够增强数字孪生与仿真平台的集成能力。

Abstract: The increasing capabilities of Digital Twins (DTs) in the context of the
Internet of Things (IoT) and Industrial IoT (IIoT) call for seamless
integration with simulation platforms to support system design, validation, and
real-time operation. This paper introduces the concept, design, and
experimental evaluation of the DT Simulation Bridge - a software framework that
enables diverse interaction patterns between active DTs and simulation
environments. The framework supports both the DT development lifecycle and the
incorporation of simulations during active operation. Through bidirectional
data exchange, simulations can update DT models dynamically, while DTs provide
real-time feedback to adapt simulation parameters. We describe the
architectural design and core software components that ensure flexible
interoperability and scalable deployment. Experimental results show that the DT
Simulation Bridge enhances design agility, facilitates virtual commissioning,
and supports live behavioral analysis under realistic conditions, demonstrating
its effectiveness across a range of industrial scenarios.

</details>


### [5] [Towards Energy-Efficient Serverless Computing with Hardware Isolation](https://arxiv.org/abs/2510.08180)
*Natalie Carl,Tobias Pfandzelter,David Bermbach*

Main category: cs.DC

TL;DR: 论文提出重新设计无服务器计算的硬件架构，使用硬件隔离而非软件隔离，通过为每个函数分配独立处理器来大幅降低能耗。


<details>
  <summary>Details</summary>
Motivation: 当前无服务器平台在传统服务器硬件上运行数千个函数，需要昂贵的软件隔离机制和大量空闲服务器资源，导致能源效率低下。

Method: 采用硬件隔离方法，为每个函数分配独立处理器，构建仅在实际工作时消耗能源的无服务器硬件堆栈。

Result: 初步评估显示，该方法可将能耗开销降低90.63%，平均节省70.8MW。

Conclusion: 硬件隔离的无服务器架构能显著提高能源效率，实现真正的按需资源消耗。

Abstract: Serverless computing provides just-in-time infrastructure provisioning with
rapid elasticity and a finely-grained pricing model. As full control of
resource allocation is in the hands of the cloud provider and applications only
consume resources when they actually perform work, we believe that serverless
computing is uniquely positioned to maximize energy efficiency.
  However, the focus of current serverless platforms is to run hundreds or
thousands of serverless functions from different tenants on traditional server
hardware, requiring expensive software isolation mechanisms and a high degree
of overprovisioning, i.e., idle servers, to anticipate load spikes. With shared
caches, high clock frequencies, and many-core architectures, servers today are
optimized for large, singular workloads but not to run thousands of isolated
functions.
  We propose rethinking the serverless hardware architecture to align it with
the requirements of serverless software. Specifically, we propose using
hardware isolation with individual processors per function instead of software
isolation resulting in a serverless hardware stack that consumes energy only
when an application actually performs work. In preliminary evaluation with real
hardware and a typical serverless workload we find that this could reduce
energy consumption overheads by 90.63% or an average 70.8MW.

</details>


### [6] [Distributed Resource Selection for Self-Organising Cloud-Edge Systems](https://arxiv.org/abs/2510.08228)
*Quentin Renau,Amjad Ullah,Emma Hart*

Main category: cs.DC

TL;DR: 提出了一种分布式资源选择机制，用于云边异构环境中的动态资源分配，通过分布式决策过程确保效率、可扩展性和弹性。


<details>
  <summary>Details</summary>
Motivation: 在高度动态的云边环境中，集中式协调成为瓶颈，需要分布式方法来实现高效、可扩展和弹性的资源分配。

Method: 采用基于共识的机制，利用本地知识和代理间协作，无需依赖中央控制器，实现分布式编排。

Result: 计算时间是影响分配决策的关键因素。该方法在不牺牲最优性或增加成本的情况下，持续提供快速分配，在集中式启发式算法运行速度慢30倍的情况下仍能及时获得结果。

Conclusion: 该机制为分布式编排铺平了道路，在规模较大时能够实现及时的资源分配，而穷举搜索不可行，集中式方法效率低下。

Abstract: This paper presents a distributed resource selection mechanism for diverse
cloud-edge environments, enabling dynamic and context-aware allocation of
resources to meet the demands of complex distributed applications. By
distributing the decision-making process, our approach ensures efficiency,
scalability, and resilience in highly dynamic cloud-edge environments where
centralised coordination becomes a bottleneck. The proposed mechanism aims to
function as a core component of a broader, distributed, and self-organising
orchestration system that facilitates the intelligent placement and adaptation
of applications in real-time. This work leverages a consensus-based mechanism
utilising local knowledge and inter-agent collaboration to achieve efficient
results without relying on a central controller, thus paving the way for
distributed orchestration. Our results indicate that computation time is the
key factor influencing allocation decisions. Our approach consistently delivers
rapid allocations without compromising optimality or incurring additional cost,
achieving timely results at scale where exhaustive search is infeasible and
centralised heuristics run up to 30 times slower.

</details>


### [7] [Energy-Efficient Maximal Independent Sets in Radio Networks](https://arxiv.org/abs/2510.08244)
*Dominick Banasik,Varsha Dani,Fabien Dufoulon,Aayush Gupta,Thomas P. Hayes,Gopal Pandurangan*

Main category: cs.DC

TL;DR: 本文提出了无线网络中更节能的最大独立集（MIS）分布式算法，在CD模型中达到O(log n)能量复杂度（最优），在no-CD模型中达到O(log²n log log n)能量复杂度，显著优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 无线网络（特别是自组织和传感器网络）通常由电池供电，能量是宝贵资源。需要设计能量复杂度尽可能低的分布式算法来延长网络寿命。

Method: 使用随机化分布式算法，在两种无线电网络模型（有碰撞检测CD和无碰撞检测no-CD）中分别设计MIS算法，通过优化节点唤醒轮次来最小化能量消耗。

Result: CD模型：能量复杂度O(log n)（最优），轮复杂度O(log² n)，失败概率1/poly(n)；no-CD模型：能量复杂度O(log²n log log n)，轮复杂度O(log³ n log Δ)，显著优于现有O(log³ n)算法。

Conclusion: 在无线网络中实现了更节能的MIS算法，证明了CD模型中的能量复杂度下界，并在no-CD模型中取得了显著改进，为能量受限的无线网络提供了高效解决方案。

Abstract: The maximal independent set (MIS) is one of the most fundamental problems in
distributed computing, and it has been studied intensively for over four
decades. This paper focuses on the MIS problem in the Radio Network model, a
standard model widely used to model wireless networks, particularly ad hoc
wireless and sensor networks. Energy is a premium resource in these networks,
which are typically battery-powered. Hence, designing distributed algorithms
that use as little energy as possible is crucial. We use the well-established
energy model where a node can be sleeping or awake in a round, and only the
awake rounds (when it can send or listen) determine the energy complexity of
the algorithm, which we want to minimize.
  We present new, more energy-efficient MIS algorithms in radio networks with
arbitrary and unknown graph topology. We present algorithms for two popular
variants of the radio model -- with collision detection (CD) and without
collision detection (no-CD). Specifically, we obtain the following results:
  1. CD model: We present a randomized distributed MIS algorithm with energy
complexity $O(\log n)$, round complexity $O(\log^2 n)$, and failure probability
$1 / poly(n)$, where $n$ is the network size. We show that our energy
complexity is optimal by showing a matching $\Omega(\log n)$ lower bound.
  2. no-CD model: In the more challenging no-CD model, we present a randomized
distributed MIS algorithm with energy complexity $O(\log^2n \log \log n)$,
round complexity $O(\log^3 n \log \Delta)$, and failure probability $1 /
poly(n)$. The energy complexity of our algorithm is significantly lower than
the round (and energy) complexity of $O(\log^3 n)$ of the best known
distributed MIS algorithm of Davies [PODC 2023] for arbitrary graph topology.

</details>


### [8] [Investigating Matrix Repartitioning to Address the Over- and Undersubscription Challenge for a GPU-based CFD Solver](https://arxiv.org/abs/2510.08536)
*Gregor Olenik,Marcel Koch,Hartwig Anzt*

Main category: cs.DC

TL;DR: 提出了一种重新分区策略来改进OpenFOAM中基于插件的GPU加速方法，通过平衡CPU矩阵组装和GPU线性求解来缓解资源过载问题。


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算越来越依赖GPU，但在复杂科学框架如OpenFOAM中集成GPU加速仍面临挑战。现有方法要么完全重构代码库，要么使用基于插件的GPU求解器，在性能和开发工作量之间存在权衡。

Method: 提出重新分区策略，包括详细的计算模型、新颖的矩阵重新分区和更新程序，以更好地平衡CPU矩阵组装和基于GPU的线性求解。

Result: 在大规模CFD模拟中，所提方法显著缓解了过载问题，提高了异构CPU-GPU环境中的求解器性能和资源利用率。

Conclusion: 该重新分区方法有效改进了OpenFOAM中基于插件的GPU加速，为异构计算环境提供了更好的性能平衡。

Abstract: Modern high-performance computing (HPC) increasingly relies on GPUs, but
integrating GPU acceleration into complex scientific frameworks like OpenFOAM
remains a challenge. Existing approaches either fully refactor the codebase or
use plugin-based GPU solvers, each facing trade-offs between performance and
development effort. In this work, we address the limitations of plugin-based
GPU acceleration in OpenFOAM by proposing a repartitioning strategy that better
balances CPU matrix assembly and GPU-based linear solves. We present a detailed
computational model, describe a novel matrix repartitioning and update
procedure, and evaluate its performance on large-scale CFD simulations. Our
results show that the proposed method significantly mitigates oversubscription
issues, improving solver performance and resource utilization in heterogeneous
CPU-GPU environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [How long can you sleep? Idle Time System Inefficiencies and Opportunities](https://arxiv.org/abs/2510.07449)
*Georgia Antoniou,Haris Volos,Jawad Haj Yahya,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: 该研究通过排队模型揭示现代服务器运行延迟关键应用时的空闲机会，发现实际空闲时间与理论模型存在显著差距，主要由于空闲管理不准确和深度空闲状态切换延迟高。


<details>
  <summary>Details</summary>
Motivation: 揭示现代服务器在运行延迟关键应用时的空闲机会，量化实际空闲时间与理论模型之间的差距，为系统设计提供早期探索方法。

Method: 使用三种排队模型（M/M/1、cxM/M/1和M/M/c）估计CPU核心和系统级别的理论空闲时间分布，并与实际服务器空闲情况进行比较分析。

Result: 比较发现实际服务器空闲时间与理论模型存在显著差异，存在大量未利用的深度空闲状态机会，主要归因于空闲管理不准确和深度空闲状态切换延迟高。

Conclusion: 提出的方法为早期系统设计探索提供了手段，能够洞察不同服务器配置和负载下的空闲时间行为和优化机会。

Abstract: This work introduces a model-based framework that reveals the idle
opportunity of modern servers running latency-critical applications.
Specifically, three queuing models, M/M/1, cxM/M/1, and M/M/c, are used to
estimate the theoretical idle time distribution at the CPU core and system
(package) level. A comparison of the actual idleness of a real server and that
from the theoretical models reveals significant missed opportunities to enter
deep idle states. This inefficiency is attributed to the idle-governor
inaccuracy and the high latency to transition to/from legacy deep-idle states.
The proposed methodology offers the means for an early-stage design exploration
and insights into idle time behavior and opportunities for varying server
system configurations and load.

</details>


### [10] [DL-PIM: Improving Data Locality in Processing-in-Memory Systems](https://arxiv.org/abs/2510.07719)
*Parker Hao Tian,Zahra Yousefijamarani,Alaa Alameldeen*

Main category: cs.AR

TL;DR: DL-PIM是一种新型PIM架构，通过动态检测数据移动开销并主动将数据移动到本地内存的保留区域，使用分布式地址间接硬件查找表重定向流量，从而改善数据局部性并提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统PIM架构虽然能提高能效和性能，但其优势依赖于数据与处理单元的邻近性。数据移动开销会因需要在处理单元和远程内存位置之间移动数据而降低PIM的性能和能效。

Method: 提出DL-PIM架构，动态检测数据移动开销，主动将数据移动到请求处理单元的本地内存保留区域；使用分布式地址间接硬件查找表重定向流量；在HMC和HBM两种3D堆叠内存上实现；采用自适应机制评估间接访问的成本和收益，动态启用或禁用间接访问。

Result: DL-PIM将HMC中每个请求的平均内存延迟降低54%，HBM中降低50%；对于具有大量数据重用的工作负载，HMC性能提升15%，HBM提升5%；所有代表性工作负载在HMC中实现6%加速，HBM中实现3%加速。

Conclusion: DL-PIM通过增强数据局部性有效提升了PIM架构的整体系统性能，证明了动态数据移动管理策略在改善内存访问效率方面的价值。

Abstract: PIM architectures aim to reduce data transfer costs between processors and
memory by integrating processing units within memory layers. Prior PIM
architectures have shown potential to improve energy efficiency and
performance. However, such advantages rely on data proximity to the processing
units performing computations. Data movement overheads can degrade PIM's
performance and energy efficiency due to the need to move data between a
processing unit and a distant memory location. %they face challenges due to the
overhead of transferring data from remote memory locations to processing units
inside memory for computation. In this paper, we demonstrate that a large
fraction of PIM's latency per memory request is attributed to data transfers
and queuing delays from remote memory accesses. To improve PIM's data locality,
we propose DL-PIM, a novel architecture that dynamically detects the overhead
of data movement, and proactively moves data to a reserved area in the local
memory of the requesting processing unit. DL-PIM uses a distributed
address-indirection hardware lookup table to redirect traffic to the current
data location. We propose DL-PIM implementations on two 3D stacked memories:
HMC and HBM. While some workloads benefit from DL-PIM, others are negatively
impacted by the additional latency due to indirection accesses. Therefore, we
propose an adaptive mechanism that assesses the cost and benefit of indirection
and dynamically enables or disables it to prevent degrading workloads that
suffer from indirection. Overall, DL-PIM reduces the average memory latency per
request by 54% in HMC and 50% in HBM which resulted in performance improvement
of 15% for workloads with substantial data reuse in HMC and 5% in HBM. For all
representative workloads, DL-PIM achieved a 6% speedup in HMC and a 3% speedup
in HBM, showing that DL-PIM enhances data locality and overall system
performance.

</details>


### [11] [A Scalable FPGA Architecture With Adaptive Memory Utilization for GEMM-Based Operations](https://arxiv.org/abs/2510.08137)
*Anastasios Petropoulos,Theodore Antonakopoulos*

Main category: cs.AR

TL;DR: 提出基于FPGA的可动态配置DNN推理加速器，采用脉动阵列、高带宽内存和UltraRAM，支持多种处理单元配置，具有高吞吐效率和模拟内存计算能力。


<details>
  <summary>Details</summary>
Motivation: 随着DNN推理对专用硬件计算效率需求的增长，需要设计灵活可配置的加速器架构来适应不同模型和未来FPGA设计。

Method: 使用FPGA构建动态可配置加速器，包含脉动阵列、高带宽内存和UltraRAM，提供两种不同计算能力的处理单元配置，采用启发式权重传输调度策略。

Result: 该架构在吞吐效率上优于先前工作，并能扩展模拟内存计算设备以支持下一代异构AIMC芯片设计。

Conclusion: 该工作提出了一种适用于各种模型和未来FPGA设计的通用DNN推理加速架构，具有高度适应性和扩展性。

Abstract: Deep neural network (DNN) inference relies increasingly on specialized
hardware for high computational efficiency. This work introduces a
field-programmable gate array (FPGA)-based dynamically configurable accelerator
featuring systolic arrays, high-bandwidth memory, and UltraRAMs. We present two
processing unit (PU) configurations with different computing capabilities using
the same interfaces and peripheral blocks. By instantiating multiple PUs and
employing a heuristic weight transfer schedule, the architecture achieves
notable throughput efficiency over prior works. Moreover, we outline how the
architecture can be extended to emulate analog in-memory computing (AIMC)
devices to aid next-generation heterogeneous AIMC chip designs and investigate
device-level noise behavior. Overall, this brief presents a versatile DNN
inference acceleration architecture adaptable to various models and future FPGA
designs.

</details>


### [12] [FMCache: File-System Metadata Caching in Programmable Switches](https://arxiv.org/abs/2510.08351)
*Qingxiu Liu,Jiazhen Cai,Siyuan Sheng,Yuhui Chen,Lu Tang,Zhirong Shen,Patrick P. C. Lee*

Main category: cs.AR

TL;DR: FMCache是一个利用可编程交换机在数据平面直接服务文件系统元数据请求的框架，解决了多客户端场景下元数据缓存一致性的问题，相比传统HDFS实现了最高181.6%的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 分布式文件系统中，跨多个元数据服务器的快速可扩展元数据管理至关重要。客户端缓存可以减轻服务器负载，但在客户端数量增加时，维护缓存一致性会带来显著开销和复杂性。

Method: 提出FMCache框架，利用可编程交换机在交换机数据平面直接服务来自多个客户端的文件系统元数据请求，解决了文件系统特定的路径依赖问题，并在严格的交换机资源约束下实现。

Result: 在Tofino交换机测试平台上使用真实文件系统元数据工作负载进行评估，FMCache相比原始HDFS实现了最高181.6%的吞吐量提升，与客户端缓存结合时还能带来额外最高139.6%的吞吐量增益，同时保持低延迟和有限的交换机资源使用。

Conclusion: FMCache通过利用可编程交换机有效解决了分布式文件系统中元数据管理的可扩展性问题，显著提升了系统吞吐量，同时保持了低延迟和资源效率。

Abstract: Fast and scalable metadata management across multiple metadata servers is
crucial for distributed file systems to handle numerous files and directories.
Client-side caching of frequently accessed metadata can mitigate server loads,
but incurs significant overhead and complexity in maintaining cache consistency
when the number of clients increases. We propose FMCache, an in-switch
file-system metadata caching framework that leverages programmable switches to
serve file-system metadata requests from multiple clients directly in the
switch data plane. Unlike prior in-switch key-value caching approaches, FMCache
addresses file-system-specific path dependencies under stringent switch
resource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on
a Tofino-switch testbed using real-world file-system metadata workloads.
FMCache achieves up to 181.6% higher throughput than vanilla HDFS and
complements client-side caching with additional throughput gains of up to
139.6%. It also incurs low latencies and limited switch resource usage.

</details>


### [13] [SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM Inference](https://arxiv.org/abs/2510.08544)
*Hengrui Zhang,Pratyush Patel,August Ning,David Wentzlaff*

Main category: cs.AR

TL;DR: SPAD提出专用硬件设计，分别为LLM推理的prefill和decode阶段设计专用芯片，相比通用GPU能显著降低硬件成本和功耗。


<details>
  <summary>Details</summary>
Motivation: 当前数据中心GPU/TPU采用"越多越好"的设计理念，导致prefill阶段内存带宽利用不足，decode阶段计算资源利用不足，增加了服务成本。

Method: 采用"少即是多"方法设计专用芯片：Prefill芯片使用更大的脉动阵列和成本效益高的GDDR内存；Decode芯片保持高内存带宽但减少计算容量。

Result: 相比H100，Prefill芯片平均性能提升8%，硬件成本降低52%；Decode芯片达到97%性能，TDP降低28%。端到端模拟显示硬件成本降低19%-41%，TDP降低2%-17%。

Conclusion: SPAD设计具有长期适用性，即使模型和工作负载变化，重新分配芯片类型仍能实现11%-43%的硬件成本降低。

Abstract: Large Language Models (LLMs) have gained popularity in recent years, driving
up the demand for inference. LLM inference is composed of two phases with
distinct characteristics: a compute-bound prefill phase followed by a
memory-bound decode phase. To efficiently serve LLMs, prior work proposes
prefill-decode disaggregation to run each phase on separate hardware. However,
existing hardware poorly matches the different requirements of each phase.
Current datacenter GPUs and TPUs follow a more-is-better design philosophy that
maximizes compute and memory resources, causing memory bandwidth
underutilization in the prefill phase and compute underutilization in the
decode phase. Such underutilization directly translates into increased serving
costs.
  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting
a less-is-more methodology to design specialized chips tailored to the distinct
characteristics of prefill and decode phases. The proposed Prefill Chips have
larger systolic arrays and use cost-effective GDDR memory, whereas the proposed
Decode Chips retain high memory bandwidth but reduce compute capacity. Compared
to modeled H100s, simulations show that the proposed Prefill Chips deliver 8%
higher prefill performance on average at 52% lower hardware cost, while the
proposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.
  End-to-end simulations on production traces show that SPAD reduces hardware
cost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while
offering the same performance. Even when models and workloads change, SPAD can
reallocate either type of chip to run either phase and still achieve 11%-43%
lower hardware costs, demonstrating the longevity of the SPAD design.

</details>
