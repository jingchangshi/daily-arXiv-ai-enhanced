<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols](https://arxiv.org/abs/2509.08182)
*Faruk Alpay,Taylan Alpay*

Main category: cs.PL

TL;DR: 本文提出了基于XML标签的结构化提示方法，通过格理论和不动点定理来形式化分析提示工程的收敛性，并展示了在语法约束解码和人机交互中的实际应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型输出格式不可控的问题，需要开发一种能够确保输出符合预定模式（schema）的结构化提示方法，同时提供理论保证和实际部署方案。

Method: 采用逻辑优先的方法，将XML提示形式化为完全格结构，应用Knaster-Tarski不动点定理和Banach压缩映射原理来证明提示操作的收敛性，并结合上下文无关文法和约束解码技术。

Result: 建立了XML提示的完整理论框架，证明了单调提示操作存在最小不动点，在任务感知度量下具有迭代收敛性，并通过CFG保证了输出的良好格式和任务性能。

Conclusion: 该研究为结构化提示提供了坚实的数学基础，统一了语法约束解码、固定点语义和人机交互循环，为实际系统部署提供了可验证的理论保证和实践模式。

Abstract: Structured prompting with XML tags has emerged as an effective way to steer
large language models (LLMs) toward parseable, schema-adherent outputs in
real-world systems. We develop a logic-first treatment of XML prompting that
unifies (i) grammar-constrained decoding, (ii) fixed-point semantics over
lattices of hierarchical prompts, and (iii) convergent human-AI interaction
loops. We formalize a complete lattice of XML trees under a refinement order
and prove that monotone prompt-to-prompt operators admit least fixed points
(Knaster-Tarski) that characterize steady-state protocols; under a task-aware
contraction metric on trees, we further prove Banach-style convergence of
iterative guidance. We instantiate these results with context-free grammars
(CFGs) for XML schemas and show how constrained decoding guarantees
well-formedness while preserving task performance. A set of multi-layer
human-AI interaction recipes demonstrates practical deployment patterns,
including multi-pass "plan $\to$ verify $\to$ revise" routines and agentic tool
use. We provide mathematically complete proofs and tie our framework to recent
advances in grammar-aligned decoding, chain-of-verification, and programmatic
prompting.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Aurora: Architecting Argonne's First Exascale Supercomputer for Accelerated Scientific Discovery](https://arxiv.org/abs/2509.08207)
*Benjamin S. Allen,James Anchell,Victor Anisimov,Thomas Applencourt,Abhishek Bagusetty,Ramesh Balakrishnan,Riccardo Balin,Solomon Bekele,Colleen Bertoni,Cyrus Blackworth,Renzo Bustamante,Kevin Canada,John Carrier,Christopher Chan-nui,Lance C. Cheney,Taylor Childers,Paul Coffman,Susan Coghlan,Michael D'Mello,Murali Emani,Kyle G. Felker,Sam Foreman,Olivier Franza,Longfei Gao,Marta García,María Garzarán,Balazs Gerofi,Yasaman Ghadar,Neha Gupta,Kevin Harms,Väinö Hatanpää,Brian Holland,Carissa Holohan,Brian Homerding,Khalid Hossain,Louise Huot,Huda Ibeid,Joseph A. Insley,Sai Jayanthi,Hong Jiang,Wei Jiang,Xiao-Yong Jin,Jeongnim Kim,Christopher Knight,Kalyan Kumaran,JaeHyuk Kwack,Ti Leggett,Ben Lenard,Chris Lewis,Nevin Liber,Johann Lombardi,Raymond M. Loy,Ye Luo,Bethany Lusch,Nilakantan Mahadevan,Victor A. Mateevitsi,Gordon McPheeters,Ryan Milner,Vitali A. Morozov,Servesh Muralidharan,Tom Musta,Mrigendra Nagar,Vikram Narayana,Marieme Ngom,Anthony-Trung Nguyen,Nathan Nichols,Aditya Nishtala,James C. Osborn,Michael E. Papka,Scott Parker,Saumil S. Patel,Adrian C. Pope,Sucheta Raghunanda,Esteban Rangel,Paul M. Rich,Silvio Rizzi,Kris Rowe,Varuni Sastry,Adam Scovel,Filippo Simini,Haritha Siddabathuni Som,Patrick Steinbrecher,Rick Stevens,Xinmin Tian,Peter Upton,Thomas Uram,Archit K. Vasan,Álvaro Vázquez-Mayagoitia,Kaushik Velusamy,Brice Videau,Venkatram Vishwanath,Brian Whitney,Timothy J. Williams,Michael Woodacre,Sam Zeltner,Gengbin Zheng,Huihuo Zheng*

Main category: cs.DC

TL;DR: Aurora是阿贡国家实验室的百亿亿次超级计算机，采用英特尔Xeon Sapphire Rapids CPU和Ponte Vecchio GPU，集成DAOS存储系统和HPE Slingshot互连技术，支持科学发现应用。


<details>
  <summary>Details</summary>
Motivation: 为加速科学发现而设计，通过创新的架构技术实现百亿亿次计算能力，满足现代科学计算对高性能计算资源的需求。

Method: 采用英特尔Xeon Sapphire Rapids CPU（支持HBM高带宽内存）和Ponte Vecchio GPU的组合架构，集成分布式异步对象存储(DAOS)系统，使用HPE Slingshot互连技术，并基于Intel oneAPI编程环境。

Result: 论文深入探讨了Aurora的节点架构、互连技术、软件生态系统和存储系统，提供了标准基准测试性能数据，并通过早期科学计划和百亿亿次计算项目展示了应用准备情况。

Conclusion: Aurora代表了超级计算技术的重大进步，其创新的硬件架构和软件生态系统为科学发现提供了强大的计算平台，证明了百亿亿次计算系统的可行性和应用价值。

Abstract: Aurora is Argonne National Laboratory's pioneering Exascale supercomputer,
designed to accelerate scientific discovery with cutting-edge architectural
innovations. Key new technologies include the Intel(TM) Xeon(TM) Data Center
GPU Max Series (code-named Sapphire Rapids) with support for High Bandwidth
Memory (HBM), alongside the Intel(TM) Data Center GPU Max Series (code-named
Ponte Vecchio) on each compute node. Aurora also integrates the Distributed
Asynchronous Object Storage (DAOS), a novel exascale storage solution, and
leverages Intel's oneAPI programming environment. This paper presents an
in-depth exploration of Aurora's node architecture, the HPE Slingshot
interconnect, the supporting software ecosystem, and DAOS. We provide insights
into standard benchmark performance and applications readiness efforts via
Aurora's Early Science Program and the Exascale Computing Project.

</details>


### [3] [Design and Implementation of Code Completion System Based on LLM and CodeBERT Hybrid Subsystem](https://arxiv.org/abs/2509.08215)
*Bingbing Zhang,Ziyu Lin,Yingxin Su*

Main category: cs.DC

TL;DR: 基于CodeBERT和GPT-3.5的混合模型在代码建议和自动完成任务中表现优异，结合了两者的优势


<details>
  <summary>Details</summary>
Motivation: 提高软件开发中的编码效率和准确性，利用CodeBERT在代码摘要和语义理解上的优势，以及GPT-3.5在代码生成方面的强大能力

Method: 实现了一种混合模型，整合CodeBERT的上下文感知能力和GPT-3.5的高级代码生成能力

Result: 在准确性、生成代码质量和性能效率三个主要指标上超过了基准模型，稳健性测试证明了模型的可靠性和稳定性

Conclusion: 混合混合深度学习模型可以充分发挥各自优势，在软件开发行业具有重要意义，为代码建议工具的发展提供了新方向

Abstract: In the rapidly evolving industry of software development, coding efficiency
and accuracy play significant roles in delivering high-quality software.
Various code suggestion and completion tools, such as CodeBERT from Microsoft
and GPT-3.5 from OpenAI, have been developed using deep learning techniques and
integrated into IDEs to assist software engineers' development. Research has
shown that CodeBERT has outstanding performance in code summarization and
capturing code semantics, while GPT-3.5 demonstrated its adept capability at
code generation. This study focuses on implementing a hybrid model that
integrates CodeBERT and GPT-3.5 models to accomplish code suggestion and
autocomplete tasks, leveraging the context-aware effectiveness of CodeBERT and
taking advantage of advanced code generation abilities of GPT-3.5. Evaluated in
three main metrics: accuracy, quality of generated code and performance
efficiency with various software and hardware, the hybrid model outperforms
benchmarks, demonstrating its feasibility and effectiveness. Robustness testing
further confirms the reliability and stability of the hybrid model. This study
not only emphasizes the importance of deep learning in the software development
industry, but also reveals the potential of synthesizing complementary deep
learning models to fully exploit strengths of each model.

</details>


### [4] [Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and Dynamic Parallelism](https://arxiv.org/abs/2509.08309)
*Zizhao Mo,Jianxiong Liao,Huanle Xu,Zhi Zhou,Chengzhong Xu*

Main category: cs.DC

TL;DR: Hetis是一个针对异构GPU集群的LLM服务系统，通过细粒度动态并行化设计解决内存和计算效率问题，提升服务吞吐量2.25倍，降低延迟1.49倍


<details>
  <summary>Details</summary>
Motivation: LLM服务需要充分利用异构硬件资源，但现有并行化方法在异构环境中扩展效率低下，存在内存和计算效率问题

Method: 采用细粒度动态并行化设计：选择性并行化计算密集型操作，以头部粒度动态分配Attention计算到低端GPU，并配备在线负载调度策略

Result: 相比现有系统，服务吞吐量提升最高2.25倍，延迟降低1.49倍

Conclusion: Hetis通过精细化的异构资源管理策略，有效解决了LLM在异构GPU集群中的服务效率问题

Abstract: The significant resource demands in LLM serving prompts production clusters
to fully utilize heterogeneous hardware by partitioning LLM models across a mix
of high-end and low-end GPUs. However, existing parallelization approaches
often struggle to scale efficiently in heterogeneous environments due to their
coarse-grained and static parallelization strategies.
  In this paper, we introduce Hetis, a new LLM system tailored for
heterogeneous GPU clusters. Hetis addresses two critical challenges: (1) memory
inefficiency caused by the mismatch between memory capacity and computational
power in heterogeneous devices, and (2) computational inefficiency arising from
performance gaps across different LLM modules. To tackle these issues, Hetis
employs a fine-grained and dynamic parallelism design. Specifically, it
selectively parallelizes compute-intensive operations to reduce latency and
dynamically distributes Attention computations to low-end GPUs at a head
granularity, leveraging the distinct characteristics of each module.
Additionally, Hetis features an online load dispatching policy that
continuously optimizes serving performance by carefully balancing network
latency, computational load, and memory intensity. Evaluation results
demonstrate that Hetis can improve serving throughput by up to $2.25\times$ and
reduce latency by $1.49\times$ compared to existing systems.

</details>


### [5] [An HPC Benchmark Survey and Taxonomy for Characterization](https://arxiv.org/abs/2509.08347)
*Andreas Herten,Olga Pearce,Filipe S. M. Guimarães*

Main category: cs.DC

TL;DR: 这篇论文对高性能计算领域的几乎所有量化测试标准进行了系统性调查和分类，提供了详细的表格总结和交互式网站工具


<details>
  <summary>Details</summary>
Motivation: 高性能计算领域需要准确评估硬件、软件和算法性能的标准化测试工具，但现有测试标准分散且缺乏统一分类体系

Method: 调查现有HPC测试标准，通过表格形式汇总关键信息，并提出了一个测试标准分类法来进行明确的特征化描述

Result: 完整的HPC测试标准调查结果，包括互动式网站工具和详细的分类表格，为系统架构师、研究人员和科学用户提供了综合性的参考资源

Conclusion: 该研究为HPC社区提供了一个统一的测试标准分类框架和完整的调查数据，有助于提高硬件软件评估的准确性和效率，促进HPC领域的发展

Abstract: The field of High-Performance Computing (HPC) is defined by providing
computing devices with highest performance for a variety of demanding
scientific users. The tight co-design relationship between HPC providers and
users propels the field forward, paired with technological improvements,
achieving continuously higher performance and resource utilization. A key
device for system architects, architecture researchers, and scientific users
are benchmarks, allowing for well-defined assessment of hardware, software, and
algorithms. Many benchmarks exist in the community, from individual niche
benchmarks testing specific features, to large-scale benchmark suites for whole
procurements. We survey the available HPC benchmarks, summarizing them in table
form with key details and concise categorization, also through an interactive
website. For categorization, we present a benchmark taxonomy for well-defined
characterization of benchmarks.

</details>


### [6] [Towards Communication-Efficient Decentralized Federated Graph Learning over Non-IID Data](https://arxiv.org/abs/2509.08409)
*Shilong Wang,Jianchun Liu,Hongli Xu,Chenxia Tang,Qianpiao Ma,Liusheng Huang*

Main category: cs.DC

TL;DR: Duplex是一个联合优化网络拓扑和图采样的统一框架，通过考虑两者的耦合关系，在降低通信成本的同时提升DFGL训练性能。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦图学习(DFGL)虽然避免了参数服务器的瓶颈，但图节点嵌入的跨工作节点通信带来了巨大的通信开销。现有方法单独使用稀疏网络拓扑或图邻居采样，但直接组合会导致训练性能显著下降。

Method: 提出Duplex框架，通过考虑网络拓扑和图采样的耦合关系进行联合优化。引入学习驱动算法自适应确定最优网络拓扑和图采样比例，以应对统计异构性和动态网络环境等挑战。

Result: 实验结果显示，Duplex在达到目标精度时减少完成时间20.1%-48.8%，通信成本降低16.7%-37.6%，在相同资源预算下精度提升3.3%-7.9%。

Conclusion: Duplex通过联合优化网络拓扑和图采样，有效解决了DFGL中的通信效率问题，在减少通信开销的同时提升了训练性能和准确性。

Abstract: Decentralized Federated Graph Learning (DFGL) overcomes potential bottlenecks
of the parameter server in FGL by establishing a peer-to-peer (P2P)
communication network among workers. However, while extensive cross-worker
communication of graph node embeddings is crucial for DFGL training, it
introduces substantial communication costs. Most existing works typically
construct sparse network topologies or utilize graph neighbor sampling methods
to alleviate the communication overhead in DFGL. Intuitively, integrating these
methods may offer promise for doubly improving communication efficiency in
DFGL. However, our preliminary experiments indicate that directly combining
these methods leads to significant training performance degradation if they are
jointly optimized. To address this issue, we propose Duplex, a unified
framework that jointly optimizes network topology and graph sampling by
accounting for their coupled relationship, thereby significantly reducing
communication cost while enhancing training performance in DFGL. To overcome
practical DFGL challenges, eg, statistical heterogeneity and dynamic network
environments, Duplex introduces a learning-driven algorithm to adaptively
determine optimal network topologies and graph sampling ratios for workers.
Experimental results demonstrate that Duplex reduces completion time by
20.1%--48.8% and communication costs by 16.7%--37.6% to achieve target
accuracy, while improving accuracy by 3.3%--7.9% under identical resource
budgets compared to baselines.

</details>


### [7] [A 410GFLOP/s, 64 RISC-V Cores, 204.8GBps Shared-Memory Cluster in 12nm FinFET with Systolic Execution Support for Efficient B5G/6G AI-Enhanced O-RAN](https://arxiv.org/abs/2509.08608)
*Yichao Zhang,Marco Bertuletti,Sergio Mazzola,Samuel Riedel,Luca Benini*

Main category: cs.DC

TL;DR: HeartStream是一个64核共享L1内存集群，专为AI增强的O-RAN设计，在基带处理和AI计算方面具有高能效表现，满足5G/6G上行链路的时间和功耗约束。


<details>
  <summary>Details</summary>
Motivation: 为满足B5G/6G无线通信系统对高效能基带处理和AI增强功能的需求，需要设计专门的硬件架构来提升能效并满足严格的功耗和延迟要求。

Method: 设计64核RV核心集群，采用共享L1内存架构，支持复数指令（16位实部和虚部）、乘积累加、除法和平方根、SIMD指令，以及硬件管理的脉动队列。

Result: 在800MHz@0.8V下提供243GFLOP/s的复数无线工作负载处理能力，AI处理达72GOP/s，软件定义PUSCH效率达49.6GFLOP/s/W，功耗仅0.68W(645MHz@0.65V)。

Conclusion: HeartStream在基带处理关键内核上能效提升达1.89倍，完全兼容基站功率和处理延迟限制，满足4ms端到端B5G/6G上行链路约束，是高效的AI增强O-RAN解决方案。

Abstract: We present HeartStream, a 64-RV-core shared-L1-memory cluster (410 GFLOP/s
peak performance and 204.8 GBps L1 bandwidth) for energy-efficient AI-enhanced
O-RAN. The cores and cluster architecture are customized for baseband
processing, supporting complex (16-bit real&imaginary) instructions:
multiply&accumulate, division&square-root, SIMD instructions, and
hardware-managed systolic queues, improving up to 1.89x the energy efficiency
of key baseband kernels. At 800MHz@0.8V, HeartStream delivers up to 243GFLOP/s
on complex-valued wireless workloads. Furthermore, the cores also support
efficient AI processing on received data at up to 72 GOP/s. HeartStream is
fully compatible with base station power and processing latency limits: it
achieves leading-edge software-defined PUSCH efficiency (49.6GFLOP/s/W) and
consumes just 0.68W (645MHz@0.65V), within the 4 ms end-to-end constraint for
B5G/6G uplink.

</details>


### [8] [Reconfigurable Holographic Surfaces and Near Field Communication for Non-Terrestrial Networks: Potential and Challenges](https://arxiv.org/abs/2509.08770)
*Muhammad Ali Jamshed,Muhammad Ahmed Mohsin,Hongliang Zhang,Bushra Haq,Aryan Kaushik,Boya Di,Weiwei Jiang*

Main category: cs.DC

TL;DR: 这篇论文提出了一种结合近场通信(NFC)和可重构全息表面(RHS)的方案，用于改善非地面网络(NTN)的性能，包括卫星、高空平台和无人机等平台。


<details>
  <summary>Details</summary>
Motivation: 应对超低延迟、流动覆盖和高数据速率的挑战，提升非地面网络的能量效率、谱效率和空间分辨率。

Method: 设计了一种系统架构，将RHS与NTN平台(卫星、HAPS、UAV)集成，实现近场区域的精确放形和智能波前沿控制。

Result: 证明该集成方案能够提高能量效率、谱效率和空间分辨率，并通过公共安全场景的用例分析进一步验证了UAV-RHS融合的优势。

Conclusion: 这种新题集成方案为非地面网络提供了有效的性能提升途径，并持续探索其应用潜力、挑战和未来发展方向。

Abstract: To overcome the challenges of ultra-low latency, ubiquitous coverage, and
soaring data rates, this article presents a combined use of Near Field
Communication (NFC) and Reconfigurable Holographic Surfaces (RHS) for
Non-Terrestrial Networks (NTN). A system architecture has been presented, which
shows that the integration of RHS with NTN platforms such as satellites, High
Altitute Platform Stations (HAPS), and Uncrewed Aerial Vehicles (UAV) can
achieve precise beamforming and intelligent wavefront control in near-field
regions, enhancing Energy Efficiency (EE), spectral utilization, and spatial
resolution. Moreover, key applications, challenges, and future directions have
been identified to fully adopt this integration. In addition, a use case
analysis has been presented to improve the EE of the system in a public safety
use case scenario, further strengthening the UAV-RHS fusion.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [Analyzing the capabilities of HLS and RTL tools in the design of an FPGA Montgomery Multiplier](https://arxiv.org/abs/2509.08067)
*Rares Ifrim,Decebal Popescu*

Main category: cs.AR

TL;DR: 分析了基于CIOS算法的Montgomery模乘法器在FPGA上的多种实现方案，比较了Verilog简单、Verilog优化和HLS方案的性能差异，并与Rust软件实现进行对比


<details>
  <summary>Details</summary>
Motivation: 为BLS12-381椭圆曲线密码操作提供高频率、高吞吐量的硬件基础，实现每秒数百万次操作的目标

Method: 采用Coarsely Integrated Operand Scanning(CIOS)方法，在AMD-Xilinx工具和Alveo FPGA板卡上实现三类设计：Verilog简单方案、手动优化DSP的Verilog方案、高级综合(HLS)方案，并与Rust软件实现进行对比

Result: 分析了不同设计选择和工具配置对频率、延迟和资源消耗的影响，优化的DSP使用能够提高性能并减少LUT和FF资源消耗

Conclusion: 通过精心设计的DSP原语使用和流水线优化，FPGA实现能够达到高频率高吞吐量的目标，为椭圆曲线密码操作提供了高效的硬件基础

Abstract: We present the analysis of various FPGA design implementations of a
Montgomery Modular Multiplier, compatible with the BLS12-381 elliptic curve,
using the Coarsely Integrated Operand Scanning approach of working with
complete partial products on different digit sizes. The scope of the
implemented designs is to achieve a high-frequency, high-throughput solution
capable of computing millions of operations per second, which can provide a
strong foundation for different Elliptic Curve Cryptography operations such as
point addition and point multiplication. One important constraint for our
designs was to only use FPGA DSP primitives for the arithmetic operations
between digits employed in the CIOS algorithm as these primitives, when
pipelined properly, can operate at a high frequency while also relaxing the
resource consumption of FPGA LUTs and FFs. The target of the analysis is to see
how different design choices and tool configurations influence the frequency,
latency and resource consumption when working with the latest AMD-Xilinx tools
and Alveo FPGA boards in an RTL-HLS hybrid approach. We compare three
categories of designs: a Verilog naive approach where we rely on the Vivado
synthesizer to automatically choose when and where to use DSPs, a Verilog
optimized approach by manually instantiating the DSP primitives ourselves and a
complete High-Level Synthesis approach. We also compare the FPGA
implementations with an optimized software implementation of the same
Montgomery multiplier written in Rust.

</details>


### [10] [Lifetime-Aware Design of Item-Level Intelligence](https://arxiv.org/abs/2509.08193)
*Shvetank Prakash,Andrew Cheng,Olof Kindgren,Ashiq Ahamed,Graham Knight,Jed Kufel,Francisco Rodriguez,Arya Tschand,David Kong,Mariam Elgamal,Jerry Huang,Emma Chen,Gage Hills,Richard Price,Emre Ozer,Vijay Janapa Reddi*

Main category: cs.AR

TL;DR: FlexiFlow是一个面向物品级智能(ILI)的生命周期感知设计框架，通过柔性电子技术实现低成本、可持续的边缘计算，在碳足迹优化方面取得显著效果


<details>
  <summary>Details</summary>
Motivation: 传统计算架构无法满足万亿级部署的物品级智能应用需求，这些应用存在1000倍的操作寿命差异，需要重新思考架构设计决策

Method: 提出包含FlexiBench工作负载套件、FlexiBits面积优化RISC-V核心(1/4/8位数据路径)和碳感知模型的整体框架，使用柔性电子PDK进行流片验证

Result: 实现了2.65-3.50倍的能效提升，生命周期感知微架构设计减少碳足迹1.62倍，算法决策减少碳足迹14.5倍，达到30.9kHz操作频率

Conclusion: FlexiFlow为极端边缘计算提供了新的设计方法论，需要重新评估传统设计方法以适应新的约束条件

Abstract: We present FlexiFlow, a lifetime-aware design framework for item-level
intelligence (ILI) where computation is integrated directly into disposable
products like food packaging and medical patches. Our framework leverages
natively flexible electronics which offer significantly lower costs than
silicon but are limited to kHz speeds and several thousands of gates. Our
insight is that unlike traditional computing with more uniform deployment
patterns, ILI applications exhibit 1000X variation in operational lifetime,
fundamentally changing optimal architectural design decisions when considering
trillion-item deployment scales. To enable holistic design and optimization, we
model the trade-offs between embodied carbon footprint and operational carbon
footprint based on application-specific lifetimes. The framework includes: (1)
FlexiBench, a workload suite targeting sustainability applications from
spoilage detection to health monitoring; (2) FlexiBits, area-optimized RISC-V
cores with 1/4/8-bit datapaths achieving 2.65X to 3.50X better energy
efficiency per workload execution; and (3) a carbon-aware model that selects
optimal architectures based on deployment characteristics. We show that
lifetime-aware microarchitectural design can reduce carbon footprint by 1.62X,
while algorithmic decisions can reduce carbon footprint by 14.5X. We validate
our approach through the first tape-out using a PDK for flexible electronics
with fully open-source tools, achieving 30.9kHz operation. FlexiFlow enables
exploration of computing at the Extreme Edge where conventional design
methodologies must be reevaluated to account for new constraints and
considerations.

</details>


### [11] [FASE: FPGA-Assisted Syscall Emulation for Rapid End-to-End Processor Performance Validation](https://arxiv.org/abs/2509.08405)
*Chengzhen Meng,Xiuzhuang Chen,Hongjun Dai*

Main category: cs.AR

TL;DR: FASE框架是首个在FPGA平台上实现系统调用仿真的工作，通过最小化硬件接口、优化通信协议和远程系统调用处理，使复杂多线程基准测试能直接在处理器设计上运行，无需集成SoC或目标操作系统，实现早期性能验证。


<details>
  <summary>Details</summary>
Motivation: 传统工作流程将验证过程推迟到RTL设计和SoC集成完成后，显著延长了开发和迭代周期。AI工作负载和领域特定架构的快速发展导致处理器微架构日益多样化，需要快速准确的性能验证方法。

Method: 提出FASE框架（FPGA辅助系统调用仿真），包含三个关键创新：1）仅暴露最小CPU接口；2）提出主机-目标协议（HTP）最小化跨设备数据流量；3）提出主机端运行时远程处理Linux风格系统调用。

Result: 在Xilinx FPGA上进行实验，单线程CoreMark性能误差小于1%，相比Proxy Kernel效率提升2000倍以上。复杂OpenMP基准测试显示，大多数单线程工作负载验证准确率超过96%，多线程工作负载超过91.5%。

Conclusion: FASE框架显著降低了开发复杂性和反馈时间，所有组件均已开源发布，为早期阶段性能验证提供了高效准确的解决方案。

Abstract: The rapid advancement of AI workloads and domain-specific architectures has
led to increasingly diverse processor microarchitectures, whose design
exploration requires fast and accurate performance validation. However,
traditional workflows defer validation process until RTL design and SoC
integration are complete, significantly prolonging development and iteration
cycle.
  In this work, we present FASE framework, FPGA-Assisted Syscall Emulation, the
first work for adapt syscall emulation on FPGA platforms, enabling complex
multi-thread benchmarks to directly run on the processor design without
integrating SoC or target OS for early-stage performance validation. FASE
introduces three key innovations to address three critical challenges for
adapting FPGA-based syscall emulation: (1) only a minimal CPU interface is
exposed, with other hardware components untouched, addressing the lack of a
unified hardware interface in FPGA systems; (2) a Host-Target Protocol (HTP) is
proposed to minimize cross-device data traffic, mitigating the low-bandwidth
and high-latency communication between FPGA and host; and (3) a host-side
runtime is proposed to remotely handle Linux-style system calls, addressing the
challenge of cross-device syscall delegation.
  Experiments ware conducted on Xilinx FPGA with open-sourced RISC-V SMP
processor Rocket. With single-thread CoreMark, FASE introduces less than 1%
performance error and achieves over 2000x higher efficiency compared to Proxy
Kernel due to FPGA acceleration. With complex OpenMP benchmarks, FASE
demonstrates over 96% performance validation accuracy for most single-thread
workloads and over 91.5% for most multi-thread workloads compared to full SoC
validation, significantly reducing development complexity and time-to-feedback.
All components of FASE framework are released as open-source.

</details>


### [12] [AutoVeriFix: Automatically Correcting Errors and Enhancing Functional Correctness in LLM-Generated Verilog Code](https://arxiv.org/abs/2509.08416)
*Yan Tan,Xiangchen Meng,Zijun Jiang,Yangdi Lyu*

Main category: cs.AR

TL;DR: AutoVeriFix是一个两阶段框架，通过Python参考模型和自动化测试来提升LLM生成的Verilog代码功能正确性


<details>
  <summary>Details</summary>
Motivation: 解决LLM在硬件描述语言(如Verilog)代码生成中因训练数据稀缺导致的功能错误问题

Method: 两阶段框架：第一阶段用LLM生成Python参考模型定义电路行为，第二阶段基于Python模型创建自动化测试来指导Verilog RTL实现，通过仿真差异迭代修正错误

Result: 实验结果表明该方法在提升生成Verilog代码功能正确性方面显著优于现有最先进方法

Conclusion: AutoVeriFix通过Python辅助的两阶段方法有效解决了LLM生成Verilog代码的功能正确性问题，提高了代码的准确性和可靠性

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
generating software code for high-level programming languages such as Python
and C++. However, their application to hardware description languages, such as
Verilog, is challenging due to the scarcity of high-quality training data.
Current approaches to Verilog code generation using LLMs often focus on
syntactic correctness, resulting in code with functional errors. To address
these challenges, we present AutoVeriFix, a novel Python-assisted two-stage
framework designed to enhance the functional correctness of LLM-generated
Verilog code. In the first stage, LLMs are employed to generate high-level
Python reference models that define the intended circuit behavior. In the
second stage, these Python models facilitate the creation of automated tests
that guide the generation of Verilog RTL implementations. Simulation
discrepancies between the reference model and the Verilog code are iteratively
used to identify and correct errors, thereby improving the functional accuracy
and reliability of the LLM-generated Verilog code. Experimental results
demonstrate that our approach significantly outperforms existing
state-of-the-art methods in improving the functional correctness of generated
Verilog code.

</details>


### [13] [BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter 1.58-bit LLM Inference](https://arxiv.org/abs/2509.08542)
*Wenlun Zhang,Xinyu Li,Shimpei Ando,Kentaro Yoshioka*

Main category: cs.AR

TL;DR: BitROM是首个基于CiROM的LLM加速器，通过1.58位量化、双向ROM阵列和三模式累加器等创新，在65nm工艺下实现20.8 TOPS/W能效和4,967 kB/mm²的比特密度，比现有数字CiROM设计提升10倍面积效率。


<details>
  <summary>Details</summary>
Motivation: 传统CiROM加速器由于LLM参数量巨大而难以扩展，如LLaMA-7B在先进CMOS节点下需要超过1,000 cm²的硅面积，限制了在边缘设备上的实际部署。

Method: 1) 新颖的双向ROM阵列，每个晶体管存储两个三元权重；2) 针对三元权重计算优化的三模式本地累加器；3) 集成解码-刷新eDRAM支持片上KV缓存管理；4) 集成LoRA适配器支持跨下游任务的高效迁移学习。

Result: 在65nm CMOS工艺下实现20.8 TOPS/W的能效和4,967 kB/mm²的比特密度，面积效率比现有数字CiROM设计提升10倍，DR eDRAM减少43.6%的外部DRAM访问。

Conclusion: BitROM通过协同设计和多项创新，成功克服了CiROM加速器在LLM上的可扩展性限制，为边缘应用中的高效LLM推理提供了实用解决方案。

Abstract: Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy
efficiency for CNNs by eliminating runtime weight updates. However, their
scalability to Large Language Models (LLMs) is fundamentally constrained by
their vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA
series - demands more than 1,000 cm2 of silicon area even in advanced CMOS
nodes. This paper presents BitROM, the first CiROM-based accelerator that
overcomes this limitation through co-design with BitNet's 1.58-bit quantization
model, enabling practical and efficient LLM inference at the edge. BitROM
introduces three key innovations: 1) a novel Bidirectional ROM Array that
stores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator
optimized for ternary-weight computations; and 3) an integrated Decode-Refresh
(DR) eDRAM that supports on-die KV-cache management, significantly reducing
external memory access during decoding. In addition, BitROM integrates
LoRA-based adapters to enable efficient transfer learning across various
downstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit
density of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over
prior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%
reduction in external DRAM access, further enhancing deployment efficiency for
LLMs in edge applications.

</details>
