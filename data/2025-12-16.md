<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 17]
- [cs.AR](#cs.AR) [Total: 9]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Accelerating Sparse Matrix-Matrix Multiplication on GPUs with Processing Near HBMs](https://arxiv.org/abs/2512.12036)
*Shiju Li,Younghoon Min,Hane Yie,Hoshik Kim,Soohong Ahn,Joonseop Sim,Chul-Ho Lee,Jongryool Kim*

Main category: cs.DC

TL;DR: 本文提出了一种基于哈希的多阶段SpGEMM GPU实现和间接内存访问加速技术，通过硬件-软件协同设计显著提升了稀疏矩阵乘法的性能，在多种图分析任务和GNN训练中取得了显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 稀疏通用矩阵乘法(SpGEMM)是科学计算和数据分析中的基础操作，但通常受限于不规则的内存访问模式。现有GPU实现难以高效处理复杂的应用特定工作负载，需要新的优化方法。

Method: 提出了基于哈希的多阶段SpGEMM GPU实现和间接内存访问加速(AIA)技术，这是一种新颖的定制近内存处理方法。采用硬件-软件协同设计框架，专门针对GPU HBM进行优化。

Result: 在图分析应用中，AIA相比纯软件实现减少17.3%时间；相比cuSPARSE，图收缩减少76.5%时间，马尔可夫聚类减少58.4%时间。在GNN训练中，混合方法相比纯软件实现平均加速1.43倍，相比cuSPARSE加速1.95倍，在大规模数据集上最高可达4.18倍加速。

Conclusion: 提出的硬件-软件协同设计框架显著提升了SpGEMM在GPU上的性能，特别是在处理复杂应用特定工作负载时表现出色，为图分析和GNN训练等应用提供了实用的高性能解决方案。

Abstract: Sparse General Matrix-Matrix Multiplication (SpGEMM) is a fundamental operation in numerous scientific computing and data analytics applications, often bottlenecked by irregular memory access patterns. This paper presents Hash based Multi-phase SpGEMM on GPU and the Acceleration of Indirect Memory Access (AIA) technique, a novel custom near-memory processing approach to optimizing SpGEMM on GPU HBM. Our hardware-software co-designed framework for SpGEMM demonstrates significant performance improvements over state-of-the-art methods, particularly in handling complex, application-specific workloads. We evaluate our approach on various graph workloads, including graph contraction, Markov clustering, and Graph Neural Networks (GNNs), showcasing its practical applicability. For graph analytics applications, AIA demonstrates up to 17.3% time reduction from the software-only implementation, while achieving time reduction of 76.5% for Graph Contraction and 58.4% for Markov Clustering compared to cuSPARSE. For GNN training applications with structured global pruning, our hybrid approach delivers an average of 1.43x speedup over software-only implementation across six benchmark datasets and three architectures (GCN, GIN, GraphSAGE), and shows 1.95x speedup for GNN workloads when compared to cuSPARSE, with up to 4.18x gains on large-scale datasets.

</details>


### [2] [Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates](https://arxiv.org/abs/2512.12295)
*Wenjun Yu,Sitian Chen,Cheng Chen,Amelie Chi Zhou*

Main category: cs.DC

TL;DR: LiveUpdate通过在推理节点上部署低秩适配训练器，消除跨集群同步开销，实现深度学习推荐模型的在线更新，在保证低延迟的同时提升模型新鲜度和准确性。


<details>
  <summary>Details</summary>
Motivation: 生产环境中的DLRMs面临新鲜度与准确性的权衡问题，因为跨训练/推理集群同步海量嵌入表参数会导致数分钟的延迟，降低推荐质量和收入。同时观察到推理节点CPU利用率低（峰值≤20%）且嵌入表梯度具有内在低秩结构。

Method: 提出LiveUpdate系统，将低秩适配训练器与推理节点共置，消除跨集群同步。采用动态秩适配通过奇异值监控控制内存开销（<2%嵌入表大小），以及NUMA感知资源调度配合硬件强制QoS消除更新对推理的干扰。

Result: 相比基线delta-update方法，LiveUpdate将更新成本降低2倍，在1小时窗口内实现更高准确性，P99延迟影响小于20ms，内存开销控制在嵌入表的2%以内，准确率比最先进的delta-update方法提升0.04%到0.24%。

Conclusion: LiveUpdate通过将空闲推理资源转化为新鲜度引擎，实现了DLRMs的在线模型更新，在降低同步成本的同时提高了推荐准确性，解决了生产环境中模型新鲜度与准确性的核心矛盾。

Abstract: Deep Learning Recommendation Models (DLRMs) underpin personalized services but face a critical freshness-accuracy tradeoff due to massive parameter synchronization overheads. Production DLRMs deploy decoupled training/inference clusters, where synchronizing petabyte-scale embedding tables (EMTs) causes multi-minute staleness, degrading recommendation quality and revenue. We observe that (1) inference nodes exhibit sustained CPU underutilization (peak <= 20%), and (2) EMT gradients possess intrinsic low-rank structure, enabling compact update representation. We present LiveUpdate, a system that eliminates inter-cluster synchronization by colocating Low-Rank Adaptation (LoRA) trainers within inference nodes. LiveUpdate addresses two core challenges: (1) dynamic rank adaptation via singular value monitoring to constrain memory overhead (<2% of EMTs), and (2) NUMA-aware resource scheduling with hardware-enforced QoS to eliminate update inference contention (P99 latency impact <20ms). Evaluations show LiveUpdate reduces update costs by 2x versus delta-update baselines while achieving higher accuracy within 1-hour windows. By transforming idle inference resources into freshness engines, LiveUpdate delivers online model updates while outperforming state-of-the-art delta-update methods by 0.04% to 0.24% in accuracy.

</details>


### [3] [A Conflict-Aware Resource Management Framework for the Computing Continuum](https://arxiv.org/abs/2512.12299)
*Vlad Popescu-Vifor,Ilir Murturi,Praveen Kumar Donta,Schahram Dustdar*

Main category: cs.DC

TL;DR: 提出基于深度强化学习的自适应冲突解决框架，用于计算连续体中的资源编排，解决异构设备环境下的资源冲突问题


<details>
  <summary>Details</summary>
Motivation: 计算连续体（边缘、雾、云）中设备异构性和去中心化需求增加，导致资源编排面临新挑战。代理决策可能导致持续冲突循环、资源利用效率低下和服务性能下降

Method: 提出基于深度强化学习（DRL）的自适应冲突解决框架，集成DRL模型，基于实时性能反馈和历史状态信息调解资源冲突，在Kubernetes测试平台上进行原型实现和验证

Result: 框架在动态场景中实现了高效资源重新分配和自适应学习，展示了方法可行性和架构弹性，为计算连续体中的冲突感知编排提供了可扩展且弹性的解决方案

Conclusion: 该DRL框架能够有效解决计算连续体中的资源冲突问题，提高资源利用效率和服务性能，为异构分布式环境中的资源编排提供了创新解决方案

Abstract: The increasing device heterogeneity and decentralization requirements in the computing continuum (i.e., spanning edge, fog, and cloud) introduce new challenges in resource orchestration. In such environments, agents are often responsible for optimizing resource usage across deployed services. However, agent decisions can lead to persistent conflict loops, inefficient resource utilization, and degraded service performance. To overcome such challenges, we propose a novel framework for adaptive conflict resolution in resource-oriented orchestration using a Deep Reinforcement Learning (DRL) approach. The framework enables handling resource conflicts across deployments and integrates a DRL model trained to mediate such conflicts based on real-time performance feedback and historical state information. The framework has been prototyped and validated on a Kubernetes-based testbed, illustrating its methodological feasibility and architectural resilience. Preliminary results show that the framework achieves efficient resource reallocation and adaptive learning in dynamic scenarios, thus providing a scalable and resilient solution for conflict-aware orchestration in the computing continuum.

</details>


### [4] [Reputation-Based Leader Election under Partial Synchrony: Towards a Protocol-Independent Abstraction with Enhanced Guarantees](https://arxiv.org/abs/2512.12409)
*Xuyang Liu,Zijian Zhang,Zhen Li,Jiahang Sun,Jiamou Liu,Peng Jiang*

Main category: cs.DC

TL;DR: 本文提出了一种协议无关的领导者选举抽象框架SWLE，用于部分同步的拜占庭容错系统，通过基于共识行为的信誉评分动态调整领导者提名，显著提升性能并降低拜占庭领导者频率。


<details>
  <summary>Details</summary>
Motivation: 现有基于信誉的领导者选举框架存在协议特定证明、适用范围窄或网络稳定后恢复无界等问题，缺乏通用解决方案。需要一种协议无关的抽象框架来形式化领导者选举的正确性和有效性保证。

Method: 首先提出协议无关的抽象框架，形式化领导者选举的正确性和有效性属性。在此基础上设计滑动窗口领导者选举(SWLE)机制，通过基于共识行为的信誉评分动态调整领导者提名，实施拜占庭成本放大。

Result: SWLE在基础协议上引入最小额外开销，满足所有抽象属性并提供优越有效性。在16服务器跨4个中国北方区域的部署中，相比最先进方案，吞吐量提升4.2倍，延迟降低75%，拜占庭领导者频率降低27%，同时在无故障场景下保持效率。

Conclusion: SWLE为部分同步拜占庭容错系统提供了通用、有效的领导者选举解决方案，解决了现有方法的局限性，显著提升了系统性能和拜占庭容错能力。

Abstract: Leader election serves a well-defined role in leader-based Byzantine Fault Tolerant (BFT) protocols. Existing reputation-based leader election frameworks for partially synchronous BFTs suffer from either protocol-specific proofs, narrow applicability, or unbounded recovery after network stabilization, leaving an open problem. This paper presents a novel protocol-independent abstraction formalizing generic correctness properties and effectiveness guarantees for leader election under partial synchrony, enabling protocol-independent analysis and design. Building on this, we design the Sliding Window Leader Election (SWLE) mechanism. SWLE dynamically adjusts leader nominations via consensus-behavior-based reputation scores, enforcing Byzantine-cost amplification. We demonstrate SWLE introduces minimal extra overhead to the base protocol and prove it satisfies all abstraction properties and provides superior effectiveness. We show, with a 16-server deployment across 4 different regions in northern China, SWLE achieves up to 4.2x higher throughput, 75% lower latency and 27% Byzantine leader frequency compared to the state-of-the-art solution under common Byzantine faults, while maintaining efficiency in fault-free scenarios.

</details>


### [5] [HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments](https://arxiv.org/abs/2512.12476)
*Yongjun He,Shuai Zhang,Jiading Gai,Xiyuan Zhang,Boran Han,Bernie Wang,Huzefa Rangwala,George Karypis*

Main category: cs.DC

TL;DR: HetRL是一个用于异构GPU环境中高效RL训练的系统，通过多级搜索框架和连续减半算法优化调度，相比现有系统平均提升3.17倍吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模扩大和GPU更新频繁，需要利用跨区域未充分利用的中端或旧代GPU来缓解单一区域内高端GPU短缺问题。然而，在异构计算资源上实现高性能的LLM强化学习训练面临挑战，因为工作流涉及多个模型和任务，具有复杂的计算和数据依赖关系。

Method: HetRL将异构环境中的RL训练调度建模为约束联合优化问题，提出新颖的调度算法：(1) 使用多级搜索框架分解复杂搜索空间；(2) 通过连续减半分配搜索预算。

Result: 在消耗20,000 GPU小时的广泛评估中，HetRL在各种工作负载和设置下，相比最先进系统实现了最高9.17倍、平均3.17倍的吞吐量提升。

Conclusion: HetRL系统能够有效解决异构GPU环境中LLM强化学习训练的调度挑战，显著提升训练效率，充分利用异构计算资源。

Abstract: As large language models (LLMs) continue to scale and new GPUs are released even more frequently, there is an increasing demand for LLM post-training in heterogeneous environments to fully leverage underutilized mid-range or previous-generation GPUs across regions and alleviate the shortage of homogeneous high-end GPUs within a single region. However, achieving high-performance reinforcement learning (RL) training for LLMs on such computing resources remains challenging because the workflow involves multiple models and tasks with complex computation and data dependencies. In this paper, we present HetRL, a distributed system for efficient RL training in infrastructures with heterogeneous GPUs and networks. HetRL formulates the scheduling of RL training in heterogeneous environments as a constrained joint optimization problem and introduces a novel scheduling algorithm that (1) decomposes the complex search space with a multi-level search framework; and (2) allocates the search budget via successive halving. Our extensive evaluation, consuming 20,000 GPU-hours, shows that HetRL delivers up to 9.17x the throughput of state-of-the-art systems, and 3.17x on average, under various workloads and settings.

</details>


### [6] [Strategic Server Deployment under Uncertainty in Mobile Edge Computing](https://arxiv.org/abs/2512.12532)
*Duc A. Tran,Dung Truong,Duy Le*

Main category: cs.DC

TL;DR: 本文提出了一种基于随机双层优化的移动边缘计算服务器部署方法，通过子模函数近似和贪心算法解决服务器选择和用户分配问题，在不确定的工作负载和服务器容量下实现计算效率和通信效率的优化。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算中服务器部署是一个基础任务，需要在不确定的工作负载和服务器容量环境下，同时优化计算效率（最大化边缘处理的工作负载）和通信效率（最小化用户单元与服务器间的通信成本）。

Method: 将问题建模为随机双层优化问题，通过子模函数近似目标函数，利用最先进的子模最大化贪心算法有效求解服务器选择和用户分配问题。

Result: 使用真实世界数据评估表明，所提算法优于其他方法，改进幅度最高可达55%。

Conclusion: 该研究为移动边缘计算中的服务器部署问题提供了一种有效的随机优化方法，能够在不确定环境下持续实现计算和通信效率目标。

Abstract: Server deployment is a fundamental task in mobile edge computing: where to place the edge servers and what user cells to assign to them. To make this decision is context-specific, but common goals are 1) computing efficiency: maximize the amount of workload processed by the edge, and 2) communication efficiency: minimize the communication cost between the cells and their assigned servers. We focus on practical scenarios where the user workload in each cell is unknown and time-varying, and so are the effective capacities of the servers. Our research problem is to choose a subset of candidate servers and assign them to the user cells such that the above goals are sustainably achieved under the above uncertainties. We formulate this problem as a stochastic bilevel optimization, which is strongly NP-hard and unseen in the literature. By approximating the objective function with submodular functions, we can utilize state-of-the-art greedy algorithms for submodular maximization to effectively solve our problem. We evaluate the proposed algorithm using real-world data, showing its superiority to alternative methods; the improvement can be as high as 55%

</details>


### [7] [Design in Tiles: Automating GEMM Deployment on Tile-Based Many-PE Accelerators](https://arxiv.org/abs/2512.13638)
*Aofeng Shen,Chi Zhang,Yakup Budanaz,Alexandru Calotoiu,Torsten Hoefler,Luca Benini*

Main category: cs.DC

TL;DR: 提出DiT框架，自动化优化基于tile的PE加速器上的GEMM部署，相比NVIDIA GH200专家调优库获得1.2-2.0倍加速


<details>
  <summary>Details</summary>
Motivation: 基于tile的多PE加速器在GEMM上能获得有竞争力的性能，但极难编程，因为其最优软件映射与硬件设计深度耦合，难以手动部署

Method: 提出"Design in Tiles (DiT)"框架，将部署工具链与可配置的可执行模型连接起来，自动化优化加速器上的GEMM部署

Result: 在大型加速配置（32x32 tiles，1979 TFLOPS@FP8，4 TB/s带宽）上评估，相比NVIDIA GH200专家调优的GEMM库，获得更高的PE利用率，在多样矩阵形状上实现1.2-2.0倍加速

Conclusion: DiT框架能有效自动化基于tile的PE加速器的GEMM部署，显著提升性能并简化编程难度

Abstract: Tile-based many-Processing Element (PE) accelerators can achieve competitive performance on General Matrix Multiplication (GEMM), but they are extremely hard to program, as their optimal software mapping is deeply coupled with hardware design which is unwieldy to manual deployment. We propose "Design in Tiles (DiT)", an automated framework connecting a deployment toolchain with a configurable executable model for these accelerators. For evaluation, we apply our framework to GEMM targeting a large acceleration configuration (e.g., 32x32 tiles, 1979 TFLOPS@FP8, 4 TB/s Bandwidth) comparable to an NVIDIA GH200. We achieve higher PE utilization than GH200 with its expert-tuned GEMM libraries, achieving 1.2-2.0x speedup across diverse matrix shapes.

</details>


### [8] [Ethical Risk Analysis of L2 Rollups](https://arxiv.org/abs/2512.12732)
*Georgy Ishmaev,Emmanuelle Anceaume,Davide Frey,François Taïani*

Main category: cs.DC

TL;DR: 该研究将伦理风险分析应用于Layer 2 rollup架构，通过构建决策权限与风险暴露的分类框架，结合对129个项目的横截面分析和2022-2025年事件集，发现L2组件控制安排中的伦理风险普遍存在，并提出了基于伦理的缓解策略建议。


<details>
  <summary>Details</summary>
Motivation: Layer 2 rollup虽然提高了吞吐量和降低了费用，但通过运营者自由裁量权和信息不对称可能重新引入风险。研究旨在探究哪些运营者和治理设计会产生伦理上有问题的用户风险。

Method: 1. 将伦理风险分析适配到rollup架构；2. 构建基于角色的决策权限与风险暴露分类法；3. 结合两个经验信号：对L2BEAT上129个项目的横截面快照分析，以及2022-2025年手工整理的事件集；4. 分析影响用户资金风险的机制，包括升级时机和退出窗口、提议者活跃度和白名单、强制包含可用性以及数据可用性选择。

Result: 研究发现L2组件控制安排中的伦理风险普遍存在：约86%的项目存在无退出窗口的即时升级，约50%的项目存在可能冻结提款的提议者控制。报告的事件主要集中在排序器活跃度和包含问题上，与这些依赖关系一致。

Conclusion: 研究将这些发现转化为基于伦理的缓解策略建议，包括技术组件和治理机制，为Layer 2 rollup的伦理风险管理和治理设计提供了实证基础和理论框架。

Abstract: Layer 2 rollups improve throughput and fees, but can reintroduce risk through operator discretion and information asymmetry. We ask which operator and governance designs produce ethically problematic user risk. We adapt Ethical Risk Analysis to rollup architectures, build a role-based taxonomy of decision authority and exposure, and pair the framework with two empirical signals, a cross sectional snapshot of 129 projects from L2BEAT and a hand curated incident set covering 2022 to 2025. We analyze mechanisms that affect risks to users funds, including upgrade timing and exit windows, proposer liveness and whitelisting, forced inclusion usability, and data availability choices. We find that ethical hazards rooted in L2 components control arrangements are widespread: instant upgrades without exit windows appear in about 86 percent of projects, and proposer controls that can freeze withdrawals in about 50 percent. Reported incidents concentrate in sequencer liveness and inclusion, consistent with these dependencies. We translate these findings into ethically grounded suggestions on mitigation strategies including technical components and governance mechanisms.

</details>


### [9] [Fine-Grained Energy Prediction For Parallellized LLM Inference With PIE-P](https://arxiv.org/abs/2512.12801)
*Anurag Dutt,Young Won Choi,Avirup Sil,Anshul Gandhi,Aruna Balasubramanian,Niranjan Balasubramanian*

Main category: cs.DC

TL;DR: PIE-P：一个用于多GPU并行推理的细粒度能耗预测框架，解决现有方法仅适用于单GPU环境的问题


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，能耗成本成为关键问题。现有能耗测量方法存在局限：硬件监控不可及，软件工具不准确，现有预测技术仅限于单GPU环境，无法适用于现代多GPU并行推理场景

Method: 开发了可扩展的预测框架，通过精确采样、细粒度建模GPU间通信、仔细计算并行化开销来解决多GPU环境中的非确定性通信、额外通信开销和通信/同步阶段能耗隔离困难等问题

Result: PIE-P在各种并行策略（张量、流水线、数据并行）下都能提供准确且细粒度的能耗预测，显著优于基线方法

Conclusion: PIE-P填补了多GPU并行推理能耗预测的空白，为现代LLM推理的能耗优化提供了有效的预测工具

Abstract: With the widespread adoption of Large Language Models (LLMs), energy costs of running LLMs is quickly becoming a critical concern. However, precisely measuring the energy consumption of LLMs is often infeasible because hardware-based power monitors are not always accessible and software-based energy measurement tools are not accurate. While various prediction techniques have been developed to estimate LLM energy consumption, these approaches are limited to single-GPU environments and thus are not applicable to modern LLM inference which is typically parallelized across multiple GPUs. In this work, we remedy this gap and introduce PIE-P, a fine-grained energy prediction framework for multi-GPU inference, including tensor, pipeline, and data parallelism. Predicting the energy under parallelized inference is complicated by the non-determinism in inter-GPU communication, additional communication overheads, and difficulties in isolating energy during the communication/synchronization phase. We develop a scalable prediction framework that addresses these issues via precise sampling, fine-grained modeling of inter-GPU communication, and careful accounting of parallelization overhead. Our evaluation results show that PIE-P yields accurate and fine-grained energy predictions across parallelism strategies, significantly outperforming baselines.

</details>


### [10] [PROSERVE: Unified Multi-Priority Request Scheduling for LLM Serving](https://arxiv.org/abs/2512.12928)
*Weizhe Huang,Tao Peng,Tongxuan Liu,Donghe Jin,Xianzhe Dong,Ke Zhang*

Main category: cs.DC

TL;DR: PROSERVE是一个面向多优先级LLM服务的两层次调度框架，通过动态批处理和服务增益导向的路由，在满足不同优先级请求SLO的同时最大化系统服务增益。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务调度器无法同时优化SLO达成率和客户端优先级。实际应用中，不同优先级的请求（如业务关键功能）具有不同的业务价值，需要差异化的性能保证。

Method: 提出PROSERVE两层次调度框架：1) 引擎层的SlideBatching动态调整批处理形成和请求排序，平衡截止时间优先和密度优先策略；2) 服务层的GoRouting基于增益导向和能力感知进行分布式实例调度，为未来高优先级或长请求预留容量。

Result: 在四个开源数据集和真实工业追踪上的评估显示，PROSERVE相比最先进基线提升系统增益达35%，SLO达成率提升达52%。

Conclusion: PROSERVE通过形式化多优先级请求调度为服务增益最大化问题，提供了一种有效的解决方案，能够更好地满足实际LLM服务部署中不同优先级客户的需求。

Abstract: The widespread deployment of large language models (LLMs) for interactive applications necessitates serving systems that can handle thousands of concurrent requests with diverse Service Level Objective (SLO) requirements. A critical yet often overlooked dimension in this context is the inherent priority difference among clients; for instance, business-critical functions demand higher performance guarantees, as fulfilling such requests yields significantly greater business value. However, existing LLM serving schedulers fail to jointly optimize for both SLO attainment and client-level priorities.
  To bridge this gap, we first \textit{formalize multi-priority request scheduling as a service gain maximization problem}, where satisfying latency requirements for requests of different priorities contributes varying levels of gain. We then propose PROSERVE, a unified two-tier scheduling framework designed to maximize overall service gain. At the engine level, SlideBatching dynamically adapts batch formation and request ordering under varying load conditions, employing a sliding boundary mechanism to balance deadline-first and density-first strategies. At the service level, GoRouting performs gain-oriented and capability-aware dispatching across distributed instances, proactively reserving capacity for future high-priority or long requests. Extensive evaluation across four open-source datasets and a real-world industrial trace demonstrates that \systemname{} consistently outperforms state-of-the-art baselines, improving system gain by up to 35% and boosting SLO attainment by up to 52%.

</details>


### [11] [FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection](https://arxiv.org/abs/2512.12949)
*Ziyu Huang,Yangjie Zhou,Zihan Liu,Xinhao Luo,Yijia Diao,Minyi Guo,Jidong Zhai,Yu Feng,Chen Zhang,Anbang Wu,Jingwen Leng*

Main category: cs.DC

TL;DR: FlashFuser：首个利用现代GPU核间连接（DSM）进行内核融合的编译器框架，通过分布式内存层次优化减少内存访问，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 随着计算吞吐量增长超过内存带宽改进，深度学习工作负载变得内存受限。现有编译器框架的融合策略仅限于本地暂存内存，当中间结果超过容量（如FFN）时融合失败。现代GPU（如NVIDIA H100）引入了分布式共享内存（DSM）机制，提供了更大、高带宽、低延迟的片上内存池，但这一硬件潜力尚未被软件框架充分利用。

Method: FlashFuser通过三个核心贡献将融合技术扩展到DSM领域：1）提出基于DSM的通信抽象，形式化集群数据交换模式（如reduce、shuffle、multiply）；2）引入数据流分析器，将循环调度、资源映射和分块选择推广到分布式内存层次；3）集成统一搜索引擎，使用分析成本建模和DSM感知剪枝策略发现最优执行计划。

Result: 在NVIDIA H100 GPU上的评估显示，FlashFuser减少58%的内存访问，相比高度优化的库实现3.3倍内核加速，相比最先进编译器实现4.1倍加速，带来1.24倍的端到端加速。

Conclusion: FlashFuser成功利用现代GPU的核间连接机制进行内核融合，填补了硬件潜力与软件框架之间的空白，显著提升了内存受限深度学习工作负载的性能。

Abstract: The scaling of computation throughput continues to outpace improvements in memory bandwidth, making many deep learning workloads memory-bound. Kernel fusion is a key technique to alleviate this problem, but the fusion strategies of existing compilers and frameworks are limited to using local scratchpad memory. When the intermediate results exceed the limited capacity (such as FFN), the fusion fails. Although modern GPUs (like the NVIDIA H100) now incorporate an inter-core connection mechanism known as Distributed Shared Memory(DSM)--providing a larger, high-bandwidth, and low-latency on-chip memory pool--this hardware potential has yet to be exploited by software frameworks. To bridge this gap, we present FlashFuser, the first compiler framework to utilize inter-core connection for kernel fusion on modern GPUs. FlashFuser extends established fusion techniques to the DSM domain through three core contributions. First, we propose a powerful DSM-based communication abstraction that formalizes complex cluster-based data exchange patterns, such as reduce, shuffle and multiply. Second, we introduce a dataflow analyzer that generalizes loop scheduling, resource mapping, and tile selection to the distributed memory hierarchy; it determines the optimal execution order and tile sizes by quantifying data movement across memory levels. Finally, FlashFuser integrates these components into a unified search engine that employs analytical cost modeling and DSM-aware pruning strategies to efficiently discover the optimal execution plan. Our evaluation on an NVIDIA H100 GPU shows that FlashFuser reduces memory access by 58% and delivers kernel speedups of 3.3x against highly-tuned libraries and 4.1x against state-of-the-art compilers, resulting in a 1.24x end-to-end speedup.

</details>


### [12] [Toward Self-Healing Networks-on-Chip: RL-Driven Routing in 2D Torus Architectures](https://arxiv.org/abs/2512.13096)
*Mohammad Walid Charrwi,Zaid Hussain*

Main category: cs.DC

TL;DR: 该论文研究了在2D环面片上网络(NoC)中节点故障条件下的自适应最小路由，比较了基于强化学习(RL)的策略与自适应路由基线。RL方法在吞吐量和故障恢复能力方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在节点故障条件下，基于强化学习的自适应路由策略能否比传统自适应路由方法在2D环面NoC中提供更好的性能。环面拓扑因其低直径和高连接性而被采用，但在节点故障情况下需要有效的路由策略来维持网络性能。

Method: 研究方法包括：1) 将每个路由器建模为强化学习代理，学习基于网络状态转发数据包；2) 使用固定最小路径并在故障周围简单重路由的自适应方案作为基线；3) 在模拟中实现两种方法，注入最多50个均匀随机分布的节点故障；4) 测量关键指标：吞吐量vs负载、包交付率vs故障密度、故障自适应评分vs故障密度。

Result: 实验结果显示：1) RL方法在高负载下实现显著更高的吞吐量(约20-30%增益)；2) RL在故障增加时保持更高的可靠性，包交付率在约30-40个故障前保持在90%以上，而自适应方案在同一故障点下降到约70%；3) RL路由器每周期传递更多数据包，通过利用路径多样性适应故障，而自适应方案在故障累积时性能急剧下降；4) 故障自适应评分同样支持RL路由。

Conclusion: 结论是基于强化学习的自适应路由在环面NoC中展示了吞吐量和故障恢复能力的明显优势。RL方法能够更好地适应网络故障，维持更高的网络性能和可靠性，为故障容忍的片上网络设计提供了有前景的方向。

Abstract: We investigate adaptive minimal routing in 2D torus networks on chip NoCs under node fault conditions comparing a reinforcement learning RL based strategy to an adaptive routing baseline A torus topology is used for its low diameter high connectivity properties The RL approach models each router as an agent that learns to forward packets based on network state while the adaptive scheme uses fixed minimal paths with simple rerouting around faults We implement both methods in simulation injecting up to 50 node faults uniformly at random Key metrics are measured 1 throughput vs offered load at fault density 02 2 packet delivery ratio PDR vs fault density and 3 a fault adaptive score FT vs fault density Experimental results show the RL method achieves significantly higher throughput at high load approximately 2030 gain and maintains higher reliability under increasing faults The RL router delivers more packets per cycle and adapts to faults by exploiting path diversity whereas the adaptive scheme degrades sharply as faults accumulate In particular the RL approach preserves end to end connectivity longer PDR remains above 90 until approximately 3040 faults while adaptive PDR drops to approximately 70 at the same point The fault adaptive score likewise favors RL routing Thus RL based adaptive routing demonstrates clear advantages in throughput and fault resilience for torus NoCs

</details>


### [13] [SPARS: A Reinforcement Learning-Enabled Simulator for Power Management in HPC Job Scheduling](https://arxiv.org/abs/2512.13268)
*Muhammad Alfian Amrizal,Raka Satya Prasasta,Santana Yuda Pradata,Kadek Gemilang Santiyuda,Reza Pulungan,Hiroyuki Takizawa*

Main category: cs.DC

TL;DR: SPARS是一个基于强化学习的HPC集群功耗管理模拟器，通过集成作业调度和节点电源状态管理，在节能和性能之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 高性能计算集群能耗巨大，空闲节点造成大量能源浪费。关闭未使用节点可以缓解问题，但不当的开关时机会导致延迟和性能下降，需要在节能和性能之间找到平衡。

Method: SPARS采用离散事件模拟框架，集成作业调度和节点电源状态管理。支持传统调度策略（如FCFS、EASY Backfilling）以及使用强化学习代理动态决定节点开关时机的增强变体。系统使用JSON配置工作负载和平台参数，记录能耗、等待时间等指标，并提供甘特图可视化。

Result: 相比基于Batsim的框架，SPARS提供轻量级事件处理和一致的模拟结果，易于实验复现和扩展。其模块化设计允许轻松集成新的调度启发式算法或学习算法。

Conclusion: SPARS为研究人员和从业者提供了一个灵活、可复现、可扩展的平台，能够系统评估功耗感知调度策略，探索能效与性能之间的权衡，加速可持续HPC运营的发展。

Abstract: High-performance computing (HPC) clusters consume enormous amounts of energy, with idle nodes as a major source of waste. Powering down unused nodes can mitigate this problem, but poorly timed transitions introduce long delays and reduce overall performance. To address this trade-off, we present SPARS, a reinforcement learning-enabled simulator for power management in HPC job scheduling. SPARS integrates job scheduling and node power state management within a discrete-event simulation framework. It supports traditional scheduling policies such as First Come First Served and EASY Backfilling, along with enhanced variants that employ reinforcement learning agents to dynamically decide when nodes should be powered on or off. Users can configure workloads and platforms in JSON format, specifying job arrivals, execution times, node power models, and transition delays. The simulator records comprehensive metrics-including energy usage, wasted power, job waiting times, and node utilization-and provides Gantt chart visualizations to analyze scheduling dynamics and power transitions. Unlike widely used Batsim-based frameworks that rely on heavy inter-process communication, SPARS provides lightweight event handling and consistent simulation results, making experiments easier to reproduce and extend. Its modular design allows new scheduling heuristics or learning algorithms to be integrated with minimal effort. By providing a flexible, reproducible, and extensible platform, SPARS enables researchers and practitioners to systematically evaluate power-aware scheduling strategies, explore the trade-offs between energy efficiency and performance, and accelerate the development of sustainable HPC operations.

</details>


### [14] [Temporal parallelisation of continuous-time maximum-a-posteriori trajectory estimation](https://arxiv.org/abs/2512.13319)
*Hassan Razavi,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.DC

TL;DR: 提出一种并行时间方法，用于计算部分观测随机微分方程的连续时间MAP轨迹估计，通过并行架构提升计算速度。


<details>
  <summary>Details</summary>
Motivation: 传统连续时间MAP轨迹估计算法通常是顺序执行的，无法充分利用现代并行计算架构（如GPU）的优势，限制了计算速度的提升。

Method: 将MAP估计问题重新表述为基于Onsager-Machlup泛函的连续时间最优控制问题，然后采用并行时间求解方法，利用并行关联扫描算法实现并行化。在线性高斯情况下得到并行Kalman-Bucy滤波器和并行连续时间Rauch-Tung-Striebel平滑器，并通过泰勒展开扩展到非线性模型。

Result: GPU实验表明，在线性和非线性模型上，所提框架在保持顺序算法精度的同时，实现了显著的计算加速。

Conclusion: 该方法成功地将连续时间MAP轨迹估计问题并行化，为利用现代并行计算硬件加速状态估计提供了有效框架。

Abstract: This paper proposes a parallel-in-time method for computing continuous-time maximum-a-posteriori (MAP) trajectory estimates of the states of partially observed stochastic differential equations (SDEs), with the goal of improving computational speed on parallel architectures. The MAP estimation problem is reformulated as a continuous-time optimal control problem based on the Onsager-Machlup functional. This reformulation enables the use of a previously proposed parallel-in-time solution for optimal control problems, which we adapt to the current problem. The structure of the resulting optimal control problem admits a parallel solution based on parallel associative scan algorithms. In the linear Gaussian special case, it yields a parallel Kalman-Bucy filter and a parallel continuous-time Rauch-Tung-Striebel smoother. These linear computational methods are further extended to nonlinear continuous-time state-space models through Taylor expansions. We also present the corresponding parallel two-filter smoother. The graphics processing unit (GPU) experiments on linear and nonlinear models demonstrate that the proposed framework achieves a significant speedup in computations while maintaining the accuracy of sequential algorithms.

</details>


### [15] [SIGMA: An AI-Empowered Training Stack on Early-Life Hardware](https://arxiv.org/abs/2512.13488)
*Lei Qu,Lianhai Ren,Peng Cheng,Rui Gao,Ruizhe Wang,Tianyu Chen,Xiao Liu,Xingjian Zhang,Yeyun Gong,Yifan Xiong,Yucheng Ding,Yuting Jiang,Zhenghao Lin,Zhongxin Guo,Ziyue Yang*

Main category: cs.DC

TL;DR: SIGMA是一个开源训练栈，旨在提高早期AI硬件上大规模分布式训练的可靠性、稳定性和效率，通过LUCIA训练平台(LTP)和LUCIA训练框架(LTF)成功训练了200B参数的MoE模型。


<details>
  <summary>Details</summary>
Motivation: 解决早期AI加速器在大规模训练中面临的三大挑战：系统频繁中断和未定义故障模式影响可靠性；数值错误和训练不稳定性威胁正确性和收敛性；并行优化复杂性与不可预测的本地噪声降低效率。

Method: 开发了SIGMA开源训练栈，包含LUCIA训练平台(LTP)针对早期AI加速器集群优化的系统，以及LUCIA训练框架(LTF)用于大规模模型训练。LTP专注于提高集群可靠性和利用率，LTF在此基础上训练大规模模型。

Result: LTP实现了94.45%的有效集群加速器利用率，显著减少了节点回收和作业恢复时间。LTF使用2,048个AI加速器成功训练了200B参数的SIGMA-MOE模型，实现了21.08%的MFU、最先进的下游精度，在75天训练中仅遇到一次稳定性事件。

Conclusion: SIGMA不仅解决了大规模训练的关键挑战，还为AI基础设施和平台创新设立了新基准，提供了强大且经济高效的替代方案，显著推进了AI能力和可扩展性。

Abstract: An increasing variety of AI accelerators is being considered for large-scale training. However, enabling large-scale training on early-life AI accelerators faces three core challenges: frequent system disruptions and undefined failure modes that undermine reliability; numerical errors and training instabilities that threaten correctness and convergence; and the complexity of parallelism optimization combined with unpredictable local noise that degrades efficiency. To address these challenges, SIGMA is an open-source training stack designed to improve the reliability, stability, and efficiency of large-scale distributed training on early-life AI hardware. The core of this initiative is the LUCIA TRAINING PLATFORM (LTP), the system optimized for clusters with early-life AI accelerators. Since its launch in March 2025, LTP has significantly enhanced training reliability and operational productivity. Over the past five months, it has achieved an impressive 94.45% effective cluster accelerator utilization, while also substantially reducing node recycling and job-recovery times. Building on the foundation of LTP, the LUCIA TRAINING FRAMEWORK (LTF) successfully trained SIGMA-MOE, a 200B MoE model, using 2,048 AI accelerators. This effort delivered remarkable stability and efficiency outcomes, achieving 21.08% MFU, state-of-the-art downstream accuracy, and encountering only one stability incident over a 75-day period. Together, these advances establish SIGMA, which not only tackles the critical challenges of large-scale training but also establishes a new benchmark for AI infrastructure and platform innovation, offering a robust, cost-effective alternative to prevailing established accelerator stacks and significantly advancing AI capabilities and scalability. The source code of SIGMA is available at https://github.com/microsoft/LuciaTrainingPlatform.

</details>


### [16] [Janus: Disaggregating Attention and Experts for Scalable MoE Inference](https://arxiv.org/abs/2512.13525)
*Zhexiang Zhang,Ye Wang,Xiangyu Wang,Yumiao Zhao,Jingzhe Jiang,Qizhen Weng,Shaohuai Shi,Yin Chen,Minchen Yu*

Main category: cs.DC

TL;DR: Janus是一个可扩展的MoE推理系统，通过将注意力模块和专家模块分离到不同的GPU子集群上，实现独立管理和扩展，显著提升推理效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有MoE推理系统通常将整个模型作为单一整体部署，对注意力模块和专家模块采用统一的资源配置，忽略了它们不同的需求，导致可扩展性有限和资源效率低下。

Method: 1. 将注意力模块和专家模块分离到独立的GPU子集群；2. 采用自适应两阶段通信方案，利用节点内和节点间带宽层次结构；3. 引入轻量级调度器作为GPU内核，平衡GPU间的激活专家数量；4. 执行细粒度资源管理，动态调整专家放置并独立扩展资源和MoE资源。

Result: Janus相比现有最先进系统，实现了高达3.9倍的每GPU吞吐量提升，同时满足每个token的延迟要求。

Conclusion: Janus通过解耦注意力模块和专家模块，结合自适应通信、轻量级调度和细粒度资源管理，为大规模MoE模型推理提供了高效、可扩展的解决方案。

Abstract: Large Mixture-of-Experts (MoE) model inference is challenging due to high resource demands and dynamic workloads. Existing solutions often deploy the entire model as a single monolithic unit, which applies a unified resource configuration to both attention and expert modules despite their different requirements, leading to limited scalability and resource inefficiency. In this paper, we propose Janus, a scalable MoE inference system that disaggregates attention and experts on separate GPU sub-clusters, enabling each module to be managed and scaled independently. Janus incorporates three key designs for efficient, disaggregated MoE inference. First, it proposes an adaptive two-phase communication scheme that exploits intra- and inter-node bandwidth hierarchies for low-latency data exchange. Second, motivated by the memory-bound nature of MoE modules, Janus introduces a lightweight scheduler and implements it as a GPU kernel to balance the number of activated experts across GPUs at minimal overhead, thereby reducing inference latency. Third, Janus performs fine-grained resource management to dynamically adjust expert placement and independently scale attention and MoE resources to improve overall efficiency. Evaluation shows Janus achieves up to 3.9 higher perGPU throughput than state-of-the-art systems while meeting per-token latency requirements.

</details>


### [17] [astroCAMP: A Community Benchmark and Co-Design Framework for Sustainable SKA-Scale Radio Imaging](https://arxiv.org/abs/2512.13591)
*Denisa-Andreea Constantinescu,Rubén Rodríguez Álvarez,Jacques Morin,Etienne Orliac,Mickaël Dardaillon,Sunrise Wang,Hugo Miomandre,Miguel Peón-Quirós,Jean-François Nezan,David Atienza*

Main category: cs.DC

TL;DR: astroCAMP框架为SKA射电望远镜项目提供硬件-软件协同设计，通过统一度量标准、标准化数据集和多目标优化，解决当前成像管线性能低下（仅4-14%硬件峰值）和能效差的问题，以在功率限制下最大化科学产出。


<details>
  <summary>Details</summary>
Motivation: SKA项目面临严重性能瓶颈：当前射电干涉成像管线仅能利用4-14%的硬件峰值性能，存在内存和I/O瓶颈，导致能效低下、运营成本和碳排放高。同时缺乏标准化度量和保真度容差，阻碍了硬件-软件协同设计和质量-效率权衡的探索。

Method: 提出astroCAMP框架，包含三个核心组件：1) 统一的扩展度量套件，涵盖科学保真度、计算性能、可持续性和生命周期经济学；2) 标准化的SKA代表性数据集和参考输出，支持跨CPU、GPU和新兴加速器的可重复基准测试；3) 多目标协同设计公式，将科学质量约束与时间、能耗、碳排放和总拥有成本联系起来。

Result: 发布了数据集、基准测试结果和可重复性工具包，在AMD EPYC 9334处理器和NVIDIA H100 GPU上评估了WSClean和IDG的协同设计度量。展示了astroCAMP在异构CPU-FPGA设计空间探索中的应用潜力，能够识别SKA规模成像部署的帕累托最优操作点。

Conclusion: astroCAMP为下一代成像管线和可持续HPC架构的协同设计提供了框架，能够在SKA的运营和环境限制下最大化科学回报。呼吁SKA社区定义可量化的保真度度量和阈值，以加速SKA规模成像的原则性优化。

Abstract: The Square Kilometre Array (SKA) project will operate one of the world's largest continuous scientific data systems, sustaining petascale imaging under strict power caps. Yet, current radio-interferometric pipelines utilize only a small fraction of hardware peak performance, typically 4-14%, due to memory and I/O bottlenecks, resulting in poor energy efficiency and high operational and carbon costs. Progress is further limited by the absence of standardised metrics and fidelity tolerances, preventing principled hardware-software co-design and rigorous exploration of quality-efficiency trade-offs. We introduce astroCAMP, a framework for guiding the co-design of next-generation imaging pipelines and sustainable HPC architectures that maximise scientific return within SKA's operational and environmental limits. astroCAMP provides: (1) a unified, extensible metric suite covering scientific fidelity, computational performance, sustainability, and lifecycle economics; (2) standardised SKA-representative datasets and reference outputs enabling reproducible benchmarking across CPUs, GPUs, and emerging accelerators; and (3) a multi-objective co-design formulation linking scientific-quality constraints to time-, energy-, carbon-to-solution, and total cost of ownership. We release datasets, benchmarking results, and a reproducibility kit, and evaluate co-design metrics for WSClean and IDG on an AMD EPYC 9334 processor and an NVIDIA H100 GPU. Further, we illustrate the use of astroCAMP for heterogeneous CPU-FPGA design-space exploration, and its potential to facilitate the identification of Pareto-optimal operating points for SKA-scale imaging deployments. Last, we make a call to the SKA community to define quantifiable fidelity metrics and thresholds to accelerate principled optimisation for SKA-scale imaging.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [FSL-HDnn: A 40 nm Few-shot On-Device Learning Accelerator with Integrated Feature Extraction and Hyperdimensional Computing](https://arxiv.org/abs/2512.11826)
*Weihong Xu,Chang Eun Song,Haichao Yang,Leo Liu,Meng-Fan Chang,Carlos H. Diaz,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: FSL-HDnn是一种能效优化的加速器，通过权重聚类特征提取器和超维度计算分类器实现端到端少样本学习，在40nm工艺下达到6mJ/图像的训练能效。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限边缘设备上设备端学习（ODL）的基本挑战，包括计算复杂度高、延迟大和能效低的问题。

Method: 采用两个协同模块：1）基于权重聚类的参数高效特征提取器降低计算复杂度；2）基于超维度计算（HDC）的少样本学习分类器消除梯度反向传播，实现单次训练。还提出分支特征提取的早退机制和批处理单次训练优化策略。

Result: 在40nm CMOS工艺下，芯片在10-way 5-shot少样本学习任务上实现6mJ/图像的训练能效和28图像/秒的端到端训练吞吐量。端到端训练延迟比现有ODL芯片降低2-20.9倍。

Conclusion: FSL-HDnn通过创新的权重聚类特征提取和HDC分类器架构，显著提升了边缘设备上少样本学习的能效和延迟性能，为资源受限环境下的设备端学习提供了有效解决方案。

Abstract: This paper introduces FSL-HDnn, an energy-efficient accelerator that implements the end-to-end pipeline of feature extraction and on-device few-shot learning (FSL). The accelerator addresses fundamental challenges of on-device learning (ODL) for resource-constrained edge applications through two synergistic modules: a parameter-efficient feature extractor employing weight clustering and an FSL classifier based on hyperdimensional computing (HDC). The feature extractor exploits the weight clustering mechanism to reduce computational complexity, while the HDC-based FSL classifier eliminates gradient-based back propagation operations, enabling single-pass training with substantially reduced latency. Additionally, FSL-HDnn enables low-latency ODL and inference via two proposed optimization strategies, including an early-exit mechanism with branch feature extraction and batched single-pass training that improves hardware utilization. Measurement results demonstrate that our chip fabricated in a 40 nm CMOS process delivers superior training energy efficiency of 6 mJ/image and end-to-end training throughput of 28 images/s on a 10-way 5-shot FSL task. The end-to-end training latency is also reduced by 2x to 20.9x compared to state-of-the-art ODL chips.

</details>


### [19] [DreamRAM: A Fine-Grained Configurable Design Space Modeling Tool for Custom 3D Die-Stacked DRAM](https://arxiv.org/abs/2512.12106)
*Victor Cai,Jennifer Zhou,Haebin Do,David Brooks,Gu-Yeon Wei*

Main category: cs.AR

TL;DR: DreamRAM是一个可配置的3D堆叠DRAM建模工具，用于定制化内存架构设计，支持带宽、容量、能耗、延迟和面积等多维度参数配置，能够探索传统固定设计无法满足的多样化应用需求。


<details>
  <summary>Details</summary>
Motivation: 3D堆叠DRAM已成为高性能计算、图形和机器学习等应用的关键技术，但不同应用对功耗、性能和面积的需求差异很大，固定的商品化DRAM设计无法满足这些多样化需求。堆叠技术通过3D集成和扩展的总芯片面积为DRAM设计提供了巨大的设计空间。

Method: DreamRAM是一个可配置的建模工具，在MAT、子阵列、bank和bank间级别暴露细粒度设计参数，包括扩展文献中的部分页面和子阵列并行性方案。工具通过分析建模线间距、宽度、长度、电容和缩放参数来捕捉物理布局和布线设计选择的性能权衡。路由感知能力使DreamRAM能够建模自定义的MAT级路由方案Dataline-Over-MAT（DLOMAT）。

Result: DreamRAM已针对行业HBM3和HBM2E设计进行校准和验证。在该工具丰富的设计空间中，识别出相比基线设计分别实现66%更高带宽、100%更高容量以及每比特功耗和能耗降低45%的设计方案，分别在等带宽、等容量和等功耗基础上进行评估。

Conclusion: DreamRAM为定制化3D堆叠DRAM设计提供了一个强大的建模工具，能够探索传统固定设计无法满足的多样化应用需求，通过细粒度参数配置和路由感知建模，实现了显著的性能提升和功耗优化。

Abstract: 3D die-stacked DRAM has emerged as a key technology for delivering high bandwidth and high density for applications such as high-performance computing, graphics, and machine learning. However, different applications place diverse and sometimes diverging demands on power, performance, and area that cannot be universally satisfied with fixed commodity DRAM designs. Die stacking creates the opportunity for a large DRAM design space through 3D integration and expanded total die area. To open and navigate this expansive design space of customized memory architectures that cater to application-specific needs, we introduce DreamRAM, a configurable bandwidth, capacity, energy, latency, and area modeling tool for custom 3D die-stacked DRAM designs. DreamRAM exposes fine-grained design customization parameters at the MAT, subarray, bank, and inter-bank levels, including extensions of partial page and subarray parallelism proposals found in the literature, to open a large previously-unexplored design space. DreamRAM analytically models wire pitch, width, length, capacitance, and scaling parameters to capture the performance tradeoffs of physical layout and routing design choices. Routing awareness enables DreamRAM to model a custom MAT-level routing scheme, Dataline-Over-MAT (DLOMAT), to facilitate better bandwidth tradeoffs. DreamRAM is calibrated and validated against published industry HBM3 and HBM2E designs. Within DreamRAM's rich design space, we identify designs that achieve each of 66% higher bandwidth, 100% higher capacity, and 45% lower power and energy per bit compared to the baseline design, each on an iso-bandwidth, iso-capacity, and iso-power basis.

</details>


### [20] [HaShiFlex: A High-Throughput Hardened Shifter DNN Accelerator with Fine-Tuning Flexibility](https://arxiv.org/abs/2512.12847)
*Jonathan Herbst,Michael Pellauer,Sherief Reda*

Main category: cs.AR

TL;DR: 本文提出了一种高吞吐量神经网络加速器，通过将大部分网络层直接嵌入硬件、采用Po2量化权重、以及可编程最终层，实现了20-67倍于GPU的推理吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 针对边缘连续感知或数据中心部署等模型参数稳定的应用场景，需要高吞吐量、高能效的神经网络推理加速方案，同时保持一定的灵活性以适应可能的微调需求。

Method: 1) 将大部分网络层直接硬件化，减少数据传输和内存使用；2) 采用幂次二(Po2)量化权重，用加法替代乘法；3) 保留小型神经处理单元作为可编程最终分类层；4) 基于MobileNetV2在7nm ASIC流程中实现。

Result: 在MobileNetV2上，相比完全可编程GPU，推理吞吐量提升20倍（121万图像/秒），保留微调灵活性；若无需部署后微调，吞吐量提升可达67倍（400万图像/秒）。

Conclusion: 该硬件化加速器设计在保持一定灵活性的同时，显著提升了神经网络推理的吞吐量和能效，特别适用于模型参数稳定的应用场景，展示了硬件化与可编程性结合的优越性。

Abstract: We introduce a high-throughput neural network accelerator that embeds most network layers directly in hardware, minimizing data transfer and memory usage while preserving a degree of flexibility via a small neural processing unit for the final classification layer. By leveraging power-of-two (Po2) quantization for weights, we replace multiplications with simple rewiring, effectively reducing each convolution to a series of additions. This streamlined approach offers high-throughput, energy-efficient processing, making it highly suitable for applications where model parameters remain stable, such as continuous sensing tasks at the edge or large-scale data center deployments. Furthermore, by including a strategically chosen reprogrammable final layer, our design achieves high throughput without sacrificing fine-tuning capabilities. We implement this accelerator in a 7nm ASIC flow using MobileNetV2 as a baseline and report throughput, area, accuracy, and sensitivity to quantization and pruning - demonstrating both the advantages and potential trade-offs of the proposed architecture. We find that for MobileNetV2, we can improve inference throughput by 20x over fully programmable GPUs, processing 1.21 million images per second through a full forward pass while retaining fine-tuning flexibility. If absolutely no post-deployment fine tuning is required, this advantage increases to 67x at 4 million images per second.

</details>


### [21] [KANELÉ: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation](https://arxiv.org/abs/2512.12850)
*Duc Hoang,Aarush Gupta,Philip Harris*

Main category: cs.AR

TL;DR: KANELÉ框架首次系统化地将KAN网络部署到FPGA上，通过量化剪枝协同优化，实现高达2700倍加速和显著资源节省，在符号/物理公式任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: FPGA上需要低延迟、资源高效的神经网络推理以满足实时性和低功耗需求。传统LUT网络虽有优势，但KAN网络因其可学习一维样条激活函数，天然适合离散化和高效LUT映射，为FPGA部署提供了新机会。

Method: 提出KANELÉ框架，首次系统化设计KAN在FPGA上的实现流程。通过协同优化训练、量化和剪枝技术，将KAN网络转化为紧凑、高吞吐、低延迟的FPGA架构，充分利用KAN的样条激活函数特性进行高效LUT映射。

Result: 相比现有KAN-on-FPGA方法，实现高达2700倍加速和数量级资源节省。在广泛基准测试中，KANELÉ匹配或超越其他LUT架构，尤其在符号/物理公式任务上表现突出，并能平衡FPGA硬件资源使用。框架还成功扩展到实时、高能效控制系统。

Conclusion: KANELÉ框架成功将KAN网络优势与FPGA硬件特性结合，为低延迟、资源高效的神经网络推理提供了创新解决方案，特别适合实时控制和符号/物理公式处理任务，展现了KAN在边缘计算中的巨大潜力。

Abstract: Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANELÉ, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANELÉ matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems.

</details>


### [22] [SliceMoE: Bit-Sliced Expert Caching under Miss-Rate Constraints for Efficient MoE Inference](https://arxiv.org/abs/2512.12990)
*Yuseon Choi,Sangjin Kim,Jungjun Oh,Gwangtae Park,Byeongcheol Kim,Hoi-Jun Yoo*

Main category: cs.AR

TL;DR: SliceMoE：一种面向设备端部署的节能MoE推理框架，通过动态位切片缓存和预测性缓存预热技术，在保持精度的同时显著降低能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: MoE模型通过条件计算实现高效扩展，但其庞大的参数量和昂贵的专家卸载使得设备端部署面临挑战。现有的加速技术如预取或专家聚类往往会增加能耗或降低专家多样性。

Method: 提出SliceMoE框架：1）动态位切片缓存（DBSC）：以切片粒度缓存专家，按需分配精度以扩展有效专家容量；2）校准自由非对称套娃量化（AMAT）：基于截断的量化方案，支持混合精度专家而无需内存复制；3）预测性缓存预热（PCW）：在预填充阶段重塑缓存内容以减少早期解码冷缺失。

Result: 在DeepSeek-V2-Lite和Qwen1.5-MoE-A2.7B上评估，SliceMoE将解码阶段能耗分别降低2.37倍和2.85倍，解码延迟分别提升1.81倍和1.64倍，同时保持接近高比特精度。

Conclusion: 切片级缓存技术能够实现高效的设备端MoE部署，为大规模MoE模型在资源受限设备上的应用提供了可行方案。

Abstract: MoE models offer efficient scaling through conditional computation, but their large parameter size and expensive expert offloading make on-device deployment challenging. Existing acceleration techniques such as prefetching or expert clustering often increase energy usage or reduce expert diversity. We present SliceMoE, an energy-efficient MoE inference framework for miss-rate-constrained deployment. SliceMoE introduces Dynamic Bit-Sliced Caching (DBSC), which caches experts at slice-level granularity and assigns precision on demand to expand effective expert capacity. To support mixed-precision experts without memory duplication, we propose Calibration-Free Asymmetric Matryoshka Quantization (AMAT), a truncation-based scheme that maintains compatibility between low-bit and high-bit slices. We further introduce Predictive Cache Warmup (PCW) to reduce early-decode cold misses by reshaping cache contents during prefill. Evaluated on DeepSeek-V2-Lite and Qwen1.5-MoE-A2.7B, SliceMoE reduces decode-stage energy consumption by up to 2.37x and 2.85x, respectively, and improves decode latency by up to 1.81x and 1.64x, while preserving near-high-bit accuracy. These results demonstrate that slice-level caching enables an efficient on-device MoE deployment.

</details>


### [23] [An Optimal Alignment-Driven Iterative Closed-Loop Convergence Framework for High-Performance Ultra-Large Scale Layout Pattern Clustering](https://arxiv.org/abs/2512.13133)
*Shuo Liu*

Main category: cs.AR

TL;DR: 提出基于最优对齐的迭代闭环收敛框架，解决VLSI布局模式聚类中的计算复杂度、对齐精度和速度-质量权衡问题，在EDA竞赛中实现93.4%压缩比和100倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着VLSI技术缩放，布局模式爆炸式增长成为DFM应用（如OPC）的关键瓶颈。现有聚类方法面临计算复杂度高（O(N²)比较）、中心对齐离散采样次优、速度-质量权衡困难等问题。

Method: 1) 提出混合高性能算法解决对齐模糊性：基于FFT的相位相关方法用于余弦相似度约束，鲁棒几何最小-最大策略用于边缘位移约束；2) 将聚类建模为集合覆盖问题，在粗到细迭代细化循环中使用基于信息量的懒惰贪婪启发式算法确保收敛；3) 多阶段剪枝机制过滤99%以上冗余计算。

Result: 在2025年中国研究生EDA精英挑战赛基准测试中，实现相对于原始输入93.4%的压缩比，比官方基线加速超过100倍，能在数秒内处理数万个模式，在77支队伍中获得第一名。

Conclusion: 该方法通过最优对齐驱动和迭代闭环收敛框架，成功解决了NP难的布局聚类问题，在可扩展性和精度之间实现了最佳平衡，证明了其在VLSI DFM应用中的优越性。

Abstract: With the aggressive scaling of VLSI technology, the explosion of layout patterns creates a critical bottleneck for DFM applications like OPC. Pattern clustering is essential to reduce data complexity, yet existing methods struggle with computational prohibitiveness ($O(N^2)$ comparisons), sub-optimal discrete sampling for center alignment, and difficult speed-quality trade-offs. To address these, we propose an Optimal Alignment-Driven Iterative Closed-Loop Convergence Framework. First, to resolve alignment ambiguity, we introduce a hybrid suite of high-performance algorithms: an FFT-based Phase Correlation method for cosine similarity constraints, and a Robust Geometric Min-Max strategy for edge displacement constraints that analytically solves for the global optimum. Second, we model clustering as a Set Cover Problem (SCP) using a Surprisal-Based Lazy Greedy heuristic within a coarse-to-fine iterative refinement loop to ensure convergence. Additionally, a multi-stage pruning mechanism filters over 99% of redundant computations. Experimental results on the 2025 China Postgraduate EDA Elite Challenge benchmark demonstrate a 93.4% compression ratio relative to raw inputs and an over 100x speedup compared to the official baseline, effectively handling tens of thousands of patterns in seconds. Securing First Place among 77 teams, this approach proves its superiority in solving the NP-Hard layout clustering problem with an optimal balance of scalability and precision.

</details>


### [24] [Striking the Balance: GEMM Performance Optimization Across Generations of Ryzen AI NPUs](https://arxiv.org/abs/2512.13282)
*Endri Taka,Andre Roesti,Joseph Melber,Pranathi Vasireddy,Kristof Denolf,Diana Marculescu*

Main category: cs.AR

TL;DR: 本文提出了一种系统化方法，用于优化AMD Ryzen AI NPU（XDNA和XDNA2）上的GEMM计算，实现了int8精度下最高6.76 TOPS（XDNA）和38.05 TOPS（XDNA2）的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习工作负载对计算和内存需求极高，需要针对AMD Ryzen AI XDNA NPU等专用硬件进行优化。优化GEMM算法对于提升深度学习工作负载性能至关重要。

Method: 提出了一种通用的系统化方法来优化两个NPU世代（XDNA和XDNA2）上的GEMM工作负载。实现利用了AMD NPU的独特架构特性，并在系统层面解决了关键性能瓶颈。

Result: 在不同GEMM规模上的端到端性能评估显示：对于8位整数（int8）精度，实现了最高6.76 TOPS（XDNA）和38.05 TOPS（XDNA2）的吞吐量；对于脑浮点数（bf16）精度，实现了最高3.14 TOPS（XDNA）和14.71 TOPS（XDNA2）的吞吐量。

Conclusion: 这项工作为优化Ryzen AI NPU上的GEMM工作负载提供了重要的性能洞察，展示了系统化优化方法在专用硬件上的有效性。

Abstract: The high computational and memory demands of modern deep learning (DL) workloads have led to the development of specialized hardware devices from cloud to edge, such as AMD's Ryzen AI XDNA NPUs. Optimizing general matrix multiplication (GEMM) algorithms for these architectures is critical for improving DL workload performance. To this end, this paper presents a common systematic methodology to optimize GEMM workloads across the two current NPU generations, namely XDNA and XDNA2. Our implementations exploit the unique architectural features of AMD's NPUs and address key performance bottlenecks at the system level. End-to-end performance evaluation across various GEMM sizes demonstrates state-of-the-art throughput of up to 6.76 TOPS (XDNA) and 38.05 TOPS (XDNA2) for 8-bit integer (int8) precision. Similarly, for brain floating-point (bf16) precision, our GEMM implementations attain up to 3.14 TOPS (XDNA) and 14.71 TOPS (XDNA2). This work provides significant insights into key performance aspects of optimizing GEMM workloads on Ryzen AI NPUs.

</details>


### [25] [Reproducibility and Standardization in gem5 Resources v25.0](https://arxiv.org/abs/2512.13479)
*Kunal Pai,Harshil Patel,Erin Le,Noah Krim,Mahyar Samani,Bobby R. Bruce,Jason Lowe-Power*

Main category: cs.AR

TL;DR: 改进gem5仿真器和gem5 Resources资源库，通过标准化磁盘镜像创建、重构退出事件系统、引入并行仿真功能，解决计算机架构研究中可重复性、定制化和工作流协调问题。


<details>
  <summary>Details</summary>
Motivation: 计算机架构研究中基于仿真的可重复性面临三大挑战：1）创建自定义磁盘镜像复杂且缺乏跨ISA标准化；2）gem5有限的访客-主机通信功能限制了动态控制和监控；3）多工作负载仿真需要易错的外部脚本协调。

Method: 1）使用Packer标准化x86、ARM和RISC-V的磁盘镜像创建，提供带预标注基准套件的验证基础镜像；2）重构退出事件系统为基于类的模型，引入超调用增强访客-主机通信；3）实现Suites和MultiSim支持从gem5配置脚本直接进行并行全系统仿真。

Result: 提供了12个新磁盘镜像、6个新内核和超过200个跨三种ISA的工作负载。新功能降低了设置复杂性，提供了可扩展的验证资源，显著改善了可重复性和标准化。

Conclusion: 通过标准化磁盘镜像创建、增强访客-主机通信功能、以及内置并行仿真支持，显著提升了gem5仿真器的可用性和研究可重复性，为计算机架构社区提供了更强大、更易用的研究工具。

Abstract: Reproducibility in simulation-based computer architecture research requires coordinating artifacts like disk images, kernels, and benchmarks, but existing workflows are inconsistent. We improve gem5, an open-source simulator with over 1600 forks, and gem5 Resources, a centralized repository of over 2000 pre-packaged artifacts, to address these issues. While gem5 Resources enables artifact sharing, researchers still face challenges. Creating custom disk images is complex and time-consuming, with no standardized process across ISAs, making it difficult to extend and share images. gem5 provides limited guest-host communication features through a set of predefined exit events that restrict researchers' ability to dynamically control and monitor simulations. Lastly, running simulations with multiple workloads requires researchers to write custom external scripts to coordinate multiple gem5 simulations which creates error-prone and hard-to-reproduce workflows. To overcome this, we introduce several features in gem5 and gem5 Resources. We standardize disk-image creation across x86, ARM, and RISC-V using Packer, and provide validated base images with pre-annotated benchmark suites (NPB, GAPBS). We provide 12 new disk images, 6 new kernels, and over 200 workloads across three ISAs. We refactor the exit event system to a class-based model and introduce hypercalls for enhanced guest-host communication that allows researchers to define custom behavior for their exit events. We also provide a utility to remotely monitor simulations and the gem5-bridge driver for user-space m5 operations. Additionally, we implemented Suites and MultiSim to enable parallel full-system simulations from gem5 configuration scripts, eliminating the need for external scripting. These features reduce setup complexity and provide extensible, validated resources that improve reproducibility and standardization.

</details>


### [26] [Lyra: A Hardware-Accelerated RISC-V Verification Framework with Generative Model-Based Processor Fuzzing](https://arxiv.org/abs/2512.13686)
*Juncheng Huo,Yunfan Gao,Xinxin Liu,Sa Wang,Yungang Bao,Xitong Gao,Kan Shi*

Main category: cs.AR

TL;DR: Lyra：一个异构RISC-V验证框架，结合硬件加速和ISA感知生成模型，显著提升验证覆盖率和速度


<details>
  <summary>Details</summary>
Motivation: 处理器设计日益复杂，但验证仍受限于缓慢的软件仿真和低质量的随机测试激励。现有软件模糊测试方法依赖语义盲随机突变，难以生成能探索复杂行为的高质量激励，导致覆盖率收敛缓慢和验证成本过高。

Method: Lyra采用异构验证框架：1) 在FPGA SoC上并行执行被测设计和参考模型，实现高吞吐量差分检查和硬件级覆盖率收集；2) 训练领域专用生成模型LyraGen，该模型具有内在语义感知能力，能生成高质量、语义丰富的指令序列。

Result: Lyra相比最先进的软件模糊测试器：1) 实现高达1.27倍的覆盖率提升；2) 端到端验证加速107倍至3343倍；3) 始终表现出更低的收敛难度。

Conclusion: Lyra通过结合硬件加速验证和ISA感知生成模型，有效解决了处理器验证中的覆盖率和速度瓶颈问题，显著降低了验证成本。

Abstract: As processor designs grow more complex, verification remains bottlenecked by slow software simulation and low-quality random test stimuli. Recent research has applied software fuzzers to hardware verification, but these rely on semantically blind random mutations that may generate shallow, low-quality stimuli unable to explore complex behaviors. These limitations result in slow coverage convergence and prohibitively high verification costs. In this paper, we present Lyra, a heterogeneous RISC-V verification framework that addresses both challenges by pairing hardware-accelerated verification with an ISA-aware generative model. Lyra executes the DUT and reference model concurrently on an FPGA SoC, enabling high-throughput differential checking and hardware-level coverage collection. Instead of creating verification stimuli randomly or through simple mutations, we train a domain-specialized generative model, LyraGen, with inherent semantic awareness to generate high-quality, semantically rich instruction sequences. Empirical results show Lyra achieves up to $1.27\times$ higher coverage and accelerates end-to-end verification by up to $107\times$ to $3343\times$ compared to state-of-the-art software fuzzers, while consistently demonstrating lower convergence difficulty.

</details>
