{"id": "2512.09412", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.09412", "abs": "https://arxiv.org/abs/2512.09412", "authors": ["Patrick Bahr"], "title": "Simple Modal Types for Functional Reactive Programming", "comment": null, "summary": "Functional reactive programming (FRP) is a declarative programming paradigm for implementing reactive programs at a high level of abstraction. It applies functional programming principles to construct and manipulate time-varying values, also known as signals. However, for this programming paradigm to work in practice, an FRP language must ensure that programs are causal, productive, and free from space leaks. Over the past fifteen years, several modal type systems to enforce these operational properties have been developed.\n  We present a new FRP language with a significantly simplified modal type system that imposes fewer restrictions than previous modal FRP languages while still guaranteeing the central operational properties of causality, productivity, and absence of space leaks. The key enabling idea is to alter the semantics of signals so that the type system can safely allow more programs to type-check, which also makes the language more expressive. With this new semantics, signals are modelled as mutable references whose mutability is tightly controlled by the 'later' type modality. This disciplined form of mutability also enables more efficient in-place updates of signals, all while preserving a functional programming style.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u51fd\u6570\u54cd\u5e94\u5f0f\u7f16\u7a0b\u8bed\u8a00\uff0c\u91c7\u7528\u7b80\u5316\u7684\u6a21\u6001\u7c7b\u578b\u7cfb\u7edf\uff0c\u5728\u4fdd\u8bc1\u56e0\u679c\u6027\u3001\u751f\u4ea7\u6027\u548c\u65e0\u7a7a\u95f4\u6cc4\u6f0f\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u4e86\u5bf9\u7a0b\u5e8f\u7684\u9650\u5236\uff0c\u63d0\u9ad8\u4e86\u8868\u8fbe\u80fd\u529b\u548c\u8fd0\u884c\u6548\u7387\u3002", "motivation": "\u51fd\u6570\u54cd\u5e94\u5f0f\u7f16\u7a0b\uff08FRP\uff09\u867d\u7136\u63d0\u4f9b\u4e86\u9ad8\u5c42\u6b21\u7684\u58f0\u660e\u5f0f\u7f16\u7a0b\u8303\u5f0f\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u786e\u4fdd\u7a0b\u5e8f\u7684\u56e0\u679c\u6027\u3001\u751f\u4ea7\u6027\u548c\u65e0\u7a7a\u95f4\u6cc4\u6f0f\u3002\u73b0\u6709\u7684\u6a21\u6001\u7c7b\u578b\u7cfb\u7edf\u867d\u7136\u80fd\u4fdd\u8bc1\u8fd9\u4e9b\u6027\u8d28\uff0c\u4f46\u5f80\u5f80\u5bf9\u7a0b\u5e8f\u65bd\u52a0\u4e86\u8fc7\u591a\u9650\u5236\uff0c\u5f71\u54cd\u4e86\u8bed\u8a00\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6539\u53d8\u4fe1\u53f7\u7684\u8bed\u4e49\uff0c\u5c06\u4fe1\u53f7\u5efa\u6a21\u4e3a\u53d7\"later\"\u6a21\u6001\u7c7b\u578b\u4e25\u683c\u63a7\u5236\u7684\u53ef\u53d8\u5f15\u7528\u3002\u8fd9\u79cd\u65b0\u7684\u8bed\u4e49\u5141\u8bb8\u7c7b\u578b\u7cfb\u7edf\u5b89\u5168\u5730\u63a5\u53d7\u66f4\u591a\u7a0b\u5e8f\uff0c\u540c\u65f6\u901a\u8fc7\u53d7\u63a7\u7684\u53ef\u53d8\u6027\u652f\u6301\u66f4\u9ad8\u6548\u7684\u539f\u4f4d\u66f4\u65b0\u3002", "result": "\u65b0\u8bed\u8a00\u5728\u4fdd\u6301\u51fd\u6570\u5f0f\u7f16\u7a0b\u98ce\u683c\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4ee5\u5f80\u6a21\u6001FRP\u8bed\u8a00\u66f4\u5c11\u7684\u9650\u5236\u3001\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u80fd\u4fdd\u8bc1\u56e0\u679c\u6027\u3001\u751f\u4ea7\u6027\u548c\u65e0\u7a7a\u95f4\u6cc4\u6f0f\u7684\u6838\u5fc3\u64cd\u4f5c\u6027\u8d28\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u4fe1\u53f7\u7684\u8bed\u4e49\uff0c\u53ef\u4ee5\u6784\u5efa\u4e00\u4e2a\u65e2\u4fdd\u6301FRP\u6838\u5fc3\u4fdd\u8bc1\u53c8\u5177\u6709\u66f4\u597d\u5b9e\u7528\u6027\u7684\u8bed\u8a00\uff0c\u5728\u7c7b\u578b\u7cfb\u7edf\u7684\u4e25\u8c28\u6027\u548c\u7f16\u7a0b\u7075\u6d3b\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2512.09304", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.09304", "abs": "https://arxiv.org/abs/2512.09304", "authors": ["Siyuan Ma", "Jiajun Hu", "Jeeho Ryoo", "Aman Arora", "Lizy Kurian John"], "title": "RACAM: Enhancing DRAM with Reuse-Aware Computation and Automated Mapping for ML Inference", "comment": null, "summary": "In-DRAM Processing-In-Memory (DRAM-PIM) has emerged as a promising approach to accelerate memory-intensive workloads by mitigating data transfer overhead between DRAM and the host processor. Bit-serial DRAM-PIM architectures, further enhance efficiency by supporting runtime variable data precision, which is critical for emerging workloads, such as large language model (LLM) inference. However, existing works still have major limitations: lack of data reuse, significant amounts of redundant data transfer, and insufficient support for workload mapping. To address these issues, we propose RACAM, the first in-DRAM bit-serial architecture which uses dedicated locality buffers, bit-serial PEs, popcount reduction units and broadcast units to enable data reuse and alleviate redundant data transfers. Furthermore, a workload mapping mechanism is proposed to fully explore the massive parallelism of DRAM architecture and identify the best mapping scheme of a given workload. We evaluate RACAM against GPUs and the state-of-the-art, in-DRAM PIM system, Proteus, across end-to-end LLM inferences. RACAM achieves 9x to 102x speedup over GPUs and 233x higher performance per mm2 compared to Proteus in case of GPT3.", "AI": {"tldr": "RACAM\u662f\u4e00\u79cd\u65b0\u578b\u7684DRAM\u5185\u5b58\u5185\u5904\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u4e13\u7528\u5c40\u90e8\u6027\u7f13\u51b2\u533a\u3001\u4f4d\u4e32\u884c\u5904\u7406\u5355\u5143\u3001popcount\u5f52\u7ea6\u5355\u5143\u548c\u5e7f\u64ad\u5355\u5143\uff0c\u89e3\u51b3\u4e86\u73b0\u6709DRAM-PIM\u67b6\u6784\u4e2d\u6570\u636e\u91cd\u7528\u4e0d\u8db3\u3001\u5197\u4f59\u6570\u636e\u4f20\u8f93\u548c\u8d1f\u8f7d\u6620\u5c04\u652f\u6301\u4e0d\u591f\u7684\u95ee\u9898\uff0c\u5728LLM\u63a8\u7406\u4e2d\u76f8\u6bd4GPU\u548c\u73b0\u6709\u6700\u4f73\u65b9\u6848\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u73b0\u6709DRAM-PIM\u67b6\u6784\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u7f3a\u4e4f\u6570\u636e\u91cd\u7528\u3001\u5927\u91cf\u5197\u4f59\u6570\u636e\u4f20\u8f93\u3001\u4ee5\u53ca\u5bf9\u8d1f\u8f7d\u6620\u5c04\u652f\u6301\u4e0d\u8db3\u3002\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86DRAM-PIM\u5728\u5185\u5b58\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff09\u4e2d\u7684\u6548\u7387\u63d0\u5347\u6f5c\u529b\u3002", "method": "\u63d0\u51faRACAM\u67b6\u6784\uff0c\u5305\u542b\uff1a1) \u4e13\u7528\u5c40\u90e8\u6027\u7f13\u51b2\u533a\u5b9e\u73b0\u6570\u636e\u91cd\u7528\uff1b2) \u4f4d\u4e32\u884c\u5904\u7406\u5355\u5143\u652f\u6301\u8fd0\u884c\u65f6\u53ef\u53d8\u6570\u636e\u7cbe\u5ea6\uff1b3) popcount\u5f52\u7ea6\u5355\u5143\u548c\u5e7f\u64ad\u5355\u5143\u51cf\u5c11\u5197\u4f59\u6570\u636e\u4f20\u8f93\uff1b4) \u8d1f\u8f7d\u6620\u5c04\u673a\u5236\u5145\u5206\u5229\u7528DRAM\u67b6\u6784\u7684\u5927\u89c4\u6a21\u5e76\u884c\u6027\u3002", "result": "\u5728\u7aef\u5230\u7aefLLM\u63a8\u7406\u8bc4\u4f30\u4e2d\uff0cRACAM\u76f8\u6bd4GPU\u5b9e\u73b0\u4e869-102\u500d\u52a0\u901f\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f73DRAM-PIM\u7cfb\u7edfProteus\uff0c\u5728GPT3\u63a8\u7406\u4e2d\u5b9e\u73b0\u4e86\u6bcf\u5e73\u65b9\u6beb\u7c73233\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "RACAM\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u8d1f\u8f7d\u6620\u5c04\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86DRAM-PIM\u67b6\u6784\u7684\u5173\u952e\u74f6\u9888\uff0c\u4e3a\u5185\u5b58\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\uff08\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff09\u63d0\u4f9b\u4e86\u9ad8\u6548\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2512.09277", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.09277", "abs": "https://arxiv.org/abs/2512.09277", "authors": ["Yanpeng Yu", "Haiyue Ma", "Krish Agarwal", "Nicolai Oswald", "Qijing Huang", "Hugo Linsenmaier", "Chunhui Mei", "Ritchie Zhao", "Ritika Borkar", "Bita Darvish Rouhani", "David Nellans", "Ronny Krashinsky", "Anurag Khandelwal"], "title": "Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens", "comment": null, "summary": "Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.\n  We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.", "AI": {"tldr": "METRO\u662f\u4e00\u79cd\u9488\u5bf9\u5185\u5b58\u53d7\u9650\u573a\u666f\u7684MoE\u6a21\u578b\u4e13\u5bb6\u5e76\u884c\u8def\u7531\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861GPU\u4e0a\u7684\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\u800c\u975e\u4ee4\u724c\u6570\u91cf\u6765\u63d0\u5347\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u53ef\u964d\u4f4e\u89e3\u7801\u5ef6\u8fdf11-22%\uff0c\u63d0\u5347\u541e\u5410\u91cf\u6700\u9ad8\u8fbe4.11\u500d\u3002", "motivation": "\u73b0\u6709\u4e13\u5bb6\u5e76\u884c\u65b9\u6cd5\u901a\u8fc7\u5e73\u8861\u5404GPU\u5904\u7406\u7684\u4ee4\u724c\u6570\u91cf\u6765\u89e3\u51b3\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u4f46\u5728\u5185\u5b58\u53d7\u9650\u573a\u666f\uff08\u7279\u522b\u662fMoE\u670d\u52a1\u7684\u89e3\u7801\u9636\u6bb5\uff09\u4e2d\uff0c\u8fd9\u79cd\u5e73\u8861\u53cd\u800c\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u56e0\u4e3a\u4f1a\u589e\u52a0\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\uff0c\u52a0\u5267\u5185\u5b58\u538b\u529b\u3002", "method": "\u63d0\u51faMETRO\uff08Minimum Expert Token ROuting\uff09\u7b97\u6cd5\uff1a1\uff09\u5e73\u8861\u5404GPU\u7684\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\u800c\u975e\u4ee4\u724c\u6570\u91cf\uff1b2\uff09\u8054\u5408\u4f18\u5316\u7b97\u6cd5\u6548\u7387\u5e76\u5229\u7528GPU\u5e76\u884c\u5904\u7406\u80fd\u529b\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u8def\u7531\u8d28\u91cf\uff1b3\uff09\u91c7\u7528\u65b0\u9896\u7684allGather\u65b9\u6848\u6536\u96c6\u5168\u5c40top-k\u4fe1\u606f\uff0c\u76f8\u6bd4\u4f20\u7edfallToAll\u5f00\u9500\u66f4\u5c0f\u3002", "result": "\u5728\u771f\u5b9e\u7cfb\u7edf\uff08vLLM over 8 A100 GPUs\uff09\u548c\u4e13\u6709\u6a21\u62df\u5668\uff088-16 B200 GPUs\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff1a1\uff09\u89e3\u7801\u5ef6\u8fdf\u964d\u4f4e11-22%\uff1b2\uff09Qwen3\u548cDeepSeek-V3\u670d\u52a1\u7684\u603b\u4ee4\u724c\u541e\u5410\u91cf\u63d0\u53473-21%\uff1b3\uff09\u5728\u56fa\u5b9a\u89e3\u7801SLO\u4e0b\uff0c\u89e3\u7801\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53474.11\u500d\u3002", "conclusion": "\u5728\u5185\u5b58\u53d7\u9650\u7684MoE\u670d\u52a1\u573a\u666f\u4e2d\uff0c\u5e73\u8861\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\u800c\u975e\u4ee4\u724c\u6570\u91cf\u662f\u5173\u952e\u4f18\u5316\u65b9\u5411\u3002METRO\u7b97\u6cd5\u901a\u8fc7\u521b\u65b0\u7684\u8def\u7531\u7b56\u7565\u548c\u901a\u4fe1\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e13\u5bb6\u5e76\u884c\u670d\u52a1\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2512.09427", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09427", "abs": "https://arxiv.org/abs/2512.09427", "authors": ["Guoqiang Zou", "Wanyu Wang", "Hao Zheng", "Longxiang Yin", "Yinhe Han"], "title": "ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators", "comment": "10 pages, 5 figures", "summary": "Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.", "AI": {"tldr": "ODMA\u662f\u4e00\u4e2a\u9762\u5411\u968f\u673a\u8bbf\u95ee\u53d7\u9650\u5185\u5b58\uff08RACM\uff09\u52a0\u901f\u5668\u7684\u6309\u9700\u5185\u5b58\u5206\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u957f\u5ea6\u9884\u6d4b\u5668\u3001\u52a8\u6001\u6876\u5206\u533a\u548c\u5927\u6876\u4fdd\u62a4\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347LLM\u670d\u52a1\u7684\u5185\u5b58\u5229\u7528\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5185\u5b58\u7ba1\u7406\u65b9\u6848\u5728\u968f\u673a\u8bbf\u95ee\u5e26\u5bbd\u53d7\u9650\u7684\u52a0\u901f\u5668\uff08\u5982\u57fa\u4e8eLPDDR5\u7684Cambricon MLU370\uff09\u4e0a\u6548\u7387\u4f4e\u4e0b\uff1a\u9759\u6001\u9884\u5206\u914d\u6d6a\u8d39\u5185\u5b58\uff0c\u800c\u7ec6\u7c92\u5ea6\u5206\u9875\uff08\u5982PagedAttention\uff09\u56e0\u9ad8\u968f\u673a\u8bbf\u95ee\u6210\u672c\u800c\u4e0d\u9002\u7528\u3002\u73b0\u6709HBM\u4e2d\u5fc3\u5316\u89e3\u51b3\u65b9\u6848\u672a\u80fd\u5145\u5206\u5229\u7528RACM\u52a0\u901f\u5668\u7684\u7279\u6027\u3002", "method": "ODMA\u6846\u67b6\u7ed3\u5408\u8f7b\u91cf\u7ea7\u957f\u5ea6\u9884\u6d4b\u5668\u548c\u52a8\u6001\u6876\u5206\u533a\u7b56\u7565\uff0c\u901a\u8fc7\u5927\u6876\u4fdd\u62a4\u673a\u5236\u5904\u7406\u5206\u5e03\u6f02\u79fb\u548c\u957f\u5c3e\u8bf7\u6c42\u3002\u8fb9\u754c\u6839\u636e\u5b9e\u65f6\u8ddf\u8e2a\u6570\u636e\u5b9a\u671f\u66f4\u65b0\u4ee5\u6700\u5927\u5316\u5185\u5b58\u5229\u7528\u7387\u3002\u8be5\u65b9\u6848\u4e13\u95e8\u9488\u5bf9RACM\u52a0\u901f\u5668\u7684\u786c\u4ef6\u7279\u6027\u8bbe\u8ba1\u3002", "result": "\u5728Alpaca\u548cGoogle-NQ\u6570\u636e\u96c6\u4e0a\uff0cODMA\u5c06\u9884\u6d4b\u51c6\u786e\u7387\u4ece82.68%\u63d0\u5347\u81f393.36%\u3002\u5728Cambricon MLU370-X4\u4e0a\u670d\u52a1DeepSeek-R1-Distill-Qwen-7B\u6a21\u578b\uff0c\u5185\u5b58\u5229\u7528\u7387\u4ece55.05%\u63d0\u5347\u81f372.45%\uff0cRPS\u548cTPS\u5206\u522b\u6bd4\u9759\u6001\u57fa\u7ebf\u63d0\u9ad829%\u548c27%\u3002", "conclusion": "\u786c\u4ef6\u611f\u77e5\u7684\u5185\u5b58\u5206\u914d\u65b9\u6848\u80fd\u591f\u89e3\u9501RACM\u5e73\u53f0\u4e0a\u9ad8\u6548\u7684LLM\u670d\u52a1\uff0cODMA\u6846\u67b6\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u8bbe\u8ba1\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6848\u5728\u968f\u673a\u8bbf\u95ee\u53d7\u9650\u52a0\u901f\u5668\u4e0a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.09309", "categories": ["cs.DC", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09309", "abs": "https://arxiv.org/abs/2512.09309", "authors": ["Zihao Ding", "Mufeng Zhu", "Zhongze Tang", "Sheng Wei", "Yao Liu"], "title": "A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge", "comment": "16 pages, 7 figures. Published in the Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing (SEC '25), Dec 3-6, 2025, Washington, D.C., USA", "summary": "Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u5206\u5e03\u5f0f\u5378\u8f7d\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u6570\u636e\u5206\u5272\u5230\u591a\u4e2a\u4e91\u670d\u52a1\u5668\uff0c\u9632\u6b62\u5355\u670d\u52a1\u5668\u91cd\u5efa\u5b8c\u6574\u56fe\u50cf\uff0c\u4fdd\u62a4\u9690\u79c1", "motivation": "\u79fb\u52a8\u548c\u53ef\u7a7f\u6234\u8bbe\u5907\u8ba1\u7b97\u80fd\u529b\u6709\u9650\uff0c\u4f46\u4e91\u5378\u8f7d\u4f1a\u5e26\u6765\u4f20\u8f93\u548c\u670d\u52a1\u5668\u7aef\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u9700\u8981\u4fdd\u62a4\u89c6\u89c9\u6570\u636e\u9690\u79c1", "method": "\u4f7f\u7528\u672c\u5730\u53ef\u4fe1\u8fb9\u7f18\u8bbe\u5907\u4f5c\u4e3a\u534f\u8c03\u5668\uff0c\u5c06\u89c6\u89c9\u6570\u636e\u5206\u5272\u6210\u5c0f\u5757\u5206\u53d1\u5230\u591a\u4e2a\u72ec\u7acb\u4e91\u670d\u52a1\u5668\uff0c\u6700\u7ec8\u5408\u5e76\u8ba1\u7b97\u53ea\u5728\u8fb9\u7f18\u8bbe\u5907\u8fdb\u884c", "result": "\u5728SAM\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u4fdd\u6301\u63a5\u8fd1\u57fa\u51c6\u7684\u5206\u5272\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5bb9\u91cd\u5efa\u548c\u7528\u6237\u6570\u636e\u66b4\u9732\u98ce\u9669", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u9690\u79c1\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u4e91\u8fde\u7eed\u4f53\u4e2d\u7684\u89c6\u89c9\u4efb\u52a1"}}
{"id": "2512.09331", "categories": ["cs.DC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.09331", "abs": "https://arxiv.org/abs/2512.09331", "authors": ["Nam Anh Dang", "Ben Landrum", "Ken Birman"], "title": "Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN", "comment": "12 pages, 14 figures, submitted to VLDB 2026", "summary": "Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.", "AI": {"tldr": "BatANN\uff1a\u9996\u4e2a\u5f00\u6e90\u5206\u5e03\u5f0f\u78c1\u76d8\u5411\u91cf\u68c0\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u5355\u5168\u5c40\u56fe\u5b9e\u73b0\u5bf9\u6570\u641c\u7d22\u6548\u7387\uff0c\u572810\u53f0\u670d\u52a1\u5668\u4e0a\u5bf910\u4ebf\u7ea7\u6570\u636e\u96c6\u8fbe\u52300.95\u53ec\u56de\u7387\uff0c\u541e\u5410\u91cf\u6bd4\u57fa\u51c6\u63d0\u53472.5-6.49\u500d", "motivation": "\u968f\u7740\u6570\u636e\u96c6\u6269\u5c55\u5230\u6570\u5341\u4ebf\u5411\u91cf\uff0c\u78c1\u76d8\u5411\u91cf\u68c0\u7d22\u6210\u4e3a\u5b9e\u7528\u65b9\u6848\uff0c\u4f46\u9700\u8981\u9884\u89c1\u5230\u672a\u6765\u6570\u636e\u96c6\u53ef\u80fd\u8d85\u8fc7\u5355\u53f0\u670d\u52a1\u5668\u5bb9\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faBatANN\u5206\u5e03\u5f0f\u78c1\u76d8\u8fd1\u4f3c\u6700\u8fd1\u90bb\u7cfb\u7edf\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u5f53\u8bbf\u95ee\u5b58\u50a8\u5728\u5176\u4ed6\u673a\u5668\u4e0a\u7684\u90bb\u57df\u65f6\uff0c\u5c06\u67e5\u8be2\u5b8c\u6574\u72b6\u6001\u53d1\u9001\u5230\u76ee\u6807\u673a\u5668\u7ee7\u7eed\u6267\u884c\uff0c\u63d0\u9ad8\u5c40\u90e8\u6027\uff0c\u4fdd\u6301\u5355\u5168\u5c40\u56fe\u7684\u5bf9\u6570\u641c\u7d22\u6548\u7387", "result": "\u5728100M\u548c1B\u70b9\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u752810\u53f0\u670d\u52a1\u5668\u8fbe\u52300.95\u53ec\u56de\u7387\u65f6\uff0c\u541e\u5410\u91cf\u5206\u522b\u6bd4scatter-gather\u57fa\u51c6\u63d0\u53476.21-6.49\u500d\u548c2.5-5.10\u500d\uff0c\u5e73\u5747\u5ef6\u8fdf\u4f4e\u4e8e6\u6beb\u79d2\uff0c\u4e14\u57fa\u4e8e\u6807\u51c6TCP\u5b9e\u73b0", "conclusion": "BatANN\u662f\u9996\u4e2a\u5f00\u6e90\u5206\u5e03\u5f0f\u78c1\u76d8\u5411\u91cf\u68c0\u7d22\u7cfb\u7edf\uff0c\u80fd\u5728\u5355\u5168\u5c40\u56fe\u4e0a\u64cd\u4f5c\uff0c\u5b9e\u73b0\u8fd1\u7ebf\u6027\u7684\u541e\u5410\u91cf\u6269\u5c55\uff0c\u4e3a\u5927\u89c4\u6a21\u5411\u91cf\u641c\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.09472", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09472", "abs": "https://arxiv.org/abs/2512.09472", "authors": ["Chiheng Lou", "Sheng Qi", "Rui Kang", "Yong Zhang", "Chen Sun", "Pengcheng Wang", "Bingyang Liu", "Xuanzhe Liu", "Xin Jin"], "title": "WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving", "comment": null, "summary": "Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.\n  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\\times$ more requests compared to the GPU-sharing system.", "AI": {"tldr": "WarmServe\u662f\u4e00\u4e2a\u591aLLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u901a\u7528GPU\u5de5\u4f5c\u5668\u5b9e\u73b0\u57fa\u4e8e\u672a\u6765\u5de5\u4f5c\u8d1f\u8f7d\u9884\u6d4b\u7684\u9884\u70ed\uff0c\u663e\u8457\u63d0\u5347\u9996\u4ee4\u724c\u65f6\u95f4(TTFT)\u6027\u80fd", "motivation": "\u73b0\u6709\u591aLLM\u670d\u52a1\u7cfb\u7edf\u5728\u63d0\u5347GPU\u5229\u7528\u7387\u7684\u540c\u65f6\u727a\u7272\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u7279\u522b\u662f\u9996\u4ee4\u724c\u65f6\u95f4(TTFT)\u3002\u95ee\u9898\u7684\u6839\u6e90\u5728\u4e8e\u8fd9\u4e9b\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u672a\u6765\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\u7684\u8ba4\u77e5\uff0c\u800c\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u5177\u6709\u9ad8\u5ea6\u5468\u671f\u6027\u548c\u957f\u671f\u53ef\u9884\u6d4b\u6027\u3002", "method": "\u63d0\u51fa\u901a\u7528GPU\u5de5\u4f5c\u5668\u5b9e\u73b0\u4e00\u5bf9\u591aGPU\u9884\u70ed\uff0c\u57fa\u4e8e\u6b64\u8bbe\u8ba1WarmServe\u7cfb\u7edf\uff1a1)\u91c7\u7528\u9a71\u9010\u611f\u77e5\u6a21\u578b\u653e\u7f6e\u7b56\u7565\u51cf\u8f7b\u96c6\u7fa4\u7ea7\u9884\u70ed\u5e72\u6270\uff1b2)\u901a\u8fc7\u4e3b\u52a8\u9884\u70ed\u63d0\u524d\u51c6\u5907\u901a\u7528GPU\u5de5\u4f5c\u5668\uff1b3)\u4f7f\u7528\u96f6\u5f00\u9500\u5185\u5b58\u5207\u6362\u673a\u5236\u7ba1\u7406GPU\u5185\u5b58\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\uff0cWarmServe\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u6269\u7f29\u7cfb\u7edf\u5c06TTFT\u63d0\u5347\u9ad8\u8fbe50.8\u500d\uff0c\u540c\u65f6\u76f8\u6bd4GPU\u5171\u4eab\u7cfb\u7edf\u80fd\u591f\u670d\u52a1\u591a\u8fbe2.5\u500d\u7684\u8bf7\u6c42\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u5de5\u4f5c\u8d1f\u8f7d\u7684\u53ef\u9884\u6d4b\u6027\u5e76\u8bbe\u8ba1\u901a\u7528GPU\u5de5\u4f5c\u5668\uff0cWarmServe\u6210\u529f\u89e3\u51b3\u4e86\u591aLLM\u670d\u52a1\u4e2dGPU\u5229\u7528\u7387\u4e0e\u63a8\u7406\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u670d\u52a1\u3002"}}
{"id": "2512.09502", "categories": ["cs.DC", "cs.NE", "physics.comp-ph", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.09502", "abs": "https://arxiv.org/abs/2512.09502", "authors": ["Bruno Golosio", "Gianmarco Tiddia", "Jos\u00e9 Villamar", "Luca Pontisso", "Luca Sergi", "Francesco Simula", "Pooja Babu", "Elena Pastorelli", "Abigail Morrison", "Markus Diesmann", "Alessandro Lonardo", "Pier Stanislao Paolucci", "Johanna Senk"], "title": "Scalable Construction of Spiking Neural Networks using up to thousands of GPUs", "comment": null, "summary": "Diverse scientific and engineering research areas deal with discrete, time-stamped changes in large systems of interacting delay differential equations. Simulating such complex systems at scale on high-performance computing clusters demands efficient management of communication and memory. Inspired by the human cerebral cortex -- a sparsely connected network of $\\mathcal{O}(10^{10})$ neurons, each forming $\\mathcal{O}(10^{3})$--$\\mathcal{O}(10^{4})$ synapses and communicating via short electrical pulses called spikes -- we study the simulation of large-scale spiking neural networks for computational neuroscience research. This work presents a novel network construction method for multi-GPU clusters and upcoming exascale supercomputers using the Message Passing Interface (MPI), where each process builds its local connectivity and prepares the data structures for efficient spike exchange across the cluster during state propagation. We demonstrate scaling performance of two cortical models using point-to-point and collective communication, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u591aGPU\u96c6\u7fa4\u548c\u767e\u4ebf\u4ebf\u6b21\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684MPI\u7f51\u7edc\u6784\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6a21\u62df", "motivation": "\u5927\u89c4\u6a21\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u5728\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u9700\u8981\u9ad8\u6548\u7ba1\u7406\u901a\u4fe1\u548c\u5185\u5b58\uff0c\u7279\u522b\u662f\u5728\u591aGPU\u96c6\u7fa4\u548c\u767e\u4ebf\u4ebf\u6b21\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a", "method": "\u4f7f\u7528MPI\u7684\u6d88\u606f\u4f20\u9012\u63a5\u53e3\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\u6784\u5efa\u672c\u5730\u8fde\u63a5\u6027\u5e76\u51c6\u5907\u6570\u636e\u7ed3\u6784\uff0c\u4ee5\u5728\u72b6\u6001\u4f20\u64ad\u671f\u95f4\u5b9e\u73b0\u9ad8\u6548\u7684\u96c6\u7fa4\u95f4\u8109\u51b2\u4ea4\u6362", "result": "\u5c55\u793a\u4e86\u4e24\u79cd\u76ae\u5c42\u6a21\u578b\u4f7f\u7528\u70b9\u5bf9\u70b9\u548c\u96c6\u4f53\u901a\u4fe1\u7684\u6269\u5c55\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7f51\u7edc\u6784\u5efa\u548c\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883"}}
{"id": "2512.09568", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09568", "abs": "https://arxiv.org/abs/2512.09568", "authors": ["Zhi Zhao", "Hang Xiao", "Wei Rang"], "title": "PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing", "comment": "24 pages,5 figures", "summary": "Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.", "AI": {"tldr": "PHWSOA\u7b97\u6cd5\u7ed3\u5408\u9cb8\u9c7c\u4f18\u5316\u548c\u6d77\u9e25\u4f18\u5316\uff0c\u901a\u8fc7\u5e15\u7d2f\u6258\u4f18\u5316\u540c\u65f6\u51cf\u5c11\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3001\u6539\u5584\u865a\u62df\u673a\u8d1f\u8f7d\u5747\u8861\u5e76\u964d\u4f4e\u6210\u672c\uff0c\u5728\u4e91\u4efb\u52a1\u8c03\u5ea6\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u4e91\u4efb\u52a1\u8c03\u5ea6\u65b9\u6848\u5927\u591a\u53ea\u4f18\u5316\u5355\u4e00\u6216\u6709\u9650\u6307\u6807\uff08\u5982\u6267\u884c\u65f6\u95f4\u6216\u8d44\u6e90\u5229\u7528\u7387\uff09\uff0c\u7f3a\u4e4f\u5168\u9762\u7684\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3001\u865a\u62df\u673a\u8d1f\u8f7d\u5747\u8861\u548c\u7ecf\u6d4e\u6210\u672c\u7b49\u591a\u4e2a\u5173\u952e\u76ee\u6807\u3002", "method": "\u63d0\u51fa\u5e15\u7d2f\u6258\u6df7\u5408\u9cb8\u9c7c-\u6d77\u9e25\u4f18\u5316\u7b97\u6cd5(PHWSOA)\uff0c\u7ed3\u5408\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5\u7684\u5168\u5c40\u63a2\u7d22\u80fd\u529b\u548c\u6d77\u9e25\u4f18\u5316\u7b97\u6cd5\u7684\u5c40\u90e8\u5f00\u53d1\u80fd\u529b\uff0c\u91c7\u7528Halton\u5e8f\u5217\u521d\u59cb\u5316\u63d0\u9ad8\u79cd\u7fa4\u591a\u6837\u6027\uff0c\u5e15\u7d2f\u6258\u5f15\u5bfc\u53d8\u5f02\u9632\u6b62\u65e9\u719f\u6536\u655b\uff0c\u5e76\u884c\u5904\u7406\u52a0\u901f\u6536\u655b\uff0c\u5e76\u96c6\u6210\u52a8\u6001\u865a\u62df\u673a\u8d1f\u8f7d\u91cd\u5206\u914d\u673a\u5236\u3002", "result": "\u5728CloudSim\u6a21\u62df\u5668\u4e0a\u4f7f\u7528NASA-iPSC\u548cHPC2N\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u5b9e\u9a8c\uff0cPHWSOA\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\uff1a\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u51cf\u5c1172.1%\uff0c\u865a\u62df\u673a\u8d1f\u8f7d\u5747\u8861\u6539\u558436.8%\uff0c\u6210\u672c\u8282\u770123.5%\uff0c\u4f18\u4e8eWOA\u3001GA\u3001PEWOA\u548cGCWOA\u7b49\u7b97\u6cd5\u3002", "conclusion": "PHWSOA\u901a\u8fc7\u6709\u6548\u7684\u591a\u76ee\u6807\u4f18\u5316\u7b56\u7565\uff0c\u5728\u4e91\u4efb\u52a1\u8c03\u5ea6\u4e2d\u5b9e\u73b0\u4e86\u7efc\u5408\u6027\u80fd\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u4e91\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u8d44\u6e90\u7ba1\u7406\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2512.09664", "categories": ["cs.DC", "cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.09664", "abs": "https://arxiv.org/abs/2512.09664", "authors": ["Antonio Terpin", "Alan Bonomi", "Francesco Banelli", "Raffaello D'Andrea"], "title": "SynthPix: A lightspeed PIV images generator", "comment": "Code: https://github.com/antonioterpin/synthpix", "summary": "We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.", "AI": {"tldr": "SynthPix\u662f\u57fa\u4e8eJAX\u5b9e\u73b0\u7684\u9ad8\u6027\u80fd\u5e76\u884cPIV\u5408\u6210\u56fe\u50cf\u751f\u6210\u5668\uff0c\u76f8\u6bd4\u73b0\u6709\u5de5\u5177\u63d0\u5347\u6570\u4e2a\u6570\u91cf\u7ea7\u7684\u56fe\u50cf\u5bf9\u751f\u6210\u901f\u5ea6", "motivation": "\u4e3a\u6570\u636e\u9965\u6e34\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u51cf\u5c11\u5b9e\u65f6PIV\u53cd\u9988\u63a7\u5236\u7814\u7a76\u4e2d\u5feb\u901f\u6d41\u573a\u4f30\u8ba1\u7b97\u6cd5\u7684\u5f00\u53d1\u8fed\u4ee3\u65f6\u95f4", "method": "\u4f7f\u7528JAX\u6846\u67b6\u5b9e\u73b0\uff0c\u652f\u6301\u4e0e\u73b0\u6709\u5de5\u5177\u76f8\u540c\u7684\u914d\u7f6e\u53c2\u6570\uff0c\u4f46\u901a\u8fc7\u52a0\u901f\u5668\u5e76\u884c\u5316\u5b9e\u73b0\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210", "result": "\u5b9e\u73b0\u4e86\u6570\u4e2a\u6570\u91cf\u7ea7\u7684\u541e\u5410\u91cf\u63d0\u5347\uff08\u6bcf\u79d2\u56fe\u50cf\u5bf9\u751f\u6210\u901f\u5ea6\uff09\uff0c\u4e3a\u6d41\u4f53\u52a8\u529b\u5b66\u793e\u533a\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177", "conclusion": "SynthPix\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684PIV\u5408\u6210\u56fe\u50cf\u751f\u6210\u5668\uff0c\u80fd\u591f\u663e\u8457\u52a0\u901f\u6d41\u573a\u4f30\u8ba1\u65b9\u6cd5\u7684\u5f00\u53d1\u548c\u8bad\u7ec3\uff0c\u5bf9\u6d41\u4f53\u52a8\u529b\u5b66\u7814\u7a76\u5177\u6709\u91cd\u8981\u4ef7\u503c"}}
{"id": "2512.09685", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09685", "abs": "https://arxiv.org/abs/2512.09685", "authors": ["Zeyu Zhang", "Haiying Shen"], "title": "Straggler Tolerant and Resilient DL Training on Homogeneous GPUs", "comment": null, "summary": "Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.", "AI": {"tldr": "STAR\u7cfb\u7edf\u901a\u8fc7\u65b0\u7684\u540c\u6b65\u6a21\u5f0f\u548c\u8d44\u6e90\u7ba1\u7406\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3GPU\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684straggler\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u5c3d\u7ba1GPU\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u5f88\u6d41\u884c\uff0c\u4f46straggler\u95ee\u9898\u7684\u666e\u904d\u6027\u3001\u539f\u56e0\u548c\u5f71\u54cd\uff0c\u4ee5\u53ca\u73b0\u6709\u7f13\u89e3\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4ecd\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u53d1\u73b0straggler\u5e7f\u6cdb\u5b58\u5728\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u540c\u6b65\u5230\u5f02\u6b65SGD\u5207\u6362\uff09\u53ef\u80fd\u65e0\u6cd5\u6539\u5584\u8bad\u7ec3\u65f6\u95f4\u751a\u81f3\u4ea7\u751f\u66f4\u591astraggler\u3002", "method": "\u63d0\u51faSTAR\u7cfb\u7edf\uff0c\u5305\u542b\uff1a1\uff09\u65b0\u7684\u540c\u6b65\u6a21\u5f0f\uff0c\u5c06worker\u5206\u7ec4\u8fdb\u884c\u53c2\u6570\u66f4\u65b0\uff1b2\uff09\u542f\u53d1\u5f0f\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9009\u62e9\u6700\u4f18\u540c\u6b65\u6a21\u5f0f\u4ee5\u6700\u5c0f\u5316\u8bad\u7ec3\u65f6\u95f4\uff1b3\uff09\u8d44\u6e90\u91cd\u5206\u914d\u652f\u6301\u6240\u9009\u6a21\u5f0f\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u5171\u5b58\u4f5c\u4e1a\u7684\u5f71\u54cd\uff1b4\uff09\u4e3b\u52a8\u9884\u9632straggler\uff0c\u907f\u514dCPU\u548c\u5e26\u5bbd\u8fc7\u8f7d\u3002", "result": "\u5728AWS\u4e0a\u7684trace\u9a71\u52a8\u8bc4\u4f30\u663e\u793a\uff0cSTAR\u5728PS\u67b6\u6784\u4e2d\u964d\u4f4e48-84%\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u5728all-reduce\u67b6\u6784\u4e2d\u964d\u4f4e51-70%\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u540c\u6b65SGD\u7684\u6536\u655b\u7cbe\u5ea6\u3002", "conclusion": "STAR\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86GPU\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684straggler\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u540c\u65f6\u5f00\u6e90\u4e86\u4ee3\u7801\u3002"}}
{"id": "2512.09710", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09710", "abs": "https://arxiv.org/abs/2512.09710", "authors": ["Hagit Attiya", "Panagiota Fatourou", "Eleftherios Kosmas", "Yuanhao Wei"], "title": "Recoverable Lock-Free Locks", "comment": null, "summary": "This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u540c\u65f6\u5b9e\u73b0\u65e0\u9501\u548c\u53ef\u6062\u590d\u6027\u7684\u8f6c\u6362\u65b9\u6cd5\uff0c\u5c06\u57fa\u4e8e\u9501\u7684\u5b9e\u73b0\u8f6c\u6362\u4e3a\u53ef\u6062\u590d\u7684\u65e0\u9501\u5b9e\u73b0", "motivation": "\u73b0\u6709\u7cfb\u7edf\u8981\u4e48\u5173\u6ce8\u65e0\u9501\u6027\uff0c\u8981\u4e48\u5173\u6ce8\u53ef\u6062\u590d\u6027\uff0c\u7f3a\u4e4f\u540c\u65f6\u5177\u5907\u8fd9\u4e24\u79cd\u7279\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u5c06\u73b0\u6709\u7684\u57fa\u4e8e\u9501\u7684\u5b9e\u73b0\u8f6c\u6362\u4e3a\u65e2\u65e0\u9501\u53c8\u53ef\u6062\u590d\u7684\u7cfb\u7edf\u3002", "method": "\u4ece\u57fa\u4e8e\u9501\u7684\u5b9e\u73b0\u5f00\u59cb\uff0c\u4e3a\u9501\u83b7\u53d6\u548c\u9501\u91ca\u653e\u64cd\u4f5c\u63d0\u4f9b\u53ef\u6062\u590d\u7684\u65e0\u9501\u66ff\u4ee3\u65b9\u6848\u3002\u652f\u6301\u5d4c\u5957\u9501\u4ee5\u589e\u5f3a\u901a\u7528\u6027\uff0c\u786e\u4fdd\u53ef\u6062\u590d\u6027\u800c\u4e0d\u635f\u5bb3\u539f\u59cb\u57fa\u4e8e\u9501\u5b9e\u73b0\u7684\u6b63\u786e\u6027\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u540c\u65f6\u5177\u5907\u65e0\u9501\u548c\u53ef\u6062\u590d\u6027\u7684\u8f6c\u6362\u65b9\u6cd5\uff0c\u80fd\u591f\u5c06\u9501\u57fa\u7cfb\u7edf\u8f6c\u6362\u4e3a\u53ef\u6062\u590d\u7684\u65e0\u9501\u7cfb\u7edf\uff0c\u652f\u6301\u5d4c\u5957\u9501\u5e76\u4fdd\u6301\u539f\u59cb\u5b9e\u73b0\u7684\u6b63\u786e\u6027\u3002", "conclusion": "\u8be5\u8f6c\u6362\u65b9\u6cd5\u586b\u8865\u4e86\u65e0\u9501\u6027\u548c\u53ef\u6062\u590d\u6027\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u6784\u5efa\u65e2\u9ad8\u6548\u53c8\u53ef\u9760\u7684\u65e0\u9501\u53ef\u6062\u590d\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
