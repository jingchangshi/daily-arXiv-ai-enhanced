<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Higher-Order Behavioural Conformances via Fibrations](https://arxiv.org/abs/2507.18509)
*Henning Urbat*

Main category: cs.PL

TL;DR: 论文提出了一种统一的范畴论方法（Howe's方法），用于证明高阶语言中行为一致性的程序同余性，适用于概率等高阶语言。


<details>
  <summary>Details</summary>
Motivation: 随着具有定量特征（如概率）的语言兴起，需要扩展共归纳方法以支持更精细的行为一致性概念（如行为距离），并确保其作为程序同余性的合理性。

Method: 采用抽象的范畴论框架（AHOS）建模高阶语言，并通过纤维化建模行为一致性概念，提出通用的Howe's方法。

Result: 在AHOS框架下，证明了最大行为（双）一致性形成同余性的基本定理。

Conclusion: 该方法适用于概率高阶语言，成功推导了双相似性和行为伪度量的同余性。

Abstract: Coinduction is a widely used technique for establishing behavioural
equivalence of programs in higher-order languages. In recent years, the rise of
languages with quantitative (e.g.~probabilistic) features has led to extensions
of coinductive methods to more refined types of behavioural conformances, most
notably notions of behavioural distance. To guarantee soundness of coinductive
reasoning, one needs to show that the behavioural conformance at hand forms a
program congruence, i.e. it is suitably compatible with the operations of the
language. This is usually achieved by a complex proof technique known as
\emph{Howe's method}, which needs to be carefully adapted to both the specific
language and the targeted notion of behavioural conformance. We develop a
uniform categorical approach to Howe's method that features two orthogonal
dimensions of abstraction: (1) the underlying higher-order language is modelled
by an \emph{abstract higher-order specification} (AHOS), a novel and very
general categorical account of operational semantics, and (2) notions of
behavioural conformance (such as relations or metrics) are modelled via
fibrations over the base category of an AHOS. Our main result is a fundamental
congruence theorem at this level of generality: Under natural conditions on the
categorical ingredients and the operational rules of a language modelled by an
AHOS, the greatest behavioural (bi)conformance on its operational model forms a
congruence. We illustrate our theory by deriving congruence of bisimilarity and
behavioural pseudometrics for probabilistic higher-order languages.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Incentivised Orchestrated Training Architecture (IOTA): A Technical Primer for Release](https://arxiv.org/abs/2507.17766)
*Felix Quinque,Alan Aboudib,Szymon Fonau,Rodrigo Lopez Portillo Alcocer,Brian McCrindle,Steffen Cruz*

Main category: cs.DC

TL;DR: Bittensor的SN9展示了去中心化预训练LLM的可行性，但存在模型本地化和奖励分配问题。IOTA通过协作架构解决了这些问题，实现了模型规模扩展和公平激励。


<details>
  <summary>Details</summary>
Motivation: 解决SN9中模型本地化和"赢家通吃"奖励机制的问题，推动去中心化预训练的进一步发展。

Method: 提出IOTA架构，包括数据并行、管道并行的SWARM架构，连续激励机制，激活压缩技术，Butterfly All-Reduce算法和CLASP公平贡献评估方案。

Result: 实现了模型规模的动态扩展，提高了训练速度，确保了公平激励和贡献评估。

Conclusion: IOTA通过协作架构和新技术解决了SN9的局限性，为去中心化预训练提供了更高效的解决方案。

Abstract: In August 2024, Bittensor's Subnet 9 (SN9) demonstrated that a distributed
network of incentivized, permissionless actors could each pretrain large
language models (LLMs) ranging from 700 million to 14 billion parameters, while
surpassing established baselines. While that work validated blockchain-based
decentralized pretraining as viable, it contained core issues: (i) every miner
had to fit an entire model locally, and (ii) "winner-takes-all" rewards
encouraged model hoarding.
  Here we introduce IOTA (Incentivized Orchestrated Training Architecture), an
architecture that addresses these limitations by transforming SN9's previously
isolated competitors into a single cooperating unit that can scale arbitrarily
while still rewarding each contributor fairly.
  Key preliminary results: (1) Data- and Pipeline-parallel SWARM architecture -
An orchestrator distributes model layers across heterogeneous miners and
streams activations between them, enabling model sizes to scale with the number
of participants rather than being constrained by the VRAM of a single machine;
(2) Granular, continuous incentives - Validators measure each miner's
contribution and allocate token emissions proportionally; (3) Activation
compression - We used model-bottlenecks to cut communication bandwidths of
activations by up to 128x, vastly improving training speed; (4) Butterfly
All-Reduce - Miners average disjoint parameter slices in O(1) bandwidth,
offering linear scalability, redundancy and built-in collusion detection; (5)
CLASP (Contribution Loss Assessment via Sampling of Pathways) - A fair
attribution scheme assigns credit to miners proportional to their marginal
utility and detects exploits, even when contributions are interdependent across
the pipeline.

</details>


### [3] [PolyServe: Efficient Multi-SLO Serving at Scale](https://arxiv.org/abs/2507.17769)
*Kan Zhu,Haiyang Shi,Le Xu,Jiaxin Shan,Arvind Krishnamurthy,Baris Kasikci,Liguang Xie*

Main category: cs.DC

TL;DR: PolyServe是一种新型的多SLO调度策略，通过分组请求、负载均衡和动态共享资源，优化LLM应用的延迟和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有系统将工作负载简单分为延迟敏感（LS）和尽力而为（BE），忽略了延迟敏感类别内的多样性，导致用户体验和调度效率低下。

Method: PolyServe将请求按每令牌延迟需求分组，调度到服务器子集，并通过负载梯度促进自动扩展。它还允许宽松SLO请求共享严格SLO实例以提高利用率。

Result: PolyServe实现了1.23倍的吞吐量提升，达到最优吞吐量的92.5%。

Conclusion: PolyServe通过精细调度和资源管理，显著提升了多SLO场景下的性能和效率。

Abstract: Advances in Large Language Models (LLMs) have led to a surge of LLM-powered
applications. These applications have diverse token-generation latency
requirements. As a result, simply classifying workloads as latency-sensitive
(LS) or best-effort (BE) overlooks the nuances within the latency-sensitive
category and results in suboptimal user experiences and scheduling
opportunities. However, efficiently serving requests with multiple SLO
requirements poses significant challenges. First, all requests within a batch
generate new tokens simultaneously, which can misalign them with their distinct
SLO requirements. Moreover, while existing systems focus on auto-scaling for
handling various overall request rates, the diversity of SLOs necessitates
fine-grained auto-scaling among these SLO tiers. Finally, unlike LS/BE
scenarios, where BE requests can be aborted at any time to ensure the SLO
attainment of LS requests, those with different latency-sensitive SLOs cannot
tolerate prolonged delays, and tail latency must be controlled.
  To tackle these challenges, we propose PolyServe, a novel multi-SLO
scheduling policy at scale that maintains high SLO attainment while maximizing
throughput. PolyServe first groups requests into multiple bins based on their
per-token latency requirement, then schedules each bin to a subset of the
server fleet. PolyServe routes requests to the highest-load but still
SLO-attainable server to create a load gradient that facilitates auto-scaling.
To increase utilization, PolyServe permits looser-SLO requests to share
tighter-SLO instances when their own servers are saturated. PolyServe uses
profiling data to guide scheduling decisions and manage tail latency through
request-wait-time-aware scheduling, dynamic chunking, and continuous chunked
prefill prediction. PolyServe achieves 1.23x goodput gain compared to existing
policies, achieving up to 92.5% of optimal goodput.

</details>


### [4] [Comparative Evaluation of PyTorch, JAX, SciPy, and Neal for Solving QUBO Problems at Scale](https://arxiv.org/abs/2507.17770)
*Pei-Kun Yang*

Main category: cs.DC

TL;DR: 该研究比较了五种基于软件的QUBO求解器（Neal、PyTorch CPU/GPU、JAX、SciPy）的性能，发现PyTorch在可扩展性和运行时间上表现最佳，而Neal在解质量上最优但受限于内存。


<details>
  <summary>Details</summary>
Motivation: 评估不同QUBO求解器在大规模问题上的性能，为实际应用提供选择依据。

Method: 使用随机生成的QUBO矩阵（1000x1000至45000x45000）和六种收敛阈值（10^-1至10^-6），比较各求解器的解质量和计算时间。

Result: Neal解质量最佳但仅支持6000变量；PyTorch可扩展至45000变量且运行时间短；JAX性能接近PyTorch但支持变量较少；SciPy表现最差。

Conclusion: PyTorch在解质量、可扩展性和运行时间上表现均衡，适合大规模QUBO问题。

Abstract: Quadratic Unconstrained Binary Optimization (QUBO) is a versatile framework
for modeling combinatorial optimization problems. This study benchmarks five
software-based QUBO solvers: Neal, PyTorch (CPU), PyTorch (GPU), JAX, and
SciPy, on randomly generated QUBO matrices ranging from 1000x1000 to
45000x45000, under six convergence thresholds from 10^-1 to 10^-6. We evaluate
their performance in terms of solution quality (energy) and computational time.
Among the solvers tested, Neal achieved the lowest energy values but was
limited to problems with up to 6000 variables due to high memory consumption.
PyTorch produced slightly higher energy results than Neal but demonstrated
superior scalability, solving instances with up to 45000 variables. Its support
for GPU acceleration and CPU multi-threading also resulted in significantly
shorter runtimes. JAX yielded energy values slightly above those of PyTorch and
was limited to 25000 variables, with runtimes comparable to PyTorch on GPU.
SciPy was the most constrained solver, handling only up to 6000 variables and
consistently producing the highest energy values with the longest computation
times. These findings highlight trade-offs between solution quality,
scalability, and runtime efficiency, and suggest that PyTorch is the most
balanced choice for large-scale QUBO problems when computational resources
permit.

</details>


### [5] [Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN Inference Acceleration](https://arxiv.org/abs/2507.17771)
*Dmitri Lyalikov*

Main category: cs.DC

TL;DR: 论文探讨了异构架构中深度学习加速器（DLA）和神经处理单元（NPU）的系统集成与编译/执行模型，以解决CNN在嵌入式平台上的资源限制问题。通过RISC-V Vector 1.0扩展，实现了图像预处理和YOLOv3执行的显著加速。


<details>
  <summary>Details</summary>
Motivation: 现代CNN在资源受限的嵌入式平台上的部署需求推动了异构架构的发展，但需要解决系统集成和编译/执行模型的效率问题。

Method: 利用RISC-V Vector 1.0扩展，设计灵活的编程模型和缓存层次结构，优化预处理和CPU回退过程。

Result: 实验显示图像预处理速度提升9倍，YOLOv3回退层执行速度提升3倍，同时功耗低于传统并行执行平台。

Conclusion: RISC-V Vector 1.0扩展为异构嵌入式SoC提供了高效的编程模型，平衡了计算与内存占用，支持现代深度学习数据流。

Abstract: The emergence of heterogeneity and domain-specific architectures targeting
deep learning inference show great potential for enabling the deployment of
modern CNNs on resource-constrained embedded platforms. A significant
development is the diversification of custom hardware solely targeting the most
expensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural
processing units), among others, can overcome the approaching limits of
traditional silicon scaling and provide a solution to the power/performance
tradeoff within embedded SoCs. Efficient DSA utilization requires proper system
integration and a compilation/execution model for balanced execution in these
heterogeneous architectures. There is a critical need for proper system
integration and an efficient compilation/execution model for balanced execution
in these heterogeneous architectures. This work highlights the hardware
integration challenges for efficiently placing these units within the memory
hierarchy and correct proximity to other execution blocks. We experimentally
verify performance bottlenecks in CNN execution and pre/post-processing at
runtime, where previous attention has generally been given to accelerator
speedup alone. This work takes advantage of the ratification of the RISC-V
Vector 1.0 extension and demonstrates its potential as a flexible target within
a well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and
CPU fallback processes. Our results show up to a 9x speedup of image
pre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.
We demonstrate RVV-1.0 in exposing a flexible programming model that can enable
a balanced computation and memory footprint on accelerator-rich embedded SoCs
supporting modern deep-learning dataflows while consuming less power than
traditional parallel execution platforms.

</details>


### [6] [Caching Techniques for Reducing the Communication Cost of Federated Learning in IoT Environments](https://arxiv.org/abs/2507.17772)
*Ahmad Alhonainy,Praveen Rao*

Main category: cs.DC

TL;DR: 论文提出了一种基于缓存策略（FIFO、LRU和优先级）的联邦学习方法，以减少不必要的模型更新传输，从而降低通信成本，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在分布式设备上联合训练共享模型时，通信成本是主要瓶颈，尤其是在资源受限的环境中。

Method: 采用FIFO、LRU和优先级缓存策略，选择性转发重要模型更新以减少带宽使用。

Result: 在CIFAR-10和医疗数据集上的实验表明，通信量减少且准确性损失最小。

Conclusion: 智能缓存提高了可扩展性和内存效率，支持边缘物联网网络中的可靠FL，适用于智能城市和医疗等延迟敏感场景。

Abstract: Federated Learning (FL) allows multiple distributed devices to jointly train
a shared model without centralizing data, but communication cost remains a
major bottleneck, especially in resource-constrained environments. This paper
introduces caching strategies - FIFO, LRU, and Priority-Based - to reduce
unnecessary model update transmissions. By selectively forwarding significant
updates, our approach lowers bandwidth usage while maintaining model accuracy.
Experiments on CIFAR-10 and medical datasets show reduced communication with
minimal accuracy loss. Results confirm that intelligent caching improves
scalability, memory efficiency, and supports reliable FL in edge IoT networks,
making it practical for deployment in smart cities, healthcare, and other
latency-sensitive applications.

</details>


### [7] [MultiKernelBench: A Multi-Platform Benchmark for Kernel Generation](https://arxiv.org/abs/2507.17773)
*Zhongzhen Wen,Yinghui Zhang,Zhong Li,Zhongxin Liu,Linna Xie,Tian Zhang*

Main category: cs.DC

TL;DR: MultiKernelBench是一个全面的多平台基准测试，用于评估基于LLM的深度学习内核生成，支持多种硬件平台并提供模块化设计。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在硬件支持、内核分类和任务覆盖方面存在不足，MultiKernelBench旨在解决这些问题。

Method: 设计了模块化后端抽象层，支持三种硬件平台，并提出类别感知的一次性提示方法。

Result: 评估了七种先进LLM，揭示了任务难度差异、平台泛化能力不足以及提示策略的有效性。

Conclusion: MultiKernelBench为LLM在DL内核生成领域的评估提供了全面且可扩展的解决方案。

Abstract: The automatic generation of deep learning (DL) kernels using large language
models (LLMs) has emerged as a promising approach to reduce the manual effort
and hardware-specific expertise required for writing high-performance operator
implementations. However, existing benchmarks for evaluating LLMs in this
domain suffer from limited hardware support, coarse-grained kernel
categorization, and imbalanced task coverage. To address these limitations, we
introduce MultiKernelBench, the first comprehensive, multi-platform benchmark
for LLM-based DL kernel generation. MultiKernelBench spans 285 tasks across 14
well-defined kernel categories and supports three major hardware platforms:
Nvidia GPUs, Huawei NPUs, and Google TPUs. To enable future extensibility, we
design a modular backend abstraction layer that decouples platform-specific
logic from the core benchmarking infrastructure, allowing easy integration of
new hardware platforms. We further propose a simple yet effective
category-aware one-shot prompting method that improves generation quality by
providing in-category exemplars. Through systematic evaluations of seven
state-of-the-art LLMs, we reveal significant variation in task difficulty, poor
generalization to platforms with less training exposure, and the effectiveness
of targeted prompting strategies. MultiKernelBench is publicly available at
https://github.com/wzzll123/MultiKernelBench.

</details>


### [8] [CHAMP: A Configurable, Hot-Swappable Edge Architecture for Adaptive Biometric Tasks](https://arxiv.org/abs/2507.17793)
*Joel Brogan,Matthew Yohe,David Cornett*

Main category: cs.DC

TL;DR: CHAMP是一个模块化边缘计算平台，支持动态更换AI功能模块，适用于灵活的高性能边缘AI系统。


<details>
  <summary>Details</summary>
Motivation: 为需要灵活、高性能边缘AI系统的现场操作员提供可快速适应的技术解决方案。

Method: 采用低功耗FPGA加速器和高速总线，结合定制操作系统VDiSK，实现即插即用的AI流水线和加密生物特征数据集。

Result: 实验显示1到5个神经计算加速器的吞吐量接近线性扩展，同时揭示了USB3总线的性能增益和饱和限制。

Conclusion: CHAMP在生物识别、监控和灾难响应中有广泛应用前景，未来可改进总线协议、模块功能和系统软件。

Abstract: What if you could piece together your own custom biometrics and AI analysis
system, a bit like LEGO blocks? We aim to bring that technology to field
operators in the field who require flexible, high-performance edge AI system
that can be adapted on a moment's notice. This paper introduces CHAMP
(Configurable Hot-swappable Architecture for Machine Perception), a modular
edge computing platform that allows operators to dynamically swap in
specialized AI "capability cartridges" for tasks like face recognition, object
tracking, and document analysis. CHAMP leverages low-power FPGA-based
accelerators on a high-throughput bus, orchestrated by a custom operating
system (VDiSK) to enable plug-and-play AI pipelines and cryptographically
secured biometric datasets. In this paper we describe the CHAMP design,
including its modular scaling with multiple accelerators and the VDiSK
operating system for runtime reconfiguration, along with its cryptographic
capabilities to keep data stored on modules safe and private. Experiments
demonstrate near-linear throughput scaling from 1 to 5 neural compute
accelerators, highlighting both the performance gains and saturation limits of
the USB3-based bus. Finally, we discuss applications of CHAMP in field
biometrics, surveillance, and disaster response, and outline future
improvements in bus protocols, cartridge capabilities, and system software.

</details>


### [9] [Optimizing Edge Gaming Slices through an Enhanced User Plane Function and Analytics in Beyond-5G Networks](https://arxiv.org/abs/2507.17843)
*Bruno Marques da Silva,Larissa Ferreira Rodrigues Moreira,Flávio de Oliveira Silva,Rodrigo Moreira*

Main category: cs.DC

TL;DR: 提出了一种闭环架构，结合NWDAF和UPF来估计用户延迟，并通过AI模型实现游戏分类，为移动边缘游戏研究开辟新途径。


<details>
  <summary>Details</summary>
Motivation: 当前边缘游戏技术在提升吞吐量和降低延迟方面表现良好，但用户延迟测量的非侵入性功能仍需改进。

Method: 提出闭环架构，整合NWDAF和UPF，嵌入AI模型以实现延迟感知和游戏分类。

Result: 结果表明，该架构能有效估计用户延迟，并通过AI模型实现游戏分类。

Conclusion: 该架构为5G控制平面的延迟感知和移动边缘游戏研究提供了新方向。

Abstract: The latest generation of games and pervasive communication technologies poses
challenges in service management and Service-Level Agreement compliance for
mobile users. State-of-the-art edge-gaming techniques enhance throughput,
reduce latency, and leverage cloud computing. However, further development of
core functions such as the User Plane Function (UPF) is needed for
non-intrusive user latency measurement. This paper proposes a closed-loop
architecture integrating the Network Data Analytics Function (NWDAF) and UPF to
estimate user latency and enhance the 5G control plane by making it
latency-aware. The results show that embedding an artificial intelligence model
within NWDAF enables game classification and opens new avenues for mobile edge
gaming research.

</details>


### [10] [PowerTrip: Exploiting Federated Heterogeneous Datacenter Power for Distributed ML Training](https://arxiv.org/abs/2507.17904)
*Talha Mehboob,Luanzheng Guo,Nathan Tallent,Michael Zink,David Irwin*

Main category: cs.DC

TL;DR: PowerTrip系统动态选择分布式站点以优化大模型训练中的电力-通信权衡，显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型训练受限于区域电网的电力供应，需跨地理分布站点分配工作负载，但通信开销成为主要挑战。

Method: PowerTrip采用动态贪婪方法，基于电力-成本启发式选择高电力可用性和低延迟的站点，优化训练效率。

Result: 使用真实Google电力数据评估，PowerTrip比现有基线策略减少50%的训练时间。

Conclusion: PowerTrip有效解决了电力受限、地理分布式环境中的大模型训练问题。

Abstract: The exponential growth of large-scale AI models has led to computational and
power demands that can exceed the capacity of a single data center. This is due
to the limited power supplied by regional grids that leads to limited regional
computational power. Consequently, distributing training workloads across
geographically distributed sites has become essential. However, this approach
introduces a significant challenge in the form of communication overhead,
creating a fundamental trade-off between the performance gains from accessing
greater aggregate power and the performance losses from increased network
latency. Although prior work has focused on reducing communication volume or
using heuristics for distribution, these methods assume constant homogeneous
power supplies and ignore the challenge of heterogeneous power availability
between sites.
  To address the challenge of training large models in power-constrained,
geo-distributed environments, we introduce PowerTrip, a system that dynamically
selects a subset of sites during runtime to optimize the power-communication
trade-off. Specifically, PowerTrip selects sites based on a power-to-cost
heuristic, prioritizing those with high power availability and low network
latency. PowerTrip employs a dynamic greedy approach and uses the marginal gain
in training efficiency, i.e., accuracy improvement per unit of time, to
optimize for the number of sites where the performance penalty from network
overhead negates the benefit of adding more computational power. Our
evaluation, which uses real-world Google power traces to model realistic power
capacity constraints, demonstrates that PowerTrip can reduce time-to-accuracy
by up to 50% compared to existing baseline policies.

</details>


### [11] [C-Koordinator: Interference-aware Management for Large-scale and Co-located Microservice Clusters](https://arxiv.org/abs/2507.18005)
*Shengye Song,Minxian Xu,Zuowei Zhang,Chengxi Gao,Fansong Zeng,Yu Ding,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 论文分析了大规模共置微服务集群的特性，提出以CPI作为干扰度量指标，并设计了C-Koordinator平台，通过多维度指标预测CPI干扰，显著提升了应用性能。


<details>
  <summary>Details</summary>
Motivation: 微服务共置虽提升资源利用率，但会引发资源竞争和干扰，需设计干扰感知策略以优化性能。

Method: 基于CPI干扰预测，设计C-Koordinator平台，结合共置和干扰缓解策略。

Result: 干扰预测模型准确率超90.3%，应用延迟在各百分位（P50、P90、P99）显著降低16.7%-36.1%。

Conclusion: C-Koordinator能有效缓解共置环境中的干扰，稳定应用性能。

Abstract: Microservices transform traditional monolithic applications into lightweight,
loosely coupled application components and have been widely adopted in many
enterprises. Cloud platform infrastructure providers enhance the resource
utilization efficiency of microservices systems by co-locating different
microservices. However, this approach also introduces resource competition and
interference among microservices. Designing interference-aware strategies for
large-scale, co-located microservice clusters is crucial for enhancing resource
utilization and mitigating competition-induced interference. These challenges
are further exacerbated by unreliable metrics, application diversity, and node
heterogeneity.
  In this paper, we first analyze the characteristics of large-scale and
co-located microservices clusters at Alibaba and further discuss why cycle per
instruction (CPI) is adopted as a metric for interference measurement in
large-scale production clusters, as well as how to achieve accurate prediction
of CPI through multi-dimensional metrics. Based on CPI interference prediction
and analysis, we also present the design of the C-Koordinator platform, an
open-source solution utilized in Alibaba cluster, which incorporates
co-location and interference mitigation strategies. The interference prediction
models consistently achieve over 90.3% accuracy, enabling precise prediction
and rapid mitigation of interference in operational environments. As a result,
application latency is reduced and stabilized across all percentiles (P50, P90,
P99) response time (RT), achieving improvements ranging from 16.7% to 36.1%
under various system loads compared with state-of-the-art system. These results
demonstrate the system's ability to maintain smooth application performance in
co-located environments.

</details>


### [12] [Unlock the Potential of Fine-grained LLM Serving via Dynamic Module Scaling](https://arxiv.org/abs/2507.18006)
*Jingfeng Wu,Yiyuan He,Minxian Xu,Xitong Gao,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: CoCoServe是一个弹性系统，通过模块级操作实现动态和细粒度的扩展，优化LLM服务的资源利用和性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务系统在资源管理和动态负载适应上面临挑战，静态部署导致资源利用不足和性能下降。

Method: 提出模块级复制和迁移操作，开发自动扩展机制，动态调节资源分配和性能优化。

Result: CoCoServe可降低成本46%，延迟减少14%-75%，吞吐量提升1.16x-4x。

Conclusion: CoCoServe显著提升了LLM服务的效率和成本效益。

Abstract: The rise of large language models (LLMs) has created new opportunities across
various fields but has also introduced significant challenges in resource
management. Current LLM serving systems face a fundamental tension: balancing
serving demands with limited resources while adapting to unpredictable traffic
patterns. Static deployments lead to suboptimal resource utilization and
performance degradation under dynamic workloads. Furthermore, the high cost of
adjusting instances hinders dynamic scaling, limiting the true potential of
efficient LLM serving.
  To address this, we propose CoCoServe, an elastic system that facilitates
dynamic and fine-grained scaling. Its key innovation lies in the module-level
operations for the replication and migration of LLM modules, such as decoder
layers and projections. Through a comprehensive analysis of the trade-offs
associated with these operations, we develop an auto-scaling mechanism that
dynamically regulates module-level resource allocation and performance
optimization, enabling a more cost-effective deployment of LLMs. Our evaluation
demonstrates that the scaling operations employed by CoCoServe exhibit
excellent scalability and can reduce costs by 46% while maintaining
availability. Compared to state-of-the-art LLM serving systems (e.g., Hugging
Face Transformers and vLLM), our approach reduces latency by 14%-75% and
achieves 1.16x-4x throughput on average across different model sizes and
workloads.

</details>


### [13] [Cloud Native System for LLM Inference Serving](https://arxiv.org/abs/2507.18007)
*Minxian Xu,Junhan Liao,Jingfeng Wu,Yiyuan He,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 论文探讨了如何利用云原生技术优化大型语言模型（LLM）的推理服务，解决传统方法在资源效率、延迟和扩展性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的计算需求高，传统推理服务方法存在资源效率低、成本高、延迟大和扩展性差的问题。

Method: 采用云原生技术（如容器化、微服务和动态调度），结合Kubernetes自动扩展，优化LLM推理服务。

Result: 实验表明，云原生架构能动态适应工作负载变化，提高资源分配效率，降低延迟并提升吞吐量。

Conclusion: 云原生框架有望重塑LLM推理服务的未来，为云计算和人工智能领域提供重要见解。

Abstract: Large Language Models (LLMs) are revolutionizing numerous industries, but
their substantial computational demands create challenges for efficient
deployment, particularly in cloud environments. Traditional approaches to
inference serving often struggle with resource inefficiencies, leading to high
operational costs, latency issues, and limited scalability. This article
explores how Cloud Native technologies, such as containerization,
microservices, and dynamic scheduling, can fundamentally improve LLM inference
serving. By leveraging these technologies, we demonstrate how a Cloud Native
system enables more efficient resource allocation, reduces latency, and
enhances throughput in high-demand scenarios. Through real-world evaluations
using Kubernetes-based autoscaling, we show that Cloud Native architectures can
dynamically adapt to workload fluctuations, mitigating performance bottlenecks
while optimizing LLM inference serving performance. This discussion provides a
broader perspective on how Cloud Native frameworks could reshape the future of
scalable LLM inference serving, offering key insights for researchers,
practitioners, and industry leaders in cloud computing and artificial
intelligence.

</details>


### [14] [FCPO: Federated Continual Policy Optimization for Real-Time High-Throughput Edge Video Analytics](https://arxiv.org/abs/2507.18047)
*Lucas Liebe,Thanh-Tung Nguyen,Dongman Lee*

Main category: cs.DC

TL;DR: FCPO结合持续强化学习（CRL）和联邦强化学习（FRL），优化边缘视频分析（EVA）的实时推理服务，显著提升吞吐量、降低延迟和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 边缘视频分析（EVA）的复杂性增加，现有调度系统在动态环境中表现不佳，局部强化学习（RL）存在扩展性和适应性不足的问题。

Method: FCPO整合CRL和FRL，动态调整推理批次大小、输入分辨率和多线程处理，通过特定代理聚合方案和多样性感知经验缓冲区实现。

Result: 实验显示，FCPO在吞吐量、延迟和收敛速度上显著优于现有RL方法，内存消耗降低10倍。

Conclusion: FCPO为动态边缘环境中的实时推理服务提供了一种高效、可扩展的解决方案。

Abstract: The growing complexity of Edge Video Analytics (EVA) facilitates new kind of
intelligent applications, but creates challenges in real-time inference serving
systems. State-of-the-art (SOTA) scheduling systems optimize global workload
distributions for heterogeneous devices but often suffer from extended
scheduling cycles, leading to sub-optimal processing in rapidly changing Edge
environments. Local Reinforcement Learning (RL) enables quick adjustments
between cycles but faces scalability, knowledge integration, and adaptability
issues. Thus, we propose FCPO, which combines Continual RL (CRL) with Federated
RL (FRL) to address these challenges. This integration dynamically adjusts
inference batch sizes, input resolutions, and multi-threading during pre- and
post-processing. CRL allows agents to learn from changing Markov Decision
Processes, capturing dynamic environmental variations, while FRL improves
generalization and convergence speed by integrating experiences across
inference models. FCPO combines these via an agent-specific aggregation scheme
and a diversity-aware experience buffer. Experiments on a real-world EVA
testbed showed over 5 times improvement in effective throughput, 60% reduced
latency, and 20% faster convergence with up to 10 times less memory consumption
compared to SOTA RL-based approaches.

</details>


### [15] [A large-scale distributed parallel discrete event simulation engines based on Warped2 for Wargaming simulation](https://arxiv.org/abs/2507.18050)
*Xiaoning Jia,Ruilin Kong,Guangya Si,Bilong Shen,Zhe Ji*

Main category: cs.DC

TL;DR: 论文提出了一种优化的并行离散事件仿真（PDES）框架，解决了传统引擎在资源分配和复杂实体交互中的局限性，通过四项改进显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统并行离散事件仿真引擎（如Warped2）在资源分配和复杂实体交互方面存在不足，限制了其在大规模仿真中的性能。

Method: 提出了四项改进：异步监听线程、METIS负载均衡策略、实体交互求解器和空间哈希算法。

Result: 实验验证显示，框架实现了16倍加速，同步开销减少58.18%，负载均衡贡献了57%的改进。

Conclusion: 该优化框架为大规模仿真提供了高效解决方案，显著提升了PDES的性能。

Abstract: Rising demand for complex simulations highlights conventional
engines'scalability limits, spurring Parallel Discrete Event Simulation (PDES)
adoption.Warped2, a PDES engine leveraging Time Warp synchronization with
Pending Event Set optimization, delivers strong performance, it struggles with
inherent wargaming limitations: inefficient LP resource allocation during
synchronization and unaddressed complex entity interaction patterns. To address
these challenges, we present an optimized framework featuring four synergistic
improvements: (1) Asynchronous listener threads are introduced to address event
monitoring latency in large-scale scenarios, instead of synchronous polling
mechanisms, (2) METIS-based load rebalancing strategy is incorporated to
address the issue of dynamic event allocation during real-world simulation, (3)
Entity interaction solver with constraint satisfaction mechanisms is designed
to mitigate state conflicts, and (4) Spatial hashing algorithm to overcome
O(n^2) complexity bottlenecks in large-scale nearest-neighbor searches.
Experimental validation through a GridWorld demo demonstrates significant
enhancements in temporal fidelity and computational efficiency. Benchmark
results show our framework achieves 16x acceleration over baseline
implementations and maintains 8x speedup over 1-thread configuration across MPI
and Pthreads implementations.The combined load balancing and LP migration
strategy reduces synchronization overhead by 58.18%, with load balancing
accounting for 57% of the total improvement as the dominant optimization
factor. These improvements provide an enhanced solution for PDES implementation
in large-scale simulation scenarios.

</details>


### [16] [Towards Designing an Energy Aware Data Replication Strategy for Cloud Systems Using Reinforcement Learning](https://arxiv.org/abs/2507.18459)
*Amir Najjar,Riad Mokadem,Jean-Marc Pierson*

Main category: cs.DC

TL;DR: 提出了一种基于强化学习的云系统数据复制策略，动态适应工作负载变化，优化服务质量与资源利用。


<details>
  <summary>Details</summary>
Motivation: 全球数据量快速增长，传统基于阈值的复制策略依赖人工调整，难以适应动态变化。

Method: 利用强化学习模型，定义状态、动作和奖励，自动学习系统特性并适应工作负载变化。

Result: 策略在保证服务质量的同时，优化了提供商利润与环境影响的平衡。

Conclusion: 强化学习为云系统数据复制提供了动态自适应的有效解决方案。

Abstract: The rapid growth of global data volumes has created a demand for scalable
distributed systems that can maintain a high quality of service. Data
replication is a widely used technique that provides fault tolerance, improved
performance and higher availability. Traditional implementations often rely on
threshold-based activation mechanisms, which can vary depending on workload
changes and system architecture. System administrators typically bear the
responsibility of adjusting these thresholds. To address this challenge,
reinforcement learning can be used to dynamically adapt to workload changes and
different architectures. In this paper, we propose a novel data replication
strategy for cloud systems that employs reinforcement learning to automatically
learn system characteristics and adapt to workload changes. The strategy's aim
is to provide satisfactory Quality of Service while optimizing a trade-off
between provider profit and environmental impact. We present the architecture
behind our solution and describe the reinforcement learning model by defining
the states, actions and rewards.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [Designing High-Performance and Thermally Feasible Multi-Chiplet Architectures enabled by Non-bendable Glass Interposer](https://arxiv.org/abs/2507.18040)
*Harsh Sharma,Janardhan Rao Doppa,Umit Y. Ogras,Partha Pratim Pande*

Main category: cs.AR

TL;DR: 玻璃中介层多芯片架构在性能和能效上优于硅中介层系统，但面临封装变形问题。作者提出一种热、变形和性能感知的设计框架，通过架构与封装协同优化，显著提升性能并降低功耗。


<details>
  <summary>Details</summary>
Motivation: 解决玻璃中介层多芯片系统因尺寸增大导致的封装变形问题，同时优化性能和功耗。

Method: 提出一种热、变形和性能感知的设计框架，通过架构与封装协同优化，平衡性能、功耗和结构可靠性。

Result: 优化后的多芯片架构在深度神经网络任务中性能提升64.7%，功耗降低40%，且制造成本更低。

Conclusion: 该框架为玻璃中介层多芯片系统提供了一种有效的解决方案，实现了性能、功耗和可靠性的优化。

Abstract: Multi-chiplet architectures enabled by glass interposer offer superior
electrical performance, enable higher bus widths due to reduced crosstalk, and
have lower capacitance in the redistribution layer than current silicon
interposer-based systems. These advantages result in lower energy per bit,
higher communication frequencies, and extended interconnect range. However,
deformation of the package (warpage) in glass interposer-based systems becomes
a critical challenge as system size increases, leading to severe mechanical
stress and reliability concerns. Beyond a certain size, conventional packaging
techniques fail to manage warpage effectively, necessitating new approaches to
mitigate warpage induced bending with scalable performance for glass interposer
based multi-chiplet systems. To address these inter-twined challenges, we
propose a thermal-, warpage-, and performance-aware design framework that
employs architecture and packaging co-optimization. The proposed framework
disintegrates the surface and embedded chiplets to balance conflicting design
objectives, ensuring optimal trade-offs between performance, power, and
structural reliability. Our experiments demonstrate that optimized
multi-chiplet architectures from our design framework achieve up to 64.7%
performance improvement and 40% power reduction compared to traditional 2.5D
systems to execute deep neural network workloads with lower fabrication costs.

</details>


### [18] [Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving](https://arxiv.org/abs/2507.18454)
*Juntao Zhao,Jiuru Li,Chuan Wu*

Main category: cs.AR

TL;DR: Sandwich是一种基于CPU的LLM服务引擎，通过为预填充和解码阶段设计不同的执行计划，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有CPU解决方案忽略了LLM推理中预填充和解码阶段的工作负载差异，导致性能不佳。

Method: Sandwich采用硬件中心的方法，针对预填充和解码阶段分别优化执行计划，并在不同CPU平台上进行评估。

Result: Sandwich在吞吐量、延迟和资源需求方面均有显著提升，生成的GEMM内核性能优于现有解决方案。

Conclusion: Sandwich为CPU上的LLM服务提供了一种高效、低成本的优化方案。

Abstract: Utilizing CPUs to serve large language models (LLMs) is a resource-friendly
alternative to GPU serving. Existing CPU-based solutions ignore workload
differences between the prefill and the decode phases of LLM inference,
applying a static per-NUMA (Non-Uniform Memory Access) node model partition and
utilizing vendor libraries for operator-level execution, which is suboptimal.
We propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses
different execution plans for the prefill and decode phases and optimizes them
separately.
  We evaluate Sandwich across diverse baselines and datasets on five CPU
platforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON.
Sandwich achieves an average 2.01x throughput improvement and 90% satisfactory
time-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up
to 3.40x lower requirements in single sequence serving, and significant
improvement in Goodput in continuous-batching serving. The GEMM kernels
generated by Sandwich outperform representative vendor kernels and other
dynamic shape solutions, achieving performance comparable to static compilers
with three orders of magnitude less kernel tuning costs.

</details>


### [19] [PRACtical: Subarray-Level Counter Update and Bank-Level Recovery Isolation for Efficient PRAC Rowhammer Mitigation](https://arxiv.org/abs/2507.18581)
*Ravan Nazaraliyev,Saber Ganjisaffar,Nurlan Nazaraliyev,Nael Abu-Ghazaleh*

Main category: cs.AR

TL;DR: PRACtical是一种优化DDR5中PRAC+ABO性能的方法，通过集中式计数器更新和银行级粒度缓解措施，提升性能并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着DRAM密度增加，Rowhammer问题加剧，现有DDR5标准中的PRAC和ABO机制存在性能开销和通道级停顿问题。

Method: 引入集中式增量电路以减少计数器更新延迟，并实现银行级粒度的缓解措施，避免整个通道停顿。

Result: PRACtical平均提升性能8%（最高20%），降低能耗19%，并将性能退化限制在6%以内。

Conclusion: PRACtical在保持Rowhammer防护的同时，显著优化了性能和能耗。

Abstract: As DRAM density increases, Rowhammer becomes more severe due to heightened
charge leakage, reducing the number of activations needed to induce bit flips.
The DDR5 standard addresses this threat with in-DRAM per-row activation
counters (PRAC) and the Alert Back-Off (ABO) signal to trigger mitigation.
However, PRAC adds performance overhead by incrementing counters during the
precharge phase, and recovery refreshes stalls the entire memory channel, even
if only one bank is under attack.
  We propose PRACtical, a performance-optimized approach to PRAC+ABO that
maintains the same security guarantees. First, we reduce counter update latency
by introducing a centralized increment circuit, enabling overlap between
counter updates and subsequent row activations in other subarrays. Second, we
enhance the $RFM_{ab}$ mitigation by enabling bank-level granularity: instead
of stalling the entire channel, only affected banks are paused. This is
achieved through a DRAM-resident register that identifies attacked banks.
  PRACtical improves performance by 8% on average (up to 20%) over the
state-of-the-art, reduces energy by 19%, and limits performance degradation
from aggressive performance attacks to less than 6%, all while preserving
Rowhammer protection.

</details>
