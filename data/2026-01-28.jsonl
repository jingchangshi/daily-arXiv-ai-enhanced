{"id": "2601.19213", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.19213", "abs": "https://arxiv.org/abs/2601.19213", "authors": ["Weiming Hu", "Zihan Zhang", "Haoyan Zhang", "Chen Zhang", "Cong Guo", "Yu Feng", "Tianchi Hu", "Guanglin Li", "Guipeng Hu", "Junsong Wang", "Jingwen Leng"], "title": "M$^{\\text{2}}$XFP: A Metadata-Augmented Microscaling Data Format for Efficient Low-bit Quantization", "comment": "17 pages, 13 figures, ASPLOS 2026", "summary": "Existing low-bit Microscaling (MX) formats, such as MXFP4, often suffer from substantial accuracy degradation due to the use of a shared scaling factor with the Power-of-Two format. In this work, we explore strategies that introduce minimal metadata to recover accuracy lost during quantization while maintaining high bit efficiency across a wide range of large language models. We propose a complete algorithm-hardware co-design based on flexible metadata, featuring an online quantization with simple encoding. To support the proposed method efficiently, we implement a lightweight hardware unit and integrate it into the accelerator. Evaluation results demonstrate that our method substantially narrows the accuracy gap, achieving on average a 70.63% reduction in accuracy loss compared to MXFP4 and a 37.30% reduction relative to the latest NVFP4 on LLM benchmarks. Furthermore, our design delivers up to 1.91$\\times$ speedup and 1.75$\\times$ energy savings over state-of-the-art accelerators. Our code is available at https://github.com/SJTU-ReArch-Group/M2XFP_ASPLOS26."}
{"id": "2601.19263", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.19263", "abs": "https://arxiv.org/abs/2601.19263", "authors": ["Aybars Yunusoglu", "Talha Coskun", "Hiruna Vishwamith", "Murat Isik", "I. Can Dikmen"], "title": "A Reconfigurable Framework for AI-FPGA Agent Integration and Acceleration", "comment": "Accepted at 27th International Symposium on Quality Electronic Design (ISQED'26)", "summary": "Artificial intelligence (AI) is increasingly deployed in real-time and energy-constrained environments, driving demand for hardware platforms that can deliver high performance and power efficiency. While central processing units (CPUs) and graphics processing units (GPUs) have traditionally served as the primary inference engines, their general-purpose nature often leads to inefficiencies under strict latency or power budgets. Field-Programmable Gate Arrays (FPGAs) offer a promising alternative by enabling custom-tailored parallelism and hardware-level optimizations. However, mapping AI workloads to FPGAs remains challenging due to the complexity of hardware-software co-design and data orchestration. This paper presents AI FPGA Agent, an agent-driven framework that simplifies the integration and acceleration of deep neural network inference on FPGAs. The proposed system employs a runtime software agent that dynamically partitions AI models, schedules compute-intensive layers for hardware offload, and manages data transfers with minimal developer intervention. The hardware component includes a parameterizable accelerator core optimized for high-throughput inference using quantized arithmetic. Experimental results demonstrate that the AI FPGA Agent achieves over 10x latency reduction compared to CPU baselines and 2-3x higher energy efficiency than GPU implementations, all while preserving classification accuracy within 0.2% of full-precision references. These findings underscore the potential of AI-FPGA co-design for scalable, energy-efficient AI deployment."}
{"id": "2601.19384", "categories": ["cs.AR", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2601.19384", "abs": "https://arxiv.org/abs/2601.19384", "authors": ["Julien Eudine", "Chu Li", "Zhuo Cheng", "Renzo Andri", "Can Firtina", "Mohammad Sadrosadati", "Nika Mansouri Ghiasi", "Konstantina Koliogeorgi", "Anirban Nag", "Arash Tavakkol", "Haiyu Mao", "Onur Mutlu", "Shai Bergman", "Ji Zhang"], "title": "GenPairX: A Hardware-Algorithm Co-Designed Accelerator for Paired-End Read Mapping", "comment": null, "summary": "Genome sequencing has become a central focus in computational biology. A genome study typically begins with sequencing, which produces millions to billions of short DNA fragments known as reads. Read mapping aligns these reads to a reference genome. Read mapping for short reads comes in two forms: single-end and paired-end, with the latter being more prevalent due to its higher accuracy and support for advanced analysis. Read mapping remains a major performance bottleneck in genome analysis due to expensive dynamic programming. Prior efforts have attempted to mitigate this cost by employing filters to identify and potentially discard computationally expensive matches and leveraging hardware accelerators to speed up the computations. While partially effective, these approaches have limitations. In particular, existing filters are often ineffective for paired-end reads, as they evaluate each read independently and exhibit relatively low filtering ratios. In this work, we propose GenPairX, a hardware-algorithm co-designed accelerator that efficiently minimizes the computational load of paired-end read mapping while enhancing the throughput of memory-intensive operations. GenPairX introduces: (1) a novel filtering algorithm that jointly considers both reads in a pair to improve filtering effectiveness, and a lightweight alignment algorithm to replace most of the computationally expensive dynamic programming operations, and (2) two specialized hardware mechanisms to support the proposed algorithms. Our evaluations show that GenPairX delivers substantial performance improvements over state-of-the-art solutions, achieving 1575x and 1.43x higher throughput per watt compared to leading CPU-based and accelerator-based read mappers, respectively, all without compromising accuracy."}
{"id": "2601.19747", "categories": ["cs.AR", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19747", "abs": "https://arxiv.org/abs/2601.19747", "authors": ["Jiale Liu", "Taiyu Zhou", "Tianqi Jiang"], "title": "Veri-Sure: A Contract-Aware Multi-Agent Framework with Temporal Tracing and Formal Verification for Correct RTL Code Generation", "comment": null, "summary": "In the rapidly evolving field of Electronic Design Automation (EDA), the deployment of Large Language Models (LLMs) for Register-Transfer Level (RTL) design has emerged as a promising direction. However, silicon-grade correctness remains bottlenecked by: (i) limited test coverage and reliability of simulation-centric evaluation, (ii) regressions and repair hallucinations introduced by iterative debugging, and (iii) semantic drift as intent is reinterpreted across agent handoffs. In this work, we propose Veri-Sure, a multi-agent framework that establishes a design contract to align agents' intent and uses a patching mechanism guided by static dependency slicing to perform precise, localized repairs. By integrating a multi-branch verification pipeline that combines trace-driven temporal analysis with formal verification consisting of assertion-based checking and boolean equivalence proofs, Veri-Sure enables functional correctness beyond pure simulations. We also introduce VerilogEval-v2-EXT, extending the original benchmark with 53 more industrial-grade design tasks and stratified difficulty levels, and show that Veri-Sure achieves state-of-the-art verified-correct RTL code generation performance, surpassing standalone LLMs and prior agentic systems."}
{"id": "2601.18983", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.18983", "abs": "https://arxiv.org/abs/2601.18983", "authors": ["Dimitrios Tomaras", "Vana Kalogeraki", "Dimitrios Gunopulos"], "title": "Trustworthy Scheduling for Big Data Applications", "comment": "6 pages", "summary": "Recent advances in modern containerized execution environments have resulted in substantial benefits in terms of elasticity and more efficient utilization of computing resources. Although existing schedulers strive to optimize performance metrics like task execution times and resource utilization, they provide limited transparency into their decision-making processes or the specific actions developers must take to meet Service Level Objectives (SLOs). In this work, we propose X-Sched, a middleware that uses explainability techniques to generate actionable guidance on resource configurations that makes task execution in containerized environments feasible, under resource and time constraints. X-Sched addresses this gap by integrating counterfactual explanations with advanced machine learning models, such as Random Forests, to efficiently identify optimal configurations. This approach not only ensures that tasks are executed in line with performance goals but also gives users clear, actionable insights into the rationale behind scheduling decisions. Our experimental results validated with data from real-world execution environments, illustrate the efficiency, benefits and practicality of our approach."}
{"id": "2601.19207", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.19207", "abs": "https://arxiv.org/abs/2601.19207", "authors": ["Matthew Britton", "Sasha Pak", "Alex Potanin"], "title": "Refactoring and Equivalence in Rust: Expanding the REM Toolchain with a Novel Approach to Automated Equivalence Proofs", "comment": null, "summary": "Refactoring tools are central to modern development, with extract-function refactorings used heavily in day-to-day work. For Rust, however, ownership, borrowing, and advanced type features make automated extract-function refactoring challenging. Existing tools either rely on slow compiler-based analysis, support only restricted language fragments, or provide little assurance beyond \"it still compiles.\" This paper presents REM2.0, a new extract-function and verification toolchain for Rust. REM2.0 works atop rust-analyzer as a persistent daemon, providing low-latency refactorings with a VSCode front-end. It adds a repairer that automatically adjusts lifetimes and signatures when extraction exposes borrow-checker issues, and an optional verification pipeline connecting to CHARON and AENEAS to generate Coq equivalence proofs for a supported Rust subset. The architecture is evaluated on three benchmark suites. On the original REM artefact, REM2.0 achieves 100% compatibility while reducing latency from ~1000ms to single-digit milliseconds in the daemon. On 40 feature-focused extractions from 20 highly starred GitHub repositories, REM2.0 handles most examples involving async/await, const fn, non-local control flow, generics, and higher-ranked trait bounds. On twenty verification benchmarks, the CHARON/AENEAS pipeline constructs end-to-end equivalence proofs for cases within its current subset. Overall, results show that a rust-analyzer-based design can provide fast, feature-rich extract-function refactoring for real Rust programs, while opt-in verification delivers machine-checked behaviour preservation."}
{"id": "2601.19092", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.19092", "abs": "https://arxiv.org/abs/2601.19092", "authors": ["Bohan Hou", "Hongyi Jin", "Guanjie Wang", "Jinqi Chen", "Yaxing Cai", "Lijie Yang", "Zihao Ye", "Yaoyao Ding", "Ruihang Lai", "Tianqi Chen"], "title": "Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers", "comment": null, "summary": "Scaling modern deep learning workloads demands coordinated placement of data and compute across device meshes, memory hierarchies, and heterogeneous accelerators. We present Axe Layout, a hardware-aware abstraction that maps logical tensor coordinates to a multi-axis physical space via named axes. Axe unifies tiling, sharding, replication, and offsets across inter-device distribution and on-device layouts, enabling collective primitives to be expressed consistently from device meshes to threads. Building on Axe, we design a multi-granularity, distribution-aware DSL and compiler that composes thread-local control with collective operators in a single kernel. Experiments show that our unified approach can bring performance close to hand-tuned kernels on across latest GPU devices and multi-device environments and accelerator backends."}
{"id": "2601.19426", "categories": ["cs.PL", "cs.LO", "math.CT"], "pdf": "https://arxiv.org/pdf/2601.19426", "abs": "https://arxiv.org/abs/2601.19426", "authors": ["Samy Avrillon", "Ambrus Kaposi", "Ambroise Lafont", "Niyousha Najmaei", "Johann Rosain"], "title": "For Generalised Algebraic Theories, Two Sorts Are Enough", "comment": null, "summary": "Generalised algebraic theories (GATs) allow multiple sorts indexed over each other. For example, the theories of categories or Martin-L{รถ}f type theories form GATs. Categories have two sorts, objects and morphisms, and the latter are double-indexed over the former. Martin-L{รถ}f type theory has four sorts: contexts, substitutions, types and terms. For example, types are indexed over contexts, and terms are indexed over both contexts and types. In this paper we show that any GAT can be reduced to a GAT with only two sorts, and there is a section-retraction correspondence (formally, a strict coreflection) between models of the original and the reduced GAT. In particular, any model of the original GAT can be turned into a model of the reduced (two-sorted) GAT and back, and this roundtrip is the identity.\n  The reduced GAT is simpler than the original GAT in the following aspects: it does not have sort equalities; it does not have interleaved sorts and operations; if the original GAT did not have interleaved sorts and operations, then the reduced GAT won't have operations interleaved between different sorts. In a type-theoretic metatheory, the initial algebra of a GAT is called a quotient inductive-inductive type (QIIT). Our reduction provides a way to implement QIITs with sort equalities or interleaved constructors which are not allowed by Cubical Agda. An instance of our reduction is the well-known method of reducing mutual inductive types to a single indexed family. Our approach is semantic in that it does not rely on a syntactic description of GATs, but instead, on Uemura's bi-initial characterisation of the category of (finite) GATs in the 2-category of finitely complete categories with a chosen exponentiable morphism."}
{"id": "2601.19160", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.19160", "abs": "https://arxiv.org/abs/2601.19160", "authors": ["Sheng Qi", "Zhiquan Zhang", "Xuanzhe Liu", "Xin Jin"], "title": "KUBEDIRECT: Unleashing the Full Power of the Cluster Manager for Serverless Computing", "comment": "Accepted by NSDI'26", "summary": "FaaS platforms rely on cluster managers like Kubernetes for resource management. Kubernetes is popular due to its state-centric APIs that decouple the control plane into modular controllers. However, to scale out a burst of FaaS instances, message passing becomes the primary bottleneck as controllers have to exchange extensive state through the API Server. Existing solutions opt for a clean-slate redesign of cluster managers, but at the expense of compatibility with existing ecosystem and substantial engineering effort.\n  We present KUBEDIRECT, a Kubernetes-based cluster manager for FaaS. We find that there exists a common narrow waist across FaaS platform that allows us to achieve both efficiency and external compatibility. Our insight is that the sequential structure of the narrow waist obviates the need for a single source of truth, allowing us to bypass the API Server and perform direct message passing for efficiency. However, our approach introduces a set of ephemeral states across controllers, making it challenging to enforce end-to-end semantics due to the absence of centralized coordination. KUBEDIRECT employs a novel state management scheme that leverages the narrow waist as a hierarchical write-back cache, ensuring consistency and convergence to the desired state. KUBEDIRECT can seamlessly integrate with Kubernetes, adding ~150 LoC per controller. Experiments show that KUBEDIRECT reduces serving latency by 26.7x over Knative, and has similar performance as the state-of-the-art clean-slate platform Dirigent."}
{"id": "2601.19092", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.19092", "abs": "https://arxiv.org/abs/2601.19092", "authors": ["Bohan Hou", "Hongyi Jin", "Guanjie Wang", "Jinqi Chen", "Yaxing Cai", "Lijie Yang", "Zihao Ye", "Yaoyao Ding", "Ruihang Lai", "Tianqi Chen"], "title": "Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers", "comment": null, "summary": "Scaling modern deep learning workloads demands coordinated placement of data and compute across device meshes, memory hierarchies, and heterogeneous accelerators. We present Axe Layout, a hardware-aware abstraction that maps logical tensor coordinates to a multi-axis physical space via named axes. Axe unifies tiling, sharding, replication, and offsets across inter-device distribution and on-device layouts, enabling collective primitives to be expressed consistently from device meshes to threads. Building on Axe, we design a multi-granularity, distribution-aware DSL and compiler that composes thread-local control with collective operators in a single kernel. Experiments show that our unified approach can bring performance close to hand-tuned kernels on across latest GPU devices and multi-device environments and accelerator backends."}
{"id": "2601.19362", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.19362", "abs": "https://arxiv.org/abs/2601.19362", "authors": ["Xinyi Wan", "Penghui Qi", "Guangxing Huang", "Chaoyi Ruan", "Min Lin", "Jialin Li"], "title": "Revisiting Parameter Server in LLM Post-Training", "comment": "Accepted in ICLR'26", "summary": "Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose \\textbf{On-Demand Communication (ODC)}, which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc."}
{"id": "2601.19563", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.19563", "abs": "https://arxiv.org/abs/2601.19563", "authors": ["Juan Zhu", "Zixin Wang", "Shenghui Song", "Jun Zhang", "Khaled Ben Letaief"], "title": "Modular Foundation Model Inference at the Edge: Network-Aware Microservice Optimization", "comment": "4 figures. Conference", "summary": "Foundation models (FMs) unlock unprecedented multimodal and multitask intelligence, yet their cloud-centric deployment precludes real-time responsiveness and compromises user privacy. Meanwhile, monolithic execution at the edge remains infeasible under stringent resource limits and uncertain network dynamics. To bridge this gap, we propose a microservice-based FM inference framework that exploits the intrinsic functional asymmetry between heavyweight core services and agile light services. Our two-tier deployment strategy ensures robust Quality of Service (QoS) under resource contention. Specifically, core services are placed statically via a long-term network-aware integer program with sparsity constraints to form a fault-tolerant backbone. On the other hand, light services are orchestrated dynamically by a low-complexity online controller that integrates effective capacity theory with Lyapunov optimization, providing probabilistic latency guarantees under real-time workload fluctuations. Simulations demonstrate that our framework achieves over 84% average on-time task completion with moderate deployment costs and maintains strong robustness as the system load scales."}
