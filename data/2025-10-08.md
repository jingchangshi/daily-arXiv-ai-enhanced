<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 17]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices](https://arxiv.org/abs/2510.05109)
*Yilong Li,Shuai Zhang,Yijing Zeng,Hao Zhang,Xinmiao Xiong,Jingyu Liu,Pan Hu,Suman Banerjee*

Main category: cs.DC

TL;DR: NANOMIND是一个硬件-软件协同设计的大模型推理框架，通过将大模型分解为模块化组件并在异构加速器上动态调度，实现更高的能效和性能。


<details>
  <summary>Details</summary>
Motivation: 现有大模型通常以整体方式运行，无法充分利用现代SoC中的异构加速器（NPU、GPU、DSP等），导致高延迟和低能效。

Method: 将大模型分解为视觉、语言、音频等模块化"砖块"，通过模块级动态卸载技术将每个模块映射到最合适的加速器上运行，结合定制硬件设计、系统级调度和优化的低位计算内核。

Result: 系统在资源效率上优于现有实现，能耗降低42.3%，GPU内存使用减少11.2%，电池供电设备可运行LLaVA-OneVision近半天，LLaMA-3-8B语音交互达20.8小时。

Conclusion: NANOMIND框架通过模块化分解和智能调度，实现了在资源受限设备上高效运行大模型，为边缘智能设备提供了可行的解决方案。

Abstract: Large Multimodal Models (LMMs) are inherently modular, consisting of vision
and audio encoders, projectors, and large language models. Yet, they are almost
always executed monolithically, which underutilizes the heterogeneous
accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end
latency. In this paper, we present NANOMIND, a hardware--software co-design
inference framework for Large Multimodal Models (LMMs) that breaks large models
into modular ``bricks'' (vision, language, audio, etc.) and maps each to its
ideal accelerator. The key insight is that large models can be broken into
modular components and scheduled to run on the most appropriate compute units.
It performs module-level dynamic offloading across accelerators on
unified-memory SoCs. By combining customized hardware design, system-level
scheduling, and optimized low-bit computation kernels, we demonstrate our
framework with a compact, battery-powered device capable of running LMMs
entirely on device. This prototype functions as a self-contained intelligent
assistant that requires no network connectivity, while achieving higher
throughput and superior power efficiency under strict resource constraints. The
design further bypasses CPU bottlenecks and reduces redundant memory usage
through token-aware buffer management and module-level coordination. Our system
outperforms existing implementations in resource efficiency, cutting energy
consumption by 42.3\% and GPU memory usage by 11.2\%. This enables a
battery-powered device to run LLaVA-OneVision with a camera for nearly half a
day and LLaMA-3-8B for voice interactions up to almost 20.8 hours.

</details>


### [2] [cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications](https://arxiv.org/abs/2510.05476)
*Xi Wang,Bin Ma,Jongryool Kim,Byungil Koh,Hoshik Kim,Dong Li*

Main category: cs.DC

TL;DR: cMPI是首个在真实CXL平台上利用CXL内存共享优化MPI点对点通信的工作，将跨节点通信转换为CXL内存内的内存事务和数据拷贝，绕过传统网络协议。


<details>
  <summary>Details</summary>
Motivation: 传统MPI库使用复杂软件栈的网络互连和协议进行跨节点通信，存在性能瓶颈。CXL内存共享技术提供了新的通信优化机会。

Method: 通过CXL内存共享技术，将MPI点对点通信（包括单边和双边通信）转换为CXL内存内的内存事务和数据拷贝，解决了CXL内存共享在MPI通信中的数据对象管理、缓存一致性和原子操作等挑战。

Result: CXL内存共享相比TCP互连实现7.2x-8.1x的延迟降低，在小型消息通信中，相比标准以太网NIC和高性能SmartNIC分别实现49x和72x的延迟和带宽提升。

Conclusion: CXL内存共享技术能够显著优化MPI通信性能，为高性能计算提供新的通信优化途径。

Abstract: Message Passing Interface (MPI) is a foundational programming model for
high-performance computing. MPI libraries traditionally employ network
interconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP
and RoCE) with complex software stacks for cross-node communication. We present
cMPI, the first work to optimize MPI point-to-point communication (both
one-sided and two-sided) using CXL memory sharing on a real CXL platform,
transforming cross-node communication into memory transactions and data copies
within CXL memory, bypassing traditional network protocols. We analyze
performance across various interconnects and find that CXL memory sharing
achieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in
small- and medium-scale clusters. We address challenges of CXL memory sharing
for MPI communication, including data object management over the dax
representation [50], cache coherence, and atomic operations. Overall, cMPI
outperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x
and 72x in latency and bandwidth, respectively, for small messages.

</details>


### [3] [Agora: Bridging the GPU Cloud Resource-Price Disconnect](https://arxiv.org/abs/2510.05111)
*Ian McDougall,Noah Scott,Joon Huh,Kirthevasan Kandasamy,Karthikeyan Sankaralingam*

Main category: cs.DC

TL;DR: 本文提出了一种基于特征的GPU云服务定价框架，以解决FLOPs与内存带宽之间的价格性能脱节问题，并通过Agora系统实现高效定价。


<details>
  <summary>Details</summary>
Motivation: 现代GPU的浮点运算能力持续提升而内存带宽增长滞后，导致基于时间的传统定价模型对带宽密集型工作负载不经济，造成市场扭曲和硬件分配效率低下。

Method: 提出基于特征的定价框架，将成本与资源消耗直接关联，包括内存带宽等关键指标，并设计Agora系统架构实现安全高效的资源测量和定价。

Result: 实验表明50微秒采样可实现近乎理想的定价效果，仅损失5%收入；10微秒采样效果更好，仅损失2.4%收入。现代遥测系统已能支持这种测量频率。

Conclusion: 基于特征的定价框架能够为云GPU资源创建更透明高效的市场，通过Agora系统的实际验证证明了该方法的可行性和有效性。

Abstract: The historic trend of Moore's Law, which predicted exponential growth in
computational performance per dollar, has diverged for modern Graphics
Processing Units (GPUs). While Floating Point Operations per Second (FLOPs)
capabilities have continued to scale economically, memory bandwidth has not,
creating a significant price-performance disconnect. This paper argues that the
prevailing time-based pricing models for cloud GPUs are economically
inefficient for bandwidth-bound workloads. These models fail to account for the
rising marginal cost of memory bandwidth, leading to market distortions and
suboptimal hardware allocation. To address this, we propose a novel
feature-based pricing framework that directly links cost to resource
consumption, including but not limited to memory bandwidth. We provide a robust
economic and algorithmic definition of this framework and introduce Agora, a
practical and secure system architecture for its implementation. Our
implementation of Agora shows that a 50us sampling provides nearly perfect
pricing as what ideal sampling would provide - losing only 5\% of revenue. 10us
sampling is even better result in 2.4\% loss. Modern telemetry systems can
already provide this rate of measurement, and our prototype implementation
shows the system design for feature-based pricing is buildable. Our evaluation
across diverse GPU applications and hardware generations empirically validates
the effectiveness of our approach in creating a more transparent and efficient
market for cloud GPU resources.

</details>


### [4] [Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting](https://arxiv.org/abs/2510.05497)
*Zhongkai Yu,Yue Guan,Zihao Yu,Chenyang Zhou,Shuyi Pei,Yangwook Kang,Yufei Ding,Po-An Tsai*

Main category: cs.DC

TL;DR: 该论文对MoE架构LLM的数据移动瓶颈进行了全面分析，通过大规模测试发现了6个关键洞察，并在晶圆级GPU上实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: MoE架构LLM的随机专家选择机制在多单元服务系统中引入了显著的数据移动开销，成为主要性能瓶颈。

Method: 对三个最先进的大规模MoE模型（200B-671B）进行数据移动中心化分析，使用超过24,000个请求，生成150GB+的跟踪文件，从时间和空间角度进行系统分析。

Result: 提炼出6个关键设计洞察，在晶圆级GPU上通过架构修改实现了DeepSeek V3 6.3倍和Qwen3 4.0倍的平均加速。

Conclusion: 这是首个大规模MoE模型的全面数据中心化分析，为未来服务系统设计提供了重要指导，相关数据和框架将公开发布。

Abstract: Large Language Models (LLMs) with Mixture of Experts (MoE) architectures
achieve remarkable performance improvements, but their random expert selection
mechanism introduces significant data movement overhead that becomes the
dominant bottleneck in multi-unit serving systems. To forecast the patterns
underlying this data movement, we conduct comprehensive data-movement-centric
profiling across three state-of-the-art large-scale MoE models (200B- 671B)
using over 24,000 requests spanning diverse workloads. With the resulting
150GB+ trace files, we perform systematic analysis from both temporal and
spatial perspectives and distill six key insights to guide the design of
diverse future serving systems. Taking wafer-scale GPUs as a case study, we
demonstrate that minor architectural modifications leveraging our insights
achieve substantial performance gains, delivering 6.3X and 4.0X average
speedups on DeepSeek V3 and Qwen3, respectively. Our work provides the first
comprehensive data-centric analysis of MoE models at scale. Our profiling
traces and analysis results are publicly available at
{https://huggingface.co/datasets/core12345/MoE_expert_selection_trace. We will
also release our simulation framework shortly to facilitate future research in
this area.

</details>


### [5] [A Flexible Programmable Pipeline Parallelism Framework for Efficient DNN Training](https://arxiv.org/abs/2510.05112)
*Lijuan Jiang,Xingjian Qian,Zhenxiang Ma,Zan Zong,Hengjie Li,Chao Yang,Jidong Zhai*

Main category: cs.DC

TL;DR: FlexPipe是一个可编程的流水线并行框架，通过领域特定语言和自动调度器实现高效的流水线调度探索和定制，相比现有框架获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方法依赖预定义调度策略，无法自动适应新兴模型架构，且手动实现调度策略编码负担重、灵活性差。

Method: 提出FlexPipe框架，包含简洁的领域特定语言(DSL)和自动调度器，支持广泛的调度类型探索和灵活定制。

Result: 相比主流大规模并行框架Megtron-LM性能提升达2.28倍，相比最先进的自动流水线并行框架性能提升达1.49倍。

Conclusion: FlexPipe通过增强的生产力、可编程性、可调试性和易调优性，为流水线并行提供了高效的自动化调度解决方案。

Abstract: Pipeline parallelism is an essential distributed parallelism method.
Increasingly complex and diverse DNN models necessitate meticulously customized
pipeline schedules for performance. However, existing practices typically rely
on predefined schedules, each with strengths, but fail to adapt automatically
to the emerging model architectures. Exploring novel high-efficiency schedules
is daunting due to the enormous and varying schedule space. Besides, manually
implementing schedules can be challenging due to the onerous coding burdens and
constantly changing needs. Unfortunately, existing frameworks have limitations
in automated schedule exploration and lack flexibility and controllability.
  This paper presents FlexPipe, a programmable pipeline parallelism framework
with enhanced productivity, programmability, debuggability, and ease of tuning.
FlexPipe has two main components: a succinct domain-specific language (DSL) and
an automated scheduler. FlexPipe enables automated schedule exploration for
various parallel scenarios within a broad spectrum of schedule types at a small
search cost. Besides, users can swiftly develop and customize schedules using
the FlexPipe DSL, which embodies flexible controllability in the pipeline order
of micro-batch computations over stages. It also provides convenient mechanisms
to include new operations in schedules to meet changing demands. Our evaluation
results demonstrate that FlexPipe achieves up to 2.28X performance speedup
compared to the popular large-scale parallel framework Megtron-LM, and gains up
to 1.49X performance speedup compared to the state-of-the-art automated
pipeline parallelism framework.

</details>


### [6] [Lumos: Performance Characterization of WebAssembly as a Serverless Runtime in the Edge-Cloud Continuum](https://arxiv.org/abs/2510.05118)
*Cynthia Marcelino,Noah Krennmair,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Lumos是一个用于评估服务器无运行时性能的模型和基准测试工具，特别关注WebAssembly在边缘云连续体中的表现。研究发现AoT编译的Wasm镜像比容器小30倍，冷启动延迟降低16%，但解释型Wasm的延迟高55倍且I/O序列化开销大10倍。


<details>
  <summary>Details</summary>
Motivation: WebAssembly作为轻量级可移植运行时在边缘云连续体等资源受限环境中执行无服务器函数，但其性能优势与权衡尚未被充分理解。

Method: 开发Lumos性能模型和基准测试工具，识别边缘云连续体中的工作负载、系统和环境级性能驱动因素，对最先进的容器和Wasm运行时（解释模式和AoT编译模式）进行基准测试。

Result: AoT编译的Wasm镜像比容器小30倍，冷启动延迟降低16%；解释型Wasm的延迟高55倍，I/O序列化开销大10倍。

Conclusion: AoT编译的Wasm在边缘云连续体中具有显著的性能和资源效率优势，但解释型Wasm存在严重的性能问题。

Abstract: WebAssembly has emerged as a lightweight and portable runtime to execute
serverless functions, particularly in heterogeneous and resource-constrained
environments such as the Edge Cloud Continuum. However, the performance
benefits versus trade-offs remain insufficiently understood. This paper
presents Lumos, a performance model and benchmarking tool for characterizing
serverless runtimes. Lumos identifies workload, system, and environment-level
performance drivers in the Edge-Cloud Continuum. We benchmark state-of-the-art
containers and the Wasm runtime in interpreted mode and with ahead-of-time
compilation. Our performance characterization shows that AoT-compiled Wasm
images are up to 30x smaller and decrease cold-start latency by up to 16%
compared to containers, while interpreted Wasm suffers up to 55x higher warm
latency and up to 10x I/O-serialization overhead.

</details>


### [7] [Artificial Intelligence for Cost-Aware Resource Prediction in Big Data Pipelines](https://arxiv.org/abs/2510.05127)
*Harshit Goyal*

Main category: cs.DC

TL;DR: 使用随机森林回归预测大数据管道资源利用率的AI方法，在Google Borg集群数据上实现高精度预测，为云环境成本感知自动扩缩容提供支持。


<details>
  <summary>Details</summary>
Motivation: 现代云计算中高效资源分配是关键挑战，过度配置导致不必要成本，配置不足则存在性能下降和SLA违规风险。

Method: 预处理Google Borg集群跟踪数据，清理、转换并提取相关特征（CPU、内存、使用分布），使用随机森林回归模型预测资源利用率。

Result: 模型实现高预测精度（R平方=0.99，MAE=0.0048，RMSE=0.137），捕捉了工作负载特征与资源利用率之间的非线性关系。对小到中等规模作业表现优异，大规模作业方差较高。

Conclusion: AI驱动预测在云环境中具有成本感知自动扩缩容的潜力，可减少不必要的资源配置同时保障服务质量。

Abstract: Efficient resource allocation is a key challenge in modern cloud computing.
Over-provisioning leads to unnecessary costs, while under-provisioning risks
performance degradation and SLA violations. This work presents an artificial
intelligence approach to predict resource utilization in big data pipelines
using Random Forest regression. We preprocess the Google Borg cluster traces to
clean, transform, and extract relevant features (CPU, memory, usage
distributions). The model achieves high predictive accuracy (R Square = 0.99,
MAE = 0.0048, RMSE = 0.137), capturing non-linear relationships between
workload characteristics and resource utilization. Error analysis reveals
impressive performance on small-to-medium jobs, with higher variance in rare
large-scale jobs. These results demonstrate the potential of AI-driven
prediction for cost-aware autoscaling in cloud environments, reducing
unnecessary provisioning while safeguarding service quality.

</details>


### [8] [FlashResearch: Real-time Agent Orchestration for Efficient Deep Research](https://arxiv.org/abs/2510.05145)
*Lunyiu Nie,Nedim Lipka,Ryan A. Rossi,Swarat Chaudhuri*

Main category: cs.DC

TL;DR: FlashResearch是一个高效深度研究框架，通过将顺序推理转换为并行运行时编排，动态分解复杂查询为树状子任务，实现5倍加速同时保持质量。


<details>
  <summary>Details</summary>
Motivation: 深度研究代理的顺序推理过程存在高延迟、运行时适应性差和资源分配低效的问题，限制了交互式应用的实际使用。

Method: 提出自适应规划器动态分配计算资源，实时编排层监控研究进度并剪枝冗余路径，多维度并行化框架在研究广度和深度上实现并发。

Result: 实验显示FlashResearch在固定时间预算内持续提升最终报告质量，在保持可比质量的同时实现高达5倍的加速。

Conclusion: FlashResearch通过并行运行时编排有效解决了深度研究代理的顺序处理瓶颈，为交互式研究应用提供了实用解决方案。

Abstract: Deep research agents, which synthesize information across diverse sources,
are significantly constrained by their sequential reasoning processes. This
architectural bottleneck results in high latency, poor runtime adaptability,
and inefficient resource allocation, making them impractical for interactive
applications. To overcome this, we introduce FlashResearch, a novel framework
for efficient deep research that transforms sequential processing into
parallel, runtime orchestration by dynamically decomposing complex queries into
tree-structured sub-tasks. Our core contributions are threefold: (1) an
adaptive planner that dynamically allocates computational resources by
determining research breadth and depth based on query complexity; (2) a
real-time orchestration layer that monitors research progress and prunes
redundant paths to reallocate resources and optimize efficiency; and (3) a
multi-dimensional parallelization framework that enables concurrency across
both research breadth and depth. Experiments show that FlashResearch
consistently improves final report quality within fixed time budgets, and can
deliver up to a 5x speedup while maintaining comparable quality.

</details>


### [9] [Percepta: High Performance Stream Processing at the Edge](https://arxiv.org/abs/2510.05149)
*Clarisse Sousa,Tiago Fonseca,Luis Lino Ferreira,Ricardo Venâncio,Ricardo Severino*

Main category: cs.DC

TL;DR: Percepta是一个轻量级数据流处理系统，专门为边缘计算环境中的AI工作负载设计，特别支持强化学习等应用。


<details>
  <summary>Details</summary>
Motivation: 实时数据和物联网设备的普及暴露了云中心解决方案在延迟、带宽和隐私方面的局限性，推动了边缘计算的发展。物联网带来的数据速率协调、协议转换、数据丢失处理以及与AI模型集成等问题需要解决。

Method: 开发了Percepta系统，具备奖励函数计算、模型重训练数据存储、实时数据准备等专门功能，支持持续决策制定。还包括数据标准化、异构协议和采样率协调、以及缺失数据鲁棒处理等功能。

Result: Percepta系统能够有效处理边缘AI部署中的各种挑战，为强化学习等AI工作负载提供专门支持。

Conclusion: Percepta是一个适合边缘AI部署的轻量级数据流处理系统，能够解决物联网环境中的数据协调、协议转换和AI集成等关键问题。

Abstract: The rise of real-time data and the proliferation of Internet of Things (IoT)
devices have highlighted the limitations of cloud-centric solutions,
particularly regarding latency, bandwidth, and privacy. These challenges have
driven the growth of Edge Computing. Associated with IoT appears a set of other
problems, like: data rate harmonization between multiple sources, protocol
conversion, handling the loss of data and the integration with Artificial
Intelligence (AI) models. This paper presents Percepta, a lightweight Data
Stream Processing (DSP) system tailored to support AI workloads at the edge,
with a particular focus on such as Reinforcement Learning (RL). It introduces
specialized features such as reward function computation, data storage for
model retraining, and real-time data preparation to support continuous
decision-making. Additional functionalities include data normalization,
harmonization across heterogeneous protocols and sampling rates, and robust
handling of missing or incomplete data, making it well suited for the
challenges of edge-based AI deployment.

</details>


### [10] [SATER: A Self-Aware and Token-Efficient Approach to Routing and Cascading](https://arxiv.org/abs/2510.05164)
*Yuanzhe Shen,Yide Liu,Zisu Huang,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.DC

TL;DR: SATER是一个双模式兼容方法，通过最短响应偏好优化和置信度感知拒绝机制来微调模型，显著减少冗余输出和响应时间，同时提高预生成路由的性能和级联路由的效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能优异但依赖昂贵的商业API或云服务，而小型语言模型成本较低但能力有限。现有路由策略各有优缺点，需要解决性能和成本之间的权衡问题。

Method: SATER采用双模式兼容方法，通过最短响应偏好优化和置信度感知拒绝机制来微调模型，减少冗余输出和响应时间。

Result: 在三个小型语言模型和六个不同类型和复杂度的数据集上的实验表明，SATER在保持相当性能的同时，计算成本降低超过50%，级联延迟降低超过80%。

Conclusion: SATER有效解决了语言模型选择中性能与成本的权衡问题，显著提高了路由策略的效率和效果。

Abstract: Large language models (LLMs) demonstrate remarkable performance across
diverse tasks, yet their effectiveness frequently depends on costly commercial
APIs or cloud services. Model selection thus entails a critical trade-off
between performance and cost: high-performing LLMs typically incur substantial
expenses, whereas budget-friendly small language models (SLMs) are constrained
by limited capabilities. Current research primarily proposes two routing
strategies: pre-generation routing and cascade routing. Both approaches have
distinct characteristics, with cascade routing typically offering superior
cost-effectiveness and accuracy despite its higher latency. To further address
the limitations of both approaches, we introduce SATER, a dual-mode compatible
approach that fine-tunes models through shortest-response preference
optimization and a confidence-aware rejection mechanism. SATER significantly
reduces redundant outputs and response times, while improving both the
performance of pre-generation routing and the efficiency of cascade routing.
Experiments across three SLMs and six datasets, varying in type and complexity,
demonstrate that SATER achieves comparable performance while consistently
reducing computational costs by over 50\% and cascade latency by over 80\%.

</details>


### [11] [OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training](https://arxiv.org/abs/2510.05186)
*Hongpei Li,Han Zhang,Huikang Liu,Dongdong Ge,Yinyu Ye*

Main category: cs.DC

TL;DR: 提出了一种基于优化视角的流水线调度方法，通过联合考虑内存容量、激活重用和流水线气泡最小化，动态优化内存与时间的权衡，相比现有方法显著提升了吞吐量和内存利用率。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方法虽然通过激活卸载减少了内存消耗，但大多是启发式和粗粒度的，未能充分考虑内存、计算和调度延迟之间的细粒度权衡。

Method: 将流水线调度建模为约束优化问题，联合考虑内存容量、激活重用和流水线气泡最小化，通过求解该模型生成细粒度调度方案。

Result: 实验结果显示，在相同设备内存限制下，流水线空闲时间减少高达50%，在某些情况下能够在有限内存预算内训练更大模型。

Conclusion: 该方法从优化角度重新审视流水线调度问题，动态优化内存与时间的权衡，相比固定模式的现有方法，在吞吐量和内存利用率方面均有显著提升。

Abstract: Pipeline parallelism (PP) has become a standard technique for scaling large
language model (LLM) training across multiple devices. However, despite recent
progress in reducing memory consumption through activation offloading, existing
approaches remain largely heuristic and coarse-grained, often overlooking the
fine-grained trade-offs between memory, computation, and scheduling latency. In
this work, we revisit the pipeline scheduling problem from a principled
optimization perspective. We observe that prevailing strategies either rely on
static rules or aggressively offload activations without fully leveraging the
interaction between memory constraints and scheduling efficiency. To address
this, we formulate scheduling as a constrained optimization problem that
jointly accounts for memory capacity, activation reuse, and pipeline bubble
minimization. Solving this model yields fine-grained schedules that reduce
pipeline bubbles while adhering to strict memory budgets. Our approach
complements existing offloading techniques: whereas prior approaches trade
memory for time in a fixed pattern, we dynamically optimize the tradeoff with
respect to model structure and hardware configuration. Experimental results
demonstrate that our method consistently improves both throughput and memory
utilization. In particular, we reduce idle pipeline time by up to 50% under the
same per-device memory limit, and in some cases, enable the training of larger
models within limited memory budgets.

</details>


### [12] [Performance of a high-order MPI-Kokkos accelerated fluid solver](https://arxiv.org/abs/2510.05254)
*Filipp Sporykhin,Holger Homann*

Main category: cs.DC

TL;DR: 该论文研究了高阶间断Galerkin方法在现代高性能计算架构上的性能表现，发现八阶方法比低阶方法更快，GPU在大规模计算中性能优于CPU但能耗表现复杂。


<details>
  <summary>Details</summary>
Motivation: 研究现代数值方法在高性能计算架构上的性能，特别是针对多核CPU和GPU加速器，以优化计算效率和能耗。

Method: 使用Kokkos库和MPI实现单源代码，在多种GPU系统上运行空间节点间断Galerkin方案（最高八阶）和时间Runge-Kutta方法（最高六阶），求解线性对流方程和等温欧拉方程。

Result: 高阶方法计算更快，八阶模拟比三阶或四阶更快达到相同全局误差；GPU在大规模计算（超过10^7自由度）中显著优于CPU，但小网格计算CPU更快；新GPU需要更大网格才能高效使用。

Conclusion: 高阶数值方法在现代硬件上具有性能优势，GPU适合大规模计算但能耗表现存在反弹效应，新GPU需要更大计算规模才能发挥能效优势。

Abstract: This work discusses the performance of a modern numerical scheme for fluid
dynamical problems on modern high-performance computing architectures. Our code
implements a spatial nodal discontinuous Galerkin scheme that we test up to an
order of convergence of eight. It is temporally coupled to a set of Runge-Kutta
methods of orders up to six. The code integrates the linear advection equations
as well as the isothermal Euler equations in one, two, and three dimensions. In
order to target modern hardware involving many-core Central Processing Units
and accelerators such as Graphic Processing Units we use the Kokkos library in
conjunction with the Message Passing Interface to run our single source code on
various GPU systems. We find that the higher the order the faster is the code.
Eighth-order simulations attain a given global error with much less computing
time than third- or fourth-order simulations. The RK scheme has a smaller
impact on the code performance and a classical fourth-order scheme seems to
generally be a good choice. The code performs very well on all considered GPUs.
The many-CPU performance is also very good and perfect weak scaling is observed
up to many hundreds of CPU cores using MPI. We note that small grid-size
simulations are faster on CPUs than on GPUs while GPUs win significantly over
CPUs for simulations involving more than $10^7$ degrees of freedom ($\approx
3100^2$ grid points). When it comes to the environmental impact of numerical
simulations we estimate that GPUs consume less energy than CPUs for large
grid-size simulations but more energy on small grids. We observe a tendency
that the more modern is the GPU the larger needs to be the grid in order to use
it efficiently. This yields a rebound effect because larger simulations need
longer computing times and in turn more energy that is not compensated by the
energy efficiency gain of the newer GPUs.

</details>


### [13] [Toward Systems Foundations for Agentic Exploration](https://arxiv.org/abs/2510.05556)
*Jiakai Xu,Tianle Zhou,Eugene Wu,Kostis Kaffes*

Main category: cs.DC

TL;DR: 论文分析了现有快照/恢复机制在支持LLM智能体探索方面的不足，指出了三个核心挑战：分支语义、外部副作用和原生分支技术。


<details>
  <summary>Details</summary>
Motivation: 当前通用的快照/恢复工具（如CRIU或容器提交）在支持LLM智能体的分支回溯探索时性能不足，特别是在真实部署环境中无法有效处理共享资源。

Method: 通过基准测试评估了六种快照/恢复机制，分析了它们在隔离测试环境和真实部署场景中的表现。

Result: 研究发现通用工具即使在隔离环境中也不够快，在真实部署中（智能体共享文件、套接字和云API）完全失效。

Conclusion: 需要解决三个根本性挑战：分支语义定义、外部副作用处理、以及微秒级原生分支技术，才能有效支持智能体探索。

Abstract: Agentic exploration, letting LLM-powered agents branch, backtrack, and search
across many execution paths, demands systems support well beyond today's
pass-at-k resets. Our benchmark of six snapshot/restore mechanisms shows that
generic tools such as CRIU or container commits are not fast enough even in
isolated testbeds, and they crumble entirely in real deployments where agents
share files, sockets, and cloud APIs with other agents and human users. In this
talk, we pinpoint three open fundamental challenges: fork semantics, which
concerns how branches reveal or hide tentative updates; external side-effects,
where fork awareness must be added to services or their calls intercepted; and
native forking, which requires cloning databases and runtimes in microseconds
without bulk copying.

</details>


### [14] [Decoupling Correctness from Policy: A Deterministic Causal Structure for Multi-Agent Systems](https://arxiv.org/abs/2510.05621)
*Zhiyuan Ren,Tao Zhang,Wenchi Chen*

Main category: cs.DC

TL;DR: 提出了确定性因果结构(DCS)作为分布式多智能体系统的形式化基础，将正确性与操作策略解耦，确保策略演化时不会破坏完整性保证。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中正确性常与调度、批处理等操作策略纠缠，导致系统脆弱，性能驱动的策略演化可能破坏完整性保证。

Method: 开发了最小化公理理论，证明了四个结果：存在性和唯一性、策略无关不变性、观测等价性和公理最小性。

Result: DCS解决了CRDT等值中心收敛模型无法处理的因果歧义问题，移除任何公理都会使确定性崩溃为歧义。

Conclusion: DCS确立了正确性作为固定、策略无关的底层基础，形成了"正确性即底盘"范式，可在其上模块化、安全、可演进地构建分布式智能系统。

Abstract: In distributed multi-agent systems, correctness is often entangled with
operational policies such as scheduling, batching, or routing, which makes
systems brittle since performance-driven policy evolution may break integrity
guarantees. This paper introduces the Deterministic Causal Structure (DCS), a
formal foundation that decouples correctness from policy. We develop a minimal
axiomatic theory and prove four results: existence and uniqueness,
policy-agnostic invariance, observational equivalence, and axiom minimality.
These results show that DCS resolves causal ambiguities that value-centric
convergence models such as CRDTs cannot address, and that removing any axiom
collapses determinism into ambiguity. DCS thus emerges as a boundary principle
of asynchronous computation, analogous to CAP and FLP: correctness is preserved
only within the expressive power of a join-semilattice. All guarantees are
established by axioms and proofs, with only minimal illustrative constructions
included to aid intuition. This work establishes correctness as a fixed,
policy-agnostic substrate, a Correctness-as-a-Chassis paradigm, on which
distributed intelligent systems can be built modularly, safely, and evolvably.

</details>


### [15] [Intertemporal Pricing of Time-Bound Stablecoins: Measuring and Controlling the Liquidity-of-Time Premium](https://arxiv.org/abs/2510.05711)
*Ailiya Borjigin,Cong He*

Main category: cs.DC

TL;DR: 本文提出了时间流动性溢价(TLP)概念，为在传统市场闭市期间提供流动性的额外回报或成本，并建立了无套利定价模型和动态风险控制机制来管理时间绑定稳定币。


<details>
  <summary>Details</summary>
Motivation: 解决传统证券市场在闭市期间缺乏流动性的问题，通过时间绑定稳定币实现跨市场连续流动性，减少时间区隔带来的市场低效率。

Method: 结合金融工程（无套利条件、期权式定价）和实证金融（事件研究），构建定价模型和动态LTV调整机制，通过模拟场景分析波动率和抵押率的影响。

Result: 研究表明TLP随闭市时长和波动率增长，但可通过自适应LTV控制；提供了回测结果和可视化分析（期限结构曲线、资本效率与尾部风险权衡等）。

Conclusion: 时间绑定稳定币是减少时间市场低效的有效工具，为未来研究和部署提供了理论基础和实践指导。

Abstract: Time-bound stablecoins are DeFi assets that temporarily tokenize traditional
securities during market off-hours, enabling continuous cross-market liquidity.
We introduce the Liquidity-of-Time Premium (TLP): the extra return or cost of
providing liquidity when the primary market is closed. We build a no-arbitrage
pricing model that yields a band for fair values over different expiries, and a
dynamic risk-control mechanism that adjusts loan-to-value (LTV) ratios in real
time to keep TLP within a target range. Our analysis blends financial
engineering (no-arbitrage conditions, option-style pricing) with empirical
finance (event studies on cross-listed stocks and futures) to measure TLP under
time-zone frictions. We define TLP formally, derive closed-form expressions for
its term structure under idealized assumptions, and simulate scenarios that
vary volatility and collateralization. We then propose an LTV policy that
raises or lowers collateral to expand or curtail time-bound stablecoin supply,
analogous to a central bank adjusting rates to defend a peg. We outline
empirical proxies for TLP, including ADR premiums, overseas index futures
versus cash index divergence, and pre-market versus official close gaps.
Results show that TLP grows with closure length and volatility, yet can be
contained by adaptive LTV. We provide backtests and figures (term-structure
curves, capital-efficiency versus tail-risk trade-offs, time-liquidity
heatmaps) and discuss protocol design (vault structure, closing-price oracles,
on-chain auction liquidations). The findings position time-bound stablecoins as
a tool to reduce temporal market inefficiencies and inform future research and
deployment.

</details>


### [16] [A Review of Ontology-Driven Big Data Analytics in Healthcare: Challenges, Tools, and Applications](https://arxiv.org/abs/2510.05738)
*Ritesh Chandra,Sonali Agarwal,Navjot Singh,Sadhana Tiwari*

Main category: cs.DC

TL;DR: 本文系统综述了本体驱动的语义数据管理在医疗大数据分析中的应用，将相关研究分为六大类别，并探讨了本体技术与大数据框架的整合潜力。


<details>
  <summary>Details</summary>
Motivation: 医疗数据的指数级增长（来自电子健康记录、医学影像、可穿戴设备等）推动了数据湖和集中式架构的采用，但缺乏有效治理会导致数据沼泽问题。本体驱动的语义数据管理能够通过将元数据与医疗知识图谱链接，增强语义互操作性、提高数据可发现性。

Method: 采用系统性研究策略，制定关键研究问题，在主要学术数据库中进行结构化文献检索，将选定研究分析并分类为六大类别：本体驱动集成框架、语义建模元数据丰富、基于本体的数据访问、基本语义数据管理、基于本体的决策支持推理、非结构化数据的语义标注。

Result: 识别了本体技术在大数据框架（如Hadoop、Spark、Kafka等）中的整合潜力，能够提供可扩展和智能的医疗分析。对每个类别回顾了最新技术、代表性案例研究、技术和组织挑战以及新兴趋势。

Conclusion: 本体驱动的语义数据管理为构建可持续、可互操作和高性能的医疗数据生态系统提供了指导，整合了人工智能、机器学习、物联网和实时分析等新兴技术。

Abstract: Exponential growth in heterogeneous healthcare data arising from electronic
health records (EHRs), medical imaging, wearable sensors, and biomedical
research has accelerated the adoption of data lakes and centralized
architectures capable of handling the Volume, Variety, and Velocity of Big Data
for advanced analytics. However, without effective governance, these
repositories risk devolving into disorganized data swamps. Ontology-driven
semantic data management offers a robust solution by linking metadata to
healthcare knowledge graphs, thereby enhancing semantic interoperability,
improving data discoverability, and enabling expressive, domain-aware access.
This review adopts a systematic research strategy, formulating key research
questions and conducting a structured literature search across major academic
databases, with selected studies analyzed and classified into six categories of
ontology-driven healthcare analytics: (i) ontology-driven integration
frameworks, (ii) semantic modeling for metadata enrichment, (iii)
ontology-based data access (OBDA), (iv) basic semantic data management, (v)
ontology-based reasoning for decision support, and (vi) semantic annotation for
unstructured data. We further examine the integration of ontology technologies
with Big Data frameworks such as Hadoop, Spark, Kafka, and so on, highlighting
their combined potential to deliver scalable and intelligent healthcare
analytics. For each category, recent techniques, representative case studies,
technical and organizational challenges, and emerging trends such as artificial
intelligence, machine learning, the Internet of Things (IoT), and real-time
analytics are reviewed to guide the development of sustainable, interoperable,
and high-performance healthcare data ecosystems.

</details>


### [17] [EARL: Efficient Agentic Reinforcement Learning Systems for Large Language Models](https://arxiv.org/abs/2510.05943)
*Zheyue Tan,Mustapha Abdullahi,Tuo Shi,Huining Yuan,Zelai Xu,Chao Yu,Boxun Li,Bo Zhao*

Main category: cs.DC

TL;DR: EARL是一个用于高效智能体强化学习的可扩展系统，通过动态并行化选择器和数据分发器解决长上下文训练中的内存和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 智能体强化学习在大型语言模型后训练中面临两个瓶颈：上下文长度快速增长导致内存使用激增和内存不足故障，以及中间张量累积造成跨设备数据传输成为系统瓶颈。

Method: EARL设计了并行化选择器，根据序列长度和系统负载动态调整模型和训练并行度；以及数据分发器，执行布局感知的分散式中间数据批次交换。

Result: 该系统提高了吞吐量，减少了长上下文故障，并实现了智能体LLM的稳定大规模训练，无需依赖上下文长度的硬性限制或惩罚。

Conclusion: EARL通过动态并行化调整和高效数据管理，有效解决了智能体强化学习中的可扩展性问题，为大规模智能体LLM训练提供了稳定高效的解决方案。

Abstract: Reinforcement learning (RL) has become a pivotal component of large language
model (LLM) post-training, and agentic RL extends this paradigm to operate as
agents through multi-turn interaction and tool use. Scaling such systems
exposes two practical bottlenecks: (1) context length grows rapidly during
training, inflating memory usage and latency, and triggering out-of-memory
(OOM) failures; and (2) intermediate tensors accumulate with context length,
making cross-device data movement a major system bottleneck.
  We present EARL, a scalable system for efficient agentic RL. EARL designs a
parallelism selector that dynamically adapts model and training parallelism
across RL stages based on sequence length and system load, and a data
dispatcher that performs layout-aware, decentralized exchange of intermediate
data batches. Together, these components increase throughput, reduce
long-context failures, and enable stable large-scale training of agentic LLMs
without relying on hard limits or penalties of context length.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [18] [Stratum: System-Hardware Co-Design with Tiered Monolithic 3D-Stackable DRAM for Efficient MoE Serving](https://arxiv.org/abs/2510.05245)
*Yue Pan,Zihan Xia,Po-Kai Hsu,Lanxiang Hu,Hyungyo Kim,Janak Sharda,Minxuan Zhou,Nam Sung Kim,Shimeng Yu,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: 提出Stratum系统，结合Mono3D DRAM、近内存处理和GPU加速，解决MoE模型硬件部署中的数据量挑战，实现8.29倍解码吞吐量提升和7.66倍能效改进。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然通过稀疏门控实现了亿级参数容量和较小推理成本，但其MoE层引入的海量数据给硬件部署带来挑战。

Method: 采用系统-硬件协同设计，结合Mono3D DRAM（提供比HBM更高内部带宽）、近内存处理和GPU加速，通过基于主题的专家使用预测优化数据分配。

Result: 相比GPU基线，在各种基准测试中实现了最高8.29倍的解码吞吐量提升和7.66倍的能效改进。

Conclusion: Stratum系统通过创新的内存技术和数据处理策略，有效解决了MoE模型硬件部署中的性能瓶颈问题。

Abstract: As Large Language Models (LLMs) continue to evolve, Mixture of Experts (MoE)
architecture has emerged as a prevailing design for achieving state-of-the-art
performance across a wide range of tasks. MoE models use sparse gating to
activate only a handful of expert sub-networks per input, achieving
billion-parameter capacity with inference costs akin to much smaller models.
However, such models often pose challenges for hardware deployment due to the
massive data volume introduced by the MoE layers. To address the challenges of
serving MoE models, we propose Stratum, a system-hardware co-design approach
that combines the novel memory technology Monolithic 3D-Stackable DRAM (Mono3D
DRAM), near-memory processing (NMP), and GPU acceleration. The logic and Mono3D
DRAM dies are connected through hybrid bonding, whereas the Mono3D DRAM stack
and GPU are interconnected via silicon interposer. Mono3D DRAM offers higher
internal bandwidth than HBM thanks to the dense vertical interconnect pitch
enabled by its monolithic structure, which supports implementations of
higher-performance near-memory processing. Furthermore, we tackle the latency
differences introduced by aggressive vertical scaling of Mono3D DRAM along the
z-dimension by constructing internal memory tiers and assigning data across
layers based on access likelihood, guided by topic-based expert usage
prediction to boost NMP throughput. The Stratum system achieves up to 8.29x
improvement in decoding throughput and 7.66x better energy efficiency across
various benchmarks compared to GPU baselines.

</details>


### [19] [DeepV: A Model-Agnostic Retrieval-Augmented Framework for Verilog Code Generation with a High-Quality Knowledge Base](https://arxiv.org/abs/2510.05327)
*Zahin Ibnat,Paul E. Calzada,Rasin Mohammed Ihtemam,Sujan Kumar Saha,Jingbo Zhou,Farimah Farahmandi,Mark Tehranipoor*

Main category: cs.AR

TL;DR: DeepV是一个模型无关的RAG框架，用于生成RTL设计，通过高质量数据集增强上下文，无需RTL特定训练，在VerilogEval基准测试上性能提升近17%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在硬件设计自动化中存在缺陷：无法整合新IP到知识库，导致代码生成质量差；基于旧模型的微调方法无法与通用LLM竞争；现有RAG技术存在代码库质量低、计算成本高或未直接用于RTL生成的问题。

Method: 提出DeepV框架，采用模型无关的检索增强生成方法，利用大规模高质量数据集增强上下文，无需进行RTL特定训练。

Result: 在最新商业LLM（OpenAI GPT-5）上，VerilogEval基准测试性能提升近17%。

Conclusion: DeepV是一个有效的RAG框架，能够显著提升RTL代码生成性能，已作为开源工具在Hugging Face上发布供社区使用。

Abstract: As large language models (LLMs) continue to be integrated into modern
technology, there has been an increased push towards code generation
applications, which also naturally extends to hardware design automation.
LLM-based solutions for register transfer level (RTL) code generation for
intellectual property (IP) designs have grown, especially with fine-tuned LLMs,
prompt engineering, and agentic approaches becoming popular in literature.
However, a gap has been exposed in these techniques, as they fail to integrate
novel IPs into the model's knowledge base, subsequently resulting in poorly
generated code. Additionally, as general-purpose LLMs continue to improve,
fine-tuned methods on older models will not be able to compete to produce more
accurate and efficient designs. Although some retrieval augmented generation
(RAG) techniques exist to mitigate challenges presented in fine-tuning
approaches, works tend to leverage low-quality codebases, incorporate
computationally expensive fine-tuning in the frameworks, or do not use RAG
directly in the RTL generation step. In this work, we introduce DeepV: a
model-agnostic RAG framework to generate RTL designs by enhancing context
through a large, high-quality dataset without any RTL-specific training. Our
framework benefits the latest commercial LLM, OpenAI's GPT-5, with a near 17%
increase in performance on the VerilogEval benchmark. We host DeepV for use by
the community in a Hugging Face (HF) Space:
https://huggingface.co/spaces/FICS-LLM/DeepV.

</details>


### [20] [From Principles to Practice: A Systematic Study of LLM Serving on Multi-core NPUs](https://arxiv.org/abs/2510.05632)
*Tianhao Zhu,Dahu Feng,Erhu Feng,Yubin Xia*

Main category: cs.AR

TL;DR: 本文提出了一种针对多核NPU的LLM推理优化方案，通过多级仿真框架和系统分析，在张量并行策略、核心放置、内存管理等方面提供最优解，相比现有设计可实现1.32-6.03倍的加速。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，对高性能推理服务的需求日益增长。现有AI加速器多采用多核架构但缺乏SIMT架构的灵活性，导致计算资源利用率不足和推理性能不佳。

Method: 提出多级仿真框架（事务级和性能模型仿真），系统分析并优化张量并行策略、核心放置策略、内存管理方法，以及在PD解聚和PD融合之间的选择。

Result: 在不同硬件配置下，相比最先进的多核NPU设计，实现了1.32倍到6.03倍的加速。

Conclusion: 该工作为多核NPU在各种LLM工作负载下设计最优硬件架构和服务策略提供了指导。

Abstract: With the widespread adoption of Large Language Models (LLMs), the demand for
high-performance LLM inference services continues to grow. To meet this demand,
a growing number of AI accelerators have been proposed, such as Google TPU,
Huawei NPU, Graphcore IPU, and Cerebras WSE, etc. Most of these accelerators
adopt multi-core architectures to achieve enhanced scalability, but lack the
flexibility of SIMT architectures. Therefore, without careful configuration of
the hardware architecture, as well as deliberate design of tensor parallelism
and core placement strategies, computational resources may be underutilized,
resulting in suboptimal inference performance.
  To address these challenges, we first present a multi-level simulation
framework with both transaction-level and performance-model-based simulation
for multi-core NPUs. Using this simulator, we conduct a systematic analysis and
further propose the optimal solutions for tensor parallelism strategies, core
placement policies, memory management methods, as well as the selection between
PD-disaggregation and PD-fusion on multi-core NPUs. We conduct comprehensive
experiments on representative LLMs and various NPU configurations. The
evaluation results demonstrate that, our solution can achieve 1.32x-6.03x
speedup compared to SOTA designs for multi-core NPUs across different hardware
configurations. As for LLM serving, our work offers guidance on designing
optimal hardware architectures and serving strategies for multi-core NPUs
across various LLM workloads.

</details>


### [21] [An opportunity to improve Data Center Efficiency: Optimizing the Server's Upgrade Cycle](https://arxiv.org/abs/2510.05787)
*Panagiota Nikolaou,Freddy Gabbay,Jawad Haj-Yahya,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: 该研究通过优化服务器升级计划来提高数据中心效率，确定替换旧服务器的最佳时机。研究发现全局升级计划优于局部升级计划，能显著提升QPS/(TCOxCO2)指标。


<details>
  <summary>Details</summary>
Motivation: 提高数据中心效率，通过优化服务器升级时机来最大化性能与成本效益比。

Method: 基于历史服务器数据，制定覆盖数据中心整个生命周期的全局升级计划，考虑服务器进入年份、性能和功耗等信息。

Result: 全局升级计划可在非固定时间周期进行升级，比基于固定周期和当前可用服务器模型的局部计划表现更好。

Conclusion: 在数据中心设计阶段制定全局升级计划，考虑未来服务器发布，能够显著提升效率指标。

Abstract: This work aims to improve a data center's efficiency by optimizing the server
upgrade plan: determine the optimal timing for replacing old servers with new
ones. The opportunity presented by this approach is demonstrated through a
study based on historical server data. The study establishes a significant
opportunity to increase the QPS/(TCOxCO2) metric by formulating a global
upgrade plan at the data center's design time covering its entire life cycle.
This plan leverages information, such as server entry year, performance, and
active power consumption for both existing and future servers. Our findings
reveal that an optimal global upgrade plan, may involve upgrades at non fixed
time periods and outperforms local upgrade plans. Local upgrade plans follow a
fixed, equal-length cycle and make decisions based only on currently available
server models. These local plans select the best available server at each
upgrade cycle without accounting for future server releases.

</details>
