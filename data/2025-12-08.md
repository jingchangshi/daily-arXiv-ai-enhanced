<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [NVLang: Unified Static Typing for Actor-Based Concurrency on the BEAM](https://arxiv.org/abs/2512.05224)
*Miguel de Oliveira Guerreiro*

Main category: cs.PL

TL;DR: NVLang是一个静态类型函数式语言，为BEAM虚拟机提供全面的类型安全，通过代数数据类型编码actor消息协议，消除消息传递错误。


<details>
  <summary>Details</summary>
Motivation: Erlang/OTP等基于actor的系统虽然可靠，但缺乏消息协议的静态保证，协议违规只能在运行时发现，导致生产故障。

Method: 使用代数数据类型编码actor消息协议，每个actor声明其消息词汇的sum类型；引入类型化进程标识符(Pid[T])和类型化future(Future[T])；扩展Hindley-Milner类型推断来跟踪消息协议；编译到Core Erlang实现与现有生态互操作。

Result: NVLang在编译时强制执行协议一致性，消除了一整类消息传递错误，同时保持了与动态类型替代方案相媲美的简洁语法；形式化了类型系统并提供了类型健全性证明。

Conclusion: NVLang为BEAM虚拟机带来了全面的类型安全，同时保持了actor模型的简单性和强大功能，使开发人员能够编写既可靠又类型安全的并发程序。

Abstract: Actor-based systems like Erlang/OTP power critical infrastructure -- from telecommunications to messaging platforms -- handling millions of concurrent connections with legendary reliability. Yet these systems lack static guarantees about message protocols: processes communicate by sending arbitrary messages that pattern-matched at runtime, deferring protocol violations to production failures.
  We present NVLang, a statically typed functional language that brings comprehensive type safety to the BEAM virtual machine while preserving actor model's simplicity and power. NVLang's central contribution that algebraic data types (ADTs) naturally encode actor message protocols: each actor declares the sum type representing its message vocabulary, and the type system enforces protocol conformance at compile time. We introduce typed process identifiers (Pid[T]) that encode the protocol an actor expects, and typed futures (Future[T]) that provide type-safe request-reply patterns.
  By extending Hindley-Milner type inference to track message protocols, NVLang eliminates an entire class of message-passing errors while maintaining clean syntax that rivals dynamically typed alternatives. Our implementation compiles to Core Erlang, enabling seamless interoperability with the existing Erlang ecosystem. We formalize the type system and provide proof sketches for type soundness, demonstrating that well-typed NVLang programs cannot send messages that violate actor protocols.

</details>


### [2] [Compiler-supported reduced precision and AoS-SoA transformations for heterogeneous hardware](https://arxiv.org/abs/2512.05516)
*Pawel K. Radtke,Tobias Weinzierl*

Main category: cs.PL

TL;DR: 该研究评估了在多个GPU平台上对粒子模拟代码进行AoS-to-SoA转换和降低精度数据布局的性能，发现Nvidia G200平台可获得约2.6倍加速，而AMD MI300A性能更稳健但收益较小。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在GPU加速计算中，AoS（数组结构）与SoA（结构数组）数据布局转换以及降低精度处理的优化问题。AoS是许多拉格朗日代码的首选存储格式，而SoA更适合SIMT架构。同时，降低精度是解决带宽限制的常用方法，但不确定这些转换应该在CPU还是GPU上执行，特别是在现代超级芯片中CPU和GPU共享数据空间的情况下。

Method: 引入编译器注解来促进AoS-to-SoA转换和精度降低，让程序员能够结合GPU卸载来编排这些转换。在多个GPU平台（包括Nvidia G200和AMD MI300A）上评估粒子模拟代码的性能，比较不同转换策略的效果。

Result: 对于某些计算内核，Nvidia G200平台获得了约2.6倍的加速，而AMD MI300A表现出更稳健的性能但收益较小。研究还发现，编译器注解方法能够有效管理数据转换和GPU卸载的编排。

Conclusion: 基于编译器的技术适用于各种拉格朗日代码及其他应用。研究为在异构计算环境中优化数据布局和精度转换提供了实用方法，特别是在CPU和GPU共享数据空间的现代架构中。

Abstract: This study evaluates AoS-to-SoA transformations over reduced-precision data layouts for a particle simulation code on several GPU platforms: We hypothesize that SoA fits particularly well to SIMT, while AoS is the preferred storage format for many Lagrangian codes. Reduced-precision (below IEEE accuracy) is an established tool to address bandwidth constraints, although it remains unclear whether AoS and precision conversions should execute on a CPU or be deployed to a GPU if the compute kernel itself must run on an accelerator. On modern superchips where CPUs and GPUs share (logically) one data space, it is also unclear whether it is advantageous to stream data to the accelerator prior to the calculation, or whether we should let the accelerator transform data on demand, i.e.~work in-place logically. We therefore introduce compiler annotations to facilitate such conversions and to give the programmer the option to orchestrate the conversions in combination with GPU offloading. For some of our compute kernels of interest, Nvidia's G200 platforms yield a speedup of around 2.6 while AMD's MI300A exhibits more robust performance yet profits less. We assume that our compiler-based techniques are applicable to a wide variety of Lagrangian codes and beyond.

</details>


### [3] [Verified VCG and Verified Compiler for Dafny](https://arxiv.org/abs/2512.05262)
*Daniel Nezamabadi,Magnus O. Myreen,Yong Kiam Tan*

Main category: cs.PL

TL;DR: 为Dafny语言开发了经过验证的验证条件生成器和编译器，确保从源代码到机器代码的全程正确性保证


<details>
  <summary>Details</summary>
Motivation: Dafny语言现有的编译器和验证器存在正确性缺陷，需要建立从源代码到机器代码的完整形式化验证链

Method: 为Dafny子集定义函数式大步语义，基于此语义在HOL4定理证明器中实现经过验证的验证条件生成器和编译器

Result: 成功开发了经过验证的Dafny验证条件生成器和编译器，支持相互递归方法调用、while循环和数组等核心特性

Conclusion: 实现了从Dafny源代码到CakeML再到机器代码的完整验证链，为Dafny程序提供了基础性正确性保证

Abstract: Dafny is a verification-aware programming language that comes with a compiler and static program verifier. However, neither the compiler nor the verifier is proved correct; in fact, soundness bugs have been found in both tools. This paper shows that the aforementioned Dafny tools can be developed with foundational correctness guarantees. We present a functional big-step semantics for an imperative subset of Dafny and, based on this semantics, a verified verification condition generator (VCG) and a verified compiler for Dafny. The subset of Dafny we have formalized includes mutually recursive method calls, while loops, and arrays -- these language features are significant enough to cover challenging examples such as McCarthy's 91 function and array-based programs that are used when teaching Dafny. The verified VCG allows one to prove functional correctness of annotated Dafny programs, while the verified compiler can be used to compile verified Dafny programs to CakeML programs. From there, one can obtain executable machine code via the (already verified) CakeML compiler, all while provably maintaining the functional correctness guarantees that were proved for the source-level Dafny programs. Our work has been mechanized in the HOL4 theorem prover.

</details>


### [4] [Compiling Away the Overhead of Race Detection](https://arxiv.org/abs/2512.05555)
*Alexey Paznikov,Andrey Kogutenko,Yaroslav Osipov,Michael Schwarz,Umang Mathur*

Main category: cs.PL

TL;DR: 通过静态分析识别并消除动态数据竞争检测中的冗余检测点，显著降低运行时开销，平均加速1.34倍，最高可达2.5倍。


<details>
  <summary>Details</summary>
Motivation: 动态数据竞争检测器因对内存访问的广泛插桩导致高运行时开销，限制了其实际应用。许多插桩是冗余的，需要优化。

Method: 提出一套编译器集成的静态分析方法：1) 基于内存访问模式、同步和线程创建的跨过程分析，消除可证明无竞争的访问插桩；2) 基于支配关系的消除分析，识别并消除报告等效竞争时的冗余检查。

Result: 在LLVM中实现五种静态分析并与ThreadSanitizer集成，在真实应用上平均获得1.34倍加速，高线程竞争下峰值加速达2.5倍，编译时间增加可忽略。

Conclusion: 静态分析能有效消除动态数据竞争检测中的冗余插桩，显著降低运行时开销，已被ThreadSanitizer维护者接受并正在集成到上游。

Abstract: Dynamic data race detectors are indispensable for flagging concurrency errors in software, but their high runtime overhead limits their adoption. This overhead stems primarily from pervasive instrumentation of memory accesses - a significant fraction of which is redundant. We addresses this inefficiency through a static, compiler-integrated approach that identifies and eliminates redundant instrumentation, drastically reducing the runtime cost of dynamic data race detectors. We introduce a suite of interprocedural static analyses reasoning about memory access patterns, synchronization, and thread creation to eliminate instrumentation for provably race-free accesses and show that the completeness properties of the data race detector are preserved. We further observe that many inserted checks flag a race if and only if a preceding check has already flagged an equivalent race for the same memory location - albeit potentially at a different access. We characterize this notion of equivalence and show that, when limiting reporting to at least one representative for each equivalence class, a further class of redundant checks can be eliminated. We identify such accesses using a novel dominance-based elimination analysis. Based on these two insights, we have implemented five static analyses within the LLVM, integrated with the instrumentation pass of the race detector ThreadSanitizer. Our experimental evaluation on a diverse suite of real-world applications demonstrates that our approach significantly reduces race detection overhead, achieving a geomean speedup of 1.34x, with peak speedups reaching 2.5x under high thread contention. This performance is achieved with a negligible increase in compilation time and, being fully automatic, places no additional burden on developers. Our optimizations have been accepted by the ThreadSanitizer maintainers and are in the process of being upstreamed.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [FedGMR: Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity](https://arxiv.org/abs/2512.05372)
*Chengjie Ma,Seungeun Oh,Jihong Park,Seong-Lyun Kim*

Main category: cs.DC

TL;DR: FedGMR提出了一种渐进式模型恢复的联邦学习方法，通过逐步增加带宽受限客户端的子模型密度，解决异构环境中客户端参与度低和收敛慢的问题。


<details>
  <summary>Details</summary>
Motivation: 在异构联邦学习环境中，带宽受限客户端由于通信能力有限，其小规模子模型在训练初期学习快，但后期参数不足导致收敛慢、泛化差，难以有效参与训练过程。

Method: FedGMR采用渐进式模型恢复策略，在训练过程中逐步增加每个客户端的子模型密度；同时设计了面向异步异构联邦学习的掩码感知聚合规则，并提供收敛性理论保证。

Result: 在FEMNIST、CIFAR-10和ImageNet-100数据集上的实验表明，FedGMR在高异构性和非独立同分布设置下实现了更快的收敛速度和更高的准确率。

Conclusion: FedGMR通过渐进式模型恢复有效解决了带宽受限客户端在异构联邦学习中的参与问题，理论分析和实验验证了其在提升收敛速度和模型精度方面的优越性。

Abstract: Federated learning (FL) holds strong potential for distributed machine learning, but in heterogeneous environments, Bandwidth-Constrained Clients (BCCs) often struggle to participate effectively due to limited communication capacity. Their small sub-models learn quickly at first but become under-parameterized in later stages, leading to slow convergence and degraded generalization. We propose FedGMR - Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity. FedGMR progressively increases each client's sub-model density during training, enabling BCCs to remain effective contributors throughout the process. In addition, we develop a mask-aware aggregation rule tailored for asynchronous MHFL and provide convergence guarantees showing that aggregated error scales with the average sub-model density across clients and rounds, while GMR provably shrinks this gap toward full-model FL. Extensive experiments on FEMNIST, CIFAR-10, and ImageNet-100 demonstrate that FedGMR achieves faster convergence and higher accuracy, especially under high heterogeneity and non-IID settings.

</details>


### [6] [Are Bus-Mounted Edge Servers Feasible?](https://arxiv.org/abs/2512.05543)
*Xuezhi Li,Jiancong He,Ming Xie,Xuyang Chen,Le Chang,Li Jiang,Gui Gui*

Main category: cs.DC

TL;DR: 该论文研究在公交车部署边缘服务器的可行性，通过真实轨迹数据分析公交车覆盖范围，并提出基于贪心算法的公交车选择方案以最大化需求点覆盖。


<details>
  <summary>Details</summary>
Motivation: 物联网车辆(IoV)需要边缘计算服务，但固定位置的路侧单元(RSU)或基站边缘服务器部署后位置和容量固定，难以处理时空动态的用户需求。移动服务器(如公交车)有潜力为系统增加计算弹性。

Method: 1. 使用上海公交车/出租车/电信数据集分析公交车和基站的覆盖范围；2. 建立数学模型并设计简单的贪心启发式算法，在有限预算下选择最优公交车以最大化需求点覆盖；3. 进行基于轨迹的仿真验证算法性能。

Result: 公交车边缘服务器覆盖了大部分地理区域和需求点，显示出巨大潜力。提出的公交车选择算法能有效处理动态用户需求，并在服务器容量和购买数量等现实约束下表现良好。

Conclusion: 公交车搭载的边缘服务器在城市区域车辆网络中具有可行性、益处和价值，能够有效补充固定边缘服务器系统。

Abstract: Placement of edge servers is the prerequisite of provisioning edge computing services for Internet of Vehicles (IoV). Fixed-site edge servers at Road Side Units (RSUs) or base stations are able to offer basic service coverage for end users, i.e., vehicles on road. However, the server locations and capacity are fixed after deployment, rendering their inefficiency in handling spationtemporal user dynamics. Mobile servers such as buses, on the other hand, have the potential of adding computation elasticity to such system. To this end, this paper studies the feasibility of bus-mounted edge servers based on real traces. First, we investigate the coverage of the buses and base stations using the Shanghai bus/taxi/Telecom datasets, which shows a great potential of bus-based edge servers as they cover a great portion of geographic area and demand points. Next, we build a mathematical model and design a simple greedy heuristic algorithm to select a limited number of buses that maximizes the coverage of demand points, i.e., with a limited purchase budget. We perform trace-driven simulations to verify the performance of the proposed bus selection algorithm. The results show that our approach effectively handles the dynamic user demand under realistic constraints such as server capacity and purchase quantity. Thus, we claim: bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable.

</details>
