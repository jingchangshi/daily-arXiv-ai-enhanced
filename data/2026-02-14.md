<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Compiler-Guided Inference-Time Adaptation: Improving GPT-5 Programming Performance in Idris](https://arxiv.org/abs/2602.11481)
*Minda Li,Bhaskar Krishnamachari*

Main category: cs.PL

TL;DR: GPT-5在低资源函数式编程语言Idris中表现不佳，但通过基于编译错误的迭代提示可以显著提升性能，从22/56提升到54/56


<details>
  <summary>Details</summary>
Motivation: 研究GPT-5在低资源或不常用编程语言（如Idris）中的能力，这些语言相对于Python、C++、Java等高资源语言研究不足

Method: 使用Exercism平台的Idris练习进行评估，比较多种精炼策略：基于平台反馈的迭代提示、添加文档和错误分类指南、使用本地编译错误和失败测试用例的迭代提示

Result: 零次提示仅解决22/56个问题，远低于Python（45/50）和Erlang（35/47）。使用本地编译错误的迭代提示效果最好，将性能提升到54/56个问题

Conclusion: 虽然大语言模型在低资源环境中可能初始表现不佳，但结构化的编译器级别反馈在释放其能力方面起着关键作用

Abstract: GPT-5, a state of the art large language model from OpenAI, demonstrates strong performance in widely used programming languages such as Python, C++, and Java; however, its ability to operate in low resource or less commonly used languages remains underexplored. This work investigates whether GPT-5 can effectively acquire proficiency in an unfamiliar functional programming language, Idris, through iterative, feedback driven prompting. We first establish a baseline showing that with zero shot prompting the model solves only 22 out of 56 Idris exercises using the platform Exercism, substantially underperforming relative to higher resource languages (45 out of 50 in Python and 35 out of 47 in Erlang). We then evaluate several refinement strategies, including iterative prompting based on platform feedback, augmenting prompts with documentation and error classification guides, and iterative prompting using local compilation errors and failed test cases. Among these approaches, incorporating local compilation errors yields the most substantial improvements. Using this structured, error guided refinement loop, GPT-5 performance increased to an impressive 54 solved problems out of 56. These results suggest that while large language models may initially struggle in low resource settings, structured compiler level feedback can play a critical role in unlocking their capabilities.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Real Life Is Uncertain. Consensus Should Be Too!](https://arxiv.org/abs/2602.11362)
*Reginald Frank,Soujanya Ponnapalli,Octavio Lomeli,Neil Giridharan,Marcos K Aguilera,Natacha Crooks*

Main category: cs.DC

TL;DR: 论文提出用概率性故障模型替代传统的f阈值故障模型，以更好地反映现实世界的复杂性，并优化共识协议的成本和性能。


<details>
  <summary>Details</summary>
Motivation: 传统共识协议基于f阈值故障模型（最多f台机器故障），这种模型过度简化了现实世界，限制了在成本和性能方面的优化机会。现实中的故障具有复杂性和细微差别，需要更精确的模型来捕捉。

Method: 提出概率性故障模型，利用个体机器的"故障曲线"，绕过传统瓶颈（如多数仲裁交集），设计更可靠、高效、经济、可持续的系统。

Result: 概率性共识协议能够更精确地建模现实故障，提供比传统阈值模型更好的可靠性、效率、成本效益和可持续性。

Conclusion: 概率性故障模型比传统的f阈值模型更能反映分布式系统的实际故障特性，为共识协议的设计提供了新的优化方向，有望构建更适应现实世界的分布式系统。

Abstract: Modern distributed systems rely on consensus protocols to build a fault-tolerant-core upon which they can build applications. Consensus protocols are correct under a specific failure model, where up to $f$ machines can fail. We argue that this $f$-threshold failure model oversimplifies the real world and limits potential opportunities to optimize for cost or performance. We argue instead for a probabilistic failure model that captures the complex and nuanced nature of faults observed in practice. Probabilistic consensus protocols can explicitly leverage individual machine \textit{failure curves} and explore side-stepping traditional bottlenecks such as majority quorum intersection, enabling systems that are more reliable, efficient, cost-effective, and sustainable.

</details>


### [3] [RL over Commodity Networks: Overcoming the Bandwidth Barrier with Lossless Sparse Deltas](https://arxiv.org/abs/2602.11456)
*Chaoyi Ruan,Geng Luo,Xinyi Wan,Long Zhao,Qinghe Wang,Jiaan Zhu,Duling Xu,Guanbin Xu,Dehui Wei,Xiang Liu,Cheng Li,Haifeng Sun,Congcong Miao,Jialin Li*

Main category: cs.DC

TL;DR: SparrowRL：针对松散耦合GPU集群的稀疏增量传输RL训练系统，通过仅传输1%变化的参数，在普通网络环境下实现高效RL微调


<details>
  <summary>Details</summary>
Motivation: 传统RL训练需要频繁同步大模型参数，依赖昂贵的RDMA HPC集群。普通以太网/WAN连接无法承受全权重广播（同步8B模型需100+秒），限制了RL在松散耦合GPU资源上的实用性

Method: 基于RL微调产生稀疏更新的观察（仅约1%参数变化），设计SparrowRL系统：1）将每一步表示为稀疏增量检查点；2）流水线化增量提取与多流传输；3）传输与rollout生成重叠；4）吞吐量和带宽感知调度加租约容错机制

Result: 在Qwen3模型（4B-14B）跨最多4个地理区域的部署中：1）Qwen3-8B每步传输负载减少79倍；2）WAN上吞吐量比全权重广播提升2.4-9.5倍；3）与理想RDMA单数据中心基线的吞吐差距缩小到8.91%以内；4）按需跨云GPU比预留RDMA集群每美元token数高1.21-1.59倍

Conclusion: SparrowRL通过稀疏增量传输机制，使RL训练能在普通网络连接的松散耦合GPU资源上高效运行，显著降低成本，为更广泛的RL应用部署提供了可行方案

Abstract: LLM post-training with reinforcement learning (RL) requires frequent synchronization of large model parameters between the trainer and distributed rollout actors. High-throughput RL post-training therefore relies on dedicated RDMA HPC clusters, an infrastructure cost most organizations cannot absorb. A natural alternative is to aggregate loosely-coupled GPUs over standard Ethernet and WAN links, but this commodity connectivity cannot sustain full-weight broadcasts: synchronizing an 8B model can take over 100~seconds on bandwidth-limited links, while rollout generation typically takes tens of seconds.
  Toward making RL practical in this regime, we observe that RL fine-tuning yields highly sparse per-step updates, with only around 1\% of parameter elements changing. Atop this insight, we present SparrowRL, a novel high-performance RL training system that preserves bit-exact updates without dropping or quantizing information, designed for commodity-networked, loosely-coupled GPU resources. SparrowRL represents each step as a sparse delta checkpoint, pipelines delta extraction with multi-stream transmission, overlaps transfer with rollout generation, and coordinates heterogeneous workers with throughput- and bandwidth-aware scheduling plus lease-based fault tolerance. On Qwen3 models from 4B to 14B deployed across up to four geographic regions, SparrowRL reduces per-step transfer payload by 79$\times$ for Qwen3-8B and improves throughput by 2.4--9.5$\times$ over full-weight broadcast across WAN, narrowing the throughput gap relative to an ideal RDMA single-datacenter baseline to within 8.91\%. By leveraging on-demand, cross-cloud GPUs over commodity links, SparrowRL delivers 1.21--1.59$\times$ higher tokens per dollar than reserved RDMA clusters at comparable throughput.

</details>


### [4] [Differentially Private Perturbed Push-Sum Protocol and Its Application in Non-Convex Optimization](https://arxiv.org/abs/2602.11544)
*Yiming Zhou,Kaiping Xue,Enhong Chen*

Main category: cs.DC

TL;DR: 提出DPPS协议级差分隐私方案和PartPSP非凸优化算法，通过敏感性估计和部分通信机制平衡隐私与性能


<details>
  <summary>Details</summary>
Motivation: 去中心化网络中节点无法确保邻居安全保存共享信息，现有差分隐私方法多针对特定任务，缺乏通用协议级解决方案

Method: 提出DPPS轻量级差分隐私协议，包含新颖的敏感性估计机制；设计PartPSP算法，通过参数分区（本地/共享）和部分通信机制降低噪声影响

Result: DPPS提供严格差分隐私保证，PartPSP在非凸目标下收敛，相同隐私预算下通过部分通信获得更好优化性能

Conclusion: DPPS作为即插即用隐私保护方案有效，PartPSP在隐私-效用权衡上优于现有隐私保护去中心化优化算法

Abstract: In decentralized networks, nodes cannot ensure that their shared information will be securely preserved by their neighbors, making privacy vulnerable to inference by curious nodes. Adding calibrated random noise before communication to satisfy differential privacy offers a proven defense; however, most existing methods are tailored to specific downstream tasks and lack a general, protocol-level privacy-preserving solution. To bridge this gap, we propose Differentially Private Perturbed Push-Sum (DPPS), a lightweight differential privacy protocol for decentralized communication. Since protocol-level differential privacy introduces the unique challenge of obtaining the sensitivity for each communication round, DPPS introduces a novel sensitivity estimation mechanism that requires each node to compute and broadcast only one scalar per round, enabling rigorous differential privacy guarantees. This design allows DPPS to serve as a plug-and-play, low-cost privacy-preserving solution for downstream applications built on it. To provide a concrete instantiation of DPPS and better balance the privacy-utility trade-off, we design PartPSP, a privacy-preserving decentralized algorithm for non-convex optimization that integrates a partial communication mechanism. By partitioning model parameters into local and shared components and applying DPPS only to the shared parameters, PartPSP reduces the dimensionality of consensus data, thereby lowering the magnitude of injected noise and improving optimization performance. We theoretically prove that PartPSP converges under non-convex objectives and, with partial communication, achieves better optimization performance under the same privacy budget. Experimental results validate the effectiveness of DPPS's privacy-preserving and demonstrate that PartPSP outperforms existing privacy-preserving decentralized optimization algorithms.

</details>


### [5] [LAER-MoE: Load-Adaptive Expert Re-layout for Efficient Mixture-of-Experts Training](https://arxiv.org/abs/2602.11686)
*Xinyi Liu,Yujie Wang,Fangcheng Fu,Xuefeng Xiao,Huixia Li,Jiashi Li,Bin Cui*

Main category: cs.DC

TL;DR: LAER-MoE是一个高效的MoE训练框架，通过FSEP并行范式解决专家负载不均衡问题，实现最高1.69倍加速


<details>
  <summary>Details</summary>
Motivation: 专家并行训练中，动态路由导致专家负载严重不均衡，少数过载专家成为训练瓶颈，需要解决负载不均衡问题

Method: 提出FSEP并行范式，将专家参数完全分片到所有设备，通过All-to-All通信在训练时按专家粒度恢复部分专家；采用细粒度通信调度和负载均衡规划器优化专家布局和token路由

Result: 在A100集群上的实验表明，相比当前最先进的训练系统，LAER-MoE实现了最高1.69倍的加速

Conclusion: LAER-MoE通过创新的FSEP并行范式和负载均衡优化，有效解决了MoE训练中的负载不均衡问题，显著提升了训练效率

Abstract: Expert parallelism is vital for effectively training Mixture-of-Experts (MoE) models, enabling different devices to host distinct experts, with each device processing different input data. However, during expert parallel training, dynamic routing results in significant load imbalance among experts: a handful of overloaded experts hinder overall iteration, emerging as a training bottleneck.
  In this paper, we introduce LAER-MoE, an efficient MoE training framework. The core of LAER-MoE is a novel parallel paradigm, Fully Sharded Expert Parallel (FSEP), which fully partitions each expert parameter by the number of devices and restores partial experts at expert granularity through All-to-All communication during training. This allows for flexible re-layout of expert parameters during training to enhance load balancing. In particular, we perform fine-grained scheduling of communication operations to minimize communication overhead. Additionally, we develop a load balancing planner to formulate re-layout strategies of experts and routing schemes for tokens during training. We perform experiments on an A100 cluster, and the results indicate that our system achieves up to 1.69x acceleration compared to the current state-of-the-art training systems. Source code available at https://github.com/PKU-DAIR/Hetu-Galvatron/tree/laer-moe.

</details>


### [6] [Designing Scalable Rate Limiting Systems: Algorithms, Architecture, and Distributed Solutions](https://arxiv.org/abs/2602.11741)
*Bo Guan*

Main category: cs.DC

TL;DR: 论文提出了一种基于Redis Sorted Set的分布式限流系统架构，通过Rolling Window算法在准确性和内存成本之间取得平衡，采用三层架构和Lua脚本保证原子性，并在Redis Cluster上部署以实现可用性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 设计一个同时具备准确性、可用性和可扩展性的分布式限流系统面临根本性挑战，主要源于算法精度、可用性、一致性和分区容忍性之间的权衡。需要在生产环境中构建一个实用的分布式限流解决方案。

Method: 1. 选择Redis内存缓存数据库及其Sorted Set数据结构，提供O(log(N))时间复杂度的操作；2. 采用Rolling Window作为限流算法，并与Token Bucket和Fixed Window算法进行对比；3. 使用服务器端Lua脚本将清理、计数和插入操作捆绑为原子操作；4. 提出三层架构管理限流规则的存储和更新；5. 通过脚本哈希实现规则变更无需修改缓存脚本；6. 在Redis Cluster上部署，通过数据分片和复制提供可用性和可扩展性。

Result: 1. 量化了Rolling Window算法在准确性和内存成本方面的权衡；2. 通过Lua脚本原子操作消除了并发环境中的竞态条件；3. 三层架构支持灵活的规则管理；4. Redis Cluster部署提供了高可用性和可扩展性；5. 接受CAP定理中的AP（可用性和分区容忍性）作为实际工程权衡。

Conclusion: 该论文提出了一种实用的分布式限流系统架构，通过Redis Sorted Set和Rolling Window算法在准确性和效率之间取得平衡，采用原子操作和三层架构设计，并在Redis Cluster上实现高可用性和可扩展性，为生产环境中的分布式限流问题提供了可行的解决方案。

Abstract: Designing a rate limiter that is simultaneously accurate, available, and scalable presents a fundamental challenge in distributed systems, primarily due to the trade-offs between algorithmic precision, availability, consistency, and partition tolerance. This article presents a concrete architecture for a distributed rate limiting system in a production-grade environment. Our design chooses the in-memory cache database, the Redis, along with its Sorted Set data structure, which provides $O(log (N))$ time complexity operation for the key-value pair dataset with efficiency and low latency, and maintains precision. The core contribution is quantifying the accuracy and memory cost trade-off of the chosen Rolling Window as the implemented rate limiting algorithm against the Token Bucket and Fixed Window algorithms. In addition, we explain how server-side Lua scripting is critical to bundling cleanup, counting, and insertion into a single atomic operation, thereby eliminating race conditions in concurrent environments. In the system architecture, we propose a three-layer architecture that manages the storage and updating of the limit rules. Through script load by hashing the rule parameters, rules can be changed without modifying the cached scripts. Furthermore, we analyze the deployment of this architecture on a Redis Cluster, which provides the availability and scalability by data sharding and replication. We explain the acceptance of AP (Availability and Partition Tolerance) from the CAP theorem as the pragmatic engineering trade-off for this use case.

</details>


### [7] [An Auction-Based Mechanism for Optimal Task Allocation and Resource Aware Containerization](https://arxiv.org/abs/2602.11998)
*Ramakant kumar*

Main category: cs.DC

TL;DR: 提出AUC-RAC拍卖机制，通过Docker Swarm连接多个本地服务器，优化IoT设备计算任务卸载，提高资源利用效率


<details>
  <summary>Details</summary>
Motivation: 在IoT环境中，分布式计算需要多个设备协同执行资源密集型任务，但云容器化环境中的资源管理和成本优化仍然具有挑战性

Method: 提出AUC-RAC拍卖机制，基于Docker Swarm架构连接Manager Node和Worker Nodes，使用Docker容器化技术，通过拍卖竞价过程优化计算任务分配

Result: 实验分析表明，该方法通过促进本地服务器之间的协作，为IoT设备提供了改进的任务卸载和计算密集型服务

Conclusion: AUC-RAC拍卖机制能够有效优化IoT环境中计算任务的分配，提高资源利用效率，改善计算密集型服务的性能

Abstract: Distributed computing has enabled cooperation between multiple computing devices for the simultaneous execution of resource-hungry tasks. Such execution also plays a pivotal role in the parallel execution of numerous tasks in the Internet of Things (IoT) environment. Leveraging the computing resources of multiple devices, the offloading and processing of computationintensive tasks can be carried out more efficiently. However, managing resources and optimizing costs remain challenging for successfully executing tasks in cloud-based containerization for IoT. This paper proposes AUC-RAC, an auction-based mechanism for efficient offloading of computation tasks among multiple local servers in the context of IoT devices. The approach leverages the concept of Docker swarm, which connects multiple local servers in the form of Manager Node (MN) and Worker Nodes (WNs). It uses Docker containerization to execute tasks simultaneously. In this system, IoT devices send tasks to the MN, which then sends the task details to all its WNs to participate in the auction-based bidding process. The auctionbased bidding process optimizes the allocation of computation tasks among multiple systems, considering their resource sufficiency. The experimental analysis establishes that the approach offers improved offloading and computation-intensive services for IoT devices by enabling cooperation between local servers.

</details>


### [8] [Contention Resolution, With and Without a Global Clock](https://arxiv.org/abs/2602.12070)
*Zixi Cai,Kuowen Chen,Shengquan Du,Tsvi Kopelowitz,Seth Pettie,Ben Plosk*

Main category: cs.DC

TL;DR: 本文研究了具有全局时钟的竞争解决协议，设计了新的协议获得更低的延迟，证明了随机协议在全局时钟和本地时钟下的复杂度差距，以及期望延迟和高概率延迟之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 竞争解决是分布式计算中的经典问题，传统研究假设参与者只能访问本地时钟。本文引入全局时钟假设，这在技术上更现实且算法上更有趣，能丰富问题并开启新技术。

Method: 设计了新的竞争解决协议，分析了随机协议在全局时钟和本地时钟下的性能差异，研究了期望延迟和高概率延迟之间的权衡关系，提供了新的上界和下界分析。

Result: 1. 设计了新协议，延迟为O(n(log log n)^{1+o(1)})，建立了全局时钟和本地时钟下随机协议的复杂度差距；2. 证明了无记忆协议的期望延迟和高概率延迟之间存在log n因子差距；3. 证明了不可能同时获得最优的期望延迟和高概率延迟。

Conclusion: 全局时钟假设显著改变了竞争解决问题的复杂度格局，带来了新的算法可能性，但也揭示了期望延迟和高概率延迟之间的基本权衡，为分布式系统设计提供了重要见解。

Abstract: In the Contention Resolution problem $n$ parties each wish to have exclusive use of a shared resource for one unit of time. The problem has been studied since the early 1970s, under a variety of assumptions on feedback given to the parties, how the parties wake up, knowledge of $n$, and so on. The most consistent assumption is that parties do not have access to a global clock, only their local time since wake-up. This is surprising because the assumption of a global clock is both technologically realistic and algorithmically interesting. It enriches the problem, and opens the door to entirely new techniques. Our primary results are: [1] We design a new Contention Resolution protocol that guarantees latency $$O\left(\left(n\log\log n\log^{(3)} n\log^{(4)} n\cdots \log^{(\log^* n)} n\right)\cdot 2^{\log^* n}\right) \le n(\log\log n)^{1+o(1)}$$ in expectation and with high probability. This already establishes at least a roughly $\log n$ complexity gap between randomized protocols in GlobalClock and LocalClock. [2] Prior analyses of randomized ContentionResolution protocols in LocalClock guaranteed a certain latency with high probability, i.e., with probability $1-1/\text{poly}(n)$. We observe that it is just as natural to measure expected latency, and prove a $\log n$-factor complexity gap between the two objectives for memoryless protocols. The In-Expectation complexity is $Θ(n \log n/\log\log n)$ whereas the With-High-Probability latency is $Θ(n\log^2 n/\log\log n)$. Three of these four upper and lower bounds are new. [3] Given the complexity separation above, one would naturally want a ContentionResolution protocol that is optimal under both the In-Expectation and With-High-Probability metrics. This is impossible! It is even impossible to achieve In-Expectation latency $o(n\log^2 n/(\log\log n)^2)$ and With-High-Probability latency $n\log^{O(1)} n$ simultaneously.

</details>


### [9] [OServe: Accelerating LLM Serving via Spatial-Temporal Workload Orchestration](https://arxiv.org/abs/2602.12151)
*Youhe Jiang,Fangcheng Fu,Taiyi Wang,Guoliang He,Eiko Yoneki*

Main category: cs.DC

TL;DR: OServe是一个针对大语言模型服务的异构灵活部署系统，通过感知工作负载的调度算法和自适应切换机制，解决服务中的时空异构性问题，相比现有系统性能提升最高2倍。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统通常假设工作负载在空间上均匀、时间上稳定，采用同质化静态模型部署。但实际工作负载存在显著的空间异构性（不同请求的计算和内存需求不同）和时间异构性（工作负载组成随时间变化），这种假设与现实不匹配导致性能不佳。

Method: 1. 引入工作负载感知调度算法，根据实时工作负载特征优化异构模型部署；2. 提出高效的工作负载自适应切换方法，根据预测的工作负载变化迁移模型部署。

Result: 在真实世界跟踪数据上的实验表明，OServe相比最先进的服务系统性能提升最高达2倍（平均1.5倍）。

Conclusion: OServe通过异构灵活的模型部署有效解决了LLM服务中的时空异构性问题，显著提升了服务性能，为处理实际工作负载的复杂性提供了有效解决方案。

Abstract: Serving Large Language Models (LLMs) can benefit immensely from parallelizing both the model and input requests across multiple devices, but incoming workloads exhibit substantial spatial and temporal heterogeneity. Spatially, workloads comprise heterogeneous requests with varying compute and memory demands. Temporally, workload composition varies over time. Nevertheless, existing systems typically assume spatially uniform and temporally stable workloads, employing a homogeneous, static model deployment. This mismatch between the assumption and real-world spatial-temporal heterogeneity results in suboptimal performance. We present OServe, an LLM serving system with heterogeneous and flexible model deployment that addresses both spatial and temporal heterogeneity. First, OServe introduces a novel workload-aware scheduling algorithm that optimizes heterogeneous model deployments according to real-time workload characteristics. Second, OServe proposes an efficient workload-adaptive switching method that migrates model deployments in response to predicted workload changes. Experiments on real-world traces show that OServe improves performance by up to 2$\times$ (average: 1.5$\times$) compared to state-of-the-art serving systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [A 16 nm 1.60TOPS/W High Utilization DNN Accelerator with 3D Spatial Data Reuse and Efficient Shared Memory Access](https://arxiv.org/abs/2602.11357)
*Xiaoling Yi,Ryan Antonio,Yunhao Deng,Fanchen Kong,Joren Dumoulin,Jun Yin,Marian Verhelst*

Main category: cs.AR

TL;DR: Voltra芯片采用3D空间数据流和灵活共享内存架构，通过3D空间数据复用和动态内存分配，相比传统2D设计提升空间利用率2.0倍，时间利用率2.12-2.94倍，在16nm工艺下实现1.60 TOPS/W能效和1.25 TOPS/mm²面积效率。


<details>
  <summary>Details</summary>
Motivation: 提高AI工作负载的计算利用率对于通用DNN加速器的效率至关重要。传统设计在空间和时间利用率方面存在限制，需要新的架构来优化数据复用和内存访问。

Method: 提出Voltra芯片架构，采用3D空间数据流实现三维平衡数据复用，结合灵活的数据流处理器支持混合粒度硬件预取和动态内存分配，优化共享内存访问。

Result: 3D设计相比传统2D设计提升空间利用率2.0倍；灵活内存架构提升时间利用率2.12-2.94倍，总延迟加速1.15-2.36倍；16nm工艺下实现1.60 TOPS/W峰值系统能效和1.25 TOPS/mm²系统面积效率。

Conclusion: Voltra芯片通过创新的3D空间数据流和灵活共享内存架构，在保持与最先进方案竞争力的同时，实现了跨多样化工作负载的高计算利用率。

Abstract: Achieving high compute utilization across a wide range of AI workloads is crucial for the efficiency of versatile DNN accelerators. This paper presents the Voltra chip and its utilization-optimised DNN accelerator architecture, which leverages 3-Dimensional (3D) spatial data reuse along with efficient and flexible shared memory access. The 3D spatial dataflow enables balanced spatial data reuse across three dimensions, improving spatial utilization by up to 2.0x compared to a conventional 2D design. Inside the shared memory access architecture, Voltra incorporates flexible data streamers that enable mixed-grained hardware data pre-fetching and dynamic memory allocation, further improving the temporal utilization by 2.12-2.94x and achieving 1.15-2.36x total latency speedup compared with the non-prefetching and separated memory architecture, respectively. Fabricated in 16nm technology, our chip achieves 1.60 TOPS/W peak system energy efficiency and 1.25 TOPS/mm2 system area efficiency, which is competitive with state-of-the-art solutions while achieving high utilization across diverse workloads.

</details>


### [11] [EM-Aware Physical Synthesis: Neural Inductor Modeling and Intelligent Placement & Routing for RF Circuits](https://arxiv.org/abs/2602.11461)
*Yilun Huang,Asal Mehradfar,Salman Avestimehr,Hamidreza Aghasi*

Main category: cs.AR

TL;DR: 提出一个ML驱动的RF物理合成框架，将电路网表转换为可制造的GDSII布局，解决了现有ML方法无法生成可制造布局的问题。


<details>
  <summary>Details</summary>
Motivation: 现有ML方法在拓扑选择和参数优化方面取得成功，但由于组件模型过于简化且缺乏布线能力，无法生成可制造的布局。需要解决这些限制以实现自动化RF物理设计。

Method: 通过三个关键创新：1) 基于18,210个电感几何结构训练的神经网络框架，预测Q因子误差小于2%；2) 智能P-Cell优化器，减少布局面积同时保持DRC合规；3) 完整的布局布线引擎，具有频率相关的EM间距规则和DRC感知合成。

Result: 神经网络模型在1-100 GHz范围内表现出卓越精度，实现实时推理的EM精确组件合成。框架成功生成DRC感知的RF电路GDSII布局，高Q布局成功率93.77%。

Conclusion: 该框架代表了向自动化RF物理设计迈出的重要一步，能够生成可制造的GDSII布局，解决了现有ML方法的局限性。

Abstract: This paper presents an ML-driven framework for automated RF physical synthesis that transforms circuit netlists into manufacturable GDSII layouts. While recent ML approaches demonstrate success in topology selection and parameter optimization, they fail to produce manufacturable layouts due to oversimplified component models and lack of routing capabilities. Our framework addresses these limitations through three key innovations: (1) a neural network framework trained on 18,210 inductor geometries with frequency sweeps from 1-100 GHz, generating 7.5 million training samples, that predicts inductor Q-factor with less than 2% error and enables fast gradient-based layout optimization with a 93.77% success rate in producing high-Q layouts; (2) an intelligent P-Cell optimizer that reduces layout area while maintaining design-rule-check (DRC) compliance; and (3) a complete placement and routing engine with frequency-dependent EM spacing rules and DRC-aware synthesis. The neural inductor model demonstrates superior accuracy across 1-100 GHz, enabling EM-accurate component synthesis with real-time inference. The framework successfully generates DRC-aware GDSII layouts for RF circuits, representing a significant step toward automated RF physical design.

</details>


### [12] [PAM: Processing Across Memory Hierarchy for Efficient KV-centric LLM Serving System](https://arxiv.org/abs/2602.11521)
*Lian Liu,Shixin Zhao,Yutian Zhou,Yintao He,Mengdi Wang,Yinhe Han,Ying Wang*

Main category: cs.AR

TL;DR: PAM是一个KV中心的LLM服务系统，通过协调异构PIM内存设备的分层架构，平衡高内存带宽与可扩展容量，解决KV相关操作的内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛采用，KV相关操作（注意力计算和KV缓存存储）成为关键瓶颈，需要大量内存带宽和容量。现有LLM服务系统针对计算密集型工作负载优化，无法有效处理这些内存密集型操作，即使使用PIM技术，当前的单级内存设计也无法同时满足带宽和容量需求。

Method: 1. 利用KV访问模式中的上下文局部性，智能地将KV令牌分布在内存层次结构中；2. 引入PAMattention算法，支持跨异构PIM设备的细粒度并行注意力计算；3. 包含设备内KV映射、设备间KV迁移接口和设备间在线KV调度算法，动态平衡计算工作负载。

Result: PAM通过同时解决带宽和容量需求，显著提高了LLM服务系统的效率和可扩展性，为大规模AI时代提供了经济高效的高性能解决方案。

Conclusion: PAM通过协调异构PIM内存设备的分层架构，平衡高内存带宽与可扩展容量，有效解决了LLM服务中的KV相关内存瓶颈问题，为大规模AI应用提供了可行的解决方案。

Abstract: The widespread adoption of Large Language Models (LLMs) has exponentially increased the demand for efficient serving systems. With growing requests and context lengths, key-value (KV)-related operations, including attention computation and KV cache storage, have emerged as critical bottlenecks. They require massive memory bandwidth and capacity. Unfortunately, existing LLM serving systems, optimized for compute-bound workloads, fail to handle these memory-intensive operations effectively. Even with Processing-In-Memory (PIM) technology, current single-level memory designs cannot simultaneously satisfy the bandwidth and capacity requirements.
  To address these challenges, we propose Processing Across Memory (PAM), a KV-centric LLM serving system that coordinates heterogeneous PIM-enabled memory devices within a hierarchical architecture. PAM introduces a novel computing paradigm to balance high memory bandwidth with scalable capacity. First, PAM exploits the inherent context locality in KV access patterns to intelligently distribute KV tokens across the memory hierarchy. Second, to further exploit context locality, it introduces the PAMattention algorithm, enabling fine-grained parallel attention computation across heterogeneous PIM devices. Finally, PAM incorporates an intra-device KV mapping, inter-device KV migration interface, and an inter-device online KV scheduling algorithm to dynamically balance computational workloads. By addressing both bandwidth and capacity demands simultaneously, PAM significantly enhances the efficiency and scalability of LLM serving systems, paving the way for cost-effective, high-performance solutions in the era of large-scale AI.

</details>


### [13] [Benchmarking for Single Feature Attribution with Microarchitecture Cliffs](https://arxiv.org/abs/2602.11580)
*Hao Zhen,Qingxuan Kang,Yungang Bao,Trevor E. Carlson*

Main category: cs.AR

TL;DR: 提出Microarchitecture Cliffs基准生成方法，用于暴露模拟器与RTL之间的微架构行为差异，帮助校准模拟器模型，显著提升性能预测精度。


<details>
  <summary>Details</summary>
Motivation: 架构模拟器在早期微架构探索中至关重要，但其有效性受限于保真度：模拟器可能与最终RTL行为存在偏差，导致不可靠的性能估计。因此，需要将模拟器行为与RTL作为真实微架构进行校准，以实现准确的性能建模。

Method: 提出Cliff基准生成方法，在识别需要校准的关键架构组件后，通过一组基准测试将微架构差异精确归因于单个微架构特征。同时开发了一套自动化工具来提高Cliff工作流程的效率。

Result: 将Cliff方法应用于校准XiangShan版本的gem5（XS-GEM5）与XiangShan开源CPU（XS-RTL）。在Cliff基准上，将XS-GEM5的性能误差从59.2%降低到仅1.4%。同时，校准使代表性紧密耦合微架构特征的相对误差降低了48.03%，并在SPECint2017和SPECfp2017上分别降低了15.1%和21.0%的绝对性能误差。

Conclusion: Microarchitecture Cliffs方法能有效暴露模拟器与RTL之间的微架构行为差异，通过精确的差异归因和自动化工具，显著提高模拟器校准的准确性和效率，为可靠的性能建模提供有力支持。

Abstract: Architectural simulators play a critical role in early microarchitectural exploration due to their flexibility and high productivity. However, their effectiveness is often constrained by fidelity: simulators may deviate from the behavior of the final RTL, leading to unreliable performance estimates. Consequently, model calibration, which aligns simulator behavior with the RTL as the ground-truth microarchitecture, becomes essential for achieving accurate performance modeling.
  To facilitate model calibration accuracy, we propose Microarchitecture Cliffs, a benchmark generation methodology designed to expose mismatches in microarchitectural behavior between the simulator and RTL. After identifying the key architectural components that require calibration, the Cliff methodology enables precise attribution of microarchitectural differences to a single microarchitectural feature through a set of benchmarks. In addition, we develop a set of automated tools to improve the efficiency of the Cliff workflow.
  We apply the Cliff methodology to calibrate the XiangShan version of gem5 (XS-GEM5) against the XiangShan open-source CPU (XS-RTL). We reduce the performance error of XS-GEM5 from 59.2% to just 1.4% on the Cliff benchmarks. Meanwhile, the calibration guided by Cliffs effectively reduces the relative error of a representative tightly coupled microarchitectural feature by 48.03%. It also substantially lowers the absolute performance error, with reductions of 15.1% and 21.0% on SPECint2017 and SPECfp2017, respectively.

</details>


### [14] [Device-Circuit Co-Design of Variation-Resilient Read and Write Drivers for Antiferromagnetic Tunnel Junction (AFMTJ) Memories](https://arxiv.org/abs/2602.11614)
*Yousuf Choudhary,Tosiron Adegbija*

Main category: cs.AR

TL;DR: 该论文提出了一种针对反铁磁隧道结（AFMTJ）的器件-电路协同设计读写接口，通过非对称脉冲驱动器和自定时感测放大器解决了AFMTJ超快动态和低TMR带来的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: AFMTJ具有皮秒级开关速度和高度集成密度，适用于内存计算，但其超快动态特性和低隧道磁阻（TMR）使得现有MRAM接口不可靠，需要专门优化的读写接口。

Method: 使用校准的SPICE AFMTJ模型作为基准，分析了传统驱动器的局限性，提出了非对称脉冲驱动器实现确定性皮秒开关，以及具有动态触发点调谐的自定时感测放大器用于低TMR检测。

Result: SPICE和蒙特卡洛评估实验表明，所提电路在保持AFMTJ延迟和能耗优势的同时，在现实PVT和3D集成寄生参数下实现了稳健的读写良率，优于标准MRAM前端电路。

Conclusion: 通过器件-电路协同设计方法，成功开发了针对AFMTJ特性的优化读写接口，解决了超快动态和低TMR带来的可靠性挑战，为AFMTJ在内存计算中的应用提供了可行解决方案。

Abstract: Antiferromagnetic Tunnel Junctions (AFMTJs) offer picosecond switching and high integration density for in-memory computing, but their ultrafast dynamics and low tunnel magnetoresistance (TMR) make state-of-the-art MRAM interfaces unreliable. This work develops a device-circuit co-designed read/write interface optimized for AFMTJ behavior. Using a calibrated SPICE AFMTJ model as a baseline, we identify the limitations of conventional drivers and propose an asymmetric pulse driver (PD) for deterministic picosecond switching and a self-timed sense amplifier (STSA) with dynamic trip-point tuning for low-TMR sensing. Our experiments using SPICE and Monte Carlo evaluations demonstrate that the proposed circuits preserve AFMTJ latency and energy benefits while achieving robust read/write yield under realistic PVT and 3D integration parasitics, outperforming standard MRAM front-ends under the same conditions.

</details>


### [15] [MING: An Automated CNN-to-Edge MLIR HLS framework](https://arxiv.org/abs/2602.11966)
*Jiahong Bi,Lars Schütze,Jeronimo Castrillon*

Main category: cs.AR

TL;DR: MING是一个基于MLIR的FPGA HLS框架，针对边缘设备资源约束优化，通过流式架构和缓冲管理，相比现有框架在CNN内核上实现15-200倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习应用向边缘计算迁移，FPGA因其能效优势被广泛采用。现有基于MLIR的HLS框架往往忽视边缘设备的严格资源约束，需要解决这一问题。

Method: 提出MING框架，采用基于MLIR的抽象和自动化HLS设计流程，采用流式架构配合精心管理的缓冲区，专门针对资源约束优化并确保低延迟。

Result: 相比现有框架，MING在标准CNN内核（最多四层）上平均实现15倍加速，单层内核可达200倍加速。对于大输入尺寸的内核，MING能生成满足硬件资源约束的高效设计，而现有框架难以实现。

Conclusion: MING框架成功解决了边缘设备资源约束下的FPGA HLS设计问题，通过流式架构和缓冲管理实现了显著的性能提升，特别适合资源受限的边缘计算场景。

Abstract: Driven by the increasing demand for low-latency and real-time processing, machine learning applications are steadily migrating toward edge computing platforms, where Field-Programmable Gate Arrays (FPGAs) are widely adopted for their energy efficiency compared to CPUs and GPUs. To generate high-performance and low-power FPGA designs, several frameworks built upon High Level Synthesis (HLS) vendor tools have been proposed, among which MLIR-based frameworks are gaining significant traction due to their extensibility and ease of use. However, existing state-of-the-art frameworks often overlook the stringent resource constraints of edge devices. To address this limitation, we propose MING, an Multi-Level Intermediate Representation (MLIR)-based framework that abstracts and automates the HLS design process. Within this framework, we adopt a streaming architecture with carefully managed buffers, specifically designed to handle resource constraints while ensuring low-latency. In comparison with recent frameworks, our approach achieves on average 15x speedup for standard Convolutional Neural Network (CNN) kernels with up to four layers, and up to 200x for single-layer kernels. For kernels with larger input sizes, MING is capable of generating efficient designs that respect hardware resource constraints, whereas state-of-the-art frameworks struggle to meet.

</details>
