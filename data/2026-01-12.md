<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Categorical Foundations for CuTe Layouts](https://arxiv.org/abs/2601.05972)
*Jack Carlisle,Jay Shah,Reuben Stern,Paul VanKoughnett*

Main category: cs.PL

TL;DR: 该论文为NVIDIA CUTLASS库的布局代数提供了一个范畴论框架，定义了Tuple和Nest两个范畴，其态射产生布局，并实现了Python验证实现。


<details>
  <summary>Details</summary>
Motivation: CUTLASS库提供了强大的多维张量布局操作，但其底层代数缺乏形式化理论基础。论文旨在为这些布局操作建立严格的范畴论框架，以更好地理解和形式化布局代数。

Method: 定义了两个范畴Tuple和Nest，其态射产生布局。在这些范畴的态射上定义了一套操作，并证明这些操作与CUTLASS中相应布局操作的兼容性。完全刻画了从该构造产生的布局类型。

Result: 建立了CUTLASS布局代数的范畴论框架，证明了范畴操作与布局操作的兼容性，完整描述了可生成的布局类型，并提供了Python实现验证与CUTLASS行为的一致性。

Conclusion: 该范畴论框架为CUTLASS布局代数提供了坚实的理论基础，有助于更好地理解、分析和优化GPU张量布局，实现代码可在GitHub仓库获取。

Abstract: NVIDIA's CUTLASS library provides a robust and expressive set of methods for describing and manipulating multi-dimensional tensor data on the GPU. These methods are conceptually grounded in the abstract notion of a CuTe layout and a rich algebra of such layouts, including operations such as composition, logical product, and logical division. In this paper, we present a categorical framework for understanding this layout algebra by focusing on a naturally occurring class of tractable layouts. To this end, we define two categories Tuple and Nest whose morphisms give rise to layouts. We define a suite of operations on morphisms in these categories and prove their compatibility with the corresponding layout operations. Moreover, we give a complete characterization of the layouts which arise from our construction. Finally, we provide a Python implementation of our categorical constructions, along with tests that demonstrate alignment with CUTLASS behavior. This implementation can be found at our git repository https://github.com/ColfaxResearch/layout-categories.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Self-Evolving Distributed Memory Architecture for Scalable AI Systems](https://arxiv.org/abs/2601.05569)
*Zixuan Li,Chuanzhen Wang,Haotian Sun*

Main category: cs.DC

TL;DR: 提出Self-Evolving Distributed Memory Architecture (SEDMA)框架，通过三层统一内存管理解决分布式AI系统的内存效率问题，在多个基准测试中显著提升内存利用率、计算速度和通信效率。


<details>
  <summary>Details</summary>
Motivation: 分布式AI系统面临跨计算、通信和部署层的严重内存管理挑战：RRAM内存计算受限于设备非理想性和固定阵列大小；去中心化AI框架在NAT约束网络中因静态路由而内存效率低下；多智能体部署系统将应用逻辑与执行环境紧密耦合，阻碍自适应内存优化。这些问题的根源在于缺乏跨架构层的协调内存管理。

Method: 提出三层框架SEDMA：1) 基于设备特性的内存引导矩阵处理与动态分区；2) 考虑网络拓扑和计算容量的内存感知对等节点选择；3) 通过持续重配置实现运行时自适应部署优化。框架维护双内存系统，同时跟踪长期性能模式和短期工作负载统计。

Result: 在COCO 2017、ImageNet和SQuAD上的实验表明，该方法达到87.3%的内存利用效率和142.5 ops/s，优于Ray Distributed的72.1%和98.7 ops/s；通信延迟降低30.2%至171.2毫秒，资源利用率提升至82.7%。

Conclusion: 贡献包括：跨三层架构的协调内存管理、工作负载自适应资源分配、以及支持动态系统优化的双内存架构。该框架显著提升了分布式AI系统的内存效率和整体性能。

Abstract: Distributed AI systems face critical memory management challenges across computation, communication, and deployment layers. RRAM based in memory computing suffers from scalability limitations due to device non idealities and fixed array sizes. Decentralized AI frameworks struggle with memory efficiency across NAT constrained networks due to static routing that ignores computational load. Multi agent deployment systems tightly couple application logic with execution environments, preventing adaptive memory optimization. These challenges stem from a fundamental lack of coordinated memory management across architectural layers. We introduce Self Evolving Distributed Memory Architecture for Scalable AI Systems, a three layer framework that unifies memory management across computation, communication, and deployment. Our approach features (1) memory guided matrix processing with dynamic partitioning based on device characteristics, (2) memory aware peer selection considering network topology and computational capacity, and (3) runtime adaptive deployment optimization through continuous reconfiguration. The framework maintains dual memory systems tracking both long term performance patterns and short term workload statistics. Experiments on COCO 2017, ImageNet, and SQuAD show that our method achieves 87.3 percent memory utilization efficiency and 142.5 operations per second compared to Ray Distributed at 72.1 percent and 98.7 operations per second, while reducing communication latency by 30.2 percent to 171.2 milliseconds and improving resource utilization to 82.7 percent. Our contributions include coordinated memory management across three architectural layers, workload adaptive resource allocation, and a dual memory architecture enabling dynamic system optimization.

</details>


### [3] [Performance-Portable Optimization and Analysis of Multiple Right-Hand Sides in a Lattice QCD Solver](https://arxiv.org/abs/2601.05816)
*Shiting Long,Gustavo Ramirez-Hidalgo,Stepan Nassyr,Jose Jimenez-Merchan,Andreas Frommer,Dirk Pleiter*

Main category: cs.DC

TL;DR: 扩展DD-αAMG求解器以支持多右端项，优化数据布局提升SIMD利用，在x86和Arm集群上实现性能可移植性，并评估Arm SME指令集的潜力。


<details>
  <summary>Details</summary>
Motivation: 稀疏线性系统迭代求解器的高计算成本和科学应用中常见的内存带宽限制，需要优化数据局部性和数据传输效率。

Method: 扩展DD-αAMG求解器支持多右端项（有/无奇偶预处理），引入灵活接口支持多种数据布局以优化自动向量化，实现新的数据布局提升SIMD利用率，在x86和Arm集群上评估，并探索Arm SME指令集实现。

Result: 在x86和Arm集群上展示了性能可移植性和相似的加速效果，性能分析揭示了架构约束和编译器行为带来的复杂性，对Arm SME指令集提供了早期评估。

Conclusion: 通过多右端项支持和数据布局优化，有效提升了DD-αAMG求解器的性能，实现了跨架构的性能可移植性，并为Arm SME指令集的应用提供了有价值的见解。

Abstract: Managing the high computational cost of iterative solvers for sparse linear systems is a known challenge in scientific computing. Moreover, scientific applications often face memory bandwidth constraints, making it critical to optimize data locality and enhance the efficiency of data transport. We extend the lattice QCD solver DD-$α$AMG to incorporate multiple right-hand sides (rhs) for both the Wilson-Dirac operator evaluation and the GMRES solver, with and without odd-even preconditioning. To optimize auto-vectorization, we introduce a flexible interface that supports various data layouts and implement a new data layout for better SIMD utilization. We evaluate our optimizations on both x86 and Arm clusters, demonstrating performance portability with similar speedups. A key contribution of this work is the performance analysis of our optimizations, which reveals the complexity introduced by architectural constraints and compiler behavior. Additionally, we explore different implementations leveraging a new matrix instruction set for Arm called SME and provide an early assessment of its potential benefits.

</details>


### [4] [Multi-Modal Style Transfer-based Prompt Tuning for Efficient Federated Domain Generalization](https://arxiv.org/abs/2601.05955)
*Yuliang Chen,Xi Lin,Jun Wu,Xiangrui Cai,Qiaolun Zhang,Xichun Fan,Jiapeng Xu,Xiu Su*

Main category: cs.DC

TL;DR: FaST-PT是一个联邦域泛化框架，通过多模态风格转移进行本地特征增强，并采用双提示模块（全局提示和域提示）来捕获通用和特定领域知识，实现高效的无监督域适应。


<details>
  <summary>Details</summary>
Motivation: 现有联邦域泛化方法面临跨客户端数据异构性挑战，且通信和计算开销较大。需要一种既能处理数据异质性又能高效适应未见域的新框架。

Method: 1. 提出轻量级多模态风格转移方法，在文本监督下转换图像嵌入以扩展训练数据分布；2. 设计双提示模块，将提示分解为全局提示（捕获跨客户端通用知识）和域提示（捕获本地特定领域知识）；3. 引入域感知提示生成机制，为每个样本自适应生成合适提示，通过知识融合促进未见域适应。

Result: 在PACS和DomainNet等四个跨域基准数据集上的实验表明，FaST-PT优于FedDG-GA和DiPrompt等SOTA联邦域泛化方法。消融研究进一步验证了其有效性和效率。

Conclusion: FaST-PT通过本地特征增强和高效的双提示机制，成功解决了联邦域泛化中的数据异构性和计算开销问题，在未见域适应方面表现出色。

Abstract: Federated Domain Generalization (FDG) aims to collaboratively train a global model across distributed clients that can generalize well on unseen domains. However, existing FDG methods typically struggle with cross-client data heterogeneity and incur significant communication and computation overhead. To address these challenges, this paper presents a new FDG framework, dubbed FaST-PT, which facilitates local feature augmentation and efficient unseen domain adaptation in a distributed manner. First, we propose a lightweight Multi-Modal Style Transfer (MST) method to transform image embedding under text supervision, which could expand the training data distribution and mitigate domain shift. We then design a dual-prompt module that decomposes the prompt into global and domain prompts. Specifically, global prompts capture general knowledge from augmented embedding across clients, while domain prompts capture domain-specific knowledge from local data. Besides, Domain-aware Prompt Generation (DPG) is introduced to adaptively generate suitable prompts for each sample, which facilitates unseen domain adaptation through knowledge fusion. Extensive experiments on four cross-domain benchmark datasets, e.g., PACS and DomainNet, demonstrate the superior performance of FaST-PT over SOTA FDG methods such as FedDG-GA and DiPrompt. Ablation studies further validate the effectiveness and efficiency of FaST-PT.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [5] [LACIN: Linearly Arranged Complete Interconnection Networks](https://arxiv.org/abs/2601.05668)
*Ramón Beivide,Cristóbal Camarero,Carmen Martínez,Enrique Vallejo,Mateo Valero*

Main category: cs.AR

TL;DR: LACIN是一种使用相同索引端口连接交换机的完整图网络实现方法，可降低大规模网络的布线和路由复杂性


<details>
  <summary>Details</summary>
Motivation: 基于完整图拓扑的网络（如Dragonfly和HyperX）在大型系统中链接数量巨大且快速增长，导致布线复杂和路由困难

Method: 提出LACIN实现方法，使用相同索引的端口连接交换机，简化网络实现

Result: LACIN减少了网络的布线复杂性和路由复杂性，便于不同规模并行计算机网络的部署

Conclusion: LACIN为从VLSI系统到最大超级计算机的各种规模并行计算机提供了更易部署的网络实现方案

Abstract: Several interconnection networks are based on the complete graph topology. Networks with a moderate size can be based on a single complete graph. However, large-scale networks such as Dragonfly and HyperX use, respectively, a hierarchical or a multi-dimensional composition of complete graphs.
  The number of links in these networks is huge and grows rapidly with their size. This paper introduces LACIN, a set of complete graph implementations that use identically indexed ports to link switches. This way of implementing the network reduces the complexity of its cabling and its routing. LACIN eases the deployment of networks for parallel computers of different scales, from VLSI systems to the largest supercomputers.

</details>
