{"id": "2510.05245", "categories": ["cs.AR", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05245", "abs": "https://arxiv.org/abs/2510.05245", "authors": ["Yue Pan", "Zihan Xia", "Po-Kai Hsu", "Lanxiang Hu", "Hyungyo Kim", "Janak Sharda", "Minxuan Zhou", "Nam Sung Kim", "Shimeng Yu", "Tajana Rosing", "Mingu Kang"], "title": "Stratum: System-Hardware Co-Design with Tiered Monolithic 3D-Stackable DRAM for Efficient MoE Serving", "comment": null, "summary": "As Large Language Models (LLMs) continue to evolve, Mixture of Experts (MoE)\narchitecture has emerged as a prevailing design for achieving state-of-the-art\nperformance across a wide range of tasks. MoE models use sparse gating to\nactivate only a handful of expert sub-networks per input, achieving\nbillion-parameter capacity with inference costs akin to much smaller models.\nHowever, such models often pose challenges for hardware deployment due to the\nmassive data volume introduced by the MoE layers. To address the challenges of\nserving MoE models, we propose Stratum, a system-hardware co-design approach\nthat combines the novel memory technology Monolithic 3D-Stackable DRAM (Mono3D\nDRAM), near-memory processing (NMP), and GPU acceleration. The logic and Mono3D\nDRAM dies are connected through hybrid bonding, whereas the Mono3D DRAM stack\nand GPU are interconnected via silicon interposer. Mono3D DRAM offers higher\ninternal bandwidth than HBM thanks to the dense vertical interconnect pitch\nenabled by its monolithic structure, which supports implementations of\nhigher-performance near-memory processing. Furthermore, we tackle the latency\ndifferences introduced by aggressive vertical scaling of Mono3D DRAM along the\nz-dimension by constructing internal memory tiers and assigning data across\nlayers based on access likelihood, guided by topic-based expert usage\nprediction to boost NMP throughput. The Stratum system achieves up to 8.29x\nimprovement in decoding throughput and 7.66x better energy efficiency across\nvarious benchmarks compared to GPU baselines.", "AI": {"tldr": "\u63d0\u51faStratum\u7cfb\u7edf\uff0c\u7ed3\u5408Mono3D DRAM\u3001\u8fd1\u5185\u5b58\u5904\u7406\u548cGPU\u52a0\u901f\uff0c\u89e3\u51b3MoE\u6a21\u578b\u786c\u4ef6\u90e8\u7f72\u4e2d\u7684\u6570\u636e\u91cf\u6311\u6218\uff0c\u5b9e\u73b08.29\u500d\u89e3\u7801\u541e\u5410\u91cf\u63d0\u5347\u548c7.66\u500d\u80fd\u6548\u6539\u8fdb\u3002", "motivation": "MoE\u6a21\u578b\u867d\u7136\u901a\u8fc7\u7a00\u758f\u95e8\u63a7\u5b9e\u73b0\u4e86\u4ebf\u7ea7\u53c2\u6570\u5bb9\u91cf\u548c\u8f83\u5c0f\u63a8\u7406\u6210\u672c\uff0c\u4f46\u5176MoE\u5c42\u5f15\u5165\u7684\u6d77\u91cf\u6570\u636e\u7ed9\u786c\u4ef6\u90e8\u7f72\u5e26\u6765\u6311\u6218\u3002", "method": "\u91c7\u7528\u7cfb\u7edf-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u7ed3\u5408Mono3D DRAM\uff08\u63d0\u4f9b\u6bd4HBM\u66f4\u9ad8\u5185\u90e8\u5e26\u5bbd\uff09\u3001\u8fd1\u5185\u5b58\u5904\u7406\u548cGPU\u52a0\u901f\uff0c\u901a\u8fc7\u57fa\u4e8e\u4e3b\u9898\u7684\u4e13\u5bb6\u4f7f\u7528\u9884\u6d4b\u4f18\u5316\u6570\u636e\u5206\u914d\u3002", "result": "\u76f8\u6bd4GPU\u57fa\u7ebf\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad88.29\u500d\u7684\u89e3\u7801\u541e\u5410\u91cf\u63d0\u5347\u548c7.66\u500d\u7684\u80fd\u6548\u6539\u8fdb\u3002", "conclusion": "Stratum\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684\u5185\u5b58\u6280\u672f\u548c\u6570\u636e\u5904\u7406\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86MoE\u6a21\u578b\u786c\u4ef6\u90e8\u7f72\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2510.05327", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05327", "abs": "https://arxiv.org/abs/2510.05327", "authors": ["Zahin Ibnat", "Paul E. Calzada", "Rasin Mohammed Ihtemam", "Sujan Kumar Saha", "Jingbo Zhou", "Farimah Farahmandi", "Mark Tehranipoor"], "title": "DeepV: A Model-Agnostic Retrieval-Augmented Framework for Verilog Code Generation with a High-Quality Knowledge Base", "comment": "22 pages, 6 figures", "summary": "As large language models (LLMs) continue to be integrated into modern\ntechnology, there has been an increased push towards code generation\napplications, which also naturally extends to hardware design automation.\nLLM-based solutions for register transfer level (RTL) code generation for\nintellectual property (IP) designs have grown, especially with fine-tuned LLMs,\nprompt engineering, and agentic approaches becoming popular in literature.\nHowever, a gap has been exposed in these techniques, as they fail to integrate\nnovel IPs into the model's knowledge base, subsequently resulting in poorly\ngenerated code. Additionally, as general-purpose LLMs continue to improve,\nfine-tuned methods on older models will not be able to compete to produce more\naccurate and efficient designs. Although some retrieval augmented generation\n(RAG) techniques exist to mitigate challenges presented in fine-tuning\napproaches, works tend to leverage low-quality codebases, incorporate\ncomputationally expensive fine-tuning in the frameworks, or do not use RAG\ndirectly in the RTL generation step. In this work, we introduce DeepV: a\nmodel-agnostic RAG framework to generate RTL designs by enhancing context\nthrough a large, high-quality dataset without any RTL-specific training. Our\nframework benefits the latest commercial LLM, OpenAI's GPT-5, with a near 17%\nincrease in performance on the VerilogEval benchmark. We host DeepV for use by\nthe community in a Hugging Face (HF) Space:\nhttps://huggingface.co/spaces/FICS-LLM/DeepV.", "AI": {"tldr": "DeepV\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684RAG\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210RTL\u8bbe\u8ba1\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u589e\u5f3a\u4e0a\u4e0b\u6587\uff0c\u65e0\u9700RTL\u7279\u5b9a\u8bad\u7ec3\uff0c\u5728VerilogEval\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6027\u80fd\u63d0\u5347\u8fd117%\u3002", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u5728\u786c\u4ef6\u8bbe\u8ba1\u81ea\u52a8\u5316\u4e2d\u5b58\u5728\u7f3a\u9677\uff1a\u65e0\u6cd5\u6574\u5408\u65b0IP\u5230\u77e5\u8bc6\u5e93\uff0c\u5bfc\u81f4\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u5dee\uff1b\u57fa\u4e8e\u65e7\u6a21\u578b\u7684\u5fae\u8c03\u65b9\u6cd5\u65e0\u6cd5\u4e0e\u901a\u7528LLM\u7ade\u4e89\uff1b\u73b0\u6709RAG\u6280\u672f\u5b58\u5728\u4ee3\u7801\u5e93\u8d28\u91cf\u4f4e\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u672a\u76f4\u63a5\u7528\u4e8eRTL\u751f\u6210\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDeepV\u6846\u67b6\uff0c\u91c7\u7528\u6a21\u578b\u65e0\u5173\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u589e\u5f3a\u4e0a\u4e0b\u6587\uff0c\u65e0\u9700\u8fdb\u884cRTL\u7279\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728\u6700\u65b0\u5546\u4e1aLLM\uff08OpenAI GPT-5\uff09\u4e0a\uff0cVerilogEval\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u63d0\u5347\u8fd117%\u3002", "conclusion": "DeepV\u662f\u4e00\u4e2a\u6709\u6548\u7684RAG\u6846\u67b6\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347RTL\u4ee3\u7801\u751f\u6210\u6027\u80fd\uff0c\u5df2\u4f5c\u4e3a\u5f00\u6e90\u5de5\u5177\u5728Hugging Face\u4e0a\u53d1\u5e03\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2510.05632", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05632", "abs": "https://arxiv.org/abs/2510.05632", "authors": ["Tianhao Zhu", "Dahu Feng", "Erhu Feng", "Yubin Xia"], "title": "From Principles to Practice: A Systematic Study of LLM Serving on Multi-core NPUs", "comment": null, "summary": "With the widespread adoption of Large Language Models (LLMs), the demand for\nhigh-performance LLM inference services continues to grow. To meet this demand,\na growing number of AI accelerators have been proposed, such as Google TPU,\nHuawei NPU, Graphcore IPU, and Cerebras WSE, etc. Most of these accelerators\nadopt multi-core architectures to achieve enhanced scalability, but lack the\nflexibility of SIMT architectures. Therefore, without careful configuration of\nthe hardware architecture, as well as deliberate design of tensor parallelism\nand core placement strategies, computational resources may be underutilized,\nresulting in suboptimal inference performance.\n  To address these challenges, we first present a multi-level simulation\nframework with both transaction-level and performance-model-based simulation\nfor multi-core NPUs. Using this simulator, we conduct a systematic analysis and\nfurther propose the optimal solutions for tensor parallelism strategies, core\nplacement policies, memory management methods, as well as the selection between\nPD-disaggregation and PD-fusion on multi-core NPUs. We conduct comprehensive\nexperiments on representative LLMs and various NPU configurations. The\nevaluation results demonstrate that, our solution can achieve 1.32x-6.03x\nspeedup compared to SOTA designs for multi-core NPUs across different hardware\nconfigurations. As for LLM serving, our work offers guidance on designing\noptimal hardware architectures and serving strategies for multi-core NPUs\nacross various LLM workloads.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6838NPU\u7684LLM\u63a8\u7406\u4f18\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u7ea7\u4eff\u771f\u6846\u67b6\u548c\u7cfb\u7edf\u5206\u6790\uff0c\u5728\u5f20\u91cf\u5e76\u884c\u7b56\u7565\u3001\u6838\u5fc3\u653e\u7f6e\u3001\u5185\u5b58\u7ba1\u7406\u7b49\u65b9\u9762\u63d0\u4f9b\u6700\u4f18\u89e3\uff0c\u76f8\u6bd4\u73b0\u6709\u8bbe\u8ba1\u53ef\u5b9e\u73b01.32-6.03\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u968f\u7740LLM\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5bf9\u9ad8\u6027\u80fd\u63a8\u7406\u670d\u52a1\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u73b0\u6709AI\u52a0\u901f\u5668\u591a\u91c7\u7528\u591a\u6838\u67b6\u6784\u4f46\u7f3a\u4e4fSIMT\u67b6\u6784\u7684\u7075\u6d3b\u6027\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u4e0d\u8db3\u548c\u63a8\u7406\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u591a\u7ea7\u4eff\u771f\u6846\u67b6\uff08\u4e8b\u52a1\u7ea7\u548c\u6027\u80fd\u6a21\u578b\u4eff\u771f\uff09\uff0c\u7cfb\u7edf\u5206\u6790\u5e76\u4f18\u5316\u5f20\u91cf\u5e76\u884c\u7b56\u7565\u3001\u6838\u5fc3\u653e\u7f6e\u7b56\u7565\u3001\u5185\u5b58\u7ba1\u7406\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5728PD\u89e3\u805a\u548cPD\u878d\u5408\u4e4b\u95f4\u7684\u9009\u62e9\u3002", "result": "\u5728\u4e0d\u540c\u786c\u4ef6\u914d\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u591a\u6838NPU\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e861.32\u500d\u52306.03\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u591a\u6838NPU\u5728\u5404\u79cdLLM\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u8bbe\u8ba1\u6700\u4f18\u786c\u4ef6\u67b6\u6784\u548c\u670d\u52a1\u7b56\u7565\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.05787", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2510.05787", "abs": "https://arxiv.org/abs/2510.05787", "authors": ["Panagiota Nikolaou", "Freddy Gabbay", "Jawad Haj-Yahya", "Yiannakis Sazeides"], "title": "An opportunity to improve Data Center Efficiency: Optimizing the Server's Upgrade Cycle", "comment": "This work was has been submitted and presented at the 1st\n  International Workshop on Data Center Energy Efficiency (DCEE-2025) at\n  ISCA-2025, June 21, 2025, Tokyo, Japan", "summary": "This work aims to improve a data center's efficiency by optimizing the server\nupgrade plan: determine the optimal timing for replacing old servers with new\nones. The opportunity presented by this approach is demonstrated through a\nstudy based on historical server data. The study establishes a significant\nopportunity to increase the QPS/(TCOxCO2) metric by formulating a global\nupgrade plan at the data center's design time covering its entire life cycle.\nThis plan leverages information, such as server entry year, performance, and\nactive power consumption for both existing and future servers. Our findings\nreveal that an optimal global upgrade plan, may involve upgrades at non fixed\ntime periods and outperforms local upgrade plans. Local upgrade plans follow a\nfixed, equal-length cycle and make decisions based only on currently available\nserver models. These local plans select the best available server at each\nupgrade cycle without accounting for future server releases.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u670d\u52a1\u5668\u5347\u7ea7\u8ba1\u5212\u6765\u63d0\u9ad8\u6570\u636e\u4e2d\u5fc3\u6548\u7387\uff0c\u786e\u5b9a\u66ff\u6362\u65e7\u670d\u52a1\u5668\u7684\u6700\u4f73\u65f6\u673a\u3002\u7814\u7a76\u53d1\u73b0\u5168\u5c40\u5347\u7ea7\u8ba1\u5212\u4f18\u4e8e\u5c40\u90e8\u5347\u7ea7\u8ba1\u5212\uff0c\u80fd\u663e\u8457\u63d0\u5347QPS/(TCOxCO2)\u6307\u6807\u3002", "motivation": "\u63d0\u9ad8\u6570\u636e\u4e2d\u5fc3\u6548\u7387\uff0c\u901a\u8fc7\u4f18\u5316\u670d\u52a1\u5668\u5347\u7ea7\u65f6\u673a\u6765\u6700\u5927\u5316\u6027\u80fd\u4e0e\u6210\u672c\u6548\u76ca\u6bd4\u3002", "method": "\u57fa\u4e8e\u5386\u53f2\u670d\u52a1\u5668\u6570\u636e\uff0c\u5236\u5b9a\u8986\u76d6\u6570\u636e\u4e2d\u5fc3\u6574\u4e2a\u751f\u547d\u5468\u671f\u7684\u5168\u5c40\u5347\u7ea7\u8ba1\u5212\uff0c\u8003\u8651\u670d\u52a1\u5668\u8fdb\u5165\u5e74\u4efd\u3001\u6027\u80fd\u548c\u529f\u8017\u7b49\u4fe1\u606f\u3002", "result": "\u5168\u5c40\u5347\u7ea7\u8ba1\u5212\u53ef\u5728\u975e\u56fa\u5b9a\u65f6\u95f4\u5468\u671f\u8fdb\u884c\u5347\u7ea7\uff0c\u6bd4\u57fa\u4e8e\u56fa\u5b9a\u5468\u671f\u548c\u5f53\u524d\u53ef\u7528\u670d\u52a1\u5668\u6a21\u578b\u7684\u5c40\u90e8\u8ba1\u5212\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u5728\u6570\u636e\u4e2d\u5fc3\u8bbe\u8ba1\u9636\u6bb5\u5236\u5b9a\u5168\u5c40\u5347\u7ea7\u8ba1\u5212\uff0c\u8003\u8651\u672a\u6765\u670d\u52a1\u5668\u53d1\u5e03\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6548\u7387\u6307\u6807\u3002"}}
{"id": "2510.05109", "categories": ["cs.DC", "cs.AI", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.05109", "abs": "https://arxiv.org/abs/2510.05109", "authors": ["Yilong Li", "Shuai Zhang", "Yijing Zeng", "Hao Zhang", "Xinmiao Xiong", "Jingyu Liu", "Pan Hu", "Suman Banerjee"], "title": "Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices", "comment": null, "summary": "Large Multimodal Models (LMMs) are inherently modular, consisting of vision\nand audio encoders, projectors, and large language models. Yet, they are almost\nalways executed monolithically, which underutilizes the heterogeneous\naccelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end\nlatency. In this paper, we present NANOMIND, a hardware--software co-design\ninference framework for Large Multimodal Models (LMMs) that breaks large models\ninto modular ``bricks'' (vision, language, audio, etc.) and maps each to its\nideal accelerator. The key insight is that large models can be broken into\nmodular components and scheduled to run on the most appropriate compute units.\nIt performs module-level dynamic offloading across accelerators on\nunified-memory SoCs. By combining customized hardware design, system-level\nscheduling, and optimized low-bit computation kernels, we demonstrate our\nframework with a compact, battery-powered device capable of running LMMs\nentirely on device. This prototype functions as a self-contained intelligent\nassistant that requires no network connectivity, while achieving higher\nthroughput and superior power efficiency under strict resource constraints. The\ndesign further bypasses CPU bottlenecks and reduces redundant memory usage\nthrough token-aware buffer management and module-level coordination. Our system\noutperforms existing implementations in resource efficiency, cutting energy\nconsumption by 42.3\\% and GPU memory usage by 11.2\\%. This enables a\nbattery-powered device to run LLaVA-OneVision with a camera for nearly half a\nday and LLaMA-3-8B for voice interactions up to almost 20.8 hours.", "AI": {"tldr": "NANOMIND\u662f\u4e00\u4e2a\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u5927\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5927\u6a21\u578b\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u7ec4\u4ef6\u5e76\u5728\u5f02\u6784\u52a0\u901f\u5668\u4e0a\u52a8\u6001\u8c03\u5ea6\uff0c\u5b9e\u73b0\u66f4\u9ad8\u7684\u80fd\u6548\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u901a\u5e38\u4ee5\u6574\u4f53\u65b9\u5f0f\u8fd0\u884c\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u73b0\u4ee3SoC\u4e2d\u7684\u5f02\u6784\u52a0\u901f\u5668\uff08NPU\u3001GPU\u3001DSP\u7b49\uff09\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u4f4e\u80fd\u6548\u3002", "method": "\u5c06\u5927\u6a21\u578b\u5206\u89e3\u4e3a\u89c6\u89c9\u3001\u8bed\u8a00\u3001\u97f3\u9891\u7b49\u6a21\u5757\u5316\"\u7816\u5757\"\uff0c\u901a\u8fc7\u6a21\u5757\u7ea7\u52a8\u6001\u5378\u8f7d\u6280\u672f\u5c06\u6bcf\u4e2a\u6a21\u5757\u6620\u5c04\u5230\u6700\u5408\u9002\u7684\u52a0\u901f\u5668\u4e0a\u8fd0\u884c\uff0c\u7ed3\u5408\u5b9a\u5236\u786c\u4ef6\u8bbe\u8ba1\u3001\u7cfb\u7edf\u7ea7\u8c03\u5ea6\u548c\u4f18\u5316\u7684\u4f4e\u4f4d\u8ba1\u7b97\u5185\u6838\u3002", "result": "\u7cfb\u7edf\u5728\u8d44\u6e90\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u5b9e\u73b0\uff0c\u80fd\u8017\u964d\u4f4e42.3%\uff0cGPU\u5185\u5b58\u4f7f\u7528\u51cf\u5c1111.2%\uff0c\u7535\u6c60\u4f9b\u7535\u8bbe\u5907\u53ef\u8fd0\u884cLLaVA-OneVision\u8fd1\u534a\u5929\uff0cLLaMA-3-8B\u8bed\u97f3\u4ea4\u4e92\u8fbe20.8\u5c0f\u65f6\u3002", "conclusion": "NANOMIND\u6846\u67b6\u901a\u8fc7\u6a21\u5757\u5316\u5206\u89e3\u548c\u667a\u80fd\u8c03\u5ea6\uff0c\u5b9e\u73b0\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fd0\u884c\u5927\u6a21\u578b\uff0c\u4e3a\u8fb9\u7f18\u667a\u80fd\u8bbe\u5907\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05476", "categories": ["cs.DC", "cs.AR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.05476", "abs": "https://arxiv.org/abs/2510.05476", "authors": ["Xi Wang", "Bin Ma", "Jongryool Kim", "Byungil Koh", "Hoshik Kim", "Dong Li"], "title": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications", "comment": null, "summary": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages.", "AI": {"tldr": "cMPI\u662f\u9996\u4e2a\u5728\u771f\u5b9eCXL\u5e73\u53f0\u4e0a\u5229\u7528CXL\u5185\u5b58\u5171\u4eab\u4f18\u5316MPI\u70b9\u5bf9\u70b9\u901a\u4fe1\u7684\u5de5\u4f5c\uff0c\u5c06\u8de8\u8282\u70b9\u901a\u4fe1\u8f6c\u6362\u4e3aCXL\u5185\u5b58\u5185\u7684\u5185\u5b58\u4e8b\u52a1\u548c\u6570\u636e\u62f7\u8d1d\uff0c\u7ed5\u8fc7\u4f20\u7edf\u7f51\u7edc\u534f\u8bae\u3002", "motivation": "\u4f20\u7edfMPI\u5e93\u4f7f\u7528\u590d\u6742\u8f6f\u4ef6\u6808\u7684\u7f51\u7edc\u4e92\u8fde\u548c\u534f\u8bae\u8fdb\u884c\u8de8\u8282\u70b9\u901a\u4fe1\uff0c\u5b58\u5728\u6027\u80fd\u74f6\u9888\u3002CXL\u5185\u5b58\u5171\u4eab\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u7684\u901a\u4fe1\u4f18\u5316\u673a\u4f1a\u3002", "method": "\u901a\u8fc7CXL\u5185\u5b58\u5171\u4eab\u6280\u672f\uff0c\u5c06MPI\u70b9\u5bf9\u70b9\u901a\u4fe1\uff08\u5305\u62ec\u5355\u8fb9\u548c\u53cc\u8fb9\u901a\u4fe1\uff09\u8f6c\u6362\u4e3aCXL\u5185\u5b58\u5185\u7684\u5185\u5b58\u4e8b\u52a1\u548c\u6570\u636e\u62f7\u8d1d\uff0c\u89e3\u51b3\u4e86CXL\u5185\u5b58\u5171\u4eab\u5728MPI\u901a\u4fe1\u4e2d\u7684\u6570\u636e\u5bf9\u8c61\u7ba1\u7406\u3001\u7f13\u5b58\u4e00\u81f4\u6027\u548c\u539f\u5b50\u64cd\u4f5c\u7b49\u6311\u6218\u3002", "result": "CXL\u5185\u5b58\u5171\u4eab\u76f8\u6bd4TCP\u4e92\u8fde\u5b9e\u73b07.2x-8.1x\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u5728\u5c0f\u578b\u6d88\u606f\u901a\u4fe1\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6\u4ee5\u592a\u7f51NIC\u548c\u9ad8\u6027\u80fdSmartNIC\u5206\u522b\u5b9e\u73b049x\u548c72x\u7684\u5ef6\u8fdf\u548c\u5e26\u5bbd\u63d0\u5347\u3002", "conclusion": "CXL\u5185\u5b58\u5171\u4eab\u6280\u672f\u80fd\u591f\u663e\u8457\u4f18\u5316MPI\u901a\u4fe1\u6027\u80fd\uff0c\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u63d0\u4f9b\u65b0\u7684\u901a\u4fe1\u4f18\u5316\u9014\u5f84\u3002"}}
{"id": "2510.05111", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05111", "abs": "https://arxiv.org/abs/2510.05111", "authors": ["Ian McDougall", "Noah Scott", "Joon Huh", "Kirthevasan Kandasamy", "Karthikeyan Sankaralingam"], "title": "Agora: Bridging the GPU Cloud Resource-Price Disconnect", "comment": "15 pages, 6 figures", "summary": "The historic trend of Moore's Law, which predicted exponential growth in\ncomputational performance per dollar, has diverged for modern Graphics\nProcessing Units (GPUs). While Floating Point Operations per Second (FLOPs)\ncapabilities have continued to scale economically, memory bandwidth has not,\ncreating a significant price-performance disconnect. This paper argues that the\nprevailing time-based pricing models for cloud GPUs are economically\ninefficient for bandwidth-bound workloads. These models fail to account for the\nrising marginal cost of memory bandwidth, leading to market distortions and\nsuboptimal hardware allocation. To address this, we propose a novel\nfeature-based pricing framework that directly links cost to resource\nconsumption, including but not limited to memory bandwidth. We provide a robust\neconomic and algorithmic definition of this framework and introduce Agora, a\npractical and secure system architecture for its implementation. Our\nimplementation of Agora shows that a 50us sampling provides nearly perfect\npricing as what ideal sampling would provide - losing only 5\\% of revenue. 10us\nsampling is even better result in 2.4\\% loss. Modern telemetry systems can\nalready provide this rate of measurement, and our prototype implementation\nshows the system design for feature-based pricing is buildable. Our evaluation\nacross diverse GPU applications and hardware generations empirically validates\nthe effectiveness of our approach in creating a more transparent and efficient\nmarket for cloud GPU resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7684GPU\u4e91\u670d\u52a1\u5b9a\u4ef7\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3FLOPs\u4e0e\u5185\u5b58\u5e26\u5bbd\u4e4b\u95f4\u7684\u4ef7\u683c\u6027\u80fd\u8131\u8282\u95ee\u9898\uff0c\u5e76\u901a\u8fc7Agora\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u5b9a\u4ef7\u3002", "motivation": "\u73b0\u4ee3GPU\u7684\u6d6e\u70b9\u8fd0\u7b97\u80fd\u529b\u6301\u7eed\u63d0\u5347\u800c\u5185\u5b58\u5e26\u5bbd\u589e\u957f\u6ede\u540e\uff0c\u5bfc\u81f4\u57fa\u4e8e\u65f6\u95f4\u7684\u4f20\u7edf\u5b9a\u4ef7\u6a21\u578b\u5bf9\u5e26\u5bbd\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u7ecf\u6d4e\uff0c\u9020\u6210\u5e02\u573a\u626d\u66f2\u548c\u786c\u4ef6\u5206\u914d\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7279\u5f81\u7684\u5b9a\u4ef7\u6846\u67b6\uff0c\u5c06\u6210\u672c\u4e0e\u8d44\u6e90\u6d88\u8017\u76f4\u63a5\u5173\u8054\uff0c\u5305\u62ec\u5185\u5b58\u5e26\u5bbd\u7b49\u5173\u952e\u6307\u6807\uff0c\u5e76\u8bbe\u8ba1Agora\u7cfb\u7edf\u67b6\u6784\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u8d44\u6e90\u6d4b\u91cf\u548c\u5b9a\u4ef7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e50\u5fae\u79d2\u91c7\u6837\u53ef\u5b9e\u73b0\u8fd1\u4e4e\u7406\u60f3\u7684\u5b9a\u4ef7\u6548\u679c\uff0c\u4ec5\u635f\u59315%\u6536\u5165\uff1b10\u5fae\u79d2\u91c7\u6837\u6548\u679c\u66f4\u597d\uff0c\u4ec5\u635f\u59312.4%\u6536\u5165\u3002\u73b0\u4ee3\u9065\u6d4b\u7cfb\u7edf\u5df2\u80fd\u652f\u6301\u8fd9\u79cd\u6d4b\u91cf\u9891\u7387\u3002", "conclusion": "\u57fa\u4e8e\u7279\u5f81\u7684\u5b9a\u4ef7\u6846\u67b6\u80fd\u591f\u4e3a\u4e91GPU\u8d44\u6e90\u521b\u5efa\u66f4\u900f\u660e\u9ad8\u6548\u7684\u5e02\u573a\uff0c\u901a\u8fc7Agora\u7cfb\u7edf\u7684\u5b9e\u9645\u9a8c\u8bc1\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2510.05497", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05497", "abs": "https://arxiv.org/abs/2510.05497", "authors": ["Zhongkai Yu", "Yue Guan", "Zihao Yu", "Chenyang Zhou", "Shuyi Pei", "Yangwook Kang", "Yufei Ding", "Po-An Tsai"], "title": "Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting", "comment": null, "summary": "Large Language Models (LLMs) with Mixture of Experts (MoE) architectures\nachieve remarkable performance improvements, but their random expert selection\nmechanism introduces significant data movement overhead that becomes the\ndominant bottleneck in multi-unit serving systems. To forecast the patterns\nunderlying this data movement, we conduct comprehensive data-movement-centric\nprofiling across three state-of-the-art large-scale MoE models (200B- 671B)\nusing over 24,000 requests spanning diverse workloads. With the resulting\n150GB+ trace files, we perform systematic analysis from both temporal and\nspatial perspectives and distill six key insights to guide the design of\ndiverse future serving systems. Taking wafer-scale GPUs as a case study, we\ndemonstrate that minor architectural modifications leveraging our insights\nachieve substantial performance gains, delivering 6.3X and 4.0X average\nspeedups on DeepSeek V3 and Qwen3, respectively. Our work provides the first\ncomprehensive data-centric analysis of MoE models at scale. Our profiling\ntraces and analysis results are publicly available at\n{https://huggingface.co/datasets/core12345/MoE_expert_selection_trace. We will\nalso release our simulation framework shortly to facilitate future research in\nthis area.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9MoE\u67b6\u6784LLM\u7684\u6570\u636e\u79fb\u52a8\u74f6\u9888\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6d4b\u8bd5\u53d1\u73b0\u4e866\u4e2a\u5173\u952e\u6d1e\u5bdf\uff0c\u5e76\u5728\u6676\u5706\u7ea7GPU\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "MoE\u67b6\u6784LLM\u7684\u968f\u673a\u4e13\u5bb6\u9009\u62e9\u673a\u5236\u5728\u591a\u5355\u5143\u670d\u52a1\u7cfb\u7edf\u4e2d\u5f15\u5165\u4e86\u663e\u8457\u7684\u6570\u636e\u79fb\u52a8\u5f00\u9500\uff0c\u6210\u4e3a\u4e3b\u8981\u6027\u80fd\u74f6\u9888\u3002", "method": "\u5bf9\u4e09\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u89c4\u6a21MoE\u6a21\u578b\uff08200B-671B\uff09\u8fdb\u884c\u6570\u636e\u79fb\u52a8\u4e2d\u5fc3\u5316\u5206\u6790\uff0c\u4f7f\u7528\u8d85\u8fc724,000\u4e2a\u8bf7\u6c42\uff0c\u751f\u6210150GB+\u7684\u8ddf\u8e2a\u6587\u4ef6\uff0c\u4ece\u65f6\u95f4\u548c\u7a7a\u95f4\u89d2\u5ea6\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002", "result": "\u63d0\u70bc\u51fa6\u4e2a\u5173\u952e\u8bbe\u8ba1\u6d1e\u5bdf\uff0c\u5728\u6676\u5706\u7ea7GPU\u4e0a\u901a\u8fc7\u67b6\u6784\u4fee\u6539\u5b9e\u73b0\u4e86DeepSeek V3 6.3\u500d\u548cQwen3 4.0\u500d\u7684\u5e73\u5747\u52a0\u901f\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5927\u89c4\u6a21MoE\u6a21\u578b\u7684\u5168\u9762\u6570\u636e\u4e2d\u5fc3\u5316\u5206\u6790\uff0c\u4e3a\u672a\u6765\u670d\u52a1\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u76f8\u5173\u6570\u636e\u548c\u6846\u67b6\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2510.05112", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05112", "abs": "https://arxiv.org/abs/2510.05112", "authors": ["Lijuan Jiang", "Xingjian Qian", "Zhenxiang Ma", "Zan Zong", "Hengjie Li", "Chao Yang", "Jidong Zhai"], "title": "A Flexible Programmable Pipeline Parallelism Framework for Efficient DNN Training", "comment": null, "summary": "Pipeline parallelism is an essential distributed parallelism method.\nIncreasingly complex and diverse DNN models necessitate meticulously customized\npipeline schedules for performance. However, existing practices typically rely\non predefined schedules, each with strengths, but fail to adapt automatically\nto the emerging model architectures. Exploring novel high-efficiency schedules\nis daunting due to the enormous and varying schedule space. Besides, manually\nimplementing schedules can be challenging due to the onerous coding burdens and\nconstantly changing needs. Unfortunately, existing frameworks have limitations\nin automated schedule exploration and lack flexibility and controllability.\n  This paper presents FlexPipe, a programmable pipeline parallelism framework\nwith enhanced productivity, programmability, debuggability, and ease of tuning.\nFlexPipe has two main components: a succinct domain-specific language (DSL) and\nan automated scheduler. FlexPipe enables automated schedule exploration for\nvarious parallel scenarios within a broad spectrum of schedule types at a small\nsearch cost. Besides, users can swiftly develop and customize schedules using\nthe FlexPipe DSL, which embodies flexible controllability in the pipeline order\nof micro-batch computations over stages. It also provides convenient mechanisms\nto include new operations in schedules to meet changing demands. Our evaluation\nresults demonstrate that FlexPipe achieves up to 2.28X performance speedup\ncompared to the popular large-scale parallel framework Megtron-LM, and gains up\nto 1.49X performance speedup compared to the state-of-the-art automated\npipeline parallelism framework.", "AI": {"tldr": "FlexPipe\u662f\u4e00\u4e2a\u53ef\u7f16\u7a0b\u7684\u6d41\u6c34\u7ebf\u5e76\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u548c\u81ea\u52a8\u8c03\u5ea6\u5668\u5b9e\u73b0\u9ad8\u6548\u7684\u6d41\u6c34\u7ebf\u8c03\u5ea6\u63a2\u7d22\u548c\u5b9a\u5236\uff0c\u76f8\u6bd4\u73b0\u6709\u6846\u67b6\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u6d41\u6c34\u7ebf\u5e76\u884c\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u8c03\u5ea6\u7b56\u7565\uff0c\u65e0\u6cd5\u81ea\u52a8\u9002\u5e94\u65b0\u5174\u6a21\u578b\u67b6\u6784\uff0c\u4e14\u624b\u52a8\u5b9e\u73b0\u8c03\u5ea6\u7b56\u7565\u7f16\u7801\u8d1f\u62c5\u91cd\u3001\u7075\u6d3b\u6027\u5dee\u3002", "method": "\u63d0\u51faFlexPipe\u6846\u67b6\uff0c\u5305\u542b\u7b80\u6d01\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00(DSL)\u548c\u81ea\u52a8\u8c03\u5ea6\u5668\uff0c\u652f\u6301\u5e7f\u6cdb\u7684\u8c03\u5ea6\u7c7b\u578b\u63a2\u7d22\u548c\u7075\u6d3b\u5b9a\u5236\u3002", "result": "\u76f8\u6bd4\u4e3b\u6d41\u5927\u89c4\u6a21\u5e76\u884c\u6846\u67b6Megtron-LM\u6027\u80fd\u63d0\u5347\u8fbe2.28\u500d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u6d41\u6c34\u7ebf\u5e76\u884c\u6846\u67b6\u6027\u80fd\u63d0\u5347\u8fbe1.49\u500d\u3002", "conclusion": "FlexPipe\u901a\u8fc7\u589e\u5f3a\u7684\u751f\u4ea7\u529b\u3001\u53ef\u7f16\u7a0b\u6027\u3001\u53ef\u8c03\u8bd5\u6027\u548c\u6613\u8c03\u4f18\u6027\uff0c\u4e3a\u6d41\u6c34\u7ebf\u5e76\u884c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05118", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05118", "abs": "https://arxiv.org/abs/2510.05118", "authors": ["Cynthia Marcelino", "Noah Krennmair", "Thomas Pusztai", "Stefan Nastic"], "title": "Lumos: Performance Characterization of WebAssembly as a Serverless Runtime in the Edge-Cloud Continuum", "comment": null, "summary": "WebAssembly has emerged as a lightweight and portable runtime to execute\nserverless functions, particularly in heterogeneous and resource-constrained\nenvironments such as the Edge Cloud Continuum. However, the performance\nbenefits versus trade-offs remain insufficiently understood. This paper\npresents Lumos, a performance model and benchmarking tool for characterizing\nserverless runtimes. Lumos identifies workload, system, and environment-level\nperformance drivers in the Edge-Cloud Continuum. We benchmark state-of-the-art\ncontainers and the Wasm runtime in interpreted mode and with ahead-of-time\ncompilation. Our performance characterization shows that AoT-compiled Wasm\nimages are up to 30x smaller and decrease cold-start latency by up to 16%\ncompared to containers, while interpreted Wasm suffers up to 55x higher warm\nlatency and up to 10x I/O-serialization overhead.", "AI": {"tldr": "Lumos\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u670d\u52a1\u5668\u65e0\u8fd0\u884c\u65f6\u6027\u80fd\u7684\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u7279\u522b\u5173\u6ce8WebAssembly\u5728\u8fb9\u7f18\u4e91\u8fde\u7eed\u4f53\u4e2d\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0AoT\u7f16\u8bd1\u7684Wasm\u955c\u50cf\u6bd4\u5bb9\u5668\u5c0f30\u500d\uff0c\u51b7\u542f\u52a8\u5ef6\u8fdf\u964d\u4f4e16%\uff0c\u4f46\u89e3\u91ca\u578bWasm\u7684\u5ef6\u8fdf\u9ad855\u500d\u4e14I/O\u5e8f\u5217\u5316\u5f00\u9500\u592710\u500d\u3002", "motivation": "WebAssembly\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u53ef\u79fb\u690d\u8fd0\u884c\u65f6\u5728\u8fb9\u7f18\u4e91\u8fde\u7eed\u4f53\u7b49\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u6267\u884c\u65e0\u670d\u52a1\u5668\u51fd\u6570\uff0c\u4f46\u5176\u6027\u80fd\u4f18\u52bf\u4e0e\u6743\u8861\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002", "method": "\u5f00\u53d1Lumos\u6027\u80fd\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u8bc6\u522b\u8fb9\u7f18\u4e91\u8fde\u7eed\u4f53\u4e2d\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3001\u7cfb\u7edf\u548c\u73af\u5883\u7ea7\u6027\u80fd\u9a71\u52a8\u56e0\u7d20\uff0c\u5bf9\u6700\u5148\u8fdb\u7684\u5bb9\u5668\u548cWasm\u8fd0\u884c\u65f6\uff08\u89e3\u91ca\u6a21\u5f0f\u548cAoT\u7f16\u8bd1\u6a21\u5f0f\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "AoT\u7f16\u8bd1\u7684Wasm\u955c\u50cf\u6bd4\u5bb9\u5668\u5c0f30\u500d\uff0c\u51b7\u542f\u52a8\u5ef6\u8fdf\u964d\u4f4e16%\uff1b\u89e3\u91ca\u578bWasm\u7684\u5ef6\u8fdf\u9ad855\u500d\uff0cI/O\u5e8f\u5217\u5316\u5f00\u9500\u592710\u500d\u3002", "conclusion": "AoT\u7f16\u8bd1\u7684Wasm\u5728\u8fb9\u7f18\u4e91\u8fde\u7eed\u4f53\u4e2d\u5177\u6709\u663e\u8457\u7684\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u4f18\u52bf\uff0c\u4f46\u89e3\u91ca\u578bWasm\u5b58\u5728\u4e25\u91cd\u7684\u6027\u80fd\u95ee\u9898\u3002"}}
{"id": "2510.05127", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05127", "abs": "https://arxiv.org/abs/2510.05127", "authors": ["Harshit Goyal"], "title": "Artificial Intelligence for Cost-Aware Resource Prediction in Big Data Pipelines", "comment": "14 pages, 3 figures", "summary": "Efficient resource allocation is a key challenge in modern cloud computing.\nOver-provisioning leads to unnecessary costs, while under-provisioning risks\nperformance degradation and SLA violations. This work presents an artificial\nintelligence approach to predict resource utilization in big data pipelines\nusing Random Forest regression. We preprocess the Google Borg cluster traces to\nclean, transform, and extract relevant features (CPU, memory, usage\ndistributions). The model achieves high predictive accuracy (R Square = 0.99,\nMAE = 0.0048, RMSE = 0.137), capturing non-linear relationships between\nworkload characteristics and resource utilization. Error analysis reveals\nimpressive performance on small-to-medium jobs, with higher variance in rare\nlarge-scale jobs. These results demonstrate the potential of AI-driven\nprediction for cost-aware autoscaling in cloud environments, reducing\nunnecessary provisioning while safeguarding service quality.", "AI": {"tldr": "\u4f7f\u7528\u968f\u673a\u68ee\u6797\u56de\u5f52\u9884\u6d4b\u5927\u6570\u636e\u7ba1\u9053\u8d44\u6e90\u5229\u7528\u7387\u7684AI\u65b9\u6cd5\uff0c\u5728Google Borg\u96c6\u7fa4\u6570\u636e\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\uff0c\u4e3a\u4e91\u73af\u5883\u6210\u672c\u611f\u77e5\u81ea\u52a8\u6269\u7f29\u5bb9\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u73b0\u4ee3\u4e91\u8ba1\u7b97\u4e2d\u9ad8\u6548\u8d44\u6e90\u5206\u914d\u662f\u5173\u952e\u6311\u6218\uff0c\u8fc7\u5ea6\u914d\u7f6e\u5bfc\u81f4\u4e0d\u5fc5\u8981\u6210\u672c\uff0c\u914d\u7f6e\u4e0d\u8db3\u5219\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u548cSLA\u8fdd\u89c4\u98ce\u9669\u3002", "method": "\u9884\u5904\u7406Google Borg\u96c6\u7fa4\u8ddf\u8e2a\u6570\u636e\uff0c\u6e05\u7406\u3001\u8f6c\u6362\u5e76\u63d0\u53d6\u76f8\u5173\u7279\u5f81\uff08CPU\u3001\u5185\u5b58\u3001\u4f7f\u7528\u5206\u5e03\uff09\uff0c\u4f7f\u7528\u968f\u673a\u68ee\u6797\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u8d44\u6e90\u5229\u7528\u7387\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff08R\u5e73\u65b9=0.99\uff0cMAE=0.0048\uff0cRMSE=0.137\uff09\uff0c\u6355\u6349\u4e86\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\u4e0e\u8d44\u6e90\u5229\u7528\u7387\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u5173\u7cfb\u3002\u5bf9\u5c0f\u5230\u4e2d\u7b49\u89c4\u6a21\u4f5c\u4e1a\u8868\u73b0\u4f18\u5f02\uff0c\u5927\u89c4\u6a21\u4f5c\u4e1a\u65b9\u5dee\u8f83\u9ad8\u3002", "conclusion": "AI\u9a71\u52a8\u9884\u6d4b\u5728\u4e91\u73af\u5883\u4e2d\u5177\u6709\u6210\u672c\u611f\u77e5\u81ea\u52a8\u6269\u7f29\u5bb9\u7684\u6f5c\u529b\uff0c\u53ef\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8d44\u6e90\u914d\u7f6e\u540c\u65f6\u4fdd\u969c\u670d\u52a1\u8d28\u91cf\u3002"}}
{"id": "2510.05145", "categories": ["cs.DC", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.05145", "abs": "https://arxiv.org/abs/2510.05145", "authors": ["Lunyiu Nie", "Nedim Lipka", "Ryan A. Rossi", "Swarat Chaudhuri"], "title": "FlashResearch: Real-time Agent Orchestration for Efficient Deep Research", "comment": null, "summary": "Deep research agents, which synthesize information across diverse sources,\nare significantly constrained by their sequential reasoning processes. This\narchitectural bottleneck results in high latency, poor runtime adaptability,\nand inefficient resource allocation, making them impractical for interactive\napplications. To overcome this, we introduce FlashResearch, a novel framework\nfor efficient deep research that transforms sequential processing into\nparallel, runtime orchestration by dynamically decomposing complex queries into\ntree-structured sub-tasks. Our core contributions are threefold: (1) an\nadaptive planner that dynamically allocates computational resources by\ndetermining research breadth and depth based on query complexity; (2) a\nreal-time orchestration layer that monitors research progress and prunes\nredundant paths to reallocate resources and optimize efficiency; and (3) a\nmulti-dimensional parallelization framework that enables concurrency across\nboth research breadth and depth. Experiments show that FlashResearch\nconsistently improves final report quality within fixed time budgets, and can\ndeliver up to a 5x speedup while maintaining comparable quality.", "AI": {"tldr": "FlashResearch\u662f\u4e00\u4e2a\u9ad8\u6548\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u987a\u5e8f\u63a8\u7406\u8f6c\u6362\u4e3a\u5e76\u884c\u8fd0\u884c\u65f6\u7f16\u6392\uff0c\u52a8\u6001\u5206\u89e3\u590d\u6742\u67e5\u8be2\u4e3a\u6811\u72b6\u5b50\u4efb\u52a1\uff0c\u5b9e\u73b05\u500d\u52a0\u901f\u540c\u65f6\u4fdd\u6301\u8d28\u91cf\u3002", "motivation": "\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u987a\u5e8f\u63a8\u7406\u8fc7\u7a0b\u5b58\u5728\u9ad8\u5ef6\u8fdf\u3001\u8fd0\u884c\u65f6\u9002\u5e94\u6027\u5dee\u548c\u8d44\u6e90\u5206\u914d\u4f4e\u6548\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u4ea4\u4e92\u5f0f\u5e94\u7528\u7684\u5b9e\u9645\u4f7f\u7528\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u89c4\u5212\u5668\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u5b9e\u65f6\u7f16\u6392\u5c42\u76d1\u63a7\u7814\u7a76\u8fdb\u5ea6\u5e76\u526a\u679d\u5197\u4f59\u8def\u5f84\uff0c\u591a\u7ef4\u5ea6\u5e76\u884c\u5316\u6846\u67b6\u5728\u7814\u7a76\u5e7f\u5ea6\u548c\u6df1\u5ea6\u4e0a\u5b9e\u73b0\u5e76\u53d1\u3002", "result": "\u5b9e\u9a8c\u663e\u793aFlashResearch\u5728\u56fa\u5b9a\u65f6\u95f4\u9884\u7b97\u5185\u6301\u7eed\u63d0\u5347\u6700\u7ec8\u62a5\u544a\u8d28\u91cf\uff0c\u5728\u4fdd\u6301\u53ef\u6bd4\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe5\u500d\u7684\u52a0\u901f\u3002", "conclusion": "FlashResearch\u901a\u8fc7\u5e76\u884c\u8fd0\u884c\u65f6\u7f16\u6392\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u987a\u5e8f\u5904\u7406\u74f6\u9888\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u7814\u7a76\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05149", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05149", "abs": "https://arxiv.org/abs/2510.05149", "authors": ["Clarisse Sousa", "Tiago Fonseca", "Luis Lino Ferreira", "Ricardo Ven\u00e2ncio", "Ricardo Severino"], "title": "Percepta: High Performance Stream Processing at the Edge", "comment": null, "summary": "The rise of real-time data and the proliferation of Internet of Things (IoT)\ndevices have highlighted the limitations of cloud-centric solutions,\nparticularly regarding latency, bandwidth, and privacy. These challenges have\ndriven the growth of Edge Computing. Associated with IoT appears a set of other\nproblems, like: data rate harmonization between multiple sources, protocol\nconversion, handling the loss of data and the integration with Artificial\nIntelligence (AI) models. This paper presents Percepta, a lightweight Data\nStream Processing (DSP) system tailored to support AI workloads at the edge,\nwith a particular focus on such as Reinforcement Learning (RL). It introduces\nspecialized features such as reward function computation, data storage for\nmodel retraining, and real-time data preparation to support continuous\ndecision-making. Additional functionalities include data normalization,\nharmonization across heterogeneous protocols and sampling rates, and robust\nhandling of missing or incomplete data, making it well suited for the\nchallenges of edge-based AI deployment.", "AI": {"tldr": "Percepta\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6570\u636e\u6d41\u5904\u7406\u7cfb\u7edf\uff0c\u4e13\u95e8\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u7684AI\u5de5\u4f5c\u8d1f\u8f7d\u8bbe\u8ba1\uff0c\u7279\u522b\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u7b49\u5e94\u7528\u3002", "motivation": "\u5b9e\u65f6\u6570\u636e\u548c\u7269\u8054\u7f51\u8bbe\u5907\u7684\u666e\u53ca\u66b4\u9732\u4e86\u4e91\u4e2d\u5fc3\u89e3\u51b3\u65b9\u6848\u5728\u5ef6\u8fdf\u3001\u5e26\u5bbd\u548c\u9690\u79c1\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u4e86\u8fb9\u7f18\u8ba1\u7b97\u7684\u53d1\u5c55\u3002\u7269\u8054\u7f51\u5e26\u6765\u7684\u6570\u636e\u901f\u7387\u534f\u8c03\u3001\u534f\u8bae\u8f6c\u6362\u3001\u6570\u636e\u4e22\u5931\u5904\u7406\u4ee5\u53ca\u4e0eAI\u6a21\u578b\u96c6\u6210\u7b49\u95ee\u9898\u9700\u8981\u89e3\u51b3\u3002", "method": "\u5f00\u53d1\u4e86Percepta\u7cfb\u7edf\uff0c\u5177\u5907\u5956\u52b1\u51fd\u6570\u8ba1\u7b97\u3001\u6a21\u578b\u91cd\u8bad\u7ec3\u6570\u636e\u5b58\u50a8\u3001\u5b9e\u65f6\u6570\u636e\u51c6\u5907\u7b49\u4e13\u95e8\u529f\u80fd\uff0c\u652f\u6301\u6301\u7eed\u51b3\u7b56\u5236\u5b9a\u3002\u8fd8\u5305\u62ec\u6570\u636e\u6807\u51c6\u5316\u3001\u5f02\u6784\u534f\u8bae\u548c\u91c7\u6837\u7387\u534f\u8c03\u3001\u4ee5\u53ca\u7f3a\u5931\u6570\u636e\u9c81\u68d2\u5904\u7406\u7b49\u529f\u80fd\u3002", "result": "Percepta\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5904\u7406\u8fb9\u7f18AI\u90e8\u7f72\u4e2d\u7684\u5404\u79cd\u6311\u6218\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u7b49AI\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e13\u95e8\u652f\u6301\u3002", "conclusion": "Percepta\u662f\u4e00\u4e2a\u9002\u5408\u8fb9\u7f18AI\u90e8\u7f72\u7684\u8f7b\u91cf\u7ea7\u6570\u636e\u6d41\u5904\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u89e3\u51b3\u7269\u8054\u7f51\u73af\u5883\u4e2d\u7684\u6570\u636e\u534f\u8c03\u3001\u534f\u8bae\u8f6c\u6362\u548cAI\u96c6\u6210\u7b49\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2510.05164", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05164", "abs": "https://arxiv.org/abs/2510.05164", "authors": ["Yuanzhe Shen", "Yide Liu", "Zisu Huang", "Ruicheng Yin", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "SATER: A Self-Aware and Token-Efficient Approach to Routing and Cascading", "comment": "Accepted to EMNLP 2025 Main", "summary": "Large language models (LLMs) demonstrate remarkable performance across\ndiverse tasks, yet their effectiveness frequently depends on costly commercial\nAPIs or cloud services. Model selection thus entails a critical trade-off\nbetween performance and cost: high-performing LLMs typically incur substantial\nexpenses, whereas budget-friendly small language models (SLMs) are constrained\nby limited capabilities. Current research primarily proposes two routing\nstrategies: pre-generation routing and cascade routing. Both approaches have\ndistinct characteristics, with cascade routing typically offering superior\ncost-effectiveness and accuracy despite its higher latency. To further address\nthe limitations of both approaches, we introduce SATER, a dual-mode compatible\napproach that fine-tunes models through shortest-response preference\noptimization and a confidence-aware rejection mechanism. SATER significantly\nreduces redundant outputs and response times, while improving both the\nperformance of pre-generation routing and the efficiency of cascade routing.\nExperiments across three SLMs and six datasets, varying in type and complexity,\ndemonstrate that SATER achieves comparable performance while consistently\nreducing computational costs by over 50\\% and cascade latency by over 80\\%.", "AI": {"tldr": "SATER\u662f\u4e00\u4e2a\u53cc\u6a21\u5f0f\u517c\u5bb9\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u77ed\u54cd\u5e94\u504f\u597d\u4f18\u5316\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u62d2\u7edd\u673a\u5236\u6765\u5fae\u8c03\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u5197\u4f59\u8f93\u51fa\u548c\u54cd\u5e94\u65f6\u95f4\uff0c\u540c\u65f6\u63d0\u9ad8\u9884\u751f\u6210\u8def\u7531\u7684\u6027\u80fd\u548c\u7ea7\u8054\u8def\u7531\u7684\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u5f02\u4f46\u4f9d\u8d56\u6602\u8d35\u7684\u5546\u4e1aAPI\u6216\u4e91\u670d\u52a1\uff0c\u800c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u6210\u672c\u8f83\u4f4e\u4f46\u80fd\u529b\u6709\u9650\u3002\u73b0\u6709\u8def\u7531\u7b56\u7565\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u9700\u8981\u89e3\u51b3\u6027\u80fd\u548c\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "SATER\u91c7\u7528\u53cc\u6a21\u5f0f\u517c\u5bb9\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u77ed\u54cd\u5e94\u504f\u597d\u4f18\u5316\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u62d2\u7edd\u673a\u5236\u6765\u5fae\u8c03\u6a21\u578b\uff0c\u51cf\u5c11\u5197\u4f59\u8f93\u51fa\u548c\u54cd\u5e94\u65f6\u95f4\u3002", "result": "\u5728\u4e09\u4e2a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u548c\u516d\u4e2a\u4e0d\u540c\u7c7b\u578b\u548c\u590d\u6742\u5ea6\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSATER\u5728\u4fdd\u6301\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u8d85\u8fc750%\uff0c\u7ea7\u8054\u5ef6\u8fdf\u964d\u4f4e\u8d85\u8fc780%\u3002", "conclusion": "SATER\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u8a00\u6a21\u578b\u9009\u62e9\u4e2d\u6027\u80fd\u4e0e\u6210\u672c\u7684\u6743\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8def\u7531\u7b56\u7565\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2510.05186", "categories": ["cs.DC", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.05186", "abs": "https://arxiv.org/abs/2510.05186", "authors": ["Hongpei Li", "Han Zhang", "Huikang Liu", "Dongdong Ge", "Yinyu Ye"], "title": "OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training", "comment": "Use Mathematical Programming to model Pipeline Parallelism with\n  Offloading to balance efficiency and memory requirement", "summary": "Pipeline parallelism (PP) has become a standard technique for scaling large\nlanguage model (LLM) training across multiple devices. However, despite recent\nprogress in reducing memory consumption through activation offloading, existing\napproaches remain largely heuristic and coarse-grained, often overlooking the\nfine-grained trade-offs between memory, computation, and scheduling latency. In\nthis work, we revisit the pipeline scheduling problem from a principled\noptimization perspective. We observe that prevailing strategies either rely on\nstatic rules or aggressively offload activations without fully leveraging the\ninteraction between memory constraints and scheduling efficiency. To address\nthis, we formulate scheduling as a constrained optimization problem that\njointly accounts for memory capacity, activation reuse, and pipeline bubble\nminimization. Solving this model yields fine-grained schedules that reduce\npipeline bubbles while adhering to strict memory budgets. Our approach\ncomplements existing offloading techniques: whereas prior approaches trade\nmemory for time in a fixed pattern, we dynamically optimize the tradeoff with\nrespect to model structure and hardware configuration. Experimental results\ndemonstrate that our method consistently improves both throughput and memory\nutilization. In particular, we reduce idle pipeline time by up to 50% under the\nsame per-device memory limit, and in some cases, enable the training of larger\nmodels within limited memory budgets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u89c6\u89d2\u7684\u6d41\u6c34\u7ebf\u8c03\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u8003\u8651\u5185\u5b58\u5bb9\u91cf\u3001\u6fc0\u6d3b\u91cd\u7528\u548c\u6d41\u6c34\u7ebf\u6c14\u6ce1\u6700\u5c0f\u5316\uff0c\u52a8\u6001\u4f18\u5316\u5185\u5b58\u4e0e\u65f6\u95f4\u7684\u6743\u8861\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u548c\u5185\u5b58\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709\u6d41\u6c34\u7ebf\u5e76\u884c\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u6fc0\u6d3b\u5378\u8f7d\u51cf\u5c11\u4e86\u5185\u5b58\u6d88\u8017\uff0c\u4f46\u5927\u591a\u662f\u542f\u53d1\u5f0f\u548c\u7c97\u7c92\u5ea6\u7684\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u8c03\u5ea6\u5ef6\u8fdf\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u6743\u8861\u3002", "method": "\u5c06\u6d41\u6c34\u7ebf\u8c03\u5ea6\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u8054\u5408\u8003\u8651\u5185\u5b58\u5bb9\u91cf\u3001\u6fc0\u6d3b\u91cd\u7528\u548c\u6d41\u6c34\u7ebf\u6c14\u6ce1\u6700\u5c0f\u5316\uff0c\u901a\u8fc7\u6c42\u89e3\u8be5\u6a21\u578b\u751f\u6210\u7ec6\u7c92\u5ea6\u8c03\u5ea6\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u76f8\u540c\u8bbe\u5907\u5185\u5b58\u9650\u5236\u4e0b\uff0c\u6d41\u6c34\u7ebf\u7a7a\u95f2\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe50%\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u591f\u5728\u6709\u9650\u5185\u5b58\u9884\u7b97\u5185\u8bad\u7ec3\u66f4\u5927\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ece\u4f18\u5316\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u6d41\u6c34\u7ebf\u8c03\u5ea6\u95ee\u9898\uff0c\u52a8\u6001\u4f18\u5316\u5185\u5b58\u4e0e\u65f6\u95f4\u7684\u6743\u8861\uff0c\u76f8\u6bd4\u56fa\u5b9a\u6a21\u5f0f\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u541e\u5410\u91cf\u548c\u5185\u5b58\u5229\u7528\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.05254", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05254", "abs": "https://arxiv.org/abs/2510.05254", "authors": ["Filipp Sporykhin", "Holger Homann"], "title": "Performance of a high-order MPI-Kokkos accelerated fluid solver", "comment": "12 pages, 16 figures. submitted to Computer Physics Communications", "summary": "This work discusses the performance of a modern numerical scheme for fluid\ndynamical problems on modern high-performance computing architectures. Our code\nimplements a spatial nodal discontinuous Galerkin scheme that we test up to an\norder of convergence of eight. It is temporally coupled to a set of Runge-Kutta\nmethods of orders up to six. The code integrates the linear advection equations\nas well as the isothermal Euler equations in one, two, and three dimensions. In\norder to target modern hardware involving many-core Central Processing Units\nand accelerators such as Graphic Processing Units we use the Kokkos library in\nconjunction with the Message Passing Interface to run our single source code on\nvarious GPU systems. We find that the higher the order the faster is the code.\nEighth-order simulations attain a given global error with much less computing\ntime than third- or fourth-order simulations. The RK scheme has a smaller\nimpact on the code performance and a classical fourth-order scheme seems to\ngenerally be a good choice. The code performs very well on all considered GPUs.\nThe many-CPU performance is also very good and perfect weak scaling is observed\nup to many hundreds of CPU cores using MPI. We note that small grid-size\nsimulations are faster on CPUs than on GPUs while GPUs win significantly over\nCPUs for simulations involving more than $10^7$ degrees of freedom ($\\approx\n3100^2$ grid points). When it comes to the environmental impact of numerical\nsimulations we estimate that GPUs consume less energy than CPUs for large\ngrid-size simulations but more energy on small grids. We observe a tendency\nthat the more modern is the GPU the larger needs to be the grid in order to use\nit efficiently. This yields a rebound effect because larger simulations need\nlonger computing times and in turn more energy that is not compensated by the\nenergy efficiency gain of the newer GPUs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u9ad8\u9636\u95f4\u65adGalerkin\u65b9\u6cd5\u5728\u73b0\u4ee3\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u516b\u9636\u65b9\u6cd5\u6bd4\u4f4e\u9636\u65b9\u6cd5\u66f4\u5feb\uff0cGPU\u5728\u5927\u89c4\u6a21\u8ba1\u7b97\u4e2d\u6027\u80fd\u4f18\u4e8eCPU\u4f46\u80fd\u8017\u8868\u73b0\u590d\u6742\u3002", "motivation": "\u7814\u7a76\u73b0\u4ee3\u6570\u503c\u65b9\u6cd5\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\u4e0a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u9488\u5bf9\u591a\u6838CPU\u548cGPU\u52a0\u901f\u5668\uff0c\u4ee5\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u548c\u80fd\u8017\u3002", "method": "\u4f7f\u7528Kokkos\u5e93\u548cMPI\u5b9e\u73b0\u5355\u6e90\u4ee3\u7801\uff0c\u5728\u591a\u79cdGPU\u7cfb\u7edf\u4e0a\u8fd0\u884c\u7a7a\u95f4\u8282\u70b9\u95f4\u65adGalerkin\u65b9\u6848\uff08\u6700\u9ad8\u516b\u9636\uff09\u548c\u65f6\u95f4Runge-Kutta\u65b9\u6cd5\uff08\u6700\u9ad8\u516d\u9636\uff09\uff0c\u6c42\u89e3\u7ebf\u6027\u5bf9\u6d41\u65b9\u7a0b\u548c\u7b49\u6e29\u6b27\u62c9\u65b9\u7a0b\u3002", "result": "\u9ad8\u9636\u65b9\u6cd5\u8ba1\u7b97\u66f4\u5feb\uff0c\u516b\u9636\u6a21\u62df\u6bd4\u4e09\u9636\u6216\u56db\u9636\u66f4\u5feb\u8fbe\u5230\u76f8\u540c\u5168\u5c40\u8bef\u5dee\uff1bGPU\u5728\u5927\u89c4\u6a21\u8ba1\u7b97\uff08\u8d85\u8fc710^7\u81ea\u7531\u5ea6\uff09\u4e2d\u663e\u8457\u4f18\u4e8eCPU\uff0c\u4f46\u5c0f\u7f51\u683c\u8ba1\u7b97CPU\u66f4\u5feb\uff1b\u65b0GPU\u9700\u8981\u66f4\u5927\u7f51\u683c\u624d\u80fd\u9ad8\u6548\u4f7f\u7528\u3002", "conclusion": "\u9ad8\u9636\u6570\u503c\u65b9\u6cd5\u5728\u73b0\u4ee3\u786c\u4ef6\u4e0a\u5177\u6709\u6027\u80fd\u4f18\u52bf\uff0cGPU\u9002\u5408\u5927\u89c4\u6a21\u8ba1\u7b97\u4f46\u80fd\u8017\u8868\u73b0\u5b58\u5728\u53cd\u5f39\u6548\u5e94\uff0c\u65b0GPU\u9700\u8981\u66f4\u5927\u8ba1\u7b97\u89c4\u6a21\u624d\u80fd\u53d1\u6325\u80fd\u6548\u4f18\u52bf\u3002"}}
{"id": "2510.05556", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2510.05556", "abs": "https://arxiv.org/abs/2510.05556", "authors": ["Jiakai Xu", "Tianle Zhou", "Eugene Wu", "Kostis Kaffes"], "title": "Toward Systems Foundations for Agentic Exploration", "comment": null, "summary": "Agentic exploration, letting LLM-powered agents branch, backtrack, and search\nacross many execution paths, demands systems support well beyond today's\npass-at-k resets. Our benchmark of six snapshot/restore mechanisms shows that\ngeneric tools such as CRIU or container commits are not fast enough even in\nisolated testbeds, and they crumble entirely in real deployments where agents\nshare files, sockets, and cloud APIs with other agents and human users. In this\ntalk, we pinpoint three open fundamental challenges: fork semantics, which\nconcerns how branches reveal or hide tentative updates; external side-effects,\nwhere fork awareness must be added to services or their calls intercepted; and\nnative forking, which requires cloning databases and runtimes in microseconds\nwithout bulk copying.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u73b0\u6709\u5feb\u7167/\u6062\u590d\u673a\u5236\u5728\u652f\u6301LLM\u667a\u80fd\u4f53\u63a2\u7d22\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u6307\u51fa\u4e86\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u5206\u652f\u8bed\u4e49\u3001\u5916\u90e8\u526f\u4f5c\u7528\u548c\u539f\u751f\u5206\u652f\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u901a\u7528\u7684\u5feb\u7167/\u6062\u590d\u5de5\u5177\uff08\u5982CRIU\u6216\u5bb9\u5668\u63d0\u4ea4\uff09\u5728\u652f\u6301LLM\u667a\u80fd\u4f53\u7684\u5206\u652f\u56de\u6eaf\u63a2\u7d22\u65f6\u6027\u80fd\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u90e8\u7f72\u73af\u5883\u4e2d\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5171\u4eab\u8d44\u6e90\u3002", "method": "\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4e86\u516d\u79cd\u5feb\u7167/\u6062\u590d\u673a\u5236\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u9694\u79bb\u6d4b\u8bd5\u73af\u5883\u548c\u771f\u5b9e\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u901a\u7528\u5de5\u5177\u5373\u4f7f\u5728\u9694\u79bb\u73af\u5883\u4e2d\u4e5f\u4e0d\u591f\u5feb\uff0c\u5728\u771f\u5b9e\u90e8\u7f72\u4e2d\uff08\u667a\u80fd\u4f53\u5171\u4eab\u6587\u4ef6\u3001\u5957\u63a5\u5b57\u548c\u4e91API\uff09\u5b8c\u5168\u5931\u6548\u3002", "conclusion": "\u9700\u8981\u89e3\u51b3\u4e09\u4e2a\u6839\u672c\u6027\u6311\u6218\uff1a\u5206\u652f\u8bed\u4e49\u5b9a\u4e49\u3001\u5916\u90e8\u526f\u4f5c\u7528\u5904\u7406\u3001\u4ee5\u53ca\u5fae\u79d2\u7ea7\u539f\u751f\u5206\u652f\u6280\u672f\uff0c\u624d\u80fd\u6709\u6548\u652f\u6301\u667a\u80fd\u4f53\u63a2\u7d22\u3002"}}
{"id": "2510.05621", "categories": ["cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.05621", "abs": "https://arxiv.org/abs/2510.05621", "authors": ["Zhiyuan Ren", "Tao Zhang", "Wenchi Chen"], "title": "Decoupling Correctness from Policy: A Deterministic Causal Structure for Multi-Agent Systems", "comment": null, "summary": "In distributed multi-agent systems, correctness is often entangled with\noperational policies such as scheduling, batching, or routing, which makes\nsystems brittle since performance-driven policy evolution may break integrity\nguarantees. This paper introduces the Deterministic Causal Structure (DCS), a\nformal foundation that decouples correctness from policy. We develop a minimal\naxiomatic theory and prove four results: existence and uniqueness,\npolicy-agnostic invariance, observational equivalence, and axiom minimality.\nThese results show that DCS resolves causal ambiguities that value-centric\nconvergence models such as CRDTs cannot address, and that removing any axiom\ncollapses determinism into ambiguity. DCS thus emerges as a boundary principle\nof asynchronous computation, analogous to CAP and FLP: correctness is preserved\nonly within the expressive power of a join-semilattice. All guarantees are\nestablished by axioms and proofs, with only minimal illustrative constructions\nincluded to aid intuition. This work establishes correctness as a fixed,\npolicy-agnostic substrate, a Correctness-as-a-Chassis paradigm, on which\ndistributed intelligent systems can be built modularly, safely, and evolvably.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u786e\u5b9a\u6027\u56e0\u679c\u7ed3\u6784(DCS)\u4f5c\u4e3a\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5f62\u5f0f\u5316\u57fa\u7840\uff0c\u5c06\u6b63\u786e\u6027\u4e0e\u64cd\u4f5c\u7b56\u7565\u89e3\u8026\uff0c\u786e\u4fdd\u7b56\u7565\u6f14\u5316\u65f6\u4e0d\u4f1a\u7834\u574f\u5b8c\u6574\u6027\u4fdd\u8bc1\u3002", "motivation": "\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u6b63\u786e\u6027\u5e38\u4e0e\u8c03\u5ea6\u3001\u6279\u5904\u7406\u7b49\u64cd\u4f5c\u7b56\u7565\u7ea0\u7f20\uff0c\u5bfc\u81f4\u7cfb\u7edf\u8106\u5f31\uff0c\u6027\u80fd\u9a71\u52a8\u7684\u7b56\u7565\u6f14\u5316\u53ef\u80fd\u7834\u574f\u5b8c\u6574\u6027\u4fdd\u8bc1\u3002", "method": "\u5f00\u53d1\u4e86\u6700\u5c0f\u5316\u516c\u7406\u7406\u8bba\uff0c\u8bc1\u660e\u4e86\u56db\u4e2a\u7ed3\u679c\uff1a\u5b58\u5728\u6027\u548c\u552f\u4e00\u6027\u3001\u7b56\u7565\u65e0\u5173\u4e0d\u53d8\u6027\u3001\u89c2\u6d4b\u7b49\u4ef7\u6027\u548c\u516c\u7406\u6700\u5c0f\u6027\u3002", "result": "DCS\u89e3\u51b3\u4e86CRDT\u7b49\u503c\u4e2d\u5fc3\u6536\u655b\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u7684\u56e0\u679c\u6b67\u4e49\u95ee\u9898\uff0c\u79fb\u9664\u4efb\u4f55\u516c\u7406\u90fd\u4f1a\u4f7f\u786e\u5b9a\u6027\u5d29\u6e83\u4e3a\u6b67\u4e49\u3002", "conclusion": "DCS\u786e\u7acb\u4e86\u6b63\u786e\u6027\u4f5c\u4e3a\u56fa\u5b9a\u3001\u7b56\u7565\u65e0\u5173\u7684\u5e95\u5c42\u57fa\u7840\uff0c\u5f62\u6210\u4e86\"\u6b63\u786e\u6027\u5373\u5e95\u76d8\"\u8303\u5f0f\uff0c\u53ef\u5728\u5176\u4e0a\u6a21\u5757\u5316\u3001\u5b89\u5168\u3001\u53ef\u6f14\u8fdb\u5730\u6784\u5efa\u5206\u5e03\u5f0f\u667a\u80fd\u7cfb\u7edf\u3002"}}
{"id": "2510.05711", "categories": ["cs.DC", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.05711", "abs": "https://arxiv.org/abs/2510.05711", "authors": ["Ailiya Borjigin", "Cong He"], "title": "Intertemporal Pricing of Time-Bound Stablecoins: Measuring and Controlling the Liquidity-of-Time Premium", "comment": "23 pages, 5 figures", "summary": "Time-bound stablecoins are DeFi assets that temporarily tokenize traditional\nsecurities during market off-hours, enabling continuous cross-market liquidity.\nWe introduce the Liquidity-of-Time Premium (TLP): the extra return or cost of\nproviding liquidity when the primary market is closed. We build a no-arbitrage\npricing model that yields a band for fair values over different expiries, and a\ndynamic risk-control mechanism that adjusts loan-to-value (LTV) ratios in real\ntime to keep TLP within a target range. Our analysis blends financial\nengineering (no-arbitrage conditions, option-style pricing) with empirical\nfinance (event studies on cross-listed stocks and futures) to measure TLP under\ntime-zone frictions. We define TLP formally, derive closed-form expressions for\nits term structure under idealized assumptions, and simulate scenarios that\nvary volatility and collateralization. We then propose an LTV policy that\nraises or lowers collateral to expand or curtail time-bound stablecoin supply,\nanalogous to a central bank adjusting rates to defend a peg. We outline\nempirical proxies for TLP, including ADR premiums, overseas index futures\nversus cash index divergence, and pre-market versus official close gaps.\nResults show that TLP grows with closure length and volatility, yet can be\ncontained by adaptive LTV. We provide backtests and figures (term-structure\ncurves, capital-efficiency versus tail-risk trade-offs, time-liquidity\nheatmaps) and discuss protocol design (vault structure, closing-price oracles,\non-chain auction liquidations). The findings position time-bound stablecoins as\na tool to reduce temporal market inefficiencies and inform future research and\ndeployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u65f6\u95f4\u6d41\u52a8\u6027\u6ea2\u4ef7(TLP)\u6982\u5ff5\uff0c\u4e3a\u5728\u4f20\u7edf\u5e02\u573a\u95ed\u5e02\u671f\u95f4\u63d0\u4f9b\u6d41\u52a8\u6027\u7684\u989d\u5916\u56de\u62a5\u6216\u6210\u672c\uff0c\u5e76\u5efa\u7acb\u4e86\u65e0\u5957\u5229\u5b9a\u4ef7\u6a21\u578b\u548c\u52a8\u6001\u98ce\u9669\u63a7\u5236\u673a\u5236\u6765\u7ba1\u7406\u65f6\u95f4\u7ed1\u5b9a\u7a33\u5b9a\u5e01\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bc1\u5238\u5e02\u573a\u5728\u95ed\u5e02\u671f\u95f4\u7f3a\u4e4f\u6d41\u52a8\u6027\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u65f6\u95f4\u7ed1\u5b9a\u7a33\u5b9a\u5e01\u5b9e\u73b0\u8de8\u5e02\u573a\u8fde\u7eed\u6d41\u52a8\u6027\uff0c\u51cf\u5c11\u65f6\u95f4\u533a\u9694\u5e26\u6765\u7684\u5e02\u573a\u4f4e\u6548\u7387\u3002", "method": "\u7ed3\u5408\u91d1\u878d\u5de5\u7a0b\uff08\u65e0\u5957\u5229\u6761\u4ef6\u3001\u671f\u6743\u5f0f\u5b9a\u4ef7\uff09\u548c\u5b9e\u8bc1\u91d1\u878d\uff08\u4e8b\u4ef6\u7814\u7a76\uff09\uff0c\u6784\u5efa\u5b9a\u4ef7\u6a21\u578b\u548c\u52a8\u6001LTV\u8c03\u6574\u673a\u5236\uff0c\u901a\u8fc7\u6a21\u62df\u573a\u666f\u5206\u6790\u6ce2\u52a8\u7387\u548c\u62b5\u62bc\u7387\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660eTLP\u968f\u95ed\u5e02\u65f6\u957f\u548c\u6ce2\u52a8\u7387\u589e\u957f\uff0c\u4f46\u53ef\u901a\u8fc7\u81ea\u9002\u5e94LTV\u63a7\u5236\uff1b\u63d0\u4f9b\u4e86\u56de\u6d4b\u7ed3\u679c\u548c\u53ef\u89c6\u5316\u5206\u6790\uff08\u671f\u9650\u7ed3\u6784\u66f2\u7ebf\u3001\u8d44\u672c\u6548\u7387\u4e0e\u5c3e\u90e8\u98ce\u9669\u6743\u8861\u7b49\uff09\u3002", "conclusion": "\u65f6\u95f4\u7ed1\u5b9a\u7a33\u5b9a\u5e01\u662f\u51cf\u5c11\u65f6\u95f4\u5e02\u573a\u4f4e\u6548\u7684\u6709\u6548\u5de5\u5177\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2510.05738", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05738", "abs": "https://arxiv.org/abs/2510.05738", "authors": ["Ritesh Chandra", "Sonali Agarwal", "Navjot Singh", "Sadhana Tiwari"], "title": "A Review of Ontology-Driven Big Data Analytics in Healthcare: Challenges, Tools, and Applications", "comment": null, "summary": "Exponential growth in heterogeneous healthcare data arising from electronic\nhealth records (EHRs), medical imaging, wearable sensors, and biomedical\nresearch has accelerated the adoption of data lakes and centralized\narchitectures capable of handling the Volume, Variety, and Velocity of Big Data\nfor advanced analytics. However, without effective governance, these\nrepositories risk devolving into disorganized data swamps. Ontology-driven\nsemantic data management offers a robust solution by linking metadata to\nhealthcare knowledge graphs, thereby enhancing semantic interoperability,\nimproving data discoverability, and enabling expressive, domain-aware access.\nThis review adopts a systematic research strategy, formulating key research\nquestions and conducting a structured literature search across major academic\ndatabases, with selected studies analyzed and classified into six categories of\nontology-driven healthcare analytics: (i) ontology-driven integration\nframeworks, (ii) semantic modeling for metadata enrichment, (iii)\nontology-based data access (OBDA), (iv) basic semantic data management, (v)\nontology-based reasoning for decision support, and (vi) semantic annotation for\nunstructured data. We further examine the integration of ontology technologies\nwith Big Data frameworks such as Hadoop, Spark, Kafka, and so on, highlighting\ntheir combined potential to deliver scalable and intelligent healthcare\nanalytics. For each category, recent techniques, representative case studies,\ntechnical and organizational challenges, and emerging trends such as artificial\nintelligence, machine learning, the Internet of Things (IoT), and real-time\nanalytics are reviewed to guide the development of sustainable, interoperable,\nand high-performance healthcare data ecosystems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u672c\u4f53\u9a71\u52a8\u7684\u8bed\u4e49\u6570\u636e\u7ba1\u7406\u5728\u533b\u7597\u5927\u6570\u636e\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u5c06\u76f8\u5173\u7814\u7a76\u5206\u4e3a\u516d\u5927\u7c7b\u522b\uff0c\u5e76\u63a2\u8ba8\u4e86\u672c\u4f53\u6280\u672f\u4e0e\u5927\u6570\u636e\u6846\u67b6\u7684\u6574\u5408\u6f5c\u529b\u3002", "motivation": "\u533b\u7597\u6570\u636e\u7684\u6307\u6570\u7ea7\u589e\u957f\uff08\u6765\u81ea\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u3001\u533b\u5b66\u5f71\u50cf\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\u7b49\uff09\u63a8\u52a8\u4e86\u6570\u636e\u6e56\u548c\u96c6\u4e2d\u5f0f\u67b6\u6784\u7684\u91c7\u7528\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u6cbb\u7406\u4f1a\u5bfc\u81f4\u6570\u636e\u6cbc\u6cfd\u95ee\u9898\u3002\u672c\u4f53\u9a71\u52a8\u7684\u8bed\u4e49\u6570\u636e\u7ba1\u7406\u80fd\u591f\u901a\u8fc7\u5c06\u5143\u6570\u636e\u4e0e\u533b\u7597\u77e5\u8bc6\u56fe\u8c31\u94fe\u63a5\uff0c\u589e\u5f3a\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\u3001\u63d0\u9ad8\u6570\u636e\u53ef\u53d1\u73b0\u6027\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u7814\u7a76\u7b56\u7565\uff0c\u5236\u5b9a\u5173\u952e\u7814\u7a76\u95ee\u9898\uff0c\u5728\u4e3b\u8981\u5b66\u672f\u6570\u636e\u5e93\u4e2d\u8fdb\u884c\u7ed3\u6784\u5316\u6587\u732e\u68c0\u7d22\uff0c\u5c06\u9009\u5b9a\u7814\u7a76\u5206\u6790\u5e76\u5206\u7c7b\u4e3a\u516d\u5927\u7c7b\u522b\uff1a\u672c\u4f53\u9a71\u52a8\u96c6\u6210\u6846\u67b6\u3001\u8bed\u4e49\u5efa\u6a21\u5143\u6570\u636e\u4e30\u5bcc\u3001\u57fa\u4e8e\u672c\u4f53\u7684\u6570\u636e\u8bbf\u95ee\u3001\u57fa\u672c\u8bed\u4e49\u6570\u636e\u7ba1\u7406\u3001\u57fa\u4e8e\u672c\u4f53\u7684\u51b3\u7b56\u652f\u6301\u63a8\u7406\u3001\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u8bed\u4e49\u6807\u6ce8\u3002", "result": "\u8bc6\u522b\u4e86\u672c\u4f53\u6280\u672f\u5728\u5927\u6570\u636e\u6846\u67b6\uff08\u5982Hadoop\u3001Spark\u3001Kafka\u7b49\uff09\u4e2d\u7684\u6574\u5408\u6f5c\u529b\uff0c\u80fd\u591f\u63d0\u4f9b\u53ef\u6269\u5c55\u548c\u667a\u80fd\u7684\u533b\u7597\u5206\u6790\u3002\u5bf9\u6bcf\u4e2a\u7c7b\u522b\u56de\u987e\u4e86\u6700\u65b0\u6280\u672f\u3001\u4ee3\u8868\u6027\u6848\u4f8b\u7814\u7a76\u3001\u6280\u672f\u548c\u7ec4\u7ec7\u6311\u6218\u4ee5\u53ca\u65b0\u5174\u8d8b\u52bf\u3002", "conclusion": "\u672c\u4f53\u9a71\u52a8\u7684\u8bed\u4e49\u6570\u636e\u7ba1\u7406\u4e3a\u6784\u5efa\u53ef\u6301\u7eed\u3001\u53ef\u4e92\u64cd\u4f5c\u548c\u9ad8\u6027\u80fd\u7684\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u6574\u5408\u4e86\u4eba\u5de5\u667a\u80fd\u3001\u673a\u5668\u5b66\u4e60\u3001\u7269\u8054\u7f51\u548c\u5b9e\u65f6\u5206\u6790\u7b49\u65b0\u5174\u6280\u672f\u3002"}}
{"id": "2510.05943", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05943", "abs": "https://arxiv.org/abs/2510.05943", "authors": ["Zheyue Tan", "Mustapha Abdullahi", "Tuo Shi", "Huining Yuan", "Zelai Xu", "Chao Yu", "Boxun Li", "Bo Zhao"], "title": "EARL: Efficient Agentic Reinforcement Learning Systems for Large Language Models", "comment": null, "summary": "Reinforcement learning (RL) has become a pivotal component of large language\nmodel (LLM) post-training, and agentic RL extends this paradigm to operate as\nagents through multi-turn interaction and tool use. Scaling such systems\nexposes two practical bottlenecks: (1) context length grows rapidly during\ntraining, inflating memory usage and latency, and triggering out-of-memory\n(OOM) failures; and (2) intermediate tensors accumulate with context length,\nmaking cross-device data movement a major system bottleneck.\n  We present EARL, a scalable system for efficient agentic RL. EARL designs a\nparallelism selector that dynamically adapts model and training parallelism\nacross RL stages based on sequence length and system load, and a data\ndispatcher that performs layout-aware, decentralized exchange of intermediate\ndata batches. Together, these components increase throughput, reduce\nlong-context failures, and enable stable large-scale training of agentic LLMs\nwithout relying on hard limits or penalties of context length.", "AI": {"tldr": "EARL\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u6548\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u5e76\u884c\u5316\u9009\u62e9\u5668\u548c\u6570\u636e\u5206\u53d1\u5668\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u4e2d\u9762\u4e34\u4e24\u4e2a\u74f6\u9888\uff1a\u4e0a\u4e0b\u6587\u957f\u5ea6\u5feb\u901f\u589e\u957f\u5bfc\u81f4\u5185\u5b58\u4f7f\u7528\u6fc0\u589e\u548c\u5185\u5b58\u4e0d\u8db3\u6545\u969c\uff0c\u4ee5\u53ca\u4e2d\u95f4\u5f20\u91cf\u7d2f\u79ef\u9020\u6210\u8de8\u8bbe\u5907\u6570\u636e\u4f20\u8f93\u6210\u4e3a\u7cfb\u7edf\u74f6\u9888\u3002", "method": "EARL\u8bbe\u8ba1\u4e86\u5e76\u884c\u5316\u9009\u62e9\u5668\uff0c\u6839\u636e\u5e8f\u5217\u957f\u5ea6\u548c\u7cfb\u7edf\u8d1f\u8f7d\u52a8\u6001\u8c03\u6574\u6a21\u578b\u548c\u8bad\u7ec3\u5e76\u884c\u5ea6\uff1b\u4ee5\u53ca\u6570\u636e\u5206\u53d1\u5668\uff0c\u6267\u884c\u5e03\u5c40\u611f\u77e5\u7684\u5206\u6563\u5f0f\u4e2d\u95f4\u6570\u636e\u6279\u6b21\u4ea4\u6362\u3002", "result": "\u8be5\u7cfb\u7edf\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\uff0c\u51cf\u5c11\u4e86\u957f\u4e0a\u4e0b\u6587\u6545\u969c\uff0c\u5e76\u5b9e\u73b0\u4e86\u667a\u80fd\u4f53LLM\u7684\u7a33\u5b9a\u5927\u89c4\u6a21\u8bad\u7ec3\uff0c\u65e0\u9700\u4f9d\u8d56\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u786c\u6027\u9650\u5236\u6216\u60e9\u7f5a\u3002", "conclusion": "EARL\u901a\u8fc7\u52a8\u6001\u5e76\u884c\u5316\u8c03\u6574\u548c\u9ad8\u6548\u6570\u636e\u7ba1\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u667a\u80fd\u4f53LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
