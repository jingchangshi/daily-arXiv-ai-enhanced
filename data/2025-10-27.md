<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Lincoln AI Computing Survey (LAICS) and Trends](https://arxiv.org/abs/2510.20931)
*Albert Reuther,Peter Michaleas,Michael Jones,Vijay Gadepally,Jeremy Kepner*

Main category: cs.DC

TL;DR: 本文是对过去七年AI加速器和处理器调查的更新，重点关注生成式AI模型训练和推理的计算系统，收集并分析商业加速器的峰值性能和功耗数据。


<details>
  <summary>Details</summary>
Motivation: 由于生成式AI模型在过去一年受到极大关注，需要更新AI加速器和处理器的调查数据，以反映当前计算系统的发展状况。

Method: 延续传统方法，收集公开宣布的商业加速器的峰值性能和功耗数据，在散点图上绘制并分析趋势，按市场细分进行分组展示，新增计算架构分类。

Result: 更新了林肯AI计算调查(LAICS)，包含新加速器的简要描述，提供了性能-功耗散点图和各细分市场的放大视图。

Conclusion: 这项年度调查持续跟踪AI加速器的发展趋势，为理解生成式AI计算系统的演进提供了有价值的参考。

Abstract: In the past year, generative AI (GenAI) models have received a tremendous
amount of attention, which in turn has increased attention to computing systems
for training and inference for GenAI. Hence, an update to this survey is due.
This paper is an update of the survey of AI accelerators and processors from
past seven years, which is called the Lincoln AI Computing Survey -- LAICS
(pronounced "lace"). This multi-year survey collects and summarizes the current
commercial accelerators that have been publicly announced with peak performance
and peak power consumption numbers. In the same tradition of past papers of
this survey, the performance and power values are plotted on a scatter graph,
and a number of dimensions and observations from the trends on this plot are
again discussed and analyzed. Market segments are highlighted on the scatter
plot, and zoomed plots of each segment are also included. A brief description
of each of the new accelerators that have been added in the survey this year is
included, and this update features a new categorization of computing
architectures that implement each of the accelerators.

</details>


### [2] [Towards Straggler-Resilient Split Federated Learning: An Unbalanced Update Approach](https://arxiv.org/abs/2510.21155)
*Dandan Liang,Jianing Zhang,Evan Chen,Zhe Li,Rui Li,Haibo Yang*

Main category: cs.DC

TL;DR: MU-SplitFed是一种抗拖后腿的Split Federated Learning算法，通过不平衡更新机制解耦训练进度与拖后腿延迟，在存在拖后腿节点时显著提升系统性能。


<details>
  <summary>Details</summary>
Motivation: Split Federated Learning结合了联邦学习的并行性和分割学习的计算卸载优势，但受到分布式系统中拖后腿问题的严重影响。由于分割服务器与客户端之间的依赖关系，服务器模型更新需要等待客户端激活，这种同步要求导致显著时间延迟，拖后腿成为系统可扩展性和效率的关键瓶颈。

Method: 提出MU-SplitFed算法，采用零阶优化方法，通过简单而有效的不平衡更新机制，使服务器在每轮客户端通信中执行τ次本地更新，从而将训练进度与拖后腿延迟解耦。

Result: 对于非凸目标函数，MU-SplitFed实现了O(√(d/(τT)))的收敛速率，在通信轮次上获得了τ倍的线性加速。实验表明，在存在拖后腿节点的情况下，MU-SplitFed始终优于基线方法，并通过自适应调整τ有效减轻拖后腿影响。

Conclusion: MU-SplitFed通过不平衡更新机制成功解决了Split Federated Learning中的拖后腿问题，显著提升了系统的可扩展性和训练效率，为边缘设备上的分布式学习提供了有效的解决方案。

Abstract: Split Federated Learning (SFL) enables scalable training on edge devices by
combining the parallelism of Federated Learning (FL) with the computational
offloading of Split Learning (SL). Despite its great success, SFL suffers
significantly from the well-known straggler issue in distributed learning
systems. This problem is exacerbated by the dependency between Split Server and
clients: the Split Server side model update relies on receiving activations
from clients. Such synchronization requirement introduces significant time
latency, making straggler a critical bottleneck to the scalability and
efficiency of the system. To mitigate this problem, we propose MU-SplitFed, a
straggler-resilient SFL algorithm in zeroth-order optimization that decouples
training progress from straggler delays via a simple yet effective unbalanced
update mechanism.
  By enabling the server to perform $\tau$ local updates per client round,
MU-SplitFed achieves a convergence rate of $O(\sqrt{d/(\tau T)})$ for
non-convex objectives, demonstrating a linear speedup of $\tau$ in
communication rounds. Experiments demonstrate that MU-SplitFed consistently
outperforms baseline methods with the presence of stragglers and effectively
mitigates their impact through adaptive tuning of $\tau$. The code for this
project is available at https://github.com/Johnny-Zip/MU-SplitFed.

</details>


### [3] [From SLA to vendor-neutral metrics: An intelligent knowledge-based approach for multi-cloud SLA-based broker](https://arxiv.org/abs/2510.21173)
*Víctor Rampérez,Javier Soriano,David Lizcano,Shadi Aljawarneh,Juan A. Lara*

Main category: cs.DC

TL;DR: 提出一个智能知识系统，能够自动将高级SLA转换为供应商中立指标，解决多云环境中的供应商锁定问题。


<details>
  <summary>Details</summary>
Motivation: 云消费者缺乏专业知识来实施确保服务级别合规的机制，且不同云提供商使用不同的低级指标，导致供应商锁定，阻碍多云环境的优势发挥。

Method: 开发智能知识系统自动翻译高级SLA为供应商中立指标条件，定义供应商中立指标集并解释如何在各云提供商中测量，通过IaaS和PaaS用例在多云环境中验证。

Result: 评估显示两种解决方案的互补性使云消费者能够自动透明地在多云环境中利用多个应用领域，得到云专家认可。

Conclusion: 该解决方案成功解决了多云环境中的SLA翻译和供应商锁定问题，使云消费者能够充分利用多云优势。

Abstract: Cloud computing has been consolidated as a support for the vast majority of
current and emerging technologies. However, there are some barriers that
prevent the exploitation of the full potential of this technology. First, the
major cloud providers currently put the onus of implementing the mechanisms
that ensure compliance with the desired service levels on cloud consumers.
However, consumers do not have the required expertise. Since each cloud
provider exports a different set of low-level metrics, the strategies defined
to ensure compliance with the established service-level agreement (SLA) are
bound to a particular cloud provider. This fosters provider lock-in and
prevents consumers from benefiting from the advantages of multi-cloud
environments. This paper presents a solution to the problem of automatically
translating SLAs into objectives expressed as metrics that can be measured
across multiple cloud providers. First, we propose an intelligent
knowledge-based system capable of automatically translating high-level SLAs
defined by cloud consumers into a set of conditions expressed as vendor-neutral
metrics, providing feedback to cloud consumers (intelligent tutoring system).
Secondly, we present the set of vendor-neutral metrics and explain how they can
be measured for the different cloud providers. Finally, we report a validation
based on two use cases (IaaS and PaaS) in a multi-cloud environment formed by
leading cloud providers. This evaluation has demonstrated that, thanks to the
complementarity of the two solutions, cloud consumers can automatically and
transparently exploit the multi-cloud in many application domains, as endorsed
by the cloud experts consulted in the course of this study.

</details>


### [4] [Generative Federated Learning for Smart Prediction and Recommendation Applications](https://arxiv.org/abs/2510.21183)
*Anwesha Mukherjee,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 提出了一种结合生成对抗网络和联邦学习的生成联邦学习模型，用于解决智能预测和推荐应用中的高响应时间、数据隐私和数据稀缺问题。以心脏健康监测应用为例，通过生成真实合成数据集改善数据多样性和质量，并比较了集中式和去中心化联邦学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决智能预测和推荐应用面临的三个主要挑战：高响应时间、数据隐私泄露风险以及数据稀缺和类别不平衡问题。

Method: 提出生成联邦学习框架，结合生成对抗网络生成真实合成数据集进行数据增强，并在边缘计算环境中实现集中式和去中心化联邦学习。集中式方法中边缘节点与中央服务器通信构建全局和个性化模型，去中心化方法中边缘节点直接交换模型更新。

Result: 提出的框架在心脏健康监测应用中表现优于现有方法：预测准确率比传统框架提高12%，响应时间比纯云系统减少73%。

Conclusion: 生成联邦学习框架能有效解决数据隐私、数据稀缺和响应时间问题，在智能医疗监测应用中具有显著优势。

Abstract: This paper proposes a generative adversarial network and federated
learning-based model to address various challenges of the smart prediction and
recommendation applications, such as high response time, compromised data
privacy, and data scarcity. The integration of the generative adversarial
network and federated learning is referred to as Generative Federated Learning
(GFL). As a case study of the proposed model, a heart health monitoring
application is considered. The realistic synthetic datasets are generated using
the generated adversarial network-based proposed algorithm for improving data
diversity, data quality, and data augmentation, and remove the data scarcity
and class imbalance issues. In this paper, we implement the centralized and
decentralized federated learning approaches in an edge computing paradigm. In
centralized federated learning, the edge nodes communicate with the central
server to build the global and personalized local models in a collaborative
manner. In the decentralized federated learning approach, the edge nodes
communicate among themselves to exchange model updates for collaborative
training. The comparative study shows that the proposed framework outperforms
the existing heart health monitoring applications. The results show that using
the proposed framework (i) the prediction accuracy is improved by 12% than the
conventional framework, and (ii) the response time is reduced by 73% than the
conventional cloud-only system.

</details>


### [5] [Arbitration-Free Consistency is Available (and Vice Versa)](https://arxiv.org/abs/2510.21304)
*Hagit Attiya,Constantin Enea,Enrique Román-Calvo*

Main category: cs.DC

TL;DR: 该论文提出了仲裁自由一致性(AFC)定理，揭示了分布式存储系统中可用性与一致性权衡的根本原理：只有在对象规范和一致性模型是仲裁自由的情况下，才能实现可用的实现。


<details>
  <summary>Details</summary>
Motivation: 经典结果如CAP定理只描述了读写接口的极端情况，缺乏对对象语义和一致性模型组合的精确解释。需要开发一个通用框架来理解哪些组合允许可用实现。

Method: 开发了一个通用语义框架，将操作语义和一致性模型结合在存储规范中。该框架涵盖多种对象类型和一致性模型。

Result: 证明了仲裁自由一致性定理：对象规范在一致性模型中允许可用实现当且仅当它是仲裁自由的，即不需要总仲裁顺序来解决可见性或读取依赖。

Conclusion: AFC定理统一并推广了先前结果，揭示了仲裁自由性是划分无协调一致性与固有同步行为的基本属性。

Abstract: The fundamental tension between \emph{availability} and \emph{consistency}
shapes the design of distributed storage systems. Classical results capture
extreme points of this trade-off: the CAP theorem shows that strong models like
linearizability preclude availability under partitions, while weak models like
causal consistency remain implementable without coordination. These theorems
apply to simple read-write interfaces, leaving open a precise explanation of
the combinations of object semantics and consistency models that admit
available implementations.
  This paper develops a general semantic framework in which storage
specifications combine operation semantics and consistency models. The
framework encompasses a broad range of objects (key-value stores, counters,
sets, CRDTs, and transactional databases) and consistency models (from causal
consistency and sequential consistency to snapshot isolation and transactional
and non-transactional SQL).
  Within this framework, we prove the \emph{Arbitration-Free Consistency} (AFC)
theorem, showing that an object specification within a consistency model admits
an available implementation if and only if it is \emph{arbitration-free}, that
is, it does not require a total arbitration order to resolve visibility or read
dependencies.
  The AFC theorem unifies and generalizes previous results, revealing
arbitration-freedom as the fundamental property that delineates
coordination-free consistency from inherently synchronized behavior.

</details>


### [6] [Parsley's Group Size Study](https://arxiv.org/abs/2510.21348)
*João A. Silva,Hervé Paulino,João M. Lourenço*

Main category: cs.DC

TL;DR: Parsley是一种基于组的弹性分布式哈希表，采用主动节点重定位技术和动态数据分片机制来提高鲁棒性和负载均衡。通过引入软限制阈值来主动维持稳定的组规模，防止硬限制被违反，从而在节点频繁变动时提升系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有分布式哈希表系统通常没有充分论证组规模限制参数的合理性，这可能导致系统在节点变动时性能下降。Parsley旨在通过系统化的参数分析来优化组规模管理，提高系统在动态环境中的稳定性。

Method: 采用预emptive节点重定位技术和动态数据分片机制，引入软限制阈值作为硬限制的缓冲区。通过系统化的覆盖网络特性研究来分析参数对性能和可扩展性的影响，包括拓扑操作、大组行为观察和权衡分析。

Result: 研究提供了对所选配置值的合理解释，展示了软限制如何帮助系统在节点变动时保持稳定。通过主动预防硬限制违规，系统在节点频繁加入和离开时表现出更好的鲁棒性。

Conclusion: Parsley通过引入软限制和系统化的参数分析，为分布式哈希表的组规模管理提供了更可靠的方法。这种方法能够显著提高系统在动态环境中的稳定性和性能，为类似系统的参数配置提供了理论基础。

Abstract: Parsley is a resilient group-based Distributed Hash Table that incorporates a
preemptive peer relocation technique and a dynamic data sharding mechanism to
enhance robustness and balance. In addition to the hard limits on group size,
defined by minimum and maximum thresholds, Parsley introduces two soft limits
that define a target interval for maintaining stable group sizes. These soft
boundaries allow the overlay to take proactive measures to prevent violations
of the hard limits, improving system stability under churn. This work provides
an in-depth analysis of the rationale behind the parameter values adopted for
Parsley's evaluation. Unlike related systems, which specify group size limits
without justification, we conduct a systematic overlay characterization study
to understand the effects of these parameters on performance and scalability.
The study examines topology operations, the behavior of large groups, and the
overall trade-offs observed, offering a grounded explanation for the chosen
configuration values.

</details>


### [7] [LIDC: A Location Independent Multi-Cluster Computing Framework for Data Intensive Science](https://arxiv.org/abs/2510.21373)
*Sankalpa Timilsina,Susmit Shannigrahi*

Main category: cs.DC

TL;DR: 提出了一种使用语义名称的分布式控制平面，用于在地理分散的计算集群上放置计算任务，替代传统的集中式控制器方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于Kubernetes等集中式控制器的计算放置方法不适合多组织协作环境，且工作流通常需要针对单一平台的手动配置，无法适应基础设施的动态变化。

Method: 使用语义名称将计算任务与命名的Kubernetes服务端点进行匹配，实现去中心化的控制平面。

Result: 该方法使计算作业的放置与位置无关，允许任何具有足够资源的集群执行计算，并支持动态计算放置而无需预先了解集群位置或预定义配置。

Conclusion: 基于语义名称的分布式控制平面为地理分布式计算平台提供了更灵活、适应性更强的计算放置解决方案。

Abstract: Scientific communities are increasingly using geographically distributed
computing platforms. The current methods of compute placement predominantly use
logically centralized controllers such as Kubernetes (K8s) to match tasks to
available resources. However, this centralized approach is unsuitable in
multi-organizational collaborations. Furthermore, workflows often need to use
manual configurations tailored for a single platform and cannot adapt to
dynamic changes across infrastructure. Our work introduces a decentralized
control plane for placing computations on geographically dispersed compute
clusters using semantic names. We assign semantic names to computations to
match requests with named Kubernetes (K8s) service endpoints. We show that this
approach provides multiple benefits. First, it allows placement of
computational jobs to be independent of location, enabling any cluster with
sufficient resources to execute the computation. Second, it facilitates dynamic
compute placement without requiring prior knowledge of cluster locations or
predefined configurations.

</details>


### [8] [Learning to Schedule: A Supervised Learning Framework for Network-Aware Scheduling of Data-Intensive Workloads](https://arxiv.org/abs/2510.21419)
*Sankalpa Timilsina,Susmit Shannigrahi*

Main category: cs.DC

TL;DR: 提出基于监督学习的网络感知作业调度器，通过预测作业完成时间来优化节点选择，相比Kubernetes默认调度器提升了34-54%的节点选择准确率


<details>
  <summary>Details</summary>
Motivation: 分布式云环境中网络拥塞、带宽不对称和节点间数据混洗等因素导致应用性能下降，传统基于CPU/内存的主机级指标无法捕捉这些网络条件，导致调度决策不佳

Method: 使用监督学习预测候选作业的完成时间，通过预测-排序机制收集所有节点的实时遥测数据，使用训练好的监督模型估计每个节点的作业持续时间并进行排序选择最佳放置位置

Result: 在FABRIC测试平台上部署的地理分布式Kubernetes集群上评估，运行网络密集型Spark工作负载，相比仅基于当前资源可用性的默认Kubernetes调度器，节点选择准确率提高了34-54%

Conclusion: 该工作的创新点在于展示了监督学习在多地集群上实现实时网络感知作业调度的可行性

Abstract: Distributed cloud environments hosting data-intensive applications often
experience slowdowns due to network congestion, asymmetric bandwidth, and
inter-node data shuffling. These factors are typically not captured by
traditional host-level metrics like CPU or memory. Scheduling without
accounting for these conditions can lead to poor placement decisions, longer
data transfers, and suboptimal job performance. We present a network-aware job
scheduler that uses supervised learning to predict the completion time of
candidate jobs. Our system introduces a prediction-and-ranking mechanism that
collects real-time telemetry from all nodes, uses a trained supervised model to
estimate job duration per node, and ranks them to select the best placement. We
evaluate the scheduler on a geo-distributed Kubernetes cluster deployed on the
FABRIC testbed by running network-intensive Spark workloads. Compared to the
default Kubernetes scheduler, which makes placement decisions based on current
resource availability alone, our proposed supervised scheduler achieved 34-54%
higher accuracy in selecting optimal nodes for job placement. The novelty of
our work lies in the demonstration of supervised learning for real-time,
network-aware job scheduling on a multi-site cluster.

</details>


### [9] [On Reduction and Synthesis of Petri's Cycloids](https://arxiv.org/abs/2510.21493)
*Rüdiger Valk,Daniel Moldt*

Main category: cs.DC

TL;DR: 该论文研究环状Petri网的结构特性，通过定义归约系统来研究不可约环状网的性质，并提出了从Petri网结构合成环状网参数的方法，用于环状网同构判定。


<details>
  <summary>Details</summary>
Motivation: 环状网是Petri网的特殊类型，用于建模动作和事件过程，是Petri一般系统理论的基础。通过四个参数提供代数形式来描述强同步顺序过程，需要进一步研究其结构特性。

Method: 定义了类似重写系统的环状网归约系统，证明了不可约环状网的性质，推导了从Petri网结构合成环状网参数的方法。

Result: 开发了环状网同构判定的高效决策程序，实现了从网络结构到代数参数的转换。

Conclusion: 提出的归约系统和参数合成方法为环状网的结构分析和同构判定提供了有效的理论工具。

Abstract: Cycloids are particular Petri nets for modelling processes of actions and
events, belonging to the fundaments of Petri's general systems theory. Defined
by four parameters they provide an algebraic formalism to describe strongly
synchronized sequential processes. To further investigate their structure,
reduction systems of cycloids are defined in the style of rewriting systems and
properties of irreducible cycloids are proved. In particular the synthesis of
cycloid parameters from their Petri net structure is derived, leading to an
efficient method for a decision procedure for cycloid isomorphism.

</details>


### [10] [Distributed $(Δ+1)$-Coloring in Graphs of Bounded Neighborhood Independence](https://arxiv.org/abs/2510.21549)
*Marc Fuchs,Fabian Kuhn*

Main category: cs.DC

TL;DR: 本文显著改进了在邻域独立度为θ的图中进行(Δ+1)-染色的确定性分布式算法复杂度，从指数时间提升到拟多对数时间


<details>
  <summary>Details</summary>
Motivation: 分布式染色问题是分布式图算法中的关键问题，但确定(Δ+1)-染色的确定性复杂度仍是重要开放问题。本文旨在研究特定图族中该问题的复杂度改进

Method: 利用图的邻域独立性θ特性，设计新的分布式算法，将时间复杂度从2^O(√logΔ)改进为(θ·logΔ)^O(loglogΔ/logloglogΔ)

Result: 在邻域独立性θ为多对数级别的图中，(Δ+1)-染色可在拟多对数时间内完成，同时发现超图边染色方法在秩≥3时失效

Conclusion: 本文显著推进了对(Δ+1)-染色确定性复杂度的理解，为特定图族提供了更快的算法，并揭示了现有方法的局限性

Abstract: The distributed coloring problem is arguably one of the key problems studied
in the area of distributed graph algorithms. The most standard variant of the
problem asks for a proper vertex coloring of a graph with $\Delta+1$ colors,
where $\Delta$ is the maximum degree of the graph. Despite an immense amount of
work on distributed coloring problems in the distributed setting, determining
the deterministic complexity of $(\Delta+1)$-coloring in the standard message
passing model remains one of the most important open questions of the area. In
this paper, we aim to improve our understanding of the deterministic complexity
of $(\Delta+1)$-coloring as a function of $\Delta$ in a special family of
graphs for which significantly faster algorithms are already known. The
neighborhood independence $\theta$ of a graph is the maximum number of pairwise
non-adjacent neighbors of some node of the graph. In general, in graphs of
neighborhood independence $\theta=O(1)$ (e.g., line graphs), it is known that
$(\Delta+1)$-coloring can be solved in $2^{O(\sqrt{\log\Delta})}+O(\log^* n)$
rounds. In the present paper, we significantly improve this result, and we show
that in graphs of neighborhood independence $\theta$, a $(\Delta+1)$-coloring
can be computed in $(\theta\cdot\log\Delta)^{O(\log\log\Delta /
\log\log\log\Delta)}+O(\log^* n)$ rounds and thus in quasipolylogarithmic time
in $\Delta$ as long as $\theta$ is at most polylogarithmic in $\Delta$. We also
show that the known approach that leads to a polylogarithmic in $\Delta$
algorithm for $(2\Delta-1)$-edge coloring already fails for edge colorings of
hypergraphs of rank at least $3$.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [FIFOAdvisor: A DSE Framework for Automated FIFO Sizing of High-Level Synthesis Designs](https://arxiv.org/abs/2510.20981)
*Stefan Abi-Karam,Rishov Sarkar,Suhail Basalama,Jason Cong,Callie Hao*

Main category: cs.AR

TL;DR: FIFOAdvisor是一个自动确定HLS设计中FIFO大小的框架，通过99.9%周期精确的模拟和双目标黑盒优化，在保持最小延迟开销的同时显著降低内存使用。


<details>
  <summary>Details</summary>
Motivation: 数据流硬件设计中FIFO通道缓冲区的正确大小调整具有挑战性 - 过小的FIFO会导致停顿和死锁，过大的FIFO会浪费内存。现有方法依赖限制性假设、保守过度分配或缓慢的RTL模拟。

Method: 利用LightningSim（99.9%周期精确模拟器）进行毫秒级增量运行，将FIFO大小调整制定为双目标黑盒优化问题，探索启发式和基于搜索的方法来表征延迟-资源权衡。

Result: 在Stream-HLS设计基准测试中，FIFOAdvisor在保持最小延迟开销的同时显著降低内存使用，相比传统HLS/RTL协同模拟提供显著的运行时加速。

Conclusion: FIFOAdvisor为快速设计空间探索提供了实用解决方案，特别适用于具有数据相关控制流的复杂加速器，能够找到延迟-内存的帕累托最优前沿。

Abstract: Dataflow hardware designs enable efficient FPGA implementations via
high-level synthesis (HLS), but correctly sizing first-in-first-out (FIFO)
channel buffers remains challenging. FIFO sizes are user-defined and balance
latency and area-undersized FIFOs cause stalls and potential deadlocks, while
oversized ones waste memory. Determining optimal sizes is non-trivial: existing
methods rely on restrictive assumptions, conservative over-allocation, or slow
RTL simulations. We emphasize that runtime-based analyses (i.e., simulation)
are the only reliable way to ensure deadlock-free FIFO optimization for
data-dependent designs.
  We present FIFOAdvisor, a framework that automatically determines FIFO sizes
in HLS designs. It leverages LightningSim, a 99.9\% cycle-accurate simulator
supporting millisecond-scale incremental runs with new FIFO configurations.
FIFO sizing is formulated as a dual-objective black-box optimization problem,
and we explore heuristic and search-based methods to characterize the
latency-resource trade-off. FIFOAdvisor also integrates with Stream-HLS, a
framework for optimizing affine dataflow designs lowered from C++, MLIR, or
PyTorch, enabling deeper optimization of FIFOs in these workloads.
  We evaluate FIFOAdvisor on Stream-HLS design benchmarks spanning linear
algebra and deep learning workloads. Our results reveal Pareto-optimal
latency-memory frontiers across optimization strategies. Compared to baseline
designs, FIFOAdvisor achieves much lower memory usage with minimal delay
overhead. Additionally, it delivers significant runtime speedups over
traditional HLS/RTL co-simulation, making it practical for rapid design space
exploration. We further demonstrate its capability on a complex accelerator
with data-dependent control flow.
  Code and results: https://github.com/sharc-lab/fifo-advisor

</details>


### [12] [Hardware-Efficient Accurate 4-bit Multiplier for Xilinx 7 Series FPGAs](https://arxiv.org/abs/2510.21533)
*Misaki Kida,Shimpei Sato*

Main category: cs.AR

TL;DR: 提出了一种针对AMD Xilinx 7系列FPGA的高效4位乘法器设计，仅使用11个LUT和2个CARRY4模块，相比之前的12-LUT设计减少了资源使用并缩短了关键路径。


<details>
  <summary>Details</summary>
Motivation: 随着IoT和边缘推理的普及，需要在查找表(LUT)乘法器中同时优化面积和延迟，以并行实现大量低比特位运算。

Method: 通过重新组织映射到LUT的逻辑函数，减少LUT数量并缩短关键路径，设计仅使用11个LUT和两个CARRY4模块的4位乘法器。

Result: 电路实现了最小资源使用和2.750 ns的关键路径延迟，相比之前的12-LUT设计减少了一个LUT。

Conclusion: 该设计在AMD Xilinx 7系列FPGA上实现了硬件高效的4位乘法器，在资源使用和性能方面都有显著改进。

Abstract: As IoT and edge inference proliferate,there is a growing need to
simultaneously optimize area and delay in lookup-table (LUT)-based multipliers
that implement large numbers of low-bitwidth operations in parallel. This paper
proposes a hardwareefficientaccurate 4-bit multiplier design for AMD Xilinx
7-series FPGAs using only 11 LUTs and two CARRY4 blocks. By reorganizing the
logic functions mapped to the LUTs, the proposed method reduces the LUT count
by one compared with the prior 12-LUT design while also shortening the critical
path. Evaluation confirms that the circuit attains minimal resource usage and a
critical-path delay of 2.750 ns.

</details>


### [13] [Accelerating Electrostatics-based Global Placement with Enhanced FFT Computation](https://arxiv.org/abs/2510.21547)
*Hangyu Zhang,Sachin S. Sapatnekar*

Main category: cs.AR

TL;DR: 使用AccFFT加速FFT计算，显著降低全局布局算法的运行时间，在ePlace-MS和Pplace-MS算法中实现了5.78倍FFT计算加速和32%总运行时间改进。


<details>
  <summary>Details</summary>
Motivation: 现代VLSI设计需要高质量和高效的电路布局，而基于静电学的分析布局方法在可扩展性和解质量方面已有改进，但FFT计算仍是性能瓶颈。

Method: 采用AccFFT加速技术来优化电场计算中的FFT运算，并将其集成到ePlace-MS和Pplace-MS布局算法中。

Result: 实验结果显示FFT计算速度提升5.78倍，总运行时间改善32%，详细布局后缩放半周长线长仅减少1.0%。

Conclusion: AccFFT技术能有效加速全局布局算法，显著减少运行时间同时保持布局质量。

Abstract: Global placement is essential for high-quality and efficient circuit
placement for complex modern VLSI designs. Recent advancements, such as
electrostatics-based analytic placement, have improved scalability and solution
quality. This work demonstrates that using an accelerated FFT technique,
AccFFT, for electric field computation significantly reduces runtime.
Experimental results on standard benchmarks show significant improvements when
incorporated into the ePlace-MS and Pplace-MS algorithms, e.g., a 5.78x speedup
in FFT computation and a 32% total runtime improvement against ePlace-MS, with
1.0% reduction of scaled half-perimeter wirelength after detailed placement.

</details>
