{"id": "2509.14388", "categories": ["cs.AR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14388", "abs": "https://arxiv.org/abs/2509.14388", "authors": ["Lennart Bamberg", "Filippo Minnella", "Roberto Bosio", "Fabrizio Ottati", "Yuebin Wang", "Jongmin Lee", "Luciano Lavagno", "Adam Fuks"], "title": "eIQ Neutron: Redefining Edge-AI Inference with Integrated NPU and Compiler Innovations", "comment": "Submitted to IEEE Transactions on Computers", "summary": "Neural Processing Units (NPUs) are key to enabling efficient AI inference in\nresource-constrained edge environments. While peak tera operations per second\n(TOPS) is often used to gauge performance, it poorly reflects real-world\nperformance and typically rather correlates with higher silicon cost. To\naddress this, architects must focus on maximizing compute utilization, without\nsacrificing flexibility. This paper presents the eIQ Neutron efficient-NPU,\nintegrated into a commercial flagship MPU, alongside co-designed compiler\nalgorithms. The architecture employs a flexible, data-driven design, while the\ncompiler uses a constrained programming approach to optimize compute and data\nmovement based on workload characteristics. Compared to the leading embedded\nNPU and compiler stack, our solution achieves an average speedup of 1.8x (4x\npeak) at equal TOPS and memory resources across standard AI-benchmarks. Even\nagainst NPUs with double the compute and memory resources, Neutron delivers up\nto 3.3x higher performance.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86eIQ Neutron\u9ad8\u6548NPU\u67b6\u6784\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u6570\u636e\u9a71\u52a8\u8bbe\u8ba1\u548c\u7f16\u8bd1\u5668\u4f18\u5316\uff0c\u5728\u540c\u7b49TOPS\u548c\u5185\u5b58\u8d44\u6e90\u4e0b\u6bd4\u9886\u5148\u7684\u5d4c\u5165\u5f0fNPU\u6027\u80fd\u63d0\u53471.8\u500d\uff0c\u5373\u4f7f\u9762\u5bf9\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u7ffb\u500d\u7684NPU\u4e5f\u80fd\u5b9e\u73b0\u6700\u9ad83.3\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u4f7f\u7528\u5cf0\u503cTOPS\u6765\u8861\u91cfNPU\u6027\u80fd\u7684\u65b9\u6cd5\u4e0d\u80fd\u51c6\u786e\u53cd\u6620\u5b9e\u9645\u6027\u80fd\uff0c\u4e14\u901a\u5e38\u4e0e\u66f4\u9ad8\u7684\u7845\u6210\u672c\u76f8\u5173\u3002\u9700\u8981\u5728\u4fdd\u6301\u7075\u6d3b\u6027\u7684\u540c\u65f6\u6700\u5927\u5316\u8ba1\u7b97\u5229\u7528\u7387\u3002", "method": "\u91c7\u7528\u7075\u6d3b\u7684\u6570\u636e\u9a71\u52a8\u67b6\u6784\u8bbe\u8ba1\uff0c\u7ed3\u5408\u534f\u540c\u8bbe\u8ba1\u7684\u7f16\u8bd1\u5668\u7b97\u6cd5\uff0c\u4f7f\u7528\u7ea6\u675f\u7f16\u7a0b\u65b9\u6cd5\u6839\u636e\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\u4f18\u5316\u8ba1\u7b97\u548c\u6570\u636e\u79fb\u52a8\u3002", "result": "\u5728\u6807\u51c6AI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u9886\u5148\u7684\u5d4c\u5165\u5f0fNPU\u548c\u7f16\u8bd1\u5668\u5806\u6808\uff0c\u5728\u540c\u7b49TOPS\u548c\u5185\u5b58\u8d44\u6e90\u4e0b\u5e73\u5747\u52a0\u901f1.8\u500d\uff08\u5cf0\u503c4\u500d\uff09\u3002\u5373\u4f7f\u9762\u5bf9\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u7ffb\u500d\u7684NPU\uff0c\u4e5f\u80fd\u5b9e\u73b0\u6700\u9ad83.3\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "eIQ Neutron NPU\u901a\u8fc7\u67b6\u6784\u548c\u7f16\u8bd1\u5668\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u7075\u6d3b\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u5229\u7528\u7387\uff0c\u8bc1\u660e\u4e86\u5728\u8fb9\u7f18AI\u63a8\u7406\u4e2d\u5b9e\u73b0\u9ad8\u6548\u6027\u80fd\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.14551", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.14551", "abs": "https://arxiv.org/abs/2509.14551", "authors": ["Xinyue Wu", "Zixuan Li", "Fan Hu", "Ting Lin", "Xiaotian Zhao", "Runxi Wang", "Xinfei Guo"], "title": "Shift-Left Techniques in Electronic Design Automation: A Survey", "comment": null, "summary": "The chip design process involves numerous steps, beginning with defining\nproduct requirements and progressing through architectural planning,\nsystem-level design, and the physical layout of individual circuit blocks. As\nthe enablers of large-scale chip development, Electronic Design Automation\n(EDA) tools play a vital role in helping designers achieve high-quality\nresults. The Shift-Left methodology introduces a pathway toward creating\ndigital twins and fusing multiple design steps, thereby transitioning\ntraditionally sequential, physically-aware processes into virtual design\nenvironments. This shift allows designers to establish stronger correlations\nearlier and optimize designs more effectively. However, challenges remain,\nespecially in accurately replicating downstream behaviors and determining the\nright scope and timing for adoption. These challenges, in turn, have revealed\nnew opportunities for EDA vendors, physical designers, and logic designers\nalike. As the industry advances toward intelligent EDA tools and techniques, it\nis timely to reflect on Shift-Left progress made and the challenges that\nremain. The rise of AI techniques and the momentum of open-source design flows\nhave significantly strengthened prediction and modeling capabilities, making\ndata-driven methods increasingly relevant to the EDA community. This, in turn,\nenhances the ''Shift-Left'' features embedded in current tools. In this paper,\nwe present a comprehensive survey of existing and emerging paradigms in\nShift-Left research within EDA and the broader design ecosystem. Our goal is to\nprovide a unique perspective on the state of the field and its future\ndirections. Relevant papers mentioned are organized in\nhttps://github.com/iCAS-SJTU/Shift-Left-EDA-Papers.", "AI": {"tldr": "\u672c\u6587\u5bf9EDA\u9886\u57df\u4e2d\u7684Shift-Left\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u8be5\u65b9\u6cd5\u7684\u8fdb\u5c55\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u91cd\u70b9\u5173\u6ce8AI\u6280\u672f\u548c\u5f00\u6e90\u8bbe\u8ba1\u6d41\u7a0b\u5982\u4f55\u589e\u5f3a\u9884\u6d4b\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u82af\u7247\u8bbe\u8ba1\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u4f20\u7edf\u4e32\u884c\u8bbe\u8ba1\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\u3002Shift-Left\u65b9\u6cd5\u901a\u8fc7\u521b\u5efa\u6570\u5b57\u5b6a\u751f\u548c\u878d\u5408\u591a\u4e2a\u8bbe\u8ba1\u6b65\u9aa4\uff0c\u4f7f\u8bbe\u8ba1\u5e08\u80fd\u591f\u66f4\u65e9\u5efa\u7acb\u5f3a\u76f8\u5173\u6027\u5e76\u4f18\u5316\u8bbe\u8ba1\uff0c\u4f46\u51c6\u786e\u590d\u5236\u4e0b\u6e38\u884c\u4e3a\u548c\u786e\u5b9a\u91c7\u7528\u65f6\u673a\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86EDA\u548c\u66f4\u5e7f\u6cdb\u8bbe\u8ba1\u751f\u6001\u7cfb\u7edf\u4e2d\u73b0\u6709\u548c\u65b0\u5174\u7684Shift-Left\u7814\u7a76\u8303\u5f0f\uff0c\u6536\u96c6\u6574\u7406\u4e86\u76f8\u5173\u8bba\u6587\u5e76\u8fdb\u884c\u4e86\u5206\u7c7b\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u6280\u672f\u548c\u5f00\u6e90\u8bbe\u8ba1\u6d41\u7a0b\u7684\u5174\u8d77\u663e\u8457\u589e\u5f3a\u4e86\u9884\u6d4b\u548c\u5efa\u6a21\u80fd\u529b\uff0c\u4f7f\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728EDA\u793e\u533a\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u5f53\u524d\u5de5\u5177\u4e2d\u7684Shift-Left\u529f\u80fd\u3002", "conclusion": "Shift-Left\u65b9\u6cd5\u5728EDA\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u6311\u6218\u3002AI\u548c\u5f00\u6e90\u5de5\u5177\u7684\u878d\u5408\u4e3a\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\uff0c\u9700\u8981\u7ee7\u7eed\u63a2\u7d22\u667a\u80fdEDA\u5de5\u5177\u548c\u6280\u672f\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2509.14668", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.14668", "abs": "https://arxiv.org/abs/2509.14668", "authors": ["Yonghao Wang", "Jiaxin Zhou", "Hongqin Lyu", "Zhiteng Chao", "Tiancheng Wang", "Huawei Li"], "title": "DeepAssert: An LLM-Aided Verification Framework with Fine-Grained Assertion Generation for Modules with Extracted Module Specifications", "comment": "7 pages, 8 figures", "summary": "Assertion-Based Verification (ABV) is a crucial method for ensuring that\nlogic designs conform to their architectural specifications. However, existing\nassertion generation methods primarily rely on information either from the\ndesign specification, or register-transfer level (RTL) code. The former methods\nare typically limited to generating assertions for the top-level design. As the\ntop-level design is composed of different modules without module-level\nspecifications, they are unable to generate deep assertions that target the\ninternal functionality of modules. The latter methods often rely on a golden\nRTL model, which is difficult to obtain. To address the above limitations, this\npaper presents a novel large language model (LLM)-aided verification framework\nnamed DeepAssert. DeepAssert is capable of analyzing the invocation\nrelationships between modules and extracting independent specifications for\neach module with its I/O port information. These extracted specifications are\nsubsequently used to guide LLMs to automatically generate fine-grained deep\nassertions for these modules. Our evaluation demonstrates that DeepAssert\nsignificantly outperforms existing methods such as AssertLLM and Spec2Assertion\nin generating high-quality deep assertions for modules. Furthermore, when\nintegrated with these methods, DeepAssert can enhance the overall quality of\nthe assertions generated. This allows for a more comprehensive and effective\nverification process.", "AI": {"tldr": "DeepAssert\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u9a8c\u8bc1\u6846\u67b6\uff0c\u80fd\u591f\u5206\u6790\u6a21\u5757\u95f4\u8c03\u7528\u5173\u7cfb\u5e76\u81ea\u52a8\u751f\u6210\u7ec6\u7c92\u5ea6\u6df1\u5ea6\u65ad\u8a00\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65ad\u8a00\u751f\u6210\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u8bbe\u8ba1\u89c4\u8303\uff08\u53ea\u80fd\u751f\u6210\u9876\u5c42\u65ad\u8a00\uff09\uff0c\u8981\u4e48\u9700\u8981\u9ec4\u91d1RTL\u6a21\u578b\uff08\u96be\u4ee5\u83b7\u53d6\uff09\uff0c\u65e0\u6cd5\u751f\u6210\u9488\u5bf9\u6a21\u5757\u5185\u90e8\u529f\u80fd\u7684\u6df1\u5ea6\u65ad\u8a00\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6a21\u5757\u95f4\u8c03\u7528\u5173\u7cfb\uff0c\u63d0\u53d6\u5404\u6a21\u5757\u7684\u72ec\u7acb\u89c4\u8303\u53ca\u5176I/O\u7aef\u53e3\u4fe1\u606f\uff0c\u7136\u540e\u5229\u7528LLM\u81ea\u52a8\u4e3a\u8fd9\u4e9b\u6a21\u5757\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u6df1\u5ea6\u65ad\u8a00\u3002", "result": "DeepAssert\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u6a21\u5757\u6df1\u5ea6\u65ad\u8a00\u65b9\u9762\u663e\u8457\u4f18\u4e8eAssertLLM\u548cSpec2Assertion\u7b49\u65b9\u6cd5\uff0c\u4e14\u80fd\u63d0\u5347\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6574\u4f53\u65ad\u8a00\u8d28\u91cf\u3002", "conclusion": "DeepAssert\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u6709\u6548\u7684\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u6a21\u5757\u7ea7\u6df1\u5ea6\u65ad\u8a00\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.14781", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.14781", "abs": "https://arxiv.org/abs/2509.14781", "authors": ["Yimin Wang", "Yue Jiet Chong", "Xuanyao Fong"], "title": "LEAP: LLM Inference on Scalable PIM-NoC Architecture with Balanced Dataflow and Fine-Grained Parallelism", "comment": "Accepted to the 2025 International Conference on Computer-Aided\n  Design (ICCAD'25)", "summary": "Large language model (LLM) inference has been a prevalent demand in daily\nlife and industries. The large tensor sizes and computing complexities in LLMs\nhave brought challenges to memory, computing, and databus. This paper proposes\na computation/memory/communication co-designed non-von Neumann accelerator by\naggregating processing-in-memory (PIM) and computational network-on-chip (NoC),\ntermed LEAP. The matrix multiplications in LLMs are assigned to PIM or NoC\nbased on the data dynamicity to maximize data locality. Model partition and\nmapping are optimized by heuristic design space exploration. Dedicated\nfine-grained parallelism and tiling techniques enable high-throughput dataflow\nacross the distributed resources in PIM and NoC. The architecture is evaluated\non Llama 1B/8B/13B models and shows $\\sim$2.55$\\times$ throughput (tokens/sec)\nimprovement and $\\sim$71.94$\\times$ energy efficiency (tokens/Joule) boost\ncompared to the A100 GPU.", "AI": {"tldr": "LEAP\u662f\u4e00\u79cd\u975e\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\u7684LLM\u63a8\u7406\u52a0\u901f\u5668\uff0c\u901a\u8fc7PIM\u548cNoC\u534f\u540c\u8bbe\u8ba1\uff0c\u5728Llama\u6a21\u578b\u4e0a\u76f8\u6bd4A100 GPU\u5b9e\u73b0\u4e862.55\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c71.94\u500d\u80fd\u6548\u63d0\u5347", "motivation": "LLM\u63a8\u7406\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u5927\u5f20\u91cf\u5c3a\u5bf8\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u7ed9\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u6570\u636e\u603b\u7ebf\u5e26\u6765\u4e86\u6311\u6218\uff0c\u9700\u8981\u65b0\u7684\u786c\u4ef6\u67b6\u6784\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898", "method": "\u63d0\u51fa\u8ba1\u7b97/\u5185\u5b58/\u901a\u4fe1\u534f\u540c\u8bbe\u8ba1\u7684\u975e\u51af\u00b7\u8bfa\u4f9d\u66fc\u52a0\u901f\u5668LEAP\uff0c\u7ed3\u5408PIM\u548c\u8ba1\u7b97NoC\uff0c\u6839\u636e\u6570\u636e\u52a8\u6001\u6027\u5206\u914d\u77e9\u9635\u4e58\u6cd5\uff0c\u91c7\u7528\u542f\u53d1\u5f0f\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u4f18\u5316\u6a21\u578b\u5206\u533a\u548c\u6620\u5c04\uff0c\u4f7f\u7528\u7ec6\u7c92\u5ea6\u5e76\u884c\u548c\u5206\u5757\u6280\u672f\u5b9e\u73b0\u9ad8\u541e\u5410\u6570\u636e\u6d41", "result": "\u5728Llama 1B/8B/13B\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4A100 GPU\u5b9e\u73b0\u4e86\u7ea62.55\u500d\u7684\u541e\u5410\u91cf\uff08tokens/sec\uff09\u63d0\u5347\u548c\u7ea671.94\u500d\u7684\u80fd\u6548\uff08tokens/Joule\uff09\u63d0\u5347", "conclusion": "LEAP\u67b6\u6784\u901a\u8fc7PIM\u548cNoC\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u901a\u4fe1\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u548c\u80fd\u6548"}}
{"id": "2509.14496", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.14496", "abs": "https://arxiv.org/abs/2509.14496", "authors": ["Wyatt Petula", "Anushcka Joshi", "Peggy Tu", "Amrutha Somasundar", "Suman Saha"], "title": "DeliverC: Teaching Pointers through GenAI-Powered Game-Based Learning", "comment": "The paper before Camera-ready paper. The paper has been accepted by\n  SIGCSE 2026", "summary": "While game-based learning is widely used in programming education, few tools\noffer adaptive, real-time support for complex topics, such as C pointers. We\npresent DeliverC, a GenAI-enhanced game that integrates GPT-4-mini to provide\npersonalized hints and generate pointer-related challenges on the fly. In a\npilot study involving 25 undergraduate students, we investigated the impact of\nthe system on learning through gameplay data and a 15-item survey that covered\nconstructs such as motivation, self-efficacy, metacognition, and feedback\nquality. Results show that most students felt more confident and reflective\nafter using the tool, and error rates decreased as students progressed through\nscaffolded levels. However, participation decreased with task difficulty, and\nsome students reported receiving unclear or vague feedback. These findings\nsuggest that DeliverC can enhance engagement and understanding in systems\nprogramming, although refinement in AI-generated feedback is still needed. Our\nstudy highlights the potential of combining GenAI with game-based learning to\nsupport personalized and interactive practice in traditionally challenging\nprogramming domains.", "AI": {"tldr": "DeliverC\u662f\u4e00\u4e2a\u96c6\u6210GPT-4-mini\u7684GenAI\u589e\u5f3a\u6e38\u620f\uff0c\u4e3aC\u8bed\u8a00\u6307\u9488\u5b66\u4e60\u63d0\u4f9b\u5b9e\u65f6\u4e2a\u6027\u5316\u63d0\u793a\u548c\u6311\u6218\u751f\u6210\u3002\u8bd5\u70b9\u7814\u7a76\u8868\u660e\u8be5\u7cfb\u7edf\u80fd\u63d0\u5347\u5b66\u751f\u81ea\u4fe1\u548c\u53cd\u601d\u80fd\u529b\uff0c\u4f46AI\u751f\u6210\u53cd\u9988\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u89e3\u51b3\u7f16\u7a0b\u6559\u80b2\u4e2d\u590d\u6742\u4e3b\u9898\uff08\u5982C\u6307\u9488\uff09\u7f3a\u4e4f\u5b9e\u65f6\u81ea\u9002\u5e94\u652f\u6301\u5de5\u5177\u7684\u95ee\u9898\uff0c\u63a2\u7d22GenAI\u4e0e\u6e38\u620f\u5316\u5b66\u4e60\u7ed3\u5408\u5728\u4f20\u7edf\u6311\u6218\u6027\u7f16\u7a0b\u9886\u57df\u7684\u6f5c\u529b\u3002", "method": "\u5f00\u53d1DeliverC\u6e38\u620f\uff0c\u96c6\u6210GPT-4-mini\u63d0\u4f9b\u4e2a\u6027\u5316\u63d0\u793a\u548c\u5b9e\u65f6\u751f\u6210\u6307\u9488\u76f8\u5173\u6311\u6218\u3002\u901a\u8fc725\u540d\u672c\u79d1\u751f\u7684\u8bd5\u70b9\u7814\u7a76\uff0c\u6536\u96c6\u6e38\u620f\u6570\u636e\u548c15\u9879\u95ee\u5377\u8c03\u67e5\uff08\u6db5\u76d6\u52a8\u673a\u3001\u81ea\u6211\u6548\u80fd\u3001\u5143\u8ba4\u77e5\u548c\u53cd\u9988\u8d28\u91cf\u7b49\u6784\u5ff5\uff09\u3002", "result": "\u5927\u591a\u6570\u5b66\u751f\u4f7f\u7528\u540e\u611f\u5230\u66f4\u81ea\u4fe1\u548c\u5584\u4e8e\u53cd\u601d\uff0c\u9519\u8bef\u7387\u968f\u7740\u811a\u624b\u67b6\u5f0f\u5173\u5361\u8fdb\u5c55\u800c\u4e0b\u964d\u3002\u4f46\u53c2\u4e0e\u5ea6\u968f\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\u800c\u964d\u4f4e\uff0c\u90e8\u5206\u5b66\u751f\u53cd\u9988AI\u751f\u6210\u7684\u63d0\u793a\u4e0d\u591f\u6e05\u6670\u3002", "conclusion": "DeliverC\u80fd\u591f\u589e\u5f3a\u7cfb\u7edf\u7f16\u7a0b\u5b66\u4e60\u7684\u53c2\u4e0e\u5ea6\u548c\u7406\u89e3\uff0c\u4f46\u9700\u8981\u6539\u8fdbAI\u751f\u6210\u53cd\u9988\u7684\u8d28\u91cf\u3002\u7814\u7a76\u8bc1\u660e\u4e86GenAI\u4e0e\u6e38\u620f\u5316\u5b66\u4e60\u7ed3\u5408\u5728\u652f\u6301\u4e2a\u6027\u5316\u4ea4\u4e92\u5f0f\u7f16\u7a0b\u5b9e\u8df5\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.14920", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.14920", "abs": "https://arxiv.org/abs/2509.14920", "authors": ["Amine Barrak", "Fabio Petrillo", "Fehmi Jaafar"], "title": "Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless and GPU-Based Training Architectures", "comment": null, "summary": "The field of distributed machine learning (ML) faces increasing demands for\nscalable and cost-effective training solutions, particularly in the context of\nlarge, complex models. Serverless computing has emerged as a promising paradigm\nto address these challenges by offering dynamic scalability and\nresource-efficient execution. Building upon our previous work, which introduced\nthe Serverless Peer Integrated for Robust Training (SPIRT) architecture, this\npaper presents a comparative analysis of several serverless distributed ML\narchitectures. We examine SPIRT alongside established architectures like\nScatterReduce, AllReduce, and MLLess, focusing on key metrics such as training\ntime efficiency, cost-effectiveness, communication overhead, and fault\ntolerance capabilities. Our findings reveal that SPIRT provides significant\nimprovements in reducing training times and communication overhead through\nstrategies such as parallel batch processing and in-database operations\nfacilitated by RedisAI. However, traditional architectures exhibit scalability\nchallenges and varying degrees of vulnerability to faults and adversarial\nattacks. The cost analysis underscores the long-term economic benefits of SPIRT\ndespite its higher initial setup costs. This study not only highlights the\nstrengths and limitations of current serverless ML architectures but also sets\nthe stage for future research aimed at developing new models that combine the\nmost effective features of existing systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u6bd4\u5206\u6790\u4e86\u591a\u79cd\u65e0\u670d\u52a1\u5668\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u67b6\u6784\uff0c\u53d1\u73b0SPIRT\u67b6\u6784\u5728\u8bad\u7ec3\u6548\u7387\u3001\u901a\u4fe1\u5f00\u9500\u548c\u6548\u679c\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6848", "motivation": "\u5e94\u5bf9\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u5bf9\u53ef\u6269\u5c55\u6027\u548c\u6210\u672c\u6548\u679c\u7684\u65e5\u76ca\u589e\u957f\u9700\u6c42\uff0c\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5bf9\u6bd4\u5206\u6790SPIRT\u4e0eScatterReduce\u3001AllReduce\u3001MLLess\u7b49\u67b6\u6784\uff0c\u91cd\u70b9\u8003\u5bdf\u8bad\u7ec3\u65f6\u95f4\u6548\u7387\u3001\u6210\u672c\u6548\u679c\u3001\u901a\u4fe1\u5f00\u9500\u548c\u654c\u5bb9\u80fd\u529b", "result": "SPIRT\u901a\u8fc7\u5e76\u884c\u6279\u5904\u7406\u548cRedisAI\u6570\u636e\u5e93\u64cd\u4f5c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u4f20\u7edf\u67b6\u6784\u9047\u5230\u6269\u5c55\u6027\u6311\u6218\u548c\u654c\u5bb9\u95ee\u9898", "conclusion": "SPIRT\u867d\u7136\u521d\u59cb\u6210\u672c\u8f83\u9ad8\uff0c\u4f46\u5177\u6709\u957f\u671f\u7ecf\u6d4e\u6548\u76ca\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u7ed3\u5408\u5404\u79cd\u67b6\u6784\u4f18\u70b9\u7684\u65b0\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2509.15036", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.15036", "abs": "https://arxiv.org/abs/2509.15036", "authors": ["Yuehai Chen", "Farhad Merchant"], "title": "NEURAL: An Elastic Neuromorphic Architecture with Hybrid Data-Event Execution and On-the-fly Attention Dataflow", "comment": "Accepted by ASP-DAC 2026; 7 pages, 10 figures", "summary": "Spiking neural networks (SNNs) have emerged as a promising alternative to\nartificial neural networks (ANNs), offering improved energy efficiency by\nleveraging sparse and event-driven computation. However, existing hardware\nimplementations of SNNs still suffer from the inherent spike sparsity and\nmulti-timestep execution, which significantly increase latency and reduce\nenergy efficiency. This study presents NEURAL, a novel neuromorphic\narchitecture based on a hybrid data-event execution paradigm by decoupling\nsparsity-aware processing from neuron computation and using elastic\nfirst-in-first-out (FIFO). NEURAL supports on-the-fly execution of spiking\nQKFormer by embedding its operations within the baseline computing flow without\nrequiring dedicated hardware units. It also integrates a novel\nwindow-to-time-to-first-spike (W2TTFS) mechanism to replace average pooling and\nenable full-spike execution. Furthermore, we introduce a knowledge distillation\n(KD)-based training framework to construct single-timestep SNN models with\ncompetitive accuracy. NEURAL is implemented on a Xilinx Virtex-7 FPGA and\nevaluated using ResNet-11, QKFResNet-11, and VGG-11. Experimental results\ndemonstrate that, at the algorithm level, the VGG-11 model trained with KD\nimproves accuracy by 3.20% on CIFAR-10 and 5.13% on CIFAR-100. At the\narchitecture level, compared to existing SNN accelerators, NEURAL achieves a\n50% reduction in resource utilization and a 1.97x improvement in energy\nefficiency.", "AI": {"tldr": "NEURAL\u662f\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u6570\u636e-\u4e8b\u4ef6\u6267\u884c\u8303\u5f0f\u7684\u795e\u7ecf\u5f62\u6001\u67b6\u6784\uff0c\u901a\u8fc7\u89e3\u8026\u7a00\u758f\u611f\u77e5\u5904\u7406\u548c\u795e\u7ecf\u5143\u8ba1\u7b97\uff0c\u4f7f\u7528\u5f39\u6027FIFO\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\uff0c\u5728\u8d44\u6e90\u5229\u7528\u548c\u80fd\u6548\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709SNN\u52a0\u901f\u5668\u3002", "motivation": "\u73b0\u6709SNN\u786c\u4ef6\u5b9e\u73b0\u53d7\u9650\u4e8e\u8109\u51b2\u7a00\u758f\u6027\u548c\u591a\u65f6\u95f4\u6b65\u6267\u884c\uff0c\u5bfc\u81f4\u5ef6\u8fdf\u589e\u52a0\u548c\u80fd\u6548\u964d\u4f4e\uff0c\u9700\u8981\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faNEURAL\u67b6\u6784\uff1a1\uff09\u6df7\u5408\u6570\u636e-\u4e8b\u4ef6\u6267\u884c\u8303\u5f0f\uff1b2\uff09\u89e3\u8026\u7a00\u758f\u611f\u77e5\u5904\u7406\u4e0e\u795e\u7ecf\u5143\u8ba1\u7b97\uff1b3\uff09\u5f39\u6027FIFO\uff1b4\uff09W2TTFS\u673a\u5236\u66ff\u4ee3\u5e73\u5747\u6c60\u5316\uff1b5\uff09\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u8bad\u7ec3\u6846\u67b6\u6784\u5efa\u5355\u65f6\u95f4\u6b65SNN\u6a21\u578b\u3002", "result": "\u7b97\u6cd5\u5c42\u9762\uff1aVGG-11\u6a21\u578b\u5728CIFAR-10\u548cCIFAR-100\u4e0a\u51c6\u786e\u7387\u5206\u522b\u63d0\u53473.20%\u548c5.13%\uff1b\u67b6\u6784\u5c42\u9762\uff1a\u76f8\u6bd4\u73b0\u6709SNN\u52a0\u901f\u5668\uff0c\u8d44\u6e90\u5229\u7528\u7387\u964d\u4f4e50%\uff0c\u80fd\u6548\u63d0\u53471.97\u500d\u3002", "conclusion": "NEURAL\u67b6\u6784\u901a\u8fc7\u521b\u65b0\u7684\u6df7\u5408\u6267\u884c\u8303\u5f0f\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86SNN\u786c\u4ef6\u5b9e\u73b0\u7684\u5ef6\u8fdf\u548c\u80fd\u6548\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.15005", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.15005", "abs": "https://arxiv.org/abs/2509.15005", "authors": ["Facundo Dom\u00ednguez", "Arnaud Spiwack"], "title": "Refinement-Types Driven Development: A study", "comment": "11 pages, 3 figures, artifacts\n  https://github.com/tweag/ifl2025-liquidhaskell", "summary": "This paper advocates for the broader application of SMT solvers in everyday\nprogramming, challenging the conventional wisdom that these tools are solely\nfor formal methods and verification. We claim that SMT solvers, when seamlessly\nintegrated into a compiler's static checks, significantly enhance the\ncapabilities of ordinary type checkers in program composition. Specifically, we\nargue that refinement types, as embodied by Liquid Haskell, enable the use of\nSMT solvers in mundane programming tasks. Through a case study on handling\nbinder scopes in compilers, we envision a future where ordinary programming is\nmade simpler and more enjoyable with the aid of refinement types and SMT\nsolvers. As a secondary contribution, we present a prototype implementation of\na theory of finite maps for Liquid Haskell's solver, developed to support our\ncase study.", "AI": {"tldr": "\u63a8\u5e7fSMT\u6c42\u89e3\u5668\u5728\u666e\u901a\u7f16\u7a0b\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u7cbe\u7ec6\u7c7b\u578b\u4e0e\u7f16\u8bd1\u5668\u6574\u5408\u6765\u63d0\u5347\u7a0b\u5e8f\u7ec4\u5408\u80fd\u529b", "motivation": "\u8d28\u7591SMT\u6c42\u89e3\u5668\u4ec5\u9002\u7528\u4e8e\u5f62\u5f0f\u9a8c\u8bc1\u7684\u4f20\u7edf\u89c2\u5ff5\uff0c\u8ba4\u4e3a\u5176\u5728\u666e\u901a\u7f16\u7a0b\u4e2d\u4e5f\u6709\u5e7f\u9614\u5e94\u7528\u6f5c\u529b", "method": "\u91c7\u7528\u7cbe\u7ec6\u7c7b\u578b\uff08Liquid Haskell\uff09\u6280\u672f\uff0c\u5c06SMT\u6c42\u89e3\u5668\u96c6\u6210\u5230\u7f16\u8bd1\u5668\u9759\u6001\u68c0\u67e5\u4e2d\uff0c\u5e76\u901a\u8fc7\u7ed1\u5b9a\u8303\u56f4\u5904\u7406\u6848\u4f8b\u8fdb\u884c\u9a8c\u8bc1", "result": "\u5b9e\u73b0\u4e86Liquid Haskell\u6709\u9650\u6620\u5c04\u7406\u8bba\u7684\u539f\u578b\u5b9e\u73b0\uff0c\u652f\u6301\u4e86\u6848\u4f8b\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u6280\u672f\u7684\u53ef\u884c\u6027", "conclusion": "\u7cbe\u7ec6\u7c7b\u578b\u548cSMT\u6c42\u89e3\u5668\u80fd\u591f\u4f7f\u666e\u901a\u7f16\u7a0b\u66f4\u7b80\u5355\u3001\u66f4\u6109\u5feb\uff0c\u4e3a\u65e5\u5e38\u7f16\u7a0b\u5e26\u6765\u65b0\u7684\u53ef\u80fd\u6027"}}
{"id": "2509.15182", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.15182", "abs": "https://arxiv.org/abs/2509.15182", "authors": ["Muhammad Ahmed Mohsin", "Ahsan Bilal", "Muhammad Umer", "Asad Aali", "Muhammad Ali Jamshed", "Dean F. Hougen", "John M. Cioffi"], "title": "Conditional Prior-based Non-stationary Channel Estimation Using Accelerated Diffusion Models", "comment": "ICASSP 2026", "summary": "Wireless channels in motion-rich urban microcell (UMi) settings are\nnon-stationary; mobility and scatterer dynamics shift the distribution over\ntime, degrading classical and deep estimators. This work proposes conditional\nprior diffusion for channel estimation, which learns a history-conditioned\nscore to denoise noisy channel snapshots. A temporal encoder with cross-time\nattention compresses a short observation window into a context vector, which\ncaptures the channel's instantaneous coherence and steers the denoiser via\nfeature-wise modulation. In inference, an SNR-matched initialization selects\nthe diffusion step whose marginal aligns with the measured input SNR, and the\nprocess follows a shortened, geometrically spaced schedule, preserving the\nsignal-to-noise trajectory with far fewer iterations. Temporal\nself-conditioning with the previous channel estimate and a training-only\nsmoothness penalty further stabilizes evolution without biasing the test-time\nestimator. Evaluations on a 3GPP benchmark show lower NMSE across all SNRs than\nLMMSE, GMM, LSTM, and LDAMP baselines, demonstrating stable performance and\nstrong high SNR fidelity.", "AI": {"tldr": "\u63d0\u51fa\u6761\u4ef6\u5148\u9a8c\u6269\u6563\u65b9\u6cd5\u7528\u4e8e\u975e\u5e73\u7a33\u65e0\u7ebf\u4fe1\u9053\u4f30\u8ba1\uff0c\u901a\u8fc7\u5386\u53f2\u6761\u4ef6\u5316\u5206\u6570\u5b66\u4e60\u6765\u964d\u566a\uff0c\u57283GPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u79fb\u52a8\u4e30\u5bcc\u7684\u57ce\u5e02\u5fae\u8702\u7a9d\u73af\u5883\u4e2d\u65e0\u7ebf\u4fe1\u9053\u5177\u6709\u975e\u5e73\u7a33\u7279\u6027\uff0c\u4f20\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u4f30\u8ba1\u5668\u6027\u80fd\u4f1a\u56e0\u4fe1\u9053\u5206\u5e03\u968f\u65f6\u95f4\u53d8\u5316\u800c\u4e0b\u964d", "method": "\u4f7f\u7528\u5e26\u8de8\u65f6\u95f4\u6ce8\u610f\u529b\u7684\u65f6\u5e8f\u7f16\u7801\u5668\u538b\u7f29\u89c2\u6d4b\u7a97\u53e3\u4e3a\u4e0a\u4e0b\u6587\u5411\u91cf\uff0c\u901a\u8fc7\u7279\u5f81\u8c03\u5236\u5f15\u5bfc\u53bb\u566a\u5668\uff1b\u91c7\u7528SNR\u5339\u914d\u521d\u59cb\u5316\u548c\u51e0\u4f55\u95f4\u9694\u7684\u7f29\u77ed\u91c7\u6837\u8ba1\u5212", "result": "\u57283GPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u6709SNR\u4e0b\u5747\u83b7\u5f97\u6bd4LMMSE\u3001GMM\u3001LSTM\u548cLDAMP\u57fa\u7ebf\u66f4\u4f4e\u7684NMSE\uff0c\u8868\u73b0\u51fa\u7a33\u5b9a\u6027\u80fd\u548c\u9ad8SNR\u4fdd\u771f\u5ea6", "conclusion": "\u6761\u4ef6\u5148\u9a8c\u6269\u6563\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u975e\u5e73\u7a33\u4fe1\u9053\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u65f6\u5e8f\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u4f18\u5316\u7684\u6269\u6563\u8fc7\u7a0b\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd"}}
{"id": "2509.15205", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.15205", "abs": "https://arxiv.org/abs/2509.15205", "authors": ["Kartik Prabhu", "Jeffrey Yu", "Xinyuan Allen Pan", "Zhouhua Xie", "Abigail Aleshire", "Zihan Chen", "Ammar Ali Ratnani", "Priyanka Raina"], "title": "Voyager: An End-to-End Framework for Design-Space Exploration and Generation of DNN Accelerators", "comment": null, "summary": "While deep neural networks (DNNs) have achieved state-of-the-art performance\nin fields from computer vision to natural language processing, efficiently\nrunning these computationally demanding models requires hardware accelerators.\nHowever, designing these accelerators is a time-consuming, labor-intensive\nprocess that does not scale well. While prior efforts have sought to automate\nDNN accelerator generation, they offer limited parameterization, cannot produce\nhigh-performance, tapeout-ready designs, provide limited support for datatypes\nand quantization schemes, and lack an integrated, end-to-end software compiler.\nThis work proposes Voyager, a high-level synthesis (HLS)-based framework for\ndesign space exploration (DSE) and generation of DNN accelerators. Voyager\novercomes the limitations of prior work by offering extensive configurability\nacross technology nodes, clock frequencies, and scales, with customizable\nparameters such as number of processing elements, on-chip buffer sizes, and\nexternal memory bandwidth. Voyager supports a wider variety of datatypes and\nquantization schemes versus prior work, including both built-in floating-point,\nposit and integer formats, as well as user-defined formats with both per-tensor\nscaling and microscaling quantization. Voyager's PyTorch-based compiler\nefficiently maps networks end-to-end on the generated hardware, with support\nfor quantization, fusion, and tiling. We evaluate Voyager on state-of-the-art\nvision and language models. Voyager enables fast DSE with full-dataset accuracy\nevaluation for datatypes and quantization schemes. Generated designs achieve a\nhigh utilization across models and scales, up to 99.8%, and outperform prior\ngenerators with up to 61% lower latency and 56% lower area. Compared to\nhand-optimized accelerators, Voyager achieves comparable performance, while\noffering much greater automation in design and workload mapping.", "AI": {"tldr": "Voyager\u662f\u4e00\u4e2a\u57fa\u4e8eHLS\u7684DNN\u52a0\u901f\u5668\u81ea\u52a8\u751f\u6210\u6846\u67b6\uff0c\u63d0\u4f9b\u9ad8\u5ea6\u53ef\u914d\u7f6e\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u652f\u6301\u591a\u79cd\u6570\u636e\u7c7b\u578b\u548c\u91cf\u5316\u65b9\u6848\uff0c\u5e76\u80fd\u751f\u6210\u9ad8\u6027\u80fd\u7684tapeout-ready\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edfDNN\u52a0\u901f\u5668\u8bbe\u8ba1\u8fc7\u7a0b\u8017\u65f6\u8d39\u529b\u4e14\u6269\u5c55\u6027\u5dee\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u5de5\u5177\u5b58\u5728\u53c2\u6570\u5316\u6709\u9650\u3001\u4e0d\u652f\u6301\u9ad8\u6027\u80fd\u8bbe\u8ba1\u3001\u6570\u636e\u7c7b\u578b\u652f\u6301\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u9ad8\u5c42\u6b21\u7efc\u5408(HLS)\u6846\u67b6\uff0c\u63d0\u4f9b\u53ef\u914d\u7f6e\u7684\u6280\u672f\u8282\u70b9\u3001\u65f6\u949f\u9891\u7387\u3001\u5904\u7406\u5355\u5143\u6570\u91cf\u3001\u7247\u4e0a\u7f13\u5b58\u5927\u5c0f\u7b49\u53c2\u6570\uff0c\u652f\u6301\u6d6e\u70b9\u3001posit\u548c\u6574\u6570\u7b49\u591a\u79cd\u6570\u636e\u7c7b\u578b\u53ca\u91cf\u5316\u65b9\u6848\uff0c\u96c6\u6210PyTorch\u7f16\u8bd1\u5668\u8fdb\u884c\u7aef\u5230\u7aef\u7f51\u7edc\u6620\u5c04\u3002", "result": "\u5728\u5148\u8fdb\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0c\u8bbe\u8ba1\u5229\u7528\u7387\u9ad8\u8fbe99.8%\uff0c\u76f8\u6bd4\u73b0\u6709\u751f\u6210\u5668\u5ef6\u8fdf\u964d\u4f4e61%\u3001\u9762\u79ef\u51cf\u5c1156%\uff0c\u6027\u80fd\u4e0e\u624b\u5de5\u4f18\u5316\u52a0\u901f\u5668\u76f8\u5f53\u4f46\u81ea\u52a8\u5316\u7a0b\u5ea6\u66f4\u9ad8\u3002", "conclusion": "Voyager\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684DNN\u52a0\u901f\u5668\u8bbe\u8ba1\u4e0e\u6620\u5c04\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u786c\u4ef6\u52a0\u901f\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.15192", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.15192", "abs": "https://arxiv.org/abs/2509.15192", "authors": ["Muhammad Ahmed Mohsin", "Muhammad Umer", "Ahsan Bilal", "Muhammad Ibtsaam Qadir", "Muhammad Ali Jamshed", "Dean F. Hougen", "John M. Cioffi"], "title": "Channel Prediction under Network Distribution Shift Using Continual Learning-based Loss Regularization", "comment": "ICASSP 2026", "summary": "Modern wireless networks face critical challenges when mobile users traverse\nheterogeneous network configurations with varying antenna layouts, carrier\nfrequencies, and scattering statistics. Traditional predictors degrade under\ndistribution shift, with NMSE rising by 37.5\\% during cross-configuration\nhandovers. This work addresses catastrophic forgetting in channel prediction by\nproposing a continual learning framework based on loss regularization. The\napproach augments standard training objectives with penalty terms that\nselectively preserve network parameters essential for previous configurations\nwhile enabling adaptation to new environments. Two prominent regularization\nstrategies are investigated: Elastic Weight Consolidation (EWC) and Synaptic\nIntelligence (SI). Across 3GPP scenarios and multiple architectures, SI lowers\nthe high-SNR NMSE floor by up to 1.8 dB ($\\approx$32--34\\%), while EWC achieves\nup to 1.4 dB ($\\approx$17--28\\%). Notably, standard EWC incurs\n$\\mathcal{O}(MK)$ complexity (storing $M$ Fisher diagonal entries and\ncorresponding parameter snapshots across $K$ tasks) unless consolidated,\nwhereas SI maintains $\\mathcal{O}(M)$ memory complexity (storing $M$ model\nparameters), independent of task sequence length, making it suitable for\nresource-constrained wireless infrastructure", "AI": {"tldr": "\u901a\u8fc7\u63a7\u5236\u635f\u5931\u6b63\u5219\u5316\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u5f02\u6784\u7f51\u7edc\u4e2d\u9891\u9053\u9884\u6d4b\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0cSI\u65b9\u6cd5\u5728\u8d44\u6e90\u7ea6\u675f\u73af\u5883\u4e0b\u8868\u73b0\u66f4\u4f18", "motivation": "\u73b0\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u79fb\u52a8\u7528\u6237\u7a7f\u8d8a\u5f02\u6784\u7f51\u7edc\u914d\u7f6e\u65f6\u9047\u5230\u9891\u9053\u9884\u6d4b\u6027\u80fd\u6f0f\u6d1e\uff0c\u4f20\u7edf\u9884\u6d4b\u5668\u5728\u5206\u5e03\u504f\u79fb\u4e0bNMSE\u4e0a\u534737.5%", "method": "\u63d0\u51fa\u57fa\u4e8e\u635f\u5931\u6b63\u5219\u5316\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6dfb\u52a0\u60e9\u7f5a\u9879\u9009\u62e9\u6027\u4fdd\u4fdd\u5173\u952e\u7f51\u7edc\u53c2\u6570\uff0c\u7814\u7a76\u4e86EWC\u548cSI\u4e24\u79cd\u6b63\u5219\u5316\u7b56\u7565", "result": "\u57283GPP\u573a\u666f\u4e0b\uff0cSI\u5c06\u9ad8SNR\u7684NMSE\u5e95\u5668\u964d\u4f4e1.8dB(\u7ea632-34%)\uff0cEWC\u964d\u4f4e1.4dB(\u7ea617-28%)\uff0cSI\u5177\u6709\u66f4\u4f18\u7684\u5185\u5b58\u6548\u7387", "conclusion": "\u8be5\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u7f51\u7edc\u73af\u5883\u4e0b\u7684\u9891\u9053\u9884\u6d4b\u95ee\u9898\uff0cSI\u65b9\u6cd5\u5728\u8d44\u6e90\u7ea6\u675f\u7684\u65e0\u7ebf\u7ecf\u57fa\u7840\u8bbe\u65bd\u4e2d\u5177\u6709\u66f4\u5f3a\u7684\u5b9e\u7528\u6027"}}
