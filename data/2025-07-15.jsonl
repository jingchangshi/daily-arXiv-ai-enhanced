{"id": "2507.08923", "categories": ["cs.AR", "cs.CY", "cs.PF", "B.8.2; C.0; C.1.4; C.4; C.5.5; J.4; K.1; K.4.1; K.6.4"], "pdf": "https://arxiv.org/pdf/2507.08923", "abs": "https://arxiv.org/abs/2507.08923", "authors": ["Rubén Rodríguez Álvarez", "Denisa-Andreea Constantinescu", "Miguel Peón-Quirós", "David Atienza"], "title": "CEO-DC: An Actionable Framework to Close the Carbon Gap in HPC Data Centers", "comment": "15 pages, 11 figures, 2 tables", "summary": "The rapid expansion of data centers (DCs) to support large-scale AI and\nscientific workloads is driving unsustainable growth in energy consumption and\ngreenhouse gas emissions. While successive generations of hardware platforms\nhave improved performance and energy efficiency, the question remains whether\nnew, more efficient platforms can realistically offset the rising emissions\nassociated with increasing demand. Prior studies often overlook the complex\ntrade-offs in such transitions by failing to account for both the economic\nincentives and the projected compute demand growth over the operational\nlifetime of the devices. In response, we present CEO-DC, an integrated model\nand decision-making methodology for Carbon and Economy Optimization in Data\nCenters. CEO-DC models the competing forces of cost, carbon, and compute demand\nto guide optimal platform procurement and replacement strategies. We propose\nmetrics to steer procurement, platform design, and policy decisions toward\nsustainable DC technologies. Given current platform trends, our AI case study\nusing CEO-DC shows that upgrading legacy devices on a 4-year cycle reduces\ntotal emissions. However, these upgrades fail to scale with DC demand growth\ntrends without increasing total emissions in over 44% of cases, and require\neconomic incentives for adoption in over 72%. Furthermore, current carbon\nprices are insufficient to motivate upgrades in 9 out of the 14 countries with\nthe highest number of DCs globally. We also find that optimizing platforms for\nenergy efficiency at the expense of latency can increase the carbon price\nrequired to justify their adoption. In summary, CEO-DC provides actionable\ninsights for DC architects, platform designers, and policymakers by timing\nlegacy platform upgrades, constraining DC growth to sustainable levels,\noptimizing platform performance-to-cost ratios, and increasing incentives."}
{"id": "2507.09010", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09010", "abs": "https://arxiv.org/abs/2507.09010", "authors": ["Chun-Ting Chen", "HanGyeol Mun", "Jian Meng", "Mohamed S. Abdelfattah", "Jae-sun Seo"], "title": "Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference", "comment": "Accepted as a conference paper at the 2025 IEEE/ACM International\n  Symposium on Low Power Electronics and Design (ISLPED)", "summary": "Edge inference for large language models (LLM) offers secure, low-latency,\nand cost-effective inference solutions. We emphasize that an edge accelerator\nshould achieve high area efficiency and minimize external memory access (EMA)\nduring the memory-bound decode stage, while maintaining high energy efficiency\nduring the compute intensive prefill stage. This paper proposes an edge LLM\ninference accelerator featuring a hybrid systolic array (HSA) architecture that\noptimizes inference efficiency in both stages. To further reduce EMA, we adopt\nMXINT4 weight quantization and propose an optimized dataflow tailored for HSA,\nensuring negligible dequantization overhead and achieving 100% hardware\nutilization with minimal accuracy loss under edge DRAM bandwidth constraints.\nFor non-linear operations, we incorporate optimized root mean square\nnormalization (RMSNorm) and rotary position embedding (RoPE) units, reducing\ntheir latency, area, and memory access overhead while enabling end-to-end\ninference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while\nrunning a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x\nimprovement over existing approaches, while maintaining superior energy\nefficiency in token generation."}
{"id": "2507.09201", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.09201", "abs": "https://arxiv.org/abs/2507.09201", "authors": ["Weihong Xu", "Haein Choi", "Po-kai Hsu", "Shimeng Yu", "Tajana Rosing"], "title": "SLIM: A Heterogeneous Accelerator for Edge Inference of Sparse Large Language Model via Adaptive Thresholding", "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional proficiency in\nunderstanding and generating human language, but efficient inference on\nresource-constrained embedded devices remains challenging due to large model\nsizes and memory-intensive operations in feedforward network (FFN) and\nmulti-head attention (MHA) layers. While existing accelerators offload LLM\ninference to expensive heterogeneous computing systems, they fail to exploit\nthe significant sparsity inherent in LLM operations, leaving hardware resources\nunderutilized. We propose SLIM, an algorithm-hardware co-design optimized for\nsparse LLM serving on edge devices. SLIM exploits LLM sparsity through an\nadaptive thresholding algorithm that enables runtime-configurable sparsity with\nnegligible accuracy loss, fetching only activated neurons to dramatically\nreduce data movement. Our heterogeneous hardware architecture strategically\ncombines near-storage processing (NSP) and processing-in-memory (PIM): FFN\nweights are stored in high-density 3D NAND and computed using NSP units, while\nmemory-intensive MHA operations are processed in PIM modules. This design\nsignificantly reduces memory footprint, data movement, and energy consumption.\nOur comprehensive evaluation demonstrates SLIM's effectiveness, achieving\n13-18x throughput improvements over SSD-GPU systems and 9-10x better energy\nefficiency over DRAM-GPU systems while maintaining low latency, making\ncost-effective LLM deployment viable for edge computing environments."}
{"id": "2507.09660", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.09660", "abs": "https://arxiv.org/abs/2507.09660", "authors": ["Shuvra S. Bhattacharyya", "Marilyn Wolf"], "title": "Tools and Methodologies for System-Level Design", "comment": "This is a preprint of a chapter to appear in the forthcoming volume\n  Handbook on Electronic Design Automation (third edition), published by Taylor\n  & Francis. The final version may differ", "summary": "System-level design, once the province of board designers, has now become a\ncentral concern for chip designers. Because chip design is a less forgiving\ndesign medium -- design cycles are longer and mistakes are harder to correct --\nsystem-on-chip designers need a more extensive tool suite than may be used by\nboard designers and a variety of tools and methodologies have been developed\nfor system-level design of systems-on-chips (SoCs). System-level design is less\namenable to synthesis than are logic or physical design. As a result,\nsystem-level tools concentrate on modeling, simulation, design space\nexploration, and design verification. The goal of modeling is to correctly\ncapture the system's operational semantics, which helps with both\nimplementation and verification. The study of models of computation provides a\nframework for the description of digital systems. Not only do we need to\nunderstand a particular style of computation, such as dataflow, but we also\nneed to understand how different models of computation can reliably communicate\nwith each other. Design space exploration tools, such as hardware/software\nco-design, develop candidate designs to understand trade-offs. Simulation can\nbe used not only to verify functional correctness but also to supply\nperformance and power/energy information for design analysis. This chapter\nemploys two applications -- video and neural networks -- as examples. Both are\nleading-edge applications that illustrate many important aspects of\nsystem-level design."}
{"id": "2507.09539", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.09539", "abs": "https://arxiv.org/abs/2507.09539", "authors": ["Anna Bolotina", "Christoph M. Kirsch", "Stefanie Muroya Lei", "Matthias Pleschinger"], "title": "Bounded Model Checking of RISC-V Machine Code with Context-Free-Language Ordered Binary Decision Diagrams", "comment": null, "summary": "Symbolic execution is a powerful technique for analyzing the behavior of\nsoftware yet scalability remains a challenge due to state explosion in control\nand data flow. Existing tools typically aim at managing control flow\ninternally, often at the expense of completeness, while offloading reasoning\nover data flow to SMT solvers. Moreover, reasoning typically happens on source\ncode or intermediate representation level to leverage structural information,\nmaking machine code generation part of the trust base. We are interested in\nchanging the equation in two non-trivial ways: pushing reasoning down to\nmachine code level, and then offloading reasoning entirely into SMT solvers and\nother, possibly more efficient solver technology. In more abstract terms, we\nare asking if bit-precise reasoning technology can be made scalable on\nsoftware, and not just hardware. For this purpose, we developed two tools\ncalled rotor and bitme for model generation and bounded model checking,\nrespectively. We chose RISC-V restricted to integer arithmetic as modeling\ntarget for rotor since RISC-V integer semantics is essentially equivalent to\nestablished SMT semantics over bitvectors and arrays of bitvectors. While\nstate-of-the-art SMT solvers struggle in our experiments, we have evidence that\nthere is potential for improvement. To show the potential, we have slightly\ngeneralized and then implemented in bitme two types of binary decision diagrams\n(BDDs): algebraic decision diagrams (ADDs) and context-free-language ordered\nbinary decision diagrams (CFLOBDDs). Bitme uses BDDs to propagate program input\nthrough models, essentially generalizing constant propagation to domain\npropagation. SMT solvers only get involved when model input cannot be\npropagated, significanly speeding up SMT solving. We then study the impact on\nstate explosion of CFLOBDDs, which are potentially more scalable than ADDs."}
{"id": "2507.08954", "categories": ["cs.DC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.08954", "abs": "https://arxiv.org/abs/2507.08954", "authors": ["Alexander Fuerst", "Siddharth Anil", "Vishakha Dixit", "Purushottam", "Kulkarni", "Prateek Sharma"], "title": "MQFQ-Sticky: Fair Queueing For Serverless GPU Functions", "comment": null, "summary": "Hardware accelerators like GPUs are now ubiquitous in data centers, but are\nnot fully supported by common cloud abstractions such as Functions as a Service\n(FaaS). Many popular and emerging FaaS applications such as machine learning\nand scientific computing can benefit from GPU acceleration. However, FaaS\nframeworks (such as OpenWhisk) are not capable of providing this acceleration\nbecause of the impedance mismatch between GPUs and the FaaS programming model,\nwhich requires virtualization and sandboxing of each function. The challenges\nare amplified due to the highly dynamic and heterogeneous FaaS workloads. This\npaper presents the design and implementation of a FaaS system for providing GPU\nacceleration in a black-box manner (without modifying function code). Running\nsmall functions in containerized sandboxes is challenging due to limited GPU\nconcurrency and high cold-start overheads, resulting in heavy queueing of\nfunction invocations. We show how principles from I/O scheduling, such as fair\nqueuing and anticipatory scheduling, can be translated to function scheduling\non GPUs. We develop MQFQ-Sticky, an integrated fair queueing and GPU memory\nmanagement approach, which balances the tradeoffs between locality, fairness,\nand latency. Empirical evaluation on a range of workloads shows that it reduces\nfunction latency by 2x to 20x compared to existing GPU and CPU queueing\npolicies."}
{"id": "2507.09730", "categories": ["cs.AR", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.09730", "abs": "https://arxiv.org/abs/2507.09730", "authors": ["Jiechen Huang", "Wenjian Yu"], "title": "Efficient FRW Transitions via Stochastic Finite Differences for Handling Non-Stratified Dielectrics", "comment": "5 pages, 6 figures", "summary": "The accuracy of floating-random-walk (FRW) based capacitance extraction\nstands only when the recursive FRW transitions are sampled unbiasedly according\nto surrounding dielectrics. Advanced technology profiles, featuring complicated\nnon-stratified dielectrics, challenge the accuracy of existing FRW transition\nschemes that approximate dielectrics with stratified or eight-octant patterns.\nIn this work, we propose an algorithm named MicroWalk, enabling accurate FRW\ntransitions for arbitrary dielectrics while keeping high efficiency. It is\nprovably unbiased and equivalent to using transition probabilities solved by\nfinite difference method, but at orders of magnitude lower cost (802$\\times$\nfaster). An enhanced 3-D capacitance solver is developed with a hybrid strategy\nfor complicated dielectrics, combining MicroWalk with the special treatment for\nthe first transition cube and the analytical algorithm for stratified cubes.\nExperiments on real-world structures show that our solver achieves a\nsignificant accuracy advantage over existing FRW solvers, while preserving high\nefficiency."}
{"id": "2507.09883", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.09883", "abs": "https://arxiv.org/abs/2507.09883", "authors": ["Swarn Priya", "Frédéric Besson", "Connor Sughrue", "Tim Steenvoorden", "Jamie Fulford", "Freek Verbeek", "Binoy Ravindran"], "title": "BeePL: Correct-by-compilation kernel extensions", "comment": "45 pages, 18 figures", "summary": "eBPF is a technology that allows developers to safely extend kernel\nfunctionality without modifying kernel source code or developing loadable\nkernel modules. Since the kernel governs critical system operations and\nenforces isolation boundaries between user space and privileged data, any\nmechanism that modifies its behavior must meet the highest standards of safety\nand correctness. To this end, the eBPF toolchain includes a verifier, which\nstatically checks safety properties such as memory access validity, bounded\nloops, and type correctness before loading the program into the kernel.\nHowever, the existing verifier is both overly conservative in some\ncases-rejecting valid programs-and unsound in others, permitting unsafe\nbehavior that violates the intended semantics of the kernel interface.\n  To address these challenges, we introduce BeePL, a domain-specific language\nfor eBPF with a formally verified type system. The BeePL type system, along\nwith the language design, statically enforces key safety properties such as\ntype-correct memory access, safe pointer usage, absence of unbounded loops, and\nstructured control flow. These guarantees are backed by formal type soundness\nproofs, ensuring that well-typed programs satisfy the safety invariants\nrequired by the eBPF execution environment. BeePL also proves that well-typed\nsource programs meet critical eBPF-specific properties related to memory\nsafety, termination, and control flow, enabling high-level reasoning prior to\ncompilation. For properties not fully enforceable statically-such as dynamic\nbounds and undefined behavior-BeePL inserts semantics-preserving runtime checks\nduring compilation. We develop a verified compilation strategy that extends\nCompCert to generate BPF bytecode from BeePL programs, establishing a\nprincipled foundation for an end-to-end verifiable toolchain for safe kernel\nextensions."}
{"id": "2507.09546", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09546", "abs": "https://arxiv.org/abs/2507.09546", "authors": ["Xiangwang Hou", "Jingjing Wang", "Jun Du", "Chunxiao Jiang", "Yong Ren", "Dusit Niyato"], "title": "Lightweight Federated Learning over Wireless Edge Networks", "comment": null, "summary": "With the exponential growth of smart devices connected to wireless networks,\ndata production is increasing rapidly, requiring machine learning (ML)\ntechniques to unlock its value. However, the centralized ML paradigm raises\nconcerns over communication overhead and privacy. Federated learning (FL)\noffers an alternative at the network edge, but practical deployment in wireless\nnetworks remains challenging. This paper proposes a lightweight FL (LTFL)\nframework integrating wireless transmission power control, model pruning, and\ngradient quantization. We derive a closed-form expression of the FL convergence\ngap, considering transmission error, model pruning error, and gradient\nquantization error. Based on these insights, we formulate an optimization\nproblem to minimize the convergence gap while meeting delay and energy\nconstraints. To solve the non-convex problem efficiently, we derive closed-form\nsolutions for the optimal model pruning ratio and gradient quantization level,\nand employ Bayesian optimization for transmission power control. Extensive\nexperiments on real-world datasets show that LTFL outperforms state-of-the-art\nschemes."}
{"id": "2507.09774", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.09774", "abs": "https://arxiv.org/abs/2507.09774", "authors": ["MD Zobaer Hossain Bhuiyan", "Abir Bin Faruque", "Mahtab Newaz", "Mohammad Abdul Qayum"], "title": "Low-Cost Fuel Dispenser Prototype Using STM32 and an H-bridge motor driver", "comment": null, "summary": "This paper presents the design and development of a low-cost fuel dispensing\nsystem prototype based on the STM32 microcontroller and L298N motor driver. The\nsystem aims to provide an affordable and scalable solution for fuel delivery in\nremote or small-scale environments where conventional, high-cost systems are\nnot feasible. The core control unit is built using an STM32 microcontroller,\nwhich manages user input through a 4x4 matrix keypad and displays operational\ndata on a 16x4 LCD screen via I2C communication. A 12V DC pump motor is used to\nsimulate the fuel dispensing mechanism, precisely controlled via the dual\nH-bridge L298N motor driver. The system is powered by a 11.1V battery and is\ndesigned for ease of deployment and portability. The keypad allows users to\ninput the desired fuel amount, while the system ensures accurate motor runtime\ncorresponding to the volume to be dispensed. This project demonstrates how\nembedded systems can be leveraged to build cost-effective, user-friendly, and\nenergy-efficient solutions. The proposed design can be further enhanced with\nflow sensors, GSM connectivity, RFID cards, and payment integration for\nreal-world applications in fuel stations or agricultural use."}
{"id": "2507.10301", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.10301", "abs": "https://arxiv.org/abs/2507.10301", "authors": ["Wenhao Tang", "Sam Lindley"], "title": "Rows and Capabilities as Modal Effects", "comment": null, "summary": "Effect handlers allow programmers to model and compose computational effects\nmodularly. Effect systems statically guarantee that all effects are handled.\nSeveral recent practical effect systems are based on either row polymorphism or\ncapabilities. However, there remains a gap in understanding the precise\nrelationship between effect systems with such disparate foundations. The main\ndifficulty is that in both row-based and capability-based systems, effect\ntracking is typically entangled with other features such as functions.\n  We propose a uniform framework for encoding, analysing, and comparing effect\nsystems. Our framework exploits and generalises modal effect types, a recent\nnovel effect system which decouples effect tracking from functions via\nmodalities. Modalities offer fine-grained control over when and how effects are\ntracked, enabling us to express different strategies for effect tracking. We\ngive encodings as macro translations from existing row-based and\ncapability-based effect systems into our framework and show that these\nencodings preserve types and semantics. Our encodings reveal the essence of\neffect tracking mechanisms in different effect systems, enable a direct\nanalysis on their differences, and provide valuable insights on language\ndesign."}
{"id": "2507.09926", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.09926", "abs": "https://arxiv.org/abs/2507.09926", "authors": ["Zixuan Song", "Zhishu Shen", "Xiaoyu Zheng", "Qiushi Zheng", "Zheng Lei", "Jiong Jin"], "title": "Intelligent Task Management via Dynamic Multi-region Division in LEO Satellite Networks", "comment": null, "summary": "As a key complement to terrestrial networks and a fundamental component of\nfuture 6G systems, Low Earth Orbit (LEO) satellite networks are expected to\nprovide high-quality communication services when integrated with ground-based\ninfrastructure, thereby attracting significant research interest. However, the\nlimited satellite onboard resources and the uneven distribution of\ncomputational workloads often result in congestion along inter-satellite links\n(ISLs) that degrades task processing efficiency. Effectively managing the\ndynamic and large-scale topology of LEO networks to ensure balanced task\ndistribution remains a critical challenge. To this end, we propose a dynamic\nmulti-region division framework for intelligent task management in LEO\nsatellite networks. This framework optimizes both intra- and inter-region\nrouting to minimize task delay while balancing the utilization of computational\nand communication resources. Based on this framework, we propose a dynamic\nmulti-region division algorithm based on the Genetic Algorithm (GA), which\nadaptively adjusts the size of each region based on the workload status of\nindividual satellites. Additionally, we incorporate an adaptive routing\nalgorithm and a task splitting and offloading scheme based on Multi-Agent Deep\nDeterministic Policy Gradient (MA-DDPG) to effectively accommodate the arriving\ntasks. Simulation results demonstrate that our proposed framework outperforms\ncomparative methods in terms of the task delay, energy consumption per task,\nand task completion rate."}
{"id": "2507.09780", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09780", "abs": "https://arxiv.org/abs/2507.09780", "authors": ["Feilong Qiaoyuan", "Jihe Wang", "Zhiyu Sun", "Linying Wu", "Yuanhua Xiao", "Danghui Wang"], "title": "BitParticle: Partializing Sparse Dual-Factors to Build Quasi-Synchronizing MAC Arrays for Energy-efficient DNNs", "comment": "9 pages, 13 figures, 3 Tables", "summary": "Bit-level sparsity in quantized deep neural networks (DNNs) offers\nsignificant potential for optimizing Multiply-Accumulate (MAC) operations.\nHowever, two key challenges still limit its practical exploitation. First,\nconventional bit-serial approaches cannot simultaneously leverage the sparsity\nof both factors, leading to a complete waste of one factor' s sparsity. Methods\ndesigned to exploit dual-factor sparsity are still in the early stages of\nexploration, facing the challenge of partial product explosion. Second, the\nfluctuation of bit-level sparsity leads to variable cycle counts for MAC\noperations. Existing synchronous scheduling schemes that are suitable for\ndual-factor sparsity exhibit poor flexibility and still result in significant\nunderutilization of MAC units. To address the first challenge, this study\nproposes a MAC unit that leverages dual-factor sparsity through the emerging\nparticlization-based approach. The proposed design addresses the issue of\npartial product explosion through simple control logic, resulting in a more\narea- and energy-efficient MAC unit. In addition, by discarding less\nsignificant intermediate results, the design allows for further hardware\nsimplification at the cost of minor accuracy loss. To address the second\nchallenge, a quasi-synchronous scheme is introduced that adds cycle-level\nelasticity to the MAC array, reducing pipeline stalls and thereby improving MAC\nunit utilization. Evaluation results show that the exact version of the\nproposed MAC array architecture achieves a 29.2% improvement in area efficiency\ncompared to the state-of-the-art bit-sparsity-driven architecture, while\nmaintaining comparable energy efficiency. The approximate variant further\nimproves energy efficiency by 7.5%, compared to the exact version. Index-Terms:\nDNN acceleration, Bit-level sparsity, MAC unit"}
{"id": "2507.10482", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.10482", "abs": "https://arxiv.org/abs/2507.10482", "authors": ["Simon Guilloud", "Viktor Kunčak"], "title": "Orthologic Type Systems", "comment": null, "summary": "We propose to use orthologic as the basis for designing type systems\nsupporting intersection, union, and negation types in the presence of subtyping\nassumptions. We show how to extend orthologic to support monotonic and\nantimonotonic functions, supporting the use of type constructors in such type\nsystems. We present a proof system for orthologic with function symbols,\nshowing that it admits partial cut elimination. Using these insights, we\npresent an $\\mathcal O(n^2(1+m))$ algorithm for deciding the subtyping relation\nunder $m$ assumptions. We also show $O(n^2)$ polynomial-time normalization\nalgorithm, allowing simplification of types to their minimal canonical form."}
{"id": "2507.10026", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.10026", "abs": "https://arxiv.org/abs/2507.10026", "authors": ["Zhifei Xu", "Zhiqing Tang", "Jiong Lou", "Zhi Yao", "Xuan Xie", "Tian Wang", "Yinglong Wang", "Weijia Jia"], "title": "EAT: QoS-Aware Edge-Collaborative AIGC Task Scheduling via Attention-Guided Diffusion Reinforcement Learning", "comment": null, "summary": "The growth of Artificial Intelligence (AI) and large language models has\nenabled the use of Generative AI (GenAI) in cloud data centers for diverse\nAI-Generated Content (AIGC) tasks. Models like Stable Diffusion introduce\nunavoidable delays and substantial resource overhead, which are unsuitable for\nusers at the network edge with high QoS demands. Deploying AIGC services on\nedge servers reduces transmission times but often leads to underutilized\nresources and fails to optimally balance inference latency and quality. To\naddress these issues, this paper introduces a QoS-aware\n\\underline{E}dge-collaborative \\underline{A}IGC \\underline{T}ask scheduling\n(EAT) algorithm. Specifically: 1) We segment AIGC tasks and schedule patches to\nvarious edge servers, formulating it as a gang scheduling problem that balances\ninference latency and quality while considering server heterogeneity, such as\ndiffering model distributions and cold start issues. 2) We propose a\nreinforcement learning-based EAT algorithm that uses an attention layer to\nextract load and task queue information from edge servers and employs a\ndiffusion-based policy network for scheduling, efficiently enabling model\nreuse. 3) We develop an AIGC task scheduling system that uses our EAT algorithm\nto divide tasks and distribute them across multiple edge servers for\nprocessing. Experimental results based on our system and large-scale\nsimulations show that our EAT algorithm can reduce inference latency by up to\n56\\% compared to baselines. We release our open-source code at\nhttps://github.com/zzf1955/EAT."}
{"id": "2507.10178", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10178", "abs": "https://arxiv.org/abs/2507.10178", "authors": ["Wonung Kim", "Yubin Lee", "Yoonsung Kim", "Jinwoo Hwang", "Seongryong Oh", "Jiyong Jung", "Aziz Huseynov", "Woong Gyu Park", "Chang Hyun Park", "Divya Mahajan", "Jongse Park"], "title": "Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving", "comment": null, "summary": "Transformers are the driving force behind today's Large Language Models\n(LLMs), serving as the foundation for their performance and versatility. Yet,\ntheir compute and memory costs grow with sequence length, posing scalability\nchallenges for long-context inferencing. In response, the algorithm community\nis exploring alternative architectures, such as state space models (SSMs),\nlinear attention, and recurrent neural networks (RNNs), which we refer to as\npost-transformers. This shift presents a key challenge: building a serving\nsystem that efficiently supports both transformer and post-transformer LLMs\nwithin a unified framework. To address this challenge, we analyze the\nperformance characteristics of transformer and post-transformer LLMs. Despite\ntheir algorithmic differences, both are fundamentally limited by memory\nbandwidth under batched inference due to attention in transformers and state\nupdates in post-transformers. Further analyses suggest two additional insights:\n(1) state update operations, unlike attention, incur high hardware cost, making\nper-bank PIM acceleration inefficient, and (2) different low-precision\narithmetic methods offer varying accuracy-area tradeoffs, while we identify\nMicrosoft's MX as the Pareto-optimal choice. Building on these insights, we\ndesign Pimba as an array of State-update Processing Units (SPUs), each shared\nbetween two banks to enable interleaved access to PIM. Each SPU includes a\nState-update Processing Engine (SPE) that comprises element-wise multipliers\nand adders using MX-based quantized arithmetic, enabling efficient execution of\nstate update and attention operations. Our evaluation shows that, compared to\nLLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x\nhigher token generation throughput, respectively."}
{"id": "2507.10069", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10069", "abs": "https://arxiv.org/abs/2507.10069", "authors": ["Zedong Liu", "Shenggan Cheng", "Guangming Tan", "Yang You", "Dingwen Tao"], "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism", "comment": null, "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."}
{"id": "2507.10139", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10139", "abs": "https://arxiv.org/abs/2507.10139", "authors": ["Filipe Miguel Gonçalves de Almeida", "CJ Carey", "Hendrik Fichtenberger", "Jonathan Halcrow", "Silvio Lattanzi", "André Linhares", "Tao Meng", "Ashkan Norouzi-Fard", "Nikos Parotsidis", "Bryan Perozzi", "David Simcha"], "title": "Large-Scale Graph Building in Dynamic Environments: Low Latency and High Quality", "comment": null, "summary": "Learning and constructing large-scale graphs has attracted attention in\nrecent decades, resulting in a rich literature that introduced various systems,\ntools, and algorithms. Grale is one of such tools that is designed for offline\nenvironments and is deployed in more than 50 different industrial settings at\nGoogle. Grale is widely applicable because of its ability to efficiently learn\nand construct a graph on datasets with multiple types of features. However, it\nis often the case that applications require the underlying data to evolve\ncontinuously and rapidly and the updated graph needs to be available with low\nlatency. Such setting make the use of Grale prohibitive. While there are\nApproximate Nearest Neighbor (ANN) systems that handle dynamic updates with low\nlatency, they are mostly limited to similarities over a single embedding.\n  In this work, we introduce a system that inherits the advantages and the\nquality of Grale, and maintains a graph construction in a dynamic setting with\ntens of milliseconds of latency per request. We call the system Dynamic Grale\nUsing ScaNN (Dynamic GUS). Our system has a wide range of applications with\nover 10 deployments at Google. One of the applications is in Android Security\nand Privacy, where Dynamic Grale Using ScaNN enables capturing harmful\napplications 4 times faster, before they can reach users."}
{"id": "2507.10150", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.10150", "abs": "https://arxiv.org/abs/2507.10150", "authors": ["Ruihao Gong", "Shihao Bai", "Siyu Wu", "Yunqian Fan", "Zaijun Wang", "Xiuhong Li", "Hailong Yang", "Xianglong Liu"], "title": "Past-Future Scheduler for LLM Serving under SLA Guarantees", "comment": "Accepted to ASPLOS 2025", "summary": "The exploration and application of Large Language Models (LLMs) is thriving.\nTo reduce deployment costs, continuous batching has become an essential feature\nin current service frameworks. The effectiveness of continuous batching relies\non an accurate estimate of the memory requirements of requests. However, due to\nthe diversity in request output lengths, existing frameworks tend to adopt\naggressive or conservative schedulers, which often result in significant\noverestimation or underestimation of memory consumption. Consequently, they\nsuffer from harmful request evictions or prolonged queuing times, failing to\nachieve satisfactory throughput under strict Service Level Agreement (SLA)\nguarantees (a.k.a. goodput), across various LLM application scenarios with\ndiffering input-output length distributions. To address this issue, we propose\na novel Past-Future scheduler that precisely estimates the peak memory\nresources required by the running batch via considering the historical\ndistribution of request output lengths and calculating memory occupancy at each\nfuture time point. It adapts to applications with all types of input-output\nlength distributions, balancing the trade-off between request queuing and\nharmful evictions, thereby consistently achieving better goodput. Furthermore,\nto validate the effectiveness of the proposed scheduler, we developed a\nhigh-performance LLM serving framework, LightLLM, that implements the\nPast-Future scheduler. Compared to existing aggressive or conservative\nschedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\\times$\nhigher goodput than other schedulers under heavy loads. LightLLM is open source\nto boost the research in such direction (https://github.com/ModelTC/lightllm)."}
{"id": "2507.10259", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.10259", "abs": "https://arxiv.org/abs/2507.10259", "authors": ["Chengze Du", "Zhiwei Yu", "Heng Xu", "Haojie Wang", "Bo liu", "Jialong Li"], "title": "Cross-Timeslot Optimization for Distributed GPU Inference Using Reinforcement Learning", "comment": "17 pages, 12 figures", "summary": "The rapid growth of large language model (LLM) services imposes increasing\ndemands on distributed GPU inference infrastructure. Most existing scheduling\nsystems rely on the current system state to make decisions, without considering\nhow task demand and resource availability evolve over time. This lack of\ntemporal awareness leads to inefficient GPU utilization, high task migration\noverhead, and poor system responsiveness under dynamic workloads. In this work,\nwe identify the fundamental limitations of these instantaneous-state-only\nscheduling approaches and propose Temporal Optimal Resource scheduling via\nTwo-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling\nframework that captures both long-term workload patterns and short-term\nexecution constraints. It adopts a two-layer design: a macro-level scheduler\nleverages reinforcement learning and optimal transport to coordinate\ninter-region task distribution, while a micro-level allocator refines\ntask-to-server assignments within each region to reduce latency and switching\ncosts. Experimental results across multiple network topologies show that TORTA\nreduces average inference response time by up to 15\\%, improves load balance by\napproximately 4-5\\%, and cuts total operational cost by 10-20\\% compared to\nstate-of-the-art baseline methods."}
{"id": "2507.10367", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.10367", "abs": "https://arxiv.org/abs/2507.10367", "authors": ["Jingwei Xu", "Junbin Kang", "Mingkai Dong", "Mingyu Liu", "Lu Zhang", "Shaohong Guo", "Ziyan Qiu", "Mingzhen You", "Ziyi Tian", "Anqi Yu", "Tianhong Ding", "Xinwei Hu", "Haibo Chen"], "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline", "comment": "Accepted by NSDI'26", "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."}
{"id": "2507.10392", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.10392", "abs": "https://arxiv.org/abs/2507.10392", "authors": ["Runsheng Benson Guo", "Utkarsh Anand", "Khuzaima Daudjee", "Rathijit Sen"], "title": "Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters", "comment": null, "summary": "Large language models (LLMs) require vast amounts of GPU compute to train,\nbut limited availability and high costs of GPUs make homogeneous clusters\nimpractical for many organizations. Instead, assembling heterogeneous clusters\nby pooling together GPUs of different generations allows them to achieve higher\naggregate compute and make use of all available GPUs. However, training on\nheterogeneous clusters presents several challenges, including load balancing\nacross GPUs, optimizing memory usage to accommodate varying memory capacities,\nand ensuring communication-efficient training over diverse network\ninterconnects potentially spanning multiple datacenters. In this paper, we make\nthe case that efficient training on heterogeneous clusters requires (1) the\nintegration of pipeline parallelism and data parallelism in a manner that is\nboth communication- and memory-efficient, and (2) a more adaptable\nconfiguration of pipeline and data parallelism, which includes the capability\nto flexibly partition GPUs into asymmetric pipeline parallel stages and to\nincorporate heterogeneous GPUs within the same data parallelism group. We\npropose Zorse, the first system to unify all these capabilities while\nincorporating a planner that automatically configures training strategies for a\ngiven workload. Our evaluation shows that Zorse significantly outperforms\nstate-of-the-art systems in heterogeneous training scenarios."}
{"id": "2507.10413", "categories": ["cs.DC", "cs.CC", "cs.IT", "cs.LO", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.10413", "abs": "https://arxiv.org/abs/2507.10413", "authors": ["Gabriel Rocha"], "title": "Consensus, Inconsistency, Emergence: what's paraconsistency got to do with it?", "comment": "10 pages", "summary": "The consensus problem, briefly stated, consists of having processes in an\nasynchronous distributed system agree on a value. It is widely known that the\nconsensus problem does not have a deterministic solution that ensures both\ntermination and consistency, if there is at least one faulty process in the\nsystem. This result, known as the FLP impossibility theorem, led to several\ngeneralizations and developments in theoretical distributed computing. This\npaper argues that the FLP impossibility theorem holds even under a generalized\ndefinition of computation through oracles. Furthermore, using a theoretical\nmachinery from complex systems, this paper also posits that inconsistency may\nbe an emergent feature of consensus over distributed systems by examining how a\nsystem transitions phases. Under the same complex systems framework, this paper\nexamines paraconsistent logics, arguing that while inconsistency is not an\nemergent feature for these logics, triviality may be. Lastly, some attention is\ngiven to the possibility of developing consensus algorithms capable of\nparaconsistent reasoning."}
{"id": "2507.10430", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10430", "abs": "https://arxiv.org/abs/2507.10430", "authors": ["Ji Liu", "Beichen Ma", "Yang Zhou", "Jingbo Zhou", "Ruoming Jin", "Dejing Dou", "Huaiyu Dai", "Haixun Wang", "Patrick Valduriez"], "title": "Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout", "comment": "29 pages, to appear in ACM Transactions on Knowledge Discovery from\n  Data (TKDD)", "summary": "Federated Learning (FL) is a promising distributed machine learning approach\nthat enables collaborative training of a global model using multiple edge\ndevices. The data distributed among the edge devices is highly heterogeneous.\nThus, FL faces the challenge of data distribution and heterogeneity, where\nnon-Independent and Identically Distributed (non-IID) data across edge devices\nmay yield in significant accuracy drop. Furthermore, the limited computation\nand communication capabilities of edge devices increase the likelihood of\nstragglers, thus leading to slow model convergence. In this paper, we propose\nthe FedDHAD FL framework, which comes with two novel methods: Dynamic\nHeterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH\ndynamically adjusts the weights of each local model within the model\naggregation process based on the non-IID degree of heterogeneous data to deal\nwith the statistical data heterogeneity. FedAD performs neuron-adaptive\noperations in response to heterogeneous devices to improve accuracy while\nachieving superb efficiency. The combination of these two methods makes FedDHAD\nsignificantly outperform state-of-the-art solutions in terms of accuracy (up to\n6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to\n15.0% smaller)."}
{"id": "2507.09201", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.09201", "abs": "https://arxiv.org/abs/2507.09201", "authors": ["Weihong Xu", "Haein Choi", "Po-kai Hsu", "Shimeng Yu", "Tajana Rosing"], "title": "SLIM: A Heterogeneous Accelerator for Edge Inference of Sparse Large Language Model via Adaptive Thresholding", "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional proficiency in\nunderstanding and generating human language, but efficient inference on\nresource-constrained embedded devices remains challenging due to large model\nsizes and memory-intensive operations in feedforward network (FFN) and\nmulti-head attention (MHA) layers. While existing accelerators offload LLM\ninference to expensive heterogeneous computing systems, they fail to exploit\nthe significant sparsity inherent in LLM operations, leaving hardware resources\nunderutilized. We propose SLIM, an algorithm-hardware co-design optimized for\nsparse LLM serving on edge devices. SLIM exploits LLM sparsity through an\nadaptive thresholding algorithm that enables runtime-configurable sparsity with\nnegligible accuracy loss, fetching only activated neurons to dramatically\nreduce data movement. Our heterogeneous hardware architecture strategically\ncombines near-storage processing (NSP) and processing-in-memory (PIM): FFN\nweights are stored in high-density 3D NAND and computed using NSP units, while\nmemory-intensive MHA operations are processed in PIM modules. This design\nsignificantly reduces memory footprint, data movement, and energy consumption.\nOur comprehensive evaluation demonstrates SLIM's effectiveness, achieving\n13-18x throughput improvements over SSD-GPU systems and 9-10x better energy\nefficiency over DRAM-GPU systems while maintaining low latency, making\ncost-effective LLM deployment viable for edge computing environments."}
