{"id": "2511.14953", "categories": ["cs.PL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14953", "abs": "https://arxiv.org/abs/2511.14953", "authors": ["Joey Velez-Ginorio", "Nada Amin", "Konrad Kording", "Steve Zdancewic"], "title": "Compiling to recurrent neurons", "comment": null, "summary": "Discrete structures are currently second-class in differentiable programming. Since functions over discrete structures lack overt derivatives, differentiable programs do not differentiate through them and limit where they can be used. For example, when programming a neural network, conditionals and iteration cannot be used everywhere; they can break the derivatives necessary for gradient-based learning to work. This limits the class of differentiable algorithms we can directly express, imposing restraints on how we build neural networks and differentiable programs more generally. However, these restraints are not fundamental. Recent work shows conditionals can be first-class, by compiling them into differentiable form as linear neurons. Similarly, this work shows iteration can be first-class -- by compiling to linear recurrent neurons. We present a minimal typed, higher-order and linear programming language with iteration called $\\textsf{Cajal}\\scriptstyle(\\mathbb{\\multimap}, \\mathbb{2}, \\mathbb{N})$. We prove its programs compile correctly to recurrent neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation, we conduct two experiments where we link these recurrent neurons against a neural network solving an iterative image transformation task. This determines part of its function prior to learning. As a result, the network learns faster and with greater data-efficiency relative to a neural network programmed without first-class iteration. A key lesson is that recurrent neurons enable a rich interplay between learning and the discrete structures of ordinary programming."}
{"id": "2511.15000", "categories": ["cs.PL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.15000", "abs": "https://arxiv.org/abs/2511.15000", "authors": ["Alexander J Root", "Christophe Gyurgyik", "Purvi Goel", "Kayvon Fatahalian", "Jonathan Ragan-Kelley", "Andrew Adams", "Fredrik Kjolstad"], "title": "Compiling Set Queries into Work-Efficient Tree Traversals", "comment": null, "summary": "Trees can accelerate queries that search or aggregate values over large collections. They achieve this by storing metadata that enables quick pruning (or inclusion) of subtrees when predicates on that metadata can prove that none (or all) of the data in a subtree affect the query result. Existing systems implement this pruning logic manually for each query predicate and data structure. We generalize and mechanize this class of optimization. Our method derives conditions for when subtrees can be pruned (or included wholesale), expressed in terms of the metadata available at each node. We efficiently generate these conditions using symbolic interval analysis, extended with new rules to handle geometric predicates (e.g., intersection, containment). Additionally, our compiler fuses compound queries (e.g., reductions on filters) into a single tree traversal. These techniques enable the automatic derivation of generalized single-index and dual-index tree joins that support a wide class of join predicates beyond standard equality and range predicates. The generated traversals match the behavior of expert-written code that implements query-specific traversals, and can asymptotically outperform the linear scans and nested-loop joins that existing systems fall back to when hand-written cases do not apply."}
{"id": "2511.15028", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.15028", "abs": "https://arxiv.org/abs/2511.15028", "authors": ["Christophe Gyurgyik", "Alexander J Root", "Fredrik Kjolstad"], "title": "Data Layout Polymorphism for Bounding Volume Hierarchies", "comment": null, "summary": "Bounding volume hierarchies are ubiquitous acceleration structures in graphics, scientific computing, and data analytics. Their performance depends critically on data layout choices that affect cache utilization, memory bandwidth, and vectorization -- increasingly dominant factors in modern computing. Yet, in most programming systems, these layout choices are hopelessly entangled with the traversal logic. This entanglement prevents developers from independently optimizing data layouts and algorithms across different contexts, perpetuating a false dichotomy between performance and portability. We introduce Scion, a domain-specific language and compiler for specifying the data layouts of bounding volume hierarchies independent of tree traversal algorithms. We show that Scion can express a broad spectrum of layout optimizations used in high performance computing while remaining architecture-agnostic. We demonstrate empirically that Pareto-optimal layouts (along performance and memory footprint axes) vary across algorithms, architectures, and workload characteristics. Through systematic design exploration, we also identify a novel ray tracing layout that combines optimization techniques from prior work, achieving Pareto-optimality across diverse architectures and scenes."}
{"id": "2511.15073", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.15073", "abs": "https://arxiv.org/abs/2511.15073", "authors": ["Youwei Xiao", "Zizhang Luo", "Weijie Peng", "Yuyang Zou", "Yun Liang"], "title": "Cement2: Temporal Hardware Transactions for High-Level and Efficient FPGA Programming", "comment": null, "summary": "Hardware design faces a fundamental challenge: raising abstraction to improve productivity while maintaining control over low-level details like cycle accuracy. Traditional RTL design in languages like SystemVerilog composes modules through wiring-style connections that provide weak guarantees for behavioral correctness. While high-level synthesis (HLS) and emerging abstractions attempt to address this, they either introduce unpredictable overhead or restrict design generality. Although transactional HDLs provide a promising foundation by lifting design abstraction to atomic and composable rules, they solely model intra-cycle behavior and do not reflect the native temporal design characteristics, hindering applicability and productivity for FPGA programming scenarios.\n  We propose temporal hardware transactions, a new abstraction that brings cycle-level timing awareness to designers at the transactional language level. Our approach models temporal relationships between rules and supports the description of rules whose actions span multiple clock cycles, providing intuitive abstraction to describe multi-cycle architectural behavior. We implement this in Cement2, a transactional HDL embedded in Rust, enabling programming hardware constructors to build both intra-cycle and temporal transactions. Cement2's synthesis framework lowers description abstraction through multiple analysis and optimization phases, generating efficient hardware. With Cement2's abstraction, we program a RISC-V soft-core processor, custom CPU instructions, linear algebra kernels, and systolic array accelerators, leveraging the high-level abstraction for boosted productivity. Evaluation shows that Cement2 does not sacrifice performance and resources compared to hand-coded RTL designs, demonstrating the high applicability for general FPGA design tasks."}
{"id": "2511.14990", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.14990", "abs": "https://arxiv.org/abs/2511.14990", "authors": ["Zhuolun Jiang", "Songyue Wang", "Xiaokun Pei", "Tianyue Lu", "Mingyu Chen"], "title": "CoroAMU: Unleashing Memory-Driven Coroutines through Latency-Aware Decoupled Operations", "comment": null, "summary": "Modern data-intensive applications face memory latency challenges exacerbated by disaggregated memory systems. Recent work shows that coroutines are promising in effectively interleaving tasks and hiding memory latency, but they struggle to balance latency-hiding efficiency with runtime overhead. We present CoroAMU, a hardware-software co-designed system for memory-centric coroutines. It introduces compiler procedures that optimize coroutine code generation, minimize context, and coalesce requests, paired with a simple interface. With hardware support of decoupled memory operations, we enhance the Asynchronous Memory Unit to further exploit dynamic coroutine schedulers by coroutine-specific memory operations and a novel memory-guided branch prediction mechanism. It is implemented with LLVM and open-source XiangShan RISC-V processor over the FPGA platform. Experiments demonstrate that the CoroAMU compiler achieves a 1.51x speedup over state-of-the-art coroutine methods on Intel server processors. When combined with optimized hardware of decoupled memory access, it delivers 3.39x and 4.87x average performance improvements over the baseline processor on FPGA-emulated disaggregated systems under 200ns and 800ns latency respectively."}
{"id": "2511.14852", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14852", "abs": "https://arxiv.org/abs/2511.14852", "authors": ["Mingkun Yu", "Heming Zhong", "Dan Huang", "Yutong Lu", "Jiazhi Jiang"], "title": "PolyKAN: Efficient Fused GPU Operators for Polynomial Kolmogorov-Arnold Network Variants", "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) promise higher expressive capability and stronger interpretability than Multi-Layer Perceptron, particularly in the domain of AI for Science. However, practical adoption has been hindered by low GPU utilization of existing parallel implementations. To address this challenge, we present a GPU-accelerated operator library, named PolyKAN which is the first general open-source implementation of KAN and its variants. PolyKAN fuses the forward and backward passes of polynomial KAN layers into a concise set of optimized CUDA kernels. Four orthogonal techniques underpin the design: (i) \\emph{lookup-table} with linear interpolation that replaces runtime expensive math-library functions; (ii) \\emph{2D tiling} to expose thread-level parallelism with preserving memory locality; (iii) a \\emph{two-stage reduction} scheme converting scattered atomic updates into a single controllable merge step; and (iv) \\emph{coefficient-layout reordering} yielding unit-stride reads under the tiled schedule. Using a KAN variant, Chebyshev KAN, as a case-study, PolyKAN delivers $1.2$--$10\\times$ faster inference and $1.4$--$12\\times$ faster training than a Triton + cuBLAS baseline, with identical accuracy on speech, audio-enhancement, and tabular-regression workloads on both highend GPU and consumer-grade GPU."}
{"id": "2511.15323", "categories": ["cs.PL", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.15323", "abs": "https://arxiv.org/abs/2511.15323", "authors": ["Youwei Xiao", "Yuyang Zou", "Yun Liang"], "title": "SkyEgg: Joint Implementation Selection and Scheduling for Hardware Synthesis using E-graphs", "comment": null, "summary": "Hardware synthesis from high-level descriptions remains fundamentally limited by the sequential optimization of interdependent design decisions. Current methodologies, including state-of-the-art high-level synthesis (HLS) tools, artificially separate implementation selection from scheduling, leading to suboptimal designs that cannot fully exploit modern FPGA heterogeneous architectures. Implementation selection is typically performed by ad-hoc pattern matching on operations, a process that does not consider the impact on scheduling. Subsequently, scheduling algorithms operate on fixed selection solutions with inaccurate delay estimates, which misses critical optimization opportunities from appropriately configured FPGA blocks like DSP slices.\n  We present SkyEgg, a novel hardware synthesis framework that jointly optimizes implementation selection and scheduling using the e-graph data structure. Our key insight is that both algebraic transformations and hardware implementation choices can be uniformly represented as rewrite rules within an e-graph, modeling the complete design space of implementation candidates to be selected and scheduled together. First, SkyEgg constructs an e-graph from the input program. It then applies both algebraic and implementation rewrites through equality saturation. Finally, it formulates the joint optimization as a mixed-integer linear programming (MILP) problem on the saturated e-graph. We provide both exact MILP solving and an efficient ASAP heuristic for scalable synthesis. Our evaluation on benchmarks from diverse applications targeting Xilinx Kintex UltraScale+ FPGAs demonstrates that SkyEgg achieves an average speedup of 3.01x over Vitis HLS, with improvements up to 5.22x for complex expressions."}
{"id": "2511.15367", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.15367", "abs": "https://arxiv.org/abs/2511.15367", "authors": ["Xin Yang", "Xin Fan", "Zengshi Wang", "Jun Han"], "title": "DARE: An Irregularity-Tolerant Matrix Processing Unit with a Densifying ISA and Filtered Runahead Execution", "comment": "8 pages, 9 figures, accepted to DATE 2026", "summary": "Deep Neural Networks (DNNs) are widely applied across domains and have shown strong effectiveness. As DNN workloads increasingly run on CPUs, dedicated Matrix Processing Units (MPUs) and Matrix Instruction Set Architectures (ISAs) have been introduced. At the same time, sparsity techniques are widely adopted in algorithms to reduce computational cost.\n  Despite these advances, insufficient hardware-algorithm co-optimization leads to suboptimal performance. On the memory side, sparse DNNs incur irregular access patterns that cause high cache miss rates. While runahead execution is a promising prefetching technique, its direct application to MPUs is often ineffective due to significant prefetch redundancy. On the compute side, stride constraints in current Matrix ISAs prevent the densification of multiple logically related sparse operations, resulting in poor utilization of MPU processing elements.\n  To address these irregularities, we propose DARE, an irregularity-tolerant MPU with a Densifying ISA and filtered Runahead Execution. DARE extends the ISA to support densifying sparse operations and equips a lightweight runahead mechanism with filtering capability. Experimental results show that DARE improves performance by 1.04$\\times$ to 4.44$\\times$ and increases energy efficiency by 1.00$\\times$ to 22.8$\\times$ over the baseline, with 3.91$\\times$ lower hardware overhead than NVR."}
{"id": "2511.14966", "categories": ["cs.DC", "cs.MS", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.14966", "abs": "https://arxiv.org/abs/2511.14966", "authors": ["David L. Cole", "Jordan Jalving", "Jonah Langlieb", "Jesse D. Jenkins"], "title": "A Graph-Based, Distributed Memory, Modeling Abstraction for Optimization", "comment": "32 pages, 7 Figures", "summary": "We present a general, flexible modeling abstraction for building and working with distributed optimization problems called a RemoteOptiGraph. This abstraction extends the OptiGraph model in Plasmo$.$jl, where optimization problems are represented as hypergraphs with nodes that define modular subproblems (variables, constraints, and objectives) and edges that encode algebraic linking constraints between nodes. The RemoteOptiGraph allows OptiGraphs to be utilized in distributed memory environments through InterWorkerEdges, which manage linking constraints that span workers. This abstraction offers a unified approach for modeling optimization problems on distributed memory systems (avoiding bespoke modeling approaches), and provides a basis for developing general-purpose meta-algorithms that can exploit distributed memory structure such as Benders or Lagrangian decompositions. We implement this abstraction in the open-source package, Plasmo$.$jl and we illustrate how it can be used by solving a mixed integer capacity expansion model for the western United States containing over 12 million variables and constraints. The RemoteOptiGraph abstraction together with Benders decomposition performs 7.5 times faster than solving the same problem without decomposition."}
{"id": "2511.15581", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.15581", "abs": "https://arxiv.org/abs/2511.15581", "authors": ["Kayo Tei", "Haruto Mishina", "Naoki Yamamoto", "Kazunori Ueda"], "title": "Graph Rewriting Language as a Platform for Quantum Diagrammatic Calculi", "comment": "27 pages, 27 figures. Extended version (with Appendices) of the paper to be presented at the 28th International Symposium on Practical Aspects of Declarative Languages (PADL 2026), January 2026", "summary": "Systematic discovery of optimization paths in quantum circuit simplification remains a challenge. Today, ZX-calculus, a computing model for quantum circuit transformation, is attracting attention for its highly abstract graph-based approach. Whereas existing tools such as PyZX and Quantomatic offer domain-specific support for quantum circuit optimization, visualization and theorem-proving, we present a complementary approach using LMNtal, a general-purpose hierarchical graph rewriting language, to establish a diagrammatic transformation and verification platform with model checking. Our methodology shows three advantages: (1) manipulation of ZX-diagrams through native graph transformation rules, enabling direct implementation of basic rules; (2) quantified pattern matching via QLMNtal extensions, greatly simplifying rule specification; and (3) interactive visualization and validation of optimization paths through state space exploration. Through case studies, we demonstrate how our framework helps understand optimization paths and design new algorithms and strategies. This suggests that the declarative language LMNtal and its toolchain could serve as a new platform to investigate quantum circuit transformation from a different perspective."}
{"id": "2511.15397", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.15397", "abs": "https://arxiv.org/abs/2511.15397", "authors": ["Cong Wang", "Zexin Fu", "Jiayi Huang", "Shanshi Huang"], "title": "Hemlet: A Heterogeneous Compute-in-Memory Chiplet Architecture for Vision Transformers with Group-Level Parallelism", "comment": null, "summary": "Vision Transformers (ViTs) have established new performance benchmarks in vision tasks such as image recognition and object detection. However, these advancements come with significant demands for memory and computational resources, presenting challenges for hardware deployment. Heterogeneous compute-in-memory (CIM) accelerators have emerged as a promising solution for enabling energy-efficient deployment of ViTs. Despite this potential, monolithic CIM-based designs face scalability issues due to the size limitations of a single chip. To address this challenge, emerging chiplet-based techniques offer a more scalable alternative. However, chiplet designs come with their own costs, as they introduce more expensive communication through the network-on-package (NoP) compared to the network-on-chip (NoC), which can hinder improvements in throughput.\n  This work introduces Hemlet, a heterogeneous CIM chiplet system designed to accelerate ViT. Hemlet facilitates flexible resource scaling through the integration of heterogeneous analog CIM (ACIM), digital CIM (DCIM), and Intermediate Data Process (IDP) chiplets. To improve throughput while reducing communication ove"}
{"id": "2511.15076", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15076", "abs": "https://arxiv.org/abs/2511.15076", "authors": ["Khaled Hamidouche", "John Bachan", "Pak Markthub", "Peter-Jan Gootzen", "Elena Agostini", "Sylvain Jeaugey", "Aamir Shafi", "Georgios Theodorakis", "Manjunath Gorentla Venkata"], "title": "GPU-Initiated Networking for NCCL", "comment": "13 pages, 9 figures, 3 tables", "summary": "Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead.\n  NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN's practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure."}
{"id": "2511.15503", "categories": ["cs.AR", "cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.15503", "abs": "https://arxiv.org/abs/2511.15503", "authors": ["Peiming Yang", "Sankeerth Durvasula", "Ivan Fernandez", "Mohammad Sadrosadati", "Onur Mutlu", "Gennady Pekhimenko", "Christina Giannoula"], "title": "A Tensor Compiler for Processing-In-Memory Architectures", "comment": null, "summary": "Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU."}
{"id": "2511.15361", "categories": ["cs.DC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.15361", "abs": "https://arxiv.org/abs/2511.15361", "authors": ["Preston Vander Vos", "Alberto Sonnino", "Giorgos Tsimos", "Philipp Jovanovic", "Lefteris Kokoris-Kogias"], "title": "BlueBottle: Fast and Robust Blockchains through Subsystem Specialization", "comment": null, "summary": "Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f<3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption."}
{"id": "2511.15505", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.15505", "abs": "https://arxiv.org/abs/2511.15505", "authors": ["Anastasios Petropoulos", "Theodore Antonakopoulos"], "title": "Instruction-Based Coordination of Heterogeneous Processing Units for Acceleration of DNN Inference", "comment": "Accepted at the 18th IEEE International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC-2025)", "summary": "This paper presents an instruction-based coordination architecture for Field-Programmable Gate Array (FPGA)-based systems with multiple high-performance Processing Units (PUs) for accelerating Deep Neural Network (DNN) inference. This architecture enables programmable multi-PU synchronization through instruction controller units coupled with peer-to-peer instruction synchronization units, utilizing instruction types organized into load, compute, and store functional groups. A compilation framework is presented that transforms DNN models into executable instruction programs, enabling flexible partitioning of DNN models into topologically contiguous subgraphs mapped to available PUs. Multiple deployment strategies are supported, enabling pipeline parallelism among PUs and batch-level parallelism across different PU subsets, with runtime switching among them without FPGA reconfiguration. The proposed approach enables design space exploration, supporting dynamic trade-offs between single-batch and multi-batch performance. Experimental results on ResNet-50 demonstrate notable compute efficiency, up to $98\\%$, and throughput efficiency gains, up to $2.7\\times$, over prior works across different configurations."}
{"id": "2511.15388", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.15388", "abs": "https://arxiv.org/abs/2511.15388", "authors": ["Lucianna Kiffer", "Lioba Heimbach", "Dennis Trautwein", "Yann Vonlanthen", "Oliver Gasser"], "title": "Multiple Sides of 36 Coins: Measuring Peer-to-Peer Infrastructure Across Cryptocurrencies", "comment": "In Proceedings of ACM SIGMETRICS 2026", "summary": "Blockchain technologies underpin an expanding ecosystem of decentralized applications, financial systems, and infrastructure. However, the fundamental networking layer that sustains these systems, the peer-to-peer layer, of all but the top few ecosystems remains largely opaque. In this paper, we present the first longitudinal, cross-network measurement study of 36 public blockchain networks. Over 9 months, we deployed 15 active crawlers, sourced data from two additional community crawlers, and conducted hourly connectivity probes to observe the evolving state of these networks. Furthermore, by leveraging Ethereum's discovery protocols, we inferred metadata for an additional 19 auxiliary networks that utilize the Ethereum peer discovery protocol. We also explored Internet-wide scans, which only require probing each protocol's default ports with a simple, network-specific payload. This approach allows us to rapidly identify responsive peers across the entire address space without having to implement custom discovery and handshake logic for every blockchain. We validated this method on Bitcoin and similar networks with known ground truth, then applied it to Cardano, which we could not crawl directly.\n  Our study uncovers dramatic variation in network size from under 10 to more than 10,000 active nodes. We quantify trends in IPv4 versus IPv6 usage, analyze autonomous systems and geographic concentration, and characterize churn, diurnal behavior, and the coverage and redundancy of discovery protocols. These findings expose critical differences in network resilience, decentralization, and observability. Beyond characterizing each network, our methodology demonstrates a general framework for measuring decentralized networks at scale. This opens the door for continued monitoring, benchmarking, and more transparent assessments of blockchain infrastructure across diverse ecosystems."}
{"id": "2511.15564", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.15564", "abs": "https://arxiv.org/abs/2511.15564", "authors": ["Paul Scheffler", "Thomas Benz", "Tim Fischer", "Lorenzo Leone", "Sina Arjmandpour", "Luca Benini"], "title": "Toward Open-Source Chiplets for HPC and AI: Occamy and Beyond", "comment": "8 pages, 8 figures, 1 table, submitted to 2026 IEEE CICC for possible publication", "summary": "We present a roadmap for open-source chiplet-based RISC-V systems targeting high-performance computing and artificial intelligence, aiming to close the performance gap to proprietary designs. Starting with Occamy, the first open, silicon-proven dual-chiplet RISC-V manycore in 12nm FinFET, we scale to Ramora, a mesh-NoC-based dual-chiplet system, and to Ogopogo, a 7nm quad-chiplet concept architecture achieving state-of-the-art compute density. Finally, we explore possible avenues to extend openness beyond logic-core RTL into simulation, EDA, PDKs, and off-die PHYs."}
{"id": "2511.15421", "categories": ["cs.DC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.15421", "abs": "https://arxiv.org/abs/2511.15421", "authors": ["Ethan Hicks", "Joseph Oglio", "Mikhail Nesterenko", "Gokarna Sharma"], "title": "When Can You Trust Bitcoin? Value-Dependent Block Confirmation to Determine Transaction Finalit", "comment": null, "summary": "We study financial transaction confirmation finality in Bitcoin as a function of transaction amount and user risk tolerance. A transaction is recorded in a block on a blockchain. However, a transaction may be revoked due to a fork in the blockchain, the odds of which decrease over time but never reach zero. Therefore, a transaction is considered confirmed if its block is sufficiently deep in the blockchain. This depth is usually set empirically at some fixed number such as six blocks. We analyze forks under varying network delays in simulation and actual Bitcoin data. Based on this analysis, we establish a relationship between block depth and the probability of confirmation revocation due to a fork. We use prospect theory to relate transaction confirmation probability to transaction amount and user risk tolerance."}
{"id": "2511.15076", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.15076", "abs": "https://arxiv.org/abs/2511.15076", "authors": ["Khaled Hamidouche", "John Bachan", "Pak Markthub", "Peter-Jan Gootzen", "Elena Agostini", "Sylvain Jeaugey", "Aamir Shafi", "Georgios Theodorakis", "Manjunath Gorentla Venkata"], "title": "GPU-Initiated Networking for NCCL", "comment": "13 pages, 9 figures, 3 tables", "summary": "Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead.\n  NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN's practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure."}
{"id": "2511.15491", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.15491", "abs": "https://arxiv.org/abs/2511.15491", "authors": ["Laurent Feuilloley", "Josef Erik Sedláček", "Martin Slávik"], "title": "Proving there is a leader without naming it", "comment": null, "summary": "Local certification is a mechanism for certifying to the nodes of a network that a certain property holds. In this framework, nodes are assigned labels, called certificates, which are supposed to prove that the property holds. The nodes then communicate with their neighbors to verify the correctness of these certificates.\n  Certifying that there is a unique leader in a network is one of the most classical problems in this setting. It is well-known that this can be done using certificates that encode node identifiers and distances in the graph. These require $O(\\log n)$ and $O(\\log D)$ bits respectively, where $n$ is the number of nodes and $D$ is the diameter. A matching lower bound is known in cycle graphs (where $n$ and $D$ are equal up to multiplicative constants).\n  A recent line of work has shown that network structure greatly influences local certification. For example, certifying that a network does not contain triangles takes $Θ(n)$ bits in general graphs, but only $O(\\log n)$ bits in graphs of bounded treewidth. This observation raises the question: Is it possible to achieve sublogarithmic leader certification in graph classes that do not contain cycle graphs? And since in that case we cannot write identifiers in a certificate, do we actually need identifiers at all in such topologies? [We answer these questions with results on small diameter graphs, chordal graphs, grids, and dense graphs. See full abstract in the paper.]"}
{"id": "2511.15503", "categories": ["cs.AR", "cs.DC", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.15503", "abs": "https://arxiv.org/abs/2511.15503", "authors": ["Peiming Yang", "Sankeerth Durvasula", "Ivan Fernandez", "Mohammad Sadrosadati", "Onur Mutlu", "Gennady Pekhimenko", "Christina Giannoula"], "title": "A Tensor Compiler for Processing-In-Memory Architectures", "comment": null, "summary": "Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU."}
