{"id": "2601.01158", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.01158", "abs": "https://arxiv.org/abs/2601.01158", "authors": ["Yilun Zhao", "Yu Chen", "Kaiyan Chang", "He Li", "Bing Li", "Yinhe Han", "Ying Wang"], "title": "A System Architecture for Low Latency Multiprogramming Quantum Computing", "comment": null, "summary": "As quantum systems scale, Multiprogramming Quantum Computing (MPQC) becomes essential to improve device utilization and throughput. However, current MPQC pipelines rely on expensive online compilation to co-optimize concurrently running programs, because quantum executables are device-dependent, non-portable across qubit regions, and highly susceptible to noise and crosstalk. This online step dominates runtime and impedes low-latency deployments for practical, real-world workloads in the future, such as repeatedly invoked Quantum Neural Network (QNN) services.\n  We present FLAMENCO, a fidelity-aware multi-version compilation system that enables independent offline compilation and low-latency, high-fidelity multiprogramming at runtime. At the architecture level, FLAMENCO abstracts devices into compute units to drastically shrink the search space of region allocation. At compile time, it generates diverse executable versions for each program -- each bound to a distinct qubit region -- allowing dynamic region selection at runtime and overcoming non-portability. At runtime, FLAMENCO employs a streamlined orchestrator that leverages post-compilation fidelity metrics to avoid conflicts and mitigate crosstalk, achieving reliable co-execution without online co-optimization. Comprehensive evaluations against state-of-the-art MPQC baselines show that FLAMENCO removes online compilation overhead, achieves over 5$\\times$ runtime speedup, improves execution fidelity, and maintains high utilization as concurrency increases.", "AI": {"tldr": "FLAMENCO\u662f\u4e00\u4e2a\u4fdd\u771f\u5ea6\u611f\u77e5\u7684\u591a\u7248\u672c\u7f16\u8bd1\u7cfb\u7edf\uff0c\u901a\u8fc7\u79bb\u7ebf\u7f16\u8bd1\u751f\u6210\u591a\u6837\u5316\u7684\u53ef\u6267\u884c\u7248\u672c\uff0c\u8fd0\u884c\u65f6\u8fdb\u884c\u52a8\u6001\u533a\u57df\u9009\u62e9\u548c\u51b2\u7a81\u907f\u514d\uff0c\u6d88\u9664\u4e86\u91cf\u5b50\u591a\u7a0b\u5e8f\u8ba1\u7b97\u4e2d\u7684\u5728\u7ebf\u7f16\u8bd1\u5f00\u9500\uff0c\u5b9e\u73b0\u4e865\u500d\u4ee5\u4e0a\u7684\u8fd0\u884c\u65f6\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u7cfb\u7edf\u89c4\u6a21\u6269\u5927\uff0c\u591a\u7a0b\u5e8f\u91cf\u5b50\u8ba1\u7b97\uff08MPQC\uff09\u5bf9\u63d0\u9ad8\u8bbe\u5907\u5229\u7528\u7387\u548c\u541e\u5410\u91cf\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684MPQC\u6d41\u6c34\u7ebf\u4f9d\u8d56\u6602\u8d35\u7684\u5728\u7ebf\u7f16\u8bd1\u6765\u5171\u540c\u4f18\u5316\u5e76\u53d1\u8fd0\u884c\u7684\u7a0b\u5e8f\uff0c\u56e0\u4e3a\u91cf\u5b50\u53ef\u6267\u884c\u6587\u4ef6\u662f\u8bbe\u5907\u4f9d\u8d56\u7684\u3001\u8de8\u91cf\u5b50\u6bd4\u7279\u533a\u57df\u4e0d\u53ef\u79fb\u690d\u7684\uff0c\u5e76\u4e14\u5bf9\u566a\u58f0\u548c\u4e32\u6270\u9ad8\u5ea6\u654f\u611f\u3002\u8fd9\u4e00\u5728\u7ebf\u6b65\u9aa4\u4e3b\u5bfc\u4e86\u8fd0\u884c\u65f6\u95f4\uff0c\u963b\u788d\u4e86\u672a\u6765\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5982\u91cd\u590d\u8c03\u7528\u7684\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u670d\u52a1\uff09\u7684\u4f4e\u5ef6\u8fdf\u90e8\u7f72\u3002", "method": "1\uff09\u5728\u67b6\u6784\u5c42\u9762\uff0c\u5c06\u8bbe\u5907\u62bd\u8c61\u4e3a\u8ba1\u7b97\u5355\u5143\u4ee5\u5927\u5e45\u7f29\u5c0f\u533a\u57df\u5206\u914d\u7684\u641c\u7d22\u7a7a\u95f4\uff1b2\uff09\u5728\u7f16\u8bd1\u65f6\uff0c\u4e3a\u6bcf\u4e2a\u7a0b\u5e8f\u751f\u6210\u7ed1\u5b9a\u5230\u4e0d\u540c\u91cf\u5b50\u6bd4\u7279\u533a\u57df\u7684\u591a\u6837\u5316\u53ef\u6267\u884c\u7248\u672c\uff0c\u5b9e\u73b0\u8fd0\u884c\u65f6\u52a8\u6001\u533a\u57df\u9009\u62e9\uff1b3\uff09\u5728\u8fd0\u884c\u65f6\uff0c\u91c7\u7528\u7b80\u5316\u7684\u7f16\u6392\u5668\uff0c\u5229\u7528\u7f16\u8bd1\u540e\u7684\u4fdd\u771f\u5ea6\u6307\u6807\u907f\u514d\u51b2\u7a81\u5e76\u51cf\u8f7b\u4e32\u6270\uff0c\u5b9e\u73b0\u65e0\u9700\u5728\u7ebf\u5171\u540c\u4f18\u5316\u7684\u53ef\u9760\u534f\u540c\u6267\u884c\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684MPQC\u57fa\u7ebf\u76f8\u6bd4\uff0cFLAMENCO\u6d88\u9664\u4e86\u5728\u7ebf\u7f16\u8bd1\u5f00\u9500\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc75\u500d\u7684\u8fd0\u884c\u65f6\u52a0\u901f\uff0c\u63d0\u9ad8\u4e86\u6267\u884c\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u5e76\u53d1\u589e\u52a0\u65f6\u4fdd\u6301\u4e86\u9ad8\u5229\u7528\u7387\u3002", "conclusion": "FLAMENCO\u901a\u8fc7\u4fdd\u771f\u5ea6\u611f\u77e5\u7684\u591a\u7248\u672c\u7f16\u8bd1\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u72ec\u7acb\u7684\u79bb\u7ebf\u7f16\u8bd1\u548c\u8fd0\u884c\u65f6\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u591a\u7a0b\u5e8f\u6267\u884c\uff0c\u89e3\u51b3\u4e86\u5f53\u524dMPQC\u6d41\u6c34\u7ebf\u4e2d\u5728\u7ebf\u7f16\u8bd1\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u91cf\u5b50\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01265", "categories": ["cs.AR", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.01265", "abs": "https://arxiv.org/abs/2601.01265", "authors": ["Nick Lindsay", "Caroline Trippel", "Anurag Khandelwal", "Abhishek Bhattacharjee"], "title": "CounterPoint: Using Hardware Event Counters to Refute and Refine Microarchitectural Assumptions (Extended Version)", "comment": "This is an extended version of a paper which has been accepted to the 31st ACM International Conference on Architectural Support for Programming Languages and Operating Systems conference (ASPLOS, March 2026). 20 pages, 20 figures, 8 tables", "summary": "Hardware event counters offer the potential to reveal not only performance bottlenecks but also detailed microarchitectural behavior. In practice, this promise is undermined by their vague specifications, opaque designs, and multiplexing noise, making event counter data hard to interpret.\n  We introduce CounterPoint, a framework that tests user-specified microarchitectural models - expressed as $\u03bc$path Decision Diagrams - for consistency with performance counter data. When mismatches occur, CounterPoint pinpoints plausible microarchitectural features that could explain them, using multi-dimensional counter confidence regions to mitigate multiplexing noise. We apply CounterPoint to the Haswell Memory Management Unit as a case study, shedding light on multiple undocumented and underdocumented microarchitectural behaviors. These include a load-store queue-side TLB prefetcher, merging page table walkers, abortable page table walks, and more.\n  Overall, CounterPoint helps experts reconcile noisy hardware performance counter measurements with their mental model of the microarchitecture - uncovering subtle, previously hidden hardware features along the way.", "AI": {"tldr": "CounterPoint\u6846\u67b6\u901a\u8fc7\u6d4b\u8bd5\u7528\u6237\u6307\u5b9a\u7684\u5fae\u67b6\u6784\u6a21\u578b\u4e0e\u6027\u80fd\u8ba1\u6570\u5668\u6570\u636e\u7684\u4e00\u81f4\u6027\uff0c\u5e2e\u52a9\u4e13\u5bb6\u89e3\u91ca\u6a21\u7cca\u7684\u786c\u4ef6\u4e8b\u4ef6\u8ba1\u6570\u5668\u6570\u636e\uff0c\u5e76\u53d1\u73b0\u672a\u6587\u6863\u5316\u7684\u5fae\u67b6\u6784\u884c\u4e3a\u3002", "motivation": "\u786c\u4ef6\u4e8b\u4ef6\u8ba1\u6570\u5668\u867d\u7136\u80fd\u63ed\u793a\u6027\u80fd\u74f6\u9888\u548c\u5fae\u67b6\u6784\u884c\u4e3a\uff0c\u4f46\u7531\u4e8e\u5176\u89c4\u8303\u6a21\u7cca\u3001\u8bbe\u8ba1\u4e0d\u900f\u660e\u4ee5\u53ca\u591a\u8def\u590d\u7528\u566a\u58f0\uff0c\u5b9e\u9645\u6570\u636e\u96be\u4ee5\u89e3\u91ca\uff0c\u9700\u8981\u5de5\u5177\u6765\u5e2e\u52a9\u4e13\u5bb6\u7406\u89e3\u8fd9\u4e9b\u8ba1\u6570\u5668\u6570\u636e\u3002", "method": "\u5f15\u5165CounterPoint\u6846\u67b6\uff0c\u6d4b\u8bd5\u7528\u6237\u6307\u5b9a\u7684\u5fae\u67b6\u6784\u6a21\u578b\uff08\u8868\u793a\u4e3a\u03bc\u8def\u5f84\u51b3\u7b56\u56fe\uff09\u4e0e\u6027\u80fd\u8ba1\u6570\u5668\u6570\u636e\u7684\u4e00\u81f4\u6027\u3002\u5f53\u51fa\u73b0\u4e0d\u5339\u914d\u65f6\uff0c\u4f7f\u7528\u591a\u7ef4\u8ba1\u6570\u5668\u7f6e\u4fe1\u533a\u57df\u6765\u51cf\u8f7b\u591a\u8def\u590d\u7528\u566a\u58f0\uff0c\u5e76\u5b9a\u4f4d\u53ef\u80fd\u7684\u5fae\u67b6\u6784\u7279\u5f81\u6765\u89e3\u91ca\u8fd9\u4e9b\u4e0d\u5339\u914d\u3002", "result": "\u5c06CounterPoint\u5e94\u7528\u4e8eHaswell\u5185\u5b58\u7ba1\u7406\u5355\u5143\u6848\u4f8b\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u591a\u4e2a\u672a\u6587\u6863\u5316\u6216\u6587\u6863\u4e0d\u8db3\u7684\u5fae\u67b6\u6784\u884c\u4e3a\uff0c\u5305\u62ec\u8d1f\u8f7d\u5b58\u50a8\u961f\u5217\u4fa7TLB\u9884\u53d6\u5668\u3001\u5408\u5e76\u9875\u8868\u904d\u5386\u5668\u3001\u53ef\u4e2d\u6b62\u9875\u8868\u904d\u5386\u7b49\u3002", "conclusion": "CounterPoint\u5e2e\u52a9\u4e13\u5bb6\u5c06\u5608\u6742\u7684\u786c\u4ef6\u6027\u80fd\u8ba1\u6570\u5668\u6d4b\u91cf\u4e0e\u4ed6\u4eec\u5bf9\u5fae\u67b6\u6784\u7684\u5fc3\u7406\u6a21\u578b\u76f8\u534f\u8c03\uff0c\u5728\u6b64\u8fc7\u7a0b\u4e2d\u53d1\u73b0\u4e86\u4ee5\u524d\u9690\u85cf\u7684\u5fae\u5999\u786c\u4ef6\u7279\u6027\u3002"}}
{"id": "2601.02053", "categories": ["cs.AR", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.02053", "abs": "https://arxiv.org/abs/2601.02053", "authors": ["Leandro Lanzieri", "Jiri Kral", "Goerschwin Fey", "Holger Schlarb", "Thomas C. Schmidt"], "title": "Ageing Monitoring for Commercial Microcontrollers Based on Timing Windows", "comment": null, "summary": "Microcontrollers are increasingly present in embedded deployments and dependable applications, for which malfunctions due to hardware ageing can have severe impact. The lack of deployable techniques for ageing monitoring on these devices has spread the application of guard bands to prevent timing errors due to degradation. Applying this static technique can limit performance and lead to sudden failures as devices age. In this paper, we follow a software-based self-testing approach to design monitoring of hardware degradation for microcontrollers. Deployable in the field, our technique leverages timing windows of variable lengths to determine the maximum operational frequency of the devices. We empirically validate the method on real hardware and find that it consistently detects temperature-induced degradations in maximum operating frequency of up to 13.79 % across devices for 60 \u00b0C temperature increase.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8f6f\u4ef6\u81ea\u6d4b\u8bd5\u7684\u5fae\u63a7\u5236\u5668\u786c\u4ef6\u8001\u5316\u76d1\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u53d8\u957f\u5ea6\u65f6\u95f4\u7a97\u53e3\u786e\u5b9a\u8bbe\u5907\u6700\u5927\u5de5\u4f5c\u9891\u7387\uff0c\u53ef\u73b0\u573a\u90e8\u7f72\u68c0\u6d4b\u6e29\u5ea6\u5f15\u8d77\u7684\u6027\u80fd\u9000\u5316", "motivation": "\u5fae\u63a7\u5236\u5668\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u548c\u53ef\u9760\u6027\u5e94\u7528\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u786c\u4ef6\u8001\u5316\u5bfc\u81f4\u7684\u6545\u969c\u53ef\u80fd\u4ea7\u751f\u4e25\u91cd\u5f71\u54cd\u3002\u76ee\u524d\u7f3a\u4e4f\u53ef\u90e8\u7f72\u7684\u8001\u5316\u76d1\u6d4b\u6280\u672f\uff0c\u666e\u904d\u91c7\u7528\u9759\u6001\u9632\u62a4\u5e26\u65b9\u6cd5\u9632\u6b62\u65f6\u5e8f\u9519\u8bef\uff0c\u4f46\u8fd9\u4f1a\u9650\u5236\u6027\u80fd\u4e14\u53ef\u80fd\u5bfc\u81f4\u8bbe\u5907\u8001\u5316\u65f6\u7a81\u7136\u5931\u6548\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8f6f\u4ef6\u7684\u81ea\u6d4b\u8bd5\u65b9\u6cd5\u8bbe\u8ba1\u5fae\u63a7\u5236\u5668\u786c\u4ef6\u9000\u5316\u76d1\u6d4b\u6280\u672f\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u53ef\u53d8\u957f\u5ea6\u65f6\u95f4\u7a97\u53e3\u786e\u5b9a\u8bbe\u5907\u7684\u6700\u5927\u5de5\u4f5c\u9891\u7387\uff0c\u53ef\u5728\u73b0\u573a\u90e8\u7f72\u3002", "result": "\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u5b9e\u8bc1\u9a8c\u8bc1\u8be5\u65b9\u6cd5\uff0c\u53d1\u73b0\u5728\u6e29\u5ea6\u5347\u9ad860\u00b0C\u65f6\uff0c\u80fd\u4e00\u81f4\u68c0\u6d4b\u5230\u6700\u5927\u5de5\u4f5c\u9891\u7387\u9ad8\u8fbe13.79%\u7684\u6e29\u5ea6\u5f15\u8d77\u7684\u9000\u5316\uff0c\u4e14\u5728\u4e0d\u540c\u8bbe\u5907\u95f4\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f6f\u4ef6\u81ea\u6d4b\u8bd5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u76d1\u6d4b\u5fae\u63a7\u5236\u5668\u7684\u786c\u4ef6\u9000\u5316\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u9759\u6001\u9632\u62a4\u5e26\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8001\u5316\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02135", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2601.02135", "abs": "https://arxiv.org/abs/2601.02135", "authors": ["Liu Shijie", "Zeng Zhenghao", "Jiao Han", "Huang Yihua"], "title": "HFRWKV: A High-Performance Fully On-Chip Hardware Accelerator for RWKV", "comment": null, "summary": "RWKV is a modern RNN architecture that approaches the performance of Transformers, with the advantage of processing long contexts at a linear memory cost. However, its sequential computation pattern struggles to efficiently leverage GPU parallelism, which leads to low compute resource utilization. Furthermore, frequent off-chip weight accesses create a memory bottleneck. To address these challenges, we propose HFRWKV, an FPGA-based hardware accelerator specifically designed for RWKV. Within the matrix operation module, we propose a novel hardware-friendly hybrid-precision quantization strategy, which enhances performance while maintaining acceptable accuracy. For the complex operations including exponentiation and division, we introduce a method featuring reusable architectures combined with lookup tables or piecewise linear approximation, which is algorithmically refined to effectively balance precision and hardware resource consumption. Based on this foundation, we adopt a fully on-chip computing system integrating parallel matrix-vector processing array and an efficient pipeline architecture. Through computation reordering and chunked double buffering, it effectively eliminates data transfer bottlenecks and improves overall throughput. We implement HFRWKV on the Alveo U50 and U280 platform. Experimental results show that compared to a CPU, a throughput improvement of 63.48$\\times$ and an energy efficiency improvement of 139.17$\\times$. Compared to GPUs, achieves a throughput improvement of 32.33$\\times$ and an energy efficiency improvement of 171.36$\\times$.", "AI": {"tldr": "HFRWKV\uff1a\u9488\u5bf9RWKV\u6a21\u578b\u7684FPGA\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u3001\u53ef\u590d\u7528\u67b6\u6784\u548c\u5168\u7247\u4e0a\u8ba1\u7b97\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u548c\u80fd\u6548", "motivation": "RWKV\u4f5c\u4e3a\u73b0\u4ee3RNN\u67b6\u6784\uff0c\u867d\u7136\u80fd\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u4e14\u5185\u5b58\u6210\u672c\u7ebf\u6027\u589e\u957f\uff0c\u4f46\u5176\u987a\u5e8f\u8ba1\u7b97\u6a21\u5f0f\u65e0\u6cd5\u5145\u5206\u5229\u7528GPU\u5e76\u884c\u6027\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u4f4e\uff0c\u4e14\u9891\u7e41\u7684\u7247\u5916\u6743\u91cd\u8bbf\u95ee\u9020\u6210\u5185\u5b58\u74f6\u9888", "method": "1. \u63d0\u51fa\u786c\u4ef6\u53cb\u597d\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u7b56\u7565\uff1b2. \u9488\u5bf9\u6307\u6570\u548c\u9664\u6cd5\u7b49\u590d\u6742\u64cd\u4f5c\uff0c\u91c7\u7528\u53ef\u590d\u7528\u67b6\u6784\u7ed3\u5408\u67e5\u627e\u8868\u6216\u5206\u6bb5\u7ebf\u6027\u903c\u8fd1\uff1b3. \u91c7\u7528\u5168\u7247\u4e0a\u8ba1\u7b97\u7cfb\u7edf\uff0c\u96c6\u6210\u5e76\u884c\u77e9\u9635\u5411\u91cf\u5904\u7406\u9635\u5217\u548c\u9ad8\u6548\u6d41\u6c34\u7ebf\u67b6\u6784\uff1b4. \u901a\u8fc7\u8ba1\u7b97\u91cd\u6392\u5e8f\u548c\u5206\u5757\u53cc\u7f13\u51b2\u6d88\u9664\u6570\u636e\u4f20\u8f93\u74f6\u9888", "result": "\u5728Alveo U50\u548cU280\u5e73\u53f0\u4e0a\u5b9e\u73b0\uff0c\u76f8\u6bd4CPU\uff1a\u541e\u5410\u91cf\u63d0\u534763.48\u500d\uff0c\u80fd\u6548\u63d0\u5347139.17\u500d\uff1b\u76f8\u6bd4GPU\uff1a\u541e\u5410\u91cf\u63d0\u534732.33\u500d\uff0c\u80fd\u6548\u63d0\u5347171.36\u500d", "conclusion": "HFRWKV\u6210\u529f\u89e3\u51b3\u4e86RWKV\u5728GPU\u4e0a\u7684\u5e76\u884c\u6027\u4e0d\u8db3\u548c\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7FPGA\u4e13\u7528\u786c\u4ef6\u52a0\u901f\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u80fd\u6548\u6539\u8fdb"}}
{"id": "2601.00882", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2601.00882", "abs": "https://arxiv.org/abs/2601.00882", "authors": ["Mingxiu Wang", "Jiawei Wang", "Xiao Cheng"], "title": "BALI: Branch-Aware Loop Invariant Inference with Large Language Models", "comment": "4 pages, 1 figure, AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification", "summary": "Loop invariants are fundamental for reasoning about the correctness of iterative algorithms. However, deriving suitable invariants remains a challenging and often manual task, particularly for complex programs. In this paper, we introduce BALI, a branch-aware framework that integrates large language models (LLMs) to enhance the inference and verification of loop invariants. Our approach combines automated reasoning with branch-aware static program analysis to improve both precision and scalability. Specifically, unlike prior LLM-only guess-and-check methods, BALI first verifies branch-sequence-level (path-level) clauses with SMT and then composes them into program-level invariants. We outline its key components, present preliminary results, and discuss future directions toward fully automated invariant discovery.", "AI": {"tldr": "BALI\uff1a\u4e00\u4e2a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5206\u652f\u611f\u77e5\u9759\u6001\u5206\u6790\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u5faa\u73af\u4e0d\u53d8\u5f0f\u7684\u63a8\u7406\u548c\u9a8c\u8bc1\uff0c\u901a\u8fc7\u8def\u5f84\u7ea7\u5b50\u53e5\u9a8c\u8bc1\u548c\u7ec4\u5408\u6765\u63d0\u9ad8\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5faa\u73af\u4e0d\u53d8\u5f0f\u5bf9\u4e8e\u9a8c\u8bc1\u8fed\u4ee3\u7b97\u6cd5\u7684\u6b63\u786e\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u63a8\u5bfc\u5408\u9002\u7684\u4e0d\u53d8\u5f0f\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u4e14\u901a\u5e38\u9700\u8981\u624b\u52a8\u5b8c\u6210\u7684\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u590d\u6742\u7a0b\u5e8f\u3002\u73b0\u6709\u57fa\u4e8eLLM\u7684\u731c\u6d4b-\u68c0\u67e5\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "BALI\u6846\u67b6\u7ed3\u5408\u4e86\u81ea\u52a8\u5316\u63a8\u7406\u548c\u5206\u652f\u611f\u77e5\u9759\u6001\u7a0b\u5e8f\u5206\u6790\u3002\u4e0d\u540c\u4e8e\u5148\u524d\u7684\u7eafLLM\u65b9\u6cd5\uff0cBALI\u9996\u5148\u4f7f\u7528SMT\u9a8c\u8bc1\u5206\u652f\u5e8f\u5217\u7ea7\uff08\u8def\u5f84\u7ea7\uff09\u5b50\u53e5\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u5b50\u53e5\u7ec4\u5408\u6210\u7a0b\u5e8f\u7ea7\u4e0d\u53d8\u5f0f\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u521d\u6b65\u7ed3\u679c\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u5faa\u73af\u4e0d\u53d8\u5f0f\u63a8\u7406\u7684\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "BALI\u4e3a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u4e0d\u53d8\u5f0f\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u548c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86\u5faa\u73af\u4e0d\u53d8\u5f0f\u7684\u63a8\u7406\u548c\u9a8c\u8bc1\u8fc7\u7a0b\u3002"}}
{"id": "2601.01031", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.01031", "abs": "https://arxiv.org/abs/2601.01031", "authors": ["Bharadwaj Veeravalli"], "title": "A Multi-Port Concurrent Communication Model for handling Compute Intensive Tasks on Distributed Satellite System Constellations", "comment": null, "summary": "We develop an integrated Multi-Port Concurrent Communication Divisible Load Theory (MPCC-DLT) framework for relay-centric distributed satellite systems (DSS), capturing concurrent data dissemination, parallel computation, and result return under heterogeneous onboard processing and inter-satellite link conditions. We propose a formulation that yields closed-form expressions for optimal load allocation and completion time that explicitly quantify the joint impact of computation speed, link bandwidth, and result-size overhead. We further derive deadline feasibility conditions that enable explicit sizing of cooperative satellite clusters to meet time-critical task requirements. Extensive simulation results demonstrate that highly distributable tasks achieve substantial latency reduction, while communication-heavy tasks exhibit diminishing returns due to result-transfer overheads. To bridge theory and practice, we extend the MPCC-DLT framework with a real-time admission control mechanism that handles stochastic task arrivals and deadline constraints, enabling blocking-aware operation. Our real-time simulations illustrate how task structure and system parameters jointly govern deadline satisfaction and operating regimes. Overall, this work provides the first analytically tractable MPCC-DLT model for distributed satellite systems and offers actionable insights for application-aware scheduling and system-level design of future satellite constellations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7528\u4e8e\u5206\u5e03\u5f0f\u536b\u661f\u7cfb\u7edf\u7684\u591a\u7aef\u53e3\u5e76\u53d1\u901a\u4fe1\u53ef\u5206\u5272\u8d1f\u8f7d\u7406\u8bba\u6846\u67b6\uff0c\u91cf\u5316\u8ba1\u7b97\u3001\u901a\u4fe1\u548c\u7ed3\u679c\u4f20\u8f93\u5f00\u9500\u7684\u8054\u5408\u5f71\u54cd\uff0c\u63d0\u4f9b\u6700\u4f18\u8d1f\u8f7d\u5206\u914d\u548c\u5b8c\u6210\u65f6\u95f4\u7684\u95ed\u5f0f\u89e3\uff0c\u5e76\u6269\u5c55\u4e86\u5b9e\u65f6\u51c6\u5165\u63a7\u5236\u673a\u5236\u3002", "motivation": "\u5206\u5e03\u5f0f\u536b\u661f\u7cfb\u7edf\u9700\u8981\u5904\u7406\u5e76\u53d1\u6570\u636e\u5206\u53d1\u3001\u5e76\u884c\u8ba1\u7b97\u548c\u7ed3\u679c\u8fd4\u56de\uff0c\u4f46\u73b0\u6709\u7406\u8bba\u7f3a\u4e4f\u5bf9\u5f02\u6784\u673a\u8f7d\u5904\u7406\u548c\u661f\u95f4\u94fe\u8def\u6761\u4ef6\u7684\u7efc\u5408\u5206\u6790\uff0c\u9700\u8981\u53ef\u5206\u6790\u7684\u7406\u8bba\u6846\u67b6\u6765\u6307\u5bfc\u5e94\u7528\u611f\u77e5\u8c03\u5ea6\u548c\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u3002", "method": "\u5f00\u53d1\u4e86MPCC-DLT\u6846\u67b6\uff0c\u5efa\u7acb\u4e86\u5305\u542b\u8ba1\u7b97\u901f\u5ea6\u3001\u94fe\u8def\u5e26\u5bbd\u548c\u7ed3\u679c\u5927\u5c0f\u5f00\u9500\u7684\u6570\u5b66\u6a21\u578b\uff0c\u63a8\u5bfc\u51fa\u6700\u4f18\u8d1f\u8f7d\u5206\u914d\u548c\u5b8c\u6210\u65f6\u95f4\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u6269\u5c55\u4e86\u5904\u7406\u968f\u673a\u4efb\u52a1\u5230\u8fbe\u548c\u622a\u6b62\u65f6\u95f4\u7ea6\u675f\u7684\u5b9e\u65f6\u51c6\u5165\u63a7\u5236\u673a\u5236\u3002", "result": "\u9ad8\u5ea6\u53ef\u5206\u5e03\u7684\u4efb\u52a1\u80fd\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\uff0c\u800c\u901a\u4fe1\u5bc6\u96c6\u578b\u4efb\u52a1\u7531\u4e8e\u7ed3\u679c\u4f20\u8f93\u5f00\u9500\u800c\u6536\u76ca\u9012\u51cf\uff1b\u63a8\u5bfc\u51fa\u7684\u622a\u6b62\u65f6\u95f4\u53ef\u884c\u6027\u6761\u4ef6\u80fd\u660e\u786e\u786e\u5b9a\u534f\u4f5c\u536b\u661f\u96c6\u7fa4\u89c4\u6a21\uff1b\u5b9e\u65f6\u4eff\u771f\u5c55\u793a\u4e86\u4efb\u52a1\u7ed3\u6784\u548c\u7cfb\u7edf\u53c2\u6570\u5982\u4f55\u5171\u540c\u51b3\u5b9a\u622a\u6b62\u65f6\u95f4\u6ee1\u8db3\u60c5\u51b5\u548c\u8fd0\u884c\u72b6\u6001\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5206\u5e03\u5f0f\u536b\u661f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9996\u4e2a\u53ef\u5206\u6790\u5904\u7406\u7684MPCC-DLT\u6a21\u578b\uff0c\u4e3a\u672a\u6765\u536b\u661f\u661f\u5ea7\u7684\u5e94\u7528\u611f\u77e5\u8c03\u5ea6\u548c\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u5f25\u5408\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2601.02045", "categories": ["cs.PL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.02045", "abs": "https://arxiv.org/abs/2601.02045", "authors": ["Shuoming Zhang", "Jiacheng Zhao", "Qiuchu Yu", "Chunwei Xia", "Zheng Wang", "Xiaobing Feng", "Huimin Cui"], "title": "The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers", "comment": "Accepted by CCF Transactions on High Performance Computing", "summary": "This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools.", "AI": {"tldr": "\u672c\u6587\u5bf9LLM\u8d4b\u80fd\u7f16\u8bd1\u9886\u57df\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u591a\u7ef4\u5206\u7c7b\u6cd5\uff0c\u603b\u7ed3\u4e86\u4e09\u5927\u4f18\u52bf\uff0c\u5e76\u6307\u51fa\u4e86\u8be5\u9886\u57df\u7684\u5173\u952e\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u548c\u7406\u89e3\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u7814\u7a76\u8005\u5f00\u59cb\u63a2\u7d22\u5982\u4f55\u5c06LLMs\u5e94\u7528\u4e8e\u7f16\u8bd1\u9886\u57df\uff0c\u4ee5\u5f00\u53d1\u65b0\u4e00\u4ee3\u667a\u80fd\u3001\u81ea\u9002\u5e94\u3001\u534f\u540c\u7684\u7f16\u8bd1\u5de5\u5177\u3002\u672c\u7efc\u8ff0\u65e8\u5728\u7cfb\u7edf\u68b3\u7406\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u56de\u7b54\u5173\u952e\u7814\u7a76\u95ee\u9898\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u57fa\u7840\u8def\u7ebf\u56fe\u3002", "method": "\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u5168\u9762\u7684\u591a\u7ef4\u5206\u7c7b\u6cd5\u6765\u7cfb\u7edf\u5206\u6790\u73b0\u6709\u5de5\u4f5c\uff1a1\uff09\u8bbe\u8ba1\u54f2\u5b66\uff08\u9009\u62e9\u5668\u3001\u7ffb\u8bd1\u5668\u3001\u751f\u6210\u5668\uff09\uff1b2\uff09LLM\u65b9\u6cd5\u8bba\uff1b3\uff09\u4ee3\u7801\u62bd\u8c61\u5c42\u6b21\uff1b4\uff09\u4efb\u52a1\u7c7b\u578b\u3002\u540c\u65f6\u8bc6\u522b\u4e86\u8be5\u9886\u57df\u7684\u4e3b\u8981\u6311\u6218\u548c\u673a\u9047\u3002", "result": "\u8bc6\u522b\u4e86LLM\u8d4b\u80fd\u7f16\u8bd1\u7684\u4e09\u5927\u4e3b\u8981\u4f18\u52bf\uff1a1\uff09\u7f16\u8bd1\u5f00\u53d1\u7684\u6c11\u4e3b\u5316\uff1b2\uff09\u65b0\u9896\u4f18\u5316\u7b56\u7565\u7684\u53d1\u73b0\uff1b3\uff09\u7f16\u8bd1\u5668\u4f20\u7edf\u8303\u56f4\u7684\u6269\u5c55\u3002\u6307\u51fa\u4e86\u5173\u952e\u6311\u6218\u5305\u62ec\u786e\u4fdd\u6b63\u786e\u6027\u548c\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u8ba4\u4e3a\u5f00\u53d1\u6df7\u5408\u7cfb\u7edf\u662f\u6700\u6709\u524d\u666f\u7684\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u672c\u7efc\u8ff0\u4e3aLLM\u8d4b\u80fd\u7f16\u8bd1\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ef4\u5206\u7c7b\u6cd5\u3001\u4f18\u52bf\u5206\u6790\u548c\u6311\u6218\u8bc6\u522b\uff0c\u4e3a\u65b0\u4e00\u4ee3\u667a\u80fd\u3001\u81ea\u9002\u5e94\u3001\u534f\u540c\u7f16\u8bd1\u5de5\u5177\u7684\u53d1\u5c55\u7ed8\u5236\u4e86\u8def\u7ebf\u56fe\uff0c\u5c06\u6210\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u7684\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2601.01125", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01125", "abs": "https://arxiv.org/abs/2601.01125", "authors": ["Mohammad Goudarzi", "Arash Shaghaghi", "Zhiyu Wang", "Rajkumar Buyya"], "title": "Performance and Security Aware Distributed Service Placement in Fog Computing", "comment": null, "summary": "The rapid proliferation of IoT applications has intensified the demand for efficient and secure service placement in Fog computing. However, heterogeneous resources, dynamic workloads, and diverse security requirements make optimal service placement highly challenging. Most solutions focus primarily on performance metrics while overlooking the security implications of deployment decisions. This paper proposes a Security and Performance-Aware Distributed Deep Reinforcement Learning (SPA-DDRL) framework for joint optimization of service response time and security compliance in Fog computing. The problem is formulated as a weighted multi-objective optimization task, minimizing latency while maximizing a security score derived from the security capabilities of Fog nodes. The security score features a new three-tier hierarchy, where configuration-level checks verify proper settings, capability-level assessments evaluate the resource security features, and control-level evaluations enforce stringent policies, thereby ensuring compliant solutions that align with performance objectives. SPA-DDRL adopts a distributed broker-learner architecture where multiple brokers perform autonomous service-placement decisions and a centralized learner coordinates global policy optimization through shared prioritized experiences. It integrates three key improvements, including Long Short-Term Memory networks, Prioritized Experience Replay, and off-policy correction mechanisms to improve the agent's performance. Experiments based on real IoT workloads show that SPA-DDRL significantly improves both service response time and placement security compared to current approaches, achieving a 16.3% improvement in response time and a 33% faster convergence rate. It also maintains consistent, feasible, security-compliant solutions across all system scales, while baseline techniques fail or show performance degradation.", "AI": {"tldr": "\u63d0\u51faSPA-DDRL\u6846\u67b6\uff0c\u4f7f\u7528\u5206\u5e03\u5f0f\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u96fe\u8ba1\u7b97\u4e2d\u8054\u5408\u4f18\u5316\u670d\u52a1\u54cd\u5e94\u65f6\u95f4\u548c\u5b89\u5168\u5408\u89c4\u6027\uff0c\u901a\u8fc7\u4e09\u5c42\u5b89\u5168\u8bc4\u5206\u4f53\u7cfb\u548c\u6539\u8fdb\u7b97\u6cd5\u5b9e\u73b0\u6027\u80fd\u4e0e\u5b89\u5168\u5e73\u8861\u3002", "motivation": "\u7269\u8054\u7f51\u5e94\u7528\u5feb\u901f\u589e\u957f\u5bf9\u96fe\u8ba1\u7b97\u4e2d\u7684\u9ad8\u6548\u5b89\u5168\u670d\u52a1\u90e8\u7f72\u63d0\u51fa\u66f4\u9ad8\u8981\u6c42\uff0c\u4f46\u73b0\u6709\u65b9\u6848\u5927\u591a\u53ea\u5173\u6ce8\u6027\u80fd\u6307\u6807\u800c\u5ffd\u89c6\u5b89\u5168\u5f71\u54cd\uff0c\u5f02\u6784\u8d44\u6e90\u3001\u52a8\u6001\u8d1f\u8f7d\u548c\u591a\u6837\u5316\u5b89\u5168\u9700\u6c42\u4f7f\u5f97\u6700\u4f18\u670d\u52a1\u90e8\u7f72\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u5b89\u5168\u4e0e\u6027\u80fd\u611f\u77e5\u7684\u5206\u5e03\u5f0f\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u52a0\u6743\u591a\u76ee\u6807\u4f18\u5316\u4efb\u52a1\uff0c\u6700\u5c0f\u5316\u5ef6\u8fdf\u540c\u65f6\u6700\u5927\u5316\u57fa\u4e8e\u96fe\u8282\u70b9\u5b89\u5168\u80fd\u529b\u7684\u5b89\u5168\u8bc4\u5206\u3002\u91c7\u7528\u4e09\u5c42\u5b89\u5168\u8bc4\u5206\u4f53\u7cfb\uff08\u914d\u7f6e\u7ea7\u3001\u80fd\u529b\u7ea7\u3001\u63a7\u5236\u7ea7\uff09\uff0c\u4f7f\u7528\u5206\u5e03\u5f0f\u4ee3\u7406-\u5b66\u4e60\u8005\u67b6\u6784\uff0c\u96c6\u6210LSTM\u7f51\u7edc\u3001\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u548c\u79bb\u7b56\u7565\u4fee\u6b63\u673a\u5236\u3002", "result": "\u57fa\u4e8e\u771f\u5b9e\u7269\u8054\u7f51\u8d1f\u8f7d\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSPA-DDRL\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u6539\u5584\u670d\u52a1\u54cd\u5e94\u65f6\u95f4\u548c\u90e8\u7f72\u5b89\u5168\u6027\uff0c\u54cd\u5e94\u65f6\u95f4\u63d0\u534716.3%\uff0c\u6536\u655b\u901f\u5ea6\u52a0\u5feb33%\uff0c\u5728\u6240\u6709\u7cfb\u7edf\u89c4\u6a21\u4e0b\u4fdd\u6301\u4e00\u81f4\u3001\u53ef\u884c\u3001\u5b89\u5168\u5408\u89c4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "SPA-DDRL\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u96fe\u8ba1\u7b97\u4e2d\u670d\u52a1\u90e8\u7f72\u7684\u6027\u80fd\u4e0e\u5b89\u5168\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4e09\u5c42\u5b89\u5168\u8bc4\u5206\u4f53\u7cfb\u548c\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u5408\u89c4\u7684\u670d\u52a1\u90e8\u7f72\u3002"}}
{"id": "2601.02060", "categories": ["cs.PL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.02060", "abs": "https://arxiv.org/abs/2601.02060", "authors": ["Nguyet-Anh H. Lang", "Eric Lang", "Thanh Le-Cong", "Bach Le", "Quyet-Thang Huynh"], "title": "Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming", "comment": null, "summary": "Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.", "AI": {"tldr": "FPEval\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u51fd\u6570\u5f0f\u7f16\u7a0b\u8bed\u8a00\u4e2d\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u6846\u67b6\uff0c\u57fa\u4e8eFPBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0LLM\u5728\u7eaf\u51fd\u6570\u5f0f\u8bed\u8a00\u4e2d\u9519\u8bef\u7387\u8f83\u9ad8\uff0c\u4e14\u7ecf\u5e38\u751f\u6210\u975e\u60ef\u7528\u7684\u547d\u4ee4\u5f0f\u98ce\u683c\u4ee3\u7801\u3002", "motivation": "\u51fd\u6570\u5f0f\u7f16\u7a0b\u867d\u7136\u80fd\u63d0\u4f9b\u53ef\u9760\u548c\u5b89\u5168\u7684\u8f6f\u4ef6\u7cfb\u7edf\u57fa\u7840\uff0c\u4f46\u7531\u4e8e\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\uff0c\u91c7\u7528\u7387\u4e0d\u9ad8\u3002LLM\u4e3a\u964d\u4f4e\u8fd9\u4e00\u95e8\u69db\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u547d\u4ee4\u5f0f\u8bed\u8a00\uff0c\u5bf9\u51fd\u6570\u5f0f\u7f16\u7a0b\u8bed\u8a00\u7684\u8bc4\u4f30\u4e0d\u8db3\u3002", "method": "\u5f15\u5165FPEval\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8eFPBench\u57fa\u51c6\uff08\u5305\u542b721\u4e2a\u7f16\u7a0b\u4efb\u52a1\uff0c\u6db5\u76d6\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\u548c\u4e09\u79cd\u4e3b\u6d41\u51fd\u6570\u5f0f\u8bed\u8a00\uff1aHaskell\u3001OCaml\u548cScala\uff09\u3002\u8be5\u6846\u67b6\u63d0\u4f9b\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u7840\u8bbe\u65bd\uff0c\u5305\u62ec\u6d4b\u8bd5\u9a8c\u8bc1\u548c\u9759\u6001\u5206\u6790\u5de5\u5177\uff0c\u8bc4\u4f30\u529f\u80fd\u6b63\u786e\u6027\u3001\u4ee3\u7801\u98ce\u683c\u548c\u53ef\u7ef4\u62a4\u6027\u3002", "result": "LLM\u5728\u51fd\u6570\u5f0f\u7f16\u7a0b\u4e2d\u7684\u6027\u80fd\u968f\u6a21\u578b\u8fdb\u6b65\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5728\u7eaf\u51fd\u6570\u5f0f\u8bed\u8a00\uff08Haskell\u548cOCaml\uff09\u4e2d\u7684\u9519\u8bef\u7387\u663e\u8457\u9ad8\u4e8e\u6df7\u5408\u8bed\u8a00\uff08Scala\uff09\u6216\u547d\u4ee4\u5f0f\u8bed\u8a00\uff08Java\uff09\u3002LLM\u7ecf\u5e38\u751f\u6210\u9075\u5faa\u547d\u4ee4\u5f0f\u6a21\u5f0f\u7684\u975e\u60ef\u7528\u51fd\u6570\u5f0f\u4ee3\u7801\uff0c\u5f71\u54cd\u4ee3\u7801\u98ce\u683c\u548c\u957f\u671f\u53ef\u7ef4\u62a4\u6027\u3002LLM\u5728\u83b7\u5f97\u9759\u6001\u5206\u6790\u53cd\u9988\u548c\u624b\u5de5\u5236\u4f5c\u7684\u5e38\u89c1\u95ee\u9898\u6307\u4ee4\u540e\uff0c\u80fd\u591f\u90e8\u5206\u81ea\u6211\u4fee\u590d\u6b63\u786e\u6027\u548c\u8d28\u91cf\u95ee\u9898\u3002", "conclusion": "\u867d\u7136LLM\u5728\u51fd\u6570\u5f0f\u7f16\u7a0b\u4ee3\u7801\u751f\u6210\u65b9\u9762\u6709\u8fdb\u6b65\uff0c\u4f46\u5728\u7eaf\u51fd\u6570\u5f0f\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\u4ecd\u6709\u8f83\u5927\u63d0\u5347\u7a7a\u95f4\uff0c\u4e14\u751f\u6210\u7684\u4ee3\u7801\u98ce\u683c\u95ee\u9898\u9700\u8981\u5173\u6ce8\u3002\u901a\u8fc7\u63d0\u4f9b\u9002\u5f53\u7684\u53cd\u9988\u673a\u5236\uff0cLLM\u80fd\u591f\u6539\u5584\u4ee3\u7801\u8d28\u91cf\u548c\u6b63\u786e\u6027\u3002"}}
{"id": "2601.01209", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.01209", "abs": "https://arxiv.org/abs/2601.01209", "authors": ["Xin Tan", "Yicheng Feng", "Yu Zhou", "Yimin Jiang", "Yibo Zhu", "Hong Xu"], "title": "OrchestrRL: Dynamic Compute and Network Orchestration for Disaggregated RL", "comment": null, "summary": "Post-training with reinforcement learning (RL) has greatly enhanced the capabilities of large language models. Disaggregating the generation and training stages in RL into a parallel, asynchronous pipeline offers the potential for flexible scaling and improved throughput. However, it still faces two critical challenges. First, the generation stage often becomes a bottleneck due to dynamic workload shifts and severe execution imbalances. Second, the decoupled stages result in diverse and dynamic network traffic patterns that overwhelm conventional network fabrics. This paper introduces OrchestrRL, an orchestration framework that dynamically manages compute and network rhythms in disaggregated RL. To improve generation efficiency, OrchestrRL employs an adaptive compute scheduler that dynamically adjusts parallelism to match workload characteristics within and across generation steps. This accelerates execution while continuously rebalancing requests to mitigate stragglers. To address the dynamic network demands inherent in disaggregated RL -- further intensified by parallelism switching -- we co-design RFabric, a reconfigurable hybrid optical-electrical fabric. RFabric leverages optical circuit switches at selected network tiers to reconfigure the topology in real time, enabling workload-aware circuits for (i) layer-wise collective communication during training iterations, (ii) generation under different parallelism configurations, and (iii) periodic inter-cluster weight synchronization. We evaluate OrchestrRL on a physical testbed with 48 H800 GPUs, demonstrating up to a 1.40x throughput improvement. Furthermore, we develop RLSim, a high-fidelity simulator, to evaluate RFabric at scale. Our results show that RFabric achieves superior performance-cost efficiency compared to static Fat-Tree networks, establishing it as a highly effective solution for large-scale RL workloads.", "AI": {"tldr": "OrchestrRL\u662f\u4e00\u4e2a\u7528\u4e8e\u89e3\u8026RL\u8bad\u7ec3\u548c\u751f\u6210\u9636\u6bb5\u7684\u7f16\u6392\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8ba1\u7b97\u8c03\u5ea6\u548c\u53ef\u91cd\u6784\u6df7\u5408\u5149\u7535\u7f51\u7edc\u89e3\u51b3\u8ba1\u7b97\u74f6\u9888\u548c\u52a8\u6001\u7f51\u7edc\u6d41\u91cf\u95ee\u9898\u3002", "motivation": "\u5c06RL\u8bad\u7ec3\u89e3\u8026\u4e3a\u5e76\u884c\u5f02\u6b65\u6d41\u6c34\u7ebf\u867d\u7136\u80fd\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u751f\u6210\u9636\u6bb5\u56e0\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u548c\u4e25\u91cd\u6267\u884c\u4e0d\u5747\u8861\u6210\u4e3a\u74f6\u9888\uff1b2) \u89e3\u8026\u9636\u6bb5\u4ea7\u751f\u591a\u6837\u52a8\u6001\u7f51\u7edc\u6d41\u91cf\u6a21\u5f0f\uff0c\u538b\u57ae\u4f20\u7edf\u7f51\u7edc\u67b6\u6784\u3002", "method": "OrchestrRL\u91c7\u7528\u81ea\u9002\u5e94\u8ba1\u7b97\u8c03\u5ea6\u5668\u52a8\u6001\u8c03\u6574\u5e76\u884c\u5ea6\u4ee5\u5339\u914d\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\uff0c\u540c\u65f6\u8bbe\u8ba1RFabric\u53ef\u91cd\u6784\u6df7\u5408\u5149\u7535\u7f51\u7edc\u67b6\u6784\uff0c\u5229\u7528\u5149\u8def\u4ea4\u6362\u673a\u5b9e\u65f6\u91cd\u6784\u62d3\u6251\uff0c\u652f\u6301\u4e0d\u540c\u5e76\u884c\u914d\u7f6e\u4e0b\u7684\u751f\u6210\u548c\u8bad\u7ec3\u901a\u4fe1\u3002", "result": "\u572848\u4e2aH800 GPU\u7684\u7269\u7406\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0cOrchestrRL\u5b9e\u73b0\u4e86\u6700\u9ad81.40\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002RLSim\u6a21\u62df\u5668\u663e\u793aRFabric\u76f8\u6bd4\u9759\u6001Fat-Tree\u7f51\u7edc\u5177\u6709\u66f4\u4f18\u7684\u6027\u80fd\u6210\u672c\u6548\u7387\u3002", "conclusion": "OrchestrRL\u901a\u8fc7\u534f\u8c03\u8ba1\u7b97\u548c\u7f51\u7edc\u8282\u594f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89e3\u8026RL\u8bad\u7ec3\u4e2d\u7684\u74f6\u9888\u95ee\u9898\uff0cRFabric\u4e3a\u5927\u89c4\u6a21RL\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.02218", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.02218", "abs": "https://arxiv.org/abs/2601.02218", "authors": ["Berke Ates", "Filip Dobrosavljevi\u0107", "Theodoros Theodoridis", "Zhendong Su"], "title": "MLIR-Smith: A Novel Random Program Generator for Evaluating Compiler Pipelines", "comment": null, "summary": "Compilers are essential for the performance and correct execution of software and hold universal relevance across various scientific disciplines. Despite this, there is a notable lack of tools for testing and evaluating them, especially within the adaptable Multi-Level Intermediate Representation (MLIR) context. This paper addresses the need for a tool that can accommodate MLIR's extensibility, a feature not provided by previous methods such as Csmith. Here we introduce MLIR-Smith, a novel random program generator specifically designed to test and evaluate MLIR-based compiler optimizations. We demonstrate the utility of MLIR-Smith by conducting differential testing on MLIR, LLVM, DaCe, and DCIR, which led to the discovery of multiple bugs in these compiler pipelines. The introduction of MLIR-Smith not only fills a void in the realm of compiler testing but also emphasizes the importance of comprehensive testing within these systems. By providing a tool that can generate random MLIR programs, this paper enhances our ability to evaluate and improve compilers and paves the way for future tools, potentially shaping the wider landscape of software testing and quality assurance.", "AI": {"tldr": "MLIR-Smith\uff1a\u4e13\u95e8\u4e3a\u6d4b\u8bd5MLIR\u7f16\u8bd1\u5668\u4f18\u5316\u8bbe\u8ba1\u7684\u968f\u673a\u7a0b\u5e8f\u751f\u6210\u5de5\u5177\uff0c\u586b\u8865\u4e86MLIR\u53ef\u6269\u5c55\u6027\u6d4b\u8bd5\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u53d1\u73b0\u4e86\u591a\u4e2a\u7f16\u8bd1\u5668\u9519\u8bef\u3002", "motivation": "\u7f16\u8bd1\u5668\u5bf9\u8f6f\u4ef6\u6027\u80fd\u548c\u6b63\u786e\u6267\u884c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728MLIR\u8fd9\u79cd\u53ef\u6269\u5c55\u4e2d\u95f4\u8868\u793a\u6846\u67b6\u4e2d\u7f3a\u4e4f\u5408\u9002\u7684\u6d4b\u8bd5\u5de5\u5177\u3002\u73b0\u6709\u65b9\u6cd5\u5982Csmith\u65e0\u6cd5\u9002\u5e94MLIR\u7684\u53ef\u6269\u5c55\u6027\uff0c\u9700\u8981\u4e13\u95e8\u5de5\u5177\u6765\u6d4b\u8bd5MLIR\u7f16\u8bd1\u5668\u4f18\u5316\u3002", "method": "\u63d0\u51faMLIR-Smith\uff0c\u4e00\u79cd\u4e13\u95e8\u4e3aMLIR\u8bbe\u8ba1\u7684\u968f\u673a\u7a0b\u5e8f\u751f\u6210\u5668\u3002\u8be5\u5de5\u5177\u80fd\u591f\u751f\u6210\u968f\u673a\u7684MLIR\u7a0b\u5e8f\uff0c\u7528\u4e8e\u6d4b\u8bd5\u548c\u8bc4\u4f30MLIR\u7f16\u8bd1\u5668\u4f18\u5316\u3002\u901a\u8fc7\u5dee\u5206\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5728MLIR\u3001LLVM\u3001DaCe\u548cDCIR\u7b49\u591a\u4e2a\u7f16\u8bd1\u5668\u6d41\u6c34\u7ebf\u4e2d\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u4f7f\u7528MLIR-Smith\u8fdb\u884c\u5dee\u5206\u6d4b\u8bd5\uff0c\u5728MLIR\u3001LLVM\u3001DaCe\u548cDCIR\u7f16\u8bd1\u5668\u6d41\u6c34\u7ebf\u4e2d\u53d1\u73b0\u4e86\u591a\u4e2a\u9519\u8bef\u3002\u8bc1\u660e\u4e86\u8be5\u5de5\u5177\u5728\u7f16\u8bd1\u5668\u6d4b\u8bd5\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "MLIR-Smith\u586b\u8865\u4e86MLIR\u7f16\u8bd1\u5668\u6d4b\u8bd5\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u5f3a\u8c03\u4e86\u7f16\u8bd1\u5668\u7cfb\u7edf\u5168\u9762\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\u3002\u8be5\u5de5\u5177\u4e0d\u4ec5\u589e\u5f3a\u4e86\u8bc4\u4f30\u548c\u6539\u8fdb\u7f16\u8bd1\u5668\u7684\u80fd\u529b\uff0c\u8fd8\u4e3a\u672a\u6765\u8f6f\u4ef6\u6d4b\u8bd5\u548c\u8d28\u91cf\u4fdd\u8bc1\u5de5\u5177\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2601.01310", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01310", "abs": "https://arxiv.org/abs/2601.01310", "authors": ["Songyu Zhang", "Aaron Tam", "Myungjin Lee", "Shixiong Qi", "K. K. Ramakrishnan"], "title": "Making MoE based LLM inference resilient with Tarragon", "comment": null, "summary": "Mixture-of-Experts (MoE) models are increasingly used to serve LLMs at scale, but failures become common as deployment scale grows. Existing systems exhibit poor failure resilience: even a single worker failure triggers a coarse-grained, service-wide restart, discarding accumulated progress and halting the entire inference pipeline during recovery--an approach clearly ill-suited for latency-sensitive, LLM services.\n  We present Tarragon, a resilient MoE inference framework that confines the failures impact to individual workers while allowing the rest of the pipeline to continue making forward progress. Tarragon exploits the natural separation between the attention and expert computation in MoE-based transformers, treating attention workers (AWs) and expert workers (EWs) as distinct failure domains. Tarragon introduces a reconfigurable datapath to mask failures by rerouting requests to healthy workers. On top of this datapath, Tarragon implements a self-healing mechanism that relaxes the tightly synchronized execution of existing MoE frameworks. For stateful AWs, Tarragon performs asynchronous, incremental KV cache checkpointing with per-request restoration, and for stateless EWs, it leverages residual GPU memory to deploy shadow experts. These together keep recovery cost and recomputation overhead extremely low. Our evaluation shows that, compared to state-of-the-art MegaScale-Infer, Tarragon reduces failure-induced stalls by 160-213x (from ~64 s down to 0.3-0.4 s) while preserving performance when no failures occur.", "AI": {"tldr": "Tarragon\u662f\u4e00\u4e2a\u5f39\u6027\u7684MoE\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u5de5\u4f5c\u8005\u548c\u4e13\u5bb6\u5de5\u4f5c\u8005\u4f5c\u4e3a\u72ec\u7acb\u7684\u6545\u969c\u57df\uff0c\u5b9e\u73b0\u6545\u969c\u9694\u79bb\u548c\u5feb\u901f\u6062\u590d\uff0c\u663e\u8457\u51cf\u5c11\u6545\u969c\u5bfc\u81f4\u7684\u63a8\u7406\u505c\u6ede\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709MoE\u63a8\u7406\u7cfb\u7edf\u5728\u6545\u969c\u6062\u590d\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff1a\u5355\u4e2a\u5de5\u4f5c\u8005\u6545\u969c\u5c31\u4f1a\u89e6\u53d1\u7c97\u7c92\u5ea6\u7684\u670d\u52a1\u7ea7\u91cd\u542f\uff0c\u4e22\u5f03\u5df2\u7d2f\u79ef\u7684\u8fdb\u5ea6\u5e76\u6682\u505c\u6574\u4e2a\u63a8\u7406\u7ba1\u9053\uff0c\u8fd9\u5bf9\u4e8e\u5ef6\u8fdf\u654f\u611f\u7684LLM\u670d\u52a1\u6765\u8bf4\u662f\u4e0d\u53ef\u63a5\u53d7\u7684\u3002", "method": "1. \u5c06\u6ce8\u610f\u529b\u5de5\u4f5c\u8005\u548c\u4e13\u5bb6\u5de5\u4f5c\u8005\u4f5c\u4e3a\u72ec\u7acb\u7684\u6545\u969c\u57df\uff1b2. \u5f15\u5165\u53ef\u91cd\u65b0\u914d\u7f6e\u7684\u6570\u636e\u8def\u5f84\uff0c\u901a\u8fc7\u91cd\u5b9a\u5411\u8bf7\u6c42\u5230\u5065\u5eb7\u5de5\u4f5c\u8005\u6765\u5c4f\u853d\u6545\u969c\uff1b3. \u5b9e\u73b0\u81ea\u6108\u673a\u5236\uff0c\u653e\u677e\u73b0\u6709MoE\u6846\u67b6\u7684\u7d27\u5bc6\u540c\u6b65\u6267\u884c\uff1b4. \u5bf9\u6709\u72b6\u6001\u7684\u6ce8\u610f\u529b\u5de5\u4f5c\u8005\u91c7\u7528\u5f02\u6b65\u589e\u91cfKV\u7f13\u5b58\u68c0\u67e5\u70b9\uff1b5. \u5bf9\u65e0\u72b6\u6001\u7684\u4e13\u5bb6\u5de5\u4f5c\u8005\u5229\u7528\u5269\u4f59GPU\u5185\u5b58\u90e8\u7f72\u5f71\u5b50\u4e13\u5bb6\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684MegaScale-Infer\u76f8\u6bd4\uff0cTarragon\u5c06\u6545\u969c\u5bfc\u81f4\u7684\u505c\u6ede\u65f6\u95f4\u51cf\u5c11\u4e86160-213\u500d\uff08\u4ece\u7ea664\u79d2\u964d\u81f30.3-0.4\u79d2\uff09\uff0c\u540c\u65f6\u5728\u65e0\u6545\u969c\u60c5\u51b5\u4e0b\u4fdd\u6301\u6027\u80fd\u4e0d\u53d8\u3002", "conclusion": "Tarragon\u901a\u8fc7\u521b\u65b0\u7684\u6545\u969c\u9694\u79bb\u548c\u5feb\u901f\u6062\u590d\u673a\u5236\uff0c\u4e3a\u5927\u89c4\u6a21MoE\u63a8\u7406\u670d\u52a1\u63d0\u4f9b\u4e86\u9ad8\u5f39\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u670d\u52a1\u7684\u53ef\u7528\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2601.01500", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01500", "abs": "https://arxiv.org/abs/2601.01500", "authors": ["Jinxiao Zhang", "Yunpu Xu", "Xiyong Wu", "Runmin Dong", "Shenggan Cheng", "Yi Zhao", "Mengxuan Chen", "Qinrui Zheng", "Jianting Liu", "Haohuan Fu"], "title": "DiT-HC: Enabling Efficient Training of Visual Generation Model DiT on HPC-oriented CPU Cluster", "comment": null, "summary": "Generative foundation models have become an important tool for data reconstruction and simulation in scientific computing, showing a tight integration with traditional numerical simulations. At the same time, with the development of new hardware features, such as matrix acceleration units and high-bandwidth memory, CPU-based clusters offer promising opportunities to accelerate and scale such models, facilitating the unification of artificial intelligence and scientific computing. We present DiT-HC, the first system to train and scale the generative model DiT on a next-generation HPC CPU cluster. DiT-HC introduces three key techniques: (1) communication-free tensor parallelism (CFTP) with AutoMem for automated memory-aware dataflow, (2) HCOps, a suite of optimized GEMM and operator kernels leveraging vector and matrix acceleration units, and (3) a custom MPI backend that overlaps computation, communication, and memory movement. Experiments show 8.2 to 87.7 times speedups over native or public CPU libraries and 90.6% weak scaling efficiency on 256 nodes. These results demonstrate the feasibility of large-scale generative model training on CPU clusters and provide new insights for future HPC-AI co-design.", "AI": {"tldr": "DiT-HC\uff1a\u9996\u4e2a\u5728\u4e0b\u4e00\u4ee3HPC CPU\u96c6\u7fa4\u4e0a\u8bad\u7ec3\u548c\u6269\u5c55\u751f\u6210\u6a21\u578bDiT\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u901a\u4fe1\u81ea\u7531\u5f20\u91cf\u5e76\u884c\u3001\u4f18\u5316\u7b97\u5b50\u548c\u81ea\u5b9a\u4e49MPI\u540e\u7aef\u5b9e\u73b0\u663e\u8457\u52a0\u901f", "motivation": "\u968f\u7740\u751f\u6210\u57fa\u7840\u6a21\u578b\u5728\u79d1\u5b66\u8ba1\u7b97\u4e2d\u6570\u636e\u91cd\u5efa\u548c\u6a21\u62df\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u4ee5\u53ca\u65b0\u786c\u4ef6\u7279\u6027\uff08\u77e9\u9635\u52a0\u901f\u5355\u5143\u3001\u9ad8\u5e26\u5bbd\u5185\u5b58\uff09\u7684\u53d1\u5c55\uff0cCPU\u96c6\u7fa4\u4e3a\u52a0\u901f\u548c\u6269\u5c55\u6b64\u7c7b\u6a21\u578b\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4fc3\u8fdbAI\u4e0e\u79d1\u5b66\u8ba1\u7b97\u7684\u878d\u5408", "method": "\u63d0\u51faDiT-HC\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1) \u5e26AutoMem\u7684\u901a\u4fe1\u81ea\u7531\u5f20\u91cf\u5e76\u884c\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u5185\u5b58\u611f\u77e5\u6570\u636e\u6d41\uff1b2) HCOps\u4f18\u5316\u7b97\u5b50\u5957\u4ef6\uff0c\u5229\u7528\u5411\u91cf\u548c\u77e9\u9635\u52a0\u901f\u5355\u5143\uff1b3) \u81ea\u5b9a\u4e49MPI\u540e\u7aef\uff0c\u91cd\u53e0\u8ba1\u7b97\u3001\u901a\u4fe1\u548c\u5185\u5b58\u79fb\u52a8", "result": "\u5b9e\u9a8c\u663e\u793a\u6bd4\u539f\u751f\u6216\u516c\u5171CPU\u5e93\u52a0\u901f8.2\u523087.7\u500d\uff0c\u5728256\u4e2a\u8282\u70b9\u4e0a\u5b9e\u73b090.6%\u7684\u5f31\u6269\u5c55\u6548\u7387", "conclusion": "\u8bc1\u660e\u4e86\u5728CPU\u96c6\u7fa4\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765HPC-AI\u534f\u540c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3"}}
{"id": "2601.01596", "categories": ["cs.DC", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2601.01596", "abs": "https://arxiv.org/abs/2601.01596", "authors": ["Congrong Ren", "Robert Underwood", "Sheng Di", "Emrecan Kutay", "Zarija Lukic", "Aylin Yener", "Franck Cappello", "Hanqi Guo"], "title": "FFCz: Fast Fourier Correction for Spectrum-Preserving Lossy Compression of Scientific Data", "comment": null, "summary": "This paper introduces a novel technique to preserve spectral features in lossy compression based on a novel fast Fourier correction algorithm\\added{ for regular-grid data}. Preserving both spatial and frequency representations of data is crucial for applications such as cosmology, turbulent combustion, and X-ray diffraction, where spatial and frequency views provide complementary scientific insights. In particular, many analysis tasks rely on frequency-domain representations to capture key features, including the power spectrum of cosmology simulations, the turbulent energy spectrum in combustion, and diffraction patterns in reciprocal space for ptychography. However, existing compression methods guarantee accuracy only in the spatial domain while disregarding the frequency domain. To address this limitation, we propose an algorithm that corrects the errors produced by off-the-shelf ``base'' compressors such as SZ3, ZFP, and SPERR, thereby preserving both spatial and frequency representations by bounding errors in both domains. By expressing frequency-domain errors as linear combinations of spatial-domain errors, we derive a region that jointly bounds errors in both domains. Given as input the spatial errors from a base compressor and user-defined error bounds in the spatial and frequency domains, we iteratively project the spatial error vector onto the regions defined by the spatial and frequency constraints until it lies within their intersection. We further accelerate the algorithm using GPU parallelism to achieve practical performance. We validate our approach with datasets from cosmology simulations, X-ray diffraction, combustion simulation, and electroencephalography demonstrating its effectiveness in preserving critical scientific information in both spatial and frequency domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5feb\u901f\u5085\u91cc\u53f6\u6821\u6b63\u7b97\u6cd5\u7684\u65b0\u578b\u6280\u672f\uff0c\u5728\u4fdd\u6301\u7a7a\u95f4\u57df\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u6821\u6b63\u73b0\u6709\u538b\u7f29\u5668\u7684\u8bef\u5dee\u6765\u4fdd\u62a4\u9891\u57df\u7279\u5f81\uff0c\u9002\u7528\u4e8e\u5b87\u5b99\u5b66\u3001\u71c3\u70e7\u6a21\u62df\u7b49\u9700\u8981\u9891\u57df\u5206\u6790\u7684\u79d1\u5b66\u5e94\u7528\u3002", "motivation": "\u8bb8\u591a\u79d1\u5b66\u5e94\u7528\uff08\u5982\u5b87\u5b99\u5b66\u6a21\u62df\u3001\u6e4d\u6d41\u71c3\u70e7\u3001X\u5c04\u7ebf\u884d\u5c04\uff09\u9700\u8981\u540c\u65f6\u4fdd\u7559\u6570\u636e\u7684\u7a7a\u95f4\u548c\u9891\u57df\u8868\u793a\uff0c\u56e0\u4e3a\u4e24\u8005\u63d0\u4f9b\u4e92\u8865\u7684\u79d1\u5b66\u6d1e\u5bdf\u3002\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u53ea\u4fdd\u8bc1\u7a7a\u95f4\u57df\u7cbe\u5ea6\uff0c\u5ffd\u7565\u4e86\u9891\u57df\u7279\u5f81\u7684\u4fdd\u62a4\uff0c\u8fd9\u9650\u5236\u4e86\u538b\u7f29\u6570\u636e\u5728\u9891\u57df\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u53ef\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5feb\u901f\u5085\u91cc\u53f6\u6821\u6b63\u7b97\u6cd5\uff0c\u5c06\u9891\u57df\u8bef\u5dee\u8868\u8fbe\u4e3a\u7a7a\u95f4\u57df\u8bef\u5dee\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u63a8\u5bfc\u51fa\u540c\u65f6\u7ea6\u675f\u4e24\u4e2a\u57df\u8bef\u5dee\u7684\u533a\u57df\u3002\u7ed9\u5b9a\u57fa\u7840\u538b\u7f29\u5668\u7684\u7a7a\u95f4\u8bef\u5dee\u548c\u7528\u6237\u5b9a\u4e49\u7684\u7a7a\u95f4/\u9891\u57df\u8bef\u5dee\u754c\u9650\uff0c\u901a\u8fc7\u8fed\u4ee3\u6295\u5f71\u5c06\u7a7a\u95f4\u8bef\u5dee\u5411\u91cf\u6295\u5f71\u5230\u4e24\u4e2a\u7ea6\u675f\u533a\u57df\u7684\u4ea4\u96c6\u5185\u3002\u5229\u7528GPU\u5e76\u884c\u52a0\u901f\u5b9e\u73b0\u5b9e\u7528\u6027\u80fd\u3002", "result": "\u5728\u5b87\u5b99\u5b66\u6a21\u62df\u3001X\u5c04\u7ebf\u884d\u5c04\u3001\u71c3\u70e7\u6a21\u62df\u548c\u8111\u7535\u56fe\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u4fdd\u7559\u4e86\u7a7a\u95f4\u548c\u9891\u57df\u7684\u5173\u952e\u79d1\u5b66\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u538b\u7f29\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5ffd\u7565\u9891\u57df\u7279\u5f81\u4fdd\u62a4\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6821\u6b63\u57fa\u7840\u538b\u7f29\u5668\u7684\u8bef\u5dee\uff0c\u5b9e\u73b0\u4e86\u540c\u65f6\u7ea6\u675f\u7a7a\u95f4\u548c\u9891\u57df\u8bef\u5dee\u7684\u76ee\u6807\uff0c\u4e3a\u9700\u8981\u9891\u57df\u5206\u6790\u7684\u79d1\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01712", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01712", "abs": "https://arxiv.org/abs/2601.01712", "authors": ["Jiarui Wang", "Huichao Chai", "Yuanhang Zhang", "Zongjin Zhou", "Wei Guo", "Xingkun Yang", "Qiang Tang", "Bo Pan", "Jiawei Zhu", "Ke Cheng", "Yuting Yan", "Shulan Wang", "Yingjie Zhu", "Zhengfan Yuan", "Jiaqi Huang", "Yuhan Zhang", "Xiaosong Sun", "Zhinan Zhang", "Hong Zhu", "Yongsheng Zhang", "Tiantian Dong", "Zhong Xiao", "Deliang Liu", "Chengzhou Lu", "Yuan Sun", "Zhiyuan Chen", "Xinming Han", "Zaizhu Liu", "Yaoyuan Wang", "Ziyang Zhang", "Yong Liu", "Jinxin Xu", "Yajing Sun", "Zhoujun Yu", "Wenting Zhou", "Qidong Zhang", "Zhengyong Zhang", "Zhonghai Gu", "Yibo Jin", "Yongxiang Feng", "Pengfei Zuo"], "title": "RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference", "comment": null, "summary": "Real-time recommender systems execute multi-stage cascades (retrieval, pre-processing, fine-grained ranking) under strict tail-latency SLOs, leaving only tens of milliseconds for ranking. Generative recommendation (GR) models can improve quality by consuming long user-behavior sequences, but in production their online sequence length is tightly capped by the ranking-stage P99 budget. We observe that the majority of GR tokens encode user behaviors that are independent of the item candidates, suggesting an opportunity to pre-infer a user-behavior prefix once and reuse it during ranking rather than recomputing it on the critical path. Realizing this idea at industrial scale is non-trivial: the prefix cache must survive across multiple pipeline stages before the final ranking instance is determined, the user population implies cache footprints far beyond a single device, and indiscriminate pre-inference would overload shared resources under high QPS. We present RelayGR, a production system that enables in-HBM relay-race inference for GR. RelayGR selectively pre-infers long-term user prefixes, keeps their KV caches resident in HBM over the request lifecycle, and ensures the subsequent ranking can consume them without remote fetches. RelayGR combines three techniques: 1) a sequence-aware trigger that admits only at-risk requests under a bounded cache footprint and pre-inference load, 2) an affinity-aware router that co-locates cache production and consumption by routing both the auxiliary pre-infer signal and the ranking request to the same instance, and 3) a memory-aware expander that uses server-local DRAM to capture short-term cross-request reuse while avoiding redundant reloads. We implement RelayGR on Huawei Ascend NPUs and evaluate it with real queries. Under a fixed P99 SLO, RelayGR supports up to 1.5$\\times$ longer sequences and improves SLO-compliant throughput by up to 3.6$\\times$.", "AI": {"tldr": "RelayGR\u7cfb\u7edf\u901a\u8fc7\u9884\u63a8\u65ad\u548c\u7f13\u5b58\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u524d\u7f00\uff0c\u5728\u4e25\u683c\u5ef6\u8fdfSLO\u4e0b\u663e\u8457\u63d0\u5347\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u7684\u5e8f\u5217\u957f\u5ea6\u548c\u541e\u5410\u91cf", "motivation": "\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u9700\u8981\u5904\u7406\u957f\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u6765\u63d0\u5347\u63a8\u8350\u8d28\u91cf\uff0c\u4f46\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u53d7\u5230\u4e25\u683c\u5c3e\u90e8\u5ef6\u8fdfSLO\u7684\u9650\u5236\uff0c\u53ea\u80fd\u5904\u7406\u5f88\u77ed\u7684\u5e8f\u5217\u3002\u7814\u7a76\u53d1\u73b0\u5927\u90e8\u5206GR token\u7f16\u7801\u7684\u7528\u6237\u884c\u4e3a\u4e0e\u5019\u9009\u7269\u54c1\u65e0\u5173\uff0c\u8fd9\u63d0\u4f9b\u4e86\u9884\u63a8\u65ad\u548c\u7f13\u5b58\u7684\u53ef\u80fd\u6027", "method": "RelayGR\u7cfb\u7edf\u91c7\u7528\u4e09\u79cd\u5173\u952e\u6280\u672f\uff1a1) \u5e8f\u5217\u611f\u77e5\u89e6\u53d1\u5668\uff0c\u5728\u6709\u9650\u7f13\u5b58\u5360\u7528\u548c\u9884\u63a8\u65ad\u8d1f\u8f7d\u4e0b\u53ea\u5904\u7406\u6709\u98ce\u9669\u7684\u8bf7\u6c42\uff1b2) \u4eb2\u548c\u611f\u77e5\u8def\u7531\u5668\uff0c\u5c06\u7f13\u5b58\u751f\u4ea7\u548c\u6d88\u8d39\u8def\u7531\u5230\u540c\u4e00\u5b9e\u4f8b\uff1b3) \u5185\u5b58\u611f\u77e5\u6269\u5c55\u5668\uff0c\u5229\u7528\u670d\u52a1\u5668\u672c\u5730DRAM\u6355\u83b7\u77ed\u671f\u8de8\u8bf7\u6c42\u91cd\u7528", "result": "\u5728\u534e\u4e3a\u6607\u817eNPU\u4e0a\u5b9e\u73b0RelayGR\uff0c\u5728\u56fa\u5b9aP99 SLO\u4e0b\uff0c\u652f\u63011.5\u500d\u66f4\u957f\u7684\u5e8f\u5217\u957f\u5ea6\uff0c\u5e76\u5c06SLO\u5408\u89c4\u541e\u5410\u91cf\u63d0\u53473.6\u500d", "conclusion": "RelayGR\u901a\u8fc7\u521b\u65b0\u7684\u9884\u63a8\u65ad\u548c\u7f13\u5b58\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5ef6\u8fdf\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u8d28\u91cf\u548c\u7cfb\u7edf\u6027\u80fd"}}
{"id": "2601.01787", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01787", "abs": "https://arxiv.org/abs/2601.01787", "authors": ["Yuxiao Li", "Mingze Xia", "Xin Liang", "Bei Wang", "Robert Underwood", "Sheng Di", "Hemant Sharma", "Dishant Beniwal", "Franck Cappello", "Hanqi Guo"], "title": "pMSz: A Distributed Parallel Algorithm for Correcting Extrema and Morse Smale Segmentations in Lossy Compression", "comment": null, "summary": "Lossy compression, widely used by scientists to reduce data from simulations, experiments, and observations, can distort features of interest even under bounded error. Such distortions may compromise downstream analyses and lead to incorrect scientific conclusions in applications such as combustion and cosmology. This paper presents a distributed and parallel algorithm for correcting topological features, specifically, piecewise linear Morse Smale segmentations (PLMSS), which decompose the domain into monotone regions labeled by their corresponding local minima and maxima. While a single GPU algorithm (MSz) exists for PLMSS correction after compression, no methodology has been developed that scales beyond a single GPU for extreme scale data. We identify the key bottleneck in scaling PLMSS correction as the parallel computation of integral paths, a communication-intensive computation that is notoriously difficult to scale. Instead of explicitly computing and correcting integral paths, our algorithm simplifies MSz by preserving steepest ascending and descending directions across all locations, thereby minimizing interprocess communication while introducing negligible additional storage overhead. With this simplified algorithm and relaxed synchronization, our method achieves over 90% parallel efficiency on 128 GPUs on the Perlmutter supercomputer for real world datasets.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5e03\u5f0f\u5e76\u884c\u7b97\u6cd5\uff0c\u7528\u4e8e\u6821\u6b63\u538b\u7f29\u540e\u6570\u636e\u7684\u62d3\u6251\u7279\u5f81\uff08PLMSS\uff09\uff0c\u5728128\u4e2aGPU\u4e0a\u5b9e\u73b0\u8d85\u8fc790%\u7684\u5e76\u884c\u6548\u7387", "motivation": "\u6709\u635f\u538b\u7f29\u4f1a\u626d\u66f2\u6570\u636e\u4e2d\u7684\u5173\u952e\u7279\u5f81\uff0c\u53ef\u80fd\u5f71\u54cd\u4e0b\u6e38\u79d1\u5b66\u5206\u6790\u5e76\u5bfc\u81f4\u9519\u8bef\u7ed3\u8bba\u3002\u73b0\u6709\u5355GPU\u7b97\u6cd5\u65e0\u6cd5\u6269\u5c55\u5230\u6781\u7aef\u89c4\u6a21\u6570\u636e", "method": "\u7b80\u5316MSz\u7b97\u6cd5\uff0c\u901a\u8fc7\u4fdd\u7559\u6240\u6709\u4f4d\u7f6e\u7684\u9661\u5ced\u4e0a\u5347\u548c\u4e0b\u964d\u65b9\u5411\u6765\u907f\u514d\u663e\u5f0f\u8ba1\u7b97\u79ef\u5206\u8def\u5f84\uff0c\u51cf\u5c11\u8fdb\u7a0b\u95f4\u901a\u4fe1\uff0c\u91c7\u7528\u677e\u5f1b\u540c\u6b65\u673a\u5236", "result": "\u5728Perlmutter\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684128\u4e2aGPU\u4e0a\uff0c\u5bf9\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u5b9e\u73b0\u4e86\u8d85\u8fc790%\u7684\u5e76\u884c\u6548\u7387", "conclusion": "\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86PLMSS\u6821\u6b63\u7684\u6269\u5c55\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u79d1\u5b66\u6570\u636e\u7684\u62d3\u6251\u7279\u5f81\u4fdd\u62a4\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.01980", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.01980", "abs": "https://arxiv.org/abs/2601.01980", "authors": ["Manuel Parra-Roy\u00f3n", "\u00c1lvaro Rodr\u00edguez-Gallardo", "Susana S\u00e1nchez-Exp\u00f3sito", "Laura Darriba-Pol", "Jes\u00fas S\u00e1nchez-Casta\u00f1eda", "M. \u00c1ngeles Mendoza", "Juli\u00e1n Garrido", "Javier Mold\u00f3n", "Lourdes Verdes-Montenegro"], "title": "Bringing computation to the data: A MOEA-driven approach for optimising data processing in the context of the SKA and SRCNet", "comment": "8 pages", "summary": "The Square Kilometre Array (SKA) will generate unprecedented data volumes, making efficient data processing a critical challenge. Within this context, the SKA Regional Centres Network (SRCNet) must operate in a near-exascale environment where traditional data-centric computing models based on moving large datasets to centralised resources are no longer viable due to network and storage bottlenecks.\n  To address this limitation, this work proposes a shift towards distributed and in-situ computing, where computation is moved closer to the data. We explore the integration of Function-as-a-Service (FaaS) with an intelligent decision-making entity based on Evolutionary Algorithms (EAs) to optimise data-intensive workflows within SRCNet. FaaS enables lightweight and modular function execution near data sources while abstracting infrastructure management.\n  The proposed decision-making entity employs Multi-Objective Evolutionary Algorithms (MOEAs) to explore near-optimal execution plans considering execution time and energy consumption, together with constraints related to data location and transfer costs. This work establishes a baseline framework for efficient and cost-aware computation-to-data strategies within the SRCNet architecture.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5728SKA\u5929\u6587\u53f0\u6570\u636e\u5904\u7406\u4e2d\u91c7\u7528\u5206\u5e03\u5f0f\u548c\u539f\u4f4d\u8ba1\u7b97\uff0c\u7ed3\u5408FaaS\u548c\u8fdb\u5316\u7b97\u6cd5\u4f18\u5316\u6570\u636e\u5bc6\u96c6\u578b\u5de5\u4f5c\u6d41\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u96c6\u4e2d\u5f0f\u8ba1\u7b97\u7684\u6570\u636e\u4f20\u8f93\u74f6\u9888\u95ee\u9898\u3002", "motivation": "SKA\u5929\u6587\u53f0\u5c06\u4ea7\u751f\u524d\u6240\u672a\u6709\u7684\u6570\u636e\u91cf\uff0c\u4f20\u7edf\u7684\u6570\u636e\u4e2d\u5fc3\u8ba1\u7b97\u6a21\u578b\u7531\u4e8e\u7f51\u7edc\u548c\u5b58\u50a8\u74f6\u9888\u5df2\u4e0d\u53ef\u884c\u3002SRCNet\u9700\u8981\u5728\u8fd1\u827e\u7ea7\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u9700\u8981\u65b0\u7684\u8ba1\u7b97\u8303\u5f0f\u6765\u89e3\u51b3\u6570\u636e\u4f20\u8f93\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5411\u5206\u5e03\u5f0f\u548c\u539f\u4f4d\u8ba1\u7b97\u8f6c\u53d8\uff0c\u5c06\u8ba1\u7b97\u79fb\u81f3\u6570\u636e\u9644\u8fd1\u3002\u96c6\u6210\u51fd\u6570\u5373\u670d\u52a1\uff08FaaS\uff09\u4e0e\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\uff08EAs\uff09\u7684\u667a\u80fd\u51b3\u7b56\u5b9e\u4f53\uff0c\u4f18\u5316SRCNet\u4e2d\u7684\u6570\u636e\u5bc6\u96c6\u578b\u5de5\u4f5c\u6d41\u3002\u4f7f\u7528\u591a\u76ee\u6807\u8fdb\u5316\u7b97\u6cd5\uff08MOEAs\uff09\u63a2\u7d22\u8003\u8651\u6267\u884c\u65f6\u95f4\u548c\u80fd\u8017\u7684\u6700\u4f18\u6267\u884c\u8ba1\u5212\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u6210\u672c\u611f\u77e5\u7684\u8ba1\u7b97\u5230\u6570\u636e\u7b56\u7565\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u80fd\u591f\u5728\u8003\u8651\u6570\u636e\u4f4d\u7f6e\u548c\u4f20\u8f93\u6210\u672c\u7ea6\u675f\u7684\u540c\u65f6\uff0c\u4f18\u5316\u6267\u884c\u65f6\u95f4\u548c\u80fd\u8017\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aSRCNet\u67b6\u6784\u4e2d\u7684\u8ba1\u7b97\u5230\u6570\u636e\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7FaaS\u548c\u8fdb\u5316\u7b97\u6cd5\u7684\u7ed3\u5408\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9SKA\u6570\u636e\u5904\u7406\u4e2d\u7684\u5927\u89c4\u6a21\u6311\u6218\u3002"}}
{"id": "2601.02092", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.02092", "abs": "https://arxiv.org/abs/2601.02092", "authors": ["Abdullah Al Asif", "Sixing Yu", "Juan Pablo Munoz", "Arya Mazaheri", "Ali Jannesari"], "title": "SuperSFL: Resource-Heterogeneous Federated Split Learning with Weight-Sharing Super-Networks", "comment": null, "summary": "SplitFed Learning (SFL) combines federated learning and split learning to enable collaborative training across distributed edge devices; however, it faces significant challenges in heterogeneous environments with diverse computational and communication capabilities. This paper proposes \\textit{SuperSFL}, a federated split learning framework that leverages a weight-sharing super-network to dynamically generate resource-aware client-specific subnetworks, effectively mitigating device heterogeneity. SuperSFL introduces Three-Phase Gradient Fusion (TPGF), an optimization mechanism that coordinates local updates, server-side computation, and gradient fusion to accelerate convergence. In addition, a fault-tolerant client-side classifier and collaborative client--server aggregation enable uninterrupted training under intermittent communication failures. Experimental results on CIFAR-10 and CIFAR-100 with up to 100 heterogeneous clients show that SuperSFL converges $2$--$5\\times$ faster in terms of communication rounds than baseline SFL while achieving higher accuracy, resulting in up to $20\\times$ lower total communication cost and $13\\times$ shorter training time. SuperSFL also demonstrates improved energy efficiency compared to baseline methods, making it a practical solution for federated learning in heterogeneous edge environments.", "AI": {"tldr": "SuperSFL\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5206\u5272\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6743\u91cd\u5171\u4eab\u8d85\u7f51\u7edc\u52a8\u6001\u751f\u6210\u8d44\u6e90\u611f\u77e5\u7684\u5ba2\u6237\u7aef\u5b50\u7f51\u7edc\uff0c\u89e3\u51b3\u5f02\u6784\u73af\u5883\u4e2d\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u5dee\u5f02\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684SplitFed Learning\u5728\u5f02\u6784\u8fb9\u7f18\u8bbe\u5907\u73af\u5883\u4e2d\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u8bbe\u5907\u5177\u6709\u4e0d\u540c\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u80fd\u529b\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\u548c\u6536\u655b\u7f13\u6162\u3002", "method": "1) \u4f7f\u7528\u6743\u91cd\u5171\u4eab\u8d85\u7f51\u7edc\u52a8\u6001\u751f\u6210\u8d44\u6e90\u611f\u77e5\u7684\u5ba2\u6237\u7aef\u5b50\u7f51\u7edc\uff1b2) \u5f15\u5165\u4e09\u9636\u6bb5\u68af\u5ea6\u878d\u5408(TPGF)\u673a\u5236\u534f\u8c03\u672c\u5730\u66f4\u65b0\u3001\u670d\u52a1\u5668\u7aef\u8ba1\u7b97\u548c\u68af\u5ea6\u878d\u5408\uff1b3) \u91c7\u7528\u5bb9\u9519\u5ba2\u6237\u7aef\u5206\u7c7b\u5668\u548c\u534f\u4f5c\u5f0f\u5ba2\u6237\u7aef-\u670d\u52a1\u5668\u805a\u5408\u673a\u5236\u3002", "result": "\u5728CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e100\u4e2a\u5f02\u6784\u5ba2\u6237\u7aef\u6d4b\u8bd5\uff0cSuperSFL\u6bd4\u57fa\u7ebfSFL\u6536\u655b\u901f\u5ea6\u5feb2-5\u500d\uff0c\u901a\u4fe1\u8f6e\u6b21\u51cf\u5c11\uff0c\u603b\u901a\u4fe1\u6210\u672c\u964d\u4f4e20\u500d\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed13\u500d\uff0c\u540c\u65f6\u8fbe\u5230\u66f4\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "SuperSFL\u901a\u8fc7\u521b\u65b0\u7684\u8d85\u7f51\u7edc\u67b6\u6784\u548c\u4f18\u5316\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5206\u5272\u5b66\u4e60\u5728\u5f02\u6784\u73af\u5883\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u8282\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u3002"}}
{"id": "2601.02286", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.02286", "abs": "https://arxiv.org/abs/2601.02286", "authors": ["Rahul Sengupta", "Nooshin Yousefzadeh", "Manav Sanghvi", "Yash Ranjan", "Anand Rangarajan", "Sanjay Ranka", "Yashaswi Karnati", "Jeremy Dilmore", "Tushar Patel", "Ryan Casburn"], "title": "BigSUMO: A Scalable Framework for Big Data Traffic Analytics and Parallel Simulation", "comment": "6 pages, 10 figures", "summary": "With growing urbanization worldwide, efficient management of traffic infrastructure is critical for transportation agencies and city planners. It is essential to have tools that help analyze large volumes of stored traffic data and make effective interventions. To address this need, we present ``BigSUMO\", an end-to-end, scalable, open-source framework for analytics, interruption detection, and parallel traffic simulation. Our system ingests high-resolution loop detector and signal state data, along with sparse probe trajectory data. It first performs descriptive analytics and detects potential interruptions. It then uses the SUMO microsimulator for prescriptive analytics, testing hundreds of what-if scenarios to optimize traffic performance. The modular design allows integration of different algorithms for data processing and outlier detection. Built using open-source software and libraries, the pipeline is cost-effective, scalable, and easy to deploy. We hope BigSUMO will be a valuable aid in developing smart city mobility solutions.", "AI": {"tldr": "BigSUMO\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u3001\u53ef\u6269\u5c55\u7684\u5f00\u6e90\u4ea4\u901a\u5206\u6790\u6846\u67b6\uff0c\u7ed3\u5408\u63cf\u8ff0\u6027\u5206\u6790\u548cSUMO\u5fae\u89c2\u4eff\u771f\uff0c\u7528\u4e8e\u4ea4\u901a\u4e2d\u65ad\u68c0\u6d4b\u548c\u4f18\u5316\u573a\u666f\u6d4b\u8bd5\u3002", "motivation": "\u968f\u7740\u5168\u7403\u57ce\u5e02\u5316\u8fdb\u7a0b\u52a0\u5feb\uff0c\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u7684\u9ad8\u6548\u7ba1\u7406\u5bf9\u4ea4\u901a\u673a\u6784\u548c\u57ce\u5e02\u89c4\u5212\u8005\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u80fd\u591f\u5206\u6790\u5927\u91cf\u5b58\u50a8\u4ea4\u901a\u6570\u636e\u5e76\u5236\u5b9a\u6709\u6548\u5e72\u9884\u63aa\u65bd\u7684\u5de5\u5177\u3002", "method": "BigSUMO\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u5f00\u6e90\u6846\u67b6\uff0c\u9996\u5148\u6444\u5165\u9ad8\u5206\u8fa8\u7387\u73af\u5f62\u68c0\u6d4b\u5668\u548c\u4fe1\u53f7\u72b6\u6001\u6570\u636e\u4ee5\u53ca\u7a00\u758f\u63a2\u6d4b\u8f68\u8ff9\u6570\u636e\uff0c\u8fdb\u884c\u63cf\u8ff0\u6027\u5206\u6790\u548c\u4e2d\u65ad\u68c0\u6d4b\uff0c\u7136\u540e\u4f7f\u7528SUMO\u5fae\u89c2\u4eff\u771f\u5668\u8fdb\u884c\u89c4\u8303\u6027\u5206\u6790\uff0c\u6d4b\u8bd5\u6570\u767e\u79cd\u5047\u8bbe\u573a\u666f\u4ee5\u4f18\u5316\u4ea4\u901a\u6027\u80fd\u3002", "result": "\u8be5\u7cfb\u7edf\u5177\u6709\u6210\u672c\u6548\u76ca\u3001\u53ef\u6269\u5c55\u4e14\u6613\u4e8e\u90e8\u7f72\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u4ea4\u901a\u6570\u636e\u5206\u6790\u3001\u4e2d\u65ad\u68c0\u6d4b\u548c\u4f18\u5316\u573a\u666f\u6d4b\u8bd5\uff0c\u4e3a\u667a\u6167\u57ce\u5e02\u4ea4\u901a\u89e3\u51b3\u65b9\u6848\u5f00\u53d1\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8f85\u52a9\u5de5\u5177\u3002", "conclusion": "BigSUMO\u6846\u67b6\u6709\u671b\u6210\u4e3a\u5f00\u53d1\u667a\u6167\u57ce\u5e02\u4ea4\u901a\u89e3\u51b3\u65b9\u6848\u7684\u91cd\u8981\u5de5\u5177\uff0c\u5176\u5f00\u6e90\u3001\u6a21\u5757\u5316\u8bbe\u8ba1\u4f7f\u5176\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u90e8\u7f72\u4fbf\u5229\u6027\u3002"}}
{"id": "2601.02311", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02311", "abs": "https://arxiv.org/abs/2601.02311", "authors": ["Deep Pankajbhai Mehta"], "title": "Placement Semantics for Distributed Deep Learning: A Systematic Framework for Analyzing Parallelism Strategies", "comment": "8 pages, 3 tables", "summary": "Training large language models requires distributing computation across many accelerators, yet practitioners select parallelism strategies (data, tensor, pipeline, ZeRO) through trial and error because no unified systematic framework predicts their behavior. We introduce placement semantics: each strategy is specified by how it places four training states (parameters, optimizer, gradients, activations) across devices using five modes (replicated, sharded, sharded-with-gather, materialized, offloaded). From placement alone, without implementation details, we derive memory consumption and communication volume. Our predictions match published results exactly: ZeRO-3 uses 8x less memory than data parallelism at 1.5x communication cost, as reported in the original paper. We prove two conditions (gradient integrity, state consistency) are necessary and sufficient for distributed training to match single-device results, and provide composition rules for combining strategies safely. The framework unifies ZeRO Stages 1-3, Fully Sharded Data Parallel (FSDP), tensor parallelism, and pipeline parallelism as instances with different placement choices.", "AI": {"tldr": "\u63d0\u51fa\u4e86placement semantics\u6846\u67b6\uff0c\u901a\u8fc7\u72b6\u6001\u653e\u7f6e\u6a21\u5f0f\u7edf\u4e00\u5206\u6790\u5404\u79cd\u5e76\u884c\u7b56\u7565\u7684\u5185\u5b58\u6d88\u8017\u548c\u901a\u4fe1\u5f00\u9500\uff0c\u65e0\u9700\u5b9e\u73b0\u7ec6\u8282\u5373\u53ef\u7cbe\u786e\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u9700\u8981\u8de8\u591a\u4e2a\u52a0\u901f\u5668\u5206\u5e03\u8ba1\u7b97\uff0c\u4f46\u5b9e\u8df5\u8005\u53ea\u80fd\u901a\u8fc7\u8bd5\u9519\u9009\u62e9\u5e76\u884c\u7b56\u7565\uff08\u6570\u636e\u3001\u5f20\u91cf\u3001\u6d41\u6c34\u7ebf\u3001ZeRO\uff09\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7cfb\u7edf\u5316\u6846\u67b6\u6765\u9884\u6d4b\u5b83\u4eec\u7684\u884c\u4e3a\u3002", "method": "\u5f15\u5165placement semantics\uff1a\u6bcf\u4e2a\u7b56\u7565\u901a\u8fc7\u4e94\u79cd\u6a21\u5f0f\uff08\u590d\u5236\u3001\u5206\u7247\u3001\u5206\u7247\u5e26\u805a\u96c6\u3001\u7269\u5316\u3001\u5378\u8f7d\uff09\u5c06\u56db\u79cd\u8bad\u7ec3\u72b6\u6001\uff08\u53c2\u6570\u3001\u4f18\u5316\u5668\u3001\u68af\u5ea6\u3001\u6fc0\u6d3b\uff09\u653e\u7f6e\u5230\u8bbe\u5907\u4e0a\u3002\u4ec5\u4ece\u653e\u7f6e\u65b9\u5f0f\u5373\u53ef\u63a8\u5bfc\u5185\u5b58\u6d88\u8017\u548c\u901a\u4fe1\u91cf\u3002", "result": "\u9884\u6d4b\u7ed3\u679c\u4e0e\u5df2\u53d1\u8868\u7ed3\u679c\u5b8c\u5168\u5339\u914d\uff1aZeRO-3\u6bd4\u6570\u636e\u5e76\u884c\u5c11\u75288\u500d\u5185\u5b58\uff0c\u901a\u4fe1\u6210\u672c\u589e\u52a01.5\u500d\uff0c\u4e0e\u539f\u59cb\u8bba\u6587\u62a5\u544a\u4e00\u81f4\u3002\u8bc1\u660e\u4e86\u68af\u5ea6\u5b8c\u6574\u6027\u548c\u72b6\u6001\u4e00\u81f4\u6027\u662f\u5206\u5e03\u5f0f\u8bad\u7ec3\u5339\u914d\u5355\u8bbe\u5907\u7ed3\u679c\u7684\u5145\u5206\u5fc5\u8981\u6761\u4ef6\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06ZeRO Stages 1-3\u3001FSDP\u3001\u5f20\u91cf\u5e76\u884c\u548c\u6d41\u6c34\u7ebf\u5e76\u884c\u7edf\u4e00\u4e3a\u5177\u6709\u4e0d\u540c\u653e\u7f6e\u9009\u62e9\u7684\u5b9e\u4f8b\uff0c\u63d0\u4f9b\u4e86\u7ec4\u5408\u7b56\u7565\u7684\u5b89\u5168\u7ec4\u5408\u89c4\u5219\u3002"}}
